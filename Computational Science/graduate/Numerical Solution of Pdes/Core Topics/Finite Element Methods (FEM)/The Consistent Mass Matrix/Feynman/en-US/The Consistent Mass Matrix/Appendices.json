{
    "hands_on_practices": [
        {
            "introduction": "This practice explores a fundamental and highly favorable property of the consistent mass matrix $M$. By analyzing the effect of simple diagonal (Jacobi) preconditioning, you will discover that the resulting system's condition number is bounded by a small constant, independent of the mesh size. This exercise reveals why iterative methods are exceptionally effective for solving linear systems involving the consistent mass matrix, a cornerstone of many numerical schemes.",
            "id": "3454404",
            "problem": "Let $\\Omega \\subset \\mathbb{R}^{d}$ be a bounded Lipschitz domain and consider the Consistent Finite Element Method (FEM) mass matrix $M \\in \\mathbb{R}^{n \\times n}$ associated with the continuous, piecewise linear Lagrange basis $\\{\\phi_{i}\\}_{i=1}^{n}$ on a conforming, quasiuniform simplicial mesh of $\\Omega$. The mass matrix entries are defined by $M_{ij} = \\int_{\\Omega} \\phi_{i}(\\boldsymbol{x})\\,\\phi_{j}(\\boldsymbol{x})\\,\\mathrm{d}\\boldsymbol{x}$. Define the Jacobi (diagonal) preconditioner $P = \\mathrm{diag}(M)$, i.e., $P_{ii} = M_{ii}$ and $P_{ij} = 0$ for $i \\neq j$.\n\nStarting from the definition of the consistent mass matrix and only using elementwise properties of the hat functions on a simplex (without invoking pre-derived formulas for the target bound), propose the preconditioner based on Jacobi scaling and analyze the spectrum of the preconditioned operator $P^{-1}M$ (equivalently $P^{-1/2} M P^{-1/2}$). Under the quasiuniformity assumption, derive mesh-independent lower and upper bounds for the Rayleigh quotient\n$$\nR(v) = \\frac{v^{\\top} M v}{v^{\\top} P v}, \\quad v \\in \\mathbb{R}^{n} \\setminus \\{0\\},\n$$\nby decomposing the contributions elementwise and using only first principles about barycentric coordinates on a $d$-simplex. From these bounds, determine the smallest universal constant $C(d)$ depending only on the spatial dimension $d$, such that the condition number satisfies\n$$\n\\kappa\\!\\left(P^{-1} M\\right) \\leq C(d),\n$$\nindependently of the mesh size and shape regularity parameters within the quasiuniform class.\n\nExpress your final answer as a single closed-form expression in terms of $d$. No rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in the field of numerical analysis for partial differential equations, specifically concerning the preconditioning of finite element method (FEM) matrices. All terms are standard and precisely defined, and the required assumptions (quasiuniform simplicial mesh) are provided. We may therefore proceed with a full solution.\n\nThe problem asks for the smallest mesh-independent constant $C(d)$ such that the spectral condition number of the Jacobi-preconditioned consistent mass matrix, $\\kappa(P^{-1}M)$, is bounded by $C(d)$. The matrix $M$ is symmetric and positive definite (SPD). The preconditioner $P = \\mathrm{diag}(M)$ is also SPD since its diagonal entries $M_{ii} = \\int_{\\Omega} \\phi_i^2 \\mathrm{d}\\boldsymbol{x}$ are strictly positive.\n\nThe eigenvalues of the preconditioned matrix $P^{-1}M$ are real and positive. They are identical to the eigenvalues of the symmetrically preconditioned matrix $P^{-1/2}MP^{-1/2}$, which is SPD. The spectrum of $P^{-1}M$, $\\{\\lambda_k\\}$, can be characterized by the Rayleigh quotient:\n$$\n\\lambda_{\\min}(P^{-1}M) = \\min_{v \\in \\mathbb{R}^n \\setminus \\{0\\}} \\frac{v^{\\top} M v}{v^{\\top} P v} \\quad \\text{and} \\quad \\lambda_{\\max}(P^{-1}M) = \\max_{v \\in \\mathbb{R}^n \\setminus \\{0\\}} \\frac{v^{\\top} M v}{v^{\\top} P v}\n$$\nThe condition number is given by $\\kappa(P^{-1}M) = \\frac{\\lambda_{\\max}(P^{-1}M)}{\\lambda_{\\min}(P^{-1}M)}$. Our task is to find the sharpest possible upper and lower bounds for this Rayleigh quotient that are independent of the mesh.\n\nLet $v \\in \\mathbb{R}^n$ be a vector of coefficients. This vector corresponds to a continuous piecewise linear function $v_h(\\boldsymbol{x}) = \\sum_{i=1}^n v_i \\phi_i(\\boldsymbol{x})$.\nThe numerator of the Rayleigh quotient is:\n$$\nv^{\\top} M v = \\sum_{i,j=1}^n v_i M_{ij} v_j = \\sum_{i,j=1}^n v_i v_j \\int_{\\Omega} \\phi_i(\\boldsymbol{x}) \\phi_j(\\boldsymbol{x}) \\mathrm{d}\\boldsymbol{x} = \\int_{\\Omega} \\left(\\sum_{i=1}^n v_i \\phi_i(\\boldsymbol{x})\\right) \\left(\\sum_{j=1}^n v_j \\phi_j(\\boldsymbol{x})\\right) \\mathrm{d}\\boldsymbol{x} = \\int_{\\Omega} v_h(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x}\n$$\nThe denominator is:\n$$\nv^{\\top} P v = \\sum_{i=1}^n v_i^2 P_{ii} = \\sum_{i=1}^n v_i^2 M_{ii} = \\sum_{i=1}^n v_i^2 \\int_{\\Omega} \\phi_i(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x}\n$$\nWe decompose these global integrals into a sum over the elements $K$ of the simplicial mesh $\\mathcal{T}_h$. Let $\\Omega = \\bigcup_{K \\in \\mathcal{T}_h} K$.\n$$\nv^{\\top} M v = \\sum_{K \\in \\mathcal{T}_h} \\int_K v_h(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x}\n$$\nThe basis function $\\phi_i$ has support over the patch of elements connected to node $i$. Thus, we can also decompose the denominator sum:\n$$\nv^{\\top} P v = \\sum_{i=1}^n v_i^2 \\sum_{K \\in \\mathcal{T}_h, x_i \\in K} \\int_K \\phi_i(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x} = \\sum_{K \\in \\mathcal{T}_h} \\sum_{i=1}^n v_i^2 \\mathbb{I}(x_i \\in K) \\int_K \\phi_i(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x}\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\nLet us analyze the contributions on a single element $K$, which is a $d$-simplex. Let its vertices be $\\{x_{i_\\alpha}\\}_{\\alpha=0}^d$. The corresponding local basis functions are the barycentric coordinates $\\{\\lambda_\\alpha(\\boldsymbol{x})\\}_{\\alpha=0}^d$, such that $\\phi_{i_\\alpha}(\\boldsymbol{x})|_K = \\lambda_\\alpha(\\boldsymbol{x})$. The function $v_h(\\boldsymbol{x})$ restricted to $K$ is $v_h(\\boldsymbol{x})|_K = \\sum_{\\alpha=0}^d v_{i_\\alpha} \\lambda_\\alpha(\\boldsymbol{x})$. Let $v_K \\in \\mathbb{R}^{d+1}$ be the vector of local coefficients, $(v_K)_\\alpha = v_{i_\\alpha}$.\n\nThe element contribution to the numerator is:\n$$\n\\int_K v_h(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x} = \\int_K \\left(\\sum_{\\alpha=0}^d v_{i_\\alpha} \\lambda_\\alpha(\\boldsymbol{x})\\right) \\left(\\sum_{\\beta=0}^d v_{i_\\beta} \\lambda_\\beta(\\boldsymbol{x})\\right) \\mathrm{d}\\boldsymbol{x} = \\sum_{\\alpha,\\beta=0}^d v_{i_\\alpha} v_{i_\\beta} \\int_K \\lambda_\\alpha(\\boldsymbol{x}) \\lambda_\\beta(\\boldsymbol{x}) \\mathrm{d}\\boldsymbol{x}\n$$\nThis can be written as $v_K^{\\top} M^K v_K$, where $M^K$ is the $(d+1) \\times (d+1)$ element mass matrix with entries $M^K_{\\alpha\\beta} = \\int_K \\lambda_\\alpha(\\boldsymbol{x}) \\lambda_\\beta(\\boldsymbol{x}) \\mathrm{d}\\boldsymbol{x}$.\n\nThe element contribution to the denominator can be written as:\n$$\n\\sum_{i=1}^n v_i^2 \\mathbb{I}(x_i \\in K) \\int_K \\phi_i(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x} = \\sum_{\\alpha=0}^d v_{i_\\alpha}^2 \\int_K \\lambda_\\alpha(\\boldsymbol{x})^2 \\mathrm{d}\\boldsymbol{x} = v_K^{\\top} P^K v_K\n$$\nwhere $P^K = \\mathrm{diag}(M^K_{\\alpha\\alpha})$ is a diagonal matrix containing the diagonal entries of the element mass matrix.\n\nThe global Rayleigh quotient is therefore a weighted sum of local Rayleigh quotients:\n$$\nR(v) = \\frac{v^{\\top} M v}{v^{\\top} P v} = \\frac{\\sum_{K \\in \\mathcal{T}_h} v_K^{\\top} M^K v_K}{\\sum_{K \\in \\mathcal{T}_h} v_K^{\\top} P^K v_K}\n$$\nThe bounds on $R(v)$ can be found by bounding the local quotient $\\frac{v_K^{\\top} M^K v_K}{v_K^{\\top} P^K v_K}$ for an arbitrary element $K$. Let $\\lambda^K_{\\min}$ and $\\lambda^K_{\\max}$ be the minimum and maximum eigenvalues of the generalized eigenvalue problem $M^K z = \\lambda P^K z$. Then we have:\n$$\n\\lambda^K_{\\min} \\le \\frac{v_K^{\\top} M^K v_K}{v_K^{\\top} P^K v_K} \\le \\lambda^K_{\\max}\n$$\nSince $v_K^{\\top} P^K v_K \\ge 0$, we can write $\\lambda^K_{\\min} (v_K^{\\top} P^K v_K) \\le v_K^{\\top} M^K v_K \\le \\lambda^K_{\\max} (v_K^{\\top} P^K v_K)$. If these bounds $\\lambda^K_{\\min}$ and $\\lambda^K_{\\max}$ are independent of the element $K$, let's call them $\\lambda_{\\min}$ and $\\lambda_{\\max}$. Summing over all elements $K \\in \\mathcal{T}_h$ yields:\n$$\n\\lambda_{\\min} \\sum_K v_K^{\\top} P^K v_K \\le \\sum_K v_K^{\\top} M^K v_K \\le \\lambda_{\\max} \\sum_K v_K^{\\top} P^K v_K\n$$\nDividing by $\\sum_K v_K^{\\top} P^K v_K = v^{\\top}P v > 0$ gives $\\lambda_{\\min} \\le R(v) \\le \\lambda_{\\max}$.\n\nThe quasiuniformity assumption ensures that any element $K$ is an affine, non-degenerate map of a reference element $\\hat{K}$. The entries of $M^K$ and $P^K$ scale proportionally to the volume of the element, $|K|$. This scaling factor cancels in the generalized eigenvalue problem, making the eigenvalues $\\lambda$ independent of the element's size and shape. We can therefore compute these eigenvalues on a reference $d$-simplex.\n\nWe use the standard integral formula for barycentric coordinates over a $d$-simplex $K$:\n$$\n\\int_K \\lambda_0^{a_0} \\lambda_1^{a_1} \\cdots \\lambda_d^{a_d} \\mathrm{d}\\boldsymbol{x} = \\frac{a_0! a_1! \\cdots a_d! d!}{(d + \\sum_{\\alpha=0}^d a_\\alpha)!} |K|\n$$\nThe entries of the element mass matrix $M^K$ are:\nFor $\\alpha = \\beta$ (diagonal entries):\n$$\nM^K_{\\alpha\\alpha} = \\int_K \\lambda_\\alpha^2 \\mathrm{d}\\boldsymbol{x} = \\frac{2! \\, d!}{(d+2)!} |K| = \\frac{2 |K|}{(d+1)(d+2)}\n$$\nFor $\\alpha \\ne \\beta$ (off-diagonal entries):\n$$\nM^K_{\\alpha\\beta} = \\int_K \\lambda_\\alpha \\lambda_\\beta \\mathrm{d}\\boldsymbol{x} = \\frac{1! \\, 1! \\, d!}{(d+2)!} |K| = \\frac{|K|}{(d+1)(d+2)}\n$$\nThe element mass matrix $M^K \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ is:\n$$\nM^K = \\frac{|K|}{(d+1)(d+2)} \\begin{pmatrix} 2 & 1 & \\cdots & 1 \\\\ 1 & 2 & \\cdots & 1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & 1 & \\cdots & 2 \\end{pmatrix} = \\frac{|K|}{(d+1)(d+2)} (I + J)\n$$\nwhere $I$ is the identity matrix and $J$ is the matrix of all ones.\n\nThe local preconditioner matrix $P^K$ is the diagonal of $M^K$:\n$$\nP^K = \\frac{2|K|}{(d+1)(d+2)} I\n$$\nThe generalized eigenvalue problem $M^K z = \\lambda P^K z$ becomes:\n$$\n\\frac{|K|}{(d+1)(d+2)} (I + J) z = \\lambda \\left( \\frac{2|K|}{(d+1)(d+2)} I \\right) z\n$$\nThe constant factors cancel, yielding a standard eigenvalue problem:\n$$\n(I+J)z = 2\\lambda z \\implies Jz = (2\\lambda - 1)z\n$$\nWe need to find the eigenvalues of the matrix $J \\in \\mathbb{R}^{(d+1)\\times(d+1)}$. $J$ has rank $1$.\n1. The vector $\\mathbf{1} = (1, 1, \\dots, 1)^{\\top}$ is an eigenvector: $J\\mathbf{1} = (d+1)\\mathbf{1}$. The corresponding eigenvalue of $J$ is $d+1$.\n   For this eigenvector, we have $d+1 = 2\\lambda - 1$, which gives $2\\lambda = d+2$, so $\\lambda = \\frac{d+2}{2}$.\n2. The null space of $J$ has dimension $d$. Any vector $z$ with $\\sum z_\\alpha = 0$ is an eigenvector with eigenvalue $0$.\n   For these $d$ linearly independent eigenvectors, we have $0 = 2\\lambda - 1$, which gives $2\\lambda = 1$, so $\\lambda = \\frac{1}{2}$.\n\nThe eigenvalues of the local generalized problem are $\\lambda_{\\max} = \\frac{d+2}{2}$ (with multiplicity $1$) and $\\lambda_{\\min} = \\frac{1}{2}$ (with multiplicity $d$).\nThese bounds are independent of the element $K$. Therefore, for any non-zero vector $v$, the global Rayleigh quotient is bounded by:\n$$\n\\frac{1}{2} \\leq \\frac{v^{\\top} M v}{v^{\\top} P v} \\leq \\frac{d+2}{2}\n$$\nThese bounds are sharp, as they can be attained by choosing a function $v_h$ that is supported on a single element and mimics the local eigenvectors.\nThe spectrum of the preconditioned operator $P^{-1}M$ is thus contained in the interval $[\\frac{1}{2}, \\frac{d+2}{2}]$.\n\nThe condition number is bounded by the ratio of the maximum and minimum eigenvalues:\n$$\n\\kappa(P^{-1}M) = \\frac{\\lambda_{\\max}(P^{-1}M)}{\\lambda_{\\min}(P^{-1}M)} \\le \\frac{\\sup R(v)}{\\inf R(v)} = \\frac{(d+2)/2}{1/2} = d+2\n$$\nThis bound depends only on the spatial dimension $d$ and is independent of the mesh size or the shape regularity (within the non-degenerate class). Thus, the smallest such universal constant is $C(d) = d+2$.\n\nFor example:\nIn $d=1$ (line segments), $C(1)=3$.\nIn $d=2$ (triangles), $C(2)=4$.\nIn $d=3$ (tetrahedra), $C(3)=5$.\nThis result demonstrates the effectiveness of Jacobi (diagonal) scaling as a preconditioner for the FEM consistent mass matrix, yielding a condition number that is bounded by a small constant depending only on the dimension, which guarantees mesh-independent performance of iterative solvers like the preconditioned conjugate gradient method.",
            "answer": "$$\\boxed{d+2}$$"
        },
        {
            "introduction": "While the consistent mass matrix possesses excellent theoretical properties, its practical implementation involves numerical integration. This exercise serves as a crucial case study on the dangers of using quadrature rules that are not sufficiently accurate, a practice known as underintegration. By constructing a simple 1D diffusion problem, you will see firsthand how this practice can introduce non-physical, high-frequency oscillations into the numerical solution, compromising both stability and accuracy.",
            "id": "3454415",
            "problem": "Consider the transient diffusion equation on the unit interval,\n$$\n\\frac{\\partial u}{\\partial t} = \\kappa \\frac{\\partial^{2} u}{\\partial x^{2}}, \\quad x \\in (0,1), \\ t>0,\n$$\nwith homogeneous Dirichlet boundary conditions $u(0,t)=u(1,t)=0$ and an $L^{2}$ initial condition. Use a standard Galerkin finite element (FE) discretization with continuous, piecewise-linear basis functions on a uniform mesh of $3$ elements (nodes at $x=0$, $x=\\tfrac{1}{3}$, $x=\\tfrac{2}{3}$, $x=1$). Let the semi-discrete system be\n$$\nM \\, \\dot{\\mathbf{u}}(t) + \\kappa \\, K \\, \\mathbf{u}(t) = \\mathbf{0},\n$$\nwhere $M$ is the mass matrix and $K$ is the stiffness matrix. Impose the boundary conditions strongly so that the system is expressed in terms of the two interior degrees of freedom at $x=\\tfrac{1}{3}$ and $x=\\tfrac{2}{3}$.\n\nAdopt the following two choices for the mass matrix on each element of length $h=\\tfrac{1}{3}$:\n- Consistent mass: $M_{ij}^{(e)} = \\int_{e} \\varphi_{i} \\varphi_{j} \\, dx$ computed exactly.\n- Underintegrated mass: approximate $M_{ij}^{(e)}$ by the one-point Gauss rule at the element center, i.e., $\\int_{e} f(x)\\,dx \\approx h \\, f(x_{c})$, where $x_{c}$ is the element midpoint.\n\nUse exact integration for the stiffness matrix, $K_{ij}^{(e)} = \\int_{e} \\varphi_{i}' \\varphi_{j}' \\, dx$, so that the stiffness is identical in the two cases. Advance the semi-discrete system in time with the Crank–Nicolson (CN) scheme (Crank–Nicolson (CN) is the trapezoidal rule applied to the semi-discrete system), and let $\\mu$ denote a generalized eigenvalue of the matrix pencil $(K,M)$, that is, $K \\mathbf{v} = \\mu \\, M \\mathbf{v}$.\n\nStarting only from the weak form definitions of $M$ and $K$, the properties of the one-point Gauss rule, and the CN method as the trapezoidal rule in time, do the following:\n- Assemble the $2 \\times 2$ interior matrices $M$ and $K$ for both the consistent and the underintegrated mass cases.\n- Show that the two-by-two Toeplitz structure implies a highest-frequency generalized eigenvector proportional to $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ and determine its generalized eigenvalue $\\mu$ for each mass choice.\n- Derive the one-step CN amplification factor $g$ for a generalized eigenmode in terms of $\\mu$, $\\kappa$, and the time step $\\Delta t$.\n\nThen, for the underintegrated mass case with $\\kappa=1$ and $\\Delta t = 0.02$, compute the one-step CN amplification factor $g$ for the highest-frequency mode. Provide the exact value. Do not round. The amplification factor is dimensionless, so no units are required.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically sound, and well-posed. The problem statement provides all necessary information: the partial differential equation $\\frac{\\partial u}{\\partial t} = \\kappa \\frac{\\partial^{2} u}{\\partial x^{2}}$ on the domain $x \\in (0,1)$, homogeneous Dirichlet boundary conditions, a uniform mesh of $3$ elements, the use of continuous piecewise-linear basis functions, and specific instructions for assembling the mass and stiffness matrices and for the time-stepping scheme. The problem is a standard exercise in the finite element method for parabolic PDEs and is therefore valid.\n\nThe domain is the unit interval $[0,1]$. A uniform mesh with $3$ elements implies nodes at $x_0=0$, $x_1=\\frac{1}{3}$, $x_2=\\frac{2}{3}$, and $x_3=1$. The element length is $h=\\frac{1}{3}$. The problem specifies using the two interior degrees of freedom, which correspond to nodes $x_1$ and $x_2$. The basis functions are the standard linear 'hat' functions, $\\varphi_i(x)$, such that $\\varphi_i(x_j) = \\delta_{ij}$. The active basis functions are $\\varphi_1(x)$ and $\\varphi_2(x)$.\n\nFirst, we assemble the $2 \\times 2$ interior stiffness matrix $K$. The element stiffness matrix for linear basis functions on an element of length $h$ is given by $K^{(e)} = \\frac{1}{h} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$. The global stiffness matrix $K$ is assembled from the contributions of the three elements $e_1=[0, \\frac{1}{3}]$, $e_2=[\\frac{1}{3}, \\frac{2}{3}]$, and $e_3=[\\frac{2}{3}, 1]$.\nThe diagonal entries are $K_{11} = K^{(e_1)}_{22} + K^{(e_2)}_{11} = \\frac{1}{h} + \\frac{1}{h} = \\frac{2}{h}$ and $K_{22} = K^{(e_2)}_{22} + K^{(e_3)}_{11} = \\frac{1}{h} + \\frac{1}{h} = \\frac{2}{h}$.\nThe off-diagonal entry is $K_{12} = K_{21} = K^{(e_2)}_{12} = -\\frac{1}{h}$.\nWith $h=\\frac{1}{3}$, we have $\\frac{1}{h}=3$. Thus, the stiffness matrix is:\n$$ K = \\begin{pmatrix} \\frac{2}{h} & -\\frac{1}{h} \\\\ -\\frac{1}{h} & \\frac{2}{h} \\end{pmatrix} = \\begin{pmatrix} 6 & -3 \\\\ -3 & 6 \\end{pmatrix} $$\n\nNext, we assemble the consistent mass matrix, $M_{\\text{cons}}$. The element consistent mass matrix is $M_{\\text{cons}}^{(e)} = \\frac{h}{6} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. Assembling for the interior nodes:\n$M_{11} = M^{(e_1)}_{22} + M^{(e_2)}_{11} = \\frac{2h}{6} + \\frac{2h}{6} = \\frac{4h}{6} = \\frac{2h}{3}$.\n$M_{22} = M^{(e_2)}_{22} + M^{(e_3)}_{11} = \\frac{2h}{6} + \\frac{2h}{6} = \\frac{2h}{3}$.\n$M_{12} = M_{21} = M^{(e_2)}_{12} = \\frac{h}{6}$.\nWith $h=\\frac{1}{3}$:\n$$ M_{\\text{cons}} = \\begin{pmatrix} \\frac{2h}{3} & \\frac{h}{6} \\\\ \\frac{h}{6} & \\frac{2h}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{9} & \\frac{1}{18} \\\\ \\frac{1}{18} & \\frac{2}{9} \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\end{pmatrix} $$\n\nNow, assemble the underintegrated mass matrix, $M_{\\text{under}}$. The one-point Gauss rule on an element is $\\int_e f(x) dx \\approx h f(x_c)$, where $x_c$ is the element midpoint. We apply this to $M_{ij}^{(e)} = \\int_e \\varphi_i \\varphi_j dx$. For local linear basis functions $\\psi_L$ and $\\psi_R$ on an element, their values at the midpoint are $\\psi_L(x_c) = \\psi_R(x_c) = \\frac{1}{2}$.\nThe entries of the element mass matrix are approximated as:\n$M_{11}^{(e)} \\approx h (\\psi_L(x_c))^2 = h(\\frac{1}{2})^2 = \\frac{h}{4}$.\n$M_{22}^{(e)} \\approx h (\\psi_R(x_c))^2 = h(\\frac{1}{2})^2 = \\frac{h}{4}$.\n$M_{12}^{(e)} \\approx h \\psi_L(x_c) \\psi_R(x_c) = h(\\frac{1}{2})(\\frac{1}{2}) = \\frac{h}{4}$.\nSo, $M_{\\text{under}}^{(e)} = \\frac{h}{4} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$. Assembling for the interior nodes:\n$M_{11} = M^{(e_1)}_{22} + M^{(e_2)}_{11} = \\frac{h}{4} + \\frac{h}{4} = \\frac{h}{2}$.\n$M_{22} = M^{(e_2)}_{22} + M^{(e_3)}_{11} = \\frac{h}{4} + \\frac{h}{4} = \\frac{h}{2}$.\n$M_{12} = M_{21} = M^{(e_2)}_{12} = \\frac{h}{4}$.\nWith $h=\\frac{1}{3}$:\n$$ M_{\\text{under}} = \\begin{pmatrix} \\frac{h}{2} & \\frac{h}{4} \\\\ \\frac{h}{4} & \\frac{h}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{12} \\\\ \\frac{1}{12} & \\frac{1}{6} \\end{pmatrix} = \\frac{1}{12} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\n\nThe matrices $K$, $M_{\\text{cons}}$, and $M_{\\text{under}}$ are all $2 \\times 2$ symmetric Toeplitz matrices of the form $\\begin{pmatrix} a & b \\\\ b & a \\end{pmatrix}$. The eigenvectors of such matrices are always $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $\\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, with corresponding eigenvalues $a+b$ and $a-b$. The highest-frequency mode is associated with the eigenvector with alternating signs, $\\mathbf{v} = \\mathbf{v}_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\nFor $K = \\begin{pmatrix} 6 & -3 \\\\ -3 & 6 \\end{pmatrix}$, the eigenvalue for $\\mathbf{v}$ is $6 - (-3) = 9$. Thus, $K\\mathbf{v} = 9\\mathbf{v}$.\nFor $M_{\\text{cons}} = \\frac{1}{18} \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\end{pmatrix}$, the eigenvalue for $\\mathbf{v}$ is $\\frac{1}{18}(4-1) = \\frac{3}{18} = \\frac{1}{6}$. Thus, $M_{\\text{cons}}\\mathbf{v} = \\frac{1}{6}\\mathbf{v}$.\nFor $M_{\\text{under}} = \\frac{1}{12} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$, the eigenvalue for $\\mathbf{v}$ is $\\frac{1}{12}(2-1) = \\frac{1}{12}$. Thus, $M_{\\text{under}}\\mathbf{v} = \\frac{1}{12}\\mathbf{v}$.\nThe generalized eigenvalue problem is $K\\mathbf{v} = \\mu M\\mathbf{v}$. Substituting the relations for the eigenvector $\\mathbf{v}$:\n$9\\mathbf{v} = \\mu \\lambda_M \\mathbf{v}$, where $\\lambda_M$ is the eigenvalue of the mass matrix $M$. This gives $\\mu = \\frac{9}{\\lambda_M}$.\nFor consistent mass: $\\mu_{\\text{cons}} = \\frac{9}{1/6} = 54$.\nFor underintegrated mass: $\\mu_{\\text{under}} = \\frac{9}{1/12} = 108$.\n\nThe Crank-Nicolson scheme applied to $M \\dot{\\mathbf{u}} + \\kappa K \\mathbf{u} = \\mathbf{0}$ is:\n$$ M \\frac{\\mathbf{u}^{n+1} - \\mathbf{u}^n}{\\Delta t} + \\kappa K \\frac{\\mathbf{u}^{n+1} + \\mathbf{u}^n}{2} = \\mathbf{0} $$\nLet $\\mathbf{u}^n$ be a generalized eigenmode $\\mathbf{v}$. The one-step amplification factor $g$ is defined by $\\mathbf{u}^{n+1} = g\\mathbf{u}^n = g\\mathbf{v}$.\n$$ M \\frac{g\\mathbf{v} - \\mathbf{v}}{\\Delta t} + \\kappa K \\frac{g\\mathbf{v} + \\mathbf{v}}{2} = \\mathbf{0} $$\nUsing $K\\mathbf{v} = \\mu M\\mathbf{v}$ and factoring out the non-zero vector $M\\mathbf{v}$:\n$$ \\frac{g-1}{\\Delta t} + \\kappa \\mu \\frac{g+1}{2} = 0 $$\nSolving for $g$:\n$2(g-1) + \\kappa \\mu \\Delta t (g+1) = 0 \\implies g(2 + \\kappa \\mu \\Delta t) = 2 - \\kappa \\mu \\Delta t$.\n$$ g = \\frac{2 - \\kappa \\mu \\Delta t}{2 + \\kappa \\mu \\Delta t} = \\frac{1 - \\frac{\\kappa \\mu \\Delta t}{2}}{1 + \\frac{\\kappa \\mu \\Delta t}{2}} $$\n\nFinally, we compute $g$ for the underintegrated mass case, highest-frequency mode, with $\\kappa=1$ and $\\Delta t = 0.02$. The generalized eigenvalue is $\\mu = \\mu_{\\text{under}} = 108$.\nThe parameter group $\\frac{\\kappa \\mu \\Delta t}{2}$ is:\n$$ \\frac{1 \\cdot 108 \\cdot 0.02}{2} = \\frac{2.16}{2} = 1.08 $$\nSubstituting this value into the expression for $g$:\n$$ g = \\frac{1 - 1.08}{1 + 1.08} = \\frac{-0.08}{2.08} = -\\frac{8}{208} $$\nSimplifying the fraction by dividing the numerator and denominator by their greatest common divisor, which is $8$:\n$$ g = -\\frac{8 \\div 8}{208 \\div 8} = -\\frac{1}{26} $$",
            "answer": "$$\\boxed{-\\frac{1}{26}}$$"
        },
        {
            "introduction": "In large-scale explicit simulations, repeatedly inverting the consistent mass matrix $M$ is often a computational bottleneck. Building on the insight that $M$ is well-conditioned , this exercise investigates the practical strategy of using iterative methods to only *approximate* its inverse. You will analyze how controlled, inexact solves affect the stability and efficiency of time-stepping schemes, revealing the trade-offs that are central to high-performance scientific computing.",
            "id": "3454386",
            "problem": "In the finite element semi-discretization of linear partial differential equations, the consistent mass matrix $M \\in \\mathbb{R}^{n \\times n}$ arises from the $L^{2}$ inner product of basis functions, and is symmetric positive definite (SPD). Consider a shape-regular quasi-uniform mesh and standard continuous piecewise polynomial basis functions of fixed degree. Two canonical semi-discrete models are:\n- Parabolic: $M \\dot{\\mathbf{u}}(t) + K \\mathbf{u}(t) = \\mathbf{f}(t)$,\n- Hyperbolic: $M \\ddot{\\mathbf{u}}(t) + K \\mathbf{u}(t) = \\mathbf{f}(t)$,\nwhere $K$ is the stiffness matrix and $\\mathbf{f}$ a load vector. Explicit time-stepping schemes for either case require, at each step, applying an operator approximating $M^{-1}$ to a known vector. Let $\\tilde{M}^{-1}$ denote an approximation to $M^{-1}$ produced by an iterative method such as Conjugate Gradient (CG) with Jacobi (diagonal) preconditioning or a single multigrid V-cycle, and assume any such $\\tilde{M}^{-1}$ is SPD.\n\nStarting from the definition of $M$ as the Riesz map for the discrete $L^{2}$ inner product, the equivalence of norms on finite-dimensional spaces, and the standard energy method and spectral stability analysis for explicit time integrators, select all statements that are correct and justify when and why exact inversion of $M$ is unnecessary in explicit time-stepping.\n\nA. On a shape-regular quasi-uniform mesh with continuous piecewise linear finite elements, the diagonally scaled mass matrix $D^{-1/2} M D^{-1/2}$, where $D = \\operatorname{diag}(M)$, has a condition number bounded independently of the mesh size $h$. Consequently, Preconditioned Conjugate Gradient (PCG) with Jacobi preconditioning for solving $M \\mathbf{x} = \\mathbf{b}$ requires a number of iterations that does not grow with the number of unknowns to reach a fixed relative tolerance.\n\nB. In an explicit scheme for either the parabolic or the hyperbolic semi-discrete system, replacing $M^{-1}$ by a single multigrid V-cycle that defines an SPD operator $\\tilde{M}^{-1}$ spectrally equivalent to $M^{-1}$ modifies the Courant–Friedrichs–Lewy (CFL) time-step restriction only by a mesh-independent constant factor.\n\nC. For the semi-discrete heat equation advanced by forward Euler, using any inexact solve of $M \\mathbf{x} = \\mathbf{b}$ in place of the exact $M^{-1}$ makes the method unconditionally unstable, regardless of the quality of the approximation.\n\nD. If $M$ is time-independent and SPD, computing and applying its exact Cholesky factorization once will always lead to a lower total time-to-solution than performing a small fixed number of Jacobi iterations per time step, for any problem size and time horizon.\n\nE. On meshes with strong local refinement where the ratio of largest to smallest element volumes is large, the condition number of $M$ with respect to the Euclidean norm can deteriorate proportionally to this ratio; in such cases, multigrid preconditioning can reduce the PCG iteration count for $M \\mathbf{x} = \\mathbf{b}$ to a mesh-independent constant.\n\nF. In explicit leap-frog time stepping for the hyperbolic system, replacing $M^{-1}$ by any SPD operator $\\tilde{M}^{-1}$ that is spectrally equivalent to $M^{-1}$, i.e., there exist constants $0 < \\alpha \\le \\beta < \\infty$ such that $\\alpha M^{-1} \\preceq \\tilde{M}^{-1} \\preceq \\beta M^{-1}$, preserves energy stability provided the time step is reduced according to the constants $\\alpha$ and $\\beta$.",
            "solution": "The problem asks for an evaluation of statements regarding the necessity of exact inversion of the consistent mass matrix $M$ in explicit time-stepping schemes. The mass matrix $M$ is symmetric positive definite (SPD) and arises from the $L^2$ inner product of finite element basis functions, $M_{ij} = (\\phi_i, \\phi_j)_{L^2}$. It serves as the Riesz map for the discrete $L^2$ inner product, meaning the vector $M\\mathbf{u}$ represents the action of the functional corresponding to $\\mathbf{u}$ in the dual space. The stability of explicit schemes depends on the spectrum of operators involving $M^{-1}$, specifically $M^{-1}K$. We analyze each statement based on these principles.\n\n**A. On a shape-regular quasi-uniform mesh with continuous piecewise linear finite elements, the diagonally scaled mass matrix $D^{-1/2} M D^{-1/2}$, where $D = \\operatorname{diag}(M)$, has a condition number bounded independently of the mesh size $h$. Consequently, Preconditioned Conjugate Gradient (PCG) with Jacobi preconditioning for solving $M \\mathbf{x} = \\mathbf{b}$ requires a number of iterations that does not grow with the number of unknowns to reach a fixed relative tolerance.**\n\nThis statement is correct.\nThe Jacobi preconditioner is $P = D = \\operatorname{diag}(M)$. The convergence rate of PCG depends on the condition number of the preconditioned matrix, $\\kappa(P^{-1}M) = \\kappa(D^{-1}M)$. Since $D$ is SPD, this condition number is equal to the condition number of the symmetrically scaled matrix $D^{-1/2} M D^{-1/2}$.\nFor a shape-regular, quasi-uniform mesh of characteristic size $h$ in a $d$-dimensional domain, the basis functions $\\phi_i$ have local support. The diagonal entries of the mass matrix are $M_{ii} = \\int_{\\Omega} \\phi_i^2 \\, dV$, which scale as $O(h^d)$. The non-zero off-diagonal entries $M_{ij} = \\int_{\\Omega} \\phi_i \\phi_j \\, dV$ also scale as $O(h^d)$.\nThe entries of the diagonally scaled matrix are $(D^{-1/2} M D^{-1/2})_{ij} = M_{ij} / \\sqrt{M_{ii}M_{jj}}$. Since both numerator and denominator scale as $h^d$, the entries of this scaled matrix are of order $O(1)$ and independent of $h$. It is a standard result in finite element analysis that for a fixed polynomial degree on a quasi-uniform mesh, the eigenvalues of $D^{-1/2} M D^{-1/2}$ are bounded above and below by constants that are independent of the mesh size $h$.\nTherefore, its condition number $\\kappa(D^{-1/2} M D^{-1/2})$ is bounded by a constant independent of $h$.\nThe number of PCG iterations required to reach a fixed relative tolerance is proportional to the square root of the condition number of the preconditioned system. Since this condition number is bounded independently of $h$ (and thus the number of unknowns $n$), the number of PCG iterations is also bounded by a mesh-independent constant.\n\n**Verdict: Correct.**\n\n**B. In an explicit scheme for either the parabolic or the hyperbolic semi-discrete system, replacing $M^{-1}$ by a single multigrid V-cycle that defines an SPD operator $\\tilde{M}^{-1}$ spectrally equivalent to $M^{-1}$ modifies the Courant–Friedrichs–Lewy (CFL) time-step restriction only by a mesh-independent constant factor.**\n\nThis statement is correct.\nThe stability of an explicit scheme depends on the eigenvalues of the iteration matrix. For both the parabolic and hyperbolic cases, the time step restriction $\\Delta t$ is determined by the largest eigenvalue of $M^{-1}K$. For example, with forward Euler for the parabolic problem, stability requires $\\Delta t \\le 2/\\lambda_{\\max}(M^{-1}K)$. For leap-frog for the hyperbolic problem, stability requires $\\Delta t \\le 2/\\sqrt{\\lambda_{\\max}(M^{-1}K)}$.\nIf we replace $M^{-1}$ with an approximation $\\tilde{M}^{-1}$, the stability condition will depend on $\\lambda_{\\max}(\\tilde{M}^{-1}K)$.\nThe statement assumes $\\tilde{M}^{-1}$ is spectrally equivalent to $M^{-1}$, meaning there exist constants $0 < c_1 \\le c_2 < \\infty$, independent of the mesh size $h$, such that $c_1 M^{-1} \\preceq \\tilde{M}^{-1} \\preceq c_2 M^{-1}$ in the sense of positive definite matrices. This implies $c_1 \\mathbf{v}^T M^{-1} \\mathbf{v} \\le \\mathbf{v}^T \\tilde{M}^{-1} \\mathbf{v} \\le c_2 \\mathbf{v}^T M^{-1} \\mathbf{v}$ for all vectors $\\mathbf{v}$.\nLet $\\mu_j$ be the eigenvalues of the generalized eigenproblem $K\\mathbf{v} = \\mu M\\mathbf{v}$ (i.e., eigenvalues of $M^{-1}K$) and $\\tilde{\\mu}_j$ be the eigenvalues of $K\\mathbf{v} = \\tilde{\\mu} \\tilde{M}\\mathbf{v}$, where $\\tilde{M} = (\\tilde{M}^{-1})^{-1}$ (i.e., eigenvalues of $\\tilde{M}^{-1}K$). The spectral equivalence of the inverses implies $c_2^{-1} M \\preceq \\tilde{M} \\preceq c_1^{-1} M$. By the Courant-Fischer min-max principle for generalized eigenvalue problems, we have $\\frac{\\mathbf{v}^T K \\mathbf{v}}{\\mathbf{v}^T \\tilde{M} \\mathbf{v}} \\le \\frac{\\mathbf{v}^T K \\mathbf{v}}{c_2^{-1}\\mathbf{v}^T M \\mathbf{v}} = c_2 \\frac{\\mathbf{v}^T K \\mathbf{v}}{\\mathbf{v}^T M \\mathbf{v}}$, and similarly $\\frac{\\mathbf{v}^T K \\mathbf{v}}{\\mathbf{v}^T \\tilde{M} \\mathbf{v}} \\ge c_1 \\frac{\\mathbf{v}^T K \\mathbf{v}}{\\mathbf{v}^T M \\mathbf{v}}$. This leads to $c_1 \\mu_j \\le \\tilde{\\mu}_j \\le c_2 \\mu_j$ for all eigenvalues.\nThus, $\\lambda_{\\max}(\\tilde{M}^{-1}K) \\le c_2 \\lambda_{\\max}(M^{-1}K)$.\nThe new CFL condition will be of the form $\\Delta t_{new} \\le C / \\lambda_{\\max}(\\tilde{M}^{-1}K)$ (parabolic) or $\\Delta t_{new} \\le C' / \\sqrt{\\lambda_{\\max}(\\tilde{M}^{-1}K)}$ (hyperbolic). In either case, the new maximum allowable time step is at least $1/c_2$ or $1/\\sqrt{c_2}$ times the original one. Since $c_2$ is a mesh-independent constant, the CFL condition is only modified by a mesh-independent factor. It is also a known property that a single multigrid V-cycle can serve as a preconditioner for the mass matrix that is spectrally equivalent to $M^{-1}$.\n\n**Verdict: Correct.**\n\n**C. For the semi-discrete heat equation advanced by forward Euler, using any inexact solve of $M \\mathbf{x} = \\mathbf{b}$ in place of the exact $M^{-1}$ makes the method unconditionally unstable, regardless of the quality of the approximation.**\n\nThis statement is incorrect.\nThe forward Euler scheme with an inexact solve operator $\\tilde{M}^{-1}$ is $\\mathbf{u}^{n+1} = \\mathbf{u}^n - \\Delta t \\tilde{M}^{-1} K \\mathbf{u}^n$. Stability requires the spectral radius of the amplification matrix $G = I - \\Delta t \\tilde{M}^{-1} K$ to be less than or equal to $1$, i.e., $\\rho(G) \\le 1$.\nThe statement claims unconditional instability for *any* inexact solve. We can construct a counterexample. Let $\\tilde{M}^{-1}$ be an SPD operator, for instance, the one from option B produced by a multigrid V-cycle. Since $\\tilde{M}^{-1}$ is SPD and $K$ is at least positive semi-definite, the product $\\tilde{M}^{-1}K$ has real, non-negative eigenvalues $\\tilde{\\mu}_j$. The eigenvalues of $G$ are $\\lambda_j(G) = 1 - \\Delta t \\tilde{\\mu}_j$.\nThe stability condition $\\rho(G) \\le 1$ translates to $|1 - \\Delta t \\tilde{\\mu}_j| \\le 1$ for all $j$. This holds if and only if $0 \\le \\Delta t \\tilde{\\mu}_j \\le 2$. Since $\\tilde{\\mu}_j \\ge 0$, we only need to satisfy $\\Delta t \\le 2/\\lambda_{\\max}(\\tilde{M}^{-1}K)$.\nThis is a conditional stability requirement, not unconditional instability. As long as $\\lambda_{\\max}(\\tilde{M}^{-1}K)$ is finite (which it is for a finite-dimensional system), one can always find a sufficiently small $\\Delta t > 0$ to ensure stability. The statement is therefore false.\n\n**Verdict: Incorrect.**\n\n**D. If $M$ is time-independent and SPD, computing and applying its exact Cholesky factorization once will always lead to a lower total time-to-solution than performing a small fixed number of Jacobi iterations per time step, for any problem size and time horizon.**\n\nThis statement is incorrect.\nThis is a claim about computational complexity and cannot be universally true. Let $n$ be the number of degrees of freedom and $N_{steps}$ be the number of time steps.\nMethod 1 (Cholesky): The cost involves an initial factorization $M=LL^T$ and then, at each time step, two sparse triangular solves. The factorization cost for a sparse matrix arising from a $d$-dimensional FEM problem is roughly $O(n^2)$ for $d=3$ and $O(n^{1.5})$ for $d=2$ with optimal ordering, due to fill-in. The cost of the subsequent solves is roughly $O(n^{4/3})$ for $d=3$ and $O(n \\log n)$ for $d=2$. The total cost is $C_{fact} + N_{steps} \\times C_{solve}$.\nMethod 2 (Iterative): A fixed number, say $k$, of Jacobi iterations are performed per time step. Each Jacobi iteration involves one sparse matrix-vector product with $M$, which costs $O(n)$ as $M$ has $O(n)$ non-zero entries. The cost per time step is $O(k \\cdot n)$. The total cost is $N_{steps} \\times O(k \\cdot n)$.\nThe claim that Cholesky is \"always\" better is false.\nCounterexample 1: Large problem (large $n$), short time horizon (small $N_{steps}$). The initial factorization cost $C_{fact}$ can be very large. If $N_{steps}$ is small enough, the total cost of the iterative method, $N_{steps} \\times O(k \\cdot n)$, can be much smaller than $C_{fact}$ alone.\nCounterexample 2: For a 3D problem (large $n$), the cost per step for the Cholesky-based solve, $O(n^{4/3})$, is asymptotically larger than the cost per step for the iterative method, $O(k \\cdot n)$. For a sufficiently long time horizon ($N_{steps} \\gg 1$), the iterative method will eventually become more efficient.\nTherefore, the choice between methods depends on $n$, $d$, $N_{steps}$, and $k$.\n\n**Verdict: Incorrect.**\n\n**E. On meshes with strong local refinement where the ratio of largest to smallest element volumes is large, the condition number of $M$ with respect to the Euclidean norm can deteriorate proportionally to this ratio; in such cases, multigrid preconditioning can reduce the PCG iteration count for $M \\mathbf{x} = \\mathbf{b}$ to a mesh-independent constant.**\n\nThis statement is correct.\nFirst part: On a non-quasi-uniform mesh, the eigenvalues of the mass matrix $M$ are related to the local element volumes. The spectrum of $M$ is roughly contained in an interval whose endpoints are proportional to the minimum and maximum element volumes in the mesh. Thus, $\\lambda_{\\min}(M) \\propto \\min_e(\\text{vol}(e))$ and $\\lambda_{\\max}(M) \\propto \\max_e(\\text{vol}(e))$. The condition number $\\kappa_2(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$ will therefore scale with the ratio of the largest to smallest element volumes. If this ratio is large due to strong local refinement, $\\kappa_2(M)$ becomes large. A simple Jacobi preconditioner is no longer effective as its entries also span a wide range of scales.\nSecond part: Multigrid methods are particularly powerful for such problems. A well-designed multigrid preconditioner acts as an approximation to $M^{-1}$ that is spectrally equivalent to $M^{-1}$ with constants independent of not only the mesh size $h$ but also of anisotropies and local variations in coefficients (or in this case, element size). Using such a multigrid method as a preconditioner for CG results in a preconditioned system with a condition number bounded by a small constant, independent of the mesh size and refinement ratio. This leads to a PCG iteration count that is also bounded by a mesh-independent constant, confirming the optimality of the multigrid-preconditioned solver.\n\n**Verdict: Correct.**\n\n**F. In explicit leap-frog time stepping for the hyperbolic system, replacing $M^{-1}$ by any SPD operator $\\tilde{M}^{-1}$ that is spectrally equivalent to $M^{-1}$, i.e., there exist constants $0 < \\alpha \\le \\beta < \\infty$ such that $\\alpha M^{-1} \\preceq \\tilde{M}^{-1} \\preceq \\beta M^{-1}$, preserves energy stability provided the time step is reduced according to the constants $\\alpha$ and $\\beta$.**\n\nThis statement is correct.\nThe standard leap-frog scheme for $M\\ddot{\\mathbf{u}} + K\\mathbf{u} = \\mathbf{0}$ is $M\\frac{\\mathbf{u}^{n+1} - 2\\mathbf{u}^n + \\mathbf{u}^{n-1}}{(\\Delta t)^2} + K\\mathbf{u}^n = \\mathbf{0}$. The stability of this scheme requires the time step $\\Delta t$ to satisfy $\\Delta t \\sqrt{\\lambda_{\\max}(M^{-1}K)} \\le 2$.\nWhen $M^{-1}$ is replaced by an SPD approximation $\\tilde{M}^{-1}$, the scheme becomes $\\frac{\\mathbf{u}^{n+1} - 2\\mathbf{u}^n + \\mathbf{u}^{n-1}}{(\\Delta t)^2} + \\tilde{M}^{-1} K\\mathbf{u}^n = \\mathbf{0}$.\nThe stability analysis for this modified scheme leads to a new CFL condition: $\\Delta t \\sqrt{\\lambda_{\\max}(\\tilde{M}^{-1}K)} \\le 2$.\nFrom the analysis in option B, spectral equivalence $\\alpha M^{-1} \\preceq \\tilde{M}^{-1} \\preceq \\beta M^{-1}$ implies that the eigenvalues $\\tilde{\\mu}_j$ of $\\tilde{M}^{-1}K$ are bounded by the eigenvalues $\\mu_j$ of $M^{-1}K$ as $\\alpha \\mu_j \\le \\tilde{\\mu}_j \\le \\beta \\mu_j$.\nIn particular, $\\lambda_{\\max}(\\tilde{M}^{-1}K) \\le \\beta \\lambda_{\\max}(M^{-1}K)$.\nThe stability condition is satisfied if we choose $\\Delta t$ such that $\\Delta t \\sqrt{\\beta \\lambda_{\\max}(M^{-1}K)} \\le 2$. This is equivalent to $\\Delta t \\le \\frac{2}{\\sqrt{\\beta \\lambda_{\\max}(M^{-1}K)}} = \\frac{1}{\\sqrt{\\beta}} \\frac{2}{\\sqrt{\\lambda_{\\max}(M^{-1}K)}}$.\nCompared to the original time step limit $\\Delta t_{orig} \\le 2/\\sqrt{\\lambda_{\\max}(M^{-1}K)}$, the new time step limit is reduced by a factor of $1/\\sqrt{\\beta}$. Since $\\beta$ is a finite constant, the scheme remains conditionally stable, preserving the \"energy stability\" (meaning no growing modes) characteristic of leap-frog, provided the time step is made appropriately smaller.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ABEF}$$"
        }
    ]
}