{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering any numerical method is to move from theory to practice. This exercise guides you through the process of implementing the Backward-Time Central-Space (BTCS) method from first principles to solve the one-dimensional heat equation. By performing a carefully designed grid refinement study, you will empirically verify the method's theoretical convergence rate of $\\mathcal{O}(\\Delta t + \\Delta x^2)$ and learn how to isolate temporal and spatial error contributions, a fundamental skill in computational science for validating code and understanding algorithm performance .",
            "id": "3365331",
            "problem": "Consider the one-dimensional heat equation with constant diffusivity on a finite interval with homogeneous Dirichlet boundary conditions. The governing partial differential equation is $u_t = \\alpha u_{xx}$ posed on $x \\in [0,1]$, $t \\in [0,T]$, with boundary conditions $u(0,t)=0$, $u(1,t)=0$, and initial condition $u(x,0)=\\sin(\\pi x)$. The exact solution consistent with these data is $u(x,t)=\\exp(-\\alpha \\pi^2 t)\\sin(\\pi x)$. You will use the backward-time central-space (BTCS) method, defined as backward Euler in time together with the standard second-order central difference stencil in space, to approximate the solution numerically. Begin from the fundamental definitions of consistent spatial finite differences and the backward Euler time discretization, and derive the linear system that must be solved at each time step under Dirichlet boundary conditions. Ensure the algebraic system you derive is dimensionally consistent and respects the boundary values.\n\nYour task is to design and implement a grid refinement experiment to empirically verify the $\\mathcal{O}(\\Delta t+\\Delta x^2)$ convergence behavior of the BTCS method and to demonstrate how to separate temporal and spatial error contributions. Use the discrete $L^2$ norm to quantify errors, defined for a grid function $e_i$ on interior nodes by $\\|e\\|_{L^2_h} = \\sqrt{\\Delta x}\\left(\\sum_i e_i^2\\right)^{1/2}$. The observed rate with respect to a refinement parameter $h$ is to be measured by the slope of the least-squares fit of $\\log(E)$ versus $\\log(h)$ over a refinement sequence, where $E$ denotes the error in $\\| \\cdot \\|_{L^2_h}$.\n\nImplement the following three experiments, each constructed to probe a different facet of the error:\n\n- Spatial-refinement-dominated experiment (to measure the $\\mathcal{O}(\\Delta x^2)$ contribution): fix a very small time step so the temporal error is negligible relative to spatial error. Use $\\alpha = 1$, $T=0.01$, and a target time step $\\Delta t_{\\mathrm{target}}=10^{-5}$. For each $N_x \\in \\{16,32,64,128\\}$, set $\\Delta x = 1/N_x$, set the number of time steps $N_t=\\lceil T/\\Delta t_{\\mathrm{target}} \\rceil$, and then set the actual time step to $\\Delta t = T/N_t$ so that the simulation lands exactly at $t=T$. Compute the discrete $L^2$ error at $t=T$ against the exact solution and estimate the observed spatial rate by fitting $\\log(E)$ against $\\log(\\Delta x)$.\n\n- Temporal-refinement-dominated experiment (to measure the $\\mathcal{O}(\\Delta t)$ contribution): fix a very fine spatial grid so the spatial error is negligible relative to temporal error. Use $\\alpha = 1$, $T=0.01$, fix $N_x=512$ so that $\\Delta x = 1/512$, and perform runs with $N_t \\in \\{4,8,16,32\\}$, i.e., $\\Delta t = T/N_t$. Compute the discrete $L^2$ error at $t=T$ and estimate the observed temporal rate by fitting $\\log(E)$ against $\\log(\\Delta t)$.\n\n- Coupled refinement experiment (to verify the combined $\\mathcal{O}(\\Delta t + \\Delta x^2)$ behavior where contributions are of the same order): use $\\alpha = 1$, $T=0.01$, choose $N_x \\in \\{16,32,64,128\\}$, and set a coupling constant $c=0.4$. For each $N_x$, first compute the nominal $\\Delta t_{\\mathrm{nom}} = c\\,\\Delta x^2$, then set $N_t=\\max\\{1,\\mathrm{round}(T/\\Delta t_{\\mathrm{nom}})\\}$ and use the actual $\\Delta t=T/N_t$. Compute the discrete $L^2$ error at $t=T$ and estimate the observed coupled rates by fitting $\\log(E)$ against $\\log(\\Delta x)$ and separately against $\\log(\\Delta t)$.\n\nIn all experiments, implement BTCS in fully discrete form for the interior points only, treating the Dirichlet boundaries exactly. Construct the linear system based on first principles, without using any pre-packaged partial differential equation solvers. Ensure that the linear algebra is performed consistently for each time step.\n\nTest suite and required outputs:\n\n- Use the three experiments specified above, with the stated parameter values. These three experiments collectively serve as the test suite. They cover a general case, isolate spatial and temporal errors, and include edge cases such as coarse grids ($N_x=16$) and very few time steps ($N_t=4$).\n\n- For the spatial-refinement-dominated experiment, report one float: the estimated spatial order $p_{\\mathrm{space}}$.\n\n- For the temporal-refinement-dominated experiment, report one float: the estimated temporal order $p_{\\mathrm{time}}$.\n\n- For the coupled refinement experiment, report two floats: the estimated order with respect to spatial refinement $p_{\\mathrm{coupled},x}$ and the estimated order with respect to temporal refinement $p_{\\mathrm{coupled},t}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). The list must be ordered as $[p_{\\mathrm{space}},p_{\\mathrm{time}},p_{\\mathrm{coupled},x},p_{\\mathrm{coupled},t}]$. All outputs must be floats. No physical units are involved in this problem; all quantities are dimensionless.",
            "solution": "We begin with the fundamental formulation of the one-dimensional heat equation $u_t=\\alpha u_{xx}$ on $x\\in[0,1]$, subject to $u(0,t)=0$, $u(1,t)=0$, and $u(x,0)=\\sin(\\pi x)$. The exact solution for these data is $u(x,t)=\\exp(-\\alpha \\pi^2 t)\\sin(\\pi x)$, which provides a reliable reference to assess numerical error. The backward-time central-space method uses backward Euler in time and the standard second-order central finite difference for the Laplacian in space.\n\nDerivation of the fully discrete scheme follows from core definitions. Let the spatial grid have $N_x$ uniform subintervals with spacing $\\Delta x = 1/N_x$ and interior nodes $x_i=i\\Delta x$ for $i=1,2,\\dots,N_x-1$. Let the time grid have $N_t$ steps with $\\Delta t = T/N_t$ and times $t^n = n\\Delta t$ for $n=0,1,\\dots,N_t$. Define $u_i^n \\approx u(x_i,t^n)$. The backward Euler time discretization is $u_t(x_i,t^{n+1}) \\approx (u_i^{n+1} - u_i^n)/\\Delta t$. The second-order central difference for the second derivative is $u_{xx}(x_i,t^{n+1}) \\approx (u_{i-1}^{n+1} - 2u_i^{n+1} + u_{i+1}^{n+1})/\\Delta x^2$. Substituting these into $u_t=\\alpha u_{xx}$ yields\n$$(u_i^{n+1} - u_i^n)/\\Delta t = \\alpha \\frac{u_{i-1}^{n+1} - 2 u_i^{n+1} + u_{i+1}^{n+1}}{\\Delta x^2}.$$\nRearranging and defining $r = \\alpha \\Delta t / \\Delta x^2$, we obtain the linear system for interior indices $i=1,\\dots,N_x-1$,\n$$-r\\,u_{i-1}^{n+1} + (1+2r)\\,u_i^{n+1} - r\\,u_{i+1}^{n+1} = u_i^n.$$\nThe boundary conditions are $u_0^{n+1}=u_{N_x}^{n+1}=0$, so there are no additional terms on the right-hand side for homogeneous Dirichlet boundaries. In matrix form, for the interior unknown vector $u^{n+1}\\in\\mathbb{R}^{N_x-1}$, we solve\n$$(I - \\Delta t\\,\\alpha\\,L_h) u^{n+1} = u^n,$$\nwhere $L_h$ is the discrete Laplacian operator with second-order central differences scaled by $\\Delta x^{-2}$. The coefficient matrix is strictly diagonally dominant and symmetric positive definite for $\\alpha>0$, so the system is uniquely solvable for any $\\Delta t>0$. The method is unconditionally stable and exhibits a local truncation error that is first order in time and second order in space for sufficiently smooth solutions; thus the global error for smooth solutions behaves as $\\mathcal{O}(\\Delta t + \\Delta x^2)$.\n\nTo empirically verify this behavior and to separate temporal and spatial contributions, we design three experiments:\n\n1. Spatial-refinement-dominated experiment: fix a very small $\\Delta t$ so that the $\\mathcal{O}(\\Delta t)$ term becomes negligible compared to $\\mathcal{O}(\\Delta x^2)$ across spatial refinements. We use $\\alpha=1$, $T=0.01$, and a target $\\Delta t_{\\mathrm{target}}=10^{-5}$. For each $N_x\\in\\{16,32,64,128\\}$, set $N_t=\\lceil T/\\Delta t_{\\mathrm{target}}\\rceil$ and $\\Delta t=T/N_t$. For each grid we compute the error $E(\\Delta x)$ in the discrete $L^2$ norm at $t=T$ and estimate the spatial order by least-squares fitting of $\\log(E)$ against $\\log(\\Delta x)$. Because $\\Delta t$ is much smaller than $\\Delta x^2$ on the refined grids (for instance, for $N_x=128$, $\\Delta x^2 \\approx 6.10\\times 10^{-5}$ while $\\Delta t=10^{-5}$), the observed slope should be close to $2$.\n\n2. Temporal-refinement-dominated experiment: fix a very fine spatial grid so that the $\\mathcal{O}(\\Delta x^2)$ spatial error is negligible compared to the time discretization error. We use $\\alpha=1$, $T=0.01$, fix $N_x=512$ so that $\\Delta x^2 \\approx 3.81\\times 10^{-6}$, and vary $N_t\\in\\{4,8,16,32\\}$ so that $\\Delta t$ ranges from $2.5\\times 10^{-3}$ down to $3.125\\times 10^{-4}$, all much larger than $\\Delta x^2$. We compute the error $E(\\Delta t)$ and fit $\\log(E)$ against $\\log(\\Delta t)$. The slope should be close to $1$.\n\n3. Coupled refinement experiment: set $\\Delta t = c\\,\\Delta x^2$ (modulo rounding to align the final time), so that the temporal and spatial errors are of the same order. We use $\\alpha=1$, $T=0.01$, $c=0.4$, and $N_x\\in\\{16,32,64,128\\}$. For each $N_x$, compute $\\Delta t_{\\mathrm{nom}}=c\\,\\Delta x^2$, then choose $N_t=\\max\\{1,\\mathrm{round}(T/\\Delta t_{\\mathrm{nom}})\\}$ and $\\Delta t=T/N_t$. Compute the error at $t=T$; when fitting against $\\Delta x$, the error behaves like $\\mathcal{O}(\\Delta x^2)$, yielding an observed slope near $2$. When fitting against $\\Delta t$, use the relation $\\Delta t\\propto \\Delta x^2$ so that the error behaves like $\\mathcal{O}(\\Delta t)$, giving a slope near $1$.\n\nAlgorithmic design details are as follows:\n\n- Assemble the tridiagonal matrix with main diagonal entries $(1+2r)$ and off-diagonals $(-r)$, where $r=\\alpha \\Delta t/\\Delta x^2$. This corresponds to the operator $(I - \\Delta t\\,\\alpha\\,L_h)$ acting on the interior vector.\n\n- Pre-factor the sparse matrix once per choice of $(\\Delta x,\\Delta t)$ using a sparse direct solver to accelerate the repeated solves at each time step.\n\n- Advance in time for $N_t$ steps using the recurrence given by the linear system. The right-hand side at each step is the previous interior vector, since boundary values are zero and constant.\n\n- Evaluate the exact solution at interior grid points $x_i=i\\Delta x$ for $i=1,\\dots,N_x-1$ and $t=T$, compute the discrete $L^2$ norm of the error $e_i=u_i^{N_t}-u(x_i,T)$ as $\\|e\\|_{L^2_h}=\\sqrt{\\Delta x}\\left(\\sum_i e_i^2\\right)^{1/2}$.\n\n- Estimate observed convergence rates as slopes in a least-squares sense using a linear fit of $\\log(E)$ versus $\\log(h)$, where $h$ is $\\Delta x$ or $\\Delta t$ depending on the experiment.\n\nThe final program executes the three experiments, computes the four requested metrics $[p_{\\mathrm{space}},p_{\\mathrm{time}},p_{\\mathrm{coupled},x},p_{\\mathrm{coupled},t}]$, and prints them on a single line as a comma-separated list in square brackets. For a smooth exact solution, we expect values close to $[2,1,2,1]$, up to small deviations due to finite sample sizes, rounding of $N_t$, and the least-squares fit over a small number of refinement levels.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, csc_matrix\nfrom scipy.sparse.linalg import splu\n\ndef exact_solution(x, t, alpha):\n    # Exact solution: exp(-alpha*pi^2*t) * sin(pi*x)\n    return np.exp(-alpha * (np.pi**2) * t) * np.sin(np.pi * x)\n\ndef build_btcs_solver(alpha, dx, dt, nx_interior):\n    # Build (I - dt*alpha*L_h) where L_h is centered second-difference / dx^2\n    r = alpha * dt / (dx * dx)\n    main = (1.0 + 2.0 * r) * np.ones(nx_interior)\n    off = (-r) * np.ones(nx_interior - 1)\n    A = diags(diagonals=[off, main, off], offsets=[-1, 0, 1], format='csc')\n    # Factorize once for repeated solves\n    solver = splu(A)\n    return solver\n\ndef btcs_heat(alpha, T, Nx, Nt):\n    # Returns interior solution at final time T, and grid spacing dx\n    dx = 1.0 / Nx\n    dt = T / Nt\n    nx_interior = Nx - 1\n    # Interior grid points x_i = i*dx, i = 1..Nx-1\n    x_interior = np.linspace(dx, 1.0 - dx, nx_interior)\n\n    # Initial condition on interior: sin(pi x)\n    u = np.sin(np.pi * x_interior).copy()\n\n    # Pre-factorized solver for the linear system\n    solver = build_btcs_solver(alpha, dx, dt, nx_interior)\n\n    # Time stepping\n    for _ in range(Nt):\n        u = solver.solve(u)\n\n    return u, x_interior, dx, dt\n\ndef discrete_L2_error(u_num, x_interior, dx, T, alpha):\n    u_ex = exact_solution(x_interior, T, alpha)\n    e = u_num - u_ex\n    return np.sqrt(dx) * np.linalg.norm(e, ord=2)\n\ndef observed_order(h_list, E_list):\n    # Fit log(E) = p * log(h) + c\n    logh = np.log(np.array(h_list))\n    logE = np.log(np.array(E_list))\n    p, _ = np.polyfit(logh, logE, 1)\n    return p\n\ndef spatial_refinement_experiment():\n    alpha = 1.0\n    T = 0.01\n    Nx_list = [16, 32, 64, 128]\n    # Target very small dt to suppress temporal error; adjust Nt to land on T\n    dt_target = 1e-5\n    Nt = int(np.ceil(T / dt_target))\n    # Final actual dt used\n    dt_actual = T / Nt\n\n    dx_list = []\n    E_list = []\n    for Nx in Nx_list:\n        u_num, x_interior, dx, _ = btcs_heat(alpha, T, Nx, Nt)\n        E = discrete_L2_error(u_num, x_interior, dx, T, alpha)\n        dx_list.append(dx)\n        E_list.append(E)\n    p_space = observed_order(dx_list, E_list)\n    return p_space\n\ndef temporal_refinement_experiment():\n    alpha = 1.0\n    T = 0.01\n    Nx = 512  # very fine spatial grid to suppress spatial error\n    Nt_list = [4, 8, 16, 32]\n\n    dt_list = []\n    E_list = []\n    for Nt in Nt_list:\n        u_num, x_interior, dx, dt = btcs_heat(alpha, T, Nx, Nt)\n        E = discrete_L2_error(u_num, x_interior, dx, T, alpha)\n        dt_list.append(dt)\n        E_list.append(E)\n    p_time = observed_order(dt_list, E_list)\n    return p_time\n\ndef coupled_refinement_experiment():\n    alpha = 1.0\n    T = 0.01\n    Nx_list = [16, 32, 64, 128]\n    c = 0.4\n\n    dx_list = []\n    dt_list = []\n    E_list = []\n    for Nx in Nx_list:\n        dx = 1.0 / Nx\n        dt_nom = c * dx * dx\n        # Choose Nt so that dt ~ c*dx^2 and T is reached exactly\n        Nt = int(max(1, round(T / dt_nom)))\n        u_num, x_interior, dx_used, dt_used = btcs_heat(alpha, T, Nx, Nt)\n        E = discrete_L2_error(u_num, x_interior, dx_used, T, alpha)\n        dx_list.append(dx_used)\n        dt_list.append(dt_used)\n        E_list.append(E)\n    p_coupled_x = observed_order(dx_list, E_list)\n    p_coupled_t = observed_order(dt_list, E_list)\n    return p_coupled_x, p_coupled_t\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Three experiments: spatial-dominated, temporal-dominated, coupled refinement\n    results = []\n    p_space = spatial_refinement_experiment()\n    results.append(f\"{p_space:.6f}\")\n    p_time = temporal_refinement_experiment()\n    results.append(f\"{p_time:.6f}\")\n    p_cx, p_ct = coupled_refinement_experiment()\n    results.append(f\"{p_cx:.6f}\")\n    results.append(f\"{p_ct:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "No single numerical method is universally superior; choosing the right tool requires understanding its trade-offs. This practice situates the BTCS method in a comparative context by contrasting it with the popular Crank-Nicolson (CN) scheme . Through theoretical analysis, you will explore the differences in accuracy and stability, paying special attention to the behavior of each method when the time step $\\Delta t$ is large, which reveals the crucial distinction between A-stability and the more stringent L-stability that makes BTCS particularly robust for stiff problems.",
            "id": "3365265",
            "problem": "Consider the linear parabolic partial differential equation $u_t = \\kappa\\,u_{xx}$ on the spatial interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0,t)=0$ and $u(1,t)=0$, and smooth initial data $u(x,0)=u_0(x)$. Let the spatial grid be uniform with spacing $\\Delta x$ and interior grid points $x_j=j\\,\\Delta x$ for $j=1,2,\\dots,N_x$, and let the temporal grid be uniform with step $\\Delta t$ and $t^n=n\\,\\Delta t$ for $n=0,1,\\dots,N_t$, so that the final time is $T=N_t\\,\\Delta t$. The backward-time central-space method (BTCS) is obtained by applying backward Euler in time and the second-order central difference in space, while the Crank–Nicolson method (CN) is obtained by applying the trapezoidal rule in time and the same second-order central difference in space. For both methods, the discrete Laplacian is formed with the standard three-point central stencil. Assume exact linear solves and no roundoff issues.\n\nUsing consistency (via Taylor expansion) and stability (via Fourier or von Neumann analysis) from first principles of linear parabolic problems, compare the global error at the fixed final time $T$ for BTCS and Crank–Nicolson on the given mesh, and explain any observed differences in accuracy and possible oscillations when $\\Delta t$ is large relative to $\\Delta x^2$. Which of the following statements are true?\n\nA. For fixed $T$ and a fixed uniform mesh, the global error of BTCS scales like $\\mathcal{O}(\\Delta x^2+\\Delta t)$, whereas the global error of Crank–Nicolson scales like $\\mathcal{O}(\\Delta x^2+\\Delta t^2)$.\n\nB. Both BTCS and Crank–Nicolson are unconditionally stable for the heat equation, but when $\\Delta t$ is large compared to $\\Delta x^2$, Crank–Nicolson can exhibit oscillations in time and space because the amplification factor for high-frequency modes approaches $-1$, while BTCS maintains positive amplification and strong damping of high-frequency modes.\n\nC. For any $\\Delta t$ and $\\Delta x$, Crank–Nicolson strictly preserves the discrete maximum principle and the positivity of the numerical solution, whereas BTCS may violate positivity for large $\\Delta t$.\n\nD. As $\\Delta t$ increases with $\\Delta x$ fixed, the magnitude of the Crank–Nicolson amplification factor for the highest spatial frequencies approaches $1$ with sign approaching $-1$ (indicating lack of stiff decay and potential oscillations), whereas the BTCS amplification factor for the same modes approaches $0$ (indicating strong stiff decay), i.e., BTCS is $L$-stable while Crank–Nicolson is not.\n\nE. At fixed $T$, the accumulated global error of Crank–Nicolson must be smaller than that of BTCS for all choices of $\\Delta t$ and $\\Delta x$ because Crank–Nicolson is second-order accurate in time.",
            "solution": "The user has provided a problem statement comparing the Backward-Time Central-Space (BTCS) and Crank-Nicolson (CN) methods for the linear heat equation.\n\n### Step 1: Extract Givens\n-   **Partial Differential Equation (PDE)**: $u_t = \\kappa\\,u_{xx}$\n-   **Spatial Domain**: $(0,1)$\n-   **Boundary Conditions (BCs)**: Homogeneous Dirichlet, $u(0,t)=0$ and $u(1,t)=0$.\n-   **Initial Condition (IC)**: $u(x,0)=u_0(x)$, where $u_0(x)$ is a smooth function.\n-   **Discretization**: Uniform spatial grid with spacing $\\Delta x$ ($x_j=j\\,\\Delta x$) and uniform temporal grid with step $\\Delta t$ ($t^n=n\\,\\Delta t$).\n-   **BTCS Method**: Backward Euler in time, second-order central difference in space.\n-   **Crank-Nicolson (CN) Method**: Trapezoidal rule in time, second-order central difference in space.\n-   **Assumptions**: Exact linear solves and no roundoff errors.\n-   **Analysis Tools**: Consistency (Taylor expansion) and stability (Fourier or von Neumann analysis).\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard exercise in the numerical analysis of partial differential equations. The heat equation is a fundamental model, and BTCS and CN are canonical finite difference methods. The analytical tools requested (Taylor and von Neumann analyses) are standard and appropriate. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The problem asks for a comparison of well-defined numerical methods using established theoretical frameworks. The questions regarding accuracy, error scaling, stability, and oscillations are precise and have definite answers within the field of numerical analysis.\n3.  **Objective**: The problem is stated using precise, objective, and technical terminology. There is no ambiguity or subjective content.\n\nThe problem statement is complete, consistent, and well-posed. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be derived.\n\n### Derivation of Method Properties\n\nLet $U_j^n$ be the numerical approximation of the solution $u(x_j, t^n)$. The second-order central difference operator for the spatial derivative is given by $\\delta_x^2 U_j = \\frac{U_{j+1} - 2U_j + U_{j-1}}{\\Delta x^2}$. The semi-discretized PDE is $\\frac{d U_j}{dt} = \\kappa \\delta_x^2 U_j$.\n\n**1. Finite Difference Schemes**\n\n-   **BTCS (Backward Euler time-stepping)**:\n    $$\n    \\frac{U_j^{n+1} - U_j^n}{\\Delta t} = \\kappa \\delta_x^2 U_j^{n+1} = \\kappa \\frac{U_{j+1}^{n+1} - 2U_j^{n+1} + U_{j-1}^{n+1}}{\\Delta x^2}\n    $$\n    This is an implicit method.\n\n-   **Crank-Nicolson (Trapezoidal rule time-stepping)**:\n    $$\n    \\frac{U_j^{n+1} - U_j^n}{\\Delta t} = \\frac{\\kappa}{2} \\left( \\delta_x^2 U_j^{n+1} + \\delta_x^2 U_j^n \\right)\n    $$\n    This is also an implicit method.\n\n**2. Consistency and Local Truncation Error (LTE)**\n\nThe LTE, $\\tau$, is the residual obtained by substituting the exact solution $u(x,t)$ into the discrete scheme.\n-   **BTCS**: We perform a Taylor expansion around the point $(x_j, t^{n+1})$.\n    $$\n    \\tau_{BTCS} = \\frac{u_j^{n+1} - u_j^n}{\\Delta t} - \\kappa \\delta_x^2 u_j^{n+1} = \\left( u_t - \\frac{\\Delta t}{2}u_{tt} + \\dots \\right) - \\kappa \\left( u_{xx} + \\frac{\\Delta x^2}{12}u_{xxxx} + \\dots \\right)\n    $$\n    Since $u_t=\\kappa u_{xx}$, the leading error terms are $\\tau_{BTCS} = -\\frac{\\Delta t}{2}u_{tt} - \\kappa\\frac{\\Delta x^2}{12}u_{xxxx} + \\mathcal{O}(\\Delta t^2, \\Delta x^4)$. The method is first-order accurate in time and second-order in space. The LTE is $\\mathcal{O}(\\Delta t, \\Delta x^2)$.\n\n-   **Crank-Nicolson**: We perform a Taylor expansion around $(x_j, t^{n+1/2})$. The time derivative is a central difference about $t^{n+1/2}$, and the spatial term is an average of the discretizations at $t^n$ and $t^{n+1}$.\n    $$\n    \\frac{u_j^{n+1} - u_j^n}{\\Delta t} = u_t(t^{n+1/2}) + \\mathcal{O}(\\Delta t^2)\n    $$\n    $$\n    \\frac{1}{2}(\\delta_x^2 u_j^{n+1} + \\delta_x^2 u_j^n) = \\delta_x^2 u_j(t^{n+1/2}) + \\mathcal{O}(\\Delta t^2) = u_{xx}(t^{n+1/2}) + \\mathcal{O}(\\Delta x^2) + \\mathcal{O}(\\Delta t^2)\n    $$\n    The LTE is $\\tau_{CN} = \\mathcal{O}(\\Delta t^2, \\Delta x^2)$. The method is second-order accurate in both time and space.\n\n**3. Stability (von Neumann Analysis)**\n\nWe seek solutions of the form $U_j^n = g^n e^{ikx_j}$, where $k$ is the spatial wavenumber and $g$ is the amplification factor. Let $\\xi = k\\Delta x$. The discrete Laplacian acts on this mode as $\\delta_x^2 e^{ikx_j} = -\\frac{4\\sin^2(\\xi/2)}{\\Delta x^2} e^{ikx_j}$. Let $r=\\frac{\\kappa\\Delta t}{\\Delta x^2}$. For stability, we require $|g| \\le 1$ for all $\\xi \\in [-\\pi, \\pi]$.\n\n-   **BTCS**: Substituting the mode into the scheme gives:\n    $$\n    \\frac{g-1}{\\Delta t} = g \\left( -\\kappa \\frac{4\\sin^2(\\xi/2)}{\\Delta x^2} \\right) \\implies g-1 = -4rg\\sin^2(\\xi/2)\n    $$\n    $$\n    g_{BTCS} = \\frac{1}{1 + 4r\\sin^2(\\xi/2)}\n    $$\n    Since $r > 0$ and $\\sin^2(\\xi/2) \\ge 0$, the denominator is always $\\ge 1$. Therefore, $0 < g_{BTCS} \\le 1$ for all $r$ and $\\xi$. The method is unconditionally stable.\n\n-   **Crank-Nicolson**: Substituting the mode into the scheme gives:\n    $$\n    \\frac{g-1}{\\Delta t} = \\frac{g+1}{2} \\left( -\\kappa \\frac{4\\sin^2(\\xi/2)}{\\Delta x^2} \\right) \\implies g-1 = -(g+1) (2r\\sin^2(\\xi/2))\n    $$\n    $$\n    g_{CN} = \\frac{1 - 2r\\sin^2(\\xi/2)}{1 + 2r\\sin^2(\\xi/2)}\n    $$\n    The magnitude is $|g_{CN}| = \\left| \\frac{1 - X}{1 + X} \\right|$ where $X = 2r\\sin^2(\\xi/2) \\ge 0$. Since $X \\ge 0$, $|1-X| \\le 1+X$, so $|g_{CN}| \\le 1$ for all $r$ and $\\xi$. The method is unconditionally stable.\n\n**4. Behavior for Large $\\Delta t$ and L-stability**\n\nWhen $\\Delta t$ is large relative to $\\Delta x^2$, the parameter $r$ is large. We analyze the amplification factor for high-frequency modes (e.g., the highest resolvable frequency on the grid, where $\\xi = \\pi$ and $\\sin^2(\\xi/2) = 1$).\n\n-   **BTCS**: As $r \\to \\infty$, $g_{BTCS} = \\frac{1}{1+4r} \\to 0$. High-frequency components are strongly damped. A-stable methods for which the amplification factor $g(z) \\to 0$ as $\\text{Re}(z) \\to -\\infty$ (where $z=\\lambda \\Delta t$ from the test equation $y'=\\lambda y$) are called L-stable. BTCS is L-stable. This property is desirable for stiff problems, as it damps spurious high-frequency oscillations.\n\n-   **Crank-Nicolson**: As $r \\to \\infty$, $g_{CN} = \\frac{1-2r}{1+2r} \\to -1$. The magnitude of the amplification factor approaches $1$, so high-frequency modes are not damped. The negative sign indicates that the amplitude of these modes flips sign at each time step, leading to non-physical oscillations. CN is A-stable but not L-stable.\n\n**5. Positivity and Maximum Principle**\n\n-   **BTCS**: The scheme can be written as $A U^{n+1} = U^n$, where the matrix $A$ is tridiagonal, symmetric positive definite, and an M-matrix for all $r>0$. The inverse of an M-matrix has non-negative entries, $A^{-1} \\ge 0$. Thus, if the solution $U^n$ is non-negative, $U^{n+1}=A^{-1}U^n$ will also be non-negative. BTCS is unconditionally positivity-preserving and satisfies a discrete maximum principle.\n\n-   **Crank-Nicolson**: The scheme can be written as $A U^{n+1} = B U^n$. Positivity requires the operator that maps $U^n \\to U^{n+1}$ to be positivity-preserving. The right-hand side is $(B U^n)_j = \\frac{r}{2} U_{j-1}^n + (1-r) U_j^n + \\frac{r}{2} U_{j+1}^n$. For this expression to be non-negative for any non-negative $U^n$, all coefficients must be non-negative. This requires $1-r \\ge 0$, which means $r = \\frac{\\kappa\\Delta t}{\\Delta x^2} \\le 1$. If this condition is violated (i.e., for large $\\Delta t$), CN can produce negative values from positive data and violates the maximum principle.\n\n### Option-by-Option Analysis\n\n**A. For fixed $T$ and a fixed uniform mesh, the global error of BTCS scales like $\\mathcal{O}(\\Delta x^2+\\Delta t)$, whereas the global error of Crank–Nicolson scales like $\\mathcal{O}(\\Delta x^2+\\Delta t^2)$.**\nThe phrasing \"for a fixed uniform mesh\" is slightly imprecise; the statement concerns the asymptotic behavior as the mesh is refined. For a stable method applied to a well-posed problem, the order of the global error is the same as the order of the LTE. Our consistency analysis showed that the LTE for BTCS is $\\mathcal{O}(\\Delta t, \\Delta x^2)$ and for CN is $\\mathcal{O}(\\Delta t^2, \\Delta x^2)$. The global error at a fixed final time $T$ therefore scales as $\\mathcal{O}(\\Delta t + \\Delta x^2)$ for BTCS and $\\mathcal{O}(\\Delta t^2 + \\Delta x^2)$ for CN.\n**Verdict: Correct.**\n\n**B. Both BTCS and Crank–Nicolson are unconditionally stable for the heat equation, but when $\\Delta t$ is large compared to $\\Delta x^2$, Crank–Nicolson can exhibit oscillations in time and space because the amplification factor for high-frequency modes approaches $-1$, while BTCS maintains positive amplification and strong damping of high-frequency modes.**\nOur stability analysis confirmed that both methods are unconditionally stable. Our analysis of large $\\Delta t$ (large $r$) showed that for high-frequency modes, $g_{CN} \\to -1$, causing oscillations, while $g_{BTCS} \\to 0$, causing strong damping. The amplification factor for BTCS is always positive. The statement accurately describes the behavior derived from first principles.\n**Verdict: Correct.**\n\n**C. For any $\\Delta t$ and $\\Delta x$, Crank–Nicolson strictly preserves the discrete maximum principle and the positivity of the numerical solution, whereas BTCS may violate positivity for large $\\Delta t$.**\nOur analysis of positivity showed the exact opposite. BTCS is unconditionally positivity-preserving. Crank-Nicolson only preserves positivity under the condition $r = \\frac{\\kappa\\Delta t}{\\Delta x^2} \\le 1$. For large $\\Delta t$, CN can violate positivity.\n**Verdict: Incorrect.**\n\n**D. As $\\Delta t$ increases with $\\Delta x$ fixed, the magnitude of the Crank–Nicolson amplification factor for the highest spatial frequencies approaches $1$ with sign approaching $-1$ (indicating lack of stiff decay and potential oscillations), whereas the BTCS amplification factor for the same modes approaches $0$ (indicating strong stiff decay), i.e., BTCS is $L$-stable while Crank–Nicolson is not.**\nThis is a more formal and quantitative restatement of the behavior described in option B. Our analysis of the amplification factors in the limit of large $r$ (increasing $\\Delta t$ with fixed $\\Delta x$) shows $g_{CN} \\to -1$ (magnitude $1$, sign $-1$) and $g_{BTCS} \\to 0$. This confirms the limits. The interpretation in terms of stiff decay and the definitions of L-stability (BTCS is L-stable, CN is not) are correct.\n**Verdict: Correct.**\n\n**E. At fixed $T$, the accumulated global error of Crank–Nicolson must be smaller than that of BTCS for all choices of $\\Delta t$ and $\\Delta x$ because Crank–Nicolson is second-order accurate in time.**\nWhile CN has a higher order of accuracy asymptotically (for small $\\Delta t$), the statement claims its error is smaller \"for all choices of $\\Delta t$ and $\\Delta x$\". This is false. When $\\Delta t$ is large relative to $\\Delta x^2$, the CN solution is polluted by non-physical oscillations of $\\mathcal{O}(1)$ magnitude, as shown in our stability analysis. These oscillations represent a large error. The BTCS solution, while overly damped, remains smooth and is often qualitatively more accurate in this regime. Therefore, the global error of CN is not always smaller than that of BTCS.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "The theoretical properties of a numerical scheme have direct, measurable consequences in practical applications. This hands-on exercise places the BTCS method in the context of semi-implicit solvers, a common strategy for complex models like the Navier-Stokes equations . You will write a program to numerically compute the amplification factor for the highest-frequency spatial mode, allowing you to directly observe the strong damping properties (L-stability) of BTCS and compare them to the less dissipative behavior of the Crank-Nicolson method.",
            "id": "3365299",
            "problem": "Consider the viscous substep that arises in semi-implicit treatments of the incompressible Navier–Stokes equations, where the convective term is handled explicitly and the viscous term is handled implicitly. In the absence of the convective contribution, the substep reduces to the linear diffusion equation for a single velocity component, which is the one-dimensional heat equation\n$$\n\\partial_t u(x,t) = \\nu \\,\\partial_{xx} u(x,t),\n$$\nposed on a periodic interval of length $1$, with kinematic viscosity $\\nu > 0$. You must analyze and implement three time-integration strategies for the implicit diffusion substep on a uniform periodic grid.\n\nFundamental base and discretization framework:\n- Use a uniform grid with $N$ points and grid spacing $h = 1/N$. Let the unknown at grid point $j$ and time level $n$ be denoted by $u_j^n$, where $j \\in \\{0,1,\\dots,N-1\\}$.\n- Approximate the Laplacian using the standard second-order central difference with periodic wrap:\n$$\n(\\Delta_h u)_j = \\frac{u_{j+1} - 2 u_j + u_{j-1}}{h^2},\n$$\nwhere indices are taken modulo $N$.\n- Introduce the dimensionless step size parameter\n$$\nr = \\frac{\\nu\\,\\Delta t}{h^2},\n$$\nwhere $\\Delta t$ is the time step.\n\nTime discretizations to implement:\n- Backward-Time Central-Space (BTCS): the backward Euler time discretization with the above spatial discretization. This is the standard unconditionally stable implicit scheme for diffusion obtained by replacing $\\partial_t u$ by a backward difference and $\\partial_{xx} u$ by the central difference at the new time level.\n- Crank–Nicolson (CN): the trapezoidal (time-centered) method for the diffusion substep, obtained by the average of the spatial operator at time levels $n$ and $n+1$.\n- Crank–Nicolson with Rannacher startup: replace the first full $\\Delta t$ step by two backward Euler half-steps of size $\\Delta t/2$ (with the same spatial operator) to provide additional damping of high-frequency components. For the purpose of this problem, you will only consider the propagation over one total time step $\\Delta t$ implemented as two backward Euler solves of size $\\Delta t/2$; no subsequent steps are taken.\n\nAssessment task:\n- Consider the highest-resolvable spatial Fourier mode on the periodic grid, given by the grid function $v_j = (-1)^j$. This is an eigenvector of the discrete Laplacian on the uniform periodic grid. For each of the three schemes listed above, define the one-step amplification magnitude as follows: starting from $u^0 = v$, compute $u^1$ after one total time step $\\Delta t$ according to the scheme, and report the ratio\n$$\nG = \\frac{\\|u^1\\|_2}{\\|u^0\\|_2},\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm. This quantity measures viscous damping for that highest-frequency mode. Because the grid operator is circulant, $v$ is an eigenvector and $G$ coincides with the magnitude of the corresponding scalar amplification factor.\n- Your program must construct the periodic discrete Laplacian matrix corresponding to the central difference, form the linear systems implied by each time discretization, and compute $G$ numerically via linear algebra for the specified parameter sets below. Do not use any pre-derived closed-form amplification formulas in your implementation; the result should follow from assembling and applying the linear systems.\n\nTest suite:\n- Use viscosity $\\nu = 0.01$ and grid size $N = 64$ (so $h = 1/64$).\n- Evaluate the one-step amplification magnitude $G$ for each of the following three time steps $\\Delta t$:\n  - Small step: $\\Delta t = 0.0005$.\n  - Moderate step: $\\Delta t = 0.005$.\n  - Large step: $\\Delta t = 0.05$.\n- For each $\\Delta t$, compute and report the triple of amplification magnitudes in the order: BTCS, CN, CN with Rannacher startup (two backward Euler half-steps).\n\nFinal output specification:\n- Your program should produce a single line of output containing all nine results as a comma-separated list enclosed in square brackets, ordered by increasing $\\Delta t$ and, for each $\\Delta t$, by the scheme order specified above. Explicitly, the output format must be\n$$\n[\\text{G\\_BTCS}(\\Delta t_1),\\text{G\\_CN}(\\Delta t_1),\\text{G\\_Rannacher}(\\Delta t_1),\\text{G\\_BTCS}(\\Delta t_2),\\text{G\\_CN}(\\Delta t_2),\\text{G\\_Rannacher}(\\Delta t_2),\\text{G\\_BTCS}(\\Delta t_3),\\text{G\\_CN}(\\Delta t_3),\\text{G\\_Rannacher}(\\Delta t_3)],\n$$\nwhere $\\Delta t_1 = 0.0005$, $\\Delta t_2 = 0.005$, and $\\Delta t_3 = 0.05$. Each number must be rounded to $6$ decimal places. No other text should be printed.",
            "solution": "The problem requires an analysis and numerical implementation of three implicit time-integration schemes for the one-dimensional diffusion equation, $\\partial_t u = \\nu \\partial_{xx} u$, on a periodic domain. The objective is to compute the one-step amplification magnitude for the highest-frequency Fourier mode on a discrete grid.\n\nThe governing partial differential equation (PDE) is the heat equation:\n$$\n\\partial_t u(x,t) = \\nu \\,\\partial_{xx} u(x,t)\n$$\nwhere $u(x,t)$ is the quantity of interest (e.g., a velocity component), $x \\in [0,1)$ is the spatial coordinate with periodic boundary conditions, $t$ is time, and $\\nu > 0$ is the constant kinematic viscosity.\n\nWe first discretize the spatial domain. A uniform grid with $N$ points is used, with grid spacing $h = 1/N$. The grid points are $x_j = j h$ for $j \\in \\{0, 1, \\dots, N-1\\}$. Let $U(t)$ be the column vector of grid values at time $t$, $U(t) = [u(x_0,t), u(x_1,t), \\dots, u(x_{N-1},t)]^T$. The second spatial derivative $\\partial_{xx} u$ is approximated using the standard second-order central difference operator with periodic boundary conditions:\n$$\n(\\Delta_h u)_j = \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2}\n$$\nwhere indices are taken modulo $N$. This spatial discretization transforms the PDE into a system of ordinary differential equations (ODEs):\n$$\n\\frac{d U}{dt} = \\nu A U(t)\n$$\nHere, $A$ is the $N \\times N$ matrix representation of the discrete Laplacian operator $\\Delta_h$. It is a real, symmetric, circulant matrix given by:\n$$\nA = \\frac{1}{h^2}\n\\begin{pmatrix}\n-2 & 1 & 0 & \\dots & 0 & 1 \\\\\n1 & -2 & 1 & \\ddots & & 0 \\\\\n0 & 1 & -2 & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 1 & 0 \\\\\n0 & & \\ddots & 1 & -2 & 1 \\\\\n1 & 0 & \\dots & 0 & 1 & -2\n\\end{pmatrix}\n$$\nLet $U^n$ denote the numerical approximation of $U(t)$ at time $t_n = n \\Delta t$. We now introduce the three time-stepping schemes.\n\n1.  **Backward-Time Central-Space (BTCS)**: This scheme uses a first-order backward Euler method for the time derivative, evaluating the spatial term at the future time level $n+1$.\n    $$\n    \\frac{U^{n+1} - U^n}{\\Delta t} = \\nu A U^{n+1}\n    $$\n    Rearranging terms to solve for $U^{n+1}$ yields the linear system:\n    $$\n    (I - \\nu \\Delta t A) U^{n+1} = U^n\n    $$\n    where $I$ is the $N \\times N$ identity matrix.\n\n2.  **Crank–Nicolson (CN)**: This scheme is based on the second-order trapezoidal rule for time integration, averaging the spatial term at time levels $n$ and $n+1$.\n    $$\n    \\frac{U^{n+1} - U^n}{\\Delta t} = \\frac{\\nu A}{2} (U^n + U^{n+1})\n    $$\n    Grouping terms involving $U^{n+1}$ on the left-hand side and terms involving $U^n$ on the right-hand side, we obtain the linear system:\n    $$\n    \\left(I - \\frac{\\nu \\Delta t}{2} A\\right) U^{n+1} = \\left(I + \\frac{\\nu \\Delta t}{2} A\\right) U^n\n    $$\n\n3.  **Crank–Nicolson with Rannacher Startup**: This procedure damps high-frequency oscillations by taking two backward Euler steps, each with a half time step $\\Delta t/2$. For this problem, this two-step procedure constitutes the entire evolution over one full time step $\\Delta t$.\n    - Step 1 (from $t_n$ to $t_{n+1/2} = t_n + \\Delta t/2$):\n      $$\n      \\left(I - \\nu \\frac{\\Delta t}{2} A\\right) U^{n+1/2} = U^n\n      $$\n    - Step 2 (from $t_{n+1/2}$ to $t_{n+1} = t_n + \\Delta t$):\n      $$\n      \\left(I - \\nu \\frac{\\Delta t}{2} A\\right) U^{n+1} = U^{n+1/2}\n      $$\n    Solving the first equation for $U^{n+1/2}$ and substituting into the second gives the overall update:\n    $$\n    U^{n+1} = \\left(I - \\nu \\frac{\\Delta t}{2} A\\right)^{-1} \\left[ \\left(I - \\nu \\frac{\\Delta t}{2} A\\right)^{-1} U^n \\right] = \\left(I - \\nu \\frac{\\Delta t}{2} A\\right)^{-2} U^n\n    $$\n    Numerically, this is implemented by solving two sequential linear systems with the same system matrix.\n\nThe assessment task is to compute the one-step amplification magnitude, $G = \\|U^1\\|_2 / \\|U^0\\|_2$, for the specific initial condition $U^0$ corresponding to the grid function $v_j = (-1)^j$. This function represents the highest-resolvable spatial frequency on the periodic grid. Since $v_j$ is an eigenvector of the circulant matrix $A$, the resulting vector $U^1$ will be a scalar multiple of $U^0$, and $G$ will be the magnitude of this scalar multiplier (the amplification factor). The numerical procedure is as follows:\n- Construct the matrix $A$ for the given $N=64$ and $h=1/64$.\n- Construct the initial vector $U^0$ with elements $(U^0)_j = (-1)^j$.\n- For each scheme and each given $\\Delta t$:\n    - Assemble the corresponding matrix or matrices for the linear system(s).\n    - Solve for the solution vector $U^1$.\n    - Compute the Euclidean norms $\\|U^1\\|_2$ and $\\|U^0\\|_2$.\n    - Calculate the ratio $G = \\|U^1\\|_2 / \\|U^0\\|_2$.\nThe results are then reported for the specified set of parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes three time-integration schemes for the 1D heat equation by\n    computing the amplification magnitude for the highest-frequency Fourier mode.\n    \"\"\"\n    # Define parameters from the problem statement\n    nu = 0.01\n    N = 64\n    h = 1.0 / N\n\n    # Define the test cases for the time step delta_t\n    test_cases = [0.0005, 0.005, 0.05]\n\n    # Construct the initial condition vector u0 for the highest-frequency mode\n    # u_j = (-1)^j\n    j_indices = np.arange(N)\n    u0 = (-1.0)**j_indices\n    norm_u0 = np.linalg.norm(u0)\n\n    # Construct the discrete Laplacian matrix A\n    # A = (1/h^2) * L, where L is circulant with (-2, 1, ..., 1) in the first row.\n    diag_val = -2.0\n    off_diag_val = 1.0\n    \n    A_unscaled = np.diag(diag_val * np.ones(N)) + \\\n                 np.diag(off_diag_val * np.ones(N - 1), k=1) + \\\n                 np.diag(off_diag_val * np.ones(N - 1), k=-1)\n    \n    # Apply periodic boundary conditions for the corners\n    A_unscaled[0, N - 1] = off_diag_val\n    A_unscaled[N - 1, 0] = off_diag_val\n\n    A = A_unscaled / (h**2)\n\n    all_results = []\n\n    # Iterate through each test case (each delta_t)\n    for delta_t in test_cases:\n        # 1. Backward-Time Central-Space (BTCS)\n        # (I - nu * dt * A) * u1 = u0\n        mat_btcs = np.identity(N) - nu * delta_t * A\n        u1_btcs = np.linalg.solve(mat_btcs, u0)\n        g_btcs = np.linalg.norm(u1_btcs) / norm_u0\n        all_results.append(g_btcs)\n\n        # 2. Crank-Nicolson (CN)\n        # (I - nu*dt/2 * A) * u1 = (I + nu*dt/2 * A) * u0\n        mat_cn_lhs = np.identity(N) - (nu * delta_t / 2.0) * A\n        mat_cn_rhs = np.identity(N) + (nu * delta_t / 2.0) * A\n        rhs_cn = mat_cn_rhs @ u0\n        u1_cn = np.linalg.solve(mat_cn_lhs, rhs_cn)\n        g_cn = np.linalg.norm(u1_cn) / norm_u0\n        all_results.append(g_cn)\n        \n        # 3. Rannacher Startup (two BTCS half-steps)\n        # (I - nu*dt/2 * A) * u_half = u0\n        # (I - nu*dt/2 * A) * u1 = u_half\n        # The matrix is the same as the CN left-hand side matrix.\n        mat_rannacher = mat_cn_lhs\n        u_half = np.linalg.solve(mat_rannacher, u0)\n        u1_rannacher = np.linalg.solve(mat_rannacher, u_half)\n        g_rannacher = np.linalg.norm(u1_rannacher) / norm_u0\n        all_results.append(g_rannacher)\n\n    # Format the final output string to 6 decimal places per number\n    # The format '{:.6f}'.format is used to ensure trailing zeros are printed.\n    print(f\"[{','.join(map('{:.6f}'.format, all_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}