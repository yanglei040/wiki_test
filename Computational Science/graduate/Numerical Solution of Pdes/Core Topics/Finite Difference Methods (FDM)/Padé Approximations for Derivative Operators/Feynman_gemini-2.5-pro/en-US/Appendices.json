{
    "hands_on_practices": [
        {
            "introduction": "The first step towards mastering Padé approximations is understanding their construction. This exercise guides you through the foundational process of deriving the coefficients for a high-order compact derivative operator from first principles. By systematically matching terms in a Taylor series expansion, you will see how a compact stencil can achieve an impressive sixth-order accuracy, a feat that would require a much wider stencil in an explicit scheme. ",
            "id": "3428918",
            "problem": "Consider a smooth function $f(x)$ sampled on a uniform grid $x_{i} = x_{0} + i h$ with grid spacing $h > 0$ and periodic boundary conditions, as is standard in the numerical solution of partial differential equations (PDE). Define a compact Padé-type tridiagonal approximation to the first derivative $f'(x)$ by the implicit stencil\n$$\n\\alpha\\, D_{i-1} + D_{i} + \\alpha\\, D_{i+1} \\;=\\; \\frac{1}{h}\\left[ a\\left(f_{i+1} - f_{i-1}\\right) + b\\left(f_{i+{2}} - f_{i-{2}}\\right) \\right],\n$$\nwhere $D_{i}$ is the discrete approximation to $f'(x_{i})$, and $\\alpha$, $a$, and $b$ are grid-independent coefficients to be determined. The goal is to choose $\\alpha$, $a$, and $b$ so that the approximation is sixth-order accurate, meaning the truncation error is $\\mathcal{O}(h^{6})$.\n\nStarting only from Taylor series expansions of $f(x)$ about $x_{i}$ and the definition of the discrete Fourier symbol for periodic grids, derive the conditions that enforce sixth-order accuracy and solve for the unique coefficients $\\alpha$, $a$, and $b$ consistent with a tridiagonal left-hand side and a right-hand side using up to second neighbors only. Then, using the discrete Fourier mode $f_{j} = \\exp(\\mathrm{i} k x_{j})$ with nondimensional wavenumber $\\xi = k h$, form the modified wavenumber symbol\n$$\n\\kappa(\\xi) \\quad \\text{such that} \\quad D_{j} \\;=\\; \\frac{\\mathrm{i}\\, \\kappa(\\xi)}{h}\\, f_{j},\n$$\nand verify the sixth-order accuracy by expanding $\\kappa(\\xi)$ in a Taylor series about $\\xi = 0$.\n\nExpress the final coefficients $\\alpha$, $a$, $b$, and $c$ in exact rational form, where $c$ is the coefficient multiplying $\\left(f_{i+3} - f_{i-3}\\right)$ which must be set to zero in this construction. Provide your final answer as a single row matrix. No rounding is required and no physical units apply.",
            "solution": "The problem is first validated and found to be a well-posed and scientifically sound problem in the field of numerical analysis. It asks for the derivation of coefficients for a sixth-order compact finite difference scheme for the first derivative.\n\nThe given stencil is:\n$$\n\\alpha\\, D_{i-1} + D_{i} + \\alpha\\, D_{i+1} \\;=\\; \\frac{1}{h}\\left[ a\\left(f_{i+1} - f_{i-1}\\right) + b\\left(f_{i+{2}} - f_{i-{2}}\\right) \\right]\n$$\nTo determine the order of accuracy, we analyze the truncation error, which is obtained by substituting the exact derivative $f'(x_i)$ for its approximation $D_i$ and the exact function values $f(x_i)$ for $f_i$. Let $f_i^{(n)}$ denote the $n$-th derivative of $f(x)$ evaluated at $x_i$. The truncation error $T_i$ is defined by:\n$$\nT_i = \\left( \\alpha\\, f'_{i-1} + f'_{i} + \\alpha\\, f'_{i+1} \\right) - \\frac{1}{h}\\left[ a\\left(f_{i+1} - f_{i-1}\\right) + b\\left(f_{i+{2}} - f_{i-{2}}\\right) \\right]\n$$\nWe expand each term in a Taylor series about the point $x_i$. The expansions for the function values are:\n$$\nf_{i\\pm m} = f(x_i \\pm mh) = \\sum_{n=0}^{\\infty} \\frac{(\\pm mh)^n}{n!} f_i^{(n)}\n$$\nThis gives the centered differences:\n$$\nf_{i+1} - f_{i-1} = 2hf_i' + \\frac{2h^3}{3!}f_i''' + \\frac{2h^5}{5!}f_i^{(5)} + \\frac{2h^7}{7!}f_i^{(7)} + \\mathcal{O}(h^9)\n$$\n$$\nf_{i+2} - f_{i-2} = 4hf_i' + \\frac{16h^3}{3!}f_i''' + \\frac{64h^5}{5!}f_i^{(5)} + \\frac{256h^7}{7!}f_i^{(7)} + \\mathcal{O}(h^9)\n$$\nThe right-hand side (RHS) of the stencil operator, when applied to the exact function, becomes:\n$$\n\\text{RHS} = \\frac{a}{h}\\left(2hf_i' + \\frac{h^3}{3}f_i''' + \\frac{h^5}{60}f_i^{(5)} + \\frac{h^7}{2520}f_i^{(7)}\\right) + \\frac{b}{h}\\left(4hf_i' + \\frac{8h^3}{3}f_i''' + \\frac{32h^5}{60}f_i^{(5)} + \\frac{128h^7}{2520}f_i^{(7)}\\right) + \\dots\n$$\n$$\n\\text{RHS} = (2a+4b)f_i' + \\left(\\frac{a}{3}+\\frac{8b}{3}\\right)h^2f_i''' + \\left(\\frac{a}{60}+\\frac{32b}{60}\\right)h^4f_i^{(5)} + \\left(\\frac{a}{2520}+\\frac{128b}{2520}\\right)h^6f_i^{(7)} + \\mathcal{O}(h^8)\n$$\nThe expansions for the derivative terms on the left-hand side (LHS) are:\n$$\nf'_{i\\pm 1} = f'(x_i \\pm h) = f'_i \\pm hf''_i + \\frac{h^2}{2!}f'''_i \\pm \\frac{h^3}{3!}f^{(4)}_i + \\frac{h^4}{4!}f^{(5)}_i \\pm \\frac{h^5}{5!}f^{(6)}_i + \\frac{h^6}{6!}f^{(7)}_i + \\mathcal{O}(h^7)\n$$\nThe LHS becomes:\n$$\n\\text{LHS} = \\alpha(f'_i - hf''_i + \\frac{h^2}{2}f'''_i - \\dots) + f'_i + \\alpha(f'_i + hf''_i + \\frac{h^2}{2}f'''_i + \\dots)\n$$\nAs the scheme is centered, all even-order derivative terms cancel.\n$$\n\\text{LHS} = (1+2\\alpha)f'_i + \\alpha h^2 f'''_i + \\frac{\\alpha h^4}{12}f_i^{(5)} + \\frac{\\alpha h^6}{360}f_i^{(7)} + \\mathcal{O}(h^8)\n$$\nFor the truncation error $T_i = \\text{LHS} - \\text{RHS}$ to be $\\mathcal{O}(h^6)$, the coefficients of $f'_i$, $h^2 f'''_i$, and $h^4 f^{(5)}_i$ must match between the LHS and RHS. This yields a system of three linear equations for $\\alpha$, $a$, and $b$:\n1. $f'_i$:  $1+2\\alpha = 2a+4b$\n2. $h^2 f'''_i$: $\\alpha = \\frac{a}{3} + \\frac{8b}{3} \\implies 3\\alpha = a+8b$\n3. $h^4 f^{(5)}_i$: $\\frac{\\alpha}{12} = \\frac{a}{60} + \\frac{32b}{60} \\implies 5\\alpha = a+32b$\n\nWe solve this system. Subtracting equation (2) from equation (3):\n$$\n(a+32b) - (a+8b) = 5\\alpha - 3\\alpha \\implies 24b = 2\\alpha \\implies \\alpha = 12b\n$$\nSubstitute $\\alpha=12b$ into equation (2):\n$$\na+8b = 3(12b) = 36b \\implies a = 28b\n$$\nFinally, substitute $\\alpha=12b$ and $a=28b$ into equation (1):\n$$\n1 + 2(12b) = 2(28b) + 4b\n$$\n$$\n1 + 24b = 56b + 4b\n$$\n$$\n1 = 36b \\implies b = \\frac{1}{36}\n$$\nFrom this, we find the other coefficients:\n$$\na = 28b = \\frac{28}{36} = \\frac{7}{9}\n$$\n$$\n\\alpha = 12b = \\frac{12}{36} = \\frac{1}{3}\n$$\nThe problem specifies a stencil using up to second neighbors on the RHS, meaning the coefficient $c$ for the term $(f_{i+3} - f_{i-3})$ is zero. Thus, $c=0$.\nThe coefficients are: $\\alpha = 1/3$, $a = 7/9$, $b = 1/36$, and $c=0$.\n\nTo verify the order of accuracy, we analyze the modified wavenumber $\\kappa(\\xi)$. We substitute the Fourier mode $f_j = \\exp(\\mathrm{i} k x_j)$ and the definition $D_j = \\frac{\\mathrm{i}}{h}\\kappa(\\xi)f_j$ into the stencil, with $\\xi=kh$ and $f_{j \\pm m} = f_j \\exp(\\pm \\mathrm{i} m \\xi)$.\n$$\n\\alpha \\frac{\\mathrm{i}\\kappa}{h}f_{i-1} + \\frac{\\mathrm{i}\\kappa}{h}f_i + \\alpha \\frac{\\mathrm{i}\\kappa}{h}f_{i+1} = \\frac{1}{h}\\left[ a(f_{i+1}-f_{i-1}) + b(f_{i+2}-f_{i-2}) \\right]\n$$\nDividing by $f_i/h$ and substituting the exponential forms gives:\n$$\n\\mathrm{i}\\kappa \\left( \\alpha \\exp(-\\mathrm{i}\\xi) + 1 + \\alpha \\exp(\\mathrm{i}\\xi) \\right) = a(\\exp(\\mathrm{i}\\xi) - \\exp(-\\mathrm{i}\\xi)) + b(\\exp(2\\mathrm{i}\\xi) - \\exp(-2\\mathrm{i}\\xi))\n$$\nUsing Euler's identities, $2\\cos(x) = \\exp(\\mathrm{i}x)+\\exp(-\\mathrm{i}x)$ and $2\\mathrm{i}\\sin(x) = \\exp(\\mathrm{i}x)-\\exp(-\\mathrm{i}x)$:\n$$\n\\mathrm{i}\\kappa(1+2\\alpha\\cos\\xi) = a(2\\mathrm{i}\\sin\\xi) + b(2\\mathrm{i}\\sin(2\\xi))\n$$\nSolving for the modified wavenumber $\\kappa(\\xi)$:\n$$\n\\kappa(\\xi) = \\frac{2a\\sin\\xi + 2b\\sin(2\\xi)}{1+2\\alpha\\cos\\xi}\n$$\nThe exact differentiation corresponds to $\\kappa_{exact}(\\xi) = \\xi$. The numerator $N(\\xi)$ and denominator $D(\\xi)$ expand as:\n$$\nN(\\xi) = (2a+4b)\\xi - \\frac{1}{3}(a+8b)\\xi^3 + \\frac{1}{60}(a+32b)\\xi^5 - \\dots\n$$\n$$\nD(\\xi) = (1+2\\alpha) - \\alpha\\xi^2 + \\frac{\\alpha}{12}\\xi^4 - \\dots\n$$\nFor the scheme to be sixth-order accurate, we require $\\kappa(\\xi) = N(\\xi)/D(\\xi) = \\xi + \\mathcal{O}(\\xi^7)$. This is equivalent to ensuring $N(\\xi) = D(\\xi)\\xi + \\mathcal{O}(\\xi^7)$. Comparing the series for $N(\\xi)$ and $D(\\xi)\\xi$ term by term shows that our system of equations for $\\alpha, a, b$ is precisely what's needed to match the coefficients of $\\xi^1, \\xi^3, \\xi^5$, thus eliminating the error terms up to $\\mathcal{O}(\\xi^7)$.\nThe deviation from $\\kappa(\\xi)=\\xi$ first appears at order $\\xi^7$:\n$$\n\\kappa(\\xi)-\\xi \\approx C_7\\xi^7\n$$\nwhere $C_7$ is the leading error coefficient. The error in physical space is $\\mathcalO(h^6)$, confirming the sixth-order accuracy.\nSo the coefficients $\\alpha=1/3$, $a=7/9$, $b=1/36$, and implicitly $c=0$ are correct.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3} & \\frac{7}{9} & \\frac{1}{36} & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the formal order of accuracy provides a valuable measure, it doesn't fully capture a scheme's performance for wave-like phenomena. This practice delves into the crucial concepts of dispersion and dissipation error, which quantify how well a numerical scheme propagates waves of different frequencies. You will derive the scheme's symbol in Fourier space and analyze these errors, gaining a deeper, more practical understanding of what makes a scheme 'spectrally accurate'. ",
            "id": "3428900",
            "problem": "Consider a uniform, infinite, periodic grid with spacing $h > 0$ and integer indices $j \\in \\mathbb{Z}$. Let $f_j$ denote grid samples of a sufficiently smooth function $f(x)$ at $x = j h$. A translation-invariant linear approximation to the first derivative is said to be rational (a Padé approximation) if it satisfies an implicit difference relation of the form\n$$\n\\sum_{\\ell=-L}^{L} a_{\\ell} f^\\prime_{j+\\ell} \\;=\\; \\frac{1}{h}\\sum_{m=-M}^{M} b_m f_{j+m},\n$$\nwhere $a_{\\ell}$ and $b_m$ are real coefficients and $L, M$ are finite integers. For periodic problems, Fourier analysis using the fundamental ansatz $f_j = e^{\\mathrm{i} j \\theta}$ with the nondimensional wavenumber $\\,\\theta = k h\\,$ (angle unit: radians) characterizes the discrete derivative operator by its symbol $\\widehat{D}(\\theta)$, defined by $f^\\prime_j \\approx \\widehat{D}(\\theta) f_j$. The exact continuous derivative corresponds to the symbol $\\mathrm{i} k = (\\mathrm{i}/h)\\theta$.\n\nDispersion error and dissipation error are defined in terms of the modified wavenumber produced by the discrete operator:\n- The dispersion error is the deviation of the imaginary part from the exact nondimensional wavenumber and is given by\n$$\nE_{\\mathrm{disp}}(\\theta) \\;=\\; \\operatorname{Im}\\!\\big(\\widehat{D}(\\theta)\\,h\\big) \\;-\\; \\theta.\n$$\n- The dissipation error is the real part of the nondimensional symbol and is given by\n$$\nE_{\\mathrm{diss}}(\\theta) \\;=\\; \\operatorname{Re}\\!\\big(\\widehat{D}(\\theta)\\,h\\big).\n$$\n\nIn this problem, analyze the following candidate compact (Padé) scheme for the first derivative:\n$$\n\\frac{1}{4} f^\\prime_{j-1} + f^\\prime_j + \\frac{1}{4} f^\\prime_{j+1} \\;=\\; \\frac{3}{4h}\\,\\big(f_{j+1} - f_{j-1}\\big).\n$$\nStarting from the Fourier ansatz and the above implicit relation, derive the discrete symbol $\\widehat{D}(\\theta)$ and then compute the dispersion and dissipation errors over a uniform grid of $\\theta$ values.\n\nUse the following test suite and specifications:\n- Use a uniform grid of $M = 1001$ points covering $\\theta \\in [0,\\pi]$ inclusive, in radians.\n- Compute the following quantities:\n  1. The boolean value indicating whether $\\max_{\\theta \\in [0,\\pi]} |E_{\\mathrm{diss}}(\\theta)| < \\varepsilon$, with tolerance $\\varepsilon = 10^{-12}$.\n  2. The maximum absolute dispersion error over the grid, $\\max_{\\theta \\in [0,\\pi]} |E_{\\mathrm{disp}}(\\theta)|$, as a floating-point number.\n  3. The value of $\\theta$ (in radians) at which the maximum absolute dispersion error occurs, as a floating-point number.\n  4. The dispersion error at $\\theta = \\pi/4$, as a floating-point number.\n  5. The $L^\\infty$ norm of the dissipation error over the grid, $\\max_{\\theta \\in [0,\\pi]} |E_{\\mathrm{diss}}(\\theta)|$, as a floating-point number.\n\nYour program should produce a single line of output containing these five results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). No physical units are required because all quantities are nondimensionalized using $\\theta = k h$ and the angle unit is explicitly radians.",
            "solution": "The problem is assessed to be valid as it represents a well-posed, scientifically grounded exercise in the numerical analysis of partial differential equations. All provided definitions, conditions, and parameters are consistent and sufficient for deriving a unique, meaningful solution.\n\nThe core of the problem is to determine the spectral properties of a given compact finite difference scheme by deriving its symbol, $\\widehat{D}(\\theta)$, and then analyzing the associated dispersion and dissipation errors.\n\nThe specified implicit difference scheme is:\n$$\n\\frac{1}{4} f^\\prime_{j-1} + f^\\prime_j + \\frac{1}{4} f^\\prime_{j+1} \\;=\\; \\frac{3}{4h}\\,\\big(f_{j+1} - f_{j-1}\\big)\n$$\n\nThis must conform to the general rational approximation form:\n$$\n\\sum_{\\ell=-L}^{L} a_{\\ell} f^\\prime_{j+\\ell} \\;=\\; \\frac{1}{h}\\sum_{m=-M}^{M} b_m f_{j+m}\n$$\n\nBy comparing the two forms, we identify the coefficients $a_{\\ell}$ and $b_m$. The left-hand side (LHS) involves derivative terms, giving the $a_{\\ell}$ coefficients with $L=1$:\n$a_{-1} = 1/4$, $a_0 = 1$, and $a_1 = 1/4$.\n\nThe right-hand side (RHS) involves function values, giving the $b_m$ coefficients with $M=1$:\n$b_{-1} = -3/4$, $b_0 = 0$, and $b_1 = 3/4$.\n\nTo find the symbol $\\widehat{D}(\\theta)$, we use the Fourier ansatz $f_j = e^{\\mathrm{i} j \\theta}$ and the definition of the discrete operator, $f'_j \\approx \\widehat{D}(\\theta) f_j$. Applying these to any grid point $j+k$ gives:\n$$\nf_{j+k} = e^{\\mathrm{i} (j+k) \\theta} = f_j e^{\\mathrm{i} k \\theta}\n$$\n$$\nf'_{j+k} \\approx \\widehat{D}(\\theta) f_{j+k} = \\widehat{D}(\\theta) f_j e^{\\mathrm{i} k \\theta}\n$$\n\nSubstituting these expressions into the scheme's general form yields:\n$$\n\\sum_{\\ell=-L}^{L} a_{\\ell} \\left( \\widehat{D}(\\theta) f_j e^{\\mathrm{i} \\ell \\theta} \\right) \\;=\\; \\frac{1}{h}\\sum_{m=-M}^{M} b_m \\left( f_j e^{\\mathrm{i} m \\theta} \\right)\n$$\n\nSince $f_j = e^{\\mathrm{i} j \\theta}$ is not identically zero, we can divide it out from both sides:\n$$\n\\widehat{D}(\\theta) \\sum_{\\ell=-L}^{L} a_{\\ell} e^{\\mathrm{i} \\ell \\theta} \\;=\\; \\frac{1}{h}\\sum_{m=-M}^{M} b_m e^{\\mathrm{i} m \\theta}\n$$\n\nThis allows us to solve for the symbol $\\widehat{D}(\\theta)$:\n$$\n\\widehat{D}(\\theta) = \\frac{1}{h} \\frac{\\sum_{m=-M}^{M} b_m e^{\\mathrm{i} m \\theta}}{\\sum_{\\ell=-L}^{L} a_{\\ell} e^{\\mathrm{i} \\ell \\theta}}\n$$\n\nLet's evaluate the numerator and denominator sums with the identified coefficients.\nThe denominator sum, which we can call $\\mathcal{A}(\\theta)$, is:\n$$\n\\mathcal{A}(\\theta) = \\sum_{\\ell=-1}^{1} a_{\\ell} e^{\\mathrm{i} \\ell \\theta} = a_{-1}e^{-\\mathrm{i}\\theta} + a_0 + a_1e^{\\mathrm{i}\\theta}\n$$\n$$\n\\mathcal{A}(\\theta) = \\frac{1}{4}e^{-\\mathrm{i}\\theta} + 1 + \\frac{1}{4}e^{\\mathrm{i}\\theta} = 1 + \\frac{1}{4}(e^{\\mathrm{i}\\theta} + e^{-\\mathrm{i}\\theta})\n$$\nUsing Euler's identity $e^{\\mathrm{i}x} + e^{-\\mathrm{i}x} = 2\\cos(x)$, we get:\n$$\n\\mathcal{A}(\\theta) = 1 + \\frac{1}{2}\\cos(\\theta)\n$$\n\nThe numerator sum, which we can call $\\mathcal{B}(\\theta)$, is:\n$$\n\\mathcal{B}(\\theta) = \\sum_{m=-1}^{1} b_m e^{\\mathrm{i} m \\theta} = b_{-1}e^{-\\mathrm{i}\\theta} + b_0 + b_1e^{\\mathrm{i}\\theta}\n$$\n$$\n\\mathcal{B}(\\theta) = -\\frac{3}{4}e^{-\\mathrm{i}\\theta} + 0 + \\frac{3}{4}e^{\\mathrm{i}\\theta} = \\frac{3}{4}(e^{\\mathrm{i}\\theta} - e^{-\\mathrm{i}\\theta})\n$$\nUsing Euler's identity $e^{\\mathrm{i}x} - e^{-\\mathrm{i}x} = 2\\mathrm{i}\\sin(x)$, we get:\n$$\n\\mathcal{B}(\\theta) = \\frac{3}{4}(2\\mathrm{i}\\sin(\\theta)) = \\frac{3}{2}\\mathrm{i}\\sin(\\theta)\n$$\n\nSubstituting $\\mathcal{A}(\\theta)$ and $\\mathcal{B}(\\theta)$ back into the expression for $\\widehat{D}(\\theta)$:\n$$\n\\widehat{D}(\\theta) = \\frac{1}{h} \\frac{\\frac{3}{2}\\mathrm{i}\\sin(\\theta)}{1 + \\frac{1}{2}\\cos(\\theta)} = \\frac{\\mathrm{i}}{h} \\frac{3\\sin(\\theta)}{2+\\cos(\\theta)}\n$$\n\nThe problem defines the dispersion and dissipation errors in terms of the nondimensional modified wavenumber, $\\widehat{D}(\\theta)h$:\n$$\n\\widehat{D}(\\theta)h = \\mathrm{i} \\frac{3\\sin(\\theta)}{2+\\cos(\\theta)}\n$$\n\nThis expression is purely imaginary for all real $\\theta$ where the denominator is non-zero (which is always, since $\\cos(\\theta) \\ge -1$).\n\nThe dissipation error is defined as $E_{\\mathrm{diss}}(\\theta) = \\operatorname{Re}(\\widehat{D}(\\theta)h)$. From our derived expression:\n$$\nE_{\\mathrm{diss}}(\\theta) = \\operatorname{Re}\\left(\\mathrm{i} \\frac{3\\sin(\\theta)}{2+\\cos(\\theta)}\\right) = 0\n$$\nThis scheme is perfectly non-dissipative.\n\nThe dispersion error is defined as $E_{\\mathrm{disp}}(\\theta) = \\operatorname{Im}(\\widehat{D}(\\theta)h) - \\theta$.\n$$\nE_{\\mathrm{disp}}(\\theta) = \\operatorname{Im}\\left(\\mathrm{i} \\frac{3\\sin(\\theta)}{2+\\cos(\\theta)}\\right) - \\theta = \\frac{3\\sin(\\theta)}{2+\\cos(\\theta)} - \\theta\n$$\n\nWith these analytical expressions for the errors, we can proceed with the numerical computation as requested.\n\n1.  **Boolean for dissipation error threshold**: Since $E_{\\mathrm{diss}}(\\theta) = 0$ for all $\\theta$, $\\max_{\\theta \\in [0,\\pi]} |E_{\\mathrm{diss}}(\\theta)| = 0$. Given the tolerance $\\varepsilon = 10^{-12}$, the condition $0 < 10^{-12}$ is true. The result is `True`.\n\n2.  **Maximum absolute dispersion error**: We need to find $\\max_{\\theta \\in [0,\\pi]} |E_{\\mathrm{disp}}(\\theta)|$. Let's analyze the derivative of $E_{\\mathrm{disp}}(\\theta)$:\n    $$\n    \\frac{d}{d\\theta}E_{\\mathrm{disp}}(\\theta) = \\frac{d}{d\\theta}\\left(\\frac{3\\sin(\\theta)}{2+\\cos(\\theta)}\\right) - 1\n    $$\n    The derivative of the fraction is $\\frac{3\\cos(\\theta)(2+\\cos(\\theta)) - 3\\sin(\\theta)(-\\sin(\\theta))}{(2+\\cos(\\theta))^2} = \\frac{6\\cos(\\theta) + 3\\cos^2(\\theta) + 3\\sin^2(\\theta)}{(2+\\cos(\\theta))^2} = \\frac{6\\cos(\\theta)+3}{(2+\\cos(\\theta))^2}$.\n    Thus, $\\frac{d}{d\\theta}E_{\\mathrm{disp}}(\\theta) = \\frac{6\\cos(\\theta)+3}{(2+\\cos(\\theta))^2} - 1 = \\frac{6\\cos(\\theta)+3 - (4+4\\cos(\\theta)+\\cos^2(\\theta))}{(2+\\cos(\\theta))^2} = \\frac{-\\cos^2(\\theta)+2\\cos(\\theta)-1}{(2+\\cos(\\theta))^2} = -\\frac{(\\cos(\\theta)-1)^2}{(2+\\cos(\\theta))^2}$.\n    This derivative is less than or equal to zero for all $\\theta \\in [0, \\pi]$, with equality only at $\\theta=0$. This implies that $E_{\\mathrm{disp}}(\\theta)$ is a monotonically decreasing function on the interval $[0, \\pi]$.\n    The error at the endpoints are:\n    $E_{\\mathrm{disp}}(0) = \\frac{3\\sin(0)}{2+\\cos(0)} - 0 = 0$.\n    $E_{\\mathrm{disp}}(\\pi) = \\frac{3\\sin(\\pi)}{2+\\cos(\\pi)} - \\pi = \\frac{0}{2-1} - \\pi = -\\pi$.\n    Since the function is monotonic, the maximum absolute value must occur at one of the endpoints. We have $|E_{\\mathrm{disp}}(0)|=0$ and $|E_{\\mathrm{disp}}(\\pi)|=\\pi$. Therefore, the maximum absolute dispersion error is $\\pi$.\n\n3.  **Value of $\\theta$ at maximum error**: Based on the above analysis, the maximum absolute error occurs at $\\theta=\\pi$.\n\n4.  **Dispersion error at $\\theta=\\pi/4$**: We evaluate the expression directly:\n    $$\n    E_{\\mathrm{disp}}(\\pi/4) = \\frac{3\\sin(\\pi/4)}{2+\\cos(\\pi/4)} - \\frac{\\pi}{4} = \\frac{3(\\sqrt{2}/2)}{2+(\\sqrt{2}/2)} - \\frac{\\pi}{4} = \\frac{3\\sqrt{2}}{4+\\sqrt{2}} - \\frac{\\pi}{4}\n    $$\n\n5.  **$L^\\infty$ norm of the dissipation error**: This is $\\max_{\\theta \\in [0,\\pi]} |E_{\\mathrm{diss}}(\\theta)|$. As established, $E_{\\mathrm{diss}}(\\theta)=0$, so this value is $0$.\n\nThese results will be computed numerically in the final program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the dispersion and dissipation errors of a compact finite difference scheme.\n    \"\"\"\n    \n    # 1. Setup the computational grid and constants.\n    M = 1001\n    epsilon = 1e-12\n    # Create a uniform grid for theta from 0 to pi.\n    theta = np.linspace(0, np.pi, M)\n\n    # 2. Define and compute the error functions.\n    # The nondimensional symbol is D_hat * h = i * (3*sin(theta)) / (2+cos(theta)).\n    # The dissipation error is the real part, which is identically zero.\n    # The dispersion error is the imaginary part minus theta.\n    \n    # E_diss(theta) = Re(D_hat * h)\n    e_diss = np.zeros_like(theta)\n    \n    # E_disp(theta) = Im(D_hat * h) - theta\n    # Handle the case theta=0 to avoid potential 0/0 (though not an issue here).\n    # The expression is well-behaved, but it's good practice.\n    numerator = 3 * np.sin(theta)\n    denominator = 2 + np.cos(theta)\n    imaginary_part = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n    e_disp = imaginary_part - theta\n\n    # 3. Compute the five required quantities.\n    \n    # Quantity 1: Boolean indicating if max dissipation error is below a tolerance.\n    # Since e_diss is identically 0, max|e_diss| is 0.\n    l_inf_diss_error = np.max(np.abs(e_diss))\n    is_low_dissipation = l_inf_diss_error  epsilon\n    \n    # Quantity 2: Maximum absolute dispersion error over the grid.\n    max_abs_disp_error = np.max(np.abs(e_disp))\n    \n    # Quantity 3: The value of theta where the max absolute dispersion error occurs.\n    # As shown in the derivation, this occurs at theta = pi.\n    # We can confirm this numerically.\n    max_error_index = np.argmax(np.abs(e_disp))\n    theta_at_max_error = theta[max_error_index]\n    \n    # Quantity 4: The dispersion error at theta = pi/4.\n    # Computed directly for higher accuracy than interpolating from the grid.\n    theta_val = np.pi / 4.0\n    disp_err_at_pi_4 = (3 * np.sin(theta_val)) / (2 + np.cos(theta_val)) - theta_val\n    \n    # Quantity 5: The L-infinity norm of the dissipation error.\n    # This is the same as calculated for Quantity 1.\n    l_inf_diss_error_recomputed = l_inf_diss_error\n\n    # 4. Collate results and print in the specified format.\n    results = [\n        is_low_dissipation,\n        max_abs_disp_error,\n        theta_at_max_error,\n        disp_err_at_pi_4,\n        l_inf_diss_error_recomputed\n    ]\n\n    # Convert all results to strings and join with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A numerically superior scheme is only practical if its computational cost is manageable. This final exercise shifts our focus from theoretical accuracy to performance in practice, a critical consideration in high-performance computing. By estimating the floating-point operations (FLOPs) and considering factors like memory access and parallelization, you will analyze the fundamental trade-offs between highly accurate compact schemes and their computationally cheaper explicit counterparts. ",
            "id": "3428912",
            "problem": "Consider two one-dimensional finite-difference discretizations for approximating the first derivative of a sufficiently smooth function $u(x)$ on a uniform grid with spacing $h0$ and interior indices $i=1,\\dots,N$:\n\n1. An explicit central difference of formal order six that exploits antisymmetry to write\n$$\nu'_i \\approx w_1\\,(u_{i+1}-u_{i-1}) + w_2\\,(u_{i+2}-u_{i-2}) + w_3\\,(u_{i+3}-u_{i-3}),\n$$\nwhere the constants $w_1,w_2,w_3$ absorb the factor $1/h$ and are precomputed.\n\n2. A tridiagonal compact Padé scheme of formal order six,\n$$\na\\,u'_{i-1} + u'_i + a\\,u'_{i+1} \\;=\\; \\alpha\\,(u_{i+1}-u_{i-1}) + \\beta\\,(u_{i+2}-u_{i-2}),\n$$\nwith constants $a,\\alpha,\\beta$ that absorb $1/h$ and are precomputed, for $i=2,\\dots,N-1$, coupled with suitable nonperiodic boundary closures that preserve a tridiagonal system. The derivative values $\\{u'_i\\}$ are obtained by assembling the right-hand side and solving the tridiagonal linear system with the Thomas algorithm.\n\nAssume the following cost model and implementation details, which are standard in performance modeling:\n\n- Each floating-point addition, subtraction, multiplication, or division counts as one floating-point operation (FLOP).\n- The explicit stencil is implemented by first forming the three antisymmetric differences $(u_{i+1}-u_{i-1})$, $(u_{i+2}-u_{i-2})$, $(u_{i+3}-u_{i-3})$, then accumulating the weighted sum.\n- The compact right-hand side is assembled by forming $(u_{i+1}-u_{i-1})$ and $(u_{i+2}-u_{i-2})$ and combining them linearly.\n- The Thomas algorithm for a length-$N$ tridiagonal system with constant coefficients performs $O(N)$ work with approximately $8N$ FLOPs in total (forward elimination and back substitution combined), so that the cost attributable to the solve is approximately $8$ FLOPs per unknown.\n- Coefficients are reused (not recomputed) and divisions by $h$ or $1/h$ are absorbed into $w_j,\\alpha,\\beta$.\n\nWithin this setting:\n\n(a) Derive a per-grid-point FLOP estimate for the explicit stencil.\n\n(b) Derive a per-grid-point FLOP estimate for the compact scheme, including both right-hand-side assembly and the tridiagonal solve.\n\n(c) Based on these counts and fundamental architectural considerations (arithmetic intensity versus memory traffic, cache reuse, and communication patterns under domain decomposition), identify the statement that most accurately characterizes the performance trade-offs between these two approaches on modern processors such as central processing units (CPU) and graphics processing units (GPU), and under distributed-memory parallelization.\n\nChoose the single best option:\n\nA. The explicit stencil costs about $8$ FLOPs per interior point, whereas the compact scheme costs about $13$ FLOPs per interior point (approximately $5$ for right-hand-side assembly plus $8$ for the tridiagonal solve). Despite the higher per-point cost, compact schemes can outperform explicit ones at a given error tolerance due to superior spectral resolution, especially in dispersion-sensitive problems; however, on memory-bandwidth-bound hardware the explicit stencil often attains higher raw throughput, and in distributed-memory settings explicit schemes scale more easily because they avoid global line solves, even though compact schemes reduce halo width.\n\nB. The compact scheme is cheaper per point than the explicit stencil because its stencil touches fewer neighbors; its tridiagonal solve via the Thomas algorithm adds negligible cost (about $2$ FLOPs per point), so compact schemes are generally both faster and more accurate regardless of hardware or parallelization strategy.\n\nC. Both schemes have the same per-point FLOP count (about $8$ per point), but compact schemes are always slower in practice because the Thomas algorithm is $O(N^2)$; therefore, explicit schemes dominate both in serial and parallel regardless of accuracy requirements.\n\nD. The explicit stencil costs about $2$ FLOPs per point because the coefficients are precomputed, while the compact scheme costs about $8$ FLOPs per point for the solve; compact schemes therefore have strictly higher arithmetic intensity and always achieve better strong scaling due to their smaller halo width, even though they require solving along grid lines.",
            "solution": "The problem statement is scientifically grounded, well-posed, objective, and self-contained. It presents a standard comparison between two well-established numerical methods for approximating derivatives, using a clearly defined cost model. The task is to perform a quantitative FLOP count analysis and a qualitative performance comparison based on fundamental principles of high-performance computing. All necessary information is provided. The problem is therefore valid.\n\nWe will proceed by first deriving the per-grid-point FLOP counts as requested in parts (a) and (b), and then using this analysis, along with architectural considerations, to evaluate the options as requested in part (c). A floating-point operation (FLOP) is defined as a single addition, subtraction, multiplication, or division.\n\n**(a) Per-grid-point FLOP estimate for the explicit stencil**\n\nThe explicit central difference scheme is given by:\n$$\nu'_i \\approx w_1\\,(u_{i+1}-u_{i-1}) + w_2\\,(u_{i+2}-u_{i-2}) + w_3\\,(u_{i+3}-u_{i-3})\n$$\nThe problem specifies the implementation follows the order of operations: first compute the differences, then the weighted sum. Let us count the FLOPs for each interior point $i$:\n\n1.  **Compute differences:**\n    -   $d_1 = u_{i+1}-u_{i-1}$: $1$ subtraction.\n    -   $d_2 = u_{i+2}-u_{i-2}$: $1$ subtraction.\n    -   $d_3 = u_{i+3}-u_{i-3}$: $1$ subtraction.\n    Total subtractions: $3$ FLOPs.\n\n2.  **Compute weighted sum:** This involves evaluating $w_1 d_1 + w_2 d_2 + w_3 d_3$.\n    -   $t_1 = w_1 \\times d_1$: $1$ multiplication.\n    -   $t_2 = w_2 \\times d_2$: $1$ multiplication.\n    -   $t_3 = w_3 \\times d_3$: $1$ multiplication.\n    -   $S = t_1 + t_2$: $1$ addition.\n    -   $u'_i \\approx S + t_3$: $1$ addition.\n    Total multiplications: $3$ FLOPs.\n    Total additions: $2$ FLOPs.\n\nThe total FLOP count per interior point for the explicit stencil is the sum of these operations: $3$ (subtractions) $+ 3$ (multiplications) $+ 2$ (additions) $= 8$ FLOPs.\n\n**(b) Per-grid-point FLOP estimate for the compact scheme**\n\nThe compact scheme involves two stages: assembling the right-hand side (RHS) and solving the resulting tridiagonal system.\n\n1.  **RHS Assembly:** The RHS for the equation at point $i$ is:\n    $$\n    R_i = \\alpha\\,(u_{i+1}-u_{i-1}) + \\beta\\,(u_{i+2}-u_{i-2})\n    $$\n    Following the specified implementation:\n    -   $d_1 = u_{i+1}-u_{i-1}$: $1$ subtraction.\n    -   $d_2 = u_{i+2}-u_{i-2}$: $1$ subtraction.\n    -   $t_1 = \\alpha \\times d_1$: $1$ multiplication.\n    -   $t_2 = \\beta \\times d_2$: $1$ multiplication.\n    -   $R_i = t_1 + t_2$: $1$ addition.\n    The total FLOP count per interior point for RHS assembly is: $2$ (subtractions) $+ 2$ (multiplications) $+ 1$ (addition) $= 5$ FLOPs.\n\n2.  **Tridiagonal Solve:** The problem states that the Thomas algorithm is used to solve the system for the $N$ unknowns $\\{u'_i\\}$. The cost is given as approximately $8N$ FLOPs in total, which translates to a per-grid-point cost of $8$ FLOPs.\n\nThe total per-grid-point FLOP estimate for the compact scheme is the sum of the RHS assembly cost and the solve cost: $5$ FLOPs $+ 8$ FLOPs $= 13$ FLOPs.\n\n**(c) Evaluation of Options**\n\nBased on our analysis—explicit scheme costs $8$ FLOPs/point, compact scheme costs $13$ FLOPs/point—and fundamental principles of numerical analysis and computer architecture, we can evaluate each statement.\n\n**Option A:** \"The explicit stencil costs about $8$ FLOPs per interior point, whereas the compact scheme costs about $13$ FLOPs per interior point (approximately $5$ for right-hand-side assembly plus $8$ for the tridiagonal solve). Despite the higher per-point cost, compact schemes can outperform explicit ones at a given error tolerance due to superior spectral resolution, especially in dispersion-sensitive problems; however, on memory-bandwidth-bound hardware the explicit stencil often attains higher raw throughput, and in distributed-memory settings explicit schemes scale more easily because they avoid global line solves, even though compact schemes reduce halo width.\"\n-   The FLOP counts ($8$, $13$, $5$, $8$) are identical to our derivation.\n-   The statement about \"superior spectral resolution\" is a well-known major advantage of compact (Padé) schemes. They represent high-wavenumber phenomena with much less error than explicit stencils of the same formal order, allowing for coarser grids to achieve the same accuracy. This can lead to overall faster time-to-solution.\n-   The claim that explicit stencils achieve higher \"raw throughput\" on memory-bandwidth-bound hardware is correct. The reason is that their computation is free of data dependencies, making it easy to vectorize and parallelize at the instruction level (e.g., using SIMD). The Thomas algorithm, with its inherent sequential recursion, is a major bottleneck for throughput-oriented architectures like GPUs.\n-   The statement that explicit schemes \"scale more easily\" in distributed memory is also correct. The explicit stencil requires only local data from neighboring processors (a halo of width $3$), which is simple point-to-point communication. The tridiagonal solve in the compact scheme creates a global data dependency along the grid line, which requires complex and less scalable parallel algorithms (e.g., parallel cyclic reduction), hindering strong scaling.\n-   The final clause that \"compact schemes reduce halo width\" is true for the RHS assembly part (stencil half-width of $2$ versus $3$ for the explicit scheme), but it correctly frames this as a minor point compared to the major challenge of the global solve.\nThis option provides a comprehensive and accurate summary of the trade-offs.\n**Verdict: Correct.**\n\n**Option B:** \"The compact scheme is cheaper per point than the explicit stencil because its stencil touches fewer neighbors; its tridiagonal solve via the Thomas algorithm adds negligible cost (about $2$ FLOPs per point), so compact schemes are generally both faster and more accurate regardless of hardware or parallelization strategy.\"\n-   The claim that the compact scheme is cheaper is false ($13$ FLOPs vs. $8$ FLOPs).\n-   The claim that the Thomas algorithm cost is \"negligible\" at about $2$ FLOPs per point is false; the problem states, and a standard analysis confirms, it is closer to $8$ FLOPs/point.\n-   The concluding generalization is incorrect because performance depends critically on hardware and parallelization strategy.\n**Verdict: Incorrect.**\n\n**Option C:** \"Both schemes have the same per-point FLOP count (about $8$ per point), but compact schemes are always slower in practice because the Thomas algorithm is $O(N^2)$; therefore, explicit schemes dominate both in serial and parallel regardless of accuracy requirements.\"\n-   The claim of equal FLOP counts is false.\n-   The statement that the Thomas algorithm is $O(N^2)$ is a fundamental error; it is a famously efficient $O(N)$ algorithm.\n-   The conclusion that explicit schemes \"dominate... regardless of accuracy requirements\" is false; as noted in A, the superior accuracy of compact schemes is their primary advantage.\n**Verdict: Incorrect.**\n\n**Option D:** \"The explicit stencil costs about $2$ FLOPs per point because the coefficients are precomputed, while the compact scheme costs about $8$ FLOPs per point for the solve; compact schemes therefore have strictly higher arithmetic intensity and always achieve better strong scaling due to their smaller halo width, even though they require solving along grid lines.\"\n-   The FLOP count of $2$ for the explicit stencil is false; it is $8$. Precomputing coefficients does not eliminate the multiplications and additions.\n-   The cost of the compact scheme is $13$ FLOPs/point, not just the $8$ from the solve.\n-   The claim that compact schemes \"always achieve better strong scaling\" is diametrically opposed to the truth. The recursive nature of the Thomas algorithm makes parallelization difficult and limits strong scaling, whereas the local nature of the explicit stencil is highly amenable to it. The smaller halo width is a minor factor completely overwhelmed by the difficulty of parallelizing the line solve.\n**Verdict: Incorrect.**\n\nIn summary, Option A is the only statement that is factually correct in its quantitative claims and presents a nuanced, accurate picture of the complex performance trade-offs involved.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}