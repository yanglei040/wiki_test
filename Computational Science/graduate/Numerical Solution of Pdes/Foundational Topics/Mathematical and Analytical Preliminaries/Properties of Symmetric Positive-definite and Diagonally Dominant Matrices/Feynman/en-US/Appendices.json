{
    "hands_on_practices": [
        {
            "introduction": "The finite difference discretization of the one-dimensional Poisson equation gives rise to a classic tridiagonal matrix that serves as a canonical example of a symmetric positive-definite (SPD) and diagonally dominant system. This exercise explores one of its most important characteristics: the condition number, which governs the sensitivity of the solution to perturbations and impacts the convergence rate of iterative solvers. By deriving the asymptotic relationship between the condition number and the grid spacing $h$, you will gain a fundamental understanding of why refining a numerical grid, while improving accuracy, inherently produces a more ill-conditioned linear system .",
            "id": "3436714",
            "problem": "Consider the boundary value problem for the one-dimensional Poisson equation on the unit interval, given by $-u''(x) = f(x)$ for $x \\in (0,1)$ with Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$. Discretize the interval into $n$ interior grid points with uniform spacing $h = \\frac{1}{n+1}$, and approximate $-u''(x)$ by the standard centered finite difference operator. This yields a linear system $A \\mathbf{u} = \\mathbf{b}$, where $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix with entries $A_{ii} = \\frac{2}{h^{2}}$ and $A_{i,i\\pm 1} = -\\frac{1}{h^{2}}$. The matrix $A$ is symmetric, diagonally dominant, and symmetric positive-definite (SPD).\n\nLet $\\kappa_{2}(A)$ denote the $2$-norm condition number of $A$, defined by $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. Starting from first principles, including the discrete operator definition and the spectral characterization of the $2$-norm for symmetric matrices, derive the asymptotic scaling of $\\kappa_{2}(A)$ as $h \\to 0$ and determine the exact constant $C$ in the leading-order relation $\\kappa_{2}(A) \\sim C h^{-2}$. Express $C$ as a closed-form analytic expression. Your final answer must be the value of $C$; no rounding is required.",
            "solution": "The problem asks for the derivation of the asymptotic scaling of the condition number $\\kappa_{2}(A)$ for the matrix $A$ arising from the finite difference discretization of the one-dimensional Poisson equation. We are tasked with finding the constant $C$ in the relation $\\kappa_{2}(A) \\sim C h^{-2}$ as the grid spacing $h \\to 0$.\n\nThe problem states that discretizing the differential operator $-u''(x)$ on a uniform grid with $n$ interior points and spacing $h = \\frac{1}{n+1}$ using a centered finite difference approximation results in an $n \\times n$ matrix $A$. The entries of this matrix are given as $A_{ii} = \\frac{2}{h^2}$ and $A_{i, i\\pm 1} = -\\frac{1}{h^2}$. We can express the matrix $A$ as a scaling of the simpler tridiagonal matrix $T$ which has entries $T_{ii}=2$ and $T_{i, i\\pm 1}=-1$. Specifically, $A = \\frac{1}{h^2} T$, where $T \\in \\mathbb{R}^{n \\times n}$ is given by:\n$$\nT = \\begin{pmatrix}\n2 & -1 & 0 & \\cdots & 0 \\\\\n-1 & 2 & -1 & \\ddots & \\vdots \\\\\n0 & -1 & \\ddots & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & 2 & -1 \\\\\n0 & \\cdots & 0 & -1 & 2\n\\end{pmatrix}\n$$\nThe problem states that $A$ is symmetric and positive-definite (SPD). This is a well-known property of this matrix. For a symmetric matrix, the $2$-norm $\\|A\\|_2$ is equal to its spectral radius $\\rho(A)$, which is the maximum of the absolute values of its eigenvalues. Since $A$ is positive-definite, all its eigenvalues are positive real numbers. Therefore, $\\|A\\|_2 = \\lambda_{\\max}(A)$, the largest eigenvalue of $A$.\n\nThe inverse matrix $A^{-1}$ is also symmetric and positive-definite. Its eigenvalues are the reciprocals of the eigenvalues of $A$. Thus, the largest eigenvalue of $A^{-1}$ is the reciprocal of the smallest eigenvalue of $A$, i.e., $\\lambda_{\\max}(A^{-1}) = \\frac{1}{\\lambda_{\\min}(A)}$. Consequently, $\\|A^{-1}\\|_2 = \\frac{1}{\\lambda_{\\min}(A)}$.\n\nThe $2$-norm condition number $\\kappa_2(A)$ is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. Using the spectral characterization for symmetric matrices, we have:\n$$\n\\kappa_2(A) = \\lambda_{\\max}(A) \\cdot \\frac{1}{\\lambda_{\\min}(A)} = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\n$$\nTo find the condition number, we first need to determine the eigenvalues of $A$. The eigenvalues of the matrix $T$ are a standard result in linear algebra. For an $n \\times n$ matrix $T$ of the form given above, the eigenvalues are:\n$$\n\\mu_k = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right) \\quad \\text{for } k=1, 2, \\ldots, n\n$$\nSince $A = \\frac{1}{h^2} T$, the eigenvalues of $A$, denoted $\\lambda_k$, are simply the eigenvalues of $T$ scaled by $\\frac{1}{h^2}$:\n$$\n\\lambda_k = \\frac{\\mu_k}{h^2} = \\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) \\quad \\text{for } k=1, 2, \\ldots, n\n$$\nWe need to find the minimum and maximum of these eigenvalues. The term $\\frac{k\\pi}{n+1}$ lies in the interval $(0, \\pi)$ for $k \\in \\{1, \\ldots, n\\}$. In this interval, the function $\\cos(x)$ is strictly decreasing. Therefore, the function $1-\\cos(x)$ is strictly increasing. The minimum eigenvalue $\\lambda_{\\min}(A)$ will occur at the smallest value of $k$, which is $k=1$. The maximum eigenvalue $\\lambda_{\\max}(A)$ will occur at the largest value of $k$, which is $k=n$.\n\nThe minimum eigenvalue is:\n$$\n\\lambda_{\\min}(A) = \\lambda_1 = \\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right)\n$$\nThe maximum eigenvalue is:\n$$\n\\lambda_{\\max}(A) = \\lambda_n = \\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{n\\pi}{n+1}\\right)\\right)\n$$\nThe condition number is the ratio of these two eigenvalues:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\frac{\\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{n\\pi}{n+1}\\right)\\right)}{\\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right)} = \\frac{1 - \\cos\\left(\\frac{n\\pi}{n+1}\\right)}{1 - \\cos\\left(\\frac{\\pi}{n+1}\\right)}\n$$\nWe are interested in the asymptotic behavior as $h \\to 0$. Since $h = \\frac{1}{n+1}$, this is equivalent to $n \\to \\infty$. Let's analyze the numerator and denominator separately.\n\nFor the numerator, we use the identity $\\cos(\\pi - x) = -\\cos(x)$:\n$$\n1 - \\cos\\left(\\frac{n\\pi}{n+1}\\right) = 1 - \\cos\\left(\\frac{(n+1-1)\\pi}{n+1}\\right) = 1 - \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) = 1 - \\left(-\\cos\\left(\\frac{\\pi}{n+1}\\right)\\right) = 1 + \\cos\\left(\\frac{\\pi}{n+1}\\right)\n$$\nAs $n \\to \\infty$, $\\frac{\\pi}{n+1} \\to 0$, and thus $\\cos\\left(\\frac{\\pi}{n+1}\\right) \\to \\cos(0) = 1$. The numerator asymptotically approaches $1+1=2$.\n\nFor the denominator, we analyze the term $1 - \\cos\\left(\\frac{\\pi}{n+1}\\right)$ as $n \\to \\infty$. The argument of the cosine, $x = \\frac{\\pi}{n+1}$, is small. We can use the Taylor series expansion for $\\cos(x)$ around $x=0$: $\\cos(x) = 1 - \\frac{x^2}{2!} + O(x^4)$.\n$$\n1 - \\cos\\left(\\frac{\\pi}{n+1}\\right) = 1 - \\left(1 - \\frac{1}{2}\\left(\\frac{\\pi}{n+1}\\right)^2 + O\\left(\\left(\\frac{\\pi}{n+1}\\right)^4\\right)\\right) = \\frac{\\pi^2}{2(n+1)^2} + O\\left(\\frac{1}{(n+1)^4}\\right)\n$$\nSubstituting $h = \\frac{1}{n+1}$, the denominator has the leading-order behavior $\\frac{\\pi^2 h^2}{2}$.\n\nCombining the asymptotic behavior of the numerator and the denominator, we find the asymptotic scaling of the condition number:\n$$\n\\kappa_2(A) \\sim \\frac{2}{\\frac{\\pi^2 h^2}{2}} = \\frac{4}{\\pi^2 h^2} = \\frac{4}{\\pi^2} h^{-2}\n$$\nThis is the leading-order relation for $\\kappa_2(A)$ as $h \\to 0$. The problem asks for the constant $C$ in the relation $\\kappa_2(A) \\sim C h^{-2}$. By comparing our result with this form, we can identify the constant $C$.\n$$\nC = \\frac{4}{\\pi^2}\n$$\nThis is a closed-form analytic expression for the constant $C$.",
            "answer": "$$\n\\boxed{\\frac{4}{\\pi^2}}\n$$"
        },
        {
            "introduction": "Moving beyond specific examples, this practice examines the structural properties of a general linear operator that may arise from more complex numerical methods, such as nonconforming or stabilized finite element schemes. We decompose the system matrix into its symmetric, skew-symmetric, and symmetric perturbation components to analyze their distinct roles. This exercise reveals the profound principle that the energy of the system, defined by the quadratic form $\\boldsymbol{v}^{\\top} A \\boldsymbol{v}$, depends only on the symmetric part of the matrix $A$, providing a deeper insight into the stability and positive-definiteness of numerical discretizations .",
            "id": "3436701",
            "problem": "Consider the second-order elliptic model problem on the unit square with homogeneous Dirichlet boundary conditions, posed in the variational setting: find $u \\in H_{0}^{1}(\\Omega)$ such that the bilinear form $a(u,v)$ satisfies $a(u,v) = \\ell(v)$ for all $v \\in H_{0}^{1}(\\Omega)$, where $\\Omega = (0,1)^{2}$ and $a(u,v) = \\int_{\\Omega} \\kappa(\\boldsymbol{x}) \\nabla u \\cdot \\nabla v \\, \\mathrm{d}\\boldsymbol{x}$. Assume $\\kappa(\\boldsymbol{x}) \\in L^{\\infty}(\\Omega)$ with $0 < \\underline{\\kappa} \\leq \\kappa(\\boldsymbol{x}) \\leq \\overline{\\kappa} < \\infty$. Let $V_{h}$ be a nonconforming finite-dimensional subspace approximating $H_{0}^{1}(\\Omega)$, and let $u_{h} \\in V_{h}$ denote the computed discrete solution.\n\nSuppose the assembled linear system for $u_{h}$ is $A u_{h} = f$ with a real, square matrix $A \\in \\mathbb{R}^{n \\times n}$ that decomposes as $A = S + J + K$, where the following hold:\n- $S$ is symmetric and positive-definite, induced by the discrete diffusion bilinear form (coercive due to $\\underline{\\kappa} > 0$ and a discrete PoincarÃ© inequality).\n- $J$ is symmetric, arising from consistency corrections due to nonconformity.\n- $K$ is skew-symmetric, stemming from stabilization terms that model weakly upwinded fluxes at interelement faces.\n\nAssume the spectral condition $\\lambda_{\\min}(S) \\geq \\alpha > 0$ for some constant $\\alpha$ independent of $h$, and the symmetric perturbation is controlled in the matrix spectral norm by $\\|J\\|_{2} \\leq \\theta \\, \\lambda_{\\min}(S)$ with $0 \\leq \\theta < 1$. Consider the symmetrized operator $A_{s} = \\frac{1}{2}(A + A^{\\top})$.\n\nDefine the energy seminorms induced by $A$ and $A_{s}$ by $\\|v\\|_{A} = \\sqrt{v^{\\top} A v}$ and $\\|v\\|_{A_{s}} = \\sqrt{v^{\\top} A_{s} v}$ for $v \\in \\mathbb{R}^{n}$. Work from first principles to:\n- Justify conditions under which $A_{s}$ is symmetric positive-definite in terms of $\\alpha$ and $\\theta$.\n- Establish a precise relationship between the energy seminorms $\\|u_{h}\\|_{A}$ and $\\|u_{h}\\|_{A_{s}}$.\n\nReport the exact value of the ratio\n$$\nr = \\frac{\\|u_{h}\\|_{A_{s}}}{\\|u_{h}\\|_{A}}\n$$\nas a single, closed-form expression. No rounding is required, and no physical units are involved.",
            "solution": "The problem requires us to first validate the conditions under which the symmetrized matrix $A_s$ is positive-definite and then to determine the ratio of two energy seminorms defined by the matrix $A$ and its symmetric part $A_s$. We shall proceed by formal manipulation of the given matrix properties.\n\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by the decomposition $A = S + J + K$, where:\n- $S$ is symmetric and positive-definite (SPD).\n- $J$ is symmetric.\n- $K$ is skew-symmetric, meaning $K^{\\top} = -K$.\n\nThe symmetrized operator $A_s$ is defined as $A_s = \\frac{1}{2}(A + A^{\\top})$. Let us express $A_s$ in terms of $S$, $J$, and $K$.\nThe transpose of $A$ is $A^{\\top} = (S + J + K)^{\\top} = S^{\\top} + J^{\\top} + K^{\\top}$.\nGiven the properties of $S$, $J$, and $K$, this simplifies to $A^{\\top} = S + J - K$.\nSubstituting the expressions for $A$ and $A^{\\top}$ into the definition of $A_s$:\n$$\nA_s = \\frac{1}{2} \\left( (S + J + K) + (S + J - K) \\right) = \\frac{1}{2} (2S + 2J) = S + J\n$$\nSo, the symmetrized operator is simply the sum of the two symmetric matrices $S$ and $J$.\n\nThe first task is to justify the conditions under which $A_s$ is symmetric positive-definite.\nFirst, we check for symmetry. Since $S$ and $J$ are both symmetric, their sum $A_s = S+J$ is also symmetric: $(S+J)^{\\top} = S^{\\top} + J^{\\top} = S+J$.\nNext, we check for positive-definiteness. A symmetric matrix is positive-definite if the associated quadratic form $v^{\\top}A_s v$ is strictly positive for any non-zero vector $v \\in \\mathbb{R}^n$.\n$$\nv^{\\top}A_s v = v^{\\top}(S+J)v = v^{\\top}Sv + v^{\\top}Jv\n$$\nFor the SPD matrix $S$, the Rayleigh-Ritz theorem states that $v^{\\top}Sv \\geq \\lambda_{\\min}(S) v^{\\top}v$, where $\\lambda_{\\min}(S)$ is the smallest eigenvalue of $S$. We are given that $\\lambda_{\\min}(S) \\geq \\alpha > 0$. The expression $v^{\\top}v$ is the squared Euclidean norm, $\\|v\\|_2^2$.\nSo, $v^{\\top}Sv \\geq \\lambda_{\\min}(S) \\|v\\|_2^2$.\n\nFor the symmetric matrix $J$, a similar property holds: $v^{\\top}Jv \\geq \\lambda_{\\min}(J) \\|v\\|_2^2$. The spectral norm of a symmetric matrix, $\\|J\\|_2$, is given by $\\max_i |\\lambda_i(J)|$. This implies that $\\lambda_{\\min}(J) \\geq -\\|J\\|_2$.\nTherefore, we can bound the quadratic form of $J$ from below: $v^{\\top}Jv \\geq -\\|J\\|_2 \\|v\\|_2^2$.\n\nCombining these two inequalities, we get a lower bound for $v^{\\top}A_s v$:\n$$\nv^{\\top}A_s v \\geq \\lambda_{\\min}(S) \\|v\\|_2^2 - \\|J\\|_2 \\|v\\|_2^2 = \\left(\\lambda_{\\min}(S) - \\|J\\|_2\\right) \\|v\\|_2^2\n$$\nFor $A_s$ to be positive-definite, we need $\\lambda_{\\min}(S) - \\|J\\|_2 > 0$.\nThe problem provides the condition $\\|J\\|_2 \\leq \\theta \\, \\lambda_{\\min}(S)$ with $0 \\leq \\theta < 1$.\nUsing this condition, we have:\n$$\n\\lambda_{\\min}(S) - \\|J\\|_2 \\geq \\lambda_{\\min}(S) - \\theta \\, \\lambda_{\\min}(S) = (1-\\theta) \\lambda_{\\min}(S)\n$$\nSince $0 \\leq \\theta < 1$, the factor $(1-\\theta)$ is strictly positive. We are also given $\\lambda_{\\min}(S) \\geq \\alpha > 0$.\nThus, for any non-zero vector $v$:\n$$\nv^{\\top}A_s v \\geq (1-\\theta) \\lambda_{\\min}(S) \\|v\\|_2^2 \\geq (1-\\theta) \\alpha \\|v\\|_2^2 > 0\n$$\nThis demonstrates that $A_s = S+J$ is indeed symmetric positive-definite under the specified conditions. These conditions ensure that the symmetric perturbation $J$ is not large enough to destroy the positive-definiteness of the dominant symmetric part $S$.\n\nThe second task is to establish the relationship between the energy seminorms $\\|u_h\\|_A = \\sqrt{u_h^{\\top} A u_h}$ and $\\|u_h\\|_{A_s} = \\sqrt{u_h^{\\top} A_s u_h}$. Let us analyze the quadratic form $v^{\\top}Av$ for an arbitrary vector $v \\in \\mathbb{R}^n$.\n$$\n\\|v\\|_A^2 = v^{\\top}Av = v^{\\top}(S+J+K)v = v^{\\top}(S+J)v + v^{\\top}Kv\n$$\nThe term $v^{\\top}Kv$ is the quadratic form of the skew-symmetric matrix $K$. For any real vector $v$, this term is always zero. To prove this, let $c = v^{\\top}Kv$. Since $c$ is a scalar, it is equal to its transpose: $c = c^{\\top}$.\n$$\nc = (v^{\\top}Kv)^{\\top} = v^{\\top}K^{\\top}v\n$$\nUsing the skew-symmetric property $K^{\\top}=-K$, we have:\n$$\nc = v^{\\top}(-K)v = -v^{\\top}Kv = -c\n$$\nThe only scalar satisfying $c=-c$ is $c=0$. Thus, $v^{\\top}Kv = 0$.\nThis a fundamental property: the skew-symmetric part of a matrix does not contribute to its quadratic form. Therefore, the expression for $\\|v\\|_A^2$ simplifies to:\n$$\n\\|v\\|_A^2 = v^{\\top}(S+J)v\n$$\nNow, we examine the second energy seminorm, $\\|v\\|_{A_s}$. As established earlier, $A_s = S+J$.\n$$\n\\|v\\|_{A_s}^2 = v^{\\top}A_s v = v^{\\top}(S+J)v\n$$\nBy comparing the expressions for the squared seminorms, we arrive at the precise relationship:\n$$\n\\|v\\|_A^2 = \\|v\\|_{A_s}^2\n$$\nThis equality holds for all vectors $v \\in \\mathbb{R}^n$, and specifically for the discrete solution vector $u_h$. Since $A_s$ is positive-definite, both $\\|u_h\\|_A^2$ and $\\|u_h\\|_{A_s}^2$ are non-negative. Taking the principal square root of both sides, we find:\n$$\n\\|u_h\\|_A = \\|u_h\\|_{A_s}\n$$\nThe two energy seminorms are identical.\n\nThe final task is to report the exact value of the ratio $r = \\frac{\\|u_h\\|_{A_s}}{\\|u_h\\|_A}$.\nBased on the equality derived above, and assuming a non-trivial solution $u_h \\neq 0$ (which implies $\\|u_h\\|_{A_s} > 0$ because $A_s$ is positive-definite), we can write:\n$$\nr = \\frac{\\|u_h\\|_{A_s}}{\\|u_h\\|_A} = \\frac{\\|u_h\\|_{A_s}}{\\|u_h\\|_{A_s}} = 1\n$$\nThe ratio is exactly $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "This practice confronts a common and challenging scenario in computational science: the discretization of an anisotropic diffusion problem, where properties vary with direction. When the principal axes of anisotropy are not aligned with the computational grid, standard finite difference or finite volume methods can produce counter-intuitive results. By assembling the stiffness matrix for a specific case, you will demonstrate that even when the resulting matrix is Symmetric Positive-Definite (SPD), reflecting the underlying physics of diffusion, it may fail to be an M-matrix, thus losing the guarantee of a discrete maximum principle. This hands-on calculation highlights the critical distinction between different desirable matrix properties and their implications for the qualitative accuracy of a numerical solution .",
            "id": "3436749",
            "problem": "Consider the linear, second-order anisotropic diffusion operator on a two-dimensional domain with homogeneous Dirichlet boundary conditions,\n$$\n\\mathcal{L} u = - \\nabla \\cdot \\left( \\boldsymbol{K}(x,y) \\nabla u \\right),\n$$\nwhere the diffusion tensor is defined by a rotated principal-axis model\n$$\n\\boldsymbol{K}(x,y) = \\boldsymbol{R}\\!\\left(\\theta(x)\\right) \\begin{pmatrix} k_{1} & 0 \\\\ 0 & k_{2} \\end{pmatrix} \\boldsymbol{R}\\!\\left(\\theta(x)\\right)^{\\top}, \\quad \\boldsymbol{R}(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix},\n$$\nwith $k_{1} = 1$, $k_{2} = 50$, and a spatially varying rotation angle\n$$\n\\theta(x) = \\frac{\\pi}{4} + \\frac{\\pi}{4}\\, x.\n$$\nAssume the domain is the square\n$$\n\\Omega = (0,3) \\times (0,3),\n$$\nand consider a uniform Cartesian grid with spacing $h=1$, so the interior unknowns are located at the four points $(x_{i},y_{j})$ with $(i,j) \\in \\{1,2\\} \\times \\{1,2\\}$ (and Dirichlet boundary conditions $u=0$ on $\\partial\\Omega$). Let the unknowns be ordered lexicographically as\n$$\n\\boldsymbol{u} = \\begin{pmatrix} u_{1,1} \\\\ u_{2,1} \\\\ u_{1,2} \\\\ u_{2,2} \\end{pmatrix}.\n$$\n\nDiscretize the operator using a standard five-point finite-volume balance over unit square control volumes with two-point normal fluxes and centered tangential gradients at faces. Specifically, at each interior node $(i,j)$, approximate the outgoing fluxes across the east (E), west (W), north (N), and south (S) faces by\n$$\nJ_{\\mathrm{E}} = k_{11}^{\\mathrm{E}} \\left( u_{i+1,j} - u_{i,j} \\right) + k_{12}^{\\mathrm{E}} \\frac{u_{i,j+1} - u_{i,j-1}}{2},\n$$\n$$\nJ_{\\mathrm{W}} = - \\left[ k_{11}^{\\mathrm{W}} \\left( u_{i,j} - u_{i-1,j} \\right) + k_{12}^{\\mathrm{W}} \\frac{u_{i,j+1} - u_{i,j-1}}{2} \\right],\n$$\n$$\nJ_{\\mathrm{N}} = k_{12}^{\\mathrm{N}} \\frac{u_{i+1,j} - u_{i-1,j}}{2} + k_{22}^{\\mathrm{N}} \\left( u_{i,j+1} - u_{i,j} \\right),\n$$\n$$\nJ_{\\mathrm{S}} = - \\left[ k_{12}^{\\mathrm{S}} \\frac{u_{i+1,j} - u_{i-1,j}}{2} + k_{22}^{\\mathrm{S}} \\left( u_{i,j} - u_{i,j-1} \\right) \\right],\n$$\nand set the discrete operator at $(i,j)$ as the sum of outgoing fluxes,\n$$\n(\\mathcal{L}_{h} u)_{i,j} = J_{\\mathrm{E}} + J_{\\mathrm{W}} + J_{\\mathrm{N}} + J_{\\mathrm{S}}.\n$$\nThe face coefficients are evaluated from $\\boldsymbol{K}$ at face-centered $x$-locations, and by symmetry of $\\boldsymbol{K}$ one has\n$$\nk_{11}(x) = k_{1} \\cos^{2}\\theta(x) + k_{2} \\sin^{2}\\theta(x), \\quad k_{22}(x) = k_{1} \\sin^{2}\\theta(x) + k_{2} \\cos^{2}\\theta(x), \\quad k_{12}(x) = \\left( k_{1} - k_{2} \\right) \\sin\\theta(x) \\cos\\theta(x).\n$$\n\nTasks:\n- Assemble the $4 \\times 4$ stiffness matrix $A$ associated with $\\mathcal{L}_{h}$ and the ordering of $\\boldsymbol{u}$ above. Using the explicit trigonometric values at the face angles $x = 0.5, 1.0, 1.5, 2.0, 2.5$, determine at least one off-diagonal entry that becomes positive, thereby demonstrating loss of the $M$-matrix property in the five-point stencil.\n- Using the assembled matrix $A$, evaluate the quadratic form $x^{\\top} A x$ for the vector\n$$\n\\boldsymbol{x} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix}.\n$$\nProvide your final answer as a single exact closed-form analytic expression. No rounding is required, and no units are involved.",
            "solution": "The task is to assemble the $4 \\times 4$ stiffness matrix $A$ for the given anisotropic diffusion operator discretized on a $2 \\times 2$ grid of interior nodes, and then to evaluate a quadratic form $\\boldsymbol{x}^{\\top} A \\boldsymbol{x}$.\n\nFirst, we establish the general form of a row of the stiffness matrix $A$. The discrete operator at an interior node $(i,j)$ is given by\n$$(\\mathcal{L}_{h} u)_{i,j} = J_{\\mathrm{E}} + J_{\\mathrm{W}} + J_{\\mathrm{N}} + J_{\\mathrm{S}}$$\nSubstituting the given flux expressions:\n\\begin{align*}\n(\\mathcal{L}_{h} u)_{i,j} = & \\left( k_{11}^{\\mathrm{E}} ( u_{i+1,j} - u_{i,j} ) + k_{12}^{\\mathrm{E}} \\frac{u_{i,j+1} - u_{i,j-1}}{2} \\right) \\\\\n& - \\left( k_{11}^{\\mathrm{W}} ( u_{i,j} - u_{i-1,j} ) + k_{12}^{\\mathrm{W}} \\frac{u_{i,j+1} - u_{i,j-1}}{2} \\right) \\\\\n& + \\left( k_{12}^{\\mathrm{N}} \\frac{u_{i+1,j} - u_{i-1,j}}{2} + k_{22}^{\\mathrm{N}} ( u_{i,j+1} - u_{i,j} ) \\right) \\\\\n& - \\left( k_{12}^{\\mathrm{S}} \\frac{u_{i+1,j} - u_{i-1,j}}{2} + k_{22}^{\\mathrm{S}} ( u_{i,j} - u_{i,j-1} ) \\right)\n\\end{align*}\nCollecting coefficients for each unknown $u_{k,l}$:\n\\begin{align*}\n(\\mathcal{L}_{h} u)_{i,j} = & (-k_{11}^{\\mathrm{E}} - k_{11}^{\\mathrm{W}} - k_{22}^{\\mathrm{N}} - k_{22}^{\\mathrm{S}}) u_{i,j} \\\\\n& + (k_{11}^{\\mathrm{E}} + \\frac{k_{12}^{\\mathrm{N}} - k_{12}^{\\mathrm{S}}}{2}) u_{i+1,j} \\\\\n& + (k_{11}^{\\mathrm{W}} - \\frac{k_{12}^{\\mathrm{N}} - k_{12}^{\\mathrm{S}}}{2}) u_{i-1,j} \\\\\n& + (k_{22}^{\\mathrm{N}} + \\frac{k_{12}^{\\mathrm{E}} - k_{12}^{\\mathrm{W}}}{2}) u_{i,j+1} \\\\\n& + (k_{22}^{\\mathrm{S}} - \\frac{k_{12}^{\\mathrm{E}} - k_{12}^{\\mathrm{W}}}{2}) u_{i,j-1}\n\\end{align*}\nThe stiffness matrix $A$ for the system $A \\boldsymbol{u} = \\boldsymbol{f}$ corresponding to the operator equation $(\\mathcal{L}_{h} u) = f$ has entries $A_{IJ}$ where $I$ is the index for node $(i,j)$ and $J$ is the index for a neighboring node. By standard convention for stiffness matrices of elliptic operators, the diagonal entry $A_{II}$ is the negative of the coefficient of $u_{i,j}$, and an off-diagonal entry $A_{IJ}$ is the negative of the coefficient of the corresponding neighbor $u_{J}$.\nThus, the stencil for the stiffness matrix entries is:\n- Diagonal term: $k_{11}^{\\mathrm{E}} + k_{11}^{\\mathrm{W}} + k_{22}^{\\mathrm{N}} + k_{22}^{\\mathrm{S}}$\n- Off-diagonal for $u_{i+1,j}$: $-(k_{11}^{\\mathrm{E}} + \\frac{k_{12}^{\\mathrm{N}} - k_{12}^{\\mathrm{S}}}{2})$\n- Off-diagonal for $u_{i-1,j}$: $-(k_{11}^{\\mathrm{W}} - \\frac{k_{12}^{\\mathrm{N}} - k_{12}^{\\mathrm{S}}}{2})$\n- Off-diagonal for $u_{i,j+1}$: $-(k_{22}^{\\mathrm{N}} + \\frac{k_{12}^{\\mathrm{E}} - k_{12}^{\\mathrm{W}}}{2})$\n- Off-diagonal for $u_{i,j-1}$: $-(k_{22}^{\\mathrm{S}} - \\frac{k_{12}^{\\mathrm{E}} - k_{12}^{\\mathrm{W}}}{2})$\n\nNext, we evaluate the tensor components $k_{ab}(x)$ at the required face-centered locations. We have $k_{1}=1$, $k_{2}=50$, and $\\theta(x) = \\frac{\\pi}{4}(1+x)$.\nThe components are $k_{11}(x) = 1 + 49 \\sin^2\\theta(x)$, $k_{22}(x) = 1 + 49 \\cos^2\\theta(x)$, and $k_{12}(x) = -49 \\sin\\theta(x)\\cos\\theta(x) = -\\frac{49}{2}\\sin(2\\theta(x))$.\n\nThe required $x$-locations are $x \\in \\{0.5, 1, 1.5, 2, 2.5\\}$.\n- $x=0.5: \\theta = 3\\pi/8$. $k_{11}(0.5) = \\frac{102+49\\sqrt{2}}{4}$, $k_{22}(0.5) = \\frac{102-49\\sqrt{2}}{4}$, $k_{12}(0.5) = -\\frac{49\\sqrt{2}}{4}$.\n- $x=1.0: \\theta = \\pi/2$. $k_{11}(1) = 50$, $k_{22}(1) = 1$, $k_{12}(1) = 0$.\n- $x=1.5: \\theta = 5\\pi/8$. $k_{11}(1.5) = \\frac{102+49\\sqrt{2}}{4}$, $k_{22}(1.5) = \\frac{102-49\\sqrt{2}}{4}$, $k_{12}(1.5) = \\frac{49\\sqrt{2}}{4}$.\n- $x=2.0: \\theta = 3\\pi/4$. $k_{11}(2) = 1+49(\\frac{1}{2}) = \\frac{51}{2}$, $k_{22}(2) = \\frac{51}{2}$, $k_{12}(2) = 49/2$.\n- $x=2.5: \\theta = 7\\pi/8$. $k_{11}(2.5) = \\frac{102-49\\sqrt{2}}{4}$, $k_{12}(2.5) = \\frac{49\\sqrt{2}}{4}$.\n\nThe unknowns are ordered $\\boldsymbol{u} = (u_{1,1}, u_{2,1}, u_{1,2}, u_{2,2})^{\\top}$. The problem requires calculating $\\boldsymbol{x}^{\\top} A \\boldsymbol{x}$ for $\\boldsymbol{x} = (1, 0, -1, 0)^{\\top}$. This calculation expands to:\n$$\\boldsymbol{x}^{\\top} A \\boldsymbol{x} = \\begin{pmatrix} 1 & 0 & -1 & 0 \\end{pmatrix} A \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{pmatrix} = A_{11} - A_{13} - A_{31} + A_{33}$$\nWe only need to compute these four entries of the matrix $A$.\n\n- $A_{11}$ (diagonal at $(1,1)$):\n  Faces are at E($x=1.5$), W($x=0.5$), N($x=1$), S($x=1$).\n  $A_{11} = k_{11}(1.5) + k_{11}(0.5) + k_{22}(1) + k_{22}(1) = \\frac{102+49\\sqrt{2}}{4} + \\frac{102+49\\sqrt{2}}{4} + 1 + 1 = \\frac{102+49\\sqrt{2}}{2} + 2 = \\frac{106+49\\sqrt{2}}{2}$.\n\n- $A_{33}$ (diagonal at $(1,2)$):\n  Faces are at E($x=1.5$), W($x=0.5$), N($x=1$), S($x=1$).\n  The coefficients are identical to those for $A_{11}$ because the problem is invariant in the $y$-direction.\n  $A_{33} = A_{11} = \\frac{106+49\\sqrt{2}}{2}$.\n\n- $A_{13}$ (off-diagonal connecting $(1,1)$ to $(1,2)$):\n  This corresponds to the neighbor $u_{1,2} = u_{i,j+1}$ for node $(i,j)=(1,1)$.\n  $A_{13} = -(k_{22}^{\\mathrm{N}} + \\frac{k_{12}^{\\mathrm{E}} - k_{12}^{\\mathrm{W}}}{2}) = -(k_{22}(1) + \\frac{k_{12}(1.5) - k_{12}(0.5)}{2}) = -(1 + \\frac{49\\sqrt{2}/4 - (-49\\sqrt{2}/4)}{2}) = -(1 + \\frac{98\\sqrt{2}/4}{2}) = -(1 + \\frac{49\\sqrt{2}}{4})$.\n\n- $A_{31}$ (off-diagonal connecting $(1,2)$ to $(1,1)$):\n  This corresponds to the neighbor $u_{1,1} = u_{i,j-1}$ for node $(i,j)=(1,2)$.\n  $A_{31} = -(k_{22}^{\\mathrm{S}} - \\frac{k_{12}^{\\mathrm{E}} - k_{12}^{\\mathrm{W}}}{2}) = -(k_{22}(1) - \\frac{k_{12}(1.5) - k_{12}(0.5)}{2}) = -1 + \\frac{49\\sqrt{2}}{4}$.\n\nWith these matrix entries, we address the first task.\nAn M-matrix must have non-positive off-diagonal entries. We check the sign of $A_{31}$:\n$A_{31} = \\frac{49\\sqrt{2}}{4} - 1$. Since $\\sqrt{2} \\approx 1.414$, we have $\\frac{49\\sqrt{2}}{4} \\approx \\frac{49 \\times 1.414}{4} \\approx 17.3 > 1$.\nThus, $A_{31} > 0$. This positive off-diagonal entry demonstrates that the resulting stiffness matrix $A$ is not an M-matrix.\n\nNow, we proceed to the second task and evaluate the quadratic form:\n$$\\boldsymbol{x}^{\\top} A \\boldsymbol{x} = A_{11} + A_{33} - (A_{13} + A_{31})$$\nFirst, we sum the diagonal terms:\n$$A_{11} + A_{33} = 2 \\times \\left(\\frac{106+49\\sqrt{2}}{2}\\right) = 106 + 49\\sqrt{2}$$\nNext, we sum the off-diagonal terms:\n$$A_{13} + A_{31} = -\\left(1 + \\frac{49\\sqrt{2}}{4}\\right) + \\left(\\frac{49\\sqrt{2}}{4} - 1\\right) = -1 - \\frac{49\\sqrt{2}}{4} + \\frac{49\\sqrt{2}}{4} - 1 = -2$$\nFinally, we compute the quadratic form:\n$$\\boldsymbol{x}^{\\top} A \\boldsymbol{x} = (106 + 49\\sqrt{2}) - (-2) = 106 + 49\\sqrt{2} + 2 = 108 + 49\\sqrt{2}$$\nThis is the final exact analytical expression.",
            "answer": "$$\\boxed{108 + 49\\sqrt{2}}$$"
        }
    ]
}