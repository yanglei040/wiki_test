{
    "hands_on_practices": [
        {
            "introduction": "In numerical simulations, identifying the boundary of a black hole is not straightforward, as the global nature of the event horizon is difficult to track on a time-evolving grid. This exercise introduces the more practical concept of a quasi-local apparent horizon, defined as a marginally outer trapped surface (MOTS). You will derive the fundamental equation that apparent horizon finders solve , connecting the geometric condition of vanishing null expansion, $\\theta_{(l)} = 0$, to the concrete variables of the $3+1$ formalism.",
            "id": "3464688",
            "problem": "Consider a numerically evolved binary black hole spacetime within General Relativity (GR), decomposed in the $3+1$ formalism. Let $\\Sigma_{t}$ be a spatial hypersurface at coordinate time $t$ with unit timelike normal $n^{a}$, induced spatial metric $\\gamma_{ab} = g_{ab} + n_{a} n_{b}$, and extrinsic curvature $K_{ab} = - \\gamma_{a}^{\\ c} \\gamma_{b}^{\\ d} \\nabla_{c} n_{d}$. Inside $\\Sigma_{t}$, consider a smooth, closed two-surface $\\mathcal{S}$ with outward pointing unit spatial normal $s^{a}$, satisfying $s^{a} s_{a} = 1$ and $n^{a} s_{a} = 0$. The induced two-metric on $\\mathcal{S}$ is $q_{ab} = \\gamma_{ab} - s_{a} s_{b}$. Define the outgoing null normal by $l^{a} = n^{a} + s^{a}$.\n\nThe expansion of the outgoing null congruence is defined as $\\theta_{(l)} = q^{ab} \\nabla_{a} l_{b}$, where $\\nabla_{a}$ is the spacetime covariant derivative. Using only the preceding definitions and the spatial Levi-Civita connection $D_{a}$ associated with $\\gamma_{ab}$, derive an explicit expression for $\\theta_{(l)}$ written entirely in terms of the $3+1$ variables on $\\Sigma_{t}$, namely $D_{i}$, $K_{ij}$, its trace $K = \\gamma^{ij} K_{ij}$, and $s^{i}$, where indices $i,j$ are spatial indices on $\\Sigma_{t}$.\n\nYour final answer must be a single closed-form analytic expression for $\\theta_{(l)}$ in terms of $D_{i}$, $K_{ij}$, $K$, and $s^{i}$. Do not provide intermediate steps in the final answer. No numerical evaluation or rounding is required, and no units are to be reported. This expression implicitly defines the apparent horizon condition $\\theta_{(l)} = 0$ for a marginally outer trapped surface (MOTS) on $\\Sigma_{t}$.",
            "solution": "The user wants to derive the expansion of the outgoing null congruence, $\\theta_{(l)}$, on a closed two-surface $\\mathcal{S}$ embedded within a spatial hypersurface $\\Sigma_t$ in the $3+1$ decomposition of General Relativity. The final expression must be in terms of the spatial covariant derivative $D_i$, the extrinsic curvature $K_{ij}$ and its trace $K$, and the spatial normal to the surface $s^i$.\n\nThe validation of the problem statement has been performed and the problem is deemed valid. It is a standard derivation in numerical relativity and differential geometry as applied to General Relativity. All definitions and the context are scientifically sound and well-posed.\n\nWe begin the derivation from the provided definition of the expansion:\n$$\n\\theta_{(l)} = q^{ab} \\nabla_{a} l_{b}\n$$\nThe givens are:\n1.  The outgoing null normal vector: $l^{a} = n^{a} + s^{a}$, which implies $l_{b} = n_{b} + s_{b}$.\n2.  The projector onto the two-surface $\\mathcal{S}$: $q_{ab} = \\gamma_{ab} - s_{a} s_{b}$. Its inverse on the tangent space of the surface can be written as $q^{ab} = \\gamma^{ab} - s^{a} s^{b}$, where $\\gamma^{ab}$ is the inverse of the spatial metric $\\gamma_{ab}$. Note that indices on spatial tensors like $s^a$ and $\\gamma_{ab}$ can be raised and lowered using $\\gamma_{ab}$ and $\\gamma^{ab}$.\n\nSubstituting the definition of $l_b$ into the expansion formula, we get:\n$$\n\\theta_{(l)} = q^{ab} \\nabla_{a} (n_{b} + s_{b}) = q^{ab} \\nabla_{a} n_{b} + q^{ab} \\nabla_{a} s_{b}\n$$\nWe will analyze the two terms on the right-hand side separately.\n\n**Term 1: $q^{ab} \\nabla_{a} n_{b}$**\n\nThis term involves the spacetime covariant derivative of the timelike unit normal vector $n_b$ to the spatial hypersurface $\\Sigma_t$. The decomposition of $\\nabla_a n_b$ in the $3+1$ formalism is given by the Weingarten equation:\n$$\n\\nabla_a n_b = -K_{ab} + n_a a_b\n$$\nwhere $K_{ab} = - \\gamma_{a}^{\\ c} \\gamma_{b}^{\\ d} \\nabla_{c} n_{d}$ is the extrinsic curvature (a spatial tensor), and $a_b = n^c \\nabla_c n_b$ is the acceleration of the fiducial observers whose worldlines are orthogonal to $\\Sigma_t$.\n\nSubstituting this into the first term:\n$$\nq^{ab} \\nabla_{a} n_{b} = q^{ab} (-K_{ab} + n_a a_b) = - q^{ab} K_{ab} + q^{ab} n_a a_b\n$$\nThe tensor $q^{ab}$ is a projector onto the two-surface $\\mathcal{S}$, which lies within the spatial hypersurface $\\Sigma_t$. By construction, any vector in the tangent space of $\\mathcal{S}$ is orthogonal to $n^a$. The projector $q^{ab}$ is therefore a spatial tensor, meaning it has no components along $n^a$. This can be expressed as $q^{ab} n_a = 0$ and $q^{ab} n_b = 0$.\nThe property $q^{ab} n_a = 0$ immediately implies that the second part of the expression vanishes: $q^{ab} n_a a_b = 0$.\n\nThe first part becomes:\n$$\n-q^{ab} K_{ab} = -(\\gamma^{ab} - s^a s^b) K_{ab} = -(\\gamma^{ab} K_{ab} - s^a s^b K_{ab})\n$$\nBy definition, $\\gamma^{ab} K_{ab}$ is the trace of the extrinsic curvature, denoted by $K$. The term $s^a s^b K_{ab}$ is the projection of the extrinsic curvature onto the normal direction $s^a$ twice. Since all tensors in this expression ($K_{ab}$, $\\gamma^{ab}$, $s^a$) are spatial, we can use spatial indices $i, j, \\ldots$:\n$$\nq^{ab} \\nabla_{a} n_{b} = -K + K_{ij} s^i s^j\n$$\n\n**Term 2: $q^{ab} \\nabla_{a} s_{b}$**\n\nThis term involves the spacetime covariant derivative of the spatial unit normal vector $s_b$ to the two-surface $\\mathcal{S}$. We need to relate this to the spatial covariant derivative $D_a$ associated with the spatial metric $\\gamma_{ab}$.\nThe operator $D_a$ is defined by its action on spatial tensors, which results from projecting the four-dimensional covariant derivative onto the spatial hypersurface. For a spatial covector like $s_b$, the relationship is:\n$$\nD_a s_b = \\gamma_a^c \\gamma_b^d \\nabla_c s_d\n$$\nwhere $\\gamma_a^c = g_a^c + n_a n^c$ is the projector onto the spatial slice $\\Sigma_t$. We want to evaluate $q^{ab} \\nabla_a s_b$. Since $q^{ab}$ is a spatial tensor, it satisfies $q^{ab} = \\gamma_c^a \\gamma_d^b q^{cd}$. We can thus write:\n$$\nq^{ab} \\nabla_a s_b = (\\gamma_c^a \\gamma_d^b q^{cd}) \\nabla_a s_b\n$$\nThe relationship between $\\nabla_a s_b$ and $D_a s_b$ can be found by decomposing $\\nabla_a s_b$ into parts tangential and orthogonal to $\\Sigma_t$. Any tensor $T_{ab}$ can be decomposed as $T_{ab} = \\gamma_a^c \\gamma_b^d T_{cd} + \\dots$ where the remaining terms involve at least one $n_a$ or $n_b$. When we contract with $q^{ab}$, these additional terms vanish because $q^{ab}n_a=0$ and $q^{ab}n_b=0$. Therefore:\n$$\nq^{ab} \\nabla_a s_b = q^{ab} (\\gamma_a^c \\gamma_b^d \\nabla_c s_d) = q^{ab} D_a s_b\n$$\nThis demonstrates that for our purpose, we can replace the spacetime derivative with the spatial one. The expression is now purely spatial, so we switch to spatial indices:\n$$\nq^{ij} D_i s_j = (\\gamma^{ij} - s^i s^j) D_i s_j = \\gamma^{ij} D_i s_j - s^i s^j D_i s_j\n$$\nThe first part is the spatial divergence of $s^j$: $\\gamma^{ij} D_i s_j = D_i (\\gamma^{ij} s_j) - s_j (D_i \\gamma^{ij})$. Since $D_i$ is metric-compatible ($D_i \\gamma^{ij} = 0$), this simplifies to $D_i s^i$. This term, $D_i s^i$, represents the extrinsic curvature of the 2-surface $\\mathcal{S}$ as embedded in the 3-space $\\Sigma_t$, and is often denoted by $\\mathcal{K}$.\n\nFor the second part, $-s^i s^j D_i s_j$, we use the fact that $s^i$ is a unit vector in the spatial slice: $s_j s^j = \\gamma_{jk} s^j s^k = 1$. Let's take the covariant derivative along the direction of $s^i$:\n$$\ns^i D_i (s_j s^j) = s^i D_i (1) = 0\n$$\nUsing the product rule and metric compatibility of $D_i$:\n$$\ns^i ( (D_i s_j)s^j + s_j(D_i s^j) ) = 0\n$$\nRearranging terms gives:\n$$\ns^j (s^i D_i s_j) + s_j (s^i D_i s^j) = 0 \\implies 2 s^j (s^i D_i s_j) = 0\n$$\nThe term $s^i D_i s_j$ is the acceleration of the congruence of curves normal to $\\mathcal{S}$. The result $s^j (s^i D_i s_j) = 0$ means this acceleration is orthogonal to $s^j$. The quantity we need to evaluate is $-s^j(s^i D_i s_j)$, which is precisely what we have just shown to be zero.\nTherefore, the second term is zero: $-s^i s^j D_i s_j = 0$.\n\nSo, we have:\n$$\nq^{ab} \\nabla_{a} s_{b} = D_i s^i\n$$\n\n**Combining the results**\n\nWe can now sum the two derived parts to obtain the final expression for the expansion $\\theta_{(l)}$:\n$$\n\\theta_{(l)} = (q^{ab} \\nabla_{a} n_{b}) + (q^{ab} \\nabla_{a} s_{b}) = (-K + K_{ij} s^i s^j) + (D_i s^i)\n$$\nRearranging the terms for clarity, we get:\n$$\n\\theta_{(l)} = D_i s^i + K_{ij} s^i s^j - K\n$$\nThis equation provides the expansion of the outgoing null congruence in terms of the intrinsic geometry of the spatial slice ($D_i$), its extrinsic curvature ($K_{ij}$ and $K$), and the geometry of the 2-surface within the slice ($s^i$). The condition for an apparent horizon (a marginally outer trapped surface) is $\\theta_{(l)} = 0$.",
            "answer": "$$\n\\boxed{D_{i} s^{i} + K_{ij} s^{i} s^{j} - K}\n$$"
        },
        {
            "introduction": "A numerical simulation is only as good as its verification, and for solving partial differential equations like Einstein's, convergence testing is the cornerstone of this process. The principle is that for a well-behaved numerical method, the error should decrease as a predictable power of the grid resolution, $\\varepsilon \\propto h^{p}$. In this essential practice , you will implement a convergence test analysis, a fundamental skill for any computational physicist, to determine the empirical order of convergence from synthetic simulation data.",
            "id": "3464789",
            "problem": "Consider numerical evolutions of binary black hole inspiral, merger, and ringdown in geometric units with $G = c = 1$ and total mass $M = 1$, using the Baumgarte–Shapiro–Shibata–Nakamura (BSSN) formulation and the Covariant Conformal Z4 (CCZ4) formulation. In such evolutions, one monitors constraint violations (for example, Hamiltonian and Z4 constraints) and apparent horizon properties (for example, mass and dimensionless spin) as functions of the grid resolution $h$, where $h$ is the uniform spatial grid spacing expressed in units of the total mass $M$.\n\nThe foundational numerical analysis principle is that a consistent and stable numerical discretization of order $p$ produces truncation error that asymptotically scales as a power law in the grid spacing, so that an observable error $\\varepsilon$ obeys $ \\varepsilon \\sim \\kappa \\, h^{p} $ for some constant $\\kappa$ independent of $h$ in the asymptotic regime. Consequently, the empirical order $p$ can be inferred from measurements of an error proxy versus $h$ by identifying a power-law relationship.\n\nYou are to implement a program that, given several data sets of resolution $h$ and corresponding measurements, performs convergence tests by estimating the empirical order $p$ of convergence. For constraint measurements, you will treat the provided constraint norms $ \\lVert \\mathcal{C} \\rVert $ directly as error proxies. For apparent horizon measurements, you will treat the absolute deviations of the measured property from the continuum target value as the error proxy. Your program must use a robust method that estimates $p$ by fitting a power law $\\varepsilon \\propto h^{p}$ across all resolutions in a given data set and returning the exponent $p$.\n\nYour program must compute the empirical order $p$ for each of the following test cases:\n\n- Test case A (BSSN constraint norm, happy path): resolutions $h = [1/48,\\, 1/64,\\, 1/80,\\, 1/96]$ and measured constraint norms $ \\lVert \\mathcal{C} \\rVert = [1.884\\times 10^{-4},\\, 5.960\\times 10^{-5},\\, 2.442\\times 10^{-5},\\, 1.178\\times 10^{-5}] $.\n- Test case B (CCZ4 constraint norm, different order): resolutions $h = [1/40,\\, 1/56,\\, 1/72,\\, 1/88]$ and measured constraint norms $ \\lVert \\mathcal{C} \\rVert = [9.766\\times 10^{-3},\\, 1.815\\times 10^{-3},\\, 5.168\\times 10^{-4},\\, 1.894\\times 10^{-4}] $.\n- Test case C (apparent horizon mass, second-order behavior): continuum target mass $M_{\\infty} = 0.951$, resolutions $h = [1/48,\\, 1/64,\\, 1/80,\\, 1/96]$, and measured horizon masses $ M(h) = [0.9510173611,\\, 0.951009765625,\\, 0.95100625,\\, 0.9510043402778] $. Use the absolute deviations $ \\lvert M(h) - M_{\\infty} \\rvert $ as the error proxy.\n- Test case D (apparent horizon dimensionless spin, higher-order behavior): continuum target spin $ \\chi_{\\infty} = 0.686 $, resolutions $h = [1/44,\\, 1/60,\\, 1/88,\\, 1/100]$, and measured horizon spins $ \\chi(h) = [0.6861334,\\, 0.68603858,\\, 0.68600834,\\, 0.686005] $. Use the absolute deviations $ \\lvert \\chi(h) - \\chi_{\\infty} \\rvert $ as the error proxy.\n- Test case E (constraint norm near floor, boundary case): resolutions $h = [1/96,\\, 1/100,\\, 1/120,\\, 1/160]$ and measured constraint norms $ \\lVert \\mathcal{C} \\rVert = [1.05\\times 10^{-6},\\, 1.02\\times 10^{-6},\\, 1.01\\times 10^{-6},\\, 1.00\\times 10^{-6}] $.\n\nRequirements:\n- For each test case, estimate the empirical order $p$ by fitting a power-law relation between the error proxy and $h$ over all resolutions provided for that case. The estimation must be done using a mathematically sound algorithm that is invariant under the scaling of the error proxy and does not rely on any single pairwise ratio alone.\n- Angles are not involved; no angle unit is required.\n- If any computed error proxy value is non-positive, the program must handle it gracefully by not attempting to take logarithms of non-positive numbers.\n- The final outputs are dimensionless and require no physical unit. However, you must interpret measured masses and spins in geometric units and form error proxies as absolute deviations from the specified continuum target values.\n- Your program should produce a single line of output containing the estimated orders as a comma-separated list enclosed in square brackets, rounded to three decimal places, in the order $[p_{A},\\, p_{B},\\, p_{C},\\, p_{D},\\, p_{E}]$.\n\nYour program must be self-contained, require no input, and use only the Python standard library plus the specified numerical libraries.",
            "solution": "The problem has been examined and is deemed valid. It is scientifically grounded in the principles of numerical analysis as applied to computational relativity, is well-posed with a clear objective and sufficient data, and is expressed in precise, objective language. The problem is a standard exercise in convergence testing, a fundamental practice for validating numerical codes in scientific computing.\n\nThe goal is to determine the empirical order of convergence, denoted by $p$, for several numerical data sets. The underlying principle is that the truncation error, $\\varepsilon$, of a numerical approximation scheme of order $p$ scales with the grid resolution $h$ according to the power-law relation:\n$$ \\varepsilon(h) \\approx \\kappa h^p $$\nwhere $\\kappa$ is a proportionality constant that depends on the specific problem and the numerical method, but not on the resolution $h$, at least in the asymptotic limit where $h$ is sufficiently small.\n\nTo determine the exponent $p$ from a set of measurements of an error proxy $\\varepsilon_i$ at corresponding resolutions $h_i$, we can transform the power-law relation into a linear one by taking the natural logarithm of both sides:\n$$ \\ln(\\varepsilon) \\approx \\ln(\\kappa) + p \\ln(h) $$\nThis equation is of the form $y = c + mx$, where the variables are $y = \\ln(\\varepsilon)$ and $x = \\ln(h)$, the y-intercept is $c = \\ln(\\kappa)$, and, most importantly, the slope is the convergence order $m = p$.\n\nThe problem requires a robust estimation of $p$ using all available data points for each case. The method of linear least-squares regression is ideally suited for this task. Given a set of $N$ data points $(x_i, y_i)$, this method finds the line that minimizes the sum of the squared vertical distances from the points to the line. The slope of this line provides a robust estimate for $p$. An explicit formula for the slope $p$ given $N$ data points $(x_i, y_i) = (\\ln(h_i), \\ln(\\varepsilon_i))$ is:\n$$ p = \\frac{N \\sum_{i=1}^{N}(x_i y_i) - (\\sum_{i=1}^{N} x_i)(\\sum_{i=1}^{N} y_i)}{N \\sum_{i=1}^{N}(x_i^2) - (\\sum_{i=1}^{N} x_i)^2} $$\nThis approach satisfies the problem's requirements: it uses all available data, and it is invariant to the scaling of the error proxy $\\varepsilon$, as multiplying $\\varepsilon$ by a constant only shifts the intercept $\\ln(\\kappa)$ without affecting the slope $p$.\n\nThe algorithm to be implemented is as follows:\n$1$. For each test case, assemble the list of resolutions $\\{h_i\\}$ and corresponding measurements.\n$2$. Construct the error proxy values $\\{\\varepsilon_i\\}$.\n    - For constraint norms, $\\varepsilon_i$ is the given norm value, $\\lVert \\mathcal{C} \\rVert_i$.\n    - For apparent horizon properties, $\\varepsilon_i$ is the absolute deviation from the continuum target value, e.g., $\\varepsilon_i = \\lvert M(h_i) - M_{\\infty} \\rvert$.\n$3$. Filter the data to retain only points where the error proxy $\\varepsilon_i$ is strictly positive, as the logarithm is undefined for non-positive values. This ensures graceful handling as required. The provided test data do not contain non-positive errors, but this step is crucial for a robust implementation.\n$4$. Transform the filtered data by taking the natural logarithm of each resolution $h_i$ and each error proxy $\\varepsilon_i$ to obtain the set of points $(\\ln(h_i), \\ln(\\varepsilon_i))$.\n$5$. Apply linear least-squares regression to these log-transformed points to compute the slope, which is the estimate for the convergence order $p$.\n$6$. Collect the computed orders for all test cases and format them as specified.\n\nFor test case E, the measured constraint norms are nearly constant and close to a floor value, likely due to finite floating-point precision or other systematic effects that do not decrease with resolution. In this scenario, the error $\\varepsilon$ is approximately constant, so $\\varepsilon \\propto h^0$. The fitting procedure is therefore expected to yield a convergence order $p$ close to $0$.\n\nThis procedure will be applied to each of the five test cases provided.\n- Case A: BSSN constraint norm. $\\varepsilon_i = \\lVert \\mathcal{C} \\rVert_i$. Expected $p \\approx 4$.\n- Case B: CCZ4 constraint norm. $\\varepsilon_i = \\lVert \\mathcal{C} \\rVert_i$. Expected $p \\approx 5$.\n- Case C: Apparent horizon mass. $\\varepsilon_i = \\lvert M(h_i) - M_{\\infty} \\rvert$. Expected $p \\approx 2$.\n- Case D: Apparent horizon spin. $\\varepsilon_i = \\lvert \\chi(h_i) - \\chi_{\\infty} \\rvert$. Expected $p \\approx 4$.\n- Case E: Constraint norm at floor. $\\varepsilon_i = \\lVert \\mathcal{C} \\rVert_i$. Expected $p \\approx 0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats # Scipy is allowed but numpy.polyfit is sufficient and used here.\n\ndef solve():\n    \"\"\"\n    Computes the empirical order of convergence for several numerical relativity test cases\n    by fitting a power law to the error proxy vs. grid resolution data.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: BSSN constraint norm\n        {\n            \"name\": \"A\",\n            \"resolutions\": np.array([1/48, 1/64, 1/80, 1/96]),\n            \"measurements\": np.array([1.884e-4, 5.960e-5, 2.442e-5, 1.178e-5]),\n            \"target\": None\n        },\n        # Case B: CCZ4 constraint norm\n        {\n            \"name\": \"B\",\n            \"resolutions\": np.array([1/40, 1/56, 1/72, 1/88]),\n            \"measurements\": np.array([9.766e-3, 1.815e-3, 5.168e-4, 1.894e-4]),\n            \"target\": None\n        },\n        # Case C: Apparent horizon mass\n        {\n            \"name\": \"C\",\n            \"resolutions\": np.array([1/48, 1/64, 1/80, 1/96]),\n            \"measurements\": np.array([0.9510173611, 0.951009765625, 0.95100625, 0.9510043402778]),\n            \"target\": 0.951\n        },\n        # Case D: Apparent horizon dimensionless spin\n        {\n            \"name\": \"D\",\n            \"resolutions\": np.array([1/44, 1/60, 1/88, 1/100]),\n            \"measurements\": np.array([0.6861334, 0.68603858, 0.68600834, 0.686005]),\n            \"target\": 0.686\n        },\n        # Case E: Constraint norm near floor\n        {\n            \"name\": \"E\",\n            \"resolutions\": np.array([1/96, 1/100, 1/120, 1/160]),\n            \"measurements\": np.array([1.05e-6, 1.02e-6, 1.01e-6, 1.00e-6]),\n            \"target\": None\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        h = case[\"resolutions\"]\n        measurements = case[\"measurements\"]\n        target = case[\"target\"]\n\n        # Calculate the error proxy\n        if target is not None:\n            # For apparent horizon properties, error is the absolute deviation from the continuum target\n            error_proxy = np.abs(measurements - target)\n        else:\n            # For constraint norms, the norm itself is the error proxy\n            error_proxy = measurements\n\n        # Gracefully handle non-positive error proxies by filtering them out.\n        # This is a robustness measure; the given data has only positive errors.\n        valid_indices = np.where(error_proxy > 0)\n        \n        h_filtered = h[valid_indices]\n        error_proxy_filtered = error_proxy[valid_indices]\n\n        # A linear fit requires at least 2 points.\n        if len(h_filtered)  2:\n            # Convergence order is undefined with fewer than 2 points.\n            # Storing as NaN (Not a Number) for this edge case.\n            p = np.nan\n        else:\n            # Transform to log-log space for linear fitting\n            log_h = np.log(h_filtered)\n            log_error = np.log(error_proxy_filtered)\n\n            # Perform a linear least-squares fit (degree 1 polynomial)\n            # np.polyfit returns the coefficients [slope, intercept]\n            # The slope is the desired convergence order 'p'.\n            p, _ = np.polyfit(log_h, log_error, 1)\n\n        results.append(p)\n\n    # Format the final output as a comma-separated list of orders,\n    # rounded to three decimal places, enclosed in square brackets.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Numerical relativity simulations compute the spacetime metric on a finite computational domain, whereas gravitational wave observatories measure the strain at a distance that is effectively infinite. To bridge this gap, we must extrapolate the waveform data from the finite radii of the simulation to future null infinity ($R \\to \\infty$). This hands-on exercise  guides you through implementing and testing a standard extrapolation procedure on synthetic data, allowing you to quantify its accuracy and appreciate the subtleties involved in producing a clean gravitational wave signal.",
            "id": "3464669",
            "problem": "You are given a simplified but scientifically consistent model for gravitational-wave strain extraction in Numerical Relativity (NR) from Binary Black Hole (BBH) simulations. At large extraction radius, the strain measured on a coordinate sphere of radius $R$ admits an asymptotic expansion consistent with the leading-order radiative falloff, written schematically as\n$$\nh(t,R) = h_\\infty(t) + \\frac{c(t)}{R} + \\frac{d(t)}{R^2} + \\mathcal{O}\\!\\left(\\frac{1}{R^3}\\right),\n$$\nwhere $h_\\infty(t)$ is the strain at null infinity, $c(t)$ collects subleading radiative and gauge contributions that scale as $1/R$, and $d(t)$ captures higher-order finite-radius effects scaling as $1/R^2$. The goal is to implement a time-local extrapolation scheme that uses only the leading-order model $h(t,R) \\approx h_\\infty(t) + c(t)/R$ and to quantify the extrapolation residuals against known ground truth $h_\\infty(t)$. Use purely mathematical and algorithmic operations to construct strain data and evaluate method performance.\n\nConstruct synthetic, deterministic strain data as follows. Define a dimensionless strain at null infinity\n$$\nh_\\infty(t) = A(t)\\cos\\big(\\phi(t)\\big),\n$$\nwith amplitude $A(t)$ modeling inspiral growth and ringdown decay,\n$$\nA(t) = \\begin{cases}\nA_0 \\big(1 + \\eta t\\big),  0 \\le t \\le t_m, \\\\\nA_0 \\big(1 + \\eta t_m\\big) \\exp\\big(-\\lambda (t - t_m)\\big),  t  t_m,\n\\end{cases}\n$$\nand phase $\\phi(t)$ modeling a chirp,\n$$\n\\phi(t) = \\phi_0 + \\omega_0 t + \\gamma t^2.\n$$\nLet $c(t)$ and $d(t)$ be coherent subleading terms,\n$$\nc(t) = \\alpha_c A(t)\\cos\\big(\\phi(t) + \\delta_c\\big), \\quad d(t) = \\alpha_d A(t)\\cos\\big(\\phi(t) + \\delta_d\\big).\n$$\nFor each extraction radius $R_i$, construct the finite-radius strain time series $h_i(t)$ by\n$$\nh_i(t) = h_\\infty(t) + \\frac{c(t)}{R_i} + \\frac{d(t)}{R_i^2} + \\epsilon \\sin\\big(2\\pi f_i t\\big),\n$$\nwhere $f_i$ is a distinct deterministic frequency per radius and $\\epsilon$ is a deterministic small amplitude. All quantities are dimensionless.\n\nAt each time sample $t_k$, estimate $\\hat{h}_\\infty(t_k)$ and $\\hat{c}(t_k)$ by linear least squares fitting of the model $h(t,R) \\approx h_\\infty(t) + c(t)/R$ to the values $\\{h_i(t_k)\\}$ across all radii $\\{R_i\\}$, ignoring the $1/R^2$ and higher-order terms. Also construct a naive approximation $h_{\\max R}(t)$ by selecting the strain measured at the largest radius.\n\nDefine the following metrics to quantify extrapolation residuals relative to the ground truth $h_\\infty(t)$ on a common uniform time grid $\\{t_k\\}_{k=1}^N$:\n- The relative root-mean-square error,\n$$\n\\mathrm{relL2}(x,y) = \\frac{\\sqrt{\\sum_{k=1}^N \\big(x_k - y_k\\big)^2}}{\\sqrt{\\sum_{k=1}^N x_k^2}}.\n$$\n- The mismatch,\n$$\n\\mathrm{mismatch}(x,y) = 1 - \\frac{\\sum_{k=1}^N x_k y_k}{\\sqrt{\\sum_{k=1}^N x_k^2}\\sqrt{\\sum_{k=1}^N y_k^2}}.\n$$\n- The normalized residual root-mean-square after fitting across all radii and times,\n$$\n\\mathrm{residRMS} = \\frac{\\sqrt{\\sum_{i,k} \\big[h_i(t_k) - \\big(\\hat{h}_\\infty(t_k) + \\hat{c}(t_k)/R_i\\big)\\big]^2}}{\\sqrt{\\sum_k h_\\infty(t_k)^2}}.\n$$\n\nImplement the above using the following fixed, deterministic choices shared across all test cases:\n- Use a uniform time grid $t_k$ with $N = 2048$ samples over the interval $[0,1)$ seconds, i.e., $t_k = k/N$ for $k = 0,1,\\dots,N-1$.\n- Use $A_0 = 1$, $\\eta = 1.5$, $t_m = 0.7$, $\\lambda = 8$, $\\phi_0 = 0$, $\\omega_0 = 50$, $\\gamma = 80$, $\\delta_c = 0.3$, and $\\delta_d = -0.6$.\n- For each radius index $i$ (starting at $i=0$ for the first radius), set $f_i = 3 + i$.\n\nYour program must compute four floats per test case:\n- $\\mathrm{relL2}\\big(h_\\infty, \\hat{h}_\\infty\\big)$,\n- $\\mathrm{relL2}\\big(h_\\infty, h_{\\max R}\\big)$,\n- $\\mathrm{mismatch}\\big(h_\\infty, \\hat{h}_\\infty\\big)$,\n- $\\mathrm{residRMS}$.\n\nDesign a test suite with three cases:\n- Case $1$ (general happy path): $\\{R_i\\} = [100, 150, 200, 300, 400]$, $\\alpha_c = 0.25$, $\\alpha_d = 0.05$, $\\epsilon = 10^{-4}$.\n- Case $2$ (boundary condition with minimal radii and no higher-order terms or noise): $\\{R_i\\} = [100, 300]$, $\\alpha_c = 0.30$, $\\alpha_d = 0$, $\\epsilon = 0$.\n- Case $3$ (edge case with smaller radii, stronger higher-order contamination, and larger deterministic noise): $\\{R_i\\} = [50, 75, 100]$, $\\alpha_c = 0.20$, $\\alpha_d = 0.20$, $\\epsilon = 5\\times 10^{-4}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its four computed floats as a nested list in the same order as specified above. For example, the output format must be of the form $[\\,[r_{11},r_{12},r_{13},r_{14}],\\,[r_{21},r_{22},r_{23},r_{24}],\\,[r_{31},r_{32},r_{33},r_{34}]\\,]$.\n\nAll quantities are dimensionless, and no physical units are required. Angles are in radians. Express all computed numbers as decimal floats. The program must be standalone and must not read input or write files.",
            "solution": "The problem posed is a well-defined computational exercise in numerical data analysis, specifically tailored to a common task in numerical relativity: the extrapolation of gravitational wave signals, computed at finite radii, to null infinity. All provided equations, parameters, and procedures are scientifically consistent with simplified models used in the field, mathematically sound, and computationally tractable. The problem is self-contained and free of ambiguity. Therefore, the problem is deemed valid and a solution will be constructed.\n\nThe core task is to generate synthetic gravitational wave data at several finite extraction radii and then use a linear extrapolation model to estimate the wave at null infinity. The accuracy of this extrapolation is then quantified against the known ground truth.\n\nFirst, we construct the necessary time-domain waveforms based on the provided analytical models. A uniform time grid $\\{t_k\\}_{k=0}^{N-1}$ is defined with $N = 2048$ points over the interval $[0, 1)$, such that $t_k = k/N$.\n\nThe ground truth waveform at null infinity, $h_\\infty(t)$, is defined by an amplitude $A(t)$ and a phase $\\phi(t)$. The amplitude $A(t)$ models the inspiral and ringdown of a binary black hole merger:\n$$\nA(t) = \\begin{cases}\nA_0 \\big(1 + \\eta t\\big),  0 \\le t \\le t_m \\\\\nA_0 \\big(1 + \\eta t_m\\big) \\exp\\big(-\\lambda (t - t_m)\\big),  t  t_m\n\\end{cases}\n$$\nThe phase $\\phi(t)$ models the \"chirp\" of the signal, where the frequency increases over time:\n$$\n\\phi(t) = \\phi_0 + \\omega_0 t + \\gamma t^2\n$$\nUsing the given parameters $A_0 = 1$, $\\eta = 1.5$, $t_m = 0.7$, $\\lambda = 8$, $\\phi_0 = 0$, $\\omega_0 = 50$, and $\\gamma = 80$, we compute the time series for $A(t_k)$, $\\phi(t_k)$, and subsequently the ground truth waveform $h_\\infty(t_k) = A(t_k)\\cos(\\phi(t_k))$.\n\nNext, we generate the subleading contributions, $c(t)$ and $d(t)$, which represent physical and gauge effects that contaminate the signal at finite radius. They are modeled coherently with the primary signal but with phase shifts $\\delta_c$ and $\\delta_d$:\n$$\nc(t) = \\alpha_c A(t)\\cos\\big(\\phi(t) + \\delta_c\\big)\n$$\n$$\nd(t) = \\alpha_d A(t)\\cos\\big(\\phi(t) + \\delta_d\\big)\n$$\nHere, $\\delta_c = 0.3$ and $\\delta_d = -0.6$, while $\\alpha_c$ and $\\alpha_d$ are specific to each test case.\n\nFor each extraction radius $R_i$ in a given set $\\{R_i\\}$, the synthetic strain data $h_i(t)$ is assembled. It includes the ground truth, the subleading terms scaled by powers of $1/R_i$, and a deterministic, high-frequency noise term unique to each radius:\n$$\nh_i(t_k) = h_\\infty(t_k) + \\frac{c(t_k)}{R_i} + \\frac{d(t_k)}{R_i^2} + \\epsilon \\sin\\big(2\\pi f_i t_k\\big)\n$$\nThe noise frequency is $f_i = 3 + i$, where $i$ is the zero-based index of the radius. The parameters $\\epsilon$, $\\{R_i\\}$, $\\alpha_c$, and $\\alpha_d$ are provided for each of the three test cases.\n\nThe next step is the extrapolation procedure. At each time sample $t_k$, we have a set of measurements $\\{h_i(t_k)\\}$ at different radii $\\{R_i\\}$. We fit a linear model of the form $y = a + bx$ to this data, where $y_i = h_i(t_k)$, the independent variable is $x_i = 1/R_i$, and the coefficients to be determined are $a = \\hat{h}_\\infty(t_k)$ and $b = \\hat{c}(t_k)$. This is a linear least squares problem. For a set of $M$ radii, the problem at each time $t_k$ is to find $(\\hat{h}_\\infty, \\hat{c})$ that minimize $\\sum_{i=1}^M [h_i(t_k) - (\\hat{h}_\\infty + \\hat{c}/R_i)]^2$. This is performed for all $k=0, \\dots, N-1$ to obtain the full time series for the extrapolated waveform $\\hat{h}_\\infty(t)$ and the subleading term $\\hat{c}(t)$. Additionally, a naive waveform approximation, $h_{\\max R}(t)$, is taken to be the data from the largest available radius, $h_{\\max R}(t_k) = h_i(t_k)$ where $R_i = \\max(\\{R_j\\})$.\n\nFinally, we compute four metrics to evaluate the performance of the extrapolation.\n1.  The relative L2 error of the extrapolated waveform: $\\mathrm{relL2}\\big(h_\\infty, \\hat{h}_\\infty\\big) = \\frac{\\sqrt{\\sum_{k} (h_\\infty(t_k) - \\hat{h}_\\infty(t_k))^2}}{\\sqrt{\\sum_{k} h_\\infty(t_k)^2}}$. This measures the overall amplitude difference.\n2.  The relative L2 error of the naive waveform: $\\mathrm{relL2}\\big(h_\\infty, h_{\\max R}\\big) = \\frac{\\sqrt{\\sum_{k} (h_\\infty(t_k) - h_{\\max R}(t_k))^2}}{\\sqrt{\\sum_{k} h_\\infty(t_k)^2}}$. This serves as a baseline for comparison.\n3.  The mismatch of the extrapolated waveform: $\\mathrm{mismatch}\\big(h_\\infty, \\hat{h}_\\infty\\big) = 1 - \\frac{\\sum_{k} h_\\infty(t_k) \\hat{h}_\\infty(t_k)}{\\sqrt{\\sum_k h_\\infty(t_k)^2}\\sqrt{\\sum_k \\hat{h}_\\infty(t_k)^2}}$. This is sensitive to phase errors between the signals.\n4.  The normalized residual root-mean-square of the fit: $\\mathrm{residRMS} = \\frac{\\sqrt{\\sum_{i,k} [h_i(t_k) - (\\hat{h}_\\infty(t_k) + \\hat{c}(t_k)/R_i)]^2}}{\\sqrt{\\sum_k h_\\infty(t_k)^2}}$. This quantity measures how well the linear model $h_\\infty+c/R$ describes the full dataset, which includes $1/R^2$ terms and noise. A large value indicates significant model misspecification.\n\nThese computations are performed for each of the three specified test cases, and the four resulting scalar metrics for each case are collected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the gravitational wave extrapolation problem for three test cases.\n    \"\"\"\n\n    # Global fixed parameters\n    N = 2048\n    A0 = 1.0\n    eta = 1.5\n    t_m = 0.7\n    lmbda = 8.0  # 'lambda' is a reserved keyword\n    phi0 = 0.0\n    omega0 = 50.0\n    gamma = 80.0\n    delta_c = 0.3\n    delta_d = -0.6\n\n    # Time grid\n    t = np.arange(N) / N\n\n    # Construct ground truth waveform h_infty(t)\n    # Amplitude A(t)\n    A_t_inspiral = A0 * (1 + eta * t)\n    A_t_ringdown = A0 * (1 + eta * t_m) * np.exp(-lmbda * (t - t_m))\n    A_t = np.where(t = t_m, A_t_inspiral, A_t_ringdown)\n    \n    # Phase phi(t)\n    phi_t = phi0 + omega0 * t + gamma * t**2\n    \n    # Ground truth strain h_infty(t)\n    h_inf_t = A_t * np.cos(phi_t)\n    norm_h_inf = np.linalg.norm(h_inf_t)\n\n    # Test cases\n    test_cases = [\n        {\n            \"R_vals\": np.array([100, 150, 200, 300, 400]),\n            \"alpha_c\": 0.25,\n            \"alpha_d\": 0.05,\n            \"epsilon\": 1e-4,\n        },\n        {\n            \"R_vals\": np.array([100, 300]),\n            \"alpha_c\": 0.30,\n            \"alpha_d\": 0.0,\n            \"epsilon\": 0.0,\n        },\n        {\n            \"R_vals\": np.array([50, 75, 100]),\n            \"alpha_c\": 0.20,\n            \"alpha_d\": 0.20,\n            \"epsilon\": 5e-4,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        R_vals = case[\"R_vals\"]\n        alpha_c = case[\"alpha_c\"]\n        alpha_d = case[\"alpha_d\"]\n        epsilon = case[\"epsilon\"]\n\n        # 1. Generate synthetic strain data for all radii\n        c_t = alpha_c * A_t * np.cos(phi_t + delta_c)\n        d_t = alpha_d * A_t * np.cos(phi_t + delta_d)\n        \n        num_radii = len(R_vals)\n        H_data = np.zeros((num_radii, N))\n\n        for i, R_i in enumerate(R_vals):\n            f_i = 3.0 + i\n            noise_term = epsilon * np.sin(2 * np.pi * f_i * t)\n            H_data[i, :] = h_inf_t + c_t / R_i + d_t / R_i**2 + noise_term\n\n        # 2. Perform extrapolation using linear least squares\n        # The independent variable for the fit is 1/R\n        x_fit = 1 / R_vals\n        \n        # np.polyfit fits for each column of y if y is 2D.\n        # H_data is (num_radii, N), x_fit is (num_radii,).\n        # This will perform N fits, one for each time step.\n        # The result 'coeffs' will have shape (2, N).\n        # coeffs[0, :] will be the slopes (c_hat_t)\n        # coeffs[1, :] will be the intercepts (h_inf_hat_t)\n        coeffs = np.polyfit(x_fit, H_data, 1)\n        c_hat_t = coeffs[0, :]\n        h_inf_hat_t = coeffs[1, :]\n\n        # 3. Get the naive approximation from the largest radius\n        idx_max_R = np.argmax(R_vals)\n        h_maxR_t = H_data[idx_max_R, :]\n\n        # 4. Calculate the performance metrics\n        # Metric 1: relL2(h_infty, h_inf_hat)\n        relL2_h_inf_hat = np.linalg.norm(h_inf_t - h_inf_hat_t) / norm_h_inf\n\n        # Metric 2: relL2(h_infty, h_maxR)\n        relL2_h_maxR = np.linalg.norm(h_inf_t - h_maxR_t) / norm_h_inf\n\n        # Metric 3: mismatch(h_infty, h_inf_hat)\n        norm_h_inf_hat = np.linalg.norm(h_inf_hat_t)\n        inner_product = np.dot(h_inf_t, h_inf_hat_t)\n        mismatch_h_inf_hat = 1.0 - inner_product / (norm_h_inf * norm_h_inf_hat)\n\n        # Metric 4: residRMS\n        # Reconstruct the fitted model values at all radii and times\n        # H_fit[i, k] = h_inf_hat_t[k] + c_hat_t[k] / R_vals[i]\n        H_fit = h_inf_hat_t[np.newaxis, :] + c_hat_t[np.newaxis, :] / R_vals[:, np.newaxis]\n        \n        # Sum of squared residuals over all radii and times\n        total_residual_sq_sum = np.sum((H_data - H_fit)**2)\n        residRMS_val = np.sqrt(total_residual_sq_sum) / norm_h_inf\n        \n        case_results = [\n            relL2_h_inf_hat,\n            relL2_h_maxR,\n            mismatch_h_inf_hat,\n            residRMS_val,\n        ]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format requires nested lists, so we format each sublist.\n    formatted_results = [f\"[{','.join(f'{x:.17g}' for x in res)}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}