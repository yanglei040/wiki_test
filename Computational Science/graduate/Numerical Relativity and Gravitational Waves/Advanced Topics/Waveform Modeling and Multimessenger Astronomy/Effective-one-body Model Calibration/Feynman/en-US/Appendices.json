{
    "hands_on_practices": [
        {
            "introduction": "The Effective-One-Body (EOB) formalism provides a powerful framework for modeling gravitational waves, but its resummed expressions contain free parameters that must be fixed. This process, known as calibration, often begins by ensuring the EOB model's predictions match well-established post-Newtonian (PN) expansions at low velocities. This foundational exercise guides you through calibrating a simple EOB energy flux ansatz by matching it to the known 1.5PN-accurate result, linking basic radiation theory to modern waveform modeling .",
            "id": "3472667",
            "problem": "Consider a non-spinning compact binary with component masses $m_{1}$ and $m_{2}$, total mass $M \\equiv m_{1}+m_{2}$, reduced mass $\\mu \\equiv m_{1} m_{2}/M$, and symmetric mass ratio $\\nu \\equiv \\mu/M \\in (0,1/4]$. The binary is on a quasi-circular orbit of orbital angular frequency $\\Omega$. Work in geometric units with $G=c=1$.\n\n1) Starting from the far-zone transverse-traceless mass quadrupole radiation formula for the gravitational-wave power radiated to infinity,\n$$\n\\mathcal{F} \\equiv -\\frac{dE}{dt} \\;=\\; \\frac{1}{5}\\,\\Big\\langle \\dddot{I}_{ij}\\,\\dddot{I}_{ij} \\Big\\rangle,\n$$\nwhere $I_{ij}$ is the Newtonian trace-free mass quadrupole moment of the source and angle brackets denote an average over several orbital cycles, model the binary as two point masses in Newtonian circular motion and derive the leading-order gravitational-wave energy flux $\\mathcal{F}_{\\mathrm{N}}(\\Omega;M,\\nu)$ at infinity as a function of the orbital frequency $\\Omega$. Use the Newtonian circular-orbit relation and define the gauge-invariant velocity parameter $v \\equiv (M\\Omega)^{1/3}$.\n\n2) In the Effective-One-Body (EOB) model, consider a simple resummed flux ansatz\n$$\n\\mathcal{F}_{\\mathrm{EOB}}(\\Omega;\\alpha,\\beta) \\;=\\; \\mathcal{F}_{\\mathrm{N}}(\\Omega)\\,\\exp\\!\\big(\\alpha\\,v^{2}+\\beta\\,v^{3}\\big),\n$$\nwith constants $(\\alpha,\\beta)$ to be calibrated. Match the Taylor expansion of $\\mathcal{F}_{\\mathrm{EOB}}/\\mathcal{F}_{\\mathrm{N}}$ through relative orders $v^{2}$ and $v^{3}$ to the well-tested post-Newtonian (PN) expansion for non-spinning binaries on circular orbits,\n$$\n\\frac{\\mathcal{F}}{\\mathcal{F}_{\\mathrm{N}}} \\;=\\; 1 + C_{2}(\\nu)\\,v^{2} + C_{3}\\,v^{3} + \\mathcal{O}(v^{4}),\n$$\nwith the known coefficients\n$$\nC_{2}(\\nu) \\;=\\; -\\frac{1247}{336} - \\frac{35}{12}\\,\\nu, \\qquad C_{3} \\;=\\; 4\\pi.\n$$\nDetermine $(\\alpha,\\beta)$ in terms of $(\\nu)$ and known constants.\n\n3) Evaluate your expression for $\\alpha$ at the equal-mass value $\\nu = 1/4$ and report the numerical value rounded to four significant figures. Express your final answer as a pure number (dimensionless).",
            "solution": "### Part 1: Derivation of the Newtonian Gravitational-Wave Flux\n\nWe begin by modeling the compact binary as two point masses, $m_{1}$ and $m_{2}$, in a circular orbit in the $x-y$ plane. The total mass is $M = m_{1}+m_{2}$, the reduced mass is $\\mu = m_{1}m_{2}/M$, and the symmetric mass ratio is $\\nu = \\mu/M$. In the center-of-mass frame, their positions are $\\vec{r}_{1} = (m_{2}/M)\\vec{r}$ and $\\vec{r}_{2} = -(m_{1}/M)\\vec{r}$, where $\\vec{r} = \\vec{r}_{1}-\\vec{r}_{2}$ is the separation vector. For a circular orbit with orbital angular frequency $\\Omega$ and separation distance $r$, the components of $\\vec{r}$ are $x(t) = r\\cos(\\Omega t)$, $y(t) = r\\sin(\\Omega t)$, and $z(t)=0$.\n\nThe mass quadrupole moment tensor is given by $Q_{ij} = \\sum_{A=1,2} m_{A} x_{A,i} x_{A,j}$. Substituting the center-of-mass positions, we find:\n$$\nQ_{ij} = m_{1} \\left(\\frac{m_{2}}{M} x_{i}\\right) \\left(\\frac{m_{2}}{M} x_{j}\\right) + m_{2} \\left(-\\frac{m_{1}}{M} x_{i}\\right) \\left(-\\frac{m_{1}}{M} x_{j}\\right) = \\left( \\frac{m_{1}m_{2}^{2} + m_{2}m_{1}^{2}}{M^{2}} \\right) x_{i}x_{j}\n$$\n$$\nQ_{ij} = \\frac{m_{1}m_{2}(m_{1}+m_{2})}{M^{2}} x_{i}x_{j} = \\frac{\\mu M}{M} x_{i}x_{j} = \\mu x_{i}x_{j}\n$$\nThe problem requires the trace-free mass quadrupole moment, $I_{ij}$. The definition used in the power formula, $\\mathcal{F} = \\frac{1}{5}\\langle \\dddot{I}_{ij}\\dddot{I}_{ij} \\rangle$, refers to the transverse-traceless (TT) projection of the quadrupole moment. However, a standard result simplifies the calculation: the radiated power can be calculated using the trace-free part of the source's quadrupole moment, which is $\\mathcal{I}_{ij} = Q_{ij} - \\frac{1}{3}\\delta_{ij}Q_{kk}$. The gravitational-wave power is $\\mathcal{F} = \\frac{1}{5}\\langle \\dddot{\\mathcal{I}}_{ij}\\dddot{\\mathcal{I}}_{ij} \\rangle$.\nThe trace is $Q_{kk} = \\sum_{k} Q_{kk} = \\mu (x^{2}+y^{2}+z^{2}) = \\mu r^{2}$. For a circular orbit, $r$ is constant, so the trace $Q_{kk}$ is constant. Consequently, its third time derivative vanishes, $\\dddot{Q}_{kk}=0$. This implies $\\dddot{\\mathcal{I}}_{ij} = \\dddot{Q}_{ij}$. We can therefore compute $\\langle \\dddot{Q}_{ij}\\dddot{Q}_{ij} \\rangle$.\n\nThe components of $Q_{ij}$ are:\n$$\nQ_{xx} = \\mu r^{2} \\cos^{2}(\\Omega t) = \\frac{1}{2}\\mu r^{2}(1 + \\cos(2\\Omega t))\n$$\n$$\nQ_{yy} = \\mu r^{2} \\sin^{2}(\\Omega t) = \\frac{1}{2}\\mu r^{2}(1 - \\cos(2\\Omega t))\n$$\n$$\nQ_{xy} = \\mu r^{2} \\cos(\\Omega t)\\sin(\\Omega t) = \\frac{1}{2}\\mu r^{2}\\sin(2\\Omega t)\n$$\nAll other components ($Q_{zz}$, $Q_{xz}$, $Q_{yz}$) are zero. Taking the third time derivative:\n$$\n\\dddot{Q}_{xx} = \\frac{1}{2}\\mu r^{2} \\frac{d^{3}}{dt^{3}}\\cos(2\\Omega t) = \\frac{1}{2}\\mu r^{2} (2\\Omega)^{3}\\sin(2\\Omega t) = 4\\mu r^{2}\\Omega^{3}\\sin(2\\Omega t)\n$$\n$$\n\\dddot{Q}_{yy} = -\\frac{1}{2}\\mu r^{2} \\frac{d^{3}}{dt^{3}}\\cos(2\\Omega t) = -4\\mu r^{2}\\Omega^{3}\\sin(2\\Omega t)\n$$\n$$\n\\dddot{Q}_{xy} = \\frac{1}{2}\\mu r^{2} \\frac{d^{3}}{dt^{3}}\\sin(2\\Omega t) = \\frac{1}{2}\\mu r^{2} (-(2\\Omega)^{3})\\cos(2\\Omega t) = -4\\mu r^{2}\\Omega^{3}\\cos(2\\Omega t)\n$$\nThe sum of squares is $\\dddot{Q}_{ij}\\dddot{Q}_{ij} = \\sum_{i,j=1}^{3} (\\dddot{Q}_{ij})^{2} = (\\dddot{Q}_{xx})^{2} + (\\dddot{Q}_{yy})^{2} + 2(\\dddot{Q}_{xy})^{2}$:\n$$\n\\dddot{Q}_{ij}\\dddot{Q}_{ij} = (16\\mu^{2}r^{4}\\Omega^{6}\\sin^{2}(2\\Omega t)) + (16\\mu^{2}r^{4}\\Omega^{6}\\sin^{2}(2\\Omega t)) + 2(16\\mu^{2}r^{4}\\Omega^{6}\\cos^{2}(2\\Omega t))\n$$\n$$\n= 32\\mu^{2}r^{4}\\Omega^{6}(\\sin^{2}(2\\Omega t) + \\cos^{2}(2\\Omega t)) = 32\\mu^{2}r^{4}\\Omega^{6}\n$$\nThis result is constant, so its time average is the same value. The radiated power is:\n$$\n\\mathcal{F}_{\\mathrm{N}} = \\frac{1}{5} \\langle \\dddot{I}_{ij}\\dddot{I}_{ij} \\rangle = \\frac{1}{5} (32\\mu^{2}r^{4}\\Omega^{6}) = \\frac{32}{5}\\mu^{2}r^{4}\\Omega^{6}\n$$\nTo express this in terms of $M$, $\\nu$, and $\\Omega$, we use Kepler's third law for a Newtonian circular orbit. In geometric units ($G=1$), the balance of gravitational and centripetal forces gives $\\mu \\Omega^{2} r = M\\mu/r^{2}$, which simplifies to $\\Omega^{2} r^{3} = M$. Thus, $r = (M/\\Omega^{2})^{1/3}$.\nSubstituting this into the expression for the flux:\n$$\n\\mathcal{F}_{\\mathrm{N}} = \\frac{32}{5}\\mu^{2}\\left(\\frac{M}{\\Omega^{2}}\\right)^{4/3}\\Omega^{6} = \\frac{32}{5}\\mu^{2}M^{4/3}\\Omega^{-8/3}\\Omega^{6} = \\frac{32}{5}\\mu^{2}M^{4/3}\\Omega^{10/3}\n$$\nFinally, using $\\mu = \\nu M$, we obtain the desired expression for the Newtonian flux:\n$$\n\\mathcal{F}_{\\mathrm{N}}(\\Omega; M, \\nu) = \\frac{32}{5}(\\nu M)^{2}M^{4/3}\\Omega^{10/3} = \\frac{32}{5}\\nu^{2}M^{10/3}\\Omega^{10/3} = \\frac{32}{5}\\nu^{2}(M\\Omega)^{10/3}\n$$\n\n### Part 2: Calibration of EOB Parameters\n\nWe are given the EOB flux ansatz $\\mathcal{F}_{\\mathrm{EOB}} = \\mathcal{F}_{\\mathrm{N}}\\,\\exp(\\alpha\\,v^{2}+\\beta\\,v^{3})$ and the post-Newtonian (PN) expansion $\\mathcal{F}/\\mathcal{F}_{\\mathrm{N}} = 1 + C_{2}(\\nu)\\,v^{2} + C_{3}\\,v^{3} + \\mathcal{O}(v^{4})$. The task is to determine the calibration parameters $(\\alpha, \\beta)$ by matching the Taylor expansion of the EOB model to the PN expansion through orders $v^2$ and $v^3$.\n\nWe expand the exponential in the EOB flux ratio:\n$$\n\\frac{\\mathcal{F}_{\\mathrm{EOB}}}{\\mathcal{F}_{\\mathrm{N}}} = \\exp(\\alpha\\,v^{2}+\\beta\\,v^{3})\n$$\nUsing the Taylor series for the exponential function, $\\exp(x) = 1 + x + x^{2}/2! + \\dots$, with $x = \\alpha v^{2} + \\beta v^{3}$:\n$$\n\\exp(\\alpha\\,v^{2}+\\beta\\,v^{3}) = 1 + (\\alpha v^{2} + \\beta v^{3}) + \\frac{1}{2}(\\alpha v^{2} + \\beta v^{3})^{2} + \\mathcal{O}(v^{6})\n$$\nExpanding and keeping terms up to order $v^{3}$:\n$$\n= 1 + \\alpha v^{2} + \\beta v^{3} + \\mathcal{O}(v^{4})\n$$\nWe now equate the coefficients of this expansion with those of the given PN expansion term by term.\n$$\n1 + \\alpha v^{2} + \\beta v^{3} + \\mathcal{O}(v^{4}) = 1 + C_{2}(\\nu)\\,v^{2} + C_{3}\\,v^{3} + \\mathcal{O}(v^{4})\n$$\nBy comparing the coefficients for the powers of $v$:\n- For $v^{2}$: $\\alpha = C_{2}(\\nu)$\n- For $v^{3}$: $\\beta = C_{3}$\n\nSubstituting the provided expressions for $C_{2}(\\nu)$ and $C_{3}$:\n$$\n\\alpha = -\\frac{1247}{336} - \\frac{35}{12}\\,\\nu\n$$\n$$\n\\beta = 4\\pi\n$$\nThese are the expressions for the calibration parameters $(\\alpha, \\beta)$.\n\n### Part 3: Numerical Evaluation of $\\alpha$\n\nWe need to evaluate the expression for $\\alpha$ at the equal-mass point, where $\\nu = 1/4$.\n$$\n\\alpha\\left(\\nu=\\frac{1}{4}\\right) = -\\frac{1247}{336} - \\frac{35}{12}\\left(\\frac{1}{4}\\right)\n$$\n$$\n= -\\frac{1247}{336} - \\frac{35}{48}\n$$\nTo combine the fractions, we find a common denominator, which is $336$ since $336 = 7 \\times 48$.\n$$\n\\alpha\\left(\\frac{1}{4}\\right) = -\\frac{1247}{336} - \\frac{35 \\times 7}{48 \\times 7} = -\\frac{1247}{336} - \\frac{245}{336}\n$$\n$$\n= -\\frac{1247 + 245}{336} = -\\frac{1492}{336}\n$$\nWe simplify the fraction by dividing the numerator and denominator by their greatest common divisor, which is $4$.\n$$\n\\alpha\\left(\\frac{1}{4}\\right) = -\\frac{1492 \\div 4}{336 \\div 4} = -\\frac{373}{84}\n$$\nFinally, we compute the numerical value and round to four significant figures:\n$$\n\\alpha\\left(\\frac{1}{4}\\right) = -\\frac{373}{84} \\approx -4.44047619...\n$$\nRounding to four significant figures gives $-4.440$.",
            "answer": "$$\\boxed{-4.440}$$"
        },
        {
            "introduction": "A key principle of the EOB framework is that it must reproduce the exact solutions of General Relativity in the appropriate physical limits. This practice demonstrates how to enforce such a condition by requiring the EOB effective potential to correctly predict the location of the Innermost Stable Circular Orbit (ISCO) in the test-mass limit, where the dynamics reduce to that of a Schwarzschild spacetime. By applying this physical constraint, you will derive a relationship between the model's calibration coefficients, a crucial step in building a physically consistent EOB potential .",
            "id": "3472716",
            "problem": "Consider the nonspinning Effective-One-Body (EOB) potential $A(u;\\nu)$ for binary black holes, where $u \\equiv M/r$ and $\\nu$ is the symmetric mass ratio. In the test-mass limit $\\nu \\to 0$, consistency with geodesic motion in a Schwarzschild spacetime requires that the circular-orbit structure be reproduced. Suppose the EOB potential in the test-mass limit is modeled as a small deformation of the Schwarzschild potential by introducing calibration coefficients,\n$$\nA(u;\\nu{=}0) = 1 - 2 u + \\alpha\\, u^{3} + \\beta\\, u^{4} + \\gamma\\, u^{5} + \\mathcal{O}(u^{6}),\n$$\nwith $|\\alpha|,|\\beta|,|\\gamma|$ assumed small, and where $\\alpha,\\beta,\\gamma$ are independent of $u$.\n\nUsing the condition for circular equatorial geodesics in a spherically symmetric effective metric and the marginal stability condition that defines the innermost stable circular orbit (ISCO), enforce that in the test-mass limit the ISCO occurs at the Schwarzschild value $u_{\\mathrm{ISCO}} = 1/6$. Work consistently to linear order in the small calibration coefficients $\\alpha,\\beta,\\gamma$.\n\nDerive the linear restriction imposed by $u_{\\mathrm{ISCO}} = 1/6$ on $(\\alpha,\\beta,\\gamma)$, and then express $\\alpha$ as a function of $\\beta$ and $\\gamma$. Your final answer must be a single closed-form analytic expression for $\\alpha(\\beta,\\gamma)$ with no units. Do not introduce any numerical approximations; provide the exact rational coefficients.",
            "solution": "### Step 1: Formalism for Circular Orbits and ISCO\n\nFor a test particle in a static, spherically symmetric spacetime, the radial motion is governed by an effective potential. The EOB formalism models the dynamics using an effective metric whose $g_{tt}$ component is given by $-A(r)$. The square of the effective potential for a particle with specific angular momentum $L$ is $V_{\\mathrm{eff}}^2(r) = A(r) \\left(1 + \\frac{L^2}{r^2}\\right)$.\n\nCircular orbits exist at radii $r$ where the effective potential is extremal, i.e., $\\frac{d(V_{\\mathrm{eff}}^2)}{dr} = 0$.\nThe ISCO corresponds to the marginal stability condition, where the orbit is at an inflection point of the effective potential, i.e., $\\frac{d^2(V_{\\mathrm{eff}}^2)}{dr^2} = 0$.\n\nA combined condition for the ISCO radius $r_{\\mathrm{ISCO}}$ can be derived by solving these two equations simultaneously, which eliminates the specific angular momentum $L$. This yields the following differential equation for $A(r)$ which must be satisfied at $r=r_{\\mathrm{ISCO}}$:\n$$\n2r (A'(r))^2 - 3A(r)A'(r) - rA(r)A''(r) = 0\n$$\nwhere primes denote derivatives with respect to $r$.\n\n### Step 2: Transforming the ISCO Condition to the Variable $u$\n\nThe problem is formulated in terms of the variable $u \\equiv M/r$, where $M$ is the total mass. We must transform the ISCO condition into an equation in terms of $u$. The derivatives with respect to $r$ can be expressed in terms of derivatives with respect to $u$ using the chain rule:\n$r = M/u$\n$\\frac{d}{dr} = \\frac{du}{dr} \\frac{d}{du} = \\left(-\\frac{M}{r^2}\\right) \\frac{d}{du} = -\\frac{u^2}{M} \\frac{d}{du}$\n\nLet $A_u \\equiv \\frac{dA}{du}$ and $A_{uu} \\equiv \\frac{d^2A}{du^2}$.\nThe first and second derivatives of $A$ with respect to $r$ are:\n$A'(r) = -\\frac{u^2}{M} A_u$\n$A''(r) = \\frac{d}{dr} \\left(-\\frac{u^2}{M} A_u\\right) = -\\frac{u^2}{M} \\frac{d}{du} \\left(-\\frac{u^2}{M} A_u\\right) = \\frac{u^2}{M} \\left(\\frac{2u}{M} A_u + \\frac{u^2}{M} A_{uu}\\right) = \\frac{2u^3}{M^2} A_u + \\frac{u^4}{M^2} A_{uu}$\n\nSubstituting these into the ISCO condition:\n$$\n2\\left(\\frac{M}{u}\\right) \\left(-\\frac{u^2}{M} A_u\\right)^2 - 3A \\left(-\\frac{u^2}{M} A_u\\right) - \\left(\\frac{M}{u}\\right) A \\left(\\frac{2u^3}{M^2} A_u + \\frac{u^4}{M^2} A_{uu}\\right) = 0\n$$\nSimplifying the terms:\n$$\n2\\frac{M}{u} \\frac{u^4}{M^2} A_u^2 + 3A \\frac{u^2}{M} A_u - \\frac{M}{u} A \\frac{u^3}{M^2} (2A_u + u A_{uu}) = 0\n$$\n$$\n\\frac{2u^3}{M} A_u^2 + \\frac{3u^2}{M} A A_u - \\frac{u^2}{M} A (2A_u + u A_{uu}) = 0\n$$\nMultiplying by $M/u^2$ (for $u \\neq 0$):\n$$\n2u A_u^2 + 3A A_u - A (2A_u + u A_{uu}) = 0\n$$\nThis simplifies to the final ISCO condition in terms of $u$:\n$$\nC(u) \\equiv 2u (A_u)^2 + A A_u - u A A_{uu} = 0\n$$\n\n### Step 3: Applying the Condition to the EOB Potential\n\nThe given EOB potential in the test-mass limit is:\n$$\nA(u) = 1 - 2u + \\alpha u^3 + \\beta u^4 + \\gamma u^5 + \\mathcal{O}(u^6)\n$$\nWe work to linear order in the small coefficients $\\alpha$, $\\beta$, $\\gamma$. Let's represent $A(u)$ as the sum of the Schwarzschild part $A_S(u) = 1-2u$ and a small perturbation $\\delta A(u) = \\alpha u^3 + \\beta u^4 + \\gamma u^5$.\n\nThe derivatives are:\n$A_u = -2 + 3\\alpha u^2 + 4\\beta u^3 + 5\\gamma u^4 \\equiv A_{S,u} + \\delta A_u$\n$A_{uu} = 6\\alpha u + 12\\beta u^2 + 20\\gamma u^3 \\equiv A_{S,uu} + \\delta A_{uu} = \\delta A_{uu}$ (since $A_{S,uu}=0$).\n\nWe substitute these into the ISCO condition $C(u)=0$ and linearize.\n$A_u^2 = (A_{S,u} + \\delta A_u)^2 = A_{S,u}^2 + 2A_{S,u}\\delta A_u + \\mathcal{O}(\\alpha^2, \\dots) \\approx (-2)^2 + 2(-2)\\delta A_u = 4 - 4\\delta A_u$\n$A A_u = (A_S + \\delta A)(A_{S,u} + \\delta A_u) \\approx A_S A_{S,u} + A_S\\delta A_u + \\delta A A_{S,u}$\n$uAA_{uu} = u(A_S + \\delta A)(\\delta A_{uu}) \\approx u A_S \\delta A_{uu}$\n\nThe ISCO condition $C(u)=0$ becomes:\n$$\n2u(4 - 4\\delta A_u) + (A_S(-2) + A_S\\delta A_u + \\delta A(-2)) - u A_S \\delta A_{uu} = 0\n$$\nGrouping terms of zeroth and first order in $(\\alpha, \\beta, \\gamma)$:\nZeroth order part, $C_0(u)$:\n$$\nC_0(u) = 8u - 2A_S = 8u - 2(1 - 2u) = 12u - 2\n$$\nFor the Schwarzschild spacetime, the ISCO is at $C_0(u)=0$, which gives $12u-2=0 \\implies u=1/6$. This is correct.\n\nFirst order part, $\\delta C(u)$:\n$$\n\\delta C(u) = -8u \\delta A_u + A_S \\delta A_u - 2\\delta A - u A_S \\delta A_{uu} = 0\n$$\n$$\n\\delta C(u) = (A_S - 8u)\\delta A_u - 2\\delta A - uA_S\\delta A_{uu} = 0\n$$\nWe must enforce this condition at the Schwarzschild ISCO location, $u_0 = 1/6$.\nAt $u_0=1/6$:\n$A_S(u_0) = 1 - 2(1/6) = 1 - 1/3 = 2/3$.\n$A_S(u_0) - 8u_0 = 2/3 - 8/6 = 2/3 - 4/3 = -2/3$.\n\nThe condition becomes:\n$$\n-\\frac{2}{3}\\delta A_u(u_0) - 2\\delta A(u_0) - u_0\\left(\\frac{2}{3}\\right)\\delta A_{uu}(u_0) = 0\n$$\nSubstituting the expressions for the perturbations:\n$\\delta A(u_0) = \\alpha u_0^3 + \\beta u_0^4 + \\gamma u_0^5$\n$\\delta A_u(u_0) = 3\\alpha u_0^2 + 4\\beta u_0^3 + 5\\gamma u_0^4$\n$\\delta A_{uu}(u_0) = 6\\alpha u_0 + 12\\beta u_0^2 + 20\\gamma u_0^3$\n\nWe collect coefficients for $\\alpha, \\beta, \\gamma$.\n\nCoefficient of $\\alpha$:\n$$\n(A_S-8u)(3u^2) - 2(u^3) - u A_S (6u) \\Big|_{u_0=1/6} = \\left(-\\frac{2}{3}\\right)3\\left(\\frac{1}{6}\\right)^2 - 2\\left(\\frac{1}{6}\\right)^3 - \\left(\\frac{1}{6}\\right)\\left(\\frac{2}{3}\\right)6\\left(\\frac{1}{6}\\right) = -2\\left(\\frac{1}{36}\\right) - 2\\left(\\frac{1}{216}\\right) - \\left(\\frac{1}{9}\\right) = -\\frac{1}{18} - \\frac{1}{108} - \\frac{1}{9} = -\\frac{6}{108} - \\frac{1}{108} - \\frac{12}{108} = -\\frac{19}{108}\n$$\nCoefficient of $\\beta$:\n$$\n(A_S-8u)(4u^3) - 2(u^4) - u A_S (12u^2) \\Big|_{u_0=1/6} = \\left(-\\frac{2}{3}\\right)4\\left(\\frac{1}{6}\\right)^3 - 2\\left(\\frac{1}{6}\\right)^4 - \\left(\\frac{1}{9}\\right)12\\left(\\frac{1}{6}\\right)^2 = -\\frac{8}{3}\\frac{1}{216} - \\frac{2}{1296} - \\frac{12}{9 \\times 36} = -\\frac{8}{648} - \\frac{2}{1296} - \\frac{1}{27} = -\\frac{16}{1296} - \\frac{2}{1296} - \\frac{48}{1296} = -\\frac{66}{1296} = -\\frac{11}{216}\n$$\nCoefficient of $\\gamma$:\n$$\n(A_S-8u)(5u^4) - 2(u^5) - u A_S (20u^3) \\Big|_{u_0=1/6} = \\left(-\\frac{2}{3}\\right)5\\left(\\frac{1}{6}\\right)^4 - 2\\left(\\frac{1}{6}\\right)^5 - \\left(\\frac{1}{9}\\right)20\\left(\\frac{1}{6}\\right)^3 = -\\frac{10}{3}\\frac{1}{1296} - \\frac{2}{7776} - \\frac{20}{9 \\times 216} = -\\frac{10}{3888} - \\frac{2}{7776} - \\frac{20}{1944} = -\\frac{20}{7776} - \\frac{2}{7776} - \\frac{80}{7776} = -\\frac{102}{7776} = -\\frac{17}{1296}\n$$\n\n### Step 4: Final Derivation\n\nThe linear restriction on $(\\alpha, \\beta, \\gamma)$ is:\n$$\n\\alpha \\left(-\\frac{19}{108}\\right) + \\beta \\left(-\\frac{11}{216}\\right) + \\gamma \\left(-\\frac{17}{1296}\\right) = 0\n$$\nMultiplying the entire equation by $-1296$ to clear denominators:\n$1296 = 12 \\times 108 = 6 \\times 216$.\n$$\n\\alpha \\left(\\frac{19}{108} \\times 1296\\right) + \\beta \\left(\\frac{11}{216} \\times 1296\\right) + \\gamma \\left(\\frac{17}{1296} \\times 1296\\right) = 0\n$$\n$$\n\\alpha(19 \\times 12) + \\beta(11 \\times 6) + \\gamma(17) = 0\n$$\n$$\n228\\alpha + 66\\beta + 17\\gamma = 0\n$$\nSolving for $\\alpha$ as a function of $\\beta$ and $\\gamma$:\n$$\n228\\alpha = -66\\beta - 17\\gamma\n$$\n$$\n\\alpha = -\\frac{66}{228}\\beta - \\frac{17}{228}\\gamma\n$$\nSimplifying the rational coefficients:\n$$\n\\frac{66}{228} = \\frac{33}{114} = \\frac{11}{38}\n$$\nThe fraction $\\frac{17}{228}$ is irreducible because $17$ is prime and $228 = 2^2 \\times 3 \\times 19$.\nTherefore, the final expression for $\\alpha$ is:\n$$\n\\alpha = -\\frac{11}{38}\\beta - \\frac{17}{228}\\gamma\n$$",
            "answer": "$$\n\\boxed{-\\frac{11}{38}\\beta - \\frac{17}{228}\\gamma}\n$$"
        },
        {
            "introduction": "Moving from theoretical principles to practical application, EOB models are ultimately calibrated against high-fidelity Numerical Relativity (NR) simulations. This exercise delves into the statistical heart of this process, highlighting the importance of accurately modeling NR errors. You will implement and compare two Bayesian fitting approaches—one assuming uniform (homoscedastic) noise and another using more realistic, non-uniform (heteroscedastic) noise—to see how a proper statistical treatment directly impacts the inferred EOB parameter values .",
            "id": "3472659",
            "problem": "You are given a calibration task for the Effective-One-Body (EOB) model in the context of Numerical Relativity (NR) and gravitational waves. The goal is to calibrate small offsets in EOB parameters by fitting NR-informed residuals under a linearized model and to examine how incorporating NR error bars as heteroscedastic noise in the likelihood changes the posterior for the EOB parameters relative to homoscedastic fits.\n\nAssume the following calibration setup. Let $n$ be the number of NR-informed residual data points, and let $k$ be the number of EOB parameter offsets being calibrated. Let $\\boldsymbol{\\theta} \\in \\mathbb{R}^k$ denote the small parameter offsets relative to a reference EOB model, and let $\\mathbf{y} \\in \\mathbb{R}^n$ denote the residuals measuring the discrepancy between the EOB reference and NR-derived summary quantities (e.g., phasing differences or mismatches aggregated across configurations). Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times k}$ be the design matrix constructed from linearized sensitivities of the residuals to $\\boldsymbol{\\theta}$ around the reference, so that the linearized model is\n$$\n\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\boldsymbol{\\varepsilon}$ is a random noise term.\n\nConsider two likelihood formulations for $\\boldsymbol{\\varepsilon}$:\n- Heteroscedastic Gaussian noise informed by NR error bars, $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ with diagonal covariance $\\boldsymbol{\\Sigma} = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$.\n- Homoscedastic Gaussian noise with constant variance, $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$.\n\nAssume a Gaussian prior on the parameters, $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)$, with $\\boldsymbol{\\Sigma}_0$ symmetric positive definite.\n\nStarting from Bayes’ theorem and the linear Gaussian model assumptions above, derive the posterior distributions $p(\\boldsymbol{\\theta} \\mid \\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)$ for the heteroscedastic case and $p(\\boldsymbol{\\theta} \\mid \\mathbf{y}, \\mathbf{X}, \\sigma^2, \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)$ for the homoscedastic case. Then implement a program that, for each test case provided below, computes the following three quantitative comparisons between the heteroscedastic and homoscedastic posteriors:\n1. The Euclidean norm of the difference between the posterior means, i.e., $||\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}}||_2$.\n2. The ratio of the determinants of the posterior covariance matrices, i.e., $\\det(\\boldsymbol{\\Sigma}_{\\text{het}})/\\det(\\boldsymbol{\\Sigma}_{\\text{hom}})$.\n3. The symmetrized Kullback–Leibler divergence between the two multivariate normal posteriors, i.e., $\\mathrm{KL}(\\mathcal{N}_{\\text{het}} \\,\\|\\, \\mathcal{N}_{\\text{hom}}) + \\mathrm{KL}(\\mathcal{N}_{\\text{hom}} \\,\\|\\, \\mathcal{N}_{\\text{het}})$.\n\nAll quantities are dimensionless, and no physical units are required. Angles do not appear. The final outputs must be decimal floats.\n\nUse the following test suite. Each test case is specified by $(\\mathbf{X}, \\mathbf{y}, \\boldsymbol{\\sigma}^2, \\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0, \\sigma^2)$, where $\\boldsymbol{\\sigma}^2$ denotes the vector of heteroscedastic variances and $\\sigma^2$ is the homoscedastic variance.\n\n- Test Case 1 (happy path, moderate heteroscedasticity):\n  - $n=6, k=2$,\n  - $\\mathbf{X} = \\begin{bmatrix} 0.8 & -0.1 \\\\ 1.2 & 0.0 \\\\ 0.5 & 0.3 \\\\ 1.5 & -0.2 \\\\ 0.9 & 0.4 \\\\ 1.1 & -0.3 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} -0.020 \\\\ 0.010 \\\\ 0.005 \\\\ -0.030 \\\\ 0.012 \\\\ -0.015 \\end{bmatrix}$,\n  - $\\boldsymbol{\\sigma}^2 = \\begin{bmatrix} 1.0\\times 10^{-4} \\\\ 2.0\\times 10^{-4} \\\\ 1.5\\times 10^{-4} \\\\ 1.0\\times 10^{-4} \\\\ 2.5\\times 10^{-4} \\\\ 1.8\\times 10^{-4} \\end{bmatrix}$,\n  - $\\boldsymbol{\\mu}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(0.05^2, 0.05^2)$,\n  - $\\sigma^2 = \\mathrm{mean}(\\boldsymbol{\\sigma}^2)$.\n- Test Case 2 (boundary condition, nearly homoscedastic):\n  - $n=5, k=2$,\n  - $\\mathbf{X} = \\begin{bmatrix} 0.7 & 0.2 \\\\ 0.9 & -0.1 \\\\ 1.1 & 0.0 \\\\ 1.3 & 0.3 \\\\ 0.8 & -0.2 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.006 \\\\ -0.012 \\\\ 0.008 \\\\ 0.002 \\\\ -0.009 \\end{bmatrix}$,\n  - $\\boldsymbol{\\sigma}^2 = \\begin{bmatrix} 2.0\\times 10^{-4} \\\\ 2.0\\times 10^{-4} \\\\ 2.0\\times 10^{-4} \\\\ 2.0\\times 10^{-4} \\\\ 2.0\\times 10^{-4} \\end{bmatrix}$,\n  - $\\boldsymbol{\\mu}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(0.05^2, 0.05^2)$,\n  - $\\sigma^2 = 2.0\\times 10^{-4}$.\n- Test Case 3 (edge case, one extremely precise datum):\n  - $n=6, k=2$,\n  - $\\mathbf{X} = \\begin{bmatrix} 1.4 & 0.1 \\\\ 0.6 & -0.2 \\\\ 1.0 & 0.2 \\\\ 0.7 & 0.0 \\\\ 1.2 & -0.3 \\\\ 0.9 & 0.4 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} 0.015 \\\\ -0.010 \\\\ 0.005 \\\\ 0.000 \\\\ -0.020 \\\\ 0.010 \\end{bmatrix}$,\n  - $\\boldsymbol{\\sigma}^2 = \\begin{bmatrix} 1.0\\times 10^{-6} \\\\ 3.0\\times 10^{-4} \\\\ 3.0\\times 10^{-4} \\\\ 3.0\\times 10^{-4} \\\\ 3.0\\times 10^{-4} \\\\ 3.0\\times 10^{-4} \\end{bmatrix}$,\n  - $\\boldsymbol{\\mu}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(0.05^2, 0.05^2)$,\n  - $\\sigma^2 = \\mathrm{mean}(\\boldsymbol{\\sigma}^2)$.\n- Test Case 4 (edge case, two highly uncertain data points):\n  - $n=6, k=2$,\n  - $\\mathbf{X} = \\begin{bmatrix} 0.5 & 0.5 \\\\ 1.0 & -0.4 \\\\ 0.8 & 0.1 \\\\ 1.3 & -0.1 \\\\ 0.6 & 0.3 \\\\ 1.1 & -0.2 \\end{bmatrix}$,\n  - $\\mathbf{y} = \\begin{bmatrix} -0.005 \\\\ 0.020 \\\\ -0.010 \\\\ 0.015 \\\\ -0.004 \\\\ 0.008 \\end{bmatrix}$,\n  - $\\boldsymbol{\\sigma}^2 = \\begin{bmatrix} 2.0\\times 10^{-4} \\\\ 1.0\\times 10^{-2} \\\\ 2.0\\times 10^{-4} \\\\ 1.0\\times 10^{-2} \\\\ 2.5\\times 10^{-4} \\\\ 2.0\\times 10^{-4} \\end{bmatrix}$,\n  - $\\boldsymbol{\\mu}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(0.05^2, 0.05^2)$,\n  - $\\sigma^2 = \\mathrm{mean}(\\boldsymbol{\\sigma}^2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output three decimal floats in this order: $||\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}}||_2$, $\\det(\\boldsymbol{\\Sigma}_{\\text{het}})/\\det(\\boldsymbol{\\Sigma}_{\\text{hom}})$, and the symmetrized Kullback–Leibler divergence. Concatenate these triplets across the four test cases into one flat list, resulting in twelve floats in total. For example, the final output format must be\n$$\n[\\text{case1\\_norm},\\text{case1\\_det\\_ratio},\\text{case1\\_KL\\_sym},\\text{case2\\_norm},\\dots,\\text{case4\\_KL\\_sym}].\n$$",
            "solution": "The objective is to derive the posterior distributions for the Effective-One-Body (EOB) parameter offsets $\\boldsymbol{\\theta}$ under two different noise models (heteroscedastic and homoscedastic) and then to compute quantitative metrics comparing these two posteriors. The derivation begins with Bayes' theorem, which states that the posterior probability of the parameters given the data is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\theta} \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n$$\nThe problem defines a linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon}$, a Gaussian likelihood for the noise term $\\boldsymbol{\\varepsilon}$, and a Gaussian prior for the parameters $\\boldsymbol{\\theta}$. Since the conjugate prior for a Gaussian likelihood is a Gaussian distribution, the resulting posterior will also be a Gaussian distribution, $\\boldsymbol{\\theta} \\mid \\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{post}}, \\boldsymbol{\\Sigma}_{\\text{post}})$. We can determine the posterior mean $\\boldsymbol{\\mu}_{\\text{post}}$ and covariance $\\boldsymbol{\\Sigma}_{\\text{post}}$ by analyzing the exponent of the posterior probability density function.\n\nThe prior distribution for $\\boldsymbol{\\theta} \\in \\mathbb{R}^k$ is given as $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)$, so its probability density function is:\n$$\np(\\boldsymbol{\\theta}) \\propto \\exp\\left( -\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0) \\right)\n$$\nThe likelihood function $p(\\mathbf{y} \\mid \\boldsymbol{\\theta})$ depends on the model for the noise $\\boldsymbol{\\varepsilon}$. In general, for $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C})$, the likelihood is:\n$$\np(\\mathbf{y} \\mid \\boldsymbol{\\theta}) \\propto \\exp\\left( -\\frac{1}{2} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}) \\right)\n$$\nThe log-posterior is therefore proportional to the sum of the exponents:\n$$\n\\log p(\\boldsymbol{\\theta} \\mid \\mathbf{y}) = -\\frac{1}{2} \\left[ (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^T \\mathbf{C}^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}) + (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_0) \\right] + \\text{const.}\n$$\nWe expand the quadratic forms and group terms involving $\\boldsymbol{\\theta}$:\n$$\n\\begin{aligned}\n\\text{Exponent} &= - \\frac{1}{2} \\left[ \\mathbf{y}^T\\mathbf{C}^{-1}\\mathbf{y} - 2\\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{y} + \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{X}\\boldsymbol{\\theta} \\right. \\\\\n& \\qquad \\left. + \\boldsymbol{\\theta}^T\\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\theta} - 2\\boldsymbol{\\theta}^T\\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0 + \\boldsymbol{\\mu}_0^T\\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0 \\right] \\\\\n&= - \\frac{1}{2} \\left[ \\boldsymbol{\\theta}^T(\\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1})\\boldsymbol{\\theta} - 2\\boldsymbol{\\theta}^T(\\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{y} + \\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0) \\right] + \\text{const.}\n\\end{aligned}\n$$\nBy completing the square, we can match this to the exponent of a multivariate Gaussian distribution $\\mathcal{N}(\\boldsymbol{\\mu}_{\\text{post}}, \\boldsymbol{\\Sigma}_{\\text{post}})$, which has the form $-\\frac{1}{2}(\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_{\\text{post}})^T \\boldsymbol{\\Sigma}_{\\text{post}}^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\mu}_{\\text{post}})$. Comparing the quadratic and linear terms in $\\boldsymbol{\\theta}$, we identify the posterior precision matrix $\\boldsymbol{\\Sigma}_{\\text{post}}^{-1}$ and mean $\\boldsymbol{\\mu}_{\\text{post}}$.\n\nThe posterior precision matrix is:\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1}\n$$\nThe posterior covariance matrix is its inverse:\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}} = (\\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1})^{-1}\n$$\nThe posterior mean is found from $\\boldsymbol{\\Sigma}_{\\text{post}}^{-1}\\boldsymbol{\\mu}_{\\text{post}} = \\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{y} + \\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0$, which gives:\n$$\n\\boldsymbol{\\mu}_{\\text{post}} = \\boldsymbol{\\Sigma}_{\\text{post}} (\\mathbf{X}^T\\mathbf{C}^{-1}\\mathbf{y} + \\boldsymbol{\\Sigma}_0^{-1}\\boldsymbol{\\mu}_0)\n$$\n\nWe now apply these general results to the two specific cases.\n\n**1. Heteroscedastic Posterior**\nThe noise covariance is $\\mathbf{C} = \\boldsymbol{\\Sigma} = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$. Its inverse is $\\boldsymbol{\\Sigma}^{-1} = \\mathrm{diag}(1/\\sigma_1^2, \\dots, 1/\\sigma_n^2)$.\nThe posterior covariance matrix $\\boldsymbol{\\Sigma}_{\\text{het}}$ is:\n$$\n\\boldsymbol{\\Sigma}_{\\text{het}} = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1})^{-1}\n$$\nThe posterior mean $\\boldsymbol{\\mu}_{\\text{het}}$ is:\n$$\n\\boldsymbol{\\mu}_{\\text{het}} = \\boldsymbol{\\Sigma}_{\\text{het}} (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{y} + \\boldsymbol{\\Sigma}_0^{-1} \\boldsymbol{\\mu}_0)\n$$\nThe posterior distribution is $\\mathcal{N}_{\\text{het}} = \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{het}}, \\boldsymbol{\\Sigma}_{\\text{het}})$.\n\n**2. Homoscedastic Posterior**\nThe noise covariance is $\\mathbf{C} = \\sigma^2 \\mathbf{I}_n$, where $\\mathbf{I}_n$ is the $n \\times n$ identity matrix. Its inverse is $\\mathbf{C}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}_n$.\nThe posterior covariance matrix $\\boldsymbol{\\Sigma}_{\\text{hom}}$ is:\n$$\n\\boldsymbol{\\Sigma}_{\\text{hom}} = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T \\mathbf{X} + \\boldsymbol{\\Sigma}_0^{-1}\\right)^{-1}\n$$\nThe posterior mean $\\boldsymbol{\\mu}_{\\text{hom}}$ is:\n$$\n\\boldsymbol{\\mu}_{\\text{hom}} = \\boldsymbol{\\Sigma}_{\\text{hom}} \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T \\mathbf{y} + \\boldsymbol{\\Sigma}_0^{-1} \\boldsymbol{\\mu}_0\\right)\n$$\nThe posterior distribution is $\\mathcal{N}_{\\text{hom}} = \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{hom}}, \\boldsymbol{\\Sigma}_{\\text{hom}})$.\n\n**Comparison Metrics**\nWith the posterior means and covariances derived, we define the three comparison metrics.\n1.  **Euclidean norm of the difference between posterior means:**\n    $$\n    ||\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}}||_2 = \\sqrt{ (\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}})^T (\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}}) }\n    $$\n2.  **Ratio of the determinants of the posterior covariance matrices:**\n    $$\n    \\frac{\\det(\\boldsymbol{\\Sigma}_{\\text{het}})}{\\det(\\boldsymbol{\\Sigma}_{\\text{hom}})}\n    $$\n3.  **Symmetrized Kullback–Leibler divergence:** For two multivariate normal distributions $\\mathcal{N}_1 = \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1)$ and $\\mathcal{N}_2 = \\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2)$ of dimension $k$, the KL divergence is\n    $$\n    \\mathrm{KL}(\\mathcal{N}_1 \\,\\|\\, \\mathcal{N}_2) = \\frac{1}{2} \\left[ \\log\\left(\\frac{\\det \\boldsymbol{\\Sigma}_2}{\\det \\boldsymbol{\\Sigma}_1}\\right) - k + \\mathrm{Tr}(\\boldsymbol{\\Sigma}_2^{-1} \\boldsymbol{\\Sigma}_1) + (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}_2^{-1} (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1) \\right]\n    $$\n    The symmetrized KL divergence is $\\mathrm{KL}(\\mathcal{N}_{\\text{het}} \\,\\|\\, \\mathcal{N}_{\\text{hom}}) + \\mathrm{KL}(\\mathcal{N}_{\\text{hom}} \\,\\|\\, \\mathcal{N}_{\\text{het}})$. After algebraic simplification, this becomes:\n    $$\n    \\frac{1}{2} \\left[ \\mathrm{Tr}(\\boldsymbol{\\Sigma}_{\\text{het}}^{-1}\\boldsymbol{\\Sigma}_{\\text{hom}} + \\boldsymbol{\\Sigma}_{\\text{hom}}^{-1}\\boldsymbol{\\Sigma}_{\\text{het}} - 2\\mathbf{I}_k) + (\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}})^T (\\boldsymbol{\\Sigma}_{\\text{het}}^{-1} + \\boldsymbol{\\Sigma}_{\\text{hom}}^{-1}) (\\boldsymbol{\\mu}_{\\text{het}} - \\boldsymbol{\\mu}_{\\text{hom}}) \\right]\n    $$\nThe implementation will compute these quantities for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the EOB calibration problem for the given test cases.\n    \"\"\"\n    \n    # Define a helper function to compute posterior and comparison metrics for one case.\n    def compute_posterior_comparison(X, y, sigma_het_sq, mu0, Sigma0, sigma_hom_sq):\n        \"\"\"\n        Calculates the posterior distributions for heteroscedastic and homoscedastic\n        models and computes the three comparison metrics.\n        \"\"\"\n        n, k = X.shape\n\n        # Reshape vectors to be column vectors (n x 1 or k x 1)\n        y = y.reshape(-1, 1)\n        sigma_het_sq = sigma_het_sq.reshape(-1, 1)\n        mu0 = mu0.reshape(-1, 1)\n\n        # Pre-compute prior precision matrix\n        Sigma0_inv = np.linalg.inv(Sigma0)\n\n        # 1. Heteroscedastic Posterior Calculation\n        # The noise covariance inverse is diagonal with 1/sigma_i^2 on the diagonal.\n        # Efficiently compute X.T @ Sigma_inv @ X and X.T @ Sigma_inv @ y\n        # by scaling rows of X and elements of y before matrix multiplication.\n        W_het = 1.0 / sigma_het_sq.flatten()\n        X_T_S_inv_X_het = X.T * W_het @ X\n        X_T_S_inv_y_het = X.T @ (W_het * y.flatten())\n        X_T_S_inv_y_het = X_T_S_inv_y_het.reshape(-1, 1)\n\n        Sigma_post_het_inv = X_T_S_inv_X_het + Sigma0_inv\n        Sigma_post_het = np.linalg.inv(Sigma_post_het_inv)\n        mu_post_het = Sigma_post_het @ (X_T_S_inv_y_het + Sigma0_inv @ mu0)\n\n        # 2. Homoscedastic Posterior Calculation\n        const_inv_var_hom = 1.0 / sigma_hom_sq\n        Sigma_post_hom_inv = const_inv_var_hom * (X.T @ X) + Sigma0_inv\n        Sigma_post_hom = np.linalg.inv(Sigma_post_hom_inv)\n        mu_post_hom = Sigma_post_hom @ (const_inv_var_hom * (X.T @ y) + Sigma0_inv @ mu0)\n\n        # 3. Compute Comparison Metrics\n        # Metric 1: Euclidean norm of the difference between posterior means\n        delta_mu = mu_post_het - mu_post_hom\n        norm_diff = np.linalg.norm(delta_mu)\n\n        # Metric 2: Ratio of the determinants of the posterior covariances\n        det_het = np.linalg.det(Sigma_post_het)\n        det_hom = np.linalg.det(Sigma_post_hom)\n        det_ratio = det_het / det_hom\n\n        # Metric 3: Symmetrized Kullback-Leibler divergence\n        trace_term = np.trace(Sigma_post_hom_inv @ Sigma_post_het + Sigma_post_het_inv @ Sigma_post_hom - 2 * np.eye(k))\n        mahalanobis_term = delta_mu.T @ (Sigma_post_het_inv + Sigma_post_hom_inv) @ delta_mu\n        kl_sym = 0.5 * (trace_term + mahalanobis_term.item())\n\n        return (norm_diff, det_ratio, kl_sym)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"X\": np.array([[0.8, -0.1], [1.2, 0.0], [0.5, 0.3], [1.5, -0.2], [0.9, 0.4], [1.1, -0.3]]),\n            \"y\": np.array([-0.020, 0.010, 0.005, -0.030, 0.012, -0.015]),\n            \"sigma_het_sq\": np.array([1.0e-4, 2.0e-4, 1.5e-4, 1.0e-4, 2.5e-4, 1.8e-4]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"Sigma0\": np.diag([0.05**2, 0.05**2]),\n            \"sigma_hom_sq\": np.mean(np.array([1.0e-4, 2.0e-4, 1.5e-4, 1.0e-4, 2.5e-4, 1.8e-4]))\n        },\n        # Test Case 2\n        {\n            \"X\": np.array([[0.7, 0.2], [0.9, -0.1], [1.1, 0.0], [1.3, 0.3], [0.8, -0.2]]),\n            \"y\": np.array([0.006, -0.012, 0.008, 0.002, -0.009]),\n            \"sigma_het_sq\": np.array([2.0e-4, 2.0e-4, 2.0e-4, 2.0e-4, 2.0e-4]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"Sigma0\": np.diag([0.05**2, 0.05**2]),\n            \"sigma_hom_sq\": 2.0e-4\n        },\n        # Test Case 3\n        {\n            \"X\": np.array([[1.4, 0.1], [0.6, -0.2], [1.0, 0.2], [0.7, 0.0], [1.2, -0.3], [0.9, 0.4]]),\n            \"y\": np.array([0.015, -0.010, 0.005, 0.000, -0.020, 0.010]),\n            \"sigma_het_sq\": np.array([1.0e-6, 3.0e-4, 3.0e-4, 3.0e-4, 3.0e-4, 3.0e-4]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"Sigma0\": np.diag([0.05**2, 0.05**2]),\n            \"sigma_hom_sq\": np.mean(np.array([1.0e-6, 3.0e-4, 3.0e-4, 3.0e-4, 3.0e-4, 3.0e-4]))\n        },\n        # Test Case 4\n        {\n            \"X\": np.array([[0.5, 0.5], [1.0, -0.4], [0.8, 0.1], [1.3, -0.1], [0.6, 0.3], [1.1, -0.2]]),\n            \"y\": np.array([-0.005, 0.020, -0.010, 0.015, -0.004, 0.008]),\n            \"sigma_het_sq\": np.array([2.0e-4, 1.0e-2, 2.0e-4, 1.0e-2, 2.5e-4, 2.0e-4]),\n            \"mu0\": np.array([0.0, 0.0]),\n            \"Sigma0\": np.diag([0.05**2, 0.05**2]),\n            \"sigma_hom_sq\": np.mean(np.array([2.0e-4, 1.0e-2, 2.0e-4, 1.0e-2, 2.5e-4, 2.0e-4]))\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        norm_diff, det_ratio, kl_sym = compute_posterior_comparison(\n            case[\"X\"],\n            case[\"y\"],\n            case[\"sigma_het_sq\"],\n            case[\"mu0\"],\n            case[\"Sigma0\"],\n            case[\"sigma_hom_sq\"]\n        )\n        results.extend([norm_diff, det_ratio, kl_sym])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.12f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}