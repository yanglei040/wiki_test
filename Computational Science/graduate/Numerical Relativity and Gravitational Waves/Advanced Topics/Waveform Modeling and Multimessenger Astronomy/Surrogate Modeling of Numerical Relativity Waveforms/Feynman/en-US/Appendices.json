{
    "hands_on_practices": [
        {
            "introduction": "Before generating costly numerical relativity simulations, an efficient sampling strategy is paramount. This exercise explores how to leverage the physical symmetries of non-precessing binary black hole systems to dramatically reduce the size of the parameter space that must be explored. By calculating the \"sampling savings,\" you will gain a practical understanding of how theoretical insights directly translate into computational efficiency in surrogate model construction .",
            "id": "3488521",
            "problem": "You are constructing a reduced-order surrogate model for aligned-spin Binary Black Hole (BBH) waveforms in Numerical Relativity (NR). The waveforms are decomposed into spin-weighted spherical harmonics, so that the strain is written as $$h(t,\\theta,\\phi)=\\sum_{\\ell=2}^{\\ell_{\\max}}\\sum_{m=-\\ell}^{\\ell}h_{\\ell m}(t)\\,{}_{-2}Y_{\\ell m}(\\theta,\\phi),$$ where $h_{\\ell m}(t)$ are the mode amplitudes and ${}_{-2}Y_{\\ell m}(\\theta,\\phi)$ are the spin-weighted spherical harmonics of spin weight $-2$. For nonprecessing, quasi-circular, aligned-spin systems, two symmetries are relevant:\n\n1. Exchange symmetry of component labels: the physics is invariant under $(m_{1},m_{2},\\chi_{1},\\chi_{2})\\mapsto(m_{2},m_{1},\\chi_{2},\\chi_{1})$, which maps mass ratio $q=m_{1}/m_{2}$ to $q\\mapsto 1/q$ and swaps spins $(\\chi_{1},\\chi_{2})\\mapsto(\\chi_{2},\\chi_{1})$.\n2. Azimuthal mode symmetry: for nonprecessing systems, $h_{\\ell,-m}(t)=(-1)^{\\ell}h_{\\ell m}(t)^*$, so that negative-$m$ modes are determined by positive-$m$ modes.\n\nYour initial “naive” sampling plan is as follows. You target mass ratios $q\\in[1,8]$ and dimensionless aligned spins $\\chi_{i}\\in[-0.9,0.9]$ with $i\\in\\{1,2\\}$, but you do not enforce label ordering or exploit mode symmetry when counting unique samples. Concretely:\n- You lay out samples uniformly in $\\ln q$ over the symmetric interval $q\\in[1/8,8]$.\n- You lay out samples uniformly over the spin square $[-0.9,0.9]\\times[-0.9,0.9]$ for $(\\chi_{1},\\chi_{2})$, treating $(\\chi_{1},\\chi_{2})$ and $(\\chi_{2},\\chi_{1})$ as distinct.\n- You include all azimuthal indices $m$ for each multipole $\\ell\\in\\{2,3,4\\}$, so that each $\\ell$ contributes all $2\\ell+1$ values of $m$.\n\nYou then decide to exploit both symmetries to reduce unique sampling:\n- Use the label-exchange symmetry to restrict to $q\\in[1,8]$ only (equivalently, $\\ln q\\ge 0$) and to the triangular spin domain $\\{(\\chi_{1},\\chi_{2})\\in[-0.9,0.9]^{2}\\mid \\chi_{1}\\ge\\chi_{2}\\}$.\n- Use the azimuthal symmetry to retain only $m\\ge 0$ for each $\\ell$.\n\nAssume a constant sampling density in $\\ln q$ and over the spin square, and ignore measure-zero boundaries (e.g., $\\chi_{1}=\\chi_{2}$). Define the sampling savings $S$ as the fraction of naive samples that can be eliminated by applying these symmetries, i.e., $$S=1-\\frac{\\text{number of samples needed after symmetry reduction}}{\\text{number of samples in the naive plan}}.$$\n\nCompute $S$ for the case $\\ell\\in\\{2,3,4\\}$. Express your final result as a decimal number rounded to four significant figures. No units are required.",
            "solution": "The problem requires the computation of the sampling savings, $S$, obtained by exploiting symmetries in a surrogate modeling problem for gravitational waveforms from aligned-spin binary black hole mergers. The savings $S$ is defined as:\n$$S = 1 - \\frac{N_{\\text{reduced}}}{N_{\\text{naive}}}$$\nwhere $N_{\\text{naive}}$ is the number of samples in a naive plan and $N_{\\text{reduced}}$ is the number of samples in a plan that exploits symmetries.\n\nIn the context of surrogate modeling, a \"sample\" can be understood as a unique piece of information that must be included in the training data. For a model of waveform modes $h_{\\ell m}$ over a parameter space defined by mass ratio $q$ and spins $(\\chi_1, \\chi_2)$, the total number of samples is proportional to the product of the \"volume\" of the parameter space being sampled and the number of distinct waveform modes being modeled. Let $V_{\\text{param}}$ be the measure of the parameter space and $N_{\\text{modes}}$ be the number of modes. Then, the total number of samples $N$ is proportional to their product:\n$$N \\propto V_{\\text{param}} \\times N_{\\text{modes}}$$\nWe will analyze the naive and reduced plans separately to determine this quantity for each case.\n\nFirst, we analyze the \"naive\" sampling plan.\n\nThe parameter space consists of the mass ratio $q$ and two aligned spins, $\\chi_1$ and $\\chi_2$.\nFor the mass ratio, the sampling is uniform in $\\ln q$ over the interval $q \\in [1/8, 8]$. This corresponds to the interval for $\\ln q$ from $\\ln(1/8) = -\\ln(8)$ to $\\ln(8)$. The length of this interval is $L_{q, \\text{naive}} = \\ln(8) - (-\\ln(8)) = 2\\ln(8)$.\nFor the spins, the sampling is uniform over the square domain $(\\chi_1, \\chi_2) \\in [-0.9, 0.9] \\times [-0.9, 0.9]$. The side length of this square is $0.9 - (-0.9) = 1.8$. The area of this domain is $A_{\\chi, \\text{naive}} = (1.8)^2 = 3.24$.\nThe measure of the naive parameter space is proportional to the product of these quantities:\n$$V_{\\text{param, naive}} \\propto L_{q, \\text{naive}} \\times A_{\\chi, \\text{naive}} = 2\\ln(8) \\times (1.8)^2$$\n\nFor the waveform modes, the naive plan includes all azimuthal indices $m$ for each multipole $\\ell \\in \\{2, 3, 4\\}$. For a given $\\ell$, there are $2\\ell+1$ possible values for $m$, ranging from $-\\ell$ to $\\ell$. The total number of modes is the sum over the specified $\\ell$ values:\n$$N_{\\text{modes, naive}} = \\sum_{\\ell=2}^{4} (2\\ell+1) = (2(2)+1) + (2(3)+1) + (2(4)+1) = 5 + 7 + 9 = 21$$\nThe total number of naive samples is therefore:\n$$N_{\\text{naive}} \\propto V_{\\text{param, naive}} \\times N_{\\text{modes, naive}} \\propto (2\\ln(8) \\times (1.8)^2) \\times 21$$\n\nNext, we analyze the \"reduced\" sampling plan, which exploits two symmetries.\n\nThe first symmetry is the label-exchange symmetry, $(m_1, m_2, \\chi_1, \\chi_2) \\mapsto (m_2, m_1, \\chi_2, \\chi_1)$. This maps $q = m_1/m_2$ to $1/q$ and $(\\chi_1, \\chi_2)$ to $(\\chi_2, \\chi_1)$.\nTo exploit this symmetry, the parameter space is reduced. The mass ratio is restricted to $q \\in [1, 8]$, which corresponds to $\\ln q \\in [0, \\ln(8)]$. The length of this interval is $L_{q, \\text{reduced}} = \\ln(8)$. This is half the length of the naive interval: $L_{q, \\text{reduced}} = \\frac{1}{2}L_{q, \\text{naive}}$.\nThe spin space is restricted to the triangular domain $\\{(\\chi_{1},\\chi_{2})\\in[-0.9,0.9]^{2}\\mid \\chi_{1}\\ge\\chi_{2}\\}$. This domain is a triangle formed by the diagonal $\\chi_1 = \\chi_2$ and two sides of the square $[-0.9, 0.9]^2$. Since the boundary $\\chi_1 = \\chi_2$ is a set of measure zero, the area of this triangular region is exactly half the area of the square: $A_{\\chi, \\text{reduced}} = \\frac{1}{2}A_{\\chi, \\text{naive}} = \\frac{1}{2}(1.8)^2$.\nThe measure of the reduced parameter space is thus:\n$$V_{\\text{param, reduced}} \\propto L_{q, \\text{reduced}} \\times A_{\\chi, \\text{reduced}} = (\\ln(8)) \\times \\left(\\frac{1}{2}(1.8)^2\\right) = \\frac{1}{4} (2\\ln(8) \\times (1.8)^2)$$\nThis shows that $V_{\\text{param, reduced}} = \\frac{1}{4}V_{\\text{param, naive}}$.\n\nThe second symmetry is the azimuthal mode symmetry, $h_{\\ell,-m}(t)=(-1)^{\\ell}h_{\\ell m}(t)^*$. This relation implies that modes with negative $m$ are not independent and can be calculated from modes with positive $m$. Therefore, we only need to sample and model modes with $m \\ge 0$. For a given $\\ell$, these are the modes with $m \\in \\{0, 1, \\dots, \\ell\\}$, which amounts to $\\ell+1$ modes.\nThe number of modes in the reduced plan is:\n$$N_{\\text{modes, reduced}} = \\sum_{\\ell=2}^{4} (\\ell+1) = (2+1) + (3+1) + (4+1) = 3 + 4 + 5 = 12$$\nThe total number of reduced samples is:\n$$N_{\\text{reduced}} \\propto V_{\\text{param, reduced}} \\times N_{\\text{modes, reduced}} \\propto \\left(\\frac{1}{4} (2\\ln(8) \\times (1.8)^2)\\right) \\times 12$$\n\nWe can now compute the ratio of reduced to naive samples. The proportionality constants and the parameter space terms $2\\ln(8) \\times (1.8)^2$ cancel out.\n$$\\frac{N_{\\text{reduced}}}{N_{\\text{naive}}} = \\frac{V_{\\text{param, reduced}} \\times N_{\\text{modes, reduced}}}{V_{\\text{param, naive}} \\times N_{\\text{modes, naive}}} = \\left(\\frac{V_{\\text{param, reduced}}}{V_{\\text{param, naive}}}\\right) \\times \\left(\\frac{N_{\\text{modes, reduced}}}{N_{\\text{modes, naive}}}\\right)$$\nSubstituting the derived ratios:\n$$\\frac{N_{\\text{reduced}}}{N_{\\text{naive}}} = \\left(\\frac{1}{4}\\right) \\times \\left(\\frac{12}{21}\\right) = \\frac{12}{84} = \\frac{1}{7}$$\n\nFinally, we calculate the sampling savings $S$:\n$$S = 1 - \\frac{N_{\\text{reduced}}}{N_{\\text{naive}}} = 1 - \\frac{1}{7} = \\frac{6}{7}$$\nTo express this as a decimal number rounded to four significant figures, we perform the division:\n$$\\frac{6}{7} \\approx 0.8571428...$$\nRounding to four significant figures yields $0.8571$.",
            "answer": "$$\n\\boxed{0.8571}\n$$"
        },
        {
            "introduction": "With a set of waveforms from our sampling plan, the next step is to build the interpolant. This practice provides a comprehensive, hands-on implementation of the Empirical Interpolation Method (EIM), a powerful technique for creating accurate surrogates . You will construct a reduced basis from a \"training set\" of waveforms and then implement the EIM's greedy algorithm to select optimal interpolation points, culminating in a functional surrogate whose accuracy you will rigorously test.",
            "id": "3488482",
            "problem": "Implement a complete, runnable program that constructs an empirical interpolation surrogate for a toy family of gravitational-wave-like waveforms and verifies the convergence of the reconstruction error with increasing empirical node count. The task is to proceed from foundational principles of reduced-order modeling and linear approximation for parametric function families. The waveform family is given in analytic form so that the surrogate construction and error quantification can be validated directly. Angles must be treated in radians.\n\nYou are given a parametric waveform family $w(t;\\lambda)$ defined for $t \\in [0,1]$ and $\\lambda \\in [0,1]$. The waveform is\n$$\nw(t;\\lambda) \\equiv \\left(1 + 0.3\\,\\lambda\\right)\\,\\left(t+0.01\\right)^{1/4}\\,\\exp\\!\\left(-t^{8}\\right)\\,\\sin\\!\\big(\\varphi(t;\\lambda)\\big),\n$$\nwhere the phase is\n$$\n\\varphi(t;\\lambda) \\equiv 2\\pi f_0\\,t + \\beta\\,\\lambda\\,t^{3/2} + \\kappa\\,t^{2}.\n$$\nUse $f_0 = 20$, $\\beta = 35$, and $\\kappa = 3$. The argument of the sine is in radians.\n\nFoundational starting point: Use the principle that a smooth, low-intrinsic-dimensional parametric family of functions admits accurate low-rank approximations captured by the leading singular vectors of its snapshot matrix. Empirical Interpolation Method (EIM) constructs an interpolation operator that matches a function at adaptively chosen nodes to recover coefficients in a reduced basis expansion. You must implement the following steps from first principles:\n\n1. Time discretization and snapshots:\n   - Discretize the interval $[0,1]$ with a uniform grid of $N_t = 2000$ points.\n   - Construct a training parameter set $\\Lambda_{\\mathrm{train}}$ of $M = 20$ equispaced values in $[0,1]$.\n   - Form the snapshot matrix $S \\in \\mathbb{R}^{N_t \\times M}$ with columns $S_{:,j} = w(t;\\lambda_j)$ for $\\lambda_j \\in \\Lambda_{\\mathrm{train}}$.\n\n2. Reduced basis by singular value decomposition (SVD):\n   - Compute a thin singular value decomposition $S = U \\Sigma V^{\\top}$.\n   - The reduced basis functions are the first $r$ left singular vectors $U_{:,1},\\dots,U_{:,r}$, with $1 \\le r \\le M$.\n\n3. Empirical Interpolation Method (EIM) node selection:\n   - Define the empirical interpolation nodes inductively. Let $\\Phi_k$ denote the $k$-dimensional basis $\\{U_{:,1},\\dots,U_{:,k}\\}$ viewed as functions on the time grid.\n   - Initialization: Choose the first node index $i_1$ as the index of the maximal absolute value of $U_{:,1}$ on the grid.\n   - Inductive step: Given nodes $\\{i_1,\\dots,i_{k-1}\\}$, find coefficients $\\{a_j\\}_{j=1}^{k-1}$ that interpolate $U_{:,k}$ in the subspace spanned by $\\Phi_{k-1}$ at the existing nodes. That is, find $a \\in \\mathbb{R}^{k-1}$ such that for all $\\ell \\in \\{1,\\dots,k-1\\}$,\n     $$\n     \\sum_{j=1}^{k-1} a_j\\,U_{i_\\ell,j} = U_{i_\\ell,k}.\n     $$\n     Then form the residual vector $r^{(k)} = U_{:,k} - \\sum_{j=1}^{k-1} a_j\\,U_{:,j}$ and choose $i_k$ as the index of the maximal absolute value of $r^{(k)}$ on the grid.\n   - This yields, for each $m \\in \\{1,2,\\dots,M\\}$, a set of $m$ nodes $\\{i_1,\\dots,i_m\\}$.\n\n4. Empirical interpolation reconstruction:\n   - For a given waveform $y = w(\\cdot;\\lambda)$ and a chosen $m$, compute coefficients $c \\in \\mathbb{R}^{m}$ by enforcing interpolation at the empirical nodes:\n     $$\n     \\sum_{j=1}^{m} c_j\\,U_{i_\\ell,j} = y_{i_\\ell}, \\quad \\ell=1,\\dots,m.\n     $$\n     Solve the linear system for $c$ and reconstruct $\\widehat{y}^{(m)} = \\sum_{j=1}^{m} c_j\\,U_{:,j}$.\n\n5. Error quantification and singular value decay:\n   - For a given $\\lambda$, compute the relative $L^2$ error on $[0,1]$,\n     $$\n     \\varepsilon^{(m)}(\\lambda) \\equiv \\frac{\\left(\\int_0^1 \\big|y(t) - \\widehat{y}^{(m)}(t)\\big|^2\\,dt\\right)^{1/2}}{\\left(\\int_0^1 |y(t)|^2\\,dt\\right)^{1/2}},\n     $$\n     using the trapezoidal rule on the time grid.\n   - Estimate an exponential decay rate of the singular values by fitting a straight line to $\\log(\\sigma_k)$ as a function of $k$, where $\\{\\sigma_k\\}$ are the singular values from $\\Sigma$. Specifically, for indices $k$ such that $\\sigma_k$ remains larger than a fixed small fraction of $\\sigma_1$ (e.g., greater than $10^{-12}\\,\\sigma_1$), solve a least-squares problem for parameters $(a,b)$ in\n     $\\log \\sigma_k \\approx a + b\\,k$,\n     and report the rate estimate $\\rho \\equiv -b$.\n\nTest suite specification:\n- Use the test parameter set $\\Lambda_{\\mathrm{test}} = \\{0.07,\\,0.31,\\,0.59,\\,0.93\\}$.\n- Use the node counts $\\mathcal{M} = \\{1,\\,2,\\,3,\\,5,\\,8,\\,12,\\,16\\}$.\n- For each $m \\in \\mathcal{M}$, compute the maximum test error\n  $E_m \\equiv \\max_{\\lambda \\in \\Lambda_{\\mathrm{test}}} \\varepsilon^{(m)}(\\lambda)$.\n- Also compute the singular value decay rate estimate $\\rho$ as described above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The first entry must be a list of the seven values $[E_1,E_2,E_3,E_5,E_8,E_{12},E_{16}]$ in this order, and the second entry must be the scalar $\\rho$. Use angles in radians and no physical units. Each printed numeric value must be rounded to six significant figures in scientific notation. For example, output of the form\n  $$\n  [[\\text{e}_1,\\text{e}_2,\\dots,\\text{e}_7],\\text{r}]\n  $$\n  where each $\\text{e}_j$ and $\\text{r}$ appear as numbers like $1.234567\\text{e}{-03}$.\n\nScientific realism:\n- The waveform family $w(t;\\lambda)$ is analytic, smooth in both $t$ and $\\lambda$, and exhibits chirp-like phase evolution and amplitude modulation that model salient features of numerical relativity waveforms without invoking full general relativity. This ensures the reduced basis and empirical interpolation approach is applicable and the singular values decay meaningfully.\n\nDesign for coverage:\n- The chosen $\\mathcal{M}$ covers a boundary case $m=1$, modest intermediate sizes, and a larger size approaching the training set dimension.\n- The test set $\\Lambda_{\\mathrm{test}}$ includes values near both ends and interior of the parameter range to probe interpolation behavior across the domain.\n\nYour implementation must be self-contained, require no user input, and adhere to the specified final output format exactly.",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of numerical analysis and reduced-order modeling, with all necessary data and algorithmic steps clearly specified. There are no contradictions, ambiguities, or violations of scientific principles that would render it invalid. The task is to implement the Empirical Interpolation Method (EIM) to construct a surrogate model for a given parametric family of functions, a standard and important technique in computational science.\n\nThe solution is implemented by following the prescribed sequence of steps, which are rooted in the principles of linear algebra and approximation theory.\n\n**1. Waveform Discretization and Snapshot Matrix Generation**\nThe foundational step is to represent the continuous, parametric family of functions $w(t;\\lambda)$ in a discrete form suitable for numerical linear algebra. The waveform is defined as:\n$$\nw(t;\\lambda) = \\left(1 + 0.3\\,\\lambda\\right)\\,\\left(t+0.01\\right)^{1/4}\\,\\exp\\!\\left(-t^{8}\\right)\\,\\sin\\!\\big(2\\pi f_0\\,t + \\beta\\,\\lambda\\,t^{3/2} + \\kappa\\,t^{2}\\big)\n$$\nwith constants $f_0 = 20$, $\\beta = 35$, and $\\kappa = 3$.\n\nFirst, the time domain $t \\in [0,1]$ is discretized into a uniform grid of $N_t = 2000$ points. Second, the parameter domain $\\lambda \\in [0,1]$ is sampled at $M=20$ equidistant points to form a training set $\\Lambda_{\\mathrm{train}}$. For each training parameter $\\lambda_j \\in \\Lambda_{\\mathrm{train}}$, the waveform $w(t; \\lambda_j)$ is evaluated on the time grid, yielding a vector in $\\mathbb{R}^{N_t}$. These vectors are assembled as columns to form the snapshot matrix $S \\in \\mathbb{R}^{N_t \\times M}$. The matrix $S$ is thus a discrete representation of the function family.\n\n**2. Reduced Basis via Singular Value Decomposition (SVD)**\nThe principle of low-rank approximation states that if the functions in the family are highly correlated, the snapshot matrix $S$ can be accurately approximated by a low-rank matrix. The optimal low-rank basis, in the least-squares sense, is given by the leading left singular vectors of $S$. A thin Singular Value Decomposition is computed:\n$$\nS = U \\Sigma V^{\\top}\n$$\nwhere $U \\in \\mathbb{R}^{N_t \\times M}$ is a matrix with orthonormal columns (the left singular vectors), $\\Sigma \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix of singular values $\\{\\sigma_k\\}$, and $V \\in \\mathbb{R}^{M \\times M}$ is an orthogonal matrix. The columns of $U$, denoted $\\{U_{:,1}, \\dots, U_{:,M}\\}$, form the reduced basis. The rapid decay of the singular values $\\sigma_k$ indicates that the function family has a low intrinsic dimensionality and is well-suited for reduced-order modeling.\n\n**3. Empirical Interpolation Method (EIM) for Node Selection**\nWhile the SVD provides an optimal basis, evaluating the projection coefficients for a new waveform would require computing its inner products with the basis functions, an operation as costly as evaluating the full waveform. EIM circumvents this by selecting a small set of \"empirical\" time points, or nodes, at which the waveform is evaluated. The coefficients are then found by solving a small linear system.\n\nThe EIM nodes are selected greedily to be optimal for interpolation within the reduced basis.\n- **Step 1 ($k=1$):** The first basis function, $U_{:,1}$, is considered. The first node, $i_1$, is chosen as the time index where $|U_{:,1}(t)|$ is maximum. This point captures the most significant feature of the most dominant mode.\n- **Inductive Step ($k > 1$):** To find the $k$-th node $i_k$, we first consider the $k$-th basis function $U_{:,k}$. We construct its interpolant within the subspace spanned by the previous basis functions, $\\{U_{:,1}, \\dots, U_{:,k-1}\\}$, by enforcing equality at the previously selected nodes $\\{i_1, \\dots, i_{k-1}\\}$. This requires solving a $(k-1) \\times (k-1)$ linear system for the interpolation coefficients $\\{a_j\\}_{j=1}^{k-1}$:\n$$\n\\sum_{j=1}^{k-1} a_j\\,U_{i_\\ell, j} = U_{i_\\ell, k}, \\quad \\text{for } \\ell = 1, \\dots, k-1\n$$\nThe residual, $r^{(k)}(t) = U_{:,k} - \\sum_{j=1}^{k-1} a_j U_{:,j}$, represents the part of $U_{:,k}$ that cannot be captured by the preceding basis functions at the existing nodes. The new node $i_k$ is chosen as the time index where this residual $|r^{(k)}(t)|$ is maximized. This process is repeated for $k=1, \\dots, M$ to generate a sequence of $M$ empirical nodes.\n\n**4. Waveform Reconstruction and Error Quantification**\nFor a test waveform $y(t) = w(t; \\lambda)$ with $\\lambda \\in \\Lambda_{\\mathrm{test}}$, and for a chosen number of basis functions/nodes $m$, the surrogate approximation $\\widehat{y}^{(m)}$ is constructed as a linear combination of the first $m$ basis functions: $\\widehat{y}^{(m)} = \\sum_{j=1}^{m} c_j U_{:,j}$. The coefficients $c = \\{c_j\\}_{j=1}^m$ are determined by solving the $m \\times m$ linear system that enforces equality between the true waveform and the surrogate at the first $m$ empirical nodes $\\{i_1, \\dots, i_m\\}$:\n$$\n\\sum_{j=1}^{m} c_j\\,U_{i_\\ell, j} = y_{i_\\ell}, \\quad \\ell=1,\\dots,m\n$$\nThe accuracy of this surrogate is quantified by the relative $L^2$ error, $\\varepsilon^{(m)}(\\lambda)$, calculated numerically using the trapezoidal rule on the time grid. The maximum error $E_m$ over the test set $\\Lambda_{\\mathrm{test}}$ is then determined for each $m \\in \\mathcal{M} = \\{1, 2, 3, 5, 8, 12, 16\\}$.\n\n**5. Singular Value Decay Rate**\nThe exponential decay of the singular values is a key indicator of the model's reducibility. The decay rate $\\rho$ is estimated by performing a linear least-squares fit to the model $\\log \\sigma_k \\approx a + b k$ for all singular values $\\sigma_k$ that are numerically significant (i.e., $\\sigma_k > 10^{-12} \\sigma_1$). The estimated rate is then $\\rho = -b$.\n\nThe final program implements these steps to compute the required error metrics and the singular value decay rate, formatting the output as a single line according to the problem specification.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs an empirical interpolation surrogate for a toy waveform family\n    and verifies the convergence of the reconstruction error.\n    \"\"\"\n    # Step 0: Define constants and parameters\n    f0 = 20.0\n    beta = 35.0\n    kappa = 3.0\n    Nt = 2000\n    M = 20\n    test_lambdas = [0.07, 0.31, 0.59, 0.93]\n    m_counts = [1, 2, 3, 5, 8, 12, 16]\n\n    def get_waveform(t, lam, f0, beta, kappa):\n        \"\"\"\n        Computes the toy gravitational waveform w(t; lambda).\n        \"\"\"\n        # Phase function in radians\n        phase = 2.0 * np.pi * f0 * t + beta * lam * np.power(t, 1.5) + kappa * np.power(t, 2.0)\n        # Amplitude modulation\n        amplitude = (1.0 + 0.3 * lam) * np.power(t + 0.01, 0.25) * np.exp(-np.power(t, 8.0))\n        return amplitude * np.sin(phase)\n\n    # Step 1: Time discretization, training set, and snapshot matrix\n    t_grid = np.linspace(0.0, 1.0, Nt)\n    train_lambdas = np.linspace(0.0, 1.0, M)\n    \n    S = np.zeros((Nt, M))\n    for j, lam in enumerate(train_lambdas):\n        S[:, j] = get_waveform(t_grid, lam, f0, beta, kappa)\n\n    # Step 2: Reduced basis by SVD\n    U, svals, _ = np.linalg.svd(S, full_matrices=False)\n    \n    # Step 5 (part 1): Singular value decay rate estimation\n    s_thresh = svals[0] * 1e-12\n    s_fit = svals[svals > s_thresh]\n    k_fit = np.arange(1, len(s_fit) + 1)\n    \n    # Perform a linear least-squares fit for log(sigma_k) vs. k\n    # np.polyfit for degree 1 returns [slope, intercept]\n    slope, _ = np.polyfit(k_fit, np.log(s_fit), 1)\n    rho = -slope\n    \n    # Step 3: Empirical Interpolation Method (EIM) node selection\n    eim_nodes = []\n    for k in range(M):\n        if k == 0:\n            # First residual is just the first basis vector\n            residual = U[:, 0]\n        else:\n            # For the (k+1)-th node (0-indexed k), we have k nodes in eim_nodes.\n            # We solve for the coefficients that interpolate U[:, k] using the basis U[:, :k]\n            # at the previously selected k nodes.\n            A_mat = U[eim_nodes, :k]  # k x k matrix\n            b_vec = U[eim_nodes, k]   # k-dim vector\n            \n            coeffs = np.linalg.solve(A_mat, b_vec)\n            \n            # Compute the residual vector over the full time domain\n            projection = U[:, :k] @ coeffs\n            residual = U[:, k] - projection\n\n        # Select new node as the point of maximum absolute residual\n        new_node_idx = np.argmax(np.abs(residual))\n        eim_nodes.append(int(new_node_idx))\n        \n    # Step 4 & 5 (part 2): Reconstruction and error quantification\n    max_errors = []\n    for m in m_counts:\n        nodes_m = eim_nodes[:m]\n        U_m = U[:, :m]\n        \n        # Define the m x m interpolation matrix\n        interp_matrix = U_m[nodes_m, :]\n        \n        current_max_error = 0.0\n        for lam_test in test_lambdas:\n            y_true = get_waveform(t_grid, lam_test, f0, beta, kappa)\n            \n            # Get waveform values at the interpolation nodes\n            y_nodes = y_true[nodes_m]\n            \n            # Solve for reconstruction coefficients\n            c_coeffs = np.linalg.solve(interp_matrix, y_nodes)\n            \n            # Reconstruct the surrogate waveform\n            y_recon = U_m @ c_coeffs\n            \n            # Calculate relative L2 error using the trapezoidal rule\n            norm_diff_sq = np.trapz((y_true - y_recon)**2, t_grid)\n            norm_true_sq = np.trapz(y_true**2, t_grid)\n            \n            norm_diff = np.sqrt(norm_diff_sq)\n            norm_true = np.sqrt(norm_true_sq)\n            \n            rel_error = norm_diff / norm_true if norm_true > 1e-15 else 0.0\n            \n            current_max_error = max(current_max_error, rel_error)\n        \n        max_errors.append(current_max_error)\n        \n    # Format the final output as specified (using {:.6e} for 7 significant figures\n    # to match the provided formatting example, despite the \"six\" in text).\n    errors_formatted = [\"{:.6e}\".format(e) for e in max_errors]\n    rho_formatted = \"{:.6e}\".format(rho)\n    \n    print(f\"[[{','.join(errors_formatted)}],{rho_formatted}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An accurate model is only useful if it is also numerically robust. This final exercise addresses the critical issue of stability, investigating how the choice of interpolation nodes can affect the model's sensitivity to small errors in the input data . You will compute the condition number of the interpolation matrix and a related error amplification factor, learning to identify a truncation for your basis that provides a good compromise between model complexity and numerical stability.",
            "id": "3488481",
            "problem": "Consider a surrogate modeling setting for numerical relativity waveforms in which a reduced basis matrix $U \\in \\mathbb{R}^{m \\times r}$ is used to reconstruct waveform samples from a selected set of nodes $\\mathcal{T} = (t_{i_1}, \\dots, t_{i_r})$ that index rows of the basis, with empirical interpolation based reconstruction governed by the square interpolation submatrix $U(\\mathcal{T}) \\in \\mathbb{R}^{r \\times r}$ formed by restricting $U$ to the rows given by $\\mathcal{T}$. The Empirical Interpolation Method (EIM) is used to compute coefficients $c \\in \\mathbb{R}^r$ via $U(\\mathcal{T}) c = w(\\mathcal{T})$, reconstructing the waveform approximation $U c$. We are concerned with the amplification of node errors in $w(\\mathcal{T})$ into the reconstructed approximation $U c$ due to the conditioning of $U(\\mathcal{T})$ and the operator norm of $U$.\n\nStart from the following fundamental base:\n- Singular Value Decomposition (SVD): For a matrix $A$, the singular values are denoted by $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ for the largest and smallest singular values, respectively. The spectral norm is $\\lVert A \\rVert_2 = \\sigma_{\\max}(A)$, and if $A$ is invertible, $\\lVert A^{-1} \\rVert_2 = 1/\\sigma_{\\min}(A)$.\n- Condition number in the $2$-norm: $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2 = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$ for invertible $A$.\n- Linear error propagation: For perturbations $\\delta w(\\mathcal{T})$ at the nodes, the induced coefficient error satisfies $\\delta c = U(\\mathcal{T})^{-1} \\delta w(\\mathcal{T})$, and the reconstructed waveform error is $U \\delta c$. Therefore, a bound on the amplification factor of node errors into the reconstructed approximation is $G = \\lVert U \\rVert_2 \\lVert U(\\mathcal{T})^{-1} \\rVert_2$.\n\nTask: Given $U$ and an ordered node set $\\mathcal{T}$, compute the conditioning of the square interpolation matrix $U_k(\\mathcal{T}_k)$ for truncations $k = 1, 2, \\dots, r$, where $U_k$ is the truncation of $U$ to its first $k$ columns and $\\mathcal{T}_k$ is the truncation of $\\mathcal{T}$ to its first $k$ indices, and choose the maximal truncation $k$ such that the amplification factor $G_k = \\lVert U_k \\rVert_2 \\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$ is strictly less than $10$. For the selected truncation, also report the $2$-norm condition number $\\kappa_2(U_k(\\mathcal{T}_k)) = \\sigma_{\\max}(U_k(\\mathcal{T}_k))/\\sigma_{\\min}(U_k(\\mathcal{T}_k))$. If $U_k(\\mathcal{T}_k)$ is singular, treat $\\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$ and $\\kappa_2(U_k(\\mathcal{T}_k))$ as $+\\infty$.\n\nTo make the problem concrete and universally applicable in purely mathematical terms, use a discrete time grid and a Vandermonde-type reduced basis defined as follows. For each test case,\n- Let $m$ be the number of sample times and $r$ the reduced basis size.\n- Define a uniform grid $t_i = \\frac{i}{m-1}$ for $i = 0, 1, \\dots, m-1$.\n- Define the reduced basis matrix $U \\in \\mathbb{R}^{m \\times r}$ with entries $U_{i,j} = t_i^j$ for column indices $j = 0, 1, \\dots, r-1$.\n- Define the ordered node set $\\mathcal{T}$ as a list of $r$ distinct integer indices in $\\{0,1,\\dots,m-1\\}$.\n\nYou must compute, for each test case, the maximal truncation $k$ satisfying $G_k < 10$, together with the corresponding condition number $\\kappa_2(U_k(\\mathcal{T}_k))$ and amplification factor $G_k$. Use the spectral norm ($2$-norm) throughout. Round the floating-point outputs to six decimal places.\n\nTest suite:\n- Case 1: $m = 12$, $r = 6$, $\\mathcal{T} = [0,2,4,6,8,10]$.\n- Case 2: $m = 12$, $r = 6$, $\\mathcal{T} = [0,1,2,3,4,5]$.\n- Case 3: $m = 20$, $r = 10$, $\\mathcal{T} = [10,11,12,13,14,15,16,17,18,19]$.\n- Case 4: $m = 20$, $r = 10$, $\\mathcal{T} = [19,18,17,16,15,14,13,12,11,10]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[k, \\kappa, G]$, where $k$ is an integer, and $\\kappa$ and $G$ are floats rounded to six decimal places. For example, the final output should look like $[[k_1,\\kappa_1,G_1],[k_2,\\kappa_2,G_2],[k_3,\\kappa_3,G_3],[k_4,\\kappa_4,G_4]]$.",
            "solution": "The objective is to determine, for several test cases, the maximal truncation level $k$ of a reduced basis approximation that maintains a specified level of numerical stability. This stability is quantified by an amplification factor $G_k$ which must remain strictly less than $10$. For this maximal $k$, we must also report the condition number $\\kappa_2(U_k(\\mathcal{T}_k))$ of the associated interpolation matrix and the amplification factor $G_k$.\n\nThe solution proceeds by implementing the definitions and algorithm described in the problem statement. For each test case defined by the parameters $m$ (number of time samples), $r$ (basis size), and $\\mathcal{T}$ (ordered set of node indices), we perform the following steps.\n\nFirst, we construct the necessary mathematical objects. A uniform time grid $\\{t_i\\}_{i=0}^{m-1}$ is defined by $t_i = \\frac{i}{m-1}$. Using this grid, we form the $m \\times r$ basis matrix $U$ whose entries are given by $U_{i,j} = t_i^j$ for $i \\in \\{0, \\dots, m-1\\}$ and $j \\in \\{0, \\dots, r-1\\}$. This is a Vandermonde-type matrix.\n\nNext, we iterate through all possible truncation levels, from $k=1$ to $k=r$. For each $k$, we define the truncated basis matrix $U_k$ as the first $k$ columns of $U$, and the truncated ordered node set $\\mathcal{T}_k$ as the first $k$ indices from $\\mathcal{T}$. These are used to construct the square $k \\times k$ interpolation submatrix $U_k(\\mathcal{T}_k)$, which is formed by selecting the rows of $U_k$ corresponding to the indices in $\\mathcal{T}_k$.\n\nThe core of the task is to compute the amplification factor $G_k$ and the condition number $\\kappa_2(U_k(\\mathcal{T}_k))$. These quantities are defined in terms of matrix norms, which are most reliably computed via the Singular Value Decomposition (SVD).\nThe amplification factor is $G_k = \\lVert U_k \\rVert_2 \\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$. The spectral norm $\\lVert A \\rVert_2$ of a matrix $A$ is its largest singular value, $\\sigma_{\\max}(A)$. If $A$ is invertible, the norm of its inverse, $\\lVert A^{-1} \\rVert_2$, is the reciprocal of its smallest singular value, $1/\\sigma_{\\min}(A)$.\nThus, for each $k$, we compute:\n1. The SVD of $U_k$ to find its largest singular value, $\\sigma_{\\max}(U_k) = \\lVert U_k \\rVert_2$.\n2. The SVD of $U_k(\\mathcal{T}_k)$ to find its largest and smallest singular values, $\\sigma_{\\max}(U_k(\\mathcal{T}_k))$ and $\\sigma_{\\min}(U_k(\\mathcal{T}_k))$.\n\nIf $U_k(\\mathcal{T}_k)$ is numerically singular, its smallest singular value $\\sigma_{\\min}(U_k(\\mathcal{T}_k))$ is effectively zero. In this case, as per the problem, $\\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$ and $\\kappa_2(U_k(\\mathcal{T}_k))$ are treated as infinite. Numerically, this is handled by checking if $\\sigma_{\\min}(U_k(\\mathcal{T}_k))$ is below a small threshold relative to machine precision.\n\nIf the matrix is not singular, we compute the quantities:\n- Amplification factor: $G_k = \\sigma_{\\max}(U_k) \\cdot \\frac{1}{\\sigma_{\\min}(U_k(\\mathcal{T}_k))}$\n- Condition number: $\\kappa_2(U_k(\\mathcal{T}_k)) = \\frac{\\sigma_{\\max}(U_k(\\mathcal{T}_k))}{\\sigma_{\\min}(U_k(\\mathcal{T}_k))}$\n\nWe then check if $G_k  10$. We need to find the maximal $k$ for which this condition holds. An iterative search is performed from $k=1$ to $r$. As we iterate, we keep track of the latest (and thus largest) value of $k$ that satisfies the condition. The values of $\\kappa_2(U_k(\\mathcal{T}_k))$ and $G_k$ corresponding to this final $k$ are stored. It is established that for $k=1$, $G_1 = \\sqrt{m}$ which is less than $10$ for all test cases, guaranteeing that a solution exists.\n\nFinally, for each test case, the resulting triplet $[k, \\kappa, G]$, with the floating-point numbers $\\kappa$ and $G$ rounded to six decimal places, is collected. The results for all test cases are then formatted into a single string as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        (12, 6, [0, 2, 4, 6, 8, 10]),\n        (12, 6, [0, 1, 2, 3, 4, 5]),\n        (20, 10, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n        (20, 10, [19, 18, 17, 16, 15, 14, 13, 12, 11, 10])\n    ]\n\n    all_results = []\n    \n    for m, r, T_nodes in test_cases:\n        # Define the uniform time grid t_i = i/(m-1)\n        t = np.linspace(0.0, 1.0, m)\n        \n        # Define the reduced basis matrix U with U_ij = t_i^j\n        # np.vander with increasing=True matches this definition.\n        U = np.vander(t, N=r, increasing=True)\n        \n        max_k_found = 0\n        final_kappa = np.nan\n        final_G = np.nan\n        \n        # Iterate through all truncations k = 1, ..., r\n        for k in range(1, r + 1):\n            # U_k is the first k columns of U\n            U_k = U[:, :k]\n            \n            # T_k is the first k indices from the ordered node set T\n            T_k_indices = T_nodes[:k]\n            \n            # Form the square interpolation submatrix U_k(T_k)\n            U_k_T_k = U_k[T_k_indices, :]\n            \n            # Compute the 2-norm of U_k (its largest singular value)\n            norm_U_k = np.linalg.norm(U_k, 2)\n            \n            G_k = np.inf\n            kappa_k = np.inf\n            \n            try:\n                # Compute singular values of the interpolation matrix\n                s_U_k_T_k = np.linalg.svd(U_k_T_k, compute_uv=False)\n                \n                sigma_max_val = s_U_k_T_k[0]\n                sigma_min_val = s_U_k_T_k[-1]\n\n                # Check for numerical singularity. A matrix is singular if sigma_min is\n                # close to zero. We use a standard threshold based on machine epsilon.\n                if sigma_min_val  np.finfo(float).eps * sigma_max_val * k:\n                    G_k = np.inf\n                    kappa_k = np.inf\n                else:\n                    # norm(A_inv) = 1/sigma_min(A)\n                    norm_inv_U_k_T_k = 1.0 / sigma_min_val\n                    # G_k = ||U_k|| * ||U_k(T_k)^-1||\n                    G_k = norm_U_k * norm_inv_U_k_T_k\n                    # kappa_k = sigma_max / sigma_min\n                    kappa_k = sigma_max_val / sigma_min_val\n\n            except np.linalg.LinAlgError:\n                # This case handles matrices that are exactly singular,\n                # though svd is robust and usually won't raise this.\n                G_k = np.inf\n                kappa_k = np.inf\n            \n            # Check if the amplification factor is below the threshold\n            if G_k  10.0:\n                # If so, this k is a candidate. Since we iterate k from 1 to r,\n                # the last one to satisfy the condition will be the maximal k.\n                max_k_found = k\n                final_kappa = kappa_k\n                final_G = G_k\n        \n        all_results.append([max_k_found, final_kappa, final_G])\n\n    # Format the final output string as per requirements\n    result_strings = []\n    for result in all_results:\n        k, kappa, G = result\n        # Format floats to exactly six decimal places\n        result_strings.append(f\"[{k},{kappa:.6f},{G:.6f}]\")\n    \n    # Print the single-line output\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}