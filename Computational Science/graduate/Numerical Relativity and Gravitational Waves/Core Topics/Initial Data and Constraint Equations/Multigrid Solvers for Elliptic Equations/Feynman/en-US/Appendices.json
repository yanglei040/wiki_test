{
    "hands_on_practices": [
        {
            "introduction": "Before building a full multigrid solver, it is crucial to understand its most fundamental component: the smoother. A smoother is not an exact solver, but an iterative procedure designed to efficiently reduce high-frequency components of the error. This exercise  provides hands-on practice with Local Fourier Analysis (LFA), a powerful analytical technique used to predict a smoother's performance and to optimize its parameters, such as the relaxation weight $\\omega$ in the Jacobi method.",
            "id": "3480279",
            "problem": "Consider the standard second-order finite-difference discretization of the one-dimensional Poisson equation $-u^{\\prime \\prime}(x) = f(x)$ on a uniform infinite grid $\\{ x_{i} = i h : i \\in \\mathbb{Z} \\}$ with mesh spacing $h$. The discrete operator $A$ acts on a grid function $\\{ u_{i} \\}$ by\n$$\n(A u)_{i} = \\frac{1}{h^{2}}\\left( - u_{i-1} + 2 u_{i} - u_{i+1} \\right).\n$$\nIn a multigrid algorithm designed for solving elliptic constraint equations that arise in the $3 + 1$ formulation of general relativity, weighted Jacobi relaxation is often used as a smoother for high-frequency error components. The weighted Jacobi relaxation updates the error $e^{(k)}$ via\n$$\ne^{(k+1)} = S_{\\omega} \\, e^{(k)}, \\quad S_{\\omega} = I - \\omega D^{-1} A,\n$$\nwhere $D$ is the diagonal of $A$ and $\\omega \\in (0,1)$ is the relaxation parameter.\n\nUsing Local Fourier Analysis (LFA), treat the error as a superposition of Fourier modes $e_{i}(\\theta) = \\exp(\\mathrm{i} \\theta i)$ with wavenumber $\\theta \\in [-\\pi, \\pi]$. For the purpose of multigrid smoothing, focus on the high-frequency subspace that is not representable on the next coarser grid, namely $\\theta \\in [\\pi/2, \\pi]$. Derive the symbol of the error-propagation operator $S_{\\omega}$ acting on a Fourier mode and express the spectral radius of $S_{\\omega}$ restricted to the high-frequency subspace as a function of $\\omega$. Then determine the value of $\\omega$ that minimizes this restricted spectral radius. Express your final answer as an exact real number. No rounding is required, and no physical units are associated with $\\omega$.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective. The problem statement provides a standard Local Fourier Analysis (LFA) task for the weighted Jacobi smoother applied to the one-dimensional Poisson equation. All definitions, including the discrete operator, the smoother, the Fourier modes, and the high-frequency subspace, are standard and consistent with the literature on multigrid methods. The context of elliptic equations in numerical relativity is appropriate. The problem is therefore deemed valid.\n\nThe solution proceeds in four steps: first, we determine the diagonal operator $D$; second, we use LFA to find the symbol of the error propagation operator $S_{\\omega}$; third, we determine the spectral radius of $S_{\\omega}$ restricted to the high-frequency subspace; and fourth, we find the relaxation parameter $\\omega$ that minimizes this spectral radius.\n\nStep 1: Determine the operator $D$.\nThe discrete operator $A$ is given by\n$$ (A u)_{i} = \\frac{1}{h^{2}}\\left( - u_{i-1} + 2 u_{i} - u_{i+1} \\right) = \\frac{2}{h^2} u_i - \\frac{1}{h^2}(u_{i-1} + u_{i+1}) $$\nThe diagonal part of $A$, denoted by $D$, consists of all terms that multiply $u_i$. Therefore, the action of $D$ on a grid function $u$ is given by\n$$ (D u)_i = \\frac{2}{h^2} u_i $$\nThis means $D$ is a scalar multiple of the identity operator, $D = \\frac{2}{h^2} I$. The inverse operator $D^{-1}$ is then $D^{-1} = \\frac{h^2}{2} I$.\n\nStep 2: Derive the symbol of the error propagation operator $S_{\\omega}$.\nLocal Fourier Analysis examines the action of a grid operator on a single Fourier mode $e_i(\\theta) = \\exp(\\mathrm{i} \\theta i)$, where $\\theta \\in [-\\pi, \\pi]$ is the wavenumber. The eigenvalue of the operator corresponding to this mode is called its symbol.\nLet's find the symbol of $A$, denoted $\\hat{A}(\\theta)$. Applying $A$ to $e_i(\\theta)$:\n$$ (A e(\\theta))_i = \\frac{1}{h^2} \\left( - e_{i-1}(\\theta) + 2 e_i(\\theta) - e_{i+1}(\\theta) \\right) $$\nUsing the property that $e_{i \\pm 1}(\\theta) = \\exp(\\mathrm{i} \\theta (i \\pm 1)) = \\exp(\\pm \\mathrm{i} \\theta) e_i(\\theta)$, we get\n$$ (A e(\\theta))_i = \\frac{1}{h^2} \\left( - \\exp(-\\mathrm{i} \\theta) + 2 - \\exp(\\mathrm{i} \\theta) \\right) e_i(\\theta) = \\frac{1}{h^2} \\left( 2 - (\\exp(\\mathrm{i} \\theta) + \\exp(-\\mathrm{i} \\theta)) \\right) e_i(\\theta) $$\nUsing Euler's formula, $\\cos(\\theta) = \\frac{1}{2}(\\exp(\\mathrm{i} \\theta) + \\exp(-\\mathrm{i} \\theta))$, this simplifies to\n$$ (A e(\\theta))_i = \\frac{2}{h^2} (1 - \\cos(\\theta)) e_i(\\theta) $$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2 \\sin^2(\\theta/2)$, we find the symbol of $A$:\n$$ \\hat{A}(\\theta) = \\frac{4}{h^2} \\sin^2\\left(\\frac{\\theta}{2}\\right) $$\nThe error propagation operator for weighted Jacobi is $S_{\\omega} = I - \\omega D^{-1} A$. The symbol of $S_{\\omega}$, denoted $\\hat{S}_{\\omega}(\\theta)$, is found by replacing each operator with its symbol. The symbol of $I$ is $1$, and the symbol of $D^{-1} = \\frac{h^2}{2} I$ is $\\frac{h^2}{2}$.\n$$ \\hat{S}_{\\omega}(\\theta) = 1 - \\omega \\left(\\frac{h^2}{2}\\right) \\hat{A}(\\theta) = 1 - \\omega \\left(\\frac{h^2}{2}\\right) \\left( \\frac{4}{h^2} \\sin^2\\left(\\frac{\\theta}{2}\\right) \\right) = 1 - 2\\omega \\sin^2\\left(\\frac{\\theta}{2}\\right) $$\nThis is the amplification factor of the Fourier mode with wavenumber $\\theta$ after one step of weighted Jacobi smoothing.\n\nStep 3: Determine the restricted spectral radius.\nThe goal of a smoother is to damp high-frequency error components. In this context, the high-frequency subspace corresponds to wavenumbers $\\theta$ that cannot be represented on a coarser grid, i.e., $\\theta \\in [\\pi/2, \\pi]$. We need to find the spectral radius of $S_{\\omega}$ restricted to this subspace, which is the maximum absolute value of its symbol over this range. Let this be $\\mu(\\omega)$.\n$$ \\mu(\\omega) = \\max_{\\theta \\in [\\pi/2, \\pi]} \\left| \\hat{S}_{\\omega}(\\theta) \\right| = \\max_{\\theta \\in [\\pi/2, \\pi]} \\left| 1 - 2\\omega \\sin^2\\left(\\frac{\\theta}{2}\\right) \\right| $$\nLet $x = \\sin^2(\\theta/2)$. For $\\theta \\in [\\pi/2, \\pi]$, the argument $\\theta/2$ ranges from $\\pi/4$ to $\\pi/2$. The function $\\sin(\\phi)$ is monotonically increasing on this interval.\nAt $\\theta = \\pi/2$, $x = \\sin^2(\\pi/4) = (\\frac{\\sqrt{2}}{2})^2 = \\frac{1}{2}$.\nAt $\\theta = \\pi$, $x = \\sin^2(\\pi/2) = 1^2 = 1$.\nSo, the range for $x$ is $[1/2, 1]$. The problem reduces to\n$$ \\mu(\\omega) = \\max_{x \\in [1/2, 1]} |1 - 2\\omega x| $$\nThe function $g(x) = 1 - 2\\omega x$ is linear in $x$. For a positive relaxation parameter $\\omega$, it is a decreasing function. Therefore, its absolute value on the interval $[1/2, 1]$ will be maximized at one of the endpoints, $x=1/2$ or $x=1$.\nAt $x=1/2$: $|g(1/2)| = |1 - 2\\omega(1/2)| = |1 - \\omega|$.\nAt $x=1$: $|g(1)| = |1 - 2\\omega(1)| = |1 - 2\\omega|$.\nSo, the restricted spectral radius is $\\mu(\\omega) = \\max(|1-\\omega|, |1-2\\omega|)$.\n\nStep 4: Minimize the spectral radius.\nWe want to find the value of $\\omega \\in (0,1)$ that minimizes $\\mu(\\omega) = \\max(|1-\\omega|, |1-2\\omega|)$.\nGiven $\\omega \\in (0,1)$, we know that $1-\\omega > 0$, so $|1-\\omega| = 1-\\omega$.\nFor the term $|1-2\\omega|$, we consider two cases:\n1. If $\\omega \\in (0, 1/2]$, then $1-2\\omega \\ge 0$, so $|1-2\\omega| = 1-2\\omega$. In this range, $1-\\omega \\ge 1-2\\omega$, so $\\mu(\\omega) = 1-\\omega$. This function is decreasing on $(0, 1/2]$.\n2. If $\\omega \\in (1/2, 1)$, then $1-2\\omega < 0$, so $|1-2\\omega| = -(1-2\\omega) = 2\\omega-1$. In this range, $\\mu(\\omega) = \\max(1-\\omega, 2\\omega-1)$.\n\nTo find the minimum of $\\mu(\\omega)$, we look for the point where the two functions $1-\\omega$ and $2\\omega-1$ are equal, as one is decreasing and the other is increasing.\n$$ 1 - \\omega = 2\\omega - 1 \\implies 3\\omega = 2 \\implies \\omega = \\frac{2}{3} $$\nThis value $\\omega = 2/3$ lies in the interval $(1/2, 1)$, so this is a valid candidate for the minimum.\nLet's analyze the behavior of $\\mu(\\omega)$ on $(1/2, 1)$:\n- If $\\omega \\in (1/2, 2/3)$, then $3\\omega < 2$, which implies $1-\\omega > 2\\omega-1$. So $\\mu(\\omega) = 1-\\omega$, which is a decreasing function.\n- If $\\omega \\in (2/3, 1)$, then $3\\omega > 2$, which implies $1-\\omega < 2\\omega-1$. So $\\mu(\\omega) = 2\\omega-1$, which is an increasing function.\n\nCombining all segments, the function $\\mu(\\omega)$ is decreasing on $(0, 2/3)$ and increasing on $(2/3, 1)$. Therefore, the global minimum of $\\mu(\\omega)$ for $\\omega \\in (0,1)$ is achieved at $\\omega = 2/3$.\nThe minimum value of the spectral radius is $\\mu(2/3) = 1 - 2/3 = 1/3$.\nThe problem asks for the value of $\\omega$ that minimizes this radius.",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "After analyzing a smoother's properties theoretically, the next logical step is to observe and quantify its behavior in a computational setting. This exercise  moves from analysis to implementation, asking you to code the widely-used Gauss-Seidel smoother. By applying it to specific Fourier error modes on a grid, you will numerically verify the core principle of smoothing: the rapid damping of high-frequency errors, which are invisible to coarser grids, leaving behind smooth, low-frequency errors that can be efficiently solved on a smaller scale.",
            "id": "3480320",
            "problem": "Consider the two-dimensional Poisson equation on the unit square with homogeneous Dirichlet boundary conditions, written as $$\\nabla^2 u(x,y) = 0 \\quad \\text{for} \\quad (x,y)\\in(0,1)\\times(0,1), \\quad \\text{with} \\quad u(x,y)=0 \\quad \\text{on} \\quad \\partial\\Omega.$$ Use the standard second-order central difference scheme on an $N\\times N$ interior grid with uniform spacing $h=1/(N+1)$ to obtain the discrete linear system $$A \\mathbf{u} = \\mathbf{f},$$ where $A$ is the five-point discrete Laplacian operator and $\\mathbf{u}$ is the array of interior unknowns. In multigrid methods, Gauss-Seidel (GS) relaxation is employed as a smoother to damp high-frequency error components. The smoothing property can be quantified by the amplification factor of Fourier-like error modes.\n\nYour task is to start from the basic discrete operator and the linear iteration splitting for Gauss-Seidel, derive the pointwise update for lexicographic Gauss-Seidel relaxation on the homogeneous system (that is, with $\\mathbf{f}=\\mathbf{0}$ so that the exact solution is $\\mathbf{u}^\\ast=\\mathbf{0}$), and then implement two pre-smoothing and two post-smoothing steps (total of four GS sweeps) applied to an initial error given by a single discrete sine mode. Specifically:\n\n- Construct the discrete sine mode $$\\phi_{i,j}^{(k,\\ell)} = \\sin\\!\\bigg(\\frac{k\\pi i}{N+1}\\bigg)\\,\\sin\\!\\bigg(\\frac{\\ell\\pi j}{N+1}\\bigg),\\quad 1\\le i,j\\le N,$$ and normalize it in the discrete $\\ell^2$ norm so that $$\\sum_{i=1}^N\\sum_{j=1}^N \\left(\\phi_{i,j}^{(k,\\ell)}\\right)^2 = 1.$$ Use this normalized mode as the initial error $\\mathbf{e}^{(0)}=\\phi^{(k,\\ell)}$.\n\n- Apply lexicographic Gauss-Seidel relaxation derived from the matrix splitting of the discrete Laplacian to the homogeneous system $A\\mathbf{u}=\\mathbf{0}$ for a total of four sweeps, starting from $\\mathbf{u}^{(0)}=\\mathbf{e}^{(0)}$. Let $\\mathbf{u}^{(4)}$ denote the result after four sweeps.\n\n- Quantify the reduction in the selected mode by projecting the final error $\\mathbf{u}^{(4)}$ onto the original mode and defining the mode-specific amplification factor as $$\\alpha_{k,\\ell}(N) = \\left|\\frac{\\sum_{i=1}^N\\sum_{j=1}^N \\phi_{i,j}^{(k,\\ell)}\\,u^{(4)}_{i,j}}{\\sum_{i=1}^N\\sum_{j=1}^N \\phi_{i,j}^{(k,\\ell)}\\,u^{(0)}_{i,j}}\\right|.$$ Because the initial mode is normalized, the denominator equals $1$ up to numerical roundoff. Report the absolute value to measure damping irrespective of any phase inversion.\n\nUse this construction to evaluate the amplification factor for a set of representative modes that probe high-frequency and anisotropic behavior. Your program must implement the lexicographic Gauss-Seidel update starting from first principles (the discrete operator and the iteration splitting) and compute the amplification factors for the following test suite:\n\n- Case 1 (happy path, low-frequency reference): $N=32$, $k=1$, $\\ell=1$.\n- Case 2 (mid-high-frequency, isotropic): $N=32$, $k=16$, $\\ell=16$.\n- Case 3 (near-Nyquist, isotropic extreme): $N=32$, $k=32$, $\\ell=32$.\n- Case 4 (anisotropic high-frequency in $x$): $N=32$, $k=32$, $\\ell=1$.\n- Case 5 (larger grid, mid-high-frequency, isotropic): $N=64$, $k=32$, $\\ell=32$.\n- Case 6 (small grid edge case, near-Nyquist): $N=4$, $k=4$, $\\ell=4$.\n\nAll computations are purely numerical with no physical units. Angles in the sine modes are in radians. Your program should produce a single line of output containing the six amplification factors as a comma-separated list enclosed in square brackets, rounded to six decimal places, in the same order as the cases above. For example, the output format must be exactly like \"[result1,result2,result3,result4,result5,result6]\".",
            "solution": "The user has provided a problem in the domain of numerical analysis, specifically concerning the evaluation of a Gauss-Seidel smoother's performance, which is a key component of multigrid solvers for elliptic partial differential equations. The problem is scientifically and mathematically sound, well-posed, and all necessary parameters and definitions are provided for its resolution.\n\nThe core of the problem is to numerically evaluate the damping properties of a lexicographic Gauss-Seidel (GS) relaxation scheme for the two-dimensional Poisson equation. This is achieved by measuring the reduction of specific error modes after a fixed number of GS sweeps.\n\nThe problem begins with the homogeneous Poisson equation on a unit square, $\\nabla^2 u(x,y) = 0$, with homogeneous Dirichlet boundary conditions $u=0$ on the boundary. We discretize this equation on a uniform grid with $N \\times N$ interior points and grid spacing $h=1/(N+1)$. The standard five-point central difference approximation to the Laplacian at a grid point $(i,j)$ is:\n$$ (L_h u)_{i,j} = \\frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j}}{h^2} $$\nSetting $(L_h u)_{i,j} = 0$ for all interior points $1 \\le i,j \\le N$ gives the discrete linear system $A\\mathbf{u} = \\mathbf{0}$. The equation for each point $(i,j)$ is:\n$$ 4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = 0 $$\nwhere the factor of $h^2$ has been dropped as it scales the entire system.\n\nGauss-Seidel is an iterative method for solving such linear systems. For a system $A\\mathbf{u}=\\mathbf{f}$, the matrix $A$ is split into its diagonal ($D$), strict lower triangular ($L$), and strict upper triangular ($U$) parts, $A=L+D+U$. The GS iteration is then defined as $(L+D)\\mathbf{u}^{(m+1)} = \\mathbf{f} - U\\mathbf{u}^{(m)}$. In our case, the right-hand side is zero, so $(L+D)\\mathbf{u}^{(m+1)} = -U\\mathbf{u}^{(m)}$.\n\nThis matrix form can be expressed as a pointwise update rule. To compute the new value $u_{i,j}^{(m+1)}$ at iteration step $m+1$, we rearrange the discrete equation:\n$$ u_{i,j} = \\frac{1}{4}\\left( u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} \\right) $$\nIn a lexicographic (row-by-row, column-by-column) sweep, when we update $u_{i,j}$, the values for \"past\" neighbors (e.g., $u_{i-1,j}$ and $u_{i,j-1}$) have already been updated in the current sweep, while values for \"future\" neighbors (e.g., $u_{i+1,j}$ and $u_{i,j+1}$) are from the previous sweep. This leads to the update rule:\n$$ u_{i,j}^{(m+1)} = \\frac{1}{4}\\left( u_{i-1,j}^{(m+1)} + u_{i,j-1}^{(m+1)} + u_{i+1,j}^{(m)} + u_{i,j+1}^{(m)} \\right) $$\nThis is implemented by iterating through the grid points $i=1,\\dots,N$ and $j=1,\\dots,N$ and updating the grid values in place.\n\nThe initial error is given by a single discrete sine mode, which are the eigenfunctions of the discrete Laplacian operator on this grid:\n$$ \\phi_{i,j}^{(k,\\ell)} = \\sin\\!\\bigg(\\frac{k\\pi i}{N+1}\\bigg)\\,\\sin\\!\\bigg(\\frac{\\ell\\pi j}{N+1}\\bigg), \\quad 1 \\le i,j,k,\\ell \\le N $$\nThis initial mode $\\mathbf{e}^{(0)}$ is first normalized in the discrete $\\ell^2$ norm, yielding $\\boldsymbol{\\phi}^{(k,\\ell)}$ such that $\\sum_{i,j} (\\phi_{i,j}^{(k,\\ell)})^2 = 1$. This normalized mode serves as the initial state of the grid for the iteration, $\\mathbf{u}^{(0)} = \\boldsymbol{\\phi}^{(k,\\ell)}$.\n\nWe then apply four full sweeps of the lexicographic Gauss-Seidel relaxation to this initial state, producing the final state $\\mathbf{u}^{(4)}$. The problem asks to quantify the damping of the original mode by projecting the final error field $\\mathbf{u}^{(4)}$ back onto the initial normalized mode $\\boldsymbol{\\phi}^{(k,\\ell)}$. The mode-specific amplification factor is defined as:\n$$ \\alpha_{k,\\ell}(N) = \\left|\\frac{\\langle \\mathbf{u}^{(4)}, \\boldsymbol{\\phi}^{(k,\\ell)} \\rangle}{\\langle \\mathbf{u}^{(0)}, \\boldsymbol{\\phi}^{(k,\\ell)} \\rangle}\\right| $$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the discrete inner product $\\sum_{i,j} a_{i,j}b_{i,j}$. Since $\\mathbf{u}^{(0)} = \\boldsymbol{\\phi}^{(k,\\ell)}$ and $\\boldsymbol{\\phi}^{(k,\\ell)}$ is normalized, the denominator is $\\langle \\boldsymbol{\\phi}^{(k,\\ell)}, \\boldsymbol{\\phi}^{(k,\\ell)} \\rangle=1$. Therefore, the amplification factor simplifies to the magnitude of the projection:\n$$ \\alpha_{k,\\ell}(N) = \\left| \\sum_{i=1}^N\\sum_{j=1}^N u^{(4)}_{i,j}\\,\\phi_{i,j}^{(k,\\ell)} \\right| $$\nThis procedure is carried out numerically for each of the specified test cases $(N,k,\\ell)$. An $(N+2) \\times (N+2)$ grid is used to naturally incorporate the homogeneous Dirichlet boundary conditions by fixing the values on the border to $0$. The iteration and projection are performed only on the $N \\times N$ interior grid points.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Gauss-Seidel amplification factor for specific sine modes.\n\n    This function iterates through a set of test cases, each defined by a grid\n    size N and mode numbers (k, l). For each case, it:\n    1. Constructs the initial error as a normalized discrete sine mode.\n    2. Applies four sweeps of lexicographic Gauss-Seidel relaxation.\n    3. Computes the amplification factor by projecting the final error onto\n       the initial mode.\n    The results are collected and printed in the specified format.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (32, 1, 1),    # Case 1: Low-frequency reference\n        (32, 16, 16),  # Case 2: Mid-high-frequency, isotropic\n        (32, 32, 32),  # Case 3: Near-Nyquist, isotropic extreme\n        (32, 32, 1),   # Case 4: Anisotropic high-frequency\n        (64, 32, 32),  # Case 5: Larger grid, mid-high-frequency\n        (4, 4, 4),     # Case 6: Small grid edge case, near-Nyquist\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, k, l = case\n\n        # Grid spacing h.\n        h = 1.0 / (N + 1)\n        \n        # 1. Construct the initial error mode on an (N+2)x(N+2) grid to\n        #    include boundaries. Boundaries are initialized to 0.\n        \n        # Create coordinate arrays for the interior grid.\n        i_vals = np.arange(1, N + 1)\n        j_vals = np.arange(1, N + 1)\n        ii, jj = np.meshgrid(i_vals, j_vals, indexing='ij')\n\n        # Calculate the un-normalized sine mode on the interior.\n        phi_interior = np.sin(k * np.pi * ii * h) * np.sin(l * np.pi * jj * h)\n\n        # 2. Normalize the mode in the discrete l2 norm.\n        norm = np.linalg.norm(phi_interior)\n        \n        # Check for zero norm to prevent division by zero, although not expected here.\n        if norm == 0:\n            normalized_phi_interior = phi_interior\n        else:\n            normalized_phi_interior = phi_interior / norm\n\n        # The initial state u_0 is the normalized mode.\n        # This grid will be modified by the GS sweeps.\n        u = np.zeros((N + 2, N + 2))\n        u[1:N+1, 1:N+1] = normalized_phi_interior\n\n        # Store a copy of the normalized initial mode for the final projection.\n        phi_normalized = u.copy()\n        \n        # 3. Apply four sweeps of lexicographic Gauss-Seidel relaxation.\n        num_sweeps = 4\n        for _ in range(num_sweeps):\n            # The loops must be in lexicographic order (row-by-row, col-by-col).\n            # The update is done in-place, which is the definition of GS.\n            for i in range(1, N + 1):\n                for j in range(1, N + 1):\n                    # Pointwise update rule derived from the 5-point stencil.\n                    u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n\n        # 4. Quantify the amplification factor.\n        # The final state of the grid is u (which is u^(4)).\n        # We project u^(4) onto the initial normalized mode phi_normalized.\n        # The denominator is 1 due to normalization.\n        numerator = np.sum(phi_normalized * u)\n        \n        amplification_factor = np.abs(numerator)\n        \n        results.append(amplification_factor)\n\n    # Final print statement in the exact required format.\n    # Round results to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust multigrid solver requires more than just an effective smoother; it also needs an accurate representation of the problem on the coarser grids. For problems with variable coefficients, such as those found in the constraint equations of numerical relativity, simply re-discretizing the PDE on the coarse grid can be inaccurate. This advanced practice  delves into the construction of the coarse-grid operator by implementing and comparing the straightforward rediscretization method with the more sophisticated and robust Galerkin approach, $A_c = R A_f P$, demonstrating why the latter is often essential for real-world applications.",
            "id": "3480295",
            "problem": "Consider the variable-coefficient Poisson-like operator that arises in elliptic constraints of numerical relativity, for example in constructing gravitational wave initial data, given by the partial differential equation $- \\nabla \\cdot (\\kappa(x,y) \\nabla u(x,y)) = f(x,y)$ on the open unit square $\\Omega = (0,1) \\times (0,1)$ with homogeneous Dirichlet boundary conditions $u(x,y) = 0$ on $\\partial \\Omega$. Define a uniform nodal grid of size $N \\times N$ (including boundary nodes) with spacing $h = 1/(N-1)$ and interior index set $\\{1,2,\\dots,N-2\\} \\times \\{1,2,\\dots,N-2\\}$. Use the conservative second-order finite-difference discretization based on face-centered coefficients to construct a symmetric positive-definite matrix operator $A_f \\in \\mathbb{R}^{n_f \\times n_f}$ with $n_f = (N_f-2)^2$ acting on the fine-grid interior unknown vector, where $N_f$ is the fine-grid nodal count.\n\nThe discrete operator at an interior node $(i,j)$ is defined by the flux form\n$$\n(L_h u)_{i,j} = -\\frac{1}{h^2} \\Big( \\kappa_{i+\\frac{1}{2},j} (u_{i+1,j} - u_{i,j}) - \\kappa_{i-\\frac{1}{2},j} (u_{i,j} - u_{i-1,j}) + \\kappa_{i,j+\\frac{1}{2}} (u_{i,j+1} - u_{i,j}) - \\kappa_{i,j-\\frac{1}{2}} (u_{i,j} - u_{i,j-1}) \\Big),\n$$\nwhere the face-centered coefficients are given by $\\kappa_{i+\\frac{1}{2},j} = \\kappa(x_{i+\\frac{1}{2}}, y_j)$ and $\\kappa_{i,j+\\frac{1}{2}} = \\kappa(x_i, y_{j+\\frac{1}{2}})$, with $x_{i+\\frac{1}{2}} = x_i + h/2$ and $y_{j+\\frac{1}{2}} = y_j + h/2$. The continuous coefficient is defined as\n$$\n\\kappa(x,y) = 1 + a \\sin(2\\pi m x)\\cos(2\\pi m y),\n$$\nwith amplitude $a \\in [0,1)$ and integer frequency $m \\geq 1$, ensuring $\\kappa(x,y) > 0$.\n\nLet the coarse grid be geometrically nested with nodal count $N_c = (N_f + 1)/2$, spacing $H = 1/(N_c-1)$, and interior size $n_c = (N_c - 2)^2$. Define the standard bilinear interpolation (prolongation) operator $P \\in \\mathbb{R}^{n_f \\times n_c}$ from coarse interior unknowns to fine interior unknowns as follows: for each fine interior node $(i,j)$, the interpolated value is the bilinear combination of the surrounding coarse-grid nodal values; when a required coarse-grid node lies on the boundary, its contribution is zero due to homogeneous Dirichlet boundary conditions. Define the full-weighting restriction operator $R \\in \\mathbb{R}^{n_c \\times n_f}$ using the $3 \\times 3$ stencil weights\n$$\n\\frac{1}{16}\n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 1\n\\end{bmatrix}\n$$\ncentered at the fine node that coincides with the coarse node.\n\nUsing these, the Galerkin coarse-grid operator is $A_c^{\\mathrm{Gal}} = R A_f P \\in \\mathbb{R}^{n_c \\times n_c}$. The rediscretized coarse-grid operator $A_c^{\\mathrm{Red}} \\in \\mathbb{R}^{n_c \\times n_c}$ is obtained by applying the same conservative second-order scheme on the coarse grid with spacing $H$ and evaluating the continuous $\\kappa(x,y)$ at coarse faces.\n\nYour tasks:\n- Construct $A_f$ on the fine grid using the specified flux-form discretization with face-centered coefficients evaluated from $\\kappa(x,y)$.\n- Construct $P$ using bilinear interpolation consistent with homogeneous Dirichlet boundaries and $R$ using full weighting as specified.\n- Compute the Galerkin coarse operator $A_c^{\\mathrm{Gal}} = R A_f P$.\n- Construct $A_c^{\\mathrm{Red}}$ by rediscretization on the coarse grid using the same scheme and face-centered coefficients from the same $\\kappa(x,y)$.\n- For each test case, compute the relative Frobenius-norm difference\n$$\n\\delta = \\frac{\\lVert A_c^{\\mathrm{Gal}} - A_c^{\\mathrm{Red}} \\rVert_F}{\\lVert A_c^{\\mathrm{Red}} \\rVert_F}.\n$$\n\nFundamental base requirements for derivation and algorithm:\n- Begin from the definition of the conservative second-order discretization for $- \\nabla \\cdot (\\kappa \\nabla u)$ on a structured grid.\n- Use the geometric multigrid definitions of Galerkin coarse-grid operator, bilinear interpolation, and full-weighting restriction.\n- Ensure consistency with homogeneous Dirichlet boundary conditions through zero boundary contributions where applicable.\n\nTest suite:\n- Case 1 (general variable coefficient, moderate grid): $N_f = 33$, $a = 0.5$, $m = 1$.\n- Case 2 (constant coefficient, baseline): $N_f = 33$, $a = 0.0$, $m = 1$.\n- Case 3 (variable coefficient, smaller grid): $N_f = 17$, $a = 0.9$, $m = 1$.\n- Case 4 (higher-frequency coefficient, moderate grid): $N_f = 33$, $a = 0.7$, $m = 4$.\n\nOutput specification:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets of the four floating-point values $\\delta$ corresponding to the four test cases in the order listed above. For example, the format must be exactly like $[\\delta_1,\\delta_2,\\delta_3,\\delta_4]$, with each $\\delta$ printed as a Python float.",
            "solution": "The problem requires a comparison between two different coarse-grid operators used in multigrid methods for solving a variable-coefficient elliptic partial differential equation. The comparison is based on the relative Frobenius-norm difference between the Galerkin coarse-grid operator, $A_c^{\\mathrm{Gal}}$, and the rediscretized coarse-grid operator, $A_c^{\\mathrm{Red}}$.\n\nThe problem is well-posed and scientifically grounded in the field of numerical analysis, particularly as it applies to solving elliptic constraint equations in numerical relativity. All parameters and definitions are provided, allowing for a direct and unambiguous implementation. I will proceed with a systematic construction of the required matrices.\n\n### 1. Discretization of the Continuous Operator\n\nThe continuous problem is to solve the partial differential equation (PDE)\n$$\n- \\nabla \\cdot (\\kappa(x,y) \\nabla u(x,y)) = f(x,y)\n$$\non the unit square $\\Omega = (0,1) \\times (0,1)$ with homogeneous Dirichlet boundary conditions $u(x,y) = 0$ on the boundary $\\partial \\Omega$. The coefficient $\\kappa(x,y)$ is given by $\\kappa(x,y) = 1 + a \\sin(2\\pi m x)\\cos(2\\pi m y)$.\n\nWe discretize this equation on a uniform grid with $N \\times N$ nodes and spacing $h = 1/(N-1)$. Let $u_{i,j}$ denote the value of the solution at the grid node $(x_i, y_j) = (ih, jh)$. The problem specifies a conservative second-order finite-difference scheme. The discrete operator $L_h$ at an interior node $(i,j)$, for $i,j \\in \\{1, \\dots, N-2\\}$, is\n$$\n(L_h u)_{i,j} = -\\frac{1}{h^2} \\Big( \\kappa_{i+\\frac{1}{2},j} (u_{i+1,j} - u_{i,j}) - \\kappa_{i-\\frac{1}{2},j} (u_{i,j} - u_{i-1,j}) + \\kappa_{i,j+\\frac{1}{2}} (u_{i,j+1} - u_{i,j}) - \\kappa_{i,j-\\frac{1}{2}} (u_{i,j} - u_{i,j-1}) \\Big).\n$$\nThe face-centered coefficients are evaluations of $\\kappa(x,y)$ at midpoints between nodes, e.g., $\\kappa_{i+\\frac{1}{2},j} = \\kappa(x_i + h/2, y_j)$.\n\nThe discrete system of equations is $A \\mathbf{u} = \\mathbf{f}$, where $\\mathbf{u}$ is the vector of unknown values at the $n = (N-2)^2$ interior grid points. The matrix $A$ represents the negative of the discrete operator, $-L_h$. By re-arranging the terms of $(L_h u)_{i,j}$, we can identify the stencil for the matrix $A$:\n$$\n(A \\mathbf{u})_{i,j} = \\frac{1}{h^2} \\Big[ (\\kappa_{i+\\frac{1}{2},j} + \\kappa_{i-\\frac{1}{2},j} + \\kappa_{i,j+\\frac{1}{2}} + \\kappa_{i,j-\\frac{1}{2}}) u_{i,j} - \\kappa_{i+\\frac{1}{2},j} u_{i+1,j} - \\kappa_{i-\\frac{1}{2},j} u_{i-1,j} - \\kappa_{i,j+\\frac{1}{2}} u_{i,j+1} - \\kappa_{i,j-\\frac{1}{2}} u_{i,j-1} \\Big].\n$$\nThe vector $\\mathbf{u}$ is formed by arranging the $u_{i,j}$ values in a specific order, typically row-major. For interior indices $i,j \\in \\{1, \\dots, N-2\\}$, the mapping from a 2D grid index $(i,j)$ to a 1D vector index $k$ is $k = (i-1)(N-2) + (j-1)$. This structure defines a sparse, symmetric positive-definite matrix $A$ of size $n \\times n$. The homogeneous Dirichlet boundary conditions are handled by noting that any $u_{i,j}$ where $i$ or $j$ is $0$ or $N-1$ is zero, and thus these terms drop out of the equations for interior nodes adjacent to the boundary.\n\nBoth the fine-grid operator $A_f$ and the coarse-grid rediscretized operator $A_c^{\\mathrm{Red}}$ are constructed using this method, but with different grid parameters ($N_f, h$) and ($N_c, H$), respectively.\n\n### 2. Inter-Grid Transfer Operators: Prolongation and Restriction\n\nWe are given a fine grid with $N_f$ points per dimension and a coarse grid with $N_c = (N_f+1)/2$ points. The corresponding interior problem sizes are $n_f = (N_f-2)^2$ and $n_c = (N_c-2)^2$.\n\n**Prolongation (Interpolation) $P$**: The prolongation operator $P \\in \\mathbb{R}^{n_f \\times n_c}$ maps a vector of coarse-grid interior values to a vector of fine-grid interior values. We use bilinear interpolation. For a fine-grid interior point $(i_f, j_f)$, its value is an interpolation of the surrounding four coarse-grid points. The weights depend on the position of the fine point relative to the coarse grid.\n- A fine point coinciding with a coarse point $(2i_c, 2j_c)$ takes the value from that coarse point.\n- A fine point on a grid edge, e.g., $(2i_c+1, 2j_c)$, is the average of two adjacent coarse points.\n- A fine point at a cell center, $(2i_c+1, 2j_c+1)$, is the average of four surrounding coarse points.\nContributions from coarse-grid boundary points are zero due to the homogeneous Dirichlet conditions. Each row of the matrix $P$ is constructed by determining these weights for the corresponding fine-grid point.\n\n**Restriction $R$**: The restriction operator $R \\in \\mathbb{R}^{n_c \\times n_f}$ maps a fine-grid vector to a coarse-grid vector. We use full-weighting restriction. The value at a coarse-grid interior point $(i_c, j_c)$ is a weighted average of the values at the $3 \\times 3$ block of fine-grid points centered at its corresponding fine-grid location $(2i_c, 2j_c)$. The weights are given by the stencil:\n$$\n\\frac{1}{16}\n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 1\n\\end{bmatrix}.\n$$\nAs with prolongation, fine-grid points on the boundary have a value of zero and do not contribute to the sum.\n\n### 3. Coarse-Grid Operators and Comparison\n\nTwo methods are used to define the coarse-grid operator.\n\n**Rediscretized Operator $A_c^{\\mathrm{Red}}$**: This operator is constructed by applying the same finite-difference discretization scheme directly on the coarse grid with spacing $H = 1/(N_c-1)$. The coefficient function $\\kappa(x,y)$ is evaluated at the face centers of the coarse-grid cells. This approach is simple but may fail to capture fine-scale variations in $\\kappa(x,y)$ that are aliased on the coarse grid.\n\n**Galerkin Operator $A_c^{\\mathrm{Gal}}$**: This operator is defined by the so-called Galerkin projection: $A_c^{\\mathrm{Gal}} = R A_f P$. It represents the fine-grid operator as \"viewed\" from the coarse grid. This construction is more computationally expensive but is guaranteed to capture the properties of the fine-grid operator $A_f$, making it robust for problems with complex or rapidly varying coefficients.\n\nThe fundamental difference between these two operators is the source of the numerical error. $A_c^{\\mathrm{Red}}$ inherits its properties from the truncation error of the discretization on the coarse grid. $A_c^{\\mathrm{Gal}}$ inherits its properties from the fine-grid operator via projection. For a constant coefficient $\\kappa$, the two are not identical; $A_c^{\\mathrm{Red}}$ typically has a 5-point stencil, while $A_c^{\\mathrm{Gal}}$ has a 9-point stencil. For variable $\\kappa$, the difference can be more pronounced.\n\nThe quantity to compute is the relative Frobenius-norm difference:\n$$\n\\delta = \\frac{\\lVert A_c^{\\mathrm{Gal}} - A_c^{\\mathrm{Red}} \\rVert_F}{\\lVert A_c^{\\mathrm{Red}} \\rVert_F},\n$$\nwhere $\\lVert M \\rVert_F = \\sqrt{\\sum_{i,j} |M_{ij}|^2}$. This metric quantifies the discrepancy between the two coarse-grid operator definitions.\n\nThe algorithm proceeds by implementing functions to construct each of the four matrices ($A_f$, $A_c^{\\mathrm{Red}}$, $P$, $R$) for the given parameters, then performing the matrix multiplication to obtain $A_c^{\\mathrm{Gal}}$, and finally computing $\\delta$. This process is repeated for each of the four test cases.",
            "answer": "```python\nimport numpy as np\n\ndef kappa(x, y, a, m):\n    \"\"\"Computes the continuous coefficient kappa(x,y).\"\"\"\n    return 1.0 + a * np.sin(2 * np.pi * m * x) * np.cos(2 * np.pi * m * y)\n\ndef construct_operator(N, a, m):\n    \"\"\"\n    Constructs the discrete operator matrix A for a given grid size N\n    and coefficient parameters a, m.\n    \"\"\"\n    if N <= 2:\n        return np.array([[]])\n    \n    n = (N - 2)**2\n    h = 1.0 / (N - 1)\n    A = np.zeros((n, n))\n    \n    width = N - 2\n    coords = np.linspace(0, 1, N)\n\n    for i in range(1, N - 1):      # 1-based row index\n        for j in range(1, N - 1):  # 1-based column index\n            k = (i - 1) * width + (j - 1) # 0-based 1D index\n            \n            x_i, y_j = coords[i], coords[j]\n            \n            k_ip12 = kappa(x_i + h / 2.0, y_j, a, m)\n            k_im12 = kappa(x_i - h / 2.0, y_j, a, m)\n            k_jp12 = kappa(x_i, y_j + h / 2.0, a, m)\n            k_jm12 = kappa(x_i, y_j - h / 2.0, a, m)\n\n            # Diagonal entry\n            A[k, k] = (k_ip12 + k_im12 + k_jp12 + k_jm12) / h**2\n            \n            # Off-diagonal entries (connections to neighbors)\n            # Connection to u_{i, j+1} (North)\n            if j < N - 2:\n                A[k, k + 1] = -k_jp12 / h**2\n            # Connection to u_{i, j-1} (South)\n            if j > 1:\n                A[k, k - 1] = -k_jm12 / h**2\n            # Connection to u_{i+1, j} (East)\n            if i < N - 2:\n                A[k, k + width] = -k_ip12 / h**2\n            # Connection to u_{i-1, j} (West)\n            if i > 1:\n                A[k, k - width] = -k_im12 / h**2\n                \n    return A\n\ndef construct_prolongation(N_f, N_c):\n    \"\"\"Constructs the bilinear interpolation (prolongation) operator P.\"\"\"\n    n_f = (N_f - 2)**2\n    n_c = (N_c - 2)**2\n    if n_f == 0 or n_c == 0:\n        return np.zeros((n_f, n_c))\n\n    P = np.zeros((n_f, n_c))\n    width_f = N_f - 2\n    width_c = N_c - 2\n    \n    for i_f in range(1, N_f - 1):\n        for j_f in range(1, N_f - 1):\n            k_f = (i_f - 1) * width_f + (j_f - 1)\n            \n            is_i_f_even = (i_f % 2 == 0)\n            is_j_f_even = (j_f % 2 == 0)\n\n            if is_i_f_even and is_j_f_even:\n                # Type 1: Fine node coincides with a coarse node\n                i_c, j_c = i_f // 2, j_f // 2\n                if 1 <= i_c <= N_c - 2 and 1 <= j_c <= N_c - 2:\n                    k_c = (i_c - 1) * width_c + (j_c - 1)\n                    P[k_f, k_c] = 1.0\n\n            elif not is_i_f_even and is_j_f_even:\n                # Type 2: Fine node on a vertical coarse-grid edge\n                j_c = j_f // 2\n                i_c_lo = (i_f - 1) // 2\n                i_c_hi = i_c_lo + 1\n                if 1 <= j_c <= N_c - 2:\n                    if 1 <= i_c_lo <= N_c - 2:\n                        k_c1 = (i_c_lo - 1) * width_c + (j_c - 1)\n                        P[k_f, k_c1] = 0.5\n                    if 1 <= i_c_hi <= N_c - 2:\n                        k_c2 = (i_c_hi - 1) * width_c + (j_c - 1)\n                        P[k_f, k_c2] = 0.5\n            \n            elif is_i_f_even and not is_j_f_even:\n                # Type 3: Fine node on a horizontal coarse-grid edge\n                i_c = i_f // 2\n                j_c_lo = (j_f - 1) // 2\n                j_c_hi = j_c_lo + 1\n                if 1 <= i_c <= N_c - 2:\n                    if 1 <= j_c_lo <= N_c - 2:\n                        k_c1 = (i_c - 1) * width_c + (j_c_lo - 1)\n                        P[k_f, k_c1] = 0.5\n                    if 1 <= j_c_hi <= N_c - 2:\n                        k_c2 = (i_c - 1) * width_c + (j_c_hi - 1)\n                        P[k_f, k_c2] = 0.5\n\n            elif not is_i_f_even and not is_j_f_even:\n                # Type 4: Fine node at a coarse-cell center\n                i_c_lo, j_c_lo = (i_f - 1) // 2, (j_f - 1) // 2\n                i_c_hi, j_c_hi = i_c_lo + 1, j_c_lo + 1\n                \n                nodes = [(i_c_lo, j_c_lo), (i_c_hi, j_c_lo), \n                         (i_c_lo, j_c_hi), (i_c_hi, j_c_hi)]\n                for i_c, j_c in nodes:\n                    if 1 <= i_c <= N_c - 2 and 1 <= j_c <= N_c - 2:\n                        k_c = (i_c - 1) * width_c + (j_c - 1)\n                        P[k_f, k_c] = 0.25\n    return P\n\ndef construct_restriction(N_f, N_c):\n    \"\"\"Constructs the full-weighting restriction operator R.\"\"\"\n    n_f = (N_f - 2)**2\n    n_c = (N_c - 2)**2\n    if n_f == 0 or n_c == 0:\n        return np.zeros((n_c, n_f))\n        \n    R = np.zeros((n_c, n_f))\n    width_f = N_f - 2\n    width_c = N_c - 2\n    \n    weights = np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n    \n    for i_c in range(1, N_c - 1):\n        for j_c in range(1, N_c - 1):\n            k_c = (i_c - 1) * width_c + (j_c - 1)\n            i_f_center, j_f_center = 2 * i_c, 2 * j_c\n            \n            for di in range(-1, 2):\n                for dj in range(-1, 2):\n                    i_f, j_f = i_f_center + di, j_f_center + dj\n                    \n                    if 1 <= i_f <= N_f - 2 and 1 <= j_f <= N_f - 2:\n                        k_f = (i_f - 1) * width_f + (j_f - 1)\n                        R[k_c, k_f] = weights[di + 1, dj + 1]\n    return R\n\ndef compute_delta(N_f, a, m):\n    \"\"\"\n    Computes the relative Frobenius-norm difference for a given test case.\n    \"\"\"\n    # Grid parameters\n    N_c = (N_f + 1) // 2\n    \n    # Construct operators\n    A_f = construct_operator(N_f, a, m)\n    P = construct_prolongation(N_f, N_c)\n    R = construct_restriction(N_f, N_c)\n    A_c_red = construct_operator(N_c, a, m)\n    \n    # Compute Galerkin operator\n    A_c_gal = R @ A_f @ P\n    \n    # Compute relative difference\n    diff_norm = np.linalg.norm(A_c_gal - A_c_red, 'fro')\n    red_norm = np.linalg.norm(A_c_red, 'fro')\n    \n    if red_norm == 0:\n        return 0.0 if diff_norm == 0 else np.inf\n        \n    return diff_norm / red_norm\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (33, 0.5, 1),\n        (33, 0.0, 1),\n        (17, 0.9, 1),\n        (33, 0.7, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        Nf, a, m = case\n        delta = compute_delta(Nf, a, m)\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}