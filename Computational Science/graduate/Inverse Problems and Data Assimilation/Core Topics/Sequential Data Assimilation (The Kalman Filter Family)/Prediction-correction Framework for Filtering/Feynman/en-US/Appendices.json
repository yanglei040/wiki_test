{
    "hands_on_practices": [
        {
            "introduction": "Mastering the prediction-correction framework begins with understanding its core mechanics in the simplest setting. This exercise provides a concrete, step-by-step walkthrough of the Kalman filter for a scalar system . By manually computing the state predictions, innovations, and incremental log-likelihoods, you will gain a hands-on appreciation for how the filter sequentially processes information and quantifies the surprise of new data.",
            "id": "3413392",
            "problem": "Consider a scalar linear-Gaussian state-space model given by the random walk dynamics and direct noisy observations:\n- State evolution: $x_{k} = x_{k-1} + w_{k}$ with process noise $w_{k} \\sim \\mathcal{N}(0, Q)$.\n- Observation: $y_{k} = x_{k} + v_{k}$ with observation noise $v_{k} \\sim \\mathcal{N}(0, R)$.\nAssume $w_{k}$ and $v_{k}$ are independent across time and mutually independent. The state transition matrix is $F = 1$ and the observation matrix is $H = 1$. The process noise variance and observation noise variance are known and equal to $Q = 0.2$ and $R = 0.5$, respectively. The prior on the initial state is Gaussian with mean $m_{0|0} = 0$ and variance $P_{0|0} = 1$.\n\nYou are given the observation sequence $\\{y_{k}\\}_{k=1}^{4}$ with values $y_{1} = 0.3$, $y_{2} = -0.1$, $y_{3} = 0.5$, and $y_{4} = 0.2$.\n\nUsing the prediction-correction framework, define the one-step predictor $(m_{k|k-1}, P_{k|k-1})$, the innovation $\\nu_{k} = y_{k} - m_{k|k-1}$, and the innovation variance $S_{k}$. From first principles of linear-Gaussian models and Gaussian conditional densities, derive the expression of the incremental log-likelihood $\\ell_{k} = \\ln p(y_{k} \\mid y_{1:k-1})$ in terms of $\\nu_{k}$ and $S_{k}$, and then compute $\\nu_{k}$, $S_{k}$, and $\\ell_{k}$ for $k = 1, 2, 3, 4$ for the given data. Finally, report the total log-likelihood $L = \\sum_{k=1}^{4} \\ell_{k}$.\n\nRound your final reported value of $L$ to four significant figures. Express the answer as a pure number (no units).",
            "solution": "The problem requires the computation of the total log-likelihood for a given sequence of observations from a scalar linear-Gaussian state-space model. The solution methodology is based on the prediction-correction framework, commonly known as the Kalman filter, which allows for the sequential evaluation of the incremental log-likelihoods.\n\nFirst, we establish the theoretical foundation for the incremental log-likelihood. The total log-likelihood $L$ of the observation sequence $y_{1:T} = \\{y_1, y_2, \\ldots, y_T\\}$ is given by the chain rule of probability:\n$$ L = \\ln p(y_{1:T}) = \\ln \\left( p(y_1) \\prod_{k=2}^{T} p(y_k \\mid y_{1:k-1}) \\right) = \\sum_{k=1}^{T} \\ln p(y_k \\mid y_{1:k-1}) $$\nwhere we define $p(y_1 \\mid y_{1:0})$ as $p(y_1)$. The term $\\ell_k = \\ln p(y_k \\mid y_{1:k-1})$ is the incremental log-likelihood at time step $k$.\n\nIn a linear-Gaussian state-space model, all conditional distributions are Gaussian. The filtering distribution at time $k-1$ is $p(x_{k-1} \\mid y_{1:k-1}) = \\mathcal{N}(x_{k-1} ; m_{k-1|k-1}, P_{k-1|k-1})$.\n\nThe prediction step of the filter computes the one-step-ahead predictive distribution of the state, $p(x_k \\mid y_{1:k-1})$.\nGiven $x_k = F x_{k-1} + w_k$ where $w_k \\sim \\mathcal{N}(0, Q)$, the predictive distribution for $x_k$ is Gaussian with mean $m_{k|k-1}$ and variance $P_{k|k-1}$:\n$$ m_{k|k-1} = \\mathbb{E}[x_k \\mid y_{1:k-1}] = \\mathbb{E}[F x_{k-1} + w_k \\mid y_{1:k-1}] = F m_{k-1|k-1} $$\n$$ P_{k|k-1} = \\text{Var}(x_k \\mid y_{1:k-1}) = \\text{Var}(F x_{k-1} + w_k \\mid y_{1:k-1}) = F P_{k-1|k-1} F^T + Q $$\nThe pair $(m_{k|k-1}, P_{k|k-1})$ is the one-step predictor.\n\nNext, we determine the conditional distribution of the observation $y_k$ given past observations $y_{1:k-1}$. Given the observation model $y_k = H x_k + v_k$ with $v_k \\sim \\mathcal{N}(0, R)$, the predictive distribution for $y_k$ is also Gaussian. Its mean is:\n$$ \\mathbb{E}[y_k \\mid y_{1:k-1}] = \\mathbb{E}[H x_k + v_k \\mid y_{1:k-1}] = H m_{k|k-1} $$\nAnd its variance, which we denote as the innovation variance $S_k$, is:\n$$ S_k = \\text{Var}(y_k \\mid y_{1:k-1}) = \\text{Var}(H x_k + v_k \\mid y_{1:k-1}) = H P_{k|k-1} H^T + R $$\nThus, $p(y_k \\mid y_{1:k-1}) = \\mathcal{N}(y_k ; H m_{k|k-1}, S_k)$.\n\nThe innovation is defined as the difference between the actual observation and its predicted mean:\n$$ \\nu_k = y_k - \\mathbb{E}[y_k \\mid y_{1:k-1}] = y_k - H m_{k|k-1} $$\nThe probability density function for a scalar Gaussian variable $z$ with mean $\\mu$ and variance $\\sigma^2$ is $p(z) = (2\\pi\\sigma^2)^{-1/2} \\exp(-\\frac{(z-\\mu)^2}{2\\sigma^2})$. For $p(y_k \\mid y_{1:k-1})$, we have $z=y_k$, $\\mu=H m_{k|k-1}$, and $\\sigma^2=S_k$. The incremental log-likelihood is therefore:\n$$ \\ell_k = \\ln p(y_k \\mid y_{1:k-1}) = -\\frac{1}{2} \\ln(2\\pi S_k) - \\frac{(y_k - H m_{k|k-1})^2}{2S_k} = -\\frac{1}{2}\\left(\\ln(2\\pi S_k) + \\frac{\\nu_k^2}{S_k}\\right) $$\nThis is the required expression. To proceed with the calculation, we also need the correction step equations to update the state estimate after observing $y_k$:\n- Kalman Gain: $K_k = P_{k|k-1} H^T S_k^{-1}$\n- Corrected Mean: $m_{k|k} = m_{k|k-1} + K_k \\nu_k$\n- Corrected Variance: $P_{k|k} = (I - K_k H) P_{k|k-1}$\n\nNow, we apply these equations for $k=1, 2, 3, 4$ using the given parameters:\n- Model parameters: $F=1, H=1, Q=0.2, R=0.5$.\n- Initial state: $m_{0|0} = 0, P_{0|0} = 1$.\n- Observations: $y_1=0.3, y_2=-0.1, y_3=0.5, y_4=0.2$.\n\n**Step k=1:**\n- Prediction:\n  - $m_{1|0} = F m_{0|0} = 1 \\cdot 0 = 0$\n  - $P_{1|0} = F P_{0|0} F^T + Q = 1 \\cdot 1 \\cdot 1 + 0.2 = 1.2$\n- Innovation and Likelihood:\n  - $\\nu_1 = y_1 - H m_{1|0} = 0.3 - 1 \\cdot 0 = 0.3$\n  - $S_1 = H P_{1|0} H^T + R = 1 \\cdot 1.2 \\cdot 1 + 0.5 = 1.7$\n  - $\\ell_1 = -\\frac{1}{2}\\left(\\ln(2\\pi \\cdot 1.7) + \\frac{0.3^2}{1.7}\\right) \\approx -\\frac{1}{2}(2.36850 + 0.05294) \\approx -1.21072$\n- Correction:\n  - $K_1 = P_{1|0} H^T S_1^{-1} = 1.2 \\cdot 1 \\cdot (1.7)^{-1} = \\frac{1.2}{1.7}$\n  - $m_{1|1} = m_{1|0} + K_1 \\nu_1 = 0 + \\frac{1.2}{1.7} \\cdot 0.3 = \\frac{0.36}{1.7} \\approx 0.21176$\n  - $P_{1|1} = (1 - K_1 H) P_{1|0} = (1 - \\frac{1.2}{1.7} \\cdot 1) \\cdot 1.2 = \\frac{0.5}{1.7} \\cdot 1.2 = \\frac{0.6}{1.7} \\approx 0.35294$\n\n**Step k=2:**\n- Prediction:\n  - $m_{2|1} = F m_{1|1} = 1 \\cdot \\frac{0.36}{1.7} = \\frac{0.36}{1.7} \\approx 0.21176$\n  - $P_{2|1} = F P_{1|1} F^T + Q = 1 \\cdot \\frac{0.6}{1.7} \\cdot 1 + 0.2 = \\frac{0.6 + 0.2 \\cdot 1.7}{1.7} = \\frac{0.94}{1.7} \\approx 0.55294$\n- Innovation and Likelihood:\n  - $\\nu_2 = y_2 - H m_{2|1} = -0.1 - \\frac{0.36}{1.7} = \\frac{-0.17 - 0.36}{1.7} = -\\frac{0.53}{1.7} \\approx -0.31176$\n  - $S_2 = H P_{2|1} H^T + R = \\frac{0.94}{1.7} + 0.5 = \\frac{0.94 + 0.5 \\cdot 1.7}{1.7} = \\frac{1.79}{1.7} \\approx 1.05294$\n  - $\\ell_2 = -\\frac{1}{2}\\left(\\ln(2\\pi S_2) + \\frac{\\nu_2^2}{S_2}\\right) = -\\frac{1}{2}\\left(\\ln(2\\pi \\frac{1.79}{1.7}) + \\frac{(-0.53/1.7)^2}{1.79/1.7}\\right) \\approx -0.99087$\n- Correction:\n  - $K_2 = P_{2|1} H^T S_2^{-1} = \\frac{0.94}{1.7} \\cdot (\\frac{1.79}{1.7})^{-1} = \\frac{0.94}{1.79}$\n  - $m_{2|2} = m_{2|1} + K_2 \\nu_2 = \\frac{0.36}{1.7} + \\frac{0.94}{1.79} \\cdot (-\\frac{0.53}{1.7}) \\approx 0.04804$\n  - $P_{2|2} = (1 - K_2 H) P_{2|1} = (1 - \\frac{0.94}{1.79}) \\frac{0.94}{1.7} = \\frac{0.85}{1.79} \\frac{0.94}{1.7} \\approx 0.26258$\n\n**Step k=3:**\n- Prediction:\n  - $m_{3|2} = F m_{2|2} \\approx 0.04804$\n  - $P_{3|2} = F P_{2|2} F^T + Q \\approx 0.26258 + 0.2 = 0.46258$\n- Innovation and Likelihood:\n  - $\\nu_3 = y_3 - H m_{3|2} = 0.5 - 0.04804 = 0.45196$\n  - $S_3 = H P_{3|2} H^T + R = 0.46258 + 0.5 = 0.96258$\n  - $\\ell_3 = -\\frac{1}{2}\\left(\\ln(2\\pi \\cdot 0.96258) + \\frac{0.45196^2}{0.96258}\\right) \\approx -\\frac{1}{2}(1.79965 + 0.21220) \\approx -1.00593$\n- Correction:\n  - $K_3 = P_{3|2} S_3^{-1} = \\frac{0.46258}{0.96258} \\approx 0.48055$\n  - $m_{3|3} = m_{3|2} + K_3 \\nu_3 \\approx 0.04804 + 0.48055 \\cdot 0.45196 \\approx 0.26522$\n  - $P_{3|3} = (1 - K_3 H) P_{3|2} \\approx (1 - 0.48055) \\cdot 0.46258 \\approx 0.24030$\n\n**Step k=4:**\n- Prediction:\n  - $m_{4|3} = F m_{3|3} \\approx 0.26522$\n  - $P_{4|3} = F P_{3|3} F^T + Q \\approx 0.24030 + 0.2 = 0.44030$\n- Innovation and Likelihood:\n  - $\\nu_4 = y_4 - H m_{4|3} = 0.2 - 0.26522 = -0.06522$\n  - $S_4 = H P_{4|3} H^T + R = 0.44030 + 0.5 = 0.94030$\n  - $\\ell_4 = -\\frac{1}{2}\\left(\\ln(2\\pi \\cdot 0.94030) + \\frac{(-0.06522)^2}{0.94030}\\right) \\approx -\\frac{1}{2}(1.77631 + 0.00452) \\approx -0.89042$\n\nFinally, the total log-likelihood $L$ is the sum of the incremental log-likelihoods:\n$$ L = \\sum_{k=1}^{4} \\ell_k = \\ell_1 + \\ell_2 + \\ell_3 + \\ell_4 $$\n$$ L \\approx -1.21072 + (-0.99087) + (-1.00593) + (-0.89042) \\approx -4.09794 $$\nRounding to four significant figures, we get $L \\approx -4.098$.",
            "answer": "$$\n\\boxed{-4.098}\n$$"
        },
        {
            "introduction": "While the standard Kalman filter is formulated in terms of state mean and covariance, an alternative and often more insightful perspective is the information form. This exercise challenges you to derive the filter's update rule for information, the inverse of the covariance matrix . You will discover that the Bayesian update becomes a simple addition of information from the prior and the new measurement, elegantly connecting the filtering process to the classical concept of Fisher Information.",
            "id": "3413388",
            "problem": "Consider the scalar linear-Gaussian state-space model used in the prediction-correction framework for filtering:\n- State dynamics: $x_{k} = a\\,x_{k-1} + w_{k}$ with $w_{k} \\sim \\mathcal{N}(0,q)$.\n- Observation model: $y_{k} = h\\,x_{k} + v_{k}$ with $v_{k} \\sim \\mathcal{N}(0,r)$.\nAssume that conditioned on all observations up to time $k-1$, the posterior for the state is Gaussian, $x_{k-1}\\,|\\,y_{1:k-1} \\sim \\mathcal{N}(m_{k-1},P_{k-1})$, and define the associated information (precision) as $J_{k-1} \\equiv P_{k-1}^{-1}$. Using only the following foundational ingredients:\n- Bayes’ rule and the independence of process and observation noises,\n- The properties of Gaussian convolution and conditioning,\n- The definition of Fisher Information (FI) for a scalar parameter $x$ from a likelihood $p(y\\,|\\,x)$ as the negative expectation of the Hessian of the log-likelihood, $I(x) \\equiv -\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial x^{2}} \\ln p(y\\,|\\,x)\\right]$,\nderive, within the prediction-correction framework, a closed-form analytic expression for the posterior information at time $k$, denoted $J_{k}$, as a function of $J_{k-1}$, $a$, $q$, $h$, and $r$. Your final answer must be a single analytic expression in these symbols only, with no numerical substitution. Express only $J_{k}$, and do not report any intermediate quantities.",
            "solution": "We begin from the assumed Gaussian posterior at time $k-1$, namely $x_{k-1}\\,|\\,y_{1:k-1} \\sim \\mathcal{N}(m_{k-1},P_{k-1})$ with information $J_{k-1} \\equiv P_{k-1}^{-1}$. The prediction step uses the linear-Gaussian state dynamics $x_{k} = a\\,x_{k-1} + w_{k}$ with $w_{k} \\sim \\mathcal{N}(0,q)$ independent of $x_{k-1}$. By Gaussian propagation through a linear map, the prior (predicted) distribution at time $k$ is Gaussian:\n$$\nx_{k}\\,|\\,y_{1:k-1} \\sim \\mathcal{N}\\!\\big(a\\,m_{k-1},\\,a^{2}P_{k-1} + q\\big).\n$$\nDenote the predicted variance by $P_{k}^{-} \\equiv a^{2}P_{k-1} + q$. The corresponding predicted information (precision) is\n$$\nJ_{k}^{-} \\equiv \\left(P_{k}^{-}\\right)^{-1} = \\left(a^{2}P_{k-1} + q\\right)^{-1} = \\left(\\frac{a^{2}}{J_{k-1}} + q\\right)^{-1},\n$$\nwhere we used $P_{k-1} = J_{k-1}^{-1}$.\n\nNext, we consider the correction step using the observation model $y_{k} = h\\,x_{k} + v_{k}$ with $v_{k} \\sim \\mathcal{N}(0,r)$ independent of $x_{k}$. The likelihood is\n$$\np(y_{k}\\,|\\,x_{k}) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\!\\left(-\\frac{(y_{k} - h\\,x_{k})^{2}}{2r}\\right).\n$$\nIts log-likelihood is\n$$\n\\ln p(y_{k}\\,|\\,x_{k}) = -\\frac{1}{2}\\ln(2\\pi r) - \\frac{(y_{k} - h\\,x_{k})^{2}}{2r}.\n$$\nDifferentiating with respect to $x_{k}$, we obtain\n$$\n\\frac{\\partial}{\\partial x_{k}} \\ln p(y_{k}\\,|\\,x_{k}) = \\frac{h\\,(y_{k} - h\\,x_{k})}{r}, \\qquad\n\\frac{\\partial^{2}}{\\partial x_{k}^{2}} \\ln p(y_{k}\\,|\\,x_{k}) = -\\frac{h^{2}}{r}.\n$$\nTherefore, the Fisher Information (FI) contributed by the likelihood, defined as $I(x_{k}) \\equiv -\\mathbb{E}\\!\\left[\\frac{\\partial^{2}}{\\partial x_{k}^{2}} \\ln p(y_{k}\\,|\\,x_{k})\\right]$, equals\n$$\nI(x_{k}) = -\\left(-\\frac{h^{2}}{r}\\right) = \\frac{h^{2}}{r},\n$$\nbecause the second derivative is constant in $y_{k}$ and $x_{k}$ for this linear-Gaussian model, making the expectation redundant.\n\nBy Bayes’ rule, the posterior density is proportional to the product of the prior and the likelihood:\n$$\np(x_{k}\\,|\\,y_{1:k}) \\propto p(x_{k}\\,|\\,y_{1:k-1})\\,p(y_{k}\\,|\\,x_{k}).\n$$\nTaking logarithms,\n$$\n\\ln p(x_{k}\\,|\\,y_{1:k}) = \\ln p(x_{k}\\,|\\,y_{1:k-1}) + \\ln p(y_{k}\\,|\\,x_{k}) + C,\n$$\nfor a constant $C$ independent of $x_{k}$. Differentiating twice with respect to $x_{k}$ and taking minus signs, the posterior information (negative Hessian of the log-posterior) equals the sum of the prior information and the likelihood’s Fisher Information:\n$$\nJ_{k} \\equiv -\\frac{\\partial^{2}}{\\partial x_{k}^{2}} \\ln p(x_{k}\\,|\\,y_{1:k}) = \\underbrace{-\\frac{\\partial^{2}}{\\partial x_{k}^{2}} \\ln p(x_{k}\\,|\\,y_{1:k-1})}_{J_{k}^{-}} + \\underbrace{-\\frac{\\partial^{2}}{\\partial x_{k}^{2}} \\ln p(y_{k}\\,|\\,x_{k})}_{I(x_{k})}.\n$$\nFor a Gaussian prior with variance $P_{k}^{-}$, the negative second derivative of its log-density with respect to $x_{k}$ is the constant precision $J_{k}^{-} = (P_{k}^{-})^{-1}$. We have already computed $I(x_{k}) = h^{2}/r$. Consequently,\n$$\nJ_{k} = J_{k}^{-} + \\frac{h^{2}}{r} = \\left(\\frac{a^{2}}{J_{k-1}} + q\\right)^{-1} + \\frac{h^{2}}{r}.\n$$\nThis is the desired closed-form analytic expression for the posterior information at time $k$ in terms of $J_{k-1}$, $a$, $q$, $h$, and $r$.",
            "answer": "$$\\boxed{\\left(\\frac{a^{2}}{J_{k-1}}+q\\right)^{-1}+\\frac{h^{2}}{r}}$$"
        },
        {
            "introduction": "Real-world systems are rarely linear, posing a significant challenge to the standard Kalman filter. This coding exercise guides you through implementing the Unscented Kalman Filter (UKF), a powerful method for handling nonlinear dynamics and measurement models . Instead of linearizing the system, you will use the Unscented Transform to deterministically propagate a set of 'sigma points' through the nonlinear functions, providing a more accurate and robust way to estimate the state's statistics.",
            "id": "3413348",
            "problem": "Consider a discrete-time nonlinear state-space model with a two-dimensional state evolving according to a prediction-correction framework for filtering. The state dynamics are given by a nonlinear mapping $f:\\mathbb{R}^2\\to\\mathbb{R}^2$ and the observations by a nonlinear mapping $h:\\mathbb{R}^2\\to\\mathbb{R}^2$. Assume additive, independent, zero-mean Gaussian process noise and observation noise with covariances $Q\\in\\mathbb{R}^{2\\times 2}$ and $R\\in\\mathbb{R}^{2\\times 2}$, respectively. The trigonometric functions must be interpreted in radians. The stated goal is to compute, for a single assimilation time step from time $k=0$ to time $k=1$, the filtered state mean and covariance using the Unscented Kalman Filter (UKF), which relies on the Unscented Transform (UT). The design and implementation must start from the foundational probabilistic rules for state estimation and must not assume linearity nor small-noise approximations.\n\nThe foundational base to use includes the following elements: the definition of a state-space model, Bayes' rule for updating a probability density function given new data, and the definition of mean and covariance of a random vector. The target concept is the Unscented Transform and the Unscented Kalman Filter; specific formulas for sigma points or weights are not provided and must be derived from the base principles. The state dynamics and observation models are defined as follows:\n$$\nf(x) = \\begin{bmatrix}\nx_1 + 0.2\\,\\sin(x_2)\\\\\nx_2 + 0.1\\,x_1\\,x_2\n\\end{bmatrix},\\qquad\nh(x) = \\begin{bmatrix}\n\\frac{1}{2}x_1^2 + \\cos(x_2)\\\\\n\\exp(0.3\\,x_1) + \\frac{1}{2}x_2^2\n\\end{bmatrix},\n$$\nwhere $x = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}\\in\\mathbb{R}^2$ and all angles in the trigonometric functions are in radians.\n\nYou are given an initial Gaussian prior at time $k=0$ with mean $m_0\\in\\mathbb{R}^2$ and covariance $P_0\\in\\mathbb{R}^{2\\times 2}$. The process noise covariance is $Q\\in\\mathbb{R}^{2\\times 2}$, and the observation at time $k=1$ is $y_1\\in\\mathbb{R}^2$ with observation noise covariance $R\\in\\mathbb{R}^{2\\times 2}$. The Unscented Transform must be parameterized by scaling parameters $\\alpha\\in\\mathbb{R}_{>0}$, $\\beta\\in\\mathbb{R}$, and $\\kappa\\in\\mathbb{R}$, which influence the spread and weights of the sigma points in the Unscented Transform. The Unscented Kalman Filter must perform the following steps: prediction of the state distribution through the nonlinear dynamics and correction using the observation through the measurement model, aggregating the corresponding means and covariances while respecting the probabilistic structure.\n\nYour task is to write a complete, runnable program that, for each test case in the following test suite, computes one assimilation step of the Unscented Kalman Filter from time $k=0$ to $k=1$ using the Unscented Transform. For each case, output the filtered mean components and the trace of the filtered covariance, namely the vector $\\left[m_1^{(1)},\\,m_1^{(2)},\\,\\mathrm{tr}(P_1)\\right]$, where $m_1\\in\\mathbb{R}^2$ and $P_1\\in\\mathbb{R}^{2\\times 2}$ denote the posterior mean and covariance at time $k=1$ after assimilating $y_1$.\n\nUse the following test suite of parameter values, where all matrices are symmetric and all covariances are positive definite:\n\n- Case $1$ (general case):\n  - $m_0 = \\begin{bmatrix}0.3\\\\-0.6\\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix}0.25 & 0.05\\\\0.05 & 0.2\\end{bmatrix}$,\n  - $Q = \\begin{bmatrix}0.01 & 0.005\\\\0.005 & 0.02\\end{bmatrix}$,\n  - $R = \\begin{bmatrix}0.02 & 0\\\\0 & 0.02\\end{bmatrix}$,\n  - $y_1 = \\begin{bmatrix}0.7\\\\1.2\\end{bmatrix}$,\n  - $\\alpha = 0.5$, $\\beta = 2$, $\\kappa = 0$.\n\n- Case $2$ (near-degenerate prior covariance, small noise):\n  - $m_0 = \\begin{bmatrix}-0.1\\\\0.1\\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix}10^{-6} & 0\\\\0 & 10^{-6}\\end{bmatrix}$,\n  - $Q = \\begin{bmatrix}10^{-8} & 0\\\\0 & 10^{-8}\\end{bmatrix}$,\n  - $R = \\begin{bmatrix}10^{-4} & 5\\cdot 10^{-5}\\\\5\\cdot 10^{-5} & 10^{-4}\\end{bmatrix}$,\n  - $y_1 = \\begin{bmatrix}0\\\\0\\end{bmatrix}$,\n  - $\\alpha = 0.05$, $\\beta = 2$, $\\kappa = 1$.\n\n- Case $3$ (highly nonlinear region, unusual spread parameter):\n  - $m_0 = \\begin{bmatrix}2.0\\\\-2.0\\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix}0.5 & -0.1\\\\-0.1 & 0.3\\end{bmatrix}$,\n  - $Q = \\begin{bmatrix}0.02 & 0\\\\0 & 0.02\\end{bmatrix}$,\n  - $R = \\begin{bmatrix}0.05 & 0.01\\\\0.01 & 0.05\\end{bmatrix}$,\n  - $y_1 = \\begin{bmatrix}-0.5\\\\2.5\\end{bmatrix}$,\n  - $\\alpha = 0.9$, $\\beta = 2$, $\\kappa = -1$.\n\nAll trigonometric quantities must use radians. The final output format must be a single line containing a comma-separated list enclosed in square brackets, where each element corresponds to a test case result vector in the same order as above. For example, the program should print a line of the form $\\left[\\left[\\cdot,\\cdot,\\cdot\\right],\\left[\\cdot,\\cdot,\\cdot\\right],\\left[\\cdot,\\cdot,\\cdot\\right]\\right]$. Each entry must be a float. No other text should be printed.",
            "solution": "The problem requires the implementation of a single prediction-correction step of the Unscented Kalman Filter (UKF) for a discrete-time nonlinear state-space system. The solution must be derived from foundational principles of Bayesian filtering and the Unscented Transform (UT).\n\n**1. Foundational Framework: Bayesian Filtering**\n\nWe consider a state-space model defined by a process equation and a measurement equation:\n$$x_k = f(x_{k-1}) + w_{k-1}$$\n$$y_k = h(x_k) + v_k$$\nwhere $x_k \\in \\mathbb{R}^{n_x}$ is the state vector at time $k$, $y_k \\in \\mathbb{R}^{n_y}$ is the observation, $f$ and $h$ are nonlinear functions, and $w_k \\sim \\mathcal{N}(0, Q_k)$ and $v_k \\sim \\mathcal{N}(0, R_k)$ are independent, zero-mean Gaussian noise processes with covariances $Q_k$ and $R_k$, respectively. In this problem, the state dimension is $n_x=2$ and the observation dimension is $n_y=2$.\n\nThe objective of filtering is to estimate the posterior probability density function (PDF) $p(x_k | y_{1:k})$ of the current state given all observations up to time $k$. This is achieved via a recursive prediction-correction cycle based on Bayes' rule.\n\n*   **Prediction Step**: Given the posterior at time $k-1$, $p(x_{k-1} | y_{1:k-1})$, the predicted (or prior) distribution for the state at time $k$ is computed via the Chapman-Kolmogorov equation:\n    $$p(x_k | y_{1:k-1}) = \\int p(x_k | x_{k-1}) p(x_{k-1} | y_{1:k-1}) dx_{k-1}$$\n    where $p(x_k | x_{k-1})$ is defined by the process model $x_k = f(x_{k-1}) + w_{k-1}$.\n\n*   **Correction Step**: Upon receiving a new observation $y_k$, the predicted PDF is updated to the posterior PDF using Bayes' rule:\n    $$p(x_k | y_{1:k}) = \\frac{p(y_k | x_k) p(x_k | y_{1:k-1})}{p(y_k | y_{1:k-1})}$$\n    where $p(y_k | x_k)$ is the likelihood, defined by the measurement model $y_k = h(x_k) + v_k$.\n\nFor nonlinear functions $f$ and $h$, these integrals are generally intractable. The UKF provides a method to approximate the evolution of the state's mean and covariance without linearizing the system dynamics, as done in the Extended Kalman Filter (EKF).\n\n**2. The Unscented Transform (UT)**\n\nThe core of the UKF is the Unscented Transform, a technique for propagating the statistics of a random variable through a nonlinear function. Instead of approximating the function, the UT approximates the probability distribution with a deterministically chosen set of points called sigma points. These points are selected to capture the mean and covariance of the original distribution.\n\nGiven a random variable $x \\in \\mathbb{R}^{n_x}$ with mean $m$ and covariance $P$, the UT proceeds as follows:\n\n1.  **Generate Sigma Points**: A set of $2n_x+1$ sigma points $\\mathcal{X}_i$ and associated weights $W_i^{(m)}$ (for the mean) and $W_i^{(c)}$ (for the covariance) are generated. The selection is parameterized by $\\alpha$, $\\beta$, and $\\kappa$. A standard formulation is:\n    *   Define a scaling parameter $\\lambda = \\alpha^2 (n_x + \\kappa) - n_x$.\n    *   The sigma points are:\n        $$\\mathcal{X}_0 = m$$\n        $$\\mathcal{X}_i = m + (\\sqrt{(n_x+\\lambda)P})_i, \\quad i=1, \\dots, n_x$$\n        $$\\mathcal{X}_{i+n_x} = m - (\\sqrt{(n_x+\\lambda)P})_i, \\quad i=1, \\dots, n_x$$\n        where $(\\sqrt{(n_x+\\lambda)P})_i$ is the $i$-th column of a matrix square root of $(n_x+\\lambda)P$, typically computed via Cholesky decomposition.\n\n    *   The weights are:\n        $$W_0^{(m)} = \\frac{\\lambda}{n_x+\\lambda}$$\n        $$W_0^{(c)} = \\frac{\\lambda}{n_x+\\lambda} + (1 - \\alpha^2 + \\beta)$$\n        $$W_i^{(m)} = W_i^{(c)} = \\frac{1}{2(n_x+\\lambda)}, \\quad i=1, \\dots, 2n_x$$\n    This choice of points and weights ensures that the sample mean and covariance of the sigma points exactly match the original mean $m$ and covariance $P$.\n\n2.  **Propagate Points**: The sigma points are propagated through the nonlinear function $g(\\cdot)$:\n    $$\\mathcal{Y}_i = g(\\mathcal{X}_i)$$\n\n3.  **Estimate Output Statistics**: The mean and covariance of the transformed variable are estimated as the weighted sample mean and covariance of the propagated points:\n    $$m_y \\approx \\sum_{i=0}^{2n_x} W_i^{(m)} \\mathcal{Y}_i$$\n    $$P_y \\approx \\sum_{i=0}^{2n_x} W_i^{(c)} (\\mathcal{Y}_i - m_y)(\\mathcal{Y}_i - m_y)^T$$\n\n**3. The Unscented Kalman Filter Algorithm (Single Step)**\n\nThe UKF applies the UT to the prediction and correction steps of the Bayesian filter. We are given the initial state mean $m_0$ and covariance $P_0$ at $k=0$, and an observation $y_1$ at $k=1$. The state dimension is $n_x=2$.\n\n**A. Prediction Step (Time Update)**\n\nThe goal is to compute the predicted mean $m_{1|0}$ and covariance $P_{1|0}$ of the state at $k=1$ before observing $y_1$.\n\n1.  **Generate Sigma Points**: Calculate the UT parameters $\\lambda$, $W^{(m)}$, and $W^{(c)}$ using $n_x=2$ and the given $\\alpha, \\beta, \\kappa$. Generate $2n_x+1 = 5$ sigma points $\\mathcal{X}_{0,i}$ from the initial distribution $\\mathcal{N}(m_0, P_0)$.\n2.  **Propagate through Dynamics**: Propagate each sigma point through the state transition function $f(\\cdot)$:\n    $$\\mathcal{X}_{1|0,i} = f(\\mathcal{X}_{0,i})$$\n3.  **Calculate Predicted Mean and Covariance**: The predicted mean $m_{1|0}$ is the weighted average of the propagated points. The predicted covariance $P_{1|0}$ is the weighted sample covariance plus the process noise covariance $Q$, as the noise is additive.\n    $$m_{1|0} = \\sum_{i=0}^{2n_x} W_i^{(m)} \\mathcal{X}_{1|0,i}$$\n    $$P'_{1|0} = \\sum_{i=0}^{2n_x} W_i^{(c)} (\\mathcal{X}_{1|0,i} - m_{1|0})(\\mathcal{X}_{1|0,i} - m_{1|0})^T$$\n    $$P_{1|0} = P'_{1|0} + Q$$\n\n**B. Correction Step (Measurement Update)**\n\nThe goal is to update the predicted state using the observation $y_1$ to find the posterior mean $m_1$ and covariance $P_1$.\n\n1.  **Regenerate Sigma Points**: Generate a new set of $2n_x+1=5$ sigma points, which we denote $\\mathcal{Z}_{1|0,i}$, from the predicted distribution $\\mathcal{N}(m_{1|0}, P_{1|0})$.\n2.  **Propagate through Observation Model**: Propagate these new sigma points through the observation function $h(\\cdot)$:\n    $$\\mathcal{Y}_i = h(\\mathcal{Z}_{1|0,i})$$\n3.  **Calculate Observation Statistics**:\n    *   Predicted observation mean: $m_y = \\sum_{i=0}^{2n_x} W_i^{(m)} \\mathcal{Y}_i$\n    *   Innovation (or residual) covariance: $P_{yy} = \\left(\\sum_{i=0}^{2n_x} W_i^{(c)} (\\mathcal{Y}_i - m_y)(\\mathcal{Y}_i - m_y)^T\\right) + R$\n    *   State-observation cross-covariance: $P_{xy} = \\sum_{i=0}^{2n_x} W_i^{(c)} (\\mathcal{Z}_{1|0,i} - m_{1|0})(\\mathcal{Y}_i - m_y)^T$\n\n4.  **Calculate Update Quantities**:\n    *   Kalman Gain: $K = P_{xy} P_{yy}^{-1}$\n    *   Updated (filtered) state mean: $m_1 = m_{1|0} + K(y_1 - m_y)$\n    *   Updated (filtered) state covariance: $P_1 = P_{1|0} - K P_{yy} K^T$\n\nThe final result for each test case is the vector $[m_1^{(1)}, m_1^{(2)}, \\mathrm{tr}(P_1)]$, where $m_1 = [m_1^{(1)}, m_1^{(2)}]^T$ and $\\mathrm{tr}(P_1)$ is the trace of the final posterior covariance matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Unscented Kalman Filter problem for the given test cases.\n    \"\"\"\n\n    def f(x):\n        \"\"\"State transition function.\"\"\"\n        x1, x2 = x\n        return np.array([\n            x1 + 0.2 * np.sin(x2),\n            x2 + 0.1 * x1 * x2\n        ])\n\n    def h(x):\n        \"\"\"Observation function.\"\"\"\n        x1, x2 = x\n        return np.array([\n            0.5 * x1**2 + np.cos(x2),\n            np.exp(0.3 * x1) + 0.5 * x2**2\n        ])\n\n    def ukf_step(m0, P0, y1, Q, R, alpha, beta, kappa):\n        \"\"\"\n        Performs one step of the Unscented Kalman Filter.\n        \"\"\"\n        # Setup\n        nx = m0.shape[0]\n        \n        # Calculate UT weights and parameters\n        lambda_ = alpha**2 * (nx + kappa) - nx\n        \n        wm = np.full(2 * nx + 1, 1 / (2 * (nx + lambda_)))\n        wm[0] = lambda_ / (nx + lambda_)\n        \n        wc = np.full(2 * nx + 1, 1 / (2 * (nx + lambda_)))\n        wc[0] = lambda_ / (nx + lambda_) + (1 - alpha**2 + beta)\n\n        # --- Prediction Step ---\n        \n        # 1. Generate sigma points from initial state (m0, P0)\n        # Using Cholesky decomposition for the matrix square root\n        P_sqrt_term = (nx + lambda_) * P0\n        L = np.linalg.cholesky(P_sqrt_term)\n\n        sigma_points_0 = np.zeros((2 * nx + 1, nx))\n        sigma_points_0[0] = m0\n        for i in range(nx):\n            sigma_points_0[i + 1]      = m0 + L[:, i]\n            sigma_points_0[i + 1 + nx] = m0 - L[:, i]\n\n        # 2. Propagate sigma points through the state transition function f\n        propagated_sigma_points = np.array([f(sp) for sp in sigma_points_0])\n        \n        # 3. Calculate predicted mean (m1_pred)\n        m1_pred = np.sum(wm[:, np.newaxis] * propagated_sigma_points, axis=0)\n        \n        # 4. Calculate predicted covariance (P1_pred)\n        # The process noise Q is added at the end for the additive noise model\n        P1_pred = np.zeros((nx, nx))\n        for i in range(2 * nx + 1):\n            diff = propagated_sigma_points[i] - m1_pred\n            P1_pred += wc[i] * np.outer(diff, diff)\n        P1_pred += Q\n\n        # --- Correction Step ---\n        \n        # 1. Generate new sigma points from the predicted state (m1_pred, P1_pred)\n        P1_pred_sqrt_term = (nx + lambda_) * P1_pred\n        L1 = np.linalg.cholesky(P1_pred_sqrt_term)\n            \n        sigma_points_1 = np.zeros((2 * nx + 1, nx))\n        sigma_points_1[0] = m1_pred\n        for i in range(nx):\n            sigma_points_1[i + 1]      = m1_pred + L1[:, i]\n            sigma_points_1[i + 1 + nx] = m1_pred - L1[:, i]\n\n        # 2. Propagate new sigma points through the observation function h\n        obs_sigma_points = np.array([h(sp) for sp in sigma_points_1])\n        \n        # 3. Calculate predicted observation mean (my_pred)\n        my_pred = np.sum(wm[:, np.newaxis] * obs_sigma_points, axis=0)\n\n        # 4. Calculate innovation covariance (Pyy) and cross-covariance (Pxy)\n        # The observation noise R is added at the end\n        ny = y1.shape[0]\n        Pyy = np.zeros((ny, ny))\n        Pxy = np.zeros((nx, ny))\n        \n        for i in range(2 * nx + 1):\n            diff_y = obs_sigma_points[i] - my_pred\n            diff_x = sigma_points_1[i] - m1_pred\n            Pyy += wc[i] * np.outer(diff_y, diff_y)\n            Pxy += wc[i] * np.outer(diff_x, diff_y)\n        Pyy += R\n\n        # 5. Calculate Kalman gain (K)\n        K = Pxy @ np.linalg.inv(Pyy)\n        \n        # 6. Update state mean (m1)\n        m1 = m1_pred + K @ (y1 - my_pred)\n        \n        # 7. Update state covariance (P1)\n        # This form is numerically stable and ensures P1 is symmetric\n        P1 = P1_pred - K @ Pyy @ K.T\n        \n        return [m1[0], m1[1], np.trace(P1)]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"m0\": np.array([0.3, -0.6]),\n            \"P0\": np.array([[0.25, 0.05], [0.05, 0.2]]),\n            \"Q\": np.array([[0.01, 0.005], [0.005, 0.02]]),\n            \"R\": np.array([[0.02, 0], [0, 0.02]]),\n            \"y1\": np.array([0.7, 1.2]),\n            \"alpha\": 0.5, \"beta\": 2, \"kappa\": 0\n        },\n        # Case 2\n        {\n            \"m0\": np.array([-0.1, 0.1]),\n            \"P0\": np.array([[1e-6, 0], [0, 1e-6]]),\n            \"Q\": np.array([[1e-8, 0], [0, 1e-8]]),\n            \"R\": np.array([[1e-4, 5e-5], [5e-5, 1e-4]]),\n            \"y1\": np.array([0, 0]),\n            \"alpha\": 0.05, \"beta\": 2, \"kappa\": 1\n        },\n        # Case 3\n        {\n            \"m0\": np.array([2.0, -2.0]),\n            \"P0\": np.array([[0.5, -0.1], [-0.1, 0.3]]),\n            \"Q\": np.array([[0.02, 0], [0, 0.02]]),\n            \"R\": np.array([[0.05, 0.01], [0.01, 0.05]]),\n            \"y1\": np.array([-0.5, 2.5]),\n            \"alpha\": 0.9, \"beta\": 2, \"kappa\": -1\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = ukf_step(\n            case[\"m0\"], case[\"P0\"], case[\"y1\"], case[\"Q\"], case[\"R\"],\n            case[\"alpha\"], case[\"beta\"], case[\"kappa\"]\n        )\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    formatted_results = []\n    for res in results:\n        # Format each sublist without spaces inside the brackets.\n        formatted_res = f\"[{res[0]},{res[1]},{res[2]}]\"\n        formatted_results.append(formatted_res)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}