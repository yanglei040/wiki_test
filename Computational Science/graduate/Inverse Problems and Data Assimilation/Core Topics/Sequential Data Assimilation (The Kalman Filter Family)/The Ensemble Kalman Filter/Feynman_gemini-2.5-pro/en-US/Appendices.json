{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the mechanics of the Ensemble Kalman Filter, there is no substitute for performing an analysis step by hand. This first exercise walks you through the fundamental calculations at the heart of the EnKF. By computing the sample mean, covariance, and Kalman gain from a small, given ensemble, you will build a concrete understanding of how the filter uses ensemble statistics to approximate the Bayesian update equations. ",
            "id": "3425297",
            "problem": "Consider the linear-Gaussian data assimilation setting used in the Ensemble Kalman Filter (EnKF), where the state lives in $\\mathbb{R}^{2}$ and the observation space is $\\mathbb{R}^{1}$. You are given an ensemble of $N=4$ state vectors\n$$\nx^{(1)}=\\begin{pmatrix}2\\\\0\\end{pmatrix},\\quad\nx^{(2)}=\\begin{pmatrix}0\\\\2\\end{pmatrix},\\quad\nx^{(3)}=\\begin{pmatrix}2\\\\4\\end{pmatrix},\\quad\nx^{(4)}=\\begin{pmatrix}4\\\\2\\end{pmatrix},\n$$\nand a linear observation operator\n$$\nH=\\begin{pmatrix}1  1\\end{pmatrix}\\in\\mathbb{R}^{1\\times 2}.\n$$\nAssume the observation error $\\eta$ is additive, independent of the state, and Gaussian, with zero mean and covariance $R\\in\\mathbb{R}^{1\\times 1}$ given by $R=\\frac{4}{3}$.\n\nUsing only foundational definitions for the sample mean, anomalies matrix, sample covariance, and linear-Gaussian update, carry out the following:\n\n1. Compute the sample mean $\\bar{x}\\in\\mathbb{R}^{2}$ and the anomalies matrix $A\\in\\mathbb{R}^{2\\times 4}$ whose $i$-th column is $x^{(i)}-\\bar{x}$.\n2. Compute the sample covariance $P\\in\\mathbb{R}^{2\\times 2}$ defined by $P=\\frac{1}{N-1}AA^{\\top}$.\n3. Compute the state–observation cross-covariance $P^{xy}\\in\\mathbb{R}^{2\\times 1}$ defined by $P^{xy}=\\frac{1}{N-1}AA^{\\top}H^{\\top}$.\n4. In the linear-Gaussian model $y=Hx+\\eta$ with $\\eta\\sim\\mathcal{N}(0,R)$, derive the innovation covariance $S\\in\\mathbb{R}^{1\\times 1}$ and the Kalman gain $K\\in\\mathbb{R}^{2\\times 1}$ using the standard linear-Gaussian Bayesian update reasoning from first principles. Then compute $S$ and $K$ for the given $H$, $P$, and $R$.\n\nYour final answer must be the first component of the Kalman gain vector $K$, expressed as a single exact real number. Do not round and do not include any units in your final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, self-contained, and consistent. It presents a standard computational exercise within the framework of the Ensemble Kalman Filter, a well-established method in data assimilation. All necessary data, definitions, and constraints are provided, and there are no contradictions or ambiguities. The problem is therefore deemed valid, and a solution is provided below.\n\nThe solution proceeds by following the four steps outlined in the problem statement.\n\n1. Computation of the sample mean $\\bar{x}$ and the anomalies matrix $A$.\nThe sample mean $\\bar{x} \\in \\mathbb{R}^{2}$ is the average of the $N=4$ ensemble members $x^{(i)}$:\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x^{(i)} = \\frac{1}{4} \\left( x^{(1)} + x^{(2)} + x^{(3)} + x^{(4)} \\right)\n$$\nSubstituting the given ensemble vectors:\n$$\n\\bar{x} = \\frac{1}{4} \\left( \\begin{pmatrix}2\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\2\\end{pmatrix} + \\begin{pmatrix}2\\\\4\\end{pmatrix} + \\begin{pmatrix}4\\\\2\\end{pmatrix} \\right) = \\frac{1}{4} \\begin{pmatrix}2+0+2+4\\\\0+2+4+2\\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix}8\\\\8\\end{pmatrix} = \\begin{pmatrix}2\\\\2\\end{pmatrix}.\n$$\nThe anomalies matrix $A \\in \\mathbb{R}^{2\\times 4}$ contains the anomaly vectors $a^{(i)} = x^{(i)} - \\bar{x}$ as its columns.\n$$\na^{(1)} = x^{(1)} - \\bar{x} = \\begin{pmatrix}2\\\\0\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}0\\\\-2\\end{pmatrix}\n$$\n$$\na^{(2)} = x^{(2)} - \\bar{x} = \\begin{pmatrix}0\\\\2\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}-2\\\\0\\end{pmatrix}\n$$\n$$\na^{(3)} = x^{(3)} - \\bar{x} = \\begin{pmatrix}2\\\\4\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}0\\\\2\\end{pmatrix}\n$$\n$$\na^{(4)} = x^{(4)} - \\bar{x} = \\begin{pmatrix}4\\\\2\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}2\\\\0\\end{pmatrix}\n$$\nAssembling these column vectors into the matrix $A$:\n$$\nA = \\begin{pmatrix} 0  -2  0  2 \\\\ -2  0  2  0 \\end{pmatrix}.\n$$\n\n2. Computation of the sample covariance $P$.\nThe sample covariance matrix $P \\in \\mathbb{R}^{2\\times 2}$ is defined as $P=\\frac{1}{N-1}AA^{\\top}$. With $N=4$, this becomes $P=\\frac{1}{3}AA^{\\top}$.\nFirst, we compute the product $AA^{\\top}$:\n$$\nA A^{\\top} = \\begin{pmatrix} 0  -2  0  2 \\\\ -2  0  2  0 \\end{pmatrix} \\begin{pmatrix} 0  -2 \\\\ -2  0 \\\\ 0  2 \\\\ 2  0 \\end{pmatrix}\n$$\nThe components of the resulting $2 \\times 2$ matrix are:\n$$\n(AA^{\\top})_{11} = (0)(0) + (-2)(-2) + (0)(0) + (2)(2) = 0 + 4 + 0 + 4 = 8\n$$\n$$\n(AA^{\\top})_{12} = (0)(-2) + (-2)(0) + (0)(2) + (2)(0) = 0 + 0 + 0 + 0 = 0\n$$\n$$\n(AA^{\\top})_{21} = (-2)(0) + (0)(-2) + (2)(0) + (0)(0) = 0 + 0 + 0 + 0 = 0\n$$\n$$\n(AA^{\\top})_{22} = (-2)(-2) + (0)(0) + (2)(2) + (0)(0) = 4 + 0 + 4 + 0 = 8\n$$\nSo,\n$$\nAA^{\\top} = \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix}.\n$$\nNow, we compute $P$:\n$$\nP = \\frac{1}{3} \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{3}  0 \\\\ 0  \\frac{8}{3} \\end{pmatrix}.\n$$\n\n3. Computation of the state–observation cross-covariance $P^{xy}$.\nThe state–observation cross-covariance $P^{xy} \\in \\mathbb{R}^{2\\times 1}$ is defined as $P^{xy}=\\frac{1}{N-1}AA^{\\top}H^{\\top}$. This is equivalent to $P^{xy} = PH^{\\top}$.\nGiven the observation operator $H = \\begin{pmatrix} 1  1 \\end{pmatrix}$, its transpose is $H^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe can now compute $P^{xy}$:\n$$\nP^{xy} = PH^{\\top} = \\begin{pmatrix} \\frac{8}{3}  0 \\\\ 0  \\frac{8}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (\\frac{8}{3})(1) + (0)(1) \\\\ (0)(1) + (\\frac{8}{3})(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{3} \\\\ \\frac{8}{3} \\end{pmatrix}.\n$$\n\n4. Derivation and computation of the innovation covariance $S$ and the Kalman gain $K$.\nWe derive the expressions for $S$ and $K$ from first principles in the linear-Gaussian setting. The state $x$ is a random variable with mean $E[x] = \\bar{x}$ and covariance $\\text{cov}(x) = P$. The observation model is $y = Hx + \\eta$, where the observation error $\\eta$ is a random variable with $E[\\eta]=0$ and $\\text{cov}(\\eta)=R$, and $\\eta$ is independent of $x$.\n\nThe innovation covariance $S \\in \\mathbb{R}^{1\\times 1}$ is the covariance of the observation, $S = \\text{cov}(y)$.\n$$\nS = \\text{cov}(Hx + \\eta)\n$$\nSince $x$ and $\\eta$ are independent, the covariance of their sum is the sum of their covariances:\n$$\nS = \\text{cov}(Hx) + \\text{cov}(\\eta)\n$$\nUsing the property of covariance for a linear transformation, $\\text{cov}(Ax) = A \\text{cov}(x) A^{\\top}$, we have $\\text{cov}(Hx) = H \\text{cov}(x) H^{\\top} = HPH^{\\top}$. The covariance of the error is given as $\\text{cov}(\\eta)=R$.\nTherefore, the innovation covariance is:\n$$\nS = HPH^{\\top} + R.\n$$\nThe Kalman gain $K \\in \\mathbb{R}^{2\\times 1}$ is the linear operator that provides the optimal update of the state estimate. It is given by $K = P^{xy}S^{-1}$, where $P^{xy}$ is the cross-covariance between the state $x$ and the observation $y$. Let's derive $P^{xy}$:\n$$\nP^{xy} = \\text{cov}(x, y) = \\text{cov}(x, Hx + \\eta)\n$$\nBy the linearity of covariance:\n$$\nP^{xy} = \\text{cov}(x, Hx) + \\text{cov}(x, \\eta)\n$$\nSince $x$ and $\\eta$ are independent, $\\text{cov}(x, \\eta) = 0$. Using the property $\\text{cov}(x, Ax) = \\text{cov}(x,x)A^{\\top}$, we get:\n$$\nP^{xy} = \\text{cov}(x, x)H^{\\top} = PH^{\\top}.\n$$\nThis confirms the consistency of the provided definition for $P^{xy}$ and the first-principles derivation.\nCombining these results, the Kalman gain is:\n$$\nK = PH^{\\top} (HPH^{\\top} + R)^{-1}.\n$$\nNow we compute the numerical values for $S$ and $K$.\nFirst, compute $HPH^{\\top}$:\n$$\nHPH^{\\top} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{8}{3}  0 \\\\ 0  \\frac{8}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{3}  \\frac{8}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{8}{3} + \\frac{8}{3} = \\frac{16}{3}.\n$$\nNext, compute $S = HPH^{\\top} + R$. Given $R = \\frac{4}{3}$:\n$$\nS = \\frac{16}{3} + \\frac{4}{3} = \\frac{20}{3}.\n$$\nSince $S$ is a scalar ($1 \\times 1$ matrix), its inverse is its reciprocal:\n$$\nS^{-1} = \\left(\\frac{20}{3}\\right)^{-1} = \\frac{3}{20}.\n$$\nFinally, we compute the Kalman gain $K = P^{xy}S^{-1} = PH^{\\top}S^{-1}$:\n$$\nK = \\begin{pmatrix} \\frac{8}{3} \\\\ \\frac{8}{3} \\end{pmatrix} \\left(\\frac{3}{20}\\right) = \\begin{pmatrix} \\frac{8}{3} \\cdot \\frac{3}{20} \\\\ \\frac{8}{3} \\cdot \\frac{3}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{20} \\\\ \\frac{8}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix}.\n$$\nThe problem asks for the first component of the Kalman gain vector $K$.\nThe first component is $K_1 = \\frac{2}{5}$.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "Having mastered the basic mechanics, we now turn to the stochastic variant of the EnKF, where each ensemble member is updated with a uniquely perturbed observation. This approach requires us to think about the filter's output not as a single result, but as a distribution. This exercise will guide you in calculating the *expected* analysis mean and variance, providing insight into the statistical properties of the stochastic EnKF and its long-term behavior. ",
            "id": "3425299",
            "problem": "Consider the scalar linear-Gaussian data assimilation problem with state dynamics and observation model given by the random-walk system and a direct noisy observation,\n$$\nx_{k+1} = x_{k} + \\eta_{k}, \\quad \\eta_{k} \\sim \\mathcal{N}(0,1),\n$$\n$$\ny_{k} = x_{k} + \\epsilon_{k}, \\quad \\epsilon_{k} \\sim \\mathcal{N}(0,4),\n$$\nwhere all random variables are mutually independent across time and between process and observation noise. Let the forecast (prior) ensemble at time step $k$ be\n$$\nx^{f} = \\{0,\\,1,\\,2\\},\n$$\nwith ensemble size $N_{e} = 3$, and suppose the observation at time $k$ is $y_{k} = 1$.\n\nYou will perform one analysis step of the stochastic Ensemble Kalman Filter (EnKF), defined as follows. The EnKF (Ensemble Kalman Filter) uses the unbiased sample covariance of the forecast ensemble, computed with denominator $N_{e}-1$, to form the Kalman gain. The observation operator is the identity, so the analysis update for each ensemble member is\n$$\nx^{a}_{i} = x^{f}_{i} + K\\left((y_{k} + \\epsilon_{i}) - x^{f}_{i}\\right),\n$$\nwhere $K$ is the scalar Kalman gain computed from the ensemble, and $\\epsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,4)$ are independent perturbed observation draws, independent of the forecast ensemble. The analysis ensemble mean is the arithmetic mean of $\\{x^{a}_{i}\\}_{i=1}^{N_{e}}$ and the analysis ensemble variance is the unbiased sample variance with denominator $N_{e}-1$.\n\nStarting only from the definitions of Gaussian likelihoods and priors together with the linear model above, derive the Kalman gain used by the EnKF in this setting, then derive the stochastic EnKF analysis update, and compute the expectation (over the perturbed observations $\\{\\epsilon_{i}\\}_{i=1}^{N_{e}}$) of:\n- the analysis ensemble mean, and\n- the analysis ensemble variance (unbiased, denominator $N_{e}-1$),\nconditional on the given forecast ensemble and $y_{k} = 1$.\n\nExpress your final answer as an exact pair consisting of the expected analysis mean and the expected analysis variance, in that order, written as a single row using the $\\texttt{pmatrix}$ notation. No rounding is required and no physical units apply.",
            "solution": "The user has provided a well-posed problem in the context of data assimilation using the Ensemble Kalman Filter (EnKF). The problem is scientifically grounded, formally stated, and complete. All necessary data, models, and definitions are provided. I will proceed with the solution.\n\nThe problem asks for the expectation of the analysis ensemble mean and the analysis ensemble variance, conditional on the given forecast ensemble and observation. The expectation is taken with respect to the random draws used for the perturbed observations.\n\nFirst, I will compute the statistics of the forecast ensemble and the Kalman gain.\nThe given forecast ensemble is $x^{f} = \\{x^{f}_{1}, x^{f}_{2}, x^{f}_{3}\\} = \\{0, 1, 2\\}$, with an ensemble size of $N_{e} = 3$.\nThe forecast ensemble mean, $\\bar{x}^{f}$, is the arithmetic average of the ensemble members:\n$$\n\\bar{x}^{f} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{f}_{i} = \\frac{0 + 1 + 2}{3} = 1.\n$$\nThe problem specifies using the unbiased sample covariance (variance in this scalar case) of the forecast ensemble, denoted $P^{f}$. The denominator for the unbiased sample variance is $N_{e}-1$.\n$$\nP^{f} = \\frac{1}{N_{e}-1} \\sum_{i=1}^{N_{e}} (x^{f}_{i} - \\bar{x}^{f})^{2} = \\frac{1}{3-1} \\left( (0-1)^{2} + (1-1)^{2} + (2-1)^{2} \\right) = \\frac{1}{2} (1 + 0 + 1) = 1.\n$$\nThe observation model is $y_{k} = x_{k} + \\epsilon_{k}$, which is linear with an observation operator $H=1$. The observation noise variance is given as $R = 4$. The Kalman gain $K$ for the EnKF is computed using the sample statistics:\n$$\nK = P^{f} H^{T} (H P^{f} H^{T} + R)^{-1}.\n$$\nIn this scalar case with $H=1$, the formula simplifies to:\n$$\nK = \\frac{P^{f}}{P^{f} + R} = \\frac{1}{1 + 4} = \\frac{1}{5}.\n$$\n\nNext, I will find the expected analysis ensemble mean.\nThe stochastic EnKF analysis update for each ensemble member $x^{f}_{i}$ is given by:\n$$\nx^{a}_{i} = x^{f}_{i} + K\\left((y_{k} + \\epsilon_{i}) - x^{f}_{i}\\right),\n$$\nwhere $y_{k}=1$ is the observation and $\\epsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, R)$ with $R=4$.\nThe analysis ensemble mean, $\\bar{x}^{a}$, is the average of the analysis ensemble members:\n$$\n\\bar{x}^{a} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{a}_{i} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\left[ x^{f}_{i} + K\\left(y_{k} + \\epsilon_{i} - x^{f}_{i}\\right) \\right].\n$$\nRearranging the terms, we get:\n$$\n\\bar{x}^{a} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{f}_{i} + K \\left( y_{k} - \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{f}_{i} \\right) + K \\left( \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\epsilon_{i} \\right) = \\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}) + K\\bar{\\epsilon},\n$$\nwhere $\\bar{\\epsilon} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\epsilon_{i}$.\nWe need to find the expectation of $\\bar{x}^{a}$ over the random variables $\\{\\epsilon_i\\}$, conditional on the fixed forecast ensemble $\\{x^{f}_{i}\\}$ and observation $y_k$.\n$$\nE[\\bar{x}^{a} | \\{x^{f}_{i}\\}, y_{k}] = E[\\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}) + K\\bar{\\epsilon}].\n$$\nSince $\\bar{x}^{f}$, $K$, and $y_{k}$ are constant with respect to this expectation, we have:\n$$\nE[\\bar{x}^{a}] = \\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}) + K E[\\bar{\\epsilon}].\n$$\nThe perturbations $\\epsilon_{i}$ are drawn i.i.d. from $\\mathcal{N}(0,R)$, so their expectation is $E[\\epsilon_{i}] = 0$. The expectation of their mean is also zero:\n$$\nE[\\bar{\\epsilon}] = E\\left[\\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\epsilon_{i}\\right] = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} E[\\epsilon_{i}] = 0.\n$$\nTherefore, the expected analysis mean is:\n$$\nE[\\bar{x}^{a}] = \\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}).\n$$\nSubstituting the numerical values $\\bar{x}^{f}=1$, $y_{k}=1$, and $K=1/5$:\n$$\nE[\\bar{x}^{a}] = 1 + \\frac{1}{5}(1 - 1) = 1.\n$$\n\nFinally, I will find the expected analysis ensemble variance.\nThe analysis ensemble variance, $P^{a}$, is the unbiased sample variance of the analysis ensemble:\n$$\nP^{a} = \\frac{1}{N_{e}-1} \\sum_{i=1}^{N_{e}} (x^{a}_{i} - \\bar{x}^{a})^{2}.\n$$\nFirst, let's find the deviation of each member from the mean, $x^{a}_{i} - \\bar{x}^{a}$:\n$$\nx^{a}_{i} = (1-K)x^{f}_{i} + K y_{k} + K\\epsilon_{i}\n$$\n$$\n\\bar{x}^{a} = (1-K)\\bar{x}^{f} + K y_{k} + K\\bar{\\epsilon}\n$$\nSubtracting the two gives:\n$$\nx^{a}_{i} - \\bar{x}^{a} = (1-K)(x^{f}_{i} - \\bar{x}^{f}) + K(\\epsilon_{i} - \\bar{\\epsilon}).\n$$\nNow we compute the sum of squares:\n$$\n\\sum_{i=1}^{N_{e}} (x^{a}_{i} - \\bar{x}^{a})^{2} = \\sum_{i=1}^{N_{e}} \\left[ (1-K)(x^{f}_{i} - \\bar{x}^{f}) + K(\\epsilon_{i} - \\bar{\\epsilon}) \\right]^{2}\n$$\n$$\n= \\sum_{i=1}^{N_{e}} \\left[ (1-K)^{2}(x^{f}_{i} - \\bar{x}^{f})^{2} + K^{2}(\\epsilon_{i} - \\bar{\\epsilon})^{2} + 2K(1-K)(x^{f}_{i} - \\bar{x}^{f})(\\epsilon_{i} - \\bar{\\epsilon}) \\right]\n$$\n$$\n= (1-K)^{2} \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f})^{2} + K^{2} \\sum_{i=1}^{N_{e}}(\\epsilon_{i} - \\bar{\\epsilon})^{2} + 2K(1-K) \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f})(\\epsilon_{i} - \\bar{\\epsilon}).\n$$\nThe analysis variance is $P^{a} = \\frac{1}{N_{e}-1}\\sum(x^{a}_{i} - \\bar{x}^{a})^{2}$. Using the definitions $P^{f} = \\frac{1}{N_e-1}\\sum(x^{f}_{i} - \\bar{x}^{f})^{2}$ and letting $S^{2}_{\\epsilon} = \\frac{1}{N_e-1}\\sum(\\epsilon_{i} - \\bar{\\epsilon})^{2}$ be the sample variance of the perturbations, we have:\n$$\nP^{a} = (1-K)^{2} P^{f} + K^{2} S^{2}_{\\epsilon} + \\frac{2K(1-K)}{N_{e}-1} \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f})(\\epsilon_{i} - \\bar{\\epsilon}).\n$$\nWe take the expectation of $P^{a}$ over $\\{\\epsilon_i\\}$. The forecast statistics $P^{f}$ and $x^{f}_{i}$ are fixed.\n$$\nE[P^{a}] = (1-K)^{2} P^{f} + K^{2} E[S^{2}_{\\epsilon}] + \\frac{2K(1-K)}{N_{e}-1} \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f}) E[\\epsilon_{i} - \\bar{\\epsilon}].\n$$\nThe expectation of the cross term is zero because $E[\\epsilon_{i} - \\bar{\\epsilon}] = E[\\epsilon_{i}] - E[\\bar{\\epsilon}] = 0 - 0 = 0$.\nThe expectation of the unbiased sample variance $S^{2}_{\\epsilon}$ of i.i.d. variables is the population variance $R$. That is, $E[S^{2}_{\\epsilon}] = R$.\nThus, the expected analysis variance simplifies to:\n$$\nE[P^{a}] = (1-K)^{2} P^{f} + K^{2} R.\n$$\nSubstituting the numerical values $P^{f}=1$, $R=4$, and $K=1/5$:\n$$\nE[P^{a}] = \\left(1 - \\frac{1}{5}\\right)^{2} (1) + \\left(\\frac{1}{5}\\right)^{2} (4) = \\left(\\frac{4}{5}\\right)^{2} + \\frac{1}{25}(4) = \\frac{16}{25} + \\frac{4}{25} = \\frac{20}{25} = \\frac{4}{5}.\n$$\nThe expected analysis mean is $1$ and the expected analysis variance is $4/5$. The final answer is the pair $(1, 4/5)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{4}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A critical challenge in applying the EnKF is ensuring the ensemble is large enough to represent all relevant error structures. The analysis update is fundamentally confined to the subspace spanned by the ensemble anomalies. This final practice explores what happens when this subspace is too small to cover directions of the state space that are not informed by observations. You will derive, from first principles, the minimum ensemble size required to guarantee that the filter can correct errors within the unobserved nullspace of the system, revealing a crucial link between ensemble rank, observability, and filter performance. ",
            "id": "3425309",
            "problem": "Consider a linear inverse problem with state vector $x \\in \\mathbb{R}^{d}$ and observations $y \\in \\mathbb{R}^{r}$ given by $y = H x + v$, where the observation operator $H \\in \\mathbb{R}^{r \\times d}$ has rank $r$ with $r  d$, and the observation noise $v$ is Gaussian with zero mean and covariance $R \\in \\mathbb{R}^{r \\times r}$ that is symmetric positive definite. An Ensemble Kalman Filter (EnKF) is used with $N$ ensemble members $\\{x^{f,(i)}\\}_{i=1}^{N}$ drawn from a full-rank Gaussian prior $\\mathcal{N}(m^{f}, B)$ with $B \\in \\mathbb{R}^{d \\times d}$ symmetric positive definite. Let the ensemble mean be $\\bar{x}^{f} = \\frac{1}{N} \\sum_{i=1}^{N} x^{f,(i)}$ and the anomaly matrix be $A \\in \\mathbb{R}^{d \\times N}$ with columns $a^{(i)} = x^{f,(i)} - \\bar{x}^{f}$, so that $\\sum_{i=1}^{N} a^{(i)} = 0$. The sample forecast covariance is $P^{f} = \\frac{1}{N-1} A A^{\\top}$.\n\nAssume a single linear analysis step of the EnKF is performed with a deterministic Kalman gain and identical observations $y$ used across ensemble members. You may use only the following fundamental facts: (i) the analysis increment of the EnKF is a linear map of the forecast anomalies through the sample covariance $P^{f}$ and the observation operator $H$, (ii) the range of $P^{f}$ equals the span of the columns of $A$, and (iii) the nullspace $\\mathcal{N}(H) = \\{ z \\in \\mathbb{R}^{d} : H z = 0 \\}$ has dimension $d - r$. No other shortcut formulas may be invoked.\n\nPartial observations with rank-deficient $H$ leave directions in $\\mathcal{N}(H)$ unobserved. For the EnKF analysis to be able to represent and adjust any unobserved direction in $\\mathcal{N}(H)$ in one analysis step, the ensemble anomaly subspace must span $\\mathcal{N}(H)$. Derive from first principles the minimal ensemble size $N_{\\mathrm{min}}$ (as a function of $d$ and $r$) such that, under a construction of the ensemble anomalies that includes $d - r$ linearly independent vectors in $\\mathcal{N}(H)$ and satisfies the mean-zero constraint on anomalies, the span of the anomalies contains $\\mathcal{N}(H)$ and hence the EnKF analysis increments can represent all unobserved directions.\n\nProvide your final answer as a closed-form expression for $N_{\\mathrm{min}}$ in terms of $d$ and $r$. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the minimal ensemble size $N_{\\mathrm{min}}$ required for the Ensemble Kalman Filter (EnKF) analysis update to be capable of representing any direction in the nullspace of the observation operator $H$. This capability is contingent on the subspace spanned by the ensemble anomalies containing the nullspace of $H$.\n\nLet the set of $N$ forecast ensemble anomalies be $\\{a^{(1)}, a^{(2)}, \\dots, a^{(N)}\\}$, where each $a^{(i)} \\in \\mathbb{R}^{d}$. By definition, these anomalies satisfy the mean-zero constraint:\n$$ \\sum_{i=1}^{N} a^{(i)} = 0 $$\nLet $S$ be the subspace spanned by these anomalies, commonly referred to as the ensemble subspace:\n$$ S = \\text{span}\\{a^{(1)}, a^{(2)}, \\dots, a^{(N)}\\} $$\nThe problem states that the analysis increments for the EnKF are confined to this subspace $S$. The directions in the state space that are not constrained by the observations $y$ are the vectors in the nullspace of the observation operator $H$, denoted by $\\mathcal{N}(H)$. For the EnKF analysis to be able to make corrections in any of these unobserved directions, the ensemble subspace $S$ must contain the nullspace $\\mathcal{N}(H)$. This gives the fundamental requirement:\n$$ \\mathcal{N}(H) \\subseteq S $$\nThis set inclusion implies a relationship between the dimensions of these two subspaces:\n$$ \\dim(\\mathcal{N}(H)) \\le \\dim(S) $$\nWe will now determine the dimensions of $\\mathcal{N}(H)$ and $S$.\n\nFirst, we determine the dimension of the nullspace, $\\dim(\\mathcal{N}(H))$. The observation operator $H$ is a linear map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{r}$. The rank-nullity theorem states that $\\text{rank}(H) + \\dim(\\mathcal{N}(H)) = d$. The problem specifies that $\\text{rank}(H) = r$. Therefore:\n$$ r + \\dim(\\mathcal{N}(H)) = d $$\n$$ \\dim(\\mathcal{N}(H)) = d - r $$\n\nNext, we determine an upper bound for the dimension of the ensemble subspace, $\\dim(S)$. The $N$ anomaly vectors that span $S$ are not linearly independent due to the mean-zero constraint $\\sum_{i=1}^{N} a^{(i)} = 0$. This constraint means that any one anomaly vector can be expressed as a linear combination of the others, for example, $a^{(N)} = -\\sum_{i=1}^{N-1} a^{(i)}$. Consequently, the set of $N$ anomalies can span a subspace of dimension at most $N-1$. Thus, we have the inequality:\n$$ \\dim(S) \\le N-1 $$\n\nCombining our findings, we have a chain of inequalities:\n$$ d-r = \\dim(\\mathcal{N}(H)) \\le \\dim(S) \\le N-1 $$\nFrom this, we derive a necessary condition on the ensemble size $N$:\n$$ d-r \\le N-1 $$\n$$ N \\ge d-r+1 $$\nThis establishes that the minimal ensemble size, $N_{\\mathrm{min}}$, must be at least $d-r+1$.\n\nTo show that this lower bound is achievable, we must demonstrate that an ensemble of size $N = d-r+1$ can be constructed to satisfy all the problem's conditions. The conditions are:\n1.  The mean-zero constraint is satisfied.\n2.  The ensemble anomalies $\\{a^{(i)}\\}$ include a subset of $d-r$ linearly independent vectors in $\\mathcal{N}(H)$.\n3.  The span of the anomalies, $S$, contains $\\mathcal{N}(H)$.\n\nLet us construct such an ensemble with $N = d-r+1$ members.\nLet $\\{v_1, v_2, \\dots, v_{d-r}\\}$ be a basis for the nullspace $\\mathcal{N}(H)$. These are $d-r$ linearly independent vectors.\nWe define the first $d-r$ anomalies to be these basis vectors:\n$$ a^{(i)} = v_i \\quad \\text{for } i = 1, 2, \\dots, d-r $$\nThis satisfies condition (2) by construction.\n\nTo satisfy the mean-zero constraint (condition 1), the sum of all $N = d-r+1$ anomalies must be the zero vector. We define the final anomaly, $a^{(d-r+1)}$, to ensure this:\n$$ a^{(d-r+1)} = -\\sum_{i=1}^{d-r} a^{(i)} = -\\sum_{i=1}^{d-r} v_i $$\nWith this definition, the sum of all anomalies is:\n$$ \\sum_{i=1}^{d-r+1} a^{(i)} = \\left(\\sum_{i=1}^{d-r} v_i\\right) + \\left(-\\sum_{i=1}^{d-r} v_i\\right) = 0 $$\nThe mean-zero constraint is satisfied.\n\nFinally, we check if the span of our constructed anomalies contains $\\mathcal{N}(H)$ (condition 3). The ensemble subspace is:\n$$ S = \\text{span}\\{ a^{(1)}, \\dots, a^{(d-r)}, a^{(d-r+1)} \\} = \\text{span}\\{ v_1, \\dots, v_{d-r}, -\\sum_{i=1}^{d-r} v_i \\} $$\nSince the last vector in the set, $-\\sum_{i=1}^{d-r} v_i$, is a linear combination of the first $d-r$ vectors, it does not extend the span. Therefore, the span is determined solely by the basis vectors of $\\mathcal{N}(H)$:\n$$ S = \\text{span}\\{v_1, v_2, \\dots, v_{d-r}\\} = \\mathcal{N}(H) $$\nThis shows that $\\mathcal{N}(H) \\subseteq S$ is satisfied (in this case, with equality).\n\nWe have shown that an ensemble of size $N = d-r+1$ can be constructed to meet all requirements. Since we also proved that any ensemble satisfying the requirements must have a size of at least $d-r+1$, we conclude that the minimal ensemble size is precisely this value.\n$$ N_{\\mathrm{min}} = d-r+1 $$",
            "answer": "$$\n\\boxed{d - r + 1}\n$$"
        }
    ]
}