{
    "hands_on_practices": [
        {
            "introduction": "A fundamental objective of any estimation problem is to ensure a unique and stable solution exists. In variational data assimilation, this translates to ensuring the posterior precision matrix—the Hessian of the negative log-posterior—is positive definite. This exercise provides direct, hands-on experience with this concept by exploring how different smoothing priors and sparse observations lead to unobservable modes, and how to strategically add minimal data to regularize the problem .",
            "id": "3406074",
            "problem": "You are asked to formalize and compute unobservability in linear-Gaussian smoothing objectives by constructing examples with large nullspaces, and then to propose and verify minimal additional observations that regularize the posterior. Work entirely in finite-dimensional linear algebra, treating the full space-time trajectory as a single stacked vector and all penalties as quadratic forms.\n\nConsider a discrete-time state sequence $\\{x_t\\}_{t=0}^T$ with $x_t \\in \\mathbb{R}^n$. Stack the trajectory as $z \\in \\mathbb{R}^{n(T+1)}$, where $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top$. A quadratic smoothing objective can be written as\n$$\nJ(z) \\;=\\; \\tfrac{1}{2}\\,\\|L z\\|_2^2 \\;+\\; \\tfrac{1}{2}\\,\\|C z - y\\|_2^2,\n$$\nwhere $L$ encodes smoothing or dynamical penalties and $C$ encodes available linear observations. Under a linear-Gaussian Bayesian interpretation, the Maximum A Posteriori (MAP) estimator coincides with the minimizer of $J(z)$, and the Hessian (posterior precision) is\n$$\n\\mathcal{I} \\;=\\; L^\\top L \\;+\\; C^\\top C.\n$$\nUnobservability is present when $\\mathcal{I}$ is singular; its nullspace $\\mathcal{N}(\\mathcal{I})$ identifies directions that are not penalized by dynamics/smoothing nor by the observations. The dimension of $\\mathcal{N}(\\mathcal{I})$ is the nullity of $\\mathcal{I}$, equivalently $n(T+1) - \\mathrm{rank}(\\mathcal{I})$.\n\nYou will study two canonical smoothing operators that yield large nullspaces:\n\n- First-difference (random-walk) smoothing: for $t \\in \\{0,\\dots,T-1\\}$, penalize $x_{t+1} - x_t$. This corresponds to $L \\in \\mathbb{R}^{nT \\times n(T+1)}$ with block structure such that the $t$-th row-block equals $[0,\\dots,0,-I_n, I_n, 0,\\dots,0]$ acting on $[x_t^\\top, x_{t+1}^\\top]^\\top$.\n- Second-difference (acceleration) smoothing: for $t \\in \\{1,\\dots,T-1\\}$, penalize $x_{t+1} - 2 x_t + x_{t-1}$. This corresponds to $L \\in \\mathbb{R}^{n(T-1) \\times n(T+1)}$ with block structure such that the $t$-th row-block equals $[0,\\dots,0, I_n, -2 I_n, I_n, 0,\\dots,0]$ acting on $[x_{t-1}^\\top, x_t^\\top, x_{t+1}^\\top]^\\top$.\n\nAssume all penalties are isotropic with identity weights for simplicity, so the $2$-norms above are Euclidean norms. Observations are pointwise component measurements: if you observe component indices $S \\subset \\{0,\\dots,n-1\\}$ of $x_t$ at time $t$, then $C$ contains one row $e_{t,i}^\\top$ per $(t,i) \\in \\{t\\} \\times S$, where $e_{t,i} \\in \\mathbb{R}^{n(T+1)}$ is the canonical basis vector selecting the $i$-th component at time $t$ (that is, it has a $1$ at the position corresponding to $(t,i)$ and $0$ elsewhere). Take measurement noise precision equal to identity, so $C^\\top C$ contributes directly to $\\mathcal{I}$.\n\nYour program must, for each specified test case:\n- Construct $L$ as either a first-difference or second-difference operator for the given $n$ and $T$.\n- Construct $C$ from the provided set of observed components at specified times.\n- Compute the nullity $k_0$ of $\\mathcal{I} = L^\\top L + C^\\top C$.\n- Propose a minimal set of additional linear observations to regularize the posterior. You must implement this by computing a basis $\\{u_j\\}_{j=1}^{k_0}$ for $\\mathcal{N}(\\mathcal{I})$ and taking the rows of the additional observation operator $C_{\\mathrm{add}}$ to be $u_j^\\top$. This choice uses $k_0$ scalar observations and guarantees that the updated precision\n$$\n\\mathcal{I}_{\\mathrm{new}} \\;=\\; \\mathcal{I} \\;+\\; C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n$$\nis positive definite when $k_0 > 0$, and remains unchanged when $k_0 = 0$.\n- Set $k_{\\min} = k_0$ as the theoretically minimal number of additional scalar observations required, and verify success by checking that the nullity of $\\mathcal{I}_{\\mathrm{new}}$ is $0$.\n\nNumerical linear algebra hint for implementation (not a solution method shortcut): use symmetric eigendecomposition of $\\mathcal{I}$ to obtain its eigenvalues and eigenvectors, with a numerically reasonable threshold to define “zero” eigenvalues.\n\nTest suite. Your program must solve the following cases:\n\n- Case A: first-difference, $n = 3$, $T = 5$, no observations.\n- Case B: first-difference, $n = 3$, $T = 5$, observations at time $t = 0$ of components $\\{0,1\\}$.\n- Case C: second-difference, $n = 2$, $T = 6$, no observations.\n- Case D: second-difference, $n = 2$, $T = 6$, observations at time $t = 0$ of components $\\{0,1\\}$.\n- Case E: second-difference, $n = 2$, $T = 3$, observations at times $t = 0$ and $t = 3$ of component $\\{0\\}$ (that is, observe component $0$ at both endpoints).\n- Case F: second-difference, $n = 1$, $T = 4$, observations at times $t = 0$ and $t = 4$ of component $\\{0\\}$.\n\nRequired outputs. For each case, output a list $[k_0, k_{\\min}, s]$ where $k_0$ is the computed nullity of $\\mathcal{I}$, $k_{\\min}$ is the minimal number of additional scalar observations you propose (equal to $k_0$ by construction), and $s$ is a boolean indicating whether the updated precision $\\mathcal{I}_{\\mathrm{new}}$ is numerically full rank (nullity equal to $0$). Your program should produce a single line of output containing the results for Cases A–F as a comma-separated list of these lists, enclosed in square brackets; for example, a valid format is like $[[r_1],[r_2],\\dots]$ where each $[r_j]$ is the triplet for case $j$ in the specified order A–F.",
            "solution": "The problem requires an analysis of unobservability in linear smoothing problems by constructing and regularizing systems with singular posterior precision matrices. The entire space-time trajectory of the state $\\{x_t\\}_{t=0}^T$, where $x_t \\in \\mathbb{R}^n$, is represented as a single stacked vector $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top \\in \\mathbb{R}^{n(T+1)}$.\n\nThe smoothing problem is framed as the minimization of a quadratic objective function $J(z)$:\n$$\nJ(z) = \\tfrac{1}{2} \\|L z\\|_2^2 + \\tfrac{1}{2} \\|C z - y\\|_2^2\n$$\nThe first term corresponds to a prior or smoothing penalty, encoded by the matrix $L$. The second term represents the penalty for mismatching a set of linear observations $y$, encoded by the matrix $C$. In a Bayesian context, this corresponds to finding the Maximum A Posteriori (MAP) estimate for a linear-Gaussian model. The posterior precision matrix, which is the Hessian of $J(z)$, is given by:\n$$\n\\mathcal{I} = L^\\top L + C^\\top C\n$$\nUnobservability arises when $\\mathcal{I}$ is singular, meaning it has a non-trivial nullspace, $\\mathcal{N}(\\mathcal{I})$. The dimension of this nullspace, its nullity, quantifies the number of independent directions in the state space that are not constrained by either the dynamics (smoothing) or the observations. Any vector $v \\in \\mathcal{N}(\\mathcal{I})$ can be added to a solution $z^*$ without changing the cost $J$, since for any $v \\in \\mathcal{N}(\\mathcal{I})$, $v$ must be in $\\mathcal{N}(L)$ and $\\mathcal{N}(C)$ (assuming $y=0$ for the nullspace analysis), thus $L(\\alpha v)=0$ and $C(\\alpha v)=0$.\n\nThe procedure to solve each test case involves the following steps:\n\n1.  **Construct the Prior Precision Matrix $L^\\top L$**:\n    The total dimension of the problem is $d = n(T+1)$. The matrix $L^\\top L$ is a $d \\times d$ matrix.\n    -   For a **first-difference** smoother, the penalty is on $x_{t+1} - x_t$ for $t=0, \\dots, T-1$. This implies that the nullspace of $L$ consists of trajectories where $x_t$ is constant for all $t$. Specifically, if $z \\in \\mathcal{N}(L)$, then $x_0 = x_1 = \\dots = x_T$. This is an $n$-dimensional nullspace, spanned by trajectories of the form $[v^\\top, v^\\top, \\dots, v^\\top]^\\top$ for any $v \\in \\mathbb{R}^n$. The matrix $L$ can be constructed with $n T$ rows, where each block of $n$ rows corresponding to time $t$ has a $-I_n$ block at column block $t$ and an $I_n$ block at column block $t+1$.\n    -   For a **second-difference** smoother, the penalty is on $x_{t+1} - 2x_t + x_{t-1}$ for $t=1, \\dots, T-1$. The nullspace of $L$ consists of trajectories that evolve linearly with time, i.e., $x_t = x_0 + t(x_1 - x_0)$. Such a trajectory is fully determined by the initial two states $x_0$ and $x_1$. This constitutes a $2n$-dimensional nullspace. The matrix $L$ is constructed with $n(T-1)$ rows, where each block of $n$ rows for time $t$ has blocks $I_n, -2I_n, I_n$ at column blocks $t-1, t, t+1$ respectively.\n    In both cases, we compute $L$ and then form the prior precision $L^\\top L$.\n\n2.  **Construct the Observation Precision Matrix $C^\\top C$**:\n    Observations are pointwise measurements of specific components. An observation of the $i$-th component of $x_t$ corresponds to a row in $C$ given by $e_{t,i}^\\top$, where $e_{t,i}$ is the canonical basis vector in $\\mathbb{R}^{n(T+1)}$ with a $1$ at the position corresponding to the $i$-th component of $x_t$ and zeros elsewhere. With an identity measurement noise precision, the total contribution from observations to the posterior precision is $C^\\top C = \\sum_{(t,i) \\in \\text{obs}} e_{t,i} e_{t,i}^\\top$. This is a diagonal matrix with $1$s at indices corresponding to the observed components. We add this matrix to $L^\\top L$ to form the full posterior precision $\\mathcal{I}$. The global index for component $i$ of state $x_t$ is $t \\cdot n + i$.\n\n3.  **Compute Initial Nullity ($k_0$)**:\n    The posterior precision matrix $\\mathcal{I}$ is real and symmetric by construction. We compute its eigenvalues and eigenvectors using a symmetric eigensolver (`numpy.linalg.eigh`). The nullity $k_0$ is the number of eigenvalues that are numerically close to zero, judged by a small tolerance (e.g., $10^{-9}$). The corresponding eigenvectors $\\{u_j\\}_{j=1}^{k_0}$ form an orthonormal basis for the nullspace $\\mathcal{N}(\\mathcal{I})$.\n\n4.  **Propose and Apply Regularization**:\n    To make the problem well-posed, we must add new observations that penalize the unobserved modes. The problem specifies a minimal regularization strategy. We introduce $k_{\\min} = k_0$ new scalar observations. The new observation operator, $C_{\\mathrm{add}}$, is defined by taking its rows to be the basis vectors of the nullspace, i.e., $C_{\\mathrm{add}} = [u_1, u_2, \\dots, u_{k_0}]^\\top$. The contribution to the precision matrix is $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}$. Since the basis vectors $\\{u_j\\}$ are orthonormal, $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}} = \\sum_{j=1}^{k_0} u_j u_j^\\top$, which is the projection matrix onto $\\mathcal{N}(\\mathcal{I})$.\n    The new, regularized precision matrix is:\n    $$\n    \\mathcal{I}_{\\mathrm{new}} = \\mathcal{I} + C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n    $$\n\n5.  **Verify Regularization**:\n    We verify the success of the regularization by computing the nullity of $\\mathcal{I}_{\\mathrm{new}}$. For any vector $v$ in the original nullspace, $v = \\sum_k \\alpha_k u_k$. Then $\\mathcal{I}v=0$. The new matrix acts as:\n    $$\n    \\mathcal{I}_{\\mathrm{new}} v = (\\mathcal{I} + \\sum_{j=1}^{k_0} u_j u_j^\\top) (\\sum_{k=1}^{k_0} \\alpha_k u_k) = 0 + \\sum_{j,k} \\alpha_k u_j (u_j^\\top u_k) = \\sum_{j,k} \\alpha_k u_j \\delta_{jk} = \\sum_k \\alpha_k u_k = v\n    $$\n    This shows that the vectors spanning the original nullspace are now eigenvectors of $\\mathcal{I}_{\\mathrm{new}}$ with eigenvalue $1$. For any eigenvector $w$ of $\\mathcal{I}$ with non-zero eigenvalue $\\lambda$ (so $w \\perp \\mathcal{N}(\\mathcal{I})$), we have $\\mathcal{I}_{\\mathrm{new}} w = \\mathcal{I}w + (\\sum u_j u_j^\\top)w = \\lambda w + 0 = \\lambda w$. The other eigenvalues and eigenvectors remain unchanged. Thus, $\\mathcal{I}_{\\mathrm{new}}$ has no zero eigenvalues and is positive definite. We confirm this numerically by computing the eigenvalues of $\\mathcal{I}_{\\mathrm{new}}$ and checking that its nullity is $0$. The success is recorded as a boolean value $s$.\n\nThis complete procedure is implemented for each of the specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of test cases for analyzing unobservability\n    in linear-Gaussian smoothing problems.\n    \"\"\"\n    \n    # Test suite as specified in the problem statement\n    test_cases = [\n        # Case A: first-difference, n=3, T=5, no observations\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': []},\n        # Case B: first-difference, n=3, T=5, obs at t=0 of components {0,1}\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': [(0, [0, 1])]},\n        # Case C: second-difference, n=2, T=6, no observations\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': []},\n        # Case D: second-difference, n=2, T=6, obs at t=0 of components {0,1}\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': [(0, [0, 1])]},\n        # Case E: second-difference, n=2, T=3, obs at t=0, t=3 of component {0}\n        {'type': 'second_difference', 'n': 2, 'T': 3, 'obs': [(0, [0]), (3, [0])]},\n        # Case F: second-difference, n=1, T=4, obs at t=0, t=4 of component {0}\n        {'type': 'second_difference', 'n': 1, 'T': 4, 'obs': [(0, [0]), (4, [0])]},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(case['type'], case['n'], case['T'], case['obs'])\n        results.append(result)\n\n    # Format the final output string exactly as required\n    # e.g., [[k0_A,kmin_A,s_A],[k0_B,kmin_B,s_B],...]\n    result_strings = [f\"[{r[0]},{r[1]},{str(r[2]).lower()}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef _solve_case(smoother_type, n, T, observations):\n    \"\"\"\n    Solves a single instance of the observability problem.\n    \"\"\"\n    # Total dimension of the stacked state vector z\n    dim = n * (T + 1)\n    \n    # Numerical tolerance for identifying zero eigenvalues\n    TOL = 1e-9\n\n    # 1. Construct the smoothing operator L\n    if smoother_type == 'first_difference':\n        # L has n*T rows and n*(T+1) columns\n        L = np.zeros((n * T, dim))\n        identity_n = np.eye(n)\n        for t in range(T):\n            row_slice = slice(t * n, (t + 1) * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_t1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_t] = -identity_n\n            L[row_slice, col_slice_t1] = identity_n\n    \n    elif smoother_type == 'second_difference':\n        # L has n*(T-1) rows and n*(T+1) columns\n        L = np.zeros((n * (T - 1), dim))\n        identity_n = np.eye(n)\n        for t_idx, t in enumerate(range(1, T)):\n            row_slice = slice(t_idx * n, (t_idx + 1) * n)\n            col_slice_tm1 = slice((t - 1) * n, t * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_tp1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_tm1] = identity_n\n            L[row_slice, col_slice_t] = -2 * identity_n\n            L[row_slice, col_slice_tp1] = identity_n\n    else:\n        raise ValueError(\"Unknown smoother type\")\n\n    # 2. Construct the posterior precision matrix I\n    # Start with the prior precision L^T L\n    I = L.T @ L\n    \n    # Add observation precision C^T C\n    for t, components in observations:\n        for i in components:\n            idx = t * n + i\n            I[idx, idx] += 1.0\n\n    # 3. Compute initial nullity k0 and find nullspace basis\n    eigenvalues, eigenvectors = np.linalg.eigh(I)\n    is_zero_eigenvalue = np.abs(eigenvalues) < TOL\n    k0 = int(np.sum(is_zero_eigenvalue))\n\n    # 4. Propose and apply regularization\n    k_min = k0\n    \n    if k0 == 0:\n        I_new = I\n    else:\n        nullspace_basis = eigenvectors[:, is_zero_eigenvalue]\n        # C_add.T @ C_add = sum(u_j @ u_j.T)\n        C_add_T_C_add = nullspace_basis @ nullspace_basis.T\n        I_new = I + C_add_T_C_add\n        \n    # 5. Verify successful regularization\n    new_eigenvalues, _ = np.linalg.eigh(I_new)\n    new_nullity = np.sum(np.abs(new_eigenvalues) < TOL)\n    s = (new_nullity == 0)\n    \n    return [k0, k_min, s]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Often, not only the state trajectory is unknown, but also static parameters within the dynamic model itself. This practice demonstrates a powerful and general technique for this common scenario: augmenting the state vector with the unknown parameters and solving a larger estimation problem. You will use the Schur complement of the Hessian to understand the effective curvature, or information content, for the parameter after accounting for the uncertainty in the states .",
            "id": "3405992",
            "problem": "Consider a scalar linear-Gaussian state-space model with a single time step, in which the state-transition parameter is static and unknown. The system is\n$$x_{1} = \\theta x_{0} + w_{0}, \\quad w_{0} \\sim \\mathcal{N}(0,q),$$\nand the observation is\n$$y_{1} = x_{1} + v_{1}, \\quad v_{1} \\sim \\mathcal{N}(0,r).$$\nAssume independent Gaussian priors\n$$x_{0} \\sim \\mathcal{N}(0,\\sigma_{0}^{2}), \\quad \\theta \\sim \\mathcal{N}(0,\\sigma_{\\theta}^{2}).$$\nLet the augmented variable be\n$$z \\equiv \\begin{pmatrix} x_{0} \\\\ x_{1} \\\\ \\theta \\end{pmatrix}.$$\nForm the negative log-posterior objective (up to an additive constant),\n$$J(z) = \\frac{1}{2q}\\left(x_{1} - \\theta x_{0}\\right)^{2} + \\frac{1}{2r}\\left(y_{1} - x_{1}\\right)^{2} + \\frac{1}{2\\sigma_{0}^{2}} x_{0}^{2} + \\frac{1}{2\\sigma_{\\theta}^{2}} \\theta^{2}.$$\nUsing the augmented-state smoothing viewpoint, approximate the Hessian of $J(z)$ by the Gauss–Newton Hessian constructed from the Jacobian of the residuals. Partition this Hessian into blocks with respect to the state vector $\\begin{pmatrix} x_{0} & x_{1} \\end{pmatrix}^{\\top}$ and the parameter $\\theta$. Denote the blocks as\n$$H_{xx} \\in \\mathbb{R}^{2 \\times 2}, \\quad H_{x\\theta} \\in \\mathbb{R}^{2 \\times 1}, \\quad H_{\\theta x} = H_{x\\theta}^{\\top}, \\quad H_{\\theta\\theta} \\in \\mathbb{R}.$$\nCompute, in closed form, the Schur complement\n$$S \\equiv H_{\\theta\\theta} - H_{\\theta x}\\,H_{xx}^{-1}\\,H_{x\\theta},$$\nwhich is the Gauss–Newton curvature of the trajectory objective with respect to $\\theta$ after eliminating the state components by smoothing. Express your final result as a simplified analytic expression in terms of the symbols $q$, $r$, $\\sigma_{0}^{2}$, $\\sigma_{\\theta}^{2}$, $\\theta$, and $x_{0}$ only. Do not introduce any numerical values or approximations. Your final answer must be a single closed-form expression. No rounding is required and no units are needed.",
            "solution": "- State-space model: $x_{1} = \\theta x_{0} + w_{0}$, with noise $w_{0} \\sim \\mathcal{N}(0,q)$.\n- Observation model: $y_{1} = x_{1} + v_{1}$, with noise $v_{1} \\sim \\mathcal{N}(0,r)$.\n- Priors: $x_{0} \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$ and $\\theta \\sim \\mathcal{N}(0,\\sigma_{\\theta}^{2})$. The random variables $w_0$, $v_1$, $x_0$, and $\\theta$ are independent.\n- Augmented variable: $z \\equiv \\begin{pmatrix} x_{0} \\\\ x_{1} \\\\ \\theta \\end{pmatrix}^{\\top}$.\n- Negative log-posterior objective function (up to an additive constant):\n$$J(z) = \\frac{1}{2q}\\left(x_{1} - \\theta x_{0}\\right)^{2} + \\frac{1}{2r}\\left(y_{1} - x_{1}\\right)^{2} + \\frac{1}{2\\sigma_{0}^{2}} x_{0}^{2} + \\frac{1}{2\\sigma_{\\theta}^{2}} \\theta^{2}.$$\n- Objective: Compute the Schur complement $S \\equiv H_{\\theta\\theta} - H_{\\theta x}\\,H_{xx}^{-1}\\,H_{x\\theta}$, where $H$ is the Gauss–Newton approximation of the Hessian of $J(z)$, partitioned with respect to the state vector $\\begin{pmatrix} x_{0} & x_{1} \\end{pmatrix}^{\\top}$ and the parameter $\\theta$.\n\nThe goal is to compute the Schur complement $S = H_{\\theta\\theta} - H_{\\theta x}\\,H_{xx}^{-1}\\,H_{x\\theta}$. The first step is to compute the Gauss-Newton Hessian of $J(z)$. The objective function $J(z)$ is a sum of squared terms, characteristic of a non-linear least-squares problem. We can write $J(z) = \\frac{1}{2} \\mathbf{R}(z)^{\\top} \\mathbf{R}(z)$, where $\\mathbf{R}(z)$ is the vector of weighted residuals.\n\nThe residuals are:\n$$\n\\mathbf{R}(z) = \\begin{pmatrix}\n(x_{1} - \\theta x_{0}) / \\sqrt{q} \\\\\n(y_{1} - x_{1}) / \\sqrt{r} \\\\\nx_{0} / \\sigma_{0} \\\\\n\\theta / \\sigma_{\\theta}\n\\end{pmatrix}\n$$\nThe Gauss-Newton Hessian is given by $H = \\mathcal{J}^{\\top}\\mathcal{J}$, where $\\mathcal{J}$ is the Jacobian of the residual vector $\\mathbf{R}(z)$ with respect to the augmented state $z = (x_0, x_1, \\theta)^{\\top}$.\nThe Jacobian $\\mathcal{J} = \\frac{\\partial \\mathbf{R}}{\\partial z^{\\top}}$ is calculated as:\n$$\n\\mathcal{J}(z) = \n\\begin{pmatrix}\n-\\theta/\\sqrt{q} & 1/\\sqrt{q} & -x_0/\\sqrt{q} \\\\\n0 & -1/\\sqrt{r} & 0 \\\\\n1/\\sigma_0 & 0 & 0 \\\\\n0 & 0 & 1/\\sigma_\\theta\n\\end{pmatrix}\n$$\nThe Gauss-Newton Hessian is $H = \\mathcal{J}^{\\top}\\mathcal{J}$:\n$$\nH = \n\\begin{pmatrix}\n-\\theta/\\sqrt{q} & 0 & 1/\\sigma_0 & 0 \\\\\n1/\\sqrt{q} & -1/\\sqrt{r} & 0 & 0 \\\\\n-x_0/\\sqrt{q} & 0 & 0 & 1/\\sigma_\\theta\n\\end{pmatrix}\n\\begin{pmatrix}\n-\\theta/\\sqrt{q} & 1/\\sqrt{q} & -x_0/\\sqrt{q} \\\\\n0 & -1/\\sqrt{r} & 0 \\\\\n1/\\sigma_0 & 0 & 0 \\\\\n0 & 0 & 1/\\sigma_\\theta\n\\end{pmatrix}\n$$\nPerforming the matrix multiplication gives:\n$$\nH = \n\\begin{pmatrix}\n\\frac{\\theta^2}{q} + \\frac{1}{\\sigma_0^2} & -\\frac{\\theta}{q} & \\frac{\\theta x_0}{q} \\\\\n-\\frac{\\theta}{q} & \\frac{1}{q} + \\frac{1}{r} & -\\frac{x_0}{q} \\\\\n\\frac{\\theta x_0}{q} & -\\frac{x_0}{q} & \\frac{x_0^2}{q} + \\frac{1}{\\sigma_\\theta^2}\n\\end{pmatrix}\n$$\nNow, we partition this Hessian according to the state vector $x = (x_0, x_1)^{\\top}$ and the parameter $\\theta$:\n$$\nH_{xx} = \\begin{pmatrix} \\frac{\\theta^2}{q} + \\frac{1}{\\sigma_0^2} & -\\frac{\\theta}{q} \\\\ -\\frac{\\theta}{q} & \\frac{1}{q} + \\frac{1}{r} \\end{pmatrix},\n\\quad\nH_{x\\theta} = \\begin{pmatrix} \\frac{\\theta x_0}{q} \\\\ -\\frac{x_0}{q} \\end{pmatrix}\n$$\n$$\nH_{\\theta x} = H_{x\\theta}^{\\top} = \\begin{pmatrix} \\frac{\\theta x_0}{q} & -\\frac{x_0}{q} \\end{pmatrix},\n\\quad\nH_{\\theta\\theta} = \\frac{x_0^2}{q} + \\frac{1}{\\sigma_\\theta^2}\n$$\nTo compute the Schur complement, we must first find $H_{xx}^{-1}$. The determinant of $H_{xx}$ is:\n$$\n\\det(H_{xx}) = \\left(\\frac{\\theta^2}{q} + \\frac{1}{\\sigma_0^2}\\right) \\left(\\frac{1}{q} + \\frac{1}{r}\\right) - \\left(-\\frac{\\theta}{q}\\right)^2\n$$\n$$\n= \\frac{\\theta^2}{q^2} + \\frac{\\theta^2}{qr} + \\frac{1}{q\\sigma_0^2} + \\frac{1}{r\\sigma_0^2} - \\frac{\\theta^2}{q^2} = \\frac{\\theta^2}{qr} + \\frac{1}{q\\sigma_0^2} + \\frac{1}{r\\sigma_0^2}\n$$\n$$\n= \\frac{\\theta^2\\sigma_0^2 + r + q}{qr\\sigma_0^2}\n$$\nThe inverse is then:\n$$\nH_{xx}^{-1} = \\frac{1}{\\det(H_{xx})} \\begin{pmatrix} \\frac{1}{q} + \\frac{1}{r} & \\frac{\\theta}{q} \\\\ \\frac{\\theta}{q} & \\frac{\\theta^2}{q} + \\frac{1}{\\sigma_0^2} \\end{pmatrix} = \\frac{qr\\sigma_0^2}{\\theta^2\\sigma_0^2 + r + q} \\begin{pmatrix} \\frac{q+r}{qr} & \\frac{\\theta}{q} \\\\ \\frac{\\theta}{q} & \\frac{\\theta^2\\sigma_0^2 + q}{q\\sigma_0^2} \\end{pmatrix}\n$$\nNext, we compute the quadratic form $H_{\\theta x} H_{xx}^{-1} H_{x\\theta}$.\n$$\nH_{\\theta x} H_{xx}^{-1} H_{x\\theta} = \\begin{pmatrix} \\frac{\\theta x_0}{q} & -\\frac{x_0}{q} \\end{pmatrix} H_{xx}^{-1} \\begin{pmatrix} \\frac{\\theta x_0}{q} \\\\ -\\frac{x_0}{q} \\end{pmatrix}\n$$\nLet's factor out common terms:\n$$\n= \\frac{x_0^2}{q^2} \\begin{pmatrix} \\theta & -1 \\end{pmatrix} \\left( \\frac{qr\\sigma_0^2}{\\theta^2\\sigma_0^2 + r + q} \\begin{pmatrix} \\frac{q+r}{qr} & \\frac{\\theta}{q} \\\\ \\frac{\\theta}{q} & \\frac{\\theta^2\\sigma_0^2 + q}{q\\sigma_0^2} \\end{pmatrix} \\right) \\begin{pmatrix} \\theta \\\\ -1 \\end{pmatrix}\n$$\nFirst, multiply the row vector with the matrix:\n$$\n\\begin{pmatrix} \\theta & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{q+r}{qr} & \\frac{\\theta}{q} \\\\ \\frac{\\theta}{q} & \\frac{\\theta^2\\sigma_0^2 + q}{q\\sigma_0^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\theta(q+r)}{qr} - \\frac{\\theta}{q} & \\frac{\\theta^2}{q} - \\frac{\\theta^2\\sigma_0^2 + q}{q\\sigma_0^2} \\end{pmatrix}\n$$\nThe first element simplifies to $\\frac{\\theta}{q}(\\frac{q+r}{r}-1) = \\frac{\\theta}{q}(\\frac{q}{r}) = \\frac{\\theta}{r}$.\nThe second element simplifies to $\\frac{1}{q\\sigma_0^2}(\\theta^2\\sigma_0^2 - (\\theta^2\\sigma_0^2+q)) = -\\frac{q}{q\\sigma_0^2} = -\\frac{1}{\\sigma_0^2}$.\nSo the intermediate row vector is $\\begin{pmatrix} \\frac{\\theta}{r} & -\\frac{1}{\\sigma_0^2} \\end{pmatrix}$.\nNow, continue the full product:\n$$\nH_{\\theta x} H_{xx}^{-1} H_{x\\theta} = \\frac{x_0^2}{q^2} \\frac{qr\\sigma_0^2}{\\theta^2\\sigma_0^2 + r + q} \\begin{pmatrix} \\frac{\\theta}{r} & -\\frac{1}{\\sigma_0^2} \\end{pmatrix} \\begin{pmatrix} \\theta \\\\ -1 \\end{pmatrix}\n$$\n$$\n= \\frac{x_0^2 r\\sigma_0^2}{q(\\theta^2\\sigma_0^2 + r + q)} \\left( \\frac{\\theta^2}{r} + \\frac{1}{\\sigma_0^2} \\right) = \\frac{x_0^2 r\\sigma_0^2}{q(\\theta^2\\sigma_0^2 + r + q)} \\left( \\frac{\\theta^2\\sigma_0^2 + r}{r\\sigma_0^2} \\right)\n$$\nCanceling terms yields:\n$$\nH_{\\theta x} H_{xx}^{-1} H_{x\\theta} = \\frac{x_0^2}{q} \\frac{\\theta^2\\sigma_0^2 + r}{\\theta^2\\sigma_0^2 + r + q}\n$$\nFinally, we compute the Schur complement $S$:\n$$\nS = H_{\\theta\\theta} - H_{\\theta x} H_{xx}^{-1} H_{x\\theta} = \\left(\\frac{x_0^2}{q} + \\frac{1}{\\sigma_\\theta^2}\\right) - \\frac{x_0^2}{q} \\frac{\\theta^2\\sigma_0^2 + r}{\\theta^2\\sigma_0^2 + r + q}\n$$\n$$\nS = \\frac{1}{\\sigma_\\theta^2} + \\frac{x_0^2}{q} \\left( 1 - \\frac{\\theta^2\\sigma_0^2 + r}{\\theta^2\\sigma_0^2 + r + q} \\right)\n$$\n$$\nS = \\frac{1}{\\sigma_\\theta^2} + \\frac{x_0^2}{q} \\left( \\frac{(\\theta^2\\sigma_0^2 + r + q) - (\\theta^2\\sigma_0^2 + r)}{\\theta^2\\sigma_0^2 + r + q} \\right)\n$$\n$$\nS = \\frac{1}{\\sigma_\\theta^2} + \\frac{x_0^2}{q} \\left( \\frac{q}{\\theta^2\\sigma_0^2 + r + q} \\right)\n$$\nThis simplifies to the final expression:\n$$\nS = \\frac{1}{\\sigma_\\theta^2} + \\frac{x_0^2}{\\theta^2\\sigma_0^2 + r + q}\n$$",
            "answer": "$$\\boxed{\\frac{1}{\\sigma_{\\theta}^{2}} + \\frac{x_{0}^{2}}{\\theta^{2}\\sigma_{0}^{2} + r + q}}$$"
        },
        {
            "introduction": "While linear-Gaussian models provide a clean theoretical foundation, most real-world applications are nonlinear, leading to non-convex objective functions with multiple local minima. This practice confronts this complexity directly, using a system with a quadratic observation model to create sign ambiguities and distinct solutions. You will implement advanced numerical strategies, including homotopy methods to robustly track solutions and the Laplace approximation to compare the evidence for different local minima, which are essential skills for practical data assimilation .",
            "id": "3406013",
            "problem": "Consider a discrete-time, scalar, nonlinear state-space model used for fixed-interval smoothing over times $t = 0, 1, \\dots, T$. The latent state sequence is $x_{0:T} \\equiv (x_0, x_1, \\dots, x_T) \\in \\mathbb{R}^{T+1}$. The model is specified by the following elements:\n\n- A Gaussian prior for the initial state: $x_0 \\sim \\mathcal{N}(m_0, P_0)$, with prior mean $m_0 \\in \\mathbb{R}$ and prior variance $P_0 \\in \\mathbb{R}_{>0}$.\n- A Markovian dynamics model: $x_{t+1} = f(x_t) + w_t$, where $f(x) = a x$ for a given $a \\in \\mathbb{R}$ and $w_t \\sim \\mathcal{N}(0, q)$ with process noise variance $q \\in \\mathbb{R}_{>0}$, independent across $t$.\n- A nonlinear observation model: $y_t = h(x_t) + v_t$, where $h(x) = x^2$ and $v_t \\sim \\mathcal{N}(0, r)$ with observation noise variance $r \\in \\mathbb{R}_{>0}$, independent across $t$ and independent of the process noise.\n\nAll distributions are understood to be conditionally independent according to the above structure. Let $y_{0:T} \\equiv (y_0, y_1, \\dots, y_T)$ denote the observed data.\n\nYour tasks are:\n\n1. Starting from Bayes' theorem and the definition of the joint probability density under the above Gaussian assumptions, derive the fixed-interval smoothing objective $J(x_{0:T})$ as the negative log of the posterior density $p(x_{0:T} \\mid y_{0:T})$ up to an additive constant that does not depend on $x_{0:T}$. Express $J(x_{0:T})$ in terms of the model parameters and residuals between the state sequence and the model/observations. Do not invoke unproven shortcut formulas; start from the factorization of the joint density and proceed via the properties of Gaussian densities.\n2. For the specific choice $f(x) = a x$ and $h(x) = x^2$, derive the exact gradient $\\nabla J(x_{0:T})$ and the exact Hessian matrix $\\nabla^2 J(x_{0:T})$ with respect to $x_{0:T}$. Your derivation must be explicit in $a$, $q$, $r$, $m_0$, $P_0$, and $y_{0:T}$.\n3. A local minimizer $x_{0:T}^\\star$ corresponds to a candidate for the Maximum A Posteriori (MAP) estimate. The Laplace approximation constructs a Gaussian approximation to the posterior near a local minimizer via the second-order Taylor expansion of $J(x_{0:T})$. Using your Hessian, write down the Gaussian approximation and the corresponding approximation to the local contribution to the model evidence, with logarithm given by\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{(T+1)}{2} \\log(2\\pi),\n$$\nnoting that the final term arises from normalizing a $(T+1)$-dimensional Gaussian density.\n4. Propose a homotopy in the observation noise variance $r$ by introducing a scalar parameter $\\lambda \\in (0, 1]$ and defining $r(\\lambda) = r_{\\text{base}} / \\lambda$ with fixed $r_{\\text{base}} \\in \\mathbb{R}_{>0}$. Explain why decreasing $\\lambda$ increases $r(\\lambda)$ (weakening observation influence), and increasing $\\lambda$ decreases $r(\\lambda)$ (strengthening observation influence).\n5. Implement a continuation strategy in $\\lambda$ to track a family of local minimizers from a weak-observation regime (smaller $\\lambda$) to the target regime $\\lambda = 1$. At each $\\lambda$ on a specified discrete schedule, use Newton's method with a backtracking line search and Levenberg–Marquardt-style damping to find a local minimizer, initialized from the previous $\\lambda$'s solution. Ensure that at the final $\\lambda = 1$, your candidate is a local minimum by verifying that the Hessian is positive definite.\n\nFor numerical study and automated testing, use the following test suite. For each case, you must:\n\n- Construct the smoothing objective for the given parameters.\n- Use two multi-start initializations at $\\lambda = 1$: one with $x_t^{(0)} = +\\sqrt{y_t}$ for all $t$, and one with $x_t^{(0)} = -\\sqrt{y_t}$ for all $t$. Run Newton's method from both initializations to seek distinct local minima. Identify distinct minima by an $\\ell_2$-distance threshold of $10^{-3}$ and retain only those with positive definite Hessians.\n- Compute the Laplace-approximated log-evidence at each distinct local minimum using your Hessian.\n- Run the homotopy tracking from the smaller $\\lambda$ to $\\lambda = 1$, starting from the positive initialization at the smallest $\\lambda$, and report the sign of the time-average of the final tracked solution as $+1$ if the average is positive and $-1$ if the average is negative.\n- At the final homotopy step, compute the condition number of the Hessian as the ratio of its largest to smallest eigenvalue.\n\nTest suite:\n\n- Case $\\#1$ (symmetric, two-minima regime):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 0.05$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.0$,\n  - $P_0 = 1.0$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - Homotopy schedule $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$, so $r(\\lambda) = r_{\\text{base}}/\\lambda$.\n- Case $\\#2$ (prior breaks symmetry, single-minimum regime likely):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 0.05$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.5$,\n  - $P_0 = 0.01$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - Homotopy schedule $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$.\n- Case $\\#3$ (weak dynamics coupling, multiple nearly-decoupled modes):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 10.0$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.0$,\n  - $P_0 = 1.0$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - Homotopy schedule $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$.\n\nRequired final output for each case is a list of four values:\n- The integer number of distinct local minima found at $\\lambda = 1$ from the two prescribed initializations.\n- The floating-point difference in Laplace-approximated log-evidence between the positive-basin and negative-basin minima at $\\lambda = 1$, computed as $\\log Z_{\\text{Lap}}^{(+)} - \\log Z_{\\text{Lap}}^{(-)}$. If only one minimum is found, report $0.0$.\n- The integer sign of the time-average of the final homotopy-tracked solution at $\\lambda = 1$, reported as $+1$ or $-1$.\n- The floating-point condition number of the Hessian at the final homotopy-tracked solution at $\\lambda = 1$.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of the per-case lists, with no spaces, enclosed in square brackets. For example: $[\\,[1,0.0,1,12.345678],[\\dots],[\\dots]\\,]$. No physical units are involved in this problem, and all numbers should be printed as plain decimals.",
            "solution": "This problem requires the derivation and implementation of a fixed-interval smoothing algorithm for a nonlinear state-space model. The solution involves finding Maximum A Posteriori (MAP) estimates of the state trajectory by minimizing a cost function, which corresponds to the negative log-posterior probability density. The specific model involves linear dynamics and quadratic observations, leading to a non-convex optimization problem with potentially multiple local minima.\n\n### Task 1: Derivation of the Smoothing Objective $J(x_{0:T})$\n\nThe objective is to find the state trajectory $x_{0:T} = (x_0, x_1, \\dots, x_T)$ that maximizes the posterior probability density $p(x_{0:T} \\mid y_{0:T})$. By Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto p(y_{0:T} \\mid x_{0:T}) p(x_{0:T})\n$$\nMaximizing the posterior is equivalent to maximizing its logarithm, or minimizing its negative logarithm. We define the objective function $J(x_{0:T})$ as the negative log-posterior, up to an additive constant that does not depend on the state sequence $x_{0:T}$.\n$$\nJ(x_{0:T}) = -\\log p(x_{0:T} \\mid y_{0:T}) + C = -\\log p(y_{0:T} \\mid x_{0:T}) - \\log p(x_{0:T}) + C'\n$$\nThe state-space model's structure implies conditional independencies, which allow us to factor the likelihood and prior terms.\nThe prior on the state trajectory $p(x_{0:T})$ is given by the Markov property:\n$$\np(x_{0:T}) = p(x_0) \\prod_{t=0}^{T-1} p(x_{t+1} \\mid x_t)\n$$\nGiven the states $x_{0:T}$, the observations $y_{0:T}$ are conditionally independent:\n$$\np(y_{0:T} \\mid x_{0:T}) = \\prod_{t=0}^{T} p(y_t \\mid x_t)\n$$\nSubstituting these into the expression for $J(x_{0:T})$ gives:\n$$\nJ(x_{0:T}) = -\\log p(x_0) - \\sum_{t=0}^{T-1} \\log p(x_{t+1} \\mid x_t) - \\sum_{t=0}^{T} \\log p(y_t \\mid x_t) + C'\n$$\nWe now substitute the given Gaussian probability densities:\n-   Prior: $p(x_0) = \\mathcal{N}(x_0; m_0, P_0) \\propto \\exp\\left(-\\frac{1}{2P_0}(x_0 - m_0)^2\\right)$\n-   Dynamics: $p(x_{t+1} \\mid x_t) = \\mathcal{N}(x_{t+1}; f(x_t), q) = \\mathcal{N}(x_{t+1}; ax_t, q) \\propto \\exp\\left(-\\frac{1}{2q}(x_{t+1} - ax_t)^2\\right)$\n-   Observations: $p(y_t \\mid x_t) = \\mathcal{N}(y_t; h(x_t), r) = \\mathcal{N}(y_t; x_t^2, r) \\propto \\exp\\left(-\\frac{1}{2r}(y_t - x_t^2)^2\\right)$\n\nTaking the negative logarithm of each term and summing, while ignoring normalization constants (e.g., factors of $1/\\sqrt{2\\pi\\sigma^2}$), we obtain the smoothing objective:\n$$\nJ(x_{0:T}) = \\frac{1}{2P_0}(x_0 - m_0)^2 + \\sum_{t=0}^{T-1} \\frac{1}{2q}(x_{t+1} - ax_t)^2 + \\sum_{t=0}^{T} \\frac{1}{2r}(y_t - x_t^2)^2\n$$\nThis is a standard least-squares objective, composed of a prior term penalizing deviation from the prior mean, a dynamics term penalizing deviations from the state transition model, and an observation term penalizing mismatch with the measurements.\n\n### Task 2: Gradient $\\nabla J(x_{0:T})$ and Hessian $\\nabla^2 J(x_{0:T})$\n\nTo minimize $J(x_{0:T})$ using a gradient-based method like Newton's method, we need its gradient vector $\\nabla J(x_{0:T})$ and Hessian matrix $\\nabla^2 J(x_{0:T})$. The state vector is $x = (x_0, \\dots, x_T)^T$.\n\nThe gradient is the vector of partial derivatives $\\frac{\\partial J}{\\partial x_k}$ for $k=0, \\dots, T$.\n-   For $k=0$:\n    $$\n    \\frac{\\partial J}{\\partial x_0} = \\frac{1}{P_0}(x_0 - m_0) + \\frac{1}{q}(x_{1} - ax_0)(-a) + \\frac{1}{r}(y_0 - x_0^2)(-2x_0) = \\frac{x_0 - m_0}{P_0} - \\frac{a}{q}(x_1 - ax_0) - \\frac{2x_0}{r}(y_0 - x_0^2)\n    $$\n-   For $k \\in \\{1, \\dots, T-1\\}$:\n    $$\n    \\frac{\\partial J}{\\partial x_k} = \\frac{1}{q}(x_k - ax_{k-1}) + \\frac{1}{q}(x_{k+1} - ax_k)(-a) + \\frac{1}{r}(y_k - x_k^2)(-2x_k) = \\frac{1}{q}(x_k - ax_{k-1}) - \\frac{a}{q}(x_{k+1} - ax_k) - \\frac{2x_k}{r}(y_k - x_k^2)\n    $$\n-   For $k=T$:\n    $$\n    \\frac{\\partial J}{\\partial x_T} = \\frac{1}{q}(x_T - ax_{T-1}) + \\frac{1}{r}(y_T - x_T^2)(-2x_T) = \\frac{1}{q}(x_T - ax_{T-1}) - \\frac{2x_T}{r}(y_T - x_T^2)\n    $$\n\nThe Hessian is the symmetric matrix of second partial derivatives $H_{ij} = \\frac{\\partial^2 J}{\\partial x_i \\partial x_j}$. Due to the Markovian structure of the dynamics, this matrix is tridiagonal.\nThe diagonal elements $H_{k,k}$ are:\n-   For $k=0$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_0^2} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{\\partial}{\\partial x_0}\\left(-\\frac{2x_0y_0}{r} + \\frac{2x_0^3}{r}\\right) = \\frac{1}{P_0} + \\frac{a^2}{q} - \\frac{2y_0}{r} + \\frac{6x_0^2}{r} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{2}{r}(3x_0^2 - y_0)\n    $$\n-   For $k \\in \\{1, \\dots, T-1\\}$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_k^2} = \\frac{1}{q} + \\frac{a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k) = \\frac{1+a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k)\n    $$\n-   For $k=T$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_T^2} = \\frac{1}{q} + \\frac{2}{r}(3x_T^2 - y_T)\n    $$\nThe off-diagonal elements $H_{k, k+1} = H_{k+1, k}$ for $k=0, \\dots, T-1$ are:\n$$\n\\frac{\\partial^2 J}{\\partial x_k \\partial x_{k+1}} = \\frac{\\partial}{\\partial x_{k+1}}\\left( \\dots - \\frac{a}{q}(x_{k+1} - ax_k) \\dots \\right) = -\\frac{a}{q}\n$$\nAll other off-diagonal elements $H_{i,j}$ where $|i-j|>1$ are zero.\n\n### Task 3: Laplace Approximation\n\nThe Laplace approximation provides a Gaussian approximation to the posterior distribution around a MAP estimate $x_{0:T}^\\star$, which is a local minimum of $J(x_{0:T})$. The objective function is expanded to second order around $x_{0:T}^\\star$:\n$$\nJ(x_{0:T}) \\approx J(x_{0:T}^\\star) + \\nabla J(x_{0:T}^\\star)^T (x_{0:T} - x_{0:T}^\\star) + \\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T \\nabla^2 J(x_{0:T}^\\star) (x_{0:T} - x_{0:T}^\\star)\n$$\nSince $x_{0:T}^\\star$ is a minimum, $\\nabla J(x_{0:T}^\\star) = 0$. The posterior is then approximated as:\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto \\exp(-J(x_{0:T})) \\approx \\exp(-J(x_{0:T}^\\star)) \\exp\\left(-\\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T H^\\star (x_{0:T} - x_{0:T}^\\star)\\right)\n$$\nwhere $H^\\star = \\nabla^2 J(x_{0:T}^\\star)$. This shows the posterior is approximately Gaussian: $\\mathcal{N}(x_{0:T}^\\star, (H^\\star)^{-1})$. The model evidence (or marginal likelihood) $p(y_{0:T}) = \\int p(y_{0:T}, x_{0:T}) dx_{0:T}$ can be approximated by integrating the unnormalized posterior density. As given, the local contribution to the log evidence is:\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{T+1}{2} \\log(2\\pi)\n$$\nThis formula combines the height of the posterior peak (via $-J(x_{0:T}^\\star)$) and the volume of the peak (via $-\\frac{1}{2}\\log\\det H^\\star$, which measures its 'width'). When comparing the evidence of different local minima, terms not depending on $x_{0:T}^\\star$ cancel out, making the choice of which constants are included in $J$ unimportant for relative comparison.\n\n### Task 4: Homotopy in Observation Noise Variance\n\nA homotopy, or continuation method, is proposed to solve the optimization problem. The observation noise variance $r$ is parameterized by $\\lambda \\in (0, 1]$ as $r(\\lambda) = r_{\\text{base}} / \\lambda$.\n-   When $\\lambda$ is small (close to $0$), $r(\\lambda)$ becomes very large. A large observation variance $r$ implies that the observations $y_t$ are uninformative. The term $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ in the objective function becomes small, so the posterior is dominated by the prior and dynamics, which define a convex problem. This \"weak-observation\" problem is easier to solve and typically has a single minimum close to the prior mean.\n-   As $\\lambda$ increases towards $1$, $r(\\lambda)$ decreases, approaching the target value $r_{\\text{base}}$. The influence of the non-convex observation term $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ increases, and the objective function deforms from the simple, convex-like shape to the complex, possibly multi-modal target shape.\n\nThe strategy is to start from the solution of the easy problem at a small $\\lambda$ and use it as the initial guess for a slightly larger $\\lambda$. By gradually stepping $\\lambda$ from a small value to $1$, we can track a local minimum through the deformation, which is generally more robust than starting the optimization directly at $\\lambda=1$.\n\n### Task 5: Numerical Implementation\n\nThe core of the implementation is a damped Newton's method to find local minima of $J(x_{0:T})$.\n-   **Newton Step**: At each iteration, we solve the linear system $H_k p_k = -g_k$ for the Newton step $p_k$, where $g_k$ and $H_k$ are the gradient and Hessian at the current estimate $x_k$.\n-   **Damping**: The Hessian may not be positive definite away from a minimum. A Levenberg-Marquardt style modification is used: we solve $(H_k + \\mu I) p_k = -g_k$, where $\\mu \\ge 0$ is a damping parameter. $\\mu$ is chosen to be zero if $H_k$ is positive definite, and increased otherwise until $(H_k + \\mu I)$ is positive definite, ensuring a descent direction.\n-   **Line Search**: A backtracking line search is used to determine the step size $\\alpha$. We start with $\\alpha=1$ and reduce it until the Armijo condition $J(x_k + \\alpha p_k) \\le J(x_k) + c \\alpha g_k^T p_k$ is satisfied for a small constant $c$.\n-   **Multi-start and Homotopy**: This Newton solver is used for two purposes:\n    1.  At $\\lambda=1$, it is run from two different initial guesses ($x_t^{(0)} = \\pm\\sqrt{y_t}$) to find potentially distinct local minima.\n    2.  It is used at each step of the homotopy schedule, with the solution from the previous $\\lambda$ value providing the initial guess for the current $\\lambda$.\n\nThe final step for each identified minimum involves checking if the Hessian is positive definite (all eigenvalues are positive) and then computing the required quantities: Laplace log-evidence, sign of the average state, and Hessian condition number.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the smoothing problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case #1 (symmetric, two-minima regime)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #2 (prior breaks symmetry, single-minimum regime likely)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.5, \"P0\": 0.01,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #3 (weak dynamics coupling, multiple nearly-decoupled modes)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 10.0, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p = case\n        dim = p[\"T\"] + 1\n\n        def J_objective(x, r, p):\n            term_prior = 0.5 * (x[0] - p[\"m0\"])**2 / p[\"P0\"]\n            term_dyn = 0.5 * np.sum((x[1:] - p[\"a\"] * x[:-1])**2) / p[\"q\"]\n            term_obs = 0.5 * np.sum((p[\"y\"] - x**2)**2) / r\n            return term_prior + term_dyn + term_obs\n\n        def grad_J(x, r, p):\n            grad = np.zeros_like(x)\n            # Observation part\n            grad += -2 * x * (p[\"y\"] - x**2) / r\n            \n            # Prior part\n            grad[0] += (x[0] - p[\"m0\"]) / p[\"P0\"]\n            \n            # Dynamics part\n            dyn_res = x[1:] - p[\"a\"] * x[:-1]\n            grad[1:] += dyn_res / p[\"q\"]\n            grad[:-1] -= p[\"a\"] * dyn_res / p[\"q\"]\n            \n            return grad\n\n        def hess_J(x, r, p):\n            hess = np.zeros((dim, dim))\n            \n            # Diagonal\n            diag = np.zeros(dim)\n            diag += 2 * (3 * x**2 - p[\"y\"]) / r\n            diag[0] += 1 / p[\"P0\"] + p[\"a\"]**2 / p[\"q\"]\n            diag[1:p[\"T\"]] += (1 + p[\"a\"]**2) / p[\"q\"]\n            diag[p[\"T\"]] += 1 / p[\"q\"]\n            np.fill_diagonal(hess, diag)\n            \n            # Off-diagonal\n            off_diag_val = -p[\"a\"] / p[\"q\"]\n            np.fill_diagonal(hess[1:], off_diag_val)\n            np.fill_diagonal(hess[:, 1:], off_diag_val)\n            \n            return hess\n\n        def newton_solver(x_init, lam, p, max_iter=100, tol=1e-8):\n            x = x_init.copy()\n            r = p[\"r_base\"] / lam\n            \n            for _ in range(max_iter):\n                g = grad_J(x, r, p)\n                if np.linalg.norm(g) < tol:\n                    break\n                \n                H = hess_J(x, r, p)\n                \n                # Levenberg-Marquardt Damping\n                mu = 0\n                while True:\n                    try:\n                        H_lm = H + mu * np.eye(dim)\n                        L = scipy.linalg.cholesky(H_lm, lower=True)\n                        break\n                    except np.linalg.LinAlgError:\n                        if mu == 0:\n                            mu = 1e-6\n                        else:\n                            mu *= 10\n                \n                # Solve linear system H_lm * step = -g\n                step = scipy.linalg.cho_solve((L, True), -g)\n\n                if np.linalg.norm(step) < tol:\n                    break\n\n                # Backtracking Line Search\n                alpha = 1.0\n                c = 1e-4\n                J_current = J_objective(x, r, p)\n                while alpha > 1e-8:\n                    x_new = x + alpha * step\n                    if J_objective(x_new, r, p) < J_current + c * alpha * np.dot(g, step):\n                        break\n                    alpha *= 0.5\n                \n                x = x + alpha * step\n\n            return x\n\n        # 1. Multi-start Newton's at lambda=1\n        x_init_pos = np.sqrt(p[\"y\"])\n        x_init_neg = -np.sqrt(p[\"y\"])\n        \n        x_sol_pos = newton_solver(x_init_pos, 1.0, p)\n        x_sol_neg = newton_solver(x_init_neg, 1.0, p)\n\n        minima = []\n        r_final = p[\"r_base\"]\n        \n        # Check first minimum\n        H_pos = hess_J(x_sol_pos, r_final, p)\n        try:\n            eigvals_pos = np.linalg.eigvalsh(H_pos)\n            if np.all(eigvals_pos > 0):\n                minima.append({'x': x_sol_pos, 'H': H_pos, 'origin': 'pos', 'eigvals':eigvals_pos})\n        except np.linalg.LinAlgError:\n            pass\n\n        # Check second minimum\n        is_new = True\n        for m in minima:\n            if np.linalg.norm(m['x'] - x_sol_neg) < 1e-3:\n                is_new = False\n                break\n        \n        if is_new:\n            H_neg = hess_J(x_sol_neg, r_final, p)\n            try:\n                eigvals_neg = np.linalg.eigvalsh(H_neg)\n                if np.all(eigvals_neg > 0):\n                    minima.append({'x': x_sol_neg, 'H': H_neg, 'origin': 'neg', 'eigvals':eigvals_neg})\n            except np.linalg.LinAlgError:\n                pass\n        \n        num_minima = len(minima)\n\n        # 2. Laplace log-evidence difference\n        log_evidence_diff = 0.0\n        if num_minima == 2:\n            m_pos = next(m for m in minima if m['origin'] == 'pos')\n            m_neg = next(m for m in minima if m['origin'] == 'neg')\n\n            J_pos = J_objective(m_pos['x'], r_final, p)\n            logdet_pos = np.sum(np.log(m_pos['eigvals']))\n            \n            J_neg = J_objective(m_neg['x'], r_final, p)\n            logdet_neg = np.sum(np.log(m_neg['eigvals']))\n            \n            log_z_pos = -J_pos - 0.5 * logdet_pos\n            log_z_neg = -J_neg - 0.5 * logdet_neg\n            \n            log_evidence_diff = log_z_pos - log_z_neg\n\n        # 3. Homotopy tracking\n        x_homotopy = np.sqrt(p[\"y\"])\n        for lam in p[\"lambda_schedule\"]:\n            x_homotopy = newton_solver(x_homotopy, lam, p)\n        \n        sign_avg_homotopy = int(np.sign(np.mean(x_homotopy)))\n\n        # 4. Condition number\n        H_homotopy = hess_J(x_homotopy, r_final, p)\n        cond_num = np.inf\n        try:\n            eigvals_homotopy = np.linalg.eigvalsh(H_homotopy)\n            if np.all(eigvals_homotopy > 0):\n                cond_num = eigvals_homotopy[-1] / eigvals_homotopy[0]\n        except np.linalg.LinAlgError:\n            pass\n        \n        all_results.append(f\"[{num_minima},{log_evidence_diff:.8f},{sign_avg_homotopy},{cond_num:.8f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}