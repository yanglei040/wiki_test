{
    "hands_on_practices": [
        {
            "introduction": "在尝试估计一个系统的状态之前，我们必须回答一个基本问题：现有信息是否足以完全确定该状态？本练习提供了一种动手实践的方法，通过分析后验精度矩阵来理解可观测性。你将为不同的平滑先验和观测模式构建此矩阵，学习将其零空间识别为不可观测模式，并探索如何通过最少的额外数据使问题良定 。",
            "id": "3406074",
            "problem": "本题要求您通过构造具有大零空间的示例，来形式化并计算线性高斯平滑目标中的不可观测性，然后提出并验证能够正则化后验的最小附加观测。请完全在有限维线性代数框架下进行操作，将整个时空轨迹视为单个堆叠向量，并将所有惩罚项视为二次型。\n\n考虑一个离散时间状态序列 $\\{x_t\\}_{t=0}^T$，其中 $x_t \\in \\mathbb{R}^n$。将轨迹堆叠为 $z \\in \\mathbb{R}^{n(T+1)}$，其中 $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top$。一个二次平滑目标可以写作\n$$\nJ(z) \\;=\\; \\tfrac{1}{2}\\,\\|L z\\|_2^2 \\;+\\; \\tfrac{1}{2}\\,\\|C z - y\\|_2^2,\n$$\n其中 $L$ 编码平滑或动态惩罚，$C$ 编码可用的线性观测。在线性高斯贝叶斯解释下，最大后验（MAP）估计量与 $J(z)$ 的最小化器重合，其 Hessian 矩阵（后验精度）为\n$$\n\\mathcal{I} \\;=\\; L^\\top L \\;+\\; C^\\top C.\n$$\n当 $\\mathcal{I}$ 是奇异矩阵时，存在不可观测性；其零空间 $\\mathcal{N}(\\mathcal{I})$ 标识了既不受动态/平滑惩罚，也不受观测惩罚的方向。$\\mathcal{N}(\\mathcal{I})$ 的维度是 $\\mathcal{I}$ 的零度，等价于 $n(T+1) - \\mathrm{rank}(\\mathcal{I})$。\n\n您将研究两种产生大零空间的典型平滑算子：\n\n- 一阶差分（随机游走）平滑：对于 $t \\in \\{0,\\dots,T-1\\}$，惩罚 $x_{t+1} - x_t$。这对应于 $L \\in \\mathbb{R}^{nT \\times n(T+1)}$，其块结构使得第 $t$ 个行块等于 $[0,\\dots,0,-I_n, I_n, 0,\\dots,0]$，作用于 $[x_t^\\top, x_{t+1}^\\top]^\\top$。\n- 二阶差分（加速度）平滑：对于 $t \\in \\{1,\\dots,T-1\\}$，惩罚 $x_{t+1} - 2 x_t + x_{t-1}$。这对应于 $L \\in \\mathbb{R}^{n(T-1) \\times n(T+1)}$，其块结构使得第 $t$ 个行块等于 $[0,\\dots,0, I_n, -2 I_n, I_n, 0,\\dots,0]$，作用于 $[x_{t-1}^\\top, x_t^\\top, x_{t+1}^\\top]^\\top$。\n\n为简化起见，假设所有惩罚都是各向同性的，且权重为单位矩阵，因此上述 2-范数均为欧几里得范数。观测是逐点的分量测量：如果您在时间 $t$ 观测到 $x_t$ 的分量索引 $S \\subset \\{0,\\dots,n-1\\}$，那么 $C$ 对每个 $(t,i) \\in \\{t\\} \\times S$ 包含一行 $e_{t,i}^\\top$，其中 $e_{t,i} \\in \\mathbb{R}^{n(T+1)}$ 是选择时间 $t$ 处第 $i$ 个分量的标准基向量（即，它在对应于 $(t,i)$ 的位置为 $1$，其他位置为 $0$）。设测量噪声精度等于单位矩阵，因此 $C^\\top C$ 直接贡献于 $\\mathcal{I}$。\n\n对于每个指定的测试用例，您的程序必须：\n- 为给定的 $n$ 和 $T$ 构造一阶差分或二阶差分算子 $L$。\n- 根据在指定时间观测到的分量集合构造 $C$。\n- 计算 $\\mathcal{I} = L^\\top L + C^\\top C$ 的零度 $k_0$。\n- 提出一个最小的附加线性观测集来正则化后验。您必须通过计算 $\\mathcal{N}(\\mathcal{I})$ 的一组基 $\\{u_j\\}_{j=1}^{k_0}$ 并将附加观测算子 $C_{\\mathrm{add}}$ 的行向量取为 $u_j^\\top$ 来实现此操作。此选择使用了 $k_0$ 个标量观测，并保证更新后的精度\n$$\n\\mathcal{I}_{\\mathrm{new}} \\;=\\; \\mathcal{I} \\;+\\; C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n$$\n在 $k_0 > 0$ 时是正定的，在 $k_0 = 0$ 时保持不变。\n- 将 $k_{\\min} = k_0$ 设为理论上所需的最小附加标量观测数，并通过检查 $\\mathcal{I}_{\\mathrm{new}}$ 的零度是否为 $0$ 来验证成功。\n\n数值线性代数实现提示（非解题捷径）：使用 $\\mathcal{I}$ 的对称特征分解来获取其特征值和特征向量，并使用一个数值上合理的阈值来定义“零”特征值。\n\n测试套件。您的程序必须解决以下情况：\n\n- 情况 A：一阶差分，$n = 3$，$T = 5$，无观测。\n- 情况 B：一阶差分，$n = 3$，$T = 5$，在时间 $t = 0$ 观测分量 $\\{0,1\\}$。\n- 情况 C：二阶差分，$n = 2$，$T = 6$，无观测。\n- 情况 D：二阶差分，$n = 2$，$T = 6$，在时间 $t = 0$ 观测分量 $\\{0,1\\}$。\n- 情况 E：二阶差分，$n = 2$，$T = 3$，在时间 $t = 0$ 和 $t = 3$ 观测分量 $\\{0\\}$（即，在两个端点都观测分量 $0$）。\n- 情况 F：二阶差分，$n = 1$，$T = 4$，在时间 $t = 0$ 和 $t = 4$ 观测分量 $\\{0\\}$。\n\n所需输出。对于每种情况，输出一个列表 $[k_0, k_{\\min}, s]$，其中 $k_0$ 是计算出的 $\\mathcal{I}$ 的零度，$k_{\\min}$ 是您提出的最小附加标量观测数（根据构造等于 $k_0$），$s$ 是一个布尔值，指示更新后的精度 $\\mathcal{I}_{\\mathrm{new}}$ 是否在数值上是满秩的（零度等于 $0$）。您的程序应生成单行输出，其中包含情况 A–F 的结果，形式为由这些列表组成的逗号分隔列表，并用方括号括起来；例如，一个有效的格式如 $[[r_1],[r_2],\\dots]$，其中每个 $[r_j]$ 是按指定顺序 A–F 的情况 $j$ 的三元组。",
            "solution": "该问题要求通过构造和正则化具有奇异后验精度矩阵的系统，来分析线性平滑问题中的不可观测性。状态 $\\{x_t\\}_{t=0}^T$（其中 $x_t \\in \\mathbb{R}^n$）的整个时空轨迹表示为单个堆叠向量 $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top \\in \\mathbb{R}^{n(T+1)}$。\n\n平滑问题被构建为最小化一个二次目标函数 $J(z)$：\n$$\nJ(z) = \\tfrac{1}{2} \\|L z\\|_2^2 + \\tfrac{1}{2} \\|C z - y\\|_2^2\n$$\n第一项对应于由矩阵 $L$ 编码的先验或平滑惩罚。第二项代表与一组由矩阵 $C$ 编码的线性观测 $y$ 不匹配的惩罚。在贝叶斯背景下，这对应于为一个线性高斯模型寻找最大后验（MAP）估计。后验精度矩阵，即 $J(z)$ 的 Hessian 矩阵，由下式给出：\n$$\n\\mathcal{I} = L^\\top L + C^\\top C\n$$\n当 $\\mathcal{I}$ 是奇异的，即它有一个非平凡的零空间 $\\mathcal{N}(\\mathcal{I})$ 时，就会出现不可观测性。这个零空间的维度，即其零度，量化了状态空间中既不受动态（平滑）也不受观测约束的独立方向的数量。任何向量 $v \\in \\mathcal{N}(\\mathcal{I})$ 都可以加到一个解 $z^*$ 上而不改变成本 $J$，因为对于任何 $v \\in \\mathcal{N}(\\mathcal{I})$，$v$ 必须同时在 $\\mathcal{N}(L)$ 和 $\\mathcal{N}(C)$ 中（假设在零空间分析中 $y=0$），因此 $L(\\alpha v)=0$ 和 $C(\\alpha v)=0$。\n\n解决每个测试用例的步骤如下：\n\n1.  **构造先验精度矩阵 $L^\\top L$**：\n    问题的总维度是 $d = n(T+1)$。矩阵 $L^\\top L$ 是一个 $d \\times d$ 矩阵。\n    -   对于**一阶差分**平滑器，惩罚项是 $x_{t+1} - x_t$，$t=0, \\dots, T-1$。这意味着 $L$ 的零空间由所有 $t$ 的 $x_t$ 都为常数的轨迹组成。具体来说，如果 $z \\in \\mathcal{N}(L)$，那么 $x_0 = x_1 = \\dots = x_T$。这是一个 $n$ 维零空间，由形如 $[v^\\top, v^\\top, \\dots, v^\\top]^\\top$（对于任何 $v \\in \\mathbb{R}^n$）的轨迹张成。矩阵 $L$ 可以构造成有 $nT$ 行，其中对应于时间 $t$ 的每 $n$ 行块在列块 $t$ 处有一个 $-I_n$ 块，在列块 $t+1$ 处有一个 $I_n$ 块。\n    -   对于**二阶差分**平滑器，惩罚项是 $x_{t+1} - 2x_t + x_{t-1}$，$t=1, \\dots, T-1$。$L$ 的零空间由随时间线性演化的轨迹组成，即 $x_t = x_0 + t(x_1 - x_0)$。这样的轨迹完全由初始两个状态 $x_0$ 和 $x_1$ 决定。这构成了一个 $2n$ 维的零空间。矩阵 $L$ 构造成有 $n(T-1)$ 行，其中对应于时间 $t$ 的每 $n$ 行块分别在列块 $t-1, t, t+1$ 处有 $I_n, -2I_n, I_n$ 块。\n    在两种情况下，我们都计算 $L$，然后构成先验精度 $L^\\top L$。\n\n2.  **构造观测精度矩阵 $C^\\top C$**：\n    观测是特定分量的逐点测量。$x_t$ 的第 $i$ 个分量的观测对应于 $C$ 中的一行，由 $e_{t,i}^\\top$ 给出，其中 $e_{t,i}$ 是 $\\mathbb{R}^{n(T+1)}$ 中的标准基向量，在对应于 $x_t$ 的第 $i$ 个分量的位置上为 1，其他位置为零。在测量噪声精度为单位矩阵的情况下，观测对后验精度的总贡献是 $C^\\top C = \\sum_{(t,i) \\in \\text{obs}} e_{t,i} e_{t,i}^\\top$。这是一个对角矩阵，其对角线上与被观测分量对应的索引位置为 1。我们将此矩阵加到 $L^\\top L$ 上以构成完整的后验精度 $\\mathcal{I}$。状态 $x_t$ 的分量 $i$ 的全局索引是 $t \\cdot n + i$。\n\n3.  **计算初始零度 ($k_0$)**：\n    后验精度矩阵 $\\mathcal{I}$ 根据构造是实对称的。我们使用对称特征求解器（`numpy.linalg.eigh`）计算其特征值和特征向量。零度 $k_0$ 是数值上接近于零（由一个小的容差，如 $10^{-9}$ 判断）的特征值的数量。对应的特征向量 $\\{u_j\\}_{j=1}^{k_0}$ 构成了零空间 $\\mathcal{N}(\\mathcal{I})$ 的一组标准正交基。\n\n4.  **提出并应用正则化**：\n    为了使问题良定，我们必须添加新的观测来惩罚未被观测的模式。问题指定了一种最小正则化策略。我们引入 $k_{\\min} = k_0$ 个新的标量观测。新的观测算子 $C_{\\mathrm{add}}$ 的定义是将其行向量取为零空间的基向量，即 $C_{\\mathrm{add}} = [u_1, u_2, \\dots, u_{k_0}]^\\top$。对精度矩阵的贡献是 $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}$。由于基向量 $\\{u_j\\}$ 是标准正交的，$C_{\\mathrm{add}}^\\top C_{\\mathrm{add}} = \\sum_{j=1}^{k_0} u_j u_j^\\top$，这是到 $\\mathcal{N}(\\mathcal{I})$ 上的投影矩阵。\n    新的、正则化后的精度矩阵是：\n    $$\n    \\mathcal{I}_{\\mathrm{new}} = \\mathcal{I} + C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n    $$\n\n5.  **验证正则化**：\n    我们通过计算 $\\mathcal{I}_{\\mathrm{new}}$ 的零度来验证正则化是否成功。对于原始零空间中的任何向量 $v = \\sum_k \\alpha_k u_k$，有 $\\mathcal{I}v=0$。新矩阵的作用如下：\n    $$\n    \\mathcal{I}_{\\mathrm{new}} v = (\\mathcal{I} + \\sum_{j=1}^{k_0} u_j u_j^\\top) (\\sum_{k=1}^{k_0} \\alpha_k u_k) = 0 + \\sum_{j,k} \\alpha_k u_j (u_j^\\top u_k) = \\sum_{j,k} \\alpha_k u_j \\delta_{jk} = \\sum_k \\alpha_k u_k = v\n    $$\n    这表明，张成原始零空间的向量现在是 $\\mathcal{I}_{\\mathrm{new}}$ 的特征值为 $1$ 的特征向量。对于 $\\mathcal{I}$ 的任何具有非零特征值 $\\lambda$ 的特征向量 $w$（因此 $w \\perp \\mathcal{N}(\\mathcal{I})$），我们有 $\\mathcal{I}_{\\mathrm{new}} w = \\mathcal{I}w + (\\sum u_j u_j^\\top)w = \\lambda w + 0 = \\lambda w$。其他特征值和特征向量保持不变。因此，$\\mathcal{I}_{\\mathrm{new}}$ 没有零特征值并且是正定的。我们通过计算 $\\mathcal{I}_{\\mathrm{new}}$ 的特征值并检查其零度是否为 $0$ 来进行数值确认。成功与否记录为一个布尔值 $s$。\n\n这个完整的流程将为每个指定的测试用例实现。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of test cases for analyzing unobservability\n    in linear-Gaussian smoothing problems.\n    \"\"\"\n    \n    # Test suite as specified in the problem statement\n    test_cases = [\n        # Case A: first-difference, n=3, T=5, no observations\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': []},\n        # Case B: first-difference, n=3, T=5, obs at t=0 of components {0,1}\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': [(0, [0, 1])]},\n        # Case C: second-difference, n=2, T=6, no observations\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': []},\n        # Case D: second-difference, n=2, T=6, obs at t=0 of components {0,1}\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': [(0, [0, 1])]},\n        # Case E: second-difference, n=2, T=3, obs at t=0, t=3 of component {0}\n        {'type': 'second_difference', 'n': 2, 'T': 3, 'obs': [(0, [0]), (3, [0])]},\n        # Case F: second-difference, n=1, T=4, obs at t=0, t=4 of component {0}\n        {'type': 'second_difference', 'n': 1, 'T': 4, 'obs': [(0, [0]), (4, [0])]},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(case['type'], case['n'], case['T'], case['obs'])\n        results.append(result)\n\n    # Format the final output string exactly as required\n    # e.g., [[k0_A,kmin_A,s_A],[k0_B,kmin_B,s_B],...]\n    result_strings = [f\"[{r[0]},{r[1]},{str(r[2]).lower()}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef _solve_case(smoother_type, n, T, observations):\n    \"\"\"\n    Solves a single instance of the observability problem.\n    \"\"\"\n    # Total dimension of the stacked state vector z\n    dim = n * (T + 1)\n    \n    # Numerical tolerance for identifying zero eigenvalues\n    TOL = 1e-9\n\n    # 1. Construct the smoothing operator L\n    if smoother_type == 'first_difference':\n        # L has n*T rows and n*(T+1) columns\n        L = np.zeros((n * T, dim))\n        identity_n = np.eye(n)\n        for t in range(T):\n            row_slice = slice(t * n, (t + 1) * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_t1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_t] = -identity_n\n            L[row_slice, col_slice_t1] = identity_n\n    \n    elif smoother_type == 'second_difference':\n        # L has n*(T-1) rows and n*(T+1) columns\n        L = np.zeros((n * (T - 1), dim))\n        identity_n = np.eye(n)\n        for t_idx, t in enumerate(range(1, T)):\n            row_slice = slice(t_idx * n, (t_idx + 1) * n)\n            col_slice_tm1 = slice((t - 1) * n, t * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_tp1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_tm1] = identity_n\n            L[row_slice, col_slice_t] = -2 * identity_n\n            L[row_slice, col_slice_tp1] = identity_n\n    else:\n        raise ValueError(\"Unknown smoother type\")\n\n    # 2. Construct the posterior precision matrix I\n    # Start with the prior precision L^T L\n    I = L.T @ L\n    \n    # Add observation precision C^T C\n    for t, components in observations:\n        for i in components:\n            idx = t * n + i\n            I[idx, idx] += 1.0\n\n    # 3. Compute initial nullity k0 and find nullspace basis\n    eigenvalues, eigenvectors = np.linalg.eigh(I)\n    is_zero_eigenvalue = np.abs(eigenvalues)  TOL\n    k0 = int(np.sum(is_zero_eigenvalue))\n\n    # 4. Propose and apply regularization\n    k_min = k0\n    \n    if k0 == 0:\n        I_new = I\n    else:\n        nullspace_basis = eigenvectors[:, is_zero_eigenvalue]\n        # C_add.T @ C_add = sum(u_j @ u_j.T)\n        C_add_T_C_add = nullspace_basis @ nullspace_basis.T\n        I_new = I + C_add_T_C_add\n        \n    # 5. Verify successful regularization\n    new_eigenvalues, _ = np.linalg.eigh(I_new)\n    new_nullity = np.sum(np.abs(new_eigenvalues)  TOL)\n    s = (new_nullity == 0)\n    \n    return [k0, k_min, s]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "现实世界中的估计问题通常是非线性的，这会导致非凸的优化目标函数，其包含的多个局部最小值可能会困住标准优化器。这项高级练习介绍了一种强大的策略——同伦（homotopy）或延拓（continuation）方法，用以应对此类复杂情况。你将实现该技术，将一个简单凸问题的解逐渐追踪到最终的复杂非线性问题，从而增加找到全局最优解或高质量局部最优解的机会 。",
            "id": "3406013",
            "problem": "考虑一个用于在时间 $t = 0, 1, \\dots, T$ 上进行固定区间平滑的离散时间、标量、非线性状态空间模型。隐状态序列为 $x_{0:T} \\equiv (x_0, x_1, \\dots, x_T) \\in \\mathbb{R}^{T+1}$。该模型由以下要素指定：\n\n- 初始状态的高斯先验：$x_0 \\sim \\mathcal{N}(m_0, P_0)$，其中先验均值为 $m_0 \\in \\mathbb{R}$，先验方差为 $P_0 \\in \\mathbb{R}_{0}$。\n- 马尔可夫动态模型：$x_{t+1} = f(x_t) + w_t$，其中对于给定的 $a \\in \\mathbb{R}$，$f(x) = a x$，$w_t \\sim \\mathcal{N}(0, q)$，过程噪声方差为 $q \\in \\mathbb{R}_{0}$，且在时间 $t$ 上独立。\n- 非线性观测模型：$y_t = h(x_t) + v_t$，其中 $h(x) = x^2$，$v_t \\sim \\mathcal{N}(0, r)$，观测噪声方差为 $r \\in \\mathbb{R}_{0}$，在时间 $t$ 上独立，并且与过程噪声独立。\n\n根据上述结构，所有分布均被理解为条件独立的。令 $y_{0:T} \\equiv (y_0, y_1, \\dots, y_T)$ 表示观测数据。\n\n您的任务是：\n\n1. 从贝叶斯定理和上述高斯假设下的联合概率密度定义出发，推导固定区间平滑目标 $J(x_{0:T})$，该目标是后验密度 $p(x_{0:T} \\mid y_{0:T})$ 的负对数（不含一个不依赖于 $x_{0:T}$ 的加性常数）。用模型参数以及状态序列与模型/观测值之间的残差来表示 $J(x_{0:T})$。不要引用未经证明的简化公式；从联合密度的因式分解开始，并通过高斯密度的性质进行推导。\n2. 对于 $f(x) = a x$ 和 $h(x) = x^2$ 的特定选择，推导关于 $x_{0:T}$ 的精确梯度 $\\nabla J(x_{0:T})$ 和精确 Hessian 矩阵 $\\nabla^2 J(x_{0:T})$。您的推导必须显式地包含 $a$、$q$、$r$、$m_0$、$P_0$ 和 $y_{0:T}$。\n3. 一个局部极小值点 $x_{0:T}^\\star$ 对应于最大后验 (MAP) 估计的一个候选。拉普拉斯近似通过对 $J(x_{0:T})$ 进行二阶泰勒展开，在局部极小值点附近构造后验的高斯近似。使用您推导的 Hessian 矩阵，写出高斯近似以及对模型证据的局部贡献的相应近似，其对数由下式给出\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{(T+1)}{2} \\log(2\\pi),\n$$\n注意，最后一项来自于对一个 $(T+1)$ 维高斯密度的归一化。\n4. 通过引入标量参数 $\\lambda \\in (0, 1]$ 并定义 $r(\\lambda) = r_{\\text{base}} / \\lambda$（其中 $r_{\\text{base}} \\in \\mathbb{R}_{0}$ 是固定的），提出一个关于观测噪声方差 $r$ 的同伦。解释为什么减小 $\\lambda$ 会增大 $r(\\lambda)$（减弱观测影响），而增大 $\\lambda$ 会减小 $r(\\lambda)$（加强观测影响）。\n5. 在 $\\lambda$ 中实施一个连续策略，以追踪从弱观测机制（较小的 $\\lambda$）到目标机制 $\\lambda=1$ 的一个局部极小值点族。在指定离散时间表上的每个 $\\lambda$ 处，使用带有回溯线搜索和 Levenberg–Marquardt 式阻尼的牛顿法来寻找一个局部极小值点，并从前一个 $\\lambda$ 的解进行初始化。通过验证 Hessian 矩阵是正定的，确保在最终的 $\\lambda = 1$ 时，您的候选解是一个局部最小值。\n\n对于数值研究和自动化测试，请使用以下测试套件。对于每种情况，您必须：\n\n- 为给定参数构建平滑目标。\n- 在 $\\lambda=1$ 时使用两种多起点初始化：一种是对于所有 $t$，$x_t^{(0)} = +\\sqrt{y_t}$；另一种是对于所有 $t$，$x_t^{(0)} = -\\sqrt{y_t}$。从这两种初始化开始运行牛顿法，以寻找不同的局部最小值。通过 $10^{-3}$ 的 $\\ell_2$ 距离阈值识别不同的最小值，并仅保留那些 Hessian 矩阵为正定的最小值。\n- 在每个不同的局部最小值处，使用您的 Hessian 矩阵计算拉普拉斯近似的对数证据。\n- 从较小的 $\\lambda$ 到 $\\lambda = 1$ 运行同伦追踪，从最小 $\\lambda$ 处的正初始化开始，并报告最终追踪解的时间平均值的符号：如果平均值为正，则报告 $+1$；如果平均值为负，则报告 $-1$。\n- 在最后的同伦步骤中，计算 Hessian 矩阵的条件数，即其最大特征值与最小特征值的比率。\n\n测试套件：\n\n- 案例 #1 (对称，双最小值机制):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 0.05$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.0$,\n  - $P_0 = 1.0$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - 同伦时间表 $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$，因此 $r(\\lambda) = r_{\\text{base}}/\\lambda$。\n- 案例 #2 (先验打破对称性，可能是单最小值机制):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 0.05$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.5$,\n  - $P_0 = 0.01$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - 同伦时间表 $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$。\n- 案例 #3 (弱动态耦合，多个近乎解耦的模式):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 10.0$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.0$,\n  - $P_0 = 1.0$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - 同伦时间表 $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$。\n\n每种情况所需的最终输出是一个包含四个值的列表：\n- 从两种指定的初始化在 $\\lambda=1$ 时找到的不同局部最小值的整数数量。\n- 在 $\\lambda=1$ 时，正吸引盆和负吸引盆的最小值之间的拉普拉斯近似对数证据的浮点数差异，计算为 $\\log Z_{\\text{Lap}}^{(+)} - \\log Z_{\\text{Lap}}^{(-)}$。如果只找到一个最小值，则报告 $0.0$。\n- 在 $\\lambda=1$ 时，最终同伦追踪解的时间平均值的整数符号，报告为 $+1$ 或 $-1$。\n- 在 $\\lambda=1$ 时，最终同伦追踪解处的 Hessian 矩阵的浮点数条件数。\n\n您的程序应生成单行输出，其中包含三种情况的结果，格式为每个情况的列表组成的逗号分隔列表，不含空格，并用方括号括起来。例如：$[\\,[1,0.0,1,12.345678],[\\dots],[\\dots]\\,]$。此问题不涉及物理单位，所有数字应以普通小数形式打印。",
            "solution": "此问题要求推导和实现一个用于非线性状态空间模型的固定区间平滑算法。解决方案涉及通过最小化一个成本函数来寻找状态轨迹的最大后验 (MAP) 估计，该成本函数对应于负对数后验概率密度。该特定模型涉及线性动态和二次观测，导致一个可能具有多个局部最小值的非凸优化问题。\n\n### 任务 1: 平滑目标 $J(x_{0:T})$ 的推导\n\n目标是找到状态轨迹 $x_{0:T} = (x_0, x_1, \\dots, x_T)$，以最大化后验概率密度 $p(x_{0:T} \\mid y_{0:T})$。根据贝叶斯定理，后验与似然和先验的乘积成正比：\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto p(y_{0:T} \\mid x_{0:T}) p(x_{0:T})\n$$\n最大化后验等价于最大化其对数，或最小化其负对数。我们将目标函数 $J(x_{0:T})$ 定义为负对数后验，不含一个不依赖于状态序列 $x_{0:T}$ 的加性常数。\n$$\nJ(x_{0:T}) = -\\log p(x_{0:T} \\mid y_{0:T}) + C = -\\log p(y_{0:T} \\mid x_{0:T}) - \\log p(x_{0:T}) + C'\n$$\n状态空间模型的结构意味着条件独立性，这使我们能够分解似然项和先验项。\n状态轨迹的先验 $p(x_{0:T})$ 由马尔可夫性质给出：\n$$\np(x_{0:T}) = p(x_0) \\prod_{t=0}^{T-1} p(x_{t+1} \\mid x_t)\n$$\n给定状态 $x_{0:T}$，观测值 $y_{0:T}$ 是条件独立的：\n$$\np(y_{0:T} \\mid x_{0:T}) = \\prod_{t=0}^{T} p(y_t \\mid x_t)\n$$\n将这些代入 $J(x_{0:T})$ 的表达式中得到：\n$$\nJ(x_{0:T}) = -\\log p(x_0) - \\sum_{t=0}^{T-1} \\log p(x_{t+1} \\mid x_t) - \\sum_{t=0}^{T} \\log p(y_t \\mid x_t) + C'\n$$\n现在我们代入给定的高斯概率密度：\n-   先验: $p(x_0) = \\mathcal{N}(x_0; m_0, P_0) \\propto \\exp\\left(-\\frac{1}{2P_0}(x_0 - m_0)^2\\right)$\n-   动态: $p(x_{t+1} \\mid x_t) = \\mathcal{N}(x_{t+1}; f(x_t), q) = \\mathcal{N}(x_{t+1}; ax_t, q) \\propto \\exp\\left(-\\frac{1}{2q}(x_{t+1} - ax_t)^2\\right)$\n-   观测: $p(y_t \\mid x_t) = \\mathcal{N}(y_t; h(x_t), r) = \\mathcal{N}(y_t; x_t^2, r) \\propto \\exp\\left(-\\frac{1}{2r}(y_t - x_t^2)^2\\right)$\n\n对每一项取负对数并求和，同时忽略归一化常数（例如 $1/\\sqrt{2\\pi\\sigma^2}$ 因子），我们得到平滑目标：\n$$\nJ(x_{0:T}) = \\frac{1}{2P_0}(x_0 - m_0)^2 + \\sum_{t=0}^{T-1} \\frac{1}{2q}(x_{t+1} - ax_t)^2 + \\sum_{t=0}^{T} \\frac{1}{2r}(y_t - x_t^2)^2\n$$\n这是一个标准的最小二乘目标，由惩罚与先验均值偏差的先验项、惩罚与状态转移模型偏差的动态项以及惩罚与测量值失配的观测项组成。\n\n### 任务 2: 梯度 $\\nabla J(x_{0:T})$ 和 Hessian 矩阵 $\\nabla^2 J(x_{0:T})$\n\n为了使用像牛顿法这样的基于梯度的方法来最小化 $J(x_{0:T})$，我们需要其梯度向量 $\\nabla J(x_{0:T})$ 和 Hessian 矩阵 $\\nabla^2 J(x_{0:T})$。状态向量为 $x = (x_0, \\dots, x_T)^T$。\n\n梯度是对于 $k=0, \\dots, T$ 的偏导数向量 $\\frac{\\partial J}{\\partial x_k}$。\n-   对于 $k=0$:\n    $$\n    \\frac{\\partial J}{\\partial x_0} = \\frac{1}{P_0}(x_0 - m_0) + \\frac{1}{q}(x_{1} - ax_0)(-a) + \\frac{1}{r}(y_0 - x_0^2)(-2x_0) = \\frac{x_0 - m_0}{P_0} - \\frac{a}{q}(x_1 - ax_0) - \\frac{2x_0}{r}(y_0 - x_0^2)\n    $$\n-   对于 $k \\in \\{1, \\dots, T-1\\}$:\n    $$\n    \\frac{\\partial J}{\\partial x_k} = \\frac{1}{q}(x_k - ax_{k-1}) + \\frac{1}{q}(x_{k+1} - ax_k)(-a) + \\frac{1}{r}(y_k - x_k^2)(-2x_k) = \\frac{1}{q}(x_k - ax_{k-1}) - \\frac{a}{q}(x_{k+1} - ax_k) - \\frac{2x_k}{r}(y_k - x_k^2)\n    $$\n-   对于 $k=T$:\n    $$\n    \\frac{\\partial J}{\\partial x_T} = \\frac{1}{q}(x_T - ax_{T-1}) + \\frac{1}{r}(y_T - x_T^2)(-2x_T) = \\frac{1}{q}(x_T - ax_{T-1}) - \\frac{2x_T}{r}(y_T - x_T^2)\n    $$\n\nHessian 矩阵是二阶偏导数的对称矩阵 $H_{ij} = \\frac{\\partial^2 J}{\\partial x_i \\partial x_j}$。由于动态的马尔可夫结构，该矩阵是三对角的。\n对角元素 $H_{k,k}$ 是：\n-   对于 $k=0$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_0^2} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{\\partial}{\\partial x_0}\\left(-\\frac{2x_0y_0}{r} + \\frac{2x_0^3}{r}\\right) = \\frac{1}{P_0} + \\frac{a^2}{q} - \\frac{2y_0}{r} + \\frac{6x_0^2}{r} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{2}{r}(3x_0^2 - y_0)\n    $$\n-   对于 $k \\in \\{1, \\dots, T-1\\}$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_k^2} = \\frac{1}{q} + \\frac{a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k) = \\frac{1+a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k)\n    $$\n-   对于 $k=T$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_T^2} = \\frac{1}{q} + \\frac{2}{r}(3x_T^2 - y_T)\n    $$\n对于 $k=0, \\dots, T-1$ 的非对角元素 $H_{k, k+1} = H_{k+1, k}$ 是：\n$$\n\\frac{\\partial^2 J}{\\partial x_k \\partial x_{k+1}} = \\frac{\\partial}{\\partial x_{k+1}}\\left( \\dots - \\frac{a}{q}(x_{k+1} - ax_k) \\dots \\right) = -\\frac{a}{q}\n$$\n所有其他 $|i-j|1$ 的非对角元素 $H_{i,j}$ 均为零。\n\n### 任务 3: 拉普拉斯近似\n\n拉普拉斯近似提供了在 MAP 估计 $x_{0:T}^\\star$（即 $J(x_{0:T})$ 的一个局部最小值）附近的后验分布的高斯近似。目标函数在 $x_{0:T}^\\star$ 附近展开到二阶：\n$$\nJ(x_{0:T}) \\approx J(x_{0:T}^\\star) + \\nabla J(x_{0:T}^\\star)^T (x_{0:T} - x_{0:T}^\\star) + \\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T \\nabla^2 J(x_{0:T}^\\star) (x_{0:T} - x_{0:T}^\\star)\n$$\n因为 $x_{0:T}^\\star$ 是一个最小值，所以 $\\nabla J(x_{0:T}^\\star) = 0$。后验则近似为：\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto \\exp(-J(x_{0:T})) \\approx \\exp(-J(x_{0:T}^\\star)) \\exp\\left(-\\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T H^\\star (x_{0:T} - x_{0:T}^\\star)\\right)\n$$\n其中 $H^\\star = \\nabla^2 J(x_{0:T}^\\star)$。这表明后验近似为高斯分布：$\\mathcal{N}(x_{0:T}^\\star, (H^\\star)^{-1})$。模型证据（或边际似然）$p(y_{0:T}) = \\int p(y_{0:T}, x_{0:T}) dx_{0:T}$ 可以通过对未归一化的后验密度积分来近似。如题目所给，对数证据的局部贡献为：\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{T+1}{2} \\log(2\\pi)\n$$\n这个公式结合了后验峰的高度（通过 $-J(x_{0:T}^\\star)$）和峰的体积（通过 $-\\frac{1}{2}\\log\\det H^\\star$，它衡量其“宽度”）。在比较不同局部最小值的证据时，不依赖于 $x_{0:T}^\\star$ 的项会抵消掉，这使得在 $J$ 中包含哪些常数对于相对比较而言变得不重要。\n\n### 任务 4: 观测噪声方差的同伦\n\n提出一种同伦法，或称连续方法，来解决该优化问题。观测噪声方差 $r$ 通过 $\\lambda \\in (0, 1]$ 参数化为 $r(\\lambda) = r_{\\text{base}} / \\lambda$。\n-   当 $\\lambda$ 很小（接近 0）时，$r(\\lambda)$ 变得非常大。一个大的观测方差 $r$ 意味着观测值 $y_t$ 不提供信息。目标函数中的项 $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ 变得很小，因此后验主要由先验和动态决定，这两者定义了一个凸问题。这种“弱观测”问题更容易解决，并且通常在先验均值附近有一个单一的最小值。\n-   随着 $\\lambda$ 向 1 增加，$r(\\lambda)$ 减小，接近目标值 $r_{\\text{base}}$。非凸观测项 $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ 的影响增加，目标函数从简单的、类凸的形状变形为复杂的、可能多峰的目标形状。\n\n策略是从小 $\\lambda$ 处的简单问题的解开始，并将其用作稍大 $\\lambda$ 的初始猜测。通过将 $\\lambda$ 从一个小值逐步增加到 1，我们可以在变形过程中追踪一个局部最小值，这通常比直接在 $\\lambda=1$ 处开始优化更鲁棒。\n\n### 任务 5: 数值实现\n\n实现的核心是使用阻尼牛顿法来寻找 $J(x_{0:T})$ 的局部最小值。\n-   **牛顿步**：在每次迭代中，我们求解线性系统 $H_k p_k = -g_k$ 以获得牛顿步 $p_k$，其中 $g_k$ 和 $H_k$ 分别是当前估计 $x_k$ 处的梯度和 Hessian 矩阵。\n-   **阻尼**：Hessian 矩阵在远离最小值点时可能不是正定的。使用 Levenberg-Marquardt 式修正：我们求解 $(H_k + \\mu I) p_k = -g_k$，其中 $\\mu \\ge 0$ 是一个阻尼参数。如果 $H_k$ 是正定的，则 $\\mu$ 被选为零，否则增加 $\\mu$ 直到 $(H_k + \\mu I)$ 为正定，以确保一个下降方向。\n-   **线搜索**：使用回溯线搜索来确定步长 $\\alpha$。我们从 $\\alpha=1$ 开始，并减小它，直到对于一个小的常数 $c$，满足 Armijo 条件 $J(x_k + \\alpha p_k) \\le J(x_k) + c \\alpha g_k^T p_k$。\n-   **多起点和同伦**：这个牛顿求解器用于两个目的：\n    1.  在 $\\lambda=1$ 时，它从两个不同的初始猜测（$x_t^{(0)} = \\pm\\sqrt{y_t}$）开始运行，以寻找可能不同的局部最小值。\n    2.  它在同伦时间表的每一步中使用，前一个 $\\lambda$ 值的解为当前 $\\lambda$ 提供初始猜测。\n\n对于每个识别出的最小值，最后一步包括检查 Hessian 矩阵是否为正定（所有特征值均为正），然后计算所需的量：拉普拉斯对数证据、平均状态的符号和 Hessian 条件数。",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the smoothing problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case #1 (symmetric, two-minima regime)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #2 (prior breaks symmetry, single-minimum regime likely)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.5, \"P0\": 0.01,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #3 (weak dynamics coupling, multiple nearly-decoupled modes)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 10.0, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p = case\n        dim = p[\"T\"] + 1\n\n        def J_objective(x, r, p):\n            term_prior = 0.5 * (x[0] - p[\"m0\"])**2 / p[\"P0\"]\n            term_dyn = 0.5 * np.sum((x[1:] - p[\"a\"] * x[:-1])**2) / p[\"q\"]\n            term_obs = 0.5 * np.sum((p[\"y\"] - x**2)**2) / r\n            return term_prior + term_dyn + term_obs\n\n        def grad_J(x, r, p):\n            grad = np.zeros_like(x)\n            # Observation part\n            grad += -2 * x * (p[\"y\"] - x**2) / r\n            \n            # Prior part\n            grad[0] += (x[0] - p[\"m0\"]) / p[\"P0\"]\n            \n            # Dynamics part\n            dyn_res = x[1:] - p[\"a\"] * x[:-1]\n            grad[1:] += dyn_res / p[\"q\"]\n            grad[:-1] -= p[\"a\"] * dyn_res / p[\"q\"]\n            \n            return grad\n\n        def hess_J(x, r, p):\n            hess = np.zeros((dim, dim))\n            \n            # Diagonal\n            diag = np.zeros(dim)\n            diag += 2 * (3 * x**2 - p[\"y\"]) / r\n            diag[0] += 1 / p[\"P0\"] + p[\"a\"]**2 / p[\"q\"]\n            diag[1:p[\"T\"]] += (1 + p[\"a\"]**2) / p[\"q\"]\n            diag[p[\"T\"]] += 1 / p[\"q\"]\n            np.fill_diagonal(hess, diag)\n            \n            # Off-diagonal\n            off_diag_val = -p[\"a\"] / p[\"q\"]\n            np.fill_diagonal(hess[1:], off_diag_val)\n            np.fill_diagonal(hess[:, 1:], off_diag_val)\n            \n            return hess\n\n        def newton_solver(x_init, lam, p, max_iter=100, tol=1e-8):\n            x = x_init.copy()\n            r = p[\"r_base\"] / lam\n            \n            for _ in range(max_iter):\n                g = grad_J(x, r, p)\n                if np.linalg.norm(g)  tol:\n                    break\n                \n                H = hess_J(x, r, p)\n                \n                # Levenberg-Marquardt Damping\n                mu = 0\n                while True:\n                    try:\n                        H_lm = H + mu * np.eye(dim)\n                        L = scipy.linalg.cholesky(H_lm, lower=True)\n                        break\n                    except np.linalg.LinAlgError:\n                        if mu == 0:\n                            mu = 1e-6\n                        else:\n                            mu *= 10\n                \n                # Solve linear system H_lm * step = -g\n                step = scipy.linalg.cho_solve((L, True), -g)\n\n                if np.linalg.norm(step)  tol:\n                    break\n\n                # Backtracking Line Search\n                alpha = 1.0\n                c = 1e-4\n                J_current = J_objective(x, r, p)\n                while alpha > 1e-8:\n                    x_new = x + alpha * step\n                    if J_objective(x_new, r, p)  J_current + c * alpha * np.dot(g, step):\n                        break\n                    alpha *= 0.5\n                \n                x = x + alpha * step\n\n            return x\n\n        # 1. Multi-start Newton's at lambda=1\n        x_init_pos = np.sqrt(p[\"y\"])\n        x_init_neg = -np.sqrt(p[\"y\"])\n        \n        x_sol_pos = newton_solver(x_init_pos, 1.0, p)\n        x_sol_neg = newton_solver(x_init_neg, 1.0, p)\n\n        minima = []\n        r_final = p[\"r_base\"]\n        \n        # Check first minimum\n        H_pos = hess_J(x_sol_pos, r_final, p)\n        try:\n            eigvals_pos = np.linalg.eigvalsh(H_pos)\n            if np.all(eigvals_pos > 0):\n                minima.append({'x': x_sol_pos, 'H': H_pos, 'origin': 'pos', 'eigvals':eigvals_pos})\n        except np.linalg.LinAlgError:\n            pass\n\n        # Check second minimum\n        is_new = True\n        for m in minima:\n            if np.linalg.norm(m['x'] - x_sol_neg)  1e-3:\n                is_new = False\n                break\n        \n        if is_new:\n            H_neg = hess_J(x_sol_neg, r_final, p)\n            try:\n                eigvals_neg = np.linalg.eigvalsh(H_neg)\n                if np.all(eigvals_neg > 0):\n                    minima.append({'x': x_sol_neg, 'H': H_neg, 'origin': 'neg', 'eigvals':eigvals_neg})\n            except np.linalg.LinAlgError:\n                pass\n        \n        num_minima = len(minima)\n\n        # 2. Laplace log-evidence difference\n        log_evidence_diff = 0.0\n        if num_minima == 2:\n            m_pos = next((m for m in minima if m['origin'] == 'pos'), None)\n            m_neg = next((m for m in minima if m['origin'] == 'neg'), None)\n            \n            if m_pos is not None and m_neg is not None:\n                J_pos = J_objective(m_pos['x'], r_final, p)\n                logdet_pos = np.sum(np.log(m_pos['eigvals']))\n                \n                J_neg = J_objective(m_neg['x'], r_final, p)\n                logdet_neg = np.sum(np.log(m_neg['eigvals']))\n                \n                log_z_pos = -J_pos - 0.5 * logdet_pos\n                log_z_neg = -J_neg - 0.5 * logdet_neg\n                \n                log_evidence_diff = log_z_pos - log_z_neg\n\n        # 3. Homotopy tracking\n        x_homotopy = np.sqrt(p[\"y\"])\n        for lam in p[\"lambda_schedule\"]:\n            x_homotopy = newton_solver(x_homotopy, lam, p)\n        \n        sign_avg_homotopy = int(np.sign(np.mean(x_homotopy)))\n\n        # 4. Condition number\n        H_homotopy = hess_J(x_homotopy, r_final, p)\n        cond_num = np.inf\n        try:\n            eigvals_homotopy = np.linalg.eigvalsh(H_homotopy)\n            if np.all(eigvals_homotopy > 0):\n                cond_num = eigvals_homotopy[-1] / eigvals_homotopy[0]\n        except np.linalg.LinAlgError:\n            pass\n        \n        all_results.append(f\"[{num_minima},{log_evidence_diff:.8f},{sign_avg_homotopy},{cond_num:.8f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}