{
    "hands_on_practices": [
        {
            "introduction": "动手实践的第一步是回归基本原理。通过从头推导标量稳态卡尔曼增益，你将亲身体验线性和高斯性假设如何简化贝叶斯推断，最终得到一个简洁的解析解。这个练习旨在揭示这些核心假设在数据同化算法中的基础性作用及其数学上的便利性 。",
            "id": "3365459",
            "problem": "考虑一个用于逆问题和数据同化的标量线性高斯状态空间模型，其由动力学方程 $x_{k+1} = a x_{k} + w_{k}$ 和观测方程 $y_{k} = b x_{k} + \\epsilon_{k}$ 给出，其中 $w_{k} \\sim \\mathcal{N}(0, Q)$ 和 $\\epsilon_{k} \\sim \\mathcal{N}(0, R)$ 是相互独立的，并且在时间上也是独立的。假设 $Q > 0$、$R > 0$ 且 $b \\neq 0$。进一步假设参数满足标准的可检测性和可镇定性条件，从而使得稳态卡尔曼滤波器存在。\n\n仅从模型的线性性质和所有不确定性的高斯性出发，并使用贝叶斯定理以及高斯密度乘积法则，推导稳态标量卡尔曼增益，使其纯粹表示为 $a$、$b$、$Q$ 和 $R$ 的函数。您的推导必须从第一性原理出发，通过以下步骤进行：\n- 建立单步预测分布和测量似然的高斯形式。\n- 形成后验分布，并根据新息和增益因子确定其均值。\n- 推导由高斯后验和线性动力学所蕴含的稳态误差协方差的不动点关系。\n- 求解所得的代数关系，以消除任何协方差变量，并获得仅用 $a$、$b$、$Q$ 和 $R$ 表示的稳态卡尔曼增益的闭式解析表达式。\n\n提供稳态卡尔曼增益的最终表达式。不需要进行数值计算，也无需四舍五入。最终答案必须是仅包含 $a$、$b$、$Q$ 和 $R$ 的单个闭式解析表达式，不含任何剩余的辅助变量。",
            "solution": "该问题要求推导标量线性高斯状态空间模型的稳态卡尔曼增益。推导必须从第一性原理出发，即贝叶斯定理和高斯分布的性质。\n\n状态空间模型由以下公式给出：\n状态动力学：$x_{k+1} = a x_{k} + w_{k}$，其中 $w_{k} \\sim \\mathcal{N}(0, Q)$\n观测模型：$y_{k} = b x_{k} + \\epsilon_{k}$，其中 $\\epsilon_{k} \\sim \\mathcal{N}(0, R)$\n\n此处，$x_k$ 是时间 $k$ 的隐藏状态，$y_k$ 是观测值。过程噪声 $w_k$ 和测量噪声 $\\epsilon_k$ 被假定为独立的、零均值的高斯白噪声序列，其方差分别为 $Q > 0$ 和 $R > 0$。参数 $a$ 和 $b$ 是标量，且 $b \\neq 0$。\n\n卡尔曼滤波器的核心是一个贝叶斯更新循环，其中时间 $k-1$ 状态的后验分布用于预测时间 $k$ 的状态，然后该预测作为先验，用于融合时间 $k$ 的测量值。\n\n设给定截至时间 $k-1$ 的所有观测值（记为 $y_{1:k-1}$）时，状态 $x_{k-1}$ 的后验分布为高斯分布：\n$$p(x_{k-1} | y_{1:k-1}) = \\mathcal{N}(x_{k-1}; \\hat{x}_{k-1|k-1}, P_{k-1|k-1})$$\n其中 $\\hat{x}_{k-1|k-1}$ 是滤波后的状态估计，而 $P_{k-1|k-1}$ 是其误差协方差。\n\n**步骤 1：预测（时间更新）**\n第一步是推导单步预测分布 $p(x_k | y_{1:k-1})$，它作为时间 $k$ 更新步骤的先验。使用状态动力学方程 $x_k = a x_{k-1} + w_{k-1}$，我们求出 $x_k$ 的均值和方差。由于 $x_{k-1}$ 和 $w_{k-1}$ 是独立的高斯变量，因此 $x_k$ 也是高斯变量。\n预测均值为：\n$$\\hat{x}_{k|k-1} = E[x_k | y_{1:k-1}] = E[a x_{k-1} + w_{k-1} | y_{1:k-1}] = a E[x_{k-1} | y_{1:k-1}] + E[w_{k-1}] = a \\hat{x}_{k-1|k-1}$$\n预测误差协方差为：\n$$P_{k|k-1} = \\text{Var}(x_k - \\hat{x}_{k|k-1}) = \\text{Var}(a(x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1}) = a^2 P_{k-1|k-1} + Q$$\n因此，预测分布（先验）为：\n$$p(x_k | y_{1:k-1}) = \\mathcal{N}(x_k; \\hat{x}_{k|k-1}, P_{k|k-1})$$\n\n**步骤 2：更新（测量更新）**\n第二步是利用来自测量值 $y_k$ 的新信息来更新先验。根据贝叶斯定理，后验分布正比于似然与先验的乘积：\n$$p(x_k | y_{1:k}) \\propto p(y_k | x_k) p(x_k | y_{1:k-1})$$\n由 $y_k = b x_k + \\epsilon_k$ 可知，测量似然也是高斯分布：\n$$p(y_k | x_k) = \\mathcal{N}(y_k; b x_k, R)$$\n由于两个高斯概率密度函数 (PDF) 的乘积是一个未归一化的高斯 PDF，因此后验 $p(x_k | y_{1:k})$ 是高斯分布。我们可以通过分析乘积的指数部分来找到其均值和方差：\n$$\\ln p(x_k | y_{1:k}) \\propto -\\frac{1}{2}\\left( \\frac{(x_k - \\hat{x}_{k|k-1})^2}{P_{k|k-1}} + \\frac{(y_k - b x_k)^2}{R} \\right)$$\n展开 $x_k$ 的二次项：\n$$\\text{exponent} \\propto x_k^2 \\left(\\frac{1}{P_{k|k-1}} + \\frac{b^2}{R}\\right) - 2x_k \\left(\\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R}\\right)$$\n后验 PDF 为 $p(x_k | y_{1:k}) = \\mathcal{N}(x_k; \\hat{x}_{k|k}, P_{k|k})$，其指数部分正比于 $-\\frac{1}{2 P_{k|k}}(x_k - \\hat{x}_{k|k})^2 \\propto \\frac{x_k^2}{P_{k|k}} - \\frac{2x_k \\hat{x}_{k|k}}{P_{k|k}}$。\n通过比较 $x_k^2$ 的系数，我们得到后验协方差 $P_{k|k}$ 的倒数：\n$$\\frac{1}{P_{k|k}} = \\frac{1}{P_{k|k-1}} + \\frac{b^2}{R} \\implies P_{k|k} = \\left(\\frac{1}{P_{k|k-1}} + \\frac{b^2}{R}\\right)^{-1} = \\frac{P_{k|k-1}R}{R + b^2 P_{k|k-1}}$$\n通过比较 $x_k$ 的系数，我们得到后验均值 $\\hat{x}_{k|k}$：\n$$\\frac{\\hat{x}_{k|k}}{P_{k|k}} = \\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R} \\implies \\hat{x}_{k|k} = P_{k|k} \\left(\\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R}\\right)$$\n为了得到标准的卡尔曼滤波器形式，我们代入 $P_{k|k}$ 的表达式：\n$$\\hat{x}_{k|k} = \\frac{P_{k|k-1}R}{R + b^2 P_{k|k-1}} \\left(\\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R}\\right) = \\frac{R}{R + b^2 P_{k|k-1}} \\hat{x}_{k|k-1} + \\frac{b P_{k|k-1}}{R + b^2 P_{k|k-1}} y_k$$\n这可以重写为包含卡尔曼增益 $K_k$ 的熟悉形式：\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - b \\hat{x}_{k|k-1})$$\n通过比较 $\\hat{x}_{k|k}$ 的两个表达式，特别是 $y_k$ 的系数，我们确定卡尔曼增益：\n$$K_k = \\frac{b P_{k|k-1}}{R + b^2 P_{k|k-1}}$$\n\n**步骤 3：稳态分析**\n问题要求的是稳态卡尔曼增益。在稳态下，当 $k \\to \\infty$ 时，误差协方差收敛到常数值。令 $P_{k|k-1} \\to P_p$ 且 $P_{k|k} \\to P_a$。卡尔曼增益也收敛到一个常数值 $K$。协方差的递归方程变成了代数方程：\n1. 预测：$P_p = a^2 P_a + Q$\n2. 更新：$P_a = \\frac{P_p R}{R + b^2 P_p}$\n稳态增益为 $K = \\frac{b P_p}{R + b^2 P_p}$。\n\n为了找到纯粹用模型参数表示的 $K$，我们必须消除协方差变量。我们可以为 $K$ 推导出一个单一的代数方程。从增益方程中，我们将 $P_p$ 表示为 $K$ 的函数：\n$$K(R + b^2 P_p) = b P_p \\implies KR + K b^2 P_p = b P_p \\implies KR = P_p(b - K b^2) = P_p b(1 - K b)$$\n$$P_p = \\frac{KR}{b(1-Kb)}$$\n同样，从增益方程我们有 $\\frac{P_p}{R + b^2 P_p} = \\frac{K}{b}$。将此代入 $P_a$ 的更新方程中：\n$$P_a = R \\left( \\frac{K}{b} \\right)$$\n现在将 $P_p$ 和 $P_a$ 的这些表达式代入预测方程中：\n$$P_p = a^2 P_a + Q \\implies \\frac{KR}{b(1-Kb)} = a^2 \\left(\\frac{RK}{b}\\right) + Q$$\n为了求解 $K$，我们将方程两边乘以 $b(1-Kb)$：\n$$KR = a^2 R K (1-Kb) + Q b (1-Kb)$$\n$$KR = a^2 R K - a^2 b R K^2 + Qb - Q b^2 K$$\n整理各项，得到关于稳态增益 $K$ 的二次方程：\n$$(a^2 b R) K^2 + (R - a^2 R + Q b^2) K - Q b = 0$$\n$$(a^2 b R) K^2 + (R(1 - a^2) + Q b^2) K - Q b = 0$$\n\n**步骤 4：求解卡尔曼增益**\n这是一个形式为 $\\mathcal{A}K^2 + \\mathcal{B}K + \\mathcal{C} = 0$ 的二次方程，其中：\n$\\mathcal{A} = a^2 b R$\n$\\mathcal{B} = R(1 - a^2) + Q b^2$\n$\\mathcal{C} = -Q b$\n\n其解为 $K = \\frac{-\\mathcal{B} \\pm \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}}}{2\\mathcal{A}}$。稳态解的存在意味着我们必须选择对应于稳定滤波器的根，即误差动力学稳定的那个根。这对应于一个唯一的增益 $K$，其符号与 $b$ 的符号相匹配。为了避免在 $a$ 很小时出现灾难性抵消问题，并将解写成在 $a=0$ 时良定的形式，我们将分子和分母同乘以分子的共轭：\n$$K = \\frac{-\\mathcal{B} + \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}}}{2\\mathcal{A}} = \\frac{4\\mathcal{A}\\mathcal{C}}{2\\mathcal{A}(-\\mathcal{B} - \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}})} = \\frac{2\\mathcal{C}}{-\\mathcal{B} - \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}}}$$\n代入 $\\mathcal{A}$、$\\mathcal{B}$ 和 $\\mathcal{C}$ 的表达式：\n$$K = \\frac{2(-Qb)}{-(R(1 - a^2) + Q b^2) - \\sqrt{(R(1 - a^2) + Q b^2)^2 - 4(a^2 b R)(-Qb)}}$$\n$$K = \\frac{-2Qb}{-(R(1 - a^2) + Q b^2) - \\sqrt{(R(1 - a^2) + Q b^2)^2 + 4 a^2 b^2 Q R}}$$\n$$K = \\frac{2Qb}{R(1 - a^2) + Q b^2 + \\sqrt{(R(1 - a^2) + Q b^2)^2 + 4 a^2 b^2 Q R}}$$\n这就是稳态标量卡尔曼增益的闭式解析表达式。",
            "answer": "$$\n\\boxed{\\frac{2Qb}{R(1 - a^2) + Q b^2 + \\sqrt{\\left(R(1 - a^2) + Q b^2\\right)^2 + 4 a^2 b^2 Q R}}}\n$$"
        },
        {
            "introduction": "在理想化的模型之外，观测误差往往是相关的。本练习将向你展示如何处理这种情况，即通过构建一个“白化”变换，将具有相关高斯噪声的复杂问题转化为一个等价的、具有独立标准高斯噪声的简单问题。这个过程不仅是一个实用的数据预处理技巧，也深刻展示了线性变换在线性-高斯框架下的强大能力 。",
            "id": "3365471",
            "problem": "考虑一个逆问题中的线性观测模型，其满足线性和高斯性假设。设观测值由 $y = H x + \\epsilon$ 建模，其中 $y \\in \\mathbb{R}^{3}$ 是观测值，$x \\in \\mathbb{R}^{2}$ 是状态，$H \\in \\mathbb{R}^{3 \\times 2}$ 是线性观测算子，$\\epsilon \\in \\mathbb{R}^{3}$ 是观测误差，服从多元正态分布 $\\epsilon \\sim \\mathcal{N}(0, R)$，其协方差矩阵 $R \\in \\mathbb{R}^{3 \\times 3}$ 是对称正定（SPD）的。假设以下具体的、具有科学真实性的矩阵和向量：\n$$\nR = \\begin{pmatrix}\n4  2  0 \\\\\n2  10  6 \\\\\n0  6  5\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  2\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n4 \\\\\n0 \\\\\n3\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix}.\n$$\n仅从多元正态概率密度函数（PDF）的定义和线性观测模型出发，执行以下操作：\n\n1. 根据残差 $d = y - H x$ 和协方差 $R$，推导与线性和高斯性一致的负对数似然（数据失配）。\n\n2. 构建一个白化变换，即找到一个矩阵 $W \\in \\mathbb{R}^{3 \\times 3}$ 使得 $W R W^{\\top} = I$。这通过将 $R$ 进行 Cholesky 分解得到一个对角线元素为正的下三角矩阵 $L$（其中 $R = L L^{\\top}$），然后设 $W = L^{-1}$ 来实现。\n\n3. 使用您构建的 $W$，定义白化变量 $\\tilde{y} = W y$ 和 $\\tilde{H} = W H$，并通过将数据失配表示为白化残差的欧几里得范数来展示白化的效果，即证明它变成了标准的最小二乘形式。\n\n4. 对于给定的 $y$、$H$ 和 $x$，使用精确算术计算白化数据失配的数值，并在可能的情况下简化为精确的有理数。\n\n您的最终答案必须是单一的封闭形式解析表达式：明确的白化矩阵 $W$。不需要四舍五入，且本问题中的量没有单位。",
            "solution": "该问题被验证为自洽的，其科学基础在于应用于逆问题的线性代数和统计学原理，并且是适定的。所提供的协方差矩阵 $R$ 是对称的，其顺序主子式分别为 $4$、$36$ 和 $36$，所有这些都是正数，这证实了 $R$ 是正定的。因此，其 Cholesky 分解存在且唯一。我们可以按照指定的四个步骤进行求解。\n\n**1. 负对数似然的推导**\n\n问题假设了一个线性观测模型 $y = H x + \\epsilon$，其中观测误差 $\\epsilon$ 服从均值为零、协方差矩阵为 $R$ 的多元正态分布，记作 $\\epsilon \\sim \\mathcal{N}(0, R)$。给定一个状态 $x$，观测值 $y$ 也因此服从正态分布，$y \\sim \\mathcal{N}(Hx, R)$。\n\n对于均值为 $\\mu$、协方差矩阵为 $\\Sigma$ 的多元正态随机向量 $z \\in \\mathbb{R}^k$，其概率密度函数（PDF）由下式给出：\n$$\np(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^{\\top} \\Sigma^{-1} (z-\\mu)\\right)\n$$\n在我们的情境中，观测向量 $y$ 在 $\\mathbb{R}^3$ 中，所以 $k=3$。均值为 $\\mu = Hx$，协方差为 $\\Sigma=R$。因此，给定状态 $x$ 时观测到 $y$ 的似然函数为：\n$$\n\\mathcal{L}(x|y) = p(y|x) = \\frac{1}{\\sqrt{(2\\pi)^3 \\det(R)}} \\exp\\left(-\\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\\right)\n$$\n负对数似然是通过对似然函数取自然对数然后取负得到的：\n$$\n-\\ln(\\mathcal{L}(x|y)) = -\\ln\\left(\\frac{1}{\\sqrt{(2\\pi)^3 \\det(R)}}\\right) - \\ln\\left(\\exp\\left(-\\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\\right)\\right)\n$$\n$$\n-\\ln(\\mathcal{L}(x|y)) = \\ln\\left(\\sqrt{(2\\pi)^3 \\det(R)}\\right) + \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n$$\n-\\ln(\\mathcal{L}(x|y)) = \\frac{3}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln(\\det(R)) + \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n在优化和数据同化中，数据失配函数（通常表示为 $J(x)$）由负对数似然中依赖于状态 $x$ 的项组成。常数项通常被舍去。因此，数据失配与二次项成正比。我们将失配定义为：\n$$\nJ(x) = \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n将残差定义为 $d = y - Hx$，数据失配表示为：\n$$\nJ(x) = \\frac{1}{2} d^{\\top} R^{-1} d\n$$\n\n**2. 白化变换的构建**\n\n我们的任务是找到一个白化矩阵 $W$ 使得 $W R W^{\\top} = I$。这可以通过首先找到 $R$ 的 Cholesky 分解 $R = L L^{\\top}$ 来实现，其中 $L$ 是一个对角线元素为正的下三角矩阵，然后设 $W = L^{-1}$。\n\n给定 $R = \\begin{pmatrix} 4  2  0 \\\\ 2  10  6 \\\\ 0  6  5 \\end{pmatrix}$，我们寻求 $L = \\begin{pmatrix} L_{11}  0  0 \\\\ L_{21}  L_{22}  0 \\\\ L_{31}  L_{32}  L_{33} \\end{pmatrix}$ 使得 $L L^{\\top} = R$。\n\n$L$ 的分量按顺序求解：\n- $L_{11}^2 = R_{11} = 4 \\implies L_{11} = 2$。\n- $L_{21}L_{11} = R_{21} = 2 \\implies L_{21}(2) = 2 \\implies L_{21} = 1$。\n- $L_{31}L_{11} = R_{31} = 0 \\implies L_{31}(2) = 0 \\implies L_{31} = 0$。\n- $L_{21}^2 + L_{22}^2 = R_{22} = 10 \\implies 1^2 + L_{22}^2 = 10 \\implies L_{22}^2 = 9 \\implies L_{22} = 3$。\n- $L_{32}L_{22} + L_{31}L_{21} = R_{32} = 6 \\implies L_{32}(3) + (0)(1) = 6 \\implies L_{32} = 2$。\n- $L_{31}^2 + L_{32}^2 + L_{33}^2 = R_{33} = 5 \\implies 0^2 + 2^2 + L_{33}^2 = 5 \\implies L_{33}^2 = 1 \\implies L_{33} = 1$。\n\n因此，Cholesky 因子为 $L = \\begin{pmatrix} 2  0  0 \\\\ 1  3  0 \\\\ 0  2  1 \\end{pmatrix}$。\n\n接下来，我们通过向前代入法求解 $L W = I$ 来找到白化矩阵 $W = L^{-1}$。设 $W = (w_{ij})$。\n$$\n\\begin{pmatrix} 2  0  0 \\\\ 1  3  0 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} w_{11}  w_{12}  w_{13} \\\\ w_{21}  w_{22}  w_{23} \\\\ w_{31}  w_{32}  w_{33} \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n对 $W$ 的每一列进行求解：\n- 第1列：$2w_{11}=1 \\implies w_{11}=\\frac{1}{2}$。$w_{11}+3w_{21}=0 \\implies \\frac{1}{2}+3w_{21}=0 \\implies w_{21}=-\\frac{1}{6}$。$2w_{21}+w_{31}=0 \\implies 2(-\\frac{1}{6})+w_{31}=0 \\implies w_{31}=\\frac{1}{3}$。另外 $w_{12}=w_{13}=0$。\n- 第2列：$3w_{22}=1 \\implies w_{22}=\\frac{1}{3}$。$2w_{22}+w_{32}=0 \\implies 2(\\frac{1}{3})+w_{32}=0 \\implies w_{32}=-\\frac{2}{3}$。另外 $w_{23}=0$。\n- 第3列：$w_{33}=1$。\n\n得到的白化矩阵为 $W = L^{-1} = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ -\\frac{1}{6}  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  -\\frac{2}{3}  1 \\end{pmatrix}$。\n\n**3. 白化对数据失配的影响**\n\n我们从数据失配表达式 $J(x) = \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)$ 开始。\n由 $R = L L^{\\top}$，我们有 $R^{-1} = (L L^{\\top})^{-1} = (L^{\\top})^{-1}L^{-1}$。由于转置的逆等于逆的转置，即 $(L^{\\top})^{-1} = (L^{-1})^{\\top}$。又因 $W=L^{-1}$，我们得到 $R^{-1} = W^{\\top}W$。\n将此代入失配函数：\n$$\nJ(x) = \\frac{1}{2} (y-Hx)^{\\top} (W^{\\top}W) (y-Hx)\n$$\n使用属性 $(AB)^{\\top} = B^{\\top}A^{\\top}$，我们可以重新组合各项：\n$$\nJ(x) = \\frac{1}{2} [W(y-Hx)]^{\\top} [W(y-Hx)]\n$$\n定义白化观测值 $\\tilde{y} = Wy$、白化算子 $\\tilde{H} = WH$ 以及白化残差 $\\tilde{d} = \\tilde{y} - \\tilde{H}x = W(y-Hx)$，表达式变为：\n$$\nJ(x) = \\frac{1}{2} \\tilde{d}^{\\top}\\tilde{d} = \\frac{1}{2} \\|\\tilde{d}\\|_2^2 = \\frac{1}{2} \\|\\tilde{y} - \\tilde{H}x\\|_2^2\n$$\n这表明，由协方差矩阵的逆加权的广义最小二乘代价函数，被转换为了一个标准的、无权重的最小二乘问题，该问题涉及白化残差的平方和。\n\n**4. 白化数据失配的数值计算**\n\n首先，我们用给定的值计算残差 $d = y - Hx$：\n$H = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  2 \\end{pmatrix}$，$x = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$，$y = \\begin{pmatrix} 4 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n$$\nHx = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  2 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(-1) \\\\ (1)(2) + (1)(-1) \\\\ (0)(2) + (2)(-1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\n$$\nd = y - Hx = \\begin{pmatrix} 4 \\\\ 0 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 5 \\end{pmatrix}\n$$\n接下来，我们计算白化残差 $\\tilde{d} = W d$：\n$$\n\\tilde{d} = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ -\\frac{1}{6}  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  -\\frac{2}{3}  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{2})(2) + (0)(-1) + (0)(5) \\\\ (-\\frac{1}{6})(2) + (\\frac{1}{3})(-1) + (0)(5) \\\\ (\\frac{1}{3})(2) + (-\\frac{2}{3})(-1) + (1)(5) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{1}{3} - \\frac{1}{3} \\\\ \\frac{2}{3} + \\frac{2}{3} + 5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{2}{3} \\\\ \\frac{4}{3} + \\frac{15}{3} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{2}{3} \\\\ \\frac{19}{3} \\end{pmatrix}\n$$\n最后，我们计算白化数据失配 $J = \\frac{1}{2}\\|\\tilde{d}\\|_2^2$：\n$$\nJ = \\frac{1}{2} \\left( (1)^2 + \\left(-\\frac{2}{3}\\right)^2 + \\left(\\frac{19}{3}\\right)^2 \\right) = \\frac{1}{2} \\left( 1 + \\frac{4}{9} + \\frac{361}{9} \\right)\n$$\n$$\nJ = \\frac{1}{2} \\left( \\frac{9}{9} + \\frac{4}{9} + \\frac{361}{9} \\right) = \\frac{1}{2} \\left( \\frac{9+4+361}{9} \\right) = \\frac{1}{2} \\left( \\frac{374}{9} \\right) = \\frac{187}{9}\n$$\n数据失配的数值为 $\\frac{187}{9}$。问题要求最终答案为白化矩阵 $W$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ -\\frac{1}{6}  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  -\\frac{2}{3}  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "高斯假设虽然方便，但在现实数据中，尤其是在存在离群值（outliers）时，这一假设可能不再成立。本计算练习将引导你探索当高斯性假设被违背时的后果。通过编写代码比较基于高斯似然和基于更稳健的Student-$t$分布的估算结果，你将量化离群值对估算精度的影响，并学习如何实现一种对异常数据不那么敏感的强大估计算法 。",
            "id": "3365464",
            "problem": "考虑一个线性逆问题，其正向算子由一个已知矩阵 $H \\in \\mathbb{R}^{m \\times n}$ 表示，未知状态向量为 $x \\in \\mathbb{R}^{n}$，观测数据为 $y \\in \\mathbb{R}^{m}$。观测模型为 $y = H x + \\varepsilon$，其中 $\\varepsilon$ 表示测量噪声。你需要分析在两种噪声建模假设下的最大后验 (MAP) 估计器：一个错误指定的高斯似然和一个正确指定的重尾学生t似然，并量化每种 MAP 估计相对于已知真值的偏差。\n\n使用贝叶斯法则以及高斯和学生t概率密度函数 (PDF) 的定义作为基本基础。具体来说：\n- 对于一个方差参数为 $\\sigma^{2}$ 的标量，其高斯 PDF 为 $p(r) \\propto \\exp\\left(-\\frac{r^{2}}{2 \\sigma^{2}}\\right)$。\n- 具有自由度 $\\nu$ 和尺度 $\\sigma$ 的学生t PDF 为 $p(r) \\propto \\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu + 1}{2}}$。\n\n假设 $x$ 服从高斯先验，即 $x \\sim \\mathcal{N}(m, P)$，其中 $m \\in \\mathbb{R}^{n}$，$P \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。在高斯似然下，MAP 估计最小化一个凸二次目标。在学生t似然下，MAP 估计最小化一个基于对数项之和的鲁棒目标。从第一性原理推导 MAP 估计器，并设计一个算法，为给定的 $H$、$y$、$m$、$P$、$\\sigma$ 和 $\\nu$ 计算两种 MAP 估计。然后，比较它们相对于已知真值 $x^{\\star}$ 的偏差。\n\n你必须编写一个完整的、可运行的程序，该程序：\n1. 构建以下固定的问题实例：\n   - 维度：$m = 8$, $n = 2$。\n   - 正向矩阵 $H$ (逐行明确列出)：\n     - 行 1：$\\left[1.0,\\,0.5\\right]$\n     - 行 2：$\\left[0.8,\\,-0.3\\right]$\n     - 行 3：$\\left[-0.6,\\,1.2\\right]$\n     - 行 4：$\\left[0.0,\\,1.0\\right]$\n     - 行 5：$\\left[1.5,\\,-0.7\\right]$\n     - 行 6：$\\left[-1.0,\\,-0.2\\right]$\n     - 行 7：$\\left[0.3,\\,0.8\\right]$\n     - 行 8：$\\left[-0.4,\\,0.5\\right]$\n   - 真值状态 $x^{\\star} = \\left[1.0,\\,-2.0\\right]$。\n   - 先验均值 $m = \\left[0.0,\\,0.0\\right]$。\n   - 先验协方差 $P = \\mathrm{diag}\\left(10.0,\\,10.0\\right)$。\n   - 噪声尺度 $\\sigma = 1.0$ (注意：不涉及物理单位)。\n2. 通过使用固定的残差向量 $r$ 创建 $y = H x^{\\star} + r$，并为学生t似然指定 $\\nu$，形成包含四种情况的测试套件：\n   - 情况 1 (重尾残差，中等自由度)：\n     - $\\nu = 3$\n     - $r = \\left[0.2,\\,-0.1,\\,0.15,\\,-0.05,\\,0.1,\\,-0.1,\\,12.0,\\,-15.0\\right]$\n   - 情况 2 (近高斯行为)：\n     - $\\nu = 30$\n     - $r = \\left[0.05,\\,-0.02,\\,0.03,\\,-0.04,\\,0.01,\\,-0.01,\\,0.02,\\,-0.03\\right]$\n   - 情况 3 (极端重尾，多个大离群值)：\n     - $\\nu = 1.5$\n     - $r = \\left[20.0,\\,-25.0,\\,0.2,\\,0.0,\\,-0.1,\\,18.0,\\,-22.0,\\,0.3\\right]$\n   - 情况 4 (无残差的基线)：\n     - $\\nu = 3$\n     - $r = \\left[0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\right]$\n3. 为每种情况计算：\n   - 高斯似然 MAP 估计 $x_{\\mathrm{G}}$。\n   - 使用从第一性原理推导并通过收敛迭代方案实现的鲁棒算法计算学生t似然 MAP 估计 $x_{\\mathrm{T}}$。\n   - 每种假设下偏差的欧几里得范数：$\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}$ 和 $\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}$。\n   - 相对改进比 $\\rho = \\frac{\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}}{\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}}$ (如果分母为 $0$，则将 $\\rho$ 设置为 $+\\infty$）。\n   - 两种估计之间的差异范数 $\\left\\|x_{\\mathrm{G}} - x_{\\mathrm{T}}\\right\\|_{2}$。\n4. 生成单行输出，其中包含所有四种情况的汇总结果，格式为方括号括起来的逗号分隔列表。每种情况的结果必须是按以下顺序排列的四个浮点数列表：\n   - $\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}$，\n   - $\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}$，\n   - $\\rho$，\n   - $\\left\\|x_{\\mathrm{G}} - x_{\\mathrm{T}}\\right\\|_{2}$。\n例如，最终输出必须采用格式 $\\left[\\left[a_{1},b_{1},c_{1},d_{1}\\right],\\left[a_{2},b_{2},c_{2},d_{2}\\right],\\left[a_{3},b_{3},c_{3},d_{3}\\right],\\left[a_{4},b_{4},c_{4},d_{4}\\right]\\right]$。\n\n覆盖性设计：\n- 情况 1 测试在中度重尾建模下少数几个大离群值的影响。\n- 情况 2 近似高斯情况。\n- 情况 3 测试存在多个大离群值的极端重尾情况。\n- 情况 4 是一个无残差的边界情况。\n\n不涉及角度和物理单位；将所有量报告为无单位的纯浮点数。你的实现必须是自包含的，并且不得读取任何输入。最终程序必须为给定的测试套件确定性地计算指定的输出。",
            "solution": "提出的问题要求针对一个线性逆问题，基于对测量噪声统计的不同假设，推导并比较两种最大后验 (MAP) 估计器。该任务的核心是应用贝叶斯法则，找到在给定测量值 $y$ 的情况下，使状态向量 $x$ 的后验概率最大化的估计器。\n\n观测模型由 $y = Hx + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{m}$ 是观测值，$x \\in \\mathbb{R}^{n}$ 是未知状态向量，$H \\in \\mathbb{R}^{m \\times n}$ 是正向算子，$\\varepsilon \\in \\mathbb{R}^{m}$ 是测量噪声。\n\n根据贝叶斯法则，$x$ 在给定 $y$ 的条件下的后验概率密度函数 (PDF) 为：\n$$p(x|y) \\propto p(y|x) p(x)$$\n其中 $p(y|x)$ 是似然，$p(x)$ 是状态向量的先验 PDF。\n\nMAP 估计 $x_{\\mathrm{MAP}}$ 是使此后验概率最大化的 $x$ 的值。最大化 $p(x|y)$ 等同于最小化其负对数。我们将目标函数 $J(x)$ 定义为：\n$$J(x) = -\\ln(p(y|x)) - \\ln(p(x))$$\n因此，MAP 估计为 $x_{\\mathrm{MAP}} = \\arg\\min_x J(x)$。\n\n问题指定 $x$ 服从高斯先验，即 $x \\sim \\mathcal{N}(m, P)$。先验 PDF 为：\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x-m)^T P^{-1} (x-m)\\right)$$\n目标函数中相应的负对数先验项为：\n$$J_{\\mathrm{prior}}(x) = -\\ln(p(x)) = \\frac{1}{2}(x-m)^T P^{-1} (x-m) + \\mathrm{const.}$$\n\n似然项取决于噪声 $\\varepsilon$ 的假定分布。我们假设噪声分量 $\\varepsilon_i = y_i - (Hx)_i$ 是独立同分布的。那么，负对数似然是各个分量之和：\n$$J_{\\mathrm{likelihood}}(x) = -\\ln(p(y|x)) = -\\sum_{i=1}^{m} \\ln p(\\varepsilon_i) = -\\sum_{i=1}^{m} \\ln p(y_i - (Hx)_i)$$\n\n我们现在为两种指定的噪声模型推导估计器。\n\n**1. 高斯似然 MAP 估计器 ($x_{\\mathrm{G}}$)**\n\n在高斯噪声的假设下，每个分量 $\\varepsilon_i$ 的分布遵循 $p(\\varepsilon_i) \\propto \\exp\\left(-\\frac{\\varepsilon_i^2}{2\\sigma^2}\\right)$。负对数似然项是：\n$$J_{\\mathrm{G-like}}(x) = \\sum_{i=1}^{m} \\frac{(y_i - (Hx)_i)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}\\|y - Hx\\|_2^2$$\n为高斯 MAP 估计 $x_{\\mathrm{G}}$ 而需要最小化的总目标函数是：\n$$J_{\\mathrm{G}}(x) = \\frac{1}{2\\sigma^2}\\|y - Hx\\|_2^2 + \\frac{1}{2}(x-m)^T P^{-1} (x-m)$$\n这是一个关于 $x$ 的二次函数。通过将其关于 $x$ 的梯度设置为零来找到其最小值：\n$$\\nabla_x J_{\\mathrm{G}}(x) = \\frac{1}{\\sigma^2} H^T (Hx - y) + P^{-1}(x - m) = 0$$\n重新整理各项以求解 $x$ 可得：\n$$(H^T H + \\sigma^2 P^{-1}) x = H^T y + \\sigma^2 P^{-1} m$$\n这是一个线性方程组。其唯一解即为高斯 MAP 估计：\n$$x_{\\mathrm{G}} = (H^T H + \\sigma^2 P^{-1})^{-1} (H^T y + \\sigma^2 P^{-1} m)$$\n这是一个可以直接计算的闭式解。\n\n**2. 学生t似然 MAP 估计器 ($x_{\\mathrm{T}}$)**\n\n在学生t分布噪声的假设下，每个分量 $\\varepsilon_i$ 的 PDF 为 $p(\\varepsilon_i) \\propto \\left(1 + \\frac{\\varepsilon_i^2}{\\nu \\sigma^2}\\right)^{-(\\nu+1)/2}$。负对数似然项变为：\n$$J_{\\mathrm{T-like}}(x) = \\sum_{i=1}^{m} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - (Hx)_i)^2}{\\nu \\sigma^2}\\right)$$\n学生t MAP 估计 $x_{\\mathrm{T}}$ 的总目标函数是：\n$$J_{\\mathrm{T}}(x) = \\sum_{i=1}^{m} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - (Hx)_i)^2}{\\nu \\sigma^2}\\right) + \\frac{1}{2}(x-m)^T P^{-1} (x-m)$$\n该目标函数是凸的但非二次的，因此需要一种迭代方法来找到其最小值。我们再次将梯度设置为零：\n$$\\nabla_x J_{\\mathrm{T}}(x) = -\\sum_{i=1}^{m} \\left(\\frac{\\nu+1}{\\nu\\sigma^2 + (y_i - (Hx)_i)^2}\\right) (y_i - (Hx)_i) H_i^T + P^{-1}(x-m) = 0$$\n其中 $H_i$ 是 $H$ 的第 $i$ 行。我们定义一组依赖于当前 $x$ 估计值的权重 $w_i(x)$：\n$$w_i(x) = \\frac{\\nu+1}{\\nu\\sigma^2 + (y_i - (Hx)_i)^2}$$\n这些权重代表了每次测量的影响。具有大残差 $|y_i - (Hx)_i|$ 的离群值将获得较小的权重，这赋予了估计器鲁棒性。\n使用这些权重，梯度条件可以重写为矩阵形式，其中 $W(x)$ 是一个对角矩阵，其对角元为 $w_i(x)$：\n$$H^T W(x) (Hx - y) + P^{-1}(x - m) = 0$$\n$$(H^T W(x) H + P^{-1}) x = H^T W(x) y + P^{-1} m$$\n这个方程的结构提示了一种迭代重加权最小二乘 (IRLS) 算法。从一个初始猜测 $x^{(0)}$ 开始，我们可以对 $k=0, 1, 2, \\dots$ 进行如下迭代：\n1.  计算残差：$r^{(k)} = y - Hx^{(k)}$。\n2.  计算权重：$W^{(k)} = \\mathrm{diag}\\left(\\frac{\\nu+1}{\\nu\\sigma^2 + (r_i^{(k)})^2}\\right)$。\n3.  求解线性系统以获得下一个估计值 $x^{(k+1)}$：\n    $$x^{(k+1)} = (H^T W^{(k)} H + P^{-1})^{-1} (H^T W^{(k)} y + P^{-1} m)$$\n重复此迭代直到收敛，即直到变化量 $\\|x^{(k+1)} - x^{(k)}\\|_2$ 小于一个很小的容差。一个合适的初始猜测是高斯 MAP 估计，即 $x^{(0)} = x_{\\mathrm{G}}$。\n\n最终的程序实现了这两种估计器，并为指定的测试用例计算了所需的比较指标，从而证明了鲁棒的学生t模型在存在重尾噪声（离群值）时的优越性能。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares MAP estimates under Gaussian and Student-t likelihoods\n    for a linear inverse problem across four test cases.\n    \"\"\"\n    # 1. Construct the fixed problem instance\n    M, N = 8, 2\n    H = np.array([\n        [1.0, 0.5],\n        [0.8, -0.3],\n        [-0.6, 1.2],\n        [0.0, 1.0],\n        [1.5, -0.7],\n        [-1.0, -0.2],\n        [0.3, 0.8],\n        [-0.4, 0.5]\n    ])\n    x_star = np.array([1.0, -2.0])\n    m_prior = np.array([0.0, 0.0])\n    P_prior = np.diag([10.0, 10.0])\n    sigma = 1.0\n\n    # Pre-compute the inverse of the prior covariance matrix\n    P_inv = np.linalg.inv(P_prior)\n    H_T = H.T\n\n    # 2. Define the test suite\n    test_cases = [\n        # Case 1: Heavy-tailed residuals, moderate nu\n        {'nu': 3.0, 'r': np.array([0.2, -0.1, 0.15, -0.05, 0.1, -0.1, 12.0, -15.0])},\n        # Case 2: Near-Gaussian behavior\n        {'nu': 30.0, 'r': np.array([0.05, -0.02, 0.03, -0.04, 0.01, -0.01, 0.02, -0.03])},\n        # Case 3: Extremely heavy-tailed, multiple large outliers\n        {'nu': 1.5, 'r': np.array([20.0, -25.0, 0.2, 0.0, -0.1, 18.0, -22.0, 0.3])},\n        # Case 4: Baseline with no residuals\n        {'nu': 3.0, 'r': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}\n    ]\n\n    all_case_results = []\n\n    # 3. Compute results for each case\n    for case in test_cases:\n        nu = case['nu']\n        r = case['r']\n        \n        # Form the observation vector y\n        y = H @ x_star + r\n\n        # Compute the Gaussian-likelihood MAP estimate (x_G)\n        A_G = H_T @ H + (sigma**2) * P_inv\n        b_G = H_T @ y + (sigma**2) * P_inv @ m_prior\n        x_G = np.linalg.solve(A_G, b_G)\n\n        # Compute the Student-t-likelihood MAP estimate (x_T) using IRLS\n        # Initialize the iteration with the Gaussian estimate\n        x_T = np.copy(x_G)\n        num_iterations = 50  # Sufficient for convergence in this problem\n        \n        for _ in range(num_iterations):\n            residuals = y - H @ x_T\n            \n            # Calculate weights for the current estimate\n            weights_diag = (nu + 1) / (nu * sigma**2 + residuals**2)\n            W = np.diag(weights_diag)\n            \n            # Form and solve the linear system for the next estimate\n            A_T = H_T @ W @ H + P_inv\n            b_T = H_T @ W @ y + P_inv @ m_prior\n            \n            x_T_new = np.linalg.solve(A_T, b_T)\n            \n            # Check for convergence\n            if np.linalg.norm(x_T_new - x_T)  1e-12:\n                x_T = x_T_new\n                break\n            \n            x_T = x_T_new\n        \n        # Calculate the required metrics\n        bias_G_norm = np.linalg.norm(x_G - x_star)\n        bias_T_norm = np.linalg.norm(x_T - x_star)\n        \n        # The prior ensures the denominator is non-zero, but handle for robustness\n        if bias_T_norm == 0.0:\n            rho = float('inf')\n        else:\n            rho = bias_G_norm / bias_T_norm\n            \n        diff_norm = np.linalg.norm(x_G - x_T)\n        \n        single_case_result = [bias_G_norm, bias_T_norm, rho, diff_norm]\n        all_case_results.append(single_case_result)\n\n    # Format and print the final output string\n    # E.g., [[a1,b1,c1,d1],[a2,b2,c2,d2]] with no spaces\n    output_str = ','.join(str(res).replace(' ', '') for res in all_case_results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}