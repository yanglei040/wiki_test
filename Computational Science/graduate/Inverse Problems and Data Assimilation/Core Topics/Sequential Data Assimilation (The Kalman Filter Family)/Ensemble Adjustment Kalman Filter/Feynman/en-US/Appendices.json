{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Ensemble Adjustment Kalman Filter (EAKF), we begin with a foundational exercise. This practice demystifies the filter's core deterministic update mechanism in a simple, one-dimensional linear setting . By explicitly calculating the posterior mean and variance from first principles and then deriving the EAKF's anomaly adjustment, you will bridge the gap between abstract Bayesian theory and the concrete steps of an ensemble data assimilation cycle.",
            "id": "3378748",
            "problem": "Consider a one-dimensional linear-Gaussian data assimilation setting for a scalar state variable $x$ representing air temperature. You are given a forecast (prior) ensemble of size $N = 5$ with members (in Kelvin) \n$$\n\\{x_{1}^{f},x_{2}^{f},x_{3}^{f},x_{4}^{f},x_{5}^{f}\\}=\\{299.0,\\,300.5,\\,298.7,\\,301.2,\\,300.1\\}\\ \\mathrm{K}.\n$$\nThe observation operator is the identity, so the observation $y$ measures $x$ directly. A single observation $y$ with value $y=300.8\\ \\mathrm{K}$ is available, and the observation error is Gaussian with zero mean and known variance $R=0.25\\ \\mathrm{K}^{2}$. Assume the forecast ensemble is an unbiased Monte Carlo representation of a Gaussian prior distribution for $x$.\n\nStarting from first principles for linear-Gaussian inverse problems and data assimilation, namely Bayesâ€™ theorem specialized to linear-Gaussian models and the resulting Kalman filter characterization of the posterior mean and covariance, and using the defining property of the Ensemble Adjustment Kalman Filter (EAKF) that it applies a deterministic, mean-preserving linear map to the ensemble anomalies so that the analysis ensemble sample covariance matches the Kalman posterior covariance, carry out the following:\n\n- Derive the scalar posterior (analysis) mean and variance implied by the linear-Gaussian Bayesian update for this setting.\n- From the requirement that the EAKF analysis ensemble must exactly match these posterior first and second moments in the one-dimensional case, determine the deterministic adjustment applied to each ensemble anomaly.\n- Apply this adjustment to compute the EAKF-updated value of the fourth ensemble member $x_{4}^{a}$.\n\nUse the unbiased finite-ensemble sample covariance with divisor $N-1$ when estimating the forecast covariance from the given ensemble. Express your final numerical answer for $x_{4}^{a}$ in Kelvin and round your answer to four significant figures.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in the field of data assimilation, providing all necessary information for a unique solution.\n\nThe problem asks for the updated value of a specific ensemble member using the Ensemble Adjustment Kalman Filter (EAKF). The solution requires first deriving the theoretical posterior mean and variance from the linear-Gaussian Bayesian update, and then using the defining properties of the EAKF to determine the transformation applied to the ensemble.\n\n**1. Bayesian Framework for Linear-Gaussian Systems**\n\nThe problem is set in a linear-Gaussian framework. The prior distribution of the state variable $x$ is assumed to be Gaussian, represented by the forecast ensemble statistics:\n$$x \\sim \\mathcal{N}(\\bar{x}^f, P^f)$$\nwhere $\\bar{x}^f$ is the forecast mean and $P^f$ is the forecast error covariance (a scalar variance in this one-dimensional case).\n\nThe observation model is linear with additive Gaussian noise:\n$$y = Hx + v, \\quad v \\sim \\mathcal{N}(0, R)$$\nwhere $y$ is the observation, $H$ is the observation operator, $v$ is the observation error, and $R$ is the observation error variance. From this, the likelihood function $p(y|x)$ is also Gaussian:\n$$y|x \\sim \\mathcal{N}(Hx, R)$$\n\nAccording to Bayes' theorem, the posterior distribution $p(x|y)$ is proportional to the product of the likelihood and the prior, $p(x|y) \\propto p(y|x)p(x)$. For this linear-Gaussian system, the posterior is also a Gaussian distribution:\n$$x|y \\sim \\mathcal{N}(\\bar{x}^a, P^a)$$\nThe posterior (or analysis) mean $\\bar{x}^a$ and variance $P^a$ are given by the standard Kalman filter update equations.\n\n**2. Derivation of Posterior Mean and Variance**\n\nFirst, we estimate the prior moments, the forecast mean $\\bar{x}^f$ and variance $P^f$, from the given forecast ensemble $\\{x_{1}^{f},x_{2}^{f},x_{3}^{f},x_{4}^{f},x_{5}^{f}\\}=\\{299.0, 300.5, 298.7, 301.2, 300.1\\}$. The ensemble size is $N=5$.\n\nThe forecast mean is the sample mean of the ensemble:\n$$\n\\bar{x}^f = \\frac{1}{N} \\sum_{i=1}^{N} x_i^f = \\frac{1}{5}(299.0 + 300.5 + 298.7 + 301.2 + 300.1) = \\frac{1499.5}{5} = 299.9\\ \\mathrm{K}\n$$\n\nThe forecast variance is the unbiased sample variance, using a divisor of $N-1$:\n$$\nP^f = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i^f - \\bar{x}^f)^2\n$$\nThe squared anomalies $(x_i^f - \\bar{x}^f)^2$ are:\n$(299.0 - 299.9)^2 = (-0.9)^2 = 0.81$\n$(300.5 - 299.9)^2 = (0.6)^2 = 0.36$\n$(298.7 - 299.9)^2 = (-1.2)^2 = 1.44$\n$(301.2 - 299.9)^2 = (1.3)^2 = 1.69$\n$(300.1 - 299.9)^2 = (0.2)^2 = 0.04$\n$$\nP^f = \\frac{1}{4}(0.81 + 0.36 + 1.44 + 1.69 + 0.04) = \\frac{4.34}{4} = 1.085\\ \\mathrm{K}^2\n$$\n\nNow, we apply the Kalman filter update equations. The observation operator is identity, so in this scalar case, $H=1$. The observation value is $y=300.8\\ \\mathrm{K}$ and its error variance is $R=0.25\\ \\mathrm{K}^2$.\n\nThe Kalman gain $K$ is:\n$$\nK = P^f H^T (H P^f H^T + R)^{-1} = P^f (P^f + R)^{-1} = \\frac{1.085}{1.085 + 0.25} = \\frac{1.085}{1.335}\n$$\n\nThe analysis mean $\\bar{x}^a$ is:\n$$\n\\bar{x}^a = \\bar{x}^f + K(y - H\\bar{x}^f) = 299.9 + \\frac{1.085}{1.335}(300.8 - 299.9) = 299.9 + \\frac{1.085}{1.335}(0.9)\n$$\n$$\n\\bar{x}^a = 299.9 + \\frac{0.9765}{1.335} \\approx 299.9 + 0.731460674 \\approx 300.63146\\ \\mathrm{K}\n$$\n\nThe analysis variance $P^a$ is:\n$$\nP^a = (I - KH)P^f = (1 - K)P^f = \\left(1 - \\frac{1.085}{1.335}\\right) \\times 1.085 = \\frac{0.25}{1.335} \\times 1.085 = \\frac{0.27125}{1.335}\n$$\n$$\nP^a \\approx 0.20318352\\ \\mathrm{K}^2\n$$\n\n**3. Ensemble Adjustment Kalman Filter (EAKF) Update**\n\nThe EAKF updates each forecast ensemble member $x_i^f$ to an analysis member $x_i^a$ via a deterministic linear transformation that preserves the mean and adjusts the anomalies:\n$$\nx_i^a = \\bar{x}^a + \\alpha (x_i^f - \\bar{x}^f)\n$$\nwhere $\\bar{x}^a$ is the analysis mean calculated above. The scaling factor $\\alpha$ is chosen such that the sample variance of the resulting analysis ensemble $\\{x_i^a\\}$ matches the theoretical analysis variance $P^a$.\n\nThe sample variance of the analysis ensemble is:\n$$\nP^a_{\\text{ens}} = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i^a - \\bar{x}^a)^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\alpha(x_i^f - \\bar{x}^f))^2 = \\alpha^2 \\left(\\frac{1}{N-1} \\sum_{i=1}^{N} (x_i^f - \\bar{x}^f)^2\\right) = \\alpha^2 P^f\n$$\nSetting $P^a_{\\text{ens}} = P^a$, we get:\n$$\n\\alpha^2 P^f = P^a \\implies \\alpha = \\sqrt{\\frac{P^a}{P^f}}\n$$\nSubstituting $P^a = (1-K)P^f$:\n$$\n\\alpha = \\sqrt{\\frac{(1-K)P^f}{P^f}} = \\sqrt{1-K}\n$$\nNow we compute the value of $\\alpha$:\n$$\n\\alpha = \\sqrt{1 - \\frac{1.085}{1.335}} = \\sqrt{\\frac{1.335 - 1.085}{1.335}} = \\sqrt{\\frac{0.25}{1.335}} \\approx 0.43274233\n$$\n\n**4. Computation of the Updated Fourth Ensemble Member**\n\nWe now apply the EAKF update rule to compute the analysis value for the fourth ensemble member, $x_4^a$.\nThe given forecast member is $x_4^f = 301.2\\ \\mathrm{K}$.\nThe forecast anomaly for this member is $x_4^f - \\bar{x}^f = 301.2 - 299.9 = 1.3\\ \\mathrm{K}$.\n\nThe updated value $x_4^a$ is:\n$$\nx_4^a = \\bar{x}^a + \\alpha (x_4^f - \\bar{x}^f)\n$$\nUsing the previously calculated values:\n$$\nx_4^a \\approx 300.63146067 + (0.43274233 \\times 1.3)\n$$\n$$\nx_4^a \\approx 300.63146067 + 0.56256503\n$$\n$$\nx_4^a \\approx 301.1940257\\ \\mathrm{K}\n$$\n\nThe problem requires the answer to be rounded to four significant figures. The number is $301.194...$. The first four significant figures are $3, 0, 1, 1$. The fifth digit is $9$, so we round the fourth significant figure up.\n$$\nx_4^a \\approx 301.2\\ \\mathrm{K}\n$$",
            "answer": "$$\\boxed{301.2}$$"
        },
        {
            "introduction": "Having mastered the linear case, we now introduce a common real-world complexity: nonlinear observation operators. This practice reveals a subtle but critical limitation of sequential EAKF, where the final analysis can depend on the order in which observations are processed . Working through this carefully constructed counterexample will illuminate how finite-ensemble statistics lead to state-dependent linearizations, providing deep insight into the filter's behavior and motivating practical mitigation strategies.",
            "id": "3378631",
            "problem": "Consider a scalar state variable $x \\in \\mathbb{R}$ with a prior finite ensemble of size $N=3$ given by the members $x^{(1)}=-2$, $x^{(2)}=1$, and $x^{(3)}=1$. Assume additive, independent, Gaussian observation errors. Two observations are available:\n- The first is $y_{1}$ with observation function $h_{1}(x)=x$ and error variance $R_{1}=1$.\n- The second is $y_{2}$ with observation function $h_{2}(x)=x^{2}$ and error variance $R_{2}=1$.\n\nAssume the realized observation values equal the prior ensemble mean of the corresponding observation functions so that the innovation is zero for both $y_{1}$ and $y_{2}$, and only the ensemble anomalies are adjusted.\n\nThe assimilation is performed with the Ensemble Adjustment Kalman Filter (EAKF). For each scalar observation $y$ whose observation function is $z=h(x)$ applied to each ensemble member to produce $\\{z^{(i)}\\}_{i=1}^{N}$, the EAKF deterministically adjusts the observation-space anomalies so that the posterior variance of $z$ agrees with the scalar Kalman posterior variance, and then maps those adjusted observation-space anomalies back to state-space anomalies using the least-squares linear regression slope computed from the finite ensemble, $b=\\operatorname{cov}(x,z)/\\operatorname{var}(z)$. Work from the following bases:\n- The scalar linear-Gaussian posterior variance formula, $\\operatorname{var}(z \\mid y)=\\left(\\operatorname{var}(z)^{-1}+R^{-1}\\right)^{-1}=\\frac{\\operatorname{var}(z)\\,R}{\\operatorname{var}(z)+R}$.\n- The sample mean, variance, and covariance definitions, $\\overline{u}=\\frac{1}{N}\\sum_{i=1}^{N}u^{(i)}$, $\\operatorname{var}(u)=\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(u^{(i)}-\\overline{u}\\right)^{2}$, $\\operatorname{cov}(u,v)=\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(u^{(i)}-\\overline{u}\\right)\\left(v^{(i)}-\\overline{v}\\right)$.\n\nTask:\n1. Process the two observations in the order $y_{1}$ then $y_{2}$, using the EAKF anomaly-adjustment logic described above. Compute the resulting posterior ensemble variance of the state $x$.\n2. Process the two observations in the reverse order, $y_{2}$ then $y_{1}$, and compute the resulting posterior ensemble variance of $x$.\n3. Provide the exact closed-form expression for the difference $\\Delta=\\operatorname{var}^{\\text{post}}_{x}\\big|_{y_{1}\\rightarrow y_{2}}-\\operatorname{var}^{\\text{post}}_{x}\\big|_{y_{2}\\rightarrow y_{1}}$, simplified as a single rational number. No rounding is required.\n4. Briefly explain why the two posterior variances differ for this finite ensemble and discuss at least two mitigation strategies that can reduce or eliminate such order dependence in practice.\n\nYour final answer must be the exact value of $\\Delta$ as a single simplified fraction.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and based on established principles of the Ensemble Adjustment Kalman Filter (EAKF) in data assimilation. All necessary data, definitions, and conditions are provided, and there are no internal contradictions. Therefore, the problem is valid and a solution can be computed.\n\nThe task is to compute the final posterior state variance after assimilating two observations, $y_1$ and $y_2$, in two different orders, find the difference in these variances, and explain the result. The ensemble size is $N=3$. The sample mean, variance, and covariance are defined as:\n$$ \\overline{u}=\\frac{1}{N}\\sum_{i=1}^{N}u^{(i)} $$\n$$ \\operatorname{var}(u)=\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(u^{(i)}-\\overline{u}\\right)^{2} $$\n$$ \\operatorname{cov}(u,v)=\\frac{1}{N-1}\\sum_{i=1}^{N}\\left(u^{(i)}-\\overline{u}\\right)\\left(v^{(i)}-\\overline{v}\\right) $$\n\nThe initial prior ensemble for the state $x$ is $\\{x^{(1)}, x^{(2)}, x^{(3)}\\} = \\{-2, 1, 1\\}$.\nThe initial prior mean is $\\bar{x} = \\frac{1}{3}(-2+1+1) = 0$.\nThe initial prior variance is $\\operatorname{var}(x) = \\frac{1}{3-1}\\left((-2-0)^2 + (1-0)^2 + (1-0)^2\\right) = \\frac{1}{2}(4+1+1) = 3$.\n\nThe problem states that innovation is zero, so the posterior mean will equal the prior mean at each step. We only need to update the ensemble anomalies.\n\n**1. Processing order $y_1$ then $y_2$**\n\n**Step 1: Assimilate $y_1$**\nThe first observation is $y_1$ with observation function $h_1(x)=x$ and error variance $R_1=1$.\nMap the prior ensemble to observation space: $\\{z_1^{(i)}\\} = \\{h_1(x^{(i)})\\} = \\{-2, 1, 1\\}$.\nThe statistics for $z_1$ are identical to those for $x$:\n$\\bar{z}_1 = \\bar{x} = 0$.\n$\\operatorname{var}(z_1) = \\operatorname{var}(x) = 3$.\n\nCalculate the posterior variance in observation space using the scalar Kalman formula:\n$$ \\operatorname{var}(z_1|y_1) = \\frac{\\operatorname{var}(z_1) R_1}{\\operatorname{var}(z_1) + R_1} = \\frac{3 \\cdot 1}{3+1} = \\frac{3}{4} $$\nThe EAKF shrinks the prior observation anomalies by a factor $\\alpha_1$:\n$$ \\alpha_1 = \\sqrt{\\frac{\\operatorname{var}(z_1|y_1)}{\\operatorname{var}(z_1)}} = \\sqrt{\\frac{3/4}{3}} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2} $$\nThe adjusted observation anomalies are $z_1'^{\\text{post}(i)} = \\alpha_1 z_1'^{\\text{prior}(i)}$.\nTo update the state anomalies, we compute the linear regression slope $b_1$:\n$$ b_1 = \\frac{\\operatorname{cov}(x, z_1)}{\\operatorname{var}(z_1)} = \\frac{\\operatorname{cov}(x, x)}{\\operatorname{var}(x)} = 1 $$\nThe posterior state anomalies are found by applying an update based on the change in observation-space anomalies. The update is $x'^{\\text{post}(i)} = x'^{\\text{prior}(i)} + b_1(z_1'^{\\text{post}(i)} - z_1'^{\\text{prior}(i)})$. Since $z_1=x$, we have $z_1' = x'$, so this becomes $x'^{\\text{post}(i)} = x'^{\\text{prior}(i)} + 1 \\cdot (\\alpha_1 x'^{\\text{prior}(i)} - x'^{\\text{prior}(i)}) = \\alpha_1 x'^{\\text{prior}(i)}$.\nThe prior state anomalies are $\\{x'^{(1)}, x'^{(2)}, x'^{(3)}\\} = \\{-2, 1, 1\\}$.\nThe posterior state anomalies are:\n$x'^{\\text{post}(1)} = \\frac{1}{2}(-2) = -1$\n$x'^{\\text{post}(2)} = \\frac{1}{2}(1) = \\frac{1}{2}$\n$x'^{\\text{post}(3)} = \\frac{1}{2}(1) = \\frac{1}{2}$\nThe new ensemble for $x$ (which is the prior for the next step) is $\\{x^{(i)}\\} = \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$, with mean $\\bar{x}=0$.\nThe variance of this intermediate ensemble is $\\operatorname{var}(x) = \\frac{1}{2}((-1)^2 + (\\frac{1}{2})^2 + (\\frac{1}{2})^2) = \\frac{1}{2}(1+\\frac{1}{4}+\\frac{1}{4}) = \\frac{3}{4}$.\n\n**Step 2: Assimilate $y_2$**\nThe second observation is $y_2$ with $h_2(x)=x^2$ and $R_2=1$. The prior ensemble is $x \\in \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$.\nMap this ensemble to observation space: $\\{z_2^{(i)}\\} = \\{h_2(x^{(i)})\\} = \\{(-1)^2, (\\frac{1}{2})^2, (\\frac{1}{2})^2\\} = \\{1, \\frac{1}{4}, \\frac{1}{4}\\}$.\nCalculate statistics for this step's prior:\n$\\bar{x} = 0$, $\\operatorname{var}(x) = \\frac{3}{4}$.\n$\\bar{z}_2 = \\frac{1}{3}(1+\\frac{1}{4}+\\frac{1}{4}) = \\frac{1}{2}$.\n$\\operatorname{var}(z_2) = \\frac{1}{2}((1-\\frac{1}{2})^2 + (\\frac{1}{4}-\\frac{1}{2})^2 + (\\frac{1}{4}-\\frac{1}{2})^2) = \\frac{1}{2}((\\frac{1}{2})^2 + (-\\frac{1}{4})^2 + (-\\frac{1}{4})^2) = \\frac{1}{2}(\\frac{1}{4}+\\frac{1}{16}+\\frac{1}{16}) = \\frac{3}{16}$.\n$\\operatorname{cov}(x, z_2) = \\frac{1}{2}\\left( (-1-0)(1-\\frac{1}{2}) + (\\frac{1}{2}-0)(\\frac{1}{4}-\\frac{1}{2}) + (\\frac{1}{2}-0)(\\frac{1}{4}-\\frac{1}{2}) \\right) = \\frac{1}{2}(-\\frac{1}{2} - \\frac{1}{8} - \\frac{1}{8}) = -\\frac{3}{8}$.\n\nCalculate the posterior variance in observation space:\n$$ \\operatorname{var}(z_2|y_2) = \\frac{\\operatorname{var}(z_2) R_2}{\\operatorname{var}(z_2) + R_2} = \\frac{3/16 \\cdot 1}{3/16+1} = \\frac{3/16}{19/16} = \\frac{3}{19} $$\nThe shrinking factor is $\\alpha_2 = \\sqrt{\\frac{\\operatorname{var}(z_2|y_2)}{\\operatorname{var}(z_2)}} = \\sqrt{\\frac{3/19}{3/16}} = \\sqrt{\\frac{16}{19}} = \\frac{4}{\\sqrt{19}}$.\nThe regression slope is $b_2 = \\frac{\\operatorname{cov}(x, z_2)}{\\operatorname{var}(z_2)} = \\frac{-3/8}{3/16} = -2$.\n\nThe final posterior state anomalies are $x'^{\\text{post}(i)} = x'^{\\text{prior}(i)} + b_2(\\alpha_2 - 1)z_2'^{\\text{prior}(i)}$.\nPrior anomalies: $x' \\in \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$, $z_2' \\in \\{\\frac{1}{2}, -\\frac{1}{4}, -\\frac{1}{4}\\}$.\n$x'^{\\text{post}(1)} = -1 + (-2)(\\frac{4}{\\sqrt{19}}-1)(\\frac{1}{2}) = -1 - (\\frac{4}{\\sqrt{19}}-1) = -\\frac{4}{\\sqrt{19}}$.\n$x'^{\\text{post}(2)} = \\frac{1}{2} + (-2)(\\frac{4}{\\sqrt{19}}-1)(-\\frac{1}{4}) = \\frac{1}{2} + \\frac{1}{2}(\\frac{4}{\\sqrt{19}}-1) = \\frac{2}{\\sqrt{19}}$.\n$x'^{\\text{post}(3)} = \\frac{1}{2} + (-2)(\\frac{4}{\\sqrt{19}}-1)(-\\frac{1}{4}) = \\frac{2}{\\sqrt{19}}$.\nThe final posterior variance is:\n$$ \\operatorname{var}^{\\text{post}}_{x}\\big|_{y_{1}\\rightarrow y_{2}} = \\frac{1}{2}\\left( (-\\frac{4}{\\sqrt{19}})^2 + (\\frac{2}{\\sqrt{19}})^2 + (\\frac{2}{\\sqrt{19}})^2 \\right) = \\frac{1}{2}\\left(\\frac{16}{19} + \\frac{4}{19} + \\frac{4}{19}\\right) = \\frac{12}{19} $$\n\n**2. Processing order $y_2$ then $y_1$**\n\n**Step 1: Assimilate $y_2$**\nThe prior is the initial ensemble $x \\in \\{-2, 1, 1\\}$, with $\\bar{x}=0, \\operatorname{var}(x)=3$.\nObservation function $h_2(x)=x^2, R_2=1$.\nMap to observation space: $\\{z_2^{(i)}\\} = \\{h_2(x^{(i)})\\} = \\{(-2)^2, 1^2, 1^2\\} = \\{4, 1, 1\\}$.\nCalculate statistics:\n$\\bar{z}_2 = \\frac{1}{3}(4+1+1) = 2$.\n$\\operatorname{var}(z_2) = \\frac{1}{2}((4-2)^2 + (1-2)^2 + (1-2)^2) = \\frac{1}{2}(4+1+1) = 3$.\n$\\operatorname{cov}(x, z_2) = \\frac{1}{2}\\left( (-2-0)(4-2) + (1-0)(1-2) + (1-0)(1-2) \\right) = \\frac{1}{2}(-4-1-1) = -3$.\n\nPosterior variance in observation space:\n$$ \\operatorname{var}(z_2|y_2) = \\frac{\\operatorname{var}(z_2) R_2}{\\operatorname{var}(z_2) + R_2} = \\frac{3 \\cdot 1}{3+1} = \\frac{3}{4} $$\nShrinking factor $\\alpha_2 = \\sqrt{\\frac{3/4}{3}} = \\frac{1}{2}$. Regression slope $b_2 = \\frac{-3}{3}=-1$.\nUpdate state anomalies: $x'^{\\text{post}(i)} = x'^{\\text{prior}(i)} + b_2(\\alpha_2 - 1)z_2'^{\\text{prior}(i)}$.\nPrior anomalies: $x' \\in \\{-2, 1, 1\\}$, $z_2' \\in \\{2, -1, -1\\}$.\n$x'^{\\text{post}(1)} = -2 + (-1)(\\frac{1}{2}-1)(2) = -2 + (-1)(-\\frac{1}{2})(2) = -2+1 = -1$.\n$x'^{\\text{post}(2)} = 1 + (-1)(\\frac{1}{2}-1)(-1) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n$x'^{\\text{post}(3)} = 1 + (-1)(\\frac{1}{2}-1)(-1) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\nThe intermediate ensemble is $x \\in \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$, with $\\bar{x}=0$ and variance $\\operatorname{var}(x) = \\frac{3}{4}$.\n\n**Step 2: Assimilate $y_1$**\nThe prior is $x \\in \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$ with $\\operatorname{var(x)}=\\frac{3}{4}$. Observation is $h_1(x)=x, R_1=1$.\nMap to observation space: $z_1 \\in \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$.\n$\\bar{z}_1 = 0$, $\\operatorname{var}(z_1) = \\frac{3}{4}$.\nPosterior variance in observation space:\n$$ \\operatorname{var}(z_1|y_1) = \\frac{\\operatorname{var}(z_1) R_1}{\\operatorname{var}(z_1) + R_1} = \\frac{3/4 \\cdot 1}{3/4+1} = \\frac{3/4}{7/4} = \\frac{3}{7} $$\nShrinking factor $\\alpha_1 = \\sqrt{\\frac{3/7}{3/4}} = \\sqrt{\\frac{4}{7}} = \\frac{2}{\\sqrt{7}}$.\nThe update to state anomalies is $x'^{\\text{post}(i)} = \\alpha_1 x'^{\\text{prior}(i)}$.\nPrior anomalies: $x' \\in \\{-1, \\frac{1}{2}, \\frac{1}{2}\\}$.\n$x'^{\\text{post}(1)} = \\frac{2}{\\sqrt{7}}(-1) = -\\frac{2}{\\sqrt{7}}$.\n$x'^{\\text{post}(2)} = \\frac{2}{\\sqrt{7}}(\\frac{1}{2}) = \\frac{1}{\\sqrt{7}}$.\n$x'^{\\text{post}(3)} = \\frac{2}{\\sqrt{7}}(\\frac{1}{2}) = \\frac{1}{\\sqrt{7}}$.\nThe final posterior variance is:\n$$ \\operatorname{var}^{\\text{post}}_{x}\\big|_{y_{2}\\rightarrow y_{1}} = \\frac{1}{2}\\left( (-\\frac{2}{\\sqrt{7}})^2 + (\\frac{1}{\\sqrt{7}})^2 + (\\frac{1}{\\sqrt{7}})^2 \\right) = \\frac{1}{2}\\left(\\frac{4}{7} + \\frac{1}{7} + \\frac{1}{7}\\right) = \\frac{3}{7} $$\n\n**3. Difference in Posterior Variances**\nThe difference is $\\Delta = \\operatorname{var}^{\\text{post}}_{x}\\big|_{y_{1}\\rightarrow y_{2}}-\\operatorname{var}^{\\text{post}}_{x}\\big|_{y_{2}\\rightarrow y_{1}}$.\n$$ \\Delta = \\frac{12}{19} - \\frac{3}{7} = \\frac{12 \\cdot 7 - 3 \\cdot 19}{19 \\cdot 7} = \\frac{84 - 57}{133} = \\frac{27}{133} $$\n\n**4. Explanation and Mitigation**\n\nThe posterior variances differ because the Ensemble Adjustment Kalman Filter, when applied sequentially with a finite ensemble and a nonlinear observation operator, is not invariant to the order of observation processing. The core reason lies in the state-dependent linearization of the observation operator. The filter approximates the effect of an observation on the state via a linear regression, whose coefficient $b=\\operatorname{cov}(x,z)/\\operatorname{var}(z)$ is calculated from the current ensemble.\n\nIn the case of $y_1 \\rightarrow y_2$: The linear observation $y_1$ first reduces the variance of the ensemble. The subsequent assimilation of the nonlinear observation $y_2$ (with $h_2(x)=x^2$) uses a regression coefficient calculated from this already-shrunk ensemble ($\\{-1, 1/2, 1/2\\}$), for which we found $b_2 = -2$.\n\nIn the case of $y_2 \\rightarrow y_1$: The nonlinear observation $y_2$ is assimilated first. The regression coefficient is calculated from the original, wider ensemble ($\\{-2, 1, 1\\}$), yielding a different coefficient, $b_2 = -1$. This different linearization leads to a different intermediate ensemble, and although the linear update in the second step is path-independent in its form, its starting point is different, leading to a different final posterior.\n\nThis order dependence is a direct consequence of using a finite ensemble to estimate the cross-covariance for updating the state in the presence of nonlinearity. The true Bayesian update is order-independent, but the EAKF is an approximation.\n\nTwo mitigation strategies to reduce or eliminate this order dependence are:\n1.  **Batch Assimilation**: Process all observations simultaneously. This involves augmenting the observation vector to $Y = [y_1, y_2]^T$ and the observation operator to $H(x)=[h_1(x), h_2(x)]^T$. The EAKF update is then performed once using matrix operations for the observation-space covariances and regression coefficients. This mathematically eliminates order dependence by treating the observations as a single block.\n2.  **Increasing Ensemble Size**: The order-dependence effect is exacerbated by sampling error from the small ensemble size ($N=3$). A much larger ensemble would provide more stable and accurate estimates of the covariance and regression statistics. As $N \\rightarrow \\infty$, the sample covariances converge to the true covariances of the underlying distribution, and the filter's behavior approaches that of the exact Kalman filter (for linear problems) or becomes a more accurate Monte Carlo approximation (for nonlinear problems), reducing the practical impact of order dependence.",
            "answer": "$$\\boxed{\\frac{27}{133}}$$"
        },
        {
            "introduction": "We next confront a fundamental challenge in large-scale applications: the \"curse of dimensionality,\" where the ensemble size $N$ is dwarfed by the state space dimension. This exercise explores the mathematical consequences of the resulting rank-deficient prior covariance, a situation where the ensemble cannot represent all possible error structures . By analyzing the filter's update within the constrained ensemble subspace, you will quantify the filter's inability to reduce error outside this subspace, a key concept for understanding the behavior of EAKF in high-dimensional systems.",
            "id": "3378735",
            "problem": "Consider a linear inverse problem in data assimilation with a state vector $\\;x \\in \\mathbb{R}^{n}\\;$ and an observation operator $\\;H \\in \\mathbb{R}^{m \\times n}\\;$ of rank $\\;m\\;$. Observations are given by $\\;y = H x + e\\;$ with $\\;e \\sim \\mathcal{N}(0,R)\\;$ and $\\;R \\in \\mathbb{R}^{m \\times m}\\;$ symmetric positive definite. An ensemble of size $\\;N\\;$ defines a prior mean $\\;\\bar{x}^{f}\\;$ and a sample prior covariance $\\;P^{f}\\;$ of rank $\\;r = N - 1\\;$. Assume $\\;r < m\\;$ so the prior ensemble anomalies do not span the full observed space.\n\nThe Ensemble Adjustment Kalman Filter (EAKF) performs a deterministic update that adjusts ensemble anomalies to achieve a posterior covariance consistent with linear Gaussian Bayesian analysis within the ensemble subspace. Define the prior observation-space covariance $\\;S = H P^{f} H^{\\top}\\;$ and the whitened matrix $\\;\\tilde{S} = R^{-1/2} S R^{-1/2}\\;$. Let $\\;\\tilde{S}\\;$ have the eigen-decomposition $\\;\\tilde{S} = Q \\Lambda Q^{\\top}\\;$ with $\\;Q \\in \\mathbb{R}^{m \\times m}\\;$ orthonormal and $\\;\\Lambda = \\mathrm{diag}(\\lambda_{1},\\ldots,\\lambda_{m})\\;$, where the last $\\;m-r\\;$ eigenvalues are zero due to rank deficiency.\n\nStarting from the linear Gaussian least-squares formulation of the Bayesian analysis restricted to the ensemble subspace and the associated normal equations, derive the minimal attainable analysis variance along observed directions and show how rank deficiency bounds the reduction of the innovation variance. Then, quantify the residual misfit norm $\\;\\|y - H \\bar{x}_{a}\\|_{R^{-1}}^{2}\\;$, where $\\;\\bar{x}_{a}\\;$ is the EAKF analysis mean, in terms of $\\;\\Lambda\\;$ and the whitened innovation vector $\\;\\tilde{d} = R^{-1/2}(y - H \\bar{x}^{f})\\;$.\n\nFinally, evaluate these quantities for the specific case $\\;m = 3\\;$, $\\;r = 2\\;$ with $\\;Q = I_{3}\\;$ and $\\;\\Lambda = \\mathrm{diag}(4,1,0)\\;$. Take the whitened innovation vector $\\;\\tilde{d} = [2,\\,-1,\\,3]^{\\top}\\;$. Report:\n- the numerical value of $\\;\\|y - H \\bar{x}_{a}\\|_{R^{-1}}^{2}\\;$, and\n- the minimal attainable average analysis variance in the observed space measured by $\\;\\mathrm{tr}\\big((I + \\tilde{S})^{-1}\\big)\\;$.\n\nRound both requested numerical quantities to four significant figures. Both answers are dimensionless; do not include units.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n- State vector: $x \\in \\mathbb{R}^{n}$\n- Observation operator: $H \\in \\mathbb{R}^{m \\times n}$ with rank $m$\n- Observation model: $y = H x + e$, where $e$ is the observation error\n- Observation error distribution: $e \\sim \\mathcal{N}(0,R)$, where $R \\in \\mathbb{R}^{m \\times m}$ is symmetric and positive definite.\n- Prior forecast mean: $\\bar{x}^{f}$\n- Sample prior forecast covariance: $P^{f}$ with rank $r = N - 1$, where $N$ is the ensemble size.\n- Rank condition: $r < m$\n- Prior observation-space covariance: $S = H P^{f} H^{\\top}$\n- Whitened prior observation-space covariance: $\\tilde{S} = R^{-1/2} S R^{-1/2}$\n- Eigen-decomposition of $\\tilde{S}$: $\\tilde{S} = Q \\Lambda Q^{\\top}$, with $Q \\in \\mathbb{R}^{m \\times m}$ being orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_{1},\\ldots,\\lambda_{m})$.\n- Eigenvalue property due to rank deficiency: The last $m-r$ eigenvalues in $\\Lambda$ are zero.\n- Analysis mean: $\\bar{x}_{a}$\n- Whitened innovation vector: $\\tilde{d} = R^{-1/2}(y - H \\bar{x}^{f})$\n- Specific case parameters: $m = 3$, $r = 2$, $Q = I_{3}$, $\\Lambda = \\mathrm{diag}(4,1,0)$, and $\\tilde{d} = [2,\\,-1,\\,3]^{\\top}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the framework of linear inverse problems and data assimilation. The formulation uses standard definitions from ensemble Kalman filtering. The givens are internally consistent; for instance, the condition $m=3$ and $r=2$ implies $m-r=1$ zero eigenvalue for $S$ and $\\tilde{S}$, which is consistent with the provided $\\Lambda = \\mathrm{diag}(4,1,0)$. The problem is scientifically grounded, objective, and contains all necessary information to proceed to a solution. There are no contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be developed.\n\n### Derivations\n\nThe analysis state in a linear-Gaussian setting is found by minimizing the cost function $J(x) = (x - \\bar{x}^f)^\\top (P^f)^{-1} (x - \\bar{x}^f) + (y - Hx)^\\top R^{-1} (y - Hx)$. The solution for the mean state, known as the Best Linear Unbiased Estimator (BLUE), is given by the standard Kalman filter update equations.\n\nThe analysis mean $\\bar{x}_{a}$ is:\n$$ \\bar{x}_{a} = \\bar{x}^{f} + K (y - H\\bar{x}^{f}) $$\nwhere $K$ is the Kalman gain. Since $P^f$ is rank-deficient, we use the form of $K$ that does not require $(P^f)^{-1}$:\n$$ K = P^{f}H^{\\top}(H P^{f} H^{\\top} + R)^{-1} = P^{f}H^{\\top}(S + R)^{-1} $$\nThe analysis error covariance matrix $P_a = \\mathrm{Cov}(\\bar{x}_a - x_{\\text{true}})$ is:\n$$ P_{a} = (I - KH)P^{f} = P^{f} - P^{f}H^{\\top}(S+R)^{-1}HP^{f} $$\nThe problem asks for the analysis variance along observed directions, which corresponds to the analysis error covariance projected into the observation space, $H P_{a} H^{\\top}$.\n$$ H P_{a} H^{\\top} = H(P^{f} - P^{f}H^{\\top}(S+R)^{-1}HP^{f})H^{\\top} = S - S(S+R)^{-1}S $$\nUsing the identity $A - A(A+B)^{-1}A = A(A+B)^{-1}B$, we can simplify this to:\n$$ H P_{a} H^{\\top} = S(S+R)^{-1}R $$\nTo analyze this in terms of the given whitened quantities, we transform this expression. Let $\\tilde{P}_a^{\\text{obs}} = R^{-1/2}(H P_a H^\\top)R^{-1/2}$.\n$$ \\tilde{P}_a^{\\text{obs}} = R^{-1/2} [S(S+R)^{-1}R] R^{-1/2} = (R^{-1/2} S R^{-1/2}) (R^{1/2}(S+R)^{-1}R^{1/2}) $$\nThe second term is $(R^{-1/2}(S+R)R^{-1/2})^{-1} = (\\tilde{S}+I)^{-1}$. So,\n$$ \\tilde{P}_a^{\\text{obs}} = \\tilde{S}(\\tilde{S}+I)^{-1} $$\nThe eigenvalues of this whitened analysis error covariance matrix in observation space are $\\frac{\\lambda_i}{1+\\lambda_i}$. These represent the minimal attainable analysis error variance for each mode, as a fraction of the observation error variance.\n\nNext, we address the reduction of the innovation variance. The innovation vector is $d = y - H\\bar{x}^f$. The analysis residual is $d_a = y - H\\bar{x}_a$.\n$$ d_a = y - H(\\bar{x}^f + K d) = d - HKd = (I - HK)d $$\nSubstituting $K=P^fH^\\top(S+R)^{-1}$ and $S=HP^fH^\\top$:\n$$ d_a = (I - S(S+R)^{-1})d $$\nUsing the identity $I - A(A+B)^{-1} = B(A+B)^{-1}$, we get:\n$$ d_a = R(S+R)^{-1}d $$\nWe convert this to whitened quantities, $\\tilde{d} = R^{-1/2}d$ and $\\tilde{d}_a = R^{-1/2}d_a$.\n$$ \\tilde{d}_a = R^{-1/2} R(S+R)^{-1}R^{1/2}\\tilde{d} = R^{1/2}(S+R)^{-1}R^{1/2}\\tilde{d} = (\\tilde{S}+I)^{-1}\\tilde{d} $$\nUsing the eigen-decomposition $\\tilde{S}=Q\\Lambda Q^\\top$, we can project onto the eigenbasis of $\\tilde{S}$. Let $\\tilde{d}' = Q^\\top \\tilde{d}$ and $\\tilde{d}_a' = Q^\\top \\tilde{d}_a$.\n$$ \\tilde{d}_a' = Q^\\top(Q(\\Lambda+I)Q^\\top)^{-1}Q\\tilde{d}' = (\\Lambda+I)^{-1}\\tilde{d}' $$\nComponent-wise, this is $\\tilde{d}_{a,i}' = \\frac{1}{1+\\lambda_i}\\tilde{d}_i'$.\nWhen $\\lambda_i > 0$, the innovation component is reduced by a factor $1+\\lambda_i$. When $\\lambda_i = 0$ due to the rank deficiency of $P^f$, we have $\\tilde{d}_{a,i}' = \\tilde{d}_i'$. This means the innovation (misfit) is not reduced at all in the directions of observation space not spanned by the ensemble. This is the bound on the reduction of innovation variance imposed by rank deficiency.\n\nNow, we quantify the residual misfit norm $\\|y - H \\bar{x}_{a}\\|_{R^{-1}}^{2}$. This norm is equivalent to the squared Euclidean norm of the whitened analysis residual vector $\\tilde{d}_a$.\n$$ \\|y - H \\bar{x}_{a}\\|_{R^{-1}}^{2} = \\|\\tilde{d}_a\\|^2 = \\|(\\tilde{S}+I)^{-1}\\tilde{d}\\|^2 $$\nUsing the eigen-decomposition:\n$$ \\|\\tilde{d}_a\\|^2 = \\|Q(\\Lambda+I)^{-1}Q^\\top \\tilde{d}\\|^2 $$\nSince $Q$ is an orthonormal matrix, it preserves the Euclidean norm, $\\|Qz\\|^2=\\|z\\|^2$.\n$$ \\|\\tilde{d}_a\\|^2 = \\|(\\Lambda+I)^{-1}Q^\\top \\tilde{d}\\|^2 $$\nLetting $\\tilde{d}' = Q^\\top \\tilde{d}$, the vector inside the norm has components $\\frac{\\tilde{d}'_i}{1+\\lambda_i}$. The squared norm is the sum of the squares of these components:\n$$ \\|y - H \\bar{x}_{a}\\|_{R^{-1}}^{2} = \\sum_{i=1}^m \\frac{(\\tilde{d}'_i)^2}{(1+\\lambda_i)^2} $$\n\n### Numerical Evaluation\nWe are given $m = 3$, $r = 2$, $Q = I_{3}$, $\\Lambda = \\mathrm{diag}(4,1,0)$, and $\\tilde{d} = [2,\\,-1,\\,3]^{\\top}$.\n\nFirst, we compute the residual misfit norm. Since $Q=I_3$, the projected innovation $\\tilde{d}' = Q^\\top \\tilde{d} = \\tilde{d}$. So, $\\tilde{d}'_1=2$, $\\tilde{d}'_2=-1$, and $\\tilde{d}'_3=3$. The eigenvalues are $\\lambda_1=4$, $\\lambda_2=1$, and $\\lambda_3=0$.\n$$ \\|y - H \\bar{x}_{a}\\|_{R^{-1}}^{2} = \\frac{(2)^2}{(1+4)^2} + \\frac{(-1)^2}{(1+1)^2} + \\frac{(3)^2}{(1+0)^2} $$\n$$ = \\frac{4}{25} + \\frac{1}{4} + \\frac{9}{1} = 0.16 + 0.25 + 9 = 9.41 $$\nRounding to four significant figures, this is $9.410$.\n\nSecond, we compute the quantity $\\mathrm{tr}\\big((I + \\tilde{S})^{-1}\\big)$, which the problem defines as the measure for the minimal attainable average analysis variance in the observed space.\nGiven $Q = I_3$, $\\tilde{S} = Q\\Lambda Q^\\top = \\Lambda = \\mathrm{diag}(4,1,0)$.\n$$ I + \\tilde{S} = \\mathrm{diag}(1,1,1) + \\mathrm{diag}(4,1,0) = \\mathrm{diag}(5,2,1) $$\nThe inverse is:\n$$ (I + \\tilde{S})^{-1} = \\mathrm{diag}(5^{-1}, 2^{-1}, 1^{-1}) = \\mathrm{diag}(\\frac{1}{5}, \\frac{1}{2}, 1) $$\nThe trace is the sum of the diagonal elements:\n$$ \\mathrm{tr}\\big((I + \\tilde{S})^{-1}\\big) = \\frac{1}{5} + \\frac{1}{2} + 1 = 0.2 + 0.5 + 1 = 1.7 $$\nRounding to four significant figures, this is $1.700$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 9.410 & 1.700 \\end{pmatrix} } $$"
        }
    ]
}