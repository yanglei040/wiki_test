{
    "hands_on_practices": [
        {
            "introduction": "The Extended Kalman Filter's measurement update step is fundamental to its operation. While the update equations can be expressed in several algebraically equivalent ways, their numerical properties can differ significantly. This exercise  invites you to explore the crucial relationship between the computationally simple \"compact form\" and the numerically superior \"Joseph form\" of the covariance update, which is guaranteed to preserve the symmetry and positive semi-definiteness of the covariance matrix. Understanding this distinction is vital for implementing robust and reliable filters in practice.",
            "id": "3375487",
            "problem": "Consider a discrete-time data assimilation setting with a nonlinear observation model used in the Extended Kalman Filter (EKF). Let the true state at time index $k$ be $x_k \\in \\mathbb{R}^n$ and the measurement be $y_k \\in \\mathbb{R}^m$ with model $y_k = h(x_k) + v_k$, where $h:\\mathbb{R}^n \\to \\mathbb{R}^m$ is differentiable and $v_k$ is zero-mean measurement noise with covariance $R_k \\in \\mathbb{S}_+^m$ (the set of symmetric positive semidefinite $m \\times m$ matrices). Suppose we have a prior estimate $x_{k|k-1}$ with prior error covariance $P_{k|k-1} \\in \\mathbb{S}_+^n$. Assume the usual independence assumptions of the EKF measurement update: the prior estimation error $e_{k|k-1} := x_k - x_{k|k-1}$ is independent of $v_k$, and both are zero mean. Let $H_k := \\left.\\frac{\\partial h}{\\partial x}\\right|_{x_{k|k-1}}$ denote the Jacobian of $h$ at the linearization point $x_{k|k-1}$.\n\nUnder the standard linearization used in the EKF measurement update and the above assumptions, the posterior error $e_{k|k} := x_k - x_{k|k}$ is an affine function of the prior error $e_{k|k-1}$ and of $v_k$, and the posterior covariance $P_{k|k}$ can be written in a form that is manifestly symmetric and positive semidefinite (often called the Joseph form). An alternative algebraic expression, sometimes used in implementations, omits an additive term and appears more compact.\n\nWhich of the following statements about the exact algebraic relationship between these two covariance update expressions in the EKF measurement update is correct?\n\nA. If the Kalman gain $K_k$ is computed as $K_k = P_{k|k-1} H_k^\\top S_k^{-1}$ with innovation covariance $S_k = H_k P_{k|k-1} H_k^\\top + R_k$ (assumed invertible), then the Joseph form and the compact expression are exactly equal in exact arithmetic; any discrepancy in practice is due to finite-precision effects. In particular, the Joseph form preserves symmetry and positive semidefiniteness by construction.\n\nB. The Joseph form and the compact expression are exactly equal if and only if $H_k$ is orthonormal and $R_k = 0$. Otherwise, the Joseph form is strictly larger because of the additive term involving $R_k$.\n\nC. The Joseph form and the compact expression are exactly equal only in the linear Kalman filter with linear $h$ and Gaussian noises, and only when the innovation happens to be zero at time $k$; in the EKF they generally differ even in exact arithmetic.\n\nD. The Joseph form and the compact expression can never be exactly equal for any nontrivial $R_k \\succ 0$; the compact expression is a heuristic approximation that always underestimates the true posterior covariance, regardless of how $K_k$ is computed.",
            "solution": "The user has provided a problem concerning the algebraic relationship between different expressions for the posterior error covariance matrix in the Extended Kalman Filter (EKF) measurement update.\n\n### Problem Validation\n\nFirst, the problem statement must be validated for scientific soundness, self-consistency, and clarity.\n\n**1. Extracted Givens:**\n- State vector at time $k$: $x_k \\in \\mathbb{R}^n$.\n- Measurement vector at time $k$: $y_k \\in \\mathbb{R}^m$.\n- Nonlinear observation model: $y_k = h(x_k) + v_k$.\n- Observation function $h:\\mathbb{R}^n \\to \\mathbb{R}^m$ is differentiable.\n- Measurement noise $v_k$: zero-mean, $E[v_k]=0$, with covariance $R_k \\in \\mathbb{S}_+^m$.\n- Prior state estimate and its error covariance: $x_{k|k-1}$ and $P_{k|k-1} \\in \\mathbb{S}_+^n$.\n- Prior error $e_{k|k-1} := x_k - x_{k|k-1}$ is zero-mean and independent of $v_k$.\n- Jacobian of $h$ evaluated at the prior estimate: $H_k := \\left.\\frac{\\partial h}{\\partial x}\\right|_{x_{k|k-1}}$.\n- The problem discusses two forms for the posterior covariance $P_{k|k}$: the \"Joseph form\" and a more \"compact expression\".\n\n**2. Validation Analysis:**\nThe problem is set within the standard theoretical framework of the Extended Kalman Filter. All terms and assumptions—nonlinear model, linearization via Jacobian, zero-mean independent noise, prior and posterior estimates—are standard in estimation theory. The core of the problem is to compare two well-known algebraic formulas for the posterior covariance update. These are:\n1.  The **Joseph form**: $P_{k|k} = (I - K_k H_k) P_{k|k-1} (I - K_k H_k)^\\top + K_k R_k K_k^\\top$. This form is general for any gain matrix $K_k$ and is numerically stable, ensuring the resulting $P_{k|k}$ is symmetric and positive semidefinite.\n2.  The **compact form**: $P_{k|k} = (I - K_k H_k) P_{k|k-1}$. This form is algebraically simpler and computationally less expensive but is only valid when the gain $K_k$ is the optimal Kalman gain. It is also less numerically stable.\n\nThe question asks for the exact algebraic relationship between these two forms. This is a well-posed, objective, and scientifically grounded problem that requires a standard algebraic derivation. There are no contradictions, ambiguities, or factual errors in the setup.\n\n**3. Verdict:** The problem statement is valid.\n\n### Derivation and Solution\n\nThe objective is to determine the condition under which the Joseph form for the posterior covariance is algebraically equivalent to the compact form.\n\nThe posterior error, based on the EKF linearization $h(x_k) \\approx h(x_{k|k-1}) + H_k(x_k - x_{k|k-1})$, is given by:\n$$ e_{k|k} = x_k - x_{k|k} = x_k - (x_{k|k-1} + K_k(y_k - h(x_{k|k-1}))) $$\n$$ e_{k|k} \\approx (x_k - x_{k|k-1}) - K_k(h(x_{k|k-1}) + H_k(x_k - x_{k|k-1}) + v_k - h(x_{k|k-1})) $$\n$$ e_{k|k} \\approx (I - K_k H_k)e_{k|k-1} - K_k v_k $$\n\nThe posterior error covariance is $P_{k|k} = E[e_{k|k}e_{k|k}^\\top]$. Using the independence of $e_{k|k-1}$ and $v_k$ and their zero-mean property, the cross-terms vanish:\n$$ P_{k|k} \\approx E[((I - K_k H_k)e_{k|k-1} - K_k v_k)((I - K_k H_k)e_{k|k-1} - K_k v_k)^\\top] $$\n$$ P_{k|k} = (I - K_k H_k)E[e_{k|k-1}e_{k|k-1}^\\top](I - K_k H_k)^\\top + K_k E[v_k v_k^\\top]K_k^\\top $$\nThis yields the **Joseph form** of the covariance update, which is valid for any gain $K_k$:\n$$ P_{k|k}^{(J)} = (I - K_k H_k) P_{k|k-1} (I - K_k H_k)^\\top + K_k R_k K_k^\\top $$\nThis form is manifestly symmetric and positive semidefinite if $P_{k|k-1}$ and $R_k$ are, as it is a sum of matrices of the form $AQA^\\top$.\n\nThe question concerns the relationship between this and a \"compact expression\". The most common compact expression for $P_{k|k}$ is derived by substituting the optimal Kalman gain into the Joseph form. Let's perform this algebraic simplification.\n\nWe expand the Joseph form:\n$$ P_{k|k}^{(J)} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1} H_k^\\top K_k^\\top + K_k H_k P_{k|k-1} H_k^\\top K_k^\\top + K_k R_k K_k^\\top $$\n$$ P_{k|k}^{(J)} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1} H_k^\\top K_k^\\top + K_k (H_k P_{k|k-1} H_k^\\top + R_k) K_k^\\top $$\nLet $S_k = H_k P_{k|k-1} H_k^\\top + R_k$ be the innovation covariance. The expression becomes:\n$$ P_{k|k}^{(J)} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1} H_k^\\top K_k^\\top + K_k S_k K_k^\\top $$\nThis equation holds for any gain $K_k$.\n\nNow, let's introduce the specific form of the **optimal Kalman gain**, which minimizes the trace of $P_{k|k}$:\n$$ K_k = P_{k|k-1} H_k^\\top S_k^{-1} $$\nFrom this definition, we can write:\n$$ K_k S_k = P_{k|k-1} H_k^\\top $$\nNow, substitute this relationship back into the expanded Joseph form equation. Specifically, we replace the term $P_{k|k-1} H_k^\\top K_k^\\top$:\n$$ P_{k|k-1} H_k^\\top K_k^\\top = (K_k S_k) K_k^\\top $$\nThe covariance equation becomes:\n$$ P_{k|k}^{(J)} = P_{k|k-1} - K_k H_k P_{k|k-1} - (K_k S_k) K_k^\\top + K_k S_k K_k^\\top $$\nThe last two terms cancel out perfectly:\n$$ P_{k|k}^{(J)} = P_{k|k-1} - K_k H_k P_{k|k-1} $$\nThis expression can be factored as:\n$$ P_{k|k}^{(J)} = (I - K_k H_k) P_{k|k-1} $$\nThis is the widely used **compact form** of the covariance update, let's call it $P_{k|k}^{(C)}$.\nOur derivation shows that $P_{k|k}^{(J)} = P_{k|k}^{(C)}$ if and only if $K_k$ is the optimal Kalman gain $P_{k|k-1} H_k^\\top S_k^{-1}$ (assuming $K_k S_k K_k^\\top - P_{k|k-1} H_k^\\top K_k^\\top \\neq 0$ for non-optimal gains). The two forms are algebraically identical under this condition. Any differences in practice arise from finite-precision arithmetic, where the compact form can lose symmetry and positive-definiteness, while the Joseph form inherently preserves these properties.\n\n### Evaluation of Options\n\n**A. If the Kalman gain $K_k$ is computed as $K_k = P_{k|k-1} H_k^\\top S_k^{-1}$ with innovation covariance $S_k = H_k P_{k|k-1} H_k^\\top + R_k$ (assumed invertible), then the Joseph form and the compact expression are exactly equal in exact arithmetic; any discrepancy in practice is due to finite-precision effects. In particular, the Joseph form preserves symmetry and positive semidefiniteness by construction.**\nThis statement is perfectly aligned with the derivation above. The algebraic equivalence in exact arithmetic is proven. The comment on discrepancies being due to finite-precision effects and the numerical superiority of the Joseph form in preserving symmetry and positive semidefiniteness are standard and correct characterizations.\nVerdict: **Correct**.\n\n**B. The Joseph form and the compact expression are exactly equal if and only if $H_k$ is orthonormal and $R_k = 0$. Otherwise, the Joseph form is strictly larger because of the additive term involving $R_k$.**\nThis statement is incorrect. The derivation shows the two forms are equal whenever the optimal gain is used, regardless of the properties of $H_k$ or whether $R_k$ is zero. The conditions $H_k$ orthonormal and $R_k=0$ are neither necessary nor sufficient in the general case. The \"if and only if\" claim is definitively false.\nVerdict: **Incorrect**.\n\n**C. The Joseph form and the compact expression are exactly equal only in the linear Kalman filter with linear $h$ and Gaussian noises, and only when the innovation happens to be zero at time $k$; in the EKF they generally differ even in exact arithmetic.**\nThis statement is incorrect on multiple grounds. The derivation was performed for the EKF linearization and holds regardless of whether the underlying system is truly linear. The value of the innovation ($y_k - h(x_{k|k-1})$) has no bearing on the covariance update formulas, which depend on the model statistics ($P_{k|k-1}, R_k$) not the specific measurement realization. The claim that they generally differ in exact arithmetic is directly contradicted by the derivation.\nVerdict: **Incorrect**.\n\n**D. The Joseph form and the compact expression can never be exactly equal for any nontrivial $R_k \\succ 0$; the compact expression is a heuristic approximation that always underestimates the true posterior covariance, regardless of how $K_k$ is computed.**\nThis statement is incorrect. The derivation shows they are exactly equal when the optimal gain is used, even for $R_k \\succ 0$. The compact expression is not a heuristic; it is an algebraically equivalent form under a specific condition (use of optimal gain). It is not an underestimation in exact arithmetic.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The accuracy of the Extended Kalman Filter is fundamentally limited by its core assumption: that a first-order Taylor expansion can adequately represent the system's nonlinearities. This practice  provides a rigorous, hands-on method to quantify the error, or \"linearization bias,\" that arises from this approximation. By deriving worst-case error bounds using Taylor's theorem, you will develop a concrete understanding of how the prior uncertainty and the model's curvature interact to determine the EKF's performance, enabling you to critically assess when its estimates are reliable.",
            "id": "3375491",
            "problem": "Consider a scalar nonlinear observation model in a data assimilation setting: a state $x \\in \\mathbb{R}$ has a Gaussian prior $x \\sim \\mathcal{N}(m, P)$ with variance $P \\in \\mathbb{R}_{0}$, and the observation is $y = h(x) + v$, where $v$ is zero-mean observation noise independent of $x$ and $h:\\mathbb{R} \\to \\mathbb{R}$ is thrice continuously differentiable. The Extended Kalman Filter (EKF) predicts the observation via first-order linearization $h(x) \\approx h(m) + h^{\\prime}(m)(x - m)$, while a second-order EKF augments this with the quadratic term obtained by taking the prior expectation, that is $h(x) \\approx h(m) + h^{\\prime}(m)(x - m) + \\tfrac{1}{2} h^{\\prime\\prime}(m) (x - m)^{2}$, whose expectation contributes $\\tfrac{1}{2} h^{\\prime\\prime}(m) P$.\n\nDefine the EKF linearization bias as $b_{\\mathrm{EKF}} := \\mathbb{E}[h(x)] - h(m)$ and the second-order linearization bias as $b_{\\mathrm{SO}} := \\mathbb{E}[h(x)] - \\big(h(m) + \\tfrac{1}{2} h^{\\prime\\prime}(m) P\\big)$. Suppose that for all $x$ in an interval containing the bulk of the Gaussian prior mass, the following curvature bounds hold: $|h^{\\prime\\prime}(x)| \\leq K_{2}$ and $|h^{(3)}(x)| \\leq K_{3}$ for some known $K_{2}, K_{3} \\in \\mathbb{R}_{0}$.\n\nUsing Taylor’s theorem with the Lagrange form of the remainder and basic Gaussian moment identities, derive tight worst-case upper bounds $B_{\\mathrm{EKF}}(P)$ and $B_{\\mathrm{SO}}(P)$ such that $|b_{\\mathrm{EKF}}| \\leq B_{\\mathrm{EKF}}(P)$ and $|b_{\\mathrm{SO}}| \\leq B_{\\mathrm{SO}}(P)$, expressed only in terms of $P$, $K_{2}$, and $K_{3}$. Then define the bias reduction factor $R(P) := \\dfrac{B_{\\mathrm{SO}}(P)}{B_{\\mathrm{EKF}}(P)}$ and provide its exact closed-form expression. State explicitly the condition on $P$, $K_{2}$, and $K_{3}$ under which second-order terms significantly reduce the linearization bias, interpreted as $R(P)  1$.\n\nYour final answer must be the exact closed-form expression for $R(P)$, simplified as much as possible. No numerical rounding is required, and no units are involved.",
            "solution": "The user wants to derive worst-case upper bounds for the linearization bias in first- and second-order Extended Kalman Filters (EKF), and then to find the ratio of these bounds.\n\nThe problem is first validated.\n**Step 1: Extract Givens**\n- State $x \\in \\mathbb{R}$ with prior distribution $x \\sim \\mathcal{N}(m, P)$, where $m$ is the mean and $P>0$ is the variance.\n- Observation model: $y = h(x) + v$, where $h:\\mathbb{R} \\to \\mathbb{R}$ is thrice continuously differentiable and $v$ is zero-mean noise independent of $x$.\n- EKF linearization bias: $b_{\\mathrm{EKF}} := \\mathbb{E}[h(x)] - h(m)$.\n- Second-order linearization bias: $b_{\\mathrm{SO}} := \\mathbb{E}[h(x)] - \\big(h(m) + \\tfrac{1}{2} h^{\\prime\\prime}(m) P\\big)$.\n- Curvature bounds: $|h^{\\prime\\prime}(x)| \\leq K_{2}$ and $|h^{(3)}(x)| \\leq K_{3}$ for positive constants $K_{2}, K_{3}$.\n- Objective 1: Find tight worst-case upper bounds $B_{\\mathrm{EKF}}(P)$ and $B_{\\mathrm{SO}}(P)$ for $|b_{\\mathrm{EKF}}|$ and $|b_{\\mathrm{SO}}|$.\n- Objective 2: Find the bias reduction factor $R(P) := \\frac{B_{\\mathrm{SO}}(P)}{B_{\\mathrm{EKF}}(P)}$.\n- Objective 3: Find the condition on $P$, $K_{2}$, and $K_{3}$ for which $R(P)  1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in estimation theory and data assimilation. The use of Taylor series to analyze filter error is a standard and valid technique. The problem is well-posed, with all necessary information provided and clearly defined objectives. The terminology is precise and objective. There are no contradictions, factual errors, or unscientific claims. The problem is a formal mathematical derivation and is fully verifiable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be derived.\n\n**Derivation of the EKF bias bound $B_{\\mathrm{EKF}}(P)$**\n\nThe EKF linearization bias is defined as $b_{\\mathrm{EKF}} = \\mathbb{E}[h(x)] - h(m)$. We use Taylor's theorem with the Lagrange form of the remainder to expand $h(x)$ around the prior mean $m$. The expansion up to the first-order term is:\n$$h(x) = h(m) + h^{\\prime}(m)(x - m) + \\frac{1}{2} h^{\\prime\\prime}(\\xi)(x - m)^{2}$$\nwhere $\\xi$ is some value between $m$ and $x$.\n\nWe take the expectation of this expression with respect to the distribution of $x \\sim \\mathcal{N}(m, P)$.\n$$\\mathbb{E}[h(x)] = \\mathbb{E}\\left[h(m) + h^{\\prime}(m)(x - m) + \\frac{1}{2} h^{\\prime\\prime}(\\xi)(x - m)^{2}\\right]$$\nBy linearity of expectation:\n$$\\mathbb{E}[h(x)] = h(m) + h^{\\prime}(m)\\mathbb{E}[x - m] + \\frac{1}{2}\\mathbb{E}[h^{\\prime\\prime}(\\xi)(x - m)^{2}]$$\nFor a Gaussian distribution, the first central moment is zero, $\\mathbb{E}[x - m] = 0$. Thus, the expression for the bias becomes:\n$$b_{\\mathrm{EKF}} = \\mathbb{E}[h(x)] - h(m) = \\frac{1}{2}\\mathbb{E}[h^{\\prime\\prime}(\\xi)(x - m)^{2}]$$\nTo find the worst-case upper bound, we take the absolute value:\n$$|b_{\\mathrm{EKF}}| = \\left| \\frac{1}{2}\\mathbb{E}[h^{\\prime\\prime}(\\xi)(x - m)^{2}] \\right|$$\nUsing the property $|\\mathbb{E}[Z]| \\leq \\mathbb{E}[|Z|]$, we get:\n$$|b_{\\mathrm{EKF}}| \\leq \\frac{1}{2}\\mathbb{E}[|h^{\\prime\\prime}(\\xi)(x - m)^{2}|] = \\frac{1}{2}\\mathbb{E}[|h^{\\prime\\prime}(\\xi)|(x - m)^{2}]$$\nWe are given the bound $|h^{\\prime\\prime}(x)| \\leq K_{2}$. Since $\\xi$ lies in the interval between $m$ and $x$, this bound applies, i.e., $|h^{\\prime\\prime}(\\xi)| \\leq K_{2}$.\n$$|b_{\\mathrm{EKF}}| \\leq \\frac{1}{2}\\mathbb{E}[K_{2}(x - m)^{2}] = \\frac{K_{2}}{2}\\mathbb{E}[(x - m)^{2}]$$\nThe term $\\mathbb{E}[(x - m)^{2}]$ is the second central moment, which is the variance $P$ of the distribution.\n$$|b_{\\mathrm{EKF}}| \\leq \\frac{1}{2}K_{2}P$$\nThis bound is tight, as it can be achieved for a function with constant second derivative $h^{\\prime\\prime}(x) = K_{2}$. Therefore, the worst-case upper bound is:\n$$B_{\\mathrm{EKF}}(P) = \\frac{1}{2}K_{2}P$$\n\n**Derivation of the second-order bias bound $B_{\\mathrm{SO}}(P)$**\n\nThe second-order bias is defined as $b_{\\mathrm{SO}} = \\mathbb{E}[h(x)] - \\left(h(m) + \\frac{1}{2} h^{\\prime\\prime}(m) P\\right)$. We use a higher-order Taylor expansion for $h(x)$ around $m$:\n$$h(x) = h(m) + h^{\\prime}(m)(x - m) + \\frac{1}{2}h^{\\prime\\prime}(m)(x - m)^{2} + \\frac{1}{6}h^{(3)}(\\zeta)(x - m)^{3}$$\nwhere $\\zeta$ is some value between $m$ and $x$. Taking the expectation:\n$$\\mathbb{E}[h(x)] = h(m) + h^{\\prime}(m)\\mathbb{E}[x - m] + \\frac{1}{2}h^{\\prime\\prime}(m)\\mathbb{E}[(x - m)^{2}] + \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}]$$\nThe central moments of the Gaussian distribution are $\\mathbb{E}[x-m]=0$ and $\\mathbb{E}[(x-m)^2]=P$.\n$$\\mathbb{E}[h(x)] = h(m) + \\frac{1}{2}h^{\\prime\\prime}(m)P + \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}]$$\nSubstituting this into the definition of $b_{\\mathrm{SO}}$:\n$$b_{\\mathrm{SO}} = \\left(h(m) + \\frac{1}{2}h^{\\prime\\prime}(m)P + \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}]\\right) - \\left(h(m) + \\frac{1}{2} h^{\\prime\\prime}(m) P\\right)$$\n$$b_{\\mathrm{SO}} = \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}]$$\nTo find the bound, we take the absolute value and use the given bound $|h^{(3)}(x)| \\leq K_{3}$:\n$$|b_{\\mathrm{SO}}| = \\left| \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}] \\right| \\leq \\frac{1}{6}\\mathbb{E}[|h^{(3)}(\\zeta)(x - m)^{3}|] = \\frac{1}{6}\\mathbb{E}[|h^{(3)}(\\zeta)| |x - m|^{3}] \\leq \\frac{K_{3}}{6}\\mathbb{E}[|x - m|^{3}]$$\nWe need to calculate the third absolute central moment of a Gaussian random variable $z = x-m \\sim \\mathcal{N}(0, P)$. The standard deviation is $\\sigma = \\sqrt{P}$. The general formula for the $k$-th absolute central moment of a Gaussian distribution is $\\mathbb{E}[|z|^k] = \\sigma^k 2^{k/2} \\frac{\\Gamma((k+1)/2)}{\\sqrt{\\pi}}$. For $k=3$:\n$$\\mathbb{E}[|x - m|^3] = (\\sqrt{P})^{3} 2^{3/2} \\frac{\\Gamma((3+1)/2)}{\\sqrt{\\pi}} = P^{3/2} (2\\sqrt{2}) \\frac{\\Gamma(2)}{\\sqrt{\\pi}}$$\nSince $\\Gamma(2) = 1! = 1$:\n$$\\mathbb{E}[|x - m|^{3}] = 2\\sqrt{\\frac{2}{\\pi}} P^{3/2}$$\nSubstituting this into the inequality for $|b_{\\mathrm{SO}}|$:\n$$|b_{\\mathrm{SO}}| \\leq \\frac{K_{3}}{6} \\left( 2\\sqrt{\\frac{2}{\\pi}} P^{3/2} \\right) = \\frac{K_{3}}{3}\\sqrt{\\frac{2}{\\pi}} P^{3/2}$$\nThis bound is tight under worst-case assumptions about the function $h(x)$. The upper bound is:\n$$B_{\\mathrm{SO}}(P) = \\frac{K_{3}}{3}\\sqrt{\\frac{2}{\\pi}} P^{3/2}$$\n\n**Bias Reduction Factor $R(P)$ and Condition for Improvement**\n\nThe bias reduction factor is the ratio of the two bounds:\n$$R(P) = \\frac{B_{\\mathrm{SO}}(P)}{B_{\\mathrm{EKF}}(P)} = \\frac{\\frac{K_{3}}{3}\\sqrt{\\frac{2}{\\pi}} P^{3/2}}{\\frac{1}{2}K_{2}P}$$\nSimplifying the expression:\n$$R(P) = \\left(\\frac{K_{3}}{3} \\cdot \\frac{2}{K_{2}}\\right) \\sqrt{\\frac{2}{\\pi}} \\left(\\frac{P^{3/2}}{P}\\right) = \\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2}{\\pi}} P^{1/2}$$\n$$R(P) = \\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2P}{\\pi}}$$\nThe second-order correction provides a significant reduction in the linearization bias bound if $R(P)  1$:\n$$\\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2P}{\\pi}}  1$$\nSince all terms are positive, we can square both sides:\n$$\\frac{4K_{3}^2}{9K_{2}^2} \\frac{2P}{\\pi}  1$$\n$$\\frac{8K_{3}^2 P}{9\\pi K_{2}^2}  1$$\nSolving for $P$ gives the condition for bias reduction:\n$$P  \\frac{9\\pi K_{2}^2}{8K_{3}^2}$$\nThis shows that the second-order correction is beneficial when the prior variance $P$ is sufficiently small, with the threshold depending on the ratio of the curvature bounds $K_2$ and $K_3$. The problem asks for the closed-form expression for $R(P)$.",
            "answer": "$$\\boxed{\\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2P}{\\pi}}}$$"
        },
        {
            "introduction": "After analyzing the first-order linearization bias, the natural next step is to develop a more accurate filter. This exercise  guides you through the derivation of a second-order Extended Kalman Filter. By systematically incorporating second-derivative information into the update step, you will learn how to correct for the mean prediction bias and adjust the innovation covariance to better account for model curvature. This practice demonstrates a powerful method for improving estimation accuracy in moderately nonlinear systems where the standard EKF may fall short.",
            "id": "3375506",
            "problem": "Consider a single-step measurement update in a nonlinear Bayesian data assimilation problem. Let the state be scalar with prior distribution $x \\sim \\mathcal{N}(m, P)$, and let the measurement model be $y = h(x) + v$, where $v \\sim \\mathcal{N}(0, R)$ is independent of $x$. Your task is to derive the second-order Extended Kalman Filter (EKF) measurement update quantities using a second-order Taylor expansion and Gaussian moment identities, starting from first principles. Specifically:\n\n1. Using a second-order Taylor expansion of $h(x)$ around the prior mean $m$, derive expressions for the predicted measurement mean and innovation variance that are accurate up to and including terms involving the second derivative $h''(m)$. You must show how the second derivative of $h$ contributes to the innovation mean and covariance, and justify any vanishing terms using the properties of Gaussian central moments.\n2. Show that, to this order, the state–measurement cross-covariance entering the Kalman gain remains $P h'(m)$.\n3. Define the second-order EKF gain $K_{2}$ as the ratio of this cross-covariance to the innovation variance you derived in part 1.\n4. Now specialize to the quadratic measurement function $h(x) = c x^{2}$, with a fixed nonzero constant $c \\in \\mathbb{R}$. Compute $K_{2}$ for this measurement and, for comparison, compute the first-order EKF gain $K_{1}$ obtained by truncating the expansion after the first derivative. \n\nProvide as your final answer the closed-form analytic expression for the ratio $K_{2}/K_{1}$ as a function of $c$, $m$, $P$, and $R$. No numerical evaluation is required, and no units are needed. Express your final answer as a single simplified algebraic expression.",
            "solution": "The problem asks for the derivation of the second-order Extended Kalman Filter (EKF) measurement update quantities and the subsequent calculation of a specific gain ratio for a quadratic measurement model. The validation of the problem confirms it is scientifically sound, well-posed, objective, and complete. We proceed with the solution.\n\nThe state is a scalar random variable $x$ with a prior Gaussian distribution $x \\sim \\mathcal{N}(m, P)$, where $m$ is the prior mean and $P$ is the prior variance. The measurement model is given by $y = h(x) + v$, where the measurement noise $v$ is also Gaussian, $v \\sim \\mathcal{N}(0, R)$, and is independent of $x$.\n\nThe core of the second-order EKF is to approximate the nonlinear function $h(x)$ using a second-order Taylor series expansion around the prior mean $m$:\n$$\nh(x) \\approx h(m) + h'(m)(x-m) + \\frac{1}{2}h''(m)(x-m)^2\n$$\nwhere $h'(m)$ and $h''(m)$ are the first and second derivatives of $h(x)$ evaluated at $x=m$.\n\n### Part 1: Predicted Measurement Mean and Innovation Variance\n\nFirst, we derive the predicted measurement mean, $\\mu_y = E[y]$. Since $y = h(x) + v$ and $v$ has zero mean and is independent of $x$, we have $E[y] = E[h(x)] + E[v] = E[h(x)]$. Using the Taylor expansion for $h(x)$:\n$$\n\\mu_y = E[y] \\approx E\\left[h(m) + h'(m)(x-m) + \\frac{1}{2}h''(m)(x-m)^2\\right]\n$$\nBy the linearity of the expectation operator:\n$$\n\\mu_y \\approx h(m) + h'(m)E[x-m] + \\frac{1}{2}h''(m)E[(x-m)^2]\n$$\nWe use the properties of the Gaussian distribution $x \\sim \\mathcal{N}(m, P)$. The central moments are:\n1.  $E[x-m] = 0$ (the first central moment is zero).\n2.  $E[(x-m)^2] = P$ (the second central moment is the variance).\nSubstituting these into the expression for $\\mu_y$:\n$$\n\\mu_y \\approx h(m) + h'(m)(0) + \\frac{1}{2}h''(m)P = h(m) + \\frac{1}{2}h''(m)P\n$$\nThis shows that the second derivative $h''(m)$ introduces a bias correction term $\\frac{1}{2}h''(m)P$ to the predicted measurement mean compared to the first-order EKF, where the predicted mean is simply $h(m)$.\n\nNext, we derive the innovation variance, $S = \\text{Var}(y)$. Since $x$ and $v$ are independent, $h(x)$ and $v$ are also independent. Therefore, the variance of their sum is the sum of their variances:\n$$\nS = \\text{Var}(y) = \\text{Var}(h(x) + v) = \\text{Var}(h(x)) + \\text{Var}(v)\n$$\nWe know $\\text{Var}(v) = R$. We need to compute $\\text{Var}(h(x))$. Using the definition $\\text{Var}(Z) = E[(Z - E[Z])^2]$:\n$$\n\\text{Var}(h(x)) \\approx E\\left[ \\left( h(x) - E[h(x)] \\right)^2 \\right]\n$$\nFrom our previous results for the Taylor expansion and the mean:\n$$\nh(x) - E[h(x)] \\approx \\left(h(m) + h'(m)(x-m) + \\frac{1}{2}h''(m)(x-m)^2\\right) - \\left(h(m) + \\frac{1}{2}h''(m)P\\right)\n$$\n$$\nh(x) - E[h(x)] \\approx h'(m)(x-m) + \\frac{1}{2}h''(m)\\left((x-m)^2 - P\\right)\n$$\nNow we square this expression and take the expectation:\n$$\n\\text{Var}(h(x)) \\approx E\\left[ \\left( h'(m)(x-m) + \\frac{1}{2}h''(m)\\left((x-m)^2 - P\\right) \\right)^2 \\right]\n$$\nExpanding the square gives three terms:\n$$\nE\\left[ (h'(m))^2(x-m)^2 + h'(m)h''(m)(x-m)((x-m)^2 - P) + \\frac{1}{4}(h''(m))^2((x-m)^2 - P)^2 \\right]\n$$\nWe evaluate the expectation of each term:\n1.  First term: $E[(h'(m))^2(x-m)^2] = (h'(m))^2 E[(x-m)^2] = (h'(m))^2 P$.\n2.  Cross term: $E[h'(m)h''(m)(x-m)((x-m)^2 - P)] = h'(m)h''(m) \\left( E[(x-m)^3] - P E[x-m] \\right)$. For a Gaussian distribution, all odd central moments are zero. Thus, $E[x-m]=0$ and $E[(x-m)^3]=0$. This term vanishes entirely.\n3.  Third term: $E\\left[\\frac{1}{4}(h''(m))^2((x-m)^2 - P)^2\\right] = \\frac{1}{4}(h''(m))^2 E\\left[(x-m)^4 - 2P(x-m)^2 + P^2\\right]$. The fourth central moment of a Gaussian distribution is $E[(x-m)^4] = 3P^2$. Substituting this:\n    $$\n    \\frac{1}{4}(h''(m))^2 (3P^2 - 2P(P) + P^2) = \\frac{1}{4}(h''(m))^2 (2P^2) = \\frac{1}{2}(h''(m))^2 P^2\n    $$\nCombining these results, the variance of $h(x)$ is:\n$$\n\\text{Var}(h(x)) \\approx (h'(m))^2 P + \\frac{1}{2}(h''(m))^2 P^2\n$$\nThe total innovation variance is therefore:\n$$\nS \\approx (h'(m))^2 P + \\frac{1}{2}(h''(m))^2 P^2 + R\n$$\nThe second derivative term contributes $\\frac{1}{2}(h''(m))^2 P^2$ to the innovation variance, accounting for the uncertainty introduced by the curvature of the measurement function.\n\n### Part 2: State-Measurement Cross-Covariance\n\nWe need to compute the cross-covariance, $\\text{Cov}(x, y)$.\n$$\n\\text{Cov}(x, y) = \\text{Cov}(x, h(x) + v) = \\text{Cov}(x, h(x)) + \\text{Cov}(x, v)\n$$\nSince $x$ and $v$ are independent, $\\text{Cov}(x, v) = 0$. We use the definition $\\text{Cov}(A, B) = E[(A-E[A])(B-E[B])]$.\n$$\n\\text{Cov}(x, y) = \\text{Cov}(x, h(x)) = E[(x-m)(h(x) - E[h(x)])]\n$$\nSubstituting the approximation for $h(x) - E[h(x)]$:\n$$\n\\text{Cov}(x, y) \\approx E\\left[ (x-m) \\left( h'(m)(x-m) + \\frac{1}{2}h''(m)\\left((x-m)^2 - P\\right) \\right) \\right]\n$$\n$$\n\\text{Cov}(x, y) \\approx E[h'(m)(x-m)^2] + E\\left[\\frac{1}{2}h''(m)(x-m)\\left((x-m)^2 - P\\right)\\right]\n$$\n1.  First term: $E[h'(m)(x-m)^2] = h'(m)E[(x-m)^2] = h'(m)P$.\n2.  Second term: $\\frac{1}{2}h''(m) E[(x-m)^3 - P(x-m)] = \\frac{1}{2}h''(m) (E[(x-m)^3] - P E[x-m])$. As before, since the odd central moments of a Gaussian are zero, this term is $0$.\n\nThus, the cross-covariance to this order of approximation is:\n$$\n\\text{Cov}(x, y) \\approx P h'(m)\n$$\nThis shows that, up to the approximations made, the second derivative of $h(x)$ does not contribute to the state-measurement cross-covariance.\n\n### Part 3: Second-Order EKF Gain\n\nThe Kalman gain $K$ is the ratio of the state-measurement cross-covariance to the innovation variance. The second-order EKF gain, $K_2$, is therefore:\n$$\nK_2 = \\frac{\\text{Cov}(x, y)}{S} \\approx \\frac{P h'(m)}{(h'(m))^2 P + \\frac{1}{2}(h''(m))^2 P^2 + R}\n$$\n\n### Part 4: Specialization to $h(x) = c x^2$\n\nWe now specialize to the measurement function $h(x) = c x^2$, where $c$ is a non-zero constant. We compute the required derivatives at $x=m$:\n-   $h'(x) = 2cx \\implies h'(m) = 2cm$\n-   $h''(x) = 2c \\implies h''(m) = 2c$\n\nSubstitute these into the expression for $K_2$:\n$$\nK_2 = \\frac{P (2cm)}{(2cm)^2 P + \\frac{1}{2}(2c)^2 P^2 + R} = \\frac{2cmP}{4c^2m^2P + \\frac{1}{2}(4c^2)P^2 + R} = \\frac{2cmP}{4c^2m^2P + 2c^2P^2 + R}\n$$\n\nFor comparison, we compute the first-order EKF gain, $K_1$. This gain is derived using only the first-order Taylor expansion, which means the innovation variance $S_1$ omits the term involving $h''$.\n$$\nS_1 = (h'(m))^2 P + R\n$$\nThe cross-covariance remains $P h'(m)$. Therefore, the first-order gain is:\n$$\nK_1 = \\frac{P h'(m)}{(h'(m))^2 P + R}\n$$\nSubstituting the derivatives for $h(x) = c x^2$:\n$$\nK_1 = \\frac{P(2cm)}{(2cm)^2 P + R} = \\frac{2cmP}{4c^2m^2P + R}\n$$\n\nFinally, we compute the ratio $K_2 / K_1$:\n$$\n\\frac{K_2}{K_1} = \\frac{\\frac{2cmP}{4c^2m^2P + 2c^2P^2 + R}}{\\frac{2cmP}{4c^2m^2P + R}}\n$$\nAssuming $c \\neq 0$, $m \\neq 0$, and $P \\neq 0$, the numerator $2cmP$ is non-zero and cancels out:\n$$\n\\frac{K_2}{K_1} = \\frac{4c^2m^2P + R}{4c^2m^2P + 2c^2P^2 + R}\n$$\nThis is the final simplified algebraic expression for the ratio.",
            "answer": "$$\n\\boxed{\\frac{4c^2m^2P + R}{4c^2m^2P + 2c^2P^2 + R}}\n$$"
        }
    ]
}