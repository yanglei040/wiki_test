{
    "hands_on_practices": [
        {
            "introduction": "理论学习的最佳伙伴是动手实践。本节的第一个练习将带您从最基本的微积分原理出发，亲手推导带有通用正则化算子 $L$ 的吉洪诺夫泛函所对应的正规方程。通过求解一个具体的符号示例 ()，您将不仅巩固对正规方程 $(A^\\top A + \\lambda L^\\top L)x = A^\\top b$ 的来源的理解，还将学会如何通过分析算子 $A$ 和 $L$ 的零空间来确保解的唯一性，这是处理正则化问题的一项基本功。",
            "id": "3405686",
            "problem": "考虑一个线性逆模型，其未知状态向量为 $x \\in \\mathbb{R}^{3}$，观测算子为 $A \\in \\mathbb{R}^{2 \\times 3}$，数据为 $b \\in \\mathbb{R}^{2}$。设\n$$\nA \\;=\\; \\begin{pmatrix}\n1  -1  0 \\\\\n0  1  -1\n\\end{pmatrix}, \\qquad\nb \\;=\\; \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix},\n$$\n并假设我们使用如下给出的线性算子 $L \\in \\mathbb{R}^{2 \\times 3}$ 进行正则化\n$$\nL \\;=\\; \\begin{pmatrix}\n1  0  0 \\\\\n0  1  -1\n\\end{pmatrix}.\n$$\n矩阵 $A$ 有一个非平凡的零空间，而矩阵 $L$ 惩罚与该零空间互补的方向。对于一个固定的正则化参数 $\\lambda > 0$，考虑吉洪诺夫泛函\n$$\nJ(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda \\,\\|L x\\|_{2}^{2}.\n$$\n从线性代数和微积分的基本原理出发，且不使用任何预先给出的最优性公式，执行以下操作：\n- 通过计算梯度 $\\nabla J(x)$ 并将其设为零，推导最小化子 $x_{\\lambda}$ 的一阶最优性条件。通过分析 $A$ 和 $L$ 的零空间，论证所得到的线性系统对于所有 $\\lambda > 0$ 都是非奇异的。\n- 使用该最优性条件，显式求解 $x_{\\lambda}$，得到一个关于 $\\lambda$ 的闭式表达式。\n\n你的最终答案必须是 $x_{\\lambda}$ 的显式表达式，写成一个单行向量的形式。不需要四舍五入，也没有单位。请以精确的符号形式表示最终答案。",
            "solution": "吉洪诺夫泛函由下式给出\n$$ J(x) = \\|A x - b\\|_{2}^{2} + \\lambda \\|L x\\|_{2}^{2} $$\n其中 $x \\in \\mathbb{R}^{3}$，$A \\in \\mathbb{R}^{2 \\times 3}$，$b \\in \\mathbb{R}^{2}$，$L \\in \\mathbb{R}^{2 \\times 3}$，且 $\\lambda > 0$。我们给定了具体的矩阵和向量\n$$\nA = \\begin{pmatrix}\n1  -1  0 \\\\\n0  1  -1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}, \\quad\nL = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  -1\n\\end{pmatrix}.\n$$\n\n为了找到 $J(x)$ 的最小化子 $x_{\\lambda}$，我们首先推导一阶最优性条件，即令 $J(x)$ 关于 $x$ 的梯度为零。我们首先使用平方欧几里得范数的定义 $\\|v\\|_2^2 = v^T v$ 来展开泛函。\n$$ J(x) = (Ax - b)^T (Ax - b) + \\lambda (Lx)^T (Lx) $$\n利用转置的性质 $(MN)^T=N^T M^T$，我们展开各项：\n$$ J(x) = (x^T A^T - b^T)(Ax - b) + \\lambda (x^T L^T L x) $$\n$$ J(x) = x^T A^T A x - x^T A^T b - b^T A x + b^T b + \\lambda x^T L^T L x $$\n由于 $b^T A x$ 是一个标量，它等于其转置 $(b^T A x)^T = x^T A^T b$。因此，我们可以合并线性项：\n$$ J(x) = x^T (A^T A + \\lambda L^T L) x - 2 b^T A x + b^T b $$\n这是一个关于 $x$ 的二次型。为了求梯度 $\\nabla J(x) = \\frac{dJ}{dx}$，我们使用向量微积分的标准结果：$\\nabla_x (x^T M x) = (M + M^T)x$ 和 $\\nabla_x (c^T x) = c$。\n矩阵 $H = A^T A + \\lambda L^T L$ 是对称的，因为 $(A^T A)^T = A^T (A^T)^T = A^T A$ 且 $(L^T L)^T = L^T (L^T)^T = L^T L$。因此，二次项的梯度是 $2(A^T A + \\lambda L^T L)x$。\n线性项可以写成 $-2(A^T b)^T x$，所以它的梯度是 $-2A^T b$。\n项 $b^T b$ 相对于 $x$ 是常数，所以它的梯度为零。\n结合这些结果，泛函的梯度为：\n$$ \\nabla J(x) = 2(A^T A + \\lambda L^T L)x - 2A^T b $$\n一阶最优性条件是 $\\nabla J(x) = 0$：\n$$ 2(A^T A + \\lambda L^T L)x - 2A^T b = 0 $$\n$$ (A^T A + \\lambda L^T L)x = A^T b $$\n这就是吉洪诺夫正则化问题的正规方程组。\n\n接下来，我们必须证明系统矩阵 $H(\\lambda) = A^T A + \\lambda L^T L$ 对于任何 $\\lambda > 0$ 都是非奇异的。一个矩阵是非奇异的，当且仅当其零空间（或核）只包含零向量。设 $v$ 是 $H(\\lambda)$ 零空间中的一个向量，因此 $H(\\lambda)v = 0$。\n$$ (A^T A + \\lambda L^T L)v = 0 $$\n从左侧乘以 $v^T$：\n$$ v^T(A^T A + \\lambda L^T L)v = v^T 0 = 0 $$\n$$ v^T A^T A v + \\lambda v^T L^T L v = 0 $$\n$$ (Av)^T(Av) + \\lambda (Lv)^T(Lv) = 0 $$\n$$ \\|Av\\|_2^2 + \\lambda \\|Lv\\|_2^2 = 0 $$\n因为范数是非负的，并且我们已知 $\\lambda > 0$，所以这个和为零的唯一可能是两项都各自为零：\n$$ \\|Av\\|_2^2 = 0 \\quad \\text{and} \\quad \\|Lv\\|_2^2 = 0 $$\n这意味着 $Av = 0$ 且 $Lv = 0$。因此，$H(\\lambda)$ 零空间中的向量 $v$ 必须同时位于 $A$ 的零空间和 $L$ 的零空间中。换句话说，$\\ker(H(\\lambda)) = \\ker(A) \\cap \\ker(L)$。\n我们来求这些零空间。\n对于 $\\ker(A)$，我们求解 $Ax=0$，其中 $x = (x_1, x_2, x_3)^T$：\n$$ \\begin{pmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这得到 $x_1 - x_2 = 0 \\implies x_1 = x_2$ 和 $x_2 - x_3 = 0 \\implies x_2 = x_3$。因此，$x_1=x_2=x_3$。$\\ker(A)$ 中的任何向量都具有 $c(1, 1, 1)^T$ 的形式，其中 $c$ 是某个标量。所以，$\\ker(A) = \\text{span}\\left\\{(1, 1, 1)^T\\right\\}$。\n\n对于 $\\ker(L)$，我们求解 $Lx=0$：\n$$ \\begin{pmatrix} 1  0  0 \\\\ 0  1  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这得到 $x_1=0$ 和 $x_2 - x_3 = 0 \\implies x_2=x_3$。$\\ker(L)$ 中的任何向量都具有 $c(0, 1, 1)^T$ 的形式，其中 $c$ 是某个标量。所以，$\\ker(L) = \\text{span}\\left\\{(0, 1, 1)^T\\right\\}$。\n\n现在我们求交集 $\\ker(A) \\cap \\ker(L)$。交集中的向量 $v$ 必须是 $(1, 1, 1)^T$ 的标量倍，同时也是 $(0, 1, 1)^T$ 的标量倍。设 $v = c_1(1, 1, 1)^T = c_2(0, 1, 1)^T$。比较第一个分量，我们得到 $c_1 \\cdot 1 = c_2 \\cdot 0$，这意味着 $c_1=0$。如果 $c_1=0$，那么 $v = 0 \\cdot (1, 1, 1)^T = (0, 0, 0)^T$。交集只包含零向量：$\\ker(A) \\cap \\ker(L) = \\{0\\}$。\n由于 $\\ker(H(\\lambda))=\\{0\\}$，矩阵 $H(\\lambda)$ 对于所有 $\\lambda > 0$ 都是非奇异的，并且存在唯一解 $x_{\\lambda}$。\n\n最后，我们通过求解线性系统来求解 $x_{\\lambda}$。首先，我们计算矩阵 $A^T A$ 和 $L^T L$，以及向量 $A^T b$。\n$$ A^T = \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{pmatrix}, \\quad L^T = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix} $$\n$$ A^T A = \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{pmatrix} = \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix} $$\n$$ L^T L = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  -1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  -1 \\\\ 0  -1  1 \\end{pmatrix} $$\n系统矩阵是：\n$$ H(\\lambda) = A^T A + \\lambda L^T L = \\begin{pmatrix} 1+\\lambda  -1  0 \\\\ -1  2+\\lambda  -1-\\lambda \\\\ 0  -1-\\lambda  1+\\lambda \\end{pmatrix} $$\n右侧项是：\n$$ A^T b = \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\n要求解 $x_{\\lambda} = (x_1, x_2, x_3)^T$ 的系统是：\n$$ \\begin{pmatrix} 1+\\lambda  -1  0 \\\\ -1  2+\\lambda  -1-\\lambda \\\\ 0  -1-\\lambda  1+\\lambda \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\n这对应于以下方程组：\n1) $(1+\\lambda)x_1 - x_2 = 2$\n2) $-x_1 + (2+\\lambda)x_2 - (1+\\lambda)x_3 = -1$\n3) $-(1+\\lambda)x_2 + (1+\\lambda)x_3 = -1$\n\n由方程（3），因为 $\\lambda > 0$，所以 $1+\\lambda \\ne 0$，我们可以用它来除：\n$$ -x_2 + x_3 = -\\frac{1}{1+\\lambda} \\implies x_3 = x_2 - \\frac{1}{1+\\lambda} $$\n将这个 $x_3$ 的表达式代入方程（2）：\n$$ -x_1 + (2+\\lambda)x_2 - (1+\\lambda)\\left(x_2 - \\frac{1}{1+\\lambda}\\right) = -1 $$\n$$ -x_1 + (2+\\lambda)x_2 - (1+\\lambda)x_2 + 1 = -1 $$\n$$ -x_1 + (2+\\lambda - 1-\\lambda)x_2 = -2 $$\n$$ -x_1 + x_2 = -2 $$\n现在我们得到一个关于 $x_1$ 和 $x_2$ 的双方程系统：\n(a) $-x_1 + x_2 = -2$\n(b) $(1+\\lambda)x_1 - x_2 = 2$ （来自方程 1）\n\n将方程（a）和（b）相加：\n$$ (-x_1 + x_2) + ((1+\\lambda)x_1 - x_2) = -2 + 2 $$\n$$ -x_1 + (1+\\lambda)x_1 = 0 $$\n$$ \\lambda x_1 = 0 $$\n因为 $\\lambda > 0$，我们必须有 $x_1 = 0$。\n\n将 $x_1=0$ 代回方程（a）：\n$$ -0 + x_2 = -2 \\implies x_2 = -2 $$\n最后，将 $x_2=-2$ 代入 $x_3$ 的表达式中：\n$$ x_3 = -2 - \\frac{1}{1+\\lambda} = \\frac{-2(1+\\lambda) - 1}{1+\\lambda} = \\frac{-2-2\\lambda-1}{1+\\lambda} = -\\frac{2\\lambda+3}{\\lambda+1} $$\n解向量是：\n$$ x_{\\lambda} = \\begin{pmatrix} 0 \\\\ -2 \\\\ -\\frac{2\\lambda+3}{\\lambda+1} \\end{pmatrix} $$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0 & -2 & -\\frac{2\\lambda+3}{\\lambda+1} \\end{pmatrix} } $$"
        },
        {
            "introduction": "在掌握了正规方程的代数推导之后，下一个关键步骤是深入理解正则化是如何工作的。本练习 () 引导我们运用奇异值分解（SVD）这一强大的工具，它能将复杂的矩阵运算分解为一系列简单的标量操作。通过这种方式，我们可以清晰地看到吉洪诺夫正则化如同一个“滤波器”，它通过正则化参数 $\\lambda$ 来精确地衰减与小奇异值相关的数据分量，从而有效地抑制噪声并稳定解。",
            "id": "3405692",
            "problem": "考虑一个数据同化中的线性逆问题，其中观测数据向量 $\\mathbf{y} \\in \\mathbb{R}^{m}$ 被建模为 $\\mathbf{y} = A \\mathbf{x}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$，以及 $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{m}$ 代表观测噪声。为稳定反演过程，使用零阶吉洪诺夫正则化 (Tikhonov regularization)，并将估计量 $\\mathbf{x}_{\\lambda}$ 定义为以下泛函的最小化子：\n$$\nJ(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2},\n$$\n其中 $\\lambda > 0$。从核心定义出发，推导与此最小化问题相关的正规方程，然后在奇异值分解（SVD）的基中表示解，其中 $A = U \\Sigma V^{\\top}$，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，其非负对角线元素为 $\\sigma_{1}, \\dots, \\sigma_{r}$，其中 $r = \\operatorname{rank}(A)$ 且对于 $i \\le r$ 有 $\\sigma_{i} > 0$。假设数据 $\\mathbf{y}$ 完全投影到单个左奇异向量 $u_{i}$ 上，即，对于某个索引 $i \\in \\{1, \\dots, r\\}$ 和标量 $\\beta_{i} \\in \\mathbb{R}$，有 $\\mathbf{y} = \\beta_{i} u_{i}$，所有其他分量均为零。设 $\\sigma_{i}$ 为严格正数，且与 $A$ 的最大奇异值相比很小。\n\n将衰减因子 $f(\\sigma_{i}, \\lambda)$ 定义为 $\\mathbf{x}_{\\lambda}$ 中 $v_{i}$ 的系数与数据系数 $\\beta_{i}$ 之间的比率。以 $\\sigma_{i}$ 和 $\\lambda$ 的闭式解析表达式形式显式计算 $f(\\sigma_{i}, \\lambda)$。最终答案必须是单个解析表达式。无需四舍五入，也无需报告单位。",
            "solution": "问题在于，为一个与线性逆问题的吉洪诺夫正则化解相关的衰减因子寻找一个闭式表达式。该过程首先从验证问题陈述开始。\n\n### 第 1 步：提取已知条件\n-   **线性模型**：观测数据向量 $\\mathbf{y} \\in \\mathbb{R}^{m}$ 通过 $\\mathbf{y} = A \\mathbf{x}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$ 与真实状态 $\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ 相关，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{m}$ 是观测噪声。\n-   **吉洪诺夫泛函**：对于正则化参数 $\\lambda > 0$，正则化解 $\\mathbf{x}_{\\lambda}$ 是泛函 $J(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}$ 的最小化子。\n-   **奇异值分解 (SVD)**：矩阵 $A$ 的 SVD 由 $A = U \\Sigma V^{\\top}$ 给出，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵。矩阵 $\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，其主对角线上的非负元素为 $\\sigma_{1}, \\dots, \\sigma_{r}$，其中 $r = \\operatorname{rank}(A)$ 且对于 $i \\le r$ 有 $\\sigma_{i} > 0$。\n-   **数据条件**：数据向量被指定为 $\\mathbf{y} = \\beta_{i} u_{i}$，其中 $i \\in \\{1, \\dots, r\\}$ 是某个索引，$\\beta_{i} \\in \\mathbb{R}$ 是一个标量系数。这里，$u_i$ 是 $U$ 的第 $i$ 列。\n-   **奇异值条件**：与数据对应的奇异值 $\\sigma_{i}$ 是严格正的，并且与 $A$ 的最大奇异值相比很小。\n-   **衰减因子的定义**：因子 $f(\\sigma_{i}, \\lambda)$ 被定义为解 $\\mathbf{x}_{\\lambda}$ 中 $v_{i}$ 的系数与数据系数 $\\beta_{i}$ 的比率。\n\n### 第 2 步：使用提取的已知条件进行验证\n-   **科学上合理**：该问题是逆问题理论中的一个标准练习，特别是关于吉洪诺夫正则化。使用 SVD 是分析此类问题的典型方法。该设置在根本上是合理的。\n-   **适定性**：泛函 $J(\\mathbf{x})$ 是两个平方范数之和。由于 $A^{\\top}A$ 是半正定的，对于任何 $\\lambda > 0$，矩阵 $A^{\\top}A + \\lambda I$ 都是正定的。这保证了 $J(\\mathbf{x})$ 是严格凸的，并有唯一最小值。该问题是适定的。\n-   **目标明确**：该问题以精确的数学语言陈述，所有术语和变量都已明确定义。\n-   **完整性**：推导解所需的所有信息都已提供。将数据向量简化为单个 SVD 分量是一种用以分离和分析正则化器效果的标准技术，而不是一个缺陷。\n\n### 第 3 步：结论和行动\n问题是有效的。我们继续求解。\n\n吉洪诺夫正则化解 $\\mathbf{x}_{\\lambda}$ 是使以下泛函最小化的向量 $\\mathbf{x}$：\n$$J(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}$$\n这可以用内积写成：\n$$J(\\mathbf{x}) = (A \\mathbf{x} - \\mathbf{y})^{\\top}(A \\mathbf{x} - \\mathbf{y}) + \\lambda \\mathbf{x}^{\\top}\\mathbf{x}$$\n展开各项，我们得到：\n$$J(\\mathbf{x}) = \\mathbf{x}^{\\top}A^{\\top}A\\mathbf{x} - 2\\mathbf{y}^{\\top}A\\mathbf{x} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda \\mathbf{x}^{\\top}\\mathbf{x}$$\n为了找到最小值，我们计算 $J(\\mathbf{x})$ 关于 $\\mathbf{x}$ 的梯度并将其设为零。梯度为：\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = 2A^{\\top}A\\mathbf{x} - 2A^{\\top}\\mathbf{y} + 2\\lambda\\mathbf{x}$$\n令 $\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{0}$ 得到：\n$$A^{\\top}A\\mathbf{x}_{\\lambda} - A^{\\top}\\mathbf{y} + \\lambda\\mathbf{x}_{\\lambda} = \\mathbf{0}$$\n重新整理各项，我们得到吉洪诺夫正则化的正规方程：\n$$(A^{\\top}A + \\lambda I)\\mathbf{x}_{\\lambda} = A^{\\top}\\mathbf{y}$$\n其中 $I$ 是 $n \\times n$ 的单位矩阵。由于 $\\lambda > 0$，矩阵 $(A^{\\top}A + \\lambda I)$ 是可逆的。解可以正式地写为：\n$$\\mathbf{x}_{\\lambda} = (A^{\\top}A + \\lambda I)^{-1}A^{\\top}\\mathbf{y}$$\n为了分析这个解，我们引入 $A$ 的 SVD，即 $A = U \\Sigma V^{\\top}$。其转置为 $A^{\\top} = V \\Sigma^{\\top} U^{\\top}$。我们将这些代入 $\\mathbf{x}_{\\lambda}$ 的表达式中。\n首先，我们计算 $A^{\\top}A$ 项：\n$$A^{\\top}A = (V \\Sigma^{\\top} U^{\\top})(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} (U^{\\top}U) \\Sigma V^{\\top} = V (\\Sigma^{\\top}\\Sigma) V^{\\top}$$\n利用 $U$ 的正交性 ($U^{\\top}U = I_m$)。\n正规方程变为：\n$$(V \\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda I)\\mathbf{x}_{\\lambda} = V \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\n由于 $I = VV^{\\top}$，我们可以将 $V$ 和 $V^{\\top}$ 因子提出：\n$$V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}\\mathbf{x}_{\\lambda} = V \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\n左乘 $V^{\\top}$ 并利用 $V^{\\top}V = I_n$：\n$$(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}\\mathbf{x}_{\\lambda} = \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\n矩阵 $(\\Sigma^{\\top}\\Sigma + \\lambda I)$ 是一个大小为 $n \\times n$ 的对角矩阵。其对角线元素对于 $j=1, \\dots, r$ 是 $(\\sigma_{j}^{2} + \\lambda)$，对于 $j > r$ 是 $\\lambda$。该矩阵是可逆的。\n让我们在 SVD 基中定义解和数据：$\\hat{\\mathbf{x}} = V^{\\top}\\mathbf{x}_{\\lambda}$ 和 $\\hat{\\mathbf{y}} = U^{\\top}\\mathbf{y}$。方程变为：\n$$(\\Sigma^{\\top}\\Sigma + \\lambda I)\\hat{\\mathbf{x}} = \\Sigma^{\\top}\\hat{\\mathbf{y}}$$\n该向量方程的第 $j$ 个分量是：\n$$(\\sigma_{j}^{2} + \\lambda)\\hat{x}_{j} = \\sigma_{j}\\hat{y}_{j}$$\n其中 $\\hat{x}_j$ 和 $\\hat{y}_j$ 分别是 $\\hat{\\mathbf{x}}$ 和 $\\hat{\\mathbf{y}}$ 的第 $j$ 个分量，并且我们定义对于 $j > r$ 有 $\\sigma_j=0$。这给出了分量形式的解：\n$$\\hat{x}_{j} = \\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}\\hat{y}_{j} = \\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(u_{j}^{\\top}\\mathbf{y})$$\n完整的解 $\\mathbf{x}_{\\lambda}$ 可以通过从 $V$ 基变换回来重构：\n$$\\mathbf{x}_{\\lambda} = V\\hat{\\mathbf{x}} = \\sum_{j=1}^{n} \\hat{x}_{j} v_{j} = \\sum_{j=1}^{r} \\left(\\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(u_{j}^{\\top}\\mathbf{y})\\right)v_{j}$$\n现在我们使用问题中给出的数据向量的特定形式：$\\mathbf{y} = \\beta_{i} u_{i}$，其中 $i \\in \\{1, \\dots, r\\}$ 是一个固定索引。\n我们计算内积 $u_{j}^{\\top}\\mathbf{y}$：\n$$u_{j}^{\\top}\\mathbf{y} = u_{j}^{\\top}(\\beta_{i}u_{i}) = \\beta_{i}(u_{j}^{\\top}u_{i})$$\n由于 $U$ 的列是正交的，我们有 $u_{j}^{\\top}u_{i} = \\delta_{ij}$，即克罗内克（Kronecker）δ。因此，内积仅在 $j = i$ 时非零：\n$$u_{j}^{\\top}\\mathbf{y} = \\beta_{i}\\delta_{ij}$$\n将此代入 $\\mathbf{x}_{\\lambda}$ 的表达式中：\n$$\\mathbf{x}_{\\lambda} = \\sum_{j=1}^{r} \\left(\\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(\\beta_{i}\\delta_{ij})\\right)v_{j}$$\n克罗内克 δ 使求和塌缩为 $j=i$ 的单个项：\n$$\\mathbf{x}_{\\lambda} = \\left(\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}\\right)v_{i}$$\n问题将衰减因子 $f(\\sigma_{i}, \\lambda)$ 定义为 $\\mathbf{x}_{\\lambda}$ 中 $v_{i}$ 的系数与数据系数 $\\beta_{i}$ 之间的比率。\n从 $\\mathbf{x}_{\\lambda}$ 的表达式中，$v_{i}$ 的系数是 $\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}$。\n数据系数给定为 $\\beta_{i}$。\n该比率为：\n$$f(\\sigma_{i}, \\lambda) = \\frac{\\text{coefficient of } v_{i}}{\\text{data coefficient } \\beta_{i}} = \\frac{\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}}{\\beta_{i}}$$\n假设 $\\beta_{i} \\neq 0$（否则问题是平凡的），我们从分子和分母中消去 $\\beta_{i}$：\n$$f(\\sigma_{i}, \\lambda) = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda}$$\n这就是所指定衰减因子的最终闭式解析表达式。",
            "answer": "$$\\boxed{\\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda}}$$"
        },
        {
            "introduction": "正则化并非没有代价，它在稳定解的同时也引入了偏差。这个练习 () 是一个重要的思辨性实践，旨在揭示正则化固有的权衡。在一个简化的无噪声场景中，我们将探讨正则化偏差与真实解范数之间的关系。您会发现，即使没有噪声，如果真实解的范数非常大，正则化也可能导致显著的偏差，这有助于我们更批判性地看待并应用正则化方法。",
            "id": "3283983",
            "problem": "考虑由 $y = A x_{\\text{true}}$ 建模的无噪声数据的线性逆问题，其中 $A \\in \\mathbb{R}^{n \\times n}$ 且 $x_{\\text{true}} \\in \\mathbb{R}^{n}$。吉洪诺夫（Tikhonov）正则化估计 $x_{\\lambda}$ 定义为目标函数\n$$\nJ(x) = \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2,\n$$\n的最小值点，其中 $\\lambda > 0$ 是正则化参数，$\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。假设特殊情况 $A = I_n$（$n \\times n$ 的单位矩阵），因此 $y = x_{\\text{true}}$。\n\n问题关注的是，即使在没有噪声的情况下，当 $\\lVert x_{\\text{true}} \\rVert_2$ 非常大时，吉洪诺夫正则化是否会以及如何失效。以下哪个陈述是正确的？\n\nA) 在这种情况下，对于所有 $\\lambda > 0$，$x_{\\lambda} = x_{\\text{true}}$，因此吉洪诺夫正则化不引入偏差。\n\nB) 在这种情况下，$x_{\\lambda}$ 与 $x_{\\text{true}}$ 相差一个偏差向量 $b_{\\lambda}$，其范数满足 $\\lVert b_{\\lambda} \\rVert_2 = \\dfrac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2$，因此绝对误差随 $\\lVert x_{\\text{true}} \\rVert_2$ 线性增长。\n\nC) 对于固定的 $\\lambda > 0$，当 $\\lVert x_{\\text{true}} \\rVert_2 \\to \\infty$ 时，相对误差 $\\dfrac{\\lVert x_{\\lambda} - x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2}$ 趋向于 $0$，因此对于大信号，吉洪诺夫正则化变得精确。\n\nD) 如果 $A$ 具有标准正交列（因此 $A^{\\mathsf T} A = I_n$），那么即使在无噪声数据 $y = A x_{\\text{true}}$ 的情况下，吉洪诺夫估计也等于非正则化的最小二乘解，且与 $\\lambda$ 无关。\n\nE) 在 $A = I_n$ 的情况下，吉洪诺夫估计将 $y$ 的每个分量都按相同因子收缩，这种收缩导致了低估，即使噪声趋于零，这种低估也不会消失。\n\n选择所有正确的陈述。",
            "solution": "### 问题验证\n\n**第一步：提取已知条件**\n- 问题是一个由 $y = A x_{\\text{true}}$ 建模的无噪声数据的线性逆问题。\n- 矩阵 $A$ 在 $\\mathbb{R}^{n \\times n}$ 中，真实解 $x_{\\text{true}}$ 在 $\\mathbb{R}^{n}$ 中。\n- 吉洪诺夫正则化估计 $x_{\\lambda}$ 是最小化以下目标函数的解：\n$$\nJ(x) = \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2\n$$\n- 正则化参数 $\\lambda$ 是正的，即 $\\lambda > 0$。\n- 符号 $\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。\n- 对于主要问题，假设一个特殊情况：$A = I_n$（$n \\times n$ 单位矩阵）。\n- 在这种特殊情况下，数据就是真实解：$y = x_{\\text{true}}$。\n- 问题是关于在没有噪声的情况下，当 $\\lVert x_{\\text{true}} \\rVert_2$ 非常大时吉洪诺夫正则化的行为。\n\n**第二步：使用提取的已知条件进行验证**\n- **科学基础（关键）**：该问题植根于逆问题和正则化理论这一成熟的数学领域。吉洪诺夫正则化是一种标准且基础的技术。所有概念都基于线性代数和微积分。该问题在科学上和数学上都是合理的。\n- **适定性**：该问题是适定的。对于 $\\lambda > 0$，目标函数 $J(x)$ 是严格凸的，保证了唯一的最小值点 $x_{\\lambda}$。问题要求在特定条件下对这个唯一解进行分析。\n- **客观性（关键）**：语言精确、量化，没有主观或含糊的术语。所有术语如“欧几里得范数”、“单位矩阵”和目标函数都有正式定义。\n- **不完整或矛盾的设置**：设置是完整且自洽的。针对问题的具体情境，明确陈述了假设 $A = I_n$ 和 $y=x_{\\text{true}}$。\n- **其他缺陷**：该问题没有表现出任何其他缺陷，例如不切实际、不适定、琐碎或无法验证。这是数值分析中一个标准的概念性问题。\n\n**第三步：结论与行动**\n问题陈述是有效的。我将继续进行解的推导。\n\n### 解的推导\n\n吉洪诺夫正则化解 $x_{\\lambda}$ 是最小化目标函数的向量 $x$：\n$$\nJ(x) = \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_2^2\n$$\n为了找到最小值点，我们计算 $J(x)$ 关于 $x$ 的梯度并将其设为零。\n$$\nJ(x) = (Ax - y)^{\\mathsf T}(Ax - y) + \\lambda x^{\\mathsf T}x = x^{\\mathsf T}A^{\\mathsf T}Ax - 2y^{\\mathsf T}Ax + y^{\\mathsf T}y + \\lambda x^{\\mathsf T}x\n$$\n梯度为：\n$$\n\\nabla_x J(x) = 2A^{\\mathsf T}Ax - 2A^{\\mathsf T}y + 2\\lambda x\n$$\n将梯度设为零，$\\nabla_x J(x) = 0$，得到吉洪诺夫正则化的正规方程：\n$$\n(A^{\\mathsf T}A + \\lambda I)x = A^{\\mathsf T}y\n$$\n其中 $I$ 是适当大小的单位矩阵。因此解为：\n$$\nx_{\\lambda} = (A^{\\mathsf T}A + \\lambda I)^{-1} A^{\\mathsf T}y\n$$\n问题指定了特殊情况，即 $A = I_n$，因此 $y = A x_{\\text{true}} = I_n x_{\\text{true}} = x_{\\text{true}}$。将这些代入通解中：\n$$\nA^{\\mathsf T}A + \\lambda I = I_n^{\\mathsf T}I_n + \\lambda I_n = I_n + \\lambda I_n = (1+\\lambda)I_n\n$$\n$$\nA^{\\mathsf T}y = I_n^{\\mathsf T}x_{\\text{true}} = x_{\\text{true}}\n$$\n正规方程变为：\n$$\n(1+\\lambda)I_n x = x_{\\text{true}}\n$$\n求解 $x$，即我们的吉洪诺夫估计 $x_{\\lambda}$，我们得到：\n$$\nx_{\\lambda} = \\frac{1}{1+\\lambda} x_{\\text{true}}\n$$\n这个表达式构成了评估每个选项的基础。\n\n### 逐项分析\n\n**A) 在这种情况下，对于所有 $\\lambda > 0$，$x_{\\lambda} = x_{\\text{true}}$，因此吉洪诺夫正则化不引入偏差。**\n根据我们的推导，$x_{\\lambda} = \\frac{1}{1+\\lambda} x_{\\text{true}}$。因为问题陈述 $\\lambda > 0$，标量因子 $\\frac{1}{1+\\lambda}$ 严格小于 $1$。因此，除非 $x_{\\text{true}}=0$，否则 $x_{\\lambda}$ 不等于 $x_{\\text{true}}$。估计值 $x_\\lambda$ 和真实值 $x_{\\text{true}}$ 之间的差就是偏差。在这种情况下，偏差为 $x_{\\lambda} - x_{\\text{true}} = (\\frac{1}{1+\\lambda} - 1)x_{\\text{true}} = \\frac{-\\lambda}{1+\\lambda}x_{\\text{true}}$，它不是零。因此，吉洪诺夫正则化引入了偏差。\n**结论：不正确。**\n\n**B) 在这种情况下，$x_{\\lambda}$ 与 $x_{\\text{true}}$ 相差一个偏差向量 $b_{\\lambda}$，其范数满足 $\\lVert b_{\\lambda} \\rVert_2 = \\dfrac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2$，因此绝对误差随 $\\lVert x_{\\text{true}} \\rVert_2$ 线性增长。**\n偏差向量定义为 $b_{\\lambda} = x_{\\lambda} - x_{\\text{true}}$。如同在选项 A 中计算的，\n$$\nb_{\\lambda} = \\frac{1}{1+\\lambda}x_{\\text{true}} - x_{\\text{true}} = \\left(\\frac{1}{1+\\lambda} - \\frac{1+\\lambda}{1+\\lambda}\\right)x_{\\text{true}} = \\frac{-\\lambda}{1+\\lambda}x_{\\text{true}}\n$$\n这个偏差向量的范数就是绝对误差：\n$$\n\\lVert b_{\\lambda} \\rVert_2 = \\left\\lVert \\frac{-\\lambda}{1+\\lambda}x_{\\text{true}} \\right\\rVert_2 = \\left| \\frac{-\\lambda}{1+\\lambda} \\right| \\lVert x_{\\text{true}} \\rVert_2\n$$\n因为 $\\lambda > 0$，我们有 $\\left| \\frac{-\\lambda}{1+\\lambda} \\right| = \\frac{\\lambda}{1+\\lambda}$。所以，\n$$\n\\lVert b_{\\lambda} \\rVert_2 = \\frac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2\n$$\n这个方程表明，对于固定的 $\\lambda > 0$，绝对误差 $\\lVert b_{\\lambda} \\rVert_2$ 与 $\\lVert x_{\\text{true}} \\rVert_2$ 成正比。这就构成了线性增长。\n**结论：正确。**\n\n**C) 对于固定的 $\\lambda > 0$，当 $\\lVert x_{\\text{true}} \\rVert_2 \\to \\infty$ 时，相对误差 $\\dfrac{\\lVert x_{\\lambda} - x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2}$ 趋向于 $0$，因此对于大信号，吉洪诺夫正则化变得精确。**\n相对误差是绝对误差与真实解范数的比值。使用选项 B 的结果，对于 $x_{\\text{true}} \\neq 0$：\n$$\n\\text{相对误差} = \\frac{\\lVert x_{\\lambda} - x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2} = \\frac{\\frac{\\lambda}{1+\\lambda} \\lVert x_{\\text{true}} \\rVert_2}{\\lVert x_{\\text{true}} \\rVert_2} = \\frac{\\lambda}{1+\\lambda}\n$$\n对于固定的 $\\lambda > 0$，这个值是一个正常数。它不依赖于 $\\lVert x_{\\text{true}} \\rVert_2$，因此当 $\\lVert x_{\\text{true}} \\rVert_2 \\to \\infty$ 时，它不会趋向于 $0$。该陈述是错误的。\n**结论：不正确。**\n\n**D) 如果 $A$ 具有标准正交列（因此 $A^{\\mathsf T} A = I_n$），那么即使在无噪声数据 $y = A x_{\\text{true}}$ 的情况下，吉洪诺夫估计也等于非正则化的最小二乘解，且与 $\\lambda$ 无关。**\n这个选项提出了一个与主问题不同的情景。在这里，$A$ 是一个具有标准正交列的 $n \\times n$ 矩阵，这意味着 $A$ 是一个正交矩阵，所以 $A^{\\mathsf T}A = I_n$。\n吉洪诺夫估计是 $x_{\\lambda} = (A^{\\mathsf T}A + \\lambda I_n)^{-1}A^{\\mathsf T}y$。代入 $A^{\\mathsf T}A = I_n$：\n$$\nx_{\\lambda} = (I_n + \\lambda I_n)^{-1}A^{\\mathsf T}y = ((1+\\lambda)I_n)^{-1}A^{\\mathsf T}y = \\frac{1}{1+\\lambda} A^{\\mathsf T}y\n$$\n非正则化的最小二乘解 $x_{LS}$ 最小化 $\\lVert Ax-y \\rVert_2^2$。相应的正规方程是 $A^{\\mathsf T}Ax = A^{\\mathsf T}y$。代入 $A^{\\mathsf T}A = I_n$：\n$$\nI_n x_{LS} = A^{\\mathsf T}y \\implies x_{LS} = A^{\\mathsf T}y\n$$\n比较这两个解，我们看到 $x_{\\lambda} = \\frac{1}{1+\\lambda} x_{LS}$。由于 $\\lambda > 0$，因子 $\\frac{1}{1+\\lambda}$ 不等于 $1$。因此，$x_{\\lambda} \\neq x_{LS}$（除非 $x_{LS}=0$）。吉洪诺夫估计依赖于 $\\lambda$。\n**结论：不正确。**\n\n**E) 在 $A = I_n$ 的情况下，吉洪诺夫估计将 $y$ 的每个分量都按相同因子收缩，这种收缩导致了低估，即使噪声趋于零，这种低估也不会消失。**\n设定是 $A=I_n$，所以 $y=x_{\\text{true}}$。我们推导出的解是 $x_{\\lambda} = \\frac{1}{1+\\lambda} x_{\\text{true}}$。这可以重写为 $x_{\\lambda} = \\frac{1}{1+\\lambda} y$。\n这个方程意味着对于任何分量 $i$，$(x_{\\lambda})_i = \\frac{1}{1+\\lambda} y_i$。这是对向量 $y$ 的每个分量按因子 $\\frac{1}{1+\\lambda}$ 进行的均匀收缩。由于 $\\lambda>0$，这个因子小于 $1$，导致估计值 $\\lVert x_{\\lambda} \\rVert_2$ 的大小小于真实解 $\\lVert x_{\\text{true}} \\rVert_2$ 的大小，这是一种低估。问题是在无噪声环境下制定的。偏差，或“低估”，是 $b_{\\lambda} = -\\frac{\\lambda}{1+\\lambda}x_{\\text{true}}$。这种偏差是正则化所固有的，即使在零噪声下也存在。它是 $\\lambda$ 和 $x_{\\text{true}}$ 的函数，而不是噪声的函数。因此，当噪声趋于零时，这种低估不会消失（它在零噪声时就存在）。\n**结论：正确。**",
            "answer": "$$\\boxed{BE}$$"
        }
    ]
}