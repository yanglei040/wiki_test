{
    "hands_on_practices": [
        {
            "introduction": "差异原则是在已知噪声水平时，为迭代正则化方法选择停止时机的基石。这项实践将引导您在一个清晰的线性问题背景下实现此规则，并探索迭代终止点对所用噪声模型准确性的敏感度。通过系统性地改变估计的噪声协方差，您将亲身体验模型设定不准确如何影响解的质量，这是数据同化领域一个核心的实践挑战 。",
            "id": "3423231",
            "problem": "考虑数据同化中的线性观测模型，其中测量向量 $y \\in \\mathbb{R}^m$ 由 $y = A x^\\star + \\varepsilon$ 给出。矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知的正演算子，$x^\\star \\in \\mathbb{R}^n$ 是未知状态，$\\varepsilon \\in \\mathbb{R}^m$ 是加性噪声，其被建模为零均值的多元高斯分布，协方差为 $\\Gamma \\in \\mathbb{R}^{m \\times m}$，即 $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$。在第 $k$ 次迭代时的加权残差为 $r_k = y - A x_k$。使用估计的协方差 $\\Gamma_{\\text{est}}$ 定义加权残差统计量为 $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$。\n\n在假设 $x_k$ 能够充分解释数据且 $\\Gamma_{\\text{est}} = \\Gamma$ 的条件下，统计量 $S_k(\\Gamma_{\\text{est}})$ 近似服从具有 $m$ 个自由度的卡方随机变量分布。一个常见的基于差异的停止准则是，在最早的迭代 $k$ 处终止，使得 $S_k(\\Gamma_{\\text{est}})$ 低于具有 $m$ 个自由度的卡方分布在概率 $0.95$ 处的上分位数。目标是计算出现这种情况的最早迭代指数，并论证使用 $\\Gamma_{\\text{est}}$ 而非 $\\Gamma$ 所引入的I型和II型错误之间的权衡。\n\n使用以下设置使计算具体化且可复现：\n\n- 设 $m = 500$ 且 $n = 50$。\n- 生成矩阵 $A$，其元素从标准正态分布中独立抽取，然后将每列归一化为单位欧几里得范数。\n- 生成真实协方差 $\\Gamma$，它是一个对角矩阵，其元素为 $\\Gamma_{ii} = 0.5 + 1.5 u_i$，其中 $u_i$ 是从 $[0,1]$ 上的均匀分布中独立抽取的。\n- 生成真实状态 $x^\\star$，其元素从标准正态分布中独立抽取。\n- 生成噪声向量 $\\varepsilon$，其为独立的高斯项，方差等于 $\\Gamma$ 相应的对角线元素。\n- 构造 $y = A x^\\star + \\varepsilon$。\n- 初始化 $x_0 = 0$，并使用协方差加权的 Landweber 迭代进行迭代\n  $$x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k,$$\n  其中 $r_k = y - A x_k$，步长 $\\alpha$ 选择为\n  $$\\alpha = \\frac{0.95}{\\lambda_{\\max}\\!\\left(A^\\top \\Gamma_{\\text{est}}^{-1} A\\right)},$$\n  其中 $\\lambda_{\\max}(\\cdot)$ 表示最大特征值。对于具有 Lipschitz 连续梯度的凸二次目标，此选择可确保收敛。\n\n- 使用具有 $m$ 个自由度的卡方分布在概率 $0.95$ 处的上分位数作为停止阈值，记为 $q_{0.95}(m)$：\n  $$q_{0.95}(m) = \\inf\\{q \\in \\mathbb{R} : \\mathbb{P}(\\chi^2_m \\le q) \\ge 0.95\\}.$$\n\n- 迭代方法最多应运行 $K_{\\max} = 200$ 次迭代。最早的迭代指数是满足 $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ 的最小 $k \\in \\{0,1,2,\\dots,K_{\\max}\\}$。如果在 $K_{\\max}$ 之前不存在这样的 $k$，则报告 $-1$。\n\n为了评估停止准则的不同方面，使用一个测试套件，该套件通过一个标量误定因子 $s$ 来改变估计的协方差，定义 $\\Gamma_{\\text{est}} = s \\Gamma$：\n\n- 测试用例 1：$s = 1.0$ (正确指定的协方差，一般情况)。\n- 测试用例 2：$s = 2.0$ (过分散的估计协方差，更容易停止；保守停止的边界)。\n- 测试用例 3：$s = 0.5$ (欠分散的估计协方差；难以停止)。\n- 测试用例 4：$s = 0.25$ (严重欠分散的估计协方差；可能在 $K_{\\max}$ 内永远不会停止的边界情况)。\n\n所有伪随机抽样必须使用等于 $314159$ 的固定种子生成，以确保可复现性。此问题不涉及物理单位。输入或输出中不出现角度和百分比。\n\n您的程序应为每个测试用例计算满足 $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ 的最早迭代指数 $k$，如果到 $K_{\\max}$ 仍未达到，则返回 $-1$。您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[k_1,k_2,k_3,k_4]$）。",
            "solution": "问题陈述经过验证。\n\n### 步骤1：提取给定条件\n- **模型**：线性观测模型 $y = A x^\\star + \\varepsilon$。\n- **维度**：$m = 500$ (测量空间)，$n = 50$ (状态空间)。\n- **正演算子**：$A \\in \\mathbb{R}^{m \\times n}$，其元素从 $\\mathcal{N}(0, 1)$ 生成，并且各列被归一化为单位欧几里得范数。\n- **真实状态**：$x^\\star \\in \\mathbb{R}^n$，其元素来自 $\\mathcal{N}(0, 1)$。\n- **噪声模型**：$\\varepsilon \\in \\mathbb{R}^m$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$。\n- **真实噪声协方差**：$\\Gamma \\in \\mathbb{R}^{m \\times m}$ 为对角矩阵，其中 $\\Gamma_{ii} = 0.5 + 1.5 u_i$，而 $u_i \\sim U[0, 1]$。\n- **数据向量**：$y = A x^\\star + \\varepsilon$。\n- **迭代**：协方差加权的 Landweber 迭代：\n  $$x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k$$\n- **初始状态**：$x_0 = 0$。\n- **残差**：$r_k = y - A x_k$。\n- **估计协方差**：$\\Gamma_{\\text{est}} = s \\Gamma$，其中 $s$ 为一个标量误定因子。\n- **步长**：$\\alpha = \\frac{0.95}{\\lambda_{\\max}(A^\\top \\Gamma_{\\text{est}}^{-1} A)}$。\n- **停止统计量**：$S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$。\n- **停止准则**：在最小的 $k \\in \\{0, 1, \\dots, K_{\\max}\\}$ 处终止，使得 $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$。\n- **停止阈值**：$q_{0.95}(m)$ 是 $\\chi^2_m$ 分布在概率 $0.95$ 处的上分位数。\n- **最大迭代次数**：$K_{\\max} = 200$。\n- **失败时的返回值**：如果在 $K_{\\max}$ 次迭代内条件未满足，则返回 $-1$。\n- **随机种子**：$314159$。\n- **测试用例**：\n    - 用例 1：$s = 1.0$\n    - 用例 2：$s = 2.0$\n    - 用例 3：$s = 0.5$\n    - 用例 4：$s = 0.25$\n- **输出格式**：一个逗号分隔的最早迭代指数列表 $[k_1, k_2, k_3, k_4]$。\n\n### 步骤2：使用提取的给定条件进行验证\n1.  **科学或事实的健全性**：该问题在科学和数学上是健全的。它采用标准的线性反问题公式、一个著名的迭代求解器（Landweber 迭代，一种梯度下降形式）以及一个广泛使用的统计停止准则（基于卡方检验的差异性原理）。步长的选择在理论上是为了确保收敛。\n2.  **不可形式化或不相关**：该问题可直接形式化，并且是反问题和数据同化中迭代方法停止准则这一主题的核心。\n3.  **不完整或矛盾的设置**：问题是完全指定的。所有参数、常数和数据生成过程都已定义。没有矛盾之处。\n4.  **不切实际或不可行**：该设置是测试数值方法的常见且现实的模拟。指定的矩阵和向量生成在计算上是可行的。\n5.  **不适定或结构不良**：该问题是适定的。使用固定的随机种子确保了单一、确定性和可复现的解决方案。最大迭代次数防止了无限执行。\n6.  **伪深刻、微不足道或同义反复**：该问题并非微不足道。它需要实现一个数值算法，并理解迭代求解器、统计误差模型和停止准则之间的相互作用。协方差误定的分析是该领域的核心概念。\n7.  **超出科学可验证性**：该问题完全可以通过数学计算进行验证。\n\n### 步骤3：结论与行动\n问题被认为是**有效的**。将提供一个解决方案。\n\n### 基于原则的设计\n该问题要求实现一个迭代算法来解决一个线性反问题，其停止准则基于残差的统计特性。我们将首先按照规定生成合成数据，然后为每个测试用例实现迭代求解器，最后收集结果。\n\n**1. 数据生成**\n首先，我们使用指定的种子建立一个可复现的伪随机环境。我们只生成一次问题数据，因为它对所有测试用例都是通用的。\n- 正演算子 $A \\in \\mathbb{R}^{500 \\times 50}$ 的元素从标准正态分布生成。然后对其列进行归一化。列归一化至关重要，因为它标准化了状态向量 $x$ 的每个分量对测量值 $y$ 的影响。\n- 真实状态向量 $x^\\star \\in \\mathbb{R}^{50}$ 从标准正态分布中抽取。\n- 真实噪声协方差矩阵 $\\Gamma \\in \\mathbb{R}^{500 \\times 500}$ 是对角的。其对角线元素 $\\Gamma_{ii}$ 从一个缩放的均匀分布中抽取，确保了测量中存在异构的噪声方差，这是一个现实的场景。\n- 噪声向量 $\\varepsilon \\in \\mathbb{R}^{500}$ 从一个多元正态分布 $\\mathcal{N}(0, \\Gamma)$ 中生成。由于 $\\Gamma$ 是对角的，这等同于独立地从 $\\mathcal{N}(0, \\Gamma_{ii})$ 中生成每个分量 $\\varepsilon_i$。\n- 然后将测量向量合成为 $y = Ax^\\star + \\varepsilon$。\n\n**2. 迭代解法与停止准则**\n问题使用协方差加权的 Landweber 迭代。该方法是用于最小化加权最小二乘泛函 $J(x) = \\frac{1}{2} (y - Ax)^\\top \\Gamma_{\\text{est}}^{-1} (y - Ax)$ 的梯度下降算法。此泛函的梯度为 $\\nabla_x J(x) = -A^\\top \\Gamma_{\\text{est}}^{-1} (y - Ax)$。梯度下降更新为 $x_{k+1} = x_k - \\alpha \\nabla_x J(x_k)$，这就给出了指定的迭代公式：\n$$ x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k $$\n其中 $r_k = y - A x_k$ 是第 $k$ 次迭代的残差。\n\n选择步长 $\\alpha$ 以保证收敛。梯度 $\\nabla_x J(x)$ 是 Lipschitz 连续的，其常数为 $L = \\lambda_{\\max}(A^\\top \\Gamma_{\\text{est}}^{-1} A)$。为保证收敛，步长必须满足 $0  \\alpha  2/L$。选择 $\\alpha = 0.95/L$ 属于此范围，并提供了一个稳健的下降率。\n\n停止准则是差异性原理的一种实现。统计量 $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$ 测量了残差的马氏距离的平方。在理想假设下，即迭代 $x_k$ 已收敛到一个能够完美解释数据直到噪声水平的解（即 $r_k \\approx \\varepsilon$），并且估计的协方差是正确的（即 $\\Gamma_{\\text{est}} = \\Gamma$），量 $r_k^\\top \\Gamma^{-1} r_k$ 将近似于 $\\varepsilon^\\top \\Gamma^{-1} \\varepsilon$。后一个量服从具有 $m$ 个自由度的卡方分布 $\\chi^2_m$。停止规则 $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ 测试了加权残差范数在给定噪声模型下是否统计上合理。当残差小到在统计上与假定的噪声无法区分时，我们停止。使用 $95\\%$ 的分位数是为了提供一个高置信度的界限。\n\n**3. 协方差误定 ($s$) 的分析**\n这些测试用例探究了误定噪声协方差大小的影响。\n- **用例 $s = 1.0$ ($\\Gamma_{\\text{est}} = \\Gamma$)**：这是理想情况，其中用于反演的统计模型与现实相符。停止准则预计会按设计执行，当残差与真实噪声水平一致时停止迭代。\n- **用例 $s = 2.0$ ($\\Gamma_{\\text{est}} = 2\\Gamma$)**：这里，我们高估了噪声方差。这使得逆协方差 $\\Gamma_{\\text{est}}^{-1} = \\frac{1}{2}\\Gamma^{-1}$ 变小。停止统计量 $S_k$ 变为 $S_k(2\\Gamma) = r_k^\\top (2\\Gamma)^{-1} r_k = \\frac{1}{2} r_k^\\top \\Gamma^{-1} r_k$。对于给定的残差 $r_k$，该准则变得更容易满足。这会带来I型错误的风险：在 $x_k$ 还没有充分收敛到接近 $x^\\star$ 之前就过早停止，因为算法对大残差过于“宽容”。\n- **用例 $s = 0.5$ ($\\Gamma_{\\text{est}} = 0.5\\Gamma$)**：这里，我们低估了噪声方差。逆协方差 $\\Gamma_{\\text{est}}^{-1} = 2\\Gamma^{-1}$ 更大。统计量变为 $S_k(0.5\\Gamma) = 2 r_k^\\top \\Gamma^{-1} r_k$。现在该准则更难满足。算法对残差变得不那么“宽容”，并且可能在找到合理解决方案后继续迭代很长时间，试图拟合数据中的噪声。这会带来II型错误的风险：未能及时停止，可能导致过拟合。\n- **用例 $s = 0.25$ ($\\Gamma_{\\text{est}} = 0.25\\Gamma$)**：这是对噪声方差的严重低估。统计量 $S_k(0.25\\Gamma) = 4 r_k^\\top \\Gamma^{-1} r_k$ 被显著放大。很可能残差范数永远无法减小到足以满足停止准则的程度，特别是因为真实的噪声基底是 $\\mathbb{E}[\\varepsilon^\\top\\Gamma^{-1}\\varepsilon] = m$。停止准则将要求 $4 r_k^\\top \\Gamma^{-1} r_k \\le q_{0.95}(m)$，或 $r_k^\\top \\Gamma^{-1} r_k \\le q_{0.95}(m)/4$。对于 $m=500$，$q_{0.95}(500) \\approx 545$。这要求残差范数远小于其期望值，这可能是无法达到的。\n\n**4. 计算算法**\n对于测试套件中的每个 $s$ 值：\n1.  设置 $\\Gamma_{\\text{est}} = s \\Gamma$。由于 $\\Gamma$ 是对角的，$\\Gamma_{\\text{est}}$ 也是对角的，其逆矩阵可以轻易计算。\n2.  计算矩阵 $H = A^\\top \\Gamma_{\\text{est}}^{-1} A$。由于该矩阵是对称的，其最大特征值 $\\lambda_{\\max}(H)$ 可以高效计算。\n3.  计算步长 $\\alpha = 0.95 / \\lambda_{\\max}(H)$。\n4.  从 $\\chi^2_m$ 分布确定停止阈值 $q_{0.95}(m)$。\n5.  初始化 $x_k = \\vec{0}$ 并设置一个标志 `found_k = -1`。\n6.  从 $k=0$ 到 $K_{\\max}=200$ 开始迭代。\n7.  在每次迭代 $k$ 时，检查停止条件。如果 $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$，则将 $k$ 存储为结果，并中断此测试用例的内部循环。\n8.  如果条件不满足，则更新状态 $x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k$。\n9.  循环结束后，将找到的索引（如果未找到则为 $-1$）附加到结果列表中。\n这个过程将对所有四个 $s$ 值重复进行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the data assimilation problem with a chi-square stopping rule\n    for different covariance mis-specification factors.\n    \"\"\"\n    # Define parameters from the problem statement\n    m = 500\n    n = 50\n    K_max = 200\n    seed = 314159\n    \n    # Test cases for covariance mis-specification factor 's'\n    test_cases = [1.0, 2.0, 0.5, 0.25]\n\n    # --- 1. Data Generation (common for all test cases) ---\n    rng = np.random.default_rng(seed)\n    \n    # Generate A and normalize its columns\n    A = rng.normal(size=(m, n))\n    col_norms = np.linalg.norm(A, axis=0)\n    A = A / col_norms\n    \n    # Generate true state x_star\n    x_star = rng.normal(size=n)\n    \n    # Generate true diagonal covariance Gamma\n    u = rng.uniform(size=m)\n    gamma_diag = 0.5 + 1.5 * u\n    \n    # Generate noise epsilon from N(0, Gamma)\n    epsilon = rng.normal(loc=0.0, scale=np.sqrt(gamma_diag))\n    \n    # Form the measurement vector y\n    y = A @ x_star + epsilon\n    \n    # --- 2. Iteration and Stopping Criterion Evaluation ---\n    \n    # Calculate the chi-square stopping threshold\n    # q_{0.95}(m)\n    q_threshold = stats.chi2.ppf(0.95, df=m)\n    \n    results = []\n    \n    for s in test_cases:\n        # Initialize x_k for the current test case\n        x_k = np.zeros(n)\n        \n        # Define estimated covariance and its inverse\n        gamma_est_diag = s * gamma_diag\n        gamma_est_inv_diag = 1.0 / gamma_est_diag\n        \n        # Calculate the step size alpha\n        # H = A^T * Gamma_est^{-1} * A\n        # This is an efficient way to compute for diagonal Gamma_est^{-1}\n        # It's A.T @ (D * A) where D is the diagonal matrix\n        H = (A.T * gamma_est_inv_diag) @ A\n        \n        # The matrix H is symmetric, use eigvalsh for efficiency\n        lambda_max = np.linalg.eigvalsh(H)[-1]\n        alpha = 0.95 / lambda_max\n        \n        found_k = -1\n\n        for k in range(K_max + 1):\n            # Calculate residual r_k\n            r_k = y - A @ x_k\n            \n            # Calculate the stopping statistic S_k\n            # S_k = r_k^T * Gamma_est^{-1} * r_k\n            S_k = np.sum(r_k**2 * gamma_est_inv_diag)\n            \n            # Check the stopping criterion\n            if S_k = q_threshold:\n                found_k = k\n                break\n            \n            # If not stopping, perform the Landweber update\n            # We don't need to update if k == K_max, but it does no harm\n            grad = A.T @ (gamma_est_inv_diag * r_k)\n            x_k = x_k + alpha * grad\n\n        results.append(found_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了线性问题的基础后，我们将挑战更为复杂的非线性参数估计问题，这在真实世界的数据同化应用中极为常见。本练习将指导您设计一个混合终止准则，它巧妙地结合了基于卡方检验的差异原则与判断数值收敛的“小步长”条件。通过实现这一更为鲁棒的准则，您将学会如何在保证解的统计一致性的同时，避免不必要的计算，从而获得稳定且有意义的结果 。",
            "id": "3423267",
            "problem": "考虑一个非线性参数估计问题，其中 $m$ 个观测值 $y_i$ 被建模为一个受加性噪声污染的参数化正演模型 $g(x;t_i)$ 的输出。假设噪声为均值为 $0$、方差为 $\\sigma^2$ 的独立同分布 (IID) 高斯噪声，并定义残差向量 $r(x) \\in \\mathbb{R}^m$，其分量为 $r_i(x) = y_i - g(x;t_i)$，其中 $i \\in \\{1,\\dots,m\\}$。令缩放平方和为 $S(x) = \\|r(x)\\|_2^2 / \\sigma^2$。在所述假设下，在真实参数 $x^\\star$ 处计算的 $S(x^\\star)$ 服从自由度为 $m$ 的卡方分布。在用于反演问题和数据同化的迭代方法中，研究者寻求一种能在统计一致性与数值稳定性之间取得平衡的有原则的停止准则。设计一个组合停止准则，该准则同时使用卡方失配检验和小步长条件。具体而言，定义由高斯-牛顿型更新生成的一系列迭代值 $x_k \\in \\mathbb{R}^p$ ($p \\in \\mathbb{N}$)，并在满足卡方失配检验通过或步长足够小的最小索引 $k$ 处停止。失配检验使用自由度为 $m$ 的卡方分布的上 $(1-\\alpha)$-分位数 $q_{m,1-\\alpha}$，而小步长条件为 $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$，其中给定 $\\alpha \\in (0,1)$ 和 $\\varepsilon  0$。如果在预设的最大迭代次数 $k_{\\max}$ 内两个条件都未满足，则返回 $k_{\\max}$。\n\n为以下数据同化场景实现此停止准则。使用非线性正演模型 $g(x;t) = x_1 \\exp(-x_2 t)$，参数为 $x = (x_1,x_2) \\in \\mathbb{R}^2$。在第 $k$ 次迭代时，通过求解线性系统 $(J(x_k)^\\top J(x_k) + \\lambda I) s_k = - J(x_k)^\\top r(x_k)$ 来计算高斯-牛顿步长 $s_k \\in \\mathbb{R}^2$，其中 $J(x_k) \\in \\mathbb{R}^{m \\times 2}$ 是 $g$ 在 $x_k$ 处的雅可比矩阵，$I \\in \\mathbb{R}^{2 \\times 2}$ 是单位矩阵，$\\lambda  0$ 是一个固定的阻尼参数。使用 $x_{k+1} = x_k + s_k$ 进行更新。在整个过程中必须使用欧几里得范数 $\\|\\cdot\\|_2$。\n\n使用以下包含三个案例的测试套件来计算停止索引，该索引定义为满足 $S(x_k) \\le q_{m,1-\\alpha}$ 或 $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$ 的最小 $k \\in \\{0,1,\\dots,k_{\\max}\\}$；如果在 $k_{\\max}$ 次迭代内两个条件都未满足，则输出 $k_{\\max}$。对于每个案例，通过 $y_i = g(x^\\mathrm{true};t_i) + v_i$ 从真实参数 $x^\\mathrm{true}$ 和指定的噪声向量 $v \\in \\mathbb{R}^m$ 确定性地构建观测数据 $y_i$。\n\n案例 1（统计一致的噪声，预期失配检验会通过）：\n- $m = 20$，$t_i$ 在 $[0,2]$ 区间内线性分布，其中 $i \\in \\{1,\\dots,20\\}$，\n- $x^\\mathrm{true} = (1.0, 0.5)$，\n- $\\sigma = 0.05$，$v_i = \\sigma \\cdot 0.3 \\cdot (-1)^i$，\n- 初始猜测 $x_0 = (0.8, 0.8)$，\n- $\\alpha = 0.05$，$\\varepsilon = 10^{-6}$，$k_{\\max} = 50$，$\\lambda = 10^{-4}$。\n\n案例 2（噪声尺度不匹配，预期小步长条件会触发）：\n- $m = 20$，$t_i$ 在 $[0,2]$ 区间内线性分布，其中 $i \\in \\{1,\\dots,20\\}$，\n- $x^\\mathrm{true} = (1.0, 0.5)$，\n- $\\sigma = 0.02$，$v_i = 0.15 \\cos(2 t_i)$，\n- 初始猜测 $x_0 = (0.9, 0.3)$，\n- $\\alpha = 0.05$，$\\varepsilon = 10^{-6}$，$k_{\\max} = 100$，$\\lambda = 10^{-3}$。\n\n案例 3（在迭代上限前两个条件均未满足）：\n- $m = 5$，$t_i$ 在 $[0,1]$ 区间内线性分布，其中 $i \\in \\{1,\\dots,5\\}$，\n- $x^\\mathrm{true} = (1.0, 0.5)$，\n- $\\sigma = 0.01$，$v_i = 0.1 + 0.05 t_i$，\n- 初始猜测 $x_0 = (0.2, 1.5)$，\n- $\\alpha = 0.01$，$\\varepsilon = 10^{-12}$，$k_{\\max} = 3$，$\\lambda = 10^{-3}$。\n\n您的程序必须对每个案例使用所述的组合规则计算停止索引，并生成单行输出，其中包含三个整数索引，格式为方括号内逗号分隔的列表，例如 $[k_1,k_2,k_3]$。不涉及物理单位，所有角度（如有）必须作为不带角度单位的实数处理。最终输出为整数。",
            "solution": "该问题要求为应用于非线性参数估计问题的阻尼高斯-牛顿迭代法实现一个组合停止准则。解决方案涉及基于所提供的数学框架开发算法，并对三个不同的测试案例执行该算法。\n\n首先，我们形式化迭代算法的各个组成部分。正演模型由 $g(x;t) = x_1 \\exp(-x_2 t)$ 给出，其中 $x = (x_1, x_2)$ 是参数向量。雅可比矩阵 $J(x) \\in \\mathbb{R}^{m \\times 2}$ 的元素对应于模型函数关于参数的偏导数，并在每个时间点 $t_i$ 进行求值。雅可比矩阵的列为：\n$$\n\\frac{\\partial g}{\\partial x_1}(x;t) = \\exp(-x_2 t)\n$$\n$$\n\\frac{\\partial g}{\\partial x_2}(x;t) = -x_1 t \\exp(-x_2 t)\n$$\n对于给定的参数估计 $x$，残差向量为 $r(x)$，其分量为 $r_i(x) = y_i - g(x; t_i)$，其中 $y_i$ 是观测数据。量 $S(x) = \\|r(x)\\|_2^2 / \\sigma^2$ 是缩放的残差平方和，作为我们的失配函数。\n\n迭代过程从一个初始猜测 $x_0$ 开始，并为 $k=0, 1, 2, \\dots$ 生成一系列参数估计 $x_k$。在每次迭代 $k$ 中，通过计算一个步长 $s_k$ 并设置 $x_{k+1} = x_k + s_k$ 来找到下一个迭代值 $x_{k+1}$。步长 $s_k$ 通过求解阻尼正规方程获得，这是 Levenberg-Marquardt 算法（高斯-牛顿法的一种改进）的一个特点：\n$$\n(J(x_k)^\\top J(x_k) + \\lambda I) s_k = - J(x_k)^\\top r(x_k)\n$$\n这里，$J(x_k)$ 是在 $x_k$ 处求值的雅可比矩阵，$r(x_k)$ 是在 $x_k$ 处的残差，$I$ 是 $2 \\times 2$ 的单位矩阵，$\\lambda  0$ 是一个阻尼参数，可确保系统是良态的并促进稳定性。\n\n问题的核心是组合停止准则。迭代在第一个满足以下两个条件之一的索引 $k$（从 $k=0$ 开始）处停止：\n1.  **卡方失配检验**：$S(x_k) \\le q_{m,1-\\alpha}$。这是 Morozov 差异原则的一种形式。当模型的预测误差（失配）与数据中预期的噪声水平在统计上一致时，它会停止迭代。阈值 $q_{m,1-\\alpha}$ 是自由度为 $m$ 的卡方分布的 $(1-\\alpha)$ 分位数，在问题的假设下，这是真实参数 $x^\\star$ 处的 $S(x^\\star)$ 的分布。\n2.  **小步长条件**：$\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$。该条件表明算法已收敛到一个稳定点，因为后续迭代对参数向量产生的变化可以忽略不计。缩放因子 $(1 + \\|x_k\\|_2)$ 提供了一个相对容差，对大小参数范数均有效。\n\n整个算法按以下步骤进行，其中 $k = 0, 1, \\dots, k_{\\max}-1$：\n1.  给定当前迭代值 $x_k$，计算残差 $r(x_k)$ 和失配 $S(x_k) = \\|r(x_k)\\|_2^2 / \\sigma^2$。\n2.  检查失配检验：如果 $S(x_k) \\le q_{m,1-\\alpha}$，则停止索引为 $k$。终止。\n3.  如果失配检验失败，通过求解上述线性系统来计算高斯-牛顿步长 $s_k$。\n4.  检查小步长条件：如果 $\\|s_k\\|_2 \\le \\varepsilon(1 + \\|x_k\\|_2)$，则停止索引为 $k$。终止。\n5.  如果两个检验都失败，更新参数估计：$x_{k+1} = x_k + s_k$。继续下一次迭代，令 $k \\leftarrow k+1$。\n如果循环完成至 $k=k_{\\max}-1$ 仍未满足任何一个条件，则过程停止，返回的索引为 $k_{\\max}$。\n\n该算法将应用于指定的三个测试案例中的每一个，使用它们各自的数据和控制参数，以确定每种情况下的停止索引。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves for the stopping index in three data assimilation scenarios using a combined\n    stopping criterion for a damped Gauss-Newton iteration.\n    \"\"\"\n\n    def g(x, t):\n        \"\"\"\n        The nonlinear forward model g(x;t) = x1 * exp(-x2 * t).\n\n        Args:\n            x (np.ndarray): Parameter vector [x1, x2].\n            t (np.ndarray): Time points.\n\n        Returns:\n            np.ndarray: Model output.\n        \"\"\"\n        x1, x2 = x\n        return x1 * np.exp(-x2 * t)\n\n    def jacobian(x, t):\n        \"\"\"\n        Computes the Jacobian matrix of the forward model g(x;t).\n\n        Args:\n            x (np.ndarray): Parameter vector [x1, x2].\n            t (np.ndarray): Time points.\n\n        Returns:\n            np.ndarray: The m x 2 Jacobian matrix.\n        \"\"\"\n        x1, x2 = x\n        m = len(t)\n        J = np.zeros((m, 2))\n        exp_term = np.exp(-x2 * t)\n        J[:, 0] = exp_term\n        J[:, 1] = -x1 * t * exp_term\n        return J\n\n    def run_iteration(params):\n        \"\"\"\n        Executes the iterative algorithm for a single test case.\n        \"\"\"\n        m, t, x_true, sigma, v, x0, alpha, epsilon, k_max, lam = params\n\n        # Generate observed data y\n        y = g(x_true, t) + v\n\n        # Calculate the chi-square quantile for the misfit test\n        q_chi2 = chi2.ppf(1 - alpha, df=m)\n\n        # Identity matrix for the damped system\n        I = np.identity(2)\n\n        # Initialize the iteration\n        x_k = np.array(x0, dtype=float)\n        \n        for k in range(k_max):\n            # Compute residual and scaled sum of squares (misfit) at x_k\n            r_k = y - g(x_k, t)\n            S_k = np.linalg.norm(r_k)**2 / sigma**2\n\n            # 1. Chi-square Misfit Test\n            if S_k = q_chi2:\n                return k\n\n            # Compute the Jacobian at x_k\n            J_k = jacobian(x_k, t)\n\n            # Formulate and solve the damped linear system for the step s_k\n            # (J^T J + lambda*I) s_k = -J^T r_k\n            A = J_k.T @ J_k + lam * I\n            b = -J_k.T @ r_k\n            s_k = np.linalg.solve(A, b)\n\n            # 2. Small-Step Condition\n            step_norm = np.linalg.norm(s_k)\n            x_k_norm = np.linalg.norm(x_k)\n            threshold = epsilon * (1.0 + x_k_norm)\n            \n            if step_norm = threshold:\n                return k\n\n            # Update the parameter estimate for the next iteration\n            x_k += s_k\n            \n        # If loop completes without stopping, return k_max\n        return k_max\n\n    # Define the test cases from the problem statement.\n    case1_t = np.linspace(0, 2, 20)\n    case1_v = 0.05 * 0.3 * ((-1)**np.arange(1, 21))\n    \n    case2_t = np.linspace(0, 2, 20)\n    case2_v = 0.15 * np.cos(2 * case2_t)\n\n    case3_t = np.linspace(0, 1, 5)\n    case3_v = 0.1 + 0.05 * case3_t\n\n    test_cases = [\n        # Case 1\n        (20, case1_t, np.array([1.0, 0.5]), 0.05, case1_v, \n         np.array([0.8, 0.8]), 0.05, 1e-6, 50, 1e-4),\n        # Case 2\n        (20, case2_t, np.array([1.0, 0.5]), 0.02, case2_v, \n         np.array([0.9, 0.3]), 0.05, 1e-6, 100, 1e-3),\n        # Case 3\n        (5, case3_t, np.array([1.0, 0.5]), 0.01, case3_v,\n         np.array([0.2, 1.5]), 0.01, 1e-12, 3, 1e-3),\n    ]\n\n    results = []\n    for case in test_cases:\n        stop_index = run_iteration(case)\n        results.append(stop_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当我们缺乏可靠的噪声估计时，基于差异原则的停止准则便无法应用。这项实践探索了一种重要的替代方案：启发式方法，它通过监测迭代过程自身的“行为”来寻找最佳停止点。您将实现一个基于残差范数对数曲率的检测器，以识别迭代从拟合信号转向拟合噪声的“半收敛”现象，这为您提供了一种在不依赖严格统计假设的情况下对迭代进行正则化的强大工具 。",
            "id": "3423234",
            "problem": "考虑一个线性反问题：从满足 $y = A x^\\dagger + e$ 的噪声数据 $y \\in \\mathbb{R}^n$ 中恢复未知向量 $x^\\dagger \\in \\mathbb{R}^n$。其中，$A : \\mathbb{R}^n \\to \\mathbb{R}^n$ 是一个具有平滑作用的紧线性算子，$e$ 是一个均值为零的随机噪声向量。已知迭代梯度法会表现出半收敛现象：重构误差 $\\|x_k - x^\\dagger\\|_2$ 最初随迭代指数 $k$ 减小，然后在迭代开始拟合噪声后增加。需要一个实用的停止准则，以便在半收敛开始时终止迭代。您将设计并评估一个这样的准则，该准则通过追踪残差范数的对数与迭代指数的离散曲率来检测半收敛的开始。\n \n从以下在线性反问题和数据同化中有效的基础要素开始：\n- 用于最小化二次数据失配 $\\tfrac{1}{2}\\|A x - y\\|_2^2$ 的 Landweber 迭代更新公式为 $x_{k+1} = x_k + \\alpha A^\\top (y - A x_k)$，步长为 $\\alpha \\in (0, 2/\\|A\\|_2^2)$，其中 $\\|\\cdot\\|_2$ 对算子表示谱范数，对向量表示欧几里得范数，$A^\\top$ 是伴随算子。\n- 在第 $k$ 次迭代时的残差为 $r_k = y - A x_k$，其欧几里得范数为 $\\|r_k\\|_2$。\n- 序列 $s_k$ 的离散二阶有限差分为 $\\Delta^2 s_k = s_{k-1} - 2 s_k + s_{k+1}$，它捕捉了 $s_k$ 相对于离散指数 $k$ 的曲率。\n\n您将实现并评估一个基于 $s_k = \\log \\|r_k\\|_2$ 曲率的半收敛检测器：\n- 对 $k = 0, 1, \\dots, K_{\\max}$，计算 $s_k = \\log(\\|r_k\\|_2)$，并设置保护措施以确保对数参数严格为正。\n- 对 $k = 1, \\dots, K_{\\max} - 1$，计算中心离散曲率 $c_k = \\Delta^2 s_k = s_{k-1} - 2 s_k + s_{k+1}$。\n- 使用长度为 $w$ 的移动平均窗口对 $c_k$ 进行平滑，以形成 $\\tilde{c}_k$。\n- 从 $\\tilde{c}_k$ 的前 $K_{\\mathrm{burn}}$ 个曲率样本中估计基线均值 $\\mu_0$ 和标准差 $\\sigma_0$。\n- 定义一个曲率阈值检测器：检测指数 $k_{\\mathrm{det}}$ 是满足 $\\tilde{c}_k  \\mu_0 + \\gamma \\sigma_0$ 和 $\\tilde{c}_k  0$ 的最小 $k \\in \\{K_{\\mathrm{burn}}, \\dots, K_{\\max}-2\\}$。如果不存在这样的 $k$，则设 $k_{\\mathrm{det}} = K_{\\max} + 1$（表示未检测到）。\n\n对于前向算子 $A$，使用周期性一维卷积 $A x = h \\ast x$，其中 $h$ 是长度为 $n$、标准差为 $\\sigma_h$ 的对称高斯核，并进行归一化以使 $\\sum_{i=1}^n h_i = 1$。通过离散傅里叶变换使用循环卷积来实现 $A$ 和 $A^\\top$，由于对称性，$A^\\top = A$。选择 Landweber 步长 $\\alpha = \\frac{1.9}{\\|A\\|_2^2}$，其中 $\\|A\\|_2$ 等于在周期性边界条件下 $h$ 的离散傅里叶变换的最大模。初始化 $x_0 = 0$ 并运行 $K_{\\max}$ 次迭代。\n\n构建一个非平凡的基准真相 $x^\\dagger$，它由多种特征混合而成（例如，一个块、一个高斯凸起和几个尖峰），确保 $x^\\dagger \\in \\mathbb{R}^n$ 不恒为零。生成 $y^\\dagger = A x^\\dagger$ 并用噪声 $e$ 对其进行扰动，以获得 $y = y^\\dagger + e$。\n\n需要考虑的噪声模型：\n- 白高斯噪声：$e \\sim \\mathcal{N}(0, \\sigma^2 I)$。\n- 具有一阶自回归相关性的有色噪声（一阶自回归 (AR(1))）：对于空间指数 $i = 1, \\dots, n$，构建一个满足 $e_i = \\rho e_{i-1} + \\eta_i$ 的平稳过程，其中 $\\eta_i \\sim \\mathcal{N}(0, \\tau^2)$ 是独立同分布的，$|\\rho|  1$，并且选择 $\\tau$ 以使 $\\mathrm{Var}(e_i)$ 达到指定尺度。最后，重新缩放实现的 $e$ 以强制达到规定的相对噪声水平 $\\varepsilon = \\|e\\|_2 / \\|y^\\dagger\\|_2$。\n- 零噪声边界情况：设置 $\\varepsilon = 0$，此时 $y = y^\\dagger$。\n\n对于每个噪声实例，将最优迭代指数 $k_{\\mathrm{opt}}$ 定义为在 $k \\in \\{0, 1, \\dots, K_{\\max}\\}$ 上重构误差 $\\|x_k - x^\\dagger\\|_2$ 的最小化子。如果 $k_{\\mathrm{det}}  k_{\\mathrm{opt}} - \\delta$，则称该检测为假阳性，其中 $\\delta$ 是一个非负的迭代容差。\n\n需要实现的任务：\n- 按照上述规定实现 Landweber 迭代、基于残差曲率的检测器和噪声生成器。\n- 对下文的每个测试用例，进行 $N_{\\mathrm{trials}}$ 次独立试验，并估计假阳性率，即发生假阳性的试验所占的比例。同时，在发生检测的试验中（即 $k_{\\mathrm{det}} \\le K_{\\max}$），计算平均有符号检测偏移量 $\\overline{\\Delta k}$，其定义为在已检测到的试验中 $k_{\\mathrm{det}} - k_{\\mathrm{opt}}$ 的算术平均值。如果没有试验发生检测，则定义 $\\overline{\\Delta k} = 0$。\n\n测试套件和参数：\n- 使用 $n = 128$，$\\sigma_h = 2.0$，$K_{\\max} = 150$，窗口长度 $w = 5$，预烧期 $K_{\\mathrm{burn}} = 10$，阈值参数 $\\gamma = 3.0$，容差 $\\delta = 2$，$N_{\\mathrm{trials}} = 20$。使用欧几里得范数和自然对数。\n- 测试用例 1（理想情况）：相对水平为 $\\varepsilon = 0.02$ 的白噪声。\n- 测试用例 2（有色噪声）：AR(1) 参数为 $\\rho = 0.8$，相对水平为 $\\varepsilon = 0.02$。\n- 测试用例 3（强有色噪声）：AR(1) 参数为 $\\rho = 0.95$，相对水平为 $\\varepsilon = 0.02$。\n- 测试用例 4（边界情况）：零噪声，$\\varepsilon = 0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按以下顺序且不含空格：$[\\mathrm{fp}_1,\\overline{\\Delta k}_1,\\mathrm{fp}_2,\\overline{\\Delta k}_2,\\mathrm{fp}_3,\\overline{\\Delta k}_3,\\mathrm{fp}_4,\\overline{\\Delta k}_4]$，其中 $\\mathrm{fp}_j$ 是测试用例 $j$ 的估计假阳性率（以小数表示），$\\overline{\\Delta k}_j$ 是测试用例 $j$ 的如上定义的平均有符号检测偏移量。\n- 本问题不涉及物理单位。不出现角度。所有概率或比率必须以小数表示，而不是百分比。\n\n您的程序必须是完全确定性和自包含的，使用您选择的固定伪随机数生成器种子，以确保跨次运行的可复现性。",
            "solution": "所提出的问题是有效的。它在科学上基于线性反问题和迭代正则化的既定理论，特别是使用了 Landweber 方法。该问题作为一个数值实验是适定的，所有参数、算法和评估指标都以清晰、客观和自洽的方式指定。没有矛盾、歧义或违反科学原则之处。任务是为迭代求解器实现并评估一个特定的停止准则，这是数值分析和数据同化中的一个标准且相关的主题。因此，我将继续提供完整的解决方案。\n\n问题的核心是通过提前终止迭代方案来正则化不适定线性系统 $y = A x^\\dagger + e$ 的解。所选择的迭代方法是 Landweber 迭代，这是一种应用于最小化数据失配泛函 $J(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ 的梯度下降形式。$J(x)$ 的梯度是 $\\nabla J(x) = A^\\top(A x - y)$，从而得到更新规则：\n$$\nx_{k+1} = x_k - \\alpha \\nabla J(x_k) = x_k + \\alpha A^\\top (y - A x_k)\n$$\n其中 $k$ 是迭代指数，$\\alpha$ 是步长。为保证收敛，步长必须满足 $\\alpha \\in (0, 2/\\|A\\|_2^2)$，其中 $\\|A\\|_2$ 是算子 $A$ 的谱范数。问题指定步长为 $\\alpha = 1.9/\\|A\\|_2^2$。\n\n在带噪声数据的不适定问题的迭代解法中，一个关键现象是半收敛。重构误差 $\\|x_k - x^\\dagger\\|_2$ 最初随着迭代解 $x_k$ 逼近真实解 $x^\\dagger$ 而减小。然而，经过一定次数的迭代后，迭代解开始拟合数据 $y$ 中的噪声分量 $e$。因为算子 $A$ 具有平滑作用，其逆（或伪逆）是放大作用的。这种放大作用于噪声，导致误差 $\\|x_k - x^\\dagger\\|_2$ 增长，这种现象被称为噪声放大。最优迭代指数 $k_{\\mathrm{opt}}$ 是使该误差最小化的指数：$k_{\\mathrm{opt}} = \\arg\\min_k \\|x_k - x^\\dagger\\|_2$。停止准则的目标是在不知道真实解 $x^\\dagger$ 的情况下估计 $k_{\\mathrm{opt}}$。\n\n所提出的停止准则基于残差范数 $\\|r_k\\|_2 = \\|y - A x_k\\|_2$ 的行为。随着迭代的进行，残差范数减小。当迭代从拟合信号过渡到拟合噪声时，这种下降的速率会发生变化。该准则通过监控序列 $s_k = \\log \\|r_k\\|_2$ 的曲率来形式化这一点。一个大的正曲率表明对数残差的衰减正在突然减慢，这被视为噪声放大开始的一个启发式指标。\n\n实现计划如下：\n\n1.  **算子构建**：前向算子 $A$ 是与对称高斯核 $h$ 的一维循环卷积。根据卷积定理，卷积的傅里叶变换是傅里叶变换的乘积。因此，可以使用快速傅里叶变换 (FFT) 高效计算 $A$ 的作用：$A x = \\mathcal{F}^{-1}(\\hat{h} \\cdot \\hat{x})$，其中 $\\mathcal{F}$ 表示 FFT，$\\hat{h}, \\hat{x}$ 分别是 $h$ 和 $x$ 的傅里叶表示。因为核 $h$ 是实数且对称的，所以算子 $A$ 是自伴的，即 $A^\\top = A$。谱范数 $\\|A\\|_2$ 是 $A$ 的最大奇异值。对于由卷积定义的循环矩阵，奇异值是特征值的模，即传递函数 $\\hat{h}$ 的各项。因此，$\\|A\\|_2 = \\max_j |\\hat{h}_j|$。\n\n2.  **模拟循环**：对于每个测试用例，我们执行 $N_{\\mathrm{trials}}=20$ 次独立模拟。在每次试验中：\n    a. 根据测试用例（白噪声、AR(1) 或零噪声）生成一个特定的噪声向量 $e$，并对其进行缩放以确保相对噪声水平 $\\varepsilon = \\|e\\|_2 / \\|y^\\dagger\\|_2$，其中 $y^\\dagger = A x^\\dagger$。AR(1) 噪声通过滤波一个白噪声序列生成，并移除一个预烧期以确保平稳性。\n    b. 从 $x_0 = 0$ 开始，运行 Landweber 迭代 $K_{\\max}=150$ 步。在每一步 $k$，我们存储重构误差 $\\|x_k - x^\\dagger\\|_2$ 和残差范数 $\\|r_k\\|_2$。\n    c. 从存储的误差中，找到最优迭代指数 $k_{\\mathrm{opt}}$。\n    d. 应用停止准则以找到检测指数 $k_{\\mathrm{det}}$。这包括计算序列 $s_k = \\log(\\|r_k\\|_2 + \\epsilon_{\\text{mach}})$，其中 $\\epsilon_{\\text{mach}}$ 是一个很小的机器精度以防止 $\\log(0)$。然后，对 $k=1, \\dots, K_{\\max}-1$ 计算离散曲率 $c_k = s_{k-1} - 2s_k + s_{k+1}$。该序列通过窗口大小为 $w=5$ 的移动平均进行平滑，得到 $\\tilde{c}_k$。从 $\\{\\tilde{c}_k\\}$ 的前 $K_{\\mathrm{burn}}=10$ 个样本计算基线统计量，即均值 $\\mu_0$ 和标准差 $\\sigma_0$。检测指数 $k_{\\mathrm{det}}$ 是第一个满足 $\\tilde{c}_k > \\mu_0 + \\gamma \\sigma_0$ 和 $\\tilde{c}_k > 0$ 的 $k \\ge K_{\\mathrm{burn}}$。如果找不到这样的 $k$，则 $k_{\\mathrm{det}} = K_{\\max}+1$。\n\n3.  **评估**：在完成一个测试用例的所有试验后，计算两个指标：\n    a. 假阳性率 ($\\mathrm{fp}$)：检测过早发生的试验所占的比例，即 $k_{\\mathrm{det}}  k_{\\mathrm{opt}} - \\delta$，容差为 $\\delta=2$。\n    b. 平均有符号检测偏移量 ($\\overline{\\Delta k}$)：在所有发生检测的试验中（$k_{\\mathrm{det}} \\le K_{\\max}$），差值 $k_{\\mathrm{det}} - k_{\\mathrm{opt}}$ 的平均值。如果没有发生检测，则此值为 $0$。\n\n最终的代码将以结构化、确定性的方式实现此逻辑，使用固定的伪随机数生成器种子以确保可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import lfilter\n\ndef solve():\n    \"\"\"\n    Main function to run the entire simulation as specified in the problem.\n    \"\"\"\n\n    # --- Global parameters ---\n    PARAMS = {\n        'n': 128,              # Signal dimension\n        'sigma_h': 2.0,        # Gaussian kernel standard deviation\n        'K_max': 150,          # Maximum number of iterations\n        'w': 5,                # Moving average window length\n        'K_burn': 10,          # Burn-in period for statistics\n        'gamma': 3.0,          # Threshold parameter\n        'delta': 2,            # False positive tolerance\n        'N_trials': 20,        # Number of trials per test case\n        'rng_seed': 42         # Seed for reproducibility\n    }\n\n    # --- Test case definitions ---\n    TEST_CASES = [\n        {'noise_type': 'white', 'epsilon': 0.02, 'rho': None},\n        {'noise_type': 'ar1', 'epsilon': 0.02, 'rho': 0.8},\n        {'noise_type': 'ar1', 'epsilon': 0.02, 'rho': 0.95},\n        {'noise_type': 'zero', 'epsilon': 0.0, 'rho': None},\n    ]\n\n    def create_operator(n, sigma_h):\n        \"\"\"Creates the convolution operator A, its adjoint AT, and its norm.\"\"\"\n        x_grid = np.arange(n)\n        kernel = np.exp(-((x_grid - n // 2)**2) / (2 * sigma_h**2))\n        kernel = np.roll(kernel, -n // 2)\n        kernel /= np.sum(kernel)\n        h_hat = np.fft.fft(kernel)\n        A_norm = np.max(np.abs(h_hat))\n        \n        def A(x):\n            return np.fft.ifft(h_hat * np.fft.fft(x)).real\n        \n        AT = A  # Operator is self-adjoint\n        return A, AT, A_norm\n\n    def create_ground_truth(n):\n        \"\"\"Creates a nontrivial ground truth signal x_dagger.\"\"\"\n        x_dagger = np.zeros(n)\n        x_dagger[n//6:n//3] = 1.0\n        x_grid = np.arange(n)\n        x_dagger += 1.5 * np.exp(-((x_grid - n // 2)**2) / (2 * 5.0**2))\n        x_dagger[int(n * 0.7)] = -1.0\n        x_dagger[int(n * 0.8)] = 0.8\n        return x_dagger\n\n    def generate_noise(noise_type, y_dagger, epsilon, n, rho, rng):\n        \"\"\"Generates noise vector e with a prescribed relative norm.\"\"\"\n        if noise_type == 'zero' or epsilon == 0:\n            return np.zeros(n)\n\n        y_dagger_norm = np.linalg.norm(y_dagger)\n        target_noise_norm = epsilon * y_dagger_norm\n        if y_dagger_norm == 0:\n            return np.zeros(n)\n\n        if noise_type == 'white':\n            e_unscaled = rng.standard_normal(n)\n        elif noise_type == 'ar1':\n            burn_in = 2 * n\n            eta = rng.standard_normal(n + burn_in)\n            ar_process_full = lfilter([1.0], [1.0, -rho], eta)\n            e_unscaled = ar_process_full[burn_in:]\n        else:\n            raise ValueError(f\"Unknown noise type: {noise_type}\")\n\n        e_norm = np.linalg.norm(e_unscaled)\n        e = e_unscaled * (target_noise_norm / e_norm) if e_norm > 0 else np.zeros(n)\n        return e\n\n    def run_single_trial(A, AT, A_norm, x_dagger, case_params, common_params, rng):\n        \"\"\"Runs one full simulation trial and returns k_opt and k_det.\"\"\"\n        # Unpack parameters\n        n, K_max, w = common_params['n'], common_params['K_max'], common_params['w']\n        K_burn, gamma = common_params['K_burn'], common_params['gamma']\n        \n        # 1. Generate data\n        y_dagger = A(x_dagger)\n        e = generate_noise(case_params['noise_type'], y_dagger, case_params['epsilon'], n, case_params['rho'], rng)\n        y = y_dagger + e\n\n        # 2. Run Landweber iteration\n        alpha = 1.9 / (A_norm**2)\n        x_k = np.zeros(n)\n        errors = []\n        residuals = []\n        for _ in range(K_max + 1):\n            errors.append(np.linalg.norm(x_k - x_dagger))\n            r_k = y - A(x_k)\n            residuals.append(np.linalg.norm(r_k))\n            x_k = x_k + alpha * AT(r_k)\n\n        # 3. Find optimal stopping index\n        k_opt = np.argmin(errors)\n\n        # 4. Implement curvature detector\n        log_res_arr = np.log(np.array(residuals) + np.finfo(float).eps)\n        curvatures = log_res_arr[:-2] - 2 * log_res_arr[1:-1] + log_res_arr[2:]\n\n        smoothed_curvatures = np.zeros_like(curvatures)\n        for i in range(len(curvatures)):\n            start = max(0, i - (w - 1) // 2)\n            end = min(len(curvatures), i + w // 2 + 1)\n            smoothed_curvatures[i] = np.mean(curvatures[start:end])\n        \n        if K_burn > 0 and len(smoothed_curvatures) >= K_burn:\n            mu0 = np.mean(smoothed_curvatures[:K_burn])\n            sigma0 = np.std(smoothed_curvatures[:K_burn])\n        else:\n            mu0, sigma0 = 0.0, 0.0\n\n        k_det = K_max + 1\n        for k in range(K_burn, K_max - 1): # k is in {K_burn, ..., K_max-2}\n            c_tilde_k = smoothed_curvatures[k - 1]\n            threshold = mu0 + gamma * sigma0 if sigma0 > 0 else mu0\n            if c_tilde_k > threshold and c_tilde_k > 0:\n                k_det = k\n                break\n        return k_opt, k_det\n\n    # --- Main execution logic ---\n    rng = np.random.default_rng(PARAMS['rng_seed'])\n    A, AT, A_norm = create_operator(PARAMS['n'], PARAMS['sigma_h'])\n    x_dagger = create_ground_truth(PARAMS['n'])\n\n    all_results = []\n    \n    for case in TEST_CASES:\n        false_positives = 0\n        detection_offsets = []\n        \n        for _ in range(PARAMS['N_trials']):\n            k_opt, k_det = run_single_trial(A, AT, A_norm, x_dagger, case, PARAMS, rng)\n            \n            if k_det  k_opt - PARAMS['delta']:\n                false_positives += 1\n            \n            if k_det = PARAMS['K_max']:\n                detection_offsets.append(k_det - k_opt)\n        \n        fp_rate = false_positives / PARAMS['N_trials']\n        mean_offset = np.mean(detection_offsets) if detection_offsets else 0.0\n            \n        all_results.extend([fp_rate, mean_offset])\n        \n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}