{
    "hands_on_practices": [
        {
            "introduction": "Understanding linear least squares begins with its geometric interpretation. The method finds the best approximation by projecting the data vector onto the subspace spanned by the model's columns. This first exercise () invites you to explore this core concept by determining the specific condition under which an exact solution, or a zero residual, exists, which occurs precisely when the data vector lies within this column space.",
            "id": "3398138",
            "problem": "Consider a linear inverse problem in which the observation operator is represented by a real matrix $A \\in \\mathbb{R}^{4 \\times 2}$ and the data vector is $b(\\alpha) \\in \\mathbb{R}^{4}$. Let $A$ be constructed with the two columns $c_{1}$ and $c_{2}$ given by\n$$\nc_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\qquad c_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix},\n$$\nso that\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n1  0 \\\\\n0  1 \\\\\n1  -1\n\\end{pmatrix}.\n$$\nLet the data vector depend on a real parameter $\\alpha \\in \\mathbb{R}$ as\n$$\nb(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}.\n$$\nIn the sense of linear Least Squares (LS), the residual is defined as $r(\\alpha) = b(\\alpha) - A x(\\alpha)$ for some $x(\\alpha) \\in \\mathbb{R}^{2}$ chosen to minimize the Euclidean norm $ \\| r(\\alpha) \\|_{2}$. Denote by $P$ the orthogonal projector onto the column space $\\mathcal{R}(A)$ of $A$, and by $I$ the identity on $\\mathbb{R}^{4}$. Your tasks are:\n- Using only foundational definitions of linear least squares and the normal equations, determine whether there exist values of $\\alpha$ for which the LS residual can be zero. Equivalently, ascertain for which $\\alpha$ we have $b(\\alpha) \\in \\mathcal{R}(A)$.\n- Compute the orthogonal projection $P\\,b(\\alpha)$ and the orthogonal complement $(I-P)\\,b(\\alpha)$ explicitly as functions of $\\alpha$.\n- Finally, report the unique real value of $\\alpha$ (if any) for which the LS residual is zero.\n\nThe final answer must be the single value of $\\alpha$. No rounding is required, and no units are involved.",
            "solution": "**Problem Validation**\n\n**Step 1: Extract Givens**\n- The observation operator is a real matrix $A \\in \\mathbb{R}^{4 \\times 2}$.\n- The columns of $A$ are $c_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ and $c_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\n- The matrix $A$ is given by $A = \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix}$.\n- The data vector is $b(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}$, where $\\alpha \\in \\mathbb{R}$.\n- The residual is defined as $r(\\alpha) = b(\\alpha) - A x(\\alpha)$, where $x(\\alpha) \\in \\mathbb{R}^{2}$ is chosen to minimize the Euclidean norm $\\| r(\\alpha) \\|_{2}$.\n- $P$ is the orthogonal projector onto the column space $\\mathcal{R}(A)$ of $A$.\n- $I$ is the identity matrix on $\\mathbb{R}^{4}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard problem in linear algebra concerning linear least squares. It is well-posed, providing all necessary information (the matrix $A$ and the parameterized vector $b(\\alpha)$) to determine a unique solution for the requested quantities. The problem is expressed in objective and precise mathematical language. The columns of $A$ are linearly independent, as one is not a scalar multiple of the other, which ensures that the matrix $A^\\top A$ is invertible and a unique least squares solution exists for any given $b(\\alpha)$. The problem is fully self-contained and analytically solvable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution**\n\nThe problem asks for the value of the parameter $\\alpha$ for which the linear system $A x = b(\\alpha)$ has an exact solution, which is equivalent to the least squares residual being zero. This occurs if and only if the data vector $b(\\alpha)$ lies in the column space of the matrix $A$, denoted $\\mathcal{R}(A)$. The column space $\\mathcal{R}(A)$ is the set of all linear combinations of the columns of $A$.\n\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$. The condition $b(\\alpha) \\in \\mathcal{R}(A)$ means there exist scalars $x_1$ and $x_2$ such that $x_1 c_1 + x_2 c_2 = b(\\alpha)$. This translates to the following system of linear equations:\n$$\nx_1 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + x_2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}\n$$\nThis vector equation is equivalent to a system of four scalar equations:\n$1.$ $x_1 + x_2 = 2$\n$2.$ $x_1 + 0 \\cdot x_2 = 1 \\implies x_1 = 1$\n$3.$ $0 \\cdot x_1 + x_2 = 1 \\implies x_2 = 1$\n$4.$ $x_1 - x_2 = \\alpha$\n\nFrom the second and third equations, we immediately find that $x_1 = 1$ and $x_2 = 1$. We must check if these values are consistent with the first equation. Substituting into the first equation gives $1 + 1 = 2$, which is true. Thus, the first three equations are consistent and uniquely determine $x_1=1$ and $x_2=1$.\nFor the entire system to be consistent, these values must also satisfy the fourth equation. Substituting $x_1 = 1$ and $x_2 = 1$ into the fourth equation yields:\n$$\n1 - 1 = \\alpha \\implies \\alpha = 0\n$$\nTherefore, the vector $b(\\alpha)$ is in the column space of $A$ if and only if $\\alpha = 0$. This is the unique value for which the least squares residual is zero.\n\nThe second part of the task is to compute the orthogonal projection $P\\,b(\\alpha)$ and its complement $(I-P)\\,b(\\alpha)$. The orthogonal projector onto the column space $\\mathcal{R}(A)$ is given by the formula $P = A(A^\\top A)^{-1}A^\\top$. First, we compute the matrix $A^\\top A$:\n$$\nA^\\top A = \\begin{pmatrix} 1  1  0  1 \\\\ 1  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1+1\\cdot1+0\\cdot0+1\\cdot1  1\\cdot1+1\\cdot0+0\\cdot1+1\\cdot(-1) \\\\ 1\\cdot1+0\\cdot1+1\\cdot0+(-1)\\cdot1  1\\cdot1+0\\cdot0+1\\cdot1+(-1)\\cdot(-1) \\end{pmatrix}\n$$\n$$\nA^\\top A = \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} = 3I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix. The fact that $A^\\top A$ is a diagonal matrix confirms that the columns of $A$ are orthogonal. The inverse is easily found:\n$$\n(A^\\top A)^{-1} = \\frac{1}{3} I_2 = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix}\n$$\nThe projection of $b(\\alpha)$ onto $\\mathcal{R}(A)$ is the vector $A x(\\alpha)$, where $x(\\alpha)$ is the solution to the normal equations $A^\\top A x(\\alpha) = A^\\top b(\\alpha)$.\nLet's solve for $x(\\alpha)$:\n$$\n3I_2 x(\\alpha) = A^\\top b(\\alpha) \\implies x(\\alpha) = \\frac{1}{3} A^\\top b(\\alpha)\n$$\nWe compute $A^\\top b(\\alpha)$:\n$$\nA^\\top b(\\alpha) = \\begin{pmatrix} 1  1  0  1 \\\\ 1  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix} = \\begin{pmatrix} 1\\cdot2+1\\cdot1+0\\cdot1+1\\cdot\\alpha \\\\ 1\\cdot2+0\\cdot1+1\\cdot1+(-1)\\cdot\\alpha \\end{pmatrix} = \\begin{pmatrix} 3+\\alpha \\\\ 3-\\alpha \\end{pmatrix}\n$$\nSo, the least squares solution vector is:\n$$\nx(\\alpha) = \\frac{1}{3} \\begin{pmatrix} 3+\\alpha \\\\ 3-\\alpha \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\end{pmatrix}\n$$\nNow we can compute the projection $P\\,b(\\alpha) = A x(\\alpha)$:\n$$\nP\\,b(\\alpha) = \\begin{pmatrix} 1  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} (1+\\frac{\\alpha}{3}) + (1-\\frac{\\alpha}{3}) \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ (1+\\frac{\\alpha}{3}) - (1-\\frac{\\alpha}{3}) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ \\frac{2\\alpha}{3} \\end{pmatrix}\n$$\nThe orthogonal complement projection $(I-P)b(\\alpha)$ is the residual vector $r(\\alpha)$, given by $r(\\alpha) = b(\\alpha) - P\\,b(\\alpha)$:\n$$\n(I-P)\\,b(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ \\frac{2\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 2-2 \\\\ 1-(1+\\frac{\\alpha}{3}) \\\\ 1-(1-\\frac{\\alpha}{3}) \\\\ \\alpha - \\frac{2\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\end{pmatrix}\n$$\nThe LS residual is zero when $(I-P)b(\\alpha)$ is the zero vector.\n$$\n\\begin{pmatrix} 0 \\\\ -\\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis requires $-\\frac{\\alpha}{3} = 0$ and $\\frac{\\alpha}{3} = 0$, both of which imply $\\alpha=0$. This result confirms our initial finding.\nThe unique real value of $\\alpha$ for which the LS residual is zero is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Real-world inverse problems are often subject to physical constraints, such as conservation laws, that must be precisely satisfied. The standard least squares framework can be elegantly extended to handle such linear equality constraints using the null-space method. This practice problem () guides you through this powerful technique, where you will parameterize the feasible solution space to reduce the constrained problem to an unconstrained one of lower dimension.",
            "id": "3398146",
            "problem": "In a linear data assimilation setting, consider a state vector $x \\in \\mathbb{R}^{3}$ representing analysis increments at three spatial locations. A set of $4$ linear observations is modeled by a linear operator $A \\in \\mathbb{R}^{4 \\times 3}$ and a data vector $b \\in \\mathbb{R}^{4}$. The analysis increment is required to satisfy a linear balance constraint represented by $C \\in \\mathbb{R}^{1 \\times 3}$ and $d \\in \\mathbb{R}^{1}$. The objective is to find the unique vector $x^{\\star}$ that minimizes the squared two-norm of the observation misfit subject to the balance constraint:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad C x = d.\n$$\nAssume all errors are uncorrelated with equal variance so that no weighting other than the two-norm is required. The matrices and vectors are\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  0 \\\\\n2  1  1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\ 2 \\\\ 1 \\\\ 4\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1  -1  0\n\\end{pmatrix}, \\quad\nd = \\begin{pmatrix}\n1\n\\end{pmatrix}.\n$$\nStarting from first principles (the definition of constrained least squares and basic properties of linear subspaces), parameterize the feasible set defined by the linear constraint using a basis for the nullspace of $C$ together with a particular feasible vector, and reduce the constrained problem to an unconstrained least-squares problem in the reduced coordinates. Then, using the normal equations associated with the reduced problem, compute the unique constrained minimizer $x^{\\star}$.\n\nReport only the second component $x^{\\star}_{2}$ of the constrained least-squares solution. Express your final answer as an exact fraction. No units are required.",
            "solution": "The problem is to find the vector $x^{\\star} \\in \\mathbb{R}^{3}$ that solves the constrained linear least-squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad C x = d\n$$\nwhere the matrices $A$, $C$ and vectors $b$, $d$ are given as:\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  0 \\\\\n2  1  1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\ 2 \\\\ 1 \\\\ 4\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1  -1  0\n\\end{pmatrix}, \\quad\nd = \\begin{pmatrix}\n1\n\\end{pmatrix}.\n$$\nThe problem requires a solution method based on the parameterization of the feasible set. The feasible set is the set of all vectors $x \\in \\mathbb{R}^{3}$ that satisfy the linear constraint $C x = d$. This set forms an affine subspace, which can be expressed as the sum of a particular solution to the non-homogeneous equation and the general solution to the corresponding homogeneous equation.\n\nLet $x \\in \\mathbb{R}^{3}$ be a vector in the feasible set. It can be written as $x = x_p + x_h$, where $x_p$ is any particular solution satisfying $C x_p = d$, and $x_h$ is a vector in the nullspace of $C$, satisfying $C x_h = 0$.\n\nFirst, we find a particular solution $x_p$. The constraint is $x_1 - x_2 = 1$. A simple choice for $x_p$ is to set $x_1 = 1$ and $x_2 = 0$, with $x_3$ arbitrarily set to $0$.\n$$\nx_p = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nWe verify that $C x_p = \\begin{pmatrix} 1  -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1$, which matches $d$.\n\nNext, we characterize the nullspace of $C$, which is the set of all vectors $x_h = \\begin{pmatrix} x_{h1} \\\\ x_{h2} \\\\ x_{h3} \\end{pmatrix}$ such that $C x_h = 0$. This gives the equation $x_{h1} - x_{h2} = 0$, or $x_{h1} = x_{h2}$. The vectors in the nullspace are of the form $\\begin{pmatrix} s \\\\ s \\\\ t \\end{pmatrix}$ for arbitrary scalars $s, t \\in \\mathbb{R}$. A basis for this two-dimensional nullspace can be formed by the vectors corresponding to $(s=1, t=0)$ and $(s=0, t=1)$:\n$$\nz_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad z_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nAny vector $x_h$ in the nullspace of $C$ can be written as a linear combination of these basis vectors: $x_h = w_1 z_1 + w_2 z_2 = Z w$, where $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is the vector of reduced coordinates and $Z$ is the matrix whose columns are the basis vectors:\n$$\nZ = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nThus, any feasible vector $x$ can be parameterized as:\n$$\nx = x_p + Z w = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}.\n$$\nSubstituting this parameterization into the objective function $\\|A x - b\\|_{2}^{2}$, we obtain a reduced, unconstrained least-squares problem in terms of $w$:\n$$\n\\min_{w \\in \\mathbb{R}^{2}} \\|A (x_p + Z w) - b\\|_{2}^{2} = \\min_{w \\in \\mathbb{R}^{2}} \\|(A Z) w - (b - A x_p)\\|_{2}^{2}.\n$$\nLet $\\hat{A} = A Z$ and $\\hat{b} = b - A x_p$. We compute these terms.\n$$\n\\hat{A} = AZ = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\\\ 2  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(1)+1(0)  1(0)+0(0)+1(1) \\\\ 0(1)+1(1)+1(0)  0(0)+1(0)+1(1) \\\\ 1(1)+1(1)+0(0)  1(0)+1(0)+0(1) \\\\ 2(1)+1(1)+1(0)  2(0)+1(0)+1(1) \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\\\ 2  0 \\\\ 3  1 \\end{pmatrix}.\n$$\nNext, we compute $A x_p$:\n$$\nA x_p = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\\\ 2  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\nNow we compute $\\hat{b}$:\n$$\n\\hat{b} = b - A x_p = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 2 \\end{pmatrix}.\n$$\nThe solution $w^{\\star}$ to the unconstrained problem $\\min_{w} \\|\\hat{A} w - \\hat{b}\\|_{2}^{2}$ is found by solving the normal equations:\n$$\n(\\hat{A}^\\top \\hat{A}) w^{\\star} = \\hat{A}^\\top \\hat{b}.\n$$\nWe compute the matrix $\\hat{A}^\\top \\hat{A}$ and the vector $\\hat{A}^\\top \\hat{b}$.\n$$\n\\hat{A}^\\top \\hat{A} = \\begin{pmatrix} 1  1  2  3 \\\\ 1  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  1 \\\\ 2  0 \\\\ 3  1 \\end{pmatrix} = \\begin{pmatrix} 1+1+4+9  1+1+0+3 \\\\ 1+1+0+3  1+1+0+1 \\end{pmatrix} = \\begin{pmatrix} 15  5 \\\\ 5  3 \\end{pmatrix}.\n$$\n$$\n\\hat{A}^\\top \\hat{b} = \\begin{pmatrix} 1  1  2  3 \\\\ 1  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1(1)+1(2)+2(0)+3(2) \\\\ 1(1)+1(2)+0(0)+1(2) \\end{pmatrix} = \\begin{pmatrix} 1+2+0+6 \\\\ 1+2+0+2 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix}.\n$$\nThe normal equations are a $2 \\times 2$ system for $w^{\\star}$:\n$$\n\\begin{pmatrix} 15  5 \\\\ 5  3 \\end{pmatrix} w^{\\star} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix}.\n$$\nTo solve for $w^{\\star}$, we find the inverse of the matrix $\\hat{A}^\\top \\hat{A}$. The determinant is $\\det(\\hat{A}^\\top \\hat{A}) = (15)(3) - (5)(5) = 45 - 25 = 20$.\nThe inverse is:\n$$\n(\\hat{A}^\\top \\hat{A})^{-1} = \\frac{1}{20} \\begin{pmatrix} 3  -5 \\\\ -5  15 \\end{pmatrix}.\n$$\nNow we can solve for $w^{\\star}$:\n$$\nw^{\\star} = \\frac{1}{20} \\begin{pmatrix} 3  -5 \\\\ -5  15 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 3(9) - 5(5) \\\\ -5(9) + 15(5) \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 27 - 25 \\\\ -45 + 75 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 2 \\\\ 30 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{20} \\\\ \\frac{30}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nSo, $w_1^{\\star} = \\frac{1}{10}$ and $w_2^{\\star} = \\frac{3}{2}$.\nFinally, we compute the constrained minimizer $x^{\\star}$ using the optimal reduced coordinates $w^{\\star}$:\n$$\nx^{\\star} = x_p + Z w^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nThe solution vector is $x^{\\star} = \\begin{pmatrix} \\frac{11}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}$. The problem asks for the second component of this vector, $x^{\\star}_{2}$.\n$$\nx^{\\star}_{2} = \\frac{1}{10}.\n$$",
            "answer": "$$ \\boxed{\\frac{1}{10}} $$"
        },
        {
            "introduction": "Beyond its geometric and algebraic interpretations, the method of least squares has deep roots in probability theory, arising as the Maximum A Posteriori (MAP) estimate in linear-Gaussian models. This probabilistic viewpoint is crucial for quantifying uncertainty. In this advanced exercise (), you will investigate the consequences of using a mis-specified observation error covariance, a common scenario in data assimilation, and derive a correction factor to ensure the posterior uncertainty is accurately represented.",
            "id": "3398168",
            "problem": "Consider a linear Gaussian inverse problem in the context of variational data assimilation. Let the unknown state be $x \\in \\mathbb{R}^{n}$ with a Gaussian prior $x \\sim \\mathcal{N}(x_{b}, B)$, where $x_{b} \\in \\mathbb{R}^{n}$ is the prior mean and $B \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Observations $y \\in \\mathbb{R}^{m}$ are given by the linear forward operator $H \\in \\mathbb{R}^{m \\times n}$ with additive Gaussian noise, $y = H x + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$ and $R_{\\text{true}} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite. A practitioner performs a Maximum A Posteriori (MAP) estimate using a mis-specified observation noise covariance $R \\in \\mathbb{R}^{m \\times m}$, assuming the quadratic cost functional\n$$\nJ_{R}(x) = \\frac{1}{2} \\| y - H x \\|_{R^{-1}}^{2} + \\frac{1}{2} \\| x - x_{b} \\|_{B^{-1}}^{2},\n$$\nwhere $\\| v \\|_{M}^{2} \\equiv v^{\\top} M v$ for a symmetric positive definite matrix $M$.\n\nStarting from the definitions of the Gaussian prior and likelihood and the associated least-squares cost $J_{R}(x)$, derive the normal equations for the MAP estimate under the assumed $R$, and derive the corresponding posterior covariance matrix $A(R)$ as the inverse of the Hessian of $J_{R}(x)$.\n\nNow suppose the true observation covariance is a scalar multiple of the assumed one, $R_{\\text{true}} = \\eta R$ with $\\eta  1$, representing underestimation of observation noise in the assimilation. Let $A(R)$ and $A(R_{\\text{true}})$ denote the posterior covariance matrices obtained when using $R$ and $R_{\\text{true}}$, respectively. Define the prior-preconditioned Gaussâ€“Newton matrix $K \\in \\mathbb{R}^{n \\times n}$ by\n$$\nK = B^{1/2} H^{\\top} R^{-1} H B^{1/2},\n$$\nwhere $B^{1/2}$ is any symmetric matrix square root of $B$. Consider a scalar inflation of the mis-specified posterior covariance of the form $\\alpha \\, A(R)$ with $\\alpha  0$ chosen to correct the underestimation of uncertainty by matching the generalized posterior volume, i.e., enforcing\n$$\n\\det\\big( \\alpha \\, A(R) \\big) = \\det\\big( A(R_{\\text{true}}) \\big).\n$$\nExpress the inflation factor $\\alpha$ in closed-form in terms of the eigenvalues $\\{ \\lambda_{i} \\}_{i=1}^{n}$ of $K$ and the scalar $\\eta$. Your final answer must be a single closed-form analytic expression. No numerical approximation is required.",
            "solution": "The analysis begins with the definition of the quadratic cost functional $J_{R}(x)$, which is minimized to find the Maximum A Posteriori (MAP) estimate of the state $x$. The cost functional combines information from the prior distribution of the state and the observations. It is given by:\n$$\nJ_{R}(x) = \\frac{1}{2} \\| y - H x \\|_{R^{-1}}^{2} + \\frac{1}{2} \\| x - x_{b} \\|_{B^{-1}}^{2}\n$$\nUsing the definition $\\| v \\|_{M}^{2} = v^{\\top} M v$, we expand the cost functional:\n$$\nJ_{R}(x) = \\frac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x) + \\frac{1}{2} (x - x_{b})^{\\top} B^{-1} (x - x_{b})\n$$\nTo find the MAP estimate, we must find the value of $x$ that minimizes $J_{R}(x)$. Since $J_{R}(x)$ is a convex function, its minimum can be found by setting its gradient with respect to $x$ to zero. The gradient is:\n$$\n\\nabla_{x} J_{R}(x) = \\frac{d}{dx} \\left[ \\frac{1}{2} (x^{\\top} H^{\\top} R^{-1} H x - 2 y^{\\top} R^{-1} H x) + \\frac{1}{2} (x^{\\top} B^{-1} x - 2 x_{b}^{\\top} B^{-1} x) + \\text{const} \\right]\n$$\n$$\n\\nabla_{x} J_{R}(x) = H^{\\top} R^{-1} H x - H^{\\top} R^{-1} y + B^{-1} x - B^{-1} x_{b}\n$$\nSetting the gradient to zero, $\\nabla_{x} J_{R}(x) = 0$, gives the normal equations for the MAP estimate, which we denote as $x_{a}$:\n$$\n(B^{-1} + H^{\\top} R^{-1} H) x_{a} = B^{-1} x_{b} + H^{\\top} R^{-1} y\n$$\nNext, we derive the posterior covariance matrix $A(R)$. In the context of Gaussian assumptions, the posterior distribution is also Gaussian, and its covariance matrix is given by the inverse of the Hessian of the negative log-posterior, which is equivalent to the Hessian of the cost functional $J_{R}(x)$. The Hessian, $\\mathcal{H}$, is the second derivative of $J_{R}(x)$ with respect to $x$:\n$$\n\\mathcal{H} = \\nabla_{x}^{2} J_{R}(x) = \\frac{d}{dx^{\\top}} (\\nabla_{x} J_{R}(x)) = B^{-1} + H^{\\top} R^{-1} H\n$$\nThe posterior covariance matrix $A(R)$ is the inverse of the Hessian:\n$$\nA(R) = \\mathcal{H}^{-1} = (B^{-1} + H^{\\top} R^{-1} H)^{-1}\n$$\nThe problem defines the prior-preconditioned Gauss-Newton matrix as $K = B^{1/2} H^{\\top} R^{-1} H B^{1/2}$, where $B^{1/2}$ is a symmetric matrix square root of $B$. We can express $A(R)$ in terms of $K$. Let's manipulate the Hessian $\\mathcal{H}$:\n$$\n\\mathcal{H} = B^{-1/2} B^{-1/2} + H^{\\top} R^{-1} H = B^{-1/2} (I + B^{1/2} H^{\\top} R^{-1} H B^{1/2}) B^{-1/2}\n$$\n$$\n\\mathcal{H} = B^{-1/2} (I + K) B^{-1/2}\n$$\nTaking the inverse to find $A(R)$:\n$$\nA(R) = \\mathcal{H}^{-1} = (B^{-1/2} (I + K) B^{-1/2})^{-1} = B^{1/2} (I + K)^{-1} B^{1/2}\n$$\nNow, we consider the case where the true observation error covariance is $R_{\\text{true}} = \\eta R$ for some scalar $\\eta  1$. The corresponding posterior covariance, $A(R_{\\text{true}})$, is obtained by replacing $R$ with $R_{\\text{true}} = \\eta R$ in the formula. This affects the term $H^{\\top} R^{-1} H$, which becomes $H^{\\top} (\\eta R)^{-1} H = \\frac{1}{\\eta} H^{\\top} R^{-1} H$.\nThe corresponding Gauss-Newton matrix, let's call it $K_{\\text{true}}$, is:\n$$\nK_{\\text{true}} = B^{1/2} H^{\\top} R_{\\text{true}}^{-1} H B^{1/2} = B^{1/2} \\left(\\frac{1}{\\eta} H^{\\top} R^{-1} H \\right) B^{1/2} = \\frac{1}{\\eta} K\n$$\nTherefore, the true posterior covariance matrix is:\n$$\nA(R_{\\text{true}}) = B^{1/2} (I + K_{\\text{true}})^{-1} B^{1/2} = B^{1/2} \\left(I + \\frac{1}{\\eta} K \\right)^{-1} B^{1/2}\n$$\nThe problem requires us to find an inflation factor $\\alpha$ such that the determinant of the inflated mis-specified covariance matches the determinant of the true covariance:\n$$\n\\det\\big( \\alpha \\, A(R) \\big) = \\det\\big( A(R_{\\text{true}}) \\big)\n$$\nUsing the property $\\det(cM) = c^{n} \\det(M)$ for a matrix $M \\in \\mathbb{R}^{n \\times n}$:\n$$\n\\alpha^{n} \\det(A(R)) = \\det(A(R_{\\text{true}}))\n$$\nSolving for $\\alpha^{n}$:\n$$\n\\alpha^{n} = \\frac{\\det(A(R_{\\text{true}}))}{\\det(A(R))}\n$$\nLet's compute the determinants. Using the property $\\det(XYZ) = \\det(X)\\det(Y)\\det(Z)$ and $\\det(X^{-1}) = (\\det(X))^{-1}$:\n$$\n\\det(A(R)) = \\det(B^{1/2} (I + K)^{-1} B^{1/2}) = \\det(B^{1/2}) \\det((I + K)^{-1}) \\det(B^{1/2}) = \\det(B) \\frac{1}{\\det(I+K)}\n$$\n$$\n\\det(A(R_{\\text{true}})) = \\det(B^{1/2} \\left(I + \\frac{1}{\\eta} K \\right)^{-1} B^{1/2}) = \\det(B) \\frac{1}{\\det(I + \\frac{1}{\\eta} K)}\n$$\nSubstituting these into the expression for $\\alpha^{n}$:\n$$\n\\alpha^{n} = \\frac{\\det(B) / \\det(I + \\frac{1}{\\eta} K)}{\\det(B) / \\det(I+K)} = \\frac{\\det(I+K)}{\\det(I + \\frac{1}{\\eta} K)}\n$$\nThe determinant of a matrix is the product of its eigenvalues. Let $\\{\\lambda_{i}\\}_{i=1}^{n}$ be the eigenvalues of $K$.\nThe eigenvalues of the matrix $(I+K)$ are $\\{1+\\lambda_{i}\\}_{i=1}^{n}$.\nThe eigenvalues of the matrix $(I + \\frac{1}{\\eta} K)$ are $\\{1 + \\frac{\\lambda_{i}}{\\eta}\\}_{i=1}^{n}$.\nTherefore, we can write the determinants as products of these eigenvalues:\n$$\n\\det(I+K) = \\prod_{i=1}^{n} (1+\\lambda_{i})\n$$\n$$\n\\det\\left(I + \\frac{1}{\\eta} K\\right) = \\prod_{i=1}^{n} \\left(1+\\frac{\\lambda_{i}}{\\eta}\\right)\n$$\nSubstituting these products into the equation for $\\alpha^{n}$:\n$$\n\\alpha^{n} = \\frac{\\prod_{i=1}^{n} (1+\\lambda_{i})}{\\prod_{i=1}^{n} (1+\\frac{\\lambda_{i}}{\\eta})} = \\prod_{i=1}^{n} \\frac{1+\\lambda_{i}}{1+\\frac{\\lambda_{i}}{\\eta}}\n$$\nSimplifying the term inside the product:\n$$\n\\frac{1+\\lambda_{i}}{1+\\frac{\\lambda_{i}}{\\eta}} = \\frac{1+\\lambda_{i}}{\\frac{\\eta + \\lambda_{i}}{\\eta}} = \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}}\n$$\nSo, the expression for $\\alpha^{n}$ becomes:\n$$\n\\alpha^{n} = \\prod_{i=1}^{n} \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}}\n$$\nFinally, solving for $\\alpha$ by taking the $n$-th root gives the required closed-form expression:\n$$\n\\alpha = \\left( \\prod_{i=1}^{n} \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}} \\right)^{\\frac{1}{n}}\n$$",
            "answer": "$$\n\\boxed{\\left( \\prod_{i=1}^{n} \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}} \\right)^{\\frac{1}{n}}}\n$$"
        }
    ]
}