{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds our understanding in the fundamental mechanics of Krylov subspace methods. By manually performing the steps of the Conjugate Gradient (CG) algorithm for a small, well-posed system, you will gain direct insight into how residuals and search directions are constructed to ensure optimality and rapid convergence. Verifying the core orthogonality properties will demystify the mechanism behind the algorithm's efficiency .",
            "id": "3371322",
            "problem": "Consider a linear Gaussian inverse problem in which the Maximum A Posteriori (MAP) estimate is obtained by minimizing the strictly convex quadratic objective $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$ with respect to $x \\in \\mathbb{R}^{3}$. The information (Hessian) matrix $A$ is symmetric positive definite, with known eigenvalues $\\{1, 2, 4\\}$, and is given explicitly by\n$$\nA \\;=\\; \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix}.\n$$\nThe data vector is $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$, and the initial guess is $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nStarting from the core definitions that characterize the Conjugate Gradient (CG) method for symmetric positive definite systems and its interpretation as steepest descent with respect to the $A$-inner product on Krylov subspaces generated by the initial residual, perform exactly three CG iterations to minimize $J(x)$, constructing the search directions from the orthogonalization conditions intrinsic to CG. Compute all quantities exactly (no floating-point rounding), and at each iteration:\n1. Determine the step size by minimizing $J$ along the current search direction.\n2. Update the approximate solution, residual, and next search direction derived from the fundamental orthogonality relations.\n3. Verify the $A$-conjugacy of search directions, that is, $p_{i}^{\\top} A p_{j} = 0$ for $i \\neq j$, and the mutual orthogonality of residuals, that is, $r_{i}^{\\top} r_{j} = 0$ for $i \\neq j$, for the three iterations you carry out.\n\nReport the final approximate solution after three iterations, $x_{3}$, as a single row vector. Express your answer exactly; do not round. Your final answer must be a single mathematical object.",
            "solution": "The problem requires the application of the Conjugate Gradient (CG) method to minimize the quadratic objective function $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$. The minimizer of $J(x)$ is the solution to the linear system $Ax = b$. The CG method is an iterative algorithm for solving such systems where $A$ is symmetric and positive definite.\n\nThe core formulas for the Conjugate Gradient algorithm, starting from an initial guess $x_0$, are as follows:\nInitial residual: $r_0 = b - A x_0$\nInitial search direction: $p_0 = r_0$\n\nFor each iteration $k = 0, 1, 2, \\dots$:\nStep size: $\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$\nSolution update: $x_{k+1} = x_k + \\alpha_k p_k$\nResidual update: $r_{k+1} = r_k - \\alpha_k A p_k$\nSearch direction parameter: $\\beta_k = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}$\nSearch direction update: $p_{k+1} = r_{k+1} + \\beta_k p_k$\n\nLet us apply this algorithm for the given $A$, $b$, and $x_0$.\nGiven:\n$$\nA = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n**Initialization ($k=0$):**\nThe initial residual is:\n$$\nr_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe first search direction is the initial residual:\n$$\np_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n\n**Iteration 1 ($k=0$):**\nWe first compute the product $A p_0$:\n$$\nA p_0 = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} \\\\ -\\frac{1}{2} + \\frac{3}{2} \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}\n$$\nThe step size $\\alpha_0$ is calculated by minimizing $J(x_0+\\alpha p_0)$:\n$$\nr_0^{\\top} r_0 = 1^2 + 1^2 + 1^2 = 3\n$$\n$$\np_0^{\\top} A p_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = 1 + 1 + 4 = 6\n$$\n$$\n\\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} = \\frac{3}{6} = \\frac{1}{2}\n$$\nUpdate the solution:\n$$\nx_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\nUpdate the residual:\n$$\nr_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix}\n$$\nUpdate the search direction:\n$$\nr_1^{\\top} r_1 = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + (-1)^2 = \\frac{1}{4} + \\frac{1}{4} + 1 = \\frac{3}{2}\n$$\n$$\n\\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} = \\frac{3/2}{3} = \\frac{1}{2}\n$$\n$$\np_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix}\n$$\n\n**Iteration 2 ($k=1$):**\nCompute the product $A p_1$:\n$$\nA p_1 = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} & 0 \\\\\n-\\frac{1}{2} & \\frac{3}{2} & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} \\\\ -\\frac{1}{2} + \\frac{3}{2} \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\nCalculate the step size $\\alpha_1$:\n$$\np_1^{\\top} A p_1 = \\begin{pmatrix} 1 & 1 & -1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = 1 + 1 + 1 = 3\n$$\n$$\n\\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} = \\frac{3/2}{3} = \\frac{1}{2}\n$$\nUpdate the solution:\n$$\nx_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\nUpdate the residual:\n$$\nr_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nSince $r_2=0$, the algorithm has converged to the exact solution $x_2$. The CG method is guaranteed to find the exact solution in at most $n$ iterations for an $n \\times n$ system; in this case, convergence occurred in $2$ iterations because the initial residual $r_0$ lies in a two-dimensional Krylov subspace (spanned by the eigenvectors corresponding to eigenvalues $1$ and $4$).\n\n**Iteration 3 ($k=2$):**\nThe problem requests \"exactly three CG iterations\". Standard implementations of CG would terminate upon finding a zero residual. To fulfill the request, we proceed formally.\nUpdate the search direction:\n$$\nr_2^{\\top} r_2 = 0^2 + 0^2 + 0^2 = 0\n$$\n$$\n\\beta_1 = \\frac{r_2^{\\top} r_2}{r_1^{\\top} r_1} = \\frac{0}{3/2} = 0\n$$\n$$\np_2 = r_2 + \\beta_1 p_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe next step would be to calculate $\\alpha_2 = \\frac{r_2^{\\top} r_2}{p_2^{\\top} A p_2} = \\frac{0}{0}$, which is indeterminate. However, the update to the solution is $x_3 = x_2 + \\alpha_2 p_2 = x_2 + \\alpha_2 \\cdot 0 = x_2$. Any choice of $\\alpha_2$ would lead to $x_3=x_2$. Therefore, the state of the system does not change.\n$$\nx_3 = x_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\nThe residual would also remain zero: $r_3 = r_2 - \\alpha_2 A p_2 = 0 - \\alpha_2 \\cdot 0 = 0$.\n\n**Verification of Orthogonality Properties:**\nThe problem asks to verify the $A$-conjugacy of search directions and the orthogonality of residuals for the three iterations completed. The vectors generated are:\n$p_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, $p_1 = \\begin{pmatrix} 1 & 1 & -1/2 \\end{pmatrix}^{\\top}$, $p_2 = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}^{\\top}$\n$r_0 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, $r_1 = \\begin{pmatrix} 1/2 & 1/2 & -1 \\end{pmatrix}^{\\top}$, $r_2 = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}^{\\top}$\n\n1.  **$A$-conjugacy of search directions ($p_i^{\\top} A p_j = 0$ for $i \\neq j$):**\n    -   $p_0^{\\top} A p_1$:\n        $$p_0^{\\top} (A p_1) = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = 1 + 1 - 2 = 0$$\n    -   $p_0^{\\top} A p_2$:\n        $$p_0^{\\top} (A p_2) = p_0^{\\top} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    -   $p_1^{\\top} A p_2$:\n        $$p_1^{\\top} (A p_2) = p_1^{\\top} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    The search directions are $A$-conjugate.\n\n2.  **Orthogonality of residuals ($r_i^{\\top} r_j = 0$ for $i \\neq j$):**\n    -   $r_0^{\\top} r_1$:\n        $$r_0^{\\top} r_1 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$$\n    -   $r_0^{\\top} r_2$:\n        $$r_0^{\\top} r_2 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    -   $r_1^{\\top} r_2$:\n        $$r_1^{\\top} r_2 = \\begin{pmatrix} 1/2 & 1/2 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    The residuals are mutually orthogonal.\n\nThe final approximate solution after three iterations is $x_3$. As demonstrated, $x_3 = x_2$.\n$$\nx_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\nWe can verify this is the exact solution $x = A^{-1}b$:\n$$\nA^{-1} = \\begin{pmatrix} \\frac{3}{4} & \\frac{1}{4} & 0 \\\\ \\frac{1}{4} & \\frac{3}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix}\n$$\n$$\nx = A^{-1}b = \\begin{pmatrix} \\frac{3}{4} & \\frac{1}{4} & 0 \\\\ \\frac{1}{4} & \\frac{3}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} + \\frac{1}{4} \\\\ \\frac{1}{4} + \\frac{3}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\frac{1}{4} \\end{pmatrix}\n$$\nThe result is correct. The final answer is requested as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & \\frac{1}{4}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "We now transition from well-posed systems to a more common scenario in inverse problems: ill-posedness, where noise can be catastrophically amplified. This practice demonstrates the concept of iterative regularization, a powerful implicit feature of methods like the Least Squares QR (LSQR) algorithm. You will see firsthand how a direct solution method can fail by overfitting noise, while early termination of an iterative solver, guided by a principled stopping rule, yields a stable and meaningful solution .",
            "id": "3371338",
            "problem": "Consider solving a linear inverse problem of the form $A x \\approx b$ with $A \\in \\mathbb{R}^{m \\times n}$, where the singular values of $A$ decay slowly and the data $b$ are contaminated with strong additive noise. The goal is to examine how the orthogonal-triangular factorization (QR) approach to least squares behaves without explicit regularization, and to contrast it with the Least Squares with QR-like iterations (LSQR) method that exhibits iterative regularization when stopped by a principled rule. The context is inverse problems and data assimilation, where ill-posedness and noise amplification are central concerns.\n\nUse the following fundamental base:\n- The singular value decomposition (SVD): For a matrix $A$, write $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal and $\\Sigma$ is diagonal with nonnegative entries $\\sigma_i$.\n- Least squares orthogonal-triangular: Solve $\\min_{x} \\| A x - b \\|_2$ by computing an orthogonal-triangular factorization $A = Q R$ with $Q$ orthogonal and $R$ upper triangular, then solve $R x = Q^{\\top} b$.\n- Iterative regularization: In algorithms such as Conjugate-Gradient Least Squares (CGLS) and Least Squares with QR-like iterations (LSQR), early termination can act as a regularization mechanism that suppresses the influence of small singular values, which otherwise amplify noise.\n\nYou must implement a self-contained program that constructs synthetic test problems where $A$ has slowly decaying singular values and $b$ contains strong noise, then demonstrates that:\n- Unregularized QR least squares overfits the noise and yields a higher reconstruction error compared to LSQR stopped by a discrepancy principle.\n- Fully iterated LSQR (run to the maximum practical number of iterations) also overfits compared to suitably early stopping.\n\nYou must proceed purely in mathematical terms:\n1. For each test case, let $m = n$ and define $A$ to be diagonal with entries $\\sigma_i = (i+1)^{-p}$ for $i = 0, 1, \\dots, n-1$, where $p$ is a positive exponent controlling the slow decay of singular values. This choice makes $A$ a compact operator with slowly decaying singular values, a canonical mild ill-posedness scenario.\n2. Define a ground-truth vector $x_{\\text{true}} \\in \\mathbb{R}^n$ by $x_{\\text{true},i} = (-1)^i (i+1)^{-1}$, which is a smooth signal with decaying coefficients.\n3. Form the clean right-hand side $b_{\\text{clean}} = A x_{\\text{true}}$.\n4. Add strong noise: let $\\eta$ be a random vector with independent standard normal entries, and scale it so that $\\| \\eta \\|_2 = \\delta \\| b_{\\text{clean}} \\|_2$ for a prescribed relative noise level $\\delta > 0$. Set $b = b_{\\text{clean}} + \\eta$. All norms are the Euclidean norm.\n5. Compute the unregularized QR least squares solution $x_{\\text{QR}}$ to $\\min_x \\| A x - b \\|_2$ by orthogonal-triangular factorization and back-substitution.\n6. Implement LSQR iteration (Paige and Saunders) starting from $x_0 = 0$. Use the Morozov discrepancy principle with parameter $\\tau > 1$ to choose the stopping index $k_{\\text{stop}}$ as the smallest $k$ for which $\\| A x_k - b \\|_2 \\le \\tau \\| \\eta \\|_2$, where $x_k$ is the $k$-th LSQR iterate. Also compute a “fully iterated” LSQR solution $x_{\\text{LSQR-full}}$ by running LSQR for a fixed maximum number of iterations (equal to $n$).\n7. Quantify reconstruction errors as relative errors $E(\\hat{x}) = \\| \\hat{x} - x_{\\text{true}} \\|_2 / \\| x_{\\text{true}} \\|_2$ for $\\hat{x} \\in \\{ x_{\\text{QR}}, x_{k_{\\text{stop}}}, x_{\\text{LSQR-full}} \\}$.\n8. Your program must return, for each test case, two boolean values:\n   - Whether $E(x_{k_{\\text{stop}}}) < E(x_{\\text{QR}})$.\n   - Whether $E(x_{k_{\\text{stop}}}) < E(x_{\\text{LSQR-full}})$.\nThese booleans test the claim that unregularized QR overfits unless regularized and that LSQR’s iterative regularization succeeds with suitable stopping.\n\nNo physical units or angles are involved; all quantities are dimensionless.\n\nTest Suite:\nProvide three test cases $(n, p, \\delta, \\tau)$ that probe different regimes:\n- Case $1$: $n = 80$, $p = 0.3$, $\\delta = 0.2$, $\\tau = 1.1$ (strong noise, slow decay).\n- Case $2$: $n = 80$, $p = 0.5$, $\\delta = 0.1$, $\\tau = 1.05$ (moderate noise, slower decay).\n- Case $3$: $n = 50$, $p = 0.2$, $\\delta = 0.3$, $\\tau = 1.1$ (very strong noise, very slow decay).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain six boolean values in the order of the test cases, where for each case you output first the comparison with QR and then the comparison with fully iterated LSQR. For example, the output format is $[\\text{b}_1,\\text{b}_2,\\text{b}_3,\\text{b}_4,\\text{b}_5,\\text{b}_6]$, where each $\\text{b}_i$ is either True or False.",
            "solution": "The problem requires an analysis and comparison of two methods for solving a linear system $A x \\approx b$ characteristic of discrete ill-posed inverse problems. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is ill-conditioned, specifically having singular values that decay slowly to zero, and the data vector $b \\in \\mathbb{R}^{n}$ is contaminated with significant additive noise. We will contrast the direct, unregularized least squares solution via orthogonal-triangular (QR) factorization with an iterative method, LSQR, which exhibits regularization properties when stopped early.\n\nThe core of the problem lies in the amplification of noise by the \"inverse\" of an ill-conditioned operator. A linear system is ill-posed if small perturbations in the data $b$ can lead to large changes in the solution $x$. For a matrix $A$ with singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$, the solution to the least squares problem $\\min_{x} \\| A x - b \\|_2$ is formally $x = A^{\\dagger} b = V \\Sigma^{\\dagger} U^{\\top} b$, where $\\dagger$ denotes the pseudoinverse. If the singular values $\\sigma_i$ of $A$ decay to zero without a significant gap, the corresponding singular values of $A^{\\dagger}$, which are $1/\\sigma_i$, become very large. When the data is noisy, $b = b_{\\text{true}} + \\eta$, the solution becomes $x = A^{\\dagger}b_{\\text{true}} + A^{\\dagger}\\eta = x_{\\text{true}} + A^{\\dagger}\\eta$. The term $A^{\\dagger}\\eta$ represents the propagated noise. Its components associated with small $\\sigma_i$ are massively amplified, rendering the solution meaningless.\n\nThe problem is constructed to exhibit this exact behavior. The matrix $A$ is defined to be diagonal, so its diagonal entries are its singular values, $\\sigma_i = (i+1)^{-p}$ for $i = 0, 1, \\dots, n-1$. With $p > 0$, these values decay slowly. The ground truth signal $x_{\\text{true}}$ is smooth. Noise $\\eta$ is added to the clean data $b_{\\text{clean}} = A x_{\\text{true}}$ to form $b = b_{\\text{clean}} + \\eta$.\n\n**1. Unregularized QR Least Squares Solution ($x_{\\text{QR}}$)**\n\nThe first method is the direct solution of the least squares problem. For a square, invertible matrix $A$, this is equivalent to solving $A x = b$. The problem specifies using a QR factorization, $A=QR$, where $Q$ is orthogonal and $R$ is upper triangular. The least squares solution is found by solving the triangular system $R x = Q^{\\top} b$. For an ill-conditioned matrix $A$, this procedure is numerically equivalent to direct inversion and does not mitigate noise amplification. The computed solution, $x_{\\text{QR}}$, will be close to the true least squares solution $A^{-1}b$:\n$$x_{\\text{QR}} \\approx A^{-1}(A x_{\\text{true}} + \\eta) = x_{\\text{true}} + A^{-1} \\eta$$\nThe error term, $A^{-1}\\eta$, contains components $(A^{-1}\\eta)_i = \\sigma_i^{-1} \\eta_i = (i+1)^p \\eta_i$. Since $p>0$, this term explodes for larger $i$, and the solution $x_{\\text{QR}}$ becomes dominated by amplified noise, leading to a large reconstruction error $E(x_{\\text{QR}})$. This is a phenomenon known as \"overfitting\" the noise.\n\n**2. LSQR with Iterative Regularization ($x_{k_{\\text{stop}}}$)**\n\nLSQR is an iterative algorithm for solving least squares problems, mathematically equivalent to the conjugate gradient method on the normal equations $A^{\\top}A x = A^{\\top}b$ but with superior numerical stability. A key property of such iterative methods is that the solution estimate at iteration $k$, denoted $x_k$, primarily captures information related to the largest singular values of $A$. Components of the solution corresponding to small singular values, which are most susceptible to noise amplification, are only incorporated in later iterations.\n\nThis behavior allows for \"iterative regularization\": by stopping the algorithm early, we effectively filter out a significant portion of the amplified noise. The solution $x_k$ is constrained to the Krylov subspace $\\mathcal{K}_k(A^\\top A, A^\\top b)$, which acts as a low-dimensional projection that regularizes the problem. The dimension of this subspace, $k$, is the regularization parameter.\n\n**3. The Morozov Discrepancy Principle**\n\nTo make iterative regularization effective, a principled stopping rule is needed. The Morozov discrepancy principle provides such a rule. It is based on the idea that one should not seek a solution that fits the data more closely than the level of noise in the data. Given a noise level estimate, in this case the known norm of the noise $\\| \\eta \\|_2$, we stop at the first iteration $k$ for which the residual norm falls below a threshold related to this noise level:\n$$\\| A x_k - b \\|_2 \\le \\tau \\| \\eta \\|_2$$\nHere, $\\tau > 1$ is a safety factor. Stopping at this index $k_{\\text{stop}}$ yields a regularized solution $x_{k_{\\text{stop}}}$ that balances fidelity to the data with suppression of noise.\n\n**4. Fully Iterated LSQR ($x_{\\text{LSQR-full}}$)**\n\nIf LSQR is run for a sufficient number of iterations (e.g., $n$ for an $n \\times n$ non-singular matrix), it will converge to the unregularized least squares solution. Therefore, the \"fully iterated\" solution $x_{\\text{LSQR-full}}$ will be approximately the same as $x_{\\text{QR}}$ and will also suffer from severe noise amplification.\n\n**Synthesis**\n\nThe numerical experiment is designed to demonstrate that:\n-   $E(x_{k_{\\text{stop}}}) < E(x_{\\text{QR}}):$ The regularized LSQR solution is superior to the unregularized direct solution because early stopping prevents noise amplification.\n-   $E(x_{k_{\\text{stop}}}) < E(x_{\\text{LSQR-full}}):$ The same early stopping provides a better solution than letting the LSQR algorithm run to convergence, which effectively removes the regularization effect.\n\nThe implementation will construct the specified matrices and vectors, run the QR and LSQR solvers, apply the discrepancy principle, and compute the reconstruction errors to verify these inequalities for the given test cases. We expect both boolean comparisons to be `True` for all test cases, confirming the effectiveness of iterative regularization for this class of problems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef lsqr_custom(A, b, max_iter, tau, norm_eta):\n    \"\"\"\n    Implements the LSQR algorithm by Paige and Saunders (1982).\n\n    This implementation finds the solution x_k_stop regularized by the\n    discrepancy principle and the fully iterated solution x_full.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    x_stop = None\n    k_stop = -1\n    stop_threshold = tau * norm_eta\n\n    # Initialize\n    beta = np.linalg.norm(b)\n    u = b / beta if beta > 0 else np.zeros(m)\n    \n    Atu = A.T @ u\n    alpha = np.linalg.norm(Atu)\n    v = Atu / alpha if alpha > 0 else np.zeros(n)\n\n    w = v.copy()\n    phi_bar = beta\n    rho_bar = alpha\n\n    # Main iteration loop\n    for k in range(max_iter):\n        # Bidiagonalization\n        Av = A @ v\n        u_hat = Av - alpha * u\n        beta = np.linalg.norm(u_hat)\n        \n        if beta > 1e-15:\n            u = u_hat / beta\n            Atu = A.T @ u\n            v_hat = Atu - beta * v\n            alpha = np.linalg.norm(v_hat)\n            if alpha > 1e-15:\n                v = v_hat / alpha\n            else:\n                v = np.zeros_like(v_hat) # Should not happen in this problem\n        else:\n            beta = 0.0\n            alpha = 0.0 # Should not happen\n\n        # Plane rotation\n        rho = np.sqrt(rho_bar**2 + beta**2)\n        c = rho_bar / rho\n        s = beta / rho\n        \n        theta = s * alpha\n        rho_bar = -c * alpha\n        phi = c * phi_bar\n        phi_bar = s * phi_bar\n        \n        # Update solution and search direction\n        x = x + (phi / rho) * w\n        w = v - (theta / rho) * w\n        \n        # Check discrepancy principle\n        residual_norm = np.abs(phi_bar)\n        if residual_norm <= stop_threshold and k_stop == -1:\n            x_stop = x.copy()\n            k_stop = k\n\n    # If the stopping condition was never met, something is wrong,\n    # but for robustness we assign the full solution.\n    if x_stop is None:\n        x_stop = x.copy()\n\n    return x_stop, x\n\ndef run_experiment(n, p, delta, tau):\n    \"\"\"\n    Runs one full test case for the given parameters.\n    \"\"\"\n    # 1. Construct the problem (A, xtrue, b)\n    i_vals = np.arange(n)\n    \n    # Ground truth vector xtrue\n    xtrue = ((-1.0)**i_vals) * ((i_vals + 1.0)**(-1.0))\n    \n    # Diagonal matrix A with slowly decaying singular values\n    singular_values = (i_vals + 1.0)**(-p)\n    A = np.diag(singular_values)\n    \n    # Clean data b_clean\n    b_clean = A @ xtrue\n    \n    # Add strong noise\n    eta_raw = np.random.randn(n)\n    norm_b_clean = np.linalg.norm(b_clean)\n    norm_eta_raw = np.linalg.norm(eta_raw)\n    \n    # Scale noise vector to have the prescribed relative norm\n    eta = eta_raw * (delta * norm_b_clean / norm_eta_raw)\n    norm_eta = np.linalg.norm(eta)\n    \n    # Noisy data b\n    b = b_clean + eta\n\n    # 2. Compute unregularized QR least squares solution\n    Q, R = np.linalg.qr(A)\n    # Solve Rx = Q^T b\n    x_QR = scipy.linalg.solve_triangular(R, Q.T @ b, lower=False)\n    \n    # 3. Compute LSQR solutions\n    max_iterations = n\n    x_lsqr_stop, x_lsqr_full = lsqr_custom(A, b, max_iterations, tau, norm_eta)\n    \n    # 4. Quantify reconstruction errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    err_qr = np.linalg.norm(x_QR - xtrue) / norm_xtrue\n    err_lsqr_stop = np.linalg.norm(x_lsqr_stop - xtrue) / norm_xtrue\n    err_lsqr_full = np.linalg.norm(x_lsqr_full - xtrue) / norm_xtrue\n    \n    # 5. Return boolean comparisons\n    bool1 = err_lsqr_stop < err_qr\n    bool2 = err_lsqr_stop < err_lsqr_full\n    \n    return bool1, bool2\n\ndef solve():\n    # Set a fixed seed for reproducibility of random noise\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, delta, tau)\n        (80, 0.3, 0.2, 1.1),\n        (80, 0.5, 0.1, 1.05),\n        (50, 0.2, 0.3, 1.1),\n    ]\n\n    results = []\n    for case in test_cases:\n        b1, b2 = run_experiment(*case)\n        results.extend([b1, b2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A final crucial aspect of real-world data assimilation is handling complex error statistics, where observational errors are often correlated. This exercise explores how to correctly formulate the least-squares problem by \"whitening\" the system with respect to the error covariance matrix $R$. By comparing the convergence of LSQR on the original and whitened systems, you will learn how this preconditioning step correctly reflects the problem's statistical nature and can dramatically improve solver performance .",
            "id": "3371360",
            "problem": "Consider a linear inverse problem in data assimilation with an observation operator $A \\in \\mathbb{R}^{m \\times n}$, a state vector $x \\in \\mathbb{R}^{n}$, and observations $y \\in \\mathbb{R}^{m}$. Observational errors $e \\in \\mathbb{R}^{m}$ are modeled as a zero-mean Gaussian random vector with a symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$, that is $e \\sim \\mathcal{N}(0, R)$ and $y = A x_{\\text{true}} + e$. The maximum likelihood estimator in this setting solves the weighted least squares problem $\\min_{x} \\| R^{-1/2}(A x - y)\\|_{2}^{2}$. Define the whitening operator $W = R^{-1/2}$, so that the whitened system is $W A x \\approx W y$.\n\nGolub–Kahan bidiagonalization (GKB) is a fundamental process underpinning Krylov subspace methods such as Least Squares QR (LSQR), which solves $\\min_{x} \\|A x - y\\|_{2}^{2}$ by generating approximations via GKB. In the presence of correlated errors $R \\neq I$, one may either apply LSQR to the unwhitened system $A x \\approx y$ or to the whitened system $W A x \\approx W y$. Whitening changes the singular spectrum of $A$ seen by GKB and can alter LSQR convergence.\n\nImplement LSQR from first principles using Golub–Kahan bidiagonalization to solve a general least squares problem $\\min_{x} \\|M x - b\\|_{2}^{2}$ where $M$ is provided only through matrix-vector products and $M^{\\top}$ is provided through transpose-vector products, and assess the impact of whitening on LSQR convergence when $R \\neq I$. Use the following fundamental base:\n- Observations conform to $y = A x_{\\text{true}} + e$ with $e \\sim \\mathcal{N}(0, R)$.\n- Weighted least squares arises from the negative log-likelihood, which is proportional to $\\| R^{-1/2}(A x - y)\\|_{2}^{2}$.\n- Whitening via $W = R^{-1/2}$ transforms the weighted problem into an unweighted least squares problem for $W A x \\approx W y$.\n- Golub–Kahan bidiagonalization generates LSQR iterates by orthogonalizing $M$ and $M^{\\top}$ actions without forming normal equations.\n\nFor the test suite below, construct $A$ with a controlled singular spectrum and draw $e$ with the specified correlated covariance $R$. Specifically:\n- For each test case, generate $A$ with prescribed singular values $\\{s_i\\}_{i=1}^p$ where $p = \\min(m,n)$ and $s_i$ decay geometrically to impose a desired condition number. Form $A = U_{p} \\operatorname{diag}(s) V_{p}^{\\top}$ from random orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ obtained via orthogonal-triangular (QR) factorization of Gaussian matrices, with $U_{p} \\in \\mathbb{R}^{m \\times p}$ and $V_{p} \\in \\mathbb{R}^{n \\times p}$ the first $p$ columns of $U$ and $V$.\n- Define $R$ as a stationary first-order autoregressive Toeplitz covariance: $R_{ij} = \\sigma^{2} \\rho^{|i-j|}$ with $|\\rho| < 1$ and $\\sigma > 0$. Compute the lower-triangular Cholesky factor $L$ such that $R = L L^{\\top}$. Generate $e = L z$ where $z \\sim \\mathcal{N}(0, I_{m})$.\n- Use the deterministic true state $x_{\\text{true}} \\in \\mathbb{R}^{n}$ defined by $(x_{\\text{true}})_{j} = \\sin(2 \\pi j / n) + \\tfrac{1}{2} \\sin(4 \\pi j / n)$ for $j = 0, 1, \\ldots, n-1$.\n\nImplement two LSQR runs per test case:\n1. Unwhitened: apply LSQR to $A x \\approx y$ using only the actions $v \\mapsto A v$ and $u \\mapsto A^{\\top} u$.\n2. Whitened: apply LSQR to $W A x \\approx W y$, realized via $v \\mapsto W(A v)$ and $u \\mapsto A^{\\top}(W^{\\top} u)$.\n\nUse the following convergence assessment aligned with the weighted residual: let $r_{0} = \\| W y \\|_{2}$, and for the LSQR approximation $x_{k}$ at iteration $k$, let $r_{k} = \\| W (A x_{k} - y) \\|_{2}$. Define a relative tolerance $\\tau$ and record the smallest iteration index $k$ such that $r_{k} \\leq \\tau r_{0}$, or the iteration limit if no such $k$ occurs.\n\nTest Suite:\n- Case 1 (happy path): $m = 150$, $n = 100$, $\\rho = 0.7$, $\\sigma = 0.05$, base-$10$ condition exponent $c = 4$ (so that $\\max s_{i} / \\min s_{i} \\approx 10^{c}$), random seed $s = 1$, iteration limit $K = 300$, tolerance $\\tau = 10^{-3}$.\n- Case 2 (boundary, identity covariance): $m = 150$, $n = 100$, $\\rho = 0.0$, $\\sigma = 0.05$, $c = 4$, $s = 2$, $K = 300$, $\\tau = 10^{-3}$.\n- Case 3 (strong correlation): $m = 120$, $n = 80$, $\\rho = 0.95$, $\\sigma = 0.10$, $c = 5$, $s = 3$, $K = 400$, $\\tau = 10^{-3}$.\n- Case 4 (edge, small dimension, very strong correlation): $m = 60$, $n = 60$, $\\rho = 0.98$, $\\sigma = 0.05$, $c = 4$, $s = 4$, $K = 400$, $\\tau = 10^{-3}$.\n\nRequired Output:\n- For each test case, compute the integer difference $\\Delta k = k_{\\text{whitened}} - k_{\\text{unwhitened}}$, where $k_{\\text{whitened}}$ and $k_{\\text{unwhitened}}$ are the iteration counts needed to satisfy $r_{k} \\leq \\tau r_{0}$ as defined above, with the iteration count capped at the iteration limit if the tolerance is not reached.\n- Your program should produce a single line of output containing the four differences as a comma-separated list enclosed in square brackets (e.g., \"[$\\Delta k_{1}$,$\\Delta k_{2}$,$\\Delta k_{3}$,$\\Delta k_{4}$]\").\n\nNo physical or angle units are involved; all quantities are dimensionless. Ensure all computations are deterministic by using the provided random seeds. The program must be self-contained and require no user input.",
            "solution": "The user wants to analyze the impact of whitening on the convergence of the LSQR algorithm for a linear inverse problem with correlated noise.\n\nThe problem is stated as solving for a state vector $x \\in \\mathbb{R}^{n}$ from observations $y \\in \\mathbb{R}^{m}$ given the linear model $y = A x_{\\text{true}} + e$, where $A \\in \\mathbb{R}^{m \\times n}$ is the observation operator and $e \\in \\mathbb{R}^{m}$ is a noise vector. The noise is modeled as a zero-mean Gaussian random variable, $e \\sim \\mathcal{N}(0, R)$, with a symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$.\n\n### 1. Problem Formulation: Weighted Least Squares\n\nThe presence of a non-identity covariance matrix $R$ means the components of the noise vector $e$ are correlated and/or have non-uniform variance. The maximum likelihood principle for Gaussian noise dictates that the optimal estimate of $x$ is found by minimizing the Mahalanobis distance, which leads to the weighted least squares (WLS) problem:\n$$ \\min_{x} (A x - y)^{\\top} R^{-1} (A x - y) $$\nThis objective function can be expressed in terms of the Euclidean norm by introducing a \"whitening\" matrix $W \\in \\mathbb{R}^{m \\times m}$ such that $W^{\\top}W = R^{-1}$. The WLS problem is then equivalent to the standard (unweighted) least squares problem:\n$$ \\min_{x} \\|W(A x - y)\\|_{2}^{2} $$\nA numerically stable and computationally efficient choice for $W$ is derived from the Cholesky factorization of $R$. Since $R$ is symmetric and positive definite, it admits a unique decomposition $R = L L^{\\top}$, where $L$ is a lower-triangular matrix. From this, we have $R^{-1} = (L^{\\top})^{-1}L^{-1} = (L^{-1})^{\\top}L^{-1}$. We can therefore define the whitening operator as $W = L^{-1}$. Applying this operator to a vector $v$ is equivalent to solving the lower-triangular system $Lz = v$ for $z$ via forward substitution, which is computationally efficient. The whitened linear system is thus $(WA)x \\approx Wy$.\n\n### 2. The LSQR Algorithm and Golub-Kahan Bidiagonalization\n\nLSQR is an iterative Krylov subspace method developed by Paige and Saunders for solving sparse or structured least squares problems of the form $\\min_{x} \\| M x - b \\|_{2}^{2}$. It is numerically superior to methods based on the normal equations $M^{\\top} M x = M^{\\top} b$ as it avoids the explicit formation of $M^{\\top} M$, which can square the condition number of the problem.\n\nThe foundation of LSQR is the Golub-Kahan bidiagonalization (GKB) process. For a matrix $M \\in \\mathbb{R}^{m \\times n}$ and a starting vector $b \\in \\mathbb{R}^{m}$, GKB generates a sequence of orthonormal vectors $\\{u_k\\}_{k=1}^{i+1} \\subset \\mathbb{R}^{m}$ and $\\{v_k\\}_{k=1}^{i} \\subset \\mathbb{R}^{n}$ such that:\n$$ M V_i = U_{i+1} B_i $$\n$$ M^{\\top} U_i = V_i B_i^{\\top} $$\nwhere $V_i = [v_1, \\dots, v_i]$ and $U_{i+1} = [u_1, \\dots, u_{i+1}]$ have orthonormal columns, and $B_i$ is a lower bidiagonal matrix of size $(i+1) \\times i$. LSQR uses this process, with $u_1 = b/\\|b\\|_2$, to find an approximate solution $x_k \\in \\operatorname{span}\\{v_1, \\dots, v_k\\}$ by solving the small bidiagonal least squares problem $\\min_{z} \\| B_k z - \\|b\\|_2 e_1 \\|_{2}^{2}$ at each iteration $k$. An efficient recursive update for $x_k$ is used, avoiding the need to solve this small system from scratch at each step.\n\n### 3. Comparing Unwhitened and Whitened Approaches\n\nThe problem requires a comparison of LSQR convergence for two distinct setups:\n\n1.  **Unwhitened LSQR**: The algorithm is applied directly to the original, unweighted system $\\min_x \\| A x - y \\|_{2}^{2}$. Here, the matrix for GKB is $M=A$ and the right-hand side is $b=y$. The convergence rate of LSQR is governed by the singular value distribution (the spectrum) of $A$. The iterates $x_k$ produced by this method do not minimize the correct WLS objective function at each step.\n\n2.  **Whitened LSQR**: The algorithm is applied to the whitened system $\\min_x \\| (WA) x - (Wy) \\|_{2}^{2}$. Here, the matrix is $M=WA$ and the right-hand side is $b=Wy$. The convergence is now governed by the singular value distribution of the whitened operator $WA$. This approach directly minimizes the correct WLS objective function.\n\nThe core of the comparison lies in how whitening affects the spectrum. The operator $W$ effectively acts as a left preconditioner. If $R$ is far from a scaled identity matrix (i.e., if noise is significantly correlated), the singular values of $WA$ may be more favorably clustered or have a smaller condition number than those of $A$, typically leading to faster convergence of LSQR toward the true WLS solution.\n\n### 4. Convergence Assessment\n\nTo ensure a fair comparison, the convergence for both methods is measured against the true WLS residual norm. For an iterate $x_k$ (from either method), the weighted residual norm is $r_k = \\| W (A x_k - y) \\|_{2}$. The initial residual norm is $r_0 = \\| W (A x_0 - y) \\|_{2} = \\| W y \\|_{2}$ (for $x_0=0$). The stopping criterion is $r_k \\leq \\tau r_0$ for a given tolerance $\\tau$.\n\n-   For the **whitened** run, $r_k$ is precisely the residual norm that LSQR internally minimizes ($\\| (WA)x_k - (Wy) \\|_{2}$), so the standard LSQR stopping criterion based on its recursively updated residual norm estimate can be used.\n-   For the **unwhitened** run, the internally tracked residual is $\\| A x_k - y \\|_{2}$. This is different from the required $r_k$. Therefore, at each iteration, the iterate $x_k$ must be explicitly formed, and the weighted residual norm $r_k = \\| W (A x_k - y) \\|_{2}$ must be computed and checked against the criterion.\n\nThe analysis will quantify the difference in the number of iterations, $\\Delta k = k_{\\text{whitened}} - k_{\\text{unwhitened}}$, to reach the desired tolerance. A negative $\\Delta k$ indicates that whitening accelerates convergence.\n\n### 5. Algorithmic Implementation\n\nThe implementation proceeds as follows:\n1.  For each test case, set the random seed for reproducibility.\n2.  Construct the matrix $A$ with a prescribed singular value distribution. This involves generating random orthogonal matrices $U$ and $V$ via QR factorization of Gaussian random matrices and forming $A = U_p \\operatorname{diag}(s) V_p^{\\top}$.\n3.  Construct the Toeplitz covariance matrix $R_{ij} = \\sigma^2 \\rho^{|i-j|}$ and compute its Cholesky factor $L$, so that $R=LL^{\\top}$.\n4.  Define the true state vector $x_{\\text{true}}$, generate the correlated noise vector $e = Lz$ where $z \\sim \\mathcal{N}(0, I_m)$, and form the observation vector $y = Ax_{\\text{true}} + e$.\n5.  Implement a general-purpose LSQR function that takes matrix-vector product operators as input. This function will be configured to handle both internal and external stopping criteria.\n6.  **Unwhitened Run**: Call the LSQR function with operators for $A$ and $A^{\\top}$, and the right-hand side $y$. Use an external convergence check based on the weighted residual norm $r_k$.\n7.  **Whitened Run**: Call the LSQR function with operators for $WA$ and $(WA)^{\\top}=A^{\\top}W^{\\top}$, and the right-hand side $Wy$. Use the internal LSQR stopping criterion, which corresponds to the weighted residual norm.\n8.  Calculate and store the difference in iteration counts, $\\Delta k$.\n9.  After processing all test cases, format the results as a comma-separated list.\n\nThe case with $\\rho=0.0$ serves as a critical sanity check. Here, $R = \\sigma^2 I$, so $W = (1/\\sigma)I$. The whitened system is just a scaling of the unwhitened one, to which LSQR is invariant. The convergence criterion also simplifies to be identical for both runs. Thus, for this case, we expect $k_{\\text{whitened}} = k_{\\text{unwhitened}}$, resulting in $\\Delta k = 0$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz, cholesky, solve_triangular\n\ndef run_lsqr(M_mv, Mt_mv, b, max_iter, tol, # Core LSQR params\n             check_mv=None, check_rhs=None, check_ref_norm=None): # External check params\n    \"\"\"\n    Solves min ||M*x - b||_2^2 using LSQR.\n\n    The stopping criterion depends on the `check_mv` parameter.\n    If `check_mv` is None (internal check):\n        Uses the LSQR estimate for the residual norm ||M*x - b||.\n        Stops when ||M*x_k - b||_2 <= tol * ||b||_2.\n    If `check_mv` is not None (external check):\n        Computes x_k at each step and evaluates an external condition.\n        Stops when ||check_mv(x_k) - check_rhs||_2 <= tol * check_ref_norm.\n    \n    Returns the number of iterations taken.\n    \"\"\"\n    use_external_check = check_mv is not None\n\n    # Get dimension of x from the transpose matrix-vector product\n    n = Mt_mv(np.zeros(b.shape[0])).shape[0]\n    x = np.zeros(n)\n\n    beta = np.linalg.norm(b)\n    if beta == 0: \n        return 0\n    u = b / beta\n\n    v_tilde = Mt_mv(u)\n    alpha = np.linalg.norm(v_tilde)\n    if alpha == 0:\n        return 0\n    v = v_tilde / alpha\n    \n    w = v.copy()\n    phi_bar = beta\n    rho_bar = alpha\n\n    # This is the reference norm for the internal check.\n    internal_ref_norm = beta\n    \n    for k in range(1, max_iter + 1):\n        # Step 1: Golub-Kahan bidiagonalization\n        p = M_mv(v) - alpha * u\n        beta = np.linalg.norm(p)\n        if beta > 1e-15:\n            u = p / beta\n        \n        q = Mt_mv(u) - beta * v\n        alpha = np.linalg.norm(q)\n        if alpha > 1e-15:\n            v = q / alpha\n\n        # Step 2: Plane rotation to update QR factorization of the bidiagonal matrix\n        rho = np.sqrt(rho_bar**2 + beta**2)\n        c = rho_bar / rho\n        s = beta / rho\n        \n        theta = s * alpha\n        rho_bar = -c * alpha\n        phi = c * phi_bar\n        phi_bar = s * phi_bar\n        \n        # Step 3: Update solution x and search direction w\n        x = x + (phi / rho) * w\n        w = v - (theta / rho) * w\n\n        # Step 4: Convergence Check\n        if use_external_check:\n            # For the unwhitened case, the check is on the weighted residual\n            residual_vec = check_mv(x) - check_rhs\n            residual_norm = np.linalg.norm(residual_vec)\n            if residual_norm <= tol * check_ref_norm:\n                return k\n        else:\n            # For the whitened case, the check is on ||WAx-Wy|| which is the\n            # internal LSQR residual norm, estimated by |phi_bar|.\n            residual_norm_estimate = abs(phi_bar)\n            if residual_norm_estimate <= tol * internal_ref_norm:\n                return k\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        # (m, n, rho, sigma, c, seed, K, tau)\n        (150, 100, 0.7, 0.05, 4, 1, 300, 1e-3),\n        (150, 100, 0.0, 0.05, 4, 2, 300, 1e-3),\n        (120, 80, 0.95, 0.10, 5, 3, 400, 1e-3),\n        (60, 60, 0.98, 0.05, 4, 4, 400, 1e-3),\n    ]\n\n    results = []\n    \n    for m, n, rho, sigma, c, seed, K, tau in test_cases:\n        np.random.seed(seed)\n        \n        # 1. Construct matrix A with a specified singular value distribution\n        p = min(m, n)\n        svals = np.logspace(0, -c, p)\n        U_rand = np.random.randn(m, m)\n        V_rand = np.random.randn(n, n)\n        U, _ = np.linalg.qr(U_rand)\n        V, _ = np.linalg.qr(V_rand)\n        A = U[:, :p] @ np.diag(svals) @ V[:, :p].T\n\n        # 2. Construct covariance R and its Cholesky factor L\n        indices_m = np.arange(m)\n        R_row0 = sigma**2 * rho**indices_m\n        R = toeplitz(R_row0)\n        L = cholesky(R, lower=True)\n\n        # 3. Construct true state xtrue and observations y\n        indices_n = np.arange(n)\n        x_true = np.sin(2 * np.pi * indices_n / n) + 0.5 * np.sin(4 * np.pi * indices_n / n)\n        z = np.random.randn(m)\n        e = L @ z\n        y = A @ x_true + e\n        \n        # 4. Define operators for matrix-vector products\n        def A_mv(v): return A @ v\n        def At_mv(u): return A.T @ u\n        def W_mv(v): return solve_triangular(L, v, lower=True, check_finite=False)\n        def Wt_mv(u): return solve_triangular(L.T, u, lower=False, check_finite=False)\n        \n        # 5. Run Unwhitened LSQR: min ||A*x - y||\n        # The convergence check is on the weighted residual: ||W(Ax - y)||\n        def unwhitened_check_mv(x_k): return W_mv(A_mv(x_k) - y)\n        unwhitened_ref_norm = np.linalg.norm(W_mv(y))\n\n        k_unwhitened = run_lsqr(A_mv, At_mv, y, K, tau,\n                                check_mv=unwhitened_check_mv,\n                                check_rhs=np.zeros_like(y), # check is ||W(Ax-y)|| vs 0\n                                check_ref_norm=unwhitened_ref_norm)\n        \n        # 6. Run Whitened LSQR: min ||(WA)*x - (Wy)||\n        # The convergence check is internal to LSQR\n        def WA_mv(v): return W_mv(A_mv(v))\n        def AtWt_mv(u): return At_mv(Wt_mv(u))\n        b_whitened = W_mv(y)\n\n        k_whitened = run_lsqr(WA_mv, AtWt_mv, b_whitened, K, tau)\n\n        # 7. Calculate the difference in iterations\n        delta_k = k_whitened - k_unwhitened\n        results.append(delta_k)\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}