{
    "hands_on_practices": [
        {
            "introduction": "对任何算法的深刻理解都始于亲手计算。本练习旨在通过引导您对一个定义明确的小型系统进行分步计算，从而揭开共轭梯度（CG）方法的神秘面纱。通过手动计算迭代向量并验证关键的正交性和 A-共轭性，您将为CG方法为何如此高效建立起坚实的直观认识。",
            "id": "3371322",
            "problem": "考虑一个线性高斯逆问题，其中最大后验 (MAP) 估计是通过最小化严格凸二次目标函数 $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$（关于 $x \\in \\mathbb{R}^{3}$）得到的。信息（海森）矩阵 $A$ 是对称正定的，其已知特征值为 $\\{1, 2, 4\\}$，并由以下公式明确给出：\n$$\nA \\;=\\; \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2}  0 \\\\\n-\\frac{1}{2}  \\frac{3}{2}  0 \\\\\n0  0  4\n\\end{pmatrix}.\n$$\n数据向量为 $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$，初始猜测值为 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n从表征对称正定系统共轭梯度 (CG) 法的核心定义及其在由初始残差生成的克雷洛夫子空间上关于 $A$-内积的最速下降解释出发，执行恰好三次 CG 迭代以最小化 $J(x)$，并根据 CG 内在的正交化条件构造搜索方向。精确计算所有量（不进行浮点舍入），并在每次迭代中：\n1. 通过沿当前搜索方向最小化 $J$ 来确定步长。\n2. 更新从基本正交关系导出的近似解、残差和下一个搜索方向。\n3. 对您执行的三次迭代，验证搜索方向的 $A$-共轭性，即对于 $i \\neq j$ 有 $p_{i}^{\\top} A p_{j} = 0$，以及残差的相互正交性，即对于 $i \\neq j$ 有 $r_{i}^{\\top} r_{j} = 0$。\n\n以单个行向量的形式报告三次迭代后的最终近似解 $x_{3}$。请精确表达您的答案，不要四舍五入。您的最终答案必须是单个数学对象。",
            "solution": "本问题要求应用共轭梯度(CG)法来最小化二次目标函数 $J(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$。$J(x)$ 的最小化器是线性系统 $Ax = b$ 的解。CG 法是求解此类系统的迭代算法，其中 $A$ 是对称且正定的。\n\n从初始猜测值 $x_0$ 开始，共轭梯度算法的核心公式如下：\n初始残差：$r_0 = b - A x_0$\n初始搜索方向：$p_0 = r_0$\n\n对于每次迭代 $k = 0, 1, 2, \\dots$：\n步长：$\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$\n解的更新：$x_{k+1} = x_k + \\alpha_k p_k$\n残差的更新：$r_{k+1} = r_k - \\alpha_k A p_k$\n搜索方向参数：$\\beta_k = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}$\n搜索方向的更新：$p_{k+1} = r_{k+1} + \\beta_k p_k$\n\n让我们对给定的 $A$、$b$ 和 $x_0$ 应用此算法。\n已知：\n$$\nA = \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2}  0 \\\\\n-\\frac{1}{2}  \\frac{3}{2}  0 \\\\\n0  0  4\n\\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n**初始化 ($k=0$)：**\n初始残差为：\n$$\nr_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n第一个搜索方向是初始残差：\n$$\np_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n\n**迭代 1 ($k=0$)：**\n我们首先计算乘积 $A p_0$：\n$$\nA p_0 = \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2}  0 \\\\\n-\\frac{1}{2}  \\frac{3}{2}  0 \\\\\n0  0  4\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} \\\\ -\\frac{1}{2} + \\frac{3}{2} \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}\n$$\n步长 $\\alpha_0$ 通过最小化 $J(x_0+\\alpha p_0)$ 计算得出：\n$$\nr_0^{\\top} r_0 = 1^2 + 1^2 + 1^2 = 3\n$$\n$$\np_0^{\\top} A p_0 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = 1 + 1 + 4 = 6\n$$\n$$\n\\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} = \\frac{3}{6} = \\frac{1}{2}\n$$\n更新解：\n$$\nx_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\n更新残差：\n$$\nr_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix}\n$$\n更新搜索方向：\n$$\nr_1^{\\top} r_1 = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + (-1)^2 = \\frac{1}{4} + \\frac{1}{4} + 1 = \\frac{3}{2}\n$$\n$$\n\\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} = \\frac{3/2}{3} = \\frac{1}{2}\n$$\n$$\np_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix}\n$$\n\n**迭代 2 ($k=1$)：**\n计算乘积 $A p_1$：\n$$\nA p_1 = \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2}  0 \\\\\n-\\frac{1}{2}  \\frac{3}{2}  0 \\\\\n0  0  4\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} - \\frac{1}{2} \\\\ -\\frac{1}{2} + \\frac{3}{2} \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\n计算步长 $\\alpha_1$：\n$$\np_1^{\\top} A p_1 = \\begin{pmatrix} 1  1  -1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = 1 + 1 + 1 = 3\n$$\n$$\n\\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} = \\frac{3/2}{3} = \\frac{1}{2}\n$$\n更新解：\n$$\nx_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\n更新残差：\n$$\nr_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n由于 $r_2=0$，算法已收敛到精确解 $x_2$。对于一个 $n \\times n$ 的系统，CG 方法保证在至多 $n$ 次迭代内找到精确解；在本例中，收敛发生在2次迭代内，因为初始残差 $r_0$ 位于一个二维克雷洛夫子空间中（由对应于特征值1和4的特征向量张成）。\n\n**迭代 3 ($k=2$)：**\n题目要求“精确地进行三次CG迭代”。CG的标准实现会在发现零残差时终止。为了满足要求，我们形式上继续进行。\n更新搜索方向：\n$$\nr_2^{\\top} r_2 = 0^2 + 0^2 + 0^2 = 0\n$$\n$$\n\\beta_1 = \\frac{r_2^{\\top} r_2}{r_1^{\\top} r_1} = \\frac{0}{3/2} = 0\n$$\n$$\np_2 = r_2 + \\beta_1 p_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n下一步将是计算 $\\alpha_2 = \\frac{r_2^{\\top} r_2}{p_2^{\\top} A p_2} = \\frac{0}{0}$，这是不确定的。然而，对解的更新是 $x_3 = x_2 + \\alpha_2 p_2 = x_2 + \\alpha_2 \\cdot 0 = x_2$。任何对 $\\alpha_2$ 的选择都会导致 $x_3=x_2$。因此，系统状态不发生改变。\n$$\nx_3 = x_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\n残差也将保持为零：$r_3 = r_2 - \\alpha_2 A p_2 = 0 - \\alpha_2 \\cdot 0 = 0$。\n\n**正交性验证：**\n问题要求对完成的三次迭代验证搜索方向的 $A$-共轭性和残差的正交性。生成的向量是：\n$p_0 = \\begin{pmatrix} 1  1  1 \\end{pmatrix}^{\\top}$, $p_1 = \\begin{pmatrix} 1  1  -1/2 \\end{pmatrix}^{\\top}$, $p_2 = \\begin{pmatrix} 0  0  0 \\end{pmatrix}^{\\top}$\n$r_0 = \\begin{pmatrix} 1  1  1 \\end{pmatrix}^{\\top}$, $r_1 = \\begin{pmatrix} 1/2  1/2  -1 \\end{pmatrix}^{\\top}$, $r_2 = \\begin{pmatrix} 0  0  0 \\end{pmatrix}^{\\top}$\n\n1.  **搜索方向的 $A$-共轭性 ($p_i^{\\top} A p_j = 0$ for $i \\neq j$):**\n    -   $p_0^{\\top} A p_1$:\n        $$p_0^{\\top} (A p_1) = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = 1 + 1 - 2 = 0$$\n    -   $p_0^{\\top} A p_2$:\n        $$p_0^{\\top} (A p_2) = p_0^{\\top} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    -   $p_1^{\\top} A p_2$:\n        $$p_1^{\\top} (A p_2) = p_1^{\\top} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    搜索方向是 $A$-共轭的。\n\n2.  **残差的正交性 ($r_i^{\\top} r_j = 0$ for $i \\neq j$):**\n    -   $r_0^{\\top} r_1$:\n        $$r_0^{\\top} r_1 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ -1 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$$\n    -   $r_0^{\\top} r_2$:\n        $$r_0^{\\top} r_2 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    -   $r_1^{\\top} r_2$:\n        $$r_1^{\\top} r_2 = \\begin{pmatrix} 1/2  1/2  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$$\n    残差是相互正交的。\n\n三次迭代后的最终近似解是 $x_3$。如前所示，$x_3 = x_2$。\n$$\nx_3 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/4 \\end{pmatrix}\n$$\n我们可以验证这是否为精确解 $x = A^{-1}b$：\n$$\nA^{-1} = \\begin{pmatrix} \\frac{3}{4}  \\frac{1}{4}  0 \\\\ \\frac{1}{4}  \\frac{3}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix}\n$$\n$$\nx = A^{-1}b = \\begin{pmatrix} \\frac{3}{4}  \\frac{1}{4}  0 \\\\ \\frac{1}{4}  \\frac{3}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} + \\frac{1}{4} \\\\ \\frac{1}{4} + \\frac{3}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\frac{1}{4} \\end{pmatrix}\n$$\n结果是正确的。最终答案要求以单个行向量的形式给出。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  \\frac{1}{4}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在掌握了针对良态对称系统的基本方法后，我们转向更具挑战性的不适定反演问题领域。本练习将展示克雷洛夫子空间方法最强大的特性之一：迭代正则化。您将构建一个含强噪声的问题，并通过比较直接求解器与LSQR算法的结果，亲眼见证提前终止迭代如何防止噪声被放大，从而在直接法失效时获得有意义的解。",
            "id": "3371338",
            "problem": "考虑求解形如 $A x \\approx b$ 的线性反问题，其中 $A \\in \\mathbb{R}^{m \\times n}$，矩阵 $A$ 的奇异值缓慢衰减，且数据 $b$ 被强加性噪声污染。本题的目标是检验用于最小二乘问题的正交三角分解 (QR) 方法在没有显式正则化的情况下的表现，并将其与通过特定准则提前终止时表现出迭代正则化效应的类QR迭代最小二乘法 (LSQR) 进行对比。问题背景为反问题和数据同化，其核心问题在于不适定性和噪声放大。\n\n使用以下基本概念：\n- 奇异值分解 (SVD)：对于矩阵 $A$，可写作 $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交矩阵，$\\Sigma$ 是对角元为非负数 $\\sigma_i$ 的对角矩阵。\n- 最小二乘正交三角分解法：通过计算正交三角分解 $A = Q R$（其中 $Q$ 是正交矩阵，$R$ 是上三角矩阵），然后求解 $R x = Q^{\\top} b$ 来求解 $\\min_{x} \\lVert A x - b \\rVert_2$。\n- 迭代正则化：在共轭梯度最小二乘法 (CGLS) 和类QR迭代最小二乘法 (LSQR) 等算法中，提前终止可以作为一种正则化机制，抑制小奇异值的影响，否则这些小奇异值会放大噪声。\n\n您必须实现一个独立的程序，构建一个合成测试问题，其中矩阵 $A$ 的奇异值缓慢衰减，向量 $b$ 包含强噪声，然后验证：\n- 未正则化的QR最小二乘法对噪声产生过拟合，与通过偏差原理终止的LSQR相比，其重构误差更高。\n- 完全迭代的LSQR（运行至实际最大迭代次数）与适当的提前终止相比，同样会产生过拟合。\n\n您必须纯粹以数学方式进行：\n1. 对于每个测试用例，令 $m = n$，并定义 $A$ 为一个对角矩阵，其对角元为 $\\sigma_i = (i+1)^{-p}$，$i = 0, 1, \\dots, n-1$，其中 $p$ 是一个控制奇异值缓慢衰减的正指数。这种选择使 $A$ 成为一个具有缓慢衰减奇异值的紧算子，这是一个典型的轻度不适定场景。\n2. 定义真值向量 $x_{\\text{true}} \\in \\mathbb{R}^n$ 为 $x_{\\text{true},i} = (-1)^i (i+1)^{-1}$，这是一个系数衰减的平滑信号。\n3. 构造干净的右端项 $b_{\\text{clean}} = A x_{\\text{true}}$。\n4. 添加强噪声：设 $\\eta$ 是一个各分量为独立标准正态分布的随机向量，并对其进行缩放，使得对于指定的相对噪声水平 $\\delta > 0$，满足 $\\lVert \\eta \\rVert_2 = \\delta \\lVert b_{\\text{clean}} \\rVert_2$。设 $b = b_{\\text{clean}} + \\eta$。所有范数均为欧几里得范数。\n5. 通过正交三角分解和回代法，计算 $\\min_x \\lVert A x - b \\rVert_2$ 的未正则化QR最小二乘解 $x_{\\text{QR}}$。\n6. 从 $x_0 = 0$ 开始，实现 LSQR 迭代（由 Paige 和 Saunders 提出）。使用参数为 $\\tau > 1$ 的 Morozov 偏差原理来选择终止迭代步数 $k_{\\text{stop}}$，即满足 $\\lVert A x_k - b \\rVert_2 \\le \\tau \\lVert \\eta \\rVert_2$ 的最小 $k$，其中 $x_k$ 是第 $k$ 次 LSQR 迭代的结果。同时，通过运行 LSQR 固定的最大迭代次数（等于 $n$），计算一个“完全迭代”的 LSQR 解 $x_{\\text{LSQR-full}}$。\n7. 将重构误差量化为相对误差 $E(\\hat{x}) = \\lVert \\hat{x} - x_{\\text{true}} \\rVert_2 / \\lVert x_{\\text{true}} \\rVert_2$，其中 $\\hat{x} \\in \\{ x_{\\text{QR}}, x_{k_{\\text{stop}}}, x_{\\text{LSQR-full}} \\}$。\n8. 对于每个测试用例，您的程序必须返回两个布尔值：\n   - $E(x_{k_{\\text{stop}}})  E(x_{\\text{QR}})$ 是否成立。\n   - $E(x_{k_{\\text{stop}}})  E(x_{\\text{LSQR-full}})$ 是否成立。\n这些布尔值用于检验以下论断：未正则化的QR方法会产生过拟合，而LSQR的迭代正则化在适当终止时是成功的。\n\n不涉及物理单位或角度；所有量均为无量纲。\n\n测试套件：\n提供三个测试用例 $(n, p, \\delta, \\tau)$ 来探测不同情况：\n- 用例 1: $n = 80$, $p = 0.3$, $\\delta = 0.2$, $\\tau = 1.1$ (强噪声，缓慢衰减)。\n- 用例 2: $n = 80$, $p = 0.5$, $\\delta = 0.1$, $\\tau = 1.05$ (中等噪声，更慢衰减)。\n- 用例 3: $n = 50$, $p = 0.2$, $\\delta = 0.3$, $\\tau = 1.1$ (非常强的噪声，非常缓慢的衰减)。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按测试用例的顺序包含六个布尔值，每个用例先输出与QR的比较结果，然后是与完全迭代的LSQR的比较结果。例如，输出格式为 $[\\text{b}_1,\\text{b}_2,\\text{b}_3,\\text{b}_4,\\text{b}_5,\\text{b}_6]$，其中每个 $\\text{b}_i$ 是 True 或 False。",
            "solution": "该问题要求分析和比较两种用于求解线性系统 $A x \\approx b$ 的方法，该系统是离散不适定反问题的典型特征。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是病态的，具体表现为其奇异值缓慢衰减至零，并且数据向量 $b \\in \\mathbb{R}^{n}$ 被显著的加性噪声污染。我们将把通过正交三角 (QR) 分解得到的直接、未正则化的最小二乘解，与一种迭代方法 LSQR 进行对比，后者在提前终止时表现出正则化特性。\n\n问题的核心在于病态算子的“逆”对噪声的放大效应。如果数据 $b$ 的微小扰动会导致解 $x$ 的巨大变化，则称该线性系统是不适定的。对于具有奇异值分解 (SVD) $A = U \\Sigma V^{\\top}$ 的矩阵 $A$，最小二乘问题 $\\min_{x} \\lVert A x - b \\rVert_2$ 的解在形式上为 $x = A^{\\dagger} b = V \\Sigma^{\\dagger} U^{\\top} b$，其中 $\\dagger$ 表示伪逆。如果矩阵 $A$ 的奇异值 $\\sigma_i$ 在衰减到零的过程中没有明显的间断，那么 $A^{\\dagger}$ 的相应奇异值（即 $1/\\sigma_i$）会变得非常大。当数据含噪声时，即 $b = b_{\\text{true}} + \\eta$，解变为 $x = A^{\\dagger}b_{\\text{true}} + A^{\\dagger}\\eta = x_{\\text{true}} + A^{\\dagger}\\eta$。其中 $A^{\\dagger}\\eta$ 项代表传播的噪声。与小奇异值 $\\sigma_i$ 相关的分量会被极大地放大，从而使解变得毫无意义。\n\n本题的构造正是为了展示这种行为。矩阵 $A$ 被定义为对角矩阵，因此其对角元就是它的奇异值，即 $\\sigma_i = (i+1)^{-p}$，$i = 0, 1, \\dots, n-1$。当 $p  0$ 时，这些值缓慢衰减。真值信号 $x_{\\text{true}}$ 是平滑的。在干净数据 $b_{\\text{clean}} = A x_{\\text{true}}$ 中加入噪声 $\\eta$，得到 $b = b_{\\text{clean}} + \\eta$。\n\n**1. 未正则化的 QR 最小二乘解 ($x_{\\text{QR}}$)**\n\n第一种方法是直接求解最小二乘问题。对于一个方阵、可逆矩阵 $A$，这等价于求解 $A x = b$。题目指定使用 QR 分解，$A=QR$，其中 $Q$ 是正交矩阵，$R$ 是上三角矩阵。最小二乘解通过求解三角系统 $R x = Q^{\\top} b$ 得到。对于病态矩阵 $A$，此过程在数值上等价于直接求逆，并不能减缓噪声放大。计算出的解 $x_{\\text{QR}}$ 将接近于真实的最小二乘解 $A^{-1}b$：\n$$x_{\\text{QR}} \\approx A^{-1}(A x_{\\text{true}} + \\eta) = x_{\\text{true}} + A^{-1} \\eta$$\n误差项 $A^{-1}\\eta$ 包含分量 $(A^{-1}\\eta)_i = \\sigma_i^{-1} \\eta_i = (i+1)^p \\eta_i$。由于 $p0$，当 $i$ 较大时，该项会爆炸性增长，导致解 $x_{\\text{QR}}$ 被放大的噪声所主导，从而产生巨大的重构误差 $E(x_{\\text{QR}})$。这种现象被称为对噪声的“过拟合”。\n\n**2. 带迭代正则化的 LSQR ($x_{k_{\\text{stop}}}$)**\n\nLSQR 是一种求解最小二乘问题的迭代算法，它在数学上等价于应用于正规方程 $A^{\\top}A x = A^{\\top}b$ 的共轭梯度法，但具有更高的数值稳定性。此类迭代方法的一个关键特性是，在第 $k$ 次迭代时的解估计值 $x_k$ 主要捕捉与 $A$ 的较大奇异值相关的信息。而与小奇异值对应的解分量（这些分量最易受噪声放大影响）只在后续的迭代中才被引入。\n\n这种行为实现了“迭代正则化”：通过提前终止算法，我们能有效地滤除一大部分被放大的噪声。解 $x_k$ 被限制在 Krylov 子空间 $\\mathcal{K}_k(A^\\top A, A^\\top b)$ 中，该子空间起到了对问题进行正则化的低维投影作用。这个子空间的维度 $k$ 就是正则化参数。\n\n**3. Morozov 偏差原理**\n\n为使迭代正则化有效，需要一个有原则的终止准则。Morozov 偏差原理就提供了这样一种准则。它基于这样一种思想：所寻求的解对数据的拟合程度不应超过数据中的噪声水平。给定一个噪声水平的估计值，在本例中即已知的噪声范数 $\\lVert \\eta \\rVert_2$，我们在残差范数降至与此噪声水平相关的某一阈值以下的第一次迭代 $k$ 处停止：\n$$\\lVert A x_k - b \\rVert_2 \\le \\tau \\lVert \\eta \\rVert_2$$\n此处，$\\tau  1$ 是一个安全因子。在这一 $k_{\\text{stop}}$ 步终止可以得到一个正则化的解 $x_{k_{\\text{stop}}}$，该解在数据保真度与噪声抑制之间取得了平衡。\n\n**4. 完全迭代的 LSQR ($x_{\\text{LSQR-full}}$)**\n\n如果 LSQR 运行了足够多的迭代次数（例如，对于一个 $n \\times n$ 的非奇异矩阵，迭代 $n$ 次），它将收敛到未正则化的最小二乘解。因此，“完全迭代”的解 $x_{\\text{LSQR-full}}$ 将与 $x_{\\text{QR}}$ 近似相同，并且同样会遭受严重的噪声放大。\n\n**总结**\n\n该数值实验旨在证明：\n-   $E(x_{k_{\\text{stop}}})  E(x_{\\text{QR}})$：正则化的 LSQR 解优于未正则化的直接解，因为提前终止阻止了噪声放大。\n-   $E(x_{k_{\\text{stop}}})  E(x_{\\text{LSQR-full}})$：同样的提前终止策略比让 LSQR 算法运行至收敛提供了更好的解，因为后者实际上消除了正则化效应。\n\n该实现将构建指定的矩阵和向量，运行 QR 和 LSQR 求解器，应用偏差原理，并计算重构误差，以验证这些不等式在给定测试用例下成立。我们预期对于所有测试用例，两个布尔比较的结果都将是 `True`，从而证实迭代正则化对于此类问题的有效性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef lsqr_custom(A, b, max_iter, tau, norm_eta):\n    \"\"\"\n    Implements the LSQR algorithm by Paige and Saunders (1982).\n\n    This implementation finds the solution x_k_stop regularized by the\n    discrepancy principle and the fully iterated solution x_full.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    x_stop = None\n    k_stop = -1\n    stop_threshold = tau * norm_eta\n\n    # Initialize\n    beta = np.linalg.norm(b)\n    u = b / beta if beta > 0 else np.zeros(m)\n    \n    Atu = A.T @ u\n    alpha = np.linalg.norm(Atu)\n    v = Atu / alpha if alpha > 0 else np.zeros(n)\n\n    w = v.copy()\n    phi_bar = beta\n    rho_bar = alpha\n\n    # Main iteration loop\n    for k in range(max_iter):\n        # Bidiagonalization\n        Av = A @ v\n        u_hat = Av - alpha * u\n        beta = np.linalg.norm(u_hat)\n        \n        if beta > 1e-15:\n            u = u_hat / beta\n            Atu = A.T @ u\n            v_hat = Atu - beta * v\n            alpha = np.linalg.norm(v_hat)\n            if alpha > 1e-15:\n                v = v_hat / alpha\n            else:\n                v = np.zeros_like(v_hat) # Should not happen in this problem\n        else:\n            beta = 0.0\n            alpha = 0.0 # Should not happen\n\n        # Plane rotation\n        rho = np.sqrt(rho_bar**2 + beta**2)\n        c = rho_bar / rho\n        s = beta / rho\n        \n        theta = s * alpha\n        rho_bar = -c * alpha\n        phi = c * phi_bar\n        phi_bar = s * phi_bar\n        \n        # Update solution and search direction\n        x = x + (phi / rho) * w\n        w = v - (theta / rho) * w\n        \n        # Check discrepancy principle\n        residual_norm = np.abs(phi_bar)\n        if residual_norm = stop_threshold and k_stop == -1:\n            x_stop = x.copy()\n            k_stop = k\n\n    # If the stopping condition was never met, something is wrong,\n    # but for robustness we assign the full solution.\n    if x_stop is None:\n        x_stop = x.copy()\n\n    return x_stop, x\n\ndef run_experiment(n, p, delta, tau):\n    \"\"\"\n    Runs one full test case for the given parameters.\n    \"\"\"\n    # 1. Construct the problem (A, xtrue, b)\n    i_vals = np.arange(n)\n    \n    # Ground truth vector xtrue\n    xtrue = ((-1.0)**i_vals) * ((i_vals + 1.0)**(-1.0))\n    \n    # Diagonal matrix A with slowly decaying singular values\n    singular_values = (i_vals + 1.0)**(-p)\n    A = np.diag(singular_values)\n    \n    # Clean data b_clean\n    b_clean = A @ xtrue\n    \n    # Add strong noise\n    eta_raw = np.random.randn(n)\n    norm_b_clean = np.linalg.norm(b_clean)\n    norm_eta_raw = np.linalg.norm(eta_raw)\n    \n    # Scale noise vector to have the prescribed relative norm\n    eta = eta_raw * (delta * norm_b_clean / norm_eta_raw)\n    norm_eta = np.linalg.norm(eta)\n    \n    # Noisy data b\n    b = b_clean + eta\n\n    # 2. Compute unregularized QR least squares solution\n    Q, R = np.linalg.qr(A)\n    # Solve Rx = Q^T b\n    x_QR = scipy.linalg.solve_triangular(R, Q.T @ b, lower=False)\n    \n    # 3. Compute LSQR solutions\n    max_iterations = n\n    x_lsqr_stop, x_lsqr_full = lsqr_custom(A, b, max_iterations, tau, norm_eta)\n    \n    # 4. Quantify reconstruction errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    err_qr = np.linalg.norm(x_QR - xtrue) / norm_xtrue\n    err_lsqr_stop = np.linalg.norm(x_lsqr_stop - xtrue) / norm_xtrue\n    err_lsqr_full = np.linalg.norm(x_lsqr_full - xtrue) / norm_xtrue\n    \n    # 5. Return boolean comparisons\n    bool1 = err_lsqr_stop  err_qr\n    bool2 = err_lsqr_stop  err_lsqr_full\n    \n    return bool1, bool2\n\ndef solve():\n    # Set a fixed seed for reproducibility of random noise\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, delta, tau)\n        (80, 0.3, 0.2, 1.1),\n        (80, 0.5, 0.1, 1.05),\n        (50, 0.2, 0.3, 1.1),\n    ]\n\n    results = []\n    for case in test_cases:\n        b1, b2 = run_experiment(*case)\n        results.extend([b1, b2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的练习将深入探讨克雷洛夫子空间方法的理论统一性。您将从第一性原理出发，推导LSQR算法所依赖的Golub-Kahan双对角化过程与CGLS算法背后的法方程Lanczos过程之间的联系。随后的计算实验将让您在数值上验证，尽管LSQR和CGLS的表述不同，它们却能生成相同的迭代序列，从而加深您对这些强大算法背后深层数学结构的理解。",
            "id": "3371365",
            "problem": "考虑一个实矩形矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个右端向量 $b \\in \\mathbb{R}^{m}$。设共轭梯度最小二乘（CGLS）算法表示将共轭梯度法应用于正规方程 $A^{\\top} A x = A^{\\top} b$（初始猜测为 $x_{0} = 0$）的方法，设最小二乘QR（LSQR）算法表示基于 Golub–Kahan 双对角化和正交变换来求解最小二乘问题 $\\min_{x \\in \\mathbb{R}^{n}} \\lVert A x - b \\rVert_{2}$ 的方法。设从 $u_{1} = b / \\lVert b \\rVert_{2}$ 开始的 Golub–Kahan 双对角化（GKB）过程产生标准正交基 $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ 和 $V_{k} \\in \\mathbb{R}^{n \\times k}$，以及一个上双对角矩阵 $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ 和正标量 $\\{\\alpha_{j}\\}$ 和 $\\{\\beta_{j}\\}$，使得 $A V_{k} = U_{k+1} B_{k}$。设应用于对称矩阵 $A^{\\top} A$ 且初始向量为 $q_{1} = A^{\\top} b / \\lVert A^{\\top} b \\rVert_{2}$ 的 Lanczos 过程产生一个标准正交基 $Q_{k} \\in \\mathbb{R}^{n \\times k}$ 和一个对称三对角矩阵 $T_{k} \\in \\mathbb{R}^{k \\times k}$，对于某个标量 $\\gamma_{k+1}$，满足 $A^{\\top} A Q_{k} = Q_{k} T_{k} + \\gamma_{k+1} q_{k+1} e_{k}^{\\top}$。\n\n任务：\n- 从 Krylov 子空间的基本定义以及 Golub–Kahan 双对角化和 Lanczos 过程的正交性质出发，推导连接投影正规矩阵 $V_{k}^{\\top} A^{\\top} A V_{k}$ 和 GKB 双对角矩阵 $B_{k}$ 的关系。您的推导必须从第一性原理开始，即 Golub–Kahan 双对角化和 Lanczos 的定义关系、基向量的标准正交性，以及对称矩阵 $M$ 的 Krylov 子空间定义 $\\mathcal{K}_{k}(M, v) = \\mathrm{span}\\{v, M v, \\dots, M^{k-1} v\\}$。不要使用任何未经证明的捷径结果。\n- 使用相同的原理，证明为什么在精确算术中，对于每一个 $k \\in \\{1, 2, \\dots\\}$，由 LSQR 产生的第 $k$ 个迭代解 $x_{k}^{\\mathrm{LSQR}}$ 和由 CGLS 产生的第 $k$ 个迭代解 $x_{k}^{\\mathrm{CGLS}}$ 是相同的，前提是两者都从 $x_{0} = 0$ 开始，并由相同的矩阵对 $(A, b)$ 驱动。\n- 设计一个计算实验，以数值方式验证这些理论等价性。该实验必须：\n  - 实现 LSQR 和 CGLS 以记录完整的迭代序列 $\\{x_{k}\\}_{k=1}^{K}$，其中 $K$ 是预设的迭代上限。如果任一算法由于结构性中断（例如，由于精确达到解而导致更新中出现零分母）而提前终止，则为了报告目的，必须通过重复最后一个可用的迭代解将序列扩展到长度 $K$。\n  - 实现 Golub–Kahan 双对角化以计算给定 $k$ 的 $V_{k}$ 和 $B_{k}$，然后构建并使用 Frobenius 范数比较两个小矩阵 $V_{k}^{\\top} A^{\\top} A V_{k}$ 和 $B_{k}^{\\top} B_{k}$。\n  - 对于每个测试用例，报告两个布尔值：\n    - 第一个布尔值指示欧几里得范数 $\\lVert x_{j}^{\\mathrm{LSQR}} - x_{j}^{\\mathrm{CGLS}} \\rVert_{2}$ 在 $j \\in \\{1, \\dots, K\\}$ 上的最大值是否小于或等于一个预设的容差 $\\varepsilon_{x}$。\n    - 第二个布尔值指示 Frobenius 范数 $\\lVert V_{K}^{\\top} A^{\\top} A V_{K} - B_{K}^{\\top} B_{K} \\rVert_{F}$ 是否小于或等于一个预设的容差 $\\varepsilon_{T}$。\n- 使用以下由矩阵和右端向量组成的测试套件，以及迭代上限，所有参数均确定性地表示：\n  - 测试用例 1（瘦高、满列秩、中度病态）：$m = 8$, $n = 5$，$A \\in \\mathbb{R}^{8 \\times 5}$ 是一个矩形对角矩阵，其元素为 $A_{i,i} = s_{i}$ (对于 $i \\in \\{1, \\dots, 5\\}$) 且 $A_{i,j} = 0$ (否则)，其中 $(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}) = (10, 3, 1, 0.1, 0.01)$。右端向量是 $b \\in \\mathbb{R}^{8}$，其元素为 $b_{j} = \\sin(j)$ (对于 $j \\in \\{1, \\dots, 8\\}$)。使用 $K = 5$。\n  - 测试用例 2（瘦高、秩亏）：$m = 8$, $n = 5$，$A \\in \\mathbb{R}^{8 \\times 5}$ 是一个矩形对角矩阵，其中 $(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}) = (10, 3, 1, 0, 0)$。右端向量是 $b \\in \\mathbb{R}^{8}$，其元素为 $b_{j} = \\cos(j)$ (对于 $j \\in \\{1, \\dots, 8\\}$)。使用 $K = 3$。\n  - 测试用例 3（方阵、对称正定）：$m = n = 6$，$A \\in \\mathbb{R}^{6 \\times 6}$ 是一个对角矩阵，对角线元素为 $(5, 4, 3, 2, 1, 0.5)$。右端向量是 $b \\in \\mathbb{R}^{6}$，其元素为 $b_{j} = \\sin(j)$ (对于 $j \\in \\{1, \\dots, 6\\}$)。使用 $K = 6$。\n  - 测试用例 4（零右端向量）：$A$ 与测试用例 1 中相同，$b = 0 \\in \\mathbb{R}^{8}$。使用 $K = 3$。\n- 使用容差 $\\varepsilon_{x} = 10^{-10}$ 和 $\\varepsilon_{T} = 10^{-12}$。\n- 最终输出格式要求：您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，结果按测试用例 1 到 4 的顺序排列，每个测试用例贡献两个布尔值条目：首先是迭代解等价性布尔值，然后是投影矩阵等价性布尔值。例如，输出必须具有形式 $[b_{1,1}, b_{1,2}, b_{2,1}, b_{2,2}, b_{3,1}, b_{3,2}, b_{4,1}, b_{4,2}]$，其中每个 $b_{i,j}$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$。",
            "solution": "### 第 1 部分：投影正规矩阵恒等式的推导\n\n题目要求我们从第一性原理出发，推导 $V_{k}^{\\top} A^{\\top} A V_{k}$ 和 GKB 双对角矩阵 $B_{k}$ 之间的关系。\n\nGolub-Kahan 双对角化（GKB）过程在 $k$ 步之后，生成标准正交向量集 $\\{u_j\\}_{j=1}^{k+1}$ 和 $\\{v_j\\}_{j=1}^{k}$，它们分别作为矩阵 $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ 和 $V_{k} \\in \\mathbb{R}^{n \\times k}$ 的列。这些矩阵通过方程与一个下双对角矩阵 $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ 相关联：\n$$\nA V_k = U_{k+1} B_k\n$$\n矩阵 $B_k$ 的结构如下：\n$$\nB_k = \\begin{pmatrix}\n\\alpha_1   0  \\dots  0 \\\\\n\\beta_2   \\alpha_2    \\vdots \\\\\n0   \\beta_3  \\ddots  0 \\\\\n\\vdots     \\ddots   \\alpha_k \\\\\n0   \\dots  0  \\beta_{k+1}\n\\end{pmatrix}\n$$\n其中 $\\{\\alpha_j  0\\}$ 和 $\\{\\beta_j  0\\}$ 是该过程生成的标量（假设没有发生中断）。\n\n矩阵 $U_{k+1}$ 和 $V_k$ 的核心性质是它们的标准正交性：\n$$\nU_{k+1}^{\\top} U_{k+1} = I_{k+1} \\in \\mathbb{R}^{(k+1) \\times (k+1)}\n$$\n$$\nV_{k}^{\\top} V_{k} = I_{k} \\in \\mathbb{R}^{k \\times k}\n$$\n其中 $I$ 表示指定维度的单位矩阵。\n\n我们的目标是分析表达式 $V_{k}^{\\top} A^{\\top} A V_{k}$。我们从基本的 GKB 关系 $A V_k = U_{k+1} B_k$ 开始。\n对该方程进行转置，我们得到：\n$$\n(A V_k)^{\\top} = (U_{k+1} B_k)^{\\top}\n$$\n$$\nV_k^{\\top} A^{\\top} = B_k^{\\top} U_{k+1}^{\\top}\n$$\n现在，我们可以将 $V_k^{\\top} A^{\\top}$ 的这个表达式代入我们感兴趣的项 $V_{k}^{\\top} A^{\\top} A V_{k}$ 中：\n$$\nV_{k}^{\\top} A^{\\top} A V_{k} = (V_k^{\\top} A^{\\top})(A V_k) = (B_k^{\\top} U_{k+1}^{\\top})(U_{k+1} B_k)\n$$\n根据矩阵乘法的结合律，我们可以重新组合中间的项：\n$$\nV_{k}^{\\top} A^{\\top} A V_{k} = B_k^{\\top} (U_{k+1}^{\\top} U_{k+1}) B_k\n$$\n使用标准正交性质 $U_{k+1}^{\\top} U_{k+1} = I_{k+1}$，上式简化为：\n$$\nV_{k}^{\\top} A^{\\top} A V_{k} = B_k^{\\top} I_{k+1} B_k = B_k^{\\top} B_k\n$$\n推导到此完成。得到的矩阵 $T_k = B_k^{\\top} B_k$ 是一个 $k \\times k$ 的对称三对角矩阵：\n$$\nT_k = B_k^{\\top} B_k = \\begin{pmatrix}\n\\alpha_1^2+\\beta_2^2  \\alpha_2\\beta_2   \\\\\n\\alpha_2\\beta_2  \\alpha_2^2+\\beta_3^2  \\ddots  \\\\\n  \\ddots  \\ddots  \\alpha_k\\beta_k \\\\\n   \\alpha_k\\beta_k  \\alpha_k^2+\\beta_{k+1}^2\n\\end{pmatrix}\n$$\n这表明正规矩阵 $A^{\\top}A$ 在 GKB 隐式生成的 Krylov 子空间上的投影可以直接从双对角矩阵 $B_k$ 中获得。\n\n### 第 2 部分：迭代解等价性的证明 ($x_{k}^{\\mathrm{LSQR}} = x_{k}^{\\mathrm{CGLS}}$)\n\n我们需要证明，在给定零初始猜测 $x_0=0$ 的情况下，对于任意步 $k$，LSQR 和 CGLS 产生的迭代解是相同的。\n\n**CGLS 方法：**\nCGLS 算法在数学上等价于应用标准共轭梯度（CG）方法求解正规方程 $A^{\\top} A x = A^{\\top} b$。对于初始猜测 $x_0 = 0$，该系统的初始残差为 $r_0 = A^{\\top} b - A^{\\top} A x_0 = A^{\\top} b$。\nCG 方法在由矩阵 $A^{\\top}A$ 和初始残差 $r_0$ 生成的 Krylov 子空间内构建其第 $k$ 个迭代解 $x_k^{\\mathrm{CGLS}}$。因此，\n$$\nx_k^{\\mathrm{CGLS}} \\in \\mathcal{K}_k(A^{\\top}A, A^{\\top} b) = \\mathrm{span}\\{A^{\\top}b, (A^{\\top}A)A^{\\top}b, \\dots, (A^{\\top}A)^{k-1}A^{\\top}b\\}\n$$\n迭代解 $x_k^{\\mathrm{CGLS}}$ 是此子空间中满足 Galerkin 条件的唯一向量：新的残差 $r_k = A^{\\top}b - A^{\\top}A x_k^{\\mathrm{CGLS}}$ 必须与搜索子空间正交，即\n$$\nr_k \\perp \\mathcal{K}_k(A^{\\top}A, A^{\\top} b)\n$$\n\n**LSQR 方法：**\nLSQR 方法旨在求解最小二乘问题 $\\min_{x} \\lVert Ax-b \\rVert_2$。它依赖于 GKB 过程，该过程将 $A$ 和 $b$ 联系起来。第 $k$ 个 LSQR 迭代解 $x_k^{\\mathrm{LSQR}}$ 是在 GKB 向量 $\\{v_j\\}_{j=1}^k$ 张成的子空间内构建的。\n$$\nx_k^{\\mathrm{LSQR}} \\in \\mathrm{span}(V_k) = \\mathrm{span}\\{v_1, \\dots, v_k\\}\n$$\n让我们建立 $\\mathrm{span}(V_k)$ 和 CGLS 的 Krylov 子空间之间的关系。GKB 的递推关系如下：\n$\\beta_1 u_1 = b$\n$\\alpha_1 v_1 = A^{\\top} u_1$\n对于 $j=1, 2, \\dots$：\n$\\beta_{j+1} u_{j+1} = A v_j - \\alpha_j u_j$\n$\\alpha_{j+1} v_{j+1} = A^{\\top} u_{j+1} - \\beta_{j+1} v_j$\n\n从这些关系中，我们可以看到 $v_1 = (1/\\alpha_1) A^{\\top} u_1 = (1/(\\alpha_1 \\beta_1)) A^{\\top}b$。因此 $v_1$ 与 $A^{\\top}b$ 平行。\n通过形式归纳法可以证明 $\\mathrm{span}(V_k)$ 与 Krylov 子空间 $\\mathcal{K}_k(A^{\\top}A, A^{\\top}b)$ 相同。关键关系是 $A \\mathrm{span}(V_j) \\subseteq \\mathrm{span}(U_{j+1})$ 和 $A^{\\top} \\mathrm{span}(U_{j+1}) \\subseteq \\mathrm{span}(V_{j+1})$。这意味着 $A^{\\top}A \\mathrm{span}(V_j) \\subseteq A^{\\top}\\mathrm{span}(U_{j+1}) \\subseteq \\mathrm{span}(V_{j+1})$。由于 $v_1 \\in \\mathcal{K}_1(A^{\\top}A, A^{\\top}b)$，这个性质确保了标准正交基 $V_k$ 张成 $\\mathcal{K}_k(A^{\\top}A, A^{\\top}b)$。\n\nLSQR 迭代解 $x_k^{\\mathrm{LSQR}} = V_k y_k$ 是通过求解较小的投影最小二乘问题得到的：\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\lVert A(V_k y) - b \\rVert_2\n$$\n使用 $A V_k = U_{k+1} B_k$ 和 $b = \\lVert b \\rVert_2 u_1 = \\beta_1 u_1 = \\beta_1 U_{k+1} e_1$（其中 $e_1=[1,0,\\dots,0]^{\\top} \\in \\mathbb{R}^{k+1}$）：\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\lVert U_{k+1} B_k y - \\beta_1 U_{k+1} e_1 \\rVert_2\n$$\n由于 $U_{k+1}$ 是一个等距算子，这等价于：\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\lVert B_k y - \\beta_1 e_1 \\rVert_2\n$$\n解 $y_k$ 必须满足这个较小问题的正规方程：\n$$\nB_k^{\\top} B_k y_k = B_k^{\\top} (\\beta_1 e_1)\n$$\n\n**等价性证明：**\n我们已经证明了 $x_k^{\\mathrm{CGLS}}$ 和 $x_k^{\\mathrm{LSQR}}$ 都属于同一个子空间 $\\mathcal{K}_k(A^{\\top}A, A^{\\top}b)$。为了证明它们是相同的，我们必须证明 $x_k^{\\mathrm{LSQR}}$ 也满足 CGLS 的 Galerkin 条件。该条件是 $r_k \\perp \\mathcal{K}_k(A^{\\top}A, A^{\\top}b)$，可以写成 $V_k^{\\top} (A^{\\top}b - A^{\\top}A x_k^{\\mathrm{LSQR}}) = 0$。\n代入 $x_k^{\\mathrm{LSQR}} = V_k y_k$ 并使用 $V_k^{\\top}A^{\\top}AV_k = B_k^{\\top}B_k$，条件变为：\n$$\nV_k^{\\top} A^{\\top} b - (V_k^{\\top} A^{\\top} A V_k) y_k = V_k^{\\top} A^{\\top} b - (B_k^{\\top} B_k) y_k = 0\n$$\n根据 LSQR 的正规方程，我们知道 $(B_k^{\\top} B_k) y_k = B_k^{\\top} (\\beta_1 e_1)$。所以我们必须证明 $V_k^{\\top} A^{\\top} b = B_k^{\\top} (\\beta_1 e_1)$。\n让我们计算左边：\n$V_k^{\\top} A^{\\top} b = V_k^{\\top} A^{\\top} (\\beta_1 u_1) = \\beta_1 V_k^{\\top} (A^{\\top} u_1)$。\n根据 GKB 递推关系，$A^{\\top} u_1 = \\alpha_1 v_1$。\n所以，$V_k^{\\top} A^{\\top} b = \\beta_1 V_k^{\\top} (\\alpha_1 v_1) = \\alpha_1 \\beta_1 (V_k^{\\top} v_1)$。\n由于 $v_1$ 是标准正交矩阵 $V_k$ 的第一列，所以 $V_k^{\\top} v_1 = e_1 \\in \\mathbb{R}^k$。\n因此，$V_k^{\\top} A^{\\top} b = \\alpha_1 \\beta_1 e_1$。\n\n现在我们来计算右边：$B_k^{\\top} (\\beta_1 e_1)$。\n$e_1 \\in \\mathbb{R}^{k+1}$，且 $B_k^{\\top}$ 是 $k \\times (k+1)$ 矩阵。$B_k^{\\top}$ 的第一列是 $B_k$ 第一行的转置。$B_k$ 的第一行是 $[\\alpha_1, 0, \\dots, 0]$。\n所以，$B_k^\\top e_1$ 正是 $B_k^\\top$ 的第一列，即 $[\\alpha_1, 0, \\dots, 0]^\\top = \\alpha_1 e_1 \\in \\mathbb{R}^k$。\n因此，$B_k^{\\top} (\\beta_1 e_1) = \\beta_1 (B_k^{\\top} e_1) = \\beta_1 (\\alpha_1 e_1) = \\alpha_1 \\beta_1 e_1$。\n\n由于两边都等于 $\\alpha_1 \\beta_1 e_1$，我们已经证明 $x_k^{\\mathrm{LSQR}}$ 满足 CGLS 的 Galerkin 条件。由于它位于同一个唯一定义的子空间中，只要两者都从 $x_0=0$ 开始，就必然有 $x_k^{\\mathrm{LSQR}} = x_k^{\\mathrm{CGLS}}$ 对所有 $k$ 成立。",
            "answer": "```python\nimport numpy as np\n\ndef cgls(A, b, K, tol=1e-15):\n    \"\"\"\n    Implements the Conjugate Gradient Least Squares (CGLS) algorithm.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    # Handle the b=0 case\n    if np.linalg.norm(b) == 0:\n        return [np.zeros(n) for _ in range(K)]\n\n    r = A.T @ b - A.T @ (A @ x)\n    p = r.copy()\n    \n    x_hist = []\n    rs_old = r.T @ r\n    \n    if np.sqrt(rs_old)  tol:\n        for _ in range(K):\n            x_hist.append(x.copy())\n        return x_hist\n\n    for k in range(K):\n        Ap = A @ p\n        alpha = rs_old / (Ap.T @ Ap)\n        x = x + alpha * p\n        r = r - alpha * (A.T @ Ap)\n        \n        x_hist.append(x.copy())\n        \n        rs_new = r.T @ r\n        if np.sqrt(rs_new)  tol:\n            # Algorithm converged, pad the history\n            for _ in range(k + 1, K):\n                x_hist.append(x.copy())\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x_hist\n\ndef gkb(A, b, K, tol=1e-15):\n    \"\"\"\n    Implements k-step Golub-Kahan Bidiagonalization.\n    \"\"\"\n    m, n = A.shape\n    \n    U_cols = []\n    V_cols = []\n    alphas = []\n    betas = []\n\n    u = b.copy()\n    beta = np.linalg.norm(u)\n    betas.append(beta)\n\n    if beta  tol:\n        # Breakdown, b=0\n        B = np.zeros((K + 1, K))\n        V = np.empty((n, 0))\n        return V, B\n\n    u = u / beta\n    U_cols.append(u)\n    \n    for k in range(K):\n        v = A.T @ U_cols[k]\n        if k > 0:\n            v = v - betas[k] * V_cols[k-1]\n        \n        alpha = np.linalg.norm(v)\n        alphas.append(alpha)\n\n        if alpha  tol:\n            # Breakdown\n            v = np.zeros_like(v)\n            V_cols.append(v)\n            u = np.zeros_like(u)\n            U_cols.append(u)\n            betas.append(0.0)\n            break\n        \n        v = v / alpha\n        V_cols.append(v)\n        \n        u = A @ v - alphas[k] * U_cols[k]\n        beta = np.linalg.norm(u)\n        betas.append(beta)\n\n        if beta  tol:\n            # Breakdown\n            u = np.zeros_like(u)\n            U_cols.append(u)\n            break\n            \n        u = u / beta\n        U_cols.append(u)\n\n    # Form V and B matrices\n    num_steps = len(alphas)\n    V = np.array(V_cols[:num_steps]).T\n    B = np.zeros((num_steps + 1, num_steps))\n    \n    for i in range(num_steps):\n        B[i, i] = alphas[i]\n        B[i + 1, i] = betas[i + 1]\n        \n    # If K > num_steps, the matrices are smaller\n    if K > V.shape[1]:\n        V_padded = np.zeros((n, K))\n        V_padded[:, :V.shape[1]] = V\n        V = V_padded\n        B_padded = np.zeros((K + 1, K))\n        B_padded[:B.shape[0], :B.shape[1]] = B\n        B = B_padded\n\n    return V, B\n\ndef lsqr(A, b, K, tol=1e-15):\n    \"\"\"\n    Implements LSQR by repeatedly solving the projected problem.\n    \"\"\"\n    m, n = A.shape\n    x_hist = []\n    \n    u = b.copy()\n    beta1 = np.linalg.norm(u)\n\n    if beta1  tol:\n        return [np.zeros(n) for _ in range(K)]\n\n    U_cols = []\n    V_cols = []\n    alphas = []\n    betas = [beta1]\n    \n    u = u / beta1\n    U_cols.append(u)\n\n    last_x = np.zeros(n)\n\n    for k in range(1, K + 1):\n        # Perform one step of GKB\n        # v_k\n        v_prev = V_cols[-1] if k > 1 else np.zeros(n)\n        v = A.T @ U_cols[k-1] - betas[k-1] * v_prev\n        alpha_k = np.linalg.norm(v)\n        if alpha_k  tol:\n            x_hist.append(last_x.copy())\n            continue\n\n        v = v / alpha_k\n        V_cols.append(v)\n        alphas.append(alpha_k)\n        \n        # u_{k+1}\n        u = A @ v - alpha_k * U_cols[k-1]\n        beta_kp1 = np.linalg.norm(u)\n        if beta_kp1  tol:\n            u = np.zeros(m)\n        else:\n            u = u / beta_kp1\n        \n        U_cols.append(u)\n        betas.append(beta_kp1)\n        \n        # At step k, form and solve the small problem\n        V_k = np.array(V_cols).T\n        B_k = np.zeros((k + 1, k))\n        for i in range(k):\n            B_k[i, i] = alphas[i]\n            B_k[i + 1, i] = betas[i + 1]\n\n        b_small = np.zeros(k + 1)\n        b_small[0] = beta1\n        \n        y_k, _, _, _ = np.linalg.lstsq(B_k, b_small, rcond=None)\n        \n        x_k = V_k @ y_k\n        x_hist.append(x_k)\n        last_x = x_k\n\n    # Pad if breakdown occurred\n    while len(x_hist)  K:\n        x_hist.append(last_x.copy())\n\n    return x_hist\n\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment.\n    \"\"\"\n    \n    # Tolerances\n    eps_x = 1e-10\n    eps_T = 1e-12\n\n    # Test Cases\n    test_cases = []\n    \n    # Case 1\n    m1, n1, K1 = 8, 5, 5\n    s1 = np.array([10.0, 3.0, 1.0, 0.1, 0.01])\n    A1 = np.zeros((m1, n1))\n    np.fill_diagonal(A1, s1)\n    b1 = np.sin(np.arange(1, m1 + 1))\n    test_cases.append({'A': A1, 'b': b1, 'K': K1})\n\n    # Case 2\n    m2, n2, K2 = 8, 5, 3\n    s2 = np.array([10.0, 3.0, 1.0, 0.0, 0.0])\n    A2 = np.zeros((m2, n2))\n    np.fill_diagonal(A2, s2)\n    b2 = np.cos(np.arange(1, m2 + 1))\n    test_cases.append({'A': A2, 'b': b2, 'K': K2})\n\n    # Case 3\n    m3, n3, K3 = 6, 6, 6\n    s3 = np.array([5.0, 4.0, 3.0, 2.0, 1.0, 0.5])\n    A3 = np.diag(s3)\n    b3 = np.sin(np.arange(1, m3 + 1))\n    test_cases.append({'A': A3, 'b': b3, 'K': K3})\n\n    # Case 4\n    K4 = 3\n    b4 = np.zeros(m1)\n    test_cases.append({'A': A1, 'b': b4, 'K': K4})\n\n    results = []\n    for case in test_cases:\n        A, b, K = case['A'], case['b'], case['K']\n        \n        # Run CGLS and LSQR\n        x_cgls_hist = cgls(A, b, K)\n        x_lsqr_hist = lsqr(A, b, K)\n        \n        # Check iterate equivalence\n        diff_norms = [np.linalg.norm(xc - xl) for xc, xl in zip(x_cgls_hist, x_lsqr_hist)]\n        max_diff_norm = np.max(diff_norms) if diff_norms else 0.0\n        bool1 = max_diff_norm = eps_x\n        results.append(bool1)\n        \n        # Run GKB for K steps\n        V_K, B_K = gkb(A, b, K)\n        \n        # Check matrix equivalence\n        if V_K.shape[1] == 0: # b=0 case\n            frob_norm = 0.0\n        else:\n            M1 = V_K.T @ A.T @ A @ V_K\n            M2 = B_K.T @ B_K\n            frob_norm = np.linalg.norm(M1 - M2, 'fro')\n\n        bool2 = frob_norm = eps_T\n        results.append(bool2)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}