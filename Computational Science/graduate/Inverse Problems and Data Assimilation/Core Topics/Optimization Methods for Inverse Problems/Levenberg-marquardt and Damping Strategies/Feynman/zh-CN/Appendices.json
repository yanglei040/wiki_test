{
    "hands_on_practices": [
        {
            "introduction": "理论是基础，但真正的理解源于实践。本节将通过一系列动手实践，加深您对Levenberg-Marquardt算法的理解。我们将从一个揭示阻尼策略为何至关重要的诊断性问题开始。在某些非线性最小二乘问题中，高斯-牛顿法所使用的Hessian矩阵近似 $J(x)^T J(x)$ 可能严重偏离真实的曲率信息，甚至符号都可能是错误的，从而导致优化失败。这个练习将引导您分析一个简单的一维案例，在该案例中，由于真实Hessian矩阵为负，标准的牛顿步会使目标函数值增加，而Levenberg-Marquardt步则能通过智能地引入梯度下降分量来修正这一失败。",
            "id": "3247339",
            "problem": "考虑由残差 $r(x) = x^3 - 10$ 和目标函数 $f(x) = \\tfrac{1}{2}\\,r(x)^2$ 定义的单变量非线性最小二乘问题。令 $J(x)$ 表示 $r(x)$ 的雅可比矩阵（此处为标量导数），令 $H(x)$ 表示 $f(x)$ 的精确海森矩阵（二阶导数）。取初始点 $x_0 = 0.5$。\n\n考虑使用以下两种方法来最小化 $f(x)$：\n- 用于无约束最小化的全牛顿法，该方法在计算步长的线性系统中使用精确的海森矩阵 $H(x)$。\n- Levenberg–Marquardt (LM) 方法，一种阻尼高斯-牛顿方案，对于给定的阻尼参数 $\\lambda > 0$，它在每次迭代中通过求解 $(J(x)^\\top J(x) + \\lambda I)\\,p = -J(x)^\\top r(x)$ 来计算 $p$，并调整 $\\lambda$ 以使 $f$ 的值减小。\n\n仅使用梯度、海森矩阵和雅可比矩阵的定义，以及用于下降的标准泰勒推理，分析这些方法在 $x_0$ 处的行为。选择所有正确的陈述。\n\nA. 在 $x_0 = 0.5$ 处，精确的海森矩阵 $H(x_0)$ 是负的，而 $J(x_0)^\\top J(x_0)$ 是正的，因此两者的符号不同。\n\nB. 从 $x_0 = 0.5$ 开始，用于最小化 $f$ 的一个纯全牛顿步会移动到一个更小的 $x$ 并增加 $f$ 的值，而对于足够大的 $\\lambda$，Levenberg–Marquardt 步通过移动到更大的 $x$ 来使 $f$ 的值减小。\n\nC. 在极小值点 $x^\\star = \\sqrt[3]{10}$ 处，有 $H(x^\\star) = J(x^\\star)^\\top J(x^\\star)$；因此，在 $x^\\star$ 附近，全牛顿法和 Levenberg–Marquardt 法都表现出相同的局部二次收敛性（假设在成功步之后 LM 阻尼趋于 $0$）。\n\nD. 对于任意固定的 $\\lambda > 0$，Levenberg–Marquardt 步等于负梯度步，即 $p = -\\nabla f(x)$。\n\nE. 如果从一个负的 $x_0$ 开始，全牛顿步保证是一个下降方向，因为在最小二乘问题中，真实的海森矩阵总是正的。",
            "solution": "该问题陈述是有效的。它提出了一个定义明确的数值优化数学问题，没有科学或逻辑上的不一致。\n\n我们已知残差函数 $r(x) = x^3 - 10$ 和目标函数 $f(x) = \\frac{1}{2}r(x)^2 = \\frac{1}{2}(x^3 - 10)^2$。初始点为 $x_0 = 0.5$。我们必须分析两种优化方法：全牛顿法和 Levenberg-Marquardt (LM) 方法。\n\n首先，我们计算 $r(x)$ 和 $f(x)$ 的必要导数。\n$r(x)$ 的雅可比矩阵是它的一阶导数：\n$$J(x) = \\frac{dr}{dx} = 3x^2$$\n$f(x)$ 的梯度是它的一阶导数：\n$$\\nabla f(x) = \\frac{df}{dx} = r(x) \\frac{dr}{dx} = (x^3 - 10)(3x^2) = 3x^5 - 30x^2$$\n作为验证，最小二乘问题的梯度由 $\\nabla f(x) = J(x)^\\top r(x)$ 给出。在这个一维情况下，即为 $J(x)r(x) = (3x^2)(x^3 - 10)$，结果吻合。\n\n$f(x)$ 的精确海森矩阵是它的二阶导数：\n$$H(x) = \\frac{d^2f}{dx^2} = \\frac{d}{dx}(3x^5 - 30x^2) = 15x^4 - 60x$$\n最小二乘目标函数的海森矩阵的通用公式是 $H(x) = J(x)^\\top J(x) + \\sum_i r_i(x) \\nabla^2 r_i(x)$。这里，对于单个残差 $r(x)$，该公式变为 $H(x) = (J(x))^2 + r(x) \\frac{d^2r}{dx^2}$。\n残差的二阶导数是 $\\frac{d^2r}{dx^2} = 6x$。\n所以，$H(x) = (3x^2)^2 + (x^3 - 10)(6x) = 9x^4 + 6x^4 - 60x = 15x^4 - 60x$。这证实了我们的计算。\n\n海森矩阵的高斯-牛顿近似由 $J(x)^\\top J(x)$ 给出。在一维情况下，即为：\n$$J(x)^\\top J(x) = (J(x))^2 = (3x^2)^2 = 9x^4$$\n\n现在，我们在初始点 $x_0 = 0.5$ 处计算这些量的值：\n- 雅可比矩阵: $J(x_0) = J(0.5) = 3(0.5)^2 = 3(0.25) = 0.75 = \\frac{3}{4}$\n- 梯度: $\\nabla f(x_0) = \\nabla f(0.5) = 3(0.5)^5 - 30(0.5)^2 = 3(\\frac{1}{32}) - 30(\\frac{1}{4}) = \\frac{3}{32} - \\frac{240}{32} = -\\frac{237}{32}$\n- 精确海森矩阵: $H(x_0) = H(0.5) = 15(0.5)^4 - 60(0.5) = 15(\\frac{1}{16}) - 30 = \\frac{15 - 480}{16} = -\\frac{465}{16} = -29.0625$\n- 高斯-牛顿海森矩阵近似: $J(x_0)^\\top J(x_0) = 9(0.5)^4 = 9(\\frac{1}{16}) = \\frac{9}{16} = 0.5625$\n\n有了这些值，我们可以分析每个选项。\n\n**A. 在 $x_0 = 0.5$ 处，精确的海森矩阵 $H(x_0)$ 是负的，而 $J(x_0)^\\top J(x_0)$ 是正的，因此两者的符号不同。**\n根据我们的计算，$H(x_0) = -\\frac{465}{16}$，为负值。同时，$J(x_0)^\\top J(x_0) = \\frac{9}{16}$，为正值。\n该陈述正确地指出一个是负的，另一个是正的，因此它们的符号不同。这凸显了高斯-牛顿近似效果很差的一种情况，因为它未能捕捉目标函数 $f(x)$ 在 $x_0$ 处的局部曲率。\n**结论：正确。**\n\n**B. 从 $x_0 = 0.5$ 开始，用于最小化 $f$ 的一个纯全牛顿步会移动到一个更小的 $x$ 并增加 $f$ 的值，而对于足够大的 $\\lambda$，Levenberg–Marquardt 步通过移动到更大的 $x$ 来使 $f$ 的值减小。**\n让我们分析全牛顿步 $p_N$，它通过求解 $H(x_0) p_N = -\\nabla f(x_0)$ 得到。\n$$(-\\frac{465}{16}) p_N = -(-\\frac{237}{32}) = \\frac{237}{32}$$\n$$p_N = \\frac{237}{32} \\cdot (-\\frac{16}{465}) = -\\frac{237}{2 \\cdot 465} = -\\frac{237}{930} \\approx -0.255$$\n由于 $p_N  0$，新点 $x_1 = x_0 + p_N$ 将小于 $x_0$。\n为了判断 $f$ 是否增加，我们检查方向导数 $\\nabla f(x_0)^\\top p_N$ 的符号。\n$$\\nabla f(x_0)^\\top p_N = (-\\frac{237}{32}) \\cdot (-\\frac{237}{930}) = \\frac{237^2}{32 \\cdot 930}  0$$\n正的方向导数意味着函数沿此步长方向增加。这是预料之中的，因为只有当海森矩阵是正定的时候，牛顿步才是一个下降方向，而这里的情况并非如此（$H(x_0)  0$）。\n\n现在，我们来分析 Levenberg-Marquardt 步 $p_{LM}$，它通过求解 $(J(x_0)^\\top J(x_0) + \\lambda I) p_{LM} = -J(x_0)^\\top r(x_0) = -\\nabla f(x_0)$ (对于 $\\lambda  0$) 得到。\n$$(\\frac{9}{16} + \\lambda) p_{LM} = \\frac{237}{32}$$\n由于 $\\lambda  0$，项 $(\\frac{9}{16} + \\lambda)$ 是正的。等式右边也是正的。因此，$p_{LM}  0$。一个正的步长意味着该方法移动到一个更大的 $x$。\n方向导数为 $\\nabla f(x_0)^\\top p_{LM}$：\n$$\\nabla f(x_0)^\\top p_{LM} = (-\\frac{237}{32}) \\cdot \\left( \\frac{237/32}{9/16 + \\lambda} \\right) = - \\frac{(237/32)^2}{9/16 + \\lambda}  0$$\n由于方向导数为负，对于任何 $\\lambda  0$，LM 步都是一个下降方向。LM 算法通过调整 $\\lambda$ 来确保步长导致函数值减小。当 $\\lambda \\to \\infty$ 时，步长方向接近负梯度方向，$p_{LM} \\approx \\frac{1}{\\lambda}(-\\nabla f(x_0))$，这是最速下降方向，并保证对于足够小的步长，函数值会局部减小。方向 $-\\nabla f(x_0)$ 是正的，这与 $p_{LM}  0$ 一致。\n该陈述的两个部分都是正确的。\n**结论：正确。**\n\n**C. 在极小值点 $x^\\star = \\sqrt[3]{10}$ 处，有 $H(x^\\star) = J(x^\\star)^\\top J(x^\\star)$；因此，在 $x^\\star$ 附近，全牛顿法和 Levenberg–Marquardt 法都表现出相同的局部二次收敛性（假设在成功步之后 LM 阻尼趋于 $0$）。**\n当残差 $r(x) = x^3 - 10$ 为零时，目标函数 $f(x) = \\frac{1}{2}(x^3 - 10)^2$ 被最小化。这发生在 $x^\\star = \\sqrt[3]{10}$ 处。这是一个“零残差”问题。\n精确海森矩阵是 $H(x) = J(x)^\\top J(x) + r(x)r''(x)$。在极小值点 $x^\\star$ 处，由于 $r(x^\\star) = 0$，第二项消失：\n$$H(x^\\star) = J(x^\\star)^\\top J(x^\\star) + 0 \\cdot r''(x^\\star) = J(x^\\star)^\\top J(x^\\star)$$\n所以陈述的第一部分是正确的。\n如果解处的海森矩阵 $H(x^\\star)$ 是正定的，全牛顿法会表现出局部二次收敛性。这里，$H(x^\\star) = J(x^\\star)^\\top J(x^\\star) = 9(x^\\star)^4 = 9(10^{4/3})  0$，所以条件满足。\nLevenberg-Marquardt 方法，当阻尼参数 $\\lambda$ 趋于 $0$ 时（这在成功地接近解的步中发生），就变成了高斯-牛顿法。步长通过 $(J(x)^\\top J(x)) p = -J(x)^\\top r(x)$ 计算。\n对于零残差问题，高斯-牛顿法本身就表现出二次收敛性。这是因为在解附近，高斯-牛顿海森矩阵近似 $J(x)^\\top J(x)$ 是对真实海森矩阵 $H(x)$ 的一个非常好的近似，因为 $H(x) - J(x)^\\top J(x) = r(x)r''(x)$ 且 $r(x) \\to 0$。\n由于当 $\\lambda \\to 0$ 时，LM 方法变为高斯-牛顿法，并且对于这个零残差问题，高斯-牛顿法的迭代与全牛顿法的迭代是渐近相同的，因此两种方法将共享相同的局部二次收敛率。\n**结论：正确。**\n\n**D. 对于任意固定的 $\\lambda  0$，Levenberg–Marquardt 步等于负梯度步，即 $p = -\\nabla f(x)$。**\nLM 步 $p$ 由 $(J(x)^\\top J(x) + \\lambda I) p = -\\nabla f(x)$ 定义。负梯度步就是 $-\\nabla f(x)$。如果该陈述为真，将 $p = -\\nabla f(x)$ 代入 LM 方程必须得出一个恒等式。\n$$(J(x)^\\top J(x) + \\lambda I)(-\\nabla f(x)) = -\\nabla f(x)$$\n假设 $\\nabla f(x) \\neq 0$，我们可以除以 $-\\nabla f(x)$ 得到：\n$$J(x)^\\top J(x) + \\lambda I = I$$\n在这个一维情况下，这意味着 $9x^4 + \\lambda = 1$。这个方程对于任意的 $x$ 和任意固定的 $\\lambda  0$ 并不成立。例如，在 $x_0=0.5$ 处，它将要求 $9/16 + \\lambda = 1$，即 $\\lambda = 7/16$。这对于“任意固定的 $\\lambda  0$”是不成立的。\n在 $\\lambda \\to \\infty$ 的极限情况下，LM 步变为 $p_{LM} \\approx \\frac{1}{\\lambda}(-\\nabla f(x))$，这是一个*缩放后*的负梯度步，而不是与之相等。\n**结论：不正确。**\n\n**E. 如果从一个负的 $x_0$ 开始，全牛顿步保证是一个下降方向，因为在最小二乘问题中，真实的海森矩阵总是正的。**\n这个陈述基于一个给定的原因得出一个结论。我们必须对两者都进行评估。\n给出的原因是“在最小二乘问题中，真实的海森矩阵总是正的”。这是一个错误的普遍性陈述。海森矩阵是 $H(x) = J(x)^\\top J(x) + \\sum_i r_i(x) \\nabla^2 r_i(x)$。虽然 $J(x)^\\top J(x)$ 是半正定的，但涉及残差的第二项可以是负的，使得整个海森矩阵成为不定的或负定的。我们自己的问题就提供了一个反例：在 $x_0=0.5$ 处，我们发现 $H(0.5)  0$。\n因为所提供的理由从根本上是错误的，所以整个逻辑陈述是无效的。\n尽管结论（“如果从一个负的 $x_0$ 开始，全牛顿步保证是一个下降方向”）对于这个特定问题可能是正确的（对于 $x  0$，$H(x) = 15x^4 - 60x = 15x(x^3-4)$，其中 $x$ 和 $(x^3-4)$ 都是负的，使得 $H(x)>0$，这保证了下降方向），但不能从一个错误的前提中得出正确的结论。该陈述的推理有缺陷，使其从科学的角度来看是不可接受的。\n**结论：不正确。**",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "在诊断了阻尼的必要性之后，下一步是构建一个完整的算法。Levenberg-Marquardt算法的精髓不仅在于添加阻尼项，更在于根据局部二次模型的预测效果动态地调整阻尼参数 $\\lambda$。这个适应过程的核心是“增益比” $\\rho$，它量化了实际函数下降量与模型预测下降量之间的一致性。这个编程练习将指导您实现完整的Levenberg-Marquardt算法，包括迭代循环、求解阻尼化的法方程、计算增益比以及根据 $\\rho$ 值更新阻尼参数的逻辑。",
            "id": "3396965",
            "problem": "考虑一个非线性最小二乘反问题，目标是寻找参数 $\\theta \\in \\mathbb{R}^p$ 以最小化目标函数 $\\Phi(\\theta)$，该函数定义为 $\\Phi(\\theta) = \\dfrac{1}{2} \\sum_{i=1}^{n} r_i(\\theta)^2$，其中残差为 $r_i(\\theta) = y_i - f(x_i, \\theta)$。从线性化 $r(\\theta + s) \\approx r(\\theta) + J(\\theta) s$ 出发，其中 $J(\\theta)$ 是 $r(\\theta)$ 的雅可比矩阵，并借助当前迭代点 $\\theta$ 附近目标函数的二次模型，推导通过最小化正则化模型 $m(s)$ 得到的阻尼高斯-牛顿步。该模型定义为 $m(s) = \\dfrac{1}{2} \\| r(\\theta) + J(\\theta) s \\|_2^2 + \\dfrac{1}{2} \\lambda \\| s \\|_2^2$，其中 $\\lambda  0$ 是一个标量阻尼参数。然后，将增益比 $\\rho$ 定义为实际下降量与在二次模型下计算的预测下降量之商。提供一个基于 $\\rho$ 的、有原则的 $s$ 接受准则，以及一个跨迭代调整 $\\lambda$ 的乘法策略，该策略在二次模型可靠时减小 $\\lambda$，在模型不可靠时增大 $\\lambda$。\n\n你的推导必须从线性化 $r(\\theta + s) \\approx r(\\theta) + J(\\theta) s$ 和 $\\Phi(\\theta)$ 的定义开始，并使用最小二乘问题数值优化中经过检验的理论进行。在未说明为何最小化 $m(s)$ 会导出最终步长公式的情况下，你不得假定任何特定的公式。\n\n完成推导后，实现一个程序，应用由增益比控制的自适应阻尼策略的 Levenberg-Marquardt 方法，在以下数据同化设置中拟合参数。正向模型为 $f(x, \\theta)$，其中 $f(x, \\theta) = \\theta_1 \\exp(\\theta_2 x)$，因此参数向量为 $\\theta = [\\theta_1, \\theta_2]^\\top$。对于每次迭代，通过求解阻尼正规方程计算步长 $s$，并根据实际下降量和预测下降量计算增益比 $\\rho$。使用以下接受准则：如果 $\\rho  0$，则接受 $s$；否则，拒绝 $s$。使用以下自适应阻尼规则：如果 $\\rho  0.75$，则设置 $\\lambda \\leftarrow \\lambda / 2$；如果 $\\rho  0.25$，则设置 $\\lambda \\leftarrow 2 \\lambda$；否则，保持 $\\lambda$ 不变。如果一个步长被拒绝，则设置 $\\lambda \\leftarrow 2 \\lambda$ 并在下一次迭代中重新计算。当 $\\| \\nabla \\Phi(\\theta) \\|_\\infty \\leq 10^{-8}$、$\\| s \\|_2 \\leq 10^{-10}$ 或迭代次数达到 $50$ 时终止。\n\n该程序必须评估三个测试用例以测试该方法的覆盖范围：\n\n- 测试用例 1（理想路径）。从中度噪声数据中恢复参数。设 $n = 7$，$x = [0, 0.5, 1, 1.5, 2, 2.5, 3]$，真实参数为 $\\theta^\\star = [2, -0.5]^\\top$，观测值为 $y_i = \\theta_1^\\star \\exp(\\theta_2^\\star x_i) + \\epsilon_i$，固定扰动为 $\\epsilon = [0.05, -0.02, 0.04, -0.03, 0.01, -0.02, 0]$。使用初始猜测值 $\\theta_0 = [1, 0]^\\top$ 和初始阻尼 $\\lambda_0 = 10^{-2}$。\n\n- 测试用例 2（近乎完美拟合的边界条件）。模型相同，但数据无噪声，真实参数不同。设 $n = 7$，$x = [0, 0.5, 1, 1.5, 2, 2.5, 3]$，真实参数为 $\\theta^\\star = [1.5, 0.2]^\\top$，观测值为 $y_i = \\theta_1^\\star \\exp(\\theta_2^\\star x_i)$。使用初始猜测值 $\\theta_0 = [1, 0]^\\top$ 和初始阻尼 $\\lambda_0 = 10^{-2}$。\n\n- 测试用例 3（具有病态雅可比矩阵的重要边缘情况）。所有输入均为零，因此雅可比矩阵的第二列消失。设 $n = 6$，$x_i = 0$ 对所有 $i$ 成立，真实参数为 $\\theta^\\star = [3, -1]^\\top$，观测值为 $y_i = \\theta_1^\\star$。使用初始猜测值 $\\theta_0 = [1, 1]^\\top$ 和初始阻尼 $\\lambda_0 = 10^{-2}$。\n\n对于每个测试用例，按规定运行算法，并返回一个列表，其中包含最终目标函数值 $\\Phi(\\theta_{\\text{final}})$、最终阻尼参数 $\\lambda_{\\text{final}}$ 和执行的迭代次数。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素本身都是 $[\\Phi(\\theta_{\\text{final}}), \\lambda_{\\text{final}}, \\text{iterations}]$ 形式的列表。",
            "solution": "用户提供了一个在数值优化和反问题领域中良态的问题陈述。该问题具有科学依据、自洽且可形式化。它被验证为一个有效的问题。\n\n### 第 1 部分：Levenberg-Marquardt 步长和自适应策略的推导\n\n目标是找到参数向量 $\\theta \\in \\mathbb{R}^p$，以最小化非线性最小二乘目标函数：\n$$\n\\Phi(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} r_i(\\theta)^2 = \\frac{1}{2} \\|r(\\theta)\\|_2^2\n$$\n其中 $r(\\theta)$ 是残差向量。Levenberg-Marquardt (LM) 方法是一种迭代算法，在每次迭代 $k$ 时，找到一个步长 $s$ 来将当前参数估计 $\\theta_k$ 更新为 $\\theta_{k+1} = \\theta_k + s$。\n\n推导从残差向量 $r(\\theta)$ 在当前迭代点 $\\theta$ 周围的一阶泰勒展开开始：\n$$\nr(\\theta + s) \\approx r(\\theta) + J(\\theta) s\n$$\n这里，$J(\\theta)$ 是 $r(\\theta)$ 的雅可比矩阵，其元素为 $J_{ij}(\\theta) = \\frac{\\partial r_i}{\\partial \\theta_j}$。\n\n将此线性化代入目标函数 $\\Phi(\\theta+s)$，得到目标函数的二次模型：\n$$\n\\Phi(\\theta + s) \\approx \\frac{1}{2} \\|r(\\theta) + J(\\theta) s\\|_2^2\n$$\n直接最小化这个关于 $s$ 的二次模型会得到高斯-牛顿步。然而，如果雅可比矩阵 $J(\\theta)$ 是病态的（即 $J(\\theta)^\\top J(\\theta)$ 是奇异或近奇异的），高斯-牛顿法可能会不稳定。\n\nLevenberg-Marquardt 方法通过引入一个 Tikhonov 型正则化项来解决这个问题，这在当前迭代点周围创建了一个信赖域。步长 $s$ 是通过最小化一个正则化的二次模型 $m(s)$ 来找到的：\n$$\nm(s) = \\frac{1}{2} \\|r(\\theta) + J(\\theta) s\\|_2^2 + \\frac{1}{2} \\lambda \\|s\\|_2^2\n$$\n其中 $\\lambda  0$ 是一个标量阻尼参数。为简化符号，我们令 $r = r(\\theta)$ 和 $J = J(\\theta)$。\n\n为了找到最小化 $m(s)$ 的步长 $s$，我们计算 $m(s)$ 关于 $s$ 的梯度并令其为零。首先，展开 $m(s)$ 的表达式：\n$$\nm(s) = \\frac{1}{2} (r + Js)^\\top (r + Js) + \\frac{1}{2} \\lambda s^\\top s\n$$\n$$\nm(s) = \\frac{1}{2} (r^\\top r + r^\\top J s + s^\\top J^\\top r + s^\\top J^\\top J s) + \\frac{1}{2} \\lambda s^\\top s\n$$\n由于 $r^\\top J s$ 是一个标量，它等于其转置 $s^\\top J^\\top r$。因此：\n$$\nm(s) = \\frac{1}{2} r^\\top r + r^\\top J s + \\frac{1}{2} s^\\top J^\\top J s + \\frac{1}{2} \\lambda s^\\top I s\n$$\n$$\nm(s) = \\frac{1}{2} \\|r\\|_2^2 + (J^\\top r)^\\top s + \\frac{1}{2} s^\\top (J^\\top J + \\lambda I) s\n$$\n$m(s)$ 关于 $s$ 的梯度是：\n$$\n\\nabla_s m(s) = J^\\top r + (J^\\top J + \\lambda I) s\n$$\n将梯度设为零，$\\nabla_s m(s) = 0$，得到驻点：\n$$\n(J^\\top J + \\lambda I) s = -J^\\top r\n$$\n这就是**阻尼正规方程**。这个线性系统的解 $s$ 就是 Levenberg-Marquardt 步长。项 $J^\\top J$ 是 $\\Phi(\\theta)$ 的 Hessian 矩阵的高斯-牛顿近似，而 $J^\\top r$ 是 $\\Phi(\\theta)$ 的梯度。对于 $\\lambda  0$，矩阵 $(J^\\top J + \\lambda I)$ 保证是对称正定的，从而确保存在唯一的解 $s$，并且它是一个下降方向。\n\n步长 $s$ 的有效性通过比较目标函数的实际下降量与模型预测的下降量来评估。这个比较由**增益比** $\\rho$ 来量化。\n\n**实际下降量**是目标函数的真实变化：\n$$\n\\Delta\\Phi_{\\text{actual}} = \\Phi(\\theta) - \\Phi(\\theta+s) = \\frac{1}{2} \\|r(\\theta)\\|_2^2 - \\frac{1}{2} \\|r(\\theta+s)\\|_2^2\n$$\n**预测下降量**是二次模型 $m(s)$ 从 $s=0$ 到计算出的步长 $s$ 的变化：\n$$\n\\Delta\\Phi_{\\text{pred}} = m(0) - m(s)\n$$\n我们的模型是 $m(s) = \\Phi(\\theta) + g^\\top s + \\frac{1}{2} s^\\top (J^\\top J + \\lambda I)s$，其中 $g=J^\\top r$ 是梯度。\n因此，$\\Delta\\Phi_{\\text{pred}} = -g^\\top s - \\frac{1}{2} s^\\top (J^\\top J + \\lambda I) s$。\n利用阻尼正规方程 $(J^\\top J + \\lambda I)s = -g$，我们可以替换 $g$：\n$\\Delta\\Phi_{\\text{pred}} = s^\\top (J^\\top J + \\lambda I) s - \\frac{1}{2} s^\\top (J^\\top J + \\lambda I) s = \\frac{1}{2} s^\\top (J^\\top J + \\lambda I) s$。\n或者，我们可以替换最后一项：\n$\\Delta\\Phi_{\\text{pred}} = -g^\\top s + \\frac{1}{2} s^\\top g = -\\frac{1}{2} g^\\top s$。\n对于 $\\lambda > 0$ 且 $s \\neq 0$，该表达式保证为正。\n\n**增益比** $\\rho$ 定义为：\n$$\n\\rho = \\frac{\\Delta\\Phi_{\\text{actual}}}{\\Delta\\Phi_{\\text{pred}}} = \\frac{\\Phi(\\theta) - \\Phi(\\theta+s)}{-\\frac{1}{2} s^\\top (J(\\theta)^\\top r(\\theta))}\n$$\n$\\rho$ 的值表明了二次模型近似的质量：\n- $\\rho \\approx 1$：模型是一个极好的近似。\n- $\\rho  0$：步长导致了目标函数的减少。\n- $\\rho \\le 0$：步长导致了目标函数的增加（或没有变化）。\n\n基于增益比 $\\rho$，可以制定一个有原则的**接受准则**和**阻尼策略**：\n1.  **接受准则**：如果步长 $s$ 导致目标函数减小，即 $\\rho  0$，则接受该步长。如果步长被接受，参数被更新：$\\theta_{k+1} = \\theta_k + s$。如果步长被拒绝（$\\rho \\le 0$），则不更新参数：$\\theta_{k+1} = \\theta_k$。\n2.  **自适应阻尼策略**：阻尼参数 $\\lambda$ 根据模型的可靠性进行调整，而模型的可靠性由 $\\rho$ 指示。\n    - 如果 $\\rho$ 很大（例如 $\\rho  0.75$），模型非常可靠。我们可以更积极，通过减小阻尼向收敛更快的 Gauss-Newton 方法靠近：$\\lambda \\leftarrow \\lambda / 2$。\n    - 如果 $\\rho$ 很小或为负（例如 $\\rho  0.25$），模型不可靠。线性化效果很差，因此我们必须更加保守，通过增加阻尼来采取更小的、更接近最速下降方向的步长：$\\lambda \\leftarrow 2 \\lambda$。这也包括了步长被拒绝的情况。\n    - 如果 $\\rho$ 在一个中间范围（例如 $0.25 \\le \\rho \\le 0.75$），模型是足够的，阻尼参数可以保持不变。\n\n步长计算、接受准则和自适应阻尼的这种组合构成了 Levenberg-Marquardt 算法的核心。\n\n### 第 2 部分：实现\n以下代码实现了 Levenberg-Marquardt 算法及上述自适应策略，以解决指定的三个测试用例。",
            "answer": "```python\nimport numpy as np\n\ndef run_lm(case_params):\n    \"\"\"\n    Executes the Levenberg-Marquardt algorithm for a given test case.\n\n    Args:\n        case_params (tuple): A tuple containing the problem setup:\n            (x data, y data, initial theta, initial lambda, max iterations,\n             gradient tolerance, step tolerance).\n\n    Returns:\n        list: A list containing the final objective function value,\n              the final damping parameter, and the number of iterations performed.\n    \"\"\"\n    x, y, theta0, lambda0, max_iter, grad_tol, step_tol = case_params\n\n    theta = np.copy(theta0).astype(float)\n    lam = float(lambda0)\n\n    def forward_model(x_vals, p):\n        return p[0] * np.exp(p[1] * x_vals)\n\n    def jacobian(x_vals, p):\n        # Jacobian of the residual vector r = y - f(x, theta)\n        # J_ij = dr_i / d_theta_j = - df_i / d_theta_j\n        J = np.zeros((len(x_vals), 2))\n        exp_term = np.exp(p[1] * x_vals)\n        J[:, 0] = -exp_term\n        J[:, 1] = -p[0] * x_vals * exp_term\n        return J\n\n    for k in range(max_iter):\n        # 1. Compute residuals, objective function, and gradient\n        r = y - forward_model(x, theta)\n        phi = 0.5 * np.dot(r, r)\n        J = jacobian(x, theta)\n        g = np.dot(J.T, r)  # Gradient of the objective function, g = J^T * r\n\n        # 2. Check for convergence based on gradient norm\n        if np.linalg.norm(g, np.inf) = grad_tol:\n            return [phi, lam, k]\n\n        # 3. Form and solve the damped normal equations (A + lam*I)s = -g\n        A = np.dot(J.T, J)\n        I = np.identity(A.shape[0])\n        b = -g\n\n        try:\n            s = np.linalg.solve(A + lam * I, b)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular even with damping, increase damping significantly\n            lam *= 10\n            continue \n\n        # 4. Check for convergence based on step size\n        if np.linalg.norm(s) = step_tol:\n            return [phi, lam, k]\n\n        # 5. Calculate predicted reduction and gain ratio\n        # Predicted reduction: delta_pred = -0.5 * s^T * g\n        pred_reduction = -0.5 * np.dot(s, g)\n\n        theta_new = theta + s\n        r_new = y - forward_model(x, theta_new)\n        phi_new = 0.5 * np.dot(r_new, r_new)\n        actual_reduction = phi - phi_new\n\n        # Handle potential division by zero if pred_reduction is numerically zero\n        if abs(pred_reduction) = np.finfo(float).eps:\n            # If predicted reduction is negligible, implies g or s is near zero,\n            # which should be caught by termination criteria.\n            # A negative rho indicates a failed step.\n            rho = -1.0\n        else:\n            rho = actual_reduction / pred_reduction\n\n        # 6. Update parameters (theta) based on acceptance rule\n        if rho > 0:\n            theta = theta_new\n\n        # 7. Update damping parameter (lambda) based on adaptive strategy\n        if rho > 0.75:\n            lam = lam / 2.0\n        elif rho  0.25: # This also covers the rejection case where rho = 0\n            lam = lam * 2.0\n            \n    # 8. Reached max iterations, return current state\n    final_r = y - forward_model(x, theta)\n    final_phi = 0.5 * np.dot(final_r, final_r)\n    return [final_phi, lam, max_iter]\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases and runs the LM algorithm for each, printing the results.\n    \"\"\"\n    # Common parameters for all test cases\n    max_iter = 50\n    grad_tol = 1e-8\n    step_tol = 1e-10\n\n    # Test Case 1: Ideal path with noisy data\n    x1 = np.array([0, 0.5, 1, 1.5, 2, 2.5, 3])\n    theta_star1 = np.array([2.0, -0.5])\n    epsilon1 = np.array([0.05, -0.02, 0.04, -0.03, 0.01, -0.02, 0.0])\n    y1 = theta_star1[0] * np.exp(theta_star1[1] * x1) + epsilon1\n    theta0_1 = np.array([1.0, 0.0])\n    lambda0_1 = 1e-2\n\n    # Test Case 2: Near-perfect fit with noiseless data\n    x2 = np.array([0, 0.5, 1, 1.5, 2, 2.5, 3])\n    theta_star2 = np.array([1.5, 0.2])\n    y2 = theta_star2[0] * np.exp(theta_star2[1] * x2)\n    theta0_2 = np.array([1.0, 0.0])\n    lambda0_2 = 1e-2\n\n    # Test Case 3: Ill-conditioned Jacobian\n    n3 = 6\n    x3 = np.zeros(n3)\n    theta_star3 = np.array([3.0, -1.0])\n    y3 = np.full(n3, theta_star3[0])\n    theta0_3 = np.array([1.0, 1.0])\n    lambda0_3 = 1e-2\n    \n    test_cases = [\n        (x1, y1, theta0_1, lambda0_1, max_iter, grad_tol, step_tol),\n        (x2, y2, theta0_2, lambda0_2, max_iter, grad_tol, step_tol),\n        (x3, y3, theta0_3, lambda0_3, max_iter, grad_tol, step_tol)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_lm(case)\n        # Format numbers to avoid excessive precision in output string\n        result_formatted = [\n            float(f\"{result[0]:.6g}\"),\n            float(f\"{result[1]:.6g}\"),\n            int(result[2])\n        ]\n        results.append(result_formatted)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "为了更深入地理解Levenberg-Marquardt算法，我们可以从现代优化的信赖域（Trust-Region）视角来审视它。在这种观点下，阻尼参数 $\\lambda$ 并不仅仅是一个“平滑”项，而是用于确保优化步长 $\\|s\\|$ 被约束在一个“信赖半径” $\\Delta$ 内的工具。这个高级编程练习将引导您探索阻尼参数 $\\lambda$ 和信赖半径 $\\Delta$ 之间的深刻联系，您将通过求解一个单调的标量方程 $\\|s(\\lambda)\\| = \\Delta$ 来精确确定与给定信赖半径相对应的 $\\lambda$ 值。这个过程利用了雅可比矩阵的奇异值分解（SVD），从而提供了一种数值上极其稳健的求解策略。",
            "id": "3396992",
            "problem": "考虑一个非线性最小二乘反问题，其目标是在 Gauss-Newton 方法所使用的标准线性化下，最小化 $ \\frac{1}{2} \\| \\mathbf{F}(\\mathbf{x}) - \\mathbf{y} \\|_2^2 $。设 $ \\mathbf{J} \\in \\mathbb{R}^{m \\times n} $ 为 $ \\mathbf{F} $ 在当前迭代点的雅可比矩阵，设 $ \\mathbf{r} = \\mathbf{F}(\\mathbf{x}) - \\mathbf{y} \\in \\mathbb{R}^m $ 为残差。Levenberg-Marquardt (LM) 步 $ \\mathbf{s}(\\lambda) \\in \\mathbb{R}^n $ 满足阻尼正规方程\n$$\n(\\mathbf{J}^\\top \\mathbf{J} + \\lambda \\mathbf{I}) \\mathbf{s}(\\lambda) = - \\mathbf{J}^\\top \\mathbf{r},\n$$\n其中 $ \\lambda \\ge 0 $ 是阻尼参数。在信赖域 (TR) 视角下，当 $ \\| \\mathbf{s}(0) \\|_2 > \\Delta $ 时，隐式地选择阻尼参数 $ \\lambda $，使得 LM 步服从信赖域约束 $ \\| \\mathbf{s}(\\lambda) \\|_2 = \\Delta $（对于一个给定的半径 $ \\Delta > 0 $），否则 $ \\lambda = 0 $。\n\n从雅可比矩阵的奇异值分解 (SVD) 出发，$ \\mathbf{J} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top $，其中 $ \\mathbf{U} \\in \\mathbb{R}^{m \\times m} $ 是正交矩阵，$ \\mathbf{V} \\in \\mathbb{R}^{n \\times n} $ 是正交矩阵，而 $ \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{m \\times n} $ 在其对角线上包含非负奇异值 $ \\sigma_i $（对于 $ i = 1, \\dots, k $，其中 $ k = \\min(m,n) $），推导出其唯一根可得到满足 $ \\| \\mathbf{s}(\\lambda) \\|_2 = \\Delta $ 的 $ \\lambda $ 的单调标量方程，并设计一个仅使用 SVD 分量和残差 $ \\mathbf{r} $ 的鲁棒求根策略。\n\n您的程序必须实现一个结合了 Newton 方法和二分法的保险策略，仅使用 SVD 分量来求解该单调标量方程，并且必须以数值鲁棒的方式处理以下情况：\n- 秩亏的 $ \\mathbf{J} $ (某些 $ \\sigma_i = 0 $)，\n- 边界情况 $ \\Delta = \\| \\mathbf{s}(0) \\|_2 $，\n- 情况 $ \\Delta > \\| \\mathbf{s}(0) \\|_2 $，\n- $ \\mathbf{r} $ 完全位于 $ \\mathbf{J} $ 的左零空间中的情况。\n\n您的实现不得依赖外部输入。请使用以下测试套件，每个套件由一个矩阵 $ \\mathbf{J} $、一个残差向量 $ \\mathbf{r} $ 和一个信赖域半径 $ \\Delta $ 指定。所有矩阵和向量都是显式给出的，本问题不涉及任何所需的角度单位。没有物理单位；所有量都是无量纲的。\n\n测试套件：\n1. 正常情况（需要中等阻尼）：\n   $$\n   \\mathbf{J}_1 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\\\ 0  2 \\end{bmatrix}, \\quad \\mathbf{r}_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix}, \\quad \\Delta_1 = 0.25.\n   $$\n2. 边界情况（无阻尼，恰好在信赖域上）：与上述相同的 $ \\mathbf{J}_1 $ 和 $ \\mathbf{r}_1 $，其中 $ \\Delta_2 = \\| \\mathbf{s}(0) \\|_2 $（对于这些 $ \\mathbf{J}_1 $ 和 $ \\mathbf{r}_1 $）。\n3. 大信赖域（无阻尼）：与上述相同的 $ \\mathbf{J}_1 $ 和 $ \\mathbf{r}_1 $，其中 $ \\Delta_3 = 1 $。\n4. 秩亏雅可比矩阵（需要阻尼）：\n   $$\n   \\mathbf{J}_4 = \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix}, \\quad \\mathbf{r}_4 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\Delta_4 = 0.5.\n   $$\n5. 残差在左零空间（零步长）：\n   $$\n   \\mathbf{J}_5 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\\\ 0  2 \\end{bmatrix}, \\quad \\mathbf{r}_5 = \\begin{bmatrix} 0 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\quad \\Delta_5 = 0.1.\n   $$\n\n您的程序应计算上述每个测试用例的阻尼参数 $ \\lambda $。最终输出必须是单行，包含五个用逗号分隔的 $ \\lambda $ 值，并用方括号括起来，例如 $[\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4,\\lambda_5]$。每个 $ \\lambda_i $ 必须打印为浮点数。\n\n核心推导必须从阻尼正规方程的定义和奇异值分解 (SVD) 的性质开始。问题陈述中不应提供任何捷径或目标公式。该程序必须是一个用现代编程语言编写的完整、可运行的实现，并且不需要任何用户交互。",
            "solution": "用户希望推导并实现一个鲁棒的方法来求解 Levenberg-Marquardt (LM) 信赖域子问题。核心任务是找到满足信赖域约束 $ \\| \\mathbf{s}(\\lambda) \\|_2 = \\Delta $ 的阻尼参数 $ \\lambda $。\n\n问题陈述已经过验证，并被视为**有效**。这是一个适定的、有科学依据的数值优化问题，并为获得完整解提供了足够的信息。\n\n### 步骤 1：长期方程的推导\n\nLevenberg-Marquardt 步 $ \\mathbf{s}(\\lambda) $ 是阻尼正规方程的解：\n$$ (\\mathbf{J}^\\top \\mathbf{J} + \\lambda \\mathbf{I}) \\mathbf{s}(\\lambda) = - \\mathbf{J}^\\top \\mathbf{r} $$\n其中 $ \\mathbf{J} \\in \\mathbb{R}^{m \\times n} $ 是雅可比矩阵，$ \\mathbf{r} \\in \\mathbb{R}^m $ 是残差，$ \\lambda \\ge 0 $ 是阻尼参数。\n\n我们首先将雅可比矩阵的奇异值分解 (SVD)，$ \\mathbf{J} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top $，代入正规方程。此处，$ \\mathbf{U} \\in \\mathbb{R}^{m \\times m} $ 和 $ \\mathbf{V} \\in \\mathbb{R}^{n \\times n} $ 是正交矩阵，而 $ \\boldsymbol{\\Sigma} \\in \\mathbb{R}^{m \\times n} $ 是一个矩形对角矩阵，其对角线上有非负奇异值 $ \\sigma_i $，其中 $ i=1, \\dots, k $ 且 $ k = \\min(m,n) $。\n\n项 $ \\mathbf{J}^\\top \\mathbf{J} $ 变为：\n$$ \\mathbf{J}^\\top \\mathbf{J} = (\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top)^\\top (\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top) = \\mathbf{V} \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top $$\n由于 $ \\mathbf{U} $ 是正交的，$ \\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I} $，表达式可简化为：\n$$ \\mathbf{J}^\\top \\mathbf{J} = \\mathbf{V} (\\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma}) \\mathbf{V}^\\top $$\n矩阵 $ \\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma} $ 是一个 $ n \\times n $ 的对角矩阵，其对角线元素为 $ \\sigma_i^2 $。\n\n右侧项 $ -\\mathbf{J}^\\top \\mathbf{r} $ 变为：\n$$ -\\mathbf{J}^\\top \\mathbf{r} = -(\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top)^\\top \\mathbf{r} = -\\mathbf{V} \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{r} $$\n\n将这些代回正规方程并使用 $ \\mathbf{I} = \\mathbf{V} \\mathbf{I} \\mathbf{V}^\\top $：\n$$ (\\mathbf{V} (\\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma}) \\mathbf{V}^\\top + \\lambda \\mathbf{V} \\mathbf{I} \\mathbf{V}^\\top) \\mathbf{s}(\\lambda) = -\\mathbf{V} \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{r} $$\n$$ \\mathbf{V} (\\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma} + \\lambda \\mathbf{I}) \\mathbf{V}^\\top \\mathbf{s}(\\lambda) = -\\mathbf{V} \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{r} $$\n左乘 $ \\mathbf{V}^\\top $ (因为 $ \\mathbf{V}^\\top = \\mathbf{V}^{-1} $) 可以分离出步长向量 $ \\mathbf{s}(\\lambda) $：\n$$ (\\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma} + \\lambda \\mathbf{I}) \\mathbf{V}^\\top \\mathbf{s}(\\lambda) = -\\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{r} $$\n矩阵 $ \\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma} + \\lambda \\mathbf{I} $ 是一个对角矩阵，对角线元素为 $ \\sigma_i^2 + \\lambda $。对于 $ \\lambda \\ge 0 $，只要当 $ \\lambda=0 $ 时没有 $ \\sigma_i=0 $ (即 $ \\mathbf{J} $ 具有满列秩)，该矩阵就是可逆的。我们稍后将处理秩亏的情况。假设可逆，我们可以解出 $ \\mathbf{s}(\\lambda) $：\n$$ \\mathbf{s}(\\lambda) = -\\mathbf{V} (\\boldsymbol{\\Sigma}^\\top \\boldsymbol{\\Sigma} + \\lambda \\mathbf{I})^{-1} \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{r} $$\n信赖域公式要求我们找到 $ \\lambda $ 使得 $ \\| \\mathbf{s}(\\lambda) \\|_2 = \\Delta $。由于 $ \\mathbf{V} $ 是正交的，它保持欧几里得范数不变，因此 $ \\| \\mathbf{s}(\\lambda) \\|_2 = \\| \\mathbf{V}^\\top \\mathbf{s}(\\lambda) \\|_2 $。我们来考察向量 $ \\mathbf{V}^\\top \\mathbf{s}(\\lambda) $ 的分量。设 $ \\mathbf{c} = \\mathbf{U}^\\top \\mathbf{r} $。那么 $ \\boldsymbol{\\Sigma}^\\top \\mathbf{c} $ 的第 $ i $ 个分量是 $ \\sigma_i c_i $。因此 $ \\mathbf{V}^\\top \\mathbf{s}(\\lambda) $ 的第 $ i $ 个分量是：\n$$ (\\mathbf{V}^\\top \\mathbf{s}(\\lambda))_i = - \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda} = - \\frac{\\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r})}{\\sigma_i^2 + \\lambda} $$\n步长的平方范数是这些分量平方的和：\n$$ \\| \\mathbf{s}(\\lambda) \\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r})}{\\sigma_i^2 + \\lambda} \\right)^2 $$\n问题是找到标量函数 $ \\phi(\\lambda) = 0 $ 的根 $ \\lambda  0 $，其中：\n$$ \\phi(\\lambda) = \\| \\mathbf{s}(\\lambda) \\|_2 - \\Delta = \\left[ \\sum_{i=1}^n \\left( \\frac{\\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r})}{\\sigma_i^2 + \\lambda} \\right)^2 \\right]^{1/2} - \\Delta $$\n这就是关于 $ \\lambda $ 的“长期方程”。对于 $ \\lambda \\ge 0 $，它是一个严格递减的函数。\n\n### 步骤 2：算法设计\n\n一个鲁棒的算法必须首先处理平凡情况，然后对非平凡情况采用保险的求根方法。\n\n**初始检查：**\nLevenberg-Marquardt 算法仅当 Gauss-Newton 步 $ \\mathbf{s}(0) $（$ \\lambda=0 $ 时的解）在信赖域之外，即 $ \\| \\mathbf{s}(0) \\|_2  \\Delta $ 时，才需要阻尼（$ \\lambda  0 $）。如果 $ \\| \\mathbf{s}(0) \\|_2 \\le \\Delta $，则最优步是 Gauss-Newton 步，相应的阻尼参数为 $ \\lambda = 0 $。\n\nGauss-Newton 步的平方范数是：\n$$ \\| \\mathbf{s}(0) \\|_2^2 = \\sum_{\\sigma_i  \\epsilon} \\left( \\frac{\\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r})}{\\sigma_i^2} \\right)^2 = \\sum_{\\sigma_i  \\epsilon} \\left( \\frac{\\mathbf{u}_i^\\top \\mathbf{r}}{\\sigma_i} \\right)^2 $$\n其中 $ \\epsilon $ 是一个小的容差，用于检查数值上为零的奇异值。这正是伪逆解 $ -\\mathbf{J}^\\dagger \\mathbf{r} $ 的范数。\n\n**特殊情况分析：**\n1.  **秩亏的 $ \\mathbf{J} $：** 如果某些 $ \\sigma_i = 0 $，则对应的项 $ \\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r}) $ 为零。对于任何 $ \\lambda \\ge 0 $，这些项在 $ \\| \\mathbf{s}(\\lambda) \\|_2^2 $ 的求和中都会消失。因此，只要对所有奇异值正确地执行求和，我们的公式就对秩亏具有内在的鲁棒性。\n2.  **残差在左零空间 ($ \\mathbf{J}^\\top \\mathbf{r} = \\mathbf{0} $):** 如果 $ \\mathbf{r} $ 与 $ \\mathbf{J} $ 的列空间正交，则 $ \\mathbf{J}^\\top \\mathbf{r} = \\mathbf{0} $。用 SVD 分量表示，这意味着 $ \\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{r} = \\mathbf{0} $，即对所有 $ i $ 都有 $ \\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r}) = 0 $。这导致对所有 $ \\lambda \\ge 0 $ 都有 $ \\mathbf{s}(\\lambda) = \\mathbf{0} $。因此，$ \\| \\mathbf{s}(0) \\|_2 = 0 \\le \\Delta $（对于任何 $ \\Delta>0 $），所以解是 $ \\lambda = 0 $。\n\n**保险求根：**\n当 $ \\| \\mathbf{s}(0) \\|_2  \\Delta $ 时，我们必须找到唯一的 $ \\lambda  0 $ 来解 $ \\phi(\\lambda) = 0 $。我们可以使用混合的 Newton-二分法。\n\nNewton 方法提供快速的局部收敛。迭代公式为 $ \\lambda_{k+1} = \\lambda_k - \\phi(\\lambda_k) / \\phi'(\\lambda_k) $。导数 $ \\phi'(\\lambda) $ 是：\n$$ \\phi'(\\lambda) = \\frac{d}{d\\lambda} (\\| \\mathbf{s}(\\lambda) \\|_2) = \\frac{1}{2 \\| \\mathbf{s}(\\lambda) \\|_2} \\frac{d}{d\\lambda} \\| \\mathbf{s}(\\lambda) \\|_2^2 $$\n$$ \\frac{d}{d\\lambda} \\| \\mathbf{s}(\\lambda) \\|_2^2 = \\sum_{i=1}^n (\\sigma_i c_i)^2 \\frac{d}{d\\lambda} (\\sigma_i^2 + \\lambda)^{-2} = -2 \\sum_{i=1}^n \\frac{(\\sigma_i c_i)^2}{(\\sigma_i^2 + \\lambda)^3} $$\n$$ \\phi'(\\lambda) = -\\frac{1}{\\| \\mathbf{s}(\\lambda) \\|_2} \\sum_{i=1}^n \\frac{(\\sigma_i (\\mathbf{u}_i^\\top \\mathbf{r}))^2}{(\\sigma_i^2 + \\lambda)^3} $$\n算法流程如下：\n1.  建立一个包含根的区间 $ [\\lambda_L, \\lambda_U] $。因为 $ \\phi(0)  0 $ 且当 $ \\lambda \\to \\infty $ 时 $ \\phi(\\lambda) \\to -\\Delta  0 $，我们可以设置 $ \\lambda_L=0 $ 并找到一个初始的 $ \\lambda_U $ (例如, $ \\lambda_U = \\|\\mathbf{J}^\\top\\mathbf{r}\\|_2 / \\Delta $)，并在必要时扩展它直到 $ \\phi(\\lambda_U)  0 $。\n2.  迭代：\n    a. 计算一个 Newton 步 $ \\lambda_{\\text{newton}} = \\lambda_k - \\phi(\\lambda_k) / \\phi'(\\lambda_k) $。\n    b. 如果 $ \\lambda_{\\text{newton}} $ 落在当前区间 $ [\\lambda_L, \\lambda_U] $ 内，则接受它。否则，退回到二分步 $ (\\lambda_L + \\lambda_U)/2 $。\n    c. 根据 $ \\phi $ 的符号，使用新值更新区间 $ [\\lambda_L, \\lambda_U] $。\n3.  当区间足够小或 $ |\\phi(\\lambda)| $ 接近于零时终止。\n\n这种组合策略利用了 Newton 方法的速度，同时通过二分法的保险机制保证了收敛性。\n\n### 实现计划摘要\n\n1.  对于每个测试用例 ($ \\mathbf{J}, \\mathbf{r}, \\Delta $):\n2.  计算薄奇异值分解：$ \\mathbf{J} = \\mathbf{U} \\mathbf{s_{vec}} \\mathbf{V}^\\top $。\n3.  计算 $ \\mathbf{c} = \\mathbf{U}^\\top \\mathbf{r} $。\n4.  使用 SVD 分量计算 $ \\| \\mathbf{s}(0) \\|_2 $。对于秩亏情况，使用一个容差来避免除以零。\n5.  如果 $ \\| \\mathbf{s}(0) \\|_2 \\le \\Delta $，则结果为 $ \\lambda = 0 $。\n6.  否则，启动保险的 Newton-二分求根器求解 $ \\phi(\\lambda) = 0 $ 以找到 $ \\lambda  0 $。\n7.  收集每个测试用例得到的结果 $ \\lambda $。\n8.  格式化并打印最终列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the Levenberg-Marquardt damping parameter for a suite of test cases.\n    \"\"\"\n\n    def compute_lambda(J, r, delta, tol=1e-12, max_iter=50):\n        \"\"\"\n        Computes the LM damping parameter lambda using a safeguarded Newton's method.\n        \n        Args:\n            J (np.ndarray): The Jacobian matrix (m x n).\n            r (np.ndarray): The residual vector (m,).\n            delta (float): The trust region radius.\n            tol (float): Convergence tolerance.\n            max_iter (int): Maximum number of iterations for the root-finder.\n\n        Returns:\n            float: The computed damping parameter lambda.\n        \"\"\"\n        # Step 1: Compute SVD of J\n        # Using thin SVD is sufficient and more efficient.\n        try:\n            u, s_vec, vh = np.linalg.svd(J, full_matrices=False)\n        except np.linalg.LinAlgError:\n            # SVD can fail for some pathological matrices, though not in this problem.\n            # A fallback or error is appropriate in a general-purpose library.\n            # For this problem, we assume SVD converges.\n            pass\n\n        # Step 2: Check if damping is needed\n        # Compute the norm of the Gauss-Newton step, s(0), using SVD components.\n        c = u.T @ r\n        \n        # Identify non-negligible singular values\n        sigma_tol = np.finfo(float).eps * max(J.shape) * (s_vec[0] if s_vec.size > 0 else 1)\n        \n        s_vec_nonzero_mask = s_vec > sigma_tol\n        s_gn_sq_norm = 0.0\n        if np.any(s_vec_nonzero_mask):\n             s_gn_sq_norm = np.sum((c[s_vec_nonzero_mask] / s_vec[s_vec_nonzero_mask])**2)\n        norm_s0 = np.sqrt(s_gn_sq_norm)\n\n        # If Gauss-Newton step is within trust region, no damping is needed.\n        if norm_s0 = delta:\n            return 0.0\n\n        # Step 3: Damping is needed. Solve the secular equation for lambda > 0.\n        # phi(lam) = ||s(lam)|| - delta = 0\n        \n        beta = s_vec * c\n\n        def phi(lam):\n            s_lam_sq_norm = np.sum((beta / (s_vec**2 + lam))**2)\n            return np.sqrt(s_lam_sq_norm) - delta\n\n        def phi_prime(lam):\n            s_lam_sq_norm_vec = (beta / (s_vec**2 + lam))**2\n            norm_s_lam = np.sqrt(np.sum(s_lam_sq_norm_vec))\n            \n            if norm_s_lam  tol:\n                # Avoid division by zero if step is tiny\n                return -1.0\n            \n            num = np.sum(s_lam_sq_norm_vec / (s_vec**2 + lam))\n            return -num / norm_s_lam\n\n        # Step 4: Safeguarded Newton-Bisection Method\n        lam_L = 0.0 # From theory, lambda > 0 and phi(0) > 0\n\n        # Find an upper bound lambda_U such that phi(lambda_U)  0\n        jT_r_norm = np.linalg.norm((s_vec * c))\n        lam_U = jT_r_norm / delta\n        for _ in range(10): # Limit expansion iterations\n            if phi(lam_U)  0:\n                break\n            lam_U *= 2.0\n        \n        # In case expansion fails (unlikely for this function)\n        if phi(lam_U) >= 0:\n            # A failsafe; this indicates an issue in logic or extreme inputs\n            # Bisection will still converge if lam_L, lam_U is a valid bracket.\n            # This path shouldn't be taken for the test cases.\n            lam_U = 1e12 \n        \n        lam = (lam_L + lam_U) / 2.0 # Start with bisection\n\n        for _ in range(max_iter):\n            phi_val = phi(lam)\n\n            if abs(phi_val)  tol:\n                break\n                \n            phi_prime_val = phi_prime(lam)\n            \n            # Newton step\n            # Note: phi(lam) > 0 for lam  root, so phi/phi' is negative, newton step is positive\n            newton_step = -phi_val / phi_prime_val\n            lam_newton = lam + newton_step\n            \n            # Safeguard: fall back to bisection if Newton step is outside bracket\n            if not (lam_L  lam_newton  lam_U):\n                lam_next = (lam_L + lam_U) / 2.0\n            else:\n                lam_next = lam_newton\n\n            # Update bracket\n            if phi(lam_next) > 0:\n                lam_L = lam_next\n            else:\n                lam_U = lam_next\n            \n            lam = lam_next\n\n        return lam\n\n    # --- Test Suite ---\n    \n    # Case 1: Happy path\n    J1 = np.array([[3.0, 0.0], [0.0, 1.0], [0.0, 2.0]])\n    r1 = np.array([1.0, -2.0, 0.5])\n    Delta1 = 0.25\n\n    # Case 2: Boundary case (delta = ||s(0)||)\n    # Pre-calculate ||s(0)|| for J1, r1\n    s0_norm_case1 = np.linalg.norm(np.linalg.lstsq(J1, -r1, rcond=None)[0])\n    J2, r2 = J1, r1\n    Delta2 = s0_norm_case1\n\n    # Case 3: Large trust-region (no damping)\n    J3, r3 = J1, r1\n    Delta3 = 1.0\n\n    # Case 4: Rank-deficient Jacobian\n    J4 = np.array([[1.0, 0.0], [0.0, 0.0]])\n    r4 = np.array([1.0, 2.0])\n    Delta4 = 0.5\n\n    # Case 5: Residual in left-nullspace\n    J5 = np.array([[3.0, 0.0], [0.0, 1.0], [0.0, 2.0]])\n    r5 = np.array([0.0, 2.0, -1.0])\n    Delta5 = 0.1\n\n    test_cases = [\n        (J1, r1, Delta1),\n        (J2, r2, Delta2),\n        (J3, r3, Delta3),\n        (J4, r4, Delta4),\n        (J5, r5, Delta5),\n    ]\n\n    results = []\n    for J, r, delta in test_cases:\n        lambda_val = compute_lambda(J, r, delta)\n        results.append(lambda_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}