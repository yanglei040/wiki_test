{
    "hands_on_practices": [
        {
            "introduction": "理论与实践的结合始于对基本原理的深刻理解。最速下降法虽然概念上简单，但其收敛性能在很大程度上取决于问题的尺度。本练习旨在通过一个简洁的分析实例，揭示这一核心问题。通过求解一个精心设计的二维最小二乘问题 ()，您将亲手推导并观察到，当算子矩阵 $A$ 的列范数差异悬殊时，目标函数的Hessian矩阵将变得病态，从而导致优化过程收敛缓慢。这个练习为理解预处理技术的必要性奠定了重要的理论基础。",
            "id": "3422227",
            "problem": "考虑一个线性反问题，其状态向量为双参数 $x \\in \\mathbb{R}^{2}$，测量算子为 $A \\in \\mathbb{R}^{2 \\times 2}$，通过 $y = A x$ 将 $x$ 与测量数据 $b \\in \\mathbb{R}^{2}$ 关联起来。采用最小二乘 (LS) 目标函数 $J(x) = \\frac{1}{2} \\|A x - b\\|_{2}^{2}$ 和最速下降 (SD) 迭代法，该方法从当前点 $x$ 出发，沿着负梯度方向移动，并在此方向上进行精确线搜索。在反问题和数据同化领域中，已知 SD 方法对 $x$ 各分量的缩放很敏感。\n\n您需要通过一个具体例子和变量变换来分析这种对缩放的敏感性。令\n$$\nA(\\epsilon) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\n其中 $\\epsilon \\in (0,1)$ 表示 $A(\\epsilon)$ 的各列之间存在巨大的尺度差异。\n\n任务：\n1. 从 LS 目标函数和梯度的定义出发，解释当 $A(\\epsilon)$ 的各列具有非常不同的欧几里得范数时，为什么最速下降法对 $x$ 分量的缩放敏感。\n2. 引入一个对角缩放矩阵 $S \\in \\mathbb{R}^{2 \\times 2}$ 和变量变换 $y = S x$。说明前向算子如何映射为 $A' = A(\\epsilon) S^{-1}$，并论证应通过 $A(\\epsilon)$ 的欧几里得列范数来选择 $S$，以平衡这些列。\n3. 对于未缩放的变量（即 $S$ 等于单位矩阵），从 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，执行一步带精确线搜索的最速下降迭代。将得到的迭代结果记为 $x_{1}$，并计算 LS 目标函数值 $J(x_{1})$，将其表示为 $\\epsilon$ 的封闭形式函数。\n4. 将您的最终答案表示为关于 $\\epsilon$ 的单一封闭形式解析表达式。无需四舍五入。\n\n您的推导必须从核心定义和公认事实（LS 目标函数、其梯度和精确线搜索）出发，不得使用任何预先给出的简化公式。",
            "solution": "该问题要求分析最速下降法在线性最小二乘问题中对缩放的敏感性，然后对一次迭代进行具体计算。我们将首先建立通用框架，然后讨论缩放敏感性，最后进行详细计算。\n\n最小二乘目标函数由 $J(x) = \\frac{1}{2} \\|Ax - b\\|_{2}^{2}$ 给出。我们可以用向量-转置表示法将其写为 $J(x) = \\frac{1}{2} (Ax - b)^T (Ax - b)$。展开此表达式可得：\n$$\nJ(x) = \\frac{1}{2} (x^T A^T - b^T)(Ax - b) = \\frac{1}{2} (x^T A^T A x - x^T A^T b - b^T A x + b^T b)\n$$\n由于 $b^T A x$ 是一个标量，它等于其转置 $(b^T A x)^T = x^T A^T b$。因此，目标函数为：\n$$\nJ(x) = \\frac{1}{2} (x^T A^T A x - 2 b^T A x + b^T b)\n$$\n为找到最速下降方向，我们计算 $J(x)$ 关于 $x$ 的梯度。使用向量微积分的标准法则（$\\nabla_x(x^T M x) = (M+M^T)x$ 和 $\\nabla_x(c^T x) = c$），并注意到 $A^T A$ 是对称的，我们得到：\n$$\n\\nabla J(x) = \\frac{1}{2} (2 A^T A x - 2 A^T b) = A^T(Ax - b)\n$$\n最速下降法通过迭代 $x_{k+1} = x_k + \\alpha_k p_k$ 更新当前估计值 $x_k$，其中搜索方向 $p_k$ 是负梯度，即 $p_k = -\\nabla J(x_k)$，而 $\\alpha_k$ 是步长。\n\n任务1：对缩放的敏感性。\n最速下降法的收敛速度由问题的条件决定。目标函数 $J(x)$ 的等值线的几何形状由其海森矩阵 $H_J = \\nabla^2 J(x)$ 决定。对于最小二乘问题，海森矩阵是常数：\n$$\nH_J = \\nabla (A^T(Ax-b)) = A^T A\n$$\n海森矩阵 $H_J$ 的特征值决定了 $J(x)$ 的椭球形等值线的主轴长度。如果特征值的量级差异巨大，等值线将是高度拉长的（偏心）。最速下降方向与等值线垂直，因此将几乎垂直于指向最小值的方向，从而导致特有的Z字形模式和缓慢的收敛。$H_J$ 的最大特征值与最小特征值之比，即条件数 $\\kappa(H_J) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$。大的条件数表示缩放不良和收敛缓慢。\n\n对于给定的问题，$A(\\epsilon) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}$。海森矩阵为：\n$$\nH_J = A(\\epsilon)^T A(\\epsilon) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon^2 \\end{pmatrix}\n$$\n这个对角矩阵的特征值是 $\\lambda_1 = 1$ 和 $\\lambda_2 = \\epsilon^2$。条件数是 $\\kappa(H_J) = \\frac{1}{\\epsilon^2}$。由于 $\\epsilon \\in (0,1)$，当 $\\epsilon \\to 0$ 时，条件数 $\\kappa(H_J) \\to \\infty$。这种由 $A(\\epsilon)$ 各列的欧几里得范数差异巨大（$\\|a_1\\|_2=1$, $\\|a_2\\|_2=\\epsilon$）引起的极端病态，是该方法敏感性的原因。\n\n任务2：变量变换。\n为缓解此问题，我们引入一个对角缩放矩阵 $S$ 和一个新变量 $y$，使得 $x = S^{-1}y$。前向模型 $Ax=b$ 变为 $A S^{-1} y = b$。新的前向算子是 $A' = A S^{-1}$，新的最小二乘问题是最小化 $J'(y) = \\frac{1}{2} \\|A' y - b\\|_2^2$。这个新问题的海森矩阵是 $H_{J'} = (A')^T A' = (S^{-1})^T A^T A S^{-1}$。目标是选择 $S$ 使 $H_{J'}$ 良态，理想情况下 $\\kappa(H_{J'}) \\approx 1$。一个标准的选择是缩放算子的列，使其具有单位范数。令 $A = [a_1, a_2]$ 和 $S = \\text{diag}(s_1, s_2)$。那么 $A' = A S^{-1} = [s_1^{-1}a_1, s_2^{-1}a_2]$。为了使新的列具有单位范数，我们必须选择 $s_i = \\|a_i\\|_2$。对于 $A(\\epsilon)$，我们有 $\\|a_1\\|_2 = 1$ 和 $\\|a_2\\|_2 = \\epsilon$。因此，一个最优缩放矩阵是 $S = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}$。通过这个选择，$A' = A(\\epsilon) S^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon^{-1} \\end{pmatrix} = I$，即单位矩阵。新的海森矩阵将是 $I^T I = I$，这是完全良态的。\n\n任务3和4：对未缩放问题执行一步最速下降。\n我们现在对原始的、未缩放的问题进行计算，从 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始。\n初始残差是 $r_0 = Ax_0 - b = -b = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$。\n在 $x_0$ 处的梯度是 $\\nabla J(x_0) = A^T r_0 = A^T(-b)$：\n$$\n\\nabla J(x_0) = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\epsilon \\end{pmatrix}\n$$\n搜索方向是负梯度：\n$$\np_0 = -\\nabla J(x_0) = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}\n$$\n下一个迭代点是 $x_1 = x_0 + \\alpha p_0 = \\alpha p_0$。我们通过最小化关于 $\\alpha$ 的 $J(x_0 + \\alpha p_0)$ 来找到最优步长 $\\alpha$。令 $g(\\alpha) = J(x_0 + \\alpha p_0)$。最小值在 $\\frac{dg}{d\\alpha} = 0$ 处找到。\n$$\n\\frac{dg}{d\\alpha} = \\frac{d}{d\\alpha} J(x_0+\\alpha p_0) = \\nabla J(x_0 + \\alpha p_0)^T p_0 = 0\n$$\n使用梯度公式，$\\nabla J(x_0+\\alpha p_0) = A^T(A(x_0+\\alpha p_0)-b) = A^T(Ax_0-b) + \\alpha A^T A p_0 = \\nabla J(x_0) + \\alpha A^T A p_0$。\n将此代入最小化条件：\n$$\n(\\nabla J(x_0) + \\alpha A^T A p_0)^T p_0 = 0 \\implies \\nabla J(x_0)^T p_0 + \\alpha p_0^T A^T A p_0 = 0\n$$\n使用 $p_0 = -\\nabla J(x_0)$，我们得到 $-p_0^T p_0 + \\alpha p_0^T A^T A p_0 = 0$。\n求解 $\\alpha$ 可得精确线搜索步长：\n$$\n\\alpha = \\frac{p_0^T p_0}{p_0^T A^T A p_0} = \\frac{\\|p_0\\|_2^2}{\\|A p_0\\|_2^2}\n$$\n我们现在为我们的具体问题计算此表达式中的各项。\n分子是 $\\|p_0\\|_2^2 = 1^2 + \\epsilon^2 = 1 + \\epsilon^2$。\n对于分母，我们首先计算 $A p_0$：\n$$\nA p_0 = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\epsilon^2 \\end{pmatrix}\n$$\n然后，$\\|A p_0\\|_2^2 = 1^2 + (\\epsilon^2)^2 = 1 + \\epsilon^4$。\n因此，步长为：\n$$\n\\alpha = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4}\n$$\n新的迭代点 $x_1$ 是：\n$$\nx_1 = x_0 + \\alpha p_0 = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon(1 + \\epsilon^2)}{1 + \\epsilon^4} \\end{pmatrix}\n$$\n最后，我们计算这个新点的目标函数值，$J(x_1) = \\frac{1}{2} \\|A x_1 - b\\|_2^2$。\n新的残差是 $r_1 = A x_1 - b = A(\\alpha p_0) - b = \\alpha (A p_0) - b$：\n$$\nr_1 = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\begin{pmatrix} 1 \\\\ \\epsilon^2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} - 1 \\\\ \\frac{\\epsilon^2(1 + \\epsilon^2)}{1 + \\epsilon^4} - 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2 - (1 + \\epsilon^4)}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon^2 + \\epsilon^4 - (1 + \\epsilon^4)}{1 + \\epsilon^4} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\epsilon^2 - \\epsilon^4}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon^2 - 1}{1 + \\epsilon^4} \\end{pmatrix}\n$$\n现在我们求此残差的范数平方：\n$$\n\\|r_1\\|_2^2 = \\left( \\frac{\\epsilon^2(1 - \\epsilon^2)}{1 + \\epsilon^4} \\right)^2 + \\left( \\frac{-(1 - \\epsilon^2)}{1 + \\epsilon^4} \\right)^2 = \\frac{\\epsilon^4(1 - \\epsilon^2)^2}{(1 + \\epsilon^4)^2} + \\frac{(1 - \\epsilon^2)^2}{(1 + \\epsilon^4)^2}\n$$\n提取公因式：\n$$\n\\|r_1\\|_2^2 = \\frac{(1 - \\epsilon^2)^2 (\\epsilon^4 + 1)}{(1 + \\epsilon^4)^2} = \\frac{(1 - \\epsilon^2)^2}{1 + \\epsilon^4}\n$$\n在 $x_1$ 处的目标函数值是该值的一半：\n$$\nJ(x_1) = \\frac{1}{2}\\|r_1\\|_2^2 = \\frac{(1 - \\epsilon^2)^2}{2(1 + \\epsilon^4)}\n$$\n这就是最速下降法一步之后目标函数值的最终封闭形式表达式。",
            "answer": "$$\n\\boxed{\\frac{(1 - \\epsilon^2)^2}{2(1 + \\epsilon^4)}}\n$$"
        },
        {
            "introduction": "在理解了尺度和条件数对最速下降法性能的影响之后，我们将把这些概念应用于一个重要的实际问题：三维变分（3D-Var）资料同化。在天气预报和海洋学等领域，3D-Var 是融合模型预测与观测资料的关键技术。本练习 () 要求您实现标准的和预处理的最速下降算法，并使用背景误差协方差矩阵 $B$ 作为预处理器。通过对比两种方法的收敛速度并分析Hessian矩阵条件数的变化，您将体验到有效的预处理技术在解决大规模反问题时所带来的显著性能提升。",
            "id": "3422256",
            "problem": "考虑三维变分 (3D-Var) 数据同化目标函数\n$$\nJ(x) = \\frac{1}{2}\\|H x - y\\|_{R^{-1}}^2 + \\frac{1}{2}\\|x - x_b\\|_{B^{-1}}^2,\n$$\n其中 $x \\in \\mathbb{R}^n$ 是待估计的状态，$x_b \\in \\mathbb{R}^n$ 是背景场状态，$y \\in \\mathbb{R}^m$ 是观测值，$H \\in \\mathbb{R}^{m \\times n}$ 是线性观测算子，$R \\in \\mathbb{R}^{m \\times m}$ 是观测误差协方差（假设为对称正定），$B \\in \\mathbb{R}^{n \\times n}$ 是背景场误差协方差（假设为对称正定）。加权范数 $\\|v\\|_{M}^2$ 定义为 $\\|v\\|_{M}^2 = v^\\top M v$，其中 $M$ 为任意对称正定矩阵。\n\n从加权最小二乘和二次型导数的核心定义出发，推导用于最小化 $J(x)$ 的最速下降算法，包括无预处理和使用预处理子 $P = B$ 两种情况。在每次迭代中使用精确线性搜索，该搜索需从第一性原理计算得出（不要使用现成公式）。使用背景场状态 $x_b$ 作为初始猜测值。实现这两种算法并比较它们的行为。\n\n对于下述每个测试用例，您的程序必须计算以下量：\n- 作为 $J(x)$ 的Hessian矩阵的对称正定矩阵 $A$ 的二范数条件数。\n- 对称预处理算子 $B^{1/2} A B^{1/2}$ 的二范数条件数，其中 $B^{1/2}$ 是 $B$ 的唯一对称平方根。\n- 无预处理的最速下降法（方向为 $-g$）在精确线性搜索下达到 $\\|x_k - x^\\star\\|_2 / \\|x^\\star\\|_2 \\le \\varepsilon$ 所需的迭代次数，其中 $x^\\star$ 是唯一最小化子，$\\varepsilon = 10^{-8}$。\n- 预处理的最速下降法（方向为 $-B g$）在精确线性搜索下达到相同容差所需的迭代次数。\n- 一个布尔值，指示预处理是否比无预处理方法所需迭代次数严格更少。\n\n使用实值计算，不要引入任何任意的单位转换。不涉及角度。所有输出都应为数值或布尔类型。测试套件如下，所有矩阵和向量均已明确给出：\n\n- 测试用例1（结构与背景场匹配）：\n  - $n = 3$, $m = 3$.\n  - $B = \\mathrm{diag}(9, 1, 0.25)$.\n  - $R = I_3$.\n  - $H = 2 B^{-1/2} = \\mathrm{diag}\\left(\\frac{2}{3}, 2, 4\\right)$.\n  - $x_b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$.\n\n- 测试用例2（具有异构观测方差的单位观测算子）：\n  - $n = 3$, $m = 3$.\n  - $B = \\mathrm{diag}(1, 100, 10000)$.\n  - $R = \\mathrm{diag}(1, 0.01, 100)$.\n  - $H = I_3$.\n  - $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- 测试用例3（低秩观测算子）：\n  - $n = 3$, $m = 2$.\n  - $B = \\mathrm{diag}(4, 0.04, 25)$.\n  - $R = \\mathrm{diag}(0.5, 0.1)$.\n  - $H = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}$.\n  - $x_b = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 1.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$.\n\n您的实现必须：\n- 根据定义构建 $J(x)$ 的二次型及其梯度，确定对称正定矩阵 $A$ 和向量 $b$，使得 $\\nabla J(x) = A x - b$ 且 $x^\\star$ 求解 $A x^\\star = b$。\n- 对无预处理（$P = I$）和预处理（$P = B$）的最速下降方向，在每次迭代中使用从第一性原理推导的精确线性搜索。\n- 通过对对称正定矩阵进行特征值分析来计算二范数条件数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例对应一个条目。每个条目本身应该是一个列表 $[\\kappa(A), \\kappa(B^{1/2} A B^{1/2}), N_{\\text{unprec}}, N_{\\text{prec}}, \\text{better}]$，其中 $\\kappa(\\cdot)$ 表示二范数条件数，$N_{\\text{unprec}}$ 和 $N_{\\text{prec}}$ 是整数，$\\text{better}$ 是一个布尔值。例如，最终输出格式必须是\n$$\n[\\,[\\kappa_1,\\kappa_{\\text{pre},1},N_{\\text{unprec},1},N_{\\text{prec},1},\\text{better}_1],\\,[\\kappa_2,\\kappa_{\\text{pre},2},N_{\\text{unprec},2},N_{\\text{prec},2},\\text{better}_2],\\,[\\kappa_3,\\kappa_{\\text{pre},3},N_{\\text{unprec},3},N_{\\text{prec},3},\\text{better}_3]\\,].\n$$",
            "solution": "该问题要求推导并实现最速下降算法，以最小化三维变分 (3D-Var) 数据同化目标函数，包括无预处理和有预处理两种情况。\n\n目标函数由下式给出\n$$\nJ(x) = \\frac{1}{2}\\|H x - y\\|_{R^{-1}}^2 + \\frac{1}{2}\\|x - x_b\\|_{B^{-1}}^2\n$$\n其中 $x \\in \\mathbb{R}^n$ 是状态向量，$x_b \\in \\mathbb{R}^n$ 是背景场状态，$y \\in \\mathbb{R}^m$ 是观测值，$H \\in \\mathbb{R}^{m \\times n}$ 是线性观测算子，$R \\in \\mathbb{R}^{m \\times m}$ 和 $B \\in \\mathbb{R}^{n \\times n}$ 分别是对称正定 (SPD) 的观测误差和背景场误差协方差矩阵。加权范数定义为 $\\|v\\|_M^2 = v^\\top M v$。\n\n首先，我们将目标函数表示为标准二次型。通过展开加权范数，我们得到：\n$$\nJ(x) = \\frac{1}{2}(H x - y)^\\top R^{-1} (H x - y) + \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b)\n$$\n展开两项可得：\n$$\nJ(x) = \\frac{1}{2}(x^\\top H^\\top R^{-1}Hx - x^\\top H^\\top R^{-1}y - y^\\top R^{-1}Hx + y^\\top R^{-1}y) + \\frac{1}{2}(x^\\top B^{-1}x - x^\\top B^{-1}x_b - x_b^\\top B^{-1}x + x_b^\\top B^{-1}x_b)\n$$\n由于 $R$ 和 $B$ 是对称的，它们的逆也是对称的。利用标量转置性质（$u^\\top M v = v^\\top M^\\top u$），我们可以合并包含 $x$ 的项：\n$$\nJ(x) = \\frac{1}{2}x^\\top (H^\\top R^{-1} H + B^{-1})x - x^\\top (H^\\top R^{-1}y + B^{-1}x_b) + C\n$$\n其中 $C = \\frac{1}{2}y^\\top R^{-1}y + \\frac{1}{2}x_b^\\top B^{-1}x_b$ 是一个不依赖于 $x$ 的常数。这是标准二次型 $J(x) = \\frac{1}{2}x^\\top A x - x^\\top b + C$，其中 Hessian 矩阵 $A$ 和向量 $b$ 被确定为：\n$$\nA = H^\\top R^{-1} H + B^{-1}\n$$\n$$\nb = H^\\top R^{-1}y + B^{-1}x_b\n$$\n矩阵 $R^{-1}$ 是对称正定的，因为 $R$ 是对称正定的。矩阵 $H^\\top R^{-1} H$ 是对称半正定的。矩阵 $B^{-1}$ 是对称正定的，因为 $B$ 是对称正定的。一个对称半正定矩阵与一个对称正定矩阵之和是对称正定的。因此，Hessian 矩阵 $A$ 是对称正定的，这保证了 $J(x)$ 是严格凸的，并拥有唯一的最小化子，记为 $x^\\star$。\n\n目标函数的梯度是：\n$$\n\\nabla J(x) = g(x) = A x - b\n$$\n通过将梯度设为零 $\\nabla J(x^\\star) = 0$ 来找到最小化子 $x^\\star$，这导出了线性系统：\n$$\nA x^\\star = b\n$$\n\n最速下降算法是寻找 $x^\\star$ 的一种迭代方法。从初始猜测 $x_0 = x_b$ 开始，迭代按 $x_{k+1} = x_k + \\alpha_k p_k$ 进行更新，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。对于精确线性搜索，选择 $\\alpha_k$ 以最小化 $J(x_k + \\alpha p_k)$。$\\phi(\\alpha) = J(x_k + \\alpha p_k)$ 对 $\\alpha$ 的导数是 $\\phi'(\\alpha) = \\nabla J(x_k + \\alpha p_k)^\\top p_k$。将其设为零可得：\n$$\n(A(x_k + \\alpha_k p_k) - b)^\\top p_k = 0 \\implies (A x_k - b + \\alpha_k A p_k)^\\top p_k = 0 \\implies (g_k + \\alpha_k A p_k)^\\top p_k = 0\n$$\n求解 $\\alpha_k$ 得到最优步长的通用公式：\n$$\n\\alpha_k = -\\frac{g_k^\\top p_k}{p_k^\\top A p_k}\n$$\n\n**1. 无预处理的最速下降**\n搜索方向是负梯度，即 $p_k = -g_k$。将此代入步长公式：\n$$\n\\alpha_k = -\\frac{g_k^\\top (-g_k)}{(-g_k)^\\top A (-g_k)} = \\frac{g_k^\\top g_k}{g_k^\\top A g_k}\n$$\n更新规则是 $x_{k+1} = x_k - \\alpha_k g_k$。\n\n**2. 使用 $P=B$ 的预处理最速下降**\n预处理子 $P=B$ 用于定义搜索方向 $p_k = -P g_k = -B g_k$。将这个新方向代入 $\\alpha_k$ 的公式中：\n$$\n\\alpha_k = -\\frac{g_k^\\top (-B g_k)}{(-B g_k)^\\top A (-B g_k)} = \\frac{g_k^\\top B g_k}{g_k^\\top B^\\top A B g_k}\n$$\n由于 $B$ 是对称的（$B^\\top = B$），上式可简化为：\n$$\n\\alpha_k = \\frac{g_k^\\top B g_k}{g_k^\\top B A B g_k}\n$$\n更新规则是 $x_{k+1} = x_k - \\alpha_k B g_k$。\n\n**收敛性分析**\n对于具有对称正定 Hessian 矩阵 $A$ 的二次函数，最速下降法的收敛速率由 $A$ 的二范数条件数 $\\kappa_2(A)$ 决定。对于一个对称正定矩阵 $M$，$\\kappa_2(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是最大和最小特征值。条件数越大，意味着收敛越慢。\n\n预处理旨在将问题转换为一个具有更低条件数的问题。使用对称预处理子 $P=B$ 等价于对一个变换后的问题应用标准最速下降法。通过变量替换 $\\hat{x} = B^{-1/2}x$，目标函数变为 $\\hat{J}(\\hat{x}) = \\frac{1}{2}\\hat{x}^\\top (B^{1/2} A B^{1/2}) \\hat{x} - \\hat{x}^\\top (B^{1/2} b) + C$。因此，预处理算法的收敛速率由对称预处理后的 Hessian 矩阵的条件数 $\\kappa_2(B^{1/2} A B^{1/2})$ 控制。一个有效的预处理子会使得 $\\kappa_2(B^{1/2} A B^{1/2}) \\ll \\kappa_2(A)$。实现将计算这些条件数，并统计两种方法达到收敛所需的迭代次数。",
            "answer": "```python\nimport numpy as np\n\ndef steepest_descent(A, b, x_star, x_b, B=None, method='unpreconditioned', epsilon=1e-8, max_iter=500000):\n    \"\"\"\n    Performs steepest descent for the quadratic problem 1/2*x'Ax - b'x.\n    \"\"\"\n    x = x_b.copy()\n    \n    x_star_norm = np.linalg.norm(x_star)\n    \n    if x_star_norm  1e-12:\n        # If the true solution is the zero vector, use absolute error\n        error_func = lambda x_k: np.linalg.norm(x_k)\n    else:\n        # Otherwise, use relative error\n        error_func = lambda x_k: np.linalg.norm(x_k - x_star) / x_star_norm\n\n    for k in range(max_iter + 1):\n        if error_func(x) = epsilon:\n            return k\n\n        g = A @ x - b\n        \n        # In case the initial guess is the solution\n        if np.linalg.norm(g)  1e-15:\n            return k\n\n        if method == 'unpreconditioned':\n            # Search direction p_k = -g_k\n            alpha_num = g.T @ g\n            alpha_den = g.T @ A @ g\n            if alpha_den == 0: break\n            alpha = alpha_num / alpha_den\n            x = x - alpha * g\n        elif method == 'preconditioned':\n            # Search direction p_k = -B @ g_k\n            p = B @ g\n            alpha_num = g.T @ p\n            alpha_den = p.T @ A @ p\n            if alpha_den == 0: break\n            alpha = alpha_num / alpha_den\n            x = x - alpha * p\n        else:\n            raise ValueError(\"Invalid method specified.\")\n            \n    return max_iter # Return max_iter to indicate non-convergence\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            'n': 3, 'm': 3,\n            'B': np.diag([9.0, 1.0, 0.25]),\n            'R': np.diag([1.0, 1.0, 1.0]),\n            'H': np.diag([2.0/3.0, 2.0, 4.0]),\n            'x_b': np.array([0.1, -0.2, 0.3]),\n            'y': np.array([1.0, -2.0, 0.5])\n        },\n        # Test case 2\n        {\n            'n': 3, 'm': 3,\n            'B': np.diag([1.0, 100.0, 10000.0]),\n            'R': np.diag([1.0, 0.01, 100.0]),\n            'H': np.diag([1.0, 1.0, 1.0]),\n            'x_b': np.array([1.0, -1.0, 0.0]),\n            'y': np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 3\n        {\n            'n': 3, 'm': 2,\n            'B': np.diag([4.0, 0.04, 25.0]),\n            'R': np.diag([0.5, 0.1]),\n            'H': np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            'x_b': np.array([0.5, -0.5, 1.0]),\n            'y': np.array([1.0, -2.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        H, R, B, x_b, y = case['H'], case['R'], case['B'], case['x_b'], case['y']\n        \n        # Invert R and B\n        R_inv = np.linalg.inv(R)\n        B_inv = np.linalg.inv(B)\n        \n        # Formulate the Hessian A and vector b for the system Ax = b\n        A = H.T @ R_inv @ H + B_inv\n        b = H.T @ R_inv @ y + B_inv @ x_b\n        \n        # Calculate the true solution x_star\n        x_star = np.linalg.solve(A, b)\n        \n        # --- Condition number calculations ---\n        # 1. Unpreconditioned Hessian A\n        eigvals_A = np.linalg.eigvalsh(A)\n        kappa_A = np.max(eigvals_A) / np.min(eigvals_A)\n        \n        # 2. Symmetrically preconditioned Hessian\n        B_sqrt = np.sqrt(B) # B is diagonal, so sqrt is element-wise\n        A_pre = B_sqrt @ A @ B_sqrt\n        eigvals_A_pre = np.linalg.eigvalsh(A_pre)\n        kappa_A_pre = np.max(eigvals_A_pre) / np.min(eigvals_A_pre)\n        \n        # --- Iteration counts ---\n        # 3. Unpreconditioned steepest descent\n        N_unprec = steepest_descent(A, b, x_star, x_b, method='unpreconditioned')\n        \n        # 4. Preconditioned steepest descent\n        N_prec = steepest_descent(A, b, x_star, x_b, B=B, method='preconditioned')\n        \n        # 5. Comparison\n        better = N_prec  N_unprec\n        \n        results.append([kappa_A, kappa_A_pre, N_unprec, N_prec, better])\n\n    # Format the final output string precisely\n    output_strings = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' for consistent output format\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{str(res[4]).lower()}]\"\n        output_strings.append(res_str)\n        \n    print(f\"[{','.join(output_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "经典最小二乘法的有效性基于高斯噪声假设，但现实世界中的数据往往受到包含异常值的重尾噪声污染。最后一个练习将引导您探索如何通过改进目标函数来增强算法的鲁棒性。本练习 () 引入了Huber损失函数，它巧妙地结合了二次损失和线性损失的优点，对小误差采用二次惩罚，对大误差（异常值）则采用线性惩罚。您将需要推导Huber损失下的梯度表达式，并通过编程实现来量化比较它与标准二次损失在面对异常值时下降方向的差异，从而深刻理解鲁棒统计在反问题求解中的价值。",
            "id": "3422261",
            "problem": "考虑一个由矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 表示的线性观测算子和一个测量向量 $b \\in \\mathbb{R}^{m}$。定义残差 $r(x) = A x - b$。在用于反问题和数据同化的经典最小二乘法中，我们考虑目标函数 $f(x) = \\frac{1}{2}\\lVert r(x) \\rVert_2^2$，其中 $\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。在重尾噪声的影响下，大的残差（离群值）可能会主导 $f(x)$ 的下降方向。为减轻此影响，考虑使用 Huber 损失，其阈值参数为 $\\delta > 0$，对标量残差 $t$ 逐元素定义如下：\n$$\n\\phi_\\delta(t) = \n\\begin{cases}\n\\frac{1}{2} t^2,  \\text{if } |t| \\le \\delta, \\\\\n\\delta |t| - \\frac{1}{2} \\delta^2,  \\text{if } |t|  \\delta.\n\\end{cases}\n$$\n定义鲁棒目标函数 $g_\\delta(x) = \\sum_{i=1}^{m} \\phi_\\delta(r_i(x))$，其中 $r_i(x)$ 表示 $r(x)$ 的第 $i$ 个分量。\n\n在欧几里得几何下，对于一个可微的目标函数 $h(x)$，其在点 $x$ 处的最速下降方向是负梯度 $- \\nabla h(x)$。您必须从上述定义出发，利用第一性原理和链式法则推导梯度表达式，不得假设任何预先给定的梯度公式。\n\n您的任务是实现一个程序，对下面指定的每个测试用例，计算：\n1. 在给定点 $x$ 处，二次损失 $f(x)$ 的最速下降方向，以及在同一点 $x$ 处，Huber 损失 $g_\\delta(x)$ 的最速下降方向。\n2. 这两个方向之间的夹角，以弧度为单位。\n3. 每种损失的定量离群值敏感度度量，定义为来自离群值索引集的梯度贡献的欧几里得范数与完整梯度的欧几里得范数之比。具体来说，定义离群值索引集 $O = \\{ i \\in \\{1,\\dots,m\\} : |r_i(x)|  \\delta \\}$。对于二次损失，通过将 $r(x)$ 中所有非离群值残差置零并重新计算梯度来获得离群值贡献。对于 Huber 损失，通过将 Huber 损失关于残差的逐元素导数向量中的所有非离群值元素置零，然后重新计算梯度来计算离群值贡献。如果完整梯度是零向量，则在该测试用例中将夹角和比率都定义为标量 $0$。\n\n您必须以弧度表示夹角。不涉及物理单位。所有输出必须是实数。您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个三元素列表 $[\\text{angle}, \\text{outlier\\_fraction\\_L2}, \\text{outlier\\_fraction\\_Huber}]$，完整输出是这些列表的列表，例如 $[[0.1,0.2,0.05],[\\dots],\\dots]$。\n\n使用以下测试套件。对于每个测试用例，通过 $b = A x_{\\text{true}} + \\eta$ 确定性地构造 $b$，其中 $x_{\\text{true}}$ 和 $\\eta$ 已给出。重尾噪声由 $\\eta$ 中的两个大数值项表示。\n\n令 $m = 10$ 且 $n = 4$。定义\n$$\nA = \\begin{bmatrix}\n1.0  0.0  2.0  -1.0 \\\\\n0.5  -1.0  0.0  1.5 \\\\\n-0.3  0.7  1.0  0.0 \\\\\n2.0  -0.5  0.0  0.3 \\\\\n-1.5  0.0  1.2  0.8 \\\\\n0.0  1.0  -0.5  2.0 \\\\\n-0.7  0.4  0.0  1.1 \\\\\n1.3  0.0  -1.0  0.0 \\\\\n0.2  -0.9  0.5  -0.3 \\\\\n-0.1  0.2  1.5  1.0\n\\end{bmatrix},\n\\quad\nx_{\\text{true}} = \\begin{bmatrix} 1.5 \\\\ -0.5 \\\\ 0.8 \\\\ 0.0 \\end{bmatrix},\n\\quad\n\\eta = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.0 \\\\ 0.0 \\\\ -0.1 \\\\ 0.05 \\\\ 20.0 \\\\ -25.0 \\\\ 0.2 \\end{bmatrix}.\n$$\n构造 $b = A x_{\\text{true}} + \\eta$。\n\n将测试用例定义为元组 $(A, b, \\delta, x)$:\n- 案例 1 (一般重尾场景): $(A, b, 1.0, x = \\mathbf{0})$，其中 $\\mathbf{0}$ 是 $\\mathbb{R}^4$ 中的零向量。\n- 案例 2 (无离群值边界): $(A, b, 30.0, x = \\mathbf{0})$。\n- 案例 3 (残差等于负噪声，更强的离群值影响): $(A, b, 0.5, x = x_{\\text{true}})$。\n\n对于每个案例，按规定计算三个输出。您的程序应生成单行输出，其中包含格式完全符合要求的汇总结果，即用方括号括起来的逗号分隔列表，且无多余空格，例如：\n$[[\\text{angle}_1,\\text{fracL2}_1,\\text{fracHuber}_1],[\\text{angle}_2,\\text{fracL2}_2,\\text{fracHuber}_2],[\\text{angle}_3,\\text{fracL2}_3,\\text{fracHuber}_3]]$。",
            "solution": "用户提供的问题是有效的，因为它在数值优化和鲁棒统计学方面有科学依据，提供了所有必要的数据和定义，是适定的，并且表述客观。因此，我们可以着手给出一个形式化的解法。\n\n目标是在一个线性反问题的背景下，比较标准二次损失函数和鲁棒 Huber 损失函数的最速下降方向。这包括推导两个目标函数的梯度，然后用它们来计算下降方向之间的夹角以及一个离群值敏感度的度量。\n\n设状态向量为 $x \\in \\mathbb{R}^n$，观测算子为矩阵 $A \\in \\mathbb{R}^{m \\times n}$，测量向量为 $b \\in \\mathbb{R}^m$。残差向量定义为 $r(x) = Ax - b \\in \\mathbb{R}^m$。\n\n**1. 二次目标函数 $f(x)$ 的梯度**\n\n经典最小二乘目标函数由下式给出：\n$$ f(x) = \\frac{1}{2} \\lVert r(x) \\rVert_2^2 = \\frac{1}{2} r(x)^T r(x) = \\frac{1}{2} \\sum_{i=1}^{m} [r_i(x)]^2 $$\n为求梯度 $\\nabla f(x)$，我们计算其关于向量 $x$ 的每个分量 $x_j$（其中 $j \\in \\{1, \\dots, n\\}$）的偏导数。使用链式法则：\n$$ \\frac{\\partial f}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{1}{2} \\sum_{i=1}^{m} [r_i(x)]^2 \\right) = \\sum_{i=1}^{m} r_i(x) \\frac{\\partial r_i(x)}{\\partial x_j} $$\n残差的第 $i$ 个分量是 $r_i(x) = (\\sum_{k=1}^{n} A_{ik} x_k) - b_i$。它关于 $x_j$ 的偏导数是：\n$$ \\frac{\\partial r_i(x)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^{n} A_{ik} x_k - b_i \\right) = A_{ij} $$\n将此代回到 $f(x)$ 的偏导数表达式中：\n$$ \\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^{m} r_i(x) A_{ij} = \\sum_{i=1}^{m} (A^T)_{ji} r_i(x) $$\n这是矩阵-向量积 $A^T r(x)$ 的第 $j$ 个分量。因此，$f(x)$ 的梯度是：\n$$ \\nabla f(x) = A^T r(x) = A^T (Ax - b) $$\n$f(x)$ 在点 $x$ 处的最速下降方向是负梯度，即 $d_f(x) = -\\nabla f(x) = -A^T (Ax - b)$。\n\n**2. 鲁棒 Huber 目标函数 $g_\\delta(x)$ 的梯度**\n\nHuber 目标函数定义为 $g_\\delta(x) = \\sum_{i=1}^{m} \\phi_\\delta(r_i(x))$，其中 $\\phi_\\delta(t)$ 是带有阈值 $\\delta > 0$ 的标量参数 $t$ 的 Huber 损失：\n$$\n\\phi_\\delta(t) = \n\\begin{cases}\n\\frac{1}{2} t^2,  \\text{if } |t| \\le \\delta, \\\\\n\\delta |t| - \\frac{1}{2} \\delta^2,  \\text{if } |t|  \\delta.\n\\end{cases}\n$$\n$\\phi_\\delta(t)$ 关于 $t$ 的导数，记为 $\\psi_\\delta(t)$，是：\n$$\n\\psi_\\delta(t) = \\frac{d\\phi_\\delta(t)}{dt} = \n\\begin{cases}\nt,  \\text{if } |t| \\le \\delta, \\\\\n\\delta \\cdot \\text{sgn}(t),  \\text{if } |t|  \\delta.\n\\end{cases}\n$$\n该函数是连续的。它可以紧凑地表示为 $\\psi_\\delta(t) = \\text{clip}(t, -\\delta, \\delta) = \\min(\\delta, \\max(-\\delta, t))$。\n\n使用链式法则求梯度 $\\nabla g_\\delta(x)$：\n$$ \\frac{\\partial g_\\delta}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{m} \\phi_\\delta(r_i(x)) \\right) = \\sum_{i=1}^{m} \\frac{d\\phi_\\delta(r_i(x))}{dr_i} \\frac{\\partial r_i(x)}{\\partial x_j} $$\n代入 $\\frac{d\\phi_\\delta(r_i(x))}{dr_i} = \\psi_\\delta(r_i(x))$ 和 $\\frac{\\partial r_i(x)}{\\partial x_j} = A_{ij}$：\n$$ \\frac{\\partial g_\\delta}{\\partial x_j} = \\sum_{i=1}^{m} \\psi_\\delta(r_i(x)) A_{ij} = \\sum_{i=1}^{m} (A^T)_{ji} \\psi_\\delta(r_i(x)) $$\n设 $\\Psi_\\delta(r)$ 是将 $\\psi_\\delta$ 逐元素地应用于向量 $r$ 的向量函数。则 $g_\\delta(x)$ 的梯度为：\n$$ \\nabla g_\\delta(x) = A^T \\Psi_\\delta(r(x)) = A^T \\Psi_\\delta(Ax - b) $$\n$g_\\delta(x)$ 在点 $x$ 处的最速下降方向是 $d_g(x) = -\\nabla g_\\delta(x) = -A^T \\Psi_\\delta(Ax - b)$。\n\n**3. 最速下降方向之间的夹角**\n\n两个最速下降方向 $d_f$ 和 $d_g$ 之间的夹角 $\\theta$（以弧度为单位）使用点积公式计算：\n$$ \\theta = \\arccos\\left( \\frac{d_f^T d_g}{\\lVert d_f \\rVert_2 \\lVert d_g \\rVert_2} \\right) = \\arccos\\left( \\frac{(\\nabla f)^T (\\nabla g)}{\\lVert \\nabla f \\rVert_2 \\lVert \\nabla g \\rVert_2} \\right) $$\n根据问题规定，如果任一梯度为零向量（即 $\\lVert \\nabla f \\rVert_2 = 0$ 或 $\\lVert \\nabla g \\rVert_2 = 0$），则夹角定义为 $0$。\n\n**4. 离群值敏感度度量**\n\n离群值索引集为 $O = \\{ i \\in \\{1, \\dots, m\\} : |r_i(x)| > \\delta \\}$。\n\n对于二次损失，来自离群值的梯度贡献是使用一个掩码后的残差向量 $r_{\\text{outlier}}$ 计算的，其中如果 $i \\in O$，则 $(r_{\\text{outlier}})_i = r_i$，否则为 $0$。该贡献为 $\\nabla f_{\\text{outlier}} = A^T r_{\\text{outlier}}$。敏感度度量是范数之比：\n$$ \\text{fracL2} = \\frac{\\lVert \\nabla f_{\\text{outlier}} \\rVert_2}{\\lVert \\nabla f \\rVert_2} $$\n\n对于 Huber 损失，来自离群值的梯度贡献是使用一个掩码后的向量 $\\Psi_{\\delta, \\text{outlier}}(r)$ 计算的，其中如果 $i \\in O$，则 $(\\Psi_{\\delta, \\text{outlier}}(r))_i = \\psi_\\delta(r_i)$，否则为 $0$。该贡献为 $\\nabla g_{\\delta, \\text{outlier}} = A^T \\Psi_{\\delta, \\text{outlier}}(r)$。敏感度度量是：\n$$ \\text{fracHuber} = \\frac{\\lVert \\nabla g_{\\delta, \\text{outlier}} \\rVert_2}{\\lVert \\nabla g_\\delta \\rVert_2} $$\n在这两种情况下，如果分母中完整梯度的范数为零，则该比率定义为 $0$。\n\n以下实现为指定的测试用例计算这些量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing metrics for quadratic and Huber losses\n    for a series of test cases.\n    \"\"\"\n    A = np.array([\n        [1.0, 0.0, 2.0, -1.0],\n        [0.5, -1.0, 0.0, 1.5],\n        [-0.3, 0.7, 1.0, 0.0],\n        [2.0, -0.5, 0.0, 0.3],\n        [-1.5, 0.0, 1.2, 0.8],\n        [0.0, 1.0, -0.5, 2.0],\n        [-0.7, 0.4, 0.0, 1.1],\n        [1.3, 0.0, -1.0, 0.0],\n        [0.2, -0.9, 0.5, -0.3],\n        [-0.1, 0.2, 1.5, 1.0]\n    ])\n    x_true = np.array([1.5, -0.5, 0.8, 0.0])\n    eta = np.array([0.1, -0.2, 0.05, 0.0, 0.0, -0.1, 0.05, 20.0, -25.0, 0.2])\n\n    b = A @ x_true + eta\n\n    test_cases = [\n        # Case 1: General heavy-tailed scenario\n        (A, b, 1.0, np.zeros(4)),\n        # Case 2: No outliers boundary\n        (A, b, 30.0, np.zeros(4)),\n        # Case 3: Residuals equal to negative noise\n        (A, b, 0.5, x_true)\n    ]\n\n    results = []\n    for case in test_cases:\n        A_case, b_case, delta, x_case = case\n        \n        # 1. Compute residual\n        r = A_case @ x_case - b_case\n\n        # 2. Compute gradients for both loss functions\n        grad_f = A_case.T @ r\n        \n        # psi_delta(t) = clip(t, -delta, delta)\n        psi_r = np.clip(r, -delta, delta)\n        grad_g = A_case.T @ psi_r\n        \n        norm_grad_f = np.linalg.norm(grad_f)\n        norm_grad_g = np.linalg.norm(grad_g)\n\n        # 3. Compute angle between gradient vectors\n        angle = 0.0\n        if norm_grad_f > 1e-12 and norm_grad_g > 1e-12:\n            cos_angle = np.dot(grad_f, grad_g) / (norm_grad_f * norm_grad_g)\n            # Clip for numerical stability\n            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n        \n        # 4. Compute outlier sensitivity metrics\n        outlier_indices_mask = np.abs(r) > delta\n        \n        # L2 sensitivity\n        frac_l2 = 0.0\n        if norm_grad_f > 1e-12:\n            r_outlier = np.where(outlier_indices_mask, r, 0.0)\n            grad_f_outlier = A_case.T @ r_outlier\n            frac_l2 = np.linalg.norm(grad_f_outlier) / norm_grad_f\n        \n        # Huber sensitivity\n        frac_huber = 0.0\n        if norm_grad_g > 1e-12:\n            psi_r_outlier = np.where(outlier_indices_mask, psi_r, 0.0)\n            grad_g_outlier = A_case.T @ psi_r_outlier\n            frac_huber = np.linalg.norm(grad_g_outlier) / norm_grad_g\n            \n        results.append([angle, frac_l2, frac_huber])\n\n    # Format the final output string without extra whitespace\n    formatted_results = []\n    for res in results:\n        formatted_results.append(f'[{res[0]},{res[1]},{res[2]}]')\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}