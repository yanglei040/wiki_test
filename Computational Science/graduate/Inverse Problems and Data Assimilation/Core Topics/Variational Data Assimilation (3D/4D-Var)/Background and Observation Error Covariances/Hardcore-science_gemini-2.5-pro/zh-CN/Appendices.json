{
    "hands_on_practices": [
        {
            "introduction": "虽然我们力求模型尽可能真实，但在实践中，为了方便，我们常常会做一些简化的假设，例如忽略观测误差之间的相关性。本练习旨在探讨这种简化的后果。我们将解析地推导，在观测误差协方差矩阵 $R$ 中忽略真实存在的相关性，会如何影响一个关键的数据同化诊断量——信号自由度（Degrees of Freedom for Signal, DFS）()。这项实践定量地揭示了看似无害的假设背后隐藏的风险，阐明了它们如何导致对观测信息量的过高估计（即过拟合）。",
            "id": "3366409",
            "problem": "考虑一个标量状态变量的线性高斯数据同化设定，其中状态 $x$ 的先验（背景）是均值为 $x_{b}$、方差为 $B=\\sigma_{b}^{2}$ 的高斯分布。两个独立的仪器使用线性观测算子 $H=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 观测同一状态变量，得到观测模型 $y = H x + \\epsilon$。观测误差 $\\epsilon$ 是零均值高斯分布，其协方差为 $R$，其中真实的观测误差协方差为\n$$\nR_{\\text{true}} \\;=\\; \\sigma_{o}^{2}\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\n相关系数为 $\\rho \\in (-1,1)$。一个忽略观测误差相关性的同化系统转而使用对角近似\n$$\nR_{\\text{diag}} \\;=\\; \\sigma_{o}^{2} I_{2}.\n$$\n设卡尔曼增益定义为 $K \\;=\\; B H^{\\top} \\left(H B H^{\\top} + R \\right)^{-1}$，信号自由度 (Degrees of Freedom for Signal, DFS) 定义为 $\\mathrm{DFS} \\;=\\; \\mathrm{tr}\\!\\left(HK\\right)$。\n\n仅从这些定义以及线性高斯估计和线性代数的标准性质出发，推导在 $R_{\\text{true}}$ 和 $R_{\\text{diag}}$ 条件下的 DFS 的闭式表达式，然后计算乘性膨胀因子\n$$\n\\Phi(\\rho,\\sigma_{b}^{2},\\sigma_{o}^{2}) \\;=\\; \\frac{\\mathrm{DFS}(R_{\\text{diag}})}{\\mathrm{DFS}(R_{\\text{true}})}.\n$$\n请以 $\\rho$、$\\sigma_{b}^{2}$ 和 $\\sigma_{o}^{2}$ 的单一简化解析表达式的形式给出最终结果。无需进行数值计算或四舍五入。",
            "solution": "经验证，该问题是自洽的，其科学基础在于数据同化和线性代数的原理，并且在数学上是适定的。参数定义清晰，目标是推导一个解析表达式，这是一个可行的任务。因此，我们可以着手求解。\n\n问题的核心是，在关于观测误差协方差矩阵 $R$ 的两种不同假设下，计算定义为 $\\mathrm{DFS} = \\mathrm{tr}(HK)$ 的信号自由度 (DFS)，然后求出所得 DFS 值的比率。卡尔曼增益 $K$ 由 $K = B H^{\\top} (H B H^{\\top} + R)^{-1}$ 给出。\n\n给定的量有：\n- 状态 $x$ 是一个标量。\n- 背景误差方差是一个标量 $B = \\sigma_{b}^{2}$。\n- 观测算子是 $H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，这是一个 $2 \\times 1$ 的矩阵。因此，其转置为 $H^{\\top} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$，是一个 $1 \\times 2$ 的矩阵。\n\n首先，我们计算出现在卡尔曼增益表达式中的项 $HBH^{\\top}$。\n$$\nHBH^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\sigma_{b}^{2} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\sigma_{b}^{2} \\begin{pmatrix} 1 \\cdot 1 & 1 \\cdot 1 \\\\ 1 \\cdot 1 & 1 \\cdot 1 \\end{pmatrix} = \\sigma_{b}^{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n$$\n这是一个 $2 \\times 2$ 的矩阵。\n\n可以使用迹的性质 $\\mathrm{tr}(AB) = \\mathrm{tr}(BA)$ 来计算 DFS。\n$$\n\\mathrm{DFS} = \\mathrm{tr}(HK) = \\mathrm{tr}(KH).\n$$\n由于状态是标量，$B$ 是一个 $1 \\times 1$ 矩阵。增益 $K$ 的维度为 $(1 \\times 1) \\times (1 \\times 2) \\times ((2 \\times 1) \\times (1 \\times 1) \\times (1 \\times 2) + (2 \\times 2))^{-1}$，这使得 $K$ 成为一个 $1 \\times 2$ 的矩阵。乘积 $KH$ 是一个 $(1 \\times 2) \\times (2 \\times 1) = 1 \\times 1$ 的矩阵。$1 \\times 1$ 矩阵的迹就是其唯一的元素。因此，$\\mathrm{DFS} = KH$。让我们来计算这个乘积：\n$$\n\\mathrm{DFS} = KH = \\left( B H^{\\top} (H B H^{\\top} + R)^{-1} \\right) H = B \\left( H^{\\top} (H B H^{\\top} + R)^{-1} H \\right).\n$$\n对于标量状态，这种形式在计算上很方便。\n\n我们现在将为两个指定的协方差矩阵计算 DFS。\n\n**1. 使用真实观测误差协方差 $R_{\\text{true}}$ 计算 DFS**\n\n真实协方差由 $R_{\\text{true}} = \\sigma_{o}^{2}\\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ 给出。我们定义待求逆的矩阵为 $M_{\\text{true}} = HBH^{\\top} + R_{\\text{true}}$。\n$$\nM_{\\text{true}} = \\sigma_{b}^{2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} + \\sigma_{o}^{2} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} = \\begin{pmatrix} \\sigma_{b}^{2} + \\sigma_{o}^{2} & \\sigma_{b}^{2} + \\rho\\sigma_{o}^{2} \\\\ \\sigma_{b}^{2} + \\rho\\sigma_{o}^{2} & \\sigma_{b}^{2} + \\sigma_{o}^{2} \\end{pmatrix}.\n$$\n为了求这个 $2 \\times 2$ 矩阵的逆，我们首先计算其行列式。\n\\begin{align*}\n\\det(M_{\\text{true}}) &= (\\sigma_{b}^{2} + \\sigma_{o}^{2})^{2} - (\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2})^{2} \\\\\n&= \\left( (\\sigma_{b}^{2} + \\sigma_{o}^{2}) - (\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) \\right) \\left( (\\sigma_{b}^{2} + \\sigma_{o}^{2}) + (\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) \\right) \\\\\n&= (\\sigma_{o}^{2} - \\rho\\sigma_{o}^{2}) (2\\sigma_{b}^{2} + \\sigma_{o}^{2} + \\rho\\sigma_{o}^{2}) \\\\\n&= \\sigma_{o}^{2}(1-\\rho) (2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)).\n\\end{align*}\n那么逆矩阵为：\n$$\nM_{\\text{true}}^{-1} = \\frac{1}{\\det(M_{\\text{true}})} \\begin{pmatrix} \\sigma_{b}^{2} + \\sigma_{o}^{2} & -(\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) \\\\ -(\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) & \\sigma_{b}^{2} + \\sigma_{o}^{2} \\end{pmatrix}.\n$$\n现在我们计算标量 $H^{\\top} M_{\\text{true}}^{-1} H$。\n\\begin{align*}\nH^{\\top} M_{\\text{true}}^{-1} H &= \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\left( \\frac{1}{\\det(M_{\\text{true}})} \\begin{pmatrix} \\sigma_{b}^{2} + \\sigma_{o}^{2} & -(\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) \\\\ -(\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) & \\sigma_{b}^{2} + \\sigma_{o}^{2} \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\\\\n&= \\frac{1}{\\det(M_{\\text{true}})} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} (\\sigma_{b}^{2} + \\sigma_{o}^{2}) - (\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) \\\\ -(\\sigma_{b}^{2} + \\rho\\sigma_{o}^{2}) + (\\sigma_{b}^{2} + \\sigma_{o}^{2}) \\end{pmatrix} \\\\\n&= \\frac{1}{\\det(M_{\\text{true}})} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\sigma_{o}^{2}(1-\\rho) \\\\ \\sigma_{o}^{2}(1-\\rho) \\end{pmatrix} \\\\\n&= \\frac{1}{\\det(M_{\\text{true}})} \\left( \\sigma_{o}^{2}(1-\\rho) + \\sigma_{o}^{2}(1-\\rho) \\right) = \\frac{2\\sigma_{o}^{2}(1-\\rho)}{\\det(M_{\\text{true}})}.\n\\end{align*}\n代入行列式的表达式：\n$$\nH^{\\top} (HBH^{\\top} + R_{\\text{true}})^{-1} H = \\frac{2\\sigma_{o}^{2}(1-\\rho)}{\\sigma_{o}^{2}(1-\\rho) (2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho))} = \\frac{2}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)}.\n$$\n最后，我们得到真实协方差下的 DFS：\n$$\n\\mathrm{DFS}(R_{\\text{true}}) = B \\left( H^{\\top} (HBH^{\\top} + R_{\\text{true}})^{-1} H \\right) = \\sigma_{b}^{2} \\cdot \\frac{2}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)} = \\frac{2\\sigma_{b}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)}.\n$$\n\n**2. 使用对角观测误差协方差 $R_{\\text{diag}}$ 计算 DFS**\n\n对角近似为 $R_{\\text{diag}} = \\sigma_{o}^{2} I_{2} = \\sigma_{o}^{2}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$。这对应于 $R_{\\text{true}}$ 在相关系数 $\\rho=0$ 时的特殊情况。因此，我们可以通过在 $\\mathrm{DFS}(R_{\\text{true}})$ 的结果中令 $\\rho=0$ 来得到 $\\mathrm{DFS}(R_{\\text{diag}})$ 的表达式。\n$$\n\\mathrm{DFS}(R_{\\text{diag}}) = \\frac{2\\sigma_{b}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+0)} = \\frac{2\\sigma_{b}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}}.\n$$\n\n**3. 计算乘性膨胀因子 $\\Phi$**\n\n膨胀因子 $\\Phi$ 定义为对角近似下的 DFS 与真实协方差下的 DFS 之比。\n$$\n\\Phi(\\rho,\\sigma_{b}^{2},\\sigma_{o}^{2}) = \\frac{\\mathrm{DFS}(R_{\\text{diag}})}{\\mathrm{DFS}(R_{\\text{true}})} = \\frac{\\frac{2\\sigma_{b}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}}}{\\frac{2\\sigma_{b}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)}}.\n$$\n分子中的公因子 $2\\sigma_{b}^{2}$ 被约掉，我们剩下：\n$$\n\\Phi(\\rho,\\sigma_{b}^{2},\\sigma_{o}^{2}) = \\frac{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}}.\n$$\n这个表达式可以被简化以突显相关性的影响：\n$$\n\\Phi = \\frac{(2\\sigma_{b}^{2} + \\sigma_{o}^{2}) + \\rho\\sigma_{o}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}} = 1 + \\frac{\\rho\\sigma_{o}^{2}}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}}.\n$$\n问题要求提供一个单一的简化解析表达式，分数形式最为合适。\n$$\n\\Phi(\\rho,\\sigma_{b}^{2},\\sigma_{o}^{2}) = \\frac{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}}.\n$$\n这个最终表达式量化了忽略观测误差相关性如何导致感知信号自由度的膨胀。如果 $\\rho > 0$，则 $\\Phi > 1$，表明高估了观测的信息含量。如果 $\\rho < 0$，则 $\\Phi < 1$，表明低估了观测的信息含量。",
            "answer": "$$ \\boxed{ \\frac{2\\sigma_{b}^{2} + \\sigma_{o}^{2}(1+\\rho)}{2\\sigma_{b}^{2} + \\sigma_{o}^{2}} } $$"
        },
        {
            "introduction": "在现实世界中，我们构建的背景误差和观测误差模型永远不可能是完美的。这就引出了一个至关重要的实践问题：给定一个存在偏差（misspecified）的模型，我们应如何选择其参数以获得最佳的分析结果？本计算练习将直面这一问题 ()。你将使用一个“真实”的背景协方差 $B_{\\text{true}}$ 来模拟系统，但使用一个具有错误相关长度尺度 $L$ 的模型 $B(L)$ 来进行分析。通过搜索能最小化实际分析误差的长度尺度 $L^{\\star}$，你将领会模型调优（model tuning）这一重要概念。这项实践弥合了理想化理论与业务化数据同化现实之间的鸿沟，在后者中，优化不完美的模型是一项核心任务。",
            "id": "3366397",
            "problem": "考虑一个一维空间状态，由向量 $x \\in \\mathbb{R}^{N}$ 表示。该状态是由时间白、空间均匀的高斯强迫驱动的线性平流扩散随机偏微分方程 (SPDE) 的稳态解。一个经过充分检验的结论是，稳态空间背景误差协方差具有指数核，因此对于网格点位置 $s_{1},\\dots,s_{N}$、背景方差 $\\sigma_{b}^{2} > 0$ 和真实相关长度 $L_{\\text{true}} > 0$，真实的背景误差协方差矩阵 $B_{\\text{true}} \\in \\mathbb{R}^{N \\times N}$ 可建模为 $[B_{\\text{true}}]_{ij} = \\sigma_{b}^{2} \\exp(-|s_{i} - s_{j}|/L_{\\text{true}})$。观测模型为 $y = H x + e$，其中 $H \\in \\mathbb{R}^{m \\times N}$ 选择 $m$ 个点观测，且 $e \\sim \\mathcal{N}(0, R)$，其中 $R = \\sigma_{o}^{2} I_{m}$（对于某个 $\\sigma_{o}^{2} > 0$）。分析员使用一个错误指定的背景误差协方差 $B(L)$ 进行线性高斯分析，该协方差具有相同的参数形式，但具有一个可调的相关长度 $L$（以及相同的方差 $\\sigma_{b}^{2}$）：$[B(L)]_{ij} = \\sigma_{b}^{2} \\exp(-|s_{i} - s_{j}|/L)$。\n\n您的任务是，当分析使用根据错误指定的 $B(L)$ 计算出的卡尔曼滤波器 (KF) 增益时，推导并实现预期的分析误差方差。从线性高斯估计理论的基础出发，使用 $B(L)$ 和已知的 $R$ 通过 KF 计算出的分析为 $x_{a}(L) = K(L) y$，其中 $K(L) = B(L) H^{\\top} \\left(H B(L) H^{\\top} + R\\right)^{-1}$。实际分析误差为 $x - x_{a}(L)$，其中 $x \\sim \\mathcal{N}(0, B_{\\text{true}})$ 和 $e \\sim \\mathcal{N}(0, R)$ 相互独立。推导预期分析误差协方差 $\\Sigma_{a}(L) = \\mathbb{E}\\left[(x - x_{a}(L))(x - x_{a}(L))^{\\top}\\right]$ 和每个状态分量的预期后验方差 $J(L) = \\frac{1}{N} \\mathrm{tr}\\left(\\Sigma_{a}(L)\\right)$ 的表达式。然后，对于下面列出的每个测试用例，在一组指定的候选值网格上，计算使 $J(L)$ 最小化的长度 $L$。\n\n您必须实现一个程序，该程序：\n- 分别使用指定的 $L_{\\text{true}}$ 和候选 $L$ 值，通过指数核构造 $B_{\\text{true}}$ 和 $B(L)$。\n- 将观测矩阵 $H$ 构建为与指定观测索引相对应的单位矩阵的行，并设置 $R = \\sigma_{o}^{2} I_{m}$。\n- 对每个候选 $L$ 计算 $K(L)$、$\\Sigma_{a}(L)$ 和 $J(L)$。\n- 选择使 $J(L)$ 最小化的 $L$，并返回最小化值 $L^{\\star}$ 和最小值 $J(L^{\\star})$。\n\n所有计算纯粹是数学上的；没有需要报告的物理单位。不涉及角度。最终输出必须是精确到六位小数的浮点数。\n\n测试套件：\n- 情况 A (理想情况，中等密度观测)：\n  - 网格：$N = 60$，$s_{i} = \\frac{i}{N-1}$ 对于 $i = 0, \\dots, N-1$。\n  - 真实协方差参数：$\\sigma_{b}^{2} = 1.0$，$L_{\\text{true}} = 0.12$。\n  - 观测：索引 $0, 6, 12, \\dots, 54$ (每 $6$ 个网格点一次)，因此 $m = 10$。\n  - 观测误差：$\\sigma_{o}^{2} = 0.04$ 且 $R = \\sigma_{o}^{2} I_{m}$。\n  - 候选相关长度：线性网格 $L \\in [0.02, 0.50]$，包含 $121$ 个均匀间隔的值 (包括端点)。\n- 情况 B (边缘情况，单点观测)：\n  - 网格：$N = 60$，$s_{i} = \\frac{i}{N-1}$。\n  - 真实协方差参数：$\\sigma_{b}^{2} = 1.0$，$L_{\\text{true}} = 0.15$。\n  - 观测：单个索引 $30$ (中心点)，因此 $m = 1$。\n  - 观测误差：$\\sigma_{o}^{2} = 0.01$ 且 $R = \\sigma_{o}^{2} I_{1}$。\n  - 候选相关长度：线性网格 $L \\in [0.01, 0.60]$，包含 $120$ 个均匀间隔的值 (包括端点)。\n- 情况 C (偏向边界，非常密集的观测与中等噪声)：\n  - 网格：$N = 40$，$s_{i} = \\frac{i}{N-1}$。\n  - 真实协方差参数：$\\sigma_{b}^{2} = 1.0$，$L_{\\text{true}} = 0.05$。\n  - 观测：索引 $0, 1, 2, \\dots, 39$ (所有网格点)，因此 $m = 40$。\n  - 观测误差：$\\sigma_{o}^{2} = 0.5$ 且 $R = \\sigma_{o}^{2} I_{m}$。\n  - 候选相关长度：线性网格 $L \\in [0.01, 0.30]$，包含 $100$ 个均匀间隔的值 (包括端点)。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素是对应测试用例的一个双元素列表 $[L^{\\star}, J(L^{\\star})]$。\n- 数值必须四舍五入到六位小数。\n- 例如，三个测试用例的输出必须如下所示：$[[0.123456,0.234567],[0.234567,0.345678],[0.345678,0.456789]]$。",
            "solution": "我们使用线性高斯估计来为一个一维稳态平流扩散场设定问题。这类场的稳态背景误差协方差可以通过平稳指数核很好地建模，这是一个与 Ornstein–Uhlenbeck 过程以及 Matérn 类协方差的高斯马尔可夫随机场表示相一致的经典结果。因此，对于网格点 $s_{1}, \\dots, s_{N}$ 和真实相关长度 $L_{\\text{true}}$，我们取 $[B_{\\text{true}}]_{ij} = \\sigma_{b}^{2} \\exp(-|s_{i} - s_{j}|/L_{\\text{true}})$。分析使用一个形式相同但具有可调参数 $L$ 的错误指定的协方差 $[B(L)]_{ij} = \\sigma_{b}^{2} \\exp(-|s_{i} - s_{j}|/L)$。\n\n观测模型为 $y = H x + e$，其中 $H \\in \\mathbb{R}^{m \\times N}$ 选择 $m$ 个点观测，且 $e \\sim \\mathcal{N}(0, R)$ 独立于 $x$。使用 $B(L)$ 作为背景误差协方差，通过卡尔曼滤波器 (KF) 计算出的分析为 $x_{a}(L) = K(L) y$，其中 KF 增益为\n$$\nK(L) = B(L) H^{\\top} \\left(H B(L) H^{\\top} + R\\right)^{-1}.\n$$\n这源于通过最小化后验预期二次损失推导出的标准线性高斯贝叶斯估计器。尽管该估计器对于 $B(L)$ 是线性最优的，但当 $B(L) \\neq B_{\\text{true}}$ 时，它通常是不匹配的。\n\n定义 $A(L) = I_{N} - K(L) H$。实际分析误差为 $x - x_{a}(L) = A(L) x - K(L) e$。对 $x$ 和 $e$ 的联合分布（独立，零均值）取期望，得到预期分析误差协方差\n$$\n\\Sigma_{a}(L) = \\mathbb{E}\\left[(x - x_{a}(L))(x - x_{a}(L))^{\\top}\\right]\n= A(L) B_{\\text{true}} A(L)^{\\top} + K(L) R K(L)^{\\top}.\n$$\n此表达式是通过协方差的双线性和 $x$ 与 $e$ 的独立性得到的，使用了 $\\mathbb{E}[x x^{\\top}] = B_{\\text{true}}$、$\\mathbb{E}[e e^{\\top}] = R$ 以及交叉项 $\\mathbb{E}[x e^{\\top}] = 0$。\n\n每个状态分量的预期后验方差则定义为\n$$\nJ(L) = \\frac{1}{N} \\mathrm{tr}\\left(\\Sigma_{a}(L)\\right),\n$$\n它是所有状态分量上后验误差方variance的平均值，并且是一个对网格排列不变的性能标量度量。\n\n$B(L)$ 中错误指定的相关长度的影响：\n- 如果 $L$ 相对于 $L_{\\text{true}}$ 过小，那么 $B(L)$ 近似为对角矩阵，导致增益 $K(L)$ 是局域化的。分析倾向于使来自点观测的信息传播不足，在远离观测位置的地方留下较大的后验方差。\n- 如果 $L$ 相对于 $L_{\\text{true}}$ 过大，那么 $B(L)$ 会施加强的长程相干性，增益 $K(L)$ 会过度传播观测影响，可能导致观测噪声被广泛注入，并降低对真实场的局部保真度，这同样会增加 $J(L)$。\n- 因此，存在一种权衡，会产生一个最优的 $L^{\\star}$，它在信息传播和噪声注入之间取得平衡，以最小化 $J(L)$。\n\n计算 $J(L)$ 和寻找 $L^{\\star}$ 的算法设计：\n1. 构建网格坐标 $s_{i} = \\frac{i}{N-1}$ 对于 $i = 0, \\dots, N-1$。\n2. 构造 $B_{\\text{true}} \\in \\mathbb{R}^{N \\times N}$，其元素为 $[B_{\\text{true}}]_{ij} = \\sigma_{b}^{2} \\exp(-|s_{i} - s_{j}|/L_{\\text{true}})$。\n3. 构建观测矩阵 $H \\in \\mathbb{R}^{m \\times N}$，方法是选择指定的索引；$H$ 由与观测网格点对应的单位矩阵的行组成。设置 $R = \\sigma_{o}^{2} I_{m}$。\n4. 对于测试用例指定范围内的每个候选 $L$：\n   - 构造 $B(L)$，其元素为 $[B(L)]_{ij} = \\sigma_{b}^{2} \\exp(-|s_{i} - s_{j}|/L)$。\n   - 计算 $S(L) = H B(L) H^{\\top} + R$ 并求解以 $S(L)$ 为系数的线性系统（避免显式求逆）：\n     $$\n     K(L) = B(L) H^{\\top} \\left(S(L)\\right)^{-1}.\n     $$\n     计算上，通过线性求解计算 $K(L)^{\\top} = \\left(S(L)\\right)^{-1} \\left(H B(L)\\right)$。\n   - 形成 $A(L) = I_{N} - K(L) H$。\n   - 计算 $\\Sigma_{a}(L) = A(L) B_{\\text{true}} A(L)^{\\top} + K(L) R K(L)^{\\top}$ 和 $J(L) = \\frac{1}{N} \\mathrm{tr}\\left(\\Sigma_{a}(L)\\right)$。\n5. 在候选集上选择最小化 $J(L)$ 的 $L^{\\star}$，并报告 $[L^{\\star}, J(L^{\\star})]$。\n\n测试套件的数值规格确保覆盖以下情况：\n- 具有中等密度观测的典型情况 (情况 A)。\n- 单个观测导致信息高度局域化的边缘情况 (情况 B)。\n- 观测噪声和模型平滑度相互作用的、具有非常密集观测的偏向边界情况 (情况 C)。\n\n所要求的程序需实现上述计算，并打印单行输出，该行包含一个含三个双元素列表的列表，每个对应一个测试用例，所有数字都四舍五入到六位小数，并严格按照 $[[L^{\\star},J(L^{\\star})],\\dots]$ 的格式。",
            "answer": "```python\nimport numpy as np\n\ndef exponential_covariance_matrix(s, sigma_b2, L):\n    # Build covariance matrix with entries sigma_b2 * exp(-|si - sj| / L)\n    N = s.size\n    # Use broadcasting for efficiency\n    dists = np.abs(s.reshape(-1, 1) - s.reshape(1, -1))\n    return sigma_b2 * np.exp(-dists / L)\n\ndef build_observation_matrix(N, obs_indices):\n    m = len(obs_indices)\n    H = np.zeros((m, N), dtype=float)\n    for r, c in enumerate(obs_indices):\n        H[r, c] = 1.0\n    return H\n\ndef compute_J_for_L(B_true, H, R, B_assumed):\n    N = B_true.shape[0]\n    I_N = np.eye(N, dtype=float)\n    # Compute S = H B_assumed H^T + R\n    HB = H @ B_assumed  # shape (m, N)\n    S = HB @ H.T + R    # shape (m, m)\n    # Solve for K^T: S * X = HB  => X = S^{-1} HB\n    # K^T has shape (m, N); K has shape (N, m)\n    # Use solve for numerical stability\n    K_T = np.linalg.solve(S, HB)\n    K = K_T.T\n    # A = I - K H\n    A = I_N - K @ H\n    # Sigma_a = A B_true A^T + K R K^T\n    Sigma_a = A @ B_true @ A.T + K @ R @ K.T\n    # J(L) = (1/N) * trace(Sigma_a)\n    J = float(np.trace(Sigma_a) / N)\n    return J\n\ndef grid_search_optimal_L(N, s, obs_indices, sigma_b2, L_true, sigma_o2, L_min, L_max, n_candidates):\n    # Precompute true covariance and observation structures\n    B_true = exponential_covariance_matrix(s, sigma_b2, L_true)\n    H = build_observation_matrix(N, obs_indices)\n    m = len(obs_indices)\n    R = sigma_o2 * np.eye(m, dtype=float)\n\n    # Candidate L values (inclusive grid)\n    L_values = np.linspace(L_min, L_max, n_candidates)\n    best_L = None\n    best_J = np.inf\n\n    for L in L_values:\n        B_assumed = exponential_covariance_matrix(s, sigma_b2, L)\n        J = compute_J_for_L(B_true, H, R, B_assumed)\n        if J  best_J:\n            best_J = J\n            best_L = float(L)\n\n    return best_L, float(best_J)\n\ndef format_number(x):\n    # Format float with exactly six decimal places\n    return f\"{x:.6f}\"\n\ndef format_result_list(results):\n    # results is a list of [L_star, J_star]; return a string like [[a,b],[c,d],...]\n    inner = []\n    for pair in results:\n        a, b = pair\n        inner.append(f\"[{format_number(a)},{format_number(b)}]\")\n    return f\"[{','.join(inner)}]\"\n\ndef solve():\n    results = []\n\n    # Case A\n    N_A = 60\n    s_A = np.linspace(0.0, 1.0, N_A)\n    sigma_b2_A = 1.0\n    L_true_A = 0.12\n    obs_indices_A = list(range(0, N_A, 6))  # every 6th grid point\n    sigma_o2_A = 0.04\n    L_min_A, L_max_A, n_candidates_A = 0.02, 0.50, 121\n    L_star_A, J_star_A = grid_search_optimal_L(\n        N_A, s_A, obs_indices_A, sigma_b2_A, L_true_A, sigma_o2_A, L_min_A, L_max_A, n_candidates_A\n    )\n    results.append([L_star_A, J_star_A])\n\n    # Case B\n    N_B = 60\n    s_B = np.linspace(0.0, 1.0, N_B)\n    sigma_b2_B = 1.0\n    L_true_B = 0.15\n    obs_indices_B = [30]  # single central observation\n    sigma_o2_B = 0.01\n    L_min_B, L_max_B, n_candidates_B = 0.01, 0.60, 120\n    L_star_B, J_star_B = grid_search_optimal_L(\n        N_B, s_B, obs_indices_B, sigma_b2_B, L_true_B, sigma_o2_B, L_min_B, L_max_B, n_candidates_B\n    )\n    results.append([L_star_B, J_star_B])\n\n    # Case C\n    N_C = 40\n    s_C = np.linspace(0.0, 1.0, N_C)\n    sigma_b2_C = 1.0\n    L_true_C = 0.05\n    obs_indices_C = list(range(N_C))  # all grid points observed\n    sigma_o2_C = 0.5\n    L_min_C, L_max_C, n_candidates_C = 0.01, 0.30, 100\n    L_star_C, J_star_C = grid_search_optimal_L(\n        N_C, s_C, obs_indices_C, sigma_b2_C, L_true_C, sigma_o2_C, L_min_C, L_max_C, n_candidates_C\n    )\n    results.append([L_star_C, J_star_C])\n\n    print(format_result_list(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}