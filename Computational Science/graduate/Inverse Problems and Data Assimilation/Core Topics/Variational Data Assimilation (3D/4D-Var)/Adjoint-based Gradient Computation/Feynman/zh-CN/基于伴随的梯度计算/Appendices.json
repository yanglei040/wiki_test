{
    "hands_on_practices": [
        {
            "introduction": "神经微分方程（Neural ODEs）是将深度学习与动力系统相结合的前沿工具，为了有效地训练这些模型，我们需要计算损失函数相对于网络参数的梯度。此练习将指导您为常微分方程系统实现连续伴随方法，这是计算此类梯度的内存高效策略。通过这个基础实践，您将掌握前向积分、后向伴随求解和梯度累积的核心流程，并学会使用有限差分法来验证您的梯度计算的正确性。",
            "id": "3333095",
            "problem": "考虑一个简单的双物种细胞系统，其中状态向量 $x(t) \\in \\mathbb{R}^2$ 表示信使核糖核酸 (mRNA) 和蛋白质的无量纲化浓度。该系统根据常微分方程 $dx/dt = f_{\\theta}(x,t)$ 演化，其中 $f_{\\theta}$ 由一个小型前馈神经网络参数化，旨在近似非线性反应动力学。时间单位为秒。该神经网络定义如下：隐藏层预激活为 $z = W_1 x + b_1$，隐藏层激活为 $h = \\tanh(z)$，输出为\n$$\nf_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x,\n$$\n其中 $W_1 \\in \\mathbb{R}^{3 \\times 2}$，$b_1 \\in \\mathbb{R}^{3}$，$W_2 \\in \\mathbb{R}^{2 \\times 3}$，$b_2 \\in \\mathbb{R}^{2}$，$k_d \\in \\mathbb{R}^{2}$，且 $\\odot$ 表示逐元素乘法。损失定义在时间 $T$ 的终端状态上，由下式给出\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T; \\theta) - x_{\\mathrm{target}} \\|_2^2.\n$$\n初始条件为 $x(0) = x_0$。目标是使用伴随方法计算梯度 $d\\mathcal{L}/d\\theta$，并将其与梯度的有限差分近似进行比较，以评估其正确性以及求解器离散化对偏差的影响。\n\n从微积分的链式法则和常微分方程 (ODE) 的伴随灵敏度定义出发，推导出实现与上述连续时间神经 ODE 模型一致的、基于伴随的梯度计算 $d\\mathcal{L}/d\\theta$ 所需的关系。您的程序必须：\n- 使用固定步长的显式四阶龙格－库塔方法对正向动力学进行积分。\n- 使用一阶显式方法，沿保存的正向轨迹评估雅可比矩阵 $ \\partial f_{\\theta} / \\partial x $，在时间上向后实现伴随积分。\n- 通过瞬时灵敏度 $a(t)^\\top \\, \\partial f_{\\theta} / \\partial \\theta$ 的时间积分来累积参数梯度，其中 $a(t)$ 表示伴随状态，并通过对参数扰动重新积分正向动力学计算出的中心有限差分来验证结果。\n\n您的实现必须完全自包含，不调用任何外部数据。正向模型和所有参数的数值规定如下：\n- 模型参数 $\\theta$：\n  - $W_1 = \\begin{bmatrix} 0.8  -0.5 \\\\ 0.3  0.9 \\\\ -0.7  0.2 \\end{bmatrix}$，\n  - $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，\n  - $W_2 = \\begin{bmatrix} 0.5  -0.3  0.1 \\\\ -0.4  0.6  -0.2 \\end{bmatrix}$，\n  - $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\end{bmatrix}$，\n  - $k_d = \\begin{bmatrix} 0.3 \\\\ 0.5 \\end{bmatrix}$。\n- 初始状态和目标：\n  - $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.2 \\end{bmatrix}$，\n  - $x_{\\mathrm{target}} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$。\n- 最终时间：$T = 2.0$ 秒。\n\n您必须使用参数缩放因子 $s$ 来探索神经网络分量接近线性的区域，方法是将权重和偏置缩放为 $\\{ W_1, b_1, W_2, b_2 \\} \\mapsto s \\cdot \\{ W_1, b_1, W_2, b_2 \\}$，同时保持 $k_d$ 不变。\n\n定义以下测试用例套件，每个用例由一个元组 $(N, \\epsilon, \\tau, s)$ 指定，其中 $N$ 是正向积分的龙格－库塔步数（恒定步长 $\\Delta t = T/N$），$\\epsilon$ 是有限差分步长，$\\tau$ 是梯度一致性的绝对容差，$s$ 是缩放因子：\n- 用例 1：$(N=\\ 500,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 2 \\times 10^{-2},\\ s=\\ 1.0)$，\n- 用例 2：$(N=\\ 50,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 2 \\times 10^{-2},\\ s=\\ 1.0)$，\n- 用例 3：$(N=\\ 20,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 5 \\times 10^{-2},\\ s=\\ 1.0)$，\n- 用例 4：$(N=\\ 60,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 1 \\times 10^{-2},\\ s=\\ 0.05)$。\n\n对于每个用例，计算：\n- 使用连续伴随法和伴随方程的后向显式积分计算的基于伴随的梯度 $d\\mathcal{L}/d\\theta$。\n- 对 $\\theta$ 的每个标量分量应用扰动 $\\pm \\epsilon$（在扰动前应用缩放 $s$）计算的 $d\\mathcal{L}/d\\theta$ 的中心有限差分近似。\n- 两个梯度向量之间的最大绝对分量级差异。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目是一个布尔值，指示该用例的最大绝对差异是否严格小于规定的容差 $\\tau$。例如，输出格式必须完全像 $[b_1,b_2,b_3,b_4]$，其中每个 $b_i$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$，没有空格。除 $T$ 的单位为秒外，所有数值答案都必须是无单位的标量。本问题不使用角度。所有结果都是基于上述数据确定的，不需要任何用户输入。",
            "solution": "该问题要求计算一个损失函数相对于一个神经普通微分方程 (ODE) 模型参数的梯度。该梯度将使用连续伴随灵敏度方法计算，并与有限差分近似进行验证。\n\n### 基于原理的设计：伴随灵敏度分析\n\n该系统由一个带有初始条件 $x(0) = x_0$ 的 ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ 描述。目标是计算一个依赖于最终时间 $T$ 状态的损失函数 $\\mathcal{L}(\\theta) = g(x(T))$ 的梯度。根据链式法则，该梯度为：\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial g}{\\partial x(T)} \\frac{dx(T)}{d\\theta}\n$$\n项 $\\frac{dx(T)}{d\\theta}$ 表示最终状态对参数变化的灵敏度。其直接计算需要对每个参数积分一个灵敏度方程，这在计算上可能非常昂贵。伴随方法提供了一种更有效的替代方案，尤其是在参数数量众多的情况下。\n\n伴随方法引入一个伴随状态向量 $a(t) \\in \\mathbb{R}^n$，它是以下终端值问题的解：\n$$\n\\frac{da}{dt} = - \\left( \\frac{\\partial f_{\\theta}}{\\partial x} \\right)^\\top a(t) \\quad \\text{其中} \\quad a(T) = \\left( \\frac{\\partial g}{\\partial x(T)} \\right)^\\top\n$$\n损失函数相对于参数 $\\theta$ 的梯度则由以下积分给出：\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\int_0^T a(t)^\\top \\frac{\\partial f_{\\theta}(x(t), t)}{\\partial \\theta} dt\n$$\n此公式需要对伴随 ODE 进行一次时间反向积分，并重用通过对原始 ODE 进行正向积分获得的状态轨迹 $x(t)$。\n\n### 在具体神经 ODE 模型上的应用\n\n该问题为动力学、参数和损失函数提供了具体形式。\n\n1.  **系统动力学**：状态为 $x(t) \\in \\mathbb{R}^2$。动力学函数为 $f_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x$，其中 $h = \\tanh(z)$ 且 $z = W_1 x + b_1$。参数为 $\\theta = \\{W_1, b_1, W_2, b_2, k_d\\}$。\n\n2.  **损失函数和伴随终端条件**：损失为 $\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T) - x_{\\mathrm{target}} \\|_2^2$。损失相对于最终状态的梯度为 $\\frac{\\partial \\mathcal{L}}{\\partial x(T)} = (x(T) - x_{\\mathrm{target}})^\\top$。因此，伴随状态 $a(t) \\in \\mathbb{R}^2$ 的终端条件为：\n    $$\n    a(T) = x(T) - x_{\\mathrm{target}}\n    $$\n\n3.  **动力学的雅可比矩阵 ($\\partial f_{\\theta} / \\partial x$)**：为了定义伴随 ODE，我们首先需要 $f_{\\theta}$ 关于状态 $x$ 的雅可比矩阵。使用链式法则：\n    $$\n    \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial x} - \\frac{\\partial (k_d \\odot x)}{\\partial x}\n    $$\n    这些导数是 $\\frac{\\partial z}{\\partial x} = W_1$，$\\frac{\\partial(k_d \\odot x)}{\\partial x} = \\text{diag}(k_d)$，以及 $\\frac{\\partial h}{\\partial z} = \\text{diag}(1 - \\tanh^2(z)) = \\text{diag}(1 - h^2)$，其中平方是逐元素的。完整的雅可比矩阵是一个 $2 \\times 2$ 矩阵：\n    $$\n    J(x, \\theta) = \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\text{diag}(1 - h^2) W_1 - \\text{diag}(k_d)\n    $$\n    因此，伴随 ODE 为 $\\frac{da}{dt} = -J(x(t), \\theta)^\\top a(t)$。\n\n4.  **参数梯度 ($\\partial f_{\\theta} / \\partial \\theta$)**：我们需要 $f_{\\theta}$ 相对于每个参数块的偏导数。\n    -   **关于 $W_2$ 的梯度**：$\\frac{\\partial f_{\\theta}}{\\partial (W_2)_{ij}} = e_i h_j^\\top$，其中 $e_i$ 是第 $i$ 个标准基向量。矩阵 $W_2$ 梯度的被积函数是 $a h^\\top$。\n        $$ \\frac{d\\mathcal{L}}{dW_2} = \\int_0^T a(t) h(t)^\\top dt $$\n    -   **关于 $b_2$ 的梯度**：$\\frac{\\partial f_{\\theta}}{\\partial b_2} = I$，单位矩阵。向量 $b_2$ 梯度的被积函数是 $a$。\n        $$ \\frac{d\\mathcal{L}}{db_2} = \\int_0^T a(t) dt $$\n    -   **关于 $W_1$ 的梯度**：使用链式法则，$\\frac{\\partial f_{\\theta}}{\\partial W_1} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial W_1}$。矩阵 $W_1$ 梯度的被积函数是 $\\text{diag}(1-h^2) (W_2^\\top a) x^\\top$。\n        $$ \\frac{d\\mathcal{L}}{dW_1} = \\int_0^T \\text{diag}(1 - h(t)^2) (W_2^\\top a(t)) x(t)^\\top dt $$\n    -   **关于 $b_1$ 的梯度**：类似地，向量 $b_1$ 的被积函数是 $\\text{diag}(1-h^2) (W_2^\\top a)$。\n        $$ \\frac{d\\mathcal{L}}{db_1} = \\int_0^T \\text{diag}(1 - h(t)^2) W_2^\\top a(t) dt $$\n    -   **关于 $k_d$ 的梯度**：$\\frac{\\partial f_{\\theta}}{\\partial k_d} = -\\text{diag}(x)$。向量 $k_d$ 的被积函数是 $-(a \\odot x)$。\n        $$ \\frac{d\\mathcal{L}}{dk_d} = \\int_0^T -(a(t) \\odot x(t)) dt $$\n\n### 算法设计与数值实现\n\n该算法分为三个主要阶段：一个用于求解状态轨迹的正向传递，一个用于求解伴随轨迹并累积参数梯度的反向传递，以及一个使用有限差分进行验证的步骤。\n\n1.  **正向传递**：使用固定步长 $\\Delta t = T/N$ 的显式四阶龙格－库塔 (RK4) 方法，从 $t=0$ 到 $t=T$ 对状态 ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ 进行积分。在时间点 $\\{t_0, t_1, \\dots, t_N\\}$ 的状态 $\\{x_0, x_1, \\dots, x_N\\}$ 被存储起来，以供反向传递使用。\n\n2.  **反向传递（伴随和梯度）**：伴随 ODE 和参数梯度积分被同时求解。\n    -   伴随 ODE 从 $t=T$ 到 $t=0$ 进行反向积分。问题指定了“一阶显式方法”。这实现为时间反向系统的显式欧拉法。给定在时间 $t_i$ 的 $a_i$，前一个时间步 $t_{i-1}$ 的状态近似为：\n        $$ a_{i-1} = a_i - \\Delta t \\left( \\frac{da}{dt} \\right)\\bigg|_{t_i} = a_i - \\Delta t \\left( -J(x_i, \\theta)^\\top a_i \\right) = a_i + \\Delta t J(x_i, \\theta)^\\top a_i $$\n    -   参数梯度积分使用与后向欧拉积分一致的简单求积法则来近似。使用右黎曼和，从 $i=N$ 到 $i=1$ 累积每个时间步 $t_i$ 的梯度贡献：\n        $$ \\frac{d\\mathcal{L}}{d\\theta} \\approx \\sum_{i=1}^N \\left( a_i^\\top \\frac{\\partial f_{\\theta}}{\\partial \\theta}\\bigg|_{x_i} \\right) \\Delta t $$\n    -   该过程从 $a_N = x_N - x_{\\text{target}}$ 开始，向后迭代，在每一步更新伴随状态并累加到总梯度中。\n\n3.  **通过有限差分进行验证**：将计算出的伴随梯度与中心有限差分近似进行比较。对于每个标量参数 $\\theta_j$，其梯度分量近似为：\n    $$\n    \\frac{d\\mathcal{L}}{d\\theta_j} \\approx \\frac{\\mathcal{L}(\\theta + \\epsilon e_j) - \\mathcal{L}(\\theta - \\epsilon e_j)}{2\\epsilon}\n    $$\n    其中 $e_j$ 是一个标准基向量，$\\epsilon$ 是一个小的扰动。这需要对每个参数重新积分正向 ODE 两次。然后将伴随梯度向量和有限差分梯度向量之间的最大绝对差与给定的容差 $\\tau$ 进行比较。这种比较评估了在特定数值离散化下连续伴随方法的准确性。由于连续伴随方法的“优化后离散”性质与 RK4 求解器上有限差分检查所隐式定义的“离散后优化”性质之间的差异，预计会出现偏差。这些偏差预计会随着积分步长 $\\Delta t$ 趋近于 $0$ 而减小。具有不同 $N$ 和非线性缩放因子 $s$ 的测试用例旨在探索这种行为。",
            "answer": "```python\nimport numpy as np\n\nclass ModelParams:\n    \"\"\"\n    A helper class to manage model parameters, including packing to a flat vector\n    and unpacking from it, and applying scaling.\n    \"\"\"\n    def __init__(self, W1, b1, W2, b2, kd):\n        self.W1 = np.array(W1, dtype=np.float64)\n        self.b1 = np.array(b1, dtype=np.float64)\n        self.W2 = np.array(W2, dtype=np.float64)\n        self.b2 = np.array(b2, dtype=np.float64)\n        self.kd = np.array(kd, dtype=np.float64)\n        \n        self.shapes = [self.W1.shape, self.b1.shape, self.W2.shape, self.b2.shape, self.kd.shape]\n        self.sizes = [p.size for p in [self.W1, self.b1, self.W2, self.b2, self.kd]]\n        self.total_size = sum(self.sizes)\n\n    def pack(self):\n        \"\"\"Packs all parameters into a single flat numpy array.\"\"\"\n        return np.concatenate([p.flatten() for p in [self.W1, self.b1, self.W2, self.b2, self.kd]])\n\n    @classmethod\n    def from_flat(cls, theta_flat):\n        \"\"\"Creates a ModelParams object from a flat numpy array.\"\"\"\n        theta_flat = np.array(theta_flat, dtype=np.float64)\n        # Fixed shapes from the problem description\n        shapes = [(3, 2), (3,), (2, 3), (2,), (2,)]\n        sizes = [np.prod(s) for s in shapes]\n        \n        ptr = 0\n        unpacked_params = []\n        for i, shape in enumerate(shapes):\n            size = sizes[i]\n            param_flat = theta_flat[ptr:ptr+size]\n            unpacked_params.append(param_flat.reshape(shape))\n            ptr += size\n        \n        return cls(*unpacked_params)\n\n    def scale(self, s):\n        \"\"\"Applies scaling factor s to network weights and biases.\"\"\"\n        return ModelParams(self.W1 * s, self.b1 * s, self.W2 * s, self.b2 * s, self.kd)\n\ndef ode_func(x, params: ModelParams):\n    \"\"\"The ODE function dx/dt = f(x, t, theta).\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    return params.W2 @ h + params.b2 - params.kd * x\n\ndef run_rk4(x0, T, N, params: ModelParams):\n    \"\"\"Integrates the ODE using a 4th-order Runge-Kutta method.\"\"\"\n    dt = T / N\n    x_traj = np.zeros((N + 1, x0.shape[0]), dtype=np.float64)\n    x_traj[0] = x0\n    x = x0.copy()\n    \n    for i in range(N):\n        k1 = ode_func(x, params)\n        k2 = ode_func(x + 0.5 * dt * k1, params)\n        k3 = ode_func(x + 0.5 * dt * k2, params)\n        k4 = ode_func(x + dt * k3, params)\n        x += (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n        x_traj[i+1] = x\n        \n    return x_traj\n\ndef compute_loss(x_final, xtarget):\n    \"\"\"Computes the loss function L.\"\"\"\n    return 0.5 * np.sum((x_final - xtarget)**2)\n\ndef compute_dfdx(x, params: ModelParams):\n    \"\"\"Computes the Jacobian of the ODE function w.r.t. x.\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    diag_1_minus_h2 = np.diag(1 - h**2)\n    return params.W2 @ diag_1_minus_h2 @ params.W1 - np.diag(params.kd)\n\ndef compute_adjoint_gradient(x0, T, N, params: ModelParams, xtarget):\n    \"\"\"Computes the gradient dL/dtheta using the adjoint method.\"\"\"\n    dt = T / N\n    x_traj = run_rk4(x0, T, N, params)\n    \n    grad_W1 = np.zeros_like(params.W1)\n    grad_b1 = np.zeros_like(params.b1)\n    grad_W2 = np.zeros_like(params.W2)\n    grad_b2 = np.zeros_like(params.b2)\n    grad_kd = np.zeros_like(params.kd)\n    \n    a = x_traj[N] - xtarget\n    \n    for i in range(N, 0, -1):\n        x = x_traj[i]\n        \n        z = params.W1 @ x + params.b1\n        h = np.tanh(z)\n        \n        # Accumulate gradient contributions from time t_i\n        grad_b2 += a * dt\n        grad_W2 += np.outer(a, h) * dt\n        grad_kd += -(a * x) * dt\n        \n        v = (1 - h**2) * (params.W2.T @ a)\n        grad_b1 += v * dt\n        grad_W1 += np.outer(v, x) * dt\n        \n        # Update adjoint state from t_i to t_{i-1}\n        jacobian = compute_dfdx(x, params)\n        a = a + dt * (jacobian.T @ a)\n        \n    packed_grads = np.concatenate([\n        grad_W1.flatten(), grad_b1.flatten(), grad_W2.flatten(),\n        grad_b2.flatten(), grad_kd.flatten()\n    ])\n    return packed_grads\n\ndef compute_fd_gradient(x0, T, N, base_params: ModelParams, xtarget, epsilon):\n    \"\"\"Computes the gradient dL/dtheta using central finite differences.\"\"\"\n    base_theta = base_params.pack()\n    grad = np.zeros_like(base_theta)\n    \n    def loss_func(theta_flat):\n        params_pert = ModelParams.from_flat(theta_flat)\n        x_traj = run_rk4(x0, T, N, params_pert)\n        return compute_loss(x_traj[-1], xtarget)\n\n    for i in range(len(base_theta)):\n        theta_plus = base_theta.copy()\n        theta_plus[i] += epsilon\n        \n        theta_minus = base_theta.copy()\n        theta_minus[i] -= epsilon\n        \n        loss_plus = loss_func(theta_plus)\n        loss_minus = loss_func(theta_minus)\n        \n        grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n        \n    return grad\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    W1_base = np.array([[0.8, -0.5], [0.3, 0.9], [-0.7, 0.2]])\n    b1_base = np.array([0.1, -0.2, 0.05])\n    W2_base = np.array([[0.5, -0.3, 0.1], [-0.4, 0.6, -0.2]])\n    b2_base = np.array([0.0, 0.05])\n    kd_base = np.array([0.3, 0.5])\n    base_params_obj = ModelParams(W1_base, b1_base, W2_base, b2_base, kd_base)\n\n    x0 = np.array([0.5, 0.2])\n    xtarget = np.array([0.1, -0.1])\n    T = 2.0\n\n    test_cases = [\n        (500, 1e-6, 2e-2, 1.0),\n        (50, 1e-6, 2e-2, 1.0),\n        (20, 1e-6, 5e-2, 1.0),\n        (60, 1e-6, 1e-2, 0.05),\n    ]\n\n    results = []\n    \n    for N, epsilon, tau, s in test_cases:\n        scaled_params = base_params_obj.scale(s)\n        \n        grad_adj = compute_adjoint_gradient(x0, T, N, scaled_params, xtarget)\n        grad_fd = compute_fd_gradient(x0, T, N, scaled_params, xtarget, epsilon)\n\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        \n        results.append(max_abs_diff  tau)\n        \n    bool_str_results = [str(b) for b in results]\n    print(f\"[{','.join(bool_str_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多现实世界的物理模型，例如水文学中的干湿演替过程，都包含由“if-then”逻辑或 `max` 函数表示的非光滑动态特性。这些不连续点对标准的、基于连续方法的伴随技术构成了挑战。本练习将引导您为这类问题开发一个离散伴随模型，确保梯度的计算精确地反映离散前向模型中的非光滑事件处理，这是处理复杂、事件驱动模拟的关键高级技术。",
            "id": "3364071",
            "problem": "考虑一个最小单元的半离散浅水质量平衡模型，其中干湿过程通过一个非光滑投影来表示。令 $h(t)$ 表示基床高程参数 $z$ 以上的水深，并令上游自由表面强迫为 $H_{u}(t)$。在时间 $t$ 流入单元的通量建模为 $F_{\\text{in}}(t) = \\alpha \\max(H_{u}(t) - z - h(t), 0)$，当单元为湿润状态时，流出量建模为 $F_{\\text{out}}(t) = \\beta h(t)$。局部质量平衡常微分方程为\n$$\n\\frac{dh}{dt} = \\alpha \\max(H_{u}(t) - z - h(t), 0) - \\beta h(t).\n$$\n为确保水深的非负性，前向时间步进在每一步应用一个投影。使用时间步长为 $\\Delta t$ 的显式前向欧拉积分器，并定义 $t_n = n \\Delta t$（对于 $n=0,1,\\dots,N$）。对于 $h_n \\approx h(t_n)$，执行以下更新：\n$$\nh_{n+1}^{\\star} = h_{n} + \\Delta t \\left(\\alpha \\max(H_{u}(t_n) - z - h_n, 0) - \\beta h_n \\right), \\quad h_{n+1} = \\max(h_{n+1}^{\\star}, 0).\n$$\n定义投影的激活指示函数为\n$$\na_{n+1} = \\begin{cases}\n1  \\text{if } h_{n+1}^{\\star}  0,\\\\\n0  \\text{if } h_{n+1}^{\\star} \\le 0,\n\\end{cases}\n$$\n以及入流激活函数为\n$$\nm_n = \\begin{cases}\n1  \\text{if } H_{u}(t_n) - z - h_n  0,\\\\\n0  \\text{if } H_{u}(t_n) - z - h_n \\le 0.\n\\end{cases}\n$$\n假设上游自由表面由下式给出\n$$\nH_{u}(t) = H_0 + A \\sin(\\omega t),\n$$\n其中 $H_0$、$A$ 和 $\\omega$ 是指定的常数。令观测模型为直接水深采样 $y_n = h(t_n)$，并令目标观测值对所有 $n$ 恒为零，$y_n^{\\text{obs}} = 0$。考虑二次目标函数（对 $z$ 带有 Tikhonov 二次先验）\n$$\nJ(z) = \\frac{1}{2}\\sum_{n=0}^{N} \\left(h_n - y_n^{\\text{obs}}\\right)^2 + \\frac{\\gamma}{2}\\left(z - z_{\\text{prior}}\\right)^2.\n$$\n您的任务是通过伴随方法计算梯度 $\\frac{dJ}{dz}$，该方法需精确匹配离散时间步进和投影（干湿）事件处理。具体来说，通过应用带有投影的广义导数的链式法则，为单步映射 $h_n \\mapsto h_{n+1} = \\max\\!\\left(h_n + \\Delta t f_n(h_n, z), 0\\right)$ 实现离散伴随，其中\n$$\nf_n(h_n, z) = \\alpha \\max(H_u(t_n) - z - h_n, 0) - \\beta h_n,\n$$\n使用事件处理策略\n$$\n\\frac{\\partial h_{n+1}}{\\partial h_n} = a_{n+1}\\left(1 + \\Delta t \\frac{\\partial f_n}{\\partial h_n}\\right), \\quad \\frac{\\partial h_{n+1}}{\\partial z} = a_{n+1}\\left(\\Delta t \\frac{\\partial f_n}{\\partial z}\\right),\n$$\n以及\n$$\n\\frac{\\partial f_n}{\\partial h_n} = -\\alpha m_n - \\beta, \\quad \\frac{\\partial f_n}{\\partial z} = -\\alpha m_n.\n$$\n在不可微的扭折点处，使用上面 $a_{n+1}$ 和 $m_n$ 的定义，即当相应的激活前的值为非正时赋值为 $0$。这构成了与离散动力学中的干湿转换一致的伴随传播事件处理策略。\n\n通过与目标函数对齐的逆时递归来定义离散伴随序列 $\\lambda_n$。令\n$$\n\\lambda_N = \\frac{\\partial}{\\partial h_N}\\left(\\frac{1}{2}(h_N - y_N^{\\text{obs}})^2\\right) = h_N - y_N^{\\text{obs}},\n$$\n并且，对于 $n = N-1, \\dots, 0$，\n$$\n\\lambda_n = \\frac{\\partial}{\\partial h_n}\\left(\\frac{1}{2}(h_n - y_n^{\\text{obs}})^2\\right) + \\left(\\frac{\\partial h_{n+1}}{\\partial h_n}\\right) \\lambda_{n+1} = (h_n - y_n^{\\text{obs}}) + a_{n+1}\\left(1 + \\Delta t \\frac{\\partial f_n}{\\partial h_n}\\right)\\lambda_{n+1}.\n$$\n则梯度为\n$$\n\\frac{dJ}{dz} = \\gamma (z - z_{\\text{prior}}) + \\sum_{n=0}^{N-1} \\lambda_{n+1}\\, a_{n+1}\\left(\\Delta t \\frac{\\partial f_n}{\\partial z}\\right).\n$$\n实现一个程序，该程序：\n- 使用指定的显式格式和投影来积分前向模型。\n- 计算 $J(z)$。\n- 使用上面定义的带有事件处理策略的离散伴随方法计算 $\\frac{dJ}{dz}$。\n- 计算中心有限差分近似\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\text{FD}} \\approx \\frac{J(z+\\epsilon) - J(z-\\epsilon)}{2\\epsilon},\n$$\n对于一个足够小的 $\\epsilon$，对扰动后的 $z$ 值使用相同的前向求解器和投影。\n- 对于下面的每个测试案例，返回绝对误差\n$$\ne = \\left|\\left.\\frac{dJ}{dz}\\right|_{\\text{adj}} - \\left.\\frac{dJ}{dz}\\right|_{\\text{FD}}\\right|.\n$$\n\n所有量均为无量纲；不需要物理单位。$\\sin(\\cdot)$ 中使用的角度以弧度为单位。任何地方都不要使用百分号；所有比率都应表示为小数。\n\n测试套件（每个案例定义 $\\alpha$、$\\beta$、$H_0$、$A$、$\\omega$、$\\Delta t$、$N$、$h_0$、$\\gamma$、$z_{\\text{prior}}$ 和评估点 $z$）：\n- 案例 1（始终干燥）：$\\alpha = 3.0$，$\\beta = 0.5$，$H_0 = 0.2$，$A = 0.0$，$\\omega = 1.0$，$\\Delta t = 0.01$，$N = 500$，$h_0 = 0.0$，$\\gamma = 0.1$，$z_{\\text{prior}} = 0.0$，$z = 0.6$。\n- 案例 2（持续淹没）：$\\alpha = 2.0$，$\\beta = 0.2$，$H_0 = 1.0$，$A = 0.3$，$\\omega = 1.5$，$\\Delta t = 0.01$，$N = 1000$，$h_0 = 0.0$，$\\gamma = 0.1$，$z_{\\text{prior}} = 0.0$，$z = 0.2$。\n- 案例 3（间歇性干湿）：$\\alpha = 1.5$，$\\beta = 0.1$，$H_0 = 0.55$，$A = 0.35$，$\\omega = 2.0$，$\\Delta t = 0.01$，$N = 1200$，$h_0 = 0.0$，$\\gamma = 0.1$，$z_{\\text{prior}} = 0.0$，$z = 0.6$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[r1,r2,r3]\"），其中每个条目是相应测试案例的绝对误差 $e$，表示为浮点数。",
            "solution": "用户的要求是使用离散伴随方法计算目标函数关于模型参数的梯度，并通过与有限差分近似进行比较来验证该梯度。该模型是一个简化的单单元水文系统，由于干湿和入流条件（由 `max` 函数表示），其具有非光滑动力学特性。\n\n该问题在计算上定义明确，在科学上合理，且内部一致。它提出了数据同化和反演问题领域的标准任务：通过梯度检查来验证伴随模型。为处理不可微性，所提供的关于前向模型、目标函数和离散伴随的方程是完整的，并且为一种特定的基于次梯度的伴随方法正确地制定了公式。因此，该问题是有效的，我们可以继续进行求解。\n\n求解过程包括三个主要阶段：\n$1$. 实现前向模型，以模拟在给定基床高程 $z$ 的情况下水深 $h_n$ 随时间的变化。该模型还计算目标函数 $J(z)$。\n$2$. 实现离散伴随模型，该模型涉及逆时递归以计算伴随变量 $\\lambda_n$，然后通过求和来组装梯度 $\\frac{dJ}{dz}$。\n$3$. 使用中心有限差分法计算数值梯度，作为验证伴随推导梯度的基准。\n\n### 1. 前向模型和目标函数\n\n系统的状态是在离散时间步 $t_n = n \\Delta t$ 时的水深 $h_n$。状态的演化由带有非负投影的显式前向欧拉格式控制：\n$$\nh_{n+1}^{\\star} = h_{n} + \\Delta t \\left(\\alpha \\max(H_{u}(t_n) - z - h_n, 0) - \\beta h_n \\right)\n$$\n$$\nh_{n+1} = \\max(h_{n+1}^{\\star}, 0)\n$$\n其中 $H_{u}(t_n) = H_0 + A \\sin(\\omega t_n)$ 是外部强迫。模拟从给定的初始条件 $h_0$ 开始，运行 $N$ 步以生成状态轨迹 $\\{h_n\\}_{n=0}^{N}$。\n\n在此前向传递过程中，我们还必须计算并存储两个指示变量的历史记录，这对伴随计算至关重要。这些变量表示每个时间步上 `max` 函数的广义导数。\n入流激活指示函数 $m_n$ 是：\n$$\nm_n = \\begin{cases}\n1  \\text{if } H_{u}(t_n) - z - h_n  0 \\\\\n0  \\text{if } H_{u}(t_n) - z - h_n \\le 0\n\\end{cases}\n$$\n投影激活指示函数 $a_{n+1}$ 是：\n$$\na_{n+1} = \\begin{cases}\n1  \\text{if } h_{n+1}^{\\star}  0 \\\\\n0  \\text{if } h_{n+1}^{\\star} \\le 0\n\\end{cases}\n$$\n这些选择对应于在不可微点选择一个特定的次梯度。\n\n一旦计算出完整的轨迹 $\\{h_n\\}_{n=0}^{N}$，就计算目标函数 $J(z)$。鉴于目标观测值为 $y_n^{\\text{obs}} = 0$，目标函数为：\n$$\nJ(z) = \\frac{1}{2}\\sum_{n=0}^{N} h_n^2 + \\frac{\\gamma}{2}\\left(z - z_{\\text{prior}}\\right)^2\n$$\n我们将此函数表示为 `forward_model`，它将成为伴随和有限差分计算的基础构件。\n\n### 2. 离散伴随梯度计算\n\n伴随方法通过反转前向模型计算图中的信息流，提供了一种高效的方法来计算标量目标函数关于大量参数的梯度。其推导依赖于链式法则，该法则系统地应用于离散时间模拟的每一步。\n\n该过程包括两个传递：\na. **前向传递：** 对给定的参数 $z$ 运行上述前向模型。存储状态 $h_n$ 和指示变量 $m_n$、$a_{n+1}$ 的完整时间历史。\n\nb. **后向传递：** 计算伴随变量（或协态变量）$\\lambda_n$，它表示最终目标函数 $J$ 对状态 $h_n$ 扰动的敏感度。计算从时间 $n=N$ 到 $n=0$ 逆向进行。\n\n在时间 $t_N$ 的伴随变量的终端条件是目标函数关于最终状态 $h_N$ 的导数：\n$$\n\\lambda_N = \\frac{\\partial J}{\\partial h_N} = h_N - y_N^{\\text{obs}} = h_N\n$$\n对于 $n = N-1, \\dots, 0$ 的递归关系由链式法则导出，将敏感度从步骤 $n+1$ 传播回步骤 $n$：\n$$\n\\lambda_n = \\frac{\\partial J}{\\partial h_n} + \\frac{\\partial h_{n+1}}{\\partial h_n} \\lambda_{n+1}\n$$\n这里，$\\frac{\\partial J}{\\partial h_n} = h_n - y_n^{\\text{obs}} = h_n$ 是在时间 $n$ 对目标函数的直接贡献。项 $\\frac{\\partial h_{n+1}}{\\partial h_n}$ 是单步映射 $h_n \\mapsto h_{n+1}$ 的雅可比矩阵。使用所提供的事件处理策略：\n$$\n\\frac{\\partial h_{n+1}}{\\partial h_n} = a_{n+1}\\left(1 + \\Delta t \\frac{\\partial f_n}{\\partial h_n}\\right) = a_{n+1}\\left(1 + \\Delta t (-\\alpha m_n - \\beta)\\right)\n$$\n因此，后向递归变为：\n$$\n\\lambda_n = h_n + a_{n+1}\\left(1 - \\Delta t (\\alpha m_n + \\beta)\\right)\\lambda_{n+1}\n$$\n\nc. **梯度组装：** 一旦计算出 $n=N, \\dots, 1$ 的伴随变量 $\\lambda_n$，就可以组装总梯度 $\\frac{dJ}{dz}$。它由正则化项的导数加上每个时间步贡献的总和组成，这些贡献通过伴随变量进行传播：\n$$\n\\frac{dJ}{dz} = \\frac{\\partial J}{\\partial z} + \\sum_{n=0}^{N-1} \\lambda_{n+1} \\frac{\\partial h_{n+1}}{\\partial z}\n$$\n直接偏导数 $\\frac{\\partial J}{\\partial z}$ 就是正则化项的导数，即 $\\gamma(z - z_{\\text{prior}})$。状态转移的敏感度 $\\frac{\\partial h_{n+1}}{\\partial z}$ 如下给出：\n$$\n\\frac{\\partial h_{n+1}}{\\partial z} = a_{n+1}\\left(\\Delta t \\frac{\\partial f_n}{\\partial z}\\right) = a_{n+1}\\left(\\Delta t (-\\alpha m_n)\\right)\n$$\n代入这些表达式，得到伴随梯度的最终公式：\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\text{adj}} = \\gamma (z - z_{\\text{prior}}) - \\Delta t \\alpha \\sum_{n=0}^{N-1} \\lambda_{n+1}\\, a_{n+1}\\, m_n\n$$\n在实现中，我们将使用存储的 $m_n$ 和 $a_{n+1}$ 的历史（分别作为 `m[n]` 和 `a[n]`）以及计算出的伴随变量 $\\lambda_{n+1}$ (`lam[n+1]`) 来评估此和。\n\n### 3. 有限差分梯度与验证\n\n为了验证伴随实现的正确性，我们使用中心有限差分公式计算梯度，该公式为可微函数提供了可靠的数值近似。对于一个小的扰动 $\\epsilon$：\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\text{FD}} \\approx \\frac{J(z+\\epsilon) - J(z-\\epsilon)}{2\\epsilon}\n$$\n这需要两次独立执行完整的前向模型：一次使用参数 $z+\\epsilon$，另一次使用 $z-\\epsilon$。选择一个足够小的 $\\epsilon$（例如 $10^{-7}$）以平衡截断误差和浮点舍入误差。\n\n最后一步是计算伴随推导的梯度与有限差分近似之间的绝对误差：\n$$\ne = \\left|\\left.\\frac{dJ}{dz}\\right|_{\\text{adj}} - \\left.\\frac{dJ}{dz}\\right|_{\\text{FD}}\\right|\n$$\n一个小的误差为伴随模型及其实现的正确性提供了高度的置信度。此过程将对提供的三个测试案例中的每一个重复进行。",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(params, z_eval):\n    \"\"\"\n    Runs the forward simulation and computes the objective function J.\n\n    Args:\n        params (dict): A dictionary containing all model parameters.\n        z_eval (float): The value of the bed elevation parameter z to use.\n\n    Returns:\n        tuple: A tuple containing:\n            - h_hist (np.ndarray): History of water depth h_n of size N+1.\n            - m_hist (np.ndarray): History of inflow indicator m_n of size N.\n            - a_hist (np.ndarray): History of projection indicator a_{n+1} of size N.\n            - J (float): The value of the objective function.\n    \"\"\"\n    alpha = params['alpha']\n    beta = params['beta']\n    H0 = params['H0']\n    A = params['A']\n    omega = params['omega']\n    dt = params['dt']\n    N = params['N']\n    h0 = params['h0']\n    gamma = params['gamma']\n    z_prior = params['z_prior']\n\n    h_hist = np.zeros(N + 1)\n    m_hist = np.zeros(N)\n    a_hist = np.zeros(N)\n\n    h_hist[0] = h0\n\n    for n in range(N):\n        t_n = n * dt\n        H_u_n = H0 + A * np.sin(omega * t_n)\n        \n        # Inflow activation\n        inflow_arg = H_u_n - z_eval - h_hist[n]\n        m_hist[n] = 1.0 if inflow_arg > 0.0 else 0.0\n        \n        # Flux calculation\n        f_n = alpha * max(inflow_arg, 0.0) - beta * h_hist[n]\n        \n        # Euler step\n        h_star = h_hist[n] + dt * f_n\n        \n        # Projection activation\n        a_hist[n] = 1.0 if h_star > 0.0 else 0.0\n        \n        # State update with projection\n        h_hist[n + 1] = max(h_star, 0.0)\n\n    # Objective function\n    # Note: y_n^obs = 0 for all n\n    J_obs = 0.5 * np.sum(h_hist**2)\n    J_reg = 0.5 * gamma * (z_eval - z_prior)**2\n    J = J_obs + J_reg\n    \n    return h_hist, m_hist, a_hist, J\n\ndef compute_adjoint_gradient(params, z_eval, h_hist, m_hist, a_hist):\n    \"\"\"\n    Computes the gradient dJ/dz using the discrete adjoint method.\n    \"\"\"\n    alpha = params['alpha']\n    beta = params['beta']\n    dt = params['dt']\n    N = params['N']\n    gamma = params['gamma']\n    z_prior = params['z_prior']\n\n    lam = np.zeros(N + 1)\n    \n    # Terminal condition for adjoint variable (y_N^obs = 0)\n    lam[N] = h_hist[N]\n    \n    # Backward recursion for adjoint variables\n    for n in range(N - 1, -1, -1):\n        # Propagated part\n        df_dhn = -alpha * m_hist[n] - beta\n        dhnp1_dhn = a_hist[n] * (1.0 + dt * df_dhn)\n        \n        # Direct part from objective function (y_n^obs = 0)\n        dJ_dhn = h_hist[n]\n        \n        lam[n] = dJ_dhn + dhnp1_dhn * lam[n + 1]\n\n    # Assemble the gradient\n    grad_sum = 0.0\n    for n in range(N):\n        df_dzn = -alpha * m_hist[n]\n        dhnp1_dz = a_hist[n] * (dt * df_dzn)\n        grad_sum += lam[n + 1] * dhnp1_dz\n\n    grad_adj = gamma * (z_eval - z_prior) + grad_sum\n    \n    return grad_adj\n\ndef compute_fd_gradient(params, z_eval, epsilon=1e-7):\n    \"\"\"\n    Computes the gradient dJ/dz using the central finite-difference method.\n    \"\"\"\n    _, _, _, J_plus = run_simulation(params, z_eval + epsilon)\n    _, _, _, J_minus = run_simulation(params, z_eval - epsilon)\n    \n    grad_fd = (J_plus - J_minus) / (2.0 * epsilon)\n    return grad_fd\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute errors.\n    \"\"\"\n    test_cases = [\n        # Case 1 (always dry)\n        {\n            'alpha': 3.0, 'beta': 0.5, 'H0': 0.2, 'A': 0.0, 'omega': 1.0, \n            'dt': 0.01, 'N': 500, 'h0': 0.0, 'gamma': 0.1, \n            'z_prior': 0.0, 'z_eval': 0.6\n        },\n        # Case 2 (sustained inundation)\n        {\n            'alpha': 2.0, 'beta': 0.2, 'H0': 1.0, 'A': 0.3, 'omega': 1.5,\n            'dt': 0.01, 'N': 1000, 'h0': 0.0, 'gamma': 0.1, \n            'z_prior': 0.0, 'z_eval': 0.2\n        },\n        # Case 3 (intermittent wetting and drying)\n        {\n            'alpha': 1.5, 'beta': 0.1, 'H0': 0.55, 'A': 0.35, 'omega': 2.0,\n            'dt': 0.01, 'N': 1200, 'h0': 0.0, 'gamma': 0.1, \n            'z_prior': 0.0, 'z_eval': 0.6\n        },\n    ]\n\n    results = []\n    \n    epsilon_fd = 1e-7\n\n    for case_params in test_cases:\n        z = case_params['z_eval']\n        \n        # 1. Run forward model to get states and indicators\n        h_hist, m_hist, a_hist, _ = run_simulation(case_params, z)\n        \n        # 2. Compute adjoint gradient\n        grad_adj = compute_adjoint_gradient(case_params, z, h_hist, m_hist, a_hist)\n        \n        # 3. Compute finite-difference gradient for verification\n        grad_fd = compute_fd_gradient(case_params, z, epsilon=epsilon_fd)\n        \n        # 4. Calculate absolute error\n        error = abs(grad_adj - grad_fd)\n        results.append(error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "将伴随方法应用于空间分布系统，如由偏微分方程（PDE）描述的系统，是解决地球物理勘探和医学成像等领域逆问题的核心。本练习将您的技能从常微分方程扩展到椭圆型偏微分方程，并引入了总变差（TV）正则化项，这是一种在恢复分段常数或具有清晰边界的解时非常强大的技术。您将学习如何将 PDE 求解器与伴随梯度计算相结合，并分析正则化平滑参数对梯度准确性和最终反演结果的影响。",
            "id": "3364106",
            "problem": "考虑以下离散逆问题，用于恢复二维单位正方形上的未知标量电导率场。设物理域为单位正方形，所有边界均为零狄利克雷边界条件，并考虑椭圆偏微分方程\n$$\n-\\nabla\\cdot\\left(\\sigma(x,y)\\,\\nabla u(x,y)\\right) = s(x,y) \\quad \\text{在内部}, \\quad u(x,y)=0 \\quad \\text{在边界上},\n$$\n其中 $u$ 是状态变量，$\\sigma$ 是待推断的正电导率，$s$ 是已知源项。观测算子选择 $u$ 的一组固定的内部网格点值。\n\n您需要在一个无量纲的环境中工作，并在一个均匀的笛卡尔网格上离散化该问题，该网格具有 $N_x=N_y=32$ 个单元中心，两个方向上的网格尺寸均为 $h=1/(N_x)$。使用对称两点通量近似法，并对内表面采用调和面平均法来施加零狄利克雷边界条件。设未知离散电导率 $\\sigma\\in\\mathbb{R}^{N_xN_y}$ 以单元为中心且严格为正。离散正演算子 $A(\\sigma)$ 是由这种通量一致的离散化方法产生的稀疏刚度矩阵，离散正演状态 $u(\\sigma)$ 求解\n$$\nA(\\sigma)\\,u = b,\n$$\n其中 $b$ 是代表源项的离散载荷向量。\n\n使用一个选择算子 $P$ 来定义数据失配项，该算子在 16 个均匀分布的内部网格索引处提取 $u$ 的内部值（避免最外层单元以保持在域内）。定义目标泛函\n$$\nJ(\\sigma) = \\frac{1}{2}\\,\\|P\\,u(\\sigma) - d\\|_2^2 + \\alpha \\,\\mathrm{TV}_\\varepsilon(\\sigma),\n$$\n其中 $\\alpha0$ 是正则化权重，$d$ 是使用相同正演模型从已知 $\\sigma_{\\mathrm{true}}$ 生成的无噪声合成数据，而 $\\mathrm{TV}_\\varepsilon$ 是在网格上定义的光滑各向同性全变分，通过每个坐标方向上的前向有限差分以及带有光滑参数 $\\varepsilon0$ 的各向同性欧几里得范数来定义：\n$$\n\\mathrm{TV}_\\varepsilon(\\sigma) = \\sum_{i,j} \\sqrt{\\left(D_x \\sigma\\right)_{i,j}^2 + \\left(D_y \\sigma\\right)_{i,j}^2 + \\varepsilon^2}.\n$$\n此处，$(D_x \\sigma)_{i,j}$ 和 $(D_y \\sigma)_{i,j}$ 分别表示 $x$ 和 $y$ 方向上的前向差分，对于跨越边界的差分采用齐次诺伊曼扩展。源项 $s(x,y)$ 是一个归一化的高斯函数，中心位于 $(x_c,y_c)=(0.3,0.7)$，标准差为 $0.07$，其离散对应项 $b$ 通过在单元中心进行逐点求值得到。\n\n任务：\n- 从带有偏微分方程约束的拉格朗日量和标准一阶最优性原理出发，推导连续伴随方程以及数据失配项相对于 $\\sigma$ 的梯度的连续表达式，用状态 $u$ 和伴随状态 $\\lambda$ 表示。然后，在指定的通量一致离散化和面调和平均条件下，为数据失配部分推导一个一致的离散伴随系统和相应的离散梯度。\n- 从 $\\mathrm{TV}_\\varepsilon(\\sigma)$ 的定义出发，使用离散散度作为齐次诺伊曼条件下前向差分梯度的负伴随，推导光滑各向同性全变分的离散梯度。\n- 实现一个求解器，该求解器：\n  1. 对任何容许的 $\\sigma$ 组装 $A(\\sigma)$ 并求解正演问题。\n  2. 使用观测算子构建离散伴随问题的右端项并求解伴随问题。\n  3. 使用推导出的公式计算 $J(\\sigma)$ 及其梯度 $\\nabla J(\\sigma)$。\n  4. 在初始猜测值 $\\sigma_0$ 处，针对一个随机方向 $\\delta\\sigma$，对完整目标函数（数据失配项加光滑全变分）执行中心有限差分方向导数测试。使用的步长为 $h_{\\mathrm{fd}}=10^{-4}$，并进行缩放以确保 $\\sigma_0 \\pm h_{\\mathrm{fd}}\\,\\delta\\sigma$ 保持严格为正。报告由下式定义的相对梯度误差\n     $$\n     E_{\\mathrm{rel}} = \\frac{\\left|\\langle \\nabla J(\\sigma_0), \\delta\\sigma\\rangle - \\frac{J(\\sigma_0+h_{\\mathrm{fd}}\\delta\\sigma)-J(\\sigma_0-h_{\\mathrm{fd}}\\delta\\sigma)}{2h_{\\mathrm{fd}}}\\right|}{\\max\\left\\{1, \\left|\\frac{J(\\sigma_0+h_{\\mathrm{fd}}\\delta\\sigma)-J(\\sigma_0-h_{\\mathrm{fd}}\\delta\\sigma)}{2h_{\\mathrm{fd}}}\\right|, \\left|\\langle \\nabla J(\\sigma_0), \\delta\\sigma\\rangle\\right|\\right\\}}.\n     $$\n  5. 运行一个固定预算的梯度下降反演，采用 Armijo 回溯线搜索，迭代 10 次以最小化 $J(\\sigma)$。每次更新后，通过投影到 $[\\sigma_{\\min},\\infty)$（其中 $\\sigma_{\\min}=0.2$）来强制实施 $\\sigma$ 的正性。使用初始步长 $10^{-1}$，Armijo 参数 $c=10^{-4}$，以及回溯因子 $\\beta=1/2$。\n  6. 在 10 次迭代后，计算恢复的 $\\sigma$ 的边缘锐度度量。方法是在真实模型中圆形夹杂物边界周围一个两单元宽的带上，对离散梯度的大小 $\\sqrt{\\left(D_x \\sigma\\right)_{i,j}^2 + \\left(D_y \\sigma\\right)_{i,j}^2}$ 进行平均。真实模型为：在以 $(0.5,0.5)$ 为中心、半径为 $0.25$ 的圆盘内 $\\sigma_{\\mathrm{true}}=2$，在圆盘外 $\\sigma_{\\mathrm{true}}=1$。该带定义为 $\\{(x_i,y_j): | \\sqrt{(x_i-0.5)^2+(y_j-0.5)^2} - 0.25| \\le 2h\\}$。\n\n测试套件和要求的输出：\n- 使用 $\\alpha=10^{-3}$，初始猜测值 $\\sigma_0 \\equiv 1$，以及通过求解上述定义的 $\\sigma_{\\mathrm{true}}$ 的正演问题生成的无噪声数据 $d$。\n- 评估以下四种情况：\n  1. 当 $\\varepsilon=10^{-1}$ 时的梯度检查相对误差 $E_{\\mathrm{rel}}$。\n  2. 当 $\\varepsilon=10^{-3}$ 时的梯度检查相对误差 $E_{\\mathrm{rel}}$。\n  3. 当 $\\varepsilon=10^{-1}$ 时，反演 10 次迭代后的边缘锐度度量。\n  4. 当 $\\varepsilon=10^{-3}$ 时，反演 10 次迭代后的边缘锐度度量。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述顺序列出结果，例如：$[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是一个浮点数。\n\n所有量都是无量纲的，应如此处理。不使用角度（如有）。不使用百分比。最终输出必须严格遵循指定的单行格式。",
            "solution": "用户想要解决一个离散逆问题。\n\n**1. 问题验证**\n\n*   **步骤 1：提取已知条件**\n    *   **PDE：** $-\\nabla\\cdot\\left(\\sigma(x,y)\\,\\nabla u(x,y)\\right) = s(x,y)$ 在单位正方形上。\n    *   **边界条件：** $u(x,y)=0$ 在所有边界上。\n    *   **离散化：** 均匀笛卡尔网格，$N_x=N_y=32$ 个单元中心。网格尺寸 $h=1/N_x$。对称两点通量近似，对内表面采用调和面平均法。\n    *   **未知量：** 以单元为中心的电导率 $\\sigma\\in\\mathbb{R}^{N_xN_y}$，$\\sigma0$。\n    *   **正演模型：** $A(\\sigma)\\,u = b$，其中 $A(\\sigma)$ 是稀疏刚度矩阵，$b$ 是离散载荷向量。\n    *   **目标泛函：** $J(\\sigma) = \\frac{1}{2}\\,\\|P\\,u(\\sigma) - d\\|_2^2 + \\alpha \\,\\mathrm{TV}_\\varepsilon(\\sigma)$。\n    *   **观测算子 $P$：** 在 16 个均匀分布的内部网格索引处选择 $u$ 的值。\n    *   **数据 $d$：** 无噪声合成数据，$d = P u(\\sigma_{\\mathrm{true}})$。\n    *   **正则化：** 光滑各向同性全变分，$\\mathrm{TV}_\\varepsilon(\\sigma) = \\sum_{i,j} \\sqrt{\\left(D_x \\sigma\\right)_{i,j}^2 + \\left(D_y \\sigma\\right)_{i,j}^2 + \\varepsilon^2}$。\n    *   **差分算子：** $D_x, D_y$ 是带齐次诺伊曼扩展的前向有限差分。\n    *   **源项 $s(x,y)$：** 归一化高斯函数，中心在 $(x_c,y_c)=(0.3,0.7)$，标准差为 $0.07$。离散版本 $b$ 通过在单元中心逐点求值得到。\n    *   **真实电导率 $\\sigma_{\\mathrm{true}}$：** 在以 $(0.5,0.5)$ 为中心、半径为 $0.25$ 的圆盘内值为 $2$，外部为 $1$。\n    *   **参数：** $\\alpha=10^{-3}$，初始猜测 $\\sigma_0 \\equiv 1$，$\\sigma_{\\min}=0.2$。\n    *   **梯度检查：** 中心有限差分测试，步长为 $h_{\\mathrm{fd}}=10^{-4}$。\n    *   **反演算法：** 梯度下降法迭代 10 次，采用 Armijo 回溯线搜索（初始步长 $10^{-1}$，$c=10^{-4}$，$\\beta=1/2$）。\n    *   **输出度量：** 基于真实不连续性周围带内的平均梯度幅值的边缘锐度度量。\n    *   **测试案例：** 基于 $\\varepsilon=10^{-1}$ 和 $\\varepsilon=10^{-3}$ 计算四个值：两个梯度检查误差和两个锐度度量。\n\n*   **步骤 2：使用提取的已知条件进行验证**\n    *   该问题具有**科学依据**，基于标准的椭圆偏微分方程理论和逆问题的吉洪诺夫正则化方法。\n    *   该问题在正演模型是标准的可解边界值问题，且优化问题结构化以便于数值求解的意义上是**适定的**，这是处理不适定逆问题的标准实践。\n    *   该问题是**客观的**，并用精确的数学公式描述。\n    *   在边界处如何构建对称离散化矩阵 $A(\\sigma)$ 方面存在一个微小的模糊之处。“对内表面”这一短语暗示边界表面的处理方式不同。一种保持狄利克雷边界矩阵对称性的标准有限体积法涉及到与边界相邻的面的特定传输系数。这种解释与问题的整体结构一致，并在解题中予以采纳。源项 $b$ 的离散化也根据标准有限体积实践取为 $b_i = s(x_i, y_i) h^2$。这些是对微小模糊之处的合理解决。\n    *   该问题不违反任何无效性标准。它是一个定义明确、非平凡且在计算科学与工程领域中的标准问题。\n\n*   **步骤 3：结论与行动**\n    *   问题被判定为**有效**。将提供一个解决方案。\n\n**2. 理论推导**\n\n**2.1. 连续伴随状态法**\n\n设问题域为 $\\Omega=(0,1)^2$。目标泛函为 $J(\\sigma) = \\frac{1}{2} \\int_\\Omega (P u - d)^2 d\\mathbf{x} + \\alpha \\mathrm{TV}_\\varepsilon(\\sigma)$，其中 $P$ 是一个观测算子。状态 $u$ 受 PDE 约束 $-\\nabla\\cdot(\\sigma\\nabla u) = s$ 在 $\\Omega$ 内，以及 $u=0$ 在 $\\partial\\Omega$ 上。\n\n我们通过使用伴随状态（或拉格朗日乘子）$\\lambda$ 将目标泛函与 PDE 约束增广，从而构成拉格朗日量 $\\mathcal{L}$：\n$$\n\\mathcal{L}(u, \\sigma, \\lambda) = J(u, \\sigma) - \\int_\\Omega \\lambda \\left[ \\nabla\\cdot(\\sigma\\nabla u) + s \\right] d\\mathbf{x}\n$$\n一阶最优性条件要求 $\\mathcal{L}$ 相对于每个变量的弗雷歇导数为零。我们首先关注数据失配部分。设 $J_m(u) = \\frac{1}{2}\\int_\\Omega (Pu-d)^2 d\\mathbf{x}$。\n\n关于状态 $u$ 在方向 $\\delta u$（其中 $\\delta u = 0$ 在 $\\partial\\Omega$上）上的导数为：\n$$\nD_u\\mathcal{L}[\\delta u] = \\int_\\Omega (Pu-d) P\\delta u \\,d\\mathbf{x} - \\int_\\Omega \\lambda \\nabla\\cdot(\\sigma\\nabla \\delta u) \\,d\\mathbf{x} = 0\n$$\n对第二项使用两次格林第一恒等式，并选择伴随边界条件 $\\lambda=0$ 在 $\\partial\\Omega$ 上：\n$$\n\\int_\\Omega \\lambda \\nabla\\cdot(\\sigma\\nabla \\delta u) \\,d\\mathbf{x} = \\int_\\Omega \\delta u \\nabla\\cdot(\\sigma\\nabla \\lambda) \\,d\\mathbf{x}\n$$\n因此，条件变为：\n$$\n\\int_\\Omega \\left[ P^T(Pu-d) - \\nabla\\cdot(\\sigma\\nabla \\lambda) \\right] \\delta u \\,d\\mathbf{x} = 0\n$$\n因为这必须对所有容许的 $\\delta u$ 成立，我们得到**伴随方程**：\n$$\n-\\nabla\\cdot(\\sigma\\nabla \\lambda) = P^T(Pu-d) \\quad \\text{在 } \\Omega \\text{ 内}, \\quad \\lambda = 0 \\quad \\text{在 } \\partial\\Omega \\text{ 上}\n$$\n这里，$P^T$ 是观测算子的伴随算子。如果 $P$ 提取点值，那么 $P^T$ 在这些点注入加权的狄拉克δ函数。\n\n接下来，我们求关于控制变量 $\\sigma$ 在方向 $\\delta\\sigma$ 上的导数。数据失配贡献的导数为：\n$$\nD_\\sigma\\mathcal{L}[\\delta\\sigma] = - \\int_\\Omega \\lambda \\nabla\\cdot(\\delta\\sigma\\nabla u) \\,d\\mathbf{x}\n$$\n使用格林第一恒等式：\n$$\nD_\\sigma\\mathcal{L}[\\delta\\sigma] = \\int_\\Omega \\nabla\\lambda \\cdot (\\delta\\sigma \\nabla u) \\,d\\mathbf{x} = \\int_\\Omega (\\nabla u \\cdot \\nabla \\lambda) \\delta\\sigma \\,d\\mathbf{x}\n$$\n由此，我们确定数据失配项关于 $\\sigma$ 的梯度：\n$$\n\\nabla_\\sigma J_m(\\sigma) = \\nabla u \\cdot \\nabla \\lambda\n$$\n\n**2.2. 数据失配的离散伴随和梯度**\n\n令离散状态 $u \\in\\mathbb{R}^N$（其中 $N=N_x N_y$）求解线性系统 $A(\\sigma)u=b$。离散目标函数为 $J(\\sigma) = \\frac{1}{2}\\|Pu-d\\|_2^2 + \\alpha \\mathrm{TV}_\\varepsilon(\\sigma)$。数据失配项的离散拉格朗日量为：\n$$\n\\mathcal{L}_d(u, \\sigma, \\lambda) = \\frac{1}{2}(Pu-d)^T(Pu-d) + \\lambda^T(A(\\sigma)u-b)\n$$\n将关于 $u$ 的梯度设为零，得到离散伴随方程：\n$$\n\\frac{\\partial \\mathcal{L}_d}{\\partial u} = P^T(Pu-d) + A(\\sigma)^T\\lambda = 0 \\implies A(\\sigma)^T\\lambda = -P^T(Pu-d)\n$$\n所选择的离散化方法（有限体积法，带调和平均和特定的边界处理）产生一个对称刚度矩阵 $A(\\sigma)$，因此 $A(\\sigma)^T=A(\\sigma)$。伴随状态 $\\lambda$ 通过求解以下方程得到：\n$$\nA(\\sigma)\\lambda = -P^T(Pu-d)\n$$\n$\\mathcal{L}_d$ 关于单个电导率参数 $\\sigma_k$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}_d}{\\partial \\sigma_k} = \\lambda^T \\frac{\\partial A(\\sigma)}{\\partial \\sigma_k} u\n$$\n让我们更具体地说明。与算子相关的能量是 $\\frac{1}{2}u^T A(\\sigma) u$。对于我们的离散化，这可以表示为面上所有项的和：\n$$\n\\frac{1}{2} u^T A(\\sigma) u = \\frac{1}{2} \\sum_{\\langle k,m \\rangle} T_{km}(\\sigma_k, \\sigma_m)(u_k-u_m)^2 + \\frac{1}{2} \\sum_{k \\in \\partial\\Omega_d} \\sum_{f \\in \\partial B_k} T_{kf}(\\sigma_k) u_k^2\n$$\n其中 $\\langle k,m \\rangle$ 表示单元 $k$ 和 $m$ 之间的内表面，$\\partial\\Omega_d$ 是离散边界单元的集合，$\\partial B_k$ 是单元 $k$ 的边界表面。传输系数对于内表面是 $T_{km} = \\frac{2\\sigma_k\\sigma_m}{\\sigma_k+\\sigma_m}$，对于边界表面是 $T_{kf}=2\\sigma_k$。\n项 $\\lambda^T A(\\sigma) u$ 具有相似的结构：\n$$\n\\lambda^T A(\\sigma) u = \\sum_{\\langle k,m \\rangle} T_{km}(\\lambda_k-\\lambda_m)(u_k-u_m) + \\sum_{k \\in \\partial\\Omega_d} \\sum_{f \\in \\partial B_k} T_{kf} \\lambda_k u_k\n$$\n对 $\\sigma_k$ 求导得到梯度的第 $k$ 个分量：\n$$\n(\\nabla_\\sigma J_m)_k = \\sum_{m \\sim k} \\frac{\\partial T_{km}}{\\partial \\sigma_k} (\\lambda_k-\\lambda_m)(u_k-u_m) + \\sum_{f \\in \\partial B_k} \\frac{\\partial T_{kf}}{\\partial \\sigma_k} \\lambda_k u_k\n$$\n由于 $\\frac{\\partial T_{km}}{\\partial \\sigma_k} = \\frac{2\\sigma_m^2}{(\\sigma_k+\\sigma_m)^2}$ 和 $\\frac{\\partial T_{kf}}{\\partial \\sigma_k} = 2$，上式变为：\n$$\n(\\nabla_\\sigma J_m)_k = \\sum_{m \\sim k} \\frac{2\\sigma_m^2}{(\\sigma_k+\\sigma_m)^2} (\\lambda_k-\\lambda_m)(u_k-u_m) + \\sum_{f \\in \\partial B_k} 2\\lambda_k u_k\n$$\n其中第一个和是对单元 $k$ 的内部邻居求和，第二个和是对其边界表面求和。\n\n**2.3. 光滑全变分的离散梯度**\n\n光滑 TV 项为 $R(\\sigma) = \\mathrm{TV}_\\varepsilon(\\sigma) = \\sum_{i,j} \\sqrt{(D_x\\sigma)_{ij}^2 + (D_y\\sigma)_{ij}^2 + \\varepsilon^2}$。令 $G\\sigma = \\begin{pmatrix} D_x \\sigma \\\\ D_y \\sigma \\end{pmatrix}$。令 $w_{ij}(\\sigma) = \\sqrt{(D_x\\sigma)_{ij}^2 + (D_y\\sigma)_{ij}^2 + \\varepsilon^2}$。\n$R$ 相对于 $\\sigma_k$ 的梯度通过链式法则求得：\n$$\n\\frac{\\partial R}{\\partial \\sigma_k} = \\sum_{i,j} \\frac{\\partial w_{ij}}{\\partial \\sigma_k} = \\sum_{i,j} \\frac{1}{w_{ij}} \\left( (D_x\\sigma)_{ij} \\frac{\\partial(D_x\\sigma)_{ij}}{\\partial \\sigma_k} + (D_y\\sigma)_{ij} \\frac{\\partial(D_y\\sigma)_{ij}}{\\partial \\sigma_k} \\right)\n$$\n这个表达式可以使用差分算子的形式伴随（转置）来紧凑地书写。令 $v_x = (D_x\\sigma)./w$ 和 $v_y = (D_y\\sigma)./w$ 为向量场。则梯度为：\n$$\n\\nabla R(\\sigma) = -(D_x^T v_x + D_y^T v_y) = -\\mathrm{div}(v)\n$$\n前向差分算子 $D_x, D_y$ 定义时带有齐次诺伊曼扩展，意味着 $(D_x\\sigma)_{N_x-1,j}=0$ 和 $(D_y\\sigma)_{i,N_y-1}=0$。伴随算子 $D_x^T, D_y^T$ 是后向差分算子，其在边界处的结构由离散分部积分恒等式 $\\langle D\\sigma, v \\rangle = \\langle \\sigma, D^T v \\rangle$ 决定。对于指定的 $D_x$，分量 $(D_x^T v_x)_{i,j}$ 为：\n\\begin{itemize}\n    \\item 当 $i \\in \\{1,\\dots,N_x-2\\}$ 时为 $(v_x)_{i-1,j} - (v_x)_{i,j}$\n    \\item 当 $i=0$ 时为 $- (v_x)_{0,j}$\n    \\item 当 $i=N_x-1$ 时为 $(v_x)_{N_x-2,j}$\n\\end{itemize}\n类似的形式也适用于 $D_y^T$。\n\n**3. 实现计划**\n解决方案将使用 Python 的 `numpy` 和 `scipy` 库来实现。\n1.  **网格与模型设置：** 一个 `InverseProblem` 类将封装网格、物理参数和离散化细节。\n2.  **矩阵组装：** 一个方法 `assemble_A(sigma)` 将根据推导的有限体积格式构建稀疏刚度矩阵 $A$。\n3.  **求解器：** `scipy.sparse.linalg.spsolve` 将用于求解正演（$A u = b$）和伴随（$A \\lambda = g$）系统。\n4.  **目标函数与梯度：** 方法 `compute_objective(sigma)` 和 `compute_gradient(sigma)` 将实现完整的目标泛函 $J(\\sigma)$ 及其解析梯度 $\\nabla J(\\sigma) = \\nabla J_m + \\alpha \\nabla R$。\n5.  **梯度检查：** 一个函数将把解析梯度在随机方向上的投影与中心有限差分近似进行比较。\n6.  **优化：** 将实现一个固定迭代次数的梯度下降循环，包括 Armijo 回溯线搜索和正性投影。\n7.  **后处理：** 将在最终反演的电导率场上计算边缘锐度度量。\n8.  **主脚本：** `solve` 函数将为两个指定的 $\\varepsilon$ 值协调这些步骤，并打印所需的四个结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csc_matrix\nfrom scipy.sparse.linalg import spsolve\n\nclass InverseProblem:\n    def __init__(self, Nx=32, Ny=32, alpha=1e-3, epsilon=1e-1):\n        self.Nx = Nx\n        self.Ny = Ny\n        self.N = Nx * Ny\n        self.h = 1.0 / Nx\n\n        self.alpha = alpha\n        self.epsilon = epsilon\n        \n        self.x = np.linspace(0.5 * self.h, 1.0 - 0.5 * self.h, Nx)\n        self.y = np.linspace(0.5 * self.h, 1.0 - 0.5 * self.h, Ny)\n        self.xx, self.yy = np.meshgrid(self.x, self.y)\n        \n        # Source term\n        s = np.exp(-((self.xx - 0.3)**2 + (self.yy - 0.7)**2) / (2 * 0.07**2))\n        self.b = (self.h**2 * s).flatten()\n\n        # True conductivity\n        radius_sq = 0.25**2\n        self.sigma_true = np.ones((self.Ny, self.Nx))\n        self.sigma_true[(self.xx - 0.5)**2 + (self.yy - 0.5)**2  radius_sq] = 2.0\n        self.sigma_true = self.sigma_true.flatten()\n\n        # Observation operator P\n        obs_indices_1d = np.round(np.linspace(4, self.Nx - 5, 4)).astype(int)\n        self.obs_indices = []\n        for i in obs_indices_1d:\n            for j in obs_indices_1d:\n                self.obs_indices.append(j * self.Nx + i)\n        \n        self.P = lil_matrix((len(self.obs_indices), self.N))\n        for i, idx in enumerate(self.obs_indices):\n            self.P[i, idx] = 1.0\n        self.P = self.P.tocsc()\n        \n        # Generate synthetic data\n        u_true = self.solve_forward(self.sigma_true)\n        self.d_obs = self.P @ u_true\n\n    def _to_1d(self, i, j):\n        return j * self.Nx + i\n\n    def assemble_A(self, sigma):\n        A = lil_matrix((self.N, self.N))\n        sigma_2d = sigma.reshape((self.Ny, self.Nx))\n\n        for j in range(self.Ny):\n            for i in range(self.Nx):\n                k = self._to_1d(i, j)\n                diag_val = 0.0\n                \n                # East neighbor\n                if i  self.Nx - 1:\n                    m = self._to_1d(i + 1, j)\n                    T_em = 2 * sigma_2d[j, i] * sigma_2d[j, i + 1] / (sigma_2d[j, i] + sigma_2d[j, i + 1])\n                    A[k, m] = -T_em\n                    A[m, k] = -T_em\n                    diag_val += T_em\n                else: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                # West neighbor\n                if i > 0:\n                    # Already handled by symmetry\n                    pass\n                else: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                # North neighbor\n                if j  self.Ny - 1:\n                    m = self._to_1d(i, j + 1)\n                    T_nm = 2 * sigma_2d[j, i] * sigma_2d[j + 1, i] / (sigma_2d[j, i] + sigma_2d[j + 1, i])\n                    A[k, m] = -T_nm\n                    A[m, k] = -T_nm\n                    diag_val += T_nm\n                else: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                # South neighbor\n                if j > 0:\n                    # Already handled by symmetry\n                    pass\n                else: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                A[k, k] += diag_val\n        return A.tocsc()\n\n    def solve_forward(self, sigma):\n        A = self.assemble_A(sigma)\n        return spsolve(A, self.b)\n\n    def solve_adjoint(self, sigma, u):\n        A = self.assemble_A(sigma)\n        residual = self.P @ u - self.d_obs\n        adjoint_rhs = -self.P.T @ residual\n        return spsolve(A, adjoint_rhs)\n\n    def compute_objective(self, sigma):\n        # Data misfit term\n        u = self.solve_forward(sigma)\n        residual = self.P @ u - self.d_obs\n        J_misfit = 0.5 * np.dot(residual, residual)\n        \n        # Regularization term\n        sigma_2d = sigma.reshape((self.Ny, self.Nx))\n        Dx_sigma = np.zeros_like(sigma_2d)\n        Dy_sigma = np.zeros_like(sigma_2d)\n        Dx_sigma[:, :-1] = sigma_2d[:, 1:] - sigma_2d[:, :-1]\n        Dy_sigma[:-1, :] = sigma_2d[1:, :] - sigma_2d[:-1, :]\n        \n        tv_integrand = np.sqrt(Dx_sigma**2 + Dy_sigma**2 + self.epsilon**2)\n        J_tv = np.sum(tv_integrand)\n        \n        return J_misfit + self.alpha * J_tv\n\n    def compute_gradient(self, sigma):\n        u = self.solve_forward(sigma)\n        lam = self.solve_adjoint(sigma, u)\n        \n        sigma_2d = sigma.reshape((self.Ny, self.Nx))\n        u_2d = u.reshape((self.Ny, self.Nx))\n        lam_2d = lam.reshape((self.Ny, self.Nx))\n        \n        # Misfit gradient\n        grad_J_misfit = np.zeros_like(sigma_2d)\n        for j in range(self.Ny):\n            for i in range(self.Nx):\n                grad_val = 0.0\n                \n                # East\n                if i  self.Nx - 1:\n                    s_k, s_m = sigma_2d[j, i], sigma_2d[j, i + 1]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j,i+1]) * (lam_2d[j,i] - lam_2d[j,i+1])\n                else: # Boundary\n                    grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                # West\n                if i > 0:\n                    s_k, s_m = sigma_2d[j, i], sigma_2d[j, i - 1]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j,i-1]) * (lam_2d[j,i] - lam_2d[j,i-1])\n                else: # Boundary\n                    grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                # North\n                if j  self.Ny - 1:\n                    s_k, s_m = sigma_2d[j, i], sigma_2d[j + 1, i]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j+1,i]) * (lam_2d[j,i] - lam_2d[j+1,i])\n                else: # Boundary\n                    grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                # South\n                if j > 0:\n                    s_k, s_m = sigma_2d[j, i], sigma_2d[j - 1, i]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j-1,i]) * (lam_2d[j,i] - lam_2d[j-1,i])\n                else: # Boundary\n                    grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                \n                grad_J_misfit[j, i] = grad_val\n\n        # TV gradient\n        grad_J_tv = np.zeros_like(sigma_2d)\n        Dx_sigma = np.zeros_like(sigma_2d)\n        Dy_sigma = np.zeros_like(sigma_2d)\n        Dx_sigma[:, :-1] = sigma_2d[:, 1:] - sigma_2d[:, :-1]\n        Dy_sigma[:-1, :] = sigma_2d[1:, :] - sigma_2d[:-1, :]\n        \n        w = np.sqrt(Dx_sigma**2 + Dy_sigma**2 + self.epsilon**2)\n        vx = Dx_sigma / w\n        vy = Dy_sigma / w\n        \n        # Divergence\n        # Dx^T part\n        grad_J_tv[:, 1:-1] -= (vx[:, 1:-1] - vx[:, :-2])\n        grad_J_tv[:, 0] -= vx[:, 0]\n        grad_J_tv[:, -1] += vx[:, -2]\n        # Dy^T part\n        grad_J_tv[1:-1, :] -= (vy[1:-1, :] - vy[:-2, :])\n        grad_J_tv[0, :] -= vy[0, :]\n        grad_J_tv[-1, :] += vy[-2, :]\n        \n        return grad_J_misfit.flatten() + self.alpha * grad_J_tv.flatten()\n\n\ndef solve():\n    np.random.seed(42)\n    s0 = np.ones(32*32)\n    ds = np.random.randn(32*32)\n    h_fd = 1e-4\n    \n    results = []\n    \n    # Cases 1 and 3: epsilon = 1e-1\n    epsilon1 = 1e-1\n    model1 = InverseProblem(epsilon=epsilon1)\n\n    # Grad check\n    grad_J = model1.compute_gradient(s0)\n    J_plus = model1.compute_objective(s0 + h_fd * ds)\n    J_minus = model1.compute_objective(s0 - h_fd * ds)\n    \n    fd_deriv = (J_plus - J_minus) / (2 * h_fd)\n    adj_deriv = np.dot(grad_J, ds)\n    \n    rel_error1 = np.abs(adj_deriv - fd_deriv) / np.max([1.0, np.abs(adj_deriv), np.abs(fd_deriv)])\n    results.append(rel_error1)\n    \n    # Inversion\n    sigma_k = np.copy(s0)\n    c_armijo = 1e-4\n    beta_armijo = 0.5\n    sigma_min = 0.2\n    \n    for k in range(10):\n        J_k = model1.compute_objective(sigma_k)\n        grad_J_k = model1.compute_gradient(sigma_k)\n        pk = -grad_J_k\n        \n        tk = 1e-1\n        while model1.compute_objective(np.maximum(sigma_min, sigma_k + tk * pk)) > J_k + c_armijo * tk * np.dot(grad_J_k, pk):\n            tk *= beta_armijo\n        \n        sigma_k += tk * pk\n        sigma_k = np.maximum(sigma_k, sigma_min)\n    \n    final_sigma1 = sigma_k.reshape((32, 32))\n    \n    # Edge sharpness\n    Dx_s = np.zeros_like(final_sigma1)\n    Dy_s = np.zeros_like(final_sigma1)\n    Dx_s[:, :-1] = final_sigma1[:, 1:] - final_sigma1[:, :-1]\n    Dy_s[:-1, :] = final_sigma1[1:, :] - final_sigma1[:-1, :]\n    grad_mag = np.sqrt(Dx_s**2 + Dy_s**2)\n    \n    band_radius = np.sqrt((model1.xx - 0.5)**2 + (model1.yy - 0.5)**2)\n    band_mask = np.abs(band_radius - 0.25) = (2 * model1.h)\n    \n    sharpness1 = np.mean(grad_mag[band_mask])\n    \n    # Cases 2 and 4: epsilon = 1e-3\n    epsilon2 = 1e-3\n    model2 = InverseProblem(epsilon=epsilon2)\n\n    # Grad check\n    grad_J = model2.compute_gradient(s0)\n    J_plus = model2.compute_objective(s0 + h_fd * ds)\n    J_minus = model2.compute_objective(s0 - h_fd * ds)\n    \n    fd_deriv = (J_plus - J_minus) / (2 * h_fd)\n    adj_deriv = np.dot(grad_J, ds)\n\n    # Insert grad check error for eps2\n    rel_error2 = np.abs(adj_deriv - fd_deriv) / np.max([1.0, np.abs(adj_deriv), np.abs(fd_deriv)])\n    \n    # Inversion\n    sigma_k = np.copy(s0)\n    for k in range(10):\n        J_k = model2.compute_objective(sigma_k)\n        grad_J_k = model2.compute_gradient(sigma_k)\n        pk = -grad_J_k\n        \n        tk = 1e-1\n        # The line search should check the objective at the *projected* point to be robust\n        while model2.compute_objective(np.maximum(sigma_min, sigma_k + tk * pk)) > J_k + c_armijo * tk * np.dot(grad_J_k, pk):\n            tk *= beta_armijo\n        \n        sigma_k += tk * pk\n        sigma_k = np.maximum(sigma_k, sigma_min)\n\n    final_sigma2 = sigma_k.reshape((32, 32))\n\n    # Edge sharpness\n    Dx_s = np.zeros_like(final_sigma2)\n    Dy_s = np.zeros_like(final_sigma2)\n    Dx_s[:, :-1] = final_sigma2[:, 1:] - final_sigma2[:, :-1]\n    Dy_s[:-1, :] = final_sigma2[1:, :] - final_sigma2[:-1, :]\n    grad_mag = np.sqrt(Dx_s**2 + Dy_s**2)\n    \n    sharpness2 = np.mean(grad_mag[band_mask])\n    \n    results.insert(1, rel_error2)\n    results.append(sharpness1)\n    results.append(sharpness2)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}