{
    "hands_on_practices": [
        {
            "introduction": "A primary motivation for using control variable transforms in variational data assimilation is preconditioning, which aims to make the optimization problem easier for numerical solvers. This exercise provides a hands-on numerical demonstration of how the classic $L=B^{1/2}$ transform improves the conditioning of the Hessian matrix. By comparing the condition numbers before and after the transform under varying observation error scenarios, you will gain a concrete understanding of why this technique is crucial for the efficiency of large-scale optimization systems. ",
            "id": "3372107",
            "problem": "Consider a one-dimensional variational data assimilation problem with a linear observation operator. The cost function is given by the standard quadratic form in variational data assimilation:\n$$\nJ(x) = \\frac{1}{2} \\left\\| x - x_b \\right\\|_{B^{-1}}^2 + \\frac{1}{2} \\left\\| y - H x \\right\\|_{R^{-1}}^2,\n$$\nwhere $x \\in \\mathbb{R}^n$ is the state vector, $x_b \\in \\mathbb{R}^n$ is the background state, $y \\in \\mathbb{R}^n$ are observations, $B \\in \\mathbb{R}^{n \\times n}$ is the background error covariance matrix, $H \\in \\mathbb{R}^{n \\times n}$ is the linear observation operator, and $R \\in \\mathbb{R}^{n \\times n}$ is the observation error covariance matrix. Assume $H = I$ and $R = \\sigma_o^2 I$, where $I$ is the identity matrix and $\\sigma_o^2 > 0$ is the observation error variance.\n\nThe Gauss-Newton approximation of the Hessian of $J(x)$ at any point is\n$$\nK = B^{-1} + H^\\top R^{-1} H.\n$$\nWith $H = I$ and $R = \\sigma_o^2 I$, this simplifies to\n$$\nK = B^{-1} + \\frac{1}{\\sigma_o^2} I.\n$$\n\nIntroduce the control variable transform defined by $x = x_b + L v$, where $L \\in \\mathbb{R}^{n \\times n}$ is chosen such that $L L^\\top = B$ (so $L$ is a matrix square root of $B$), and $v \\in \\mathbb{R}^n$ is the control variable. In $v$-space, the Gauss-Newton Hessian becomes\n$$\nK_v = I + L^\\top H^\\top R^{-1} H L.\n$$\nFor $H = I$ and $R = \\sigma_o^2 I$, this is\n$$\nK_v = I + \\frac{1}{\\sigma_o^2} L^\\top L = I + \\frac{1}{\\sigma_o^2} B.\n$$\n\nLet the background error covariance $B$ be defined by an exponential correlation on a uniform one-dimensional grid of $n$ points over the unit interval $[0,1]$. The grid points are $x_i = \\frac{i}{n-1}$ for $i = 0, 1, \\dots, n-1$. The entries of $B$ are defined as\n$$\nB_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{|x_i - x_j|}{\\ell}\\right),\n$$\nwhere $\\sigma_b^2 > 0$ is a background variance and $\\ell > 0$ is a correlation length scale, both dimensionless.\n\nYour task is to:\n- Construct $B$ for $n = 64$, $\\sigma_b^2 = 1$, and $\\ell = 0.2$.\n- Compute $L = B^{1/2}$ using an eigenvalue decomposition of $B$.\n- For each specified value of $\\sigma_o^2$, compute the $2$-norm condition number (Spectral Norm (2-norm) condition number) of the untransformed Hessian $K$ and the transformed Hessian $K_v$. The $2$-norm condition number of a symmetric positive definite matrix $A$ is defined as\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)},\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the largest and smallest eigenvalues of $A$, respectively.\n\nProvide numerical results for the following test suite of observation error variances:\n- $\\sigma_o^2 = 10^{-8}$ (edge case: extremely accurate observations),\n- $\\sigma_o^2 = 10^{-4}$,\n- $\\sigma_o^2 = 10^{-2}$,\n- $\\sigma_o^2 = 10^{0}$ (general case),\n- $\\sigma_o^2 = 10^{2}$ (edge case: extremely noisy observations).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the list should itself be a two-element list containing the pair $[\\kappa_2(K), \\kappa_2(K_v)]$ for the corresponding $\\sigma_o^2$. For example, the output format must be\n$$\n[[\\kappa_2(K_1), \\kappa_2(K_{v,1})], [\\kappa_2(K_2), \\kappa_2(K_{v,2})], \\dots],\n$$\nprinted as a single line in standard Python list syntax. No physical units are involved; all quantities are dimensionless.",
            "solution": "The problem statement is a well-posed and scientifically grounded exercise in numerical linear algebra applied to variational data assimilation. It asks for the computation of condition numbers for two different formulations of the Hessian matrix in a standard data assimilation cost function. All parameters and definitions are provided, and the problem is self-contained and free of contradictions or ambiguities. It is a valid problem.\n\nThe core task is to compute the spectral condition number, $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$, for two matrices: the untransformed Hessian $K$ and the transformed Hessian $K_v$.\nThe provided definitions are:\n1.  The untransformed Hessian: $K = B^{-1} + \\frac{1}{\\sigma_o^2} I$\n2.  The transformed Hessian: $K_v = I + \\frac{1}{\\sigma_o^2} B$\n\nHere, $B$ is the background error covariance matrix, $I$ is the identity matrix, and $\\sigma_o^2$ is the observation error variance. To compute the condition numbers, we must find the minimum and maximum eigenvalues of $K$ and $K_v$. These can be directly related to the eigenvalues of $B$.\n\nLet $\\{\\lambda_i(B)\\}$ be the set of eigenvalues of the matrix $B$. Since $B$ is constructed to be symmetric and positive definite, its eigenvalues are real and positive. Let $\\lambda_{\\min}(B)$ and $\\lambda_{\\max}(B)$ denote the smallest and largest eigenvalues of $B$, respectively.\n\n**Eigenvalues of $K$**\n\nThe eigenvalues of the inverse matrix $B^{-1}$ are the reciprocals of the eigenvalues of $B$, i.e., $\\{1/\\lambda_i(B)\\}$. The ordering is reversed, so the largest eigenvalue of $B^{-1}$ is $1/\\lambda_{\\min}(B)$, and the smallest is $1/\\lambda_{\\max}(B)$.\nAdding a scaled identity matrix $\\alpha I$ to any matrix shifts its eigenvalues by $\\alpha$. In our case, $\\alpha = 1/\\sigma_o^2$.\nTherefore, the extremal eigenvalues of $K = B^{-1} + \\frac{1}{\\sigma_o^2} I$ are:\n$$\n\\lambda_{\\max}(K) = \\lambda_{\\max}(B^{-1}) + \\frac{1}{\\sigma_o^2} = \\frac{1}{\\lambda_{\\min}(B)} + \\frac{1}{\\sigma_o^2}\n$$\n$$\n\\lambda_{\\min}(K) = \\lambda_{\\min}(B^{-1}) + \\frac{1}{\\sigma_o^2} = \\frac{1}{\\lambda_{\\max}(B)} + \\frac{1}{\\sigma_o^2}\n$$\nThe condition number of $K$ is then:\n$$\n\\kappa_2(K) = \\frac{\\lambda_{\\max}(K)}{\\lambda_{\\min}(K)} = \\frac{\\frac{1}{\\lambda_{\\min}(B)} + \\frac{1}{\\sigma_o^2}}{\\frac{1}{\\lambda_{\\max}(B)} + \\frac{1}{\\sigma_o^2}}\n$$\n\n**Eigenvalues of $K_v$**\n\nThe eigenvalues of the matrix $\\frac{1}{\\sigma_o^2}B$ are $\\{\\frac{\\lambda_i(B)}{\\sigma_o^2}\\}$. Adding the identity matrix $I$ shifts these eigenvalues by $1$.\nTherefore, the extremal eigenvalues of $K_v = I + \\frac{1}{\\sigma_o^2} B$ are:\n$$\n\\lambda_{\\max}(K_v) = 1 + \\frac{\\lambda_{\\max}(B)}{\\sigma_o^2}\n$$\n$$\n\\lambda_{\\min}(K_v) = 1 + \\frac{\\lambda_{\\min}(B)}{\\sigma_o^2}\n$$\nThe condition number of $K_v$ is then:\n$$\n\\kappa_2(K_v) = \\frac{\\lambda_{\\max}(K_v)}{\\lambda_{\\min}(K_v)} = \\frac{1 + \\frac{\\lambda_{\\max}(B)}{\\sigma_o^2}}{1 + \\frac{\\lambda_{\\min}(B)}{\\sigma_o^2}}\n$$\n\nThis analytical approach reveals that we only need to compute the extremal eigenvalues of the matrix $B$. The explicit formation of $B^{-1}$ or the matrix square root $L=B^{1/2}$ is not required for the calculation of the condition numbers.\n\nThe numerical implementation plan is as follows:\n1.  Define the problem parameters: grid size $n = 64$, background variance $\\sigma_b^2 = 1$, and correlation length $\\ell = 0.2$.\n2.  Create the one-dimensional grid of $n$ points over the interval $[0, 1]$.\n3.  Construct the $n \\times n$ background error covariance matrix $B$ using its definition: $B_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{|x_i - x_j|}{\\ell}\\right)$.\n4.  Compute the eigenvalues of the real symmetric matrix $B$. A specialized algorithm for symmetric matrices, such as `numpy.linalg.eigvalsh`, is appropriate as it is efficient and numerically stable. This function returns sorted eigenvalues.\n5.  Extract the minimum and maximum eigenvalues, $\\lambda_{\\min}(B)$ and $\\lambda_{\\max}(B)$.\n6.  Iterate through the given list of observation error variances $\\sigma_o^2 = \\{10^{-8}, 10^{-4}, 10^{-2}, 10^{0}, 10^{2}\\}$.\n7.  In each iteration, calculate $\\kappa_2(K)$ and $\\kappa_2(K_v)$ using the derived formulas.\n8.  Store each pair $[\\kappa_2(K), \\kappa_2(K_v)]$ and format the final list of pairs into the required single-line string output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the condition numbers of the untransformed (K) and transformed (Kv)\n    Hessians for a 1D variational data assimilation problem.\n    \"\"\"\n    # 1. Define problem parameters\n    n = 64\n    sigma_b_sq = 1.0\n    l_corr = 0.2\n    \n    test_cases_sigma_o_sq = [1e-8, 1e-4, 1e-2, 1.0, 1e2]\n\n    # 2. Create the grid\n    x_grid = np.linspace(0.0, 1.0, n)\n\n    # 3. Construct the background error covariance matrix B\n    # Use broadcasting for an efficient construction of the distance matrix\n    dist_matrix = np.abs(x_grid[:, np.newaxis] - x_grid)\n    B = sigma_b_sq * np.exp(-dist_matrix / l_corr)\n    \n    # 4. Compute eigenvalues of B\n    # B is a real symmetric matrix. numpy.linalg.eigvalsh is efficient and\n    # returns sorted eigenvalues in ascending order.\n    eigvals_B = np.linalg.eigvalsh(B)\n    \n    # 5. Extract minimum and maximum eigenvalues\n    lambda_min_B = eigvals_B[0]\n    lambda_max_B = eigvals_B[-1]\n    \n    results = []\n    \n    # 6. Loop through test cases for sigma_o^2\n    for s2o in test_cases_sigma_o_sq:\n        # 7. Calculate condition number for K = B^-1 + (1/s2o) * I\n        lambda_max_K = 1.0 / lambda_min_B + 1.0 / s2o\n        lambda_min_K = 1.0 / lambda_max_B + 1.0 / s2o\n        cond_K = lambda_max_K / lambda_min_K\n        \n        # Calculate condition number for Kv = I + (1/s2o) * B\n        lambda_max_Kv = 1.0 + (1.0 / s2o) * lambda_max_B\n        lambda_min_Kv = 1.0 + (1.0 / s2o) * lambda_min_B\n        cond_Kv = lambda_max_Kv / lambda_min_Kv\n        \n        results.append([cond_K, cond_Kv])\n\n    # 8. Format the final output string as specified\n    string_parts = []\n    for res_pair in results:\n        string_parts.append(f\"[{res_pair[0]},{res_pair[1]}]\")\n    final_string = f\"[{','.join(string_parts)}]\"\n    \n    print(final_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond preconditioning, control variable transforms are essential for enforcing physical constraints, such as positivity. This exercise challenges you to compare two common positivity-enforcing transforms: the exponential map and the softplus function. By deriving and numerically evaluating the gradient and Hessian of the cost function for each, you will investigate their differing impacts on the optimization landscape, revealing important practical trade-offs related to gradient saturation and curvature. ",
            "id": "3372041",
            "problem": "Consider a scalar positive state variable $x \\in (0,\\infty)$ to be estimated in a nonlinear variational Data Assimilation (DA) problem. Let $z \\in \\mathbb{R}$ be an unconstrained control variable related to $x$ through a control variable transform $x = \\tau(z)$. The variational objective is defined by a background term and a single nonlinear observation term with Gaussian errors:\n$$\nJ(z) = \\frac{1}{2}\\frac{\\left(x(z) - x_b\\right)^2}{\\sigma_b^2} + \\frac{1}{2}\\frac{\\left(h\\big(x(z)\\big) - y\\right)^2}{\\sigma_o^2},\n$$\nwhere $x_b \\in (0,\\infty)$ is the background (prior) mean, $\\sigma_b^2 > 0$ is the background variance, $y \\in \\mathbb{R}$ is the observation, $\\sigma_o^2 > 0$ is the observation-error variance, and $h:(0,\\infty)\\to\\mathbb{R}$ is a twice-differentiable observation operator. In this problem, use the specific nonlinear observation operator $h(x) = \\sqrt{x}$.\n\nTwo different positivity-enforcing control variable transforms are to be compared:\n- Softplus transform: $x(z) = \\log\\!\\left(1 + e^{z}\\right)$.\n- Exponential log-transform: $x(z) = e^{z}$.\n\nTasks:\n1. Starting from the definitions of $J(z)$, $h(x)$, and the two transforms above, and using only standard rules of differential calculus (including the chain rule), derive for each transform explicit expressions for the first derivative $\\frac{dJ}{dz}$ and the second derivative $\\frac{d^2J}{dz^2}$, both evaluated at an arbitrary $z \\in \\mathbb{R}$.\n2. Implement these expressions in a program and evaluate them for each transform on the test suite below. For each test case, compute the following two scalar diagnostics:\n   - The gradient saturation ratio $r_g = \\frac{\\left|\\frac{dJ}{dz}\\right|_{\\text{softplus}}}{\\left|\\frac{dJ}{dz}\\right|_{\\exp}}$.\n   - The curvature ratio $r_H = \\frac{\\left.\\frac{d^2J}{dz^2}\\right|_{\\text{softplus}}}{\\left.\\frac{d^2J}{dz^2}\\right|_{\\exp}}$.\n3. Report all results as a single flat list in the specified order.\n\nTest suite (each case is a tuple $(z, x_b, \\sigma_b^2, y, \\sigma_o^2)$):\n- Case 1 (general): $(0.0, 1.0, 0.25, 1.2, 0.04)$.\n- Case 2 (strongly negative control, small state): $(-20.0, 1.0, 0.25, 0.8, 0.04)$.\n- Case 3 (strongly positive control, large state under exponential): $(10.0, 2.0, 0.25, 100.0, 0.04)$.\n- Case 4 (moderately negative control, curvature near small state): $(-5.0, 0.5, 0.25, 0.1, 0.04)$.\n\nYour program must output a single line containing a flat list with eight floating-point numbers in the order\n$$\n\\left[r_g^{(1)}, r_H^{(1)}, r_g^{(2)}, r_H^{(2)}, r_g^{(3)}, r_H^{(3)}, r_g^{(4)}, r_H^{(4)}\\right],\n$$\nwhere the superscripts refer to the test case index. Round each numeric output to six decimal places. The output must be a single line, representing the list in standard bracketed form with comma-separated values (for example, $[0.123456,0.654321,\\dots]$). No physical units are involved; all quantities are nondimensional real numbers. Angles do not appear anywhere in the problem, so no angle unit is required.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of variational data assimilation and differential calculus, is well-posed, and all necessary information is provided.\n\nThe objective is to derive the first and second derivatives of a variational cost function $J(z)$ with respect to a control variable $z$, for two different control variable transforms $x(z)$, and then to evaluate these derivatives and their ratios for a set of test cases.\n\nThe cost function is given by:\n$$\nJ(z) = \\frac{1}{2}\\frac{\\left(x(z) - x_b\\right)^2}{\\sigma_b^2} + \\frac{1}{2}\\frac{\\left(h\\big(x(z)\\big) - y\\right)^2}{\\sigma_o^2}\n$$\nwhere $x$ is the state variable, $z$ is the control variable, $x_b$ is the background state, $y$ is the observation, $\\sigma_b^2$ and $\\sigma_o^2$ are variances, and $h(x)$ is the observation operator. For this problem, $h(x) = \\sqrt{x}$.\n\nThe derivatives of $J$ with respect to $z$ are found using the chain rule:\n$$\n\\frac{dJ}{dz} = \\frac{dJ}{dx} \\frac{dx}{dz}\n$$\n$$\n\\frac{d^2J}{dz^2} = \\frac{d}{dz}\\left(\\frac{dJ}{dx} \\frac{dx}{dz}\\right) = \\frac{d}{dx}\\left(\\frac{dJ}{dx}\\right) \\left(\\frac{dx}{dz}\\right) \\frac{dx}{dz} + \\frac{dJ}{dx} \\frac{d^2x}{dz^2} = \\frac{d^2J}{dx^2}\\left(\\frac{dx}{dz}\\right)^2 + \\frac{dJ}{dx}\\frac{d^2x}{dz^2}\n$$\n\nFirst, we compute the derivatives of $J$ with respect to $x$. These terms are common to both transforms.\nThe first derivative of $h(x) = x^{1/2}$ is $h'(x) = \\frac{d h}{d x} = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}$.\nThe second derivative is $h''(x) = \\frac{d^2 h}{d x^2} = -\\frac{1}{4}x^{-3/2} = -\\frac{1}{4x\\sqrt{x}}$.\n\nThe first derivative of $J$ with respect to $x$ is:\n$$\n\\frac{dJ}{dx} = \\frac{x - x_b}{\\sigma_b^2} + \\frac{h(x) - y}{\\sigma_o^2} h'(x) = \\frac{x - x_b}{\\sigma_b^2} + \\frac{\\sqrt{x} - y}{\\sigma_o^2} \\frac{1}{2\\sqrt{x}} = \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right)\n$$\nThe second derivative of $J$ with respect to $x$ is:\n$$\n\\frac{d^2J}{dx^2} = \\frac{d}{dx}\\left(\\frac{dJ}{dx}\\right) = \\frac{1}{\\sigma_b^2} + \\frac{d}{dx}\\left(\\frac{1}{2\\sigma_o^2}\\left(1 - yx^{-1/2}\\right)\\right) = \\frac{1}{\\sigma_b^2} - \\frac{y}{2\\sigma_o^2} \\left(-\\frac{1}{2}x^{-3/2}\\right) = \\frac{1}{\\sigma_b^2} + \\frac{y}{4\\sigma_o^2 x^{3/2}} = \\frac{1}{\\sigma_b^2} + \\frac{y}{4x\\sqrt{x}\\sigma_o^2}\n$$\n\nNow we derive the expressions for each specific transform.\n\n**1. Exponential Log-Transform: $x(z) = e^z$**\n\nThe derivatives of the transform with respect to $z$ are:\n$$\n\\frac{dx}{dz} = e^z = x\n$$\n$$\n\\frac{d^2x}{dz^2} = e^z = x\n$$\nSubstituting these into the chain rule formulas:\n\nThe first derivative $\\frac{dJ}{dz}$ is:\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\exp} = \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) x\n$$\nwhere $x = e^z$.\n\nThe second derivative $\\frac{d^2J}{dz^2}$ is:\n$$\n\\left.\\frac{d^2J}{dz^2}\\right|_{\\exp} = \\left( \\frac{1}{\\sigma_b^2} + \\frac{y}{4x\\sqrt{x}\\sigma_o^2} \\right) \\left(x\\right)^2 + \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) x\n$$\nwhere $x = e^z$. Note that the second term is equal to $\\left.\\frac{dJ}{dz}\\right|_{\\exp}$.\n\n**2. Softplus Transform: $x(z) = \\log(1+e^z)$**\n\nThe derivatives of the transform with respect to $z$ are:\n$$\n\\frac{dx}{dz} = \\frac{e^z}{1+e^z}\n$$\nThis is the logistic sigmoid function, often denoted $\\sigma(z)$. It can also be written as $(1+e^{-z})^{-1}$.\n\n$$\n\\frac{d^2x}{dz^2} = \\frac{d}{dz}\\left(\\frac{e^z}{1+e^z}\\right) = \\frac{e^z(1+e^z) - e^z(e^z)}{(1+e^z)^2} = \\frac{e^z}{(1+e^z)^2}\n$$\nThis can be written compactly as $\\frac{dx}{dz}\\left(1 - \\frac{dx}{dz}\\right)$.\n\nSubstituting these into the chain rule formulas:\n\nThe first derivative $\\frac{dJ}{dz}$ is:\n$$\n\\left.\\frac{dJ}{dz}\\right|_{\\text{softplus}} = \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) \\left(\\frac{e^z}{1+e^z}\\right)\n$$\nwhere $x = \\log(1+e^z)$.\n\nThe second derivative $\\frac{d^2J}{dz^2}$ is:\n$$\n\\left.\\frac{d^2J}{dz^2}\\right|_{\\text{softplus}} = \\left( \\frac{1}{\\sigma_b^2} + \\frac{y}{4x\\sqrt{x}\\sigma_o^2} \\right) \\left(\\frac{e^z}{1+e^z}\\right)^2 + \\left( \\frac{x - x_b}{\\sigma_b^2} + \\frac{1}{2\\sigma_o^2}\\left(1 - \\frac{y}{\\sqrt{x}}\\right) \\right) \\left(\\frac{e^z}{(1+e^z)^2}\\right)\n$$\nwhere $x = \\log(1+e^z)$.\n\nThese explicit expressions are implemented in the provided Python code to compute the gradient saturation ratio $r_g$ and the curvature ratio $r_H$ for the given test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and evaluating derivatives for two control\n    variable transforms and computing their ratios.\n    \"\"\"\n    test_cases = [\n        # (z, x_b, sigma_b^2, y, sigma_o^2)\n        (0.0, 1.0, 0.25, 1.2, 0.04),\n        (-20.0, 1.0, 0.25, 0.8, 0.04),\n        (10.0, 2.0, 0.25, 100.0, 0.04),\n        (-5.0, 0.5, 0.25, 0.1, 0.04),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        z, xb, s2b, y, s2o = case\n\n        # --- Calculations for Exponential Transform ---\n        # x(z) and its derivatives\n        x_exp = np.exp(z)\n        dxdz_exp = x_exp\n        d2xdz2_exp = x_exp\n\n        # Derivatives of J w.r.t x, evaluated at x_exp\n        sqrt_x_exp = np.sqrt(x_exp)\n        dJdx_exp = (x_exp - xb) / s2b + (1.0 - y / sqrt_x_exp) / (2.0 * s2o)\n        d2Jdx2_exp = 1.0 / s2b + y / (4.0 * s2o * x_exp * sqrt_x_exp)\n\n        # Derivatives of J w.r.t z using the chain rule\n        dJdz_exp = dJdx_exp * dxdz_exp\n        d2Jdz2_exp = d2Jdx2_exp * (dxdz_exp**2) + dJdx_exp * d2xdz2_exp\n\n        # --- Calculations for Softplus Transform ---\n        # x(z) and its derivatives (using numerically stable forms)\n        if z > 0:\n            x_soft = z + np.log(1.0 + np.exp(-z))\n        else:\n            x_soft = np.log(1.0 + np.exp(z))\n        \n        dxdz_soft = 1.0 / (1.0 + np.exp(-z))  # Stable sigmoid\n        d2xdz2_soft = dxdz_soft * (1.0 - dxdz_soft)\n        \n        # Derivatives of J w.r.t x, evaluated at x_soft\n        sqrt_x_soft = np.sqrt(x_soft)\n        dJdx_soft = (x_soft - xb) / s2b + (1.0 - y / sqrt_x_soft) / (2.0 * s2o)\n        d2Jdx2_soft = 1.0 / s2b + y / (4.0 * s2o * x_soft * sqrt_x_soft)\n\n        # Derivatives of J w.r.t z using the chain rule\n        dJdz_soft = dJdx_soft * dxdz_soft\n        d2Jdz2_soft = d2Jdx2_soft * (dxdz_soft**2) + dJdx_soft * d2xdz2_soft\n\n        # --- Compute Ratios ---\n        # Gradient saturation ratio\n        rg = np.abs(dJdz_soft) / np.abs(dJdz_exp)\n        \n        # Curvature ratio\n        rH = d2Jdz2_soft / d2Jdz2_exp\n\n        results.extend([rg, rH])\n\n    # Format the final output string\n    formatted_results = \",\".join([f\"{val:.6f}\" for val in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the concept of transform-induced curvature, this practice delves into the precise mathematical structure of the Hessian for exponential barrier transforms. You will apply the second-order chain rule to account for the \"barrier curvature\" term, which arises from the transform's nonlinearity and interacts with the cost function's gradient. This exercise will deepen your understanding of how inequality constraints modify the optimization landscape, a crucial consideration for developing robust assimilation systems. ",
            "id": "3372076",
            "problem": "Consider a deterministic inverse problem under the framework of Bayesian data assimilation with inequality constraints enforced by control variable transforms. You are to implement barrier-based transforms for inequality constraints and study how the curvature induced by the barrier interacts with the data fit terms after transformation of variables. The state vector is denoted by $x \\in \\mathbb{R}^n$ and the unconstrained control variables are denoted by $z \\in \\mathbb{R}^n$. Each component $x_i$ is linked to $z_i$ by either a lower-bound or an upper-bound barrier transform of the form $x_i = a_i + \\exp(z_i)$ for a lower bound $a_i$, or $x_i = b_i - \\exp(z_i)$ for an upper bound $b_i$. More generally, for each component define a signed transform $x_i = c_i + s_i \\exp(z_i)$, where $s_i \\in \\{+1,-1\\}$ encodes the bound type ($+1$ for lower bound, $-1$ for upper bound) and $c_i$ is the corresponding bound value.\n\nStart from the following fundamental base, which you must use to derive all required quantities: the quadratic data assimilation objective with linear observation operator,\n$$\nJ(x) = \\tfrac{1}{2} (H x - y)^\\top R^{-1} (H x - y) + \\tfrac{1}{2} (x - m)^\\top B^{-1} (x - m),\n$$\nwhere $H \\in \\mathbb{R}^{m \\times n}$ is the observation matrix, $y \\in \\mathbb{R}^m$ are observations, $R^{-1} \\in \\mathbb{R}^{m \\times m}$ is the inverse of the observation error covariance (assumed symmetric positive definite), $m \\in \\mathbb{R}^n$ is the background state, and $B^{-1} \\in \\mathbb{R}^{n \\times n}$ is the inverse of the background error covariance (assumed symmetric positive definite).\n\nYour tasks are:\n\n- Implement the barrier transforms $x_i = c_i + s_i \\exp(z_i)$, compute the Jacobian $\\mathrm{d}x/\\mathrm{d}z$ and the second derivatives $\\mathrm{d}^2 x/\\mathrm{d}z^2$ according to the transform definition, and then use the chain rule to derive the gradient of $J$ with respect to $z$ and the symmetric Hessian of $J$ with respect to $z$ at a specified $z$.\n\n- Explicitly compute the gradient with respect to $x$ and the Hessian with respect to $x$ based on the quadratic objective and use them within the chain rule, ensuring that the contribution from the barrier curvature (arising from the second derivative of the transform) is properly included.\n\n- For each provided test case, compute $x$ at the specified $z$, then compute the gradient with respect to $x$, assemble the Hessian with respect to $x$, and finally assemble the transformed Hessian with respect to $z$. For each test case, report a single scalar that quantifies the curvature change due to the barrier transform, defined as the ratio of the smallest singular value of the Hessian in the transformed variables to the smallest singular value of the Hessian in the original variables. Formally, if $\\sigma_{\\min}(H_x)$ denotes the smallest singular value of the Hessian with respect to $x$ and $\\sigma_{\\min}(H_z)$ denotes the smallest singular value of the Hessian with respect to $z$, output $\\sigma_{\\min}(H_z)/\\sigma_{\\min}(H_x)$ as a floating-point number.\n\nYour program should implement the above logic and compute results for the following test suite. In all cases, $n = 3$ and $m = 3$.\n\n- Test case $1$ (all lower-bound, identity observations):\n  - $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n  - $R^{-1} = \\operatorname{diag}(1, 0.5, 0.2)$,\n  - $B^{-1} = \\operatorname{diag}(0.1, 0.1, 0.1)$,\n  - $y = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}$,\n  - $m = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0 \\end{bmatrix}$,\n  - $s = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $z = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\n- Test case $2$ (all upper-bound, identity observations):\n  - $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n  - $R^{-1} = \\operatorname{diag}(1, 0.5, 0.2)$,\n  - $B^{-1} = \\operatorname{diag}(0.1, 0.1, 0.1)$,\n  - $y = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}$,\n  - $m = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0 \\end{bmatrix}$,\n  - $s = \\begin{bmatrix} -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 3 \\\\ 3 \\\\ 3 \\end{bmatrix}$,\n  - $z = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test case $3$ (mixed bounds, non-identity observations):\n  - $H = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1 \\end{bmatrix}$,\n  - $R^{-1} = \\operatorname{diag}(0.5, 1.0, 2.0)$,\n  - $B^{-1} = \\operatorname{diag}(0.05, 0.2, 0.5)$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$,\n  - $m = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ -0.5 \\end{bmatrix}$,\n  - $s = \\begin{bmatrix} +1 \\\\ -1 \\\\ +1 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 0.0 \\\\ 2.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $z = \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.7 \\end{bmatrix}$.\n\n- Test case $4$ (lower-bound edge near the bound, identity observations):\n  - $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n  - $R^{-1} = \\operatorname{diag}(1, 0.5, 0.2)$,\n  - $B^{-1} = \\operatorname{diag}(0.1, 0.1, 0.1)$,\n  - $y = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}$,\n  - $m = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0 \\end{bmatrix}$,\n  - $s = \\begin{bmatrix} +1 \\\\ +1 \\\\ +1 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $z = \\begin{bmatrix} -10.0 \\\\ -5.0 \\\\ -1.0 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_k$ is the floating-point ratio $\\sigma_{\\min}(H_z)/\\sigma_{\\min}(H_x)$ for the $k$-th test case. No physical units are involved, and no angle units are required. The output values must be computed numerically by your program from the given inputs; do not hard-code the final ratios.",
            "solution": "The problem requires the derivation and implementation of the Hessian matrix for a data assimilation objective function under a control variable transform. The goal is to quantify the change in curvature, measured by the ratio of the smallest singular values of the Hessians in the transformed and original variables.\n\nLet the state vector be $x \\in \\mathbb{R}^n$ and the unconstrained control variables be $z \\in \\mathbb{R}^n$. The objective function $J(x)$ is given by a standard quadratic form used in variational data assimilation:\n$$\nJ(x) = \\tfrac{1}{2} (H x - y)^\\top R^{-1} (H x - y) + \\tfrac{1}{2} (x - m)^\\top B^{-1} (x - m)\n$$\nwhere $H$ is the observation operator, $y$ are observations, $R^{-1}$ is the inverse observation error covariance, $m$ is the background state, and $B^{-1}$ is the inverse background error covariance. $R^{-1}$ and $B^{-1}$ are symmetric positive definite matrices.\n\nFirst, we compute the gradient and Hessian of $J$ with respect to the state variables $x$.\nThe gradient, denoted $g_x = \\nabla_x J$, is found by differentiating $J(x)$:\n$$\n\\nabla_x J(x) = H^\\top R^{-1} (H x - y) + B^{-1} (x - m)\n$$\nThe Hessian, denoted $H_x = \\nabla_{xx}^2 J$, is found by differentiating the gradient with respect to $x$:\n$$\n\\nabla_{xx}^2 J(x) = H^\\top R^{-1} H + B^{-1}\n$$\nSince $R^{-1}$ and $B^{-1}$ are symmetric, $H_x$ is also a symmetric matrix. Furthermore, since $B^{-1}$ is positive definite and $H^\\top R^{-1} H$ is positive semi-definite, their sum $H_x$ is positive definite.\n\nThe state variables $x$ are related to the unconstrained control variables $z$ via an exponential barrier transform, defined component-wise as:\n$$\nx_i = f_i(z_i) = c_i + s_i \\exp(z_i)\n$$\nwhere $s_i \\in \\{+1, -1\\}$ determines if the bound $c_i$ is a lower or upper bound. In vector form, we can write $x = f(z)$.\n\nTo analyze the objective function in terms of $z$, we define $\\tilde{J}(z) = J(f(z))$. The gradient and Hessian of $\\tilde{J}$ with respect to $z$ are found using the chain rule.\n\nThe Jacobian of the transformation $f(z)$ is a diagonal matrix, which we denote as $T'$. Its elements are given by:\n$$\n(T')_{ij} = \\frac{\\partial x_i}{\\partial z_j} = \\delta_{ij} \\frac{\\mathrm{d} f_i}{\\mathrm{d} z_i} = \\delta_{ij} s_i \\exp(z_i)\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. Thus, $T' = \\operatorname{diag}(s_1 \\exp(z_1), \\dots, s_n \\exp(z_n))$.\n\nThe gradient of $\\tilde{J}(z)$ is:\n$$\n\\nabla_z \\tilde{J} = (\\nabla_z f)^\\top (\\nabla_x J) = (T')^\\top g_x = T' g_x\n$$\nwhere the last equality holds because $T'$ is diagonal.\n\nThe Hessian of $\\tilde{J}(z)$, denoted $H_z = \\nabla_{zz}^2 \\tilde{J}$, requires the second-order chain rule. For the $(k,j)$-th component:\n$$\n(H_z)_{kj} = \\frac{\\partial^2 \\tilde{J}}{\\partial z_k \\partial z_j} = \\frac{\\partial}{\\partial z_k} \\left( \\sum_i \\frac{\\partial J}{\\partial x_i} \\frac{\\partial x_i}{\\partial z_j} \\right)\n$$\nApplying the product rule and chain rule again:\n$$\n(H_z)_{kj} = \\sum_i \\left( \\left( \\sum_l \\frac{\\partial^2 J}{\\partial x_l \\partial x_i} \\frac{\\partial x_l}{\\partial z_k} \\right) \\frac{\\partial x_i}{\\partial z_j} + \\frac{\\partial J}{\\partial x_i} \\frac{\\partial^2 x_i}{\\partial z_k \\partial z_j} \\right)\n$$\nLet's analyze the two terms. The first term in matrix notation is $(T')^\\top H_x T'$. Since $T'$ is diagonal, this is $T' H_x T'$.\nThe second term involves the second derivatives of the transform. Since $x_i$ depends only on $z_i$, we have:\n$$\n\\frac{\\partial^2 x_i}{\\partial z_k \\partial z_j} = \\delta_{ik} \\delta_{ij} \\frac{\\mathrm{d}^2 f_i}{\\mathrm{d} z_i^2} = \\delta_{ik} \\delta_{ij} s_i \\exp(z_i)\n$$\nThis term is non-zero only when $i=j=k$. The contribution to the Hessian is a diagonal matrix. The $(j,j)$ diagonal element is the sum over $i$, which is non-zero only for $i=j$, giving: $(\\nabla_x J)_j \\cdot (s_j \\exp(z_j))$.\nThis term represents the curvature induced by the barrier transform itself, and it depends on the gradient of the original cost function.\n\nCombining the terms, the full Hessian $H_z$ is:\n$$\nH_z = T' H_x T' + C_{barrier}\n$$\nwhere $T' = \\operatorname{diag}(s \\odot \\exp(z))$ and $C_{barrier}$ is a diagonal matrix representing the barrier curvature:\n$$\nC_{barrier} = \\operatorname{diag}(g_x \\odot s \\odot \\exp(z))\n$$\nHere, $\\odot$ denotes the element-wise (Hadamard) product. Both terms in the expression for $H_z$ contribute to a symmetric matrix, so $H_z$ is symmetric.\n\nThe procedure to compute the required ratio for each test case is as follows:\n1.  Given the input parameters $H, R^{-1}, B^{-1}, y, m, s, c, z$.\n2.  Compute the state vector $x = c + s \\odot \\exp(z)$.\n3.  Compute the gradient in the original variables: $g_x = H^\\top R^{-1} (H x - y) + B^{-1} (x - m)$.\n4.  Compute the Hessian in the original variables: $H_x = H^\\top R^{-1} H + B^{-1}$.\n5.  Compute the smallest singular value of $H_x$, $\\sigma_{\\min}(H_x)$. Note that since $H_x$ is symmetric positive definite, its singular values are its eigenvalues.\n6.  Compute the Jacobian of the transform, $T' = \\operatorname{diag}(s \\odot \\exp(z))$.\n7.  Compute the barrier curvature matrix, $C_{barrier} = \\operatorname{diag}(g_x \\odot s \\odot \\exp(z))$.\n8.  Assemble the Hessian in the transformed variables: $H_z = T' H_x T' + C_{barrier}$.\n9.  Compute the smallest singular value of $H_z$, $\\sigma_{\\min}(H_z)$. Since $H_z$ is symmetric but not guaranteed to be positive definite, its singular values are the absolute values of its eigenvalues.\n10. Calculate the final ratio $\\sigma_{\\min}(H_z) / \\sigma_{\\min}(H_x)$.\n\nThis algorithm will be implemented numerically to solve the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"R_inv\": np.diag([1, 0.5, 0.2]),\n            \"B_inv\": np.diag([0.1, 0.1, 0.1]),\n            \"y\": np.array([2, 1, -1]),\n            \"m_bg\": np.array([0.5, -0.5, 0]),\n            \"s\": np.array([1, 1, 1]),\n            \"c\": np.array([0, 0, 0]),\n            \"z\": np.array([0, 0, 0]),\n        },\n        # Test case 2\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"R_inv\": np.diag([1, 0.5, 0.2]),\n            \"B_inv\": np.diag([0.1, 0.1, 0.1]),\n            \"y\": np.array([2, 1, -1]),\n            \"m_bg\": np.array([0.5, -0.5, 0]),\n            \"s\": np.array([-1, -1, -1]),\n            \"c\": np.array([3, 3, 3]),\n            \"z\": np.array([1.0, -0.5, 0.5]),\n        },\n        # Test case 3\n        {\n            \"H\": np.array([[1, 2, 0], [0, 1, 1], [1, 0, 1]]),\n            \"R_inv\": np.diag([0.5, 1.0, 2.0]),\n            \"B_inv\": np.diag([0.05, 0.2, 0.5]),\n            \"y\": np.array([1.0, -2.0, 0.5]),\n            \"m_bg\": np.array([0.0, 1.0, -0.5]),\n            \"s\": np.array([1, -1, 1]),\n            \"c\": np.array([0.0, 2.0, -1.0]),\n            \"z\": np.array([0.2, -1.0, 0.7]),\n        },\n        # Test case 4\n        {\n            \"H\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]),\n            \"R_inv\": np.diag([1, 0.5, 0.2]),\n            \"B_inv\": np.diag([0.1, 0.1, 0.1]),\n            \"y\": np.array([2, 1, -1]),\n            \"m_bg\": np.array([0.5, -0.5, 0]),\n            \"s\": np.array([1, 1, 1]),\n            \"c\": np.array([0, 0, 0]),\n            \"z\": np.array([-10.0, -5.0, -1.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        H, R_inv, B_inv, y, m_bg, s, c, z = (\n            case[\"H\"],\n            case[\"R_inv\"],\n            case[\"B_inv\"],\n            case[\"y\"],\n            case[\"m_bg\"],\n            case[\"s\"],\n            case[\"c\"],\n            case[\"z\"],\n        )\n\n        # 1. Compute state vector x from control vector z\n        exp_z = np.exp(z)\n        x = c + s * exp_z\n\n        # 2. Compute gradient (g_x) and Hessian (H_x) in original variables\n        # Gradient: g_x = H.T @ R_inv @ (H @ x - y) + B_inv @ (x - m_bg)\n        g_x = H.T @ R_inv @ (H @ x - y) + B_inv @ (x - m_bg)\n        # Hessian: H_x = H.T @ R_inv @ H + B_inv\n        H_x = H.T @ R_inv @ H + B_inv\n\n        # 3. Compute smallest singular value of H_x\n        # For a symmetric matrix, singular values are the abs of eigenvalues.\n        # Since H_x is SPD, singular values are the eigenvalues.\n        # np.linalg.svd returns only the singular values if compute_uv=False.\n        singular_values_x = np.linalg.svd(H_x, compute_uv=False)\n        sigma_min_x = np.min(singular_values_x)\n\n        # 4. Compute Jacobian of transform and barrier curvature term\n        # Jacobian of transform is T' = diag(s * exp(z))\n        t_prime_diag = s * exp_z\n        T_prime = np.diag(t_prime_diag)\n\n        # Barrier curvature matrix: C_barrier = diag(g_x * s * exp(z))\n        C_barrier_diag = g_x * t_prime_diag\n        C_barrier = np.diag(C_barrier_diag)\n\n        # 5. Assemble Hessian in transformed variables: H_z = T' @ H_x @ T' + C_barrier\n        H_z = T_prime @ H_x @ T_prime + C_barrier\n        \n        # 6. Compute smallest singular value of H_z\n        singular_values_z = np.linalg.svd(H_z, compute_uv=False)\n        sigma_min_z = np.min(singular_values_z)\n\n        # 7. Compute the ratio\n        if sigma_min_x == 0:\n            # Avoid division by zero, although H_x is positive definite so this won't happen.\n            ratio = np.inf\n        else:\n            ratio = sigma_min_z / sigma_min_x\n        \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}