{
    "hands_on_practices": [
        {
            "introduction": "变分数据同化的核心是最小化一个代价函数，这需要计算其梯度。伴随方法为高效计算梯度提供了强大的工具。本练习将通过一个经典的一维线性平流方程，引导您手工推导离散伴随模型。通过这个看似简单的例子，您将深入理解离散伴随方法（“先离散后优化”策略）的构建过程及其与正向模型数值格式的内在联系，为后续更复杂的实践打下坚实的理论基础。",
            "id": "3408488",
            "problem": "考虑线性平流方程 $u_{t} + a\\,u_{x} = 0$，定义在周期性区域 $x \\in [0,L]$ 上，其中波速 $a > 0$ 为常数。设空间网格是均匀的，有 $N_{x}$ 个点，间距为 $\\Delta x = L/N_{x}$，并采用周期性索引。设时间步长为 $\\Delta t$，定义 Courant 数为 $C = a\\,\\Delta t/\\Delta x$。考虑采用空间一阶迎风有限差分格式和时间前向欧拉法进行状态更新：\n$$\nu^{n+1}_{j} = u^{n}_{j} - C\\left(u^{n}_{j} - u^{n}_{j-1}\\right), \\quad j = 0,\\dots,N_{x}-1,\n$$\n其中 $j-1$ 采用周期性索引。假设在一个四维变分 (4D-Var) 数据同化设定中，一个二次代价泛函依赖于初始条件 $u^{0}$ 和最终时间层 $n = N_{t}$ 的模型状态，通过一个线性观测算子 $H$ 和正定协方差矩阵，但除了线性和正定性之外，不假设这些项的任何特定形式。使用离散内积 $\\langle p, q\\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_{x}-1} p_{j} q_{j}$。\n\n任务：\n1. 从一个离散拉格朗日量的定义出发，该拉格朗日量在每个时间步使用拉格朗日乘子 $\\lambda^{n+1}$ 来强制执行模型更新约束 $u^{n+1} - M u^{n} = 0$（其中 $M$ 表示由上述格式定义的线性更新算子）。推导关于内积 $\\langle \\cdot, \\cdot\\rangle_{\\Delta x}$ 的 $\\lambda^{n}$ 的离散伴随递归关系。将伴随更新明确地表示为 $\\lambda^{n}_{j}$ 关于 $\\lambda^{n+1}$ 的逐点模板。\n2. 在周期性边界条件下，对齐次伴随递归（无源项）进行 von Neumann 分析，考虑形式为 $\\varphi_{j}(\\theta) = \\exp(i \\theta j)$（其中 $\\theta \\in [-\\pi,\\pi]$）的离散傅里叶模态。推导伴随放大因子 $G_{\\mathrm{adj}}(\\theta; C)$ 及其模 $|G_{\\mathrm{adj}}(\\theta; C)|$ 作为 $\\theta$ 和 $C$ 的函数。\n3. 使用模 $|G_{\\mathrm{adj}}(\\theta; C)|$，确定 $C$ 应满足的条件，使得对于所有傅里叶模态，伴随更新在由 $\\langle \\cdot, \\cdot\\rangle_{\\Delta x}$ 导出的离散 $\\ell^{2}$ 范数下是非扩张的，即对于所有 $\\theta \\in [-\\pi,\\pi]$，有 $|G_{\\mathrm{adj}}(\\theta; C)| \\leq 1$。\n4. 满足此非扩张条件的最大允许 Courant 数 $C_{\\max}$ 是多少？以无单位的单个实数形式报告 $C_{\\max}$。\n\n你的最终答案必须仅为单个数字 $C_{\\max}$。无需四舍五入。",
            "solution": "用户提供了一个来自数值分析和数据同化领域的有效且适定的问题陈述。该问题具有科学依据，并包含了推导唯一解所需的所有必要信息。以下步骤将导向最终答案。\n\n### 任务1：离散伴随递归的推导\n\n正向模型是线性平流方程 $u_t + a u_x = 0$ 的离散近似。离散化由空间上的一阶迎风格式和时间上的前向欧拉法给出：\n$$\nu^{n+1}_{j} = u^{n}_{j} - C \\left(u^{n}_{j} - u^{n}_{j-1}\\right)\n$$\n其中 $C = a \\Delta t / \\Delta x$ 是 Courant 数， $u^n_j$ 是在时间层 $n$ 和空间网格点 $j$ 上的状态。我们可以将其重写为线性更新规则：\n$$\nu^{n+1}_{j} = (1-C) u^{n}_{j} + C u^{n}_{j-1}\n$$\n以向量形式，这可以写成 $u^{n+1} = M u^n$，其中 $u^n$ 是时间 $n$ 时所有空间值的向量，而 $M$ 是一个线性算子，其作用于向量 $v$ 的结果由 $(Mv)_j = (1-C)v_j + C v_{j-1}$ 给出。\n\n问题指定了一个 4D-Var 设定，其中模型方程被视为约束。构造一个拉格朗日量 $\\mathcal{L}$，它包括一个代价函数 $J(u^0, u^{N_t})$ 和在每个时间步由拉格朗日乘子 $\\lambda^{n+1}$ 强制执行的约束项：\n$$\n\\mathcal{L}(u^0, \\dots, u^{N_t}; \\lambda^1, \\dots, \\lambda^{N_t}) = J(u^0, u^{N_t}) + \\sum_{n=0}^{N_t-1} \\langle \\lambda^{n+1}, u^{n+1} - M u^n \\rangle_{\\Delta x}\n$$\n内积定义为 $\\langle p, q\\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_{x}-1} p_{j} q_{j}$。伴随方程是通过求 $\\mathcal{L}$ 关于状态变量 $u^n$（对于 $n = 1, \\dots, N_t-1$）的 Fréchet 导数并将其设为零来找到的。对于一个给定的中间状态 $u^n$，它出现在求和中的两项里：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u^n} = \\frac{\\partial}{\\partial u^n} \\left( \\langle \\lambda^n, u^n - M u^{n-1} \\rangle_{\\Delta x} + \\langle \\lambda^{n+1}, u^{n+1} - M u^n \\rangle_{\\Delta x} \\right) = 0\n$$\n$\\langle p, q \\rangle$ 关于 $q$ 的导数是 $p$。因此，第一项关于 $u^n$ 的导数是 $\\lambda^n$。对于第二项，$\\langle \\lambda^{n+1}, -M u^n \\rangle_{\\Delta x} = \\langle -M^* \\lambda^{n+1}, u^n \\rangle_{\\Delta x}$，其中 $M^*$ 是 $M$ 关于内积 $\\langle \\cdot, \\cdot \\rangle_{\\Delta x}$ 的伴随算子。这一项关于 $u^n$ 的导数是 $-M^* \\lambda^{n+1}$。将总导数设为零，得到伴随递归关系：\n$$\n\\lambda^n - M^* \\lambda^{n+1} = 0 \\quad \\implies \\quad \\lambda^n = M^* \\lambda^{n+1}\n$$\n为了找到伴随算子 $M^*$ 的显式模板，我们使用其定义：对于任意向量 $p, q$，有 $\\langle M^* p, q \\rangle_{\\Delta x} = \\langle p, M q \\rangle_{\\Delta x}$。\n$$\n\\langle p, M q \\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_x-1} p_j (Mq)_j = \\Delta x \\sum_{j=0}^{N_x-1} p_j \\left( (1-C)q_j + C q_{j-1} \\right)\n$$\n$$\n= \\Delta x \\sum_{j=0}^{N_x-1} (1-C) p_j q_j + \\Delta x \\sum_{j=0}^{N_x-1} C p_j q_{j-1}\n$$\n我们对第二个和式进行索引更换。令 $k = j-1$。由于周期性边界条件，$j=k+1$，对 $k$ 从 $0$ 到 $N_x-1$ 求和等价于对 $j$ 求和。\n$$\n= \\Delta x \\sum_{j=0}^{N_x-1} (1-C) p_j q_j + \\Delta x \\sum_{k=0}^{N_x-1} C p_{k+1} q_k = \\Delta x \\sum_{j=0}^{N_x-1} \\left( (1-C)p_j + C p_{j+1} \\right) q_j\n$$\n将其与 $\\langle M^* p, q \\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_x-1} (M^*p)_j q_j$ 进行比较，我们确定伴随算子的作用为：\n$$\n(M^*p)_j = (1-C)p_j + C p_{j+1}\n$$\n因此，表示为 $\\lambda_j^n$ 逐点模板的伴随递归关系是：\n$$\n\\lambda^n_j = (1-C)\\lambda^{n+1}_j + C \\lambda^{n+1}_{j+1}\n$$\n这是一个一阶下风格式，它是一阶迎风正向模型的伴随。\n\n### 任务2：伴随递归的 Von Neumann 分析\n\n为了进行 von Neumann 稳定性分析，我们考察单个离散傅里叶模态 $\\lambda_j = \\exp(i\\theta j)$ 在一个从 $n+1$ 到 $n$ 的后向时间步长内的放大情况。设 $\\lambda_j^{n+1} = \\exp(i\\theta j)$ 和 $\\lambda_j^n = G_{\\mathrm{adj}}(\\theta; C) \\exp(i\\theta j)$，其中 $G_{\\mathrm{adj}}(\\theta; C)$ 是伴随放大因子。将这些代入伴随递归关系中：\n$$\nG_{\\mathrm{adj}}(\\theta; C) \\exp(i\\theta j) = (1-C) \\exp(i\\theta j) + C \\exp(i\\theta(j+1))\n$$\n两边同除以 $\\exp(i\\theta j)$，得到放大因子的表达式：\n$$\nG_{\\mathrm{adj}}(\\theta; C) = (1-C) + C \\exp(i\\theta) = (1-C) + C(\\cos\\theta + i\\sin\\theta)\n$$\n$$\nG_{\\mathrm{adj}}(\\theta; C) = (1-C+C\\cos\\theta) + i(C\\sin\\theta)\n$$\n放大因子的模的平方是 $|G_{\\mathrm{adj}}(\\theta; C)|^2$：\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = (1-C+C\\cos\\theta)^2 + (C\\sin\\theta)^2\n$$\n$$\n= (1-C)^2 + 2C(1-C)\\cos\\theta + C^2\\cos^2\\theta + C^2\\sin^2\\theta\n$$\n$$\n= 1-2C+C^2 + 2C(1-C)\\cos\\theta + C^2(\\cos^2\\theta+\\sin^2\\theta)\n$$\n这个推导有误。让我们重新展开：\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = (1-C)^2 + 2(1-C)C\\cos(\\theta) + C^2\\cos^2(\\theta) + C^2\\sin^2(\\theta)\n$$\n$$\n= (1-2C+C^2) + (2C-2C^2)\\cos(\\theta) + C^2\n$$\n$$\n= 1 - 2C + 2C^2 + (2C - 2C^2)\\cos(\\theta)\n$$\n$$\n= 1 + (2C^2 - 2C) - (2C^2 - 2C)\\cos(\\theta)\n$$\n$$\n= 1 + 2C(C-1)(1-\\cos\\theta)\n$$\n使用半角恒等式 $1-\\cos\\theta = 2\\sin^2(\\theta/2)$，我们得到：\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = 1 + 4C(C-1)\\sin^2(\\theta/2)\n$$\n模为 $|G_{\\mathrm{adj}}(\\theta; C)| = \\sqrt{1 + 4C(C-1)\\sin^2(\\theta/2)}$。\n\n### 任务3：非扩张条件\n\n如果对于所有傅里叶模态，放大因子的模小于或等于 $1$，即对于所有 $\\theta \\in [-\\pi, \\pi]$，有 $|G_{\\mathrm{adj}}(\\theta; C)| \\le 1$，则伴随更新是非扩张的。这等价于 $|G_{\\mathrm{adj}}(\\theta; C)|^2 \\le 1$。\n$$\n1 + 4C(C-1)\\sin^2(\\theta/2) \\leq 1\n$$\n两边减去 $1$ 得到：\n$$\n4C(C-1)\\sin^2(\\theta/2) \\leq 0\n$$\n项 $\\sin^2(\\theta/2)$ 对所有实数 $\\theta$ 都是非负的。常数 $4$ 是正的。问题陈述 $a>0$，这意味着 $C = a\\Delta t/\\Delta x > 0$，因为 $\\Delta t$ 和 $\\Delta x$ 都是正的。为了使这个不等式对所有 $\\theta$（包括那些 $\\sin^2(\\theta/2) > 0$ 的情况）都成立，我们必须有：\n$$\nC(C-1) \\leq 0\n$$\n由于 $C > 0$，我们可以用 $C$ 除以两边而不改变不等式的方向：\n$$\nC - 1 \\leq 0 \\implies C \\leq 1\n$$\n将其与物理约束 $C > 0$ 相结合，伴随更新为非扩张的条件是 $0 < C \\leq 1$。\n\n### 任务4：最大允许 Courant 数\n\n非扩张的条件是 Courant 数 $C$ 必须在区间 $(0, 1]$ 内。满足此条件的最大 $C$ 值是 $1$。\n因此，最大允许 Courant 数 $C_{\\max}$ 是 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "在实际应用中，推导和编写复杂的伴随模型代码极易出错。因此，如何验证梯度计算的正确性，是每个实践者必须掌握的关键技能。本练习将指导您实现泰勒余项检验（Taylor Remainder Test），这是一种强大而通用的数值方法，用于验证您所实现的梯度是否与代价函数的实际变化率相符。通过对比正确梯度和有意引入错误的梯度所产生的不同数值收敛率，您将深刻体会到梯度检验在代码调试和质量控制中的重要性。",
            "id": "3408591",
            "problem": "构建一个完整、可运行的程序，使用泰勒余数检验来验证一个变分数据同化目标函数的伴随导出梯度的二阶一致性。从强约束变分数据同化目标函数的基本定义和标量场的泰勒定理出发。您必须仅使用核心定义和链式法则推导梯度的算法形式，然后设计数值实验，以验证在梯度正确时泰勒余数的预期二阶行为，并在梯度被有意扰动时检测其退化。\n\n给定以下设定。设状态维度为 $n=5$，观测维度为 $m=3$。定义背景项，其中背景状态为 $x_b \\in \\mathbb{R}^n$，对称正定背景协方差为 $B \\in \\mathbb{R}^{n \\times n}$。定义前向模型（单步模型算子）$M:\\mathbb{R}^n \\to \\mathbb{R}^n$ 和观测算子 $H:\\mathbb{R}^n \\to \\mathbb{R}^m$。设观测为 $y \\in \\mathbb{R}^m$，对称正定观测协方差为 $R \\in \\mathbb{R}^{m \\times m}$。考虑目标函数\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big).\n$$\n使用以下具体的、完全指定的要素：\n\n- 维度：$n=5$，$m=3$。\n- 时间步长：$\\Delta t = 0.1$。\n- 背景和观测协方差：按如下方式确定性地构造 $B$ 和 $R$。设 $L_B \\in \\mathbb{R}^{n \\times n}$ 是一个下三角矩阵，其对角线元素为 $[1.0,\\,1.2,\\,1.4,\\,1.6,\\,1.8]$，次对角线元素为 $0.05$（所有其他元素为 $0$）。令 $B = L_B L_B^\\top$。设 $L_R \\in \\mathbb{R}^{m \\times m}$ 是一个下三角矩阵，其对角线元素为 $[0.7,\\,0.9,\\,1.1]$，次对角线元素为 $0.02$，并令 $R = L_R L_R^\\top$。\n- 背景状态：$x_b = [0.5,\\,-0.3,\\,0.8,\\,-1.0,\\,0.2]^\\top$。\n- 真值和观测：设 $x_{\\text{true}} = [-0.4,\\,0.7,\\,-0.2,\\,0.3,\\,-0.6]^\\top$，定义 $y = H(M(x_{\\text{true}}))$，不添加噪声。\n- 前向模型：对于任意 $x \\in \\mathbb{R}^n$，定义 $f(x) = \\sin(x) + 0.1\\,x \\odot x$，其中 $\\sin(\\cdot)$ 和 $\\odot$ 逐元素作用，并设 $M(x) = x + \\Delta t\\, f(x)$。\n- 观测算子：设 $C \\in \\mathbb{R}^{m \\times n}$ 为\n$$\nC = \\begin{bmatrix}\n1  0  0.2  0  0\\\\\n0  0.5  0  0.1  0\\\\\n0  0  0  0.3  1\n\\end{bmatrix},\n$$\n并定义观测算子 $H: \\mathbb{R}^n \\to \\mathbb{R}^m$，其分量形式为 $H(z)_i = (Cz)_i + 0.05 z_i^2$，对于 $i=1, \\dots, m$。\n- 评估状态：定义 $x_0 = [0.1,\\, -0.2,\\, 0.3,\\, -0.4,\\, 0.5]^\\top$。\n- 方向向量：设 $p$ 为一个确定性单位向量，定义为 $p = \\frac{1}{\\|v\\|}v$，其中 $v = [1.0,\\, -2.0,\\, 3.0,\\, -4.0,\\, 5.0]^\\top$。\n\n使用基本的矩阵微积分和链式法则，将梯度 $\\nabla J(x)$ 表示为 $B^{-1}$、$R^{-1}$ 以及 $M$ 和 $H$ 的雅可比矩阵的函数。然后，基于标量场泰勒定理的恒等式实现泰勒余数检验：\n$$\nJ(x + \\epsilon p) - J(x) \\;=\\; \\epsilon\\, \\nabla J(x)^\\top p \\;+\\; \\mathcal{O}(\\epsilon^2),\n$$\n对于小的 $|\\epsilon|$，使用一系列 $\\epsilon$ 值。对于每次检验，计算一阶余数\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x + \\epsilon p) - J(x) - \\epsilon\\, \\nabla J(x)^\\top p\\right|.\n$$\n通过对 $\\log r_1(\\epsilon)$ 与 $\\log \\epsilon$ 的关系进行直线拟合来估计经验收敛率 $\\alpha$，并报告斜率 $\\alpha$，对于正确的梯度，该斜率应约等于 $2$。\n\n设计并执行以下测试套件，该套件必须内置于程序中，无需用户输入：\n\n- 测试 A（非线性，正确梯度）：使用上面定义的 $M$ 和 $H$ 以及解析推导的正确 $\\nabla J(x)$。在 $x=x_0$ 处沿方向 $p$ 评估斜率 $\\alpha_A$，使用一组在 $[10^{-1},\\, 10^{-5}]$ 范围内的递减 $\\epsilon$ 值，其中包含 $10^{-k}$ 和适用的几何中间值（如 $3\\times 10^{-k}$，其中 $k$ 为整数）。\n- 测试 B（非线性，有意错误的梯度）：重复测试 A，但在梯度组装中，用一个故意不正确的变体替换伴随传播步骤，即在需要 $M'(x)^\\top$ 的地方使用 $M'(x)$。将得到的斜率记为 $\\alpha_B$；它应该接近 $1$ 而不是 $2$。\n- 测试 C（线性二次参考）：定义一个线性模型和观测 $M_{\\text{lin}}(x) = A x$ 和 $H_{\\text{lin}}(z) = C z$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是\n$$\nA = I + \\Delta t\\, \\mathrm{diag}([0.2,\\,-0.1,\\,0.05,\\,-0.2,\\,0.1]),\n$$\n其中 $I$ 是单位矩阵，$\\mathrm{diag}(\\cdot)$ 是具有给定条目的对角矩阵。使用相同的 $C$、$B$、$R$、$x_b$、$x_0$ 和 $p$，并定义 $y_{\\text{lin}} = H_{\\text{lin}}(M_{\\text{lin}}(x_{\\text{true}}))$。在此线性二次设定中使用精确梯度计算斜率 $\\alpha_C$；在舍入误差占主导地位之前，它应该接近 $2$。\n- 测试 D（正交方向）：回到非线性的 $M$ 和 $H$，构造一个单位方向 $p_\\perp$，使得 $\\nabla J(x_0)^\\top p_\\perp = 0$ 在数值上成立，方法是将 $p$ 投影到与 $\\nabla J(x_0)$ 正交的方向并进行归一化。使用 $r_1(\\epsilon)$ 计算斜率 $\\alpha_D$；它应该接近 $2$，因为一阶项在此方向上消失了。\n\n您的程序必须：\n\n- 仅使用基本线性代数和应用于指定的 $M$ 和 $H$ 的链式法则来实现 $J(x)$ 及其梯度。\n- 通过在伴随累积中用 $M'(x)$ 替换 $M'(x)^\\top$ 来实现测试 B 的错误梯度。\n- 使用一个固定的 $\\epsilon$ 值序列，包含 $[10^{-1},\\,3\\cdot 10^{-2},\\,10^{-2},\\,3\\cdot 10^{-3},\\,10^{-3},\\,3\\cdot 10^{-4},\\,10^{-4},\\,3\\cdot 10^{-5},\\,10^{-5}]$。\n- 通过对整个序列中 $\\log r_1(\\epsilon)$ 对 $\\log \\epsilon$ 进行最小二乘线性回归来估计斜率 $\\alpha_A, \\alpha_B, \\alpha_C, \\alpha_D$，排除任何因 $r_1(\\epsilon)$ 在浮点运算中下溢为零的 $\\epsilon$ 值。\n\n最终输出格式：您的程序应生成单行输出，其中包含四个估计的斜率，四舍五入到三位小数，以逗号分隔列表的形式包含在方括号中，顺序为 $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$。不涉及任何物理单位。如果存在任何角度，必须视为无量纲实数值。输出为实数。",
            "solution": "该问题要求构建一个数值实验，以验证一个变分数据同化目标函数的伴随导出梯度的正确性。验证将通过泰勒余数检验进行，该检验依赖于一阶泰勒展开中余项的二阶行为。分析首先从梯度的形式化推导开始，然后进行四个特定数值实验的设计与实现。\n\n首先，我们建立理论基础。目标函数 $J(x)$ 被给定为背景项 $J_b(x)$ 和观测项 $J_o(x)$ 的和：\n$$\nJ(x) = J_b(x) + J_o(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n其中 $x \\in \\mathbb{R}^n$ 是控制变量（模型状态）。其他量在问题描述中已定义：$x_b$ 是背景状态，$B$ 是背景误差协方差，$y$ 是观测向量，$R$ 是观测误差协方差，$M$ 是前向模型，$H$ 是观测算子。\n\n验证方法基于标量场 $J:\\mathbb{R}^n \\to \\mathbb{R}$ 的泰勒定理。将 $J(x)$ 在点 $x_0$ 处沿方向 $p \\in \\mathbb{R}^n$ 展开：\n$$\nJ(x_0 + \\epsilon p) = J(x_0) + \\epsilon\\, \\nabla J(x_0)^\\top p + \\tfrac{\\epsilon^2}{2} p^\\top \\nabla^2 J(x_0) p + \\mathcal{O}(\\epsilon^3)\n$$\n其中 $\\epsilon$ 是一个小的标量，$\\nabla J(x_0)$ 是 $J$ 在 $x_0$ 处的梯度，$\\nabla^2 J(x_0)$ 是海森矩阵。如果我们计算的梯度是正确的，那么定义为\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x_0 + \\epsilon p) - J(x_0) - \\epsilon\\, \\nabla J(x_0)^\\top p\\right|\n$$\n的一阶余数必须由二阶项主导：\n$$\nr_1(\\epsilon) = \\left|\\tfrac{\\epsilon^2}{2} p^\\top \\nabla^2 J(x_0) p + \\mathcal{O}(\\epsilon^3)\\right| \\approx C \\epsilon^2\n$$\n对于某个常数 $C$（假设 $p^\\top \\nabla^2 J(x_0) p \\neq 0$）。取对数，我们发现 $\\log r_1(\\epsilon) \\approx \\log C + 2 \\log \\epsilon$。这意味着 $\\log r_1(\\epsilon)$ 相对于 $\\log \\epsilon$ 的图应该是一条斜率近似为 $2$ 的直线。如果计算的梯度不正确，$\\epsilon\\, \\nabla J(x_0)^\\top p$ 项将不能正确抵消 $J$ 的一阶变化，余数 $r_1(\\epsilon)$ 将为 $\\mathcal{O}(\\epsilon)$ 阶，导致斜率近似为 $1$。\n\n接下来，我们推导梯度 $\\nabla J(x)$。背景项 $J_b(x)$ 的梯度对于二次型是标准的：\n$$\n\\nabla J_b(x) = B^{-1}(x - x_b)\n$$\n对于观测项 $J_o(x)$，我们应用链式法则。设 $d(x) = H(M(x)) - y$。根据链式法则，$d(x)$ 关于 $x$ 的雅可比矩阵是 $H$ (在 $M(x)$ 处求值) 和 $M$ (在 $x$ 处求值) 的雅可比矩阵的乘积：\n$$\n\\frac{\\partial d}{\\partial x} = H'(M(x)) M'(x)\n$$\n$J_o(x) = \\frac{1}{2} d(x)^\\top R^{-1} d(x)$ 的梯度则是：\n$$\n\\nabla J_o(x) = \\left(\\frac{\\partial d}{\\partial x}\\right)^\\top R^{-1} d(x) = \\big(H'(M(x)) M'(x)\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n使用性质 $(AB)^\\top = B^\\top A^\\top$，我们得到伴随公式：\n$$\n\\nabla J_o(x) = (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n总梯度是两个分量的和：\n$$\n\\nabla J(x) = B^{-1}(x - x_b) + (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n为实现这一点，我们需要具体模型函数的雅可比矩阵。\n前向模型是 $M(x) = x + \\Delta t\\, f(x)$，其中 $f(x)_i = \\sin(x_i) + 0.1\\, x_i^2$。由于 $f$ 是逐元素作用的，其雅可比矩阵 $f'(x)$ 是一个对角矩阵，对角元素为 $\\frac{\\partial f_i}{\\partial x_i} = \\cos(x_i) + 0.2\\, x_i$。因此，$M(x)$ 的雅可比矩阵是：\n$$\nM'(x) = I + \\Delta t\\, \\mathrm{diag}\\big(\\cos(x) + 0.2\\, x\\big)\n$$\n观测算子由 $H(z)_i = (Cz)_i + 0.05 z_i^2$ 给出，对于 $i=1, \\dots, m$。其雅可比矩阵 $H'(z) \\in \\mathbb{R}^{m \\times n}$ 的元素为：\n$$\n[H'(z)]_{ij} = \\frac{\\partial H_i}{\\partial z_j} = C_{ij} + 0.1 z_i \\delta_{ij}\n$$\n其中 $\\delta_{ij}$ 是克罗内克δ函数。这意味着 $H'(z)$ 是矩阵 $C$ 加上一个在其前 $m$ 个对角元素上包含 $0.1 z_i$ 的对角矩阵。\n\n问题指定了四个测试：\n**测试 A（非线性，正确梯度）：** 此测试实现推导出的梯度 $\\nabla J(x_0)$，预计将产生约等于 $2$ 的收敛率 $\\alpha_A$。\n\n**测试 B（非线性，错误梯度）：** 此测试旨在通过用 $M'(x)$ 替换 $(M'(x))^\\top$ 来使用不正确的梯度。然而，一个关键的观察是，指定的模型 $M(x)$ 导致其雅可比矩阵是对称的，$M'(x) = (M'(x))^\\top$，因为它是单位矩阵和一个对角矩阵的和。因此，这个“错误的”梯度与正确的梯度是相同的。问题中陈述的期望 $\\alpha_B \\approx 1$ 将不会被满足；相反，我们必须预测 $\\alpha_B \\approx 2$。这个结果凸显了验证的一个重要方面：如果特定的数据或模型具有掩盖错误的对称性，测试可能无法揭示错误。\n\n**测试 C（线性二次参考）：** 对于线性模型 $M_{\\text{lin}}(x)=Ax$ 和算子 $H_{\\text{lin}}(z)=Cz$，目标函数是纯二次的。泰勒展开到二阶是精确的，意味着 $r_1(\\epsilon) = |\\frac{\\epsilon^2}{2} p^\\top \\nabla^2 J p|$。$\\log r_1(\\epsilon)$ 与 $\\log \\epsilon$ 的关系应该是斜率为精确 $2$ 的完美线性关系，仅受浮点精度限制。这为“完美”的二阶收敛提供了一个基准。\n\n**测试 D（正交方向）：** 在此测试中，方向向量 $p_\\perp$ 被构造成与梯度正交，即 $\\nabla J(x_0)^\\top p_\\perp = 0$。泰勒展开随后简化为 $J(x_0 + \\epsilon p_\\perp) - J(x_0) = \\mathcal{O}(\\epsilon^2)$，余数变为 $r_1(\\epsilon) = |J(x_0 + \\epsilon p_\\perp) - J(x_0)|$。此余数仍为 $\\mathcal{O}(\\epsilon^2)$ 阶，因此斜率 $\\alpha_D$ 预计接近 $2$。此测试证实了即使在一阶项恒为零的情况下，函数的二次行为也能被正确捕捉。\n\n程序通过定义 $J$、$\\nabla J$、模型及其雅可比矩阵的函数来实现这些推导和测试。然后，它对四个测试中的每一个遍历指定的 $\\epsilon$ 值，计算余数，并使用 `numpy.polyfit` 对数据的对数进行最小二乘线性回归，以估计收敛斜率 $\\alpha_A, \\alpha_B, \\alpha_C$ 和 $\\alpha_D$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not strictly needed as numpy.polyfit is sufficient.\n\ndef solve():\n    \"\"\"\n    Constructs and executes a test suite to verify an adjoint-derived gradient\n    for a variational data assimilation objective function using Taylor remainder tests.\n    \"\"\"\n    # --- Problem Setup: Constants and Givens ---\n    n = 5  # State dimension\n    m = 3  # Observation dimension\n    delta_t = 0.1\n\n    # Background state\n    xb = np.array([0.5, -0.3, 0.8, -1.0, 0.2])\n    # True state for generating observations\n    xtrue = np.array([-0.4, 0.7, -0.2, 0.3, -0.6])\n    # State for evaluation\n    x0 = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\n\n    # Direction vector p\n    v = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    p = v / np.linalg.norm(v)\n\n    # Covariance matrices B and R\n    L_B = np.diag([1.0, 1.2, 1.4, 1.6, 1.8])\n    for i in range(1, n):\n        L_B[i, i - 1] = 0.05\n    B = L_B @ L_B.T\n    B_inv = np.linalg.inv(B)\n\n    L_R = np.diag([0.7, 0.9, 1.1])\n    for i in range(1, m):\n        L_R[i, i - 1] = 0.02\n    R = L_R @ L_R.T\n    R_inv = np.linalg.inv(R)\n\n    # Observation operator matrix C\n    C = np.array([\n        [1, 0, 0.2, 0, 0],\n        [0, 0.5, 0, 0.1, 0],\n        [0, 0, 0, 0.3, 1]\n    ])\n\n    # Linear model matrix A for Test C\n    A = np.eye(n) + delta_t * np.diag([0.2, -0.1, 0.05, -0.2, 0.1])\n    \n    # Epsilon values for Taylor test\n    epsilons = np.array([\n        1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5\n    ])\n\n    # --- Model and Operator Definitions ---\n\n    # Nonlinear forward model M(x)\n    def M_nl(x):\n        f = np.sin(x) + 0.1 * x**2\n        return x + delta_t * f\n\n    # Nonlinear observation operator H(z)\n    def H_nl(z):\n        h = C @ z\n        h += 0.05 * z[:m]**2\n        return h\n\n    # Jacobian of M_nl(x)\n    def M_nl_jac(x):\n        diag_f_prime = np.cos(x) + 0.2 * x\n        return np.eye(n) + delta_t * np.diag(diag_f_prime)\n\n    # Jacobian of H_nl(z)\n    def H_nl_jac(z):\n        jac = np.copy(C)\n        jac[:m, :m] += np.diag(0.1 * z[:m])\n        return jac\n\n    # Linear model and operator for Test C\n    def M_lin(x):\n        return A @ x\n    def H_lin(z):\n        return C @ z\n\n    # --- Objective Function and Gradient ---\n\n    def objective_J(x, y, M_func, H_func, B_inv, R_inv, xb):\n        J_b = 0.5 * (x - xb).T @ B_inv @ (x - xb)\n        hx = H_func(M_func(x))\n        J_o = 0.5 * (hx - y).T @ R_inv @ (hx - y)\n        return J_b + J_o\n\n    def gradient_J(x, y, M_func, H_func, B_inv, R_inv, xb, M_jac_T_provider, H_jac_func):\n        # Background gradient component\n        grad_b = B_inv @ (x - xb)\n        \n        # Observation gradient component (adjoint formulation)\n        z = M_func(x)\n        hx = H_func(z)\n        H_jac_val = H_jac_func(z) # H'(M(x))\n        M_jac_T_val = M_jac_T_provider(x) # M'(x)^T (or faulty version)\n        \n        # dJ/dH * dH/dz * dz/dx\n        # Adjoint variable: (H'(z))^T * R^-1 * (H(z) - y)\n        adj = H_jac_val.T @ (R_inv @ (hx - y))\n        # Propagate adjoint variable back through model: (M'(x))^T * adj\n        grad_o = M_jac_T_val @ adj\n        \n        return grad_b + grad_o\n\n    # --- Test Execution ---\n    \n    def run_taylor_test(x0_test, p_test, grad_j_val, M_func, H_func, y_test, B_inv, R_inv, xb):\n        J0 = objective_J(x0_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n        gradJ_p = grad_j_val.T @ p_test\n        \n        log_eps, log_remainders = [], []\n        for eps in epsilons:\n            J_eps = objective_J(x0_test + eps * p_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n            remainder = abs(J_eps - J0 - eps * gradJ_p)\n            if remainder > 0:\n                log_eps.append(np.log(eps))\n                log_remainders.append(np.log(remainder))\n        \n        # Perform linear regression to find slope\n        if len(log_eps) > 1:\n            slope, _ = np.polyfit(log_eps, log_remainders, 1)\n            return slope\n        return np.nan\n\n    all_slopes = []\n\n    # Generate observation data `y` for nonlinear tests\n    y_nl = H_nl(M_nl(xtrue))\n\n    # --- Test A: Nonlinear, Correct Gradient ---\n    grad_A = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x).T, H_nl_jac)\n    alpha_A = run_taylor_test(x0, p, grad_A, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_A)\n\n    # --- Test B: Nonlinear, Faulty Gradient ---\n    # Faulty implementation using M'(x) instead of M'(x)^T\n    # Note: For this problem's M, M'(x) is symmetric, so this \"faulty\"\n    # gradient is identical to the correct one. Expected slope is ~2.\n    grad_B = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x), H_nl_jac) # Using jacobian, not its transpose\n    alpha_B = run_taylor_test(x0, p, grad_B, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_B)\n\n    # --- Test C: Linear-Quadratic Reference ---\n    y_lin = H_lin(M_lin(xtrue))\n    \n    def M_lin_jac_T(x): # M_lin'(x) = A, so M_lin'(x)^T = A^T\n        return A.T\n    def H_lin_jac(z): # H_lin'(z) = C\n        return C\n        \n    grad_C = gradient_J(x0, y_lin, M_lin, H_lin, B_inv, R_inv, xb, M_lin_jac_T, H_lin_jac)\n    alpha_C = run_taylor_test(x0, p, grad_C, M_lin, H_lin, y_lin, B_inv, R_inv, xb)\n    all_slopes.append(alpha_C)\n    \n    # --- Test D: Orthogonal Direction ---\n    g_nl = grad_A # Use correct nonlinear gradient\n    # Project p to be orthogonal to the gradient g_nl\n    p_ortho = p - (g_nl.T @ p / (g_nl.T @ g_nl)) * g_nl\n    p_ortho_norm = p_ortho / np.linalg.norm(p_ortho)\n    \n    alpha_D = run_taylor_test(x0, p_ortho_norm, g_nl, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_D)\n\n    # --- Final Output ---\n    formatted_slopes = [f\"{s:.3f}\" for s in all_slopes]\n    print(f\"[{','.join(formatted_slopes)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在获得并验证了梯度之后，下一步就是利用它来驱动优化算法。对于大规模数据同化问题，由于维度极高，显式地构建和存储优化问题中的矩阵（如Hessian矩阵）是完全不可行的。本练习模拟了共轭梯度法（四维变分场中常用的求解器）的一次迭代，强制您采用“无矩阵”（matrix-free）的思维方式，即将算子作用于向量的过程视为一系列矩阵向量乘积的组合。这个实践将让您一窥真实世界数据同化系统计算核心的运作机制。",
            "id": "3408588",
            "problem": "给定一个线性化增量四维变分 (4D-Var) 数据同化子问题，该子问题处于预处理控制变量的单次内循环迭代中。令状态增量为 $\\,\\delta x \\in \\mathbb{R}^n\\,$，预处理控制变量为 $\\,v \\in \\mathbb{R}^n\\,$，其定义为 $\\,\\delta x = B^{1/2} v\\,$。其中 $\\,B \\in \\mathbb{R}^{n \\times n}\\,$ 是背景误差协方差，$\\,B^{1/2}\\,$ 是其对称正定平方根。令 $\\,M \\in \\mathbb{R}^{n \\times n}\\,$ 表示数据同化窗口上的切线性模型算子，$\\,H \\in \\mathbb{R}^{m \\times n}\\,$ 表示线性化观测算子，$\\,R \\in \\mathbb{R}^{m \\times m}\\,$ 表示观测误差协方差，其对称正定平方根为 $\\,R^{1/2}\\,$，逆平方根为 $\\,R^{-1/2}\\,$。在预处理变量中，背景项为 $\\,\\tfrac{1}{2}\\lVert v - v_b \\rVert_2^2\\,$，观测失配项为 $\\,\\tfrac{1}{2}\\lVert R^{-1/2}(H M B^{1/2} v - d)\\rVert_2^2\\,$，其中给定了 $\\,v_b \\in \\mathbb{R}^n\\,$ 和 $\\,d \\in \\mathbb{R}^m\\,$。\n\n从关于 $\\,v\\,$ 的二次目标函数出发，内循环线性系统的一阶最优性（正规）方程具有以下形式\n$$\nA v = b,\n$$\n其中\n$$\nA \\equiv I + Z^\\top Z,\\quad Z \\equiv R^{-1/2} H M B^{1/2},\\quad b \\equiv v_b + Z^\\top R^{-1/2} d,\n$$\n且 $\\,I\\,$ 是 $\\,\\mathbb{R}^n\\,$ 上的单位矩阵。根据构造，矩阵 $\\,A\\,$ 是对称正定的。\n\n您的任务是编写一个程序，执行一步共轭梯度 (CG) 方法来求解对称正定线性系统 $\\,A v = b\\,$。求解过程只能使用由给定算子 $\\,B^{-1/2}, M, H, R^{-1/2}\\,$ 及其隐含的逆 $\\,B^{1/2} = \\left(B^{-1/2}\\right)^{-1}\\,$ 构造的算子作用（由于测试用例是低维的，您可以通过数值方法对 $\\,B^{-1/2}\\,$ 求逆来计算 $\\,B^{1/2}\\,$）。具体来说，给定一个初始猜测 $\\,v_0\\,$，执行以下单次 CG 迭代：\n- 计算初始残差 $\\,r_0 = b - A v_0\\,$。\n- 设置初始搜索方向 $\\,p_0 = r_0\\,$。\n- 计算步长\n$$\n\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}.\n$$\n- 更新迭代解 $\\,v_1 = v_0 + \\alpha_0 p_0\\,$。\n- 更新残差 $\\,r_1 = r_0 - \\alpha_0 A p_0\\,$。\n对于每个测试用例，报告此单次 CG 步骤后的欧几里得范数 $\\,\\lVert r_1 \\rVert_2\\,$。\n\n算子实现要求：您不能显式地组装 $\\,A\\,$。相反，对于任意的 $\\,x \\in \\mathbb{R}^n\\,$，您必须使用由 $\\,Z\\,$ 和 $\\,Z^\\top\\,$ 导出的复合算子来实现其作用 $\\,y = A x\\,$，即\n$$\nA x = x + Z^\\top (Z x),\\quad Z x = R^{-1/2} H M B^{1/2} x,\\quad Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y.\n$$\n所有转置均为通常的欧几里得伴随。\n\n测试套件。对于以下每个案例，$\\,n\\,$ 和 $\\,m\\,$ 分别表示状态和观测维度。所有矩阵都是小型的、实值的，并且指定的平方根矩阵是对称正定的。请严格使用给定的参数。计算并返回每个测试的单步残差范数 $\\,\\lVert r_1 \\rVert_2\\,$，结果为浮点数。\n\n- 案例 1（理想路径，混合强度观测）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(2, 1, 1/2)$。\n  - $M = \\begin{bmatrix}1  0  0\\\\ 0  1  0\\\\ 0  0  1\\end{bmatrix}$。\n  - $H = \\begin{bmatrix}1  0  0\\\\ 0  1  1\\end{bmatrix}$。\n  - $R^{-1/2} = \\mathrm{diag}(1, 2)$。\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}1\\\\-1\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 案例 2（无观测；仅背景）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(1, 1, 1)$。\n  - $M = I$（3 阶）。\n  - $H = \\begin{bmatrix}0  0  0\\\\ 0  0  0\\end{bmatrix}$。\n  - $R^{-1/2} = I$（2 阶）。\n  - $v_b = \\begin{bmatrix}1\\\\-1\\\\1/2\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}3\\\\-2\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 案例 3（观测极弱；几乎仅背景）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(4/5, 6/5, 1)$。\n  - $M = I$（3 阶）。\n  - $H = \\begin{bmatrix}1  0  0\\\\ 0  0  1\\end{bmatrix}$。\n  - $R^{-1/2} = \\mathrm{diag}(10^{-3}, 2 \\cdot 10^{-3})$。\n  - $v_b = \\begin{bmatrix}1/5\\\\-1/10\\\\1/20\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}1/2\\\\-1\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 案例 4（动力学和完全观测）：\n  - $n = 3$, $m = 3$。\n  - $B^{-1/2} = \\mathrm{diag}(3/2, 3/4, 1/2)$。\n  - $M = \\begin{bmatrix}1  1  0\\\\ 0  1  1\\\\ 0  0  1\\end{bmatrix}$。\n  - $H = I$（3 阶）。\n  - $R^{-1/2} = \\mathrm{diag}(1, 1/2, 1/4)$。\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}1/2\\\\-1/2\\\\1\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 案例 5（具有非零初始猜测的一般情况）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(6/5, 9/10, 7/10)$。\n  - $M = \\begin{bmatrix}1  1/5  0\\\\ 0  1  3/10\\\\ 0  0  1\\end{bmatrix}$。\n  - $H = \\begin{bmatrix}1/2  -1/2  0\\\\ 0  1  -1\\end{bmatrix}$。\n  - $R^{-1/2} = \\mathrm{diag}(7/10, 13/10)$。\n  - $v_b = \\begin{bmatrix}1/10\\\\-1/5\\\\3/10\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}-1/2\\\\4/5\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}1/20\\\\1/20\\\\-1/10\\end{bmatrix}$。\n\n您的程序必须：\n- 对每个案例，通过数值方法构造 $\\,B^{1/2} = \\left(B^{-1/2}\\right)^{-1}\\,$。\n- 仅使用 $\\,B^{1/2}, M, H, R^{-1/2}\\,$ 及其所需的转置进行算子复合来实现 $\\,y = A x\\,$。\n- 对每个案例，完全按照上述规定执行一次共轭梯度步骤，并计算 $\\,\\lVert r_1 \\rVert_2\\,$。\n- 输出一行，其中包含五个残差范数的列表，格式为逗号分隔并用方括号括起，例如 $[a_1,a_2,a_3,a_4,a_5]$，其中每个 $\\,a_i\\,$ 都是一个浮点数。\n\n此问题不涉及物理单位或角度。最终输出是无单位的实数。",
            "solution": "用户提供了有效的问题陈述。\n\n### 第 1 步：提取已知条件\n- **问题领域**：线性化增量四维变分 (4D-Var) 数据同化子问题。\n- **待解系统**：一个对称正定线性系统 $A v = b$，其中 $v \\in \\mathbb{R}^n$ 是预处理控制变量。\n- **矩阵和向量定义**：\n  - $A \\equiv I + Z^\\top Z$，其中 $I$ 是 $\\mathbb{R}^n$ 上的单位矩阵。\n  - $Z \\equiv R^{-1/2} H M B^{1/2}$。\n  - $b \\equiv v_b + Z^\\top R^{-1/2} d$。\n- **构成算子**：\n  - $B^{1/2} \\in \\mathbb{R}^{n \\times n}$：背景误差协方差 $B$ 的对称正定平方根。问题提供了其逆 $B^{-1/2}$。\n  - $M \\in \\mathbb{R}^{n \\times n}$：切线性模型算子。\n  - $H \\in \\mathbb{R}^{m \\times n}$：线性化观测算子。\n  - $R^{-1/2} \\in \\mathbb{R}^{m \\times m}$：观测误差协方差 $R$ 的对称正定逆平方根。\n- **给定数据**：\n  - $v_b \\in \\mathbb{R}^n$：一个与背景状态相关的向量。\n  - $d \\in \\mathbb{R}^m$：一个与观测-模型不匹配（新息向量）相关的向量。\n  - $v_0 \\in \\mathbb{R}^n$：解 $v$ 的初始猜测。\n- **任务**：执行一步共轭梯度 (CG) 算法。\n  1. 计算初始残差：$r_0 = b - A v_0$。\n  2. 设置初始搜索方向：$p_0 = r_0$。\n  3. 计算步长：$\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}$。\n  4. 更新迭代解：$v_1 = v_0 + \\alpha_0 p_0$。\n  5. 更新残差：$r_1 = r_0 - \\alpha_0 A p_0$。\n- **输出要求**：报告每个提供的五个测试用例的更新后残差的欧几里得范数 $\\|r_1\\|_2$。\n- **实现约束**：矩阵 $A$ 不得显式组装。$A$ 对向量 $x$ 的作用，即 $A x$，必须使用给定算子的复合来计算：$A x = x + Z^\\top (Z x)$，其中 $Z x = R^{-1/2} H M B^{1/2} x$ 且 $Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学依据（关键）**：该问题是数值天气预报和地球物理数据同化中的一个标准公式。使用预处理共轭梯度法求解二次代价函数的正规方程是该领域的一项基石技术。所有概念都基于线性代数、优化和反问题理论的既定原则。该问题在科学上是合理的。\n- **适定性**：系统矩阵 $A = I + Z^\\top Z$ 是对称正定的。矩阵 $Z^\\top Z$ 是对称半正定的，加上单位矩阵 $I$ 确保了 $A$ 是严格正定的。因此，线性系统 $Av=b$ 有唯一解。共轭梯度算法是针对此类系统的一种定义明确且收敛的方法。该问题是适定的。\n- **客观性（关键）**：该问题以精确的数学符号指定，并为所有测试用例提供了明确的数值数据。没有歧义或主观性。\n- **不完整或矛盾的设置**：该问题是自包含的。执行计算所需的所有矩阵、向量和初始条件都为每个测试用例提供了。所有矩阵和向量的维度都是一致的。$A$ 和 $b$ 的推导与标准 4D-Var 二次代价函数的最小化是一致的。\n- **不切实际或不可行**：维度和数值的选择是为了计算方便，但在离散化模型的背景下并非物理上不可能或科学上难以置信。\n- **结论**：问题有效、适定，并提供了求解所需的所有必要信息。\n\n### 第 3 步：判断与行动\n问题有效。将提供完整的解决方案。\n\n### 基于原则的设计\n此问题的核心是求解线性系统 $A v = b$，它代表了 4D-Var 数据同化中出现的二次最小化问题的一阶最优性条件（或正规方程）。共轭梯度 (CG) 方法是一种迭代算法，非常适合求解大规模对称正定 (SPD) 系统，特别是当系统矩阵 $A$ 没有显式给出，但其对向量的作用可以高效计算时。这被称为“无矩阵”方法。\n\n我们的解决方案通过实现 CG 算法的单一步骤来进行，并严格遵守无矩阵约束。\n\n**1. 算子定义与作用**\n\n系统矩阵定义为 $A \\equiv I + Z^\\top Z$，其中 $Z \\equiv R^{-1/2} H M B^{1/2}$。我们不构造稠密矩阵 $A$，而是实现一个函数，对于任何给定的向量 $x$ 计算乘积 $y = Ax$。这是通过复合构成算子的作用来实现的：\n- $Z$ 对向量 $x$ 的作用是按矩阵-向量乘积序列计算的：$Z x = R^{-1/2}(H(M(B^{1/2} x)))$。\n- 伴随算子 $Z^\\top$ 对向量 $y$ 的作用也类似地计算：$Z^\\top y = B^{1/2}(M^\\top(H^\\top(R^{-1/2} y)))$。注意，由于 $B^{1/2}$ 和 $R^{-1/2}$ 是对称的，它们的转置就是它们自身。\n- $A$ 对 $x$ 的完整作用则是 $A x = x + Z^\\top(Z x)$。\n\n这些操作需要矩阵 $B^{1/2}$。它是通过对给定的矩阵 $B^{-1/2}$ 进行数值求逆来计算的，每个测试用例计算一次，即 $B^{1/2} = (B^{-1/2})^{-1}$。\n\n**2. 右端项构造**\n\n右端项向量 $b$ 定义为 $b \\equiv v_b + Z^\\top R^{-1/2} d$。这也使用算子作用来计算。首先，计算向量 $y_d = R^{-1/2} d$。然后，将 $Z^\\top$ 的作用应用于 $y_d$，并将结果加到 $v_b$ 上：\n$$b = v_b + Z^\\top(y_d) = v_b + B^{1/2}(M^\\top(H^\\top(R^{-1/2} d)))$$\n\n**3. 单次 CG 迭代**\n\n在具备计算 $Ax$ 和 $b$ 的能力后，我们从初始猜测 $v_0$ 开始，精确执行一次 CG 算法迭代。\n\n- **初始残差**：算法开始时计算初始残差，它衡量初始猜测距离满足方程的程度：\n$$r_0 = b - A v_0$$\n乘积 $A v_0$ 使用上述的无矩阵函数计算。\n\n- **初始搜索方向**：在 CG 算法中，第一个搜索方向就是初始残差：\n$$p_0 = r_0$$\n\n- **最优步长**：我们通过计算步长 $\\alpha_0$ 来确定沿搜索方向 $p_0$ 移动多远，该步长最小化 A-范数下的残差。公式为：\n$$\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}$$\n分母项 $p_0^\\top A p_0$ 的计算方法是：首先找到向量 $w = A p_0$，然后计算点积 $p_0^\\top w$。\n\n- **更新迭代解和残差**：最后，我们更新解向量和残差。问题只要求新残差 $r_1$ 的范数。更新残差的公式在计算上比重新评估 $r_1 = b - A v_1$ 更经济：\n$$r_1 = r_0 - \\alpha_0 A p_0$$\n\n- **最终结果**：计算并报告每个测试用例的新残差的欧几里得范数 $\\|r_1\\|_2 = \\sqrt{r_1^\\top r_1}$。该值表示单次 CG 步骤中实现的误差减少量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_cg_step(case_data):\n    \"\"\"\n    Performs one step of the Conjugate Gradient method for a given test case.\n\n    Args:\n        case_data (dict): A dictionary containing all matrices and vectors for the test case.\n\n    Returns:\n        float: The Euclidean norm of the residual after one CG step, ||r_1||_2.\n    \"\"\"\n    # Unpack all data for the current case\n    B_inv_sqrt = case_data[\"B_inv_sqrt\"]\n    M = case_data[\"M\"]\n    H = case_data[\"H\"]\n    R_inv_sqrt = case_data[\"R_inv_sqrt\"]\n    v_b = case_data[\"v_b\"]\n    d = case_data[\"d\"]\n    v0 = case_data[\"v0\"]\n\n    # 1. Compute B_sqrt and required transposes\n    # As per the problem, B^{1/2} = (B^{-1/2})^{-1}\n    B_sqrt = np.linalg.inv(B_inv_sqrt)\n    M_T = M.T\n    H_T = H.T\n    # R_inv_sqrt is symmetric, so its transpose is itself.\n\n    # 2. Define the matrix-free operator actions\n    def Z_action(x):\n        \"\"\"Computes Zx = R^{-1/2} H M B^{1/2} x.\"\"\"\n        res = B_sqrt @ x\n        res = M @ res\n        res = H @ res\n        res = R_inv_sqrt @ res\n        return res\n\n    def Z_transpose_action(y):\n        \"\"\"Computes Z^T y = B^{1/2} M^T H^T R^{-1/2} y.\"\"\"\n        res = R_inv_sqrt @ y\n        res = H_T @ res\n        res = M_T @ res\n        res = B_sqrt @ res\n        return res\n\n    def A_action(x):\n        \"\"\"Computes Ax = (I + Z^T Z)x = x + Z^T(Z(x)).\"\"\"\n        return x + Z_transpose_action(Z_action(x))\n\n    # 3. Compute the right-hand side vector b\n    # b = v_b + Z^T (R^{-1/2} d)\n    term_in_Z_T = R_inv_sqrt @ d\n    b = v_b + Z_transpose_action(term_in_Z_T)\n\n    # 4. Perform one step of the Conjugate Gradient algorithm\n    # Compute the initial residual: r_0 = b - A v_0\n    Av0 = A_action(v0)\n    r0 = b - Av0\n    \n    # If the initial residual is zero, the initial guess is the exact solution.\n    # The next residual will also be zero.\n    if np.linalg.norm(r0)  1e-15:\n        return 0.0\n\n    # Set the initial search direction: p_0 = r_0\n    p0 = r0\n    \n    # Compute the matrix-vector product A*p_0 needed for alpha_0 and r_1\n    Ap0 = A_action(p0)\n    \n    # Compute the step size: alpha_0 = (r_0^T r_0) / (p_0^T A p_0)\n    r0_dot_r0 = np.dot(r0, r0)\n    p0_dot_Ap0 = np.dot(p0, Ap0)\n\n    # Since A is SPD, p0_dot_Ap0 is zero iff p0 is zero.\n    # This is handled by the initial check on norm(r0).\n    # This check is for robustness.\n    if abs(p0_dot_Ap0)  1e-15:\n        alpha0 = 0.0\n    else:\n        alpha0 = r0_dot_r0 / p0_dot_Ap0\n    \n    # Update the residual: r_1 = r_0 - alpha_0 * A * p_0\n    r1 = r0 - alpha0 * Ap0\n    \n    # 5. Compute and return the Euclidean norm of the new residual, ||r_1||\n    norm_r1 = np.linalg.norm(r1)\n    \n    return norm_r1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, mixed-strength observations):\n        {\n            \"B_inv_sqrt\": np.diag([2.0, 1.0, 0.5]),\n            \"M\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1.0, 2.0]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([1.0, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 2 (no observations; background-only):\n        {\n            \"B_inv_sqrt\": np.diag([1.0, 1.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.zeros((2, 3)),\n            \"R_inv_sqrt\": np.identity(2),\n            \"v_b\": np.array([1.0, -1.0, 0.5]),\n            \"d\": np.array([3.0, -2.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 3 (very weak observations; nearly background-only):\n        {\n            \"B_inv_sqrt\": np.diag([4.0/5.0, 6.0/5.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1e-3, 2e-3]),\n            \"v_b\": np.array([1.0/5.0, -1.0/10.0, 1.0/20.0]),\n            \"d\": np.array([0.5, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 4 (dynamics and full observations):\n        {\n            \"B_inv_sqrt\": np.diag([3.0/2.0, 3.0/4.0, 1.0/2.0]),\n            \"M\": np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.identity(3),\n            \"R_inv_sqrt\": np.diag([1.0, 0.5, 0.25]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.5, -0.5, 1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 5 (general case with nonzero initial guess):\n        {\n            \"B_inv_sqrt\": np.diag([6.0/5.0, 9.0/10.0, 7.0/10.0]),\n            \"M\": np.array([[1.0, 1.0/5.0, 0.0], [0.0, 1.0, 3.0/10.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[0.5, -0.5, 0.0], [0.0, 1.0, -1.0]]),\n            \"R_inv_sqrt\": np.diag([7.0/10.0, 13.0/10.0]),\n            \"v_b\": np.array([1.0/10.0, -2.0/10.0, 3.0/10.0]),\n            \"d\": np.array([-0.5, 4.0/5.0]),\n            \"v0\": np.array([1.0/20.0, 1.0/20.0, -1.0/10.0]),\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        norm_r1 = run_single_cg_step(case_data)\n        results.append(norm_r1)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}