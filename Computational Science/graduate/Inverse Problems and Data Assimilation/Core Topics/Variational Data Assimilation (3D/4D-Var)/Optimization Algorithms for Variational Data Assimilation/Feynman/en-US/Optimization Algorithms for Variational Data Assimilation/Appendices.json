{
    "hands_on_practices": [
        {
            "introduction": "Before implementing any optimization algorithm, we must first master its fundamental component: the adjoint model. This practice guides you through the process of deriving the discrete adjoint for a simple but illustrative physical system—the linear advection equation. By starting from the discrete forward model and applying the definition of the adjoint operator, you will construct the backward-in-time adjoint equations and analyze their numerical stability, a critical property for reliable gradient calculations .",
            "id": "3408488",
            "problem": "Consider the linear advection equation $u_{t} + a\\,u_{x} = 0$ on the periodic domain $x \\in [0,L]$ with constant wave speed $a > 0$. Let the spatial grid be uniform with $N_{x}$ points, spacing $\\Delta x = L/N_{x}$, and periodic indexing. Let the time step be $\\Delta t$, and define the Courant number $C = a\\,\\Delta t/\\Delta x$. Consider the first-order upwind finite difference in space with forward Euler in time for the state update:\n$$\nu^{n+1}_{j} = u^{n}_{j} - C\\left(u^{n}_{j} - u^{n}_{j-1}\\right), \\quad j = 0,\\dots,N_{x}-1,\n$$\nwith periodic indexing for $j-1$. Assume a Four Dimensional Variational (4D-Var) data assimilation setting in which a quadratic cost functional depends on the initial condition $u^{0}$ and on the model state at a final time level $n = N_{t}$ through a linear observation operator $H$ and positive-definite covariance matrices, but do not assume any particular form of these terms beyond linearity and positive-definiteness. Use the discrete inner product $\\langle p, q\\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_{x}-1} p_{j} q_{j}$.\n\nTasks:\n1. Starting from the definition of a discrete Lagrangian that enforces the model update constraints $u^{n+1} - M u^{n} = 0$ at each time step (with $M$ denoting the linear update operator defined by the above scheme) using Lagrange multipliers $\\lambda^{n+1}$, derive the discrete adjoint recursion for $\\lambda^{n}$ with respect to the inner product $\\langle \\cdot, \\cdot\\rangle_{\\Delta x}$. Express the adjoint update explicitly as a pointwise stencil for $\\lambda^{n}_{j}$ in terms of $\\lambda^{n+1}$.\n2. Perform a von Neumann analysis of the homogeneous adjoint recursion (no source terms) under periodic boundary conditions by considering discrete Fourier modes of the form $\\varphi_{j}(\\theta) = \\exp(i \\theta j)$ with $\\theta \\in [-\\pi,\\pi]$. Derive the adjoint amplification factor $G_{\\mathrm{adj}}(\\theta; C)$ and its magnitude $|G_{\\mathrm{adj}}(\\theta; C)|$ as a function of $\\theta$ and $C$.\n3. Using the magnitude $|G_{\\mathrm{adj}}(\\theta; C)|$, determine the condition on $C$ under which the adjoint update is non-expansive in the discrete $\\ell^{2}$ norm induced by $\\langle \\cdot, \\cdot\\rangle_{\\Delta x}$ for all Fourier modes, i.e., $|G_{\\mathrm{adj}}(\\theta; C)| \\leq 1$ for all $\\theta \\in [-\\pi,\\pi]$.\n4. What is the largest allowable Courant number $C_{\\max}$ satisfying this non-expansiveness condition? Report $C_{\\max}$ as a single real number with no units.\n\nYour final answer must be the single number $C_{\\max}$ only. No rounding is required.",
            "solution": "### Task 1: Derivation of the Discrete Adjoint Recursion\n\nThe forward model is a discrete approximation of the linear advection equation $u_t + a u_x = 0$. The discretization is given by the first-order upwind scheme in space and forward Euler in time:\n$$\nu^{n+1}_{j} = u^{n}_{j} - C \\left(u^{n}_{j} - u^{n}_{j-1}\\right)\n$$\nwhere $C = a \\Delta t / \\Delta x$ is the Courant number. We can rewrite this as a linear update rule $u^{n+1} = M u^n$, where $u^n$ is the state vector and $M$ is the linear operator whose action on a vector $v$ is given by $(Mv)_j = (1-C)v_j + C v_{j-1}$.\n\nThe adjoint equations are found by considering a Lagrangian $\\mathcal{L}$ that includes terms for the model constraints enforced by Lagrange multipliers $\\lambda^{n+1}$:\n$$\n\\mathcal{L} = J(u^0, u^{N_t}) + \\sum_{n=0}^{N_t-1} \\langle \\lambda^{n+1}, u^{n+1} - M u^n \\rangle_{\\Delta x}\n$$\nSetting the derivative of $\\mathcal{L}$ with respect to an intermediate state $u^n$ to zero gives the adjoint recursion:\n$$\n\\lambda^n = M^* \\lambda^{n+1}\n$$\nwhere $M^*$ is the adjoint of $M$ with respect to the inner product $\\langle \\cdot, \\cdot \\rangle_{\\Delta x}$. To find the explicit stencil for $M^*$, we use its definition: $\\langle M^* p, q \\rangle_{\\Delta x} = \\langle p, M q \\rangle_{\\Delta x}$ for any vectors $p, q$.\n$$\n\\langle p, M q \\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_x-1} p_j (Mq)_j = \\Delta x \\sum_{j=0}^{N_x-1} p_j \\left( (1-C)q_j + C q_{j-1} \\right)\n$$\nRe-indexing the second term (letting $k = j-1$) and using periodic boundary conditions, we get:\n$$\n\\langle p, M q \\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_x-1} \\left( (1-C)p_j + C p_{j+1} \\right) q_j\n$$\nBy comparing this with $\\langle M^* p, q \\rangle_{\\Delta x} = \\Delta x \\sum_{j} (M^*p)_j q_j$, we identify the action of the adjoint operator: $(M^*p)_j = (1-C)p_j + C p_{j+1}$. The adjoint recursion is therefore:\n$$\n\\lambda^n_j = (1-C)\\lambda^{n+1}_j + C \\lambda^{n+1}_{j+1}\n$$\nThis is a first-order downwind scheme, the adjoint of the first-order upwind forward model.\n\n### Task 2: Von Neumann Analysis of the Adjoint Recursion\n\nWe substitute a single Fourier mode, $\\lambda_j^{n+1} = \\exp(i\\theta j)$ and $\\lambda_j^n = G_{\\mathrm{adj}}(\\theta; C) \\exp(i\\theta j)$, into the adjoint recursion to find the amplification factor $G_{\\mathrm{adj}}(\\theta; C)$.\n$$\nG_{\\mathrm{adj}}(\\theta; C) \\exp(i\\theta j) = (1-C) \\exp(i\\theta j) + C \\exp(i\\theta(j+1))\n$$\nDividing by $\\exp(i\\theta j)$ gives:\n$$\nG_{\\mathrm{adj}}(\\theta; C) = (1-C) + C \\exp(i\\theta) = (1-C+C\\cos\\theta) + i(C\\sin\\theta)\n$$\nThe magnitude squared of the amplification factor is:\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = (1-C+C\\cos\\theta)^2 + (C\\sin\\theta)^2\n$$\n$$\n= (1-C)^2 + 2C(1-C)\\cos\\theta + C^2\\cos^2\\theta + C^2\\sin^2\\theta\n$$\n$$\n= 1 - 2C + C^2 + 2C(1-C)\\cos\\theta + C^2\n$$\n$$\n= 1 + (2C^2 - 2C) - (2C^2 - 2C)\\cos\\theta\n$$\n$$\n= 1 + 2C(C-1)(1-\\cos\\theta)\n$$\nUsing the half-angle identity $1-\\cos\\theta = 2\\sin^2(\\theta/2)$, we get:\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = 1 + 4C(C-1)\\sin^2(\\theta/2)\n$$\nThe magnitude is $|G_{\\mathrm{adj}}(\\theta; C)| = \\sqrt{1 + 4C(C-1)\\sin^2(\\theta/2)}$.\n\n### Task 3: Condition for Non-Expansiveness\n\nThe adjoint update is non-expansive if $|G_{\\mathrm{adj}}(\\theta; C)| \\le 1$ for all $\\theta \\in [-\\pi, \\pi]$, which is equivalent to $|G_{\\mathrm{adj}}(\\theta; C)|^2 \\le 1$.\n$$\n1 + 4C(C-1)\\sin^2(\\theta/2) \\leq 1\n$$\nThis simplifies to $4C(C-1)\\sin^2(\\theta/2) \\leq 0$. Since $C>0$ (from $a>0$) and $\\sin^2(\\theta/2) \\geq 0$, the inequality holds for all $\\theta$ only if the term $C(C-1)$ is non-positive.\n$$\nC(C-1) \\leq 0\n$$\nSince $C > 0$, we can divide by $C$ to get $C - 1 \\leq 0$, which implies $C \\leq 1$.\nThe condition for the adjoint update to be non-expansive is $0  C \\leq 1$.\n\n### Task 4: Largest Allowable Courant Number\n\nThe condition for non-expansiveness is that the Courant number $C$ must be in the interval $(0, 1]$. The largest value of $C$ that satisfies this condition is $1$.\nThus, the largest allowable Courant number $C_{\\max}$ is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "With the adjoint model in hand, we can compute the gradient of the cost function, which in turn allows us to approximate the Hessian's action on a vector. This next practice moves from derivation to application, challenging you to implement a single iteration of the Conjugate Gradient (CG) algorithm, a workhorse for solving the linear systems that arise in 4D-Var. This exercise emphasizes the \"matrix-free\" approach essential for large-scale problems, where the Hessian is never explicitly formed, but its action is computed through successive applications of the tangent linear and adjoint models .",
            "id": "3408588",
            "problem": "You are given a linearized incremental Four-Dimensional Variational (4D-Var) data assimilation subproblem at a single inner-loop iteration in preconditioned control variables. Let the state increment be $\\delta x \\in \\mathbb{R}^n$ and the preconditioned control variable be $v \\in \\mathbb{R}^n$ defined by $\\delta x = B^{1/2} v$ where $B \\in \\mathbb{R}^{n \\times n}$ is the background-error covariance and $B^{1/2}$ is its symmetric positive-definite square root. Let $M \\in \\mathbb{R}^{n \\times n}$ denote the tangent linear model operator over the data assimilation window, $H \\in \\mathbb{R}^{m \\times n}$ the linearized observation operator, and $R \\in \\mathbb{R}^{m \\times m}$ the observation-error covariance with symmetric positive-definite square root $R^{1/2}$ and inverse square root $R^{-1/2}$. The background term in preconditioned variables is $\\tfrac{1}{2}\\lVert v - v_b \\rVert_2^2$ and the observation misfit is $\\tfrac{1}{2}\\lVert R^{-1/2}(H M B^{1/2} v - d)\\rVert_2^2$ where $v_b \\in \\mathbb{R}^n$ and $d \\in \\mathbb{R}^m$ are given.\n\nStarting from the quadratic objective in $v$, the first-order optimality (normal) equations for the inner-loop linear system take the form\n$$\nA v = b,\n$$\nwhere\n$$\nA \\equiv I + Z^\\top Z,\\quad Z \\equiv R^{-1/2} H M B^{1/2},\\quad b \\equiv v_b + Z^\\top R^{-1/2} d,\n$$\nand $I$ is the identity on $\\mathbb{R}^n$. The matrix $A$ is symmetric positive definite by construction.\n\nYour task is to write a program that performs one step of the Conjugate Gradient (CG) method for solving the symmetric positive-definite linear system $A v = b$ using only operator actions constructed from the given operators $B^{-1/2}, M, H, R^{-1/2}$ and the implied inverse $B^{1/2} = \\left(B^{-1/2}\\right)^{-1}$ (you may compute $B^{1/2}$ numerically by inverting $B^{-1/2}$ since the test cases are low-dimensional). Specifically, given an initial guess $v_0$, perform the following single CG iteration:\n- Compute the initial residual $r_0 = b - A v_0$.\n- Set the initial search direction $p_0 = r_0$.\n- Compute the step size\n$$\n\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}.\n$$\n- Update the iterate $v_1 = v_0 + \\alpha_0 p_0$.\n- Update the residual $r_1 = r_0 - \\alpha_0 A p_0$.\nReport the Euclidean norm $\\lVert r_1 \\rVert_2$ after this single CG step for each test case.\n\nOperator implementation requirement: you must not assemble $A$ explicitly. Instead, implement the action $y = A x$ for an arbitrary $x \\in \\mathbb{R}^n$ using the compositions induced by $Z$ and $Z^\\top$, i.e.,\n$$\nA x = x + Z^\\top (Z x),\\quad Z x = R^{-1/2} H M B^{1/2} x,\\quad Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y.\n$$\nAll transposes are the usual Euclidean adjoints.\n\nTest suite. For each case below, $n$ and $m$ denote state and observation dimensions, respectively. All matrices are small, real-valued, and the specified square-root matrices are symmetric positive definite. Use exactly the parameters given. Compute and return the single-step residual norm $\\lVert r_1 \\rVert_2$ for each test as a floating-point number.\n\n- Case $1$ (happy path, mixed-strength observations):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(2, 1, 1/2)$.\n  - $M = \\begin{bmatrix}1  0  0\\\\ 0  1  0\\\\ 0  0  1\\end{bmatrix}$.\n  - $H = \\begin{bmatrix}1  0  0\\\\ 0  1  1\\end{bmatrix}$.\n  - $R^{-1/2} = \\mathrm{diag}(1, 2)$.\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}1\\\\-1\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $2$ (no observations; background-only):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(1, 1, 1)$.\n  - $M = I$ of size $3$.\n  - $H = \\begin{bmatrix}0  0  0\\\\ 0  0  0\\end{bmatrix}$.\n  - $R^{-1/2} = I$ of size $2$.\n  - $v_b = \\begin{bmatrix}1\\\\-1\\\\1/2\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}3\\\\-2\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $3$ (very weak observations; nearly background-only):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(4/5, 6/5, 1)$.\n  - $M = I$ of size $3$.\n  - $H = \\begin{bmatrix}1  0  0\\\\ 0  0  1\\end{bmatrix}$.\n  - $R^{-1/2} = \\mathrm{diag}(10^{-3}, 2 \\cdot 10^{-3})$.\n  - $v_b = \\begin{bmatrix}1/5\\\\-1/10\\\\1/20\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}1/2\\\\-1\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $4$ (dynamics and full observations):\n  - $n = 3$, $m = 3$.\n  - $B^{-1/2} = \\mathrm{diag}(3/2, 3/4, 1/2)$.\n  - $M = \\begin{bmatrix}1  1  0\\\\ 0  1  1\\\\ 0  0  1\\end{bmatrix}$.\n  - $H = I$ of size $3$.\n  - $R^{-1/2} = \\mathrm{diag}(1, 1/2, 1/4)$.\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}1/2\\\\-1/2\\\\1\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$.\n\n- Case $5$ (general case with nonzero initial guess):\n  - $n = 3$, $m = 2$.\n  - $B^{-1/2} = \\mathrm{diag}(6/5, 9/10, 7/10)$.\n  - $M = \\begin{bmatrix}1  1/5  0\\\\ 0  1  3/10\\\\ 0  0  1\\end{bmatrix}$.\n  - $H = \\begin{bmatrix}1/2  -1/2  0\\\\ 0  1  -1\\end{bmatrix}$.\n  - $R^{-1/2} = \\mathrm{diag}(7/10, 13/10)$.\n  - $v_b = \\begin{bmatrix}1/10\\\\-1/5\\\\3/10\\end{bmatrix}$.\n  - $d = \\begin{bmatrix}-1/2\\\\4/5\\end{bmatrix}$.\n  - $v_0 = \\begin{bmatrix}1/20\\\\1/20\\\\-1/10\\end{bmatrix}$.\n\nYour program must:\n- Construct $B^{1/2} = \\left(B^{-1/2}\\right)^{-1}$ for each case numerically.\n- Implement $y = A x$ using only operator compositions with $B^{1/2}, M, H, R^{-1/2}$ and their transposes as needed.\n- For each case, perform exactly one Conjugate Gradient step as specified above and compute $\\lVert r_1 \\rVert_2$.\n- Output a single line containing the list of the five residual norms as a comma-separated list enclosed in square brackets, e.g., $[a_1,a_2,a_3,a_4,a_5]$ where each $a_i$ is a floating-point number.\n\nNo physical units or angles are involved in this problem. The final outputs are real numbers without units.",
            "solution": "The core of this problem is to implement a single step of the Conjugate Gradient (CG) algorithm to solve the linear system $Av=b$ without explicitly forming the matrix $A$. This \"matrix-free\" approach is standard in large-scale data assimilation. The solution involves three main parts: defining the matrix-free actions of the operators, constructing the right-hand-side vector $b$, and executing the prescribed CG steps.\n\n**1. Operator Actions**\n\nThe system matrix is $A = I + Z^\\top Z$, where $Z = R^{-1/2} H M B^{1/2}$. The action of $A$ on a vector $x$ is computed as $Ax = x + Z^\\top(Zx)$. This requires functions to compute the action of $Z$ and its adjoint $Z^\\top$.\n- The action $Zx$ is a sequence of matrix-vector products: $R^{-1/2}(H(M(B^{1/2}x)))$.\n- The action $Z^\\top y$ applies the adjoint operators in reverse order: $B^{1/2}(M^\\top(H^\\top(R^{-1/2}y)))$.\nNote that $B^{1/2}$ is computed by inverting the given $B^{-1/2}$.\n\n**2. Right-Hand Side Construction**\n\nThe vector $b$ is given by $b = v_b + Z^\\top R^{-1/2}d$. This is computed by first applying $R^{-1/2}$ to $d$, then applying the $Z^\\top$ operator action, and finally adding the result to $v_b$.\n\n**3. CG Iteration**\n\nWith these components, we perform one CG step starting from $v_0$:\n- Calculate the initial residual: $r_0 = b - Av_0$.\n- Set the initial search direction: $p_0 = r_0$.\n- Compute the vector $w = Ap_0$ needed for the next steps.\n- Calculate the optimal step size: $\\alpha_0 = (r_0^\\top r_0) / (p_0^\\top w)$.\n- Update the residual: $r_1 = r_0 - \\alpha_0 w$.\nThe final step is to compute and report the Euclidean norm of this new residual, $\\|r_1\\|_2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_cg_step(case_data):\n    \"\"\"\n    Performs one step of the Conjugate Gradient method for a given test case.\n\n    Args:\n        case_data (dict): A dictionary containing all matrices and vectors for the test case.\n\n    Returns:\n        float: The Euclidean norm of the residual after one CG step, ||r_1||_2.\n    \"\"\"\n    # Unpack all data for the current case\n    B_inv_sqrt = case_data[\"B_inv_sqrt\"]\n    M = case_data[\"M\"]\n    H = case_data[\"H\"]\n    R_inv_sqrt = case_data[\"R_inv_sqrt\"]\n    v_b = case_data[\"v_b\"]\n    d = case_data[\"d\"]\n    v0 = case_data[\"v0\"]\n\n    # 1. Compute B_sqrt and required transposes\n    # As per the problem, B^{1/2} = (B^{-1/2})^{-1}\n    B_sqrt = np.linalg.inv(B_inv_sqrt)\n    M_T = M.T\n    H_T = H.T\n    # R_inv_sqrt is symmetric, so its transpose is itself.\n\n    # 2. Define the matrix-free operator actions\n    def Z_action(x):\n        \"\"\"Computes Zx = R^{-1/2} H M B^{1/2} x.\"\"\"\n        res = B_sqrt @ x\n        res = M @ res\n        res = H @ res\n        res = R_inv_sqrt @ res\n        return res\n\n    def Z_transpose_action(y):\n        \"\"\"Computes Z^T y = B^{1/2} M^T H^T R^{-1/2} y.\"\"\"\n        res = R_inv_sqrt @ y\n        res = H_T @ res\n        res = M_T @ res\n        res = B_sqrt @ res\n        return res\n\n    def A_action(x):\n        \"\"\"Computes Ax = (I + Z^T Z)x = x + Z^T(Z(x)).\"\"\"\n        return x + Z_transpose_action(Z_action(x))\n\n    # 3. Compute the right-hand side vector b\n    # b = v_b + Z^T (R^{-1/2} d)\n    term_in_Z_T = R_inv_sqrt @ d\n    b = v_b + Z_transpose_action(term_in_Z_T)\n\n    # 4. Perform one step of the Conjugate Gradient algorithm\n    # Compute the initial residual: r_0 = b - A v_0\n    Av0 = A_action(v0)\n    r0 = b - Av0\n    \n    # If the initial residual is zero, the initial guess is the exact solution.\n    # The next residual will also be zero.\n    if np.linalg.norm(r0)  1e-15:\n        return 0.0\n\n    # Set the initial search direction: p_0 = r_0\n    p0 = r0\n    \n    # Compute the matrix-vector product A*p_0 needed for alpha_0 and r_1\n    Ap0 = A_action(p0)\n    \n    # Compute the step size: alpha_0 = (r_0^T r_0) / (p_0^T A p_0)\n    r0_dot_r0 = np.dot(r0, r0)\n    p0_dot_Ap0 = np.dot(p0, Ap0)\n\n    # Since A is SPD, p0_dot_Ap0 is zero iff p0 is zero.\n    # This is handled by the initial check on norm(r0).\n    # This check is for robustness.\n    if abs(p0_dot_Ap0)  1e-15:\n        alpha0 = 0.0\n    else:\n        alpha0 = r0_dot_r0 / p0_dot_Ap0\n    \n    # Update the residual: r_1 = r_0 - alpha_0 * A * p_0\n    r1 = r0 - alpha0 * Ap0\n    \n    # 5. Compute and return the Euclidean norm of the new residual, ||r_1||\n    norm_r1 = np.linalg.norm(r1)\n    \n    return norm_r1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, mixed-strength observations):\n        {\n            \"B_inv_sqrt\": np.diag([2.0, 1.0, 0.5]),\n            \"M\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1.0, 2.0]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([1.0, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 2 (no observations; background-only):\n        {\n            \"B_inv_sqrt\": np.diag([1.0, 1.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.zeros((2, 3)),\n            \"R_inv_sqrt\": np.identity(2),\n            \"v_b\": np.array([1.0, -1.0, 0.5]),\n            \"d\": np.array([3.0, -2.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 3 (very weak observations; nearly background-only):\n        {\n            \"B_inv_sqrt\": np.diag([4.0/5.0, 6.0/5.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1e-3, 2e-3]),\n            \"v_b\": np.array([1.0/5.0, -1.0/10.0, 1.0/20.0]),\n            \"d\": np.array([0.5, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 4 (dynamics and full observations):\n        {\n            \"B_inv_sqrt\": np.diag([3.0/2.0, 3.0/4.0, 1.0/2.0]),\n            \"M\": np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.identity(3),\n            \"R_inv_sqrt\": np.diag([1.0, 0.5, 0.25]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.5, -0.5, 1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 5 (general case with nonzero initial guess):\n        {\n            \"B_inv_sqrt\": np.diag([6.0/5.0, 9.0/10.0, 7.0/10.0]),\n            \"M\": np.array([[1.0, 1.0/5.0, 0.0], [0.0, 1.0, 3.0/10.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[0.5, -0.5, 0.0], [0.0, 1.0, -1.0]]),\n            \"R_inv_sqrt\": np.diag([7.0/10.0, 13.0/10.0]),\n            \"v_b\": np.array([1.0/10.0, -2.0/10.0, 3.0/10.0]),\n            \"d\": np.array([-0.5, 4.0/5.0]),\n            \"v0\": np.array([1.0/20.0, 1.0/20.0, -1.0/10.0]),\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        norm_r1 = run_single_cg_step(case_data)\n        results.append(norm_r1)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For linear models, verifying a gradient is straightforward, but for the nonlinear systems common in geophysics, correctness is much harder to ensure. This final practice introduces the indispensable tool for this task: the Taylor remainder test, also known as a \"gradient check\" or \"adjoint test\". You will implement a numerical experiment to verify that your adjoint-derived gradient for a nonlinear model is correct by confirming that the Taylor series remainder converges at the expected second-order rate, a gold-standard technique for debugging and validating any variational data assimilation system .",
            "id": "3408591",
            "problem": "Construct a complete, runnable program that verifies the second-order consistency of an adjoint-derived gradient for a variational data assimilation objective using Taylor remainder tests. Begin from the fundamental definition of a strong-constraint variational data assimilation objective and Taylor’s theorem for scalar fields. You must derive the algorithmic form of the gradient using only core definitions and the chain rule, and then design numerical experiments to verify the expected second-order behavior of the Taylor remainder when the gradient is correct, and to detect its loss when the gradient is intentionally perturbed.\n\nYou are given the following setting. Let the state dimension be $n=5$ and the observation dimension be $m=3$. Define the background term with background state $x_b \\in \\mathbb{R}^n$ and symmetric positive-definite background covariance $B \\in \\mathbb{R}^{n \\times n}$. Define the forward model (single-step model operator) $M:\\mathbb{R}^n \\to \\mathbb{R}^n$ and the observation operator $H:\\mathbb{R}^n \\to \\mathbb{R}^m$. Let the observation $y \\in \\mathbb{R}^m$ and the symmetric positive-definite observation covariance $R \\in \\mathbb{R}^{m \\times m}$. Consider the objective\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big).\n$$\nUse the following concrete, fully specified ingredients:\n\n- Dimensions: $n=5$, $m=3$.\n- Time step: $\\Delta t = 0.1$.\n- Background and observation covariances: construct $B$ and $R$ deterministically as follows. Let $L_B \\in \\mathbb{R}^{n \\times n}$ be the lower-triangular matrix with diagonal entries $[1.0,\\,1.2,\\,1.4,\\,1.6,\\,1.8]$ and sub-diagonal entries $0.05$ (all other entries $0$). Set $B = L_B L_B^\\top$. Let $L_R \\in \\mathbb{R}^{m \\times m}$ be the lower-triangular matrix with diagonal entries $[0.7,\\,0.9,\\,1.1]$ and sub-diagonal entries $0.02$, and set $R = L_R L_R^\\top$.\n- Background state: $x_b = [0.5,\\,-0.3,\\,0.8,\\,-1.0,\\,0.2]^\\top$.\n- Truth and observations: set $x_{\\text{true}} = [-0.4,\\,0.7,\\,-0.2,\\,0.3,\\,-0.6]^\\top$, define $y = H(M(x_{\\text{true}}))$ with no added noise.\n- Forward model: for any $x \\in \\mathbb{R}^n$, define $f(x) = \\sin(x) + 0.1\\,x \\odot x$ where $\\sin(\\cdot)$ and $\\odot$ act elementwise, and set $M(x) = x + \\Delta t\\, f(x)$.\n- Observation operator: let $C \\in \\mathbb{R}^{m \\times n}$ be\n$$\nC = \\begin{bmatrix}\n1  0  0.2  0  0\\\\\n0  0.5  0  0.1  0\\\\\n0  0  0  0.3  1\n\\end{bmatrix},\n$$\nand define $H(z) = C z + 0.05\\, z \\odot z$ for any $z \\in \\mathbb{R}^n$, where $\\odot$ denotes elementwise multiplication. The nonlinear term is applied in $\\mathbb{R}^n$ and then implicitly restricted to the first $m$ components.\n- Evaluation state: define $x_0 = [0.1,\\, -0.2,\\, 0.3,\\, -0.4,\\, 0.5]^\\top$.\n- Direction vector: let $p$ be a deterministic unit vector defined as $p = \\frac{1}{\\|v\\|}v$ where $v = [1.0,\\, -2.0,\\, 3.0,\\, -4.0,\\, 5.0]^\\top$.\n\nUse basic matrix calculus and the chain rule to express the gradient $\\nabla J(x)$ in terms of $B^{-1}$, $R^{-1}$, and the Jacobians of $M$ and $H$. Then, implement the Taylor remainder test based on the identity from Taylor’s theorem for scalar fields:\n$$\nJ(x + \\epsilon p) - J(x) \\;=\\; \\epsilon\\, \\nabla J(x)^\\top p \\;+\\; \\mathcal{O}(\\epsilon^2),\n$$\nfor small $|\\epsilon|$, using a sequence of $\\epsilon$ values. For each test, compute the first-order remainder\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x + \\epsilon p) - J(x) - \\epsilon\\, \\nabla J(x)^\\top p\\right|.\n$$\nEstimate the empirical convergence rate $\\alpha$ by fitting a straight line to $\\log r_1(\\epsilon)$ versus $\\log \\epsilon$ and reporting the slope $\\alpha$, which should be approximately $2$ for a correct gradient.\n\nDesign and execute the following test suite, which must be built into the program with no user input:\n\n- Test A (nonlinear, correct gradient): use the $M$ and $H$ defined above and the analytically derived correct $\\nabla J(x)$. Evaluate the slope $\\alpha_A$ at $x_0$ along direction $p$, using a set of decreasing $\\epsilon$ values in $[10^{-1},\\, 10^{-5}]$.\n- Test B (nonlinear, intentionally faulty gradient): repeat Test A but replace the adjoint-propagation step by a deliberately incorrect variant that uses $M'(x)$ where $M'(x)^\\top$ is required in the gradient assembly. Denote the resulting slope by $\\alpha_B$; it should be close to $1$ rather than $2$.\n- Test C (linear-quadratic reference): define a linear model and observation by $M_{\\text{lin}}(x) = A x$ and $H_{\\text{lin}}(z) = C z$, where $A \\in \\mathbb{R}^{n \\times n}$ is\n$$\nA = I + \\Delta t\\, \\mathrm{diag}([0.2,\\,-0.1,\\,0.05,\\,-0.2,\\,0.1]),\n$$\nwith $I$ the identity and $\\mathrm{diag}(\\cdot)$ the diagonal matrix with the given entries. Use the same $C$, $B$, $R$, $x_b$, $x_0$, and $p$, and define $y_{\\text{lin}} = H_{\\text{lin}}(M_{\\text{lin}}(x_{\\text{true}}))$. Compute the slope $\\alpha_C$ using the exact gradient in this linear-quadratic setting; it should be near $2$ until roundoff dominates.\n- Test D (orthogonal direction): return to the nonlinear $M$ and $H$ and construct a unit direction $p_\\perp$ such that $\\nabla J(x_0)^\\top p_\\perp = 0$ numerically by projecting $p$ to be orthogonal to $\\nabla J(x_0)$ and normalizing. Compute the slope $\\alpha_D$ using $r_1(\\epsilon)$; it should be near $2$ because the first-order term vanishes in this direction.\n\nYour program must:\n\n- Implement $J(x)$ and its gradient using only basic linear algebra and the chain rule applied to the specified $M$ and $H$.\n- Implement the faulty gradient for Test B by replacing $M'(x)^\\top$ with $M'(x)$ in the adjoint accumulation.\n- Use a fixed sequence of $\\epsilon$ values containing $[10^{-1},\\,3\\cdot 10^{-2},\\,10^{-2},\\,3\\cdot 10^{-3},\\,10^{-3},\\,3\\cdot 10^{-4},\\,10^{-4},\\,3\\cdot 10^{-5},\\,10^{-5}]$.\n- Estimate slopes $\\alpha_A, \\alpha_B, \\alpha_C, \\alpha_D$ by least-squares linear regression of $\\log r_1(\\epsilon)$ against $\\log \\epsilon$ over the entire sequence, excluding any $\\epsilon$ for which $r_1(\\epsilon)$ underflows to zero in floating point.\n\nFinal Output Format: Your program should produce a single line of output containing the four estimated slopes, rounded to three decimal places, as a comma-separated list enclosed in square brackets, in the order $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$. No physical units are involved. The outputs are real numbers.",
            "solution": "This problem verifies the correctness of an adjoint-derived gradient using a Taylor remainder test. The test relies on the second-order convergence of the remainder in a first-order Taylor expansion when the gradient is correct.\n\n**1. Gradient Derivation**\nThe objective function is $J(x) = J_b(x) + J_o(x)$. Using the chain rule, the gradient is the sum of the background and observation components:\n$$\n\\nabla J(x) = \\nabla J_b(x) + \\nabla J_o(x)\n$$\nThe background term gradient is $\\nabla J_b(x) = B^{-1}(x - x_b)$.\nThe observation term gradient is derived using the adjoint method:\n$$\n\\nabla J_o(x) = (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\nThe Jacobians $M'(x)$ and $H'(z)$ are derived from the specific functional forms of $M(x)$ and $H(z)$. For the given nonlinear model, $M'(x) = I + \\Delta t\\, \\mathrm{diag}(\\cos(x) + 0.2\\, x)$, and the Jacobian of the observation operator is $H'(z) = C + \\mathrm{diag}(0.1\\, z)$ (with the diagonal part affecting only the first $m$ components).\n\n**2. Taylor Remainder Test**\nThe test is based on the Taylor expansion: $J(x_0 + \\epsilon p) = J(x_0) + \\epsilon\\, \\nabla J(x_0)^\\top p + \\mathcal{O}(\\epsilon^2)$. The first-order remainder is defined as:\n$$\nr_1(\\epsilon) = \\left|J(x_0 + \\epsilon p) - J(x_0) - \\epsilon\\, \\nabla J(x_0)^\\top p\\right|\n$$\nFor a correct gradient, $r_1(\\epsilon)$ should be $\\mathcal{O}(\\epsilon^2)$. This means a log-log plot of $r_1(\\epsilon)$ vs. $\\epsilon$ will be a straight line with a slope of approximately 2.\n\n**3. Test Suite Execution**\nThe program implements the four specified tests:\n- **Test A (Correct Gradient):** Uses the correctly derived gradient. The slope $\\alpha_A$ is expected to be ~2.\n- **Test B (Faulty Gradient):** Replaces $(M'(x))^\\top$ with $M'(x)$ in the gradient calculation. Since the problem's $M'(x)$ is symmetric, this \"bug\" has no effect, and the slope $\\alpha_B$ is also expected to be ~2, not 1 as might be naively expected. This demonstrates that a test's effectiveness can be data-dependent.\n- **Test C (Linear-Quadratic Reference):** Uses a linear model and operator, resulting in a purely quadratic cost function. The Taylor remainder is exactly second-order, so the slope $\\alpha_C$ should be very close to 2, limited only by machine precision.\n- **Test D (Orthogonal Direction):** Uses a direction $p_\\perp$ orthogonal to the gradient, making the first-order term $\\epsilon \\nabla J^\\top p_\\perp$ zero. The remainder is still $\\mathcal{O}(\\epsilon^2)$, so the slope $\\alpha_D$ is expected to be ~2.\n\nFor each test, the program computes $r_1(\\epsilon)$ for a sequence of $\\epsilon$ values and then uses a least-squares fit on the log-transformed data to estimate the convergence slope.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and executes a test suite to verify an adjoint-derived gradient\n    for a variational data assimilation objective function using Taylor remainder tests.\n    \"\"\"\n    # --- Problem Setup: Constants and Givens ---\n    n = 5  # State dimension\n    m = 3  # Observation dimension\n    delta_t = 0.1\n\n    # Background state\n    xb = np.array([0.5, -0.3, 0.8, -1.0, 0.2])\n    # True state for generating observations\n    xtrue = np.array([-0.4, 0.7, -0.2, 0.3, -0.6])\n    # State for evaluation\n    x0 = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\n\n    # Direction vector p\n    v = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    p = v / np.linalg.norm(v)\n\n    # Covariance matrices B and R\n    L_B = np.diag([1.0, 1.2, 1.4, 1.6, 1.8])\n    for i in range(1, n):\n        L_B[i, i - 1] = 0.05\n    B = L_B @ L_B.T\n    B_inv = np.linalg.inv(B)\n\n    L_R = np.diag([0.7, 0.9, 1.1])\n    for i in range(1, m):\n        L_R[i, i - 1] = 0.02\n    R = L_R @ L_R.T\n    R_inv = np.linalg.inv(R)\n\n    # Observation operator matrix C\n    C = np.array([\n        [1, 0, 0.2, 0, 0],\n        [0, 0.5, 0, 0.1, 0],\n        [0, 0, 0, 0.3, 1]\n    ])\n\n    # Linear model matrix A for Test C\n    A = np.eye(n) + delta_t * np.diag([0.2, -0.1, 0.05, -0.2, 0.1])\n    \n    # Epsilon values for Taylor test\n    epsilons = np.array([\n        1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5\n    ])\n\n    # --- Model and Operator Definitions ---\n\n    # Nonlinear forward model M(x)\n    def M_nl(x):\n        f = np.sin(x) + 0.1 * x**2\n        return x + delta_t * f\n\n    # Nonlinear observation operator H(z)\n    def H_nl(z):\n        h = C @ z\n        # Per problem description, nonlinear part is componentwise on first m components\n        h += 0.05 * (z[:m]**2)\n        return h\n\n    # Jacobian of M_nl(x)\n    def M_nl_jac(x):\n        diag_f_prime = np.cos(x) + 0.2 * x\n        return np.eye(n) + delta_t * np.diag(diag_f_prime)\n\n    # Jacobian of H_nl(z)\n    def H_nl_jac(z):\n        jac = np.copy(C)\n        jac[:m, :m] += np.diag(0.1 * z[:m])\n        return jac\n\n    # Linear model and operator for Test C\n    def M_lin(x):\n        return A @ x\n    def H_lin(z):\n        return C @ z\n\n    # --- Objective Function and Gradient ---\n\n    def objective_J(x, y, M_func, H_func, B_inv, R_inv, xb):\n        J_b = 0.5 * (x - xb).T @ B_inv @ (x - xb)\n        hx = H_func(M_func(x))\n        J_o = 0.5 * (hx - y).T @ R_inv @ (hx - y)\n        return J_b + J_o\n\n    def gradient_J(x, y, M_func, H_func, B_inv, R_inv, xb, M_jac_T_provider, H_jac_func):\n        # Background gradient component\n        grad_b = B_inv @ (x - xb)\n        \n        # Observation gradient component (adjoint formulation)\n        z = M_func(x)\n        hx = H_func(z)\n        H_jac_val = H_jac_func(z) # H'(M(x))\n        M_jac_T_val = M_jac_T_provider(x) # M'(x)^T (or faulty version)\n        \n        # Adjoint variable: (H'(z))^T * R^-1 * (H(z) - y)\n        adj = H_jac_val.T @ (R_inv @ (hx - y))\n        # Propagate adjoint variable back through model: (M'(x))^T * adj\n        grad_o = M_jac_T_val @ adj\n        \n        return grad_b + grad_o\n\n    # --- Test Execution ---\n    \n    def run_taylor_test(x0_test, p_test, grad_j_val, M_func, H_func, y_test):\n        J0 = objective_J(x0_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n        gradJ_p = grad_j_val.T @ p_test\n        \n        log_eps, log_remainders = [], []\n        for eps in epsilons:\n            J_eps = objective_J(x0_test + eps * p_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n            remainder = abs(J_eps - J0 - eps * gradJ_p)\n            if remainder > 1e-15: # Avoid log(0)\n                log_eps.append(np.log(eps))\n                log_remainders.append(np.log(remainder))\n        \n        # Perform linear regression to find slope\n        if len(log_eps) > 1:\n            slope, _ = np.polyfit(log_eps, log_remainders, 1)\n            return slope\n        return np.nan\n\n    all_slopes = []\n\n    # Generate observation data `y` for nonlinear tests\n    y_nl = H_nl(M_nl(xtrue))\n\n    # --- Test A: Nonlinear, Correct Gradient ---\n    grad_A = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x).T, H_nl_jac)\n    alpha_A = run_taylor_test(x0, p, grad_A, M_nl, H_nl, y_nl)\n    all_slopes.append(alpha_A)\n\n    # --- Test B: Nonlinear, Faulty Gradient ---\n    # Faulty implementation using M'(x) instead of M'(x)^T\n    grad_B = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x), H_nl_jac) # Using jacobian, not its transpose\n    alpha_B = run_taylor_test(x0, p, grad_B, M_nl, H_nl, y_nl)\n    all_slopes.append(alpha_B)\n\n    # --- Test C: Linear-Quadratic Reference ---\n    y_lin = H_lin(M_lin(xtrue))\n    \n    def M_lin_jac_T(x): return A.T\n    def H_lin_jac(z): return C\n        \n    grad_C = gradient_J(x0, y_lin, M_lin, H_lin, B_inv, R_inv, xb, M_lin_jac_T, H_lin_jac)\n    alpha_C = run_taylor_test(x0, p, grad_C, M_lin, H_lin, y_lin)\n    all_slopes.append(alpha_C)\n    \n    # --- Test D: Orthogonal Direction ---\n    g_nl = grad_A # Use correct nonlinear gradient\n    # Project p to be orthogonal to the gradient g_nl\n    p_ortho = p - (g_nl.T @ p / (g_nl.T @ g_nl)) * g_nl\n    p_ortho_norm = p_ortho / np.linalg.norm(p_ortho)\n    \n    alpha_D = run_taylor_test(x0, p_ortho_norm, g_nl, M_nl, H_nl, y_nl)\n    all_slopes.append(alpha_D)\n\n    # --- Final Output ---\n    formatted_slopes = [f\"{s:.3f}\" for s in all_slopes]\n    print(f\"[{','.join(formatted_slopes)}]\")\n\nsolve()\n```"
        }
    ]
}