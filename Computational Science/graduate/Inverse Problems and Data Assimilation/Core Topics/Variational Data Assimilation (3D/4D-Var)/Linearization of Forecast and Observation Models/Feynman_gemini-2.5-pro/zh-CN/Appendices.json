{
    "hands_on_practices": [
        {
            "introduction": "变分资料同化的基石是能够高效而准确地计算预报和观测模型的切线性模型及其伴随模型。本练习将通过一个物理上一致但非平凡的辐射传输模型，为您提供亲手实现这些关键组件的机会。您将学习实施标准的验证程序——泰勒检验和伴随检验——这是确保复杂代码在实际应用中正确无误的必要步骤，从而掌握资料同化领域的核心实践技能 。",
            "id": "3398784",
            "problem": "考虑一个一维、平面平行、纯吸收、非散射的离散层辐射传输正演/观测模型 $H:\\mathbb{R}^N\\to\\mathbb{R}^K$。该模型基于比尔-朗伯定律，通过以下物理上一致的构造来定义。设状态向量为 $x\\in\\mathbb{R}^N$，其中层光学厚度由 $\\tau_i=\\exp(x_i)$ 给出，以强制其为正值。对于每个光谱通道 $k=1,\\dots,K$，定义通道-层光学厚度 $\\Delta\\tau_{k,i}=w_{k,i}\\,\\tau_i$，其中 $w_{k,i}>0$ 是已知的通道-层权重系数。设通道 $k$ 的大气层顶太阳源项为 $s_k>0$，并设每层 $i$ 有一个依赖于通道的发射/源项 $b_{k,i}\\ge 0$，假定其在层内为常数。定义从第 $i$ 层顶部到第 $i+1$ 层顶部的向上透射率为 $Z_{k,i}=\\exp(-\\Delta\\tau_{k,i})$，到第 $i$ 层正上方的累积向上透射率为 $T_{k,i-1}=\\prod_{j=1}^{i-1} Z_{k,j}$（其中 $T_{k,0}=1$），以及第 $i$ 层的发射逃逸分数为 $L_{k,i}=1-Z_{k,i}$。则通道 $k$ 的大气层顶上行辐射率为\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}.\n$$\n综合所有通道，正演模型为 $H(x)=(y_1,\\dots,y_K)^\\top$。\n\n您必须实现：\n1. 上面定义的正演模型 $H(x)$。\n2. 对于任意方向 $\\delta\\in\\mathbb{R}^N$ 的方向导数（切线性作用）$D H_x(\\delta)$，使用第一性原理和链式法则。计算方向导数时，必须避免显式地构造完整的雅可比矩阵。\n3. 对于任意 $\\eta\\in\\mathbb{R}^K$ 的伴随作用 $A_x(\\eta)=\\big(D H_x\\big)^\\top \\eta$，通过推导和编码一个满足以下内积恒等式的数学上精确的反向模式/伴随累积过程\n$$\n\\langle D H_x(\\delta),\\,\\eta\\rangle_{\\mathbb{R}^K}=\\langle \\delta,\\,A_x(\\eta)\\rangle_{\\mathbb{R}^N}.\n$$\n\n通过泰勒检验进行验证：对于一组正步长 $\\alpha$，验证泰勒余项度量\n$$\nR(\\alpha) = \\frac{\\left\\|H(x+\\alpha \\delta)-H(x)-D H_x(\\alpha \\delta)\\right\\|_2}{\\alpha}\n$$\n随 $\\alpha$ 线性变化，即 $R(\\alpha)=\\mathcal{O}(\\alpha)$，这对应于 $\\log R(\\alpha)$ 相对于 $\\log \\alpha$ 的双对数关系中斜率接近 1。通过对给定的 $\\alpha$ 值，使用最小二乘法拟合 $\\log_{10} R(\\alpha)$ 与 $\\log_{10} \\alpha$ 的关系，实现对斜率的数值估计。\n\n伴随代码诊断：通过计算相对差异\n$$\n\\mathrm{err}_{\\mathrm{adj}}=\\frac{\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle - \\langle \\delta,\\,A_x(\\eta)\\rangle\\right|}{\\max\\left(10^{-16},\\,\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle\\right|+\\left|\\langle \\delta,\\,A_x(\\eta)\\rangle\\right|\\right)}.\n$$\n来实现伴随内积检验。此外，实现一个在伴随累积中省略太阳项贡献的“有缺陷的”伴随变体，并证明对于这个有缺陷的变体，内积恒等式不成立，而正演/切线性的泰勒检验仍然正确。如果泰勒尺度变换失败（斜率远非 1），报告伴随相对误差以帮助诊断伴随模型是否实现错误。\n\n使用以下参数集测试套件，每个套件提供 $(N,K,w,s,b,x,\\delta,\\eta,\\text{faulty})$，其中 $N$ 是层数， $K$ 是通道数， $w\\in\\mathbb{R}^{K\\times N}$， $s\\in\\mathbb{R}^K$， $b\\in\\mathbb{R}^{K\\times N}$， $x\\in\\mathbb{R}^N$， $\\delta\\in\\mathbb{R}^N$ 以及 $\\eta\\in\\mathbb{R}^K$。所有量都是无量纲的。用于泰勒检验的步长列表为 $\\alpha\\in\\{10^{-1},\\,5\\times 10^{-2},\\,2.5\\times 10^{-2},\\,1.25\\times 10^{-2},\\,6.25\\times 10^{-3}\\}$：\n\n- 案例 1（通用“理想路径”，正确的伴随模型）：\n    - $N=6$， $K=4$。\n    - $w_{k,i}=0.2+0.05\\,k+0.03\\,i$ 对于 $k\\in\\{1,2,3,4\\}$， $i\\in\\{1,\\dots,6\\}$。\n    - $s_k=1.0+0.2\\,k$。\n    - $b_{k,i}=0.1+0.05\\,i+0.03\\,k$。\n    - $x=[-0.8,\\,-0.3,\\,0.2,\\,0.7,\\,-0.5,\\,0.1]^\\top$。\n    - $\\delta=[0.1,\\,-0.2,\\,0.05,\\,-0.1,\\,0.2,\\,-0.05]^\\top$。\n    - $\\eta=[0.3,\\,-0.5,\\,0.7,\\,-0.2]^\\top$。\n    - faulty = False。\n\n- 案例 2（边界情况：极小光学厚度，正确的伴随模型）：\n    - $N=6$， $K=4$。\n    - $w_{k,i}=0.05+0.02\\,k+0.01\\,i$。\n    - $s_k=0.2+0.1\\,k$。\n    - $b_{k,i}=0.02+0.03\\,i+0.01\\,k$。\n    - $x=[-5.5,\\,-4.8,\\,-4.2,\\,-3.9,\\,-3.5,\\,-3.2]^\\top$。\n    - $\\delta=[0.02,\\,-0.01,\\,0.03,\\,-0.02,\\,0.01,\\,-0.03]^\\top$。\n    - $\\eta=[1.0,\\,-0.8,\\,0.6,\\,-0.4]^\\top$。\n    - faulty = False。\n\n- 案例 3（边缘情况：极大光学厚度，正确的伴随模型）：\n    - $N=6$, $K=4$。\n    - $w_{k,i}=0.7+0.1\\,k+0.2\\,i$。\n    - $s_k=1.0+0.3\\,k$。\n    - $b_{k,i}=0.4+0.1\\,i+0.05\\,k$。\n    - $x=[3.0,\\,3.5,\\,4.0,\\,4.5,\\,5.0,\\,3.8]^\\top$。\n    - $\\delta=[-0.05,\\,0.1,\\,-0.08,\\,0.06,\\,-0.04,\\,0.02]^\\top$。\n    - $\\eta=[-0.5,\\,0.4,\\,-0.3,\\,0.2]^\\top$。\n    - faulty = False。\n\n- 案例 4（诊断：与案例 1 相同，但使用有缺陷的伴随模型）：\n    - 使用案例 1 的参数并设置 faulty = True。\n\n对于每个案例，计算：\n- 使用最小二乘法对给定的 $\\alpha$ 值，估计 $\\log_{10} R(\\alpha)$ 相对于 $\\log_{10} \\alpha$ 的斜率（浮点数）。\n- 一个布尔值，指示泰勒检验是否通过，定义为斜率在区间 $[0.9,\\,1.1]$ 内。\n- 伴随内积相对误差（浮点数），如果 faulty = False 则由精确伴随模型计算，如果 faulty = True 则由有缺陷的伴随模型计算。\n- 一个布尔值，指示伴随内积检验是否通过，定义为 $\\mathrm{err}_{\\mathrm{adj}}  10^{-10}$。\n\n您的程序应生成单行输出，其中包含所有案例的结果，形式为逗号分隔的案例结果列表，每个案例结果列表按 [slope, taylor_pass_boolean, adjoint_relative_error, adjoint_pass_boolean] 的顺序排列，并用方括号括起来。例如，输出必须具有以下形式\n$$\n\\texttt{[[slope\\_1,True,err\\_1,True],[slope\\_2,True,err\\_2,True],[slope\\_3,True,err\\_3,True],[slope\\_4,True,err\\_4,False]]}\n$$\n其中布尔值和浮点数反映您计算出的值。不需要物理单位，也不出现角度；所有量都是无量纲的。程序必须是自包含的，并且不需要任何输入。",
            "solution": "问题陈述经评估有效。它在科学上基于辐射传输原理，在数学上是适定的，并以客观、形式化的语言表达。它为一个非平凡的计算任务提供了一个完整且一致的设置，该任务涉及实现一个正演模型、其切线性对应部分以及相应的伴随模型，并包括标准的验证程序。\n\n解决方案首先从第一性原理推导切线性模型和伴随模型的数学表达式，然后概述它们的实现方法。\n\n设状态向量为 $x \\in \\mathbb{R}^N$，测量向量为 $y \\in \\mathbb{R}^K$。正演模型是一个函数 $H:\\mathbb{R}^N \\to \\mathbb{R}^K$，将状态映射到测量值。状态向量的分量 $x_i$ 通过变换 $\\tau_i = \\exp(x_i)$ 与层光学厚度 $\\tau_i$ 相关。这确保了 $\\tau_i  0$。\n\n通道 $k$ 的上行辐射率 $y_k$ 由下式给出：\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}\n$$\n其中中间量定义如下：\n- 通道-层光学厚度：$\\Delta\\tau_{k,i} = w_{k,i}\\,\\tau_i = w_{k,i} \\exp(x_i)$\n- 层的向上透射率：$Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$\n- 层的发射逃逸分数：$L_{k,i} = 1 - Z_{k,i}$\n- 到第 $i$ 层正上方的累积向上透射率：$T_{k,i} = \\prod_{j=1}^{i} Z_{k,j}$，基例为 $T_{k,0} = 1$。因此，主方程中的项 $T_{k,i-1}$ 在 $i=1$ 时是空积，正确计算为 $1$。\n\n**1. 正演模型实现**\n正演模型 $H(x)$ 的实现方式是为每个通道 $k=1,\\dots,K$ 和每个层 $i=1,\\dots,N$ 计算一系列中间变量：\n1.  计算层光学厚度 $\\tau_i = \\exp(x_i)$，对于 $i=1,\\dots,N$。\n2.  对每个通道 $k$，计算通道-层光学厚度 $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$。\n3.  对每个通道 $k$，计算层透射率 $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$。\n4.  对每个通道 $k$，递归计算累积透射率 $T_{k,i}$：$T_{k,0}=1$ 且 $T_{k,i} = T_{k,i-1} Z_{k,i}$，对于 $i=1,\\dots,N$。\n5.  然后使用主方程计算辐射率 $y_k$，该方程可以表示为累积透射率的形式 $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$。\n所有中间量（$\\tau_i, \\Delta\\tau_{k,i}, Z_{k,i}, T_{k,i}$）都被存储起来，以用于切线性和伴随计算。\n\n**2. 切线性模型：方向导数 $D H_x(\\delta)$**\n切线性模型计算 $H$ 的雅可比矩阵对扰动向量 $\\delta \\in \\mathbb{R}^N$ 的作用，记为 $\\delta y = DH_x(\\delta)$，而无需显式构造雅可比矩阵。这是通过将初始扰动 $\\delta x = \\delta$ 沿着正演模型中的操作链进行传播来实现的。令 $\\delta v$ 表示变量 $v$ 的扰动。\n\n1.  层光学厚度的扰动：$\\delta\\tau_i = \\frac{d\\tau_i}{dx_i}\\delta x_i = \\exp(x_i)\\delta x_i = \\tau_i \\delta x_i$。\n2.  通道-层光学厚度的扰动：$\\delta(\\Delta\\tau_{k,i}) = w_{k,i}\\delta\\tau_i = w_{k,i}\\tau_i\\delta x_i = \\Delta\\tau_{k,i}\\delta x_i$。\n3.  层透射率的扰动：$\\delta Z_{k,i} = \\frac{dZ_{k,i}}{d\\Delta\\tau_{k,i}}\\delta(\\Delta\\tau_{k,i}) = -\\exp(-\\Delta\\tau_{k,i})\\delta(\\Delta\\tau_{k,i}) = -Z_{k,i}\\delta(\\Delta\\tau_{k,i})$。\n4.  累积透射率的扰动：由 $T_{k,i} = T_{k,i-1}Z_{k,i}$，根据乘法法则得到 $\\delta T_{k,i} = \\delta T_{k,i-1}Z_{k,i} + T_{k,i-1}\\delta Z_{k,i}$。此式从 $\\delta T_{k,0} = 0$ 开始递归计算。\n5.  最终辐射率扰动为 $\\delta y_k = s_k \\delta T_{k,N} + \\sum_{i=1}^N b_{k,i}(\\delta T_{k,i-1} - \\delta T_{k,i})$。\n\n这种扰动的前向传播产生向量 $\\delta y = (\\delta y_1, \\dots, \\delta y_K)^\\top$。\n\n**3. 伴随模型：转置作用 $A_x(\\eta) = (D H_x)^\\top \\eta$**\n伴随模型计算雅可比矩阵的转置对向量 $\\eta \\in \\mathbb{R}^K$ 的作用。这是通过使用反向模式微分来实现的，该方法将敏感度从输出反向传播到输入。令 $\\bar{v}$ 表示最终标量目标函数 $L = \\langle y, \\eta \\rangle = \\sum_k y_k \\eta_k$ 相对于中间变量 $v$ 的导数。最终结果是导数向量 $\\bar{x}_j = \\partial L / \\partial x_j$。\n\n算法流程如下：\n1.  将所有伴随变量（$\\bar{\\tau}_i, \\bar{\\Delta\\tau}_{k,i}, \\dots$）初始化为 $0$。伴随模型的输入是 $\\bar{y}_k = \\eta_k$，对于 $k=1,\\dots,K$。\n2.  计算是按每个通道 $k$进行的，对 $\\bar{x}$ 的贡献被累积起来。对于每个通道 $k$：\n    a. 初始化一个大小为 $N+1$ 的伴随累积透射率数组 $\\bar{T}_k$ 为零。\n    b. 对于 $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$，对 $\\bar{T}_k$ 的伴随贡献为：\n       - $\\bar{T}_{k,N} \\mathrel{+}= \\eta_k s_k$。对于有缺陷的伴随模型，此步骤将被省略。\n       - 对于 $i=1,\\dots,N$：$\\bar{T}_{k,i-1} \\mathrel{+}= \\eta_k b_{k,i}$ 且 $\\bar{T}_{k,i} \\mathrel{-}= \\eta_k b_{k,i}$。\n    c. 通过累积透射率计算将敏感度反向传播。对于 $i=N,\\dots,1$：\n       由 $T_{k,i} = T_{k,i-1}Z_{k,i}$，我们有：\n       - $\\bar{Z}_{k,i} \\mathrel{+}= \\bar{T}_{k,i} T_{k,i-1}$\n       - $\\bar{T}_{k,i-1} \\mathrel{+}= \\bar{T}_{k,i} Z_{k,i}$\n    d. 对于 $i=1,\\dots,N$，从 $\\bar{Z}_{k,i}$ 传播到 $\\bar{\\Delta\\tau}_{k,i}$：\n       由 $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$，我们有 $\\bar{\\Delta\\tau}_{k,i} \\mathrel{+}= \\bar{Z}_{k,i} (-Z_{k,i})$。\n3.  遍历所有通道 $k$ 后，累积对 $\\bar{\\tau}_i$ 的贡献：\n    由 $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$，我们有 $\\bar{\\tau}_i \\mathrel{+}= \\sum_k \\bar{\\Delta\\tau}_{k,i} w_{k,i}$。\n4.  最后，从 $\\bar{\\tau}_i$ 传播到 $\\bar{x}_i$：\n    由 $\\tau_i = \\exp(x_i)$，我们有 $\\bar{x}_i = \\bar{\\tau}_i \\exp(x_i) = \\bar{\\tau}_i \\tau_i$。\n\n得到的向量 $\\bar{x}$ 就是所求的伴随作用 $A_x(\\eta)$。\n\n**4. 验证程序**\n- **泰勒检验：** 通过验证模型是否一阶精确来检查切线性模型的质量。泰勒余项度量 $R(\\alpha) = \\|H(x+\\alpha\\delta) - H(x) - \\alpha DH_x(\\delta)\\|_2 / \\alpha$ 应为 $\\mathcal{O}(\\alpha)$。这意味着 $R(\\alpha)$ 与 $\\alpha$ 的双对数图的斜率将为 $1$。我们通过线性最小二乘回归来估计这个斜率。斜率在 $[0.9, 1.1]$ 范围内表示通过。\n- **伴随检验：** 伴随模型的正确性通过基本内积恒等式 $\\langle DH_x(\\delta), \\eta \\rangle = \\langle \\delta, (DH_x)^\\top\\eta \\rangle$ 来验证。计算此恒等式两边的相对差异。值低于 $10^{-10}$ 表示通过，确认伴随代码是切线性代码的真正转置。省略太阳项的有缺陷的伴随模型预计会无法通过此检验。",
            "answer": "```python\nimport numpy as np\n\nclass RadiativeTransferModel:\n    \"\"\"\n    Implements a discrete-layer radiative transfer model and its derivatives.\n    \"\"\"\n    def __init__(self, N, K, w, s, b):\n        self.N = N\n        self.K = K\n        self.w = w\n        self.s = s\n        self.b = b\n\n        # Pre-allocate for intermediate variables\n        self.tau = np.zeros(N)\n        self.Delta_tau = np.zeros((K, N))\n        self.Z = np.zeros((K, N))\n        self.T = np.zeros((K, N + 1))\n\n    def forward(self, x):\n        \"\"\"Computes the forward model H(x) and stores intermediate values.\"\"\"\n        self.tau = np.exp(x)\n        self.Delta_tau = self.w * self.tau\n        self.Z = np.exp(-self.Delta_tau)\n        \n        self.T[:, 0] = 1.0\n        for i in range(self.N):\n            self.T[:, i + 1] = self.T[:, i] * self.Z[:, i]\n            \n        y = self.s * self.T[:, self.N]\n        for i in range(self.N):\n            # L_ki * T_{k,i-1} = (1 - Z_ki) * T_{k,i-1} = T_{k,i-1} - T_{k,i}\n            y += self.b[:, i] * (self.T[:, i] - self.T[:, i + 1])\n        \n        return y\n\n    def tangent_linear(self, x, delta_x):\n        \"\"\"Computes the directional derivative DH_x(delta_x).\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n\n        delta_tau = self.tau * delta_x\n        delta_Delta_tau = self.w * delta_tau\n        delta_Z = -self.Z * delta_Delta_tau\n        \n        delta_T = np.zeros((self.K, self.N + 1))\n        # delta_T[:, 0] is always 0\n        for i in range(self.N):\n            delta_T[:, i + 1] = delta_T[:, i] * self.Z[:, i] + self.T[:, i] * delta_Z[:, i]\n\n        delta_y = self.s * delta_T[:, self.N]\n        for i in range(self.N):\n            delta_y += self.b[:, i] * (delta_T[:, i] - delta_T[:, i + 1])\n            \n        return delta_y\n\n    def adjoint(self, x, eta, faulty=False):\n        \"\"\"Computes the adjoint action (DH_x)^T * eta.\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n        \n        bar_tau = np.zeros(self.N)\n\n        for k in range(self.K):\n            bar_T_k = np.zeros(self.N + 1)\n            eta_k = eta[k]\n\n            # Contributions from y_k = s_k*T_kN + sum_i b_ki*(T_{k,i-1}-T_{k,i})\n            if not faulty:\n                bar_T_k[self.N] += eta_k * self.s[k]\n            \n            for i in range(self.N): # 1-based index i=1,...,N\n                i_idx = i\n                bar_T_k[i_idx] += eta_k * self.b[k, i_idx]\n                bar_T_k[i_idx + 1] -= eta_k * self.b[k, i_idx]\n\n            bar_Delta_tau_k = np.zeros(self.N)\n            # Propagate backwards from T\n            for i in range(self.N, 0, -1): # 1-based index i=N,...,1\n                i_idx = i - 1\n                \n                # From T_k,i = T_{k,i-1} * Z_{k,i}\n                bar_Z_ki = bar_T_k[i] * self.T[k, i_idx]\n                bar_T_k[i_idx] += bar_T_k[i] * self.Z[k, i_idx]\n\n                # From Z_k,i = exp(-Delta_tau_{k,i})\n                bar_Delta_tau_k[i_idx] += bar_Z_ki * (-self.Z[k, i_idx])\n\n            # Accumulate contributions to bar_tau for channel k\n            # From Delta_tau_ki = w_ki * tau_i\n            bar_tau += bar_Delta_tau_k * self.w[k, :]\n\n        # Final propagation from tau to x\n        # From tau_i = exp(x_i)\n        bar_x = bar_tau * self.tau\n        \n        return bar_x\n\ndef solve():\n    alpha_steps = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3])\n\n    # --- Test Case Data Generation ---\n    test_cases_params = []\n    \n    # Case 1\n    N, K = 6, 4\n    w = np.array([[0.2 + 0.05*(k+1) + 0.03*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.2*(k+1) for k in range(K)])\n    b = np.array([[0.1 + 0.05*(i+1) + 0.03*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-0.8, -0.3, 0.2, 0.7, -0.5, 0.1])\n    delta = np.array([0.1, -0.2, 0.05, -0.1, 0.2, -0.05])\n    eta = np.array([0.3, -0.5, 0.7, -0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 2\n    N, K = 6, 4\n    w = np.array([[0.05 + 0.02*(k+1) + 0.01*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([0.2 + 0.1*(k+1) for k in range(K)])\n    b = np.array([[0.02 + 0.03*(i+1) + 0.01*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-5.5, -4.8, -4.2, -3.9, -3.5, -3.2])\n    delta = np.array([0.02, -0.01, 0.03, -0.02, 0.01, -0.03])\n    eta = np.array([1.0, -0.8, 0.6, -0.4])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 3\n    N, K = 6, 4\n    w = np.array([[0.7 + 0.1*(k+1) + 0.2*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.3*(k+1) for k in range(K)])\n    b = np.array([[0.4 + 0.1*(i+1) + 0.05*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([3.0, 3.5, 4.0, 4.5, 5.0, 3.8])\n    delta = np.array([-0.05, 0.1, -0.08, 0.06, -0.04, 0.02])\n    eta = np.array([-0.5, 0.4, -0.3, 0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 4 (same as 1, but faulty adjoint)\n    params1_copy = test_cases_params[0].copy()\n    params1_copy['faulty'] = True\n    test_cases_params.append(params1_copy)\n    \n    # --- Main Loop ---\n    results = []\n    for params in test_cases_params:\n        p = params\n        model = RadiativeTransferModel(p['N'], p['K'], p['w'], p['s'], p['b'])\n        \n        # --- Taylor Test ---\n        y0 = model.forward(p['x'])\n        dy = model.tangent_linear(p['x'], p['delta'])\n        \n        remainders = []\n        for alpha in alpha_steps:\n            y_alpha = model.forward(p['x'] + alpha * p['delta'])\n            remainder_norm = np.linalg.norm(y_alpha - y0 - alpha * dy)\n            remainders.append(remainder_norm / alpha)\n        \n        log_alphas = np.log10(alpha_steps)\n        log_remainders = np.log10(remainders)\n        slope = np.polyfit(log_alphas, log_remainders, 1)[0]\n        taylor_pass = 0.9 = slope = 1.1\n\n        # --- Adjoint Test ---\n        adj = model.adjoint(p['x'], p['eta'], faulty=p['faulty'])\n        \n        ip1 = np.dot(dy, p['eta'])\n        ip2 = np.dot(p['delta'], adj)\n        \n        adjoint_err = np.abs(ip1 - ip2) / max(1e-16, np.abs(ip1) + np.abs(ip2))\n        adjoint_pass = adjoint_err  1e-10\n        \n        results.append([slope, taylor_pass, adjoint_err, adjoint_pass])\n\n    # --- Formatting Output ---\n    case_results_str = [f'[{r[0]},{r[1]},{r[2]},{r[3]}]' for r in results]\n    final_str = f\"[{','.join(case_results_str)}]\"\n    print(final_str)\n\nsolve()\n```"
        },
        {
            "introduction": "线性化是一种强大的近似，但理解其适用范围的边界至关重要。本练习使用著名的逻辑斯蒂映射——一个能产生混沌行为的简单模型——来探索线性近似失效的条件。通过将线性模型的有效性半径与系统的局部李雅普诺夫指数联系起来，我们将深入理解系统不稳定性是如何从根本上限制线性化在预报和资料同化中的应用范围的 。",
            "id": "3398752",
            "problem": "考虑由逻辑斯谛映射定义的离散时间非线性预测模型，该映射由函数 $f(x) = r\\,x\\,\\left(1-x\\right)$ 定义，其中参数 $r \\in (0,4]$，预测递推关系为 $x_{t+1} = f(x_t)$，时间 $t \\geq 0$ 为整数。观测模型为恒等模型，因此在时间 $t$ 的观测值为 $y_t = x_t$。我们感兴趣的逆问题是，通过最小化一个最小二乘目标函数，从一个观测窗口 $\\{y_1,\\dots,y_T\\}$ 中估计初始条件 $x_0$。这需要使用高斯-牛顿法，该方法基于对预测和观测模型在当前迭代值（即初始条件的当前估计值）附近进行线性化。\n\n任务是将局部拉伸率（由局部李雅普诺夫指数量化）与一阶线性化的有效性半径联系起来，并确定高斯-牛顿步长预计会发散的阈值。您的推导和计算必须遵守以下约束。\n\n1) 在分析 $f$ 的线性化误差时，只能使用带拉格朗日余项的泰勒定理和链式法则作为基础工具。具体来说，考虑单步泰勒展开\n$$\nf(x_t + \\delta x_t) = f(x_t) + f'(x_t)\\,\\delta x_t + \\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2,\n$$\n其中 $\\xi_t$ 位于 $x_t$ 和 $x_t + \\delta x_t$ 之间。在每个时间步 $t = 0,1,\\dots,T-1$ 施加一个相对小的条件，即余项的量级受限于一个固定比例 $\\alpha \\in (0,1)$ 的线性项量级。根据这个条件和跨时间的灵敏度链式法则，推导出一个形式为\n$$\n|\\delta x_0| \\le \\rho(r,\\alpha; x_0,\\dots,x_{T-1})\n$$\n的严格充分界，其中 $\\rho$ 是一个可计算的有效性半径，它保证在 $T$ 个步骤上的累积一阶线性化保持在规定的相对误差内。使用 $f$ 的导数以及这些导数沿着当前迭代值周围的线性化轨迹的乘积来表示 $\\rho$。然后，将灵敏度的指数增长或衰减与有限时间局部李雅普诺夫指数关联起来，该指数对于轨迹 $\\{x_k\\}_{k=0}^{t-1}$ 定义为\n$$\n\\lambda_t(x_0) = \\frac{1}{t} \\sum_{k=0}^{t-1} \\log \\left| f'(x_k) \\right|。\n$$\n解释 $\\rho$ 如何随 $\\exp\\!\\left(-\\sum_{k=0}^{t-1} \\log|f'(x_k)|\\right)$ 缩放，并因此说明它如何依赖于局部李雅普诺夫指数。\n\n2) 将您的推导具体化到逻辑斯谛映射，其中 $f'(x) = r(1-2x)$ 和 $f''(x) = -2r$，并提供一个仅用 $r$、$\\alpha$、线性化轨迹状态 $\\{x_t\\}$ 和前向灵敏度乘积 $\\prod_{k=0}^{t-1} f'(x_k)$ 表示的半径 $\\rho$ 的显式可计算表达式。\n\n3) 对于下面列出的每个测试用例，实现单次高斯-牛顿迭代，以从 $T$ 步的窗口内的恒等观测值中估计初始条件 $x_0$，使用当前迭代值 $x_0^{(0)}$ 和从真实初始条件 $x_0^\\star$ 生成的无噪声观测值。具体来说，令残差为 $r_t = x_{t}(x_0^{(0)}) - y_t$（对于 $t=1,\\dots,T$），雅可比矩阵项为 $J_t = \\frac{\\partial x_t}{\\partial x_0}\\big|_{x_0^{(0)}}$（对于 $t=1,\\dots,T$）。使用正规方程计算此一维最小二乘问题的高斯-牛顿步长 $\\delta x_0$，并更新 $x_0^{(1)} = x_0^{(0)} + \\delta x_0$。将目标函数定义为\n$$\n\\Phi(x_0) = \\tfrac{1}{2}\\sum_{t=1}^{T} \\left(x_t(x_0) - y_t\\right)^2,\n$$\n并确定两个布尔值：\n- 预测发散指标，如果 $|\\delta x_0|$ 超过推导出的半径 $\\rho$，则为 $\\mathrm{True}$。\n- 实际发散指标，如果 $\\Phi(x_0^{(1)})  \\Phi(x_0^{(0)})$ 或者 $x_0^{(1)} \\notin [0,1]$，则为 $\\mathrm{True}$。\n\n4) 使用以下测试套件，其中每个案例都是一个元组 $(r, T, x_0^\\star, x_0^{(0)}, \\alpha)$:\n- 案例 A: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.2,\\,5,\\,0.61,\\,0.60,\\,0.1\\,)$\n- 案例 B: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.9,\\,5,\\,0.61,\\,0.60,\\,0.1\\,)$\n- 案例 C: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.9,\\,10,\\,0.61,\\,0.609,\\,0.1\\,)$\n- 案例 D: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,2.5,\\,4,\\,0.20,\\,0.80,\\,0.1\\,)$\n- 案例 E: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.5,\\,6,\\,0.5005,\\,0.48,\\,0.1\\,)$\n\n对于每个案例，计算：\n- 在任务 1-2 中推导出的、沿着 $x_0^{(0)}$ 的线性化轨迹评估的半径 $\\rho$。\n- 高斯-牛顿步长的绝对值 $|\\delta x_0|$。\n- 预测发散布尔值。\n- 实际发散布尔值。\n\n5) 您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。对于每个案例，按此顺序附加四个值：$\\rho$、 $|\\delta x_0|$、预测发散布尔值、实际发散布尔值。所有浮点数输出必须四舍五入到六位小数。因此，最终输出必须是一个包含 20 个条目的扁平列表，按案例 A 到 E 的顺序排列，例如 $[\\rho_A,|\\delta_A|,\\mathrm{pred}_A,\\mathrm{act}_A,\\rho_B,|\\delta_B|,\\mathrm{pred}_B,\\mathrm{act}_B,\\dots]$。\n\n不涉及物理单位。不涉及角度。如果出现百分比，必须表示为小数，但此处不需要。确保您的实现是自包含的、确定性的，并且不需要用户输入。",
            "solution": "我们从预测模型、观测模型和线性化框架的核心定义开始。模型是逻辑斯谛映射 $f(x) = r\\,x\\,(1-x)$，给出了预测序列 $x_{t+1} = f(x_t)$。观测模型是恒等模型，$y_t = x_t$。逆问题是从 $\\{y_1,\\dots,y_T\\}$ 进行 $x_0$ 的最小二乘估计，在一维情况下，这会产生一个由雅可比矩阵项 $J_t = \\frac{\\partial x_t}{\\partial x_0}$ 计算得到的高斯-牛顿步长。\n\n原理 1：带余项的泰勒定理和每步相对误差。对于时间 $t$ 的单步，带拉格朗日余项的泰勒定理给出\n$$\nf(x_t + \\delta x_t) = f(x_t) + f'(x_t)\\,\\delta x_t + \\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2,\n$$\n对于某个 $\\xi_t$ 位于 $x_t$ 和 $x_t + \\delta x_t$ 之间。为保证一阶线性近似主导二阶项，我们施加充分条件，即对于一个固定的 $\\alpha \\in (0,1)$，\n$$\n\\left|\\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2\\right| \\le \\alpha\\,\\left| f'(x_t)\\,\\delta x_t \\right|, \\quad \\text{对于每个 } t=0,1,\\dots,T-1。\n$$\n消去一个因子 $|\\delta x_t|$（假设 $\\delta x_t \\neq 0$；$\\delta x_t = 0$ 的情况是平凡安全的）得到\n$$\n\\frac{1}{2}\\,|f''(\\xi_t)|\\,|\\delta x_t| \\le \\alpha\\,|f'(x_t)|.\n$$\n一个不需要知道 $\\xi_t$ 的充分条件可以通过用一个一致界对 $|f''(\\xi_t)|$ 进行上界来获得，或者对于一个包含 $x_t$ 和 $x_t + \\delta x_t$ 的区间 $I_t$，使用 $|f''(\\xi_t)| \\le \\sup_{z \\in I_t} |f''(z)|$。对于逻辑斯谛映射，$f''(x) = -2r$ 是常数，所以 $|f''(\\xi_t)| = 2r$ 在任何地方都成立。因此，每步的条件简化为\n$$\n|\\delta x_t| \\le \\frac{2\\alpha\\,|f'(x_t)|}{|f''(\\xi_t)|} = \\frac{2\\alpha\\,|r(1-2x_t)|}{2r} = \\alpha\\,|1-2x_t|.\n$$\n\n原理 2：灵敏度的链式法则。跨越 $t$ 步，扰动传播的一阶满足\n$$\n\\delta x_t \\approx \\left(\\prod_{k=0}^{t-1} f'(x_k)\\right)\\,\\delta x_0 \\equiv J_t\\,\\delta x_0,\n$$\n约定 $J_0 = 1$。将此与每步的充分条件相结合，对于每个 $t=0,1,\\dots,T-1$ 得到界\n$$\n|J_t|\\,|\\delta x_0| \\le \\alpha\\,|1-2x_t| \\quad \\Rightarrow \\quad |\\delta x_0| \\le \\frac{\\alpha\\,|1-2x_t|}{|J_t|}.\n$$\n因此，通过取最小值，我们得到了在整个窗口上的一致充分界：\n$$\n\\rho(r,\\alpha; x_0,\\dots,x_{T-1}) \\equiv \\min_{0 \\le t \\le T-1} \\frac{\\alpha\\,|1-2x_t|}{\\left|\\prod_{k=0}^{t-1} f'(x_k)\\right|}。\n$$\n这是在围绕当前迭代值的轨迹 $\\{x_t\\}$ 进行线性化时，具有容差 $\\alpha$ 的一阶线性化的可计算有效性半径。\n\n原理 3：与局部李雅普诺夫指数的关系。定义沿线性化轨迹的有限时间局部李雅普诺夫指数为\n$$\n\\lambda_t(x_0) = \\frac{1}{t} \\sum_{k=0}^{t-1} \\log |f'(x_k)|, \\quad t \\ge 1。\n$$\n注意到 $\\left|\\prod_{k=0}^{t-1} f'(x_k)\\right| = \\exp\\!\\left(\\sum_{k=0}^{t-1} \\log |f'(x_k)|\\right) = \\exp\\!\\left(t\\,\\lambda_t(x_0)\\right)$，我们得到缩放关系\n$$\n\\rho_t \\equiv \\frac{\\alpha\\,|1-2x_t|}{|J_t|} = \\alpha\\,|1-2x_t|\\,\\exp\\!\\left(-\\sum_{k=0}^{t-1} \\log |f'(x_k)|\\right) = \\alpha\\,|1-2x_t|\\,\\exp\\!\\left(-t\\,\\lambda_t(x_0)\\right).\n$$\n因此，\n$$\n\\rho = \\min_{0 \\le t \\le T-1} \\rho_t,\n$$\n这表明在任何前缀 $(0,\\dots,t-1)$ 上的正平均局部李雅普诺夫指数会使可接受的半径随 $t$ 指数级缩小，而负平均拉伸则会扩大半径。这直接将局部李雅普诺夫指数与线性化的有效性半径联系起来。\n\n原理 4：针对逻辑斯谛映射的具体化。对于 $f(x)=r\\,x(1-x)$，我们有 $f'(x)=r(1-2x)$ 和 $f''(x)=-2r$。显式的每步半径界及其聚合为\n$$\n\\rho_t = \\frac{\\alpha\\,|1-2x_t|}{\\left|\\prod_{k=0}^{t-1} r(1-2x_k)\\right|}, \\quad \\rho = \\min_{0 \\le t \\le T-1} \\rho_t。\n$$\n这完全可以从当前线性化轨迹 $\\{x_t\\}$ 和灵敏度 $J_t$ 计算得出。\n\n原理 5：一维高斯-牛顿步长。对于恒等观测和残差 $r_t = x_t(x_0^{(0)}) - y_t$（$t=1,\\dots,T$），以及雅可比矩阵项 $J_t = \\frac{\\partial x_t}{\\partial x_0}\\big|_{x_0^{(0)}}$，高斯-牛顿步长求解正规方程\n$$\n\\left(\\sum_{t=1}^T J_t^2\\right)\\,\\delta x_0 = - \\sum_{t=1}^T J_t\\,r_t,\n$$\n因此，只要 $\\sum_{t=1}^T J_t^2  0$，\n$$\n\\delta x_0 = - \\frac{\\sum_{t=1}^T J_t\\,r_t}{\\sum_{t=1}^T J_t^2}。\n$$\n然后我们构建更新后的迭代值 $x_0^{(1)} = x_0^{(0)} + \\delta x_0$ 并计算目标函数\n$$\n\\Phi(x_0) = \\tfrac{1}{2}\\sum_{t=1}^T \\left(x_t(x_0) - y_t\\right)^2。\n$$\n如果 $|\\delta x_0|  \\rho$，我们声明预测发散；如果 $\\Phi(x_0^{(1)})  \\Phi(x_0^{(0)})$ 或 $x_0^{(1)} \\notin [0,1]$，我们声明实际发散。\n\n算法设计：\n- 从 $(r, x_0^\\star)$ 生成真实轨迹 $\\{y_t\\}_{t=1}^T$。\n- 从 $(r, x_0^{(0)})$ 生成线性化轨迹 $\\{x_t\\}_{t=0}^T$。\n- 迭代计算灵敏度 $J_0=1$ 和 $J_{t+1} = J_t\\,f'(x_t)$。\n- 计算半径 $\\rho = \\min_{0 \\le t \\le T-1} \\alpha\\,|1-2x_t| / |J_t|$。\n- 从 $\\{J_t\\}_{t=1}^T$ 和残差计算高斯-牛顿步长 $\\delta x_0$。\n- 评估预测和实际发散。\n- 对每个测试用例重复上述步骤。\n- 将浮点数输出四舍五入到六位小数，并打印一个扁平列表 $[\\rho_A,|\\delta_A|,\\mathrm{pred}_A,\\mathrm{act}_A,\\dots]$。\n\n测试套件覆盖范围：\n- 案例 A 是一个中等拉伸区域，其中 $r=3.2$，初始误差较小；通常在短窗口内 $\\lambda_t$ 不会是强正值，因此 $\\rho$ 是中等大小，并且 $|\\delta x_0| \\le \\rho$ 是合理的。\n- 案例 B 和 C 使用 $r=3.9$，这是一个强混沌区域。当 $T=5$，尤其是 $T=10$ 时，$\\sum \\log|f'|$ 倾向于为正，这会缩小 $\\rho$ 并经常导致 $|\\delta x_0|  \\rho$。\n- 案例 D 使用 $r=2.5$，初始差异较大；尽管拉伸较弱，但大的残差可能会产生一个违反半径或离开单位区间的步长。\n- 案例 E 使用 $r=3.5$，其中 $x_0^\\star$ 接近 $0.5$，此时 $|f'(x)|$ 很小；这测试了对近临界点的敏感性，在这些点 $|1-2x_t|$ 可能很小，从而局部地缩小 $\\rho$ 并可能引发发散。\n\n至此，推导和算法计划完成。附带的程序精确地实现了这些步骤，并生成所需的单行输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logistic_step(x, r):\n    return r * x * (1.0 - x)\n\ndef logistic_traj_and_sensitivities(r, x0, T):\n    \"\"\"\n    Compute trajectory x[0..T] with x[0]=x0, and sensitivities J[0..T]\n    where J[t] = d x_t / d x0 evaluated along the trajectory of x0.\n    \"\"\"\n    x = np.empty(T + 1, dtype=float)\n    J = np.empty(T + 1, dtype=float)\n    x[0] = x0\n    J[0] = 1.0\n    for t in range(T):\n        # Derivative f'(x_t) = r (1 - 2 x_t)\n        fp = r * (1.0 - 2.0 * x[t])\n        x[t + 1] = logistic_step(x[t], r)\n        J[t + 1] = J[t] * fp\n    return x, J\n\ndef observations_from_truth(r, x0_true, T):\n    y = np.empty(T + 1, dtype=float)\n    y[0] = x0_true\n    for t in range(T):\n        y[t + 1] = logistic_step(y[t], r)\n    # Return y[1..T]\n    return y[1:]\n\ndef compute_radius_alpha(r, alpha, x_traj, J_sens):\n    \"\"\"\n    Compute rho = min_{t=0..T-1} alpha * |1-2 x_t| / |J_t|\n    where x_traj has length T+1, J_sens has length T+1, and we use t = 0..T-1.\n    \"\"\"\n    T = len(x_traj) - 1\n    eps = 1e-15\n    radii = []\n    for t in range(T):\n        numerator = alpha * abs(1.0 - 2.0 * x_traj[t])\n        denom = abs(J_sens[t])\n        # Avoid division by zero; use a very small denominator to keep a very large radius.\n        denom = max(denom, eps)\n        radii.append(numerator / denom)\n    rho = min(radii) if radii else 0.0\n    return rho\n\ndef gauss_newton_step(r, x0_guess, T, y_obs):\n    \"\"\"\n    One Gauss-Newton step for initial condition estimation with identity observations.\n    Returns:\n      delta_x0, phi_before, phi_after, x0_new\n    \"\"\"\n    # Trajectory and sensitivities at guess\n    xg, Jg = logistic_traj_and_sensitivities(r, x0_guess, T)\n    # Residuals r_t for t=1..T: r_t = x_t - y_t\n    residuals = xg[1:] - y_obs\n    # Jacobian entries J_t for t=1..T are Jg[1:]\n    Jrows = Jg[1:]\n    # Normal equations in 1D\n    JTJ = float(np.dot(Jrows, Jrows))\n    JTr = float(np.dot(Jrows, residuals))\n    # Objective before\n    phi_before = 0.5 * float(np.dot(residuals, residuals))\n    if JTJ  1e-18:\n        # Ill-conditioned or zero sensitivity: take no step (or treat as divergence later)\n        delta = 0.0\n    else:\n        delta = - JTr / JTJ\n    x0_new = x0_guess + delta\n    # Objective after\n    x_new_traj, _ = logistic_traj_and_sensitivities(r, x0_new, T)\n    res_after = x_new_traj[1:] - y_obs\n    phi_after = 0.5 * float(np.dot(res_after, res_after))\n    return delta, phi_before, phi_after, x0_new, xg, Jg\n\ndef run_case(case):\n    r, T, x0_true, x0_guess, alpha = case\n    # Generate noise-free observations y_t from truth\n    y_obs = observations_from_truth(r, x0_true, T)\n    # Compute one GN step at the guess\n    delta, phi_before, phi_after, x0_new, xg, Jg = gauss_newton_step(r, x0_guess, T, y_obs)\n    # Compute rho at the guess trajectory\n    rho = compute_radius_alpha(r, alpha, xg, Jg)\n    # Predicted divergence if |delta| > rho\n    predicted_diverge = abs(delta) > rho\n    # Actual divergence if objective increases or new iterate leaves [0,1]\n    actual_diverge = (phi_after > phi_before) or (x0_new  0.0) or (x0_new > 1.0)\n    return rho, abs(delta), predicted_diverge, actual_diverge\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (r, T, x0_true, x0_guess, alpha)\n    test_cases = [\n        (3.2, 5, 0.61, 0.60, 0.1),      # Case A\n        (3.9, 5, 0.61, 0.60, 0.1),      # Case B\n        (3.9, 10, 0.61, 0.609, 0.1),    # Case C\n        (2.5, 4, 0.20, 0.80, 0.1),      # Case D\n        (3.5, 6, 0.5005, 0.48, 0.1),    # Case E\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, abs_delta, pred_div, act_div = run_case(case)\n        # Round floats to six decimal places as required\n        rho_r = round(float(rho), 6)\n        abs_delta_r = round(float(abs_delta), 6)\n        results.extend([rho_r, abs_delta_r, pred_div, act_div])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "大多数线性化理论都假定底层模型是光滑可微的，然而真实世界的模型常常包含“开关”或条件逻辑，这会产生不可微点。本练习将挑战这一假设，通过一个具有切换动力学的系统来分析这种情况，这在模拟相变等物理现象的模型中很常见。通过计算单侧方向导数，您将直接观察到不可微性如何影响代价函数的地形，并揭示其对标准优化算法带来的挑战 。",
            "id": "3398756",
            "problem": "考虑一个具有切换动力学的二维预报映射，定义为\n$$\nM(x) \\;=\\;\n\\begin{cases}\nM_{1}(x),  \\text{if } c(x) \\ge 0,\\\\\nM_{2}(x),  \\text{if } c(x)  0,\n\\end{cases}\n$$\n其中 $x \\in \\mathbb{R}^{2}$，切换函数为 $c(x) = w^{\\top} x$，其中 $w = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，分支模型为\n$$\nM_{1}(x) \\,=\\, F_{1} x + g_{1}(x), \\qquad M_{2}(x) \\,=\\, F_{2} x + g_{2}(x),\n$$\n其中\n$$\nF_{1} \\,=\\, \\begin{pmatrix} 2  0.5 \\\\ 0  1.2 \\end{pmatrix}, \\quad g_{1}(x) \\,=\\, \\begin{pmatrix} \\sin(x_{1}) \\\\ x_{1} x_{2} \\end{pmatrix}, \\qquad\nF_{2} \\,=\\, \\begin{pmatrix} 1.5  -0.2 \\\\ 0.3  0.8 \\end{pmatrix}, \\quad g_{2}(x) \\,=\\, \\begin{pmatrix} \\cos(x_{2}) - 1 \\\\ 0.5\\, x_{1}^{2} \\end{pmatrix}.\n$$\n观测算子是非线性的，\n$$\nh(z) \\,=\\, \\begin{pmatrix} z_{1} + \\exp(z_{2}) - 1 \\\\ z_{1} z_{2} \\end{pmatrix}, \\qquad z \\in \\mathbb{R}^{2}.\n$$\n定义一步三维变分（Three-Dimensional Variational Data Assimilation）代价函数\n$$\nJ(x) \\,=\\, \\tfrac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\tfrac{1}{2}\\,\\big(h(M(x)) - y\\big)^{\\top} R^{-1} \\big(h(M(x)) - y\\big),\n$$\n其中 $B = I_{2}$，$R = I_{2}$，背景态为 $x_{b} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$，观测值为 $y = \\begin{pmatrix} 1 \\\\ -0.5 \\end{pmatrix}$。\n\n在切换点 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$（其中 $c(x_{0}) = 0$）进行分析，并考虑沿方向 $p = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 的同化轨迹，使得 $x(\\varepsilon) = x_{0} + \\varepsilon p$ 对于任意符号的 $\\varepsilon$ 都会穿过切换面 $c(x) = 0$。从 Gateaux 方向导数的基本定义以及可微映射复合的链式法则出发，完成以下任务：\n\n1. 对于每个分支 $M_{i}$，$i \\in \\{1,2\\}$，推导在 $x_{0}$ 处的切线性模型，即雅可比矩阵 $\\mathrm{D}M_{i}(x_{0})$。\n2. 推导观测算子在预报状态 $M(x_{0})$ 处的雅可比矩阵 $\\mathrm{D}h(M(x_{0}))$。\n3. 使用方向导数的定义和应用于 $h \\circ M$ 的链式法则，获得单侧方向导数的表达式\n$$\nD_{+}J(x_{0};p) \\,=\\, \\lim_{\\varepsilon \\downarrow 0} \\frac{J(x_{0} + \\varepsilon p) - J(x_{0})}{\\varepsilon}, \\qquad\nD_{-}J(x_{0};p) \\,=\\, \\lim_{\\varepsilon \\uparrow 0} \\frac{J(x_{0} + \\varepsilon p) - J(x_{0})}{\\varepsilon},\n$$\n分别用 $\\mathrm{D}M_{1}(x_{0})$ 和 $\\mathrm{D}M_{2}(x_{0})$ 表示。\n4. 对于给定的 $F_{1}$、$g_{1}$、$F_{2}$、$g_{2}$、$h$、$x_{b}$ 和 $y$，数值计算 $D_{+}J(x_{0};p)$ 和 $D_{-}J(x_{0};p)$。\n5. 计算穿过切换面时方向导数的有符号跳跃，\n$$\n\\Delta \\,=\\, D_{+}J(x_{0};p) \\;-\\; D_{-}J(x_{0};p),\n$$\n该跳跃量化了 $J$ 沿同化轨迹穿过 $c(x) = 0$ 时的不可微性。\n\n将您计算出的 $\\Delta$ 的最终答案四舍五入到四位有效数字，并仅报告此舍入值。不需要单位。",
            "solution": "该问题要求计算三维变分（3D-Var）代价函数 $J(x)$ 在预报模型 $M(x)$ 的一个不可微点处的方向导数跳跃。代价函数由下式给出\n$$J(x) = \\frac{1}{2}(x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\frac{1}{2}(h(M(x)) - y)^{\\top} R^{-1} (h(M(x)) - y)$$\n当 $B = I_2$ 和 $R = I_2$ 时，该式可简化为\n$$J(x) = \\frac{1}{2}\\|x - x_{b}\\|^{2} + \\frac{1}{2}\\|h(M(x)) - y\\|^{2}$$\n预报模型 $M(x)$ 有一个基于 $c(x) = w^{\\top}x = x_1 - x_2$ 符号的切换。\n$$\nM(x) =\n\\begin{cases}\nM_{1}(x),  \\text{if } x_1 - x_2 \\ge 0,\\\\\nM_{2}(x),  \\text{if } x_1 - x_2  0,\n\\end{cases}\n$$\n题目要求我们分析 $J(x)$ 在 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$（其中 $c(x_0)=0$）处沿方向 $p = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 的行为。\n\n路径由 $x(\\varepsilon) = x_0 + \\varepsilon p = \\varepsilon p$ 定义。沿此路径的切换条件是 $c(x(\\varepsilon)) = w^{\\top}(\\varepsilon p) = \\varepsilon (w^{\\top}p)$。\n计算 $w^{\\top}p$：\n$$w^{\\top}p = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (-1)(-1) = 2$$\n因此，$c(x(\\varepsilon)) = 2\\varepsilon$。对于 $\\varepsilon  0$，$c(x(\\varepsilon))  0$ 且 $M(x(\\varepsilon))=M_1(x(\\varepsilon))$。对于 $\\varepsilon  0$，$c(x(\\varepsilon))  0$ 且 $M(x(\\varepsilon))=M_2(x(\\varepsilon))$。\n\n解题过程按要求分为五个步骤。\n\n**1. 推导切线性模型 $\\mathrm{D}M_{1}(x_{0})$ 和 $\\mathrm{D}M_{2}(x_{0})$**\n\n切线性模型是预报模型的雅可比矩阵。\n对于第一个分支，$M_{1}(x) = F_{1} x + g_{1}(x)$，其雅可比矩阵为 $\\mathrm{D}M_{1}(x) = F_{1} + \\mathrm{D}g_{1}(x)$。\n函数 $g_{1}(x)$ 及其雅可比矩阵 $\\mathrm{D}g_{1}(x)$ 为：\n$$g_{1}(x) = \\begin{pmatrix} \\sin(x_{1}) \\\\ x_{1} x_{2} \\end{pmatrix} \\implies \\mathrm{D}g_{1}(x) = \\begin{pmatrix} \\cos(x_{1})  0 \\\\ x_{2}  x_{1} \\end{pmatrix}$$\n在 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处求值：\n$$\\mathrm{D}g_{1}(x_{0}) = \\begin{pmatrix} \\cos(0)  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$$\n因此，第一个分支在 $x_0$ 处的切线性模型为：\n$$\\mathrm{D}M_{1}(x_{0}) = F_{1} + \\mathrm{D}g_{1}(x_{0}) = \\begin{pmatrix} 2  0.5 \\\\ 0  1.2 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 3  0.5 \\\\ 0  1.2 \\end{pmatrix}$$\n\n对于第二个分支，$M_{2}(x) = F_{2} x + g_{2}(x)$，其雅可比矩阵为 $\\mathrm{D}M_{2}(x) = F_{2} + \\mathrm{D}g_{2}(x)$。\n函数 $g_{2}(x)$ 及其雅可比矩阵 $\\mathrm{D}g_{2}(x)$ 为：\n$$g_{2}(x) = \\begin{pmatrix} \\cos(x_{2}) - 1 \\\\ 0.5\\, x_{1}^{2} \\end{pmatrix} \\implies \\mathrm{D}g_{2}(x) = \\begin{pmatrix} 0  -\\sin(x_{2}) \\\\ x_{1}  0 \\end{pmatrix}$$\n在 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处求值：\n$$\\mathrm{D}g_{2}(x_{0}) = \\begin{pmatrix} 0  -\\sin(0) \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$$\n因此，第二个分支在 $x_0$ 处的切线性模型为：\n$$\\mathrm{D}M_{2}(x_{0}) = F_{2} + \\mathrm{D}g_{2}(x_{0}) = \\begin{pmatrix} 1.5  -0.2 \\\\ 0.3  0.8 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1.5  -0.2 \\\\ 0.3  0.8 \\end{pmatrix}$$\n\n**2. 推导雅可比矩阵 $\\mathrm{D}h(M(x_{0}))$**\n\n首先，我们必须计算状态 $M(x_0)$。由于 $c(x_0)=0$，我们使用第一个分支的定义：\n$$M(x_{0}) = M_{1}(x_{0}) = F_{1}x_{0} + g_{1}(x_{0}) = \\begin{pmatrix} 2  0.5 \\\\ 0  1.2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\sin(0) \\\\ 0 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n通过计算 $M_2(x_0)$ 来检查 $M(x)$ 在 $x_0$ 处的连续性：\n$$M_{2}(x_{0}) = F_{2}x_{0} + g_{2}(x_{0}) = \\begin{pmatrix} 1.5  -0.2 \\\\ 0.3  0.8 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\cos(0)-1 \\\\ 0.5 \\cdot 0^2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n由于 $M_1(x_0) = M_2(x_0)$，模型 $M(x)$ 在 $x_0$ 处是连续的。令 $z_0 = M(x_0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n现在我们求观测算子 $h(z)$ 在 $z_0$ 处的雅可比矩阵。\n$$h(z) = \\begin{pmatrix} z_{1} + \\exp(z_{2}) - 1 \\\\ z_{1} z_{2} \\end{pmatrix} \\implies \\mathrm{D}h(z) = \\begin{pmatrix} 1  \\exp(z_{2}) \\\\ z_{2}  z_{1} \\end{pmatrix}$$\n在 $z_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 处求值：\n$$\\mathrm{D}h(M(x_{0})) = \\mathrm{D}h(z_{0}) = \\begin{pmatrix} 1  \\exp(0) \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}$$\n\n**3. 推导单侧方向导数 $D_{+}J(x_{0};p)$ 和 $D_{-}J(x_{0};p)$**\n\n代价函数为 $J(x) = J_b(x) + J_o(x)$，其中 $J_b(x) = \\frac{1}{2}\\|x-x_b\\|^2$ 是背景项，$J_o(x) = \\frac{1}{2}\\|h(M(x))-y\\|^2$ 是观测项。\n方向导数是线性的，因此 $D J(x_0; p) = D J_b(x_0; p) + D J_o(x_0; p)$。\n背景项 $J_b(x)$ 处处可微。其梯度为 $\\nabla J_b(x) = x - x_b$。方向导数为 $D J_b(x_0; p) = \\nabla J_b(x_0)^\\top p = (x_0 - x_b)^\\top p$。对于两个单侧导数，该项是相同的。\n\n对于右侧导数 $D_{+}J(x_{0};p)$，我们考虑 $\\varepsilon \\downarrow 0$。在这种情况下，$c(x_0+\\varepsilon p)  0$，所以 $M(x_0+\\varepsilon p)=M_1(x_0+\\varepsilon p)$。\n$$D_{+}J(x_{0};p) = \\lim_{\\varepsilon \\downarrow 0} \\frac{J(x_{0} + \\varepsilon p) - J(x_{0})}{\\varepsilon}$$\n这是函数 $x \\mapsto \\frac{1}{2}\\|x-x_b\\|^2 + \\frac{1}{2}\\|h(M_1(x))-y\\|^2$ 在 $x_0$ 处沿方向 $p$ 的方向导数。由于该函数是可微函数的复合，我们可以应用链式法则。$M_1$ 分支的 $J_o(x)$ 的梯度是 $\\nabla J_{o,1}(x) = (\\mathrm{D}M_1(x))^\\top (\\mathrm{D}h(M_1(x)))^\\top (h(M_1(x))-y)$。\n$$D_{+}J(x_0;p) = (x_0 - x_b)^\\top p + (h(M(x_0))-y)^\\top \\mathrm{D}h(M(x_0)) \\mathrm{D}M_1(x_0) p$$\n\n对于左侧导数 $D_{-}J(x_{0};p)$，我们考虑 $\\varepsilon \\uparrow 0$。在这种情况下，$c(x_0+\\varepsilon p)  0$，所以 $M(x_0+\\varepsilon p)=M_2(x_0+\\varepsilon p)$。\n通过类似的论证，这对应于 $x \\mapsto \\frac{1}{2}\\|x-x_b\\|^2 + \\frac{1}{2}\\|h(M_2(x))-y\\|^2$ 的方向导数。\n$$D_{-}J(x_0;p) = (x_0 - x_b)^\\top p + (h(M(x_0))-y)^\\top \\mathrm{D}h(M(x_0)) \\mathrm{D}M_2(x_0) p$$\n\n**4. $D_{+}J(x_{0};p)$ 和 $D_{-}J(x_{0};p)$ 的数值计算**\n\n我们将上述推导出的表达式的数值进行汇总计算。\n给定值：$x_0=\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，$x_b=\\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$，$y=\\begin{pmatrix} 1 \\\\ -0.5 \\end{pmatrix}$，$p=\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n背景项导数：\n$$(x_0 - x_b)^\\top p = \\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}\\right)^\\top \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -0.2  0.1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = -0.2 - 0.1 = -0.3$$\n\n观测项分量：\n新息向量 $d = h(M(x_0)) - y$：\n$$h(M(x_0)) = h\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0+\\exp(0)-1 \\\\ 0 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n$$d = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix}$$\n令 $\\mathbf{H}' = \\mathrm{D}h(M(x_0)) = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}$。\n令 $\\mathbf{M}_1' = \\mathrm{D}M_1(x_0) = \\begin{pmatrix} 3  0.5 \\\\ 0  1.2 \\end{pmatrix}$ 和 $\\mathbf{M}_2' = \\mathrm{D}M_2(x_0) = \\begin{pmatrix} 1.5  -0.2 \\\\ 0.3  0.8 \\end{pmatrix}$。\n\n$D_{+}J(x_0; p)$ 的观测部分：\n$$d^\\top \\mathbf{H}' \\mathbf{M}_1' p = \\begin{pmatrix} -1  0.5 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 3  0.5 \\\\ 0  1.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$$\n$$d^\\top \\mathbf{H}' = \\begin{pmatrix} -1  -1 \\end{pmatrix}$$\n$$\\mathbf{M}_1' p = \\begin{pmatrix} 3  0.5 \\\\ 0  1.2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ -1.2 \\end{pmatrix}$$\n$$d^\\top \\mathbf{H}' \\mathbf{M}_1' p = \\begin{pmatrix} -1  -1 \\end{pmatrix} \\begin{pmatrix} 2.5 \\\\ -1.2 \\end{pmatrix} = -2.5 + 1.2 = -1.3$$\n总的右侧导数：\n$$D_{+}J(x_0; p) = -0.3 + (-1.3) = -1.6$$\n\n$D_{-}J(x_0; p)$ 的观测部分：\n$$d^\\top \\mathbf{H}' \\mathbf{M}_2' p = \\begin{pmatrix} -1  -1 \\end{pmatrix} \\mathbf{M}_2' p$$\n$$\\mathbf{M}_2' p = \\begin{pmatrix} 1.5  -0.2 \\\\ 0.3  0.8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1.5 + 0.2 \\\\ 0.3 - 0.8 \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ -0.5 \\end{pmatrix}$$\n$$d^\\top \\mathbf{H}' \\mathbf{M}_2' p = \\begin{pmatrix} -1  -1 \\end{pmatrix} \\begin{pmatrix} 1.7 \\\\ -0.5 \\end{pmatrix} = -1.7 + 0.5 = -1.2$$\n总的左侧导数：\n$$D_{-}J(x_0; p) = -0.3 + (-1.2) = -1.5$$\n\n**5. 跳跃 $\\Delta$ 的计算**\n\n方向导数的有符号跳跃为：\n$$\\Delta = D_{+}J(x_0; p) - D_{-}J(x_0; p) = (-1.6) - (-1.5) = -0.1$$\n问题要求将此值四舍五入到四位有效数字。\n$$-0.1 = -0.1000$$\n\n或者，跳跃也可以计算为：\n$$\\Delta = d^\\top \\mathbf{H}' (\\mathbf{M}_1' - \\mathbf{M}_2') p$$\n$$\\mathbf{M}_1' - \\mathbf{M}_2' = \\begin{pmatrix} 3 - 1.5  0.5 - (-0.2) \\\\ 0 - 0.3  1.2 - 0.8 \\end{pmatrix} = \\begin{pmatrix} 1.5  0.7 \\\\ -0.3  0.4 \\end{pmatrix}$$\n$$(\\mathbf{M}_1' - \\mathbf{M}_2') p = \\begin{pmatrix} 1.5  0.7 \\\\ -0.3  0.4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1.5 - 0.7 \\\\ -0.3 - 0.4 \\end{pmatrix} = \\begin{pmatrix} 0.8 \\\\ -0.7 \\end{pmatrix}$$\n$$\\Delta = (d^\\top \\mathbf{H}') ((\\mathbf{M}_1' - \\mathbf{M}_2') p) = \\begin{pmatrix} -1  -1 \\end{pmatrix} \\begin{pmatrix} 0.8 \\\\ -0.7 \\end{pmatrix} = -0.8 + 0.7 = -0.1$$\n结果得到确认。四舍五入到四位有效数字，结果是 $-0.1000$。",
            "answer": "$$\\boxed{-0.1000}$$"
        }
    ]
}