{
    "hands_on_practices": [
        {
            "introduction": "This first exercise builds the foundation of our topic. Starting from Bayes' rule, you will derive the Maximum a Posteriori (MAP) objective function for a standard linear-Gaussian inverse problem. The primary goal is to see how the statistical assumptions—specifically the variances of the prior and the observation noise—directly translate into the weighted terms of a Tikhonov regularization functional . This practice illuminates the deep connection between the probabilistic Bayesian perspective and the deterministic variational approach.",
            "id": "3401540",
            "problem": "Consider a linear inverse problem in which a state vector $x \\in \\mathbb{R}^{n}$ is observed through a linear operator $H \\in \\mathbb{R}^{m \\times n}$ with additive noise, yielding observations $y \\in \\mathbb{R}^{m}$ that satisfy $y = H x + \\varepsilon$. Assume the observational noise $\\varepsilon$ is Gaussian with zero mean and covariance matrix $R \\in \\mathbb{R}^{m \\times m}$, and the prior distribution for $x$ is Gaussian with mean $x_{b} \\in \\mathbb{R}^{n}$ and covariance matrix $B \\in \\mathbb{R}^{n \\times n}$. Both $R$ and $B$ are symmetric positive definite and diagonal, with $R = \\operatorname{diag}(r_{1}, \\dots, r_{m})$ and $B = \\operatorname{diag}(b_{1}, \\dots, b_{n})$, where $r_{i} > 0$ and $b_{j} > 0$ for all indices.\n\nStarting from Bayes’ rule and the definition of the multivariate normal density, derive the Maximum a Posteriori (MAP) objective $J(x)$ defined as the negative logarithm of the posterior density up to an additive constant. Then rewrite $J(x)$ in a component-wise form that makes explicit the contribution of each observation and each state component when $R$ and $B$ are diagonal. Based on this component-wise form, identify the anisotropic Tikhonov weights that arise in the data-misfit and prior-penalty terms.\n\nYour final answer must be the single closed-form analytic expression for the MAP objective $J(x)$ written in component-wise form. No rounding is required. Do not include units in your final answer.",
            "solution": "The problem as stated is valid. It is scientifically grounded in Bayesian statistics and linear algebra, well-posed with a clear objective, objective in its language, and contains all necessary information for a rigorous derivation. We may therefore proceed with the solution.\n\nThe problem requires the derivation of the Maximum a Posteriori (MAP) objective function for a linear inverse problem with Gaussian assumptions. The MAP estimate of the state vector $x$ is the mode of the posterior probability distribution $p(x|y)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nwhere $p(y|x)$ is the likelihood of observing $y$ given the state $x$, and $p(x)$ is the prior probability of the state $x$.\n\nThe problem states that the observation model is $y = Hx + \\varepsilon$, where the noise term $\\varepsilon$ follows a multivariate normal distribution with zero mean and covariance matrix $R$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, R)$. Consequently, the likelihood function $p(y|x)$ describes a random variable $y$ that is normally distributed with mean $Hx$ and covariance matrix $R$. The probability density function (PDF) for this likelihood is:\n$$p(y|x) = \\frac{1}{(2\\pi)^{m/2} (\\det R)^{1/2}} \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right)$$\n\nThe prior distribution for the state $x$ is given as a multivariate normal distribution with mean $x_b$ and covariance matrix $B$, denoted as $x \\sim \\mathcal{N}(x_b, B)$. The PDF for the prior is:\n$$p(x) = \\frac{1}{(2\\pi)^{n/2} (\\det B)^{1/2}} \\exp\\left(-\\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)\\right)$$\n\nSubstituting these PDFs into Bayes' rule, we obtain the posterior distribution:\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right) \\exp\\left(-\\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)\\right)$$\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (y - Hx)^T R^{-1} (y - Hx) + (x - x_b)^T B^{-1} (x - x_b) \\right] \\right)$$\n\nThe MAP estimate is the value of $x$ that maximizes $p(x|y)$. Because the natural logarithm is a monotonically increasing function, maximizing $p(x|y)$ is equivalent to maximizing $\\ln(p(x|y))$, which in turn is equivalent to minimizing $-\\ln(p(x|y))$. We define the MAP objective function $J(x)$ as the negative logarithm of the posterior, up to an additive constant that does not depend on $x$.\n$$J(x) = -\\ln(p(x|y)) + \\text{constant}$$\n$$J(x) = \\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx) + \\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)$$\nThis expression is the general form of the MAP objective function for a linear-Gaussian problem. The first term is a measure of the data misfit, weighted by the inverse of the observation error covariance. The second term is a regularization or penalty term that measures the departure of the solution $x$ from the prior mean $x_b$, weighted by the inverse of the prior covariance.\n\nThe problem specifies that the covariance matrices $R \\in \\mathbb{R}^{m \\times m}$ and $B \\in \\mathbb{R}^{n \\times n}$ are diagonal:\n$$R = \\operatorname{diag}(r_1, r_2, \\dots, r_m)$$\n$$B = \\operatorname{diag}(b_1, b_2, \\dots, b_n)$$\nSince $R$ and $B$ are symmetric positive definite, all $r_i > 0$ and $b_j > 0$. Their inverses are also diagonal:\n$$R^{-1} = \\operatorname{diag}\\left(\\frac{1}{r_1}, \\frac{1}{r_2}, \\dots, \\frac{1}{r_m}\\right)$$\n$$B^{-1} = \\operatorname{diag}\\left(\\frac{1}{b_1}, \\frac{1}{b_2}, \\dots, \\frac{1}{b_n}\\right)$$\n\nWe can now rewrite the two quadratic forms in $J(x)$ in a component-wise manner.\nFor the data-misfit term, let the residual vector be $d = y - Hx$. Its $i$-th component is $d_i = y_i - (Hx)_i = y_i - \\sum_{j=1}^{n} H_{ij} x_j$. The quadratic form is:\n$$(y - Hx)^T R^{-1} (y - Hx) = d^T R^{-1} d = \\sum_{i=1}^{m} d_i (R^{-1})_{ii} d_i = \\sum_{i=1}^{m} \\frac{1}{r_i} d_i^2$$\n$$(y - Hx)^T R^{-1} (y - Hx) = \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2$$\n\nFor the prior-penalty term, let the deviation from the prior be $z = x - x_b$. Its $j$-th component is $z_j = x_j - (x_b)_j$. The quadratic form is:\n$$(x - x_b)^T B^{-1} (x - x_b) = z^T B^{-1} z = \\sum_{j=1}^{n} z_j (B^{-1})_{jj} z_j = \\sum_{j=1}^{n} \\frac{1}{b_j} z_j^2$$\n$$(x - x_b)^T B^{-1} (x - x_b) = \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2$$\n\nCombining these two component-wise expressions, the full MAP objective function $J(x)$ is:\n$$J(x) = \\frac{1}{2} \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2 + \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2$$\n\nThis form reveals the connection to anisotropic Tikhonov regularization. The objective function is a sum of squared norms, where each component of the sum is individually weighted.\nThe coefficients $\\{1/r_i\\}_{i=1}^m$ act as anisotropic weights for the data-misfit term. They are the inverse variances of the observation errors. An observation $y_i$ with a small error variance $r_i$ (i.e., a highly certain observation) is assigned a large weight $1/r_i$, penalizing deviations from it more heavily.\nSimilarly, the coefficients $\\{1/b_j\\}_{j=1}^n$ act as anisotropic weights for the prior-penalty (regularization) term. They are the inverse variances of the prior state components. A state component $x_j$ with a small prior variance $b_j$ (i.e., a highly certain prior estimate) is assigned a large weight $1/b_j$, penalizing deviations from its prior mean $(x_b)_j$ more heavily. The term Tikhonov regularization is justified as this objective function is equivalent to a weighted least-squares problem, which is a generalized form of Tikhonov regularization.",
            "answer": "$$\\boxed{J(x) = \\frac{1}{2} \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2 + \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2}$$"
        },
        {
            "introduction": "Now that we have the MAP/Tikhonov framework, this practice explores a critical question: what happens when our prior assumptions are wrong? This exercise guides you to derive the bias in the MAP estimate that arises from an incorrect prior mean . You will then use a numerical experiment to see firsthand how this bias is influenced by the regularization parameter and, crucially, by its alignment with the unobservable parts of the model space (the nullspace of the forward operator).",
            "id": "3401519",
            "problem": "Consider a linear inverse problem with additive noise and a Gaussian prior. Let the true state be $x^\\star \\in \\mathbb{R}^n$ and the observations be generated by $y = H x^\\star + \\varepsilon$, where $H \\in \\mathbb{R}^{m \\times n}$ is a known forward operator, and $\\varepsilon \\in \\mathbb{R}^m$ is observation noise with zero mean and identity covariance, i.e., $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Cov}(\\varepsilon) = I_m$. Suppose that the prior on $x$ is Gaussian with mean $x_b \\in \\mathbb{R}^n$ and isotropic precision $\\lambda I_n$ with $\\lambda \\ge 0$, so the Maximum a Posteriori (MAP) estimator coincides with the Tikhonov-regularized solution centered at $x_b$. Assume the Tikhonov regularization matrix is the identity, and the observation error covariance is the identity.\n\nYour tasks are:\n\n- Starting from the definitions of Gaussian likelihoods and priors and the Bayes rule characterization of the MAP estimator (which minimizes the negative log-posterior), derive from first principles a closed-form expression for the expected bias of the MAP estimator, defined as $\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] - x^\\star$, as a function of $\\lambda$, the forward operator $H$, and the bias in the prior mean $x_b - x^\\star$, under the assumption of zero-mean noise. Your derivation must be purely mathematical, beginning from the standard linear-Gaussian model and the Tikhonov functional, and must clearly explain the role of the nullspace of $H$.\n\n- Use your derived expression to quantify how the bias depends on $\\lambda$ and on the alignment of the prior-mean error $x_b - x^\\star$ with the nullspace of $H$ and the orthogonal complement of the nullspace.\n\nThen, write a program that computes the Euclidean norm of the expected bias vector for a set of specified test cases. Use the following test suite:\n\n- Dimensions: $m = 2$, $n = 3$.\n- Forward operator:\n$$\nH = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n$$\n- True state:\n$$\nx^\\star = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix}.\n$$\n- Three distinct prior-mean scenarios $x_b$ that induce different alignments of the prior-mean error $\\Delta_b := x_b - x^\\star$ with the nullspace of $H$:\n    - Case $\\mathrm{A}$ (pure-nullspace prior-mean error): \n    $$\n    x_b^{(\\mathrm{A})} = x^\\star + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    - Case $\\mathrm{B}$ (pure-range prior-mean error): \n    $$\n    x_b^{(\\mathrm{B})} = x^\\star + \\begin{bmatrix} 0.4 \\\\ -0.6 \\\\ 0 \\end{bmatrix}.\n    $$\n    - Case $\\mathrm{C}$ (mixed alignment prior-mean error): \n    $$\n    x_b^{(\\mathrm{C})} = x^\\star + \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ -0.25 \\end{bmatrix}.\n    $$\n- Regularization parameters:\n$$\n\\lambda \\in \\{0,\\ 0.1,\\ 1,\\ 10\\}.\n$$\n\nFor each pair consisting of one of the three $x_b$ cases and one $\\lambda$ value, compute the Euclidean norm of the expected bias vector. This results in $12$ values, ordered as follows: all four $\\lambda$ values for Case $\\mathrm{A}$, then all four $\\lambda$ values for Case $\\mathrm{B}$, then all four $\\lambda$ values for Case $\\mathrm{C}$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,\\dots,r_{12}]$. Each $r_k$ must be a floating-point number rounded to $6$ decimal places. No other text should be printed.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of Bayesian inverse problems, is well-posed with all necessary information provided, and is stated using objective, formal language. We will proceed with the derivation and solution.\n\nThe problem asks for the derivation of the expected bias of the Maximum a Posteriori (MAP) estimator for a linear inverse problem with Gaussian noise and a Gaussian prior. The MAP estimator, $\\hat{x}_{\\mathrm{MAP}}$, is the value of $x$ that maximizes the posterior probability density function $p(x|y)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\n\nThe MAP estimate is found by maximizing this posterior, which is equivalent to minimizing its negative logarithm. We will formulate the likelihood and prior and then find the minimizer of the negative log-posterior.\n\nThe observation model is given by $y = Hx + \\varepsilon$, where the noise $\\varepsilon$ is assumed to follow a zero-mean Gaussian distribution with an identity covariance matrix, $\\varepsilon \\sim \\mathcal{N}(0, I_m)$. This implies that the likelihood of the observation $y$ given the state $x$ is also Gaussian: $y|x \\sim \\mathcal{N}(Hx, I_m)$. The probability density function for the likelihood is:\n$$\np(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T (I_m)^{-1} (y - Hx)\\right) = \\exp\\left(-\\frac{1}{2}\\|y - Hx\\|_2^2\\right)\n$$\n\nThe prior on the state $x$ is given as Gaussian with mean $x_b$ and isotropic precision $\\lambda I_n$, which corresponds to a covariance matrix of $(\\lambda I_n)^{-1} = \\lambda^{-1} I_n$. So, $x \\sim \\mathcal{N}(x_b, \\lambda^{-1} I_n)$. The prior probability density function is:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T (\\lambda I_n) (x - x_b)\\right) = \\exp\\left(-\\frac{\\lambda}{2}\\|x - x_b\\|_2^2\\right)\n$$\n\nThe posterior density is therefore proportional to:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2}\\|y - Hx\\|_2^2 - \\frac{\\lambda}{2}\\|x - x_b\\|_2^2\\right)\n$$\n\nThe MAP estimator $\\hat{x}_{\\mathrm{MAP}}$ is the value of $x$ that maximizes $p(x|y)$, or equivalently, minimizes the negative log-posterior functional $J(x)$:\n$$\nJ(x) = \\frac{1}{2}\\|y - Hx\\|_2^2 + \\frac{\\lambda}{2}\\|x - x_b\\|_2^2\n$$\nThis is precisely the Tikhonov regularization functional, as stated in the problem. To find the minimizer, we compute the gradient of $J(x)$ with respect to $x$ and set it to the zero vector.\n$$\n\\nabla_x J(x) = \\nabla_x \\left( \\frac{1}{2}(y - Hx)^T(y-Hx) + \\frac{\\lambda}{2}(x - x_b)^T(x - x_b) \\right)\n$$\n$$\n\\nabla_x J(x) = -H^T(y - Hx) + \\lambda(x - x_b)\n$$\nSetting the gradient to zero to find the minimum:\n$$\n-H^T y + H^T Hx + \\lambda x - \\lambda x_b = 0\n$$\n$$\n(H^T H + \\lambda I_n) x = H^T y + \\lambda x_b\n$$\nFor $\\lambda > 0$, the matrix $(H^T H + \\lambda I_n)$ is symmetric and positive definite, and thus invertible. Solving for $x$ gives the MAP estimator:\n$$\n\\hat{x}_{\\mathrm{MAP}} = (H^T H + \\lambda I_n)^{-1} (H^T y + \\lambda x_b)\n$$\nNext, we compute the expected value of the estimator, $\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}]$. The expectation is taken over the distribution of the noise $\\varepsilon$. We substitute $y = Hx^\\star + \\varepsilon$ into the expression for $\\hat{x}_{\\mathrm{MAP}}$:\n$$\n\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] = \\mathbb{E}\\left[ (H^T H + \\lambda I_n)^{-1} (H^T (Hx^\\star + \\varepsilon) + \\lambda x_b) \\right]\n$$\nUsing the linearity of the expectation operator and noting that $\\mathbb{E}[\\varepsilon] = 0$:\n$$\n\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] = (H^T H + \\lambda I_n)^{-1} (H^T Hx^\\star + H^T \\mathbb{E}[\\varepsilon] + \\lambda x_b)\n$$\n$$\n\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] = (H^T H + \\lambda I_n)^{-1} (H^T Hx^\\star + \\lambda x_b)\n$$\nThe expected bias of the estimator is defined as $\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = \\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] - x^\\star$.\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} (H^T Hx^\\star + \\lambda x_b) - x^\\star\n$$\nTo simplify, we write $x^\\star = (H^T H + \\lambda I_n)^{-1}(H^T H + \\lambda I_n) x^\\star$:\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} \\left[ (H^T Hx^\\star + \\lambda x_b) - (H^T H + \\lambda I_n) x^\\star \\right]\n$$\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} \\left[ H^T Hx^\\star + \\lambda x_b - H^T Hx^\\star - \\lambda x^\\star \\right]\n$$\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} \\left[ \\lambda (x_b - x^\\star) \\right]\n$$\nThis yields the final closed-form expression for the expected bias:\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = \\lambda (H^T H + \\lambda I_n)^{-1} (x_b - x^\\star)\n$$\nLet $\\Delta_b = x_b - x^\\star$ be the error in the prior mean. The bias is $\\lambda (H^T H + \\lambda I_n)^{-1} \\Delta_b$.\n\nTo understand the role of the nullspace of $H$, we analyze this expression using the Singular Value Decomposition (SVD) of $H$, which is $H = U \\Sigma V^T$. It follows that $H^T H = V \\Sigma^T \\Sigma V^T$. The bias expression becomes:\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = \\lambda (V \\Sigma^T \\Sigma V^T + \\lambda I_n)^{-1} \\Delta_b = \\lambda (V (\\Sigma^T \\Sigma + \\lambda I_n) V^T)^{-1} \\Delta_b = \\lambda V (\\Sigma^T \\Sigma + \\lambda I_n)^{-1} V^T \\Delta_b\n$$\nThe columns of $V$ form an orthonormal basis for $\\mathbb{R}^n$. Let this basis be $\\{v_1, \\dots, v_n\\}$. The space $\\mathbb{R}^n$ can be decomposed into the nullspace of $H$, $\\mathcal{N}(H)$, and its orthogonal complement, the range of $H^T$, $\\mathcal{R}(H^T)$. Let the rank of $H$ be $r$. Then $\\mathcal{R}(H^T) = \\mathrm{span}\\{v_1, \\dots, v_r\\}$ and $\\mathcal{N}(H) = \\mathrm{span}\\{v_{r+1}, \\dots, v_n\\}$. The diagonal entries of $(\\Sigma^T \\Sigma + \\lambda I_n)^{-1}$ are $1/(\\sigma_i^2 + \\lambda)$, where $\\sigma_i$ are the singular values of $H$. For $i > r$, $\\sigma_i = 0$.\n\nThe operator $\\lambda (\\Sigma^T \\Sigma + \\lambda I_n)^{-1}$ is diagonal with entries $\\frac{\\lambda}{\\sigma_i^2 + \\lambda}$.\n- For components of $\\Delta_b$ in $\\mathcal{R}(H^T)$ (corresponding to $\\sigma_i > 0$), the scaling factor is $\\frac{\\lambda}{\\sigma_i^2 + \\lambda}$. As $\\lambda \\to 0$, this factor approaches $0$, meaning the bias from these components is eliminated. As $\\lambda \\to \\infty$, this factor approaches $1$, meaning the bias approaches the full prior-mean error component.\n- For components of $\\Delta_b$ in $\\mathcal{N}(H)$ (corresponding to $\\sigma_i = 0$), the scaling factor is $\\frac{\\lambda}{0 + \\lambda} = 1$, regardless of the value of $\\lambda \\ge 0$.\n\nThis means any component of the prior-mean error $\\Delta_b$ that lies in the nullspace of $H$ is passed directly and unattenuated into the expected bias of the MAP estimator. The observations $y$ provide no information to correct this part of the prior belief. Conversely, the bias from error components in $\\mathcal{R}(H^T)$ is controlled by $\\lambda$: low $\\lambda$ allows the data to reduce this bias, while high $\\lambda$ causes the estimator to trust the biased prior.\n\nFor the specific numerical problem, $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$. The nullspace $\\mathcal{N}(H)$ is the set of vectors of the form $[0, 0, z]^T$, spanned by $[0, 0, 1]^T$. The range of $H^T$, $\\mathcal{R}(H^T)$, is spanned by $[1, 0, 0]^T$ and $[0, 1, 0]^T$.\nWe have $H^T H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$. The bias expression simplifies to:\n$$\n\\mathrm{Bias} = \\lambda \\left( \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} + \\lambda \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\right)^{-1} \\Delta_b = \\lambda \\begin{bmatrix} 1+\\lambda & 0 & 0 \\\\ 0 & 1+\\lambda & 0 \\\\ 0 & 0 & \\lambda \\end{bmatrix}^{-1} \\Delta_b\n$$\n$$\n\\mathrm{Bias} = \\lambda \\begin{bmatrix} \\frac{1}{1+\\lambda} & 0 & 0 \\\\ 0 & \\frac{1}{1+\\lambda} & 0 \\\\ 0 & 0 & \\frac{1}{\\lambda} \\end{bmatrix} \\Delta_b = \\begin{bmatrix} \\frac{\\lambda}{1+\\lambda} & 0 & 0 \\\\ 0 & \\frac{\\lambda}{1+\\lambda} & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\Delta_b\n$$\nThis analytical result is used for $\\lambda > 0$. For $\\lambda = 0$, the bias is the projection of $\\Delta_b$ onto the nullspace of $H$.\n$$\n\\mathrm{Bias}|_{\\lambda=0} = P_{\\mathcal{N}(H)} \\Delta_b = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\Delta_b = \\begin{bmatrix} 0 \\\\ 0 \\\\ (\\Delta_b)_3 \\end{bmatrix}\n$$\nThe implementation will use these simplified forms to compute the bias for each case.\n- Case A: $\\Delta_b = [0, 0, 0.3]^T$ (pure-nullspace error). The bias will be $[0, 0, 0.3]^T$ for all $\\lambda$.\n- Case B: $\\Delta_b = [0.4, -0.6, 0]^T$ (pure-range error). The bias will be $[\\frac{\\lambda}{1+\\lambda}(0.4), \\frac{\\lambda}{1+\\lambda}(-0.6), 0]^T$.\n- Case C: $\\Delta_b = [0.2, -0.1, -0.25]^T$ (mixed error). The bias will be $[\\frac{\\lambda}{1+\\lambda}(0.2), \\frac{\\lambda}{1+\\lambda}(-0.1), -0.25]^T$.\nThe Euclidean norm of the resulting bias vector is then computed for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Euclidean norm of the expected bias of the MAP estimator\n    for a linear inverse problem under different prior assumptions and\n    regularization strengths.\n    \"\"\"\n    \n    # Define problem constants\n    # H is not explicitly needed for the final formula, but its structure defines the formula.\n    # n = 3\n    # x_star = np.array([1.0, -2.0, 0.5])\n\n    # Define the set of regularization parameters lambda\n    lambdas = [0.0, 0.1, 1.0, 10.0]\n\n    # Define the three prior-mean error scenarios\n    # Case A: Pure-nullspace prior-mean error\n    delta_b_A = np.array([0.0, 0.0, 0.3])\n    # Case B: Pure-range prior-mean error\n    delta_b_B = np.array([0.4, -0.6, 0.0])\n    # Case C: Mixed alignment prior-mean error\n    delta_b_C = np.array([0.2, -0.1, -0.25])\n    \n    test_cases = [delta_b_A, delta_b_B, delta_b_C]\n\n    results = []\n\n    # Iterate through each prior-mean error case\n    for delta_b in test_cases:\n        # Iterate through each value of lambda\n        for lam in lambdas:\n            if lam == 0.0:\n                # For lambda = 0, the bias is the projection of the prior-mean error\n                # onto the nullspace of H. For the given H, this isolates the 3rd component.\n                bias_vector = np.array([0.0, 0.0, delta_b[2]])\n            else:\n                # For lambda > 0, use the derived formula for the bias vector components.\n                # The components in the range of H^T are scaled by lambda / (1 + lambda).\n                # The component in the nullspace of H is unscaled (factor of 1).\n                factor = lam / (1.0 + lam)\n                bias_vector = np.array([\n                    factor * delta_b[0], \n                    factor * delta_b[1], \n                    delta_b[2]\n                ])\n            \n            # Compute the Euclidean norm of the bias vector\n            norm_of_bias = np.linalg.norm(bias_vector)\n            \n            # Append the result to the list\n            results.append(norm_of_bias)\n\n    # Format the final output string as a list of floats with 6 decimal places.\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    \n    # Print the final result in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the theoretical form and properties of the MAP estimator, we turn to the practical challenge of computation. For the large-scale systems found in fields like geosciences or medical imaging, explicitly forming the matrices involved is computationally infeasible. This exercise will have you derive the essential components for modern iterative optimization algorithms—the gradient and the Hessian-vector product—using only matrix-free operator actions , bridging the gap between abstract theory and efficient, scalable numerical implementation.",
            "id": "3401544",
            "problem": "Consider a linear observation model in an inverse problem and data assimilation setting. Let the unknown state vector be $x \\in \\mathbb{R}^{n}$ and the observed data be $y \\in \\mathbb{R}^{m}$. The observation operator is the linear map $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$. The observational noise $\\varepsilon \\in \\mathbb{R}^{m}$ is modeled as a realization of a zero-mean Gaussian random variable with covariance matrix $R \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. A Gaussian prior for the state $x$ is given by $x \\sim \\mathcal{N}(x_{b}, B)$, where $x_{b} \\in \\mathbb{R}^{n}$ is the background (prior mean) and $B \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Assume access to matrix-free operator actions for $A$, the adjoint $A^{T}$, and the inverses $R^{-1}$ and $B^{-1}$, but do not assume access to any explicit matrix factorizations or entries.\n\nStarting from Bayes' rule and the definitions of Gaussian probability density functions, derive the negative log-posterior objective for Maximum A Posteriori (MAP) estimation and show its equivalence to Tikhonov regularization. Then, derive expressions for the gradient and Hessian-vector product of this objective, explicitly in terms of the available operator actions $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$. Your derivation must begin from the foundational definitions of the Gaussian likelihood and prior and proceed without invoking any pre-derived formulas.\n\nYou must express the final gradient as a function of $x$ and the Hessian-vector product as a function of an arbitrary vector $v \\in \\mathbb{R}^{n}$, both using only compositions of the available operator actions. Do not introduce any additional matrices beyond those specified. The target is a matrix-free implementation description: the expressions should be written so that each term corresponds to applying $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$ to vectors.\n\nProvide your final answer as two closed-form analytic expressions: one for the gradient and one for the Hessian-vector product. The final answer must be a single row matrix containing these two expressions. No numerical computation is required.",
            "solution": "The problem statement will first be validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- Unknown state vector: $x \\in \\mathbb{R}^{n}$\n- Observed data: $y \\in \\mathbb{R}^{m}$\n- Observation operator (linear map): $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$\n- Linear observation model: $y = Ax + \\varepsilon$\n- Observational noise: $\\varepsilon \\in \\mathbb{R}^{m}$ is a realization of a zero-mean Gaussian random variable, so $\\varepsilon \\sim \\mathcal{N}(0, R)$.\n- Noise covariance matrix: $R \\in \\mathbb{R}^{m \\times m}$, symmetric positive definite.\n- Prior distribution for the state: $x \\sim \\mathcal{N}(x_{b}, B)$.\n- Prior mean (background state): $x_{b} \\in \\mathbb{R}^{n}$.\n- Prior covariance matrix: $B \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite.\n- Available operator actions: $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$ applied to vectors.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard formulation in Bayesian inverse problems and data assimilation, forming the basis for methods like 3D-Var and 4D-Var. The assumptions of Gaussian priors and noise are fundamental and widely used in these fields. The problem is scientifically sound.\n- **Well-Posed:** The problem is a request for a derivation of standard results. The assumptions that $R$ and $B$ are symmetric positive definite ensure that the probability density functions are well-defined and the resulting optimization problem is strictly convex with a unique minimum. The problem is well-posed.\n- **Objective:** The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n\nThe problem does not violate any of the disqualifying criteria listed (Scientific Unsoundness, Non-Formalizable, Incomplete Setup, Unrealistic, Ill-Posed, Pseudo-Profound, Outside Verifiability). It is a standard, well-defined theoretical problem in computational science.\n\n#### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the MAP Objective and Equivalence to Tikhonov Regularization\n\nThe goal of Maximum A Posteriori (MAP) estimation is to find the state $x$ that maximizes the posterior probability density function (PDF) $p(x|y)$ given the observation $y$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nMaximizing $p(x|y)$ is equivalent to maximizing its logarithm, $\\ln(p(x|y))$, which is in turn equivalent to minimizing its negative logarithm, $-\\ln(p(x|y))$. The MAP estimate is therefore given by:\n$$x_{\\text{MAP}} = \\arg\\max_{x} p(x|y) = \\arg\\min_{x} [-\\ln(p(y|x)) - \\ln(p(x))]$$\nWe now define the likelihood and prior terms based on the given Gaussian distributions. The PDF for a general multivariate Gaussian random variable $z \\in \\mathbb{R}^k$ with mean $\\mu$ and covariance $\\Sigma$ is:\n$$p(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^T \\Sigma^{-1} (z-\\mu)\\right)$$\n\n**1. Likelihood Term:**\nThe likelihood $p(y|x)$ is the PDF of the observation $y$ given the state $x$. The observation model is $y = Ax + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that for a given $x$, $y$ is a Gaussian random variable with mean $E[y|x] = E[Ax + \\varepsilon] = Ax + E[\\varepsilon] = Ax$ and covariance $\\text{Cov}(y|x) = \\text{Cov}(Ax + \\varepsilon) = \\text{Cov}(\\varepsilon) = R$. Thus, $y|x \\sim \\mathcal{N}(Ax, R)$. The likelihood function is:\n$$p(y|x) \\propto \\exp\\left(-\\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)\\right)$$\nThe negative log-likelihood, ignoring constant terms, is:\n$$-\\ln(p(y|x)) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + C_1$$\n\n**2. Prior Term:**\nThe prior distribution for the state $x$ is given as $x \\sim \\mathcal{N}(x_{b}, B)$. Its PDF is:\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})\\right)$$\nThe negative log-prior, ignoring constant terms, is:\n$$-\\ln(p(x)) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b}) + C_2$$\n\n**3. MAP Objective Function:**\nCombining the negative log-likelihood and negative log-prior, we obtain the objective function $J(x)$ to be minimized:\n$$J(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$$\nThis is the negative log-posterior objective function for MAP estimation (up to an additive constant and a scaling factor of $2$).\n\n**Equivalence to Tikhonov Regularization:**\nTikhonov regularization is a method for solving ill-posed inverse problems, typically of the form $Ax=y$. The regularized solution is found by minimizing an objective function that combines a data-fidelity term and a regularization term:\n$$\\min_{x} \\left( \\|Ax - y\\|^2_{W} + \\|\\mathcal{L}(x - x_0)\\|^2_{P} \\right)$$\nwhere $\\|\\cdot\\|_M$ denotes the weighted norm defined by $\\|v\\|_M^2 = v^T M v$, and $\\mathcal{L}$ is a regularization operator.\n\nThe MAP objective function $J(x)$ can be written using this norm notation. By setting the weighting matrix for the data-fidelity term to $W=R^{-1}$, the regularization operator to the identity $\\mathcal{L}=I$, the reference solution to the prior mean $x_0 = x_b$, and the regularization weighting matrix to $P=B^{-1}$, our objective becomes:\n$$2J(x) = \\|Ax - y\\|^2_{R^{-1}} + \\|x - x_{b}\\|^2_{B^{-1}}$$\nThis demonstrates that MAP estimation under Gaussian assumptions is mathematically equivalent to Tikhonov regularization. The first term, $\\|Ax - y\\|^2_{R^{-1}}$, measures the misfit between the model prediction $Ax$ and the data $y$, weighted by the inverse of the observation error covariance. The second term, $\\|x - x_{b}\\|^2_{B^{-1}}$, is a regularization term that penalizes solutions deviating from the prior belief $x_b$, weighted by the inverse of the prior covariance.\n\n### Gradient and Hessian-Vector Product Derivation\n\n**1. Gradient of the Objective Function:**\nThe gradient $\\nabla J(x)$ is found by differentiating $J(x)$ with respect to the vector $x$. We differentiate the two terms of $J(x)$ separately.\nLet $J_d(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)$ and $J_b(x) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$.\n\nFor the data term $J_d(x)$, we consider a small perturbation $x \\to x + \\delta x$.\nThe change in the residual is $y - A(x+\\delta x) = (y-Ax) - A\\delta x$.\n$$J_d(x+\\delta x) = \\frac{1}{2} ((y-Ax) - A\\delta x)^T R^{-1} ((y-Ax) - A\\delta x)$$\n$$J_d(x+\\delta x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) - \\frac{1}{2} (A\\delta x)^T R^{-1} (y-Ax) - \\frac{1}{2} (y-Ax)^T R^{-1} (A\\delta x) + O(\\|\\delta x\\|^2)$$\nSince $R$ and thus $R^{-1}$ are symmetric, the two linear terms are equal.\n$$J_d(x+\\delta x) = J_d(x) - (y-Ax)^T R^{-1} A \\delta x + O(\\|\\delta x\\|^2)$$\nThe directional derivative is $\\nabla J_d(x)^T \\delta x = (A^T R^{-1} (Ax-y))^T \\delta x$. Therefore, the gradient of the data term is:\n$$\\nabla J_d(x) = A^T R^{-1} (Ax - y)$$\n\nFor the background term $J_b(x)$, a similar derivation yields:\n$$\\nabla J_b(x) = B^{-1} (x - x_b)$$\n\nThe total gradient of the objective function is the sum of the gradients of its parts:\n$$\\nabla J(x) = \\nabla J_d(x) + \\nabla J_b(x) = A^T R^{-1} (Ax - y) + B^{-1} (x - x_b)$$\nThis expression is structured for matrix-free implementation. For instance, computing $A^T R^{-1} (Ax - y)$ involves applying $A$ to $x$, a vector subtraction, applying $R^{-1}$ to the result, and finally applying $A^T$.\n\n**2. Hessian-Vector Product:**\nThe Hessian matrix $H(x) = \\nabla^2 J(x)$ is obtained by differentiating the gradient $\\nabla J(x)$ with respect to $x$. The Hessian-vector product $H(x)v$ for an arbitrary vector $v \\in \\mathbb{R}^n$ can be derived by finding the directional derivative of the gradient, i.e., $H(x)v = \\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)]_{\\epsilon=0}$.\n\nLet's compute $\\nabla J(x+\\epsilon v)$:\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (A(x+\\epsilon v) - y) + B^{-1} ((x+\\epsilon v) - x_b)$$\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (Ax - y) + \\epsilon A^T R^{-1} Av + B^{-1} (x - x_b) + \\epsilon B^{-1} v$$\n$$\\nabla J(x+\\epsilon v) = \\nabla J(x) + \\epsilon (A^T R^{-1} Av + B^{-1} v)$$\nNow, we differentiate with respect to $\\epsilon$:\n$$\\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)] = A^T R^{-1} Av + B^{-1} v$$\nThis expression is independent of $\\epsilon$, so setting $\\epsilon=0$ gives the Hessian-vector product:\n$$H(x)v = A^T R^{-1} Av + B^{-1} v$$\nThe Hessian itself is $H(x) = A^T R^{-1} A + B^{-1}$, which is constant with respect to $x$. This is expected for a quadratic objective function. The computation of $H(x)v$ is matrix-free: it involves applying $A$ to $v$, then $R^{-1}$, then $A^T$, and adding the result of applying $B^{-1}$ to $v$.",
            "answer": "$$\\boxed{\\begin{pmatrix} A^{T} R^{-1} (Ax - y) + B^{-1} (x - x_{b}) & A^{T} R^{-1} Av + B^{-1}v \\end{pmatrix}}$$"
        }
    ]
}