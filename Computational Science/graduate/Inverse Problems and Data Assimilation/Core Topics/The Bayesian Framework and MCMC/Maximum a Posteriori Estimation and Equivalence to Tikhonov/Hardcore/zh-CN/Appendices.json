{
    "hands_on_practices": [
        {
            "introduction": "本节的第一个练习是基础性的，它将指导您从第一性原理出发，为一个线性高斯问题推导最大后验（MAP）目标函数。通过这个练习，您将亲身体会统计假设（高斯先验和似然）如何直接转化为一个吉洪诺夫（Tikhonov）正则化的最小二乘问题。特别地，您将看到误差的方差如何自然地成为各项异性权重，从而加深对正则化作用的理解。",
            "id": "3401540",
            "problem": "考虑一个线性反问题，其中状态向量 $x \\in \\mathbb{R}^{n}$ 通过一个线性算子 $H \\in \\mathbb{R}^{m \\times n}$ 进行观测，并带有加性噪声，从而得到满足 $y = H x + \\varepsilon$ 的观测值 $y \\in \\mathbb{R}^{m}$。假设观测噪声 $\\varepsilon$ 服从零均值、协方差矩阵为 $R \\in \\mathbb{R}^{m \\times m}$ 的高斯分布，并且 $x$ 的先验分布是均值为 $x_{b} \\in \\mathbb{R}^{n}$、协方差矩阵为 $B \\in \\mathbb{R}^{n \\times n}$ 的高斯分布。$R$ 和 $B$ 均为对称正定对角矩阵，其中 $R = \\operatorname{diag}(r_{1}, \\dots, r_{m})$，$B = \\operatorname{diag}(b_{1}, \\dots, b_{n})$，并且对于所有索引，$r_{i} > 0$ 和 $b_{j} > 0$。\n\n从贝叶斯法则和多元正态密度的定义出发，推导最大后验 (MAP) 目标函数 $J(x)$，其定义为后验密度的负对数（相差一个加性常数）。然后，当 $R$ 和 $B$ 是对角矩阵时，将 $J(x)$ 重写为分量形式，以明确显示每个观测值和每个状态分量的贡献。基于此分量形式，识别出在数据失配项和先验惩罚项中出现的各向异性吉洪诺夫权重。\n\n您的最终答案必须是以分量形式写出的 MAP 目标函数 $J(x)$ 的单一闭式解析表达式。无需四舍五入。最终答案中不要包含单位。",
            "solution": "所述问题是有效的。它在贝叶斯统计和线性代数方面有科学依据，问题适定且目标明确，语言客观，并包含了进行严格推导所需的所有信息。因此，我们可以开始求解。\n\n该问题要求在有高斯假设的情况下，推导一个线性反问题的最大后验 (MAP) 目标函数。状态向量 $x$ 的 MAP 估计是后验概率分布 $p(x|y)$ 的众数。根据贝叶斯法则，后验概率正比于似然与先验的乘积：\n$$p(x|y) \\propto p(y|x) p(x)$$\n其中 $p(y|x)$ 是在给定状态 $x$ 的条件下观测到 $y$ 的似然，而 $p(x)$ 是状态 $x$ 的先验概率。\n\n问题陈述了观测模型为 $y = Hx + \\varepsilon$，其中噪声项 $\\varepsilon$ 服从零均值、协方差矩阵为 $R$ 的多元正态分布，记作 $\\varepsilon \\sim \\mathcal{N}(0, R)$。因此，似然函数 $p(y|x)$ 描述了一个随机变量 $y$，它服从均值为 $Hx$、协方差矩阵为 $R$ 的正态分布。该似然的概率密度函数 (PDF) 为：\n$$p(y|x) = \\frac{1}{(2\\pi)^{m/2} (\\det R)^{1/2}} \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right)$$\n\n状态 $x$ 的先验分布被给定为均值为 $x_b$、协方差矩阵为 $B$ 的多元正态分布，记作 $x \\sim \\mathcal{N}(x_b, B)$。该先验的 PDF 为：\n$$p(x) = \\frac{1}{(2\\pi)^{n/2} (\\det B)^{1/2}} \\exp\\left(-\\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)\\right)$$\n\n将这些 PDF 代入贝叶斯法则，我们得到后验分布：\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right) \\exp\\left(-\\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)\\right)$$\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (y - Hx)^T R^{-1} (y - Hx) + (x - x_b)^T B^{-1} (x - x_b) \\right] \\right)$$\n\nMAP 估计是使 $p(x|y)$ 最大化的 $x$ 的值。由于自然对数是单调递增函数，最大化 $p(x|y)$ 等价于最大化 $\\ln(p(x|y))$，而这又等价于最小化 $-\\ln(p(x|y))$。我们将 MAP 目标函数 $J(x)$ 定义为后验概率的负对数，两者相差一个不依赖于 $x$ 的加性常数。\n$$J(x) = -\\ln(p(x|y)) + \\text{constant}$$\n$$J(x) = \\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx) + \\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)$$\n该表达式是线性高斯问题的 MAP 目标函数的一般形式。第一项是数据失配的度量，由观测误差协方差的逆矩阵加权。第二项是正则化项或惩罚项，度量解 $x$ 对先验均值 $x_b$ 的偏离，由先验协方差的逆矩阵加权。\n\n问题指明协方差矩阵 $R \\in \\mathbb{R}^{m \\times m}$ 和 $B \\in \\mathbb{R}^{n \\times n}$ 是对角矩阵：\n$$R = \\operatorname{diag}(r_1, r_2, \\dots, r_m)$$\n$$B = \\operatorname{diag}(b_1, b_2, \\dots, b_n)$$\n由于 $R$ 和 $B$ 是对称正定的，所有的 $r_i > 0$ 和 $b_j > 0$。它们的逆矩阵也是对角矩阵：\n$$R^{-1} = \\operatorname{diag}\\left(\\frac{1}{r_1}, \\frac{1}{r_2}, \\dots, \\frac{1}{r_m}\\right)$$\n$$B^{-1} = \\operatorname{diag}\\left(\\frac{1}{b_1}, \\frac{1}{b_2}, \\dots, \\frac{1}{b_n}\\right)$$\n\n现在我们可以将 $J(x)$ 中的两个二次型以分量形式重写。\n对于数据失配项，设残差向量为 $d = y - Hx$。其第 $i$ 个分量为 $d_i = y_i - (Hx)_i = y_i - \\sum_{j=1}^{n} H_{ij} x_j$。该二次型为：\n$$(y - Hx)^T R^{-1} (y - Hx) = d^T R^{-1} d = \\sum_{i=1}^{m} d_i (R^{-1})_{ii} d_i = \\sum_{i=1}^{m} \\frac{1}{r_i} d_i^2$$\n$$(y - Hx)^T R^{-1} (y - Hx) = \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2$$\n\n对于先验惩罚项，设与先验的偏差为 $z = x - x_b$。其第 $j$ 个分量为 $z_j = x_j - (x_b)_j$。该二次型为：\n$$(x - x_b)^T B^{-1} (x - x_b) = z^T B^{-1} z = \\sum_{j=1}^{n} z_j (B^{-1})_{jj} z_j = \\sum_{j=1}^{n} \\frac{1}{b_j} z_j^2$$\n$$(x - x_b)^T B^{-1} (x - x_b) = \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2$$\n\n结合这两个分量表达式，完整的 MAP 目标函数 $J(x)$ 为：\n$$J(x) = \\frac{1}{2} \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2 + \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2$$\n\n这种形式揭示了其与各向异性吉洪诺夫正则化的联系。目标函数是平方范数之和，其中和的每个分量都被单独加权。\n系数 $\\{1/r_i\\}_{i=1}^m$ 作为数据失配项的各向异性权重。它们是观测误差方差的倒数。一个具有小误差方差 $r_i$ 的观测值 $y_i$（即一个高度确定的观测）被赋予一个大的权重 $1/r_i$，从而更重地惩罚对其的偏离。\n类似地，系数 $\\{1/b_j\\}_{j=1}^n$ 作为先验惩罚（正则化）项的各向异性权重。它们是先验状态分量方差的倒数。一个具有小先验方差 $b_j$ 的状态分量 $x_j$（即一个高度确定的先验估计）被赋予一个大的权重 $1/b_j$，从而更重地惩罚其对先验均值 $(x_b)_j$ 的偏离。使用吉洪诺夫正则化这一术语是合理的，因为该目标函数等价于一个加权最小二乘问题，而后者是吉洪诺夫正则化的一种广义形式。",
            "answer": "$$\\boxed{J(x) = \\frac{1}{2} \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2 + \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2}$$"
        },
        {
            "introduction": "在吉洪诺夫框架的基础上，这个练习将探讨正则化中一个更微妙但至关重要的问题：估计量的偏差。您将推导先验均值 $x_b$ 中的误差如何传播到最终估计中，并揭示前向算子 $H$ 的零空间（nullspace）所扮演的关键角色——在这个空间里，观测数据无法提供信息来修正先验信念。这项练习结合了分析推导和编码任务，让您能够量化这种效应，从而深入理解数据同化的局限性。",
            "id": "3401519",
            "problem": "考虑一个带有加性噪声和高斯先验的线性逆问题。设真实状态为 $x^\\star \\in \\mathbb{R}^n$，观测值由 $y = H x^\\star + \\varepsilon$ 生成，其中 $H \\in \\mathbb{R}^{m \\times n}$ 是一个已知的前向算子，$\\varepsilon \\in \\mathbb{R}^m$ 是观测噪声，其均值为零，协方差为单位矩阵，即 $\\mathbb{E}[\\varepsilon] = 0$ 和 $\\mathrm{Cov}(\\varepsilon) = I_m$。假设 $x$ 的先验是高斯分布，均值为 $x_b \\in \\mathbb{R}^n$，各向同性精度为 $\\lambda I_n$（其中 $\\lambda \\ge 0$），因此最大后验（MAP）估计量与以 $x_b$ 为中心的 Tikhonov 正则化解一致。假设 Tikhonov 正则化矩阵是单位矩阵，观测误差协方差也是单位矩阵。\n\n你的任务是：\n\n- 从高斯似然和先验的定义以及 MAP 估计量（最小化负对数后验）的贝叶斯法则表征出发，基于第一性原理，推导 MAP 估计量的期望偏差的闭式表达式。该偏差定义为 $\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] - x^\\star$，应表示为 $\\lambda$、前向算子 $H$ 以及先验均值偏差 $x_b - x^\\star$ 的函数，并假设噪声均值为零。你的推导必须是纯数学的，从标准的线性高斯模型和 Tikhonov 泛函开始，并且必须清楚地解释 $H$ 的零空间所扮演的角色。\n\n- 使用你推导出的表达式来量化偏差如何依赖于 $\\lambda$ 以及先验均值误差 $x_b - x^\\star$ 与 $H$ 的零空间及其正交补空间的一致性程度。\n\n然后，编写一个程序，为一组指定的测试用例计算期望偏差向量的欧几里得范数。使用以下测试套件：\n\n- 维度：$m = 2$， $n = 3$。\n- 前向算子：\n$$\nH = \\begin{bmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{bmatrix}.\n$$\n- 真实状态：\n$$\nx^\\star = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix}.\n$$\n- 三种不同的先验均值情景 $x_b$，它们导致先验均值误差 $\\Delta_b := x_b - x^\\star$ 与 $H$ 的零空间具有不同的一致性：\n    - 情况 $\\mathrm{A}$（纯零空间先验均值误差）：\n    $$\n    x_b^{(\\mathrm{A})} = x^\\star + \\begin{bmatrix} 0 \\\\ 0 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    - 情况 $\\mathrm{B}$（纯值域先验均值误差）：\n    $$\n    x_b^{(\\mathrm{B})} = x^\\star + \\begin{bmatrix} 0.4 \\\\ -0.6 \\\\ 0 \\end{bmatrix}.\n    $$\n    - 情况 $\\mathrm{C}$（混合一致性先验均值误差）：\n    $$\n    x_b^{(\\mathrm{C})} = x^\\star + \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ -0.25 \\end{bmatrix}.\n    $$\n- 正则化参数：\n$$\n\\lambda \\in \\{0, 0.1, 1, 10\\}.\n$$\n\n对于由三种 $x_b$ 情况之一和 $\\lambda$ 值之一组成的每一对，计算期望偏差向量的欧几里得范数。这将产生 12 个值，排序如下：首先是情况 $\\mathrm{A}$ 的所有四个 $\\lambda$ 值，然后是情况 $\\mathrm{B}$ 的所有四个 $\\lambda$ 值，最后是情况 $\\mathrm{C}$ 的所有四个 $\\lambda$ 值。\n\n最终输出格式：你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $[r_1,r_2,\\dots,r_{12}]$。每个 $r_k$ 必须是四舍五入到 $6$ 位小数的浮点数。不应打印任何其他文本。",
            "solution": "该问题是有效的，因为它在科学上基于贝叶斯逆问题的原理，是适定的（提供了所有必要信息），并使用客观、正式的语言进行陈述。我们将继续进行推导和求解。\n\n该问题要求推导在线性逆问题中，当噪声和先验均为高斯分布时，最大后验（MAP）估计量的期望偏差。MAP 估计量 $\\hat{x}_{\\mathrm{MAP}}$ 是使后验概率密度函数 $p(x|y)$ 最大化的 $x$ 值。根据贝叶斯法则，后验概率与似然和先验的乘积成正比：\n\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\n\nMAP 估计是通过最大化该后验概率找到的，这等同于最小化其负对数。我们将构建似然和先验，然后找到负对数后验的最小值点。\n\n观测模型由 $y = Hx + \\varepsilon$ 给出，其中噪声 $\\varepsilon$ 假定服从零均值、单位协方差矩阵的高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, I_m)$。这意味着给定状态 $x$ 时，观测值 $y$ 的似然也是高斯分布：$y|x \\sim \\mathcal{N}(Hx, I_m)$。似然的概率密度函数为：\n$$\np(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T (I_m)^{-1} (y - Hx)\\right) = \\exp\\left(-\\frac{1}{2}\\|y - Hx\\|_2^2\\right)\n$$\n\n状态 $x$ 的先验被假定为高斯分布，均值为 $x_b$，各向同性精度为 $\\lambda I_n$，这对应于协方差矩阵 $(\\lambda I_n)^{-1} = \\lambda^{-1} I_n$。因此，$x \\sim \\mathcal{N}(x_b, \\lambda^{-1} I_n)$。先验概率密度函数为：\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T (\\lambda I_n) (x - x_b)\\right) = \\exp\\left(-\\frac{\\lambda}{2}\\|x - x_b\\|_2^2\\right)\n$$\n\n因此，后验密度正比于：\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2}\\|y - Hx\\|_2^2 - \\frac{\\lambda}{2}\\|x - x_b\\|_2^2\\right)\n$$\n\nMAP 估计量 $\\hat{x}_{\\mathrm{MAP}}$ 是最大化 $p(x|y)$ 的 $x$ 值，等价地，是最小化负对数后验泛函 $J(x)$ 的 $x$ 值：\n$$\nJ(x) = \\frac{1}{2}\\|y - Hx\\|_2^2 + \\frac{\\lambda}{2}\\|x - x_b\\|_2^2\n$$\n这正是问题中所述的 Tikhonov 正则化泛函。为了找到最小值点，我们计算 $J(x)$ 关于 $x$ 的梯度，并将其设为零向量。\n$$\n\\nabla_x J(x) = \\nabla_x \\left( \\frac{1}{2}(y - Hx)^T(y-Hx) + \\frac{\\lambda}{2}(x - x_b)^T(x - x_b) \\right)\n$$\n$$\n\\nabla_x J(x) = -H^T(y - Hx) + \\lambda(x - x_b)\n$$\n将梯度设为零以求最小值：\n$$\n-H^T y + H^T Hx + \\lambda x - \\lambda x_b = 0\n$$\n$$\n(H^T H + \\lambda I_n) x = H^T y + \\lambda x_b\n$$\n当 $\\lambda > 0$ 时，矩阵 $(H^T H + \\lambda I_n)$ 是对称正定的，因此是可逆的。求解 $x$ 得到 MAP 估计量：\n$$\n\\hat{x}_{\\mathrm{MAP}} = (H^T H + \\lambda I_n)^{-1} (H^T y + \\lambda x_b)\n$$\n接下来，我们计算估计量的期望值 $\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}]$。期望是针对噪声 $\\varepsilon$ 的分布计算的。我们将 $y = Hx^\\star + \\varepsilon$ 代入 $\\hat{x}_{\\mathrm{MAP}}$ 的表达式中：\n$$\n\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] = \\mathbb{E}\\left[ (H^T H + \\lambda I_n)^{-1} (H^T (Hx^\\star + \\varepsilon) + \\lambda x_b) \\right]\n$$\n利用期望算子的线性性质，并注意到 $\\mathbb{E}[\\varepsilon] = 0$：\n$$\n\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] = (H^T H + \\lambda I_n)^{-1} (H^T Hx^\\star + H^T \\mathbb{E}[\\varepsilon] + \\lambda x_b)\n$$\n$$\n\\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] = (H^T H + \\lambda I_n)^{-1} (H^T Hx^\\star + \\lambda x_b)\n$$\n估计量的期望偏差定义为 $\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = \\mathbb{E}[\\hat{x}_{\\mathrm{MAP}}] - x^\\star$。\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} (H^T Hx^\\star + \\lambda x_b) - x^\\star\n$$\n为了简化，我们将 $x^\\star$ 写成 $x^\\star = (H^T H + \\lambda I_n)^{-1}(H^T H + \\lambda I_n) x^\\star$：\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} \\left[ (H^T Hx^\\star + \\lambda x_b) - (H^T H + \\lambda I_n) x^\\star \\right]\n$$\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} \\left[ H^T Hx^\\star + \\lambda x_b - H^T Hx^\\star - \\lambda x^\\star \\right]\n$$\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = (H^T H + \\lambda I_n)^{-1} \\left[ \\lambda (x_b - x^\\star) \\right]\n$$\n这就得出了期望偏差的最终闭式表达式：\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = \\lambda (H^T H + \\lambda I_n)^{-1} (x_b - x^\\star)\n$$\n令 $\\Delta_b = x_b - x^\\star$ 为先验均值中的误差。偏差为 $\\lambda (H^T H + \\lambda I_n)^{-1} \\Delta_b$。\n\n为了理解 $H$ 的零空间的作用，我们使用 $H$ 的奇异值分解（SVD）来分析这个表达式，即 $H = U \\Sigma V^T$。由此可得 $H^T H = V \\Sigma^T \\Sigma V^T$。偏差表达式变为：\n$$\n\\mathrm{Bias}(\\hat{x}_{\\mathrm{MAP}}) = \\lambda (V \\Sigma^T \\Sigma V^T + \\lambda I_n)^{-1} \\Delta_b = \\lambda (V (\\Sigma^T \\Sigma + \\lambda I_n) V^T)^{-1} \\Delta_b = \\lambda V (\\Sigma^T \\Sigma + \\lambda I_n)^{-1} V^T \\Delta_b\n$$\n$V$ 的列构成了 $\\mathbb{R}^n$ 的一个标准正交基。设该基为 $\\{v_1, \\dots, v_n\\}$。空间 $\\mathbb{R}^n$ 可以分解为 $H$ 的零空间 $\\mathcal{N}(H)$ 和其正交补空间，即 $H^T$ 的值域 $\\mathcal{R}(H^T)$。设 $H$ 的秩为 $r$。则 $\\mathcal{R}(H^T) = \\mathrm{span}\\{v_1, \\dots, v_r\\}$ 且 $\\mathcal{N}(H) = \\mathrm{span}\\{v_{r+1}, \\dots, v_n\\}$。$(\\Sigma^T \\Sigma + \\lambda I_n)^{-1}$ 的对角线元素为 $1/(\\sigma_i^2 + \\lambda)$，其中 $\\sigma_i$ 是 $H$ 的奇异值。对于 $i > r$，$\\sigma_i = 0$。\n\n算子 $\\lambda (\\Sigma^T \\Sigma + \\lambda I_n)^{-1}$ 是对角的，其对角线元素为 $\\frac{\\lambda}{\\sigma_i^2 + \\lambda}$。\n- 对于 $\\Delta_b$ 在 $\\mathcal{R}(H^T)$ 中的分量（对应于 $\\sigma_i > 0$），缩放因子为 $\\frac{\\lambda}{\\sigma_i^2 + \\lambda}$。当 $\\lambda \\to 0$ 时，该因子趋近于 $0$，意味着来自这些分量的偏差被消除。当 $\\lambda \\to \\infty$ 时，该因子趋近于 $1$，意味着偏差趋近于完整的先验均值误差分量。\n- 对于 $\\Delta_b$ 在 $\\mathcal{N}(H)$ 中的分量（对应于 $\\sigma_i = 0$），缩放因子为 $\\frac{\\lambda}{0 + \\lambda} = 1$，无论 $\\lambda \\ge 0$ 的值是多少。\n\n这意味着先验均值误差 $\\Delta_b$ 中任何位于 $H$ 零空间的分量都会直接且不衰减地传递到 MAP 估计量的期望偏差中。观测值 $y$ 没有提供任何信息来纠正先验信念的这一部分。相反，来自 $\\mathcal{R}(H^T)$ 中误差分量的偏差由 $\\lambda$ 控制：低 $\\lambda$ 值允许数据减少这种偏差，而高 $\\lambda$ 值则导致估计量相信有偏的先验。\n\n对于具体的数值问题，$H = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\end{bmatrix}$。零空间 $\\mathcal{N}(H)$ 是形如 $[0, 0, z]^T$ 的向量集合，由 $[0, 0, 1]^T$ 张成。$H^T$ 的值域 $\\mathcal{R}(H^T)$ 由 $[1, 0, 0]^T$ 和 $[0, 1, 0]^T$ 张成。\n我们有 $H^T H = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{bmatrix}$。偏差表达式简化为：\n$$\n\\mathrm{Bias} = \\lambda \\left( \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{bmatrix} + \\lambda \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix} \\right)^{-1} \\Delta_b = \\lambda \\begin{bmatrix} 1+\\lambda  0  0 \\\\ 0  1+\\lambda  0 \\\\ 0  0  \\lambda \\end{bmatrix}^{-1} \\Delta_b\n$$\n$$\n\\mathrm{Bias} = \\lambda \\begin{bmatrix} \\frac{1}{1+\\lambda}  0  0 \\\\ 0  \\frac{1}{1+\\lambda}  0 \\\\ 0  0  \\frac{1}{\\lambda} \\end{bmatrix} \\Delta_b = \\begin{bmatrix} \\frac{\\lambda}{1+\\lambda}  0  0 \\\\ 0  \\frac{\\lambda}{1+\\lambda}  0 \\\\ 0  0  1 \\end{bmatrix} \\Delta_b\n$$\n这个解析结果用于 $\\lambda > 0$ 的情况。对于 $\\lambda = 0$，偏差是 $\\Delta_b$ 在 $H$ 的零空间上的投影。\n$$\n\\mathrm{Bias}|_{\\lambda=0} = P_{\\mathcal{N}(H)} \\Delta_b = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{bmatrix} \\Delta_b = \\begin{bmatrix} 0 \\\\ 0 \\\\ (\\Delta_b)_3 \\end{bmatrix}\n$$\n实现将使用这些简化形式来计算每种情况下的偏差。\n- 情况 A：$\\Delta_b = [0, 0, 0.3]^T$（纯零空间误差）。对于所有 $\\lambda$，偏差都将是 $[0, 0, 0.3]^T$。\n- 情况 B：$\\Delta_b = [0.4, -0.6, 0]^T$（纯值域误差）。偏差将是 $[\\frac{\\lambda}{1+\\lambda}(0.4), \\frac{\\lambda}{1+\\lambda}(-0.6), 0]^T$。\n- 情况 C：$\\Delta_b = [0.2, -0.1, -0.25]^T$（混合误差）。偏差将是 $[\\frac{\\lambda}{1+\\lambda}(0.2), \\frac{\\lambda}{1+\\lambda}(-0.1), -0.25]^T$。\n然后为每种情况计算所得偏差向量的欧几里得范数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Euclidean norm of the expected bias of the MAP estimator\n    for a linear inverse problem under different prior assumptions and\n    regularization strengths.\n    \"\"\"\n    \n    # Define problem constants\n    # H is not explicitly needed for the final formula, but its structure defines the formula.\n    # n = 3\n    # x_star = np.array([1.0, -2.0, 0.5])\n\n    # Define the set of regularization parameters lambda\n    lambdas = [0.0, 0.1, 1.0, 10.0]\n\n    # Define the three prior-mean error scenarios\n    # Case A: Pure-nullspace prior-mean error\n    delta_b_A = np.array([0.0, 0.0, 0.3])\n    # Case B: Pure-range prior-mean error\n    delta_b_B = np.array([0.4, -0.6, 0.0])\n    # Case C: Mixed alignment prior-mean error\n    delta_b_C = np.array([0.2, -0.1, -0.25])\n    \n    test_cases = [delta_b_A, delta_b_B, delta_b_C]\n\n    results = []\n\n    # Iterate through each prior-mean error case\n    for delta_b in test_cases:\n        # Iterate through each value of lambda\n        for lam in lambdas:\n            if lam == 0.0:\n                # For lambda = 0, the bias is the projection of the prior-mean error\n                # onto the nullspace of H. For the given H, this isolates the 3rd component.\n                bias_vector = np.array([0.0, 0.0, delta_b[2]])\n            else:\n                # For lambda > 0, use the derived formula for the bias vector components.\n                # The components in the range of H^T are scaled by lambda / (1 + lambda).\n                # The component in the nullspace of H is unscaled (factor of 1).\n                factor = lam / (1.0 + lam)\n                bias_vector = np.array([\n                    factor * delta_b[0], \n                    factor * delta_b[1], \n                    delta_b[2]\n                ])\n            \n            # Compute the Euclidean norm of the bias vector\n            norm_of_bias = np.linalg.norm(bias_vector)\n            \n            # Append the result to the list\n            results.append(norm_of_bias)\n\n    # Format the final output string as a list of floats with 6 decimal places.\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    \n    # Print the final result in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的反问题，尤其是在地球科学等领域，其维度之大使得我们无法显式地构建和存储其中的矩阵。这个最终练习旨在连接理论与大规模计算，展示如何以“无矩阵”（matrix-free）的方式实现优化算法。您将为 MAP 目标函数推导梯度和Hessian矩阵-向量乘积的表达式，这些表达式仅依赖于算子（如 $A$、$A^T$）的应用，这是高效求解大规模数据同化问题的关键。",
            "id": "3401544",
            "problem": "考虑一个逆问题和数据同化背景下的线性观测模型。令未知状态向量为 $x \\in \\mathbb{R}^{n}$，观测数据为 $y \\in \\mathbb{R}^{m}$。观测算子是线性映射 $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$。观测噪声 $\\varepsilon \\in \\mathbb{R}^{m}$ 被建模为零均值高斯随机变量的一个实现，其协方差矩阵 $R \\in \\mathbb{R}^{m \\times m}$ 是对称正定的。状态 $x$ 的高斯先验由 $x \\sim \\mathcal{N}(x_{b}, B)$ 给出，其中 $x_{b} \\in \\mathbb{R}^{n}$ 是背景（先验均值），$B \\in \\mathbb{R}^{n \\times n}$ 是对称正定的。假设可以进行无矩阵（matrix-free）的算子操作，包括 $A$、其伴随算子 $A^{T}$ 以及逆算子 $R^{-1}$ 和 $B^{-1}$，但不假设可以获取任何显式的矩阵分解或矩阵元素。\n\n从 Bayes 法则和高斯概率密度函数的定义出发，推导用于最大后验（MAP）估计的负对数后验目标函数，并证明其与 Tikhonov 正则化的等价性。然后，推导该目标函数的梯度和海森-向量乘积（Hessian-vector product）的表达式，要求明确地使用可用的算子操作 $A$、$A^{T}$、$R^{-1}$ 和 $B^{-1}$ 来表示。你的推导必须从高斯似然和先验的基本定义开始，并且过程中不得引用任何预先推导好的公式。\n\n你必须将最终的梯度表示为 $x$ 的函数，将海森-向量乘积表示为一个任意向量 $v \\in \\mathbb{R}^{n}$ 的函数，两者都只使用可用算子操作的组合来表示。除已指定的矩阵外，不要引入任何额外的矩阵。目标是给出一个无矩阵实现的描述：表达式的写法应使每一项都对应于将算子 $A$、$A^{T}$、$R^{-1}$ 和 $B^{-1}$ 应用于向量。\n\n请以两个封闭形式的解析表达式给出你的最终答案：一个用于梯度，一个用于海森-向量乘积。最终答案必须是一个包含这两个表达式的单行矩阵。不需要进行数值计算。",
            "solution": "首先将根据指定标准对问题陈述进行验证。\n\n### 问题验证\n\n#### 步骤 1：提取已知条件\n- 未知状态向量：$x \\in \\mathbb{R}^{n}$\n- 观测数据：$y \\in \\mathbb{R}^{m}$\n- 观测算子（线性映射）：$A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$\n- 线性观测模型：$y = Ax + \\varepsilon$\n- 观测噪声：$\\varepsilon \\in \\mathbb{R}^{m}$ 是零均值高斯随机变量的一个实现，因此 $\\varepsilon \\sim \\mathcal{N}(0, R)$。\n- 噪声协方差矩阵：$R \\in \\mathbb{R}^{m \\times m}$，对称正定。\n- 状态的先验分布：$x \\sim \\mathcal{N}(x_{b}, B)$。\n- 先验均值（背景状态）：$x_{b} \\in \\mathbb{R}^{n}$。\n- 先验协方差矩阵：$B \\in \\mathbb{R}^{n \\times n}$，对称正定。\n- 可用的算子操作：将 $A$、$A^{T}$、$R^{-1}$ 和 $B^{-1}$ 应用于向量。\n\n#### 步骤 2：使用提取的已知条件进行验证\n- **科学依据充分：** 该问题是贝叶斯逆问题和数据同化中的一个标准提法，构成了像 3D-Var 和 4D-Var 等方法的基础。高斯先验和噪声的假设是这些领域中基础且广泛使用的。该问题在科学上是合理的。\n- **适定性：** 该问题要求推导标准结果。$R$ 和 $B$ 是对称正定的假设确保了概率密度函数是良定义的，并且由此产生的优化问题是严格凸的，具有唯一的最小值。该问题是适定的。\n- **客观性：** 该问题以精确的数学语言陈述，没有主观性或模糊性。\n\n该问题不违反任何列出的不合格标准（科学上不合理、非形式化、设置不完整、不切实际、不适定、伪深刻、无法外部验证）。它是一个计算科学中标准的、定义明确的理论问题。\n\n#### 步骤 3：结论和行动\n问题有效。将提供一个完整、有理有据的解答。\n\n### MAP 目标函数的推导及其与 Tikhonov 正则化的等价性\n\n最大后验（MAP）估计的目标是找到一个状态 $x$，使得在给定观测值 $y$ 的条件下，后验概率密度函数（PDF）$p(x|y)$ 最大化。根据 Bayes 法则，后验概率与似然和先验的乘积成正比：\n$$p(x|y) \\propto p(y|x) p(x)$$\n最大化 $p(x|y)$ 等价于最大化其对数 $\\ln(p(x|y))$，这又等价于最小化其负对数 $-\\ln(p(x|y))$。因此，MAP 估计由下式给出：\n$$x_{\\text{MAP}} = \\arg\\max_{x} p(x|y) = \\arg\\min_{x} [-\\ln(p(y|x)) - \\ln(p(x))]$$\n我们现在根据给定的高斯分布来定义似然项和先验项。对于一个均值为 $\\mu$、协方差为 $\\Sigma$ 的一般多元高斯随机变量 $z \\in \\mathbb{R}^k$，其概率密度函数（PDF）为：\n$$p(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^T \\Sigma^{-1} (z-\\mu)\\right)$$\n\n**1. 似然项：**\n似然 $p(y|x)$ 是在给定状态 $x$ 的条件下观测值 $y$ 的概率密度函数。观测模型为 $y = Ax + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, R)$。这意味着对于给定的 $x$，$y$ 是一个高斯随机变量，其均值为 $E[y|x] = E[Ax + \\varepsilon] = Ax + E[\\varepsilon] = Ax$，协方差为 $\\text{Cov}(y|x) = \\text{Cov}(Ax + \\varepsilon) = \\text{Cov}(\\varepsilon) = R$。因此，$y|x \\sim \\mathcal{N}(Ax, R)$。似然函数为：\n$$p(y|x) \\propto \\exp\\left(-\\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)\\right)$$\n忽略常数项，负对数似然为：\n$$-\\ln(p(y|x)) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + C_1$$\n\n**2. 先验项：**\n状态 $x$ 的先验分布由 $x \\sim \\mathcal{N}(x_{b}, B)$ 给出。其概率密度函数为：\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})\\right)$$\n忽略常数项，负对数先验为：\n$$-\\ln(p(x)) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b}) + C_2$$\n\n**3. MAP 目标函数：**\n结合负对数似然和负对数先验，我们得到需要最小化的目标函数 $J(x)$：\n$$J(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$$\n这就是用于 MAP 估计的负对数后验目标函数（不计一个加性常数和一个比例因子 2）。\n\n**与 Tikhonov 正则化的等价性：**\nTikhonov 正则化是一种求解不适定逆问题的方法，通常问题形式为 $Ax=y$。正则化解通过最小化一个结合了数据保真项和正则化项的目标函数来找到：\n$$\\min_{x} \\left( \\|Ax - y\\|^2_{W} + \\|\\mathcal{L}(x - x_0)\\|^2_{P} \\right)$$\n其中 $\\|\\cdot\\|_M$ 表示由 $\\|v\\|_M^2 = v^T M v$ 定义的加权范数，而 $\\mathcal{L}$ 是一个正则化算子。\n\nMAP 目标函数 $J(x)$ 可以用这种范数记法来书写。通过将数据保真项的加权矩阵设为 $W=R^{-1}$，正则化算子设为单位算子 $\\mathcal{L}=I$，参考解设为先验均值 $x_0 = x_b$，以及正则化加权矩阵设为 $P=B^{-1}$，我们的目标函数变为：\n$$2J(x) = \\|Ax - y\\|^2_{R^{-1}} + \\|x - x_{b}\\|^2_{B^{-1}}$$\n这表明，在高斯假设下的 MAP 估计在数学上等价于 Tikhonov 正则化。第一项 $\\|Ax - y\\|^2_{R^{-1}}$ 衡量了模型预测 $Ax$ 与数据 $y$ 之间的失配度，并由观测误差协方差的逆进行加权。第二项 $\\|x - x_{b}\\|^2_{B^{-1}}$ 是一个正则化项，它惩罚偏离先验信念 $x_b$ 的解，并由先验协方差的逆进行加权。\n\n### 梯度和海森-向量乘积的推导\n\n**1. 目标函数的梯度：**\n梯度 $\\nabla J(x)$ 是通过对 $J(x)$ 关于向量 $x$ 求导得到的。我们分别对 $J(x)$ 的两项进行求导。\n令 $J_d(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)$ 且 $J_b(x) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$。\n\n对于数据项 $J_d(x)$，我们考虑一个微小扰动 $x \\to x + \\delta x$。\n残差的变化为 $y - A(x+\\delta x) = (y-Ax) - A\\delta x$。\n$$J_d(x+\\delta x) = \\frac{1}{2} ((y-Ax) - A\\delta x)^T R^{-1} ((y-Ax) - A\\delta x)$$\n$$J_d(x+\\delta x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) - \\frac{1}{2} (A\\delta x)^T R^{-1} (y-Ax) - \\frac{1}{2} (y-Ax)^T R^{-1} (A\\delta x) + O(\\|\\delta x\\|^2)$$\n因为 $R$ 以及 $R^{-1}$ 是对称的，所以两个线性项相等。\n$$J_d(x+\\delta x) = J_d(x) - (y-Ax)^T R^{-1} A \\delta x + O(\\|\\delta x\\|^2)$$\n方向导数为 $\\nabla J_d(x)^T \\delta x = (A^T R^{-1} (Ax-y))^T \\delta x$。因此，数据项的梯度为：\n$$\\nabla J_d(x) = A^T R^{-1} (Ax - y)$$\n\n对于背景项 $J_b(x)$，类似的推导可得：\n$$\\nabla J_b(x) = B^{-1} (x - x_b)$$\n\n目标函数的总梯度是其各部分梯度的和：\n$$\\nabla J(x) = \\nabla J_d(x) + \\nabla J_b(x) = A^T R^{-1} (Ax - y) + B^{-1} (x - x_b)$$\n这个表达式是为了无矩阵实现而构建的。例如，计算 $A^T R^{-1} (Ax - y)$ 涉及将 $A$ 应用于 $x$，进行向量减法，将 $R^{-1}$ 应用于结果，最后应用 $A^T$。\n\n**2. 海森-向量乘积：**\n海森矩阵 $H(x) = \\nabla^2 J(x)$ 是通过对梯度 $\\nabla J(x)$ 关于 $x$ 求导得到的。对于任意向量 $v \\in \\mathbb{R}^n$，海森-向量乘积 $H(x)v$ 可以通过求梯度的方向导数来推导，即 $H(x)v = \\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)]_{\\epsilon=0}$。\n\n让我们计算 $\\nabla J(x+\\epsilon v)$：\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (A(x+\\epsilon v) - y) + B^{-1} ((x+\\epsilon v) - x_b)$$\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (Ax - y) + \\epsilon A^T R^{-1} Av + B^{-1} (x - x_b) + \\epsilon B^{-1} v$$\n$$\\nabla J(x+\\epsilon v) = \\nabla J(x) + \\epsilon (A^T R^{-1} Av + B^{-1} v)$$\n现在，我们对 $\\epsilon$ 求导：\n$$\\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)] = A^T R^{-1} Av + B^{-1} v$$\n这个表达式与 $\\epsilon$ 无关，因此令 $\\epsilon=0$ 即可得到海森-向量乘积：\n$$H(x)v = A^T R^{-1} Av + B^{-1} v$$\n海森矩阵本身是 $H(x) = A^T R^{-1} A + B^{-1}$，它关于 $x$ 是一个常数。这对于一个二次目标函数是符合预期的。$H(x)v$ 的计算是无矩阵的：它涉及将 $A$ 应用于 $v$，然后是 $R^{-1}$，再然后是 $A^T$，最后加上将 $B^{-1}$ 应用于 $v$ 的结果。",
            "answer": "$$\\boxed{\\begin{pmatrix} A^{T} R^{-1} (Ax - y) + B^{-1} (x - x_{b}) & A^{T} R^{-1} Av + B^{-1}v \\end{pmatrix}}$$"
        }
    ]
}