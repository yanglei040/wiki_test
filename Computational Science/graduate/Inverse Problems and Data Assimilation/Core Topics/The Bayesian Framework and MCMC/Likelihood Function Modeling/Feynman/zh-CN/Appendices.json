{
    "hands_on_practices": [
        {
            "introduction": "许多逆问题和数据同化方法都建立在数据误差服从高斯分布的假设之上，这一假设构成了构建似然函数的基础。本练习将带领我们从高斯分布的概率密度函数出发，推导最常见的目标函数——负对数似然函数。 更进一步，它通过一个思想实验定量地分析了当我们对误差方差的估计存在偏差时，会对模型拟合产生多大的影响，这对于理解模型的稳健性至关重要。",
            "id": "3397432",
            "problem": "考虑一个具有独立高斯测量误差的逆问题的经典观测模型。设参数为 $x \\in \\mathbb{R}^{n}$，前向映射为 $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$，其分量为 $h_{i}(x)$，$i \\in \\{1,\\ldots,m\\}$。假设观测向量 $y \\in \\mathbb{R}^{m}$ 按如下方式生成\n$$\ny = h(x_{\\mathrm{true}}) + \\varepsilon,\n$$\n其中噪声 $\\varepsilon$ 服从零均值高斯分布，其对角协方差为\n$$\nR = \\mathrm{diag}\\!\\left(r_{1}^{2},\\ldots,r_{m}^{2}\\right),\n$$\n所有 $r_{i} > 0$。假设 $R$ 已知，且各分量上的误差是独立的。\n\n从具有独立分量的多元高斯分布的概率密度函数出发，完成以下任务：\n\n1. 推导对数似然 $\\ln p\\!\\left(y \\mid x, R\\right)$，并将其表示为每次测量的贡献之和。明确指出每次测量 $i$ 对对数似然的贡献，将其表示为残差 $d_{i}(x) = y_{i} - h_{i}(x)$ 和 $r_{i}$ 的函数。\n\n2. 为分析模型拟合对噪声方差误设定的敏感性，考虑一位分析师使用了一个误设定的对角噪声协方差 $\\widetilde{R} = \\mathrm{diag}\\!\\left(\\tilde{r}_{1}^{2},\\ldots,\\tilde{r}_{m}^{2}\\right)$，其中 $\\tilde{r}_{i} = \\alpha_{i} r_{i}$，标量 $\\alpha_{i} > 0$。关注单个测量索引 $i$，并假设 $x = x_{\\mathrm{true}}$，因此 $y_{i} - h_{i}(x_{\\mathrm{true}})$ 服从真实分布 $\\mathcal{N}(0,r_{i}^{2})$。仅使用高斯随机变量的基本定义和矩恒等式，推导每次测量的期望超额负对数似然，\n$$\n\\Delta(\\alpha_{i}) \\equiv \\mathbb{E}_{y_{i} \\sim \\mathcal{N}(h_{i}(x_{\\mathrm{true}}), r_{i}^{2})}\\!\\Big[-\\ln p\\!\\left(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}\\right) + \\ln p\\!\\left(y_{i}\\mid x_{\\mathrm{true}}, r_{i}\\right)\\Big],\n$$\n将其表示为仅关于 $\\alpha_{i}$ 的闭式函数。该量等于 Kullback–Leibler 散度（从真实单次测量模型到误设定的单次测量模型），并量化了对数似然对方法差误设定的敏感性。\n\n请以仅包含 $\\alpha_{i}$ 的简化解析表达式形式提供 $\\Delta(\\alpha_{i})$ 的最终答案。不要包含单位。如果简化需要常数，请保持其精确值。您的最终答案必须是一个单一的闭式表达式。",
            "solution": "所述问题在科学上是合理的、自洽的且适定的。它提出了一个统计建模和逆问题理论中的标准任务。所有必要的定义和条件都已提供，足以推导出唯一的解析解。因此，该问题被认为是有效的。\n\n根据要求，解答分为两部分推导。\n\n首先，我们推导对数似然函数。观测模型为 $y = h(x_{\\mathrm{true}}) + \\varepsilon$，其中噪声向量 $\\varepsilon$ 服从均值为 $0$ 的多元高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, R)$。这意味着在给定参数向量 $x$ 的条件下，观测向量 $y$ 的条件概率密度函数（PDF）为 $y \\sim \\mathcal{N}(h(x), R)$，表示为：\n$$p(y \\mid x, R) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det(R)}} \\exp\\left(-\\frac{1}{2} (y - h(x))^{T} R^{-1} (y - h(x))\\right)$$\n噪声协方差矩阵 $R$ 是一个对角矩阵 $R = \\mathrm{diag}(r_{1}^{2}, r_{2}^{2}, \\ldots, r_{m}^{2})$。这个对角矩阵的行列式是其对角元素的乘积，$\\det(R) = \\prod_{i=1}^{m} r_{i}^{2}$。它的逆矩阵也是一个对角矩阵，$R^{-1} = \\mathrm{diag}(r_{1}^{-2}, r_{2}^{-2}, \\ldots, r_{m}^{-2})$。指数中的二次型简化为求和形式：\n$$(y - h(x))^{T} R^{-1} (y - h(x)) = \\sum_{i=1}^{m} \\frac{(y_{i} - h_{i}(x))^{2}}{r_{i}^{2}}$$\n因为行列式和二次型分别分解为乘积和求和，所以多元PDF可以写成 $m$ 个独立的单变量高斯PDF的乘积：\n$$p(y \\mid x, R) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi r_{i}^{2}}} \\exp\\left(-\\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}}\\right)$$\n对数似然 $\\ln p(y \\mid x, R)$ 是该表达式的自然对数。乘积的对数是对数的和：\n$$\\ln p(y \\mid x, R) = \\sum_{i=1}^{m} \\ln\\left[ \\frac{1}{\\sqrt{2\\pi r_{i}^{2}}} \\exp\\left(-\\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}}\\right) \\right]$$\n$$\\ln p(y \\mid x, R) = \\sum_{i=1}^{m} \\left( \\ln\\left((2\\pi r_{i}^{2})^{-1/2}\\right) - \\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}} \\right)$$\n由此可得对数似然为每次测量贡献之和：\n$$\\ln p(y \\mid x, R) = \\sum_{i=1}^{m} \\left( -\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}} \\right)$$\n因此，每次测量 $i$ 对对数似然的贡献，作为残差 $d_{i}(x) = y_{i} - h_{i}(x)$ 和标准差 $r_{i}$ 的函数，为：\n$$-\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{d_{i}(x)^{2}}{2r_{i}^{2}}$$\n\n其次，我们推导期望超额负对数似然 $\\Delta(\\alpha_{i})$。该量定义为：\n$$\\Delta(\\alpha_{i}) \\equiv \\mathbb{E}_{y_{i} \\sim \\mathcal{N}(h_{i}(x_{\\mathrm{true}}), r_{i}^{2})}\\!\\Big[-\\ln p(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}) + \\ln p(y_{i}\\mid x_{\\mathrm{true}}, r_{i})\\Big]$$\n使用上面确定的每次测量的贡献，我们可以写出在 $x = x_{\\mathrm{true}}$ 时测量 $i$ 的两个对数似然项。令 $\\delta_{i} = y_{i} - h_{i}(x_{\\mathrm{true}})$。\n真实的对数似然是 $\\ln p(y_{i}\\mid x_{\\mathrm{true}}, r_{i}) = -\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{\\delta_{i}^{2}}{2r_{i}^{2}}$。\n误设定的对数似然是 $\\ln p(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}) = -\\frac{1}{2}\\ln(2\\pi \\tilde{r}_{i}^{2}) - \\frac{\\delta_{i}^{2}}{2\\tilde{r}_{i}^{2}}$。\n期望内的表达式为：\n$$-\\ln p(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}) + \\ln p(y_{i}\\mid x_{\\mathrm{true}}, r_{i}) = \\left(\\frac{1}{2}\\ln(2\\pi \\tilde{r}_{i}^{2}) + \\frac{\\delta_{i}^{2}}{2\\tilde{r}_{i}^{2}}\\right) + \\left(-\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{\\delta_{i}^{2}}{2r_{i}^{2}}\\right)$$\n$$= \\frac{1}{2}\\left(\\ln(2\\pi \\tilde{r}_{i}^{2}) - \\ln(2\\pi r_{i}^{2})\\right) + \\frac{\\delta_{i}^{2}}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right) = \\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{\\delta_{i}^{2}}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)$$\n我们现在对真实分布求期望，其中 $y_{i} \\sim \\mathcal{N}(h_{i}(x_{\\mathrm{true}}), r_{i}^{2})$，这意味着残差 $\\delta_{i}$ 是一个随机变量，服从分布 $\\delta_{i} \\sim \\mathcal{N}(0, r_{i}^{2})$。根据期望的线性性质：\n$$\\Delta(\\alpha_{i}) = \\mathbb{E}\\left[\\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right)\\right] + \\mathbb{E}\\left[\\frac{\\delta_{i}^{2}}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)\\right]$$\n第一项是关于随机变量 $\\delta_{i}$ 的常数，第二项中的常数可以被提出来：\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{1}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)\\mathbb{E}[\\delta_{i}^{2}]$$\n期望 $\\mathbb{E}[\\delta_{i}^{2}]$ 是均值为 $\\mu = 0$、方差为 $\\sigma^{2} = r_{i}^{2}$ 的高斯随机变量 $\\delta_{i}$ 的二阶矩。根据方差的定义，$\\mathrm{Var}(\\delta_{i}) = \\mathbb{E}[\\delta_{i}^{2}] - (\\mathbb{E}[\\delta_{i}])^{2}$，我们有 $\\mathbb{E}[\\delta_{i}^{2}] = \\mathrm{Var}(\\delta_{i}) + (\\mathbb{E}[\\delta_{i}])^{2} = r_{i}^{2} + 0^{2} = r_{i}^{2}$。\n将 $\\mathbb{E}[\\delta_{i}^{2}] = r_{i}^{2}$ 代入 $\\Delta(\\alpha_{i})$ 的表达式中：\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{1}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)r_{i}^{2}$$\n现在，我们使用给定的关系 $\\tilde{r}_{i} = \\alpha_{i} r_{i}$，这意味着 $\\tilde{r}_{i}^{2} = \\alpha_{i}^{2} r_{i}^{2}$。由于 $\\alpha_{i} > 0$：\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}\\ln\\left(\\frac{\\alpha_{i}^{2} r_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{1}{2}\\left(\\frac{r_{i}^{2}}{\\alpha_{i}^{2} r_{i}^{2}} - \\frac{r_{i}^{2}}{r_{i}^{2}}\\right) = \\frac{1}{2}\\ln(\\alpha_{i}^{2}) + \\frac{1}{2}\\left(\\frac{1}{\\alpha_{i}^{2}} - 1\\right)$$\n使用对数性质 $\\ln(a^{b}) = b \\ln(a)$，我们简化第一项：\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}(2\\ln(\\alpha_{i})) + \\frac{1}{2\\alpha_{i}^{2}} - \\frac{1}{2}$$\n最终的简化表达式是：\n$$\\Delta(\\alpha_{i}) = \\ln(\\alpha_{i}) + \\frac{1}{2\\alpha_{i}^{2}} - \\frac{1}{2}$$",
            "answer": "$$\\boxed{\\ln(\\alpha_{i}) + \\frac{1}{2 \\alpha_{i}^{2}} - \\frac{1}{2}}$$"
        },
        {
            "introduction": "现实世界的数据并非总是符合高斯分布，例如，在处理事件计数（如光子探测、放射性衰变）时，泊松分布是更合适的模型。本练习将我们的视野从高斯模型扩展到更广泛的广义线性模型（GLM）框架。 通过为泊松计数数据推导似然函数，我们将揭示“典则连接函数”（canonical link function）在模型构建中的关键作用，特别是它如何确保代价函数具有理想的凸性，从而极大地简化了参数估计中的优化问题。",
            "id": "3397372",
            "problem": "考虑一个数据同化场景，其中参数向量 $\\theta \\in \\mathbb{R}^{p}$ 通过一个前向算子 $H:\\mathbb{R}^{p}\\to\\mathbb{R}^{n}$ 映射到一个预测场。观测值为计数 $d=\\{d_{i}\\}_{i=1}^{n}$, 其中每个 $d_{i}$ 是传感器 $i$ 处计数过程的一次实现。以 $\\theta$ 为条件，假设观测值相互独立，并服从速率为 $\\lambda_{i}(\\theta)>0$ 的泊松分布，因此每个 $d_{i}$ 的概率质量函数由定义 $\\mathbb{P}(D_{i}=d_{i}\\mid\\theta)=\\frac{\\exp(-\\lambda_{i}(\\theta))\\,\\lambda_{i}(\\theta)^{d_{i}}}{d_{i}!}$ 给出。似然函数定义为数据的联合概率，并视为 $\\theta$ 的函数。\n\na) 从给定的泊松概率质量函数定义和独立性假设出发，推导似然函数 $L(\\theta;d)$ 和对数似然函数 $\\ell(\\theta;d)=\\ln L(\\theta;d)$, 并用 $\\{\\lambda_{i}(\\theta)\\}_{i=1}^{n}$ 和 $\\{d_{i}\\}_{i=1}^{n}$ 表示。\n\nb) 现在，通过定义 $\\eta_{i}(\\theta)=\\ln\\lambda_{i}(\\theta)$，为计数数据引入指数族的典范连接函数，并假设前向算子在参数上是线性的，使得 $\\eta(\\theta)=X\\theta$, 其中 $X\\in\\mathbb{R}^{n\\times p}$ 是一个已知的设计矩阵。定义负对数似然函数 $J(\\theta)=-\\ell(\\theta;d)$。仅使用微分的第一性原理和链式法则，推导 $\\nabla J(\\theta)$ 和 $\\nabla^{2}J(\\theta)$，并陈述一个使 $J(\\theta)$ 对 $\\theta$ 呈凸性的条件。简要解释典范连接函数 $\\ln\\lambda$ 如何与这种凸性相关。\n\nc) 在 b) 部分的假设下，提供一个用 $X$, $\\theta$ 和 $d$ 表示的对数似然函数 $\\ell(\\theta;d)$ 的单一闭式解析表达式。您最终报告的答案必须是这个表达式。不要省略任何依赖于 $\\theta$ 或 $d$ 的项。不需要四舍五入，也不适用任何物理单位。",
            "solution": "该问题陈述已经过验证，被认为是科学上合理的、适定的和客观的。它展示了广义线性模型（特别是泊松回归）背景下的一个标准推导，这是统计建模、逆问题和数据同化中的一个基本课题。所有术语都有明确定义，前提条件内部一致且足以推导出唯一解。\n\na) 似然函数和对数似然函数的推导。\n\n似然函数 $L(\\theta;d)$ 定义为观测到数据 $d = \\{d_i\\}_{i=1}^n$ 的联合概率，并被视为参数向量 $\\theta \\in \\mathbb{R}^p$ 的函数。鉴于观测值 $\\{d_i\\}_{i=1}^n$ 在以 $\\theta$ 为条件时被假定为相互独立的，联合概率是各个概率的乘积：\n$$\nL(\\theta;d) = \\mathbb{P}(D_1=d_1, D_2=d_2, \\dots, D_n=d_n \\mid \\theta) = \\prod_{i=1}^{n} \\mathbb{P}(D_i=d_i \\mid \\theta)\n$$\n问题陈述指出，每个观测值 $d_i$ 服从速率为 $\\lambda_i(\\theta) > 0$ 的泊松分布。单个观测值的概率质量函数（PMF）如下：\n$$\n\\mathbb{P}(D_i = d_i \\mid \\theta) = \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!}\n$$\n将此 PMF 代入似然函数的乘积形式中得到：\n$$\nL(\\theta;d) = \\prod_{i=1}^{n} \\left( \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!} \\right)\n$$\n这可以通过分离乘积来改写：\n$$\nL(\\theta;d) = \\left( \\prod_{i=1}^{n} \\exp(-\\lambda_i(\\theta)) \\right) \\left( \\prod_{i=1}^{n} \\lambda_i(\\theta)^{d_i} \\right) \\left( \\prod_{i=1}^{n} \\frac{1}{d_i!} \\right) = \\exp\\left(-\\sum_{i=1}^{n} \\lambda_i(\\theta)\\right) \\left( \\prod_{i=1}^{n} \\lambda_i(\\theta)^{d_i} \\right) \\left( \\frac{1}{\\prod_{i=1}^{n} d_i!} \\right)\n$$\n对数似然函数 $\\ell(\\theta;d)$ 是似然函数的自然对数，即 $\\ell(\\theta;d) = \\ln L(\\theta;d)$。对 $L(\\theta;d)$ 的乘积形式取对数，将乘积转化为和：\n$$\n\\ell(\\theta;d) = \\ln \\left( \\prod_{i=1}^{n} \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!} \\right) = \\sum_{i=1}^{n} \\ln \\left( \\frac{\\exp(-\\lambda_i(\\theta)) \\, \\lambda_i(\\theta)^{d_i}}{d_i!} \\right)\n$$\n使用对数性质（$\\ln(ab) = \\ln a + \\ln b$、$\\ln(a/b) = \\ln a - \\ln b$ 和 $\\ln(a^c) = c \\ln a$），我们可以展开求和内的项：\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ \\ln(\\exp(-\\lambda_i(\\theta))) + \\ln(\\lambda_i(\\theta)^{d_i}) - \\ln(d_i!) \\right]\n$$\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ -\\lambda_i(\\theta) + d_i \\ln(\\lambda_i(\\theta)) - \\ln(d_i!) \\right]\n$$\n这就是对数似然函数的表达式。\n\nb) 负对数似然函数的梯度和Hessian矩阵的推导，以及凸性分析。\n\n负对数似然函数定义为 $J(\\theta) = -\\ell(\\theta;d)$。使用 (a) 部分的结果：\n$$\nJ(\\theta) = - \\sum_{i=1}^{n} \\left[ -\\lambda_i(\\theta) + d_i \\ln(\\lambda_i(\\theta)) - \\ln(d_i!) \\right] = \\sum_{i=1}^{n} \\left[ \\lambda_i(\\theta) - d_i \\ln(\\lambda_i(\\theta)) + \\ln(d_i!) \\right]\n$$\n项 $\\sum_{i=1}^n \\ln(d_i!)$ 是一个关于 $\\theta$ 的常数，不会影响导数。\n\n我们现在引入假设 $\\eta_i(\\theta) = \\ln \\lambda_i(\\theta)$ 和 $\\eta(\\theta) = X\\theta$。后者意味着对于每个分量 $i$，$\\eta_i(\\theta) = (X\\theta)_i = \\sum_{j=1}^{p} X_{ij}\\theta_j$。根据连接函数的定义，我们有 $\\lambda_i(\\theta) = \\exp(\\eta_i(\\theta)) = \\exp((X\\theta)_i)$。将此代入 $J(\\theta)$：\n$$\nJ(\\theta) = \\sum_{i=1}^{n} \\left[ \\exp((X\\theta)_i) - d_i (X\\theta)_i \\right] + C\n$$\n其中 $C = \\sum_{i=1}^n \\ln(d_i!)$ 是常数项。\n\n为求梯度 $\\nabla J(\\theta)$，我们计算关于每个分量 $\\theta_k$（$k=1, \\dots, p$）的偏导数：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k} \\sum_{i=1}^{n} \\left[ \\exp\\left(\\sum_{j=1}^{p} X_{ij}\\theta_j\\right) - d_i \\sum_{j=1}^{p} X_{ij}\\theta_j \\right]\n$$\n根据微分的线性性质：\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^{n} \\left[ \\frac{\\partial}{\\partial \\theta_k} \\exp\\left(\\sum_{j=1}^{p} X_{ij}\\theta_j\\right) - \\frac{\\partial}{\\partial \\theta_k} \\left(d_i \\sum_{j=1}^{p} X_{ij}\\theta_j\\right) \\right]\n$$\n使用链式法则，$\\frac{\\partial}{\\partial \\theta_k} \\exp((X\\theta)_i) = \\exp((X\\theta)_i) \\cdot \\frac{\\partial (X\\theta)_i}{\\partial \\theta_k} = \\exp((X\\theta)_i) \\cdot X_{ik}$。线性项的导数是 $\\frac{\\partial}{\\partial \\theta_k} (d_i \\sum_j X_{ij}\\theta_j) = d_i X_{ik}$。\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_k} = \\sum_{i=1}^{n} \\left[ \\exp((X\\theta)_i) X_{ik} - d_i X_{ik} \\right] = \\sum_{i=1}^{n} X_{ik} (\\exp((X\\theta)_i) - d_i)\n$$\n这是梯度向量的第 $k$ 个分量。在矩阵表示法中，认识到 $X_{ik}$ 是 $X^T$ 的 $(k,i)$ 元素，这可以写成：\n$$\n\\nabla J(\\theta) = X^T (\\exp(X\\theta) - d)\n$$\n其中 $\\exp(X\\theta)$ 是分量为 $\\exp((X\\theta)_i) = \\lambda_i(\\theta)$ 的向量，而 $d$ 是观测值 $d_i$ 的向量。\n\n为了求 Hessian 矩阵 $\\nabla^2 J(\\theta)$，我们将梯度的每个分量对 $\\theta_l$（$l=1, \\dots, p$）求偏导数：\n$$\n[\\nabla^2 J(\\theta)]_{kl} = \\frac{\\partial^2 J(\\theta)}{\\partial \\theta_l \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_l} \\left( \\sum_{i=1}^{n} X_{ik} (\\exp((X\\theta)_i) - d_i) \\right)\n$$\n$$\n[\\nabla^2 J(\\theta)]_{kl} = \\sum_{i=1}^{n} X_{ik} \\frac{\\partial}{\\partial \\theta_l} (\\exp((X\\theta)_i) - d_i) = \\sum_{i=1}^{n} X_{ik} \\left( \\exp((X\\theta)_i) \\cdot X_{il} \\right)\n$$\n$$\n[\\nabla^2 J(\\theta)]_{kl} = \\sum_{i=1}^{n} X_{ik} \\lambda_i(\\theta) X_{il}\n$$\n该表达式是矩阵乘积 $X^T W X$ 的第 $(k,l)$ 个元素，其中 $W$ 是一个对角矩阵，对角线上的元素为 $W_{ii} = \\lambda_i(\\theta)$。因此，Hessian 矩阵为：\n$$\n\\nabla^2 J(\\theta) = X^T \\text{diag}(\\lambda_1(\\theta), \\dots, \\lambda_n(\\theta)) X\n$$\n如果一个函数的 Hessian 矩阵是半正定的，则该函数是凸函数。对于任意向量 $v \\in \\mathbb{R}^p$，我们必须检验二次型 $v^T (\\nabla^2 J(\\theta)) v$：\n$$\nv^T (\\nabla^2 J(\\theta)) v = v^T (X^T W X) v = (Xv)^T W (Xv)\n$$\n设 $u = Xv$，这是一个在 $\\mathbb{R}^n$ 中的向量。该表达式变为 $u^T W u$。由于 $W$ 是一个对角矩阵，这等于：\n$$\nu^T W u = \\sum_{i=1}^n u_i W_{ii} u_i = \\sum_{i=1}^n u_i^2 \\lambda_i(\\theta)\n$$\n问题陈述断言，对于所有 $i$，泊松速率 $\\lambda_i(\\theta)$ 均为正。由于 $u_i^2 \\ge 0$，和中的每一项都是非负的。因此，$\\sum_{i=1}^n u_i^2 \\lambda_i(\\theta) \\ge 0$。这证实了 Hessian 矩阵 $\\nabla^2 J(\\theta)$ 对所有 $\\theta$ 都是半正定的。因此，负对数似然函数 $J(\\theta)$ 是 $\\theta$ 的一个凸函数。\n\n使用典范连接函数 $\\eta = \\ln \\lambda$ 对这种凸性至关重要。该连接函数确保了 $J(\\theta)$ 中对应于观测值 $i$ 的部分对其线性预测变量 $\\eta_i$ 的二阶导数 $\\frac{\\partial^2}{\\partial \\eta_i^2}(\\lambda_i - d_i \\eta_i) = \\frac{\\partial^2}{\\partial \\eta_i^2}(\\exp(\\eta_i) - d_i \\eta_i) = \\exp(\\eta_i) = \\lambda_i$ 等于泊松分布的方差函数。由于速率参数 $\\lambda_i$ 必须为正，这保证了 Hessian 矩阵 $X^T W X$ 中的对角矩阵 $W$ 具有严格为正的对角元素，从而确保了半正定性，因此无论设计矩阵 $X$ 如何，函数都具有凸性。\n\nc) 对数似然函数的闭式解析表达式。\n\n我们从 (a) 部分推导的对数似然函数的一般表达式开始：\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ d_i \\ln(\\lambda_i(\\theta)) - \\lambda_i(\\theta) - \\ln(d_i!) \\right]\n$$\n我们代入 (b) 部分中基于带典范连接的线性模型的关系式：$\\ln(\\lambda_i(\\theta)) = \\eta_i(\\theta) = (X\\theta)_i$ 和 $\\lambda_i(\\theta) = \\exp((X\\theta)_i)$。\n$$\n\\ell(\\theta;d) = \\sum_{i=1}^{n} \\left[ d_i (X\\theta)_i - \\exp((X\\theta)_i) - \\ln(d_i!) \\right]\n$$\n这就是所要求的用 $X$、$\\theta$ 和 $d$ 表示的对数似然函数的闭式表达式。它包含了所有按要求依赖于 $\\theta$ 或 $d$ 的项。",
            "answer": "$$\\boxed{\\sum_{i=1}^{n} \\left(d_i (X\\theta)_i - \\exp((X\\theta)_i) - \\ln(d_i!)\\right)}$$"
        },
        {
            "introduction": "经典的误差模型往往难以应对数据中的异常值（outliers），这些异常值可能严重扭曲参数估计的结果。为了构建更稳健的似然函数，我们可以使用混合模型来描述这种“正常”数据与“异常”数据共存的现象。 本练习将引导我们进入一个更高级的主题：如何使用期望最大化（Expectation-Maximization, EM）算法来估计高斯混合模型的参数。掌握这一方法，对于处理含有污染或具有重尾特性的复杂误差分布至关重要。",
            "id": "3397447",
            "problem": "考虑一个一维逆问题，其观测模型由 $y_i = h_i(x) + \\varepsilon_i$ 给出，其中 $i = 1, \\dots, N$，$x$ 表示未知状态，$h_i(x)$ 是在 $x$ 处求值的已知正演模型。假设在当前迭代点 $x^{(t)}$，残差 $r_i = y_i - h_i(x^{(t)})$ 被建模为从一个双组分高斯混合分布中的独立抽样，以捕捉重尾误差过程：残差有 $\\pi_1$ 的概率从 $\\mathcal{N}(\\mu_1, \\sigma_1^2)$（“内点”组分）中抽取，并有 $\\pi_2$ 的概率从 $\\mathcal{N}(\\mu_2, \\sigma_2^2)$（“离群点”组分）中抽取，其中 $\\pi_1 + \\pi_2 = 1$，$\\sigma_1^2$ 和 $\\sigma_2^2$ 是已知的正常数，且 $\\sigma_1^2 \\ll \\sigma_2^2$。均值 $\\mu_1, \\mu_2$ 和权重 $\\pi_1, \\pi_2$ 是未知的，必须进行估计。目标是建立似然函数模型，并使用期望最大化（EM; Expectation-Maximization）算法推导估计更新。\n\n从混合模型观测数据似然的基本定义出发，并引入潜在指示变量 $z_{ik} \\in \\{0,1\\}$（对于 $k \\in \\{1,2\\}$ 且 $z_{i1} + z_{i2} = 1$），为该混合误差模型推导EM算法：\n\n1. 推导期望步骤（E-步）的后验概率（responsibilities）$\\gamma_{ik} = \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}]$，其中 $\\theta^{(t)} = (\\pi_1^{(t)}, \\pi_2^{(t)}, \\mu_1^{(t)}, \\mu_2^{(t)})$ 表示当前的参数估计值。\n2. 推导最大化步骤（M-步）中混合权重和均值的参数更新，假设方差 $\\sigma_1^2$ 和 $\\sigma_2^2$ 是固定且已知的。您的推导必须只使用核心原理，包括独立似然的乘积形式、混合模型定义以及EM所需的琴生不等式（Jensen’s inequality），避免使用任何预先给定的简化公式。\n3. 分析该模型EM算法的收敛特性，解释为什么观测数据对数似然在迭代过程中是非递减的，并描述不动点的性质。在固定方差与自由方差的背景下，讨论与混合模型相关的潜在退化和病态问题（例如标签切换对称性、组分坍缩和似然奇异性），并确定在此特定设置下缓解这些问题的条件。\n\n最后，对于任意残差 $r_i$ 和当前参数值 $(\\pi_1, \\pi_2, \\mu_1, \\mu_2)$ 以及已知的 $\\sigma_1^2, \\sigma_2^2$，提供内点组分的后验概率 $\\gamma_{i1}$ 的闭式解析表达式。这个表达式是您最终答案中必须返回的唯一量。无需四舍五入，不涉及单位；请以精确的符号形式表示您的答案。",
            "solution": "首先根据指定标准验证问题。\n\n### 步骤1：提取已知条件\n-   **观测模型**: $y_i = h_i(x) + \\varepsilon_i$，对于 $i = 1, \\dots, N$。\n-   **未知状态**: $x$。\n-   **正演模型**: $h_i(x)$ 已知。\n-   **当前迭代点**: $x^{(t)}$。\n-   **残差**: $r_i = y_i - h_i(x^{(t)})$。\n-   **误差模型**: 残差 $r_i$ 是从双组分高斯混合分布中的独立抽样。\n-   **组分1（内点）**: 以概率 $\\pi_1$ 从 $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ 中抽取。\n-   **组分2（离群点）**: 以概率 $\\pi_2$ 从 $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ 中抽取。\n-   **混合约束**: $\\pi_1 + \\pi_2 = 1$。\n-   **已知参数**: 方差 $\\sigma_1^2$ 和 $\\sigma_2^2$ 是已知的正常数，且 $\\sigma_1^2 \\ll \\sigma_2^2$。\n-   **待估未知参数**: 均值 $\\mu_1, \\mu_2$ 和混合权重 $\\pi_1, \\pi_2$。\n-   **潜在变量**: 指示变量 $z_{ik} \\in \\{0,1\\}$，对于 $k \\in \\{1,2\\}$ 且 $z_{i1} + z_{i2} = 1$，其中如果残差 $r_i$ 源于组分 $k$，则 $z_{ik}=1$。\n-   **参数矢量**: $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$。当前估计为 $\\theta^{(t)}$。\n-   **任务1**: 推导E-步的后验概率 $\\gamma_{ik} = \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}]$。\n-   **任务2**: 推导M-步中 $\\pi_k$ 和 $\\mu_k$ 的参数更新。\n-   **任务3**: 分析该模型EM算法的收敛性和潜在病态问题。\n-   **最终答案**: 提供内点组分的后验概率 $\\gamma_{i1}$ 的闭式解析表达式。\n\n### 步骤2：使用提取的已知条件进行验证\n评估问题的有效性。\n-   **科学依据**: 该问题描述了期望最大化（EM）算法在高斯混合模型（GMM）中的应用。这是统计学、机器学习和数据同化中用于建模异构数据和进行稳健估计的一项标准和基本技术。在逆问题中使用重尾混合模型处理残差是处理离群点的一种科学上合理的方法。\n-   **适定性**: 该问题是适定的。它提供了清晰的模型规范，并要求推导EM算法的具体步骤，分析其性质，并给出一个最终的、特定的数学表达式。推导的每个部分都存在唯一且有意义的解。\n-   **客观性**: 问题以精确、客观、形式化的数学语言陈述。它没有任何主观或模糊的术语。\n\n该问题没有说明中列出的任何缺陷。它是科学上合理的、自洽的并且可形式化的。约束条件（固定方差）被明确陈述，并且是分析的关键。因此，该问题被认为是有效的。\n\n### 步骤3：结论与行动\n问题有效。将提供完整解答。\n\n***\n\n按照要求，解答分三部分推导。令 $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$ 为待估计的参数矢量。方差 $\\sigma_1^2$ 和 $\\sigma_2^2$ 是已知常数。我们给定一组 $N$ 个残差 $R = \\{r_1, \\dots, r_N\\}$。\n\n在此混合模型下，单个残差 $r_i$ 的概率密度函数（PDF）为：\n$$p(r_i \\mid \\theta) = \\sum_{k=1}^2 \\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)$$\n其中 $\\mathcal{N}(r \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r-\\mu)^2}{2\\sigma^2}\\right)$ 是高斯PDF。\n\n我们引入潜在指示变量 $z_{ik} \\in \\{0,1\\}$，使得如果 $r_i$ 是由组分 $k$ 生成的，则 $z_{ik}=1$，否则 $z_{ik}=0$，且 $\\sum_{k=1}^2 z_{ik}=1$。所有潜在变量的集合为 $Z = \\{z_{ik}\\}_{i=1, k=1}^{N, 2}$。单个观测 $(r_i, z_i)$ 的完整数据似然为 $p(r_i, z_i \\mid \\theta) = \\prod_{k=1}^2 [\\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)]^{z_{ik}}$。\n\n### 1. E-步：后验概率的推导\n期望步骤（E-步）计算在给定观测数据和当前参数估计 $\\theta^{(t)}$ 的条件下，潜在变量的期望。这个期望是组分 $k$ 生成观测值 $r_i$ 的后验概率，称为后验概率（responsibility），记为 $\\gamma_{ik}$。\n\n$$ \\gamma_{ik} \\equiv \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}] = P(z_{ik}=1 \\mid r_i, \\theta^{(t)}) $$\n\n使用贝叶斯定理，该后验概率为：\n$$ \\gamma_{ik} = \\frac{P(r_i \\mid z_{ik}=1, \\theta^{(t)}) P(z_{ik}=1 \\mid \\theta^{(t)})}{P(r_i \\mid \\theta^{(t)})} $$\n\n各项可确定为：\n-   $P(r_i \\mid z_{ik}=1, \\theta^{(t)}) = \\mathcal{N}(r_i \\mid \\mu_k^{(t)}, \\sigma_k^2)$ 是在给定 $r_i$ 来自组分 $k$ 的条件下观测到 $r_i$ 的似然。\n-   $P(z_{ik}=1 \\mid \\theta^{(t)}) = \\pi_k^{(t)}$ 是选择组分 $k$ 的先验概率。\n-   $P(r_i \\mid \\theta^{(t)}) = \\sum_{j=1}^2 P(r_i \\mid z_{ij}=1, \\theta^{(t)}) P(z_{ij}=1 \\mid \\theta^{(t)}) = \\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(r_i \\mid \\mu_j^{(t)}, \\sigma_j^2)$ 是 $r_i$ 的边际似然，通过对所有可能的组分求和得到。\n\n将这些代入 $\\gamma_{ik}$ 的表达式中，得到E-步的更新规则：\n$$ \\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(r_i \\mid \\mu_k^{(t)}, \\sigma_k^2)}{\\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(r_i \\mid \\mu_j^{(t)}, \\sigma_j^2)} $$\n对每个数据点 $i=1, \\dots, N$ 和每个组分 $k=1, 2$ 计算这些后验概率。\n\n### 2. M-步：参数更新的推导\n最大化步骤（M-步）更新参数 $\\theta$ 以最大化期望的完整数据对数似然 $Q(\\theta \\mid \\theta^{(t)})$，其中期望是关于在E-步中计算的潜在变量的后验分布来求的。\n\n整个数据集 $R$ 和潜在变量 $Z$ 的完整数据对数似然为：\n$$ \\ln p(R, Z \\mid \\theta) = \\ln \\left( \\prod_{i=1}^N \\prod_{k=1}^2 [\\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)]^{z_{ik}} \\right) = \\sum_{i=1}^N \\sum_{k=1}^2 z_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2) \\right) $$\n\n在M-步中要最大化的函数是 $Q(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{Z \\mid R, \\theta^{(t)}}[\\ln p(R, Z \\mid \\theta)]$:\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^N \\sum_{k=1}^2 \\mathbb{E}[z_{ik}] \\left( \\ln \\pi_k + \\ln \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2) \\right) $$\n其中 $\\mathbb{E}[z_{ik}]$ 是在E-步中计算的后验概率 $\\gamma_{ik}$。\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\left( \\ln \\pi_k - \\frac{1}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{(r_i - \\mu_k)^2}{2\\sigma_k^2} \\right) $$\n\n我们对该函数关于 $\\mu_k$ 和 $\\pi_k$ 进行最大化。\n\n**均值 $\\mu_k$ 的更新**：\n我们通过将 $Q$ 对 $\\mu_k$ 的偏导数设为零来找到最大值。我们只需考虑涉及 $\\mu_k$ 的项。\n$$ \\frac{\\partial Q}{\\partial \\mu_k} = \\frac{\\partial}{\\partial \\mu_k} \\sum_{i=1}^N \\gamma_{ik} \\left( -\\frac{(r_i - \\mu_k)^2}{2\\sigma_k^2} \\right) = \\sum_{i=1}^N \\gamma_{ik} \\left( \\frac{r_i - \\mu_k}{\\sigma_k^2} \\right) = 0 $$\n由于 $\\sigma_k^2 > 0$，我们可以将其乘掉：\n$$ \\sum_{i=1}^N \\gamma_{ik} (r_i - \\mu_k) = 0 \\implies \\sum_{i=1}^N \\gamma_{ik} r_i - \\mu_k \\sum_{i=1}^N \\gamma_{ik} = 0 $$\n求解 $\\mu_k$ 得到新估计值 $\\mu_k^{(t+1)}$ 的更新规则：\n$$ \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma_{ik} r_i}{\\sum_{i=1}^N \\gamma_{ik}} $$\n这是残差的加权平均，其中权重是后验概率。令 $N_k = \\sum_{i=1}^N \\gamma_{ik}$ 为分配给组分 $k$ 的有效点数。则 $\\mu_k^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} r_i$。\n\n**混合权重 $\\pi_k$ 的更新**：\n我们最大化 $Q$ 中涉及 $\\pi_k$ 的项，并满足约束条件 $\\sum_{k=1}^2 \\pi_k = 1$。$Q$ 的相关部分是 $\\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\ln \\pi_k$。我们使用拉格朗日乘子 $\\lambda$：\n$$ \\mathcal{L}(\\pi_1, \\pi_2, \\lambda) = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\ln \\pi_k + \\lambda \\left( \\sum_{k=1}^2 \\pi_k - 1 \\right) $$\n对 $\\pi_k$ 求导并设为零：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = \\sum_{i=1}^N \\frac{\\gamma_{ik}}{\\pi_k} + \\lambda = 0 \\implies \\pi_k = -\\frac{\\sum_{i=1}^N \\gamma_{ik}}{\\lambda} = -\\frac{N_k}{\\lambda} $$\n对 $k$ 求和并使用约束条件：\n$$ \\sum_{k=1}^2 \\pi_k = 1 \\implies \\sum_{k=1}^2 \\left(-\\frac{N_k}{\\lambda}\\right) = 1 \\implies -\\frac{1}{\\lambda} \\sum_{k=1}^2 N_k = 1 $$\n我们知道 $\\sum_{k=1}^2 N_k = \\sum_{k=1}^2 \\sum_{i=1}^N \\gamma_{ik} = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} = \\sum_{i=1}^N 1 = N$。\n因此，$-\\frac{N}{\\lambda} = 1 \\implies \\lambda = -N$。\n将 $\\lambda$ 代回到 $\\pi_k$ 的表达式中：\n$$ \\pi_k^{(t+1)} = -\\frac{N_k}{(-N)} = \\frac{N_k}{N} = \\frac{\\sum_{i=1}^N \\gamma_{ik}}{N} $$\n这是组分 $k$ 在所有数据点上的平均后验概率。\n\n### 3. 收敛特性和病态问题\n**收敛性**：EM算法生成一个参数估计序列 $\\theta^{(t)}$，使得观测数据对数似然 $\\ell(\\theta^{(t)}) = \\ln p(R|\\theta^{(t)}) = \\sum_{i=1}^N \\ln p(r_i|\\theta^{(t)})$ 在每次迭代中都是非递减的，即 $\\ell(\\theta^{(t+1)}) \\geq \\ell(\\theta^{(t)})$。此性质源于对数似然与 $Q$ 函数之间的关系。对数似然的变化可以写为：\n$$ \\ell(\\theta) - \\ell(\\theta^{(t)}) = [Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})] + [\\text{KL}(p(Z|R,\\theta^{(t)})||p(Z|R,\\theta))] $$\n其中KL是Kullback-Leibler散度。M-步旨在找到最大化 $Q(\\theta|\\theta^{(t)})$ 的 $\\theta^{(t+1)}$，所以第一项 $[Q(\\theta^{(t+1)}|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})]$ 是非负的。KL散度总是非负的。因此，设置 $\\theta = \\theta^{(t+1)}$ 保证了 $\\ell(\\theta^{(t+1)}) - \\ell(\\theta^{(t)}) \\geq 0$。由于对数似然有上界（如下所示），序列 $\\ell(\\theta^{(t)})$ 必将收敛到似然函数的一个驻点（局部最大值点或鞍点）。\n\n**病态问题**：\n1.  **标签切换对称性**：混合模型的似然函数对于组分索引的置换是不变的。即，$p(r_i \\mid \\pi_1, \\mu_1, \\sigma_1^2; \\pi_2, \\mu_2, \\sigma_2^2) = p(r_i \\mid \\pi_2, \\mu_2, \\sigma_2^2; \\pi_1, \\mu_1, \\sigma_1^2)$。这意味着似然曲面有多个等价的最大值点。在本问题中，条件 $\\sigma_1^2 \\ll \\sigma_2^2$ 提供了一种打破这种对称性的自然方式，通过将组分1识别为“内点”，组分2识别为“离群点”，从而使解具有可解释性。\n\n2.  **似然奇异性（组分坍缩）**：这是GMM中当方差也需要估计时的一种严重病态问题。如果某个组分的均值 $\\mu_k$ 变得等于一个数据点 $r_i$，并且其方差 $\\sigma_k^2$ 被允许趋近于零，那么项 $\\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)$ 将趋于无穷大，使得总似然无界。这会导致一个虚假的、退化的解。**在此特定问题中，这种病态问题被完全避免了，因为方差 $\\sigma_1^2$ 和 $\\sigma_2^2$ 被指定为已知的、固定的正常数。** 在固定的正常数方差下，高斯PDF $\\mathcal{N}(r \\mid \\mu, \\sigma^2)$ 对所有 $r$ 和 $\\mu$ 都是有界的。因此，对数似然函数也是有上界的，不会发生奇异性。\n\n3.  **边界上的退化**：EM算法可能收敛到一个解，其中某个混合权重 $\\pi_k$ 趋近于0。如果一个组分对数据的解释非常差，以至于其对所有 $i$ 的后验概率 $\\gamma_{ik}$ 都变得可以忽略不计时，就会发生这种情况。虽然这有时被视为一种病态，但它也可能表明一个更简单的模型（具有更少的组分）对于数据是足够的。在这个双组分模型中，如果 $\\pi_2 \\to 0$，则意味着数据可以被单个高斯分布充分描述，而没有显著的离群点群体。如果强烈希望得到一个双组分的解，可以使用正则化或在 $\\pi_k$ 上施加贝叶斯先验来防止这种情况。\n\n最后的要求是，对于任意残差 $r_i$ 和当前参数 $(\\pi_1, \\pi_2, \\mu_1, \\mu_2)$，内点组分的后验概率 $\\gamma_{i1}$ 的闭式表达式。根据E-步的推导：\n$$ \\gamma_{i1} = \\frac{\\pi_1 \\mathcal{N}(r_i \\mid \\mu_1, \\sigma_1^2)}{\\pi_1 \\mathcal{N}(r_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\mathcal{N}(r_i \\mid \\mu_2, \\sigma_2^2)} $$\n代入高斯PDF的完整表达式：\n$$ \\gamma_{i1} = \\frac{\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\pi_2 \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)} $$\n分子和分母中的公因子 $\\frac{1}{\\sqrt{2\\pi}}$ 可以消去：\n$$ \\gamma_{i1} = \\frac{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\frac{\\pi_2}{\\sigma_2} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)} $$\n这就是最终的解析表达式。",
            "answer": "$$\\boxed{\\frac{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\frac{\\pi_2}{\\sigma_2} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)}}$$"
        }
    ]
}