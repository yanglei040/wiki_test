{
    "hands_on_practices": [
        {
            "introduction": "This exercise serves as a critical cautionary tale in statistical modeling. It demonstrates that taking shortcuts in constructing a likelihood function—specifically by omitting parameter-dependent normalization terms—is not a benign simplification and can lead to fundamentally flawed results. By working through this hypothetical scenario, you will prove that such an improper likelihood yields an inconsistent estimator, reinforcing the necessity of adhering to the full, correct form of the likelihood for valid inference.",
            "id": "3397329",
            "problem": "Consider a simple scalar observation model commonly used to motivate likelihood construction in inverse problems and data assimilation. Let $\\{y_i\\}_{i=1}^{n}$ be independent and identically distributed (i.i.d.) draws from a Gaussian measurement model with parameter-dependent mean and variance given by $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\theta)$, where $\\theta \\in (0,\\infty)$ is the unknown physical parameter to be estimated.\n\nYou are told that the scientifically correct likelihood for a Gaussian model must include the normalization factor that depends on the covariance determinant. In this one-dimensional setting, the normalization factor is a function of $\\theta$. An analyst, however, mistakenly constructs an improper pseudo-likelihood by deleting every $\\theta$-dependent normalization factor and retaining only the exponential term. This is equivalent to maximizing the likelihood built solely from the negative quadratic form in the exponent.\n\nUsing only first principles, namely the definition of the likelihood for independent Gaussian data and standard limit theorems for sample averages, carry out the following steps:\n\n1. Starting from the definition of the Gaussian density and the independence of $\\{y_i\\}_{i=1}^{n}$, write down the correct log-likelihood and the improper pseudo-log-likelihood (that omits every $\\theta$-dependent normalization factor). Carefully identify which term is dropped in forming the improper pseudo-log-likelihood.\n2. Derive the maximizer $\\tilde{\\theta}_n$ of the improper pseudo-log-likelihood over $\\theta \\in (0,\\infty)$.\n3. Compute the probability limit (in the sense of convergence in probability) of $\\tilde{\\theta}_n$ as $n \\to \\infty$ under the true data-generating parameter $\\theta_0 \\in (0,\\infty)$, expressing your answer as a closed-form function of $\\theta_0$.\n\nYour final answer must be a single closed-form analytic expression for the probability limit. No rounding is required, and no units are involved.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The data $\\{y_i\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) draws.\n- The data-generating model is a Gaussian distribution where the mean and variance are both equal to an unknown parameter $\\theta$: $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\theta)$.\n- The parameter space for $\\theta$ is $(0, \\infty)$.\n- An improper pseudo-likelihood is constructed by deleting every $\\theta$-dependent normalization factor from the correct likelihood, which is equivalent to building the likelihood from the negative quadratic form in the exponent.\n- The true, data-generating parameter value is denoted by $\\theta_0 \\in (0, \\infty)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on the established principles of statistical inference, namely maximum likelihood estimation for a Gaussian model. The chosen model, $\\mathcal{N}(\\theta, \\theta)$, is a valid statistical model that provides a non-trivial but tractable setting to explore the consequences of model misspecification. The problem is mathematically and scientifically sound.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information: the statistical model, the definition of the correct and improper likelihoods, and a clear objective. The sequence of tasks leads to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise, objective language, free of subjective claims or ambiguity.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, missing information, or ill-posed structure. It is a standard, valid problem in theoretical statistics, relevant to the fields of inverse problems and data assimilation where correct likelihood specification is critical.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds by following the three steps outlined in the problem statement.\n\n**1. Correct and Improper Log-Likelihoods**\n\nThe probability density function (PDF) for a single observation $y_i$ from a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by $f(y_i \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right)$.\nIn this problem, the mean is $\\mu = \\theta$ and the variance is $\\sigma^2 = \\theta$. Substituting these into the PDF gives:\n$$f(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\theta}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\theta}\\right)$$\nSince the observations $\\{y_i\\}_{i=1}^{n}$ are i.i.d., the total likelihood function $L(\\theta \\mid \\{y_i\\})$ is the product of the individual PDFs:\n$$L(\\theta) = \\prod_{i=1}^{n} f(y_i \\mid \\theta) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\theta)^{1/2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\theta}\\right)$$\n$$L(\\theta) = (2\\pi\\theta)^{-n/2} \\exp\\left(-\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2\\right)$$\nThe correct log-likelihood function, $\\ell(\\theta)$, is the natural logarithm of $L(\\theta)$:\n$$\\ell(\\theta) = \\ln(L(\\theta)) = \\ln\\left((2\\pi\\theta)^{-n/2}\\right) + \\ln\\left(\\exp\\left(-\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2\\right)\\right)$$\n$$\\ell(\\theta) = -\\frac{n}{2}\\ln(2\\pi\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2$$\n$$\\ell(\\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\theta) - \\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2$$\nThe problem states that the improper pseudo-likelihood is formed by deleting every $\\theta$-dependent normalization factor. In the full likelihood $L(\\theta)$, the term $(2\\pi\\theta)^{-n/2}$ contains the normalization factors. The $\\theta$-dependent part is $\\theta^{-n/2}$. In the log-likelihood $\\ell(\\theta)$, this corresponds to the term $-\\frac{n}{2}\\ln(\\theta)$. Dropping this term (and the constant term $-\\frac{n}{2}\\ln(2\\pi)$, which does not affect maximization) yields the improper pseudo-log-likelihood, $\\tilde{\\ell}(\\theta)$:\n$$\\tilde{\\ell}(\\theta) = -\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i - \\theta)^2$$\nThis aligns with the problem's description of retaining only the logarithm of the exponential term. The term dropped from the correct log-likelihood to obtain the kernel of the improper one is precisely $-\\frac{n}{2}\\ln(\\theta)$.\n\n**2. Derivation of the Improper Maximizer $\\tilde{\\theta}_n$**\n\nTo find the maximizer $\\tilde{\\theta}_n$ of the improper pseudo-log-likelihood $\\tilde{\\ell}(\\theta)$, we first expand the expression and then differentiate with respect to $\\theta$.\n$$\\tilde{\\ell}(\\theta) = -\\frac{1}{2\\theta}\\sum_{i=1}^{n} (y_i^2 - 2y_i\\theta + \\theta^2) = -\\frac{1}{2\\theta}\\left(\\sum_{i=1}^{n}y_i^2 - 2\\theta\\sum_{i=1}^{n}y_i + n\\theta^2\\right)$$\n$$\\tilde{\\ell}(\\theta) = -\\frac{1}{2\\theta}\\sum_{i=1}^{n}y_i^2 + \\sum_{i=1}^{n}y_i - \\frac{n\\theta}{2}$$\nNow, we compute the first derivative with respect to $\\theta$ and set it to zero to find the critical points:\n$$\\frac{d\\tilde{\\ell}}{d\\theta} = \\frac{d}{d\\theta}\\left(-\\frac{1}{2}\\theta^{-1}\\sum_{i=1}^{n}y_i^2 + \\sum_{i=1}^{n}y_i - \\frac{n\\theta}{2}\\right) = \\frac{1}{2}\\theta^{-2}\\sum_{i=1}^{n}y_i^2 - \\frac{n}{2}$$\nSetting the derivative to zero:\n$$\\frac{1}{2\\theta^2}\\sum_{i=1}^{n}y_i^2 - \\frac{n}{2} = 0 \\implies \\frac{1}{\\theta^2}\\sum_{i=1}^{n}y_i^2 = n$$\n$$\\theta^2 = \\frac{1}{n}\\sum_{i=1}^{n}y_i^2$$\nSince the parameter space is $\\theta \\in (0,\\infty)$, we take the positive square root. The maximizer $\\tilde{\\theta}_n$ is:\n$$\\tilde{\\theta}_n = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}y_i^2}$$\nTo confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\tilde{\\ell}}{d\\theta^2} = \\frac{d}{d\\theta}\\left(\\frac{1}{2}\\theta^{-2}\\sum_{i=1}^{n}y_i^2 - \\frac{n}{2}\\right) = -\\theta^{-3}\\sum_{i=1}^{n}y_i^2 = -\\frac{1}{\\theta^3}\\sum_{i=1}^{n}y_i^2$$\nFor $\\theta > 0$ and assuming not all $y_i$ are zero (a probability-zero event), $\\sum y_i^2 > 0$, so $\\frac{d^2\\tilde{\\ell}}{d\\theta^2}  0$. This confirms that $\\tilde{\\theta}_n$ is indeed a local maximum. As it's the only critical point in $(0, \\infty)$, it is the global maximizer.\n\n**3. Probability Limit of $\\tilde{\\theta}_n$**\n\nWe need to compute the probability limit of $\\tilde{\\theta}_n$ as $n \\to \\infty$. The true data are generated i.i.d. from $\\mathcal{N}(\\theta_0, \\theta_0)$.\n$$\\text{plim}_{n\\to\\infty} \\tilde{\\theta}_n = \\text{plim}_{n\\to\\infty} \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}y_i^2}$$\nThe expression inside the square root, $\\frac{1}{n}\\sum_{i=1}^{n}y_i^2$, is the sample mean of the random variables $Z_i = y_i^2$. By the Law of Large Numbers, the sample mean of i.i.d. random variables converges in probability to their expected value, provided the expectation exists.\n$$\\frac{1}{n}\\sum_{i=1}^{n}y_i^2 \\xrightarrow{p} E[y^2]$$\nwhere the expectation is taken with respect to the true data-generating distribution $y \\sim \\mathcal{N}(\\theta_0, \\theta_0)$. The second moment $E[y^2]$ can be calculated from the mean $E[y]$ and variance $\\text{Var}(y)$ using the formula $E[y^2] = \\text{Var}(y) + (E[y])^2$. For our distribution, we have:\n$$E[y] = \\theta_0$$\n$$\\text{Var}(y) = \\theta_0$$\nSubstituting these into the formula for the second moment gives:\n$$E[y^2] = \\theta_0 + (\\theta_0)^2 = \\theta_0 + \\theta_0^2$$\nNow, we apply the Continuous Mapping Theorem. Since the square root function $g(x)=\\sqrt{x}$ is continuous for $x > 0$, and since $\\theta_0 > 0$ implies $\\theta_0 + \\theta_0^2 > 0$, we can pass the probability limit inside the function:\n$$\\text{plim}_{n\\to\\infty} \\tilde{\\theta}_n = \\sqrt{\\text{plim}_{n\\to\\infty} \\frac{1}{n}\\sum_{i=1}^{n}y_i^2} = \\sqrt{E[y^2]}$$\nTherefore, the probability limit of the improper estimator is:\n$$\\text{plim}_{n\\to\\infty} \\tilde{\\theta}_n = \\sqrt{\\theta_0 + \\theta_0^2}$$\nThis result shows that the estimator derived from the improper pseudo-likelihood is inconsistent, as it does not converge to the true parameter value $\\theta_0$.",
            "answer": "$$\\boxed{\\sqrt{\\theta_0 + \\theta_0^2}}$$"
        },
        {
            "introduction": "Building on the foundation of a correctly specified Gaussian likelihood, this practice explores the practical consequences of model misspecification. You will quantify the \"cost\" of using an incorrect noise variance by deriving the Kullback-Leibler divergence between the true and assumed models. This exercise provides a sharp, analytical insight into model sensitivity, a crucial skill for diagnosing and understanding the behavior of data assimilation systems.",
            "id": "3397432",
            "problem": "Consider a classical observation model for an inverse problem with independent Gaussian measurement errors. Let the parameter be $x \\in \\mathbb{R}^{n}$ and the forward map be $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ with components $h_{i}(x)$ for $i \\in \\{1,\\ldots,m\\}$. Suppose an observation vector $y \\in \\mathbb{R}^{m}$ is generated according to\n$$\ny = h(x_{\\mathrm{true}}) + \\varepsilon,\n$$\nwhere the noise $\\varepsilon$ has a zero-mean Gaussian distribution with block-diagonal covariance\n$$\nR = \\mathrm{diag}\\!\\left(r_{1}^{2},\\ldots,r_{m}^{2}\\right),\n$$\nwith all $r_{i} > 0$. Assume $R$ is known and the errors are independent across components.\n\nStarting from the probability density function of a multivariate Gaussian distribution with independent components, do the following:\n\n1. Derive the log-likelihood $\\ln p\\!\\left(y \\mid x, R\\right)$ and express it as a sum of per-measurement contributions. Clearly identify the contribution of each measurement $i$ to the log-likelihood as a function of the residual $d_{i}(x) = y_{i} - h_{i}(x)$ and $r_{i}$.\n\n2. To analyze the sensitivity of model fit to noise-variance mis-specification, consider an analyst who uses a misspecified diagonal noise covariance $\\widetilde{R} = \\mathrm{diag}\\!\\left(\\tilde{r}_{1}^{2},\\ldots,\\tilde{r}_{m}^{2}\\right)$, where $\\tilde{r}_{i} = \\alpha_{i} r_{i}$ for some scalar $\\alpha_{i} > 0$. Focus on a single measurement index $i$ and assume $x = x_{\\mathrm{true}}$, so that $y_{i} - h_{i}(x_{\\mathrm{true}})$ has the true distribution $\\mathcal{N}(0,r_{i}^{2})$. Using only fundamental definitions and moment identities for Gaussian random variables, derive the expected excess negative log-likelihood per measurement,\n$$\n\\Delta(\\alpha_{i}) \\equiv \\mathbb{E}_{y_{i} \\sim \\mathcal{N}(h_{i}(x_{\\mathrm{true}}), r_{i}^{2})}\\!\\Big[-\\ln p\\!\\left(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}\\right) + \\ln p\\!\\left(y_{i}\\mid x_{\\mathrm{true}}, r_{i}\\right)\\Big],\n$$\nas a closed-form function of $\\alpha_{i}$ only. This quantity equals the Kullback–Leibler divergence (from the true to the misspecified per-measurement model) and quantifies the sensitivity of the log-likelihood to variance mis-specification.\n\nProvide your final answer as the simplified analytic expression for $\\Delta(\\alpha_{i})$ in terms of $\\alpha_{i}$ only. Do not include units. If simplification requires constants, keep them exact. Your final answer must be a single closed-form expression.",
            "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. It presents a standard task in statistical modeling and inverse problems theory. All necessary definitions and conditions are provided to derive a unique, analytical solution. Therefore, the problem is deemed valid.\n\nThe solution is derived in two parts as requested.\n\nFirst, we derive the log-likelihood function. The observation model is $y = h(x_{\\mathrm{true}}) + \\varepsilon$, where the noise vector $\\varepsilon$ follows a $0$-mean multivariate Gaussian distribution, $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that the conditional probability density function (PDF) of the observation vector $y$ given the parameter vector $x$ is $y \\sim \\mathcal{N}(h(x), R)$, expressed as:\n$$p(y \\mid x, R) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det(R)}} \\exp\\left(-\\frac{1}{2} (y - h(x))^{T} R^{-1} (y - h(x))\\right)$$\nThe noise covariance matrix $R$ is given as a diagonal matrix $R = \\mathrm{diag}(r_{1}^{2}, r_{2}^{2}, \\ldots, r_{m}^{2})$. The determinant of this diagonal matrix is the product of its diagonal entries, $\\det(R) = \\prod_{i=1}^{m} r_{i}^{2}$. Its inverse is also a diagonal matrix, $R^{-1} = \\mathrm{diag}(r_{1}^{-2}, r_{2}^{-2}, \\ldots, r_{m}^{-2})$. The quadratic form in the exponent simplifies to a sum:\n$$(y - h(x))^{T} R^{-1} (y - h(x)) = \\sum_{i=1}^{m} \\frac{(y_{i} - h_{i}(x))^{2}}{r_{i}^{2}}$$\nBecause both the determinant and the quadratic form decompose into products and sums, respectively, the multivariate PDF can be written as a product of $m$ independent univariate Gaussian PDFs:\n$$p(y \\mid x, R) = \\prod_{i=1}^{m} \\frac{1}{\\sqrt{2\\pi r_{i}^{2}}} \\exp\\left(-\\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}}\\right)$$\nThe log-likelihood, $\\ln p(y \\mid x, R)$, is the natural logarithm of this expression. The logarithm of a product is a sum of logarithms:\n$$\\ln p(y \\mid x, R) = \\sum_{i=1}^{m} \\ln\\left[ \\frac{1}{\\sqrt{2\\pi r_{i}^{2}}} \\exp\\left(-\\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}}\\right) \\right]$$\n$$\\ln p(y \\mid x, R) = \\sum_{i=1}^{m} \\left( \\ln\\left((2\\pi r_{i}^{2})^{-1/2}\\right) - \\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}} \\right)$$\nThis yields the log-likelihood as a sum of per-measurement contributions:\n$$\\ln p(y \\mid x, R) = \\sum_{i=1}^{m} \\left( -\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{(y_{i} - h_{i}(x))^{2}}{2r_{i}^{2}} \\right)$$\nThe contribution of each measurement $i$ to the log-likelihood, as a function of the residual $d_{i}(x) = y_{i} - h_{i}(x)$ and the standard deviation $r_{i}$, is therefore:\n$$-\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{d_{i}(x)^{2}}{2r_{i}^{2}}$$\n\nSecond, we derive the expected excess negative log-likelihood $\\Delta(\\alpha_{i})$. This quantity is defined as:\n$$\\Delta(\\alpha_{i}) \\equiv \\mathbb{E}_{y_{i} \\sim \\mathcal{N}(h_{i}(x_{\\mathrm{true}}), r_{i}^{2})}\\!\\Big[-\\ln p(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}) + \\ln p(y_{i}\\mid x_{\\mathrm{true}}, r_{i})\\Big]$$\nUsing the per-measurement contribution identified above, we can write the two log-likelihood terms for measurement $i$ at $x = x_{\\mathrm{true}}$. Let $\\delta_{i} = y_{i} - h_{i}(x_{\\mathrm{true}})$.\nThe true log-likelihood is $\\ln p(y_{i}\\mid x_{\\mathrm{true}}, r_{i}) = -\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{\\delta_{i}^{2}}{2r_{i}^{2}}$.\nThe misspecified log-likelihood is $\\ln p(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}) = -\\frac{1}{2}\\ln(2\\pi \\tilde{r}_{i}^{2}) - \\frac{\\delta_{i}^{2}}{2\\tilde{r}_{i}^{2}}$.\nThe expression inside the expectation is:\n$$-\\ln p(y_{i}\\mid x_{\\mathrm{true}}, \\tilde{r}_{i}) + \\ln p(y_{i}\\mid x_{\\mathrm{true}}, r_{i}) = \\left(\\frac{1}{2}\\ln(2\\pi \\tilde{r}_{i}^{2}) + \\frac{\\delta_{i}^{2}}{2\\tilde{r}_{i}^{2}}\\right) + \\left(-\\frac{1}{2}\\ln(2\\pi r_{i}^{2}) - \\frac{\\delta_{i}^{2}}{2r_{i}^{2}}\\right)$$\n$$= \\frac{1}{2}\\left(\\ln(2\\pi \\tilde{r}_{i}^{2}) - \\ln(2\\pi r_{i}^{2})\\right) + \\frac{\\delta_{i}^{2}}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right) = \\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{\\delta_{i}^{2}}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)$$\nWe now take the expectation with respect to the true distribution, where $y_{i} \\sim \\mathcal{N}(h_{i}(x_{\\mathrm{true}}), r_{i}^{2})$, which means the residual $\\delta_{i}$ is a random variable with distribution $\\delta_{i} \\sim \\mathcal{N}(0, r_{i}^{2})$. By linearity of expectation:\n$$\\Delta(\\alpha_{i}) = \\mathbb{E}\\left[\\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right)\\right] + \\mathbb{E}\\left[\\frac{\\delta_{i}^{2}}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)\\right]$$\nThe first term is a constant with respect to the random variable $\\delta_{i}$, and the constants in the second term can be factored out:\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{1}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)\\mathbb{E}[\\delta_{i}^{2}]$$\nThe expectation $\\mathbb{E}[\\delta_{i}^{2}]$ is the second moment of a Gaussian random variable $\\delta_{i}$ with mean $\\mu = 0$ and variance $\\sigma^{2} = r_{i}^{2}$. From the definition of variance, $\\mathrm{Var}(\\delta_{i}) = \\mathbb{E}[\\delta_{i}^{2}] - (\\mathbb{E}[\\delta_{i}])^{2}$, we have $\\mathbb{E}[\\delta_{i}^{2}] = \\mathrm{Var}(\\delta_{i}) + (\\mathbb{E}[\\delta_{i}])^{2} = r_{i}^{2} + 0^{2} = r_{i}^{2}$.\nSubstituting $\\mathbb{E}[\\delta_{i}^{2}] = r_{i}^{2}$ into the expression for $\\Delta(\\alpha_{i})$:\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}\\ln\\left(\\frac{\\tilde{r}_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{1}{2}\\left(\\frac{1}{\\tilde{r}_{i}^{2}} - \\frac{1}{r_{i}^{2}}\\right)r_{i}^{2}$$\nNow, we use the given relation $\\tilde{r}_{i} = \\alpha_{i} r_{i}$, which implies $\\tilde{r}_{i}^{2} = \\alpha_{i}^{2} r_{i}^{2}$. Since $\\alpha_{i} > 0$:\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}\\ln\\left(\\frac{\\alpha_{i}^{2} r_{i}^{2}}{r_{i}^{2}}\\right) + \\frac{1}{2}\\left(\\frac{r_{i}^{2}}{\\alpha_{i}^{2} r_{i}^{2}} - \\frac{r_{i}^{2}}{r_{i}^{2}}\\right) = \\frac{1}{2}\\ln(\\alpha_{i}^{2}) + \\frac{1}{2}\\left(\\frac{1}{\\alpha_{i}^{2}} - 1\\right)$$\nUsing the logarithm property $\\ln(a^{b}) = b \\ln(a)$, we simplify the first term:\n$$\\Delta(\\alpha_{i}) = \\frac{1}{2}(2\\ln(\\alpha_{i})) + \\frac{1}{2\\alpha_{i}^{2}} - \\frac{1}{2}$$\nThe final simplified expression is:\n$$\\Delta(\\alpha_{i}) = \\ln(\\alpha_{i}) + \\frac{1}{2\\alpha_{i}^{2}} - \\frac{1}{2}$$",
            "answer": "$$\\boxed{\\ln(\\alpha_{i}) + \\frac{1}{2 \\alpha_{i}^{2}} - \\frac{1}{2}}$$"
        },
        {
            "introduction": "Real-world observation errors are often not perfectly Gaussian, exhibiting heavier tails due to outliers or instrument malfunctions. This exercise introduces a powerful method for robust likelihood modeling using a Gaussian Mixture Model, which can capture such complex error structures. You will derive the Expectation-Maximization (EM) algorithm to estimate the model parameters, providing hands-on experience with a fundamental technique for fitting models with latent variables.",
            "id": "3397447",
            "problem": "Consider a one-dimensional inverse problem in which an observation model is given by $y_i = h_i(x) + \\varepsilon_i$ for $i = 1, \\dots, N$, where $x$ denotes the unknown state and $h_i(x)$ is a known forward model evaluated at $x$. Suppose that, at a current iterate $x^{(t)}$, residuals $r_i = y_i - h_i(x^{(t)})$ are modeled as independent draws from a two-component mixture of Gaussian distributions to capture a heavy-tailed error process: with probability $\\pi_1$ the residual is drawn from $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ (the \"inlier\" component) and with probability $\\pi_2$ the residual is drawn from $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ (the \"outlier\" component), where $\\pi_1 + \\pi_2 = 1$, and $\\sigma_1^2$ and $\\sigma_2^2$ are known positive constants with $\\sigma_1^2 \\ll \\sigma_2^2$. The means $\\mu_1$, $\\mu_2$ and weights $\\pi_1$, $\\pi_2$ are unknown and must be estimated. The goal is to model the likelihood function and derive estimation updates using Expectation-Maximization (EM).\n\nStarting from the fundamental definition of the observed-data likelihood for mixture models and the introduction of latent indicator variables $z_{ik} \\in \\{0,1\\}$ for $k \\in \\{1,2\\}$ with $z_{i1} + z_{i2} = 1$, derive the EM algorithm for this mixture error model:\n\n1. Derive the expectation step (E-step) responsibilities $\\gamma_{ik} = \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}]$, where $\\theta^{(t)} = (\\pi_1^{(t)}, \\pi_2^{(t)}, \\mu_1^{(t)}, \\mu_2^{(t)})$ denotes the current parameter estimates.\n2. Derive the maximization step (M-step) parameter updates for the mixture weights and means, assuming the variances $\\sigma_1^2$ and $\\sigma_2^2$ are fixed and known. Your derivation must use only core principles, including the product form of independent likelihoods, the mixture model definition, and Jensen’s inequality as needed for EM, avoiding any pre-stated shortcut formulas.\n3. Analyze the convergence properties of the EM algorithm for this model, explaining why the observed-data log-likelihood is non-decreasing across iterations and characterizing the nature of the fixed points. Discuss potential degeneracies and pathologies relevant to mixture models (such as label-switching symmetry, component collapse, and likelihood singularities) in the context of fixed versus free variances, and identify conditions that mitigate these issues in this specific setting.\n\nFinally, for an arbitrary residual $r_i$ and current parameter values $(\\pi_1, \\pi_2, \\mu_1, \\mu_2)$ with known $\\sigma_1^2, \\sigma_2^2$, provide the closed-form analytical expression for the responsibility of the inlier component, $\\gamma_{i1}$. This single expression is the only quantity you must return in your final answer. No rounding is required and no units are involved; express your answer in exact symbolic form.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n-   **Observation Model**: $y_i = h_i(x) + \\varepsilon_i$ for $i = 1, \\dots, N$.\n-   **Unknown State**: $x$.\n-   **Forward Model**: $h_i(x)$ is known.\n-   **Current Iterate**: $x^{(t)}$.\n-   **Residuals**: $r_i = y_i - h_i(x^{(t)})$.\n-   **Error Model**: Residuals $r_i$ are independent draws from a two-component mixture of Gaussian distributions.\n-   **Component 1 (Inlier)**: Drawn from $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ with probability $\\pi_1$.\n-   **Component 2 (Outlier)**: Drawn from $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ with probability $\\pi_2$.\n-   **Mixture Constraint**: $\\pi_1 + \\pi_2 = 1$.\n-   **Known Parameters**: Variances $\\sigma_1^2$ and $\\sigma_2^2$ are known, positive constants, with $\\sigma_1^2 \\ll \\sigma_2^2$.\n-   **Unknown Parameters to Estimate**: Means $\\mu_1, \\mu_2$ and mixture weights $\\pi_1, \\pi_2$.\n-   **Latent Variables**: Indicator variables $z_{ik} \\in \\{0,1\\}$ for $k \\in \\{1,2\\}$ with $z_{i1} + z_{i2} = 1$, where $z_{ik}=1$ if residual $r_i$ originated from component $k$.\n-   **Parameter Vector**: $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$. Current estimate is $\\theta^{(t)}$.\n-   **Task 1**: Derive the E-step responsibilities $\\gamma_{ik} = \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}]$.\n-   **Task 2**: Derive the M-step parameter updates for $\\pi_k$ and $\\mu_k$.\n-   **Task 3**: Analyze convergence and potential pathologies of the EM algorithm for this model.\n-   **Final Answer**: Provide the closed-form analytical expression for the inlier responsibility, $\\gamma_{i1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n-   **Scientifically Grounded**: The problem describes the application of the Expectation-Maximization (EM) algorithm to a Gaussian Mixture Model (GMM). This is a standard and fundamental technique in statistics, machine learning, and data assimilation for modeling heterogeneous data and robust estimation. The use of a heavy-tailed mixture model for residuals in an inverse problem is a scientifically sound approach to handle outliers.\n-   **Well-Posed**: The problem is well-defined. It provides a clear model specification and asks for the derivation of the specific steps of the EM algorithm, an analysis of its properties, and a final, specific mathematical expression. A unique and meaningful solution exists for each part of the derivation.\n-   **Objective**: The problem is stated in precise, objective, and formal mathematical language. It is free of any subjective or ambiguous terminology.\n\nThe problem does not exhibit any of the flaws listed in the instructions. It is scientifically sound, self-contained, and formalizable. The constraints (fixed variances) are clearly stated and are key to the analysis. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n***\n\nThe solution is derived in three parts as requested. Let $\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$ be the vector of parameters to be estimated. The variances $\\sigma_1^2$ and $\\sigma_2^2$ are known constants. We are given a set of $N$ residuals $R = \\{r_1, \\dots, r_N\\}$.\n\nThe probability density function (PDF) for a single residual $r_i$ under this mixture model is:\n$$p(r_i \\mid \\theta) = \\sum_{k=1}^2 \\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)$$\nwhere $\\mathcal{N}(r \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(r-\\mu)^2}{2\\sigma^2}\\right)$ is the Gaussian PDF.\n\nWe introduce latent indicator variables $z_{ik} \\in \\{0,1\\}$ such that $z_{ik}=1$ if $r_i$ was generated by component $k$, and $z_{ik}=0$ otherwise, with $\\sum_{k=1}^2 z_{ik}=1$. The set of all latent variables is $Z = \\{z_{ik}\\}_{i=1, k=1}^{N, 2}$. The complete-data likelihood for a single observation $(r_i, z_i)$ is $p(r_i, z_i \\mid \\theta) = \\prod_{k=1}^2 [\\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)]^{z_{ik}}$.\n\n### 1. E-Step: Derivation of Responsibilities\nThe Expectation step (E-step) computes the expectation of the latent variables given the observed data and the current parameter estimates $\\theta^{(t)}$. This expectation is the posterior probability that component $k$ was responsible for observation $r_i$, and is called the responsibility, denoted $\\gamma_{ik}$.\n\n$$ \\gamma_{ik} \\equiv \\mathbb{E}[z_{ik} \\mid r_i, \\theta^{(t)}] = P(z_{ik}=1 \\mid r_i, \\theta^{(t)}) $$\n\nUsing Bayes' theorem, this posterior probability is:\n$$ \\gamma_{ik} = \\frac{P(r_i \\mid z_{ik}=1, \\theta^{(t)}) P(z_{ik}=1 \\mid \\theta^{(t)})}{P(r_i \\mid \\theta^{(t)})} $$\n\nThe terms are identified as:\n-   $P(r_i \\mid z_{ik}=1, \\theta^{(t)}) = \\mathcal{N}(r_i \\mid \\mu_k^{(t)}, \\sigma_k^2)$ is the likelihood of observing $r_i$ given it came from component $k$.\n-   $P(z_{ik}=1 \\mid \\theta^{(t)}) = \\pi_k^{(t)}$ is the prior probability of choosing component $k$.\n-   $P(r_i \\mid \\theta^{(t)}) = \\sum_{j=1}^2 P(r_i \\mid z_{ij}=1, \\theta^{(t)}) P(z_{ij}=1 \\mid \\theta^{(t)}) = \\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(r_i \\mid \\mu_j^{(t)}, \\sigma_j^2)$ is the marginal likelihood of $r_i$, obtained by summing over all possible components.\n\nSubstituting these into the expression for $\\gamma_{ik}$ yields the E-step update rule:\n$$ \\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(r_i \\mid \\mu_k^{(t)}, \\sigma_k^2)}{\\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(r_i \\mid \\mu_j^{(t)}, \\sigma_j^2)} $$\nThese responsibilities are computed for each data point $i=1, \\dots, N$ and for each component $k=1, 2$.\n\n### 2. M-Step: Derivation of Parameter Updates\nThe Maximization step (M-step) updates the parameters $\\theta$ to maximize the expected complete-data log-likelihood, $Q(\\theta \\mid \\theta^{(t)})$, where the expectation is taken with respect to the posterior distribution of the latent variables computed in the E-step.\n\nThe complete-data log-likelihood for the entire dataset $R$ and latent variables $Z$ is:\n$$ \\ln p(R, Z \\mid \\theta) = \\ln \\left( \\prod_{i=1}^N \\prod_{k=1}^2 [\\pi_k \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)]^{z_{ik}} \\right) = \\sum_{i=1}^N \\sum_{k=1}^2 z_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2) \\right) $$\n\nThe function to be maximized in the M-step is $Q(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{Z \\mid R, \\theta^{(t)}}[\\ln p(R, Z \\mid \\theta)]$:\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^N \\sum_{k=1}^2 \\mathbb{E}[z_{ik}] \\left( \\ln \\pi_k + \\ln \\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2) \\right) $$\nwhere $\\mathbb{E}[z_{ik}]$ is the responsibility $\\gamma_{ik}$ computed in the E-step.\n$$ Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\left( \\ln \\pi_k - \\frac{1}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{(r_i - \\mu_k)^2}{2\\sigma_k^2} \\right) $$\n\nWe maximize this function with respect to $\\mu_k$ and $\\pi_k$.\n\n**Update for means $\\mu_k$**:\nWe find the maximum by setting the partial derivative of $Q$ with respect to $\\mu_k$ to zero. We only need to consider terms involving $\\mu_k$.\n$$ \\frac{\\partial Q}{\\partial \\mu_k} = \\frac{\\partial}{\\partial \\mu_k} \\sum_{i=1}^N \\gamma_{ik} \\left( -\\frac{(r_i - \\mu_k)^2}{2\\sigma_k^2} \\right) = \\sum_{i=1}^N \\gamma_{ik} \\left( \\frac{r_i - \\mu_k}{\\sigma_k^2} \\right) = 0 $$\nSince $\\sigma_k^2 > 0$, we can multiply it out:\n$$ \\sum_{i=1}^N \\gamma_{ik} (r_i - \\mu_k) = 0 \\implies \\sum_{i=1}^N \\gamma_{ik} r_i - \\mu_k \\sum_{i=1}^N \\gamma_{ik} = 0 $$\nSolving for $\\mu_k$ gives the update rule for the new estimate $\\mu_k^{(t+1)}$:\n$$ \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^N \\gamma_{ik} r_i}{\\sum_{i=1}^N \\gamma_{ik}} $$\nThis is a weighted average of the residuals, where the weights are the responsibilities. Let $N_k = \\sum_{i=1}^N \\gamma_{ik}$ be the effective number of points assigned to component $k$. Then $\\mu_k^{(t+1)} = \\frac{1}{N_k} \\sum_{i=1}^N \\gamma_{ik} r_i$.\n\n**Update for mixture weights $\\pi_k$**:\nWe maximize the terms in $Q$ involving $\\pi_k$, subject to the constraint $\\sum_{k=1}^2 \\pi_k = 1$. The relevant part of $Q$ is $\\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\ln \\pi_k$. We use a Lagrange multiplier $\\lambda$:\n$$ \\mathcal{L}(\\pi_1, \\pi_2, \\lambda) = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} \\ln \\pi_k + \\lambda \\left( \\sum_{k=1}^2 \\pi_k - 1 \\right) $$\nTaking the derivative with respect to $\\pi_k$ and setting to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = \\sum_{i=1}^N \\frac{\\gamma_{ik}}{\\pi_k} + \\lambda = 0 \\implies \\pi_k = -\\frac{\\sum_{i=1}^N \\gamma_{ik}}{\\lambda} = -\\frac{N_k}{\\lambda} $$\nSumming over $k$ and using the constraint:\n$$ \\sum_{k=1}^2 \\pi_k = 1 \\implies \\sum_{k=1}^2 \\left(-\\frac{N_k}{\\lambda}\\right) = 1 \\implies -\\frac{1}{\\lambda} \\sum_{k=1}^2 N_k = 1 $$\nWe know that $\\sum_{k=1}^2 N_k = \\sum_{k=1}^2 \\sum_{i=1}^N \\gamma_{ik} = \\sum_{i=1}^N \\sum_{k=1}^2 \\gamma_{ik} = \\sum_{i=1}^N 1 = N$.\nTherefore, $-\\frac{N}{\\lambda} = 1 \\implies \\lambda = -N$.\nSubstituting $\\lambda$ back into the expression for $\\pi_k$:\n$$ \\pi_k^{(t+1)} = -\\frac{N_k}{(-N)} = \\frac{N_k}{N} = \\frac{\\sum_{i=1}^N \\gamma_{ik}}{N} $$\nThis is the average responsibility for component $k$ over all data points.\n\n### 3. Convergence Properties and Pathologies\n**Convergence**: The EM algorithm produces a sequence of parameter estimates $\\theta^{(t)}$ such that the observed-data log-likelihood $\\ell(\\theta^{(t)}) = \\ln p(R|\\theta^{(t)}) = \\sum_{i=1}^N \\ln p(r_i|\\theta^{(t)})$ is non-decreasing at each iteration, i.e., $\\ell(\\theta^{(t+1)}) \\geq \\ell(\\theta^{(t)})$. This property arises from the relationship between the log-likelihood and the $Q$ function. The change in log-likelihood can be written as:\n$$ \\ell(\\theta) - \\ell(\\theta^{(t)}) = [Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})] + [\\text{KL}(p(Z|R,\\theta^{(t)})||p(Z|R,\\theta))] $$\nwhere KL is the Kullback-Leibler divergence. The M-step is designed to find $\\theta^{(t+1)}$ that maximizes $Q(\\theta|\\theta^{(t)})$, so the first term $[Q(\\theta^{(t+1)}|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)})]$ is non-negative. The KL divergence is always non-negative. Therefore, setting $\\theta = \\theta^{(t+1)}$ guarantees that $\\ell(\\theta^{(t+1)}) - \\ell(\\theta^{(t)}) \\geq 0$. As the log-likelihood is bounded from above (as shown below), the sequence $\\ell(\\theta^{(t)})$ must converge to a stationary point (a local maximum or saddle point) of the likelihood function.\n\n**Pathologies**:\n1.  **Label-switching symmetry**: The mixture model's likelihood function is invariant to permuting the component indices. That is, $p(r_i \\mid \\pi_1, \\mu_1, \\sigma_1^2; \\pi_2, \\mu_2, \\sigma_2^2) = p(r_i \\mid \\pi_2, \\mu_2, \\sigma_2^2; \\pi_1, \\mu_1, \\sigma_1^2)$. This means the likelihood surface has multiple, equivalent maxima. In this problem, the condition $\\sigma_1^2 \\ll \\sigma_2^2$ provides a natural way to break this symmetry by identifying component 1 as the \"inlier\" and component 2 as the \"outlier,\" thus making the solution interpretable.\n\n2.  **Likelihood Singularities (Component Collapse)**: This is a severe pathology in GMMs where the variances are also estimated. If one component's mean $\\mu_k$ becomes equal to a data point $r_i$, and its variance $\\sigma_k^2$ is allowed to approach zero, the term $\\mathcal{N}(r_i \\mid \\mu_k, \\sigma_k^2)$ tends to infinity, making the overall likelihood unbounded. This leads to a spurious, degenerate solution. **In this specific problem, this pathology is completely avoided because the variances $\\sigma_1^2$ and $\\sigma_2^2$ are specified as known, fixed positive constants.** With fixed positive variances, the Gaussian PDF $\\mathcal{N}(r \\mid \\mu, \\sigma^2)$ is bounded for all $r$ and $\\mu$. Consequently, the log-likelihood function is also bounded from above, and no singularities can occur.\n\n3.  **Degeneracies at the boundary**: The EM algorithm can converge to a solution where a mixture weight $\\pi_k$ approaches 0. This happens if a component explains the data so poorly that its responsibilities $\\gamma_{ik}$ become negligible for all $i$. While sometimes viewed as a pathology, this can also be an indication that a simpler model (with fewer components) is sufficient for the data. In this two-component model, if $\\pi_2 \\to 0$, it implies the data is adequately described by a single Gaussian distribution without a significant outlier population. Regularization or Bayesian priors on $\\pi_k$ can prevent this if a two-component solution is strongly desired.\n\nThe final requirement is the closed-form expression for the responsibility of the inlier component, $\\gamma_{i1}$, for an arbitrary residual $r_i$ and current parameters $(\\pi_1, \\pi_2, \\mu_1, \\mu_2)$. Based on the E-step derivation:\n$$ \\gamma_{i1} = \\frac{\\pi_1 \\mathcal{N}(r_i \\mid \\mu_1, \\sigma_1^2)}{\\pi_1 \\mathcal{N}(r_i \\mid \\mu_1, \\sigma_1^2) + \\pi_2 \\mathcal{N}(r_i \\mid \\mu_2, \\sigma_2^2)} $$\nSubstituting the full expression for the Gaussian PDF:\n$$ \\gamma_{i1} = \\frac{\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\pi_2 \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)} $$\nThe common factor of $\\frac{1}{\\sqrt{2\\pi}}$ in the numerator and denominator can be cancelled:\n$$ \\gamma_{i1} = \\frac{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\frac{\\pi_2}{\\sigma_2} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)} $$\nThis is the final analytical expression.",
            "answer": "$$\\boxed{\\frac{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right)}{\\frac{\\pi_1}{\\sigma_1} \\exp\\left(-\\frac{(r_i - \\mu_1)^2}{2\\sigma_1^2}\\right) + \\frac{\\pi_2}{\\sigma_2} \\exp\\left(-\\frac{(r_i - \\mu_2)^2}{2\\sigma_2^2}\\right)}}$$"
        }
    ]
}