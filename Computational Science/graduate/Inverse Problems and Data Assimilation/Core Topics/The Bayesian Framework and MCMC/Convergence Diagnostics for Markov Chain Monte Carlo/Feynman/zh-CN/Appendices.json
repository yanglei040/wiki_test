{
    "hands_on_practices": [
        {
            "introduction": "MCMC 采样器生成的是一个相关的样本序列，而非独立的样本序列。这种自相关性降低了我们拥有的“有效”独立样本数量，从而影响了后验估计的精度。本练习 () 提供了量化此效应的基础实践，通过给定的经验自相关数据，您将亲手计算有效样本量 ($ESS$) 和蒙特卡洛标准误 ($MCSE$)，这是评估任何 MCMC 链效率的基本指标。",
            "id": "3372636",
            "problem": "在一个线性观测模型的贝叶斯逆问题中，一个马尔可夫链蒙特卡洛（MCMC, Markov chain Monte Carlo）算法产生了一个标量后验泛函 $Y_{t} = g(\\theta_{t})$ 的平稳评估序列 $\\{Y_{t}\\}_{t=1}^{N}$，其中 $N = 50000$。假设马尔可夫链中心极限定理对 $\\{Y_{t}\\}$ 成立，并且已达到平稳性。给定经验边际方差估计值 $s^{2} = 2.25$ 和滞后阶数最高为 $L=10$ 的经验自相关 $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$：\n$$\n\\hat{\\rho}_{1} = 0.65,\\quad \\hat{\\rho}_{2} = 0.48,\\quad \\hat{\\rho}_{3} = 0.36,\\quad \\hat{\\rho}_{4} = 0.27,\\quad \\hat{\\rho}_{5} = 0.20,\\\\\n\\hat{\\rho}_{6} = 0.14,\\quad \\hat{\\rho}_{7} = 0.10,\\quad \\hat{\\rho}_{8} = 0.07,\\quad \\hat{\\rho}_{9} = 0.04,\\quad \\hat{\\rho}_{10} = 0.02.\n$$\n请仅从严平稳过程的自协方差函数定义，以及马尔可夫链中心极限定理中关于用零点谱密度来刻画样本均值渐近方差的结论出发，为后验均值 $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$ 的有效样本量（ESS）和蒙特卡洛标准误（MCSE）推导出一个相合的大样本估计量，该估计量通过有限滞后截断使用经验自相关 $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$。然后，使用上述数值，计算 ESS 和 MCSE 的值。最后，请用文字说明当经验自相关在中等和较大滞后阶数上存在噪声时，重叠分批均值（OBM, overlapping batch means）方法如何能够稳定 MCSE 的估计，并将您的说明与零频率处谱密度的相合性联系起来。\n\n请仅报告 MCSE 的值作为您的最终数值答案，并四舍五入到四位有效数字。",
            "solution": "该问题是有效的，因为它在科学上基于马尔可夫链蒙特卡洛（MCMC）方法的统计理论，是良定的（有足够的信息得到唯一解），并以客观、正式的语言表述。它代表了 MCMC 输出分析中的一个标准的、非平凡的任务。\n\n我们首先推导有效样本量（ESS）和蒙特卡洛标准误（MCSE）的估计量。设 $\\{Y_{t}\\}_{t=1}^{N}$ 是一个弱平稳序列，其均值为 $\\mu = \\mathbb{E}[Y_{t}]$，方差为 $\\sigma^{2} = \\text{Var}(Y_{t})$，自协方差函数为 $\\gamma_{k} = \\text{Cov}(Y_{t}, Y_{t+k})$。样本均值由 $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$ 给出。\n\n样本均值的方差为\n$$\n\\text{Var}(\\bar{Y}_{N}) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} Y_{t}\\right) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\text{Cov}(Y_{t}, Y_{s}) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\gamma_{t-s}.\n$$\n对于大样本容量 $N$，该方差可近似为\n$$\n\\text{Var}(\\bar{Y}_{N}) \\approx \\frac{1}{N} \\sum_{k=-(N-1)}^{N-1} \\left(1 - \\frac{|k|}{N}\\right) \\gamma_{k} \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\n马尔可夫链中心极限定理（CLT）指出，在适当条件下，\n$$\n\\sqrt{N}(\\bar{Y}_{N} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{\\text{asy}}^{2}),\n$$\n其中渐近方差 $\\sigma_{\\text{asy}}^{2}$ 由所有自协方差之和给出：\n$$\n\\sigma_{\\text{asy}}^{2} = \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\n这与过程的谱密度 $f(\\omega) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k} \\exp(-ik\\omega)$ 相关。在频率 $\\omega=0$ 处，我们有 $f(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k}$，这直接得出 $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$。\n\n根据平稳性，$\\gamma_{k} = \\gamma_{-k}$。我们可以使用自相关函数 $\\rho_{k} = \\gamma_{k}/\\gamma_{0} = \\gamma_{k}/\\sigma^{2}$ 将渐近方差重写为：\n$$\n\\sigma_{\\text{asy}}^{2} = \\gamma_{0} + \\sum_{k=1}^{\\infty} \\gamma_{k} + \\sum_{k=-\\infty}^{-1} \\gamma_{k} = \\gamma_{0} + 2\\sum_{k=1}^{\\infty} \\gamma_{k} = \\sigma^{2}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right).\n$$\n项 $\\tau = 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}$ 被称为积分自相关时间（IACT）。\n\n蒙特卡洛标准误（MCSE）是估计量 $\\bar{Y}_{N}$ 的标准差，对于大的 $N$ 来说，它是\n$$\n\\text{MCSE}(\\bar{Y}_{N}) = \\sqrt{\\text{Var}(\\bar{Y}_{N})} \\approx \\sqrt{\\frac{\\sigma_{\\text{asy}}^{2}}{N}} = \\sqrt{\\frac{\\sigma^{2}}{N}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right)}.\n$$\n有效样本量（ESS）定义为能够为样本均值产生相同方差的等效独立同分布（i.i.d.）样本的大小。对于大小为 $M$ 的 i.i.d. 样本，均值的方差是 $\\sigma^{2}/M$。将其与 MCMC 均值的方差相等，可得：\n$$\n\\frac{\\sigma^{2}}{\\text{ESS}} = \\text{Var}(\\bar{Y}_{N}) \\approx \\frac{\\sigma_{\\text{asy}}^{2}}{N} \\implies \\text{ESS} = \\frac{N\\sigma^{2}}{\\sigma_{\\text{asy}}^{2}} = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}}.\n$$\n为了构建相合估计量，我们用它们的经验对应量替换理论量。边际方差 $\\sigma^{2}$ 由样本方差 $s^{2}$ 估计。自相关 $\\rho_{k}$ 由样本自相关 $\\hat{\\rho}_{k}$ 估计。问题指定使用有限滞后截断，这意味着我们用一个截至滞后 $L$ 的有限和来近似自相关的无限和。由此得到的 IACT 估计量是：\n$$\n\\hat{\\tau}_{L} = 1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}.\n$$\n这导出了以下大样本估计量：\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{L}} = \\frac{N}{1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}}\n$$\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{s^{2}\\hat{\\tau}_{L}}{N}} = \\sqrt{\\frac{s^{2}}{N}\\left(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}\\right)}.\n$$\n现在，我们使用所提供的数据计算数值：$N = 50000$，$s^{2} = 2.25$，以及滞后阶数最高为 $L=10$ 的样本自相关。\n首先，我们对给定的经验自相关求和：\n$$\n\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 0.65 + 0.48 + 0.36 + 0.27 + 0.20 + 0.14 + 0.10 + 0.07 + 0.04 + 0.02 = 2.33.\n$$\n接下来，我们计算估计的 IACT：\n$$\n\\hat{\\tau}_{10} = 1 + 2\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 1 + 2(2.33) = 1 + 4.66 = 5.66.\n$$\n利用这个值，我们计算有效样本量：\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{10}} = \\frac{50000}{5.66} \\approx 8833.922.\n$$\n最后，我们计算蒙特卡洛标准误：\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{2.25}{50000 / 5.66}} = \\sqrt{\\frac{2.25 \\times 5.66}{50000}} = \\sqrt{\\frac{12.735}{50000}} = \\sqrt{0.0002547} \\approx 0.0159593.\n$$\n四舍五入到四位有效数字，MCSE 为 $0.01596$。\n\n最后，我们讨论重叠分批均值（OBM）方法的作用。渐近方差的简单有限滞后截断估计量是 $\\hat{\\sigma}_{\\text{asy, L}}^{2} = s^{2}(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}) = \\sum_{k=-L}^{L} \\hat{\\gamma}_{k}$。众所周知，该估计量具有高方差，因为样本自协方差 $\\hat{\\gamma}_{k}$ 本身就存在噪声，尤其是在中等到较大的滞后阶数 $k$ 上。直接对这些含噪项求和可能导致对 $\\sigma_{\\text{asy}}^{2}$ 的估计不稳定。如果负样本相关的和足够大，甚至可能得到负的估计值，这对于方差而言是无意义的。\n\nOBM 方法提供了一种更稳定的替代方案。它使用重叠观测批次的均值的方差来估计 $\\sigma_{\\text{asy}}^{2}$。对于批大小为 $b$ 的情况，如果 $b \\to \\infty$ 且 $b/N \\to 0$，则当 $N \\to \\infty$ 时，$\\sigma_{\\text{asy}}^{2}$ 的 OBM 估计量是相合的。关键的见解在于，OBM 估计量等价于一个在零频率处的基于核的谱密度估计量。具体来说，OBM 方差估计量可以表示为以下形式：\n$$\n\\hat{\\sigma}_{\\text{OBM}}^{2} \\approx \\sum_{k=-(b-1)}^{b-1} K\\left(\\frac{k}{b}\\right) \\hat{\\gamma}_{k},\n$$\n其中，$K(x) = 1 - |x|$（对于 $|x| \\le 1$）是 Bartlett（或三角）核。相比之下，有限滞后截断方法对应于使用矩形核，$K(x) = 1$（对于 $|x| \\le 1$）。\n\nBartlett 核通过降低含噪高滞后自协方差的贡献权重，稳定了对 $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$ 的估计。权重从滞后 $k=0$ 处的 $1$ 线性递减到滞后 $|k|=b$ 处的 $0$。与给予所有滞后直至截断点 $L$ 的项以同等权重的矩形核相比，这种权重递减降低了总估计量的方差。通过系统地减小求和中最不可靠项的影响，OBM 方法为零点谱密度提供了一个更稳健和统计上相合的估计，从而也为 MCSE 提供了更稳定的估计。",
            "answer": "$$\\boxed{0.01596}$$"
        },
        {
            "introduction": "虽然单链诊断很有用，但评估收敛性的一种更稳健的方法是从分散的初始点运行多个链，并验证它们是否都收敛到同一个分布。Gelman-Rubin 的潜尺度收缩因子 ($\\hat{R}$) 是完成此任务的黄金标准。本练习 () 将挑战您将此概念推广到多元情况，思考如何找到表现出最差收敛性的参数线性组合，这对于诊断复杂的高维模型至关重要。",
            "id": "3372662",
            "problem": "考虑一个数据同化中的逆问题，其参数矢量 $\\theta \\in \\mathbb{R}^p$ 的后验分布。您运行 $m$ 条独立的马尔可夫链，每条链的长度为 $n$，目标是这个后验分布。设 $\\bar{\\theta}_{\\cdot j}$ 表示链 $j \\in \\{1,\\dots,m\\}$ 的样本均值，$\\bar{\\theta}_{\\cdot\\cdot}$ 表示所有链的总均值。设 $S_j$ 为来自链 $j$ 的无偏链内样本协方差。定义链内协方差估计量 $W = \\frac{1}{m} \\sum_{j=1}^m S_j$ 和链间协方差估计量 $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})^\\top$。考虑混合协方差估计量 $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$。对于任何非零的 $a \\in \\mathbb{R}^p$，定义标量泛函 $y = a^\\top \\theta$ 和相应的单变量链内和链间方差估计量 $W_a = a^\\top W a$ 和 $B_a = a^\\top B a$，以及混合方差 $\\hat{V}_a = \\frac{n-1}{n} W_a + \\frac{1}{n} B_a$。\n\n从协方差矩阵的定义和对称矩阵的瑞利商出发，推断一个多变量潜在尺度缩减因子，该因子能捕捉所有线性泛函 $a^\\top \\theta$ 中最差的收敛性缺失情况。下列哪个陈述是正确的？选择所有适用项。\n\nA. 一个有效的多变量 $\\hat{R}$ 由 $\\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$ 给出，并且它等于 $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$，其中 $\\hat{R}(a^\\top \\theta) = \\sqrt{\\hat{V}_a / W_a}$ 是标量泛函 $a^\\top \\theta$ 的单变量潜在尺度缩减因子。\n\nB. 备选统计量 $\\sqrt{\\operatorname{tr}(\\hat{V}) / \\operatorname{tr}(W)}$ 对每个非零 $a$ 都构成 $\\hat{R}(a^\\top \\theta)$ 的上界，并且在任何可逆线性重参数化 $\\tilde{\\theta} = T \\theta$（其中 $T \\in \\mathbb{R}^{p \\times p}$ 可逆）下保持不变。\n\nC. 选项A中的统计量在任何可逆线性重参数化 $\\tilde{\\theta} = T \\theta$ 下保持不变，即用 $(T W T^\\top, T \\hat{V} T^\\top)$ 替换 $(W, \\hat{V})$ 不会改变 $\\lambda_{\\max}(W^{-1} \\hat{V})$ 的值。\n\nD. 统计量 $\\sqrt{ \\left( \\det \\hat{V} / \\det W \\right)^{1/p} }$ 等于 $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$，因此对混合不良的方向比选项A中的统计量更敏感。\n\nE. $W^{-1} B$ 的主特征向量确定了一个线性泛函 $a^\\star$，使得 $\\hat{R}(a^{\\star\\top} \\theta)$ 在所有 $a \\neq 0$ 中达到最大值。在数据同化中，这将诊断与混合最差的线性预测联系起来，并可以指导重参数化或预处理以改善该方向上的混合情况。",
            "solution": "首先将验证问题陈述的科学合理性和逻辑一致性。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 参数矢量：$\\theta \\in \\mathbb{R}^p$\n- 独立马尔可夫链的数量：$m$\n- 每条链的长度：$n$\n- 链 $j$ 的样本均值：$\\bar{\\theta}_{\\cdot j}$，对于 $j \\in \\{1,\\dots,m\\}$\n- 总样本均值：$\\bar{\\theta}_{\\cdot\\cdot}$\n- 来自链 $j$ 的无偏链内样本协方差：$S_j$\n- 链内协方差估计量：$W = \\frac{1}{m} \\sum_{j=1}^m S_j$\n- 链间协方差估计量：$B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})^\\top$\n- 混合协方差估计量：$\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$\n- 一个非零矢量：$a \\in \\mathbb{R}^p$\n- 一个标量泛函：$y = a^\\top \\theta$\n- 单变量方差估计量：$W_a = a^\\top W a$, $B_a = a^\\top B a$, $\\hat{V}_a = \\frac{n-1}{n} W_a + \\frac{1}{n} B_a$\n- 任务是推断一个多变量潜在尺度缩减因子，该因子能捕捉所有线性泛函 $a^\\top \\theta$ 中最差的收敛性缺失情况。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题描述了Gelman-Rubin收敛诊断（通常表示为 $\\hat{R}$ 或PSRF）的多变量扩展。链内协方差 $W$、链间协方差 $B$ 和混合方差估计量 $\\hat{V}$ 的定义在MCMC诊断的文献中是标准的（例如，Gelman等人的《贝叶斯数据分析》；Brooks和Gelman的《监测迭代模拟收敛的通用方法》）。这些概念是统计计算及其在数据同化等领域应用的基础。该问题在科学上是合理的。\n- **适定性：** 问题定义明确。它要求评估关于一个统计量的陈述，该统计量在所有可能的线性投影上最大化一个特定的比率。这对应于线性代数中一个适定的广义特征值问题，该问题有唯一且有意义的解。\n- **客观性：** 问题以精确的数学语言陈述，没有任何主观或模棱两可的术语。\n- **完整性和一致性：** 提供了所有必要的定义和变量。单变量和多变量统计量之间的关系清晰地阐明。整个设置在内部是一致和完整的。\n\n**步骤 3：结论和行动**\n该问题是有效的。这是一个关于MCMC收敛诊断中一个标准、高级主题的、表述良好的问题。现在将进行求解过程。\n\n### 推导与求解\n\n对于一个标量，单变量潜在尺度缩减因子（PSRF）由 $\\hat{R} = \\sqrt{\\frac{\\hat{V}}{W}}$ 给出，其中 $\\hat{V}$ 是边际后验方差的估计，而 $W$ 是平均链内方差。对于一个线性泛函 $y = a^\\top \\theta$，相应的方差估计量是 $W_a = a^\\top W a$ 和 $\\hat{V}_a = a^\\top \\hat{V} a$。因此，这个泛函的单变量PSRF为：\n$$ \\hat{R}(a^\\top \\theta) = \\sqrt{\\frac{\\hat{V}_a}{W_a}} = \\sqrt{\\frac{a^\\top \\hat{V} a}{a^\\top W a}} $$\n问题要求一个能够捕捉“最坏情况”收敛性缺失的多变量度量。这对应于找到使该比率最大化的线性泛函 $a^\\top \\theta$。我们寻求计算：\n$$ \\hat{R}_{\\mathrm{mv}}^2 = \\sup_{a \\neq 0} \\frac{a^\\top \\hat{V} a}{a^\\top W a} $$\n此表达式是针对对称矩阵对 $(\\hat{V}, W)$ 的一个广义瑞利商。假设链不是退化的，则链内协方差矩阵 $W$ 是正定的，因此是可逆的。该商的上确界由广义特征值问题 $\\hat{V} a = \\lambda W a$ 的最大特征值 $\\lambda_{\\max}$ 给出。这可以重写为一个标准的特征值问题：\n$$ W^{-1} \\hat{V} a = \\lambda a $$\n因此，上确界是 $\\lambda_{\\max}(W^{-1} \\hat{V})$。最坏情况的潜在尺度缩减因子是这个值的平方根：\n$$ \\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})} $$\n\n有了这个推导，我们现在可以评估每个选项。\n\n### 逐项分析\n\n**A. 一个有效的多变量 $\\hat{R}$ 由 $\\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$ 给出，并且它等于 $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$，其中 $\\hat{R}(a^\\top \\theta) = \\sqrt{\\hat{V}_a / W_a}$ 是标量泛函 $a^\\top \\theta$ 的单变量潜在尺度缩减因子。**\n\n如上文推导所示，代表所有一维投影 $a^\\top\\theta$ 上最坏情况（最大）PSRF 的多变量 PSRF 正是 $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$。我们证明了此上确界等于 $\\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$。该陈述是基于单变量 $\\hat{R}$ 定义“最坏情况”多变量诊断的直接且正确的结果。\n\n**结论：正确。**\n\n**B. 备选统计量 $\\sqrt{\\operatorname{tr}(\\hat{V}) / \\operatorname{tr}(W)}$ 对每个非零 $a$ 都构成 $\\hat{R}(a^\\top \\theta)$ 的上界，并且在任何可逆线性重参数化 $\\tilde{\\theta} = T \\theta$（其中 $T \\in \\mathbb{R}^{p \\times p}$ 可逆）下保持不变。**\n\n我们来检验第一部分：声称对所有 $a \\neq 0$ 都有 $\\frac{\\operatorname{tr}(\\hat{V})}{\\operatorname{tr}(W)} \\ge \\frac{a^\\top \\hat{V} a}{a^\\top W a}$。这通常是错误的。考虑一个对角情形，其中 $W = \\begin{pmatrix} 1  0 \\\\ 0  100 \\end{pmatrix}$ 和 $\\hat{V} = \\begin{pmatrix} 10  0 \\\\ 0  100 \\end{pmatrix}$。我们有 $\\operatorname{tr}(W) = 101$ 和 $\\operatorname{tr}(\\hat{V}) = 110$。迹的比率为 $\\frac{110}{101} \\approx 1.089$。现在，选择 $a = (1, 0)^\\top$。比率 $\\frac{a^\\top \\hat{V} a}{a^\\top W a} = \\frac{10}{1} = 10$。由于 $10 > 1.089$，迹的比率并不为瑞利商提供上界。\n\n现在，我们来检验第二部分：在重参数化 $\\tilde{\\theta} = T \\theta$ 下的不变性。协方差矩阵变换为 $\\tilde{W} = T W T^\\top$ 和 $\\tilde{\\hat{V}} = T \\hat{V} T^\\top$。该统计量变为 $\\sqrt{\\operatorname{tr}(\\tilde{\\hat{V}}) / \\operatorname{tr}(\\tilde{W})} = \\sqrt{\\operatorname{tr}(T \\hat{V} T^\\top) / \\operatorname{tr}(T W T^\\top)}$。通常情况下，$\\operatorname{tr}(AXA^\\top) \\neq \\operatorname{tr}(X)$。例如，如果我们对单个参数进行缩放，例如 $T = \\mathrm{diag}(10, 1, \\dots, 1)$，迹将显著改变，比率也同样会改变。因此，该统计量不是不变的。\n\n**结论：不正确。**\n\n**C. 选项A中的统计量在任何可逆线性重参数化 $\\tilde{\\theta} = T \\theta$ 下保持不变，即用 $(T W T^\\top, T \\hat{V} T^\\top)$ 替换 $(W, \\hat{V})$ 不会改变 $\\lambda_{\\max}(W^{-1} \\hat{V})$ 的值。**\n\n在重参数化 $\\tilde{\\theta} = T \\theta$ 下，新的协方差矩阵为 $\\tilde{W} = TWT^\\top$ 和 $\\tilde{\\hat{V}} = T\\hat{V}T^\\top$。应用于重参数化问题的选项A中的统计量是 $\\lambda_{\\max}((\\tilde{W})^{-1} \\tilde{\\hat{V}})$。我们来计算这个矩阵：\n$$ (\\tilde{W})^{-1} \\tilde{\\hat{V}} = (T W T^\\top)^{-1} (T \\hat{V} T^\\top) = ((T^\\top)^{-1} W^{-1} T^{-1}) (T \\hat{V} T^\\top) = (T^\\top)^{-1} W^{-1} (T^{-1} T) \\hat{V} T^\\top = (T^\\top)^{-1} (W^{-1} \\hat{V}) T^\\top $$\n设 $M = W^{-1} \\hat{V}$ 且 $P = T^\\top$。新矩阵是 $P^{-1} M P$。这是对原始矩阵 $M$ 的一个相似变换。相似变换的一个基本性质是它们保持特征值不变。因此，$(\\tilde{W})^{-1} \\tilde{\\hat{V}}$ 的特征值与 $W^{-1} \\hat{V}$ 的特征值相同。因此，它们的最大特征值相等：$\\lambda_{\\max}((\\tilde{W})^{-1} \\tilde{\\hat{V}}) = \\lambda_{\\max}(W^{-1} \\hat{V})$。该统计量确实在线性重参数化下是不变的，这是收敛诊断的一个理想属性。\n\n**结论：正确。**\n\n**D. 统计量 $\\sqrt{ \\left( \\det \\hat{V} / \\det W \\right)^{1/p} }$ 等于 $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$，因此对混合不良的方向比选项A中的统计量更敏感。**\n\n第一个论点是 $\\sqrt{(\\det(\\hat{V}) / \\det(W))^{1/p}} = \\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$。从选项A我们知道右边等于 $\\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$。因此，该论点等价于 $(\\det(\\hat{V}) / \\det(W))^{1/p} = \\lambda_{\\max}(W^{-1} \\hat{V})$。\n使用性质 $\\det(AB) = \\det(A)\\det(B)$，我们有 $\\det(\\hat{V}) / \\det(W) = \\det(\\hat{V}) \\det(W^{-1}) = \\det(W^{-1}\\hat{V})$。\n设 $M = W^{-1}\\hat{V}$。它的行列式是其特征值的乘积：$\\det(M) = \\prod_{i=1}^p \\lambda_i$。\n该论点变为 $(\\prod_{i=1}^p \\lambda_i)^{1/p} = \\lambda_{\\max}$。左边是矩阵 $M$ 特征值的几何平均值。一组非负数的几何平均值小于或等于它们的最大值，等号仅在所有数都相同时成立。通常情况下，特征值不会相同，所以 $(\\prod_{i=1}^p \\lambda_i)^{1/p} \\le \\lambda_{\\max}$。陈述的第一部分是错误的。\n因为第一部分是错误的，所以第二部分（“因此更敏感”）是无意义的。事实上，通过对所有特征方向进行平均，基于行列式的统计量通常比最大特征值统计量对单个混合不良方向的敏感度*更低*。\n\n**结论：不正确。**\n\n**E. $W^{-1} B$ 的主特征向量确定了一个线性泛函 $a^\\star$，使得 $\\hat{R}(a^{\\star\\top} \\theta)$ 在所有 $a \\neq 0$ 中达到最大值。在数据同化中，这将诊断与混合最差的线性预测联系起来，并可以指导重参数化或预处理以改善该方向上的混合情况。**\n\n最大化 $\\hat{R}(a^\\top \\theta)$ 的向量 $a^\\star$ 是对应于 $W^{-1}\\hat{V}$ 最大特征值的特征向量。我们来研究 $W^{-1} \\hat{V}$ 和 $W^{-1} B$ 特征向量之间的关系。\n我们有定义 $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$。用 $W^{-1}$ 左乘得到：\n$$ W^{-1} \\hat{V} = \\frac{n-1}{n} W^{-1}W + \\frac{1}{n} W^{-1}B = \\frac{n-1}{n} I + \\frac{1}{n} W^{-1}B $$\n其中 $I$ 是单位矩阵。如果 $v$ 是 $W^{-1}B$ 的一个特征向量，其特征值为 $\\mu$，那么 $(W^{-1}B)v = \\mu v$。由此可得：\n$$ (W^{-1}\\hat{V})v = \\left(\\frac{n-1}{n} I + \\frac{1}{n} W^{-1}B\\right)v = \\frac{n-1}{n}v + \\frac{1}{n}\\mu v = \\left(\\frac{n-1+\\mu}{n}\\right)v $$\n这表明 $v$ 也是 $W^{-1}\\hat{V}$ 的一个特征向量，其特征值为 $\\lambda = \\frac{n-1+\\mu}{n}$。由于这个关系是线性的且随 $\\mu$ 递增，对应于 $W^{-1} B$ 最大特征值的特征向量与对应于 $W^{-1}\\hat{V}$ 最大特征值的特征向量是相同的。因此，$W^{-1} B$ 的主特征向量确实确定了最差收敛方向 $a^\\star$。\n\n陈述的第二部分提供了这一发现的正确解释和应用。识别参数的“混合最差”的线性组合（$a^{\\star\\top} \\theta$）对于诊断MCMC性能非常有价值。这个方向指出了参数空间难以探索的具体方式（例如，由于高的后验相关性）。这些信息可以用来改进采样器，例如通过重新参数化模型以减少这些相关性，或者为MCMC算法构建更有效的提议机制（预处理）。这是多变量PSRF诊断在数据同化等复杂应用中的一个关键实际用途。\n\n**结论：正确。**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "通用诊断工具如果能根据具体问题的结构进行定制，其威力将更加强大。在科学反演问题中，数据对不同参数的约束程度并不相同；参数空间中的某些方向受到数据的强烈约束，而另一些方向则主要由先验决定。本练习 () 提供了一个高级的动手挑战：您将实现一个敏感性加权的 $\\hat{R}$ 诊断，它优先考虑数据信息量大的方向上的收敛性，从而为实际应用中的采样器性能提供更具物理意义和实用价值的评估。",
            "id": "3372654",
            "problem": "给定来自马尔可夫链蒙特卡洛（MCMC）算法的 $m$ 条平行链，该算法旨在解决一个由可微正向算子 $\\mathcal{F}:\\mathbb{R}^d \\to \\mathbb{R}^p$、观测值 $y \\in \\mathbb{R}^p$ 和观测噪声协方差 $R \\in \\mathbb{R}^{p \\times p}$ 控制的贝叶斯逆问题。令 $S = \\partial \\mathcal{F}/\\partial \\theta \\in \\mathbb{R}^{p \\times d}$ 表示在固定线性化点处正向灵敏度的雅可比矩阵。考虑参数空间 $\\mathbb{R}^d$ 中的 $m$ 条长度为 $n$ 的链，记为 $\\{\\theta_{j,t}\\}_{j=1,\\dots,m;\\, t=1,\\dots,n}$，其中 $\\theta_{j,t} \\in \\mathbb{R}^d$。势标度缩减因子（Potential Scale Reduction Factor, PSRF）是一种单变量收敛诊断方法，它根据标量目标在各链间的基本样本均值和方差摘要来定义。其多变量扩展可以通过向 $\\theta$ 的线性泛函上投影来构建。\n\n仅从以下基本要素出发：\n- 标量序列的单变量链式样本均值和无偏样本方差的定义。\n- 标准的单变量势标度缩减因子（PSRF），它使用标量投影的链间方差和链内方差，将估计的边际方差与链内方差进行比较。\n- 观测模型的线性高斯局部近似 $y \\approx \\mathcal{F}(\\theta^\\star) + S(\\theta - \\theta^\\star) + \\varepsilon$（其中 $\\varepsilon \\sim \\mathcal{N}(0,R)$），该近似意味着由半正定矩阵 $J = S^\\top R^{-1} S \\in \\mathbb{R}^{d \\times d}$ 导出的关于参数扰动的局部信息度量。\n\n推导一个标量收敛诊断方法，该方法强调沿观测值 $y$ 提供信息最多的方向上的收敛性。具体而言，您必须：\n- 论证将多变量链投影到信息矩阵 $J$ 的特征方向 $v_i \\in \\mathbb{R}^d$ 上，并沿每个这样的方向诊断单变量 PSRF 的合理性。\n- 论证根据 $J$ 相应的特征值 $\\lambda_i \\ge 0$ 对方向性 PSRF 值进行加权，从而将它们组合成单个标量的合理性，以此优先考虑信息量更大的方向。\n- 论证对退化情况的合​​理处理，包括某些 $\\lambda_i = 0$ 的情况以及在投影方向上链内方差为零等数值问题。\n\n对于下面的每个测试用例，您的程序必须将最终的灵敏度加权 PSRF 诊断实现为 $(\\{\\theta_{j,t}\\}, S, R)$ 的确定性函数。所有计算都必须使用实数算术执行。如果 $J$ 的所有特征值都为 $0$（即 $J$ 是零矩阵），则按照约定将加权诊断定义为 $1$，因为此时 $y$ 不通过 $S$ 提供关于 $\\theta$ 的任何局部信息。\n\n测试数据生成定义：\n- 对于任何整数 $n \\ge 1$，定义时间索引 $t \\in \\{0,1,\\dots,n-1\\}$。\n- 对于任何整数 $m \\ge 1$，定义链索引 $j \\in \\{0,1,\\dots,m-1\\}$。\n- 对于任何维度 $d \\ge 1$，$\\theta_{j,t} \\in \\mathbb{R}^d$。\n- 令 $\\varphi_j$ 表示一个相位，定义为 $\\varphi_j = j \\pi / 6$。\n- 对于每个测试，链都通过具有指定均值和振幅的正弦公式确定性地生成，从而确保围绕特定于链的均值的平稳性。\n\n对于标量投影 $x_{j,t} \\in \\mathbb{R}$ 的单变量 PSRF 成分：\n- 对于每条链 $j$，样本均值 $\\mu_j = \\frac{1}{n}\\sum_{t=1}^n x_{j,t}$ 和无偏样本方差 $s_j^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_{j,t} - \\mu_j)^2$。\n- 合并链内方差 $W = \\frac{1}{m}\\sum_{j=1}^m s_j^2$。\n- 链间方差 $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\mu_j - \\bar{\\mu})^2$，其中 $\\bar{\\mu} = \\frac{1}{m}\\sum_{j=1}^m \\mu_j$。\n- 单变量 PSRF 使用这些量将估计的边际方差与 $W$ 进行比较。\n\n数值稳定性要求：\n- 在形成任何涉及链内方差的比率时，在分母上添加一个稳定器 $\\varepsilon = 10^{-12}$。\n- 如果在某个投影中，链间方差和链内方差在数值上均为零，则将该投影中的 PSRF 视为 $1$。\n\n测试套件：\n- 在所有测试中，链的数量为 $m = 3$，每条链的迭代次数为 $n = 200$。参数维度为 $d = 2$，观测维度为 $p = 2$。观测噪声协方差为 $R = I_2$，即 $2 \\times 2$ 的单位矩阵。\n- 测试 $1$（收敛良好，第一个方向信息量大）：\n  - 灵敏度矩阵 $S_1 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$。\n  - 对于每个链索引 $j \\in \\{0,1,2\\}$ 和时间 $t \\in \\{0,1,\\dots,199\\}$：\n    - 第一个分量：$\\theta_{j,t}^{(1)} = 0 + 0.2 \\sin\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n    - 第二个分量：$\\theta_{j,t}^{(2)} = 0 + 0.2 \\cos\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n- 测试 $2$（在信息量最大的方向上收敛性差）：\n  - 灵敏度矩阵 $S_2 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$。\n  - 沿第一个分量的链特定均值为 $\\mu^{(1)}_0 = -0.3$，$\\mu^{(1)}_1 = 0.0$，$\\mu^{(1)}_2 = 0.3$。沿第二个分量，所有链的均值均为 $0$。\n  - 对于每个链索引 $j \\in \\{0,1,2\\}$ 和时间 $t \\in \\{0,1,\\dots,199\\}$：\n    - 第一个分量：$\\theta_{j,t}^{(1)} = \\mu^{(1)}_j + 0.2 \\sin\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n    - 第二个分量：$\\theta_{j,t}^{(2)} = 0 + 0.2 \\cos\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n- 测试 $3$（无信息量的灵敏度）：\n  - 灵敏度矩阵 $S_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$。\n  - 链的生成方式与测试 $1$ 完全相同。\n\n您的任务：\n- 实现一个程序，该程序仅使用上述输入为每个测试计算灵敏度加权 PSRF 诊断，结果为单个标量。\n- 每个测试的结果必须是一个浮点数。报告最终结果时，将每个数字四舍五入到恰好 $6$ 位小数。\n- 最终输出格式：您的程序应生成单行输出，其中包含三个结果，以逗号分隔并用方括号括起来，例如 $[r_1,r_2,r_3]$，其中每个 $r_i$ 显示小数点后恰好 $6$ 位数字。",
            "solution": "该问题要求推导并实现一个灵敏度加权的势标度缩减因子（PSRF），作为多变量马尔可夫链蒙特卡洛（MCMC）链的标量收敛诊断。该推导必须基于第一性原理和所提供的信息进行论证。\n\n设 MCMC 链表示为 $\\{\\theta_{j,t} \\in \\mathbb{R}^d\\}_{j=1,\\dots,m; t=1,\\dots,n}$，代表在 $d$ 维参数空间中 $m$ 条长度为 $n$ 的链。我们已知正向模型的雅可比矩阵 $S \\in \\mathbb{R}^{p \\times d}$ 和观测噪声协方差 $R \\in \\mathbb{R}^{p \\times p}$。\n\n按照要求，推导过程分为三个部分：论证投影方向选择的合理性，论证方向性诊断加权组合的合理性，以及论证退化情况处理的合理性。\n\n### 投影到信息矩阵 $J$ 特征方向的合理性论证\n\n此方法的基础是贝叶斯逆问题中后验分布的局部行为。我们从给定的正向模型 $\\mathcal{F}$ 在线性化点 $\\theta^\\star$ 附近的线性高斯近似开始：\n$$y \\approx \\mathcal{F}(\\theta^\\star) + S(\\theta - \\theta^\\star) + \\varepsilon, \\quad \\text{with } \\varepsilon \\sim \\mathcal{N}(0,R)$$\n给定观测值 $y$ 时参数 $\\theta$ 的似然由残差噪声的分布确定。假设该近似是充分的，则负对数似然函数 $\\mathcal{L}(\\theta) = -\\log p(y|\\theta)$ 在 $\\theta$ 上是局部二次的：\n$$ \\mathcal{L}(\\theta) \\approx \\frac{1}{2} (S(\\theta - \\theta^\\star) - \\delta y)^\\top R^{-1} (S(\\theta - \\theta^\\star) - \\delta y) + \\text{const.} $$\n其中 $\\delta y = y - \\mathcal{F}(\\theta^\\star)$ 是线性化点处的数据残差。该负对数似然函数关于 $\\theta$ 的黑塞矩阵（Hessian）由下式给出：\n$$ H_{\\mathcal{L}} = \\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta^2} = S^\\top R^{-1} S $$\n这个矩阵恰好是问题陈述中提供的信息矩阵 $J$。在贝叶斯背景下，后验分布为 $p(\\theta|y) \\propto p(y|\\theta)p(\\theta)$。后验分布在众数附近的局部形状主要由似然决定。矩阵 $J$ 量化了负对数后验的曲率（假设先验局部平坦），这对应于后验的局部高斯近似的精度（协方差的逆）。\n\n信息矩阵 $J$ 是对称半正定的。它可以进行特征分解 $J = V \\Lambda V^\\top$，其中 $V = [v_1, \\dots, v_d]$ 是一个正交矩阵，其列是特征向量 $v_i$，而 $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$ 是相应非负特征值 $\\lambda_i \\ge 0$ 组成的对角矩阵。\n\n特征向量 $v_i$ 构成了参数空间 $\\mathbb{R}^d$ 的一个标准正交基。相关的特征值 $\\lambda_i$ 衡量了数据 $y$ 沿这些方向为参数扰动提供的信息量。大的特征值 $\\lambda_i$ 意味着后验在 $v_i$ 方向上受到严格约束，即数据信息量很大。相反，小或零的特征值 $\\lambda_i$ 意味着数据在 $v_i$ 方向上几乎不提供关于参数的信息，并且该方向上的后验方差很大（即由先验决定）。\n\nMCMC 链的收敛可能是各向异性的。至关重要的是，链需要在数据提供充分信息的方向上收敛，因为这些方向定义了后验分布景观的关键特征。沿 $J$ 的每个特征方向 $v_i$ 分别诊断收敛性，使我们能够将问题解耦为一组一维分析，每个分析对应一个具有特定、可量化信息含量的方向。与检查沿任意轴（例如，可能与高、低后验不确定性方向错位的原始参数坐标）的收敛性相比，这是一种更优的策略。因此，将 $d$ 维链 $\\theta_{j,t}$ 投影到每个特征向量 $v_i$ 上以获得标量链 $x_{j,t}^{(i)} = v_i^\\top \\theta_{j,t}$，是一种在统计学和科学上都有原则的多变量收敛诊断方法。\n\n### 方向性 PSRF 加权组合的合理性论证\n\n投影链之后，我们为 $d$ 个标量值链集 $\\{x_{j,t}^{(i)}\\}$ 中的每一个计算一个单变量 PSRF，记为 $\\hat{R}_i$。这将产生一个诊断值向量 $[\\hat{R}_1, \\dots, \\hat{R}_d]$。为了创建一个单一的标量摘要，我们必须组合这些值。\n\n简单的平均会同等对待所有方向，这是不合适的。如前所述，在数据强力约束的方向上，收敛更为关键。而简单的最大值会对信息量不足方向上的慢混合过于敏感，这在实践中可能不是一个问题。\n\n信息矩阵 $J$ 的特征值 $\\lambda_i$ 为每个方向 $v_i$ 的“重要性”提供了一个自然且定量的度量。它们是非负的，并且与数据沿相应特征向量贡献的信息量成正比。因此，使用这些特征值作为权重来构建组合诊断是合乎逻辑的。\n\n我们将灵敏度加权 PSRF，$\\hat{R}_w$，定义为方向性 PSRF 的加权平均值，其中权重是特征值：\n$$ \\hat{R}_w = \\frac{\\sum_{i=1}^d \\lambda_i \\hat{R}_i}{\\sum_{i=1}^d \\lambda_i} $$\n这种表述确保了整体诊断对于在相应特征值 $\\lambda_i$ 较大的方向上缺乏收敛（即较大的 $\\hat{R}_i$）最为敏感。在特征值小或为零的方向上收敛性差，对 $\\hat{R}_w$ 的影响很小。这与确保链已充分探索由观测数据识别出的高后验概率区域的目标相一致。\n\n### 退化情况和数值问题的处理\n\n所提出的表述需要仔细处理几种特殊情况。\n\n1.  **零信息方向（$\\lambda_i = 0$）：** 如果一个特征值 $\\lambda_i$ 为零，那么它对 $\\hat{R}_w$ 表达式的分子和分母的贡献都为零。因此，该诊断自然而然地、正确地忽略了根据线性近似完全不受数据约束的方向。\n\n2.  **任何方向均无信息（$J=0$）：** 如果 $S=0$ 或由于其他原因 $J$ 是零矩阵，则所有特征值均为零（对所有 $i$ 都有 $\\lambda_i=0$）。在这种情况下，分母 $\\sum_i \\lambda_i$ 为零，加权平均值未定义。这种情况表明数据不提供关于任何参数方向的局部信息。按照问题陈述中的规定，我们采用诊断值为 $1$ 的约定。这是合理的，因为 PSRF 值为 $1$ 表示理想的收敛；如果没有信息来指导收敛，就没有数据驱动的收敛可供诊断，因此从数据的角度来看，我们可以认为它是平凡“收敛”的。\n\n3.  **零链内方差（$W_i=0$）：** 标准的 PSRF 计算涉及除以合并链内方差 $W_i$。如果在给定的投影 $i$ 中所有链都是静态的或已塌缩到单个点，则 $W_i$ 可能为零，导致除以零。问题指定在分母上添加一个小的稳定器 $\\varepsilon = 10^{-12}$：\n    $$ \\hat{R}_i = \\sqrt{\\frac{var_{est}}{W_i + \\varepsilon}} \\quad \\text{其中} \\quad var_{est} = \\frac{n-1}{n}W_i + \\frac{1}{n}B_i $$\n    这可以防止数值计算失败。如果 $W_i$ 非常小而链间方差 $B_i$ 不小，$\\hat{R}_i$ 将变得非常大，从而正确地指示出严重的收敛问题（分离的、不移动的链）。\n\n4.  **零链间和链内方差（$B_i=0$ 和 $W_i=0$）：** 如果对于某个投影 $i$，$B_i$ 和 $W_i$ 在数值上都为零，这意味着所有链都已收敛到完全相同的点。这是一种理想的收敛状态。问题指定在这种情况下，我们应该设置 $\\hat{R}_i = 1$。这个约定与 PSRF 的解释一致，并避免了稳定化公式可能产生的数值伪影，该公式会得出 $\\sqrt{\\frac{((n-1)/n)W_i}{W_i+\\varepsilon}} \\approx \\sqrt{(n-1)/n} \\approx 1$。\n\n这套完整的论证和规则为灵敏度加权 PSRF 诊断提供了稳健且理论上合理的依据。\n\n### 实现算法\n\n基于以上内容，对于给定的测试用例 $(\\{\\theta_{j,t}\\}, S, R)$ 的算法如下：\n1.  读取输入数据：MCMC 链 $\\{\\theta_{j,t}\\}$、灵敏度矩阵 $S$ 和观测噪声协方差 $R$。参数 $m$、$n$ 和 $d$ 从链数组的维度推断得出。\n2.  计算信息矩阵 $J = S^\\top R^{-1} S$。\n3.  计算 $J$ 的特征值 $\\lambda_i$ 和相应的特征向量 $v_i$。\n4.  如果绝对特征值的总和 $\\sum_i |\\lambda_i|$ 接近于零（即 $J \\approx 0$），则返回值 $1.0$ 并终止。\n5.  初始化 `weighted_psrf_sum = 0.0` 和 `lambda_sum = 0.0`。\n6.  对于每个特征向量 $v_i$：\n    a. 设 $\\lambda_i$ 为相应的特征值。如果 $\\lambda_i \\le 0$，则跳过此方向，因为它不包含信息。\n    b. 将链投影到该方向上：对所有 $j,t$，有 $x_{j,t}^{(i)} = v_i^\\top \\theta_{j,t}$。\n    c. 为此投影的标量数据计算单变量 PSRF，$\\hat{R}_i$：\n        i.   对于每条链 $j$，计算其均值 $\\mu_j$ 和无偏样本方差 $s_j^2$。\n        ii.  计算合并链内方差 $W_i = \\frac{1}{m}\\sum_j s_j^2$。\n        iii. 计算链间方差 $B_i = \\frac{n}{m-1}\\sum_j(\\mu_j - \\bar{\\mu})^2$，其中 $\\bar{\\mu}=\\frac{1}{m}\\sum_j\\mu_j$。\n        iv.  如果 $W_i  \\varepsilon$ 且 $B_i  \\varepsilon$（其中 $\\varepsilon=10^{-12}$），则设置 $\\hat{R}_i = 1.0$。\n        v.   否则，计算估计的边际方差 $var_{est} = \\frac{n-1}{n}W_i + \\frac{1}{n}B_i$，并计算 $\\hat{R}_i = \\sqrt{var_{est} / (W_i + \\varepsilon)}$。\n    d. 更新加权和：`weighted_psrf_sum += \\lambda_i * \\hat{R}_i`。\n    e. 更新权重和：`lambda_sum += \\lambda_i`。\n7.  如果 `lambda_sum` 为零（例如，所有特征值均为非正值），则返回 $1.0$。否则，计算最终诊断 $\\hat{R}_w = \\text{weighted\\_psrf\\_sum} / \\text{lambda\\_sum}$。\n8.  返回 $\\hat{R}_w$。\n然后将此过程应用于每个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_weighted_psrf(thetas, S, R, epsilon=1e-12):\n    \"\"\"\n    Computes the sensitivity-weighted Potential Scale Reduction Factor (PSRF).\n\n    Args:\n        thetas (np.ndarray): MCMC chains of shape (m, n, d).\n        S (np.ndarray): Sensitivity matrix of shape (p, d).\n        R (np.ndarray): Observation noise covariance matrix of shape (p, p).\n        epsilon (float): Numerical stabilizer.\n\n    Returns:\n        float: The scalar sensitivity-weighted PSRF value.\n    \"\"\"\n    m, n, d = thetas.shape\n    \n    # 1. Compute the information matrix J = S^T R^-1 S\n    try:\n        R_inv = np.linalg.inv(R)\n    except np.linalg.LinAlgError:\n        return np.nan\n\n    J = S.T @ R_inv @ S\n\n    # 2. Eigen-decomposition of J\n    lambdas, V = np.linalg.eigh(J)\n\n    # 3. Handle special case where J is the zero matrix\n    if np.sum(np.abs(lambdas))  epsilon:\n        return 1.0\n\n    weighted_psrf_sum = 0.0\n    lambda_sum = 0.0\n\n    # 4. Loop over each eigendirection\n    for i in range(d):\n        lambda_i = lambdas[i]\n        \n        if lambda_i = epsilon:\n            continue\n\n        v_i = V[:, i]\n\n        # 5. Project the chains onto the eigenvector v_i\n        projected_chains = np.dot(thetas, v_i)\n\n        # 6. Compute statistics for the projected scalar chains\n        chain_means = np.mean(projected_chains, axis=1)\n        chain_vars = np.var(projected_chains, axis=1, ddof=1)\n\n        W_i = np.mean(chain_vars)\n        \n        if m > 1:\n            mu_bar = np.mean(chain_means)\n            B_i = (n / (m - 1)) * np.sum((chain_means - mu_bar)**2)\n        else:\n            B_i = 0.0\n\n        # 7. Compute the univariate PSRF (R_hat) for this projection\n        if W_i  epsilon and B_i  epsilon:\n            R_hat_i = 1.0\n        else:\n            var_est = ((n - 1) / n) * W_i + (1 / n) * B_i\n            psrf_squared = var_est / (W_i + epsilon)\n            \n            if psrf_squared  0:\n                R_hat_i = 0.0\n            else:\n                R_hat_i = np.sqrt(psrf_squared)\n\n        # 8. Update the sums for the weighted average\n        weighted_psrf_sum += lambda_i * R_hat_i\n        lambda_sum += lambda_i\n\n    # 9. Compute the final weighted PSRF\n    if lambda_sum  epsilon:\n        return 1.0\n\n    return weighted_psrf_sum / lambda_sum\n\ndef solve():\n    m = 3\n    n = 200\n    d = 2\n    p = 2\n    \n    R = np.identity(p)\n    t_indices = np.arange(n)\n    \n    # Test Case 1\n    S1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    thetas1 = np.zeros((m, n, d))\n    for j in range(m):\n        phi_j = j * np.pi / 6.0\n        thetas1[j, :, 0] = 0.0 + 0.2 * np.sin(2 * np.pi * t_indices / n + phi_j)\n        thetas1[j, :, 1] = 0.0 + 0.2 * np.cos(2 * np.pi * t_indices / n + phi_j)\n\n    # Test Case 2\n    S2 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    thetas2 = np.zeros((m, n, d))\n    mu1_j_vals = [-0.3, 0.0, 0.3]\n    for j in range(m):\n        phi_j = j * np.pi / 6.0\n        thetas2[j, :, 0] = mu1_j_vals[j] + 0.2 * np.sin(2 * np.pi * t_indices / n + phi_j)\n        thetas2[j, :, 1] = 0.0 + 0.2 * np.cos(2 * np.pi * t_indices / n + phi_j)\n        \n    # Test Case 3\n    S3 = np.array([[0.0, 0.0], [0.0, 0.0]])\n    thetas3 = thetas1\n\n    results = []\n    for thetas, S_case in [(thetas1, S1), (thetas2, S2), (thetas3, S3)]:\n        result = compute_weighted_psrf(thetas, S_case, R)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}