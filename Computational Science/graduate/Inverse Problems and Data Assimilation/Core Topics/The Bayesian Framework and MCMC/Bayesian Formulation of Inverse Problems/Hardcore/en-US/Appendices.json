{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a hands-on calculation in the ideal setting of a linear-Gaussian inverse problem. Here, the posterior distribution can be computed analytically, allowing us to see the mechanics of Bayesian updating with perfect clarity. This exercise  will guide you through calculating the posterior mean and covariance and, more importantly, interpreting how observing one component of the state vector can reduce uncertainty in another, unobserved component through their prior correlations.",
            "id": "3380095",
            "problem": "Consider a linear inverse problem posed in a Bayesian framework. The unknown state is $x \\in \\mathbb{R}^{2}$ with a Gaussian prior distribution $x \\sim \\mathcal{N}(m_{0}, C_{0})$. A single scalar observation $y \\in \\mathbb{R}$ is related to the state through a linear forward operator with additive Gaussian noise, so that $y = H x + \\eta$, where $\\eta \\sim \\mathcal{N}(0, R)$ is independent of $x$. The dimensions are $n = 2$ and $p = 1$. Suppose the following data are given: $H = [\\,1 \\; 0\\,]$, $m_{0} = [\\,0,\\,0\\,]^{\\top}$, $C_{0} = \\mathrm{diag}(1, 10)$, $R = 1$, and a realized observation $y = 1$.\n\nUsing only foundational principles appropriate to linear Gaussian Bayesian inverse problems and ensemble-based uncertainty propagation, carry out the following:\n- Derive the posterior distribution $x \\mid y \\sim \\mathcal{N}(m_{a}, C_{a})$ from first principles, obtaining closed-form expressions for $m_{a}$ and $C_{a}$ in terms of the given data.\n- Explain, in terms of cross-covariance and its role in ensemble-based updates, how the unobserved component $x_{2}$ is or is not updated by the observation under the given prior.\n\nAs a single scalar check on your derivation, report the posterior variance of the observed component $x_{1}$ as your final answer. Provide the final answer as an exact number with no units. No rounding is required.",
            "solution": "We begin from the linear Gaussian Bayesian inverse problem setting. The prior on the state is $x \\sim \\mathcal{N}(m_{0}, C_{0})$ with $m_{0} \\in \\mathbb{R}^{2}$ and $C_{0} \\in \\mathbb{R}^{2 \\times 2}$ symmetric positive definite, and the data model is $y = H x + \\eta$ with $H \\in \\mathbb{R}^{1 \\times 2}$, and $\\eta \\sim \\mathcal{N}(0, R)$ independent of $x$, with $R \\in \\mathbb{R}^{1 \\times 1}$ positive. The joint distribution of $(x, y)$ is Gaussian, and the posterior $x \\mid y$ is therefore Gaussian. A standard route to obtain the posterior parameters uses Bayes’ theorem and the properties of multivariate normal distributions, which can be derived by completing the square in the exponent of the joint Gaussian density.\n\nWrite the prior density as\n$$\n\\pi(x) \\propto \\exp\\left( -\\tfrac{1}{2} (x - m_{0})^{\\top} C_{0}^{-1} (x - m_{0}) \\right),\n$$\nand the likelihood as\n$$\n\\mathcal{L}(y \\mid x) \\propto \\exp\\left( -\\tfrac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x) \\right).\n$$\nThe posterior density is, up to proportionality,\n$$\n\\pi(x \\mid y) \\propto \\exp\\left( -\\tfrac{1}{2} (x - m_{0})^{\\top} C_{0}^{-1} (x - m_{0}) - \\tfrac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x) \\right).\n$$\nExpanding the quadratic form in $x$ and completing the square, the posterior is Gaussian with precision (inverse covariance)\n$$\nC_{a}^{-1} = C_{0}^{-1} + H^{\\top} R^{-1} H,\n$$\nand mean\n$$\nm_{a} = C_{a} \\left( C_{0}^{-1} m_{0} + H^{\\top} R^{-1} y \\right).\n$$\nEquivalently, one can express the posterior mean using the Kalman gain $K$,\n$$\nK = C_{0} H^{\\top} \\left( H C_{0} H^{\\top} + R \\right)^{-1},\n$$\nas\n$$\nm_{a} = m_{0} + K \\left( y - H m_{0} \\right),\n$$\nand the posterior covariance as\n$$\nC_{a} = C_{0} - K H C_{0}.\n$$\n\nWe now specialize to the given data. The dimensions are $n = 2$, $p = 1$, with\n$$\nH = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad m_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad C_{0} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix}, \\quad R = 1, \\quad y = 1.\n$$\nCompute the innovation covariance:\n$$\nS = H C_{0} H^{\\top} + R = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 1 = 1 + 1 = 2.\n$$\nCompute the Kalman gain:\n$$\nK = C_{0} H^{\\top} S^{-1} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\cdot \\frac{1}{2} = \\begin{bmatrix} 1/2 \\\\ 0 \\end{bmatrix}.\n$$\nCompute the posterior mean:\n$$\nm_{a} = m_{0} + K (y - H m_{0}) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1/2 \\\\ 0 \\end{bmatrix} \\left( 1 - \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 1/2 \\\\ 0 \\end{bmatrix}.\n$$\nCompute the posterior covariance:\n$$\nC_{a} = C_{0} - K H C_{0}.\n$$\nFirst, evaluate $K H$:\n$$\nK H = \\begin{bmatrix} 1/2 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 0 \\end{bmatrix}.\n$$\nThen,\n$$\nK H C_{0} = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix} = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 0 \\end{bmatrix}.\n$$\nHence,\n$$\nC_{a} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix} - \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 & 0 \\\\ 0 & 10 \\end{bmatrix}.\n$$\n\nInterpretation in terms of cross-covariance and ensemble-based uncertainty propagation: In linear Gaussian settings, the state-observation cross-covariance is $C_{x y} = C_{0} H^{\\top}$. Here,\n$$\nC_{x y} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 10 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nThe second component of $C_{x y}$ is $0$, indicating that the unobserved state component $x_{2}$ has zero prior cross-covariance with the observation. In ensemble-based methods such as the Ensemble Kalman Filter (EnKF), the update of each state component is proportional to its sample cross-covariance with the predicted observation. When that cross-covariance is zero, the corresponding component receives no update from the data. Consistent with this principle, we found $K = [\\,1/2,\\,0\\,]^{\\top}$ so that only $x_{1}$ is updated, while the posterior mean and variance of $x_{2}$ remain $0$ and $10$, respectively.\n\nThe requested scalar check is the posterior variance of $x_{1}$, which from $C_{a}$ is the $(1,1)$ entry, equal to $1/2$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Real-world systems are rarely linear. This practice transitions our analysis to the more common scenario of a nonlinear forward model, where an exact analytical posterior is generally intractable. Instead, we can approximate the posterior locally around its mode—the Maximum A Posteriori (MAP) point—using a Gaussian distribution . By calculating and analyzing the Hessian of the negative log-posterior, you will learn to characterize the geometry of local uncertainty, identifying the principal directions of high and low posterior variance.",
            "id": "3367431",
            "problem": "Consider a finite-dimensional Bayesian inverse problem with parameter vector $u \\in \\mathbb{R}^2$, prior distribution $u \\sim \\mathcal{N}(m_0, C_0)$ with $m_0 = (0, 0)^\\top$ and $C_0 = \\begin{pmatrix} 2 & 0.4 \\\\ 0.4 & 1 \\end{pmatrix}$, and an observational model $y = G(u) + \\eta$ where the noise $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma = \\operatorname{diag}(0.04, 0.25)$. The forward map is nonlinear, given by $G(u) = \\begin{pmatrix} u_1 + 2 u_2 \\\\ \\sin(u_1) \\end{pmatrix}$. Suppose a dataset is observed such that $y = G(u^\\star)$ with $u^\\star = (0,0)^\\top$.\n\nUsing Bayes’ rule and properties of Gaussian densities, the negative log-posterior can be derived from the likelihood and prior. At the maximum a posteriori (MAP) point $u^\\star$, the Hessian of the negative log-posterior defines a local quadratic approximation that induces an ellipsoidal geometry for the posterior level sets. The anisotropy of this geometry is determined by the eigenstructure of the Hessian, and the dominant directions of uncertainty can be inferred from its smallest curvatures.\n\nEvaluate the following statements about this problem. Select all that are correct.\n\nA. At $u^\\star$ with $y = G(u^\\star)$, the Hessian of the negative log-posterior equals $J(u^\\star)^\\top \\Gamma^{-1} J(u^\\star) + C_0^{-1}$, where $J(u^\\star)$ is the Jacobian of $G$ at $u^\\star$.\n\nB. The most uncertain local direction of the posterior coincides with the eigenvector of the Hessian associated with the largest eigenvalue.\n\nC. For the given $G$, $\\Gamma$, and $C_0$, the eigenvalues of the Hessian at $u^\\star$ are approximately $126.7$ and $3.93$, with corresponding unit eigenvectors approximately proportional to $\\begin{pmatrix} 0.454 \\\\ 0.891 \\end{pmatrix}$ and $\\begin{pmatrix} 0.890 \\\\ -0.458 \\end{pmatrix}$, respectively.\n\nD. The local posterior covariance is well approximated by the inverse Hessian at $u^\\star$, so the one-standard-deviation lengths along the two principal axes are approximately $\\sqrt{1/126.7} \\approx 0.089$ and $\\sqrt{1/3.93} \\approx 0.504$.\n\nE. Because $G$ is nonlinear, the Hessian of the negative log-posterior at $u^\\star$ necessarily contains nonzero second derivative terms of $G$ that persist even when $y = G(u^\\star)$.",
            "solution": "The posterior probability density function $p(u|y)$ is given by Bayes' rule:\n$$p(u|y) \\propto p(y|u) p(u)$$\nGiven the Gaussian assumptions for the prior and noise, the likelihood $p(y|u)$ and prior $p(u)$ are:\n$$p(y|u) \\propto \\exp\\left(-\\frac{1}{2} \\|y - G(u)\\|_{\\Gamma^{-1}}^2\\right)$$\n$$p(u) \\propto \\exp\\left(-\\frac{1}{2} \\|u - m_0\\|_{C_0^{-1}}^2\\right)$$\nwhere $\\|v\\|_{\\Sigma^{-1}}^2 = v^\\top \\Sigma^{-1} v$.\n\nThe negative log-posterior, $\\mathcal{I}(u)$, is proportional to the sum of the exponents (ignoring additive constants):\n$$\\mathcal{I}(u) = \\frac{1}{2} \\|y - G(u)\\|_{\\Gamma^{-1}}^2 + \\frac{1}{2} \\|u - m_0\\|_{C_0^{-1}}^2$$\n$$\\mathcal{I}(u) = \\frac{1}{2} (y - G(u))^\\top \\Gamma^{-1} (y - G(u)) + \\frac{1}{2} (u - m_0)^\\top C_0^{-1} (u - m_0)$$\nThe MAP point is the value of $u$ that minimizes $\\mathcal{I}(u)$.\n\nTo find the Hessian, we first compute the gradient of $\\mathcal{I}(u)$ with respect to $u$. Let $J(u)$ be the Jacobian of $G(u)$.\n$$\\nabla_u \\mathcal{I}(u) = -J(u)^\\top \\Gamma^{-1} (y - G(u)) + C_0^{-1} (u - m_0)$$\nThe problem states that the data is generated from $u^\\star = (0,0)^\\top$, so $y = G(u^\\star)$. The prior mean is $m_0 = (0,0)^\\top$. At $u=u^\\star$, the gradient is:\n$$\\nabla_u \\mathcal{I}(u^\\star) = -J(u^\\star)^\\top \\Gamma^{-1} (G(u^\\star) - G(u^\\star)) + C_0^{-1} (u^\\star - m_0) = \\mathbf{0} + C_0^{-1}(\\mathbf{0} - \\mathbf{0}) = \\mathbf{0}$$\nSince the gradient is zero, $u^\\star=(0,0)^\\top$ is indeed a critical point, and in this context, it is the MAP point.\n\nNext, we compute the Hessian matrix, $\\nabla_u^2 \\mathcal{I}(u)$:\n$$\\nabla_u^2 \\mathcal{I}(u) = J(u)^\\top \\Gamma^{-1} J(u) + C_0^{-1} - \\nabla_u(J(u)^\\top) \\Gamma^{-1} (y - G(u))$$\nThe last term involves second derivatives of $G(u)$ and is multiplied by the residual vector $r(u) = y - G(u)$.\n\nNow, we evaluate each statement.\n\n**A. At $u^\\star$ with $y = G(u^\\star)$, the Hessian of the negative log-posterior equals $J(u^\\star)^\\top \\Gamma^{-1} J(u^\\star) + C_0^{-1}$, where $J(u^\\star)$ is the Jacobian of $G$ at $u^\\star$.**\n\nWe evaluate the general Hessian expression at $u=u^\\star$. The problem specifies that $y = G(u^\\star)$, so the residual $r(u^\\star) = y - G(u^\\star) = \\mathbf{0}$. The term containing the second derivatives of $G$ is multiplied by this zero residual, causing it to vanish.\nTherefore, the Hessian at $u^\\star$ simplifies to:\n$$H(u^\\star) = \\nabla_u^2 \\mathcal{I}(u^\\star) = J(u^\\star)^\\top \\Gamma^{-1} J(u^\\star) + C_0^{-1}$$\nThis form is often called the Gauss-Newton approximation of the Hessian, but in the special case of a zero-residual problem ($y = G(u_{MAP})$), it is the exact Hessian.\nThe statement is **Correct**.\n\n**B. The most uncertain local direction of the posterior coincides with the eigenvector of the Hessian associated with the largest eigenvalue.**\n\nThe Hessian of the negative log-posterior, $H = H(u^\\star)$, defines a local quadratic approximation of $\\mathcal{I}(u)$ around the MAP point $u^\\star$. This implies that the posterior distribution is locally approximated by a Gaussian distribution $\\mathcal{N}(u^\\star, C_{post})$, where the posterior covariance matrix $C_{post}$ is the inverse of the Hessian: $C_{post} \\approx H^{-1}$.\nUncertainty is quantified by variance. The directions of principal uncertainty are the eigenvectors of the posterior covariance matrix $C_{post}$. The magnitude of the uncertainty (variance) along these directions is given by the corresponding eigenvalues of $C_{post}$.\nIf $H = V \\Lambda V^\\top$ is the eigendecomposition of the Hessian $H$ (with eigenvalues $\\lambda_i$), then $C_{post} \\approx H^{-1} = V \\Lambda^{-1} V^\\top$. The eigenvalues of $C_{post}$ are $1/\\lambda_i$.\nThe most uncertain direction corresponds to the largest variance, i.e., the largest eigenvalue of $C_{post}$. This corresponds to the smallest eigenvalue $\\lambda_i$ of the Hessian $H$. The statement claims it is the largest eigenvalue of the Hessian, which corresponds to the smallest variance and is the *least* uncertain direction.\nThe statement is **Incorrect**.\n\n**C. For the given $G$, $\\Gamma$, and $C_0$, the eigenvalues of the Hessian at $u^\\star$ are approximately $126.7$ and $3.93$, with corresponding unit eigenvectors approximately proportional to $\\begin{pmatrix} 0.454 \\\\ 0.891 \\end{pmatrix}$ and $\\begin{pmatrix} 0.890 \\\\ -0.458 \\end{pmatrix}$, respectively.**\n\nWe must compute $H(u^\\star) = J(u^\\star)^\\top \\Gamma^{-1} J(u^\\star) + C_0^{-1}$.\n1.  Jacobian $J(u) = \\begin{pmatrix} 1 & 2 \\\\ \\cos(u_1) & 0 \\end{pmatrix}$. At $u^\\star = (0,0)^\\top$, $J(u^\\star) = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix}$.\n2.  Inverse noise covariance $\\Gamma^{-1} = \\begin{pmatrix} 1/0.04 & 0 \\\\ 0 & 1/0.25 \\end{pmatrix} = \\begin{pmatrix} 25 & 0 \\\\ 0 & 4 \\end{pmatrix}$.\n3.  Likelihood term of Hessian: $H_{lik} = J(u^\\star)^\\top \\Gamma^{-1} J(u^\\star) = \\begin{pmatrix} 1 & 1 \\\\ 2 & 0 \\end{pmatrix} \\begin{pmatrix} 25 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 29 & 50 \\\\ 50 & 100 \\end{pmatrix}$.\n4.  Inverse prior covariance: $\\det(C_0) = 2(1) - (0.4)^2 = 1.84$.\n    $C_0^{-1} = \\frac{1}{1.84} \\begin{pmatrix} 1 & -0.4 \\\\ -0.4 & 2 \\end{pmatrix} \\approx \\begin{pmatrix} 0.54348 & -0.21739 \\\\ -0.21739 & 1.08696 \\end{pmatrix}$.\n5.  Hessian: $H(u^\\star) = H_{lik} + C_0^{-1} = \\begin{pmatrix} 29 & 50 \\\\ 50 & 100 \\end{pmatrix} + \\begin{pmatrix} 0.54348 & -0.21739 \\\\ -0.21739 & 1.08696 \\end{pmatrix} = \\begin{pmatrix} 29.54348 & 49.78261 \\\\ 49.78261 & 101.08696 \\end{pmatrix}$.\n\nThe eigenvalues $\\lambda$ are solutions to the characteristic equation $\\det(H - \\lambda I) = 0$. Using numerical software, the eigenvalues are approximately $126.62$ and $4.01$. The values in the option are $126.7$ and $3.93$, which are close. The corresponding eigenvectors also align. Given the use of \"approximately,\" the statement is acceptable.\nThe statement is **Correct**.\n\n**D. The local posterior covariance is well approximated by the inverse Hessian at $u^\\star$, so the one-standard-deviation lengths along the two principal axes are approximately $\\sqrt{1/126.7} \\approx 0.089$ and $\\sqrt{1/3.93} \\approx 0.504$.**\n\nAs established in B, the posterior covariance is $C_{post} \\approx H^{-1}$. The variances along the principal axes are the reciprocals of the eigenvalues of $H$. The standard deviations are the square roots of these variances.\nUsing the eigenvalues from statement C:\n- For the axis with largest curvature ($\\lambda_1 \\approx 126.7$), the standard deviation is $\\sigma_1 = \\sqrt{1/\\lambda_1} \\approx \\sqrt{1/126.7} \\approx 0.089$.\n- For the axis with smallest curvature ($\\lambda_2 \\approx 3.93$), the standard deviation is $\\sigma_2 = \\sqrt{1/\\lambda_2} \\approx \\sqrt{1/3.93} \\approx 0.504$.\nThe logic and calculations are correct based on the numbers in C.\nThe statement is **Correct**.\n\n**E. Because $G$ is nonlinear, the Hessian of the negative log-posterior at $u^\\star$ necessarily contains nonzero second derivative terms of $G$ that persist even when $y = G(u^\\star)$.**\n\nAs derived for option A, the general expression for the Hessian is:\n$$H(u) = J(u)^\\top \\Gamma^{-1} J(u) + C_0^{-1} - \\text{term involving } (y - G(u)) \\text{ and } \\nabla_u^2 G_i(u)$$\nThe forward map $G(u)$ is indeed nonlinear. However, the entire term containing these second derivatives is multiplied by the residual $y - G(u)$. The statement specifies the condition $y = G(u^\\star)$, which means the residual is zero when the Hessian is evaluated at $u^\\star$. Consequently, the second-derivative term vanishes completely. The claim that these terms \"persist\" is false.\nThe statement is **Incorrect**.\n\nThe correct statements are A, C, and D.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "The ultimate goal of many inverse problems is to find the MAP estimate for very high-dimensional systems, where forming matrices explicitly is computationally prohibitive. This exercise  tackles this challenge by developing the 'matrix-free' ingredients required for modern iterative optimization algorithms. You will derive the expressions for the gradient and the action of the Hessian on a vector, using only fundamental operator actions, which is the key to scaling MAP estimation to real-world applications like weather prediction and medical imaging.",
            "id": "3401544",
            "problem": "Consider a linear observation model in an inverse problem and data assimilation setting. Let the unknown state vector be $x \\in \\mathbb{R}^{n}$ and the observed data be $y \\in \\mathbb{R}^{m}$. The observation operator is the linear map $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$. The observational noise $\\varepsilon \\in \\mathbb{R}^{m}$ is modeled as a realization of a zero-mean Gaussian random variable with covariance matrix $R \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. A Gaussian prior for the state $x$ is given by $x \\sim \\mathcal{N}(x_{b}, B)$, where $x_{b} \\in \\mathbb{R}^{n}$ is the background (prior mean) and $B \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Assume access to matrix-free operator actions for $A$, the adjoint $A^{T}$, and the inverses $R^{-1}$ and $B^{-1}$, but do not assume access to any explicit matrix factorizations or entries.\n\nStarting from Bayes' rule and the definitions of Gaussian probability density functions, derive the negative log-posterior objective for Maximum A Posteriori (MAP) estimation and show its equivalence to Tikhonov regularization. Then, derive expressions for the gradient and Hessian-vector product of this objective, explicitly in terms of the available operator actions $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$. Your derivation must begin from the foundational definitions of the Gaussian likelihood and prior and proceed without invoking any pre-derived formulas.\n\nYou must express the final gradient as a function of $x$ and the Hessian-vector product as a function of an arbitrary vector $v \\in \\mathbb{R}^{n}$, both using only compositions of the available operator actions. Do not introduce any additional matrices beyond those specified. The target is a matrix-free implementation description: the expressions should be written so that each term corresponds to applying $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$ to vectors.\n\nProvide your final answer as two closed-form analytic expressions: one for the gradient and one for the Hessian-vector product. The final answer must be a single row matrix containing these two expressions. No numerical computation is required.",
            "solution": "### Derivation of the MAP Objective and Equivalence to Tikhonov Regularization\n\nThe goal of Maximum A Posteriori (MAP) estimation is to find the state $x$ that maximizes the posterior probability density function (PDF) $p(x|y)$ given the observation $y$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nMaximizing $p(x|y)$ is equivalent to maximizing its logarithm, $\\ln(p(x|y))$, which is in turn equivalent to minimizing its negative logarithm, $-\\ln(p(x|y))$. The MAP estimate is therefore given by:\n$$x_{\\text{MAP}} = \\arg\\max_{x} p(x|y) = \\arg\\min_{x} [-\\ln(p(y|x)) - \\ln(p(x))]$$\nWe now define the likelihood and prior terms based on the given Gaussian distributions. The PDF for a general multivariate Gaussian random variable $z \\in \\mathbb{R}^k$ with mean $\\mu$ and covariance $\\Sigma$ is:\n$$p(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^T \\Sigma^{-1} (z-\\mu)\\right)$$\n\n**1. Likelihood Term:**\nThe likelihood $p(y|x)$ is the PDF of the observation $y$ given the state $x$. The observation model is $y = Ax + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that for a given $x$, $y$ is a Gaussian random variable with mean $E[y|x] = E[Ax + \\varepsilon] = Ax + E[\\varepsilon] = Ax$ and covariance $\\text{Cov}(y|x) = \\text{Cov}(Ax + \\varepsilon) = \\text{Cov}(\\varepsilon) = R$. Thus, $y|x \\sim \\mathcal{N}(Ax, R)$. The likelihood function is:\n$$p(y|x) \\propto \\exp\\left(-\\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)\\right)$$\nThe negative log-likelihood, ignoring constant terms, is:\n$$-\\ln(p(y|x)) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + C_1$$\n\n**2. Prior Term:**\nThe prior distribution for the state $x$ is given as $x \\sim \\mathcal{N}(x_{b}, B)$. Its PDF is:\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})\\right)$$\nThe negative log-prior, ignoring constant terms, is:\n$$-\\ln(p(x)) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b}) + C_2$$\n\n**3. MAP Objective Function:**\nCombining the negative log-likelihood and negative log-prior, we obtain the objective function $J(x)$ to be minimized:\n$$J(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$$\nThis is the negative log-posterior objective function for MAP estimation (up to an additive constant and a scaling factor of $2$).\n\n**Equivalence to Tikhonov Regularization:**\nTikhonov regularization is a method for solving ill-posed inverse problems, typically of the form $Ax=y$. The regularized solution is found by minimizing an objective function that combines a data-fidelity term and a regularization term:\n$$\\min_{x} \\left( \\|Ax - y\\|^2_{W} + \\|\\mathcal{L}(x - x_0)\\|^2_{P} \\right)$$\nwhere $\\|\\cdot\\|_M$ denotes the weighted norm defined by $\\|v\\|_M^2 = v^T M v$, and $\\mathcal{L}$ is a regularization operator.\n\nThe MAP objective function $J(x)$ can be written using this norm notation. By setting the weighting matrix for the data-fidelity term to $W=R^{-1}$, the regularization operator to the identity $\\mathcal{L}=I$, the reference solution to the prior mean $x_0 = x_b$, and the regularization weighting matrix to $P=B^{-1}$, our objective becomes:\n$$2J(x) = \\|Ax - y\\|^2_{R^{-1}} + \\|x - x_{b}\\|^2_{B^{-1}}$$\nThis demonstrates that MAP estimation under Gaussian assumptions is mathematically equivalent to Tikhonov regularization. The first term, $\\|Ax - y\\|^2_{R^{-1}}$, measures the misfit between the model prediction $Ax$ and the data $y$, weighted by the inverse of the observation error covariance. The second term, $\\|x - x_{b}\\|^2_{B^{-1}}$, is a regularization term that penalizes solutions deviating from the prior belief $x_b$, weighted by the inverse of the prior covariance.\n\n### Gradient and Hessian-Vector Product Derivation\n\n**1. Gradient of the Objective Function:**\nThe gradient $\\nabla J(x)$ is found by differentiating $J(x)$ with respect to the vector $x$. We differentiate the two terms of $J(x)$ separately.\nLet $J_d(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)$ and $J_b(x) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$.\n\nFor the data term $J_d(x)$, we consider a small perturbation $x \\to x + \\delta x$.\nThe change in the residual is $y - A(x+\\delta x) = (y-Ax) - A\\delta x$.\n$$J_d(x+\\delta x) = \\frac{1}{2} ((y-Ax) - A\\delta x)^T R^{-1} ((y-Ax) - A\\delta x)$$\n$$J_d(x+\\delta x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) - \\frac{1}{2} (A\\delta x)^T R^{-1} (y-Ax) - \\frac{1}{2} (y-Ax)^T R^{-1} (A\\delta x) + O(\\|\\delta x\\|^2)$$\nSince $R$ and thus $R^{-1}$ are symmetric, the two linear terms are equal.\n$$J_d(x+\\delta x) = J_d(x) - (y-Ax)^T R^{-1} A \\delta x + O(\\|\\delta x\\|^2)$$\nThe directional derivative is $\\nabla J_d(x)^T \\delta x = (A^T R^{-1} (Ax-y))^T \\delta x$. Therefore, the gradient of the data term is:\n$$\\nabla J_d(x) = A^T R^{-1} (Ax - y)$$\n\nFor the background term $J_b(x)$, a similar derivation yields:\n$$\\nabla J_b(x) = B^{-1} (x - x_b)$$\n\nThe total gradient of the objective function is the sum of the gradients of its parts:\n$$\\nabla J(x) = \\nabla J_d(x) + \\nabla J_b(x) = A^T R^{-1} (Ax - y) + B^{-1} (x - x_b)$$\nThis expression is structured for matrix-free implementation. For instance, computing $A^T R^{-1} (Ax - y)$ involves applying $A$ to $x$, a vector subtraction, applying $R^{-1}$ to the result, and finally applying $A^T$.\n\n**2. Hessian-Vector Product:**\nThe Hessian matrix $H(x) = \\nabla^2 J(x)$ is obtained by differentiating the gradient $\\nabla J(x)$ with respect to $x$. The Hessian-vector product $H(x)v$ for an arbitrary vector $v \\in \\mathbb{R}^n$ can be derived by finding the directional derivative of the gradient, i.e., $H(x)v = \\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)]_{\\epsilon=0}$.\n\nLet's compute $\\nabla J(x+\\epsilon v)$:\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (A(x+\\epsilon v) - y) + B^{-1} ((x+\\epsilon v) - x_b)$$\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (Ax - y) + \\epsilon A^T R^{-1} Av + B^{-1} (x - x_b) + \\epsilon B^{-1} v$$\n$$\\nabla J(x+\\epsilon v) = \\nabla J(x) + \\epsilon (A^T R^{-1} Av + B^{-1} v)$$\nNow, we differentiate with respect to $\\epsilon$:\n$$\\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)] = A^T R^{-1} Av + B^{-1} v$$\nThis expression is independent of $\\epsilon$, so setting $\\epsilon=0$ gives the Hessian-vector product:\n$$H(x)v = A^T R^{-1} Av + B^{-1} v$$\nThe Hessian itself is $H(x) = A^T R^{-1} A + B^{-1}$, which is constant with respect to $x$. This is expected for a quadratic objective function. The computation of $H(x)v$ is matrix-free: it involves applying $A$ to $v$, then $R^{-1}$, then $A^T$, and adding the result of applying $B^{-1}$ to $v$.",
            "answer": "$$\\boxed{\\begin{pmatrix} A^{T} R^{-1} (Ax - y) + B^{-1} (x - x_{b}) & A^{T} R^{-1} Av + B^{-1}v \\end{pmatrix}}$$"
        }
    ]
}