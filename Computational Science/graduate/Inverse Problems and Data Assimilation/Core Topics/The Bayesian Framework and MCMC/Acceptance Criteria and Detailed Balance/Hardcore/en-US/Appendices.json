{
    "hands_on_practices": [
        {
            "introduction": "Understanding a principle often involves exploring what happens when it breaks. This thought experiment challenges you to analyze a subtly incorrect version of the Metropolis acceptance criterion, revealing why the precise form of the energy difference, $\\Delta E$, is non-negotiable. By working through the consequences of this mistake, you will gain a deeper, first-principles appreciation for how detailed balance ensures convergence to the correct physical ensemble. ",
            "id": "2465253",
            "problem": "A classical molecular system has potential energy function $E(\\mathbf{x})$ defined on a bounded configuration domain $\\Omega \\subset \\mathbb{R}^{3N}$ of finite volume. You wish to sample the canonical (constant number, volume, temperature) ensemble at temperature $T$ using the Metropolis Monte Carlo method, with proposals drawn from a symmetric proposal kernel $q(\\mathbf{x} \\to \\mathbf{y})$ satisfying $q(\\mathbf{x} \\to \\mathbf{y}) = q(\\mathbf{y} \\to \\mathbf{x})$. Let $\\beta = 1/(k_{\\mathrm{B}} T)$, where $k_{\\mathrm{B}}$ is the Boltzmann constant. The standard Metropolis acceptance probability for a proposed move $\\mathbf{x} \\to \\mathbf{y}$ is $a_{\\mathrm{std}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta (E(\\mathbf{y}) - E(\\mathbf{x}))]\\}$.\n\nSuppose instead that you mistakenly use the modified acceptance probability $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta |E(\\mathbf{y}) - E(\\mathbf{x})|]\\}$ for all proposed moves.\n\nSelect all statements that are correct under these conditions.\n\nA. With $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$, detailed balance with respect to the Boltzmann distribution $\\pi(\\mathbf{x}) \\propto \\exp[-\\beta E(\\mathbf{x})]$ is violated, so the Markov chain does not sample the canonical ensemble.\n\nB. With $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$, any proposal that decreases the energy, $E(\\mathbf{y}) \\le E(\\mathbf{x})$, is accepted with probability $1$, exactly as in the standard Metropolis rule.\n\nC. For symmetric proposals on the bounded domain $\\Omega$, the rule $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$ satisfies detailed balance with respect to the uniform distribution on $\\Omega$.\n\nD. In the low-temperature limit $\\beta \\to \\infty$, the rule $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$ accepts only moves with $E(\\mathbf{y}) = E(\\mathbf{x})$, which generally destroys ergodicity unless there exist connected sets of configurations with exactly equal energy of nonzero measure.\n\nE. For any $\\beta > 0$, the stationary distribution induced by $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$ coincides with that of the standard Metropolis rule $a_{\\mathrm{std}}(\\mathbf{x} \\to \\mathbf{y})$.",
            "solution": "The problem statement must first be validated for scientific and logical soundness.\n\n### Step 1: Extract Givens\n\n- System: A classical molecular system.\n- Potential energy function: $E(\\mathbf{x})$.\n- Configuration domain: $\\Omega \\subset \\mathbb{R}^{3N}$, which is bounded and has finite volume.\n- Ensemble: Canonical (constant $N$, $V$, $T$).\n- Temperature: $T$.\n- Method: Metropolis Monte Carlo.\n- Proposal kernel: $q(\\mathbf{x} \\to \\mathbf{y})$, symmetric, i.e., $q(\\mathbf{x} \\to \\mathbf{y}) = q(\\mathbf{y} \\to \\mathbf{x})$.\n- Inverse temperature: $\\beta = 1/(k_{\\mathrm{B}} T)$.\n- Standard Metropolis acceptance probability: $a_{\\mathrm{std}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta (E(\\mathbf{y}) - E(\\mathbf{x}))]\\}$.\n- Modified acceptance probability: $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta |E(\\mathbf{y}) - E(\\mathbf{x})|]\\}$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is set within the rigorous framework of statistical mechanics and computational chemistry. It investigates the properties of a Markov chain defined by a modified Metropolis-like rule. All concepts—canonical ensemble, Boltzmann distribution, detailed balance, and acceptance criteria—are fundamental and well-defined.\n- **Well-Posed:** The problem provides a clear, mathematical definition of a non-standard acceptance rule and asks for an evaluation of its properties. The questions posed are precise and can be answered using the established theory of Markov chain Monte Carlo methods.\n- **Objective:** The problem is stated in formal, objective language without ambiguity or subjective content.\n- **Completeness:** All necessary information is provided. The properties of the proposal kernel (symmetric) and the domain (bounded) are specified, which are crucial for the analysis.\n\n### Step 3: Verdict and Action\n\nThe problem statement is internally consistent, scientifically sound, and well-posed. It presents a valid theoretical question in computational statistical mechanics. We may proceed to the solution.\n\nThe central principle for establishing the stationary distribution $\\pi(\\mathbf{x})$ of a Markov chain is the condition of **detailed balance**. For a Markov chain with transition probabilities $P(\\mathbf{x} \\to \\mathbf{y})$, detailed balance is satisfied if:\n$$ \\pi(\\mathbf{x}) P(\\mathbf{x} \\to \\mathbf{y}) = \\pi(\\mathbf{y}) P(\\mathbf{y} \\to \\mathbf{x}) $$\nFor Metropolis-type algorithms, the transition probability for $\\mathbf{x} \\neq \\mathbf{y}$ is given by the product of the proposal probability $q(\\mathbf{x} \\to \\mathbf{y})$ and the acceptance probability $a(\\mathbf{x} \\to \\mathbf{y})$. With a symmetric proposal kernel, $q(\\mathbf{x} \\to \\mathbf{y}) = q(\\mathbf{y} \\to \\mathbf{x})$, the detailed balance condition simplifies to:\n$$ \\pi(\\mathbf{x}) a(\\mathbf{x} \\to \\mathbf{y}) = \\pi(\\mathbf{y}) a(\\mathbf{y} \\to \\mathbf{x}) $$\nThis can be rewritten as the Metropolis-Hastings ratio condition:\n$$ \\frac{a(\\mathbf{x} \\to \\mathbf{y})}{a(\\mathbf{y} \\to \\mathbf{x})} = \\frac{\\pi(\\mathbf{y})}{\\pi(\\mathbf{x})} $$\nWe will use this condition to analyze the provided statements.\n\nThe modified acceptance rule is $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta |E(\\mathbf{y}) - E(\\mathbf{x})|]\\}$.\nDue to the absolute value, $|E(\\mathbf{y}) - E(\\mathbf{x})| = |E(\\mathbf{x}) - E(\\mathbf{y})|$, the acceptance probability is symmetric with respect to the exchange of states:\n$$ a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = a_{\\mathrm{abs}}(\\mathbf{y} \\to \\mathbf{x}) $$\nTherefore, the ratio of acceptance probabilities is always unity:\n$$ \\frac{a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})}{a_{\\mathrm{abs}}(\\mathbf{y} \\to \\mathbf{x})} = 1 $$\n\nNow we evaluate each option.\n\n**A. With $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$, detailed balance with respect to the Boltzmann distribution $\\pi(\\mathbf{x}) \\propto \\exp[-\\beta E(\\mathbf{x})]$ is violated, so the Markov chain does not sample the canonical ensemble.**\n\nThe target distribution for the canonical ensemble is the Boltzmann distribution, $\\pi(\\mathbf{x}) = Z^{-1} \\exp[-\\beta E(\\mathbf{x})]$, where $Z$ is the partition function. For detailed balance to hold with this distribution, we must have:\n$$ \\frac{a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})}{a_{\\mathrm{abs}}(\\mathbf{y} \\to \\mathbf{x})} = \\frac{\\pi(\\mathbf{y})}{\\pi(\\mathbf{x})} = \\frac{\\exp[-\\beta E(\\mathbf{y})]}{\\exp[-\\beta E(\\mathbf{x})]} = \\exp[-\\beta (E(\\mathbf{y}) - E(\\mathbf{x}))] $$\nAs established, the left-hand side is $1$. The condition becomes:\n$$ 1 = \\exp[-\\beta (E(\\mathbf{y}) - E(\\mathbf{x}))] $$\nThis equality only holds if $E(\\mathbf{y}) - E(\\mathbf{x}) = 0$. For any proposed move between states of different energy, $E(\\mathbf{y}) \\neq E(\\mathbf{x})$, the detailed balance condition is violated. Since a molecular system will generally have a continuous spectrum of accessible energy levels, this violation is persistent and fundamental. A Markov chain that does not satisfy detailed balance with respect to the Boltzmann distribution will not converge to it as its stationary distribution. Thus, it will not sample the canonical ensemble.\nVerdict: **Correct**.\n\n**B. With $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$, any proposal that decreases the energy, $E(\\mathbf{y}) \\le E(\\mathbf{x})$, is accepted with probability $1$, exactly as in the standard Metropolis rule.**\n\nConsider a move where energy decreases, $E(\\mathbf{y}) < E(\\mathbf{x})$.\nFor the standard rule, $E(\\mathbf{y}) - E(\\mathbf{x}) < 0$, which implies $-\\beta(E(\\mathbf{y}) - E(\\mathbf{x})) > 0$. Then $\\exp[-\\beta(E(\\mathbf{y}) - E(\\mathbf{x}))] > 1$, and $a_{\\mathrm{std}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\text{value}>1\\} = 1$. The statement is correct for the standard rule.\nNow, for the modified rule $a_{\\mathrm{abs}}$:\nIf $E(\\mathbf{y}) < E(\\mathbf{x})$, then $|E(\\mathbf{y}) - E(\\mathbf{x})| = E(\\mathbf{x}) - E(\\mathbf{y}) > 0$. The acceptance probability is:\n$$ a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta (E(\\mathbf{x}) - E(\\mathbf{y}))]\\} $$\nSince $\\beta > 0$ and $E(\\mathbf{x}) - E(\\mathbf{y}) > 0$, the exponent is negative. Therefore, $\\exp[-\\beta (E(\\mathbf{x}) - E(\\mathbf{y}))] < 1$.\nThe acceptance probability is $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = \\exp[-\\beta (E(\\mathbf{x}) - E(\\mathbf{y}))]$, which is strictly less than $1$. In fact, this rule penalizes moves that decrease energy in the same way it penalizes moves that increase energy by the same amount. The statement is false.\nVerdict: **Incorrect**.\n\n**C. For symmetric proposals on the bounded domain $\\Omega$, the rule $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$ satisfies detailed balance with respect to the uniform distribution on $\\Omega$.**\n\nLet the uniform distribution be $\\pi_U(\\mathbf{x}) = C$ for all $\\mathbf{x} \\in \\Omega$, where $C$ is a normalization constant (inverse of the volume of $\\Omega$). The ratio of probabilities is:\n$$ \\frac{\\pi_U(\\mathbf{y})}{\\pi_U(\\mathbf{x})} = \\frac{C}{C} = 1 $$\nThe detailed balance condition requires:\n$$ \\frac{a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})}{a_{\\mathrm{abs}}(\\mathbf{y} \\to \\mathbf{x})} = 1 $$\nAs shown previously, $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = a_{\\mathrm{abs}}(\\mathbf{y} \\to \\mathbf{x})$, so this condition is always satisfied. Therefore, the modified acceptance rule satisfies detailed balance with respect to the uniform distribution. Assuming the Markov chain is ergodic (which is generally true for standard proposal kernels on a connected domain), its stationary distribution is the uniform distribution on $\\Omega$.\nVerdict: **Correct**.\n\n**D. In the low-temperature limit $\\beta \\to \\infty$, the rule $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$ accepts only moves with $E(\\mathbf{y}) = E(\\mathbf{x})$, which generally destroys ergodicity unless there exist connected sets of configurations with exactly equal energy of nonzero measure.**\n\nWe analyze the behavior of $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y}) = \\min\\{1, \\exp[-\\beta |E(\\mathbf{y}) - E(\\mathbf{x})|]\\}$ as $\\beta \\to \\infty$.\n- If $E(\\mathbf{y}) = E(\\mathbf{x})$, then $|E(\\mathbf{y}) - E(\\mathbf{x})| = 0$, so $a_{\\mathrm{abs}} = \\min\\{1, \\exp[0]\\} = 1$. Moves on an iso-energy surface are always accepted.\n- If $E(\\mathbf{y}) \\neq E(\\mathbf{x})$, then $|E(\\mathbf{y}) - E(\\mathbf{x})| = \\Delta E > 0$. As $\\beta \\to \\infty$, the exponent $-\\beta \\Delta E \\to -\\infty$, and thus $\\exp[-\\beta \\Delta E] \\to 0$. The acceptance probability for any move that changes energy approaches zero.\nIn the limit, the chain only accepts moves for which $\\Delta E = 0$. This means the system is trapped on the iso-energy hypersurface defined by its initial energy $E(\\mathbf{x}_0)$. It cannot access states with different energies. Ergodicity, the property that the chain can eventually visit any part of the state space $\\Omega$, is therefore destroyed. The chain becomes a sampler for the microcanonical ensemble on a specific energy shell, not for the entire space $\\Omega$. The caveat about connected sets of constant energy is a formal technicality: if such a set exists, the chain may be ergodic *within* that set, but not over the whole of $\\Omega$.\nVerdict: **Correct**.\n\n**E. For any $\\beta > 0$, the stationary distribution induced by $a_{\\mathrm{abs}}(\\mathbf{x} \\to \\mathbf{y})$ coincides with that of the standard Metropolis rule $a_{\\mathrm{std}}(\\mathbf{x} \\to \\mathbf{y})$.**\n\nThe stationary distribution for the standard Metropolis rule $a_{\\mathrm{std}}$ is the Boltzmann distribution, $\\pi_{\\mathrm{std}}(\\mathbf{x}) \\propto \\exp[-\\beta E(\\mathbf{x})]$.\nAs established in the analysis of option C, the stationary distribution for the modified rule $a_{\\mathrm{abs}}$ is the uniform distribution, $\\pi_{\\mathrm{abs}}(\\mathbf{x}) \\propto 1$.\nThe Boltzmann distribution equals the uniform distribution only in the trivial case where the potential energy $E(\\mathbf{x})$ is constant over the entire domain $\\Omega$. For any non-trivial molecular system, $E(\\mathbf{x})$ is not constant. Therefore, for any finite $\\beta > 0$, the two distributions are different.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Real-world inverse problems often yield posterior distributions with multiple, well-separated modes, a feature that can trap standard MCMC samplers. This practice delves into this critical issue by analyzing a Metropolis-Hastings algorithm on a bimodal target, a common proxy for more complex \"rugged\" energy landscapes. By deriving the acceptance probability for rare but crucial jumps between modes, you will gain a concrete understanding of how multimodality leads to metastability and poor sampler performance. ",
            "id": "3362440",
            "problem": "Consider a one-dimensional inverse problem arising in data assimilation, where a scalar parameter $x$ is to be inferred from observations under two competing physical regimes. The posterior density for $x$ is modeled as a Gaussian mixture,\n$\n\\pi(x) \\propto w_{1}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{1}}\\exp\\!\\left(-\\frac{(x-\\mu_{1})^{2}}{2\\sigma_{1}^{2}}\\right) + w_{2}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{2}}\\exp\\!\\left(-\\frac{(x-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}\\right),\n$\nwhere $w_{1},w_{2}\\in(0,1)$ with $w_{1}+w_{2}=1$, the means $\\mu_{1},\\mu_{2}\\in\\mathbb{R}$ are well separated in the sense that $\\Delta\\equiv|\\mu_{2}-\\mu_{1}|\\gg \\max\\{\\sigma_{1},\\sigma_{2}\\}$, and $\\sigma_{1},\\sigma_{2}>0$ are the component standard deviations. Assume a Metropolis–Hastings (MH) Markov Chain Monte Carlo (MCMC) algorithm with a symmetric Gaussian random-walk proposal density $q(y\\,|\\,x)=\\frac{1}{\\sqrt{2\\pi}\\,s}\\exp\\!\\big(-\\frac{(y-x)^{2}}{2s^{2}}\\big)$, where the step size $s>0$ is fixed and satisfies $s\\ll \\Delta$. The proposal is therefore symmetric in the sense that $q(y\\,|\\,x)=q(x\\,|\\,y)$.\n\nStarting from the detailed balance condition for Markov chains, and using only foundational principles such as Bayes’s rule for the posterior and the definition of the Metropolis–Hastings transition kernel, derive the acceptance criterion that enforces detailed balance for this symmetric proposal. Then, characterize the metastability caused by multimodality and a fixed step size by formally conditioning on proposals that attempt to jump between the modes: define the event $A_{\\varepsilon}$ to mean a proposal from a current state $x$ with $|x-\\mu_{1}|\\le \\varepsilon$ to a proposed state $y$ with $|y-\\mu_{2}|\\le \\varepsilon$, where $\\varepsilon>0$ is a small neighborhood radius such that $\\varepsilon\\ll \\min\\{\\sigma_{1},\\sigma_{2}\\}$ and $\\varepsilon\\ll \\Delta$. In the limit $\\varepsilon\\to 0$, compute the expected acceptance probability conditioned on $A_{\\varepsilon}$ for the given MH algorithm with fixed step size $s$.\n\nYour final answer must be a single closed-form analytic expression in terms of $w_{1}$, $w_{2}$, $\\sigma_{1}$, and $\\sigma_{2}$. No numerical approximation is required. Also, briefly explain, from first principles, how multimodality and metastability enter through the acceptance rate and the proposal mechanism in this setting. The final answer must be provided exactly as an analytic expression; do not include any units.",
            "solution": "The posterior density is specified as a Gaussian mixture. From Bayes’s rule, for data assimilation, one has $\\pi(x)\\propto p(y_{\\text{obs}}\\,|\\,x)\\,p(x)$, and here that posterior is represented explicitly by the mixture. The Metropolis–Hastings (MH) algorithm defines a Markov chain with proposal density $q(y\\,|\\,x)$ and acceptance probability $\\alpha(x,y)$, yielding a transition probability from $x$ to $y$ given by $P(x\\to y)=q(y\\,|\\,x)\\,\\alpha(x,y)$ for $y\\neq x$ and $P(x\\to x)=1-\\int q(z\\,|\\,x)\\,\\alpha(x,z)\\,\\mathrm{d}z$. The requirement that the chain have $\\pi$ as its invariant distribution and be reversible with respect to $\\pi$ is the detailed balance condition:\n$$\n\\pi(x)\\,P(x\\to y)=\\pi(y)\\,P(y\\to x),\\quad \\text{for all }x,y.\n$$\nFor $x\\neq y$, substituting $P$ gives\n$$\n\\pi(x)\\,q(y\\,|\\,x)\\,\\alpha(x,y) = \\pi(y)\\,q(x\\,|\\,y)\\,\\alpha(y,x).\n$$\nThe standard MH construction chooses\n$$\n\\alpha(x,y)=\\min\\!\\left(1,\\frac{\\pi(y)\\,q(x\\,|\\,y)}{\\pi(x)\\,q(y\\,|\\,x)}\\right),\n$$\nwhich satisfies detailed balance and maximizes acceptance subject to that constraint. Because the proposal is symmetric, namely $q(y\\,|\\,x)=q(x\\,|\\,y)$, this simplifies to\n$$\n\\alpha(x,y)=\\min\\!\\left(1,\\frac{\\pi(y)}{\\pi(x)}\\right).\n$$\n\nWe are asked to compute the expected acceptance probability conditioned on proposals that attempt to jump between the two modes. Formally, define $A_{\\varepsilon}$ as\n$$\nA_{\\varepsilon}=\\left\\{|x-\\mu_{1}|\\le \\varepsilon,\\;|y-\\mu_{2}|\\le \\varepsilon\\right\\},\\quad \\varepsilon\\ll \\min\\{\\sigma_{1},\\sigma_{2}\\},\\;\\varepsilon\\ll \\Delta.\n$$\nUnder the well-separated condition $\\Delta\\gg \\max\\{\\sigma_{1},\\sigma_{2}\\}$ and the small neighborhoods of radius $\\varepsilon$, the contribution of the opposite Gaussian component to the posterior near each mode is negligible. Thus, for $x$ with $|x-\\mu_{1}|\\le \\varepsilon$,\n$$\n\\pi(x)\\approx C\\, w_{1}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{1}}\\exp\\!\\left(-\\frac{(x-\\mu_{1})^{2}}{2\\sigma_{1}^{2}}\\right),\n$$\nand for $y$ with $|y-\\mu_{2}|\\le \\varepsilon$,\n$$\n\\pi(y)\\approx C\\, w_{2}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{2}}\\exp\\!\\left(-\\frac{(y-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}\\right),\n$$\nwhere $C>0$ is the (unknown) normalization constant of the posterior that cancels in ratios. In the limit $\\varepsilon\\to 0$, we have $x\\to \\mu_{1}$ and $y\\to \\mu_{2}$, so the exponential factors tend to $\\exp(0)=1$, and we obtain\n$$\n\\frac{\\pi(y)}{\\pi(x)} \\xrightarrow[\\varepsilon\\to 0]{} \\frac{w_{2}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{2}}}{w_{1}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{1}}}=\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}.\n$$\nTherefore, for symmetric proposals,\n$$\n\\alpha(x,y)\\xrightarrow[\\varepsilon\\to 0]{}\\min\\!\\left(1,\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}\\right).\n$$\nBecause this limiting value is constant on $A_{\\varepsilon}$ as $\\varepsilon\\to 0$, the conditional expectation of the acceptance probability given $A_{\\varepsilon}$ equals this constant:\n$$\n\\lim_{\\varepsilon\\to 0}\\,\\mathbb{E}\\!\\left[\\alpha(X,Y)\\,\\big|\\,A_{\\varepsilon}\\right] = \\min\\!\\left(1,\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}\\right).\n$$\n\nFinally, we explain the interaction of multimodality and metastability with acceptance rates and fixed step sizes from first principles. Multimodality arises because $\\pi$ has two well-separated peaks at $\\mu_{1}$ and $\\mu_{2}$. Metastability in the Markov chain is a consequence of the proposal mechanism having a fixed step size $s$ that is small relative to the separation $\\Delta$. The probability of proposing a jump from a neighborhood of $\\mu_{1}$ into a neighborhood of $\\mu_{2}$ under a Gaussian random walk is exponentially small in $\\Delta^{2}/s^{2}$; more precisely, if $X\\approx \\mu_{1}$, then $Y\\sim \\mathcal{N}(X,s^{2})$ must realize a rare tail event $|Y-\\mu_{2}|\\le \\varepsilon$, whose probability scales like $\\exp\\!\\big(-\\Delta^{2}/(2s^{2})\\big)$ up to polynomial factors. Thus, even if the acceptance of such cross-mode proposals (when they occur) is high, as given by the expression above, the overall rate of successful inter-mode transitions is dominated by the rarity of proposing cross-mode moves. This leads to long residence times within a single mode (metastability), slow mixing, and strong dependence of inter-mode transition times on the ratio $\\Delta/s$. The acceptance criterion itself is determined by detailed balance and depends on the local ratio of posterior mass near the two modes, captured here by the weights and scales of the Gaussian components; the proposal mechanism determines how often such favorable proposals are actually attempted.",
            "answer": "$$\\boxed{\\min\\!\\left(1,\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}\\right)}$$"
        },
        {
            "introduction": "Building on the challenge of sampling from multimodal distributions, this practice explores an advanced and powerful solution: Parallel Tempering. You will derive the acceptance criterion for the key \"swap move,\" where states are exchanged between parallel Markov chains running at different temperatures. This exercise demonstrates how the fundamental principle of detailed balance can be ingeniously applied to an extended state space to facilitate exploration and overcome the very metastability issues examined previously. ",
            "id": "3362466",
            "problem": "Consider a Bayesian inverse problem in data assimilation where a state variable $x$ is inferred from observations $y$ through a posterior density $\\pi(x)$ on a state space $\\mathcal{X}$. To facilitate sampling from $\\pi(x)$ in a high-dimensional setting, Parallel Tempering (also known as Replica Exchange Monte Carlo) constructs $M$ coupled Markov chains targeting tempered distributions $\\pi_{\\beta_k}(x) \\propto \\pi(x)^{\\beta_k}$, with inverse temperatures $1 = \\beta_1 > \\beta_2 > \\cdots > \\beta_M > 0$. The joint target density on the product space $\\mathcal{X}^M$ is proportional to $\\prod_{k=1}^{M} \\pi(x_k)^{\\beta_k}$, where $x_k$ denotes the state of the chain at inverse temperature $\\beta_k$.\n\nA swap move proposes exchanging the states $x_i$ and $x_{i+1}$ of two adjacent chains at inverse temperatures $\\beta_i$ and $\\beta_{i+1}$. The proposal kernel for this swap is symmetric. Using the fundamental Metropolis–Hastings acceptance criterion derived from detailed balance for Markov chain Monte Carlo (MCMC), derive the swap acceptance probability as a function of $\\pi(x_i)$, $\\pi(x_{i+1})$, $\\beta_i$, and $\\beta_{i+1}$, expressed as a single closed-form analytic expression.\n\nYour final answer must be a single symbolic expression. Do not provide intermediate steps in the final answer.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Framework**: Bayesian inverse problem in data assimilation.\n- **State Variable**: $x$ on a state space $\\mathcal{X}$.\n- **Posterior Density**: $\\pi(x)$.\n- **Algorithm**: Parallel Tempering (Replica Exchange Monte Carlo) with $M$ coupled Markov chains.\n- **Tempered Distributions**: $\\pi_{\\beta_k}(x) \\propto \\pi(x)^{\\beta_k}$ for $k \\in \\{1, 2, \\ldots, M\\}$.\n- **Inverse Temperatures**: $1 = \\beta_1 > \\beta_2 > \\cdots > \\beta_M > 0$.\n- **Joint Target Density**: Proportional to $\\prod_{k=1}^{M} \\pi(x_k)^{\\beta_k}$ on the product space $\\mathcal{X}^M$, where $x_k$ is the state of the chain at inverse temperature $\\beta_k$.\n- **Proposed Move**: Swap the states $x_i$ and $x_{i+1}$ of two adjacent chains at inverse temperatures $\\beta_i$ and $\\beta_{i+1}$.\n- **Proposal Kernel**: The proposal kernel for the swap is symmetric.\n- **Methodology**: Apply the Metropolis-Hastings acceptance criterion derived from detailed balance.\n- **Objective**: Derive the swap acceptance probability as a single closed-form analytic expression in terms of $\\pi(x_i)$, $\\pi(x_{i+1})$, $\\beta_i$, and $\\beta_{i+1}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in the theory of Markov chain Monte Carlo (MCMC) methods, specifically Parallel Tempering, which is a standard and widely used algorithm in computational statistics, statistical physics, and data assimilation.\n2.  **Well-Posed**: The problem is well-posed. It provides a clear objective and all necessary information (the form of the target density, the nature of the proposed move, and the symmetry of the proposal kernel) to derive the requested acceptance probability. A unique, analytical solution exists.\n3.  **Objective**: The problem is stated in precise, objective, and technical language, free of any subjectivity or ambiguity.\n4.  **Completeness and Consistency**: The problem is self-contained and internally consistent. The definition of the tempered distributions and the joint target density are standard and correctly formulated.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It represents a standard derivation in the field of MCMC methods. A full solution will be provided.\n\nThe core principle for constructing a Markov chain that samples from a target distribution $\\Pi(X)$ is the condition of detailed balance. For any two states $X$ and $X'$, detailed balance requires that\n$$\n\\Pi(X) P(X \\to X') = \\Pi(X') P(X' \\to X)\n$$\nwhere $P(X \\to X')$ is the transition probability from state $X$ to $X'$. The transition probability can be decomposed into a proposal probability $q(X \\to X')$ and an acceptance probability $\\alpha(X \\to X')$, such that $P(X \\to X') = q(X \\to X') \\alpha(X \\to X')$. The Metropolis-Hastings algorithm satisfies detailed balance by defining the acceptance probability as:\n$$\n\\alpha(X \\to X') = \\min\\left(1, \\frac{\\Pi(X') q(X' \\to X)}{\\Pi(X) q(X \\to X')}\\right)\n$$\nIn the context of Parallel tempering, the state of the entire system is the tuple of states from all chains, $X = (x_1, x_2, \\ldots, x_M)$. The joint target probability density, up to a normalization constant, is given by:\n$$\n\\Pi(X) \\propto \\prod_{k=1}^{M} \\pi(x_k)^{\\beta_k}\n$$\nWe consider a proposal to swap the states of two adjacent chains, indexed by $i$ and $i+1$.\nThe current state is $X_c = (x_1, \\ldots, x_i, x_{i+1}, \\ldots, x_M)$.\nThe proposed state is $X_p = (x_1, \\ldots, x_{i+1}, x_i, \\ldots, x_M)$. Note that the states are swapped, but the corresponding inverse temperatures remain fixed to their chain index. The state from chain $i+1$ (originally $x_{i+1}$) moves to chain $i$ (with temperature $\\beta_i$), and the state from chain $i$ (originally $x_i$) moves to chain $i+1$ (with temperature $\\beta_{i+1}$).\n\nThe problem states that the proposal kernel is symmetric. This means the probability of proposing to swap from $X_c$ to $X_p$ is the same as proposing to swap from $X_p$ to $X_c$. Formally, $q(X_c \\to X_p) = q(X_p \\to X_c)$. Consequently, the ratio of proposal probabilities is $1$, and the acceptance probability simplifies to:\n$$\n\\alpha(X_c \\to X_p) = \\min\\left(1, \\frac{\\Pi(X_p)}{\\Pi(X_c)}\\right)\n$$\nSince $\\Pi(X)$ is given as a proportionality, we can use the un-normalized form of the density in the ratio. Let's denote the un-normalized density as $\\tilde{\\Pi}(X) = \\prod_{k=1}^{M} \\pi(x_k)^{\\beta_k}$.\nThe un-normalized density of the current state $X_c$ is:\n$$\n\\tilde{\\Pi}(X_c) = \\left(\\prod_{k \\neq i, i+1} \\pi(x_k)^{\\beta_k}\\right) \\pi(x_i)^{\\beta_i} \\pi(x_{i+1})^{\\beta_{i+1}}\n$$\nThe un-normalized density of the proposed state $X_p$ is:\n$$\n\\tilde{\\Pi}(X_p) = \\left(\\prod_{k \\neq i, i+1} \\pi(x_k)^{\\beta_k}\\right) \\pi(x_{i+1})^{\\beta_i} \\pi(x_i)^{\\beta_{i+1}}\n$$\nThe ratio of these densities is:\n$$\n\\frac{\\tilde{\\Pi}(X_p)}{\\tilde{\\Pi}(X_c)} = \\frac{\\left(\\prod_{k \\neq i, i+1} \\pi(x_k)^{\\beta_k}\\right) \\pi(x_{i+1})^{\\beta_i} \\pi(x_i)^{\\beta_{i+1}}}{\\left(\\prod_{k \\neq i, i+1} \\pi(x_k)^{\\beta_k}\\right) \\pi(x_i)^{\\beta_i} \\pi(x_{i+1})^{\\beta_{i+1}}}\n$$\nThe product term involving chains other than $i$ and $i+1$ cancels out, leaving:\n$$\n\\frac{\\tilde{\\Pi}(X_p)}{\\tilde{\\Pi}(X_c)} = \\frac{\\pi(x_{i+1})^{\\beta_i} \\pi(x_i)^{\\beta_{i+1}}}{\\pi(x_i)^{\\beta_i} \\pi(x_{i+1})^{\\beta_{i+1}}}\n$$\nThis expression can be rearranged to group terms by state:\n$$\n\\frac{\\tilde{\\Pi}(X_p)}{\\tilde{\\Pi}(X_c)} = \\pi(x_i)^{\\beta_{i+1} - \\beta_i} \\pi(x_{i+1})^{\\beta_i - \\beta_{i+1}} = \\left(\\frac{\\pi(x_{i+1})}{\\pi(x_i)}\\right)^{\\beta_i - \\beta_{i+1}}\n$$\nThe acceptance probability for the swap move is therefore:\n$$\n\\alpha = \\min\\left(1, \\frac{\\pi(x_{i+1})^{\\beta_i} \\pi(x_i)^{\\beta_{i+1}}}{\\pi(x_i)^{\\beta_i} \\pi(x_{i+1})^{\\beta_{i+1}}}\\right)\n$$\nThis is the final closed-form expression as a function of the required quantities $\\pi(x_i)$, $\\pi(x_{i+1})$, $\\beta_i$, and $\\beta_{i+1}$. In implementation, this is often computed using logarithms to maintain numerical stability:\n$$\n\\alpha = \\min\\left(1, \\exp\\left[ (\\beta_i - \\beta_{i+1}) (\\ln\\pi(x_{i+1}) - \\ln\\pi(x_i)) \\right]\\right)\n$$\nHowever, the problem requests the expression in terms of $\\pi(x_i)$ and $\\pi(x_{i+1})$, so the former representation is the direct answer.",
            "answer": "$$\n\\boxed{\\min\\left(1, \\frac{\\pi(x_{i+1})^{\\beta_i} \\pi(x_i)^{\\beta_{i+1}}}{\\pi(x_i)^{\\beta_i} \\pi(x_{i+1})^{\\beta_{i+1}}}\\right)}\n$$"
        }
    ]
}