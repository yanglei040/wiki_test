{
    "hands_on_practices": [
        {
            "introduction": "许多物理参数（如方差或速率常数）本质上是受限的（例如，必须为正）。在MCMC采样中处理这些约束的一个常用且稳健的策略是对参数进行变换，例如使用对数变换将正实数映射到整个实数轴。本练习将指导您推导在变换空间中正确的Metropolis-Hastings接受概率，重点是应用变量变换法则并正确包含变换的雅可比行列式(Jacobian)，这是确保从正确后验分布中抽样的关键一步。",
            "id": "3402720",
            "problem": "在一步序贯数据同化设置中，考虑一个正演模型，其状态估计固定，产生的相对于观测的残差被建模为独立同分布的高斯噪声，其未知标准差为 $\\sigma \\in (0,\\infty)$。设残差数量为 $n$，并用 $S$ 表示残差平方和，即 $S = \\sum_{i=1}^{n} r_{i}^{2}$。假设在给定 $\\sigma$ 的条件下，残差的似然函数是高斯分布，并且 $\\sigma$ 的先验是 Jeffreys 先验，$p(\\sigma) \\propto \\sigma^{-1}$。\n\n为了使用 Metropolis–Hastings (MH) 算法构建马尔可夫链蒙特卡洛 (MCMC) 更新，通过变换 $u = \\ln(\\sigma)$ 来施加正性约束，使得 $u \\in \\mathbb{R}$。在变换后的空间中使用高斯随机游走提议，$q(u' \\mid u) = \\mathcal{N}(u, s^{2})$，其中 $s  0$ 为某个固定值。从后验密度的贝叶斯定理、独立残差的高斯似然以及概率密度的变量替换公式等基本定义出发，推导在变换后变量 $u$ 中的 MH 接受概率 $\\alpha(u \\rightarrow u')$ 的表达式，并明确计入变换的雅可比行列式。除了提议在 $u$ 上的对称性外，不要对 $s$ 做任何特定假设。\n\n然后，对于 $n = 10$，$S = 25$，当前状态 $u = 0$ 和提议状态 $u' = -0.2$，数值计算接受概率。将最终的接受概率数值答案四舍五入至四位有效数字。",
            "solution": "用户希望我解决一个问题，该问题涉及为一个特定的贝叶斯推断问题推导和评估 Metropolis-Hastings 接受概率。\n\n该问题被认为是有效的，因为它具有科学依据、问题适定且客观。它基于贝叶斯统计和 Metropolis-Hastings 算法的标准原理。所有必要信息均已提供，且问题没有矛盾或歧义。\n\n解决方案按要求分为两部分：第一部分是接受概率的解析推导，第二部分是其数值计算。\n\n第一部分：Metropolis-Hastings 接受概率的推导\n\n我们的目标是使用 Metropolis-Hastings (MH) 算法从参数 $u = \\ln(\\sigma)$ 的后验分布中进行抽样。后验密度，即 MCMC 抽样器的目标密度，记为 $\\pi(u)$。根据贝叶斯定理，后验概率与似然函数和先验概率的乘积成正比。\n\n首先，我们用参数 $\\sigma$ 来定义似然函数和先验。\n对于 $i=1, \\dots, n$，残差 $r_i$ 被建模为来自均值为 $0$、方差为 $\\sigma^2$ 的高斯分布的独立同分布 (i.i.d.) 抽样。给定 $\\sigma$ 时，残差的似然函数是其联合概率密度：\n$$p(\\mathbf{r} | \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)$$\n$$p(\\mathbf{r} | \\sigma) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} r_i^2\\right)$$\n使用给定的定义 $S = \\sum_{i=1}^{n} r_i^2$，似然函数变为：\n$$p(\\mathbf{r} | \\sigma) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\n去掉常数因子，似然函数正比于：\n$$p(\\mathbf{r} | \\sigma) \\propto \\sigma^{-n} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\n$\\sigma$ 的先验被指定为尺度参数的 Jeffreys 先验：\n$$p(\\sigma) \\propto \\sigma^{-1}$$\n那么，$\\sigma$ 的后验密度 $p(\\sigma | \\mathbf{r})$ 与似然函数和先验的乘积成正比：\n$$p(\\sigma | \\mathbf{r}) \\propto p(\\mathbf{r} | \\sigma) p(\\sigma)$$\n$$p(\\sigma | \\mathbf{r}) \\propto \\left[ \\sigma^{-n} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right) \\right] \\cdot \\sigma^{-1}$$\n$$p(\\sigma | \\mathbf{r}) \\propto \\sigma^{-(n+1)} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\n接下来，我们将这个密度从变量 $\\sigma$ 变换到变量 $u = \\ln(\\sigma)$。逆变换为 $\\sigma = \\exp(u)$。概率密度的变量替换公式需要变换的雅可比行列式：\n$$|\\frac{d\\sigma}{du}| = |\\frac{d}{du}(\\exp(u))| = |\\exp(u)| = \\exp(u)$$\n将 $\\sigma=\\exp(u)$ 代入 $p(\\sigma | \\mathbf{r})$ 的表达式，并乘以雅可比行列式，得到 $u$ 的后验密度 $\\pi(u)$：\n$$\\pi(u) \\propto p(\\sigma(u) | \\mathbf{r}) \\left|\\frac{d\\sigma}{du}\\right|$$\n$$\\pi(u) \\propto \\left[ (\\exp(u))^{-(n+1)} \\exp\\left(-\\frac{S}{2(\\exp(u))^2}\\right) \\right] \\cdot \\exp(u)$$\n$$\\pi(u) \\propto \\exp(-u(n+1)) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right) \\exp(u)$$\n$$\\pi(u) \\propto \\exp(-un - u + u) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)$$\n$$\\pi(u) \\propto \\exp(-un) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)$$\n这是变换后参数 $u$ 的未归一化目标后验密度。\n\n从当前状态 $u$ 转移到提议状态 $u'$ 的 MH 算法接受概率由下式给出：\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\frac{\\pi(u') q(u | u')}{\\pi(u) q(u' | u)}\\right)$$\n问题指定了一个高斯随机游走提议 $q(u'|u) = \\mathcal{N}(u, s^2)$，其形式为：\n$$q(u'|u) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u'-u)^2}{2s^2}\\right)$$\n这个提议是对称的，意味着 $q(u'|u) = q(u|u')$，因为 $(u'-u)^2 = (u-u')^2$。因此，提议密度的比率 $q(u|u')/q(u'|u)$ 等于 $1$。接受概率简化为：\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\frac{\\pi(u')}{\\pi(u)}\\right)$$\n现在我们计算后验密度的比率：\n$$\\frac{\\pi(u')}{\\pi(u)} = \\frac{\\exp(-u'n) \\exp\\left(-\\frac{S}{2\\exp(2u')}\\right)}{\\exp(-un) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)}$$\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp(-u'n - (-un)) \\cdot \\exp\\left(-\\frac{S}{2\\exp(2u')} - \\left(-\\frac{S}{2\\exp(2u)}\\right)\\right)$$\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp(n(u-u')) \\cdot \\exp\\left(\\frac{S}{2}\\left(\\frac{1}{\\exp(2u)} - \\frac{1}{\\exp(2u')}\\right)\\right)$$\n合并指数内的各项，得到比率的最终表达式：\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp\\left( n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right) \\right)$$\n因此，MH 接受概率为：\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\exp\\left[ n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right) \\right]\\right)$$\n\n第二部分：数值计算\n\n给定以下数值：\n残差数量，$n = 10$。\n残差平方和，$S = 25$。\n当前状态，$u = 0$。\n提议状态，$u' = -0.2$。\n\n让我们计算接受比率中指数函数的参数，我们称之为 $A$：\n$$A = n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right)$$\n代入给定值：\n$$A = 10(0 - (-0.2)) + \\frac{25}{2}\\left(\\exp(-2 \\cdot 0) - \\exp(-2 \\cdot (-0.2))\\right)$$\n$$A = 10(0.2) + 12.5\\left(\\exp(0) - \\exp(0.4)\\right)$$\n$$A = 2 + 12.5\\left(1 - \\exp(0.4)\\right)$$\n使用计算器计算 $\\exp(0.4)$：\n$$\\exp(0.4) \\approx 1.4918247$$\n将该值代回 $A$ 的表达式中：\n$$A \\approx 2 + 12.5(1 - 1.4918247)$$\n$$A \\approx 2 + 12.5(-0.4918247)$$\n$$A \\approx 2 - 6.14780875$$\n$$A \\approx -4.14780875$$\n接受比率为 $\\exp(A)$：\n$$\\exp(A) \\approx \\exp(-4.14780875) \\approx 0.015800445$$\n接受概率是 $1$ 和这个值的最小值：\n$$\\alpha(u \\rightarrow u') = \\min(1, 0.015800445) = 0.015800445$$\n问题要求将最终答案四舍五入到四位有效数字。\n将数值 $0.015800445$ 四舍五入到四位有效数字得到 $0.01580$。",
            "answer": "$$\\boxed{0.01580}$$"
        },
        {
            "introduction": "在许多科学和工程领域的逆问题中，正向模型（如偏微分方程求解器）的计算成本非常高，这使得标准MCMC方法的效率低下。两阶段多级Metropolis-Hastings算法（也称为延迟接受法）通过使用一个计算成本较低的粗糙模型来预筛选提议，从而有效地解决了这一挑战。本练习将引导您推导并实现这种优雅的加速技术，确保马尔可夫链的目标分布仍是精确的精细模型后验分布。",
            "id": "3402701",
            "problem": "考虑一个承压含水层中稳定一维地下水流的贝叶斯逆问题。对于 $x \\in [0,1]$，水头场 $u(x)$ 满足椭圆型偏微分方程 (PDE) $-\\frac{d}{dx}\\left(k(x)\\frac{du}{dx}\\right)=0$，并带有狄利克雷边界条件 $u(0)=1$ 和 $u(1)=0$。导水系数 $k(x)$ 通过一个双参数对数渗透率表示法被建模为分段常数：对于 $x \\in [0,0.5]$，$k(x)=\\exp(\\theta_0)$；对于 $x \\in (0.5,1]$，$k(x)=\\exp(\\theta_1)$，其中 $\\theta=(\\theta_0,\\theta_1) \\in \\mathbb{R}^2$ 是未知参数矢量。观测值被建模为在位置 $x_{\\text{obs}} \\subset (0,1)$ 处对水头的含噪测量：$y = G_h(\\theta) + \\eta$，其中 $G_h(\\theta)$ 是在尺寸为 $h$ 的细网格上的离散化正向模型预测，且 $\\eta \\sim \\mathcal{N}(0,\\sigma^2 I)$，噪声水平 $\\sigma0$ 已知。假设高斯先验 $\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$，其均值为 $\\mu \\in \\mathbb{R}^2$，协方差为 $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$。\n\n要求您推导并实现一个两阶段多层 Metropolis-Hastings (MH) 马尔可夫链蒙特卡洛 (MCMC) 算法，该算法耦合了由粗糙离散化 $G_H(\\theta)$ 定义的粗网格后验 $\\pi_H(\\theta \\mid y)$ 和由 $G_h(\\theta)$ 定义的目标细网格后验 $\\pi_h(\\theta \\mid y)$。接受机制必须严格确保马尔可夫链以 $\\pi_h(\\theta \\mid y)$ 为其不变分布。推导必须从给定目标概率密度函数 (PDF) 的 MH 算法的细致平衡定义开始，并且必须展示如何在不偏离细网格目标的情况下，引入一个初步的粗网格接受步骤。\n\n要使用的基本建模细节：\n- $h$ 网格上的似然与 $\\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2\\right)$ 成正比。\n- $H$ 网格上的似然与 $\\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2\\right)$ 成正比。\n- 先验与 $\\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$ 成正比。\n- 目标细网格后验为 $\\pi_h(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$。\n- 粗网格后验为 $\\pi_H(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$。\n\n您的实现约束：\n- 在均匀网格上使用具有 $N_h+1$ 个节点的细模型和具有 $N_H+1$ 个节点的粗模型对 PDE 进行离散化，采用一致的二阶有限差分格式，并在单元界面处对导水系数进行调和平均。节点间距为 $\\Delta x = 1/N$，内部方程需根据通量守恒进行组装。在 $x=0$ 和 $x=1$ 处精确施加狄利克雷边界条件。前向映射 $G(\\theta)$ 通过节点值的线性插值返回所请求观测位置处的水头预测值。\n- 使用对称高斯随机游走提议 $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2 I)$，步长参数为 $s0$。\n- 从第一性原理出发，推导一个两阶段接受机制，该机制使用粗糙后验进行初步接受，然后校正到精细后验，同时保持相对于精细后验的细致平衡。\n- 实现算法，使其在给定随机种子的情况下完全确定，从而结果可复现。\n\n数据生成与测试配置：\n- 真实参数为 $\\theta^\\star = (\\theta_0^\\star,\\theta_1^\\star) = (0.0,-1.0)$。\n- 先验为 $\\mu=(0,0)$ 和 $\\Sigma = \\operatorname{diag}(\\tau^2,\\tau^2)$，其中 $\\tau=1.0$。\n- 观测位置为 $x_{\\text{obs}} = (0.2, 0.5, 0.8)$。\n- 细网格尺寸为 $N_h=200$，粗网格尺寸为 $N_H=20$。\n- 对于每个测试用例，通过计算 $y_{\\text{true}} = G_h(\\theta^\\star)$ 来合成数据，然后使用提供的种子和噪声水平 $\\sigma$ 抽取噪声 $\\eta \\sim \\mathcal{N}(0,\\sigma^2 I)$，最后设置 $y=y_{\\text{true}}+\\eta$。\n\n使用以下测试套件实现两阶段多层 MH 算法，其中每个测试用例是一个元组 $(\\text{seed}_{\\text{chain}}, \\text{seed}_{\\text{data}}, s, \\sigma, n_{\\text{iter}})$：\n- 用例 A (理想路径): $(123, 2025, 0.2, 0.05, 2000)$。\n- 用例 B (更大的提议步长): $(456, 2026, 0.7, 0.05, 2000)$。\n- 用例 C (更高噪声，更少迭代次数): $(789, 2027, 0.2, 0.20, 1000)$。\n\n对于每个测试用例，从 $\\theta^{(0)}=(0,0)$ 开始运行链 $n_{\\text{iter}}$ 次迭代，并丢弃前半部分的状态作为预烧期 (burn-in)。按此顺序报告以下四个量：\n- 第一阶段接受率（粗略筛选接受概率），浮点数。\n- 总接受率（通过两个阶段的提议比例），浮点数。\n- 预烧期后 $\\theta_0$ 的后验均值，浮点数。\n- 预烧期后 $\\theta_1$ 的后验均值，浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，按顺序连接用例 A、用例 B 和用例 C 的四个数字。例如，它必须看起来像 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9,r_{10},r_{11},r_{12}]$，所有条目都打印为小数点后六位四舍五入的十进制浮点数。\n- 不应打印任何其他文本。",
            "solution": "该问题是有效的。它提出了一个适定的贝叶斯逆问题，该问题基于稳态地下水流的物理原理，并要求推导和实现一个标准但高级的 MCMC 采样算法。所有必要的数据、模型和参数都已提供，问题是自洽且科学合理的。\n\n### 1. 正向问题及其离散化\n\n物理系统由承压含水层中的一维稳态流方程控制，这是一个椭圆型偏微分方程 (PDE)：\n$$\n-\\frac{d}{dx}\\left(k(x)\\frac{du}{dx}\\right)=0, \\quad x \\in [0,1]\n$$\n带有狄利克雷边界条件 $u(0)=1$ 和 $u(1)=0$。导水系数 $k(x)$ 被建模为由参数矢量 $\\theta=(\\theta_0, \\theta_1)$ 决定的分段常数函数：\n$$\nk(x; \\theta) = \\begin{cases} \\exp(\\theta_0)  \\text{if } x \\in [0, 0.5] \\\\ \\exp(\\theta_1)  \\text{if } x \\in (0.5, 1] \\end{cases}\n$$\n为了数值求解此 PDE，我们在具有 $N+1$ 个节点的均匀网格上使用二阶有限差分格式，$x_i = i \\Delta x$ 对于 $i=0, 1, \\dots, N$，其中 $\\Delta x = 1/N$。我们在以每个内部节点 $x_i$ (对于 $i=1, \\dots, N-1$) 为中心的控制体积上应用通量守恒原理。节点 $i$ 的控制体积为 $[x_{i-1/2}, x_{i+1/2}]$，其中 $x_{i \\pm 1/2} = x_i \\pm \\Delta x/2$。\n在界面 $x_{i+1/2}$ 处的通量近似为 $F_{i+1/2} = -k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x}$。围绕节点 $i$ 的控制体积的守恒定律 $F_{i+1/2} - F_{i-1/2} = 0$ 给出：\n$$\n-k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x} + k_{i-1/2} \\frac{u_i-u_{i-1}}{\\Delta x} = 0\n$$\n$$\n-k_{i-1/2} u_{i-1} + (k_{i-1/2} + k_{i+1/2}) u_i - k_{i+1/2} u_{i+1} = 0\n$$\n此方程对所有内部节点 $i=1, \\dots, N-1$ 成立。项 $k_{i+1/2}$ 表示节点 $i$ 和 $i+1$ 控制体积之间界面的有效导水系数。根据规定，它是使用相邻单元中导水系数值的调和平均值计算的。假设单元 $i$ 的导水系数为 $k(x_i)$，则界面导水系数为 $k_{i+1/2} = \\frac{2 k(x_i) k(x_{i+1})}{k(x_i) + k(x_{i+1})}$。\n\n这些关于未知水头 $u_1, \\dots, u_{N-1}$ 的 $N-1$ 个线性方程可以写成矩阵形式 $A(\\theta)\\mathbf{u}_{\\text{int}} = \\mathbf{b}$。边界条件 $u_0=1$ 和 $u_N=0$ 按如下方式并入：\n- 对于 $i=1$：$-k_{1/2}u_0 + (k_{1/2}+k_{3/2})u_1 - k_{3/2}u_2 = 0 \\implies (k_{1/2}+k_{3/2})u_1 - k_{3/2}u_2 = k_{1/2}u_0 = k_{1/2}$。\n- 对于 $i=N-1$：$-k_{N-3/2}u_{N-2} + (k_{N-3/2}+k_{N-1/2})u_{N-1} - k_{N-1/2}u_N = 0 \\implies -k_{N-3/2}u_{N-2} + (k_{N-3/2}+k_{N-1/2})u_{N-1} = 0$。\n\n矩阵 $A(\\theta)$ 是一个对称正定三对角矩阵，该系统可以被高效求解。正向模型 $G(\\theta)$ 是这样一个函数：对于给定的 $\\theta$，它求解此系统以获得节点水头 $\\mathbf{u} = [u_0, u_1, \\dots, u_N]^\\top$，然后使用线性插值计算指定观测位置 $x_{\\text{obs}}$ 处的水头值。此过程为细网格 ($N=N_h$) 和粗网格 ($N=N_H$) 都进行了定义，分别产生前向映射 $G_h(\\theta)$ 和 $G_H(\\theta)$。\n\n### 2. 贝叶斯逆问题公式化\n\n逆问题被置于贝叶斯框架中。给定数据 $y$ 的参数 $\\theta$ 的后验概率密度函数 (PDF) 由贝叶斯定理给出：\n$$\n\\pi(\\theta \\mid y) \\propto L(y \\mid \\theta) \\pi(\\theta)\n$$\n其中 $L(y \\mid \\theta)$ 是似然，$\\pi(\\theta)$ 是先验。\n\n$\\theta$ 的先验是一个高斯分布，$\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$，所以其 PDF 为：\n$$\n\\pi(\\theta) \\propto \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n观测模型为 $y = G(\\theta) + \\eta$，其中噪声 $\\eta$ 假设为零均值高斯分布，协方差为 $\\sigma^2 I$，即 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$。这产生了一个高斯似然函数：\n$$\nL(y \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G(\\theta)\\|_2^2\\right)\n$$\n我们定义两个后验：一个基于细网格正向模型 $G_h$ 的目标后验 $\\pi_h$，和一个基于粗网格模型 $G_H$ 的近似后验 $\\pi_H$：\n$$\n\\pi_h(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2 - \\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n$$\n\\pi_H(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2 - \\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n我们的目标是使用 MCMC 方法从目标后验 $\\pi_h(\\theta \\mid y)$ 中生成样本。\n\n### 3. 两阶段多层 Metropolis-Hastings 算法\n\n标准的 Metropolis-Hastings (MH) 算法构建一个马尔可夫链，其平稳分布是目标 PDF $\\pi_h$。这是通过满足细致平衡条件来实现的：\n$$\n\\pi_h(\\theta) P(\\theta' \\mid \\theta) = \\pi_h(\\theta') P(\\theta \\mid \\theta')\n$$\n其中 $P(\\theta' \\mid \\theta)$ 是从状态 $\\theta$ 到 $\\theta'$ 的转移核。对于对称提议分布 $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$，接受概率为：\n$$\n\\alpha_h(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_h(\\theta')}{\\pi_h(\\theta)}\\right)\n$$\n评估 $\\pi_h$ 涉及运行细网格正向模型 $G_h$，这在计算上可能非常昂贵。两阶段多层（或延迟接受）MH 算法旨在通过使用计算成本低的粗糙后验 $\\pi_H$ 来预筛选提议，从而降低计算成本。\n\n在每次迭代中，给定当前状态 $\\theta$，算法按以下步骤进行：\n1.  **提议：** 生成一个候选状态 $\\theta' \\sim q(\\theta' \\mid \\theta)$。\n2.  **第一阶段 (粗略筛选)：** 使用粗糙模型计算第一阶段的接受概率：\n    $$\n    \\alpha_1(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right)\n    $$\n    提议以概率 $\\alpha_1(\\theta', \\theta)$ 通过此筛选并进入第二阶段。否则，它被拒绝，链保持在 $\\theta$。\n3.  **第二阶段 (精细校正)：** 如果提议通过第一阶段，计算第二阶段的接受概率：\n    $$\n    \\alpha_2(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n    $$\n    提议以概率 $\\alpha_2(\\theta', \\theta)$ 被接受，链的下一个状态变为 $\\theta'$。否则，它被拒绝，链保持在 $\\theta$。\n\n从 $\\theta$ 移动到 $\\theta'$ 的总接受概率是通过每个阶段的概率的乘积：\n$$\n\\alpha_{DA}(\\theta', \\theta) = \\alpha_1(\\theta', \\theta) \\alpha_2(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n为验证此算法能正确地从 $\\pi_h$ 中采样，我们必须证明它满足关于 $\\pi_h$ 的细致平衡条件。对于对称提议 $q$，这需要证明 $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta')$。让我们分析等式左边：\n$$\n\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta) \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n使用性质 $c \\min(a,b) = \\min(ca, cb)$ (对于 $c0$)，我们可以将项移入最小值算子内部：\n$$\n= \\min\\left(\\pi_h(\\theta), \\pi_h(\\theta) \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n$$\n= \\min\\left(\\pi_h(\\theta), \\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n让我们再次应用 $\\min$ 算子性质，合并这两项：\n$$\n= \\min\\left(\n\\pi_h(\\theta) \\cdot 1, \\quad\n\\pi_h(\\theta) \\cdot \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}, \\quad\n\\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)} \\cdot 1, \\quad\n\\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)} \\cdot \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\n\\right)\n$$\n简化最小值内部的项：\n$$\n= \\min\\left(\n\\pi_h(\\theta), \\quad\n\\pi_h(\\theta') \\frac{\\pi_H(\\theta)}{\\pi_H(\\theta')}, \\quad\n\\pi_h(\\theta) \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}, \\quad\n\\pi_h(\\theta')\n\\right)\n$$\n令这个结果表达式为 $f(\\theta, \\theta')$。通过观察，此表达式关于 $\\theta$ 和 $\\theta'$ 的交换是对称的，即 $f(\\theta, \\theta') = f(\\theta', \\theta)$。由于我们已经证明了 $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = f(\\theta, \\theta')$，立即可以得出 $\\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta') = f(\\theta', \\theta) = f(\\theta, \\theta')$。\n因此，$\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta')$，这证实了关于目标细网格后验 $\\pi_h$ 的细致平衡条件得到满足。该算法是无偏的，并将从正确的目标分布中产生样本。\n在实现上，处理 PDF 的对数（即负对数后验，或势能 $\\Phi(\\theta) = -\\log \\pi(\\theta \\mid y) + \\text{const}$）在数值上更稳定。接受率变成了势能的差值，比较是在对数均匀随机变量和对数比率之间进行的。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve_pde(theta, N):\n    \"\"\"\n    Solves the 1D steady-state groundwater flow PDE using finite differences.\n    \"\"\"\n    delta_x = 1.0 / N\n    x_nodes = np.linspace(0, 1, N + 1)\n    \n    # Hydraulic conductivity at nodes\n    k_vals = np.exp(theta[0] * (x_nodes = 0.5) + theta[1] * (x_nodes > 0.5))\n\n    # Interface conductivities using harmonic mean\n    # k_interface[i] corresponds to interface at x_{i+1/2}\n    k_interface = np.zeros(N)\n    # The term k_vals[i] + k_vals[i+1] can be zero if conductivities are very low,\n    # but exp(theta_i) is always positive. Add a small epsilon for stability if needed,\n    # though not strictly necessary here.\n    k_interface = 2 * k_vals[:-1] * k_vals[1:] / (k_vals[:-1] + k_vals[1:])\n    \n    # Assemble the tridiagonal system A*u_int = b for interior nodes u_1, ..., u_{N-1}\n    # A is (N-1)x(N-1), but we use scipy's banded format\n    # ab has 3 rows: [0, upper_diag], [main_diag], [lower_diag, 0]\n    ab = np.zeros((3, N - 1))\n    \n    # Main diagonal: k_{i-1/2} + k_{i+1/2}\n    ab[1, :] = k_interface[:-1] + k_interface[1:]\n    \n    # Upper diagonal: -k_{i+1/2}\n    ab[0, 1:] = -k_interface[1:-1]\n    \n    # Lower diagonal: -k_{i-1/2}\n    ab[2, :-1] = -k_interface[1:-1]\n\n    # Right-hand side vector\n    b = np.zeros(N - 1)\n    b[0] = k_interface[0] * 1.0  # From u_0 = 1 boundary condition\n\n    # Solve for interior nodes\n    u_internal = solve_banded((1, 1), ab, b)\n    \n    # Combine with boundary values\n    u_full = np.concatenate(([1.0], u_internal, [0.0]))\n    \n    return x_nodes, u_full\n\ndef forward_model_G(theta, N, x_obs):\n    \"\"\"\n    Forward model G(theta), which solves the PDE and interpolates to observation points.\n    \"\"\"\n    x_grid, u_grid = solve_pde(theta, N)\n    return np.interp(x_obs, x_grid, u_grid)\n\ndef neg_log_posterior(theta, y_obs, sigma, N, x_obs, prior_mu, prior_tau_sq):\n    \"\"\"\n    Computes the negative log-posterior (up to a constant).\n    Phi(theta) = 0.5/sigma^2 * ||y - G(theta)||^2 + 0.5/tau^2 * ||theta - mu||^2\n    \"\"\"\n    # Log-likelihood term\n    y_pred = forward_model_G(theta, N, x_obs)\n    log_lik_term = 0.5 / (sigma**2) * np.sum((y_obs - y_pred)**2)\n    \n    # Log-prior term\n    log_prior_term = 0.5 / prior_tau_sq * np.sum((theta - prior_mu)**2)\n    \n    return log_lik_term + log_prior_term\n\ndef run_mcmc(case):\n    \"\"\"\n    Runs the two-stage multi-level MH MCMC for a given test case.\n    \"\"\"\n    seed_chain, seed_data, s, sigma, n_iter = case\n    \n    # --- Problem Setup ---\n    theta_true = np.array([0.0, -1.0])\n    prior_mu = np.array([0.0, 0.0])\n    prior_tau_sq = 1.0**2\n    x_obs = np.array([0.2, 0.5, 0.8])\n    N_h, N_H = 200, 20\n    \n    # --- Synthesize Data ---\n    data_rng = np.random.default_rng(seed_data)\n    y_true = forward_model_G(theta_true, N_h, x_obs)\n    noise = data_rng.normal(0, sigma, size=len(x_obs))\n    y_obs = y_true + noise\n\n    # --- MCMC Initialization ---\n    chain_rng = np.random.default_rng(seed_chain)\n    theta_current = np.array([0.0, 0.0])\n    samples = np.zeros((n_iter, 2))\n    \n    stage1_passes = 0\n    stage2_accepts = 0\n    \n    phi_h_current = neg_log_posterior(theta_current, y_obs, sigma, N_h, x_obs, prior_mu, prior_tau_sq)\n    phi_H_current = neg_log_posterior(theta_current, y_obs, sigma, N_H, x_obs, prior_mu, prior_tau_sq)\n    \n    # --- MCMC Loop ---\n    for i in range(n_iter):\n        # 1. Propose\n        theta_proposal = theta_current + chain_rng.normal(0, s, size=2)\n        \n        # 2. Stage 1 (Coarse Screen)\n        phi_H_proposal = neg_log_posterior(theta_proposal, y_obs, sigma, N_H, x_obs, prior_mu, prior_tau_sq)\n        log_alpha1_ratio = phi_H_current - phi_H_proposal\n        \n        if np.log(chain_rng.random())  log_alpha1_ratio:\n            stage1_passes += 1\n            \n            # 3. Stage 2 (Fine Correction)\n            phi_h_proposal = neg_log_posterior(theta_proposal, y_obs, sigma, N_h, x_obs, prior_mu, prior_tau_sq)\n            log_alpha2_ratio = (phi_h_current - phi_h_proposal) - (phi_H_current - phi_H_proposal)\n            \n            if np.log(chain_rng.random())  log_alpha2_ratio:\n                # Accept\n                stage2_accepts += 1\n                theta_current = theta_proposal\n                phi_h_current = phi_h_proposal\n                phi_H_current = phi_H_proposal\n        \n        # Store current state (which is the new state if accepted, old otherwise)\n        samples[i, :] = theta_current\n\n    # --- Post-processing ---\n    burn_in = n_iter // 2\n    post_burn_samples = samples[burn_in:, :]\n    \n    stage1_rate = stage1_passes / n_iter\n    overall_rate = stage2_accepts / n_iter\n    post_mean_theta0 = np.mean(post_burn_samples[:, 0])\n    post_mean_theta1 = np.mean(post_burn_samples[:, 1])\n    \n    return stage1_rate, overall_rate, post_mean_theta0, post_mean_theta1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test cases: (seed_chain, seed_data, s, sigma, n_iter)\n    test_cases = [\n        (123, 2025, 0.2, 0.05, 2000),  # Case A\n        (456, 2026, 0.7, 0.05, 2000),  # Case B\n        (789, 2027, 0.2, 0.20, 1000),  # Case C\n    ]\n    \n    results = []\n    for case in test_cases:\n        case_results = run_mcmc(case)\n        results.extend(case_results)\n        \n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当似然函数本身难以解析计算，只能通过蒙特卡洛方法进行估计时，标准的Metropolis-Hastings算法便不再适用；这种情况在正向模型本身具有随机性时尤为常见。伪边缘Metropolis-Hastings（PMMH）算法通过在MH框架内使用似然函数的无偏估计量，为这一挑战提供了强大的解决方案。本练习将探讨PMMH的实现，并分析似然估计量的方差如何影响采样器的性能和接受概率的稳定性。",
            "id": "3402776",
            "problem": "考虑一个贝叶斯逆问题，其中未知的参数向量 $\\theta \\in \\mathbb{R}^d$ 是从一个含噪声的正向模型产生的观测向量 $y \\in \\mathbb{R}^d$ 中推断出来的。正向映射为 $G(\\theta)$，该模型受到两个独立噪声源的污染：一个内部模拟器噪声 $u$ 和一个观测噪声 $\\eta$。数据模型为\n$$\ny = G(\\theta) + u + \\eta,\n$$\n其中 $u \\sim \\mathcal{N}(0, \\sigma_{\\text{sim}}^2 I_d)$ 且 $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{obs}}^2 I_d)$，方差 $\\sigma_{\\text{sim}}^2 \\ge 0$ 和 $\\sigma_{\\text{obs}}^2  0$ 为固定值，而 $I_d$ 表示 $d \\times d$ 单位矩阵。$\\theta$ 的先验为 $\\mathcal{N}(0, \\tau^2 I_d)$，其中 $\\tau^2  0$ 已知。目标是探究在使用伪边缘方法估计似然时，模拟器噪声幅度 $\\sigma_{\\text{sim}}$ 如何影响Metropolis-Hastings接受概率的变异性。\n\n您必须实现一个伪边缘Metropolis-Hastings (PMMH) 算法，这是一种马尔可夫链蒙特卡洛 (MCMC) 方法。目标后验密度与 $p(y \\mid \\theta)\\,p(\\theta)$ 成正比，其中 $p(\\theta)$ 是先验密度，$p(y \\mid \\theta)$ 是通过对 $u$ 积分得到的边缘似然。PMMH算法用一个无偏蒙特卡洛估计量替代 $p(y \\mid \\theta)$：\n$$\n\\widehat{p}_M(y \\mid \\theta) = \\frac{1}{M} \\sum_{m=1}^M p_\\eta\\!\\bigl(y - G(\\theta) - u_m\\bigr),\n$$\n其中 $M \\in \\mathbb{N}$ 是蒙特卡洛样本的数量，$u_m \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0, \\sigma_{\\text{sim}}^2 I_d)$，$p_\\eta(\\cdot)$ 表示 $\\eta$ 的高斯密度。在Metropolis-Hastings (MH) 步骤中，您必须使用一个对称高斯随机游走提议 $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s_{\\text{prop}}^2 I_d)$。接受概率是使用当前状态和提议状态下的无偏似然估计值计算的，其方式与保持细致平衡的伪边缘方法一致。\n\n您的任务是：\n- 当 $\\widehat{p}_M(y \\mid \\theta)$ 是 $p(y \\mid \\theta)$ 的无偏估计量时，从第一性原理出发，推导伪边缘MH如何保持精确后验为其不变分布。\n- 解释对数似然估计量的方差如何定性地依赖于 $\\sigma_{\\text{sim}}$ 和 $M$，以及这如何影响MH接受概率序列的变异性。\n- 为以下具体模型和测试套件实现一个完整的PMMH采样器，然后计算每个测试用例的接受概率序列（在预烧期后）的经验方差。\n\n实现的模型规格：\n- 维度：$d = 2$。\n- 正向映射：$G(\\theta) = \\theta$。\n- 先验：$\\theta \\sim \\mathcal{N}(0, \\tau^2 I_d)$，其中 $\\tau = 1$。\n- 观测噪声：$\\sigma_{\\text{obs}} = 0.1$。\n- 提议核：高斯随机游走，标准差为 $s_{\\text{prop}} = 0.25$（即协方差为 $s_{\\text{prop}}^2 I_d$）。\n- 伪边缘似然估计量：使用 $M = 8$ 个蒙特卡洛样本。\n- 链长和预烧期：总迭代次数 $N = 6000$，预烧期 $B = 1000$。\n- 接受概率序列：对于每次迭代 $t \\in \\{1,\\dots,N\\}$，计算MH接受概率 $\\alpha_t \\in [0,1]$，并在丢弃前 $B$ 次迭代后，计算 $\\{\\alpha_t\\}_{t=B+1}^N$ 的无偏样本方差（对于 $T = N-B$，使用分母 $T-1$）。\n\n测试套件：\n- 使用以下模拟器噪声幅度：$\\sigma_{\\text{sim}} \\in \\{0.0, 0.2, 0.6, 1.0\\}$。\n- 对于由 $i \\in \\{0,1,2,3\\}$ 索引的每个测试用例，其对应的 $\\sigma_{\\text{sim}}^{(i)}$ 如上所述，使用固定的真实参数 $\\theta_\\star = [0.3, -0.2]^\\top$ 生成一个合成观测 $y^{(i)}$，通过采样 $u \\sim \\mathcal{N}(0, (\\sigma_{\\text{sim}}^{(i)})^2 I_d)$ 和 $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{obs}}^2 I_d)$ 并设置 $y^{(i)} = \\theta_\\star + u + \\eta$。\n- 随机数生成与可复现性：对于每个测试用例 $i$，为该测试用例中的所有随机性使用一个等于 $123 + i$ 的独立种子，包括生成 $y^{(i)}$、所有PMMH蒙特卡洛似然样本、所有提议以及所有MH均匀抽取。\n\n要求的程序输出：\n- 对于每个测试用例 $i$，运行一个以 $\\sigma_{\\text{sim}}^{(i)}$ 和 $y^{(i)}$ 下的后验为目标的PMMH链，计算预烧期后接受概率序列的经验方差，并按顺序将这四个方差作为一个列表返回。\n- 最终程序输出必须是单行，包含一个以逗号分隔的Python风格浮点数列表的结果，四舍五入到6位小数，例如 $[\\text{v}_0,\\text{v}_1,\\text{v}_2,\\text{v}_3]$。\n\n此问题中的所有量都是无量纲的；不需要物理单位。不涉及角度。将所有要求的数值答案表示为浮点数。最终输出格式为如上所述的单行列表，无附加文本。",
            "solution": "所提出的问题是计算统计学领域一个有效且定义明确的练习，特别关注贝叶斯逆问题背景下的伪边缘Metropolis-Hastings (PMMH) 算法。它要求理论解释和实际实现，并且所有参数和条件都已明确指定。\n\n### 伪边缘Metropolis-Hastings算法的理论证明\n\n伪边缘Metropolis-Hastings (PMMH) 算法是标准Metropolis-Hastings (MH) 方法的一个强大变体。它适用于贝叶斯问题，其中后验分布 $\\pi(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$ 涉及的似然函数 $p(y \\mid \\theta)$ 在解析上或计算上难以直接评估。PMMH通过用一个非负的、无偏的蒙特卡洛估计量（记为 $\\widehat{p}_M(y \\mid \\theta)$）来替换精确的似然 $p(y \\mid \\theta)$ 来解决这个问题。\n\nPMMH的有效性取决于它是在一个增广状态空间上操作的精确MH采样器这一事实。令 $U = \\{u_1, \\dots, u_M\\}$ 表示用于构建似然估计量的辅助随机变量集合，其中 $u_m \\sim p_u(\\cdot)$。该估计量是这些变量的函数，$\\widehat{p}_M(y \\mid \\theta) = \\widehat{p}_M(y \\mid \\theta, U)$。PMMH算法的目标是增广空间 $(\\theta, U)$ 上的一个扩展后验分布，定义为：\n$$\n\\pi_{\\text{aug}}(\\theta, U) \\propto p(\\theta) \\widehat{p}_M(y \\mid \\theta, U) p(U)\n$$\n其中 $p(U) = \\prod_{m=1}^M p_u(u_m)$ 是辅助变量的联合概率密度。此构造的关键特性是 $\\pi_{\\text{aug}}(\\theta, U)$ 关于 $U$ 的边缘分布恢复了 $\\theta$ 的真实目标后验。我们可以通过对 $U$ 积分来验证这一点：\n$$\n\\int \\pi_{\\text{aug}}(\\theta, U) \\, dU \\propto p(\\theta) \\int \\widehat{p}_M(y \\mid \\theta, U) p(U) \\, dU\n$$\n右侧的积分是估计量关于辅助变量分布的期望，$\\mathbb{E}_U[\\widehat{p}_M(y \\mid \\theta, U)]$。由于估计量被要求是无偏的，我们有：\n$$\n\\mathbb{E}_U[\\widehat{p}_M(y \\mid \\theta, U)] = p(y \\mid \\theta)\n$$\n在问题的背景下，这种无偏性建立如下：\n$$\n\\mathbb{E}_U[\\widehat{p}_M(y \\mid \\theta, U)] = \\mathbb{E}_U\\left[ \\frac{1}{M} \\sum_{m=1}^M p_\\eta(y - G(\\theta) - u_m) \\right] = \\int p_\\eta(y - G(\\theta) - u) p_u(u) \\, du = p(y \\mid \\theta)\n$$\n最后的积分表示两个独立噪声源概率密度的卷积，它产生它们和的概率密度，从而正确地给出边缘似然。因此，$\\theta$ 的边缘分布为：\n$$\n\\int \\pi_{\\text{aug}}(\\theta, U) \\, dU \\propto p(\\theta) p(y \\mid \\theta) \\propto \\pi(\\theta \\mid y)\n$$\nPMMH转移核从当前状态 $(\\theta, U)$ 提议一个新状态 $(\\theta', U')$。$\\theta'$ 的提议从一个核 $q(\\theta' \\mid \\theta)$ 中抽取，一组新的辅助变量 $U'$ 从它们的分布 $p(U')$ 中独立抽取。接受概率 $\\alpha$ 由增广目标的标准MH比率给出：\n$$\n\\alpha = \\min\\left(1, \\frac{\\pi_{\\text{aug}}(\\theta', U') Q((\\theta, U) \\mid (\\theta', U'))}{\\pi_{\\text{aug}}(\\theta, U) Q((\\theta', U') \\mid (\\theta, U))} \\right) = \\min\\left(1, \\frac{p(\\theta') \\widehat{p}_M(y \\mid \\theta', U') p(U') q(\\theta \\mid \\theta') p(U)}{p(\\theta) \\widehat{p}_M(y \\mid \\theta, U) p(U) q(\\theta' \\mid \\theta) p(U')} \\right)\n$$\n对于对称提议 $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$，这简化为实用的PMMH接受概率：\n$$\n\\alpha = \\min\\left(1, \\frac{p(\\theta') \\widehat{p}_M(y \\mid \\theta', U')}{p(\\theta) \\widehat{p}_M(y \\mid \\theta, U)} \\right)\n$$\n这表明PMMH能正确地从所期望的后验分布 $\\pi(\\theta \\mid y)$ 中采样。一个关键的实现细节是，如果一个提议被拒绝，采样器不仅必须保留当前的参数值 $\\theta$，还必须保留使用辅助变量 $U$ 计算出的似然估计值 $\\widehat{p}_M(y \\mid \\theta, U)$。\n\n### $\\sigma_{\\text{sim}}$ 和 $M$ 对接受概率变异性的影响\n\nPMMH采样器的性能对对数似然估计量的方差 $\\text{Var}[\\log \\widehat{p}_M(y \\mid \\theta)]$ 高度敏感。该估计量的高方差会降低采样器的效率，通常导致马尔可夫链混合不佳。这种低效率反映在接受概率序列 $\\{\\alpha_t\\}$ 中。\n\n似然估计量 $\\widehat{p}_M(y \\mid \\theta) = \\frac{1}{M} \\sum_{m=1}^M p_\\eta(y - G(\\theta) - u_m)$ 的方差与蒙特卡洛样本数 $M$ 成反比。增加 $M$ 会系统地减小估计量方差，这通常对采样器效率是有利的。参数 $\\sigma_{\\text{sim}}$ 控制模拟器噪声样本 $u_m \\sim \\mathcal{N}(0, \\sigma_{\\text{sim}}^2 I_d)$ 的方差。\n\n$\\sigma_{\\text{sim}}$ 的影响可以通过考察其对随机变量 $Z_m(\\theta) = p_\\eta(y - G(\\theta) - u_m)$ 的影响来理解。当 $\\sigma_{\\text{sim}}$ 增加时，样本 $u_m$ 从一个具有更大方差的分布中抽取。这反过来又增加了密度函数参数 $y - G(\\theta) - u_m$ 的方差。由于 $p_\\eta(\\cdot)$ 是一个非线性函数（高斯密度），增加其输入的方差会导致其输出 $Z_m(\\theta)$ 的方差增加。因此，对于固定的 $M$，似然估计量 $\\widehat{p}_M(y \\mid \\theta)$ 的方差随 $\\sigma_{\\text{sim}}$ 的增加而增加。\n\n在 $\\sigma_{\\text{sim}} = 0$ 的特定情况下，辅助变量 $u_m$ 均为零。估计量变得确定，$\\widehat{p}_M(y \\mid \\theta) = p_\\eta(y - G(\\theta))$，其方差为零。PMMH算法此时简化为具有完全已知似然的标准MH算法。接受概率序列 $\\{\\alpha_t\\}$ 的变异性仅源于通过提议核对参数空间的随机探索。\n\n当 $\\sigma_{\\text{sim}}  0$ 时，由于似然的估计，在每个MCMC步骤中都会引入一个额外的随机性来源。这种随机性被较大的 $\\sigma_{\\text{sim}}$ 值放大，导致接受率的更大波动。一次不幸的辅助变量抽取可能导致一个伪高的似然估计，使链“卡在”当前状态，并拒绝许多后续提议（产生一个低 $\\alpha_t$ 的序列）。这种行为增加了序列 $\\{\\alpha_t\\}$ 的整体变异性。因此，我们预期接受概率序列的经验方差是 $\\sigma_{\\text{sim}}$ 的增函数。\n\n### PMMH采样器的实现\n\n根据问题说明实现PMMH算法。对于每个对应于 $\\sigma_{\\text{sim}} \\in \\{0.0, 0.2, 0.6, 1.0\\}$ 值的测试用例：\n1. 为确保可复现性，整个模拟使用一个唯一的随机种子 ($123+i$)。\n2. 基于真实参数 $\\theta_\\star = [0.3, -0.2]^\\top$、给定的 $\\sigma_{\\text{sim}}^{(i)}$ 和 $\\sigma_{\\text{obs}}=0.1$ 生成一个合成观测 $y^{(i)}$。\n3. PMMH链在 $\\theta_0 = [0.0, 0.0]^\\top$ 初始化。计算一个初始的对数似然估计 $\\log \\widehat{p}_M(y \\mid \\theta_0)$。\n4. 链运行 $N=6000$ 次迭代。在每一步，从随机游走核 $\\mathcal{N}(\\theta, s_{\\text{prop}}^2 I_2)$ 中提议一个新状态 $\\theta'$。使用 $M=8$ 个新的噪声样本计算一个新的对数似然估计 $\\log \\widehat{p}_M(y \\mid \\theta')$。计算并存储接受概率 $\\alpha_t$。根据接受决策更新状态（包括 $\\theta$ 及其相关的对数似然估计）。\n5. 运行后，接受概率序列的前 $B=1000$ 个样本作为预烧期被丢弃。\n6. 计算剩余 $N-B=5000$ 个 $\\alpha_t$ 值的无偏样本方差。\n对每个 $\\sigma_{\\text{sim}}$ 重复此过程，并收集所得的方差。对数似然估计利用了数值稳定的 log-sum-exp 公式：\n$$\n\\log \\widehat{p}_M(y \\mid \\theta) = -\\frac{d}{2}\\log(2\\pi\\sigma_{\\text{obs}}^2) - \\log(M) + \\text{logsumexp}\\left( \\left\\{ -\\frac{\\|y-\\theta-u_m\\|^2}{2\\sigma_{\\text{obs}}^2} \\right\\}_{m=1}^M \\right)\n$$\n以下程序执行了这一完整过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Implements the Pseudo-Marginal Metropolis-Hastings (PMMH) sampler to infer a parameter vector \n    from noisy data. It then computes the empirical variance of the acceptance probability sequence\n    for different levels of simulator noise, as specified in the problem.\n    \"\"\"\n\n    # --- Model and Sampler Parameters ---\n    d = 2  # Dimension of the parameter and observation spaces\n    theta_star = np.array([0.3, -0.2])  # Ground-truth parameter for data generation\n    tau = 1.0  # Standard deviation of the Gaussian prior\n    sigma_obs = 0.1  # Standard deviation of the observation noise\n    s_prop = 0.25  # Standard deviation for the Gaussian random-walk proposal\n    M = 8  # Number of Monte Carlo samples for likelihood estimation\n    N = 6000  # Total number of MCMC iterations\n    B = 1000  # Burn-in period\n\n    # Test suite parameters from the problem statement\n    sigma_sim_values = [0.0, 0.2, 0.6, 1.0]\n    base_seed = 123\n\n    results = []\n\n    # --- Helper Functions ---\n\n    def log_prior(theta, tau_val):\n        \"\"\"Computes the log of the Gaussian prior density (up to a normalizing constant).\"\"\"\n        return -0.5 * np.sum(theta**2) / (tau_val**2)\n\n    def log_likelihood_estimate(theta, y, sigma_sim, sigma_obs_val, num_samples, rng):\n        \"\"\"\n        Computes the PMMH log-likelihood estimate for a given parameter vector theta.\n        This is an unbiased estimator of the true log-likelihood.\n        \"\"\"\n        if sigma_sim == 0.0:\n            # Deterministic case: simulator noise is zero\n            u_samples = np.zeros((num_samples, d))\n        else:\n            # Stochastic case: draw samples for the simulator noise u\n            u_samples = rng.normal(loc=0.0, scale=sigma_sim, size=(num_samples, d))\n\n        # The forward model is G(theta) = theta. The likelihood is based on the observation noise eta.\n        # We need to compute log(p_eta(y - G(theta) - u_m)) for each sample u_m.\n        log_pdf_constant = -0.5 * d * np.log(2 * np.pi * sigma_obs_val**2)\n        \n        diffs = y - theta - u_samples  # This has shape (num_samples, d).\n        sq_norms = np.sum(diffs**2, axis=1) # This has shape (num_samples,).\n        \n        # These are the log-likelihood terms for each u_m sample, excluding the constant.\n        log_terms = -0.5 * sq_norms / (sigma_obs_val**2)\n        \n        # Use log-sum-exp for a numerically stable way to compute log(sum(exp(log_terms))).\n        lse = logsumexp(log_terms)\n        \n        # The final log-likelihood estimate is log(1/M * sum(p_eta_m))\n        log_L_hat = log_pdf_constant + lse - np.log(num_samples)\n        \n        return log_L_hat\n\n    # --- Main Loop over Test Cases ---\n    for i, sigma_sim in enumerate(sigma_sim_values):\n        \n        # Set a unique seed for each test case for reproducibility.\n        seed = base_seed + i\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate synthetic observation data y for the current test case.\n        u_true = rng.normal(loc=0.0, scale=sigma_sim, size=d) if sigma_sim > 0 else np.zeros(d)\n        eta_true = rng.normal(loc=0.0, scale=sigma_obs, size=d)\n        y = theta_star + u_true + eta_true\n        \n        # 2. Initialize the PMMH chain.\n        theta_current = np.zeros(d)\n        \n        # Compute the initial log-likelihood estimate and log-posterior.\n        log_L_current = log_likelihood_estimate(theta_current, y, sigma_sim, sigma_obs, M, rng)\n        log_pi_current = log_L_current + log_prior(theta_current, tau)\n\n        acceptance_probs = []\n\n        # 3. Run the MCMC sampler for N iterations.\n        for _ in range(N):\n            # Propose a new state using a Gaussian random walk.\n            theta_proposal = rng.normal(loc=theta_current, scale=s_prop, size=d)\n            \n            # Estimate log-likelihood at the proposal using a fresh set of random numbers.\n            log_L_proposal = log_likelihood_estimate(theta_proposal, y, sigma_sim, sigma_obs, M, rng)\n            log_pi_proposal = log_L_proposal + log_prior(theta_proposal, tau)\n            \n            # Calculate the log of the Metropolis-Hastings acceptance ratio.\n            log_alpha = log_pi_proposal - log_pi_current\n            \n            alpha = min(1.0, np.exp(log_alpha))\n            acceptance_probs.append(alpha)\n            \n            # Decide whether to accept or reject the proposal.\n            if rng.uniform(0, 1)  alpha:\n                # Accept: update the current state and log-posterior.\n                theta_current = theta_proposal\n                log_L_current = log_L_proposal\n                log_pi_current = log_pi_proposal\n            # else: Reject: the state (theta, log_L, log_pi) remains unchanged.\n\n        # 4. After the chain has run, compute the variance of the acceptance probabilities after burn-in.\n        alpha_post_burn_in = np.array(acceptance_probs[B:])\n        # Use ddof=1 for the unbiased sample variance.\n        var_alpha = np.var(alpha_post_burn_in, ddof=1)\n        \n        results.append(round(var_alpha, 6))\n\n    # Final print statement must produce only the specified single-line format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}