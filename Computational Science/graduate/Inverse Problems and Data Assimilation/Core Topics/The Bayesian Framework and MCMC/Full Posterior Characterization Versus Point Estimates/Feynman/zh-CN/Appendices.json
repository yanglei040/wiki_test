{
    "hands_on_practices": [
        {
            "introduction": "在深入复杂的计算之前，建立对后验不确定性结构的基本直觉至关重要。本练习将通过一个简单的秩亏线性模型，引导您分析性地推导出后验协方差，并揭示数据无法约束的参数方向（即算子的零空间）上的不确定性是如何被放大的。通过这个练习，您将清晰地看到，一个最大后验（MAP）点估计会如何完全掩盖这些关键的不确定性维度。",
            "id": "3383422",
            "problem": "考虑一个线性高斯逆问题，其状态向量为 $x \\in \\mathbb{R}^{3}$，观测向量为 $y \\in \\mathbb{R}^{2}$，正向算子为 $A \\in \\mathbb{R}^{2 \\times 3}$，加性噪声为 $\\epsilon \\in \\mathbb{R}^{2}$。数据模型为 $y = A x + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$，$\\Gamma = \\sigma^{2} I_{2}$，$\\sigma^{2} > 0$ 为给定值。$x$ 的先验为高斯分布 $x \\sim \\mathcal{N}(m_{0}, C_{0})$，其均值为 $m_{0} = (m_{1}, m_{2}, m_{3})^{\\top}$，协方差为\n$$\nC_{0} = \\mathrm{diag}(\\beta^{2}, \\tau^{2}, \\tau^{2}),\n$$\n其中 $\\beta^{2} > 0$ 和 $\\tau^{2} > 0$ 是给定的标量。设正向算子为\n$$\nA = \\begin{pmatrix}\n1  0  0 \\\\\n0  0  0\n\\end{pmatrix},\n$$\n该算子是秩亏的。\n\n从 Bayes’ 法则和高斯密度的定义出发，推导在给定 $y$ 的条件下 $x$ 的后验协方差矩阵。然后，刻画沿着子空间 $\\mathrm{Null}(A)$ 的后验不确定性，并将其与沿数据相关方向的不确定性进行对比。具体来说：\n\n- 以闭式形式计算后验协方差，并确定 $\\mathrm{Null}(A)$。\n- 设 $v \\in \\mathrm{Null}(A)$ 为任意单位向量。计算后验方差 $v^{\\top} S v$，其中 $S$ 表示后验协方差。\n- 计算沿数据相关方向 $e_{1} = (1, 0, 0)^{\\top}$ 的后验方差。\n- 定义各向异性（膨胀）因子\n$$\nR := \\frac{\\text{沿 } \\mathrm{Null}(A) \\text{ 的后验方差}}{\\text{沿数据相关方向的后验方差}} = \\frac{v^{\\top} S v}{e_{1}^{\\top} S e_{1}},\n$$\n并仅用 $\\beta^{2}$、$\\tau^{2}$ 和 $\\sigma^{2}$ 表示 $R$。\n\n最后，计算 $x$ 的最大后验（MAP）估计，并利用你推导的公式解释完整的后验刻画如何揭示了沿 $\\mathrm{Null}(A)$ 的巨大不确定性，而这是单一的 MAP 解所无法传达的。\n\n你的最终答案必须是 $R$ 的闭式解析表达式，不得进行数值代入或四舍五入。",
            "solution": "我们从 Bayes’ 法则 $p(x \\mid y) \\propto p(y \\mid x)\\,p(x)$ 和高斯密度的定义开始。似然为 $p(y \\mid x) = \\mathcal{N}(A x, \\Gamma)$，其中 $\\Gamma = \\sigma^{2} I_{2}$；先验为 $p(x) = \\mathcal{N}(m_{0}, C_{0})$，其中 $C_{0} = \\mathrm{diag}(\\beta^{2}, \\tau^{2}, \\tau^{2})$。\n\n在线性模型 $y = A x + \\epsilon$ 中，对高斯密度乘积进行配方的一个标准结果是，后验 $p(x \\mid y)$ 是高斯分布，其精度（协方差的逆）等于先验精度与拉回到参数空间的数据精度之和。具体地，后验协方差 $S$ 满足\n$$\nS^{-1} = C_{0}^{-1} + A^{\\top} \\Gamma^{-1} A,\n$$\n后验均值 $m$ 为\n$$\nm = S\\left(C_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y \\right).\n$$\n我们针对给定的 $A$、$\\Gamma$ 和 $C_{0}$ 显式地推导 $S$。\n\n首先，计算先验精度：\n$$\nC_{0}^{-1} = \\mathrm{diag}\\!\\left(\\beta^{-2}, \\tau^{-2}, \\tau^{-2}\\right).\n$$\n接下来，计算数据精度的贡献：\n$$\n\\Gamma^{-1} = \\sigma^{-2} I_{2}, \\quad A^{\\top} \\Gamma^{-1} A = \\sigma^{-2} A^{\\top} A.\n$$\n由于\n$$\nA = \\begin{pmatrix}\n1  0  0 \\\\\n0  0  0\n\\end{pmatrix}, \\quad\nA^{\\top} = \\begin{pmatrix}\n1  0 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix},\n$$\n我们有\n$$\nA^{\\top} A = \\begin{pmatrix}\n1  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix},\n$$\n因此\n$$\nA^{\\top} \\Gamma^{-1} A = \\sigma^{-2} \\begin{pmatrix}\n1  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\n因此，\n$$\nS^{-1} = \\mathrm{diag}\\!\\left(\\beta^{-2}, \\tau^{-2}, \\tau^{-2}\\right) + \\sigma^{-2} \\begin{pmatrix}\n1  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\beta^{-2} + \\sigma^{-2}  0  0 \\\\\n0  \\tau^{-2}  0 \\\\\n0  0  \\tau^{-2}\n\\end{pmatrix}.\n$$\n对此对角矩阵求逆，得到后验协方差\n$$\nS = \\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}  0  0 \\\\\n0  \\tau^{2}  0 \\\\\n0  0  \\tau^{2}\n\\end{pmatrix}.\n$$\n\n现在我们来确定 $A$ 的零空间。方程 $A x = 0$ 可写作\n$$\n\\begin{pmatrix}\n1  0  0 \\\\\n0  0  0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix},\n$$\n这意味着 $x_{1} = 0$，而 $x_{2}$ 和 $x_{3}$ 是自由的。因此，\n$$\n\\mathrm{Null}(A) = \\left\\{ \\begin{pmatrix} 0 \\\\ t \\\\ s \\end{pmatrix} : t, s \\in \\mathbb{R} \\right\\}.\n$$\n\n设 $v \\in \\mathrm{Null}(A)$ 为任意单位向量。由于 $S$ 是对角矩阵，其对角线上的元素为 $\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}$、$\\tau^{2}$ 和 $\\tau^{2}$，并且 $v$ 的第一个坐标为零，我们可以将 $v$ 写为 $v = (0, v_{2}, v_{3})^{\\top}$，其中 $v_{2}^{2} + v_{3}^{2} = 1$。那么\n$$\nv^{\\top} S v = \\begin{pmatrix} 0  v_{2}  v_{3} \\end{pmatrix}\n\\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}  0  0 \\\\\n0  \\tau^{2}  0 \\\\\n0  0  \\tau^{2}\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ v_{2} \\\\ v_{3} \\end{pmatrix}\n= \\tau^{2} v_{2}^{2} + \\tau^{2} v_{3}^{2} = \\tau^{2}.\n$$\n因此，沿 $\\mathrm{Null}(A)$ 中任意单位方向的后验方差等于 $\\tau^{2}$。\n\n沿数据相关方向 $e_{1} = (1, 0, 0)^{\\top}$，我们有\n$$\ne_{1}^{\\top} S e_{1} = \\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}.\n$$\n\n我们定义各向异性（膨胀）因子为\n$$\nR := \\frac{v^{\\top} S v}{e_{1}^{\\top} S e_{1}}\n= \\frac{\\tau^{2}}{\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}}\n= \\tau^{2}\\left(\\beta^{-2} + \\sigma^{-2}\\right).\n$$\n该表达式仅取决于 $\\beta^{2}$、$\\tau^{2}$ 和 $\\sigma^{2}$。\n\n为了与点估计进行对比，我们计算最大后验（MAP）估计。对于高斯后验分布，后验均值等于 MAP 估计，其由下式给出：\n$$\nm = S\\left(C_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y \\right).\n$$\n计算 $C_{0}^{-1} m_{0} = \\left(\\beta^{-2} m_{1}, \\tau^{-2} m_{2}, \\tau^{-2} m_{3}\\right)^{\\top}$ 和 $A^{\\top} \\Gamma^{-1} y = \\sigma^{-2} A^{\\top} y = \\sigma^{-2} \\begin{pmatrix} y_{1} \\\\ 0 \\\\ 0 \\end{pmatrix}$。因此，\n$$\nC_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y = \\begin{pmatrix} \\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\\\ \\tau^{-2} m_{2} \\\\ \\tau^{-2} m_{3} \\end{pmatrix},\n$$\n再乘以 $S$ 得到\n$$\nm = \\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} \\left(\\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\right) \\\\\n\\tau^{2} \\cdot \\tau^{-2} m_{2} \\\\\n\\tau^{2} \\cdot \\tau^{-2} m_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} \\left(\\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\right) \\\\\nm_{2} \\\\\nm_{3}\n\\end{pmatrix}.\n$$\n因此，MAP 在未观测到的方向上选择了单一值 $x_{2} = m_{2}$ 和 $x_{3} = m_{3}$，尽管完整的后验协方差表明沿 $\\mathrm{Null}(A)$ 的不确定性很大且等于 $\\tau^{2}$。各向异性因子 $R = \\tau^{2}(\\beta^{-2} + \\sigma^{-2})$ 量化了这种相对于数据相关方向的膨胀，突显了完整的后验刻画如何捕捉到像 MAP 这样的点估计所无法传达的不确定性。\n\n所要求的膨胀因子的最终表达式是 $R = \\tau^{2}(\\beta^{-2} + \\sigma^{-2})$。",
            "answer": "$$\\boxed{\\tau^{2}\\left(\\beta^{-2}+\\sigma^{-2}\\right)}$$"
        },
        {
            "introduction": "在建立了关于不确定性结构的分析直觉之后，我们现在转向一个更实际的计算问题，以探讨这些不确定性在决策中的具体影响。本练习要求您构建一个病态的线性反问题，并在此基础上比较两种决策策略：一种是仅依赖于最大后验（MAP）点估计的“插件式”规则，另一种是考虑了整个后验分布的贝叶斯最优规则。您将通过编程实践发现，在特定风险场景下，忽略后验不确定性会导致做出次优甚至错误的决策。",
            "id": "3383401",
            "problem": "考虑一个具有高斯观测模型和高斯先验的线性逆问题，其中前向算子是病态的。你需要构建模型，计算参数的最大后验（MAP）点估计，完全刻画高斯后验分布，然后解决一个决策问题，其中后验不确定性会改变最优决策（与仅使用MAP的规则相比）。仅使用多元高斯分布和线性代数的基本定义和性质作为起点。\n\n您必须为指定的测试套件解决以下任务。\n\n模型设定：\n- 令未知参数向量为 $ \\theta \\in \\mathbb{R}^{d} $，其中 $ d = 5 $。\n- 令前向算子 $ A \\in \\mathbb{R}^{m \\times d} $，其中 $ m = 5 $，为希尔伯特矩阵，即 $ A_{ij} = \\frac{1}{i + j - 1} $，对于 $ 1 \\le i,j \\le 5 $。此矩阵 $ A $ 是病态的。\n- 令观测模型为 $ y = A \\theta + \\epsilon $，其中观测噪声 $ \\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I) $，已知方差 $ \\sigma^{2} > 0 $，$ I $ 是 $ \\mathbb{R}^{m \\times m } $ 中的单位矩阵。\n- 令先验为 $ \\theta \\sim \\mathcal{N}(0, \\tau^{2} I) $，已知方variance $ \\tau^{2} > 0 $。\n- 令用于合成数据的真实参数为 $ \\theta^{\\star} = [1, -0.5, 0.3, 0, 0.05]^{\\top} $。为保证可复现性，令实现的观测为无噪声的，即在所有测试案例中设置 $ \\epsilon = 0 $，因此 $ y = A \\theta^{\\star} $。\n\n后验刻画与MAP：\n- 在上述模型下，计算后验分布 $ p(\\theta \\mid y) $ 和最大后验（MAP）估计 $ \\hat{\\theta}_{\\mathrm{MAP}} $。您不能假定任何快捷公式；必须通过贝叶斯定理和多元高斯分布的标准性质，结合似然和先验来推导后验的形式。\n\n不确定性方向：\n- 令后验协方差表示为 $ \\Sigma_{\\mathrm{post}} \\in \\mathbb{R}^{d \\times d} $。计算其特征值和特征向量。将最不确定的方向 $ h \\in \\mathbb{R}^{d} $ 定义为与 $ \\Sigma_{\\mathrm{post}} $ 的最大特征值相关联的单位范数特征向量。为消除符号模糊性，重新定向 $ h $，使其满足 $ h^{\\top} \\mu_{\\mathrm{post}} \\ge 0 $，其中 $ \\mu_{\\mathrm{post}} $ 是后验均值。\n- 报告 $ A $ 在 $ 2 $-范数下的谱条件数，记为 $ \\kappa_{2}(A) $，以及 $ \\Sigma_{\\mathrm{post}} $ 的最大和最小特征值。这些量化了前向算子的病态性和后验分布的离散程度。\n\n后验不确定性起作用的决策问题：\n- 考虑一个二元决策 $ u \\in \\{0,1\\} $。定义关注的标量 $ s = h^{\\top} \\theta $ 和一个阈值 $ t \\in \\mathbb{R} $。决策的损失为：\n  - 如果 $ u = 1 $ 且 $ s  t $（假阳性），产生损失 $ C_{\\mathrm{fp}} $。\n  - 如果 $ u = 0 $ 且 $ s \\ge t $（假阴性），产生损失 $ C_{\\mathrm{fn}} $。\n  - 否则，损失为零。\n- 必须生成两种决策规则：\n  - 基于MAP的“插件”规则：如果 $ h^{\\top} \\hat{\\theta}_{\\mathrm{MAP}} \\ge t $，则决策 $ u_{\\mathrm{MAP}} = 1 $，否则 $ u_{\\mathrm{MAP}} = 0 $。\n  - 考虑后验不确定性的贝叶斯最优规则：使用 $ p(\\theta \\mid y) $ 决定 $ u_{\\mathrm{Bayes}} $，以最小化后验期望损失。\n- 同时计算 $ s $ 的后验标准差，记为 $ \\sigma_{s} = \\sqrt{ h^{\\top} \\Sigma_{\\mathrm{post}} h } $，以及后验概率 $ p = \\mathbb{P}( s \\ge t \\mid y ) $。\n\n测试套件：\n使用上面定义的共享 $ A $、$ \\theta^{\\star} $ 和 $ y $，并评估以下三种参数设置：\n- 案例1：$ \\sigma^{2} = 10^{-4} $，$ \\tau^{2} = 1 $，$ t = 0.04 $，$ C_{\\mathrm{fp}} = 0.25 $，$ C_{\\mathrm{fn}} = 1 $。\n- 案例2：$ \\sigma^{2} = 10^{-2} $，$ \\tau^{2} = 10^{-2} $，$ t = 0.02 $，$ C_{\\mathrm{fp}} = 0.5 $，$ C_{\\mathrm{fn}} = 0.5 $。\n- 案例3：$ \\sigma^{2} = 10^{-1} $，$ \\tau^{2} = 10 $，$ t = 0.03 $，$ C_{\\mathrm{fp}} = 0.1 $，$ C_{\\mathrm{fn}} = 0.9 $。\n\n每个案例的所需输出：\n对每个案例，按顺序输出以下列表：\n$[\\kappa_{2}(A), \\lambda_{\\max}(\\Sigma_{\\mathrm{post}}), \\lambda_{\\min}(\\Sigma_{\\mathrm{post}}), h^{\\top} \\hat{\\theta}_{\\mathrm{MAP}}, \\sigma_{s}, \\mathbb{P}(s \\ge t \\mid y), u_{\\mathrm{MAP}}, u_{\\mathrm{Bayes}}]$,\n其中 $ \\lambda_{\\max}(\\cdot) $ 和 $ \\lambda_{\\min}(\\cdot) $ 分别表示最大和最小特征值。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的三个案例列表，并用一对单独的方括号括起来，例如，$[[\\dots],[\\dots],[\\dots]]$。所有布尔值必须打印为编程语言的布尔字面量。不应打印任何额外文本。此问题不涉及任何物理单位。\n\n您的实现必须遵守执行环境的限制，并且不得读取任何输入。",
            "solution": "用户提供的问题是一个在贝叶斯逆问题和决策理论中定义明确的标准练习。它具有科学依据，完全指定，并且计算上可行。该问题是有效的。\n\n### 1. 模型与后验分布\n\n该问题指定了一个具有高斯结构的线性逆问题。\n- 未知参数 $\\theta \\in \\mathbb{R}^d$ 的**先验分布**是高斯分布：\n$$ p(\\theta) = \\mathcal{N}(\\theta; 0, \\tau^2 I_d) \\propto \\exp\\left(-\\frac{1}{2\\tau^2} \\theta^\\top\\theta\\right) $$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵（$d=5$）。\n\n- 给定 $\\theta$ 时，观测值 $y \\in \\mathbb{R}^m$ 的**似然**源自观测模型 $y = A\\theta + \\epsilon$，其中噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$：\n$$ p(y|\\theta) = \\mathcal{N}(y; A\\theta, \\sigma^2 I_m) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - A\\theta)^\\top(y - A\\theta)\\right) $$\n其中 $I_m$ 是 $m \\times m$ 的单位矩阵（$m=5$）。\n\n根据贝叶斯定理，**后验分布** $p(\\theta|y)$ 与似然和先验的乘积成正比：\n$$ p(\\theta|y) \\propto p(y|\\theta) p(\\theta) $$\n$$ p(\\theta|y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - A\\theta)^\\top(y - A\\theta) - \\frac{1}{2\\tau^2} \\theta^\\top\\theta\\right) $$\n指数中的项，我们可以表示为 $-J(\\theta)/2$，是 $\\theta$ 的二次函数。这意味着后验分布也是高斯分布，即 $p(\\theta|y) = \\mathcal{N}(\\theta; \\mu_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}})$。为了找到其均值 $\\mu_{\\mathrm{post}}$ 和协方差 $\\Sigma_{\\mathrm{post}}$，我们对指数的参数进行配方：\n$$ J(\\theta) = \\frac{1}{\\sigma^2}(y^\\top y - 2y^\\top A\\theta + \\theta^\\top A^\\top A\\theta) + \\frac{1}{\\tau^2}\\theta^\\top\\theta $$\n$$ J(\\theta) = \\theta^\\top\\left(\\frac{1}{\\sigma^2}A^\\top A + \\frac{1}{\\tau^2}I_d\\right)\\theta - \\frac{2}{\\sigma^2}y^\\top A\\theta + \\text{const.} $$\n将此与高斯分布的标准非归一化对数后验 $\\frac{1}{2}(\\theta - \\mu_{\\mathrm{post}})^\\top \\Sigma_{\\mathrm{post}}^{-1} (\\theta - \\mu_{\\mathrm{post}}) = \\frac{1}{2}\\theta^\\top\\Sigma_{\\mathrm{post}}^{-1}\\theta - \\theta^\\top\\Sigma_{\\mathrm{post}}^{-1}\\mu_{\\mathrm{post}} + \\text{const.}$ 进行比较，我们可以确定后验精度矩阵（协方差的逆）和均值。\n**后验精度**为：\n$$ \\Sigma_{\\mathrm{post}}^{-1} = \\frac{1}{\\sigma^2}A^\\top A + \\frac{1}{\\tau^2}I_d $$\n因此，**后验协方差**为：\n$$ \\Sigma_{\\mathrm{post}} = \\left(\\frac{1}{\\sigma^2}A^\\top A + \\frac{1}{\\tau^2}I_d\\right)^{-1} $$\n而**后验均值**为：\n$$ \\mu_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}}\\left(\\frac{1}{\\sigma^2}A^\\top y\\right) $$\n对于高斯分布，最大后验（MAP）估计等于后验均值，即 $\\hat{\\theta}_{\\mathrm{MAP}} = \\mu_{\\mathrm{post}}$。\n\n### 2. 不确定性与决策\n\n该问题要求分析方向 $h$ 上的不确定性，$h$ 被定义为与后验协方差 $\\Sigma_{\\mathrm{post}}$ 的最大特征值 $\\lambda_{\\max}$ 相关联的单位范数特征向量。关注的量是标量投影 $s = h^\\top\\theta$。因为 $\\theta|y$ 是高斯分布的，所以 $s|y$ 是一个标量高斯变量：\n$$ s|y \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2) $$\n其中均值为 $\\mu_s = h^\\top\\mu_{\\mathrm{post}}$，方差为 $\\sigma_s^2 = h^\\top\\Sigma_{\\mathrm{post}}h$。由于 $h$ 是 $\\Sigma_{\\mathrm{post}}$ 对应于特征值 $\\lambda_{\\max}$ 的特征向量，这可以简化为 $\\sigma_s^2 = h^\\top(\\lambda_{\\max}h) = \\lambda_{\\max}(h^\\top h) = \\lambda_{\\max}$，因为 $h$ 是一个单位向量。因此，$s$ 的后验标准差是 $\\sigma_s = \\sqrt{\\lambda_{\\max}(\\Sigma_{\\mathrm{post}})}$。\n\n我们必须比较两种关于选择 $u \\in \\{0,1\\}$ 的决策规则：\n1.  **基于MAP的“插件”规则**：此规则忽略后验不确定性，并使用点估计 $\\hat{\\theta}_{\\mathrm{MAP}}$。如果 $h^\\top\\hat{\\theta}_{\\mathrm{MAP}} \\ge t$，则决策为 $u_{\\mathrm{MAP}} = 1$，否则 $u_{\\mathrm{MAP}}=0$。\n2.  **贝叶斯最优规则**：此规则最小化后验期望损失。对于行动 $u=1$ 和 $u=0$ 的期望损失分别为：\n    $$ \\mathbb{E}[L(u=1, s)|y] = C_{\\mathrm{fp}}\\mathbb{P}(s  t | y) $$\n    $$ \\mathbb{E}[L(u=0, s)|y] = C_{\\mathrm{fn}}\\mathbb{P}(s \\ge t | y) $$\n    令 $p = \\mathbb{P}(s \\ge t | y)$，因此 $\\mathbb{P}(s  t | y) = 1-p$。如果行动 $u=1$ 的期望损失更低，即 $C_{\\mathrm{fp}}(1-p)  C_{\\mathrm{fn}}p$，则最优决策是 $u=1$。重新整理此式可得决策准则：\n    $$ u_{\\mathrm{Bayes}} = 1 \\iff p > \\frac{C_{\\mathrm{fp}}}{C_{\\mathrm{fp}} + C_{\\mathrm{fn}}} $$\n    这个规则明确地使用了后验概率 $p$，充分考虑了 $s$ 的全部后验不确定性。\n\n### 3. 实现细节\n\n前向算子 $A$ 是 $5 \\times 5$ 的希尔伯特矩阵，已知是病态的。其条件数 $\\kappa_2(A)$ 衡量了这种不稳定性。为保证可复现性，观测值 $y$ 是在没有噪声的情况下合成的（$y = A\\theta^\\star$）。\n\n对每个测试案例，算法按以下步骤进行：\n1.  给定 $\\sigma^2$ 和 $\\tau^2$，计算 $\\mu_{\\mathrm{post}}$ 和 $\\Sigma_{\\mathrm{post}}$。\n2.  对 $\\Sigma_{\\mathrm{post}}$ 进行特征分解，以找到其特征值（$\\lambda_{\\min}, \\lambda_{\\max}$）和与 $\\lambda_{\\max}$ 对应的特征向量 $h$。调整 $h$ 的方向，使其满足 $h^\\top\\mu_{\\mathrm{post}} \\ge 0$。\n3.  计算 $s$ 分布的参数：$\\mu_s = h^\\top\\mu_{\\mathrm{post}}$ 和 $\\sigma_s = \\sqrt{\\lambda_{\\max}}$。\n4.  使用正态分布的累积分布函数（CDF）计算概率 $p = \\mathbb{P}(s \\ge t | y)$：$p = 1 - \\Phi\\left(\\frac{t-\\mu_s}{\\sigma_s}\\right)$。\n5.  根据各自的规则确定决策 $u_{\\mathrm{MAP}}$ 和 $u_{\\mathrm{Bayes}}$。\n6.  收集并格式化所有必需的输出。\n\n$A$ 的病态性会传播到后验分布中，导致较高的不确定性，尤其是在与 $A$ 的小奇异值对应的方向上。先验提供的正则化确保了后验分布是良态的，但不确定性可能仍然很大，这使得基于MAP的决策和贝叶斯最优决策之间的区别变得显著，尤其是在决策成本不对称时。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport sys\nimport io\n\n# Ensure the output uses full precision representation compatible with the autograder.\n# This prevents truncation of float values.\n# np.set_printoptions(threshold=sys.maxsize, floatmode='maxprec')\n# After review, repr() is a cleaner and more standard way to achieve this.\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inverse problem and decision task as specified.\n    \"\"\"\n    # =========================================================================\n    # Model Specification and One-Time Computations\n    # =========================================================================\n    d = 5\n    m = 5\n    # The Hilbert matrix A_ij = 1/(i+j-1) for 1-based indexing.\n    # For 0-based indexing i, j, this is 1/((i+1)+(j+1)-1) = 1/(i+j+1).\n    A = np.fromfunction(lambda i, j: 1 / (i + j + 1), (m, d))\n\n    # Spectral condition number of A (2-norm).\n    kappa2_A = np.linalg.cond(A, 2)\n\n    # True parameter vector and synthesized noise-free observation.\n    theta_star = np.array([1.0, -0.5, 0.3, 0.0, 0.05])\n    y = A @ theta_star\n\n    # =========================================================================\n    # Test Suite Definition\n    # =========================================================================\n    test_cases = [\n        # (sigma^2, tau^2, t, C_fp, C_fn)\n        (1e-4, 1.0, 0.04, 0.25, 1.0),\n        (1e-2, 1e-2, 0.02, 0.5, 0.5),\n        (1e-1, 10.0, 0.03, 0.1, 0.9),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        sigma2, tau2, t, C_fp, C_fn = case\n        \n        # =====================================================================\n        # 1. Posterior Characterization\n        # =====================================================================\n        # The posterior is p(theta|y) ~ N(mu_post, Sigma_post).\n        # We use the formulation derived from first principles:\n        # mu_post = (A^T A + (sigma2/tau2)I)^-1 A^T y\n        # Sigma_post = sigma2 * (A^T A + (sigma2/tau2)I)^-1\n        alpha = sigma2 / tau2\n        \n        # Matrix to be inverted. This is proportional to the posterior precision.\n        M = A.T @ A + alpha * np.identity(d)\n        M_inv = np.linalg.inv(M)\n        \n        # Posterior mean (which is also the MAP estimate).\n        mu_post = M_inv @ A.T @ y\n        theta_map = mu_post\n        \n        # Posterior covariance.\n        Sigma_post = sigma2 * M_inv\n\n        # =====================================================================\n        # 2. Uncertainty Directions and Eigenanalysis\n        # =====================================================================\n        # Eigen decomposition of the symmetric posterior covariance matrix.\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(Sigma_post)\n        \n        lambda_min_post = eigenvalues[0]\n        lambda_max_post = eigenvalues[-1]\n        \n        # h is the unit-norm eigenvector associated with the largest eigenvalue.\n        h = eigenvectors[:, -1]\n        \n        # Re-orient h such that h^T * mu_post >= 0.\n        if h.T @ mu_post  0:\n            h = -h\n            \n        # =====================================================================\n        # 3. Decision Problem Quantities\n        # =====================================================================\n        # Quantity of interest s = h^T * theta.\n        # Since theta is Gaussian, s is also Gaussian: s | y ~ N(mu_s, sigma_s^2).\n        \n        # Expected value of s. This is also the plug-in MAP value, h^T * theta_map.\n        mu_s = h.T @ mu_post\n        \n        # Standard deviation of s. As h is an eigenvector of Sigma_post with\n        # eigenvalue lambda_max_post, h^T Sigma_post h = lambda_max_post for unit h.\n        sigma_s = np.sqrt(lambda_max_post)\n        \n        # Posterior probability P(s >= t | y).\n        # We use the survival function sf(x) = 1 - cdf(x) for better numerical precision.\n        prob_s_ge_t = norm.sf(t, loc=mu_s, scale=sigma_s)\n\n        # =====================================================================\n        # 4. Decision Rules\n        # =====================================================================\n        # MAP-based plug-in rule: u=1 if h^T * theta_map >= t, else 0.\n        u_map = 1 if mu_s >= t else 0\n        \n        # Bayes-optimal rule: minimize posterior expected loss.\n        # This is equivalent to u=1 if P(s >= t | y) > C_fp / (C_fp + C_fn).\n        p_thresh = C_fp / (C_fp + C_fn)\n        u_bayes = 1 if prob_s_ge_t > p_thresh else 0\n\n        # =====================================================================\n        # 5. Collect Results\n        # =====================================================================\n        case_results = [\n            kappa2_A,\n            lambda_max_post,\n            lambda_min_post,\n            mu_s, \n            sigma_s,\n            prob_s_ge_t,\n            u_map,\n            u_bayes\n        ]\n        all_results.append(case_results)\n\n    # =========================================================================\n    # Final Output Formatting\n    # =========================================================================\n    # The required format is a string representation of a list of lists of results,\n    # with no spaces and using standard numeric representations (e.g., via repr()).\n    inner_lists_str = [f\"[{','.join(map(repr, res))}]\" for res in all_results]\n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output_str)\n\n# Execute the solution.\nsolve()\n```"
        },
        {
            "introduction": "线性问题中的不确定性通常表现为后验分布的“宽度”，而非线性问题则可能引入更根本的复杂性，例如多峰性。本练习将通过一个简单的非单射（non-injective）二次模型，向您展示后验分布如何可能出现多个同样合理的模式，而最大后验（MAP）估计本质上只能选择其中之一。通过对比基于MAP的点估计和基于完整后验积分的预测分布，您将量化地评估在存在多峰性时，点估计对模型预测能力的损害。",
            "id": "3383451",
            "problem": "考虑一个标量非线性逆问题，其观测模型为 $y = f(\\theta) + \\epsilon$，其中参数 $\\theta \\in \\mathbb{R}$，正演映射为 $f(\\theta) = \\theta^2$，噪声 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 且与 $\\theta$ 无关。假设先验为 $\\theta \\sim \\mathcal{N}(0,\\tau^2)$。该模型是非单射的，因为 $f(\\theta) = f(-\\theta)$，这在观测数据 $y$ 包含足够信息时，可能导致 $\\theta$ 的后验分布呈现多峰性。使用以下基本原理：用于后验的贝叶斯法则 $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$ 和用于后验预测的全概率定律 $p(y^\\ast \\mid y) = \\int p(y^\\ast \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$，其中 $y^\\ast$ 表示在相同数据模型下的未来观测值。\n\n你的任务是：\n- 精确定义最大后验（MAP）估计量 $\\hat{\\theta}_{MAP}$，作为后验密度 $p(\\theta \\mid y)$ 的最大化者，并确定在 $\\hat{\\theta}_{MAP}$ 处使用点质量的相应插件式后验预测 $p_{MAP}(y^\\ast \\mid y)$。\n- 通过对 $p(\\theta \\mid y)$ 进行边缘化，精确定义完全积分后验预测 $p_{int}(y^\\ast \\mid y)$，并仅使用从贝叶斯法则和全概率定律导出的良定义积分和归一化常数来表达它。\n- 实现一个稳健的数值程序，用于对给定的 $(y,\\sigma,\\tau)$ 和一个有限的评估点集 $\\{y_k^\\ast\\}_{k=1}^K$，计算平均对数预测密度差\n$$\n\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\}) \\;=\\; \\frac{1}{K} \\sum_{k=1}^K \\left[ \\log p_{int}(y_k^\\ast \\mid y) \\;-\\; \\log p_{MAP}(y_k^\\ast \\mid y) \\right].\n$$\n评估必须以科学上合理且数值稳定的方式进行，不得借助任何基于采样的近似方法。\n\n你可以使用的基本定义（必须基于这些定义进行推导，不得跳过）：\n- 似然 $p(y \\mid \\theta)$ 是均值为 $\\theta^2$、方差为 $\\sigma^2$ 的高斯分布，即 $p(y \\mid \\theta) = \\mathcal{N}(y;\\theta^2,\\sigma^2)$。\n- 先验 $p(\\theta)$ 是均值为 $0$、方差为 $\\tau^2$ 的高斯分布，即 $p(\\theta) = \\mathcal{N}(\\theta;0,\\tau^2)$。\n- 后验 $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$。\n- MAP 估计量 $\\hat{\\theta}_{MAP}$ 是 $p(\\theta \\mid y)$ 的最大化者。\n- 在点估计 $\\hat{\\theta}$ 下的插件式预测为 $p(y^\\ast \\mid y) \\approx p(y^\\ast \\mid \\theta=\\hat{\\theta})$。\n- 完全积分预测为 $p(y^\\ast \\mid y) = \\int p(y^\\ast \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$。\n\n针对以下参数值和评估网格的测试套件，报告平均对数预测密度差 $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$ 的比较结果：\n- 情况 A（双峰，低噪声）：$(y,\\sigma,\\tau) = (4.0, 0.2, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [3.0, 4.0, 5.0]$。\n- 情况 B（众数近乎合并）：$(y,\\sigma,\\tau) = (0.5, 0.2, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [0.1, 0.5, 1.0]$。\n- 情况 C（双峰，较高噪声）：$(y,\\sigma,\\tau) = (4.0, 1.5, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [0.0, 2.0, 4.0, 8.0]$。\n- 情况 D（零点边缘情况）：$(y,\\sigma,\\tau) = (0.0, 0.2, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [0.0, 0.2, 1.0]$。\n\n数值和算法要求：\n- 所有量均为无量纲（不需要物理单位）。\n- 不涉及角度；不需要角度单位。\n- 对于每种情况，假设在 $\\hat{\\theta}_{MAP}$ 处为点质量来计算基于 MAP 的预测，并通过仅使用确定性数值积分对 $\\theta \\in \\mathbb{R}$ 进行数值边缘化来计算完全积分预测。利用任何可以从第一性原理证明的对称性来提高数值稳定性。\n- 对于每种情况，返回单个浮点数 $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$，四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$，每个 $\\Delta$ 对应上述相应的情况，并四舍五入到 $6$ 位小数，例如 $[0.123456,0.234567,0.345678,0.456789]$。",
            "solution": "我们从作为基本原理的贝叶斯法则和全概率定律出发。模型为 $y = \\theta^2 + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$，先验为 $\\theta \\sim \\mathcal{N}(0,\\tau^2)$。因此，似然为 $p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2}\\right)$，先验为 $p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\tau}\\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$。后验为 $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$，由此得到未归一化的对数后验\n$$\n\\ell(\\theta; y,\\sigma,\\tau) \\;=\\; -\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2} + \\text{constant}.\n$$\n因为 $f(\\theta)=\\theta^2$ 是偶函数，所以似然函数关于 $\\theta$ 是偶函数，并且在对称高斯先验下，后验分布也是偶函数。这意味着对于信息足够充分的数据值 $y$（相对于 $\\sigma$ 和 $\\tau$），后验分布在 $\\pm \\theta^\\star$ 处有两个对称的众数；这就是多峰性的来源。\n\n为了获得最大后验（MAP）估计量 $\\hat{\\theta}_{MAP}$，我们最大化 $\\ell(\\theta;y,\\sigma,\\tau)$。其导数为\n$$\n\\frac{d\\ell}{d\\theta}(\\theta) = \\frac{2\\theta}{\\sigma^2}(y-\\theta^2) - \\frac{\\theta}{\\tau^2} \\;=\\; \\theta\\left[\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2}\\right].\n$$\n驻点出现在 $\\theta = 0$ 或 $\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2} = 0$ 时，即\n$$\n\\theta^2 \\;=\\; y - \\frac{\\sigma^2}{2\\tau^2}.\n$$\n二阶导数为\n$$\n\\frac{d^2\\ell}{d\\theta^2}(\\theta) = \\left[\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2}\\right] - \\frac{4\\theta^2}{\\sigma^2}.\n$$\n对于驻点处的 $\\theta \\neq 0$，方括号中的项消失，且 $\\frac{d^2\\ell}{d\\theta^2} = -\\frac{4\\theta^2}{\\sigma^2}  0$，因此这些点是局部极大值点。在 $\\theta=0$ 处，曲率为 $\\frac{d^2\\ell}{d\\theta^2}(0) = \\frac{2y}{\\sigma^2} - \\frac{1}{\\tau^2}$，如果 $y \\le \\frac{\\sigma^2}{2\\tau^2}$，则其为负（在 $\\theta=0$ 处为局部极大值），如果 $y > \\frac{\\sigma^2}{2\\tau^2}$，则其为正（局部极小值）。因此：\n- 如果 $y \\le \\frac{\\sigma^2}{2\\tau^2}$，则 $\\hat{\\theta}_{MAP} = 0$。\n- 如果 $y > \\frac{\\sigma^2}{2\\tau^2}$，则在 $\\pm \\sqrt{y - \\frac{\\sigma^2}{2\\tau^2}}$ 处存在两个对称的 MAP 值。为了确定性选择，我们选择正的最大化者 $\\hat{\\theta}_{MAP} = +\\sqrt{y - \\frac{\\sigma^2}{2\\tau^2}}$。\n\n给定一个点估计 $\\hat{\\theta}$，插件式后验预测为\n$$\np_{MAP}(y^\\ast \\mid y) \\approx p(y^\\ast \\mid \\theta = \\hat{\\theta}_{MAP}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^\\ast - \\hat{\\theta}_{MAP}^2)^2}{2\\sigma^2}\\right).\n$$\n这对应于将后验不确定性坍缩到参数空间中的单个点 $\\hat{\\theta}_{MAP}$ 上，这在这个非线性模型中会将预测不确定性坍缩为仅有观测噪声方差。\n\n根据全概率定律，完全积分后验预测为\n$$\np_{int}(y^\\ast \\mid y) \\;=\\; \\int_{\\mathbb{R}} p(y^\\ast \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta \\;=\\; \\frac{\\int_{\\mathbb{R}} p(y^\\ast \\mid \\theta)\\, p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}{\\int_{\\mathbb{R}} p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}.\n$$\n代入高斯形式并仔细消去常数，得到\n$$\np_{int}(y^\\ast \\mid y) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\sigma} \\cdot \\frac{\\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta}{\\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta}.\n$$\n根据对称性，每个被积函数都是关于 $\\theta$ 的偶函数，因此我们可以将每个积分计算为在 $[0,\\infty)$ 上积分值的两倍，这可以提高数值稳定性。\n\n算法设计：\n- 使用上述驻点分析计算 $\\hat{\\theta}_{MAP}$。插件式预测 $p_{MAP}(y^\\ast \\mid y)$ 则是一个均值为 $\\hat{\\theta}_{MAP}^2$、方差为 $\\sigma^2$ 的高斯密度。\n- 对于积分预测，评估分母积分\n$$\nZ(y) = \\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta = 2 \\int_{0}^{\\infty} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta,\n$$\n以及对每个 $y^\\ast$ 评估分子积分\n$$\nN(y^\\ast,y) = \\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta = 2 \\int_{0}^{\\infty} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta.\n$$\n使用确定性数值求积法，并选择适当的绝对和相对容差以确保准确性。因为被积函数中源于 $(y-\\theta^2)^2$ 的 $\\theta^4$ 项使得函数在 $\\theta \\to \\infty$ 时超指数衰减，所以在 $[0,\\infty)$ 上的数值积分是稳定的。\n\n- 计算 $\\log p_{int}(y^\\ast \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) + \\log N(y^\\ast,y) - \\log Z(y)$ 和 $\\log p_{MAP}(y^\\ast \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(y^\\ast-\\hat{\\theta}_{MAP}^2)^2}{2\\sigma^2}$。\n- 在给定的网格 $\\{y_k^\\ast\\}_{k=1}^K$ 上对它们的差值求平均，以获得 $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$。\n\n测试套件执行：\n- 情况 A：$(y,\\sigma,\\tau) = (4.0, 0.2, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [3.0, 4.0, 5.0]$。\n- 情况 B：$(y,\\sigma,\\tau) = (0.5, 0.2, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [0.1, 0.5, 1.0]$。\n- 情况 C：$(y,\\sigma,\\tau) = (4.0, 1.5, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [0.0, 2.0, 4.0, 8.0]$。\n- 情况 D：$(y,\\sigma,\\tau) = (0.0, 0.2, 2.0)$，评估点集为 $\\{y_k^\\ast\\} = [0.0, 0.2, 1.0]$。\n\n最后，将四个平均差值汇总成单行输出 $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$，每个值四舍五入到 $6$ 位小数。该比较直接量化了在一个多峰逆问题中，完整的后验表征相对于点估计在后验预测性能上的优势（如果存在的话），并分离出对 $p(\\theta \\mid y)$ 中的众数进行积分所起的作用。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sqrt, log, pi\nfrom scipy import integrate\n\ndef theta_map_squared(y, sigma, tau):\n    # Closed-form from stationary condition:\n    # theta_MAP^2 = max(y - sigma^2/(2*tau^2), 0)\n    val = y - (sigma**2) / (2.0 * (tau**2))\n    return max(val, 0.0)\n\ndef log_norm_pdf(x, mean, std):\n    return -0.5*log(2.0*pi) - log(std) - 0.5*((x - mean)/std)**2\n\ndef denom_integrand(theta, y, sigma, tau):\n    # Unnormalized posterior integrand: exp(- (y - theta^2)^2/(2*sigma^2) - theta^2/(2*tau^2))\n    t2 = theta*theta\n    return np.exp(- ((y - t2)**2) / (2.0*sigma**2) - t2 / (2.0*tau**2))\n\ndef numer_integrand(theta, y, ystar, sigma, tau):\n    # Numerator integrand for predictive: exp(- (y - theta^2)^2/(2*sigma^2) - (ystar - theta^2)^2/(2*sigma^2) - theta^2/(2*tau^2))\n    t2 = theta*theta\n    return np.exp(- ((y - t2)**2) / (2.0*sigma**2) - ((ystar - t2)**2) / (2.0*sigma**2) - t2 / (2.0*tau**2))\n\ndef log_pint(y, ystar, sigma, tau):\n    # Compute log p_int(ystar | y) = -0.5*log(2*pi) - log(sigma) + log(num) - log(den)\n    # Using symmetry: integrate from 0 to inf and double\n    den_half, den_err = integrate.quad(lambda th: denom_integrand(th, y, sigma, tau), 0.0, np.inf, epsabs=1e-10, epsrel=1e-9, limit=500)\n    num_half, num_err = integrate.quad(lambda th: numer_integrand(th, y, ystar, sigma, tau), 0.0, np.inf, epsabs=1e-10, epsrel=1e-9, limit=500)\n    den = 2.0 * den_half\n    num = 2.0 * num_half\n    # Safety checks to avoid log(0)\n    if den == 0.0 or num == 0.0:\n        # In rare numerical issues, return -np.inf\n        return -np.inf\n    return -0.5*log(2.0*pi) - log(sigma) + log(num) - log(den)\n\ndef average_logscore_difference(y, sigma, tau, ystars):\n    # Compute MAP-based log predictive at each y* and integrated log predictive, then average difference\n    theta2_map = theta_map_squared(y, sigma, tau)\n    diffs = []\n    for ys in ystars:\n        log_p_map = log_norm_pdf(ys, theta2_map, sigma)\n        log_p_int = log_pint(y, ys, sigma, tau)\n        diffs.append(log_p_int - log_p_map)\n    return float(np.mean(diffs))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (y, sigma, tau, [y_star values])\n    test_cases = [\n        (4.0, 0.2, 2.0, [3.0, 4.0, 5.0]),           # Case A\n        (0.5, 0.2, 2.0, [0.1, 0.5, 1.0]),           # Case B\n        (4.0, 1.5, 2.0, [0.0, 2.0, 4.0, 8.0]),      # Case C\n        (0.0, 0.2, 2.0, [0.0, 0.2, 1.0]),           # Case D\n    ]\n\n    results = []\n    for (y, sigma, tau, ystars) in test_cases:\n        delta = average_logscore_difference(y, sigma, tau, ystars)\n        # Round to 6 decimals as required\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}