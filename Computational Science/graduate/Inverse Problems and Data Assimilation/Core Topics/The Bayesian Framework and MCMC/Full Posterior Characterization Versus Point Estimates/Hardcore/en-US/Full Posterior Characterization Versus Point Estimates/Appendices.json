{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we first build a foundational understanding of how data, or a lack thereof, shapes our knowledge about unknown parameters. This exercise  uses a carefully constructed linear inverse problem with a rank-deficient forward operator to analytically demonstrate how posterior uncertainty behaves. You will derive how the posterior variance remains large in directions unconstrained by the data—the nullspace of the operator—while it shrinks in data-informed directions, providing a crystal-clear illustration of why a single point estimate is insufficient as it hides these vast regions of uncertainty.",
            "id": "3383422",
            "problem": "Consider a linear Gaussian inverse problem with state vector $x \\in \\mathbb{R}^{3}$, observation vector $y \\in \\mathbb{R}^{2}$, forward operator $A \\in \\mathbb{R}^{2 \\times 3}$, and additive noise $\\epsilon \\in \\mathbb{R}^{2}$. The data model is $y = A x + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma = \\sigma^{2} I_{2}$ for a given $\\sigma^{2} > 0$. The prior on $x$ is Gaussian $x \\sim \\mathcal{N}(m_{0}, C_{0})$ with mean $m_{0} = (m_{1}, m_{2}, m_{3})^{\\top}$ and covariance\n$$\nC_{0} = \\mathrm{diag}(\\beta^{2}, \\tau^{2}, \\tau^{2}),\n$$\nwhere $\\beta^{2} > 0$ and $\\tau^{2} > 0$ are given scalars. Let the forward operator be\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nwhich is rank-deficient.\n\nStarting from Bayes’ rule and the definitions of Gaussian densities, derive the posterior covariance matrix of $x$ given $y$. Then, characterize the posterior uncertainty along the subspace $\\mathrm{Null}(A)$ and contrast it with the uncertainty along the data-informed direction. Specifically:\n\n- Compute the posterior covariance in closed form and identify $\\mathrm{Null}(A)$.\n- Let $v \\in \\mathrm{Null}(A)$ be any unit vector. Compute the posterior variance $v^{\\top} S v$, where $S$ denotes the posterior covariance.\n- Compute the posterior variance along the data-informed direction $e_{1} = (1, 0, 0)^{\\top}$.\n- Define the anisotropy (inflation) factor\n$$\nR := \\frac{\\text{posterior variance along } \\mathrm{Null}(A)}{\\text{posterior variance along the data-informed direction}} = \\frac{v^{\\top} S v}{e_{1}^{\\top} S e_{1}},\n$$\nand express $R$ in terms of $\\beta^{2}$, $\\tau^{2}$, and $\\sigma^{2}$ only.\n\nFinally, compute the Maximum A Posteriori (MAP) estimate of $x$ and explain, using your derived formulas, how the full posterior characterization reveals large uncertainty along $\\mathrm{Null}(A)$ that is not conveyed by the single MAP solution.\n\nYour final answer must be the closed-form analytic expression for $R$ with no numerical substitution or rounding.",
            "solution": "We begin from Bayes’ rule $p(x \\mid y) \\propto p(y \\mid x)\\,p(x)$, and the definitions of Gaussian densities. The likelihood is $p(y \\mid x) = \\mathcal{N}(A x, \\Gamma)$ with $\\Gamma = \\sigma^{2} I_{2}$, and the prior is $p(x) = \\mathcal{N}(m_{0}, C_{0})$ with $C_{0} = \\mathrm{diag}(\\beta^{2}, \\tau^{2}, \\tau^{2})$.\n\nA standard consequence of completing the square for the product of Gaussian densities in a linear model $y = A x + \\epsilon$ is that the posterior $p(x \\mid y)$ is Gaussian with precision (the inverse covariance) equal to the sum of the prior precision and the data precision pulled back to parameter space. Concretely, the posterior covariance $S$ satisfies\n$$\nS^{-1} = C_{0}^{-1} + A^{\\top} \\Gamma^{-1} A,\n$$\nand the posterior mean $m$ is\n$$\nm = S\\left(C_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y \\right).\n$$\nWe derive $S$ explicitly for the given $A$, $\\Gamma$, and $C_{0}$.\n\nFirst, compute the prior precision:\n$$\nC_{0}^{-1} = \\mathrm{diag}\\!\\left(\\beta^{-2}, \\tau^{-2}, \\tau^{-2}\\right).\n$$\nNext, compute the data precision contribution:\n$$\n\\Gamma^{-1} = \\sigma^{-2} I_{2}, \\quad A^{\\top} \\Gamma^{-1} A = \\sigma^{-2} A^{\\top} A.\n$$\nSince\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}, \\quad\nA^{\\top} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix},\n$$\nwe have\n$$\nA^{\\top} A = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nand therefore\n$$\nA^{\\top} \\Gamma^{-1} A = \\sigma^{-2} \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nHence,\n$$\nS^{-1} = \\mathrm{diag}\\!\\left(\\beta^{-2}, \\tau^{-2}, \\tau^{-2}\\right) + \\sigma^{-2} \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\beta^{-2} + \\sigma^{-2} & 0 & 0 \\\\\n0 & \\tau^{-2} & 0 \\\\\n0 & 0 & \\tau^{-2}\n\\end{pmatrix}.\n$$\nInverting this diagonal matrix yields the posterior covariance\n$$\nS = \\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} & 0 & 0 \\\\\n0 & \\tau^{2} & 0 \\\\\n0 & 0 & \\tau^{2}\n\\end{pmatrix}.\n$$\n\nWe now identify the nullspace of $A$. The equation $A x = 0$ reads\n$$\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix},\n$$\nwhich implies $x_{1} = 0$ and leaves $x_{2}$ and $x_{3}$ free. Therefore,\n$$\n\\mathrm{Null}(A) = \\left\\{ \\begin{pmatrix} 0 \\\\ t \\\\ s \\end{pmatrix} : t, s \\in \\mathbb{R} \\right\\}.\n$$\n\nLet $v \\in \\mathrm{Null}(A)$ be any unit vector. Since $S$ is diagonal with entries $\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}$, $\\tau^{2}$, and $\\tau^{2}$, and $v$ has zero in the first coordinate, we can write $v = (0, v_{2}, v_{3})^{\\top}$ with $v_{2}^{2} + v_{3}^{2} = 1$. Then\n$$\nv^{\\top} S v = \\begin{pmatrix} 0 & v_{2} & v_{3} \\end{pmatrix}\n\\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} & 0 & 0 \\\\\n0 & \\tau^{2} & 0 \\\\\n0 & 0 & \\tau^{2}\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ v_{2} \\\\ v_{3} \\end{pmatrix}\n= \\tau^{2} v_{2}^{2} + \\tau^{2} v_{3}^{2} = \\tau^{2}.\n$$\nThus, the posterior variance along any unit direction in $\\mathrm{Null}(A)$ equals $\\tau^{2}$.\n\nAlong the data-informed direction $e_{1} = (1, 0, 0)^{\\top}$, we have\n$$\ne_{1}^{\\top} S e_{1} = \\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}.\n$$\n\nWe define the anisotropy (inflation) factor as\n$$\nR := \\frac{v^{\\top} S v}{e_{1}^{\\top} S e_{1}}\n= \\frac{\\tau^{2}}{\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}}\n= \\tau^{2}\\left(\\beta^{-2} + \\sigma^{-2}\\right).\n$$\nThis expression depends only on $\\beta^{2}$, $\\tau^{2}$, and $\\sigma^{2}$.\n\nFor contrast with a point estimate, we compute the Maximum A Posteriori (MAP) estimate. The posterior mean equals the MAP for Gaussian posteriors, and it is given by\n$$\nm = S\\left(C_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y \\right).\n$$\nCompute $C_{0}^{-1} m_{0} = \\left(\\beta^{-2} m_{1}, \\tau^{-2} m_{2}, \\tau^{-2} m_{3}\\right)^{\\top}$ and $A^{\\top} \\Gamma^{-1} y = \\sigma^{-2} A^{\\top} y = \\sigma^{-2} \\begin{pmatrix} y_{1} \\\\ 0 \\\\ 0 \\end{pmatrix}$. Therefore,\n$$\nC_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y = \\begin{pmatrix} \\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\\\ \\tau^{-2} m_{2} \\\\ \\tau^{-2} m_{3} \\end{pmatrix},\n$$\nand multiplying by $S$ yields\n$$\nm = \\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} \\left(\\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\right) \\\\\n\\tau^{2} \\cdot \\tau^{-2} m_{2} \\\\\n\\tau^{2} \\cdot \\tau^{-2} m_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} \\left(\\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\right) \\\\\nm_{2} \\\\\nm_{3}\n\\end{pmatrix}.\n$$\nThus, the MAP selects the single values $x_{2} = m_{2}$ and $x_{3} = m_{3}$ in the unobserved directions, even though the full posterior covariance shows that the uncertainty along $\\mathrm{Null}(A)$ is large and equal to $\\tau^{2}$. The anisotropy factor $R = \\tau^{2}(\\beta^{-2} + \\sigma^{-2})$ quantifies this inflation relative to the data-informed direction, highlighting how full posterior characterization captures uncertainty that a point estimate such as the MAP does not convey.\n\nThe requested final expression for the inflation factor is $R = \\tau^{2}(\\beta^{-2} + \\sigma^{-2})$.",
            "answer": "$$\\boxed{\\tau^{2}\\left(\\beta^{-2}+\\sigma^{-2}\\right)}$$"
        },
        {
            "introduction": "Building on the concept of anisotropic uncertainty, this next practice moves from clean analytical cases to a more realistic computational scenario involving an ill-conditioned, but not rank-deficient, forward model. The key takeaway is that quantifying the full posterior is not merely an academic exercise; it is crucial for making optimal decisions under uncertainty . By constructing and comparing a naive decision rule based on a single Maximum A Posteriori (MAP) point estimate against a fully Bayes-optimal rule that leverages the entire posterior, you will see concretely how ignoring the landscape of uncertainty can lead to predictably suboptimal choices.",
            "id": "3383401",
            "problem": "Consider a linear inverse problem with a Gaussian observational model and a Gaussian prior in which the forward operator is ill-conditioned. You are asked to construct the model, compute a Maximum A Posteriori (MAP) point estimate of the parameters, fully characterize the Gaussian posterior, and then solve a decision problem where the posterior uncertainty alters the optimal decision compared to a MAP-only rule. Use only definitions and fundamental properties of multivariate Gaussian distributions and linear algebra as the starting point.\n\nYou must solve the following tasks for the specified test suite.\n\nModel specification:\n- Let the unknown parameter vector be $ \\theta \\in \\mathbb{R}^{d} $ with $ d = 5 $.\n- Let the forward operator $ A \\in \\mathbb{R}^{m \\times d} $ with $ m = 5 $ be the Hilbert matrix, i.e., $ A_{ij} = \\frac{1}{i + j - 1} $ for $ 1 \\le i,j \\le 5 $. This $ A $ is ill-conditioned.\n- Let the observation model be $ y = A \\theta + \\epsilon $, where the observational noise $ \\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I) $ with known variance $ \\sigma^{2} > 0 $ and $ I $ is the identity matrix in $ \\mathbb{R}^{m \\times m }$.\n- Let the prior be $ \\theta \\sim \\mathcal{N}(0, \\tau^{2} I) $ with known variance $ \\tau^{2} > 0 $.\n- Let the true parameter used to synthesize data be $ \\theta^{\\star} = [1, -0.5, 0.3, 0, 0.05]^{\\top} $. Let the realized observation be noise-free for reproducibility, i.e., set $ \\epsilon = 0 $ so $ y = A \\theta^{\\star} $ in all test cases.\n\nPosterior characterization and MAP:\n- Compute the posterior distribution $ p(\\theta \\mid y) $ under the model above, and the Maximum A Posteriori (MAP) estimate $ \\hat{\\theta}_{\\mathrm{MAP}} $. You must not assume any shortcut formulas; derive the form of the posterior by combining the likelihood and the prior using Bayes’ rule and standard properties of multivariate Gaussian distributions.\n\nUncertainty directions:\n- Let the posterior covariance be denoted $ \\Sigma_{\\mathrm{post}} \\in \\mathbb{R}^{d \\times d} $. Compute its eigenvalues and eigenvectors. Define the most uncertain direction $ h \\in \\mathbb{R}^{d} $ as the unit-norm eigenvector associated with the largest eigenvalue of $ \\Sigma_{\\mathrm{post}} $. To remove the sign ambiguity, re-orient $ h $ so that $ h^{\\top} \\mu_{\\mathrm{post}} \\ge 0 $, where $ \\mu_{\\mathrm{post}} $ is the posterior mean.\n- Report the spectral condition number of $ A $ in the $ 2 $-norm, denoted $ \\kappa_{2}(A) $, and the largest and smallest eigenvalues of $ \\Sigma_{\\mathrm{post}} $. These quantify ill-conditioning in the forward operator and the spread of the posterior.\n\nDecision problem where posterior uncertainty matters:\n- Consider a binary decision $ u \\in \\{0,1\\} $. Define the scalar quantity of interest $ s = h^{\\top} \\theta $ and a threshold $ t \\in \\mathbb{R} $. The loss for a decision is:\n  - If $ u = 1 $ and $ s < t $ (a false positive), incur loss $ C_{\\mathrm{fp}} $.\n  - If $ u = 0 $ and $ s \\ge t $ (a false negative), incur loss $ C_{\\mathrm{fn}} $.\n  - Otherwise, incur zero loss.\n- Two decision rules must be produced:\n  - A MAP-based plug-in rule: decide $ u_{\\mathrm{MAP}} = 1 $ if $ h^{\\top} \\hat{\\theta}_{\\mathrm{MAP}} \\ge t $, otherwise $ u_{\\mathrm{MAP}} = 0 $.\n  - A Bayes-optimal rule under posterior uncertainty: decide $ u_{\\mathrm{Bayes}} $ that minimizes posterior expected loss using $ p(\\theta \\mid y) $.\n- Also compute the posterior standard deviation of $ s $, denoted $ \\sigma_{s} = \\sqrt{ h^{\\top} \\Sigma_{\\mathrm{post}} h } $, and the posterior probability $ p = \\mathbb{P}( s \\ge t \\mid y ) $.\n\nTest suite:\nUse the shared $ A $, $ \\theta^{\\star} $, and $ y $ defined above, and evaluate the following three parameter settings:\n- Case $1$: $ \\sigma^{2} = 10^{-4} $, $ \\tau^{2} = 1 $, $ t = 0.04 $, $ C_{\\mathrm{fp}} = 0.25 $, $ C_{\\mathrm{fn}} = 1 $.\n- Case $2$: $ \\sigma^{2} = 10^{-2} $, $ \\tau^{2} = 10^{-2} $, $ t = 0.02 $, $ C_{\\mathrm{fp}} = 0.5 $, $ C_{\\mathrm{fn}} = 0.5 $.\n- Case $3$: $ \\sigma^{2} = 10^{-1} $, $ \\tau^{2} = 10 $, $ t = 0.03 $, $ C_{\\mathrm{fp}} = 0.1 $, $ C_{\\mathrm{fn}} = 0.9 $.\n\nRequired outputs per case:\nFor each case, output the following in order as a list:\n$[\\kappa_{2}(A), \\lambda_{\\max}(\\Sigma_{\\mathrm{post}}), \\lambda_{\\min}(\\Sigma_{\\mathrm{post}}), h^{\\top} \\hat{\\theta}_{\\mathrm{MAP}}, \\sigma_{s}, \\mathbb{P}(s \\ge t \\mid y), u_{\\mathrm{MAP}}, u_{\\mathrm{Bayes}}]$,\nwhere $ \\lambda_{\\max}(\\cdot) $ and $ \\lambda_{\\min}(\\cdot) $ denote the largest and smallest eigenvalues, respectively.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the three per-case lists, enclosed in a single pair of square brackets, for example, $[[\\dots],[\\dots],[\\dots]]$. All booleans must be printed as programming-language boolean literals. No additional text should be printed. No physical units are involved in this problem.\n\nYour implementation must adhere to the execution environment constraints and must not read any input.",
            "solution": "The user-provided problem is a well-posed and standard exercise in Bayesian inverse problems and decision theory. It is scientifically grounded, fully specified, and computationally feasible. The problem is valid.\n\n### 1. Model and Posterior Distribution\n\nThe problem specifies a linear inverse problem with a Gaussian structure.\n- The **prior distribution** for the unknown parameters $\\theta \\in \\mathbb{R}^d$ is Gaussian:\n$$ p(\\theta) = \\mathcal{N}(\\theta; 0, \\tau^2 I_d) \\propto \\exp\\left(-\\frac{1}{2\\tau^2} \\theta^\\top\\theta\\right) $$\nwhere $I_d$ is the $d \\times d$ identity matrix ($d=5$).\n\n- The **likelihood** of the observation $y \\in \\mathbb{R}^m$ given $\\theta$ is derived from the observation model $y = A\\theta + \\epsilon$ with noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$:\n$$ p(y|\\theta) = \\mathcal{N}(y; A\\theta, \\sigma^2 I_m) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - A\\theta)^\\top(y - A\\theta)\\right) $$\nwhere $I_m$ is the $m \\times m$ identity matrix ($m=5$).\n\nAccording to Bayes' rule, the **posterior distribution** $p(\\theta|y)$ is proportional to the product of the likelihood and the prior:\n$$ p(\\theta|y) \\propto p(y|\\theta) p(\\theta) $$\n$$ p(\\theta|y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - A\\theta)^\\top(y - A\\theta) - \\frac{1}{2\\tau^2} \\theta^\\top\\theta\\right) $$\nThe term in the exponent, which we can denote as $-J(\\theta)/2$, is a quadratic function of $\\theta$. This implies that the posterior distribution is also Gaussian, i.e., $p(\\theta|y) = \\mathcal{N}(\\theta; \\mu_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}})$. To find its mean $\\mu_{\\mathrm{post}}$ and covariance $\\Sigma_{\\mathrm{post}}$, we complete the square for the argument of the exponential:\n$$ J(\\theta) = \\frac{1}{\\sigma^2}(y^\\top y - 2y^\\top A\\theta + \\theta^\\top A^\\top A\\theta) + \\frac{1}{\\tau^2}\\theta^\\top\\theta $$\n$$ J(\\theta) = \\theta^\\top\\left(\\frac{1}{\\sigma^2}A^\\top A + \\frac{1}{\\tau^2}I_d\\right)\\theta - \\frac{2}{\\sigma^2}y^\\top A\\theta + \\text{const.} $$\nComparing this to the standard unnormalized log-posterior for a Gaussian, $\\frac{1}{2}(\\theta - \\mu_{\\mathrm{post}})^\\top \\Sigma_{\\mathrm{post}}^{-1} (\\theta - \\mu_{\\mathrm{post}}) = \\frac{1}{2}\\theta^\\top\\Sigma_{\\mathrm{post}}^{-1}\\theta - \\theta^\\top\\Sigma_{\\mathrm{post}}^{-1}\\mu_{\\mathrm{post}} + \\text{const.}$, we can identify the posterior precision matrix (inverse covariance) and the mean.\nThe **posterior precision** is:\n$$ \\Sigma_{\\mathrm{post}}^{-1} = \\frac{1}{\\sigma^2}A^\\top A + \\frac{1}{\\tau^2}I_d $$\nThus, the **posterior covariance** is:\n$$ \\Sigma_{\\mathrm{post}} = \\left(\\frac{1}{\\sigma^2}A^\\top A + \\frac{1}{\\tau^2}I_d\\right)^{-1} $$\nAnd the **posterior mean** is:\n$$ \\Sigma_{\\mathrm{post}}^{-1}\\mu_{\\mathrm{post}} = \\frac{1}{\\sigma^2}A^\\top y \\implies \\mu_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}}\\left(\\frac{1}{\\sigma^2}A^\\top y\\right) $$\nFor a Gaussian distribution, the Maximum A Posteriori (MAP) estimate is equal to the posterior mean, $\\hat{\\theta}_{\\mathrm{MAP}} = \\mu_{\\mathrm{post}}$.\n\n### 2. Uncertainty and Decision-Making\n\nThe problem requires analyzing uncertainty in the direction $h$, defined as the unit-norm eigenvector associated with the largest eigenvalue, $\\lambda_{\\max}$, of the posterior covariance $\\Sigma_{\\mathrm{post}}$. The quantity of interest is the scalar projection $s = h^\\top\\theta$. Because $\\theta|y$ is Gaussian, $s|y$ is a scalar Gaussian variable:\n$$ s|y \\sim \\mathcal{N}(\\mu_s, \\sigma_s^2) $$\nwhere the mean is $\\mu_s = h^\\top\\mu_{\\mathrm{post}}$ and the variance is $\\sigma_s^2 = h^\\top\\Sigma_{\\mathrm{post}}h$. Since $h$ is the eigenvector of $\\Sigma_{\\mathrm{post}}$ for eigenvalue $\\lambda_{\\max}$, this simplifies to $\\sigma_s^2 = h^\\top(\\lambda_{\\max}h) = \\lambda_{\\max}(h^\\top h) = \\lambda_{\\max}$, because $h$ is a unit vector. Thus, the posterior standard deviation of $s$ is $\\sigma_s = \\sqrt{\\lambda_{\\max}(\\Sigma_{\\mathrm{post}})}$.\n\nWe must compare two decision rules for a choice $u \\in \\{0,1\\}$:\n1.  **MAP-based plug-in rule**: This rule ignores posterior uncertainty and uses the point estimate $\\hat{\\theta}_{\\mathrm{MAP}}$. The decision is $u_{\\mathrm{MAP}} = 1$ if $h^\\top\\hat{\\theta}_{\\mathrm{MAP}} \\ge t$, and $u_{\\mathrm{MAP}}=0$ otherwise.\n2.  **Bayes-optimal rule**: This rule minimizes the posterior expected loss. The expected losses for actions $u=1$ and $u=0$ are:\n    $$ \\mathbb{E}[L(u=1, s)|y] = C_{\\mathrm{fp}}\\mathbb{P}(s < t | y) $$\n    $$ \\mathbb{E}[L(u=0, s)|y] = C_{\\mathrm{fn}}\\mathbb{P}(s \\ge t | y) $$\n    Let $p = \\mathbb{P}(s \\ge t | y)$, so $\\mathbb{P}(s < t | y) = 1-p$. The optimal decision is $u=1$ if its expected loss is lower, i.e., $C_{\\mathrm{fp}}(1-p) < C_{\\mathrm{fn}}p$. Rearranging this gives the decision criterion:\n    $$ u_{\\mathrm{Bayes}} = 1 \\iff p > \\frac{C_{\\mathrm{fp}}}{C_{\\mathrm{fp}} + C_{\\mathrm{fn}}} $$\n    This rule explicitly uses the posterior probability $p$, taking the full posterior uncertainty of $s$ into account.\n\n### 3. Implementation Details\n\nThe forward operator $A$ is the $5 \\times 5$ Hilbert matrix, which is known to be ill-conditioned. Its condition number $\\kappa_2(A)$ measures this instability. The observations $y$ are synthesized without noise ($y = A\\theta^\\star$) for reproducibility.\n\nThe algorithm proceeds as follows for each test case:\n1.  Given $\\sigma^2$ and $\\tau^2$, compute $\\mu_{\\mathrm{post}}$ and $\\Sigma_{\\mathrm{post}}$.\n2.  Perform an eigendecomposition of $\\Sigma_{\\mathrm{post}}$ to find its eigenvalues ($\\lambda_{\\min}, \\lambda_{\\max}$) and the eigenvector $h$ corresponding to $\\lambda_{\\max}$. Orient $h$ such that $h^\\top\\mu_{\\mathrm{post}} \\ge 0$.\n3.  Compute the parameters of the distribution of $s$: $\\mu_s = h^\\top\\mu_{\\mathrm{post}}$ and $\\sigma_s = \\sqrt{\\lambda_{\\max}}$.\n4.  Calculate the probability $p = \\mathbb{P}(s \\ge t | y)$ using the cumulative distribution function (CDF) of the normal distribution: $p = 1 - \\Phi\\left(\\frac{t-\\mu_s}{\\sigma_s}\\right)$.\n5.  Determine the decisions $u_{\\mathrm{MAP}}$ and $u_{\\mathrm{Bayes}}$ based on their respective rules.\n6.  Collect and format all required outputs.\n\nThe ill-conditioning of $A$ propagates into high posterior uncertainty, especially in directions corresponding to small singular values of $A$. The regularization provided by the prior ensures the posterior is well-behaved, but the uncertainty can remain large, making the distinction between MAP-based and Bayes-optimal decisions significant, particularly when the decision costs are asymmetric.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport sys\nimport io\n\n# Ensure the output uses full precision representation compatible with the autograder.\n# This prevents truncation of float values.\n# np.set_printoptions(threshold=sys.maxsize, floatmode='maxprec')\n# After review, repr() is a cleaner and more standard way to achieve this.\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inverse problem and decision task as specified.\n    \"\"\"\n    # =========================================================================\n    # Model Specification and One-Time Computations\n    # =========================================================================\n    d = 5\n    m = 5\n    # The Hilbert matrix A_ij = 1/(i+j-1) for 1-based indexing.\n    # For 0-based indexing i, j, this is 1/((i+1)+(j+1)-1) = 1/(i+j+1).\n    A = np.fromfunction(lambda i, j: 1 / (i + j + 1), (m, d))\n\n    # Spectral condition number of A (2-norm).\n    kappa2_A = np.linalg.cond(A, 2)\n\n    # True parameter vector and synthesized noise-free observation.\n    theta_star = np.array([1.0, -0.5, 0.3, 0.0, 0.05])\n    y = A @ theta_star\n\n    # =========================================================================\n    # Test Suite Definition\n    # =========================================================================\n    test_cases = [\n        # (sigma^2, tau^2, t, C_fp, C_fn)\n        (1e-4, 1.0, 0.04, 0.25, 1.0),\n        (1e-2, 1e-2, 0.02, 0.5, 0.5),\n        (1e-1, 10.0, 0.03, 0.1, 0.9),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        sigma2, tau2, t, C_fp, C_fn = case\n        \n        # =====================================================================\n        # 1. Posterior Characterization\n        # =====================================================================\n        # The posterior is p(theta|y) ~ N(mu_post, Sigma_post).\n        # We use the formulation derived from first principles:\n        # mu_post = (A^T A + (sigma2/tau2)I)^-1 A^T y\n        # Sigma_post = sigma2 * (A^T A + (sigma2/tau2)I)^-1\n        alpha = sigma2 / tau2\n        \n        # Matrix to be inverted. This is proportional to the posterior precision.\n        M = A.T @ A + alpha * np.identity(d)\n        M_inv = np.linalg.inv(M)\n        \n        # Posterior mean (which is also the MAP estimate).\n        mu_post = M_inv @ A.T @ y\n        theta_map = mu_post\n        \n        # Posterior covariance.\n        Sigma_post = sigma2 * M_inv\n\n        # =====================================================================\n        # 2. Uncertainty Directions and Eigenanalysis\n        # =====================================================================\n        # Eigen decomposition of the symmetric posterior covariance matrix.\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(Sigma_post)\n        \n        lambda_min_post = eigenvalues[0]\n        lambda_max_post = eigenvalues[-1]\n        \n        # h is the unit-norm eigenvector associated with the largest eigenvalue.\n        h = eigenvectors[:, -1]\n        \n        # Re-orient h such that h^T * mu_post >= 0.\n        if h.T @ mu_post < 0:\n            h = -h\n            \n        # =====================================================================\n        # 3. Decision Problem Quantities\n        # =====================================================================\n        # Quantity of interest s = h^T * theta.\n        # Since theta is Gaussian, s is also Gaussian: s | y ~ N(mu_s, sigma_s^2).\n        \n        # Expected value of s. This is also the plug-in MAP value, h^T * theta_map.\n        mu_s = h.T @ mu_post\n        \n        # Standard deviation of s. As h is an eigenvector of Sigma_post with\n        # eigenvalue lambda_max_post, h^T Sigma_post h = lambda_max_post for unit h.\n        sigma_s = np.sqrt(lambda_max_post)\n        \n        # Posterior probability P(s >= t | y).\n        # We use the survival function sf(x) = 1 - cdf(x) for better numerical precision.\n        prob_s_ge_t = norm.sf(t, loc=mu_s, scale=sigma_s)\n\n        # =====================================================================\n        # 4. Decision Rules\n        # =====================================================================\n        # MAP-based plug-in rule: u=1 if h^T * theta_map >= t, else 0.\n        u_map = 1 if mu_s >= t else 0\n        \n        # Bayes-optimal rule: minimize posterior expected loss.\n        # This is equivalent to u=1 if P(s >= t | y) > C_fp / (C_fp + C_fn).\n        p_thresh = C_fp / (C_fp + C_fn)\n        u_bayes = 1 if prob_s_ge_t > p_thresh else 0\n\n        # =====================================================================\n        # 5. Collect Results\n        # =====================================================================\n        case_results = [\n            kappa2_A,\n            lambda_max_post,\n            lambda_min_post,\n            mu_s, \n            sigma_s,\n            prob_s_ge_t,\n            u_map,\n            u_bayes\n        ]\n        all_results.append(case_results)\n\n    # =========================================================================\n    # Final Output Formatting\n    # =========================================================================\n    # The required format is a string representation of a list of lists of results,\n    # with no spaces and using standard numeric representations (e.g., via repr()).\n    inner_lists_str = [f\"[{','.join(map(repr, res))}]\" for res in all_results]\n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output_str)\n\n# Execute the solution.\nsolve()\n```"
        },
        {
            "introduction": "Our previous examples existed in the convenient world of linear models, where posteriors are unimodal. This final practice takes us into the more complex domain of nonlinear inverse problems, where a fundamentally different challenge, multimodality, often emerges. When the relationship between parameters and data is not one-to-one, multiple distinct sets of parameters can explain the observations, resulting in a posterior distribution with several peaks. This exercise  powerfully illustrates the failure of point estimates in this context, as they arbitrarily select one solution while ignoring others, and quantifies the superior predictive accuracy achieved by integrating over the full, multimodal posterior.",
            "id": "3383451",
            "problem": "Consider the scalar nonlinear inverse problem with observational model $y = f(\\theta) + \\epsilon$, where the parameter $\\theta \\in \\mathbb{R}$, the forward map is $f(\\theta) = \\theta^2$, and the noise $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent of $\\theta$. Assume the prior $\\theta \\sim \\mathcal{N}(0,\\tau^2)$. This model is non-injective because $f(\\theta) = f(-\\theta)$, which can lead to a multimodal posterior for $\\theta$ when the observed data $y$ is sufficiently informative. Use the following fundamental base: Bayes' rule for the posterior $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$ and the law of total probability for the posterior predictive $p(y^\\ast \\mid y) = \\int p(y^\\ast \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$, where $y^\\ast$ denotes a future observation under the same data model.\n\nYour tasks are:\n- Precisely define the Maximum A Posteriori (MAP) estimator $\\hat{\\theta}_{MAP}$ as a maximizer of the posterior density $p(\\theta \\mid y)$ and determine the corresponding plug-in posterior predictive $p_{MAP}(y^\\ast \\mid y)$ that uses a point-mass at $\\hat{\\theta}_{MAP}$.\n- Precisely define the fully integrated posterior predictive $p_{int}(y^\\ast \\mid y)$ by marginalizing over $p(\\theta \\mid y)$ and express it using only well-defined integrals and normalizing constants derived from Bayes' rule and the law of total probability.\n- Implement a robust numerical procedure to evaluate, for given $(y,\\sigma,\\tau)$ and a finite set of evaluation points $\\{y_k^\\ast\\}_{k=1}^K$, the average log predictive density difference\n$$\n\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\}) \\;=\\; \\frac{1}{K} \\sum_{k=1}^K \\left[ \\log p_{int}(y_k^\\ast \\mid y) \\;-\\; \\log p_{MAP}(y_k^\\ast \\mid y) \\right].\n$$\nThe evaluation must be performed in a scientifically sound and numerically stable manner without resorting to any sampling-based approximation.\n\nBase definitions you may use (and must derive from them, not shortcut beyond them):\n- The likelihood $p(y \\mid \\theta)$ is Gaussian with mean $\\theta^2$ and variance $\\sigma^2$, that is $p(y \\mid \\theta) = \\mathcal{N}(y;\\theta^2,\\sigma^2)$.\n- The prior $p(\\theta)$ is Gaussian with mean $0$ and variance $\\tau^2$, that is $p(\\theta) = \\mathcal{N}(\\theta;0,\\tau^2)$.\n- The posterior $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$.\n- The MAP estimator $\\hat{\\theta}_{MAP}$ is a maximizer of $p(\\theta \\mid y)$.\n- The plug-in predictive under a point estimate $\\hat{\\theta}$ is $p(y^\\ast \\mid y) \\approx p(y^\\ast \\mid \\theta=\\hat{\\theta})$.\n- The fully integrated predictive is $p(y^\\ast \\mid y) = \\int p(y^\\ast \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$.\n\nReport the comparison in terms of the average log predictive density difference $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$ for the following test suite of parameter values and evaluation grids:\n- Case A (bimodal, low noise): $(y,\\sigma,\\tau) = (4.0, 0.2, 2.0)$ with $\\{y_k^\\ast\\} = [3.0, 4.0, 5.0]$.\n- Case B (near-coalescing modes): $(y,\\sigma,\\tau) = (0.5, 0.2, 2.0)$ with $\\{y_k^\\ast\\} = [0.1, 0.5, 1.0]$.\n- Case C (bimodal, higher noise): $(y,\\sigma,\\tau) = (4.0, 1.5, 2.0)$ with $\\{y_k^\\ast\\} = [0.0, 2.0, 4.0, 8.0]$.\n- Case D (edge case at zero): $(y,\\sigma,\\tau) = (0.0, 0.2, 2.0)$ with $\\{y_k^\\ast\\} = [0.0, 0.2, 1.0]$.\n\nNumerical and algorithmic requirements:\n- All quantities are dimensionless (no physical units are required).\n- Angles are not involved; no angle unit is required.\n- For each case, compute the MAP-based predictive assuming a point-mass at $\\hat{\\theta}_{MAP}$ and the fully integrated predictive by numerically marginalizing over $\\theta \\in \\mathbb{R}$ using only deterministic numerical integration. Exploit any symmetry you can justify from first principles to improve numerical stability.\n- For each case, return the single float $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$, rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$, where each $\\Delta$ corresponds to the respective case above and is rounded to $6$ decimal places, for example $[0.123456,0.234567,0.345678,0.456789]$.",
            "solution": "We start from Bayes' rule and the law of total probability, which serve as the foundational principles. The model is $y = \\theta^2 + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and prior $\\theta \\sim \\mathcal{N}(0,\\tau^2)$. Therefore the likelihood is $p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2}\\right)$ and the prior is $p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\tau}\\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$. The posterior is $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$, which yields the unnormalized log posterior\n$$\n\\ell(\\theta; y,\\sigma,\\tau) \\;=\\; -\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2} + \\text{constant}.\n$$\nBecause $f(\\theta)=\\theta^2$ is even, the likelihood is even in $\\theta$, and with a symmetric Gaussian prior, the posterior is also even. This implies that for sufficiently informative data values $y$ (relative to $\\sigma$ and $\\tau$), the posterior has two symmetric modes at $\\pm \\theta^\\star$; this is the source of multimodality.\n\nTo obtain the Maximum A Posteriori (MAP) estimator $\\hat{\\theta}_{MAP}$, we maximize $\\ell(\\theta;y,\\sigma,\\tau)$. The derivative is\n$$\n\\frac{d\\ell}{d\\theta}(\\theta) = \\frac{2\\theta}{\\sigma^2}(y-\\theta^2) - \\frac{\\theta}{\\tau^2} \\;=\\; \\theta\\left[\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2}\\right].\n$$\nStationary points occur at $\\theta = 0$ or when $\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2} = 0$, i.e.,\n$$\n\\theta^2 \\;=\\; y - \\frac{\\sigma^2}{2\\tau^2}.\n$$\nThe second derivative is\n$$\n\\frac{d^2\\ell}{d\\theta^2}(\\theta) = \\left[\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2}\\right] - \\frac{4\\theta^2}{\\sigma^2}.\n$$\nFor $\\theta \\neq 0$ at stationary points, the bracketed term vanishes, and $\\frac{d^2\\ell}{d\\theta^2} = -\\frac{4\\theta^2}{\\sigma^2} < 0$, so these are local maxima. At $\\theta=0$, the curvature is $\\frac{d^2\\ell}{d\\theta^2}(0) = \\frac{2y}{\\sigma^2} - \\frac{1}{\\tau^2}$, which is negative (a local maximum at $\\theta=0$) if $y \\le \\frac{\\sigma^2}{2\\tau^2}$ and positive (a local minimum) if $y > \\frac{\\sigma^2}{2\\tau^2}$. Therefore:\n- If $y \\le \\frac{\\sigma^2}{2\\tau^2}$, then $\\hat{\\theta}_{MAP} = 0$.\n- If $y > \\frac{\\sigma^2}{2\\tau^2}$, then there are two symmetric MAP values at $\\pm \\sqrt{y - \\frac{\\sigma^2}{2\\tau^2}}$. For a deterministic choice, we select the positive maximizer $\\hat{\\theta}_{MAP} = +\\sqrt{y - \\frac{\\sigma^2}{2\\tau^2}}$.\n\nGiven a point estimate $\\hat{\\theta}$, the plug-in posterior predictive is\n$$\np_{MAP}(y^\\ast \\mid y) \\approx p(y^\\ast \\mid \\theta = \\hat{\\theta}_{MAP}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^\\ast - \\hat{\\theta}_{MAP}^2)^2}{2\\sigma^2}\\right).\n$$\nThis corresponds to collapsing posterior uncertainty onto a single point $\\hat{\\theta}_{MAP}$ in parameter space, which in this nonlinear model collapses predictive uncertainty to only the observational noise variance.\n\nThe fully integrated posterior predictive, by the law of total probability, is\n$$\np_{int}(y^\\ast \\mid y) \\;=\\; \\int_{\\mathbb{R}} p(y^\\ast \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta \\;=\\; \\frac{\\int_{\\mathbb{R}} p(y^\\ast \\mid \\theta)\\, p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}{\\int_{\\mathbb{R}} p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}.\n$$\nSubstituting the Gaussian forms and canceling constants carefully yields\n$$\np_{int}(y^\\ast \\mid y) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\sigma} \\cdot \\frac{\\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta}{\\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta}.\n$$\nBy symmetry, each integrand is an even function of $\\theta$, so we may compute each integral as twice the integral over $[0,\\infty)$, which improves numerical stability.\n\nAlgorithmic design:\n- Compute $\\hat{\\theta}_{MAP}$ using the stationary point analysis above. The plug-in predictive $p_{MAP}(y^\\ast \\mid y)$ is then a Gaussian density with mean $\\hat{\\theta}_{MAP}^2$ and variance $\\sigma^2$.\n- For the integrated predictive, evaluate the denominator integral\n$$\nZ(y) = \\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta = 2 \\int_{0}^{\\infty} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta,\n$$\nand for each $y^\\ast$ the numerator integral\n$$\nN(y^\\ast,y) = \\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta = 2 \\int_{0}^{\\infty} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta.\n$$\nUse deterministic numerical quadrature with absolute and relative tolerances chosen to ensure accuracy. Because the integrands decay super-exponentially as $\\theta \\to \\infty$ due to the $\\theta^4$ term arising from $(y-\\theta^2)^2$, numerical integration over $[0,\\infty)$ is stable.\n\n- Compute $\\log p_{int}(y^\\ast \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) + \\log N(y^\\ast,y) - \\log Z(y)$ and $\\log p_{MAP}(y^\\ast \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(y^\\ast-\\hat{\\theta}_{MAP}^2)^2}{2\\sigma^2}$.\n- Average their differences over the provided grid $\\{y_k^\\ast\\}_{k=1}^K$ to obtain $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$.\n\nTest suite execution:\n- Case A: $(y,\\sigma,\\tau) = (4.0, 0.2, 2.0)$, $\\{y_k^\\ast\\} = [3.0, 4.0, 5.0]$.\n- Case B: $(y,\\sigma,\\tau) = (0.5, 0.2, 2.0)$, $\\{y_k^\\ast\\} = [0.1, 0.5, 1.0]$.\n- Case C: $(y,\\sigma,\\tau) = (4.0, 1.5, 2.0)$, $\\{y_k^\\ast\\} = [0.0, 2.0, 4.0, 8.0]$.\n- Case D: $(y,\\sigma,\\tau) = (0.0, 0.2, 2.0)$, $\\{y_k^\\ast\\} = [0.0, 0.2, 1.0]$.\n\nFinally, aggregate the four averaged differences into a single output line $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$, rounding each to $6$ decimal places. This comparison directly quantifies the advantage (if any) of full posterior characterization over point estimates for posterior predictive performance in a multimodal inverse problem, isolating the role of integrating over modes in $p(\\theta \\mid y)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sqrt, log, pi\nfrom scipy import integrate\n\ndef theta_map_squared(y, sigma, tau):\n    # Closed-form from stationary condition:\n    # theta_MAP^2 = max(y - sigma^2/(2*tau^2), 0)\n    val = y - (sigma**2) / (2.0 * (tau**2))\n    return max(val, 0.0)\n\ndef log_norm_pdf(x, mean, std):\n    return -0.5*log(2.0*pi) - log(std) - 0.5*((x - mean)/std)**2\n\ndef denom_integrand(theta, y, sigma, tau):\n    # Unnormalized posterior integrand: exp(- (y - theta^2)^2/(2*sigma^2) - theta^2/(2*tau^2))\n    t2 = theta*theta\n    return np.exp(- ((y - t2)**2) / (2.0*sigma**2) - t2 / (2.0*tau**2))\n\ndef numer_integrand(theta, y, ystar, sigma, tau):\n    # Numerator integrand for predictive: exp(- (y - theta^2)^2/(2*sigma^2) - (ystar - theta^2)^2/(2*sigma^2) - theta^2/(2*tau^2))\n    t2 = theta*theta\n    return np.exp(- ((y - t2)**2) / (2.0*sigma**2) - ((ystar - t2)**2) / (2.0*sigma**2) - t2 / (2.0*tau**2))\n\ndef log_pint(y, ystar, sigma, tau):\n    # Compute log p_int(ystar | y) = -0.5*log(2*pi) - log(sigma) + log(num) - log(den)\n    # Using symmetry: integrate from 0 to inf and double\n    den_half, den_err = integrate.quad(lambda th: denom_integrand(th, y, sigma, tau), 0.0, np.inf, epsabs=1e-10, epsrel=1e-9, limit=500)\n    num_half, num_err = integrate.quad(lambda th: numer_integrand(th, y, ystar, sigma, tau), 0.0, np.inf, epsabs=1e-10, epsrel=1e-9, limit=500)\n    den = 2.0 * den_half\n    num = 2.0 * num_half\n    # Safety checks to avoid log(0)\n    if den <= 0.0 or num <= 0.0:\n        # In rare numerical issues, return -inf\n        return -np.inf\n    return -0.5*log(2.0*pi) - log(sigma) + log(num) - log(den)\n\ndef average_logscore_difference(y, sigma, tau, ystars):\n    # Compute MAP-based log predictive at each y* and integrated log predictive, then average difference\n    theta2_map = theta_map_squared(y, sigma, tau)\n    diffs = []\n    for ys in ystars:\n        log_p_map = log_norm_pdf(ys, theta2_map, sigma)\n        log_p_int = log_pint(y, ys, sigma, tau)\n        diffs.append(log_p_int - log_p_map)\n    return float(np.mean(diffs))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (y, sigma, tau, [y_star values])\n    test_cases = [\n        (4.0, 0.2, 2.0, [3.0, 4.0, 5.0]),           # Case A\n        (0.5, 0.2, 2.0, [0.1, 0.5, 1.0]),           # Case B\n        (4.0, 1.5, 2.0, [0.0, 2.0, 4.0, 8.0]),      # Case C\n        (0.0, 0.2, 2.0, [0.0, 0.2, 1.0]),           # Case D\n    ]\n\n    results = []\n    for (y, sigma, tau, ystars) in test_cases:\n        delta = average_logscore_difference(y, sigma, tau, ystars)\n        # Round to 6 decimals as required\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}