{
    "hands_on_practices": [
        {
            "introduction": "A common idealization in modeling continuous-time signals is the concept of Gaussian white noise. This exercise explores the fundamental connection between this continuous abstraction and the discrete, time-averaged measurements we obtain in practice. By deriving the variance of a bin-averaged signal from first principles, you will gain a deeper understanding of how the duration of a measurement interval directly influences the noise level in the resulting data, a crucial insight for experimental design. ",
            "id": "3402401",
            "problem": "Consider a continuous-time measurement error model in which the additive error process is idealized as mean-$0$ Gaussian white noise. Specifically, let $\\varepsilon(t)$ denote a mean-$0$ process with covariance distribution $\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)$, where $\\delta(\\cdot)$ is the Dirac delta distribution and $\\sigma^{2} > 0$ is a constant noise intensity parameter. Suppose measurements are collected by averaging the error over disjoint time bins $I_{i} = [t_{i}, t_{i} + \\Delta)$ of fixed width $\\Delta > 0$, yielding bin-averaged errors\n$$\n\\bar{y}_{i} = \\frac{1}{\\Delta}\\int_{I_{i}} \\varepsilon(t)\\,dt.\n$$\nStarting only from the definitions of variance and covariance, and the defining property of the Dirac delta distribution, derive the exact expression for $\\mathrm{Var}(\\bar{y}_{i})$ as a function of $\\sigma^{2}$ and $\\Delta$. In addition, provide a principled explanation for how and why the dependence on $\\Delta$ arises from the covariance structure of the process. Express your final answer in a closed-form analytic expression. No numerical approximation or rounding is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard theoretical exercise in the study of stochastic processes, specifically concerning the properties of time-averaged Gaussian white noise. All necessary definitions and parameters are provided for a unique and meaningful solution. We may therefore proceed with the derivation.\n\nThe problem asks for the derivation of $\\mathrm{Var}(\\bar{y}_{i})$, where $\\bar{y}_{i}$ is the bin-averaged error defined as:\n$$\n\\bar{y}_{i} = \\frac{1}{\\Delta}\\int_{I_{i}} \\varepsilon(t)\\,dt = \\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\n$$\nThe error process $\\varepsilon(t)$ is specified as a mean-$0$ Gaussian white noise process, characterized by its first and second moments:\n$$\n\\mathbb{E}[\\varepsilon(t)] = 0\n$$\n$$\n\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)\n$$\nwhere $\\mathbb{E}[\\cdot]$ denotes the expectation operator, $\\sigma^2$ is the constant noise intensity, and $\\delta(\\cdot)$ is the Dirac delta distribution.\n\nWe begin from the fundamental definition of variance for a random variable $X$:\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n$$\nFirst, we must calculate the expected value of $\\bar{y}_i$. Using the linearity of the expectation operator and Fubini's theorem to interchange the expectation and the integral, we find:\n$$\n\\mathbb{E}[\\bar{y}_{i}] = \\mathbb{E}\\left[\\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\\right] = \\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\mathbb{E}[\\varepsilon(t)]\\,dt\n$$\nSince the process is mean-$0$, i.e., $\\mathbb{E}[\\varepsilon(t)] = 0$ for all $t$, the integral evaluates to zero:\n$$\n\\mathbb{E}[\\bar{y}_{i}] = \\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} 0 \\,dt = 0\n$$\nWith the mean being $0$, the variance simplifies to the mean of the square of the variable:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\mathbb{E}[(\\bar{y}_{i} - 0)^2] = \\mathbb{E}[\\bar{y}_{i}^2]\n$$\nSubstituting the definition of $\\bar{y}_{i}$:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\mathbb{E}\\left[\\left(\\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\\right)^2\\right]\n$$\nWe express the squared integral as a product of two identical integrals, using distinct variables of integration, $t$ and $s$, for clarity:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\mathbb{E}\\left[\\left(\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\\right)\\left(\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(s)\\,ds\\right)\\right]\n$$\nThis product of integrals can be written as a double integral over the square domain $[t_i, t_i+\\Delta) \\times [t_i, t_i+\\Delta)$:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\mathbb{E}\\left[\\int_{t_{i}}^{t_{i} + \\Delta} \\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\varepsilon(s)\\,ds\\,dt\\right]\n$$\nAgain, we interchange the expectation and the integration operators:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\int_{t_{i}}^{t_{i} + \\Delta} \\int_{t_{i}}^{t_{i} + \\Delta} \\mathbb{E}[\\varepsilon(t)\\varepsilon(s)]\\,ds\\,dt\n$$\nNow, we substitute the given covariance distribution $\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)$:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\int_{t_{i}}^{t_{i} + \\Delta} \\int_{t_{i}}^{t_{i} + \\Delta} \\sigma^{2}\\delta(t-s)\\,ds\\,dt\n$$\nWe evaluate the inner integral with respect to $s$. The constant $\\sigma^2$ can be factored out. The core of this step is applying the sifting property of the Dirac delta distribution, which states that for a function $f(x)$, $\\int f(x)\\delta(x-c)\\,dx = f(c)$ if the integration interval contains $c$. Here, the integration variable is $s$, the function is $1$, and the \"spike\" of the delta distribution occurs at $s=t$.\n$$\n\\int_{t_{i}}^{t_{i} + \\Delta} \\delta(t-s)\\,ds\n$$\nThe outer integral over $t$ ranges from $t_{i}$ to $t_{i} + \\Delta$. For any value of $t$ within this range, the point $s=t$ lies within the integration interval of the inner integral, $[t_{i}, t_{i} + \\Delta)$. Therefore, the sifting property applies for every $t$ in the outer integration domain, and the inner integral evaluates to $1$:\n$$\n\\int_{t_{i}}^{t_{i} + \\Delta} \\delta(t-s)\\,ds = 1 \\quad \\text{for } t \\in [t_{i}, t_{i} + \\Delta)\n$$\nSubstituting this result back into the expression for the variance:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{\\sigma^{2}}{\\Delta^2} \\int_{t_{i}}^{t_{i} + \\Delta} (1)\\,dt\n$$\nThe remaining integral is straightforward to evaluate:\n$$\n\\int_{t_{i}}^{t_{i} + \\Delta} 1\\,dt = [t]_{t_{i}}^{t_{i} + \\Delta} = (t_{i} + \\Delta) - t_{i} = \\Delta\n$$\nFinally, we assemble the components to obtain the exact expression for the variance:\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{\\sigma^2}{\\Delta^2} \\cdot \\Delta = \\frac{\\sigma^2}{\\Delta}\n$$\nThe dependence of the variance on the averaging time $\\Delta$ is an inverse relationship, $\\mathrm{Var}(\\bar{y}_{i}) \\propto 1/\\Delta$. This arises directly from the covariance structure of the white noise process. The defining property $\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)$ implies that the noise process is completely uncorrelated at any two distinct points in time, no matter how close they are. The process has no \"memory\".\n\nConsequently, averaging the process over a time interval $\\Delta$ is analogous to calculating the sample mean of a number of independent and identically distributed random draws. For a set of $N$ independent random variables, the variance of their mean decreases as $1/N$. In the continuous-time case, the duration of the averaging window $\\Delta$ serves a role analogous to $N$. As $\\Delta$ increases, we are averaging over a greater number of effectively independent fluctuations of the noise process. These fluctuations, being random and uncorrelated, tend to cancel each other out more effectively over a longer duration. This leads to a reduction in the variance of the averaged value.\n\nThe $\\delta$-correlation is the mathematical idealization of a process with zero correlation time. This specific structure is what leads to the $1/\\Delta$ scaling law. If the noise were correlated over some finite time scale (i.e., a \"colored\" noise process), the variance of the time-averaged signal would decrease more slowly with $\\Delta$, as averaging over a longer window would not introduce proportionally as much new, independent information. The calculation demonstrates this formally: the double integral of the covariance over a $\\Delta \\times \\Delta$ square is non-zero only along the infinitesimally thin diagonal where $t=s$. The total \"power\" in the integral thus scales linearly with the length of this diagonal, which is proportional to $\\Delta$. This linear scaling of the integrated covariance, when normalized by the $\\Delta^2$ factor from the averaging definition, yields the final $1/\\Delta$ dependence.",
            "answer": "$$\\boxed{\\frac{\\sigma^{2}}{\\Delta}}$$"
        },
        {
            "introduction": "The Poisson distribution is the cornerstone model for count data, but its strict assumption that the mean equals the variance is often violated in real-world applications. This discrepancy is known as overdispersion or underdispersion. This practice introduces a standard statistical method, the Pearson chi-square test, to diagnose this issue and demonstrates how the quasi-likelihood framework can be used to obtain valid inferences even when the variance structure is misspecified. ",
            "id": "3402406",
            "problem": "Consider independent count observations $\\{Y_{i}\\}_{i=1}^{n}$ arising in a data assimilation setting where a forward operator maps a parameter vector $\\theta \\in \\mathbb{R}^{p}$ to expected detector intensities, with the mean of $Y_{i}$ given by $\\mu_{i}(\\theta)$. Assume the canonical log-link for counts so that $\\mu_{i}(\\theta) = \\exp(a_{i}^{\\top}\\theta)$ for known covariates $a_{i} \\in \\mathbb{R}^{p}$. The foundational basis is the following: for a Poisson measurement error model, $Y_{i} \\sim \\text{Poisson}(\\mu_{i})$ independently, with $\\mathbb{E}[Y_{i}] = \\mu_{i}$ and $\\operatorname{Var}(Y_{i}) = \\mu_{i}$.\n\n1. Starting from the Poisson log-likelihood and its score, use large-sample arguments to construct a test for dispersion based on standardized residuals. Specifically, derive the Pearson chi-square statistic\n$$\nX^{2} = \\sum_{i=1}^{n} \\frac{(Y_{i}-\\mu_{i})^{2}}{\\mu_{i}},\n$$\nevaluated at the maximum likelihood estimate (MLE) $\\hat{\\theta}$, and show why under the correctly specified Poisson variance, $X^{2}$ is approximately distributed as a chi-square with $n-p$ degrees of freedom, where $p$ is the number of parameters estimated by Maximum Likelihood Estimation (MLE). Explain how deviations of $X^{2}/(n-p)$ from $1$ provide evidence of overdispersion or underdispersion relative to a Poisson model.\n\n2. When dispersion differs from $1$, posit a quasi-likelihood framework with variance model $\\operatorname{Var}(Y_{i}\\,|\\,\\theta) = \\phi\\,\\mu_{i}(\\theta)$ for a scalar dispersion parameter $\\phi > 0$. Using only the mean-variance relationship and the defining property of quasi-likelihoods that the quasi-score is proportional to $(Y_{i}-\\mu_{i})$ scaled by $\\operatorname{Var}(Y_{i})$, derive:\n   - The quasi-score for $\\theta$ under the log-link when $\\operatorname{Var}(Y_{i}) = \\phi\\,\\mu_{i}$.\n   - The quasi-likelihood for $\\mu_{i}$ up to an additive constant in terms of $\\phi$.\n   - The asymptotic covariance adjustment for $\\hat{\\theta}$ in terms of $\\phi$ and the expected Fisher information under the Poisson mean model.\n\n3. Now, consider $n = 8$ counts with $p = 3$ fitted parameters. Suppose the fitted means (based on the forward operator and the MLE $\\hat{\\theta}$) are\n$$\n\\mu = \\big(10.5,\\; 4.8,\\; 7.9,\\; 18.6,\\; 13.2,\\; 3.1,\\; 6.5,\\; 9.7\\big),\n$$\nand the observed counts are\n$$\ny = \\big(12,\\; 5,\\; 8,\\; 20,\\; 15,\\; 3,\\; 7,\\; 9\\big).\n$$\nCompute the Pearson-based estimator of dispersion\n$$\n\\hat{\\phi}_{\\text{Pearson}} = \\frac{X^{2}}{n-p}.\n$$\nRound your answer to four significant figures. Provide your final answer as a single real number with no units.",
            "solution": "The problem is deemed valid as it is scientifically grounded in statistical theory (generalized linear models, quasi-likelihood), is well-posed with sufficient information for a unique solution, and is stated objectively without ambiguity.\n\nThe solution will be presented in three parts, corresponding to the three items in the problem statement.\n\nPart 1: Derivation and Interpretation of the Pearson Chi-Square Statistic\n\nWe begin with a set of $n$ independent count observations $\\{Y_{i}\\}_{i=1}^{n}$. The model assumes $Y_{i} \\sim \\text{Poisson}(\\mu_{i})$, where the mean $\\mu_{i}$ is related to a parameter vector $\\theta \\in \\mathbb{R}^{p}$ via a log-link function, $\\mu_{i}(\\theta) = \\exp(a_{i}^{\\top}\\theta)$. The probability mass function for a single observation is $P(Y_i = y_i) = \\frac{\\mu_i^{y_i} \\exp(-\\mu_i)}{y_i!}$. The log-likelihood for the entire dataset $\\mathbf{y} = (y_1, \\dots, y_n)$ is the sum of individual log-likelihoods:\n$$\nl(\\theta; \\mathbf{y}) = \\sum_{i=1}^{n} \\left( y_i \\ln(\\mu_i(\\theta)) - \\mu_i(\\theta) - \\ln(y_i!) \\right)\n$$\nThe score function, $\\mathbf{U}(\\theta)$, is the vector of first partial derivatives of the log-likelihood with respect to the parameters $\\theta_j$ for $j=1, \\dots, p$:\n$$\nU_j(\\theta) = \\frac{\\partial l}{\\partial \\theta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\theta_j}\n$$\nWe have $\\frac{\\partial l_i}{\\partial \\mu_i} = \\frac{y_i}{\\mu_i} - 1 = \\frac{y_i - \\mu_i}{\\mu_i}$. For the log-link, $\\frac{\\partial \\mu_i}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j}\\exp(a_{i}^{\\top}\\theta) = \\exp(a_{i}^{\\top}\\theta) \\cdot a_{ij} = \\mu_i a_{ij}$.\nSubstituting these into the score equation gives:\n$$\nU_j(\\theta) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\mu_i}{\\mu_i} \\right) (\\mu_i a_{ij}) = \\sum_{i=1}^{n} (y_i - \\mu_i) a_{ij}\n$$\nThe maximum likelihood estimate (MLE) $\\hat{\\theta}$ is found by setting the score function to zero, i.e., $\\mathbf{U}(\\hat{\\theta}) = \\mathbf{0}$. This yields $p$ equations: $\\sum_{i=1}^{n} (Y_i - \\hat{\\mu}_i) a_{ij} = 0$ for $j=1, \\dots, p$, where $\\hat{\\mu}_i = \\mu_i(\\hat{\\theta})$.\n\nThe Pearson residual for observation $i$ is defined as the raw residual scaled by the standard deviation of the observation:\n$$\nr_{P,i} = \\frac{Y_i - \\mathbb{E}[Y_i]}{\\sqrt{\\operatorname{Var}(Y_i)}} = \\frac{Y_i - \\mu_i}{\\sqrt{\\mu_i}}\n$$\nThe Pearson chi-square statistic, $X^2$, is the sum of the squares of the Pearson residuals, evaluated at the MLE $\\hat{\\mu}_i$:\n$$\nX^2 = \\sum_{i=1}^{n} \\frac{(Y_i - \\hat{\\mu}_i)^2}{\\hat{\\mu}_i}\n$$\nFor large sample sizes $n$, if the Poisson model is correctly specified, each term $(Y_i - \\mu_i)/\\sqrt{\\mu_i}$ is approximately a standard normal random variable. The sum of squares of $n$ independent standard normal variates follows a chi-square distribution with $n$ degrees of freedom, $\\chi^2_n$. However, the means $\\mu_i$ are not known and are replaced by their estimates $\\hat{\\mu}_i$. These estimates depend on the same $p$-dimensional parameter vector $\\hat{\\theta}$, which was obtained by satisfying the $p$ linear constraints $\\sum_{i=1}^{n} (Y_i - \\hat{\\mu}_i) a_{ij} = 0$. These constraints on the residuals reduce the effective degrees of freedom of the system. By a general theorem of asymptotic statistics (related to Wilks's theorem and goodness-of-fit tests), estimating $p$ parameters reduces the degrees of freedom of the chi-square statistic by $p$. Consequently, under the null hypothesis that the Poisson model is correct, $X^2$ is approximately distributed as a chi-square random variable with $n-p$ degrees of freedom:\n$$\nX^2 \\sim \\chi^2_{n-p} \\quad (\\text{approximately})\n$$\nThe expected value of a $\\chi^2_k$ random variable is $k$. Therefore, if the model is correct, we expect $X^2$ to be close to $n-p$. This leads to the definition of the dispersion parameter estimator: $\\hat{\\phi} = \\frac{X^2}{n-p}$. We expect $\\hat{\\phi} \\approx 1$.\n- If $\\hat{\\phi} \\gg 1$, it implies $X^2 \\gg n-p$. This suggests that the observed variability, captured by the squared residuals, is much larger than what is predicted by the Poisson model (where variance equals the mean). This is evidence for **overdispersion**.\n- If $\\hat{\\phi} \\ll 1$, it implies $X^2 \\ll n-p$. This suggests that the observed variability is much smaller than predicted by the Poisson model. This is evidence for **underdispersion**.\n- If $\\hat{\\phi} \\approx 1$, the data are consistent with the Poisson variance assumption.\n\nPart 2: Quasi-Likelihood Framework\n\nWe now assume a more general variance structure $\\operatorname{Var}(Y_i) = \\phi \\mu_i(\\theta)$, where $\\phi$ is a constant dispersion parameter. The quasi-likelihood approach does not require a full distributional assumption, but relies only on the mean and variance structure.\n\n- **Quasi-Score for $\\theta$**: The quasi-score function for $\\theta_j$ is defined as a sum over observations, where each term is proportional to $(Y_i-\\mu_i)$ scaled by $\\operatorname{Var}(Y_i)$:\n$$\nU_{Q,j}(\\theta) = \\sum_{i=1}^n \\frac{Y_i - \\mu_i(\\theta)}{\\operatorname{Var}(Y_i)} \\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta_j}\n$$\nSubstituting $\\operatorname{Var}(Y_i) = \\phi \\mu_i(\\theta)$ and $\\frac{\\partial \\mu_i}{\\partial \\theta_j} = \\mu_i a_{ij}$ (from the log-link), we get:\n$$\nU_{Q,j}(\\theta) = \\sum_{i=1}^n \\frac{Y_i - \\mu_i(\\theta)}{\\phi \\mu_i(\\theta)} (\\mu_i(\\theta) a_{ij}) = \\frac{1}{\\phi} \\sum_{i=1}^n (Y_i - \\mu_i(\\theta)) a_{ij}\n$$\nThis is the quasi-score for $\\theta_j$. The full quasi-score vector is $\\mathbf{U}_Q(\\theta) = \\frac{1}{\\phi} \\mathbf{A}^\\top(\\mathbf{Y}-\\boldsymbol{\\mu})$, where $\\mathbf{A}$ is the $n \\times p$ matrix of covariates $a_i^\\top$.\n\n- **Quasi-Likelihood for $\\mu_i$**: The quasi-likelihood $Q(\\mu_i; y_i)$ is a function whose derivative with respect to $\\mu_i$ yields the score-like term for that observation:\n$$\n\\frac{\\partial Q(\\mu_i; y_i)}{\\partial \\mu_i} = \\frac{y_i - \\mu_i}{\\operatorname{Var}(Y_i)} = \\frac{y_i - \\mu_i}{\\phi \\mu_i}\n$$\nIntegrating with respect to $\\mu_i$ gives the quasi-likelihood function (up to an additive constant):\n$$\nQ(\\mu_i; y_i) = \\int \\frac{y_i - t}{\\phi t} dt = \\frac{1}{\\phi} \\int \\left(\\frac{y_i}{t} - 1\\right) dt = \\frac{1}{\\phi} (y_i \\ln(\\mu_i) - \\mu_i) + \\text{constant}\n$$\nNotice this is precisely the Poisson log-likelihood for a single observation, scaled by $1/\\phi$.\n\n- **Asymptotic Covariance Adjustment for $\\hat{\\theta}$**: The estimate $\\hat{\\theta}$ is found by solving $\\mathbf{U}_Q(\\hat{\\theta})=\\mathbf{0}$. The asymptotic covariance of such an M-estimator is given by the sandwich formula $\\operatorname{Cov}(\\hat{\\theta}) \\approx \\mathcal{J}^{-1} \\mathcal{K} (\\mathcal{J}^{-1})^\\top$, where $\\mathcal{J} = -\\mathbb{E}\\left[\\frac{\\partial \\mathbf{U}_Q}{\\partial \\theta^\\top}\\right]$ and $\\mathcal{K} = \\mathbb{E}[\\mathbf{U}_Q \\mathbf{U}_Q^\\top]$.\nFirst, we compute $\\mathcal{J}$:\n$$\n\\frac{\\partial \\mathbf{U}_Q}{\\partial \\theta^\\top} = \\frac{1}{\\phi} \\frac{\\partial}{\\partial \\theta^\\top} \\left(\\sum_{i=1}^n(Y_i - \\mu_i) \\mathbf{a}_i\\right) = -\\frac{1}{\\phi} \\sum_{i=1}^n \\mathbf{a}_i \\frac{\\partial \\mu_i}{\\partial \\theta^\\top} = -\\frac{1}{\\phi} \\sum_{i=1}^n \\mathbf{a}_i (\\mu_i \\mathbf{a}_i^\\top) = -\\frac{1}{\\phi} \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}\n$$\nwhere $\\mathbf{D} = \\operatorname{diag}(\\mu_1, \\dots, \\mu_n)$. So, $\\mathcal{J} = \\frac{1}{\\phi} \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}$.\nNext, we compute $\\mathcal{K}$:\n$$\n\\mathcal{K} = \\mathbb{E}[\\mathbf{U}_Q \\mathbf{U}_Q^\\top] = \\mathbb{E}\\left[ \\left(\\frac{1}{\\phi} \\mathbf{A}^\\top(\\mathbf{Y}-\\boldsymbol{\\mu})\\right) \\left(\\frac{1}{\\phi} \\mathbf{A}^\\top(\\mathbf{Y}-\\boldsymbol{\\mu})\\right)^\\top \\right] = \\frac{1}{\\phi^2} \\mathbf{A}^\\top \\mathbb{E}[(\\mathbf{Y}-\\boldsymbol{\\mu})(\\mathbf{Y}-\\boldsymbol{\\mu})^\\top] \\mathbf{A}\n$$\nThe matrix $\\mathbb{E}[(\\mathbf{Y}-\\boldsymbol{\\mu})(\\mathbf{Y}-\\boldsymbol{\\mu})^\\top]$ is the covariance matrix of $\\mathbf{Y}$, which is $\\operatorname{Cov}(\\mathbf{Y}) = \\operatorname{diag}(\\operatorname{Var}(Y_i)) = \\operatorname{diag}(\\phi \\mu_i) = \\phi\\mathbf{D}$.\n$$\n\\mathcal{K} = \\frac{1}{\\phi^2} \\mathbf{A}^\\top (\\phi\\mathbf{D}) \\mathbf{A} = \\frac{1}{\\phi} \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}\n$$\nThe Poisson information matrix is $I_P(\\theta) = \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}$. So we have $\\mathcal{J} = \\frac{1}{\\phi}I_P(\\theta)$ and $\\mathcal{K} = \\frac{1}{\\phi}I_P(\\theta)$.\nThe asymptotic covariance is:\n$$\n\\operatorname{Cov}(\\hat{\\theta}) \\approx \\left(\\frac{1}{\\phi}I_P(\\theta)\\right)^{-1} \\left(\\frac{1}{\\phi}I_P(\\theta)\\right) \\left(\\frac{1}{\\phi}I_P(\\theta)\\right)^{-1} = \\left(\\phi I_P(\\theta)^{-1}\\right) \\left(\\frac{1}{\\phi}I_P(\\theta)\\right) \\left(\\phi I_P(\\theta)^{-1}\\right) = \\phi I_P(\\theta)^{-1}\n$$\nThe adjustment is that the naive asymptotic covariance from the Poisson model, $I_P(\\theta)^{-1}$, is scaled by the dispersion parameter $\\phi$.\n\nPart 3: Numerical Computation\n\nGiven are $n=8$ counts, $p=3$ parameters, the fitted means $\\boldsymbol{\\mu}$, and observed counts $\\mathbf{y}$. We are asked to compute the Pearson-based estimator of dispersion, $\\hat{\\phi}_{\\text{Pearson}} = \\frac{X^2}{n-p}$.\nThe fitted means are $\\boldsymbol{\\mu} = (10.5, 4.8, 7.9, 18.6, 13.2, 3.1, 6.5, 9.7)$.\nThe observed counts are $\\mathbf{y} = (12, 5, 8, 20, 15, 3, 7, 9)$.\nThe degrees of freedom are $n-p = 8-3=5$.\n\nFirst, we compute the Pearson chi-square statistic $X^2 = \\sum_{i=1}^n \\frac{(y_i - \\mu_i)^2}{\\mu_i}$.\n\\begin{align*}\nX^2 = \\frac{(12-10.5)^2}{10.5} + \\frac{(5-4.8)^2}{4.8} + \\frac{(8-7.9)^2}{7.9} + \\frac{(20-18.6)^2}{18.6} \\\\\n\\quad + \\frac{(15-13.2)^2}{13.2} + \\frac{(3-3.1)^2}{3.1} + \\frac{(7-6.5)^2}{6.5} + \\frac{(9-9.7)^2}{9.7} \\\\\n= \\frac{1.5^2}{10.5} + \\frac{0.2^2}{4.8} + \\frac{0.1^2}{7.9} + \\frac{1.4^2}{18.6} + \\frac{1.8^2}{13.2} + \\frac{(-0.1)^2}{3.1} + \\frac{0.5^2}{6.5} + \\frac{(-0.7)^2}{9.7} \\\\\n= \\frac{2.25}{10.5} + \\frac{0.04}{4.8} + \\frac{0.01}{7.9} + \\frac{1.96}{18.6} + \\frac{3.24}{13.2} + \\frac{0.01}{3.1} + \\frac{0.25}{6.5} + \\frac{0.49}{9.7} \\\\\n\\approx 0.21428571 + 0.00833333 + 0.00126582 + 0.10537634 + 0.24545455 + 0.00322581 + 0.03846154 + 0.05051546 \\\\\n\\approx 0.66691856\n\\end{align*}\nNow, we compute the dispersion estimator:\n$$\n\\hat{\\phi}_{\\text{Pearson}} = \\frac{X^2}{n-p} = \\frac{0.66691856}{5} \\approx 0.13338371\n$$\nRounding to four significant figures, we get $0.1334$.",
            "answer": "$$\\boxed{0.1334}$$"
        },
        {
            "introduction": "Choosing an appropriate measurement error model is critical for accurate data assimilation, but what are the precise consequences of choosing the wrong one? This exercise directly addresses this question by analyzing a scenario where count data, truly generated from a Poisson process, is incorrectly modeled using a Gaussian likelihood. By employing the Godambe (or \"sandwich\") information matrix, you will quantify the exact loss in statistical efficiency, providing a clear and practical lesson on the importance of correct model specification. ",
            "id": "3402423",
            "problem": "Consider a one-parameter linear inverse problem in which an unknown scalar parameter $\\theta_{0} \\in (0,\\infty)$ is observed through a known nonnegative design sequence $\\{a_{i}\\}_{i=1}^{n}$ with $a_{i}  0$ for all $i$. The true data-generating mechanism produces independent counts $Y_{i}$ with $Y_{i} \\sim \\text{Poisson}(a_{i}\\,\\theta_{0})$. However, a data assimilation algorithm incorrectly assumes a Gaussian white noise model with known constant variance $\\tau^{2} \\in (0,\\infty)$ and mean $a_{i}\\,\\theta$, that is, $Y_{i} \\stackrel{\\text{model}}{\\sim} \\mathcal{N}(a_{i}\\,\\theta,\\tau^{2})$, and uses the maximum likelihood estimator under this Gaussian model to estimate $\\theta$.\n\nStarting from the core definitions of the score function, its covariance, and the expected negative Hessian of the log-likelihood, derive the Godambe (sandwich) information for $\\theta$ under this model misspecification and obtain the associated large-sample asymptotic variance of the Gaussian pseudo-maximum likelihood estimator. Then, starting from the Fisher information for the correctly specified Poisson model, obtain the large-sample asymptotic variance of the Poisson maximum likelihood estimator. Finally, quantify the variance inflation by providing the ratio of the misspecified-model asymptotic variance to the correctly specified-model asymptotic variance.\n\nExpress your final answer as a single closed-form analytic expression in terms of $\\{a_{i}\\}_{i=1}^{n}$ only. No numerical evaluation is required, and no rounding is needed. The final answer must be a single expression, not an inequality or an equation.",
            "solution": "The solution proceeds in three parts. First, we analyze the pseudo-maximum likelihood estimator from the misspecified Gaussian model. Second, we analyze the maximum likelihood estimator from the correctly specified Poisson model. Third, we compute the ratio of their asymptotic variances.\n\n**Part 1: Asymptotic Variance of the Gaussian Pseudo-MLE**\n\nThe assumed model is $Y_i \\sim \\mathcal{N}(a_i\\theta, \\tau^2)$. The log-likelihood function under this misspecified model, $\\ell_n(\\theta; \\mathbf{Y})$, is:\n$$ \\ell_n(\\theta; \\mathbf{Y}) = \\sum_{i=1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(Y_i - a_i\\theta)^2}{2\\tau^2} \\right) \\right) = -\\frac{n}{2}\\ln(2\\pi\\tau^2) - \\frac{1}{2\\tau^2} \\sum_{i=1}^{n} (Y_i - a_i\\theta)^2 $$\nThe score function, $S_n(\\theta)$, is the derivative of $\\ell_n(\\theta; \\mathbf{Y})$ with respect to $\\theta$:\n$$ S_n(\\theta) = \\frac{\\partial \\ell_n}{\\partial \\theta} = -\\frac{1}{2\\tau^2} \\sum_{i=1}^{n} 2(Y_i - a_i\\theta)(-a_i) = \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(Y_i - a_i\\theta) $$\nThe pseudo-MLE, denoted $\\hat{\\theta}_G$, is found by setting $S_n(\\hat{\\theta}_G) = 0$:\n$$ \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(Y_i - a_i\\hat{\\theta}_G) = 0 \\implies \\sum_{i=1}^{n} a_i Y_i - \\hat{\\theta}_G \\sum_{i=1}^{n} a_i^2 = 0 $$\n$$ \\hat{\\theta}_G = \\frac{\\sum_{i=1}^{n} a_i Y_i}{\\sum_{i=1}^{n} a_i^2} $$\nThe asymptotic variance of a pseudo-MLE is given by the \"sandwich\" formula $J_n(\\theta_0)^{-1} I_n(\\theta_0) J_n(\\theta_0)^{-1}$, where $J_n(\\theta_0) = -E_{\\theta_0}\\left[\\frac{\\partial^2 \\ell_n}{\\partial \\theta^2}\\right]_{\\theta=\\theta_0}$ and $I_n(\\theta_0) = \\text{Var}_{\\theta_0}[S_n(\\theta_0)]$. The expectations and variances are taken under the true data-generating process, i.e., $Y_i \\sim \\text{Poisson}(a_i\\theta_0)$.\n\nFirst, we find $J_n(\\theta_0)$:\n$$ \\frac{\\partial^2 \\ell_n}{\\partial \\theta^2} = \\frac{\\partial S_n(\\theta)}{\\partial \\theta} = \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(-a_i) = -\\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i^2 $$\nSince this expression does not depend on the data $Y_i$ or $\\theta$, the expectation is trivial:\n$$ J_n(\\theta_0) = - \\left( -\\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i^2 \\right) = \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i^2 $$\nNext, we find $I_n(\\theta_0)$, the variance of the score function evaluated at the true parameter $\\theta_0$:\n$$ I_n(\\theta_0) = \\text{Var}_{\\theta_0}[S_n(\\theta_0)] = \\text{Var}_{\\theta_0}\\left[\\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(Y_i - a_i\\theta_0)\\right] $$\nSince the $Y_i$ are independent, the variance of the sum is the sum of variances:\n$$ I_n(\\theta_0) = \\frac{1}{(\\tau^2)^2} \\sum_{i=1}^{n} \\text{Var}_{\\theta_0}[a_i Y_i] = \\frac{1}{\\tau^4} \\sum_{i=1}^{n} a_i^2 \\text{Var}_{\\theta_0}[Y_i] $$\nUnder the true Poisson model, $\\text{Var}_{\\theta_0}[Y_i] = a_i\\theta_0$.\n$$ I_n(\\theta_0) = \\frac{1}{\\tau^4} \\sum_{i=1}^{n} a_i^2 (a_i\\theta_0) = \\frac{\\theta_0}{\\tau^4} \\sum_{i=1}^{n} a_i^3 $$\nThe large-sample asymptotic variance of $\\hat{\\theta}_G$, denoted $V_G(\\hat{\\theta}_G)$, is:\n$$ V_G(\\hat{\\theta}_G) = J_n(\\theta_0)^{-1} I_n(\\theta_0) J_n(\\theta_0)^{-1} = \\left(\\frac{1}{\\tau^2}\\sum_{i=1}^{n} a_i^2\\right)^{-1} \\left(\\frac{\\theta_0}{\\tau^4}\\sum_{i=1}^{n} a_i^3\\right) \\left(\\frac{1}{\\tau^2}\\sum_{i=1}^{n} a_i^2\\right)^{-1} $$\n$$ V_G(\\hat{\\theta}_G) = \\frac{\\tau^2}{\\sum_{i=1}^{n} a_i^2} \\frac{\\theta_0 \\sum_{i=1}^{n} a_i^3}{\\tau^4} \\frac{\\tau^2}{\\sum_{i=1}^{n} a_i^2} = \\frac{\\theta_0 \\sum_{i=1}^{n} a_i^3}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2} $$\nFor this linear estimator, this asymptotic variance is identical to the exact finite-sample variance.\n\n**Part 2: Asymptotic Variance of the Poisson MLE**\n\nThe true model is $Y_i \\sim \\text{Poisson}(a_i\\theta)$. The log-likelihood function $L_n(\\theta; \\mathbf{Y})$ is:\n$$ L_n(\\theta; \\mathbf{Y}) = \\sum_{i=1}^{n} \\log\\left( \\frac{(a_i\\theta)^{Y_i} e^{-a_i\\theta}}{Y_i!} \\right) = \\sum_{i=1}^{n} (Y_i\\log(a_i) + Y_i\\log(\\theta) - a_i\\theta - \\log(Y_i!)) $$\nThe score function is:\n$$ \\frac{\\partial L_n}{\\partial \\theta} = \\sum_{i=1}^{n} \\left(\\frac{Y_i}{\\theta} - a_i\\right) = \\frac{1}{\\theta}\\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} a_i $$\nThe MLE, denoted $\\hat{\\theta}_P$, is found by setting the score to zero:\n$$ \\frac{1}{\\hat{\\theta}_P}\\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} a_i = 0 \\implies \\hat{\\theta}_P = \\frac{\\sum_{i=1}^{n} Y_i}{\\sum_{i=1}^{n} a_i} $$\nThe asymptotic variance of the MLE is given by the inverse of the Fisher information, $I_P(\\theta_0)^{-1}$. The Fisher information is $I_P(\\theta_0) = -E_{\\theta_0}\\left[\\frac{\\partial^2 L_n}{\\partial \\theta^2}\\right]_{\\theta=\\theta_0}$.\nThe second derivative of the log-likelihood is:\n$$ \\frac{\\partial^2 L_n}{\\partial \\theta^2} = -\\frac{1}{\\theta^2}\\sum_{i=1}^{n} Y_i $$\nTaking the expectation under the true model (where $E_{\\theta_0}[Y_i] = a_i\\theta_0$):\n$$ E_{\\theta_0}\\left[\\frac{\\partial^2 L_n}{\\partial \\theta^2}\\right]_{\\theta=\\theta_0} = E_{\\theta_0}\\left[-\\frac{1}{\\theta_0^2}\\sum_{i=1}^{n} Y_i\\right] = -\\frac{1}{\\theta_0^2}\\sum_{i=1}^{n} E_{\\theta_0}[Y_i] = -\\frac{1}{\\theta_0^2}\\sum_{i=1}^{n} a_i\\theta_0 = -\\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i $$\nThe Fisher information is:\n$$ I_P(\\theta_0) = - \\left(-\\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i\\right) = \\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i $$\nThe large-sample asymptotic variance of $\\hat{\\theta}_P$, denoted $V_P(\\hat{\\theta}_P)$, is the inverse of the Fisher information:\n$$ V_P(\\hat{\\theta}_P) = [I_P(\\theta_0)]^{-1} = \\left(\\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i\\right)^{-1} = \\frac{\\theta_0}{\\sum_{i=1}^{n} a_i} $$\n\n**Part 3: Ratio of Variances (Variance Inflation)**\n\nThe final step is to compute the ratio of the misspecified-model variance to the correctly specified-model variance. This ratio quantifies the loss of efficiency due to using the incorrect Gaussian model.\n$$ \\text{Ratio} = \\frac{V_G(\\hat{\\theta}_G)}{V_P(\\hat{\\theta}_P)} = \\frac{\\frac{\\theta_0 \\sum_{i=1}^{n} a_i^3}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2}}{\\frac{\\theta_0}{\\sum_{i=1}^{n} a_i}} $$\nThe true parameter $\\theta_0$ cancels out:\n$$ \\text{Ratio} = \\frac{\\sum_{i=1}^{n} a_i^3}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2} \\cdot \\left(\\sum_{i=1}^{n} a_i\\right) = \\frac{\\left(\\sum_{i=1}^{n} a_i\\right) \\left(\\sum_{i=1}^{n} a_i^3\\right)}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2} $$\nThis expression depends only on the design sequence $\\{a_i\\}$, as required. By the Cauchy-Schwarz inequality, this ratio is always greater than or equal to $1$, representing an inflation of variance.",
            "answer": "$$\n\\boxed{\\frac{\\left(\\sum_{i=1}^{n} a_{i}\\right) \\left(\\sum_{i=1}^{n} a_{i}^{3}\\right)}{\\left(\\sum_{i=1}^{n} a_{i}^{2}\\right)^{2}}}\n$$"
        }
    ]
}