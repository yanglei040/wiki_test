{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，通过具体计算来巩固理解是至关重要的。这项练习将后验均值和协方差的一般公式应用于贝叶斯线性回归的特定场景。通过构建一个具有正交特征的数据集，你将亲手推导并观察数据结构如何简化后验计算，并揭示特征解耦如何使得对一个参数的学习不影响对其他参数的推断 。",
            "id": "3103067",
            "problem": "考虑贝叶斯线性回归（BLR），其中设计矩阵 $X \\in \\mathbb{R}^{N \\times D}$ 通过一个线性模型将权重向量 $w \\in \\mathbb{R}^{D}$ 映射到预测目标。观测目标向量为 $t \\in \\mathbb{R}^{N}$。假设似然为高斯分布，其噪声精度为 $\\beta > 0$，并且系数服从一个独立的零均值高斯先验，其精度为 $\\alpha > 0$。\n\n按如下方式构建一个具有标准正交列的特定数据集：令 $N=3$ 和 $D=2$，并取\n$$\nX \\;=\\;\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix},\n\\qquad\nt \\;=\\;\n\\begin{pmatrix}\n4 \\\\ -1 \\\\ 2\n\\end{pmatrix}.\n$$\n验证 $X$ 的列是标准正交的。然后，从高斯似然 $p(t \\mid w, X, \\beta)$ 和高斯先验 $p(w \\mid \\alpha)$ 的定义出发，应用贝叶斯定理 $p(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta)\\,p(w \\mid \\alpha)$，推导出 $w$ 上的后验分布。在你的推导过程中，通过配方法来确定后验均值和后验协方差矩阵，并利用 $X$ 列的标准正交性来简化后验协方差。\n\n根据你的推导，解释 $X$ 列的正交性如何使 $w$ 中各个系数的后验分布解耦。\n\n最后，在 $\\alpha = 2$ 和 $\\beta = 3$ 的情况下，计算此数据集的精确后验协方差矩阵。将后验协方差矩阵作为你的最终答案。无需进行四舍五入。",
            "solution": "该问题是有效的，因为它是贝叶斯线性回归中的一个标准练习，在科学上和数学上都是合理的，并为得出唯一解提供了所有必要的信息。\n\n首先，我们验证矩阵 $X$ 的列是标准正交的。将这些列记为 $x_1$ 和 $x_2$。\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\qquad x_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n标准正交性要求内积 $x_i^T x_j = \\delta_{ij}$，其中 $\\delta_{ij}$ 是克罗内克 δ 符号。我们检查内积：\n$$\nx_1^T x_1 = (1)(1) + (0)(0) + (0)(0) = 1\n$$\n$$\nx_2^T x_2 = (0)(0) + (1)(1) + (0)(0) = 1\n$$\n$$\nx_1^T x_2 = (1)(0) + (0)(1) + (0)(0) = 0\n$$\n由于 $x_1^T x_1 = 1$，$x_2^T x_2 = 1$，且 $x_1^T x_2 = 0$，所以这些列是标准正交的。一个更简洁的验证方法是计算 $X^T X$：\n$$\nX^T X = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2\n$$\n由于 $X^T X = I_D$，其中 $D=2$ 是列数，根据定义，$X$ 的列是标准正交的。\n\n接下来，我们推导权重 $w$ 的后验分布。该模型由一个高斯似然和一个高斯先验指定。\n似然为 $p(t \\mid w, X, \\beta) = \\mathcal{N}(t \\mid Xw, \\beta^{-1}I_N)$，其中 $I_N$ 是 $N \\times N$ 的单位矩阵。其概率密度函数为：\n$$\np(t \\mid w, X, \\beta) \\propto \\exp\\left( -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) \\right)\n$$\n权重的先验为 $p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, \\alpha^{-1}I_D)$，其中 $I_D$ 是 $D \\times D$ 的单位矩阵。其概率密度函数为：\n$$\np(w \\mid \\alpha) \\propto \\exp\\left( -\\frac{\\alpha}{2} w^T w \\right)\n$$\n根据贝叶斯定理，后验分布正比于似然与先验的乘积：\n$$\np(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta) \\, p(w \\mid \\alpha)\n$$\n我们处理后验的对数，因为这将指数的乘积简化为其参数的和：\n$$\n\\ln p(w \\mid t, X, \\alpha, \\beta) = -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\n我们展开二次项：\n$$\n(t - Xw)^T (t - Xw) = t^T t - t^T Xw - w^T X^T t + w^T X^T Xw = t^T t - 2w^T X^T t + w^T X^T Xw\n$$\n将此代入对数后验表达式中：\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{\\beta}{2} (t^T t - 2w^T X^T t + w^T X^T Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\n我们合并包含 $w$ 的项：\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t - \\frac{\\beta}{2} t^T t + \\text{const}\n$$\n省略不依赖于 $w$ 的项：\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t + \\text{const'}\n$$\n这是关于 $w$ 的一个二次型，这意味着后验是一个高斯分布 $p(w \\mid t, \\dots) = \\mathcal{N}(w \\mid m_N, S_N)$。一个一般多元高斯分布 $\\mathcal{N}(w \\mid m_N, S_N)$ 的对数密度为：\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} (w - m_N)^T S_N^{-1} (w - m_N) + \\text{const} = -\\frac{1}{2} (w^T S_N^{-1} w - 2w^T S_N^{-1} m_N + m_N^T S_N^{-1} m_N) + \\text{const}\n$$\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} w^T S_N^{-1} w + w^T S_N^{-1} m_N + \\text{const''}\n$$\n通过比较我们的对数后验表达式与一般对数高斯密度中关于 $w$ 的二次项和一次项的系数，我们可以确定后验精度矩阵（逆协方差）$S_N^{-1}$ 和均值 $m_N$。\n比较二次项 ($w^T(\\cdot)w$)：\n$$\nS_N^{-1} = \\beta X^T X + \\alpha I_D\n$$\n因此，后验协方差矩阵是：\n$$\nS_N = (\\beta X^T X + \\alpha I_D)^{-1}\n$$\n比较一次项 ($w^T(\\cdot)$)：\n$$\nS_N^{-1} m_N = \\beta X^T t \\implies m_N = S_N (\\beta X^T t) = \\beta (\\beta X^T X + \\alpha I_D)^{-1} X^T t\n$$\n这些是贝叶斯线性回归中后验均值和协方差的一般表达式。\n\n现在，我们利用 $X$ 的列是标准正交的这一性质，即 $X^T X = I_D$。将此代入 $S_N$ 和 $m_N$ 的表达式中：\n对于后验协方差 $S_N$：\n$$\nS_N = (\\beta I_D + \\alpha I_D)^{-1} = ((\\beta + \\alpha) I_D)^{-1} = \\frac{1}{\\alpha + \\beta} I_D\n$$\n对于后验均值 $m_N$：\n$$\nm_N = \\beta \\left(\\frac{1}{\\alpha + \\beta} I_D\\right) X^T t = \\frac{\\beta}{\\alpha + \\beta} X^T t\n$$\n$X$ 列的标准正交性显著简化了后验参数。后验协方差矩阵 $S_N = \\frac{1}{\\alpha + \\beta} I_D$ 是一个对角矩阵。在多元高斯分布中，对角协方差矩阵意味着随机变量是不相关的。对于高斯变量，不相关等价于统计独立。\n联合后验密度为：\n$$\np(w \\mid t, \\dots) \\propto \\exp\\left(-\\frac{1}{2} (w-m_N)^T S_N^{-1} (w-m_N)\\right) = \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w-m_N)^T (w-m_N)\\right)\n$$\n$$\n= \\exp\\left(-\\frac{\\alpha+\\beta}{2} \\sum_{j=1}^D (w_j - m_{N,j})^2\\right) = \\prod_{j=1}^D \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w_j - m_{N,j})^2\\right)\n$$\n这表明联合后验 $p(w \\mid t, \\dots)$ 可以分解为每个系数 $w_j$ 的单个后验的乘积：$p(w_1, \\dots, w_D \\mid t, \\dots) = \\prod_{j=1}^D p(w_j \\mid t, \\dots)$。这种分解就是系数后验分布的“解耦”。每个 $w_j$ 都有一个单变量高斯后验分布，其均值为 $m_{N,j}$，方差为 $1/(\\alpha+\\beta)$。特征的标准正交性确保了对一个权重 $w_j$ 的学习不会提供任何关于其他权重 $w_k$ ($k \\neq j$) 的信息（除了先验已知的信息之外）。\n\n最后，我们对给定的数据集，当 $\\alpha = 2$ 和 $\\beta = 3$ 时，计算精确的后验协方差矩阵。我们使用从 $X$ 列的标准正交性推导出的简化公式。权重向量的维度是 $D=2$。\n$$\nS_N = \\frac{1}{\\alpha + \\beta} I_D = \\frac{1}{2 + 3} I_2 = \\frac{1}{5} I_2\n$$\n将其写成矩阵形式：\n$$\nS_N = \\frac{1}{5} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{5} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在实际应用中，我们常常错误地认为“更多的数据总能减少不确定性”。本练习旨在挑战这一直觉，通过引入一个未知的模型偏差（一个所谓的“滋扰参数”），你将探索数据同化过程中的一个微妙现象。通过代数推导，你将量化并解释为什么在估计模型固有偏差时，我们对目标参数的后验不确定性（即方差）反而可能增加 。这个过程深刻揭示了模型中不同参数之间的耦合如何影响我们从数据中获取的信息。",
            "id": "3411826",
            "problem": "考虑一个线性逆问题，其未知二维参数向量为 $x = (x_{1}, x_{2})^{\\top}$。根据模型 $y = h_{1} x_{1} + h_{2} x_{2} + b + \\epsilon$ 采集单个标量观测 $y \\in \\mathbb{R}$，其中 $h_{1}, h_{2} \\in \\mathbb{R}$ 是已知的传感器灵敏度，$b \\in \\mathbb{R}$ 是一个未知的加性偏差，$\\epsilon \\in \\mathbb{R}$ 是观测噪声。假设高斯先验 $x \\sim \\mathcal{N}(0, C_{x})$，其中 $C_{x} = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$，$b \\sim \\mathcal{N}(0, \\sigma_{b}^{2})$，以及高斯噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$，且噪声独立于 $(x,b)$。\n\n考虑两种数据同化设置：\n- 设置 A（偏差固定）：观测模型为 $y = h_{1} x_{1} + h_{2} x_{2} + \\epsilon$，其中 $b \\equiv 0$ 为已知。\n- 设置 B（偏差未知）：观测模型为 $y = h_{1} x_{1} + h_{2} x_{2} + b + \\epsilon$，其中 $b$ 被视为一个具有其先验的未知参数。\n\n从线性高斯模型的贝叶斯推断基础出发，推导在每种设置下 $x$ 的后验协方差。然后，重点关注第一个分量，以闭式解计算引入未知偏差 $b$ 所引起的 $x_{1}$ 后验方差的增量，定义为\n$$\\Delta \\operatorname{Var}(x_{1}) = \\operatorname{Var}(x_{1} \\mid y, \\text{ 设置 B}) - \\operatorname{Var}(x_{1} \\mid y, \\text{ 设置 A})$$。\n提供 $\\Delta \\operatorname{Var}(x_{1})$ 关于 $\\sigma_{1}^{2}$、$\\sigma_{2}^{2}$、$\\sigma_{\\epsilon}^{2}$、$\\sigma_{b}^{2}$、$h_{1}$ 和 $h_{2}$ 的最终表达式。\n\n用你推导的代数式为基础，用文字解释参数-偏差耦合如何导致 $x$ 的某些分量后验方差增加的机制。\n\n你的最终答案必须是单一的闭式解析表达式。不需要进行数值四舍五入。",
            "solution": "该问题要求在两种不同的建模假设下，为一个线性高斯逆问题推导和比较参数 $x_{1}$ 的后验方差。解决方案的核心在于对此类模型应用标准贝叶斯公式来计算后验分布。对于一个具有高斯先验 $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z}_{p}, C_{p})$ 的状态向量 $\\mathbf{z}$ 和一个线性观测模型 $\\mathbf{y} = M \\mathbf{z} + \\mathbf{e}$（其中高斯噪声为 $\\mathbf{e} \\sim \\mathcal{N}(0, R)$），给定 $\\mathbf{y}$ 时 $\\mathbf{z}$ 的后验分布也是高斯的，即 $\\mathbf{z} \\mid \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{z}_{\\text{post}}, C_{\\text{post}})$，其后验协方差 $C_{\\text{post}}$ 由后验精度的逆给出：\n$$C_{\\text{post}}^{-1} = C_{p}^{-1} + M^{\\top} R^{-1} M$$\n\n我们将把这个框架应用于设置 A 和设置 B。\n\n**设置 A：偏差固定 ($b=0$)**\n\n在此设置中，未知状态向量为 $x = (x_{1}, x_{2})^{\\top}$。\n先验分布为 $x \\sim \\mathcal{N}(0, C_{x})$，因此先验均值为 $x_{p} = 0$，先验协方差为 $C_{p} = C_{x} = \\begin{pmatrix} \\sigma_{1}^{2}  0 \\\\ 0  \\sigma_{2}^{2} \\end{pmatrix}$。\n观测模型为 $y = h_{1} x_{1} + h_{2} x_{2} + \\epsilon$，可以写成 $y = Hx + \\epsilon$，其中观测算子是行矩阵 $H = \\begin{pmatrix} h_{1}  h_{2} \\end{pmatrix}$。\n观测噪声为 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$，因此观测误差协方差是标量 $R = \\sigma_{\\epsilon}^{2}$。\n\n$x$ 的后验协方差（记为 $C_{A}$）通过首先计算后验精度矩阵 $C_{A}^{-1}$ 来求得：\n$$C_{A}^{-1} = C_{x}^{-1} + H^{\\top} R^{-1} H$$\n各组成部分为：\n$$C_{x}^{-1} = \\begin{pmatrix} 1/\\sigma_{1}^{2}  0 \\\\ 0  1/\\sigma_{2}^{2} \\end{pmatrix}$$\n$$H^{\\top} R^{-1} H = \\begin{pmatrix} h_{1} \\\\ h_{2} \\end{pmatrix} (\\sigma_{\\epsilon}^{-2}) \\begin{pmatrix} h_{1}  h_{2} \\end{pmatrix} = \\frac{1}{\\sigma_{\\epsilon}^{2}} \\begin{pmatrix} h_{1}^{2}  h_{1}h_{2} \\\\ h_{1}h_{2}  h_{2}^{2} \\end{pmatrix}$$\n将它们相加得到后验精度：\n$$C_{A}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma_{1}^{2}} + \\frac{h_{1}^{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}} \\end{pmatrix}$$\n为了找到后验协方差 $C_{A}$，我们对这个 $2 \\times 2$ 矩阵求逆。给定 $y$ 时 $x_{1}$ 的方差 $\\operatorname{Var}(x_{1} \\mid y, \\text{设置 A})$ 是 $C_{A}$ 的 $(1,1)$ 元素。\n$C_{A}^{-1}$ 的行列式为：\n$$\\det(C_{A}^{-1}) = \\left(\\frac{1}{\\sigma_{1}^{2}} + \\frac{h_{1}^{2}}{\\sigma_{\\epsilon}^{2}}\\right) \\left(\\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}}\\right) - \\left(\\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}}\\right)^2 = \\frac{\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{\\epsilon}^{2}}$$\n$C_{A}$ 的 $(1,1)$ 元素是 $C_{A}^{-1}$ 的 $(2,2)$ 元素除以其行列式：\n$$\\operatorname{Var}(x_{1} \\mid y, \\text{设置 A}) = (C_{A})_{11} = \\frac{\\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}}}{\\det(C_{A}^{-1})} = \\frac{\\frac{\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{2}^{2}\\sigma_{\\epsilon}^{2}}}{\\frac{\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{\\epsilon}^{2}}}$$\n$$\\operatorname{Var}(x_{1} \\mid y, \\text{设置 A}) = \\frac{\\sigma_{1}^{2} (\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2})}{\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}$$\n\n**设置 B：偏差未知**\n\n在此设置中，未知偏差 $b$ 被视为一个随机变量。我们构成一个增广状态向量 $z = (x_{1}, x_{2}, b)^{\\top}$。\n$z$ 的先验为 $z \\sim \\mathcal{N}(0, C_{z})$，其中 $C_{z}$ 是由 $x$ 和 $b$ 的独立先验构成的块对角协方差矩阵：\n$$C_{z} = \\begin{pmatrix} C_{x}  0 \\\\ 0  \\sigma_{b}^{2} \\end{pmatrix} = \\begin{pmatrix} \\sigma_{1}^{2}  0  0 \\\\ 0  \\sigma_{2}^{2}  0 \\\\ 0  0  \\sigma_{b}^{2} \\end{pmatrix}$$\n观测模型为 $y = h_{1}x_{1} + h_{2}x_{2} + b + \\epsilon$，就增广状态而言，可以写成 $y = Kz + \\epsilon$，其中 $K = \\begin{pmatrix} h_{1}  h_{2}  1 \\end{pmatrix}$。噪声特性保持不变，$R = \\sigma_{\\epsilon}^{2}$。\n\n增广状态的后验协方差 $C_{B}$ 由其逆矩阵 $C_{B}^{-1}$ 求得：\n$$C_{B}^{-1} = C_{z}^{-1} + K^{\\top} R^{-1} K$$\n各组成部分为：\n$$C_{z}^{-1} = \\begin{pmatrix} 1/\\sigma_{1}^{2}  0  0 \\\\ 0  1/\\sigma_{2}^{2}  0 \\\\ 0  0  1/\\sigma_{b}^{2} \\end{pmatrix}$$\n$$K^{\\top} R^{-1} K = \\frac{1}{\\sigma_{\\epsilon}^{2}} \\begin{pmatrix} h_{1} \\\\ h_{2} \\\\ 1 \\end{pmatrix} \\begin{pmatrix} h_{1}  h_{2}  1 \\end{pmatrix} = \\frac{1}{\\sigma_{\\epsilon}^{2}} \\begin{pmatrix} h_{1}^{2}  h_{1}h_{2}  h_{1} \\\\ h_{1}h_{2}  h_{2}^{2}  h_{2} \\\\ h_{1}  h_{2}  1 \\end{pmatrix}$$\n将它们相加得到增广状态的后验精度：\n$$C_{B}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma_{1}^{2}} + \\frac{h_{1}^{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{h_{1}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{1}}{\\sigma_{\\epsilon}^{2}}  \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{1}{\\sigma_{b}^{2}} + \\frac{1}{\\sigma_{\\epsilon}^{2}} \\end{pmatrix}$$\n在此设置中，$x_{1}$ 的后验方差 $\\operatorname{Var}(x_{1} \\mid y, \\text{设置 B})$ 是矩阵 $C_{B}$ 的 $(1,1)$ 元素。它可以通过计算 $C_{B}^{-1}$ 的 $(1,1)$ 代数余子式除以其行列式得到。\n行列式可以使用矩阵行列式引理 $\\det(A + \\mathbf{u}\\mathbf{v}^{\\top}) = (1+\\mathbf{v}^{\\top}A^{-1}\\mathbf{u})\\det(A)$ 求得。这里，$A=C_z^{-1}$，$\\mathbf{u}\\mathbf{v}^{\\top} = K^{\\top}R^{-1}K$。这给出：\n$$\\det(C_{B}^{-1}) = \\det(C_{z}^{-1}) \\left(1 + \\frac{K C_{z} K^{\\top}}{\\sigma_{\\epsilon}^{2}}\\right) = \\frac{1}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{b}^{2}} \\left(1 + \\frac{h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2} + \\sigma_{b}^{2}}{\\sigma_{\\epsilon}^{2}}\\right) = \\frac{\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{b}^{2}\\sigma_{\\epsilon}^{2}}$$\n$C_{B}^{-1}$ 的 $(1,1)$ 代数余子式为：\n$$ (C_{B}^{-1})_{11}^{\\text{cofactor}} = \\det \\begin{pmatrix} \\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}}  \\frac{1}{\\sigma_{b}^{2}} + \\frac{1}{\\sigma_{\\epsilon}^{2}} \\end{pmatrix} = \\frac{\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{2}^{2}\\sigma_{b}^{2}\\sigma_{\\epsilon}^{2}}$$\n$x_{1}$ 的后验方差为二者之比：\n$$\\operatorname{Var}(x_{1} \\mid y, \\text{设置 B}) = (C_{B})_{11} = \\frac{(C_{B}^{-1})_{11}^{\\text{cofactor}}}{\\det(C_{B}^{-1})} = \\frac{\\sigma_{1}^{2}(\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{2}^{2}\\sigma_{2}^{2})}{\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}$$\n\n**后验方差的增加**\n\n我们现在计算 $\\Delta \\operatorname{Var}(x_{1}) = \\operatorname{Var}(x_{1} \\mid y, \\text{设置 B}) - \\operatorname{Var}(x_{1} \\mid y, \\text{设置 A})$。\n令 $V_{A} = \\operatorname{Var}(x_{1} \\mid y, \\text{A})$ 且 $V_{B} = \\operatorname{Var}(x_{1} \\mid y, \\text{B})$。\n令 $D_{A} = \\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}$ 且 $D_{B} = \\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2} = D_{A} + \\sigma_{b}^{2}$。\n再令 $N_{A} = \\sigma_{1}^{2} (\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2})$ 且 $N_{B} = \\sigma_{1}^{2}(\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{2}^{2}\\sigma_{2}^{2}) = N_{A} + \\sigma_{1}^{2}\\sigma_{b}^{2}$。\n因此我们有 $V_{A} = N_{A}/D_{A}$ 和 $V_{B} = N_{B}/D_{B}$。\n\n$$\\Delta \\operatorname{Var}(x_{1}) = \\frac{N_{A} + \\sigma_{1}^{2}\\sigma_{b}^{2}}{D_{A} + \\sigma_{b}^{2}} - \\frac{N_{A}}{D_{A}} = \\frac{D_{A}(N_{A} + \\sigma_{1}^{2}\\sigma_{b}^{2}) - N_{A}(D_{A} + \\sigma_{b}^{2})}{D_{A}(D_{A} + \\sigma_{b}^{2})}$$\n$$\\Delta \\operatorname{Var}(x_{1}) = \\frac{D_{A}N_{A} + D_{A}\\sigma_{1}^{2}\\sigma_{b}^{2} - N_{A}D_{A} - N_{A}\\sigma_{b}^{2}}{D_{A}D_{B}} = \\frac{\\sigma_{b}^{2} (D_{A}\\sigma_{1}^{2} - N_{A})}{D_{A}D_{B}}$$\n分子中的项为：\n$$D_{A}\\sigma_{1}^{2} - N_{A} = (\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})\\sigma_{1}^{2} - \\sigma_{1}^{2} (\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2}) = h_{1}^{2}\\sigma_{1}^{4}$$\n将其代回，我们得到方差增量的最终表达式：\n$$\\Delta \\operatorname{Var}(x_{1}) = \\frac{h_{1}^{2}\\sigma_{1}^{4}\\sigma_{b}^{2}}{(\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})(\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})}$$\n\n**方差增加的解释**\n\n当偏差 $b$ 未知时，$x_{1}$ 的后验方差增加是参数耦合以及数据中信息分配的一种体现。\n\n在设置 A 中，观测 $y$ 提供信息来约束两个参数 $x_{1}$ 和 $x_{2}$。在设置 B 中，同一个单一观测必须用于约束三个参数：$x_{1}$、$x_{2}$ 和 $b$。测量所提供的信息现在被“摊薄”到更多未知量上。\n\n其代数机制是观测模型引入的参数之间的耦合。在设置 B 中，后验精度矩阵 $C_{B}^{-1}$ 包含非零的非对角项，例如 $(C_{B}^{-1})_{13} = h_{1}/\\sigma_{\\epsilon}^{2}$，它将 $x_{1}$ 和 $b$ 联系起来。在设置 A 中该项为零（因为 $b$ 不是状态的一部分）。当这个精度矩阵被求逆得到协方差矩阵 $C_{B}$ 时，这种耦合确保了一个参数的不确定性会传播到其他参数。具体来说，由 $\\sigma_{b}^{2}$ 量化的偏差的先验不确定性“泄漏”到 $x_{1}$ 的后验不确定性中。观测 $y = h_{1}x_{1} + h_{2}x_{2} + b + \\epsilon$ 无法完美区分由 $x_{1}$ 引起的变化和由 $b$ 引起的变化。这种模糊性导致了 $x_1$ 的后验方差比 $b$ 已知时更大。\n\n推导出的 $\\Delta \\operatorname{Var}(x_{1})$ 表达式清晰地说明了这一点：\n- 该增量与偏差的先验方差 $\\sigma_{b}^{2}$ 成正比。如果偏差没有先验不确定性，则方差没有增加。\n- 该增量与 $h_{1}^{2}$ 成正比。如果 $x_1$ 不影响观测（$h_1=0$），它就不会通过测量与偏差耦合，其方差也不受 $b$ 中不确定性的影响。\n- 该增量还与 $\\sigma_{1}^{4}$ 成正比，表明具有较高先验不确定性的参数更容易受到这种效应的影响。\n\n本质上，估计像偏差 $b$ 这样的讨厌参数（nuisance parameter）的代价是降低了对主要目标参数估计的精度。",
            "answer": "$$\\boxed{\\frac{h_{1}^{2} \\sigma_{1}^{4} \\sigma_{b}^{2}}{(\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}) (\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})}}$$"
        },
        {
            "introduction": "数据是分批一次性到达，还是随时间流式获得？这个问题引出了批量（batch）更新和序贯（sequential）更新这两种基本的数据同化策略。本练习将引导你证明这两种方法在理论上是等价的，并推导出用于序贯更新的著名的一阶（rank-1）更新公式 。更重要的是，此练习还将迫使你思考超越理想化的数学理论，去面对在有限精度计算机上实现这些算法时遇到的数值稳定性挑战，并探讨其解决方案。",
            "id": "3411817",
            "problem": "考虑一个线性高斯逆问题，其先验和观测模型定义如下。未知状态 $x \\in \\mathbb{R}^{n}$ 服从高斯先验 $x \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$，其中协方差 $C_{\\text{prior}} \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 矩阵。观测值由线性模型 $y = H x + \\eta$ 生成，其中 $H \\in \\mathbb{R}^{m \\times n}$ 是观测算子，$\\eta \\sim \\mathcal{N}(0, R)$ 是零均值高斯噪声，其协方差 $R \\in \\mathbb{R}^{m \\times m}$ 是 SPD 矩阵。假设 $m \\geq 1$，并且随着新传感器的加入，观测行是逐个在线添加的。您将仅使用基础高斯恒等式和分块矩阵求逆工具来比较后验协方差的批量更新和增量更新。\n\n从通过贝叶斯法则定义的后验密度以及高斯似然和先验的二次型出发，完成以下任务：\n\n1) 推导当两个标量传感器同时到达时的批量后验协方差 $C_{\\text{post}}^{\\text{batch}}$ 的表达式，其中\n- 第一个传感器：$y_{1} \\in \\mathbb{R}$，观测向量 $h_{1} \\in \\mathbb{R}^{n}$，噪声方差 $r_{1} \\in \\mathbb{R}_{>0}$，\n- 第二个传感器：$y_{2} \\in \\mathbb{R}$，观测向量 $h_{2} \\in \\mathbb{R}^{n}$，噪声方差 $r_{2} \\in \\mathbb{R}_{>0}$，\n因此 $H = \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix}$ 且 $R = \\operatorname{diag}(r_{1}, r_{2})$。\n\n2) 现在假设传感器是在线到达的。处理第一个传感器以获得一个中间后验，其协方差为 $C_{1}$。然后，在给定 $y_{1}$ 的条件下，对 $(x, y_{2})$ 的联合高斯分布使用分块矩阵求逆（Schur 补）方法来处理第二个传感器，并推导出形如 $C_{2}$ 的关于 $C_{1}$、$h_{2}$ 和 $r_{2}$ 的显式秩-1 更新协方差公式。通过代数方法证明 $C_{2} = C_{\\text{post}}^{\\text{batch}}$。\n\n3) 给出第二个传感器的类似秩-1 后验均值更新公式，用 $C_{1}$、$h_{2}$、$r_{2}$ 和新息 $y_{2} - h_{2}^{\\top} m_{1}$ 表示，其中 $m_{1}$ 是同化第一个传感器后的后验均值。\n\n4) 使用以下具体的数值设置：\n- $n = 2$，\n- $C_{\\text{prior}} = \\begin{pmatrix} 2  3/5 \\\\ 3/5  1 \\end{pmatrix}$，\n- $h_{1} = \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix}$，$r_{1} = 1/2$，\n- $h_{2} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}$，$r_{2} = 2/5$，\n计算 $C_{\\text{post}}^{\\text{batch}}$ 和通过您推导的秩-1 协方差更新，按顺序处理 $y_{1}$ 再处理 $y_{2}$ 得到的 $C_{2}$。报告两者之差的弗罗贝尼乌斯范数，\n$$\n\\left\\| C_{\\text{post}}^{\\text{batch}} - C_{2} \\right\\|_{F},\n$$\n作为您的最终答案。请提供精确值，无需四舍五入。\n\n5) 简要识别并论证在有限精度算术中应用增量秩-1 协方差更新时出现的至少两个数值稳定性问题，并讨论平方根实现或 Joseph 稳定形式如何解决这些问题。\n\n您的最终答案必须是第 4 项中要求的弗罗贝尼乌斯范数的单个实数值。",
            "solution": "该问题是线性高斯模型贝叶斯推断中的一个标准练习，旨在解决批量数据同化和序贯数据同化之间的等价性问题。我们将按顺序解决问题的每个部分。\n\n### 问题验证\n\n首先根据要求对问题进行验证。\n\n**第 1 步：提取已知条件**\n- **先验分布**：$x \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$，其中 $x \\in \\mathbb{R}^{n}$ 且 $C_{\\text{prior}} \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定 (SPD) 协方差矩阵。\n- **观测模型**：$y = H x + \\eta$，其中 $H \\in \\mathbb{R}^{m \\times n}$ 是观测算子。\n- **观测噪声**：$\\eta \\sim \\mathcal{N}(0, R)$，其中 $R \\in \\mathbb{R}^{m \\times m}$ 是一个 SPD 协方差矩阵。\n- **批量设置 (第 1 部分)**：两个标量传感器。\n  - 传感器 1：$y_1 \\in \\mathbb{R}$，$h_1 \\in \\mathbb{R}^n$，噪声方差 $r_1 > 0$。\n  - 传感器 2：$y_2 \\in \\mathbb{R}$，$h_2 \\in \\mathbb{R}^n$，噪声方差 $r_2 > 0$。\n  - 组合算子和噪声协方差：$H = \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix}$，$R = \\operatorname{diag}(r_{1}, r_{2})$。\n- **序贯设置 (第 2 部分)**：传感器在线到达。首先，处理 $y_1$ 得到协方差为 $C_1$ 的中间后验。然后，处理 $y_2$。\n- **数值 (第 4 部分)**：\n  - $n = 2$\n  - $C_{\\text{prior}} = \\begin{pmatrix} 2  3/5 \\\\ 3/5  1 \\end{pmatrix}$\n  - $h_{1} = \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix}$，$r_{1} = 1/2$\n  - $h_{2} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}$，$r_{2} = 2/5$\n\n**第 2 步：使用提取的已知条件进行验证**\n该问题定义明确且科学上合理。\n- **科学基础**：它涉及贝叶斯推断的基本概念，特别是针对线性高斯系统，这是卡尔曼滤波和数据同化的基础。一个核心结论是批量更新和序贯更新的等价性。\n- **适定性**：问题被完全指定。目标是推导标准公式，证明它们的等价性，并执行具体的数值计算。所有必需的矩阵和参数都已提供。先验协方差 $C_{\\text{prior}}$ 是对称的，其主子式为 $2 > 0$ 和 $\\det(C_{\\text{prior}}) = 2(1) - (3/5)^2 = 2 - 9/25 = 41/25 > 0$，确认其为 SPD。噪声方差 $r_1, r_2$ 为正，因此 $R$ 是 SPD。\n- **目标**：问题使用精确的数学语言陈述，没有歧义或主观性。\n\n**第 3 步：结论和行动**\n问题有效。我们将进行详细解答。\n\n### 第 1 部分：批量后验协方差\n\n后验概率密度函数 $p(x|y)$ 由贝叶斯法则给出：$p(x|y) \\propto p(y|x)p(x)$。对于高斯分布，这等价于对概率密度的指数部分求和。负对数后验 $J(x)$ 是 $x$ 的二次函数：\n$$J(x) = \\frac{1}{2}(x - m_{\\text{prior}})^{\\top} C_{\\text{prior}}^{-1} (x - m_{\\text{prior}}) + \\frac{1}{2}(y - Hx)^{\\top} R^{-1} (y - Hx) + \\text{const.}$$\n后验分布也是高斯分布，$x|y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$。后验协方差的逆 $C_{\\text{post}}^{-1}$ 是 $J(x)$ 的 Hessian 矩阵，对应于 $x$ 的二次项。展开 $J(x)$ 可得：\n$$J(x) = \\frac{1}{2} x^{\\top} (C_{\\text{prior}}^{-1} + H^{\\top}R^{-1}H) x - x^{\\top} (C_{\\text{prior}}^{-1}m_{\\text{prior}} + H^{\\top}R^{-1}y) + \\text{const.}$$\n通过与一般高斯形式 $\\frac{1}{2}(x - m_{\\text{post}})^{\\top} C_{\\text{post}}^{-1} (x - m_{\\text{post}})$ 的项进行比对，我们得到逆后验协方差：\n$$(C_{\\text{post}})^{-1} = C_{\\text{prior}}^{-1} + H^{\\top}R^{-1}H$$\n对于指定的具有两个标量传感器的批量更新，我们有 $H = \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix}$ 和 $R = \\begin{pmatrix} r_{1}  0 \\\\ 0  r_{2} \\end{pmatrix}$。其逆为 $R^{-1} = \\begin{pmatrix} r_{1}^{-1}  0 \\\\ 0  r_{2}^{-1} \\end{pmatrix}$。$H^{\\top}R^{-1}H$ 项变为：\n$$H^{\\top}R^{-1}H = \\begin{pmatrix} h_{1}  h_{2} \\end{pmatrix} \\begin{pmatrix} r_{1}^{-1}  0 \\\\ 0  r_{2}^{-1} \\end{pmatrix} \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix} = r_{1}^{-1}h_{1}h_{1}^{\\top} + r_{2}^{-1}h_{2}h_{2}^{\\top}$$\n因此，逆批量后验协方差为：\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = C_{\\text{prior}}^{-1} + r_{1}^{-1}h_{1}h_{1}^{\\top} + r_{2}^{-1}h_{2}h_{2}^{\\top}$$\n批量后验协方差是该表达式的逆：\n$$C_{\\text{post}}^{\\text{batch}} = (C_{\\text{prior}}^{-1} + r_{1}^{-1}h_{1}h_{1}^{\\top} + r_{2}^{-1}h_{2}h_{2}^{\\top})^{-1}$$\n\n### 第 2 部分：序贯协方差更新与等价性\n\n首先，同化传感器 1 得到一个协方差为 $C_1$ 的后验。这是批量公式在 $H=h_1^\\top$ 和 $R=r_1$ 时的特例。\n$$C_{1}^{-1} = C_{\\text{prior}}^{-1} + r_1^{-1} h_1 h_1^\\top$$\n接下来，我们同化传感器 2，使用第 1 步的后验作为新的先验：$x \\sim \\mathcal{N}(m_1, C_1)$。观测模型是 $y_2 = h_2^\\top x + \\eta_2$，其中 $\\eta_2 \\sim \\mathcal{N}(0, r_2)$。在给定 $y_1$ 的条件下，$(x, y_2)$ 的联合分布是高斯的。这个联合分布的协方差是：\n$$\\text{Cov}\\left( \\begin{pmatrix} x \\\\ y_2 \\end{pmatrix} | y_1 \\right) = \\begin{pmatrix} C_1  C_1 h_2 \\\\ h_2^\\top C_1  h_2^\\top C_1 h_2 + r_2 \\end{pmatrix}$$\n使用条件高斯分布的公式，给定 $y_2$（和 $y_1$）后 $x$ 的后验协方差由该分块协方差矩阵的 Schur 补给出：\n$$C_2 = C_1 - (C_1 h_2) (h_2^\\top C_1 h_2 + r_2)^{-1} (h_2^\\top C_1)$$\n由于 $h_2^\\top C_1 h_2 + r_2$ 是一个标量，这简化为秩-1 更新：\n$$C_2 = C_1 - \\frac{C_1 h_2 h_2^\\top C_1}{r_2 + h_2^\\top C_1 h_2}$$\n为了证明 $C_2 = C_{\\text{post}}^{\\text{batch}}$，我们可以处理逆协方差（信息矩阵）。逆协方差的更新法则是加性的：\n$$C_2^{-1} = C_1^{-1} + r_2^{-1} h_2 h_2^\\top$$\n这可以通过将 Sherman-Morrison-Woodbury 公式应用于 $C_2$ 的表达式来证明。令 $A=C_1^{-1}$，$U=h_2$，$C=r_2^{-1}$，$V=h_2^\\top$。那么 $(C_1^{-1} + r_2^{-1} h_2 h_2^\\top)^{-1} = C_1 - C_1 h_2 (r_2 + h_2^\\top C_1 h_2)^{-1} h_2^\\top C_1$，这与 $C_2$ 的公式相符。\n代入 $C_1^{-1}$ 的表达式：\n$$C_2^{-1} = (C_{\\text{prior}}^{-1} + r_1^{-1} h_1 h_1^\\top) + r_2^{-1} h_2 h_2^\\top = C_{\\text{prior}}^{-1} + r_1^{-1} h_1 h_1^\\top + r_2^{-1} h_2 h_2^\\top$$\n这恰好是 $(C_{\\text{post}}^{\\text{batch}})^{-1}$ 的表达式。因此，$C_2 = C_{\\text{post}}^{\\text{batch}}$，证明了序贯更新和批量更新产生相同的后验协方差。\n\n### 第 3 部分：后验均值更新\n\n后验均值更新公式也源于条件高斯分布的性质。更新后的均值 $m_2$ 由以下公式给出：\n$$m_2 = m_1 + (C_1 h_2) (h_2^\\top C_1 h_2 + r_2)^{-1} (y_2 - h_2^\\top m_1)$$\n其中 $m_1$ 是同化第一个传感器后的均值。项 $y_2 - h_2^\\top m_1$ 是新息，代表观测 $y_2$ 提供的新信息。更新可以写成：\n$$m_2 = m_1 + K_2 (y_2 - h_2^\\top m_1)$$\n其中 $K_2 = C_1 h_2 (h_2^\\top C_1 h_2 + r_2)^{-1}$ 是第二个传感器的卡尔曼增益。\n\n### 第 4 部分：数值计算与弗罗贝尼乌斯范数\n\n我们计算批量法和序贯法的逆协方差矩阵，并证明它们是相同的。这意味着矩阵本身是相同的，它们之差的弗罗贝尼乌斯范数为 0。这种方法避免了复杂的矩阵求逆和分数运算。\n\n首先，计算所需的分量：\n$C_{\\text{prior}} = \\begin{pmatrix} 2  3/5 \\\\ 3/5  1 \\end{pmatrix} \\implies C_{\\text{prior}}^{-1} = \\frac{1}{2-9/25}\\begin{pmatrix} 1  -3/5 \\\\ -3/5  2 \\end{pmatrix} = \\frac{25}{41}\\begin{pmatrix} 1  -3/5 \\\\ -3/5  2 \\end{pmatrix} = \\frac{1}{41}\\begin{pmatrix} 25  -15 \\\\ -15  50 \\end{pmatrix}$.\n$h_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix}, r_1 = 1/2 \\implies r_1^{-1}=2$.\n$r_1^{-1}h_1 h_1^\\top = 2 \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix} \\begin{pmatrix} 1  1/2 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  1/2 \\end{pmatrix}$.\n$h_2 = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}, r_2 = 2/5 \\implies r_2^{-1}=5/2$.\n$r_2^{-1}h_2 h_2^\\top = \\frac{5}{2} \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} -1/3  1 \\end{pmatrix} = \\frac{5}{2} \\begin{pmatrix} 1/9  -1/3 \\\\ -1/3  1 \\end{pmatrix} = \\begin{pmatrix} 5/18  -5/6 \\\\ -5/6  5/2 \\end{pmatrix}$.\n\n**批量计算：**\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = C_{\\text{prior}}^{-1} + r_1^{-1}h_1 h_1^\\top + r_2^{-1}h_2 h_2^\\top$$\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = \\frac{1}{41}\\begin{pmatrix} 25  -15 \\\\ -15  50 \\end{pmatrix} + \\begin{pmatrix} 2  1 \\\\ 1  1/2 \\end{pmatrix} + \\begin{pmatrix} 5/18  -5/6 \\\\ -5/6  5/2 \\end{pmatrix}$$\n我们找一个公分母，即 $41 \\times 18 = 738$。\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = \\frac{18}{738}\\begin{pmatrix} 25  -15 \\\\ -15  50 \\end{pmatrix} + \\frac{738}{738}\\begin{pmatrix} 2  1 \\\\ 1  1/2 \\end{pmatrix} + \\frac{41}{738}\\begin{pmatrix} 5  -15 \\\\ -15  45 \\end{pmatrix}$$\n$$ = \\frac{1}{738} \\left[ \\begin{pmatrix} 450  -270 \\\\ -270  900 \\end{pmatrix} + \\begin{pmatrix} 1476  738 \\\\ 738  369 \\end{pmatrix} + \\begin{pmatrix} 205  -615 \\\\ -615  1845 \\end{pmatrix} \\right]$$\n合并各项：\n$$ = \\frac{1}{738} \\begin{pmatrix} 450+1476+205  -270+738-615 \\\\ -270+738-615  900+369+1845 \\end{pmatrix}$$\n$$ = \\frac{1}{738} \\begin{pmatrix} 2131  -147 \\\\ -147  3114 \\end{pmatrix}$$\n\n**序贯计算：**\n首先，求 $C_1^{-1}$：\n$$C_1^{-1} = C_{\\text{prior}}^{-1} + r_1^{-1}h_1 h_1^\\top = \\frac{1}{41}\\begin{pmatrix} 25  -15 \\\\ -15  50 \\end{pmatrix} + \\begin{pmatrix} 2  1 \\\\ 1  1/2 \\end{pmatrix}$$\n公分母是 $41 \\times 2 = 82$。\n$$C_1^{-1} = \\frac{2}{82}\\begin{pmatrix} 25  -15 \\\\ -15  50 \\end{pmatrix} + \\frac{82}{82}\\begin{pmatrix} 2  1 \\\\ 1  1/2 \\end{pmatrix} = \\frac{1}{82} \\left( \\begin{pmatrix} 50  -30 \\\\ -30  100 \\end{pmatrix} + \\begin{pmatrix} 164  82 \\\\ 82  41 \\end{pmatrix} \\right) = \\frac{1}{82}\\begin{pmatrix} 214  52 \\\\ 52  141 \\end{pmatrix}$$\n接下来，求 $C_2^{-1} = C_1^{-1} + r_2^{-1}h_2 h_2^\\top$：\n$$C_2^{-1} = \\frac{1}{82}\\begin{pmatrix} 214  52 \\\\ 52  141 \\end{pmatrix} + \\begin{pmatrix} 5/18  -5/6 \\\\ -5/6  5/2 \\end{pmatrix}$$\n$82=2 \\times 41$ 和 $18=2 \\times 9$ 的公分母是 $2 \\times 9 \\times 41 = 738$。\n$$C_2^{-1} = \\frac{9}{738}\\begin{pmatrix} 214  52 \\\\ 52  141 \\end{pmatrix} + \\frac{41}{738}\\begin{pmatrix} 5  -15 \\\\ -15  45 \\end{pmatrix}$$\n$$ = \\frac{1}{738} \\left[ \\begin{pmatrix} 1926  468 \\\\ 468  1269 \\end{pmatrix} + \\begin{pmatrix} 205  -615 \\\\ -615  1845 \\end{pmatrix} \\right] = \\frac{1}{738} \\begin{pmatrix} 2131  -147 \\\\ -147  3114 \\end{pmatrix}$$\n如推导所示，$(C_{\\text{post}}^{\\text{batch}})^{-1}$ 和 $C_2^{-1}$ 是相同的。由于矩阵的逆是唯一的，这意味着 $C_{\\text{post}}^{\\text{batch}} = C_2$。差值矩阵是零矩阵：\n$$C_{\\text{post}}^{\\text{batch}} - C_2 = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$$\n零矩阵的弗罗贝尼乌斯范数是：\n$$\\left\\| C_{\\text{post}}^{\\text{batch}} - C_2 \\right\\|_{F} = \\sqrt{0^2+0^2+0^2+0^2} = 0$$\n\n### 第 5 部分：数值稳定性问题\n\n标准的秩-1 协方差更新公式 $C_k = C_{k-1} - K_k h_k^\\top C_{k-1}$，虽然在代数上是正确的，但在有限精度算术中存在数值不稳定性。\n1.  **失去正定性**：该公式涉及两个矩阵的减法。理论上，$C_k$ 保证是对称正定的。然而，由于浮点舍入误差，计算出的结果可能会失去对称性，更严重的是，可能不再是正定的。如果协方差矩阵出现负特征值，滤波器可能会变得不稳定并发生发散。当不确定性显著减小，使得 $C_{k-1}$ 接近奇异时，这个问题尤其严重。\n2.  **灾难性抵消**：当先验不确定性很大（$C_{k-1}$ 的元素值很大）且观测信息非常丰富时，更新项 $K_k h_k^\\top C_{k-1}$ 也可能是一个大矩阵，其元素值与 $C_{k-1}$ 的元素值在数量级上接近。两个几乎相等的大数相减会导致相对精度的严重损失，这种效应被称为灾难性抵消。\n\n这些问题可以通过其他形式的公式来解决：\n-   **平方根滤波器**：这些算法传递协方差的“平方根”$S_k$，其中 $C_k = S_k S_k^\\top$。更新法则直接为 $S_k$ 推导。通过构造，得到的协方差矩阵 $C_k = S_k S_k^\\top$ 总是对称半正定的，从而防止了因舍入误差导致的正定性丢失。此外，$S_k$ 的条件数是 $C_k$ 条件数的平方根，这通常会改善问题的数值性质。\n-   **Joseph 稳定形式**：一种替代的、更稳定的协方差更新公式是 Joseph 形式：\n    $$C_k = (I - K_k h_k^\\top) C_{k-1} (I - K_k h_k^\\top)^\\top + K_k r_k K_k^\\top$$\n    这个表达式在代数上与标准形式等价，但在数值上更优越。它通过对两个矩阵求和来计算新的协方差 $C_k$，而这两个矩阵根据其结构（$A A^\\top$）保证是半正定的。半正定矩阵之和总是半正定的。这种形式避免了导致正定性丢失的直接减法，但代价是计算复杂度有所增加。",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}