{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a foundational understanding of how the structure of an experimental design impacts posterior uncertainty. We will explore a deliberately simplified scenario in Bayesian linear regression where the features are orthogonal. This exercise demonstrates how orthogonality leads to a diagonal posterior covariance matrix, effectively decoupling the uncertainties of the model parameters and providing a clear baseline for interpreting more complex, correlated posteriors .",
            "id": "3103067",
            "problem": "Consider Bayesian linear regression (BLR), where a design matrix $X \\in \\mathbb{R}^{N \\times D}$ maps a weight vector $w \\in \\mathbb{R}^{D}$ to predicted targets via a linear model. The observed target vector is $t \\in \\mathbb{R}^{N}$. Assume a Gaussian likelihood with noise precision $\\beta > 0$ and an independent zero-mean Gaussian prior over coefficients with precision $\\alpha > 0$.\n\nConstruct a specific dataset with orthonormal columns as follows: let $N=3$ and $D=2$, and take\n$$\nX \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix},\n\\qquad\nt \\;=\\;\n\\begin{pmatrix}\n4 \\\\ -1 \\\\ 2\n\\end{pmatrix}.\n$$\nVerify that the columns of $X$ are orthonormal. Then, starting from the definitions of the Gaussian likelihood $p(t \\mid w, X, \\beta)$ and the Gaussian prior $p(w \\mid \\alpha)$, and applying Bayes' theorem $p(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta)\\,p(w \\mid \\alpha)$, derive the posterior distribution over $w$. In your derivation, complete the square to identify both the posterior mean and the posterior covariance matrix, and simplify the posterior covariance using the orthonormality of the columns of $X$.\n\nExplain, based on your derivation, how orthogonality of the columns of $X$ decouples the posterior distributions of the individual coefficients in $w$.\n\nFinally, with $\\alpha = 2$ and $\\beta = 3$, evaluate the exact posterior covariance matrix for this dataset. Report the posterior covariance matrix as your final answer. No rounding is required.",
            "solution": "The problem is valid as it is a standard exercise in Bayesian linear regression, is scientifically and mathematically sound, and provides all necessary information for a unique solution.\n\nFirst, we validate that the columns of the matrix $X$ are orthonormal. Let the columns be denoted by $x_1$ and $x_2$.\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\qquad x_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nOrthonormality requires that the inner product $x_i^T x_j = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. We check the inner products:\n$$\nx_1^T x_1 = (1)(1) + (0)(0) + (0)(0) = 1\n$$\n$$\nx_2^T x_2 = (0)(0) + (1)(1) + (0)(0) = 1\n$$\n$$\nx_1^T x_2 = (1)(0) + (0)(1) + (0)(0) = 0\n$$\nSince $x_1^T x_1 = 1$, $x_2^T x_2 = 1$, and $x_1^T x_2 = 0$, the columns are orthonormal. A more compact way to verify this is to compute $X^T X$:\n$$\nX^T X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2\n$$\nSince $X^T X = I_D$ where $D=2$ is the number of columns, the columns of $X$ are, by definition, orthonormal.\n\nNext, we derive the posterior distribution over the weights $w$. The model is specified by a Gaussian likelihood and a Gaussian prior.\nThe likelihood is $p(t \\mid w, X, \\beta) = \\mathcal{N}(t \\mid Xw, \\beta^{-1}I_N)$, where $I_N$ is the $N \\times N$ identity matrix. The probability density function is:\n$$\np(t \\mid w, X, \\beta) \\propto \\exp\\left( -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) \\right)\n$$\nThe prior on the weights is $p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, \\alpha^{-1}I_D)$, where $I_D$ is the $D \\times D$ identity matrix. Its probability density function is:\n$$\np(w \\mid \\alpha) \\propto \\exp\\left( -\\frac{\\alpha}{2} w^T w \\right)\n$$\nAccording to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta) \\, p(w \\mid \\alpha)\n$$\nWe work with the logarithm of the posterior, as this simplifies the product of exponentials into a sum of their arguments:\n$$\n\\ln p(w \\mid t, X, \\alpha, \\beta) = -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\nWe expand the quadratic term:\n$$\n(t - Xw)^T (t - Xw) = t^T t - t^T Xw - w^T X^T t + w^T X^T Xw = t^T t - 2w^T X^T t + w^T X^T Xw\n$$\nSubstituting this back into the log-posterior expression:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{\\beta}{2} (t^T t - 2w^T X^T t + w^T X^T Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\nWe group terms involving $w$:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t - \\frac{\\beta}{2} t^T t + \\text{const}\n$$\nDropping terms not dependent on $w$:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t + \\text{const'}\n$$\nThis is a quadratic form in $w$, which implies the posterior is a Gaussian distribution, $p(w \\mid t, \\dots) = \\mathcal{N}(w \\mid m_N, S_N)$. The log-density of a general multivariate Gaussian $\\mathcal{N}(w \\mid m_N, S_N)$ is:\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} (w - m_N)^T S_N^{-1} (w - m_N) + \\text{const} = -\\frac{1}{2} (w^T S_N^{-1} w - 2w^T S_N^{-1} m_N + m_N^T S_N^{-1} m_N) + \\text{const}\n$$\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} w^T S_N^{-1} w + w^T S_N^{-1} m_N + \\text{const''}\n$$\nBy comparing the coefficients of the quadratic and linear terms in $w$ between our expression for the log-posterior and the general log-Gaussian density, we can identify the posterior precision matrix (inverse covariance) $S_N^{-1}$ and the mean $m_N$.\nComparing the quadratic terms ($w^T(\\cdot)w$):\n$$\nS_N^{-1} = \\beta X^T X + \\alpha I_D\n$$\nThus, the posterior covariance matrix is:\n$$\nS_N = (\\beta X^T X + \\alpha I_D)^{-1}\n$$\nComparing the linear terms ($w^T(\\cdot)$):\n$$\nS_N^{-1} m_N = \\beta X^T t \\implies m_N = S_N (\\beta X^T t) = \\beta (\\beta X^T X + \\alpha I_D)^{-1} X^T t\n$$\nThese are the general expressions for the posterior mean and covariance in Bayesian linear regression.\n\nNow, we use the property that the columns of $X$ are orthonormal, which means $X^T X = I_D$. Substituting this into the expressions for $S_N$ and $m_N$:\nFor the posterior covariance $S_N$:\n$$\nS_N = (\\beta I_D + \\alpha I_D)^{-1} = ((\\beta + \\alpha) I_D)^{-1} = \\frac{1}{\\alpha + \\beta} I_D\n$$\nFor the posterior mean $m_N$:\n$$\nm_N = \\beta \\left(\\frac{1}{\\alpha + \\beta} I_D\\right) X^T t = \\frac{\\beta}{\\alpha + \\beta} X^T t\n$$\nThe orthonormality of the columns of $X$ significantly simplifies the posterior parameters. The posterior covariance matrix $S_N = \\frac{1}{\\alpha + \\beta} I_D$ is a diagonal matrix. In a multivariate Gaussian distribution, a diagonal covariance matrix signifies that the random variables are uncorrelated. For Gaussian variables, being uncorrelated is equivalent to being statistically independent.\nThe joint posterior density is:\n$$\np(w \\mid t, \\dots) \\propto \\exp\\left(-\\frac{1}{2} (w-m_N)^T S_N^{-1} (w-m_N)\\right) = \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w-m_N)^T (w-m_N)\\right)\n$$\n$$\n= \\exp\\left(-\\frac{\\alpha+\\beta}{2} \\sum_{j=1}^D (w_j - m_{N,j})^2\\right) = \\prod_{j=1}^D \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w_j - m_{N,j})^2\\right)\n$$\nThis shows that the joint posterior $p(w \\mid t, \\dots)$ factorizes into a product of individual posteriors for each coefficient $w_j$: $p(w_1, \\dots, w_D \\mid t, \\dots) = \\prod_{j=1}^D p(w_j \\mid t, \\dots)$. This factorization is the \"decoupling\" of the posterior distributions of the coefficients. Each $w_j$ has a univariate Gaussian posterior distribution with mean $m_{N,j}$ and variance $1/(\\alpha+\\beta)$. The orthonormality of the features ensures that learning about one weight $w_j$ provides no information about any other weight $w_k$ ($k \\neq j$) beyond what is already known from the prior.\n\nFinally, we evaluate the exact posterior covariance matrix for the given dataset with $\\alpha = 2$ and $\\beta = 3$. We use the simplified formula derived from the orthonormality of $X$'s columns. The dimensionality of the weight vector is $D=2$.\n$$\nS_N = \\frac{1}{\\alpha + \\beta} I_D = \\frac{1}{2 + 3} I_2 = \\frac{1}{5} I_2\n$$\nWriting this as a matrix:\n$$\nS_N = \\frac{1}{5} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving from idealized models to more realistic scenarios, this practice introduces the challenge of an unknown systematic error, or bias, in the observation model. This exercise is designed to challenge the common intuition that assimilating more data always reduces parameter uncertainty. By analyzing how an unknown bias couples with the parameters of interest, you will see how uncertainty in one part of a model can propagate and inflate uncertainty elsewhere, a critical concept in real-world data assimilation and model calibration .",
            "id": "3411826",
            "problem": "Consider a linear inverse problem with an unknown two-dimensional parameter vector $x = (x_{1}, x_{2})^{\\top}$. A single scalar observation $y \\in \\mathbb{R}$ is collected according to the model $y = h_{1} x_{1} + h_{2} x_{2} + b + \\epsilon$, where $h_{1}, h_{2} \\in \\mathbb{R}$ are known sensor sensitivities, $b \\in \\mathbb{R}$ is an unknown additive bias, and $\\epsilon \\in \\mathbb{R}$ is observational noise. Assume Gaussian priors $x \\sim \\mathcal{N}(0, C_{x})$ with $C_{x} = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$, $b \\sim \\mathcal{N}(0, \\sigma_{b}^{2})$, and Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$, independent of $(x,b)$. \n\nTwo data assimilation settings are considered:\n- Setting A (bias fixed): the observation model is $y = h_{1} x_{1} + h_{2} x_{2} + \\epsilon$ with $b \\equiv 0$ known.\n- Setting B (bias unknown): the observation model is $y = h_{1} x_{1} + h_{2} x_{2} + b + \\epsilon$ with $b$ treated as an unknown parameter endowed with its prior.\n\nStarting from the fundamentals of Bayesian inference for linear-Gaussian models, derive the posterior covariance of $x$ in each setting. Then, focusing on the first component, compute in closed form the increase in the posterior variance of $x_{1}$ induced by introducing the unknown bias $b$, defined as \n$$\\Delta \\operatorname{Var}(x_{1}) = \\operatorname{Var}(x_{1} \\mid y, \\text{ Setting B}) - \\operatorname{Var}(x_{1} \\mid y, \\text{ Setting A}).$$\nProvide the final expression for $\\Delta \\operatorname{Var}(x_{1})$ in terms of $\\sigma_{1}^{2}$, $\\sigma_{2}^{2}$, $\\sigma_{\\epsilon}^{2}$, $\\sigma_{b}^{2}$, $h_{1}$, and $h_{2}$. \n\nExplain, in words grounded in the algebra you derive, the mechanism by which parameter–bias coupling yields an increase in the posterior variance for some components of $x$. \n\nYour final answer must be a single closed-form analytic expression. No numerical rounding is required.",
            "solution": "The problem requires the derivation and comparison of the posterior variance of a parameter $x_{1}$ under two different modeling assumptions for a linear-Gaussian inverse problem. The core of the solution lies in applying the standard Bayesian formula for the posterior distribution in such models. For a state vector $\\mathbf{z}$ with a Gaussian prior $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z}_{p}, C_{p})$ and a linear observation model $\\mathbf{y} = M \\mathbf{z} + \\mathbf{e}$ with Gaussian noise $\\mathbf{e} \\sim \\mathcal{N}(0, R)$, the posterior distribution for $\\mathbf{z}$ given $\\mathbf{y}$ is also Gaussian, $\\mathbf{z} \\mid \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{z}_{\\text{post}}, C_{\\text{post}})$, with posterior covariance $C_{\\text{post}}$ given by the inverse of the posterior precision:\n$$C_{\\text{post}}^{-1} = C_{p}^{-1} + M^{\\top} R^{-1} M$$\n\nWe will apply this framework to both Setting A and Setting B.\n\n**Setting A: Bias Fixed ($b=0$)**\n\nIn this setting, the unknown state vector is $x = (x_{1}, x_{2})^{\\top}$.\nThe prior distribution is $x \\sim \\mathcal{N}(0, C_{x})$, so the prior mean is $x_{p} = 0$ and the prior covariance is $C_{p} = C_{x} = \\begin{pmatrix} \\sigma_{1}^{2} & 0 \\\\ 0 & \\sigma_{2}^{2} \\end{pmatrix}$.\nThe observation model is $y = h_{1} x_{1} + h_{2} x_{2} + \\epsilon$, which can be written as $y = Hx + \\epsilon$, where the observation operator is the row matrix $H = \\begin{pmatrix} h_{1} & h_{2} \\end{pmatrix}$.\nThe observation noise is $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2})$, so the observation error covariance is the scalar $R = \\sigma_{\\epsilon}^{2}$.\n\nThe posterior covariance for $x$, denoted $C_{A}$, is found by first computing the posterior precision matrix $C_{A}^{-1}$:\n$$C_{A}^{-1} = C_{x}^{-1} + H^{\\top} R^{-1} H$$\nThe components are:\n$$C_{x}^{-1} = \\begin{pmatrix} 1/\\sigma_{1}^{2} & 0 \\\\ 0 & 1/\\sigma_{2}^{2} \\end{pmatrix}$$\n$$H^{\\top} R^{-1} H = \\begin{pmatrix} h_{1} \\\\ h_{2} \\end{pmatrix} (\\sigma_{\\epsilon}^{-2}) \\begin{pmatrix} h_{1} & h_{2} \\end{pmatrix} = \\frac{1}{\\sigma_{\\epsilon}^{2}} \\begin{pmatrix} h_{1}^{2} & h_{1}h_{2} \\\\ h_{1}h_{2} & h_{2}^{2} \\end{pmatrix}$$\nSumming these gives the posterior precision:\n$$C_{A}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma_{1}^{2}} + \\frac{h_{1}^{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}} \\end{pmatrix}$$\nTo find the posterior covariance $C_{A}$, we invert this $2 \\times 2$ matrix. The variance of $x_{1}$ given $y$, $\\operatorname{Var}(x_{1} \\mid y, \\text{Setting A})$, is the $(1,1)$ element of $C_{A}$.\nThe determinant of $C_{A}^{-1}$ is:\n$$\\det(C_{A}^{-1}) = \\left(\\frac{1}{\\sigma_{1}^{2}} + \\frac{h_{1}^{2}}{\\sigma_{\\epsilon}^{2}}\\right) \\left(\\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}}\\right) - \\left(\\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}}\\right)^2 = \\frac{\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{\\epsilon}^{2}}$$\nThe $(1,1)$ element of $C_{A}$ is the $(2,2)$ element of $C_{A}^{-1}$ divided by the determinant:\n$$\\operatorname{Var}(x_{1} \\mid y, \\text{Setting A}) = (C_{A})_{11} = \\frac{\\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}}}{\\det(C_{A}^{-1})} = \\frac{\\frac{\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{2}^{2}\\sigma_{\\epsilon}^{2}}}{\\frac{\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{\\epsilon}^{2}}}$$\n$$\\operatorname{Var}(x_{1} \\mid y, \\text{Setting A}) = \\frac{\\sigma_{1}^{2} (\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2})}{\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}$$\n\n**Setting B: Bias Unknown**\n\nIn this setting, the unknown bias $b$ is treated as a random variable. We form an augmented state vector $z = (x_{1}, x_{2}, b)^{\\top}$.\nThe prior for $z$ is $z \\sim \\mathcal{N}(0, C_{z})$, where $C_{z}$ is the block-diagonal covariance matrix formed from the independent priors of $x$ and $b$:\n$$C_{z} = \\begin{pmatrix} C_{x} & 0 \\\\ 0 & \\sigma_{b}^{2} \\end{pmatrix} = \\begin{pmatrix} \\sigma_{1}^{2} & 0 & 0 \\\\ 0 & \\sigma_{2}^{2} & 0 \\\\ 0 & 0 & \\sigma_{b}^{2} \\end{pmatrix}$$\nThe observation model is $y = h_{1}x_{1} + h_{2}x_{2} + b + \\epsilon$, which in terms of the augmented state is $y = Kz + \\epsilon$, with $K = \\begin{pmatrix} h_{1} & h_{2} & 1 \\end{pmatrix}$. The noise properties remain the same, $R = \\sigma_{\\epsilon}^{2}$.\n\nThe posterior covariance of the augmented state, $C_{B}$, is found from its inverse, $C_{B}^{-1}$:\n$$C_{B}^{-1} = C_{z}^{-1} + K^{\\top} R^{-1} K$$\nThe components are:\n$$C_{z}^{-1} = \\begin{pmatrix} 1/\\sigma_{1}^{2} & 0 & 0 \\\\ 0 & 1/\\sigma_{2}^{2} & 0 \\\\ 0 & 0 & 1/\\sigma_{b}^{2} \\end{pmatrix}$$\n$$K^{\\top} R^{-1} K = \\frac{1}{\\sigma_{\\epsilon}^{2}} \\begin{pmatrix} h_{1} \\\\ h_{2} \\\\ 1 \\end{pmatrix} \\begin{pmatrix} h_{1} & h_{2} & 1 \\end{pmatrix} = \\frac{1}{\\sigma_{\\epsilon}^{2}} \\begin{pmatrix} h_{1}^{2} & h_{1}h_{2} & h_{1} \\\\ h_{1}h_{2} & h_{2}^{2} & h_{2} \\\\ h_{1} & h_{2} & 1 \\end{pmatrix}$$\nSumming these gives the posterior precision for the augmented state:\n$$C_{B}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma_{1}^{2}} + \\frac{h_{1}^{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{h_{1}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{1}h_{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{1}}{\\sigma_{\\epsilon}^{2}} & \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{1}{\\sigma_{b}^{2}} + \\frac{1}{\\sigma_{\\epsilon}^{2}} \\end{pmatrix}$$\nThe posterior variance of $x_{1}$ in this setting, $\\operatorname{Var}(x_{1} \\mid y, \\text{Setting B})$, is the $(1,1)$ element of the matrix $C_{B}$. It is calculated as the $(1,1)$ cofactor of $C_{B}^{-1}$ divided by its determinant.\nThe determinant can be found using the matrix determinant lemma, $\\det(A + \\mathbf{u}\\mathbf{v}^{\\top}) = (1+\\mathbf{v}^{\\top}A^{-1}\\mathbf{u})\\det(A)$. Here, $A=C_z^{-1}$, $\\mathbf{u}\\mathbf{v}^{\\top} = K^{\\top}R^{-1}K$. This gives:\n$$\\det(C_{B}^{-1}) = \\det(C_{z}^{-1}) \\left(1 + \\frac{K C_{z} K^{\\top}}{\\sigma_{\\epsilon}^{2}}\\right) = \\frac{1}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{b}^{2}} \\left(1 + \\frac{h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2} + \\sigma_{b}^{2}}{\\sigma_{\\epsilon}^{2}}\\right) = \\frac{\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{1}^{2}\\sigma_{2}^{2}\\sigma_{b}^{2}\\sigma_{\\epsilon}^{2}}$$\nThe $(1,1)$ cofactor of $C_{B}^{-1}$ is:\n$$ (C_{B}^{-1})_{11}^{\\text{cofactor}} = \\det \\begin{pmatrix} \\frac{1}{\\sigma_{2}^{2}} + \\frac{h_{2}^{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}} \\\\ \\frac{h_{2}}{\\sigma_{\\epsilon}^{2}} & \\frac{1}{\\sigma_{b}^{2}} + \\frac{1}{\\sigma_{\\epsilon}^{2}} \\end{pmatrix} = \\frac{\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{2}^{2}\\sigma_{2}^{2}}{\\sigma_{2}^{2}\\sigma_{b}^{2}\\sigma_{\\epsilon}^{2}}$$\nThe posterior variance of $x_{1}$ is the ratio:\n$$\\operatorname{Var}(x_{1} \\mid y, \\text{Setting B}) = (C_{B})_{11} = \\frac{(C_{B}^{-1})_{11}^{\\text{cofactor}}}{\\det(C_{B}^{-1})} = \\frac{\\sigma_{1}^{2}(\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{2}^{2}\\sigma_{2}^{2})}{\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}}$$\n\n**Increase in a Posteriori Variance**\n\nWe now compute $\\Delta \\operatorname{Var}(x_{1}) = \\operatorname{Var}(x_{1} \\mid y, \\text{Setting B}) - \\operatorname{Var}(x_{1} \\mid y, \\text{Setting A})$.\nLet $V_{A} = \\operatorname{Var}(x_{1} \\mid y, \\text{A})$ and $V_{B} = \\operatorname{Var}(x_{1} \\mid y, \\text{B})$.\nLet $D_{A} = \\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}$ and $D_{B} = \\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2} = D_{A} + \\sigma_{b}^{2}$.\nAlso let $N_{A} = \\sigma_{1}^{2} (\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2})$ and $N_{B} = \\sigma_{1}^{2}(\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{2}^{2}\\sigma_{2}^{2}) = N_{A} + \\sigma_{1}^{2}\\sigma_{b}^{2}$.\nSo we have $V_{A} = N_{A}/D_{A}$ and $V_{B} = N_{B}/D_{B}$.\n\n$$\\Delta \\operatorname{Var}(x_{1}) = \\frac{N_{A} + \\sigma_{1}^{2}\\sigma_{b}^{2}}{D_{A} + \\sigma_{b}^{2}} - \\frac{N_{A}}{D_{A}} = \\frac{D_{A}(N_{A} + \\sigma_{1}^{2}\\sigma_{b}^{2}) - N_{A}(D_{A} + \\sigma_{b}^{2})}{D_{A}(D_{A} + \\sigma_{b}^{2})}$$\n$$\\Delta \\operatorname{Var}(x_{1}) = \\frac{D_{A}N_{A} + D_{A}\\sigma_{1}^{2}\\sigma_{b}^{2} - N_{A}D_{A} - N_{A}\\sigma_{b}^{2}}{D_{A}D_{B}} = \\frac{\\sigma_{b}^{2} (D_{A}\\sigma_{1}^{2} - N_{A})}{D_{A}D_{B}}$$\nThe term in the numerator is:\n$$D_{A}\\sigma_{1}^{2} - N_{A} = (\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})\\sigma_{1}^{2} - \\sigma_{1}^{2} (\\sigma_{\\epsilon}^{2} + h_{2}^{2}\\sigma_{2}^{2}) = h_{1}^{2}\\sigma_{1}^{4}$$\nSubstituting this back, we get the final expression for the increase in variance:\n$$\\Delta \\operatorname{Var}(x_{1}) = \\frac{h_{1}^{2}\\sigma_{1}^{4}\\sigma_{b}^{2}}{(\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})(\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})}$$\n\n**Explanation of the Variance Increase**\n\nThe increase in the posterior variance of $x_{1}$ when the bias $b$ is unknown is a manifestation of parameter coupling and the allocation of information from the data.\n\nIn Setting A, the observation $y$ provides information to constrain the two parameters $x_{1}$ and $x_{2}$. In Setting B, the same single observation must be used to constrain three parameters: $x_{1}$, $x_{2}$, and $b$. The information provided by the measurement is now \"spread thinner\" across a larger set of unknowns.\n\nThe algebraic mechanism for this is the coupling between parameters introduced by the observation model. In Setting B, the posterior precision matrix $C_{B}^{-1}$ contains non-zero off-diagonal terms, such as $(C_{B}^{-1})_{13} = h_{1}/\\sigma_{\\epsilon}^{2}$, which link $x_{1}$ and $b$. This term is zero in Setting A (as $b$ is not part of the state). When this precision matrix is inverted to yield the covariance matrix $C_{B}$, this coupling ensures that uncertainty in one parameter propagates to others. Specifically, the prior uncertainty in the bias, quantified by $\\sigma_{b}^{2}$, \"leaks\" into the posterior uncertainty of $x_{1}$. The observation $y = h_{1}x_{1} + h_{2}x_{2} + b + \\epsilon$ cannot perfectly distinguish between a change in $y$ caused by $x_{1}$ and one caused by $b$. This ambiguity leads to a larger posterior variance for $x_1$ than if $b$ were known.\n\nThe derived expression for $\\Delta \\operatorname{Var}(x_{1})$ makes this transparent:\n- The increase is proportional to $\\sigma_{b}^{2}$, the prior variance of the bias. If there is no prior uncertainty in the bias, there is no variance increase.\n- The increase is proportional to $h_{1}^{2}$. If $x_1$ does not influence the observation ($h_1=0$), it is not coupled to the bias through the measurement, and its variance is unaffected by the uncertainty in $b$.\n- The increase is also proportional to $\\sigma_{1}^{4}$, indicating that parameters with higher prior uncertainty are more susceptible to this effect.\n\nIn essence, estimating a nuisance parameter like the bias $b$ comes at the cost of reduced precision in the estimation of the primary parameters of interest.",
            "answer": "$$\\boxed{\\frac{h_{1}^{2} \\sigma_{1}^{4} \\sigma_{b}^{2}}{(\\sigma_{\\epsilon}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2}) (\\sigma_{\\epsilon}^{2} + \\sigma_{b}^{2} + h_{1}^{2}\\sigma_{1}^{2} + h_{2}^{2}\\sigma_{2}^{2})}}$$"
        },
        {
            "introduction": "This final practice bridges the gap between the theory of Bayesian updates and the practicalities of their implementation, particularly when data arrives sequentially. You will first formally demonstrate the theoretical equivalence between processing all data at once (batch update) and assimilating it one measurement at a time (sequential update). The exercise then pivots to the crucial issue of numerical stability, investigating why standard update formulas can fail in finite-precision arithmetic and motivating the need for robust algorithms in practical applications like Kalman filtering .",
            "id": "3411817",
            "problem": "Consider a linear Gaussian inverse problem with prior and observation model defined as follows. The unknown state $x \\in \\mathbb{R}^{n}$ has a Gaussian prior $x \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$ with symmetric positive definite (SPD) covariance $C_{\\text{prior}} \\in \\mathbb{R}^{n \\times n}$. Observations are generated by the linear model $y = H x + \\eta$, where $H \\in \\mathbb{R}^{m \\times n}$ is the observation operator and $\\eta \\sim \\mathcal{N}(0, R)$ is mean-zero Gaussian noise with SPD covariance $R \\in \\mathbb{R}^{m \\times m}$. Assume that $m \\geq 1$ and that the observation rows are added online as new sensors arrive. You will compare the batch and incremental updates for the posterior covariance using only foundational Gaussian identities and block matrix inversion tools.\n\nStarting from the definition of the posterior density via Bayes’ rule and the quadratic form of the Gaussian likelihood and prior, do the following:\n\n1) Derive the expression for the batch posterior covariance $C_{\\text{post}}^{\\text{batch}}$ when two scalar sensors arrive simultaneously, with\n- first sensor $y_{1} \\in \\mathbb{R}$, observation vector $h_{1} \\in \\mathbb{R}^{n}$, and noise variance $r_{1} \\in \\mathbb{R}_{>0}$,\n- second sensor $y_{2} \\in \\mathbb{R}$, observation vector $h_{2} \\in \\mathbb{R}^{n}$, and noise variance $r_{2} \\in \\mathbb{R}_{>0}$,\nso that $H = \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix}$ and $R = \\operatorname{diag}(r_{1}, r_{2})$.\n\n2) Now suppose the sensors arrive online. Process the first sensor to obtain an intermediate posterior with covariance $C_{1}$. Then process the second sensor using a block matrix inversion (Schur complement) argument on the joint Gaussian of $(x, y_{2})$ conditional on $y_{1}$, and derive an explicit rank-$1$ update formula for the covariance of the form $C_{2}$ in terms of $C_{1}$, $h_{2}$, and $r_{2}$. Show algebraically that $C_{2} = C_{\\text{post}}^{\\text{batch}}$.\n\n3) Provide the analogous rank-$1$ posterior mean update for the second sensor in terms of $C_{1}$, $h_{2}$, $r_{2}$, and the innovation $y_{2} - h_{2}^{\\top} m_{1}$, where $m_{1}$ is the posterior mean after assimilating the first sensor.\n\n4) Using the specific, numerically concrete setup\n- $n = 2$,\n- $C_{\\text{prior}} = \\begin{pmatrix} 2 & 3/5 \\\\ 3/5 & 1 \\end{pmatrix}$,\n- $h_{1} = \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix}$, $r_{1} = 1/2$,\n- $h_{2} = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}$, $r_{2} = 2/5$,\ncompute $C_{\\text{post}}^{\\text{batch}}$ and $C_{2}$ obtained by sequentially processing $y_{1}$ then $y_{2}$ using the rank-$1$ covariance update you derived. Report the Frobenius norm of the difference,\n$$\n\\left\\| C_{\\text{post}}^{\\text{batch}} - C_{2} \\right\\|_{F},\n$$\nas your final answer. Provide the exact value. No rounding is required.\n\n5) Briefly identify and justify at least two numerical stability caveats that arise when applying the incremental rank-$1$ covariance update in finite precision arithmetic, and discuss how a square-root implementation or Joseph-stabilized form addresses these caveats.\n\nYour final answer must be the single real number equal to the Frobenius norm requested in item $4$.",
            "solution": "### Part 1: Batch Posterior Covariance\n\nThe posterior probability density function $p(x|y)$ is given by Bayes' rule: $p(x|y) \\propto p(y|x)p(x)$. For Gaussian distributions, this is equivalent to summing the exponents of the probability densities. The negative log-posterior $J(x)$ is a quadratic function of $x$:\n$$J(x) = \\frac{1}{2}(x - m_{\\text{prior}})^{\\top} C_{\\text{prior}}^{-1} (x - m_{\\text{prior}}) + \\frac{1}{2}(y - Hx)^{\\top} R^{-1} (y - Hx) + \\text{const.}$$\nThe posterior is also Gaussian, $x|y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$. The inverse of the posterior covariance, $C_{\\text{post}}^{-1}$, is the Hessian of $J(x)$, which corresponds to the quadratic term in $x$. Expanding $J(x)$ yields:\n$$J(x) = \\frac{1}{2} x^{\\top} (C_{\\text{prior}}^{-1} + H^{\\top}R^{-1}H) x - x^{\\top} (C_{\\text{prior}}^{-1}m_{\\text{prior}} + H^{\\top}R^{-1}y) + \\text{const.}$$\nBy identifying terms with the general Gaussian form $\\frac{1}{2}(x - m_{\\text{post}})^{\\top} C_{\\text{post}}^{-1} (x - m_{\\text{post}})$, we find the inverse posterior covariance:\n$$(C_{\\text{post}})^{-1} = C_{\\text{prior}}^{-1} + H^{\\top}R^{-1}H$$\nFor the specified batch update with two scalar sensors, we have $H = \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix}$ and $R = \\begin{pmatrix} r_{1} & 0 \\\\ 0 & r_{2} \\end{pmatrix}$. Its inverse is $R^{-1} = \\begin{pmatrix} r_{1}^{-1} & 0 \\\\ 0 & r_{2}^{-1} \\end{pmatrix}$. The term $H^{\\top}R^{-1}H$ becomes:\n$$H^{\\top}R^{-1}H = \\begin{pmatrix} h_{1} & h_{2} \\end{pmatrix} \\begin{pmatrix} r_{1}^{-1} & 0 \\\\ 0 & r_{2}^{-1} \\end{pmatrix} \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix} = r_{1}^{-1}h_{1}h_{1}^{\\top} + r_{2}^{-1}h_{2}h_{2}^{\\top}$$\nThus, the inverse batch posterior covariance is:\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = C_{\\text{prior}}^{-1} + r_{1}^{-1}h_{1}h_{1}^{\\top} + r_{2}^{-1}h_{2}h_{2}^{\\top}$$\nThe batch posterior covariance is the inverse of this expression:\n$$C_{\\text{post}}^{\\text{batch}} = (C_{\\text{prior}}^{-1} + r_{1}^{-1}h_{1}h_{1}^{\\top} + r_{2}^{-1}h_{2}h_{2}^{\\top})^{-1}$$\n\n### Part 2: Sequential Covariance Update and Equivalence\n\nFirst, assimilating sensor $1$ gives a posterior with covariance $C_1$. This is a special case of the batch formula with $H=h_1^\\top$ and $R=r_1$.\n$$C_{1}^{-1} = C_{\\text{prior}}^{-1} + r_1^{-1} h_1 h_1^\\top$$\nNext, we assimilate sensor $2$, using the posterior from step $1$ as the new prior: $x \\sim \\mathcal{N}(m_1, C_1)$. The observation model is $y_2 = h_2^\\top x + \\eta_2$, with $\\eta_2 \\sim \\mathcal{N}(0, r_2)$. The joint distribution of $(x, y_2)$ conditioned on $y_1$ is Gaussian. The covariance of this joint distribution is:\n$$\\text{Cov}\\left( \\begin{pmatrix} x \\\\ y_2 \\end{pmatrix} | y_1 \\right) = \\begin{pmatrix} C_1 & C_1 h_2 \\\\ h_2^\\top C_1 & h_2^\\top C_1 h_2 + r_2 \\end{pmatrix}$$\nUsing the formula for conditional Gaussians, the posterior covariance of $x$ given $y_2$ (and $y_1$) is given by the Schur complement of the block covariance matrix:\n$$C_2 = C_1 - (C_1 h_2) (h_2^\\top C_1 h_2 + r_2)^{-1} (h_2^\\top C_1)$$\nSince $h_2^\\top C_1 h_2 + r_2$ is a scalar, this simplifies to the rank-$1$ update:\n$$C_2 = C_1 - \\frac{C_1 h_2 h_2^\\top C_1}{r_2 + h_2^\\top C_1 h_2}$$\nTo show that $C_2 = C_{\\text{post}}^{\\text{batch}}$, we can work with the inverse covariances (information matrices). The update rule for the inverse covariance is additive:\n$$C_2^{-1} = C_1^{-1} + r_2^{-1} h_2 h_2^\\top$$\nThis can be proven by applying the Sherman-Morrison-Woodbury formula to the expression for $C_2$. Let $A=C_1^{-1}$, $U=h_2$, $C=r_2^{-1}$, $V=h_2^\\top$. Then $(C_1^{-1} + r_2^{-1} h_2 h_2^\\top)^{-1} = C_1 - C_1 h_2 (r_2 + h_2^\\top C_1 h_2)^{-1} h_2^\\top C_1$, which matches the formula for $C_2$.\nSubstituting the expression for $C_1^{-1}$:\n$$C_2^{-1} = (C_{\\text{prior}}^{-1} + r_1^{-1} h_1 h_1^\\top) + r_2^{-1} h_2 h_2^\\top = C_{\\text{prior}}^{-1} + r_1^{-1} h_1 h_1^\\top + r_2^{-1} h_2 h_2^\\top$$\nThis is precisely the expression for $(C_{\\text{post}}^{\\text{batch}})^{-1}$. Therefore, $C_2 = C_{\\text{post}}^{\\text{batch}}$, demonstrating that the sequential and batch updates produce the same posterior covariance.\n\n### Part 3: Posterior Mean Update\n\nThe posterior mean update formula is also derived from the properties of conditional Gaussians. The updated mean $m_2$ is given by:\n$$m_2 = m_1 + (C_1 h_2) (h_2^\\top C_1 h_2 + r_2)^{-1} (y_2 - h_2^\\top m_1)$$\nwhere $m_1$ is the mean after assimilating the first sensor. The term $y_2 - h_2^\\top m_1$ is the innovation, representing the new information provided by the observation $y_2$. The update can be written as:\n$$m_2 = m_1 + K_2 (y_2 - h_2^\\top m_1)$$\nwhere $K_2 = C_1 h_2 (h_2^\\top C_1 h_2 + r_2)^{-1}$ is the Kalman gain for the second sensor.\n\n### Part 4: Numerical Calculation and Frobenius Norm\n\nWe compute the inverse covariance matrices for the batch and sequential methods and show they are identical. This implies the matrices themselves are identical, and the Frobenius norm of their difference is $0$. This approach avoids complex matrix inversions and fraction arithmetic.\n\nFirst, calculate the required components:\n$C_{\\text{prior}} = \\begin{pmatrix} 2 & 3/5 \\\\ 3/5 & 1 \\end{pmatrix} \\implies C_{\\text{prior}}^{-1} = \\frac{1}{2-9/25}\\begin{pmatrix} 1 & -3/5 \\\\ -3/5 & 2 \\end{pmatrix} = \\frac{25}{41}\\begin{pmatrix} 1 & -3/5 \\\\ -3/5 & 2 \\end{pmatrix} = \\frac{1}{41}\\begin{pmatrix} 25 & -15 \\\\ -15 & 50 \\end{pmatrix}$.\n$h_1 = \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix}, r_1 = 1/2 \\implies r_1^{-1}=2$.\n$r_1^{-1}h_1 h_1^\\top = 2 \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix} \\begin{pmatrix} 1 & 1/2 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 1/2 \\end{pmatrix}$.\n$h_2 = \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix}, r_2 = 2/5 \\implies r_2^{-1}=5/2$.\n$r_2^{-1}h_2 h_2^\\top = \\frac{5}{2} \\begin{pmatrix} -1/3 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} -1/3 & 1 \\end{pmatrix} = \\frac{5}{2} \\begin{pmatrix} 1/9 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix} = \\begin{pmatrix} 5/18 & -5/6 \\\\ -5/6 & 5/2 \\end{pmatrix}$.\n\n**Batch Calculation:**\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = C_{\\text{prior}}^{-1} + r_1^{-1}h_1 h_1^\\top + r_2^{-1}h_2 h_2^\\top$$\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = \\frac{1}{41}\\begin{pmatrix} 25 & -15 \\\\ -15 & 50 \\end{pmatrix} + \\begin{pmatrix} 2 & 1 \\\\ 1 & 1/2 \\end{pmatrix} + \\begin{pmatrix} 5/18 & -5/6 \\\\ -5/6 & 5/2 \\end{pmatrix}$$\nLet's find a common denominator, which is $41 \\times 18 = 738$.\n$$(C_{\\text{post}}^{\\text{batch}})^{-1} = \\frac{18}{738}\\begin{pmatrix} 25 & -15 \\\\ -15 & 50 \\end{pmatrix} + \\frac{738}{738}\\begin{pmatrix} 2 & 1 \\\\ 1 & 1/2 \\end{pmatrix} + \\frac{41}{738}\\begin{pmatrix} 5 & -15 \\\\ -15 & 45 \\end{pmatrix}$$\n$$ = \\frac{1}{738} \\left[ \\begin{pmatrix} 450 & -270 \\\\ -270 & 900 \\end{pmatrix} + \\begin{pmatrix} 1476 & 738 \\\\ 738 & 369 \\end{pmatrix} + \\begin{pmatrix} 205 & -615 \\\\ -615 & 1845 \\end{pmatrix} \\right]$$\nCombining terms:\n$$ = \\frac{1}{738} \\begin{pmatrix} 450+1476+205 & -270+738-615 \\\\ -270+738-615 & 900+369+1845 \\end{pmatrix}$$\n$$ = \\frac{1}{738} \\begin{pmatrix} 2131 & -147 \\\\ -147 & 3114 \\end{pmatrix}$$\n\n**Sequential Calculation:**\nFirst, find $C_1^{-1}$:\n$$C_1^{-1} = C_{\\text{prior}}^{-1} + r_1^{-1}h_1 h_1^\\top = \\frac{1}{41}\\begin{pmatrix} 25 & -15 \\\\ -15 & 50 \\end{pmatrix} + \\begin{pmatrix} 2 & 1 \\\\ 1 & 1/2 \\end{pmatrix}$$\nThe common denominator is $41 \\times 2 = 82$.\n$$C_1^{-1} = \\frac{2}{82}\\begin{pmatrix} 25 & -15 \\\\ -15 & 50 \\end{pmatrix} + \\frac{82}{82}\\begin{pmatrix} 2 & 1 \\\\ 1 & 1/2 \\end{pmatrix} = \\frac{1}{82} \\left( \\begin{pmatrix} 50 & -30 \\\\ -30 & 100 \\end{pmatrix} + \\begin{pmatrix} 164 & 82 \\\\ 82 & 41 \\end{pmatrix} \\right) = \\frac{1}{82}\\begin{pmatrix} 214 & 52 \\\\ 52 & 141 \\end{pmatrix}$$\nNext, find $C_2^{-1} = C_1^{-1} + r_2^{-1}h_2 h_2^\\top$:\n$$C_2^{-1} = \\frac{1}{82}\\begin{pmatrix} 214 & 52 \\\\ 52 & 141 \\end{pmatrix} + \\begin{pmatrix} 5/18 & -5/6 \\\\ -5/6 & 5/2 \\end{pmatrix}$$\nThe common denominator of $82=2 \\times 41$ and $18=2 \\times 9$ is $2 \\times 9 \\times 41 = 738$.\n$$C_2^{-1} = \\frac{9}{738}\\begin{pmatrix} 214 & 52 \\\\ 52 & 141 \\end{pmatrix} + \\frac{41}{738}\\begin{pmatrix} 5 & -15 \\\\ -15 & 45 \\end{pmatrix}$$\n$$ = \\frac{1}{738} \\left[ \\begin{pmatrix} 1926 & 468 \\\\ 468 & 1269 \\end{pmatrix} + \\begin{pmatrix} 205 & -615 \\\\ -615 & 1845 \\end{pmatrix} \\right] = \\frac{1}{738} \\begin{pmatrix} 2131 & -147 \\\\ -147 & 3114 \\end{pmatrix}$$\nAs derived, $(C_{\\text{post}}^{\\text{batch}})^{-1}$ and $C_2^{-1}$ are identical. Since a matrix inverse is unique, this means $C_{\\text{post}}^{\\text{batch}} = C_2$. The difference matrix is the zero matrix:\n$$C_{\\text{post}}^{\\text{batch}} - C_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$$\nThe Frobenius norm of the zero matrix is:\n$$\\left\\| C_{\\text{post}}^{\\text{batch}} - C_2 \\right\\|_{F} = \\sqrt{0^2+0^2+0^2+0^2} = 0$$\n\n### Part 5: Numerical Stability Caveats\n\nThe standard rank-$1$ covariance update formula, $C_k = C_{k-1} - K_k h_k^\\top C_{k-1}$, while algebraically correct, suffers from numerical instability in finite-precision arithmetic.\n1.  **Loss of Positive Definiteness**: The formula involves the subtraction of two matrices. In theory, $C_k$ is guaranteed to be symmetric positive definite. However, due to floating-point rounding errors, the computed result can lose symmetry and, more critically, may no longer be positive definite. If the covariance matrix develops negative eigenvalues, the filter can become unstable and diverge. This is especially problematic when the uncertainty is reduced significantly, making $C_{k-1}$ nearly singular.\n2.  **Catastrophic Cancellation**: When the prior uncertainty is large ($C_{k-1}$ has large entries) and the observation is very informative, the update term $K_k h_k^\\top C_{k-1}$ can also be a large matrix with entries close in magnitude to those of $C_{k-1}$. Subtracting two nearly equal large numbers can lead to a severe loss of relative precision, an effect known as catastrophic cancellation.\n\nThese issues are addressed by alternative formulations:\n-   **Square-Root Filters**: These algorithms propagate a \"square root\" $S_k$ of the covariance, where $C_k = S_k S_k^\\top$. Update rules are derived for $S_k$ directly. By construction, the resulting covariance matrix $C_k = S_k S_k^\\top$ is always symmetric and positive semi-definite, preventing the loss of positive definiteness due to round-off errors. Furthermore, the condition number of $S_k$ is the square root of that of $C_k$, which generally improves the numerical properties of the problem.\n-   **Joseph-Stabilized Form**: An alternative, more stable formula for updating the covariance is the Joseph form:\n    $$C_k = (I - K_k h_k^\\top) C_{k-1} (I - K_k h_k^\\top)^\\top + K_k r_k K_k^\\top$$\n    This expression is algebraically equivalent to the standard one but is numerically superior. It computes the new covariance $C_k$ by summing two matrices which are, by their structure ($A A^\\top$), guaranteed to be positive semi-definite. The sum of positive semi-definite matrices is always positive semi-definite. This formulation avoids the direct subtraction that causes the loss of positive definiteness, at the cost of increased computational complexity.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}