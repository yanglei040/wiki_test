{
    "hands_on_practices": [
        {
            "introduction": "动手实践的第一步是掌握广义吉洪诺夫泛函的基本力学。这个练习将抽象的公式 $J(\\mathbf{u}) = \\| A \\mathbf{u} - \\mathbf{b} \\|^2 + \\alpha \\| L \\mathbf{u} \\|^2$ 转化为一个具体的、低维度的多元微积分问题。通过直接计算，您将练习如何构建目标函数并找到其最小值，从而加深对数据保真项和正则化项如何协同工作的理解。",
            "id": "1031979",
            "problem": "考虑超定线性系统 $A \\mathbf{u} = \\mathbf{b}$，其中 $A$ 是 $3 \\times 2$ 矩阵\n$$  \nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},  \n$$  \n$\\mathbf{b}$ 是向量 $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$。广义吉洪诺夫 (Tikhonov) 正则化泛函包含一个导数算子 $L$，定义为\n$$  \nJ(\\mathbf{u}) = \\| A \\mathbf{u} - \\mathbf{b} \\|^2 + \\alpha \\| L \\mathbf{u} \\|^2,  \n$$  \n其中 $\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} \\in \\mathbb{R}^2$，$\\alpha > 0$ 是一个正则化参数，$L$ 是由 $1 \\times 2$ 矩阵 $L = \\begin{bmatrix} -1 & 1 \\end{bmatrix}$ 给出的一阶导数算子。计算 $J(\\mathbf{u})$ 在所有 $\\mathbf{u} \\in \\mathbb{R}^2$ 上的最小值。",
            "solution": "1. 写出泛函\n$$\nJ(u_1,u_2)=(u_1-1)^2+u_2^2+(u_1+u_2)^2+\\alpha(u_2-u_1)^2.\n$$  \n2. 展开并合并同类项。\n$$\nJ(u_1, u_2) = (2+\\alpha)u_1^2 + (2+\\alpha)u_2^2 + (2-2\\alpha)u_1u_2 - 2u_1 + 1.\n$$\n令 $a=2+\\alpha$，$b=2-2\\alpha$。则\n$$\nJ=a u_1^2+a u_2^2+b u_1u_2-2u_1+1.\n$$  \n3. 驻点条件\n$$\n\\frac{\\partial J}{\\partial u_1}=2a\\,u_1+b\\,u_2-2=0,\\qquad\n\\frac{\\partial J}{\\partial u_2}=2a\\,u_2+b\\,u_1=0.\n$$  \n4. 求解 $(u_1,u_2)$。解为\n$$\nu_1=\\frac{2+\\alpha}{3(1+2\\alpha)},\\quad\nu_2=-\\frac{1-\\alpha}{3(1+2\\alpha)}.\n$$  \n5. 代回 $J$ 中：经过代数运算可得\n$$\nJ_{\\min}\n=\\frac{10\\alpha^2+7\\alpha+1}{3(1+2\\alpha)^2}\n=\\frac{(5\\alpha+1)(2\\alpha+1)}{3(1+2\\alpha)^2}\n=\\frac{5\\alpha+1}{3(2\\alpha+1)}.\n$$  \n因此 $J$ 的最小值为 $(5\\alpha+1)/[3(2\\alpha+1)]\\,$。",
            "answer": "$$\\boxed{\\frac{5\\alpha+1}{3(2\\alpha+1)}}$$"
        },
        {
            "introduction": "在学会如何求解一个正则化问题之后，一个自然的延伸问题是关于解的性质。本练习探讨了吉洪诺夫解存在性和唯一性的条件，特别是当使用可能奇异的广义正则化算子 $\\Gamma$ 时。它揭示了前向算子 $A$ 和正则化算子 $\\Gamma$ 的零空间（$\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$）之间的关键相互作用，这对理解正则化方法的理论保障至关重要。",
            "id": "3283916",
            "problem": "考虑一个可能为病态的线性模型的吉洪诺夫正则化问题：给定 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，$\\Gamma \\in \\mathbb{R}^{p \\times n}$，以及一个正则化参数 $\\alpha > 0$，定义目标函数\n$$\nJ_{\\alpha}(x) \\;=\\; \\lVert A x - b \\rVert_{2}^{2} \\;+\\; \\alpha \\,\\lVert \\Gamma x \\rVert_{2}^{2}, \\quad x \\in \\mathbb{R}^{n}.\n$$\n假设 $\\Gamma$ 可能是奇异的（秩亏的）。令 $\\mathcal{N}(M) = \\{x \\in \\mathbb{R}^{n} : M x = 0\\}$ 表示矩阵 $M$ 的零空间，令 $\\mathcal{R}(M)$ 表示其值域（列空间）。当 $\\Gamma$ 是奇异时，请仅使用关于最小二乘和零空间与值域性质的基本线性代数事实，分析最小化子（minimizer）的存在性、唯一性以及其结构。\n\n下列哪个陈述是正确的？\n\nA. 对于每一个 $\\alpha > 0$ 以及每一个 $A$、$b$ 和奇异的 $\\Gamma$，关于 $J_{\\alpha}$ 的最小化问题至少有一个解 $x_{\\alpha}$。\n\nB. 最小化子是唯一的，当且仅当 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$。\n\nC. 如果 $\\Gamma$ 是奇异的，那么对于所有 $\\alpha > 0$，$J_{\\alpha}$ 相关的二次型都是奇异的，因此最小化子永远不是唯一的。\n\nD. 如果 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$，那么最小化子的集合是一个仿射子空间，其形式为 $x_{\\alpha}^{\\star} + \\big(\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)\\big)$，其中 $x_{\\alpha}^{\\star}$ 是某个特定的最小化子。\n\nE. 如果 $A$ 是满列秩的但 $\\Gamma$ 是奇异的，那么存在某个 $b$ 使得吉洪诺夫最小化子不存在。",
            "solution": "首先验证问题陈述的正确性和清晰度。\n\n### 第一步：提取已知条件\n- 矩阵：$A \\in \\mathbb{R}^{m \\times n}$，$\\Gamma \\in \\mathbb{R}^{p \\times n}$。\n- 向量：$b \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。\n- 标量：正则化参数 $\\alpha > 0$。\n- 目标函数：$J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert \\Gamma x \\rVert_{2}^{2}$。\n- 条件：矩阵 $\\Gamma$ 可能为奇异矩阵。\n- 符号：零空间 $\\mathcal{N}(M) = \\{x \\in \\mathbb{R}^{n} : M x = 0\\}$，值域 $\\mathcal{R}(M)$。\n- 任务：分析 $J_{\\alpha}(x)$ 的最小化子的存在性、唯一性和结构。\n\n### 第二步：使用提取的已知条件进行验证\n问题陈述在科学上是合理的、适定的和客观的。它提出了广义吉洪诺夫正则化的标准公式，这是数值方法和科学计算中的一个基本课题。目标函数定义清晰，任务是对一个优化问题进行标准的数学分析。语言精确无歧义。所有必要的组成部分都已提供，并且没有内部矛盾。\n\n### 第三步：结论与行动\n问题陈述有效。我将继续对每个选项进行完整的推导和分析。\n\n### 推导\n目标函数 $J_{\\alpha}(x)$ 可以重写为一个堆叠系统的欧几里得范数的平方。\n$$\nJ_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert \\Gamma x \\rVert_{2}^{2} = \\lVert A x - b \\rVert_{2}^{2} + \\lVert \\sqrt{\\alpha} \\Gamma x - 0 \\rVert_{2}^{2}\n$$\n这等价于最小化最小二乘问题 $\\lVert A_{\\alpha} x - b_{\\alpha} \\rVert_{2}^{2}$，其中 $A_{\\alpha}$ 和 $b_{\\alpha}$ 是增广矩阵和向量：\n$$\nA_{\\alpha} = \\begin{pmatrix} A \\\\ \\sqrt{\\alpha} \\Gamma \\end{pmatrix} \\in \\mathbb{R}^{(m+p) \\times n}, \\quad b_{\\alpha} = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{(m+p)}\n$$\n这个最小二乘问题的最小化子是正规方程组的解：\n$$\nA_{\\alpha}^{T} A_{\\alpha} x = A_{\\alpha}^{T} b_{\\alpha}\n$$\n让我们计算正规方程组的各个组成部分。\n矩阵是：\n$$\nA_{\\alpha}^{T} A_{\\alpha} = \\begin{pmatrix} A^T  \\sqrt{\\alpha} \\Gamma^T \\end{pmatrix} \\begin{pmatrix} A \\\\ \\sqrt{\\alpha} \\Gamma \\end{pmatrix} = A^T A + \\alpha \\Gamma^T \\Gamma\n$$\n右侧向量是：\n$$\nA_{\\alpha}^{T} b_{\\alpha} = \\begin{pmatrix} A^T  \\sqrt{\\alpha} \\Gamma^T \\end{pmatrix} \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} = A^T b\n$$\n因此，正规方程组为：\n$$\n(A^T A + \\alpha \\Gamma^T \\Gamma) x = A^T b\n$$\n我们用 $H_{\\alpha} = A^T A + \\alpha \\Gamma^T \\Gamma$ 表示海森矩阵（二次型的矩阵）。目标函数是 $J_{\\alpha}(x) = x^T H_{\\alpha} x - 2x^T A^T b + b^T b$。由于 $A^T A$ 和 $\\Gamma^T \\Gamma$ 都是半正定的，且 $\\alpha > 0$，所以 $H_{\\alpha}$ 也是半正定的。这证实了 $J_{\\alpha}(x)$ 是一个凸函数。\n\n### 逐项分析\n\n**A. 对于每一个 $\\alpha > 0$ 以及每一个 $A$、$b$ 和奇异的 $\\Gamma$，关于 $J_{\\alpha}$ 的最小化问题至少有一个解 $x_{\\alpha}$。**\n\n最小化 $J_{\\alpha}(x)$ 的问题是一个线性最小二乘问题。线性代数的一个基本定理指出，任何线性最小二乘问题 $\\min_{x \\in \\mathbb{R}^n} \\lVert Kx - d \\rVert_2^2$ 的解总是存在的。这是因为解集等价于满足 $Kx = \\text{proj}_{\\mathcal{R}(K)}(d)$ 的向量 $x$ 的集合，该集合非空，因为 $d$ 在 $K$ 的列空间上的投影总是存在的。\n或者，解集与正规方程组 $K^T K x = K^T d$ 的解集相同。一个线性系统 $Mx=c$ 有解当且仅当 $c \\in \\mathcal{R}(M)$。这里，$M = K^T K$ 且 $c = K^T d$。因为对于任何矩阵 $K$，都有 $\\mathcal{R}(K^T K) = \\mathcal{R}(K^T)$，而 $c = K^T d$ 根据定义就在 $\\mathcal{R}(K^T)$ 中，所以该系统总是相容的，并且至少有一个解。\n对于任何 $A, b, \\Gamma$ 和任何 $\\alpha > 0$，解的存在性是无条件的。$\\Gamma$ 是否奇异对最小化子的存在性没有影响。\n\n结论：**正确**。\n\n**B. 最小化子是唯一的，当且仅当 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$。**\n\n最小二乘问题 $\\min_{x} \\lVert A_{\\alpha} x - b_{\\alpha} \\rVert_{2}^{2}$ 的最小化子是唯一的，当且仅当矩阵 $A_{\\alpha}$ 具有满列秩，这等价于其零空间是平凡的，即 $\\mathcal{N}(A_{\\alpha}) = \\{0\\}$。\n我们来刻画这个零空间。一个向量 $x \\in \\mathbb{R}^n$ 属于 $\\mathcal{N}(A_{\\alpha})$ 当且仅当 $A_{\\alpha} x = 0$。\n$$\nA_{\\alpha} x = \\begin{pmatrix} A x \\\\ \\sqrt{\\alpha} \\Gamma x \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n这成立当且仅当 $A x = 0$ 和 $\\sqrt{\\alpha} \\Gamma x = 0$ 同时成立。因为 $\\alpha > 0$，第二个条件等价于 $\\Gamma x = 0$。\n所以，$x \\in \\mathcal{N}(A_{\\alpha})$ 当且仅当 $x \\in \\mathcal{N}(A)$ 且 $x \\in \\mathcal{N}(\\Gamma)$。这意味着：\n$$\n\\mathcal{N}(A_{\\alpha}) = \\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)\n$$\n因此，最小化子是唯一的当且仅当 $\\mathcal{N}(A_{\\alpha}) = \\{0\\}$，这等价于条件 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$。\n\n结论：**正确**。\n\n**C. 如果 $\\Gamma$ 是奇异的，那么对于所有 $\\alpha > 0$，$J_{\\alpha}$ 相关的二次型都是奇异的，因此最小化子永远不是唯一的。**\n\n该二次型与海森矩阵 $H_{\\alpha} = A^T A + \\alpha \\Gamma^T \\Gamma$ 相关联。该矩阵是奇异的当且仅当其零空间非平凡。如 B 的分析所示，$\\mathcal{N}(H_{\\alpha}) = \\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$。\n该陈述声称，如果 $\\Gamma$ 是奇异的（即 $\\mathcal{N}(\\Gamma) \\neq \\{0\\}$），那么 $H_{\\alpha}$ 总是奇异的（即 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$）。这不是真的。$\\mathcal{N}(\\Gamma)$ 可能是一个非平凡子空间，但它与 $\\mathcal{N}(A)$ 的交集可能是平凡的。\n考虑以下反例。令 $n=2$，$A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$，以及 $\\Gamma = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$。\n那么 $\\mathcal{N}(A) = \\text{span}\\left\\{ \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}$ 且 $\\mathcal{N}(\\Gamma) = \\text{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\}$。\n这里，$\\Gamma$ 是奇异的。然而，$\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$。\n根据 B 的结果，这种情况下最小化子是唯一的。矩阵 $H_{\\alpha}$ 是 $A^T A + \\alpha \\Gamma^T \\Gamma = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{pmatrix}$。由于 $\\alpha > 0$，$H_\\alpha$ 是可逆的。\n该陈述是错误的。\n\n结论：**错误**。\n\n**D. 如果 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$，那么最小化子的集合是一个仿射子空间，其形式为 $x_{\\alpha}^{\\star} + \\big(\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)\\big)$，其中 $x_{\\alpha}^{\\star}$ 是某个特定的最小化子。**\n\n最小化子的集合是正规方程组 $H_{\\alpha} x = A^T b$ 的解集，其中 $H_{\\alpha} = A^T A + \\alpha \\Gamma^T \\Gamma$。根据线性代数，一个相容线性系统 $Mx=c$ 的通解由 $x = x_p + z$ 给出，其中 $x_p$ 是任意一个特解，$z$ 是 $M$ 的零空间 $\\mathcal{N}(M)$ 中的任意向量。解集是仿射子空间 $x_p + \\mathcal{N}(M)$。\n在我们的情况下，我们已经确定问题总是有解的（因此系统是相容的），并且相关矩阵的零空间是 $\\mathcal{N}(H_{\\alpha}) = \\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$。\n因此，如果 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) \\neq \\{0\\}$，解不是唯一的，并且所有最小化子的集合恰好是通过将任意一个特定的最小化子 $x_{\\alpha}^{\\star}$ 与零空间 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma)$ 相加而形成的仿射子空间。该陈述准确地描述了这种结构。\n\n结论：**正确**。\n\n**E. 如果 $A$ 是满列秩的但 $\\Gamma$ 是奇异的，那么存在某个 $b$ 使得吉洪诺夫最小化子不存在。**\n\n这个陈述从根本上是错误的。如选项 A 的分析所确立的，对于任何 $A, b, \\Gamma$ 的选择以及任何 $\\alpha > 0$，最小化子*总是*存在的。存在性是无条件的。\n此外，该陈述的前提导出了唯一解的结论，这与不存在解是相反的。如果 $A$ 具有满列秩，那么 $\\mathcal{N}(A) = \\{0\\}$。这意味着 $\\mathcal{N}(A) \\cap \\mathcal{N}(\\Gamma) = \\{0\\} \\cap \\mathcal{N}(\\Gamma) = \\{0\\}$。根据 B 的结果，在这种条件下，无论 $\\Gamma$ 是否奇异，最小化子总是唯一的。唯一性意味着存在性。因此，该陈述是错误的。\n\n结论：**错误**。\n\n正确选项的最终总结：A, B, D。",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "这个练习将理论与实践联系起来，要求您为一个更真实的图像去模糊问题实现吉洪诺夫正则化。其核心教学目标是揭示数值反演方法验证中的一个关键概念：“反演犯罪”(inverse crime)。通过对比匹配和非匹配的离散化方案，您将亲身體会到，当数据生成模型与反演模型完全相同时，为何会得到过于乐观的结果，以及如何设计更可靠的数值实验。",
            "id": "3427382",
            "problem": "考虑从由第一类 Fredholm 积分建模的模糊观测中恢复单位区间 $[0,1]$ 上的未知函数 $x$ 的线性逆问题\n$$\ny(s) \\;=\\; \\int_{0}^{1} k(s,t)\\,x(t)\\,dt, \\quad s \\in [0,1],\n$$\n其高斯模糊核为\n$$\nk(s,t) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\Big(-\\frac{(s-t)^2}{2\\sigma^2}\\Big),\n$$\n其中 $\\sigma > 0$ 是模糊宽度。离散 Tikhonov 正则化问题旨在寻找一个 $x \\in \\mathbb{R}^N$ 以最小化\n$$\nJ(x) \\;=\\; \\|A x - y\\|_2^2 \\;+\\; \\alpha^2 \\|L x\\|_2^2,\n$$\n其中 $A \\in \\mathbb{R}^{N \\times N}$ 是正向算子的离散化，$L \\in \\mathbb{R}^{(N-1) \\times N}$ 是一阶导数惩罚项的离散化，$\\alpha > 0$ 是正则化参数，$y \\in \\mathbb{R}^N$ 是观测数据。\n\n您的任务是构建一个完整的“逆犯罪”测试和一个交错离散化诊断，以比较两种数据生成机制下的重构结果：\n\n- 逆犯罪（网格匹配）：通过将与反演相同的离散算子应用于同一网格上的离散化真实解来生成合成数据。这种做法可能会夸大表面性能。\n- 交错离散化（非匹配）：通过一个与反演网格不对齐的、更精细且有偏移的求积方法来生成合成数据，从而揭示建模误差并诊断真实的正则化效果。\n\n从以下基本基础开始：\n\n- Tikhonov 正则化的定义，即作为凸泛函 $J(x)$ 的最小化子。\n- 严格凸二次型的一阶最优性（欧拉-拉格朗日条件），导出正规方程\n$$\n(A^\\top A + \\alpha^2 L^\\top L)\\,x^\\star \\;=\\; A^\\top y.\n$$\n- 在均匀网格上使用标准矩形求积法离散化积分，以及使用前向有限差分离散化一阶导数。\n\n需实现的离散化细节：\n\n- 反演网格：选择 $N$ 个位于中点的点 $s_i = (i+\\tfrac{1}{2})/N$，$i=0,\\dots,N-1$。用 $x$ 在相同中点处的采样值来表示它。反演矩阵 $A \\in \\mathbb{R}^{N\\times N}$ 使用权重为 $w = 1/N$ 的矩形求积法，\n$$\nA_{ij} \\;=\\; k\\!\\big(s_i, s_j\\big)\\,w \\quad \\text{for} \\quad 0 \\le i,j \\le N-1.\n$$\n- 正则化算子 $L \\in \\mathbb{R}^{(N-1)\\times N}$ 是在同一反演网格上对一阶导数的前向有限差分近似，步长为 $h = 1/N$，\n$$\n(Lx)_j \\;=\\; \\frac{x_{j+1}-x_j}{h}, \\quad j = 0,\\dots,N-2.\n$$\n- 真实解：使用平滑函数\n$$\nx_{\\text{true}}(t) \\;=\\; \\exp\\!\\big(-50\\,(t-0.3)^2\\big) \\;+\\; 0.5\\,\\sin(6\\pi t) \\;+\\; 0.2,\n$$\n角度以弧度为单位。为便于比较，反演网格上的参考向量为 $x_{\\text{true,inv}} = \\big(x_{\\text{true}}(s_i)\\big)_{i=0}^{N-1}$。\n- 数据生成：\n  - 逆犯罪：$y_{\\text{clean}} = A\\,x_{\\text{true,inv}}$。\n  - 交错离散化：对于每个反演网格点 $s_i$，使用一个更精细、有偏移的求积方法来近似积分，该方法有 $N_f$ 个点 $t_\\ell = (\\ell + \\delta)/N_f$（$\\ell=0,\\dots,N_f-1$），权重为 $w_f = 1/N_f$，固定偏移量 $\\delta = 0.125$，\n  $$\n  y_{\\text{clean},i} \\;=\\; \\sum_{\\ell=0}^{N_f-1} k\\!\\big(s_i, t_\\ell\\big)\\,x_{\\text{true}}(t_\\ell)\\,w_f.\n  $$\n  除非另有说明，对于交错情况，取 $N_f = 4N$。\n- 噪声模型：加性独立高斯噪声 $n \\sim \\mathcal{N}(0,\\sigma_n^2 I)$，其每分量标准差为\n$$\n\\sigma_n \\;=\\; \\eta\\,\\frac{\\|y_{\\text{clean}}\\|_2}{\\sqrt{N}},\n$$\n其中 $\\eta$ 是预设的相对噪声水平，$I$ 是单位矩阵。\n- 重构：对于给定的 $(A,L,\\alpha,y)$，从正规方程计算 $x^\\star$ 并报告相对重构误差\n$$\n\\varepsilon \\;=\\; \\frac{\\|x^\\star - x_{\\text{true,inv}}\\|_2}{\\|x_{\\text{true,inv}}\\|_2}.\n$$\n\n测试套件与覆盖范围：\n\n实现以下五个测试用例，每个用例由 $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode})$ 指定，其中 $\\text{mode} \\in \\{\\text{crime},\\text{staggered}\\}$ 表示数据生成机制：\n\n- 用例 1（理想路径，逆犯罪）：$(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 80, 0.03, 0.01, 0.001, \\text{crime})$。\n- 用例 2（理想路径，交错）：$(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 320, 0.03, 0.01, 0.001, \\text{staggered})$。\n- 用例 3（更强的模糊和噪声，交错）：$(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 320, 0.05, 0.05, 0.01, \\text{staggered})$。\n- 用例 4（更粗的反演网格，交错）：$(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (40, 160, 0.03, 0.01, 0.005, \\text{staggered})$。\n- 用例 5（欠正则化，逆犯罪）：$(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 80, 0.03, 0.00001, 0.001, \\text{crime})$。\n\n随机性：\n\n- 为保证可复现性，使用固定种子：对所有噪声抽样，使用种子 0 初始化伪随机数生成器。\n\n要求的程序输出：\n\n- 对于每个用例，计算如上定义的相对误差 $\\varepsilon$。\n- 您的程序应生成单行输出，包含五个结果，形式为方括号内的逗号分隔列表，并四舍五入到 6 位小数（例如，$[0.123456,0.234567,0.345678,0.456789,0.567890]$）。\n- 不涉及物理单位，所有三角函数角度必须以弧度为单位。\n\n您的实现必须是一个完整的、可运行的程序，该程序能够构建 $A$ 和 $L$，根据指定机制生成 $y$，求解 Tikhonov 正规方程，并以指定格式报告所有测试用例的误差。",
            "solution": "用户提供的问题是逆问题领域一个明确定义的数值练习，特别关注 Tikhonov 正则化。它在科学上是合理的，数学上是一致的，并且所有参数和过程都已明确定义。任务是实现一个数值方案来求解第一类 Fredholm 积分方程，在不同的数据生成场景（“逆犯罪”与“交错离散化”）下进行测试，并量化重构误差。该问题是有效的，并且可以构建一个解决方案。\n\n解决方案的步骤是离散化连续模型，构建线性代数算子，根据指定的机制生成合成数据，求解 Tikhonov 正则化的正规方程，最后计算重构误差。\n\n1. **网格和真实解的离散化**：\n该问题在 $N$ 个点的均匀网格上进行离散化。反演网格点 $s_i$ 是 $[0,1]$ 子区间的中点：\n$$s_i = \\frac{i + 0.5}{N}, \\quad i = 0, 1, \\dots, N-1.$$\n真实解函数由下式给出：\n$$x_{\\text{true}}(t) = \\exp\\big(-50(t - 0.3)^2\\big) + 0.5 \\sin(6\\pi t) + 0.2.$$\n反演网格上的参考解 $x_{\\text{true,inv}} \\in \\mathbb{R}^N$ 是通过在网格点 $s_i$ 处对 $x_{\\text{true}}(t)$ 进行采样得到的：\n$$(x_{\\text{true,inv}})_i = x_{\\text{true}}(s_i).$$\n\n2. **算子的离散化**：\n积分算子使用反演网格上的中点矩形法则进行离散化。每个长度为 $h = \\frac{1}{N}$ 的子区间的权重为 $w=\\frac{1}{N}$。矩阵 $A \\in \\mathbb{R}^{N \\times N}$ 的元素为：\n$$A_{ij} = k(s_i, s_j) w = \\frac{1}{N} k(s_i, s_j),$$\n其中核函数为 $k(s,t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\Big(-\\frac{(s-t)^2}{2\\sigma^2}\\Big)$。\n\n一阶导数正则化算子 $L$ 使用前向有限差分格式进行离散化。对于向量 $x \\in \\mathbb{R}^N$，$Lx$ 的第 $j$ 个分量是点 $s_j$ 处导数的近似：\n$$(Lx)_j = \\frac{x_{j+1} - x_j}{h} = N(x_{j+1} - x_j), \\quad j = 0, 1, \\dots, N-2.$$\n这定义了矩阵 $L \\in \\mathbb{R}^{(N-1) \\times N}$。对于给定的第 $j$ 行，该矩阵在第 $j$ 列的元素为 $-N$，在第 $j+1$ 列的元素为 $N$。\n\n3. **数据生成 ($y$)**：\n观测数据 $y \\in \\mathbbR^N$ 首先通过创建“干净”数据 $y_{\\text{clean}}$，然后添加噪声来生成。\n\n- **逆犯罪机制**：数据使用与反演相同的离散化方法生成。\n    $$y_{\\text{clean}} = A x_{\\text{true,inv}}.$$\n    这被称为“逆犯罪”，因为用于反演的模型与数据生成过程完全匹配，这在实践中很少发生，并可能导致不切实际的好结果。\n\n- **交错离散化机制**：数据使用更精细、有偏移的网格生成，以模拟模型失配。使用一个具有 $N_f$ 个点的精细网格 $t_\\ell = \\frac{\\ell + \\delta}{N_f}$（$\\ell=0,\\dots,N_f-1$，偏移量 $\\delta=0.125$）。干净数据是积分的近似值：\n    $$ (y_{\\text{clean}})_i = \\sum_{\\ell=0}^{N_f-1} k(s_i, t_\\ell) x_{\\text{true}}(t_\\ell) w_f, $$\n    其中 $w_f = \\frac{1}{N_f}$。这种更现实的方法有助于诊断正则化方法的真实性能。\n\n- **噪声模型**：将加性高斯噪声 $n \\sim \\mathcal{N}(0, \\sigma_n^2 I)$ 添加到 $y_{\\text{clean}}$ 中。噪声的标准差 $\\sigma_n$ 由相对噪声水平 $\\eta$ 决定：\n    $$\\sigma_n = \\eta \\frac{\\|y_{\\text{clean}}\\|_2}{\\sqrt{N}}.$$\n    最终的观测数据是 $y = y_{\\text{clean}} + n$。为保证可复现性，使用固定的随机种子 0。\n\n4. **求解重构**：\nTikhonov 正则化解 $x^\\star$ 最小化 $J(x) = \\|Ax - y\\|_2^2 + \\alpha^2 \\|Lx\\|_2^2$。最小化子通过求解正规方程找到，这是一个适定的线性系统：\n$$(A^\\top A + \\alpha^2 L^\\top L) x^\\star = A^\\top y.$$\n我们构造矩阵 $M = A^\\top A + \\alpha^2 L^\\top L$ 和向量 $b = A^\\top y$，并使用标准线性求解器求解系统 $Mx^\\star = b$ 以得到 $x^\\star$。\n\n5. **误差计算**：\n重构 $x^\\star$ 的质量通过相对于在反演网g'g上采样的真实解 $x_{\\text{true,inv}}$ 的相对误差 $\\varepsilon$ 来衡量：\n    $$\\varepsilon = \\frac{\\|x^\\star - x_{\\text{true,inv}}\\|_2}{\\|x_{\\text{true,inv}}\\|_2}.$$\n\n实现过程将为所提供的五个测试用例分别执行这些步骤。使用 `numpy` 库的 Python 脚本非常适合这些矩阵和向量运算。对于每个用例，我们将配置参数 $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode})$，构建必要的组件，求解 $x^\\star$，并计算 $\\varepsilon$。最终结果将被收集并按指定格式输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for a series of test cases\n    and prints the relative reconstruction errors.\n    \"\"\"\n    \n    # Use a fixed seed for reproducibility for all noise draws.\n    RNG = np.random.default_rng(0)\n    \n    # Shift for staggered grid, as specified in the problem.\n    DELTA = 0.125\n\n    def x_true_func(t):\n        \"\"\"Computes the ground truth function x_true(t).\"\"\"\n        return np.exp(-50 * (t - 0.3)**2) + 0.5 * np.sin(6 * np.pi * t) + 0.2\n\n    def kernel_func(s, t, sigma):\n        \"\"\"Computes the Gaussian blur kernel k(s,t).\"\"\"\n        return (1.0 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-(s - t)**2 / (2 * sigma**2))\n\n    def compute_reconstruction_error(params):\n        \"\"\"Computes the reconstruction error for a single test case.\"\"\"\n        N, Nf, sigma, alpha, eta, mode = params\n\n        # 1. Discretization of Grids and Ground Truth\n        h = 1.0 / N\n        s_grid = (np.arange(N) + 0.5) * h\n        x_true_inv = x_true_func(s_grid)\n\n        # 2. Discretization of Operators\n        # Forward operator A for inversion\n        s_i_mesh, s_j_mesh = np.meshgrid(s_grid, s_grid, indexing='ij')\n        A = kernel_func(s_i_mesh, s_j_mesh, sigma) * h\n\n        # Regularization operator L\n        L = np.zeros((N - 1, N))\n        idx = np.arange(N - 1)\n        L[idx, idx] = -1.0\n        L[idx, idx + 1] = 1.0\n        L /= h\n\n        # 3. Data Generation (y)\n        if mode == 'crime':\n            y_clean = A @ x_true_inv\n        elif mode == 'staggered':\n            h_f = 1.0 / Nf\n            t_grid = (np.arange(Nf) + DELTA) * h_f\n            x_true_fine = x_true_func(t_grid)\n            \n            s_mesh, t_mesh = np.meshgrid(s_grid, t_grid, indexing='ij')\n            K_staggered = kernel_func(s_mesh, t_mesh, sigma)\n            \n            y_clean = K_staggered @ x_true_fine * h_f\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        # Add noise\n        norm_y_clean = np.linalg.norm(y_clean)\n        if norm_y_clean == 0:\n            sigma_n = 0.0\n        else:\n            sigma_n = eta * norm_y_clean / np.sqrt(N)\n        \n        noise = RNG.normal(loc=0.0, scale=sigma_n, size=N)\n        y = y_clean + noise\n\n        # 4. Solving for the Reconstruction\n        M = A.T @ A + alpha**2 * (L.T @ L)\n        b = A.T @ y\n        x_star = np.linalg.solve(M, b)\n\n        # 5. Error Calculation\n        error_num = np.linalg.norm(x_star - x_true_inv)\n        error_den = np.linalg.norm(x_true_inv)\n        epsilon = error_num / error_den\n        \n        return epsilon\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, Nf, sigma, alpha, eta, mode)\n        (80, 80, 0.03, 0.01, 0.001, 'crime'),\n        (80, 320, 0.03, 0.01, 0.001, 'staggered'),\n        (80, 320, 0.05, 0.05, 0.01, 'staggered'),\n        (40, 160, 0.03, 0.01, 0.005, 'staggered'),\n        (80, 80, 0.03, 1e-5, 0.001, 'crime'),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_reconstruction_error(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}