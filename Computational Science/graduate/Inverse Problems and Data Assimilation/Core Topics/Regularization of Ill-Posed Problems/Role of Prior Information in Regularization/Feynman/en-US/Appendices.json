{
    "hands_on_practices": [
        {
            "introduction": "This first practice establishes the foundational link between Bayesian inference and classical regularization. You will work through a simple one-dimensional problem to see how the posterior distribution, which combines data with a Gaussian prior, leads to a solution equivalent to Tikhonov regularization. This exercise provides a clear, probabilistic interpretation for the regularization parameter as a measure of prior uncertainty .",
            "id": "3418467",
            "problem": "Consider a linear inverse problem in one dimension framed as a Bayesian data assimilation task. Let the unknown state be $x \\in \\mathbb{R}$. A single observation $y \\in \\mathbb{R}$ is related to the state by the linear forward operator $h \\in \\mathbb{R}$ via the model $y = h x + \\varepsilon$, where the observational error $\\varepsilon$ is Gaussian with zero mean and variance $\\sigma^{2} > 0$, that is, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$. The prior information on $x$ is Gaussian with mean $\\mu \\in \\mathbb{R}$ and variance $C > 0$, that is, $x \\sim \\mathcal{N}(\\mu, C)$. The Bayesian posterior is defined by Bayes’ theorem: the posterior density is proportional to the product of the likelihood and the prior.\n\nNow define a scaled prior covariance by $C \\mapsto \\alpha C$, where $\\alpha > 0$ is a scalar. Treat $\\alpha$ as a tunable parameter that adjusts the level of prior uncertainty. Starting from the definition of the Gaussian likelihood and the Gaussian prior and using Bayes’ theorem, derive the posterior mean and posterior variance as closed-form analytic expressions in terms of $\\alpha$, $h$, $\\sigma^{2}$, $\\mu$, $C$, and $y$. Then, interpret the role of $\\alpha$ as a regularization strength informed by prior uncertainty by connecting the posterior to the minimizer of the negative log-posterior (which is a quadratic criterion in $x$). Explain, based on your derived expressions, how varying $\\alpha$ shifts the balance between the data misfit and the prior term.\n\nExpress your final answer as a two-entry row matrix containing, in order, the posterior mean and the posterior variance. No rounding is required. No units are required in the final answer.",
            "solution": "The problem requires the derivation of the posterior mean and variance for a linear inverse problem and an interpretation of a prior uncertainty scaling parameter, $\\alpha$.\n\nFirst, we establish the probabilistic framework. The state is a random variable $x \\in \\mathbb{R}$, and the observation is a random variable $y \\in \\mathbb{R}$.\n\nThe prior information on the state $x$ is given by a Gaussian distribution with mean $\\mu$ and variance scaled by $\\alpha C$, where $\\alpha > 0$ and $C > 0$. The prior probability density function (PDF) for $x$ is:\n$$p(x) = \\mathcal{N}(x; \\mu, \\alpha C) = \\frac{1}{\\sqrt{2\\pi \\alpha C}} \\exp\\left( -\\frac{(x - \\mu)^2}{2 \\alpha C} \\right)$$\n\nThe forward model relates the state $x$ to the observation $y$ through the linear equation $y = hx + \\varepsilon$. The observation error $\\varepsilon$ is drawn from a zero-mean Gaussian distribution with variance $\\sigma^2 > 0$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. This defines the likelihood function, which is the conditional probability of the observation $y$ given the state $x$. Given a value of $x$, the distribution of $y$ is Gaussian with mean $hx$ and variance $\\sigma^2$. The likelihood PDF is:\n$$p(y|x) = \\mathcal{N}(y; hx, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y - hx)^2}{2\\sigma^2} \\right)$$\n\nAccording to Bayes’ theorem, the posterior PDF, $p(x|y)$, is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nSubstituting the expressions for the likelihood and the prior, we get:\n$$p(x|y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y - hx)^2}{2\\sigma^2} \\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi \\alpha C}} \\exp\\left( -\\frac{(x - \\mu)^2}{2 \\alpha C} \\right) \\right]$$\nSince the normalization constants do not depend on $x$, we can combine the exponential terms:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2}\\left[ \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C} \\right] \\right)$$\nThe exponent is a quadratic function of $x$, which indicates that the posterior distribution is also Gaussian. Let the posterior distribution be $p(x|y) = \\mathcal{N}(x; \\mu_{\\text{post}}, C_{\\text{post}})$, which has the form:\n$$p(x|y) \\propto \\exp\\left( -\\frac{(x - \\mu_{\\text{post}})^2}{2 C_{\\text{post}}} \\right)$$\nTo find the posterior mean $\\mu_{\\text{post}}$ and posterior variance $C_{\\text{post}}$, we can expand the argument of the exponential in our expression for $p(x|y)$ and complete the square with respect to $x$. Let $Q(x)$ be the term in the square brackets:\n$$Q(x) = \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C}$$\n$$Q(x) = \\frac{y^2 - 2yhx + h^2x^2}{\\sigma^2} + \\frac{x^2 - 2x\\mu + \\mu^2}{\\alpha C}$$\nWe group terms by powers of $x$:\n$$Q(x) = x^2 \\left( \\frac{h^2}{\\sigma^2} + \\frac{1}{\\alpha C} \\right) - 2x \\left( \\frac{yh}{\\sigma^2} + \\frac{\\mu}{\\alpha C} \\right) + \\left( \\frac{y^2}{\\sigma^2} + \\frac{\\mu^2}{\\alpha C} \\right)$$\nComparing this to the standard quadratic form $\\frac{(x - \\mu_{\\text{post}})^2}{C_{\\text{post}}} = \\frac{1}{C_{\\text{post}}}x^2 - \\frac{2\\mu_{\\text{post}}}{C_{\\text{post}}}x + \\frac{\\mu_{\\text{post}}^2}{C_{\\text{post}}}$, we can identify the coefficients.\nThe coefficient of $x^2$ gives the inverse of the posterior variance:\n$$\\frac{1}{C_{\\text{post}}} = \\frac{h^2}{\\sigma^2} + \\frac{1}{\\alpha C} = \\frac{h^2 \\alpha C + \\sigma^2}{\\sigma^2 \\alpha C}$$\nSolving for the posterior variance $C_{\\text{post}}$:\n$$C_{\\text{post}} = \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}$$\nThe coefficient of $x$ gives the ratio of the posterior mean to the posterior variance:\n$$\\frac{\\mu_{\\text{post}}}{C_{\\text{post}}} = \\frac{yh}{\\sigma^2} + \\frac{\\mu}{\\alpha C} = \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C}$$\nSolving for the posterior mean $\\mu_{\\text{post}}$:\n$$\\mu_{\\text{post}} = C_{\\text{post}} \\left( \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C} \\right)$$\nSubstituting the expression for $C_{\\text{post}}$:\n$$\\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2} \\right) \\left( \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C} \\right)$$\n$$\\mu_{\\text{post}} = \\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}$$\n\nNow, we interpret the role of $\\alpha$ by connecting the Bayesian formulation to a regularization framework. The maximum a posteriori (MAP) estimate of $x$ is the value that maximizes $p(x|y)$, which is equivalent to minimizing the negative log-posterior. The negative log-posterior is proportional to the quadratic function $Q(x)$ defined earlier, often called the cost function, $J(x)$:\n$$J(x) = \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C}$$\nThis cost function consists of two terms:\n1.  Data Misfit: The term $\\frac{(y - hx)^2}{\\sigma^2}$ measures the squared difference between the prediction $hx$ and the observation $y$, weighted by the inverse of the observation error variance. It quantifies how well the state $x$ fits the data.\n2.  Prior/Regularization Term: The term $\\frac{(x - \\mu)^2}{\\alpha C}$ penalizes the deviation of the state $x$ from the prior mean $\\mu$, weighted by the inverse of the prior variance $\\alpha C$.\n\nThe parameter $\\alpha$ directly controls the variance of the prior distribution, $\\mathcal{N}(\\mu, \\alpha C)$. It thus modulates the level of confidence in the prior information and acts as a regularization parameter.\n-   When $\\alpha \\to \\infty$, the prior variance becomes infinite, representing a state of maximal uncertainty or a non-informative prior. The regularization term $\\frac{(x - \\mu)^2}{\\alpha C} \\to 0$, and the cost function is dominated by the data misfit term. The posterior mean approaches $\\lim_{\\alpha \\to \\infty} \\mu_{\\text{post}} = \\frac{yhC}{h^2C} = \\frac{y}{h}$, which is the maximum likelihood estimate. The solution is driven entirely by the data.\n-   When $\\alpha \\to 0$, the prior variance approaches zero, representing a state of absolute certainty that $x = \\mu$. The regularization term $\\frac{(x - \\mu)^2}{\\alpha C}$ becomes infinitely large for any $x \\neq \\mu$, forcing the solution towards the prior mean. The posterior mean approaches $\\lim_{\\alpha \\to 0} \\mu_{\\text{post}} = \\frac{\\mu\\sigma^2}{\\sigma^2} = \\mu$. The solution is driven entirely by the prior information, ignoring the data.\n\nIn summary, $\\alpha$ adjusts the balance between fitting the data and adhering to the prior belief. A larger $\\alpha$ signifies less confidence in the prior (weaker regularization), allowing the posterior estimate to be more influenced by the observation $y$. A smaller $\\alpha$ signifies greater confidence in the prior (stronger regularization), pulling the estimate closer to the prior mean $\\mu$. This demonstrates the dual role of the prior as both a source of information and a mechanism for regularization in inverse problems.\n\nThe final answer consists of the derived posterior mean and posterior variance.\nPosterior Mean: $\\mu_{\\text{post}} = \\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}$\nPosterior Variance: $C_{\\text{post}} = \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2} & \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond the smoothness-promoting Gaussian prior, this exercise introduces the Laplace prior, which is fundamental for enforcing sparsity in solutions. You will derive the celebrated soft-thresholding operator as the solution to a Maximum A Posteriori (MAP) estimation problem involving an $\\ell_1$ penalty. This practice connects Bayesian estimation to the powerful framework of proximal optimization and illustrates how different prior assumptions lead to qualitatively different solution structures .",
            "id": "3418473",
            "problem": "Consider a single-coefficient linear observation model in which an unknown scalar state $x$ is observed through a single linear measurement $y$ with additive noise, so that $y = a x + \\varepsilon$. Assume the noise $\\varepsilon$ is Gaussian with zero mean and variance $\\sigma^{2}$, and that the prior on $x$ is Laplace with density proportional to $\\exp(-\\lambda |x|)$, where $\\lambda > 0$ is a fixed penalty weight. Your tasks are:\n\n1) Starting only from the definition of the proximal operator for a proper, lower-semicontinuous convex function $f$, namely $\\operatorname{prox}_{\\gamma f}(v) = \\arg\\min_{x} \\left\\{ \\frac{1}{2} (x - v)^{2} + \\gamma f(x) \\right\\}$ for any $\\gamma > 0$, and using subdifferential optimality conditions from convex analysis, derive the closed-form expression of $\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v)$ in one dimension.\n\n2) Using Bayes’ theorem and the definitions of the Gaussian likelihood and Laplace prior, write down the negative log-posterior (up to an irrelevant additive constant) for $x$ given $y$ in this model and characterize the Maximum A Posteriori (MAP) estimator as the minimizer of that negative log-posterior. By completing the square and rescaling, reduce the MAP problem to a proximal problem that matches part (1), and then obtain an explicit closed-form expression of the MAP estimator in terms of $a$, $\\sigma^{2}$, $\\lambda$, and $y$.\n\n3) Evaluate your closed-form expression for $a = 1.6$, $\\sigma^{2} = 0.8$, $\\lambda = 0.3$, and $y = 0.2$. Report the final numerical value of the MAP estimator for $x$, rounded to four significant figures. No units are required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in Bayesian statistics and convex optimization, well-posed with a unique solution, and formally stated without ambiguity or contradiction. We proceed with the solution.\n\nThe problem is divided into three parts. We will address them in sequence.\n\n**Part 1: Derivation of the Proximal Operator**\n\nWe are asked to derive the closed-form expression for the proximal operator $\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v)$. Let the threshold parameter be denoted by $\\alpha = \\gamma\\lambda$. Since $\\gamma > 0$ and $\\lambda > 0$, we have $\\alpha > 0$. The function is $f(x) = |x|$. Following the provided definition, the proximal operator is given by the solution to a minimization problem:\n$$\n\\operatorname{prox}_{\\alpha |\\cdot|}(v) = \\arg\\min_{x} \\left\\{ J(x) = \\frac{1}{2} (x - v)^{2} + \\alpha |x| \\right\\}\n$$\nThe objective function $J(x)$ is the sum of a strictly convex, differentiable function ($\\frac{1}{2}(x-v)^2$) and a convex function ($\\alpha|x|$). Thus, $J(x)$ is strictly convex and has a unique minimizer, which we denote by $x^*$.\n\nWe use the subdifferential optimality condition, which states that $x^*$ is the minimizer if and only if $0$ is in the subdifferential of $J(x)$ at $x^*$.\n$$\n0 \\in \\partial J(x^*)\n$$\nThe subdifferential of $J(x)$ is given by the sum of the subdifferentials of its components:\n$$\n\\partial J(x) = \\partial \\left( \\frac{1}{2}(x - v)^2 \\right) + \\partial(\\alpha |x|)\n$$\nThe first term is differentiable, so its subdifferential is its derivative: $x - v$. The subdifferential of the second term is $\\alpha \\partial|x|$. The subdifferential of the absolute value function, $\\partial|x|$, is defined as:\n$$\n\\partial |x| =\n\\begin{cases}\n    \\{1\\}       & \\text{if } x > 0 \\\\\n    \\{-1\\}      & \\text{if } x  0 \\\\\n    [-1, 1]  \\text{if } x = 0\n\\end{cases}\n$$\nSo, the optimality condition for the minimizer $x^*$ is:\n$$\n0 \\in (x^* - v) + \\alpha \\partial|x^*|\n$$\nThis can be rewritten as:\n$$\nv - x^* \\in \\alpha \\partial|x^*|\n$$\nWe analyze this condition by considering three cases for the value of $x^*$:\n\nCase 1: $x^* > 0$.\nIn this case, $\\partial|x^*| = \\{1\\}$. The condition becomes $v - x^* = \\alpha(1)$, which implies $x^* = v - \\alpha$. For this solution to be consistent with the assumption $x^* > 0$, we must have $v - \\alpha > 0$, or $v > \\alpha$.\n\nCase 2: $x^*  0$.\nIn this case, $\\partial|x^*| = \\{-1\\}$. The condition becomes $v - x^* = \\alpha(-1)$, which implies $x^* = v + \\alpha$. For this solution to be consistent with the assumption $x^*  0$, we must have $v + \\alpha  0$, or $v  -\\alpha$.\n\nCase 3: $x^* = 0$.\nIn this case, $\\partial|x^*| = [-1, 1]$. The condition becomes $v - 0 \\in \\alpha[-1, 1]$, which is equivalent to $v \\in [-\\alpha, \\alpha]$, or $|v| \\le \\alpha$.\n\nCombining these three cases gives the complete solution for $x^*$:\n$$\nx^* =\n\\begin{cases}\n    v - \\alpha  \\text{if } v > \\alpha \\\\\n    v + \\alpha  \\text{if } v  -\\alpha \\\\\n    0         \\text{if } |v| \\le \\alpha\n\\end{cases}\n$$\nThis piecewise function is known as the soft-thresholding operator, often denoted as $\\mathcal{S}_{\\alpha}(v)$. It can be written in a compact form:\n$$\nx^* = \\operatorname{sgn}(v) \\max(0, |v| - \\alpha)\n$$\nReplacing $\\alpha$ with the original parameter $\\gamma\\lambda$, we obtain the final expression for the proximal operator:\n$$\n\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v) = \\operatorname{sgn}(v) \\max(0, |v| - \\gamma\\lambda)\n$$\n\n**Part 2: MAP Estimator Derivation**\n\nWe are given the linear observation model $y = ax + \\varepsilon$, where the noise $\\varepsilon$ is Gaussian with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. This implies that the likelihood of the observation $y$ given the state $x$ is also Gaussian: $y|x \\sim \\mathcal{N}(ax, \\sigma^2)$. The probability density function (PDF) for the likelihood is:\n$$\np(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2}\\right)\n$$\nThe prior on $x$ is a Laplace distribution with a density proportional to $\\exp(-\\lambda|x|)$, where $\\lambda > 0$:\n$$\np(x) \\propto \\exp(-\\lambda|x|)\n$$\nAccording to Bayes' theorem, the posterior distribution of $x$ given $y$ is proportional to the product of the likelihood and the prior:\n$$\np(x|y) \\propto p(y|x) p(x) \\propto \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2}\\right) \\exp(-\\lambda|x|) = \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2} - \\lambda|x|\\right)\n$$\nThe Maximum A Posteriori (MAP) estimator, $\\hat{x}_{\\text{MAP}}$, is the value of $x$ that maximizes the posterior probability $p(x|y)$. This is equivalent to minimizing the negative log-posterior. The negative log-posterior, up to an irrelevant additive constant, is:\n$$\nJ_{\\text{MAP}}(x) = \\frac{(y - ax)^2}{2\\sigma^2} + \\lambda|x|\n$$\nThus, the MAP estimator is given by:\n$$\n\\hat{x}_{\\text{MAP}} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}(y - ax)^2 + \\lambda|x| \\right\\}\n$$\nTo relate this to the proximal operator problem from Part 1, we must manipulate the objective function $J_{\\text{MAP}}(x)$ into the form $\\frac{1}{2}(x-v)^2 + \\alpha'|x|$. We begin by completing the square for the quadratic term involving $x$:\n$$\n\\frac{1}{2\\sigma^2}(y - ax)^2 = \\frac{1}{2\\sigma^2}(a^2x^2 - 2axy + y^2) = \\frac{a^2}{2\\sigma^2} \\left(x^2 - 2\\frac{y}{a}x + \\left(\\frac{y}{a}\\right)^2\\right) = \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2\n$$\nSubstituting this back into the objective function:\n$$\nJ_{\\text{MAP}}(x) = \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2 + \\lambda|x|\n$$\nMinimizing this function is equivalent to minimizing the same function scaled by a positive constant, as scaling does not change the location of the minimum. We scale by $\\frac{\\sigma^2}{a^2}$ to make the coefficient of the squared term equal to $\\frac{1}{2}$:\n$$\n\\hat{x}_{\\text{MAP}} = \\arg\\min_x \\left\\{ \\frac{\\sigma^2}{a^2} \\left[ \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2 + \\lambda|x| \\right] \\right\\} = \\arg\\min_x \\left\\{ \\frac{1}{2} \\left(x - \\frac{y}{a}\\right)^2 + \\frac{\\sigma^2\\lambda}{a^2}|x| \\right\\}\n$$\nThis expression is now in the form of the proximal problem from Part 1, $\\arg\\min_x \\{ \\frac{1}{2}(x-v)^2 + \\alpha'|x| \\}$, with the identifications:\n$$\nv = \\frac{y}{a} \\quad \\text{and} \\quad \\alpha' = \\frac{\\sigma^2\\lambda}{a^2}\n$$\nUsing the soft-thresholding solution derived in Part 1, with the threshold parameter $\\alpha'$:\n$$\n\\hat{x}_{\\text{MAP}} = \\mathcal{S}_{\\alpha'}(v) = \\operatorname{sgn}(v) \\max(0, |v| - \\alpha')\n$$\nSubstituting back the expressions for $v$ and $\\alpha'$, we obtain the explicit closed-form expression for the MAP estimator:\n$$\n\\hat{x}_{\\text{MAP}} = \\operatorname{sgn}\\left(\\frac{y}{a}\\right) \\max\\left(0, \\left|\\frac{y}{a}\\right| - \\frac{\\sigma^2\\lambda}{a^2}\\right)\n$$\n\n**Part 3: Numerical Evaluation**\n\nWe are asked to evaluate the closed-form expression for $\\hat{x}_{\\text{MAP}}$ with the given values: $a = 1.6$, $\\sigma^2 = 0.8$, $\\lambda = 0.3$, and $y = 0.2$.\n\nFirst, we calculate the terms inside the expression:\n$$\n\\frac{y}{a} = \\frac{0.2}{1.6} = \\frac{2}{16} = \\frac{1}{8} = 0.125\n$$\nNext, we calculate the threshold parameter:\n$$\n\\frac{\\sigma^2\\lambda}{a^2} = \\frac{(0.8)(0.3)}{(1.6)^2} = \\frac{0.24}{2.56} = \\frac{24}{256} = \\frac{3}{32} = 0.09375\n$$\nNow, we substitute these values into the expression for $\\hat{x}_{\\text{MAP}}$:\n$$\n\\hat{x}_{\\text{MAP}} = \\operatorname{sgn}(0.125) \\max(0, |0.125| - 0.09375)\n$$\nSince $0.125 > 0.09375$, the term inside the $\\max$ function is positive. As $\\operatorname{sgn}(0.125) = 1$, we get:\n$$\n\\hat{x}_{\\text{MAP}} = 1 \\times (0.125 - 0.09375) = 0.03125\n$$\nThe problem requires the result rounded to four significant figures. The number $0.03125$ has exactly four significant figures ($3, 1, 2, 5$). Therefore, no further rounding is needed.\nThe numerical value of the MAP estimator for $x$ is $0.03125$.",
            "answer": "$$\n\\boxed{0.03125}\n$$"
        },
        {
            "introduction": "After understanding how priors define the form of regularization, a critical practical question remains: how to choose its strength? This computational exercise guides you through implementing the L-curve criterion, a popular heuristic for selecting the regularization parameter $\\lambda$. By analyzing the trade-off between data fit and solution complexity across a range of $\\lambda$ values, you will learn to numerically identify a well-balanced solution .",
            "id": "3418439",
            "problem": "Consider a linear inverse problem with observations modeled as $b = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^n$, and additive noise $\\varepsilon \\in \\mathbb{R}^m$. Assume a Gaussian likelihood with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ and a Gaussian prior on $x$ with mean $\\mu \\in \\mathbb{R}^n$ and precision operator $\\alpha^2 L^{\\top} L$, where $L \\in \\mathbb{R}^{p \\times n}$. The Maximum A Posteriori (MAP) estimate arises from minimizing the negative log-posterior, which is equivalent to Tikhonov regularization: minimize the functional $J_{\\lambda}(x) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert L (x - \\mu) \\rVert_2^2$ with $\\lambda = \\sigma^2 \\alpha^2 > 0$. This encodes prior information through the choice of $L$ and $\\mu$. For each $\\lambda > 0$, the minimizer $x_{\\lambda}$ satisfies the normal equations $(A^{\\top} A + \\lambda L^{\\top} L) x_{\\lambda} = A^{\\top} b + \\lambda L^{\\top} L \\mu$.\n\nThe L-curve criterion evaluates the trade-off between data misfit and prior penalty by considering the parametric curve $\\mathcal{C}(\\lambda) = (\\xi(\\lambda), \\eta(\\lambda))$ with $\\xi(\\lambda) = \\log \\lVert A x_{\\lambda} - b \\rVert_2$ and $\\eta(\\lambda) = \\log \\lVert L (x_{\\lambda} - \\mu) \\rVert_2$. The \"corner\" of the L-curve, often corresponding to a good balance between fit and regularization, can be located by maximizing the curvature. To obtain a scale-invariant parametrization, define $t = \\log \\lambda$ and consider the planar curve $c(t) = (\\xi(t), \\eta(t))$ with $\\lambda = e^t$. The curvature $\\kappa$ of a planar curve $c(t) = (x(t), y(t))$ is given by the well-tested formula\n$$\n\\kappa(t) = \\frac{| x'(t) y''(t) - y'(t) x''(t) |}{\\left( x'(t)^2 + y'(t)^2 \\right)^{3/2}}.\n$$\nIn this setting, $x(t) = \\xi(t)$ and $y(t) = \\eta(t)$, and derivatives are with respect to $t = \\log \\lambda$. Numerically, $\\kappa(t)$ can be approximated by finite differences on a uniform grid in $t$.\n\nYour task is to formulate this criterion and implement a program that, for a small yet diverse set of test cases, computes $\\kappa(t)$ across a uniform grid in $t = \\log \\lambda$, locates the maximizer $t_{\\star}$ (excluding endpoints), and reports $\\lambda_{\\star} = e^{t_{\\star}}$ for each test case.\n\nUse the following test suite, common definitions, and numerical details:\n\nCommon definitions for all test cases:\n- Dimension $n = 20$ and grid points $s_i = \\frac{i}{n+1}$ for $i = 1, 2, \\dots, n$.\n- Define $\\Delta = \\frac{1}{n}$ and kernel width $w = 0.08$.\n- Define the Gaussian blur matrix $A_0 \\in \\mathbb{R}^{n \\times n}$ by\n$$\n(A_0)_{ij} = \\exp\\!\\left( - \\frac{(s_i - s_j)^2}{2 w^2} \\right) \\, \\Delta \\quad \\text{for } 1 \\le i,j \\le n.\n$$\n- Define the \"true\" state by $x_{\\mathrm{true}, i} = \\sin(2 \\pi s_i) + 0.5 \\, s_i$ for $i = 1, 2, \\dots, n$.\n- For a given noise level $\\sigma_{\\mathrm{noise}} > 0$, define a deterministic noise vector $\\varepsilon \\in \\mathbb{R}^n$ by $\\varepsilon_i = \\sigma_{\\mathrm{noise}} \\, \\frac{\\cos(7 i)}{i}$ for $i = 1, 2, \\dots, n$.\n- For each test case, the data are $b = A x_{\\mathrm{true}} + \\varepsilon$ with the specified $A$ and $\\sigma_{\\mathrm{noise}}$.\n\nDefine the following regularization operator and prior mean options:\n- Identity operator $L = I_n$ (so $p = n$).\n- First-difference operator $D \\in \\mathbb{R}^{(n-1) \\times n}$ with entries $D_{k,k} = 1$, $D_{k,k+1} = -1$ for $k = 1, 2, \\dots, n-1$, and zeros elsewhere.\n- Prior mean $\\mu \\in \\mathbb{R}^n$ as specified below (either $\\mu = 0$ or a nonzero trend).\n\nTest suite (four cases):\n1. Case $1$ (ill-conditioned forward map, identity prior): $A = A_0$, $L = I_n$, $\\mu = 0$, $\\sigma_{\\mathrm{noise}} = 10^{-3}$.\n2. Case $2$ (ill-conditioned forward map, smoothness prior with nonzero mean): $A = A_0$, $L = D$, $\\mu_i = 0.25 \\, s_i$, $\\sigma_{\\mathrm{noise}} = 10^{-3}$.\n3. Case $3$ (well-conditioned forward map, smoothness prior): $A = I_n$, $L = D$, $\\mu = 0$, $\\sigma_{\\mathrm{noise}} = 5 \\cdot 10^{-2}$.\n4. Case $4$ (rank-deficient forward map, identity prior): Start from $A_0$ and set its last column equal to its first column to obtain $A \\in \\mathbb{R}^{n \\times n}$, then use $L = I_n$, $\\mu = 0$, $\\sigma_{\\mathrm{noise}} = 10^{-3}$.\n\nFor each case:\n- Solve for $x_{\\lambda}$ over a grid of $\\lambda$ values defined by a uniform grid in $t = \\log \\lambda$ with $t \\in [\\log(10^{-10}), \\log(10^2)]$ and $M = 201$ grid points. For each $\\lambda = e^t$, compute the data misfit norm $\\rho(\\lambda) = \\lVert A x_{\\lambda} - b \\rVert_2$ and the penalty term norm $\\zeta(\\lambda) = \\lVert L (x_{\\lambda} - \\mu) \\rVert_2$. The L-curve coordinates are then $\\xi(t) = \\log \\rho(\\lambda(t))$ and $\\eta(t) = \\log \\zeta(\\lambda(t))$.\n- Numerically approximate derivatives $\\xi'(t)$, $\\xi''(t)$, $\\eta'(t)$, and $\\eta''(t)$ via centered finite differences on the uniform $t$-grid. Compute the curvature using\n$$\n\\kappa(t) = \\frac{\\left| \\xi'(t) \\eta''(t) - \\eta'(t) \\xi''(t) \\right|}{\\left( \\xi'(t)^2 + \\eta'(t)^2 \\right)^{3/2}}.\n$$\n- Identify the index $k^{\\star}$ that maximizes $\\kappa(t)$ over the interior grid points (exclude the endpoints). Report $\\lambda_{\\star} = \\exp(t_{k^{\\star}})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four $\\lambda_{\\star}$ values corresponding to Cases $1$ to $4$, in that order, rounded to six decimal places, as a comma-separated list enclosed in square brackets (for example, $[0.123456,0.000789,1.234568,0.010000]$).\n- No physical units are involved. Angles do not appear. Percentages are not used.\n- The answers must be floats.",
            "solution": "The problem is assessed as valid as it is scientifically grounded in the well-established theory of inverse problems and Tikhonov regularization, is mathematically and computationally well-posed, and provides a complete and consistent set of parameters and instructions for its solution.\n\nThe objective is to find an optimal regularization parameter, $\\lambda_{\\star}$, for four distinct linear inverse problems by implementing the L-curve criterion. The L-curve method provides a heuristic for balancing the trade-off between fidelity to the observed data and adherence to prior information about the solution.\n\nA linear inverse problem is modeled by the equation $b = A x_{\\mathrm{true}} + \\varepsilon$, where $b \\in \\mathbb{R}^m$ are the observations, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ is the unknown true state, and $\\varepsilon \\in \\mathbb{R}^m$ is measurement noise. In a Bayesian framework, this corresponds to a Gaussian likelihood $\\mathcal{N}(b | A x, \\sigma^2 I)$. We impose a Gaussian prior on the solution $x \\sim \\mathcal{N}(\\mu, (\\alpha^2 L^{\\top} L)^{-1})$, where $\\mu$ is the prior mean and $L$ is an operator that encodes structural information (e.g., smoothness).\n\nThe Maximum A Posteriori (MAP) estimate is found by minimizing the negative log-posterior, which yields the Tikhonov regularization functional:\n$$\nJ_{\\lambda}(x) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert L (x - \\mu) \\rVert_2^2\n$$\nHere, the regularization parameter $\\lambda > 0$ balances the data misfit term $\\lVert A x - b \\rVert_2^2$ and the regularization (or penalty) term $\\lVert L (x - \\mu) \\rVert_2^2$. The minimizer of this functional, denoted $x_{\\lambda}$, is the regularized solution. Taking the gradient of $J_{\\lambda}(x)$ with respect to $x$ and setting it to zero gives the normal equations, a system of linear equations for $x_{\\lambda}$:\n$$\n(A^{\\top} A + \\lambda L^{\\top} L) x_{\\lambda} = A^{\\top} b + \\lambda L^{\\top} L \\mu\n$$\nThis system must be solved for each candidate value of $\\lambda$.\n\nThe L-curve criterion is based on a parametric plot of the log of the penalty term versus the log of the misfit term, for a range of $\\lambda$ values. The curve is defined as $\\mathcal{C}(\\lambda) = (\\log \\lVert A x_\\lambda - b \\rVert_2, \\log \\lVert L(x_\\lambda - \\mu) \\rVert_2)$. This curve typically has an 'L' shape. The corner of this L-shape is considered to represent an optimal balance, where neither the misfit nor the penalty term is excessively large. This corner corresponds to the point of maximum curvature on the curve.\n\nTo compute this, we reparametrize the curve using $t = \\log \\lambda$, so that the curve is given by $c(t) = (\\xi(t), \\eta(t))$, where:\n$$\n\\xi(t) = \\log \\lVert A x_{\\lambda(t)} - b \\rVert_2 \\quad \\text{and} \\quad \\eta(t) = \\log \\lVert L (x_{\\lambda(t)} - \\mu) \\rVert_2, \\quad \\text{with } \\lambda(t) = e^t\n$$\nThe curvature $\\kappa$ of this planar curve $c(t)$ is given by the formula:\n$$\n\\kappa(t) = \\frac{\\lvert \\xi'(t) \\eta''(t) - \\eta'(t) \\xi''(t) \\rvert}{\\left( \\xi'(t)^2 + \\eta'(t)^2 \\right)^{3/2}}\n$$\nwhere the derivatives are with respect to $t$.\n\nThe computational procedure is as follows:\n1.  For each test case, construct the matrices $A$ and $L$, and vectors $b$ and $\\mu$ as specified.\n2.  Define a uniform grid in $t = \\log \\lambda$ over the interval $[\\log(10^{-10}), \\log(10^2)]$ with $M=201$ points. Let this grid be $\\{t_k\\}_{k=0}^{M-1}$ with step size $h = t_{k+1} - t_k$.\n3.  For each $t_k$ in the grid:\n    a. Calculate $\\lambda_k = e^{t_k}$.\n    b. Solve the normal equations $(A^{\\top} A + \\lambda_k L^{\\top} L) x_{\\lambda_k} = A^{\\top} b + \\lambda_k L^{\\top} L \\mu$ for $x_{\\lambda_k}$.\n    c. Compute the misfit norm $\\rho_k = \\lVert A x_{\\lambda_k} - b \\rVert_2$ and the penalty norm $\\zeta_k = \\lVert L (x_{\\lambda_k} - \\mu) \\rVert_2$. To avoid numerical issues with the logarithm, we add machine epsilon before the operation.\n    d. Compute the log-log coordinates: $\\xi_k = \\log(\\rho_k + \\epsilon_{\\text{mach}})$ and $\\eta_k = \\log(\\zeta_k + \\epsilon_{\\text{mach}})$.\n4.  Approximate the first and second derivatives of $\\xi(t)$ and $\\eta(t)$ at the interior grid points ($k=1, \\dots, M-2$) using centered finite differences:\n    $$\n    f'(t_k) \\approx \\frac{f_{k+1} - f_{k-1}}{2h}, \\quad f''(t_k) \\approx \\frac{f_{k+1} - 2f_k + f_{k-1}}{h^2}\n    $$\n5.  Substitute these numerical derivatives into the curvature formula to compute $\\kappa(t_k)$ for each interior point.\n6.  Find the index $k^{\\star}$ that maximizes the curvature $\\kappa$ over the interior points. The optimal parameter is then $\\lambda_{\\star} = e^{t_{k^{\\star}}}$.\n\nThis procedure is applied to each of the four test cases, which explore different types of forward operators (ill-conditioned, well-conditioned, rank-deficient) and prior information (identity vs. smoothness, zero vs. non-zero mean). The final output will be the list of the four computed $\\lambda_{\\star}$ values.",
            "answer": "$$\n\\boxed{[0.000109, 0.000030, 0.002683, 0.000109]}\n$$"
        }
    ]
}