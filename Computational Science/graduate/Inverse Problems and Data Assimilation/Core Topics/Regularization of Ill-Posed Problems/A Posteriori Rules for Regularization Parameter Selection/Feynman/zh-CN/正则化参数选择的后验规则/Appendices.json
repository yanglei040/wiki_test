{
    "hands_on_practices": [
        {
            "introduction": "本练习为Tikhonov正则化提供了一个基础的、分步的实践。通过处理一个简单的 $2 \\times 2$ 系统，您将手动计算不同参数 $\\alpha$ 下的正则化解，并亲眼观察它如何影响结果。这个练习旨在巩固正则化的核心机制，并介绍一个基本的数据驱动选择规则——Morozov差异原则（MDP）。",
            "id": "3361699",
            "problem": "考虑由 $y^{\\delta} = A x^{\\dagger} + \\eta$ 描述的带观测噪声的线性反问题，其中 $A \\in \\mathbb{R}^{2 \\times 2}$ 是病态的 (ill-conditioned)，$x^{\\dagger} \\in \\mathbb{R}^{2}$ 是未知的真实状态，$\\eta \\in \\mathbb{R}^{2}$ 是噪声。设测量矩阵为\n$$\nA = \\begin{pmatrix}\n1  0 \\\\\n0  10^{-2}\n\\end{pmatrix},\n$$\n且带有噪声的数据为\n$$\ny^{\\delta} = \\begin{pmatrix}\n1 \\\\\n0.2\n\\end{pmatrix}.\n$$\n对于带有单位罚项的吉洪诺夫正则化 (Tikhonov regularization)，对任意 $\\alpha > 0$ 定义正则化估计量 $x_{\\alpha}^{\\delta}$ 为下式的最小化子：\n$$\n\\|A x - y^{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2},\n$$\n并回想 $x_{\\alpha}^{\\delta}$ 满足正规方程 (normal equations)\n$$\n(A^{\\top} A + \\alpha I) x_{\\alpha}^{\\delta} = A^{\\top} y^{\\delta}.\n$$\n给定正则化参数的候选集\n$$\n\\mathcal{A} = \\{10^{-6}, 10^{-4}, 3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}.\n$$\n请执行以下任务：\n- 对每个 $\\alpha \\in \\mathcal{A}$，计算 $x_{\\alpha}^{\\delta}$ 和残差范数 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$。\n- 使用第一性原理和针对 $A$ 的奇异值分解 (singular value decomposition, SVD) 框架，讨论 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ 是否关于 $\\alpha$ 单调，并严格证明你的结论。\n\n假设已知噪声界 $\\|\\eta\\|_{2} \\le \\delta$，其中 $\\delta = 0.135$，并使用带有安全因子 $\\tau = 1.1$ 的莫罗佐夫差异原理 (Morozov discrepancy principle)，即选择 $\\alpha^{\\star}$ 使得 $\\|A x_{\\alpha^{\\star}}^{\\delta} - y^{\\delta}\\|_{2} \\approx \\tau \\delta$。在离散集 $\\mathcal{A}$ 中，选择满足 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2} \\ge \\tau \\delta$ 的最小 $\\alpha$。将选定的值 $\\alpha^{\\star}$ 作为你的最终答案。最终答案必须是一个实数值。",
            "solution": "问题陈述经审查确认有效。它在科学上基于成熟的线性反问题吉洪诺夫正则化理论，是适定 (well-posed) 的，提供了所有必要信息，并以客观、数学上精确的语言表述。\n\n问题的核心是分析线性系统 $y^{\\delta} = A x^{\\dagger} + \\eta$ 的吉洪诺夫正则化解 $x_{\\alpha}^{\\delta}$。该正则化解最小化泛函 $\\|A x - y^{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2}$，并由正规方程 $(A^{\\top} A + \\alpha I) x_{\\alpha}^{\\delta} = A^{\\top} y^{\\delta}$ 的解给出。\n\n**第1部分：计算正则化解和残差**\n\n首先，我们根据问题陈述定义矩阵和向量。\n矩阵 $A$ 和数据向量 $y^{\\delta}$ 为：\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  10^{-2} \\end{pmatrix}, \\quad y^{\\delta} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\n$$\n由于 $A$ 是对称矩阵，所以 $A^{\\top} = A$。我们计算正规方程的各组成部分：\n$$\nA^{\\top}A = A^2 = \\begin{pmatrix} 1^2  0 \\\\ 0  (10^{-2})^2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  10^{-4} \\end{pmatrix}\n$$\n$$\nA^{\\top}y^{\\delta} = Ay^{\\delta} = \\begin{pmatrix} 1  0 \\\\ 0  10^{-2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n正规方程变为：\n$$\n\\left( \\begin{pmatrix} 1  0 \\\\ 0  10^{-4} \\end{pmatrix} + \\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) x_{\\alpha}^{\\delta} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1+\\alpha  0 \\\\ 0  10^{-4}+\\alpha \\end{pmatrix} x_{\\alpha}^{\\delta} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n对于任意 $\\alpha > 0$，左侧的矩阵是对角的且可逆，从而得出解：\n$$\nx_{\\alpha}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+\\alpha} \\end{pmatrix}\n$$\n残差为 $r_{\\alpha}^{\\delta} = A x_{\\alpha}^{\\delta} - y^{\\delta}$。残差的平方范数为：\n$$\n\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{1}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-5}}{10^{-4}+\\alpha} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{-\\alpha}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-5} - 0.2(10^{-4}+\\alpha)}{10^{-4}+\\alpha} \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{-\\alpha}{1+\\alpha} \\\\ \\frac{-0.2\\alpha}{10^{-4}+\\alpha} \\end{pmatrix}\\right\\|_{2}^{2}\n$$\n$$\n\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}^{2} = \\alpha^2 \\left( \\frac{1}{(1+\\alpha)^2} + \\frac{0.04}{(10^{-4}+\\alpha)^2} \\right)\n$$\n我们对候选集 $\\mathcal{A} = \\{10^{-6}, 10^{-4}, 3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}$ 中的每个 $\\alpha$ 计算 $x_{\\alpha}^{\\delta}$ 和 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$。\n\n对于 $\\alpha = 10^{-6}$:\n$x_{10^{-6}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-6}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-6}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.000001} \\\\ \\frac{200}{10.1} \\end{pmatrix}$。\n$\\|A x_{10^{-6}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-6})^2 \\left( \\frac{1}{(1.000001)^2} + \\frac{0.04}{(1.01 \\times 10^{-4})^2} \\right)} \\approx 0.00198$。\n\n对于 $\\alpha = 10^{-4}$:\n$x_{10^{-4}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-4}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.0001} \\\\ 10 \\end{pmatrix}$。\n$\\|A x_{10^{-4}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-4})^2 \\left( \\frac{1}{(1.0001)^2} + \\frac{0.04}{(2 \\times 10^{-4})^2} \\right)} = \\sqrt{\\frac{10^{-8}}{(1.0001)^2} + 0.01} > 0.1$。更精确的计算得出结果约为 $\\approx 0.10000005$。\n\n对于 $\\alpha = 3 \\times 10^{-4}$:\n$x_{3 \\times 10^{-4}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+3 \\times 10^{-4}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+3 \\times 10^{-4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.0003} \\\\ 5 \\end{pmatrix}$。\n$\\|A x_{3 \\times 10^{-4}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(3 \\times 10^{-4})^2 \\left( \\frac{1}{(1.0003)^2} + \\frac{0.04}{(4 \\times 10^{-4})^2} \\right)} = \\sqrt{\\frac{9 \\times 10^{-8}}{(1.0003)^2} + 0.0225} > 0.15$。更精确的计算得出结果约为 $\\approx 0.1500003$。\n\n对于 $\\alpha = 10^{-3}$:\n$x_{10^{-3}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-3}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.001} \\\\ \\frac{20}{11} \\end{pmatrix}$。\n$\\|A x_{10^{-3}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-3})^2 \\left( \\frac{1}{(1.001)^2} + \\frac{0.04}{(1.1 \\times 10^{-3})^2} \\right)} \\approx 0.1818$。\n\n对于 $\\alpha = 10^{-2}$:\n$x_{10^{-2}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-2}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.01} \\\\ \\frac{2}{10.1} \\end{pmatrix}$。\n$\\|A x_{10^{-2}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-2})^2 \\left( \\frac{1}{(1.01)^2} + \\frac{0.04}{(1.01 \\times 10^{-2})^2} \\right)} \\approx 0.1983$。\n\n**第2部分：残差范数的单调性**\n\n我们严格证明残差范数 $\\rho(\\alpha) = \\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ 是关于 $\\alpha$ 的单调函数。考虑 $A$ 的奇异值分解 $A = U \\Sigma V^{\\top}$，其中奇异值 $\\sigma_i > 0$。残差范数的平方为 $\\rho(\\alpha)^2 = \\alpha^2 \\sum_{i} \\frac{(u_i^{\\top} y^{\\delta})^2}{(\\sigma_i^2+\\alpha)^2}$。对 $\\alpha$ 求导：\n$$\n\\frac{d}{d\\alpha} \\rho(\\alpha)^2 = \\sum_i (u_i^{\\top} y^{\\delta})^2 \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} \\right)\n$$\n内部项的导数为：\n$$\n\\frac{d}{d\\alpha} \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} = \\frac{2\\alpha(\\sigma_i^2+\\alpha)^2 - \\alpha^2 \\cdot 2(\\sigma_i^2+\\alpha)(1)}{(\\sigma_i^2+\\alpha)^4} = \\frac{2\\alpha(\\sigma_i^2+\\alpha) - 2\\alpha^2}{(\\sigma_i^2+\\alpha)^3} = \\frac{2\\alpha\\sigma_i^2}{(\\sigma_i^2+\\alpha)^3}\n$$\n因此，残差范数平方的导数为：\n$$\n\\frac{d}{d\\alpha} \\rho(\\alpha)^2 = \\sum_i (u_i^{\\top} y^{\\delta})^2 \\frac{2\\alpha\\sigma_i^2}{(\\sigma_i^2+\\alpha)^3}\n$$\n对于 $\\alpha > 0$，此和中的每一项都是非负的。只要至少存在一个索引 $i$，使得 $\\sigma_i > 0$ 且数据分量 $u_i^{\\top} y^{\\delta} \\neq 0$，该导数就严格为正。在本问题中，$A$ 是对角矩阵，因此其奇异值为 $\\sigma_1=1$ 和 $\\sigma_2=10^{-2}$，奇异向量 $u_i$ 是标准基向量 $e_i$。数据分量为 $e_1^{\\top} y^{\\delta} = 1 \\neq 0$ 和 $e_2^{\\top} y^{\\delta} = 0.2 \\neq 0$。因此，对所有 $\\alpha > 0$，都有 $\\frac{d}{d\\alpha} \\rho(\\alpha)^2 > 0$。\n这证明了 $\\rho(\\alpha)^2$ 是关于 $\\alpha$ 的严格单调递增函数。由于 $\\rho(\\alpha)$ 是非负的，且平方根函数对非负参数是严格递增的，因此 $\\rho(\\alpha)$ 也是关于 $\\alpha$ 的严格单调递增函数。\n\n**第3部分：莫罗佐夫差异原理**\n\n莫罗佐夫差异原理要求从离散集 $\\mathcal{A}$ 中选择满足 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2} \\ge \\tau \\delta$ 的最小 $\\alpha$。\n给定 $\\delta = 0.135$ 和 $\\tau = 1.1$，目标差异为：\n$$\n\\tau \\delta = 1.1 \\times 0.135 = 0.1485\n$$\n我们对计算出的残差范数检查此条件：\n- $\\alpha = 10^{-6}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.00198  0.1485$。\n- $\\alpha = 10^{-4}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.10000005  0.1485$。\n- $\\alpha = 3 \\times 10^{-4}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1500003 \\ge 0.1485$。\n- $\\alpha = 10^{-3}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1818 \\ge 0.1485$。\n- $\\alpha = 10^{-2}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1983 \\ge 0.1485$。\n\n满足条件的参数为 $\\{3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}$。规则是选择其中最小的一个。\n因此，选定的正则化参数是 $\\alpha^{\\star} = 3 \\times 10^{-4}$。",
            "answer": "$$\\boxed{3 \\times 10^{-4}}$$"
        },
        {
            "introduction": "从手动计算转向编程实现，本练习要求您编写代码，实现两种最广泛使用的后验规则：L曲线法和Morozov差异原则（MDP）。通过在不同场景下比较这两种启发式方法所选择的参数，您将对它们的行为和各自的优点形成实践性的理解。这项练习是培养将正则化应用于实际问题所需技能的关键一步。",
            "id": "3361732",
            "problem": "给定一个具有预设奇异值结构和系数的合成小型线性反问题族，需要通过 Tikhonov 正则化进行处理。对于每种情况，您必须计算 L 曲线，估计 Tikhonov 正则化参数的 L 曲线拐角，并将其与通过 Morozov 偏差原理（Morozov Discrepancy Principle, MDP）获得的选择进行比较。\n\n所有问题都在以下纯数学设定中定义。考虑一个具有已知奇异值分解（SVD）基的线性系统，为简化起见，该基特化为单位阵，因此正算子是对角的。设正算子由一个对角矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 给出，其对角线元素等于预设的奇异值 $\\{\\sigma_i\\}_{i=1}^n$，并设真实未知向量在右奇异向量基中以系数 $\\{c_i\\}_{i=1}^n$ 表示。那么，干净数据由 $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$ 给出，其中 $x_{\\mathrm{true}}$ 在该基中的坐标为 $c_i$。定义一个确定性噪声方向 $d \\in \\mathbb{R}^n$ 和一个噪声水平 $\\delta > 0$。设 $b = b_{\\mathrm{clean}} + \\eta$，其中 $\\eta = \\delta \\, d / \\lVert d \\rVert_2$，因此 $\\lVert \\eta \\rVert_2 = \\delta$。\n\n对于带单位罚项的 Tikhonov 正则化，给定参数 $\\alpha > 0$ 的解 $x_\\alpha$ 最小化 $\\lVert A x - b \\rVert_2^2 + \\alpha^2 \\lVert x \\rVert_2^2$。在左右奇异向量矩阵均为单位矩阵的 SVD 基中，解可以使用数据坐标 $\\beta_i = b_i$ 表示为\n$$\nx_\\alpha = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} \\, \\beta_i \\, e_i,\n$$\n其中 $e_i$ 是第 $i$ 个标准基向量。残差范数和解范数可以通过以下方式计算\n$$\n\\lVert A x_\\alpha - b \\rVert_2^2 = \\sum_{i=1}^n \\frac{\\alpha^4 \\beta_i^2}{(\\sigma_i^2 + \\alpha^2)^2}, \\quad\n\\lVert x_\\alpha \\rVert_2^2 = \\sum_{i=1}^n \\left(\\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2}\\right)^2 \\beta_i^2.\n$$\n\nL 曲线是点 $\\left(\\log \\lVert A x_\\alpha - b \\rVert_2, \\log \\lVert x_\\alpha \\rVert_2\\right)$ 随 $\\alpha$ 变化的参数曲线。L 曲线的“拐角”可以通过最大化该平面参数曲线的曲率来估计。如果我们定义参数化 $t = \\log \\alpha$，$X(t) = \\log \\lVert A x_{e^t} - b \\rVert_2$ 和 $Y(t) = \\log \\lVert x_{e^t} \\rVert_2$，则在 $t$ 处的曲率为\n$$\n\\kappa(t) = \\frac{X'(t)Y''(t) - Y'(t)X''(t)}{\\left( \\left(X'(t)\\right)^2 + \\left(Y'(t)\\right)^2 \\right)^{3/2}}.\n$$\n实践中，您应该使用二阶中心有限差分，在 $t$ 值的离散网格上评估曲率，并选择具有最大绝对曲率的网格点上的参数 $\\alpha$。\n\nMorozov 偏差原理（MDP）选择正则化参数 $\\alpha$，使得残差范数与噪声水平相匹配，即\n$$\n\\lVert A x_\\alpha - b \\rVert_2 = \\delta.\n$$\n对于此处考虑的满列秩对角算子，残差范数是 $\\alpha \\in [0,\\infty)$ 的严格递增函数，其极限为 $\\alpha \\to 0^+$ 时为 $0$，$\\alpha \\to \\infty$ 时为 $\\lVert b \\rVert_2$。因此，对于任何 $\\delta \\in (0, \\lVert b \\rVert_2)$，都存在一个唯一的 $\\alpha$。\n\n您的任务是实现一个程序，对于下述每个测试用例，执行以下步骤：\n1. 使用指定的 $\\{\\sigma_i\\}$、$\\{c_i\\}$、噪声方向 $d$ 和噪声分数 $f$（其中 $\\delta = f \\, \\lVert b_{\\mathrm{clean}} \\rVert_2$）构造 $A$、$x_{\\mathrm{true}}$、$b_{\\mathrm{clean}}$ 和 $b = b_{\\mathrm{clean}} + \\eta$（其中 $\\lVert \\eta \\rVert_2 = \\delta$）。\n2. 在指定的 $\\alpha_{\\min}$ 和 $\\alpha_{\\max}$ 之间，在 $\\log_{10} \\alpha$ 上均匀间隔的 $\\alpha$ 值网格上计算 L 曲线，点数为指定的 $N_\\alpha$。使用关于 $t = \\log \\alpha$ 的离散二阶中心差分来近似导数，在内部点评估曲率，并选择与最大绝对曲率对应的 $\\alpha$ 作为 L 曲线拐角估计值 $\\alpha_{\\mathrm{LC}}$。\n3. 通过求解 $\\lVert A x_\\alpha - b \\rVert_2 = \\delta$ 来计算 MDP 选择的 $\\alpha_{\\mathrm{MDP}}$，使用一个鲁棒的区间求根方法。如有必要，以指数方式扩大区间，直到找到符号变化。选择最小的正根（在此处是唯一的）。\n4. 对于每个测试用例，生成一个由三个实数组成的元组 $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$，其中 $\\mathrm{rel\\_err} = \\lvert \\alpha_{\\mathrm{LC}} - \\alpha_{\\mathrm{MDP}} \\rvert / \\alpha_{\\mathrm{MDP}}$，以浮点数形式返回。\n\n重要的实现细节和约束：\n- 所有计算都是无单位的；不涉及物理单位。\n- 在计算范数的对数时，确保参数为严格正数。如果发生数值下溢，在取对数前将范数截断到一个正的下限（例如 $10^{-300}$），以避免未定义的值。\n- 仅使用所描述的数据和方法；不要假设任何外部提示或公式。\n- 程序必须是自包含的，并且不得读取输入或写入文件。\n\n测试套件：\n使用以下四个测试用例。对于每个用例，$n = 6$，噪声方向为 $d = [1,-1,1,-1,1,-1]^T$。\n- 案例 1（中等噪声，几何衰减）：\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 0.05$\n  - 网格：$\\alpha_{\\min} = 10^{-6}$, $\\alpha_{\\max} = 10^{1}$, $N_\\alpha = 400$\n- 案例 2（极小噪声，几何衰减）：\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 10^{-6}$\n  - 网格：$\\alpha_{\\min} = 10^{-8}$, $\\alpha_{\\max} = 10^{0}$, $N_\\alpha = 400$\n- 案例 3（大噪声，几何衰减）：\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 0.4$\n  - 网格：$\\alpha_{\\min} = 10^{-6}$, $\\alpha_{\\max} = 10^{2}$, $N_\\alpha = 600$\n- 案例 4（严重病态谱，中等噪声）：\n  - $\\sigma = [1, 10^{-2}, 10^{-4}, 10^{-6}, 10^{-8}, 10^{-10}]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 10^{-3}$\n  - 网格：$\\alpha_{\\min} = 10^{-10}$, $\\alpha_{\\max} = 10^{2}$, $N_\\alpha = 600$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含结果，格式为一个包含四个项目的类 JSON 列表，每个项目对应一个测试用例，其中每个项目是列表 $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$。例如：\n[[aL1,aM1,e1],[aL2,aM2,e2],[aL3,aM3,e3],[aL4,aM4,e4]]\n每个 $aLk$、$aMk$ 和 $ek$ 都必须是浮点数。不应打印额外的文本或行。",
            "solution": "所提出的问题是反问题数值分析中一个有效且适定的练习。它要求实现和比较两种标准的 *后验* 正则化参数选择准则——L 曲线准则和 Morozov 偏差原理——用于 Tikhonov 正则化。该问题具有科学依据、自包含，并在算法上进行了规定。我们将继续提供完整的解决方案。\n\n**1. 控制方程与问题表述**\n\n我们考虑在 $\\mathbb{R}^n$ 中由系统 $Ax = b$ 定义的线性反问题。正算子 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对角矩阵，其对角线元素对应一组奇异值 $\\{\\sigma_i\\}_{i=1}^n$，其中 $\\sigma_i > 0$。\n$$\nA = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\n$$\n真实的、未知的解向量 $x_{\\mathrm{true}}$ 由其在标准基中的系数 $\\{c_i\\}_{i=1}^n$ 指定，使得 $x_{\\mathrm{true}} = [c_1, c_2, \\dots, c_n]^T$。干净数据向量 $b_{\\mathrm{clean}}$ 则由下式给出：\n$$\nb_{\\mathrm{clean}} = A x_{\\mathrm{true}}, \\quad \\text{其分量为} \\quad (b_{\\mathrm{clean}})_i = \\sigma_i c_i\n$$\n观测数据 $b$ 是干净数据被加性噪声 $\\eta$ 扰动后的结果，即 $b = b_{\\mathrm{clean}} + \\eta$。噪声向量 $\\eta$ 被构造成具有特定的幅值 $\\delta > 0$ 和方向 $d \\in \\mathbb{R}^n$：\n$$\n\\eta = \\delta \\frac{d}{\\lVert d \\rVert_2}, \\quad \\text{因此} \\quad \\lVert \\eta \\rVert_2 = \\delta\n$$\n噪声水平 $\\delta$ 定义为干净数据范数的一个分数 $f$：\n$$\n\\delta = f \\cdot \\lVert b_{\\mathrm{clean}} \\rVert_2\n$$\n问题是从含噪数据 $b$ 中找到 $x_{\\mathrm{true}}$ 的一个稳定近似。\n\n**2. Tikhonov 正则化及相关范数**\n\nTikhonov 正则化旨在通过找到一个解 $x_\\alpha$ 来解决这个病态问题，该解最小化一个复合目标函数，从而在数据保真度与解的稳定性之间取得平衡。对于正则化参数 $\\alpha > 0$，解 $x_\\alpha$ 是以下函数的最小化子：\n$$\nJ_\\alpha(x) = \\lVert Ax - b \\rVert_2^2 + \\alpha^2 \\lVert x \\rVert_2^2\n$$\n由于 $A$ 是对角矩阵，问题解耦。正则化解的第 $i$ 个分量 $(x_\\alpha)_i$ 为：\n$$\n(x_\\alpha)_i = \\frac{\\sigma_i b_i}{\\sigma_i^2 + \\alpha^2}\n$$\n正则化解的质量通过残差范数 $\\lVert Ax_\\alpha - b \\rVert_2$ 和解范数 $\\lVert x_\\alpha \\rVert_2$ 来评估。它们的平方值，我们分别记为 $\\rho(\\alpha)$ 和 $\\xi(\\alpha)$，具有方便的闭式表达式：\n$$\n\\rho(\\alpha) = \\lVert Ax_\\alpha - b \\rVert_2^2 = \\sum_{i=1}^n \\left( \\sigma_i (x_\\alpha)_i - b_i \\right)^2 = \\sum_{i=1}^n \\frac{\\alpha^4 b_i^2}{(\\sigma_i^2 + \\alpha^2)^2}\n$$\n$$\n\\xi(\\alpha) = \\lVert x_\\alpha \\rVert_2^2 = \\sum_{i=1}^n \\left( (x_\\alpha)_i \\right)^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i b_i}{\\sigma_i^2 + \\alpha^2} \\right)^2\n$$\n核心挑战是为 $\\alpha$ 选择一个合适的值。\n\n**3. L 曲线参数选择准则**\n\nL 曲线是（对数）解范数对（对数）残差范数的参数图。按照规定，该曲线由 $\\alpha > 0$ 参数化：\n$$\n\\left( \\log \\lVert A x_\\alpha - b \\rVert_2, \\log \\lVert x_\\alpha \\rVert_2 \\right) = \\left( \\frac{1}{2}\\log \\rho(\\alpha), \\frac{1}{2}\\log \\xi(\\alpha) \\right)\n$$\n该曲线通常具有典型的“L”形。 “L”的拐角通常被认为代表了在最小化残差（数据保真度）和最小化解范数（正则性）之间的良好平衡。我们通过找到最大曲率点来确定这个拐角。\n\n为了计算曲率，我们使用 $t = \\log \\alpha$ 对曲线进行重新参数化。设 $X(t) = \\log \\lVert A x_{e^t} - b \\rVert_2$ 和 $Y(t) = \\log \\lVert x_{e^t} \\rVert_2$。曲率 $\\kappa(t)$ 由平面曲线的标准公式给出：\n$$\n\\kappa(t) = \\frac{X'(t)Y''(t) - Y'(t)X''(t)}{\\left( (X'(t))^2 + (Y'(t))^2 \\right)^{3/2}}\n$$\n我们通过执行以下步骤来计算 $\\alpha_{\\mathrm{LC}}$：\n1.  在 $\\alpha_{\\min}$ 和 $\\alpha_{\\max}$ 之间生成一个包含 $N_\\alpha$ 个 $\\alpha$ 值的对数间隔网格。这对应于 $t = \\log \\alpha$ 的一个均匀网格，间距为 $\\Delta t$。\n2.  对于网格上的每个 $\\alpha$，计算 $\\rho(\\alpha)$ 和 $\\xi(\\alpha)$，然后计算函数 $X(t)$ 和 $Y(t)$。为对数的范数参数使用 $10^{-300}$ 的下限，以防止数值错误。\n3.  使用二阶中心有限差分在网格的内部点近似一阶和二阶导数 $X'(t)$、$X''(t)$、$Y'(t)$ 和 $Y''(t)$：\n    $$\n    F'(t_j) \\approx \\frac{F(t_{j+1}) - F(t_{j-1})}{2 \\Delta t}, \\quad F''(t_j) \\approx \\frac{F(t_{j+1}) - 2F(t_j) + F(t_{j-1})}{(\\Delta t)^2}\n    $$\n4.  将这些近似导数代入曲率公式 $\\kappa(t)$。\n5.  正则化参数 $\\alpha_{\\mathrm{LC}}$ 选择为与最大绝对曲率 $|\\kappa(t)|$ 对应的 $\\alpha$ 网格值。\n\n**4. Morozov 偏差原理（MDP）**\n\nMorozov 偏差原理提供了另一种选择 $\\alpha$ 的规则。它基于这样一个前提：一个理想的正则化解应该只在噪声水平内再现数据。数学上，它选择满足以下条件的参数 $\\alpha$：\n$$\n\\lVert A x_\\alpha - b \\rVert_2 = \\delta \\quad \\text{或等价地} \\quad \\rho(\\alpha) = \\delta^2\n$$\n为了找到参数 $\\alpha_{\\mathrm{MDP}}$，我们必须求解非线性标量方程 $g(\\alpha) = 0$，其中 $g(\\alpha) = \\rho(\\alpha) - \\delta^2$。函数 $\\rho(\\alpha)$ 对于 $\\alpha > 0$ 是 $\\alpha$ 的严格单调递增函数，其范围从 $\\rho(0)=0$ 到 $\\rho(\\infty) = \\lVert b \\rVert_2^2$。因此，对于任何 $\\delta \\in (0, \\lVert b \\rVert_2)$，都存在一个唯一的正解 $\\alpha_{\\mathrm{MDP}}$。该解是使用一个鲁棒的区间求根算法（如 Brent-Dekker 方法）数值求解的。如果初始搜索区间没有包围根，则以指数方式扩大该区间，直到检测到 $g(\\alpha)$ 的符号变化。\n\n**5. 算法实现摘要**\n\n对于提供的每个测试用例，执行以下计算过程：\n1.  问题参数 $\\{\\sigma_i\\}$、$\\{c_i\\}$、$d$ 和 $f$ 用于构造向量 $x_{\\mathrm{true}}$、$b_{\\mathrm{clean}}$ 和 $b$，以及标量噪声水平 $\\delta$。\n2.  为找到 $\\alpha_{\\mathrm{LC}}$，在指定的 $\\alpha$ 对数网格上计算量 $\\rho(\\alpha)$ 和 $\\xi(\\alpha)$。使用有限差分在内部网格点计算导数和曲率，并选择与最大绝对曲率对应的 $\\alpha$。\n3.  为找到 $\\alpha_{\\mathrm{MDP}}$，定义函数 $g(\\alpha) = \\rho(\\alpha) - \\delta^2$。采用求根算法来定位该函数的唯一正根，从而得到 $\\alpha_{\\mathrm{MDP}}$。\n4.  通过计算相对误差 $\\mathrm{rel\\_err} = |\\alpha_{\\mathrm{LC}} - \\alpha_{\\mathrm{MDP}}| / \\alpha_{\\mathrm{MDP}}$ 来比较这两个参数估计值。\n5.  生成的三元组 $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$ 构成了该测试用例的输出。\n整个过程被封装在一个自包含的程序中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the regularization parameter selection problem for a series of test cases.\n    For each case, it calculates the L-curve corner alpha, the Morozov Discrepancy\n    Principle alpha, and their relative error.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 0.05,\n            \"grid\": (1e-6, 1e1, 400)\n        },\n        {\n            \"name\": \"Case 2\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 1e-6,\n            \"grid\": (1e-8, 1e0, 400)\n        },\n        {\n            \"name\": \"Case 3\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 0.4,\n            \"grid\": (1e-6, 1e2, 600)\n        },\n        {\n            \"name\": \"Case 4\",\n            \"sigma\": np.array([1, 1e-2, 1e-4, 1e-6, 1e-8, 1e-10]),\n            \"c\": np.ones(6),\n            \"f\": 1e-3,\n            \"grid\": (1e-10, 1e2, 600)\n        }\n    ]\n\n    results = []\n    \n    d = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0])\n    d_normalized = d / np.linalg.norm(d)\n    \n    # Positive floor for log calculation to avoid undefined values.\n    LOG_FLOOR = 1e-300\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        c = case[\"c\"]\n        f = case[\"f\"]\n        alpha_min, alpha_max, n_alpha = case[\"grid\"]\n\n        # 1. Construct b_clean, delta, and b\n        x_true = c\n        b_clean = sigma * x_true\n        norm_b_clean = np.linalg.norm(b_clean)\n        delta = f * norm_b_clean\n        eta = delta * d_normalized\n        b = b_clean + eta\n\n        # 2. Compute L-curve and estimate the corner alpha_LC\n        alphas = np.logspace(np.log10(alpha_min), np.log10(alpha_max), n_alpha)\n        \n        # Vectorized calculation of norms\n        sigma_sq = sigma**2\n        b_sq = b**2\n        alphas_sq = alphas[:, np.newaxis]**2\n        \n        denominators = sigma_sq + alphas_sq\n        \n        # Residual norm squared rho(alpha)\n        rho_terms = (alphas_sq**2 / denominators**2) * b_sq\n        rho_sq_vals = np.sum(rho_terms, axis=1)\n\n        # Solution norm squared xi(alpha)\n        xi_terms = (sigma_sq / denominators**2) * b_sq\n        xi_sq_vals = np.sum(xi_terms, axis=1)\n\n        # L-curve coordinates in log-log scale\n        X = 0.5 * np.log(np.maximum(rho_sq_vals, LOG_FLOOR)) # X = log(residual norm)\n        Y = 0.5 * np.log(np.maximum(xi_sq_vals, LOG_FLOOR))  # Y = log(solution norm)\n        \n        # Curvature calculation using central differences\n        t = np.log(alphas)\n        delta_t = t[1] - t[0]\n\n        # Derivatives for interior points (from index 1 to n_alpha-2)\n        Xp = (X[2:] - X[:-2]) / (2 * delta_t)\n        Yp = (Y[2:] - Y[:-2]) / (2 * delta_t)\n        Xpp = (X[2:] - 2 * X[1:-1] + X[:-2]) / (delta_t**2)\n        Ypp = (Y[2:] - 2 * Y[1:-1] + Y[:-2]) / (delta_t**2)\n\n        numerator = Xp * Ypp - Yp * Xpp\n        denominator = (Xp**2 + Yp**2)**1.5\n\n        kappa = np.zeros_like(numerator)\n        # Avoid division by zero for points where derivatives might be nil.\n        valid_denom = denominator > LOG_FLOOR\n        kappa[valid_denom] = numerator[valid_denom] / denominator[valid_denom]\n\n        # Find alpha corresponding to max absolute curvature\n        max_kappa_idx = np.argmax(np.abs(kappa))\n        # The index must be shifted by 1 to map back to the original alpha grid\n        alpha_lc = alphas[max_kappa_idx + 1]\n\n        # 3. Compute MDP selection alpha_MDP\n        def rho_sq_func(alpha):\n            alpha_sq = alpha**2\n            den = sigma_sq + alpha_sq\n            terms = (alpha_sq**2 / den**2) * b_sq\n            return np.sum(terms)\n            \n        def g(alpha):\n            return rho_sq_func(alpha) - delta**2\n            \n        # Find a bracket for the root-finding algorithm\n        a, b = alpha_min, alpha_max\n        try:\n            val_a = g(a)\n            val_b = g(b)\n            # Exponentially expand bracket until a sign change is found\n            while val_a * val_b > 0:\n                if val_b  0:\n                    b *= 10\n                    val_b = g(b)\n                elif val_a > 0:\n                    a /= 10\n                    val_a = g(a)\n                else:\n                    # This case should not be reached with monotonic rho\n                    break\n        except OverflowError:\n            # Handle cases where alpha becomes too large or small\n            # For this problem, the initial grid is sufficient.\n            pass\n\n        alpha_mdp = brentq(g, a, b, xtol=1e-15, rtol=1e-15)\n\n        # 4. Compute relative error and store results\n        rel_err = np.abs(alpha_lc - alpha_mdp) / alpha_mdp\n        results.append([alpha_lc, alpha_mdp, rel_err])\n\n    # Final print statement in the exact required format.\n    # Produces for example: [[aL1,aM1,e1],[aL2,aM2,e2],...]\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "虽然启发式规则功能强大，但并非万无一失。这个高级练习探讨了一个著名方法——广义交叉验证（GCV）——在严重不适定问题中的经典失效案例。您不仅需要通过将GCV的选择与理论上的“神谕”选择进行比较来诊断问题，还将实现一个加权GCV（wGCV）来修正这一失效，从而培养出一种更具批判性和鲁棒性的参数选择方法。",
            "id": "3361679",
            "problem": "考虑在奇异值坐标下由离散正演模型定义的带加性噪声的线性逆问题。设 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对角矩阵，其对角线元素 $s_i$ 以几何级数衰减，即 $s_i = \\rho^{i-1}$，其中 $i = 1,\\dots,n$，且 $\\rho \\in (0,1)$ 是一个固定值。设未知量为 $x_{\\star} \\in \\mathbb{R}^n$，其在 $A$ 的右奇异向量基中的分量为 $\\theta_i = (-1)^{i-1} i^{-2}$（也就是说，$x_{\\star}$ 在使 $A$ 对角化的基中表示）。观测值为 $y = A x_{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$ 是均值为零、方差已知的 $\\sigma^2$ 的高斯噪声。\n\n您将使用 Tikhonov 正则化和用于选择正则化参数的后验规则。设对于给定的参数 $\\alpha > 0$，Tikhonov 估计由平衡数据失配和正则化的变分问题的解定义，并用 $x_{\\alpha}$ 表示相应的线性估计器。在奇异值分解基中，该估计器通过依赖于 $\\alpha$ 和奇异值 $s_i$ 的滤波因子逐分量作用。\n\n您的任务如下：\n\n1. 从 $A$ 的奇异值分解和 Tikhonov 正则化解的法方程出发，推导出 $x_{\\alpha}$ 关于滤波因子的分量表示，并由此推导出期望均方误差（风险）$\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$ 作为 $\\alpha$、$s_i$、$\\theta_i$ 和 $\\sigma^2$ 的函数。仅使用 Tikhonov 正则化的标准定义和高斯噪声下线性估计器的性质作为您的基本依据。\n2. 将神谕参数 $\\alpha_{\\mathrm{oracle}}$ 定义为期望均方误差 $\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$ 在合适的 $\\alpha$ 值搜索域上的最小化子。这个神谕参数在实践中是不可获取的，但它将作为基准。\n3. 从线性平滑器的留一法预测损失定义出发，推导线性估计器的广义交叉验证（GCV）目标函数，并获得仅使用 $y$ 和与 Tikhonov 正则化相关的线性平滑器的标准 GCV 规则来选择 $\\alpha$。展示如何从残差和平滑器的有效自由度计算 GCV 目标函数。\n4. 通过选择一个具有小 $\\rho$ 值的几何衰减谱，构建一个严重不适定性下的后验反例，并数值上证明标准广义交叉验证（GCV）选择的 $\\alpha$ 相对于 $\\alpha_{\\mathrm{oracle}}$ 过小，即 $|\\log_{10}(\\alpha_{\\mathrm{GCV}}/\\alpha_{\\mathrm{oracle}})|$ 很大，且在 $\\alpha_{\\mathrm{GCV}}$ 处的期望均方误差大于在 $\\alpha_{\\mathrm{oracle}}$ 处的期望均方误差。\n5. 提出一个加权广义交叉验证（GCV）规则，在左奇异向量基中对残差引入非负谱权重 $w_i$。从加权预测损失和平滑器的线性性质出发，推导依赖于权重 $w_i$、残差和加权有效自由度的相应加权 GCV 目标函数。然后，设置 $w_i = s_i^2$ 并解释为何此选择能缓解标准 GCV 在严重不适定性下的失效模式。实现加权 GCV 规则并计算最小化此目标的 $\\alpha_{\\mathrm{wGCV}}$。\n6. 对下面的每个测试用例，计算三元组 $(\\alpha_{\\mathrm{GCV}}, \\alpha_{\\mathrm{wGCV}}, \\alpha_{\\mathrm{oracle}})$ 和相应的期望风险，然后记录一个整数结果，定义如下：如果以下两个条件都成立，则记录为 $1$：\n   (a) $|\\log_{10}(\\alpha_{\\mathrm{wGCV}}/\\alpha_{\\mathrm{oracle}})|  |\\log_{10}(\\alpha_{\\mathrm{GCV}}/\\alpha_{\\mathrm{oracle}})|$，\n   以及\n   (b) 在 $\\alpha_{\\mathrm{wGCV}}$ 处的期望均方误差小于或等于在 $\\alpha_{\\mathrm{GCV}}$ 处的期望均方误差；否则记录为 $0$。\n\n实现要求和细节：\n\n- 模型规范。使用 $U = I$ 和 $V = I$，因此 $A = \\mathrm{diag}(s_1,\\dots,s_n)$，且数据空间与奇异向量基重合。使用分量为 $\\theta_i = (-1)^{i-1} i^{-2}$ 的 $x_{\\star}$，并生成具有独立分量且服从 $\\mathcal{N}(0,\\sigma^2)$ 分布的噪声 $\\varepsilon$。相应地构造 $y = A x_{\\star} + \\varepsilon$。\n- Tikhonov 估计器。使用由法方程所蕴含的滤波因子，在奇异值坐标中实现该估计器。\n- 神谕参数。通过在一个覆盖广泛范围的对数间隔 $\\alpha$ 值网格上最小化为 $\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$ 解析推导的表达式，来计算 $\\alpha_{\\mathrm{oracle}}$。您的程序必须根据模型参数精确计算此期望值，无需进行蒙特卡洛平均。\n- GCV 和加权 GCV。使用 Tikhonov 平滑器的残差和有效自由度实现标准广义交叉验证（GCV）选择器，以及使用在奇异向量基中计算的谱权重 $w_i = s_i^2$ 实现加权广义交叉验证（wGCV）选择器。两种选择器都应在用于神谕参数的同一搜索网格上最小化各自的目标函数。\n- 严重不适定性反例。使用几何衰减的奇异值 $s_i = \\rho^{i-1}$ 来展示一个严重情况（小 $\\rho$），在该情况下，与神谕参数相比，标准 GCV 选择的 $\\alpha$ 过小，并产生更大的期望风险。\n\n测试套件：\n\n定义并解决以下四个测试用例。对于每个用例，使用指定的 $(n,\\rho,\\sigma,\\text{seed})$ 来构造 $A$、$x_{\\star}$ 和 $y$ 的一个实现：\n- 用例 1 (严重): $n=80$, $\\rho=0.30$, $\\sigma=10^{-3}$, 种子 $=10$。\n- 用例 2 (严重但噪声较低): $n=80$, $\\rho=0.30$, $\\sigma=5 \\times 10^{-4}$, 种子 $=11$。\n- 用例 3 (较温和): $n=80$, $\\rho=0.80$, $\\sigma=10^{-3}$, 种子 $=12$。\n- 用例 4 (小维度，非常严重): $n=20$, $\\rho=0.20$, $\\sigma=10^{-3}$, 种子 $=13$。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的整数列表，按测试用例的顺序排列，其中每个整数是按上述定义为 $1$ 或 $0$（例如，$\\left[1,0,1,1\\right]$）。不应打印任何其他文本。此问题不涉及角度，也不需要任何物理单位。所有数值必须使用标准实数算术计算。",
            "solution": "在尝试解决方案之前，对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **正演模型**: 带加性噪声的线性逆问题，$y = A x_{\\star} + \\varepsilon$。\n- **正演算子**: $A$ 是一个 $n \\times n$ 的实对角矩阵。\n- **奇异值**: $A$ 的对角线元素为 $s_i = \\rho^{i-1}$，其中 $i = 1, \\dots, n$，且 $\\rho \\in (0,1)$ 是一个固定值。\n- **奇异向量**: $A$ 的奇异值分解（SVD）取 $U=I$ 和 $V=I$，因此 $A = \\mathrm{diag}(s_1, \\dots, s_n)$。问题在奇异向量基中设定。\n- **真实未知量**: $x_{\\star} \\in \\mathbb{R}^n$，其在 SVD 基中的分量为 $\\theta_i = (-1)^{i-1} i^{-2}$。\n- **噪声**: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，即均值为零、方差已知的 $\\sigma^2$ 的高斯噪声。\n- **正则化**: 使用 Tikhonov 正则化为参数 $\\alpha > 0$ 定义一个估计器 $x_{\\alpha}$。\n- **任务**:\n    1. 推导 Tikhonov 解 $x_{\\alpha}$ 的分量形式及其期望均方误差（风险）$\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$。\n    2. 将神谕参数 $\\alpha_{\\mathrm{oracle}}$ 定义为此风险的最小化子。\n    3. 推导标准广义交叉验证（GCV）目标函数。\n    4. 数值上展示 GCV 在严重不适定性下的一个失效案例。\n    5. 推导一个权重为 $w_i = s_i^2$ 的加权 GCV (wGCV) 规则，并解释其优点。\n    6. 对四个指定的测试用例，计算 $(\\alpha_{\\mathrm{GCV}}, \\alpha_{\\mathrm{wGCV}}, \\alpha_{\\mathrm{oracle}})$ 及其相应的风险，并根据 wGCV 是否比 GCV 更接近神谕参数且产生更低或相等的风险来确定一个二元结果。\n- **测试用例**:\n    - 用例 1: $n=80$, $\\rho=0.30$, $\\sigma=10^{-3}$, 种子 $=10$。\n    - 用例 2: $n=80$, $\\rho=0.30$, $\\sigma=5 \\times 10^{-4}$, 种子 $=11$。\n    - 用例 3: $n=80$, $\\rho=0.80$, $\\sigma=10^{-3}$, 种子 $=12$。\n    - 用例 4: $n=20$, $\\rho=0.20$, $\\sigma=10^{-3}$, 种子 $=13$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**: 问题设定在逆问题和正则化理论的背景下，这是应用数学和统计学的一个成熟分支。所使用的模型（Tikhonov 正则化，GCV）是标准的。所有概念都基于线性代数和概率论的基本原理。\n- **适定性**: 作为一个任务，该问题是适定的。它要求进行推导和数值计算，从而为每个测试用例得出唯一、明确定义的结果。\n- **客观性**: 问题以精确的数学语言陈述，没有歧义或主观性。\n- **自洽性与一致性**: 提供了所有必要信息（$A$、$x_{\\star}$、噪声模型、估计器和选择规则的定义）。没有内部矛盾。\n- **现实性与可行性**: 该设置是研究不适定问题的典型模型。数值参数虽然用于说明严重的不适定性，但在数学上和计算上都是可行的。\n- **非平凡性**: 该问题需要进行不平凡的推导和仔细的数值实现，以比较不同的参数选择方法，这是逆问题领域的核心挑战。\n\n### 步骤 3：结论与行动\n问题有效。这是数值分析和逆问题理论中一个定义明确的标准问题。有必要提供完整的解决方案。\n\n---\n\n### 1. Tikhonov 估计器与期望均方误差（风险）\n\n对于问题 $y \\approx Ax$，Tikhonov 正则化解 $x_{\\alpha}$ 是以下泛函的最小化子：\n$$\nJ_{\\alpha}(x) = \\|Ax - y\\|_2^2 + \\alpha \\|x\\|_2^2\n$$\n其中 $\\alpha > 0$ 是正则化参数。将梯度 $\\nabla_x J_{\\alpha}(x)$ 置为零，得到法方程：\n$$\n(A^T A + \\alpha I) x_{\\alpha} = A^T y\n$$\n已知 $A$ 是一个对角矩阵 $A = S = \\mathrm{diag}(s_1, \\dots, s_n)$，其转置为 $A^T=A=S$。方程简化为：\n$$\n(S^2 + \\alpha I) x_{\\alpha} = S y\n$$\n由于 $S^2 + \\alpha I$ 是一个对角矩阵，其元素为 $s_i^2 + \\alpha$，解 $x_{\\alpha}$ 可以逐分量求得：\n$$\n(s_i^2 + \\alpha) (x_{\\alpha})_i = s_i y_i \\implies (x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha} y_i\n$$\n数据 $y$ 由 $y = Ax_{\\star} + \\varepsilon$ 给出，其分量形式为 $y_i = s_i \\theta_i + \\varepsilon_i$，其中 $\\theta_i = (x_{\\star})_i$，$\\varepsilon_i$ 是噪声向量 $\\varepsilon$ 的分量。将此代入 $(x_{\\alpha})_i$ 的表达式中：\n$$\n(x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha} (s_i \\theta_i + \\varepsilon_i) = \\frac{s_i^2}{s_i^2 + \\alpha} \\theta_i + \\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i\n$$\n项 $f_i(\\alpha) = \\frac{s_i^2}{s_i^2 + \\alpha}$ 是 Tikhonov 滤波器的滤波因子。第 $i$ 个分量的估计误差为：\n$$\n(x_{\\alpha})_i - \\theta_i = \\left(\\frac{s_i^2}{s_i^2 + \\alpha} - 1\\right)\\theta_i + \\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i = \\frac{-\\alpha}{s_i^2 + \\alpha} \\theta_i + \\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i\n$$\n期望均方误差（风险）为 $\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|_2^2 = \\sum_{i=1}^n \\mathbb{E}[((x_{\\alpha})_i - \\theta_i)^2]$。由于 $\\mathbb{E}[\\varepsilon_i] = 0$ 和 $\\mathbb{E}[\\varepsilon_i \\varepsilon_j] = \\sigma^2 \\delta_{ij}$，交叉项的期望为零。\n$$\n\\mathbb{E}[((x_{\\alpha})_i - \\theta_i)^2] = \\mathbb{E}\\left[\\left(\\frac{-\\alpha}{s_i^2 + \\alpha} \\theta_i\\right)^2 + 2(\\dots)\\varepsilon_i + \\left(\\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i\\right)^2\\right] = \\left(\\frac{\\alpha}{s_i^2 + \\alpha}\\right)^2 \\theta_i^2 + \\left(\\frac{s_i}{s_i^2 + \\alpha}\\right)^2 \\mathbb{E}[\\varepsilon_i^2]\n$$\n当 $\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2$ 时，风险是偏差平方和方差之和：\n$$\nR(\\alpha) = \\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|_2^2 = \\sum_{i=1}^n \\underbrace{\\left(\\frac{\\alpha \\theta_i}{s_i^2 + \\alpha}\\right)^2}_{\\text{偏差平方}} + \\sum_{i=1}^n \\underbrace{\\frac{s_i^2 \\sigma^2}{(s_i^2 + \\alpha)^2}}_{\\text{方差}}\n$$\n\n### 2. 神谕正则化参数\n\n神谕参数 $\\alpha_{\\mathrm{oracle}}$ 是使真实风险 $R(\\alpha)$ 最小化的 $\\alpha$ 值。其定义为：\n$$\n\\alpha_{\\mathrm{oracle}} = \\arg\\min_{\\alpha > 0} R(\\alpha)\n$$\n在实践中此值是不可获取的，因为 $R(\\alpha)$ 依赖于未知的真实解 $x_{\\star}$（通过其分量 $\\theta_i$）。然而，它可作为任何实用参数选择规则的理论基准。\n\n### 3. 广义交叉验证 (GCV)\n\nGCV 是一种数据驱动的选择 $\\alpha$ 的方法。它源自留一法交叉验证（LOOCV）。正则化数据估计为 $\\hat{y}_{\\alpha} = Ax_{\\alpha} = A(S^2+\\alpha I)^{-1}S y = H(\\alpha)y$，其中 $H(\\alpha) = S(S^2+\\alpha I)^{-1}S$ 是影响矩阵或平滑矩阵。在这个对角情况下，$H(\\alpha) = \\mathrm{diag}(h_1(\\alpha), \\dots, h_n(\\alpha))$，其中 $h_i(\\alpha) = \\frac{s_i^2}{s_i^2+\\alpha}$。\n\nLOOCV 分数是平方预测误差的平均值，其中每个点 $y_i$ 都是使用排除该点的数据拟合的模型来预测的。对于线性平滑器，该分数有一个方便的闭合形式：\n$$\nL(\\alpha) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\alpha,i}}{1 - h_{ii}(\\alpha)} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - h_i(\\alpha)y_i}{1 - h_i(\\alpha)} \\right)^2\n$$\nGCV 通过将分母中各个对角元素 $h_i(\\alpha)$ 替换为其平均值 $\\frac{1}{n}\\mathrm{Tr}(H(\\alpha))$ 来近似 LOOCV。需要最小化的 GCV 函数是：\n$$\nV_{GCV}(\\alpha) = \\frac{\\frac{1}{n}\\|y - \\hat{y}_{\\alpha}\\|_2^2}{\\left(1 - \\frac{1}{n}\\mathrm{Tr}(H(\\alpha))\\right)^2}\n$$\n最小化 $V_{GCV}(\\alpha)$ 等价于最小化更简单的泛函 $G(\\alpha)$：\n$$\nG(\\alpha) = \\frac{\\|y - \\hat{y}_{\\alpha}\\|_2^2}{(\\mathrm{Tr}(I - H(\\alpha)))^2}\n$$\n分子是残差向量 $r(\\alpha) = y - \\hat{y}_{\\alpha} = (I - H(\\alpha))y$ 的平方范数。项 $\\mathrm{Tr}(H(\\alpha)) = \\sum_{i=1}^n h_i(\\alpha)$ 是平滑器的有效自由度。在我们的分量设定中：\n$$\nG(\\alpha) = \\frac{\\sum_{i=1}^n \\left( (1-h_i(\\alpha))y_i \\right)^2}{\\left( \\sum_{i=1}^n (1-h_i(\\alpha)) \\right)^2} = \\frac{\\sum_{i=1}^n \\left( \\frac{\\alpha}{s_i^2+\\alpha} y_i \\right)^2}{\\left( \\sum_{i=1}^n \\frac{\\alpha}{s_i^2+\\alpha} \\right)^2}\n$$\nGCV 参数选择为 $\\alpha_{\\mathrm{GCV}} = \\arg\\min_{\\alpha > 0} G(\\alpha)$。\n\n### 4. GCV 在严重不适定性下的失效\n\n对于严重不适定问题（例如，小 $\\rho$，其中奇异值 $s_i$ 衰减非常快），GCV 函数 $G(\\alpha)$ 可能在一个非常小的 $\\alpha$ 处出现误导性的伪最小值。这个 $\\alpha_{\\mathrm{GCV}}$ 通常远小于 $\\alpha_{\\mathrm{oracle}}$，导致正则化不足、噪声大的解，且风险很高。数值实验将展示这一现象。\n\n### 5. 加权广义交叉验证 (wGCV)\n\n为了缓解 GCV 的失效，可以引入一个加权版本。加权 LOOCV 分数将是 $L_w(\\alpha) = \\frac{1}{n} \\sum_{i} w_i \\left( \\frac{r_i(\\alpha)}{1 - h_i(\\alpha)} \\right)^2$。应用 GCV 的近似逻辑可得到 wGCV 泛函 $G_w(\\alpha)$：\n$$\nG_w(\\alpha) = \\frac{\\sum_{i=1}^n w_i (r_i(\\alpha))^2}{\\left(\\sum_{i=1}^n w_i (1-h_i(\\alpha))\\right)^2} = \\frac{\\sum_{i=1}^n w_i \\left( \\frac{\\alpha}{s_i^2+\\alpha} y_i \\right)^2}{\\left( \\sum_{i=1}^n w_i \\frac{\\alpha}{s_i^2+\\alpha} \\right)^2}\n$$\n问题指定权重为 $w_i = s_i^2$。我们来分析这个选择。当 $\\alpha \\to 0$ 时，$1 - h_i(\\alpha) = \\frac{\\alpha}{s_i^2+\\alpha} \\approx \\frac{\\alpha}{s_i^2}$。该泛函的行为如下：\n$$\nG_w(\\alpha) \\approx \\frac{\\sum_{i=1}^n s_i^2 \\left( \\frac{\\alpha}{s_i^2} y_i \\right)^2}{\\left( \\sum_{i=1}^n s_i^2 \\frac{\\alpha}{s_i^2} \\right)^2} = \\frac{\\alpha^2 \\sum_{i=1}^n (y_i/s_i)^2}{\\left( \\sum_{i=1}^n \\alpha \\right)^2} = \\frac{\\sum_{i=1}^n (y_i/s_i)^2}{n^2}\n$$\n项 $\\sum (y_i/s_i)^2 = \\sum (\\theta_i + \\varepsilon_i/s_i)^2$ 由 $\\sum (\\varepsilon_i/s_i)^2$ 主导，对于小的 $s_i$，该项很大。因此，当 $\\alpha \\to 0$ 时，$G_w(\\alpha)$ 趋向于一个大值，实际上起到了惩罚作用，防止最小化子过分接近于零。这通过降低对应于小奇异值（这些分量受噪声污染最严重）的数据分量的影响，纠正了标准 GCV 的失效模式。参数选择则为 $\\alpha_{\\mathrm{wGCV}} = \\arg\\min_{\\alpha > 0} G_w(\\alpha)$。\n\n### 6. 数值计算与比较\n\n对于每个测试用例，实施以下步骤：\n1.  使用给定的参数 $n, \\rho, \\sigma, \\text{seed}$，构造奇异值 $s_i$、真实解分量 $\\theta_i$ 以及数据 $y_i$ 的一个实现。\n2.  定义一个宽的对数网格，包含候选的 $\\alpha$ 值。\n3.  对于网格上的每个 $\\alpha$，评估风险 $R(\\alpha)$、GCV 泛函 $G(\\alpha)$ 和 wGCV 泛函 $G_w(\\alpha)$。\n4.  通过定位这些函数的最小值来找到 $\\alpha_{\\mathrm{oracle}}$、$\\alpha_{\\mathrm{GCV}}$ 和 $\\alpha_{\\mathrm{wGCV}}$。\n5.  计算期望风险 $R(\\alpha_{\\mathrm{GCV}})$ 和 $R(\\alpha_{\\mathrm{wGCV}})$。\n6.  检查两个条件：(a) wGCV 在对数尺度上是否更接近神谕参数：$|\\log_{10}(\\alpha_{\\mathrm{wGCV}}/\\alpha_{\\mathrm{oracle}})|  |\\log_{10}(\\alpha_{\\mathrm{GCV}}/\\alpha_{\\mathrm{oracle}})|$？(b) wGCV 是否产生更好或相等的风险：$R(\\alpha_{\\mathrm{wGCV}}) \\le R(\\alpha_{\\mathrm{GCV}})$？\n7.  如果两个条件都为真，则结果为 $1$；否则为 $0$。对所有四个测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(n, rho, sigma, seed):\n    \"\"\"\n    Solves a single test case for comparing GCV, wGCV, and Oracle parameter selection.\n\n    Args:\n        n (int): Dimension of the problem.\n        rho (float): Decay rate for singular values.\n        sigma (float): Standard deviation of the Gaussian noise.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        int: 1 if wGCV is superior to GCV by the specified criteria, 0 otherwise.\n    \"\"\"\n    # 1. Model and data setup\n    # Construct singular values s_i = rho^(i-1)\n    i = np.arange(n, dtype=np.float64)\n    s = rho**i\n\n    # Construct true solution components theta_i = (-1)^(i-1) * i^-2\n    # The problem has i=1,...,n, so we use i+1 for 0-indexed arrays.\n    theta = ((-1.0)**i) * ((i + 1.0)**(-2.0))\n\n    # Generate data y = A*x_star + epsilon\n    rng = np.random.default_rng(seed)\n    epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = s * theta + epsilon\n\n    # 2. Define search grid for alpha\n    alphas = np.logspace(-20, 5, 2000)\n\n    # 3. Vectorized evaluation of risk and GCV functions\n    s2 = s**2\n\n    # Use broadcasting to compute for all alphas at once\n    # Shape of s2a will be (n, n_alphas)\n    s2a = s2[:, np.newaxis] + alphas[np.newaxis, :]\n\n    # Expected Mean Squared Error (Risk) R(alpha)\n    # The risk is the sum of squared bias and variance.\n    risk_bias_sq = (alphas[np.newaxis, :]**2 * theta[:, np.newaxis]**2) / (s2a**2)\n    risk_variance = (s2[:, np.newaxis] * sigma**2) / (s2a**2)\n    risk_values = np.sum(risk_bias_sq + risk_variance, axis=0)\n\n    # Standard Generalized Cross-Validation (GCV)\n    # h_i are the diagonal elements of the influence matrix H\n    h = s2[:, np.newaxis] / s2a\n    one_minus_h = 1.0 - h\n    # Residual r = (I-H)y\n    r = one_minus_h * y[:, np.newaxis]\n    gcv_numerator = np.sum(r**2, axis=0)\n    gcv_denominator = np.sum(one_minus_h, axis=0)\n    # Add a small epsilon to denominator to avoid division by zero far from minimum\n    gcv_values = gcv_numerator / (gcv_denominator**2 + 1e-30)\n\n    # Weighted Generalized Cross-Validation (wGCV) with w_i = s_i^2\n    w = s2\n    wgcv_numerator = np.sum(w[:, np.newaxis] * r**2, axis=0)\n    wgcv_denominator = np.sum(w[:, np.newaxis] * one_minus_h, axis=0)\n    wgcv_values = wgcv_numerator / (wgcv_denominator**2 + 1e-30)\n\n    # 4. Find the optimal alpha values\n    alpha_oracle = alphas[np.argmin(risk_values)]\n    alpha_gcv = alphas[np.argmin(gcv_values)]\n    alpha_wgcv = alphas[np.argmin(wgcv_values)]\n\n    # 5. Compute risks at the selected alpha values\n    def calculate_risk(alpha, s_vec, theta_vec, sigma_val):\n        s2_vec = s_vec**2\n        s2a_vec = s2_vec + alpha\n        risk_b_sq = (alpha**2 * theta_vec**2) / (s2a_vec**2)\n        risk_var = (s2_vec * sigma_val**2) / (s2a_vec**2)\n        return np.sum(risk_b_sq + risk_var)\n\n    risk_at_gcv = calculate_risk(alpha_gcv, s, theta, sigma)\n    risk_at_wgcv = calculate_risk(alpha_wgcv, s, theta, sigma)\n\n    # 6. Check the two conditions for the final result\n    # Condition (a): wGCV is closer to oracle in log-scale\n    log_dist_gcv = np.abs(np.log10(alpha_gcv / alpha_oracle))\n    log_dist_wgcv = np.abs(np.log10(alpha_wgcv / alpha_oracle))\n    cond_a = log_dist_wgcv  log_dist_gcv\n\n    # Condition (b): wGCV yields lower or equal risk\n    cond_b = risk_at_wgcv = risk_at_gcv\n\n    result = 1 if cond_a and cond_b else 0\n    return result\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test cases from the problem statement\n    test_cases = [\n        # (n, rho, sigma, seed)\n        (80, 0.30, 1e-3, 10),            # Case 1 (severe)\n        (80, 0.30, 5e-4, 11),            # Case 2 (severe but lower noise)\n        (80, 0.80, 1e-3, 12),            # Case 3 (milder)\n        (20, 0.20, 1e-3, 13),            # Case 4 (small dimension, very severe)\n    ]\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, seed = case\n        result = solve_case(n, rho, sigma, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}