{
    "hands_on_practices": [
        {
            "introduction": "This first practice extends the standard Tikhonov framework to the more general case involving a penalty operator $L$. By performing a clever change of variables, you will see how this seemingly complex problem can be transformed back into a standard form, a process conceptually equivalent to \"whitening\" a statistical prior. This exercise  is crucial for understanding how prior information is incorporated in many data assimilation and inverse problem settings, interpreting the singular values of the transformed operator as measures of \"prior-whitened observability.\"",
            "id": "3419945",
            "problem": "Consider a linear inverse problem in a data assimilation setting, where the forward operator is a matrix $A \\in \\mathbb{R}^{m \\times n}$, the unknown state is $x \\in \\mathbb{R}^{n}$, and the data are $b \\in \\mathbb{R}^{m}$. Assume a generalized Tikhonov-regularized least-squares estimator with a positive scalar regularization level $\\lambda  0$ and an invertible regularization operator $L \\in \\mathbb{R}^{n \\times n}$, defined by the variational criterion\n$$\nJ(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda^{2}\\,\\|L x\\|_{2}^{2}.\n$$\nStarting only from the definitions of the criterion $J(x)$, the change of variables, and standard properties of the singular value decomposition (SVD), do the following:\n\n1. Show that the variable transformation $x = L^{-1} z$ reduces the generalized Tikhonov criterion to a standard-form Tikhonov criterion in the transformed variable $z$, with the effective operator $A L^{-1}$, namely\n$$\nJ(L^{-1} z) \\;=\\; \\|A L^{-1} z - b\\|_{2}^{2} \\;+\\; \\lambda^{2}\\,\\|z\\|_{2}^{2}.\n$$\n\n2. Let $B := A L^{-1}$. Derive the normal equations for the minimizer $z_{\\lambda}$ of\n$$\n\\|B z - b\\|_{2}^{2} \\;+\\; \\lambda^{2}\\,\\|z\\|_{2}^{2},\n$$\nand, using the singular value decomposition (SVD) of $B$, expressed as $B = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ having orthonormal columns, $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{r})$, and $r = \\operatorname{rank}(B)$, derive an explicit expression for $z_{\\lambda}$ in the SVD coordinates.\n\n3. Express $z_{\\lambda}$ by comparing it to the unregularized least-squares solution in the range of $B$, and identify the multiplicative attenuation applied to each spectral component. Define the associated filter factors for Tikhonov regularization as the scalars that multiply the least-squares spectral components in the subspace spanned by the right singular vectors of $B$. Derive their analytic form in terms of the singular values $\\sigma_{i}$ and the regularization level $\\lambda$.\n\n4. Interpret these filter factors in terms of prior-whitened observability: explain how the transformation by $L^{-1}$ whitens the prior and how the singular values of $A L^{-1}$, together with the filter factors, quantify the degree to which directions in the state space are informed by the data relative to the strength of the prior.\n\nProvide the final answer as the single closed-form analytic expression for the filter factors as a function of $\\sigma_{i}$ and $\\lambda$. No numerical evaluation is required.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of inverse problem theory and linear algebra, well-posed, objective, and self-contained. We will proceed with the solution by following the four specified parts.\n\nThe generalized Tikhonov criterion is given by\n$$\nJ(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2}\\|L x\\|_{2}^{2}\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, $b \\in \\mathbb{R}^{m}$, $L \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix, and $\\lambda  0$ is the regularization level.\n\n**1. Variable Transformation**\n\nWe perform the change of variables $x = L^{-1} z$, where $z \\in \\mathbb{R}^{n}$. Since $L$ is invertible, this transformation is a bijection between the space of $x$ and the space of $z$. We substitute this into the criterion $J(x)$:\n$$\nJ(L^{-1} z) = \\|A (L^{-1} z) - b\\|_{2}^{2} + \\lambda^{2}\\|L (L^{-1} z)\\|_{2}^{2}\n$$\nUsing the property of matrix multiplication, $A(L^{-1}z) = (AL^{-1})z$. In the second term, $L(L^{-1}z) = (LL^{-1})z = I_n z = z$, where $I_n$ is the $n \\times n$ identity matrix.\nThe criterion becomes:\n$$\nJ(L^{-1} z) = \\|A L^{-1} z - b\\|_{2}^{2} + \\lambda^{2}\\|z\\|_{2}^{2}\n$$\nThis is the standard-form Tikhonov criterion for the transformed variable $z$ with the effective operator $B = A L^{-1}$, as was to be shown.\n\n**2. Normal Equations and SVD-based Solution**\n\nLet $\\mathcal{J}(z) = \\|B z - b\\|_{2}^{2} + \\lambda^{2}\\|z\\|_{2}^{2}$, where $B = A L^{-1}$. To find the minimizer $z_{\\lambda}$, we must find where the gradient of $\\mathcal{J}(z)$ with respect to $z$ is zero. We first expand the squared norms:\n$$\n\\mathcal{J}(z) = (B z - b)^{\\top}(B z - b) + \\lambda^{2} z^{\\top}z = z^{\\top}B^{\\top}Bz - 2 b^{\\top}B z + b^{\\top}b + \\lambda^{2} z^{\\top}z\n$$\nThe gradient with respect to $z$ is:\n$$\n\\nabla_{z} \\mathcal{J}(z) = 2 B^{\\top}B z - 2 B^{\\top}b + 2 \\lambda^{2} z\n$$\nSetting the gradient to zero, $\\nabla_{z} \\mathcal{J}(z) = 0$, gives the normal equations:\n$$\n(B^{\\top}B + \\lambda^{2} I) z_{\\lambda} = B^{\\top}b\n$$\nThe matrix $(B^{\\top}B + \\lambda^{2} I)$ is positive definite for $\\lambda  0$ and thus invertible, guaranteeing a unique solution $z_{\\lambda}$.\n\nNow, we use the singular value decomposition (SVD) of $B = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns ($U^{\\top}U = I_r$, $V^{\\top}V = I_r$), $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{r})$ contains the positive singular values $\\sigma_i  0$, and $r = \\operatorname{rank}(B)$.\nWe substitute the SVD into the normal equations. First, we compute the terms $B^{\\top}B$ and $B^{\\top}b$:\n$$\nB^{\\top}B = (U \\Sigma V^{\\top})^{\\top}(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V \\Sigma^2 V^{\\top}\n$$\n$$\nB^{\\top}b = (U \\Sigma V^{\\top})^{\\top}b = V \\Sigma U^{\\top}b\n$$\nThe normal equations become:\n$$\n(V \\Sigma^2 V^{\\top} + \\lambda^{2} I) z_{\\lambda} = V \\Sigma U^{\\top}b\n$$\nThe solution $z_{\\lambda}$ lies in the range of $B^{\\top}$, which is the column space of $V$. Any component orthogonal to this space is mapped to zero by $B^{\\top}B$ and would be zeroed out by the regularization term. We can write $z_{\\lambda} = V \\alpha$ for some vector of coefficients $\\alpha \\in \\mathbb{R}^{r}$. Substituting this into the equation:\n$$\n(V \\Sigma^2 V^{\\top} + \\lambda^{2} I) V \\alpha = V \\Sigma U^{\\top}b\n$$\nUsing $V^{\\top}V = I_r$, we get $V \\Sigma^2 V^{\\top} V \\alpha = V \\Sigma^2 \\alpha$. The term $\\lambda^{2} I V \\alpha = \\lambda^2 V \\alpha$. So,\n$$\nV \\Sigma^2 \\alpha + \\lambda^2 V \\alpha = V \\Sigma U^{\\top}b\n$$\n$$\nV (\\Sigma^2 + \\lambda^2 I_r) \\alpha = V \\Sigma U^{\\top}b\n$$\nLeft-multiplying by $V^{\\top}$ and using $V^{\\top}V = I_r$:\n$$\n(\\Sigma^2 + \\lambda^2 I_r) \\alpha = \\Sigma U^{\\top}b\n$$\nSince $\\Sigma^2 + \\lambda^2 I_r$ is a diagonal matrix with positive diagonal entries $\\sigma_i^2 + \\lambda^2$, it is invertible. We can solve for $\\alpha$:\n$$\n\\alpha = (\\Sigma^2 + \\lambda^2 I_r)^{-1} \\Sigma U^{\\top}b\n$$\nThe solution $z_{\\lambda}$ is then $z_{\\lambda} = V \\alpha = V (\\Sigma^2 + \\lambda^2 I_r)^{-1} \\Sigma U^{\\top}b$.\nIn SVD coordinates (i.e., as a sum over singular components), this is:\n$$\nz_{\\lambda} = \\sum_{i=1}^{r} V_i \\alpha_i = \\sum_{i=1}^{r} V_i \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} (U_i^{\\top}b)\n$$\nwhere $U_i$ and $V_i$ are the $i$-th columns of $U$ and $V$, respectively, and $\\alpha_i$ is the $i$-th component of $\\alpha$.\n\n**3. Filter Factors**\n\nThe unregularized linear least-squares solution for the problem $\\min_{z} \\|Bz - b\\|_2^2$ is given by $z_{LS} = B^{\\dagger} b$, where $B^{\\dagger}$ is the Moore-Penrose pseudoinverse of $B$. Using the SVD, $B^{\\dagger} = V \\Sigma^{-1} U^{\\top}$. The solution is thus:\n$$\nz_{LS} = V \\Sigma^{-1} U^{\\top} b = \\sum_{i=1}^{r} V_i \\frac{1}{\\sigma_i} (U_i^{\\top}b)\n$$\nThis solution is restricted to the range of $B^{\\top}$ (the span of the columns of $V$). The term $\\frac{U_i^{\\top}b}{\\sigma_i}$ is the coefficient of the $i$-th basis vector $V_i$, which the problem refers to as the \"$i$-th spectral component\" of the least-squares solution.\n\nNow we compare the regularized solution $z_{\\lambda}$ to the least-squares solution $z_{LS}$. We can rewrite the expression for $z_{\\lambda}$:\n$$\nz_{\\lambda} = \\sum_{i=1}^{r} V_i \\left( \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} \\right) \\left( \\frac{1}{\\sigma_i} (U_i^{\\top}b) \\right)\n$$\nThis shows that the $i$-th spectral component of the regularized solution is obtained by multiplying the $i$-th spectral component of the least-squares solution, $\\frac{U_i^{\\top}b}{\\sigma_i}$, by a multiplicative scalar factor. These scalars are the Tikhonov filter factors, denoted by $f_i$:\n$$\nf_i(\\sigma_i, \\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}\n$$\n\n**4. Interpretation of Filter Factors**\n\nThe generalized Tikhonov criterion can be interpreted from a Bayesian perspective. The term $\\|A x - b\\|_2^2$ corresponds to the negative log-likelihood of the data assuming Gaussian noise, and the term $\\lambda^2 \\|L x\\|_2^2$ corresponds to a negative log-prior on the state $x$, implying a Gaussian prior with mean 0 and covariance matrix proportional to $(L^{\\top}L)^{-1}$.\n\nThe transformation $z = L x$ (and its inverse $x = L^{-1}z$) effectively \"whitens\" the prior. In the transformed coordinates $z$, the prior term becomes $\\lambda^2 \\|z\\|_2^2$, which corresponds to a Gaussian prior with a spherical covariance matrix $(\\lambda^{-2}I)$. The components of $z$ are a priori uncorrelated and have the same variance.\n\nThe SVD of the transformed forward operator, $B = AL^{-1}$, provides the \"observability\" of the system in these prior-whitened coordinates. The singular values $\\sigma_i$ quantify how much a perturbation along the direction of the right singular vector $V_i$ (in the $z$-space) is amplified by the forward operator $B$. A large $\\sigma_i$ means the direction $V_i$ is highly observable in the data, while a small $\\sigma_i$ means it is poorly observable.\n\nThe filter factor $f_i = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} = \\frac{1}{1 + (\\lambda/\\sigma_i)^2}$ clarifies the trade-off between data and prior information.\n- If a mode $i$ is highly observable ($\\sigma_i \\gg \\lambda$), then $f_i \\approx 1$. The regularized solution for this component is nearly identical to the least-squares solution, meaning the estimate is driven by the data.\n- If a mode $i$ is poorly observable ($\\sigma_i \\ll \\lambda$), then $f_i \\approx \\sigma_i^2 / \\lambda^2 \\approx 0$. The regularized solution for this component is strongly attenuated towards its prior mean of $0$. The estimate is determined by the prior, effectively filtering out the influence of data which is unreliable for this mode.\n\nTherefore, the singular values $\\sigma_i$ of the prior-whitened operator $AL^{-1}$ quantify the degree to which each mode is informed by the data, while the filter factors implement a weighting scheme that trusts the data for well-observed modes and relies on the prior for poorly-observed ones, with the regularization parameter $\\lambda$ setting the threshold for this trade-off.\n\nThe final answer required is the analytical expression for the filter factors.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}}\n$$"
        },
        {
            "introduction": "Regularization can be seen as a form of filtering in the spectral domain, but not all filters are created equal. This exercise  invites you to compare the smooth, \"soft\" filtering of Tikhonov regularization against the abrupt, \"hard\" filtering of the Truncated Singular Value Decomposition (TSVD). By quantifying the difference between these two fundamental approaches, you will develop a sharper intuition for how regularization modifies the solution and appreciate the nuanced control offered by Tikhonov's filter factors.",
            "id": "3419958",
            "problem": "Consider a linear inverse problem with a matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $p \\leq \\min\\{m,n\\}$, with the singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$, where the nonzero singular values satisfy $\\,\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{p}  0\\,$. For a data vector $b \\in \\mathbb{R}^{m}$, define the zero-order Tikhonov-regularized solution $x_{\\alpha}$ as the unique minimizer of\n$$\n\\|A x - b\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2},\n$$\nwith regularization parameter $\\alpha  0$. The truncated singular value decomposition (TSVD) solution of rank $r$ (with $1 \\leq r  p$) is the solution obtained by retaining only the first $r$ singular components in the SVD-based representation.\n\nStarting only from the normal equations for zero-order Tikhonov regularization and the SVD structure, derive the scalar filter factor $\\,\\phi_{i}(\\alpha)\\,$ acting on each SVD component of the least-squares coefficients, so that the Tikhonov solution can be written in the form\n$$\nx_{\\alpha} = \\sum_{i=1}^{p} \\phi_{i}(\\alpha)\\,\\frac{u_{i}^{\\top} b}{\\sigma_{i}}\\,v_{i}.\n$$\nLikewise, write the TSVD solution of rank $r$ in the same form by specifying the corresponding filter factors $\\,\\chi_{i}\\,$.\n\nDefine the operator that maps the least-squares coefficients $\\,c_{i} = (u_{i}^{\\top} b)/\\sigma_{i}\\,$ to the filtered coefficients as a diagonal operator with entries given by the filter factors. Using the spectral norm, quantify the approximation error between Tikhonov and TSVD filters by\n$$\nE(\\alpha) \\equiv \\left\\| \\operatorname{diag}\\big(\\phi_{1}(\\alpha),\\dots,\\phi_{p}(\\alpha)\\big) \\;-\\; \\operatorname{diag}(\\chi_{1},\\dots,\\chi_{p}) \\right\\|_{2}.\n$$\nAssuming only that $\\,\\sigma_{1} \\geq \\cdots \\geq \\sigma_{p}  0\\,$ and $\\,1 \\leq r  p\\,$, determine the value $\\,\\alpha^{\\star}  0\\,$ that minimizes $\\,E(\\alpha)\\,$ and the corresponding minimal error $\\,E(\\alpha^{\\star})\\,$. Express your final answer as two closed-form expressions in a single row matrix, with the first entry equal to $\\,\\alpha^{\\star}\\,$ and the second entry equal to $\\,E(\\alpha^{\\star})\\,$. No numerical approximation is required, and no units are needed in the final answer.",
            "solution": "We begin from the definition of the zero-order Tikhonov-regularized solution $x_{\\alpha}$ as the unique minimizer of\n$$\n\\|A x - b\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2}, \\quad \\alpha  0,\n$$\nwhich satisfies the normal equations\n$\n(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b.\n$\nLet the singular value decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, with nonzero singular values $\\sigma_{1} \\geq \\cdots \\geq \\sigma_{p}  0$. Let $U = [u_{1},\\dots,u_{m}]$ and $V = [v_{1},\\dots,v_{n}]$ be orthogonal matrices, and $\\Sigma$ be the $m \\times n$ diagonal matrix with diagonal entries $\\sigma_{1},\\dots,\\sigma_{p}$.\n\nExpress $x_{\\alpha}$ in the right singular vector basis by setting $y_{\\alpha} = V^{\\top} x_{\\alpha}$. Then the normal equations become\n$$\n(V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} + \\alpha I) x_{\\alpha} = V \\Sigma^{\\top} U^{\\top} b,\n$$\nwhich simplifies using orthogonality to\n$$\n(V (\\Sigma^{\\top} \\Sigma + \\alpha I) V^{\\top}) x_{\\alpha} = V \\Sigma^{\\top} U^{\\top} b.\n$$\nMultiplying on the left by $V^{\\top}$ yields\n$$\n(\\Sigma^{\\top} \\Sigma + \\alpha I) y_{\\alpha} = \\Sigma^{\\top} U^{\\top} b.\n$$\nSince $\\Sigma^{\\top} \\Sigma$ is diagonal with entries $\\sigma_{i}^{2}$ on the first $p$ diagonal positions, this decouples componentwise:\n$$\n(\\sigma_{i}^{2} + \\alpha)\\, y_{\\alpha,i} = \\sigma_{i}\\, (u_{i}^{\\top} b), \\quad i = 1,\\dots,p,\n$$\nand $y_{\\alpha,i} = 0$ for $i  p$ (corresponding to zero singular values if any). Therefore,\n$$\ny_{\\alpha,i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\alpha}\\, (u_{i}^{\\top} b), \\quad i=1,\\dots,p,\n$$\nand\n$$\nx_{\\alpha} = \\sum_{i=1}^{p} y_{\\alpha,i}\\, v_{i} = \\sum_{i=1}^{p} \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\alpha}\\, (u_{i}^{\\top} b)\\, v_{i}.\n$$\nIntroduce the least-squares coefficients $c_{i} = (u_{i}^{\\top} b)/\\sigma_{i}$. Then\n$$\nx_{\\alpha} = \\sum_{i=1}^{p} \\left( \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\alpha} \\right) \\frac{u_{i}^{\\top} b}{\\sigma_{i}}\\, v_{i} \\;=\\; \\sum_{i=1}^{p} \\phi_{i}(\\alpha)\\, c_{i}\\, v_{i},\n$$\nwhich identifies the Tikhonov filter factors as\n$$\n\\phi_{i}(\\alpha) = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\alpha}, \\quad i=1,\\dots,p.\n$$\n\nFor the truncated singular value decomposition (TSVD) of rank $r$ with $1 \\leq r  p$, the solution that retains the first $r$ singular components is\n$$\nx_{r} = \\sum_{i=1}^{r} \\frac{u_{i}^{\\top} b}{\\sigma_{i}}\\, v_{i} \\;=\\; \\sum_{i=1}^{p} \\chi_{i}\\, c_{i}\\, v_{i},\n$$\nwith the TSVD filter factors\n$$\n\\chi_{i} = \\begin{cases}\n1,  i \\le r \\\\\n0,  i > r\n\\end{cases}\n$$\n\nWe compare these two filters as diagonal operators acting on the least-squares coefficients $c_{i}$. Define the diagonal operators\n$$\nD_{\\alpha} = \\operatorname{diag}(\\phi_{1}(\\alpha),\\dots,\\phi_{p}(\\alpha)), \\qquad D_{r} = \\operatorname{diag}(\\chi_{1},\\dots,\\chi_{p}).\n$$\nBy definition, the spectral norm difference is\n$$\nE(\\alpha) = \\| D_{\\alpha} - D_{r} \\|_{2}.\n$$\nSince $D_{\\alpha} - D_{r}$ is diagonal, its spectral norm equals the largest absolute value among its diagonal entries:\n$$\nE(\\alpha) = \\max_{1 \\leq i \\leq p} |\\phi_{i}(\\alpha) - \\chi_{i}|.\n$$\nBecause $\\phi_{i}(\\alpha) = \\sigma_{i}^{2}/(\\sigma_{i}^{2}+\\alpha)$ is strictly increasing in $\\sigma_{i}$ for fixed $\\alpha  0$ (its derivative with respect to $\\sigma_{i}$ is $2 \\alpha \\sigma_{i}/(\\sigma_{i}^{2}+\\alpha)^{2}  0$ for $\\sigma_{i}  0$), we have:\n- For indices $i \\leq r$, the quantities $1 - \\phi_{i}(\\alpha)$ are maximized at the smallest $\\sigma_{i}$ among $i \\leq r$, namely at $i = r$.\n- For indices $i  r$, the quantities $\\phi_{i}(\\alpha)$ are maximized at the largest $\\sigma_{i}$ among $i  r$, namely at $i = r+1$.\n\nTherefore,\n$$\nE(\\alpha) = \\max\\!\\left\\{\\, 1 - \\phi_{r}(\\alpha),\\; \\phi_{r+1}(\\alpha) \\,\\right\\} \\;=\\; \\max\\!\\left\\{\\, \\frac{\\alpha}{\\sigma_{r}^{2} + \\alpha},\\; \\frac{\\sigma_{r+1}^{2}}{\\sigma_{r+1}^{2} + \\alpha} \\,\\right\\}.\n$$\n\nTo minimize $E(\\alpha)$ over $\\alpha  0$ in the minimax sense, we equalize the two competing terms:\n$$\n\\frac{\\alpha}{\\sigma_{r}^{2} + \\alpha} \\;=\\; \\frac{\\sigma_{r+1}^{2}}{\\sigma_{r+1}^{2} + \\alpha}.\n$$\nCross-multiplying yields\n$$\n\\alpha(\\sigma_{r+1}^{2} + \\alpha) \\;=\\; \\sigma_{r+1}^{2}(\\sigma_{r}^{2} + \\alpha)\n\\;\\;\\Longrightarrow\\;\\; \\alpha \\sigma_{r+1}^{2} + \\alpha^{2} \\;=\\; \\sigma_{r+1}^{2}\\sigma_{r}^{2} + \\sigma_{r+1}^{2}\\alpha.\n$$\nCancelling the common term $\\alpha \\sigma_{r+1}^{2}$ from both sides gives\n$$\n\\alpha^{2} \\;=\\; \\sigma_{r}^{2}\\sigma_{r+1}^{2},\n$$\nand since $\\alpha  0$ we obtain the unique minimizer\n$$\n\\alpha^{\\star} \\;=\\; \\sigma_{r}\\,\\sigma_{r+1}.\n$$\nSubstituting $\\alpha^{\\star}$ into either branch gives the minimal error:\n$$\nE(\\alpha^{\\star}) \\;=\\; \\frac{\\alpha^{\\star}}{\\sigma_{r}^{2} + \\alpha^{\\star}}\n\\;=\\; \\frac{\\sigma_{r}\\sigma_{r+1}}{\\sigma_{r}^{2} + \\sigma_{r}\\sigma_{r+1}}\n\\;=\\; \\frac{\\sigma_{r+1}}{\\sigma_{r} + \\sigma_{r+1}}.\n$$\n\nThus, the optimal regularization parameter that makes the Tikhonov filter mimic the truncated singular value decomposition step at rank $r$ in the spectral norm sense is the geometric mean of the adjacent singular values, and the corresponding operator norm approximation error equals the ratio of the smaller adjacent singular value to their sum.\n\nThe requested final answer is the row matrix containing $\\,\\alpha^{\\star}\\,$ and $\\,E(\\alpha^{\\star})\\,$ in that order.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\sigma_{r}\\sigma_{r+1}  \\dfrac{\\sigma_{r+1}}{\\sigma_{r}+\\sigma_{r+1}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "A theoretical understanding of filter factors is essential, but a practitioner must also be able to select an appropriate regularization parameter $\\alpha$ from data. This hands-on problem  guides you through the process of implementing the Generalized Cross-Validation (GCV) method, a powerful and widely-used technique for parameter choice. You will translate the theoretical SVD-based expressions for the solution and residual into a robust numerical algorithm, bridging the gap between abstract principles and practical problem-solving.",
            "id": "3419911",
            "problem": "Consider the linear discrete inverse problem of estimating an unknown state vector $x \\in \\mathbb{R}^n$ from noisy observations $b \\in \\mathbb{R}^m$ related by a known forward operator $A \\in \\mathbb{R}^{m \\times n}$ through the model $b = A x + \\varepsilon$, where $\\varepsilon$ represents additive measurement noise. We focus on zero-order Tikhonov regularization, in which $x$ is estimated by minimizing the Tikhonov functional $\\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ for a regularization parameter $\\alpha  0$. The estimation must be expressed using the Singular Value Decomposition (SVD), and the interpretation must be given in terms of filter factors. The quality of fit should be assessed using Generalized Cross-Validation (GCV) (Generalized Cross-Validation (GCV) penalizes influence by the linear prediction operator).\n\nStarting from the following fundamental base:\n- The discrete forward model $b = A x + \\varepsilon$ with $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$.\n- The zero-order Tikhonov regularized solution $\\hat{x}_\\alpha$ solves $\\min_{x} \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ for $\\alpha  0$.\n- The Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with positive singular values $\\sigma_i$, $V \\in \\mathbb{R}^{n \\times r}$, and $r = \\mathrm{rank}(A) \\le \\min(m,n)$.\n- The influence matrix (also called the hat matrix) is $H_\\alpha \\in \\mathbb{R}^{m \\times m}$ such that the predicted data is $\\hat{b}_\\alpha = A \\hat{x}_\\alpha = H_\\alpha b$.\n\nYour task is to:\n1. Derive, from first principles, the SVD-based form of the Tikhonov solution $\\hat{x}_\\alpha$ and identify the corresponding filter factors in terms of the singular values $\\sigma_i$.\n2. Derive the influence matrix $H_\\alpha$ and express its trace in terms of the singular values $\\sigma_i$ and the regularization parameter $\\alpha$.\n3. Derive the Generalized Cross-Validation (GCV) functional $G(\\alpha)$ for the zero-order Tikhonov estimator, solely in terms of quantities computed from the SVD of $A$ and the data vector $b$, without forming any large dense matrix beyond what is necessary for the SVD. The GCV functional must be derived as a function that depends on the residual norm $\\|A \\hat{x}_\\alpha - b\\|_2$ and $\\mathrm{trace}(H_\\alpha)$.\n4. Implement a numerical algorithm that:\n   - Computes the SVD of $A$.\n   - Evaluates $G(\\alpha)$ over a logarithmically spaced grid of $\\alpha \\in [10^{-8}, 10^{2}]$ using $200$ points, then refines by searching a new logarithmic grid of $200$ points centered around the minimizing $\\alpha$ within a multiplicative factor of $10$ (respecting the original bounds).\n   - Returns, for each test case, the minimizing regularization parameter $\\alpha^\\star$, the squared residual norm $\\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2$, and $\\mathrm{trace}(H_{\\alpha^\\star})$; all values must be computed in double precision.\n5. Use radians for any trigonometric functions.\n\nTest Suite:\n- Case $1$ (happy path, mildly ill-conditioned, overdetermined): Let $m = 50$, $n = 30$. Define the matrix $A \\in \\mathbb{R}^{50 \\times 30}$ by\n  $$A_{ij} = \\frac{1}{1 + |i - j|} + 10^{-3} \\cdot \\sin\\left(\\frac{i + j}{10}\\right), \\quad 1 \\le i \\le 50, \\; 1 \\le j \\le 30,$$\n  where the sine argument is in radians. Define the true state $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ by\n  $$x_{\\mathrm{true},j} = \\sin\\left(\\frac{j}{4}\\right), \\quad 1 \\le j \\le 30,$$\n  with sine in radians. Define the data $b \\in \\mathbb{R}^{50}$ by\n  $$b_i = \\sum_{j=1}^{30} A_{ij} x_{\\mathrm{true},j} + 10^{-3} \\cdot (-1)^i, \\quad 1 \\le i \\le 50.$$\n- Case $2$ (ill-posed blur, square system, stronger noise): Let $m = n = 40$. Define the matrix $A \\in \\mathbb{R}^{40 \\times 40}$ by\n  $$A_{ij} = \\exp\\left( - \\frac{(i - j)^2}{2 \\cdot 25} \\right), \\quad 1 \\le i,j \\le 40.$$\n  Define the true state $x_{\\mathrm{true}} \\in \\mathbb{R}^{40}$ by\n  $$x_{\\mathrm{true},j} = \\cos\\left(\\frac{j}{8}\\right), \\quad 1 \\le j \\le 40,$$\n  with cosine in radians. Define the data $b \\in \\mathbb{R}^{40}$ by\n  $$b_i = \\sum_{j=1}^{40} A_{ij} x_{\\mathrm{true},j} + 10^{-2} \\cdot \\sin\\left(\\frac{i}{3}\\right), \\quad 1 \\le i \\le 40,$$\n  with sine in radians.\n- Case $3$ (edge case, no information in $A$): Let $m = 20$, $n = 10$. Define $A$ by\n  $$A_{ij} = 0, \\quad 1 \\le i \\le 20, \\; 1 \\le j \\le 10,$$\n  and define $b \\in \\mathbb{R}^{20}$ by\n  $$b_i = (-1)^i, \\quad 1 \\le i \\le 20.$$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for all three test cases as a comma-separated list of lists enclosed in square brackets. For each test case, output the triple $[\\alpha^\\star, \\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2, \\mathrm{trace}(H_{\\alpha^\\star})]$. Each floating-point number must be formatted with exactly $6$ digits after the decimal point.\n- For example, the output structure must be of the form\n  $$\\left[ [\\alpha^\\star_1, r^2_1, t_1], [\\alpha^\\star_2, r^2_2, t_2], [\\alpha^\\star_3, r^2_3, t_3] \\right],$$\n  printed as a single line: \n  $$[[\\alpha^\\star_1,r^2_1,t_1],[\\alpha^\\star_2,r^2_2,t_2],[\\alpha^\\star_3,r^2_3,t_3]].$$\nAll trigonometric function arguments are in radians, and no physical units are involved. Each returned value is a float.",
            "solution": "The problem is critically validated and deemed valid. It is a well-posed, scientifically grounded problem in the field of inverse problems, with all necessary information provided and no contradictions apparent.\n\nWe are tasked with finding the zero-order Tikhonov regularized solution $\\hat{x}_\\alpha$ to the linear inverse problem $b = Ax + \\varepsilon$. The solution $\\hat{x}_\\alpha$ is the minimizer of the Tikhonov functional:\n$$\nJ(x) = \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$, and $\\alpha  0$ is the regularization parameter. The solution must be expressed using the Singular Value Decomposition (SVD) of $A$, and the optimal $\\alpha$ is to be found by minimizing the Generalized Cross-Validation (GCV) functional.\n\n**1. SVD-based Tikhonov Solution and Filter Factors**\n\nThe minimizer $\\hat{x}_\\alpha$ of $J(x)$ satisfies the condition that the gradient $\\nabla_x J(x)$ is zero. Calculating the gradient yields:\n$$\n\\nabla_x J(x) = \\nabla_x ( (Ax-b)^\\top(Ax-b) + \\alpha^2 x^\\top x ) = 2 A^\\top (Ax - b) + 2 \\alpha^2 x\n$$\nSetting the gradient to zero gives the normal equations for the Tikhonov problem:\n$$\n(A^\\top A + \\alpha^2 I_n) \\hat{x}_\\alpha = A^\\top b\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix.\n\nLet the rank of $A$ be $r \\le \\min(m, n)$. The SVD of $A$ is given by $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns ($u_1, \\ldots, u_r$), $V \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns ($v_1, \\ldots, v_r$), and $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix with positive singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_r  0$.\n\nUsing the SVD, we can express the terms in the normal equations:\n$$\nA^\\top A = (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = V \\Sigma^\\top U^\\top U \\Sigma V^\\top = V \\Sigma^2 V^\\top\n$$\n$$\nA^\\top b = (U \\Sigma V^\\top)^\\top b = V \\Sigma U^\\top b\n$$\nSubstituting these into the normal equations gives:\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) \\hat{x}_\\alpha = V \\Sigma U^\\top b\n$$\nThe solution $\\hat{x}_\\alpha$ must lie in the span of the columns of $V$, which is the orthogonal complement of the null space of $A$. Any component of $x$ in the null space of $A$ would increase the penalty term $\\alpha^2 \\|x\\|_2^2$ without decreasing the residual term $\\|A x - b\\|_2^2$. Thus, we can write the solution as a linear combination of the basis vectors $v_i$: $\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = Vc$ for some coefficient vector $c \\in \\mathbb{R}^r$. Substituting this into the equation and using $V^\\top V = I_r$:\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V \\Sigma U^\\top b\n$$\nLeft-multiplying by $V^\\top$:\n$$\nV^\\top(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V^\\top(V \\Sigma U^\\top b)\n$$\n$$\n(\\Sigma^2 + \\alpha^2 I_r) c = \\Sigma U^\\top b\n$$\nSince $\\sigma_i  0$ and $\\alpha  0$, the matrix $(\\Sigma^2 + \\alpha^2 I_r)$ is diagonal and invertible. We can solve for $c$:\n$$\nc = (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\nThe Tikhonov solution is then $\\hat{x}_\\alpha = Vc$:\n$$\n\\hat{x}_\\alpha = V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\nIn summation form, the $i$-th component of $c$ is $c_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b)$. The solution becomes:\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = \\sum_{i=1}^r \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) v_i\n$$\nThe standard (unregularized) pseudoinverse solution is $x^\\dagger = A^\\dagger b = \\sum_{i=1}^r \\frac{1}{\\sigma_i} (u_i^\\top b) v_i$. We can express $\\hat{x}_\\alpha$ in terms of the components of $x^\\dagger$:\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}\\right) \\frac{u_i^\\top b}{\\sigma_i} v_i\n$$\nThe terms $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}$ are the **filter factors**. They filter the spectral components of the solution, damping those associated with small singular values, which stabilizes the solution against noise.\n\n**2. Influence Matrix and its Trace**\n\nThe predicted data is $\\hat{b}_\\alpha = A \\hat{x}_\\alpha$. The influence matrix (or hat matrix) $H_\\alpha$ relates the original data $b$ to the predicted data $\\hat{b}_\\alpha$ via $\\hat{b}_\\alpha = H_\\alpha b$. Substituting the SVD expressions for $A$ and $\\hat{x}_\\alpha$:\n$$\n\\hat{b}_\\alpha = (U \\Sigma V^\\top) \\left( V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b \\right)\n$$\nUsing $V^\\top V = I_r$, this simplifies to:\n$$\n\\hat{b}_\\alpha = U \\Sigma (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top b\n$$\nFrom this, we identify the influence matrix:\n$$\nH_\\alpha = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top\n$$\nThe trace of the influence matrix is found using the linearity of the trace operator and the property $\\mathrm{trace}(uv^\\top) = v^\\top u$:\n$$\n\\mathrm{trace}(H_\\alpha) = \\mathrm{trace} \\left(\\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top \\right) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} \\mathrm{trace}(u_i u_i^\\top)\n$$\nSince $u_i^\\top u_i = 1$, we have $\\mathrm{trace}(u_i u_i^\\top) = 1$. Therefore:\n$$\n\\mathrm{trace}(H_\\alpha) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} = \\sum_{i=1}^r f_i(\\alpha)\n$$\nThis quantity is often interpreted as the effective degrees of freedom of the regularized model.\n\n**3. Generalized Cross-Validation (GCV) Functional**\n\nThe GCV functional provides a means to estimate the optimal $\\alpha$ by minimizing:\n$$\nG(\\alpha) = \\frac{m \\|A \\hat{x}_\\alpha - b\\|_2^2}{(m - \\mathrm{trace}(H_\\alpha))^2}\n$$\nWe need to express the numerator, the squared norm of the residual $r_\\alpha = A\\hat{x}_\\alpha - b$, in terms of SVD components. The residual can be written as $r_\\alpha = \\hat{b}_\\alpha - b = (H_\\alpha - I_m)b$.\nLet's decompose the data vector $b$ into its projection onto the column space of $A$ (which is the span of the columns of $U$) and its orthogonal complement:\n$$\nb = UU^\\top b + (I_m - UU^\\top)b = \\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\n$$\nApplying $(H_\\alpha - I_m)$ to $b$:\n$$\nr_\\alpha = \\left( \\sum_{i=1}^r f_i(\\alpha) u_i u_i^\\top \\right) b - b = \\sum_{i=1}^r f_i(\\alpha) (u_i^\\top b) u_i - \\left(\\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\\right)\n$$\n$$\nr_\\alpha = \\sum_{i=1}^r (f_i(\\alpha) - 1) (u_i^\\top b) u_i - b_{ortho}\n$$\nSubstituting $f_i(\\alpha) - 1 = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} - 1 = \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2}$:\n$$\nr_\\alpha = \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i - b_{ortho}\n$$\nThe terms in the summation are orthogonal to $b_{ortho}$. By the Pythagorean theorem, the squared norm is:\n$$\n\\|r_\\alpha\\|_2^2 = \\left\\| \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i \\right\\|_2^2 + \\|b_{ortho}\\|_2^2\n$$\nSince the vectors $u_i$ are orthonormal, this becomes:\n$$\n\\|A \\hat{x}_\\alpha - b\\|_2^2 = \\sum_{i=1}^r \\left( \\frac{\\alpha^2}{\\sigma_i^2 + \\alpha^2} \\right)^2 (u_i^\\top b)^2 + \\|(I_m - UU^\\top) b\\|_2^2\n$$\nThe second term, $\\|b_{ortho}\\|_2^2$, is the squared norm of the component of $b$ that is orthogonal to the column space of $A$. It can be computed efficiently as $\\|b\\|_2^2 - \\|UU^\\top b\\|_2^2 = \\|b\\|_2^2 - \\sum_{i=1}^r(u_i^\\top b)^2$.\n\nAll components of the GCV functional $G(\\alpha)$ can now be computed using only the singular values $\\sigma_i$, the projected data components $u_i^\\top b$, the total size $m$, and the norm of $b$. This avoids the explicit formation of large matrices like $H_\\alpha$ or $A^\\top A$. The numerical implementation will follow this SVD-based formulation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Tikhonov regularization problem for all test cases.\n    \"\"\"\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the A matrices and b vectors for the three test cases.\n        \"\"\"\n        # Case 1\n        m1, n1 = 50, 30\n        A1 = np.zeros((m1, n1), dtype=np.float64)\n        i_idx, j_idx = np.ogrid[1:m1+1, 1:n1+1]\n        A1 = 1 / (1 + np.abs(i_idx - j_idx)) + 1e-3 * np.sin((i_idx + j_idx) / 10)\n        \n        j_vec1 = np.arange(1, n1 + 1)\n        xtrue1 = np.sin(j_vec1 / 4)\n        \n        i_vec1 = np.arange(1, m1 + 1)\n        noise1 = 1e-3 * ((-1)**i_vec1)\n        b1 = A1 @ xtrue1 + noise1\n\n        # Case 2\n        m2, n2 = 40, 40\n        i_idx2, j_idx2 = np.ogrid[1:m2+1, 1:n2+1]\n        A2 = np.exp(-((i_idx2 - j_idx2)**2) / (2 * 25))\n        \n        j_vec2 = np.arange(1, n2 + 1)\n        xtrue2 = np.cos(j_vec2 / 8)\n        \n        i_vec2 = np.arange(1, m2 + 1)\n        noise2 = 1e-2 * np.sin(i_vec2 / 3)\n        b2 = A2 @ xtrue2 + noise2\n        \n        # Case 3\n        m3, n3 = 20, 10\n        A3 = np.zeros((m3, n3), dtype=np.float64)\n        \n        i_vec3 = np.arange(1, m3 + 1)\n        b3 = (-1)**i_vec3\n        \n        return [(A1, b1), (A2, b2), (A3, b3)]\n\n    def compute_gcv_outputs(A, b):\n        \"\"\"\n        Computes the optimal alpha and corresponding outputs using GCV.\n        \"\"\"\n        m, n = A.shape\n        \n        # Step 1: Compute economy SVD\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        \n        # Step 2: Pre-compute SVD-based quantities\n        r = len(s)\n        btilde = U.T @ b if r > 0 else np.array([])\n        b_norm_sq = np.linalg.norm(b)**2\n        b_ortho_norm_sq = b_norm_sq - np.sum(btilde**2) if r > 0 else b_norm_sq\n        s_sq = s**2 if r > 0 else np.array([])\n        \n        def gcv_func_vectorized(alphas):\n            \"\"\"\n            Calculates GCV values for a vector of alphas.\n            \"\"\"\n            if r == 0:\n                # If rank is 0, A is the zero matrix.\n                # trace(H) is 0, residual norm is ||b||^2.\n                # GCV is constant, so any alpha is equivalent.\n                return np.full_like(alphas, m * b_norm_sq / (m**2))\n\n            alphas_sq = alphas[:, np.newaxis]**2  # Shape (num_alphas, 1)\n            f = s_sq / (s_sq + alphas_sq)         # Shape (num_alphas, r) using broadcasting\n            \n            trace_H = np.sum(f, axis=1) # Shape (num_alphas,)\n            \n            # Residual norm calculation\n            # (1-f_i) = alpha^2 / (sigma_i^2 + alpha^2)\n            term1 = np.sum(( (alphas_sq / (s_sq + alphas_sq))**2 ) * (btilde**2), axis=1) # Shape (num_alphas,)\n            residual_norm_sq = term1 + b_ortho_norm_sq\n            \n            denom = m - trace_H\n            # Handle potential division by zero\n            gcv_vals = np.full_like(denom, np.inf)\n            safe_indices = ~np.isclose(denom, 0)\n            gcv_vals[safe_indices] = m * residual_norm_sq[safe_indices] / (denom[safe_indices]**2)\n            \n            return gcv_vals\n\n        # Step 3: Search for optimal alpha\n        # Coarse Search\n        alphas1 = np.logspace(-8, 2, 200)\n        gcv_values1 = gcv_func_vectorized(alphas1)\n        min_idx1 = np.argmin(gcv_values1)\n        alpha_min1 = alphas1[min_idx1]\n        \n        # Refined Search\n        lower_bound = max(1e-8, alpha_min1 / 10)\n        upper_bound = min(1e2, alpha_min1 * 10)\n        alphas2 = np.logspace(np.log10(lower_bound), np.log10(upper_bound), 200)\n        gcv_values2 = gcv_func_vectorized(alphas2)\n        min_idx2 = np.argmin(gcv_values2)\n        alpha_star = alphas2[min_idx2]\n        \n        # Step 4: Calculate final results for alpha_star\n        if r == 0:\n            return alpha_star, b_norm_sq, 0.0\n\n        alpha_star_sq = alpha_star**2\n        f_star = s_sq / (s_sq + alpha_star_sq)\n        \n        trace_H_star = np.sum(f_star)\n        \n        term1_star = np.sum(( (alpha_star_sq / (s_sq + alpha_star_sq))**2 ) * (btilde**2))\n        residual_norm_sq_star = term1_star + b_ortho_norm_sq\n        \n        return alpha_star, residual_norm_sq_star, trace_H_star\n\n    test_cases = generate_test_cases()\n    results = []\n\n    for A, b in test_cases:\n        alpha_star, res_norm_sq, trace_H = compute_gcv_outputs(A, b)\n        results.append(f\"[{alpha_star:.6f},{res_norm_sq:.6f},{trace_H:.6f}]\")\n\n    print(f\"[[{','.join(results)}]]\")\n\nsolve()\n```"
        }
    ]
}