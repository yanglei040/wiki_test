{
    "hands_on_practices": [
        {
            "introduction": "本练习从一个简单的三维问题入手，旨在建立关于正则化的直观理解。您需要比较两种截然不同的引入先验知识的方法：一种是强制解精确满足特定条件（硬约束），另一种则是对偏离该条件的解进行惩罚（软约束）。通过为这两种估计量显式地计算偏差和方差，您将具体地理解正则化是如何通过引入可控的偏差来换取方差的显著降低的。",
            "id": "3368100",
            "problem": "考虑一个三维线性逆问题，其中状态向量 $x \\in \\mathbb{R}^{3}$ 通过带噪声的恒等观测进行测量。观测模型为 $y = x^{\\star} + \\varepsilon$，其中真实状态为 $x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$，噪声 $\\varepsilon$ 服从多变量正态分布，其均值为零，协方差为 $\\sigma^{2} I_{3}$，其中 $\\sigma^{2} = 1$，$I_{3}$ 是 $3 \\times 3$ 单位矩阵。设 $L \\in \\mathbb{R}^{2 \\times 3}$ 是线性算子\n$$\nL = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix},\n$$\n它对 $x$ 的前两个分量施加一个可行性约束。考虑两种正则化策略：\n\n1. 硬可行性约束 $L x = 0$，它将容许的估计量限制在 $L$ 的零空间内。\n\n2. 强度为 $\\lambda = 2$ 的软二次惩罚，它在数据失配项上增加 $\\lambda \\|L x\\|^{2}$。\n\n从最小二乘估计、线性约束以及多变量正态噪声的性质（零均值，协方差 $\\sigma^{2} I_{3}$）的定义出发，推导两种策略下的估计量并计算：\n- 硬可行性约束引起的偏差向量（定义为估计量的期望减去 $x^{\\star}$）。\n- 由硬可行性约束导致的总方差变化（定义为估计量的协方差矩阵的迹相对于无约束最小二乘估计量的变化）。\n- 强度为 $\\lambda = 2$ 的软二次惩罚引起的偏差向量。\n- 在强度为 $\\lambda = 2$ 的软二次惩罚下，相对于无约束最小二乘估计量的总方差变化。\n\n将最终答案表示为一个单行矩阵，按以下顺序包含八个条目：硬约束偏差向量的三个分量、软惩罚偏差向量的三个分量、硬约束方差变化和软惩罚方差变化。使用精确值，无需四舍五-入。不涉及物理单位。",
            "solution": "该问题是适定的，具有科学依据，并为获得唯一解提供了所有必要信息。我们开始进行推导。\n\n该逆问题由观测模型 $y = x^{\\star} + \\varepsilon$ 描述，其中 $x^{\\star} \\in \\mathbb{R}^{3}$ 是真实状态，$y \\in \\mathbb{R}^{3}$ 是观测值，$\\varepsilon$ 是噪声向量。我们已知：\n- 真实状态：$x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$。\n- 噪声分布：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$，均值 $\\mathbb{E}[\\varepsilon] = 0$，协方差 $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2} I_{3}$。\n- 噪声方差：$\\sigma^{2} = 1$。因此，$\\mathrm{Cov}(\\varepsilon) = I_{3}$。\n- 观测模型可以写成 $y = H x^{\\star} + \\varepsilon$，其中正向算子为单位矩阵 $H = I_{3}$。\n\n我们首先分析无约束最小二乘估计量，它将作为方差比较的基准。\n\n**无约束最小二乘估计量**\n无约束最小二乘估计量 $\\hat{x}_{\\text{LS}}$ 最小化数据失配项（或残差平方和）$J(x) = \\|y - x\\|^{2}$。令梯度 $\\nabla_{x} J(x) = -2(y-x)$ 等于零，可得解：\n$$\n\\hat{x}_{\\text{LS}} = y\n$$\n该估计量的期望为 $\\mathbb{E}[\\hat{x}_{\\text{LS}}] = \\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}$。偏差为 $\\text{Bias}(\\hat{x}_{\\text{LS}}) = \\mathbb{E}[\\hat{x}_{\\text{LS}}] - x^{\\star} = 0$，所以它是一个无偏估计量。\n该估计量的协方差为 $\\mathrm{Cov}(\\hat{x}_{\\text{LS}}) = \\mathrm{Cov}(y) = \\mathrm{Cov}(x^{\\star} + \\varepsilon) = \\mathrm{Cov}(\\varepsilon) = \\sigma^{2} I_{3} = I_{3}$。\n总方差是协方差矩阵的迹：\n$$\n\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = \\mathrm{Tr}(I_{3}) = 1 + 1 + 1 = 3\n$$\n\n**1. 硬可行性约束**\n此策略旨在最小化 $\\|y - x\\|^{2}$，约束条件为线性约束 $Lx = 0$。算子由 $L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix}$ 给出。\n约束 $Lx=0$ 意味着：\n$$\n\\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies x_1 = 0 \\text{ 且 } x_2 = 0\n$$\n估计量必须位于 $L$ 的零空间中，该空间由向量 $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$ 张成。因此，估计量具有形式 $\\hat{x}_{\\text{hard}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\hat{x}_{3} \\end{pmatrix}$。\n将此代入目标函数：\n$$\nJ(x) = (y_1 - 0)^{2} + (y_2 - 0)^{2} + (y_3 - x_3)^{2}\n$$\n为了关于 $x_3$ 最小化 $J(x)$，我们将导数设为零：$\\frac{\\partial J}{\\partial x_3} = -2(y_3 - x_3) = 0$，得到 $\\hat{x}_3 = y_3$。\n因此，硬约束估计量为：\n$$\n\\hat{x}_{\\text{hard}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\n$$\n- **偏差向量（硬约束）：**\n该估计量的期望是 $\\mathbb{E}[\\hat{x}_{\\text{hard}}] = \\mathbb{E}\\left[\\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\\right] = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\mathbb{E}[y_3] \\end{pmatrix}$。\n由于 $y_3 = x^{\\star}_3 + \\varepsilon_3$，我们有 $\\mathbb{E}[y_3] = x^{\\star}_3 + \\mathbb{E}[\\varepsilon_3] = 3 + 0 = 3$。\n所以，$\\mathbb{E}[\\hat{x}_{\\text{hard}}] = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n偏差向量为：\n$$\n\\text{Bias}_{\\text{hard}} = \\mathbb{E}[\\hat{x}_{\\text{hard}}] - x^{\\star} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n- **总方差的变化（硬约束）：**\n该估计量的协方差矩阵为 $\\mathrm{Cov}(\\hat{x}_{\\text{hard}}) = \\mathrm{Cov}\\left(\\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\\right)$。\n协方差矩阵的分量为 $\\mathrm{Cov}(\\hat{x}_{\\text{hard}})_{ij} = \\mathrm{Cov}((\\hat{x}_{\\text{hard}})_i, (\\hat{x}_{\\text{hard}})_j)$。\n前两个分量是常数（$0$），因此它们的方差以及与任何其他变量的协方差均为 $0$。唯一非零的元素是：\n$$\n\\mathrm{Var}((\\hat{x}_{\\text{hard}})_3) = \\mathrm{Var}(y_3) = \\mathrm{Var}(x^{\\star}_3 + \\varepsilon_3) = \\mathrm{Var}(\\varepsilon_3) = \\sigma^{2} = 1\n$$\n协方差矩阵为：\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{hard}}) = \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n总方差为 $\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{hard}})) = 0 + 0 + 1 = 1$。\n相对于无约束估计量的总方差变化是：\n$$\n\\Delta\\mathrm{Var}_{\\text{hard}} = \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{hard}})) - \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = 1 - 3 = -2\n$$\n\n**2. 软二次惩罚（吉洪诺夫正则化）**\n此策略旨在最小化 $J(x) = \\|y - x\\|^{2} + \\lambda \\|L x\\|^{2}$，其中 $\\lambda=2$。这是一个吉洪诺夫正则化问题。目标函数为：\n$$\nJ(x) = (y-x)^{\\top}(y-x) + \\lambda x^{\\top}L^{\\top}Lx\n$$\n令梯度 $\\nabla_x J(x) = -2(y-x) + 2\\lambda L^{\\top}Lx$ 等于零：\n$$\n(I + \\lambda L^{\\top}L)x = y\n$$\n估计量为 $\\hat{x}_{\\text{soft}} = (I + \\lambda L^{\\top}L)^{-1} y$。\n我们来计算矩阵 $M = (I + \\lambda L^{\\top}L)^{-1}$。首先，我们计算 $L^{\\top}L$：\n$$\nL^{\\top}L = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n当 $\\lambda=2$ 时，我们有：\n$$\nI + \\lambda L^{\\top}L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} + 2 \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 3  0  0 \\\\ 0  3  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n其逆矩阵为：\n$$\nM = (I + \\lambda L^{\\top}L)^{-1} = \\begin{pmatrix} 1/3  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n估计量为 $\\hat{x}_{\\text{soft}} = M y = \\begin{pmatrix} y_1/3 \\\\ y_2/3 \\\\ y_3 \\end{pmatrix}$。\n\n- **偏差向量（软惩罚）：**\n该估计量的期望为 $\\mathbb{E}[\\hat{x}_{\\text{soft}}] = \\mathbb{E}[M y] = M \\mathbb{E}[y] = M x^{\\star}$。\n$$\n\\mathbb{E}[\\hat{x}_{\\text{soft}}] = \\begin{pmatrix} 1/3  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ 3 \\end{pmatrix}\n$$\n偏差向量为：\n$$\n\\text{Bias}_{\\text{soft}} = \\mathbb{E}[\\hat{x}_{\\text{soft}}] - x^{\\star} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2/3 - 6/3 \\\\ -1/3 + 3/3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} -4/3 \\\\ 2/3 \\\\ 0 \\end{pmatrix}\n$$\n\n- **总方差的变化（软惩罚）：**\n该估计量的协方差为 $\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = \\mathrm{Cov}(M y) = \\mathrm{Cov}(M(x^{\\star}+\\varepsilon)) = M \\mathrm{Cov}(\\varepsilon) M^{\\top}$。\n由于 $\\mathrm{Cov}(\\varepsilon) = I_3$ 且 $M$ 是对称的（$M=M^{\\top}$）：\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = M I_3 M = M^2\n$$\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = \\begin{pmatrix} 1/3  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1 \\end{pmatrix}^2 = \\begin{pmatrix} 1/9  0  0 \\\\ 0  1/9  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n总方差为 $\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{soft}})) = 1/9 + 1/9 + 1 = 2/9 + 9/9 = 11/9$。\n相对于无约束估计量的总方差变化是：\n$$\n\\Delta\\mathrm{Var}_{\\text{soft}} = \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{soft}})) - \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = \\frac{11}{9} - 3 = \\frac{11}{9} - \\frac{27}{9} = -\\frac{16}{9}\n$$\n\n**结果总结**\n- 硬约束偏差向量： `(−2, 1, 0)`\n- 软惩罚偏差向量： `(−4/3, 2/3, 0)`\n- 硬约束方差变化： `−2`\n- 软惩罚方差变化： `−16/9`\n\n最终答案是一个行矩阵，按指定顺序包含这八个值。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -2  1  0  -\\frac{4}{3}  \\frac{2}{3}  0  -2  -\\frac{16}{9} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在了解了基本的偏差-方差权衡之后，我们现在将探讨其在高维欠定系统中的惊人表现。本练习将引导您分析“双下降”现象，在这种现象中，误差会在插值点（即 $m \\approx n$）附近出现一个反常的峰值。通过推导吉洪诺夫（Tikhonov）估计量的偏差-方差分解 ，您将发现这个峰值是由方差的急剧膨胀所驱动的，并且更重要的是，您将证明正则化是如何有效抑制这种不稳定性的。",
            "id": "3368047",
            "problem": "考虑一个线性逆问题，其测量模型为 $y = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $m  n$，$x \\in \\mathbb{R}^{n}$ 是一个固定但未知的状态，$\\varepsilon \\in \\mathbb{R}^{m}$ 是观测噪声，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。令 $\\hat{x}_{\\lambda}$ 表示 Tikhonov 正则化估计量，它由 $\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ 的最小化子定义，其中 $\\lambda \\ge 0$ 是正则化参数。假设 $A$ 具有奇异值分解 (SVD)，其形式为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 在其对角线上包含非零奇异值 $\\sigma_{1}, \\dots, \\sigma_{r}$，且 $r = \\operatorname{rank}(A) \\le m$。\n\n从 $\\hat{x}_{\\lambda}$ 作为 Tikhonov 目标函数的唯一最小化子的定义以及 SVD 的性质出发，推导期望平方估计误差 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{\\lambda} - x\\|_{2}^{2}$ 的偏差-方差分解，将偏差和方差都用奇异值 $\\{\\sigma_{i}\\}_{i=1}^{r}$ 和 $x$ 关于右奇异向量 $\\{v_{i}\\}_{i=1}^{n}$ 的坐标来表示。仅使用此基于原理的分解，解释当 $m$ 在 $m  n$ 的条件下增加并趋近于 $n$（插值阈值）时，$\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{0} - x\\|_{2}^{2}$ 的定性行为，并论证在插值点附近是否会出现方差峰值。然后，阐明严格正正则化 $\\lambda  0$ 如何修正方差项以抑制这样的峰值，并通过一个依赖于 $\\lambda$ 和 $r$ 的显式界来支持你的推理。\n\n根据此推导和分析，哪种说法最准确？\n\nA. 对于 $\\lambda = 0$ 和 $m  n$，最小范数插值器 $\\hat{x}_{0}$ 的方差项为 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{0} - \\mathbb{E}_{\\varepsilon}\\hat{x}_{0}\\|_{2}^{2} = \\sigma^{2} \\sum_{i=1}^{r} \\sigma_{i}^{-2}$。当 $m$ 从下方趋近于 $n$ 时，最小非零奇异值 $\\min_{i \\le r} \\sigma_{i} \\to 0$，该方差项可能激增，从而在 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{0} - x\\|_{2}^{2}$ 中产生一个由方差驱动的峰值；对于 $\\lambda  0$，方差变为 $\\sigma^{2} \\sum_{i=1}^{r} \\frac{\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda)^{2}} \\le \\frac{\\sigma^{2} r}{4 \\lambda}$，从而抑制了插值点附近的峰值。\n\nB. 使用 $\\lambda  0$ 的正则化必然会增加插值点附近的方差，因为它会收缩信号，而非正则化估计量 $\\lambda = 0$ 的方差会因为 $A$ 的奇异值增长而随着 $m \\to n$ 减小；因此，正则化放大了双下降峰。\n\nC. 对于 $m  n$，最小范数插值器 $\\hat{x}_{0}$ 精确拟合数据，意味着 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{0} - \\mathbb{E}_{\\varepsilon}\\hat{x}_{0}\\|_{2}^{2} = 0$，从而排除了任何由方差驱动的峰值；因此，在欠定系统中，参数误差不会出现双下降现象。\n\nD. $\\mathbb{E}_{\\varepsilon}\\|\\hat{x} - x\\|_{2}^{2}$ 中的双下降行为仅仅源于由 $A$ 的零空间引起的偏差项，并且不受奇异值的影响；正则化只调整偏差，对方差项没有影响。",
            "solution": "问题要求推导 Tikhonov 正则化估计量 $\\hat{x}_{\\lambda}$ 的偏差-方差分解，并分析其行为，特别是在插值阈值 $m \\approx n$ 附近。\n\n首先，我们验证问题陈述的有效性。\n该问题为带有 Tikhonov 正则化的线性逆问题提供了一个标准设置。\n**已知条件**:\n- 模型: $y = A x + \\varepsilon$\n- $A \\in \\mathbb{R}^{m \\times n}$ 且 $m  n$\n- $x \\in \\mathbb{R}^{n}$ 是一个固定但未知的状态\n- $\\varepsilon \\in \\mathbb{R}^{m}$ 是噪声，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$\n- 估计量 $\\hat{x}_{\\lambda}$ 对 $\\lambda \\ge 0$ 最小化 $\\|A x' - y\\|_{2}^{2} + \\lambda \\|x'\\|_{2}^{2}$\n- A 的 SVD: $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 在其对角线上有非零奇异值 $\\sigma_{1}, \\dots, \\sigma_{r}$，且 $r = \\operatorname{rank}(A) \\le m$。\n\n**验证**:\n问题陈述具有科学依据，是适定且客观的。它使用了线性代数和统计学中的标准定义。其设置是自洽的，没有矛盾。所提出的问题是分析正则化估计量的一个标准理论练习，特别是在双下降现象的背景下。没有可识别的缺陷。该问题是有效的。\n\n我们现在开始求解。\n\nTikhonov 目标函数为 $J(x') = \\|A x' - y\\|_{2}^{2} + \\lambda \\|x'\\|_{2}^{2}$。为了找到最小化子 $\\hat{x}_{\\lambda}$，我们将其关于 $x'$ 的梯度设为零：\n$$ \\nabla_{x'} J(x') = 2 A^{\\top}(A x' - y) + 2 \\lambda x' = 0 $$\n$$ (A^{\\top}A + \\lambda I_n) x' = A^{\\top}y $$\n对于 $\\lambda  0$，矩阵 $A^{\\top}A + \\lambda I_n$ 是正定的，因此是可逆的。对于 $\\lambda = 0$，我们寻找的是最小范数解，它由伪逆给出。通解为：\n$$ \\hat{x}_{\\lambda} = (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top}y $$\n代入 SVD $A = U \\Sigma V^{\\top}$，我们得到 $A^{\\top}A = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V \\Sigma^{\\top}\\Sigma V^{\\top}$。\n$$ \\hat{x}_{\\lambda} = (V \\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda V V^{\\top})^{-1} V \\Sigma^{\\top} U^{\\top} y = V (\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top} U^{\\top} y $$\n矩阵 $(\\Sigma^{\\top}\\Sigma + \\lambda I_n)$ 是一个 $n \\times n$ 的对角矩阵，其对角线上的元素对于 $i=1, \\dots, r$ 是 $\\sigma_i^2 + \\lambda$，对于 $i=r+1, \\dots, n$ 是 $\\lambda$。矩阵乘积 $(\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top}$ 是一个 $n \\times m$ 的矩阵，其 $(i,i)$ 元素对于 $i=1, \\dots, r$ 是 $\\frac{\\sigma_i}{\\sigma_i^2+\\lambda}$，其他位置为零。\n\n期望平方估计误差为 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{\\lambda} - x\\|_{2}^{2}$。我们可以将其分解为偏差平方和方差：\n$$ \\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{\\lambda} - x\\|_{2}^{2} = \\underbrace{\\|\\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] - x\\|_{2}^{2}}_{\\text{偏差}^2} + \\underbrace{\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_{\\lambda} - \\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}]\\|_{2}^{2}}_{\\text{方差}} $$\n\n首先，我们计算估计量的期望值。使用 $y = Ax + \\varepsilon$ 和 $\\mathbb{E}_{\\varepsilon}[\\varepsilon]=0$：\n$$ \\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] = (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top}\\mathbb{E}_{\\varepsilon}[Ax + \\varepsilon] = (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top}Ax $$\n使用 SVD 展开：\n$$ \\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] = V (\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top}\\Sigma V^{\\top} x $$\n矩阵 $(\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top}\\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角线上的元素对于 $i=1, \\dots, r$ 是 $\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda}$，对于 $ir$ 是 $0$。令 $x_i = v_i^{\\top}x$ 为 $x$ 沿右奇异向量 $v_i$ 的坐标。\n$$ \\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] = \\sum_{i=1}^{r} \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda} (v_i^{\\top}x) v_i $$\n偏差向量为 $\\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] - x$：\n$$ \\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] - x = \\sum_{i=1}^{r} \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda} - 1\\right) (v_i^{\\top}x) v_i - \\sum_{i=r+1}^{n} (v_i^{\\top}x) v_i = \\sum_{i=1}^{r} \\frac{-\\lambda}{\\sigma_i^2 + \\lambda} (v_i^{\\top}x) v_i - \\sum_{i=r+1}^{n} (v_i^{\\top}x) v_i $$\n偏差平方是该向量的范数平方。由于 $\\{v_i\\}$ 是一个标准正交基：\n$$ \\text{偏差}^2 = \\sum_{i=1}^{r} \\left(\\frac{\\lambda}{\\sigma_i^2 + \\lambda}\\right)^2 (v_i^{\\top}x)^2 + \\sum_{i=r+1}^{n} (v_i^{\\top}x)^2 $$\n\n接下来，我们计算方差。与均值的偏差为：\n$$ \\hat{x}_{\\lambda} - \\mathbb{E}_{\\varepsilon}[\\hat{x}_{\\lambda}] = (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top} y - (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top} Ax = (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top} \\varepsilon $$\n令 $H_{\\lambda} = (A^{\\top}A + \\lambda I_n)^{-1} A^{\\top}$。方差为 $\\mathbb{E}_{\\varepsilon}\\|H_{\\lambda} \\varepsilon\\|_{2}^{2}$。对于二次型的期望，使用迹的性质，当 $\\mathbb{E}[\\varepsilon]=0$ 时，$\\mathbb{E}[\\varepsilon^{\\top}M\\varepsilon] = \\text{Tr}(M\\text{Cov}(\\varepsilon))$。这里，$M=H_\\lambda^\\top H_\\lambda$ 且 $\\text{Cov}(\\varepsilon) = \\sigma^2 I_m$。\n$$ \\text{方差} = \\mathbb{E}_{\\varepsilon}[\\varepsilon^{\\top} H_{\\lambda}^{\\top} H_{\\lambda} \\varepsilon] = \\sigma^2 \\text{Tr}(H_{\\lambda}^{\\top} H_{\\lambda}) = \\sigma^2 \\text{Tr}(H_{\\lambda} H_{\\lambda}^{\\top}) $$\n对 $H_{\\lambda} = V (\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top} U^{\\top}$ 使用 SVD：\n$$ H_{\\lambda} H_{\\lambda}^{\\top} = V (\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top} U^{\\top} U \\Sigma ((\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1})^{\\top} V^{\\top} $$\n$$ H_{\\lambda} H_{\\lambda}^{\\top} = V \\left[ (\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\Sigma^{\\top}\\Sigma (\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1} \\right] V^{\\top} $$\n括号中的矩阵是对角的。它的迹在由 $V$ 进行的正交变换下是不变的。对于 $i=1,\\dots,r$，第 $i$ 个对角元素是 $\\frac{\\sigma_i^2}{(\\sigma_i^2+\\lambda)^2}$，对于 $ir$ 则是 $0$。\n因此，迹为 $\\sum_{i=1}^r \\frac{\\sigma_i^2}{(\\sigma_i^2+\\lambda)^2}$。\n$$ \\text{方差} = \\sigma^2 \\sum_{i=1}^{r} \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda)^2} $$\n\n现在我们分析其行为。\n\n**情况 1：无正则化 ($\\lambda=0$)**\n这对应于最小范数最小二乘解 $\\hat{x}_0 = A^{\\dagger}y$。\n方差表达式变为：\n$$ \\text{方差}(\\lambda=0) = \\sigma^2 \\sum_{i=1}^{r} \\frac{\\sigma_i^2}{(\\sigma_i^2)^2} = \\sigma^2 \\sum_{i=1}^{r} \\frac{1}{\\sigma_i^2} $$\n问题要求考虑当 $m$ 从下方增加并趋近于 $n$ 时的行为。在这种情况下，特别是对于随机矩阵 $A$，系统会变得病态。具体来说，当 $m \\to n$ 时，最小的非零奇异值 $\\sigma_r$（其中 $r \\le m$）趋于零。当 $\\sigma_r \\to 0$ 时，方差和中的项 $1/\\sigma_r^2$ 会激增，导致总方差发散。这导致在插值点 $m=n$ 之前，总误差 $\\mathbb{E}_{\\varepsilon}\\|\\hat{x}_0-x\\|_2^2$ 出现一个峰值。这就是欠定系统中双下降现象特有的“方差驱动峰值”。\n\n**情况 2：有正则化 ($\\lambda0$)**\n方差为 $\\text{方差}(\\lambda) = \\sigma^2 \\sum_{i=1}^{r} \\frac{\\sigma_i^2}{(\\sigma_i^2+\\lambda)^2}$。\n我们来分析函数 $f(s) = \\frac{s}{(s+\\lambda)^2}$，其中 $s = \\sigma_i^2  0$。其导数为 $f'(s) = \\frac{\\lambda-s}{(s+\\lambda)^3}$，在 $s=\\lambda$ 时为零。二阶导数表明这是一个最大值。最大值为 $f(\\lambda) = \\frac{\\lambda}{(\\lambda+\\lambda)^2} = \\frac{1}{4\\lambda}$。\n因此，方差和中的每一项都有界：\n$$ \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda)^2} \\le \\frac{1}{4\\lambda} $$\n这使我们能够为总方差设定一个界：\n$$ \\text{方差}(\\lambda) \\le \\sum_{i=1}^{r} \\frac{\\sigma^2}{4\\lambda} = \\frac{\\sigma^2 r}{4\\lambda} $$\n这个界与奇异值 $\\sigma_i$ 的值无关。即使某些 $\\sigma_i \\to 0$，对应的项 $\\frac{\\sigma_i^2}{(\\sigma_i^2+\\lambda)^2}$ 也趋于 0。分母中的正则化参数 $\\lambda  0$ 防止了任何项的激增。因此，严格正正则化抑制了在非正则化情况下于插值阈值附近观察到的方差峰值。\n\n现在我们根据此分析评估各个选项。\n\n**A.** 该陈述与我们的推导完全一致。它正确地给出了 $\\lambda=0$ 和 $\\lambda0$ 时的方差，准确地描述了 $\\lambda=0$ 时的激增机制，为正则化方差提供了正确的上界，并正确地得出结论：正则化抑制了峰值。“最小范数插值器”一词在出现此峰值的情况下（$r=m$）在语境上是恰当的。**正确**。\n\n**B.** 这在几个方面都是不正确的。首先，正则化*减小*了方差，因为对于任何 $\\lambda0$，都有 $\\frac{\\sigma_i^2}{(\\sigma_i^2+\\lambda)^2}  \\frac{1}{\\sigma_i^2}$。其次，由于出现的较小奇异值，当 $m \\to n$ 时，$\\lambda=0$ 的方差通常会增加而不是减少。因此，正则化是抑制而非放大了峰值。**不正确**。\n\n**C.** 这在推理上存在根本缺陷。虽然 $\\hat{x}_0$ 可能插值数据 $y$（即，如果 $r=m$，则 $A\\hat{x}_0=y$），但估计量 $\\hat{x}_0$ 本身是 $y$ 中随机噪声 $\\varepsilon$ 的函数。因此，$\\hat{x}_0$ 是一个随机变量，具有非零方差，我们计算出其值为 $\\sigma^2 \\sum_{i=1}^{r} \\sigma_i^{-2}$。精确拟合意味着零方差的前提是错误的。**不正确**。\n\n**D.** 这是不正确的。我们的分析表明，$m \\approx n$ 处的峰值是由*方差*项驱动的，该项高度依赖于奇异值。由零空间引起的偏差项 $\\sum_{i=r+1}^{n} (v_i^{\\top}x)^2$ 通常随着 $m \\to n$ 而减小，因为秩 $r$ 增加了。此外，正如我们推导的公式和界所示，正则化显然对方差项有巨大影响。**不正确**。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "最后的这个练习将理论带入了计算实践中。您将实现一个完整的吉洪诺夫（Tikhonov）正则化工作流程，从求解正则化路径到确定最优的正则化参数 $\\lambda$。本练习  旨在挑战您关联三个关键视角：通过最小化统计风险找到的理论最优解，通过L曲线拐点得到的启发式选择，以及通过“激活”的奇异模式所解释的内在模型复杂度。这项编码任务综合了本章的核心概念，让您亲身体验偏差-方差分析如何指导实际的模型选择。",
            "id": "3368069",
            "problem": "考虑一个有限维欧几里得空间中的加性噪声线性逆问题：给定一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个真实状态 $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ 和噪声 $ \\varepsilon \\in \\mathbb{R}^{m}$，观测数据为 $y = A x_{\\mathrm{true}} + \\varepsilon$。噪声被建模为零均值、独立且方差为 $\\sigma_{\\varepsilon}^{2}$。对于一个正则化参数 $\\lambda  0$，将 Tikhonov 正则化解 $x_{\\lambda}$ 定义为泛函 $\\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$ 的最小化子。假设 $A$ 具有奇异值分解 $A = U \\Sigma V^{\\top}$，其奇异值为 $\\{\\sigma_{i}\\}_{i=1}^{r}$，其中 $r = \\min(m,n)$ 且 $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r}  0$。\n\n您的任务如下，并且所有推导都必须从 Tikhonov 正则化、奇异值分解和均方误差的标准偏差-方差分解的定义开始；不要使用任何未从这些基础推导出来的现成风险或曲率公式。\n\n- 定义正则化路径 $\\{x_{\\lambda} : \\lambda  0\\}$，并通过为每个奇异值 $\\sigma_{i}$ 确定一个在 $\\lambda$ 中的确定性阈值来刻画此路径上奇异模式的激活。在该阈值处，相应奇异方向的贡献从被抑制过渡到显著存在。使用基于 Tikhonov 解在奇异向量基中所隐含的滤波因子的一个有原则的准则来形式化此阈值，并将此阈值视为模式 $i$ 的一个相变点。用奇异值明确地陈述这些阈值。\n\n- 将 L-曲线定义为由 $\\log \\lambda$ 参数化的参数平面曲线 $\\left(\\log \\|A x_{\\lambda} - y\\|_{2}, \\log \\|x_{\\lambda}\\|_{2}\\right)$。使用平面参数曲线的曲率定义（根据其关于参数的一阶和二阶导数），实现一个数值程序来估计在一系列 $\\lambda$ 值网格上的曲率，并将 L-曲线的拐角确定为曲率达到其最大值时的正则化参数 $\\lambda$。您必须用 $\\log \\lambda$ 对曲线进行参数化，并使用数值稳定的有限差分。\n\n- 在奇异向量基中，使用期望平方误差 $\\mathbb{E}\\left[\\|x_{\\lambda} - x_{\\mathrm{true}}\\|_{2}^{2}\\right]$ 的偏差-方差分解，推导并实现一个以 $\\lambda$ 为函数的均方风险表达式，并在一个对数间隔的网格上确定风险最小化的正则化参数 $\\lambda_{\\mathrm{risk}}$。您的推导必须明确区分偏差和方差的贡献。\n\n- 通过为每个测试案例报告以下内容，将相变位置、L-曲线拐角和风险最小值联系起来：L-曲线拐角 $\\lambda_{\\mathrm{L}}$、风险最小值 $\\lambda_{\\mathrm{risk}}$、比率 $\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}$、在您的激活准则下 $\\lambda_{\\mathrm{risk}}$ 处激活的模式数量，以及一个布尔值，指示 $\\lambda_{\\mathrm{L}}$ 和 $\\lambda_{\\mathrm{risk}}$ 是否彼此相差在 2 倍以内。\n\n实现要求和测试套件：\n\n- 您必须将 $A$ 构建为具有指定奇异值的对角矩阵（因此 $U = I$ 且 $V = I$），并且必须使用由给定目标函数定义的精确 Tikhonov 解。数值曲率必须在 $\\log \\lambda$ 的均匀网格上使用中心有限差分计算。\n\n- 为 $\\lambda$ 使用一个对数间隔的网格，在 $\\lambda_{\\min} = 10^{-6} \\cdot \\max_{i} \\sigma_{i}$ 和 $\\lambda_{\\max} = 10^{2} \\cdot \\max_{i} \\sigma_{i}$ 之间（包括两端点）取 $N_{\\lambda} = 800$ 个点。\n\n- 对于 L-曲线，使用每个测试案例中由指定的 $\\varepsilon$ 构建的实现数据 $y$。对于风险，使用由提供的 $\\sigma_{\\varepsilon}^{2}$ 计算的期望风险。\n\n- 将模式 $i$ 的激活阈值定义为该模式对应的 Tikhonov 滤波因子等于 $1/2$ 时的 $\\lambda$ 值，并且如果一个模式在给定 $\\lambda$ 下的滤波因子至少为 $1/2$，则计为激活。\n\n- 使用以下三个测试案例，每个案例中 $m=n$，并使用指定的确定性噪声向量 $\\varepsilon$：\n    - 测试案例 1：\n        - 奇异值：$\\sigma = [10.0, 1.0, 0.1]$。\n        - 真实状态：$x_{\\mathrm{true}} = [1.0, 1.0, 1.0]$。\n        - 噪声方差：$\\sigma_{\\varepsilon}^{2} = 0.01$。\n        - 噪声向量：$\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, -1.0, 1.0]$。\n    - 测试案例 2：\n        - 奇异值：$\\sigma = [5.0, 4.0, 3.0, 2.0, 1.0]$。\n        - 真实状态：$x_{\\mathrm{true}} = [1.0, 0.5, 0.25, 0.125, 0.0625]$。\n        - 噪声方差：$\\sigma_{\\varepsilon}^{2} = 0.04$。\n        - 噪声向量：$\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, 0.0, -1.0, 0.0, 1.0]$。\n    - 测试案例 3：\n        - 奇异值：$\\sigma = [1.0, 0.3, 0.09, 0.027]$。\n        - 真实状态：$x_{\\mathrm{true}} = [1.0, 0.8, 0.6, 0.4]$。\n        - 噪声方差：$\\sigma_{\\varepsilon}^{2} = 0.0001$。\n        - 噪声向量：$\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, -2.0, 1.0, -2.0]$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含三个测试案例的结果，形式为一个用方括号括起来的逗号分隔列表。该列表必须按测试案例 1、2、3 的顺序，串联每个案例的以下五个项目：\n    - $\\lambda_{\\mathrm{L}}$ (浮点数),\n    - $\\lambda_{\\mathrm{risk}}$ (浮点数),\n    - $\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}$ (浮点数),\n    - 在 $\\lambda_{\\mathrm{risk}}$ 处激活的模式数量 (整数),\n    - 一个布尔值，指示 $\\max\\left(\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}, \\lambda_{\\mathrm{risk}} / \\lambda_{\\mathrm{L}}\\right) \\le 2$ 是否成立。\n\n因此，输出列表包含 15 个条目：每个测试案例 5 个条目，跨 3 个案例串联，并作为单行的 Python 风格列表打印，无附加文本。",
            "solution": "该问题要求对线性逆问题的 Tikhonov 正则化进行全面分析，重点关注正则化参数 $\\lambda$、模型复杂度（激活的奇异模式）、预测误差（L-曲线）和估计误差（统计风险）之间的联系。分析从第一性原理出发。\n\n### 1. Tikhonov 正则化与正则化路径\n\n问题是为线性系统 $y = Ax_{\\mathrm{true}} + \\varepsilon$ 找到一个稳定的解 $x$，其中 $y \\in \\mathbb{R}^m$ 是观测数据，$A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$x_{\\mathrm{true}} \\in \\mathbb{R}^n$ 是真实的潜在状态，$\\varepsilon \\in \\mathbb{R}^m$ 是加性噪声。\n\nTikhonov 正则化通过最小化以下泛函来寻找解：\n$$\nJ_{\\lambda}(x) = \\|Ax - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}\n$$\n其中 $\\lambda  0$ 是正则化参数。第一项强制数据保真度，而第二项惩罚大的解，从而保证稳定性。通过将 $J_{\\lambda}(x)$ 对 $x$ 的梯度设为零来找到解 $x_{\\lambda}$：\n$$\n\\nabla_{x} J_{\\lambda}(x) = 2A^{\\top}(Ax - y) + 2\\lambda^{2}x = 0\n$$\n这导出了正规方程：\n$$\n(A^{\\top}A + \\lambda^{2}I)x = A^{\\top}y\n$$\n因此，Tikhonov 正则化解为：\n$$\nx_{\\lambda} = (A^{\\top}A + \\lambda^{2}I)^{-1}A^{\\top}y\n$$\n解的集合 $\\{x_{\\lambda} : \\lambda  0\\}$ 构成了正则化路径。\n\n为了分析 $x_{\\lambda}$ 的行为，我们使用 $A$ 的奇异值分解 (SVD)：$A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个矩形对角矩阵，其对角线上有非负奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r  0$，其中 $r = \\text{rank}(A)$。\n\n将 SVD 代入 $x_{\\lambda}$ 的表达式中：\n$A^{\\top}A = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top} = V\\Sigma^{\\top}\\Sigma V^{\\top}$。\n$A^{\\top}y = (U\\Sigma V^{\\top})^{\\top}y = V\\Sigma^{\\top}U^{\\top}y$。\n所以，$(V\\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda^2 I)x_\\lambda = V\\Sigma^{\\top}U^{\\top}y$。\n从左侧乘以 $V^{\\top}$ 并注意到 $V^{\\top}V=I$：\n$(\\Sigma^{\\top}\\Sigma + \\lambda^2 I)V^{\\top}x_\\lambda = \\Sigma^{\\top}U^{\\top}y$。\n令 $\\hat{x}_{\\lambda} = V^{\\top}x_{\\lambda}$ 和 $\\hat{y} = U^{\\top}y$ 分别为投影到奇异向量基上的解和数据。矩阵 $\\Sigma^{\\top}\\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角线元素在 $i=1, \\dots, r$ 时为 $\\sigma_i^2$，否则为零。方程变为对角化的：\n$$\n(\\sigma_i^2 + \\lambda^2)(\\hat{x}_{\\lambda})_i = \\sigma_i (\\hat{y})_i \\quad \\text{for } i \\le r\n$$\n在 $V$ 基中第 $i$ 个分量的解是：\n$$\n(\\hat{x}_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2}(\\hat{y})_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}\\right) \\frac{(\\hat{y})_i}{\\sigma_i}\n$$\n项 $f_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$ 是第 $i$ 个奇异模式的 Tikhonov 滤波因子。它决定了第 $i$ 个数据分量 $(\\hat{y})_i / \\sigma_i$ 对解分量 $(\\hat{x}_{\\lambda})_i$ 的贡献程度。\n当 $\\lambda \\ll \\sigma_i$ 时，$f_i(\\lambda) \\approx 1$，该模式被通过。\n当 $\\lambda \\gg \\sigma_i$ 时，$f_i(\\lambda) \\approx 0$，该模式被滤除。\n问题将激活阈值定义为滤波因子等于 $1/2$ 的点：\n$$\nf_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} = \\frac{1}{2} \\implies 2\\sigma_i^2 = \\sigma_i^2 + \\lambda^2 \\implies \\lambda^2 = \\sigma_i^2\n$$\n由于 $\\lambda  0$ 且 $\\sigma_i  0$，模式 $i$ 的激活阈值为 $\\lambda = \\sigma_i$。\n如果在给定的 $\\lambda$ 下 $f_i(\\lambda) \\ge 1/2$，则该模式被计为激活，这等价于 $\\sigma_i^2 \\ge \\lambda^2$，或 $\\sigma_i \\ge \\lambda$。\n\n### 2. L-曲线拐角识别\n\nL-曲线是 $(\\rho_\\lambda, \\eta_\\lambda)$ 的参数图，其中 $\\rho_\\lambda = \\log \\|A x_{\\lambda} - y\\|_{2}$ 且 $\\eta_\\lambda = \\log \\|x_{\\lambda}\\|_{2}$，由 $p = \\log \\lambda$ 参数化。L-曲线的“拐角”通常对应于 $\\lambda$ 的一个良好选择，它平衡了数据保真度和解的稳定性。拐角通过找到最大曲率点来定位。\n\n问题指定 $A$ 为对角矩阵，因此 $U=V=I$ 且 $m=n=r$。SVD 是平凡的，有 $A_{ii} = \\sigma_i$。解和残差的分量为：\n$$\n(x_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} y_i\n$$\n$$\n(Ax_{\\lambda} - y)_i = \\sigma_i (x_{\\lambda})_i - y_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right) y_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} y_i\n$$\nL-曲线所需的平方范数为：\n$$\nS_\\lambda = \\|x_{\\lambda}\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i y_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\n$$\nR_\\lambda = \\|Ax_{\\lambda} - y\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 y_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\nL-曲线上的点为 $(\\rho(\\lambda), \\eta(\\lambda)) = (\\log\\sqrt{R_\\lambda}, \\log\\sqrt{S_\\lambda}) = (\\frac{1}{2}\\log R_\\lambda, \\frac{1}{2}\\log S_\\lambda)$。参数曲线 $(x(p), y(p))$ 的曲率 $K$ 由下式给出：\n$$\nK(p) = \\frac{|x'y'' - y'x''|}{(x'^2 + y'^2)^{3/2}}\n$$\n其中撇号表示对参数 $p = \\log \\lambda$ 的导数。\n在数值上，我们在 $p_j = \\log \\lambda_j$ 的均匀网格上计算曲线点 $(\\rho_j, \\eta_j)$。导数使用中心有限差分进行近似。设 $\\Delta p = p_{j+1} - p_j$：\n$$\n\\rho'_j \\approx \\frac{\\rho_{j+1} - \\rho_{j-1}}{2 \\Delta p}, \\quad \\rho''_j \\approx \\frac{\\rho_{j+1} - 2\\rho_j + \\rho_{j-1}}{(\\Delta p)^2}\n$$\n对 $\\eta_j$ 也类似。将这些近似值代入曲率公式以找到 $K_j$。正则化参数 $\\lambda_L$ 是使 $K_j$ 最大化的 $\\lambda_j$ 的值。\n\n### 3. 偏差-方差分解与风险最小化\n\n估计量 $x_\\lambda$ 的质量由均方误差（或称风险）来衡量，定义为 $Risk(\\lambda) = \\mathbb{E}[\\|x_\\lambda - x_{\\mathrm{true}}\\|_2^2]$，其中期望是关于噪声 $\\varepsilon$ 的分布计算的。噪声是零均值（$\\mathbb{E}[\\varepsilon] = 0$）、各分量独立且方差为 $\\sigma_\\varepsilon^2$（$\\mathbb{E}[\\varepsilon\\varepsilon^\\top] = \\sigma_\\varepsilon^2 I$）。\n\n风险可以分解为偏差的平方和方差：\n$$\nRisk(\\lambda) = \\underbrace{\\|\\mathbb{E}[x_\\lambda] - x_{\\mathrm{true}}\\|_2^2}_{\\text{Squared Bias}} + \\underbrace{\\mathbb{E}[\\|x_\\lambda - \\mathbb{E}[x_\\lambda]\\|_2^2]}_{\\text{Variance}}\n$$\n\n首先，我们计算解的期望值。由于 $y = Ax_{\\mathrm{true}} + \\varepsilon$ 以及期望的线性性质：\n$$\n\\mathbb{E}[x_\\lambda] = \\mathbb{E}[(A^\\top A + \\lambda^2 I)^{-1} A^\\top (A x_{\\mathrm{true}} + \\varepsilon)] = (A^\\top A + \\lambda^2 I)^{-1} A^\\top A x_{\\mathrm{true}}\n$$\n偏差为 $b(\\lambda) = \\mathbb{E}[x_\\lambda] - x_{\\mathrm{true}} = [(A^\\top A + \\lambda^2 I)^{-1} A^\\top A - I]x_{\\mathrm{true}}$。\n在 $V$ 基中使用 SVD 分量，偏差的第 $i$ 个分量是：\n$$\n(\\hat{b}(\\lambda))_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right)(\\hat{x}_{\\mathrm{true}})_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}(\\hat{x}_{\\mathrm{true}})_i\n$$\n偏差平方范数为 $\\|\\hat{b}(\\lambda)\\|_2^2$，对于本问题中的对角矩阵 $A$（$V=I, \\hat{x}_{\\text{true}}=x_{\\text{true}}$），其表达式为：\n$$\n\\text{Squared Bias} = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 (x_{\\mathrm{true}})_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\n接下来，我们计算方差。解与均值的偏差为：\n$$\nx_\\lambda - \\mathbb{E}[x_\\lambda] = (A^\\top A + \\lambda^2 I)^{-1} A^\\top \\varepsilon\n$$\n方差是该量的期望平方范数：\n$$\n\\text{Variance} = \\mathbb{E}[\\varepsilon^\\top A (A^\\top A + \\lambda^2 I)^{-2} A^\\top \\varepsilon]\n$$\n使用随机向量二次型的迹恒等式 $\\mathbb{E}[z^\\top Q z]=\\text{Tr}(Q \\text{Cov}(z))$，其中 $z=\\varepsilon$ 且 $\\text{Cov}(\\varepsilon) = \\sigma_\\varepsilon^2 I$：\n$$\n\\text{Variance} = \\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top \\sigma_\\varepsilon^2 I) = \\sigma_\\varepsilon^2 \\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top)\n$$\n利用迹的循环性质和 SVD：\n$$\n\\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top) = \\text{Tr}((A^\\top A + \\lambda^2 I)^{-2} A^\\top A) = \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\n所以方差为：\n$$\n\\text{Variance} = \\sigma_\\varepsilon^2 \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\n总风险是偏差平方和方差之和：\n$$\nRisk(\\lambda) = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 (x_{\\mathrm{true}})_i}{\\sigma_i^2 + \\lambda^2} \\right)^2 + \\sigma_\\varepsilon^2 \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\n风险最小化参数 $\\lambda_{\\mathrm{risk}}$ 是通过在 $\\lambda$ 值的网格上最小化此函数找到的。\n\n### 4. 综合与报告\n\n最后一步是为每个测试案例综合这些分析。\n1. 计算 L-曲线及其曲率以找到 $\\lambda_L$。\n2. 计算风险函数并找到其最小化子 $\\lambda_{\\mathrm{risk}}$。\n3. 计算比率 $\\lambda_L / \\lambda_{\\mathrm{risk}}$。\n4. 通过计算有多少个奇异值 $\\sigma_i$ 满足 $\\sigma_i \\ge \\lambda_{\\mathrm{risk}}$，来统计在 $\\lambda = \\lambda_{\\mathrm{risk}}$ 处激活的模式数量。\n5. 判断 $\\lambda_L$ 和 $\\lambda_{\\mathrm{risk}}$ 是否彼此相差在 2 倍以内，即 $1/2 \\le \\lambda_L / \\lambda_{\\mathrm{risk}} \\le 2$。\n\n这个过程提供了对正则化的一个多方面的视角，将一个启发式方法（L-曲线）、一个统计最优性准则（风险最小化）和一个模型复杂度解释（激活模式的数量）联系起来。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization analysis problem for three test cases.\n\n    For each case, it calculates:\n    1. lambda_L: The regularization parameter at the L-curve corner.\n    2. lambda_risk: The risk-minimizing regularization parameter.\n    3. The ratio lambda_L / lambda_risk.\n    4. The number of activated singular modes at lambda_risk.\n    5. A boolean indicating if lambda_L and lambda_risk are within a factor of 2.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"sigma\": np.array([10.0, 1.0, 0.1]),\n            \"x_true\": np.array([1.0, 1.0, 1.0]),\n            \"sigma_eps_sq\": 0.01,\n            \"epsilon\": np.sqrt(0.01) * np.array([1.0, -1.0, 1.0])\n        },\n        {\n            \"sigma\": np.array([5.0, 4.0, 3.0, 2.0, 1.0]),\n            \"x_true\": np.array([1.0, 0.5, 0.25, 0.125, 0.0625]),\n            \"sigma_eps_sq\": 0.04,\n            \"epsilon\": np.sqrt(0.04) * np.array([1.0, 0.0, -1.0, 0.0, 1.0])\n        },\n        {\n            \"sigma\": np.array([1.0, 0.3, 0.09, 0.027]),\n            \"x_true\": np.array([1.0, 0.8, 0.6, 0.4]),\n            \"sigma_eps_sq\": 0.0001,\n            \"epsilon\": np.sqrt(0.0001) * np.array([1.0, -2.0, 1.0, -2.0])\n        }\n    ]\n\n    all_results = []\n    \n    N_lambda = 800\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        x_true = case[\"x_true\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        epsilon = case[\"epsilon\"]\n        \n        n = len(sigma)\n        \n        # Construct diagonal matrix A and observed data y\n        A = np.diag(sigma)\n        y = A @ x_true + epsilon\n        \n        # Set up logarithmically spaced grid for lambda\n        lambda_min = 1e-6 * np.max(sigma)\n        lambda_max = 1e2 * np.max(sigma)\n        lambda_grid = np.logspace(np.log10(lambda_min), np.log10(lambda_max), N_lambda)\n        \n        # Vectorized calculations over the lambda grid\n        # Reshape for broadcasting: sigma(n,1), lambda_grid(1, N)\n        sigma_col = sigma[:, np.newaxis]\n        x_true_col = x_true[:, np.newaxis]\n        y_col = y[:, np.newaxis]\n        lambda_sq = lambda_grid[np.newaxis, :]**2\n        \n        # --- Bias-Variance and Risk Calculation ---\n        # Denominator term for all calculations\n        denom = sigma_col**2 + lambda_sq # Shape (n, N_lambda)\n        \n        # Squared Bias\n        bias_sq_terms = (lambda_sq * x_true_col / denom)**2\n        bias_sq = np.sum(bias_sq_terms, axis=0)\n        \n        # Variance\n        variance_terms = sigma_col**2 / (denom**2)\n        variance = sigma_eps_sq * np.sum(variance_terms, axis=0)\n        \n        # Risk\n        risk = bias_sq + variance\n        \n        # Find risk-minimizing lambda\n        risk_min_idx = np.argmin(risk)\n        lambda_risk = lambda_grid[risk_min_idx]\n        \n        # --- L-Curve Calculation ---\n        # Solution norm squared\n        sol_norm_sq_terms = (sigma_col * y_col / denom)**2\n        sol_norm_sq = np.sum(sol_norm_sq_terms, axis=0)\n        \n        # Residual norm squared\n        res_norm_sq_terms = (lambda_sq * y_col / denom)**2\n        res_norm_sq = np.sum(res_norm_sq_terms, axis=0)\n        \n        # L-curve coordinates (log norms)\n        # Add a small epsilon to avoid log(0) if norms are numerically zero\n        rho = np.log(np.sqrt(res_norm_sq) + 1e-16)\n        eta = np.log(np.sqrt(sol_norm_sq) + 1e-16)\n        \n        # Parameter p = log(lambda)\n        p_grid = np.log(lambda_grid)\n        dp = p_grid[1] - p_grid[0]\n        \n        # First and second derivatives using central differences\n        rho_p = np.gradient(rho, dp)\n        eta_p = np.gradient(eta, dp)\n        rho_pp = np.gradient(rho_p, dp)\n        eta_pp = np.gradient(eta_p, dp)\n        \n        # Curvature\n        # Calculate on interior points to avoid edge effects of np.gradient\n        numerator = np.abs(rho_p * eta_pp - eta_p * rho_pp)\n        denominator = (rho_p**2 + eta_p**2)**1.5\n        # Avoid division by zero\n        curvature = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        \n        # Find lambda at max curvature (L-curve corner)\n        # Search on interior of grid to match derivative calculation\n        lcurve_corner_idx = np.argmax(curvature[1:-1]) + 1\n        lambda_L = lambda_grid[lcurve_corner_idx]\n        \n        # --- Synthesize Results ---\n        # Ratio of lambdas\n        ratio = lambda_L / lambda_risk\n        \n        # Number of activated modes at lambda_risk\n        # Mode i is activated if sigma_i >= lambda\n        activated_modes = np.sum(sigma >= lambda_risk)\n        \n        # Boolean check: within factor of 2\n        is_close = (0.5 = ratio = 2.0)\n        \n        all_results.extend([lambda_L, lambda_risk, ratio, int(activated_modes), is_close])\n\n    # Format and print the final output as a single list string\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}