{
    "hands_on_practices": [
        {
            "introduction": "我们从一个基础练习开始，直接比较两种引入先验知识的基本方法：对解施加严格的硬约束，以及应用灵活的软惩罚。这项练习将让你计算每种方法的偏置和方差，通过清晰的并列对比，展示不同的正则化策略如何从根本上改变偏置-方差的权衡。",
            "id": "3368100",
            "problem": "考虑一个三维线性逆问题，其中状态向量 $x \\in \\mathbb{R}^{3}$ 通过带噪声的单位测量进行观测。观测模型为 $y = x^{\\star} + \\varepsilon$，其中真实状态为 $x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$，噪声 $\\varepsilon$ 服从均值为零、协方差为 $\\sigma^{2} I_{3}$ 的多元正态分布，其中 $\\sigma^{2} = 1$，$I_{3}$ 是 $3 \\times 3$ 的单位矩阵。设 $L \\in \\mathbb{R}^{2 \\times 3}$ 是线性算子\n$$\nL = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix},\n$$\n它对 $x$ 的前两个分量施加了一个可行性限制。考虑两种正则化策略：\n\n1. 硬可行性约束 $L x = 0$，它将容许的估计值限制在 $L$ 的零空间内。\n\n2. 强度为 $\\lambda = 2$ 的软二次惩罚，它将 $\\lambda \\|L x\\|^{2}$ 添加到数据失配项中。\n\n从最小二乘估计、线性约束以及多元正态噪声的性质（零均值、协方差 $\\sigma^{2} I_{3}$）的定义出发，推导两种策略下的估计量，并计算：\n- 由硬可行性限制引起的偏置向量（定义为估计量的期望值减去 $x^{\\star}$）。\n- 由硬可行性限制引起的总方差变化（定义为估计量的协方差矩阵的迹相对于无约束最小二乘估计量的迹的差值）。\n- 由强度为 $\\lambda = 2$ 的软二次惩罚引起的偏置向量。\n- 在强度为 $\\lambda = 2$ 的软二次惩罚下，相对于无约束最小二乘估计量的总方差变化。\n\n将最终答案表示为一个单行矩阵，包含八个条目，顺序如下：硬约束偏置向量的三个分量、软惩罚偏置向量的三个分量、硬约束方差变化和软惩罚方差变化。请使用精确值，无需四舍五入。不涉及物理单位。",
            "solution": "该问题是适定的、有科学依据的，并为得到唯一解提供了所有必要信息。我们开始进行推导。\n\n该逆问题由观测模型 $y = x^{\\star} + \\varepsilon$ 描述，其中 $x^{\\star} \\in \\mathbb{R}^{3}$ 是真实状态，$y \\in \\mathbb{R}^{3}$ 是观测值，$\\varepsilon$ 是噪声向量。给定条件如下：\n- 真实状态：$x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$。\n- 噪声分布：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$，其均值 $\\mathbb{E}[\\varepsilon] = 0$，协方差 $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2} I_{3}$。\n- 噪声方差：$\\sigma^{2} = 1$。因此，$\\mathrm{Cov}(\\varepsilon) = I_{3}$。\n- 观测模型可以写成 $y = H x^{\\star} + \\varepsilon$，其中前向算子是单位矩阵 $H = I_{3}$。\n\n我们首先分析无约束最小二乘估计量，它将作为方差比较的基准。\n\n**无约束最小二乘估计量**\n无约束最小二乘估计量 $\\hat{x}_{\\text{LS}}$ 最小化数据失配（或残差平方和）$J(x) = \\|y - x\\|^{2}$。通过将梯度 $\\nabla_{x} J(x) = -2(y-x)$ 设为零来获得解，可得：\n$$\n\\hat{x}_{\\text{LS}} = y\n$$\n该估计量的期望为 $\\mathbb{E}[\\hat{x}_{\\text{LS}}] = \\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}$。偏置为 $\\text{Bias}(\\hat{x}_{\\text{LS}}) = \\mathbb{E}[\\hat{x}_{\\text{LS}}] - x^{\\star} = 0$，因此它是一个无偏估计量。\n该估计量的协方差为 $\\mathrm{Cov}(\\hat{x}_{\\text{LS}}) = \\mathrm{Cov}(y) = \\mathrm{Cov}(x^{\\star} + \\varepsilon) = \\mathrm{Cov}(\\varepsilon) = \\sigma^{2} I_{3} = I_{3}$。\n总方差是协方差矩阵的迹：\n$$\n\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = \\mathrm{Tr}(I_{3}) = 1 + 1 + 1 = 3\n$$\n\n**1. 硬可行性约束**\n该策略旨在最小化 $\\|y - x\\|^{2}$，受线性约束 $Lx = 0$ 的限制。算子由 $L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix}$ 给出。\n约束 $Lx=0$ 意味着：\n$$\n\\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies x_1 = 0 \\text{ 且 } x_2 = 0\n$$\n估计量必须位于 $L$ 的零空间中，该空间由向量 $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$ 张成。因此，估计量的形式为 $\\hat{x}_{\\text{hard}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\hat{x}_{3} \\end{pmatrix}$。\n将此代入目标函数：\n$$\nJ(x) = (y_1 - 0)^{2} + (y_2 - 0)^{2} + (y_3 - x_3)^{2}\n$$\n为了最小化关于 $x_3$ 的 $J(x)$，我们将导数设为零：$\\frac{\\partial J}{\\partial x_3} = -2(y_3 - x_3) = 0$，这得到 $\\hat{x}_3 = y_3$。\n因此，硬约束估计量为：\n$$\n\\hat{x}_{\\text{hard}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\n$$\n- **偏置向量（硬约束）：**\n估计量的期望为 $\\mathbb{E}[\\hat{x}_{\\text{hard}}] = \\mathbb{E}\\left[\\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\\right] = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\mathbb{E}[y_3] \\end{pmatrix}$。\n由于 $y_3 = x^{\\star}_3 + \\varepsilon_3$，我们有 $\\mathbb{E}[y_3] = x^{\\star}_3 + \\mathbb{E}[\\varepsilon_3] = 3 + 0 = 3$。\n所以，$\\mathbb{E}[\\hat{x}_{\\text{hard}}] = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n偏置向量是：\n$$\n\\text{Bias}_{\\text{hard}} = \\mathbb{E}[\\hat{x}_{\\text{hard}}] - x^{\\star} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n- **总方差变化（硬约束）：**\n估计量的协方差矩阵为 $\\mathrm{Cov}(\\hat{x}_{\\text{hard}}) = \\mathrm{Cov}\\left(\\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\\right)$。\n协方差矩阵的分量为 $\\mathrm{Cov}(\\hat{x}_{\\text{hard}})_{ij} = \\mathrm{Cov}((\\hat{x}_{\\text{hard}})_i, (\\hat{x}_{\\text{hard}})_j)$。\n前两个分量是常数（$0$），所以它们的方差以及与任何其他变量的协方差都为 $0$。唯一非零的元素是：\n$$\n\\mathrm{Var}((\\hat{x}_{\\text{hard}})_3) = \\mathrm{Var}(y_3) = \\mathrm{Var}(x^{\\star}_3 + \\varepsilon_3) = \\mathrm{Var}(\\varepsilon_3) = \\sigma^{2} = 1\n$$\n协方差矩阵为：\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{hard}}) = \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n总方差为 $\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{hard}})) = 0 + 0 + 1 = 1$。\n相对于无约束估计量的总方差变化为：\n$$\n\\Delta\\mathrm{Var}_{\\text{hard}} = \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{hard}})) - \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = 1 - 3 = -2\n$$\n\n**2. 软二次惩罚（Tikhonov 正则化）**\n此策略旨在最小化 $J(x) = \\|y - x\\|^{2} + \\lambda \\|L x\\|^{2}$，其中 $\\lambda=2$。这是一个 Tikhonov 正则化问题。目标函数是：\n$$\nJ(x) = (y-x)^{\\top}(y-x) + \\lambda x^{\\top}L^{\\top}Lx\n$$\n将梯度 $\\nabla_x J(x) = -2(y-x) + 2\\lambda L^{\\top}Lx$ 设为零：\n$$\n(I + \\lambda L^{\\top}L)x = y\n$$\n估计量为 $\\hat{x}_{\\text{soft}} = (I + \\lambda L^{\\top}L)^{-1} y$。\n我们来计算矩阵 $M = (I + \\lambda L^{\\top}L)^{-1}$。首先，我们计算 $L^{\\top}L$：\n$$\nL^{\\top}L = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n当 $\\lambda=2$ 时，我们有：\n$$\nI + \\lambda L^{\\top}L = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} + 2 \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 3  0  0 \\\\ 0  3  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n逆矩阵是：\n$$\nM = (I + \\lambda L^{\\top}L)^{-1} = \\begin{pmatrix} 1/3  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n估计量是 $\\hat{x}_{\\text{soft}} = M y = \\begin{pmatrix} y_1/3 \\\\ y_2/3 \\\\ y_3 \\end{pmatrix}$。\n\n- **偏置向量（软惩罚）：**\n估计量的期望是 $\\mathbb{E}[\\hat{x}_{\\text{soft}}] = \\mathbb{E}[M y] = M \\mathbb{E}[y] = M x^{\\star}$。\n$$\n\\mathbb{E}[\\hat{x}_{\\text{soft}}] = \\begin{pmatrix} 1/3  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ 3 \\end{pmatrix}\n$$\n偏置向量是：\n$$\n\\text{Bias}_{\\text{soft}} = \\mathbb{E}[\\hat{x}_{\\text{soft}}] - x^{\\star} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2/3 - 6/3 \\\\ -1/3 + 3/3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} -4/3 \\\\ 2/3 \\\\ 0 \\end{pmatrix}\n$$\n\n- **总方差变化（软惩罚）：**\n估计量的协方差是 $\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = \\mathrm{Cov}(M y) = \\mathrm{Cov}(M(x^{\\star}+\\varepsilon)) = M \\mathrm{Cov}(\\varepsilon) M^{\\top}$。\n由于 $\\mathrm{Cov}(\\varepsilon) = I_3$ 且 $M$ 是对称的（$M=M^{\\top}$）：\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = M I_3 M = M^2\n$$\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = \\begin{pmatrix} 1/3  0  0 \\\\ 0  1/3  0 \\\\ 0  0  1 \\end{pmatrix}^2 = \\begin{pmatrix} 1/9  0  0 \\\\ 0  1/9  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n总方差为 $\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{soft}})) = 1/9 + 1/9 + 1 = 2/9 + 9/9 = 11/9$。\n相对于无约束估计量的总方差变化为：\n$$\n\\Delta\\mathrm{Var}_{\\text{soft}} = \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{soft}})) - \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = \\frac{11}{9} - 3 = \\frac{11}{9} - \\frac{27}{9} = -\\frac{16}{9}\n$$\n\n**结果摘要**\n- 硬约束偏置向量：`(−2, 1, 0)`\n- 软惩罚偏置向量：`(−4/3, 2/3, 0)`\n- 硬约束方差变化：`−2`\n- 软惩罚方差变化：`−16/9`\n\n最终答案是一个包含这八个值的单行矩阵，按指定顺序排列。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -2  1  0  -\\frac{4}{3}  \\frac{2}{3}  0  -2  -\\frac{16}{9} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在我们对偏置有了理解的基础上，这个练习将焦点转移到另一种流行的正则化方法——截断奇异值分解 (Truncated Singular Value Decomposition, TSVD)。在这里，你将使用偏差原理 (discrepancy principle)，一个广泛应用的启发式规则，来选择正则化参数（即截断水平 $k$）。这项练习旨在演示一种实用的参数选择方法，并将其与因丢弃奇异分量而产生的偏置直接联系起来。",
            "id": "3368040",
            "problem": "考虑一个线性反问题，其正向算子为 $A \\in \\mathbb{R}^{m \\times n}$（其中 $m=n=6$），测量模型为 $y = A x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ 且与 $x$ 独立。设 $A$ 的奇异值分解 (SVD) 为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{6 \\times 6}$ 和 $V \\in \\mathbb{R}^{6 \\times 6}$ 是正交矩阵，$\\Sigma = \\operatorname{diag}(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}, s_{6})$，奇异值为 $s_{1} \\geq s_{2} \\geq s_{3} \\geq s_{4} \\geq s_{5} \\geq s_{6} > 0$。将在截断水平 $k \\in \\{0,1,2,3,4,5,6\\}$ 下的截断奇异值分解 (TSVD) 估计量 $\\hat{x}_{k} \\in \\mathbb{R}^{6}$ 定义为仅保留前 $k$ 个奇异分量的估计量。\n\n给定奇异值 $s_{1} = 20$, $s_{2} = 10$, $s_{3} = 5$, $s_{4} = 2.5$, $s_{5} = 1.25$, $s_{6} = 0.6$，以及真实参数 $x$ 在右奇异向量基 $V$ 中的系数：\n$$\n\\alpha_{i} \\equiv v_{i}^{\\top} x, \\quad \\alpha_{1} = 1,\\ \\alpha_{2} = \\frac{1}{2},\\ \\alpha_{3} = \\frac{1}{4},\\ \\alpha_{4} = \\frac{1}{8},\\ \\alpha_{5} = \\frac{1}{16},\\ \\alpha_{6} = \\frac{1}{32}.\n$$\n假设测量噪声方差为 $\\sigma^{2} = 1$（因此 $m \\sigma^{2} = 6$）。实现的测量值 $y$ 满足 $u_{i}^{\\top} y = \\gamma_{i}$，其中左奇异向量系数为\n$$\n\\gamma_{1} = 21,\\quad \\gamma_{2} = 3,\\quad \\gamma_{3} = 4.25,\\quad \\gamma_{4} = -0.6875,\\quad \\gamma_{5} = 0.578125,\\quad \\gamma_{6} = 0.01875,\n$$\n并且这些系数通过 $u_{i}^{\\top} y = s_{i} \\alpha_{i} + \\varepsilon_{i}$ 与模型一致，其中 $u_{i}^{\\top} \\varepsilon = \\varepsilon_{i}$ 由 $\\varepsilon_{1} = 1$, $\\varepsilon_{2} = -2$, $\\varepsilon_{3} = 3$, $\\varepsilon_{4} = -1$, $\\varepsilon_{5} = \\frac{1}{2}$, $\\varepsilon_{6} = 0$ 给出。\n\n从 SVD 和 TSVD 构造的基本原理出发，应用差异原则通过强制不等式 $\\|A \\hat{x}_{k} - y\\|^{2} \\leq m \\sigma^{2}$ 成立来选择截断水平 $k$，并选择满足此条件的最小整数 $k$。然后，计算在所选 $k$ 值下 TSVD 估计量导致的参数空间偏差，其定义为\n$$\n\\sum_{i>k} \\left(v_{i}^{\\top} x\\right)^{2}.\n$$\n最终答案仅提供该偏差的数值。无需四舍五入；报告精确值。",
            "solution": "该问题要求我们首先使用差异原则确定截断奇异值分解 (TSVD) 估计量的最优截断水平 $k$，然后计算由特定公式给出的相应参数空间偏差。\n\n设线性反问题由模型 $y = A x + \\varepsilon$ 描述，其中 $y \\in \\mathbb{R}^{m}$ 表示测量值，$x \\in \\mathbb{R}^{n}$ 是未知参数向量，$A \\in \\mathbb{R}^{m \\times n}$ 是正向算子，$\\varepsilon \\in \\mathbb{R}^{m}$ 是噪声向量。给定 $m=n=6$。噪声假定服从正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$，且 $\\sigma^2=1$。\n\n矩阵 $A$ 的奇异值分解 (SVD) 为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，其列分别为左奇异向量 $\\{u_i\\}$ 和右奇异向量 $\\{v_i\\}$。$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个对角矩阵，其对角线上的非负奇异值 $s_i$ 按 $s_1 \\geq s_2 \\geq \\dots \\geq s_n > 0$ 的顺序排列。\n\n在截断水平 $k$ 下，对 $x$ 的 TSVD 估计量定义为\n$$\n\\hat{x}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{s_{i}} v_{i}\n$$\n问题给出了系数 $\\gamma_i = u_i^\\top y$，因此估计量可以写成\n$$\n\\hat{x}_{k} = \\sum_{i=1}^{k} \\frac{\\gamma_{i}}{s_{i}} v_{i}\n$$\n下一步是计算与此估计量相关的残差 $A\\hat{x}_k - y$。\n$$\nA\\hat{x}_{k} = \\left( \\sum_{j=1}^{m} s_j u_j v_j^\\top \\right) \\left( \\sum_{i=1}^{k} \\frac{\\gamma_{i}}{s_{i}} v_{i} \\right)\n$$\n利用右奇异向量的正交性 $v_j^\\top v_i = \\delta_{ij}$（克罗内克δ），我们得到\n$$\nA\\hat{x}_{k} = \\sum_{j=1}^{m} \\sum_{i=1}^{k} s_j u_j \\frac{\\gamma_i}{s_i} \\delta_{ij} = \\sum_{i=1}^{k} s_i u_i \\frac{\\gamma_i}{s_i} = \\sum_{i=1}^{k} \\gamma_i u_i\n$$\n数据向量 $y$ 可以在左奇异向量的基中展开为 $y = \\sum_{i=1}^{m} (u_i^\\top y) u_i = \\sum_{i=1}^{m} \\gamma_i u_i$。\n因此，残差为\n$$\nA\\hat{x}_{k} - y = \\sum_{i=1}^{k} \\gamma_i u_i - \\sum_{i=1}^{m} \\gamma_i u_i = - \\sum_{i=k+1}^{m} \\gamma_i u_i\n$$\n利用左奇异向量的正交性 ($u_i^\\top u_j = \\delta_{ij}$)，残差的欧几里得范数的平方为\n$$\n\\|A\\hat{x}_{k} - y\\|^2 = \\left\\| - \\sum_{i=k+1}^{m} \\gamma_i u_i \\right\\|^2 = \\sum_{i=k+1}^{m} \\gamma_i^2\n$$\n差异原则指出，我们应该选择最小的整数 $k$，使得残差范数的平方不超过来自噪声的总期望贡献，即 $\\|A\\hat{x}_{k} - y\\|^2 \\leq m \\sigma^2$。当 $m=6$ 且 $\\sigma^2=1$ 时，该条件为\n$$\n\\sum_{i=k+1}^{6} \\gamma_i^2 \\leq 6\n$$\n我们已知 $\\gamma_i$ 的值：\n$\\gamma_1 = 21$, $\\gamma_2 = 3$, $\\gamma_3 = 4.25$, $\\gamma_4 = -0.6875$, $\\gamma_5 = 0.578125$, $\\gamma_6 = 0.01875$.\n让我们计算它们的平方：\n$\\gamma_1^2 = 21^2 = 441$\n$\\gamma_2^2 = 3^2 = 9$\n$\\gamma_3^2 = (4.25)^2 = 18.0625$\n$\\gamma_4^2 = (-0.6875)^2 = 0.47265625$\n$\\gamma_5^2 = (0.578125)^2 = 0.334228515625$\n$\\gamma_6^2 = (0.01875)^2 = 0.0003515625$\n\n现在我们对递增的 $k \\in \\{0, 1, 2, 3, 4, 5, 6\\}$ 值检验该条件：\n对于 $k=0$：$\\sum_{i=1}^{6} \\gamma_i^2 = 441 + 9 + 18.0625 + \\dots = 468.869... > 6$。条件不成立。\n对于 $k=1$：$\\sum_{i=2}^{6} \\gamma_i^2 = 9 + 18.0625 + \\dots = 27.869... > 6$。条件不成立。\n对于 $k=2$：$\\sum_{i=3}^{6} \\gamma_i^2 = 18.0625 + 0.47265625 + \\dots = 18.869... > 6$。条件不成立。\n对于 $k=3$：$\\sum_{i=4}^{6} \\gamma_i^2 = \\gamma_4^2 + \\gamma_5^2 + \\gamma_6^2 = 0.47265625 + 0.334228515625 + 0.0003515625 = 0.807236328125$。\n由于 $0.807236328125 \\leq 6$，条件满足。因为这是使条件成立的最小 $k$ 值，所以选择的截断水平为 $k=3$。\n\n问题要求计算参数空间偏差的值，其明确定义为 $\\sum_{i>k} (v_i^\\top x)^2$。对于所选的 $k=3$，我们需要计算\n$$\n\\text{Bias} = \\sum_{i=4}^{6} (v_i^\\top x)^2\n$$\n系数 $\\alpha_i = v_i^\\top x$ 已给出：\n$\\alpha_4 = \\frac{1}{8}$\n$\\alpha_5 = \\frac{1}{16}$\n$\\alpha_6 = \\frac{1}{32}$\n\n将这些值代入偏差的表达式中：\n$$\n\\text{Bias} = \\alpha_4^2 + \\alpha_5^2 + \\alpha_6^2 = \\left(\\frac{1}{8}\\right)^2 + \\left(\\frac{1}{16}\\right)^2 + \\left(\\frac{1}{32}\\right)^2\n$$\n$$\n\\text{Bias} = \\frac{1}{64} + \\frac{1}{256} + \\frac{1}{1024}\n$$\n为将这些分数相加，我们使用公分母 $1024$：\n$$\n\\frac{1}{64} = \\frac{1 \\times 16}{64 \\times 16} = \\frac{16}{1024}\n$$\n$$\n\\frac{1}{256} = \\frac{1 \\times 4}{256 \\times 4} = \\frac{4}{1024}\n$$\n总和为\n$$\n\\text{Bias} = \\frac{16}{1024} + \\frac{4}{1024} + \\frac{1}{1024} = \\frac{16+4+1}{1024} = \\frac{21}{1024}\n$$\n这就是最终的精确数值。",
            "answer": "$$\n\\boxed{\\frac{21}{1024}}\n$$"
        },
        {
            "introduction": "为了综合这些概念，我们的最后一个练习将从解析计算转向计算探索。你将通过编程实现吉洪诺夫正则化 (Tikhonov regularization) 并分析整个正则化路径。这项练习将理论上的风险最小值、启发式的 L 曲线拐点，以及“激活”奇异模式的直观概念联系起来，从而提供一个关于正则化参数 $\\lambda$ 如何控制解的性质的整体视图。",
            "id": "3368069",
            "problem": "考虑一个有限维欧几里得空间中的加性噪声线性反问题：给定一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个真实状态 $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ 以及噪声 $ \\varepsilon \\in \\mathbb{R}^{m}$，观测数据为 $y = A x_{\\mathrm{true}} + \\varepsilon$，其中噪声被建模为零均值、分量独立且方差为 $\\sigma_{\\varepsilon}^{2}$。对于一个正则化参数 $\\lambda > 0$，将 Tikhonov 正则化解 $x_{\\lambda}$ 定义为泛函 $\\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$ 的最小化子。假设 $A$ 的奇异值分解为 $A = U \\Sigma V^{\\top}$，其奇异值为 $\\{\\sigma_{i}\\}_{i=1}^{r}$，其中 $r = \\min(m,n)$ 且 $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} > 0$。\n\n您的任务如下，并且所有推导都必须从 Tikhonov 正则化、奇异值分解以及均方误差的标准偏差-方差分解的定义开始；不要使用任何未从这些基础推导出的预打包风险或曲率公式。\n\n- 定义正则化路径 $\\{x_{\\lambda} : \\lambda > 0\\}$，并通过为每个奇异值 $\\sigma_{i}$ 识别一个在 $\\lambda$ 中的确定性阈值来刻画奇异模式在此路径上的激活过程，在该阈值处，相应奇异方向的贡献从被抑制过渡到显著存在。使用基于 Tikhonov 解在奇异向量基中隐含的滤波因子的有原则的准则来形式化此阈值，并将此阈值视为模式 $i$ 的相变点。用奇异值明确陈述这些阈值。\n\n- 将 L 曲线定义为由 $\\log \\lambda$ 参数化的参数化平面曲线 $\\left(\\log \\|A x_{\\lambda} - y\\|_{2}, \\log \\|x_{\\lambda}\\|_{2}\\right)$。使用平面参数化曲线的曲率定义（以其关于参数的一阶和二阶导数表示），实现一个数值程序，以在一系列 $\\lambda$值的网格上估计曲率，并将 L 曲线的拐角识别为曲率达到其最大值时的正则化参数 $\\lambda$。您必须用 $\\log \\lambda$ 对曲线进行参数化，并使用数值稳定的有限差分。\n\n- 在奇异向量基下，使用期望平方误差 $\\mathbb{E}\\left[\\|x_{\\lambda} - x_{\\mathrm{true}}\\|_{2}^{2}\\right]$ 的偏差-方差分解，推导并实现一个表示均方风险作为 $\\lambda$ 的函数的表达式，并在一个对数间隔的网格上识别风险最小化的正则化参数 $\\lambda_{\\mathrm{risk}}$。您的推导必须明确区分偏差和方差的贡献。\n\n- 通过为每个测试用例报告以下内容，将相变位置、L 曲线拐角和风险最小值联系起来：L 曲线拐角 $\\lambda_{\\mathrm{L}}$、风险最小值 $\\lambda_{\\mathrm{risk}}$、比率 $\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}$、在 $\\lambda_{\\mathrm{risk}}$ 处根据您的激活准则激活的模式数量，以及一个布尔值，指示 $\\lambda_{\\mathrm{L}}$ 和 $\\lambda_{\\mathrm{risk}}$ 是否在彼此的 $2$ 倍因子范围内。\n\n实现要求和测试套件：\n\n- 您必须将 $A$ 构建为具有指定奇异值的对角矩阵（因此 $U = I$ 和 $V = I$），并且必须使用由给定目标函数定义的精确 Tikhonov 解。数值曲率必须在 $\\log \\lambda$ 的均匀网格上使用中心有限差分计算。\n\n- 对 $\\lambda$ 使用一个对数间隔的网格，包含 $N_{\\lambda} = 800$ 个点，范围在 $\\lambda_{\\min} = 10^{-6} \\cdot \\max_{i} \\sigma_{i}$ 和 $\\lambda_{\\max} = 10^{2} \\cdot \\max_{i} \\sigma_{i}$ 之间（含两端）。\n\n- 对于 L 曲线，使用在每个测试用例中由指定的 $\\varepsilon$ 构建的实现数据 $y$。对于风险，使用由提供的 $\\sigma_{\\varepsilon}^{2}$ 计算的期望风险。\n\n- 将模式 $i$ 的激活阈值定义为对应模式的 Tikhonov 滤波因子等于 $1/2$ 时的 $\\lambda$，并将在给定 $\\lambda$ 下，滤波因子至少为 $1/2$ 的模式计为已激活。\n\n- 使用以下三个测试用例，每个用例中 $m = n$，并使用指定的确定性噪声向量 $\\varepsilon$：\n    - 测试用例 1：\n        - 奇异值：$\\sigma = [10.0, 1.0, 0.1]$。\n        - 真实状态：$x_{\\mathrm{true}} = [1.0, 1.0, 1.0]$。\n        - 噪声方差：$\\sigma_{\\varepsilon}^{2} = 0.01$。\n        - 噪声向量：$\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, -1.0, 1.0]$。\n    - 测试用例 2：\n        - 奇异值：$\\sigma = [5.0, 4.0, 3.0, 2.0, 1.0]$。\n        - 真实状态：$x_{\\mathrm{true}} = [1.0, 0.5, 0.25, 0.125, 0.0625]$。\n        - 噪声方差：$\\sigma_{\\varepsilon}^{2} = 0.04$。\n        - 噪声向量：$\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, 0.0, -1.0, 0.0, 1.0]$。\n    - 测试用例 3：\n        - 奇异值：$\\sigma = [1.0, 0.3, 0.09, 0.027]$。\n        - 真实状态：$x_{\\mathrm{true}} = [1.0, 0.8, 0.6, 0.4]$。\n        - 噪声方差：$\\sigma_{\\varepsilon}^{2} = 0.0001$。\n        - 噪声向量：$\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, -2.0, 1.0, -2.0]$。\n\n最终输出格式：\n\n- 您的程序应生成一行输出，其中包含三个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表。该列表必须按测试用例 1、2 和 3 的顺序连接每个用例的以下五个项目：\n    - $\\lambda_{\\mathrm{L}}$，浮点数，\n    - $\\lambda_{\\mathrm{risk}}$，浮点数，\n    - $\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}$，浮点数，\n    - 在 $\\lambda_{\\mathrm{risk}}$ 处激活的模式数量，整数，\n    - 一个布尔值，指示 $\\max\\left(\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}, \\lambda_{\\mathrm{risk}} / \\lambda_{\\mathrm{L}}\\right) \\le 2$ 是否成立。\n\n因此，输出列表包含 $15$ 个条目：每个测试用例 5 个条目，跨 3 个用例连接，并在一行上打印为单个 Python 风格的列表，不带任何附加文本。",
            "solution": "该问题要求对一个线性反问题的 Tikhonov 正则化进行全面分析，重点关注正则化参数 $\\lambda$、模型复杂度（激活的奇异模式）、预测误差（L 曲线）和估计误差（统计风险）之间的联系。分析从第一性原理出发。\n\n### 1. Tikhonov 正则化与正则化路径\n\n问题是为线性系统 $y = Ax_{\\mathrm{true}} + \\varepsilon$ 寻找一个稳定解 $x$，其中 $y \\in \\mathbb{R}^m$ 是观测数据，$A \\in \\mathbb{R}^{m \\times n}$ 是前向算子，$x_{\\mathrm{true}} \\in \\mathbb{R}^n$ 是真实的潜在状态，$\\varepsilon \\in \\mathbb{R}^m$ 是加性噪声。\n\nTikhonov 正则化通过最小化以下泛函来寻找解：\n$$\nJ_{\\lambda}(x) = \\|Ax - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}\n$$\n其中 $\\lambda > 0$ 是正则化参数。第一项强制对数据保真，而第二项惩罚大的解，从而强制稳定性。解 $x_{\\lambda}$ 是通过将 $J_{\\lambda}(x)$ 对 $x$ 的梯度设为零找到的：\n$$\n\\nabla_{x} J_{\\lambda}(x) = 2A^{\\top}(Ax - y) + 2\\lambda^{2}x = 0\n$$\n这导致了正规方程：\n$$\n(A^{\\top}A + \\lambda^{2}I)x = A^{\\top}y\n$$\n因此，Tikhonov 正则化解为：\n$$\nx_{\\lambda} = (A^{\\top}A + \\lambda^{2}I)^{-1}A^{\\top}y\n$$\n解的集合 $\\{x_{\\lambda} : \\lambda > 0\\}$ 构成了正则化路径。\n\n为了分析 $x_{\\lambda}$ 的行为，我们使用 $A$ 的奇异值分解（SVD）：$A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个矩形对角矩阵，其对角线上有非负奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$，其中 $r = \\text{rank}(A)$。\n\n将 SVD 代入 $x_{\\lambda}$ 的表达式中：\n$A^{\\top}A = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top} = V\\Sigma^{\\top}\\Sigma V^{\\top}$。\n$A^{\\top}y = (U\\Sigma V^{\\top})^{\\top}y = V\\Sigma^{\\top}U^{\\top}y$。\n所以，$(V\\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda^2 I)x_\\lambda = V\\Sigma^{\\top}U^{\\top}y$。\n从左边乘以 $V^{\\top}$ 并注意 $V^{\\top}V=I$：\n$(\\Sigma^{\\top}\\Sigma + \\lambda^2 I)V^{\\top}x_\\lambda = \\Sigma^{\\top}U^{\\top}y$。\n令 $\\hat{x}_{\\lambda} = V^{\\top}x_{\\lambda}$ 和 $\\hat{y} = U^{\\top}y$ 为投影到奇异向量基上的解和数据。矩阵 $\\Sigma^{\\top}\\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角线元素对于 $i=1, \\dots, r$ 为 $\\sigma_i^2$，其他为零。方程变为对角化的：\n$$\n(\\sigma_i^2 + \\lambda^2)(\\hat{x}_{\\lambda})_i = \\sigma_i (\\hat{y})_i \\quad \\text{for } i \\le r\n$$\n在 $V$ 基中第 $i$ 个分量的解是：\n$$\n(\\hat{x}_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2}(\\hat{y})_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}\\right) \\frac{(\\hat{y})_i}{\\sigma_i}\n$$\n项 $f_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$ 是第 $i$ 个奇异模式的 Tikhonov 滤波因子。它决定了第 $i$ 个数据分量 $(\\hat{y})_i / \\sigma_i$ 对解分量 $(\\hat{x}_{\\lambda})_i$ 的贡献程度。\n对于 $\\lambda \\ll \\sigma_i$，$f_i(\\lambda) \\approx 1$，模式被通过。\n对于 $\\lambda \\gg \\sigma_i$，$f_i(\\lambda) \\approx 0$，模式被滤除。\n问题将激活阈值定义为滤波因子为 $1/2$ 的点：\n$$\nf_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} = \\frac{1}{2} \\implies 2\\sigma_i^2 = \\sigma_i^2 + \\lambda^2 \\implies \\lambda^2 = \\sigma_i^2\n$$\n由于 $\\lambda > 0$ 和 $\\sigma_i > 0$，模式 $i$ 的激活阈值为 $\\lambda = \\sigma_i$。\n在给定的 $\\lambda$ 下，如果 $f_i(\\lambda) \\ge 1/2$，则该模式被计为已激活，这等价于 $\\sigma_i^2 \\ge \\lambda^2$，或 $\\sigma_i \\ge \\lambda$。\n\n### 2. L 曲线拐角识别\n\nL 曲线是 $(\\rho_\\lambda, \\eta_\\lambda)$ 的参数图，其中 $\\rho_\\lambda = \\log \\|A x_{\\lambda} - y\\|_{2}$ 和 $\\eta_\\lambda = \\log \\|x_{\\lambda}\\|_{2}$，由 $p = \\log \\lambda$ 参数化。L 曲线的“拐角”通常对应于一个好的 $\\lambda$ 选择，它平衡了数据保真度和解的稳定性。拐角是通过找到最大曲率点来定位的。\n\n问题指定 $A$ 为对角矩阵，所以 $U=V=I$ 且 $m=n=r$。SVD 是平凡的，有 $A_{ii} = \\sigma_i$。解和残差的分量为：\n$$\n(x_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} y_i\n$$\n$$\n(Ax_{\\lambda} - y)_i = \\sigma_i (x_{\\lambda})_i - y_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right) y_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} y_i\n$$\nL 曲线所需的范数平方为：\n$$\nS_\\lambda = \\|x_{\\lambda}\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i y_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\n$$\nR_\\lambda = \\|Ax_{\\lambda} - y\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 y_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\nL 曲线上的点是 $(\\rho(\\lambda), \\eta(\\lambda)) = (\\log\\sqrt{R_\\lambda}, \\log\\sqrt{S_\\lambda}) = (\\frac{1}{2}\\log R_\\lambda, \\frac{1}{2}\\log S_\\lambda)$。参数曲线 $(x(p), y(p))$ 的曲率 $K$ 由下式给出：\n$$\nK(p) = \\frac{|x'y'' - y'x''|}{(x'^2 + y'^2)^{3/2}}\n$$\n其中撇号表示关于参数 $p = \\log \\lambda$ 的导数。\n在数值上，我们在 $p_j = \\log \\lambda_j$ 的均匀网格上计算曲线点 $(\\rho_j, \\eta_j)$。导数使用中心有限差分进行近似。设 $\\Delta p = p_{j+1} - p_j$：\n$$\n\\rho'_j \\approx \\frac{\\rho_{j+1} - \\rho_{j-1}}{2 \\Delta p}, \\quad \\rho''_j \\approx \\frac{\\rho_{j+1} - 2\\rho_j + \\rho_{j-1}}{(\\Delta p)^2}\n$$\n对 $\\eta_j$ 也类似。将这些近似值代入曲率公式以找到 $K_j$。正则化参数 $\\lambda_L$ 是使 $K_j$ 最大化的 $\\lambda_j$ 值。\n\n### 3. 偏差-方差分解与风险最小化\n\n估计量 $x_\\lambda$ 的质量由均方误差（或风险）来衡量，定义为 $Risk(\\lambda) = \\mathbb{E}[\\|x_\\lambda - x_{\\mathrm{true}}\\|_2^2]$，其中期望是关于噪声 $\\varepsilon$ 的分布。噪声是零均值的（$\\mathbb{E}[\\varepsilon] = 0$），具有独立分量和方差 $\\sigma_\\varepsilon^2$（$\\mathbb{E}[\\varepsilon\\varepsilon^\\top] = \\sigma_\\varepsilon^2 I$）。\n\n风险可以分解为偏差平方和方差：\n$$\nRisk(\\lambda) = \\underbrace{\\|\\mathbb{E}[x_\\lambda] - x_{\\mathrm{true}}\\|_2^2}_{\\text{偏差平方}} + \\underbrace{\\mathbb{E}[\\|x_\\lambda - \\mathbb{E}[x_\\lambda]\\|_2^2]}_{\\text{方差}}\n$$\n\n首先，我们计算解的期望值。由于 $y = Ax_{\\mathrm{true}} + \\varepsilon$ 和期望的线性性：\n$$\n\\mathbb{E}[x_\\lambda] = \\mathbb{E}[(A^\\top A + \\lambda^2 I)^{-1} A^\\top (A x_{\\mathrm{true}} + \\varepsilon)] = (A^\\top A + \\lambda^2 I)^{-1} A^\\top A x_{\\mathrm{true}}\n$$\n偏差是 $b(\\lambda) = \\mathbb{E}[x_\\lambda] - x_{\\mathrm{true}} = [(A^\\top A + \\lambda^2 I)^{-1} A^\\top A - I]x_{\\mathrm{true}}$。\n在 $V$ 的基中使用 SVD 分量，偏差的第 $i$ 个分量是：\n$$\n(\\hat{b}(\\lambda))_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right)(\\hat{x}_{\\mathrm{true}})_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}(\\hat{x}_{\\mathrm{true}})_i\n$$\n偏差的范数平方是 $\\|\\hat{b}(\\lambda)\\|_2^2$，对于问题的对角矩阵 $A$（$V=I, \\hat{x}_{\\text{true}}=x_{\\text{true}}$）而言，它是：\n$$\n\\text{偏差平方} = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 (x_{\\mathrm{true}})_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\n接下来，我们计算方差。与均值的偏差是：\n$$\nx_\\lambda - \\mathbb{E}[x_\\lambda] = (A^\\top A + \\lambda^2 I)^{-1} A^\\top \\varepsilon\n$$\n方差是该量的期望平方范数：\n$$\n\\text{方差} = \\mathbb{E}[\\varepsilon^\\top A (A^\\top A + \\lambda^2 I)^{-2} A^\\top \\varepsilon]\n$$\n使用随机向量二次型的迹恒等式 $\\mathbb{E}[z^\\top Q z]=\\text{Tr}(Q \\text{Cov}(z))$，其中 $z=\\varepsilon$ 且 $\\text{Cov}(\\varepsilon) = \\sigma_\\varepsilon^2 I$：\n$$\n\\text{方差} = \\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top \\sigma_\\varepsilon^2 I) = \\sigma_\\varepsilon^2 \\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top)\n$$\n使用迹的循环性质和 SVD：\n$$\n\\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top) = \\text{Tr}((A^\\top A + \\lambda^2 I)^{-2} A^\\top A) = \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\n所以方差是：\n$$\n\\text{方差} = \\sigma_\\varepsilon^2 \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\n总风险是偏差平方和方差之和：\n$$\nRisk(\\lambda) = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 (x_{\\mathrm{true}})_i}{\\sigma_i^2 + \\lambda^2} \\right)^2 + \\sigma_\\varepsilon^2 \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\n风险最小化参数 $\\lambda_{\\mathrm{risk}}$ 是通过在 $\\lambda$ 值的网格上最小化此函数找到的。\n\n### 4. 综合与报告\n\n最后一步是为每个测试用例整合这些分析。\n1.  计算 L 曲线及其曲率以找到 $\\lambda_L$。\n2.  计算风险函数并找到其最小化子 $\\lambda_{\\mathrm{risk}}$。\n3.  计算比率 $\\lambda_L / \\lambda_{\\mathrm{risk}}$。\n4.  通过计算满足 $\\sigma_i \\ge \\lambda_{\\mathrm{risk}}$ 的奇异值 $\\sigma_i$ 的数量，来统计在 $\\lambda = \\lambda_{\\mathrm{risk}}$ 时激活的模式数量。\n5.  确定 $\\lambda_L$ 和 $\\lambda_{\\mathrm{risk}}$ 是否在彼此的 2 倍因子范围内，即 $1/2 \\le \\lambda_L / \\lambda_{\\mathrm{risk}} \\le 2$。\n\n此过程提供了对正则化的多方面视角，将启发式方法（L 曲线）、统计最优性准则（风险最小化）和模型复杂度解释（激活模式数量）联系起来。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization analysis problem for three test cases.\n\n    For each case, it calculates:\n    1. lambda_L: The regularization parameter at the L-curve corner.\n    2. lambda_risk: The risk-minimizing regularization parameter.\n    3. The ratio lambda_L / lambda_risk.\n    4. The number of activated singular modes at lambda_risk.\n    5. A boolean indicating if lambda_L and lambda_risk are within a factor of 2.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"sigma\": np.array([10.0, 1.0, 0.1]),\n            \"x_true\": np.array([1.0, 1.0, 1.0]),\n            \"sigma_eps_sq\": 0.01,\n            \"epsilon\": np.sqrt(0.01) * np.array([1.0, -1.0, 1.0])\n        },\n        {\n            \"sigma\": np.array([5.0, 4.0, 3.0, 2.0, 1.0]),\n            \"x_true\": np.array([1.0, 0.5, 0.25, 0.125, 0.0625]),\n            \"sigma_eps_sq\": 0.04,\n            \"epsilon\": np.sqrt(0.04) * np.array([1.0, 0.0, -1.0, 0.0, 1.0])\n        },\n        {\n            \"sigma\": np.array([1.0, 0.3, 0.09, 0.027]),\n            \"x_true\": np.array([1.0, 0.8, 0.6, 0.4]),\n            \"sigma_eps_sq\": 0.0001,\n            \"epsilon\": np.sqrt(0.0001) * np.array([1.0, -2.0, 1.0, -2.0])\n        }\n    ]\n\n    all_results = []\n    \n    N_lambda = 800\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        x_true = case[\"x_true\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        epsilon = case[\"epsilon\"]\n        \n        n = len(sigma)\n        \n        # Construct diagonal matrix A and observed data y\n        A = np.diag(sigma)\n        y = A @ x_true + epsilon\n        \n        # Set up logarithmically spaced grid for lambda\n        lambda_min = 1e-6 * np.max(sigma)\n        lambda_max = 1e2 * np.max(sigma)\n        lambda_grid = np.logspace(np.log10(lambda_min), np.log10(lambda_max), N_lambda)\n        \n        # Vectorized calculations over the lambda grid\n        # Reshape for broadcasting: sigma(n,1), lambda_grid(1, N)\n        sigma_col = sigma[:, np.newaxis]\n        x_true_col = x_true[:, np.newaxis]\n        y_col = y[:, np.newaxis]\n        lambda_sq = lambda_grid[np.newaxis, :]**2\n        \n        # --- Bias-Variance and Risk Calculation ---\n        # Denominator term for all calculations\n        denom = sigma_col**2 + lambda_sq # Shape (n, N_lambda)\n        \n        # Squared Bias\n        bias_sq_terms = (lambda_sq * x_true_col / denom)**2\n        bias_sq = np.sum(bias_sq_terms, axis=0)\n        \n        # Variance\n        variance_terms = sigma_col**2 / (denom**2)\n        variance = sigma_eps_sq * np.sum(variance_terms, axis=0)\n        \n        # Risk\n        risk = bias_sq + variance\n        \n        # Find risk-minimizing lambda\n        risk_min_idx = np.argmin(risk)\n        lambda_risk = lambda_grid[risk_min_idx]\n        \n        # --- L-Curve Calculation ---\n        # Solution norm squared\n        sol_norm_sq_terms = (sigma_col * y_col / denom)**2\n        sol_norm_sq = np.sum(sol_norm_sq_terms, axis=0)\n        \n        # Residual norm squared\n        res_norm_sq_terms = (lambda_sq * y_col / denom)**2\n        res_norm_sq = np.sum(res_norm_sq_terms, axis=0)\n        \n        # L-curve coordinates (log norms)\n        # Add a small epsilon to avoid log(0) if norms are numerically zero\n        rho = np.log(np.sqrt(res_norm_sq) + 1e-16)\n        eta = np.log(np.sqrt(sol_norm_sq) + 1e-16)\n        \n        # Parameter p = log(lambda)\n        p_grid = np.log(lambda_grid)\n        dp = p_grid[1] - p_grid[0]\n        \n        # First and second derivatives using central differences\n        rho_p = np.gradient(rho, dp)\n        eta_p = np.gradient(eta, dp)\n        rho_pp = np.gradient(rho_p, dp)\n        eta_pp = np.gradient(eta_p, dp)\n        \n        # Curvature\n        # Calculate on interior points to avoid edge effects of np.gradient\n        numerator = np.abs(rho_p * eta_pp - eta_p * rho_pp)\n        denominator = (rho_p**2 + eta_p**2)**1.5\n        # Avoid division by zero\n        curvature = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        \n        # Find lambda at max curvature (L-curve corner)\n        # Search on interior of grid to match derivative calculation\n        lcurve_corner_idx = np.argmax(curvature[1:-1]) + 1\n        lambda_L = lambda_grid[lcurve_corner_idx]\n        \n        # --- Synthesize Results ---\n        # Ratio of lambdas\n        ratio = lambda_L / lambda_risk\n        \n        # Number of activated modes at lambda_risk\n        # Mode i is activated if sigma_i >= lambda\n        activated_modes = np.sum(sigma >= lambda_risk)\n        \n        # Boolean check: within factor of 2\n        is_close = (0.5 = ratio = 2.0)\n        \n        all_results.extend([lambda_L, lambda_risk, ratio, int(activated_modes), is_close])\n\n    # Format and print the final output as a single list string\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# solve()\n```"
        }
    ]
}