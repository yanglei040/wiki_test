{
    "hands_on_practices": [
        {
            "introduction": "This first exercise lays the mathematical foundation for the bias-variance trade-off in the context of a general linear inverse problem. By employing the singular value decomposition (SVD), we will dissect a regularized estimator to derive the classic decomposition of the mean squared error. This practice is essential for understanding how Tikhonov regularization systematically manages the balance between fidelity to the data and the suppression of noise-induced variance, a cornerstone concept in regularization theory .",
            "id": "3368363",
            "problem": "Consider a linear inverse problem with data model $y = A x + \\varepsilon$ in a finite-dimensional Euclidean space, where $A$ admits a singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$ with singular values $\\{\\sigma_i\\}_{i=1}^r$, left singular vectors $\\{u_i\\}_{i=1}^r$, and right singular vectors $\\{v_i\\}_{i=1}^r$. Assume additive noise $\\varepsilon$ is zero-mean, Normally distributed (Gaussian), independent and identically distributed (i.i.d.) with covariance $\\sigma^2 I$, where $\\sigma^2  0$ is known. Consider a spectral-filter regularized estimator of the form\n$$\n\\hat{x}_{\\alpha} \\;=\\; \\sum_{i=1}^r \\frac{f_i(\\alpha)}{\\sigma_i} \\,\\langle y, u_i \\rangle \\, v_i,\n$$\nwhere $\\alpha  0$ is the regularization parameter and $\\{f_i(\\alpha)\\}_{i=1}^r$ are filter functions mapping $[0,\\infty)$ to $[0,1]$.\n\nYour tasks are:\n\n1. Starting only from the data model, orthonormality of the singular vectors, and linearity of expectation, derive a closed-form expression for the mean squared error (MSE), defined as the noise-conditional expectation $\\mathbb{E}_{\\varepsilon}[\\|\\hat{x}_{\\alpha} - x\\|^2]$, in terms of $\\{f_i(\\alpha)\\}$, $\\{\\sigma_i\\}$, the coefficients $\\{\\langle x, v_i\\rangle\\}$, and $\\sigma^2$.\n\n2. Specialize to Tikhonov regularization, where $f_i(\\alpha) = \\sigma_i^2/(\\sigma_i^2 + \\alpha)$ for each $i$. Using your expression from part $1$, interpret qualitatively how increasing $\\alpha$ changes the contributions customarily identified as the squared bias and the variance in the MSE.\n\n3. In the $1$-dimensional case $r=1$ with singular value $\\sigma_1 = s  0$ and true coefficient magnitude $a = |\\langle x, v_1\\rangle|$, assume Tikhonov regularization as in part $2$. Compute the value $\\alpha^{\\star}$ that minimizes $\\mathbb{E}_{\\varepsilon}[\\|\\hat{x}_{\\alpha} - x\\|^2]$ over $\\alpha  0$. Express your final answer as a closed-form analytic expression in terms of $a$ and $\\sigma$. No rounding is required, and there are no physical units to report.",
            "solution": "The problem statement is a standard formulation in the field of regularized linear inverse problems and is mathematically sound, self-contained, and well-posed under the reasonable assumption that the signal component in the final part is non-zero. The problem is valid.\n\n### Part 1: Derivation of the Mean Squared Error (MSE)\n\nThe mean squared error (MSE) is defined as the expectation of the squared norm of the error vector, conditional on the noise realization $\\varepsilon$:\n$$\n\\text{MSE} \\;=\\; \\mathbb{E}_{\\varepsilon}\\left[\\|\\hat{x}_{\\alpha} - x\\|^2\\right]\n$$\nThe vector space for $x$ can be decomposed into the orthogonal complement of the null space of $A$, which is spanned by the right singular vectors $\\{v_i\\}_{i=1}^r$, and the null space of $A$, spanned by $\\{v_j\\}_{j=r+1}^n$. We can thus write the true solution $x$ as:\n$$\nx \\;=\\; \\sum_{i=1}^r \\langle x, v_i \\rangle v_i + \\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\n$$\nThe estimator $\\hat{x}_{\\alpha}$ is constructed as a linear combination of $\\{v_i\\}_{i=1}^r$ and thus lies entirely in the subspace spanned by these vectors. The error vector $\\hat{x}_{\\alpha} - x$ can be split into two orthogonal components:\n$$\n\\hat{x}_{\\alpha} - x \\;=\\; \\left(\\hat{x}_{\\alpha} - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right) - \\left(\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right)\n$$\nDue to the orthogonality of these two components, the squared norm is the sum of their individual squared norms (by the Pythagorean theorem):\n$$\n\\|\\hat{x}_{\\alpha} - x\\|^2 \\;=\\; \\left\\|\\hat{x}_{\\alpha} - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right\\|^2 + \\left\\|\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right\\|^2\n$$\nSubstituting the definition of the estimator $\\hat{x}_{\\alpha}$ into the first term and using the orthonormality of the basis vectors $\\{v_i\\}_{i=1}^r$:\n$$\n\\left\\|\\sum_{i=1}^r \\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle v_i - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right\\|^2 \\;=\\; \\left\\|\\sum_{i=1}^r \\left(\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle\\right) v_i\\right\\|^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle\\right)^2\n$$\nThe second term is the squared norm of the component of $x$ in the null space of $A$, which is an irreducible error component because the estimator has no access to this subspace. This term is constant with respect to the noise $\\varepsilon$:\n$$\n\\left\\|\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right\\|^2 \\;=\\; \\sum_{j=r+1}^n \\langle x, v_j \\rangle^2\n$$\nNow we substitute the data model $y = Ax + \\varepsilon$ into the term $\\langle y, u_i \\rangle$:\n$$\n\\langle y, u_i \\rangle \\;=\\; \\langle Ax + \\varepsilon, u_i \\rangle \\;=\\; \\langle Ax, u_i \\rangle + \\langle \\varepsilon, u_i \\rangle\n$$\nUsing the property of the SVD that $Av_k = \\sigma_k u_k$ and thus $A (\\sum_k \\langle x, v_k \\rangle v_k) = \\sum_k \\langle x, v_k \\rangle \\sigma_k u_k$, we get:\n$$\n\\langle Ax, u_i \\rangle \\;=\\; \\left\\langle \\sum_{k=1}^r \\langle x, v_k \\rangle \\sigma_k u_k, u_i \\right\\rangle \\;=\\; \\sum_{k=1}^r \\langle x, v_k \\rangle \\sigma_k \\langle u_k, u_i \\rangle \\;=\\; \\sigma_i \\langle x, v_i \\rangle\n$$\nSo, $\\langle y, u_i \\rangle = \\sigma_i \\langle x, v_i \\rangle + \\langle \\varepsilon, u_i \\rangle$. Substituting this back:\n$$\n\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle \\;=\\; \\frac{f_i(\\alpha)}{\\sigma_i} (\\sigma_i \\langle x, v_i \\rangle + \\langle \\varepsilon, u_i \\rangle) - \\langle x, v_i \\rangle \\;=\\; (f_i(\\alpha)-1) \\langle x, v_i \\rangle + \\frac{f_i(\\alpha)}{\\sigma_i} \\langle \\varepsilon, u_i \\rangle\n$$\nThe squared sum becomes:\n$$\n\\sum_{i=1}^r \\left( (f_i(\\alpha)-1) \\langle x, v_i \\rangle + \\frac{f_i(\\alpha)}{\\sigma_i} \\langle \\varepsilon, u_i \\rangle \\right)^2\n$$\nNow we take the expectation $\\mathbb{E}_{\\varepsilon}$. The noise $\\varepsilon$ has zero mean, so $\\mathbb{E}_{\\varepsilon}[\\varepsilon] = 0$. This implies $\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle] = \\langle \\mathbb{E}_{\\varepsilon}[\\varepsilon], u_i \\rangle = 0$. Therefore, the cross-term in the squared expansion vanishes upon expectation. We are left with:\n$$\n\\mathbb{E}_{\\varepsilon}\\left[\\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\left(\\frac{f_i(\\alpha)}{\\sigma_i}\\right)^2 \\langle \\varepsilon, u_i \\rangle^2 \\right)\\right] = \\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\left(\\frac{f_i(\\alpha)}{\\sigma_i}\\right)^2 \\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2] \\right)\n$$\nThe term $\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2]$ is the variance of the random variable $\\langle \\varepsilon, u_i \\rangle = u_i^\\top\\varepsilon$. Given that $\\text{Cov}(\\varepsilon) = \\sigma^2 I$, the variance is:\n$$\n\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2] \\;=\\; \\text{Var}(u_i^\\top\\varepsilon) \\;=\\; u_i^\\top \\text{Cov}(\\varepsilon) u_i \\;=\\; u_i^\\top (\\sigma^2 I) u_i \\;=\\; \\sigma^2 (u_i^\\top u_i) \\;=\\; \\sigma^2\n$$\nsince $u_i$ is a unit vector.\nCombining all parts, the MSE is:\n$$\n\\text{MSE} \\;=\\; \\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\frac{f_i(\\alpha)^2 \\sigma^2}{\\sigma_i^2} \\right) + \\sum_{j=r+1}^n \\langle x, v_j \\rangle^2\n$$\n\n### Part 2: Interpretation of Tikhonov Regularization\n\nFor Tikhonov regularization, the filter functions are $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha}$. The MSE consists of three parts: squared bias, variance, and the irreducible null-space error. The regularization parameter $\\alpha$ only affects the first two.\n\nThe squared bias is the first term in the summation:\n$$\n\\text{Bias}^2(\\alpha) \\;=\\; \\sum_{i=1}^r (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha}-1\\right)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{-\\alpha}{\\sigma_i^2+\\alpha}\\right)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} \\langle x, v_i \\rangle^2\n$$\nThe variance is the second term:\n$$\n\\text{Var}(\\alpha) \\;=\\; \\sum_{i=1}^r \\frac{f_i(\\alpha)^2 \\sigma^2}{\\sigma_i^2} \\;=\\; \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha}\\right)^2 \\frac{\\sigma^2}{\\sigma_i^2} \\;=\\; \\sum_{i=1}^r \\frac{\\sigma_i^2}{(\\sigma_i^2+\\alpha)^2} \\sigma^2\n$$\nTo analyze the effect of increasing $\\alpha  0$:\n- **Squared Bias**: For each component $i$, the term $\\frac{\\alpha}{\\sigma_i^2+\\alpha}$ is a monotonically increasing function of $\\alpha$. As $\\alpha \\to 0^+$, the term approaches $0$. As $\\alpha \\to \\infty$, the term approaches $1$. Therefore, the total squared bias, $\\text{Bias}^2(\\alpha)$, is a monotonically increasing function of $\\alpha$. Increasing $\\alpha$ strengthens the regularization, shrinking the estimate towards $0$. This introduces a larger systematic deviation (bias) from the true solution.\n- **Variance**: For each component $i$, the term $\\frac{1}{(\\sigma_i^2+\\alpha)^2}$ is a monotonically decreasing function of $\\alpha$. As $\\alpha \\to 0^+$, the term is $\\frac{1}{(\\sigma_i^2)^2}$, and the variance term $(\\sigma^2/\\sigma_i^2)$ can be large for small $\\sigma_i$. As $\\alpha \\to \\infty$, the term approaches $0$. Therefore, the total variance, $\\text{Var}(\\alpha)$, is a monotonically decreasing function of $\\alpha$. Increasing $\\alpha$ suppresses the amplification of noise, particularly for components associated with small singular values, thus reducing the overall variance of the estimator.\n\nIn conclusion, increasing the regularization parameter $\\alpha$ increases the squared bias while decreasing the variance. This is the canonical bias-variance trade-off in regularization.\n\n### Part 3: Optimal $\\alpha$ for the 1D Case\n\nIn the $1$-dimensional case, we have $r=1$, $\\sigma_1 = s  0$, and $|\\langle x, v_1\\rangle| = a$. Let $c = \\langle x, v_1\\rangle$, so $c^2 = a^2$. The MSE to be minimized, as a function of $\\alpha$, is the sum of the bias and variance terms (the null-space error is constant and does not affect the location of the minimum):\n$$\nJ(\\alpha) \\;=\\; \\frac{\\alpha^2}{(s^2+\\alpha)^2} c^2 + \\frac{s^2}{(s^2+\\alpha)^2} \\sigma^2 \\;=\\; \\frac{\\alpha^2 c^2 + s^2 \\sigma^2}{(s^2+\\alpha)^2}\n$$\nTo find the value $\\alpha^{\\star}$ that minimizes $J(\\alpha)$ for $\\alpha  0$, we compute the derivative with respect to $\\alpha$ and set it to zero. We assume $a  0$, which implies $c \\neq 0$. If $a=0$, the bias term vanishes, and $J(\\alpha)$ becomes a strictly decreasing function of $\\alpha$, meaning no minimum exists in $(0, \\infty)$.\nUsing the quotient rule for differentiation:\n$$\n\\frac{dJ}{d\\alpha} \\;=\\; \\frac{(2\\alpha c^2)(s^2+\\alpha)^2 - (\\alpha^2 c^2 + s^2 \\sigma^2)(2(s^2+\\alpha))}{(s^2+\\alpha)^4}\n$$\nSetting the numerator to zero and dividing by the non-zero factor $2(s^2+\\alpha)$ (since $s0, \\alpha0$):\n$$\n(2\\alpha c^2)\\frac{1}{2}(s^2+\\alpha) - (\\alpha^2 c^2 + s^2 \\sigma^2) \\;=\\; 0\n$$\n$$\n\\alpha c^2 (s^2+\\alpha) - (\\alpha^2 c^2 + s^2 \\sigma^2) \\;=\\; 0\n$$\n$$\n\\alpha s^2 c^2 + \\alpha^2 c^2 - \\alpha^2 c^2 - s^2 \\sigma^2 \\;=\\; 0\n$$\n$$\n\\alpha s^2 c^2 - s^2 \\sigma^2 \\;=\\; 0\n$$\nSince $s  0$, we can divide by $s^2$:\n$$\n\\alpha c^2 = \\sigma^2\n$$\nSolving for $\\alpha$, we get:\n$$\n\\alpha^{\\star} \\;=\\; \\frac{\\sigma^2}{c^2}\n$$\nSubstituting $c^2=a^2$, the optimal regularization parameter is:\n$$\n\\alpha^{\\star} \\;=\\; \\frac{\\sigma^2}{a^2}\n$$\nThe second derivative test would confirm this is a minimum. The sign of the first derivative changes from negative to positive around this point, confirming it as a minimum.",
            "answer": "$$\\boxed{\\frac{\\sigma^2}{a^2}}$$"
        },
        {
            "introduction": "Moving from a static problem to a dynamic one, this practice explores the bias-variance trade-off within a simple state-space model, a scenario common in time-series analysis and data assimilation. Here, regularization is implicitly introduced through the statistical assumptions about the system's dynamics. You will derive how the assumed process noise variance, $Q$, which controls how strongly we enforce the dynamical model, creates a trade-off between bias (if the model is mismatched with reality) and variance (from measurement noise), demonstrating the universality of this principle beyond algebraic regularization .",
            "id": "3368359",
            "problem": "Consider a scalar linear state-space model over a two-step window $t \\in \\{0,1\\}$, with dynamics $x_{1}=\\rho x_{0}+w_{0}$ and observations $y_{t}=x_{t}+\\varepsilon_{t}$. Assume $w_{0}$ is drawn from a zero-mean Gaussian distribution with variance $Q$, and $\\varepsilon_{t}$ are independent zero-mean Gaussian random variables with variance $R$, all mutually independent. Let the prior for $x_{0}$ be an improper flat prior, understood as the limit of a Gaussian prior with variance tending to infinity. Define the Maximum A Posteriori (MAP) estimator of $(x_{0},x_{1})$ under these assumptions. Then, treating the true states $x_{0}$ and $x_{1}$ as fixed (unknown) constants, and averaging only over the measurement noise $\\varepsilon_{0}$ and $\\varepsilon_{1}$, derive the bias of the MAP estimator of $x_{1}$, the variance of the MAP estimator of $x_{1}$, and combine them to obtain the mean squared error at time $t=1$, defined as $\\mathbb{E}\\big[(\\hat{x}_{1}-x_{1})^{2}\\big]$, as an explicit analytic function of $Q$, $R$, $\\rho$, $x_{0}$, and $x_{1}$. Report your final answer as a single closed-form analytical expression for the mean squared error in exact form. No units are required. If you carry out any numerical simplifications, do not round.",
            "solution": "The model is linear-Gaussian. With the improper flat prior for $x_{0}$ and independent Gaussian noises, the posterior density for $(x_{0},x_{1})$ is proportional to the product of three Gaussian likelihoods: one for $y_{0}$ given $x_{0}$, one for $y_{1}$ given $x_{1}$, and one for $x_{1}$ given $x_{0}$. Therefore, the negative log-posterior (up to an additive constant) is the weighted sum of squared residuals\n$$\nJ(x_{0},x_{1}) \\;=\\; \\frac{1}{R}\\,(y_{0}-x_{0})^{2} \\;+\\; \\frac{1}{R}\\,(y_{1}-x_{1})^{2} \\;+\\; \\frac{1}{Q}\\,(x_{1}-\\rho x_{0})^{2}.\n$$\nThe Maximum A Posteriori (MAP) estimate $(\\hat{x}_{0},\\hat{x}_{1})$ minimizes $J(x_{0},x_{1})$. This is a strictly convex quadratic function, so its minimizer is obtained by setting the gradient to zero:\n$$\n\\frac{\\partial J}{\\partial x_{0}} \\;=\\; -\\frac{2}{R}(y_{0}-x_{0}) \\;-\\; \\frac{2\\rho}{Q}(x_{1}-\\rho x_{0}) \\;=\\; 0,\n$$\n$$\n\\frac{\\partial J}{\\partial x_{1}} \\;=\\; -\\frac{2}{R}(y_{1}-x_{1}) \\;+\\; \\frac{2}{Q}(x_{1}-\\rho x_{0}) \\;=\\; 0.\n$$\nRearranging these two equations gives a linear system in $(x_{0},x_{1})$. Define $A=\\frac{1}{R}$ and $B=\\frac{1}{Q}$ to streamline notation. Then the normal equations can be written as\n$$\n\\begin{pmatrix}\nA + \\rho^{2}B  -\\rho B \\\\\n-\\rho B  A + B\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{0} \\\\\nx_{1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nA y_{0} \\\\\nA y_{1}\n\\end{pmatrix}.\n$$\nLet $M$ be the $2\\times 2$ matrix above and $b$ the right-hand vector. The determinant of $M$ is\n$$\n\\det(M) \\;=\\; (A + \\rho^{2}B)(A + B) - (\\rho B)^{2} \\;=\\; A^{2} + A B (1 + \\rho^{2}) \\;=\\; A\\big(A + B(1+\\rho^{2})\\big).\n$$\nThe inverse of $M$ is\n$$\nM^{-1} \\;=\\; \\frac{1}{\\det(M)} \\begin{pmatrix}\nA + B  \\rho B \\\\\n\\rho B  A + \\rho^{2}B\n\\end{pmatrix}.\n$$\nThus, the MAP estimates are\n$$\n\\begin{pmatrix}\n\\hat{x}_{0} \\\\\n\\hat{x}_{1}\n\\end{pmatrix}\n\\;=\\;\nM^{-1} b\n\\;=\\;\n\\frac{1}{\\det(M)}\n\\begin{pmatrix}\n(A + B)A y_{0} + \\rho B A y_{1} \\\\\n\\rho B A y_{0} + (A + \\rho^{2}B) A y_{1}\n\\end{pmatrix}.\n$$\nIn particular,\n$$\n\\hat{x}_{1} \\;=\\; \\alpha\\, y_{0} + \\beta\\, y_{1},\n\\quad\\text{with}\\quad\n\\alpha \\;=\\; \\frac{\\rho A B}{\\det(M)}, \\qquad \\beta \\;=\\; \\frac{A\\left(A + \\rho^{2}B\\right)}{\\det(M)}.\n$$\nWe now quantify the bias-variance trade-off for $\\hat{x}_{1}$ by treating the true states $x_{0}$ and $x_{1}$ as fixed constants, and averaging over the independent measurement noises $\\varepsilon_{0}$ and $\\varepsilon_{1}$. By definition,\n$$\ny_{0} \\;=\\; x_{0} + \\varepsilon_{0}, \\qquad y_{1} \\;=\\; x_{1} + \\varepsilon_{1}, \\qquad \\mathbb{E}[\\varepsilon_{t}] \\;=\\; 0, \\qquad \\operatorname{Var}(\\varepsilon_{t}) \\;=\\; R, \\qquad \\operatorname{Cov}(\\varepsilon_{0},\\varepsilon_{1}) \\;=\\; 0.\n$$\nThe bias is\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\mathbb{E}[\\hat{x}_{1}] - x_{1}\n\\;=\\; \\alpha\\, x_{0} + \\beta\\, x_{1} - x_{1}\n\\;=\\; \\alpha\\, x_{0} + (\\beta - 1)\\, x_{1}.\n$$\nUsing $\\det(M)=A^{2}+A B(1+\\rho^{2})$ and the formulas for $\\alpha$ and $\\beta$,\n$$\n\\beta - 1 \\;=\\; \\frac{A(A+\\rho^{2}B)}{\\det(M)} - 1 \\;=\\; \\frac{A(A+\\rho^{2}B) - \\det(M)}{\\det(M)} \\;=\\; \\frac{-A B}{\\det(M)}.\n$$\nTherefore,\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\frac{\\rho A B}{\\det(M)}\\, x_{0} \\;-\\; \\frac{A B}{\\det(M)}\\, x_{1}\n\\;=\\; \\frac{A B}{\\det(M)}\\,\\big(\\rho x_{0} - x_{1}\\big).\n$$\nSubstituting back $A=\\frac{1}{R}$, $B=\\frac{1}{Q}$, and $\\det(M)=\\frac{1}{R}\\Big(\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}\\Big)$, the bias simplifies to\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\frac{\\frac{1}{R}\\cdot \\frac{1}{Q}}{\\frac{1}{R}\\left(\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}\\right)}\\,\\big(\\rho x_{0}-x_{1}\\big)\n\\;=\\; \\frac{\\frac{1}{Q}}{\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}}\\,\\big(\\rho x_{0}-x_{1}\\big)\n\\;=\\; \\frac{R}{Q + R(1+\\rho^{2})}\\,\\big(\\rho x_{0}-x_{1}\\big).\n$$\nNext, the variance of $\\hat{x}_{1}$ is\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; \\alpha^{2}\\operatorname{Var}(y_{0}) + \\beta^{2}\\operatorname{Var}(y_{1}) \\;=\\; R\\left(\\alpha^{2} + \\beta^{2}\\right).\n$$\nCompute $\\alpha^{2} + \\beta^{2}$ in terms of $A$ and $B$:\n$$\n\\alpha^{2} + \\beta^{2} \\;=\\; \\frac{(\\rho A B)^{2} + \\left(A(A + \\rho^{2}B)\\right)^{2}}{\\det(M)^{2}}\n\\;=\\; \\frac{A^{2}\\left(\\rho^{2}B^{2} + (A + \\rho^{2}B)^{2}\\right)}{\\det(M)^{2}}.\n$$\nBecause $\\det(M)=A\\big(A + B(1+\\rho^{2})\\big)$, we have $\\det(M)^{2} = A^{2}\\big(A + B(1+\\rho^{2})\\big)^{2}$. Hence\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{\\rho^{2}B^{2} + \\left(A + \\rho^{2}B\\right)^{2}}{\\left(A + B(1+\\rho^{2})\\right)^{2}}.\n$$\nSubstituting $A=\\frac{1}{R}$ and $B=\\frac{1}{Q}$ gives\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{\\frac{\\rho^{2}}{Q^{2}} + \\left(\\frac{1}{R} + \\frac{\\rho^{2}}{Q}\\right)^{2}}{\\left(\\frac{1}{R} + \\frac{1+\\rho^{2}}{Q}\\right)^{2}}.\n$$\nFor algebraic clarity, multiply numerator and denominator by $R^{2}Q^{2}$ to obtain an equivalent, simplified rational form:\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{Q^{2} + 2\\rho^{2} R Q + R^{2}\\rho^{2}(1+\\rho^{2})}{Q^{2} + 2(1+\\rho^{2}) R Q + R^{2}(1+\\rho^{2})^{2}}.\n$$\nFinally, the mean squared error is\n$$\n\\operatorname{MSE}(\\hat{x}_{1}) \\;=\\; \\big(\\operatorname{Bias}(\\hat{x}_{1})\\big)^{2} + \\operatorname{Var}(\\hat{x}_{1}),\n$$\nwhich yields\n$$\n\\operatorname{MSE}(\\hatx_{1}) \\;=\\; \\left(\\frac{R}{Q + R(1+\\rho^{2})}\\right)^{2}\\big(\\rho x_{0} - x_{1}\\big)^{2} \\;+\\; R\\,\\frac{Q^{2} + 2\\rho^{2} R Q + R^{2}\\rho^{2}(1+\\rho^{2})}{Q^{2} + 2(1+\\rho^{2}) R Q + R^{2}(1+\\rho^{2})^{2}}.\n$$\nThis expression explicitly quantifies the bias-variance trade-off as a function of $Q$ and $R$: as $Q$ decreases (stronger dynamical regularization), the bias term grows when the true states deviate from the exact dynamical relation ($\\rho x_{0} \\neq x_{1}$), while the variance term decreases; conversely, as $Q$ increases (weaker regularization), the bias term shrinks and the variance term increases toward $R$.",
            "answer": "$$\\boxed{\\left(\\frac{R}{Q + R(1+\\rho^{2})}\\right)^{2}\\left(\\rho x_{0} - x_{1}\\right)^{2} \\;+\\; R\\,\\frac{Q^{2} + 2\\rho^{2} R Q + R^{2}\\rho^{2}(1+\\rho^{2})}{(Q + R(1+\\rho^{2}))^{2}}}$$"
        },
        {
            "introduction": "After establishing the theoretical existence of an optimal regularization parameter, the crucial practical question is how to find it using only noisy data. This exercise tackles this challenge by examining parameter selection for Truncated SVD (TSVD) regularization. You will derive and contrast two widely used methods: the heuristic Morozov Discrepancy Principle and the statistically powerful Stein's Unbiased Risk Estimate (SURE), revealing how one can construct a data-driven criterion to navigate the bias-variance landscape and approximate the optimal model complexity .",
            "id": "3368364",
            "problem": "Consider the linear inverse problem in data assimilation with observations modeled as $y = A x_{\\star} + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator of rank $r \\leq \\min\\{m,n\\}$, $x_{\\star} \\in \\mathbb{R}^{n}$ is the unknown true state, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ is additive Gaussian noise with known noise variance $\\sigma^{2}  0$. Let the singular value decomposition of $A$ be $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ orthonormal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ having singular values $s_{1} \\geq s_{2} \\geq \\cdots \\geq s_{r}  0$ on its diagonal. Define the truncated singular value decomposition estimator with truncation level $k \\in \\{0,1,\\dots,r\\}$ by\n$$\nx^{(k)} \\;=\\; \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{s_{i}} \\, v_{i}, \\quad \\text{and predicted data} \\quad \\hat{y}^{(k)} \\;=\\; A x^{(k)} \\;=\\; \\sum_{i=1}^{k} u_{i} u_{i}^{\\top} y,\n$$\nwhere $u_{i}$ and $v_{i}$ are the left and right singular vectors of $A$. Denote the orthogonal projector onto the span of $\\{u_{1},\\dots,u_{k}\\}$ by $S_{k} := \\sum_{i=1}^{k} u_{i} u_{i}^{\\top}$, and the noiseless data by $y_{\\star} := A x_{\\star} = \\sum_{i=1}^{r} u_{i} u_{i}^{\\top} y_{\\star}$.\n\nStarting from the definitions above and the fundamental statistical principle of the bias-variance decomposition of the expected squared error for Gaussian noise, derive:\n\n1. The Morozov-type discrepancy principle stopping rule $k(\\sigma)$ that selects the minimal $k$ such that the data residual $\\|y - \\hat{y}^{(k)}\\|$ is consistent with the known noise level. Express your rule using a tolerance factor $\\tau \\geq 1$ multiplying the expected noise norm. Your derivation must proceed from the definitions of $S_{k}$ and the residual $r^{(k)} := y - \\hat{y}^{(k)} = (I - S_{k}) y$, and must justify why the stopping rule aligns the residual with the noise level.\n\n2. Using Stein's Unbiased Risk Estimate (SURE) for Gaussian noise in the data space, derive a tractable expression for the SURE objective for the linear estimator $\\hat{y}^{(k)} = S_{k} y$, and obtain the $k$ that minimizes SURE over $\\{0,1,\\dots,r\\}$. Express the minimizer explicitly in terms of the observed coefficients $u_{i}^{\\top} y$ and $\\sigma^{2}$.\n\nFinally, provide the two analytic expressions for the discrepancy-principle rule $k(\\sigma)$ and the SURE-minimizing rule over $k$ as functions of $(U,y,\\sigma,\\tau,m,r)$, written only in terms of the quantities defined above. The final answer must be provided as a pair in a single row matrix using the LaTeX $\\mathrm{pmatrix}$ environment, with no textual explanation, and it must be a closed-form analytic expression (not an inequality). No rounding is required. No units are involved.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective, providing a clear and complete setup for a standard problem in regularization theory for inverse problems.\n\nWe will now derive the two requested parameter choice rules. The problem is set in the context of a linear inverse problem $y = A x_{\\star} + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$ are the observations, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $x_{\\star} \\in \\mathbb{R}^{n}$ is the true state to be recovered, and $\\varepsilon \\in \\mathbb{R}^{m}$ is a noise vector with components drawn from a normal distribution with mean $0$ and variance $\\sigma^{2}$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$. The truncated singular value decomposition (TSVD) estimator for $x_{\\star}$ is given by $x^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{s_{i}} v_{i}$, which leads to predicted data $\\hat{y}^{(k)} = A x^{(k)}$. Using the provided definition $\\hat{y}^{(k)} = S_k y$ where $S_k = \\sum_{i=1}^{k} u_{i} u_{i}^{\\top}$ is the orthogonal projector onto the span of the first $k$ left singular vectors $\\{u_1, \\dots, u_k\\}$.\n\n### 1. Morozov-type Discrepancy Principle\n\nThe Morozov discrepancy principle is a heuristic for choosing the regularization parameter (in this case, the truncation level $k$) by requiring that the model's prediction residual be consistent with the noise level in the data. The residual for a given $k$ is $r^{(k)} = y - \\hat{y}^{(k)} = y - S_k y = (I_m - S_k)y$.\n\nThe core idea is to select a parameter $k$ such that the norm of the residual is comparable to the expected norm of the noise. If the residual is much larger, the model is underfitting (bias is high). If the residual is much smaller, the model is overfitting (fitting the noise).\n\nThe noise vector $\\varepsilon$ has covariance $\\sigma^2 I_m$. The expected squared norm of the noise is:\n$$\nE\\left[\\|\\varepsilon\\|^2\\right] = E\\left[\\varepsilon^{\\top}\\varepsilon\\right] = \\text{Tr}\\left(E\\left[\\varepsilon\\varepsilon^{\\top}\\right]\\right) = \\text{Tr}\\left(\\text{Cov}(\\varepsilon)\\right) = \\text{Tr}(\\sigma^2 I_m) = m \\sigma^2\n$$\nThe root-mean-square (RMS) magnitude of the noise is therefore $\\sqrt{m \\sigma^2} = \\sqrt{m}\\sigma$. The discrepancy principle rule is formulated by choosing the parameter $k$ such that the residual norm $\\|r^{(k)}\\|$ is balanced against this noise level, typically by satisfying $\\|r^{(k)}\\| \\leq \\tau \\sqrt{m}\\sigma$ for a tolerance factor $\\tau \\geq 1$. Squaring this gives the condition on the squared residual norm:\n$$\n\\|r^{(k)}\\|^2 \\leq \\tau^2 m \\sigma^2\n$$\nThe squared residual norm can be expressed using the data $y$ and the projector $S_k$. Since $(I_m - S_k)$ is also an orthogonal projector, we have $(I_m-S_k)^2 = (I_m-S_k)$:\n$$\n\\|r^{(k)}\\|^2 = \\|(I_m - S_k)y\\|^2 = y^{\\top}(I_m - S_k)^{\\top}(I_m - S_k)y = y^{\\top}(I_m - S_k)y\n$$\nExpanding $y$ in the orthonormal basis of left singular vectors $\\{u_i\\}_{i=1}^m$, we have $y = \\sum_{i=1}^m (u_i^{\\top} y) u_i$. Applying the projector $(I_m - S_k)$:\n$$\n(I_m - S_k)y = \\left(I_m - \\sum_{i=1}^k u_i u_i^{\\top}\\right) \\left(\\sum_{j=1}^m (u_j^{\\top} y) u_j\\right) = \\sum_{j=k+1}^m (u_j^{\\top} y) u_j\n$$\nThe squared norm is therefore the sum of squared coefficients for indices greater than $k$:\n$$\n\\|r^{(k)}\\|^2 = \\left\\| \\sum_{i=k+1}^m (u_i^{\\top} y) u_i \\right\\|^2 = \\sum_{i=k+1}^m (u_i^{\\top} y)^2\n$$\nThe quantity $\\sum_{i=k+1}^m (u_i^{\\top} y)^2$ is a monotonically non-increasing function of $k$. A larger $k$ means fewer terms in the sum, hence a smaller or equal residual. The discrepancy principle aims to find the simplest model (smallest $k$) that is consistent with the data. This translates to finding the minimal $k$ that satisfies the condition.\nThe rule is thus to choose $k(\\sigma)$ as the smallest integer in the set $\\{0, 1, \\dots, r\\}$ for which the condition is met:\n$$\nk(\\sigma) = \\min \\left\\{ k \\in \\{0, 1, \\dots, r\\} \\mid \\sum_{i=k+1}^m (u_i^\\top y)^2 \\le \\tau^2 m \\sigma^2 \\right\\}\n$$\nThis rule aligns the residual with the noise level because for an appropriate $k$, the dominant part of the true signal $y_{\\star} = A x_{\\star}$ is captured by the projection $S_k y_{\\star}$. The residual $r^{(k)} = (I-S_k)y_\\star + (I-S_k)\\varepsilon$ is then dominated by the noise component $(I-S_k)\\varepsilon$, whose expected squared norm is $E[\\|(I-S_k)\\varepsilon\\|^2] = \\text{Tr}((I-S_k)\\sigma^2 I_m) = (m-k)\\sigma^2$. The principle compares the *observed* residual norm $\\|r^{(k)}\\|^2$ to the total expected noise energy $m\\sigma^2$, stopping when the residual is \"small enough\".\n\n### 2. Stein's Unbiased Risk Estimate (SURE)\n\nSURE provides an unbiased estimate of the mean squared error (MSE) or risk, $E[\\|\\hat{y} - y_\\star\\|^2]$, for an estimator $\\hat{y}$ of a true signal $y_\\star$ observed with additive Gaussian noise. For a linear estimator of the form $\\hat{y} = L y$, where $y = y_\\star + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, the SURE formula is:\n$$\n\\text{SURE}(y; L) = \\|L y - y\\|^2 - m \\sigma^2 + 2 \\sigma^2 \\text{div}_y(Ly)\n$$\nIn our problem, the estimator for the data is $\\hat{y}^{(k)} = S_k y$, so the linear operator is $L = S_k$. We need to compute the terms for this operator.\n\nThe first term is the squared norm of the residual:\n$$\n\\|S_k y - y\\|^2 = \\|-(I_m - S_k)y\\|^2 = \\|r^{(k)}\\|^2 = \\sum_{i=k+1}^m (u_i^\\top y)^2\n$$\nThe second term is $-m\\sigma^2$.\n\nThe third term involves the divergence of the map $y \\mapsto S_k y$. For a linear map $Ly$, the divergence is the trace of the matrix representation:\n$$\n\\text{div}_y(S_k y) = \\text{Tr}(S_k)\n$$\nThe projector $S_k$ is defined as $S_k = \\sum_{i=1}^k u_i u_i^{\\top}$. Its trace is:\n$$\n\\text{Tr}(S_k) = \\text{Tr}\\left(\\sum_{i=1}^k u_i u_i^{\\top}\\right) = \\sum_{i=1}^k \\text{Tr}(u_i u_i^{\\top})\n$$\nUsing the cyclic property of the trace, $\\text{Tr}(AB) = \\text{Tr}(BA)$:\n$$\n\\text{Tr}(u_i u_i^{\\top}) = \\text{Tr}(u_i^{\\top} u_i) = \\text{Tr}(\\|u_i\\|^2)\n$$\nSince the singular vectors $u_i$ are orthonormal, $\\|u_i\\|^2 = 1$. The trace of a scalar is the scalar itself. So, $\\text{Tr}(u_i u_i^{\\top}) = 1$.\n$$\n\\text{Tr}(S_k) = \\sum_{i=1}^k 1 = k\n$$\nSubstituting these components into the SURE formula gives the objective function for a given $k$:\n$$\n\\text{SURE}(k) = \\|(I_m - S_k)y\\|^2 - m \\sigma^2 + 2 \\sigma^2 k\n$$\nUsing the expression for the residual norm in terms of the data coefficients:\n$$\n\\text{SURE}(k) = \\sum_{i=k+1}^m (u_i^\\top y)^2 - m \\sigma^2 + 2 k \\sigma^2\n$$\nTo find the optimal $k$, we must minimize this function over the allowed range $k \\in \\{0, 1, \\dots, r\\}$. The term $-m\\sigma^2$ is constant with respect to $k$ and can be ignored for the minimization. Thus, we seek to find:\n$$\nk_{\\text{SURE}} = \\arg\\min_{k \\in \\{0, 1, \\dots, r\\}} \\left( \\sum_{i=k+1}^m (u_i^\\top y)^2 + 2k\\sigma^2 \\right)\n$$\nThis expression beautifully illustrates the bias-variance trade-off. The first term, $\\sum_{i=k+1}^m (u_i^\\top y)^2$, is the squared residual, which is related to the bias of the estimate and decreases as $k$ increases. The second term, $2k\\sigma^2$, is a penalty for model complexity (degrees of freedom), which is related to the variance of the estimate and increases with $k$. The SURE criterion finds the value of $k$ that optimally balances these two competing effects.\n\nThe derived expressions for the two rules are now ready to be presented in the final format.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\min \\left\\{ k \\in \\{0, 1, \\dots, r\\} \\mid \\sum_{i=k+1}^m (u_i^\\top y)^2 \\le \\tau^2 m \\sigma^2 \\right\\}  \\arg\\min_{k \\in \\{0, 1, \\dots, r\\}} \\left( \\sum_{i=k+1}^m (u_i^\\top y)^2 + 2k\\sigma^2 \\right) \\end{pmatrix}}\n$$"
        }
    ]
}