{
    "hands_on_practices": [
        {
            "introduction": "Understanding the bias-variance trade-off begins with a solid mathematical framework. This first exercise guides you through the foundational analysis of linear inverse problems using the singular value decomposition (SVD), which provides a powerful lens for dissecting estimator performance. By deriving the mean squared error for a general class of \"spectral filter\" estimators, you will see precisely how regularization moderates the influence of data, and then apply this insight to the classic case of Tikhonov regularization .",
            "id": "3368363",
            "problem": "Consider a linear inverse problem with data model $y = A x + \\varepsilon$ in a finite-dimensional Euclidean space, where $A$ admits a singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$ with singular values $\\{\\sigma_i\\}_{i=1}^r$, left singular vectors $\\{u_i\\}_{i=1}^r$, and right singular vectors $\\{v_i\\}_{i=1}^r$. Assume additive noise $\\varepsilon$ is zero-mean, Normally distributed (Gaussian), independent and identically distributed (i.i.d.) with covariance $\\sigma^2 I$, where $\\sigma^2 > 0$ is known. Consider a spectral-filter regularized estimator of the form\n$$\n\\hat{x}_{\\alpha} \\;=\\; \\sum_{i=1}^r \\frac{f_i(\\alpha)}{\\sigma_i} \\,\\langle y, u_i \\rangle \\, v_i,\n$$\nwhere $\\alpha > 0$ is the regularization parameter and $\\{f_i(\\alpha)\\}_{i=1}^r$ are filter functions mapping $[0,\\infty)$ to $[0,1]$.\n\nYour tasks are:\n\n1. Starting only from the data model, orthonormality of the singular vectors, and linearity of expectation, derive a closed-form expression for the mean squared error (MSE), defined as the noise-conditional expectation $\\mathbb{E}_{\\varepsilon}[\\|\\hat{x}_{\\alpha} - x\\|^2]$, in terms of $\\{f_i(\\alpha)\\}$, $\\{\\sigma_i\\}$, the coefficients $\\{\\langle x, v_i\\rangle\\}$, and $\\sigma^2$.\n\n2. Specialize to Tikhonov regularization, where $f_i(\\alpha) = \\sigma_i^2/(\\sigma_i^2 + \\alpha)$ for each $i$. Using your expression from part $1$, interpret qualitatively how increasing $\\alpha$ changes the contributions customarily identified as the squared bias and the variance in the MSE.\n\n3. In the $1$-dimensional case $r=1$ with singular value $\\sigma_1 = s > 0$ and true coefficient magnitude $a = |\\langle x, v_1\\rangle|$, assume Tikhonov regularization as in part $2$. Compute the value $\\alpha^{\\star}$ that minimizes $\\mathbb{E}_{\\varepsilon}[\\|\\hat{x}_{\\alpha} - x\\|^2]$ over $\\alpha > 0$. Express your final answer as a closed-form analytic expression in terms of $a$ and $\\sigma$. No rounding is required, and there are no physical units to report.",
            "solution": "The problem statement is a standard formulation in the field of regularized linear inverse problems and is mathematically sound, self-contained, and well-posed under the reasonable assumption that the signal component in the final part is non-zero. The problem is valid.\n\n### Part 1: Derivation of the Mean Squared Error (MSE)\n\nThe mean squared error (MSE) is defined as the expectation of the squared norm of the error vector, conditional on the noise realization $\\varepsilon$:\n$$\n\\text{MSE} \\;=\\; \\mathbb{E}_{\\varepsilon}\\left[\\|\\hat{x}_{\\alpha} - x\\|^2\\right]\n$$\nThe vector space for $x$ can be decomposed into the orthogonal complement of the null space of $A$, which is spanned by the right singular vectors $\\{v_i\\}_{i=1}^r$, and the null space of $A$, spanned by $\\{v_j\\}_{j=r+1}^n$. We can thus write the true solution $x$ as:\n$$\nx \\;=\\; \\sum_{i=1}^r \\langle x, v_i \\rangle v_i + \\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\n$$\nThe estimator $\\hat{x}_{\\alpha}$ is constructed as a linear combination of $\\{v_i\\}_{i=1}^r$ and thus lies entirely in the subspace spanned by these vectors. The error vector $\\hat{x}_{\\alpha} - x$ can be split into two orthogonal components:\n$$\n\\hat{x}_{\\alpha} - x \\;=\\; \\left(\\hat{x}_{\\alpha} - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right) - \\left(\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right)\n$$\nDue to the orthogonality of these two components, the squared norm is the sum of their individual squared norms (by the Pythagorean theorem):\n$$\n\\|\\hat{x}_{\\alpha} - x\\|^2 \\;=\\; \\left\\|\\hat{x}_{\\alpha} - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right\\|^2 + \\left\\|\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right\\|^2\n$$\nSubstituting the definition of the estimator $\\hat{x}_{\\alpha}$ into the first term and using the orthonormality of the basis vectors $\\{v_i\\}_{i=1}^r$:\n$$\n\\left\\|\\sum_{i=1}^r \\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle v_i - \\sum_{i=1}^r \\langle x, v_i \\rangle v_i\\right\\|^2 \\;=\\; \\left\\|\\sum_{i=1}^r \\left(\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle\\right) v_i\\right\\|^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle\\right)^2\n$$\nThe second term is the squared norm of the component of $x$ in the null space of $A$, which is an irreducible error component because the estimator has no access to this subspace. This term is constant with respect to the noise $\\varepsilon$:\n$$\n\\left\\|\\sum_{j=r+1}^n \\langle x, v_j \\rangle v_j\\right\\|^2 \\;=\\; \\sum_{j=r+1}^n \\langle x, v_j \\rangle^2\n$$\nNow we substitute the data model $y = Ax + \\varepsilon$ into the term $\\langle y, u_i \\rangle$:\n$$\n\\langle y, u_i \\rangle \\;=\\; \\langle Ax + \\varepsilon, u_i \\rangle \\;=\\; \\langle Ax, u_i \\rangle + \\langle \\varepsilon, u_i \\rangle\n$$\nUsing the property of the SVD that $Av_k = \\sigma_k u_k$ and thus $A (\\sum_k \\langle x, v_k \\rangle v_k) = \\sum_k \\langle x, v_k \\rangle \\sigma_k u_k$, we get:\n$$\n\\langle Ax, u_i \\rangle \\;=\\; \\left\\langle \\sum_{k=1}^r \\langle x, v_k \\rangle \\sigma_k u_k, u_i \\right\\rangle \\;=\\; \\sum_{k=1}^r \\langle x, v_k \\rangle \\sigma_k \\langle u_k, u_i \\rangle \\;=\\; \\sigma_i \\langle x, v_i \\rangle\n$$\nSo, $\\langle y, u_i \\rangle = \\sigma_i \\langle x, v_i \\rangle + \\langle \\varepsilon, u_i \\rangle$. Substituting this back:\n$$\n\\frac{f_i(\\alpha)}{\\sigma_i} \\langle y, u_i \\rangle - \\langle x, v_i \\rangle \\;=\\; \\frac{f_i(\\alpha)}{\\sigma_i} (\\sigma_i \\langle x, v_i \\rangle + \\langle \\varepsilon, u_i \\rangle) - \\langle x, v_i \\rangle \\;=\\; (f_i(\\alpha)-1) \\langle x, v_i \\rangle + \\frac{f_i(\\alpha)}{\\sigma_i} \\langle \\varepsilon, u_i \\rangle\n$$\nThe squared sum becomes:\n$$\n\\sum_{i=1}^r \\left( (f_i(\\alpha)-1) \\langle x, v_i \\rangle + \\frac{f_i(\\alpha)}{\\sigma_i} \\langle \\varepsilon, u_i \\rangle \\right)^2\n$$\nNow we take the expectation $\\mathbb{E}_{\\varepsilon}$. The noise $\\varepsilon$ has zero mean, so $\\mathbb{E}_{\\varepsilon}[\\varepsilon] = 0$. This implies $\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle] = \\langle \\mathbb{E}_{\\varepsilon}[\\varepsilon], u_i \\rangle = 0$. Therefore, the cross-term in the squared expansion vanishes upon expectation. We are left with:\n$$\n\\mathbb{E}_{\\varepsilon}\\left[\\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\left(\\frac{f_i(\\alpha)}{\\sigma_i}\\right)^2 \\langle \\varepsilon, u_i \\rangle^2 \\right)\\right] = \\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\left(\\frac{f_i(\\alpha)}{\\sigma_i}\\right)^2 \\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2] \\right)\n$$\nThe term $\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2]$ is the variance of the random variable $\\langle \\varepsilon, u_i \\rangle = u_i^\\top\\varepsilon$. Given that $\\text{Cov}(\\varepsilon) = \\sigma^2 I$, the variance is:\n$$\n\\mathbb{E}_{\\varepsilon}[\\langle \\varepsilon, u_i \\rangle^2] \\;=\\; \\text{Var}(u_i^\\top\\varepsilon) \\;=\\; u_i^\\top \\text{Cov}(\\varepsilon) u_i \\;=\\; u_i^\\top (\\sigma^2 I) u_i \\;=\\; \\sigma^2 (u_i^\\top u_i) \\;=\\; \\sigma^2\n$$\nsince $u_i$ is a unit vector.\nCombining all parts, the MSE is:\n$$\n\\text{MSE} \\;=\\; \\sum_{i=1}^r \\left( (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 + \\frac{f_i(\\alpha)^2 \\sigma^2}{\\sigma_i^2} \\right) + \\sum_{j=r+1}^n \\langle x, v_j \\rangle^2\n$$\n\n### Part 2: Interpretation of Tikhonov Regularization\n\nFor Tikhonov regularization, the filter functions are $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha}$. The MSE consists of three parts: squared bias, variance, and the irreducible null-space error. The regularization parameter $\\alpha$ only affects the first two.\n\nThe squared bias is the first term in the summation:\n$$\n\\text{Bias}^2(\\alpha) \\;=\\; \\sum_{i=1}^r (f_i(\\alpha)-1)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha}-1\\right)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\left(\\frac{-\\alpha}{\\sigma_i^2+\\alpha}\\right)^2 \\langle x, v_i \\rangle^2 \\;=\\; \\sum_{i=1}^r \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} \\langle x, v_i \\rangle^2\n$$\nThe variance is the second term:\n$$\n\\text{Var}(\\alpha) \\;=\\; \\sum_{i=1}^r \\frac{f_i(\\alpha)^2 \\sigma^2}{\\sigma_i^2} \\;=\\; \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2+\\alpha}\\right)^2 \\frac{\\sigma^2}{\\sigma_i^2} \\;=\\; \\sum_{i=1}^r \\frac{\\sigma_i^2}{(\\sigma_i^2+\\alpha)^2} \\sigma^2\n$$\nTo analyze the effect of increasing $\\alpha > 0$:\n- **Squared Bias**: For each component $i$, the term $\\frac{\\alpha}{\\sigma_i^2+\\alpha}$ is a monotonically increasing function of $\\alpha$. As $\\alpha \\to 0^+$, the term approaches $0$. As $\\alpha \\to \\infty$, the term approaches $1$. Therefore, the total squared bias, $\\text{Bias}^2(\\alpha)$, is a monotonically increasing function of $\\alpha$. Increasing $\\alpha$ strengthens the regularization, shrinking the estimate towards $0$. This introduces a larger systematic deviation (bias) from the true solution.\n- **Variance**: For each component $i$, the term $\\frac{1}{(\\sigma_i^2+\\alpha)^2}$ is a monotonically decreasing function of $\\alpha$. As $\\alpha \\to 0^+$, the term is $\\frac{1}{(\\sigma_i^2)^2}$, and the variance term $(\\sigma^2/\\sigma_i^2)$ can be large for small $\\sigma_i$. As $\\alpha \\to \\infty$, the term approaches $0$. Therefore, the total variance, $\\text{Var}(\\alpha)$, is a monotonically decreasing function of $\\alpha$. Increasing $\\alpha$ suppresses the amplification of noise, particularly for components associated with small singular values, thus reducing the overall variance of the estimator.\n\nIn conclusion, increasing the regularization parameter $\\alpha$ increases the squared bias while decreasing the variance. This is the canonical bias-variance trade-off in regularization.\n\n### Part 3: Optimal $\\alpha$ for the 1D Case\n\nIn the $1$-dimensional case, we have $r=1$, $\\sigma_1 = s > 0$, and $|\\langle x, v_1\\rangle| = a$. Let $c = \\langle x, v_1\\rangle$, so $c^2 = a^2$. The MSE to be minimized, as a function of $\\alpha$, is the sum of the bias and variance terms (the null-space error is constant and does not affect the location of the minimum):\n$$\nJ(\\alpha) \\;=\\; \\frac{\\alpha^2}{(s^2+\\alpha)^2} c^2 + \\frac{s^2}{(s^2+\\alpha)^2} \\sigma^2 \\;=\\; \\frac{\\alpha^2 c^2 + s^2 \\sigma^2}{(s^2+\\alpha)^2}\n$$\nTo find the value $\\alpha^{\\star}$ that minimizes $J(\\alpha)$ for $\\alpha > 0$, we compute the derivative with respect to $\\alpha$ and set it to zero. We assume $a > 0$, which implies $c \\neq 0$. If $a=0$, the bias term vanishes, and $J(\\alpha)$ becomes a strictly decreasing function of $\\alpha$, meaning no minimum exists in $(0, \\infty)$.\nUsing the quotient rule for differentiation:\n$$\n\\frac{dJ}{d\\alpha} \\;=\\; \\frac{(2\\alpha c^2)(s^2+\\alpha)^2 - (\\alpha^2 c^2 + s^2 \\sigma^2)(2(s^2+\\alpha))}{(s^2+\\alpha)^4}\n$$\nSetting the numerator to zero and dividing by the non-zero factor $2(s^2+\\alpha)$ (since $s>0, \\alpha>0$), we get:\n$$\n\\alpha c^2 (s^2+\\alpha) - (\\alpha^2 c^2 + s^2 \\sigma^2) = 0\n$$\n$$\n\\alpha s^2 c^2 + \\alpha^2 c^2 - \\alpha^2 c^2 - s^2 \\sigma^2 \\;=\\; 0\n$$\n$$\n\\alpha s^2 c^2 - s^2 \\sigma^2 \\;=\\; 0\n$$\nSince $s > 0$, we can divide by $s^2$:\n$$\n\\alpha c^2 = \\sigma^2\n$$\nSolving for $\\alpha$, we get:\n$$\n\\alpha^{\\star} \\;=\\; \\frac{\\sigma^2}{c^2}\n$$\nSubstituting $c^2=a^2$, the optimal regularization parameter is:\n$$\n\\alpha^{\\star} \\;=\\; \\frac{\\sigma^2}{a^2}\n$$\nThe second derivative test would confirm this is a minimum. The sign of the first derivative changes from negative to positive around this point, confirming it as a minimum.",
            "answer": "$$\\boxed{\\frac{\\sigma^2}{a^2}}$$"
        },
        {
            "introduction": "After establishing the theory, it is crucial to build intuition through practical application. This exercise transitions from abstract formulas to concrete numerical exploration by focusing on the Truncated SVD (TSVD) estimator, another cornerstone of regularization . By implementing the derived expressions for bias and variance, you will directly observe how the optimal truncation level $k$ emerges from balancing these two competing error sources, and design a scenario to see how this balance can be surprisingly sensitive.",
            "id": "3368394",
            "problem": "Consider a linear inverse problem with observation model $b \\in \\mathbb{R}^{m}$ given by $b = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ has a Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ containing nonnegative singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{r}  0$ on the diagonal with $r = \\mathrm{rank}(A)$. Assume additive noise $\\varepsilon$ is zero-mean Gaussian with covariance $\\sigma_{\\varepsilon}^{2} I_{m}$, where $I_{m}$ is the $m \\times m$ identity matrix. Define the truncated Singular Value Decomposition (SVD) estimator of order $k$ (with $k \\in \\{0,1,\\dots,r\\}$) as\n$$\nx_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i},\n$$\nwhere $u_{i}$ and $v_{i}$ are the left and right singular vectors of $A$ corresponding to $\\sigma_{i}$. Let the true solution admit the expansion $x_{\\mathrm{true}} = \\sum_{i=1}^{r} \\alpha_{i} v_{i}$ with coefficients $\\alpha_{i} \\in \\mathbb{R}$ in the right singular vector basis.\n\nTasks:\n1. Starting from the SVD representation, orthogonality of $U$ and $V$, and the properties of Gaussian noise, derive the bias-variance decomposition of the expected squared error of the truncated SVD estimator:\n$$\n\\mathbb{E}\\left[\\|x_{k} - x_{\\mathrm{true}}\\|_{2}^{2}\\right] = \\mathrm{Bias}(k) + \\mathrm{Var}(k),\n$$\nwhere $\\mathrm{Bias}(k)$ and $\\mathrm{Var}(k)$ must be expressed explicitly in terms of the singular values $\\{\\sigma_{i}\\}$, coefficients $\\{\\alpha_{i}\\}$, and noise variance $\\sigma_{\\varepsilon}^{2}$, without invoking any shortcut formulas.\n\n2. Implement a program that, given a spectrum of singular values $\\{\\sigma_{i}\\}$ and signal coefficients $\\{\\alpha_{i}\\}$ along with a noise variance $\\sigma_{\\varepsilon}^{2}$, computes for every $k \\in \\{0,1,\\dots,r\\}$:\n   - The variance term $\\mathrm{Var}(k)$,\n   - The bias term $\\mathrm{Bias}(k)$,\n   - The total expected error $\\mathbb{E}\\left[\\|x_{k} - x_{\\mathrm{true}}\\|_{2}^{2}\\right]$,\n   and returns the truncation level $k^{\\star}$ that minimizes the expected error. In case of ties, select the smallest $k$ achieving the minimum.\n\n3. Design a synthetic spectrum demonstrating that adding a single small singular mode flips the optimal $k$. Specifically, construct a base case with $r = 3$ and then append a fourth mode $(\\sigma_{4}, \\alpha_{4})$ such that the optimal truncation level changes between the base and the augmented spectrum.\n\nUse the following test suite with the specified parameter values:\n- Test case $1$ (happy path): $r = 6$, $\\{\\sigma_{i}\\} = [\\,5.0,\\,3.0,\\,1.0,\\,0.5,\\,0.2,\\,0.1\\,]$, $\\{\\alpha_{i}\\} = [\\,1.0,\\,0.7,\\,0.5,\\,0.1,\\,0.05,\\,0.01\\,]$, and $\\sigma_{\\varepsilon}^{2} = 0.01$.\n- Test case $2$ (boundary condition with high noise): $r = 3$, $\\{\\sigma_{i}\\} = [\\,5.0,\\,3.0,\\,1.0\\,]$, $\\{\\alpha_{i}\\} = [\\,0.2,\\,0.2,\\,0.2\\,]$, and $\\sigma_{\\varepsilon}^{2} = 10.0$.\n- Test case $3$ (flip scenario):\n  - Base: $r = 3$, $\\{\\sigma_{i}\\} = [\\,10.0,\\,3.0,\\,0.5\\,]$, $\\{\\alpha_{i}\\} = [\\,1.0,\\,0.8,\\,0.01\\,]$, and $\\sigma_{\\varepsilon}^{2} = 0.0001$.\n  - Augmented: append $(\\sigma_{4}, \\alpha_{4}) = (\\,0.05,\\,0.3\\,)$ to obtain $r = 4$.\n\nOutput specification:\n- For each test case, report the result as follows:\n  - For test case $1$: return the list $[\\,k^{\\star},\\,E^{\\star},\\,\\mathrm{Bias}(k^{\\star}),\\,\\mathrm{Var}(k^{\\star})\\,]$, where $E^{\\star} = \\mathbb{E}\\left[\\|x_{k^{\\star}} - x_{\\mathrm{true}}\\|_{2}^{2}\\right]$.\n  - For test case $2$: return the list $[\\,k^{\\star},\\,E^{\\star},\\,\\mathrm{Bias}(k^{\\star}),\\,\\mathrm{Var}(k^{\\star})\\,]$.\n  - For test case $3$: return the list $[\\,k_{\\mathrm{base}}^{\\star},\\,E_{\\mathrm{base}}^{\\star},\\,\\mathrm{Bias}_{\\mathrm{base}}(k_{\\mathrm{base}}^{\\star}),\\,\\mathrm{Var}_{\\mathrm{base}}(k_{\\mathrm{base}}^{\\star}),\\,k_{\\mathrm{aug}}^{\\star},\\,E_{\\mathrm{aug}}^{\\star},\\,\\mathrm{Bias}_{\\mathrm{aug}}(k_{\\mathrm{aug}}^{\\star}),\\,\\mathrm{Var}_{\\mathrm{aug}}(k_{\\mathrm{aug}}^{\\star}),\\,\\mathrm{flip}\\,]$, where $\\mathrm{flip}$ is a boolean that is $\\,\\mathrm{True}\\,$ if and only if $k_{\\mathrm{aug}}^{\\star} \\ne k_{\\mathrm{base}}^{\\star}$.\n- Your program should produce a single line of output containing the three test case results aggregated in a comma-separated list enclosed in square brackets, with nested lists serialized without spaces. For example: $[[\\,\\dots\\,],[\\,\\dots\\,],[\\,\\dots\\,]]$. Booleans must appear as $\\,\\mathrm{True}\\,$ or $\\,\\mathrm{False}\\,$ without quotation marks.\n\nAngles are not involved. There are no physical units. Percentages are not involved. The required outputs are lists of integers, floats, and a boolean as specified. The program must be fully self-contained, require no input, and adhere to the execution environment described in the final answer section.",
            "solution": "The objective is to derive the bias-variance decomposition for the expected squared error of the truncated Singular Value Decomposition (SVD) estimator, $\\mathbb{E}\\left[\\|x_{k} - x_{\\mathrm{true}}\\|_{2}^{2}\\right]$. The squared error of an estimator $x_k$ for a true value $x_{\\mathrm{true}}$ can be decomposed into a squared bias term and a variance term:\n$$\n\\mathbb{E}\\left[\\|x_{k} - x_{\\mathrm{true}}\\|_{2}^{2}\\right] = \\underbrace{\\|\\mathbb{E}[x_k] - x_{\\mathrm{true}}\\|_{2}^{2}}_{\\mathrm{Bias}(k)} + \\underbrace{\\mathbb{E}\\left[\\|x_{k} - \\mathbb{E}[x_k]\\|_{2}^{2}\\right]}_{\\mathrm{Var}(k)}\n$$\nWe will derive explicit expressions for $\\mathrm{Bias}(k)$ and $\\mathrm{Var}(k)$ step-by-step.\n\nThe observation model is $b = A x_{\\mathrm{true}} + \\varepsilon$, where $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] = \\sigma_{\\varepsilon}^{2} I_{m}$. The truncated SVD estimator of order $k$ is given by\n$$\nx_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}\n$$\n\nFirst, we substitute the observation model into the estimator's definition:\n$$\nx_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} (A x_{\\mathrm{true}} + \\varepsilon)}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} A x_{\\mathrm{true}}}{\\sigma_{i}} v_{i} + \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i}\n$$\nThe first term is the signal component of the estimate, and the second is the noise component. Let's analyze the term $u_{i}^{\\top} A x_{\\mathrm{true}}$. Given the SVD of $A = \\sum_{j=1}^{r} \\sigma_{j} u_{j} v_{j}^{\\top}$ and the expansion of the true solution $x_{\\mathrm{true}} = \\sum_{j=1}^{r} \\alpha_{j} v_{j}$, we have:\n$$\nA x_{\\mathrm{true}} = \\left( \\sum_{j=1}^{r} \\sigma_{j} u_{j} v_{j}^{\\top} \\right) \\left( \\sum_{l=1}^{r} \\alpha_{l} v_{l} \\right) = \\sum_{j=1}^{r} \\sum_{l=1}^{r} \\sigma_{j} \\alpha_{l} u_{j} (v_{j}^{\\top} v_{l})\n$$\nDue to the orthogonality of the right singular vectors $\\{v_i\\}$, we have $v_{j}^{\\top} v_{l} = \\delta_{jl}$ (the Kronecker delta). Thus, the double summation collapses:\n$$\nA x_{\\mathrm{true}} = \\sum_{j=1}^{r} \\sigma_{j} \\alpha_{j} u_{j}\n$$\nNow, projecting this onto a left singular vector $u_i$ and using the orthogonality $u_{i}^{\\top} u_{j} = \\delta_{ij}$:\n$$\nu_{i}^{\\top} A x_{\\mathrm{true}} = u_{i}^{\\top} \\left( \\sum_{j=1}^{r} \\sigma_{j} \\alpha_{j} u_{j} \\right) = \\sum_{j=1}^{r} \\sigma_{j} \\alpha_{j} (u_{i}^{\\top} u_{j}) = \\sigma_{i} \\alpha_{i} \\quad (\\text{for } i \\le r)\n$$\nSubstituting this result back into the expression for $x_k$:\n$$\nx_{k} = \\sum_{i=1}^{k} \\frac{\\sigma_{i} \\alpha_{i}}{\\sigma_{i}} v_{i} + \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\alpha_{i} v_{i} + \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i}\n$$\nWith this expression for $x_k$, we can now find its expected value. Since $\\mathbb{E}[\\varepsilon]=0$ and the expectation is a linear operator:\n$$\n\\mathbb{E}[x_k] = \\mathbb{E}\\left[ \\sum_{i=1}^{k} \\alpha_{i} v_{i} + \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i} \\right] = \\sum_{i=1}^{k} \\alpha_{i} v_{i} + \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\mathbb{E}[\\varepsilon]}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\alpha_{i} v_{i}\n$$\n\nNow we derive the bias term, $\\mathrm{Bias}(k)$. This term represents the systematic error due to truncating the expansion.\n$$\n\\mathrm{Bias}(k) = \\|\\mathbb{E}[x_k] - x_{\\mathrm{true}}\\|_{2}^{2} = \\left\\| \\sum_{i=1}^{k} \\alpha_{i} v_{i} - \\sum_{i=1}^{r} \\alpha_{i} v_{i} \\right\\|_{2}^{2} = \\left\\| -\\sum_{i=k+1}^{r} \\alpha_{i} v_{i} \\right\\|_{2}^{2}\n$$\nUsing the orthogonality of $\\{v_i\\}$, the squared norm of this sum is the sum of the squares of the coefficients:\n$$\n\\mathrm{Bias}(k) = \\left( -\\sum_{i=k+1}^{r} \\alpha_{i} v_{i} \\right)^{\\top} \\left( -\\sum_{j=k+1}^{r} \\alpha_{j} v_{j} \\right) = \\sum_{i=k+1}^{r} \\sum_{j=k+1}^{r} \\alpha_{i} \\alpha_{j} (v_i^\\top v_j) = \\sum_{i=k+1}^{r} \\alpha_{i}^2\n$$\nFor $k=r$, the sum is empty and the bias is $0$. For $k=0$, the bias is $\\sum_{i=1}^{r} \\alpha_{i}^2 = \\|x_{\\mathrm{true}}\\|_{2}^{2}$.\n\nNext, we derive the variance term, $\\mathrm{Var}(k)$. This term represents the error contribution from the noise $\\varepsilon$.\n$$\n\\mathrm{Var}(k) = \\mathbb{E}\\left[\\|x_{k} - \\mathbb{E}[x_k]\\|_{2}^{2}\\right]\n$$\nThe deviation of the estimator from its mean is:\n$$\nx_{k} - \\mathbb{E}[x_k] = \\left( \\sum_{i=1}^{k} \\alpha_{i} v_{i} + \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i} \\right) - \\sum_{i=1}^{k} \\alpha_{i} v_{i} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i}\n$$\nIts squared norm is:\n$$\n\\|x_{k} - \\mathbb{E}[x_k]\\|_{2}^{2} = \\left\\| \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} v_{i} \\right\\|_{2}^{2} = \\sum_{i=1}^{k} \\left( \\frac{u_{i}^{\\top} \\varepsilon}{\\sigma_{i}} \\right)^2 = \\sum_{i=1}^{k} \\frac{(u_{i}^{\\top} \\varepsilon)^2}{\\sigma_{i}^2}\n$$\nWe now take the expectation over the noise $\\varepsilon$:\n$$\n\\mathrm{Var}(k) = \\mathbb{E}\\left[ \\sum_{i=1}^{k} \\frac{(u_{i}^{\\top} \\varepsilon)^2}{\\sigma_{i}^2} \\right] = \\sum_{i=1}^{k} \\frac{1}{\\sigma_{i}^2} \\mathbb{E}\\left[ (u_{i}^{\\top} \\varepsilon)^2 \\right]\n$$\nThe term $(u_{i}^{\\top} \\varepsilon)$ is a scalar, so $(u_{i}^{\\top} \\varepsilon)^2 = (u_{i}^{\\top} \\varepsilon)(u_{i}^{\\top} \\varepsilon)^\\top = u_{i}^{\\top} \\varepsilon \\varepsilon^{\\top} u_{i}$.\n$$\n\\mathbb{E}\\left[ (u_{i}^{\\top} \\varepsilon)^2 \\right] = \\mathbb{E}\\left[ u_{i}^{\\top} \\varepsilon \\varepsilon^{\\top} u_{i} \\right] = u_{i}^{\\top} \\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] u_{i}\n$$\nUsing the noise covariance $\\mathbb{E}[\\varepsilon \\varepsilon^{\\top}] = \\sigma_{\\varepsilon}^{2} I_m$:\n$$\n\\mathbb{E}\\left[ (u_{i}^{\\top} \\varepsilon)^2 \\right] = u_{i}^{\\top} (\\sigma_{\\varepsilon}^{2} I_m) u_{i} = \\sigma_{\\varepsilon}^{2} (u_{i}^{\\top} u_{i})\n$$\nSince $u_i$ are orthonormal unit vectors, $u_{i}^{\\top} u_{i} = 1$. Thus, $\\mathbb{E}\\left[ (u_{i}^{\\top} \\varepsilon)^2 \\right] = \\sigma_{\\varepsilon}^{2}$.\nSubstituting this back into the variance expression:\n$$\n\\mathrm{Var}(k) = \\sum_{i=1}^{k} \\frac{\\sigma_{\\varepsilon}^{2}}{\\sigma_{i}^2} = \\sigma_{\\varepsilon}^{2} \\sum_{i=1}^{k} \\frac{1}{\\sigma_{i}^2}\n$$\nFor $k=0$, the sum is empty and the variance is $0$.\n\nCombining the bias and variance terms, we obtain the final expression for the expected squared error of the truncated SVD estimator:\n$$\n\\mathbb{E}\\left[\\|x_{k} - x_{\\mathrm{true}}\\|_{2}^{2}\\right] = \\underbrace{\\sum_{i=k+1}^{r} \\alpha_{i}^2}_{\\mathrm{Bias}(k)} + \\underbrace{\\sigma_{\\varepsilon}^{2} \\sum_{i=1}^{k} \\frac{1}{\\sigma_{i}^2}}_{\\mathrm{Var}(k)}\n$$\nThis decomposition quantitatively expresses the bias-variance trade-off: increasing the truncation level $k$ decreases the bias (by including more signal components) but increases the variance (by amplifying noise through small singular values $\\sigma_i$). The optimal truncation $k^{\\star}$ minimizes this total error.",
            "answer": "```python\nimport numpy as np\n\ndef find_optimal_k(sigma, alpha, sigma_eps_sq):\n    \"\"\"\n    Computes the optimal truncation k for a truncated SVD estimator.\n\n    Args:\n        sigma (list or np.ndarray): Singular values {sigma_i}.\n        alpha (list or np.ndarray): Signal coefficients {alpha_i}.\n        sigma_eps_sq (float): Noise variance sigma_epsilon^2.\n\n    Returns:\n        list: A list containing [k_star, E_star, Bias_star, Var_star].\n    \"\"\"\n    sigma_arr = np.array(sigma)\n    alpha_arr = np.array(alpha)\n    r = len(sigma_arr)\n    \n    errors = []\n    biases = []\n    variances = []\n\n    for k in range(r + 1):\n        # Bias term: sum of squared alpha coefficients for truncated modes\n        # np.sum on an empty slice (alpha_arr[k:]) correctly returns 0.\n        bias_k = np.sum(alpha_arr[k:]**2)\n        \n        # Variance term: sum of noise amplification for included modes\n        # np.sum on an empty slice (sigma_arr[:k]) correctly returns 0.\n        var_k = sigma_eps_sq * np.sum(1 / sigma_arr[:k]**2)\n        \n        errors.append(bias_k + var_k)\n        biases.append(bias_k)\n        variances.append(var_k)\n\n    # Find the index k* that minimizes the total error.\n    # np.argmin returns the first occurrence in case of a tie, which corresponds\n    # to the smallest k as required.\n    k_star = np.argmin(errors)\n    \n    E_star = errors[k_star]\n    Bias_star = biases[k_star]\n    Var_star = variances[k_star]\n\n    return [k_star, E_star, Bias_star, Var_star]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"type\": \"happy_path\",\n            \"params\": {\n                \"sigma\": [5.0, 3.0, 1.0, 0.5, 0.2, 0.1],\n                \"alpha\": [1.0, 0.7, 0.5, 0.1, 0.05, 0.01],\n                \"sigma_eps_sq\": 0.01\n            }\n        },\n        {\n            \"type\": \"high_noise\",\n            \"params\": {\n                \"sigma\": [5.0, 3.0, 1.0],\n                \"alpha\": [0.2, 0.2, 0.2],\n                \"sigma_eps_sq\": 10.0\n            }\n        },\n        {\n            \"type\": \"flip_scenario\",\n            \"params\": {\n                \"base\": {\n                    \"sigma\": [10.0, 3.0, 0.5],\n                    \"alpha\": [1.0, 0.8, 0.01],\n                    \"sigma_eps_sq\": 0.0001\n                },\n                \"augment\": {\n                    \"sigma_add\": 0.05,\n                    \"alpha_add\": 0.3\n                }\n            }\n        }\n    ]\n\n    results = []\n    \n    # Test case 1\n    params1 = test_cases[0][\"params\"]\n    res1 = find_optimal_k(params1[\"sigma\"], params1[\"alpha\"], params1[\"sigma_eps_sq\"])\n    results.append(res1)\n    \n    # Test case 2\n    params2 = test_cases[1][\"params\"]\n    res2 = find_optimal_k(params2[\"sigma\"], params2[\"alpha\"], params2[\"sigma_eps_sq\"])\n    results.append(res2)\n    \n    # Test case 3\n    params3 = test_cases[2][\"params\"]\n    base_params = params3[\"base\"]\n    aug_params = params3[\"augment\"]\n    \n    # Base case\n    res3_base = find_optimal_k(base_params[\"sigma\"], base_params[\"alpha\"], base_params[\"sigma_eps_sq\"])\n    \n    # Augmented case\n    sigma3_aug = base_params[\"sigma\"] + [aug_params[\"sigma_add\"]]\n    alpha3_aug = base_params[\"alpha\"] + [aug_params[\"alpha_add\"]]\n    res3_aug = find_optimal_k(sigma3_aug, alpha3_aug, base_params[\"sigma_eps_sq\"])\n    \n    k_base_star = res3_base[0]\n    k_aug_star = res3_aug[0]\n    flip = k_base_star != k_aug_star\n    \n    res3 = res3_base + res3_aug + [flip]\n    results.append(res3)\n\n    def format_list(lst):\n        # Custom formatter to match the required output (no spaces after commas)\n        items = []\n        for item in lst:\n            if isinstance(item, bool):\n                items.append(str(item))\n            else:\n                items.append(str(item))\n        return f\"[{','.join(items)}]\"\n        \n    output_str = f\"[{','.join(format_list(r) for r in results)}]\"\n    print(output_str.replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n```"
        },
        {
            "introduction": "The principles of regularization extend far beyond static inverse problems. This final practice demonstrates the universality of the bias-variance trade-off by applying it within the dynamic context of data assimilation . By analyzing a simple state-space model, you will discover how the system's dynamic model acts as a form of regularization, penalizing deviations from expected behavior. This exercise reveals how the balance between trusting the physical model (low variance, high potential bias) and fitting new observations (low bias, high potential variance) is fundamental to filtering and smoothing problems.",
            "id": "3368359",
            "problem": "Consider a scalar linear state-space model over a two-step window $t \\in \\{0,1\\}$, with dynamics $x_{1}=\\rho x_{0}+w_{0}$ and observations $y_{t}=x_{t}+\\varepsilon_{t}$. Assume $w_{0}$ is drawn from a zero-mean Gaussian distribution with variance $Q$, and $\\varepsilon_{t}$ are independent zero-mean Gaussian random variables with variance $R$, all mutually independent. Let the prior for $x_{0}$ be an improper flat prior, understood as the limit of a Gaussian prior with variance tending to infinity. Define the Maximum A Posteriori (MAP) estimator of $(x_{0},x_{1})$ under these assumptions. Then, treating the true states $x_{0}$ and $x_{1}$ as fixed (unknown) constants, and averaging only over the measurement noise $\\varepsilon_{0}$ and $\\varepsilon_{1}$, derive the bias of the MAP estimator of $x_{1}$, the variance of the MAP estimator of $x_{1}$, and combine them to obtain the mean squared error at time $t=1$, defined as $\\mathbb{E}\\big[(\\hat{x}_{1}-x_{1})^{2}\\big]$, as an explicit analytic function of $Q$, $R$, $\\rho$, $x_{0}$, and $x_{1}$. Report your final answer as a single closed-form analytical expression for the mean squared error in exact form. No units are required. If you carry out any numerical simplifications, do not round.",
            "solution": "The model is linear-Gaussian. With the improper flat prior for $x_{0}$ and independent Gaussian noises, the posterior density for $(x_{0},x_{1})$ is proportional to the product of three Gaussian likelihoods: one for $y_{0}$ given $x_{0}$, one for $y_{1}$ given $x_{1}$, and one for $x_{1}$ given $x_{0}$. Therefore, the negative log-posterior (up to an additive constant) is the weighted sum of squared residuals\n$$\nJ(x_{0},x_{1}) \\;=\\; \\frac{1}{R}\\,(y_{0}-x_{0})^{2} \\;+\\; \\frac{1}{R}\\,(y_{1}-x_{1})^{2} \\;+\\; \\frac{1}{Q}\\,(x_{1}-\\rho x_{0})^{2}.\n$$\nThe Maximum A Posteriori (MAP) estimate $(\\hat{x}_{0},\\hat{x}_{1})$ minimizes $J(x_{0},x_{1})$. This is a strictly convex quadratic function, so its minimizer is obtained by setting the gradient to zero:\n$$\n\\frac{\\partial J}{\\partial x_{0}} \\;=\\; -\\frac{2}{R}(y_{0}-x_{0}) \\;-\\; \\frac{2\\rho}{Q}(x_{1}-\\rho x_{0}) \\;=\\; 0,\n$$\n$$\n\\frac{\\partial J}{\\partial x_{1}} \\;=\\; -\\frac{2}{R}(y_{1}-x_{1}) \\;+\\; \\frac{2}{Q}(x_{1}-\\rho x_{0}) \\;=\\; 0.\n$$\nRearranging these two equations gives a linear system in $(x_{0},x_{1})$. Define $A=\\frac{1}{R}$ and $B=\\frac{1}{Q}$ to streamline notation. Then the normal equations can be written as\n$$\n\\begin{pmatrix}\nA + \\rho^{2}B  -\\rho B \\\\\n-\\rho B  A + B\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{0} \\\\\nx_{1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nA y_{0} \\\\\nA y_{1}\n\\end{pmatrix}.\n$$\nLet $M$ be the $2\\times 2$ matrix above and $b$ the right-hand vector. The determinant of $M$ is\n$$\n\\det(M) \\;=\\; (A + \\rho^{2}B)(A + B) - (\\rho B)^{2} \\;=\\; A^{2} + A B (1 + \\rho^{2}) \\;=\\; A\\big(A + B(1+\\rho^{2})\\big).\n$$\nThe inverse of $M$ is\n$$\nM^{-1} \\;=\\; \\frac{1}{\\det(M)} \\begin{pmatrix}\nA + B  \\rho B \\\\\n\\rho B  A + \\rho^{2}B\n\\end{pmatrix}.\n$$\nThus, the MAP estimates are\n$$\n\\begin{pmatrix}\n\\hat{x}_{0} \\\\\n\\hat{x}_{1}\n\\end{pmatrix}\n\\;=\\;\nM^{-1} b\n\\;=\\;\n\\frac{1}{\\det(M)}\n\\begin{pmatrix}\n(A + B)A y_{0} + \\rho B A y_{1} \\\\\n\\rho B A y_{0} + (A + \\rho^{2}B) A y_{1}\n\\end{pmatrix}.\n$$\nIn particular,\n$$\n\\hat{x}_{1} \\;=\\; \\alpha\\, y_{0} + \\beta\\, y_{1},\n\\quad\\text{with}\\quad\n\\alpha \\;=\\; \\frac{\\rho A B}{\\det(M)}, \\qquad \\beta \\;=\\; \\frac{A\\left(A + \\rho^{2}B\\right)}{\\det(M)}.\n$$\nWe now quantify the bias-variance trade-off for $\\hat{x}_{1}$ by treating the true states $x_{0}$ and $x_{1}$ as fixed constants, and averaging over the independent measurement noises $\\varepsilon_{0}$ and $\\varepsilon_{1}$. By definition,\n$$\ny_{0} \\;=\\; x_{0} + \\varepsilon_{0}, \\qquad y_{1} \\;=\\; x_{1} + \\varepsilon_{1}, \\qquad \\mathbb{E}[\\varepsilon_{t}] \\;=\\; 0, \\qquad \\operatorname{Var}(\\varepsilon_{t}) \\;=\\; R, \\qquad \\operatorname{Cov}(\\varepsilon_{0},\\varepsilon_{1}) \\;=\\; 0.\n$$\nThe bias is\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\mathbb{E}[\\hat{x}_{1}] - x_{1}\n\\;=\\; \\alpha\\, x_{0} + \\beta\\, x_{1} - x_{1}\n\\;=\\; \\alpha\\, x_{0} + (\\beta - 1)\\, x_{1}.\n$$\nUsing $\\det(M)=A^{2}+A B(1+\\rho^{2})$ and the formulas for $\\alpha$ and $\\beta$,\n$$\n\\beta - 1 \\;=\\; \\frac{A(A+\\rho^{2}B)}{\\det(M)} - 1 \\;=\\; \\frac{A(A+\\rho^{2}B) - \\det(M)}{\\det(M)} \\;=\\; \\frac{-A B}{\\det(M)}.\n$$\nTherefore,\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\frac{\\rho A B}{\\det(M)}\\, x_{0} \\;-\\; \\frac{A B}{\\det(M)}\\, x_{1}\n\\;=\\; \\frac{A B}{\\det(M)}\\,\\big(\\rho x_{0} - x_{1}\\big).\n$$\nSubstituting back $A=\\frac{1}{R}$, $B=\\frac{1}{Q}$, and $\\det(M)=\\frac{1}{R}\\Big(\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}\\Big)$, the bias simplifies to\n$$\n\\operatorname{Bias}(\\hat{x}_{1}) \\;=\\; \\frac{\\frac{1}{R}\\cdot \\frac{1}{Q}}{\\frac{1}{R}\\left(\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}\\right)}\\,\\big(\\rho x_{0}-x_{1}\\big)\n\\;=\\; \\frac{\\frac{1}{Q}}{\\frac{1}{R}+\\frac{1+\\rho^{2}}{Q}}\\,\\big(\\rho x_{0}-x_{1}\\big)\n\\;=\\; \\frac{R}{Q + R(1+\\rho^{2})}\\,\\big(\\rho x_{0}-x_{1}\\big).\n$$\nNext, the variance of $\\hat{x}_{1}$ is\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; \\alpha^{2}\\operatorname{Var}(y_{0}) + \\beta^{2}\\operatorname{Var}(y_{1}) \\;=\\; R\\left(\\alpha^{2} + \\beta^{2}\\right).\n$$\nCompute $\\alpha^{2} + \\beta^{2}$ in terms of $A$ and $B$:\n$$\n\\alpha^{2} + \\beta^{2} \\;=\\; \\frac{(\\rho A B)^{2} + \\left(A(A + \\rho^{2}B)\\right)^{2}}{\\det(M)^{2}}\n\\;=\\; \\frac{A^{2}\\left(\\rho^{2}B^{2} + (A + \\rho^{2}B)^{2}\\right)}{\\det(M)^{2}}.\n$$\nBecause $\\det(M)=A\\big(A + B(1+\\rho^{2})\\big)$, we have $\\det(M)^{2} = A^{2}\\big(A + B(1+\\rho^{2})\\big)^{2}$. Hence\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{\\rho^{2}B^{2} + \\left(A + \\rho^{2}B\\right)^{2}}{\\left(A + B(1+\\rho^{2})\\right)^{2}}.\n$$\nSubstituting $A=\\frac{1}{R}$ and $B=\\frac{1}{Q}$ gives\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{\\frac{\\rho^{2}}{Q^{2}} + \\left(\\frac{1}{R} + \\frac{\\rho^{2}}{Q}\\right)^{2}}{\\left(\\frac{1}{R} + \\frac{1+\\rho^{2}}{Q}\\right)^{2}}.\n$$\nFor algebraic clarity, multiply numerator and denominator by $R^{2}Q^{2}$ to obtain an equivalent, simplified rational form:\n$$\n\\operatorname{Var}(\\hat{x}_{1}) \\;=\\; R\\,\\frac{Q^{2} + 2\\rho^{2} R Q + R^{2}\\rho^{2}(1+\\rho^{2})}{Q^{2} + 2(1+\\rho^{2}) R Q + R^{2}(1+\\rho^{2})^{2}}.\n$$\nFinally, the mean squared error is\n$$\n\\operatorname{MSE}(\\hat{x}_{1}) \\;=\\; \\big(\\operatorname{Bias}(\\hat{x}_{1})\\big)^{2} + \\operatorname{Var}(\\hat{x}_{1}),\n$$\nwhich yields\n$$\n\\operatorname{MSE}(\\hat{x}_{1}) \\;=\\; \\left(\\frac{R}{Q + R(1+\\rho^{2})}\\right)^{2}\\big(\\rho x_{0} - x_{1}\\big)^{2} \\;+\\; R\\,\\frac{Q^{2} + 2\\rho^{2} R Q + R^{2}\\rho^{2}(1+\\rho^{2})}{Q^{2} + 2(1+\\rho^{2}) R Q + R^{2}(1+\\rho^{2})^{2}}.\n$$\nThis expression explicitly quantifies the bias-variance trade-off as a function of $Q$ and $R$: as $Q$ decreases (stronger dynamical regularization), the bias term grows when the true states deviate from the exact dynamical relation ($\\rho x_{0} \\neq x_{1}$), while the variance term decreases; conversely, as $Q$ increases (weaker regularization), the bias term shrinks and the variance term increases toward $R$.",
            "answer": "$$\\boxed{\\left(\\frac{R}{Q + R(1+\\rho^{2})}\\right)^{2}\\left(\\rho x_{0} - x_{1}\\right)^{2} \\;+\\; R\\,\\frac{Q^{2} + 2\\rho^{2} R Q + R^{2}\\rho^{2}(1+\\rho^{2})}{Q^{2} + 2(1+\\rho^{2}) R Q + R^{2}(1+\\rho^{2})^{2}}}$$"
        }
    ]
}