{
    "hands_on_practices": [
        {
            "introduction": "本章的第一个实践将为你奠定坚实的理论基础。我们将深入分析最基础的迭代正则化方法——Landweber迭代法，并通过推导其最优固定步长，深刻理解迭代的收敛速度如何依赖于前向算子的谱特性（即奇异值）。这项练习旨在锻炼你的分析能力，并揭示算子属性与算法参数设计之间的内在联系，这是掌握所有迭代方法的关键一步。",
            "id": "3392775",
            "problem": "考虑一个有限维实希尔伯特空间上的线性逆问题，其精确数据模型为 $y = A x^{\\dagger}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个有界线性算子，未知量为 $x^{\\dagger} \\in \\mathbb{R}^{n}$。令 $L = \\|A\\|$ 表示由欧几里得范数诱导的算子 $A$ 的算子范数。假设初始猜测 $x_{0}$ 位于 $A$ 的零空间的正交补中，即 $x_{0} \\in \\mathcal{N}(A)^{\\perp}$，从而迭代在 $A$ 具有严格正奇异值的子空间上进行。进一步假设 $A$ 的非零奇异值由一个已知常数 $\\underline{\\sigma}  0$ 作为下界，因此所有相关奇异值满足 $\\sigma_{i} \\in [\\underline{\\sigma}, L]$。\n\n考虑使用常数步长 $\\omega  0$ 的 Landweber 迭代，\n$$\nx_{k+1} = x_{k} + \\omega A^{\\ast} \\big( y - A x_{k} \\big),\n$$\n其中 $A^{\\ast}$ 表示 $A$ 的转置（伴随）。令误差为 $e_{k} = x_{k} - x^{\\dagger}$，并在 $A$ 的奇异向量基中衡量收敛性。\n\n从奇异值分解和诱导算子范数的基本定义出发，推导迭代过程中误差分量的演化。仅使用界 $\\underline{\\sigma}$ 和 $L$，确定常数步长 $\\omega^{\\star}$，该步长通过最小化所有奇异分量上的最坏情况单次迭代收缩因子来最大化渐近线性收敛率。然后，在 $\\omega^{\\star}$ 下，量化奇异向量基中每个奇异分量所对应的单次迭代收缩因子。\n\n你的最终答案必须是单一的解析表达式，并包含：\n- 用 $\\underline{\\sigma}$ 和 $L$ 表示的最优常数步长 $\\omega^{\\star}$。\n- 在 $\\omega^{\\star}$ 下，对于奇异值为 $\\sigma \\in [\\underline{\\sigma}, L]$ 的奇异分量的单次迭代收缩因子。\n\n无需进行数值近似或四舍五入。",
            "solution": "该问题是有效的。这是一个关于 Landweber 迭代收敛性分析的适定的、有科学依据的逆问题。所有必要的信息都已提供，且术语标准明确。\n\n我们首先分析 Landweber 迭代的误差传播。迭代公式为\n$$\nx_{k+1} = x_{k} + \\omega A^{\\ast} ( y - A x_{k} ),\n$$\n其中 $y = A x^{\\dagger}$ 是精确数据，$x^{\\dagger}$ 是真实解，$A^{\\ast}$ 是 $A$ 的伴随（转置），$\\omega  0$ 是常数步长。第 $k$ 次迭代的误差定义为 $e_k = x_k - x^{\\dagger}$。\n\n为推导误差的演化，我们在迭代方程两边同时减去 $x^{\\dagger}$：\n$$\nx_{k+1} - x^{\\dagger} = x_{k} - x^{\\dagger} + \\omega A^{\\ast} ( y - A x_{k} ).\n$$\n代入误差 $e_{k+1}$ 和 $e_k$ 的定义以及数据模型 $y = A x^{\\dagger}$，我们得到：\n$$\ne_{k+1} = e_k + \\omega A^{\\ast} ( A x^{\\dagger} - A x_{k} ) = e_k - \\omega A^{\\ast} A (x_k - x^{\\dagger}) = e_k - \\omega A^{\\ast} A e_k.\n$$\n这可以写成线性误差更新方程：\n$$\ne_{k+1} = (I - \\omega A^{\\ast} A) e_k,\n$$\n其中 $I$ 是单位算子。\n\n为了分析收敛性，我们研究迭代算子 $R(\\omega) = I - \\omega A^{\\ast} A$ 的谱性质。这最好在 $A$ 的奇异值分解（SVD）基中进行。设 $A$ 的 SVD 为 $A = U \\Sigma V^{\\ast}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是包含奇异值 $\\sigma_i \\ge 0$ 的对角矩阵。$V$ 的列，记为 $v_i$，是 $A$ 的右奇异向量，并构成 $\\mathbb{R}^n$ 的一个标准正交基。\n\n算子 $A^{\\ast}A$ 可以用 SVD 表示为：\n$$\nA^{\\ast}A = (V \\Sigma^{\\ast} U^{\\ast}) (U \\Sigma V^{\\ast}) = V \\Sigma^{\\ast} \\Sigma V^{\\ast}.\n$$\n矩阵 $\\Sigma^{\\ast} \\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角线元素为 $\\sigma_i^2$。右奇异向量 $v_i$ 是 $A^{\\ast}A$ 的特征向量：\n$$\nA^{\\ast} A v_i = (V \\Sigma^{\\ast} \\Sigma V^{\\ast}) v_i = \\sigma_i^2 v_i.\n$$\n因此，奇异向量 $v_i$ 也是误差传播算子 $R(\\omega)$ 的特征向量：\n$$\nR(\\omega) v_i = (I - \\omega A^{\\ast} A) v_i = v_i - \\omega (\\sigma_i^2 v_i) = (1 - \\omega \\sigma_i^2) v_i.\n$$\n$R(\\omega)$ 对应的特征值为 $\\lambda_i = 1 - \\omega \\sigma_i^2$。\n\n问题陈述迭代被限制在 $\\mathcal{N}(A)^{\\perp}$，即与非零奇异值 $\\sigma_i  0$ 对应的右奇异向量 $v_i$ 的张成空间。将误差在此基中展开：$e_k = \\sum_{i} c_{k,i} v_i$。系数的演化由下式给出：\n$$\ne_{k+1} = \\sum_{i} c_{k+1,i} v_i = R(\\omega) e_k = \\sum_{i} c_{k,i} R(\\omega) v_i = \\sum_{i} c_{k,i} (1 - \\omega \\sigma_i^2) v_i.\n$$\n因此，误差沿每个奇异向量 $v_i$ 的分量在每次迭代中被其对应的特征值所缩放：$c_{k+1,i} = (1 - \\omega \\sigma_i^2) c_{k,i}$。第 $i$ 个分量的单次迭代收缩因子为 $\\rho_i = |1 - \\omega \\sigma_i^2|$。\n\n为使迭代对所有分量都收敛，我们要求对所有相关的 $i$ 都有 $|\\rho_i|  1$。这意味着对于给定范围 $[\\underline{\\sigma}, L]$ 内的所有奇异值 $\\sigma$，我们需要 $|1 - \\omega \\sigma^2|  1$，其中 $L = \\|A\\| = \\max_i \\sigma_i$，$\\underline{\\sigma}  0$ 是非零奇异值的下界。这个条件等价于 $-1  1 - \\omega \\sigma^2  1$，可以简化为 $0  \\omega \\sigma^2  2$，或 $0  \\omega  \\frac{2}{\\sigma^2}$。为了对所有 $\\sigma \\in [\\underline{\\sigma}, L]$ 都满足这个条件，我们必须在最严格的情况下（即 $\\sigma = L$）满足它。因此，我们必须有 $0  \\omega  \\frac{2}{L^2}$。\n\n目标是找到能最大化渐近收敛率的最优常数步长 $\\omega^{\\star}$。这等价于最小化在所有可能模式（即所有 $\\sigma \\in [\\underline{\\sigma}, L]$）上的最坏情况（最大）收缩因子。我们必须解决以下极小化极大问题：\n$$\n\\omega^{\\star} = \\arg\\min_{\\omega  0} \\left( \\max_{\\sigma \\in [\\underline{\\sigma}, L]} |1 - \\omega \\sigma^2| \\right).\n$$\n令 $s = \\sigma^2$。问题变为找到能最小化函数 $g(\\omega) = \\max_{s \\in [\\underline{\\sigma}^2, L^2]} |1 - \\omega s|$ 的 $\\omega^{\\star}$。函数 $f(s) = 1 - \\omega s$ 是一条斜率为负的直线。其绝对值 $|f(s)|$ 在区间 $[\\underline{\\sigma}^2, L^2]$ 上的最大值必然出现在区间的某个端点上。因此，\n$$\ng(\\omega) = \\max \\{ |1 - \\omega \\underline{\\sigma}^2|, |1 - \\omega L^2| \\}.\n$$\n$g(\\omega)$ 的最小值在两个端点处的值大小相等时达到：\n$$\n|1 - \\omega \\underline{\\sigma}^2| = |1 - \\omega L^2|.\n$$\n由于 $L  \\underline{\\sigma}$ 且 $\\omega  0$，我们有 $1 - \\omega \\underline{\\sigma}^2  1 - \\omega L^2$。为了使大小相等，我们必须让一个值是另一个值的相反数：\n$$\n1 - \\omega \\underline{\\sigma}^2 = -(1 - \\omega L^2) = \\omega L^2 - 1.\n$$\n解出 $\\omega$：\n$$\n2 = \\omega L^2 + \\omega \\underline{\\sigma}^2 = \\omega(L^2 + \\underline{\\sigma}^2).\n$$\n这就得出了最优步长：\n$$\n\\omega^{\\star} = \\frac{2}{L^2 + \\underline{\\sigma}^2}.\n$$\n使用这个最优步长，我们可以确定奇异值为 $\\sigma \\in [\\underline{\\sigma}, L]$ 的奇异分量的单次迭代收缩因子。这个因子，我们记为 $\\rho(\\sigma)$，是：\n$$\n\\rho(\\sigma) = |1 - \\omega^{\\star} \\sigma^2| = \\left| 1 - \\frac{2\\sigma^2}{L^2 + \\underline{\\sigma}^2} \\right|.\n$$\n当使用最优常数步长 $\\omega^{\\star}$ 时，此表达式给出了指定范围内任意模式 $\\sigma$ 的收缩因子。最坏情况的收缩因子发生在边界 $\\sigma = \\underline{\\sigma}$ 和 $\\sigma = L$ 处，其值为 $\\frac{L^2 - \\underline{\\sigma}^2}{L^2 + \\underline{\\sigma}^2}$。\n\n题目要求的两个量是最优步长 $\\omega^{\\star}$ 和通用收缩因子 $\\rho(\\sigma)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{L^2 + \\underline{\\sigma}^2} \\\\ \\left| 1 - \\frac{2 \\sigma^2}{L^2 + \\underline{\\sigma}^2} \\right| \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在第二个实践中，理论将与一个核心的实际问题相遇：数据中的噪声。我们将构建一个场景，其中一个不加约束的迭代解会灾难性地“过拟合”于噪声，从而直观地展示“半收敛”（semi-convergence）现象。你将亲手实现经典的差异原则（discrepancy principle），学习如何利用已知的噪声水平来在恰当的时机停止迭代，以获得稳定且有意义的解。",
            "id": "3392768",
            "problem": "考虑一个带加性噪声的有限维线性反问题。设 $A \\in \\mathbb{R}^{n \\times n}$ 是一个病态方阵，$x^\\dagger \\in \\mathbb{R}^n$ 是精确解，$y = A x^\\dagger$ 是无噪声数据。带噪数据为 $y^\\delta = y + e^\\delta$，其中 $\\lVert e^\\delta \\rVert_2 = \\delta$。您将研究两种迭代方法的行为：(i) 采用偏差原理停止准则的 Landweber 迭代，以及 (ii) 持续进行大量固定次数迭代的无约束梯度下降。您的目标是构建一个对抗性噪声场景，并量化基于偏差原理的停止准则相对于无约束下降法如何避免过拟合。\n\n基本原理和定义：\n- $A$ 的奇异值分解 (SVD) 为 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_n)$ 且 $\\sigma_1 \\ge \\cdots \\ge \\sigma_n  0$。\n- 用于最小化数据失配项 $\\frac{1}{2} \\lVert A x - y^\\delta \\rVert_2^2$ 的 Landweber 迭代法为 $x_{k+1}^\\delta = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$，步长 $\\omega \\in (0, 2/\\lVert A \\rVert_2^2)$，初始值 $x_0^\\delta = 0$。\n- 带参数 $\\tau  1$ 的偏差原理在第一个满足 $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$ 的索引 $k_\\ast$ 处停止。\n- 此处的无约束梯度下降与 Landweber 更新相同，但不使用 $\\delta$，而是持续进行预设的大量迭代次数 $K$。\n\n对抗性噪声的构造：\n- 您将通过 SVD 模型构造一个具有预设奇异值衰减的矩阵 $A$：$A = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是随机正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_i)$ 且 $\\sigma_i = i^{-p}$，指数 $p  0$。\n- 您将通过在右奇异向量基中指定系数 $(\\alpha_i)_{i=1}^n$ (其中 $\\alpha_i = i^{-q}$，指数 $q  0$) 来构造 $x^\\dagger$，然后进行归一化使得 $\\lVert x^\\dagger \\rVert_2 = 1$，即 $x^\\dagger = V \\alpha / \\lVert \\alpha \\rVert_2$。\n- 您将选择与最小奇异值对应的左奇异向量对齐的对抗性噪声：$e^\\delta = \\delta \\, u_n$，其中 $u_n$ 是 $U$ 的第 $n$ 列。这使得 $\\lVert e^\\delta \\rVert_2 = \\delta$ 并将噪声集中在被放大得最多的解分量上。\n\n每个测试用例需实现的数值任务：\n1. 按照描述构建 $A = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是通过对使用固定种子的随机高斯矩阵进行正交化得到的。\n2. 按照描述构造 $x^\\dagger$ 和 $y = A x^\\dagger$。\n3. 构造对抗性噪声 $e^\\delta = \\delta u_n$ 和带噪数据 $y^\\delta = y + e^\\delta$。\n4. 选择 Landweber 步长 $\\omega = \\frac{1.9}{\\sigma_1^2}$，其中 $\\sigma_1$ 是 $\\Sigma$ 中的最大奇异值。\n5. 运行带偏差原理停止准则的 Landweber 迭代：在第一个满足 $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$ 的索引 $k_\\ast$ 处停止，并设置一个足够大的迭代次数安全上限以确保在可能的情况下终止。\n6. 运行相同的迭代固定的 $K$ 次（无约束梯度下降）。\n7. 计算相对重构误差 $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{k_\\ast}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ 和 $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{K}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$，并报告其比率 $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$。\n\n测试套件：\n对于下面的每个元组 $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$，执行上述步骤并返回比率 $r$。\n\n- 情况 A（理想路径，中等病态性）：$(n, p, q, \\delta, \\tau, \\mathrm{seed}, K) = (50, 1.5, 1.0, 10^{-3}, 1.1, 0, 20000)$。\n- 情况 B（严重病态性）：$(50, 2.5, 1.0, 10^{-3}, 1.1, 1, 40000)$。\n- 情况 C（更高噪声水平和更温和的衰减）：$(80, 1.2, 0.5, 10^{-2}, 1.05, 2, 15000)$。\n- 情况 D（非常严重的病态性，小噪声）：$(50, 3.0, 1.5, 10^{-4}, 1.2, 3, 60000)$。\n\n答案规格和输出格式：\n- 对于每种情况，结果是一个浮点数 $r$。\n- 您的程序必须生成单行输出，其中包含四个比率的列表 $[r_A, r_B, r_C, r_D]$，以逗号分隔并用方括号括起。每个比率必须四舍五入到六位小数。\n- 不涉及物理单位。此处所有角度（如有）均不相关。\n\n您的程序必须是自包含的，并生成确切指定的最终输出格式，不得包含任何额外文本。",
            "solution": "用户提供的问题是有效的。这是一个逆问题领域中适定的数值实验，旨在说明迭代正则化的概念和过拟合现象。\n\n该问题在科学上基于线性反问题和数值优化的理论，所有参数和过程都得到了清晰、客观的定义。\n\n该问题研究一个形式为 $y^\\delta = A x^\\dagger + e^\\delta$ 的线性反问题，其目标是从带噪数据 $y^\\delta \\in \\mathbb{R}^n$ 中恢复真实解 $x^\\dagger \\in \\mathbb{R}^n$。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是病态的，意味着其奇异值迅速衰减。这种病态性是反问题的标志，其中数据的微小扰动会导致朴素解的巨大误差。噪声项 $e^\\delta$ 的范数已知，为 $\\lVert e^\\delta \\rVert_2 = \\delta$。\n\n我们使用 $A$ 的奇异值分解 (SVD)，$A = U \\Sigma V^\\top$，作为分析的基本工具。这里，$U = [u_1, \\dots, u_n]$ 和 $V = [v_1, \\dots, v_n]$ 是正交矩阵，其列分别是左、右奇异向量。矩阵 $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$ 包含按非递增顺序排列的奇异值：$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n  0$。$A$ 的病态性意味着比值 $\\sigma_1/\\sigma_n$ 很大。一个朴素解，$x_{\\mathrm{naive}} = A^{-1} y^\\delta$，会放大噪声。使用 SVD，这表示为 $x_{\\mathrm{naive}} = V \\Sigma^{-1} U^\\top y^\\delta$。在奇异值 $\\sigma_n$ 最小的 $u_n$ 方向上，噪声分量被放大了 $1/\\sigma_n$ 倍。该问题构造的对抗性噪声 $e^\\delta = \\delta u_n$ 正是为了最大化这种效应。\n\n该问题比较了两种求解 $x$ 的迭代方法。两者都基于 Landweber 迭代，这是应用于最小化数据失配泛函 $J(x) = \\frac{1}{2} \\lVert Ax - y^\\delta \\rVert_2^2$ 的一种梯度下降形式。迭代公式如下：\n$$x_{k+1}^\\delta = x_k^\\delta - \\omega \\nabla J(x_k^\\delta) = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$$\n从 $x_0^\\delta = 0$ 开始。为保证收敛，步长 $\\omega$ 必须选择在 $(0, 2/\\lVert A \\rVert_2^2) = (0, 2/\\sigma_1^2)$ 区间内。问题指定了 $\\omega = 1.9/\\sigma_1^2$。\n\n在 SVD 基下，Landweber 迭代解可以表示为 $x_k^\\delta = \\sum_{i=1}^n c_{k,i} v_i$。其系数演化规律如下：\n$$c_{k,i} = \\frac{\\langle y^\\delta, u_i \\rangle}{\\sigma_i} \\left(1 - (1 - \\omega \\sigma_i^2)^k\\right)$$\n当迭代次数 $k \\to \\infty$ 时，滤波因子 $(1 - (1 - \\omega \\sigma_i^2)^k)$ 趋近于 $1$。解的系数 $c_{k,i}$ 收敛于 $\\langle y^\\delta, u_i \\rangle / \\sigma_i$，这正是朴素解 $A^{-1} y^\\delta$ 的系数。对于分量 $i=n$，噪声 $e^\\delta = \\delta u_n$ 导致 $\\langle y^\\delta, u_n \\rangle = \\langle y, u_n \\rangle + \\delta$。系数 $c_{k,n}$ 收敛于 $\\frac{\\langle y, u_n \\rangle}{\\sigma_n} + \\frac{\\delta}{\\sigma_n}$。由于 $\\sigma_n$ 很小，$\\delta/\\sigma_n$ 这一项会变得巨大，导致灾难性的噪声放大。这就是在“无约束梯度下降”情况下发生的事情，即迭代运行了大量固定的步数 $K$，导致其对噪声“过拟合”。由此产生的误差 $\\varepsilon_{\\mathrm{unc}}$ 预计会很大。\n\n第二种方法使用 Landweber 迭代，但结合了一种称为偏差原理的停止准则。该原理是一种正则化形式，它规定一旦迭代解的数据失配程度与噪声水平相当，就应停止迭代。具体来说，我们在第一个满足以下条件的迭代次数 $k_\\ast$ 处停止：\n$$\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$$\n其中 $\\tau  1$ 是一个安全因子。这种停止准则可以防止迭代进行得太久，从而避免显著放大高频噪声分量。对于小的 $\\sigma_i$（即高频分量），由于 $k_\\ast$ 相对较小，滤波因子 $(1 - (1 - \\omega \\sigma_i^2)^{k_\\ast})$ 会接近于 $0$。这会抑制噪声分量，特别是与 $u_n$ 对应的分量，从而得到一个稳定、正则化的解 $x_{k_\\ast}^\\delta$，其误差 $\\varepsilon_{\\mathrm{reg}}$ 要小得多。\n\n数值任务是针对四组不同的参数集实现这一场景。对于每种情况，我们根据基于 SVD 的模型构造矩阵 $A$、真实解 $x^\\dagger$ 和带噪数据 $y^\\delta$。然后我们运行两种迭代方案：一种由偏差原理停止，另一种运行固定的 K 次迭代。最后，我们计算相对重构误差 $\\varepsilon_{\\mathrm{reg}}$ 和 $\\varepsilon_{\\mathrm{unc}}$，以及它们的比率 $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$，该比率量化了使用适当正则化策略的好处。这个比率预计会显著大于 1。\n\n实现将按以下步骤进行：\n1.  定义一个函数来处理单个测试用例，该函数接收参数 $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$作为输入。\n2.  在该函数内部，设置随机数生成器种子以确保可复现性。\n3.  通过对随机高斯矩阵应用 QR 分解，生成大小为 $n \\times n$ 的随机正交矩阵 $U$ 和 $V$。\n4.  构造奇异值对角矩阵 $\\Sigma$，其元素为 $\\sigma_i = i^{-p}$，其中 $i \\in \\{1, \\dots, n\\}$。\n5.  将矩阵 $A$ 组装为 $A = U \\Sigma V^\\top$。\n6.  从系数 $\\alpha_i = i^{-q}$ 构造真实解 $x^\\dagger$，并将其归一化以使 $\\lVert x^\\dagger \\rVert_2 = 1$，然后变换到标准基：$x^\\dagger = V (\\alpha / \\lVert \\alpha \\rVert_2)$。无噪声数据为 $y = A x^\\dagger$。\n7.  创建对抗性噪声 $e^\\delta = \\delta u_n$，其中 $u_n$ 是 $U$ 的最后一列。带噪数据变为 $y^\\delta = y + e^\\delta$。\n8.  将 Landweber 步长设置为 $\\omega = 1.9 / \\sigma_1^2$。\n9.  运行带偏差原理的 Landweber 迭代。从 $x_0^\\delta = 0$ 开始，迭代进行直到 $\\lVert A x_k^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$。得到的解为 $x_{\\mathrm{reg}} = x_{k_\\ast}^\\delta$。\n10. 无约束的 Landweber 迭代运行固定的 $K$ 步，得到解 $x_{\\mathrm{unc}} = x_K^\\delta$。\n11. 计算相对误差 $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{\\mathrm{reg}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ 和 $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{\\mathrm{unc}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$。\n12. 计算并返回比率 $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$。\n\n该过程将对所有四个测试用例重复执行，并按要求格式化最终的比率列表。",
            "answer": "```python\nimport numpy as np\n\ndef run_case(n, p, q, delta, tau, seed, K):\n    \"\"\"\n    Solves a single test case for the iterative regularization problem.\n    \"\"\"\n    # 1. Build A = U Sigma V^T\n    np.random.seed(seed)\n    \n    # Generate random orthogonal matrices U and V\n    H1 = np.random.randn(n, n)\n    U, _ = np.linalg.qr(H1)\n    \n    H2 = np.random.randn(n, n)\n    V, _ = np.linalg.qr(H2)\n    \n    # Generate singular values and Sigma matrix\n    i_vals = np.arange(1, n + 1)\n    sigma_vals = i_vals**(-p)\n    Sigma = np.diag(sigma_vals)\n    \n    A = U @ Sigma @ V.T\n\n    # 2. Construct x_dagger and y\n    alpha_coeffs = i_vals**(-q)\n    alpha_norm_factor = np.linalg.norm(alpha_coeffs)\n    alpha_normalized = alpha_coeffs / alpha_norm_factor\n    \n    x_dagger = V @ alpha_normalized\n    y = A @ x_dagger\n\n    # 3. Construct adversarial noise and noisy data\n    u_n = U[:, -1]\n    e_delta = delta * u_n\n    y_delta = y + e_delta\n\n    # 4. Choose Landweber step size\n    sigma_1 = sigma_vals[0]\n    omega = 1.9 / (sigma_1**2)\n\n    # 5. Run Landweber with discrepancy principle\n    x_k_reg = np.zeros(n)\n    # Use a large enough safety cap for iterations\n    max_iter_reg = max(100000, 2 * K)\n    \n    for _ in range(max_iter_reg):\n        residual = y_delta - A @ x_k_reg\n        residual_norm = np.linalg.norm(residual)\n        \n        if residual_norm = tau * delta:\n            break\n        \n        x_k_reg = x_k_reg + omega * (A.T @ residual)\n    else:\n        # This part should not be reached if the problem is well-posed.\n        # It indicates failure of the discrepancy principle to stop.\n        # Assign NaN or raise an error to signal failure.\n        x_k_reg = np.full(n, np.nan)\n    \n    x_reg = x_k_reg\n\n    # 6. Run unconstrained Landweber for K iterations\n    x_k_unc = np.zeros(n)\n    for _ in range(K):\n        residual = y_delta - A @ x_k_unc\n        x_k_unc = x_k_unc + omega * (A.T @ residual)\n        \n    x_unc = x_k_unc\n\n    # 7. Compute errors and ratio\n    norm_x_dagger = np.linalg.norm(x_dagger) # Should be 1.0 by construction\n    \n    err_reg = np.linalg.norm(x_reg - x_dagger)\n    err_unc = np.linalg.norm(x_unc - x_dagger)\n    \n    eps_reg = err_reg / norm_x_dagger\n    eps_unc = err_unc / norm_x_dagger\n    \n    ratio = eps_unc / eps_reg\n    return ratio\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, p, q, delta, tau, seed, K)\n        (50, 1.5, 1.0, 10**-3, 1.1, 0, 20000),  # Case A\n        (50, 2.5, 1.0, 10**-3, 1.1, 1, 40000),  # Case B\n        (80, 1.2, 0.5, 10**-2, 1.05, 2, 15000), # Case C\n        (50, 3.0, 1.5, 10**-4, 1.2, 3, 60000),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, q, delta, tau, seed, K = case\n        result = run_case(n, p, q, delta, tau, seed, K)\n        results.append(result)\n\n    # Format output as required\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}