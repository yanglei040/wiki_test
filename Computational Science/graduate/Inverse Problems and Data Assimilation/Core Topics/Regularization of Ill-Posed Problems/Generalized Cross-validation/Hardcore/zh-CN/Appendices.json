{
    "hands_on_practices": [
        {
            "introduction": "广义交叉验证（Generalized Cross-Validation, GCV）是一种用于选择正则化参数 $\\lambda$ 的数据驱动方法。其公式依赖于两个核心量：残差的范数和影响矩阵的迹（也称为模型的自由度）。这个练习  要求您使用奇异值分解（Singular Value Decomposition, SVD）来推导这两个关键组成部分的闭式表达式。掌握这一推导对于理解 GCV 如何被高效计算，以及它如何将正则化与前向算子的谱特性联系起来至关重要。",
            "id": "3385818",
            "problem": "考虑一个线性反问题，其正算子为 $A \\in \\mathbb{R}^{m \\times n}$（秩为 $r \\leq \\min\\{m,n\\}$），带噪声的观测值为 $y \\in \\mathbb{R}^{m}$。设 $A$ 的奇异值分解 (SVD) 为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 的对角线元素为 $\\sigma_{1},\\dots,\\sigma_{r} > 0$，其余元素为零。考虑零阶吉洪诺夫正则化，其目标是寻找 $x_{\\lambda} \\in \\mathbb{R}^{n}$ 以最小化 $\\| A x - y \\|_{2}^{2} + \\lambda \\| x \\|_{2}^{2}$，其中正则化参数 $\\lambda > 0$。记相关的帽子矩阵为 $H_{\\lambda} = A (A^{\\top} A + \\lambda I_{n})^{-1} A^{\\top}$，在广义交叉验证 (GCV) 背景下的自由度为 $\\mathrm{df}(\\lambda) = \\mathrm{trace}(H_{\\lambda})$，残差为 $r_{\\lambda} = y - A x_{\\lambda} = (I_{m} - H_{\\lambda}) y$。仅使用奇异值分解和矩阵迹的基本定义与性质，推導自由度和残差范数平方的闭式表达式，该表达式应使用奇异值和 $y$ 在左奇异向量基下的分量来表示。特别地，请使用左奇异向量 $u_{1},\\dots,u_{m}$（即 $U$ 的列向量）和由正则化导出的滤波因子 $f_{i}(\\lambda)$ 来表示这两个量。\n\n你的最终答案必须是一个单一的闭式解析表达式，以一个包含两个元素的行矩阵形式并排呈现推导出的两个量。最终答案中不应包含任何中间步骤，也不需要进行数值近似。",
            "solution": "问题陈述经评估有效。这是一个适定、有科学依据且自洽的请求，要求在反问题理论和吉洪诺夫正则化中进行一次标准推导。所有给出的定义和变量都是标准的且一致的。\n\n目标是推导自由度 $\\mathrm{df}(\\lambda) = \\mathrm{trace}(H_{\\lambda})$ 和残差范数平方 $\\|r_{\\lambda}\\|_{2}^{2} = \\|y - Ax_{\\lambda}\\|_{2}^{2}$ 的闭式表达式。给定正算子 $A \\in \\mathbb{R}^{m \\times n}$ 的奇异值分解 (SVD) 为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个矩形对角矩阵，其前 $r = \\mathrm{rank}(A)$ 个对角元素为正奇异值 $\\sigma_{1} > \\sigma_{2} > \\dots > \\sigma_{r} > 0$。$U$ 的列向量（记为 $u_i$）和 $V$ 的列向量（记为 $v_i$）分别是左奇异向量和右奇异向量。\n\n吉洪诺夫正则化解 $x_{\\lambda}$ 最小化泛函 $\\|Ax - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$，并由正规方程 $(A^{\\top}A + \\lambda I_{n})x_{\\lambda} = A^{\\top}y$ 给出。解为 $x_{\\lambda} = (A^{\\top}A + \\lambda I_{n})^{-1}A^{\\top}y$。帽子矩阵为 $H_{\\lambda} = A(A^{\\top}A + \\lambda I_{n})^{-1}A^{\\top}$。\n\n首先，我们推导自由度 $\\mathrm{df}(\\lambda)$ 的表达式。\n$\\mathrm{df}(\\lambda) = \\mathrm{trace}(H_{\\lambda}) = \\mathrm{trace}(A(A^{\\top}A + \\lambda I_{n})^{-1}A^{\\top})$。\n利用迹的循环性质 $\\mathrm{trace}(ABC) = \\mathrm{trace}(BCA)$，我们有：\n$\\mathrm{df}(\\lambda) = \\mathrm{trace}(A^{\\top}A(A^{\\top}A + \\lambda I_{n})^{-1})$。\n\n接下来，我们将 $A$ 的 SVD 代入 $A^{\\top}A$ 和 $(A^{\\top}A + \\lambda I_{n})^{-1}$ 中。\n$A^{\\top}A = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top}$。\n由于 $U$ 是正交的，所以 $U^{\\top}U = I_{m}$。\n$A^{\\top}A = V(\\Sigma^{\\top}\\Sigma)V^{\\top}$。\n矩阵 $\\Sigma^{\\top}\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是一个对角矩阵，其对角元素为 $\\sigma_{1}^{2}, \\dots, \\sigma_{r}^{2}, 0, \\dots, 0$。\n现在考虑项 $(A^{\\top}A + \\lambda I_{n})$：\n$A^{\\top}A + \\lambda I_{n} = V(\\Sigma^{\\top}\\Sigma)V^{\\top} + \\lambda I_{n} = V(\\Sigma^{\\top}\\Sigma)V^{\\top} + \\lambda VV^{\\top} = V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})V^{\\top}$。\n那么其逆矩阵为：\n$(A^{\\top}A + \\lambda I_n)^{-1} = (V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})V^{\\top})^{-1} = V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}V^{\\top}$。\n\n将这些表达式代回到迹的公式中：\n$\\mathrm{df}(\\lambda) = \\mathrm{trace}\\left( (V(\\Sigma^{\\top}\\Sigma)V^{\\top}) (V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}V^{\\top}) \\right)$。\n由于 $V^{\\top}V = I_{n}$：\n$\\mathrm{df}(\\lambda) = \\mathrm{trace}\\left( V(\\Sigma^{\\top}\\Sigma)(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}V^{\\top} \\right)$。\n再次利用迹的循环性质：\n$\\mathrm{df}(\\lambda) = \\mathrm{trace}\\left( (\\Sigma^{\\top}\\Sigma)(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}V^{\\top}V \\right) = \\mathrm{trace}\\left( (\\Sigma^{\\top}\\Sigma)(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1} \\right)$。\n矩阵 $\\Sigma^{\\top}\\Sigma$ 和 $(\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1}$ 均为对角矩阵。$(\\Sigma^{\\top}\\Sigma + \\lambda I_n)^{-1}$ 的第 $i$ 个对角元素在 $i=1, \\dots, r$ 时为 $1/(\\sigma_{i}^{2} + \\lambda)$，在 $i=r+1, \\dots, n$ 时为 $1/\\lambda$。$\\Sigma^{\\top}\\Sigma$ 的第 $i$ 个对角元素在 $i=1, \\dots, r$ 时为 $\\sigma_{i}^{2}$，在 $i=r+1, \\dots, n$ 时为 $0$。\n其乘积是一个对角矩阵，元素在 $i=1, \\dots, r$ 时为 $\\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$，在 $i=r+1, \\dots, n$ 时为 $0$。\n迹是这些对角元素的和：\n$\\mathrm{df}(\\lambda) = \\sum_{i=1}^{r} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$。\n吉洪诺夫正则化的滤波因子定义为 $f_{i}(\\lambda) = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$。因此，我们可以写成：\n$\\mathrm{df}(\\lambda) = \\sum_{i=1}^{r} f_{i}(\\lambda)$。\n\n其次，我们推导残差范数平方 $\\|r_{\\lambda}\\|_{2}^{2}$ 的表达式。\n残差为 $r_{\\lambda} = y - Ax_{\\lambda}$。\n我们首先使用 SVD 表示解 $x_{\\lambda}$。\n$x_{\\lambda} = V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}V^{\\top} (V\\Sigma^{\\top}U^{\\top})y = V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}\\Sigma^{\\top}U^{\\top}y$。\n那么，$Ax_{\\lambda}$ 为：\n$Ax_{\\lambda} = (U\\Sigma V^{\\top}) \\left( V(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}\\Sigma^{\\top}U^{\\top}y \\right) = U\\Sigma(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}\\Sigma^{\\top}U^{\\top}y$。\n这恰好是 $H_{\\lambda}y$。我们来分析矩阵乘积 $D_H = \\Sigma(\\Sigma^{\\top}\\Sigma + \\lambda I_{n})^{-1}\\Sigma^{\\top}$。这是一个 $m \\times m$ 的对角矩阵。对于 $i=1, \\dots, r$，其第 $i$ 个对角元素是 $\\sigma_{i} \\cdot \\frac{1}{\\sigma_{i}^{2} + \\lambda} \\cdot \\sigma_{i} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda} = f_{i}(\\lambda)$。对于 $i=r+1, \\dots, m$，$\\Sigma$ 的相应行为零，因此对角元素为 $0$。\n所以，$Ax_{\\lambda} = U D_H U^{\\top} y$。$U$ 的列向量构成了 $\\mathbb{R}^{m}$ 的一组标准正交基。我们可以将 $y$ 在此基下表示为 $y = \\sum_{i=1}^{m} (u_{i}^{\\top}y)u_{i}$。\n$Ax_{\\lambda} = U D_H U^{\\top} \\sum_{j=1}^{m} (u_{j}^{\\top}y)u_{j} = U D_H \\sum_{j=1}^{m} (u_{j}^{\\top}y)(U^{\\top}u_{j})$。\n由于 $U^{\\top}u_j$ 是第 $j$ 个标准基向量 $e_j$，\n$Ax_{\\lambda} = U \\sum_{j=1}^{m} (D_H)_{jj} (u_{j}^{\\top}y) e_{j} = \\sum_{j=1}^{m} (D_H)_{jj} (u_{j}^{\\top}y) u_{j}$。\n代入 $D_H$ 的对角元素：\n$Ax_{\\lambda} = \\sum_{i=1}^{r} f_{i}(\\lambda)(u_{i}^{\\top}y)u_{i}$。\n\n残差 $r_{\\lambda}$ 为 $y - Ax_{\\lambda}$：\n$r_{\\lambda} = \\left(\\sum_{i=1}^{m} (u_{i}^{\\top}y)u_{i}\\right) - \\left(\\sum_{i=1}^{r} f_{i}(\\lambda)(u_{i}^{\\top}y)u_{i}\\right)$。\n我们可以合并这些求和：\n$r_{\\lambda} = \\sum_{i=1}^{r} (1 - f_{i}(\\lambda))(u_{i}^{\\top}y)u_{i} + \\sum_{i=r+1}^{m} (u_{i}^{\\top}y)u_{i}$。\n由于向量 $u_{i}$ 是标准正交的， $r_{\\lambda}$ 的 $L_2$ 范数平方是在此基下系数的平方和：\n$\\|r_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{r} \\left((1 - f_{i}(\\lambda))(u_{i}^{\\top}y)\\right)^{2} + \\sum_{i=r+1}^{m} (u_{i}^{\\top}y)^{2}$。\n$= \\sum_{i=1}^{r} (1 - f_{i}(\\lambda))^{2}(u_{i}^{\\top}y)^{2} + \\sum_{i=r+1}^{m} (u_{i}^{\\top}y)^{2}$。\n此处，$1-f_i(\\lambda) = 1 - \\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda} = \\frac{\\lambda}{\\sigma_i^2+\\lambda}$。\n\n推导出的两个表达式是：\n1. 自由度：$\\mathrm{df}(\\lambda) = \\sum_{i=1}^{r} f_{i}(\\lambda)$。\n2. 残差范数平方：$\\|r_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{r} (1 - f_{i}(\\lambda))^{2}(u_{i}^{\\top}y)^{2} + \\sum_{i=r+1}^{m} (u_{i}^{\\top}y)^{2}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{i=1}^{r} f_{i}(\\lambda) & \\sum_{i=1}^{r} (1-f_{i}(\\lambda))^{2} (u_{i}^{\\top}y)^{2} + \\sum_{i=r+1}^{m} (u_{i}^{\\top}y)^{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在实践中，我们无法获知真实的解，因此 GCV 提供了一种在不需要真实解的情况下估计最优正则化参数的方法。一个自然的问题是：这个估计值与我们假设拥有“上帝视角”（即知道真实解）时所能选择的“最佳”参数相比表现如何？这个练习  设计了一个数值实验来精确地进行这种比较。通过计算真正最小化解误差的参数 $\\alpha_{opt}$，并将其与 GCV 选择的参数 $\\alpha_{gcv}$ 进行对比，您将对 GCV 的有效性及其作为未知最优选择的实用代理的角色，获得一个具体而量化的认识。",
            "id": "3283866",
            "problem": "考虑一个线性逆问题，其中未知向量 $x \\in \\mathbb{R}^{n}$ 是从数据 $b \\in \\mathbb{R}^{n}$ 中估计得出的，它们通过模型 $b = A x + \\varepsilon$ 由已知矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 相关联，其中 $\\varepsilon$ 表示加性噪声。假设未知的真实解 $x_{true}$ 是已知的，以用于评估。估计值 $x_{\\alpha}$ 是通过最小化关于 $x$ 的二次泛函 $J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha^{2} \\lVert x \\rVert_{2}^{2}$ 得到的，其中 $\\alpha > 0$ 是一个正则化参数。您的任务是选择 $\\alpha$ 以最小化差异 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$，并将此选择与通过广义交叉验证（GCV）选择的 $\\alpha$ 进行比较。广义交叉验证（GCV）是一种通过最小化函数 $G(\\alpha) = \\lVert (I - S(\\alpha)) b \\rVert_{2}^{2} / \\left(\\operatorname{trace}(I - S(\\alpha))\\right)^{2}$ 来选择 $\\alpha$ 的方法，其中 $S(\\alpha)$ 是将 $b$ 映射到拟合数据 $A x_{\\alpha}$ 的线性平滑算子。\n\n使用的基本原理：\n- 严格凸二次泛函的最小化子通过将其梯度置零来表征。\n- 线性估计器的平滑矩阵是线性算子 $S(\\alpha)$，它将数据 $b$ 映射到拟合值 $A x_{\\alpha}$。\n\n实现细节：\n- 对于所有测试用例，使用由对角线上的对角奇异值定义的方阵 $A$。具体来说，对于索引 $i$ ($1 \\leq i \\leq n$)，设置 $A_{ii} = s_{i}$ 且当 $i \\neq j$ 时 $A_{ij} = 0$，其中序列 $\\{s_{i}\\}$ 在下面的每个测试用例中描述。\n- 将真实解定义为 $x_{true} \\in \\mathbb{R}^{n}$，其分量为 $x_{true,i} = \\sin\\left( \\frac{2\\pi i}{n} \\right) + \\frac{1}{2} \\cos\\left( \\frac{\\pi i}{n} \\right)$，其中 $i = 1, 2, \\dots, n$。所有角度都必须是弧度。\n- 将噪声向量定义为 $\\varepsilon \\in \\mathbb{R}^{n}$，其分量为 $\\varepsilon_{j} = \\sigma \\sin(j)$，其中 $j = 1, 2, \\dots, n$，$\\sigma$ 是每个测试用例指定的噪声水平。所有角度都必须是弧度。\n- 为了评估 $\\alpha$，使用一个在区间 $[10^{-8}, 10^{0}]$ 内对数间隔的 $N_{\\alpha}$ 个值的网格，不包括 $\\alpha = 0$。具体来说，设置 $N_{\\alpha} = 121$，并从 $10^{-8}$ 到 $10^{0}$ 在对数尺度上均匀采样 $\\alpha$ 值。\n\n对于网格中的每个 $\\alpha$：\n- 通过最小化 $J_{\\alpha}(x)$ 来计算 $x_{\\alpha}$。\n- 计算误差范数 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$。\n- 计算与估计器 $A x_{\\alpha}$ 相关联的平滑矩阵 $S(\\alpha)$，然后计算 GCV 分数 $G(\\alpha)$。\n\n对于每个测试用例：\n- 选择最优的 $\\alpha$（记为 $\\alpha_{opt}$），它在网格上最小化 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$。\n- 选择由广义交叉验证选择的 $\\alpha$（记为 $\\alpha_{gcv}$），它在网格上最小化 $G(\\alpha)$。\n- 计算比率 $R = \\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2} / \\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}$。\n\n测试套件：\n- 测试用例 1：$n = 20$, $s_{i} = i^{-2}$, $\\sigma = 10^{-2}$。\n- 测试用例 2：$n = 20$, $s_{i} = 10^{-i/4}$, $\\sigma = 0$。\n- 测试用例 3：$n = 30$, $s_{i} = i^{-3}$, $\\sigma = 5 \\cdot 10^{-2}$。\n\n您的程序必须生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个包含三个浮点数 $[\\alpha_{opt}, \\alpha_{gcv}, R]$ 的列表，每个浮点数四舍五入到 $6$ 位小数。因此，最终输出必须具有以下形式：$[[\\alpha_{opt}^{(1)}, \\alpha_{gcv}^{(1)}, R^{(1)}],[\\alpha_{opt}^{(2)}, \\alpha_{gcv}^{(2)}, R^{(2)}],[\\alpha_{opt}^{(3)}, \\alpha_{gcv}^{(3)}, R^{(3)}]]$。",
            "solution": "该问题要求确定线性逆问题的 Tikhonov 正则化参数 $\\alpha$。我们必须找到最小化真实误差范数 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$ 的参数 $\\alpha_{opt}$，并将其与广义交叉验证（GCV）方法选择的参数 $\\alpha_{gcv}$ 进行比较。\n\n首先，我们推导正则化解 $x_{\\alpha}$ 的表达式。该解被定义为 Tikhonov 泛函的最小化子：\n$$J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha^{2} \\lVert x \\rVert_{2}^{2}$$\n其中 $\\alpha > 0$ 是正则化参数。这是一个关于 $x$ 的二次泛函。我们可以将其展开为：\n$$J_{\\alpha}(x) = (A x - b)^T (A x - b) + \\alpha^2 x^T x = x^T A^T A x - 2 b^T A x + b^T b + \\alpha^2 x^T x$$\n对于 $\\alpha > 0$，泛函 $J_{\\alpha}(x)$ 是严格凸的，因此其唯一的最小值可以通过将其关于 $x$ 的梯度设为零来找到。\n$$\\nabla_x J_{\\alpha}(x) = 2 A^T A x - 2 A^T b + 2 \\alpha^2 I x = 0$$\n其中 $I$ 是单位矩阵。重新整理各项，我们得到：\n$$(A^T A + \\alpha^2 I) x = A^T b$$\n因此，正则化解 $x_{\\alpha}$ 由下式给出：\n$$x_{\\alpha} = (A^T A + \\alpha^2 I)^{-1} A^T b$$\n这是 Tikhonov 正则化解的标准形式。问题指定矩阵 $A$ 是一个方阵对角矩阵，其元素为 $A_{ii} = s_i$ 且当 $i \\neq j$ 时 $A_{ij} = 0$。因此，$A = \\mathrm{diag}(s_1, s_2, \\dots, s_n)$。由于 $A$ 是实对角矩阵，它是对称的，所以 $A^T = A$。$x_{\\alpha}$ 的表达式得以简化。\n所涉及的矩阵都是对角矩阵：\n- $A^T A = A^2 = \\mathrm{diag}(s_1^2, s_2^2, \\dots, s_n^2)$\n- $A^T A + \\alpha^2 I = \\mathrm{diag}(s_1^2 + \\alpha^2, s_2^2 + \\alpha^2, \\dots, s_n^2 + \\alpha^2)$\n- $(A^T A + \\alpha^2 I)^{-1} = \\mathrm{diag}\\left(\\frac{1}{s_1^2 + \\alpha^2}, \\dots, \\frac{1}{s_n^2 + \\alpha^2}\\right)$\n解向量 $x_{\\alpha}$ 可以按分量计算。$x_{\\alpha}$ 的第 $i$ 个分量是：\n$$(x_{\\alpha})_i = \\left(\\frac{1}{s_i^2 + \\alpha^2}\\right) (s_i) (b_i) = \\frac{s_i}{s_i^2 + \\alpha^2} b_i$$\n这些项 $\\frac{s_i}{s_i^2 + \\alpha^2}$ 被称为滤波因子。\n\n接下来，我们讨论广义交叉验证（GCV）方法。GCV 函数定义为：\n$$G(\\alpha) = \\frac{\\lVert (I - S(\\alpha)) b \\rVert_{2}^{2}}{\\left(\\operatorname{trace}(I - S(\\alpha))\\right)^{2}}$$\n其中 $S(\\alpha)$ 是将数据向量 $b$ 映射到拟合数据 $A x_{\\alpha}$ 的平滑矩阵，即 $A x_{\\alpha} = S(\\alpha) b$。从 $x_{\\alpha}$ 的表达式，我们可以推导出 $S(\\alpha)$：\n$$S(\\alpha) = A (A^T A + \\alpha^2 I)^{-1} A^T$$\n鉴于 $A$ 是对角矩阵，$S(\\alpha)$ 也是对角矩阵：\n$$S(\\alpha) = \\mathrm{diag}(s_i) \\cdot \\mathrm{diag}\\left(\\frac{1}{s_i^2 + \\alpha^2}\\right) \\cdot \\mathrm{diag}(s_i) = \\mathrm{diag}\\left(\\frac{s_i^2}{s_i^2 + \\alpha^2}\\right)$$\n现在我们可以计算 GCV 函数的两个部分。项 $I - S(\\alpha)$ 是一个对角矩阵：\n$$I - S(\\alpha) = \\mathrm{diag}\\left(1 - \\frac{s_i^2}{s_i^2 + \\alpha^2}\\right) = \\mathrm{diag}\\left(\\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right)$$\n该矩阵的迹是其对角元素之和：\n$$\\operatorname{trace}(I - S(\\alpha)) = \\sum_{i=1}^{n} \\frac{\\alpha^2}{s_i^2 + \\alpha^2}$$\n$G(\\alpha)$ 的分子是残差向量 $(I - S(\\alpha)) b$ 的平方范数：\n$$\\lVert (I - S(\\alpha)) b \\rVert_{2}^{2} = \\sum_{i=1}^{n} \\left( \\left(\\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right) b_i \\right)^2$$\n因此，需要最小化的 GCV 函数是：\n$$G(\\alpha) = \\frac{\\sum_{i=1}^{n} \\left(\\frac{\\alpha^2 b_i}{s_i^2 + \\alpha^2}\\right)^2}{\\left(\\sum_{i=1}^{n} \\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right)^2}$$\n\n对于每个测试用例，总体算法流程如下：\n1.  定义参数 $n$、$\\sigma$ 和奇异值 $s_i$ 的函数。\n2.  根据问题规范，为 $i=1, \\dots, n$ 构建向量 $s_i$、$x_{true,i}$ 和 $\\varepsilon_i$。\n3.  使用模型 $b_i = s_i x_{true,i} + \\varepsilon_i$ 计算数据向量 $b$。\n4.  为 $\\alpha$ 创建一个从 $10^{-8}$ 到 $10^{0}$ 的对数间隔的网格，包含 $N_{\\alpha}=121$ 个值。\n5.  对于网格中的每个 $\\alpha$：\n    a. 使用 $(x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha^2} b_i$ 计算正则化解向量 $x_{\\alpha}$。\n    b. 计算并存储误差范数 $\\lVert x_{\\alpha} - x_{true} \\rVert_2$。\n    c. 使用推导出的公式计算并存储 GCV 分数 $G(\\alpha)$。\n6.  在网格中找到最小化误差范数的 $\\alpha_{opt}$ 值。这对应于存储的误差列表中的最小值。\n7.  在网格中找到最小化 GCV 分数的 $\\alpha_{gcv}$ 值。这对应于存储的 GCV 分数列表中的最小值。\n8.  计算比率 $R = \\frac{\\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2}}{\\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}}$，其中 $\\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}$ 是步骤6中找到的最小误差，而 $\\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2}$ 是对应于 $\\alpha_{gcv}$ 的误差。\n9.  存储该测试用例的结果 $[\\alpha_{opt}, \\alpha_{gcv}, R]$。\n\n对所有三个测试用例重复此过程，并按规定汇总和格式化结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n\n    def process_case(n, s_func, sigma):\n        \"\"\"\n        Solves a single test case for Tikhonov regularization and GCV.\n\n        Args:\n            n (int): The dimension of the problem.\n            s_func (function): A function that takes an array of indices i and returns singular values s_i.\n            sigma (float): The noise level.\n\n        Returns:\n            list: A list containing [alpha_opt, alpha_gcv, R] rounded to 6 decimal places.\n        \"\"\"\n        # 1. Generate problem data: s, x_true, noise, b\n        i_vals = np.arange(1.0, n + 1.0)\n        s = s_func(i_vals)\n        \n        x_true = np.sin(2 * np.pi * i_vals / n) + 0.5 * np.cos(np.pi * i_vals / n)\n        \n        if sigma == 0.0:\n            noise = np.zeros(n)\n        else:\n            noise = sigma * np.sin(i_vals)\n            \n        b = s * x_true + noise\n\n        # 2. Setup alpha grid for regularization parameter search\n        alphas = np.logspace(-8, 0, 121)\n        \n        errors = []\n        gcv_scores = []\n        \n        s2 = s**2\n\n        # 3. Loop over all alpha values to compute errors and GCV scores\n        for alpha in alphas:\n            alpha2 = alpha**2\n            \n            # Compute the Tikhonov-regularized solution x_alpha\n            # (x_alpha)_i = (s_i / (s_i^2 + alpha^2)) * b_i\n            filter_factors = s / (s2 + alpha2)\n            x_alpha = filter_factors * b\n            \n            # Compute the error norm ||x_alpha - x_true||_2\n            error = np.linalg.norm(x_alpha - x_true)\n            errors.append(error)\n            \n            # Compute the GCV score G(alpha)\n            # Numerator: ||(I - S(alpha))b||^2 = sum_i ((alpha^2 * b_i) / (s_i^2 + alpha^2))^2\n            # Denominator: (tr(I - S(alpha)))^2 = (sum_i alpha^2 / (s_i^2 + alpha^2))^2\n            common_term_gcv = alpha2 / (s2 + alpha2)\n            \n            gcv_num = np.sum((common_term_gcv * b)**2)\n            gcv_den = np.sum(common_term_gcv)**2\n            \n            if gcv_den == 0.0:\n                gcv_score = np.inf\n            else:\n                gcv_score = gcv_num / gcv_den\n            gcv_scores.append(gcv_score)\n\n        # 4. Find optimal alpha and GCV-chosen alpha from the grid\n        errors = np.array(errors)\n        gcv_scores = np.array(gcv_scores)\n        \n        # Find alpha_opt, which minimizes the true error\n        idx_opt = np.argmin(errors)\n        alpha_opt = alphas[idx_opt]\n        min_error = errors[idx_opt]\n        \n        # Find alpha_gcv, which minimizes the GCV score\n        idx_gcv = np.argmin(gcv_scores)\n        alpha_gcv = alphas[idx_gcv]\n        \n        # 5. Compute the performance ratio R\n        error_at_gcv = errors[idx_gcv]\n        \n        if min_error == 0.0:\n            R = 1.0 if error_at_gcv == 0.0 else np.inf\n        else:\n            R = error_at_gcv / min_error\n            \n        return [alpha_opt, alpha_gcv, R]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, s_func, sigma)\n        {'n': 20, 's_func': lambda i: i**(-2), 'sigma': 1e-2},\n        {'n': 20, 's_func': lambda i: 10**(-i/4.0), 'sigma': 0.0},\n        {'n': 30, 's_func': lambda i: i**(-3), 'sigma': 5e-2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['n'], case['s_func'], case['sigma'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    inner_parts = []\n    for r in all_results:\n        inner_parts.append(f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\")\n    output_str = f\"[{','.join(inner_parts)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "对于线性问题和统计噪声，GCV 是一个强大的工具，但当其基本假设被违反时会发生什么？在许多现实世界的场景中，尤其是在非线性问题中，残差可能由确定性的模型误差（或线性化误差）主导，而非随机噪声。本练习  要求您构建一个反例，其中在一个非线性问题的 Gauss-Newton 迭代中幼稚地应用 GCV 会导致算法失败。这是科学计算中的一个关键教训：它强调了在应用任何方法之前，理解其理论基础和局限性的重要性，从而避免在不适当的场景中误用它。",
            "id": "3385851",
            "problem": "考虑一个非线性反问题，即从数据 $y \\in \\mathbb{R}^m$ 中估计未知参数向量 $x \\in \\mathbb{R}^n$，其正向映射为 $F: \\mathbb{R}^n \\to \\mathbb{R}^m$。假设观测模型是无噪声的，即对于一个固定的真实参数 $x^\\star$，有 $y = F(x^\\star)$。目标是分析一种高斯-牛顿 (Gauss–Newton) 格式的行为。在第 $k$ 次迭代中，该方法围绕当前迭代点 $x_k$ 对正向映射进行线性化，计算雅可比矩阵 $J_k = \\nabla F(x_k)$，并通过求解以下形式的吉洪诺夫 (Tikhonov) 正则化线性最小二乘子问题来确定步长 $d_k$：\n$$\n\\min_{d \\in \\mathbb{R}^n} \\ \\|J_k d - r_k\\|_2^2 + \\alpha_k^2 \\|d\\|_2^2,\n$$\n其中 $r_k = y - F(x_k)$ 是当前残差，$\\alpha_k \\ge 0$ 是一个正则化参数。在应用于该非线性问题的朴素广义交叉验证 (generalized cross-validation, GCV) 方法中，参数 $\\alpha_k$ 是在每次迭代中通过最小化与基于 $J_k$ 和 $r_k$ 的线性化子问题相关的广义交叉验证泛函来选择的。\n\n构造一个反例，在该反例中，非线性设置下对广义交叉验证的朴素使用会选择过大的 $\\alpha_k$，从而导致沿信息丰富方向的过度平滑，并引起高斯-牛顿迭代的停滞。你的构造必须是显式的、算法化的，并且必须能够通过计算得到可证实的验证。\n\n使用以下 $n = 2$ 和 $m = 3$ 的显式正向映射：\n$$\nF(x) = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\ns \\tanh\\!\\big(\\gamma (x_1 + x_2)\\big)\n\\end{bmatrix},\n$$\n其中 $x = (x_1,x_2)^\\top$，$s > 0$ 是一个尺度参数，$\\gamma > 0$ 控制非线性的强度。雅可比矩阵为\n$$\nJ(x) = \\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\ns \\gamma \\operatorname{sech}^2\\!\\big(\\gamma (x_1 + x_2)\\big)  s \\gamma \\operatorname{sech}^2\\!\\big(\\gamma (x_1 + x_2)\\big)\n\\end{bmatrix}.\n$$\n\n目标解固定为 $x^\\star = (1,1)^\\top$，观测值为 $y = F(x^\\star)$。在高斯-牛顿方法中，使用指定的 $x_0$ 进行初始化，并通过 $x_{k+1} = x_k + d_k$ 进行更新。在每次迭代中，通过应用于线性化子问题的朴素广义交叉验证来选择 $\\alpha_k$（即，将 $J_k$ 和 $r_k$ 视为来自一个带单位罚分的吉洪诺夫正则化的线性模型）。运行迭代，直到达到最大迭代次数或满足停滞准则。\n\n你的程序必须实现两种方案：\n- 一种朴素广义交叉验证方案，在每次迭代中仅根据 $x_k$ 处的线性化子问题选择 $\\alpha_k$。\n- 一种固定正则化的基准高斯-牛顿方案，在所有迭代中都使用一个小的常数 $\\alpha > 0$。\n\n如下定义朴素广义交叉验证方案的停滞：最终残差范数比 $\\|r_{\\text{final}}\\|_2 / \\|r_{\\text{initial}}\\|_2$ 大于或等于 $0.95$，而基准固定正则化方案实现的残差范数比小于或等于 $0.70$。此准则捕捉了广义交叉验证沿信息丰富方向（此处为前两个分量）过度平滑并阻碍进展的现象，而适度的固定正则化则允许取得进展。\n\n使用以下测试套件，其中非线性强度 $\\gamma$ 和初始点 $x_0$ 会变化，而 $s = 1$ 保持固定：\n- 测试用例 1（强非线性，远初始点）：$\\gamma = 10$, $x_0 = (-1,-1)^\\top$。\n- 测试用例 2（中等非线性，远初始点）：$\\gamma = 0.5$, $x_0 = (-1,-1)^\\top$。\n- 测试用例 3（强非线性，近初始点）：$\\gamma = 10$, $x_0 = (0.9,0.9)^\\top$。\n\n对于每个测试用例，使用相同的最大迭代次数和更新规则运行这两种方案。最终输出必须是一行，包含一个布尔值列表，按顺序指出每个测试用例中朴素广义交叉验证方案是否根据上述准则发生停滞。\n\n你的程序应生成一行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，`[true,false,true]`）。布尔值必须格式化为 Python 布尔字面量，即 `True` 或 `False`。",
            "solution": "通过非线性正向映射 $F(x)$ 从数据 $y \\in \\mathbb{R}^m$ 估计未知参数向量 $x \\in \\mathbb{R}^n$ 的问题是科学计算的基石之一。高斯-牛顿 (Gauss–Newton) 方法是解决此类非线性最小二乘问题的标准迭代技术。在每次迭代 $k$ 中，该方法围绕当前估计值 $x_k$ 对问题进行线性化，并求解一个线性最小二乘子问题以找到更新步长 $d_k$。为了在存在病态条件时稳定解，这个子问题通常会进行正则化，例如，使用吉洪诺夫 (Tikhonov) 方法：\n$$\n\\min_{d \\in \\mathbb{R}^n} \\ \\|J_k d - r_k\\|_2^2 + \\alpha_k^2 \\|d\\|_2^2,\n$$\n其中 $J_k = \\nabla F(x_k)$ 是雅可比矩阵，$r_k = y - F(x_k)$ 是残差。这种方法的一个关键方面是正则化参数 $\\alpha_k$ 的选择。\n\n广义交叉验证 (Generalized Cross-Validation, GCV) 是一种强大的方法，用于在线性反问题 $y = Kx + \\epsilon$（其中 $\\epsilon$ 是白噪声）中选择 $\\alpha_k$。GCV 泛函 $V(\\alpha)$ 旨在估计预测误差，通过最小化该泛函可以找到最优的 $\\alpha$。其推导假设线性系统的残差向量主要由统计噪声构成。\n\n在非线性高斯-牛顿背景下对 GCV 的“朴素”应用，涉及将每一步的线性化子问题视为一个独立的线性反问题。也就是说，对于系统 $J_k d_k \\approx r_k$，GCV 通过最小化以下泛函来找到 $\\alpha_k$：\n$$\nV(\\alpha_k) = \\frac{\\|J_k d_k(\\alpha_k) - r_k\\|_2^2}{\\left( \\operatorname{Tr}\\left(I - A_k(\\alpha_k)\\right) \\right)^2},\n$$\n其中 $A_k(\\alpha_k) = J_k(J_k^\\top J_k + \\alpha_k^2 I)^{-1}J_k^\\top$ 是影响矩阵。\n\n这种朴素方法存在一个根本缺陷，而本反例正是为了揭示这一缺陷：在非线性问题中，残差 $r_k = F(x^\\star) - F(x_k)$ 与统计噪声并不可类比。它主要由**线性化误差**主导，尤其是当迭代点 $x_k$ 远离真实解 $x^\\star$ 且函数 $F$ 具有强非线性时。即 $r_k = J_k(x^\\star - x_k) + \\mathcal{O}(\\|x^\\star - x_k\\|^2)$。高阶项是确定性的，而非随机性的。GCV 无法将这种确定性结构与噪声区分开，因此将大的线性化误差误解为大的噪声。因此，它会选择一个过大的正则化参数 $\\alpha_k$ 来“平滑”这种感知到的噪声，这会过度抑制更新步长 $d_k$，并导致迭代停滞。\n\n我们使用指定的正向映射 $F(x) = [x_1, x_2, s \\tanh(\\gamma(x_1+x_2))]^\\top$ 来构造这个反例。参数 $\\gamma$ 控制非线性程度。当 $\\gamma$ 很大时，$\\tanh$ 函数会迅速饱和，这意味着其导数 $\\operatorname{sech}^2$ 对于绝对值大的自变量会趋近于零。这一结构特性是关键。\n\n我们来分析测试用例 1：$\\gamma=10$，$x_0 = (-1,-1)^\\top$，$s=1$ 以及 $x^\\star = (1,1)^\\top$。\n初始迭代点 $x_0$ 离解 $x^\\star$ “很远”。在 $x_0$ 处，$\\tanh$ 函数的自变量为 $\\gamma(x_{0,1}+x_{0,2}) = 10(-2) = -20$。这已深入 $\\tanh$ 函数的饱和区。\n在 $x_0$ 处的雅可比矩阵是：\n$$\nJ(x_0) = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 10 \\operatorname{sech}^2(-20)  10 \\operatorname{sech}^2(-20) \\end{bmatrix}.\n$$\n由于 $\\operatorname{sech}^2(-20) \\approx 4.2 \\times 10^{-18}$，雅可比矩阵的第三行实际上为零。因此，$J_0 \\approx \\begin{bsmallmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{bsmallmatrix}$。$J_0$ 的值域基本上是 $\\mathbb{R}^3$ 中的 $xy$-平面。\n\n真实数据为 $y = F(x^\\star) = [1, 1, \\tanh(20)]^\\top \\approx [1, 1, 1]^\\top$。\n在 $x_0$ 处的模型输出为 $F(x_0) = [-1, -1, \\tanh(-20)]^\\top \\approx [-1, -1, -1]^\\top$。\n初始残差为 $r_0 = y - F(x_0) \\approx [2, 2, 2]^\\top$。\n\n关键的观察是，$r_0$ 有一个很大的分量 $[0,0,2]^\\top$，该分量与 $J_0$ 的值域正交。线性模型 $J_0 d$ 无法解释残差的这个分量。GCV 泛函将这个无法解释的大残差分量解释为存在巨大噪声的证据，其最小化过程会要求一个非常大的 $\\alpha_0$ 值来抑制它。对于一个非常大的 GCV 泛函最小化器（$\\alpha_0 \\to \\infty$），得到的正则化步长为 $d_0 \\approx (J_0^\\top J_0 + \\alpha_0^2 I)^{-1} J_0^\\top r_0 \\approx \\frac{1}{\\alpha_0^2} J_0^\\top r_0$，该值会变得极小。因此，高斯-牛顿迭代几乎没有进展，即 $x_1 \\approx x_0$，过程陷入停滞。\n\n相比之下，具有小的固定 $\\alpha$ 的基准方案实质上是在执行标准的高斯-牛顿步：$d_0 \\approx J_0^\\dagger r_0 = [2,2]^\\top$。这会产生更新 $x_1 = x_0 + d_0 = (-1,-1)^\\top + (2,2)^\\top = (1,1)^\\top = x^\\star$，在单次迭代中即收敛。这鲜明地展示了朴素 GCV 方法的失败。\n\n对于测试用例 2 ($\\gamma=0.5$)，非线性较弱。雅可比矩阵处处都是良态的，线性化误差很小。GCV 表现良好，选择了合理的 $\\alpha_k$，并且不会停滞。\n对于测试用例 3 ($\\gamma=10$, $x_0=(0.9,0.9)^\\top$)，尽管非线性很强，但初始猜测值接近解。残差 $r_0 \\approx [0.1, 0.1, 0]^\\top$ 很小，并且几乎完全位于 $J_0$ 的值域内。GCV 正确地识别出几乎不需要平滑，选择了 $\\alpha_0 \\approx 0$，方法迅速收敛。\n\n所提供的程序通过计算实现了这两种方案——朴素 GCV 和固定 alpha 基准——并将它们应用于三个测试用例。它为每种情况计算了最终与初始残差范数之比，并评估了规定的停滞准则，从而验证了第一种情况下的预测停滞以及其他情况下的成功收敛。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements and tests two Gauss-Newton schemes to demonstrate the\n    failure of naive Generalized Cross-Validation (GCV) for a specific\n    nonlinear inverse problem.\n    \"\"\"\n    # --- Constants  Parameters from problem statement and implementation choices ---\n    S = 1.0\n    X_STAR = np.array([1.0, 1.0])\n    MAX_ITER = 50\n    FIXED_ALPHA = 1e-6\n    STAGNATION_THRESHOLD_GCV = 0.95\n    CONVERGENCE_THRESHOLD_FIXED = 0.70\n\n    # --- Problem-specific functions: Forward Map and Jacobian ---\n    def forward_map(x, s, gamma):\n        x1, x2 = x\n        val = s * np.tanh(gamma * (x1 + x2))\n        return np.array([x1, x2, val])\n\n    def jacobian(x, s, gamma):\n        x1, x2 = x\n        cosh_arg = gamma * (x1 + x2)\n        # Avoid overflow for large arguments in cosh\n        if np.abs(cosh_arg) > 30:\n            sech_val = 0.0\n        else:\n            sech_val = 1.0 / np.cosh(cosh_arg)\n        \n        d_tanh = s * gamma * sech_val**2\n        return np.array([\n            [1.0, 0.0],\n            [0.0, 1.0],\n            [d_tanh, d_tanh]\n        ])\n\n    # --- GCV Functional Implementation ---\n    def gcv_function(alpha, J, r):\n        m, n = J.shape\n        alpha2 = alpha**2\n        \n        try:\n            # SVD is the standard, numerically stable way to analyze the GCV functional.\n            U, s_vals, _ = np.linalg.svd(J, full_matrices=True)\n        except np.linalg.LinAlgError:\n            return np.inf\n\n        s2 = s_vals**2\n        r_hat = U.T @ r\n\n        # Numerator: ||(I - A)r||^2 where A is the influence matrix\n        f_res = alpha2 / (s2 + alpha2)\n        num_term1 = np.sum((f_res * r_hat[:n])**2) # Component in range(J)\n        num_term2 = np.sum(r_hat[n:]**2)      # Component in null(J^T)\n        numerator = num_term1 + num_term2\n\n        # Denominator: (Tr(I - A))^2\n        trace_I_minus_A = (m - n) + np.sum(alpha2 / (s2 + alpha2))\n        \n        if trace_I_minus_A  1e-15:\n            return np.inf\n            \n        denominator = trace_I_minus_A**2\n        \n        return numerator / denominator\n\n    # --- Gauss-Newton Solver ---\n    def solve_gauss_newton(x0, gamma, s, y_true, method, fixed_alpha, max_iter):\n        x_k = np.copy(x0.astype(np.float64))\n        \n        initial_residual = y_true - forward_map(x_k, s, gamma)\n        initial_residual_norm = np.linalg.norm(initial_residual)\n\n        if initial_residual_norm  1e-12: # Already converged\n            return 0.0\n\n        for _ in range(max_iter):\n            r_k = y_true - forward_map(x_k, s, gamma)\n            J_k = jacobian(x_k, s, gamma)\n\n            if method == 'gcv':\n                # Find alpha_k by minimizing the GCV functional on a log-spaced grid or via optimizer\n                res = minimize_scalar(\n                    lambda alpha: gcv_function(alpha, J_k, r_k),\n                    bounds=(1e-10, 1e4), # A wide search range for alpha\n                    method='bounded'\n                )\n                alpha_k = res.x\n            elif method == 'fixed':\n                alpha_k = fixed_alpha\n            else:\n                raise ValueError(\"Invalid method specified.\")\n\n            # Solve the Tikhonov-regularized normal equations for the step d_k\n            # (J_k^T J_k + alpha_k^2 I) d_k = J_k^T r_k\n            A = J_k.T @ J_k + (alpha_k**2) * np.identity(x_k.shape[0])\n            b = J_k.T @ r_k\n            \n            try:\n                d_k = np.linalg.solve(A, b)\n            except np.linalg.LinAlgError:\n                # If matrix is singular, iteration cannot proceed.\n                break\n\n            x_k += d_k\n        \n        final_residual_norm = np.linalg.norm(y_true - forward_map(x_k, s, gamma))\n        \n        return final_residual_norm / initial_residual_norm\n\n    # --- Main Driver Script ---\n    test_cases = [\n        # (gamma, x0)\n        (10.0, np.array([-1.0, -1.0])),\n        (0.5, np.array([-1.0, -1.0])),\n        (10.0, np.array([0.9, 0.9]))\n    ]\n    \n    results = []\n\n    for gamma, x0 in test_cases:\n        # The true data y depends on the forward map's parameters\n        y_true = forward_map(X_STAR, S, gamma)\n\n        # Run with Naive GCV\n        ratio_gcv = solve_gauss_newton(\n            x0=x0, gamma=gamma, s=S, y_true=y_true,\n            method='gcv', fixed_alpha=None, max_iter=MAX_ITER\n        )\n\n        # Run with Fixed Alpha Baseline\n        ratio_fixed = solve_gauss_newton(\n            x0=x0, gamma=gamma, s=S, y_true=y_true,\n            method='fixed', fixed_alpha=FIXED_ALPHA, max_iter=MAX_ITER\n        )\n\n        # Evaluate the stagnation criterion from the problem description\n        stagnated = (\n            ratio_gcv >= STAGNATION_THRESHOLD_GCV and\n            ratio_fixed = CONVERGENCE_THRESHOLD_FIXED\n        )\n        results.append(stagnated)\n\n    # Print the final result in the exact specified format\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}