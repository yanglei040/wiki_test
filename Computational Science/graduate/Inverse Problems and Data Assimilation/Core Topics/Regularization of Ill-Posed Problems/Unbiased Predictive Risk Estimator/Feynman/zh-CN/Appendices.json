{
    "hands_on_practices": [
        {
            "introduction": "为了通过最小化无偏预测风险估计 (UPRE) 来自动选择正则化参数 $\\lambda$，我们需要一种有效的优化算法。大多数优化方法，例如梯度下降法，都依赖于目标函数相对于 $\\lambda$ 的梯度。本练习将指导您完成一个基础但至关重要的推导：利用奇异值分解 (SVD) 表示，计算吉洪诺夫正则化 (Tikhonov regularization) 的 UPRE 函数的解析梯度 。掌握这项技能是实现任何基于梯度的 UPRE 最小化算法的第一步。",
            "id": "3429094",
            "problem": "考虑一个线性逆问题，其中数据来自正向模型 $y = A x_{\\star} + \\epsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x_{\\star} \\in \\mathbb{R}^{n}$，以及加性噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^{2} I_{m})$，其方差 $\\sigma_{\\epsilon}^{2} > 0$ 已知。对于给定的正则化参数 $\\lambda > 0$，零阶吉洪诺夫（Tikhonov）估计量定义为\n$$\nx_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2} \\right\\}.\n$$\n设相关的预测数据为 $\\hat{y}_{\\lambda} = A x_{\\lambda} = H_{\\lambda} y$，其中线性“帽子”算子为\n$$\nH_{\\lambda} := A (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m}.\n$$\n用于预测损失的无偏预测风险估计量（Unbiased Predictive Risk Estimator, UPRE）定义为\n$$\n\\mathrm{UPRE}(\\lambda) := \\|y - \\hat{y}_{\\lambda}\\|_{2}^{2} + 2 \\sigma_{\\epsilon}^{2} \\,\\mathrm{tr}(H_{\\lambda}) - m \\sigma_{\\epsilon}^{2}.\n$$\n假设 $A$ 的秩为 $r \\le \\min\\{m,n\\}$，其奇异值分解（Singular Value Decomposition, SVD）为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 的对角线元素为 $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} > 0$，且当 $i > r$ 时 $\\sigma_{i} = 0$。定义数据系数 $c_{i} := u_{i}^{\\top} y$，其中 $u_{i}$ 是 $U$ 的第 $i$ 列，以及数据空间的吉洪诺夫滤波因子\n$$\n\\phi_{i}(\\lambda) := \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} \\quad \\text{对于 } i = 1,\\ldots,r, \\quad \\text{以及} \\quad \\phi_{i}(\\lambda) := 0 \\quad \\text{对于 } i > r.\n$$\n\n从 $\\mathrm{UPRE}(\\lambda)$ 的给定定义、SVD 的基本性质以及矩阵微积分的标准法则出发，推导导数 $\\frac{d}{d\\lambda}\\mathrm{UPRE}(\\lambda)$ 的一个闭式表达式，该表达式完全由 $\\lambda$、$\\sigma_{\\epsilon}^{2}$、$\\{\\sigma_{i}\\}_{i=1}^{r}$、$\\{u_{i}^{\\top} y\\}_{i=1}^{m}$ 和 $\\{\\phi_{i}(\\lambda)\\}_{i=1}^{m}$ 表示。你的最终答案必须是仅依赖于这些量的单个解析表达式。不要假设或引用任何关于 $\\phi_{i}(\\lambda)$ 的现成导数公式；相反，应从其定义计算 $\\frac{d}{d\\lambda}\\phi_{i}(\\lambda)$。\n\n最终答案必须是单个闭式解析表达式。不需要进行数值计算。",
            "solution": "我们从无偏预测风险估计量（UPRE）的定义开始：\n$$\n\\mathrm{UPRE}(\\lambda) = \\|y - \\hat{y}_{\\lambda}\\|_{2}^{2} + 2 \\sigma_{\\epsilon}^{2}\\,\\mathrm{tr}(H_{\\lambda}) - m \\sigma_{\\epsilon}^{2},\n$$\n其中 $\\hat{y}_{\\lambda} = H_{\\lambda} y$ 且 $H_{\\lambda} = A (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top}$。\n\n为了在一组能使这些算子对角化的基中分析 $\\mathrm{UPRE}(\\lambda)$，我们使用奇异值分解（SVD）$A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 是正交矩阵，$V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，其对角元素 $(\\Sigma)_{ii} = \\sigma_{i}$（对于 $i = 1,\\ldots,r$），其他元素为零，其中 $r = \\mathrm{rank}(A)$。那么\n$$\nA^{\\top} A = V \\Sigma^{\\top} \\Sigma V^{\\top} = V \\,\\mathrm{diag}(\\sigma_{1}^{2},\\ldots,\\sigma_{r}^{2},0,\\ldots,0)\\, V^{\\top}.\n$$\n因此，\n$$\n(A^{\\top} A + \\lambda^{2} I_{n})^{-1} = V \\,\\mathrm{diag}\\!\\left(\\frac{1}{\\sigma_{1}^{2} + \\lambda^{2}},\\ldots,\\frac{1}{\\sigma_{r}^{2} + \\lambda^{2}},\\frac{1}{\\lambda^{2}},\\ldots,\\frac{1}{\\lambda^{2}}\\right) V^{\\top}.\n$$\n展开相乘，\n\\begin{align*}\nH_{\\lambda} \n= A (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top} \\\\\n= U \\Sigma V^{\\top} \\,\\left[V \\,\\mathrm{diag}\\!\\left(\\frac{1}{\\sigma_{1}^{2} + \\lambda^{2}},\\ldots,\\frac{1}{\\sigma_{r}^{2} + \\lambda^{2}},\\frac{1}{\\lambda^{2}},\\ldots,\\frac{1}{\\lambda^{2}}\\right) V^{\\top}\\right] V \\Sigma^{\\top} U^{\\top} \\\\\n= U \\left[\\Sigma \\,\\mathrm{diag}\\!\\left(\\frac{1}{\\sigma_{1}^{2} + \\lambda^{2}},\\ldots,\\frac{1}{\\sigma_{r}^{2} + \\lambda^{2}},\\frac{1}{\\lambda^{2}},\\ldots,\\frac{1}{\\lambda^{2}}\\right) \\Sigma^{\\top}\\right] U^{\\top}.\n\\end{align*}\n由于 $\\Sigma$ 只有前 $r$ 个对角元素非零，且 $\\Sigma \\Sigma^{\\top}$ 具有相同的非零对角元素 $\\sigma_{i}^{2}$（对于 $i \\le r$），中间的方括号化简为 $m \\times m$ 的对角矩阵，其元素为\n$$\n\\phi_{i}(\\lambda) = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} \\quad \\text{对于 } i=1,\\ldots,r, \\qquad \\phi_{i}(\\lambda) = 0 \\quad \\text{对于 } i > r.\n$$\n因此\n$$\nH_{\\lambda} = U \\,\\mathrm{diag}\\!\\big(\\phi_{1}(\\lambda),\\ldots,\\phi_{m}(\\lambda)\\big)\\, U^{\\top}.\n$$\n\n令 $c_{i} := u_{i}^{\\top} y$ 表示 $y$ 在标准正交基 $\\{u_{i}\\}$ 中的系数，其中 $U^{\\top} y = (c_{1},\\ldots,c_{m})^{\\top}$。那么残差为\n$$\nr_{\\lambda} := y - \\hat{y}_{\\lambda} = (I_{m} - H_{\\lambda}) y = U \\,\\mathrm{diag}\\!\\big(1 - \\phi_{1}(\\lambda),\\ldots,1 - \\phi_{m}(\\lambda)\\big)\\, U^{\\top} y,\n$$\n因此\n$$\n\\|r_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{m} \\big(1 - \\phi_{i}(\\lambda)\\big)^{2} c_{i}^{2}.\n$$\n此外，迹为\n$$\n\\mathrm{tr}(H_{\\lambda}) = \\sum_{i=1}^{m} \\phi_{i}(\\lambda) = \\sum_{i=1}^{r} \\phi_{i}(\\lambda),\n$$\n因为当 $i>r$ 时 $\\phi_{i}(\\lambda)=0$。所以，\n$$\n\\mathrm{UPRE}(\\lambda) = \\sum_{i=1}^{m} \\big(1 - \\phi_{i}(\\lambda)\\big)^{2} c_{i}^{2} + 2 \\sigma_{\\epsilon}^{2} \\sum_{i=1}^{r} \\phi_{i}(\\lambda) - m \\sigma_{\\epsilon}^{2}.\n$$\n注意，对于 $i>r$，$\\phi_{i}(\\lambda) \\equiv 0$，所以唯一与 $\\lambda$ 相关的项来自 $i \\le r$。对 $\\lambda$ 求导并应用链式法则，\n\\begin{align*}\n\\frac{d}{d\\lambda} \\mathrm{UPRE}(\\lambda)\n= \\sum_{i=1}^{r} \\frac{d}{d\\lambda} \\left[ \\big(1 - \\phi_{i}(\\lambda)\\big)^{2} c_{i}^{2} + 2 \\sigma_{\\epsilon}^{2} \\phi_{i}(\\lambda) \\right] \\\\\n= \\sum_{i=1}^{r} \\left[ 2 \\big(1 - \\phi_{i}(\\lambda)\\big) \\big(-\\phi_{i}'(\\lambda)\\big) c_{i}^{2} + 2 \\sigma_{\\epsilon}^{2} \\phi_{i}'(\\lambda) \\right] \\\\\n= 2 \\sum_{i=1}^{r} \\phi_{i}'(\\lambda) \\left[ \\sigma_{\\epsilon}^{2} - \\big(1 - \\phi_{i}(\\lambda)\\big) c_{i}^{2} \\right].\n\\end{align*}\n剩下的任务是从其定义计算 $\\phi_{i}'(\\lambda)$。对于 $i \\le r$，\n$$\n\\phi_{i}(\\lambda) = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}.\n$$\n求导得，\n$$\n\\phi_{i}'(\\lambda) = \\frac{d}{d\\lambda} \\left( \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} \\right) = - \\frac{2 \\lambda \\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}}.\n$$\n通常将 $\\phi_{i}'(\\lambda)$ 仅用 $\\phi_{i}(\\lambda)$ 和 $\\sigma_{i}$ 来表示会很方便。使用 $\\phi_{i}(\\lambda) = \\sigma_{i}^{2}/(\\sigma_{i}^{2} + \\lambda^{2})$，我们有\n$$\n\\phi_{i}(\\lambda)^{2} = \\frac{\\sigma_{i}^{4}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}} \\quad \\Longrightarrow \\quad \\phi_{i}'(\\lambda) = - \\frac{2 \\lambda \\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}} = - \\frac{2 \\lambda}{\\sigma_{i}^{2}} \\,\\phi_{i}(\\lambda)^{2}.\n$$\n将此代入 $\\mathrm{UPRE}(\\lambda)$ 的导数中，得到\n\\begin{align*}\n\\frac{d}{d\\lambda} \\mathrm{UPRE}(\\lambda)\n= 2 \\sum_{i=1}^{r} \\left( - \\frac{2 \\lambda}{\\sigma_{i}^{2}} \\,\\phi_{i}(\\lambda)^{2} \\right) \\left[ \\sigma_{\\epsilon}^{2} - \\big(1 - \\phi_{i}(\\lambda)\\big) c_{i}^{2} \\right] \\\\\n= - 4 \\lambda \\sum_{i=1}^{r} \\frac{\\phi_{i}(\\lambda)^{2}}{\\sigma_{i}^{2}} \\left[ \\sigma_{\\epsilon}^{2} - \\big(1 - \\phi_{i}(\\lambda)\\big) c_{i}^{2} \\right].\n\\end{align*}\n回顾 $c_{i} = u_{i}^{\\top} y$，我们得到了完全用 $\\lambda$、$\\sigma_{\\epsilon}^{2}$、$\\{\\sigma_{i}\\}$、$\\{u_{i}^{\\top} y\\}$ 和 $\\{\\phi_{i}(\\lambda)\\}$ 表示的所需闭式表达式：\n$$\n\\frac{d}{d\\lambda} \\mathrm{UPRE}(\\lambda) = - 4 \\lambda \\sum_{i=1}^{r} \\frac{\\phi_{i}(\\lambda)^{2}}{\\sigma_{i}^{2}} \\left( \\sigma_{\\epsilon}^{2} - \\big(1 - \\phi_{i}(\\lambda)\\big) (u_{i}^{\\top} y)^{2} \\right).\n$$\n此表达式对任何 $\\lambda > 0$ 都有效，并且在 $A$ 为满秩或秩亏（后者通过当 $i>r$ 时 $\\phi_{i}(\\lambda) \\equiv 0$ 实现）时能正确化简。",
            "answer": "$$\\boxed{-4\\lambda \\sum_{i=1}^{r} \\frac{\\phi_{i}(\\lambda)^{2}}{\\sigma_{i}^{2}} \\left( \\sigma_{\\epsilon}^{2} - \\big(1 - \\phi_{i}(\\lambda)\\big)\\,(u_{i}^{\\top} y)^{2} \\right)}$$"
        },
        {
            "introduction": "虽然梯度下降法是最小化 UPRE 的可靠方法，但通过利用二阶导数信息，我们可以设计出收敛速度更快的算法，例如牛顿法 (Newton's method)。此实践在前一个练习的基础上更进一步，要求您推导 UPRE 函数的海森矩阵 (Hessian matrix) 并构建牛顿法的迭代更新公式 。完成此练习将使您能够实现一种更高效、更强大的正则化参数自动选择工具。",
            "id": "3429134",
            "problem": "考虑带有加性高斯噪声的离散线性逆问题，其中观测数据向量 $y \\in \\mathbb{R}^{m}$ 建模为 $y = A x_{\\star} + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x_{\\star} \\in \\mathbb{R}^{n}$ 是未知真值，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$ 是均值为零、方差为 $\\sigma^{2} > 0$ 的高斯噪声。我们考虑使用正则化参数 $\\lambda > 0$ 的零阶提赫诺夫正则化，其定义为\n$$\n\\widehat{x}_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2} \\right\\}.\n$$\n相应的线性估计器可以写为 $\\widehat{x}_{\\lambda} = S(\\lambda) y$，其中 $S(\\lambda) = (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top}$。将数据空间的预测风险定义为 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A \\widehat{x}_{\\lambda} - A x_{\\star}\\|_{2}^{2}\\right]$。在高斯模型下，$R_{\\mathrm{pred}}(\\lambda)$ 的无偏预测风险估计量 (UPRE) 是函数\n$$\nU(\\lambda) \\equiv \\mathrm{UPRE}(\\lambda) = \\|(I_{m} - A S(\\lambda)) y\\|_{2}^{2} + 2 \\sigma^{2} \\,\\mathrm{tr}(A S(\\lambda)) - m \\sigma^{2}.\n$$\n设 $A$ 的奇异值分解 (SVD) 为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 在其前导 $r \\times r$ 块上的对角线元素为 $\\{\\sigma_{i}\\}_{i=1}^{r}$，满足 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r} > 0$ 且 $r = \\mathrm{rank}(A) \\le \\min\\{m, n\\}$。用 $w_{i} = u_{i}^{\\top} y$ 表示数据系数，其中 $i \\in \\{1, \\dots, m\\}$，$u_{i}$ 是 $U$ 的第 $i$ 列。\n\n任务：\n- 从 $U(\\lambda)$ 的定义和 $A$ 的 SVD 出发，通过消除矩阵的迹和范数，将 $U(\\lambda)$ 完全用 $\\{\\sigma_{i}\\}_{i=1}^{r}$、$\\{w_{i}\\}_{i=1}^{m}$ 和 $\\lambda$ 来表示。然后进行微分，得到一阶导数 $U'(\\lambda)$ 和二阶导数 $U''(\\lambda)$ 的显式公式。\n- 用 $U'(\\lambda)$ 和 $U''(\\lambda)$ 陈述最小化 $U(\\lambda)$ 的一阶最优性条件及相关的二阶条件。\n- 为 $\\lambda$ 设计一个仅使用基于 SVD 的标量的牛顿或拟牛顿迭代法。提供单步牛顿更新公式 $\\lambda_{\\mathrm{new}} = \\lambda - U'(\\lambda)/U''(\\lambda)$ 的一个闭式解析表达式，该表达式用 $\\{\\sigma_{i}\\}_{i=1}^{r}$、$\\{w_{i}\\}_{i=1}^{r}$、$\\sigma^{2}$ 和 $\\lambda$ 表示。\n\n答案格式要求：\n- 你的最终答案必须是单步牛顿更新 $\\lambda_{\\mathrm{new}}$ 的一个闭式解析表达式，用 $\\{\\sigma_{i}\\}_{i=1}^{r}$、$\\{w_{i}\\}_{i=1}^{r}$、$\\sigma^{2}$ 和 $\\lambda$ 表示。\n- 无需进行数值计算。\n- 最终答案中不要包含任何单位。",
            "solution": "### 解答\n\n解答过程首先将 UPRE 函数 $U(\\lambda)$ 及其导数用矩阵 $A$ 的 SVD 分量表示。\n\n**1. 用 SVD 坐标表示 $U(\\lambda)$**\n\n我们首先使用 $A = U \\Sigma V^{\\top}$ 的 SVD 来分析矩阵乘积 $A S(\\lambda)$。\n首先，考虑项 $A^{\\top} A + \\lambda^{2} I_{n}$：\n$$A^{\\top} A + \\lambda^{2} I_{n} = (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) + \\lambda^{2} I_{n} = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} + \\lambda^{2} V V^{\\top} = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n}) V^{\\top}$$\n其逆矩阵为：\n$$(A^{\\top} A + \\lambda^{2} I_{n})^{-1} = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} V^{\\top}$$\n现在，我们可以写出解算子 $S(\\lambda)$：\n$$S(\\lambda) = (A^{\\top} A + \\lambda^{2} I_{n})^{-1} A^{\\top} = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} V^{\\top} (V \\Sigma^{\\top} U^{\\top}) = V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top} U^{\\top}$$\n核心对象 $A S(\\lambda)$，通常称为影响矩阵，变为：\n$$A S(\\lambda) = (U \\Sigma V^{\\top}) S(\\lambda) = U \\Sigma V^{\\top} V (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top} U^{\\top} = U \\left( \\Sigma (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top} \\right) U^{\\top}$$\n矩阵 $\\Sigma (\\Sigma^{\\top} \\Sigma + \\lambda^{2} I_{n})^{-1} \\Sigma^{\\top}$ 是一个 $m \\times m$ 的对角矩阵。其对角线元素，称为滤波因子，为：\n$$f_{i} = \\begin{cases} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}  \\text{for } i = 1, \\dots, r \\\\ 0  \\text{for } i = r+1, \\dots, m \\end{cases}$$\n我们将这个对角矩阵表示为 $\\Lambda_{f}$。那么 $A S(\\lambda) = U \\Lambda_{f} U^{\\top}$。\n\n现在我们可以重写 $U(\\lambda)$ 中的各项：\n-   残差范数项：\n    $$\\|(I_{m} - A S(\\lambda)) y\\|_{2}^{2} = \\|(I_{m} - U \\Lambda_{f} U^{\\top}) y\\|_{2}^{2} = \\|U (I_{m} - \\Lambda_{f}) U^{\\top} y\\|_{2}^{2}$$\n    由于 $U$ 是正交的，这等于 $\\|(I_{m} - \\Lambda_{f}) (U^{\\top} y)\\|_{2}^{2}$。令 $w = U^{\\top} y$，所以 $w_{i} = u_i^\\top y$。$(I_{m} - \\Lambda_{f}) w$ 的分量是 $(1-f_{i})w_{i}$。范数的平方是：\n    $$\\sum_{i=1}^{m} (1 - f_{i})^{2} w_{i}^{2} = \\sum_{i=1}^{r} \\left(1 - \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)^{2} w_{i}^{2} + \\sum_{i=r+1}^{m} (1-0)^{2} w_{i}^{2} = \\sum_{i=1}^{r} \\left(\\frac{\\lambda^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)^{2} w_{i}^{2} + \\sum_{i=r+1}^{m} w_{i}^{2}$$\n-   迹项：\n    $$\\mathrm{tr}(A S(\\lambda)) = \\mathrm{tr}(U \\Lambda_{f} U^{\\top}) = \\mathrm{tr}(\\Lambda_{f} U^{\\top} U) = \\mathrm{tr}(\\Lambda_{f}) = \\sum_{i=1}^{m} f_{i} = \\sum_{i=1}^{r} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}$$\n将这些结合起来，得到 $U(\\lambda)$ 的完整表达式：\n$$U(\\lambda) = \\sum_{i=1}^{r} \\frac{\\lambda^{4}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}} w_{i}^{2} + \\sum_{i=r+1}^{m} w_{i}^{2} + 2 \\sigma^{2} \\sum_{i=1}^{r} \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} - m \\sigma^{2}$$\n\n**2. $U(\\lambda)$ 的一阶和二阶导数**\n\n为了找到 $U(\\lambda)$ 的最小值，我们计算它关于 $\\lambda$ 的导数。项 $\\sum_{i=r+1}^{m} w_{i}^{2}$ 和 $-m \\sigma^{2}$ 相对于 $\\lambda$ 是常数，在微分时会消失。\n\n对于一阶导数 $U'(\\lambda)$：\n$$ \\frac{d}{d\\lambda} \\left( \\frac{\\lambda^{4}}{(\\sigma_i^2+\\lambda^2)^2} \\right) = \\frac{4\\lambda^3(\\sigma_i^2+\\lambda^2)^2 - \\lambda^4 \\cdot 2(\\sigma_i^2+\\lambda^2)(2\\lambda)}{(\\sigma_i^2+\\lambda^2)^4} = \\frac{4\\lambda^3(\\sigma_i^2+\\lambda^2) - 4\\lambda^5}{(\\sigma_i^2+\\lambda^2)^3} = \\frac{4\\lambda^3\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^3} $$\n$$ \\frac{d}{d\\lambda} \\left( \\frac{\\sigma_i^2}{\\sigma_i^2+\\lambda^2} \\right) = \\sigma_i^2 \\frac{-1}{(\\sigma_i^2+\\lambda^2)^2} (2\\lambda) = \\frac{-2\\lambda\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^2} $$\n结合这些结果，我们得到 $U'(\\lambda)$：\n$$U'(\\lambda) = \\sum_{i=1}^{r} \\frac{4\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} - \\sum_{i=1}^{r} \\frac{4\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}$$\n\n对于二阶导数 $U''(\\lambda)$，我们对 $U'(\\lambda)$ 进行微分：\n对于 $U'(\\lambda)$ 中的第一项：\n$$ \\frac{d}{d\\lambda} \\left( \\frac{4\\lambda^3\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^3} \\right) = 4\\sigma_i^2 \\frac{3\\lambda^2(\\sigma_i^2+\\lambda^2)^3 - \\lambda^3 \\cdot 3(\\sigma_i^2+\\lambda^2)^2(2\\lambda)}{(\\sigma_i^2+\\lambda^2)^6} = 4\\sigma_i^2 \\frac{3\\lambda^2(\\sigma_i^2+\\lambda^2) - 6\\lambda^4}{(\\sigma_i^2+\\lambda^2)^4} = \\frac{12\\lambda^2\\sigma_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} $$\n对于 $U'(\\lambda)$ 中的第二项：\n$$ \\frac{d}{d\\lambda} \\left( \\frac{-4\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2+\\lambda^2)^2} \\right) = -4\\sigma^2\\sigma_i^2 \\frac{1(\\sigma_i^2+\\lambda^2)^2 - \\lambda \\cdot 2(\\sigma_i^2+\\lambda^2)(2\\lambda)}{(\\sigma_i^2+\\lambda^2)^4} = -4\\sigma^2\\sigma_i^2 \\frac{\\sigma_i^2+\\lambda^2 - 4\\lambda^2}{(\\sigma_i^2+\\lambda^2)^3} = \\frac{-4\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3} $$\n结合这些，我们得到 $U''(\\lambda)$：\n$$U''(\\lambda) = \\sum_{i=1}^{r} \\frac{12\\lambda^2\\sigma_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} w_i^2 - \\sum_{i=1}^{r} \\frac{4\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3}$$\n\n**3. 最优性条件**\n\n为了找到使 $U(\\lambda)$ 最小化的 $\\lambda > 0$ 的值，我们使用标准的微积分极值条件。\n-   **一阶必要条件：** 候选最小值 $\\lambda^{\\ast}$ 必须是一个驻点，即必须满足 $U'(\\lambda^{\\ast}) = 0$。\n-   **二阶充分条件：** 为使驻点成为一个局部最小值，二阶导数必须为正，即 $U''(\\lambda^{\\ast}) > 0$。\n\n**4. 用于最小化 $U(\\lambda)$ 的牛顿法**\n\n最小化 $U(\\lambda)$ 等价于求其导数 $U'(\\lambda)=0$ 的一个根。用于求函数 $f(x)=0$ 根的牛顿法提供了迭代更新式 $x_{k+1} = x_k - f(x_k)/f'(x_k)$。将此应用于 $f(\\lambda) = U'(\\lambda)$，我们得到 $f'(\\lambda) = U''(\\lambda)$，因此 $\\lambda$ 的更新规则是：\n$$\\lambda_{\\mathrm{new}} = \\lambda - \\frac{U'(\\lambda)}{U''(\\lambda)}$$\n代入 $U'(\\lambda)$ 和 $U''(\\lambda)$ 的表达式，得到单步牛顿更新公式：\n$$\\lambda_{\\mathrm{new}} = \\lambda - \\frac{\\sum_{i=1}^{r} \\frac{4\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} - \\sum_{i=1}^{r} \\frac{4\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}}{\\sum_{i=1}^{r} \\frac{12\\lambda^2\\sigma_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} w_i^2 - \\sum_{i=1}^{r} \\frac{4\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3}}$$\n我们可以消去公因数 $4$ 来获得最终表达式。\n$$\\lambda_{\\mathrm{new}} = \\lambda - \\frac{\\sum_{i=1}^{r} \\frac{\\lambda^3\\sigma_i^2 w_i^2}{(\\sigma_i^2 + \\lambda^2)^3} - \\sum_{i=1}^{r} \\frac{\\lambda\\sigma^2\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}}{\\sum_{i=1}^{r} \\frac{3\\lambda^2\\sigma_i^2 w_i^2(\\sigma_i^2-\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^4} - \\sum_{i=1}^{r} \\frac{\\sigma^2\\sigma_i^2(\\sigma_i^2-3\\lambda^2)}{(\\sigma_i^2+\\lambda^2)^3}}$$\n该表达式是用所需的项表示的：$\\{\\sigma_i\\}_{i=1}^r$、$\\{w_i\\}_{i=1}^r$、$\\sigma^2$ 以及当前迭代值 $\\lambda$。",
            "answer": "$$\\boxed{\\lambda - \\frac{\\sum_{i=1}^{r} \\frac{\\lambda^{3}\\sigma_{i}^{2} w_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{3}} - \\sum_{i=1}^{r} \\frac{\\lambda\\sigma^{2}\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}}}{\\sum_{i=1}^{r} \\frac{3\\lambda^{2}\\sigma_{i}^{2}w_{i}^{2}(\\sigma_{i}^{2}-\\lambda^{2})}{(\\sigma_{i}^{2}+\\lambda^{2})^{4}} - \\sum_{i=1}^{r} \\frac{\\sigma^{2}\\sigma_{i}^{2}(\\sigma_{i}^{2}-3\\lambda^{2})}{(\\sigma_{i}^{2}+\\lambda^{2})^{3}}}}$$"
        },
        {
            "introduction": "前面的练习集中于如何优化 UPRE 函数以找到单个最优正则化参数 $\\lambda$。然而，UPRE 的威力远不止于此；它还可以指导我们选择不同的正则化模型。本练习通过一个具体的双通道数据例子，让您亲手验证采用通道特定的双参数正则化方案为何优于简单的单参数方案 。通过最小化为每种方案推导出的 UPRE，您将量化更灵活模型所带来的性能提升，从而深刻理解 UPRE 在模型选择中的实际应用价值。",
            "id": "3429048",
            "problem": "考虑一个线性逆问题，其中一个共同状态向量通过两个独立的观测通道进行观测。设前向模型为 $A = I_{2}$，正则化算子为 $L = I_{2}$。真实的无噪声数据为 $y_{\\text{true}} = x_{\\text{true}} \\in \\mathbb{R}^{2}$，观测数据为 $y = y_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，协方差为对角矩阵 $\\Sigma = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$。在本问题中，设观测数据和噪声水平为\n$$\ny = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}, \\quad \\sigma_{1}^{2} = \\frac{1}{4}, \\quad \\sigma_{2}^{2} = 1.\n$$\n我们使用形式为 $x_{\\lambda} = \\arg\\min_{x \\in \\mathbb{R}^{2}} \\|A x - y\\|_{2}^{2} + \\lambda \\|L x\\|_{2}^{2}$ 的 Tikhonov 正则化估计器，在这里它简化为线性收缩估计器 $x_{\\lambda} = S y$，其中 $S$ 取决于所选的正则化参数。我们将比较两种方案：\n- 单参数方案：对两个通道应用一个共同的 $\\lambda \\ge 0$，因此 $S = s I_{2}$，其中 $s = 1/(1+\\lambda)$。\n- 双参数方案：对每个通道应用独立的 $(\\lambda_{1}, \\lambda_{2}) \\ge 0$，因此 $S = \\operatorname{diag}(s_{1}, s_{2})$，其中 $s_{j} = 1/(1+\\lambda_{j})$，对于 $j \\in \\{1,2\\}$。\n\n对于每种方案，考虑无噪声预测 $y_{\\text{true}}$ 的预测风险，即 $\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S y - y_{\\text{true}}\\|_{2}^{2}\\big]$，其中期望是针对测量噪声计算的。无偏预测风险估计器 (UPRE) 是数据 $y$ 和参数的任意函数，它是 $\\mathcal{R}(S)$ 的无偏估计器，并且可以通过最小化它相对于 $S$ 的值来选择参数。\n\n任务：\n1. 从线性高斯数据模型出发，推导此设置下无偏预测风险估计器 (UPRE) 的显式表达式，用 $y$、$\\sigma_{1}^{2}$、$\\sigma_{2}^{2}$ 和 $S = \\operatorname{diag}(s_{1}, s_{2})$ 表示。您的推导必须从定义 $\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S y - y_{\\text{true}}\\|_{2}^{2}\\big]$ 和高斯噪声下期望的性质开始，不得假定任何已有的 UPRE 公式。\n2. 在双参数方案下，对 $(s_{1}, s_{2})$ 最小化联合 UPRE，从而获得最小化联合 UPRE 的 $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star})$。评估此方案的最小 UPRE。\n3. 在单参数方案下，对 $s$（等价于 $\\lambda$）最小化 UPRE，从而获得最小化 UPRE 的 $\\lambda^{\\star}$。评估此方案的最小 UPRE。\n4. 使用第 2 和第 3 部分的结果，计算精确差值\n$$\n\\Delta \\;=\\; \\big(\\min_{\\lambda \\ge 0} \\operatorname{UPRE}_{\\text{single}}(\\lambda)\\big) \\;-\\; \\big(\\min_{\\lambda_{1}, \\lambda_{2} \\ge 0} \\operatorname{UPRE}_{\\text{two}}(\\lambda_{1}, \\lambda_{2})\\big).\n$$\n将您的最终答案表示为一个精确的有理数。这量化了通过无偏预测风险估计器 (UPRE) 在所有通道上调整单个 $\\lambda$ 的方法，相对于通过最小化联合 UPRE 选择的双参数方案，在预测风险上可能是次优的。",
            "solution": "### 任务 1：无偏预测风险估计器 (UPRE) 的推导\n\n预测风险定义为 $\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S y - y_{\\text{true}}\\|_{2}^{2}\\big]$，其中期望是针对噪声分布 $\\varepsilon$ 计算的。数据模型为 $y = y_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$。\n\n我们首先展开风险的定义，代入 $y = y_{\\text{true}} + \\varepsilon$：\n$$\n\\mathcal{R}(S) = \\mathbb{E}\\big[\\|S (y_{\\text{true}} + \\varepsilon) - y_{\\text{true}}\\|_{2}^{2}\\big] = \\mathbb{E}\\big[\\|(S - I)y_{\\text{true}} + S\\varepsilon\\|_{2}^{2}\\big]\n$$\n其中 $I$ 是单位矩阵。展开欧几里得范数的平方：\n$$\n\\mathcal{R}(S) = \\mathbb{E}\\big[ \\big( (S-I)y_{\\text{true}} \\big)^{T} \\big( (S-I)y_{\\text{true}} \\big) + 2 \\big( (S-I)y_{\\text{true}} \\big)^{T} (S\\varepsilon) + (S\\varepsilon)^{T} (S\\varepsilon) \\big]\n$$\n根据期望的线性性质，我们可以分配期望算子：\n$$\n\\mathcal{R}(S) = \\mathbb{E}\\big[ \\|(S-I)y_{\\text{true}}\\|_{2}^{2} \\big] + 2 \\mathbb{E}\\big[ y_{\\text{true}}^{T}(S-I)^{T}S\\varepsilon \\big] + \\mathbb{E}\\big[ \\|S\\varepsilon\\|_{2}^{2} \\big]\n$$\n第一项仅涉及确定性量，因此 $\\mathbb{E}\\big[ \\|(S-I)y_{\\text{true}}\\|_{2}^{2} \\big] = \\|(S-I)y_{\\text{true}}\\|_{2}^{2}$。\n对于第二项，由于 $y_{\\text{true}}$ 和 $S$ 是确定性的，我们有 $2 y_{\\text{true}}^{T}(S-I)^{T}S \\, \\mathbb{E}[\\varepsilon]$。由于 $\\mathbb{E}[\\varepsilon]=0$，该交叉项消失。\n因此，风险简化为：\n$$\n\\mathcal{R}(S) = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + \\mathbb{E}\\big[ \\|S\\varepsilon\\|_{2}^{2} \\big]\n$$\n第二项可以使用迹技巧进行评估：$\\mathbb{E}[\\|S\\varepsilon\\|_{2}^{2}] = \\mathbb{E}[\\varepsilon^{T}S^{T}S\\varepsilon] = \\mathbb{E}[\\operatorname{tr}(\\varepsilon^{T}S^{T}S\\varepsilon)] = \\mathbb{E}[\\operatorname{tr}(S^{T}S\\varepsilon\\varepsilon^{T})] = \\operatorname{tr}(S^{T}S\\mathbb{E}[\\varepsilon\\varepsilon^{T}]) = \\operatorname{tr}(S^{T}S\\Sigma)$。\n所以，真实的风险是：\n$$\n\\mathcal{R}(S) = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + \\operatorname{tr}(S^{T}S\\Sigma)\n$$\n该表达式依赖于未知的 $y_{\\text{true}}$。一个无偏预测风险估计器 $\\operatorname{UPRE}(y, S)$ 必须是观测数据 $y$ 和参数的函数，并满足 $\\mathbb{E}[\\operatorname{UPRE}(y, S)] = \\mathcal{R}(S)$。\n\n为了构造这样一个估计器，我们为未知项 $\\|(S-I)y_{\\text{true}}\\|_{2}^{2}$ 寻找一个无偏估计器。让我们考虑量 $\\|(S-I)y\\|_{2}^{2}$：\n$$\n\\|(S-I)y\\|_{2}^{2} = \\|(S-I)(y_{\\text{true}} + \\varepsilon)\\|_{2}^{2} = \\|(S-I)y_{\\text{true}} + (S-I)\\varepsilon\\|_{2}^{2}\n$$\n展开这个范数的平方得到：\n$$\n\\|(S-I)y\\|_{2}^{2} = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + 2y_{\\text{true}}^{T}(S-I)^{T}(S-I)\\varepsilon + \\|(S-I)\\varepsilon\\|_{2}^{2}\n$$\n对这个表达式取期望：\n$$\n\\mathbb{E}\\big[\\|(S-I)y\\|_{2}^{2}\\big] = \\|(S-I)y_{\\text{true}}\\|_{2}^{2} + \\mathbb{E}\\big[\\|(S-I)\\varepsilon\\|_{2}^{2}\\big]\n$$\n交叉项的期望再次为零。对第二项使用迹技巧，我们得到 $\\mathbb{E}\\big[\\|(S-I)\\varepsilon\\|_{2}^{2}\\big] = \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big)$。\n因此，我们有以下关系：\n$$\n\\|(S-I)y_{\\text{true}}\\|_{2}^{2} = \\mathbb{E}\\big[\\|(S-I)y\\|_{2}^{2}\\big] - \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big)\n$$\n这意味着量 $\\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big)$ 是 $\\|(S-I)y_{\\text{true}}\\|_{2}^{2}$ 的一个无偏估计器。将此代入 $\\mathcal{R}(S)$ 的表达式，我们得到我们的 UPRE 函数：\n$$\n\\operatorname{UPRE}(y, S) = \\left( \\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}\\big((S-I)^{T}(S-I)\\Sigma\\big) \\right) + \\operatorname{tr}(S^{T}S\\Sigma)\n$$\n矩阵 $S$ 是对称的，所以 $S^T=S$。因此 $(S-I)^{T}(S-I) = (S-I)^2 = S^2 - 2S + I$。\n$$\n\\operatorname{UPRE}(y, S) = \\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}\\big((S^2 - 2S + I)\\Sigma\\big) + \\operatorname{tr}(S^2\\Sigma)\n$$\n利用迹的线性性质：\n$$\n\\operatorname{UPRE}(y, S) = \\|(S-I)y\\|_{2}^{2} - \\operatorname{tr}(S^2\\Sigma) + 2\\operatorname{tr}(S\\Sigma) - \\operatorname{tr}(\\Sigma) + \\operatorname{tr}(S^2\\Sigma)\n$$\n$$\n\\operatorname{UPRE}(y, S) = \\|(S-I)y\\|_{2}^{2} + 2\\operatorname{tr}(S\\Sigma) - \\operatorname{tr}(\\Sigma)\n$$\n对于 $S = \\operatorname{diag}(s_{1}, s_{2})$、$\\Sigma = \\operatorname{diag}(\\sigma_{1}^{2}, \\sigma_{2}^{2})$ 和 $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$ 的特定情况：\n$\\|(S-I)y\\|_{2}^{2} = \\| \\operatorname{diag}(s_1-1, s_2-1)y \\|_{2}^{2} = (s_1-1)^2 y_1^2 + (s_2-1)^2 y_2^2$。\n$\\operatorname{tr}(S\\Sigma) = \\operatorname{tr}(\\operatorname{diag}(s_1\\sigma_1^2, s_2\\sigma_2^2)) = s_1\\sigma_1^2 + s_2\\sigma_2^2$。\n$\\operatorname{tr}(\\Sigma) = \\sigma_1^2 + \\sigma_2^2$。\n将这些代入通用的 UPRE 公式，得到显式表达式：\n$$\n\\operatorname{UPRE}(s_1, s_2) = (s_1-1)^2 y_1^2 + (s_2-1)^2 y_2^2 + 2(s_1\\sigma_1^2 + s_2\\sigma_2^2) - (\\sigma_1^2 + \\sigma_2^2)\n$$\n\n### 任务 2：双参数方案优化\n\nUPRE 表达式在 $s_1$ 和 $s_2$ 上是可分的。我们可以通过独立地最小化每个分量函数来最小化它。对于 $j \\in \\{1,2\\}$，我们最小化 $f_j(s_j) = (s_j-1)^2 y_j^2 + 2s_j\\sigma_j^2$。\n对 $s_j$ 求导并令其为零：\n$$\n\\frac{df_j}{ds_j} = 2(s_j-1)y_j^2 + 2\\sigma_j^2 = 0 \\implies s_j y_j^2 - y_j^2 + \\sigma_j^2 = 0 \\implies s_j = \\frac{y_j^2 - \\sigma_j^2}{y_j^2} = 1 - \\frac{\\sigma_j^2}{y_j^2}\n$$\n约束 $\\lambda_j \\ge 0$ 意味着 $s_j = 1/(1+\\lambda_j) \\in (0, 1]$。由于 $y_j^2 > 0$ 和 $\\sigma_j^2 > 0$，无约束最小值 $s_j^\\star$ 总是小于 1。如果 $s_j^\\star \\le 0$，则在 $(0,1]$ 上的最小值位于边界，但通常收缩因子被允许在 $[0,1]$ 范围内，对应于 $\\lambda_j \\in [0, \\infty]$，在这种情况下，最优值为 $s_j^{\\star} = \\max(0, 1-\\sigma_j^2/y_j^2)$。\n给定数据：$y_1=3$，$y_2=2$，$\\sigma_1^2 = 1/4$，$\\sigma_2^2 = 1$。\n$$\ns_1^{\\star} = 1 - \\frac{1/4}{3^2} = 1 - \\frac{1}{36} = \\frac{35}{36}\n$$\n$$\ns_2^{\\star} = 1 - \\frac{1}{2^2} = 1 - \\frac{1}{4} = \\frac{3}{4}\n$$\n两个值都位于 $(0, 1]$ 内，因此它们是有效的最小值。\n相应的最优正则化参数为 $\\lambda_j^{\\star} = 1/s_j^{\\star} - 1$：\n$$\n\\lambda_1^{\\star} = \\frac{36}{35} - 1 = \\frac{1}{35}, \\quad \\lambda_2^{\\star} = \\frac{4}{3} - 1 = \\frac{1}{3}\n$$\n为了找到最小 UPRE 值，我们将 $s_j^{\\star}$ 代回 UPRE 公式。每个分量的最小值为 $f_j(s_j^{\\star}) = (\\sigma_j^4/y_j^4)y_j^2 + 2(1-\\sigma_j^2/y_j^2)\\sigma_j^2 = 2\\sigma_j^2 - \\sigma_j^4/y_j^2$。\n因此，总的最小 UPRE 为：\n$$\n\\min_{\\lambda_1, \\lambda_2} \\operatorname{UPRE}_{\\text{two}} = \\sum_{j=1}^2 \\left( 2\\sigma_j^2 - \\frac{\\sigma_j^4}{y_j^2} \\right) - (\\sigma_1^2+\\sigma_2^2) = \\sum_{j=1}^2 \\left( \\sigma_j^2 - \\frac{\\sigma_j^4}{y_j^2} \\right)\n$$\n代入数值：\n$$\n\\min \\operatorname{UPRE}_{\\text{two}} = \\left(\\frac{1}{4} - \\frac{(1/4)^2}{3^2}\\right) + \\left(1 - \\frac{1^2}{2^2}\\right) = \\left(\\frac{1}{4} - \\frac{1}{144}\\right) + \\left(1 - \\frac{1}{4}\\right) = 1 - \\frac{1}{144} = \\frac{143}{144}\n$$\n\n### 任务 3：单参数方案优化\n\n在此方案中，$s_1 = s_2 = s$。UPRE 变为：\n$$\n\\operatorname{UPRE}_{\\text{single}}(s) = (s-1)^2 y_1^2 + (s-1)^2 y_2^2 + 2s\\sigma_1^2 + 2s\\sigma_2^2 - (\\sigma_1^2+\\sigma_2^2)\n$$\n$$\n\\operatorname{UPRE}_{\\text{single}}(s) = (s-1)^2 (y_1^2+y_2^2) + 2s(\\sigma_1^2+\\sigma_2^2) - (\\sigma_1^2+\\sigma_2^2)\n$$\n对 $s$ 进行最小化：\n$$\n\\frac{d}{ds}\\operatorname{UPRE}_{\\text{single}} = 2(s-1)(y_1^2+y_2^2) + 2(\\sigma_1^2+\\sigma_2^2) = 0\n$$\n$$\ns^{\\star} = \\frac{(y_1^2+y_2^2) - (\\sigma_1^2+\\sigma_2^2)}{y_1^2+y_2^2} = 1 - \\frac{\\sigma_1^2+\\sigma_2^2}{y_1^2+y_2^2}\n$$\n数值：$y_1^2+y_2^2 = 3^2+2^2 = 13$ 和 $\\sigma_1^2+\\sigma_2^2 = 1/4+1 = 5/4$。\n$$\ns^{\\star} = 1 - \\frac{5/4}{13} = 1 - \\frac{5}{52} = \\frac{47}{52}\n$$\n该值位于 $(0, 1]$ 内。最优参数 $\\lambda^\\star$ 为：\n$$\n\\lambda^{\\star} = \\frac{1}{s^{\\star}} - 1 = \\frac{52}{47} - 1 = \\frac{5}{47}\n$$\n最小 UPRE 值为：\n$$\n\\min_{\\lambda} \\operatorname{UPRE}_{\\text{single}} = (s^{\\star}-1)^2(y_1^2+y_2^2) + 2s^{\\star}(\\sigma_1^2+\\sigma_2^2) - (\\sigma_1^2+\\sigma_2^2)\n$$\n一般的最小值为 $(\\sigma_1^2+\\sigma_2^2) - \\frac{(\\sigma_1^2+\\sigma_2^2)^2}{y_1^2+y_2^2}$。\n$$\n\\min \\operatorname{UPRE}_{\\text{single}} = \\frac{5}{4} - \\frac{(5/4)^2}{13} = \\frac{5}{4} - \\frac{25/16}{13} = \\frac{5}{4} - \\frac{25}{208} = \\frac{5 \\times 52}{208} - \\frac{25}{208} = \\frac{260 - 25}{208} = \\frac{235}{208}\n$$\n\n### 任务 4：差值 $\\Delta$ 的计算\n\n差值定义为 $\\Delta = \\big(\\min_{\\lambda} \\operatorname{UPRE}_{\\text{single}}(\\lambda)\\big) - \\big(\\min_{\\lambda_{1}, \\lambda_{2}} \\operatorname{UPRE}_{\\text{two}}(\\lambda_{1}, \\lambda_{2})\\big)$。\n使用任务 2 和 3 的结果：\n$$\n\\Delta = \\frac{235}{208} - \\frac{143}{144}\n$$\n为了减去这些分数，我们找到一个公分母。素数分解为 $208 = 2^4 \\times 13$ 和 $144 = 2^4 \\times 3^2$。\n最小公倍数为 $\\operatorname{lcm}(208, 144) = 2^4 \\times 3^2 \\times 13 = 16 \\times 9 \\times 13 = 1872$。\n$$\n\\Delta = \\frac{235 \\times 9}{1872} - \\frac{143 \\times 13}{1872}\n$$\n分子是：\n$235 \\times 9 = 2115$\n$143 \\times 13 = 1859$\n$$\n\\Delta = \\frac{2115 - 1859}{1872} = \\frac{256}{1872}\n$$\n为了简化分数，我们使用素数分解：$256 = 2^8$ 和 $1872=2^4 \\times 117$。\n$$\n\\Delta = \\frac{2^8}{2^4 \\times 117} = \\frac{2^4}{117} = \\frac{16}{117}\n$$\n这个正差值量化了使用更灵活的双参数正则化方案相对于单参数方案，在估计的预测风险方面所获得的性能增益。",
            "answer": "$$\n\\boxed{\\frac{16}{117}}\n$$"
        }
    ]
}