{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握差异原则，我们必须首先揭示其内部工作机制。奇异值分解（SVD）为分析逆问题提供了一个自然的坐标系，其中数据和解的成分可以被清晰地分离和审视。这项练习将引导你在这个基础坐标系中推导差异函数，从而直观地展示正则化方法如何通过调整所谓的“谱滤波器”来平衡数据拟合与解的稳定性。",
            "id": "3376643",
            "problem": "考虑一个有限维线性逆问题，其数据空间维度为 $m$，参数空间维度为 $n$，且已知正演算子矩阵 $A \\in \\mathbb{C}^{m \\times n}$。给定奇异值分解 (SVD)，其定义为将矩阵分解为 $A = U \\Sigma V^{*}$，其中 $U \\in \\mathbb{C}^{m \\times m}$ 和 $V \\in \\mathbb{C}^{n \\times n}$ 是酉矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是具有非负对角元的对角矩阵（可能为矩形）。将非零奇异值记为 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r}  0$，其中 $r = \\mathrm{rank}(A)$，并令 $\\{u_{i}\\}_{i=1}^{m}$ 和 $\\{v_{i}\\}_{i=1}^{n}$ 分别表示左奇异向量和右奇异向量（即 $U$ 和 $V$ 的列）。令 $y^{\\delta} \\in \\mathbb{C}^{m}$ 表示已知噪声水平 $\\delta  0$ 的含噪数据。\n\n对于每个正则化参数 $\\alpha  0$，一大类谱正则化方法通过将标量滤波因子 $g_{\\alpha}(\\sigma)$ 应用于与 $A$ 的值域相关的奇异分量，而在其正交补上保持不变，从而在数据空间中定义一个预测 $A x_{\\alpha}$。特别地，如果将数据系数写为 $c_{i} = u_{i}^{*} y^{\\delta}$（$i = 1, \\dots, m$），那么正则化预测具有如下形式\n$$\nA x_{\\alpha} = \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) \\, c_{i} \\, u_{i}.\n$$\n用于参数选择的偏差原理旨在寻找一个 $\\alpha$，使得残差范数满足\n$$\n\\| A x_{\\alpha} - y^{\\delta} \\| = \\tau \\, \\delta,\n$$\n其中 $\\tau \\geq 1$ 是一个固定的安全因子，$\\|\\cdot\\|$ 表示 $\\mathbb{C}^{m}$ 上的欧几里得范数。\n\n从以上定义和奇异向量的正交规范性出发，推导平方偏差泛函\n$$\nD(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2}\n$$\n的闭式解析表达式，该表达式应使用奇异值 $\\sigma_{i}$、数据系数 $c_{i} = u_{i}^{*} y^{\\delta}$ 和滤波因子 $g_{\\alpha}(\\sigma_{i})$ 来表示。你的最终答案必须是 $D(\\alpha)$ 的单个解析表达式；最终答案中不应包含等式或不等式。无需四舍五入，也没有物理单位。",
            "solution": "用户希望推导平方偏差泛函 $D(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2}$ 的闭式解析表达式。\n\n问题陈述，左奇异向量集 $\\{u_i\\}_{i=1}^{m}$ 是酉矩阵 $U \\in \\mathbb{C}^{m \\times m}$ 的列，这意味着它们构成了数据空间 $\\mathbb{C}^m$ 的一个正交规范基。$\\mathbb{C}^m$ 中的任何向量，例如含噪数据向量 $y^{\\delta}$，都可以表示为这些基向量的线性组合。$y^{\\delta}$ 在此基下的展开式为：\n$$\ny^{\\delta} = \\sum_{i=1}^{m} (u_{i}^{*} y^{\\delta}) u_{i}\n$$\n问题将此展开式的系数定义为 $c_{i} = u_{i}^{*} y^{\\delta}$。代入此定义，我们得到：\n$$\ny^{\\delta} = \\sum_{i=1}^{m} c_{i} u_{i}\n$$\n正则化预测 $A x_{\\alpha}$ 由下列表达式给出：\n$$\nA x_{\\alpha} = \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i}\n$$\n其中 $r = \\mathrm{rank}(A)$ 是非零奇异值的数量。\n\n偏差泛函 $D(\\alpha)$ 是残差向量 $A x_{\\alpha} - y^{\\delta}$ 的欧几里得范数的平方。现在我们可以通过代入 $A x_{\\alpha}$ 和 $y^{\\delta}$ 的表达式来写出残差向量：\n$$\nA x_{\\alpha} - y^{\\delta} = \\left( \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i} \\right) - \\left( \\sum_{i=1}^{m} c_{i} u_{i} \\right)\n$$\n为了合并各项，我们可以将第二个求和式（从 $i=1$ 到 $m$）分成两部分：一部分从 $i=1$ 到 $r$，另一部分从 $i=r+1$ 到 $m$。这得到：\n$$\n\\sum_{i=1}^{m} c_{i} u_{i} = \\sum_{i=1}^{r} c_{i} u_{i} + \\sum_{i=r+1}^{m} c_{i} u_{i}\n$$\n将其代回残差向量的表达式中，得到：\n$$\nA x_{\\alpha} - y^{\\delta} = \\left( \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i} \\right) - \\left( \\sum_{i=1}^{r} c_{i} u_{i} + \\sum_{i=r+1}^{m} c_{i} u_{i} \\right)\n$$\n现在我们可以将对应于相同基向量 $u_{i}$ 的项组合在一起：\n$$\nA x_{\\alpha} - y^{\\delta} = \\sum_{i=1}^{r} (g_{\\alpha}(\\sigma_{i}) c_{i} - c_{i}) u_{i} - \\sum_{i=r+1}^{m} c_{i} u_{i}\n$$\n从第一个求和式中提出系数 $c_{i}$，我们得到残差向量在正交规范基 $\\{u_i\\}$ 下的表示：\n$$\nA x_{\\alpha} - y^{\\delta} = \\sum_{i=1}^{r} \\big(g_{\\alpha}(\\sigma_{i}) - 1\\big) c_{i} u_{i} + \\sum_{i=r+1}^{m} (-c_{i}) u_{i}\n$$\n在一个正交规范基 $\\{u_i\\}$ 中展开的向量 $v = \\sum_{i=1}^{m} \\beta_{i} u_{i}$ 的欧几里得范数的平方，由其系数的模的平方和给出，即 $\\|v\\|^2 = \\sum_{i=1}^{m} |\\beta_{i}|^2$。这是正交规范性属性 $u_{i}^{*}u_{j} = \\delta_{ij}$ 的一个推论，被称为帕塞瓦尔恒等式。\n\n将此应用于我们的残差向量，其系数为：当 $1 \\leq i \\leq r$ 时，$\\beta_{i} = (g_{\\alpha}(\\sigma_{i}) - 1) c_{i}$；当 $r+1 \\leq i \\leq m$ 时，$\\beta_{i} = -c_{i}$。\n因此，平方偏差泛函 $D(\\alpha)$ 为：\n$$\nD(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2} = \\sum_{i=1}^{r} \\big| \\big(g_{\\alpha}(\\sigma_{i}) - 1\\big) c_{i} \\big|^{2} + \\sum_{i=r+1}^{m} |-c_{i}|^{2}\n$$\n利用复数模的性质 $|ab| = |a||b|$，并注意到 $|-c_i| = |c_i|$，我们可以将表达式简化为：\n$$\nD(\\alpha) = \\sum_{i=1}^{r} |g_{\\alpha}(\\sigma_{i}) - 1|^{2} |c_{i}|^{2} + \\sum_{i=r+1}^{m} |c_{i}|^{2}\n$$\n这就是平方偏差泛函的最终闭式解析表达式。第一项表示残差的“数据误差”分量（在 $A$ 的值域内的部分）的范数的平方，而第二项表示残差的“噪声”分量（与 $A$ 的值域正交的部分）的范数的平方。",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{r} |g_{\\alpha}(\\sigma_{i}) - 1|^{2} |c_{i}|^{2} + \\sum_{i=r+1}^{m} |c_{i}|^{2}}\n$$"
        },
        {
            "introduction": "在理解了差异原则的基本数学原理之后，我们必须检验其在更现实情况下的稳健性。差异原则的一个核心假设是我们的前向模型完全准确，但这个假设在实践中很少成立。本练习通过一个简洁而富有洞察力的思想实验，探讨了当模型存在偏差时会发生什么，你将推导并发现模型误差如何系统性地误导差异原则，从而选择出次优的正则化参数。",
            "id": "3376624",
            "problem": "考虑一个具有二维观测值和标量状态的线性逆问题。真实的正向算子是矩阵 $A \\in \\mathbb{R}^{2 \\times 1}$，由 $A = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 给出。观测噪声是加性的且有界的，一个观测值 $y_{\\delta} \\in \\mathbb{R}^{2}$ 满足 $y_{\\delta} = A x^{\\dagger} + \\eta$，其中 $x^{\\dagger} \\in \\mathbb{R}$ 是真实状态，$\\eta \\in \\mathbb{R}^{2}$ 是噪声，其已知范数为 $\\|\\eta\\|_{2} = \\delta  0$。假设噪声实现与第二个坐标对齐，使得 $\\eta = \\begin{pmatrix} 0 \\\\ \\delta \\end{pmatrix}$，因此 $y_{\\delta} = \\begin{pmatrix} x^{\\dagger} \\\\ \\delta \\end{pmatrix}$。一位数据分析师使用一个有偏的正向算子 $A + \\Delta A = \\tilde{A}$，其中 $\\tilde{A} = \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$，并带有一个固定的偏差参数 $\\varepsilon \\in [0,1)$。\n\n该分析师应用 Tikhonov 正则化，其参数 $\\alpha \\geq 0$ 定义为泛函的最小化子\n$$\nJ_{\\alpha}(x) = \\|\\tilde{A} x - y_{\\delta}\\|_{2}^{2} + \\alpha \\, \\|x\\|_{2}^{2}.\n$$\n该分析师使用 Morozov 偏差原理（也称为偏差原理）来选择 $\\alpha$，该原理规定选择 $\\alpha$ 使得数据失配等于噪声水平的预定倍数：\n$$\n\\|\\tilde{A} \\, x_{\\alpha}(y_{\\delta}) - y_{\\delta}\\|_{2} = \\tau \\, \\delta,\n$$\n其中 $x_{\\alpha}(y_{\\delta})$ 是参数 $\\alpha$ 的 Tikhonov 解，$\\tau  1$ 是一个固定的安全因子。在本问题中，取信噪比为 $x^{\\dagger}/\\delta = 2$，安全因子为 $\\tau = \\sqrt{2}$；即 $x^{\\dagger} = 2 \\delta$ 且 $\\tau^{2} = 2$。\n\n任务：\n- 从 Tikhonov 正则化的定义和最小化子 $x_{\\alpha}(y_{\\delta})$ 的正规方程出发，根据上述偏差原理条件推导出一个关于 $\\alpha$ 的显式方程。\n- 以闭合形式求解该方程，以获得由偏差原理选择的 $\\alpha$ 作为偏差 $\\varepsilon \\in [0,1)$ 的函数。\n- 以无偏情况 $\\varepsilon = 0$ 为参考，确定偏差如何改变所选的 $\\alpha$，并解释当 $\\varepsilon  0$ 时偏差原理过度正则化的机制。\n- 提出一种基于统计的诊断方法，该方法可以在数据同化设置中使用新息统计量来检测此类模型偏差，并精确说明需要监测的标量统计量及其在具有正确指定的噪声的无偏正向模型下的期望值。\n\n请为您描述的设定提供偏差原理参数 $\\alpha$ 关于 $\\varepsilon$ 的单个闭合形式表达式，不进行数值近似。不包括单位。不要四舍五入。以其精确的解析形式表示最终结果。",
            "solution": "所述问题具有科学依据、自洽且适定。它在逆问题理论中呈现了一个标准的（尽管是简化的）场景，用以说明模型误差与用于 Tikhonov 正则化中参数选择的 Morozov 偏差原理之间的相互作用。所有提供的数据和定义都是一致且充分的，足以推导出一个唯一的解。因此，我们可以着手求解。\n\n问题要求进行多项推导和解释。我们将按顺序进行处理。\n\n首先，我们推导 Tikhonov 正则化解 $x_{\\alpha}(y_{\\delta})$，为简洁起见，我们将其记为 $x_{\\alpha}$。状态 $x$ 是一个标量，因此 Tikhonov 泛函为\n$$\nJ_{\\alpha}(x) = \\|\\tilde{A} x - y_{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2} = (\\tilde{A}x - y_{\\delta})^{\\top}(\\tilde{A}x - y_{\\delta}) + \\alpha x^2\n$$\n为了找到最小化子，我们计算关于 $x$ 的导数并将其设为零：\n$$\n\\frac{dJ_{\\alpha}}{dx} = \\frac{d}{dx} \\left( x^2 (\\tilde{A}^{\\top}\\tilde{A}) - 2x(\\tilde{A}^{\\top}y_{\\delta}) + y_{\\delta}^{\\top}y_{\\delta} + \\alpha x^2 \\right) = 2x(\\tilde{A}^{\\top}\\tilde{A}) - 2(\\tilde{A}^{\\top}y_{\\delta}) + 2\\alpha x = 0\n$$\n这得出了标量情况下的正规方程：\n$$\n(\\tilde{A}^{\\top}\\tilde{A} + \\alpha)x_{\\alpha} = \\tilde{A}^{\\top}y_{\\delta}\n$$\n解是\n$$\nx_{\\alpha} = \\frac{\\tilde{A}^{\\top}y_{\\delta}}{\\tilde{A}^{\\top}\\tilde{A} + \\alpha}\n$$\n我们已知有偏正向算子 $\\tilde{A} = \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$ 和观测值 $y_{\\delta} = \\begin{pmatrix} x^{\\dagger} \\\\ \\delta \\end{pmatrix}$。当 $x^{\\dagger} = 2\\delta$ 时，我们有 $y_{\\delta} = \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = \\delta \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。让我们计算 $x_{\\alpha}$ 表达式中的各项：\n$$\n\\tilde{A}^{\\top}\\tilde{A} = \\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix} = 1 + \\varepsilon^2\n$$\n$$\n\\tilde{A}^{\\top}y_{\\delta} = \\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = 2\\delta + \\varepsilon\\delta = \\delta(2+\\varepsilon)\n$$\n将这些代入 $x_{\\alpha}$ 的表达式中，得到：\n$$\nx_{\\alpha} = \\frac{\\delta(2+\\varepsilon)}{1+\\varepsilon^2+\\alpha}\n$$\n接下来，我们使用 Morozov 偏差原理来找到一个关于 $\\alpha$ 的方程。该原理指出，选择的 $\\alpha$ 应使残差范数满足 $\\|\\tilde{A}x_{\\alpha} - y_{\\delta}\\|_{2} = \\tau\\delta$。给定 $\\tau = \\sqrt{2}$，这变为 $\\|\\tilde{A}x_{\\alpha} - y_{\\delta}\\|_{2}^2 = 2\\delta^2$。\n\n为了计算残差范数，我们可以将观测值 $y_{\\delta}$ 在 $\\tilde{A}$ 的奇异向量基中进行分解。由于 $\\tilde{A}$ 是一个 $2 \\times 1$ 矩阵，它有一个非零奇异值 $\\sigma_1 = \\|\\tilde{A}\\|_2 = \\sqrt{\\tilde{A}^{\\top}\\tilde{A}} = \\sqrt{1+\\varepsilon^2}$。对应的左奇异向量是 $u_1 = \\frac{1}{\\sigma_1}\\tilde{A} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$。另一个左奇异向量 $u_2$ 张成 $\\tilde{A}^{\\top}$ 的零空间，并且必须与 $u_1$ 正交。我们可以选择 $u_2 = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} -\\varepsilon \\\\ 1 \\end{pmatrix}$。\n\n残差向量为 $r_{\\alpha} = \\tilde{A}x_{\\alpha} - y_{\\delta}$。其范数的平方为 $\\|r_{\\alpha}\\|_{2}^2 = \\|(u_1^{\\top}r_{\\alpha})u_1 + (u_2^{\\top}r_{\\alpha})u_2\\|_{2}^2 = (u_1^{\\top}r_{\\alpha})^2 + (u_2^{\\top}r_{\\alpha})^2$。\n$u_1$ 分量是：\n$$\nu_1^{\\top}r_{\\alpha} = u_1^{\\top}(\\tilde{A}x_{\\alpha} - y_{\\delta}) = u_1^{\\top}(\\sigma_1 u_1 x_{\\alpha}) - u_1^{\\top}y_{\\delta} = \\sigma_1 x_{\\alpha} - u_1^{\\top}y_{\\delta}\n$$\n我们有 $u_1^{\\top}y_{\\delta} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}}$。\n代入 $x_{\\alpha}$：\n$$\n\\sigma_1 x_{\\alpha} - u_1^{\\top}y_{\\delta} = \\sqrt{1+\\varepsilon^2}\\frac{\\delta(2+\\varepsilon)}{1+\\varepsilon^2+\\alpha} - \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} \\left( \\frac{1+\\varepsilon^2}{1+\\varepsilon^2+\\alpha} - 1 \\right) = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} \\left( \\frac{-\\alpha}{1+\\varepsilon^2+\\alpha} \\right)\n$$\n$u_2$ 分量是：\n$$\nu_2^{\\top}r_{\\alpha} = u_2^{\\top}(\\tilde A x_{\\alpha} - y_{\\delta}) = (u_2^{\\top}\\tilde A) x_{\\alpha} - u_2^{\\top}y_{\\delta} = 0 - u_2^{\\top}y_{\\delta}\n$$\n$$\n-u_2^{\\top}y_{\\delta} = -\\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} -\\varepsilon  1 \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = -\\frac{-2\\delta\\varepsilon + \\delta}{\\sqrt{1+\\varepsilon^2}} = \\frac{\\delta(2\\varepsilon-1)}{\\sqrt{1+\\varepsilon^2}}\n$$\n残差的范数平方为：\n$$\n\\|r_{\\alpha}\\|_{2}^2 = \\left( \\frac{-\\alpha\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}(1+\\varepsilon^2+\\alpha)} \\right)^2 + \\left( \\frac{\\delta(2\\varepsilon-1)}{\\sqrt{1+\\varepsilon^2}} \\right)^2 = \\frac{\\delta^2}{1+\\varepsilon^2} \\left[ \\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} + (2\\varepsilon-1)^2 \\right]\n$$\n将其设为等于 $2\\delta^2$ 并消去 $\\delta^2$：\n$$\n\\frac{1}{1+\\varepsilon^2} \\left[ \\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} + (1-2\\varepsilon)^2 \\right] = 2\n$$\n$$\n\\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} = 2(1+\\varepsilon^2) - (1-2\\varepsilon)^2 = 2+2\\varepsilon^2 - (1-4\\varepsilon+4\\varepsilon^2) = 1+4\\varepsilon-2\\varepsilon^2\n$$\n设新变量 $X = \\frac{\\alpha}{1+\\varepsilon^2+\\alpha}$。方程变为：\n$$\nX^2 (2+\\varepsilon)^2 = 1+4\\varepsilon-2\\varepsilon^2\n$$\n由于 $\\alpha \\ge 0$ 且 $\\varepsilon \\in [0,1)$，因此 $X \\ge 0$。我们可以解出 $X$：\n$$\nX = \\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}\n$$\n（我们取正根，并注意到 $2+\\varepsilon  0$）。现在我们用 $X$ 来求解 $\\alpha$。从 $X = \\frac{\\alpha}{1+\\varepsilon^2+\\alpha}$，我们有 $X(1+\\varepsilon^2+\\alpha) = \\alpha$，这得到 $X(1+\\varepsilon^2) = \\alpha(1-X)$，因此：\n$$\n\\alpha = \\frac{X(1+\\varepsilon^2)}{1-X}\n$$\n代入 $X$ 的表达式：\n$$\n\\alpha = \\frac{\\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}(1+\\varepsilon^2)}{1 - \\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}} = \\frac{(1+\\varepsilon^2)\\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}}{\\frac{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}}\n$$\n这简化为 $\\alpha$ 的最终闭合形式表达式：\n$$\n\\alpha(\\varepsilon) = \\frac{(1+\\varepsilon^2)\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}\n$$\n这就是所求的表达式。现在我们来分析它。对于无偏情况 $\\varepsilon=0$：\n$$\n\\alpha(0) = \\frac{(1)\\sqrt{1}}{2-\\sqrt{1}} = 1\n$$\n对于 $\\varepsilon0$，$\\alpha$ 的值增加。例如，$\\alpha'(0) = 3$。一个更大的 $\\alpha$ 意味着更强的正则化（解 $x_{\\alpha}$ 更偏向于 $0$）。在存在模型偏差的情况下，偏差原理会导致过度正则化。其机制如下：总数据失配 $\\| \\tilde{A}x_\\alpha - y_\\delta \\|^2$ 由一个可通过 $\\alpha$ 调节的部分（残差在 $\\tilde{A}$ 值域上的投影）和一个不可调节的部分（残差在 $\\tilde{A}^\\top$ 零空间上的投影）组成。后一部分，$(u_2^\\top y_\\delta)^2 = \\frac{\\delta^2(1-2\\varepsilon)^2}{1+\\varepsilon^2}$，是由于模型误差造成的“不可约”失配。对于 $\\varepsilon \\in (0,1)$，这个不可约失配比 $\\varepsilon=0$ 时要小。然而，偏差原理要求一个固定的总失配 $2\\delta^2$。为了补偿较小的不可约失配，该原理迫使失配的可调节部分比无偏情况下更大。由于可调节部分是 $\\alpha$ 的单调递增函数，因此会选择一个更大的 $\\alpha$。该原理将有偏模型无法解释的部分信号误解为噪声，并增加正则化强度来抑制它。\n\n最后，我们提出一种用于模型偏差的基于统计的诊断方法。在数据同化设置中，人们监测新息向量 $d_k = y_k - \\tilde{A}x_{b,k}$，其中 $y_k$ 是时间 $k$ 的观测值，$x_{b,k}$ 是背景（预报）状态。新息的理论期望是 $E[d_k] = E[y_k - \\tilde{A}x_{b,k}] = E[Ax_{t,k} + \\eta_k - \\tilde{A}x_{b,k}]$。在无偏模型（$\\tilde{A}=A$）、无偏预报（$E[x_{b,k}] = E[x_{t,k}]$）和零均值噪声（$E[\\eta_k]=0$）的原假设下，期望新息为零：$E[d_k] = A E[x_{t,k}]-A E[x_{b,k}] = 0$。\n如果模型有偏，$E[d_k] = (A-\\tilde{A})E[x_{t,k}] = -(\\Delta A)E[x_{t,k}]$。持续的非零均值新息是模型偏差的一个迹象。\n要监测的具体标量统计量将是时间平均新息向量的一个分量。考虑到本问题中偏差的结构 $\\Delta A = \\begin{pmatrix} 0 \\\\ \\varepsilon \\end{pmatrix}$，偏差只影响第二个分量。\n因此，需要监测的标量统计量是 $S = \\text{mean}_{k}\\left((d_k)_2\\right) = \\frac{1}{N}\\sum_{k=1}^{N}(y_k - \\tilde{A}x_{b,k})_2$。\n在无偏正向模型（$\\tilde{A}=A=\\begin{pmatrix}1\\\\0\\end{pmatrix}$）和正确指定的零均值噪声下，该统计量的期望值为：\n$$E[S] = E[(y_k - Ax_{b,k})_2] = E[(Ax_{t,k} + \\eta_k - Ax_{b,k})_2]$$\n由于 $Ax$ 的第二个分量总是 $0$，这变为 $E[(\\eta_k)_2]$。在正确指定的零均值噪声下，此期望值为 $0$。$S$ 值在统计上显著偏离 $0$ 将表明正向模型的第二个分量可能存在偏差。",
            "answer": "$$\n\\boxed{\\frac{(1+\\varepsilon^2)\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}}\n$$"
        },
        {
            "introduction": "本章的最后一个练习将我们从静态问题带入动态、时序演化的世界，这在数据同化等领域中至关重要。你将面临一个编程挑战：在一个动态系统中顺序地应用差异原则，其中每一时刻的状态都依赖于前一时刻。这项实践将理论与代码相结合，要求你实现一个完整的算法来追踪一个不断变化的状态，从而让你对差异原则在实际应用中的性能和计算策略有更深刻的理解。",
            "id": "3376673",
            "problem": "考虑一个数据同化中的线性逆问题，其中包含高斯噪声：观测模型为 $y = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$y \\in \\mathbb{R}^{m}$，并且 $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$，其中 $ \\Gamma \\in \\mathbb{R}^{m \\times m}$ 是一个对称正定协方差矩阵。一个标准的Tikhonov正则化估计被定义为二次准则\n$$\nJ_{\\alpha}(x) = \\left\\| \\Gamma^{-1/2} (A x - y) \\right\\|_2^2 + \\alpha \\left\\| R x \\right\\|_2^2\n$$\n的最小化子 $x_{\\alpha} \\in \\mathbb{R}^{n}$，其中 $R \\in \\mathbb{R}^{p \\times n}$ 编码了正则化（例如，离散导数算子或单位矩阵），$\\alpha  0$ 是正则化参数。偏差原理选择 $\\alpha$ 使得数据失配与噪声水平相匹配，即寻找满足\n$$\n\\phi(\\alpha) := \\left\\| \\Gamma^{-1/2} \\big(A x_{\\alpha} - y\\big) \\right\\|_2 = \\tau \\delta\n$$\n的 $\\alpha$，其中 $\\delta^2 = \\mathbb{E}\\left[\\left\\|\\Gamma^{-1/2} \\varepsilon \\right\\|_2^2\\right]$ 且 $\\tau \\ge 1$ 是一个安全因子。在实践中，对给定的 $\\alpha$ 求值 $\\phi(\\alpha)$ 需要计算 $x_{\\alpha}$，对于大规模问题，这通过求解 $J_{\\alpha}$ 的一阶最优性条件来完成，即求解对称正定线性系统\n$$\n\\big(A^{\\top} \\Gamma^{-1} A + \\alpha R^{\\top} R \\big) x_{\\alpha} = A^{\\top} \\Gamma^{-1} y.\n$$\n假设 $m$ 和 $n$ 很大，$A$ 是稀疏的，并且与 $A$ 和 $A^{\\top}$ 的矩阵向量乘积占主导计算成本。您打算在一组候选参数 $\\{\\alpha_k\\}_{k=1}^{N_{\\alpha}}$ 上进行区间限定和搜索以满足偏差原理，并希望减少在 $k = 1, \\dots, N_{\\alpha}$ 上求值 $\\phi(\\alpha_k)$ 的总成本。\n\n以下哪些陈述正确描述了在大型问题中，评估 $\\phi(\\alpha)$ 的计算成本考虑以及跨不同候选 $\\alpha$ 重用分解或Krylov子空间的有效策略？\n\nA. 在 $\\Gamma = I$ 和 $R = I$ 的特殊情况下，线性系统仅因在 $A^{\\top} A$ 上添加了一个标量位移 $\\alpha I$ 而不同。多位移共轭梯度法可以重用由 $A^{\\top} A$ 生成的单个Krylov子空间，以同时推进多个 $\\alpha$ 的求解，从而为所有位移提供每次迭代的更新，其额外开销相对于一次与 $A$ 的矩阵向量乘积和一次与 $A^{\\top}$ 的矩阵向量乘积而言是适度的。\n\nB. 使用一次 Golub–Kahan双对角化为 $A$ 构建一个投影子空间（如在混合最小二乘QR或混合LSQR中），可以为多个 $\\alpha$ 求解小的投影Tikhonov问题以获得 $x_{\\alpha}$ 并计算 $\\phi(\\alpha)$，而无需额外的与 $A$ 或 $A^{\\top}$ 的乘法。这在不同的 $\\alpha$ 之间重用了相同的双对角化，并将对 $\\alpha$ 的依赖性大部分转移到对小矩阵的廉价操作上。\n\nC. $\\big(A^{\\top} \\Gamma^{-1} A + \\alpha_0 R^{\\top} R \\big)$ 的一个稀疏Cholesky分解可以通过 Sherman–Morrison–Woodbury 更新被任何其他的 $\\alpha$ 重用，因为 $\\big(\\alpha - \\alpha_0\\big) R^{\\top} R$ 是一个低秩扰动。因此，一旦单个分解可用，为新的 $\\alpha$ 求值 $\\phi(\\alpha)$ 的边际成本就可以忽略不计。\n\nD. 预计算矩阵对 $\\big(A, R\\big)$ 的广义奇异值分解可将Tikhonov正则化对角化，从而允许对于任何 $\\alpha$，在广义奇异值坐标中通过逐元素公式来评估 $\\phi(\\alpha)$。对于大规模问题，完全的广义奇异值分解通常成本过高，但随机化或迭代的部分谱分解可以近似这种对角化，并在多个 $\\alpha$ 之间重用，从而大幅降低评估 $\\phi(\\alpha)$ 的成本。\n\nE. 应用牛顿法求解标量方程 $\\phi(\\alpha) = \\tau \\delta$ 可以以可忽略的每个 $\\alpha$ 的增量成本进行，因为无论 $R$ 和 $\\Gamma$ 如何，导数 $\\phi'(\\alpha)$ 都可以从Krylov递推中自动获得，无需任何额外的线性求解。\n\nF. 一种从较大的 $\\alpha$ 开始并逐渐减小 $\\alpha$ 的延拓方案，通过使用前一个 $x_{\\alpha}$ 来热启动每一次迭代求解，往往会减少每个 $\\alpha$ 的迭代次数，因为在二次Tikhonov问题中，解路径 $x_{\\alpha}$ 随 $\\alpha$ 平滑变化，从而减少了整个搜索过程中与 $A$ 和 $A^{\\top}$ 的总矩阵向量乘积次数。\n\n选择所有正确的陈述。",
            "solution": "问题陈述的严格评估如下。\n\n**步骤1：提取已知条件**\n- **问题类型**：数据同化中的线性逆问题。\n- **观测模型**：$y = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$y \\in \\mathbb{R}^{m}$。\n- **噪声模型**：$\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$，其中 $\\Gamma \\in \\mathbb{R}^{m \\times m}$ 是对称正定协方差矩阵。\n- **Tikhonov泛函**：$J_{\\alpha}(x) = \\left\\| \\Gamma^{-1/2} (A x - y) \\right\\|_2^2 + \\alpha \\left\\| R x \\right\\|_2^2$。\n- **正则化**：$R \\in \\mathbb{R}^{p \\times n}$ 是正则化矩阵，$\\alpha  0$ 是正则化参数。\n- **解**：$x_{\\alpha}$ 是 $J_{\\alpha}(x)$ 的最小化子。\n- **偏差原理**：选择 $\\alpha$ 以满足 $\\phi(\\alpha) := \\left\\| \\Gamma^{-1/2} \\big(A x_{\\alpha} - y\\big) \\right\\|_2 = \\tau \\delta$。\n- **噪声水平**：$\\delta^2 = \\mathbb{E}\\left[\\left\\|\\Gamma^{-1/2} \\varepsilon \\right\\|_2^2\\right]$。安全因子为 $\\tau \\ge 1$。\n- **正规方程**：$\\big(A^{\\top} \\Gamma^{-1} A + \\alpha R^{\\top} R \\big) x_{\\alpha} = A^{\\top} \\Gamma^{-1} y$。\n- **计算背景**：$m$ 和 $n$ 很大，$A$ 是稀疏的，与 $A$ 和 $A^{\\top}$ 的矩阵向量乘积是主要的计算成本。\n- **目标**：降低为一组候选参数 $\\{\\alpha_k\\}_{k=1}^{N_{\\alpha}}$ 求值 $\\phi(\\alpha_k)$ 的总成本。\n\n**步骤2：使用提取的已知条件进行验证**\n问题陈述是一个有效的科学问题。\n- **科学依据**：该公式是线性逆问题中Tikhonov正则化的典型表示，是计算科学与工程中一种基础且广泛使用的技术。使用通用协方差矩阵 $\\Gamma$、通用正则化算子 $R$、Tikhonov泛函的定义、由此产生的正规方程以及用于参数选择的偏差原理，都是标准且数学上正确的。\n- **适定性**：问题是适定的。对于任何 $\\alpha  0$，Tikhonov解 $x_{\\alpha}$ 的存在性和唯一性在通常的假设下得到保证，即 $A$ 和 $R$ 的零空间仅在原点相交，这使得矩阵 $A^{\\top} \\Gamma^{-1} A + \\alpha R^{\\top} R$ 对称正定。提出的问题不是求解一个具体实例，而是评估关于既定计算策略陈述的正确性，这是一个清晰且可回答的任务。\n- **客观性**：问题使用精确、客观和标准的数学术语陈述。没有歧义、主观性或非科学内容。\n\n所有验证标准均已满足。该问题是关于大规模逆问题的数值线性代数中的一个标准练习。\n\n**步骤3：结论与行动**\n问题有效。解决方案通过分析每个选项来进行。\n\n核心任务是高效地求解多个 $\\alpha$ 值的正规方程：\n$$\n\\big(A^{\\top} \\Gamma^{-1} A + \\alpha R^{\\top} R \\big) x_{\\alpha} = A^{\\top} \\Gamma^{-1} y.\n$$\n让我们定义 $\\tilde{A} = \\Gamma^{-1/2} A$ 和 $\\tilde{y} = \\Gamma^{-1/2} y$。方程变为：\n$$\n\\big(\\tilde{A}^{\\top} \\tilde{A} + \\alpha R^{\\top} R \\big) x_{\\alpha} = \\tilde{A}^{\\top} \\tilde{y}.\n$$\n这些是最小化 $J_{\\alpha}(x) = \\|\\tilde{A}x - \\tilde{y}\\|_2^2 + \\alpha \\|Rx\\|_2^2$ 的正规方程。偏差函数是 $\\phi(\\alpha) = \\|\\tilde{A}x_{\\alpha} - \\tilde{y}\\|_2$。\n\n**逐选项分析**\n\n**A. 在 $\\Gamma = I$ 和 $R = I$ 的特殊情况下，线性系统仅因在 $A^{\\top} A$ 上添加了一个标量位移 $\\alpha I$ 而不同。多位移共轭梯度法可以重用由 $A^{\\top} A$ 生成的单个Krylov子空间，以同时推进多个 $\\alpha$ 的求解，从而为所有位移提供每次迭代的更新，其额外开销相对于一次与 $A$ 的矩阵向量乘积和一次与 $A^{\\top}$ 的矩阵向量乘积而言是适度的。**\n\n当 $\\Gamma=I$ 和 $R=I$ 时，正规方程变为 $\\big(A^{\\top} A + \\alpha I \\big) x_{\\alpha} = A^{\\top} y$。这是一组形式为 $(M + \\alpha_k I) x_{\\alpha_k} = b$ 的线性系统，其中 $M = A^{\\top} A$ 和 $b = A^{\\top} y$。此类系统被称为位移线性系统。多位移共轭梯度(CG)算法专为此情况设计。它为参考系统（例如，使用 $\\alpha = 0$）构建单个Krylov子空间 $\\mathcal{K}_j(M, r_0)$，然后使用短递推关系在此同一子空间内高效地更新所有其他位移 $\\alpha_k$ 的解。每次迭代 $j$ 的主要成本是扩展Krylov基所需的单个矩阵向量乘积，这涉及一次与 $M = A^{\\top}A$ 的乘积，因此需要一次与 $A$ 的乘积和一次与 $A^{\\top}$ 的乘积。更新 $N_{\\alpha}$ 个位移的解的成本很小，通常与 $j \\cdot N_{\\alpha}$ 成比例。该陈述准确地描述了多位移CG的原理和优点。\n\n**结论：正确**\n\n**B. 使用一次 Golub–Kahan双对角化为 $A$ 构建一个投影子空间（如在混合最小二乘QR或混合LSQR中），可以为多个 $\\alpha$ 求解小的投影Tikhonov问题以获得 $x_{\\alpha}$ 并计算 $\\phi(\\alpha)$，而无需额外的与 $A$ 或 $A^{\\top}$ 的乘法。这在不同的 $\\alpha$ 之间重用了相同的双对角化，并将对 $\\alpha$ 的依赖性大部分转移到对小矩阵的廉价操作上。**\n\n这描述了一类“混合”或投影方法。对 $\\tilde{A}$ 应用 $k$ 步 Golub-Kahan双对角化(GKB)过程，产生正交矩阵 $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ 和 $V_k \\in \\mathbb{R}^{n \\times k}$，使得 $\\tilde{A} V_k = U_{k+1} B_k$，其中 $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ 是双对角的。这需要 $k$ 次与 $\\tilde{A}$ 和 $\\tilde{A}^{\\top}$ 的矩阵向量乘积。然后，解在由 $V_k$ 的列张成的子空间中近似，即 $x_{\\alpha} \\approx V_k z_k$。原始的大型Tikhonov问题被投影到一个小问题上：$\\min_{z_k} \\|B_k z_k - \\|\\tilde{y}\\|_2 e_1\\|_2^2 + \\alpha \\|R V_k z_k\\|_2^2$。一旦完成了 $k$ 步GKB并形成了矩阵 $V_k$ 和 $B_k$，就可以用可忽略的成本为许多不同的 $\\alpha$ 值求解这个小的（$k \\times k$）Tikhonov问题，因为它只涉及小矩阵。测试这些不同的 $\\alpha$ 值不需要进一步的与 $A$ 或 $A^{\\top}$ 的矩阵向量乘积。这是高效计算参数选择函数（如偏差原理或L曲线）的一种主要方法。该陈述是对这一广泛使用策略的精确描述。\n\n**结论：正确**\n\n**C. $\\big(A^{\\top} \\Gamma^{-1} A + \\alpha_0 R^{\\top} R \\big)$ 的一个稀疏Cholesky分解可以通过 Sherman–Morrison–Woodbury 更新被任何其他的 $\\alpha$ 重用，因为 $\\big(\\alpha - \\alpha_0\\big) R^{\\top} R$ 是一个低秩扰动。因此，一旦单个分解可用，为新的 $\\alpha$ 求值 $\\phi(\\alpha)$ 的边际成本就可以忽略不计。**\n\nSherman-Morrison-Woodbury (SMW) 公式允许在低秩扰动后高效地更新矩阵的逆。这里的扰动是 $U = (\\alpha - \\alpha_0) R^{\\top} R$。为使SMW公式在计算上高效，此扰动的秩必须相对于矩阵的维度（$n$）要小。$R^{\\top}R$ 的秩等于 $R$ 的秩，最多为 $p$，$R$ 的行数。在许多常见的正则化场景中，$p$ 并不小。例如，对于标准Tikhonov正则化（$R = I$），$p = n$，秩为 $n$。对于离散导数算子，$p$ 与 $n$ 的数量级相同。在这些情况下，扰动是满秩或接近满秩的，而不是低秩的。核心前提，即 $(\\alpha - \\alpha_0) R^{\\top} R$ 是一个低秩扰动，对于大规模设置中的通用Tikhonov正则化是不正确的。因此，通过这种方法重用Cholesky分解通常是不可行的。\n\n**结论：错误**\n\n**D. 预计算矩阵对 $\\big(A, R\\big)$ 的广义奇异值分解可将Tikhonov正则化对角化，从而允许对于任何 $\\alpha$，在广义奇异值坐标中通过逐元素公式来评估 $\\phi(\\alpha)$。对于大规模问题，完全的广义奇异值分解通常成本过高，但随机化或迭代的部分谱分解可以近似这种对角化，并在多个 $\\alpha$ 之间重用，从而大幅降低评估 $\\phi(\\alpha)$ 的成本。**\n\n矩阵对 $(\\tilde{A}, R)$ 的广义奇异值分解 (GSVD) 是分析Tikhonov正则化的基本工具。它同时对角化了最小二乘项和正则化项，从而为任何 $\\alpha$ 值的解和相关量（如偏差范数 $\\phi(\\alpha)$）提供了简单的标量“滤波器因子”公式。陈述的第一部分是对这一理论的完美描述。正如正确指出的，对于大规模问题，计算完整的GSVD在计算上是不可行的。然而，所提出的确切策略是一种现代且有效的方法：使用迭代或随机方法计算部分GSVD，这将问题投影到由主要的广义奇异向量张成的子空间上。这类似于使用截断的SVD。这个部分分解以高成本计算一次，然后被重用于非常快速地评估许多测试 $\\alpha$ 值的 $\\phi(\\alpha)$。该陈述正确地指出了其 underlying 理论及其实用的大规模应用调整。\n\n**结论：正确**\n\n**E. 应用牛顿法求解标量方程 $\\phi(\\alpha) = \\tau \\delta$ 可以以可忽略的每个 $\\alpha$ 的增量成本进行，因为无论 $R$ 和 $\\Gamma$ 如何，导数 $\\phi'(\\alpha)$ 都可以从Krylov递推中自动获得，无需任何额外的线性求解。**\n\n应用牛顿法求解 $f(\\alpha) = \\phi(\\alpha) - \\tau\\delta = 0$ 需要导数 $\\phi'(\\alpha)$。$\\phi'(\\alpha)$ 的解析表达式涉及项 $dx_{\\alpha}/d\\alpha$，该项通过对正规方程求导得到。这产生了另一个需要求解的线性系统：$(\\tilde{A}^{\\top} \\tilde{A} + \\alpha R^{\\top} R) (dx_{\\alpha}/d\\alpha) = -R^{\\top} R x_{\\alpha}$。通常，计算导数需要求解一个具有相同矩阵但不同右侧项的附加线性系统。这*不是*免费的。该陈述声称 $\\phi'(\\alpha)$ “可以从Krylov递推中自动获得，无需任何额外的线性求解”，这是*特定算法*的一个特性，即选项B和D中描述的投影方法。在固定的投影子空间内，投影问题的导数确实可以非常廉价地计算。然而，该陈述是一个过度概括。它并不适用于任何任意的迭代求解器（例如，对每个 $\\alpha$ 进行“暴力”CG求解），并且它含蓄地假设了投影框架的使用。声称“无论 $R$ 和 $\\Gamma$ 如何”也具有误导性；虽然数学结构成立，但效率取决于形成和使用投影算子的能力。因此，所写的陈述并非普遍正确。\n\n**结论：错误**\n\n**F. 一种从较大的 $\\alpha$ 开始并逐渐减小 $\\alpha$ 的延拓方案，通过使用前一个 $x_{\\alpha}$ 来热启动每一次迭代求解，往往会减少每个 $\\alpha$ 的迭代次数，因为在二次Tikhonov问题中，解路径 $x_{\\alpha}$ 随 $\\alpha$ 平滑变化，从而减少了整个搜索过程中与 $A$ 和 $A^{\\top}$ 的总矩阵向量乘积次数。**\n\n对于 $\\alpha  0$，Tikhonov正则化解 $x_{\\alpha}$ 是 $\\alpha$ 的一个解析函数。这意味着解路径 $\\alpha \\mapsto x_{\\alpha}$ 是平滑的。因此，如果 $\\alpha_{k+1}$ 接近 $\\alpha_k$，那么解 $x_{\\alpha_k}$ 将是计算 $x_{\\alpha_{k+1}}$ 的迭代求解器的一个非常好的初始猜测。与每次都从零向量开始相比，这种“热启动”策略显著减少了像CG这样的迭代方法收敛所需的迭代次数。从一个大的 $\\alpha$ 开始是有益的，因为问题是良态的，解接近于 $R$ 的零空间（通常接近 $0$），使得初始求解很容易。逐渐减小 $\\alpha$ 并进行热启动利用了解的平滑演变。这种启发式方法是标准实践，并且在减少总矩阵向量乘积次数方面非常有效。所提供的推理是合理的。\n\n**结论：正确**",
            "answer": "$$\\boxed{ABDF}$$"
        }
    ]
}