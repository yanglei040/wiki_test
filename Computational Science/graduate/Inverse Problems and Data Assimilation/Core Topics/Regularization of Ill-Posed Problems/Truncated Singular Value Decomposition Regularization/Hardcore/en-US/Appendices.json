{
    "hands_on_practices": [
        {
            "introduction": "The choice of the regularization parameter—in this case, the truncation level $k$—is the central challenge in applying TSVD. This exercise provides a hands-on exploration of the fundamental trade-off between capturing the true signal and amplifying noise. By tracking the solution's coefficients as more singular components are included, you will implement an algorithmic rule to detect the \"tipping point\" where noise begins to dominate the solution, building crucial intuition for parameter selection .",
            "id": "3201000",
            "problem": "You are given a family of linear inverse problems modeled as $A x \\approx b$, where $A \\in \\mathbb{R}^{m \\times n}$ is ill-conditioned, and $b \\in \\mathbb{R}^{m}$ may be contaminated by additive noise. Consider using truncated singular value decomposition (truncated SVD) as a regularization strategy. The singular value decomposition of $A$ is defined as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has nonnegative diagonal entries (the singular values) $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$. For a truncation level $k \\in \\{1,2,\\dots,n\\}$, define the truncated subspace $\\mathcal{S}_k = \\mathrm{span}\\{v_1,\\dots,v_k\\}$, where $v_i$ are the right singular vectors (columns of $V$), and let $x_k$ be the element of $\\mathcal{S}_k$ that minimizes the residual norm $\\|A x - b\\|_2$ over $x \\in \\mathcal{S}_k$.\n\nYour task is to analyze the path $x_k$ as $k$ increases, specifically focusing on when components along $v_i$ with large coefficients enter the solution. The coefficient of $v_i$ in $x_k$ is the $i$-th entry of the expansion of $x_k$ in the basis $\\{v_i\\}$. For each $i$, define the coefficient magnitude sequence to be the absolute values of these coefficients as $k$ grows. In practice, when $b$ contains noise, coefficients corresponding to small singular values can become large because the residual component aligns with left singular vectors and is amplified by $1/\\sigma_i$. This phenomenon is linked to the onset of instability and is often visible as “coefficient spikes” in the sequence of coefficients.\n\nDefine a spike detection rule in purely algorithmic terms as follows. Let $c_i$ denote the coefficient associated with $v_i$ that would appear upon extending the truncation from $k=i-1$ to $k=i$ (so $c_i$ is the coefficient multiplying $v_i$ in $x_i$). Let $p=5$, $\\alpha=3$, and $\\beta=2$. Let the baseline be the mean of the first $p$ absolute coefficients, that is, $\\mathrm{baseline} = \\frac{1}{p} \\sum_{i=1}^{p} |c_i|$. A spike is declared at the first index $i>p$ such that both $|c_i| > \\alpha \\cdot \\mathrm{baseline}$ and $|c_i| > \\beta \\cdot |c_{i-1}|$. If no such index exists, report $-1$.\n\nStarting only from the definitions of singular value decomposition and least squares minimization restricted to a subspace, your program must:\n- Derive the form of the coefficients $c_i$ that enter $x_k$ as $k$ increases by solving the constrained least squares problem over $\\mathcal{S}_k$ using orthogonality properties of $U$ and $V$.\n- Implement the above spike detection rule to determine the first spike index $k_{\\star}$.\n\nTest Suite. Implement the following $3$ test cases, each fully specified by deterministic pseudorandom construction. In every case, singular values are strictly decreasing and generated from a schedule that ensures ill-conditioning. Construct $A$ via an explicit singular value decomposition as follows: generate $U$ and $V$ as independent Haar-like orthogonal matrices by applying a QR factorization to standard normal matrices with a given random seed; define $\\Sigma$ to have the specified singular values on its diagonal; then set $A = U_{[:,1:n]} \\Sigma V^{\\top}$. Construct the ground truth in the right singular vector basis by specifying coefficients $\\hat{x}_i$ and setting $x_{\\mathrm{true}} = V \\hat{x}$, with $\\hat{x} = (\\hat{x}_1,\\dots,\\hat{x}_n)^{\\top}$. Then set $b = A x_{\\mathrm{true}} + \\varepsilon$, where the noise vector $\\varepsilon$ has independent normal entries with zero mean and standard deviation $\\sigma_{\\mathrm{noise}}$; define $\\sigma_{\\mathrm{noise}} = \\mathrm{level} \\cdot \\|A x_{\\mathrm{true}}\\|_2 / \\sqrt{m}$, so the noise scales relative to the clean data. Angles are not used in this problem. No physical units are involved.\n\n- Case $1$ (happy path): $m=50$, $n=40$, random seed $1234$, singular values $\\sigma_i = 10^{-(i-1)/8}$ for $i=1,\\dots,n$, coefficients $\\hat{x}_i = \\exp(-(i-1)/7)$ for $i=1,\\dots,n$, noise level $\\mathrm{level} = 10^{-3}$.\n- Case $2$ (boundary, no noise): $m=50$, $n=40$, random seed $2021$, singular values $\\sigma_i = 10^{-(i-1)/8}$, coefficients $\\hat{x}_i = \\exp(-(i-1)/7)$, noise level $\\mathrm{level} = 0$.\n- Case $3$ (edge, stronger ill-conditioning and more noise): $m=80$, $n=60$, random seed $999$, singular values $\\sigma_i = 10^{-(i-1)/6}$, coefficients $\\hat{x}_i = \\exp(-(i-1)/10)$, noise level $\\mathrm{level} = 2 \\cdot 10^{-2}$.\n\nFor each case, compute the sequence of coefficients $c_i$ associated with the incremental inclusion of $v_i$ and apply the spike detection rule with $p=5$, $\\alpha=3$, $\\beta=2$. The required output for each case is the integer index $k_{\\star}$ (using one-based indexing for $i$) at which the first spike occurs; if no spike is detected, output $-1$ for that case.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three cases in order, for example $[k_1,k_2,k_3]$. No additional text should be printed.",
            "solution": "The posed problem is valid. It is a well-defined task in computational science, grounded in the established theory of linear inverse problems and regularization using truncated singular value decomposition (TSVD). The problem is scientifically sound, self-contained, objective, and provides all necessary data and algorithmic definitions for a unique solution.\n\nThe core of the problem is to determine the coefficients of the TSVD solution and then apply a specific algorithmic rule to detect noise-induced instability. We begin by deriving the formula for these coefficients from first principles.\n\nThe problem is to find the solution $x_k$ that minimizes the residual norm for a given truncation level $k$. The solution $x_k$ is constrained to the subspace $\\mathcal{S}_k = \\mathrm{span}\\{v_1, \\dots, v_k\\}$, where $v_i$ are the right singular vectors of the matrix $A$.\n$$\nx_k = \\arg\\min_{x \\in \\mathcal{S}_k} \\|A x - b\\|_2^2\n$$\nAny vector $x \\in \\mathcal{S}_k$ can be uniquely represented as a linear combination of the orthonormal basis vectors $\\{v_1, \\dots, v_k\\}$:\n$$\nx = \\sum_{i=1}^k c_i v_i\n$$\nwhere $c_i$ are the coefficients we need to determine. Substituting this representation into the objective function, we seek to minimize:\n$$\n\\left\\| A \\left(\\sum_{i=1}^k c_i v_i\\right) - b \\right\\|_2^2 = \\left\\| \\left(\\sum_{i=1}^k c_i A v_i\\right) - b \\right\\|_2^2\n$$\nFrom the definition of the singular value decomposition, $A = U \\Sigma V^{\\top}$, we have the fundamental relationship $A v_i = \\sigma_i u_i$ for $i=1, \\dots, n$, where $u_i$ are the left singular vectors and $\\sigma_i$ are the singular values. Substituting this into the expression gives:\n$$\n\\left\\| \\left(\\sum_{i=1}^k c_i \\sigma_i u_i\\right) - b \\right\\|_2^2\n$$\nThe left singular vectors $\\{u_i\\}_{i=1}^m$ form an orthonormal basis for $\\mathbb{R}^m$. The squared Euclidean norm of a vector is the sum of the squares of its coordinates in any orthonormal basis. We can express the vector inside the norm, $r = (\\sum_{i=1}^k c_i \\sigma_i u_i) - b$, in the basis of left singular vectors. The coordinate of $r$ along $u_j$ is $u_j^{\\top} r$.\nThe squared norm is therefore:\n$$\n\\|r\\|_2^2 = \\sum_{j=1}^m (u_j^{\\top} r)^2 = \\sum_{j=1}^m \\left( u_j^{\\top} \\left( \\sum_{i=1}^k c_i \\sigma_i u_i \\right) - u_j^{\\top} b \\right)^2\n$$\nUsing the orthonormality property $u_j^{\\top} u_i = \\delta_{ij}$ (the Kronecker delta), the expression simplifies.\nFor $j \\in \\{1, \\dots, k\\}$, the term $u_j^{\\top} (\\sum_{i=1}^k c_i \\sigma_i u_i)$ becomes $c_j \\sigma_j$.\nFor $j \\in \\{k+1, \\dots, m\\}$, the term $u_j^{\\top} (\\sum_{i=1}^k c_i \\sigma_i u_i)$ becomes $0$.\nThis allows us to split the sum over $j$:\n$$\n\\|r\\|_2^2 = \\sum_{j=1}^k (c_j \\sigma_j - u_j^{\\top} b)^2 + \\sum_{j=k+1}^m (- u_j^{\\top} b)^2\n$$\nTo minimize this expression with respect to the coefficients $\\{c_1, \\dots, c_k\\}$, we only need to consider the first sum, as the second sum is independent of these coefficients. The first sum is a sum of non-negative terms. The minimum value of this sum, which is $0$, is achieved when each term is individually zero. Therefore, for each $j \\in \\{1, \\dots, k\\}$, we must have:\n$$\nc_j \\sigma_j - u_j^{\\top} b = 0\n$$\nSolving for $c_j$ yields the well-known formula for the coefficients of the TSVD solution:\n$$\nc_j = \\frac{u_j^{\\top} b}{\\sigma_j}\n$$\nThe problem defines $c_i$ as the coefficient that enters the solution as the truncation level is increased from $i-1$ to $i$. Our derivation shows that the coefficient for the basis vector $v_i$ is given by the formula above and does not depend on the overall truncation level $k$ (as long as $k \\ge i$). Thus, the sequence of coefficients to be analyzed is simply $\\{c_i\\}_{i=1}^n$.\n\nWith the form of the coefficients established, the rest of the problem is algorithmic.\n1.  For each test case, we construct the matrix $A$ from its specified singular value decomposition. This involves generating random orthogonal matrices $U$ and $V$ and a diagonal matrix $\\Sigma$ of specified singular values. The matrix $A$ is formed as $A = U_{econ} \\Sigma_{n \\times n} V^{\\top}$, where $U_{econ}$ consists of the first $n$ columns of $U$.\n2.  The \"true\" solution $x_{\\mathrm{true}}$ is constructed in the basis of right singular vectors, and the \"clean\" data vector is computed as $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$.\n3.  Noise is added to obtain the final data vector $b = b_{\\mathrm{clean}} + \\varepsilon$, where the noise standard deviation is scaled relative to the norm of the clean signal.\n4.  The coefficients $c_i$ are calculated for $i=1, \\dots, n$ using the derived formula $c_i = (u_i^{\\top} b)/\\sigma_i$.\n5.  The specified spike detection rule is applied to the sequence of absolute coefficients $|c_i|$. With parameters $p=5$, $\\alpha=3$, and $\\beta=2$, we compute a baseline from the first $p$ coefficients: $\\mathrm{baseline} = \\frac{1}{p} \\sum_{i=1}^{p} |c_i|$. We then search for the first index $i > p$ where both $|c_i| > \\alpha \\cdot \\mathrm{baseline}$ and $|c_i| > \\beta \\cdot |c_{i-1}|$. The $1$-based index of the first occurrence is the result. If no such index is found, the result is $-1$.\n\nThis procedure is implemented for each of the three test cases specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef run_case(m, n, seed, sigma_denominator, x_hat_denominator, level, p, alpha, beta):\n    \"\"\"\n    Runs a single test case for spike detection in TSVD.\n\n    Args:\n        m (int): Number of rows for matrix A.\n        n (int): Number of columns for matrix A.\n        seed (int): Random seed for reproducibility.\n        sigma_denominator (float): Denominator in the exponent for singular values.\n        x_hat_denominator (float): Denominator in the exponent for true coefficients.\n        level (float): Relative noise level.\n        p (int): Number of initial coefficients for baseline calculation.\n        alpha (float): Multiplier for the baseline threshold.\n        beta (float): Multiplier for the previous coefficient threshold.\n\n    Returns:\n        int: The 1-based index of the first detected spike, or -1 if no spike is found.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct matrices U, Sigma, V\n    # Generate Haar-like orthogonal matrices from QR of standard normal matrices\n    U_full, _ = np.linalg.qr(rng.standard_normal((m, m)))\n    V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n    \n    # Use the economy-size U, which has orthonormal columns\n    U_econ = U_full[:, :n]\n\n    # Singular values (sigma_i = 10**(-(i-1)/C))\n    indices = np.arange(1, n + 1)\n    s_vals = 10.0**(-(indices - 1) / sigma_denominator)\n    Sigma_n = np.diag(s_vals)\n\n    # 2. Construct A, x_true, and b\n    A = U_econ @ Sigma_n @ V.T\n\n    # True solution coefficients in the V basis (x_hat_i = exp(-(i-1)/C))\n    x_hat = np.exp(-(indices - 1) / x_hat_denominator)\n    \n    # True solution vector\n    x_true = V @ x_hat\n\n    # Clean data vector b\n    b_clean = A @ x_true\n\n    # Additive noise\n    if level > 0.0:\n        norm_b_clean = np.linalg.norm(b_clean)\n        sigma_noise = level * norm_b_clean / np.sqrt(m)\n        noise = rng.normal(0.0, sigma_noise, size=m)\n        b = b_clean + noise\n    else:\n        b = b_clean\n\n    # 3. Compute TSVD solution coefficients\n    # c_i = (u_i^T b) / sigma_i\n    uT_b = U_econ.T @ b\n    c = uT_b / s_vals\n    c_abs = np.abs(c)\n\n    # 4. Apply spike detection rule\n    if n <= p:\n        return -1\n\n    # Baseline using the first p coefficients (0-indexed to p-1)\n    baseline = np.mean(c_abs[0:p])\n\n    # Search for spike for indices i > p (1-based), which is i >= p (0-based)\n    for i in range(p, n):\n        cond1 = c_abs[i] > alpha * baseline\n        cond2 = c_abs[i] > beta * c_abs[i-1]\n        \n        if cond1 and cond2:\n            return i + 1  # Return 1-based index\n\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define spike detection parameters\n    p = 5\n    alpha = 3.0\n    beta = 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: m=50, n=40, seed=1234, sigma_i=10^-(i-1)/8, x_hat_i=exp(-(i-1)/7), level=1e-3\n        {\"m\": 50, \"n\": 40, \"seed\": 1234, \"sigma_denom\": 8.0, \"x_hat_denom\": 7.0, \"level\": 1e-3},\n        # Case 2: m=50, n=40, seed=2021, sigma_i=10^-(i-1)/8, x_hat_i=exp(-(i-1)/7), level=0\n        {\"m\": 50, \"n\": 40, \"seed\": 2021, \"sigma_denom\": 8.0, \"x_hat_denom\": 7.0, \"level\": 0.0},\n        # Case 3: m=80, n=60, seed=999, sigma_i=10^-(i-1)/6, x_hat_i=exp(-(i-1)/10), level=2e-2\n        {\"m\": 80, \"n\": 60, \"seed\": 999, \"sigma_denom\": 6.0, \"x_hat_denom\": 10.0, \"level\": 2e-2},\n    ]\n\n    results = []\n    for case in test_cases:\n        k_star = run_case(\n            m=case[\"m\"],\n            n=case[\"n\"],\n            seed=case[\"seed\"],\n            sigma_denominator=case[\"sigma_denom\"],\n            x_hat_denominator=case[\"x_hat_denom\"],\n            level=case[\"level\"],\n            p=p,\n            alpha=alpha,\n            beta=beta\n        )\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In theory, the least-squares problem can be solved via the normal equations $A^\\top A x = A^\\top b$. This practice illuminates a critical lesson in numerical computation: theoretical equivalence does not guarantee numerical stability. Forming the matrix $A^\\top A$ squares the condition number, which can catastrophically amplify noise and floating-point errors in ill-conditioned systems. By comparing the TSVD solution from the SVD of $A$ with the analogous solution from the eigendecomposition of $A^\\top A$, you will see firsthand why the normal equations are often a numerically hazardous path .",
            "id": "3428383",
            "problem": "Consider a linear inverse problem where the goal is to recover an unknown vector $x \\in \\mathbb{R}^n$ from noisy data $b \\in \\mathbb{R}^m$ produced by a matrix $A \\in \\mathbb{R}^{m \\times n}$. The fundamental base is the following set of well-tested facts and definitions:\n\n- The singular value decomposition (SVD) of a matrix $A$ is $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains nonnegative singular values on its diagonal.\n- The normal equations for least squares are $A^\\top A x = A^\\top b$, where $A^\\top A$ is symmetric positive semidefinite.\n- Truncated singular value decomposition (TSVD) regularization forms an estimator by projecting onto the subspace spanned by the leading singular vectors, suppressing directions associated with small singular values to reduce noise amplification.\n- Forming the normal equations $A^\\top A$ squares the singular values, which increases the condition number and can amplify noise-induced perturbations, especially when singular values are clustered or nearly equal.\n- Eigenvectors of $A^\\top A$ coincide with right singular vectors of $A$ in the absence of noise; however, noise in $A$ can make the eigenspaces of $A^\\top A$ ill-conditioned, so truncated eigendecomposition on $A^\\top A$ may be numerically less stable than TSVD directly on $A$.\n\nYour task is to evaluate, in a controlled numerical experiment, the stability and accuracy of two regularized estimators for $x$:\n- TSVD estimator computed directly from the SVD of the noisy matrix $A$,\n- Truncated eigendecomposition estimator computed from the eigen-decomposition of $A^\\top A$ (normal equations).\n\nConstruct synthetic matrices with a prescribed singular value spectrum that decays as a power law, optionally embedding clusters of nearly equal singular values to induce eigenspace ill-conditioning. Add controlled noise to $A$ and optionally to $b$. For each test case, compute both estimators with the same truncation level and compare their relative reconstruction errors with respect to a ground-truth $x_{\\mathrm{true}}$.\n\nDefinitions to be used:\n- Let $A_{\\mathrm{true}} = U_{m \\times n} \\Sigma V^\\top$ be a synthetic matrix with orthonormal $U_{m \\times n}$ (the first $n$ columns of an orthogonal matrix in $\\mathbb{R}^{m \\times m}$), orthonormal $V \\in \\mathbb{R}^{n \\times n}$, and singular values $\\sigma_i$ following $\\sigma_i = i^{-p}$ normalized so that $\\max_i \\sigma_i = 1$. Optionally, create a cluster by setting $\\sigma_i$ equal for a contiguous block of indices.\n- Let $A = A_{\\mathrm{true}} + \\alpha E$ be the noisy forward operator, with $E$ having independent and identically distributed Gaussian entries of zero mean and unit variance, scaled to have root-mean-square magnitude $1/\\sqrt{mn}$ so that the noise level is controlled by $\\alpha$.\n- Let $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ be a unit-norm vector drawn uniformly at random from the unit sphere.\n- Let $b = A_{\\mathrm{true}} x_{\\mathrm{true}} + \\beta \\eta$, where $\\eta \\in \\mathbb{R}^m$ is Gaussian noise with the same scaling convention as $E$, and $\\beta$ controls data noise.\n- The TSVD estimator with truncation $k$ is\n$$\nx_{\\mathrm{TSVD}}(k) \\;=\\; V_k \\Sigma_k^{-1} U_k^\\top b,\n$$\nwhere $U_k$, $\\Sigma_k$, and $V_k$ denote the leading $k$ singular vectors and singular values of $A$.\n- The truncated eigendecomposition estimator with truncation $k$ is computed from $A^\\top A = Q \\Lambda Q^\\top$ by\n$$\nx_{\\mathrm{TE}}(k) \\;=\\; Q_k \\Lambda_k^{-1} Q_k^\\top A^\\top b,\n$$\nwhere $Q_k$ and $\\Lambda_k$ denote the eigenvectors and eigenvalues of $A^\\top A$ corresponding to the largest $k$ eigenvalues.\n\nFor each test case, compute the relative errors\n$$\n\\varepsilon_{\\mathrm{TSVD}} \\;=\\; \\frac{\\| x_{\\mathrm{TSVD}}(k) - x_{\\mathrm{true}} \\|_2}{\\|x_{\\mathrm{true}}\\|_2}, \n\\qquad\n\\varepsilon_{\\mathrm{TE}} \\;=\\; \\frac{\\| x_{\\mathrm{TE}}(k) - x_{\\mathrm{true}} \\|_2}{\\|x_{\\mathrm{true}}\\|_2},\n$$\nand report the ratio $\\rho = \\varepsilon_{\\mathrm{TE}} / \\varepsilon_{\\mathrm{TSVD}}$.\n\nImplement the following steps:\n1. Generate $U$ and $V$ as orthogonal matrices via the $QR$ decomposition of random Gaussian matrices, and construct $A_{\\mathrm{true}}$ using the prescribed singular value spectrum and any indicated cluster.\n2. Form the noisy $A$ as described above, and construct $b$ from $A_{\\mathrm{true}}$ and $x_{\\mathrm{true}}$ with optional data noise.\n3. Compute $x_{\\mathrm{TSVD}}(k)$ via the SVD of $A$ and $x_{\\mathrm{TE}}(k)$ via the eigen-decomposition of $A^\\top A$.\n4. Compute the ratio $\\rho$ for each test case.\n\nThe program must produce the ratios for the following five test cases, designed to probe different stability regimes:\n- Case 1 (happy path, low operator noise): $m=80$, $n=60$, $p=2.0$, $\\alpha=10^{-3}$, $\\beta=10^{-6}$, $k=10$, no singular value cluster.\n- Case 2 (moderate operator noise): $m=80$, $n=60$, $p=2.0$, $\\alpha=5 \\times 10^{-2}$, $\\beta=10^{-6}$, $k=10$, no singular value cluster.\n- Case 3 (high operator noise, small truncation): $m=120$, $n=100$, $p=2.5$, $\\alpha=10^{-1}$, $\\beta=0$, $k=5$, no singular value cluster.\n- Case 4 (square system, stronger decay, moderate noise, larger truncation): $m=60$, $n=60$, $p=3.0$, $\\alpha=10^{-2}$, $\\beta=10^{-6}$, $k=25$, no cluster.\n- Case 5 (clustered singular values to induce eigenspace ill-conditioning): $m=100$, $n=80$, $p=3.0$, $\\alpha=5 \\times 10^{-2}$, $\\beta=10^{-6}$, $k=15$, with a contiguous cluster of equal singular values for indices $i \\in \\{12,13,\\dots,19\\}$.\n\nYour program should produce a single line of output containing the ratios $\\rho$ for the five test cases as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5]\"). No physical units are involved. Angles are not applicable. Percentages must not be used; all outputs must be raw floating-point numbers.\n\nThe numerical experiment should highlight that, particularly in the presence of operator noise and clustered singular values, truncation based on the eigen-decomposition of $A^\\top A$ can be less stable than TSVD applied directly to $A$, often yielding larger reconstruction errors. Ensure reproducibility by fixing a random seed before generating random matrices and vectors.",
            "solution": "The task is to perform a numerical experiment comparing the stability and accuracy of two regularization methods for solving linear inverse problems: the Truncated Singular Value Decomposition (TSVD) applied directly to the system matrix $A$, and a truncated eigendecomposition applied to the normal equations matrix $A^\\top A$. The core hypothesis, grounded in established numerical analysis principles, is that forming the normal equations matrix $A^\\top A$ squares the singular values of $A$, thereby squaring its condition number. This can amplify numerical errors, particularly in the presence of noise and when singular values are clustered, making the eigendecomposition of $A^\\top A$ a less stable basis for regularization than the SVD of $A$ itself.\n\nThe experiment is rigorously structured. We will generate synthetic data with controlled properties, including the singular value spectrum of the \"true\" matrix, the level of noise corrupting the matrix, and the level of noise in the data vector. For a set of predefined test cases, we will compute the solution estimate from both methods and evaluate their performance by comparing their relative reconstruction errors.\n\nThe methodology is implemented as follows:\n\n1.  **Synthetic Data Generation**: We construct a test environment for each case.\n    - **True Matrix $A_{\\mathrm{true}}$**: A matrix $A_{\\mathrm{true}} \\in \\mathbb{R}^{m \\times n}$ is created with a pre-defined singular value spectrum. This is accomplished by constructing its Singular Value Decomposition, $A_{\\mathrm{true}} = U \\Sigma_{\\mathrm{true}} V^\\top$.\n        - The orthogonal matrices $U \\in \\mathbb{R}^{m \\times n}$ (with orthonormal columns) and $V \\in \\mathbb{R}^{n \\times n}$ are generated by applying the QR decomposition to random Gaussian matrices of appropriate sizes. This is a standard method for producing random orthogonal matrices.\n        - The singular values $\\sigma_i$ are set according to a power-law decay, $\\sigma_i = i^{-p}$ for $i=1, \\dots, n$. Since the maximum value is $1^{-p}=1$ for $i=1$, this sequence is already normalized as required.\n        - For specific test cases, a cluster of singular values is created by setting $\\sigma_i$ to a constant value for a contiguous block of indices, which is designed to create ill-conditioned eigenspaces in $A^\\top A$.\n    - **True Solution $x_{\\mathrm{true}}$**: A ground-truth vector $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ is generated by creating a random vector with entries from a standard normal distribution and normalizing it to have a Euclidean norm of $1$.\n    - **Noise Generation**: Additive Gaussian noise is introduced.\n        - The noise matrix $E \\in \\mathbb{R}^{m \\times n}$ has entries drawn from $\\mathcal{N}(0, 1/(mn))$. This scaling ensures the expected root-mean-square magnitude of the noise entries is controlled.\n        - The noise vector $\\eta \\in \\mathbb{R}^m$ has entries drawn from $\\mathcal{N}(0, 1/m)$ following a consistent scaling convention.\n    - **Noisy System**: The final forward operator and data vector are formed as $A = A_{\\mathrm{true}} + \\alpha E$ and $b = A_{\\mathrm{true}} x_{\\mathrm{true}} + \\beta \\eta$, where $\\alpha$ and $\\beta$ are parameters controlling the noise levels.\n\n2.  **Estimator Computation**:\n    - **TSVD Estimator $x_{\\mathrm{TSVD}}(k)$**: The SVD of the noisy matrix $A$ is computed, $A = U_A \\Sigma_A V_A^\\top$. The solution is then estimated using the formula for the truncated pseudo-inverse:\n    $$x_{\\mathrm{TSVD}}(k) = V_{A,k} \\Sigma_{A,k}^{-1} U_{A,k}^\\top b$$\n    where $U_{A,k}$, $\\Sigma_{A,k}$, and $V_{A,k}$ represent the components corresponding to the $k$ largest singular values of $A$.\n    - **Truncated Eigendecomposition (TE) Estimator $x_{\\mathrm{TE}}(k)$**: First, the normal equations matrix $C = A^\\top A$ is formed. Its eigendecomposition, $C = Q \\Lambda Q^\\top$, is then computed. The solution is estimated as:\n    $$x_{\\mathrm{TE}}(k) = Q_k \\Lambda_k^{-1} Q_k^\\top A^\\top b$$\n    where $Q_k$ and $\\Lambda_k$ correspond to the $k$ largest eigenvalues and their associated eigenvectors. Since $C$ is symmetric positive semi-definite, we use `scipy.linalg.eigh`, which is optimized for such matrices. The eigenvalues it returns are sorted in ascending order, so they must be reversed to obtain the largest ones.\n\n3.  **Error Evaluation and Comparison**: For each estimator, the relative reconstruction error is calculated. Since $\\|x_{\\mathrm{true}}\\|_2 = 1$, the formulas simplify to:\n    $$\\varepsilon_{\\mathrm{TSVD}} = \\| x_{\\mathrm{TSVD}}(k) - x_{\\mathrm{true}} \\|_2$$\n    $$\\varepsilon_{\\mathrm{TE}} = \\| x_{\\mathrm{TE}}(k) - x_{\\mathrm{true}} \\|_2$$\n    The performance of the TE method relative to the TSVD method is quantified by the ratio $\\rho = \\varepsilon_{\\mathrm{TE}} / \\varepsilon_{\\mathrm{TSVD}}$. A ratio $\\rho > 1$ indicates that the TSVD estimator is more accurate for that specific case.\n\nTo ensure the reproducibility of the experiment, a fixed seed is used for the pseudo-random number generator before running the test cases. The implementation consolidates this entire process into a single script that iterates through the five specified test cases, computes the ratio $\\rho$ for each, and reports the results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, svd, eigh\n\ndef solve():\n    \"\"\"\n    Solves the problem by running a numerical experiment to compare TSVD and\n    truncated eigendecomposition on the normal equations for a series of test cases.\n    \"\"\"\n    # Fix the random seed for reproducibility as required by the problem.\n    np.random.seed(0)\n\n    # Define the five test cases specified in the problem statement.\n    # Format: (m, n, p, alpha, beta, k, cluster_range)\n    # cluster_range is a tuple (start, end) for 0-based indexing or None.\n    test_cases = [\n        # Case 1: Happy path, low operator noise\n        (80, 60, 2.0, 1e-3, 1e-6, 10, None),\n        # Case 2: Moderate operator noise\n        (80, 60, 2.0, 5e-2, 1e-6, 10, None),\n        # Case 3: High operator noise, small truncation\n        (120, 100, 2.5, 1e-1, 0.0, 5, None),\n        # Case 4: Square system, stronger decay, larger truncation\n        (60, 60, 3.0, 1e-2, 1e-6, 25, None),\n        # Case 5: Clustered singular values\n        (100, 80, 3.0, 5e-2, 1e-6, 15, (11, 19)),\n    ]\n\n    results = []\n\n    for case_params in test_cases:\n        m, n, p, alpha, beta, k, cluster_range = case_params\n\n        # 1. Generate synthetic data (A_true, x_true)\n        # Generate random orthogonal matrices U and V via QR decomposition\n        G_m = np.random.randn(m, m)\n        U_full, _ = qr(G_m)\n        U = U_full[:, :n]\n\n        G_n = np.random.randn(n, n)\n        V, _ = qr(G_n)\n\n        # Generate true singular values\n        indices = np.arange(1, n + 1)\n        s_true = indices**(-p)\n        # The sequence is already normalized with max(s_true) = 1.\n\n        # Apply cluster if specified\n        if cluster_range:\n            start, end = cluster_range\n            s_true[start:end] = s_true[start]\n\n        # Construct A_true using thin SVD representation\n        A_true = U @ (np.diag(s_true) @ V.T)\n\n        # Generate true solution vector x_true\n        x_true_unnormalized = np.random.randn(n)\n        x_true = x_true_unnormalized / np.linalg.norm(x_true_unnormalized)\n\n        # 2. Form noisy operator A and data vector b\n        # Generate noise matrix E and noise vector eta\n        E = np.random.randn(m, n) / np.sqrt(m * n)\n        A = A_true + alpha * E\n        \n        b = A_true @ x_true\n        if beta > 0:\n            eta = np.random.randn(m) / np.sqrt(m)\n            b += beta * eta\n\n        # 3. Compute TSVD estimator x_tsvd\n        Ua, sa, Vta = svd(A, full_matrices=False)\n        U_k = Ua[:, :k]\n        s_inv_k = 1.0 / sa[:k]\n        V_k = Vta[:k, :].T\n        \n        # Efficient computation of x_tsvd = V_k @ diag(s_inv_k) @ U_k.T @ b\n        tmp_tsvd = U_k.T @ b\n        tmp_tsvd = tmp_tsvd * s_inv_k\n        x_tsvd = V_k @ tmp_tsvd\n\n        # 4. Compute Truncated Eigendecomposition estimator x_te\n        AtA = A.T @ A\n        # eigh returns eigenvalues in ascending order\n        eigvals, eigvecs = eigh(AtA)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        eigvals_sorted = eigvals[::-1]\n        eigvecs_sorted = eigvecs[:, ::-1]\n\n        # Truncate to the k largest eigenpairs\n        Lambda_k_inv_diag = 1.0 / eigvals_sorted[:k]\n        Q_k = eigvecs_sorted[:, :k]\n        Atb = A.T @ b\n\n        # Efficient computation of x_te = Q_k @ diag(Lambda_k_inv) @ Q_k.T @ Atb\n        tmp_te = Q_k.T @ Atb\n        tmp_te = tmp_te * Lambda_k_inv_diag\n        x_te = Q_k @ tmp_te\n\n        # 5. Compute relative errors and their ratio\n        # Since ||x_true||_2 = 1, relative error is ||x_hat - x_true||_2\n        eps_tsvd = np.linalg.norm(x_tsvd - x_true)\n        eps_te = np.linalg.norm(x_te - x_true)\n\n        # Handle the case where eps_tsvd might be zero to avoid division by zero\n        if eps_tsvd == 0:\n            ratio = 1.0 if eps_te == 0 else np.inf\n        else:\n            ratio = eps_te / eps_tsvd\n        \n        results.append(ratio)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Computing the full SVD for large-scale inverse problems is often computationally infeasible. This exercise introduces a powerful and modern alternative from randomized numerical linear algebra. You will implement a randomized SVD (rSVD) algorithm, which efficiently approximates the most dominant singular components of a matrix by using random projections. This practice bridges the gap between regularization theory and practical, high-dimensional applications by having you compare the rSVD-based solution to the exact TSVD solution, directly evaluating the trade-off between computational efficiency and accuracy .",
            "id": "3200996",
            "problem": "You are given an ill-posed linear inverse problem constructed from a synthetic matrix with prescribed singular value decay. You must implement a randomized singular value decomposition (SVD) approximation and evaluate how its approximation error interacts with truncated singular value decomposition (TSVD) truncation on the accuracy of the regularized solution. The program must be self-contained, produce deterministic results, and print the answers in the specified single-line format.\n\nTask description:\n- Use the definition of singular value decomposition (SVD), truncated singular value decomposition (TSVD) regularization, and Euclidean norms as the fundamental base.\n- Construct a matrix $A \\in \\mathbb{R}^{m \\times n}$ with rank $r \\leq \\min(m,n)$ via $A = U_r \\Sigma_r V_r^\\top$, where $U_r \\in \\mathbb{R}^{m \\times r}$ and $V_r \\in \\mathbb{R}^{n \\times r}$ are orthonormal columns generated from a random Gaussian matrix followed by orthonormalization, and $\\Sigma_r = \\operatorname{diag}(\\sigma_1,\\ldots,\\sigma_r)$ has exponentially decaying singular values $\\sigma_i = \\rho^{i-1}$ for $i \\in \\{1,\\ldots,r\\}$ with $0 < \\rho < 1$. Use a fixed random seed per test case for reproducibility.\n- Define the true solution $x_{\\text{true}} \\in \\mathbb{R}^n$ by coefficients $c_i = i^{-2}$ for $i \\in \\{1,\\ldots,r\\}$ and $c_i = 0$ otherwise, and set $x_{\\text{true}} = V_r c$.\n- Form the right-hand side $b = A x_{\\text{true}} + \\varepsilon$, where $\\varepsilon$ has independent Gaussian entries with mean $0$ and standard deviation $\\eta$.\n- Implement a randomized SVD with oversampling $p$ and power iterations $q$ to approximate the leading $k$ components. Use a Gaussian test matrix of shape $n \\times (k+p)$, compute an orthonormal range basis, and apply $q$ power iterations to improve spectral separation before forming a small projected matrix and computing its conventional SVD. Ensure $k+p \\leq \\min(m,n)$ in all test cases.\n- Compute the TSVD solution using both the exact leading $k$ components from a conventional SVD of $A$ and the randomized approximation. The TSVD solution uses only the leading $k$ singular components to regularize the inverse problem.\n- For each test case, output the relative $2$-norm error between the randomized-approximate TSVD solution $x_k^{\\text{(rand)}}$ and the exact TSVD solution $x_k^{\\text{(exact)}}$, defined as $\\|x_k^{\\text{(rand)}} - x_k^{\\text{(exact)}}\\|_2 / \\|x_k^{\\text{(exact)}}\\|_2$, as a decimal number.\n\nTest suite:\n- Case $1$: $m=60$, $n=40$, $r=40$, $\\rho=0.85$, $\\eta=10^{-3}$, $k=10$, $p=15$, $q=0$, seed $=0$.\n- Case $2$: $m=60$, $n=40$, $r=40$, $\\rho=0.85$, $\\eta=10^{-3}$, $k=20$, $p=15$, $q=1$, seed $=1$.\n- Case $3$: $m=60$, $n=40$, $r=40$, $\\rho=0.85$, $\\eta=10^{-1}$, $k=10$, $p=5$, $q=0$, seed $=2$.\n- Case $4$: $m=60$, $n=40$, $r=40$, $\\rho=0.85$, $\\eta=10^{-2}$, $k=35$, $p=5$, $q=0$, seed $=3$.\n- Case $5$: $m=60$, $n=40$, $r=40$, $\\rho=0.95$, $\\eta=10^{-2}$, $k=20$, $p=0$, $q=2$, seed $=4$.\n- Case $6$: $m=80$, $n=60$, $r=60$, $\\rho=0.90$, $\\eta=5 \\cdot 10^{-3}$, $k=25$, $p=10$, $q=2$, seed $=5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the relative errors for the test cases as a comma-separated list of decimal numbers rounded to $6$ decimal places and enclosed in square brackets, for example $[0.123456,0.234567,0.345678]$.",
            "solution": "The construction and analysis rely on the core definitions of singular value decomposition (SVD), truncated singular value decomposition (TSVD), and Euclidean norms. A matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$ admits the singular value decomposition $A = U \\Sigma V^\\top$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has nonnegative singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r > 0$ on its diagonal with zeros elsewhere. In ill-posed inverse problems, small singular values cause instability because the least-squares solution $x^\\star$ through the Moore–Penrose pseudoinverse amplifies noise. Truncated singular value decomposition (TSVD) regularization addresses this by discarding components associated with small singular values, retaining only the dominant $k$ singular triplets.\n\nWe begin with the synthetic construction. Let $U_r \\in \\mathbb{R}^{m \\times r}$ and $V_r \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns obtained via orthonormalization (for example, by applying the Gram–Schmidt process or the $QR$ decomposition) to independent Gaussian random matrices. Define $\\Sigma_r = \\operatorname{diag}(\\sigma_1,\\ldots,\\sigma_r)$ with exponential decay $\\sigma_i = \\rho^{i-1}$ for $i \\in \\{1,\\ldots,r\\}$ and $0<\\rho<1$, and set $A = U_r \\Sigma_r V_r^\\top$. This yields a rank-$r$ matrix with controlled singular spectrum. To define a true solution, let coefficients $c \\in \\mathbb{R}^n$ satisfy $c_i = i^{-2}$ for $i \\in \\{1,\\ldots,r\\}$ and $c_i = 0$ otherwise, and set $x_{\\text{true}} = V_r c$. The right-hand side is $b = A x_{\\text{true}} + \\varepsilon$, where $\\varepsilon$ has independent Gaussian entries with mean $0$ and standard deviation $\\eta$. This construction aligns $x_{\\text{true}}$ with the right singular vectors and uses decaying coefficients to avoid pathological growth when inverted.\n\nFor TSVD, recall that the least-squares solution using $k$ leading singular components is obtained by projecting $b$ onto the subspace spanned by the leading left singular vectors and rescaling by the corresponding singular values. If we denote the $k$ leading singular triplets by $\\{(u_i,\\sigma_i,v_i)\\}_{i=1}^k$, then the TSVD solution is\n$$\nx_k^{\\text{(exact)}} = \\sum_{i=1}^k \\frac{u_i^\\top b}{\\sigma_i} v_i,\n$$\nwhich corresponds to applying the pseudoinverse restricted to the leading $k$ singular directions. This regularization suppresses the contribution of small singular values that would otherwise amplify noise in $b$.\n\nRandomized singular value decomposition (rSVD) approximates the dominant singular subspace efficiently by random projection. The principle is to sample the range of $A$ using a Gaussian test matrix. Let $\\Omega \\in \\mathbb{R}^{n \\times (k+p)}$ have independent standard normal entries. Form the sample matrix $Y = A \\Omega \\in \\mathbb{R}^{m \\times (k+p)}$. To improve separation between leading and trailing singular values, apply $q$ steps of power iterations:\n$$\n\\text{for } j \\in \\{1,\\ldots,q\\}: \\quad Y \\leftarrow A (A^\\top Y).\n$$\nCompute an orthonormal basis $Q \\in \\mathbb{R}^{m \\times (k+p)}$ for the columns of $Y$ via $QR$ decomposition. Project $A$ onto this subspace to obtain $B = Q^\\top A \\in \\mathbb{R}^{(k+p) \\times n}$. Compute the conventional SVD of $B$, namely $B = \\tilde{U} \\tilde{\\Sigma} \\tilde{V}^\\top$, and lift the left singular vectors back to the original space with $U_{\\text{approx}} = Q \\tilde{U}$. The leading $k$ components $(U_k^{\\text{(approx)}}, \\Sigma_k^{\\text{(approx)}}, V_k^{\\text{(approx)}})$ approximate the dominant singular triplets of $A$. The TSVD solution using the randomized approximation reads\n$$\nx_k^{\\text{(rand)}} = \\sum_{i=1}^k \\frac{(u_i^{\\text{(approx)}})^\\top b}{\\sigma_i^{\\text{(approx)}}} v_i^{\\text{(approx)}}.\n$$\n\nTo assess how randomized errors interact with TSVD truncation on solution accuracy, compute the relative error\n$$\n\\mathrm{rel\\_err} = \\frac{\\left\\|x_k^{\\text{(rand)}} - x_k^{\\text{(exact)}}\\right\\|_2}{\\left\\|x_k^{\\text{(exact)}}\\right\\|_2}.\n$$\nThis quantity isolates the impact of subspace approximation and singular value estimation errors within the $k$-dimensional truncated solution space, independent of the baseline regularization benefits of TSVD itself. Oversampling $p$ increases the probability that the sampled subspace captures the true dominant range, and power iterations $q$ suppress the influence of smaller singular directions by emphasizing the spectral gap. Larger $k$ increases sensitivity because more singular directions must be accurately approximated; conversely, smaller $k$ reduces sensitivity, focusing only on the most dominant components.\n\nAlgorithmic steps for each test case:\n- Construct $A$ with prescribed $m$, $n$, $r$, and $\\rho$ using orthonormal $U_r$, $V_r$ and diagonal $\\Sigma_r$.\n- Build $x_{\\text{true}}$ via coefficients $c_i = i^{-2}$ aligned with $V_r$.\n- Generate $b = A x_{\\text{true}} + \\varepsilon$ with Gaussian noise of standard deviation $\\eta$.\n- Compute $x_k^{\\text{(exact)}}$ from a conventional SVD of $A$ by retaining the leading $k$ singular components.\n- Compute $x_k^{\\text{(rand)}}$ using rSVD with parameters $k$, $p$, and $q$.\n- Report $\\mathrm{rel\\_err}$ rounded to $6$ decimal places.\n\nWe apply this procedure to the six specified test cases. Each case uses its own fixed random seed to ensure reproducibility. The final program must produce a single line with the list of the six relative errors, formatted as $[a_1,a_2,a_3,a_4,a_5,a_6]$, where each $a_i$ is a decimal rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef generate_orthonormal_matrix(rows, cols, rng):\n    \"\"\"\n    Generate an orthonormal matrix of shape (rows, cols) using QR decomposition\n    on a Gaussian random matrix.\n    \"\"\"\n    A = rng.randn(rows, cols)\n    Q, _ = np.linalg.qr(A, mode='reduced')\n    return Q\n\ndef build_rank_r_matrix(m, n, r, rho, seed):\n    \"\"\"\n    Construct A = U_r * Sigma_r * V_r^T with rank r and singular values\n    sigma_i = rho^(i-1). Returns A, V_r, and the RNG used.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    U_r = generate_orthonormal_matrix(m, r, rng)\n    V_r = generate_orthonormal_matrix(n, r, rng)\n    # Singular values with exponential decay\n    singular_values = rho ** np.arange(r)\n    Sigma_r = np.diag(singular_values)\n    A = U_r @ Sigma_r @ V_r.T\n    return A, V_r, rng\n\ndef build_true_solution(V_r, n, r):\n    \"\"\"\n    Build x_true = V_r * c where c_i = i^{-2} for i in 1..r and zero otherwise.\n    \"\"\"\n    c = np.zeros(n)\n    # Set first r entries with i^{-2} decay\n    indices = np.arange(1, r + 1)\n    c[:r] = 1.0 / (indices ** 2)\n    x_true = V_r @ c[:V_r.shape[1]]\n    # If n > r, pad with zeros to length n using V_r subspace representation\n    if x_true.shape[0] != n:\n        # This should not happen as x_true is constructed in R^n via V_r @ c_r\n        # V_r has shape (n, r), result is length n.\n        pass\n    return x_true\n\ndef randomized_svd(A, k, p, q, rng):\n    \"\"\"\n    Compute randomized SVD approximation of the leading k components of A\n    with oversampling p and power iterations q.\n\n    Returns (U_k, S_k, Vt_k) approximating the top k triplets.\n    \"\"\"\n    m, n = A.shape\n    l = k + p\n    # Gaussian test matrix Omega in R^{n x l}\n    Omega = rng.randn(n, l)\n    Y = A @ Omega  # m x l\n\n    # Power iterations to improve spectral separation\n    for _ in range(q):\n        Y = A @ (A.T @ Y)\n\n    # Orthonormal basis for range(Y)\n    Q, _ = np.linalg.qr(Y, mode='reduced')  # m x l (or smaller if rank-deficient)\n\n    # Project A onto the subspace\n    B = Q.T @ A  # l x n\n\n    # SVD of the small matrix\n    Ub, S, Vt = np.linalg.svd(B, full_matrices=False)\n\n    # Lift left singular vectors back to original space\n    U_approx = Q @ Ub  # m x l\n\n    # Take leading k\n    U_k = U_approx[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    return U_k, S_k, Vt_k\n\ndef tsvd_solution_from_components(U_k, S_k, Vt_k, b):\n    \"\"\"\n    Compute TSVD solution x_k = sum_i (u_i^T b / s_i) v_i using provided triplets.\n    \"\"\"\n    # Project b onto the span of U_k\n    coeffs = U_k.T @ b  # length k\n    # Scale by singular values\n    scaled = coeffs / S_k\n    # Reconstruct in original space using V_k\n    V_k = Vt_k.T  # n x k\n    x_k = V_k @ scaled\n    return x_k\n\ndef exact_tsvd_solution(A, b, k):\n    \"\"\"\n    Compute the exact TSVD solution using the top k components from conventional SVD of A.\n    \"\"\"\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n    U_k = U[:, :k]\n    S_k = S[:k]\n    Vt_k = Vt[:k, :]\n    return tsvd_solution_from_components(U_k, S_k, Vt_k, b)\n\ndef relative_error(x_hat, x_ref):\n    \"\"\"\n    Compute relative 2-norm error ||x_hat - x_ref||_2 / ||x_ref||_2.\n    If ||x_ref||_2 is zero (degenerate), return 0.0 if difference is zero, else inf.\n    \"\"\"\n    denom = np.linalg.norm(x_ref)\n    num = np.linalg.norm(x_hat - x_ref)\n    if denom == 0.0:\n        return 0.0 if num == 0.0 else float('inf')\n    return num / denom\n\ndef run_case(m, n, r, rho, eta, k, p, q, seed):\n    \"\"\"\n    Run a single test case:\n    - Build A with rank r and decay rho.\n    - Construct x_true and b with noise eta.\n    - Compute exact TSVD solution and randomized TSVD solution.\n    - Return relative error.\n    \"\"\"\n    A, V_r, rng = build_rank_r_matrix(m, n, r, rho, seed)\n\n    # Build x_true aligned with V_r\n    x_true = build_true_solution(V_r, n, r)\n\n    # Build noisy right-hand side b\n    noise = rng.randn(m) * eta\n    b = A @ x_true + noise\n\n    # Exact TSVD solution\n    x_exact = exact_tsvd_solution(A, b, k)\n\n    # Randomized TSVD solution\n    U_k, S_k, Vt_k = randomized_svd(A, k, p, q, rng)\n    x_rand = tsvd_solution_from_components(U_k, S_k, Vt_k, b)\n\n    # Relative error\n    rel_err = relative_error(x_rand, x_exact)\n    return rel_err\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\"m\": 60, \"n\": 40, \"r\": 40, \"rho\": 0.85, \"eta\": 1e-3, \"k\": 10, \"p\": 15, \"q\": 0, \"seed\": 0},\n        # Case 2\n        {\"m\": 60, \"n\": 40, \"r\": 40, \"rho\": 0.85, \"eta\": 1e-3, \"k\": 20, \"p\": 15, \"q\": 1, \"seed\": 1},\n        # Case 3\n        {\"m\": 60, \"n\": 40, \"r\": 40, \"rho\": 0.85, \"eta\": 1e-1, \"k\": 10, \"p\": 5, \"q\": 0, \"seed\": 2},\n        # Case 4\n        {\"m\": 60, \"n\": 40, \"r\": 40, \"rho\": 0.85, \"eta\": 1e-2, \"k\": 35, \"p\": 5, \"q\": 0, \"seed\": 3},\n        # Case 5\n        {\"m\": 60, \"n\": 40, \"r\": 40, \"rho\": 0.95, \"eta\": 1e-2, \"k\": 20, \"p\": 0, \"q\": 2, \"seed\": 4},\n        # Case 6\n        {\"m\": 80, \"n\": 60, \"r\": 60, \"rho\": 0.90, \"eta\": 5e-3, \"k\": 25, \"p\": 10, \"q\": 2, \"seed\": 5},\n    ]\n\n    results = []\n    for case in test_cases:\n        rel_err = run_case(**case)\n        # Round to 6 decimal places for output formatting\n        results.append(f\"{rel_err:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}