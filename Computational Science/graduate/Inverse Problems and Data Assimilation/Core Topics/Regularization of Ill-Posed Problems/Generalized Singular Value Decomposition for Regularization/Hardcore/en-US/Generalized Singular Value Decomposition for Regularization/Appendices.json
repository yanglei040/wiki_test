{
    "hands_on_practices": [
        {
            "introduction": "The fundamental power of the Generalized Singular Value Decomposition (GSVD) is its ability to simultaneously diagonalize both the forward operator $A$ and the regularization operator $L$. This practice guides you through the core algebraic steps to derive the explicit Tikhonov-regularized solution, $x_{\\lambda}$, in the GSVD basis. By working through this derivation , you will see how the complex matrix-vector system of the normal equations transforms into a simple set of scalar \"filter factors\" that modulate the solution components, providing a clear and intuitive picture of how regularization works.",
            "id": "3386295",
            "problem": "Consider matrices $A \\in \\mathbb{R}^{m \\times n}$ and $L \\in \\mathbb{R}^{p \\times n}$ with the property that the stacked matrix $\\begin{pmatrix} A \\\\ L \\end{pmatrix}$ has full column rank $n$. Assume the pair $(A,L)$ admits a Generalized Singular Value Decomposition (GSVD), that is, there exist orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{p \\times p}$, and an invertible matrix $Z \\in \\mathbb{R}^{n \\times n}$, together with positive scalars $\\{\\alpha_i\\}_{i=1}^{n}$ and $\\{\\beta_i\\}_{i=1}^{n}$ satisfying $\\alpha_i^{2} + \\beta_i^{2} = 1$ for all $i \\in \\{1,\\dots,n\\}$, such that\n$$\nA = U \\,\\operatorname{diag}(\\alpha_1,\\dots,\\alpha_n,0,\\dots,0)\\, Z^{-1}, \\quad\nL = V \\,\\operatorname{diag}(\\beta_1,\\dots,\\beta_n,0,\\dots,0)\\, Z^{-1}.\n$$\nLet $\\{u_i\\}_{i=1}^{m}$ denote the columns of $U$, and let $\\{z_i\\}_{i=1}^{n}$ denote the columns of $Z$. Given a data vector $b \\in \\mathbb{R}^{m}$ with the expansion $b = \\sum_{i=1}^{m} c_i \\, u_i$, where $c_i = u_i^{\\top} b$, consider the Tikhonov-regularized inverse problem\n$$\nx_{\\lambda} \\in \\underset{x \\in \\mathbb{R}^{n}}{\\arg\\min} \\ \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}, \\quad \\lambda  0.\n$$\nDefine the influence matrix $S_{\\lambda} \\in \\mathbb{R}^{m \\times m}$ by the mapping $b \\mapsto A x_{\\lambda} = S_{\\lambda} b$. Introduce the scalar filter factors $f_i(\\lambda)$ associated with $(A,L)$ and the regularization parameter $\\lambda$ through the decoupled normal equations in the GSVD coordinates. Using only the foundational properties of the Tikhonov functional and the GSVD, derive analytic expressions for $x_{\\lambda}$ and for $S_{\\lambda}$ in terms of $f_i(\\lambda)$, the coefficients $c_i$, and the vectors $u_i$ and $z_i$. Your final expressions must be closed-form and valid for all $\\lambda  0$ under the stated rank conditions. The final answer must be a single analytical expression. No numerical evaluation is required and no units are involved.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the theory of inverse problems and numerical linear algebra, specifically Tikhonov regularization and the Generalized Singular Value Decomposition (GSVD). The problem is well-posed, objective, and contains all necessary information for a unique solution.\n\nThe Tikhonov-regularized solution $x_{\\lambda}$ is the unique minimizer of the functional\n$$\nJ(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}\n$$\nfor a given regularization parameter $\\lambda  0$. The minimizer is found by setting the gradient of $J(x)$ with respect to $x$ to zero. The gradient is\n$$\n\\nabla_{x} J(x) = 2 A^{\\top}(A x - b) + 2 \\lambda^{2} L^{\\top}(L x).\n$$\nSetting $\\nabla_{x} J(x) = 0$ yields the normal equations for the Tikhonov problem:\n$$\n(A^{\\top}A + \\lambda^{2} L^{\\top}L) x = A^{\\top}b.\n$$\nThe problem states that the stacked matrix $\\begin{pmatrix} A \\\\ L \\end{pmatrix}$ has full column rank $n$. This guarantees that the matrix $(A^{\\top}A + \\lambda^{2} L^{\\top}L)$ is positive definite and thus invertible for any $\\lambda  0$, ensuring a unique solution $x_{\\lambda}$.\n\nWe now substitute the given GSVD of the pair $(A,L)$ into the normal equations. The decompositions are:\n$$\nA = U C Z^{-1}, \\quad L = V S Z^{-1}\n$$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthogonal matrices, $Z \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix, and $C \\in \\mathbb{R}^{m \\times n}$ and $S \\in \\mathbb{R}^{p \\times n}$ are rectangular diagonal matrices defined as\n$$\nC_{ij} = \\begin{cases} \\alpha_i  \\text{if } i=j \\le n \\\\ 0  \\text{otherwise} \\end{cases}, \\quad S_{ij} = \\begin{cases} \\beta_i  \\text{if } i=j \\le n \\\\ 0  \\text{otherwise} \\end{cases}.\n$$\nThe scalars $\\{\\alpha_i\\}_{i=1}^{n}$ and $\\{\\beta_i\\}_{i=1}^{n}$ are positive and satisfy $\\alpha_i^2 + \\beta_i^2 = 1$.\n\nThe transpose of $A$ and $L$ are $A^{\\top} = (Z^{-1})^{\\top} C^{\\top} U^{\\top}$ and $L^{\\top} = (Z^{-1})^{\\top} S^{\\top} V^{\\top}$.\nSubstituting these into the left-hand side of the normal equations gives:\n$$\nA^{\\top}A + \\lambda^{2} L^{\\top}L = (Z^{-1})^{\\top} C^{\\top} U^{\\top} U C Z^{-1} + \\lambda^{2} (Z^{-1})^{\\top} S^{\\top} V^{\\top} V S Z^{-1}.\n$$\nSince $U$ and $V$ are orthogonal, $U^{\\top}U = I_m$ and $V^{\\top}V = I_p$. The expression simplifies to:\n$$\nA^{\\top}A + \\lambda^{2} L^{\\top}L = (Z^{-1})^{\\top} (C^{\\top}C + \\lambda^{2} S^{\\top}S) Z^{-1}.\n$$\nThe products $C^{\\top}C$ and $S^{\\top}S$ are $n \\times n$ diagonal matrices:\n$$\nC^{\\top}C = \\operatorname{diag}(\\alpha_1^2, \\alpha_2^2, \\dots, \\alpha_n^2) \\equiv \\Sigma_A^2\n$$\n$$\nS^{\\top}S = \\operatorname{diag}(\\beta_1^2, \\beta_2^2, \\dots, \\beta_n^2) \\equiv \\Sigma_L^2\n$$\nThe right-hand side of the normal equations is $A^{\\top}b$. Using the expansion $b=\\sum_{j=1}^{m} c_j u_j$ where $c_j = u_j^{\\top} b$, we can write $b = U c$ where $c = (c_1, \\dots, c_m)^{\\top}$.\n$$\nA^{\\top}b = (Z^{-1})^{\\top} C^{\\top} U^{\\top} (U c) = (Z^{-1})^{\\top} C^{\\top} c.\n$$\nThe vector $C^{\\top}c$ is an $n$-dimensional vector whose $i$-th component is $(\\alpha_1 c_1, \\dots, \\alpha_n c_n)^{\\top}$.\n\nThe normal equations now become:\n$$\n(Z^{-1})^{\\top} (\\Sigma_A^2 + \\lambda^{2} \\Sigma_L^2) Z^{-1} x_{\\lambda} = (Z^{-1})^{\\top} C^{\\top} c.\n$$\nMultiplying from the left by $Z^{\\top}$ (which exists since $Z$ is invertible), we get:\n$$\n(\\Sigma_A^2 + \\lambda^{2} \\Sigma_L^2) Z^{-1} x_{\\lambda} = C^{\\top} c.\n$$\nLet us define a new variable $y = Z^{-1} x_{\\lambda}$. Since $Z= [z_1, \\dots, z_n]$, we have $x_{\\lambda} = Z y = \\sum_{i=1}^n y_i z_i$. The equation in terms of $y$ is:\n$$\n(\\Sigma_A^2 + \\lambda^{2} \\Sigma_L^2) y = C^{\\top} c.\n$$\nThis is a decoupled system of $n$ linear equations. The $i$-th equation is:\n$$\n(\\alpha_i^2 + \\lambda^2 \\beta_i^2) y_i = \\alpha_i c_i, \\quad i=1, \\dots, n.\n$$\nSolving for $y_i$:\n$$\ny_i = \\frac{\\alpha_i c_i}{\\alpha_i^2 + \\lambda^2 \\beta_i^2}.\n$$\nThe solution $x_{\\lambda}$ is then reconstructed as:\n$$\nx_{\\lambda} = \\sum_{i=1}^{n} y_i z_i = \\sum_{i=1}^{n} \\frac{\\alpha_i c_i}{\\alpha_i^2 + \\lambda^2 \\beta_i^2} z_i.\n$$\nWe introduce the scalar filter factors $f_i(\\lambda)$ as requested. A standard definition for these factors in the context of Tikhonov regularization is:\n$$\nf_i(\\lambda) = \\frac{\\alpha_i^2}{\\alpha_i^2 + \\lambda^2 \\beta_i^2}.\n$$\nWith this definition, the coefficient of $z_i$ in the sum for $x_\\lambda$ can be rewritten. Since $\\alpha_i  0$, we have:\n$$\n\\frac{\\alpha_i c_i}{\\alpha_i^2 + \\lambda^2 \\beta_i^2} = \\frac{\\alpha_i^2}{\\alpha_i^2 + \\lambda^2 \\beta_i^2} \\frac{c_i}{\\alpha_i} = f_i(\\lambda) \\frac{c_i}{\\alpha_i}.\n$$\nThus, the expression for $x_{\\lambda}$ in terms of the filter factors is:\n$$\nx_{\\lambda} = \\sum_{i=1}^{n} f_i(\\lambda) \\frac{c_i}{\\alpha_i} z_i.\n$$\nNext, we derive the expression for the influence matrix $S_{\\lambda}$, defined by $A x_{\\lambda} = S_{\\lambda} b$. We compute $A x_{\\lambda}$:\n$$\nA x_{\\lambda} = A \\left( \\sum_{j=1}^{n} y_j z_j \\right) = A Z y = (U C Z^{-1}) Z y = U C y.\n$$\nThe vector $C y \\in \\mathbb{R}^m$ has components $(C y)_i = \\sum_{j=1}^n C_{ij} y_j$. Due to the structure of $C$, this gives $(C y)_i = \\alpha_i y_i$ for $i \\le n$ and $(C y)_i = 0$ for $i  n$. Substituting the expression for $y_i$:\n$$\n(C y)_i = \\alpha_i \\left( \\frac{\\alpha_i c_i}{\\alpha_i^2 + \\lambda^2 \\beta_i^2} \\right) = \\frac{\\alpha_i^2}{\\alpha_i^2 + \\lambda^2 \\beta_i^2} c_i = f_i(\\lambda) c_i, \\quad \\text{for } i=1, \\dots, n.\n$$\nNow we can write $A x_{\\lambda}$ as a linear combination of the columns of $U$:\n$$\nA x_{\\lambda} = U (C y) = \\sum_{i=1}^{m} (C y)_i u_i = \\sum_{i=1}^{n} f_i(\\lambda) c_i u_i.\n$$\nTo find $S_{\\lambda}$, we express this result as a matrix acting on $b$. Recalling that $c_i = u_i^{\\top} b$:\n$$\nA x_{\\lambda} = \\sum_{i=1}^{n} f_i(\\lambda) (u_i^{\\top} b) u_i = \\sum_{i=1}^{n} f_i(\\lambda) u_i (u_i^{\\top} b) = \\left( \\sum_{i=1}^{n} f_i(\\lambda) u_i u_i^{\\top} \\right) b.\n$$\nBy comparing this with the definition $A x_{\\lambda} = S_{\\lambda} b$, we identify the influence matrix $S_{\\lambda}$:\n$$\nS_{\\lambda} = \\sum_{i=1}^{n} f_i(\\lambda) u_i u_i^{\\top}.\n$$\nThis is an $m \\times m$ matrix, being a sum of outer products. The expressions for $x_{\\lambda}$ and $S_{\\lambda}$ are provided as the final answer.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{i=1}^{n} f_i(\\lambda) \\frac{c_i}{\\alpha_i} z_i  \\sum_{i=1}^{n} f_i(\\lambda) u_i u_i^{\\top} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In practical applications, the scaling of the data fidelity term $\\|A x - b\\|_{2}^{2}$ relative to the penalty term $\\|L x\\|_{2}^{2}$ can seem arbitrary. This exercise explores the crucial concept of scaling invariance in Tikhonov regularization . By analyzing how the solution behaves when $A$ and $L$ are rescaled, you will uncover the precise transformation rule for the regularization parameter $\\lambda$ required to preserve the solution, deepening your understanding of the balance between the two terms in the objective function.",
            "id": "3386267",
            "problem": "Consider a linear inverse problem with Tikhonov regularization: given matrices $A \\in \\mathbb{R}^{m \\times n}$ and $L \\in \\mathbb{R}^{p \\times n}$ and data $b \\in \\mathbb{R}^{m}$, define the Tikhonov-regularized solution\n$$\nx_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2} \\right\\},\n$$\nwhere $\\lambda  0$ is the regularization parameter. Assume that the matrix pair $(A,L)$ admits a Generalized Singular Value Decomposition (GSVD), i.e., there exist orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{p \\times p}$, an invertible matrix $X \\in \\mathbb{R}^{n \\times n}$, and nonnegative diagonal matrices $C$ and $S$ (with compatible sizes) satisfying $C^{\\top}C + S^{\\top}S = I$ such that\n$$\nA = U \\begin{bmatrix} C  0 \\end{bmatrix} Q X^{-1}, \n\\qquad\nL = V \\begin{bmatrix} S  0 \\end{bmatrix} Q X^{-1},\n$$\nfor some orthogonal $Q$ of appropriate size. The generalized singular values are defined by $\\gamma_{i} = c_{i}/s_{i}$ for all indices $i$ where $s_{i}  0$.\n\nNow consider a positive rescaling of the operators by constants $\\alpha  0$ and $\\beta  0$:\n$$\n\\tilde{A} = \\alpha A, \n\\qquad \n\\tilde{L} = \\beta L.\n$$\nLet $\\tilde{\\gamma}_{i}$ denote the generalized singular values of the pair $(\\tilde{A},\\tilde{L})$ defined analogously via a GSVD of $(\\tilde{A},\\tilde{L})$. For a data vector $\\tilde{b} \\in \\mathbb{R}^{m}$, define\n$$\n\\tilde{x}_{\\tilde{\\lambda}} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\tilde{\\lambda}^{2} \\|\\tilde{L} x\\|_{2}^{2} \\right\\}.\n$$\n\nTasks:\n1. Starting from the GSVD definitions and the normal equations, show how the generalized singular values transform under the scaling $(\\tilde{A},\\tilde{L}) = (\\alpha A,\\beta L)$, and express $\\tilde{\\gamma}_{i}$ in terms of $\\gamma_{i}$, $\\alpha$, and $\\beta$.\n2. Determine a choice of $\\tilde{b}$ and an explicit transformation $\\tilde{\\lambda} = \\tilde{\\lambda}(\\lambda,\\alpha,\\beta)$ such that $\\tilde{x}_{\\tilde{\\lambda}} = x_{\\lambda}$ holds for every data vector $b$. Your final answer must be the single explicit closed-form expression for $\\tilde{\\lambda}$ as a function of $\\lambda$, $\\alpha$, and $\\beta$ (no units).",
            "solution": "The problem statement is analyzed and found to be valid. It is scientifically grounded in the theory of inverse problems and numerical linear algebra, specifically Tikhonov regularization and the Generalized Singular Value Decomposition (GSVD). The problem is well-posed, objective, and contains sufficient information to derive the requested quantities. The formulation of the GSVD, while presented in a slightly non-standard way with the inclusion of an orthogonal matrix $Q$, is mathematically equivalent to standard definitions and does not introduce contradictions.\n\nThe solution proceeds in two parts as requested by the problem statement.\n\nPart 1: Transformation of the generalized singular values $\\gamma_i$.\n\nThe generalized singular values $\\gamma_i$ of the matrix pair $(A, L)$ are defined as the square roots of the generalized eigenvalues $\\mu_i = \\gamma_i^2$ of the matrix pencil $(A^{\\top}A, L^{\\top}L)$. These eigenvalues solve the generalized eigenvalue problem\n$$\nA^{\\top}A v_i = \\mu_i L^{\\top}L v_i\n$$\nfor some generalized eigenvectors $v_i \\in \\mathbb{R}^n$.\n\nNow, consider the scaled matrices $\\tilde{A} = \\alpha A$ and $\\tilde{L} = \\beta L$, where $\\alpha  0$ and $\\beta  0$. The new generalized singular values, denoted by $\\tilde{\\gamma}_i$, correspond to the generalized eigenvalues $\\tilde{\\mu}_i = \\tilde{\\gamma}_i^2$ of the new matrix pencil $(\\tilde{A}^{\\top}\\tilde{A}, \\tilde{L}^{\\top}\\tilde{L})$. The new generalized eigenvalue problem is\n$$\n\\tilde{A}^{\\top}\\tilde{A} w_i = \\tilde{\\mu}_i \\tilde{L}^{\\top}\\tilde{L} w_i.\n$$\nSubstituting the definitions of $\\tilde{A}$ and $\\tilde{L}$, we get\n$$\n(\\alpha A)^{\\top}(\\alpha A) w_i = \\tilde{\\mu}_i (\\beta L)^{\\top}(\\beta L) w_i\n$$\n$$\n\\alpha^2 (A^{\\top}A) w_i = \\tilde{\\mu}_i \\beta^2 (L^{\\top}L) w_i.\n$$\nSince $\\alpha$ and $\\beta$ are non-zero, we can rearrange the equation to\n$$\nA^{\\top}A w_i = \\left( \\frac{\\beta^2}{\\alpha^2} \\tilde{\\mu}_i \\right) L^{\\top}L w_i.\n$$\nThis equation has the same form as the original generalized eigenvalue problem. This implies that the generalized eigenvectors are the same (i.e., we can choose $w_i = v_i$), and the generalized eigenvalues are related by\n$$\n\\mu_i = \\frac{\\beta^2}{\\alpha^2} \\tilde{\\mu}_i.\n$$\nSolving for $\\tilde{\\mu}_i$ yields\n$$\n\\tilde{\\mu}_i = \\frac{\\alpha^2}{\\beta^2} \\mu_i.\n$$\nThe generalized singular values are the positive square roots of the generalized eigenvalues, so $\\gamma_i = \\sqrt{\\mu_i}$ and $\\tilde{\\gamma}_i = \\sqrt{\\tilde{\\mu}_i}$. Taking the square root of the above relation gives\n$$\n\\tilde{\\gamma}_i = \\sqrt{\\frac{\\alpha^2}{\\beta^2} \\mu_i} = \\frac{\\alpha}{\\beta} \\sqrt{\\mu_i} = \\frac{\\alpha}{\\beta} \\gamma_i,\n$$\nsince $\\alpha, \\beta, \\gamma_i, \\tilde{\\gamma}_i$ are all defined to be non-negative (and positive where the values are meaningful).\n\nPart 2: Determination of $\\tilde{b}$ and $\\tilde{\\lambda}$ for solution invariance.\n\nThe original Tikhonov-regularized solution $x_{\\lambda}$ is the minimizer of the objective functional\n$$\nJ(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}.\n$$\nThe scaled problem seeks the minimizer $\\tilde{x}_{\\tilde{\\lambda}}$ of the objective functional\n$$\n\\tilde{J}(x) = \\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\tilde{\\lambda}^{2} \\|\\tilde{L} x\\|_{2}^{2}.\n$$\nWe are tasked to find a choice for $\\tilde{b}$ and a relationship for $\\tilde{\\lambda}$ such that $\\tilde{x}_{\\tilde{\\lambda}} = x_{\\lambda}$ for any choice of data vector $b$. This means that the set of minimizers for $J(x)$ and $\\tilde{J}(x)$ must be identical for any $b$. Two objective functionals have the same minimizers if one is a positive scaling of the other, plus a constant term that does not depend on the optimization variable $x$.\n\nLet's substitute $\\tilde{A} = \\alpha A$ and $\\tilde{L} = \\beta L$ into $\\tilde{J}(x)$:\n$$\n\\tilde{J}(x) = \\|\\alpha A x - \\tilde{b}\\|_{2}^{2} + \\tilde{\\lambda}^{2} \\|\\beta L x\\|_{2}^{2}.\n$$\nFactoring out the scaling constants $\\alpha$ and $\\beta$ from the norms, we get\n$$\n\\tilde{J}(x) = \\alpha^2 \\|A x - \\frac{1}{\\alpha}\\tilde{b}\\|_{2}^{2} + \\beta^2 \\tilde{\\lambda}^{2} \\|L x\\|_{2}^{2}.\n$$\nFor the minimizers of $J(x)$ and $\\tilde{J}(x)$ to be identical, the functionals must be structurally equivalent. Let's compare $J(x)$ and $\\tilde{J}(x)$.\n$$\nJ(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|L x\\|_{2}^{2}\n$$\n$$\n\\tilde{J}(x) = \\alpha^2 \\|A x - \\frac{1}{\\alpha}\\tilde{b}\\|_{2}^{2} + \\beta^2 \\tilde{\\lambda}^{2} \\|L x\\|_{2}^{2}\n$$\nThe most direct way to ensure the functionals are equivalent is to make $\\tilde{J}(x)$ a constant multiple of $J(x)$. Let's try to make $\\tilde{J}(x) = \\alpha^2 J(x)$.\nThis requires matching the corresponding terms.\nFirst, for the data fidelity term (the first term), we need\n$$\n\\alpha^2 \\|A x - \\frac{1}{\\alpha}\\tilde{b}\\|_{2}^{2} = \\alpha^2 \\|A x - b\\|_{2}^{2}.\n$$\nThis is satisfied if we choose\n$$\n\\frac{1}{\\alpha}\\tilde{b} = b \\implies \\tilde{b} = \\alpha b.\n$$\nThis is a valid choice for $\\tilde{b}$ that depends on $b$, as allowed. With this choice, the fidelity terms match.\n\nNext, we match the regularization terms (the second term):\n$$\n\\beta^2 \\tilde{\\lambda}^{2} \\|L x\\|_{2}^{2} = \\alpha^2 (\\lambda^{2} \\|L x\\|_{2}^{2}).\n$$\nThis equality must hold for any $x$, so we must equate the coefficients:\n$$\n\\beta^2 \\tilde{\\lambda}^{2} = \\alpha^2 \\lambda^{2}.\n$$\nSince $\\alpha, \\beta, \\lambda, \\tilde{\\lambda}$ are all positive, we can take the square root of both sides:\n$$\n\\beta \\tilde{\\lambda} = \\alpha \\lambda.\n$$\nSolving for $\\tilde{\\lambda}$ gives the required transformation:\n$$\n\\tilde{\\lambda} = \\lambda \\frac{\\alpha}{\\beta}.\n$$\nWith the choices $\\tilde{b} = \\alpha b$ and $\\tilde{\\lambda} = \\lambda \\frac{\\alpha}{\\beta}$, the scaled functional becomes $\\tilde{J}(x) = \\alpha^2 J(x)$. Since $\\alpha^2  0$, minimizing $\\tilde{J}(x)$ is equivalent to minimizing $J(x)$, and thus $\\tilde{x}_{\\tilde{\\lambda}} = x_{\\lambda}$ for any data vector $b$.\n\nThe problem asks for the single explicit closed-form expression for $\\tilde{\\lambda}$.",
            "answer": "$$\n\\boxed{\\lambda \\frac{\\alpha}{\\beta}}\n$$"
        },
        {
            "introduction": "Ultimately, the goal of regularization is to extract a meaningful signal from noisy data. This exercise bridges the abstract GSVD framework with a concrete statistical data analysis task . Using a simplified but representative scenario, you will design and apply a diagnostic tool to determine which components of the solution are dominated by the true signal and which are likely just artifacts of noise. This practice is vital for learning how to robustly interpret and gain confidence in the results of a regularized inversion.",
            "id": "3386249",
            "problem": "Consider a linear inverse problem with Tikhonov-type regularization, where the pair of matrices $(A,L) \\in \\mathbb{R}^{6 \\times 6} \\times \\mathbb{R}^{6 \\times 6}$ admits a Generalized Singular Value Decomposition (GSVD) of the form $A = U C Z^{-1}$ and $L = V S Z^{-1}$, with $U, V \\in \\mathbb{R}^{6 \\times 6}$ orthogonal, $Z \\in \\mathbb{R}^{6 \\times 6}$ invertible, and $C = \\operatorname{diag}(c_{1},\\dots,c_{6})$, $S = \\operatorname{diag}(s_{1},\\dots,s_{6})$ diagonal with $c_{i}^{2} + s_{i}^{2} = 1$ for all $i$. In this problem, assume a particularly simple GSVD triple given by $U = I_{6}$, $V = I_{6}$, $Z = I_{6}$, with\n$$\nC = \\operatorname{diag}(c_{1},\\dots,c_{6}), \\quad c = \\big(0.98,\\,0.80,\\,0.50,\\,0.25,\\,0.10,\\,0.05\\big),\n$$\nand $S = \\operatorname{diag}(s_{1},\\dots,s_{6})$ determined by $s_{i} = \\sqrt{1 - c_{i}^{2}}$.\n\nYou observe noisy data $y \\in \\mathbb{R}^{6}$ of the form $y = A x_{\\text{true}} + e$, where $x_{\\text{true}} \\in \\mathbb{R}^{6}$ is unknown and the noise $e$ is zero-mean Gaussian with covariance $\\sigma^{2} I_{6}$. The noise level is $\\sigma = 0.05$, and the realized data are\n$$\ny = \\begin{pmatrix} 0.82 \\\\ 0.56 \\\\ 0.17 \\\\ 0.06 \\\\ 0.03 \\\\ 0.02 \\end{pmatrix}.\n$$\n\nWorking in the GSVD data basis defined by $U$, the data coefficients are $\\widehat{y}_{i} = u_{i}^{\\top} y$ for $i \\in \\{1,\\dots,6\\}$ (where $u_{i}$ denotes the $i$-th column of $U$). Using only fundamental probabilistic facts about Gaussian noise and elementary bounds valid for maxima of independent Gaussian variables, design a diagnostic that identifies a transition index $k^{\\star}$ such that, with overall family-wise false-alarm probability at most $\\alpha = 0.05$, all coefficients $\\{ \\widehat{y}_{i} \\}_{i \\ge k^{\\star}}$ are consistent with being noise-dominated. Then apply your diagnostic to the given $y$ and compute the resulting integer index $k^{\\star}$. Report the single integer value of $k^{\\star}$; no rounding is necessary and no units are involved.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the fields of linear inverse problems and statistics, is well-posed, objective, and contains all necessary information to formulate and solve the problem.\n\nThe problem asks us to design a diagnostic to find a transition index $k^{\\star}$ for a set of data coefficients. This index should mark the point after which all coefficients are consistent with being dominated by noise. The diagnostic must adhere to a specified family-wise false-alarm probability.\n\nThe data model is given by $y = A x_{\\text{true}} + e$, where $A \\in \\mathbb{R}^{6 \\times 6}$, $x_{\\text{true}} \\in \\mathbb{R}^{6}$, and $e \\in \\mathbb{R}^{6}$ is a vector of noise components. The noise $e$ is specified as zero-mean Gaussian with covariance $\\sigma^{2} I_{6}$, with $\\sigma = 0.05$. This implies that the components of $e$, denoted $e_i$, are independent and identically distributed (i.i.d.) random variables from a normal distribution $\\mathcal{N}(0, \\sigma^2)$.\n\nThe data coefficients are defined in the GSVD basis as $\\widehat{y}_{i} = u_{i}^{\\top} y$, where $u_i$ is the $i$-th column of the orthogonal matrix $U$. The problem specifies a simplified GSVD where $U=I_{6}$. The columns of the identity matrix $I_6$ are the standard basis vectors, $u_i = e_i$. Therefore, the data coefficients are simply the components of the data vector itself:\n$$\n\\widehat{y}_{i} = e_{i}^{\\top} y = y_i\n$$\nThe observed data vector is given as $y = \\begin{pmatrix} 0.82  0.56  0.17  0.06  0.03  0.02 \\end{pmatrix}^{\\top}$. Thus, the coefficients are $|\\widehat{y}_1| = 0.82$, $|\\widehat{y}_2| = 0.56$, $|\\widehat{y}_3| = 0.17$, $|\\widehat{y}_4| = 0.06$, $|\\widehat{y}_5| = 0.03$, and $|\\widehat{y}_6| = 0.02$.\n\nWe need to determine for each coefficient $\\widehat{y}_i$ whether it is \"consistent with being noise-dominated\". We formalize this as a statistical hypothesis test. For each index $i \\in \\{1, \\dots, 6\\}$, we state the null hypothesis $H_{0,i}$ that the $i$-th coefficient consists only of noise.\nThe full expression for the $i$-th coefficient is $\\widehat{y}_i = u_i^\\top(A x_{\\text{true}} + e)$. Given $A = U C Z^{-1}$ and the specific values $U=I_6, Z=I_6$, this becomes $\\widehat{y}_i = e_i^\\top(C x_{\\text{true}} + e) = c_i x_{\\text{true},i} + e_i$.\nSo, the null hypothesis $H_{0,i}$ corresponds to the signal part being zero: $c_i x_{\\text{true},i} = 0$. Under $H_{0,i}$, the coefficient $\\widehat{y}_i = e_i$ is a random variable drawn from $\\mathcal{N}(0, \\sigma^2)$.\n\nThe task requires that the diagnostic has an \"overall family-wise false-alarm probability at most $\\alpha = 0.05$\". A false alarm occurs if we reject $H_{0,i}$ (i.e., we classify the coefficient as signal) when it is in fact true (the coefficient is just noise). The family-wise error rate (FWER) is the probability of making one or more such false alarms across the entire family of $N=6$ tests.\n\nTo control the FWER at level $\\alpha$, we employ the Bonferroni correction. This is a direct application of the union bound (Boole's inequality), which aligns with the problem's hint to use \"elementary bounds valid for maxima of independent Gaussian variables\". The Bonferroni method mandates that for each of the $N$ tests, the significance level must be set to $\\alpha/N$.\n\nFor a single coefficient $\\widehat{y}_i$, we perform a two-sided test. We reject $H_{0,i}$ if $|\\widehat{y}_i|$ is unusually large. Under $H_{0,i}$, the quantity $\\widehat{y}_i/\\sigma$ follows a standard normal distribution, $Z \\sim \\mathcal{N}(0,1)$. The probability of a false alarm for the $i$-th test is $P(|\\widehat{y}_i|  T)$ for some threshold $T$. We set this probability to be $\\alpha/N$.\n$$\nP(|\\widehat{y}_i|  T \\mid H_{0,i}) = P\\left(\\left|\\frac{\\widehat{y}_i}{\\sigma}\\right|  \\frac{T}{\\sigma}\\right) = P\\left(|Z|  \\frac{T}{\\sigma}\\right) = \\frac{\\alpha}{N}\n$$\nThis implies $2 \\left(1 - \\Phi\\left(\\frac{T}{\\sigma}\\right)\\right) = \\frac{\\alpha}{N}$, where $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. Solving for the threshold $T$:\n$$\n\\Phi\\left(\\frac{T}{\\sigma}\\right) = 1 - \\frac{\\alpha}{2N} \\implies T = \\sigma \\cdot \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2N}\\right)\n$$\nWe now compute this threshold $T$ using the given values:\n- Noise level $\\sigma = 0.05$.\n- Number of tests $N = 6$.\n- Family-wise significance level $\\alpha = 0.05$.\n\nThe argument to the inverse CDF is $1 - \\frac{0.05}{2 \\times 6} = 1 - \\frac{0.05}{12} \\approx 1 - 0.0041667 = 0.9958333$.\nUsing standard statistical software or tables for the inverse normal CDF, we find the critical value:\n$$\nz_{\\text{crit}} = \\Phi^{-1}(0.9958333) \\approx 2.638\n$$\nNow we can compute the decision threshold $T$:\n$$\nT = \\sigma \\cdot z_{\\text{crit}} = 0.05 \\times 2.638 = 0.1319\n$$\nThe diagnostic is as follows: a coefficient $\\widehat{y}_i$ is classified as signal-dominated if $|\\widehat{y}_i|  T$, and consistent with noise if $|\\widehat{y}_i| \\le T$.\n\nWe apply this diagnostic to our data coefficients $|\\widehat{y}_i| = |y_i|$:\n- $|\\widehat{y}_1| = 0.82  0.1319$ (Signal)\n- $|\\widehat{y}_2| = 0.56  0.1319$ (Signal)\n- $|\\widehat{y}_3| = 0.17  0.1319$ (Signal)\n- $|\\widehat{y}_4| = 0.06 \\le 0.1319$ (Noise)\n- $|\\widehat{y}_5| = 0.03 \\le 0.1319$ (Noise)\n- $|\\widehat{y}_6| = 0.02 \\le 0.1319$ (Noise)\n\nThe problem defines the transition index $k^{\\star}$ as the smallest integer index such that all coefficients $\\{\\widehat{y}_i\\}_{i \\ge k^{\\star}}$ are consistent with being noise-dominated. This means we are looking for the smallest $k$ such that for all $i \\ge k$, the condition $|\\widehat{y}_i| \\le T$ holds.\n\n- For $k=1$, the condition fails because $|\\widehat{y}_1|  T$.\n- For $k=2$, the condition fails because $|\\widehat{y}_2|  T$.\n- For $k=3$, the condition fails because $|\\widehat{y}_3|  T$.\n- For $k=4$, the condition requires $|\\widehat{y}_4| \\le T$, $|\\widehat{y}_5| \\le T$, and $|\\widehat{y}_6| \\le T$. All three of these are true.\n\nSince $k=4$ is the smallest index for which the condition holds, the transition index is $k^{\\star} = 4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        }
    ]
}