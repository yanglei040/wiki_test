## 引言
逆问题是科学与工程领域的核心挑战，其目标是从间接、带噪声的观测中恢复未知的物理量或模型参数。由于其固有的[不适定性](@entry_id:635673)，求解[逆问题](@entry_id:143129)通常需要引入先验知识来约束[解空间](@entry_id:200470)。传统方法依赖于简单的数学正则化项，但这往往难以捕捉现实世界信号的复杂结构，从而限制了重建的精度。近年来，机器学习，特别是[深度学习](@entry_id:142022)的崛起，为构建强大的数据驱动先验和设计高效的求解器提供了革命性的工具，极大地推动了该领域的发展。

本文旨在系统性地介绍用于解决[逆问题](@entry_id:143129)的各类前沿机器学习方法。我们将首先在“原理与机制”一章中，深入剖析这些方法背后的核心思想，从构建学习先验的[范式](@entry_id:161181)到各种算法框架的理论基础。接着，在“应用与交叉学科联系”一章中，我们将通过一系列实际案例，展示这些技术如何在地球科学、医学成像和[控制论](@entry_id:262536)等不同领域中与经典方法相结合，解决复杂的现实问题。最后，通过“动手实践”中的练习，读者将有机会将理论知识应用于具体问题，加深对关键概念的理解。通过这三章的学习，读者将建立一个关于机器学习如何革新逆问题求解的全面而深入的认识。

## 原理与机制

在上一章引言的基础上，本章将深入探讨解决[逆问题](@entry_id:143129)的[现代机器学习](@entry_id:637169)方法背后的核心原理与关键机制。我们将从学习先验的构建出发，系统地剖析各类算法[范式](@entry_id:161181)，并进一步探讨与[不确定性量化](@entry_id:138597)、对称性和离散化不变性等高级主题相关的理论问题。我们的目标是建立一个严谨的框架，用于理解、分析和设计基于机器学习的[逆问题](@entry_id:143129)求解器。

### 机器学习先验的[范式](@entry_id:161181)

传统[逆问题](@entry_id:143129)方法通常采用简单的正则化项（如Tikhonov或总变差正则化），这些正则化项虽然在数学上易于处理，但其表达能力有限，往往无法捕捉复杂现实世界信号（如自然图像或物理场）的精细结构。机器学习的核心贡献之一便是引入了**数据驱动的先验 (data-driven priors)**，它们能够从大规模数据集中学习到特定信号类别的高度复杂的统计规律。这些学习到的先验极大地提升了正则化的效果，从而在极度不适定的逆问题中实现前所未有的重建质量。根据其数学表述方式，学习先验主要可分为两大类：显式密度模型和隐式密度模型。

#### 显式密度模型：正态流

**显式密度模型 (explicit density models)** 直接定义了参数 $u$ 的一个易于计算的概率密度函数 $p_U(u)$。这类模型中最具代表性的是**正态流 (normalizing flows)**。正态流构建了一个从简单[先验分布](@entry_id:141376)（如[标准正态分布](@entry_id:184509)）的[潜变量](@entry_id:143771) $z \in \mathbb{R}^m$ 到目标[参数空间](@entry_id:178581) $u \in \mathbb{R}^m$ 的可逆映射（[微分同胚](@entry_id:147249)）$u = T_{\theta}(z)$。

根据[概率论中的变量替换](@entry_id:273732)公式，如果 $z$ 的[概率密度](@entry_id:175496)为 $p_Z(z)$，那么 $u$ 的诱导先验密度 $p_U(u)$ 为：
$$
p_U(u) = p_Z(T_{\theta}^{-1}(u)) \left| \det D T_{\theta}^{-1}(u) \right|
$$
其中 $T_{\theta}^{-1}$ 是可逆映射的[反函数](@entry_id:141256)，$D T_{\theta}^{-1}(u)$ 是该反函数在 $u$ 点的[雅可比矩阵](@entry_id:264467)。正态流模型的一个关键设计原则是，其逆映射 $T_{\theta}^{-1}$ 及其[雅可比行列式](@entry_id:137120)都必须是计算上易于处理的。这一特性使得诱导先验密度 $p_U(u)$ 及其梯度 $\nabla_u \log p_U(u)$ 能够被精确、高效地计算。

因此，当使用正态流作为先验时，可以直接应用经典的[贝叶斯推断](@entry_id:146958)方法。例如，可以通过最小化负对数后验来进行**最大后验估计 (Maximum A Posteriori, MAP)**，或者使用诸如[马尔可夫链蒙特卡洛 (MCMC)](@entry_id:137985) 等[采样方法](@entry_id:141232)来探索完整的后验分布 $p(u \mid y) \propto p(y \mid u) p_U(u)$ 。需要强调的是，并非所有正态流都保持体积不变（即雅可比行列式[绝对值](@entry_id:147688)恒为1）；事实上，通过改变局部体积的能力是学习复杂[分布](@entry_id:182848)的关键 。

#### 隐式密度模型：[生成对抗网络](@entry_id:634268)与[测度论](@entry_id:139744)考量

与显式模型不同，**隐式密度模型 (implicit density models)**，如**[生成对抗网络](@entry_id:634268) (Generative Adversarial Networks, GANs)** 或[变分自编码器](@entry_id:177996) (VAEs)，也通过一个生成器网络 $u = T_{\theta}(z)$ 从潜变量 $z$ 生成样本，但其诱导的先验分布 $p_U(u)$ 的密度函数通常是难以计算或不存在的。

一个在理论和实践中都至关重要的情形是当潜变量的维度 $d$ 远小于参数空间的维度 $m$ 时 ($d  m$)。在这种情况下，生成器 $T_{\theta}: \mathbb{R}^d \to \mathbb{R}^m$ 将一个低维空间映射到一个高维空间。只要 $T_{\theta}$ 是足够光滑的（例如，连续可微），其像集 $\mathcal{M} = \{ T_{\theta}(z) \mid z \in \mathbb{R}^d \}$ 将构成 $\mathbb{R}^m$ 中的一个至多 $d$ 维的[流形](@entry_id:153038)。由于 $d  m$，该[流形](@entry_id:153038)在 $\mathbb{R}^m$ 中的勒贝格测度为零。

这意味着由 $u=T_{\theta}(z)$ 诱导的先验测度 $\mu_U$ 将其全部概率[质量集中](@entry_id:175432)在一个勒贝格[零测集](@entry_id:157694)上。根据[测度论](@entry_id:139744)的定义，这样的测度相对于勒贝格测度是**奇异的 (singular)**，因此它不具有一个密度函数。在这种情况下，严格意义上的后验“密度” $p(u \mid y)$ 并不存在。

然而，这并不妨碍我们进行有效的贝叶斯推断。关键思想是将推断问题从参数空间 $u$ 提升到潜变量空间 $z$。我们可以定义一个关于 $z$ 的后验分布，其密度 $p(z \mid y)$ 是良定义的：
$$
p(z \mid y) \propto p(y \mid T_{\theta}(z)) p_Z(z)
$$
其中 $p(y \mid T_{\theta}(z))$ 是通过将生成器代入[似然函数](@entry_id:141927)得到的。由于 $z$ 的先验 $p_Z(z)$ 通常具有良好性质（如[高斯分布](@entry_id:154414)），我们可以直接在 $\mathbb{R}^d$ 中对 $p(z \mid y)$ 进行 MCMC 采样。然后，通过将后验样本 $\{z_i\}$ 通过生成器映射，$u_i = T_{\theta}(z_i)$，我们就能获得来自 $u$ 的真实后验测度的样本  。这种在低维[流形](@entry_id:153038)上定义先验的方法，是[深度生成模型](@entry_id:748264)在解决高维逆问题时能够“打破维度诅咒”的关键机制之一。

### 基于学习的逆问题求解算法

拥有了强大的学习先验后，我们需要有效的算法来结合观测数据 $y$ 以求解逆问题。[现代机器学习](@entry_id:637169)提供了多种算法[范式](@entry_id:161181)。

#### 展开迭代法和[不动点理论](@entry_id:157862)

许多经典的[逆问题](@entry_id:143129)求解算法，如[迭代软阈值算法](@entry_id:750899) (ISTA)，都可以被看作是求解一个[优化问题](@entry_id:266749)的迭代过程。**[算法展开](@entry_id:746359) (Algorithm Unrolling)** 或称**[深度展开](@entry_id:748272) (Deep Unfolding)** 的思想是将这些[迭代算法](@entry_id:160288)的固定数量的步骤“展开”成一个深度神经网络的层。例如，一个经典的更新步骤 $x^{k+1} = \mathcal{D}(x^k - \eta \nabla f(x^k))$，其中 $\nabla f$ 是数据保真项的梯度，$\mathcal{D}$ 是与正则化相关的算子（如[软阈值](@entry_id:635249)），可以被改造成一个网络层：
$$
x^{k+1} = \mathcal{P}_{\theta_k}(x^k - \eta_k A^T(A x^k - y))
$$
其中，梯度下降步骤保持不变（或步长 $\eta_k$ 变为可学习的），而原始的算子 $\mathcal{D}$ 被一个[参数化](@entry_id:272587)的[神经网](@entry_id:276355)络层 $\mathcal{P}_{\theta_k}$（例如，一个小型CNN）所取代。整个网络通过端到端的方式进行训练，以最小化最终输出 $x^K$ 与真实解之间的误差。

这类**学习迭代法 (learned iterative methods)** 可以被抽象为寻找一个算子 $T_{\theta}$ 的[不动点](@entry_id:156394)的过程，即求解 $x = T_{\theta}(x)$。因此，它们的稳定性和收敛性可以通过**[不动点理论](@entry_id:157862) (fixed-point theory)** 来进行严格分析。两个核心定理至关重要：
1.  **[巴拿赫不动点定理](@entry_id:146620) (Banach Fixed-Point Principle)**：如果算子 $T_{\theta}$ 是一个**压缩映射 (contraction mapping)**（即 Lipschitz 常数 $q  1$），那么它在完备的[希尔伯特空间](@entry_id:261193)中存在唯一的[不动点](@entry_id:156394)，并且迭代序列 $x^{k+1} = T_{\theta}(x^k)$ 从任何初始点出发都会强收敛到该[不动点](@entry_id:156394) 。
2.  **克拉斯诺谢尔斯基-曼恩定理 (Krasnosel'skii-Mann Theorem)**：如果 $T_{\theta}$ 是**非扩张的 (nonexpansive)**（即 Lipschitz 常数为 1），并且其[不动点](@entry_id:156394)集非空，那么松弛迭代 $x^{k+1} = (1-\lambda_k)x^k + \lambda_k T_{\theta}(x^k)$ 在适当的条件下会[弱收敛](@entry_id:146650)到一个[不动点](@entry_id:156394)。这为分析许多基于非扩张组件（如[ReLU激活函数](@entry_id:138370)和[谱归一化](@entry_id:637347)的线性层）构建的现代网络架构提供了理论保证 。

#### 即插即用方法（PnP）与[去噪](@entry_id:165626)正则化（RED）

在展开迭代法的框架下，**即插即用 (Plug-and-Play, PnP)** 方法是一个极具影响力的特例。它通常基于**交替方向乘子法 (Alternating Direction Method of Multipliers, [ADMM](@entry_id:163024))** 等分裂算法。对于[优化问题](@entry_id:266749) $\min_x f(x) + g(x)$，ADMM 的一个核心步骤是计算正则化项 $g(x)$ 的**[近端算子](@entry_id:635396) (proximal operator)**。[PnP-ADMM](@entry_id:753534) 的核心思想是用一个现成的[图像去噪](@entry_id:750522)器 $D_{\sigma}$ 来替换这个[近端算子](@entry_id:635396)步骤 ：
$$
v^{k+1} = \operatorname{prox}_{g/\rho}(z^k) \quad \longrightarrow \quad v^{k+1} = D_{\sigma}(z^k)
$$
这里的直觉是，[近端算子](@entry_id:635396)本身可以被解释为一种[去噪](@entry_id:165626)过程。通过插入一个强大的、由[神经网](@entry_id:276355)络实现的[去噪](@entry_id:165626)器，我们可以隐式地引入一个复杂的图像先验。PnP 方法的优势在于其灵活性，允许将任何先进的去噪器与各种数据保真项结合。其[收敛性分析](@entry_id:151547)同样依赖于[算子理论](@entry_id:139990)，特别是当去噪器 $D_{\sigma}$ 被证明或约束为非扩张的时 。

与PnP的“隐式先验”思想不同，**去噪正则化 (Regularization by Denoising, RED)** 寻求从一个[去噪](@entry_id:165626)器 $D_{\sigma}$ 出发，构建一个显式的正则化能量函数 $r(x)$。其关键假设是正则化项的梯度可以由[去噪](@entry_id:165626)残差来定义 ：
$$
\nabla r(x) = \frac{1}{\sigma^2} (x - D_{\sigma}(x))
$$
在 $D_{\sigma}$ 的雅可比矩阵满足对称性等条件下，这个[梯度场](@entry_id:264143)是可积的，从而定义了一个显式的正则化项 $r(x)$。这样，整个逆问题就可以被表述为一个清晰的[优化问题](@entry_id:266749) $\min_x f(x) + r(x)$。PnP 和 RED 的[不动点](@entry_id:156394)条件通常不相同，它们代表了两种利用[去噪](@entry_id:165626)先验的不同哲学。

#### 分期推断与[基于模拟的推断](@entry_id:754873)

对于许多复杂的科学问题，[似然函数](@entry_id:141927) $p(y \mid u)$ 可能难以评估甚至没有解析形式，但我们能够从前向模型 $y \sim G(u)$ 中进行模拟。在这种情况下，**[基于模拟的推断](@entry_id:754873) (Simulation-Based Inference, SBI)** 或称**[无似然推断](@entry_id:190479) (Likelihood-Free Inference)** 方法变得至关重要。

这类方法通过运行大量模拟来生成训练数据对 $(u_i, y_i)$，然后训练一个[神经网](@entry_id:276355)络来学习参数和观测之间的关系。主要有两种策略：
*   **[神经后验估计](@entry_id:752449) (Neural Posterior Estimation, NPE)**：直接学习一个参数化的[后验分布近似](@entry_id:753632) $q_{\phi}(u \mid y)$。
*   **神经似然估计 (Neural Likelihood Estimation, NLE)**：学习一个[参数化](@entry_id:272587)的似然函数近似 $q_{\phi}(y \mid u)$。

训练完成后，这些网络提供了一种**分期推断 (amortized inference)** 的能力：对于任何新的观测 $y_{\text{obs}}$，我们可以非常快速地获得[后验分布](@entry_id:145605)的近似或从中采样，而无需再次运行昂贵的前向模拟器。这与 MCMC 等序贯方法形成鲜明对比，后者对每个新的 $y_{\text{obs}}$ 都需要从头开始进行昂贵的迭代计算。

在处理具有挑战性的动力学系统（如混沌的 Lorenz-96 模型）的[参数推断](@entry_id:753157)时，SBI 的优势尤为突出。在长时程积分下，这类系统的[似然函数](@entry_id:141927)表面充满了剧烈[振荡](@entry_id:267781)和[局部极值](@entry_id:144991)，导致基于梯度的 MCMC 方法（即使有精确的伴随梯度）也难以混合和收敛。SBI 通过学习一个平滑的、摊销的后验或似然近似，有效地绕过了这些不稳定的梯度计算，从而在有限的计算预算下可能获得更可靠的后验估计 。当然，为了应对混沌带来的挑战，一些传统方法也可以采用似然[回火](@entry_id:182408)（likelihood tempering）或缩短[数据同化](@entry_id:153547)窗口等策略来平滑[目标分布](@entry_id:634522)，但这通常以引入偏差为代价 。

#### [物理信息神经网络](@entry_id:145229)（PINN）及其挑战

**物理信息神经网络 (Physics-Informed Neural Networks, [PINNs](@entry_id:145229))** 采用了一种截然不同的策略。它不学习先验或求解器，而是直接将解 $u$ 参数化为一个[神经网](@entry_id:276355)络 $\hat{u}_{\phi}(x)$。网络的参数 $\phi$（以及可能存在的未知 PDE 系数 $\theta$）通过最小化一个复合[损失函数](@entry_id:634569)来训练，该损失函数包括：
1.  **数据保真项**：在观测点上，网络输出 $\hat{u}_{\phi}(x_i)$ 与观测数据 $y_i$ 的差异。
2.  **物理残差项**：在从定义域中采样的[配置点](@entry_id:169000) (collocation points) 上，强制网络满足控制性[偏微分方程](@entry_id:141332)（PDE）的程度。
3.  **边界/初始条件项**。

[PINNs](@entry_id:145229) 的吸[引力](@entry_id:175476)在于其无网格的特性，以及通过[自动微分](@entry_id:144512)直接计算 PDE 残差的能力。然而，将 PDE 系数反演问题嵌入 PINN 框架（即同时优化 $\phi$ 和 $\theta$）会带来独特的挑战。一个核心问题是**梯度不匹配 (gradient mismatch)** 。在理想情况下，我们希望沿着真实简化目标 $\tilde{J}(\theta) = \min_{\phi} L(\theta, \phi)$ 的梯度方向更新 $\theta$。根据包络定理，该梯度应为 $\nabla_{\theta} \tilde{J}(\theta) = \nabla_{\theta} L(\theta, \phi^*(\theta))$，其中 $\phi^*(\theta)$ 是给定 $\theta$ 时内部优化的最优解。然而，在联合优化方案中，我们实际使用的是在当前非最优的 $\phi$ 处的偏导数 $\nabla_{\theta} L(\theta, \phi)$。这个梯度与真实梯度之间的差异，以及由有限[配置点](@entry_id:169000)引起的[离散化误差](@entry_id:748522)，都可能导致优化过程不稳定或收敛到错误的解。

此外，PINN 中[神经网](@entry_id:276355)络 $\hat{u}_{\phi}$ 的函数空间的[表达能力](@entry_id:149863)也可能影响参数的**[可辨识性](@entry_id:194150) (identifiability)**。即使原始连续问题是可辨识的，如果网络无法精确地表示解对参数 $\theta$ 的真实敏感度方向，这些方向在网络可表示的[函数空间](@entry_id:143478)上的投影可能会变得线性相关甚至为零，从而导致离散化后的问题失去[可辨识性](@entry_id:194150) 。采用弱形式的 PINN 并通过[双层优化](@entry_id:637138)来求解内部问题，是缓解这些问题、使 PINN 方法更加稳健的有效途径 。

### 深入理论探讨

除了具体的算法[范式](@entry_id:161181)，机器学习在[逆问题](@entry_id:143129)中的应用还引发了一系列深刻的理论问题，这些问题对于确保方法的可靠性和有效性至关重要。

#### 不确定性量化：近似贝叶斯方法与[隐式正则化](@entry_id:187599)

在许多高风险应用中，仅仅提供一个[点估计](@entry_id:174544)是远远不够的；对解的不确定性进行量化至关重要。虽然完全贝叶斯方法是黄金标准，但其计算成本高昂。因此，研究者开发了多种**近似贝叶斯方法**。

其中，**[蒙特卡洛](@entry_id:144354) dropout (MC dropout)** 是一种广受欢迎的技术，它通过在测试时多次启用 dropout 并进行[前向传播](@entry_id:193086)，从而得到一系列不同的预测结果，并将这些结果的离散程度解释为模型的不确定性。然而，必须谨慎对待这种解释。在一个标准的线性高斯逆问题中，我们可以精确推导出真实的贝叶斯后验协[方差](@entry_id:200758) $\Gamma_{\text{post}}$。通过分析可以发现，由 MC dropout 产生的经验协[方差](@entry_id:200758)在结构上与 $\Gamma_{\text{post}}$ 根本不兼容：前者是对角、依赖于具体观测数据的，而后者通常是稠密、且不依赖于观测数据的 。这揭示了一个深刻的教训：尽管 dropout 可以捕捉到某种形式的[模型不确定性](@entry_id:265539)，但它并不等同于、甚至不总是真实贝叶斯后验不确定性的良好近似。

另一个有趣的不确定性来源是算法本身。例如，在训练一个展开求解器时使用带噪声的[随机梯度下降](@entry_id:139134)（SGD），其更新规则可以看作是一个随机微分方程（SDE）的离散化。具体来说，更新步 $x^{k+1} = x^k - \eta \nabla f(x^k) + s z^k$ (其中 $z^k$ 是[高斯噪声](@entry_id:260752)) 在连续时间极限下对应于一个[朗之万动力学](@entry_id:142305)过程。这个过程的[平稳分布](@entry_id:194199)不是原始的[后验分布](@entry_id:145605) $p(x \mid y) \propto \exp(-f(x))$，而是一个**[回火](@entry_id:182408)的 (tempered)** [后验分布](@entry_id:145605) $p_{\infty}(x \mid y) \propto \exp(-f(x)/\tau)$，其中[有效温度](@entry_id:161960) $\tau = s^2 / (2\eta)$ 。这个结果表明，算法的选择（步长 $\eta$ 和噪声水平 $s$）隐式地定义了一个[统计模型](@entry_id:165873)。这为我们通过调整优化器参数来控制解的随机性提供了一个理论依据，这种现象被称为**[隐式正则化](@entry_id:187599) (implicit regularization)**。

#### 对称性、可辨识性与[等变网络](@entry_id:143881)

许多物理问题天然地包含对称性。例如，在相位恢复问题中，信号 $u$ 和 $-u$ 产生完全相同的观测；在未知原点的反卷积问题中，平移后的信号 $u(\cdot - \tau)$ 与原信号无法区分。当正向算子 $G$ 在某个群 $\mathcal{H}$ 的作用下具有不变性（即 $G(h \cdot u) = G(u)$）时，逆问题从根本上就是非唯一的。我们最多只能期望从观测 $y$ 中恢复出 $u$ 所属的整个**[轨道](@entry_id:137151) (orbit)** $\mathcal{H} \cdot u = \{h \cdot u : h \in \mathcal{H}\}$ 。

直接用标准[损失函数](@entry_id:634569)（如[均方误差](@entry_id:175403)）训练一个[神经网](@entry_id:276355)络来解决这类问题，会导致模型在不同模式（如 $u$ 和 $-u$）之间摇摆不定，最终可能收敛到所有模式的平均值（如 0），这显然不是期望的解 。处理对称性的现代机器学习方法包括：
1.  **学习[商空间](@entry_id:274314)表示**：将学习目标从 $u$ 变为一个对群作用不变的量。例如，对于符号对称性，可以直接预测[秩一矩阵](@entry_id:199014) $X = u u^T$，它对于 $u \to -u$ 是不变的。这从根本上消除了模式模糊性 。
2.  **[轨道](@entry_id:137151)不变损失函数**：修改损失函数，使其惩罚的是预测值与真实值[轨道](@entry_id:137151)之间的距离，而非特定代表元之间的距离。例如，使用 $L(\hat{u}, u) = \min_{h \in \mathcal{H}} \|\hat{u} - h \cdot u\|^2$。
3.  **等变架构 (Equivariant Architectures)**：当正向算子 $G$ 是等变的（即 $G(h \cdot u) = h \cdot G(u)$）时，设计一个同样满足[等变性](@entry_id:636671)的网络 $f(h \cdot y) = h \cdot f(y)$ 是一种强大的[归纳偏置](@entry_id:137419)。例如，[卷积神经网络](@entry_id:178973) (CNNs) 天然地具有[平移等变性](@entry_id:636340)，使其成为处理涉及[平移对称性](@entry_id:171614)的图像问题的理想选择 。

#### [函数空间](@entry_id:143478)视角：离散化不变性与后验收缩率

在处理函数（如图像或物理场）等无限维对象的逆问题时，一个关键的理论要求是**离散化[不变性](@entry_id:140168) (discretization invariance)**。这意味着我们定义的先验应该在某个连续的[函数空间](@entry_id:143478)中有良好的定义，而我们在不同离散化水平下看到的先验分布，应该是这个统一的连续先验在相应离散[子空间](@entry_id:150286)上的投影。

在数学上，这可以用**投影一致性 (projective consistency)** 来刻画。一个离散先验族 $\{\mu_h\}$ 如果是投影一致的，那么存在一个唯一的连续测度 $\mu$，使得每个 $\mu_h$ 都是 $\mu$ 在相应[子空间](@entry_id:150286)上的投影。基于[随机偏微分方程](@entry_id:188292)（SPDE）解构建的[高斯过程](@entry_id:182192)先验，天然满足这一属性 。然而，许多简单的[深度生成模型](@entry_id:748264)，其架构（如[上采样](@entry_id:275608)层）与特定离散化水平绑定，往往会破坏这种一致性。设计能够跨分辨率保持一致性的[生成模型](@entry_id:177561)是当前的一个重要研究方向。离散化[不变性](@entry_id:140168)保证了当我们将模型应用于更高分辨率时，我们仍然在解决同一个潜在的连续问题，而不是一个全新的、不相关的问题 。

最后，先验的选择直接影响了贝叶斯推断的[统计效率](@entry_id:164796)，这可以通过**后验收缩率 (posterior contraction rate)** 来衡量。在不适定逆问题中，经典的[光滑性](@entry_id:634843)先验（如[高斯过程](@entry_id:182192)先验）通常会导致收缩率随着样本量 $n$ 的增加而呈现多项式慢速收敛，其速率由信号的平滑度和问题的不适定程度共同决定，例如 $n^{-\alpha/(2\alpha+2s+1)}$ 。然而，如果一个[深度生成先验](@entry_id:748265)能够有效地将[解空间](@entry_id:200470)限制在一个低维[流形](@entry_id:153038)上，并且正向算子在该[流形](@entry_id:153038)上是局部可辨识的，那么问题就从一个非参数问题转化为一个参数化问题。在这种理想情况下，后验收缩率可以达到更快的[参数化](@entry_id:272587)速率 $n^{-1/2}$，从而在理论上克服了[不适定性](@entry_id:635673)带来的“诅咒”。这为使用[深度生成模型](@entry_id:748264)解决[逆问题](@entry_id:143129)提供了强有力的理论动机。