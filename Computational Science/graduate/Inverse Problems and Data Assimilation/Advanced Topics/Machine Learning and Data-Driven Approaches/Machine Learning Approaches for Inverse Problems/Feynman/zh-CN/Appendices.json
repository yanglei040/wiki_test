{
    "hands_on_practices": [
        {
            "introduction": "将经典的迭代优化算法（如近端梯度法）“展开”成一个深度神经网络架构，是连接传统逆问题方法与现代深度学习模型的强大范式。本练习旨在揭示这一过程的内在机理。通过推导，您将理解网络层和可训练参数如何隐式地学习一个谱滤波器，从而将深度学习的设计与经典的正则化理论联系起来，加深对学习迭代方法的理解。",
            "id": "3399544",
            "problem": "考虑线性逆问题 $y = A x^{\\star} + \\eta$，其中 $A \\in \\mathbb{R}^{m \\times n}$，未知目标为 $x^{\\star} \\in \\mathbb{R}^{n}$，加性噪声 $\\eta \\in \\mathbb{R}^{m}$ 满足已知边界 $\\|\\eta\\|_{2} \\leq \\delta$。假设数据保真度目标源自平方残差和二次惩罚，因此正则化成本为 $J(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\mu \\|x\\|_{2}^{2}$，其中正则化权重 $\\mu > 0$。考虑一个具有 $L$ 层的展开式近端梯度（PG）网络，其第 $k$ 层执行更新\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right), \\quad x^{0} = 0,\n$$\n其中 $\\alpha_{k} > 0$ 是一个可训练的步长，$\\mathrm{prox}_{\\gamma R}(v)$ 表示在 $v$ 处评估的 $\\gamma R$ 的近端算子。您可以将近端算子的定义和奇异值分解（SVD）的标准属性视为已知。目标是分析由可训练参数引起的隐式谱滤波，并推导出一个有原则的早停规则。\n\n您的任务：\n- 从近端微积分和线性代数的第一性原理出发，推导关于对称矩阵 $M = A^{\\top} A$ 的逐层更新，并证明该网络定义了从 $A^{\\top} y$ 到 $x^{L}$ 的线性映射。\n- 利用 $M$ 的特征分解和网络的线性性质，推导标量谱滤波器 $g_{L}(\\lambda)$，使得对于 $M$ 的任意特征对 $(\\lambda, u)$（满足 $M u = \\lambda u$），$x^{L}$ 沿 $u$ 的分量为 $g_{L}(\\lambda)\\,(u^{\\top} A^{\\top} y)$。在本部分中，假设可训练参数被选择为层常数，即 $\\alpha_{k} \\equiv \\alpha$ 且 $\\mu$ 在各层中固定。\n- 从数据模型和迭代的谱表示出发，为早停制定一个差异原则。也就是说，提出一个规则来选择最小的 $L$，使得残差 $\\|A x^{L} - y\\|_{2}$ 近似匹配噪声水平乘以一个安全因子 $\\tau > 1$，并解释为什么这能保证对于 $y$ 中扰动的稳定性。\n- 在层常数选择 $\\alpha_{k} \\equiv \\alpha$ 和固定 $\\mu$ 的条件下，以封闭形式表达式给出 $g_{L}(\\lambda)$ 的最终答案。不需要进行数值评估。\n\n所有数学表达式均以 LaTeX 表示。最终答案必须是单一的封闭形式解析表达式。不包含任何单位。",
            "solution": "按照要求，问题分四部分进行分析和解决。\n\n首先，我们推导关于矩阵 $M = A^{\\top} A$ 的逐层更新，并证明网络映射的线性性。\n第 $k$ 层的更新由下式给出\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right)\n$$\n近端算子 $\\mathrm{prox}_{\\gamma R}(v)$ 定义为一个优化问题的解：\n$$\n\\mathrm{prox}_{\\gamma R}(v) = \\arg\\min_{z} \\left\\{ \\gamma R(z) + \\frac{1}{2}\\|z-v\\|_{2}^{2} \\right\\}\n$$\n在我们的情况下，正则化函数是 $R(x) = \\|x\\|_{2}^{2}$，参数是 $\\gamma = \\alpha_k \\mu$。最小化目标是\n$$\nf(z) = \\alpha_{k} \\mu \\|z\\|_{2}^{2} + \\frac{1}{2}\\|z-v\\|_{2}^{2}\n$$\n这是一个严格凸函数。我们通过将关于 $z$ 的梯度设为零来找到最小值：\n$$\n\\nabla_{z} f(z) = 2 \\alpha_{k} \\mu z + (z - v) = 0\n$$\n$$\n(1 + 2 \\alpha_{k} \\mu) z = v \\implies z = \\frac{1}{1 + 2 \\alpha_{k} \\mu} v\n$$\n因此，近端算子是一个简单的缩放：$\\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}(v) = (1 + 2 \\alpha_{k} \\mu)^{-1} v$。\n将此代回 $x^{k+1}$ 的更新规则，我们得到：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right)\n$$\n让我们引入对称矩阵 $M = A^{\\top} A$。更新变为：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} (M x^{k} - A^{\\top}y)\\right)\n$$\n重新排列各项，我们得到用 $M$ 表示的更新规则：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left( (I - \\alpha_{k} M) x^{k} + \\alpha_{k} A^{\\top}y \\right)\n$$\n这是一个仿射递推关系。为了证明最终输出 $x^L$ 是 $A^{\\top}y$ 的线性函数，我们从初始条件 $x^0 = 0$ 开始展开递推：\n对于 $k=0$：\n$$\nx^{1} = \\frac{1}{1 + 2 \\alpha_{0} \\mu} \\left( (I - \\alpha_{0} M) x^{0} + \\alpha_{0} A^{\\top}y \\right) = \\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\n$$\n对于 $k=1$：\n$$\nx^{2} = \\frac{1}{1 + 2 \\alpha_{1} \\mu} \\left( (I - \\alpha_{1} M) x^{1} + \\alpha_{1} A^{\\top}y \\right) = \\frac{I - \\alpha_{1} M}{1 + 2 \\alpha_{1} \\mu} \\left(\\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\\right) + \\frac{\\alpha_{1}}{1 + 2 \\alpha_{1} \\mu} A^{\\top}y\n$$\n这可以写成 $x^{2} = (Q_1 + Q_2) A^{\\top}y$，其中 $Q_1$ 和 $Q_2$ 是矩阵。将此过程继续到第 $L$ 层，$x^L$ 将是各项之和，其中每一项都是涉及 $M$ 和 $\\alpha_k$ 的矩阵乘积，作用于一个与 $A^{\\top}y$ 成比例的向量。$x^L$ 的最终表达式形式为：\n$$\nx^L = G_L(\\alpha_0, \\dots, \\alpha_{L-1}, \\mu, M) A^{\\top}y\n$$\n其中 $G_L$ 是参数的矩阵值函数。这表明对于固定的参数，展开的网络定义了从 $A^{\\top}y$ 到 $x^L$ 的线性映射。\n\n其次，我们推导在层常数参数（即 $\\alpha_k \\equiv \\alpha$）情况下的标量谱滤波器 $g_{L}(\\lambda)$。\n更新规则简化为：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha \\mu} \\left( (I - \\alpha M) x^{k} + \\alpha A^{\\top}y \\right)\n$$\n令 $C = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)$ 和 $d = \\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y$。更新为 $x^{k+1} = C x^{k} + d$。\n从 $x^0=0$ 开始，我们有 $x^L = \\left(\\sum_{k=0}^{L-1} C^k\\right) d$。\n设 $(\\lambda, u)$ 是 $M$ 的一个特征对，因此 $M u = \\lambda u$。向量 $u$ 也是 $C$ 的一个特征向量：\n$$\nC u = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)u = \\frac{1}{1 + 2 \\alpha \\mu}(u - \\alpha \\lambda u) = \\left(\\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}\\right) u\n$$\n让我们将与 $\\lambda$ 对应的 $C$ 的特征值表示为 $\\gamma(\\lambda) = \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}$。\n算子 $\\sum_{k=0}^{L-1} C^k$ 是一个矩阵的几何级数。它对特征向量 $u$ 的作用是乘以标量 $\\sum_{k=0}^{L-1} \\gamma(\\lambda)^k = \\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}$。\n让我们将方程投影到特征向量 $u$ 上。$x^L$ 沿 $u$ 的分量是 $u^{\\top}x^L$。\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}\\right) u^{\\top}d\n$$\n代入 $d$ 和 $\\gamma(\\lambda)$ 的表达式：\n$$\nu^{\\top}d = u^{\\top} \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y\\right) = \\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\n$$\n$$\n1 - \\gamma(\\lambda) = 1 - \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu} = \\frac{(1 + 2 \\alpha \\mu) - (1 - \\alpha \\lambda)}{1 + 2 \\alpha \\mu} = \\frac{\\alpha \\lambda + 2 \\alpha \\mu}{1 + 2 \\alpha \\mu} = \\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}\n$$\n结合这些，$x^L$ 沿 $u$ 的分量为：\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{\\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}}\\right) \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\\right) = (1 - \\gamma(\\lambda)^L) \\frac{1}{\\lambda + 2\\mu} (u^{\\top} A^{\\top}y)\n$$\n问题要求标量滤波器 $g_L(\\lambda)$，使得 $x^L$ 沿 $u$ 的分量为 $g_L(\\lambda) (u^{\\top}A^{\\top}y)$。从推导的表达式中，我们可以确定滤波器为：\n$$\ng_L(\\lambda) = \\frac{1}{\\lambda + 2\\mu} \\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)\n$$\n\n第三，我们为早停制定一个差异原则，并解释其提供的稳定性。\nMorozov 差异原则是选择正则化参数的一种成熟方法。在迭代方法的背景下，迭代次数 $L$ 充当正则化参数。\n提出的规则是：给定数据 $y$、噪声界限 $\\delta$ 和安全因子 $\\tau > 1$，选择层数（迭代次数）$L$ 为满足残差范数条件的最小整数 $k \\geq 1$：\n$$\n\\|A x^{k} - y\\|_{2} \\leq \\tau \\delta\n$$\n该规则通过计算网络每一层的残差并在满足此标准的第一个层停止来实现。\n\n该原则保证稳定性的原因如下：\n逆问题是病态的，这意味着 $A$ 的小奇异值在朴素反演中会导致噪声放大。展开网络的迭代过程充当正则化方案。迭代次数 $L$ 控制正则化的程度。\n在初始迭代中（$L$ 较小），该方法重建解 $x^{\\star}$ 中对应于 $A$ 的大奇异值的分量，这些分量受噪声影响较小。在此阶段，残差 $\\|A x^{k} - y\\|_{2}$ 很大，主要由尚未重建的信号部分 $A(x^{\\star}-x^k)$ 主导。\n随着 $k$ 的增加，$x^k$ 越来越接近 $x^{\\star}$，残差范数减小，最终接近噪声的范数，因为 $Ax^k - y = A(x^k - x^{\\star}) - \\eta$。当残差与噪声水平 $\\delta$ 相当时，差异原则停止迭代。\n如果迭代继续超过这一点，算法将试图进一步减小残差。由于残差已经由噪声 $\\eta$ 主导，这意味着将模型拟合到噪声上。拟合噪声需要算法重建对应于小奇异值的分量，而这些分量被严重污染。这会导致解 $x^k$ 中噪声分量的大幅放大，从而破坏重建并导致不稳定。\n通过早停，差异原则防止算法进入这种噪声拟合状态。对于所选的 $L$，有效的谱滤波器 $g_L(\\lambda)$ 适当地抑制了对应于小奇异值（其中 $\\lambda = \\sigma^2$ 很小）的分量，从而防止了噪声的放大。这确保了从扰动数据 $y$ 到解 $x^L$ 的映射是一个有界算子，这正是稳定性的定义。\n\n最后，以谱滤波器 $g_{L}(\\lambda)$ 的封闭形式表达式作为最终答案。",
            "answer": "$$\n\\boxed{\\frac{1}{\\lambda + 2\\mu}\\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)}\n$$"
        },
        {
            "introduction": "对于任何学习到的逆问题求解器而言，稳定性是其是否可靠的关键——输入数据的微小扰动不应导致重建结果的巨大变化。本练习将演示如何在神经网络中通过谱归一化来强制施加稳定性，以控制其全局 Lipschitz 常数。您将把这一网络属性与由正向算子的奇异值所量化的逆问题内在难度联系起来。",
            "id": "3399532",
            "problem": "给定一个线性正演模型，其观测值由 $y = A x$ 定义，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的。考虑使用一个 $K$ 层前馈神经网络来学习一个逆映射 $\\Phi: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$，其形式为\n$$\n\\Phi(y) \\;=\\; W_{K} \\,\\rho\\!\\left(W_{K-1} \\,\\rho\\!\\left(\\cdots \\rho\\!\\left(W_{1} y\\right)\\cdots\\right)\\right),\n$$\n其中每个 $W_{k} \\in \\mathbb{R}^{d_{k} \\times d_{k-1}}$ 是一个线性层，$\\rho$ 是一个 $1$-利普希茨（Lipschitz）的逐点非线性函数（例如，整流线性单元（ReLU））。模型强制执行谱归一化，使得每一层的谱范数（算子 $2$-范数）满足 $\\|W_{k}\\|_{2} \\le s_{k}$，其中 $s_{k}$ 是已知的正常数。\n\n1. 仅使用利普希茨连续性、算子范数的定义以及它们在函数复合下的性质，推导出一个紧上界 $L_{\\mathrm{sn}}$，使得对于所有的 $y_{1},y_{2} \\in \\mathbb{R}^{m}$，\n$$\n\\|\\Phi(y_{1})-\\Phi(y_{2})\\|_{2} \\;\\le\\; L_{\\mathrm{sn}} \\,\\|y_{1}-y_{2}\\|_{2}.\n$$\n用 $\\{s_{k}\\}_{k=1}^{K}$ 表示 $L_{\\mathrm{sn}}$。\n\n2. 设 $A$ 的奇异值分解（SVD）为 $A = U \\Sigma V^{\\top}$，其奇异值为 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r} > 0$，秩为 $r$。假设可识别性被限制在与不小于阈值 $\\tau>0$ 的奇异值相关联的右奇异向量所张成的子空间内，即子空间 $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$。仅使用 SVD 和 Moore-Penrose 伪逆（MPP）的性质，确定在 $\\mathcal{V}_{\\mathrm{id}}$ 上的任何精确右逆的最小利普希茨常数，并将其表示为 $\\mathcal{V}_{\\mathrm{id}}$ 内最小奇异值 $\\sigma_{\\min}^{\\mathrm{id}}$ 的函数。\n\n3. 在实践中，为了确保条件稳定性，同时不超过由正演算子 $A$ 引起的不可避免的放大效应，人们将 $\\Phi$ 的全局利普希茨界限定在以下水平\n$$\nL_{\\star} \\;=\\; \\min\\!\\big\\{L_{\\mathrm{sn}},\\, L_{\\mathrm{id}}\\big\\},\n$$\n其中 $L_{\\mathrm{id}}$ 是在第2部分中找到的、在 $\\mathcal{V}_{\\mathrm{id}}$ 上的精确右逆的最小利普希茨常数。给定以下具体值：\n- $K=3$，$s_{1}=2$，$s_{2}=1.5$，$s_{3}=1.2$，\n- $A$ 的奇异值为 $\\{10,\\, 2,\\, 0.5,\\, 0.01\\}$，可识别性阈值为 $\\tau=0.4$，\n计算 $L_{\\star}$。\n\n将最终答案表示为一个纯数字（无单位），并四舍五入到四位有效数字。",
            "solution": "该问题被验证为自洽的，其科学基础在于线性代数和神经网络理论，并且是适定的。解题过程按要求分为三个部分。\n\n### 第1部分：利普希茨界 $L_{\\mathrm{sn}}$ 的推导\n\n目标是找到神经网络 $\\Phi(y)$ 的利普希茨常数的一个紧上界。该网络是函数的复合。我们先为每一层定义函数。对于 $k \\in \\{1, 2, \\dots, K-1\\}$，令第 $k$ 层的变换为 $f_k(z) = \\rho(W_k z)$。对于最后一层，令 $f_K(z) = W_K z$。那么整个网络就是复合函数 $\\Phi(y) = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(y)$。\n\n复合函数的利普希茨常数不大于其各个组成函数的利普希茨常数的乘积。即，$L(g \\circ h) \\le L(g)L(h)$。我们将递归地应用此性质。\n\n首先，我们确定每个层函数 $f_k$ 的利普希茨常数。\n\n对于中间层 $k \\in \\{1, 2, \\dots, K-1\\}$，我们有 $f_k(z) = \\rho(W_k z)$。设 $z_1, z_2$ 是 $f_k$ 定义域中的两个向量。我们分析其差值的范数：\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 = \\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2\n$$\n问题陈述中提到，非线性函数 $\\rho$ 是一个逐点 $1$-利普希茨函数。对于任意两个向量 $u, v$，一个逐点 $1$-利普希茨函数满足 $\\|\\rho(u) - \\rho(v)\\|_2 \\le \\|u - v\\|_2$。令 $u = W_k z_1$，$v = W_k z_2$。应用此性质，我们得到：\n$$\n\\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2 \\le \\|W_k z_1 - W_k z_2\\|_2\n$$\n利用 $W_k$ 的线性性质和算子 $2$-范数（谱范数）的定义，我们有：\n$$\n\\|W_k z_1 - W_k z_2\\|_2 = \\|W_k (z_1 - z_2)\\|_2 \\le \\|W_k\\|_2 \\|z_1 - z_2\\|_2\n$$\n问题指明应用了谱归一化，使得 $\\|W_k\\|_2 \\le s_k$。因此，函数 $f_k$ 的利普希茨常数 $L_k$ 的上界为 $s_k$：\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 \\le s_k \\|z_1 - z_2\\|_2 \\implies L_k \\le s_k\n$$\n\n对于最后一层 $k=K$，函数为 $f_K(z) = W_K z$。这是一个线性映射。其利普希茨常数就是其算子范数，即 $L_K = \\|W_K\\|_2$。我们已知上界 $\\|W_K\\|_2 \\le s_K$，所以 $L_K \\le s_K$。\n\n现在，我们将各层复合起来，求整个网络 $\\Phi$ 的利普希茨常数。\n$$\nL_{\\Phi} = L(f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1) \\le L_K \\cdot L_{K-1} \\cdots L_1\n$$\n使用我们为每一层推导出的界：\n$$\nL_{\\Phi} \\le s_K \\cdot s_{K-1} \\cdots s_1 = \\prod_{k=1}^{K} s_k\n$$\n这个上界被记为 $L_{\\mathrm{sn}}$。该界被认为是紧的，因为对于特定的权重 $W_k$ 选择（其中 $\\|W_k\\|_2=s_k$）和特定的输入（例如，如果 $\\rho$ 是恒等函数，并且权重矩阵的主奇异向量是对齐的），这个界是可以达到的。\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{K} s_k\n$$\n\n### 第2部分：最小利普希茨常数 $L_{\\mathrm{id}}$ 的推导\n\n题目要求我们求出在子空间 $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$ 上的任何精确右逆的最小利普希茨常数。设这样一个逆映射为 $\\mathcal{G}: \\mathbb{R}^m \\to \\mathbb{R}^n$。在 $\\mathcal{V}_{\\mathrm{id}}$ 上作为精确右逆的条件意味着，对于任何向量 $x \\in \\mathcal{V}_{\\mathrm{id}}$，如果 $y = Ax$，那么 $\\mathcal{G}(y) = x$。\n\n设 $L_{\\mathcal{G}}$ 是这样一个映射 $\\mathcal{G}$ 的利普希茨常数。令 $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$，并设 $v_j$ 是与该奇异值对应的右奇异向量，因此 $Av_j = \\sigma_j u_j$，其中 $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$。由于 $\\sigma_j \\ge \\tau$，所以 $v_j \\in \\mathcal{V}_{\\mathrm{id}}$。\n我们用两个特定的点来检验利普希茨条件。令 $x_1 = v_j$ 和 $x_2=0$。两者都在 $\\mathcal{V}_{\\mathrm{id}}$ 中。对应的观测值为 $y_1 = Ax_1 = Av_j = \\sigma_j u_j$ 和 $y_2 = Ax_2 = 0$。\n右逆性质要求 $\\mathcal{G}(y_1) = x_1 = v_j$ 和 $\\mathcal{G}(y_2) = x_2 = 0$。\n利普希茨常数的定义要求：\n$$\n\\|\\mathcal{G}(y_1) - \\mathcal{G}(y_2)\\|_2 \\le L_{\\mathcal{G}} \\|y_1 - y_2\\|_2\n$$\n代入这些向量：\n$$\n\\|v_j - 0\\|_2 \\le L_{\\mathcal{G}} \\|\\sigma_j u_j - 0\\|_2\n$$\n奇异向量是标准正交的，所以 $\\|v_j\\|_2 = 1$ 且 $\\|u_j\\|_2 = 1$。该不等式变为：\n$$\n1 \\le L_{\\mathcal{G}} |\\sigma_j| \\cdot \\|u_j\\|_2 = L_{\\mathcal{G}} \\sigma_j\n$$\n由于 $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$，我们得到了对*任何*此类逆映射 $\\mathcal{G}$ 的利普希茨常数的一个下界：\n$$\nL_{\\mathcal{G}} \\ge \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\n这表明最小可能的利普希茨常数至少为 $1/\\sigma_{\\min}^{\\mathrm{id}}$。为了证明这个最小值是可达到的，我们必须构造一个满足右逆性质且其利普希茨常数恰好为此值的映射。\n\n考虑截断奇异值分解（TSVD）逆，其定义为：\n$$\n\\mathcal{G}_{\\mathrm{TSVD}} = \\sum_{i : \\sigma_i \\ge \\tau} \\frac{1}{\\sigma_i} v_i u_i^{\\top}\n$$\n这是一个从 $\\mathbb{R}^m$ 到 $\\mathbb{R}^n$ 的线性映射。它的算子范数（也就是它的利普希茨常数）由该映射的最大奇异值给出，即 $\\max_{i:\\sigma_i \\ge \\tau} (1/\\sigma_i) = 1/\\min_{i:\\sigma_i \\ge \\tau} (\\sigma_i) = 1/\\sigma_{\\min}^{\\mathrm{id}}$。\n我们来验证它在 $\\mathcal{V}_{\\mathrm{id}}$ 上是一个精确右逆。任何 $x \\in \\mathcal{V}_{\\mathrm{id}}$ 都可以写成 $x = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j$。那么 $y = Ax = \\sum_{j : \\sigma_j \\ge \\tau} c_j A v_j = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j$。\n将 $\\mathcal{G}_{\\mathrm{TSVD}}$ 作用于 $y$：\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\mathcal{G}_{\\mathrm{TSVD}}\\left(\\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\mathcal{G}_{\\mathrm{TSVD}}(u_j)\n$$\n根据 $\\mathcal{G}_{\\mathrm{TSVD}}$ 的定义和 $\\{u_i\\}$ 的正交性，有 $\\mathcal{G}_{\\mathrm{TSVD}}(u_j) = (1/\\sigma_j) v_j$。\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\left(\\frac{1}{\\sigma_j} v_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j = x\n$$\n该映射满足条件。既然我们找到了一个达到下界的映射，那么这就是最小可能的利普希茨常数。\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\n\n### 第3部分：$L_{\\star}$ 的计算\n\n我们已知具体数值，需要计算 $L_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\}$。\n\n首先，计算 $L_{\\mathrm{sn}}$：\n- $K=3$\n- $s_1=2$, $s_2=1.5$, $s_3=1.2$\n- 使用第1部分的公式：\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{3} s_k = s_1 \\cdot s_2 \\cdot s_3 = 2 \\times 1.5 \\times 1.2 = 3 \\times 1.2 = 3.6\n$$\n\n接着，计算 $L_{\\mathrm{id}}$：\n- $A$ 的奇异值：$\\{10, 2, 0.5, 0.01\\}$\n- 可识别性阈值：$\\tau = 0.4$\n- 我们必须找到 $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$。\n- 大于或等于 $\\tau=0.4$ 的奇异值是 $\\{10, 2, 0.5\\}$。\n- 这些值中的最小值是 $\\sigma_{\\min}^{\\mathrm{id}} = 0.5$。\n- 使用第2部分的公式：\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}} = \\frac{1}{0.5} = 2\n$$\n\n最后，计算 $L_{\\star}$：\n$$\nL_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\} = \\min\\{3.6, 2\\} = 2\n$$\n题目要求答案四舍五入到四位有效数字。\n$$\nL_{\\star} = 2.000\n$$",
            "answer": "$$\\boxed{2.000}$$"
        },
        {
            "introduction": "现实世界中的测量噪声通常不完全是高斯的，并且可能包含大的、零星的误差（即异常值）。本练习将探讨如何通过替换标准最小二乘数据保真项为鲁棒损失函数（如 Huber 或 Tukey 损失）来设计鲁棒的逆向方法。通过推导和分析影响函数，您将对这些估计器如何限制异常值的影响获得定量的理解。",
            "id": "3399542",
            "problem": "考虑一个带有加性噪声的线性逆问题，其中观测值 $y_i \\in \\mathbb{R}$ (对于 $i \\in \\{1,\\dots,n\\}$) 由一个已知的前向模型 $y_i = a\\,\\theta^{\\star} + \\varepsilon_i$ 生成，其中 $a \\in \\mathbb{R}\\setminus\\{0\\}$ 已知，$\\theta^{\\star} \\in \\mathbb{R}$ 是待估计的未知参数，$\\{\\varepsilon_i\\}_{i=1}^{n}$ 是独立同分布的噪声项，其分布关于零对称且具有已知尺度 $s>0$。要求您设计受机器学习启发的、适用于受离群值污染数据的鲁棒数据保真项，通过影响函数分析其鲁棒性，并将其与反演中的崩溃点联系起来。\n\n一个鲁棒数据保真估计量 $\\widehat{\\theta}$ 定义为\n$$\nJ(\\theta) \\triangleq \\sum_{i=1}^{n} \\rho\\!\\left(\\frac{y_i - a\\,\\theta}{s}\\right),\n$$\n的任何最小化子，其中 $\\rho$ 是一个鲁棒损失。两种广泛使用的鲁棒损失是调节常数为 $c>0$ 的 Huber 损失，\n$$\n\\rho_H(t) \\triangleq \\begin{cases}\n\\frac{1}{2}\\,t^2,  |t| \\le c,\\\\\nc\\,|t| - \\frac{1}{2}\\,c^2,  |t| > c,\n\\end{cases}\n$$\n以及调节常数为 $c>0$ 的 Tukey 双平方（双权重）损失，\n$$\n\\rho_T(t) \\triangleq \\begin{cases}\n\\frac{c^2}{6}\\left[1 - \\left(1 - \\left(\\frac{t}{c}\\right)^2\\right)^3\\right],  |t| \\le c,\\\\\n\\frac{c^2}{6},  |t| > c.\n\\end{cases}\n$$\n令 $\\psi(t) \\triangleq \\frac{d}{dt}\\rho(t)$ 表示与 $\\rho$ 相关联的得分函数。\n\n任务：\n- 从 M-估计量作为经验期望损失的最小化子的定义以及统计泛函的影响函数 (IF) 的定义出发，推导由 $J(\\theta)$ 在模型分布下导出的 $\\theta$ 估计量的影响函数。用 $a$、$s$、$\\psi$ 以及一个在噪声分布下涉及 $\\psi'$ 的期望来表示结果。\n- 对于 Huber 损失 $\\rho_H$ 和 Tukey 双平方损失 $\\rho_T$，计算它们相关联的得分函数 $\\psi_H$ 和 $\\psi_T$、它们的导数 $\\psi_H'$ 和 $\\psi_T'$，以及 $|\\psi_H(t)|$ 和 $|\\psi_T(t)|$ 在 $t \\in \\mathbb{R}$ 上的上确界。\n- 使用推导出的影响函数，讨论两种损失中哪一种具有再下降影响，并在这个已知尺度 $s$ 的单参数逆问题的背景下，解释对于每种 $\\rho$ 的选择，崩溃点如何表现。将其与 $s$ 与 $\\theta$ 被鲁棒地联合估计的情况进行对比。\n- 现在假设标准化噪声 $T \\triangleq \\varepsilon/s$ 是标准正态分布。将粗差敏感度 $\\gamma^{\\ast}$ 定义为 $\\theta$ 的 Huber-M 估计量的影响函数绝对值在所有可能污染点 $y \\in \\mathbb{R}$ 上的上确界。$\\gamma^{\\ast}$ 作为 $a$、$s$、$c$ 和标准正态累积分布函数 $\\Phi$ 的函数的闭式表达式是什么？您的最终答案必须是一个单一的解析表达式。最终答案中不要包含单位。",
            "solution": "该问题是有效的，因为它在科学上植根于鲁棒统计和 M-估计理论，是适定的，具有明确的目标，并为严谨的数学推导提供了所有必要的信息。\n\n### 任务 1：影响函数的推导\n\n参数 $\\theta$ 的 M-估计量 $\\widehat{\\theta}$ 定义为目标函数 $J(\\theta) = \\sum_{i=1}^{n} \\rho\\left(\\frac{y_i - a\\,\\theta}{s}\\right)$ 的最小化子。最小化的一阶条件是通过将 $J(\\theta)$ 对 $\\theta$ 的导数设为零来获得的。令 $\\psi(t) = \\frac{d}{dt}\\rho(t)$。\n$$\n\\frac{d J(\\theta)}{d\\theta} = \\sum_{i=1}^{n} \\psi\\left(\\frac{y_i - a\\,\\theta}{s}\\right) \\cdot \\left(-\\frac{a}{s}\\right) = 0.\n$$\n假设 $a \\neq 0$ 和 $s > 0$，这等价于估计方程：\n$$\n\\sum_{i=1}^{n} \\psi\\left(\\frac{y_i - a\\,\\theta}{s}\\right) = 0.\n$$\n估计量 $\\widehat{\\theta}$ 可以看作是在数据 $\\{y_i\\}_{i=1}^n$ 的经验分布 $F_n$ 上求值的统计泛函 $T$，即 $\\widehat{\\theta} = T(F_n)$。对于数据 $Y$ 的一般分布 $F$，泛函 $T(F)$ 由估计方程的总体版本隐式定义：\n$$\n\\mathbb{E}_F\\left[\\psi\\left(\\frac{Y - a\\,T(F)}{s}\\right)\\right] = 0.\n$$\n令 $F_0$ 为真实数据分布，其中 $Y = a\\theta^{\\star} + \\varepsilon$。那么 $T(F_0) = \\theta^{\\star}$，因为 $\\mathbb{E}_{F_0}\\left[\\psi\\left(\\frac{Y - a\\theta^{\\star}}{s}\\right)\\right] = \\mathbb{E}_{\\varepsilon}\\left[\\psi\\left(\\frac{\\varepsilon}{s}\\right)\\right] = 0$。最后一个等式成立是因为噪声分布关于零对称，并且 $\\psi$ 是一个奇函数（因为它是偶函数 $\\rho$ 的导数）。\n\n估计量 $T$ 在分布 $F_0$ 处对污染点 $y$ 的影响函数 (IF) 定义为：\n$$\n\\text{IF}(y; T, F_0) = \\left. \\frac{d}{d\\epsilon} T((1-\\epsilon)F_0 + \\epsilon \\delta_y) \\right|_{\\epsilon=0},\n$$\n其中 $\\delta_y$ 是在 $y$ 处的狄拉克测度。令 $F_{\\epsilon} = (1-\\epsilon)F_0 + \\epsilon \\delta_y$ 和 $\\theta_{\\epsilon} = T(F_{\\epsilon})$。$\\theta_{\\epsilon}$ 的定义方程为：\n$$\n\\mathbb{E}_{F_{\\epsilon}}\\left[\\psi\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\right] = (1-\\epsilon)\\mathbb{E}_{F_0}\\left[\\psi\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\right] + \\epsilon \\psi\\left(\\frac{y - a\\,\\theta_{\\epsilon}}{s}\\right) = 0.\n$$\n将此表达式对 $\\epsilon$ 求导并应用链式法则，得到：\n$$\n-\\mathbb{E}_{F_0}\\left[\\psi\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\right] + (1-\\epsilon)\\mathbb{E}_{F_0}\\left[\\psi'\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\left(-\\frac{a}{s}\\right)\\frac{d\\theta_{\\epsilon}}{d\\epsilon}\\right] + \\psi\\left(\\frac{y - a\\,\\theta_{\\epsilon}}{s}\\right) + \\epsilon \\frac{d}{d\\epsilon}\\left[\\dots\\right] = 0.\n$$\n我们现在在 $\\epsilon = 0$ 处对此进行评估。此时，$\\theta_{\\epsilon} = \\theta^{\\star}$ 且 $\\left.\\frac{d\\theta_{\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0} = \\text{IF}(y; T, F_0)$。第一项是 $\\mathbb{E}_{F_0}[\\psi((\\cdot)/s)] = 0$。方程简化为：\n$$\n\\mathbb{E}_{F_0}\\left[\\psi'\\left(\\frac{Y - a\\,\\theta^{\\star}}{s}\\right)\\left(-\\frac{a}{s}\\right)\\text{IF}(y; T, F_0)\\right] + \\psi\\left(\\frac{y - a\\,\\theta^{\\star}}{s}\\right) = 0.\n$$\n重新整理以求解影响函数，我们得到：\n$$\n\\frac{a}{s} \\text{IF}(y; T, F_0) \\mathbb{E}_{F_0}\\left[\\psi'\\left(\\frac{Y - a\\,\\theta^{\\star}}{s}\\right)\\right] = \\psi\\left(\\frac{y - a\\,\\theta^{\\star}}{s}\\right).\n$$\n期望可以相对于噪声分布来计算，因为 $Y - a\\theta^\\star = \\varepsilon$。令 $T_{noise} = \\varepsilon/s$ 为标准化噪声。\n$$\n\\text{IF}(y; T, F_0) = \\frac{s \\cdot \\psi\\left(\\frac{y - a\\theta^{\\star}}{s}\\right)}{a \\cdot \\mathbb{E}_{T_{noise}}\\left[\\psi'(T_{noise})\\right]}.\n$$\n\n### 任务 2：Huber 和 Tukey 损失的得分函数分析\n\n对于 Huber 损失 $\\rho_H(t)$：\n得分函数 $\\psi_H(t) = \\frac{d}{dt}\\rho_H(t)$ 是：\n$$\n\\psi_H(t) = \\begin{cases} t,  &|t| \\le c, \\\\ c \\cdot \\mathrm{sgn}(t),  &|t| > c. \\end{cases}\n$$\n这可以写作 $\\psi_H(t) = \\mathrm{clip}(t, -c, c) = \\max(-c, \\min(c, t))$。\n导数 $\\psi'_H(t)$ (在 $t \\neq \\pm c$ 处定义) 是：\n$$\n\\psi'_H(t) = \\begin{cases} 1,  &|t| < c, \\\\ 0,  &|t| > c. \\end{cases}\n$$\n得分函数绝对值的上确界是 $\\sup_{t \\in \\mathbb{R}} |\\psi_H(t)| = c$。\n\n对于 Tukey 双平方损失 $\\rho_T(t)$：\n当 $|t| \\le c$ 时，得分函数 $\\psi_T(t) = \\frac{d}{dt}\\rho_T(t)$ 是：\n$$\n\\psi_T(t) = \\frac{d}{dt} \\left(\\frac{c^2}{6}\\left[1 - \\left(1 - \\frac{t^2}{c^2}\\right)^3\\right]\\right) = \\frac{c^2}{6} \\left(-3\\left(1 - \\frac{t^2}{c^2}\\right)^2 \\left(-\\frac{2t}{c^2}\\right)\\right) = t\\left(1 - \\frac{t^2}{c^2}\\right)^2.\n$$\n所以，完整的得分函数是：\n$$\n\\psi_T(t) = \\begin{cases} t\\left(1 - \\left(\\frac{t}{c}\\right)^2\\right)^2,  &|t| \\le c, \\\\ 0,  &|t| > c. \\end{cases}\n$$\n当 $|t| < c$ 时，导数 $\\psi'_T(t)$ 是：\n$$\n\\psi'_T(t) = \\frac{d}{dt} \\left[t\\left(1 - \\frac{t^2}{c^2}\\right)^2\\right] = \\left(1 - \\frac{t^2}{c^2}\\right)^2 + t \\cdot 2\\left(1 - \\frac{t^2}{c^2}\\right)\\left(-\\frac{2t}{c^2}\\right) = \\left(1 - \\frac{t^2}{c^2}\\right)\\left(1 - \\frac{5t^2}{c^2}\\right).\n$$\n当 $|t|>c$ 时，$\\psi'_T(t)=0$。为了找到 $|\\psi_T(t)|$ 的上确界，我们通过令 $\\psi'_T(t)=0$ 来找到 $\\psi_T(t)$ 在 $[-c, c]$ 上的极值。临界点在 $t^2=c^2$ 和 $t^2=c^2/5$。在 $t=\\pm c$ 时，$\\psi_T(t)=0$。最大值出现在 $t = \\pm c/\\sqrt{5}$，即 $|\\psi_T(\\pm c/\\sqrt{5})| = \\frac{c}{\\sqrt{5}}\\left(1-\\frac{1}{5}\\right)^2 = \\frac{c}{\\sqrt{5}}\\left(\\frac{4}{5}\\right)^2 = \\frac{16c}{25\\sqrt{5}}$。\n因此，$\\sup_{t \\in \\mathbb{R}} |\\psi_T(t)| = \\frac{16c}{25\\sqrt{5}}$。\n\n### 任务 3：再下降影响和崩溃点\n\n影响函数与得分函数 $\\psi$ 成正比。\n如果当污染点 $|y| \\to \\infty$ 时，$\\text{IF}(y) \\to 0$，则称该估计量具有再下降影响函数。这意味着非常大的离群值被估计量完全拒绝。\n\n对于 Huber 损失，当 $|t|>c$ 时，$|\\psi_H(t)|$ 变为常数 ($c$)。因此，其影响函数是有界的，但不再下降至零。这意味着大的离群值对估计仍有恒定的、非零的影响。\n\n对于 Tukey 双平方损失，当 $|t|>c$ 时，$\\psi_T(t) = 0$。这意味着如果一个观测值 $y_i$ 离当前估计足够远（即 $|y_i - a\\theta|/s > c$），它将被完全忽略。因此，Tukey 估计量的影响函数是再下降的。\n\n崩溃点 (BDP) 是指能够导致估计量取任意大或无意义值的最小污染数据比例。在这个已知尺度 $s$ 的单参数估计问题中，Huber 和 Tukey M-估计量都拥有 50% 的最大崩溃点。任何单个数据点对估计方程的贡献都由 $\\sup|\\psi|$ 限定。为了使总和 $\\sum_i \\psi(\\dots)$ 变得不可控地大或小，必须污染至少一半的数据点，以“投票胜过”未被污染的一半。\n\n如果尺度 $s$ 未知并与 $\\theta$ 联合估计，情况会发生显著变化。单个大的离群值 $y_k \\to \\infty$ 可能导致尺度估计 $s$ 膨胀至无穷大以容纳该离群值。\n对于 Huber 估计量，如果 $s \\to \\infty$，所有标准化残差 $(y_i - a\\theta)/s$ 将趋近于零。在零附近的区域，$\\rho_H(t) \\propto t^2$，这意味着 Huber 估计量的行为将类似于标准的最小二乘估计量，其崩溃点为 $0$（或 $1/n$）。因此，其鲁棒性将丧失。\n对于像 Tukey 这样的再下降估计量，膨胀的尺度估计也可能存在问题，因为它将所有残差都移到了中心区域，在该区域 $\\rho_T(t) \\propto t^2$，同样会丧失鲁棒性。此外，由 $\\psi_T$ 的再下降性质引入的非凸性可能导致多个局部最小值，使得 $\\theta$ 和 $s$ 的联合估计在数值上不稳定。为了在尺度未知时保持高崩溃点，必须使用鲁棒的尺度估计量（如中位数绝对偏差，MAD），可以单独使用，也可以作为更复杂的估计方案（例如 S-估计）的一部分。\n\n### 任务 4：Huber 估计量的粗差敏感度\n\n粗差敏感度 $\\gamma^{\\ast}$ 定义为影响函数绝对值在所有可能污染点 $y$ 上的上确界：\n$$\n\\gamma^{\\ast} = \\sup_{y \\in \\mathbb{R}} |\\text{IF}(y; T, F_0)|.\n$$\n使用为 Huber 估计量推导出的影响函数：\n$$\n\\gamma^{\\ast} = \\sup_{y \\in \\mathbb{R}} \\left| \\frac{s \\cdot \\psi_H\\left(\\frac{y - a\\theta^{\\star}}{s}\\right)}{a \\cdot \\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right]} \\right| = \\frac{s \\cdot \\sup_{t \\in \\mathbb{R}}|\\psi_H(t)|}{|a| \\cdot \\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right]}.\n$$\n从任务 2 中，我们有 $\\sup_{t \\in \\mathbb{R}}|\\psi_H(t)| = c$。\n\n分母需要计算 $\\psi'_H(T_{noise})$ 的期望，其中标准化噪声 $T_{noise}=\\varepsilon/s$ 假定为标准正态分布，$T_{noise} \\sim \\mathcal{N}(0, 1)$。函数 $\\psi'_H(t)$ 在 $|t| < c$ 时为 $1$，否则为 $0$。因此，它是指示函数 $I(|t|<c)$。\n$$\n\\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right] = \\mathbb{E}_{T_{noise}}\\left[I(|T_{noise}| < c)\\right] = P(|T_{noise}| < c).\n$$\n对于标准正态变量 $Z \\sim \\mathcal{N}(0, 1)$，其累积分布函数为 $\\Phi(z)$，$P(|Z| < c) = P(-c < Z < c) = \\Phi(c) - \\Phi(-c) = \\Phi(c) - (1 - \\Phi(c)) = 2\\Phi(c) - 1$。\n\n将所有部分组合在一起，我们得到粗差敏感度的表达式：\n$$\n\\gamma^{\\ast} = \\frac{s \\cdot c}{|a| \\cdot (2\\Phi(c) - 1)}.\n$$",
            "answer": "$$\n\\boxed{\\frac{s c}{|a|(2\\Phi(c) - 1)}}\n$$"
        }
    ]
}