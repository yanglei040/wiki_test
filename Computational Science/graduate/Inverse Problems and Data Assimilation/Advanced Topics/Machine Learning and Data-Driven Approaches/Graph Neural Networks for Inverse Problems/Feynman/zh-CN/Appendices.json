{
    "hands_on_practices": [
        {
            "introduction": "将基于物理的逆问题与图神经网络相结合的第一步，通常是将物理系统本身表示为一个图。本练习通过对一个简单电路网络进行建模，为此提供了基础性实践。计算正向映射及其雅可比矩阵，是开发基于梯度的求解器和训练神经网络的关键技能 。",
            "id": "3386851",
            "problem": "考虑一个一维（$1$D）链式图，其节点索引为 $i \\in \\{0,1,\\dots,N\\}$，边为 $(i,i+1)$，其中 $i \\in \\{0,1,\\dots,N-1\\}$。每条边 $(i,i+1)$ 都有一个严格为正的电导 $w_{i,i+1} \\in \\mathbb{R}_{>0}$。定义边界（狄利克雷）电势为 $v_0 = V_L$ 和 $v_N = V_R$，其中 $V_L, V_R \\in \\mathbb{R}$ 是固定且已知的。内部节点的电压 $\\{v_i\\}_{i=1}^{N-1}$ 由图上的基尔霍夫电流定律和欧姆定律确定。设输入参数化为对数电导 $x \\in \\mathbb{R}^N$，其分量为 $x_i = \\ln w_{i,i+1}$，其中 $i \\in \\{0,1,\\dots,N-1\\}$。定义输出 $y \\in \\mathbb{R}^2$ 为边界电流向量 $y = [I_0, I_N]^{\\top}$，其中 $I_0$ 是在节点 $0$ 注入的净电流（流入网络为正），$I_N$ 是在节点 $N$ 注入的净电流（流入网络为正）。这定义了一个前向映射 $y = H(x)$，也就是该链式图的图论狄利克雷-诺伊曼映射。\n\n你正在设计一个图神经网络（GNN）代理模型来替代 $H(x)$，作为数据同化中一个反问题的一部分，其中需要相对于 $x$ 的梯度来进行反向传播和高斯-牛顿正则化反演。仅从边上的欧姆定律、节点上的基尔霍夫电流定律以及对数参数化强制的正性这些核心定义出发，显式地推导出离散前向映射 $y = H(x)$（用 $\\{w_{i,i+1}\\}$ 和 $\\{V_L,V_R\\}$ 表示），然后计算在任意 $x$ 处的雅可比矩阵 $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$ 的闭式解。你的最终雅可比矩阵必须只用 $\\{w_{i,i+1}\\}_{i=0}^{N-1}$、$V_L$ 和 $V_R$ 来表示。\n\n将你的最终雅可比矩阵以单一闭式解析表达式的形式给出。不要近似。不需要单位。不涉及角度。最终答案必须只包含所要求的表达式。",
            "solution": "首先根据所需准则验证问题陈述。\n\n### 问题验证\n\n#### 第 1 步：提取已知条件\n-   **图结构：** 一个一维（$1$D）链式图，其节点索引为 $i \\in \\{0,1,\\dots,N\\}$，边为 $(i,i+1)$，其中 $i \\in \\{0,1,\\dots,N-1\\}$。\n-   **边属性：** 每条边 $(i,i+1)$ 都有一个严格为正的电导 $w_{i,i+1} \\in \\mathbb{R}_{>0}$。\n-   **边界条件：** 狄利克雷边界电势被指定为 $v_0 = V_L$ 和 $v_N = V_R$，其中 $V_L, V_R \\in \\mathbb{R}$ 是固定且已知的。\n-   **内部节点物理学：** 内部节点电压 $\\{v_i\\}_{i=1}^{N-1}$ 由基尔霍夫电流定律和欧姆定律支配。\n-   **输入参数化：** 输入参数是对数电导 $x \\in \\mathbb{R}^N$，其分量为 $x_i = \\ln w_{i,i+1}$，其中 $i \\in \\{0,1,\\dots,N-1\\}$。\n-   **输出定义：** 输出是边界电流向量 $y = [I_0, I_N]^{\\top} \\in \\mathbb{R}^2$。\n-   **电流定义：** $I_0$ 是在节点 $0$ 注入的净电流（流入网络为正），$I_N$ 是在节点 $N$ 注入的净电流（流入网络为正）。\n-   **前向映射：** 输入和输出之间的关系由映射 $y = H(x)$ 表示。\n-   **目标：**\n    1.  显式地推导出前向映射 $y = H(x)$（用 $\\{w_{i,i+1}\\}$ 和 $\\{V_L, V_R\\}$ 表示）。\n    2.  计算雅可比矩阵 $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$ 的闭式解。\n    3.  最终的雅可比矩阵只用 $\\{w_{i,i+1}\\}_{i=0}^{N-1}$、$V_L$ 和 $V_R$ 来表示。\n\n#### 第 2 步：使用提取的已知条件进行验证\n-   **科学依据：** 该问题基于电路的基本原理，即 Ohm 定律和 Kirchhoff 电流定律。这些是公认的物理定律。对数参数化是在优化和反问题中强制正性约束的标准有效技术。该问题在科学上是合理的。\n-   **适定性：** 该问题描述了一个具有固定狄利克雷边界条件的线性电阻网络。由于所有电导 $w_{i,i+1}$ 都严格为正，电阻 $1/w_{i,i+1}$ 是有限正值。这保证了内部节点电势的线性方程组有唯一解。因此，前向映射 $H(x)$ 是良定义的，其雅可比矩阵是可计算的。该问题是适定的。\n-   **目标：** 问题使用精确的数学语言陈述，所有术语都明确定义。它没有任何主观或模棱两可的陈述。\n\n该问题不违反任何无效性准则。它是数学物理学中一个标准的、适定的问题，并且与数据同化和机器学习中的计算方法直接相关。\n\n#### 第 3 步：结论与行动\n问题有效。将继续进行求解过程。\n\n### 求解推导\n\n求解过程分为两部分：首先，确定前向映射 $y=H(x)$；其次，计算其雅可比矩阵 $\\frac{\\partial y}{\\partial x}$。\n\n#### 第 1 部分：前向映射 $y = H(x)$ 的推导\n\n该电路网络是一个简单的一维串联电路。根据 Ohm 定律，从节点 $i$ 流向节点 $i+1$ 的电流为 $I_{i \\to i+1} = w_{i,i+1} (v_i - v_{i+1})$，其中 $v_i$ 是节点 $i$ 的电压，$w_{i,i+1}$ 是边 $(i, i+1)$ 的电导。\n\n对于一个仅在边界（节点 $0$ 和 $N$）有源的一维链，Kirchhoff 电流定律在任何内部节点 $j \\in \\{1, \\dots, N-1\\}$ 处都意味着流入的电流等于流出的电流。这意味着整个链中的电流是恒定的。设这个恒定电流为 $I_{\\text{chain}}$。\n$$ I_{\\text{chain}} = I_{0 \\to 1} = I_{1 \\to 2} = \\dots = I_{N-1 \\to N} $$\n整个链上的总电压降为 $v_0 - v_N = V_L - V_R$。\n单个边 $(i, i+1)$ 的电阻是其电导的倒数：$R_{i,i+1} = 1/w_{i,i+1}$。\n该串联电路的总电阻 $R_{\\text{total}}$ 是各个电阻之和：\n$$ R_{\\text{total}} = \\sum_{k=0}^{N-1} R_{k,k+1} = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} $$\n现在可以使用 Ohm 定律计算整个电路的恒定链电流 $I_{\\text{chain}}$：\n$$ I_{\\text{chain}} = \\frac{V_L - V_R}{R_{\\text{total}}} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} $$\n输出向量 $y = [I_0, I_N]^{\\top}$ 由边界电流组成。\n-   $I_0$ 是在节点 $0$ 注入的净电流。该电流必须流入链中，因此它等于从节点 $0$ 流向节点 $1$ 的电流：$I_0 = I_{0 \\to 1} = I_{\\text{chain}}$。\n-   $I_N$ 是在节点 $N$ 注入的净电流。从链中流入节点 $N$ 的电流是 $I_{N-1 \\to N} = I_{\\text{chain}}$。根据“流入网络为正”的约定，$I_N$ 必须是在节点 $N$ 流出网络的电流的相反数。流出网络的电流是 $I_{\\text{chain}}$，所以 $I_N = -I_{\\text{chain}}$。\n\n因此，前向映射为：\n$$ y = H(x) = \\begin{pmatrix} I_0 \\\\ I_N \\end{pmatrix} = \\begin{pmatrix} I_{\\text{chain}} \\\\ -I_{\\text{chain}} \\end{pmatrix} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n该表达式用电导 $\\{w_{i,i+1}\\}$ 表示输出 $y$。\n\n#### 第 2 部分：雅可比矩阵 $\\frac{\\partial y}{\\partial x}$ 的计算\n\ny 相对于 x 的雅可比矩阵是一个 $2 \\times N$ 的矩阵，其元素为 $\\frac{\\partial y_m}{\\partial x_j}$，其中 $y_m$ 是 $y$ 的第 $m$ 个分量（$m \\in \\{1,2\\}$），$x_j$ 是 $x$ 的第 $j$ 个分量（$j \\in \\{0, \\dots, N-1\\}$）。分量为 $y_1 = I_0$ 和 $y_2 = I_N$。\n\n已知 $I_N = -I_0$，雅可比矩阵的第二行就是第一行的相反数：$\\frac{\\partial I_N}{\\partial x_j} = -\\frac{\\partial I_0}{\\partial x_j}$。我们只需要计算第一行，即导数 $\\frac{\\partial I_0}{\\partial x_j}$。\n\n我们使用链式法则来求 $I_0$ 相对于 $x_j$ 的导数：\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} $$\n问题定义了参数化 $x_j = \\ln w_{j,j+1}$，这意味着 $w_{j,j+1} = \\exp(x_j)$。因此，其导数为：\n$$ \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\exp(x_j) = w_{j,j+1} $$\n接下来，我们计算 $I_0$ 相对于任意电导 $w_{j,j+1}$ 的导数。令 $S = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}$。那么 $I_0 = (V_L - V_R)S^{-1}$。\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) \\frac{d(S^{-1})}{dS} \\frac{\\partial S}{\\partial w_{j,j+1}} $$\n这些导数是：\n$$ \\frac{d(S^{-1})}{dS} = -S^{-2} $$\n$$ \\frac{\\partial S}{\\partial w_{j,j+1}} = \\frac{\\partial}{\\partial w_{j,j+1}} \\left( \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} \\right) = -\\frac{1}{w_{j,j+1}^2} $$\n结合这些结果：\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) (-S^{-2}) \\left( -\\frac{1}{w_{j,j+1}^2} \\right) = \\frac{V_L - V_R}{S^2 w_{j,j+1}^2} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} $$\n现在我们可以求出 $\\frac{\\partial I_0}{\\partial x_j}$：\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\left( \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} \\right) w_{j,j+1} = \\frac{V_L - V_R}{w_{j,j+1} \\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} $$\n该表达式是雅可比矩阵的第 $(1, j+1)$ 个元素（使用从 $0$ 到 $N-1$ 的 $j$ 来索引列）。然后，通过组合 $j=0, 1, \\dots, N-1$ 的这些分量，可以构建出完整的雅可比矩阵 $\\frac{\\partial y}{\\partial x}$：\n$$ \\frac{\\partial y}{\\partial x} = \\begin{pmatrix} \\frac{\\partial I_0}{\\partial x_0}  \\frac{\\partial I_0}{\\partial x_1}  \\cdots  \\frac{\\partial I_0}{\\partial x_{N-1}} \\\\ \\frac{\\partial I_N}{\\partial x_0}  \\frac{\\partial I_N}{\\partial x_1}  \\cdots  \\frac{\\partial I_N}{\\partial x_{N-1}} \\end{pmatrix} $$\n代入推导出的表达式：\n$$ \\frac{\\partial y}{\\partial x} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}}  \\frac{1}{w_{1,2}}  \\cdots  \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}}  -\\frac{1}{w_{1,2}}  \\cdots  -\\frac{1}{w_{N-1,N}} \\end{pmatrix} $$\n这就是雅可比矩阵的最终闭式表达式，用所要求的变量表示。",
            "answer": "$$ \\boxed{\\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}}  \\frac{1}{w_{1,2}}  \\cdots  \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}}  -\\frac{1}{w_{1,2}}  \\cdots  -\\frac{1}{w_{N-1,N}} \\end{pmatrix}} $$"
        },
        {
            "introduction": "为逆问题构建图神经网络的一种强大技术是“展开”经典的迭代优化算法，其中网络的每一层都模仿一次迭代。本练习探讨了此设计的一个关键方面：确保学习到的求解器的稳定性和收敛性。通过分析迭代算子的谱半径，你将确定最优步长，从而将深度学习架构设计与数值分析的原理联系起来 。",
            "id": "3386845",
            "problem": "在一个用于图上线性逆问题的学习型展开图神经网络 (GNN) 中，考虑数据保真函数 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{H}\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$，其中 $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。该展开架构中的一层实现仿射迭代 $\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}$，其中 $\\alpha  0$ 是一个恒定步长。仅使用实对称矩阵的谱定理、谱半径的定义以及图拉普拉斯算子的基本性质，推导使线性算子 $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$ 在欧几里得范数下为严格收缩的关于 $\\alpha$ 的充要条件。\n\n将问题具体化到 $n = 3$ 的情况，其中 $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$，$\\mathbf{L}$ 是具有单位边权重的 3 节点路径图的组合拉普拉斯算子：\n$$\n\\mathbf{L} = \\begin{pmatrix}\n1  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  1\n\\end{pmatrix}.\n$$\n计算保证 $\\rho(\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})  1$ 的最大允许恒定步长 $\\alpha$。将最终的 $\\alpha$ 以精确数的形式报告，不要四舍五入。",
            "solution": "该问题被评估为有效。其科学基础在于数值线性代数和优化的原理，特别是求解线性系统的迭代方法的收敛性分析。该问题是适定的、客观的，并包含获得唯一解所需的所有信息。其前提是一致的；指定的矩阵 $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ 确实是对称正定的，因为组合拉普拉斯算子 $\\mathbf{L}$ 是对称半正定的，这意味着它的特征值 $\\lambda_i(\\mathbf{L}) \\ge 0$，这又意味着由 $1 + \\lambda_i(\\mathbf{L})$ 给出的 $\\mathbf{H}$ 的特征值都大于或等于 1，因此是正的。\n\n该问题要求两个主要结果：首先，迭代方法收敛对步长 $\\alpha$ 的一般条件；其次，针对一个具体案例计算最大步长。\n\n首先，我们推导迭代成为严格收缩的充要条件。迭代格式由下式给出：\n$$\n\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}\n$$\n这是一个形式为 $\\mathbf{x}_{k+1} = \\mathbf{T}\\mathbf{x}_k + \\mathbf{c}$ 的仿射迭代，其中迭代矩阵为 $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$。对于任何初始向量 $\\mathbf{x}_0$，当且仅当算子 $\\mathbf{T}(\\alpha)$ 是一个严格收缩时，该迭代收敛到一个唯一的不动点。在有限维向量空间中，这等价于其谱半径小于 1，即 $\\rho(\\mathbf{T}(\\alpha))  1$。\n\n矩阵的谱半径是其特征值绝对值的最大值。设 $\\mu_i$ 为 $\\mathbf{T}(\\alpha)$ 的特征值。条件是 $\\max_i |\\mu_i|  1$。\n\n$\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\mathbf{H}^{\\top}\\mathbf{H}$ 的特征值与 $\\mathbf{H}^{\\top}\\mathbf{H}$ 的特征值相关。设 $\\nu_i$ 为 $\\mathbf{H}^{\\top}\\mathbf{H}$ 的特征值。那么，$\\mathbf{T}(\\alpha)$ 的特征值为 $\\mu_i = 1 - \\alpha\\nu_i$。\n\n问题陈述 $\\mathbf{H}$ 是一个实对称矩阵。因此，$\\mathbf{H}^{\\top} = \\mathbf{H}$，矩阵 $\\mathbf{H}^{\\top}\\mathbf{H}$ 简化为 $\\mathbf{H}^2$。设 $\\lambda_i$ 为 $\\mathbf{H}$ 的特征值。那么 $\\mathbf{H}^2$ 的特征值为 $\\nu_i = \\lambda_i^2$。因为 $\\mathbf{H}$ 也是正定的，所以它的所有特征值 $\\lambda_i$ 都是严格正实数。因此，$\\mathbf{H}^2$ 的特征值 $\\nu_i = \\lambda_i^2$ 也是严格正的。\n\n收敛条件 $\\rho(\\mathbf{T}(\\alpha))  1$ 变为 $\\max_i |1 - \\alpha \\lambda_i^2|  1$。这必须对 $\\mathbf{H}$ 的所有特征值 $\\lambda_i$ 都成立。该不等式等价于：\n$$\n-1  1 - \\alpha \\lambda_i^2  1\n$$\n我们对任意给定的特征值 $\\lambda_i > 0$ 分别分析这两个不等式。\n1. $1 - \\alpha \\lambda_i^2  1 \\implies -\\alpha \\lambda_i^2  0$。由于给定 $\\alpha > 0$ 且 $\\lambda_i^2 > 0$，此不等式恒成立。\n2. $-1  1 - \\alpha \\lambda_i^2 \\implies 2 > \\alpha \\lambda_i^2 \\implies \\alpha  \\frac{2}{\\lambda_i^2}$。\n\n这个条件必须对 $\\mathbf{H}$ 的所有特征值 $\\lambda_i$ 都成立。为确保这一点，$\\alpha$ 必须小于所有上界 $\\frac{2}{\\lambda_i^2}$ 中的最小值。\n$$\n\\alpha  \\min_i \\left(\\frac{2}{\\lambda_i^2}\\right) = \\frac{2}{\\max_i (\\lambda_i^2)} = \\frac{2}{(\\max_i \\lambda_i)^2}\n$$\n由于对于正定矩阵，$\\max_i \\lambda_i = \\lambda_{\\max}(\\mathbf{H})$，这也是它的谱半径 $\\rho(\\mathbf{H})$，因此关于 $\\alpha$ 的充要条件是：\n$$\n0  \\alpha  \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2}\n$$\n\n接下来，我们将问题具体化到 $n=3$ 且 $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ 的情况，其中 $\\mathbf{L}$ 是给定的图拉普拉斯矩阵：\n$$\n\\mathbf{L} = \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix}\n$$\n$\\mathbf{H}$ 的特征值由 $\\lambda_i(\\mathbf{H}) = 1 + \\lambda_i(\\mathbf{L})$ 给出，其中 $\\lambda_i(\\mathbf{L})$ 是 $\\mathbf{L}$ 的特征值。我们通过求解特征方程 $\\det(\\mathbf{L} - \\lambda \\mathbf{I}) = 0$ 来找到 $\\mathbf{L}$ 的特征值。\n$$\n\\det \\begin{pmatrix} 1-\\lambda  -1  0 \\\\ -1  2-\\lambda  -1 \\\\ 0  -1  1-\\lambda \\end{pmatrix} = 0\n$$\n沿第一行展开行列式：\n$$\n(1-\\lambda) \\det \\begin{pmatrix} 2-\\lambda  -1 \\\\ -1  1-\\lambda \\end{pmatrix} - (-1) \\det \\begin{pmatrix} -1  -1 \\\\ 0  1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)[(2-\\lambda)(1-\\lambda) - 1] + [(-1)(1-\\lambda) - 0] = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 2 - 1] - (1-\\lambda) = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 1] - (1-\\lambda) = 0\n$$\n提取公因式 $(1-\\lambda)$：\n$$\n(1-\\lambda)[(\\lambda^2 - 3\\lambda + 1) - 1] = 0\n$$\n$$\n(1-\\lambda)(\\lambda^2 - 3\\lambda) = 0\n$$\n$$\n\\lambda(1-\\lambda)(\\lambda-3) = 0\n$$\n$\\mathbf{L}$ 的特征值为 $\\lambda(\\mathbf{L}) \\in \\{0, 1, 3\\}$。\n\n因此，$\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ 的特征值为：\n$$\n\\lambda_1(\\mathbf{H}) = 1 + 0 = 1\n$$\n$$\n\\lambda_2(\\mathbf{H}) = 1 + 1 = 2\n$$\n$$\n\\lambda_3(\\mathbf{H}) = 1 + 3 = 4\n$$\n$\\mathbf{H}$ 的最大特征值是 $\\lambda_{\\max}(\\mathbf{H}) = 4$。\n\n最后，我们计算最大允许步长 $\\alpha$。使用推导出的条件，$\\alpha$ 的允许值区间为：\n$$\n0  \\alpha  \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2} = \\frac{2}{4^2} = \\frac{2}{16} = \\frac{1}{8}\n$$\n问题要求保证收敛的最大允许恒定步长 $\\alpha$。这个值是开区间 $(0, \\frac{1}{8})$ 的上确界。此区间内的任何步长 $\\alpha$ 都将导致严格收缩，而在边界值 $\\alpha = \\frac{1}{8}$ 处，迭代矩阵的谱半径变为 $\\rho(\\mathbf{T}(\\frac{1}{8}))=1$，这不能保证对所有初始向量都收敛到正确解。*保证*严格不等式 $\\rho  1$ 的最大值是该区间的极限。因此，最大允许值是上确界。\n\n最大允许恒定步长是 $\\frac{1}{8}$。",
            "answer": "$$\\boxed{\\frac{1}{8}}$$"
        },
        {
            "introduction": "图神经网络可以作为复杂系统动力学的强大模型，在数据同化流程中提供预测。本练习将你置于一个场景中，其中一个GNN已经预测了图上系统的演化。你的任务是执行卡尔曼滤波中至关重要的“分析步”，将GNN的预测与新的噪声观测数据相融合，以获得更优的状态估计 。",
            "id": "3386871",
            "problem": "考虑一个图上的动态逆问题，其中潜状态 $x_t \\in \\mathbb{R}^3$ 定义在一个具有三个节点和边 $\\{(1,2),(2,3)\\}$ 的路径图的节点上。一个图神经网络 (GNN) 先验导出一个线性化单步展开算子 $F \\in \\mathbb{R}^{3 \\times 3}$ 和一个过程噪声协方差 $Q \\in \\mathbb{R}^{3 \\times 3}$，此处使用图拉普拉斯算子 $L \\in \\mathbb{R}^{3 \\times 3}$ 进行建模，其中\n$$\nL \\;=\\; \\begin{bmatrix}\n1  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  1\n\\end{bmatrix}, \\qquad \nF \\;=\\; I - \\alpha L \\;\\;\\text{with}\\;\\; \\alpha \\;=\\; \\frac{1}{2}, \\qquad \nQ \\;=\\; \\sigma_q^2 I,\\;\\; \\sigma_q^2 \\;=\\; \\frac{1}{10}.\n$$\n在时间 $t$，同化数据 $y_t$ 之前，预测均值和协方差（从前一个分析步骤和 GNN 导出的动力学中获得）如下所示：\n$$\nx_{t|t-1} \\;=\\; \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \n\\qquad \nP_{t|t-1} \\;=\\; \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}.\n$$\n通过一个线性观测算子 $H \\in \\mathbb{R}^{2 \\times 3}$ 和加性高斯噪声 $\\varepsilon_t \\sim \\mathcal{N}(0,R)$，在节点 $1$ 和 $3$ 上收集带噪声的观测，其中\n$$\nH \\;=\\; \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}, \n\\qquad \nR \\;=\\; \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}, \n\\qquad \ny_t \\;=\\; \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n$$\n从线性高斯数据同化模型和联合高斯随机变量的条件化法则出发，推导将预测 $(x_{t|t-1}, P_{t|t-1})$ 与 $y_t$ 融合以获得卡尔曼增益和后验均值的分析步骤。然后对给定的数值实例计算这些表达式的值。\n\n将最终答案以单行矩阵的形式报告，该矩阵首先按列主序（column-major order）列出卡尔曼增益 $K_t \\in \\mathbb{R}^{3 \\times 2}$ 的条目，随后是后验均值 $x_{t|t} \\in \\mathbb{R}^3$ 的三个条目。具体来说，报告\n$$\n\\big[\\, K_{t,(1,1)},\\, K_{t,(2,1)},\\, K_{t,(3,1)},\\, K_{t,(1,2)},\\, K_{t,(2,2)},\\, K_{t,(3,2)},\\, x_{t|t,(1)},\\, x_{t|t,(2)},\\, x_{t|t,(3)} \\,\\big].\n$$\n使用精确值；无需四舍五入。本问题中没有物理单位。角度单位规范在此不适用。",
            "solution": "用户希望我解决一个基于卡尔曼滤波框架的数据同化问题。\n\n### 步骤 1：提取已知条件\n该问题为一个图上的动态逆问题在特定时间 $t$ 提供了以下数据和定义：\n\n潜状态向量：$x_t \\in \\mathbb{R}^3$\n预测均值：$x_{t|t-1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}$\n预测协方差：$P_{t|t-1} = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}$\n线性观测算子：$H = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}$\n观测噪声协方差：$R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$\n观测向量：$y_t = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n\n该问题还提供了关于GNN先验的背景信息，其形式为单步展开算子 $F = I - \\alpha L$ 和过程噪声协方差 $Q = \\sigma_q^2 I$，并给出了 $L$、$\\alpha$ 和 $\\sigma_q^2$ 的具体值。此信息与预测步骤（将状态从时间 $t-1$ 传播到 $t$）有关，该步骤已经执行完毕，得到了给定的预测量 $x_{t|t-1}$ 和 $P_{t|t-1}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题描述了一个标准的线性高斯数据同化场景，这是状态估计、控制理论和逆问题中的核心概念。该公式基于卡尔曼滤波器，是工程和科学领域中一个基础且广泛使用的算法。所有定义和模型都是标准的、科学上合理的。\n2.  **适定性**：该问题是适定的。它要求计算后验均值和卡尔曼增益，这些量由给定的预测状态和观测模型唯一确定。所提供的矩阵定义明确，且所需的矩阵逆存在（将在解题过程中展示）。\n3.  **客观性**：该问题以精确、客观的数学语言陈述。它提供了所有必要的数值和定义，没有主观解释的余地。\n4.  **不完整或矛盾的设置**：该问题是自洽的。虽然它提供了关于动态模型（$F, Q, L$）的额外信息，但这些信息是关于如何获得预测状态（$x_{t|t-1}, P_{t|t-1}$）的背景，对于所要求的分析步骤并非必需。这不构成矛盾，而是呈现了滤波循环中单个时间步骤的完整画面。分析步骤所需的所有组件均已提供。\n5.  **不切实际或不可行**：矩阵和向量的维度是一致的：$x_{t|t-1}$ 是 $3 \\times 1$，$P_{t|t-1}$ 是 $3 \\times 3$，$y_t$ 是 $2 \\times 1$，$H$ 是 $2 \\times 3$，$R$ 是 $2 \\times 2$。所有运算在数学上都是有效的。这些值是标准的，不代表任何物理上的不可能性。\n6.  **不适定或结构不良**：该问题结构清晰，要求从既定公式中推导出具体、可计算的结果。\n7.  **所有其他检查**：该问题是可形式化的，与指定领域相关，不故作高深，并且是科学上可验证的。\n\n### 步骤 3：结论与行动\n该问题有效。我将继续解题。\n\n该问题要求从联合高斯随机变量的条件化法则推导分析步骤的更新方程，然后计算卡尔曼增益 $K_t$ 和后验均值 $x_{t|t}$。\n\n分析步骤将预测信息与新的观测相结合。我们假设一个线性高斯模型。\n状态 $x_t$ 的先验（预测）分布是高斯分布：\n$$ x_t \\sim \\mathcal{N}(x_{t|t-1}, P_{t|t-1}) $$\n观测模型是线性的，带有加性高斯噪声：\n$$ y_t = H x_t + \\varepsilon_t, \\quad \\text{where} \\quad \\varepsilon_t \\sim \\mathcal{N}(0, R) $$\n状态 $x_t$ 和噪声 $\\varepsilon_t$ 假定是不相关的。\n\n我们构造一个联合随机向量 $\\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix}$。其分布也是高斯分布。\n该联合向量的均值是：\n$$ E\\left[ \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\right] = \\begin{pmatrix} E[x_t] \\\\ E[H x_t + \\varepsilon_t] \\end{pmatrix} = \\begin{pmatrix} x_{t|t-1} \\\\ H E[x_t] + E[\\varepsilon_t] \\end{pmatrix} = \\begin{pmatrix} x_{t|t-1} \\\\ H x_{t|t-1} \\end{pmatrix} $$\n该联合向量的协方差是：\n$$ \\text{Cov}\\left( \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\right) = \\begin{pmatrix} \\text{Cov}(x_t, x_t)  \\text{Cov}(x_t, y_t) \\\\ \\text{Cov}(y_t, x_t)  \\text{Cov}(y_t, y_t) \\end{pmatrix} $$\n让我们计算各个块：\n- $\\text{Cov}(x_t, x_t) = P_{t|t-1}$\n- $\\text{Cov}(x_t, y_t) = E[(x_t - x_{t|t-1})(y_t - H x_{t|t-1})^T] = E[(x_t - x_{t|t-1})(H x_t + \\varepsilon_t - H x_{t|t-1})^T] = E[(x_t - x_{t|t-1})(H(x_t - x_{t|t-1}) + \\varepsilon_t)^T] = E[(x_t - x_{t|t-1})(x_t - x_{t|t-1})^T H^T] + E[(x_t - x_{t|t-1})\\varepsilon_t^T] = P_{t|t-1} H^T$，因为 $x_t$ 和 $\\varepsilon_t$ 是不相关的。\n- $\\text{Cov}(y_t, x_t) = (\\text{Cov}(x_t, y_t))^T = H P_{t|t-1}$\n- $\\text{Cov}(y_t, y_t) = E[(y_t - H x_{t|t-1})(y_t - H x_{t|t-1})^T] = E[(H(x_t - x_{t|t-1}) + \\varepsilon_t)(H(x_t - x_{t|t-1}) + \\varepsilon_t)^T] = H E[(x_t - x_{t|t-1})(x_t - x_{t|t-1})^T] H^T + E[\\varepsilon_t \\varepsilon_t^T] = H P_{t|t-1} H^T + R$。\n\n所以联合分布是：\n$$ \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} x_{t|t-1} \\\\ H x_{t|t-1} \\end{pmatrix}, \\begin{pmatrix} P_{t|t-1}  P_{t|t-1} H^T \\\\ H P_{t|t-1}  H P_{t|t-1} H^T + R \\end{pmatrix} \\right) $$\n\n分析步骤要求找到给定观测 $y_t$ 下 $x_t$ 的条件分布。对于一个均值为 $\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}$、协方差为 $\\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$ 的一般分块高斯向量 $\\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}$，给定 $z_2$ 时 $z_1$ 的条件分布是高斯分布，其均值为 $\\mu_{1|2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(z_2 - \\mu_2)$，协方差为 $\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$。\n\n将此应用于我们的情况（$z_1=x_t$, $z_2=y_t$），我们找到后验（分析）均值 $x_{t|t}$ 和协方差 $P_{t|t}$：\n$$ x_{t|t} = x_{t|t-1} + (P_{t|t-1} H^T)(H P_{t|t-1} H^T + R)^{-1}(y_t - H x_{t|t-1}) $$\n项 $K_t = (P_{t|t-1} H^T)(H P_{t|t-1} H^T + R)^{-1}$ 是卡尔曼增益。\n后验均值则由更新方程给出：\n$$ x_{t|t} = x_{t|t-1} + K_t (y_t - H x_{t|t-1}) $$\n\n现在，让我们为给定的数值实例计算这些表达式的值。\n给定的矩阵和向量是：\n$$ P_{t|t-1} = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}, \\quad R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} $$\n$$ x_{t|t-1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad y_t = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n首先，我们计算卡尔曼增益 $K_t$ 所需的分量。\n项 $P_{t|t-1} H^T$：\n$$ P_{t|t-1} H^T = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  3 \\end{bmatrix} $$\n新息协方差 $S_t = H P_{t|t-1} H^T + R$：\n$$ H P_{t|t-1} H^T = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix} \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0  0 \\\\ 0  0  3 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  3 \\end{bmatrix} $$\n$$ S_t = \\begin{bmatrix} 1  0 \\\\ 0  3 \\end{bmatrix} + \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  4 \\end{bmatrix} $$\n新息协方差的逆 $S_t^{-1}$：\n$$ S_t^{-1} = \\begin{bmatrix} 2  0 \\\\ 0  4 \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} $$\n现在，我们可以计算卡尔曼增益 $K_t = P_{t|t-1} H^T S_t^{-1}$：\n$$ K_t = \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  3 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} (1)(\\frac{1}{2})+(0)(0)  (1)(0)+(0)(\\frac{1}{4}) \\\\ (0)(\\frac{1}{2})+(0)(0)  (0)(0)+(0)(\\frac{1}{4}) \\\\ (0)(\\frac{1}{2})+(3)(0)  (0)(0)+(3)(\\frac{1}{4}) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  0 \\\\ 0  \\frac{3}{4} \\end{bmatrix} $$\n\n接下来，我们计算后验均值 $x_{t|t}$。\n我们需要新息项，$y_t - H x_{t|t-1}$：\n$$ H x_{t|t-1} = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} $$\n$$ y_t - H x_{t|t-1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} $$\n最后，我们计算后验均值 $x_{t|t} = x_{t|t-1} + K_t (y_t - H x_{t|t-1})$：\n$$ x_{t|t} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  0 \\\\ 0  \\frac{3}{4} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} (\\frac{1}{2})(1)+(0)(-1) \\\\ (0)(1)+(0)(-1) \\\\ (0)(1)+(\\frac{3}{4})(-1) \\end{bmatrix} $$\n$$ x_{t|t} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{3}{4} \\end{bmatrix} = \\begin{bmatrix} 0+\\frac{1}{2} \\\\ 1+0 \\\\ 2-\\frac{3}{4} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{5}{4} \\end{bmatrix} $$\n\n该问题要求将结果表示为单个行矩阵，其中包含按列主序排列的卡尔曼增益 $K_t$ 的条目，后面跟着后验均值 $x_{t|t}$ 的条目。\n卡尔曼增益是 $K_t = \\begin{bmatrix} K_{t,(1,1)}  K_{t,(1,2)} \\\\ K_{t,(2,1)}  K_{t,(2,2)} \\\\ K_{t,(3,1)}  K_{t,(3,2)} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  0 \\\\ 0  \\frac{3}{4} \\end{bmatrix}$。\n按列主序，条目为：$K_{t,(1,1)}=\\frac{1}{2}$, $K_{t,(2,1)}=0$, $K_{t,(3,1)}=0$, $K_{t,(1,2)}=0$, $K_{t,(2,2)}=0$, $K_{t,(3,2)}=\\frac{3}{4}$。\n后验均值是 $x_{t|t} = \\begin{bmatrix} x_{t|t,(1)} \\\\ x_{t|t,(2)} \\\\ x_{t|t,(3)} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{5}{4} \\end{bmatrix}$。\n\n最终报告的向量是 $\\big[\\, \\frac{1}{2},\\, 0,\\, 0,\\, 0,\\, 0,\\, \\frac{3}{4},\\, \\frac{1}{2},\\, 1,\\, \\frac{5}{4} \\,\\big]$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  0  0  0  0  \\frac{3}{4}  \\frac{1}{2}  1  \\frac{5}{4}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}