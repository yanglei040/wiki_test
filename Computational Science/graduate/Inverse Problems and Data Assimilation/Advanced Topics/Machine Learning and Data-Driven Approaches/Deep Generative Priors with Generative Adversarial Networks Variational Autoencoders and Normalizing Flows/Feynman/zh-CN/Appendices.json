{
    "hands_on_practices": [
        {
            "introduction": "本实践练习将指导您完成一个基础但重要的任务：为一个模拟的卫星遥感问题构建数据同化流程。通过这个练习，您将学习如何在一个线性化的高斯模型框架下，利用变分自编码器（VAE）先验来处理光谱带缺失的数据。这个练习的核心是推导和计算后验协方差，并量化不确定性如何从潜空间传播到数据空间，这是在逆问题中应用生成模型的关键一步 。",
            "id": "3374817",
            "problem": "您的任务是为缺失光谱波段下的多光谱卫星遥感构建一个数学上精确且可计算验证的数据同化流程，该流程使用变分自编码器（VAE）先验。此流程必须被构建为一个配备深度生成先验和反映缺失波段选择的线性观测算子的潜变量逆问题。您的程序必须精确实现以下设置和计算，并以指定格式生成单行输出。\n\n假设一个具有 $d$ 个光谱波段的单大多光谱像素的生成模型如下：\n1. 潜变量为 $z \\in \\mathbb{R}^L$，其先验分布为 $p(z) = \\mathcal{N}(0, I_L)$，其中 $I_L$ 是 $L \\times L$ 的单位矩阵。\n2. VAE解码器映射通过在标称点周围进行一阶线性化局部近似，得到 $x \\in \\mathbb{R}^d$ 为\n$$\nx = g(z) \\approx \\mu + W z,\n$$\n其中 $\\mu \\in \\mathbb{R}^d$ 是一个固定的解码器均值，而 $W \\in \\mathbb{R}^{d \\times L}$ 是在某线性化点评估的解码器雅可比矩阵。这种近似方法与深度生成先验逆问题中的标准方法论一致。\n\n在缺失光谱波段下的观测模型为：\n$$\ny = H x + \\epsilon,\n$$\n其中 $H \\in \\mathbb{R}^{m \\times d}$ 是选择 $m$ 个观测波段（$m \\le d$）的观测算子，$\\epsilon \\sim \\mathcal{N}(0, R)$，其中 $R \\in \\mathbb{R}^{m \\times m}$ 是对称正定（对角）矩阵。\n\n您的任务：\n- 基于观测波段索引集，推导观测算子 $H$ 作为一个二元选择矩阵。其构建必须从 $H$ 选择与观测索引相对应的单位矩阵行的定义中逻辑推导得出，从而实现将 $x$ 线性投影到观测子空间。\n- 从第一性原理（贝叶斯法则和高斯模型）出发，推导潜变量后验分布 $p(z \\mid y)$ 为一个高斯分布 $\\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$，其均值为 $\\mu_{\\text{post}} \\in \\mathbb{R}^L$，协方差为 $\\Sigma_{\\text{post}} \\in \\mathbb{R}^{L \\times L}$，这与使用深度生成先验的线性高斯数据同化范式一致。\n- 通过传播的协方差量化从潜变量后验到数据空间的不确定性传播：\n$$\n\\Sigma_x = W \\Sigma_{\\text{post}} W^\\top \\in \\mathbb{R}^{d \\times d}.\n$$\n\n为以下测试套件实现上述计算。所有矩阵和向量都必须严格按照给定值构建。每个数字都是实数标量。解码器维度为 $d = 5$，潜变量维度为 $L = 3$。\n\n解码器参数：\n$$\nW = \\begin{bmatrix}\n0.90  -0.10  0.20 \\\\\n0.30  0.80  -0.50 \\\\\n-0.40  0.10  0.70 \\\\\n0.00  -0.60  0.40 \\\\\n0.50  0.20  -0.30\n\\end{bmatrix}, \\quad\n\\mu = \\begin{bmatrix}\n0.05 \\\\ -0.10 \\\\ 0.20 \\\\ -0.05 \\\\ 0.10\n\\end{bmatrix}.\n$$\n\n真值潜变量（用于为每种情况生成无噪声观测）：\n$$\nz^\\star = \\begin{bmatrix} 0.60 \\\\ -0.40 \\\\ 0.80 \\end{bmatrix}.\n$$\n计算 $x^\\star = \\mu + W z^\\star$，然后在每种情况下计算 $y = H x^\\star$（为进行确定性评估，设置 $\\epsilon = 0$；不确定性仅通过 $R$ 进入）。\n\n测试用例：\n- 情况1：观测波段索引 $\\Omega = \\{0, 2, 4\\}$，因此 $m = 3$。噪声协方差 $R = \\operatorname{diag}(0.05, 0.10, 0.08)$。\n- 情况2：观测波段索引 $\\Omega = \\{0, 1, 2, 3, 4\\}$，因此 $m = 5$。噪声协方差 $R = \\operatorname{diag}(0.05, 0.05, 0.05, 0.05, 0.05)$。\n- 情况3：观测波段索引 $\\Omega = \\varnothing$，因此 $m = 0$。噪声协方差 $R$ 是一个形状为 $0 \\times 0$ 的空矩阵（无观测）。\n- 情况4：观测波段索引 $\\Omega = \\{1, 3\\}$，因此 $m = 2$。噪声协方差 $R = \\operatorname{diag}(1.00, 2.00)$。\n\n要求输出：\n- 对于每种情况，计算潜变量后验协方差 $\\Sigma_{\\text{post}}$ 和传播的协方差 $\\Sigma_x$。\n- 从这些协方差中，提取标量度量：\n$$\n\\tau_z = \\operatorname{tr}(\\Sigma_{\\text{post}}), \\quad \\tau_x = \\operatorname{tr}(\\Sigma_x).\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果严格按以下顺序排列：\n$$\n[\\tau_z^{(1)}, \\tau_x^{(1)}, \\tau_z^{(2)}, \\tau_x^{(2)}, \\tau_z^{(3)}, \\tau_x^{(3)}, \\tau_z^{(4)}, \\tau_x^{(4)}],\n$$\n其中上标 $(k)$ 表示情况 $k$，对于 $k \\in \\{1, 2, 3, 4\\}$。\n- 所有输出必须是实数（浮点数）。不需要物理单位。\n\n您的实现必须是自包含的，且不应需要任何输入。严格按照规定构建所有量，并对每种情况执行所需的计算。程序必须使用Python语言编写，并且只能使用最终答案规范中列出的库。",
            "solution": "该问题要求推导和计算数据同化流程的不确定性度量。该流程使用线性化的变分自编码器（VAE）解码器作为深度生成先验，从不完整的观测中重建多光谱卫星像素。分析将从线性高斯模型的贝叶斯推断的第一性原理出发。\n\n### 1. 问题建模\n\n该系统由一组线性高斯模型描述。\n潜变量 $z \\in \\mathbb{R}^L$ 具有标准正态先验分布：\n$$\np(z) = \\mathcal{N}(z \\mid 0, I_L)\n$$\n其中 $I_L$ 是 $L \\times L$ 的单位矩阵。这里，先验均值为 $\\mu_{\\text{prior}} = 0$，先验协方差为 $\\Sigma_{\\text{prior}} = I_L$。\n\nVAE解码器将潜变量 $z$ 映射到数据空间向量 $x \\in \\mathbb{R}^d$，并由一个线性函数近似：\n$$\nx \\approx \\mu + W z\n$$\n其中 $\\mu \\in \\mathbb{R}^d$ 是解码器均值，而 $W \\in \\mathbb{R}^{d \\times L}$ 是解码器雅可比矩阵。\n\n观测值 $y \\in \\mathbb{R}^m$ 是完整数据向量 $x$ 的线性投影，并受到加性高斯噪声 $\\epsilon$ 的干扰：\n$$\ny = H x + \\epsilon\n$$\n其中 $H \\in \\mathbb{R}^{m \\times d}$ 是选择 $m$ 个观测光谱波段的观测算子，噪声分布为 $\\epsilon \\sim \\mathcal{N}(0, R)$。\n\n通过将 $x$ 的表达式代入观测模型，我们得到潜变量 $z$ 和观测值 $y$ 之间的直接关系：\n$$\ny = H(\\mu + Wz) + \\epsilon = H\\mu + HWz + \\epsilon\n$$\n这可以重新整理为线性逆问题的标准形式：\n$$\ny - H\\mu = (HW)z + \\epsilon\n$$\n这里，$(HW)$ 是将潜变量 $z$ 映射到（均值校正后）观测空间的前向算子，而 $\\epsilon$ 是观测噪声。\n\n### 2. 潜变量后验协方差的推导\n\n我们寻求给定观测 $y$ 下潜变量的后验分布 $p(z \\mid y)$。由于先验分布 $p(z)$ 和似然函数 $p(y \\mid z)$ 都是高斯分布，因此后验分布也将是一个高斯分布，我们将其表示为 $\\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$。\n\n根据线性高斯模型的贝叶斯法则，后验协方差矩阵的逆（后验精度矩阵）是先验精度矩阵和似然项精度矩阵之和。\n\n先验精度为 $\\Sigma_{\\text{prior}}^{-1} = I_L^{-1} = I_L$。\n\n似然函数由观测模型导出：$p(y \\mid z) = \\mathcal{N}(y \\mid H\\mu + HWz, R)$。其对应的负对数似然（忽略常数）正比于：\n$$\n\\frac{1}{2} (y - H\\mu - HWz)^\\top R^{-1} (y - H\\mu - HWz)\n$$\n关于 $z$ 的二次项是 $\\frac{1}{2} z^\\top (HW)^\\top R^{-1} (HW) z$。这个二次项的系数定义了数据精度矩阵，即 $(HW)^\\top R^{-1} (HW) = W^\\top H^\\top R^{-1} H W$。\n\n将先验精度矩阵和数据精度矩阵相加，得到后验精度矩阵：\n$$\n\\Sigma_{\\text{post}}^{-1} = \\Sigma_{\\text{prior}}^{-1} + W^\\top H^\\top R^{-1} H W = I_L + W^\\top H^\\top R^{-1} H W\n$$\n因此，后验协方差是该矩阵的逆：\n$$\n\\Sigma_{\\text{post}} = (I_L + W^\\top H^\\top R^{-1} H W)^{-1}\n$$\n请注意，这个后验协方差的表达式不依赖于具体的观测值 $y$，而只依赖于模型的结构（$W, H$）和噪声统计特性（$R$）。\n\n### 3. 观测算子 $H$ 的构建\n\n观测算子 $H \\in \\mathbb{R}^{m \\times d}$ 被定义为一个基于观测波段索引集 $\\Omega = \\{i_0, i_1, \\dots, i_{m-1}\\}$ 的二元选择矩阵。为了让 $H$ 从向量 $x \\in \\mathbb{R}^d$ 中选择指定的波段，$H$ 的每一行都必须是 $\\mathbb{R}^d$ 的一个标准基向量。具体来说，$H$ 的第 $k$ 行对应于基向量 $e_{i_k}^\\top$，该向量在索引 $i_k$ 处为1，其余位置为0。这种构建方式等价于从 $d \\times d$ 单位矩阵 $I_d$ 中取出与 $\\Omega$ 中索引相对应的行。\n\n### 4. 传播的数据空间协方差\n\n潜空间中的后验不确定性（由 $\\Sigma_{\\text{post}}$ 表征）通过线性化的解码器传播到数据空间。重建数据向量 $x$ 的后验分布近似为高斯分布。给定 $x \\approx \\mu + Wz$ 和 $z \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$，$x$ 的协方差由随机变量线性变换的标准公式给出：\n$$\n\\Sigma_x = W \\Sigma_{\\text{post}} W^\\top\n$$\n\n### 5. 不确定性度量与特殊情况\n\n潜空间和数据空间中的总不确定性通过它们各自协方差矩阵的迹来量化。\n潜空间不确定性为 $\\tau_z = \\operatorname{tr}(\\Sigma_{\\text{post}})$。\n传播的数据空间不确定性为 $\\tau_x = \\operatorname{tr}(\\Sigma_x)$。\n\n一个关键的特殊情况是当没有进行观测时，即 $m=0$ 且 $\\Omega = \\varnothing$。在这种情况下，观测没有提供关于 $z$ 的任何新信息。因此，后验分布必须与先验分布相同。\n$$\np(z \\mid y_{\\text{no-data}}) = p(z) \\implies \\Sigma_{\\text{post}} = \\Sigma_{\\text{prior}} = I_L\n$$\n这与我们推导的公式是一致的，因为当 $m=0$ 时，观测矩阵 $H$ 的维度是 $0 \\times d$，使得 $W^\\top H^\\top R^{-1} H W$ 项成为一个 $L \\times L$ 的零矩阵。因此，$\\Sigma_{\\text{post}} = (I_L + 0)^{-1} = I_L$。\n对于这种情况：\n$\\tau_z = \\operatorname{tr}(I_L) = L$。\n$\\tau_x = \\operatorname{tr}(W I_L W^\\top) = \\operatorname{tr}(W W^\\top)$。\n\n### 6. 计算流程\n\n对于每个测试用例，执行以下步骤：\n1. 定义维度 $d=5$、$L=3$ 以及矩阵 $W$ 和 $I_L$。\n2. 对于给定的观测索引集 $\\Omega$ 和噪声方差（$R$ 的对角线），确定观测空间的维度 $m = |\\Omega|$。\n3. 如果 $m=0$：\n   - $\\Sigma_{\\text{post}} = I_L$。\n4. 如果 $m>0$：\n   - 构建观测算子 $H \\in \\mathbb{R}^{m \\times d}$。\n   - 构建噪声协方差矩阵 $R \\in \\mathbb{R}^{m \\times m}$ 并计算其逆 $R^{-1}$。\n   - 计算后验精度 $\\Sigma_{\\text{post}}^{-1} = I_L + W^\\top H^\\top R^{-1} H W$。\n   - 求逆以找到后验协方差 $\\Sigma_{\\text{post}} = (\\Sigma_{\\text{post}}^{-1})^{-1}$。\n5. 计算传播的数据空间协方差 $\\Sigma_x = W \\Sigma_{\\text{post}} W^\\top$。\n6. 计算不确定性度量 $\\tau_z = \\operatorname{tr}(\\Sigma_{\\text{post}})$ 和 $\\tau_x = \\operatorname{tr}(\\Sigma_x)$。\n7. 收集所有情况的结果并按指定格式进行格式化。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the data assimilation problem under a linearized VAE prior.\n    \n    This function implements the derivation for the posterior covariance in a\n    linear-Gaussian model, computes the propagated covariance in the data space,\n    and calculates their traces as uncertainty metrics for four distinct test cases.\n    \"\"\"\n    # Decoder and latent space parameters\n    d = 5  # Data space dimension\n    L = 3  # Latent space dimension\n\n    # Decoder Jacobian W\n    W = np.array([\n        [0.90, -0.10, 0.20],\n        [0.30,  0.80, -0.50],\n        [-0.40, 0.10,  0.70],\n        [0.00, -0.60,  0.40],\n        [0.50,  0.20, -0.30]\n    ])\n\n    # Identity matrix in the latent space (prior covariance)\n    I_L = np.eye(L)\n\n    # Note: The decoder mean mu and ground-truth latent z_star are not needed for\n    # computing the posterior covariance, which only depends on the model structure\n    # (W, H) and noise statistics (R).\n\n    # Test cases defined by observed indices and diagonal of noise covariance R.\n    test_cases = [\n        # Case 1: m=3 observed bands\n        {'indices': [0, 2, 4], 'R_diag': [0.05, 0.10, 0.08]},\n        # Case 2: m=5 (all) observed bands\n        {'indices': [0, 1, 2, 3, 4], 'R_diag': [0.05, 0.05, 0.05, 0.05, 0.05]},\n        # Case 3: m=0 (no) observed bands\n        {'indices': [], 'R_diag': []},\n        # Case 4: m=2 observed bands\n        {'indices': [1, 3], 'R_diag': [1.00, 2.00]}\n    ]\n\n    results = []\n    for case in test_cases:\n        indices = case['indices']\n        R_diag = np.array(case['R_diag'])\n        m = len(indices)\n\n        if m == 0:\n            # Special case: no observations.\n            # Posterior covariance equals prior covariance.\n            Sigma_post = I_L\n        else:\n            # Construct observation operator H\n            # H is an m x d matrix that selects the observed bands.\n            H = np.zeros((m, d))\n            H[np.arange(m), indices] = 1.0\n\n            # Compute R^-1. Since R is diagonal, its inverse is a diagonal\n            # matrix with the reciprocals of the diagonal elements.\n            R_inv = np.diag(1.0 / R_diag)\n            \n            # Compute posterior precision matrix: I_L + W^T H^T R^-1 H W\n            # This is the sum of the prior and data-likelihood precisions.\n            data_precision = W.T @ H.T @ R_inv @ H @ W\n            Sigma_post_inv = I_L + data_precision\n\n            # Invert to get posterior covariance\n            Sigma_post = np.linalg.inv(Sigma_post_inv)\n\n        # Propagate covariance to data space: Sigma_x = W * Sigma_post * W^T\n        Sigma_x = W @ Sigma_post @ W.T\n\n        # Calculate uncertainty metrics (traces of covariance matrices)\n        tau_z = np.trace(Sigma_post)\n        tau_x = np.trace(Sigma_x)\n\n        results.extend([tau_z, tau_x])\n\n    # Format the output as a single comma-separated string in brackets\n    # Using a high precision format to avoid rounding issues in verification\n    formatted_results = [f\"{r:.15f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实践中，我们所选的先验模型可能无法完美捕捉真实世界的数据分布，这种情况被称为模型错配。本练习通过一个精心设计的数值实验，让您直观地探索当生成先验的支撑集不包含真实解时会发生什么。通过分析后验分布的形态变化，例如伪峰的出现，您将深刻理解先验与数据之间的冲突，并学会如何诊断这类关键问题 。",
            "id": "3374820",
            "problem": "考虑一个由变分自编码器 (VAE) 指定深度生成先验的反演问题，该编码器引入了一个潜变量模型。设潜变量为一维，记作 $z \\in \\mathbb{R}$。解码器是确定性的，$x = g(z)$，因此 $x$ 上 induced 的先验是通过 $g$ 对潜变量先验 $p(z)$ 的前推。观测值由作用在解码状态上的测量算子 $h$ 生成，并带有加性高斯噪声。在此设置中，生成模型为\n$$\nz \\sim \\mathcal{N}(0,1), \\quad x = g(z), \\quad y = h(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2).\n$$\n根据贝叶斯法则，给定观测值 $y$ 时潜变量 $z$ 的后验分布满足\n$$\n\\pi(z \\mid y) \\propto p(y \\mid z) \\, p(z),\n$$\n其中 $p(y \\mid z)$ 是由 $h \\circ g$ 导出的高斯似然，$p(z)$ 是潜变量先验。\n\n为了故意损坏 VAE 先验的训练分布并量化模型误设，假设潜变量先验被截断以移除特定区间内的质量，从而将真实状态 $x^\\dagger$ 从 induced 的先验支撑集中省略。具体来说，将损坏的潜变量先验定义为\n$$\np_{\\mathrm{corr}}(z) \\propto \\exp\\left(-\\frac{z^2}{2}\\right) \\cdot \\mathbf{1}\\left(\\lvert z - z_c \\rvert \\ge \\delta\\right),\n$$\n其中 $z_c \\in \\mathbb{R}$ 是被移除区间的中心，$\\delta > 0$ 是其半径。当解码器是确定性时，这种损坏模拟了一个训练分布，该分布省略了包含真实潜变量 $z^\\dagger$ 的潜变量邻域，因此也从先验支撑集中省略了 $x^\\dagger = g(z^\\dagger)$。\n\n假设观测模型是一维的，以便可以在固定网格上对 $z \\in \\mathbb{R}$ 的后验进行数值分析。任务是：\n- 对于一般的测量函数 $h \\circ g$、方差为 $\\sigma_y^2$ 的高斯观测噪声，以及标准正态潜变量先验或上述定义的损坏的潜变量先验，从第一性原理出发，推导未归一化后验密度 $\\pi(z \\mid y)$ 的表达式。\n- 在 $z \\in [-5,5]$ 的均匀网格上，使用 $20001$ 个等距点实现数值近似，以评估未归一化的后验，对其进行归一化，并计算：\n  1. 后验在限制于网格的 $z$ 上的模态（局部极大值）数量。模态定义为网格内部的一个索引 $i$，使得 $z_i$ 处的归一化后验密度严格大于其紧邻，并超过最大归一化后验密度的 $10^{-4}$ 倍的相对阈值。\n  2. 后验标准差 $\\sqrt{\\mathrm{Var}[z \\mid y]}$，由网格上的归一化离散分布近似。\n  3. 真实潜变量 $z^\\dagger$ 半径 $\\rho = 0.15$ 范围内的后验质量，即 $\\sum_{i: \\lvert z_i - z^\\dagger \\rvert \\le \\rho} \\pi(z_i \\mid y) \\Delta z$，其中 $\\Delta z$ 是网格间距。此值必须以小数形式报告。\n\n使用以下测试套件，该套件探索了设定良好和误设的机制，包括当先验支撑集省略真实 $x^\\dagger$ 时伪模态的出现：\n- 案例 A（误设，损坏的先验，小噪声）：$g(z) = z$, $h(x) = x$, $y = 1.0$, $\\sigma_y = 0.05$, $z^\\dagger = 1.0$, $z_c = 1.0$, $\\delta = 0.3$。\n- 案例 B（误设，损坏的先验，较大噪声）：$g(z) = z$, $h(x) = x$, $y = 1.0$, $\\sigma_y = 0.5$, $z^\\dagger = 1.0$, $z_c = 1.0$, $\\delta = 0.3$。\n- 案例 C（设定良好，标准先验）：$g(z) = z$, $h(x) = x$, $y = 1.0$, $\\sigma_y = 0.05$, $z^\\dagger = 1.0$，无损坏（$\\delta = 0.0$ 且任何 $z_c$ 都被忽略）。\n- 案例 D（非单射解码器引起的观测，标准先验）：$g(z) = z$, $h(x) = x^2$, $y = 1.0$, $\\sigma_y = 0.05$, $z^\\dagger = 1.0$，无损坏（$\\delta = 0.0$ 且任何 $z_c$ 都被忽略）。\n\n对于每个案例，计算上面列出的三个量：\n1. 模态的整数数量。\n2. 后验标准差（浮点数）。\n3. $z^\\dagger$ 半径 $\\rho$ 内的后验质量（浮点数）。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的结果列表，该列表封装在方括号中，每个案例对应一个子列表，顺序为 A、B、C、D。每个子列表的形式为 $[n, s, m]$，其中 $n$ 是模态的整数数量，$s$ 是后验标准差，$m$ 是 $z^\\dagger$ 附近的后验质量。例如，$[[n_A,s_A,m_A],[n_B,s_B,m_B],[n_C,s_C,m_C],[n_D,s_D,m_D]]$。不涉及物理单位。不出现角度。所有小数必须以标准十进制表示法打印。",
            "solution": "该问题要求在由深度生成先验正则化的反演问题背景下，分析一维潜变量 $z$ 的贝叶斯后验分布。我们将首先推导后验分布的数学表达式，然后详细说明其表征的数值方法，最后将这些方法应用于四个指定的测试案例。\n\n### 1. 后验分布的推导\n\n该问题被构建为一个分层贝叶斯模型。生成过程如下：\n1.  从先验分布 $p(z)$ 中抽取一个潜变量 $z$。\n2.  状态 $x$ 由解码器函数 $g$ 确定性地生成，因此 $x = g(z)$。\n3.  通过测量算子 $h$ 获得观测值 $y$，并带有加性高斯噪声 $\\varepsilon$，即 $y = h(x) + \\varepsilon = (h \\circ g)(z) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$。\n\n我们的目标是找到给定观测值 $y$ 时潜变量 $z$ 的后验分布，记为 $\\pi(z \\mid y)$。根据贝叶斯定理，后验与似然和先验的乘积成正比：\n$$\n\\pi(z \\mid y) \\propto p(y \\mid z) \\, p(z)\n$$\n\n似然函数 $p(y \\mid z)$ 描述了在给定特定潜码 $z$ 的情况下观测到 $y$ 的概率。从测量模型 $y = (h \\circ g)(z) + \\varepsilon$ 可知，$y$ 在给定 $z$ 下的条件分布是一个以 $(h \\circ g)(z)$ 为中心、方差为 $\\sigma_y^2$ 的高斯分布：\n$$\np(y \\mid z) = \\mathcal{N}(y; (h \\circ g)(z), \\sigma_y^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left( -\\frac{(y - (h \\circ g)(z))^2}{2\\sigma_y^2} \\right)\n$$\n\n先验分布 $p(z)$ 以两种形式指定：\n1.  **标准先验**：标准正态分布，$p(z) = \\mathcal{N}(z; 0, 1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$。\n2.  **损坏的先验**：截断的标准正态分布，$p_{\\mathrm{corr}}(z) \\propto \\exp\\left(-\\frac{z^2}{2}\\right) \\cdot \\mathbf{1}\\left(\\lvert z - z_c \\rvert \\ge \\delta\\right)$，其中 $\\mathbf{1}(\\cdot)$ 是指示函数。此先验在区间 $(z_c - \\delta, z_c + \\delta)$ 内为零。\n\n结合似然和先验，未归一化的后验密度 $\\pi_{\\text{unnorm}}(z \\mid y)$ 由乘积给出。为了数值稳定性，使用对数后验是有利的：\n$$\n\\ln \\pi(z \\mid y) = \\ln p(y \\mid z) + \\ln p(z) + C\n$$\n其中 $C$ 是一个常数。去掉不依赖于 $z$ 的常数项，未归一化对数后验的表达式变为：\n$$\n\\ln \\pi_{\\text{unnorm}}(z \\mid y) = -\\frac{(y - (h \\circ g)(z))^2}{2\\sigma_y^2} - \\frac{z^2}{2}\n$$\n对于标准先验。对于损坏的先验，我们有：\n$$\n\\ln \\pi_{\\text{unnorm}}(z \\mid y) = \n\\begin{cases} \n-\\frac{(y - (h \\circ g)(z))^2}{2\\sigma_y^2} - \\frac{z^2}{2} & \\text{若 } \\lvert z - z_c \\rvert \\ge \\delta \\\\\n-\\infty & \\text{若 } \\lvert z - z_c \\rvert < \\delta\n\\end{cases}\n$$\n\n### 2. 数值近似\n\n我们在一个离散、均匀的点网格 $z_i$ 上近似连续的后验分布。问题指定了一个 $z \\in [-5, 5]$ 的网格，有 $N = 20001$ 个点。网格间距为 $\\Delta z = (5 - (-5))/(N - 1) = 10 / 20000 = 5 \\times 10^{-4}$。\n\n对于网格上的每个点 $z_i$，我们评估未归一化的对数后验 $\\ln \\pi_{\\text{unnorm}}(z_i \\mid y)$。为了获得未归一化的后验值 $\\pi_{\\text{unnorm}}(z_i \\mid y)$，我们计算 $\\exp(\\ln \\pi_{\\text{unnorm}}(z_i \\mid y))$。为了防止数值下溢或上溢，我们使用恒等式 $\\exp(a) = \\exp(a - \\max(a_j))\\exp(\\max(a_j))$ 并去掉常数因子 $\\exp(\\max(a_j))$。我们定义一个稳定的未归一化后验：\n$$\n\\tilde{\\pi}_{\\text{unnorm}}(z_i \\mid y) = \\exp\\left( \\ln \\pi_{\\text{unnorm}}(z_i \\mid y) - \\max_{j} \\{ \\ln \\pi_{\\text{unnorm}}(z_j \\mid y) \\} \\right)\n$$\n然后通过使用求和（对于细网格，等同于梯形法则）来近似未归一化后验的积分，从而找到归一化的后验密度 $\\pi(z_i \\mid y)$：\n$$\n\\pi(z_i \\mid y) = \\frac{\\tilde{\\pi}_{\\text{unnorm}}(z_i \\mid y)}{\\sum_{j=1}^{N} \\tilde{\\pi}_{\\text{unnorm}}(z_j \\mid y) \\Delta z}\n$$\n\n### 3. 所需量的计算\n\n有了归一化的离散后验概率密度 $\\pi(z_i \\mid y)$ 和相应的概率质量 $P_i = \\pi(z_i \\mid y) \\Delta z$，我们为每个测试案例计算三个所需的量。\n\n**1. 模态数量：**\n在网格点 $z_i$（对于网格内部的 $i$，即 $i \\in \\{1, \\dots, N-2\\}$）处识别出一个模态，如果该点的后验密度严格大于其紧邻：$\\pi(z_i \\mid y) > \\pi(z_{i-1} \\mid y)$ 和 $\\pi(z_i \\mid y) > \\pi(z_{i+1} \\mid y)$。此外，为了过滤掉不显著的数值波动，模态必须满足 $\\pi(z_i \\mid y) > 10^{-4} \\cdot \\max_{j} \\{ \\pi(z_j \\mid y) \\}$。我们计算满足这两个条件的网格点数量。\n\n**2. 后验标准差：**\n后验标准差 $\\sigma_z = \\sqrt{\\mathrm{Var}[z \\mid y]}$ 是从离散近似计算得出的。\n首先，近似后验均值 $\\mu_z = \\mathrm{E}[z \\mid y]$：\n$$\n\\mu_z \\approx \\sum_{i=1}^{N} z_i \\cdot \\pi(z_i \\mid y) \\Delta z\n$$\n然后，近似后验方差 $\\mathrm{Var}[z \\mid y] = \\mathrm{E}[(z - \\mu_z)^2 \\mid y]$：\n$$\n\\mathrm{Var}[z \\mid y] \\approx \\sum_{i=1}^{N} (z_i - \\mu_z)^2 \\cdot \\pi(z_i \\mid y) \\Delta z\n$$\n标准差是该方差的平方根。\n\n**3. $z^\\dagger$ 附近的后验质量：**\n真实潜值 $z^\\dagger$ 半径 $\\rho = 0.15$ 内的后验质量是所有落在区间 $[z^\\dagger - \\rho, z^\\dagger + \\rho]$ 内的网格点 $z_i$ 的概率质量之和：\n$$\nM = \\sum_{i: \\lvert z_i - z^\\dagger \\rvert \\le \\rho} \\pi(z_i \\mid y) \\Delta z\n$$\n\n### 4. 测试用例分析\n\n*   **案例 A（误设，小噪声）：** $h \\circ g(z) = z$，$y = 1.0$，$\\sigma_y = 0.05$，损坏的先验，在 $z_c = 1.0$ 处有一个半径为 $\\delta=0.3$ 的空洞。似然是在 $z = 1.0$ 处的一个尖锐高斯峰。先验在区间 $(0.7, 1.3)$ 内为零。这两个函数的乘积将导致后验在排除区域的边缘，即 $z=0.7$ 和 $z=1.3$ 处有两个尖锐的峰值。这是先验-数据冲突的典型特征，因此我们期望有 $2$ 个模态。$z^\\dagger=1.0$ 附近的质量将为零。\n\n*   **案例 B（误设，较大噪声）：** 与案例 A 相同，但 $\\sigma_y = 0.5$。似然要宽得多。虽然先验在 $(0.7, 1.3)$ 内仍然为零，但宽泛的似然可能会“溢出”这个缺口。后验仍将呈现双峰形状，但峰值会不那么尖锐，并可能向外移动。对称地，分布可能是鞍形的，但仍有两个局部极大值。来自似然的增加的方差将增加后验标准差，与案例 A 相比。\n\n*   **案例 C（设定良好）：** $h \\circ g(z) = z$，$y = 1.0$，$\\sigma_y = 0.05$，标准正态先验。后验是两个高斯函数（似然 $\\mathcal{N}(z; 1.0, 0.05^2)$ 和先验 $\\mathcal{N}(z; 0, 1)$）的乘积，这导致一个高斯后验。这个后验是单峰的。由于噪声方差 $\\sigma_y^2$ 很小，似然将占主导地位，因此后验将在非常接近 $z = 1.0$ 的地方形成尖峰，标准差接近 $0.05$。大部分后验质量将集中在 $z^\\dagger=1.0$ 附近。\n\n*   **案例 D（非单射模型）：** $h \\circ g(z) = z^2$，$y = 1.0$，$\\sigma_y = 0.05$，标准正态先验。似然项 $\\exp(-(1.0 - z^2)^2 / (2\\sigma_y^2))$ 在 $z^2 = 1.0$ 时最大化，即在 $z = 1.0$ 和 $z = -1.0$ 处。标准正态先验 $\\exp(-z^2/2)$ 是对称的。因此，得到的后验将是双峰且对称的，峰值在 $z = 1.0$ 和 $z = -1.0$ 附近。后验质量的很大一部分将在 $z=1.0$ 附近，但也在 $z=-1.0$ 附近。\n\n这些分析指导了数值结果的实现和验证。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing posterior statistics for four test cases.\n    \"\"\"\n\n    def compute_posterior_stats(g_func, h_func, y, sigma_y, z_dagger, use_corrupted_prior, z_c, delta):\n        \"\"\"\n        Computes posterior stats for a given set of parameters.\n        \"\"\"\n        # 1. Setup grid\n        z_grid = np.linspace(-5.0, 5.0, 20001)\n        dz = z_grid[1] - z_grid[0]\n        \n        # 2. Compute log posterior\n        # Log-likelihood term\n        h_of_g = h_func(g_func(z_grid))\n        log_likelihood = -0.5 * ((y - h_of_g) / sigma_y)**2\n        \n        # Log-prior term\n        log_prior = -0.5 * z_grid**2\n        if use_corrupted_prior:\n            hole_indices = np.abs(z_grid - z_c)  delta\n            log_prior[hole_indices] = -np.inf\n            \n        log_posterior = log_prior + log_likelihood\n        \n        # 3. Normalize posterior\n        # Stabilize by subtracting max before exponentiation\n        log_posterior -= np.nanmax(log_posterior[np.isfinite(log_posterior)])\n        \n        unnorm_posterior = np.exp(log_posterior)\n        \n        # Normalize using numerical integration (sum * dz)\n        integral = np.sum(unnorm_posterior) * dz\n        if integral == 0:\n            # Handle cases where posterior is zero everywhere (e.g., if z_dagger is in a hole with zero noise)\n            norm_posterior = np.zeros_like(unnorm_posterior)\n        else:\n            norm_posterior = unnorm_posterior / integral\n            \n        # 4. Compute required quantities\n        \n        # 4.1. Number of modes\n        num_modes = 0\n        max_density = np.max(norm_posterior)\n        threshold = 1e-4 * max_density\n        \n        if max_density > 0: # Proceed only if there's a non-zero posterior\n            for i in range(1, len(z_grid) - 1):\n                is_peak = norm_posterior[i] > norm_posterior[i-1] and norm_posterior[i] > norm_posterior[i+1]\n                if is_peak and norm_posterior[i] > threshold:\n                    num_modes += 1\n\n        # 4.2. Posterior standard deviation\n        mean_z = np.sum(z_grid * norm_posterior) * dz\n        var_z = np.sum(((z_grid - mean_z)**2) * norm_posterior) * dz\n        std_dev = np.sqrt(var_z)\n        \n        # 4.3. Posterior mass near z_dagger\n        rho = 0.15\n        mass_indices = np.abs(z_grid - z_dagger) = rho\n        mass = np.sum(norm_posterior[mass_indices]) * dz\n        \n        return [num_modes, std_dev, mass]\n\n    # Define test cases from the problem statement\n    test_cases = [\n        # Case A: misspecified, corrupted prior, small noise\n        {'g': lambda z: z, 'h': lambda x: x, 'y': 1.0, 'sigma_y': 0.05, 'z_dagger': 1.0, \n         'use_corrupted': True, 'z_c': 1.0, 'delta': 0.3},\n        # Case B: misspecified, corrupted prior, larger noise\n        {'g': lambda z: z, 'h': lambda x: x, 'y': 1.0, 'sigma_y': 0.5, 'z_dagger': 1.0, \n         'use_corrupted': True, 'z_c': 1.0, 'delta': 0.3},\n        # Case C: well-specified, standard prior\n        {'g': lambda z: z, 'h': lambda x: x, 'y': 1.0, 'sigma_y': 0.05, 'z_dagger': 1.0, \n         'use_corrupted': False, 'z_c': 1.0, 'delta': 0.0},\n        # Case D: non-injective decoder-induced observation, standard prior\n        {'g': lambda z: z, 'h': lambda x: x**2, 'y': 1.0, 'sigma_y': 0.05, 'z_dagger': 1.0, \n         'use_corrupted': False, 'z_c': 1.0, 'delta': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_posterior_stats(\n            g_func=case['g'], \n            h_func=case['h'], \n            y=case['y'], \n            sigma_y=case['sigma_y'],\n            z_dagger=case['z_dagger'],\n            use_corrupted_prior=case['use_corrupted'],\n            z_c=case['z_c'],\n            delta=case['delta']\n        )\n        results.append(result)\n\n    # Format the final output string to ensure standard decimal notation\n    sublist_strs = []\n    for res in results:\n        n, s, m = res\n        # Use f-string formatting to avoid scientific notation\n        sublist_strs.append(f\"[{n},{s:.8f},{m:.8f}]\")\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(sublist_strs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当似然函数过强时，变分自编码器（VAE）在求解逆问题时可能会出现所谓的“后验坍塌”现象，即近似后验忽略了先验信息。本练习旨在帮助您诊断这种情况，并介绍一种基于归一化流的实用修正方法。您将通过一个具体的计算任务，学习如何量化数据在后验推断中的主导程度，并应用简单的线性流来改善后验分布的结构，从而增强推断的鲁棒性 。",
            "id": "3374862",
            "problem": "考虑一个由变分自编码器 (VAE) 表示的带有深度生成先验的线性逆问题。设潜变量为 $z \\in \\mathbb{R}^k$，其先验为 $p(z) = \\mathcal{N}(0, I_k)$，并设生成器被线性化为 $x = W z$，其中 $W \\in \\mathbb{R}^{n \\times k}$ 是一个固定矩阵。测量值服从 $y = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 是高斯噪声，$\\sigma  0$ 且 $m \\in \\{1,2,\\dots,n\\}$ 表示测量数量。在基于 VAE 的逆问题中，近似后验 $q(z \\mid y)$ 通常被建模为一个试图匹配真实后验 $p(z \\mid y)$ 的高斯分布。\n\n仅从贝叶斯法则、高斯先验 $p(z) = \\mathcal{N}(0, I_k)$ 和高斯似然 $p(y \\mid z) = \\mathcal{N}(A W z, \\sigma^2 I_m)$ 出发，执行以下操作：\n\n1. 对于线性化生成模型，推导精确的高斯后验 $p(z \\mid y)$。从贝叶斯法则 $p(z \\mid y) \\propto p(y \\mid z) p(z)$ 开始，并使用上述定义证明后验是高斯分布，其精度矩阵是先验精度与数据诱导的精度之和。用 $W$、$A$、$y$ 和 $\\sigma$ 表示后验均值和协方差。\n\n2. 定义一个标量的数据主导指数，用于量化似然项在后验中压倒先验项的区域。设 $C(\\sigma, m) = \\frac{1}{\\sigma^2} W^\\top A^\\top A W \\in \\mathbb{R}^{k \\times k}$ 为潜空间中数据诱导的精度。考虑每个潜维度的平均数据精度\n$$\n\\alpha(\\sigma, m) = \\frac{1}{k} \\operatorname{tr}\\!\\left(C(\\sigma, m)\\right).\n$$\n使用这个标量来分类后验是由数据主导还是由先验主导。如果 $\\alpha(\\sigma, m)  1$，则判定后验为数据主导，否则为先验主导。解释为什么鉴于先验精度为 $I_k$，这个阈值是一个自然的尺度选择。\n\n3. 提出并分析一种通过归一化流 (NF) 的修正方法，即在潜空间中应用一个可逆线性重参数化 $z_0 = T z$（归一化流的一个特例），其中 $T \\in \\mathbb{R}^{k \\times k}$ 是可逆的，以对组合精度进行预处理。证明选择\n$$\nT = \\left(C(\\sigma, m) + \\varepsilon I_k\\right)^{-1/2}\n$$\n（其中 $\\varepsilon  0$ 是一个小的正则化常数）可将数据精度转换为\n$$\nC_{\\text{flow}}(\\sigma, m) = T^\\top C(\\sigma, m) T = \\left(C(\\sigma, m) + \\varepsilon I_k\\right)^{-1/2} C(\\sigma, m) \\left(C(\\sigma, m) + \\varepsilon I_k\\right)^{-1/2},\n$$\n从而得到的特征值等于 $\\lambda_i / (\\lambda_i + \\varepsilon)$，其中 $\\{\\lambda_i\\}_{i=1}^k$ 是 $C(\\sigma, m)$ 的特征值。论证这使得组合后验精度更接近各向同性形式，并缓解了似然主导先验的区域。将流处理后的数据主导指数定义为\n$$\n\\alpha_{\\text{flow}}(\\sigma, m, \\varepsilon) = \\frac{1}{k} \\operatorname{tr}\\!\\left(C_{\\text{flow}}(\\sigma, m)\\right) = \\frac{1}{k} \\sum_{i=1}^k \\frac{\\lambda_i}{\\lambda_i + \\varepsilon}.\n$$\n使用相同的阈值对流处理后的区域进行分类，并讨论 $\\varepsilon$ 如何在精确白化和数值稳定性之间进行权衡。\n\n您的程序必须对一个固定的生成器 $W \\in \\mathbb{R}^{n \\times k}$ 和测量算子 $A \\in \\mathbb{R}^{m \\times n}$ 实现上述过程，其中测量算子选择 $\\mathbb{R}^{n \\times n}$ 中单位矩阵的前 $m$ 行，即 $A = [I_m, 0]，因此 A^\\top A = \\operatorname{diag}(\\underbrace{1,\\dots,1}_{m}, \\underbrace{0,\\dots,0}_{n-m})$。生成器矩阵指定为\n$$\nW = \\begin{bmatrix}\n1.0  0.0  0.0 \\\\\n0.0  0.7  0.0 \\\\\n0.0  0.0  0.4 \\\\\n0.2  0.1  0.05\n\\end{bmatrix},\n$$\n其中 $n = 4$ 且 $k = 3$。对于每个测试用例，计算并返回包含四个项目的元组 $[\\alpha(\\sigma, m), \\text{data\\_dominated\\_before}, \\alpha_{\\text{flow}}(\\sigma, m, \\varepsilon), \\text{data\\_dominated\\_after}]$，其中 data\\_dominated\\_before 和 data\\_dominated\\_after 是基于阈值规则的布尔值。\n\n对于归一化流正则化，使用 $\\varepsilon = 10^{-2}$。\n\n测试套件（噪声水平 $\\sigma$ 与测量噪声标准差单位相同，测量数量 $m$ 为整数）：\n\n- 情况 1：$\\sigma = 5.0$, $m = 1$。\n- 情况 2：$\\sigma = 0.1$, $m = 4$。\n- 情况 3：$\\sigma = 0.5$, $m = 2$。\n- 情况 4：$\\sigma = 0.01$, $m = 1$。\n- 情况 5：$\\sigma = 2.0$, $m = 3$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身也是一个如上所述格式的列表。例如，输出应类似于 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots]$，其中 $a_i$ 和 $c_i$ 为浮点值，$b_i$ 和 $d_i$ 为布尔值。",
            "solution": "该问题陈述被评估为有效。它在科学上基于贝叶斯推断和线性代数，表述清晰，提供了所有必要信息，并以客观、正式的语言表达。没有矛盾、歧义或伪科学主张。该问题构成了分析线性化生成模型设置中后验分布的一个标准但详细的练习。\n\n### 1. 精确高斯后验的推导\n\n我们给定一个线性高斯模型。潜变量 $z \\in \\mathbb{R}^k$ 的先验是 $p(z) = \\mathcal{N}(z \\mid 0, I_k)$。测量值 $y \\in \\mathbb{R}^m$ 在给定 $z$ 下的似然，由生成过程 $x = Wz$ 和测量模型 $y = Ax + \\varepsilon$（其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$）推导得出。将 $x$ 代入测量模型得到 $y = AWz + \\varepsilon$。这意味着似然为 $p(y \\mid z) = \\mathcal{N}(y \\mid AWz, \\sigma^2 I_m)$。\n\n根据贝叶斯法则，后验分布 $p(z \\mid y)$ 与似然和先验的乘积成正比：\n$$\np(z \\mid y) \\propto p(y \\mid z) p(z)\n$$\n为了找到后验的形式，我们可以处理对数后验，因为这将高斯密度的乘积简化为二次型的和。不依赖于 $z$ 的项被吸收到归一化常数中。\n$$\n\\ln p(z \\mid y) = \\ln p(y \\mid z) + \\ln p(z) + \\text{const.}\n$$\n对数似然是：\n$$\n\\ln p(y \\mid z) = -\\frac{1}{2\\sigma^2} (y - AWz)^\\top (y - AWz) + \\text{const.}\n$$\n展开二次项：\n$$\n(y - AWz)^\\top (y - AWz) = y^\\top y - 2y^\\top AWz + z^\\top W^\\top A^\\top AWz\n$$\n去掉与 $z$ 无关的常数项 $y^\\top y$，对数似然的贡献是：\n$$\n\\ln p(y \\mid z) = -\\frac{1}{2\\sigma^2} (z^\\top W^\\top A^\\top AWz - 2y^\\top AWz) + \\text{const.}\n$$\n对数先验是：\n$$\n\\ln p(z) = -\\frac{1}{2} z^\\top I_k z + \\text{const.} = -\\frac{1}{2} z^\\top z + \\text{const.}\n$$\n将它们结合起来，对数后验是：\n$$\n\\ln p(z \\mid y) = -\\frac{1}{2\\sigma^2} z^\\top W^\\top A^\\top AWz + \\frac{1}{\\sigma^2} y^\\top AWz - \\frac{1}{2} z^\\top z + \\text{const.}\n$$\n我们可以将关于 $z$ 的二次项和线性项重新组合：\n$$\n\\ln p(z \\mid y) = -\\frac{1}{2} \\left[ z^\\top \\left( I_k + \\frac{1}{\\sigma^2} W^\\top A^\\top AW \\right) z - 2 \\left( \\frac{1}{\\sigma^2} y^\\top AW \\right) z \\right] + \\text{const.}\n$$\n这个表达式是关于 $z$ 的二次型，这证实了后验 $p(z \\mid y)$ 是一个高斯分布，比如说 $\\mathcal{N}(z \\mid \\mu_z, \\Sigma_z)$。一个一般多元高斯分布的对数概率密度函数是：\n$$\n\\ln \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z) = -\\frac{1}{2} (z - \\mu_z)^\\top \\Sigma_z^{-1} (z - \\mu_z) + \\text{const.} = -\\frac{1}{2} (z^\\top \\Sigma_z^{-1} z - 2\\mu_z^\\top \\Sigma_z^{-1} z) + \\text{const.}\n$$\n通过将此通用形式与我们推导的对数后验进行比较，我们可以识别出后验精度矩阵 $\\Sigma_z^{-1}$ 和均值 $\\mu_z$。\n\n后验精度（协方差的逆）矩阵是乘以 $z^\\top(\\cdot)z$ 的项：\n$$\n\\Sigma_z^{-1} = I_k + \\frac{1}{\\sigma^2} W^\\top A^\\top AW\n$$\n这表明后验精度是先验精度（$I_k$）与数据诱导的精度（$ \\frac{1}{\\sigma^2} W^\\top A^\\top AW $）之和。\n\n后验均值 $\\mu_z$ 通过匹配 $z$ 的线性项来找到：\n$$\n\\mu_z^\\top \\Sigma_z^{-1} = \\frac{1}{\\sigma^2} y^\\top AW \\implies (\\Sigma_z^{-1})^\\top \\mu_z = \\left(\\frac{1}{\\sigma^2} W^\\top A^\\top y\\right)\n$$\n由于 $\\Sigma_z^{-1}$ 是对称的，所以 $(\\Sigma_z^{-1})^\\top = \\Sigma_z^{-1}$。因此：\n$$\n\\Sigma_z^{-1} \\mu_z = \\frac{1}{\\sigma^2} W^\\top A^\\top y\n$$\n解出 $\\mu_z$：\n$$\n\\mu_z = \\left( \\Sigma_z^{-1} \\right)^{-1} \\left( \\frac{1}{\\sigma^2} W^\\top A^\\top y \\right) = \\Sigma_z \\left( \\frac{1}{\\sigma^2} W^\\top A^\\top y \\right)\n$$\n所以，后验是 $p(z \\mid y) = \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z)$，其中：\n- **后验协方差**: $\\Sigma_z = \\left(I_k + \\frac{1}{\\sigma^2} W^\\top A^\\top AW \\right)^{-1}$\n- **后验均值**: $\\mu_z = \\left(I_k + \\frac{1}{\\sigma^2} W^\\top A^\\top AW \\right)^{-1} \\left(\\frac{1}{\\sigma^2} W^\\top A^\\top y\\right)$\n\n### 2. 数据主导指数\n\n后验精度 $\\Sigma_z^{-1} = I_k + C(\\sigma, m)$ 是两项之和：先验精度 $I_k$ 和数据诱导的精度 $C(\\sigma, m) = \\frac{1}{\\sigma^2} W^\\top A^\\top AW$。这两项矩阵的相对大小决定了后验的形状更多地由先验还是由数据（似然）决定。\n\n一个用于概括精度矩阵“强度”的简单标量摘要是它的迹，即其特征值之和，表示在所有正交方向上精度的总和。先验的每个潜维度的平均精度为 $\\frac{1}{k} \\operatorname{tr}(I_k) = \\frac{k}{k} = 1$。这提供了一个自然的尺度。\n\n所提出的数据主导指数 $\\alpha(\\sigma, m) = \\frac{1}{k} \\operatorname{tr}(C(\\sigma, m))$ 计算了由数据贡献的每个潜维度的平均精度。因此，阈值 $\\alpha(\\sigma, m)  1$ 是一个自然的选择，因为它直接将平均数据诱导精度与平均先验精度进行比较。如果 $\\alpha(\\sigma, m)  1$，这表明平均而言，数据项具有更强的正则化效应（即，它更多地压缩后验分布）比标准高斯先验更强。因此，这个区域被宣布为“数据主导”。反之，如果 $\\alpha(\\sigma, m) \\le 1$，则先验的影响平均更强或相当，该区域为“先验主导”。\n\n### 3. 归一化流修正方法\n\n在用变分分布（例如，在VAE中）近似后验 $p(z \\mid y)$ 时，一个主要挑战是当后验高度各向异性或由似然主导时。这通常发生在数据精度矩阵 $C(\\sigma, m)$ 的特征值很大且跨越几个数量级时。所提出的归一化流 (NF) 是一个简单的线性重参数化 $z_0 = Tz$，旨在对问题进行预处理。\n\n我们设置 $T = (C(\\sigma, m) + \\varepsilon I_k)^{-1/2}$，其中 $\\varepsilon  0$ 是一个小数。分析的重点是转换后的数据精度矩阵 $C_{\\text{flow}} = T^\\top C(\\sigma, m) T$。由于 $C(\\sigma, m)$ 是对称的，所以 $T$ 也是对称的 ($T^\\top = T$)。设 $C = C(\\sigma, m)$。\n$$\nC_{\\text{flow}} = (C + \\varepsilon I_k)^{-1/2} C (C + \\varepsilon I_k)^{-1/2}\n$$\n为了分析其特征值，我们使用 $C$ 的特征分解。由于 $C$ 是一个实对称矩阵，它可由正交矩阵 $U$ 对角化：$C = U \\Lambda U^\\top$，其中 $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_k)$ 包含 $C$ 的特征值。\n\n矩阵 $C + \\varepsilon I_k$ 共享相同的特征向量 $U$：\n$$\nC + \\varepsilon I_k = U \\Lambda U^\\top + \\varepsilon U I_k U^\\top = U(\\Lambda + \\varepsilon I_k)U^\\top\n$$\n其逆平方根则为：\n$$\nT = (C + \\varepsilon I_k)^{-1/2} = U(\\Lambda + \\varepsilon I_k)^{-1/2}U^\\top\n$$\n将此代入 $C_{\\text{flow}}$ 的表达式中：\n$$\nC_{\\text{flow}} = \\left( U(\\Lambda + \\varepsilon I_k)^{-1/2}U^\\top \\right) \\left( U \\Lambda U^\\top \\right) \\left( U(\\Lambda + \\varepsilon I_k)^{-1/2}U^\\top \\right)\n$$\n由于 $U^\\top U = I_k$，表达式简化为：\n$$\nC_{\\text{flow}} = U \\left( (\\Lambda + \\varepsilon I_k)^{-1/2} \\Lambda (\\Lambda + \\varepsilon I_k)^{-1/2} \\right) U^\\top\n$$\n中间项是一个对角矩阵，其元素为 $\\frac{\\lambda_i}{\\sqrt{\\lambda_i + \\varepsilon} \\sqrt{\\lambda_i + \\varepsilon}} = \\frac{\\lambda_i}{\\lambda_i + \\varepsilon}$。\n因此，$C_{\\text{flow}}$ 的特征值为 $\\left\\{ \\frac{\\lambda_i}{\\lambda_i + \\varepsilon} \\right\\}_{i=1}^k$。\n\n这种变换缓解了数据主导，因为每个新的特征值都是有界的：$0 \\le \\frac{\\lambda_i}{\\lambda_i + \\varepsilon}  1$，因为 $\\lambda_i \\ge 0$（作为半正定矩阵的特征值）且 $\\varepsilon  0$。流处理后的数据主导指数为 $\\alpha_{\\text{flow}} = \\frac{1}{k} \\operatorname{tr}(C_{\\text{flow}}) = \\frac{1}{k} \\sum_{i=1}^k \\frac{\\lambda_i}{\\lambda_i + \\varepsilon}$。由于和中的每一项都小于 1，平均值 $\\alpha_{\\text{flow}}$ 将始终小于 1。这种变换有效地“驯服”了数据精度，确保根据我们选择的度量，它永远不会压倒先验精度。它使数据精度矩阵更接近一个投影算子，将大的特征值压缩至 1，小的特征值压缩至 0。这使得组合后验精度条件更好，更接近各向同性，这对许多变分推断算法都有益。\n\n正则化常数 $\\varepsilon$ 提供了一个权衡。一个非常小的 $\\varepsilon$ 会导致一种在 $\\lambda_i  0$ 时积极地白化数据精度的变换（使特征值接近 1），但如果 $C$ 是奇异或病态的（即具有等于或接近 0 的特征值），则可能在数值上不稳定。一个较大的 $\\varepsilon$ 提高了数值稳定性，但导致白化效果较差，因为 $\\frac{\\lambda_i}{\\lambda_i + \\varepsilon}$ 变得更小。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Computes data-dominance indices for a linearized VAE inverse problem\n    before and after a normalizing flow reparameterization.\n    \"\"\"\n    # Define constants from the problem statement\n    W = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 0.7, 0.0],\n        [0.0, 0.0, 0.4],\n        [0.2, 0.1, 0.05]\n    ])\n    n, k = W.shape\n    epsilon = 1e-2\n\n    # Define the test cases\n    test_cases = [\n        # (sigma, m)\n        (5.0, 1),\n        (0.1, 4),\n        (0.5, 2),\n        (0.01, 1),\n        (2.0, 3),\n    ]\n\n    results = []\n    for sigma, m in test_cases:\n        # 1. Construct the data-precision matrix C\n        \n        # A^T A selects the first m components. It is a diagonal matrix\n        # with m ones and (n-m) zeros on the diagonal.\n        diag_elements = np.zeros(n)\n        diag_elements[:m] = 1.0\n        AtA = np.diag(diag_elements)\n\n        # C = (1/sigma^2) * W^T * A^T * A * W\n        C = (1 / sigma**2) * W.T @ AtA @ W\n\n        # 2. Compute the pre-flow data-dominance index\n        \n        # alpha = (1/k) * tr(C)\n        alpha = np.trace(C) / k\n        data_dominated_before = alpha > 1\n\n        # 3. Compute the post-flow data-dominance index\n        \n        # Get eigenvalues of C. Since C is real and symmetric, use eigvalsh for\n        # efficiency and numerical stability.\n        lambdas = np.linalg.eigvalsh(C)\n        \n        # The eigenvalues of C_flow are lambda_i / (lambda_i + epsilon).\n        # The trace is the sum of these eigenvalues.\n        # alpha_flow = (1/k) * sum(lambda_i / (lambda_i + epsilon))\n        alpha_flow = np.mean(lambdas / (lambdas + epsilon))\n        data_dominated_after = alpha_flow > 1\n\n        # 4. Store the results for this case\n        case_result = [alpha, data_dominated_before, alpha_flow, data_dominated_after]\n        results.append(case_result)\n\n    # Format the final output string as specified.\n    # str(list) automatically creates the correct string representation '[a, b, c, d]'\n    # including correct boolean formatting for True/False.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}