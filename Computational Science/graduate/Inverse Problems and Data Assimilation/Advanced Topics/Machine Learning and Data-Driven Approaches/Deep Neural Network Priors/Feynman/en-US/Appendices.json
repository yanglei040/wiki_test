{
    "hands_on_practices": [
        {
            "introduction": "The Deep Image Prior (DIP) framework reveals that the structure of a convolutional neural network can itself act as a powerful regularizer, even without being trained on external data. This exercise delves into the theoretical underpinnings of this phenomenon by exploring the concept of 'spectral bias'. By analyzing the optimization dynamics through the lens of the Neural Tangent Kernel (NTK), you will analytically demonstrate how DIP preferentially reconstructs low-frequency features first, providing a rigorous explanation for its effectiveness as an implicit prior. ",
            "id": "3375205",
            "problem": "Consider the Deep Image Prior (DIP) reconstruction framework, in which the unknown image is parameterized as $x = G_{\\theta}(z)$ for a fixed input $z$ and trainable parameters $\\theta$, and is estimated by minimizing the squared data misfit $\\|A G_{\\theta}(z) - y\\|^{2}$ with respect to $\\theta$ using gradient descent. Assume the generator $G_{\\theta}$ is an infinitely wide, translation-equivariant convolutional network with independent and identically distributed zero-mean filters at initialization, so that in the infinite-width and small learning-rate limit, the function-space gradient flow for $f_{t} = G_{\\theta(t)}(z)$ is well-approximated by\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}},\n$$\nwhere $\\gamma > 0$ is the learning-rate scaling and $K$ is the Neural Tangent Kernel (NTK) operator of $G_{\\theta}$ at initialization. Let $A$ be a linear, shift-invariant forward operator on images (e.g., a blur), and denote its adjoint by $A^{\\top}$. By standard properties of least-squares gradients, $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$. Assume $K$ is shift-invariant with an isotropic Gaussian Fourier multiplier and $A$ is an isotropic Gaussian blur, specifically\n$$\n\\widehat{K}(\\omega) = \\exp\\!\\big(- \\sigma^{2} \\|\\omega\\|^{2}\\big), \\quad \\widehat{A}(\\omega) = \\exp\\!\\big(- \\beta \\|\\omega\\|^{2}\\big),\n$$\nfor parameters $\\sigma^{2} > 0$ and $\\beta > 0$, where $\\omega \\in \\mathbb{R}^{2}$ denotes the spatial frequency. Under these assumptions, the Fourier modes of the residual $\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$ evolve independently, where $f_{\\infty}$ is the fixed point of the gradient flow.\n\nStarting from these premises and using only properties of linear operators, adjoints, and Fourier diagonalization of shift-invariant operators, derive the expression for the exponential decay rate $\\lambda(\\omega)$ governing the evolution of $\\widehat{r}_{t}(\\omega)$, defined by the ordinary differential equation\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega).\n$$\nThen, for $\\sigma^{2} = 10^{-3}$, $\\beta = 5 \\times 10^{-4}$, $\\gamma = 1$, and two isotropic frequency magnitudes $\\|\\omega_{l}\\| = 20$ and $\\|\\omega_{h}\\| = 60$, compute the ratio $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$. Round your final numerical answer to four significant figures. The final answer must be a single real number without units.",
            "solution": "The user-provided problem is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- The image reconstruction framework is Deep Image Prior (DIP): $x = G_{\\theta}(z)$.\n- The objective function is the squared data misfit: $\\|A G_{\\theta}(z) - y\\|^{2}$.\n- The optimization is by gradient descent with respect to parameters $\\theta$.\n- The infinite-width, small-learning-rate gradient flow for $f_{t} = G_{\\theta(t)}(z)$ is:\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\n- The learning-rate scaling is $\\gamma > 0$.\n- The operator $K$ is the Neural Tangent Kernel (NTK) of $G_{\\theta}$ at initialization.\n- The gradient of the least-squares loss is: $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$.\n- $A$ is a linear, shift-invariant forward operator, and $A^{\\top}$ is its adjoint.\n- $K$ is a shift-invariant operator.\n- The Fourier multiplier of $K$ is: $\\widehat{K}(\\omega) = \\exp(-\\sigma^{2} \\|\\omega\\|^{2})$ for $\\sigma^2 > 0$.\n- The Fourier multiplier of $A$ is: $\\widehat{A}(\\omega) = \\exp(-\\beta \\|\\omega\\|^{2})$ for $\\beta > 0$.\n- The residual in the Fourier domain is defined as: $\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$, where $f_{\\infty}$ is the fixed point of the flow.\n- The evolution of the residual is given by the form: $\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$.\n- Numerical values for parameters are: $\\sigma^{2} = 10^{-3}$, $\\beta = 5 \\times 10^{-4}$, $\\gamma = 1$.\n- Two isotropic frequency magnitudes are given: $\\|\\omega_{l}\\| = 20$ and $\\|\\omega_{h}\\| = 60$.\n- The task is to derive the expression for the decay rate $\\lambda(\\omega)$ and compute the ratio $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the theory of infinite-width neural networks, specifically the Neural Tangent Kernel (NTK) model. The application to inverse problems via the Deep Image Prior (DIP) is a current and valid area of research. All operators and concepts (gradient flow, Fourier analysis of shift-invariant operators, adjoints) are standard in applied mathematics and signal processing.\n- **Well-Posed**: The problem is well-posed. It provides all necessary equations, definitions, and constants to derive the requested quantity $\\lambda(\\omega)$ and calculate the final ratio.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Verdict**: The problem is deemed valid as it is self-contained, scientifically sound, and unambiguous.\n\n### Step 3: Proceed to Solution\n\nWe begin with the given gradient flow equation for the function $f_t$:\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\nSubstituting the provided expression for the gradient $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$, we get:\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, [2 A^{\\top} (A f_{t} - y)] = -2 \\gamma \\, K A^{\\top} (A f_{t} - y)\n$$\nAt the fixed point $f_{\\infty}$, the time derivative is zero, $\\partial_{t} f_{\\infty} = 0$. This implies that $f_{\\infty}$ satisfies:\n$$\n-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y) = 0\n$$\nWe now analyze the evolution of the residual $r_{t} = f_{t} - f_{\\infty}$. The time derivative of the residual is $\\partial_{t} r_{t} = \\partial_{t} (f_{t} - f_{\\infty}) = \\partial_{t} f_{t}$. We can subtract the fixed point equation (multiplied by $1$) from the flow equation for $f_t$:\n$$\n\\partial_{t} f_{t} - 0 = -2 \\gamma \\, K A^{\\top} (A f_{t} - y) - [-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y)]\n$$\n$$\n\\partial_{t} (f_{t} - f_{\\infty}) = -2 \\gamma \\, K A^{\\top} [ (A f_{t} - y) - (A f_{\\infty} - y) ]\n$$\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} (A f_{t} - A f_{\\infty})\n$$\nSince $A$ is a linear operator, $A f_{t} - A f_{\\infty} = A(f_{t} - f_{\\infty}) = A r_{t}$. The equation for the residual becomes:\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} A \\, r_{t}\n$$\nThis is a linear ordinary differential equation in function space. Since the operators $K$, $A^{\\top}$, and $A$ are all shift-invariant, they are diagonalized by the Fourier transform. We can solve this equation by transforming it to the Fourier domain. Let $\\widehat{r}_{t}(\\omega)$ be the Fourier transform of $r_{t}$. The Fourier transform of an application of a shift-invariant operator $L$ to a function $g$ is the product of their Fourier representations, $\\mathcal{F}\\{Lg\\}(\\omega) = \\widehat{L}(\\omega) \\widehat{g}(\\omega)$. Applying this to our equation:\n$$\n\\mathcal{F}\\{\\partial_{t} r_{t}\\}(\\omega) = \\mathcal{F}\\{-2 \\gamma \\, K A^{\\top} A \\, r_{t}\\}(\\omega)\n$$\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\widehat{(K A^{\\top} A)}(\\omega) \\, \\widehat{r}_{t}(\\omega)\n$$\nThe Fourier multiplier of the composite operator $K A^{\\top} A$ is the product of the individual Fourier multipliers:\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) \\, \\widehat{A^{\\top}}(\\omega) \\, \\widehat{A}(\\omega)\n$$\nFor a linear operator $A$, the multiplier of its adjoint $A^{\\top}$ is the complex conjugate of its own multiplier, i.e., $\\widehat{A^{\\top}}(\\omega) = (\\widehat{A}(\\omega))^{*}$. The given multiplier for $A$ is $\\widehat{A}(\\omega) = \\exp(-\\beta \\|\\omega\\|^{2})$. Since $\\beta$ and $\\|\\omega\\|^2$ are real, $\\widehat{A}(\\omega)$ is a real-valued function. Therefore, $(\\widehat{A}(\\omega))^{*} = \\widehat{A}(\\omega)$, so $\\widehat{A^{\\top}}(\\omega) = \\widehat{A}(\\omega)$.\nThe composite multiplier is then:\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) (\\widehat{A}(\\omega))^{2}\n$$\nSubstituting the given forms for $\\widehat{K}(\\omega)$ and $\\widehat{A}(\\omega)$:\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\left( \\exp(-\\beta \\|\\omega\\|^{2}) \\right)^{2} = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\exp(-2\\beta \\|\\omega\\|^{2})\n$$\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\nPlugging this back into the differential equation for $\\widehat{r}_{t}(\\omega)$:\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2}) \\, \\widehat{r}_{t}(\\omega)\n$$\nWe are given that this equation has the form $\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$. By direct comparison, we identify the decay rate $\\lambda(\\omega)$:\n$$\n\\lambda(\\omega) = 2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\nNow, we must compute the ratio $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$ for the given frequency magnitudes $\\|\\omega_{h}\\| = 60$ and $\\|\\omega_{l}\\| = 20$.\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\frac{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2})}{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2})}\n$$\nThe term $2\\gamma$ cancels out:\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2} + (\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2}) = \\exp(-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}))\n$$\nWe substitute the given numerical values:\n$\\sigma^{2} = 10^{-3}$\n$\\beta = 5 \\times 10^{-4}$\n$\\|\\omega_{l}\\| = 20 \\implies \\|\\omega_{l}\\|^{2} = 400$\n$\\|\\omega_{h}\\| = 60 \\implies \\|\\omega_{h}\\|^{2} = 3600$\n\nFirst, we calculate the coefficient in the exponent:\n$$\n\\sigma^{2} + 2\\beta = 10^{-3} + 2(5 \\times 10^{-4}) = 10^{-3} + 10 \\times 10^{-4} = 10^{-3} + 10^{-3} = 2 \\times 10^{-3}\n$$\nNext, we calculate the difference of the squared frequency magnitudes:\n$$\n\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2} = 3600 - 400 = 3200\n$$\nNow, we compute the exponent:\n$$\n-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}) = -(2 \\times 10^{-3}) \\times 3200 = -6.4\n$$\nFinally, we compute the ratio:\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-6.4) \\approx 0.0016615555...\n$$\nRounding this result to four significant figures gives $0.001662$. In scientific notation, this is $1.662 \\times 10^{-3}$.",
            "answer": "$$\\boxed{1.662 \\times 10^{-3}}$$"
        },
        {
            "introduction": "When employing a Deep Neural Network as an explicit prior, solving the inverse problem often involves minimizing a non-convex Maximum A Posteriori (MAP) objective function using gradient-based methods. The performance of these methods is critically dependent on the geometric properties of this function, which are directly shaped by the network's architecture. This numerical practice provides a concrete investigation into how common components like normalization layers alter the objective's smoothness, thereby directly impacting the choice of learning rate and the convergence behavior of the optimization algorithm. ",
            "id": "3375162",
            "problem": "Consider the Maximum A Posteriori (MAP) estimation problem in which a state vector $x \\in \\mathbb{R}^n$ is estimated from linear observations with Gaussian noise and a neural prior energy. The objective is to minimize the scalar objective\n$$\nJ(x) \\triangleq \\frac{1}{2\\sigma^2}\\lVert A x - y \\rVert_2^2 + \\lambda E_\\phi(x),\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$, $\\sigma > 0$, $\\lambda > 0$, and $E_\\phi(x)$ is the neural network energy prior. We study how normalization layers inside $E_\\phi(x)$ affect Lipschitz constants relevant for gradient-based MAP convergence.\n\nFundamental definitions to use:\n- A function $f: \\mathbb{R}^n \\to \\mathbb{R}$ has a Lipschitz continuous gradient with constant $L \\ge 0$ if for all $x, x' \\in \\mathbb{R}^n$,\n$$\n\\lVert \\nabla f(x) - \\nabla f(x') \\rVert_2 \\le L \\lVert x - x' \\rVert_2.\n$$\n- Gradient descent with fixed step size $\\eta > 0$ on an $L$-smooth objective $f$ satisfies the descent lemma\n$$\nf\\big(x - \\eta \\nabla f(x)\\big) \\le f(x) - \\eta \\Big(1 - \\frac{L\\eta}{2}\\Big)\\lVert \\nabla f(x) \\rVert_2^2,\n$$\nwhich guarantees $f$ is non-increasing whenever $\\eta \\le \\frac{1}{L}$.\n\nIn this problem, the data term is $g(x) \\triangleq \\frac{1}{2\\sigma^2}\\lVert A x - y \\rVert_2^2$, whose gradient is $\\nabla g(x) = \\frac{1}{\\sigma^2}A^\\top(Ax - y)$. It is known that $\\nabla g$ is Lipschitz with constant\n$$\nL_g = \\frac{1}{\\sigma^2} \\lVert A^\\top A \\rVert_{\\mathrm{op}} = \\frac{1}{\\sigma^2} \\sigma_{\\max}(A)^2,\n$$\nwhere $\\sigma_{\\max}(A)$ is the largest singular value of $A$.\n\nThe prior energy $E_\\phi(x)$ is realized by a two-layer neural network with a hidden layer of width $h$ and the hyperbolic tangent activation. The pre-activation $a(x) \\in \\mathbb{R}^h$ is\n$$\na(x) = W_1 x + b_1,\n$$\nfollowed by an optional normalization, then a pointwise nonlinearity $s(x) = \\tanh(z(x))$, and a linear readout\n$$\nE_\\phi(x) = w_2^\\top s(x) + c.\n$$\nThree normalization configurations for $z(x)$ are considered:\n- No normalization: $z(x) = a(x)$.\n- Batch Normalization (BN) in inference mode with frozen statistics: for each hidden unit $i \\in \\{1,\\dots,h\\}$,\n$$\nz_i(x) = \\gamma^{\\mathrm{bn}}_i \\frac{a_i(x) - \\mu^{\\mathrm{bn}}_i}{\\sqrt{(\\sigma^{\\mathrm{bn}}_i)^2 + \\varepsilon}} + \\beta^{\\mathrm{bn}}_i,\n$$\nwith constants $\\mu^{\\mathrm{bn}}_i$, $(\\sigma^{\\mathrm{bn}}_i)^2$, $\\gamma^{\\mathrm{bn}}_i$, $\\beta^{\\mathrm{bn}}_i$, and $\\varepsilon > 0$.\n- Layer Normalization (LN): for a vector $a \\in \\mathbb{R}^h$,\n$$\n\\mu(a) = \\frac{1}{h}\\sum_{i=1}^h a_i, \\quad \\mathrm{var}(a) = \\frac{1}{h}\\sum_{i=1}^h \\big(a_i - \\mu(a)\\big)^2,\n$$\nand for each hidden unit $i \\in \\{1,\\dots,h\\}$,\n$$\nz_i(x) = \\gamma^{\\mathrm{ln}}_i \\frac{a_i(x) - \\mu\\big(a(x)\\big)}{\\sqrt{\\mathrm{var}\\big(a(x)\\big) + \\varepsilon}} + \\beta^{\\mathrm{ln}}_i.\n$$\n\nBecause $E_\\phi$ may be nonconvex, we will use numerical estimation to bound the Lipschitz constant of its gradient. Specifically, define an empirical estimator over a finite sample set $\\mathcal{S} = \\{x^{(1)},\\dots,x^{(S)}\\}$:\n$$\n\\widehat{L}_{\\nabla E} \\triangleq \\max_{\\substack{i,j \\in \\{1,\\dots,S\\} \\\\ i \\ne j}} \\frac{\\lVert \\widehat{\\nabla} E_\\phi(x^{(i)}) - \\widehat{\\nabla} E_\\phi(x^{(j)}) \\rVert_2}{\\lVert x^{(i)} - x^{(j)} \\rVert_2},\n$$\nwhere $\\widehat{\\nabla} E_\\phi$ is a central-difference finite-difference approximation of the gradient with a small symmetric step $h_{\\mathrm{fd}} > 0$ in each coordinate. The overall estimated Lipschitz constant for $\\nabla J$ is then\n$$\n\\widehat{L}_J \\triangleq L_g + \\lambda \\widehat{L}_{\\nabla E}.\n$$\nWe will test gradient descent with step size selected from a dimensionless scaling factor $\\alpha > 0$ via\n$$\n\\eta = \\frac{\\alpha}{\\widehat{L}_J}.\n$$\nThe non-increase of $J$ along iterations is predicted when $\\alpha \\le 1$ by the descent lemma applied to $\\widehat{L}_J$.\n\nYou must implement a program that:\n- Computes $L_g$ from the provided $A$ and $\\sigma$.\n- Constructs $E_\\phi(x)$ for each normalization configuration.\n- Estimates $\\widehat{L}_{\\nabla E}$ using $S$ samples uniformly drawn from a hypercube and central-difference gradients with step $h_{\\mathrm{fd}}$.\n- Forms $\\widehat{L}_J$ and sets $\\eta = \\alpha/\\widehat{L}_J$.\n- Runs $T$ iterations of gradient descent on $J$ from a fixed initial condition $x_0$ using $\\nabla J(x) = \\frac{1}{\\sigma^2}A^\\top(Ax - y) + \\lambda \\widehat{\\nabla} E_\\phi(x)$, where $\\widehat{\\nabla} E_\\phi(x)$ is computed by central differences with the same $h_{\\mathrm{fd}}$.\n- Reports, for each test case, the tuple of six quantities:\n  1. The estimated $\\widehat{L}_{\\nabla E}$ as a float.\n  2. The estimated $\\widehat{L}_J$ as a float.\n  3. The step size $\\eta$ as a float.\n  4. A predicted monotonicity flag as an integer, defined as $1$ if $\\alpha \\le 1$ and $0$ otherwise.\n  5. An observed monotonicity flag as an integer, defined as $1$ if $J$ is non-increasing over all $T$ iterations (allow a numerical tolerance of $10^{-12}$) and $0$ otherwise.\n  6. A convergence flag as an integer, defined as $1$ if $\\lVert \\nabla J(x^{(k)}) \\rVert_2 \\le 10^{-3}$ for some iteration $k \\in \\{1,\\dots,T\\}$ and $0$ otherwise.\n\nUse the following fixed numerical specifications:\n- Dimensions: $n = 3$, $m = 3$, $h = 3$.\n- Data: \n  - $A = \\begin{bmatrix} 1.2 & 0.2 & -0.3 \\\\ 0.1 & 0.9 & 0.0 \\\\ -0.4 & 0.3 & 1.1 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.2 \\end{bmatrix}$,\n  - $\\sigma = 1.0$,\n  - $\\lambda = 0.5$.\n- Network:\n  - $W_1 = \\begin{bmatrix} 0.8 & -0.5 & 0.3 \\\\ 0.2 & 0.7 & -0.6 \\\\ -0.4 & 0.1 & 0.9 \\end{bmatrix}$,\n  - $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n  - $w_2 = \\begin{bmatrix} 0.7 \\\\ -0.3 \\\\ 0.5 \\end{bmatrix}$,\n  - $c = 0.0$,\n  - activation is the hyperbolic tangent $\\tanh$.\n- Batch Normalization (BN) parameters:\n  - $\\gamma^{\\mathrm{bn}} = \\begin{bmatrix} 0.5 \\\\ 0.8 \\\\ 0.6 \\end{bmatrix}$,\n  - $\\beta^{\\mathrm{bn}} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\mu^{\\mathrm{bn}} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $(\\sigma^{\\mathrm{bn}})^2 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-5}$.\n- Layer Normalization (LN) parameters:\n  - Case $\\mathrm{LN}\\text{-}1$: $\\gamma^{\\mathrm{ln}} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$, $\\beta^{\\mathrm{ln}} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $\\varepsilon = 10^{-5}$,\n  - Case $\\mathrm{LN}\\text{-}2$: $\\gamma^{\\mathrm{ln}} = \\begin{bmatrix} 2.0 \\\\ 2.0 \\\\ 2.0 \\end{bmatrix}$, $\\beta^{\\mathrm{ln}} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $\\varepsilon = 10^{-5}$.\n- Finite-difference gradient estimation:\n  - Central-difference step $h_{\\mathrm{fd}} = 10^{-5}$ along each canonical direction in $\\mathbb{R}^3$.\n- Lipschitz estimation sampling:\n  - Number of samples $S = 32$,\n  - Draw $x^{(s)}$ independently and uniformly from the cube $[-0.5, 0.5]^3$ for $s \\in \\{1,\\dots,S\\}$,\n  - Use a fixed pseudorandom seed of $0$ for reproducibility.\n- Gradient descent:\n  - Initial point $x_0 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.05 \\end{bmatrix}$,\n  - Iterations $T = 80$.\n\nTest suite. Run four test cases distinguished by normalization and step scaling $\\alpha$:\n- Case $1$: No normalization, $\\alpha = 0.8$.\n- Case $2$: Batch Normalization, $\\alpha = 1.2$.\n- Case $3$: Layer Normalization with $\\gamma^{\\mathrm{ln}} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$, $\\alpha = 1.0$.\n- Case $4$: Layer Normalization with $\\gamma^{\\mathrm{ln}} = \\begin{bmatrix} 2.0 \\\\ 2.0 \\\\ 2.0 \\end{bmatrix}$, $\\alpha = 0.8$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in order and is itself a list of six entries:\n$$\n\\big[\\,[\\widehat{L}_{\\nabla E},\\, \\widehat{L}_J,\\, \\eta,\\, \\mathrm{pred},\\, \\mathrm{obs},\\, \\mathrm{conv}],\\ \\dots\\ \\big].\n$$\nAll floating-point values should be printed in standard Python string form; all flags are integers $0$ or $1$.",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-defined numerical experiment in the field of computational science, specifically concerning optimization for inverse problems with neural network priors. The problem is scientifically grounded, self-contained, and free of contradictions or ambiguities. All necessary parameters and procedures are specified, allowing for a unique and reproducible solution.\n\nThe solution proceeds as follows:\n\n1.  **Decomposition of the Objective Function**: The objective function $J(x)$ is a sum of two terms: a data-fidelity term $g(x) = \\frac{1}{2\\sigma^2}\\lVert A x - y \\rVert_2^2$ and a regularization term (neural prior) $\\lambda E_\\phi(x)$. The gradient is correspondingly $\\nabla J(x) = \\nabla g(x) + \\lambda \\nabla E_\\phi(x)$.\n\n2.  **Lipschitz Constant of the Data Term Gradient**: The gradient of the data term, $\\nabla g(x) = \\frac{1}{\\sigma^2}A^\\top(Ax - y)$, is linear in $x$. Its Lipschitz constant $L_g$ is given by the operator norm of its Jacobian, which is $\\frac{1}{\\sigma^2}A^\\top A$. This norm is the largest eigenvalue of the matrix, which equals the square of the largest singular value of $A$, i.e., $L_g = \\frac{1}{\\sigma^2}\\sigma_{\\max}(A)^2$. This value is computed once using numerical linear algebra routines.\n\n3.  **Neural Network Prior and its Gradient**: The prior energy $E_\\phi(x)$ is defined by a two-layer neural network. Three configurations for the hidden layer are specified: no normalization, Batch Normalization (BN) in inference mode, and Layer Normalization (LN). The formulas for each are implemented as distinct cases within a function representing $E_\\phi(x)$.\n    The problem requires using a finite-difference approximation for the gradient of the prior, $\\widehat{\\nabla} E_\\phi(x)$. A central-difference scheme is specified:\n    $$\n    (\\widehat{\\nabla} E_\\phi(x))_i = \\frac{E_\\phi(x + h_{\\mathrm{fd}} e_i) - E_\\phi(x - h_{\\mathrm{fd}} e_i)}{2 h_{\\mathrm{fd}}},\n    $$\n    where $e_i$ is the $i$-th canonical basis vector and $h_{\\mathrm{fd}}$ is a small step size. This numerical gradient is used for both the Lipschitz constant estimation and the gradient descent updates.\n\n4.  **Estimation of the Prior's Gradient Lipschitz Constant**: The Lipschitz constant of $\\nabla E_\\phi$, denoted $L_{\\nabla E}$, is estimated empirically. A set of $S$ points $\\{x^{(s)}\\}_{s=1}^S$ is sampled from a uniform distribution over a hypercube. The estimator $\\widehat{L}_{\\nabla E}$ is the maximum observed ratio of the norm of gradient differences to the norm of point differences over all distinct pairs of sample points:\n    $$\n    \\widehat{L}_{\\nabla E} = \\max_{i \\ne j} \\frac{\\lVert \\widehat{\\nabla} E_\\phi(x^{(i)}) - \\widehat{\\nabla} E_\\phi(x^{(j)}) \\rVert_2}{\\lVert x^{(i)} - x^{(j)} \\rVert_2}.\n    $$\n    This value is computed for each of the four test cases, as it depends on the normalization scheme.\n\n5.  **Gradient Descent Implementation**: For each test case, we run gradient descent to minimize $J(x)$.\n    - The total Lipschitz constant of $\\nabla J$ is estimated as $\\widehat{L}_J = L_g + \\lambda \\widehat{L}_{\\nabla E}$.\n    - The step size is set to $\\eta = \\alpha / \\widehat{L}_J$, where $\\alpha$ is a case-specific scaling factor.\n    - The gradient descent update rule is $x_{k+1} = x_k - \\eta \\nabla J(x_k)$, with $\\nabla J(x_k) = \\frac{1}{\\sigma^2}A^\\top(Ax_k - y) + \\lambda \\widehat{\\nabla} E_\\phi(x_k)$.\n    - The algorithm is run for $T$ iterations starting from a given initial point $x_0$.\n\n6.  **Analysis and Reporting**: During and after the gradient descent process, several quantities are monitored to generate the required output flags for each test case:\n    - **Predicted Monotonicity**: Based on the descent lemma, monotonic decrease of the objective function is guaranteed if the step size $\\eta \\le 1/L$. In our case, this corresponds to $\\alpha/\\widehat{L}_J \\le 1/\\widehat{L}_J$, or simply $\\alpha \\le 1$. The flag is set to $1$ if $\\alpha \\le 1$ and $0$ otherwise.\n    - **Observed Monotonicity**: The sequence of objective function values $\\{J(x_k)\\}_{k=0}^T$ is recorded. The flag is set to $1$ if $J(x_{k+1}) \\le J(x_k) + \\epsilon$ for all $k \\in \\{0, \\dots, T-1\\}$, where $\\epsilon=10^{-12}$ is a small numerical tolerance. Otherwise, it is set to $0$.\n    - **Convergence**: The norm of the gradient, $\\lVert \\nabla J(x_k) \\rVert_2$, is computed at each iterate $x_k$ for $k=1, \\dots, T$. If this norm drops below a threshold of $10^{-3}$ for any of these iterations, the convergence flag is set to $1$; otherwise, it remains $0$.\n\nThe entire procedure is implemented in a Python script, which iterates through the four specified test cases, performs the computations, and formats the six-item result tuple for each case into the required final output string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MAP estimation problem with a neural prior, analyzing convergence\n    properties for different normalization schemes.\n    \"\"\"\n    # ---- Fixed Numerical Specifications ----\n    # Dimensions\n    n_dim, m_dim = 3, 3\n    # Data\n    A = np.array([[1.2, 0.2, -0.3], [0.1, 0.9, 0.0], [-0.4, 0.3, 1.1]])\n    y = np.array([0.5, -0.3, 0.2])\n    sigma = 1.0\n    lambda_reg = 0.5\n    # Network parameters\n    W1 = np.array([[0.8, -0.5, 0.3], [0.2, 0.7, -0.6], [-0.4, 0.1, 0.9]])\n    b1 = np.array([0.1, -0.2, 0.05])\n    w2 = np.array([0.7, -0.3, 0.5])\n    c = 0.0\n    # Batch Normalization parameters\n    bn_params = {\n        'gamma_bn': np.array([0.5, 0.8, 0.6]),\n        'beta_bn': np.array([0.0, 0.0, 0.0]),\n        'mu_bn': np.array([0.0, 0.0, 0.0]),\n        'sigma_sq_bn': np.array([1.0, 1.0, 1.0]),\n        'eps': 1e-5\n    }\n    # Layer Normalization base parameters\n    ln_params_base = {\n        'beta_ln': np.array([0.0, 0.0, 0.0]),\n        'eps': 1e-5\n    }\n    # Finite-difference gradient estimation\n    h_fd = 1e-5\n    # Lipschitz estimation sampling\n    S = 32\n    rng_seed = 0\n    # Gradient descent\n    x0 = np.array([0.2, -0.1, 0.05])\n    T = 80\n    \n    # ---- Helper Functions ----\n\n    def E_phi(x, norm_type, net_params):\n        a = W1 @ x + b1\n        if norm_type == 'none':\n            z = a\n        elif norm_type == 'bn':\n            z = net_params['gamma_bn'] * (a - net_params['mu_bn']) / np.sqrt(net_params['sigma_sq_bn'] + net_params['eps']) + net_params['beta_bn']\n        elif norm_type == 'ln':\n            mu_a = np.mean(a)\n            var_a = np.var(a)\n            z = net_params['gamma_ln'] * (a - mu_a) / np.sqrt(var_a + net_params['eps']) + net_params['beta_ln']\n        else:\n            raise ValueError(f\"Unknown normalization type: {norm_type}\")\n        \n        s = np.tanh(z)\n        return w2 @ s + c\n\n    def grad_E_phi_fd(x, norm_type, net_params):\n        grad = np.zeros_like(x)\n        for i in range(len(x)):\n            e_i = np.zeros_like(x)\n            e_i[i] = 1.0\n            x_plus = x + h_fd * e_i\n            x_minus = x - h_fd * e_i\n            grad[i] = (E_phi(x_plus, norm_type, net_params) - E_phi(x_minus, norm_type, net_params)) / (2 * h_fd)\n        return grad\n\n    def J_obj(x, norm_type, net_params):\n        data_term = 0.5 / (sigma**2) * np.linalg.norm(A @ x - y)**2\n        prior_term = lambda_reg * E_phi(x, norm_type, net_params)\n        return data_term + prior_term\n\n    def grad_J_obj(x, norm_type, net_params):\n        grad_g = (1 / sigma**2) * A.T @ (A @ x - y)\n        grad_E = grad_E_phi_fd(x, norm_type, net_params)\n        return grad_g + lambda_reg * grad_E\n\n    # ---- Main Script ----\n    \n    test_cases = [\n        {'norm_type': 'none', 'alpha': 0.8, 'ln_gamma': None},\n        {'norm_type': 'bn', 'alpha': 1.2, 'ln_gamma': None},\n        {'norm_type': 'ln', 'alpha': 1.0, 'ln_gamma': np.array([1.0, 1.0, 1.0])},\n        {'norm_type': 'ln', 'alpha': 0.8, 'ln_gamma': np.array([2.0, 2.0, 2.0])},\n    ]\n\n    Lg = np.linalg.svd(A, compute_uv=False).max()**2 / (sigma**2)\n    \n    rng = np.random.default_rng(rng_seed)\n    x_samples = rng.uniform(-0.5, 0.5, size=(S, n_dim))\n    \n    all_results = []\n    \n    for case in test_cases:\n        norm_type = case['norm_type']\n        alpha = case['alpha']\n        \n        net_params = bn_params.copy()\n        net_params.update(ln_params_base)\n        if case['ln_gamma'] is not None:\n            net_params['gamma_ln'] = case['ln_gamma']\n\n        # Estimate L_nabla_E\n        grad_E_samples = np.array([grad_E_phi_fd(x, norm_type, net_params) for x in x_samples])\n        \n        max_ratio = 0.0\n        for i in range(S):\n            for j in range(i + 1, S):\n                grad_diff_norm = np.linalg.norm(grad_E_samples[i] - grad_E_samples[j])\n                x_diff_norm = np.linalg.norm(x_samples[i] - x_samples[j])\n                if x_diff_norm > 1e-12:\n                    ratio = grad_diff_norm / x_diff_norm\n                    if ratio > max_ratio:\n                        max_ratio = ratio\n        L_nabla_E_hat = max_ratio\n\n        # Form L_J, eta, and predicted monotonicity\n        L_J_hat = Lg + lambda_reg * L_nabla_E_hat\n        eta = alpha / L_J_hat\n        pred_monotonicity = 1 if alpha = 1.0 else 0\n        \n        # Run Gradient Descent\n        x = x0.copy()\n        J_history = [J_obj(x, norm_type, net_params)]\n        obs_monotonicity = 1\n        conv_flag = 0\n        \n        grad_J = grad_J_obj(x, norm_type, net_params) # Grad at x_0\n\n        for _ in range(T):\n            J_k = J_history[-1]\n            x = x - eta * grad_J # Update to x_{k+1}\n            \n            J_k_plus_1 = J_obj(x, norm_type, net_params)\n            J_history.append(J_k_plus_1)\n            grad_J = grad_J_obj(x, norm_type, net_params) # Grad at x_{k+1}\n\n            if J_k_plus_1 > J_k + 1e-12:\n                obs_monotonicity = 0\n                \n            if not conv_flag and np.linalg.norm(grad_J) = 1e-3:\n                conv_flag = 1\n        \n        case_results = [L_nabla_E_hat, L_J_hat, eta, pred_monotonicity, obs_monotonicity, conv_flag]\n        all_results.append(case_results)\n\n    # ---- Format and Print Output ----\n    formatted_cases = []\n    for res in all_results:\n        # Convert each value to string for joining\n        string_vals = [str(v) for v in res]\n        formatted_cases.append(f\"[{','.join(string_vals)}]\")\n    \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond finding a solution, a crucial aspect of any inversion method is understanding the stability and robustness of the resulting estimate. This practice investigates adversarial vulnerability by analyzing how small, worst-case perturbations in the observed data can affect the final reconstruction. By deriving and comparing the sensitivity of MAP estimates obtained with a modern DNN prior versus a classical Tikhonov prior, you will gain quantitative insight into the different robustness characteristics of these regularization paradigms. ",
            "id": "3375211",
            "problem": "Consider linear inverse problems of the form $y = A x + \\eta$ with $y \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and additive noise $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. By Bayes’ rule, the posterior density satisfies $p(x \\mid y) \\propto p(y \\mid x) \\, p(x)$, where the likelihood $p(y \\mid x)$ is Gaussian. The Maximum A Posteriori (MAP) estimator $x^\\star(y)$ is any minimizer of the negative log-posterior. We study adversarial vulnerability defined by the worst-case change in the MAP estimate under a bounded perturbation $\\delta y$ to the data, that is, $\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\lVert x^\\star(y + \\delta y) - x^\\star(y) \\rVert_2$. Compare this sensitivity for two priors: a classical Tikhonov prior and a Deep Neural Network (DNN) prior modeled as an energy-based prior. The primary goals are to derive, from first principles, tractable expressions for these sensitivities and to compute them numerically for a test suite.\n\nStart from the following foundations: (i) Bayes’ rule $p(x \\mid y) \\propto p(y \\mid x) p(x)$, (ii) Gaussian likelihood $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$, (iii) the MAP estimator $x^\\star(y) \\in \\arg\\min_x \\Phi(x; y)$ where $\\Phi$ is the negative log-posterior, and (iv) the Singular Value Decomposition (SVD) characterization of operator norm for linear maps. Do not invoke any unproven shortcut formulas; derive all expressions you use from these bases together with standard first-order optimality and linearization principles.\n\nDefine the two priors as follows:\n- Tikhonov prior: $p_{\\text{Tik}}(x) \\propto \\exp\\!\\left(-\\frac{\\beta}{2} \\lVert x \\rVert_2^2\\right)$ with parameter $\\beta  0$.\n- Deep Neural Network prior: an energy-based model $p_\\phi(x) \\propto \\exp\\!\\left(- \\alpha \\, E_\\phi(x)\\right)$ with $\\alpha  0$ and energy $E_\\phi(x) = \\frac{1}{2} \\lVert x - g_\\phi(x) \\rVert_2^2$, where $g_\\phi$ is a fixed two-layer feedforward network. The function $g_\\phi$ is constructed as $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$, where $\\tanh(\\cdot)$ is applied elementwise. For any $x$, the Jacobian of $g_\\phi$ is $J_g(x) = W_2 \\, \\operatorname{diag}\\!\\left(1 - \\tanh^2(W_1 x + b_1)\\right) W_1$.\n\nTasks:\n- Using the Tikhonov prior, derive the MAP objective $\\Phi_{\\text{Tik}}(x; y)$ and the corresponding linear mapping from $y$ to $x^\\star(y)$. From the optimization and SVD facts, derive the exact worst-case perturbation $\\delta y$ (subject to $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$) and the corresponding maximal shift in the MAP estimate. Your derivation should start from the first-order optimality condition and proceed to a linear $y \\mapsto x^\\star(y)$ operator whose spectral norm controls the worst-case shift.\n- Using the DNN prior, write the MAP objective $\\Phi_\\phi(x; y)$. Derive the first-order optimality condition and, by applying a local quadratic approximation (Gauss–Newton linearization) at the MAP point, derive a linearized sensitivity mapping from $y$ to $x^\\star(y)$. Use the Implicit Function Theorem (IFT) logic together with the Gauss–Newton Hessian approximation to obtain a tractable linear map whose operator norm upper-bounds the worst-case shift. You must explicitly justify the Gauss–Newton approximation by ignoring second-order derivatives of $g_\\phi$ consistent with the standard Gauss–Newton method. Introduce a small Tikhonov damping $\\gamma = 10^{-6}$ on the Hessian approximation to ensure numerical stability.\n- Implement a program that, for each test case below, computes:\n  1. The maximal shift magnitude under the Tikhonov prior, denoted $S_{\\text{Tik}}$, for the specified $\\varepsilon$.\n  2. The linearized maximal shift magnitude under the DNN prior, denoted $S_{\\text{DNN}}$, for the specified $\\varepsilon$.\n  3. The ratio $R = S_{\\text{DNN}} / S_{\\text{Tik}}$, with the convention that if $S_{\\text{Tik}} = 0$ then $R$ is defined to be $0$.\n  \nAll computations are purely numerical; no physical units are involved. Angles, where applicable within $\\tanh(\\cdot)$, are unitless. The final output must be a single line containing a comma-separated list enclosed in square brackets arranged as $[S_{\\text{Tik}}^{(1)}, S_{\\text{DNN}}^{(1)}, R^{(1)}, S_{\\text{Tik}}^{(2)}, S_{\\text{DNN}}^{(2)}, R^{(2)}, \\dots]$ across all test cases.\n\nNeural network parameters for $g_\\phi$:\n- Dimensions: $n = 5$, hidden width $h = 3$.\n- $W_1 = \\begin{bmatrix}\n0.5  -0.3  0.1  0.0  0.2 \\\\\n0.0  0.4  -0.2  0.3  0.1 \\\\\n-0.1  0.0  0.2  -0.4  0.3\n\\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix}\n0.2  -0.1  0.0 \\\\\n0.0  0.3  -0.2 \\\\\n-0.1  0.0  0.25 \\\\\n0.0  -0.2  0.1 \\\\\n0.15  0.05  -0.05\n\\end{bmatrix}$, $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\\\ -0.03 \\\\ 0.02 \\\\ -0.01 \\end{bmatrix}$.\n\nTest suite:\n- Case $1$: $m = 5$, $n = 5$, \n  $A = \\begin{bmatrix}\n1.0  0.2  0.0  0.0  0.0 \\\\\n0.0  1.0  0.3  0.0  0.0 \\\\\n0.0  0.0  1.0  0.4  0.0 \\\\\n0.0  0.0  0.0  1.0  0.5 \\\\\n0.1  0.0  0.0  0.0  1.0\n\\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 0.5$, $\\alpha = 0.3$, $\\varepsilon = 0.01$.\n- Case $2$: $m = 5$, $n = 5$, \n  $A = \\begin{bmatrix}\n1.0  0.99  0.98  0.0  0.0 \\\\\n0.99  0.98  0.97  0.0  0.0 \\\\\n0.98  0.97  0.96  0.0  0.0 \\\\\n0.0  0.0  0.0  1.0  0.99 \\\\\n0.0  0.0  0.0  0.99  0.98\n\\end{bmatrix}$, $y = \\begin{bmatrix} -0.2 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 0.5$, $\\alpha = 1.0$, $\\varepsilon = 0.01$.\n- Case $3$: $m = 5$, $n = 5$, \n  $A$ is the same as in Case $2$, $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 0.1$, $\\alpha = 0.1$, $\\varepsilon = 0.05$.\n- Case $4$: $m = 5$, $n = 5$, \n  $A$ is the same as in Case $1$, $y = \\begin{bmatrix} -0.2 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}$, $\\sigma = 0.05$, $\\beta = 2.0$, $\\alpha = 5.0$, $\\varepsilon = 0.0$.\n\nImplementation requirements:\n- For the Tikhonov prior, derive and use the exact linear mapping from $y$ to $x^\\star(y)$ stemming from the first-order optimality condition, and compute the induced operator norm to obtain the worst-case shift.\n- For the DNN prior, first compute $x^\\star(y)$ by solving the first-order optimality condition for the MAP estimate using a damped Newton method with the Gauss–Newton Hessian approximation $H(x) \\approx \\frac{1}{\\sigma^2} A^\\top A + \\alpha \\left(I - J_g(x)\\right)^\\top \\left(I - J_g(x)\\right) + \\gamma I$, where $\\gamma = 10^{-6}$. Then compute the linearized sensitivity map from $y$ to $x^\\star(y)$ using the implicit-function linearization at $x^\\star(y)$, and take its operator norm to estimate the worst-case shift; finally multiply by $\\varepsilon$ to get $S_{\\text{DNN}}$.\n- Numerical tolerances: use a stopping criterion based on the Newton step norm being less than $10^{-10}$ or a maximum of $200$ iterations.\n- The final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, ordered as $[S_{\\text{Tik}}^{(1)}, S_{\\text{DNN}}^{(1)}, R^{(1)}, S_{\\text{Tik}}^{(2)}, S_{\\text{DNN}}^{(2)}, R^{(2)}, S_{\\text{Tik}}^{(3)}, S_{\\text{DNN}}^{(3)}, R^{(3)}, S_{\\text{Tik}}^{(4)}, S_{\\text{DNN}}^{(4)}, R^{(4)}]$.",
            "solution": "We begin by restating the fundamental Bayesian formulation. With $y = A x + \\eta$ and $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, the likelihood is $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$. A prior density $p(x)$ yields a posterior $p(x \\mid y) \\propto p(y \\mid x) p(x)$, and the Maximum A Posteriori (MAP) estimator $x^\\star(y)$ solves the optimization problem $x^\\star(y) \\in \\arg\\min_x \\Phi(x; y)$, where $\\Phi(x; y) = - \\log p(y \\mid x) - \\log p(x) + \\text{const}(y)$.\n\nClassical Tikhonov prior. The Tikhonov prior is $p_{\\text{Tik}}(x) \\propto \\exp\\!\\left(-\\frac{\\beta}{2} \\lVert x \\rVert_2^2\\right)$ with $\\beta  0$. The negative log-posterior is\n$$\n\\Phi_{\\text{Tik}}(x; y) = \\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 + \\frac{\\beta}{2} \\lVert x \\rVert_2^2.\n$$\nThe first-order optimality condition is\n$$\n\\nabla_x \\Phi_{\\text{Tik}}(x; y) = \\frac{1}{\\sigma^2} A^\\top (A x - y) + \\beta x = 0.\n$$\nRearranging gives a linear system\n$$\n\\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right) x = \\frac{1}{\\sigma^2} A^\\top y.\n$$\nThus, the MAP estimator is a linear function of the data,\n$$\nx^\\star_{\\text{Tik}}(y) = M_{\\text{Tik}} \\, y, \\quad \\text{where} \\quad M_{\\text{Tik}} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top.\n$$\nFor any perturbation $\\delta y$ with $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$, the induced change in the MAP estimate is\n$$\n\\delta x^\\star_{\\text{Tik}} = M_{\\text{Tik}} \\, \\delta y,\n$$\nso the worst-case shift magnitude is\n$$\n\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\lVert M_{\\text{Tik}} \\, \\delta y \\rVert_2 = \\varepsilon \\, \\lVert M_{\\text{Tik}} \\rVert_2,\n$$\nwhere $\\lVert \\cdot \\rVert_2$ is the operator norm (largest singular value). The worst-case direction for $\\delta y$ is the right singular vector of $M_{\\text{Tik}}$ associated with its largest singular value. This follows from the Singular Value Decomposition (SVD) characterization of the spectral norm.\n\nDeep Neural Network prior. Consider an energy-based prior with density $p_\\phi(x) \\propto \\exp\\!\\left(- \\alpha E_\\phi(x) \\right)$, $\\alpha  0$, with energy\n$$\nE_\\phi(x) = \\frac{1}{2} \\lVert x - g_\\phi(x) \\rVert_2^2,\n$$\nwhere $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$. The negative log-posterior is\n$$\n\\Phi_\\phi(x; y) = \\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 + \\alpha \\, E_\\phi(x).\n$$\nThe gradient is\n$$\n\\nabla_x \\Phi_\\phi(x; y) = \\frac{1}{\\sigma^2} A^\\top (A x - y) + \\alpha \\, \\nabla_x E_\\phi(x).\n$$\nLet $r(x) = x - g_\\phi(x)$. Then $E_\\phi(x) = \\frac{1}{2} \\lVert r(x) \\rVert_2^2$, and\n$$\n\\nabla_x E_\\phi(x) = J_r(x)^\\top r(x) = \\left(I - J_g(x)\\right)^\\top \\left(x - g_\\phi(x)\\right),\n$$\nwhere $J_g(x)$ is the Jacobian of $g_\\phi$ at $x$ and $J_r(x) = I - J_g(x)$. The exact Hessian of $E_\\phi$ is\n$$\n\\nabla_x^2 E_\\phi(x) = J_r(x)^\\top J_r(x) + \\sum_{i=1}^n r_i(x) \\, \\nabla_x^2 r_i(x),\n$$\nbut the Gauss–Newton (GN) approximation neglects the second term, yielding\n$$\n\\nabla_x^2 E_\\phi(x) \\approx J_r(x)^\\top J_r(x) = \\left(I - J_g(x)\\right)^\\top \\left(I - J_g(x)\\right).\n$$\nThis approximation is standard when residuals are moderate and provides a positive semi-definite curvature model. At a MAP point $x^\\star_\\phi(y)$ satisfying $\\nabla_x \\Phi_\\phi(x^\\star_\\phi(y); y) = 0$, the Gauss–Newton Hessian approximation of the negative log-posterior is\n$$\nH(x^\\star_\\phi(y)) \\approx \\frac{1}{\\sigma^2} A^\\top A + \\alpha \\left(I - J_g(x^\\star_\\phi(y))\\right)^\\top \\left(I - J_g(x^\\star_\\phi(y))\\right).\n$$\nTo ensure numerical stability, we add Tikhonov damping $\\gamma I$ with $\\gamma = 10^{-6}$:\n$$\n\\widetilde{H}(x^\\star_\\phi(y)) = H(x^\\star_\\phi(y)) + \\gamma I.\n$$\nTo obtain the sensitivity of $x^\\star_\\phi(y)$ to $y$, differentiate the first-order optimality condition with respect to $y$ using the Implicit Function Theorem (IFT):\n$$\n\\nabla_x^2 \\Phi_\\phi(x^\\star_\\phi(y); y) \\, \\frac{\\partial x^\\star_\\phi}{\\partial y}(y) + \\frac{\\partial}{\\partial y} \\nabla_x \\Phi_\\phi(x^\\star_\\phi(y); y) = 0.\n$$\nSince $\\frac{\\partial}{\\partial y} \\left( \\frac{1}{\\sigma^2} A^\\top (A x^\\star_\\phi(y) - y) \\right) = - \\frac{1}{\\sigma^2} A^\\top$ and the prior term depends only on $x$, the linearized sensitivity mapping is\n$$\n\\frac{\\partial x^\\star_\\phi}{\\partial y}(y) \\approx \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top.\n$$\nTherefore, for small $\\delta y$ with $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$, the worst-case linearized shift is\n$$\n\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\left\\lVert \\frac{\\partial x^\\star_\\phi}{\\partial y}(y) \\, \\delta y \\right\\rVert_2 \\approx \\varepsilon \\left\\lVert \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top \\right\\rVert_2.\n$$\nThis provides a principled, tractable mechanism to assess adversarial vulnerability under a deep neural network prior using Gauss–Newton linearization and the Implicit Function Theorem.\n\nAlgorithmic realization. For the Tikhonov prior, compute\n$$\nM_{\\text{Tik}} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top,\n$$\nand set $S_{\\text{Tik}} = \\varepsilon \\, \\lVert M_{\\text{Tik}} \\rVert_2$. For the deep neural network prior, first compute $x^\\star_\\phi(y)$ by solving $\\nabla_x \\Phi_\\phi(x; y) = 0$ using a damped Newton method with the Gauss–Newton Hessian $\\widetilde{H}(x)$. Specifically, iterate\n$$\n\\widetilde{H}(x_k) \\, \\Delta x_k = - \\nabla_x \\Phi_\\phi(x_k; y), \\quad x_{k+1} = x_k + \\Delta x_k,\n$$\nuntil $\\lVert \\Delta x_k \\rVert_2  10^{-10}$ or a maximum of $200$ iterations is reached. At the converged $x^\\star_\\phi(y)$, form\n$$\nM_{\\text{DNN}}(y) \\approx \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top,\n$$\nand set $S_{\\text{DNN}} = \\varepsilon \\, \\lVert M_{\\text{DNN}}(y) \\rVert_2$. Finally, define $R = S_{\\text{DNN}} / S_{\\text{Tik}}$; if $S_{\\text{Tik}} = 0$, return $R = 0$ by convention to avoid undefined values.\n\nTest suite specification. The matrices $A$, vectors $y$, scalars $\\sigma$, $\\beta$, $\\alpha$, and $\\varepsilon$ are given numerically in the problem statement. The deep neural network prior is specified by the numeric matrices $W_1$, $W_2$ and vectors $b_1$, $b_2$, with $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$ and $J_g(x) = W_2 \\, \\operatorname{diag}(1 - \\tanh^2(W_1 x + b_1)) W_1$. For each case, we compute $S_{\\text{Tik}}$, $S_{\\text{DNN}}$, and $R$, and report the results as a single line list ordered across cases as specified. Since Case $4$ has $\\varepsilon = 0$, both $S_{\\text{Tik}}$ and $S_{\\text{DNN}}$ are $0$ and thus $R = 0$ by the defined convention.\n\nThis solution integrates statistical inverse problem principles, optimization optimality, Gauss–Newton approximation, and the Implicit Function Theorem to derive sensitivity operators whose spectral norms govern worst-case adversarial vulnerability for both classical and deep neural network priors, followed by consistent numerical realization.",
            "answer": "```python\nimport numpy as np\n\n# Deep Neural Network prior specification: g_phi(x) = W2 * tanh(W1 x + b1) + b2\ndef g_phi(x, W1, b1, W2, b2):\n    z1 = W1 @ x + b1\n    h = np.tanh(z1)\n    return W2 @ h + b2\n\ndef J_g_phi(x, W1, b1, W2):\n    z1 = W1 @ x + b1\n    dh = 1.0 - np.tanh(z1) ** 2  # elementwise derivative of tanh\n    # J_g = W2 * diag(dh) * W1\n    return W2 @ (np.diag(dh) @ W1)\n\ndef map_tikhonov_operator(A, sigma, beta):\n    # M_Tik = (A^T A / sigma^2 + beta I)^(-1) * (A^T / sigma^2)\n    n = A.shape[1]\n    H = (A.T @ A) / (sigma**2) + beta * np.eye(n)\n    M = np.linalg.solve(H, A.T / (sigma**2))\n    return M\n\ndef newton_map_dnn_xstar(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6, tol=1e-10, maxit=200):\n    n = A.shape[1]\n    x = np.zeros(n)\n    AT = A.T\n    ATA_over_sigma2 = (AT @ A) / (sigma**2)\n    for _ in range(maxit):\n        # Compute g, Jg\n        g = g_phi(x, W1, b1, W2, b2)\n        Jg = J_g_phi(x, W1, b1, W2)\n        # Residual r = x - g(x)\n        r = x - g\n        # Gradient: A^T(Ax - y)/sigma^2 + alpha * (I - Jg)^T * r\n        data_grad = AT @ (A @ x - y) / (sigma**2)\n        I_minus_Jg = np.eye(n) - Jg\n        prior_grad = alpha * (I_minus_Jg.T @ r)\n        grad = data_grad + prior_grad\n        # Gauss-Newton Hessian approx: A^T A / sigma^2 + alpha * (I - Jg)^T (I - Jg) + gamma I\n        H_gn = ATA_over_sigma2 + alpha * (I_minus_Jg.T @ I_minus_Jg) + gamma * np.eye(n)\n        try:\n            dx = np.linalg.solve(H_gn, -grad)\n        except np.linalg.LinAlgError:\n            # Fallback to least-squares solve if ill-conditioned\n            dx = np.linalg.lstsq(H_gn, -grad, rcond=None)[0]\n        x = x + dx\n        if np.linalg.norm(dx)  tol:\n            break\n    return x\n\ndef dnn_sensitivity_operator(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6):\n    # Compute x* via Newton-GN\n    x_star = newton_map_dnn_xstar(A, y, sigma, alpha, W1, b1, W2, b2, gamma=gamma)\n    # Build GN Hessian at x*\n    n = A.shape[1]\n    AT = A.T\n    ATA_over_sigma2 = (AT @ A) / (sigma**2)\n    Jg = J_g_phi(x_star, W1, b1, W2)\n    I_minus_Jg = np.eye(n) - Jg\n    H_gn = ATA_over_sigma2 + alpha * (I_minus_Jg.T @ I_minus_Jg) + gamma * np.eye(n)\n    # Sensitivity M_DNN = H_gn^{-1} * (A^T / sigma^2)\n    try:\n        M = np.linalg.solve(H_gn, AT / (sigma**2))\n    except np.linalg.LinAlgError:\n        M = np.linalg.lstsq(H_gn, AT / (sigma**2), rcond=None)[0]\n    return M\n\ndef spectral_norm(M):\n    # Largest singular value\n    s = np.linalg.svd(M, compute_uv=False)\n    return float(s[0]) if len(s) > 0 else 0.0\n\ndef solve():\n    # Network parameters\n    W1 = np.array([\n        [0.5, -0.3, 0.1, 0.0, 0.2],\n        [0.0,  0.4, -0.2, 0.3, 0.1],\n        [-0.1, 0.0, 0.2, -0.4, 0.3]\n    ], dtype=float)\n    b1 = np.array([0.1, -0.2, 0.05], dtype=float)\n    W2 = np.array([\n        [0.2, -0.1,  0.0],\n        [0.0,  0.3, -0.2],\n        [-0.1, 0.0,  0.25],\n        [0.0, -0.2,  0.1],\n        [0.15, 0.05, -0.05]\n    ], dtype=float)\n    b2 = np.array([0.0, 0.05, -0.03, 0.02, -0.01], dtype=float)\n\n    # Test cases\n    A1 = np.array([\n        [1.0, 0.2, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.3, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.4, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.5],\n        [0.1, 0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n    A2 = np.array([\n        [1.0, 0.99, 0.98, 0.0, 0.0],\n        [0.99, 0.98, 0.97, 0.0, 0.0],\n        [0.98, 0.97, 0.96, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.99],\n        [0.0, 0.0, 0.0, 0.99, 0.98]\n    ], dtype=float)\n    y1 = np.array([1.0, -0.5, 0.3, 0.2, -0.1], dtype=float)\n    y2 = np.array([-0.2, 0.1, -0.1, 0.05, -0.05], dtype=float)\n    sigma = 0.05\n\n    test_cases = [\n        # (A, y, beta, alpha, epsilon)\n        (A1, y1, 0.5, 0.3, 0.01),\n        (A2, y2, 0.5, 1.0, 0.01),\n        (A2, y1, 0.1, 0.1, 0.05),\n        (A1, y2, 2.0, 5.0, 0.0),\n    ]\n\n    results = []\n    for A, y, beta, alpha, eps in test_cases:\n        # Tikhonov operator and sensitivity\n        M_tik = map_tikhonov_operator(A, sigma, beta)\n        s_tik = eps * spectral_norm(M_tik)\n\n        # DNN linearized sensitivity at MAP (GN-IFT)\n        M_dnn = dnn_sensitivity_operator(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6)\n        s_dnn = eps * spectral_norm(M_dnn)\n\n        # Ratio with convention: if s_tik == 0, return 0\n        R = 0.0 if np.isclose(s_tik, 0.0) else (s_dnn / s_tik)\n\n        results.extend([s_tik, s_dnn, R])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}