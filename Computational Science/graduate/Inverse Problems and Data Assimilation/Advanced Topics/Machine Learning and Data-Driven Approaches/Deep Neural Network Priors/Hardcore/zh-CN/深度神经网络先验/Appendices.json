{
    "hands_on_practices": [
        {
            "introduction": "深度神经网络先验的一个引人注目的特性是，即使未经训练，网络结构本身也能作为一种有效的正则化器。这个练习旨在揭示这种现象背后的“频谱偏置”原理。通过分析一个理想化的深度图像先验（DIP）模型，你将推导出不同频率分量在优化过程中的收敛速度，从而亲手验证为何深度生成器天然地偏好于学习“更简单”的低频信息。",
            "id": "3375205",
            "problem": "考虑深度图像先验 (DIP) 重建框架，其中未知图像被参数化为 $x = G_{\\theta}(z)$，其中 $z$ 是一个固定的输入，$\\theta$ 是可训练的参数，该图像通过使用梯度下降法最小化关于 $\\theta$ 的平方数据失配 $\\|A G_{\\theta}(z) - y\\|_2^2$ 来估计。假设生成器 $G_{\\theta}$ 是一个无限宽、平移等变的卷积网络，其在初始化时具有独立同分布的零均值滤波器，因此在无限宽度和小学习率的极限下，函数空间中 $f_{t} = G_{\\theta(t)}(z)$ 的梯度流可以很好地近似为\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|_2^2 \\big|_{f = f_{t}},\n$$\n其中 $\\gamma  0$ 是学习率缩放因子，$K$ 是 $G_{\\theta}$ 在初始化时的神经正切核 (NTK) 算子。设 $A$ 是图像上的线性、移位不变正向算子（例如，模糊），并用 $A^{\\top}$ 表示其伴随算子。根据最小二乘梯度的标准性质，$\\nabla_{f} \\|A f - y\\|_2^2 = 2 A^{\\top} (A f - y)$。假设 $K$ 是具有各向同性高斯傅里叶乘子的移位不变算子，而 $A$ 是各向同性高斯模糊，具体为\n$$\n\\widehat{K}(\\omega) = \\exp\\!\\big(- \\sigma^{2} \\|\\omega\\|^{2}\\big), \\quad \\widehat{A}(\\omega) = \\exp\\!\\big(- \\beta \\|\\omega\\|^{2}\\big),\n$$\n对于参数 $\\sigma^{2}  0$ 和 $\\beta  0$，其中 $\\omega \\in \\mathbb{R}^{2}$ 表示空间频率。在这些假设下，残差 $\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$ 的傅里叶模式独立演化，其中 $f_{\\infty}$ 是梯度流的不动点。\n\n从这些前提出发，仅使用线性算子、伴随算子和移位不变算子的傅里叶对角化性质，推导控制 $\\widehat{r}_{t}(\\omega)$ 演化的指数衰减率 $\\lambda(\\omega)$ 的表达式，该演化由以下常微分方程定义\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega).\n$$\n然后，对于 $\\sigma^{2} = 10^{-3}$，$\\beta = 5 \\times 10^{-4}$，$\\gamma = 1$，以及两个各向同性频率大小 $\\|\\omega_{l}\\| = 20$ 和 $\\|\\omega_{h}\\| = 60$，计算比率 $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$。将最终数值答案四舍五入到四位有效数字。最终答案必须是一个无单位的实数。",
            "solution": "首先验证用户提供问题的正确性和可解性。\n\n### 第 1 步：提取已知条件\n- 图像重建框架是深度图像先验 (DIP)：$x = G_{\\theta}(z)$。\n- 目标函数是平方数据失配：$\\|A G_{\\theta}(z) - y\\|^{2}$。\n- 优化方法是关于参数 $\\theta$ 的梯度下降。\n- 无限宽度、小学习率的 $f_{t} = G_{\\theta(t)}(z)$ 梯度流为：\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\n- 学习率缩放因子为 $\\gamma  0$。\n- 算子 $K$ 是 $G_{\\theta}$ 在初始化时的神经正切核 (NTK)。\n- 最小二乘损失的梯度为：$\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$。\n- $A$ 是一个线性、移位不变正向算子，$A^{\\top}$ 是其伴随算子。\n- $K$ 是一个移位不变算子。\n- $K$ 的傅里叶乘子为：$\\widehat{K}(\\omega) = \\exp(-\\sigma^{2} \\|\\omega\\|^{2})$，其中 $\\sigma^2  0$。\n- $A$ 的傅里叶乘子为：$\\widehat{A}(\\omega) = \\exp(-\\beta \\|\\omega\\|^{2})$，其中 $\\beta  0$。\n- 傅里叶域中的残差定义为：$\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$，其中 $f_{\\infty}$ 是流的不动点。\n- 残差的演化形式为：$\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$。\n- 参数的数值为：$\\sigma^{2} = 10^{-3}$，$\\beta = 5 \\times 10^{-4}$，$\\gamma = 1$。\n- 给定两个各向同性频率大小：$\\|\\omega_{l}\\| = 20$ 和 $\\|\\omega_{h}\\| = 60$。\n- 任务是推导衰减率 $\\lambda(\\omega)$ 的表达式，并计算比率 $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学依据**：该问题在无限宽度神经网络理论，特别是神经正切核 (NTK) 模型中有坚实的理论基础。通过深度图像先验 (DIP) 将其应用于逆问题是一个当前且有效的研究领域。所有算子和概念（梯度流、移位不变算子的傅里叶分析、伴随算子）在应用数学和信号处理中都是标准的。\n- **适定性**：问题是适定的。它提供了推导所需量 $\\lambda(\\omega)$ 和计算最终比率所必需的所有方程、定义和常数。\n- **客观性**：问题以精确、客观的数学语言陈述。\n- **结论**：该问题被认为是有效的，因为它是自洽的、科学合理的且没有歧义。\n\n### 第 3 步：进行求解\n\n我们从函数 $f_t$ 的给定梯度流方程开始：\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\n代入梯度的给定表达式 $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$，我们得到：\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, [2 A^{\\top} (A f_{t} - y)] = -2 \\gamma \\, K A^{\\top} (A f_{t} - y)\n$$\n在不动点 $f_{\\infty}$ 处，时间导数为零，即 $\\partial_{t} f_{\\infty} = 0$。这意味着 $f_{\\infty}$ 满足：\n$$\n-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y) = 0\n$$\n现在我们分析残差 $r_{t} = f_{t} - f_{\\infty}$ 的演化。残差的时间导数是 $\\partial_{t} r_{t} = \\partial_{t} (f_{t} - f_{\\infty}) = \\partial_{t} f_{t}$。我们可以从 $f_t$ 的流方程中减去不动点方程（乘以 $1$）：\n$$\n\\partial_{t} f_{t} - 0 = -2 \\gamma \\, K A^{\\top} (A f_{t} - y) - [-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y)]\n$$\n$$\n\\partial_{t} (f_{t} - f_{\\infty}) = -2 \\gamma \\, K A^{\\top} [ (A f_{t} - y) - (A f_{\\infty} - y) ]\n$$\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} (A f_{t} - A f_{\\infty})\n$$\n由于 $A$ 是一个线性算子，$A f_{t} - A f_{\\infty} = A(f_{t} - f_{\\infty}) = A r_{t}$。残差的方程变为：\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} A \\, r_{t}\n$$\n这是一个函数空间中的线性常微分方程。由于算子 $K$、$A^{\\top}$ 和 $A$ 都是移位不变的，它们可以被傅里叶变换对角化。我们可以通过将此方程变换到傅里叶域来求解。设 $\\widehat{r}_{t}(\\omega)$ 是 $r_{t}$ 的傅里叶变换。将一个移位不变算子 $L$ 应用于一个函数 $g$ 的傅里叶变换是它们傅里叶表示的乘积，即 $\\mathcal{F}\\{Lg\\}(\\omega) = \\widehat{L}(\\omega) \\widehat{g}(\\omega)$。将此应用于我们的方程：\n$$\n\\mathcal{F}\\{\\partial_{t} r_{t}\\}(\\omega) = \\mathcal{F}\\{-2 \\gamma \\, K A^{\\top} A \\, r_{t}\\}(\\omega)\n$$\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\widehat{(K A^{\\top} A)}(\\omega) \\, \\widehat{r}_{t}(\\omega)\n$$\n复合算子 $K A^{\\top} A$ 的傅里叶乘子是各个傅里叶乘子的乘积：\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) \\, \\widehat{A^{\\top}}(\\omega) \\, \\widehat{A}(\\omega)\n$$\n对于线性算子 $A$，其伴随算子 $A^{\\top}$ 的乘子是其自身乘子的复共轭，即 $\\widehat{A^{\\top}}(\\omega) = (\\widehat{A}(\\omega))^{*}$。给定的 $A$ 的乘子是 $\\widehat{A}(\\omega) = \\exp(-\\beta \\|\\omega\\|^{2})$。因为 $\\beta$ 和 $\\|\\omega\\|^2$ 都是实数，所以 $\\widehat{A}(\\omega)$ 是一个实值函数。因此，$(\\widehat{A}(\\omega))^{*} = \\widehat{A}(\\omega)$，所以 $\\widehat{A^{\\top}}(\\omega) = \\widehat{A}(\\omega)$。\n复合乘子则为：\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) (\\widehat{A}(\\omega))^{2}\n$$\n代入给定的 $\\widehat{K}(\\omega)$ 和 $\\widehat{A}(\\omega)$ 的形式：\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\left( \\exp(-\\beta \\|\\omega\\|^{2}) \\right)^{2} = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\exp(-2\\beta \\|\\omega\\|^{2})\n$$\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\n将此代回 $\\widehat{r}_{t}(\\omega)$ 的微分方程：\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2}) \\, \\widehat{r}_{t}(\\omega)\n$$\n我们已知这个方程的形式是 $\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$。通过直接比较，我们确定衰减率 $\\lambda(\\omega)$ 为：\n$$\n\\lambda(\\omega) = 2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\n现在，我们必须计算给定频率大小 $\\|\\omega_{h}\\| = 60$ 和 $\\|\\omega_{l}\\| = 20$ 的比率 $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$。\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\frac{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2})}{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2})}\n$$\n项 $2\\gamma$ 被消去：\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2} + (\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2}) = \\exp(-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}))\n$$\n我们代入给定的数值：\n$\\sigma^{2} = 10^{-3}$\n$\\beta = 5 \\times 10^{-4}$\n$\\|\\omega_{l}\\| = 20 \\implies \\|\\omega_{l}\\|^{2} = 400$\n$\\|\\omega_{h}\\| = 60 \\implies \\|\\omega_{h}\\|^{2} = 3600$\n\n首先，我们计算指数中的系数：\n$$\n\\sigma^{2} + 2\\beta = 10^{-3} + 2(5 \\times 10^{-4}) = 10^{-3} + 10 \\times 10^{-4} = 10^{-3} + 10^{-3} = 2 \\times 10^{-3}\n$$\n接下来，我们计算频率大小平方的差值：\n$$\n\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2} = 3600 - 400 = 3200\n$$\n现在，我们计算指数：\n$$\n-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}) = -(2 \\times 10^{-3}) \\times 3200 = -6.4\n$$\n最后，我们计算该比率：\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-6.4) \\approx 0.0016615555...\n$$\n将此结果四舍五入到四位有效数字，得到 $0.001662$。用科学记数法表示，这是 $1.662 \\times 10^{-3}$。",
            "answer": "$$\\boxed{1.662 \\times 10^{-3}}$$"
        },
        {
            "introduction": "虽然深度神经网络先验表达能力强大，但它们通常会引入高度非凸的能量函数，使得寻找全局最优解成为一个巨大挑战。本练习将引导你实现一种名为“退火”的优化策略来应对这一难题。你将通过在一个高“温度”（即更平滑）的先验下开始优化，并逐步“降温”到目标先验，从而体验如何有效避开不良局部最小值，找到更优的解。",
            "id": "3375150",
            "problem": "考虑一个确定性线性逆问题，其中未知状态向量 $x \\in \\mathbb{R}^n$ 通过线性正演算子 $A \\in \\mathbb{R}^{m \\times n}$ 进行观测，并伴有加性噪声，从而产生数据 $y \\in \\mathbb{R}^m$。最大后验 (MAP) 估计旨在最小化一个由深度神经网络 (DNN) 定义的先验下的负对数后验概率。设数据失配项为 $E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2$，先验能量为 $E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2$，其中 $g_{\\phi,\\tau}$ 是一个依赖于温度参数 $\\tau  0$ 的 DNN 映射。需要最小化的总能量为\n$$\nE_{\\text{total}}(x;\\lambda,\\tau) = E_{\\text{data}}(x) + \\lambda\\,E_{\\phi,\\tau}(x),\n$$\n其中 $\\lambda  0$ 用于权衡先验相对于数据的权重。\n\n为了探索温度退火，我们使用一个基于正弦表示网络 (Sinusoidal Representation Network, SIREN) 的 DNN 先验，该网络带有一个缩放的正弦激活函数，通过 $\\tau$ 控制平滑度。具体而言，定义 $g_{\\phi,\\tau}:\\mathbb{R}^n\\to\\mathbb{R}^n$ 为\n$$\ng_{\\phi,\\tau}(x) = \\tau \\,\\sin\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right),\n$$\n其中 $W_1 \\in \\mathbb{R}^{n \\times n}$ 且 $b_1 \\in \\mathbb{R}^n$。正弦函数是逐分量作用的。这种选择产生的先验能量曲面，其非凸性随着 $\\tau$ 的减小而增加：当 $\\tau$ 较大时，激活函数近似线性（对应凸二次先验）；而当 $\\tau$ 较小时，激活函数变得高度振荡，引入许多局部最小值。\n\n从贝叶斯公式和 MAP 的定义出发，推导出 $E_{\\text{total}}$ 的梯度由数据梯度和网络雅可比矩阵引出的先验梯度之和给出。为 MAP 实现一个梯度下降求解器，并比较两种策略：\n- 冷启动策略：在最低温度 $\\tau_{\\text{final}}$ 下直接最小化 $E_{\\text{total}}(x;\\lambda,\\tau_{\\text{final}})$。\n- 退火优化策略：从一个高温开始，通过一个调度表逐步降低 $\\tau$，在每个阶段使用前一阶段的最小化结果作为初始化来最小化 $E_{\\text{total}}(x;\\lambda,\\tau)$。\n\n使用以下 $n = m = 2$ 的精确且自洽的设置：\n- 正演算子 $A = I_2$，即 $2 \\times 2$ 单位矩阵。\n- 真实状态 $x^\\star = [2,-2]^\\top$。\n- 观测值 $y = x^\\star + \\eta$，其中固定噪声 $\\eta = [0.05,0.05]^\\top$。\n- 先验网络权重 $W_1 = I_2$。\n- 最终温度 $\\tau_{\\text{final}} = 0.2$。\n- 偏置 $b_1$ 的选择使得对于一个选定的整数向量 $k \\in \\mathbb{Z}^2$，$x^\\star$ 恰好是 $g_{\\phi,\\tau_{\\text{final}}}$ 的一个零点，即\n$$\nb_1 = k\\,\\pi\\,\\tau_{\\text{final}} - W_1 x^\\star,\n$$\n其中 $k = [3,-3]^\\top$。\n- 冷启动和退火优化的初始化均为 $x_0 = [0,0]^\\top$。\n- 梯度下降步长 $\\alpha = 0.1$。\n- 每个温度阶段的迭代次数 $S = 300$。\n\n对于上面定义的 SIREN 先验，先验能量的梯度为\n$$\n\\nabla E_{\\phi,\\tau}(x) = J_{g_{\\phi,\\tau}}(x)^\\top\\,g_{\\phi,\\tau}(x),\n$$\n其中 $g_{\\phi,\\tau}$ 的雅可比矩阵是\n$$\nJ_{g_{\\phi,\\tau}}(x) = \\cos\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\,W_1,\n$$\n余弦函数是逐分量作用的。因此，用于梯度下降的总梯度为\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (A^\\top (A x - y)) + \\lambda\\,\\Big[\\cos\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\,W_1\\Big]^\\top \\Big[\\tau \\,\\sin\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\Big].\n$$\n\n通过在一个温度调度表 $\\{\\tau_1,\\tau_2,\\dots,\\tau_T\\}$ 上迭代梯度下降来构建退火优化，将 $\\tau_t$ 处的最终迭代结果用作 $\\tau_{t+1}$ 的初始化。冷启动是在 $\\tau_{\\text{final}}$ 下的单个阶段，其总迭代次数与退火运行相同。\n\n测试套件：\n- 案例 1：$\\lambda = 5.0$，温度调度表 $[5.0,1.0,0.5,0.2]$。\n- 案例 2：$\\lambda = 12.0$，温度调度表 $[5.0,1.0,0.5,0.2]$。\n- 案例 3：$\\lambda = 5.0$，温度调度表 $[0.2,0.2,0.2,0.2]$（无退火）。\n\n对于每个案例，计算欧几里得距离 $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$ 和 $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$，并报告改进量 $I = D_{\\text{cold}} - D_{\\text{anneal}}$（一个浮点数）。正值的 $I$ 表示退火有助于逃离较差的局部最小值，从而更接近真实值。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个逗号分隔的列表，用方括号括起来，按测试套件案例的顺序列出结果，每个改进值四舍五入到六位小数（例如 $[i_1,i_2,i_3]$）。不涉及物理单位。根据构造，三角函数内的所有角度都以弧度为单位。程序必须是自包含的，且不需要任何输入。",
            "solution": "在进行求解之前，对问题进行验证。\n\n### 第 1 步：提取已知条件\n- **问题类型**：未知状态向量 $x \\in \\mathbb{R}^n$ 的确定性线性逆问题。\n- **正演模型**：$y = A x + \\eta$，其中 $y \\in \\mathbb{R}^m$ 是观测值，$A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$\\eta$ 是加性噪声。\n- **目标函数（总能量）**：$E_{\\text{total}}(x;\\lambda,\\tau) = E_{\\text{data}}(x) + \\lambda\\,E_{\\phi,\\tau}(x)$，其中 $\\lambda  0$。\n- **数据失配项**：$E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2$。\n- **先验能量项**：$E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2$。\n- **DNN 先验函数**：$g_{\\phi,\\tau}(x) = \\tau \\,\\sin\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right)$，其中 $g_{\\phi,\\tau}:\\mathbb{R}^n\\to\\mathbb{R}^n$，$W_1 \\in \\mathbb{R}^{n \\times n}$，$b_1 \\in \\mathbb{R}^n$，$\\tau  0$ 是一个温度参数，$\\sin$ 是逐分量应用的。\n- **总梯度**：$\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (A^\\top (A x - y)) + \\lambda\\,\\Big[\\cos\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\,W_1\\Big]^\\top \\Big[\\tau \\,\\sin\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\Big]$。\n- **数值参数**：\n    - $n = m = 2$。\n    - $A = I_2$（$2 \\times 2$ 单位矩阵）。\n    - 真实状态 $x^\\star = [2,-2]^\\top$。\n    - 固定噪声 $\\eta = [0.05,0.05]^\\top$。\n    - 观测值 $y = x^\\star + \\eta$。\n    - 先验权重 $W_1 = I_2$。\n    - 最终温度 $\\tau_{\\text{final}} = 0.2$。\n    - 偏置向量 $b_1 = k\\,\\pi\\,\\tau_{\\text{final}} - W_1 x^\\star$，其中整数向量 $k = [3,-3]^\\top$。\n    - 初始化 $x_0 = [0,0]^\\top$。\n    - 梯度下降步长 $\\alpha = 0.1$。\n    - 每个温度阶段的迭代次数 $S = 300$。\n- **测试案例**：\n    - 案例 1：$\\lambda = 5.0$，温度调度表 $[5.0,1.0,0.5,0.2]$。\n    - 案例 2：$\\lambda = 12.0$，温度调度表 $[5.0,1.0,0.5,0.2]$。\n    - 案例 3：$\\lambda = 5.0$，温度调度表 $[0.2,0.2,0.2,0.2]$。\n- **任务**：对于每个案例，计算改进量 $I = D_{\\text{cold}} - D_{\\text{anneal}}$，其中 $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$ 且 $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题是计算科学中一个定义明确的数值实验，专门探索用于由深度先验正则化的逆问题的优化策略。\n- **科学依据**：该框架植根于贝叶斯推断（最大后验估计）和标准优化技术（梯度下降）。使用类 SIREN 网络作为先验是该领域的一种现代方法。所有数学公式均与既定理论一致。\n- **适定性**：该问题为一项计算任务提供了所有必要的参数和明确的目标。数值程序（梯度下降）保证了解的存在性，并且所要求的输出是每个案例的唯一定义的标量值。\n- **客观性**：所有术语都以数学精度定义。没有主观或模糊的陈述。\n- **完整性与一致性**：问题是自包含的。提供的参数足以实现所需的算法。偏置 $b_1$ 的公式与在 $\\tau_{\\text{final}}$ 下使真实值 $x^\\star$ 成为先验能量景观的一个零点的目标相一致，这是构建现实测试案例的常用技术。梯度的推导与能量项的定义一致。\n\n### 第 3 步：结论与行动\n此问题是**有效的**。它是一个合理且适定的计算问题。将提供一个解决方案。\n\n### 解法推导与方法\n问题要求找到 $x$ 的最大后验 (MAP) 估计，即最小化总能量 $E_{\\text{total}}(x;\\lambda,\\tau)$。我们将使用梯度下降法，这是一种迭代优化算法，其中估计值 $x_k$在每一步 $k$ 按如下方式更新：\n$$\nx_{k+1} = x_k - \\alpha \\nabla E_{\\text{total}}(x_k;\\lambda,\\tau)\n$$\n其中 $\\alpha$ 是步长。该方法的核心是计算梯度 $\\nabla E_{\\text{total}}$。\n\n**1. 梯度推导**\n总能量为 $E_{\\text{total}} = E_{\\text{data}} + \\lambda E_{\\text{prior}}$。其梯度是各部分梯度之和。\n\n- **数据失配项 $E_{\\text{data}}(x)$ 的梯度**：\n数据失配项为 $E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2 = \\frac{1}{2}(A x - y)^\\top(A x - y)$。\n关于 $x$ 的梯度是矩阵微积分中的一个标准结果：\n$$\n\\nabla_x E_{\\text{data}}(x) = A^\\top(A x - y).\n$$\n对于 $A=I_2$ 的特定情况，这简化为 $\\nabla_x E_{\\text{data}}(x) = x - y$。\n\n- **先验能量 $E_{\\phi,\\tau}(x)$ 的梯度**：\n先验能量为 $E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2 = \\frac{1}{2}g_{\\phi,\\tau}(x)^\\top g_{\\phi,\\tau}(x)$。\n使用向量链式法则，其梯度为：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = J_{g_{\\phi,\\tau}}(x)^\\top g_{\\phi,\\tau}(x),\n$$\n其中 $J_{g_{\\phi,\\tau}}(x)$ 是 $g_{\\phi,\\tau}(x)$ 的雅可比矩阵。\n\n为求雅可比矩阵，令 $v(x) = \\frac{W_1 x + b_1}{\\tau}$。映射为 $g_{\\phi,\\tau}(x) = \\tau \\sin(v(x))$。$v(x)$ 关于 $x$ 的雅可比矩阵是 $J_v(x) = \\frac{1}{\\tau} W_1$。$\\tau \\sin(v)$ 关于 $v$ 的雅可比矩阵是一个对角矩阵，$J_{\\sin}(v) = \\text{diag}(\\tau\\cos(v_1), \\dots, \\tau\\cos(v_n))$。根据链式法则：\n$$\nJ_{g_{\\phi,\\tau}}(x) = J_{\\sin}(v(x)) \\cdot J_v(x) = \\text{diag}\\left(\\tau \\cos\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\right) \\left(\\frac{1}{\\tau} W_1\\right) = \\text{diag}\\left(\\cos\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\right) W_1.\n$$\n问题描述中的记号 $\\cos(v)W_1$ 是该乘积的简洁形式。\n将雅可比矩阵和函数 $g_{\\phi,\\tau}(x)$ 代入先验梯度公式：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = \\left(\\text{diag}\\left(\\cos(\\dots)\\right) W_1\\right)^\\top \\left(\\tau \\sin(\\dots)\\right) = W_1^\\top \\text{diag}\\left(\\cos(\\dots)\\right) \\left(\\tau \\sin(\\dots)\\right).\n$$\n这可以使用逐元素（哈达玛）积 $\\odot$ 来写成：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = W_1^\\top \\left(\\tau \\cos\\left(\\frac{W_1 x + b_1}{\\tau}\\right) \\odot \\sin\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\right).\n$$\n使用三角恒等式 $\\sin(2\\theta) = 2\\sin(\\theta)\\cos(\\theta)$，我们得到一个更简洁的形式：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = W_1^\\top \\left(\\frac{\\tau}{2} \\sin\\left(2\\frac{W_1 x + b_1}{\\tau}\\right)\\right).\n$$\n对于 $W_1 = I_2$ 的特定情况，先验梯度简化为 $\\frac{\\tau}{2} \\sin\\left(2\\frac{x + b_1}{\\tau}\\right)$。\n\n- **总梯度**：\n结合这些项，用于梯度下降的完整梯度是：\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = A^\\top(A x - y) + \\lambda W_1^\\top \\left(\\frac{\\tau}{2} \\sin\\left(2\\frac{W_1 x + b_1}{\\tau}\\right)\\right).\n$$\n当 $A=I_2$ 且 $W_1=I_2$ 时，它简化为：\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (x-y) + \\lambda \\frac{\\tau}{2} \\sin\\left(2\\frac{x + b_1}{\\tau}\\right).\n$$\n这个最终表达式将在求解器中实现。\n\n**2. 优化策略**\n\n- **退火优化**：此策略处理能量景观的非凸性，这种非凸性在低温 $\\tau$ 下尤为严重。它在高温下开始优化，此时先验近似为二次型，能量景观更平滑且局部最小值更少。然后根据一个调度表 $\\{\\tau_1, \\tau_2, \\dots, \\tau_T\\}$ 逐步降低温度 $\\tau$。对于调度表中的每个温度 $\\tau_t$，运行 $S$ 次梯度下降迭代，并将阶段 $t-1$ 的最终状态作为阶段 $t$ 的初始状态。此过程有助于估计值在复杂的能量曲面上导航，避免陷入在低 $\\tau$ 时变得显著的较差局部最小值中。初始状态为 $x_0$。\n\n- **冷启动优化**：相比之下，此策略直接尝试最小化最终的、高度非凸的目标函数 $E_{\\text{total}}(x;\\lambda, \\tau_{\\text{final}})$。它从相同的初始状态 $x_0$ 开始，运行的梯度下降总迭代次数与退火调度中的总次数相同（即 $S \\times T$，其中 $T$ 是温度阶段的数量）。这种方法容易陷入初始化点附近的局部最小值。\n\n**3. 实现细节**\n\n提供的 Python 代码将实现这两种策略。首先，定义所有常量参数（$x^\\star$、$\\eta$、$k$、$\\alpha$、$S$、$\\tau_{\\text{final}}$）。观测值 $y$ 和偏置 $b_1$ 被预先计算：\n- $y = x^\\star + \\eta = [2,-2]^\\top + [0.05,0.05]^\\top = [2.05, -1.95]^\\top$。\n- $b_1 = k\\pi\\tau_{\\text{final}} - W_1 x^\\star = [3,-3]^\\top \\pi (0.2) - I_2 [2,-2]^\\top \\approx [-0.115, 0.115]^\\top$。\n\n对于每个测试案例 $(\\lambda, \\{\\tau_t\\})$：\n1.  **退火运行**：初始化 $x_{\\text{anneal}} = x_0 = [0,0]^\\top$。遍历调度表中的每个 $\\tau$。在每个内循环中，运行 $S=300$ 次梯度下降迭代：$x_{\\text{anneal}} \\leftarrow x_{\\text{anneal}} - \\alpha \\nabla E_{\\text{total}}(x_{\\text{anneal}}; \\lambda, \\tau)$。存储最终的 $x_{\\text{anneal}}$。\n2.  **冷启动运行**：初始化 $x_{\\text{cold}} = x_0 = [0,0]^\\top$。温度固定为 $\\tau_{\\text{final}}$（调度表中的最后一个值）。总迭代次数为 $S \\times \\text{len(schedule)}$。在此固定温度下运行梯度下降的总迭代次数。存储最终的 $x_{\\text{cold}}$。\n3.  **比较**：计算欧几里得距离 $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$ 和 $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$。改进量为 $I = D_{\\text{cold}} - D_{\\text{anneal}}$。\n\n对于案例 3，调度表为 $[0.2, 0.2, 0.2, 0.2]$。“退火”运行变成在相同温度 $\\tau=0.2$ 下的四个梯度下降块序列。这在功能上与冷启动运行（使用 $\\tau=0.2$ 进行 $4 \\times 300$ 次迭代）是等价的。因此，我们期望 $x_{\\text{anneal}} = x_{\\text{cold}}$ 且改进量 $I$ 为 $0$。这可以作为对实现正确性的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MAP estimation problem using cold start and annealed gradient descent,\n    and computes the improvement offered by annealing.\n    \"\"\"\n    # --- Define problem constants and parameters ---\n    # Dimension of the state space\n    n = 2\n    # Ground truth state vector\n    x_star = np.array([2.0, -2.0])\n    # Additive noise vector\n    eta = np.array([0.05, 0.05])\n    # Observation vector y = x_star + eta\n    y = x_star + eta\n    # Prior network weights W1 (Identity matrix)\n    W1 = np.identity(n)\n    # Final temperature for the prior\n    tau_final = 0.2\n    # Integer vector for bias calculation\n    k_vec = np.array([3.0, -3.0])\n    # Bias vector b1, chosen so g(x_star) at tau_final is zero\n    b1 = k_vec * np.pi * tau_final - W1 @ x_star\n    # Initial state for gradient descent\n    x0 = np.array([0.0, 0.0])\n    # Gradient descent step size (learning rate)\n    alpha = 0.1\n    # Number of iterations per temperature stage\n    S = 300\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        (5.0, [5.0, 1.0, 0.5, 0.2]),\n        (12.0, [5.0, 1.0, 0.5, 0.2]),\n        (5.0, [0.2, 0.2, 0.2, 0.2]),\n    ]\n\n    def grad_E_total(x, tau, lam):\n        \"\"\"\n        Computes the gradient of the total energy E_total.\n        This implementation uses the simplified form derived for A=I and W1=I.\n        \"\"\"\n        # Gradient of the data misfit term: x - y\n        grad_data = x - y\n        \n        # Argument for the trigonometric function in the prior's gradient\n        # Based on the derived formula: (tau/2) * sin(2 * (x + b1) / tau)\n        arg = 2.0 * (x + b1) / tau\n        \n        # Gradient of the prior term\n        grad_prior = lam * (tau / 2.0) * np.sin(arg)\n        \n        # Total gradient is the sum of the two parts\n        return grad_data + grad_prior\n\n    results = []\n    for lam, schedule in test_cases:\n        # --- ANNEALED OPTIMIZATION ---\n        x_anneal = np.copy(x0)\n        # Iterate through the temperature schedule\n        for tau in schedule:\n            # For each temperature, run S steps of gradient descent\n            for _ in range(S):\n                grad = grad_E_total(x_anneal, tau, lam)\n                x_anneal = x_anneal - alpha * grad\n        \n        # Calculate Euclidean distance to the ground truth\n        D_anneal = np.linalg.norm(x_anneal - x_star)\n\n        # --- COLD START OPTIMIZATION ---\n        x_cold = np.copy(x0)\n        # Use the final temperature from the schedule\n        tau_cold = schedule[-1]\n        # Total number of iterations is the same as for the annealed run\n        total_iterations = S * len(schedule)\n        \n        # Run gradient descent for the total number of iterations at the fixed cold temperature\n        for _ in range(total_iterations):\n            grad = grad_E_total(x_cold, tau_cold, lam)\n            x_cold = x_cold - alpha * grad\n            \n        # Calculate Euclidean distance to the ground truth\n        D_cold = np.linalg.norm(x_cold - x_star)\n\n        # --- COMPUTE IMPROVEMENT AND STORE RESULT ---\n        improvement = D_cold - D_anneal\n        results.append(improvement)\n\n    # Format the results to six decimal places for the final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format: [i_1,i_2,i_3]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver function\nsolve()\n```"
        },
        {
            "introduction": "当我们使用从数据中学习到的先验时，一个重要但常被忽略的问题是算法的公平性，因为训练数据可能包含固有的社会偏见。这个练习将让你直面这一挑战，通过量化一个不平衡的先验如何扭曲后验推断的结果。你将学习使用总变分距离来衡量这种偏见，并实现一种重加权方案来修正后验分布，以达成更公平的分类结果。",
            "id": "3375179",
            "problem": "考虑一个数据同化背景下的线性逆问题，其中包含一个深度神经网络 (DNN) 先验。令 $x \\in \\mathbb{R}^n$ 表示潜状态，$y \\in \\mathbb{R}^m$ 表示观测数据。正向算子是一个已知矩阵 $A \\in \\mathbb{R}^{m \\times n}$，似然函数为高斯分布：$p(y \\mid x) = \\mathcal{N}(y; A x, \\sigma^2 I_m)$，其中 $I_m$ 是 $m \\times m$ 的单位矩阵，$\\sigma^2 \\gt 0$ 是观测噪声方差。DNN 先验 $p_\\phi(x)$ 在一个由索引 $k \\in \\{1, \\dots, K\\}$ 标记的类别组成的不平衡数据集上训练得到，它被近似为一个混合模型 $p_\\phi(x) \\approx \\sum_{k=1}^K \\pi_k p_k(x)$，其中 $\\pi_k$ 是训练集中的类别比例，每个类条件先验 $p_k(x)$ 被建模为高斯分布 $p_k(x) = \\mathcal{N}(x; \\mu_k, \\Sigma_k)$，其均值为 $\\mu_k \\in \\mathbb{R}^n$，协方差为对称正定矩阵 $\\Sigma_k \\in \\mathbb{R}^{n \\times n}$。\n\n基本原理：使用贝叶斯定理和线性高斯模型的规则。给定数据的类别后验分布为\n$$\nP(k \\mid y) \\propto \\pi_k \\int p(y \\mid x)\\, p_k(x)\\, dx,\n$$\n在给定假设下，该积分是一个定义明确的高斯边缘化。\n\n您的任务是评估先验 $p_\\phi(x)$ 是否会使后验重构偏向于过度代表的类别，并设计一个数学上合理的重加权方案来纠正后验偏斜。使用以下有原则的目标重加权方法：对于一个指定的目标类别分布 $\\tau_k$（其中 $\\sum_{k=1}^K \\tau_k = 1$），通过在贝叶斯更新中用 $\\tau_k$ 替换 $\\pi_k$ 来定义一个修正的类别后验分布。这对应于通过拉东-尼科迪姆导数 $w(x) = \\frac{\\sum_{k} \\tau_k p_k(x)}{\\sum_{k} \\pi_k p_k(x)}$ 对先验进行重要性重加权，在类别混合的情况下，这简化为在归一化之前将每个类别的贡献乘以 $\\frac{\\tau_k}{\\pi_k}$。对于给定的观测值 $y$，通过原始后验 $P_\\pi(k \\mid y)$（使用 $\\pi_k$）和修正后验 $P_\\tau(k \\mid y)$（使用 $\\tau_k$）之间的全变差距离来量化先验引起的偏差：\n$$\n\\mathrm{TV}\\big(P_\\pi(\\cdot \\mid y), P_\\tau(\\cdot \\mid y)\\big) = \\frac{1}{2} \\sum_{k=1}^K \\left| P_\\pi(k \\mid y) - P_\\tau(k \\mid y) \\right|.\n$$\n\n您必须实现一个完整、可运行的程序，为下面的每个测试用例计算：\n1) 原始后验类别概率 $P_\\pi(k \\mid y)$，2) 使用均匀目标 $\\tau_k = \\frac{1}{K}$ 的修正后验类别概率 $P_\\tau(k \\mid y)$，以及 3) 这两个后验之间的全变差距离 $\\mathrm{TV}$。对于该测试套件，程序应输出一行，其中包含所有测试用例的 $\\mathrm{TV}$ 值的列表，并按顺序排列。\n\n假设 $K = 3$，$n = 2$，$m = 2$，$A = \\begin{bmatrix} 1.0  0.3 \\\\ 0.2  1.0 \\end{bmatrix}$，$\\sigma^2 = 0.5$，以及一个不平衡的训练先验 $\\pi = [0.7, 0.2, 0.1]$。除非每个测试用例另有规定，否则对所有 $k$ 使用 $\\Sigma_k = \\mathrm{diag}([0.3, 0.3])$。\n\n使用线性高斯模型的高斯边缘化定律：如果 $x \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$ 且 $y \\mid x \\sim \\mathcal{N}(A x, \\sigma^2 I_m)$，则 $y \\mid k \\sim \\mathcal{N}(A \\mu_k, A \\Sigma_k A^\\top + \\sigma^2 I_m)$。因此，\n$$\nP_\\pi(k \\mid y) \\propto \\pi_k \\, \\mathcal{N}\\big(y; A \\mu_k, A \\Sigma_k A^\\top + \\sigma^2 I_m\\big),\n\\quad\nP_\\tau(k \\mid y) \\propto \\tau_k \\, \\mathcal{N}\\big(y; A \\mu_k, A \\Sigma_k A^\\top + \\sigma^2 I_m\\big),\n$$\n在每种情况下都对 $k$ 进行归一化。\n\n测试套件：\n- 用例 1 (理想情况，有适度证据支持一个过度代表的类别)：$\\mu_1 = [0.0, 0.0]$，$\\mu_2 = [2.0, 0.0]$，$\\mu_3 = [-2.0, 0.0]$，$y = [0.1, -0.1]$。\n- 用例 2 (边界情况：仅受先验影响，类别证据相同)：$\\mu_1 = [0.0, 0.0]$，$\\mu_2 = [0.0, 0.0]$，$\\mu_3 = [0.0, 0.0]$，$y = [0.0, 0.0]$。\n- 用例 3 (极端情况：有强力证据支持一个少数类别)：$\\mu_1 = [0.0, 0.0]$，$\\mu_2 = [2.0, 0.0]$，$\\mu_3 = [-2.0, 0.0]$，$y = [-1.95, -0.45]$。\n\n所有向量和矩阵都在 $\\mathbb{R}$ 中，不涉及物理单位。不使用角度。不得使用百分比；在相关情况下，将任何分数量表示为小数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按上述用例的顺序排列，例如 [result_1, result_2, result_3]，其中每个结果都是一个表示全变差距离的浮点数。\n\n您的实现必须是数值稳定的：通过对数密度评估高斯密度，并使用跨类别的 log-sum-exp 计算进行归一化。最终输出必须严格符合所述格式。",
            "solution": "该问题要求通过计算后验类别分布之间的全变差距离，来评估贝叶斯逆问题中由先验引起的偏差。原始后验 $P_\\pi(k \\mid y)$ 是使用不平衡的类别先验 $\\pi_k$ 推导出来的。修正后验 $P_\\tau(k \\mid y)$ 是使用均匀的目标先验 $\\tau_k$ 推导出来的。该分析基于已建立的线性高斯模型理论。\n\n首先，我们形式化后验类别概率 $P(k \\mid y)$ 的计算。根据贝叶斯定理，给定观测值 $y$ 的类别 $k$ 的后验概率，与给定该类别的边际似然 $p(y \\mid k)$ 和该类别的先验概率 $P(k)$ 的乘积成正比。\n$$\nP(k \\mid y) = \\frac{p(y \\mid k) P(k)}{p(y)} = \\frac{p(y \\mid k) P(k)}{\\sum_{j=1}^K p(y \\mid j) P(j)}\n$$\n在此背景下，$P(k)$ 代表类别 $k$ 的先验权重，对于原始后验是 $\\pi_k$，对于修正后验是 $\\tau_k$。\n\n边际似然 $p(y \\mid k)$ 是通过从联合分布 $p(y, x \\mid k) = p(y \\mid x) p_k(x)$ 中对潜状态 $x$ 进行积分得到的。问题指明，关于 $x$ 的类条件先验是高斯分布 $p_k(x) = \\mathcal{N}(x; \\mu_k, \\Sigma_k)$，且似然函数也是高斯分布 $p(y \\mid x) = \\mathcal{N}(y; A x, \\sigma^2 I_m)$。这构成了一个线性高斯系统。对于此类系统，$y$ 的边际分布也是高斯分布。该边际分布的参数推导如下：\n以类别 $k$ 为条件的堆叠向量 $\\begin{pmatrix} x \\\\ y \\end{pmatrix}$ 的联合分布是高斯分布。我们有 $E[x \\mid k] = \\mu_k$ 和 $\\text{Cov}(x \\mid k) = \\Sigma_k$。$y$ 的条件均值是 $E[y \\mid x, k] = Ax$。根据全期望定律，$y$ 的边际均值为 $E[y \\mid k] = E[E[y \\mid x, k] \\mid k] = E[Ax \\mid k] = A E[x \\mid k] = A\\mu_k$。边际协方差由全协方差定律给出：$\\text{Cov}(y \\mid k) = E[\\text{Cov}(y \\mid x, k) \\mid k] + \\text{Cov}(E[y \\mid x, k] \\mid k) = E[\\sigma^2 I_m \\mid k] + \\text{Cov}(Ax \\mid k) = \\sigma^2 I_m + A \\text{Cov}(x \\mid k) A^\\top = A \\Sigma_k A^\\top + \\sigma^2 I_m$。\n\n因此，边际似然由下式给出：\n$$\np(y \\mid k) = \\mathcal{N}(y; \\mu_{y|k}, \\Sigma_{y|k})\n$$\n其中均值为 $\\mu_{y|k} = A\\mu_k$，协方差为 $\\Sigma_{y|k} = A\\Sigma_k A^\\top + \\sigma^2 I_m$。\n\n为了数值稳定性，计算在对数域中进行。使用通用先验权重 $w_k$ 的类别 $k$ 的未归一化对数后验为：\n$$\n\\log \\tilde{P}(k \\mid y) = \\log w_k + \\log p(y \\mid k)\n$$\n多元高斯边际似然的对数概率密度函数是：\n$$\n\\log p(y \\mid k) = -\\frac{m}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\Sigma_{y|k}) - \\frac{1}{2}(y - \\mu_{y|k})^\\top \\Sigma_{y|k}^{-1} (y - \\mu_{y|k})\n$$\n其中 $m$ 是观测空间的维度。然后使用 log-sum-exp (LSE) 函数获得归一化的后验概率，这可以防止数值下溢或上溢：\n$$\nP(k \\mid y) = \\exp\\left( \\log \\tilde{P}(k \\mid y) - \\underset{j=1,\\dots,K}{\\text{LSE}}[\\log \\tilde{P}(j \\mid y)] \\right)\n$$\n其中 $\\text{LSE}(z_1, \\dots, z_K) = \\log \\sum_{j=1}^K \\exp(z_j)$。\n\n此过程应用两次：\n1.  为计算 $P_\\pi(k \\mid y)$，我们设置 $w_k = \\pi_k$，其中 $\\pi = [0.7, 0.2, 0.1]$。\n2.  为计算 $P_\\tau(k \\mid y)$，我们设置 $w_k = \\tau_k$，其中对于所有 $k \\in \\{1, 2, 3\\}$，$\\tau_k = 1/K = 1/3$。\n\n最后，由先验引起的偏差通过这两个后验分布之间的全变差 (TV) 距离来量化：\n$$\n\\mathrm{TV}\\big(P_\\pi(\\cdot \\mid y), P_\\tau(\\cdot \\mid y)\\big) = \\frac{1}{2} \\sum_{k=1}^K \\left| P_\\pi(k \\mid y) - P_\\tau(k \\mid y) \\right|\n$$\n\n在实现中，我们使用提供的参数：$K=3$，$n=2$，$m=2$，$A = \\begin{bmatrix} 1.0  0.3 \\\\ 0.2  1.0 \\end{bmatrix}$，$\\sigma^2 = 0.5$，以及对于所有 $k$ 的 $\\Sigma_k = \\mathrm{diag}([0.3, 0.3])$。对于每个由特定类别均值 $\\mu_k$ 和观测值 $y$ 定义的测试用例，我们首先计算恒定的边际协方差 $\\Sigma_y = A\\Sigma_k A^\\top + \\sigma^2 I_m$。然后，对于每个类别 $k$，我们计算边际均值 $\\mu_{y|k} = A\\mu_k$ 并评估对数似然 $\\log p(y \\mid k)$。这些对数似然分别与对数先验 $\\log \\pi_k$ 和 $\\log \\tau_k$ 结合，通过 log-sum-exp 进行归一化以得到 $P_\\pi(\\cdot \\mid y)$ 和 $P_\\tau(\\cdot \\mid y)$，并最终用于计算 TV 距离。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes the Total Variation (TV) distance between posterior class distributions\n    for a series of test cases in a Bayesian inverse problem.\n    \"\"\"\n    # Define problem constants\n    K = 3  # Number of classes\n    n = 2  # Dimension of state space\n    m = 2  # Dimension of observation space\n    A = np.array([[1.0, 0.3], [0.2, 1.0]])\n    sigma2 = 0.5\n    \n    # Prior class proportions (imbalanced)\n    pi = np.array([0.7, 0.2, 0.1])\n    \n    # Target class proportions (uniform)\n    tau = np.array([1.0/K] * K)\n    \n    # Class-conditional prior covariance (same for all classes)\n    Sigma_k = np.diag([0.3, 0.3])\n    \n    # Pre-compute the marginal likelihood covariance, which is constant across classes\n    # since Sigma_k is the same for all k.\n    # Sigma_y = A @ Sigma_k @ A.T + sigma^2 * I_m\n    Sigma_y = A @ Sigma_k @ A.T + sigma2 * np.identity(m)\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"mus\": [np.array([0.0, 0.0]), np.array([2.0, 0.0]), np.array([-2.0, 0.0])],\n            \"y\": np.array([0.1, -0.1]),\n        },\n        {\n            \"mus\": [np.array([0.0, 0.0]), np.array([0.0, 0.0]), np.array([0.0, 0.0])],\n            \"y\": np.array([0.0, 0.0]),\n        },\n        {\n            \"mus\": [np.array([0.0, 0.0]), np.array([2.0, 0.0]), np.array([-2.0, 0.0])],\n            \"y\": np.array([-1.95, -0.45]),\n        },\n    ]\n\n    tv_distances = []\n\n    for case in test_cases:\n        mus_k = case[\"mus\"]\n        y = case[\"y\"]\n        \n        log_marginal_likelihoods = np.zeros(K)\n        \n        # 1. Compute log marginal likelihood log p(y|k) for each class k\n        for k in range(K):\n            # Mean of the marginal likelihood distribution for class k\n            mu_y_k = A @ mus_k[k]\n            \n            # Log PDF of y under the marginal likelihood for class k\n            log_marginal_likelihoods[k] = multivariate_normal.logpdf(y, mean=mu_y_k, cov=Sigma_y)\n\n        # 2. Compute original posterior P_pi(k|y)\n        # Unnormalized log posterior: log(pi_k) + log p(y|k)\n        log_posterior_pi = np.log(pi) + log_marginal_likelihoods\n        \n        # Normalize using log-sum-exp to get log P_pi(k|y)\n        log_norm_pi = logsumexp(log_posterior_pi)\n        log_P_pi = log_posterior_pi - log_norm_pi\n        P_pi = np.exp(log_P_pi)\n\n        # 3. Compute corrected posterior P_tau(k|y)\n        # Unnormalized log posterior: log(tau_k) + log p(y|k)\n        log_posterior_tau = np.log(tau) + log_marginal_likelihoods\n        \n        # Normalize using log-sum-exp to get log P_tau(k|y)\n        log_norm_tau = logsumexp(log_posterior_tau)\n        log_P_tau = log_posterior_tau - log_norm_tau\n        P_tau = np.exp(log_P_tau)\n        \n        # 4. Compute the Total Variation (TV) distance\n        tv_dist = 0.5 * np.sum(np.abs(P_pi - P_tau))\n        tv_distances.append(tv_dist)\n\n    # Format and print the final results\n    print(f\"[{','.join(map(str, tv_distances))}]\")\n\nsolve()\n```"
        }
    ]
}