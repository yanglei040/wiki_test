{
    "hands_on_practices": [
        {
            "introduction": "傅里叶神经算子 (FNO) 的一个关键优势在于其处理高分辨率输入时的计算效率。本练习将引导您通过推导其计算复杂度，并与标准的实空间卷积进行比较，从而揭示这种效率的来源。这个练习旨在帮助您建立对 FNO 架构设计原则的基础理解。",
            "id": "3407231",
            "problem": "考虑一个具有$N$个等距网格点的一维周期性域，以及一个将$C_{\\mathrm{in}}$个输入通道映射到$C_{\\mathrm{out}}$个输出通道的傅里叶神经算子 (FNO) 层。FNO层在频域中实现：它通过快速傅里叶变换 (FFT) 计算每个输入通道的离散傅里叶变换 (DFT)，在$M$个保留的傅里叶模式上乘以一个学习到的复值谱权重矩阵，然后计算逆FFT以返回到实空间。假设使用以下标准且经过广泛验证的成本模型：对长度为$N$的序列进行的每次正向或逆向FFT的成本为$\\alpha N \\log_{2} N$次浮点运算 (flops)，且与通道索引无关；在保留模式上的谱乘法每次应用的成本为$\\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$次浮点运算，且不依赖于$N$。为了进行比较，考虑一个核宽度为$k$（即每个输入-输出通道对具有大小为$k$的紧凑模板）的算子层的实空间卷积实现，其中每个输出通道的每个网格点的成本与输入通道数和模板宽度成正比。具体来说，假设实空间卷积的成本为$\\gamma N k C_{\\mathrm{in}} C_{\\mathrm{out}}$次浮点运算。\n\n仅从DFT的线性性质和卷积定理出发，结合上述成本模型，完成以下任务：\n\n(a) 推导频域中FNO层实现和实空间卷积实现的主阶浮点运算次数，将两者都表示为$N$、$C_{\\mathrm{in}}$、$C_{\\mathrm{out}}$、$k$以及常数$\\alpha$、$\\beta$、$\\gamma$的函数。你的最终表达式必须使用自然对数$\\ln$，并且必须明确地计入底数转换$\\log_{2} N = \\ln N / \\ln 2$。在给出对$N$的主阶依赖关系时，忽略$N$的低阶可加项。\n\n(b) 在$M$与$N$无关，并且在$N$很大时谱乘法项的阶严格低于FFT项的阶的假设下，计算频域FNO实现和实空间卷积实现的主阶浮点运算次数相等时的交叉网格尺寸$N^{\\star}$的精确闭式解析表达式。将$N^{\\star}$表示为仅包含$\\alpha$、$\\gamma$、$k$、$C_{\\mathrm{in}}$、$C_{\\mathrm{out}}$和$\\ln 2$的单个表达式。使用自然指数$\\exp(\\cdot)$，并且不包含任何单位。最终答案必须是单个闭式解析表达式。",
            "solution": "该问题要求推导和比较两种神经算子层的计算成本（以浮点运算次数flops衡量）：一种是在频域中实现的傅里叶神经算子 (FNO) 层，另一种是在实空间中实现的标准卷积层。\n\n### (a) 部分：主阶浮点运算次数的推导\n\n首先，我们推导每种实现的主阶浮点运算次数，将其表示为给定参数的函数。\n\n**傅里叶神经算子 (FNO) 层成本**\n\nFNO层的计算涉及三个主要步骤。总浮点运算次数（记为$F_{\\mathrm{FNO}}$）是这些步骤成本的总和。\n\n1.  **正向傅里叶变换**：输入包含$C_{\\mathrm{in}}$个通道，每个通道是一个长度为$N$的序列。对每个通道应用快速傅里叶变换 (FFT)。单次FFT的成本给定为$\\alpha N \\log_{2}(N)$。由于离散傅里叶变换 (DFT) 的线性性质，我们可以独立处理$C_{\\mathrm{in}}$个输入通道中的每一个。\n    此步骤的总成本是通道数乘以每个通道的成本：\n    $$C_{\\mathrm{FFT}} = C_{\\mathrm{in}} \\alpha N \\log_{2}(N)$$\n\n2.  **谱乘法**：在频域中，通过在$M$个保留的傅里叶模式上与一个学习到的权重矩阵相乘，将$C_{\\mathrm{in}}$个变换后的通道映射到$C_{\\mathrm{out}}$个输出通道。该操作的成本是明确给出的，并且与网格尺寸$N$无关：\n    $$C_{\\mathrm{mul}} = \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n3.  **逆傅里叶变换**：在谱乘法之后，我们在频域中有$C_{\\mathrm{out}}$个通道。为了返回到物理域，对这些通道中的每一个应用逆快速傅里叶变换 (IFFT)。假设一次IFFT的成本与一次正向FFT的成本相同，即$\\alpha N \\log_{2}(N)$。\n    此步骤的总成本是：\n    $$C_{\\mathrm{IFFT}} = C_{\\mathrm{out}} \\alpha N \\log_{2}(N)$$\n\nFNO层的总浮点运算次数是这三部分成本的总和：\n$$F_{\\mathrm{FNO}} = C_{\\mathrm{FFT}} + C_{\\mathrm{mul}} + C_{\\mathrm{IFFT}}$$\n$$F_{\\mathrm{FNO}} = C_{\\mathrm{in}} \\alpha N \\log_{2}(N) + \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}} + C_{\\mathrm{out}} \\alpha N \\log_{2}(N)$$\n合并依赖于$N$的项：\n$$F_{\\mathrm{FNO}} = \\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\log_{2}(N) + \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n问题要求关于$N$的主阶浮点运算次数。项$\\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$相对于$N$是常数，而第一项的增长速度为$N \\ln(N)$。因此，主阶项是$\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\log_{2}(N)$。我们必须使用恒等式$\\log_{2}(N) = \\frac{\\ln(N)}{\\ln(2)}$将以2为底的对数转换为自然对数。\n\nFNO的主阶浮点运算次数$F_{\\mathrm{FNO, leading}}$是：\n$$F_{\\mathrm{FNO, leading}} = \\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\left(\\frac{\\ln(N)}{\\ln(2)}\\right) = \\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} N \\ln(N)$$\n\n**实空间卷积层成本**\n\n核宽度为$k$的实空间卷积层的成本在问题陈述中已直接给出。该操作的成本与网格尺寸$N$成线性关系。\n卷积的总浮点运算次数$F_{\\mathrm{conv}}$是：\n$$F_{\\mathrm{conv}} = \\gamma N k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n在所提供的成本模型中，这个表达式是主阶项（也是唯一的项）。\n\n### (b) 部分：交叉网格尺寸$N^{\\star}$\n\n交叉网格尺寸$N^{\\star}$是两种实现的主阶浮点运算次数相等时的$N$值。这可以通过设$F_{\\mathrm{FNO, leading}} = F_{\\mathrm{conv}}$并求解$N = N^{\\star}$来找到。\n\n在确定$F_{\\mathrm{FNO, leading}}$时，已经使用了当$N$很大时谱乘法项的阶严格低于FFT项的阶的假设。\n\n令两个主阶成本表达式相等：\n$$F_{\\mathrm{FNO, leading}}(N^{\\star}) = F_{\\mathrm{conv}}(N^{\\star})$$\n$$\\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} N^{\\star} \\ln(N^{\\star}) = \\gamma N^{\\star} k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n由于网格尺寸$N^{\\star}$必须是正整数，我们可以将等式两边同除以$N^{\\star}$（对于$N^{\\star} \\neq 0$）：\n$$\\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} \\ln(N^{\\star}) = \\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n现在，我们通过将$\\ln(N^{\\star})$分离到等式的一边来求解它：\n$$\\ln(N^{\\star}) = \\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}$$\n\n为了求出$N^{\\star}$，我们对等式两边取指数：\n$$N^{\\star} = \\exp\\left( \\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})} \\right)$$\n\n这就是交叉网格尺寸$N^{\\star}$的最终闭式解析表达式。",
            "answer": "$$\n\\boxed{\\exp\\left(\\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}\\right)}\n$$"
        },
        {
            "introduction": "在了解了 FNO 的效率之后，我们将注意力转向深度算子网络 (DeepONet) 及其训练过程。理解梯度如何计算对于定制模型和损失函数至关重要。本练习将通过分步推导分支网络 (branch network) 和主干网络 (trunk network) 的梯度，来巩固深度学习的核心机制。",
            "id": "3407183",
            "problem": "考虑在反问题和数据同化的背景下，学习一个将输入函数 $u$ 映射到输出函数 $y$ 的非线性算子。一个深度算子网络 (DeepONet) 通过一个分支网络和一个主干网络对该算子进行参数化，具体如下。分支网络将离散化的输入 $u$ 编码为一个系数向量 $b(u; \\theta_{b}) \\in \\mathbb{R}^{p}$，而主干网络将查询位置 $x$ 编码为一个基向量 $t(x; \\theta_{t}) \\in \\mathbb{R}^{p}$。算子输出定义为 $G(u)(x) = b(u; \\theta_{b})^{\\top} t(x; \\theta_{t})$，这是分支嵌入和主干嵌入的一个双线性形式。给定一个包含 $N$ 个离散化输入-输出函数对 $\\{(u_{i}, y_{i})\\}_{i=1}^{N}$ 的训练数据集，其中每个 $y_{i}$ 都在 $M$ 个查询位置 $\\{x_{j}\\}_{j=1}^{M}$ 上被观测。训练目标是正交加权经验均方损失\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right)^{2},\n$$\n其中 $w_{j} > 0$ 是在查询位置上给定的正交权重。假设 $b(u; \\theta_{b})$ 和 $t(x; \\theta_{t})$ 分别对 $\\theta_{b}$ 和 $\\theta_{t}$ 可微，并记雅可比矩阵为 $J_{b}(u; \\theta_{b}) = \\frac{\\partial b(u; \\theta_{b})}{\\partial \\theta_{b}} \\in \\mathbb{R}^{p \\times n_{b}}$ 和 $J_{t}(x; \\theta_{t}) = \\frac{\\partial t(x; \\theta_{t})}{\\partial \\theta_{t}} \\in \\mathbb{R}^{p \\times n_{t}}$，其中 $n_{b}$ 和 $n_{t}$ 是参数向量 $\\theta_{b}$ 和 $\\theta_{t}$ 的维度。从第一性原理出发，即微分的链式法则和均方损失的定义，推导梯度 $\\nabla_{\\theta_{b}} L$ 和 $\\nabla_{\\theta_{t}} L$ 的反向传播表达式，要求以闭式形式，用雅可比矩阵 $J_{b}$ 和 $J_{t}$、嵌入 $b(u_{i}; \\theta_{b})$ 和 $t(x_{j}; \\theta_{t})$ 以及残差 $G(u_{i})(x_{j}) - y_{i}(x_{j})$ 来表示。您的最终答案必须是一个单一的解析表达式，将两个梯度收集在一个行矩阵中，使用 LaTeX 的 $\\texttt{pmatrix}$ 环境，且不包含任何单位。不要进行任何数值近似或舍入。",
            "solution": "该问题要求推导 DeepONet 损失函数相对于分支网络参数 $\\theta_{b}$ 和主干网络参数 $\\theta_{t}$ 的梯度。推导将从第一性原理出发，将多变量微积分的链式法则应用于给定的损失函数。\n\n损失函数为正交加权经验均方误差：\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right)^{2}\n$$\n其中 DeepONet 算子定义为 $G(u_{i})(x_{j}) = b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})$。为清晰起见，我们将第 $i$ 个输入和第 $j$ 个查询位置的残差表示为 $r_{ij} = G(u_{i})(x_{j}) - y_{i}(x_{j})$。则损失函数可以写成：\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij}^{2}\n$$\n我们需要计算梯度 $\\nabla_{\\theta_{b}} L = \\frac{\\partial L}{\\partial \\theta_{b}}$ 和 $\\nabla_{\\theta_{t}} L = \\frac{\\partial L}{\\partial \\theta_{t}}$。它们分别是维度为 $n_{b} \\times 1$ 和 $n_{t} \\times 1$ 的列向量。\n\n首先，我们推导关于分支网络参数 $\\theta_{b}$ 的梯度。\n根据链式法则，损失函数 $L$ 对 $\\theta_{b}$ 的梯度为：\n$$\n\\nabla_{\\theta_{b}} L = \\frac{\\partial L}{\\partial \\theta_{b}} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\frac{\\partial}{\\partial \\theta_{b}} (r_{ij}^{2})\n$$\n残差平方的导数为 $\\frac{\\partial}{\\partial \\theta_{b}} (r_{ij}^{2}) = 2 r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{b}}$。将此代入方程中，得到：\n$$\n\\nabla_{\\theta_{b}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{b}}\n$$\n现在，我们计算残差 $r_{ij} = b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) - y_{i}(x_{j})$ 对 $\\theta_{b}$ 的导数。项 $y_{i}(x_{j})$ 不依赖于 $\\theta_{b}$，因此其导数为零。\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{b}} = \\frac{\\partial}{\\partial \\theta_{b}} \\left( b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) \\right)\n$$\n为了计算这个梯度，我们可以使用标量积对向量求导的恒等式。给定向量 $\\mathbf{a}$ 和 $\\mathbf{c}$，其中 $\\mathbf{a}$ 是向量 $\\mathbf{x}$ 的函数，它们的点积的梯度为 $\\nabla_{\\mathbf{x}}(\\mathbf{a}(\\mathbf{x})^\\top \\mathbf{c}) = (\\frac{\\partial \\mathbf{a}(\\mathbf{x})}{\\partial \\mathbf{x}})^\\top \\mathbf{c}$。在这里，$\\mathbf{a}$ 对应 $b(u_{i}; \\theta_{b})$，$\\mathbf{c}$ 对应 $t(x_{j}; \\theta_{t})$，$\\mathbf{x}$ 对应 $\\theta_{b}$。项 $\\frac{\\partial b(u_{i}; \\theta_{b})}{\\partial \\theta_{b}}$ 是问题中定义的雅可比矩阵 $J_{b}(u_{i}; \\theta_{b}) \\in \\mathbb{R}^{p \\times n_{b}}$。因此，\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{b}} = J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})\n$$\n该结果是一个维度为 $n_{b} \\times 1$ 的列向量。将其代回 $\\nabla_{\\theta_{b}} L$ 的表达式中：\n$$\n\\nabla_{\\theta_{b}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})\n$$\n\n接下来，我们推导关于主干网络参数 $\\theta_{t}$ 的梯度。该过程与对 $\\theta_{b}$ 的推导是对称的。\n$$\n\\nabla_{\\theta_{t}} L = \\frac{\\partial L}{\\partial \\theta_{t}} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{t}}\n$$\n我们计算残差 $r_{ij}$ 对 $\\theta_{t}$ 的导数：\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{t}} = \\frac{\\partial}{\\partial \\theta_{t}} \\left( b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) \\right)\n$$\n在这种情况下，向量 $b(u_{i}; \\theta_{b})$ 相对于 $\\theta_{t}$ 是常数。使用类似的向量微积分恒等式 $\\nabla_{\\mathbf{x}}(\\mathbf{c}^\\top \\mathbf{a}(\\mathbf{x})) = (\\frac{\\partial \\mathbf{a}(\\mathbf{x})}{\\partial \\mathbf{x}})^\\top \\mathbf{c}$，其中现在 $\\mathbf{a}$ 对应 $t(x_{j}; \\theta_{t})$，$\\mathbf{c}$ 对应 $b(u_{i}; \\theta_{b})$，$\\mathbf{x}$ 对应 $\\theta_{t}$。项 $\\frac{\\partial t(x_{j}; \\theta_{t})}{\\partial \\theta_{t}}$ 是雅可比矩阵 $J_{t}(x_{j}; \\theta_{t}) \\in \\mathbb{R}^{p \\times n_{t}}$。所以，\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{t}} = J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b})\n$$\n这是一个维度为 $n_{t} \\times 1$ 的列向量。将其代入 $\\nabla_{\\theta_{t}} L$ 的表达式中：\n$$\n\\nabla_{\\theta_{t}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b})\n$$\n$\\nabla_{\\theta_{b}} L$ 和 $\\nabla_{\\theta_{t}} L$ 的这两个表达式是梯度的最终闭式反向传播公式，用指定的量表示。问题要求将它们收集到一个单一的行矩阵中。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})  \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b}) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "本练习将抽象的算子学习模型与实际的物理应用联系起来。求解偏微分方程 (PDE) 通常需要满足特定的边界条件。本练习介绍了一种强大的技术，即结合物理信息损失 (physics-informed loss) 和提升算子 (lifting operator)，通过构造来强制执行这些条件，从而展示了如何调整算子学习框架以解决现实世界中的科学问题。",
            "id": "3407257",
            "problem": "考虑一个有界Lipschitz域 $\\Omega \\subset \\mathbb{R}^{d}$，其边界为 $\\partial \\Omega$。令 $\\mathcal{A}_{a}$ 表示一个由系数场 $a \\in \\mathcal{A}$ 参数化的线性、一致椭圆微分算子，并考虑带有Dirichlet数据的边值问题\n$$\n\\mathcal{A}_{a} u = f \\quad \\text{in } \\Omega, \n\\qquad\nu = g \\quad \\text{on } \\partial \\Omega,\n$$\n其中输入 $(a,f,g)$ 来自一个支撑在 $\\mathcal{A} \\times L^{2}(\\Omega) \\times H^{1/2}(\\partial \\Omega)$ 上的特定应用分布。给定一个大小为 $M$ 的训练数据集 $\\mathcal{D} = \\{(a_{i}, f_{i}, g_{i}, y_{i}, \\mathcal{H}_{i})\\}_{i=1}^{M}$，其中 $y_{i} \\in \\mathbb{R}^{p_{i}}$ 是通过观测算子 $\\mathcal{H}_{i}: H^{1}(\\Omega) \\to \\mathbb{R}^{p_{i}}$ 得到的观测值，且 $y_{i} = \\mathcal{H}_{i}(u_{i}) + \\eta_{i}$，$\\eta_{i}$ 为噪声。令 $\\theta$ 参数化一个傅里叶神经算子（Fourier Neural Operator, FNO）或一个深度算子网络（Deep Operator Network, DeepONet），它定义了一个算子学习器 $T_{\\theta}: (a,f,g) \\mapsto u_{\\theta}(\\cdot; a,f,g)$。\n\n您将基于残差最小化和数据同化原理，构建用于训练 $T_{\\theta}$ 的经验风险。您可以使用的基本依据是：Dirichlet边界条件的定义、经验风险最小化的概念、使用残差来满足偏微分方程，以及通过平方范数来整合观测-模型失配。\n\n对于每个训练项 $i \\in \\{1,\\dots,M\\}$，令 $\\mathcal{X}_{i}^{\\mathrm{int}} \\subset \\Omega$ 和 $\\mathcal{X}_{i}^{\\mathrm{b}} \\subset \\partial \\Omega$ 分别为内部和边界配置点的集合，其基数分别为 $N_{i}^{\\mathrm{int}}$ 和 $N_{i}^{\\mathrm{b}}$。定义残差\n$$\nr_{i}^{\\mathrm{int}}(x;\\theta) := \\mathcal{A}_{a_{i}}\\big(u_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x), \\quad x \\in \\mathcal{X}_{i}^{\\mathrm{int}},\n$$\n$$\nr_{i}^{\\mathrm{b}}(x;\\theta) := u_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x), \\quad x \\in \\mathcal{X}_{i}^{\\mathrm{b}},\n$$\n和数据失配 $m_{i}(\\theta) := \\mathcal{H}_{i}\\big(u_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\in \\mathbb{R}^{p_{i}}$。考虑增广经验风险\n$$\n\\mathcal{L}_{\\mathrm{aug}}(\\theta) \n:= \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\big| r_{i}^{\\mathrm{int}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{data}} \\cdot \\big\\| m_{i}(\\theta) \\big\\|_{2}^{2}\n\\right],\n$$\n其中 $\\lambda_{\\mathrm{int}}, \\lambda_{\\mathrm{b}}, \\lambda_{\\mathrm{data}}$ 是可调的非负权重。\n\n现在，定义一个提升算子 $L: H^{1/2}(\\partial \\Omega) \\to H^{1}(\\Omega)$，使得对所有容许的 $g$ 都有 $(L g)|_{\\partial \\Omega} = g$；并定义一个边界消失标量函数 $b \\in C^{0}(\\overline{\\Omega})$，使得 $b|_{\\partial \\Omega} = 0$ 且在某个内部参考点 $x^{\\star} \\in \\Omega$ 处 $b(x^{\\star}) = 1$。通过以下方式对模型输出进行重参数化\n$$\nu_{\\theta}(\\cdot; a,f,g) \\;=\\; L g \\;+\\; b \\, v_{\\theta}(\\cdot; a,f,g),\n$$\n其中 $v_{\\theta}$ 是学习到的齐次分量。对于深度算子网络，这可以通过将主干基与 $b$ 相乘来修改实现；对于傅里叶神经算子，这可以通过将网络的物理空间输出与 $b$ 相乘，然后在计算残差之前加上 $L g$ 来实现。\n\n从上述定义出发，除了所述的重参数化之外，不假设任何特定的架构公式，推导提升后的经验风险 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ 的显式闭式解析表达式。该表达式通过将重参数化的 $u_{\\theta}$ 代入 $\\mathcal{L}_{\\mathrm{aug}}(\\theta)$ 并利用 $L$ 的Dirichlet性质和 $b$ 的边界消失性质简化边界项而得到。\n\n您的最终答案必须是关于 $(a_{i}, f_{i}, g_{i}, y_{i}, \\mathcal{H}_{i})$、$L$、$b$ 和 $v_{\\theta}$ 的 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ 的单一简化解析表达式，并且不得包含任何其他文本。不需要进行数值评估。请将您的最终答案表示为闭式表达式。",
            "solution": "问题陈述已经过验证，被认为是科学上可靠、适定且客观的。它提出了一个清晰的符号操作任务，该任务基于科学机器学习和偏微分方程（PDE）数值分析领域的既定原则。该问题提供了一套完整的定义和约束，这些定义和约束内部一致且与算子学习主题相关。任务是在对所学函数进行提议的重参数化下，为经验风险泛函推导一个新的表达式。在训练神经算子的背景下，这是一个标准且有意义的程序，尤其适用于处理非齐次边界条件。\n\n目标是通过将重参数化的模型输出 $u_{\\theta}$ 代入增广经验风险泛函 $\\mathcal{L}_{\\mathrm{aug}}(\\theta)$，来推导提升后的经验风险（记为 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$）的显式解析表达式。\n\n增广经验风险定义为：\n$$\n\\mathcal{L}_{\\mathrm{aug}}(\\theta) \n:= \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\big| r_{i}^{\\mathrm{int}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{data}} \\cdot \\big\\| m_{i}(\\theta) \\big\\|_{2}^{2}\n\\right]\n$$\n模型输出的重参数化由下式给出：\n$$\nu_{\\theta}(\\cdot; a,f,g) \\;=\\; L g \\;+\\; b \\, v_{\\theta}(\\cdot; a,f,g)\n$$\n其中 $L$ 是一个提升算子，$b$ 是一个边界消失函数。现在，算子学习器学习函数 $v_{\\theta}$。\n\n我们将分析此代换对单个训练样本 $i$ 的损失函数三个分量中每一个的影响，在上下文清晰的情况下，为简洁起见，省略 $u_{\\theta}$ 和 $v_{\\theta}$ 的参数。\n\n1.  **内部残差项**：\n    内部残差为 $r_{i}^{\\mathrm{int}}(x;\\theta) := \\mathcal{A}_{a_{i}}(u_{\\theta})(x) - f_{i}(x)$。将 $u_{\\theta}$ 的重参数化代入：\n    $$\n    r_{i}^{\\mathrm{int}}(x;\\theta) = \\mathcal{A}_{a_{i}}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x)\n    $$\n    问题陈述指明 $\\mathcal{A}_{a}$ 是一个线性微分算子。利用此性质，我们可以将 $\\mathcal{A}_{a_{i}}$ 分配到和上：\n    $$\n    r_{i}^{\\mathrm{int}}(x;\\theta) = \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x)\n    $$\n    因此，样本 $i$ 在损失函数中对应的项是：\n    $$\n    \\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n    $$\n\n2.  **边界残差项**：\n    对于配置点 $x \\in \\mathcal{X}_{i}^{\\mathrm{b}} \\subset \\partial\\Omega$，边界残差为 $r_{i}^{\\mathrm{b}}(x;\\theta) := u_{\\theta}(x) - g_{i}(x)$。将 $u_{\\theta}$ 的重参数化代入：\n    $$\n    r_{i}^{\\mathrm{b}}(x;\\theta) = \\big(L g_{i}\\big)(x) + b(x) \\, v_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x)\n    $$\n    现在我们应用提升算子 $L$ 和边界消失函数 $b$ 的定义性质。\n    -   根据 $L$ 的定义，对于任何 $x \\in \\partial\\Omega$，我们有 $(L g_{i})|_{\\partial\\Omega}(x) = g_{i}(x)$。\n    -   根据 $b$ 的定义，对于任何 $x \\in \\partial\\Omega$，我们有 $b|_{\\partial\\Omega}(x) = 0$。\n    由于配置点 $\\mathcal{X}_{i}^{\\mathrm{b}}$ 位于边界 $\\partial\\Omega$ 上，对于任何 $x \\in \\mathcal{X}_{i}^{\\mathrm{b}}$ 我们有：\n    $$\n    r_{i}^{\\mathrm{b}}(x;\\theta) = g_{i}(x) + 0 \\cdot v_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x) = 0\n    $$\n    对于所有边界配置点，边界残差恒等于零。因此，损失函数中整个边界项消失了：\n    $$\n    \\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2} = \\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} |0|^{2} = 0\n    $$\n    这是由重参数化带来的核心简化，因为通过构造，Dirichlet边界条件得到了满足。\n\n3.  **数据失配项**：\n    数据失配为 $m_{i}(\\theta) := \\mathcal{H}_{i}(u_{\\theta}) - y_{i}$。将 $u_{\\theta}$ 的重参数化代入：\n    $$\n    m_{i}(\\theta) = \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i}\n    $$\n    问题没有指明观测算子 $\\mathcal{H}_{i}$ 是线性的，因此该项无法进一步简化。样本 $i$ 在损失函数中对应的项是：\n    $$\n    \\lambda_{\\mathrm{data}} \\cdot \\big\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\big\\|_{2}^{2}\n    $$\n\n**组合提升后的经验风险**：\n结合这三项的结果，我们通过对剩余的非零分量求和并对 $M$ 个训练样本取平均，来构建提升后的经验风险 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$。边界项现在已不存在。\n$$\n\\mathcal{L}_{\\mathrm{lift}}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n+\n\\lambda_{\\mathrm{data}} \\cdot \\left\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\right\\|_{2}^{2}\n\\right]\n$$\n此表达式是提升后的经验风险的最终简化解析形式。",
            "answer": "$$\n\\boxed{\n\\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n+\n\\lambda_{\\mathrm{data}} \\cdot \\left\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\right\\|_{2}^{2}\n\\right]\n}\n$$"
        }
    ]
}