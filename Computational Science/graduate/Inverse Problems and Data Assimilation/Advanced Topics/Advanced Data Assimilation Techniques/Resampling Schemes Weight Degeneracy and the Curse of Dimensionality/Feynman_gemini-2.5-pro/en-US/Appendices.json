{
    "hands_on_practices": [
        {
            "introduction": "Before we can appreciate the solutions to weight degeneracy, we must first understand its origins. This practice explores the theoretical roots of why importance sampling, the engine of particle filters, struggles in high-dimensional spaces. By analyzing a canonical linear-Gaussian case, you will derive how the variance of importance weights explodes exponentially with dimension, providing a rigorous demonstration of the \"curse of dimensionality\" in action .",
            "id": "3417360",
            "problem": "Let $d \\in \\mathbb{N}$ be the dimension and consider importance sampling in a linear-Gaussian setting relevant to inverse problems and data assimilation. Let the proposal density be $p(x)$, where $x \\sim \\mathcal{N}(0, I_d)$ with $I_d$ the $d \\times d$ identity matrix. The target density is proportional to $\\exp\\!\\big(-\\frac{1}{2}\\|x - \\mu\\|^2\\big)$ with a shift vector $\\mu \\in \\mathbb{R}^d$ satisfying $\\|\\mu\\| = c\\sqrt{d}$ for a fixed constant $c > 0$ that does not depend on $d$. You may assume that the target is the Gaussian density $\\pi(x) = \\mathcal{N}(\\mu, I_d)$, so that the importance weight is $w(x) = \\pi(x)/p(x)$.\n\nStarting from the definitions of Gaussian densities, the Radon–Nikodym derivative between two Gaussian measures with the same covariance, and the definition of the coefficient of variation, derive the coefficient of variation of the importance weights,\n$$\n\\mathrm{CV}(w) \\equiv \\frac{\\sqrt{\\mathrm{Var}_p(w)}}{\\mathbb{E}_p[w]},\n$$\nas an explicit closed-form expression in terms of $c$ and $d$. Your derivation should use only fundamental properties of Gaussian distributions such as the moment generating function of a Gaussian linear form. Provide your final answer as a single closed-form expression in $c$ and $d$. No numerical evaluation or rounding is required.",
            "solution": "The problem statement has been validated and is determined to be a valid, self-contained, and scientifically grounded problem in statistical analysis, specifically related to the performance of importance sampling in high dimensions. We proceed with the derivation.\n\nLet $x \\in \\mathbb{R}^d$ be a random vector. The problem defines a proposal distribution $p(x)$ and a target distribution $\\pi(x)$.\nThe proposal distribution is a standard normal distribution in $d$ dimensions, $x \\sim \\mathcal{N}(0, I_d)$, where $I_d$ is the $d \\times d$ identity matrix. Its probability density function (PDF) is:\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} \\det(I_d)^{1/2}} \\exp\\left(-\\frac{1}{2} x^T I_d^{-1} x\\right) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x\\|^2\\right)\n$$\nThe target distribution is a multivariate normal distribution with mean $\\mu \\in \\mathbb{R}^d$ and covariance $I_d$, denoted as $\\pi(x) = \\mathcal{N}(\\mu, I_d)$. Its PDF is:\n$$\n\\pi(x) = \\frac{1}{(2\\pi)^{d/2} \\det(I_d)^{1/2}} \\exp\\left(-\\frac{1}{2} (x-\\mu)^T I_d^{-1} (x-\\mu)\\right) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x-\\mu\\|^2\\right)\n$$\nThe importance weight $w(x)$ is defined as the ratio of the target density to the proposal density, which is the Radon-Nikodym derivative $\\frac{d\\Pi}{dP}(x)$ where $\\Pi$ and $P$ are the measures corresponding to densities $\\pi$ and $p$.\n$$\nw(x) = \\frac{\\pi(x)}{p(x)} = \\frac{\\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x-\\mu\\|^2\\right)}{\\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x\\|^2\\right)} = \\exp\\left(-\\frac{1}{2} \\|x-\\mu\\|^2 + \\frac{1}{2} \\|x\\|^2\\right)\n$$\nWe expand the squared norm in the exponent: $\\|x-\\mu\\|^2 = (x-\\mu)^T(x-\\mu) = x^T x - 2x^T\\mu + \\mu^T\\mu = \\|x\\|^2 - 2x^T\\mu + \\|\\mu\\|^2$.\nSubstituting this into the expression for $w(x)$ gives:\n$$\nw(x) = \\exp\\left(-\\frac{1}{2} (\\|x\\|^2 - 2x^T\\mu + \\|\\mu\\|^2) + \\frac{1}{2} \\|x\\|^2\\right) = \\exp\\left(x^T\\mu - \\frac{1}{2}\\|\\mu\\|^2\\right)\n$$\nThe coefficient of variation is defined as $\\mathrm{CV}(w) = \\frac{\\sqrt{\\mathrm{Var}_p(w)}}{\\mathbb{E}_p[w]}$. We must compute the first and second moments of $w(x)$ with respect to the proposal distribution $p(x)$.\n\nThe first moment, or expectation, of the weights is:\n$$\n\\mathbb{E}_p[w(x)] = \\int_{\\mathbb{R}^d} w(x) p(x) dx = \\int_{\\mathbb{R}^d} \\frac{\\pi(x)}{p(x)} p(x) dx = \\int_{\\mathbb{R}^d} \\pi(x) dx = 1\n$$\nThis result is expected, as the integral of any valid probability density function over its support is equal to $1$.\n\nNext, we compute the second moment, $\\mathbb{E}_p[w(x)^2]$.\n$$\nw(x)^2 = \\left(\\exp\\left(x^T\\mu - \\frac{1}{2}\\|\\mu\\|^2\\right)\\right)^2 = \\exp\\left(2x^T\\mu - \\|\\mu\\|^2\\right)\n$$\nThe expectation of this quantity is:\n$$\n\\mathbb{E}_p[w(x)^2] = \\mathbb{E}_p\\left[\\exp\\left(2x^T\\mu - \\|\\mu\\|^2\\right)\\right] = \\exp\\left(-\\|\\mu\\|^2\\right) \\mathbb{E}_p\\left[\\exp\\left(2\\mu^T x\\right)\\right]\n$$\nThe term $\\mathbb{E}_p[\\exp(2\\mu^T x)]$ is the moment generating function (MGF) of the random variable $x$, denoted $M_x(t) = \\mathbb{E}_p[\\exp(t^T x)]$, evaluated at $t=2\\mu$.\nSince $x \\sim p = \\mathcal{N}(0, I_d)$, its MGF is given by:\n$$\nM_x(t) = \\exp\\left(t^T 0 + \\frac{1}{2}t^T I_d t\\right) = \\exp\\left(\\frac{1}{2}\\|t\\|^2\\right)\n$$\nEvaluating the MGF at $t=2\\mu$:\n$$\n\\mathbb{E}_p\\left[\\exp(2\\mu^T x)\\right] = M_x(2\\mu) = \\exp\\left(\\frac{1}{2}\\|2\\mu\\|^2\\right) = \\exp\\left(\\frac{1}{2} \\cdot 4 \\|\\mu\\|^2\\right) = \\exp(2\\|\\mu\\|^2)\n$$\nSubstituting this result back into the expression for the second moment:\n$$\n\\mathbb{E}_p[w(x)^2] = \\exp(-\\|\\mu\\|^2) \\cdot \\exp(2\\|\\mu\\|^2) = \\exp(\\|\\mu\\|^2)\n$$\nThe variance of the weights is then calculated as:\n$$\n\\mathrm{Var}_p(w) = \\mathbb{E}_p[w^2] - (\\mathbb{E}_p[w])^2 = \\exp(\\|\\mu\\|^2) - 1^2 = \\exp(\\|\\mu\\|^2) - 1\n$$\nNow we assemble the coefficient of variation:\n$$\n\\mathrm{CV}(w) = \\frac{\\sqrt{\\mathrm{Var}_p(w)}}{\\mathbb{E}_p[w]} = \\frac{\\sqrt{\\exp(\\|\\mu\\|^2) - 1}}{1} = \\sqrt{\\exp(\\|\\mu\\|^2) - 1}\n$$\nFinally, we use the given condition on the norm of the shift vector, $\\|\\mu\\| = c\\sqrt{d}$. This implies $\\|\\mu\\|^2 = (c\\sqrt{d})^2 = c^2 d$. Substituting this into our expression for the coefficient of variation yields the final result.\n$$\n\\mathrm{CV}(w) = \\sqrt{\\exp(c^2 d) - 1}\n$$\nThis expression is the closed-form solution in terms of the constant $c$ and the dimension $d$.",
            "answer": "$$\n\\boxed{\\sqrt{\\exp(c^2 d) - 1}}\n$$"
        },
        {
            "introduction": "The explosion in weight variance, as demonstrated previously, leads to a phenomenon known as weight collapse or degeneracy, where only a few particles hold meaningful weight. To manage this, we need a way to measure it. This exercise introduces the Effective Sample Size ($N_{\\text{eff}}$) as the standard diagnostic, allowing you to derive a closed-form expression that quantifies exactly how the \"effective\" number of particles dwindles as the weight distribution becomes more skewed .",
            "id": "3417298",
            "problem": "Consider a Sequential Monte Carlo (SMC) particle filter in a data assimilation setting for an inverse problem with $N$ particles. After assimilating a single observation, suppose the (nonnegative) importance weights are $\\{w_{i}\\}_{i=1}^{N}$ and that the weight vector exhibits a single dominant weight, with one entry $w_{1}$ larger than the other $N-1$ entries. Assume that the $N-1$ non-dominant weights are equal. Define normalized weights by $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$, define the dominance level by $\\rho = w_{1} / \\sum_{j=1}^{N} w_{j}$, and consider the Effective Sample Size (ESS), $N_{\\mathrm{eff}}$, as the standard degeneracy diagnostic in Sequential Monte Carlo.\n\nStarting from the canonical definition of degeneracy based on normalized weights used in particle filtering, derive a closed-form analytical expression for $N_{\\mathrm{eff}}$ as a function of the particle count $N$ and the dominance level $\\rho$, under the assumption that the $N-1$ non-dominant normalized weights are equal. Then, analyze the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$ by characterizing its monotonicity and extremum as $\\rho$ varies over $[0,1]$, and interpret the implications for weight degeneracy and the curse of dimensionality.\n\nExpress your final answer for $N_{\\mathrm{eff}}$ as a single closed-form analytical expression in terms of $N$ and $\\rho$. No numerical evaluation or rounding is required.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A Sequential Monte Carlo (SMC) particle filter is considered.\n- The number of particles is $N$.\n- The set of nonnegative importance weights is $\\{w_{i}\\}_{i=1}^{N}$.\n- The weight vector has a single dominant weight, $w_{1}$, which is larger than the other $N-1$ entries.\n- The $N-1$ non-dominant weights are equal to each other.\n- The normalized weights are defined as $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$.\n- The dominance level is defined as $\\rho = w_{1} / \\sum_{j=1}^{N} w_{j}$.\n- The Effective Sample Size, $N_{\\mathrm{eff}}$, is used as the degeneracy diagnostic.\n- The task is to derive a closed-form expression for $N_{\\mathrm{eff}}$ as a function of $N$ and $\\rho$ using the canonical definition of degeneracy.\n- The task includes analyzing the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$ by characterizing its monotonicity and extremum over $\\rho \\in [0,1]$.\n- The task requires an interpretation of the results in the context of weight degeneracy and the curse of dimensionality.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is scientifically sound. It is based on standard, fundamental concepts in the field of data assimilation and statistical signal processing, specifically Sequential Monte Carlo methods (particle filters). The definitions of normalized weights, Effective Sample Size, weight degeneracy, and the curse of dimensionality are all canonical within this domain. The canonical definition of $N_{\\mathrm{eff}}$ is $N_{\\mathrm{eff}} = (\\sum_{i=1}^{N} \\tilde{w}_{i}^{2})^{-1}$.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly relevant and directly formalizable. It poses a precise mathematical question within the specified topic.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and internally consistent. The provided definitions and assumptions are sufficient to derive the requested expression. The assumption that $w_1$ is the largest weight implies $\\tilde{w}_1 > \\tilde{w}_j$ for $j \\neq 1$. This means $\\rho > (1-\\rho)/(N-1)$, which simplifies to $\\rho > 1/N$. The request to analyze the function over $\\rho \\in [0,1]$ is a standard mathematical exercise to understand the function's behavior, even if the sub-interval $\\rho \\in [0, 1/N]$ contradicts the \"dominance\" assumption. This does not invalidate the problem but is a point to be noted during the analysis. The core derivation is unaffected.\n4.  **Unrealistic or Infeasible**: The scenario described is highly realistic. Weight collapse onto a single or few particles is a common and critical issue in practical applications of particle filters, particularly in high-dimensional systems.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The assumptions lead to a unique analytical solution. The terms are defined unambiguously.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a formal derivation and analytical reasoning based on first principles of the method.\n7.  **Outside Scientific Verifiability**: The problem is entirely mathematical and its solution is verifiable through logical deduction and calculation.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. We may proceed with the solution.\n\n### Derivation of the Expression for $N_{\\mathrm{eff}}$\n\nThe canonical definition of the Effective Sample Size, $N_{\\mathrm{eff}}$, based on the normalized importance weights $\\{\\tilde{w}_{i}\\}_{i=1}^{N}$ is given by:\n$$N_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}}$$\nThe sum of the normalized weights must equal $1$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i} = 1$$\nFrom the problem statement, the dominance level $\\rho$ is the normalized weight of the first particle:\n$$\\tilde{w}_{1} = \\rho$$\nThe problem also states that the remaining $N-1$ non-dominant weights are equal. Let us denote the value of these weights by $\\tilde{w}_{k}$ for $k \\in \\{2, 3, \\ldots, N\\}$. We can express $\\tilde{w}_{k}$ in terms of $\\rho$ and $N$.\nUsing the normalization condition:\n$$\\tilde{w}_{1} + \\sum_{k=2}^{N} \\tilde{w}_{k} = 1$$\nSubstituting $\\tilde{w}_{1} = \\rho$ and noting that the $N-1$ other weights are equal:\n$$\\rho + (N-1)\\tilde{w}_{k} = 1$$\nSolving for $\\tilde{w}_{k}$ (assuming $N > 1$, which is implicit in the problem's phrasing):\n$$\\tilde{w}_{k} = \\frac{1-\\rho}{N-1}$$\nNow, we can compute the sum of the squares of the normalized weights, $\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\tilde{w}_{1}^{2} + \\sum_{k=2}^{N} \\tilde{w}_{k}^{2}$$\nSubstituting the expressions for $\\tilde{w}_{1}$ and $\\tilde{w}_{k}$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\left( \\frac{1-\\rho}{N-1} \\right)^{2}$$\nSimplifying the expression:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\frac{(1-\\rho)^{2}}{(N-1)^{2}} = \\rho^{2} + \\frac{(1-\\rho)^{2}}{N-1}$$\nTo combine the terms, we find a common denominator:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\frac{(N-1)\\rho^{2} + (1-\\rho)^{2}}{N-1} = \\frac{(N-1)\\rho^{2} + (1 - 2\\rho + \\rho^{2})}{N-1}$$\n$$= \\frac{N\\rho^{2} - \\rho^{2} + 1 - 2\\rho + \\rho^{2}}{N-1} = \\frac{N\\rho^{2} - 2\\rho + 1}{N-1}$$\nFinally, we substitute this result back into the definition of $N_{\\mathrm{eff}}$:\n$$N_{\\mathrm{eff}} = \\frac{1}{\\frac{N\\rho^{2} - 2\\rho + 1}{N-1}} = \\frac{N-1}{N\\rho^{2} - 2\\rho + 1}$$\nThis is the closed-form analytical expression for $N_{\\mathrm{eff}}$ as a function of $N$ and $\\rho$.\n\n### Sensitivity Analysis and Interpretation\n\nTo analyze the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$, we compute the first derivative $\\frac{d N_{\\mathrm{eff}}}{d \\rho}$. Let the denominator be $f(\\rho) = N\\rho^{2} - 2\\rho + 1$.\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = \\frac{d}{d\\rho} \\left( (N-1) (N\\rho^{2} - 2\\rho + 1)^{-1} \\right)$$\nUsing the chain rule:\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = (N-1) (-1) (N\\rho^{2} - 2\\rho + 1)^{-2} (2N\\rho - 2)$$\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = -2(N-1) \\frac{N\\rho - 1}{(N\\rho^{2} - 2\\rho + 1)^{2}}$$\nThe denominator $(N\\rho^{2} - 2\\rho + 1)^{2}$ is always non-negative. Its roots are complex for $N>1$, so it is strictly positive. The sign of the derivative is therefore determined by the sign of the numerator's non-constant term, $-(N\\rho - 1)$.\n1.  If $N\\rho - 1 > 0 \\implies \\rho > 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho}  0$. $N_{\\mathrm{eff}}$ is a monotonically decreasing function of $\\rho$.\n2.  If $N\\rho - 1  0 \\implies \\rho  1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} > 0$. $N_{\\mathrm{eff}}$ is a monotonically increasing function of $\\rho$.\n3.  If $N\\rho - 1 = 0 \\implies \\rho = 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} = 0$. This indicates a critical point.\n\nThe analysis reveals an extremum at $\\rho = 1/N$. Since the derivative changes from positive to negative at this point, it is a local maximum. Let's evaluate $N_{\\mathrm{eff}}$ at the boundaries of the interval $[0,1]$ and at this critical point.\n\n-   **Maximum value**: At $\\rho = 1/N$,\n    $$N_{\\mathrm{eff}}\\left(\\rho = \\frac{1}{N}\\right) = \\frac{N-1}{N\\left(\\frac{1}{N}\\right)^{2} - 2\\left(\\frac{1}{N}\\right) + 1} = \\frac{N-1}{\\frac{1}{N} - \\frac{2}{N} + 1} = \\frac{N-1}{\\frac{N-1}{N}} = N$$\n    This case corresponds to all weights being equal, $\\tilde{w}_{i} = 1/N$ for all $i$, representing the absence of degeneracy. The effective sample size is equal to the actual number of particles.\n\n-   **Minimum value**: At $\\rho = 1$,\n    $$N_{\\mathrm{eff}}(\\rho = 1) = \\frac{N-1}{N(1)^{2} - 2(1) + 1} = \\frac{N-1}{N-1} = 1$$\n    This case corresponds to complete degeneracy, where one particle has all the weight ($\\tilde{w}_{1} = 1$) and all other particles have zero weight. The effective sample size collapses to $1$.\n\n-   At $\\rho = 0$:\n    $$N_{\\mathrm{eff}}(\\rho = 0) = \\frac{N-1}{N(0)^{2} - 2(0) + 1} = N-1$$\n    This corresponds to the dominant particle having zero weight, and the weight being uniformly distributed among the other $N-1$ particles.\n\nThe physically meaningful domain for $\\rho$, given that $\\tilde{w}_1$ is the largest weight, is $[\\frac{1}{N}, 1]$. Over this domain, $N_{\\mathrm{eff}}$ is a strictly monotonically decreasing function of $\\rho$.\n\n**Interpretation**:\n-   **Weight Degeneracy**: The derived expression and its analysis mathematically formalize the concept of weight degeneracy. As the weight distribution becomes more skewed (i.e., as $\\rho$ increases from its most uniform value of $1/N$ towards $1$), the effective number of particles that contribute to the approximation of the posterior distribution decreases drastically from $N$ to $1$. A low $N_{\\mathrm{eff}}$ is a signal that the particle representation is poor and a resampling step is necessary to mitigate the degeneracy by eliminating low-weight particles and replicating high-weight ones.\n-   **Curse of Dimensionality**: In high-dimensional state spaces, it is well-known that the likelihood function $p(\\text{observation}|\\text{state})$ tends to be highly concentrated in a small volume of the space. When updating the particle weights, it becomes exceedingly rare for particles to fall within this high-likelihood region. Consequently, after the update, most particles get near-zero weights, while one or a very few particles that happened to be in the right place capture nearly all the total weight. This scenario directly leads to a $\\rho$ value approaching $1$. Our analysis shows that as $\\rho \\to 1$, $N_{\\mathrm{eff}} \\to 1$ irrespective of the total number of particles $N$. This illustrates a key aspect of the curse of dimensionality for particle filters: simply increasing $N$ is not an efficient solution, as the number of particles required to adequately sample the high-likelihood region (and thus avoid weight collapse) typically grows exponentially with the dimension of the state space.",
            "answer": "$$\n\\boxed{\\frac{N-1}{N\\rho^2 - 2\\rho + 1}}\n$$"
        },
        {
            "introduction": "The theoretical challenges of high dimensions and weight degeneracy translate directly into practical, numerical hurdles. The extremely small likelihood values encountered can easily underflow standard floating-point representations, leading to catastrophic failures in weight calculation. This hands-on coding practice will guide you through implementing the \"log-sum-exp\" trick, a vital and universally used technique to ensure your particle filter's weighting step is numerically robust, even when facing the curse of dimensionality .",
            "id": "3417322",
            "problem": "Consider a sequential importance sampling step in a particle method for inverse problems and data assimilation, where the observation operator is identity and the observation noise is independent and identically distributed Gaussian with covariance $\\sigma^2 I_d$. Let there be $N$ particles $\\{x_i\\}_{i=1}^N \\subset \\mathbb{R}^d$ and an observation $y \\in \\mathbb{R}^d$. In a standard importance weighting, the unnormalized weight for particle $i$ is proportional to the likelihood $p(y \\mid x_i)$, and the normalized weights are obtained by dividing by their sum. In high dimension $d$, direct computation of $\\exp(\\cdot)$ in the likelihood can suffer floating-point underflow, causing normalization errors that bias Effective Sample Size (ESS) estimates and resampling decisions.\n\nStarting from the fundamental base that the additive Gaussian likelihood for independent components is $p(y \\mid x) \\propto \\exp\\left(-\\tfrac{1}{2} \\lVert y - x \\rVert^2 / \\sigma^2\\right)$ for identity observation operator, derive an approach to stabilize weight computation using log-weights. Explain why adding a constant shift to all log-weights leaves normalized weights invariant, and show how choosing the maximum log-weight as the shift mitigates underflow. Define and compute the Effective Sample Size (ESS), the resampling decision under a threshold rule, and quantify normalization error.\n\nDefinitions to use:\n- Effective Sample Size (ESS): $\\mathrm{ESS}(w) = \\left(\\sum_{i=1}^N w_i^2\\right)^{-1}$ for normalized weights $w_i \\ge 0$ with $\\sum_{i=1}^N w_i = 1$.\n- Resampling decision rule: resample if $\\mathrm{ESS}(w)/N  \\tau$, where $\\tau \\in (0,1]$ is a threshold.\n- Total Variation (TV) distance between two discrete distributions $w$ and $v$: $\\mathrm{TV}(w,v) = \\tfrac{1}{2} \\sum_{i=1}^N |w_i - v_i|$.\n\nImplement the following two weighting schemes:\n- Naive scheme: compute raw weights by $w_i^{\\text{raw}} = \\exp\\left(-\\tfrac{1}{2} \\lVert y - x_i \\rVert^2 / \\sigma^2\\right)$, normalize by $w_i = w_i^{\\text{raw}} / \\sum_{j=1}^N w_j^{\\text{raw}}$. If $\\sum_{j=1}^N w_j^{\\text{raw}} = 0$ due to underflow, set all normalized weights to $0$ and $\\mathrm{ESS} = 0$.\n- Log-weight stabilized scheme: compute log-weights $\\ell_i = -\\tfrac{1}{2} \\lVert y - x_i \\rVert^2 / \\sigma^2$, then shift by $m = \\max_i \\ell_i$ and exponentiate the differences to obtain $u_i = \\exp(\\ell_i - m)$, normalize by $w_i = u_i / \\sum_{j=1}^N u_j$.\n\nFor each test case, report:\n- $E_{\\text{naive}}$: the ESS under the naive scheme,\n- $E_{\\text{log}}$: the ESS under the log-weight stabilized scheme,\n- $R_{\\text{naive}}$: the boolean resampling decision under the naive scheme,\n- $R_{\\text{log}}$: the boolean resampling decision under the log-weight stabilized scheme,\n- $T$: the Total Variation distance $\\mathrm{TV}(w^{\\text{naive}}, w^{\\text{log}})$. If naive normalization fails (sum of raw weights equals zero), define $T = 1$.\n\nUse the following deterministic test suite of parameter values, with particles and observations generated as specified. For each case, draw particles from the standard normal distribution unless otherwise noted, and set $y = s \\cdot \\mathbf{1}_d$ where $\\mathbf{1}_d$ is the $d$-dimensional vector of ones and $s$ is the offset. Use a fixed random seed per case as given. In the peaked case, set the first particle exactly equal to $y$ and set all other particles to $y + 5 \\cdot \\mathbf{1}_d$.\n- Case $1$ (happy path): $N=500$, $d=5$, $\\sigma=1.0$, $\\tau=0.5$, seed $=0$, offset $s=1.0$.\n- Case $2$ (high dimension underflow): $N=1000$, $d=500$, $\\sigma=0.2$, $\\tau=0.5$, seed $=1$, offset $s=2.0$.\n- Case $3$ (extreme underflow boundary): $N=800$, $d=2000$, $\\sigma=1.0$, $\\tau=0.5$, seed $=2$, offset $s=10.0$.\n- Case $4$ (near-equal weights): $N=300$, $d=50$, $\\sigma=10^6$, $\\tau=0.5$, seed $=3$, offset $s=5.0$.\n- Case $5$ (peaked weight): $N=1000$, $d=100$, $\\sigma=0.1$, $\\tau=0.5$, seed $=4$, offset $s=0.0$; first particle equals $y$, others equal $y + 5 \\cdot \\mathbf{1}_d$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-test-case result is itself a list $[E_{\\text{naive}}, E_{\\text{log}}, R_{\\text{naive}}, R_{\\text{log}}, T]$. For example, the output must be of the form\n$[[E_{\\text{naive}}^{(1)},E_{\\text{log}}^{(1)},R_{\\text{naive}}^{(1)},R_{\\text{log}}^{(1)},T^{(1)}],\\dots,[E_{\\text{naive}}^{(5)},E_{\\text{log}}^{(5)},R_{\\text{naive}}^{(5)},R_{\\text{log}}^{(5)},T^{(5)}]]$.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of Bayesian inference and numerical methods for particle filters, well-posed with a clear objective and deterministic test cases, and formulated with objective, unambiguous language. The core of the problem addresses a genuine and critical issue in computational statistics—numerical stability in high-dimensional spaces.\n\nThe task is to analyze and implement two methods for computing importance weights in a particle filter, demonstrating the superiority of a log-space stabilization technique over a naive approach, particularly in high-dimensional settings where floating-point underflow is a significant concern.\n\nThe foundation of the weighting step is the likelihood of an observation $y \\in \\mathbb{R}^d$ given a state (particle) $x \\in \\mathbb{R}^d$. With an identity observation operator and i.i.d. Gaussian noise with variance $\\sigma^2$, the likelihood function is given by:\n$$\np(y \\mid x) = \\prod_{j=1}^d \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_j - x_j)^2}{2\\sigma^2}\\right)\n$$\nIn importance sampling, we only need the weights up to a normalization constant. Thus, the unnormalized weight $w^{\\text{raw}}$ for a particle $x_i$ is proportional to the likelihood:\n$$\nw_i^{\\text{raw}} \\propto p(y \\mid x_i) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{j=1}^d (y_j - x_{i,j})^2\\right) = \\exp\\left(-\\frac{\\lVert y - x_i \\rVert^2}{2\\sigma^2}\\right)\n$$\nThe normalized weights $w_i$ are then computed as $w_i = w_i^{\\text{raw}} / \\sum_{k=1}^N w_k^{\\text{raw}}$.\n\n**The Problem of Underflow (Curse of Dimensionality)**\n\nIn high-dimensional spaces (large $d$), the squared Euclidean distance $\\lVert y - x_i \\rVert^2$ between a random particle $x_i$ and the observation $y$ tends to be large. This phenomenon is a manifestation of the curse of dimensionality. Consequently, the argument of the exponential function, $-\\frac{\\lVert y - x_i \\rVert^2}{2\\sigma^2}$, becomes a large negative number. For standard double-precision floating-point numbers, $\\exp(z)$ underflows to $0.0$ for $z \\lesssim -709$. When this occurs for all particles, all raw weights $w_i^{\\text{raw}}$ become zero. The sum of the weights becomes zero, making normalization impossible and leading to a catastrophic failure of the algorithm. All information about the relative quality of the particles is lost.\n\n**Log-Weight Stabilization**\n\nTo circumvent this numerical instability, we can perform the calculations in logarithmic space. The unnormalized log-weight, $\\ell_i$, is defined as the natural logarithm of the unnormalized weight:\n$$\n\\ell_i = \\ln(w_i^{\\text{raw}}) = -\\frac{\\lVert y - x_i \\rVert^2}{2\\sigma^2}\n$$\nThe normalized weights $w_i$ can be expressed in terms of the log-weights:\n$$\nw_i = \\frac{\\exp(\\ell_i)}{\\sum_{k=1}^N \\exp(\\ell_k)}\n$$\nThis expression is still susceptible to underflow in the numerator and denominator. However, we can use a property of this expression to improve its stability. Let us add an arbitrary constant $C$ to each log-weight. The new weights, $w'_i$, are:\n$$\nw'_i = \\frac{\\exp(\\ell_i + C)}{\\sum_{k=1}^N \\exp(\\ell_k + C)} = \\frac{\\exp(\\ell_i) \\exp(C)}{\\sum_{k=1}^N \\exp(\\ell_k)\\exp(C)} = \\frac{\\exp(C) \\exp(\\ell_i)}{\\exp(C) \\sum_{k=1}^N \\exp(\\ell_k)} = \\frac{\\exp(\\ell_i)}{\\sum_{k=1}^N \\exp(\\ell_k)} = w_i\n$$\nThis demonstrates that shifting all log-weights by a constant does not change the final normalized weights. To maximize numerical stability, we choose the shift to be $C = -m$, where $m = \\max_k \\ell_k$. The shifted log-weights are $\\ell_i' = \\ell_i - m$. The largest shifted log-weight is now $\\max_k \\ell_k' = \\max_k (\\ell_k - m) = m - m = 0$. All other shifted log-weights are non-positive.\nThe stabilized, unnormalized weights $u_i$ are then computed as:\n$$\nu_i = \\exp(\\ell_i - m)\n$$\nThe largest value of $u_i$ is $\\exp(0) = 1$, which prevents overflow. Since at least one $u_i$ is equal to $1$, their sum $\\sum_k u_k$ is guaranteed to be at least $1$, thus preventing a zero-sum denominator and failure due to underflow. This technique is commonly known as the log-sum-exp trick.\n\n**Metrics for Evaluation**\n\nThe problem requires the calculation of several quantities to compare the naive and stabilized schemes.\n\n1.  **Effective Sample Size (ESS)**: Measures the degeneracy of the particle weights. A value close to $N$ indicates all particles have similar weights, while a value close to $1$ indicates that one particle has a weight near $1$ and all others are near $0$. It is defined as:\n    $$\n    \\mathrm{ESS}(w) = \\frac{1}{\\sum_{i=1}^N w_i^2}\n    $$\n    For the naive scheme where all weights underflow to $0$, the sum in the denominator is $0$, leading to a division-by-zero error. The problem defines $\\mathrm{ESS}=0$ in this failure case.\n\n2.  **Resampling Decision**: A common practice in particle filtering is to resample the particles when the ESS falls below a certain fraction of the total number of particles, $N$. The rule is:\n    $$\n    \\text{Resample if } \\frac{\\mathrm{ESS}(w)}{N}  \\tau\n    $$\n    where $\\tau \\in (0,1]$ is a predefined threshold. If the naive scheme fails and $\\mathrm{ESS}=0$, this condition becomes $0  \\tau$, which is true for the given $\\tau=0.5$. Thus, failure of the naive scheme correctly triggers a resampling decision.\n\n3.  **Total Variation (TV) Distance**: Measures the difference between the two resulting probability distributions of weights, $w^{\\text{naive}}$ and $w^{\\text{log}}$.\n    $$\n    \\mathrm{TV}(w^{\\text{naive}}, w^{\\text{log}}) = \\frac{1}{2} \\sum_{i=1}^N |w_i^{\\text{naive}} - w_i^{\\text{log}}|\n    $$\n    A TV distance of $0$ means the distributions are identical, while a distance of $1$ means they are maximally different (i.e., have disjoint support). In the case of naive scheme failure, the problem specifies to set $T=1$. This is a sensible convention, as the \"distribution\" of all zeros is maximally distant from any valid probability distribution.\n\nThe implementation will proceed by first generating particles and observations for each test case according to the specified parameters. Then, for each case, it will compute the weights and derive the required metrics ($E_{\\text{naive}}, E_{\\text{log}}, R_{\\text{naive}}, R_{\\text{log}}, T$) using both the naive and the log-stabilized methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the particle weighting problem for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {'N': 500, 'd': 5, 'sigma': 1.0, 'tau': 0.5, 'seed': 0, 's': 1.0, 'peaked': False},\n        # Case 2 (high dimension underflow)\n        {'N': 1000, 'd': 500, 'sigma': 0.2, 'tau': 0.5, 'seed': 1, 's': 2.0, 'peaked': False},\n        # Case 3 (extreme underflow boundary)\n        {'N': 800, 'd': 2000, 'sigma': 1.0, 'tau': 0.5, 'seed': 2, 's': 10.0, 'peaked': False},\n        # Case 4 (near-equal weights)\n        {'N': 300, 'd': 50, 'sigma': 1e6, 'tau': 0.5, 'seed': 3, 's': 5.0, 'peaked': False},\n        # Case 5 (peaked weight)\n        {'N': 1000, 'd': 100, 'sigma': 0.1, 'tau': 0.5, 'seed': 4, 's': 0.0, 'peaked': True},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        d = case['d']\n        sigma = case['sigma']\n        tau = case['tau']\n        seed = case['seed']\n        s = case['s']\n        is_peaked = case['peaked']\n\n        # --- Generate particles and observation ---\n        y = s * np.ones(d, dtype=np.float64)\n        \n        if is_peaked:\n            particles = np.full((N, d), y + 5.0 * np.ones(d), dtype=np.float64)\n            particles[0, :] = y\n        else:\n            rng = np.random.default_rng(seed)\n            particles = rng.standard_normal(size=(N, d), dtype=np.float64)\n\n        # --- Base calculation ---\n        # Calculate squared L2 distance for all particles\n        sq_dists = np.sum((particles - y)**2, axis=1)\n        \n        # Calculate log-weights (common for both schemes)\n        log_weights = -0.5 * sq_dists / (sigma**2)\n\n        # --- Naive Scheme ---\n        w_naive = np.zeros(N, dtype=np.float64)\n        e_naive = 0.0\n        \n        raw_weights = np.exp(log_weights)\n        sum_raw_weights = np.sum(raw_weights)\n\n        naive_failed = (sum_raw_weights == 0.0)\n\n        if not naive_failed:\n            w_naive = raw_weights / sum_raw_weights\n            sum_sq_w_naive = np.sum(w_naive**2)\n            if sum_sq_w_naive > 0:\n                e_naive = 1.0 / sum_sq_w_naive\n        \n        # Resampling decision: if ESS is 0, ESS/N = 0, which is  tau\n        r_naive = (e_naive / N)  tau\n\n        # --- Log-Weight Stabilized Scheme ---\n        max_log_weight = np.max(log_weights)\n        shifted_log_weights = log_weights - max_log_weight\n        u_weights = np.exp(shifted_log_weights)\n        sum_u_weights = np.sum(u_weights)\n        \n        w_log = u_weights / sum_u_weights\n        \n        sum_sq_w_log = np.sum(w_log**2)\n        e_log = 0.0\n        if sum_sq_w_log > 0:\n            e_log = 1.0 / sum_sq_w_log\n            \n        r_log = (e_log / N)  tau\n\n        # --- Total Variation Distance ---\n        if naive_failed:\n            t_dist = 1.0\n        else:\n            t_dist = 0.5 * np.sum(np.abs(w_naive - w_log))\n            \n        results.append([e_naive, e_log, r_naive, r_log, t_dist])\n\n    # --- Format output string ---\n    # The output format must match exactly: [[v1,v2,...],[v1,v2,...]] with no spaces.\n    # Standard str(list) adds spaces, so we build the string manually.\n    sub_results_str = []\n    for r in results:\n        # Convert boolean to required 'True'/'False' string literal\n        r_str = [f\"{val}\" for val in r]\n        sub_results_str.append(f\"[{','.join(r_str)}]\")\n    \n    final_output_str = f\"[{','.join(sub_results_str)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}