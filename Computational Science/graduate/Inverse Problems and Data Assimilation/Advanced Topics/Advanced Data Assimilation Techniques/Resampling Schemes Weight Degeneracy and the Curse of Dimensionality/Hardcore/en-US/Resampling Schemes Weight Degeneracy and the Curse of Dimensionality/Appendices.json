{
    "hands_on_practices": [
        {
            "introduction": "Before we can address weight degeneracy, we must be able to measure it. The Effective Sample Size, or $N_{\\mathrm{eff}}$, is the standard diagnostic for quantifying the health of a particle ensemble. This exercise invites you to derive how $N_{\\mathrm{eff}}$ behaves in a simplified but highly illustrative scenario of weight collapse, helping you build a strong intuition for how the particle system's quality degrades as a single particle's weight begins to dominate. ",
            "id": "3417298",
            "problem": "Consider a Sequential Monte Carlo (SMC) particle filter in a data assimilation setting for an inverse problem with $N$ particles. After assimilating a single observation, suppose the (nonnegative) importance weights are $\\{w_{i}\\}_{i=1}^{N}$ and that the weight vector exhibits a single dominant weight, with one entry $w_{1}$ larger than the other $N-1$ entries. Assume that the $N-1$ non-dominant weights are equal. Define normalized weights by $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$, define the dominance level by $\\rho = w_{1} / \\sum_{j=1}^{N} w_{j}$, and consider the Effective Sample Size (ESS), $N_{\\mathrm{eff}}$, as the standard degeneracy diagnostic in Sequential Monte Carlo.\n\nStarting from the canonical definition of degeneracy based on normalized weights used in particle filtering, derive a closed-form analytical expression for $N_{\\mathrm{eff}}$ as a function of the particle count $N$ and the dominance level $\\rho$, under the assumption that the $N-1$ non-dominant normalized weights are equal. Then, analyze the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$ by characterizing its monotonicity and extremum as $\\rho$ varies over $[0,1]$, and interpret the implications for weight degeneracy and the curse of dimensionality.\n\nExpress your final answer for $N_{\\mathrm{eff}}$ as a single closed-form analytical expression in terms of $N$ and $\\rho$. No numerical evaluation or rounding is required.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A Sequential Monte Carlo (SMC) particle filter is considered.\n- The number of particles is $N$.\n- The set of nonnegative importance weights is $\\{w_{i}\\}_{i=1}^{N}$.\n- The weight vector has a single dominant weight, $w_{1}$, which is larger than the other $N-1$ entries.\n- The $N-1$ non-dominant weights are equal to each other.\n- The normalized weights are defined as $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$.\n- The dominance level is defined as $\\rho = w_{1} / \\sum_{j=1}^{N} w_{j}$.\n- The Effective Sample Size, $N_{\\mathrm{eff}}$, is used as the degeneracy diagnostic.\n- The task is to derive a closed-form expression for $N_{\\mathrm{eff}}$ as a function of $N$ and $\\rho$ using the canonical definition of degeneracy.\n- The task includes analyzing the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$ by characterizing its monotonicity and extremum over $\\rho \\in [0,1]$.\n- The task requires an interpretation of the results in the context of weight degeneracy and the curse of dimensionality.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is scientifically sound. It is based on standard, fundamental concepts in the field of data assimilation and statistical signal processing, specifically Sequential Monte Carlo methods (particle filters). The definitions of normalized weights, Effective Sample Size, weight degeneracy, and the curse of dimensionality are all canonical within this domain. The canonical definition of $N_{\\mathrm{eff}}$ is $N_{\\mathrm{eff}} = (\\sum_{i=1}^{N} \\tilde{w}_{i}^{2})^{-1}$.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly relevant and directly formalizable. It poses a precise mathematical question within the specified topic.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and internally consistent. The provided definitions and assumptions are sufficient to derive the requested expression. The assumption that $w_1$ is the largest weight implies $\\tilde{w}_1 > \\tilde{w}_j$ for $j \\neq 1$. This means $\\rho > (1-\\rho)/(N-1)$, which simplifies to $\\rho > 1/N$. The request to analyze the function over $\\rho \\in [0,1]$ is a standard mathematical exercise to understand the function's behavior, even if the sub-interval $\\rho \\in [0, 1/N]$ contradicts the \"dominance\" assumption. This does not invalidate the problem but is a point to be noted during the analysis. The core derivation is unaffected.\n4.  **Unrealistic or Infeasible**: The scenario described is highly realistic. Weight collapse onto a single or few particles is a common and critical issue in practical applications of particle filters, particularly in high-dimensional systems.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The assumptions lead to a unique analytical solution. The terms are defined unambiguously.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a formal derivation and analytical reasoning based on first principles of the method.\n7.  **Outside Scientific Verifiability**: The problem is entirely mathematical and its solution is verifiable through logical deduction and calculation.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. We may proceed with the solution.\n\n### Derivation of the Expression for $N_{\\mathrm{eff}}$\n\nThe canonical definition of the Effective Sample Size, $N_{\\mathrm{eff}}$, based on the normalized importance weights $\\{\\tilde{w}_{i}\\}_{i=1}^{N}$ is given by:\n$$N_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}}$$\nThe sum of the normalized weights must equal $1$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i} = 1$$\nFrom the problem statement, the dominance level $\\rho$ is the normalized weight of the first particle:\n$$\\tilde{w}_{1} = \\rho$$\nThe problem also states that the remaining $N-1$ non-dominant weights are equal. Let us denote the value of these weights by $\\tilde{w}_{k}$ for $k \\in \\{2, 3, \\ldots, N\\}$. We can express $\\tilde{w}_{k}$ in terms of $\\rho$ and $N$.\nUsing the normalization condition:\n$$\\tilde{w}_{1} + \\sum_{k=2}^{N} \\tilde{w}_{k} = 1$$\nSubstituting $\\tilde{w}_{1} = \\rho$ and noting that the $N-1$ other weights are equal:\n$$\\rho + (N-1)\\tilde{w}_{k} = 1$$\nSolving for $\\tilde{w}_{k}$ (assuming $N > 1$, which is implicit in the problem's phrasing):\n$$\\tilde{w}_{k} = \\frac{1-\\rho}{N-1}$$\nNow, we can compute the sum of the squares of the normalized weights, $\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\tilde{w}_{1}^{2} + \\sum_{k=2}^{N} \\tilde{w}_{k}^{2}$$\nSubstituting the expressions for $\\tilde{w}_{1}$ and $\\tilde{w}_{k}$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\left( \\frac{1-\\rho}{N-1} \\right)^{2}$$\nSimplifying the expression:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\frac{(1-\\rho)^{2}}{(N-1)^{2}} = \\rho^{2} + \\frac{(1-\\rho)^{2}}{N-1}$$\nTo combine the terms, we find a common denominator:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\frac{(N-1)\\rho^{2} + (1-\\rho)^{2}}{N-1} = \\frac{(N-1)\\rho^{2} + (1 - 2\\rho + \\rho^{2})}{N-1}$$\n$$= \\frac{N\\rho^{2} - \\rho^{2} + 1 - 2\\rho + \\rho^{2}}{N-1} = \\frac{N\\rho^{2} - 2\\rho + 1}{N-1}$$\nFinally, we substitute this result back into the definition of $N_{\\mathrm{eff}}$:\n$$N_{\\mathrm{eff}} = \\frac{1}{\\frac{N\\rho^{2} - 2\\rho + 1}{N-1}} = \\frac{N-1}{N\\rho^{2} - 2\\rho + 1}$$\nThis is the closed-form analytical expression for $N_{\\mathrm{eff}}$ as a function of $N$ and $\\rho$.\n\n### Sensitivity Analysis and Interpretation\n\nTo analyze the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$, we compute the first derivative $\\frac{d N_{\\mathrm{eff}}}{d \\rho}$. Let the denominator be $f(\\rho) = N\\rho^{2} - 2\\rho + 1$.\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = \\frac{d}{d\\rho} \\left( (N-1) (N\\rho^{2} - 2\\rho + 1)^{-1} \\right)$$\nUsing the chain rule:\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = (N-1) (-1) (N\\rho^{2} - 2\\rho + 1)^{-2} (2N\\rho - 2)$$\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = -2(N-1) \\frac{N\\rho - 1}{(N\\rho^{2} - 2\\rho + 1)^{2}}$$\nThe denominator $(N\\rho^{2} - 2\\rho + 1)^{2}$ is always non-negative. Its roots are complex for $N>1$, so it is strictly positive. The sign of the derivative is therefore determined by the sign of the numerator's non-constant term, $-(N\\rho - 1)$.\n1.  If $N\\rho - 1 > 0 \\implies \\rho > 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho}  0$. $N_{\\mathrm{eff}}$ is a monotonically decreasing function of $\\rho$.\n2.  If $N\\rho - 1  0 \\implies \\rho  1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} > 0$. $N_{\\mathrm{eff}}$ is a monotonically increasing function of $\\rho$.\n3.  If $N\\rho - 1 = 0 \\implies \\rho = 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} = 0$. This indicates a critical point.\n\nThe analysis reveals an extremum at $\\rho = 1/N$. Since the derivative changes from positive to negative at this point, it is a local maximum. Let's evaluate $N_{\\mathrm{eff}}$ at the boundaries of the interval $[0,1]$ and at this critical point.\n\n-   **Maximum value**: At $\\rho = 1/N$,\n    $$N_{\\mathrm{eff}}\\left(\\rho = \\frac{1}{N}\\right) = \\frac{N-1}{N\\left(\\frac{1}{N}\\right)^{2} - 2\\left(\\frac{1}{N}\\right) + 1} = \\frac{N-1}{\\frac{1}{N} - \\frac{2}{N} + 1} = \\frac{N-1}{\\frac{N-1}{N}} = N$$\n    This case corresponds to all weights being equal, $\\tilde{w}_{i} = 1/N$ for all $i$, representing the absence of degeneracy. The effective sample size is equal to the actual number of particles.\n\n-   **Minimum value**: At $\\rho = 1$,\n    $$N_{\\mathrm{eff}}(\\rho = 1) = \\frac{N-1}{N(1)^{2} - 2(1) + 1} = \\frac{N-1}{N-1} = 1$$\n    This case corresponds to complete degeneracy, where one particle has all the weight ($\\tilde{w}_{1} = 1$) and all other particles have zero weight. The effective sample size collapses to $1$.\n\n-   At $\\rho = 0$:\n    $$N_{\\mathrm{eff}}(\\rho = 0) = \\frac{N-1}{N(0)^{2} - 2(0) + 1} = N-1$$\n    This corresponds to the dominant particle having zero weight, and the weight being uniformly distributed among the other $N-1$ particles.\n\nThe physically meaningful domain for $\\rho$, given that $\\tilde{w}_1$ is the largest weight, is $[\\frac{1}{N}, 1]$. Over this domain, $N_{\\mathrm{eff}}$ is a strictly monotonically decreasing function of $\\rho$.\n\n**Interpretation**:\n-   **Weight Degeneracy**: The derived expression and its analysis mathematically formalize the concept of weight degeneracy. As the weight distribution becomes more skewed (i.e., as $\\rho$ increases from its most uniform value of $1/N$ towards $1$), the effective number of particles that contribute to the approximation of the posterior distribution decreases drastically from $N$ to $1$. A low $N_{\\mathrm{eff}}$ is a signal that the particle representation is poor and a resampling step is necessary to mitigate the degeneracy by eliminating low-weight particles and replicating high-weight ones.\n-   **Curse of Dimensionality**: In high-dimensional state spaces, it is well-known that the likelihood function $p(\\text{observation}|\\text{state})$ tends to be highly concentrated in a small volume of the space. When updating the particle weights, it becomes exceedingly rare for particles to fall within this high-likelihood region. Consequently, after the update, most particles get near-zero weights, while one or a very few particles that happened to be in the right place capture nearly all the total weight. This scenario directly leads to a $\\rho$ value approaching $1$. Our analysis shows that as $\\rho \\to 1$, $N_{\\mathrm{eff}} \\to 1$ irrespective of the total number of particles $N$. This illustrates a key aspect of the curse of dimensionality for particle filters: simply increasing $N$ is not an efficient solution, as the number of particles required to adequately sample the high-likelihood region (and thus avoid weight collapse) typically grows exponentially with the dimension of the state space.",
            "answer": "$$\n\\boxed{\\frac{N-1}{N\\rho^2 - 2\\rho + 1}}\n$$"
        },
        {
            "introduction": "Once a low Effective Sample Size signals that the particle weights have degenerated, a resampling step is necessary to reset the weights and focus computational resources. This practice takes you inside the mechanics of systematic resampling, a widely-used and efficient algorithm for this purpose. By manually tracing the selection of particles and analyzing their arrangement, you will uncover the subtle but important properties that distinguish this scheme from more naive approaches and understand its role in preserving particle diversity. ",
            "id": "3417312",
            "problem": "Consider a Sequential Monte Carlo (SMC) particle method applied to a Bayesian inverse problem in data assimilation with $N$ particles and normalized importance weights $\\{w_i\\}_{i=1}^N$ that sum to $1$. The particles are indexed in ascending order of a scalar state coordinate so that neighboring indices correspond to neighboring states. The SMC algorithm employs the systematic resampling scheme, defined as follows: draw a single $U$ from the uniform distribution on $[0, 1/N)$ and form the threshold sequence $t_k = U + (k-1)/N$ for $k = 1, \\dots, N$. Let $c_j = \\sum_{i=1}^j w_i$ denote the cumulative sum of weights. The resampled ancestor index at position $k$ is defined by $a_k = \\min\\{j \\in \\{1,\\dots,N\\} : c_j \\ge t_k\\}$.\n\nYou are given $N = 10$, the normalized weights\n$$\n(w_1, \\dots, w_{10}) = (0.24, 0.01, 0.01, 0.18, 0.18, 0.20, 0.05, 0.03, 0.05, 0.05),\n$$\nand a fixed $U = 0.07$. Compute the full vector of selection indices $(a_1, \\dots, a_{10})$ according to the systematic resampling definition above.\n\nTo quantify how coherent the neighbor selections are in index space (reflecting the well-known correlation structure of systematic resampling), define the neighbor-coherence index\n$$\n\\kappa = \\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\left(|a_{k+1} - a_k| \\le 1\\right),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. Provide the final answer as the value of $\\kappa$. Express the final answer as a reduced fraction. In your derivation, connect this notion of neighbor coherence to particle diversity in the presence of weight degeneracy and discuss its implication for the curse of dimensionality in high-dimensional inverse problems. The final answer must be a single number without units and must be expressed as a reduced fraction.",
            "solution": "The problem is first validated against the required criteria. The problem statement is self-contained, scientifically grounded in the theory of Sequential Monte Carlo methods, and algorithmically well-posed. All necessary data are provided, including the number of particles $N=10$, the normalized weights $\\{w_i\\}$, and the specific value of the random draw $U=0.07$. The sum of the provided weights is indeed $0.24 + 0.01 + 0.01 + 0.18 + 0.18 + 0.20 + 0.05 + 0.03 + 0.05 + 0.05 = 1.00$, confirming their normalization. The value $U=0.07$ is consistent with the required distribution $\\mathcal{U}[0, 1/N) = \\mathcal{U}[0, 0.1)$. The definitions of systematic resampling and the neighbor-coherence index $\\kappa$ are precise and allow for a unique solution. The problem is therefore deemed valid and a full solution is warranted.\n\nThe solution proceeds in three steps: first, we compute the ancestor index vector $(a_1, \\dots, a_{10})$; second, we use this vector to compute the neighbor-coherence index $\\kappa$; finally, we discuss the broader implications of this result.\n\nStep 1: Computation of the ancestor index vector $(a_1, \\dots, a_{10})$.\n\nAccording to the definition of systematic resampling, we must first compute the cumulative sum of weights, $c_j = \\sum_{i=1}^j w_i$, and the sequence of thresholds, $t_k = U + (k-1)/N$.\n\nThe given weights are:\n$$ (w_1, \\dots, w_{10}) = (0.24, 0.01, 0.01, 0.18, 0.18, 0.20, 0.05, 0.03, 0.05, 0.05) $$\nThe cumulative sums $c_j$ are:\n\\begin{align*}\nc_1 = 0.24 \\\\\nc_2 = 0.24 + 0.01 = 0.25 \\\\\nc_3 = 0.25 + 0.01 = 0.26 \\\\\nc_4 = 0.26 + 0.18 = 0.44 \\\\\nc_5 = 0.44 + 0.18 = 0.62 \\\\\nc_6 = 0.62 + 0.20 = 0.82 \\\\\nc_7 = 0.82 + 0.05 = 0.87 \\\\\nc_8 = 0.87 + 0.03 = 0.90 \\\\\nc_9 = 0.90 + 0.05 = 0.95 \\\\\nc_{10} = 0.95 + 0.05 = 1.00\n\\end{align*}\n\nThe number of particles is $N=10$ and the random draw is $U=0.07$. The threshold sequence $t_k = 0.07 + (k-1)/10$ for $k=1, \\dots, 10$ is:\n\\begin{align*}\nt_1 = 0.07 + 0.0 = 0.07 \\\\\nt_2 = 0.07 + 0.1 = 0.17 \\\\\nt_3 = 0.07 + 0.2 = 0.27 \\\\\nt_4 = 0.07 + 0.3 = 0.37 \\\\\nt_5 = 0.07 + 0.4 = 0.47 \\\\\nt_6 = 0.07 + 0.5 = 0.57 \\\\\nt_7 = 0.07 + 0.6 = 0.67 \\\\\nt_8 = 0.07 + 0.7 = 0.77 \\\\\nt_9 = 0.07 + 0.8 = 0.87 \\\\\nt_{10} = 0.07 + 0.9 = 0.97\n\\end{align*}\n\nThe ancestor index $a_k$ is the smallest index $j$ such that $c_j \\ge t_k$. We find each $a_k$ by comparing $t_k$ with the sequence of cumulative weights $c_j$:\n\\begin{itemize}\n    \\item For $t_1 = 0.07$: The first cumulative weight $c_1=0.24$ is greater than $0.07$. Thus, $a_1=1$.\n    \\item For $t_2 = 0.17$: The first cumulative weight $c_1=0.24$ is greater than $0.17$. Thus, $a_2=1$.\n    \\item For $t_3 = 0.27$: $c_3=0.26  0.27$, but $c_4=0.44 \\ge 0.27$. Thus, $a_3=4$.\n    \\item For $t_4 = 0.37$: $c_3=0.26  0.37$, but $c_4=0.44 \\ge 0.37$. Thus, $a_4=4$.\n    \\item For $t_5 = 0.47$: $c_4=0.44  0.47$, but $c_5=0.62 \\ge 0.47$. Thus, $a_5=5$.\n    \\item For $t_6 = 0.57$: $c_4=0.44  0.57$, but $c_5=0.62 \\ge 0.57$. Thus, $a_6=5$.\n    \\item For $t_7 = 0.67$: $c_5=0.62  0.67$, but $c_6=0.82 \\ge 0.67$. Thus, $a_7=6$.\n    \\item For $t_8 = 0.77$: $c_5=0.62  0.77$, but $c_6=0.82 \\ge 0.77$. Thus, $a_8=6$.\n    \\item For $t_9 = 0.87$: $c_6=0.82  0.87$, but $c_7=0.87 \\ge 0.87$. Thus, $a_9=7$.\n    \\item For $t_{10} = 0.97$: $c_9=0.95  0.97$, but $c_{10}=1.00 \\ge 0.97$. Thus, $a_{10}=10$.\n\\end{itemize}\nThe full vector of selection indices is $(a_1, \\dots, a_{10}) = (1, 1, 4, 4, 5, 5, 6, 6, 7, 10)$.\n\nStep 2: Computation of the neighbor-coherence index $\\kappa$.\n\nThe index is defined as $\\kappa = \\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\left(|a_{k+1} - a_k| \\le 1\\right)$. With $N=10$, we have $N-1=9$. We evaluate the indicator function $\\mathbf{1}(\\cdot)$ for each adjacent pair in the vector $a$:\n\\begin{itemize}\n    \\item $k=1$: $|a_2 - a_1| = |1 - 1| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=2$: $|a_3 - a_2| = |4 - 1| = 3 > 1 \\implies \\mathbf{1}(\\cdot) = 0$.\n    \\item $k=3$: $|a_4 - a_3| = |4 - 4| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=4$: $|a_5 - a_4| = |5 - 4| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=5$: $|a_6 - a_5| = |5 - 5| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=6$: $|a_7 - a_6| = |6 - 5| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=7$: $|a_8 - a_7| = |6 - 6| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=8$: $|a_9 - a_8| = |7 - 6| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$.\n    \\item $k=9$: $|a_{10} - a_9| = |10 - 7| = 3 > 1 \\implies \\mathbf{1}(\\cdot) = 0$.\n\\end{itemize}\nThe sum of the indicator function values is $1 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 0 = 7$.\nTherefore, the neighbor-coherence index is:\n$$ \\kappa = \\frac{7}{9} $$\n\nStep 3: Discussion.\n\nThe calculated value of $\\kappa = 7/9$ is high, indicating that a majority of adjacent indices in the resampled sequence correspond to ancestors that were either identical or immediate neighbors in the original ordered particle set. This high coherence is a hallmark of systematic resampling. Unlike multinomial resampling where each new particle is drawn independently, systematic resampling fixes the entire selection pattern with a single random draw $U$. The thresholds $t_k$ are equispaced, so they tend to select contiguous blocks of ancestors, preserving local structure.\n\nThis connects directly to the problem of **weight degeneracy**. In SMC, a common issue is the concentration of importance weights on a few particles, leaving the rest with negligible weights. This is **weight degeneracy**. Resampling is the standard remedy, aiming to discard low-weight particles and multiply high-weight particles to focus computational effort on promising regions of the state space. The weights provided, with $w_1=0.24$, $w_4=0.18$, $w_5=0.18$, and $w_6=0.20$, show a moderate degree of degeneracy.\n\nResampling, while necessary, introduces its own problem: a loss of **particle diversity**, as the number of unique ancestors in the resampled set is less than $N$. This impoverishment of the sample is a key limitation of particle methods. The high coherence measured by $\\kappa$ is a manifestation of how systematic resampling manages this trade-off. By preserving neighborhood structures, it avoids the complete randomization of particle locations that could occur with independent resampling, which may be beneficial if the high-weight particles form a \"cluster\" in state space.\n\nThis entire dynamic is severely exacerbated by the **curse of dimensionality**, a critical issue in high-dimensional inverse problems. As the dimension of the state space grows, the volume of the space grows exponentially. The posterior distribution targeted by the SMC sampler typically concentrates in a vanishingly small fraction of this volume. Consequently, a fixed number of random particles $N$ are increasingly unlikely to fall in this high-probability region. This leads to extreme weight degeneracy, where often only one particle has a non-zero weight after the update step. In such a catastrophic collapse, any resampling scheme, including systematic, will select only that single particle $N$ times. The resulting resampled set would have $(a_1, \\dots, a_N) = (j, j, \\dots, j)$ for some index $j$. In this case, $\\kappa$ would be $1$, but this would signify a total loss of diversity, not a healthy preservation of structure. The coherence property of systematic resampling is therefore not a panacea for the curse of dimensionality; it simply reflects the structure of the weights it is given. The fundamental problem lies in the inability of importance sampling to effectively explore high-dimensional spaces, a challenge that requires more advanced techniques beyond simple resampling.",
            "answer": "$$ \\boxed{\\frac{7}{9}} $$"
        },
        {
            "introduction": "Resampling is a powerful corrective measure, but it does not cure the underlying cause of severe weight degeneracy, especially in high-dimensional problems. This exercise confronts the famous 'curse of dimensionality' head-on, using a foundational linear-Gaussian model to prove why weight variance is destined to explode as the state dimension grows. Deriving this result provides a critical, theoretical understanding of the fundamental limits of standard particle filters and motivates the need for more advanced methods. ",
            "id": "3417360",
            "problem": "Let $d \\in \\mathbb{N}$ be the dimension and consider importance sampling in a linear-Gaussian setting relevant to inverse problems and data assimilation. Let the proposal density be $p(x)$, where $x \\sim \\mathcal{N}(0, I_d)$ with $I_d$ the $d \\times d$ identity matrix. The target density is proportional to $\\exp\\!\\big(-\\frac{1}{2}\\|x - \\mu\\|^2\\big)$ with a shift vector $\\mu \\in \\mathbb{R}^d$ satisfying $\\|\\mu\\| = c\\sqrt{d}$ for a fixed constant $c  0$ that does not depend on $d$. You may assume that the target is the Gaussian density $\\pi(x) = \\mathcal{N}(\\mu, I_d)$, so that the importance weight is $w(x) = \\pi(x)/p(x)$.\n\nStarting from the definitions of Gaussian densities, the Radonâ€“Nikodym derivative between two Gaussian measures with the same covariance, and the definition of the coefficient of variation, derive the coefficient of variation of the importance weights,\n$$\n\\mathrm{CV}(w) \\equiv \\frac{\\sqrt{\\mathrm{Var}_p(w)}}{\\mathbb{E}_p[w]},\n$$\nas an explicit closed-form expression in terms of $c$ and $d$. Your derivation should use only fundamental properties of Gaussian distributions such as the moment generating function of a Gaussian linear form. Provide your final answer as a single closed-form expression in $c$ and $d$. No numerical evaluation or rounding is required.",
            "solution": "The problem statement has been validated and is determined to be a valid, self-contained, and scientifically grounded problem in statistical analysis, specifically related to the performance of importance sampling in high dimensions. We proceed with the derivation.\n\nLet $x \\in \\mathbb{R}^d$ be a random vector. The problem defines a proposal distribution $p(x)$ and a target distribution $\\pi(x)$.\nThe proposal distribution is a standard normal distribution in $d$ dimensions, $x \\sim \\mathcal{N}(0, I_d)$, where $I_d$ is the $d \\times d$ identity matrix. Its probability density function (PDF) is:\n$$\np(x) = \\frac{1}{(2\\pi)^{d/2} \\det(I_d)^{1/2}} \\exp\\left(-\\frac{1}{2} x^T I_d^{-1} x\\right) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x\\|^2\\right)\n$$\nThe target distribution is a multivariate normal distribution with mean $\\mu \\in \\mathbb{R}^d$ and covariance $I_d$, denoted as $\\pi(x) = \\mathcal{N}(\\mu, I_d)$. Its PDF is:\n$$\n\\pi(x) = \\frac{1}{(2\\pi)^{d/2} \\det(I_d)^{1/2}} \\exp\\left(-\\frac{1}{2} (x-\\mu)^T I_d^{-1} (x-\\mu)\\right) = \\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x-\\mu\\|^2\\right)\n$$\nThe importance weight $w(x)$ is defined as the ratio of the target density to the proposal density, which is the Radon-Nikodym derivative $\\frac{d\\Pi}{dP}(x)$ where $\\Pi$ and $P$ are the measures corresponding to densities $\\pi$ and $p$.\n$$\nw(x) = \\frac{\\pi(x)}{p(x)} = \\frac{\\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x-\\mu\\|^2\\right)}{\\frac{1}{(2\\pi)^{d/2}} \\exp\\left(-\\frac{1}{2} \\|x\\|^2\\right)} = \\exp\\left(-\\frac{1}{2} \\|x-\\mu\\|^2 + \\frac{1}{2} \\|x\\|^2\\right)\n$$\nWe expand the squared norm in the exponent: $\\|x-\\mu\\|^2 = (x-\\mu)^T(x-\\mu) = x^T x - 2x^T\\mu + \\mu^T\\mu = \\|x\\|^2 - 2x^T\\mu + \\|\\mu\\|^2$.\nSubstituting this into the expression for $w(x)$ gives:\n$$\nw(x) = \\exp\\left(-\\frac{1}{2} (\\|x\\|^2 - 2x^T\\mu + \\|\\mu\\|^2) + \\frac{1}{2} \\|x\\|^2\\right) = \\exp\\left(x^T\\mu - \\frac{1}{2}\\|\\mu\\|^2\\right)\n$$\nThe coefficient of variation is defined as $\\mathrm{CV}(w) = \\frac{\\sqrt{\\mathrm{Var}_p(w)}}{\\mathbb{E}_p[w]}$. We must compute the first and second moments of $w(x)$ with respect to the proposal distribution $p(x)$.\n\nThe first moment, or expectation, of the weights is:\n$$\n\\mathbb{E}_p[w(x)] = \\int_{\\mathbb{R}^d} w(x) p(x) dx = \\int_{\\mathbb{R}^d} \\frac{\\pi(x)}{p(x)} p(x) dx = \\int_{\\mathbb{R}^d} \\pi(x) dx = 1\n$$\nThis result is expected, as the integral of any valid probability density function over its support is equal to $1$.\n\nNext, we compute the second moment, $\\mathbb{E}_p[w(x)^2]$.\n$$\nw(x)^2 = \\left(\\exp\\left(x^T\\mu - \\frac{1}{2}\\|\\mu\\|^2\\right)\\right)^2 = \\exp\\left(2x^T\\mu - \\|\\mu\\|^2\\right)\n$$\nThe expectation of this quantity is:\n$$\n\\mathbb{E}_p[w(x)^2] = \\mathbb{E}_p\\left[\\exp\\left(2x^T\\mu - \\|\\mu\\|^2\\right)\\right] = \\exp\\left(-\\|\\mu\\|^2\\right) \\mathbb{E}_p\\left[\\exp\\left(2\\mu^T x\\right)\\right]\n$$\nThe term $\\mathbb{E}_p[\\exp(2\\mu^T x)]$ is the moment generating function (MGF) of the random variable $x$, denoted $M_x(t) = \\mathbb{E}_p[\\exp(t^T x)]$, evaluated at $t=2\\mu$.\nSince $x \\sim p = \\mathcal{N}(0, I_d)$, its MGF is given by:\n$$\nM_x(t) = \\exp\\left(t^T 0 + \\frac{1}{2}t^T I_d t\\right) = \\exp\\left(\\frac{1}{2}\\|t\\|^2\\right)\n$$\nEvaluating the MGF at $t=2\\mu$:\n$$\n\\mathbb{E}_p\\left[\\exp(2\\mu^T x)\\right] = M_x(2\\mu) = \\exp\\left(\\frac{1}{2}\\|2\\mu\\|^2\\right) = \\exp\\left(\\frac{1}{2} \\cdot 4 \\|\\mu\\|^2\\right) = \\exp(2\\|\\mu\\|^2)\n$$\nSubstituting this result back into the expression for the second moment:\n$$\n\\mathbb{E}_p[w(x)^2] = \\exp(-\\|\\mu\\|^2) \\cdot \\exp(2\\|\\mu\\|^2) = \\exp(\\|\\mu\\|^2)\n$$\nThe variance of the weights is then calculated as:\n$$\n\\mathrm{Var}_p(w) = \\mathbb{E}_p[w^2] - (\\mathbb{E}_p[w])^2 = \\exp(\\|\\mu\\|^2) - 1^2 = \\exp(\\|\\mu\\|^2) - 1\n$$\nNow we assemble the coefficient of variation:\n$$\n\\mathrm{CV}(w) = \\frac{\\sqrt{\\mathrm{Var}_p(w)}}{\\mathbb{E}_p[w]} = \\frac{\\sqrt{\\exp(\\|\\mu\\|^2) - 1}}{1} = \\sqrt{\\exp(\\|\\mu\\|^2) - 1}\n$$\nFinally, we use the given condition on the norm of the shift vector, $\\|\\mu\\| = c\\sqrt{d}$. This implies $\\|\\mu\\|^2 = (c\\sqrt{d})^2 = c^2 d$. Substituting this into our expression for the coefficient of variation yields the final result.\n$$\n\\mathrm{CV}(w) = \\sqrt{\\exp(c^2 d) - 1}\n$$\nThis expression is the closed-form solution in terms of the constant $c$ and the dimension $d$.",
            "answer": "$$\n\\boxed{\\sqrt{\\exp(c^2 d) - 1}}\n$$"
        }
    ]
}