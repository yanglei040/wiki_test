{
    "hands_on_practices": [
        {
            "introduction": "The resampling step is the engine that combats weight degeneracy in particle filters, ensuring the survival of the fittest particles. This exercise provides a concrete, hands-on walkthrough of stratified resampling, an efficient and widely-used variant, allowing you to master the mechanics of how particles are duplicated and discarded based on their importance weights .",
            "id": "3409843",
            "problem": "Consider a nonlinear, non-Gaussian state-space model estimated with a Particle Filter (PF) within the Sequential Monte Carlo (SMC) framework. At a given assimilation cycle, suppose you have $N$ particles with unnormalized importance weights $\\tilde{w}_{1},\\ldots,\\tilde{w}_{N}$, where $N=10$ and\n$$\n\\tilde{w}_{1}=2,\\ \\tilde{w}_{2}=1,\\ \\tilde{w}_{3}=1,\\ \\tilde{w}_{4}=1,\\ \\tilde{w}_{5}=5,\\ \\tilde{w}_{6}=2,\\ \\tilde{w}_{7}=3,\\ \\tilde{w}_{8}=1,\\ \\tilde{w}_{9}=3,\\ \\tilde{w}_{10}=1.\n$$\nIn order to mitigate weight degeneracy and reduce sampling variance, you perform the resampling step using stratified resampling, which draws one uniform variate in each stratum of equal width on $[0,1)$.\n\nStarting from the core definitions of importance sampling and resampling in SMC, normalize the weights to obtain $w_{j}=\\tilde{w}_{j}/\\sum_{k=1}^{N}\\tilde{w}_{k}$, construct the cumulative distribution function (CDF) $C_{j}=\\sum_{k=1}^{j}w_{k}$ for $j=1,\\ldots,N$, and carry out stratified resampling using the following specific uniform samples in each stratum:\n$$\nu_{i}=\\frac{i-\\tfrac{1}{2}}{N},\\quad i=1,\\ldots,N,\n$$\nso that explicitly\n$$\nu_{1}=0.05,\\ u_{2}=0.15,\\ u_{3}=0.25,\\ u_{4}=0.35,\\ u_{5}=0.45,\\ u_{6}=0.55,\\ u_{7}=0.65,\\ u_{8}=0.75,\\ u_{9}=0.85,\\ u_{10}=0.95.\n$$\nUse the inverse CDF rule to assign ancestor indices $a_{i}$, defined as the smallest $j$ such that $u_{i}\\leq C_{j}$. Then compute the offspring counts $n_{j}$, defined as the number of times index $j$ appears among $\\{a_{1},\\ldots,a_{N}\\}$.\n\nProvide as your final answer a single row matrix that concatenates first the offspring counts $(n_{1},\\ldots,n_{N})$ and then the ancestor indices $(a_{1},\\ldots,a_{N})$, in that order. No rounding is required, and no units are involved. Your reasoning should begin from foundational SMC principles rather than shortcut formulas.",
            "solution": "The problem asks us to perform a stratified resampling step for a Particle Filter (PF) and to report the resulting offspring counts and ancestor indices. The procedure is part of the Sequential Monte Carlo (SMC) methodology used for state estimation in nonlinear, non-Gaussian systems. The core idea of resampling is to address the weight degeneracy problem, where after a few assimilation cycles, most particles have negligible importance weights. Resampling rejuvenates the particle set by duplicating particles with high weights and discarding those with low weights, effectively focusing computational resources on more probable regions of the state space.\n\nFirst, we validate the problem statement. The problem provides a well-defined set of unnormalized importance weights, a specific resampling scheme (stratified resampling), a clear algorithm for its execution, and a precise format for the output. All definitions are standard in the field of data assimilation. The data provided are self-contained, consistent, and numerically sound. The problem is scientifically grounded, well-posed, objective, and verifiable. Therefore, the problem is valid and we may proceed with the solution.\n\nThe process begins with the given set of $N=10$ unnormalized importance weights, $\\tilde{w}_{j}$ for $j=1,\\ldots,10$:\n$$\n\\tilde{w}_{1}=2,\\ \\tilde{w}_{2}=1,\\ \\tilde{w}_{3}=1,\\ \\tilde{w}_{4}=1,\\ \\tilde{w}_{5}=5,\\ \\tilde{w}_{6}=2,\\ \\tilde{w}_{7}=3,\\ \\tilde{w}_{8}=1,\\ \\tilde{w}_{9}=3,\\ \\tilde{w}_{10}=1.\n$$\nThe first step is to normalize these weights so that they sum to $1$. The sum of the unnormalized weights is:\n$$\n\\sum_{k=1}^{10} \\tilde{w}_{k} = 2+1+1+1+5+2+3+1+3+1 = 20.\n$$\nThe normalized weights, $w_{j}$, are obtained by dividing each unnormalized weight by this sum: $w_{j} = \\tilde{w}_{j} / \\sum_{k=1}^{10} \\tilde{w}_{k}$.\n$$\n\\begin{aligned}\nw_{1} = \\frac{2}{20} = 0.1 \\\\\nw_{2} = \\frac{1}{20} = 0.05 \\\\\nw_{3} = \\frac{1}{20} = 0.05 \\\\\nw_{4} = \\frac{1}{20} = 0.05 \\\\\nw_{5} = \\frac{5}{20} = 0.25 \\\\\nw_{6} = \\frac{2}{20} = 0.1 \\\\\nw_{7} = \\frac{3}{20} = 0.15 \\\\\nw_{8} = \\frac{1}{20} = 0.05 \\\\\nw_{9} = \\frac{3}{20} = 0.15 \\\\\nw_{10} = \\frac{1}{20} = 0.05\n\\end{aligned}\n$$\nThese normalized weights define a discrete probability distribution over the particle indices. The next step is to construct the cumulative distribution function (CDF), $C_{j} = \\sum_{k=1}^{j} w_{k}$.\n$$\n\\begin{aligned}\nC_{1} = w_{1} = 0.1 \\\\\nC_{2} = C_{1} + w_{2} = 0.1 + 0.05 = 0.15 \\\\\nC_{3} = C_{2} + w_{3} = 0.15 + 0.05 = 0.20 \\\\\nC_{4} = C_{3} + w_{4} = 0.20 + 0.05 = 0.25 \\\\\nC_{5} = C_{4} + w_{5} = 0.25 + 0.25 = 0.50 \\\\\nC_{6} = C_{5} + w_{6} = 0.50 + 0.1 = 0.60 \\\\\nC_{7} = C_{6} + w_{7} = 0.60 + 0.15 = 0.75 \\\\\nC_{8} = C_{7} + w_{8} = 0.75 + 0.05 = 0.80 \\\\\nC_{9} = C_{8} + w_{9} = 0.80 + 0.15 = 0.95 \\\\\nC_{10} = C_{9} + w_{10} = 0.95 + 0.05 = 1.00\n\\end{aligned}\n$$\nThe resampling step involves drawing $N$ new particles from the current set of particles, where the probability of drawing particle $j$ is $w_{j}$. Stratified resampling partitions the interval $[0,1)$ into $N$ equal strata, $[\\frac{i-1}{N}, \\frac{i}{N})$ for $i=1,\\ldots,N$. A single uniform random sample is drawn from each stratum. The problem specifies the exact samples to be used:\n$$\nu_{i} = \\frac{i - \\frac{1}{2}}{N} \\quad \\text{for } i=1,\\ldots,10.\n$$\nThe given samples are:\n$$\nu_{1}=0.05,\\ u_{2}=0.15,\\ u_{3}=0.25,\\ u_{4}=0.35,\\ u_{5}=0.45,\\ u_{6}=0.55,\\ u_{7}=0.65,\\ u_{8}=0.75,\\ u_{9}=0.85,\\ u_{10}=0.95.\n$$\nWe use the inverse CDF method to find the ancestor index $a_i$ for each sample $u_i$. The rule is to find the smallest index $j$ such that $u_i \\leq C_j$. This is equivalent to finding which interval $(C_{j-1}, C_j]$ (with $C_0=0$) contains $u_i$.\n\n- For $u_1=0.05$: We need the smallest $j$ such that $0.05 \\le C_j$. Since $C_1=0.1$, the condition holds for $j=1$. Thus, $a_1=1$.\n- For $u_2=0.15$: We need the smallest $j$ such that $0.15 \\le C_j$. $C_1=0.1  0.15$, but $C_2=0.15 \\ge 0.15$. The smallest such $j$ is $2$. Thus, $a_2=2$.\n- For $u_3=0.25$: We need the smallest $j$ such that $0.25 \\le C_j$. $C_3=0.20  0.25$, but $C_4=0.25 \\ge 0.25$. The smallest such $j$ is $4$. Thus, $a_3=4$.\n- For $u_4=0.35$: We need the smallest $j$ such that $0.35 \\le C_j$. $C_4=0.25  0.35$, but $C_5=0.50 \\ge 0.35$. The smallest such $j$ is $5$. Thus, $a_4=5$.\n- For $u_5=0.45$: We need the smallest $j$ such that $0.45 \\le C_j$. $C_4=0.25  0.45$, but $C_5=0.50 \\ge 0.45$. The smallest such $j$ is $5$. Thus, $a_5=5$.\n- For $u_6=0.55$: We need the smallest $j$ such that $0.55 \\le C_j$. $C_5=0.50  0.55$, but $C_6=0.60 \\ge 0.55$. The smallest such $j$ is $6$. Thus, $a_6=6$.\n- For $u_7=0.65$: We need the smallest $j$ such that $0.65 \\le C_j$. $C_6=0.60  0.65$, but $C_7=0.75 \\ge 0.65$. The smallest such $j$ is $7$. Thus, $a_7=7$.\n- For $u_8=0.75$: We need the smallest $j$ such that $0.75 \\le C_j$. $C_6=0.60  0.75$, but $C_7=0.75 \\ge 0.75$. The smallest such $j$ is $7$. Thus, $a_8=7$.\n- For $u_9=0.85$: We need the smallest $j$ such that $0.85 \\le C_j$. $C_8=0.80  0.85$, but $C_9=0.95 \\ge 0.85$. The smallest such $j$ is $9$. Thus, $a_9=9$.\n- For $u_{10}=0.95$: We need the smallest $j$ such that $0.95 \\le C_j$. $C_8=0.80  0.95$, but $C_9=0.95 \\ge 0.95$. The smallest such $j$ is $9$. Thus, $a_{10}=9$.\n\nThe set of ancestor indices is $\\{a_1,\\ldots,a_{10}\\} = \\{1, 2, 4, 5, 5, 6, 7, 7, 9, 9\\}$. The new set of particles will consist of copies of these ancestor particles.\n\nFinally, we compute the offspring counts, $n_j$, which is the number of times each original particle index $j$ is selected as an ancestor. We count the occurrences of each index in the set $\\{a_1, \\ldots, a_{10}\\}$.\n- $n_1$ (count of $1$'s): $1$\n- $n_2$ (count of $2$'s): $1$\n- $n_3$ (count of $3$'s): $0$\n- $n_4$ (count of $4$'s): $1$\n- $n_5$ (count of $5$'s): $2$\n- $n_6$ (count of $6$'s): $1$\n- $n_7$ (count of $7$'s): $2$\n- $n_8$ (count of $8$'s): $0$\n- $n_9$ (count of $9$'s): $2$\n- $n_{10}$ (count of $10$'s): $0$\n\nThe vector of offspring counts is $(n_1,\\ldots,n_{10}) = (1, 1, 0, 1, 2, 1, 2, 0, 2, 0)$. As a check, the sum of offspring counts must be $N$: $1+1+0+1+2+1+2+0+2+0=10$.\nThe vector of ancestor indices is $(a_1,\\ldots,a_{10}) = (1, 2, 4, 5, 5, 6, 7, 7, 9, 9)$.\n\nThe final answer is the concatenation of the offspring counts and the ancestor indices into a single row matrix.\n$$\n(n_1,\\ldots,n_{10}, a_1,\\ldots,a_{10}) = (1, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 2, 4, 5, 5, 6, 7, 7, 9, 9).\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  0  1  2  1  2  0  2  0  1  2  4  5  5  6  7  7  9  9\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond its mechanics, the resampling step introduces its own layer of stochasticity into the estimation process. This practice challenges you to derive, from first principles, the variance of a resampling-based estimator, offering deep insight into how weight dispersion and algorithmic randomness contribute to the overall uncertainty of the filter's output .",
            "id": "3409870",
            "problem": "Consider a single assimilation time $t$ in Sequential Monte Carlo (SMC) particle filtering for a nonlinear non-Gaussian state space model. You have $N=10$ particles with normalized importance weights $w_{1:10}$ and corresponding function evaluations $h\\!\\left(x_{t}^{(i)}\\right)$ of a bounded function $h(\\cdot)$. A resampling step is performed using multinomial resampling to produce offspring counts $A_{1:10}$, with $\\sum_{i=1}^{10} A_{i}=N$. The given data are:\n- Weights $w_{1:10}=\\left(0.05,\\,0.10,\\,0.15,\\,0.20,\\,0.10,\\,0.15,\\,0.10,\\,0.05,\\,0.05,\\,0.05\\right)$.\n- Function values $h\\!\\left(x_{t}^{(1:10)}\\right)=\\left(1.00,\\,-1.00,\\,2.00,\\,0.00,\\,1.50,\\,-0.50,\\,0.50,\\,-2.00,\\,0.25,\\,-0.75\\right)$.\n- One realization of offspring counts $A_{1:10}=\\left(1,\\,0,\\,3,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,0\\right)$ from the resampling step.\n\nDefine the resampling-based Monte Carlo estimator for the weighted expectation of $h$ as $\\widehat{\\mu}_{\\mathrm{res}}=\\frac{1}{N}\\sum_{i=1}^{N}A_{i}\\,h\\!\\left(x_{t}^{(i)}\\right)$. Using only the following foundational elements:\n- The conditional distribution of offspring counts under multinomial resampling, $A_{1:10}\\sim\\mathrm{Multinomial}\\!\\left(N,\\,w_{1:10}\\right)$.\n- The definition of conditional Monte Carlo variance as $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\,\\middle|\\,w_{1:10},\\,x_{t}^{(1:10)}\\right)$, i.e., variance with respect to resampling randomness given the particles and their weights.\n- The bilinearity of covariance and the properties of variance and covariance of multinomial random variables.\n\nDerive from first principles an explicit expression for $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\,\\middle|\\,w_{1:10},\\,x_{t}^{(1:10)}\\right)$ in terms of $w_{1:10}$ and $h\\!\\left(x_{t}^{(1:10)}\\right)$, and then evaluate it numerically for the data provided. Explain how your expression reveals the roles of weight dispersion and resampling randomness in this variance. Round your final numerical answer to four significant figures. The final answer must be a single real number.",
            "solution": "The problem requires the derivation and evaluation of the conditional variance of a resampling-based Monte Carlo estimator in the context of particle filtering. The validation of the problem statement has been performed. The problem is scientifically grounded, well-posed, and contains sufficient, consistent information for a unique solution.\n\nLet the set of $N$ particles at time $t$ be $\\left\\{x_{t}^{(i)}\\right\\}_{i=1}^{N}$ with corresponding normalized importance weights $\\left\\{w_{i}\\right\\}_{i=1}^{N}$, where $\\sum_{i=1}^{N} w_{i} = 1$. The function of interest is $h(\\cdot)$. For notational simplicity, we denote $h_{i} = h\\left(x_{t}^{(i)}\\right)$.\n\nThe estimator for the weighted expectation $\\mathbb{E}[h(x_t)]$, based on multinomial resampling, is defined as:\n$$\n\\widehat{\\mu}_{\\mathrm{res}} = \\frac{1}{N} \\sum_{i=1}^{N} A_{i} h_{i}\n$$\nwhere $A = (A_1, \\dots, A_N)$ is a vector of offspring counts. The problem states that this vector follows a multinomial distribution, $A \\sim \\mathrm{Multinomial}(N, w_1, \\dots, w_N)$.\n\nWe are asked to compute the conditional variance of this estimator, given the particles and their weights: $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}} \\, \\middle| \\, x_{t}^{(1:N)}, w_{1:N}\\right)$. Since the particles $x_{t}^{(1:N)}$ (and thus the function values $h_{1:N}$) and the weights $w_{1:N}$ are considered fixed (conditioned upon), the only source of randomness is the vector of offspring counts $A$. The provided specific realization of $A_{1:10}$ is a single sample from this distribution and is thus irrelevant for the calculation of the variance of the estimator, which is a property of its distribution.\n\nWe begin the derivation from first principles.\n$$\n\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\right) = \\mathrm{Var}\\!\\left(\\frac{1}{N} \\sum_{i=1}^{N} A_{i} h_{i}\\right)\n$$\nUsing the property $\\mathrm{Var}(cZ) = c^2 \\mathrm{Var}(Z)$ with $c = \\frac{1}{N}$, we extract the constant factor:\n$$\n\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\right) = \\frac{1}{N^2} \\mathrm{Var}\\!\\left(\\sum_{i=1}^{N} A_{i} h_{i}\\right)\n$$\nThe variance of a sum can be expressed using the bilinearity of covariance:\n$$\n\\mathrm{Var}\\!\\left(\\sum_{i=1}^{N} A_{i} h_{i}\\right) = \\mathrm{Cov}\\!\\left(\\sum_{i=1}^{N} A_{i} h_{i}, \\sum_{j=1}^{N} A_{j} h_{j}\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} h_{i} h_{j} \\mathrm{Cov}(A_{i}, A_{j})\n$$\nWe separate the sum into terms where $i=j$ and terms where $i \\neq j$:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} h_{i} h_{j} \\mathrm{Cov}(A_{i}, A_{j}) = \\sum_{i=1}^{N} h_{i}^2 \\mathrm{Var}(A_{i}) + \\sum_{i=1}^{N} \\sum_{j \\neq i}^{N} h_{i} h_{j} \\mathrm{Cov}(A_{i}, A_{j})\n$$\nFor a multinomial distribution $A \\sim \\mathrm{Multinomial}(N, w_1, \\dots, w_N)$, the variances and covariances of the components $A_i$ are known:\n- $\\mathrm{Var}(A_{i}) = N w_{i} (1 - w_{i})$ for $i \\in \\{1, \\dots, N\\}$\n- $\\mathrm{Cov}(A_{i}, A_{j}) = -N w_{i} w_{j}$ for $i \\neq j$\n\nSubstituting these into our expression:\n$$\n\\mathrm{Var}\\!\\left(\\sum_{i=1}^{N} A_{i} h_{i}\\right) = \\sum_{i=1}^{N} h_{i}^2 [N w_{i} (1 - w_{i})] + \\sum_{i=1}^{N} \\sum_{j \\neq i}^{N} h_{i} h_{j} [-N w_{i} w_{j}]\n$$\nFactor out the common term $N$:\n$$\n= N \\left[ \\sum_{i=1}^{N} h_{i}^2 w_{i} (1 - w_{i}) - \\sum_{i \\neq j} h_{i} h_{j} w_{i} w_{j} \\right]\n$$\n$$\n= N \\left[ \\sum_{i=1}^{N} h_{i}^2 w_{i} - \\sum_{i=1}^{N} h_{i}^2 w_{i}^2 - \\sum_{i \\neq j} h_{i} w_{i} h_{j} w_{j} \\right]\n$$\nWe recognize that the last two terms are related to the square of a sum. Specifically, consider the square of the standard importance sampling estimator $\\widehat{\\mu}_{\\mathrm{IS}} = \\sum_{k=1}^{N} w_{k} h_{k}$:\n$$\n\\left(\\sum_{k=1}^{N} w_{k} h_{k}\\right)^2 = \\sum_{k=1}^{N} (w_{k} h_{k})^2 + \\sum_{k \\neq j} w_{k} h_{k} w_{j} h_{j}\n$$\nThe term in our expression is the negative of this:\n$$\n- \\sum_{i=1}^{N} h_{i}^2 w_{i}^2 - \\sum_{i \\neq j} h_{i} w_{i} h_{j} w_{j} = - \\left( \\sum_{i=1}^{N} w_{i} h_{i} \\right)^2\n$$\nSubstituting this back, we obtain a compact form:\n$$\n\\mathrm{Var}\\!\\left(\\sum_{i=1}^{N} A_{i} h_{i}\\right) = N \\left[ \\sum_{i=1}^{N} w_{i} h_{i}^2 - \\left(\\sum_{i=1}^{N} w_{i} h_{i}\\right)^2 \\right]\n$$\nFinally, we substitute this back into the expression for $\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\right)$:\n$$\n\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\right) = \\frac{1}{N^2} \\cdot N \\left[ \\sum_{i=1}^{N} w_{i} h_{i}^2 - \\left(\\sum_{i=1}^{N} w_{i} h_{i}\\right)^2 \\right]\n$$\nThis yields the final expression for the conditional variance:\n$$\n\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}} \\, \\middle| \\, x_{t}^{(1:N)}, w_{1:N}\\right) = \\frac{1}{N} \\left[ \\sum_{i=1}^{N} w_{i} h\\left(x_{t}^{(i)}\\right)^2 - \\left(\\sum_{i=1}^{N} w_{i} h\\left(x_{t}^{(i)}\\right)\\right)^2 \\right]\n$$\nThis expression reveals that the variance introduced by the resampling step is proportional to the variance of the function $h$ with respect to the discrete probability measure defined by the weights $\\{w_i\\}$, scaled by $\\frac{1}{N}$. The resampling randomness (modeled by the multinomial distribution) transforms the deterministic quantity $\\sum w_i h_i$ into a random variable $\\widehat{\\mu}_{\\mathrm{res}}$ whose variance is analogous to the variance of a sample mean of $N$ i.i.d. draws. The \"weight dispersion\" affects the term in the brackets: if weights are highly concentrated on a few particles (low dispersion, or degeneracy), this term will be dominated by those few particles. Conversely, if weights are more evenly distributed, the term will reflect the variance of $h_i$ across a wider range of particles.\n\nNow, we evaluate this expression for the given data:\n- $N = 10$\n- $w_{1:10}=\\left(0.05,\\,0.10,\\,0.15,\\,0.20,\\,0.10,\\,0.15,\\,0.10,\\,0.05,\\,0.05,\\,0.05\\right)$\n- $h_{1:10}=\\left(1.00,\\,-1.00,\\,2.00,\\,0.00,\\,1.50,\\,-0.50,\\,0.50,\\,-2.00,\\,0.25,\\,-0.75\\right)$\n\nFirst, we calculate the weighted mean, $\\mu_w = \\sum_{i=1}^{10} w_{i} h_{i}$:\n\\begin{align*}\n\\mu_w = (0.05)(1.00) + (0.10)(-1.00) + (0.15)(2.00) + (0.20)(0.00) + (0.10)(1.50) \\\\\n+ (0.15)(-0.50) + (0.10)(0.50) + (0.05)(-2.00) + (0.05)(0.25) + (0.05)(-0.75) \\\\\n= 0.05 - 0.10 + 0.30 + 0.00 + 0.15 - 0.075 + 0.05 - 0.10 + 0.0125 - 0.0375 \\\\\n= 0.25\n\\end{align*}\nSo, $\\left(\\sum w_{i} h_{i}\\right)^2 = (0.25)^2 = 0.0625$.\n\nNext, we calculate the weighted mean of the squares, $E_w[h^2] = \\sum_{i=1}^{10} w_{i} h_{i}^2$:\n\\begin{align*}\nE_w[h^2] = (0.05)(1.00)^2 + (0.10)(-1.00)^2 + (0.15)(2.00)^2 + (0.20)(0.00)^2 + (0.10)(1.50)^2 \\\\\n+ (0.15)(-0.50)^2 + (0.10)(0.50)^2 + (0.05)(-2.00)^2 + (0.05)(0.25)^2 + (0.05)(-0.75)^2 \\\\\n= (0.05)(1.00) + (0.10)(1.00) + (0.15)(4.00) + (0.20)(0.00) + (0.10)(2.25) \\\\\n+ (0.15)(0.25) + (0.10)(0.25) + (0.05)(4.00) + (0.05)(0.0625) + (0.05)(0.5625) \\\\\n= 0.05 + 0.10 + 0.60 + 0.00 + 0.225 + 0.0375 + 0.025 + 0.20 + 0.003125 + 0.028125 \\\\\n= 1.26875\n\\end{align*}\nThe term in the brackets is the weighted variance:\n$$ \n\\sum w_{i} h_{i}^2 - \\left(\\sum w_{i} h_{i}\\right)^2 = 1.26875 - 0.0625 = 1.20625\n$$\nFinally, the conditional variance is:\n$$\n\\mathrm{Var}\\!\\left(\\widehat{\\mu}_{\\mathrm{res}}\\right) = \\frac{1}{N} (1.20625) = \\frac{1.20625}{10} = 0.120625\n$$\nRounding this to four significant figures gives $0.1206$.",
            "answer": "$$\\boxed{0.1206}$$"
        },
        {
            "introduction": "Particle filters truly shine when applied to complex systems, many of which are best described by continuous-time dynamics. This advanced exercise guides you through implementing a filter step for a state evolving via a Stochastic Differential Equation (SDE), tackling the critical weight correction required when using a proposal different from the true model dynamics .",
            "id": "3409822",
            "problem": "You are to implement a single time-step of a continuous-time bootstrap particle filter for a nonlinear, non-Gaussian data assimilation problem. The latent state evolves according to a Stochastic Differential Equation (SDE, stochastic differential equation) with constant diffusion and nonlinear drift. A set of particles approximate the filtering distribution at time $t$ and must be propagated to time $t + \\Delta t$ using Euler-Maruyama discretization and reweighted to account for both the model transition and an observation with heavy-tailed noise.\n\nFundamental setup:\n- The latent state $x_t \\in \\mathbb{R}$ evolves according to the SDE\n$$\n\\mathrm{d}x_t = f(x_t) \\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is Brownian motion, $\\sigma  0$ is a known scalar diffusion coefficient, and $f$ is a nonlinear drift. The proposal for the particle propagation is given by a drift $g$ that may differ from the true drift $f$.\n- The Euler-Maruyama (EM, Euler-Maruyama) discretization used for the proposal computes, for each particle $x^{(i)}_t$,\n$$\nx^{(i)}_{t+\\Delta t} = x^{(i)}_t + g\\big(x^{(i)}_t\\big)\\,\\Delta t + \\sigma \\,\\Delta W^{(i)},\n$$\nwhere the Brownian increment $\\Delta W^{(i)}$ is drawn as a normal random variable with mean $0$ and variance $\\Delta t$.\n- An observation at time $t+\\Delta t$ is provided via\n$$\ny = h\\big(x_{t+\\Delta t}\\big) + v,\n$$\nwith $h$ nonlinear and $v$ drawn from a Student-$t$ distribution with degrees-of-freedom $\\nu$ and scale $s  0$. The observation noise is heavy-tailed and not Gaussian.\n\nYour task for a single time-step:\n1. Initialize $N$ particles at time $t$ from a specified distribution with uniform weights.\n2. Propagate particles to time $t+\\Delta t$ using the EM proposal with drift $g$ and diffusion $\\sigma$.\n3. Compute the importance weight correction due to mismatch between the target model transition (with drift $f$) and the proposal transition (with drift $g$) across the time-step, grounded in basic principles for change of measure in SDEs with constant diffusion. Use a principled discretization of the continuous-time Radon-Nikodym derivative for a single step: it must depend on the drift difference, the diffusion, the time-step, and the realized Brownian increments for each particle.\n4. Incorporate the observation likelihood for the nonlinear measurement $h$ with Student-$t$ noise. It is acceptable to use the likelihood up to a constant factor because normalized importance weights remove constants common to all particles.\n5. Normalize the updated weights and compute the effective sample size (ESS, effective sample size), defined as\n$$\n\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N \\big(w^{(i)}\\big)^2},\n$$\nwhere $w^{(i)}$ are the normalized weights at time $t+\\Delta t$.\n6. Resampling is not performed; only report the ESS.\n\nModel specification:\n- Drift $f$ is\n$$\nf(x) = \\alpha \\,\\tanh(x),\n$$\nwith a known scalar parameter $\\alpha  0$.\n- Proposal drift $g$ is one of:\n  - $g(x) = f(x)$ (no mismatch),\n  - $g(x) = \\alpha x$ (linearization mismatch),\n  - $g(x) = 0$ (zero-drift mismatch).\n- Measurement function is\n$$\nh(x) = x^3.\n$$\n\nParticle initialization:\n- Number of particles $N = 250$.\n- Initial particles $x^{(i)}_t$ are independent and identically distributed as standard normal with mean $0$ and variance $1$, generated with a fixed random seed for reproducibility.\n- Initial weights are uniform $w^{(i)}_t = \\frac{1}{N}$.\n\nRandomness and reproducibility:\n- You must use the specified random seeds for initializing the particles and for generating Brownian increments in each test case.\n- Each test case uses its own seed for the Brownian increments; initial particles are generated once with a specified seed and reused across cases.\n\nTest suite and parameters:\n- Common parameters across all test cases: $\\alpha = 1.3$, $\\sigma = 0.6$, $N = 250$, initial particle seed $= 2025$, $h(x) = x^3$, degrees-of-freedom $\\nu = 5$, scale $s = 0.7$.\n- Test Case A (happy path):\n  - Time-step $\\Delta t = 0.1$,\n  - Proposal drift type: $g(x) = f(x)$ (no mismatch),\n  - Observation value $y = 1.0$,\n  - Brownian increment seed $= 10$.\n- Test Case B (significant mismatch in dynamics):\n  - Time-step $\\Delta t = 0.1$,\n  - Proposal drift type: $g(x) = 0$ (zero-drift),\n  - Observation value $y = 1.0$,\n  - Brownian increment seed $= 20$.\n- Test Case C (small time-step boundary):\n  - Time-step $\\Delta t = 10^{-3}$,\n  - Proposal drift type: $g(x) = \\alpha x$ (linearization),\n  - Observation value $y = 1.0$,\n  - Brownian increment seed $= 30$.\n- Test Case D (outlier observation):\n  - Time-step $\\Delta t = 0.1$,\n  - Proposal drift type: $g(x) = \\alpha x$ (linearization),\n  - Observation value $y = 8.0$,\n  - Brownian increment seed $= 40$.\n\nAlgorithmic constraints:\n- The importance weight correction for dynamics must be derived from first principles appropriate for constant-diffusion SDEs, ensuring scientific realism, and implemented for a single Euler-Maruyama step with the realized Brownian increments.\n- The observation likelihood must reflect the Student-$t$ noise for $y - h(x_{t+\\Delta t})$ with degrees-of-freedom $\\nu$ and scale $s$, up to a constant factor.\n- All computations must be numerically stable; for example, perform weight calculations in the logarithmic domain where appropriate.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely the four computed $\\mathrm{ESS}$ values for Test Cases A, B, C, and D in that order, as floating-point numbers. For example, the output format is\n$$\n[\\mathrm{ESS}_A,\\mathrm{ESS}_B,\\mathrm{ESS}_C,\\mathrm{ESS}_D].\n$$\nNo additional text should be printed.\n\nYour implementation must be a complete, runnable program, in Python, version $3.12$, using only the standard library, NumPy version $1.23.5$, and SciPy version $1.11.4$ (no other libraries). The program must be self-contained with no external inputs or files.",
            "solution": "The user-provided problem statement is deemed valid. It is scientifically grounded in the theory of stochastic differential equations and particle filtering, well-posed with all necessary parameters and conditions defined, and objective in its formulation. We can therefore proceed with a full solution.\n\nThe problem requires the implementation of a single time-step of a bootstrap particle filter for a state-space model defined in continuous time. The latent state $x_t \\in \\mathbb{R}$ evolves according to a Stochastic Differential Equation (SDE), and observations are nonlinear and corrupted by heavy-tailed noise. The core of the task is to propagate an ensemble of particles and update their importance weights according to both the model dynamics and a new observation.\n\nThe overall procedure for a single time-step from $t$ to $t+\\Delta t$ is as follows:\n1.  **Propagation:** Each particle $x^{(i)}_t$ is advanced to a new state $x^{(i)}_{t+\\Delta t}$ using a proposal distribution.\n2.  **Weighting:** The importance weight of each particle is updated to reflect how well it aligns with both the true model dynamics and the observation at time $t+\\Delta t$.\n3.  **Evaluation:** The Effective Sample Size (ESS) is computed from the new, normalized weights to quantify weight degeneracy.\n\nWe now detail each step based on the provided model specification.\n\n**1. Particle Propagation**\n\nThe state of each particle $i \\in \\{1, \\dots, N\\}$ is propagated from time $t$ to $t+\\Delta t$ using the Euler-Maruyama discretization of the proposal SDE. The proposal dynamics use a drift function $g(x)$ which may differ from the true model drift $f(x)$.\n\nThe update rule for particle $i$ is:\n$$\nx^{(i)}_{t+\\Delta t} = x^{(i)}_t + g\\big(x^{(i)}_t\\big)\\,\\Delta t + \\sigma \\,\\Delta W^{(i)}\n$$\nwhere $\\Delta W^{(i)}$ is a random sample representing the Brownian increment over the interval of length $\\Delta t$. These increments are drawn independently for each particle from a normal distribution with mean $0$ and variance $\\Delta t$, i.e., $\\Delta W^{(i)} \\sim \\mathcal{N}(0, \\Delta t)$.\n\n**2. Importance Weight Update**\n\nGiven uniform weights $w^{(i)}_t = 1/N$ at time $t$, the new, unnormalized weights $\\tilde{w}^{(i)}_{t+\\Delta t}$ at time $t+\\Delta t$ are given by the product of a dynamics correction factor and an observation likelihood term:\n$$\n\\tilde{w}^{(i)}_{t+\\Delta t} \\propto w_{\\text{dyn}}^{(i)} \\times L^{(i)}\n$$\nFor numerical stability, computations are performed in the logarithmic domain:\n$$\n\\log \\tilde{w}^{(i)}_{t+\\Delta t} = \\log w_{\\text{dyn}}^{(i)} + \\log L^{(i)} + C\n$$\nwhere $C$ is an arbitrary constant.\n\n**2.1. Dynamics Correction Weight ($w_{\\text{dyn}}^{(i)}$)**\n\nThis term corrects for the discrepancy between the proposal dynamics (with drift $g$) and the target dynamics (with drift $f$). The importance weight is the ratio of the target transition probability density to the proposal transition probability density, $w_{\\text{dyn}}^{(i)} = p(x^{(i)}_{t+\\Delta t} | x^{(i)}_t) / q(x^{(i)}_{t+\\Delta t} | x^{(i)}_t)$.\n\nFor the Euler-Maruyama scheme with constant diffusion $\\sigma$, both densities are Gaussian:\n-   Target: $p(x_{t+\\Delta t} | x_t) = \\mathcal{N}(x_{t+\\Delta t} | x_t + f(x_t)\\Delta t, \\sigma^2 \\Delta t)$\n-   Proposal: $q(x_{t+\\Delta t} | x_t) = \\mathcal{N}(x_{t+\\Delta t} | x_t + g(x_t)\\Delta t, \\sigma^2 \\Delta t)$\n\nThe logarithm of the ratio of these two densities, after canceling common terms, yields the log-importance weight. This result is a first-order discretization of the Girsanov theorem's Radon-Nikodym derivative. For particle $i$, the log-weight for the dynamics correction is:\n$$\n\\log w_{\\text{dyn}}^{(i)} = \\frac{f(x_t^{(i)}) - g(x_t^{(i)})}{\\sigma^2} \\left( \\sigma \\Delta W^{(i)} \\right) - \\frac{1}{2} \\left( \\frac{f(x_t^{(i)}) - g(x_t^{(i)})}{\\sigma} \\right)^2 \\Delta t\n$$\nHere, $f(x) = \\alpha \\tanh(x)$ is the true drift, $g(x)$ is the proposal drift specified by the test case, and $\\sigma \\Delta W^{(i)} = x^{(i)}_{t+\\Delta t} - x^{(i)}_t - g(x_t^{(i)})\\Delta t$ is the stochastic component realized during the proposal step. If the proposal drift matches the true drift ($g=f$), this term is zero, and no correction is needed.\n\n**2.2. Observation Likelihood ($L^{(i)}$)**\n\nThe observation model is $y = h(x_{t+\\Delta t}) + v$, where the noise $v$ follows a Student-$t$ distribution with $\\nu$ degrees of freedom and scale parameter $s$. The probability density function (PDF) for such a distribution is:\n$$\np(v; \\nu, s) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\pi\\nu}s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{v}{s}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n$$\nThe observation likelihood for particle $i$, given its propagated state $x^{(i)}_{t+\\Delta t}$, is the PDF evaluated at the residual $v^{(i)} = y - h(x^{(i)}_{t+\\Delta t})$. Because the final weights are normalized, any constant multiplicative factors in the likelihood are irrelevant. The log-likelihood for particle $i$ is thus:\n$$\n\\log L^{(i)} \\propto -\\frac{\\nu+1}{2} \\log\\left(1 + \\frac{1}{\\nu}\\left(\\frac{y - h\\left(x^{(i)}_{t+\\Delta t}\\right)}{s}\\right)^2\\right)\n$$\nThis expression captures the relative probability of observing $y$ for each particle's proposed state.\n\n**3. Weight Normalization and ESS Calculation**\n\nThe total unnormalized log-weights, $\\log \\tilde{w}^{(i)}_{t+\\Delta t}$, are combined and then normalized to sum to one. To prevent numerical underflow, the log-sum-exp trick is employed. First, we find the maximum log-weight, $C = \\max_i(\\log \\tilde{w}^{(i)}_{t+\\Delta t})$. Then, the normalized weights $w^{(i)}_{t+\\Delta t}$ are computed as:\n$$\nw^{(i)}_{t+\\Delta t} = \\frac{\\exp\\left(\\log \\tilde{w}^{(i)}_{t+\\Delta t} - C\\right)}{\\sum_{j=1}^{N} \\exp\\left(\\log \\tilde{w}^{(j)}_{t+\\Delta t} - C\\right)}\n$$\nFinally, the Effective Sample Size (ESS) is calculated to assess the degeneracy of the particle weights. A low ESS indicates that a few particles have very high weights, while the rest are negligible, suggesting the particle representation of the distribution is poor. The ESS is given by:\n$$\n\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N \\left(w^{(i)}_{t+\\Delta t}\\right)^2}\n$$\nAn ESS value close to the total number of particles, $N$, indicates that the weights are nearly uniform, which is ideal. An ESS close to $1$ signifies extreme degeneracy. The analysis is performed for each of the four specified test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Implements a single time-step of a bootstrap particle filter for a nonlinear,\n    non-Gaussian data assimilation problem and computes the Effective Sample Size (ESS)\n    for four different test cases.\n    \"\"\"\n    \n    # Common parameters across all test cases\n    alpha = 1.3\n    sigma = 0.6\n    N = 250\n    initial_particle_seed = 2025\n    nu = 5.0\n    s_scale = 0.7\n\n    # Define the core model functions\n    f_drift = lambda x: alpha * np.tanh(x)\n    h_obs = lambda x: x**3\n\n    # Define the parameters for each test case\n    test_cases_params = [\n        {'dt': 0.1, 'g_type': 'identity', 'y_obs': 1.0, 'brownian_seed': 10}, # Case A\n        {'dt': 0.1, 'g_type': 'zero', 'y_obs': 1.0, 'brownian_seed': 20},     # Case B\n        {'dt': 1e-3, 'g_type': 'linear', 'y_obs': 1.0, 'brownian_seed': 30},  # Case C\n        {'dt': 0.1, 'g_type': 'linear', 'y_obs': 8.0, 'brownian_seed': 40},   # Case D\n    ]\n\n    # --- Step 1: Initialize particles (done once for all cases) ---\n    # These are the particles at time t, x_t.\n    rng_initial = np.random.default_rng(initial_particle_seed)\n    x_t = rng_initial.normal(loc=0.0, scale=1.0, size=N)\n\n    results_ess = []\n    \n    for case_params in test_cases_params:\n        dt = case_params['dt']\n        g_type = case_params['g_type']\n        y_obs = case_params['y_obs']\n        brownian_seed = case_params['brownian_seed']\n\n        # Define the proposal drift function g(x) based on the test case\n        if g_type == 'identity':\n            g_drift = f_drift\n        elif g_type == 'zero':\n            g_drift = lambda x: np.zeros_like(x)\n        elif g_type == 'linear':\n            g_drift = lambda x: alpha * x\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown proposal drift type: {g_type}\")\n\n        # --- Step 2: Propagate particles (Prediction) ---\n        # Generate Brownian increments for this specific case\n        rng_brownian = np.random.default_rng(brownian_seed)\n        delta_W = rng_brownian.normal(loc=0.0, scale=np.sqrt(dt), size=N)\n        \n        g_val_t = g_drift(x_t)\n        # These are the propagated particles at time t+dt, x_{t+dt}\n        x_t_plus_dt = x_t + g_val_t * dt + sigma * delta_W\n\n        # --- Step 3  4: Compute unnormalized log weights (Update) ---\n        \n        # Part 1: Log importance weight correction for dynamics mismatch\n        f_val_t = f_drift(x_t)\n        drift_diff = f_val_t - g_val_t\n        \n        log_dyn_correction = (drift_diff / sigma**2) * (sigma * delta_W) - 0.5 * (drift_diff / sigma)**2 * dt\n\n        # Part 2: Log likelihood from the observation\n        h_val_t_plus_dt = h_obs(x_t_plus_dt)\n        log_obs_likelihood = student_t.logpdf(y_obs, df=nu, loc=h_val_t_plus_dt, scale=s_scale)\n\n        # Combine log weights\n        log_weights_unnorm = log_dyn_correction + log_obs_likelihood\n\n        # --- Step 5: Normalize weights and compute ESS ---\n        \n        # Use logsumexp for robust normalization\n        log_sum_weights = logsumexp(log_weights_unnorm)\n        log_normalized_weights = log_weights_unnorm - log_sum_weights\n        normalized_weights = np.exp(log_normalized_weights)\n        \n        # Compute Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n        results_ess.append(ess)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results_ess))}]\")\n\nsolve()\n\n```"
        }
    ]
}