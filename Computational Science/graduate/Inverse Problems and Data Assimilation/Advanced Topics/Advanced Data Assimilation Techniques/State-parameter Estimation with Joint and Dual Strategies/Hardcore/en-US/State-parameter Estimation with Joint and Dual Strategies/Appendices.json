{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin by exploring the relationship between joint and dual estimation strategies in their most fundamental form. In the idealized context of a linear-Gaussian system, the equivalence between these two approaches can be demonstrated analytically. This exercise  guides you through a pen-and-paper derivation to prove that a full joint update on an augmented state-parameter vector yields the same parameter estimate as a two-step dual strategy, where the state is updated first, followed by a parameter update via statistical regression.",
            "id": "3421586",
            "problem": "Consider a linear-Gaussian state-parameter estimation problem in the augmented space with augmented state $z = (x,\\theta)^{\\top} \\in \\mathbb{R}^{2}$. The forecast (background) distribution is Gaussian with mean $m^{f} = (m_{x}^{f}, m_{\\theta}^{f})^{\\top}$ and covariance\n$$\nP^{f} \\;=\\; \\begin{pmatrix}\np_{x}  c \\\\\nc  p_{\\theta}\n\\end{pmatrix},\n$$\nwhere $p_{x}  0$, $p_{\\theta}  0$, and $|c|  \\sqrt{p_{x} p_{\\theta}}$. The observation model is $y = H z + v$ with $H = (1,\\,0)$, so the measurement depends only on the state component $x$, and $v \\sim \\mathcal{N}(0,R)$ with $R = r  0$. Assume the large-ensemble, linear-Gaussian limit so that the Ensemble Transform Kalman Filter (ETKF) mean update coincides with that of the Kalman filter (KF).\n\nDefine the orthogonal projections onto the state and parameter subspaces in the augmented space by\n$$\nP_{x} \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  0\n\\end{pmatrix}, \n\\qquad\nP_{\\theta} \\;=\\; \\begin{pmatrix}\n0  0 \\\\\n0  1\n\\end{pmatrix}.\n$$\n\nCompare the following two strategies for the analysis mean of the parameter $\\theta$:\n\n1. Joint ETKF on the augmented state $z$, i.e., a single analysis update in the augmented space using $H$.\n\n2. A two-step ETKF (dual strategy): first update only the state $x$ using $y$; then update $\\theta$ by regressing $\\theta$ onto $x$ using the background cross-covariance, i.e., use the linear map induced by the orthogonal projection coupling $G = P_{\\theta} P^{f} P_{x} \\left(P_{x} P^{f} P_{x}\\right)^{-1}$ so that the parameter mean increments as $m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + G\\left(m_{x}^{a} - m_{x}^{f}\\right)$.\n\nStarting from the Gaussian-Bayes/Kalman update principles and the projection definitions above, derive a closed-form expression for the difference in the parameter analysis means\n$$\n\\Delta m_{\\theta} \\;=\\; m_{\\theta}^{a,\\text{dual}} - m_{\\theta}^{a,\\text{joint}}\n$$\nin terms of $m_{x}^{f}$, $m_{\\theta}^{f}$, $y$, $p_{x}$, $c$, and $r$. Express your final answer as a single simplified analytic expression. No numerical rounding is required. Angle or physical units are not applicable here.",
            "solution": "The problem requires a comparison of two state-parameter estimation strategies, a joint update and a dual (two-step) update, within a linear-Gaussian framework. We are asked to find the difference between the parameter analysis means produced by these two methods. The analysis is performed under the assumption that the Ensemble Transform Kalman Filter (ETKF) mean update is equivalent to the standard Kalman Filter (KF) update.\n\nThe general Kalman filter analysis mean update for a state vector $z$ is given by:\n$$m^{a} = m^{f} + K(y - H m^{f})$$\nwhere $m^{f}$ is the forecast (background) mean, $y$ is the observation, $H$ is the observation operator, and $K$ is the Kalman gain, defined as:\n$$K = P^{f} H^{\\top} (H P^{f} H^{\\top} + R)^{-1}$$\nHere, $P^{f}$ is the forecast error covariance and $R$ is the observation error covariance.\n\nWe are given the following:\nThe augmented state is $z = (x, \\theta)^{\\top}$.\nThe forecast mean is $m^{f} = (m_{x}^{f}, m_{\\theta}^{f})^{\\top}$.\nThe forecast covariance is $P^{f} = \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix}$.\nThe observation model is $y = H z + v$ with $H = (1, 0)$ and $v \\sim \\mathcal{N}(0, r)$, so $R=r$.\n\nFirst, let's derive the analysis mean for the parameter $\\theta$ using the joint update strategy, denoted by $m_{\\theta}^{a,\\text{joint}}$.\n\n**1. Joint ETKF/KF Update**\n\nIn this strategy, the update is applied to the full augmented state $z$. We first compute the components of the Kalman gain $K$.\nThe observation operator is $H = (1, 0)$, so its transpose is $H^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nThe term $H P^{f} H^{\\top}$ is:\n$$H P^{f} H^{\\top} = (1, 0) \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (p_{x}, c) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = p_{x}$$\nThe term $(H P^{f} H^{\\top} + R)^{-1}$ is therefore $(p_{x} + r)^{-1}$.\n\nThe term $P^{f} H^{\\top}$ is:\n$$P^{f} H^{\\top} = \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix}$$\nNow, we can compute the Kalman gain $K$:\n$$K = \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix} (p_{x} + r)^{-1} = \\frac{1}{p_{x} + r} \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix}$$\n\nThe innovation term is $y - H m^{f}$:\n$$y - H m^{f} = y - (1, 0) \\begin{pmatrix} m_{x}^{f} \\\\ m_{\\theta}^{f} \\end{pmatrix} = y - m_{x}^{f}$$\nThe analysis update for the augmented mean vector $m^{a,\\text{joint}}$ is:\n$$m^{a,\\text{joint}} = \\begin{pmatrix} m_{x}^{a,\\text{joint}} \\\\ m_{\\theta}^{a,\\text{joint}} \\end{pmatrix} = \\begin{pmatrix} m_{x}^{f} \\\\ m_{\\theta}^{f} \\end{pmatrix} + \\frac{1}{p_{x} + r} \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix} (y - m_{x}^{f})$$\nFrom this expression, we extract the analysis mean for the parameter component $\\theta$:\n$$m_{\\theta}^{a,\\text{joint}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n\nNext, let's derive the analysis mean for the parameter $\\theta$ using the dual strategy, $m_{\\theta}^{a,\\text{dual}}$.\n\n**2. Dual (Two-Step) ETKF/KF Update**\n\nThis strategy consists of two steps.\n\n**Step 2a: Update the state component $x$.**\nThis is a scalar Kalman filter problem. The state is $x$, with forecast mean $m_{x}^{f}$ and forecast variance $p_{x}$. The observation is $y = x + v$, with observation operator $H_{x}=1$ and observation error variance $r$.\nThe scalar Kalman gain $K_{x}$ is:\n$$K_{x} = p_{x} (H_{x} p_{x} H_{x}^{\\top} + r)^{-1} = p_{x} (1 \\cdot p_{x} \\cdot 1 + r)^{-1} = \\frac{p_{x}}{p_{x} + r}$$\nThe analysis mean for $x$, denoted $m_{x}^{a}$, is:\n$$m_{x}^{a} = m_{x}^{f} + K_{x} (y - H_{x} m_{x}^{f}) = m_{x}^{f} + \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f})$$\n\n**Step 2b: Update the parameter component $\\theta$ by regression.**\nThe problem specifies the update rule for the parameter mean:\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + G(m_{x}^{a} - m_{x}^{f})$$\nwhere $G$ is the regression coefficient representing the influence of an update in $x$ on $\\theta$. This coefficient is derived from the background covariance structure. For a multivariate Gaussian distribution, the conditional expectation of $\\theta$ given $x$ is $\\mathbb{E}[\\theta|x] = m_{\\theta}^{f} + \\text{Cov}(\\theta, x) \\text{Var}(x)^{-1} (x - m_{x}^{f})$. The regression coefficient is therefore $G = \\text{Cov}(\\theta, x) \\text{Var}(x)^{-1}$. From the background covariance matrix $P^{f}$, we have $\\text{Cov}(\\theta, x) = c$ and $\\text{Var}(x) = p_{x}$.\nThus, the regression coefficient is $G = \\frac{c}{p_{x}}$.\n\nThe problem provides a formal definition of $G$ using projection operators, $G = P_{\\theta} P^{f} P_{x} (P_{x} P^{f} P_{x})^{-1}$. Let's verify this yields the same scalar coefficient. The operator maps the subspace of $x$ to the subspace of $\\theta$.\n$$P_{x} P^{f} P_{x} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} p_{x}  0 \\\\ 0  0 \\end{pmatrix}$$\nThe inverse on the relevant subspace (or pseudo-inverse) is $(P_{x} P^{f} P_{x})^{\\dagger} = \\begin{pmatrix} p_{x}^{-1}  0 \\\\ 0  0 \\end{pmatrix}$.\n$$P_{\\theta} P^{f} P_{x} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix}$$\nThe full operator is $G_{op} = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix} \\begin{pmatrix} p_{x}^{-1}  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ c p_{x}^{-1}  0 \\end{pmatrix}$. This operator maps an increment vector $(m_x^a - m_x^f, 0)^{\\top}$ to $(0, \\frac{c}{p_x}(m_x^a - m_x^f))^{\\top}$. The scalar regression coefficient $G$ is indeed $\\frac{c}{p_{x}}$.\n\nNow we substitute the state increment $(m_{x}^{a} - m_{x}^{f})$ from Step 2a into the regression formula:\n$$m_{x}^{a} - m_{x}^{f} = \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f})$$\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\left(\\frac{c}{p_{x}}\\right) \\left( \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f}) \\right)$$\nThe term $p_{x}$ in the numerator and denominator cancels out:\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n\n**3. Comparison and Final Difference**\n\nWe now have the expressions for the parameter analysis mean from both strategies:\n$$m_{\\theta}^{a,\\text{joint}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\nThe two expressions are identical. This demonstrates that for a linear-Gaussian system where the observation depends only on the state component, the joint update of the parameter is equivalent to a two-step process of updating the state first and then updating the parameter via regression using background statistics.\n\nThe difference in the parameter analysis means is therefore:\n$$\\Delta m_{\\theta} = m_{\\theta}^{a,\\text{dual}} - m_{\\theta}^{a,\\text{joint}}$$\n$$\\Delta m_{\\theta} = \\left( m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f}) \\right) - \\left( m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f}) \\right)$$\n$$\\Delta m_{\\theta} = 0$$\n\nThe closed-form expression for the difference is simply the number zero.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "A successful parameter estimation hinges not only on the algorithm but also on the information content of the available data. This leads to the critical concept of parameter identifiability: can the parameters be uniquely determined from the experiment? This practice  presents a thought experiment where a flawed experimental design leads to unidentifiable parameters, challenging you to diagnose the issue by characterizing the posterior precision matrix and to propose a specific modification to the experiment that resolves the ambiguity.",
            "id": "3421599",
            "problem": "Consider a linear-Gaussian state-parameter estimation setting. The unknown parameter vector is $\\theta \\in \\mathbb{R}^{3}$ with components $\\theta = (\\theta_{1}, \\theta_{2}, \\theta_{3})^{\\top}$. You perform a single-step experiment with a deterministic state model (i.e., process noise identically zero) that maps parameters to a state $x \\in \\mathbb{R}^{2}$ according to\n$$\nx = M(\\theta) \\equiv \\begin{pmatrix} \\theta_{1} + \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix},\n$$\nand you observe\n$$\ny = H x + v, \\quad H \\equiv \\begin{pmatrix} 1  0 \\end{pmatrix},\n$$\nwith measurement noise $v \\sim \\mathcal{N}(0, \\sigma^{2})$ and known noise variance $\\sigma^{2}  0$. Assume a Gaussian prior on $\\theta$ with zero mean and diagonal covariance $\\Sigma_{0} = \\mathrm{diag}(\\tau^{2}, \\tau^{2}, \\tau_{3}^{2})$, where $\\tau^{2}  0$ and $\\tau_{3}^{2}  0$ are finite, and consider the limit $\\tau^{2} \\to \\infty$ to model a noninformative prior on $(\\theta_{1}, \\theta_{2})$.\n\n1. Using only Bayes’ rule for Gaussian densities and linear mappings, explain why $(\\theta_{1}, \\theta_{2})$ are unidentifiable from the first experiment as $\\tau^{2} \\to \\infty$, in the sense that the posterior distribution over $(\\theta_{1}, \\theta_{2})$ becomes degenerate along at least one direction. Justify your conclusion by characterizing the rank of the contribution of the likelihood to the posterior precision for $(\\theta_{1}, \\theta_{2})$.\n\n2. To resolve this ambiguity, you redesign the experiment by adding a second, independent run with a modified mapping $M^{\\prime}(\\theta)$ and observation operator $H^{\\prime}$:\n$$\nx^{\\prime} = M^{\\prime}(\\theta) \\equiv \\begin{pmatrix} \\theta_{1} - \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix}, \\qquad y^{\\prime} = H^{\\prime} x^{\\prime} + v^{\\prime}, \\quad H^{\\prime} \\equiv \\begin{pmatrix} 1  0 \\end{pmatrix},\n$$\nwith $v^{\\prime} \\sim \\mathcal{N}(0, \\sigma^{2})$ independent of $v$, and the same prior on $\\theta$ as above. Explain briefly why this change eliminates the symmetry that caused unidentifiability.\n\n3. Consider two estimation strategies:\n- Joint strategy: estimate $(x, x^{\\prime}, \\theta)$ jointly from $(y, y^{\\prime})$ in a single Bayesian update.\n- Dual strategy: first eliminate the states $(x, x^{\\prime})$ analytically using their deterministic definitions in terms of $\\theta$ and then estimate $\\theta$ directly from $(y, y^{\\prime})$.\n\nArgue, without using any algorithm-specific formulas, that both strategies must yield the same posterior distribution for $\\theta$ in this linear-Gaussian setting.\n\nCompute, in closed form, the marginal posterior variance of $\\theta_{1}$ after the second experiment is included, in the limit $\\tau^{2} \\to \\infty$ with fixed $\\sigma^{2}  0$ and fixed $\\tau_{3}^{2}  0$. Express your final answer as a symbolic function of $\\sigma^{2}$. Do not round your result. The final answer must be a single analytic expression with no units.",
            "solution": "The problem asks for an analysis of parameter identifiability in a linear-Gaussian system and the computation of a posterior variance. We will address each of the four parts sequentially.\n\nFirst, let us establish the reduced observation model, which directly relates the parameters $\\theta$ to the observations $y$ and $y^{\\prime}$. This is an instance of the dual strategy mentioned in the problem, which simplifies the analysis.\n\nFor the first experiment, the state is $x = M(\\theta) = \\begin{pmatrix} \\theta_{1} + \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix}$. The observation is $y = Hx + v = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\theta_{1} + \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix} + v = \\theta_{1} + \\theta_{2} + v$. This can be written in the form $y = K\\theta + v$ where the forward operator is $K = \\begin{pmatrix} 1  1  0 \\end{pmatrix}$. The noise $v$ is Gaussian, $v \\sim \\mathcal{N}(0, \\sigma^2)$, with covariance $R = \\sigma^2$.\n\nThe prior on $\\theta$ is $p(\\theta) \\sim \\mathcal{N}(0, \\Sigma_0)$ with $\\Sigma_0 = \\mathrm{diag}(\\tau^{2}, \\tau^{2}, \\tau_{3}^{2})$.\nThe posterior distribution for $\\theta$ given $y$ is also Gaussian, $p(\\theta|y) \\sim \\mathcal{N}(\\mu_{post}, \\Sigma_{post})$, with inverse covariance (precision) matrix given by the standard Bayesian update formula for linear-Gaussian models:\n$$\n\\Sigma_{post}^{-1} = \\Sigma_0^{-1} + K^{\\top}R^{-1}K\n$$\nThe prior precision matrix is $\\Sigma_0^{-1} = \\mathrm{diag}(\\frac{1}{\\tau^2}, \\frac{1}{\\tau^2}, \\frac{1}{\\tau_3^2})$.\nThe term $K^{\\top}R^{-1}K$ represents the information gained from the measurement, i.e., the contribution of the likelihood to the posterior precision.\n\n1. Unidentifiability in the first experiment.\nThe contribution from the likelihood is:\n$$\nK^{\\top}R^{-1}K = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} (\\sigma^2)^{-1} \\begin{pmatrix} 1  1  0 \\end{pmatrix} = \\frac{1}{\\sigma^2} \\begin{pmatrix} 1  1  0 \\\\ 1  1  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\nThe question concerns the identifiability of $(\\theta_1, \\theta_2)$. We examine the upper-left $2 \\times 2$ block of this precision contribution, which corresponds to these parameters. This block is $\\frac{1}{\\sigma^2}\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$. The rank of this matrix is $1$, since its determinant is $0$ and it is not the zero matrix. A rank of $1$ for a $2 \\times 2$ matrix implies there is a one-dimensional null space.\nThe posterior precision for $\\theta$ is:\n$$\n\\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}  \\frac{1}{\\sigma^2}  0 \\\\ \\frac{1}{\\sigma^2}  \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\nIn the limit $\\tau^2 \\to \\infty$, the prior on $(\\theta_1, \\theta_2)$ becomes noninformative, and the prior precision terms $\\frac{1}{\\tau^2}$ go to $0$. The posterior precision matrix for $\\theta$ converges to:\n$$\n\\lim_{\\tau^2 \\to \\infty} \\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma^2}  \\frac{1}{\\sigma^2}  0 \\\\ \\frac{1}{\\sigma^2}  \\frac{1}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\nThe $2 \\times 2$ submatrix for $(\\theta_1, \\theta_2)$ is $\\frac{1}{\\sigma^2}\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$. This matrix is singular. A singular precision matrix corresponds to an infinite variance (and hence a degenerate, or improper, posterior distribution) along the direction(s) of its null space. The null space of this matrix is spanned by the vector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. This means that the observation $y$ provides no information about the combination $\\theta_1 - \\theta_2$. The data only constrains the sum $\\theta_1 + \\theta_2$. Consequently, $(\\theta_1, \\theta_2)$ are unidentifiable; any pair $(\\theta_1+c, \\theta_2-c)$ for an arbitrary constant $c$ gives the same sum and is thus equally likely given the data.\n\n2. Resolution of ambiguity with the second experiment.\nThe second experiment has a state $x' = M'(\\theta) = \\begin{pmatrix} \\theta_1 - \\theta_2 \\\\ \\theta_3 \\end{pmatrix}$ and observation $y' = H'x' + v' = \\theta_1 - \\theta_2 + v'$. This corresponds to a reduced model $y' = K'\\theta + v'$ with $K' = \\begin{pmatrix} 1  -1  0 \\end{pmatrix}$.\nSince the measurements $(y, y')$ are independent, the total likelihood is the product of individual likelihoods. In the linear-Gaussian framework, this means the precision contributions add up. Let $\\mathbf{y} = \\begin{pmatrix} y \\\\ y' \\end{pmatrix}$, $\\mathbf{v} = \\begin{pmatrix} v \\\\ v' \\end{pmatrix}$, and $\\mathbf{K} = \\begin{pmatrix} K \\\\ K' \\end{pmatrix} = \\begin{pmatrix} 1  1  0 \\\\ 1  -1  0 \\end{pmatrix}$. The noise covariance is $\\mathbf{R} = \\mathrm{diag}(\\sigma^2, \\sigma^2)$.\nThe total likelihood contribution to the posterior precision is $\\mathbf{K}^{\\top}\\mathbf{R}^{-1}\\mathbf{K}$:\n$$\n\\mathbf{K}^{\\top}\\mathbf{R}^{-1}\\mathbf{K} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sigma^2}  0 \\\\ 0  \\frac{1}{\\sigma^2} \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 1  -1  0 \\end{pmatrix} = \\frac{1}{\\sigma^2} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 1  -1  0 \\end{pmatrix} = \\frac{1}{\\sigma^2} \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\nThe $2 \\times 2$ submatrix corresponding to $(\\theta_1, \\theta_2)$ is now $\\frac{2}{\\sigma^2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$. This is a diagonal, full-rank matrix (rank $2$). Even in the limit $\\tau^2 \\to \\infty$, the posterior precision for $(\\theta_1, \\theta_2)$ is non-singular. The ambiguity is resolved because the first experiment provides information on $\\theta_1 + \\theta_2$, while the second provides information on $\\theta_1 - \\theta_2$. Together, they constrain both $\\theta_1$ and $\\theta_2$ individually.\n\n3. Equivalence of joint and dual strategies.\nThis equivalence is a fundamental property of Bayesian inference that follows from the rules of probability theory.\nThe joint strategy considers an augmented state vector including parameters and states, e.g., $\\mathbf{z} = (x^{\\top}, x'^{\\top}, \\theta^{\\top})^{\\top}$. The goal is to find the marginal posterior for the parameters, $p(\\theta|y, y')$. This is obtained by first finding the joint posterior $p(x, x', \\theta|y, y')$ and then integrating out the states $x$ and $x'$:\n$$\np(\\theta|y, y') = \\iint p(x, x', \\theta|y, y') \\, dx \\, dx'\n$$\nUsing Bayes' rule, the joint posterior is $p(x, x', \\theta|y, y') \\propto p(y, y'|x, x', \\theta) p(x, x', \\theta)$. The full joint distribution is $p(y, y', x, x', \\theta) = p(y, y'|x, x', \\theta) p(x, x'|\\theta) p(\\theta)$. Due to the model structure, $y$ depends only on $x$ and $y'$ depends only on $x'$, so $p(y, y'|x, x', \\theta) = p(y|x)p(y'|x')$. The states are deterministic functions of $\\theta$, so their conditional probabilities are Dirac delta functions: $p(x|\\theta) = \\delta(x - M(\\theta))$ and $p(x'|\\theta) = \\delta(x' - M'(\\theta))$. Thus,\n$$\np(\\theta|y, y') \\propto p(\\theta) \\iint p(y|x) p(y'|x') \\delta(x - M(\\theta)) \\delta(x' - M'(\\theta)) \\, dx \\, dx'\n$$\nThe integration over the delta functions simply substitutes the deterministic relationships into the likelihoods:\n$$\np(\\theta|y, y') \\propto p(\\theta) p(y|M(\\theta)) p(y'|M'(\\theta))\n$$\nThe dual strategy starts by substituting the state definitions into the observation model, creating a reduced model that directly links parameters to observations: $y = H M(\\theta) + v$ and $y' = H' M'(\\theta) + v'$. The likelihood is then written directly as a function of $\\theta$, $p(y, y'|\\theta) = p(y|\\theta) p(y'|\\theta)$. By Bayes' rule, the posterior is $p(\\theta|y, y') \\propto p(y, y'|\\theta) p(\\theta) = p(y|\\theta) p(y'|\\theta) p(\\theta)$. Since $p(y|\\theta)$ is equivalent to $p(y|M(\\theta))$, the resulting expressions for the posterior are proportional to the identical function of $\\theta$. Therefore, the two strategies must yield the same posterior distribution. In the linear-Gaussian case, this means they result in the same posterior mean and covariance.\n\n4. Computation of the marginal posterior variance of $\\theta_{1}$.\nThe posterior precision for $\\theta$ after both experiments is $\\Sigma_{post}^{-1} = \\Sigma_0^{-1} + \\mathbf{K}^{\\top}\\mathbf{R}^{-1}\\mathbf{K}$.\n$$\n\\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{1}{\\tau^2}  0  0 \\\\ 0  \\frac{1}{\\tau^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix} + \\frac{1}{\\sigma^2} \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\tau^2} + \\frac{2}{\\sigma^2}  0  0 \\\\ 0  \\frac{1}{\\tau^2} + \\frac{2}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\nWe take the limit as $\\tau^2 \\to \\infty$:\n$$\n\\Sigma_{post, \\infty}^{-1} = \\lim_{\\tau^2 \\to \\infty} \\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{2}{\\sigma^2}  0  0 \\\\ 0  \\frac{2}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\nThe posterior covariance matrix $\\Sigma_{post, \\infty}$ is the inverse of the posterior precision matrix. Since the precision matrix is diagonal, its inverse is a diagonal matrix whose elements are the reciprocals of the original diagonal elements:\n$$\n\\Sigma_{post, \\infty} = (\\Sigma_{post, \\infty}^{-1})^{-1} = \\begin{pmatrix} \\frac{\\sigma^2}{2}  0  0 \\\\ 0  \\frac{\\sigma^2}{2}  0 \\\\ 0  0  \\tau_3^2 \\end{pmatrix}\n$$\nThe marginal posterior variance of a parameter is the corresponding diagonal element of the posterior covariance matrix. Therefore, the marginal posterior variance of $\\theta_1$ is the $(1,1)$ element of $\\Sigma_{post, \\infty}$.\n$$\n\\mathrm{Var}(\\theta_1 | y, y') = (\\Sigma_{post, \\infty})_{11} = \\frac{\\sigma^2}{2}\n$$\nThis result is a function of $\\sigma^2$ as required.",
            "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2}}$$"
        },
        {
            "introduction": "This final practice moves from idealized theory into the realm of complex, chaotic dynamics, where the assumptions of linearity and Gaussianity break down and numerical implementation becomes key. You will use the Lorenz-96 model, a canonical testbed for data assimilation methods, to compare the performance of joint and dual strategies within an Ensemble Kalman Filter (EnKF) framework. This computational exercise  is designed to develop your skills in implementing and diagnosing advanced filters, forcing you to confront practical challenges like filter stability and the impact of model error on parameter tracking.",
            "id": "3421568",
            "problem": "You are asked to implement, compare, and analyze the stability of two Ensemble Kalman Filter (EnKF) strategies—joint and dual—for simultaneous state-parameter estimation on a discrete-time Lorenz-96 system. The Lorenz-96 dynamics is given by the ordinary differential equation\n$$\n\\frac{dx_i}{dt} = \\left( x_{i+1} - x_{i-2} \\right) x_{i-1} - x_i + F,\n$$\nwith cyclic indices, discretized via a fixed-step fourth-order Runge-Kutta scheme to define the mapping\n$$\nx_{t+1} = M(x_t, \\theta_t) + \\eta_t,\n$$\nwhere $x_t \\in \\mathbb{R}^N$ is the state vector, $\\theta_t \\in \\mathbb{R}$ is a scalar, time-varying parameter that plays the role of the forcing $F$, $M(\\cdot)$ is the one-step numerical integrator, and $\\eta_t$ is additive model noise. The parameter evolves as a random walk\n$$\n\\theta_{t+1} = \\theta_t + \\xi_t,\n$$\nwhere $\\xi_t$ is Gaussian noise with variance $Q_\\theta$. Observations are linear and partial:\n$$\ny_t = H x_t + \\varepsilon_t,\n$$\nwith $H \\in \\mathbb{R}^{m \\times N}$ a selection operator, and $\\varepsilon_t$ Gaussian observation noise with covariance $R$.\n\nThe fundamental base for the solution consists of: (i) the state-space model definition, (ii) the Gaussian Bayesian update for linear observation operators,\n$$\nx^a = x^f + K(y - H x^f), \\quad K = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\nwhere $x^f$ denotes forecast, $x^a$ analysis, $P_{xy}$ the cross-covariance between state (or augmented state) and predicted observations, and $P_{yy}$ the covariance of predicted observations; and (iii) the Ensemble Kalman Filter principle, which approximates covariances by sample covariances of an ensemble.\n\nYou must implement two strategies:\n- Joint EnKF: augment the state with the parameter to form $z_t = [x_t; \\theta_t]$, forecast both with their respective dynamics, and perform a single Kalman analysis on the augmented ensemble using the linear observation operator on the state components only.\n- Dual EnKF: perform two sequential analyses per time step. First, update only the state using the state-observation cross-covariance. Second, update only the parameter using the parameter-observation cross-covariance. Each update uses independently perturbed observations consistent with $R$.\n\nUse the following fixed settings:\n- State dimension $N=10$.\n- Time step $dt=0.05$.\n- Number of assimilation steps $T=120$.\n- Observation operator $H$ selects every other state component starting at index $0$, so $m=5$.\n- True initial parameter $\\theta_0 = 8.0$.\n- True initial state $x_0$ drawn as $x_{0,i} = \\theta_0 + \\delta_i$ with $\\delta_i \\sim \\mathcal{N}(0,1)$ independently for $i=1,\\dots,N$.\n- State model noise variance $Q_x = 0.01$ (diagonal covariance).\n- Observation noise covariance $R = \\sigma_{\\text{obs}}^2 I_m$ with $\\sigma_{\\text{obs}} = 1.0$.\n- Ensemble size $N_e = 25$.\n- Initial ensemble mean guesses: $x_0^{\\text{guess}} = x_0 + \\epsilon$ with $\\epsilon_i \\sim \\mathcal{N}(0,1)$ and $\\theta_0^{\\text{guess}} = \\theta_0 - 1.0$, with ensemble spread $1.0$ in state and $0.5$ in parameter.\n\nConsider the parameter process noise variance $Q_\\theta$ as the variable under study. For each value of $Q_\\theta$ in the test suite given below, generate a single \"true\" trajectory and a corresponding observation sequence using that $Q_\\theta$ in the true parameter evolution. Then, run both filters with the same $Q_\\theta$ used in their parameter forecast models. For each strategy and each $Q_\\theta$, compute the time-averaged root mean square error (RMSE) over the assimilation window for the state\n$$\n\\mathrm{RMSE}_x = \\frac{1}{T} \\sum_{t=1}^{T} \\left\\| x_t - \\bar{x}_t \\right\\|_2 \\big/ \\sqrt{N},\n$$\nand for the parameter\n$$\n\\mathrm{RMSE}_\\theta = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\theta_t - \\bar{\\theta}_t \\right|,\n$$\nwhere $\\bar{x}_t$ and $\\bar{\\theta}_t$ are the ensemble means at time $t$ after analysis.\n\nDefine stability for a filter at a given $Q_\\theta$ as follows: the filter is stable if it produces finite values at all times and satisfies $\\mathrm{RMSE}_x \\le \\tau_x$ and $\\mathrm{RMSE}_\\theta \\le \\tau_\\theta$, with thresholds $\\tau_x = 3.0$ and $\\tau_\\theta = 2.0$. The stability threshold for a strategy over the test suite is defined as the largest $Q_\\theta$ value within the suite for which the filter is stable.\n\nTest suite for parameter process noise variance $Q_\\theta$:\n- Case $1$: $Q_\\theta = 0.0$.\n- Case $2$: $Q_\\theta = 0.001$.\n- Case $3$: $Q_\\theta = 0.01$.\n- Case $4$: $Q_\\theta = 0.05$.\n- Case $5$: $Q_\\theta = 0.1$.\n- Case $6$: $Q_\\theta = 0.5$.\n\nYour program must:\n- Implement the Lorenz-96 model integration $M(\\cdot)$ with cyclic indices and fourth-order Runge-Kutta for the given $dt$.\n- Implement the joint and dual EnKF strategies as described, using stochastic EnKF with perturbed observations.\n- For each $Q_\\theta$ in the test suite, compute stability booleans for joint and dual filters.\n- Compute the stability thresholds for both filters as the maximum $Q_\\theta$ from the suite for which each filter is stable; if none is stable, report $0.0$.\n\nFinal output specification:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets of the form\n$$\n[\\text{JT}, \\text{DT}, [s^{(J)}_1,\\dots,s^{(J)}_6], [s^{(D)}_1,\\dots,s^{(D)}_6]],\n$$\nwhere $\\text{JT}$ is the joint filter stability threshold as a float, $\\text{DT}$ is the dual filter stability threshold as a float, $s^{(J)}_k$ is the boolean stability for the joint filter at test case $k$, and $s^{(D)}_k$ is the corresponding boolean for the dual filter. No other text should be printed.",
            "solution": "The Lorenz-96 system is a canonical chaotic model used for data assimilation studies. We begin by specifying the dynamical and observational models. The discrete-time mapping $M(\\cdot)$ is obtained by integrating the ordinary differential equation\n$$\n\\frac{dx_i}{dt} = \\left( x_{i+1} - x_{i-2} \\right) x_{i-1} - x_i + F\n$$\nover a time step $dt$ using a fourth-order Runge-Kutta scheme. The indices are cyclic, meaning $x_{-1} = x_{N-1}$ and $x_{N} = x_0$, and so on, ensuring periodic boundary conditions.\n\nThe stochastic state-space model in discrete time is\n$$\nx_{t+1} = M(x_t, \\theta_t) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}\\left(0, Q_x I_N\\right),\n$$\nand the parameter evolves as\n$$\n\\theta_{t+1} = \\theta_t + \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0, Q_\\theta).\n$$\nObservations are given by\n$$\ny_t = H x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}\\left(0, R\\right),\n$$\nwith $H \\in \\mathbb{R}^{m \\times N}$ selecting a subset of components. We take $m = N/2$ by selecting indices $0,2,4,\\dots$.\n\nThe Ensemble Kalman Filter (EnKF) is based on the Gaussian Bayesian update for linear observation operators, with covariances estimated from an ensemble. In the classical linear Gaussian case, the Kalman analysis satisfies\n$$\nx^a = x^f + K(y - H x^f), \\quad K = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\nwhere $P_{xy}$ is the cross-covariance between the forecast state and forecast observation, and $P_{yy}$ is the covariance of the forecast observation. In the EnKF, given an ensemble $\\{x^{f,(k)}\\}_{k=1}^{N_e}$, these are approximated by sample covariances\n$$\n\\bar{x}^f = \\frac{1}{N_e} \\sum_{k=1}^{N_e} x^{f,(k)}, \\quad X' = \\left[x^{f,(1)} - \\bar{x}^f, \\dots, x^{f,(N_e)} - \\bar{x}^f\\right],\n$$\n$$\ny^{f,(k)} = H x^{f,(k)}, \\quad \\bar{y}^f = \\frac{1}{N_e} \\sum_{k=1}^{N_e} y^{f,(k)}, \\quad Y' = \\left[y^{f,(1)} - \\bar{y}^f, \\dots, y^{f,(N_e)} - \\bar{y}^f\\right],\n$$\n$$\nP_{xy} \\approx \\frac{1}{N_e-1} X' (Y')^\\top, \\quad P_{yy} \\approx \\frac{1}{N_e-1} Y' (Y')^\\top.\n$$\nIn the stochastic EnKF, the analysis is performed member-wise with perturbed observations\n$$\ny^{(k)} = y + \\varepsilon^{(k)}, \\quad \\varepsilon^{(k)} \\sim \\mathcal{N}(0,R),\n$$\nand each member is updated by\n$$\nx^{a,(k)} = x^{f,(k)} + K\\left(y^{(k)} - y^{f,(k)}\\right).\n$$\n\nFor joint state-parameter estimation, we consider the augmented state\n$$\nz_t = \\begin{bmatrix} x_t \\\\ \\theta_t \\end{bmatrix} \\in \\mathbb{R}^{N+1},\n$$\nwith forecast ensemble members obtained by applying their respective dynamics:\n$$\nx_{t+1}^{f,(k)} = M\\left(x_t^{a,(k)}, \\theta_t^{a,(k)}\\right) + \\eta_t^{(k)}, \\quad \\theta_{t+1}^{f,(k)} = \\theta_t^{a,(k)} + \\xi_t^{(k)}.\n$$\nWe form the augmented anomalies and observation anomalies as before, then compute\n$$\nP_{zy} \\approx \\frac{1}{N_e-1} Z' (Y')^\\top, \\quad P_{yy} \\approx \\frac{1}{N_e-1} Y' (Y')^\\top,\n$$\nand the augmented Kalman gain\n$$\nK_z = P_{zy}\\left(P_{yy} + R\\right)^{-1}.\n$$\nThe joint analysis update for each ensemble member is\n$$\nz^{a,(k)} = z^{f,(k)} + K_z\\left(y^{(k)} - y^{f,(k)}\\right),\n$$\nwhere $y^{f,(k)} = H x^{f,(k)}$ and the observation perturbations $\\varepsilon^{(k)}$ are drawn independently.\n\nFor dual EnKF, we perform two sequential analyses. First, a state-only analysis using the state-observation cross-covariance\n$$\nP_{xy} \\approx \\frac{1}{N_e-1} X' (Y')^\\top, \\quad K_x = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\nwith the member-wise update\n$$\nx^{a,(k)} = x^{f,(k)} + K_x\\left(y^{(k)} - y^{f,(k)}\\right),\n$$\nleaving $\\theta^{f,(k)}$ unchanged in this step. Second, a parameter-only analysis driven by the parameter-observation cross-covariance\n$$\nP_{\\theta y} \\approx \\frac{1}{N_e-1} \\Theta' (Y')^\\top,\n$$\nwhere $\\Theta' = [\\theta^{f,(1)} - \\bar{\\theta}^f, \\dots, \\theta^{f,(N_e)} - \\bar{\\theta}^f]$ is the $1 \\times N_e$ anomaly vector for parameter values. The parameter gain is\n$$\nK_\\theta = P_{\\theta y}\\left(P_{yy} + R\\right)^{-1},\n$$\nand the member-wise parameter update is\n$$\n\\theta^{a,(k)} = \\theta^{f,(k)} + K_\\theta\\left(y^{(k)} - y^{f,(k)}\\right).\n$$\nIn this dual approach, the predicted observations $y^{f,(k)}$ and their anomalies are computed from the forecast states; using them for both state and parameter updates ensures consistent use of the same innovations.\n\nTo assess stability, we compute the time-averaged root mean square errors:\n$$\n\\mathrm{RMSE}_x = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\left\\| x_t - \\bar{x}_t \\right\\|_2}{\\sqrt{N}}, \\quad \\mathrm{RMSE}_\\theta = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\theta_t - \\bar{\\theta}_t \\right|.\n$$\nWe declare a filter stable for a given $Q_\\theta$ if all ensemble analyses yield finite numbers (no Not-a-Number values or infinities) and the thresholds $\\tau_x = 3.0$ and $\\tau_\\theta = 2.0$ are satisfied:\n$$\n\\mathrm{RMSE}_x \\le \\tau_x, \\quad \\mathrm{RMSE}_\\theta \\le \\tau_\\theta.\n$$\nThe stability threshold over the test suite is defined as the maximum $Q_\\theta$ among the tested values for which stability holds.\n\nAlgorithmic design details:\n- The Lorenz-96 right-hand side is evaluated with cyclic indexing to preserve the dynamics.\n- The forecast noise for the state uses $Q_x I_N$ added after the numerical integration, and the parameter forecast uses $Q_\\theta$ as variance for the random walk.\n- The stochastic EnKF is used with perturbed observations $\\varepsilon^{(k)} \\sim \\mathcal{N}(0,R)$ drawn independently for each member and update.\n- Sample covariances are computed via anomalies, and observation covariance $P_{yy}$ is regularized implicitly by the positive definite $R$; a small numerical jitter can be added to the diagonal if necessary for numerical stability.\n- The observation operator $H$ is a linear selector of half the state components, ensuring a nontrivial partial observation scenario.\n- The program loops over the specified $Q_\\theta$ values, generates a consistent truth and observation sequence per case, runs both filters, and aggregates stability results.\n\nFinally, the program prints a single line with the joint threshold, dual threshold, and two lists of booleans marking stability per test case, in the exact format required:\n$$\n[\\text{JT}, \\text{DT}, [s^{(J)}_1,\\dots,s^{(J)}_6], [s^{(D)}_1,\\dots,s^{(D)}_6]].\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lorenz96_rhs(x, F):\n    \"\"\"\n    Compute the Lorenz-96 right-hand side for state x and forcing F.\n    x: array of shape (N,)\n    F: scalar forcing\n    Returns dx/dt: array of shape (N,)\n    \"\"\"\n    N = x.shape[0]\n    dx = np.empty_like(x)\n    # cyclic indices: x_{i+1}, x_{i-2}, x_{i-1}\n    for i in range(N):\n        xp1 = x[(i + 1) % N]\n        xm2 = x[(i - 2) % N]\n        xm1 = x[(i - 1) % N]\n        dx[i] = (xp1 - xm2) * xm1 - x[i] + F\n    return dx\n\ndef rk4_step(x, F, dt):\n    \"\"\"\n    One RK4 step for Lorenz-96.\n    \"\"\"\n    k1 = lorenz96_rhs(x, F)\n    k2 = lorenz96_rhs(x + 0.5 * dt * k1, F)\n    k3 = lorenz96_rhs(x + 0.5 * dt * k2, F)\n    k4 = lorenz96_rhs(x + dt * k3, F)\n    return x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n\ndef build_observation_matrix(N):\n    \"\"\"\n    Build H selecting every other component starting at index 0.\n    Returns H of shape (m, N), where m = N//2 for even N.\n    \"\"\"\n    indices = np.arange(0, N, 2)\n    m = len(indices)\n    H = np.zeros((m, N))\n    for row, idx in enumerate(indices):\n        H[row, idx] = 1.0\n    return H\n\ndef generate_truth(N, T, dt, F0, Q_theta, Q_x, H, R, rng):\n    \"\"\"\n    Generate true trajectory (X_true, Theta_true) and observations Y.\n    \"\"\"\n    X_true = np.zeros((T+1, N))\n    Theta_true = np.zeros(T+1)\n    # Initial true state around F0\n    X_true[0] = F0 + rng.normal(0.0, 1.0, size=N)\n    Theta_true[0] = F0\n\n    for t in range(T):\n        F_t = Theta_true[t]\n        x_next = rk4_step(X_true[t], F_t, dt)\n        # Add model noise\n        x_next = x_next + rng.normal(0.0, np.sqrt(Q_x), size=N)\n        X_true[t+1] = x_next\n        # Parameter random walk\n        Theta_true[t+1] = Theta_true[t] + rng.normal(0.0, np.sqrt(Q_theta))\n\n    # Observations for t=1..T corresponding to X_true[t]\n    m = H.shape[0]\n    Y = np.zeros((T, m))\n    for t in range(1, T+1):\n        y = H @ X_true[t]\n        y = y + rng.normal(0.0, np.sqrt(np.diag(R)), size=m)\n        Y[t-1] = y\n    return X_true, Theta_true, Y\n\ndef enkf_joint(N, T, dt, Q_theta, Q_x, H, R, Y, X_true, Theta_true, x0_guess, theta0_guess, Ne, rng):\n    \"\"\"\n    Joint EnKF on augmented state [x; theta].\n    Returns (rmse_x, rmse_theta, stable_boolean).\n    \"\"\"\n    m = H.shape[0]\n    # Initialize ensemble\n    x_ens = x0_guess + rng.normal(0.0, 1.0, size=(Ne, N))\n    theta_ens = theta0_guess + rng.normal(0.0, 0.5, size=Ne)\n\n    # Storage for ensemble means\n    x_mean = np.zeros((T+1, N))\n    theta_mean = np.zeros(T+1)\n    x_mean[0] = np.mean(x_ens, axis=0)\n    theta_mean[0] = np.mean(theta_ens)\n\n    # Precompute sqrt covariances for obs perturbations\n    obs_std = np.sqrt(np.diag(R))\n\n    stable = True\n    eps_jitter = 1e-9\n\n    for t in range(T):\n        # Forecast step\n        x_f = np.zeros_like(x_ens)\n        theta_f = np.zeros_like(theta_ens)\n        for k in range(Ne):\n            x_f[k] = rk4_step(x_ens[k], theta_ens[k], dt)\n            x_f[k] += rng.normal(0.0, np.sqrt(Q_x), size=N)\n            theta_f[k] = theta_ens[k] + rng.normal(0.0, np.sqrt(Q_theta))\n        # Predicted observations\n        y_pred = (H @ x_f.T).T  # shape (Ne, m)\n\n        # Compute anomalies for augmented state and obs\n        x_f_mean = np.mean(x_f, axis=0)\n        theta_f_mean = np.mean(theta_f)\n        y_mean = np.mean(y_pred, axis=0)\n\n        X_anom = x_f - x_f_mean  # (Ne, N)\n        theta_anom = theta_f - theta_f_mean  # (Ne,)\n        # Build augmented anomalies Z' of shape (Ne, N+1)\n        Z_anom = np.hstack([X_anom, theta_anom[:, None]])\n        Y_anom = y_pred - y_mean  # (Ne, m)\n\n        # Covariances\n        P_zy = (Z_anom.T @ Y_anom) / (Ne - 1)  # (N+1, m)\n        P_yy = (Y_anom.T @ Y_anom) / (Ne - 1) + R + eps_jitter * np.eye(m)  # (m, m)\n\n        # Solve for gain K = P_zy @ inv(P_yy)\n        try:\n            K = np.linalg.solve(P_yy.T, P_zy.T).T  # more stable than inv; solve P_yy^T * X^T = P_zy^T\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        # Analysis with perturbed observations\n        x_a = np.zeros_like(x_f)\n        theta_a = np.zeros_like(theta_f)\n        y_obs = Y[t]  # observation at time t+1\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            update = K @ innov  # (N+1,)\n            x_a[k] = x_f[k] + update[:N]\n            theta_a[k] = theta_f[k] + update[N]\n\n        # Check for numerical issues\n        if not (np.isfinite(x_a).all() and np.isfinite(theta_a).all()):\n            stable = False\n            break\n\n        # Prepare for next step\n        x_ens = x_a\n        theta_ens = theta_a\n\n        # Save means\n        x_mean[t+1] = np.mean(x_ens, axis=0)\n        theta_mean[t+1] = np.mean(theta_ens)\n\n    # Compute RMSEs\n    if not stable:\n        return np.inf, np.inf, False\n\n    # Time-averaged RMSE over t=1..T\n    rmse_x = 0.0\n    rmse_theta = 0.0\n    for t in range(1, T+1):\n        rmse_x += np.linalg.norm(X_true[t] - x_mean[t]) / np.sqrt(N)\n        rmse_theta += abs(Theta_true[t] - theta_mean[t])\n    rmse_x /= T\n    rmse_theta /= T\n\n    # Stability thresholds\n    tau_x = 3.0\n    tau_theta = 2.0\n    stable = np.isfinite(rmse_x) and np.isfinite(rmse_theta) and (rmse_x = tau_x) and (rmse_theta = tau_theta)\n    return rmse_x, rmse_theta, stable\n\ndef enkf_dual(N, T, dt, Q_theta, Q_x, H, R, Y, X_true, Theta_true, x0_guess, theta0_guess, Ne, rng):\n    \"\"\"\n    Dual EnKF: sequential state update then parameter update per time step.\n    Returns (rmse_x, rmse_theta, stable_boolean).\n    \"\"\"\n    m = H.shape[0]\n    # Initialize ensemble\n    x_ens = x0_guess + rng.normal(0.0, 1.0, size=(Ne, N))\n    theta_ens = theta0_guess + rng.normal(0.0, 0.5, size=Ne)\n\n    x_mean = np.zeros((T+1, N))\n    theta_mean = np.zeros(T+1)\n    x_mean[0] = np.mean(x_ens, axis=0)\n    theta_mean[0] = np.mean(theta_ens)\n\n    obs_std = np.sqrt(np.diag(R))\n    stable = True\n    eps_jitter = 1e-9\n\n    for t in range(T):\n        # Forecast step\n        x_f = np.zeros_like(x_ens)\n        theta_f = np.zeros_like(theta_ens)\n        for k in range(Ne):\n            x_f[k] = rk4_step(x_ens[k], theta_ens[k], dt)\n            x_f[k] += rng.normal(0.0, np.sqrt(Q_x), size=N)\n            theta_f[k] = theta_ens[k] + rng.normal(0.0, np.sqrt(Q_theta))\n\n        y_pred = (H @ x_f.T).T  # (Ne, m)\n        y_mean = np.mean(y_pred, axis=0)\n        Y_anom = y_pred - y_mean  # (Ne, m)\n\n        # State update\n        x_f_mean = np.mean(x_f, axis=0)\n        X_anom = x_f - x_f_mean  # (Ne, N)\n        P_xy = (X_anom.T @ Y_anom) / (Ne - 1)  # (N, m)\n        P_yy = (Y_anom.T @ Y_anom) / (Ne - 1) + R + eps_jitter * np.eye(m)\n        try:\n            K_x = np.linalg.solve(P_yy.T, P_xy.T).T  # (N, m)\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        y_obs = Y[t]\n        x_a = np.zeros_like(x_f)\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            x_a[k] = x_f[k] + K_x @ innov\n\n        # Parameter update using forecast obs anomalies (dual strategy)\n        theta_f_mean = np.mean(theta_f)\n        theta_anom = theta_f - theta_f_mean  # (Ne,)\n        # P_{theta y}: (1 x m)\n        P_thetay = (theta_anom[:, None].T @ Y_anom) / (Ne - 1)  # (1, m)\n        try:\n            K_theta = np.linalg.solve(P_yy.T, P_thetay.T).T  # (1, m)\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        theta_a = np.zeros_like(theta_f)\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            theta_a[k] = theta_f[k] + float(K_theta @ innov)\n\n        # Check for numerical issues\n        if not (np.isfinite(x_a).all() and np.isfinite(theta_a).all()):\n            stable = False\n            break\n\n        # Prepare next step\n        x_ens = x_a\n        theta_ens = theta_a\n\n        x_mean[t+1] = np.mean(x_ens, axis=0)\n        theta_mean[t+1] = np.mean(theta_ens)\n\n    if not stable:\n        return np.inf, np.inf, False\n\n    rmse_x = 0.0\n    rmse_theta = 0.0\n    for t in range(1, T+1):\n        rmse_x += np.linalg.norm(X_true[t] - x_mean[t]) / np.sqrt(N)\n        rmse_theta += abs(Theta_true[t] - theta_mean[t])\n    rmse_x /= T\n    rmse_theta /= T\n\n    tau_x = 3.0\n    tau_theta = 2.0\n    stable = np.isfinite(rmse_x) and np.isfinite(rmse_theta) and (rmse_x = tau_x) and (rmse_theta = tau_theta)\n    return rmse_x, rmse_theta, stable\n\ndef solve():\n    rng = np.random.default_rng(42)\n\n    # Fixed settings\n    N = 10\n    dt = 0.05\n    T = 120\n    Ne = 25\n    Q_x = 0.01\n    sigma_obs = 1.0\n    R = (sigma_obs ** 2) * np.eye(N // 2)\n    H = build_observation_matrix(N)\n    F0 = 8.0\n\n    # Test suite of Q_theta values\n    test_cases = [0.0, 0.001, 0.01, 0.05, 0.1, 0.5]\n\n    stable_joint = []\n    stable_dual = []\n\n    # For threshold computation\n    joint_stable_Qs = []\n    dual_stable_Qs = []\n\n    # Run through test cases\n    for Q_theta in test_cases:\n        # Re-seed for reproducibility per case\n        case_rng = np.random.default_rng(1000 + int(Q_theta * 1e6))\n\n        # Generate truth and observations\n        X_true, Theta_true, Y = generate_truth(\n            N=N, T=T, dt=dt, F0=F0, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, rng=case_rng\n        )\n\n        # Initial guesses\n        x0_guess = X_true[0] + rng.normal(0.0, 1.0, size=N)\n        theta0_guess = F0 - 1.0\n\n        # Joint EnKF\n        rmse_x_j, rmse_th_j, st_j = enkf_joint(\n            N=N, T=T, dt=dt, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, Y=Y,\n            X_true=X_true, Theta_true=Theta_true,\n            x0_guess=x0_guess, theta0_guess=theta0_guess, Ne=Ne, rng=np.random.default_rng(2000 + int(Q_theta * 1e6))\n        )\n        stable_joint.append(st_j)\n        if st_j:\n            joint_stable_Qs.append(Q_theta)\n\n        # Dual EnKF\n        rmse_x_d, rmse_th_d, st_d = enkf_dual(\n            N=N, T=T, dt=dt, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, Y=Y,\n            X_true=X_true, Theta_true=Theta_true,\n            x0_guess=x0_guess, theta0_guess=theta0_guess, Ne=Ne, rng=np.random.default_rng(3000 + int(Q_theta * 1e6))\n        )\n        stable_dual.append(st_d)\n        if st_d:\n            dual_stable_Qs.append(Q_theta)\n\n    # Stability thresholds (maximum Q_theta in test suite with stability)\n    JT = max(joint_stable_Qs) if joint_stable_Qs else 0.0\n    DT = max(dual_stable_Qs) if dual_stable_Qs else 0.0\n\n    # Final print statement in the exact required format.\n    # Convert booleans to 'True'/'False' strings automatically by str().\n    print(f\"[{JT},{DT},[{','.join(map(str, stable_joint))}],[{','.join(map(str, stable_dual))}]]\")\n\nsolve()\n```"
        }
    ]
}