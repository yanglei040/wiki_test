{
    "hands_on_practices": [
        {
            "introduction": "The heart of variational data assimilation is the minimization of a cost function, a task for which the most powerful algorithms require access to the function's gradient. This exercise provides a hands-on opportunity to derive the analytical gradient of the standard 4D-Var cost function from first principles. You will then implement a numerical gradient test to verify that your derivation and code are correct, a fundamental \"sanity check\" crucial for any practical optimization work ().",
            "id": "3423490",
            "problem": "Consider the strong-constraint Four-Dimensional Variational assimilation (4D-Var) under the perfect-model assumption, in which the discrete-time dynamical model is assumed exact and enforces zero model error. Let the state evolve according to the linear, time-invariant model $x_{k+1} = M x_k$ with $x_0$ being the control (initial condition), and let $x_k = M^k x_0$ for $k \\in \\{0,1,\\dots,K\\}$. Let the observation operator be linear, $H$, and the observation sequence be $\\{y_k\\}_{k=0}^K$. Let the background (prior) state be $x_b$ with covariance $B$, and let the observation error covariance be $R$, both symmetric positive definite. Define the 4D-Var cost functional\n$$\nJ(x_0) = \\tfrac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\tfrac{1}{2} \\sum_{k=0}^{K} \\left( H x_k - y_k \\right)^\\top R^{-1} \\left( H x_k - y_k \\right) ,\n$$\nwith $x_k = M^k x_0$. The scalar gradient test validates gradient implementations by checking the directional derivative consistency: for random directions $p$, define $\\phi(\\alpha) = J(x_0 + \\alpha p)$ and verify numerically that $\\phi'(0) = \\nabla J(x_0)^\\top p$.\n\nYour task is to implement a complete program that:\n- Constructs $J(x_0)$ according to the above definition for each test case below.\n- Computes $\\nabla J(x_0)$ with respect to $x_0$ by differentiating $J(x_0)$ from first principles of multivariate calculus and linear operators, under the perfect-model, strong-constraint setting.\n- Performs a scalar gradient test using random directions $p$ by approximating $\\phi'(0)$ with a symmetric finite-difference:\n$$\n\\phi'(0) \\approx \\frac{J(x_0 + h p) - J(x_0 - h p)}{2 h} ,\n$$\nwhere $h$ is a small step chosen to balance truncation and round-off errors.\n- For each test case, generates $10$ random directions $p$ of unit norm with a specified random seed for reproducibility, computes the absolute relative error for each direction\n$$\n\\mathrm{err}(p) = \\frac{\\left| \\phi'(0) - \\nabla J(x_0)^\\top p \\right|}{\\max\\left( 10^{-12} , \\left| \\phi'(0) \\right| + \\left| \\nabla J(x_0)^\\top p \\right| \\right)} ,\n$$\nand returns the maximum error over the $10$ directions as the test case result.\n\nYou must use the following test suite, which covers a typical case, an unstable-dynamics case, and a boundary case reducing to Three-Dimensional Variational assimilation (3D-Var) when $K = 0$:\n\n- Test Case $1$ (typical, stable linear dynamics, scalar observations):\n  - State dimension $n = 2$ and window length $K = 4$.\n  - Model $M = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.0 & 0.9 \\end{bmatrix}$.\n  - Observation operator $H = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$.\n  - Background covariance $B = \\begin{bmatrix} 0.4 & 0.0 \\\\ 0.0 & 0.6 \\end{bmatrix}$.\n  - Observation covariance $R = \\begin{bmatrix} 0.05 \\end{bmatrix}$.\n  - Background state $x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n  - Evaluation point $x_0 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$.\n  - True initial state for generating observations $x_0^{\\text{true}} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$.\n  - Observation noise sequence for $k = 0,1,2,3,4$: $\\{\\eta_k\\} = \\{0.01, -0.02, 0.015, -0.01, 0.005\\}$, with each $\\eta_k$ interpreted as a scalar forming $y_k = H M^k x_0^{\\text{true}} + \\eta_k$.\n  - Random seed for directions: $11$.\n\n- Test Case $2$ (unstable dynamics, vector observations, ill-conditioned background covariance):\n  - State dimension $n = 3$ and window length $K = 5$.\n  - Model $M = \\begin{bmatrix} 1.2 & 0.0 & 0.0 \\\\ 0.1 & 0.95 & 0.0 \\\\ 0.0 & 0.0 & 0.7 \\end{bmatrix}$.\n  - Observation operator $H = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 \\end{bmatrix}$.\n  - Background covariance $B = \\begin{bmatrix} 0.001 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & 0.0 \\\\ 0.0 & 0.0 & 0.5 \\end{bmatrix}$.\n  - Observation covariance $R = \\begin{bmatrix} 0.02 & 0.0 \\\\ 0.0 & 0.02 \\end{bmatrix}$.\n  - Background state $x_b = \\begin{bmatrix} -0.5 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix}$.\n  - Evaluation point $x_0 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$.\n  - True initial state for generating observations $x_0^{\\text{true}} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ -1.0 \\end{bmatrix}$.\n  - Observation noise sequence for $k = 0,1,2,3,4,5$: $\\{\\eta_k\\} = \\left\\{ \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.01 \\\\ -0.01 \\end{bmatrix}, \\begin{bmatrix} -0.02 \\\\ 0.015 \\end{bmatrix}, \\begin{bmatrix} 0.015 \\\\ -0.005 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 0.02 \\end{bmatrix}, \\begin{bmatrix} -0.01 \\\\ -0.01 \\end{bmatrix} \\right\\}$, with $y_k = H M^k x_0^{\\text{true}} + \\eta_k$.\n  - Random seed for directions: $22$.\n\n- Test Case $3$ (boundary case $K = 0$, zero observation operator, reduces to background-only term):\n  - State dimension $n = 2$ and window length $K = 0$.\n  - Model $M = \\begin{bmatrix} 0.8 & 0.1 \\\\ 0.0 & 0.9 \\end{bmatrix}$ (unused except for $M^0 = I$).\n  - Observation operator $H = \\begin{bmatrix} 0.0 & 0.0 \\end{bmatrix}$.\n  - Background covariance $B = \\begin{bmatrix} 0.3 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix}$.\n  - Observation covariance $R = \\begin{bmatrix} 0.1 \\end{bmatrix}$.\n  - Background state $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n  - Evaluation point $x_0 = \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix}$.\n  - True initial state for generating observations $x_0^{\\text{true}} = \\begin{bmatrix} 0.2 \\\\ 0.3 \\end{bmatrix}$.\n  - Observation noise sequence for $k = 0$: $\\{\\eta_0\\} = \\{0.02\\}$, with $y_0 = H M^0 x_0^{\\text{true}} + \\eta_0$ (note $H$ is zero, so $y_0 = \\eta_0$).\n  - Random seed for directions: $33$.\n\nImplementation requirements:\n- Use the perfect-model assumption and strong constraint strictly (i.e., $x_k = M^k x_0$ with no model error term).\n- Compute $\\nabla J(x_0)$ exactly for the given linear model using first principles; do not use automatic differentiation.\n- Use a symmetric finite-difference with a step $h$ selected adaptively, for example $h = 10^{-8} / \\max\\{1, \\|p\\|_2\\}$, in the scalar gradient test.\n- For each test case, generate $10$ random directions $p$ from a standard normal distribution with the specified seed, and normalize each to unit Euclidean norm before testing.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[ r_1, r_2, r_3 \\right]$, where each $r_i$ is the maximum absolute relative error over the $10$ random directions for test case $i$. Each $r_i$ must be a floating-point number.",
            "solution": "The state vector at time step $k$ be $x_k \\in \\mathbb{R}^n$. The evolution of the state is governed by a linear, time-invariant model:\n$$\nx_{k+1} = M x_k\n$$\nUnder the perfect-model and strong-constraint assumption, the state at any time $k$ is perfectly determined by the initial state $x_0$ through the relation:\n$$\nx_k = M^k x_0\n$$\nwhere $M^k$ is the $k$-th power of the matrix $M$.\n\nThe 4D-Var cost functional $J(x_0)$ measures the misfit between the model trajectory starting from $x_0$ and the available information, which consists of a background (or prior) estimate $x_b$ and a sequence of observations $\\{y_k\\}_{k=0}^K$. The functional is given as the sum of a background term $J_b(x_0)$ and an observation term $J_o(x_0)$:\n$$\nJ(x_0) = J_b(x_0) + J_o(x_0)\n$$\nThe background term penalizes the deviation of the initial state $x_0$ from the prior estimate $x_b$, weighted by the inverse of the background error covariance matrix $B$:\n$$\nJ_b(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b)\n$$\nThe observation term penalizes the sum of squared differences between the model-predicted observations $H x_k$ and the actual observations $y_k$ over the assimilation window $k \\in \\{0, 1, \\dots, K\\}$. These differences are weighted by the inverse of the observation error covariance matrix $R$:\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=0}^{K} (H x_k - y_k)^\\top R^{-1} (H x_k - y_k)\n$$\nSubstituting the model dynamics $x_k = M^k x_0$, the full cost functional is explicitly a function of $x_0$:\n$$\nJ(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=0}^{K} (H M^k x_0 - y_k)^\\top R^{-1} (H M^k x_0 - y_k)\n$$\nTo minimize this functional, we need its gradient, $\\nabla J(x_0)$, with respect to $x_0$. We can compute the gradient of each term separately.\n\nFor the background term $J_b(x_0)$, this is a standard quadratic form. The gradient is:\n$$\n\\nabla J_b(x_0) = B^{-1}(x_0 - x_b)\n$$\nThis result comes from the general rule for vector calculus: $\\nabla_z \\left( \\frac{1}{2} (z-c)^\\top A (z-c) \\right) = A(z-c)$ for a symmetric matrix $A$.\n\nFor the observation term $J_o(x_0)$, we differentiate the sum term-by-term. Let's consider the $k$-th term of the sum:\n$$\nJ_{o,k}(x_0) = \\frac{1}{2} (H M^k x_0 - y_k)^\\top R^{-1} (H M^k x_0 - y_k)\n$$\nThis is also a quadratic form of the type $\\frac{1}{2} (Az - d)^\\top C (Az - d)$. Its gradient with respect to $z$ is $A^\\top C(Az-d)$. In our case, $z=x_0$, $A = H M^k$, $d=y_k$, and $C=R^{-1}$. Thus, the gradient of the $k$-th term is:\n$$\n\\nabla J_{o,k}(x_0) = (H M^k)^\\top R^{-1} (H M^k x_0 - y_k)\n$$\nUsing the property of transposes $(AB)^\\top = B^\\top A^\\top$, we have $(H M^k)^\\top = (M^k)^\\top H^\\top$. So,\n$$\n\\nabla J_{o,k}(x_0) = (M^k)^\\top H^\\top R^{-1} (H M^k x_0 - y_k)\n$$\nThe gradient of the total observation term is the sum of the gradients of each individual term:\n$$\n\\nabla J_o(x_0) = \\sum_{k=0}^{K} (M^k)^\\top H^\\top R^{-1} (H M^k x_0 - y_k)\n$$\nCombining the gradients of the background and observation terms, the total gradient of the 4D-Var cost functional is:\n$$\n\\nabla J(x_0) = B^{-1}(x_0 - x_b) + \\sum_{k=0}^{K} (M^k)^\\top H^\\top R^{-1} (H M^k x_0 - y_k)\n$$\nThis analytical expression for the gradient will be implemented and tested.\n\nThe gradient test serves as a numerical verification of this derivation and its implementation. The test is based on the definition of the directional derivative. For a function $J(x_0)$ and a direction vector $p$, the function $\\phi(\\alpha) = J(x_0 + \\alpha p)$ has a derivative at $\\alpha=0$ given by $\\phi'(0) = \\nabla J(x_0)^\\top p$. We can approximate $\\phi'(0)$ numerically using a second-order accurate symmetric finite-difference formula:\n$$\n\\phi'(0) \\approx \\frac{\\phi(h) - \\phi(-h)}{2h} = \\frac{J(x_0 + h p) - J(x_0 - h p)}{2h}\n$$\nfor a small step size $h$. The problem specifies $h = 10^{-8}$ for a unit-norm direction $p$. A small discrepancy between the analytical value $\\nabla J(x_0)^\\top p$ and the numerical approximation is expected due to floating-point precision and truncation error. The provided absolute relative error metric quantifies this discrepancy robustly. For a correct implementation, this error should be very small, typically on the order of the chosen step size $h$ or related to machine precision. The program will compute this error for 10 random directions and report the maximum found for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef run_test_case(params):\n    \"\"\"\n    Runs a single 4D-Var gradient test case.\n    \"\"\"\n    # Unpack parameters\n    n = params[\"n\"]\n    K = params[\"K\"]\n    M = params[\"M\"]\n    H = params[\"H\"]\n    B = params[\"B\"]\n    R = params[\"R\"]\n    xb = params[\"xb\"]\n    x0_eval = params[\"x0_eval\"]\n    x0_true = params[\"x0_true\"]\n    etas = params[\"etas\"]\n    seed = params[\"seed\"]\n\n    # Precompute matrix inverses\n    B_inv = np.linalg.inv(B)\n    R_inv = np.linalg.inv(R)\n\n    # 1. Generate observations y_k\n    y_k_sequence = []\n    Mk_true = np.identity(n)\n    for k in range(K + 1):\n        x_k_true = Mk_true @ x0_true\n        y_k = H @ x_k_true + etas[k]\n        y_k_sequence.append(y_k)\n        if k < K:\n            Mk_true = M @ Mk_true\n\n    # 2. Define cost function J(x0)\n    def cost_function(x0):\n        # Background term\n        bg_diff = x0 - xb\n        term_b = 0.5 * bg_diff.T @ B_inv @ bg_diff\n\n        # Observation term\n        term_o = 0.0\n        Mk = np.identity(n)\n        for k in range(K + 1):\n            x_k = Mk @ x0\n            y_pred_k = H @ x_k\n            innovation_k = y_pred_k - y_k_sequence[k]\n            term_o += 0.5 * innovation_k.T @ R_inv @ innovation_k\n            if k < K:\n                Mk = M @ Mk\n        return term_b + term_o\n\n    # 3. Define analytical gradient\n    def analytical_gradient(x0):\n        # Gradient of background term\n        grad_b = B_inv @ (x0 - xb)\n\n        # Gradient of observation term\n        grad_o = np.zeros_like(x0, dtype=float)\n        Mk = np.identity(n)\n        for k in range(K + 1):\n            x_k = Mk @ x0\n            y_pred_k = H @ x_k\n            innovation_k = y_pred_k - y_k_sequence[k]\n            grad_o += Mk.T @ H.T @ R_inv @ innovation_k\n            if k < K:\n                Mk = M @ Mk\n        return grad_b + grad_o\n\n    # 4. Perform scalar gradient test\n    rng = np.random.default_rng(seed)\n    errors = []\n    \n    grad_at_x0 = analytical_gradient(x0_eval)\n\n    for _ in range(10):\n        # Generate and normalize random direction p\n        p = rng.standard_normal(size=n)\n        p /= np.linalg.norm(p)\n\n        # Analytical directional derivative\n        grad_proj = grad_at_x0.T @ p\n\n        # Numerical directional derivative (finite difference)\n        h = 1e-8 / max(1.0, np.linalg.norm(p)) # norm is 1, so h=1e-8\n        j_plus = cost_function(x0_eval + h * p)\n        j_minus = cost_function(x0_eval - h * p)\n        fd_approx = (j_plus - j_minus) / (2 * h)\n\n        # Calculate absolute relative error\n        numerator = abs(fd_approx - grad_proj)\n        denominator = max(1e-12, abs(fd_approx) + abs(grad_proj))\n        error = numerator / denominator\n        errors.append(error)\n        \n    return max(errors)\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run them.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            \"n\": 2, \"K\": 4,\n            \"M\": np.array([[1.0, 0.2], [0.0, 0.9]]),\n            \"H\": np.array([[1.0, 0.0]]),\n            \"B\": np.array([[0.4, 0.0], [0.0, 0.6]]),\n            \"R\": np.array([[0.05]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"x0_eval\": np.array([0.2, -0.1]),\n            \"x0_true\": np.array([1.0, -0.5]),\n            \"etas\": [np.array([v]) for v in [0.01, -0.02, 0.015, -0.01, 0.005]],\n            \"seed\": 11\n        },\n        # Test Case 2\n        {\n            \"n\": 3, \"K\": 5,\n            \"M\": np.array([[1.2, 0.0, 0.0], [0.1, 0.95, 0.0], [0.0, 0.0, 0.7]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"B\": np.array([[0.001, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.5]]),\n            \"R\": np.array([[0.02, 0.0], [0.0, 0.02]]),\n            \"xb\": np.array([-0.5, 0.2, 0.1]),\n            \"x0_eval\": np.array([0.1, -0.2, 0.3]),\n            \"x0_true\": np.array([0.0, 1.0, -1.0]),\n            \"etas\": [\n                np.array([0.0, 0.0]), np.array([0.01, -0.01]), \n                np.array([-0.02, 0.015]), np.array([0.015, -0.005]),\n                np.array([0.0, 0.02]), np.array([-0.01, -0.01])\n            ],\n            \"seed\": 22\n        },\n        # Test Case 3\n        {\n            \"n\": 2, \"K\": 0,\n            \"M\": np.array([[0.8, 0.1], [0.0, 0.9]]),\n            \"H\": np.array([[0.0, 0.0]]),\n            \"B\": np.array([[0.3, 0.0], [0.0, 0.3]]),\n            \"R\": np.array([[0.1]]),\n            \"xb\": np.array([1.0, -1.0]),\n            \"x0_eval\": np.array([0.5, -0.2]),\n            \"x0_true\": np.array([0.2, 0.3]),\n            \"etas\": [np.array([0.02])],\n            \"seed\": 33\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        max_error = run_test_case(params)\n        results.append(max_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Efficiently calculating the gradient of the 4D-Var cost function, which depends on a long model trajectory, is made possible by the adjoint method. The correctness of the entire assimilation process hinges on the faithful implementation of the model's adjoint operator. This practice focuses on this core component, guiding you to implement the forward and adjoint operators for a physical model governed by a partial differential equation and to verify their relationship using an adjoint test, which checks the fundamental property $\\langle M u, v \\rangle = \\langle u, M^{\\ast} v \\rangle$ ().",
            "id": "3423523",
            "problem": "Consider the strong-constraint four-dimensional variational assimilation setting under the perfect-model assumption, in which the model is assumed exact and the only source of error is the initial condition. Let the state variable be a spatially periodic scalar field $u(x,t)$ evolving on a one-dimensional domain of length $L$, with periodic boundary conditions, and governed by the linear advection-diffusion equation\n$$\n\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} \\;=\\; \\nu \\frac{\\partial^2 u}{\\partial x^2},\n$$\nwhere $c$ is a constant advection speed and $\\nu$ is a constant diffusion coefficient. Let $M$ denote the discrete linear propagator that maps the state at time $t=0$ to the state at time $t=T$ under the perfect-model assumption, after discretizing space on a uniform grid of $N$ points with grid spacing $\\Delta x = L/N$ and using periodic boundary conditions.\n\nYou will use a spectrally accurate discretization in space based on the discrete Fourier transform for spatial derivatives, consistent with the above model. In addition, define the discrete inner product $\\langle a, b \\rangle$ between two grid functions $a, b \\in \\mathbb{R}^N$ by\n$$\n\\langle a, b \\rangle \\;=\\; \\Delta x \\sum_{i=0}^{N-1} a_i\\, b_i,\n$$\nwhich is the trapezoidal-rule approximation of the spatial $L^2$ inner product on the periodic grid. With this inner product, the adjoint $M^{\\ast}$ of $M$ is defined by the relation\n$$\n\\langle M u, v \\rangle \\;=\\; \\langle u, M^{\\ast} v \\rangle \\quad \\text{for all } u, v \\in \\mathbb{R}^N.\n$$\n\nYour task is to implement a complete program that:\n- Constructs the discrete linear forward operator $M$ consistent with the above model and discretization by applying the model to canonical basis vectors to form the full matrix representation.\n- Implements the corresponding adjoint operator $M^{\\ast}$ with respect to the above inner product.\n- Performs an adjoint test by drawing random vectors $u$ and $v$ and computing both sides of the equality $\\langle M u, v \\rangle$ and $\\langle u, M^{\\ast} v \\rangle$.\n- Reports, for each test case, the relative discrepancy\n$$\n\\varepsilon \\;=\\; \\frac{\\left|\\langle M u, v \\rangle - \\langle u, M^{\\ast} v \\rangle \\right|}{\\max\\left(1, \\left|\\langle M u, v \\rangle\\right|\\right)},\n$$\nas a real number. The smaller the value of $\\varepsilon$, the better the agreement.\n\nIn implementing $M$, you must use the following context-appropriate fundamental bases:\n- The definition of the discrete Fourier transform on a uniform periodic grid to represent spatial derivatives.\n- The fact that, for a constant-coefficient linear partial differential equation on a periodic domain, the spatial Fourier modes evolve independently under a linear ordinary differential equation in time.\n- The perfect-model assumption, which implies exact propagation under the chosen discretization.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where each entry is the computed $\\varepsilon$ for one test case.\n\nUse the following test suite, which covers a typical case, limiting cases, and an edge case:\n- Test case $1$: $N=32$, $L=2\\pi$, $c=1.0$, $\\nu=0.1$, $T=1.0$, random seed $12345$.\n- Test case $2$: $N=4$, $L=1.0$, $c=0.0$, $\\nu=0.5$, $T=0.5$, random seed $7$.\n- Test case $3$: $N=64$, $L=2\\pi$, $c=2.0$, $\\nu=0.0$, $T=3.0$, random seed $999$.\n- Test case $4$: $N=16$, $L=2\\pi$, $c=0.5$, $\\nu=0.2$, $T=10.0$, random seed $2024$.\n\nThe final output must be a single line in the exact format $[\\varepsilon_1,\\varepsilon_2,\\varepsilon_3,\\varepsilon_4]$ with each $\\varepsilon_i$ a floating-point number computed as above. No additional text should be printed.",
            "solution": "The solution approach is based on the properties of the linear advection-diffusion equation on a periodic domain. The use of a Fourier spectral method for spatial discretization is particularly effective in this context.\n\n**1. Forward Model ($M$) using Spectral Discretization**\n\nThe governing partial differential equation (PDE) is:\n$$\n\\frac{\\partial u}{\\partial t} + c \\frac{\\partial u}{\\partial x} \\;=\\; \\nu \\frac{\\partial^2 u}{\\partial x^2}\n$$\nGiven the periodic domain, we use a Fourier spectral method. A state $u(x, t)$ on the discrete grid $x_j = j \\Delta x$ is represented by a vector $\\mathbf{u}(t) \\in \\mathbb{R}^N$. This state is expressed in terms of its discrete Fourier series. The key advantage of this representation is that spatial differentiation in physical space becomes multiplication in Fourier space. Specifically, for a mode with wavenumber $k_x$:\n$$\n\\frac{\\partial}{\\partial x} \\longleftrightarrow i k_x \\quad \\text{and} \\quad \\frac{\\partial^2}{\\partial x^2} \\longleftrightarrow (i k_x)^2 = -k_x^2\n$$\nSubstituting this into the PDE transforms it from a single PDE into a set of $N$ uncoupled ordinary differential equations (ODEs), one for each Fourier coefficient $\\hat{u}_k(t)$:\n$$\n\\frac{d\\hat{u}_k}{dt} + c(i k_x) \\hat{u}_k = \\nu(-k_x^2) \\hat{u}_k\n$$\nThis simplifies to:\n$$\n\\frac{d\\hat{u}_k}{dt} = (-i c k_x - \\nu k_x^2) \\hat{u}_k\n$$\nLet $\\lambda_k = -i c k_x - \\nu k_x^2$. This is a linear first-order ODE with the solution:\n$$\n\\hat{u}_k(T) = \\hat{u}_k(0) e^{\\lambda_k T}\n$$\nThis provides an exact solution for the time evolution of the semi-discretized system. The factor $e^{\\lambda_k T}$ is the propagator for mode $k$.\n\nThe discrete forward operator $M$, which maps the state vector $\\mathbf{u}(0)$ to $\\mathbf{u}(T)$, is implemented algorithmically as follows:\n1.  Compute the discrete Fourier-transformed state $\\hat{\\mathbf{u}}(0)$ from $\\mathbf{u}(0)$ using the Fast Fourier Transform (FFT).\n2.  Multiply each Fourier coefficient $\\hat{u}_k(0)$ by its corresponding propagator $e^{\\lambda_k T}$.\n3.  Compute the state $\\mathbf{u}(T)$ by applying the inverse FFT to the resulting transformed state.\n\nTo construct the explicit $N \\times N$ matrix representation of $M$, we apply this operator to each of the canonical basis vectors $\\mathbf{e}_j \\in \\mathbb{R}^N$. The $j$-th column of the matrix $M$ is the vector $M\\mathbf{e}_j$.\n\n**2. Adjoint Model ($M^{\\ast}$)**\n\nThe adjoint operator $M^{\\ast}$ is defined with respect to the given discrete inner product, $\\langle a, b \\rangle = \\Delta x \\sum_{i=0}^{N-1} a_i b_i$. In vector notation, this is $\\langle a, b \\rangle = \\Delta x \\, \\mathbf{a}^T \\mathbf{b}$. The defining relation is:\n$$\n\\langle M \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{u}, M^{\\ast} \\mathbf{v} \\rangle \\quad \\text{for all } \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^N\n$$\nSubstituting the inner product definition, we have:\n$$\n\\Delta x (M\\mathbf{u})^T \\mathbf{v} = \\Delta x \\, \\mathbf{u}^T (M^{\\ast}\\mathbf{v})\n$$\nUsing the property of matrix transposition, $(M\\mathbf{u})^T = \\mathbf{u}^T M^T$, the left-hand side becomes:\n$$\n\\Delta x \\, \\mathbf{u}^T M^T \\mathbf{v}\n$$\nComparing this to the right-hand side, $\\Delta x \\, \\mathbf{u}^T (M^{\\ast}\\mathbf{v})$, implies that the action of the operator $M^{\\ast}$ on a vector $\\mathbf{v}$ is equivalent to multiplication by the transpose of the matrix $M$:\n$$\nM^{\\ast}\\mathbf{v} = M^T \\mathbf{v}\n$$\nThus, the matrix representation of the adjoint operator $M^{\\ast}$ is simply the transpose of the matrix $M$.\n\n**3. The Adjoint Test**\n\nThe adjoint test is a numerical verification of the implemented adjoint. It involves computing both sides of the defining equality, $\\langle M \\mathbf{u}, \\mathbf{v} \\rangle$ and $\\langle \\mathbf{u}, M^{\\ast} \\mathbf{v} \\rangle$, for randomly chosen vectors $\\mathbf{u}$ and $\\mathbf{v}$. This amounts to checking the equality $\\langle M \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{u}, M^T \\mathbf{v} \\rangle$. The relative discrepancy, $\\varepsilon$, is calculated to quantify any difference, which should be close to machine precision for a correct implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of constructing and testing the forward and adjoint\n    operators for the 1D linear advection-diffusion equation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, L, c, nu, T, seed)\n        (32, 2 * np.pi, 1.0, 0.1, 1.0, 12345),\n        (4, 1.0, 0.0, 0.5, 0.5, 7),\n        (64, 2 * np.pi, 2.0, 0.0, 3.0, 999),\n        (16, 2 * np.pi, 0.5, 0.2, 10.0, 2024),\n    ]\n\n    results = []\n    for N, L, c, nu, T, seed in test_cases:\n        # Set up the computational grid and parameters\n        dx = L / N\n        \n        # Set random seed for reproducibility\n        np.random.seed(seed)\n        \n        # Generate random real-valued vectors u and v\n        u = np.random.rand(N)\n        v = np.random.rand(N)\n\n        # Calculate the discrete wavenumbers\n        # np.fft.fftfreq returns frequencies in cycles / sample-spacing unit.\n        # We multiply by 2*pi to get angular wavenumbers.\n        k = 2 * np.pi * np.fft.fftfreq(N, d=dx)\n        \n        # Calculate the growth rate lambda_k for each Fourier mode\n        # lambda_k = -i*c*k_x - nu*k_x^2\n        lambda_k = -1j * c * k - nu * k**2\n        \n        # Calculate the propagator in Fourier space for the time interval T\n        propagator = np.exp(lambda_k * T)\n        \n        # Construct the matrix M by applying the forward model to canonical basis vectors\n        M_matrix = np.zeros((N, N), dtype=np.float64)\n        e_j = np.zeros(N)\n        for j in range(N):\n            e_j[j-1 if j > 0 else N-1] = 0.0 # Reset previous basis vector entry\n            e_j[j] = 1.0\n            \n            # Apply the forward operator M to the basis vector e_j\n            # 1. Transform to Fourier space\n            e_j_hat = np.fft.fft(e_j)\n            \n            # 2. Propagate in Fourier space\n            M_e_j_hat = e_j_hat * propagator\n            \n            # 3. Transform back to physical space\n            M_e_j = np.fft.ifft(M_e_j_hat)\n            \n            # The result should be real; take the real part to discard numerical noise\n            M_matrix[:, j] = M_e_j.real\n\n        # --- Adjoint Test ---\n        # 1. Compute the left-hand side: <M*u, v>\n        Mu = M_matrix @ u\n        inner_prod_lhs = dx * np.dot(Mu, v)\n        \n        # 2. Compute the right-hand side: <u, M_adjoint*v>\n        # The matrix for the adjoint operator M* is the transpose of M\n        M_adjoint_matrix = M_matrix.T\n        M_adjoint_v = M_adjoint_matrix @ v\n        inner_prod_rhs = dx * np.dot(u, M_adjoint_v)\n        \n        # 3. Calculate the relative discrepancy epsilon\n        numerator = np.abs(inner_prod_lhs - inner_prod_rhs)\n        denominator = np.max([1.0, np.abs(inner_prod_lhs)])\n        epsilon = numerator / denominator\n        \n        results.append(epsilon)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.17e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Even with a perfect model and a correctly implemented gradient, nonlinearity in the system can introduce significant challenges, chief among them being non-convexity. This exercise uses a simple but powerful counterexample to demonstrate how a nonlinear observation operator can create a complex cost landscape with multiple distinct local minima (). By implementing and comparing two different optimization algorithms, you will gain practical insight into why finding the global minimum is not always straightforward and explore the concept of basins of attraction.",
            "id": "3423499",
            "problem": "Consider the strong-constraint four-dimensional variational assimilation under the perfect-model assumption for a one-dimensional state. The control variable is the initial state $x_0 \\in \\mathbb{R}$. The perfect-model assumption means that the model dynamics $x_{k+1} = \\mathcal{M}(x_k)$ are exact and deterministic. In this counterexample, let the model be the identity map $\\mathcal{M}(x) = x$, so that $x_k = x_0$ for all observation times $t_k$, $k = 1, \\dots, T$. The observation operator is nonlinear: $H(x) = \\sin(x)$, and all observations are identical at each time. The cost functional is the standard strong-constraint four-dimensional variational cost:\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b) + \\frac{1}{2}\\sum_{k=1}^T \\left( H(x_0) - y_k \\right)^\\top R^{-1} \\left( H(x_0) - y_k \\right),\n$$\nwith Gaussian background and observation error models. All angles are in radians.\n\nYour tasks are:\n- Starting only from the definition of $J(x_0)$, the perfect-model assumption $x_k = x_0$, and the nonlinear observation operator $H(x) = \\sin(x)$, explain why nonlinearity in $H$ can induce multiple distinct local minima of $J(x_0)$ even when the model is perfect. Provide a principled argument identifying where these minima occur and why they are local minima.\n- Design two iterative optimization schemes to minimize $J(x_0)$ over $\\mathbb{R}$:\n  1. A Gauss–Newton method applied to the nonlinear least-squares structure of $J(x_0)$.\n  2. A trust-region method in one dimension that uses the quadratic approximation built from the Jacobian and restricts steps to a trust region whose radius is adapted by comparing actual versus predicted reduction. In one dimension, the trust-region step can be chosen as the exact minimizer of the quadratic model if it lies within the trust region, or the Cauchy step at the trust-region boundary otherwise.\n- Characterize the basins of attraction induced by the two methods by running both solvers from a selection of initial guesses and recording to which local minimum index each converges. Define the local minimum index $m \\in \\mathbb{Z}$ for a converged solution $\\hat{x}$ by mapping it to the nearest multiple of $\\pi$: $m = \\mathrm{round}(\\hat{x}/\\pi)$.\n\nUse the following fixed, scientifically consistent parameters and data:\n- Number of observation times: $T = 5$.\n- Observations: $y_k = 0$ for all $k = 1, \\dots, T$.\n- Background mean: $x_b = 0.2$.\n- Background covariance: $B = \\sigma_b^2$ with $\\sigma_b^2 = 1$, so $B^{-1} = 1$.\n- Observation error covariance: $R = \\sigma_o^2$ with $\\sigma_o^2 = 10^{-2}$, so $R^{-1} = 100$.\n- Observation operator: $H(x) = \\sin(x)$.\n- Angles are to be treated in radians.\n\nImplementation requirements for the two solvers:\n- Gauss–Newton: Implement an iteration that at each step forms the Jacobian of residuals and computes a Gauss–Newton step. Include a backtracking line search that enforces strict decrease in $J(x_0)$ to guarantee robustness.\n- Trust-region: Implement a one-dimensional trust-region method with initial radius $\\Delta_0 = 1.0$, maximum radius $\\Delta_{\\max} = 10.0$, and acceptance threshold $\\eta = 0.1$. Update the trust-region radius using the ratio of actual to predicted reduction with standard rules: shrink if the ratio is small, expand if the ratio is large and the boundary is active.\n- For both solvers, terminate when either the absolute gradient magnitude is below $10^{-10}$, the absolute step size is below $10^{-10}$, or the iteration count reaches $200$.\n\nTest suite:\n- Run both solvers from each of the following initial guesses for $x_0$: $[0.1,\\ 1.5707963267948966,\\ 2.9,\\ -2.9,\\ 6.0]$.\n- For each initial guess, compute the converged solution for Gauss–Newton and for trust-region, map each converged solution $\\hat{x}$ to its local minimum index $m = \\mathrm{round}(\\hat{x}/\\pi)$, and report the pair [$m_{\\mathrm{GN}}$, $m_{\\mathrm{TR}}$] of integers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to a test case and is itself a two-element list [$m_{\\mathrm{GN}}$, $m_{\\mathrm{TR}}$] with integers. For example, the overall format must be like: \"[[a,b],[c,d],[e,f],[g,h],[i,j]]\".",
            "solution": "This problem describes a classic scenario in data assimilation where a nonlinear observation operator induces a non-convex cost functional, leading to multiple local minima, even under the simplifying perfect-model assumption. The task is to analyze this phenomenon and implement two standard optimization algorithms, Gauss-Newton and trust-region, to find these minima and characterize their basins of attraction.\n\n### Part 1: Theoretical Analysis of Multiple Minima\n\nThe strong-constraint four-dimensional variational cost functional for this one-dimensional problem is given as:\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - x_b)^\\top B^{-1}(x_0 - x_b) + \\frac{1}{2}\\sum_{k=1}^T \\left( H(x_k) - y_k \\right)^\\top R^{-1} \\left( H(x_k) - y_k \\right)\n$$\nWith the perfect model assumption $\\mathcal{M}(x) = x$, we have $x_k = x_0$ for all observation times $k$. The state $x_0 \\in \\mathbb{R}$, so all matrix operations become scalar multiplications. We substitute the given parameters: background mean $x_b = 0.2$, background error variance $B = 1$ (so $B^{-1} = 1$), number of observations $T=5$, observations $y_k=0$ for all $k$, observation error variance $R = 10^{-2}$ (so $R^{-1} = 100$), and the nonlinear observation operator $H(x_0) = \\sin(x_0)$. The cost functional $J(x_0)$ simplifies to:\n$$\nJ(x_0) = \\frac{1}{2}\\frac{(x_0 - 0.2)^2}{1} + \\frac{1}{2}\\sum_{k=1}^5 \\frac{(\\sin(x_0) - 0)^2}{10^{-2}}\n$$\n$$\nJ(x_0) = \\frac{1}{2}(x_0 - 0.2)^2 + \\frac{1}{2} \\cdot 5 \\cdot 100 \\cdot \\sin^2(x_0)\n$$\n$$\nJ(x_0) = 0.5(x_0 - 0.2)^2 + 250 \\sin^2(x_0)\n$$\nThis expression reveals that $J(x_0)$ is the sum of two components: a quadratic background term, $J_b(x_0) = 0.5(x_0 - 0.2)^2$, and a periodic observation term, $J_o(x_0) = 250 \\sin^2(x_0)$.\nThe background term $J_b(x_0)$ is a convex parabola with a single global minimum at $x_0 = 0.2$.\nThe observation term $J_o(x_0)$ is non-convex due to the $\\sin(x_0)$ operator. It has an infinite set of global minima with a value of $0$ at every point $x_0 = m\\pi$ for any integer $m \\in \\mathbb{Z}$.\nThe superposition of the smooth, overarching parabola from $J_b$ and the highly oscillatory landscape from $J_o$ results in a total cost function $J(x_0)$ that possesses multiple distinct local minima. The locations of these minima are primarily dictated by the minima of the dominant observation term ($\\approx m\\pi$) but are slightly perturbed toward the minimum of the background term ($x_0 = 0.2$).\n\nTo locate the extrema, we compute the gradient of $J(x_0)$ and find its roots:\n$$\n\\frac{dJ}{dx_0} = \\frac{d}{dx_0} \\left( 0.5(x_0 - 0.2)^2 + 250 \\sin^2(x_0) \\right) = (x_0 - 0.2) + 250 \\cdot (2\\sin(x_0)\\cos(x_0)) = (x_0 - 0.2) + 250\\sin(2x_0)\n$$\nThe minima are the solutions to the transcendental equation $(x_0 - 0.2) + 250\\sin(2x_0) = 0$. To determine if an extremum is a minimum, we examine the sign of the second derivative (the Hessian), $H(x_0) = \\frac{d^2J}{dx_0^2}$:\n$$\nH(x_0) = \\frac{d}{dx_0} \\left( (x_0 - 0.2) + 250\\sin(2x_0) \\right) = 1 + 500\\cos(2x_0)\n$$\nA point $x_0^*$ is a local minimum if $\\frac{dJ}{dx_0}(x_0^*) = 0$ and $H(x_0^*) > 0$. Let us evaluate the Hessian near the minima of the observation term, at $x_0 = m\\pi$:\n$$\nH(m\\pi) = 1 + 500\\cos(2m\\pi) = 1 + 500(1) = 501 > 0\n$$\nSince the Hessian is strongly positive at $x_0 = m\\pi$, and the roots of the gradient are very close to these points, the Hessian will remain positive at the actual minima. This confirms the existence of a local minimum near every integer multiple of $\\pi$. These multiple minima are a direct consequence of the periodic, and thus nonlinear, nature of the observation operator $H(x)=\\sin(x)$.\n\n### Part 2: Design of Iterative Optimization Schemes\n\n**1. Gauss–Newton Method with Line Search**\nThe Gauss-Newton method is tailored for minimizing a sum of squared functions, which is the structure of $J(x_0)$. The cost is $J(x_0) = \\frac{1}{2}\\mathbf{r}(x_0)^\\top\\mathbf{r}(x_0)$, where $\\mathbf{r}(x_0)$ is the vector of weighted residuals. In this problem, the residual vector has $1+T=6$ components:\n$$\n\\mathbf{r}(x_0) = \\begin{bmatrix} B^{-1/2}(x_0 - x_b) \\\\ R^{-1/2}(H(x_0) - y_1) \\\\ \\vdots \\\\ R^{-1/2}(H(x_0) - y_T) \\end{bmatrix} = \\begin{bmatrix} x_0 - 0.2 \\\\ 10\\sin(x_0) \\\\ \\vdots \\\\ 10\\sin(x_0) \\end{bmatrix}\n$$\nThe Jacobian of $\\mathbf{r}(x_0)$ is a $6 \\times 1$ matrix $\\mathbf{J}_{\\mathbf{r}}(x_0) = \\frac{d\\mathbf{r}}{dx_0}$. The Gauss-Newton approximate Hessian is $H_{GN} = \\mathbf{J}_{\\mathbf{r}}^\\top \\mathbf{J}_{\\mathbf{r}}$.\n$$\nH_{GN}(x_0) = \\left(\\frac{d(x_0-0.2)}{dx_0}\\right)^2 + \\sum_{k=1}^5 \\left(\\frac{d(10\\sin x_0)}{dx_0}\\right)^2 = 1^2 + 5 \\cdot (10\\cos x_0)^2 = 1 + 500\\cos^2(x_0)\n$$\nThe Gauss-Newton step $p_k$ at iterate $x_k$ is the solution to $H_{GN}(x_k) p_k = -\\nabla J(x_k)$, which gives $p_k = -\\nabla J(x_k) / H_{GN}(x_k)$. The next iterate is $x_{k+1} = x_k + \\alpha_k p_k$, where the step length $\\alpha_k \\in (0, 1]$ is chosen via a backtracking line search to ensure a strict decrease in the cost, i.e., $J(x_{k+1}) < J(x_k)$. Since $\\cos^2(x_0) \\ge 0$, $H_{GN}$ is always positive definite, guaranteeing that $p_k$ is a descent direction.\n\n**2. Trust-Region Method**\nThe trust-region method constructs a quadratic model $m_k(p)$ of the function at the current iterate $x_k$ and minimizes this model within a trust region of radius $\\Delta_k$. The model uses the true Hessian $H(x_k) = 1 + 500\\cos(2x_k)$:\n$$\nm_k(p) = J(x_k) + \\nabla J(x_k) p + \\frac{1}{2} H(x_k) p^2\n$$\nThe one-dimensional trust-region subproblem, $\\min_{|p| \\leq \\Delta_k} m_k(p)$, has a well-defined solution strategy:\n- If the model is convex ($H(x_k) > 0$), calculate the unconstrained Newton step, $p_N = -\\nabla J(x_k) / H(x_k)$. If $|p_N| < \\Delta_k$, take the full Newton step: $p_k = p_N$. Otherwise, the solution is at the boundary: $p_k = -\\text{sign}(\\nabla J(x_k)) \\Delta_k$.\n- If the model is non-convex or flat ($H(x_k) \\leq 0$), the model's minimum within the trust region is on its boundary. The step is the Cauchy step at the boundary: $p_k = -\\text{sign}(\\nabla J(x_k)) \\Delta_k$.\nThe quality of the step $p_k$ is assessed by the ratio $\\rho_k$ of actual reduction in $J$ to the reduction predicted by the model $m_k$. If $\\rho_k$ is greater than a threshold $\\eta=0.1$, the step is accepted ($x_{k+1} = x_k + p_k$). The trust radius $\\Delta_k$ is adjusted based on the value of $\\rho_k$: it is shrunk for poor model agreement and expanded for good agreement when the step is at the boundary. This allows the method to robustly navigate regions of both positive and negative curvature.\n\n### Part 3: Characterization of Basins of Attraction\n\nBy launching the two solvers from specified initial guesses, we computationally explore the basins of attraction for the different local minima. The converged solution $\\hat{x}$ is mapped to an integer index $m = \\mathrm{round}(\\hat{x}/\\pi)$, which identifies the minimum it has found. The numerical results demonstrate how these advanced optimization methods navigate the non-convex landscape.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the data assimilation problem.\n    It sets up parameters, implements the Gauss-Newton and Trust-Region solvers,\n    runs them on the specified test cases, and prints the results.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    T = 5.0\n    x_b = 0.2\n    B_inv = 1.0\n    R_inv = 100.0\n    y_k = 0.0\n\n    # --- Solver Parameters ---\n    MAX_ITER = 200\n    TOL_G = 1e-10\n    TOL_X = 1e-10\n\n    # --- Cost Function and its Derivatives ---\n    def J(x):\n        \"\"\"Standard strong-constraint 4D-Var cost functional.\"\"\"\n        return 0.5 * B_inv * (x - x_b)**2 + 0.5 * T * R_inv * (np.sin(x) - y_k)**2\n\n    def grad_J(x):\n        \"\"\"Gradient of the cost functional.\"\"\"\n        return B_inv * (x - x_b) + T * R_inv * (np.sin(x) - y_k) * np.cos(x)\n\n    def hessian_J(x):\n        \"\"\"True Hessian of the cost functional.\"\"\"\n        return B_inv + T * R_inv * (np.cos(x)**2 - np.sin(x)**2)\n\n    def gn_hessian_J(x):\n        \"\"\"Gauss-Newton approximation of the Hessian.\"\"\"\n        return B_inv + T * R_inv * np.cos(x)**2\n\n    # --- Solver Implementations ---\n    def gauss_newton(x_init):\n        \"\"\"\n        Gauss-Newton solver with backtracking line search.\n        \"\"\"\n        x = float(x_init)\n        for _ in range(MAX_ITER):\n            cost_current = J(x)\n            grad = grad_J(x)\n\n            if abs(grad) < TOL_G:\n                break\n\n            h_gn = gn_hessian_J(x)\n            p = -grad / h_gn\n\n            # Backtracking line search for strict decrease\n            alpha = 1.0\n            for _ in range(10):  # Max 10 line search iterations\n                x_new = x + alpha * p\n                if J(x_new) < cost_current:\n                    break\n                alpha /= 2.0\n            else:  # Could not find a step that decreases cost\n                break\n\n            step_size = abs(x_new - x)\n            x = x_new\n\n            if step_size < TOL_X:\n                break\n        return x\n\n    def trust_region(x_init):\n        \"\"\"\n        One-dimensional trust-region solver.\n        \"\"\"\n        x = float(x_init)\n        delta = 1.0\n        delta_max = 10.0\n        eta = 0.1\n\n        for _ in range(MAX_ITER):\n            cost = J(x)\n            grad = grad_J(x)\n\n            if abs(grad) < TOL_G:\n                break\n\n            hess = hessian_J(x)\n\n            # Solve the 1D trust-region subproblem\n            if hess > 0:\n                p_newton = -grad / hess\n                if abs(p_newton) < delta:\n                    p = p_newton\n                else: # Step at boundary\n                    p = -np.sign(grad) * delta\n            else: # Non-positive curvature, step at boundary\n                p = -np.sign(grad) * delta\n            \n            if abs(p) < TOL_X and abs(grad) < TOL_G:\n                break\n\n            # Evaluate step quality\n            ared = cost - J(x + p)\n            pred = -(grad * p + 0.5 * hess * p**2)\n\n            if abs(pred) < 1e-12:\n                rho = 1.0 if abs(ared) < 1e-12 else 0.0\n            else:\n                rho = ared / pred\n\n            # Update trust region radius\n            p_norm = abs(p)\n            if rho < 0.25:\n                delta *= 0.25\n            elif rho > 0.75 and np.isclose(p_norm, delta, atol=1e-12, rtol=1e-12):\n                delta = min(2.0 * delta, delta_max)\n            \n            # Accept or reject step\n            if rho > eta:\n                step_size = abs(p)\n                x += p\n                if step_size < TOL_X:\n                    break\n                    \n        return x\n\n    # --- Test Suite Execution ---\n    test_cases = [0.1, 1.5707963267948966, 2.9, -2.9, 6.0]\n    final_results = []\n\n    for x0 in test_cases:\n        x_gn = gauss_newton(x0)\n        m_gn = int(round(x_gn / np.pi))\n\n        x_tr = trust_region(x0)\n        m_tr = int(round(x_tr / np.pi))\n\n        final_results.append([m_gn, m_tr])\n\n    # --- Final Output Formatting ---\n    # Convert list of lists to the required string format \"[[a,b],[c,d],...]\"\n    result_str = \",\".join([f\"[{m_gn},{m_tr}]\" for m_gn, m_tr in final_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}