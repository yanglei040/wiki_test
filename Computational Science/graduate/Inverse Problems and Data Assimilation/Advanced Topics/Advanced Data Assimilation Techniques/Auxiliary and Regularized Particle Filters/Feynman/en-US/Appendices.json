{
    "hands_on_practices": [
        {
            "introduction": "Regularization is a powerful technique to counteract particle degeneracy by introducing controlled noise, but choosing the right amount of noise is critical. This exercise provides a foundational understanding of how to set the regularization bandwidth in a principled manner . By using a simple linear-Gaussian model, you will connect the regularization strength to the Fisher information of the likelihood, offering deep insight into how the \"informativeness\" of a new observation should guide the diversification of the particle set.",
            "id": "3366177",
            "problem": "Consider a scalar static parameter $\\theta \\in \\mathbb{R}$ to be estimated sequentially in a data assimilation setting. At time $t$, the prior distribution is Gaussian, $\\theta \\sim \\mathcal{N}(m_{t-1}, \\tau_{t-1}^{2})$, and the observation model is $y_{t} = \\theta + \\varepsilon_{t}$ with $\\varepsilon_{t} \\sim \\mathcal{N}(0, \\sigma^{2})$, independent of $\\theta$. An Auxiliary Particle Filter (APF) with a Regularized Particle Filter (RPF) step is used: after importance weighting and resampling guided by the local predictive likelihood, the particles are regularized by adding an independent Gaussian jitter $\\zeta \\sim \\mathcal{N}(0, h_{t}^{2})$ to each resampled particle. The bandwidth $h_{t}$ is chosen via the local Fisher information $I_{t}(\\theta)$ as $h_{t} = c\\, I_{t}(\\theta)^{-1/2}$ for a fixed constant $c \\in (0, \\infty)$, where $I_{t}(\\theta)$ is defined by $I_{t}(\\theta) = -\\mathbb{E}\\left[\\partial^{2}_{\\theta} \\ln p(y_{t} \\mid \\theta)\\right]$ with the expectation taken under the observation model at the true parameter value.\n\nStarting from the definitions of Bayesian updating, Fisher information, and the properties of Gaussian convolution, derive the exact expression for the contraction retention factor\n$$\\kappa(c) = \\frac{\\text{exact posterior variance at time } t}{\\text{regularized posterior variance at time } t},$$\nin terms of $\\tau_{t-1}$, $\\sigma$, and $c$. Your answer must be a single closed-form analytic expression. No rounding is required, and no units are needed.",
            "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The parameter to be estimated is a scalar $\\theta \\in \\mathbb{R}$.\n- The setting is a sequential data assimilation at time $t$.\n- The prior distribution for $\\theta$ is Gaussian: $\\theta \\sim \\mathcal{N}(m_{t-1}, \\tau_{t-1}^{2})$.\n- The observation model is linear and Gaussian: $y_{t} = \\theta + \\varepsilon_{t}$.\n- The observation noise is $\\varepsilon_{t} \\sim \\mathcal{N}(0, \\sigma^{2})$, and is independent of $\\theta$.\n- The estimation method is an Auxiliary Particle Filter (APF) with a Regularized Particle Filter (RPF) step.\n- The regularization involves adding a Gaussian jitter $\\zeta \\sim \\mathcal{N}(0, h_{t}^{2})$ to each resampled particle.\n- The regularization bandwidth $h_{t}$ is defined as $h_{t} = c\\, I_{t}(\\theta)^{-1/2}$, where $c$ is a fixed positive constant, $c \\in (0, \\infty)$.\n- The Fisher information $I_{t}(\\theta)$ is defined as $I_{t}(\\theta) = -\\mathbb{E}\\left[\\partial^{2}_{\\theta} \\ln p(y_{t} \\mid \\theta)\\right]$, with the expectation taken under the observation model $p(y_t | \\theta)$.\n- The objective is to derive the contraction retention factor $\\kappa(c) = \\frac{\\text{exact posterior variance at time } t}{\\text{regularized posterior variance at time } t}$.\n- The final expression for $\\kappa(c)$ must be in terms of $\\tau_{t-1}$, $\\sigma$, and $c$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the established principles of Bayesian statistics, filtering theory (specifically, particle filters), and information theory (Fisher information). All models and concepts (Gaussian distributions, linear models, RPF, APF) are standard in the field of inverse problems and data assimilation.\n- **Well-Posed:** The problem is fully specified. It provides all necessary models, parameters, and definitions to derive the requested quantity. The term \"exact posterior variance\" refers to the analytical Bayesian posterior variance in this simple conjugate model. The \"regularized posterior variance\" is also well-defined as the variance resulting from the convolution of the exact posterior with the regularization kernel, a standard operation in kernel density estimation and regularized particle filters. A unique, meaningful solution can be derived.\n- **Objective:** The problem is stated in precise, mathematical language, free from any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. A solution will be derived.\n\n### Derivation of the Contraction Retention Factor\n\nThe contraction retention factor $\\kappa(c)$ is the ratio of two variances: the exact posterior variance and the regularized posterior variance. We will derive each of these quantities sequentially.\n\nFirst, we calculate the exact posterior variance at time $t$, denoted as $\\tau_{\\text{post}}^{2}$. The problem describes a standard Bayesian update with a Gaussian prior and a Gaussian likelihood.\nThe prior distribution is $p(\\theta) = \\mathcal{N}(\\theta \\mid m_{t-1}, \\tau_{t-1}^{2})$.\nThe likelihood function, derived from the observation model $y_{t} = \\theta + \\varepsilon_{t}$ where $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$, is $p(y_{t} \\mid \\theta) = \\mathcal{N}(y_t \\mid \\theta, \\sigma^2)$.\n\nFor Gaussian distributions, the posterior is also Gaussian. Its precision (inverse variance) is the sum of the prior's precision and the likelihood's precision (with respect to $\\theta$).\nThe precision of the prior is $\\frac{1}{\\tau_{t-1}^{2}}$.\nThe precision of the likelihood is $\\frac{1}{\\sigma^{2}}$.\nThe precision of the posterior is therefore:\n$$\n(\\tau_{\\text{post}}^{2})^{-1} = \\frac{1}{\\tau_{t-1}^{2}} + \\frac{1}{\\sigma^{2}} = \\frac{\\sigma^{2} + \\tau_{t-1}^{2}}{\\tau_{t-1}^{2}\\sigma^{2}}\n$$\nFrom this, the exact posterior variance is:\n$$\n\\tau_{\\text{post}}^{2} = \\frac{\\tau_{t-1}^{2}\\sigma^{2}}{\\tau_{t-1}^{2} + \\sigma^{2}}\n$$\n\nNext, we determine the regularized posterior variance. The regularization step adds an independent Gaussian jitter $\\zeta \\sim \\mathcal{N}(0, h_{t}^{2})$ to the particles, which, in the large-particle limit, are distributed according to the exact posterior $\\mathcal{N}(m_t, \\tau_{\\text{post}}^{2})$. Let $\\theta_{\\text{post}}$ be a random variable representing a draw from the exact posterior. The regularized variable is $\\theta_{\\text{reg}} = \\theta_{\\text{post}} + \\zeta$. Since $\\theta_{\\text{post}}$ and $\\zeta$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\text{Var}(\\theta_{\\text{reg}}) = \\text{Var}(\\theta_{\\text{post}}) + \\text{Var}(\\zeta) = \\tau_{\\text{post}}^{2} + h_{t}^{2}\n$$\nThis is the regularized posterior variance. To proceed, we must calculate $h_{t}^{2}$.\n\nThe bandwidth $h_{t}$ depends on the Fisher information $I_{t}(\\theta)$ via $h_{t}^{2} = c^{2} I_{t}(\\theta)^{-1}$. We now compute $I_{t}(\\theta)$.\nThe log-likelihood is:\n$$\n\\ln p(y_{t} \\mid \\theta) = \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{t} - \\theta)^{2}}{2\\sigma^{2}}\\right) \\right) = -\\frac{1}{2}\\ln(2\\pi\\sigma^{2}) - \\frac{(y_{t} - \\theta)^{2}}{2\\sigma^{2}}\n$$\nThe first derivative with respect to $\\theta$ is:\n$$\n\\frac{\\partial}{\\partial\\theta} \\ln p(y_{t} \\mid \\theta) = - \\frac{2(y_{t} - \\theta)(-1)}{2\\sigma^{2}} = \\frac{y_{t} - \\theta}{\\sigma^{2}}\n$$\nThe second derivative with respect to $\\theta$ is:\n$$\n\\frac{\\partial^{2}}{\\partial\\theta^{2}} \\ln p(y_{t} \\mid \\theta) = -\\frac{1}{\\sigma^{2}}\n$$\nThe Fisher information is the negative expectation of this quantity. Since the second derivative is a constant and does not depend on the random variable $y_{t}$, the expectation is trivial:\n$$\nI_{t}(\\theta) = -\\mathbb{E}\\left[ -\\frac{1}{\\sigma^{2}} \\right] = \\frac{1}{\\sigma^{2}}\n$$\nNow we can determine the jitter variance $h_{t}^{2}$:\n$$\nh_{t}^{2} = c^{2} I_{t}(\\theta)^{-1} = c^{2} \\left(\\frac{1}{\\sigma^{2}}\\right)^{-1} = c^{2}\\sigma^{2}\n$$\nThe regularized posterior variance is therefore:\n$$\n\\text{Var}(\\theta_{\\text{reg}}) = \\tau_{\\text{post}}^{2} + c^{2}\\sigma^{2} = \\frac{\\tau_{t-1}^{2}\\sigma^{2}}{\\tau_{t-1}^{2} + \\sigma^{2}} + c^{2}\\sigma^{2}\n$$\n\nFinally, we can compute the contraction retention factor $\\kappa(c)$:\n$$\n\\kappa(c) = \\frac{\\text{exact posterior variance}}{\\text{regularized posterior variance}} = \\frac{\\tau_{\\text{post}}^{2}}{\\tau_{\\text{post}}^{2} + h_{t}^{2}}\n$$\nSubstituting the derived expressions:\n$$\n\\kappa(c) = \\frac{\\frac{\\tau_{t-1}^{2}\\sigma^{2}}{\\tau_{t-1}^{2} + \\sigma^{2}}}{\\frac{\\tau_{t-1}^{2}\\sigma^{2}}{\\tau_{t-1}^{2} + \\sigma^{2}} + c^{2}\\sigma^{2}}\n$$\nTo simplify this complex fraction, we can multiply the numerator and the denominator by $\\tau_{t-1}^{2} + \\sigma^{2}$:\n$$\n\\kappa(c) = \\frac{\\tau_{t-1}^{2}\\sigma^{2}}{\\tau_{t-1}^{2}\\sigma^{2} + c^{2}\\sigma^{2}(\\tau_{t-1}^{2} + \\sigma^{2})}\n$$\nSince $\\sigma > 0$ is a physical parameter (standard deviation of noise), $\\sigma^{2} \\neq 0$, and we can factor out $\\sigma^{2}$ from the denominator and cancel it with the numerator's $\\sigma^{2}$:\n$$\n\\kappa(c) = \\frac{\\tau_{t-1}^{2}}{\\tau_{t-1}^{2} + c^{2}(\\tau_{t-1}^{2} + \\sigma^{2})}\n$$\nThis is the final closed-form analytic expression for the contraction retention factor. It can also be written as $\\kappa(c) = \\frac{\\tau_{t-1}^{2}}{(1+c^{2})\\tau_{t-1}^{2} + c^{2}\\sigma^{2}}$. We will use the former representation in the final answer.",
            "answer": "$$\\boxed{\\frac{\\tau_{t-1}^{2}}{\\tau_{t-1}^{2} + c^{2}(\\tau_{t-1}^{2} + \\sigma^{2})}}$$"
        },
        {
            "introduction": "The resampling step is the heart of any particle filter, but it is also the primary source of sample impoverishment. Different resampling schemes have been developed to mitigate this issue, each with unique statistical properties. This practice challenges you to perform a rigorous comparison of three fundamental methods: systematic, stratified, and residual resampling . By deriving the exact conditional variance of an estimator under each scheme, you will gain a quantitative understanding of their impact on estimation accuracy and appreciate the trade-offs involved in their implementation.",
            "id": "3366188",
            "problem": "Consider a discrete-time state-space model being estimated by the Auxiliary Particle Filter (APF). At time $t$, let the particle set be $\\{x_{t}^{(i)},w_{t}^{(i)}\\}_{i=1}^{N}$, where $w_{t}^{(i)} \\ge 0$ are normalized importance weights satisfying $\\sum_{i=1}^{N} w_{t}^{(i)} = 1$. Let $m_{t}^{(i)} > 0$ denote the first-stage APF multipliers (also called auxiliary weights) used to define the APF ancestor-selection probabilities\n$$\nr_{t}^{(i)} \\;=\\; \\frac{w_{t}^{(i)} m_{t}^{(i)}}{\\sum_{k=1}^{N} w_{t}^{(k)} m_{t}^{(k)}}, \\quad i=1,\\dots,N.\n$$\nLet $\\phi:\\mathcal{X}\\to\\mathbb{R}$ be a bounded measurable test function and define $\\phi_{t}^{(i)} = \\phi\\!\\left(x_{t}^{(i)}\\right)$. The target quantity is the weighted Monte Carlo estimator\n$$\n\\hat{\\phi}_{t} \\;=\\; \\sum_{i=1}^{N} w_{t}^{(i)} \\,\\phi_{t}^{(i)}.\n$$\nSuppose resampling is performed according to $r_{t}^{(i)}$ using one of the following schemes: systematic resampling, stratified resampling, or residual resampling. To estimate $\\hat{\\phi}_{t}$ from resampled ancestors in a way that preserves unbiasedness with respect to the original $w_{t}^{(i)}$, consider the Horvitz–Thompson style resampling estimator\n$$\n\\hat{\\phi}_{t}^{(R)} \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} \\frac{w_{t}^{\\big(I_{j}\\big)}}{r_{t}^{\\big(I_{j}\\big)}} \\,\\phi_{t}^{\\big(I_{j}\\big)},\n$$\nwhere $I_{j} \\in \\{1,\\dots,N\\}$ is the resampled ancestor index for draw $j$, selected according to the scheme’s mechanism with marginal selection probabilities $r_{t}^{(i)}$.\n\nStarting from first principles of resampling-based Monte Carlo estimation and conditional variance, derive the closed-form conditional variance of $\\hat{\\phi}_{t}^{(R)}$ given $\\{x_{t}^{(i)}, w_{t}^{(i)}, m_{t}^{(i)}\\}_{i=1}^{N}$ under each scheme:\n- Systematic resampling: use the representation $U \\sim \\mathrm{Uniform}\\!\\left(0,\\frac{1}{N}\\right)$ and $U_{j} = \\frac{U + j - 1}{N}$ for $j=1,\\dots,N$; the index mapping is determined by the cumulative distribution of $r_{t}^{(i)}$.\n- Stratified resampling: use independent uniforms $U_{j} \\sim \\mathrm{Uniform}\\!\\left(\\frac{j-1}{N},\\frac{j}{N}\\right)$ for $j=1,\\dots,N$.\n- Residual resampling: decompose $N r_{t}^{(i)} = a_{t}^{(i)} + \\rho_{t}^{(i)}$ with $a_{t}^{(i)} = \\lfloor N r_{t}^{(i)} \\rfloor \\in \\mathbb{N}_{0}$ and $\\rho_{t}^{(i)} \\in [0,1)$, allocate deterministically $a_{t}^{(i)}$ copies of index $i$, and draw $R_{\\mathrm{res}} = \\sum_{i=1}^{N} \\rho_{t}^{(i)}$ residual copies from the categorical distribution with probabilities $\\bar{r}_{t}^{(i)} = \\rho_{t}^{(i)}/R_{\\mathrm{res}}$.\n\nExpress each variance in terms of the known weights $\\{w_{t}^{(i)}\\}$, the selection probabilities $\\{r_{t}^{(i)}\\}$, and the values $\\{\\phi_{t}^{(i)}\\}$. For stratified resampling, your final expression must be written using the cumulative sums $R_{t}^{(i)} = \\sum_{k=1}^{i} r_{t}^{(k)}$ with $R_{t}^{(0)} = 0$ and the intersection lengths\n$$\nL_{t}(i,j) \\;=\\; \\left| \\left[R_{t}^{(i-1)}, R_{t}^{(i)}\\right) \\,\\cap\\, \\left[ \\tfrac{j-1}{N}, \\tfrac{j}{N} \\right) \\right|, \\quad i,j \\in \\{1,\\dots,N\\}.\n$$\nFor systematic resampling, your final expression must be written as an integral over $U \\in \\left(0,\\frac{1}{N}\\right)$ of the squared estimator representation built from the same cumulative sums. For residual resampling, your final expression must use $\\{a_{t}^{(i)}\\}$, $\\{\\rho_{t}^{(i)}\\}$, and $R_{\\mathrm{res}}$.\n\nYour final answer must be a single analytical expression containing the three variances as entries of a one-row matrix, with no numerical approximation. No rounding is required and no physical units are involved.",
            "solution": "The derivations begin from the unbiasedness and conditional variance properties of Horvitz–Thompson style estimators and the structure of the three resampling schemes. Throughout, conditioning is on the fixed particle set $\\{x_{t}^{(i)}, w_{t}^{(i)}, m_{t}^{(i)}\\}_{i=1}^{N}$. Define the shorthand\n$$\n\\phi_{t}^{(i)} \\;=\\; \\phi\\!\\left(x_{t}^{(i)}\\right), \\qquad r_{t}^{(i)} \\;=\\; \\frac{w_{t}^{(i)} m_{t}^{(i)}}{\\sum_{k=1}^{N} w_{t}^{(k)} m_{t}^{(k)}}, \\qquad s_{t}^{(i)} \\;=\\; \\frac{w_{t}^{(i)}}{r_{t}^{(i)}} \\,\\phi_{t}^{(i)}.\n$$\nThe resampling estimator is\n$$\n\\hat{\\phi}_{t}^{(R)} \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} s_{t}^{\\big(I_{j}\\big)},\n$$\nwith marginal selection probabilities $r_{t}^{(i)}$ under all schemes. First, verify unbiasedness:\n$$\n\\mathbb{E}\\!\\left[ \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right]\n\\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{N} r_{t}^{(i)} s_{t}^{(i)}\n\\;=\\; \\sum_{i=1}^{N} r_{t}^{(i)} \\,\\frac{w_{t}^{(i)}}{r_{t}^{(i)}} \\,\\phi_{t}^{(i)}\n\\;=\\; \\sum_{i=1}^{N} w_{t}^{(i)} \\,\\phi_{t}^{(i)}\n\\;=\\; \\hat{\\phi}_{t}.\n$$\nThus each scheme yields a conditionally unbiased estimator of $\\hat{\\phi}_{t}$ when $s_{t}^{(i)}$ is used.\n\nThe conditional variance depends on the dependence structure of $\\{I_{j}\\}$. We now treat each scheme in turn.\n\nStratified resampling. Let $U_{j} \\sim \\mathrm{Uniform}\\!\\left(\\frac{j-1}{N},\\frac{j}{N}\\right)$ independently for $j=1,\\dots,N$, and define the cumulative sums\n$$\nR_{t}^{(i)} \\;=\\; \\sum_{k=1}^{i} r_{t}^{(k)}, \\quad R_{t}^{(0)} = 0,\n$$\nand index mapping $I_{j} = i$ whenever $U_{j} \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right)$. Then $\\hat{\\phi}_{t}^{(R)}$ can be written as\n$$\n\\hat{\\phi}_{t}^{(R)} \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} g_{t}(U_{j}), \\quad \\text{where} \\quad g_{t}(u) = s_{t}^{(i)} \\ \\text{if} \\ u \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right).\n$$\nSince $U_{j}$ are independent across $j$, we have\n$$\n\\mathrm{Var}\\!\\left( \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right)\n\\;=\\; \\frac{1}{N^{2}} \\sum_{j=1}^{N} \\mathrm{Var}\\!\\left( g_{t}(U_{j}) \\right).\n$$\nFor each stratum $j$, the expectation and second moment of $g_{t}(U_{j})$ over $U_{j} \\in \\left(\\frac{j-1}{N},\\frac{j}{N}\\right)$ equal\n$$\n\\mathbb{E}\\!\\left[ g_{t}(U_{j}) \\right] \\;=\\; N \\sum_{i=1}^{N} s_{t}^{(i)} \\, L_{t}(i,j), \n\\qquad \\mathbb{E}\\!\\left[ g_{t}(U_{j})^{2} \\right] \\;=\\; N \\sum_{i=1}^{N} \\left( s_{t}^{(i)} \\right)^{2} \\, L_{t}(i,j),\n$$\nwhere $L_{t}(i,j) = \\left| \\left[R_{t}^{(i-1)}, R_{t}^{(i)}\\right) \\cap \\left[ \\tfrac{j-1}{N}, \\tfrac{j}{N} \\right) \\right|$ is the intersection length. Therefore\n$$\n\\mathrm{Var}\\!\\left( g_{t}(U_{j}) \\right)\n\\;=\\; N \\sum_{i=1}^{N} \\left( s_{t}^{(i)} \\right)^{2} \\, L_{t}(i,j) \\;-\\; \\left( N \\sum_{i=1}^{N} s_{t}^{(i)} \\, L_{t}(i,j) \\right)^{2}.\n$$\nSumming over $j$ and using $\\sum_{j=1}^{N} L_{t}(i,j) = r_{t}^{(i)}$ yields the closed form\n$$\n\\mathrm{Var}_{\\mathrm{strat}}\\!\\left( \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right)\n\\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} r_{t}^{(i)} \\left( s_{t}^{(i)} \\right)^{2} \\;-\\; \\sum_{j=1}^{N} \\left( \\sum_{i=1}^{N} s_{t}^{(i)} \\, L_{t}(i,j) \\right)^{2}.\n$$\nSubstituting $s_{t}^{(i)} = \\frac{w_{t}^{(i)}}{r_{t}^{(i)}} \\phi_{t}^{(i)}$ simplifies the first term to $\\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\left(w_{t}^{(i)}\\right)^{2}}{r_{t}^{(i)}} \\left( \\phi_{t}^{(i)} \\right)^{2}$.\n\nResidual resampling. Decompose $N r_{t}^{(i)} = a_{t}^{(i)} + \\rho_{t}^{(i)}$ with $a_{t}^{(i)} = \\lfloor N r_{t}^{(i)} \\rfloor$ and $\\rho_{t}^{(i)} \\in [0,1)$. The scheme deterministically allocates $a_{t}^{(i)}$ copies of index $i$ and then draws $R_{\\mathrm{res}} = \\sum_{i=1}^{N} \\rho_{t}^{(i)}$ additional copies from the categorical distribution with probabilities $\\bar{r}_{t}^{(i)} = \\rho_{t}^{(i)} / R_{\\mathrm{res}}$. Writing\n$$\n\\hat{\\phi}_{t}^{(R)} \\;=\\; \\frac{1}{N} \\left( \\sum_{i=1}^{N} a_{t}^{(i)} s_{t}^{(i)} \\;+\\; \\sum_{j=1}^{R_{\\mathrm{res}}} s_{t}^{\\big(I'_{j}\\big)} \\right),\n$$\nwhere $I'_{j}$ are independent draws from $\\bar{r}_{t}$, the only random component is the residual sum. Hence\n$$\n\\mathrm{Var}_{\\mathrm{resid}}\\!\\left( \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right)\n\\;=\\; \\frac{1}{N^{2}} \\,\\mathrm{Var}\\!\\left( \\sum_{j=1}^{R_{\\mathrm{res}}} s_{t}^{\\big(I'_{j}\\big)} \\right)\n\\;=\\; \\frac{R_{\\mathrm{res}}}{N^{2}} \\left( \\sum_{i=1}^{N} \\bar{r}_{t}^{(i)} \\left( s_{t}^{(i)} \\right)^{2} \\;-\\; \\left( \\sum_{i=1}^{N} \\bar{r}_{t}^{(i)} s_{t}^{(i)} \\right)^{2} \\right).\n$$\nSubstituting $\\bar{r}_{t}^{(i)} = \\rho_{t}^{(i)} / R_{\\mathrm{res}}$ and simplifying gives\n$$\n\\mathrm{Var}_{\\mathrm{resid}}\\!\\left( \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right)\n\\;=\\; \\frac{1}{N^{2}} \\left( \\sum_{i=1}^{N} \\rho_{t}^{(i)} \\left( s_{t}^{(i)} \\right)^{2} \\;-\\; \\frac{1}{R_{\\mathrm{res}}} \\left( \\sum_{i=1}^{N} \\rho_{t}^{(i)} s_{t}^{(i)} \\right)^{2} \\right).\n$$\n\nSystematic resampling. Let $U \\sim \\mathrm{Uniform}\\!\\left(0,\\frac{1}{N}\\right)$, define $U_{j} = \\frac{U + j - 1}{N}$, and set $I_{j} = i$ when $U_{j} \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right)$. Define\n$$\nh_{t}(u) \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} g_{t}\\!\\left( \\tfrac{u + j - 1}{N} \\right)\n\\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{N} s_{t}^{(i)} \\,\\mathbf{1}\\!\\left\\{ \\tfrac{u + j - 1}{N} \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right) \\right\\}.\n$$\nThen $\\hat{\\phi}_{t}^{(R)} = h_{t}(U)$ and\n$$\n\\mathrm{Var}_{\\mathrm{sys}}\\!\\left( \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right)\n\\;=\\; \\mathbb{E}\\!\\left[ h_{t}(U)^{2} \\right] \\;-\\; \\left( \\mathbb{E}\\!\\left[ h_{t}(U) \\right] \\right)^{2}.\n$$\nNoting that $U$ has density $N$ on $\\left(0,\\tfrac{1}{N}\\right)$ and $\\mathbb{E}\\!\\left[ h_{t}(U) \\right] = \\sum_{i=1}^{N} w_{t}^{(i)} \\phi_{t}^{(i)}$ by unbiasedness, we obtain the integral representation\n$$\n\\mathrm{Var}_{\\mathrm{sys}}\\!\\left( \\hat{\\phi}_{t}^{(R)} \\,\\big|\\, \\{x_{t}^{(i)},w_{t}^{(i)},m_{t}^{(i)}\\} \\right)\n\\;=\\; N \\int_{0}^{\\frac{1}{N}} \\left( \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{N} s_{t}^{(i)} \\,\\mathbf{1}\\!\\left\\{ \\tfrac{u + j - 1}{N} \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right) \\right\\} \\right)^{2} \\,\\mathrm{d}u \\;-\\; \\left( \\sum_{i=1}^{N} w_{t}^{(i)} \\phi_{t}^{(i)} \\right)^{2}.\n$$\n\nCollecting the three expressions, and reinstating the shorthand $s_{t}^{(i)} = \\frac{w_{t}^{(i)}}{r_{t}^{(i)}} \\phi_{t}^{(i)}$, $R_{t}^{(i)} = \\sum_{k=1}^{i} r_{t}^{(k)}$, $L_{t}(i,j) = \\left| \\left[R_{t}^{(i-1)}, R_{t}^{(i)}\\right) \\cap \\left[ \\tfrac{j-1}{N}, \\tfrac{j}{N} \\right) \\right|$, and $R_{\\mathrm{res}} = \\sum_{i=1}^{N} \\rho_{t}^{(i)}$ with $\\rho_{t}^{(i)} = N r_{t}^{(i)} - \\lfloor N r_{t}^{(i)} \\rfloor$, we have:\n\n- Systematic resampling variance:\n$$\nV_{\\mathrm{sys}} \\;=\\; N \\int_{0}^{\\frac{1}{N}} \\left( \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{N} s_{t}^{(i)} \\,\\mathbf{1}\\!\\left\\{ \\tfrac{u + j - 1}{N} \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right) \\right\\} \\right)^{2} \\,\\mathrm{d}u \\;-\\; \\left( \\sum_{i=1}^{N} w_{t}^{(i)} \\phi_{t}^{(i)} \\right)^{2}.\n$$\n\n- Stratified resampling variance:\n$$\nV_{\\mathrm{strat}} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\left(w_{t}^{(i)}\\right)^{2}}{r_{t}^{(i)}} \\left( \\phi_{t}^{(i)} \\right)^{2} \\;-\\; \\sum_{j=1}^{N} \\left( \\sum_{i=1}^{N} s_{t}^{(i)} \\, L_{t}(i,j) \\right)^{2}.\n$$\n\n- Residual resampling variance:\n$$\nV_{\\mathrm{resid}} \\;=\\; \\frac{1}{N^{2}} \\left( \\sum_{i=1}^{N} \\rho_{t}^{(i)} \\left( s_{t}^{(i)} \\right)^{2} \\;-\\; \\frac{1}{R_{\\mathrm{res}}} \\left( \\sum_{i=1}^{N} \\rho_{t}^{(i)} s_{t}^{(i)} \\right)^{2} \\right).\n$$\n\nThese are closed-form conditional variances of the Horvitz–Thompson resampling estimator of $\\hat{\\phi}_{t}$ under systematic, stratified, and residual resampling in the Auxiliary Particle Filter.",
            "answer": "$$\\boxed{\\begin{pmatrix}\nN \\displaystyle\\int_{0}^{\\frac{1}{N}} \\left( \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{i=1}^{N} s_{t}^{(i)} \\,\\mathbf{1}\\!\\left\\{ \\tfrac{u + j - 1}{N} \\in \\left[ R_{t}^{(i-1)}, R_{t}^{(i)} \\right) \\right\\} \\right)^{2} \\mathrm{d}u \\;-\\; \\left( \\sum_{i=1}^{N} w_{t}^{(i)} \\phi_{t}^{(i)} \\right)^{2}\n&\n\\frac{1}{N} \\displaystyle\\sum_{i=1}^{N} \\frac{\\left(w_{t}^{(i)}\\right)^{2}}{r_{t}^{(i)}} \\left( \\phi_{t}^{(i)} \\right)^{2} \\;-\\; \\displaystyle\\sum_{j=1}^{N} \\left( \\sum_{i=1}^{N} s_{t}^{(i)} \\, L_{t}(i,j) \\right)^{2}\n&\n\\frac{1}{N^{2}} \\left( \\displaystyle\\sum_{i=1}^{N} \\rho_{t}^{(i)} \\left( s_{t}^{(i)} \\right)^{2} \\;-\\; \\frac{1}{R_{\\mathrm{res}}} \\left( \\displaystyle\\sum_{i=1}^{N} \\rho_{t}^{(i)} s_{t}^{(i)} \\right)^{2} \\right)\n\\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving beyond fixed parameters, advanced particle filters can adapt to the data stream for improved performance. The Liu–West filter is a prominent example of a regularized filter that uses a shrinkage parameter to balance prior belief and particle spread. This exercise guides you through the design and analysis of an adaptive system for this parameter . You will derive the empirical variance of the particle set and then analyze the stability of an adaptation rule that tunes the shrinkage factor in real time, a crucial skill for developing robust and intelligent estimation systems.",
            "id": "3366149",
            "problem": "Consider a static-parameter data assimilation problem in which the parameter $\\theta$ is approximated at time $t-1$ by two particles with equal weights, located at $\\theta_{t-1}^{(1)} = m + d$ and $\\theta_{t-1}^{(2)} = m - d$, where $m \\in \\mathbb{R}$ and $d > 0$. A new observation $y_t$ arrives, generated by a Gaussian likelihood $y_t \\mid \\theta \\sim \\mathcal{N}(\\theta, R)$ with known variance $R > 0$, and suppose $y_t = m + \\Delta$ for some $\\Delta \\in \\mathbb{R}$. An Auxiliary Particle Filter (APF) coupled with the Liu–West regularization proposes intermediate means for the parameter using the shrinkage rule\n$$\n\\mu_i(a) = a \\,\\theta_{t-1}^{(i)} + (1-a)\\, \\bar{\\theta}_{t-1}, \\quad \\bar{\\theta}_{t-1} = m, \\quad i \\in \\{1,2\\},\n$$\nand injects Gaussian jitter $\\epsilon^{(i)} \\sim \\mathcal{N}(0, h^2 d^2)$ to each $\\mu_i(a)$, with the Liu–West variance preservation constraint $a^2 + h^2 = 1$ and $a \\in (0,1)$. The APF first-stage selection probabilities are proportional to the predicted likelihoods evaluated at $\\mu_i(a)$,\n$$\np_i(a) \\propto \\exp\\!\\left( -\\frac{(y_t - \\mu_i(a))^2}{2R} \\right), \\quad i \\in \\{1,2\\},\n$$\nand the selected proposals are distributed as $\\theta_t^{(i)} = \\mu_i(a) + \\epsilon^{(i)}$.\n\nStarting from the definitions above, derive the empirical variance of the proposed $\\theta_t$ after selection and jitter, expressed as a function of $a$, $d$, $R$, and $\\Delta$. Then, using the well-tested Gaussian conjugate update for linear-Gaussian models, write the target posterior variance\n$$\nV^{\\ast} = \\left( d^{-2} + R^{-1} \\right)^{-1}.\n$$\nConsider the adaptation rule for the Liu–West shrinkage,\n$$\na_{t+1} = a_t + \\eta \\left( V_{\\mathrm{emp}}(a_t) - V^{\\ast} \\right),\n$$\nwhere $V_{\\mathrm{emp}}(a)$ is the empirical variance derived above and $\\eta > 0$ is a scalar gain. Analyze the stability of this one-dimensional discrete-time adaptation by linearizing about the fixed point $a^{\\ast}$ satisfying $V_{\\mathrm{emp}}(a^{\\ast}) = V^{\\ast}$, and derive the maximum allowable gain $\\eta_{\\max}$ for local asymptotic stability in terms of the derivative $V_{\\mathrm{emp}}'(a^{\\ast})$.\n\nFinally, for the specific, scientifically consistent parameter choices $d = 1$, $R = 0.5$, and $\\Delta = 1$, compute the numerical value of $\\eta_{\\max}$. Round your answer to four significant figures. The final answer must be a single real number without units.",
            "solution": "The problem asks for the derivation of the maximum stable gain $\\eta_{\\max}$ for an adaptation rule of the Liu–West shrinkage parameter $a$. This involves several steps: deriving the empirical variance of the particle set, analyzing the stability of the discrete-time adaptation map, and finally, computing a numerical value.\n\nFirst, we establish the expressions for the key quantities in the Auxiliary Particle Filter (APF) step. The prior particles are $\\theta_{t-1}^{(1)} = m + d$ and $\\theta_{t-1}^{(2)} = m - d$. The prior mean is $\\bar{\\theta}_{t-1} = \\frac{1}{2}(\\theta_{t-1}^{(1)} + \\theta_{t-1}^{(2)}) = m$. The shrinkage rule defines the intermediate means $\\mu_i(a)$ as:\n$$\n\\mu_1(a) = a\\,\\theta_{t-1}^{(1)} + (1-a)\\,\\bar{\\theta}_{t-1} = a(m+d) + (1-a)m = m + ad\n$$\n$$\n\\mu_2(a) = a\\,\\theta_{t-1}^{(2)} + (1-a)\\,\\bar{\\theta}_{t-1} = a(m-d) + (1-a)m = m - ad\n$$\nThe APF first-stage selection probabilities, $p_i(a)$, are proportional to the predicted likelihoods evaluated at these means. The observation is $y_t = m+\\Delta$ and the likelihood is Gaussian with variance $R$.\nThe weights $w_i(a)$ are given by $w_i(a) = \\exp\\left( -\\frac{(y_t - \\mu_i(a))^2}{2R} \\right)$.\nFor $i=1$: $y_t - \\mu_1(a) = (m+\\Delta) - (m+ad) = \\Delta - ad$.\nFor $i=2$: $y_t - \\mu_2(a) = (m+\\Delta) - (m-ad) = \\Delta + ad$.\nThe unnormalized weights are:\n$$\nw_1(a) = \\exp\\left( -\\frac{(\\Delta - ad)^2}{2R} \\right), \\quad w_2(a) = \\exp\\left( -\\frac{(\\Delta + ad)^2}{2R} \\right)\n$$\nThe normalized probabilities are $p_1(a) = \\frac{w_1(a)}{w_1(a)+w_2(a)}$ and $p_2(a) = \\frac{w_2(a)}{w_1(a)+w_2(a)}$.\n\nNext, we derive the empirical variance of the proposed particles, $V_{\\mathrm{emp}}(a)$. The proposed particles are $\\theta_t^{(j)} \\sim \\sum_{i=1}^2 p_i(a) \\mathcal{N}(\\mu_i(a), h^2 d^2)$, where $j \\in \\{1,2\\}$ indexes the new particles. This describes a Gaussian mixture distribution. The variance of a mixture distribution is given by the law of total variance:\n$$\nV_{\\mathrm{emp}}(a) = \\mathrm{Var}(\\theta_t) = E_k[\\mathrm{Var}(\\theta_t|k)] + \\mathrm{Var}_k(E[\\theta_t|k])\n$$\nwhere the expectation and variance are over the choice of the index $k \\in \\{1,2\\}$ with probabilities $\\{p_1(a), p_2(a)\\}$.\nThe inner conditional variance is $\\mathrm{Var}(\\theta_t|k=i) = \\mathrm{Var}(\\mu_i(a) + \\epsilon^{(i)}) = \\mathrm{Var}(\\epsilon^{(i)}) = h^2 d^2$. Since this is constant for any $i$, its expectation is $E_k[\\mathrm{Var}(\\theta_t|k)] = h^2 d^2$.\nThe inner conditional mean is $E[\\theta_t|k=i] = \\mu_i(a)$. The variance of this quantity over $k$ is:\n$$\n\\mathrm{Var}_k(E[\\theta_t|k]) = \\mathrm{Var}_k(\\mu_k(a)) = E[\\mu_k(a)^2] - (E[\\mu_k(a)])^2\n$$\n$$\n= (p_1(a)\\mu_1(a)^2 + p_2(a)\\mu_2(a)^2) - (p_1(a)\\mu_1(a) + p_2(a)\\mu_2(a))^2 = p_1(a)p_2(a)(\\mu_1(a)-\\mu_2(a))^2\n$$\nWe have $\\mu_1(a) - \\mu_2(a) = (m+ad) - (m-ad) = 2ad$.\nThe product of probabilities $p_1(a)p_2(a)$ can be simplified:\n$$\np_1(a)p_2(a) = \\frac{w_1(a)w_2(a)}{(w_1(a)+w_2(a))^2} = \\frac{\\exp\\left(-\\frac{(\\Delta-ad)^2+(\\Delta+ad)^2}{2R}\\right)}{\\left(\\exp\\left(-\\frac{(\\Delta-ad)^2}{2R}\\right) + \\exp\\left(-\\frac{(\\Delta+ad)^2}{2R}\\right)\\right)^2}\n$$\nLet the arguments of the exponentials in the denominator be $-L_1$ and $-L_2$. The product is $\\frac{\\exp(-L_1-L_2)}{(\\exp(-L_1)+\\exp(-L_2))^2} = \\frac{1}{(\\exp(L_1)+\\exp(L_2))(\\exp(-L_1)+\\exp(-L_2))} = \\frac{1}{2 + \\exp(L_1-L_2) + \\exp(-(L_1-L_2))}$.\n$L_1 - L_2 = \\frac{1}{2R}[(\\Delta+ad)^2 - (\\Delta-ad)^2] = \\frac{1}{2R}[4\\Delta ad] = \\frac{2\\Delta ad}{R}$.\nSo, $p_1(a)p_2(a) = \\frac{1}{2+2\\cosh(\\frac{2\\Delta ad}{R})} = \\frac{1}{4\\cosh^2(\\frac{\\Delta ad}{R})}$.\nSubstituting these into the variance expression:\n$$\nV_{\\mathrm{emp}}(a) = h^2 d^2 + p_1(a)p_2(a)(\\mu_1(a)-\\mu_2(a))^2 = h^2 d^2 + \\frac{1}{4\\cosh^2(\\frac{\\Delta ad}{R})} (2ad)^2\n$$\nUsing the Liu-West constraint $h^2 = 1 - a^2$, we get:\n$$\nV_{\\mathrm{emp}}(a) = (1-a^2) d^2 + \\frac{4a^2d^2}{4\\cosh^2(\\frac{\\Delta ad}{R})} = d^2 \\left( 1 - a^2 + \\frac{a^2}{\\cosh^2(\\frac{\\Delta ad}{R})} \\right)\n$$\nUsing the identity $1 - \\mathrm{sech}^2(x) = \\tanh^2(x)$, or $1 - \\frac{1}{\\cosh^2(x)} = \\tanh^2(x)$:\n$$\nV_{\\mathrm{emp}}(a) = d^2 \\left( 1 - a^2 \\left( 1 - \\frac{1}{\\cosh^2(\\frac{\\Delta ad}{R})} \\right) \\right) = d^2 \\left( 1 - a^2 \\tanh^2\\left(\\frac{\\Delta ad}{R}\\right) \\right)\n$$\nThis is the required expression for the empirical variance.\n\nThe problem provides the target posterior variance for a Gaussian prior with variance $d^2$ and a Gaussian likelihood with variance $R$, which is $V^{\\ast} = \\left( (d^2)^{-1} + R^{-1} \\right)^{-1} = \\frac{d^2 R}{d^2 + R}$.\nThe adaptation rule is $a_{t+1} = a_t + \\eta (V_{\\mathrm{emp}}(a_t) - V^{\\ast})$. This is a one-dimensional discrete map $a_{t+1} = g(a_t)$ with $g(a) = a + \\eta (V_{\\mathrm{emp}}(a) - V^{\\ast})$. A fixed point $a^{\\ast}$ of this map satisfies $g(a^{\\ast}) = a^{\\ast}$, which implies $V_{\\mathrm{emp}}(a^{\\ast}) = V^{\\ast}$.\nLocal asymptotic stability around $a^{\\ast}$ requires $|g'(a^{\\ast})| < 1$.\nThe derivative is $g'(a) = 1 + \\eta V_{\\mathrm{emp}}'(a)$. So, we need $|1 + \\eta V_{\\mathrm{emp}}'(a^{\\ast})| < 1$, which is equivalent to $-2 < \\eta V_{\\mathrm{emp}}'(a^{\\ast}) < 0$.\nTo find $V_{\\mathrm{emp}}'(a)$, let $C = \\frac{\\Delta d}{R}$. Then $V_{\\mathrm{emp}}(a) = d^2(1 - a^2\\tanh^2(Ca))$.\n$$\nV_{\\mathrm{emp}}'(a) = -d^2 \\frac{d}{da} \\left( [a \\tanh(Ca)]^2 \\right) = -d^2 \\cdot 2 a \\tanh(Ca) \\left( \\tanh(Ca) + a C \\mathrm{sech}^2(Ca) \\right)\n$$\n$$\nV_{\\mathrm{emp}}'(a) = -2d^2 \\left( a\\tanh^2(Ca) + Ca^2\\tanh(Ca)\\mathrm{sech}^2(Ca) \\right)\n$$\nSince $a, C, d, R$ are positive, $\\tanh$ and $\\mathrm{sech}^2$ are positive, thus $V_{\\mathrm{emp}}'(a) < 0$ for $a \\in (0,1)$. The stability condition becomes $0 < \\eta < \\frac{-2}{V_{\\mathrm{emp}}'(a^{\\ast})}$.\nThe maximum stable gain is $\\eta_{\\max} = \\frac{-2}{V_{\\mathrm{emp}}'(a^{\\ast})}$.\n\nWe now substitute the specific parameters $d=1$, $R=0.5$, and $\\Delta=1$.\nThe constant $C$ is $C = \\frac{1 \\cdot 1}{0.5} = 2$.\nThe target variance is $V^{\\ast} = \\frac{1^2 \\cdot 0.5}{1^2+0.5} = \\frac{0.5}{1.5} = \\frac{1}{3}$.\nThe fixed-point equation $V_{\\mathrm{emp}}(a^{\\ast}) = V^{\\ast}$ becomes:\n$$\n1^2 \\left( 1 - (a^{\\ast})^2 \\tanh^2(2a^{\\ast}) \\right) = \\frac{1}{3} \\implies (a^{\\ast})^2 \\tanh^2(2a^{\\ast}) = \\frac{2}{3}\n$$\nTaking the positive square root (since $a^{\\ast} > 0$): $a^{\\ast} \\tanh(2a^{\\ast}) = \\sqrt{\\frac{2}{3}}$.\nSolving this transcendental equation numerically yields $a^{\\ast} \\approx 0.868808$.\nNow, we compute $V_{\\mathrm{emp}}'(a^{\\ast})$. Let's evaluate the terms:\n$Ca^{\\ast} = 2a^{\\ast} \\approx 1.737616$.\n$\\tanh(Ca^{\\ast}) \\approx 0.939824$.\n$\\tanh^2(Ca^{\\ast}) \\approx 0.883270$.\n$\\mathrm{sech}^2(Ca^{\\ast}) = 1 - \\tanh^2(Ca^{\\ast}) \\approx 1 - 0.883270 = 0.116730$.\nUsing the expression for the derivative with $d=1, C=2$:\n$$\nV_{\\mathrm{emp}}'(a^{\\ast}) = -2(1)^2 \\left( a^{\\ast}\\tanh^2(2a^{\\ast}) + 2(a^{\\ast})^2\\tanh(2a^{\\ast})\\mathrm{sech}^2(2a^{\\ast}) \\right)\n$$\nGroup terms using the fixed-point relation $a^\\ast \\tanh(2a^\\ast) = \\sqrt{2/3}$:\n$$\nV_{\\mathrm{emp}}'(a^{\\ast}) = -2 \\left( \\tanh(2a^{\\ast}) [a^{\\ast}\\tanh(2a^{\\ast})] + 2a^{\\ast}[a^{\\ast}\\tanh(2a^{\\ast})]\\mathrm{sech}^2(2a^{\\ast}) \\right)\n$$\n$$\nV_{\\mathrm{emp}}'(a^{\\ast}) = -2\\sqrt{\\frac{2}{3}} \\left( \\tanh(2a^{\\ast}) + 2a^{\\ast}\\mathrm{sech}^2(2a^{\\ast}) \\right)\n$$\nSubstituting numerical values:\n$$\nV_{\\mathrm{emp}}'(a^{\\ast}) \\approx -2\\sqrt{\\frac{2}{3}} \\left( 0.939824 + 2(0.868808)(0.116730) \\right)\n$$\n$$\nV_{\\mathrm{emp}}'(a^{\\ast}) \\approx -2(0.816497) \\left( 0.939824 + 0.202816 \\right) \\approx -1.63299 \\left( 1.14264 \\right) \\approx -1.86548\n$$\nFinally, we compute the maximum gain $\\eta_{\\max}$:\n$$\n\\eta_{\\max} = \\frac{-2}{V_{\\mathrm{emp}}'(a^{\\ast})} \\approx \\frac{-2}{-1.86548} \\approx 1.07209\n$$\nRounding to four significant figures, the result is $1.072$.",
            "answer": "$$\\boxed{1.072}$$"
        }
    ]
}