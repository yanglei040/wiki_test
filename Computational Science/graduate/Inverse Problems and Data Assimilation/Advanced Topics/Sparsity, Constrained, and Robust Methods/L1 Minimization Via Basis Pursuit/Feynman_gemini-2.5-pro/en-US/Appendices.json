{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering a new technique is to solve a simple problem by hand. This exercise guides you through solving a Basis Pursuit problem from first principles, using the Karush-Kuhn-Tucker (KKT) optimality conditions to explore candidate solutions based on their support. By systematically enumerating potential sparse supports, you will gain a concrete understanding of how $\\ell_1$ minimization identifies the sparsest vector consistent with the observations .",
            "id": "3394537",
            "problem": "Consider the Basis Pursuit (BP) formulation of a linear inverse problem in the context of data assimilation: among all vectors $x \\in \\mathbb{R}^{3}$ satisfying the linear equality constraint $A x = y$, find the one that minimizes the $\\ell_{1}$-norm $\\|x\\|_{1}$. The matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and the data $y \\in \\mathbb{R}^{2}$ are given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n\\frac{3}{2} \\\\\n-\\frac{5}{4}\n\\end{pmatrix}.\n$$\nStarting from the fundamental definition of the BP problem, the subgradient characterization of the $\\ell_{1}$-norm, and optimality conditions for equality-constrained convex optimization problems, explicitly solve\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y,\n$$\nby enumerating active sets (supports) and verifying which candidate solutions both satisfy $A x = y$ and minimize $\\|x\\|_{1}$. Your derivation must begin from the definitions and optimality conditions and proceed by logically ruling out non-optimal supports. Express your final answer as the exact optimal $x^{\\star}$ using the LaTeX row-matrix $\\mathrm{pmatrix}$ syntax. No rounding is required; provide exact rational values.",
            "solution": "The problem is to solve the Basis Pursuit (BP) optimization problem, which seeks the vector $x \\in \\mathbb{R}^{3}$ with the minimum $\\ell_{1}$-norm that satisfies a system of linear equations. The problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\nwhere $\\|x\\|_{1} = \\sum_{i=1}^{3} |x_i|$, and the given matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and vector $y \\in \\mathbb{R}^{2}$ are:\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n\\frac{3}{2} \\\\\n-\\frac{5}{4}\n\\end{pmatrix}.\n$$\nThis is a convex optimization problem, as the objective function, the $\\ell_{1}$-norm, is convex, and the feasible set defined by the linear equality constraint $A x = y$ is an affine (and thus convex) subspace of $\\mathbb{R}^{3}$.\n\nWe utilize the Karush-Kuhn-Tucker (KKT) conditions for constrained optimization. Since the $\\ell_1$-norm is not differentiable everywhere, we must use the concept of subgradients. The Lagrangian for this problem is:\n$$\nL(x, \\nu) = \\|x\\|_{1} + \\nu^T (A x - y)\n$$\nwhere $\\nu \\in \\mathbb{R}^{2}$ is the vector of Lagrange multipliers. The first-order optimality condition states that the subgradient of the Lagrangian with respect to $x$ must contain the zero vector at the optimal solution $x^{\\star}$:\n$$\n0 \\in \\partial_x L(x^{\\star}, \\nu^{\\star}) = \\partial \\|x^{\\star}\\|_{1} + A^T \\nu^{\\star}\n$$\nHere, $\\partial \\|x^{\\star}\\|_{1}$ is the subdifferential of the $\\ell_{1}$-norm at $x^{\\star}$. This condition implies the existence of a vector $s \\in \\partial \\|x^{\\star}\\|_{1}$ such that $s + A^T \\nu^{\\star} = 0$, or $A^T \\nu^{\\star} = -s$.\n\nThe subdifferential $\\partial \\|x\\|_{1}$ is the set of vectors $s \\in \\mathbb{R}^3$ whose components $s_i$ satisfy:\n$$\ns_i = \\begin{cases}\n\\text{sign}(x_i)  \\text{if } x_i \\neq 0 \\\\\nc  \\text{if } x_i = 0, \\text{ for some } c \\in [-1, 1]\n\\end{cases}\n$$\nLet $S = \\text{supp}(x^{\\star}) = \\{i \\mid x_i^{\\star} \\neq 0\\}$ be the active set or support of the solution. The KKT conditions for an optimal pair $(x^{\\star}, \\nu^{\\star})$ are:\n$1$. Primal feasibility: $A x^{\\star} = y$.\n$2$. Dual feasibility: There exists a Lagrange multiplier $\\nu^{\\star} \\in \\mathbb{R}^2$ such that the vector $s = -A^T \\nu^{\\star}$ satisfies:\n    a) $s_i = \\text{sign}(x_i^{\\star})$ for all $i \\in S$.\n    b) $|s_i| \\le 1$ for all $i \\notin S$.\n\nThe system $Ax=y$ is underdetermined ($2$ equations, $3$ unknowns), so sparse solutions are expected. We will test candidate solutions by enumerating supports $S$. The size of the support, $|S|$, is expected to be at most the number of rows of $A$, which is $2$.\n\nLet's examine candidate solutions based on the size of their support.\n\nCase 1: Support size $|S|=1$.\nThis implies that only one component of $x$ is non-zero. Let $x_i$ be this component.\n- If $S=\\{1\\}$, then $x_2=x_3=0$. The constraint becomes $A_1 x_1 = y$, where $A_1$ is the first column of $A$:\n$\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} x_1 = \\begin{pmatrix} 3/2 \\\\ -5/4 \\end{pmatrix}$. This system is inconsistent, since the second equation is $0 \\cdot x_1 = -5/4$.\n- If $S=\\{2\\}$, then $x_1=x_3=0$. The constraint becomes $A_2 x_2 = y$:\n$\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} x_2 = \\begin{pmatrix} 3/2 \\\\ -5/4 \\end{pmatrix}$. This system is inconsistent, since the first equation is $0 \\cdot x_2 = 3/2$.\n- If $S=\\{3\\}$, then $x_1=x_2=0$. The constraint becomes $A_3 x_3 = y$:\n$\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} x_3 = \\begin{pmatrix} 3/2 \\\\ -5/4 \\end{pmatrix}$. This system is inconsistent, since $x_3$ cannot equal both $3/2$ and $-5/4$.\nThus, no solution with a support of size $1$ exists.\n\nCase 2: Support size $|S|=2$.\nThere are $\\binom{3}{2} = 3$ possible supports of size $2$. For a given support $S$, we set $x_i=0$ for $i \\notin S$ and solve the reduced system $A_S x_S = y$, where $A_S$ is the submatrix of $A$ containing columns indexed by $S$, and $x_S$ is the subvector of non-zero components of $x$.\n\n- Support $S=\\{1, 2\\}$:\nWe set $x_3=0$. The system becomes $A_{\\{1,2\\}}x_{\\{1,2\\}} = y$:\n$$ \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nThis gives $x_1 = 3/2$ and $x_2 = -5/4$. The candidate solution is $x^{(1)} = \\begin{pmatrix} 3/2  -5/4  0 \\end{pmatrix}^T$.\nLet's check the KKT conditions. Primal feasibility is met by construction. For dual feasibility, we need to find a $\\nu \\in \\mathbb{R}^2$. The support is $S=\\{1,2\\}$. We require $s_1 = \\text{sign}(x_1) = \\text{sign}(3/2) = 1$ and $s_2 = \\text{sign}(x_2) = \\text{sign}(-5/4) = -1$. For the inactive index $3$, we require $|s_3| \\le 1$. The vector $s$ is given by $s=-A^T \\nu$:\n$$ s = \\begin{pmatrix} s_1 \\\\ s_2 \\\\ s_3 \\end{pmatrix} = - \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} \\nu_1 \\\\ \\nu_2 \\end{pmatrix} = \\begin{pmatrix} -\\nu_1 \\\\ -\\nu_2 \\\\ -(\\nu_1+\\nu_2) \\end{pmatrix} $$\nFrom $s_1=1$ and $s_2=-1$, we get $-\\nu_1 = 1 \\implies \\nu_1 = -1$ and $-\\nu_2 = -1 \\implies \\nu_2 = 1$.\nNow we check the condition for the inactive component: $s_3 = -(\\nu_1+\\nu_2) = -(-1+1) = 0$.\nThe condition $|s_3| \\le 1$ is satisfied since $|0| \\le 1$.\nAll KKT conditions are satisfied. This means $x^{(1)} = \\begin{pmatrix} 3/2  -5/4  0 \\end{pmatrix}^T$ is an optimal solution. Its $\\ell_1$-norm is $\\|x^{(1)}\\|_1 = |3/2| + |-5/4| + |0| = 3/2 + 5/4 = 11/4$.\n\nTo demonstrate its uniqueness and for completeness, we examine the other possible supports.\n\n- Support $S=\\{1, 3\\}$:\nWe set $x_2=0$. The system is $A_{\\{1,3\\}}x_{\\{1,3\\}} = y$:\n$$ \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nThe second equation gives $x_3 = -5/4$. Substituting into the first equation: $x_1 + (-5/4) = 3/2 \\implies x_1 = 3/2 + 5/4 = 6/4 + 5/4 = 11/4$.\nThe candidate solution is $x^{(2)} = \\begin{pmatrix} 11/4  0  -5/4 \\end{pmatrix}^T$.\nIts $\\ell_1$-norm is $\\|x^{(2)}\\|_1 = |11/4| + |0| + |-5/4| = 11/4 + 5/4 = 16/4 = 4$.\nSince $4  11/4$, this solution is not the minimum $\\ell_1$-norm solution. Let's verify that it violates the KKT conditions.\nHere $S=\\{1,3\\}$, so we require $s_1 = \\text{sign}(11/4) = 1$ and $s_3 = \\text{sign}(-5/4) = -1$. The inactive index is $2$, for which we need $|s_2| \\le 1$.\nFrom $s = -A^T\\nu$, we have:\n$s_1 = -\\nu_1 = 1 \\implies \\nu_1 = -1$.\n$s_3 = -(\\nu_1+\\nu_2) = -1 \\implies \\nu_1+\\nu_2=1$.\nSubstituting $\\nu_1=-1$ gives $-1+\\nu_2=1 \\implies \\nu_2=2$.\nFor the inactive component, we find $s_2 = -\\nu_2 = -2$.\nThe condition $|s_2| \\le 1$ is violated, as $|-2|=2  1$. Hence, $x^{(2)}$ is not an optimal solution.\n\n- Support $S=\\{2, 3\\}$:\nWe set $x_1=0$. The system is $A_{\\{2,3\\}}x_{\\{2,3\\}} = y$:\n$$ \\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nThe first equation gives $x_3 = 3/2$. Substituting into the second equation: $x_2 + 3/2 = -5/4 \\implies x_2 = -5/4 - 3/2 = -5/4 - 6/4 = -11/4$.\nThe candidate solution is $x^{(3)} = \\begin{pmatrix} 0  -11/4  3/2 \\end{pmatrix}^T$.\nIts $\\ell_1$-norm is $\\|x^{(3)}\\|_1 = |0| + |-11/4| + |3/2| = 11/4 + 6/4 = 17/4$.\nSince $17/4  11/4$, this is not the optimal solution. Let's check the KKT conditions.\nHere $S=\\{2,3\\}$, so $s_2 = \\text{sign}(-11/4)=-1$ and $s_3 = \\text{sign}(3/2)=1$. We need $|s_1| \\le 1$.\n$s_2 = -\\nu_2 = -1 \\implies \\nu_2=1$.\n$s_3 = -(\\nu_1+\\nu_2) = 1 \\implies \\nu_1+\\nu_2=-1$.\nSubstituting $\\nu_2=1$ gives $\\nu_1+1=-1 \\implies \\nu_1=-2$.\nFor the inactive component, $s_1 = -\\nu_1 = -(-2) = 2$.\nThe condition $|s_1| \\le 1$ is violated, as $|2|=2  1$. Thus, $x^{(3)}$ is not an optimal solution.\n\nThe systematic enumeration of supports shows that only one candidate, $x^{(1)}$, satisfies the KKT conditions for optimality. The solution to a strictly convex problem on a convex set is unique. While the $\\ell_1$-norm is not strictly convex, uniqueness of the BP solution holds under certain conditions on the matrix $A$, which are met here. Therefore, the optimal solution is\n$$\nx^{\\star} = \\begin{pmatrix} \\frac{3}{2}  -\\frac{5}{4}  0 \\end{pmatrix}^T\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{2}  -\\frac{5}{4}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While solving small-scale problems by hand provides crucial insight, real-world data assimilation tasks require efficient iterative algorithms. This practice introduces two foundational methods for solving $\\ell_1$-regularized problems: the Iterative Shrinkage-Thresholding Algorithm (ISTA) and its accelerated variant, FISTA. By implementing a single step of each algorithm, you will directly observe the mechanism of proximal gradient descent and quantify the performance gain from Nesterov's acceleration .",
            "id": "3394533",
            "problem": "Consider the linear inverse problem with a measurement operator $A \\in \\mathbb{R}^{m \\times n}$ and data $y \\in \\mathbb{R}^{m}$, where a sparse estimate of $x \\in \\mathbb{R}^{n}$ is sought by solving the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) \\equiv g(x) + \\lambda \\|x\\|_{1},\n$$\nwith $g(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2}$ and a regularization weight $\\lambda  0$. This setting is classical in inverse problems and data assimilation, and the minimization of the $\\ell_{1}$-norm promotes sparsity in the coefficient vector $x$ (basis pursuit in its penalized form).\n\nFor this problem, let $n = m = 2$, $A = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$, $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, and $\\lambda = 2$. Consider the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). Use a constant stepsize $t$ chosen as the reciprocal of the Lipschitz constant $L$ of $\\nabla g$, i.e., $t = \\frac{1}{L}$, and initialize ISTA from $x^{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. For FISTA, use an inertial initialization consistent with Nesterov-style acceleration by taking an auxiliary point $y^{0} = x^{0} + \\beta (x^{0} - x^{-1})$ with $\\beta = \\frac{1}{2}$ and $x^{-1} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$. In both methods, the first iterate is formed by one explicit gradient step on $g$ evaluated at the current point (for ISTA, at $x^{0}$; for FISTA, at $y^{0}$), followed by the proximal map of $\\lambda \\|\\cdot\\|_{1}$ with parameter $t$.\n\nStarting from the definitions of $g$, $\\nabla g$, the Lipschitz constant $L$ of $\\nabla g$, and the proximal map for the $\\ell_{1}$-norm, derive and implement exactly one iteration of ISTA and one inertial FISTA iteration for the given $A$, $y$, $\\lambda$, $x^{0}$, $\\beta$, and $x^{-1}$. Then compute the corresponding objective function values $f(x_{\\mathrm{ISTA}}^{1})$ and $f(x_{\\mathrm{FISTA}}^{1})$ for the first iterates. Finally, provide the single real number equal to the ratio\n$$\n\\frac{f(x_{\\mathrm{ISTA}}^{1})}{f(x_{\\mathrm{FISTA}}^{1})}.\n$$\nExpress the final answer exactly; do not round.",
            "solution": "The objective function to minimize is $f(x) = g(x) + \\lambda \\|x\\|_{1}$, where $g(x) = \\frac{1}{2} \\|Ax - y\\|_{2}^{2}$, $x \\in \\mathbb{R}^{n}$. The given parameters are $n = 2$, $m = 2$, $\\lambda = 2$, and\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n\nFirst, we determine the gradient of the smooth part, $g(x)$.\n$$\ng(x) = \\frac{1}{2} (Ax-y)^T(Ax-y) = \\frac{1}{2} (x^T A^T A x - 2y^T A x + y^T y).\n$$\nThe gradient is given by\n$$\n\\nabla g(x) = A^T(Ax - y).\n$$\nThe Lipschitz constant, $L$, of $\\nabla g(x)$ is the largest eigenvalue of the Hessian matrix $\\nabla^2 g(x) = A^T A$. Since $A$ is a diagonal matrix, it is symmetric, so $A^T = A$.\n$$\nA^T A = A^2 = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}.\n$$\nThe eigenvalues of this diagonal matrix are its diagonal entries, $1$ and $4$. The Lipschitz constant is the maximum eigenvalue, so $L = 4$.\nThe stepsize for both algorithms is $t = \\frac{1}{L} = \\frac{1}{4}$.\n\nThe Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) both use the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator, $S_{\\alpha}(z)$. The general update step for a proximal gradient method is $x^{k+1} = \\mathrm{prox}_{t\\lambda\\|\\cdot\\|_1}(z^{k})$, where $z^k$ is the result of a gradient step. The proximal operator is defined component-wise as\n$$\n[\\mathrm{prox}_{t\\lambda\\|\\cdot\\|_1}(z)]_i = [S_{t\\lambda}(z)]_i = \\mathrm{sign}(z_i) \\max(|z_i| - t\\lambda, 0).\n$$\nIn this problem, the thresholding parameter is $t\\lambda = \\frac{1}{4} \\times 2 = \\frac{1}{2}$.\n\nNow, we perform one iteration for each algorithm.\n\n**Iterative Shrinkage-Thresholding Algorithm (ISTA)**\n\nThe ISTA update rule is $x^{k+1} = S_{t\\lambda}(x^k - t \\nabla g(x^k))$.\nWe start from $x^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The first iteration is $x_{\\mathrm{ISTA}}^{1} = S_{1/2}(x^0 - t \\nabla g(x^0))$.\nFirst, compute the gradient at $x^0$:\n$$\n\\nabla g(x^0) = A^T(Ax^0 - y) = A^T(0 - y) = -A^T y = -\\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = -\\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -2 \\end{pmatrix}.\n$$\nNext, perform the gradient descent step:\n$$\nz_{\\mathrm{ISTA}} = x^0 - t \\nabla g(x^0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} -3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{4} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nFinally, apply the soft-thresholding operator to obtain $x_{\\mathrm{ISTA}}^{1}$:\n$$\nx_{\\mathrm{ISTA}}^{1} = S_{1/2}(z_{\\mathrm{ISTA}}) = \\begin{pmatrix} \\mathrm{sign}(\\frac{3}{4}) \\max(|\\frac{3}{4}| - \\frac{1}{2}, 0) \\\\ \\mathrm{sign}(\\frac{1}{2}) \\max(|\\frac{1}{2}| - \\frac{1}{2}, 0) \\end{pmatrix} = \\begin{pmatrix} \\max(\\frac{3}{4} - \\frac{2}{4}, 0) \\\\ \\max(\\frac{1}{2} - \\frac{1}{2}, 0) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ 0 \\end{pmatrix}.\n$$\n\n**Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)**\n\nFISTA uses an intermediate point $y^k$ for the gradient evaluation. For $k=0$, the point $y^0$ is defined as $y^0 = x^0 + \\beta(x^0 - x^{-1})$. The FISTA update for $x^1$ is then $x_{\\mathrm{FISTA}}^{1} = S_{t\\lambda}(y^0 - t \\nabla g(y^0))$.\nWe are given $x^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x^{-1} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$, and $\\beta = \\frac{1}{2}$.\nFirst, compute the auxiliary point $y^0$:\n$$\ny^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nNext, compute the gradient at $y^0$:\n$$\nA y^0 - y = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.\n$$\n$$\n\\nabla g(y^0) = A^T(A y^0 - y) = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}.\n$$\nPerform the gradient descent step from $y^0$:\n$$\nz_{\\mathrm{FISTA}} = y^0 - t \\nabla g(y^0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{2}{4} \\\\ 0 + \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nFinally, apply the soft-thresholding operator to obtain $x_{\\mathrm{FISTA}}^{1}$:\n$$\nx_{\\mathrm{FISTA}}^{1} = S_{1/2}(z_{\\mathrm{FISTA}}) = \\begin{pmatrix} \\mathrm{sign}(\\frac{3}{2}) \\max(|\\frac{3}{2}| - \\frac{1}{2}, 0) \\\\ \\mathrm{sign}(\\frac{1}{2}) \\max(|\\frac{1}{2}| - \\frac{1}{2}, 0) \\end{pmatrix} = \\begin{pmatrix} \\max(\\frac{3}{2} - \\frac{1}{2}, 0) \\\\ \\max(\\frac{1}{2} - \\frac{1}{2}, 0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n\n**Objective Function Evaluation**\n\nNow, we compute the objective function value $f(x) = \\frac{1}{2} \\|Ax - y\\|_{2}^{2} + 2 \\|x\\|_{1}$ for both iterates.\n\nFor $x_{\\mathrm{ISTA}}^{1} = \\begin{pmatrix} 1/4 \\\\ 0 \\end{pmatrix}$:\nThe residual is $A x_{\\mathrm{ISTA}}^{1} - y = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}\\begin{pmatrix} 1/4 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -11/4 \\\\ -1 \\end{pmatrix}$.\nThe smooth term is $g(x_{\\mathrm{ISTA}}^{1}) = \\frac{1}{2} \\|A x_{\\mathrm{ISTA}}^{1} - y\\|_{2}^{2} = \\frac{1}{2} \\left( \\left(-\\frac{11}{4}\\right)^2 + (-1)^2 \\right) = \\frac{1}{2} \\left( \\frac{121}{16} + 1 \\right) = \\frac{1}{2} \\left( \\frac{137}{16} \\right) = \\frac{137}{32}$.\nThe non-smooth term is $\\lambda \\|x_{\\mathrm{ISTA}}^{1}\\|_{1} = 2 \\left( |\\frac{1}{4}| + |0| \\right) = 2 \\times \\frac{1}{4} = \\frac{1}{2}$.\nThe total objective value is $f(x_{\\mathrm{ISTA}}^{1}) = \\frac{137}{32} + \\frac{1}{2} = \\frac{137}{32} + \\frac{16}{32} = \\frac{153}{32}$.\n\nFor $x_{\\mathrm{FISTA}}^{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$:\nThe residual is $A x_{\\mathrm{FISTA}}^{1} - y = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$.\nThe smooth term is $g(x_{\\mathrm{FISTA}}^{1}) = \\frac{1}{2} \\|A x_{\\mathrm{FISTA}}^{1} - y\\|_{2}^{2} = \\frac{1}{2} \\left( (-2)^2 + (-1)^2 \\right) = \\frac{1}{2} (4 + 1) = \\frac{5}{2}$.\nThe non-smooth term is $\\lambda \\|x_{\\mathrm{FISTA}}^{1}\\|_{1} = 2 (|1| + |0|) = 2$.\nThe total objective value is $f(x_{\\mathrm{FISTA}}^{1}) = \\frac{5}{2} + 2 = \\frac{5}{2} + \\frac{4}{2} = \\frac{9}{2}$.\n\n**Final Ratio Calculation**\n\nFinally, we compute the ratio of the objective function values:\n$$\n\\frac{f(x_{\\mathrm{ISTA}}^{1})}{f(x_{\\mathrm{FISTA}}^{1})} = \\frac{153/32}{9/2} = \\frac{153}{32} \\times \\frac{2}{9} = \\frac{153}{16 \\times 9}.\n$$\nRecognizing that $153 = 17 \\times 9$, we simplify the expression:\n$$\n\\frac{17 \\times 9}{16 \\times 9} = \\frac{17}{16}.\n$$\nAs expected, the accelerated method (FISTA) achieves a lower objective function value in the first iteration.",
            "answer": "$$\\boxed{\\frac{17}{16}}$$"
        },
        {
            "introduction": "A deep understanding of any method includes knowing its limitations. This advanced practice confronts a classic scenario where standard Basis Pursuit fails to recover the true sparse solution due to high correlation between dictionary atoms. You will not only diagnose this failure by constructing a denser, lower-norm solution but also learn to remedy it by designing a weighted $\\ell_1$ minimization problem that correctly prioritizes the desired sparse structure .",
            "id": "3394575",
            "problem": "Consider the linear inverse problem with a measurement operator represented by a matrix $A \\in \\mathbb{R}^{3 \\times 5}$ whose columns are given by\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} \\tfrac{11}{10} \\\\ \\tfrac{1}{20} \\\\ 0 \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} \\tfrac{11}{10} \\\\ 0 \\\\ \\tfrac{1}{20} \\end{pmatrix}, \\quad\na_{5} = \\begin{pmatrix} \\tfrac{11}{10} \\\\ -\\tfrac{1}{20} \\\\ -\\tfrac{1}{20} \\end{pmatrix},\n$$\nwith $\\delta = \\tfrac{1}{100}$. Let the true sparse state be $x_{\\star} \\in \\mathbb{R}^{5}$ with two equal-amplitude coefficients aligned with the highly correlated columns $a_{1}$ and $a_{2}$:\n$$\nx_{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix},\n$$\nso the observation is $y = A x_{\\star} = a_{1} + a_{2} = \\begin{pmatrix} 2 \\\\ \\delta \\\\ 0 \\end{pmatrix}$.\n\nBasis Pursuit (BP) solves the convex program\n$$\n\\min_{x \\in \\mathbb{R}^{5}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y,\n$$\nwhere $\\|x\\|_{1} = \\sum_{i=1}^{5} |x_{i}|$. Weighted Basis Pursuit solves\n$$\n\\min_{x \\in \\mathbb{R}^{5}} \\sum_{i=1}^{5} w_{i} |x_{i}| \\quad \\text{subject to} \\quad A x = y,\n$$\nfor given positive weights $w = (w_{1},\\ldots,w_{5})$.\n\nTasks:\n1. Starting from the definitions of the coherence between two columns, verify that $a_{1}$ and $a_{2}$ are highly correlated by evaluating the coherence $\\mu_{12} = \\dfrac{|\\langle a_{1}, a_{2}\\rangle|}{\\|a_{1}\\|_{2}\\, \\|a_{2}\\|_{2}}$ and showing it is close to $1$ for the given $\\delta$.\n2. By reasoning from first principles of convex geometry for $\\ell_{1}$ minimization and the feasibility constraints $A x = y$, construct a feasible $x$ supported on $\\{3,4,5\\}$ that achieves a strictly smaller $\\ell_{1}$ value than $\\|x_{\\star}\\|_{1}$. Explain why BP can return this denser solution.\n3. Use the linear programming dual of Basis Pursuit to certify optimality of the constructed solution in Task 2 by producing a dual variable $p \\in \\mathbb{R}^{3}$ such that the corresponding dual constraints are satisfied, and the dual objective equals the primal $\\ell_{1}$ norm.\n4. Consider weighted BP with weights $w_{1} = w_{2} = 1$ and $w_{3} = w_{4} = w_{5} = \\rho$, where $\\rho  0$ is a scalar weight penalizing the highly correlated block $\\{3,4,5\\}$. Determine the smallest value of $\\rho$ that guarantees $x_{\\star}$ is the unique minimizer of the weighted BP problem. Express your final answer as a single exact number. No rounding is required, and no units are involved.\n\nThe final answer must be a single real number.",
            "solution": "The problem asks for four distinct tasks related to Basis Pursuit (BP) and its weighted variant for a specific sparse recovery problem. We will address each task sequentially.\n\nThe givens are the matrix $A \\in \\mathbb{R}^{3 \\times 5}$ with columns:\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} \\tfrac{11}{10} \\\\ \\tfrac{1}{20} \\\\ 0 \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} \\tfrac{11}{10} \\\\ 0 \\\\ \\tfrac{1}{20} \\end{pmatrix}, \\quad\na_{5} = \\begin{pmatrix} \\tfrac{11}{10} \\\\ -\\tfrac{1}{20} \\\\ -\\tfrac{1}{20} \\end{pmatrix}\n$$\nwith $\\delta = \\frac{1}{100}$. The true sparse state is $x_{\\star} = (1, 1, 0, 0, 0)^T$ and the observation is $y = A x_{\\star} = a_{1} + a_{2} = (2, \\delta, 0)^T$.\n\n**Task 1: Coherence Calculation**\n\nThe coherence $\\mu_{12}$ between columns $a_{1}$ and $a_{2}$ is defined as $\\mu_{12} = \\frac{|\\langle a_{1}, a_{2}\\rangle|}{\\|a_{1}\\|_{2}\\, \\|a_{2}\\|_{2}}$. We compute each term:\nThe inner product is $\\langle a_{1}, a_{2}\\rangle = (1)(1) + (0)(\\delta) + (0)(0) = 1$.\nThe $\\ell_2$-norm of $a_{1}$ is $\\|a_{1}\\|_{2} = \\sqrt{1^2 + 0^2 + 0^2} = 1$.\nThe $\\ell_2$-norm of $a_{2}$ is $\\|a_{2}\\|_{2} = \\sqrt{1^2 + \\delta^2 + 0^2} = \\sqrt{1+\\delta^2}$.\n\nSubstituting these into the coherence formula:\n$$\n\\mu_{12} = \\frac{|1|}{1 \\cdot \\sqrt{1+\\delta^2}} = \\frac{1}{\\sqrt{1+\\delta^2}}\n$$\nFor the given value $\\delta = \\frac{1}{100} = 10^{-2}$, we have $\\delta^2 = 10^{-4}$.\nThe coherence is $\\mu_{12} = \\frac{1}{\\sqrt{1+10^{-4}}}$.\nUsing the Taylor approximation $(1+x)^{-1/2} \\approx 1 - \\frac{1}{2}x$ for small $x$, with $x = \\delta^2 = 10^{-4}$, we get:\n$$\n\\mu_{12} \\approx 1 - \\frac{1}{2}(10^{-4}) = 1 - 0.00005 = 0.99995\n$$\nThis value is very close to $1$, which confirms that the columns $a_{1}$ and $a_{2}$ are highly correlated.\n\n**Task 2: Construction of a Denser Solution**\n\nThe true solution is $x_{\\star} = (1, 1, 0, 0, 0)^T$, which is $2$-sparse and has an $\\ell_{1}$ norm of $\\|x_{\\star}\\|_{1} = |1| + |1| = 2$. We seek an alternative feasible solution $x_{\\text{alt}}$ supported on the set $J = \\{3, 4, 5\\}$ such that $A x_{\\text{alt}} = y$ and $\\|x_{\\text{alt}}\\|_{1}  \\|x_{\\star}\\|_{1}$.\nLet $x_{\\text{alt}} = (0, 0, c_{3}, c_{4}, c_{5})^T$. The constraint $A x_{\\text{alt}} = y$ becomes $c_{3}a_{3} + c_{4}a_{4} + c_{5}a_{5} = y$. This is a system of linear equations:\n$$\n\\begin{pmatrix} \\tfrac{11}{10}  \\tfrac{11}{10}  \\tfrac{11}{10} \\\\ \\tfrac{1}{20}  0  -\\tfrac{1}{20} \\\\ 0  \\tfrac{1}{20}  -\\tfrac{1}{20} \\end{pmatrix}\n\\begin{pmatrix} c_{3} \\\\ c_{4} \\\\ c_{5} \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ \\delta \\\\ 0 \\end{pmatrix}\n$$\nFrom the third equation: $\\frac{1}{20}(c_{4} - c_{5}) = 0 \\implies c_{4} = c_{5}$.\nFrom the first equation: $\\frac{11}{10}(c_{3} + c_{4} + c_{5}) = 2 \\implies c_{3} + 2c_{4} = \\frac{20}{11}$.\nFrom the second equation: $\\frac{1}{20}(c_{3} - c_{5}) = \\delta \\implies c_{3} - c_{4} = 20\\delta$.\nWe now solve the system for $c_3$ and $c_4$:\n$$\n\\begin{cases}\nc_{3} + 2c_{4} = \\frac{20}{11} \\\\\nc_{3} - c_{4} = 20\\delta\n\\end{cases}\n$$\nSubtracting the second equation from the first yields $3c_{4} = \\frac{20}{11} - 20\\delta$, so $c_{4} = \\frac{20}{3}(\\frac{1}{11} - \\delta)$.\nSubstituting $\\delta = \\frac{1}{100}$:\n$c_4 = \\frac{20}{3}(\\frac{1}{11} - \\frac{1}{100}) = \\frac{20}{3}(\\frac{100-11}{1100}) = \\frac{20}{3}\\frac{89}{1100} = \\frac{89}{165}$.\nSince $c_5=c_4$, $c_5 = \\frac{89}{165}$.\nFrom $c_{3} - c_{4} = 20\\delta$, we find $c_3 = c_4 + 20\\delta = \\frac{89}{165} + 20(\\frac{1}{100}) = \\frac{89}{165} + \\frac{1}{5} = \\frac{89+33}{165} = \\frac{122}{165}$.\nThe coefficients are all positive. The alternative solution is $x_{\\text{alt}} = (0, 0, \\frac{122}{165}, \\frac{89}{165}, \\frac{89}{165})^T$.\nIts $\\ell_1$ norm is $\\|x_{\\text{alt}}\\|_1 = c_3+c_4+c_5$ (since they are positive), which from the first equation is $\\frac{20}{11}$.\nWe compare this to the $\\ell_1$ norm of the true solution: $\\|x_{\\text{alt}}\\|_{1} = \\frac{20}{11} \\approx 1.818$ while $\\|x_{\\star}\\|_{1} = 2$.\nSince $\\frac{20}{11}  2$, we have found a denser ($3$-sparse) solution with a strictly smaller $\\ell_1$ norm. Standard Basis Pursuit minimizes the $\\ell_1$ norm, so it will return this denser solution $x_{\\text{alt}}$ instead of the true sparse solution $x_{\\star}$. This failure occurs because the high coherence of the dictionary allows a more \"$\\ell_1$-efficient\" representation of the signal $y$ using other atoms.\n\n**Task 3: Dual Certificate of Optimality**\n\nThe optimality of a solution $x^*$ to the BP problem is certified by the existence of a dual vector $p \\in \\mathbb{R}^3$ satisfying the KKT conditions:\n1. $\\langle a_i, p \\rangle = \\text{sgn}(x_i^*)$ for $i \\in \\text{supp}(x^*)$.\n2. $|\\langle a_i, p \\rangle| \\le 1$ for $i \\notin \\text{supp}(x^*)$.\nHere, our candidate for the optimal solution is $x_{\\text{alt}}$, with support $J=\\{3,4,5\\}$. Since all its coefficients are positive, the first condition becomes $\\langle a_i, p \\rangle = 1$ for $i \\in \\{3,4,5\\}$. This gives the linear system $A_J^T p = \\mathbf{1}$:\n$$\n\\begin{pmatrix} \\tfrac{11}{10}  \\tfrac{1}{20}  0 \\\\ \\tfrac{11}{10}  0  \\tfrac{1}{20} \\\\ \\tfrac{11}{10}  -\\tfrac{1}{20}  -\\tfrac{1}{20} \\end{pmatrix}\n\\begin{pmatrix} p_1 \\\\ p_2 \\\\ p_3 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nEquating the first two rows gives $\\frac{11}{10}p_1 + \\frac{1}{20}p_2 = \\frac{11}{10}p_1 + \\frac{1}{20}p_3$, which implies $p_2=p_3$.\nSubstituting $p_3=p_2$ into the third equation yields $\\frac{11}{10}p_1 - \\frac{2}{20}p_2 = 1$, or $11p_1 - p_2 = 10$.\nFrom the first equation, $22p_1 + p_2 = 20$.\nAdding these two equations gives $33p_1 = 30$, so $p_1 = \\frac{30}{33} = \\frac{10}{11}$.\nThen $p_2 = 11p_1 - 10 = 11(\\frac{10}{11}) - 10 = 10-10=0$.\nThus, $p_2=p_3=0$, and the dual vector is $p = (\\frac{10}{11}, 0, 0)^T$.\nNow we must check the second condition for the inactive set $i \\in \\{1,2\\}$:\nFor $i=1$: $|\\langle a_1, p \\rangle| = |(1)(\\frac{10}{11}) + (0)(0) + (0)(0)| = \\frac{10}{11} \\leq 1$.\nFor $i=2$: $|\\langle a_2, p \\rangle| = |(1)(\\frac{10}{11}) + (\\delta)(0) + (0)(0)| = \\frac{10}{11} \\leq 1$.\nBoth conditions are satisfied. Thus, $p=(\\frac{10}{11}, 0, 0)^T$ is a valid dual certificate proving the optimality of $x_{\\text{alt}}$ for the standard Basis Pursuit problem. As a final check, strong duality must hold: $\\langle y, p \\rangle = \\|x_{\\text{alt}}\\|_1$.\n$\\langle y, p \\rangle = (2)(\\frac{10}{11}) + (\\delta)(0) + (0)(0) = \\frac{20}{11}$. This matches $\\|x_{\\text{alt}}\\|_1$.\n\n**Task 4: Weighted Basis Pursuit**\n\nWe consider the weighted BP problem with weights $w_1=w_2=1$ and $w_3=w_4=w_5=\\rho$. We seek the smallest $\\rho  0$ that guarantees $x_{\\star}=(1,1,0,0,0)^T$ is the unique minimizer.\nThe weighted $\\ell_1$ norm of the true solution is $\\|x_{\\star}\\|_{1,w} = w_1|1| + w_2|1| = 1 \\cdot 1 + 1 \\cdot 1 = 2$.\nThe weighted $\\ell_1$ norm of the alternative solution is $\\|x_{\\text{alt}}\\|_{1,w} = w_3|c_3| + w_4|c_4| + w_5|c_5| = \\rho(c_3+c_4+c_5) = \\rho \\frac{20}{11}$.\n\nFor $x_{\\star}$ to be the unique minimizer, its weighted cost must be strictly smaller than that of any other feasible solution, including $x_{\\text{alt}}$.\n$$\n\\|x_{\\star}\\|_{1,w}  \\|x_{\\text{alt}}\\|_{1,w} \\implies 2  \\rho \\frac{20}{11} \\implies \\rho  2 \\cdot \\frac{11}{20} \\implies \\rho  \\frac{11}{10}\n$$\nThis gives a necessary condition. To show it is sufficient, we use the dual certificate conditions for uniqueness in weighted BP. For $x_{\\star}$ to be the unique minimizer, there must exist a dual vector $p$ such that:\n1. $\\langle a_i, p \\rangle = w_i \\cdot \\text{sgn}((x_{\\star})_i)$ for $i \\in \\text{supp}(x_{\\star}) = \\{1,2\\}$.\n2. $|\\langle a_i, p \\rangle|  w_i$ for $i \\notin \\{1,2\\}$.\n\nThe first condition gives:\n$\\langle a_1, p \\rangle = p_1 = w_1 = 1$.\n$\\langle a_2, p \\rangle = p_1 + \\delta p_2 = w_2 = 1$.\nFrom these, we get $p_1=1$ and $1+\\delta p_2=1$, which implies $p_2=0$ (since $\\delta \\neq 0$). The component $p_3$ is unconstrained. So the dual vector has the form $p_k = (1, 0, k)^T$ for any $k \\in \\mathbb{R}$.\n\nThe second condition (for uniqueness) is a set of strict inequalities for $i \\in \\{3,4,5\\}$ and some choice of $k$:\n$|\\langle a_3, p_k \\rangle| = |\\frac{11}{10}(1) + \\frac{1}{20}(0) + 0(k)| = \\frac{11}{10}  w_3 = \\rho$.\n$|\\langle a_4, p_k \\rangle| = |\\frac{11}{10}(1) + 0(0) + \\frac{1}{20}(k)| = |\\frac{11}{10} + \\frac{k}{20}|  w_4 = \\rho$.\n$|\\langle a_5, p_k \\rangle| = |\\frac{11}{10}(1) - \\frac{1}{20}(0) - \\frac{1}{20}(k)| = |\\frac{11}{10} - \\frac{k}{20}|  w_5 = \\rho$.\n\nThis system of inequalities must be satisfiable for some $k$. This is possible if and only if $\\rho$ is strictly greater than the minimum possible value of the maximum of these three quantities:\n$$\n\\rho  \\min_{k \\in \\mathbb{R}} \\max \\left\\{ \\frac{11}{10}, \\left|\\frac{11}{10} + \\frac{k}{20}\\right|, \\left|\\frac{11}{10} - \\frac{k}{20}\\right| \\right\\}\n$$\nThe term $\\max \\{|C+x|, |C-x|\\}$ for a constant $C$ is minimized when $x=0$. Here, we let $C=\\frac{11}{10}$ and $x=\\frac{k}{20}$. The minimum of $\\max \\{|\\frac{11}{10} + \\frac{k}{20}|, |\\frac{11}{10} - \\frac{k}{20}|\\}$ is achieved at $k=0$ and its value is $\\frac{11}{10}$.\nTherefore, the minimum value of the overall maximum is also $\\frac{11}{10}$, achieved at $k=0$.\nSo the condition for the existence of a valid dual certificate for uniqueness is $\\rho  \\frac{11}{10}$.\nThe set of values for $\\rho$ that guarantee uniqueness is $(\\frac{11}{10}, \\infty)$. The question asks for the smallest value of $\\rho$ that guarantees this, which is the infimum of this set. At $\\rho=\\frac{11}{10}$, uniqueness is not guaranteed (as shown by $\\|x_{\\star}\\|_{1,w} = \\|x_{\\text{alt}}\\|_{1,w}$). Therefore, the boundary value is the answer.",
            "answer": "$$\\boxed{\\frac{11}{10}}$$"
        }
    ]
}