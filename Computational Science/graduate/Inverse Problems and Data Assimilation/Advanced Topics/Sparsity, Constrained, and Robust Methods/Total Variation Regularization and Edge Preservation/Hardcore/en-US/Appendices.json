{
    "hands_on_practices": [
        {
            "introduction": "Total Variation (TV) is defined in the continuous domain as the integral of the gradient's magnitude, but to apply it to digital images or discrete data, we must create a discrete counterpart. This exercise guides you through the construction of the two most common discrete TV functionals by approximating the gradient with finite differences and the integral with a sum . By comparing the isotropic and anisotropic versions, you will gain a crucial understanding of how the choice of discretization impacts geometric properties like rotational invariance, revealing important artifacts such as grid bias.",
            "id": "3427997",
            "problem": "Consider a $2$-dimensional discrete field $u$ defined on a rectangular grid with $N_x$ nodes along the horizontal direction and $N_y$ nodes along the vertical direction, with uniform spacings $h_x$ and $h_y$ respectively. The indices $(i,j)$ range over $\\{0,1,\\dots,N_x-1\\} \\times \\{0,1,\\dots,N_y-1\\}$. You are asked to construct discrete Total Variation (TV) functionals using forward differences with periodic boundary conditions, and to analyze their rotational invariance and grid bias. The goal is to reason from first principles: the continuous Total Variation (TV) for sufficiently smooth functions is the integral of a norm of the gradient, and finite difference approximations plus Riemann sums can be used to construct discrete approximations. Forward differences with periodic wrap-around enforce that the discrete gradient is computed by differences to forward neighbors with indices taken modulo the grid size. Total Variation (TV) refers to the integral of a norm of the gradient, with the choice of norm determining isotropy or anisotropy: the Euclidean norm yields an isotropic functional, while the sum of absolute values of components yields an anisotropic functional. Periodic Boundary Conditions (PBC) identify the boundary edges so that $(i+1)\\bmod N_x$ and $(j+1)\\bmod N_y$ are used for forward neighbors at the domain boundary.\n\nChoose all options that correctly define the discrete isotropic and anisotropic TV using forward differences with periodic boundary conditions, and that correctly characterize their rotational invariance and grid bias properties.\n\nA. On the described grid, the anisotropic discrete TV with forward differences and periodic boundary conditions is given by\n$$\n\\mathrm{TV}_{\\mathrm{aniso}}(u) \\;=\\; \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} h_x h_y \\left( \\left| \\frac{u_{(i+1)\\bmod N_x,\\, j} - u_{i,j}}{h_x} \\right| \\;+\\; \\left| \\frac{u_{i,\\, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right| \\right).\n$$\n\nB. On the described grid, the isotropic discrete TV with forward differences and periodic boundary conditions is given by\n$$\n\\mathrm{TV}_{\\mathrm{iso}}(u) \\;=\\; \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} h_x h_y \\,\\sqrt{ \\left( \\frac{u_{(i+1)\\bmod N_x,\\, j} - u_{i,j}}{h_x} \\right)^2 \\;+\\; \\left( \\frac{u_{i,\\, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right)^2 }.\n$$\n\nC. The isotropic discrete TV constructed from forward differences is exactly rotation invariant for any finite $N_x$ and $N_y$, meaning that rotating the underlying function by any angle leaves the discrete TV unchanged.\n\nD. The anisotropic discrete TV constructed from forward differences is not rotation invariant and exhibits stronger grid bias toward axis-aligned edges compared to the isotropic discretization.\n\nE. Imposing periodic boundary conditions eliminates grid bias; under periodic boundary conditions both the anisotropic and isotropic discrete TV become rotation invariant.\n\nF. As $h_x,h_y \\to 0$ with $N_x h_x$ and $N_y h_y$ fixed (refining the grid), the isotropic discrete TV converges to the rotation-invariant continuous TV functional (based on the Euclidean norm of the gradient), whereas the anisotropic discrete TV converges to a continuum anisotropic functional (based on the sum of absolute values of gradient components) that is not rotation invariant.",
            "solution": "The user wants me to validate the problem statement, and if it is valid, to derive the correct answer and evaluate all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A $2$-dimensional discrete field $u$ is defined on a rectangular grid.\n- Grid nodes: $N_x$ along the horizontal direction, $N_y$ along the vertical direction.\n- Grid spacings: uniform, $h_x$ and $h_y$.\n- Indices: $(i,j)$ range over $\\{0,1,\\dots,N_x-1\\} \\times \\{0,1,\\dots,N_y-1\\}$.\n- Method for constructing discrete Total Variation (TV) functionals: Forward differences with periodic boundary conditions.\n- Continuous TV definition: Integral of a norm of the gradient.\n- Discrete TV construction: Finite difference approximations plus Riemann sums.\n- Forward differences with periodic wrap-around: Indices for forward neighbors are taken modulo the grid size, e.g., $(i+1)\\bmod N_x$ and $(j+1)\\bmod N_y$.\n- Isotropic TV: Based on the Euclidean norm of the gradient.\n- Anisotropic TV: Based on the sum of absolute values of the gradient components.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded**: The problem is firmly rooted in standard concepts of numerical analysis, image processing, and inverse problems. Total Variation regularization, finite differences, periodic boundary conditions, and the distinction between isotropic and anisotropic forms are well-established mathematical and computational tools.\n2.  **Well-Posed**: The problem is clearly stated. It asks for the correct definitions of discrete TV functionals based on given principles and an analysis of their properties. There is sufficient information to perform this task.\n3.  **Objective**: The terminology is precise and standard within the field. The definitions and descriptions are objective and free of ambiguity or subjective claims.\n4.  **Incomplete or Contradictory Setup**: The problem statement is self-contained and consistent. It provides all necessary components: the continuous principle, the discretization method (forward differences), the boundary conditions (periodic), and the grid structure.\n5.  **Unrealistic or Infeasible**: The setup describes a standard computational scenario. It is entirely realistic and feasible.\n6.  **Ill-Posed or Poorly Structured**: The question is well-structured and leads to a unique set of correct options. Terms like \"isotropic,\" \"anisotropic,\" \"grid bias,\" and \"rotational invariance\" have clear, established meanings in this context.\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires substantive understanding of the link between continuous and discrete calculus, the geometric implications of discretization choices, and convergence properties. It is not trivial.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will now proceed with the solution.\n\n### Derivation and Analysis\n\nThe continuous Total Variation (TV) of a sufficiently smooth function $u(x, y)$ on a domain $\\Omega$ is given by\n$$\n\\mathrm{TV}(u) = \\int_{\\Omega} \\| \\nabla u(x,y) \\| \\, dx \\, dy\n$$\nwhere $\\nabla u = (\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y})$ is the gradient of $u$. The choice of norm $\\|\\cdot\\|$ determines the properties of the functional.\n\n-   **Continuous Isotropic TV**: The norm is the Euclidean ($L_2$) norm:\n    $$\n    \\mathrm{TV}_{\\mathrm{iso}}(u) = \\int_{\\Omega} \\sqrt{ \\left(\\frac{\\partial u}{\\partial x}\\right)^2 + \\left(\\frac{\\partial u}{\\partial y}\\right)^2 } \\, dx \\, dy\n    $$\n    This functional is rotation invariant because the Euclidean norm of a vector is unchanged by rotation.\n\n-   **Continuous Anisotropic TV**: The norm is the $L_1$ norm:\n    $$\n    \\mathrm{TV}_{\\mathrm{aniso}}(u) = \\int_{\\Omega} \\left( \\left|\\frac{\\partial u}{\\partial x}\\right| + \\left|\\frac{\\partial u}{\\partial y}\\right| \\right) \\, dx \\, dy\n    $$\n    This functional is not rotation invariant. For example, a gradient vector $(c, 0)$ gives a norm of $|c|$, while its $45^\\circ$ rotation, $(c/\\sqrt{2}, c/\\sqrt{2})$, gives a norm of $2|c|/\\sqrt{2} = \\sqrt{2}|c|$.\n\nTo discretize these functionals, we replace the integral with a Riemann sum over the grid cells, and the partial derivatives with finite differences. The area element $dx \\, dy$ is approximated by the area of a grid cell, $h_x h_y$. The problem specifies forward differences with periodic boundary conditions.\n\nThe discrete partial derivatives at grid point $(i,j)$ are:\n$$\n(\\nabla u)_{i,j} \\approx \\left( \\frac{u_{(i+1)\\bmod N_x, j} - u_{i,j}}{h_x}, \\frac{u_{i, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right)\n$$\nThe periodic boundary conditions are handled by the modulo operator on the indices.\n\nSubstituting these into the Riemann sum structure gives the discrete TV functionals.\n\n-   **Discrete Anisotropic TV**:\n    $$\n    \\mathrm{TV}_{\\mathrm{aniso}}(u) = \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} h_x h_y \\left( \\left| \\frac{u_{(i+1)\\bmod N_x, j} - u_{i,j}}{h_x} \\right| + \\left| \\frac{u_{i, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right| \\right)\n    $$\n\n-   **Discrete Isotropic TV**:\n    $$\n    \\mathrm{TV}_{\\mathrm{iso}}(u) = \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} h_x h_y \\sqrt{ \\left( \\frac{u_{(i+1)\\bmod N_x, j} - u_{i,j}}{h_x} \\right)^2 + \\left( \\frac{u_{i, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right)^2 }\n    $$\n\nNow we evaluate the given options.\n\n**A. On the described grid, the anisotropic discrete TV with forward differences and periodic boundary conditions is given by...**\nThe formula presented is:\n$$\n\\mathrm{TV}_{\\mathrm{aniso}}(u) \\;=\\; \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} h_x h_y \\left( \\left| \\frac{u_{(i+1)\\bmod N_x,\\, j} - u_{i,j}}{h_x} \\right| \\;+\\; \\left| \\frac{u_{i,\\, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right| \\right).\n$$\nThis expression perfectly matches the derivation above for the discrete anisotropic TV using a Riemann sum with the $L_1$ norm of the forward-difference gradient.\n**Verdict: Correct.**\n\n**B. On the described grid, the isotropic discrete TV with forward differences and periodic boundary conditions is given by...**\nThe formula presented is:\n$$\n\\mathrm{TV}_{\\mathrm{iso}}(u) \\;=\\; \\sum_{i=0}^{N_x-1} \\sum_{j=0}^{N_y-1} h_x h_y \\,\\sqrt{ \\left( \\frac{u_{(i+1)\\bmod N_x,\\, j} - u_{i,j}}{h_x} \\right)^2 \\;+\\; \\left( \\frac{u_{i,\\, (j+1)\\bmod N_y} - u_{i,j}}{h_y} \\right)^2 }.\n$$\nThis expression perfectly matches the derivation above for the discrete isotropic TV using a Riemann sum with the $L_2$ norm of the forward-difference gradient.\n**Verdict: Correct.**\n\n**C. The isotropic discrete TV constructed from forward differences is exactly rotation invariant for any finite $N_x$ and $N_y$, meaning that rotating the underlying function by any angle leaves the discrete TV unchanged.**\nThis statement is false. The discretization process itself, by defining derivatives along fixed grid axes ($x$ and $y$), breaks the perfect rotational invariance of the continuous functional. Consider an underlying continuous image of a sharp, straight edge. If the edge is axis-aligned (e.g., vertical), its discrete representation will have non-zero gradients only in the $x$-direction along a single column of pixels. If the same edge is rotated by, for instance, $45^\\circ$, its discrete representation becomes a \"staircase,\" with non-zero gradients in both $x$ and $y$ directions distributed along the staircase. The sum of the norms of these discrete gradients will not, in general, be the same as in the axis-aligned case. This dependency on orientation is called grid bias or anisotropy. Thus, the discrete functional is not exactly rotation invariant for a finite grid.\n**Verdict: Incorrect.**\n\n**D. The anisotropic discrete TV constructed from forward differences is not rotation invariant and exhibits stronger grid bias toward axis-aligned edges compared to the isotropic discretization.**\nThis statement has two parts.\n1.  *Not rotation invariant*: This is true. The continuous anisotropic functional is already not rotation invariant, and the discretization on a grid does not restore this symmetry.\n2.  *Exhibits stronger grid bias...*: This is also true. The \"cost\" assigned to a gradient depends on its orientation. For the anisotropic ($L_1$) norm, the cost $|\\delta_x u| + |\\delta_y u|$ is minimized for a given magnitude of change when the change is purely horizontal or vertical. It is maximized for diagonal changes. This means the anisotropic functional heavily penalizes diagonal edges relative to axis-aligned ones. The isotropic ($L_2$) norm, $\\sqrt{(\\delta_x u)^2 + (\\delta_y u)^2}$, though not perfectly isotropic in its discrete form, treats different orientations more evenly. Its response to orientation is \"rounder\" than the diamond shape of the $L_1$ norm. Therefore, the anisotropic discretization shows a stronger preference for axis-aligned structures, i.e., stronger grid bias.\n**Verdict: Correct.**\n\n**E. Imposing periodic boundary conditions eliminates grid bias; under periodic boundary conditions both the anisotropic and isotropic discrete TV become rotation invariant.**\nThis statement is false. Periodic boundary conditions address the topology of the domain by effectively \"wrapping\" the grid into a torus. This determines how differences are calculated for pixels at the boundary of the index range $\\{0, \\dots, N-1\\}$. However, the source of grid bias is the finite difference operator itself, which is defined with respect to the fixed Cartesian grid directions. This local geometric property is unaffected by the global boundary conditions. Therefore, PBCs do not eliminate grid bias or restore rotational invariance.\n**Verdict: Incorrect.**\n\n**F. As $h_x,h_y \\to 0$ with $N_x h_x$ and $N_y h_y$ fixed (refining the grid), the isotropic discrete TV converges to the rotation-invariant continuous TV functional (based on the Euclidean norm of the gradient), whereas the anisotropic discrete TV converges to a continuum anisotropic functional (based on the sum of absolute values of gradient components) that is not rotation invariant.**\nThis statement accurately describes the convergence properties of the discrete functionals. The discrete TV formulations in A and B are consistent, first-order Riemann sum approximations of their respective continuous integrals. For a sufficiently regular function $u$, as the grid spacing tends to zero ($h_x, h_y \\to 0$), these sums converge to the corresponding integrals.\n- The limit of the discrete isotropic TV is the continuous isotropic TV, which is a rotation-invariant functional.\n- The limit of the discrete anisotropic TV is the continuous anisotropic TV, which is not a rotation-invariant functional.\nThe statement correctly identifies the properties of these limiting continuous functionals.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ABDF}$$"
        },
        {
            "introduction": "While the primal Rudin-Osher-Fatemi (ROF) problem formulation is intuitive, solving it directly can be challenging due to the non-differentiability of the TV term. A powerful strategy in convex optimization is to solve the dual problem instead. This practice will guide you from first principles to derive the dual formulation of the ROF model, transforming the original problem into a constrained quadratic minimization that is often more amenable to simple algorithms . You will also construct the primal-dual gap, an essential expression that provides a computable certificate of optimality for any candidate solution.",
            "id": "3428023",
            "problem": "Consider the Rudin–Osher–Fatemi (ROF) model for denoising on a rectangular grid with $n$ pixels. Let $f \\in \\mathbb{R}^{n}$ denote the observed data and $u \\in \\mathbb{R}^{n}$ the unknown image to be estimated. Let $\\nabla : \\mathbb{R}^{n} \\to \\mathbb{R}^{n \\times d}$ denote the discrete forward-difference gradient operator in $d \\in \\{1,2\\}$ spatial dimensions with homogeneous Neumann boundary conditions, and let $\\mathrm{div} : \\mathbb{R}^{n \\times d} \\to \\mathbb{R}^{n}$ be its negative adjoint, so that $\\langle \\nabla u, p \\rangle = - \\langle u, \\mathrm{div}\\, p \\rangle$ for all $u \\in \\mathbb{R}^{n}$ and $p \\in \\mathbb{R}^{n \\times d}$. Define the isotropic Total Variation (TV) semi-norm by\n$$\n\\mathrm{TV}(u) = \\sum_{i=1}^{n} \\| (\\nabla u)_{i} \\|_{2},\n$$\nwhere $(\\nabla u)_{i} \\in \\mathbb{R}^{d}$ is the local discrete gradient at pixel $i$ and $\\| \\cdot \\|_{2}$ denotes the Euclidean norm in $\\mathbb{R}^{d}$. The ROF problem is\n$$\n\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u) \\right\\},\n$$\nwith regularization parameter $\\lambda > 0$. The dual feasibility set is\n$$\n\\mathcal{C} = \\left\\{ p \\in \\mathbb{R}^{n \\times d} \\,:\\, \\| p \\|_{\\infty} \\leq 1 \\right\\}, \\quad \\text{where} \\quad \\| p \\|_{\\infty} := \\max_{i} \\| p_{i} \\|_{2}.\n$$\nStarting only from these definitions, fundamental properties of convex functions and their support functions, and adjoint relationships of linear operators, do the following:\n\n1. Derive the dual formulation of the ROF problem in the form\n$$\n\\min_{p \\in \\mathcal{C}} \\frac{1}{2} \\left\\| f + \\lambda \\, \\mathrm{div}\\, p \\right\\|_{2}^{2},\n$$\nensuring that constants independent of $p$ are treated correctly and the equivalence to the usual concave dual maximization is made explicit.\n\n2. Construct a primal–dual gap function $\\mathcal{G}(u,p)$ that is finite if and only if $p \\in \\mathcal{C}$, is nonnegative for all $u \\in \\mathbb{R}^{n}$ and all $p \\in \\mathcal{C}$, and equals zero if and only if $(u,p)$ satisfy the optimality conditions of the ROF model. Your derivation must start from the ROF primal objective, the dual objective obtained in part 1, and fundamental inequalities for convex functions (such as the Fenchel–Young inequality) and adjoint identities, without invoking any prepackaged primal–dual gap formula.\n\nYour final answer must be the single, explicit analytic expression for the primal–dual gap $\\mathcal{G}(u,p)$ in terms of $u$, $p$, $f$, $\\lambda$, $\\nabla$, and $\\mathrm{div}$. No numerical evaluation is required, and no rounding should be performed. Clearly indicate the indicator function of the dual feasibility set if it appears in your final expression, and use the inner-product notation $\\langle \\cdot, \\cdot \\rangle$ consistently.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the principles of convex optimization and mathematical image processing, specifically the Rudin–Osher–Fatemi (ROF) model. The problem is well-posed, objective, and self-contained, providing all necessary definitions to perform the requested derivations. It requires a rigorous application of Fenchel-Rockafellar duality or an equivalent Lagrangian approach, which is a non-trivial but standard task in the field.\n\n### Part 1: Derivation of the Dual Formulation\n\nThe primal ROF problem is given by\n$$\n\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u) \\right\\}\n$$\nwhere $\\lambda > 0$. The Total Variation (TV) semi-norm is defined as $\\mathrm{TV}(u) = \\sum_{i=1}^{n} \\| (\\nabla u)_{i} \\|_{2}$. This can be expressed using the dual norm definition. Let the space of gradient fields be $V = \\mathbb{R}^{n \\times d}$. The TV term is a mixed-norm on this space, often written as $\\lambda \\| \\nabla u \\|_{1,2}$. The dual norm to the $\\| \\cdot \\|_{1,2}$ norm is the $\\| \\cdot \\|_{\\infty,2}$ norm, which is exactly the norm $\\| p \\|_{\\infty} := \\max_{i} \\| p_{i} \\|_{2}$ given in the problem statement. The dual formulation of the norm states that\n$$\n\\| v \\|_{1,2} = \\sup_{\\|p\\|_{\\infty,2} \\leq 1} \\langle v, p \\rangle\n$$\nIn our case, $v = \\nabla u$, and the set $\\{ p \\in \\mathbb{R}^{n \\times d} \\,:\\, \\| p \\|_{\\infty} \\leq 1 \\}$ is precisely the given feasibility set $\\mathcal{C}$. Therefore, we can write\n$$\n\\mathrm{TV}(u) = \\sup_{p \\in \\mathcal{C}} \\langle \\nabla u, p \\rangle\n$$\nSubstituting this into the primal problem, we obtain a minimax formulation:\n$$\n\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\sup_{p \\in \\mathcal{C}} \\langle \\nabla u, p \\rangle \\right\\} = \\min_{u \\in \\mathbb{R}^{n}} \\sup_{p \\in \\mathcal{C}} \\left\\{ \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\langle \\nabla u, p \\rangle \\right\\}\n$$\nLet the Lagrangian be $L(u,p) = \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\langle \\nabla u, p \\rangle$. The primal problem is $P = \\min_u \\sup_{p \\in \\mathcal{C}} L(u,p)$. The dual problem is $D = \\sup_{p \\in \\mathcal{C}} \\min_u L(u,p)$. Strong duality holds for this problem, meaning $P=D$.\n\nTo find the dual objective function, we first minimize $L(u,p)$ with respect to $u$. Using the provided adjoint relationship $\\langle \\nabla u, p \\rangle = - \\langle u, \\mathrm{div}\\, p \\rangle$, we rewrite the Lagrangian as:\n$$\nL(u,p) = \\frac{1}{2} \\| u - f \\|_{2}^{2} - \\lambda \\langle u, \\mathrm{div}\\, p \\rangle\n$$\nThis is a convex quadratic function of $u$. The minimum is found by setting its gradient with respect to $u$ to zero:\n$$\n\\nabla_u L(u,p) = (u - f) - \\lambda \\, \\mathrm{div}\\, p = 0\n$$\nThis gives the unique minimizer $u^*(p) = f + \\lambda \\, \\mathrm{div}\\, p$.\n\nSubstituting this optimal $u^*(p)$ back into $L(u,p)$ gives the dual objective function, which we denote $\\mathcal{D}_{\\text{concave}}(p)$:\n$$\n\\mathcal{D}_{\\text{concave}}(p) = \\min_u L(u,p) = \\frac{1}{2} \\| (f + \\lambda \\, \\mathrm{div}\\, p) - f \\|_{2}^{2} - \\lambda \\langle f + \\lambda \\, \\mathrm{div}\\, p, \\mathrm{div}\\, p \\rangle\n$$\n$$\n= \\frac{1}{2} \\| \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2} - \\lambda \\langle f, \\mathrm{div}\\, p \\rangle - \\lambda^2 \\langle \\mathrm{div}\\, p, \\mathrm{div}\\, p \\rangle\n$$\n$$\n= \\frac{1}{2} \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2} - \\lambda \\langle f, \\mathrm{div}\\, p \\rangle - \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2}\n$$\n$$\n= -\\frac{1}{2} \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2} - \\lambda \\langle f, \\mathrm{div}\\, p \\rangle\n$$\nThe dual problem is the maximization of this concave function over the feasible set $\\mathcal{C}$:\n$$\nD = \\sup_{p \\in \\mathcal{C}} \\left\\{ -\\frac{1}{2} \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2} - \\lambda \\langle f, \\mathrm{div}\\, p \\rangle \\right\\}\n$$\nThis is the \"usual concave dual maximization\". To convert this into the requested minimization problem, we use the identity $\\sup_x g(x) = -\\inf_x (-g(x))$. The dual problem is equivalent to:\n$$\nD = -\\min_{p \\in \\mathcal{C}} \\left\\{ \\frac{1}{2} \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2} + \\lambda \\langle f, \\mathrm{div}\\, p \\rangle \\right\\}\n$$\nLet's complete the square for the objective function inside the minimization:\n$$\n\\frac{1}{2} \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2} + \\lambda \\langle f, \\mathrm{div}\\, p \\rangle = \\frac{1}{2} \\left( \\|\\lambda \\, \\mathrm{div}\\, p\\|_2^2 + 2\\langle f, \\lambda \\, \\mathrm{div}\\, p \\rangle \\right)\n$$\n$$\n= \\frac{1}{2} \\left( \\|\\lambda \\, \\mathrm{div}\\, p\\|_2^2 + 2\\langle f, \\lambda \\, \\mathrm{div}\\, p \\rangle + \\|f\\|_2^2 - \\|f\\|_2^2 \\right)\n$$\n$$\n= \\frac{1}{2} \\left( \\| f + \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2} - \\|f\\|_{2}^{2} \\right)\n$$\nSubstituting this back, the value of the dual problem is:\n$$\nD = -\\min_{p \\in \\mathcal{C}} \\left\\{ \\frac{1}{2} \\| f + \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2} - \\frac{1}{2} \\|f\\|_{2}^{2} \\right\\} = \\max_{p \\in \\mathcal{C}} \\left\\{ -\\frac{1}{2} \\| f + \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2} + \\frac{1}{2} \\|f\\|_{2}^{2} \\right\\}\n$$\nThe minimizer of the expression inside the braces is the same as the minimizer of $\\frac{1}{2} \\| f + \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2}$, since $\\frac{1}{2}\\|f\\|_2^2$ is a constant with respect to $p$. Therefore, the dual problem can be formulated as finding the minimizer $p^*$ via:\n$$\n\\min_{p \\in \\mathcal{C}} \\frac{1}{2} \\left\\| f + \\lambda \\, \\mathrm{div}\\, p \\right\\|_{2}^{2}\n$$\nThis completes the derivation of the dual formulation.\n\n### Part 2: Construction of the Primal-Dual Gap Function\n\nThe primal-dual gap $\\mathcal{G}(u,p)$ for a primal variable $u$ and a dual variable $p$ is the difference between the primal objective value at $u$ and the dual objective value at $p$. Let $\\mathcal{P}(u)$ be the primal objective and $\\mathcal{D}(p)$ be the concave dual objective, defined for $p \\in \\mathcal{C}$.\n$$\n\\mathcal{P}(u) = \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u)\n$$\n$$\n\\mathcal{D}(p) = -\\frac{1}{2} \\lambda^2 \\| \\mathrm{div}\\, p \\|_{2}^{2} - \\lambda \\langle f, \\mathrm{div}\\, p \\rangle\n$$\nBy weak duality, $\\mathcal{P}(u) \\ge \\mathcal{D}(p)$ for any $u \\in \\mathbb{R}^n$ and $p \\in \\mathcal{C}$. The gap is $\\mathcal{G}(u,p) = \\mathcal{P}(u) - \\mathcal{D}(p)$, which is non-negative for feasible pairs. To construct a gap function that is finite if and only if $p \\in \\mathcal{C}$, we introduce the indicator function $\\iota_{\\mathcal{C}}(p)$, which is $0$ if $p \\in \\mathcal{C}$ and $+\\infty$ otherwise. The extended dual objective is $\\mathcal{D}_{ext}(p) = \\mathcal{D}(p) - \\iota_{\\mathcal{C}}(p)$. The gap function is then:\n$$\n\\mathcal{G}(u,p) = \\mathcal{P}(u) - (\\mathcal{D}(p) - \\iota_{\\mathcal{C}}(p)) = \\mathcal{P}(u) - \\mathcal{D}(p) + \\iota_{\\mathcal{C}}(p)\n$$\n$$\n\\mathcal{G}(u,p) = \\left( \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u) \\right) - \\left( -\\frac{1}{2}\\lambda^2 \\|\\mathrm{div}\\, p\\|_2^2 - \\lambda \\langle f, \\mathrm{div}\\, p \\rangle \\right) + \\iota_{\\mathcal{C}}(p)\n$$\n$$\n= \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u) + \\frac{1}{2}\\lambda^2 \\|\\mathrm{div}\\, p\\|_2^2 + \\lambda \\langle f, \\mathrm{div}\\, p \\rangle + \\iota_{\\mathcal{C}}(p)\n$$\nWe now rearrange this expression to reveal its structure. Using the adjoint identity $\\langle u, \\mathrm{div}\\, p \\rangle = -\\langle \\nabla u, p \\rangle$:\n$$\n\\mathcal{G}(u,p) = \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\frac{1}{2}\\lambda^2 \\|\\mathrm{div}\\, p\\|_2^2 + \\lambda \\langle f, \\mathrm{div}\\, p \\rangle + \\lambda \\left( \\mathrm{TV}(u) \\right) + \\iota_{\\mathcal{C}}(p)\n$$\nLet's complete the square on the terms involving $f$ and $\\mathrm{div}\\,p$:\n$$\n\\frac{1}{2}\\|u-f\\|_2^2 = \\frac{1}{2}\\|u\\|_2^2 - \\langle u,f \\rangle + \\frac{1}{2}\\|f\\|_2^2\n$$\nThe expression is complicated. A more direct route is using the definition of TV and the Fenchel-Young inequality.\nWe start from the gap definition:\n$$\n\\mathcal{G}(u,p) = \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u) - \\mathcal{D}(p) + \\iota_{\\mathcal{C}}(p)\n$$\nWe know from duality theory that $\\mathcal{D}(p) = \\min_v L(v,p) = L(u^*(p), p)$.\nLet's look at the gap differently:\n$\\mathcal{P}(u) - \\mathcal{D}(p) = L(u,p) - \\lambda(\\langle\\nabla u,p\\rangle - \\mathrm{TV}(u)) - \\min_v L(v,p)$. This is also complex.\n\nLet's return to the expression and simplify it differently.\n$$\n\\mathcal{G}(u,p) = \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda \\, \\mathrm{TV}(u) + \\frac{1}{2}\\lambda^2 \\|\\mathrm{div}\\, p\\|_2^2 + \\lambda \\langle f, \\mathrm{div}\\, p \\rangle + \\iota_{\\mathcal{C}}(p)\n$$\nSubstitute $\\langle f, \\mathrm{div}\\, p \\rangle = \\langle u, \\mathrm{div}\\, p \\rangle - \\langle u-f, \\mathrm{div}\\, p \\rangle = -\\langle \\nabla u, p \\rangle - \\langle u-f, \\mathrm{div}\\, p \\rangle$.\n$$\n= \\frac{1}{2} \\| u - f \\|_{2}^{2} + \\lambda\\left(\\mathrm{TV}(u) - \\langle \\nabla u, p \\rangle\\right) - \\lambda \\langle u-f, \\mathrm{div}\\, p \\rangle + \\frac{1}{2}\\lambda^2 \\|\\mathrm{div}\\, p\\|_2^2 + \\iota_{\\mathcal{C}}(p)\n$$\nNow group terms into a squared norm:\n$$\n\\frac{1}{2}\\|u-f\\|_2^2 - \\lambda\\langle u-f, \\mathrm{div}\\,p \\rangle + \\frac{1}{2}\\lambda^2\\|\\mathrm{div}\\,p\\|_2^2 = \\frac{1}{2}\\|(u-f) - \\lambda \\mathrm{div}\\,p\\|_2^2\n$$\nSo the gap becomes:\n$$\n\\mathcal{G}(u,p) = \\frac{1}{2} \\| u - f - \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2} + \\lambda \\left( \\mathrm{TV}(u) - \\langle \\nabla u, p \\rangle \\right) + \\iota_{\\mathcal{C}}(p)\n$$\nThis is the final expression for the primal-dual gap.\nLet us verify its properties:\n1.  **Finiteness**: $\\mathcal{G}(u,p)$ is finite if and only if $\\iota_{\\mathcal{C}}(p) = 0$, which means $p \\in \\mathcal{C}$.\n2.  **Non-negativity**: For $p \\in \\mathcal{C}$, the first term $\\frac{1}{2} \\| u - f - \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2}$ is non-negative. For the second term, we know $\\mathrm{TV}(u) = \\sup_{q \\in \\mathcal{C}} \\langle \\nabla u, q \\rangle$. Since $p \\in \\mathcal{C}$, we must have $\\langle \\nabla u, p \\rangle \\le \\mathrm{TV}(u)$. Thus, $\\mathrm{TV}(u) - \\langle \\nabla u, p \\rangle \\ge 0$. As $\\lambda  0$, the entire expression is non-negative for $p \\in \\mathcal{C}$.\n3.  **Zero Condition**: $\\mathcal{G}(u,p) = 0$ if and only if all non-negative terms are zero. This requires $p \\in \\mathcal{C}$ and\n    -   $u - f - \\lambda \\, \\mathrm{div}\\, p = 0$ (stationarity condition or primal-dual relation).\n    -   $\\mathrm{TV}(u) = \\langle \\nabla u, p \\rangle$ (complementary slackness condition).\n    These are the Karush-Kuhn-Tucker (KKT) optimality conditions for the ROF problem.\n\nThe derived expression for $\\mathcal{G}(u,p)$ satisfies all the required properties.",
            "answer": "$$\n\\boxed{\\mathcal{G}(u,p) = \\frac{1}{2} \\| u - f - \\lambda \\, \\mathrm{div}\\, p \\|_{2}^{2} + \\lambda \\left( \\sum_{i=1}^{n} \\| (\\nabla u)_{i} \\|_{2} - \\langle \\nabla u, p \\rangle \\right) + \\iota_{\\mathcal{C}}(p)}\n$$"
        },
        {
            "introduction": "Having derived the dual of the ROF problem, the next step is to devise an algorithm to solve it. This exercise introduces a classic and elegant approach: projected gradient descent, famously adapted for this problem by Chambolle. You will derive the iterative update rule and the simple projection operator that enforces the dual constraint . Most importantly, this practice demonstrates how to use the theoretical properties of the dual objective to determine the largest possible step-size that still guarantees the algorithm's convergence, a crucial link between optimization theory and robust implementation.",
            "id": "3428015",
            "problem": "Consider the Rudin–Osher–Fatemi (ROF) model for denoising an image, posed on a bounded domain $\\,\\Omega \\subset \\mathbb{R}^{2}\\,$ with homogeneous Neumann boundary conditions (NBC). Let the observed image be $\\,f \\in L^{2}(\\Omega)\\,$ and consider the ROF primal problem with unit fidelity weight:\n$$\n\\min_{u \\in BV(\\Omega)} \\left\\{ \\mathrm{TV}(u) + \\frac{1}{2} \\int_{\\Omega} (u - f)^{2} \\, dx \\right\\},\n$$\nwhere $\\mathrm{TV}(u)$ denotes the Total Variation (TV) of $\\,u$ and $\\,BV(\\Omega)\\,$ is the space of functions of bounded variation. Use the following fundamental bases:\n- The dual representation of Total Variation: for smooth vector fields $\\,p \\in C^{1}(\\Omega;\\mathbb{R}^{2})\\,$ with $\\,p \\cdot n = 0\\,$ on $\\,\\partial \\Omega$ and $\\,|p(x)| \\le 1\\,$ almost everywhere, one has\n$$\n\\mathrm{TV}(u) = \\sup_{\\substack{p \\in C^{1}(\\Omega;\\mathbb{R}^{2}) \\\\ p \\cdot n = 0,\\, |p|\\le 1}} -\\int_{\\Omega} u \\,\\mathrm{div}\\, p \\, dx.\n$$\n- The adjoint relation between divergence and gradient under NBC: for suitable $\\,v\\,$ and $\\,p\\,$,\n$$\n\\int_{\\Omega} v \\,\\mathrm{div}\\, p \\, dx \\;=\\; - \\int_{\\Omega} p \\cdot \\nabla v \\, dx.\n$$\n\n(a) Derive the ROF dual problem by classical convex duality from the primal formulation given above. Then, derive the gradient descent update for the dual variable $\\,p\\,$ (interpreted as a vector field) and the orthogonal projection onto the convex set $\\,\\{p: \\|p\\|_{\\infty} \\le 1\\}\\,$, where $\\,\\|p\\|_{\\infty} := \\operatorname*{ess\\,sup}_{x \\in \\Omega} |p(x)|\\,$ with $\\,|\\cdot|\\,$ the Euclidean norm in $\\,\\mathbb{R}^{2}$.\n\n(b) Consider a standard discrete setting on a rectangular $\\,N \\times M\\,$ grid with forward differences for the discrete gradient operator $\\,\\nabla\\,$ and its negative transpose as the discrete divergence $\\,\\mathrm{div}\\,$, together with homogeneous Neumann boundary conditions. Using operator norm bounds for these discrete operators, determine the largest admissible constant step-size $\\,\\tau\\,$ for the projected gradient descent on the dual problem that guarantees convergence. State the convergence condition in terms of the Lipschitz constant of the dual objective’s gradient and then provide the largest constant step-size as a single real number.\n\nYour final reported answer must be the largest admissible constant step-size from part (b), expressed in exact form. No rounding is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the well-established theory of convex optimization and mathematical image processing, specifically the Rudin–Osher–Fatemi (ROF) model. The problem is well-posed, objective, and contains all necessary information for a rigorous derivation.\n\n(a) Derivation of the dual problem, gradient update, and projection.\n\nThe primal ROF problem is given by:\n$$\n\\min_{u \\in BV(\\Omega)} \\left\\{ \\mathrm{TV}(u) + \\frac{1}{2} \\int_{\\Omega} (u - f)^{2} \\, dx \\right\\}\n$$\nWe use the provided dual representation of the Total Variation (TV) functional:\n$$\n\\mathrm{TV}(u) = \\sup_{p \\in K} -\\int_{\\Omega} u \\,\\mathrm{div}\\, p \\, dx\n$$\nwhere $K$ is the convex set of vector fields $K = \\{ p \\in C^{1}(\\Omega;\\mathbb{R}^{2}) : |p(x)| \\le 1 \\text{ a.e. and } p \\cdot n =\n0 \\text{ on } \\partial\\Omega \\}$. Substituting this into the primal problem, we formulate a saddle-point (min-max) problem:\n$$\n\\min_{u \\in BV(\\Omega)} \\sup_{p \\in K} \\left\\{ -\\int_{\\Omega} u \\,\\mathrm{div}\\, p \\, dx + \\frac{1}{2} \\int_{\\Omega} (u - f)^{2} \\, dx \\right\\}\n$$\nLet the Lagrangian be $L(u,p) = -\\int_{\\Omega} u \\,\\mathrm{div}\\, p \\, dx + \\frac{1}{2} \\int_{\\Omega} (u - f)^{2} \\, dx$.\nThe dual problem is obtained by swapping the order of minimization and maximization:\n$$\n\\sup_{p \\in K} \\min_{u \\in BV(\\Omega)} L(u,p)\n$$\nTo find the inner minimum, we find the stationary point of $L(u,p)$ with respect to $u$. The functional $L(u,p)$ is convex in $u$, so its minimizer is found by setting its functional derivative with respect to $u$ to zero:\n$$\n\\frac{\\delta L}{\\delta u} = -\\mathrm{div}\\, p + (u - f) = 0\n$$\nThis yields the optimal $u$ for a given $p$:\n$$\nu^*(p) = f + \\mathrm{div}\\, p\n$$\nSubstituting this back into the Lagrangian gives the dual objective function $J(p)$:\n$$\nJ(p) = L(u^*(p), p) = -\\int_{\\Omega} (f + \\mathrm{div}\\, p) \\,\\mathrm{div}\\, p \\, dx + \\frac{1}{2} \\int_{\\Omega} ((f + \\mathrm{div}\\, p) - f)^{2} \\, dx\n$$\n$$\nJ(p) = -\\int_{\\Omega} f \\,\\mathrm{div}\\, p \\, dx - \\int_{\\Omega} (\\mathrm{div}\\, p)^2 \\, dx + \\frac{1}{2} \\int_{\\Omega} (\\mathrm{div}\\, p)^2 \\, dx\n$$\n$$\nJ(p) = -\\int_{\\Omega} f \\,\\mathrm{div}\\, p \\, dx - \\frac{1}{2} \\int_{\\Omega} (\\mathrm{div}\\, p)^2 \\, dx\n$$\nThe dual problem is the maximization of this concave functional over the convex set $K$:\n$$\n\\max_{p \\in K} \\left\\{ -\\int_{\\Omega} f \\,\\mathrm{div}\\, p \\, dx - \\frac{1}{2} \\|\\mathrm{div}\\, p\\|_{L^2}^2 \\right\\}\n$$\nFor the gradient descent update, we consider the equivalent minimization problem for the negative of the dual objective. Let $H(p) = -J(p)$:\n$$\n\\min_{p \\in K} H(p) = \\min_{p \\in K} \\left\\{ \\frac{1}{2} \\|\\mathrm{div}\\, p\\|_{L^2}^2 + \\int_{\\Omega} f \\,\\mathrm{div}\\, p \\, dx \\right\\}\n$$\n$H(p)$ is a convex functional. The gradient descent update requires the gradient of $H(p)$. We compute the Gateaux derivative of $H(p)$ in the direction of a vector field $q$:\n$$\n\\delta H(p; q) = \\left. \\frac{d}{d\\epsilon} H(p+\\epsilon q) \\right|_{\\epsilon=0} = \\int_{\\Omega} (\\mathrm{div}\\, p + f) \\mathrm{div}\\, q \\, dx\n$$\nUsing the adjoint relation $\\int_{\\Omega} v \\,\\mathrm{div}\\, q \\, dx = -\\int_{\\Omega} \\nabla v \\cdot q \\, dx$, which holds because $q$ (as a variation of $p$) must also satisfy $q \\cdot n = 0$ on $\\partial\\Omega$:\n$$\n\\delta H(p; q) = -\\int_{\\Omega} \\nabla(\\mathrm{div}\\, p + f) \\cdot q \\, dx\n$$\nThis must equal $\\langle \\nabla_p H(p), q \\rangle_{L^2}$. Thus, the gradient of $H(p)$ is:\n$$\n\\nabla_p H(p) = -\\nabla(f + \\mathrm{div}\\, p)\n$$\nThe projected gradient descent update for the dual variable $p$ with a step-size $\\tau > 0$ is:\n$$\np_{k+1} = \\Pi_{K} \\left( p_k - \\tau \\nabla_p H(p_k) \\right) = \\Pi_{K} \\left( p_k + \\tau \\nabla(f + \\mathrm{div}\\, p_k) \\right)\n$$\nwhere $\\Pi_{K}$ is the orthogonal projection onto the convex set $K$. The constraint $|p(x)| \\le 1$ is pointwise. The projection can therefore be computed pointwise for each $x \\in \\Omega$. For a vector $q \\in \\mathbb{R}^2$, the projection onto the unit ball $\\{v \\in \\mathbb{R}^2 : |v| \\le 1\\}$ is:\n$$\n\\Pi(q) = \\begin{cases} q  \\text{if } |q| \\le 1 \\\\ \\frac{q}{|q|}  \\text{if } |q|  1 \\end{cases}\n$$\nThis can be written compactly as:\n$$\n\\Pi(q) = \\frac{q}{\\max(1, |q|)}\n$$\nSo, the projection operator on the vector field $p$ is applied pointwise: $(\\Pi_K(p))(x) = \\frac{p(x)}{\\max(1, |p(x)|)}$.\n\n(b) Determination of the largest admissible step-size.\n\nWe perform projected gradient descent on the convex functional $H(p)$ over the convex set $K$. The convergence of this method is guaranteed if the gradient $\\nabla_p H(p)$ is Lipschitz continuous and the step-size $\\tau$ is chosen appropriately.\nThe convergence condition for projected gradient descent is $0  \\tau  2/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla_p H(p)$.\n\nFirst, we find the Lipschitz constant $L$. We analyze the difference of the gradients at two points, $p_1$ and $p_2$:\n$$\n\\nabla_p H(p_1) - \\nabla_p H(p_2) = -\\nabla(f + \\mathrm{div}\\, p_1) - (-\\nabla(f + \\mathrm{div}\\, p_2)) = -\\nabla(\\mathrm{div}(p_1 - p_2))\n$$\nThe Lipschitz constant $L$ is the operator norm of the linear map $p \\mapsto -\\nabla(\\mathrm{div}\\, p)$.\n$$\nL = \\|-\\nabla \\circ \\mathrm{div}\\| = \\|\\nabla \\circ \\mathrm{div}\\|\n$$\nUsing the adjoint relationship $\\mathrm{div} = -\\nabla^*$, where $\\nabla^*$ is the adjoint of the gradient operator $\\nabla$:\n$$\nL = \\|\\nabla \\circ (-\\nabla^*)\\| = \\|-\\nabla \\nabla^*\\| = \\|\\nabla \\nabla^*\\|\n$$\nIt is a standard result in operator theory that $\\|\\mathcal{A}\\mathcal{A}^*\\| = \\|\\mathcal{A}\\|^2$ for any linear operator $\\mathcal{A}$. Therefore,\n$$\nL = \\|\\nabla\\|^2\n$$\nThe problem considers a discrete setting with forward differences for the gradient operator $\\nabla$. For a 2D grid with unit spacing, the operator norm $\\|\\nabla\\|$ is bounded. The operator $\\|\\nabla\\|^2$ corresponds to the largest eigenvalue of the discrete negative Laplacian with homogeneous Neumann boundary conditions, $-\\Delta_N = \\nabla^*\\nabla$.\nFor forward differences, the squared norm of the 1D discrete gradient operator is bounded by $4$. For the 2D gradient, $\\nabla = (\\nabla_x, \\nabla_y)$, the squared norm is the sum of the squared norms of its components (as they operate on orthogonal directions):\n$$\n\\|\\nabla\\|^2 = \\|\\nabla_x\\|^2 + \\|\\nabla_y\\|^2\n$$\nThe standard operator norm bound for the discrete gradient with forward differences is $\\|\\nabla\\|^2 \\le 8$. We use this established bound, so $L=8$.\n\nThe convergence condition for the step-size $\\tau$ is:\n$$\n0  \\tau  \\frac{2}{L}\n$$\nSubstituting $L=8$, we get:\n$$\n0  \\tau  \\frac{2}{8} \\quad \\implies \\quad 0  \\tau  \\frac{1}{4}\n$$\nThe question asks for the largest admissible constant step-size. This corresponds to the upper bound of the stable interval for $\\tau$.\n$$\n\\tau_{\\text{max}} = \\frac{2}{L} = \\frac{2}{8} = \\frac{1}{4}\n$$\nThis step-size marks the boundary of the region of guaranteed convergence for the projected gradient descent method.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        }
    ]
}