{
    "hands_on_practices": [
        {
            "introduction": "This first exercise goes back to the basics, ensuring a concrete grasp of the Bregman distance itself before implementing more complex algorithms. The practice requires a direct, step-by-step calculation for a simple one-dimensional signal using the Total Variation (TV) functional. This process reinforces the fundamental roles of the convex functional, the choice of subgradient, and the vectors being compared in defining the distance .",
            "id": "3369778",
            "problem": "Consider discrete one-dimensional signals of length $n=5$, with $u,v \\in \\mathbb{R}^{5}$ given by\n$$\nu=\\begin{pmatrix}2\\\\0\\\\4\\\\2\\\\1\\end{pmatrix}, \\qquad v=\\begin{pmatrix}0\\\\2\\\\5\\\\1\\\\3\\end{pmatrix}.\n$$\nDefine the anisotropic total variation (TV) of a discrete signal $w \\in \\mathbb{R}^{5}$ by the convex functional\n$$\n\\mathrm{TV}(w)=\\sum_{i=1}^{4} |w_{i+1}-w_{i}|.\n$$\nLet $D:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ be the forward finite-difference operator defined by $(Dw)_{i}=w_{i+1}-w_{i}$ for $i=1,2,3,4$. For a convex functional $J$, the Bregman distance with respect to a subgradient $p \\in \\partial J(v)$ is defined by\n$$\nD_{J}^{p}(u,v)=J(u)-J(v)-\\langle p, u-v\\rangle,\n$$\nwhere $\\langle \\cdot,\\cdot\\rangle$ denotes the standard Euclidean inner product. Use the characterization of the subdifferential of $\\mathrm{TV}$ via the chain rule for subgradients: there exists $s\\in\\mathbb{R}^{4}$ with $s_{i}\\in \\operatorname{sign}\\big((Dv)_{i}\\big)$ such that $p=D^{\\top}s$, where $D^{\\top}$ is the adjoint of $D$ under the Euclidean inner product. Here $\\operatorname{sign}(t)=1$ if $t>0$, $\\operatorname{sign}(t)=-1$ if $t<0$, and $\\operatorname{sign}(0)=[-1,1]$.\n\nCompute the Bregman distance $D_{\\mathrm{TV}}^{p}(u,v)$ using the choice $s_{i}=\\operatorname{sign}\\big((Dv)_{i}\\big)$ for $i=1,2,3,4$ and $p=D^{\\top}s$. Your final answer must be a single real number. No rounding is required.",
            "solution": "The problem asks for the computation of the Bregman distance $D_{\\mathrm{TV}}^{p}(u,v)$ for given discrete signals $u,v \\in \\mathbb{R}^{5}$. The formula for the Bregman distance with respect to a convex functional $J$ and a subgradient $p \\in \\partial J(v)$ is given by:\n$$\nD_{J}^{p}(u,v) = J(u) - J(v) - \\langle p, u-v \\rangle\n$$\nIn this problem, the functional $J$ is the anisotropic total variation, $J=\\mathrm{TV}$. Thus, we need to compute:\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = \\mathrm{TV}(u) - \\mathrm{TV}(v) - \\langle p, u-v \\rangle\n$$\nWe will compute each term separately. The vectors are given as $u=\\begin{pmatrix}2\\\\0\\\\4\\\\2\\\\1\\end{pmatrix}$ and $v=\\begin{pmatrix}0\\\\2\\\\5\\\\1\\\\3\\end{pmatrix}$.\n\nFirst, we calculate the total variation of $u$. The definition is $\\mathrm{TV}(w) = \\sum_{i=1}^{4} |w_{i+1}-w_{i}|$.\nFor $u$, the differences are:\n$u_2 - u_1 = 0-2 = -2$\n$u_3 - u_2 = 4-0 = 4$\n$u_4 - u_3 = 2-4 = -2$\n$u_5 - u_4 = 1-2 = -1$\nThe total variation of $u$ is the sum of the absolute values of these differences:\n$$\n\\mathrm{TV}(u) = |-2| + |4| + |-2| + |-1| = 2 + 4 + 2 + 1 = 9\n$$\n\nSecond, we calculate the total variation of $v$. For $v$, the differences are:\n$v_2 - v_1 = 2-0 = 2$\n$v_3 - v_2 = 5-2 = 3$\n$v_4 - v_3 = 1-5 = -4$\n$v_5 - v_4 = 3-1 = 2$\nThe total variation of $v$ is:\n$$\n\\mathrm{TV}(v) = |2| + |3| + |-4| + |2| = 2 + 3 + 4 + 2 = 11\n$$\n\nThird, we compute the subgradient $p \\in \\partial \\mathrm{TV}(v)$. The problem states that $p = D^{\\top}s$, where $s \\in \\mathbb{R}^4$ is determined by the specific choice $s_{i}=\\operatorname{sign}\\big((Dv)_{i}\\big)$. The operator $D:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ is the forward finite-difference operator, $(Dw)_i = w_{i+1}-w_i$.\nThe vector $Dv$ contains the differences we computed for $\\mathrm{TV}(v)$:\n$$\nDv = \\begin{pmatrix} v_2 - v_1 \\\\ v_3 - v_2 \\\\ v_4 - v_3 \\\\ v_5 - v_4 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ -4 \\\\ 2 \\end{pmatrix}\n$$\nNow, we find the vector $s$ by taking the sign of each component of $Dv$. The sign function is defined as $\\operatorname{sign}(t)=1$ for $t>0$ and $\\operatorname{sign}(t)=-1$ for $t<0$. Since no component of $Dv$ is zero, we have a unique $s$:\n$$\ns = \\begin{pmatrix} \\operatorname{sign}(2) \\\\ \\operatorname{sign}(3) \\\\ \\operatorname{sign}(-4) \\\\ \\operatorname{sign}(2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nNext, we determine the action of the adjoint operator $D^{\\top}:\\mathbb{R}^4 \\to \\mathbb{R}^5$. The defining property of the adjoint is $\\langle Dw, x \\rangle = \\langle w, D^{\\top}x \\rangle$ for all $w \\in \\mathbb{R}^5, x \\in \\mathbb{R}^4$.\n$$\n\\langle Dw, x \\rangle = \\sum_{i=1}^{4} (w_{i+1}-w_i)x_i = \\sum_{i=1}^{4} w_{i+1}x_i - \\sum_{i=1}^{4} w_i x_i\n$$\nBy re-indexing and grouping terms by $w_i$, we get:\n$$\n\\langle Dw, x \\rangle = -w_1 x_1 + w_2(x_1-x_2) + w_3(x_2-x_3) + w_4(x_3-x_4) + w_5 x_4\n$$\nFrom this, we can identify the components of $D^{\\top}x$:\n$(D^{\\top}x)_1 = -x_1$\n$(D^{\\top}x)_2 = x_1 - x_2$\n$(D^{\\top}x)_3 = x_2 - x_3$\n$(D^{\\top}x)_4 = x_3 - x_4$\n$(D^{\\top}x)_5 = x_4$\nApplying this to our vector $s = (1, 1, -1, 1)^{\\top}$, we find the subgradient $p=D^{\\top}s$:\n$p_1 = -s_1 = -1$\n$p_2 = s_1 - s_2 = 1 - 1 = 0$\n$p_3 = s_2 - s_3 = 1 - (-1) = 2$\n$p_4 = s_3 - s_4 = -1 - 1 = -2$\n$p_5 = s_4 = 1$\nSo, the subgradient is $p = \\begin{pmatrix} -1 \\\\ 0 \\\\ 2 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n\nFourth, we calculate the inner product $\\langle p, u-v \\rangle$. We first find the vector difference $u-v$:\n$$\nu-v = \\begin{pmatrix} 2-0 \\\\ 0-2 \\\\ 4-5 \\\\ 2-1 \\\\ 1-3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\nNow we compute the inner product:\n$$\n\\langle p, u-v \\rangle = p_1(u_1-v_1) + p_2(u_2-v_2) + p_3(u_3-v_3) + p_4(u_4-v_4) + p_5(u_5-v_5)\n$$\n$$\n\\langle p, u-v \\rangle = (-1)(2) + (0)(-2) + (2)(-1) + (-2)(1) + (1)(-2) = -2 + 0 - 2 - 2 - 2 = -8\n$$\n\nFinally, we substitute all the calculated values back into the Bregman distance formula:\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = \\mathrm{TV}(u) - \\mathrm{TV}(v) - \\langle p, u-v \\rangle = 9 - 11 - (-8)\n$$\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = -2 - (-8) = -2 + 8 = 6\n$$\nThe resulting Bregman distance is a non-negative value, as expected from its definition for a convex functional.",
            "answer": "$$\n\\boxed{6}\n$$"
        },
        {
            "introduction": "Moving from definition to application, this exercise involves implementing the split Bregman method, a powerful variant of Bregman iteration that is equivalent to the Alternating Direction Method of Multipliers (ADMM). You will tackle the classic problem of Total Variation (TV) denoising, which requires handling both a differentiable data-fidelity term and a non-differentiable regularizer. This practice highlights how the complex original problem is broken down into a sequence of simpler subproblems, a core strength of this algorithmic family .",
            "id": "3369799",
            "problem": "Consider the one-dimensional Total Variation (TV) denoising problem for an identity forward operator within the framework of inverse problems and data assimilation. Let $A$ denote the forward operator, and suppose $A = I$ (the identity). Given a data vector $g \\in \\mathbb{R}^n$, we pose the Rudin–Osher–Fatemi TV denoising objective as minimizing the sum of a data fidelity term and a TV regularization term, namely the functional whose value at a candidate signal $u \\in \\mathbb{R}^n$ is the sum of the squared data misfit and the $\\ell_1$-norm of the discrete gradient of $u$, scaled by the regularization parameter. The discrete gradient is defined using finite differences under periodic boundary conditions.\n\nDefine the finite difference operator $D : \\mathbb{R}^n \\to \\mathbb{R}^n$ by the component-wise rule\n$$(D u)_i = u_{(i+1) \\bmod n} - u_i \\quad \\text{for } i \\in \\{0,1,\\dots,n-1\\}.$$\nAssume the Split Bregman method is used to solve the denoising problem. In the Split Bregman algorithm, the iteration state comprises the primal variable $u \\in \\mathbb{R}^n$, the split variable $d \\in \\mathbb{R}^n$ that enforces $d \\approx D u$, and the Bregman variable $b \\in \\mathbb{R}^n$. The iteration uses a penalty parameter $\\lambda > 0$ and a TV regularization parameter $\\mu > 0$.\n\nImplement one full Split Bregman iteration starting from the initial state $u^{0} = 0$, $d^{0} = 0$, and $b^{0} = 0$ (the zero vectors in $\\mathbb{R}^n$). Use the principle-based derivation from the augmented Lagrangian viewpoint to compute the update of $u$ and the update of $d$ at the first iteration. The discrete operators must be implemented exactly as specified, and all computations must be carried out in double-precision floating point arithmetic.\n\nYour program must produce, for each specified test case, the vectors $u^{1}$ and $d^{1}$ yielded by a single Split Bregman iteration. Represent vectors as lists of floating-point numbers in the program output.\n\nUse the following test suite, which covers typical behavior, near-constant data, a small problem size, and a higher-penalty regime:\n\n- Test case $1$: $n = 6$, $g = [\\,1.0,\\,1.2,\\,0.9,\\,-0.5,\\,-0.4,\\,0.0\\,]$, $\\mu = 0.6$, $\\lambda = 2.0$.\n- Test case $2$: $n = 6$, $g = [\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,]$, $\\mu = 1.5$, $\\lambda = 1.0$.\n- Test case $3$: $n = 3$, $g = [\\,0.0,\\,10.0,\\,-10.0\\,]$, $\\mu = 0.4$, $\\lambda = 0.5$.\n- Test case $4$: $n = 8$, $g = [\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,1.25,\\,0.8,\\,0.3,\\,-0.2\\,]$, $\\mu = 2.0$, $\\lambda = 3.0$.\n\nFor each test case, compute $u^{1}$ and $d^{1}$ using one full Split Bregman iteration with the finite difference operator $D$ as defined above under periodic boundary conditions.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The $i$-th element of this list must itself be a list of two lists, where the first inner list is $u^{1}$ and the second inner list is $d^{1}$ for the $i$-th test case. For example, the output format should be\n$$[\\, [\\,[u^{1}_{(1)}],\\,[d^{1}_{(1)}]\\,],\\,[\\,[u^{1}_{(2)}],\\,[d^{1}_{(2)}]\\,],\\,\\dots\\, ]$$\nwith no spaces in the printed output.",
            "solution": "The problem is valid and well-posed. It describes a standard application of the Split Bregman algorithm to the Total Variation (TV) denoising problem, providing all necessary definitions, parameters, and initial conditions.\n\nThe one-dimensional Rudin–Osher–Fatemi (ROF) TV denoising problem seeks to find a signal $u \\in \\mathbb{R}^n$ that minimizes the following objective functional for a given noisy signal $g \\in \\mathbb{R}^n$:\n$$ E(u) = \\frac{1}{2} \\|u-g\\|_2^2 + \\mu \\|Du\\|_1 $$\nHere, $\\mu > 0$ is the regularization parameter that controls the strength of the denoising. The operator $D: \\mathbb{R}^n \\to \\mathbb{R}^n$ is the discrete gradient, defined with periodic boundary conditions as $(Du)_i = u_{(i+1) \\bmod n} - u_i$.\n\nThe $\\ell_1$-norm term $\\|Du\\|_1$ is non-differentiable, making direct minimization difficult. The Split Bregman method addresses this by introducing a splitting variable $d \\in \\mathbb{R}^n$ and converting the problem into a constrained optimization problem:\n$$ \\min_{u,d} \\frac{1}{2} \\|u-g\\|_2^2 + \\mu \\|d\\|_1 \\quad \\text{subject to} \\quad d = Du $$\n\nThis constrained problem is solved by minimizing an associated augmented Lagrangian, which leads to an iterative scheme. The Split Bregman iteration updates the primal variable $u$, the split variable $d$, and a Bregman (dual) variable $b \\in \\mathbb{R}^n$ in an alternating fashion. The iterative scheme, for iteration $k+1$, is given by:\n1.  Update $u$:\n    $$ u^{k+1} = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du - d^k + b^k\\|_2^2 \\right\\} $$\n2.  Update $d$:\n    $$ d^{k+1} = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|Du^{k+1} - d + b^k\\|_2^2 \\right\\} $$\n3.  Update $b$:\n    $$ b^{k+1} = b^k + (Du^{k+1} - d^{k+1}) $$\nThe parameter $\\lambda > 0$ is a penalty parameter for the constraint $d=Du$.\n\nWe are tasked with computing one full iteration, yielding $u^1$ and $d^1$, starting from the initial state $u^0 = 0$, $d^0 = 0$, and $b^0 = 0$.\n\n**Step 1: The $u$-Subproblem for $u^1$**\n\nWe substitute $k=0$ along with the initial conditions $d^0=0$ and $b^0=0$ into the $u$-update rule:\n$$ u^1 = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du - 0 + 0\\|_2^2 \\right\\} = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du\\|_2^2 \\right\\} $$\nThis is a quadratic minimization problem, which can be solved by setting the gradient of the objective with respect to $u$ to zero. The gradient of $\\frac{1}{2}\\|u-g\\|_2^2$ is $u-g$. The gradient of $\\frac{\\lambda}{2}\\|Du\\|_2^2 = \\frac{\\lambda}{2} u^T D^T D u$ is $\\lambda D^T D u$. Setting the sum of gradients to zero gives the normal equations:\n$$ (u^1 - g) + \\lambda D^T D u^1 = 0 $$\n$$ (I + \\lambda D^T D) u^1 = g $$\nwhere $I$ is the $n \\times n$ identity matrix and $D^T$ is the transpose of $D$. The matrix $C = I + \\lambda D^T D$ is circulant because $D$ (forward difference with periodic boundaries) is a circulant matrix. A circulant linear system can be solved efficiently in the Fourier domain. Let $\\mathcal{F}$ denote the Discrete Fourier Transform (DFT). Applying the DFT to the system yields:\n$$ \\mathcal{F}((I + \\lambda D^T D) u^1) = \\mathcal{F}(g) $$\n$$ \\mathcal{F}(I + \\lambda D^T D) \\mathcal{F}(u^1) = \\mathcal{F}(g) $$\nThe term $\\mathcal{F}(I + \\lambda D^T D)$ represents the eigenvalues of the circulant matrix $C$. These eigenvalues can be found by taking the DFT of the first row of $C$. The operator $D^T D$ is the negative of the 1D discrete Laplacian with periodic boundaries, $(D^T D u)_i = -u_{(i+1) \\bmod n} + 2u_i - u_{(i-1) \\bmod n}$. Thus, the first row of $C$ is $c = [1+2\\lambda, -\\lambda, 0, \\dots, 0, -\\lambda]$.\nThe DFT of $u^1$ is then:\n$$ \\mathcal{F}(u^1)_k = \\frac{\\mathcal{F}(g)_k}{ \\mathcal{F}(c)_k } $$\nFinally, we obtain $u^1$ by applying the inverse DFT:\n$$ u^1 = \\mathcal{F}^{-1}\\left( \\frac{\\mathcal{F}(g)}{\\mathcal{F}(c)} \\right) $$\n\n**Step 2: The $d$-Subproblem for $d^1$**\n\nNext, we compute $d^1$ using the newly computed $u^1$ and the previous $b^0=0$:\n$$ d^1 = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|Du^1 - d + 0\\|_2^2 \\right\\} = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|d - Du^1\\|_2^2 \\right\\} $$\nThis problem is separable, meaning we can solve for each component $d_i$ independently:\n$$ d^1_i = \\arg\\min_{d_i} \\left\\{ \\mu |d_i| + \\frac{\\lambda}{2} (d_i - (Du^1)_i)^2 \\right\\} $$\nThis is a standard problem in convex optimization, and its solution is given by the soft-shrinkage operator. Let $v_i = (Du^1)_i$ and the threshold be $T = \\mu/\\lambda$. The solution is:\n$$ d^1_i = \\text{shrink}(v_i, T) = \\text{sign}(v_i) \\max(|v_i| - T, 0) $$\nThis can be computed for all components $i=0, \\dots, n-1$ to obtain the vector $d^1$.\n\n**Summary of the Algorithm for One Iteration**\nGiven $n$, $g$, $\\mu$, and $\\lambda$:\n1.  Construct the first row of the circulant matrix $C = I + \\lambda D^T D$: $c = [1+2\\lambda, -\\lambda, 0, \\dots, 0, -\\lambda]$.\n2.  Compute the DFT of $c$ to get the eigenvalues of $C$, let this be $\\hat{c} = \\mathcal{F}(c)$.\n3.  Compute the DFT of the data vector $g$, let this be $\\hat{g} = \\mathcal{F}(g)$.\n4.  Compute the DFT of the solution $u^1$ by element-wise division: $\\hat{u}^1_k = \\hat{g}_k / \\hat{c}_k$.\n5.  Compute $u^1$ by taking the inverse DFT of $\\hat{u}^1$: $u^1 = \\mathcal{F}^{-1}(\\hat{u}^1)$.\n6.  Compute the discrete gradient of $u^1$: $v = Du^1$.\n7.  Apply the soft-shrinkage operator to $v$ with threshold $T=\\mu/\\lambda$ to obtain $d^1$.\nAll computations are performed using double-precision floating-point arithmetic.",
            "answer": "```python\nimport numpy as np\nimport json\n\ndef run_split_bregman_iteration(n: int, g: np.ndarray, mu: float, lambda_param: float) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Computes one full Split Bregman iteration for 1D TV denoising.\n\n    Args:\n        n (int): The size of the signal.\n        g (np.ndarray): The observed (noisy) signal vector.\n        mu (float): The TV regularization parameter.\n        lambda_param (float): The Bregman penalty parameter.\n\n    Returns:\n        tuple[list[float], list[float]]: A tuple containing the updated vectors u^1 and d^1 as lists.\n    \"\"\"\n    # Step 1: Compute u^1 by solving (I + lambda * D^T D)u = g\n    # This is a circulant system, solved efficiently via FFT.\n\n    # First, construct the first row of the circulant matrix C = I + lambda * D^T * D\n    c_first_row = np.zeros(n, dtype=np.float64)\n    c_first_row[0] = 1.0 + 2.0 * lambda_param\n    c_first_row[1] = -lambda_param\n    c_first_row[n-1] = -lambda_param\n\n    # Eigenvalues of C are the FFT of its first row.\n    c_eig = np.fft.fft(c_first_row)\n\n    # FFT of the data vector g\n    g_hat = np.fft.fft(g)\n\n    # Solve for u^1 in the Fourier domain\n    u1_hat = g_hat / c_eig\n\n    # Inverse FFT to get u^1 in the spatial domain.\n    # The result should be real; take the real part to discard negligible imaginary components due to floating point error.\n    u1 = np.fft.ifft(u1_hat).real\n\n    # Step 2: Compute d^1 using the soft-shrinkage operator\n    # d^1 = shrink(Du^1, mu/lambda)\n\n    # Compute the discrete gradient Du^1 with periodic boundary conditions\n    # (Du)_i = u_{i+1} - u_i\n    v = np.roll(u1, -1) - u1\n\n    # Apply the soft-shrinkage operator\n    threshold = mu / lambda_param\n    d1 = np.sign(v) * np.maximum(np.abs(v) - threshold, 0.0)\n\n    return u1.tolist(), d1.tolist()\n\ndef solve():\n    \"\"\"\n    Runs the Split Bregman iteration for the test cases specified in the problem and prints the results.\n    \"\"\"\n    test_cases = [\n        # (n, g, mu, lambda)\n        (6, [1.0, 1.2, 0.9, -0.5, -0.4, 0.0], 0.6, 2.0),\n        (6, [2.0, 2.0, 2.0, 2.0, 2.0, 2.0], 1.5, 1.0),\n        (3, [0.0, 10.0, -10.0], 0.4, 0.5),\n        (8, [0.0, 0.5, 1.0, 1.5, 1.25, 0.8, 0.3, -0.2], 2.0, 3.0),\n    ]\n\n    results = []\n    for n, g_list, mu, lambda_param in test_cases:\n        g_np = np.array(g_list, dtype=np.float64)\n        u1, d1 = run_split_bregman_iteration(n, g_np, mu, lambda_param)\n        results.append([u1, d1])\n\n    # The output format requires a json-like string with no spaces.\n    # json.dumps with custom separators is the most reliable way to achieve this.\n    print(json.dumps(results, separators=(',', ':')))\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world inverse problems are often large-scale and ill-conditioned, posing significant computational challenges to the subproblems within each Bregman iteration. This final exercise shifts focus from basic implementation to the practicalities of efficient and robust computation. You will analyze strategies for preconditioning the linear systems that arise in ADMM/split Bregman and determine which approaches preserve the method's convergence guarantees, a critical consideration for developing production-grade solvers .",
            "id": "3369779",
            "problem": "Consider the convex inverse problem with a linear observation operator $A \\in \\mathbb{R}^{m \\times n}$, data $f \\in \\mathbb{R}^{m}$, a linear regularization operator $K \\in \\mathbb{R}^{p \\times n}$, and a proper, lower semicontinuous, convex functional $J:\\mathbb{R}^{p}\\to\\mathbb{R}\\cup\\{+\\infty\\}$. We seek $u \\in \\mathbb{R}^{n}$ that minimizes\n$$\n\\Phi(u) \\equiv \\tfrac{1}{2}\\|A u - f\\|_{2}^{2} + \\lambda\\, J(Ku)\n$$\nwith $\\lambda>0$. The split Bregman method, which is equivalent to the Alternating Direction Method of Multipliers (ADMM), introduces a splitting variable $d \\approx Ku$ and, at each outer iteration indexed by $k$, forms a $u$-subproblem that is a strongly convex quadratic in $u$ due to the quadratic data misfit and the quadratic penalty from the splitting. Assume that $A$ is ill-conditioned and that $J$ is possibly nonsmooth but convex, such as the $\\ell_{1}$-norm or total variation.\n\nYou are asked to decide which strategies below correctly precondition the $u$-subproblem and, at the same time, preserve the global convergence guarantees of the split Bregman or ADMM scheme under standard convexity assumptions. Your reasoning must start from the following foundational bases: the definition of the split Bregman or ADMM updates for linearly constrained convex problems, the fact that the $u$-subproblem in this setting is a strongly convex quadratic, the convergence of the conjugate gradient method for symmetric positive definite linear systems with a preconditioner that is symmetric positive definite, and the inexact ADMM principle that permits summably small errors in subproblem solves without destroying convergence.\n\nWhich of the following strategies both effectively precondition the $u$-subproblem when $A$ is ill-conditioned and preserve global convergence guarantees?\n\nA. Use a left preconditioner $P \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite and spectrally equivalent to the $u$-subproblem Hessian, and solve the $u$-subproblem linear system by preconditioned conjugate gradients in the $P$-inner product. Terminate the inner iteration at outer step $k$ when the preconditioned residual norm is reduced by a factor $\\eta_{k} \\in (0,1)$ relative to its initial value, with $(\\eta_{k})_{k\\geq 0}$ chosen so that $\\sum_{k=0}^{\\infty} \\eta_{k} < \\infty$.\n\nB. Replace the $u$-subproblem solve by a single unpreconditioned gradient descent step with a Barzilai–Borwein step size and no residual or error control, and proceed with the remaining split Bregman updates. The global convergence of split Bregman is claimed to be retained regardless of the conditioning of $A$.\n\nC. Add a proximal regularization term $\\tfrac{1}{2}\\|u-u^{k}\\|_{Q}^{2}$ with $Q \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite to the $u$-subproblem objective, and at each outer iteration compute $u^{k+1}$ by exactly minimizing this proximalized quadratic or, more generally, by an inexact step whose error is summable across outer iterations. Interpret this as a proximal point modification of ADMM in a weighted metric that preserves convergence.\n\nD. Use a non-symmetric or indefinite left preconditioner $P$ that accelerates the inner linear algebra in practice and apply the standard conjugate gradient method to the preconditioned normal equations for the $u$-subproblem. Rely on the empirical speedup to argue that the outer split Bregman iteration still enjoys the same convergence guarantees.\n\nSelect all that apply.",
            "solution": "The problem asks for an evaluation of strategies for preconditioning the $u$-subproblem within the split Bregman (or ADMM) algorithm for a class of convex inverse problems, ensuring that the global convergence guarantees of the method are preserved.\n\nFirst, let us formalize the algorithm and the subproblem in question. The optimization problem is to find $u \\in \\mathbb{R}^{n}$ that minimizes:\n$$\n\\Phi(u) \\equiv \\tfrac{1}{2}\\|A u - f\\|_{2}^{2} + \\lambda\\, J(Ku)\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $f \\in \\mathbb{R}^{m}$, $K \\in \\mathbb{R}^{p \\times n}$, $\\lambda > 0$, and $J$ is a proper, lower semicontinuous, convex functional.\n\nBy introducing a splitting variable $d = Ku$, we can rewrite the problem as a constrained optimization problem:\n$$\n\\min_{u, d} \\left\\{ \\tfrac{1}{2}\\|Au - f\\|_{2}^{2} + \\lambda J(d) \\right\\} \\quad \\text{subject to} \\quad Ku - d = 0.\n$$\nThe augmented Lagrangian for this problem, with a penalty parameter $\\beta > 0$ and a scaled dual variable (or Bregman variable) $b \\in \\mathbb{R}^{p}$, is:\n$$\nL_{\\beta}(u, d, b) = \\tfrac{1}{2}\\|Au - f\\|_{2}^{2} + \\lambda J(d) + b^T(Ku - d) + \\tfrac{\\beta}{2}\\|Ku - d\\|_{2}^{2}.\n$$\nThe ADMM or split Bregman iteration at step $k$ consists of the following updates:\n1.  **$u$-subproblem**:\n    $$ u^{k+1} = \\arg\\min_{u} L_{\\beta}(u, d^k, b^k) = \\arg\\min_{u} \\left\\{ \\tfrac{1}{2}\\|Au - f\\|_{2}^{2} + \\tfrac{\\beta}{2}\\|Ku - d^k + \\tfrac{1}{\\beta} b^k\\|_{2}^{2} \\right\\} $$\n2.  **$d$-subproblem**:\n    $$ d^{k+1} = \\arg\\min_{d} L_{\\beta}(u^{k+1}, d, b^k) = \\arg\\min_{d} \\left\\{ \\lambda J(d) + \\tfrac{\\beta}{2}\\|Ku^{k+1} - d + \\tfrac{1}{\\beta} b^k\\|_{2}^{2} \\right\\} $$\n3.  **Dual update**:\n    $$ b^{k+1} = b^k + \\beta(Ku^{k+1} - d^{k+1}) $$\nThe $u$-subproblem is a quadratic minimization. The objective is strongly convex, as stated in the problem. The first-order optimality condition is found by setting the gradient with respect to $u$ to zero:\n$$\nA^T(Au - f) + \\beta K^T(Ku - (d^k - \\tfrac{1}{\\beta}b^k)) = 0.\n$$\nThis is a linear system of equations of the form $H u = c^k$, where the Hessian matrix is\n$$\nH = A^T A + \\beta K^T K\n$$\nand the right-hand side is\n$$\nc^k = A^T f + \\beta K^T(d^k - \\tfrac{1}{\\beta}b^k).\n$$\nSince the $u$-subproblem objective is strongly convex, its Hessian $H$ is symmetric and positive definite (SPD).\nWhen $A$ is ill-conditioned, the matrix $A^TA$ is ill-conditioned, which in turn can make $H$ ill-conditioned. This slows down iterative solvers like the conjugate gradient (CG) method. The task is to evaluate preconditioning strategies for this linear system $Hu=c^k$ that preserve the global convergence of the outer ADMM iteration. The key principle for convergence is that inexact solutions of the subproblems are permissible, provided the errors are summable over the iterations. For the $u$-subproblem, if $\\tilde{u}^{k+1}$ is the approximate solution, a sufficient condition for convergence is that the norm of the gradient of the $u$-subproblem objective, evaluated at $\\tilde{u}^{k+1}$, forms a summable sequence. This gradient is equal to $H\\tilde{u}^{k+1} - c^k$, which is the residual of the linear system. Let this residual be $r^{k+1}$. The condition is $\\sum_{k=0}^{\\infty} \\|r^{k+1}\\| < \\infty$.\n\nNow, we analyze each option.\n\n**A. Use a left preconditioner $P \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite and spectrally equivalent to the $u$-subproblem Hessian, and solve the $u$-subproblem linear system by preconditioned conjugate gradients in the $P$-inner product. Terminate the inner iteration at outer step $k$ when the preconditioned residual norm is reduced by a factor $\\eta_{k} \\in (0,1)$ relative to its initial value, with $(\\eta_{k})_{k\\geq 0}$ chosen so that $\\sum_{k=0}^{\\infty} \\eta_{k} < \\infty$.**\n\nThis strategy is sound.\n1.  **Effective Preconditioning**: Using an SPD preconditioner $P$ that is spectrally equivalent to the SPD Hessian $H$ is the canonical approach for accelerating the CG method. Preconditioned Conjugate Gradients (PCG) is the appropriate algorithm. Spectral equivalence ensures that the condition number of the preconditioned system is bounded, leading to a number of iterations that is independent of the problem size and conditioning. Thus, the preconditioning is effective.\n2.  **Convergence Preservation**: The stopping criterion for the inner PCG iteration is based on the preconditioned residual norm. Let $r_j$ be the residual at PCG step $j$. The criterion is $\\|r_{final}\\|_{P^{-1}} \\le \\eta_k \\|r_{initial}\\|_{P^{-1}}$. The $P^{-1}$-norm is defined as $\\|v\\|_{P^{-1}} = \\sqrt{v^T P^{-1} v}$. Since $P$ is SPD, so is $P^{-1}$, and the $P^{-1}$-norm is equivalent to the standard Euclidean $l_2$-norm. That is, there exist constants $C_1, C_2 > 0$ such that $C_1\\|v\\|_2 \\le \\|v\\|_{P^{-1}} \\le C_2\\|v\\|_2$ for all $v$. The constants depend on the eigenvalues of $P^{-1}$. Therefore, the stopping criterion implies $\\|r_{final}\\|_2 \\le \\frac{C_2}{C_1} \\eta_k \\|r_{initial}\\|_2$. The initial residual $r_{initial}$ is computed using the state of the algorithm at the beginning of the subproblem solve, which is bounded under standard ADMM assumptions. Let's say $\\|r_{initial}\\|_2 \\le M$ for all $k$. Then the final residual for the $u$-subproblem at outer step $k+1$, let's call it $r_{u}^{k+1}$, satisfies $\\|r_{u}^{k+1}\\|_2 \\le C' \\eta_k$ for some constant $C'$. The condition $\\sum_{k=0}^{\\infty} \\eta_k < \\infty$ then implies $\\sum_{k=0}^{\\infty} \\|r_{u}^{k+1}\\|_2 < \\infty$. This is a standard summable error condition for inexact ADMM, which guarantees global convergence.\n\n**Verdict: Correct.**\n\n**B. Replace the $u$-subproblem solve by a single unpreconditioned gradient descent step with a Barzilai–Borwein step size and no residual or error control, and proceed with the remaining split Bregman updates. The global convergence of split Bregman is claimed to be retained regardless of the conditioning of $A$.**\n\nThis strategy is flawed.\n1.  **Effective Preconditioning**: A single step of gradient descent, regardless of the step size strategy (including Barzilai-Borwein), is generally insufficient to accurately solve an ill-conditioned linear system. Gradient descent is known to converge slowly for ill-conditioned problems, and a single step will make minimal progress toward the true minimizer of the subproblem. This is not an effective way to handle the ill-conditioning.\n2.  **Convergence Preservation**: The inexact ADMM framework requires the errors in the subproblem solutions to be controlled and diminish over time (e.g., be summable). A single gradient step with no error control provides no such guarantee. The error with respect to the true subproblem solution will generally be large and not decrease in a way that satisfies the summable error condition. The claim that convergence is retained \"regardless of the conditioning of $A$\" is particularly strong and incorrect. For severely ill-conditioned problems, the error from a single gradient step would be substantial, leading to a violation of the conditions for inexact ADMM and likely causing the outer iteration to diverge.\n\n**Verdict: Incorrect.**\n\n**C. Add a proximal regularization term $\\tfrac{1}{2}\\|u-u^{k}\\|_{Q}^{2}$ with $Q \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite to the $u$-subproblem objective, and at each outer iteration compute $u^{k+1}$ by exactly minimizing this proximalized quadratic or, more generally, by an inexact step whose error is summable across outer iterations. Interpret this as a proximal point modification of ADMM in a weighted metric that preserves convergence.**\n\nThis strategy is sound and well-established.\n1.  **Effective Preconditioning**: The modified $u$-subproblem objective is $\\tfrac{1}{2}\\|Au-f\\|_2^2 + \\tfrac{\\beta}{2}\\|Ku - (d^k - \\tfrac{1}{\\beta}b^k)\\|_2^2 + \\tfrac{1}{2}\\|u-u^k\\|_Q^2$. The Hessian of this new objective is $H_{prox} = A^TA + \\beta K^TK + Q = H+Q$. Since $Q$ is SPD and $H$ is SPD (or at least PSD), $H+Q$ is also SPD. By choosing $Q$ appropriately, one can significantly improve the condition number of the Hessian. For example, if $Q = \\tau I - H$ for a sufficiently large $\\tau > 0$ (such that $Q$ is SPD), the new Hessian becomes $\\tau I$, which is perfectly conditioned. This is the essence of linearized ADMM. More generally, choosing $Q$ to be a well-structured and invertible approximation to $H$ is a form of preconditioning. So, this strategy is an effective way to deal with the ill-conditioned subproblem.\n2.  **Convergence Preservation**: Adding a proximal term $\\tfrac{1}{2}\\|u-u^k\\|_Q^2$ to the $u$-subproblem is a known technique called Proximal ADMM. The convergence of this class of methods is well-documented in optimization literature. It is indeed a valid interpretation that this is a proximal point method applied to the ADMM iteration, which is known to be robust and convergence-preserving. The statement allows for an inexact solve of this modified subproblem with summable errors, which fits squarely within the established theory of inexact proximal ADMM.\n\n**Verdict: Correct.**\n\n**D. Use a non-symmetric or indefinite left preconditioner $P$ that accelerates the inner linear algebra in practice and apply the standard conjugate gradient method to the preconditioned normal equations for the $u$-subproblem. Rely on the empirical speedup to argue that the outer split Bregman iteration still enjoys the same convergence guarantees.**\n\nThis strategy is fundamentally flawed.\n1.  **Methodological Flaw**: The standard Conjugate Gradient (CG) method is specifically designed for systems of linear equations with a symmetric positive definite (SPD) matrix. Applying it to a non-symmetric or indefinite system is incorrect. If the preconditioner $P$ is non-symmetric, the preconditioned matrix $P^{-1}H$ is also non-symmetric, and CG cannot be applied. One would need to use a different iterative solver, like GMRES or BiCGSTAB. The suggestion to apply CG to the \"preconditioned normal equations\" is also problematic, as this would involve forming a matrix like $(P^{-1}H)^T(P^{-1}H)$, which squares the condition number and is generally avoided. The description of the numerical linear algebra method is incorrect.\n2.  **Justification Flaw**: Relying on \"empirical speedup\" to justify a claim about \"convergence guarantees\" is not scientifically rigorous. Mathematical convergence proofs are required. The use of an inappropriate numerical method (CG for non-SPD systems) means there is no control over the residual or the error in the subproblem solution. Without this control, it is impossible to verify that the summable error condition for inexact ADMM is met. Empirical performance in some test cases does not constitute a proof of convergence for a general class of problems.\n\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}