{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration of FISTA, we first focus on its most critical component: the proximal operator. The proximal operator for the $\\ell_1$-norm, also known as the soft-thresholding operator, is the engine that drives sparsity in the solution. This exercise () isolates this key step, allowing you to compute it directly and build an intuition for how values are \"shrunk\" towards zero, effectively performing feature selection.",
            "id": "3446917",
            "problem": "Consider the composite convex optimization problem of minimizing the function $g(x) = f(x) + \\lambda \\|x\\|_{1}$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $\\|x\\|_{1} = \\sum_{i} |x_{i}|$. In the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), one iteration uses the proximal mapping of the scaled $\\ell_{1}$-norm. Assume a current gradient step has produced the vector $z \\in \\mathbb{R}^{7}$, and you are to compute the proximal point\n$$\nx = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z),\n$$\nwhich is defined as the unique minimizer of the function\n$$\nx \\mapsto \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2}.\n$$\nLet the given data be\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right), \\quad \\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}.\n$$\nCompute the exact vector $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$ in simplest rational form. Provide your final answer as a single row vector. Do not approximate; no rounding is required.",
            "solution": "The problem asks for the computation of the proximal point $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$, which is defined as the unique minimizer of the optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{7}} \\left( \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right)\n$$\nThe objective function can be written in terms of the components of the vectors $x = (x_1, \\dots, x_7)$ and $z = (z_1, \\dots, z_7)$:\n$$\n\\min_{x_1, \\dots, x_7} \\left( \\tau \\lambda \\sum_{i=1}^{7} |x_i| + \\frac{1}{2} \\sum_{i=1}^{7} (x_i - z_i)^2 \\right)\n$$\nThis objective function is separable, meaning it can be decomposed into a sum of functions, each depending on only one component $x_i$. Therefore, we can minimize the function with respect to each component $x_i$ independently:\n$$\nx_i = \\arg\\min_{u \\in \\mathbb{R}} \\left( \\tau \\lambda |u| + \\frac{1}{2}(u - z_i)^2 \\right) \\quad \\text{for } i = 1, \\dots, 7\n$$\nThis is the well-known soft-thresholding operator. Let $\\alpha = \\tau\\lambda$. The solution to the one-dimensional problem\n$$\n\\min_{u} \\left( \\alpha|u| + \\frac{1}{2}(u - v)^2 \\right)\n$$\nis given by the soft-thresholding function $S_{\\alpha}(v)$, which has the closed-form solution:\n$$\nS_{\\alpha}(v) = \\operatorname{sign}(v) \\max(|v| - \\alpha, 0)\n$$\nThis can be expressed piecewise as:\n$$\nS_{\\alpha}(v) = \\begin{cases} v - \\alpha & \\text{if } v > \\alpha \\\\ 0 & \\text{if } |v| \\le \\alpha \\\\ v + \\alpha & \\text{if } v < -\\alpha \\end{cases}\n$$\nFirst, we must calculate the threshold parameter $\\alpha$ using the given values for $\\lambda$ and $\\tau$:\n$$\n\\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}\n$$\nThe threshold $\\alpha$ is:\n$$\n\\alpha = \\tau \\lambda = \\left(\\frac{2}{3}\\right) \\left(\\frac{3}{5}\\right) = \\frac{6}{15} = \\frac{2}{5}\n$$\nNow, we apply the soft-thresholding operator $S_{2/5}$ to each component of the vector $z$:\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right)\n$$\nLet's compute each component $x_i = S_{2/5}(z_i)$:\n\nFor $i=1$: $z_1 = 1$. The threshold is $\\alpha = \\frac{2}{5}$. Since $z_1 = 1 > \\frac{2}{5}$, we have:\n$x_1 = z_1 - \\alpha = 1 - \\frac{2}{5} = \\frac{5}{5} - \\frac{2}{5} = \\frac{3}{5}$.\n\nFor $i=2$: $z_2 = -\\frac{1}{4}$. We compare $|z_2| = \\frac{1}{4}$ to $\\alpha = \\frac{2}{5}$. Converting to a common denominator of $20$, we have $\\frac{1}{4} = \\frac{5}{20}$ and $\\frac{2}{5} = \\frac{8}{20}$. Since $\\frac{5}{20} < \\frac{8}{20}$, $|z_2| < \\alpha$. Thus:\n$x_2 = 0$.\n\nFor $i=3$: $z_3 = \\frac{2}{5}$. We have $|z_3| = \\frac{2}{5}$, which is equal to the threshold $\\alpha = \\frac{2}{5}$. Since $|z_3| \\le \\alpha$:\n$x_3 = 0$.\n\nFor $i=4$: $z_4 = -\\frac{2}{5}$. We have $|z_4| = \\frac{2}{5}$, which is equal to the threshold $\\alpha = \\frac{2}{5}$. Since $|z_4| \\le \\alpha$:\n$x_4 = 0$.\n\nFor $i=5$: $z_5 = \\frac{7}{15}$. We compare $z_5$ to $\\alpha = \\frac{2}{5}$. Converting to a common denominator of $15$, we have $\\frac{2}{5} = \\frac{6}{15}$. Since $z_5 = \\frac{7}{15} > \\frac{6}{15} = \\alpha$, we have:\n$x_5 = z_5 - \\alpha = \\frac{7}{15} - \\frac{2}{5} = \\frac{7}{15} - \\frac{6}{15} = \\frac{1}{15}$.\n\nFor $i=6$: $z_6 = -\\frac{9}{10}$. We compare $z_6$ to $-\\alpha = -\\frac{2}{5}$. Converting to a common denominator of $10$, we have $-\\frac{2}{5} = -\\frac{4}{10}$. Since $z_6 = -\\frac{9}{10} < -\\frac{4}{10} = -\\alpha$, we have:\n$x_6 = z_6 + \\alpha = -\\frac{9}{10} + \\frac{2}{5} = -\\frac{9}{10} + \\frac{4}{10} = -\\frac{5}{10} = -\\frac{1}{2}$.\n\nFor $i=7$: $z_7 = 0$. We have $|z_7| = 0$, and $0 \\le \\frac{2}{5} = \\alpha$. Thus:\n$x_7 = 0$.\n\nCombining these components, we obtain the resulting vector $x$:\n$$\nx = \\left(\\frac{3}{5}, 0, 0, 0, \\frac{1}{15}, -\\frac{1}{2}, 0\\right)\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{5} & 0 & 0 & 0 & \\frac{1}{15} & -\\frac{1}{2} & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the core proximal mapping, we now assemble it into a complete FISTA iteration. This practice () guides you through a single cycle of the algorithm, from Nesterov's acceleration step to the final proximal update. By manually tracing the flow of information—calculating the extrapolated point $y^k$, the gradient $\\nabla g(y^k)$, and the next iterate $x^{k+1}$—you will gain a procedural understanding of how FISTA's components interlink to accelerate convergence.",
            "id": "3446895",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in one dimension, where the objective is to minimize the composite function $f(x) = g(x) + h(x)$ with $g(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$ and $h(x) = \\lambda |x|$. Let the measurement matrix be scalar $A = a$ with $a = 2$, let $b = 3$, and let $\\lambda = 1$. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) initializes with iterate values $x^{k-1} = 0$, $x^{k} = 1$, and acceleration parameter $t_{k} = 2$. Use the following scientifically grounded bases:\n\n- The gradient of the smooth term $g$ is defined as $\\nabla g(x) = A^{\\top}(Ax - b)$.\n- The Lipschitz constant $L$ of the gradient $\\nabla g$ is the squared spectral norm of $A$, which in one dimension is $L = a^{2}$.\n- The proximal operator of $h$ with parameter $\\tau > 0$ is defined by $\\operatorname{prox}_{\\tau h}(z) = \\arg\\min_{x}\\left\\{\\frac{1}{2}(x - z)^{2} + \\tau \\lambda |x|\\right\\}$.\n- The canonical Nesterov acceleration sequence for FISTA satisfies the implicit relation $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$.\n\nImplement one full FISTA iteration for this LASSO instance as follows: first determine $t_{k+1}$ from the implicit relation above; then form the extrapolated point $y^{k}$ using $x^{k}$, $x^{k-1}$, $t_{k}$, and $t_{k+1}$; compute the gradient $\\nabla g(y^{k})$; perform the gradient step $z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k})$; and apply the proximal operator with parameter $\\tau = \\frac{1}{L}$ to obtain $x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k})$. Express your final results exactly (no rounding) for the four quantities in the order $y^{k}$, $\\nabla g(y^{k})$, $x^{k+1}$, and $t_{k+1}$.",
            "solution": "The objective is to perform one full iteration of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). We follow the prescribed steps using the given initial values and parameters.\n\nFirst, we compute the Lipschitz constant $L$ of the gradient of the smooth term $g(x) = \\frac{1}{2}(ax-b)^2$. The gradient is $\\nabla g(x) = a(ax-b)$, and its derivative is $a^2$. Thus, the Lipschitz constant is:\n$$L = a^2 = 2^2 = 4$$\nThe step size used in the gradient and proximal steps will be $\\frac{1}{L} = \\frac{1}{4}$.\n\nThe FISTA iteration proceeds as follows:\n\n**1. Determine the next acceleration parameter $t_{k+1}$**\nUsing the given update rule $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$ with $t_k = 2$:\n$$t_{k+1}^{2} - t_{k+1} = 2^2 = 4 \\implies t_{k+1}^{2} - t_{k+1} - 4 = 0$$\nApplying the quadratic formula and taking the positive root (since $t_k > 0$ for all $k$):\n$$t_{k+1} = \\frac{1 + \\sqrt{(-1)^2 - 4(1)(-4)}}{2} = \\frac{1 + \\sqrt{17}}{2}$$\n\n**2. Form the extrapolated point $y^k$**\nThe extrapolation step uses the formula $y^{k} = x^{k} + \\frac{t_{k}-1}{t_{k+1}}(x^{k}-x^{k-1})$. With $x^k=1$, $x^{k-1}=0$, and $t_k=2$:\n$$y^{k} = 1 + \\frac{2-1}{\\frac{1 + \\sqrt{17}}{2}}(1 - 0) = 1 + \\frac{2}{1 + \\sqrt{17}}$$\nRationalizing the denominator gives:\n$$y^{k} = 1 + \\frac{2(1 - \\sqrt{17})}{(1 + \\sqrt{17})(1 - \\sqrt{17})} = 1 + \\frac{2(1 - \\sqrt{17})}{-16} = 1 + \\frac{\\sqrt{17} - 1}{8} = \\frac{7 + \\sqrt{17}}{8}$$\n\n**3. Compute the gradient $\\nabla g(y^k)$**\nThe gradient is $\\nabla g(x) = a(ax-b) = 2(2x-3)$. Evaluating at $y^k$:\n$$\\nabla g(y^{k}) = 2\\left(2\\left(\\frac{7 + \\sqrt{17}}{8}\\right) - 3\\right) = 2\\left(\\frac{7 + \\sqrt{17}}{4} - \\frac{12}{4}\\right) = 2\\left(\\frac{\\sqrt{17} - 5}{4}\\right) = \\frac{\\sqrt{17} - 5}{2}$$\n\n**4. Perform the proximal gradient step**\nFirst, take a gradient descent step from $y^k$ to get an intermediate point $z^k$:\n$$z^k = y^{k} - \\frac{1}{L}\\nabla g(y^{k}) = \\frac{7 + \\sqrt{17}}{8} - \\frac{1}{4}\\left(\\frac{\\sqrt{17} - 5}{2}\\right) = \\frac{7 + \\sqrt{17}}{8} - \\frac{\\sqrt{17} - 5}{8} = \\frac{12}{8} = \\frac{3}{2}$$\nNext, apply the proximal operator (soft-thresholding) to $z^k$ to get the new iterate $x^{k+1}$. The threshold is $\\frac{\\lambda}{L} = \\frac{1}{4}$.\n$$x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k}) = \\operatorname{sign}(z^k) \\max\\left(|z^k| - \\frac{\\lambda}{L}, 0\\right)$$\n$$x^{k+1} = \\operatorname{sign}\\left(\\frac{3}{2}\\right) \\max\\left(\\left|\\frac{3}{2}\\right| - \\frac{1}{4}, 0\\right) = \\frac{3}{2} - \\frac{1}{4} = \\frac{5}{4}$$\n\nThe four requested quantities are:\n- $y^{k} = \\frac{7 + \\sqrt{17}}{8}$\n- $\\nabla g(y^{k}) = \\frac{\\sqrt{17} - 5}{2}$\n- $x^{k+1} = \\frac{5}{4}$\n- $t_{k+1} = \\frac{1 + \\sqrt{17}}{2}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 + \\sqrt{17}}{8} & \\frac{\\sqrt{17} - 5}{2} & \\frac{5}{4} & \\frac{1 + \\sqrt{17}}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The final step in our hands-on journey is to transition from manual calculation to a practical, coded implementation. A critical challenge in applying FISTA is that the ideal step size, determined by the Lipschitz constant $L$, is often unknown. This exercise () tasks you with implementing FISTA with a backtracking line search, an adaptive strategy to find a suitable step size at each iteration. By writing the code and running it on test cases, you will gain direct experience with the logic that ensures robust convergence and bridge the gap between abstract algorithm design and concrete numerical problem-solving.",
            "id": "3446933",
            "problem": "You are asked to implement a numerical method grounded in convex analysis to solve a sequence of synthetic sparse reconstruction instances. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\equiv g(x) + h(x),\n$$\nwhere the smooth part is\n$$\ng(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2\n$$\nwith gradient\n$$\n\\nabla g(x) = A^\\top (A x - b),\n$$\nand the nonsmooth regularizer is the scaled one-norm\n$$\nh(x) \\equiv \\lambda \\|x\\|_1.\n$$\nThe gradient of $g$ is globally Lipschitz continuous with some constant $L^\\star \\in (0,\\infty)$ satisfying\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L^\\star \\|x - y\\|_2\n$$\nfor all $x,y \\in \\mathbb{R}^n$. In this setting, the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with backtracking uses the following fundamental base:\n- The quadratic upper bound implied by Lipschitz continuity of $\\nabla g$ that justifies a sufficient decrease condition for accepting a candidate step size.\n- The proximal operator of the scaled one-norm,\n$$\n\\operatorname{prox}_{\\tau \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^n}\\left\\{\\tfrac{1}{2}\\|x - v\\|_2^2 + \\tau \\|x\\|_1\\right\\},\n$$\nwhich is the soft-thresholding map.\n- Nesterov’s acceleration sequence with $t_1 = 1$ and extrapolation built from $t_k$.\n\nYour task is to:\n- Start from the above base and derive the sufficient decrease condition for backtracking from the Lipschitz property of $\\nabla g$ to validate a local quadratic upper model of $g$ about the current extrapolated point.\n- Implement the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with backtracking for the LASSO objective $F(x)$, assuming the Lipschitz constant $L^\\star$ is unknown. Initialize with $x_1 = 0$, $y_1 = x_1$, and $t_1 = 1$. At each outer iteration $k \\in \\{1,2,\\dots\\}$, perform a backtracking line search that starts from a current estimate $L_k$ and multiplies $L_k$ by a factor $\\eta > 1$ until the sufficient decrease condition holds. Use the proximal map associated with $h$ to form the proximal-gradient candidate from the extrapolated point and the current $L_k$. Maintain the standard FISTA acceleration update using $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_k^2}}{2}$ and the usual extrapolation.\n- Let an “increase of $L_k$” be counted each time the backtracking line search multiplies $L_k$ by $\\eta$ before an iteration $k$ is accepted. Record the total number of such increases that occur during the first $10$ outer iterations (i.e., for $k \\in \\{1,\\dots,10\\}$).\n\nNo physical units are involved. All angles, if any, are to be assumed in radians. All answers must be real-valued numbers.\n\nTest suite and required output:\nImplement your program to run the following three synthetic test cases. In each case, form $b$ from a planted vector $x_{\\mathrm{true}}$ via $b = A x_{\\mathrm{true}}$ (no noise). For each case, run FISTA with backtracking for exactly $10$ outer iterations and report the total count of increases of $L_k$ that occurred across these $10$ iterations.\n\n- Case $1$ (happy path, moderate backtracking):\n  - $A \\in \\mathbb{R}^{6 \\times 10}$:\n    $$\n    A = \\begin{bmatrix}\n    1 & 0 & 0 & 0 & 0 & 0 & 0.5 & -0.2 & 0.3 & 0.1 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & -0.1 & 0.4 & 0.2 & -0.3 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0.3 & -0.4 & 0.1 & 0.2 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & -0.2 & 0.1 & 0.5 & -0.1 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0.4 & 0.3 & -0.2 & 0.2 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & -0.3 & 0.2 & 0.1 & 0.4\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{10}$ with entries\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0 & 1.2 & 0 & -0.7 & 0 & 0 & 0 & 0 & 2.0 & 0 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$, initial $L_1 = 0.1$, and backtracking factor $\\eta = 2.0$.\n\n- Case $2$ (boundary case, no backtracking needed):\n  - Same $A$ and $x_{\\mathrm{true}}$ as in Case $1$, with $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$, initial $L_1 = 100.0$, and backtracking factor $\\eta = 2.0$.\n\n- Case $3$ (edge case, many small increases due to a small $\\eta$ and larger operator norm):\n  - $A \\in \\mathbb{R}^{8 \\times 12}$:\n    $$\n    A = \\begin{bmatrix}\n    3.0 & -2.0 & 1.0 & 0.0 & 1.0 & -1.0 & 2.0 & -3.0 & 1.5 & -0.5 & 0.5 & -1.5 \\\\\n    0.0 & 1.0 & -1.0 & 2.0 & -2.0 & 1.0 & -1.5 & 2.5 & -1.0 & 0.5 & -0.5 & 1.0 \\\\\n    1.5 & -1.0 & 0.5 & -0.5 & 1.0 & -1.5 & 2.0 & -2.0 & 1.0 & -1.0 & 0.5 & -0.5 \\\\\n    -1.0 & 2.0 & -2.0 & 1.0 & 0.0 & 1.0 & -2.5 & 3.0 & -1.5 & 1.0 & -1.0 & 0.5 \\\\\n    2.0 & -1.0 & 1.5 & -1.0 & 2.0 & -2.0 & 1.0 & -1.0 & 0.5 & -0.5 & 1.0 & -1.5 \\\\\n    -2.0 & 1.0 & -0.5 & 1.5 & -1.0 & 2.0 & -1.0 & 1.0 & -0.5 & 0.5 & -1.0 & 1.0 \\\\\n    1.0 & 0.5 & -1.5 & 2.0 & -2.5 & 1.5 & -1.0 & 2.0 & -1.0 & 0.5 & -0.5 & 1.0 \\\\\n    -1.5 & 2.5 & -1.0 & 0.5 & 1.0 & -1.0 & 2.0 & -3.0 & 1.5 & -1.0 & 1.0 & -0.5\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{12}$ with entries\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0 & 0 & 2.0 & -1.0 & 0 & 0 & 0 & 3.0 & 0 & 0 & 0 & 0.5 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.1$, initial $L_1 = 1.0$, and backtracking factor $\\eta = 1.1$.\n\nFor each case, run exactly $10$ outer iterations of FISTA with backtracking and count the total number of times that $L_k$ is multiplied by $\\eta$ across these $10$ iterations. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$). Each result must be an integer representing the count for its corresponding test case. No additional text should be printed.",
            "solution": "The problem requires the implementation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with a backtracking line search to solve the LASSO optimization problem. The primary task is to count the number of times the Lipschitz constant estimate is increased during the backtracking procedure over a fixed number of iterations.\n\nThe LASSO objective function is given by $F(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ is the smooth data fidelity term and $h(x) = \\lambda \\|x\\|_1$ is the non-smooth sparsity-inducing regularizer.\n\nFirst, we derive the sufficient decrease condition used in the backtracking line search. The gradient of the smooth term is $\\nabla g(x) = A^\\top (A x - b)$. It is given that $\\nabla g$ is Lipschitz continuous with constant $L^\\star$, which implies the following inequality for any $L \\ge L^\\star$:\n$$\ng(z) \\le g(y) + \\langle \\nabla g(y), z - y \\rangle + \\frac{L}{2} \\|z - y\\|_2^2\n$$\nThis inequality, known as the Descent Lemma, provides a quadratic upper bound for the function $g$ at a point $z$ around a point $y$.\n\nFISTA is a proximal gradient method that combines a standard proximal gradient step with a Nesterov-style acceleration. At each iteration $k$, given an extrapolated point $y_k$, the algorithm seeks to find a new point $x_k$ by minimizing a quadratic approximation of $F(x)$ around $y_k$. This minimization leads to the proximal gradient update:\n$$\nx_k = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))\n$$\nwhere $L$ is the Lipschitz constant estimate and $\\operatorname{prox}_{\\tau h}(\\cdot)$ is the proximal operator of the function $\\tau h$. For $h(x) = \\lambda \\|x\\|_1$, the proximal step involves the soft-thresholding operator, $S_{\\tau}(v)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$, such that:\n$$\nx_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b)\\right)\n$$\nSince the true Lipschitz constant $L^\\star$ is unknown, a backtracking line search is employed to find a suitable step size $1/L_k$ at each iteration $k$. Starting with an estimate for $L_k$ (e.g., the value from the previous iteration, $L_{k-1}$), we check if the quadratic upper bound holds for our candidate point $x_k$. The sufficient decrease condition is derived directly from the Descent Lemma by substituting $z=x_k$ and $y=y_k$. We require our chosen $L_k$ to satisfy:\n$$\ng(x_k) \\le g(y_k) + \\langle \\nabla g(y_k), x_k - y_k \\rangle + \\frac{L_k}{2} \\|x_k - y_k\\|_2^2\n$$\nIf this condition is not met, the estimate $L_k$ is too small. We increase it by a factor $\\eta > 1$ (i.e., $L_k \\leftarrow \\eta L_k$), recompute the candidate $x_k$ with the new $L_k$, and check the condition again. This process is repeated until the condition is satisfied. Each multiplication by $\\eta$ constitutes an \"increase\" that we must count.\n\nThe complete FISTA with backtracking algorithm proceeds as follows:\n\n1.  **Initialization**: Given an initial guess for the Lipschitz constant $L_0$, backtracking factor $\\eta > 1$. Set $x_0 = 0$, $y_1 = x_0$, and $t_1 = 1$. Let the total count of increases be $C = 0$.\n\n2.  **Iteration**: For $k = 1, 2, \\ldots, 10$:\n    a. **Backtracking Line Search**:\n        i. Start with a trial Lipschitz constant, e.g., $L_{trial} = L_{k-1}$.\n        ii. Compute the candidate point $x_{k, trial} = S_{\\lambda/L_{trial}}\\left(y_k - \\frac{1}{L_{trial}}\\nabla g(y_k)\\right)$.\n        iii. Check the sufficient decrease condition: if $g(x_{k, trial}) > g(y_k) + \\langle \\nabla g(y_k), x_{k, trial} - y_k \\rangle + \\frac{L_{trial}}{2} \\|x_{k, trial} - y_k\\|_2^2$, then update $L_{trial} \\leftarrow \\eta L_{trial}$, increment the counter $C \\leftarrow C+1$, and go back to step (ii).\n        iv. Once the condition is satisfied, set $L_k = L_{trial}$ and $x_k = x_{k, trial}$.\n\n    b. **Acceleration Step**: Update the momentum terms:\n        i. $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$.\n        ii. $y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1})$.\n\n3.  After $10$ iterations, the final value of the counter $C$ is the result for the given test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs FISTA with backtracking on a suite of test cases and reports the\n    total number of backtracking steps (increases of L).\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 0.1,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 100.0,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A\": np.array([\n                [3.0, -2.0, 1.0, 0.0, 1.0, -1.0, 2.0, -3.0, 1.5, -0.5, 0.5, -1.5],\n                [0.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.5, 2.5, -1.0, 0.5, -0.5, 1.0],\n                [1.5, -1.0, 0.5, -0.5, 1.0, -1.5, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5],\n                [-1.0, 2.0, -2.0, 1.0, 0.0, 1.0, -2.5, 3.0, -1.5, 1.0, -1.0, 0.5],\n                [2.0, -1.0, 1.5, -1.0, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5, 1.0, -1.5],\n                [-2.0, 1.0, -0.5, 1.5, -1.0, 2.0, -1.0, 1.0, -0.5, 0.5, -1.0, 1.0],\n                [1.0, 0.5, -1.5, 2.0, -2.5, 1.5, -1.0, 2.0, -1.0, 0.5, -0.5, 1.0],\n                [-1.5, 2.5, -1.0, 0.5, 1.0, -1.0, 2.0, -3.0, 1.5, -1.0, 1.0, -0.5]\n            ]),\n            \"x_true\": np.array([0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.5]),\n            \"lam\": 0.1,\n            \"L_initial\": 1.0,\n            \"eta\": 1.1\n        }\n    ]\n\n    results = []\n\n    def soft_threshold(v, tau):\n        return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        x_true = case[\"x_true\"]\n        lam = case[\"lam\"]\n        L_initial = case[\"L_initial\"]\n        eta = case[\"eta\"]\n        n_iters = 10\n\n        b = A @ x_true\n        n = A.shape[1]\n        \n        # Following standard FISTA notation: x_0, y_1, t_1\n        x_k = np.zeros(n)      # Corresponds to x_{k-1} in iteration k\n        x_km1 = np.zeros(n)    # Corresponds to x_{k-2} in iteration k\n        y_k = x_k              # y_1 = x_0\n        t_k = 1.0              # t_1\n        L = L_initial          # L_0\n        total_increases = 0\n\n        # Loop for k = 1, ..., 10\n        for _ in range(1, n_iters + 1):\n            L_inner = L\n            while True:\n                grad_g_y = A.T @ (A @ y_k - b)\n                x_k_candidate = soft_threshold(y_k - (1.0 / L_inner) * grad_g_y, lam / L_inner)\n\n                g_x = 0.5 * np.linalg.norm(A @ x_k_candidate - b)**2\n                g_y = 0.5 * np.linalg.norm(A @ y_k - b)**2\n                rhs = g_y + np.dot(grad_g_y, x_k_candidate - y_k) + \\\n                      (L_inner / 2.0) * np.linalg.norm(x_k_candidate - y_k)**2\n\n                if g_x <= rhs:\n                    L = L_inner\n                    break\n                else:\n                    L_inner *= eta\n                    total_increases += 1\n            \n            # Found a valid L and x_k_candidate (which is now x_k)\n            x_km1 = x_k\n            x_k = x_k_candidate\n            \n            # Update momentum terms\n            t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n            y_kp1 = x_k + ((t_k - 1.0) / t_kp1) * (x_k - x_km1)\n            \n            # Update state for next iteration\n            t_k = t_kp1\n            y_k = y_kp1\n\n        results.append(total_increases)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}