{
    "hands_on_practices": [
        {
            "introduction": "原始-对偶混合梯度（PDHG）算法的收敛性关键取决于原始步长 $\\tau$ 和对偶步长 $\\sigma$ 的选择。一个标准的稳定性条件是 $\\tau\\sigma\\|K\\|^2  1$，其中 $\\|K\\|$ 是线性算子 $K$ 的谱范数。本练习将指导你完成一个基本的数值计算过程——幂迭代法——来估计 $\\|K\\|$ 。掌握这一方法使你能够选择有效的步长，这是实现一个稳定的PDHG求解器的首要且至关重要的一步。",
            "id": "3413760",
            "problem": "考虑一个由实矩阵 $K \\in \\mathbb{R}^{m \\times n}$ 表示的线性正算子。在用于逆问题和数据同化的原始-对偶混合梯度 (PDHG) 方法中，原始步长 $ \\tau $ 和对偶步长 $ \\sigma $ 必须满足稳定性条件 $ \\tau \\sigma \\|K\\|^2  1 $，其中 $ \\|K\\| $ 表示由欧几里得范数（即谱范数）诱导的算子范数。从定义 $ \\|K\\| = \\sup_{\\|x\\|_2 = 1} \\|Kx\\|_2 $ 以及 $ \\|K\\| $ 与 $K$ 的最大奇异值之间的关系出发，推导一个仅使用与 $K$ 和 $K^\\top$ 的矩阵-向量乘积来估计通用（可能为矩形）矩阵的 $ \\|K\\| $ 的过程。使用此估计值来选择 $ \\tau $ 和 $ \\sigma $，使得 $ \\tau \\sigma \\|K\\|^2  1 $ 严格成立，并带有一个可调的安全因子 $ s \\in (0,1) $，该因子控制 $ \\tau \\sigma \\|K\\|^2 $ 与 $ 1 $ 的接近程度。为确保在退化情况下的行为有明确定义，采用惯例：如果 $ \\|K\\| $ 的估计值在容差范围内数值上为零，则设置 $ \\tau = 1 $ 和 $ \\sigma = 1 $。\n\n您的任务是实现一个完整的程序，该程序：\n- 确定性地将单位向量 $ v_0 \\in \\mathbb{R}^n $ 初始化为归一化的全一向量。\n- 对称半正定矩阵 $ K^\\top K $ 应用幂迭代法，仅使用 $ v \\mapsto Kv $ 和 $ u \\mapsto K^\\top u $ 操作，从而估计 $ K^\\top K $ 的最大特征值，并因此得到 $ \\|K\\| = \\sqrt{\\lambda_{\\max}(K^\\top K)} $。\n- 使用估计值 $ \\widehat{\\|K\\|} $，根据保证 $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $（带有提供的安全因子 $ s \\in (0,1) $）的规则，以及在 $ \\widehat{\\|K\\|} $ 数值上为零时的特殊情况规则 $ \\tau = \\sigma = 1 $ 来选择 $ \\tau $ 和 $ \\sigma $。\n- 对每个测试用例，报告一个包含五个条目的列表：估计的谱范数 $ \\widehat{\\|K\\|} $（浮点数），选择的 $ \\tau $（浮点数），选择的 $ \\sigma $（浮点数），乘积 $ \\tau \\sigma \\widehat{\\|K\\|}^2 $（浮点数），以及一个布尔值，指示 $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $ 是否严格满足。\n\n使用以下测试套件，它涵盖了各种矩阵形状和条件数场景：\n- 测试用例 $ 1 $ (happy path, rectangular): $ K_1 = \\begin{bmatrix} 3  0  0 \\\\ 0  1  2 \\end{bmatrix} $, $ s = 0.9 $。\n- 测试用例 $ 2 $ (boundary-near case, square identity): $ K_2 = I_4 $，其中 $ I_4 $ 是 $ 4 \\times 4 $ 单位矩阵, $ s = 0.99 $。\n- 测试用例 $ 3 $ (degenerate zero operator, rectangular): $ K_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\\\ 0  0 \\end{bmatrix} $, $ s = 0.5 $。\n- 测试用例 $ 4 $ (ill-conditioned scales, rectangular): $ K_4 = \\begin{bmatrix} 10^{-3}  0  0 \\\\ 0  10  0 \\\\ 0  0  10^{-1} \\\\ 0  0  0 \\end{bmatrix} $, $ s = 0.95 $。\n\n实现要求：\n- 使用确定性初始化 $ v_0 = \\frac{1}{\\sqrt{n}} \\mathbf{1}_n $。\n- 对幂迭代法使用 $ 10^{-12} $ 的相对变化容差和最多 $ 10000 $ 次迭代。\n- $ \\tau $ 和 $ \\sigma $ 的选择规则必须是：当 $ \\widehat{\\|K\\|} > 0 $ 时，$ \\tau = \\frac{s}{\\widehat{\\|K\\|}} $ 和 $ \\sigma = \\frac{s}{\\widehat{\\|K\\|}} $；否则，$ \\tau = 1 $，$ \\sigma = 1 $。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个格式为 $[\\widehat{\\|K\\|}, \\tau, \\sigma, \\tau \\sigma \\widehat{\\|K\\|}^2, \\text{boolean}]$ 的列表。例如，总输出应类似于 $[[\\dots],[\\dots],[\\dots],[\\dots]]$，不带任何附加文本。",
            "solution": "该问题要求为原始-对偶混合梯度 (PDHG) 方法开发一个选择原始步长 $ \\tau $ 和对偶步长 $ \\sigma $ 的程序。该选择必须满足稳定性条件 $ \\tau \\sigma \\|K\\|^2  1 $，其中 $ K \\in \\mathbb{R}^{m \\times n} $ 是线性正算子，$ \\|K\\| $ 是其谱范数。任务的核心是使用一种仅依赖于涉及 $K$ 及其转置 $K^\\top$ 的矩阵-向量乘积的迭代数值方法来估计 $ \\|K\\| $。\n\n### 理论基础：谱范数与特征值\n\n矩阵 $K$ 的谱范数，记为 $ \\|K\\| $ 或 $ \\|K\\|_2 $，定义为 $K$ 拉伸向量 $ x \\in \\mathbb{R}^n $ 的最大因子：\n$$ \\|K\\| = \\sup_{\\|x\\|_2 = 1} \\|Kx\\|_2 $$\n其中 $ \\| \\cdot \\|_2 $ 是欧几里得范数。一个等效且在计算上更有用的表征将谱范数与 $K$ 的奇异值联系起来。谱范数恰好是 $K$ 的最大奇异值，记为 $ \\sigma_{\\max}(K) $。\n\n根据定义，$K$ 的奇异值是对称半正定矩阵 $K^\\top K$ 的特征值的平方根。设 $K^\\top K$ 的特征值为 $ \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0 $。那么 $K$ 的奇异值为 $ \\sigma_i = \\sqrt{\\lambda_i} $。因此，最大奇异值对应于最大特征值：\n$$ \\|K\\| = \\sigma_{\\max}(K) = \\sqrt{\\lambda_{\\max}(K^\\top K)} $$\n这种关系将寻找 $K$ 的谱范数的问题转化为寻找矩阵 $A = K^\\top K$ 的最大特征值的问题。\n\n### 算法方法：幂迭代法\n\n幂法，或称幂迭代法，是一种用于寻找矩阵最大模特征值（主特征值）的标准算法。对于矩阵 $A = K^\\top K$，由于其对称半正定，所有特征值均为实数且非负。因此，主特征值就是最大特征值 $ \\lambda_{\\max}(A) $。\n\n幂迭代算法的步骤如下：\n1.  初始化一个向量 $ v_0 \\in \\mathbb{R}^n $，满足 $ \\|v_0\\|_2 = 1 $。问题指定了确定性初始化：$ v_0 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n $，其中 $ \\mathbf{1}_n $ 是全一向量。\n2.  对 $ k = 0, 1, 2, \\dots $进行迭代：\n    $$ v_{k+1} = \\frac{A v_k}{\\|A v_k\\|_2} $$\n    当 $ k \\to \\infty $ 时，只要 $ \\lambda_{\\max} $ 的模严格大于所有其他特征值的模，并且初始向量 $ v_0 $ 在该特征向量方向上有非零分量，向量 $ v_k $ 就会收敛到对应于 $ \\lambda_{\\max}(A) $ 的特征向量。\n\n可以在每一步估计特征值 $ \\lambda_{\\max}(A) $。一个常见的估计是瑞利商 $ \\lambda^{(k)} = v_k^\\top A v_k $。一个更简单的、直接从迭代中产生的估计是 $ \\lambda^{(k+1)} = \\|A v_k\\|_2 $。当 $v_k$ 接近真实特征向量时，$ A v_k \\approx \\lambda_{\\max} v_k $，所以 $ \\|A v_k\\|_2 \\approx |\\lambda_{\\max}| \\|v_k\\|_2 = \\lambda_{\\max} $。\n\n问题的一个关键要求是仅使用与 $K$ 和 $K^\\top$ 的乘法，而不显式地构造矩阵 $A = K^\\top K$，因为对于大规模问题，这在计算上可能成本高昂或不可行。矩阵-向量乘积 $ A v_k $ 通过两个操作序列来计算：\n1.  计算向量 $ u_k = K v_k $。\n2.  计算向量 $ w_{k+1} = K^\\top u_k = K^\\top (K v_k) $。\n\n完整的迭代步骤如下：\n$ w_{k+1} = K^\\top K v_k $\n$ \\lambda_{k+1} = \\|w_{k+1}\\|_2 $ (这是我们对 $ \\lambda_{\\max} $ 的估计)\n$ v_{k+1} = \\frac{w_{k+1}}{\\lambda_{k+1}} $ (为下一步进行归一化)\n\n当特征值估计的相对变化低于指定的容差 $ \\epsilon = 10^{-12} $（即 $ \\frac{|\\lambda_{k+1} - \\lambda_k|}{|\\lambda_{k+1}|}  \\epsilon $）或达到最大迭代次数（$10000$）时，循环终止。\n\n### 步长选择程序\n\n估计谱范数并选择步长 $ \\tau $ 和 $ \\sigma $ 的完整程序如下：\n\n1.  **估计 $ \\lambda_{\\max}(K^\\top K) $**：\n    -   初始化 $ v_0 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n $ 和一个初始特征值估计，例如 $ \\lambda_0 = 0 $。\n    -   如上所述迭代计算 $ v_{k+1} $ 和 $ \\lambda_{k+1} $ 直到收敛。设最终估计为 $ \\hat{\\lambda}_{\\max} $。\n\n2.  **估计 $ \\|K\\| $**：\n    -   谱范数估计为 $ \\widehat{\\|K\\|} = \\sqrt{\\hat{\\lambda}_{\\max}} $。\n\n3.  **选择 $ \\tau $ 和 $ \\sigma $**：\n    -   引入一个安全因子 $ s \\in (0,1) $ 以确保稳定性条件严格成立。\n    -   **情况1：$ \\widehat{\\|K\\|} > 0 $**（典型情况）。对称地选择步长：\n        $$ \\tau = \\frac{s}{\\widehat{\\|K\\|}} \\quad \\text{and} \\quad \\sigma = \\frac{s}{\\widehat{\\|K\\|}} $$\n        通过这种选择，稳定性表达式变为：\n        $$ \\tau \\sigma \\widehat{\\|K\\|}^2 = \\left(\\frac{s}{\\widehat{\\|K\\|}}\\right) \\left(\\frac{s}{\\widehat{\\|K\\|}}\\right) \\widehat{\\|K\\|}^2 = s^2 $$\n        由于 $ s \\in (0,1) $，可以保证 $ s^2  1 $，从而满足条件 $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $。\n    -   **情况2：$ \\widehat{\\|K\\|} $ 在数值上为零**。如果 $K$ 是零矩阵，则会发生这种情况。问题指定了以下惯例：\n        $$ \\tau = 1 \\quad \\text{and} \\quad \\sigma = 1 $$\n        在这种情况下，乘积 $ \\tau \\sigma \\widehat{\\|K\\|}^2 = 1 \\cdot 1 \\cdot 0^2 = 0 $，也满足稳定性条件 $ 0  1 $。\n\n此程序为配置 PDHG 算法的步长提供了一种稳健且标准的方法，在仅使用基本矩阵运算的同时确保了稳定性。最终的实现将对每个给定的测试用例应用这一完整逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of selecting PDHG step sizes for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[3.0, 0.0, 0.0], [0.0, 1.0, 2.0]]), 0.9),\n        (np.identity(4), 0.99),\n        (np.zeros((3, 2)), 0.5),\n        (np.array([[1e-3, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 1e-1], [0.0, 0.0, 0.0]]), 0.95),\n    ]\n\n    results = []\n    \n    # Define constants for the power iteration\n    max_iter = 10000\n    rel_tol = 1e-12\n    # Tolerance for checking if a value is numerically zero\n    zero_tol = 1e-14 \n\n    for K, s in test_cases:\n        m, n = K.shape\n\n        # Initialize the vector v0 deterministically.\n        if n > 0:\n            v = np.ones(n) / np.sqrt(n)\n        else: # Handle zero-column matrix\n            v = np.array([])\n            \n        lambda_est = 0.0\n\n        # Power iteration to find the largest eigenvalue of K^T * K\n        # If K is the zero matrix, K.T @ K @ v will be zero, lambda_est will remain 0.\n        if n > 0:\n            for _ in range(max_iter):\n                # Apply operator K^T * K without forming the matrix explicitly\n                K_v = K @ v\n                Kt_K_v = K.T @ K_v\n    \n                lambda_new = np.linalg.norm(Kt_K_v)\n                \n                # Check for convergence\n                if lambda_new  zero_tol:\n                    lambda_est = 0.0\n                    break\n                \n                # Normalize the vector for the next iteration\n                v = Kt_K_v / lambda_new\n    \n                # Relative change stopping criterion\n                relative_change = abs(lambda_new - lambda_est) / lambda_new\n                lambda_est = lambda_new\n    \n                if relative_change  rel_tol:\n                    break\n        \n        # Estimate the spectral norm\n        norm_K_est = np.sqrt(lambda_est)\n\n        # Select tau and sigma based on the estimated norm\n        if norm_K_est > zero_tol:\n            tau = s / norm_K_est\n            sigma = s / norm_K_est\n        else:\n            # Degenerate case rule: if norm is numerically zero\n            norm_K_est = 0.0 # Clean up tiny numerical noise\n            tau = 1.0\n            sigma = 1.0\n\n        # Calculate the stability product and check the condition\n        product = tau * sigma * norm_K_est**2\n        is_stable = product  1.0\n\n        # Append the list of results for this test case\n        results.append([norm_K_est, tau, sigma, product, is_stable])\n\n    # Final print statement in the exact required format.\n    # The string representation of a Python list includes spaces, which is\n    # consistent with the provided template's use of map(str, ...).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "PDHG算法中的一个关键步骤是对偶更新，它涉及计算数据失配项函数 $g$ 的凸共轭 $g^*$ 的邻近算子，记为 $\\mathrm{prox}_{\\sigma g^*}$。直接计算此算子可能具有挑战性，但莫罗恒等式 (Moreau identity) 提供了一座强大的桥梁，将 $\\mathrm{prox}_{\\sigma g^*}$ 与通常更容易计算的原始函数 $g$ 的邻近算子联系起来。本练习将演示如何推导并应用这一基本恒等式 ，通过一个具体的例子，你将学到一个使对偶更新的实现变得可行和高效的关键技巧。",
            "id": "3413751",
            "problem": "考虑数据模型为 $z = K x$ 的线性逆问题，以及一个非光滑数据失配势 $g(z) = \\alpha \\| z - y \\|_{2}$，其中 $K \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$z \\in \\mathbb{R}^{m}$，并且 $y \\in \\mathbb{R}^{m}$ 是一个固定的观测向量。在原始-对偶混合梯度（PDHG）方法中，对偶更新需要计算凸共轭 $g^{*}$ 的近端算子。从凸共轭和近端算子的基本定义出发，推导一个恒等式，对于任意 $\\sigma  0$ 和 $q \\in \\mathbb{R}^{m}$，该恒等式能用 $g$ 的近端算子来表示 $\\mathrm{prox}_{\\sigma g^{*}}(q)$，然后使用这个恒等式为以下实例显式计算对偶更新：\n$a = \\alpha = 1$，$\\sigma = 1$，$y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及 $q = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$。\n\n你的最终答案必须是给定数值下 $\\mathrm{prox}_{\\sigma g^{*}}(q)$ 的显式值，表示为单个行向量。无需四舍五入。",
            "solution": "问题陈述已经过验证，被认为是**有效的**。它在凸优化领域具有科学依据，是适定的、客观的，并为得到唯一解提供了所有必要信息。\n\n该问题要求分为两部分：首先，推导一个普适的恒等式，将凸共轭函数 $g^*$ 的近端算子与函数 $g$ 本身的近端算子联系起来；其次，将此恒等式应用于一个具体实例。\n\n**第一部分：Moreau 恒等式的推导**\n\n我们被要求推导 $\\mathrm{prox}_{\\sigma g^{*}}(q)$ 的一个恒等式。我们从近端算子的定义开始：\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = \\arg\\min_{z \\in \\mathbb{R}^{m}} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma g^{*}(z) \\right\\}\n$$\n凸共轭 $g^*(z)$ 定义为：\n$$\ng^{*}(z) = \\sup_{x \\in \\mathbb{R}^{m}} \\left\\{ \\langle z, x \\rangle - g(x) \\right\\}\n$$\n将共轭的定义代入近端算子的定义中，我们得到一个极小化极大问题：\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = \\arg\\min_{z} \\sup_{x} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\left( \\langle z, x \\rangle - g(x) \\right) \\right\\}\n$$\n令目标函数为 $L(z, x) = \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\langle z, x \\rangle - \\sigma g(x)$。函数 $L(z,x)$ 对于任意固定的 $x$ 是关于 $z$ 的凸函数，对于任意固定的 $z$ 是关于 $x$ 的仿射（因此是凹）函数。定义域是整个空间 $\\mathbb{R}^m$。因此，根据 Sion 极小极大定理，我们可以交换 $\\min$ 和 $\\sup$ 算子：\n$$\n\\sup_{x} \\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\langle z, x \\rangle - \\sigma g(x) \\right\\}\n$$\n我们首先求解关于 $z$ 的内部最小化问题。目标是 $z$ 的二次函数。我们通过将关于 $z$ 的梯度设为零来找到最小值：\n$$\n\\nabla_{z} L(z, x) = (z - q) + \\sigma x = 0\n$$\n这给出了唯一的极小化子 $z^{*}(x) = q - \\sigma x$。\n\n现在，我们将这个最优的 $z^{*}(x)$ 代回目标函数，以求解关于 $x$ 的外部最大化问题：\n$$\n\\sup_{x} \\left\\{ \\frac{1}{2} \\| (q - \\sigma x) - q \\|_{2}^{2} + \\sigma \\langle q - \\sigma x, x \\rangle - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\frac{1}{2} \\| -\\sigma x \\|_{2}^{2} + \\sigma \\langle q, x \\rangle - \\sigma^2 \\langle x, x \\rangle - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} + \\sigma \\langle q, x \\rangle - \\sigma^2 \\| x \\|_{2}^{2} - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\sigma \\langle q, x \\rangle - \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} - \\sigma g(x) \\right\\}\n$$\n解决这个极小化极大问题的解是最优的 $z^* = q - \\sigma x^*$，其中 $x^*$ 是使上述表达式最大化的参数。最大化此表达式等价于最小化其负值：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ -\\sigma \\langle q, x \\rangle + \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} + \\sigma g(x) \\right\\}\n$$\n将目标函数除以正常数 $\\sigma$ 不会改变极小化子：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ -\\langle q, x \\rangle + \\frac{\\sigma}{2} \\| x \\|_{2}^{2} + g(x) \\right\\}\n$$\n为了将其与近端算子联系起来，我们对包含 $x$ 的项进行配方：\n$$\n\\frac{\\sigma}{2} \\| x \\|_{2}^{2} - \\langle q, x \\rangle = \\frac{\\sigma}{2} \\left( \\| x \\|_{2}^{2} - 2\\langle x, \\frac{q}{\\sigma} \\rangle \\right) = \\frac{\\sigma}{2} \\left( \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} - \\left\\| \\frac{q}{\\sigma} \\right\\|_{2}^{2} \\right)\n$$\n将此代回 $x^*$ 的最小化问题中：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\frac{\\sigma}{2} \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} - \\frac{\\sigma}{2} \\left\\| \\frac{q}{\\sigma} \\right\\|_{2}^{2} + g(x) \\right\\}\n$$\n项 $-\\frac{\\sigma}{2} \\| q/\\sigma \\|_{2}^{2}$ 是关于 $x$ 的常数，可以忽略。将目标函数乘以 $1/\\sigma$ 也不会改变极小化子：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\frac{1}{2} \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} + \\frac{1}{\\sigma} g(x) \\right\\}\n$$\n这恰好是 $\\mathrm{prox}_{g/\\sigma}(q/\\sigma)$ 的定义。因此，$x^{*} = \\mathrm{prox}_{g/\\sigma}(q/\\sigma)$。\n原始所求量的最终结果是 $z^{*}(x^*)$：\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = q - \\sigma x^{*} = q - \\sigma \\mathrm{prox}_{g/\\sigma}(q/\\sigma)\n$$\n这个关系被称为 Moreau 恒等式（或 Moreau 分解）。\n\n**第二部分：显式计算**\n\n我们给定具体实例：\n$g(z) = \\alpha \\| z - y \\|_{2}$，其中 $\\alpha = 1$，所以 $g(z) = \\| z - y \\|_{2}$。\n参数为 $\\sigma = 1$，$y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及 $q = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$。\n\n使用上面推导的恒等式，并设 $\\sigma = 1$：\n$$\n\\mathrm{prox}_{g^{*}}(q) = q - 1 \\cdot \\mathrm{prox}_{g/1}(q/1) = q - \\mathrm{prox}_{g}(q)\n$$\n我们需要计算 $\\mathrm{prox}_{g}(q)$：\n$$\n\\mathrm{prox}_{g}(q) = \\arg\\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + g(z) \\right\\} = \\arg\\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\| z - y \\|_{2} \\right\\}\n$$\n让我们进行变量替换 $u = z - y$，这意味着 $z = u + y$。最小化问题变为：\n$$\n\\arg\\min_{u} \\left\\{ \\frac{1}{2} \\| (u + y) - q \\|_{2}^{2} + \\| u \\|_{2} \\right\\} = \\arg\\min_{u} \\left\\{ \\frac{1}{2} \\| u - (q - y) \\|_{2}^{2} + \\| u \\|_{2} \\right\\}\n$$\n这是欧几里得范数的近端算子 $\\mathrm{prox}_{\\|\\cdot\\|_2}(\\cdot)$ 在点 $w = q-y$ 处的定义。$\\mathrm{prox}_{\\lambda\\|\\cdot\\|_2}(w)$ 的通解由向量软阈值给出：\n$$\n\\mathrm{prox}_{\\lambda\\|\\cdot\\|_2}(w) = \\max\\left(0, 1 - \\frac{\\lambda}{\\|w\\|_2}\\right) w = \\begin{cases} \\left(1 - \\frac{\\lambda}{\\|w\\|_2}\\right) w  \\text{if } \\|w\\|_2  \\lambda \\\\ 0  \\text{if } \\|w\\|_2 \\le \\lambda \\end{cases}\n$$\n在我们的例子中，范数的系数是 $1$，所以 $\\lambda=1$。让我们计算 $w = q - y$：\n$$\nw = q - y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n$$\n$w$ 的欧几里得范数是：\n$$\n\\| w \\|_{2} = \\sqrt{0^2 + 2^2} = 2\n$$\n因为 $\\|w\\|_2 = 2  1 = \\lambda$，我们使用公式的第一种情况。最优的 $u$ 是：\n$$\nu_{\\mathrm{opt}} = \\mathrm{prox}_{\\|\\cdot\\|_2}(w) = \\left(1 - \\frac{1}{\\|w\\|_2}\\right) w = \\left(1 - \\frac{1}{2}\\right) w = \\frac{1}{2} w = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n原始问题在 $z$ 上的极小化子是 $z_{\\mathrm{opt}} = u_{\\mathrm{opt}} + y$。因此，\n$$\n\\mathrm{prox}_{g}(q) = z_{\\mathrm{opt}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n$$\n最后，我们可以计算所需的对偶更新项：\n$$\n\\mathrm{prox}_{g^{*}}(q) = q - \\mathrm{prox}_{g}(q) = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 4-3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在许多现实世界的应用中，例如大规模数据同化，观测算子 $K$ 体量巨大但稀疏，并且数据失配项在不同的观测数据块之间是可分的。这些结构特性是实现大规模并行计算的关键，一个可分的对偶目标函数允许将对偶更新步骤分解为许多可以同时计算的独立任务。本练习旨在挑战你思考PDHG的计算架构 ，通过分析不同的并行化策略，你将理解如何正确利用问题结构来为高维反问题设计高效、可扩展的求解器。",
            "id": "3413769",
            "problem": "考虑一个凸变分数据同化问题，其形式是最小化一个背景或模型一致性项与一个数据失配项之和：\n$$\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + g(Kx),$$\n其中 $f:\\mathbb{R}^{n}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 和 $g:\\mathbb{R}^{m}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 是正常、凸、下半连续函数，而 $K \\in \\mathbb{R}^{m\\times n}$ 是一个由观测算子和可能的模型线性化产生的线性算子。假设观测数据被分为 $J$ 个块（例如，按传感器或时间分组），从而产生一个块行结构 $K = \\begin{bmatrix} K_{1} \\\\ \\vdots \\\\ K_{J} \\end{bmatrix}$ 以及 $z = Kx$ 的相应块分解 $z = (z_{1},\\dots,z_{J})$，其中 $z_{j} = K_{j}x \\in \\mathbb{R}^{m_{j}}$ 且 $\\sum_{j=1}^{J} m_{j} = m$。假设数据失配项在观测块之间是可分的，即\n$$g(z) \\;=\\; \\sum_{j=1}^{J} g_{j}(z_{j}), \\qquad z = (z_{1},\\dots,z_{J}).$$\n\n带有对角步长的原始-对偶混合梯度（PDHG）方法使用如下的对偶和原始更新步骤：\n$$y^{k+1} \\;=\\; \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right), \\qquad x^{k+1} \\;=\\; \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right),$$\n其中包含外推 $\\bar{x}^{k} = x^{k} + \\theta(x^{k} - x^{k-1})$，对角步长算子 $\\Sigma \\succeq 0$ 和 $T \\succeq 0$，以及 $g$ 的 Fenchel 共轭 $g^{\\ast}$。在高维设定和大量观测的情况下，$K$ 通常非常稀疏，且块 $\\{K_{j}\\}_{j=1}^{J}$ 具有对应于局部化或时间上孤立的测量的小支撑集。\n\n下列哪项最能描述一种正确且能保证收敛的策略，用以利用 $K$ 的稀疏性和块结构来并行化数据同化中的对偶更新？\n\nA. 将 $K$ 按块行划分为 $K = \\begin{bmatrix} K_{1} \\\\ \\cdots \\\\ K_{J} \\end{bmatrix}$，并与可分分解 $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$ 对齐。使用块对角对偶步长 $\\Sigma = \\operatorname{diag}(\\sigma_{1} I_{m_{1}},\\dots,\\sigma_{J} I_{m_{J}})$ 和原始步长 $T = \\tau I_{n}$，选择它们以满足标准 PDHG 条件 $\\lVert \\Sigma^{1/2} K T^{1/2} \\rVert  1$（例如，通过基于块范数的对角预处理）。然后，对所有 $j$ 并行计算块残差 $r_{j}^{k} = K_{j}\\bar{x}^{k}$ 和对偶块更新 $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} r_{j}^{k})$。最后，通过一次归约操作形成 $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$，并执行原始更新。这实现了易于并行的对偶更新，每次迭代只需一次全局归约。\n\nB. 将 $K$ 按列划分，并通过独立计算局部的 $x$ 更新来并行化原始步骤，同时保持一个单一的整体对偶向量 $y$。对偶更新必须保持集中式，因为 $g$ 依赖于所有观测，所以试图分离 $y$ 会违反凸性，并且对于收敛来说是不必要的。\n\nC. 将对偶近端算子替换为在每次迭代中使用应用于 $K K^{\\top}$ 的 Jacobi 方法求解 $y$ 的正规方程，因为凸性意味着近端上升与求解相关的最优性条件是等价的。这使得并行化成为可能，而无需 $g$ 的块可分性。\n\nD. 将状态 $x$ 分割成重叠的空间子域，并完全异步地更新 $x$ 和所有对偶块 $y_{j}$，无需同步或全局归约，因为 $K$ 的稀疏性意味着 $K^{\\top} y$ 可以在局部近似而不会影响收敛保证。\n\nE. 将 $K$ 替换为一个块对角近似 $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$，该近似通过观测者之间的 gossip (闲聊) 算法构建以消除块间耦合，然后在 $(f,g,\\tilde{K})$ 上执行集中式 PDHG。在实践中，跨块项的损失可以忽略不计，并且不影响对原始解的收敛。",
            "solution": "用户要求对问题陈述进行批判性验证，然后推导正确答案并评估所有提供的选项。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n该问题是一个凸优化问题，形式如下：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + g(Kx)\n$$\n- **函数**：$f:\\mathbb{R}^{n}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 和 $g:\\mathbb{R}^{m}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 是正常、凸、下半连续函数。\n- **算子**：$K \\in \\mathbb{R}^{m\\times n}$ 是一个线性算子。\n- **块结构**：算子 $K$ 具有块行结构 $K = \\begin{bmatrix} K_{1} \\\\ \\vdots \\\\ K_{J} \\end{bmatrix}$，其中 $K_j \\in \\mathbb{R}^{m_j \\times n}$ 且 $\\sum_{j=1}^{J} m_{j} = m$。这在 $z=Kx$ 上引入了块结构 $z = (z_1, \\dots, z_J)$，其中 $z_j = K_j x$。\n- **可分性**：函数 $g$ 关于此块结构是可分的：$g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$。\n- **算法**：考虑使用原始-对偶混合梯度（PDHG）方法，其更新步骤为：\n  $$\n  y^{k+1} \\;=\\; \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right)\n  $$\n  $$\n  x^{k+1} \\;=\\; \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right)\n  $$\n- **算法参数**：\n    - $\\bar{x}^{k} = x^{k} + \\theta(x^{k} - x^{k-1})$ 是一个外推步骤（对于标准算法，$\\theta=1$）。\n    - $\\Sigma \\succeq 0$ 和 $T \\succeq 0$ 是对角步长算子。\n    - $g^{\\ast}$ 是 $g$ 的 Fenchel 共轭。\n- **背景**：设定是具有稀疏算子 $K$ 的高维数据同化。\n- **问题**：任务是确定一种正确且能保证收敛的策略，以跨越 $J$ 个观测块并行化对偶更新。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n1.  **科学上合理**：该问题表述在反问题、数据同化和机器学习领域是标准的。使用 PDHG（也称为 Chambolle-Pock 算法）解决此类问题是现代凸优化的基石。函数（$f, g$）和算子（$K$）的属性是保证算法收敛的标准假设。该问题牢固地植根于数学优化理论。\n2.  **适定性**：问题陈述是适定的。它描述了一个标准的优化问题和一个标准的求解算法。问题要求找到一种有效的并行化策略，这是在该背景下一个明确定义的计算机科学和数值分析问题。\n3.  **客观性**：语言精确、客观。所有术语，如“正常、凸、下半连续”、“Fenchel 共轭”和“近端算子”，都具有严谨的数学定义。\n4.  **完整性**：问题提供了所有必要信息。问题的结构（$\\min f(x)+\\sum_j g_j(K_j x)$）和 PDHG 算法的形式都已明确说明，这足以分析并行化策略。\n\n**第 3 步：结论与行动**\n\n问题陈述是**有效**的。它在科学上是合理的、适定的和完整的。我现在将着手推导解决方案并评估各个选项。\n\n### 解的推导\n\n问题的核心在于对偶更新的结构：\n$$\ny^{k+1} = \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right)\n$$\n能否并行化此更新取决于函数 $g$ 的可分性以及步长算子 $\\Sigma$ 的相应结构。\n\n问题陈述指出 $g(z) = \\sum_{j=1}^{J} g_j(z_j)$，其中 $z = (z_1, \\dots, z_J)$ 是 $z$ 的块分解。Fenchel 共轭的一个基本性质是，对于可分函数，其共轭也是可分的：\n$$\ng^{\\ast}(y) = \\left(\\sum_{j=1}^{J} g_j\\right)^{\\ast}(y) = \\sum_{j=1}^{J} g_j^{\\ast}(y_j),\n$$\n其中 $y = (y_1, \\dots, y_J)$ 是对偶变量的相应块分解。\n\n近端算子定义为：\n$$\n\\operatorname{prox}_{\\Sigma g^{\\ast}}(v) = \\arg\\min_{y} \\left\\{ g^{\\ast}(y) + \\frac{1}{2} \\| y - v \\|_{\\Sigma^{-1}}^2 \\right\\}\n$$\n我们选择一个与 $y$ 的块结构一致的块对角对偶步长算子 $\\Sigma$：\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_1  0  \\cdots  0 \\\\ 0  \\Sigma_2  \\cdots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 0  0  \\cdots  \\Sigma_J \\end{pmatrix},\n$$\n其中 $\\Sigma_j$ 是 $\\mathbb{R}^{m_j}$ 上的半正定算子。通过这种选择，二次惩罚项也变得可分：\n$$\n\\| y - v \\|_{\\Sigma^{-1}}^2 = (y-v)^\\top \\Sigma^{-1} (y-v) = \\sum_{j=1}^{J} (y_j - v_j)^\\top \\Sigma_j^{-1} (y_j - v_j) = \\sum_{j=1}^{J} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2.\n$$\n因此，近端算子中的最小化问题解耦为 $J$ 个独立的子问题：\n$$\n\\arg\\min_{y} \\left\\{ \\sum_{j=1}^{J} g_j^{\\ast}(y_j) + \\sum_{j=1}^{J} \\frac{1}{2} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2 \\right\\} = \\sum_{j=1}^{J} \\arg\\min_{y_j} \\left\\{ g_j^{\\ast}(y_j) + \\frac{1}{2} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2 \\right\\}.\n$$\n这意味着更新后的对偶变量的第 $j$ 个块 $y_j^{k+1}$ 可以计算为：\n$$\ny_j^{k+1} = \\operatorname{prox}_{\\Sigma_j g_j^{\\ast}}(v_j),\n$$\n其中 $v_j$ 是参数 $v = y^k + \\Sigma K \\bar{x}^k$ 的第 $j$ 个块。块 $v_j$ 由下式给出：\n$$\nv_j = y_j^k + (\\Sigma K \\bar{x}^k)_j = y_j^k + \\Sigma_j K_j \\bar{x}^k.\n$$\n因此，对偶更新可以对每个块 $j=1, \\dots, J$ 并行执行：\n$$\ny_j^{k+1} = \\operatorname{prox}_{\\Sigma_j g_j^{\\ast}}\\!\\left(y_j^{k} + \\Sigma_j K_j \\bar{x}^k\\right).\n$$\n这是一个易于并行的计算，前提是所有处理单元都可以获得完整的向量 $\\bar{x}^k$。\n\n现在，考虑原始更新：\n$$\nx^{k+1} = \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right).\n$$\n关键项是 $K^\\top y^{k+1}$。给定 $K$ 和 $y$ 的块结构，这个矩阵-向量乘积展开为：\n$$\nK^\\top y^{k+1} = \\begin{bmatrix} K_1^\\top  K_2^\\top  \\cdots  K_J^\\top \\end{bmatrix} \\begin{bmatrix} y_1^{k+1} \\\\ y_2^{k+1} \\\\ \\vdots \\\\ y_J^{k+1} \\end{bmatrix} = \\sum_{j=1}^{J} K_j^\\top y_j^{k+1}.\n$$\n此操作需要一个同步步骤：在并行计算所有 $y_j^{k+1}$ 之后，必须计算乘积 $K_j^\\top y_j^{k+1}$（这也可以并行完成），然后将它们相加。这是一个经典的 map-reduce 或归约操作。然后将得到的和用于 $x^{k+1}$ 的单一、集中式原始更新。\n\n带有矩阵步长 $\\Sigma, T$ 的 PDHG 方法的收敛性是有保证的，如果选择的原始和对偶步长使得算子范数 $\\|\\Sigma^{1/2} K T^{1/2}\\|  1$。这是标准的收敛条件。\n\n### 逐项分析\n\n**A. 将 $K$ 按块行划分为 $K = \\begin{bmatrix} K_{1} \\\\ \\dots \\\\ K_{J} \\end{bmatrix}$，并与可分分解 $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$ 对齐。使用块对角对偶步长 $\\Sigma = \\operatorname{diag}(\\sigma_{1} I_{m_{1}},\\dots,\\sigma_{J} I_{m_{J}})$ 和原始步长 $T = \\tau I_{n}$，选择它们以满足标准 PDHG 条件 $\\lVert \\Sigma^{1/2} K T^{1/2} \\rVert  1$（例如，通过基于块范数的对角预处理）。然后，对所有 $j$ 并行计算块残差 $r_{j}^{k} = K_{j}\\bar{x}^{k}$ 和对偶块更新 $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} r_{j}^{k})$。最后，通过一次归约操作形成 $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$，并执行原始更新。这实现了易于并行的对偶更新，每次迭代只需一次全局归约。**\n\n这个选项与上面的推导完全匹配。它正确地指出，将 $K$ 的块行划分与 $g$ 的可分性以及块对角步长 $\\Sigma$ 相结合，可以解耦对偶更新。特定的形式 $\\Sigma_j = \\sigma_j I_{m_j}$ 是一种常见的实用选择。对偶更新公式 $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} K_j \\bar{x}^{k})$ 是正确的。原始更新的归约步骤的正确形式 $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$ 也被陈述了。引用的收敛条件是此设置下的正确条件。将计算模式描述为“易于并行的对偶更新，每次迭代一次全局归约”是准确的。\n**结论：正确**\n\n**B. 将 $K$ 按列划分，并通过独立计算局部的 $x$ 更新来并行化原始步骤，同时保持一个单一的整体对偶向量 $y$。对偶更新必须保持集中式，因为 $g$ 依赖于所有观测，所以试图分离 $y$ 会违反凸性，并且对于收敛来说是不必要的。**\n\n这个选项在多个方面存在缺陷。首先，它建议通过按列划分 $K$ 来并行化原始步骤，这是一种不同的策略（一种原始分解的形式）。其次，其理由不正确。问题明确指出 $g$ 是可分的（$g(z) = \\sum_j g_j(z_j)$），这与它“依赖于所有观测”且不可分的说法相矛盾。正是这种可分性允许了并行的对偶更新。声称分离 $y$ “会违反凸性”是毫无根据的；$g^*$ 的可分性是 $g$ 的凸性和可分性的直接结果。\n**结论：不正确**\n\n**C. 将对偶近端算子替换为在每次迭代中使用应用于 $K K^{\\top}$ 的 Jacobi 方法求解 $y$ 的正规方程，因为凸性意味着近端上升与求解相关的最优性条件是等价的。这使得并行化成为可能，而无需 $g$ 的块可分性。**\n\n这个选项不正确。只有当 $g$（因此 $g^*$）是二次函数（特别是 $\\ell_2$ 范数的平方）时，用线性的“正规方程”求解来代替近端算子才是有效的。对于一般的凸函数 $g$，近端算子是一个非线性映射。“凸性意味着等价”的说法是一种严重的过度简化；虽然近端算子求解一个最优性条件，但这个条件通常不是一个线性系统。此外，它错误地暗示这在没有 $g$ 的可分性的情况下也有效。如果 $g$ 不可分，其近端算子也不可分，无论使用何种方法计算近端算子，对偶更新的并行化都不是直接的。\n**结论：不正确**\n\n**D. 将状态 $x$ 分割成重叠的空间子域，并完全异步地更新 $x$ 和所有对偶块 $y_{j}$，无需同步或全局归约，因为 $K$ 的稀疏性意味着 $K^{\\top} y$ 可以在局部近似而不会影响收敛保证。**\n\n这个选项提出了一种完全异步的方案，并做出了通常是错误的强硬声明。虽然异步优化算法确实存在，但它们有严格的理论要求，并且通常具有与其同步对应算法不同的收敛保证。声称 $K^\\top y$ 可以“在局部近似而不影响收敛保证”是错误的。$K$ 的稀疏性意味着和 $K^\\top y = \\sum_j K_j^\\top y_j$ 的任何分量都只有少数非零项，这使得计算效率很高。但这并*不*意味着可以忽略非局部贡献而仍然收敛到正确的解。这种近似改变了算法，使其解决的是一个不同的问题。该策略不是所要求的“能保证收allen”的。\n**结论：不正确**\n\n**E. 将 $K$ 替换为一个块对角近似 $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$，该近似通过观测者之间的 gossip (闲聊) 算法构建以消除块间耦合，然后在 $(f,g,\\tilde{K})$ 上执行集中式 PDHG。在实践中，跨块项的损失可以忽略不计，并且不影响对原始解的收敛。**\n\n这个选项存在根本性缺陷，因为它建议解决一个不同的问题。通过将 $K$ 替换为近似值 $\\tilde{K}$，算法将收敛到 $f(x) + g(\\tilde{K}x)$ 的最小化子，而不是原始目标函数。声称这“不影响对原始解的收敛”是错误的。这种近似在某些应用中可能是一种有效的启发式方法，但它不是解决原始问题的正确策略。$\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ 的构建也定义不当，因为每个 $K_j$ 都是一个大小为 $m_j \\times n$ 的“胖”矩阵；块对角排列需要对原始空间 $\\mathbb{R}^n$ 进行划分，而这是没有给出的。\n**结论：不正确**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}