{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在解决原始-对偶混合梯度（PDHG）方法中的一个核心计算步骤：对偶更新。该步骤涉及到凸共轭函数 $g^*$ 的近端算子 $\\operatorname{prox}_{\\sigma g^*}$，直接计算它可能具有挑战性。本练习将指导你推导并应用强大的 Moreau 恒等式，这是一个将 $g^*$ 的近端算子与通常更简单的 $g$ 的近端算子联系起来的基本工具。掌握这项技术对于将 PDHG 应用于那些数据失配项 $g$ 比其共轭函数更容易处理的广泛问题至关重要。",
            "id": "3413751",
            "problem": "考虑一个线性逆问题，其数据模型为 $z = K x$，数据失配势为非光滑函数 $g(z) = \\alpha \\| z - y \\|_{2}$，其中 $K \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$z \\in \\mathbb{R}^{m}$，且 $y \\in \\mathbb{R}^{m}$ 是一个固定的观测向量。在原始-对偶混合梯度 (PDHG) 方法中，对偶更新需要计算凸共轭函数 $g^{*}$ 的近端算子。从凸共轭和近端算子的基本定义出发，推导一个恒等式，将 $\\mathrm{prox}_{\\sigma g^{*}}(q)$ 用 $g$ 的近端算子表示出来（对于任意 $\\sigma > 0$ 和 $q \\in \\mathbb{R}^{m}$），然后利用此恒等式为以下实例显式计算对偶更新：\n$a = \\alpha = 1$，$\\sigma = 1$，$y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及 $q = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$。\n\n你的最终答案必须是 $\\mathrm{prox}_{\\sigma g^{*}}(q)$ 在给定数值下的显式值，并表示为单个行向量。无需四舍五入。",
            "solution": "问题陈述已经过验证，被判定为**有效的**。它在凸优化领域具有科学依据，是适定的、客观的。它为得到唯一解提供了所有必要的信息。\n\n问题要求分为两个部分：首先，推导一个普适恒等式，将凸共轭函数 $g^*$ 的近端算子与函数 $g$ 本身的近端算子联系起来；其次，将此恒等式应用于一个具体实例。\n\n**第一部分：Moreau 恒等式的推导**\n\n我们被要求推导 $\\mathrm{prox}_{\\sigma g^{*}}(q)$ 的一个恒等式。我们从近端算子的定义开始：\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = \\arg\\min_{z \\in \\mathbb{R}^{m}} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma g^{*}(z) \\right\\}\n$$\n凸共轭函数 $g^*(z)$ 的定义如下：\n$$\ng^{*}(z) = \\sup_{x \\in \\mathbb{R}^{m}} \\left\\{ \\langle z, x \\rangle - g(x) \\right\\}\n$$\n将共轭函数的定义代入近端算子的定义中，我们得到一个极小极大问题：\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = \\arg\\min_{z} \\sup_{x} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\left( \\langle z, x \\rangle - g(x) \\right) \\right\\}\n$$\n令目标函数为 $L(z, x) = \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\langle z, x \\rangle - \\sigma g(x)$。函数 $L(z,x)$ 对于任意固定的 $x$ 都是关于 $z$ 的凸函数，而对于任意固定的 $z$ 都是关于 $x$ 的仿射函数（因而是凹函数）。定义域是整个空间 $\\mathbb{R}^m$。因此，根据 Sion 极小极大定理，我们可以交换 `min` 和 `sup` 算子：\n$$\n\\sup_{x} \\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\langle z, x \\rangle - \\sigma g(x) \\right\\}\n$$\n我们首先解决关于 $z$ 的内部最小化问题。目标函数是 $z$ 的二次函数。我们通过将关于 $z$ 的梯度置零来找到最小值：\n$$\n\\nabla_{z} L(z, x) = (z - q) + \\sigma x = 0\n$$\n这给出了唯一的极小值点 $z^{*}(x) = q - \\sigma x$。\n\n现在，我们将这个最优的 $z^{*}(x)$ 代回到目标函数中，以解决关于 $x$ 的外部最大化问题：\n$$\n\\sup_{x} \\left\\{ \\frac{1}{2} \\| (q - \\sigma x) - q \\|_{2}^{2} + \\sigma \\langle q - \\sigma x, x \\rangle - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\frac{1}{2} \\| -\\sigma x \\|_{2}^{2} + \\sigma \\langle q, x \\rangle - \\sigma^2 \\langle x, x \\rangle - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} + \\sigma \\langle q, x \\rangle - \\sigma^2 \\| x \\|_{2}^{2} - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\sigma \\langle q, x \\rangle - \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} - \\sigma g(x) \\right\\}\n$$\n解决该极小极大问题的值是最优的 $z^* = q - \\sigma x^*$，其中 $x^*$ 是使上述表达式最大化的自变量。最大化此表达式等价于最小化其相反数：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ -\\sigma \\langle q, x \\rangle + \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} + \\sigma g(x) \\right\\}\n$$\n将目标函数除以正常数 $\\sigma$ 不会改变极小值点：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ -\\langle q, x \\rangle + \\frac{\\sigma}{2} \\| x \\|_{2}^{2} + g(x) \\right\\}\n$$\n为了将其与近端算子联系起来，我们对包含 $x$ 的项进行配方：\n$$\n\\frac{\\sigma}{2} \\| x \\|_{2}^{2} - \\langle q, x \\rangle = \\frac{\\sigma}{2} \\left( \\| x \\|_{2}^{2} - 2\\langle x, \\frac{q}{\\sigma} \\rangle \\right) = \\frac{\\sigma}{2} \\left( \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} - \\left\\| \\frac{q}{\\sigma} \\right\\|_{2}^{2} \\right)\n$$\n将此结果代回 $x^*$ 的最小化问题中：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\frac{\\sigma}{2} \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} - \\frac{\\sigma}{2} \\left\\| \\frac{q}{\\sigma} \\right\\|_{2}^{2} + g(x) \\right\\}\n$$\n项 $-\\frac{\\sigma}{2} \\| q/\\sigma \\|_{2}^{2}$ 是一个关于 $x$ 的常数，可以忽略。将目标函数乘以 $1/\\sigma$ 也不会改变极小值点：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\frac{1}{2} \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} + \\frac{1}{\\sigma} g(x) \\right\\}\n$$\n这正是 $\\mathrm{prox}_{g/\\sigma}(q/\\sigma)$ 的定义。所以，$x^{*} = \\mathrm{prox}_{g/\\sigma}(q/\\sigma)$。\n原始所求量的最终结果是 $z^{*}(x^*)$：\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = q - \\sigma x^{*} = q - \\sigma \\mathrm{prox}_{g/\\sigma}(q/\\sigma)\n$$\n这个关系式被称为 Moreau 恒等式（或 Moreau 分解）。\n\n**第二部分：显式计算**\n\n我们得到以下具体实例：\n$g(z) = \\alpha \\| z - y \\|_{2}$，其中 $\\alpha = 1$，所以 $g(z) = \\| z - y \\|_{2}$。\n参数为 $\\sigma = 1$，$y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及 $q = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$。\n\n使用上面推导的恒等式，并设 $\\sigma = 1$：\n$$\n\\mathrm{prox}_{g^{*}}(q) = q - 1 \\cdot \\mathrm{prox}_{g/1}(q/1) = q - \\mathrm{prox}_{g}(q)\n$$\n我们需要计算 $\\mathrm{prox}_{g}(q)$：\n$$\n\\mathrm{prox}_{g}(q) = \\arg\\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + g(z) \\right\\} = \\arg\\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\| z - y \\|_{2} \\right\\}\n$$\n我们进行变量替换 $u = z - y$，这意味着 $z = u + y$。最小化问题变为：\n$$\n\\arg\\min_{u} \\left\\{ \\frac{1}{2} \\| (u + y) - q \\|_{2}^{2} + \\| u \\|_{2} \\right\\} = \\arg\\min_{u} \\left\\{ \\frac{1}{2} \\| u - (q - y) \\|_{2}^{2} + \\| u \\|_{2} \\right\\}\n$$\n这是欧几里得范数的近端算子 $\\mathrm{prox}_{\\|\\cdot\\|_2}(\\cdot)$ 在点 $w = q-y$ 处的定义。$\\mathrm{prox}_{\\lambda\\|\\cdot\\|_2}(w)$ 的通解由向量软阈值给出：\n$$\n\\mathrm{prox}_{\\lambda\\|\\cdot\\|_2}(w) = \\max\\left(0, 1 - \\frac{\\lambda}{\\|w\\|_2}\\right) w = \\begin{cases} \\left(1 - \\frac{\\lambda}{\\|w\\|_2}\\right) w  \\text{if } \\|w\\|_2  \\lambda \\\\ 0  \\text{if } \\|w\\|_2 \\le \\lambda \\end{cases}\n$$\n在我们的例子中，范数的系数是 1，所以 $\\lambda=1$。我们来计算 $w = q - y$：\n$$\nw = q - y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n$$\nw 的欧几里得范数是：\n$$\n\\| w \\|_{2} = \\sqrt{0^2 + 2^2} = 2\n$$\n由于 $\\|w\\|_2 = 2  1 = \\lambda$，我们使用公式的第一种情况。最优的 $u$ 是：\n$$\nu_{\\mathrm{opt}} = \\mathrm{prox}_{\\|\\cdot\\|_2}(w) = \\left(1 - \\frac{1}{\\|w\\|_2}\\right) w = \\left(1 - \\frac{1}{2}\\right) w = \\frac{1}{2} w = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n原问题关于 $z$ 的极小值点是 $z_{\\mathrm{opt}} = u_{\\mathrm{opt}} + y$。因此，\n$$\n\\mathrm{prox}_{g}(q) = z_{\\mathrm{opt}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n$$\n最后，我们可以计算所求的对偶更新项：\n$$\n\\mathrm{prox}_{g^{*}}(q) = q - \\mathrm{prox}_{g}(q) = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 4-3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "为确保 PDHG 算法收敛，原始步长 $\\tau$ 和对偶步长 $\\sigma$ 必须满足一个关键的稳定性条件。该条件取决于线性算子 $K$ 的谱范数。这个动手编程练习将向你展示如何实现幂迭代法，这是一种高效的数值技术，可以在不显式构造大型矩阵的情况下估计该范数。通过实现这一方法，你将掌握一项在任何基于 PDHG 的求解器中稳健设置超参数的实用技能。",
            "id": "3413760",
            "problem": "考虑一个由实数矩阵 $K \\in \\mathbb{R}^{m \\times n}$ 表示的线性正向算子。在用于解决逆问题和数据同化的原始-对偶混合梯度（PDHG）方法中，原始步长 $ \\tau $ 和对偶步长 $ \\sigma $ 必须满足稳定性条件 $ \\tau \\sigma \\|K\\|^2  1 $，其中 $ \\|K\\| $ 表示由欧几里得范数诱导的算子范数，即谱范数。从定义 $ \\|K\\| = \\sup_{\\|x\\|_2 = 1} \\|Kx\\|_2 $ 以及 $ \\|K\\| $ 与 $K$ 的最大奇异值之间的关系出发，推导出一个仅使用与 $K$ 和 $K^\\top$ 的矩阵-向量乘积来估计通用（可能为矩形）矩阵 $ \\|K\\| $ 的过程。使用此估计值来选择 $ \\tau $ 和 $ \\sigma $，以使 $ \\tau \\sigma \\|K\\|^2  1 $ 严格成立，并带有一个可调的安全系数 $ s \\in (0,1) $，该系数控制 $ \\tau \\sigma \\|K\\|^2 $ 与 $ 1 $ 的接近程度。为确保在退化情况下的行为有明确定义，采用以下约定：如果 $ \\|K\\| $ 的估计值在某个容差范围内数值上为零，则设置 $ \\tau = 1 $ 和 $ \\sigma = 1 $。\n\n你的任务是实现一个完整的程序，该程序：\n- 将单位向量 $ v_0 \\in \\mathbb{R}^n $ 确定性地初始化为归一化的全1向量。\n- 仅使用操作 $ v \\mapsto Kv $ 和 $ u \\mapsto K^\\top u $，对对称半正定矩阵 $ K^\\top K $ 应用幂迭代，从而估计 $ K^\\top K $ 的最大特征值，并因此得到 $ \\|K\\| = \\sqrt{\\lambda_{\\max}(K^\\top K)} $。\n- 使用估计值 $ \\widehat{\\|K\\|} $，根据保证 $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $（带有提供的安全系数 $ s \\in (0,1) $）的规则来选择 $ \\tau $ 和 $ \\sigma $，以及在 $ \\widehat{\\|K\\|} $ 数值上为零时的特殊情况规则 $ \\tau = \\sigma = 1 $。\n- 为每个测试用例报告一个包含五个条目的列表：估计的谱范数 $ \\widehat{\\|K\\|} $（浮点数）、选择的 $ \\tau $（浮点数）、选择的 $ \\sigma $（浮点数）、乘积 $ \\tau \\sigma \\widehat{\\|K\\|}^2 $（浮点数）以及一个布尔值，指示 $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $ 是否严格满足。\n\n使用以下测试套件，它涵盖了各种矩阵形状和条件数场景：\n- 测试用例 $ 1 $ (正常路径，矩形): $ K_1 = \\begin{bmatrix} 3  0  0 \\\\ 0  1  2 \\end{bmatrix} $, $ s = 0.9 $。\n- 测试用例 $ 2 $ (边界邻近情况，方形单位阵): $ K_2 = I_4 $，其中 $ I_4 $ 是 $ 4 \\times 4 $ 的单位矩阵，$ s = 0.99 $。\n- 测试用例 $ 3 $ (退化零算子，矩形): $ K_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\\\ 0  0 \\end{bmatrix} $, $ s = 0.5 $。\n- 测试用例 $ 4 $ (病态尺度，矩形): $ K_4 = \\begin{bmatrix} 10^{-3}  0  0 \\\\ 0  10  0 \\\\ 0  0  10^{-1} \\\\ 0  0  0 \\end{bmatrix} $, $ s = 0.95 $。\n\n实现要求：\n- 使用确定性初始化 $ v_0 = \\frac{1}{\\sqrt{n}} \\mathbf{1}_n $。\n- 对幂迭代法使用 $ 10^{-12} $ 的相对变化容差和最多 $ 10000 $ 次迭代。\n- 当 $ \\widehat{\\|K\\|} > 0 $ 时，$ \\tau $ 和 $ \\sigma $ 的选择规则必须是 $ \\tau = \\frac{s}{\\widehat{\\|K\\|}} $ 和 $ \\sigma = \\frac{s}{\\widehat{\\|K\\|}} $；否则，$ \\tau = 1 $，$ \\sigma = 1 $。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个格式为 $[\\widehat{\\|K\\|}, \\tau, \\sigma, \\tau \\sigma \\widehat{\\|K\\|}^2, \\text{boolean}]$ 的列表。例如，整体输出应类似于 $[[\\dots],[\\dots],[\\dots],[\\dots]]$，不带任何额外文本。",
            "solution": "该问题要求开发一个程序，为原始-对偶混合梯度（PDHG）方法选择原始步长 $ \\tau $ 和对偶步长 $ \\sigma $。该选择必须满足稳定性条件 $ \\tau \\sigma \\|K\\|^2  1 $，其中 $ K \\in \\mathbb{R}^{m \\times n} $ 是线性正向算子，$ \\|K\\| $ 是其谱范数。任务的核心是使用一种仅依赖于涉及 $ K $ 及其转置 $ K^\\top $ 的矩阵-向量乘积的迭代数值方法来估计 $ \\|K\\| $。\n\n### 理论基础：谱范数与特征值\n\n矩阵 $ K $ 的谱范数，记为 $ \\|K\\| $ 或 $ \\|K\\|_2 $，定义为 $ K $ 拉伸向量 $ x \\in \\mathbb{R}^n $ 的最大因子：\n$$ \\|K\\| = \\sup_{\\|x\\|_2 = 1} \\|Kx\\|_2 $$\n其中 $ \\| \\cdot \\|_2 $ 是欧几里得范数。一个等价且在计算上更有用的特征将谱范数与 $ K $ 的奇异值联系起来。谱范数恰好是 $ K $ 的最大奇异值，记为 $ \\sigma_{\\max}(K) $。\n\n根据定义，$ K $ 的奇异值是对称半正定矩阵 $ K^\\top K $ 的特征值的平方根。设 $ K^\\top K $ 的特征值为 $ \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0 $。那么 $ K $ 的奇异值为 $ \\sigma_i = \\sqrt{\\lambda_i} $。因此，最大奇异值对应于最大特征值：\n$$ \\|K\\| = \\sigma_{\\max}(K) = \\sqrt{\\lambda_{\\max}(K^\\top K)} $$\n这种关系将寻找 $ K $ 的谱范数问题转化为寻找矩阵 $ A = K^\\top K $ 的最大特征值问题。\n\n### 算法方法：幂迭代法\n\n幂迭代法（或称幂法）是用于寻找矩阵最大模特征值（主特征值）的标准算法。对于矩阵 $ A = K^\\top K $，由于它是对称半正定的，其所有特征值都是实数且非负。因此，主特征值就是最大特征值 $ \\lambda_{\\max}(A) $。\n\n幂迭代算法的步骤如下：\n1.  初始化一个向量 $ v_0 \\in \\mathbb{R}^n $，使其 $ \\|v_0\\|_2 = 1 $。问题指定了一个确定性的初始化方法：$ v_0 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n $，其中 $ \\mathbf{1}_n $ 是全1向量。\n2.  对 $ k = 0, 1, 2, \\dots $进行迭代：\n    $$ v_{k+1} = \\frac{A v_k}{\\|A v_k\\|_2} $$\n    当 $ k \\to \\infty $ 时，只要 $ \\lambda_{\\max} $ 的模严格大于所有其他特征值的模，并且初始向量 $ v_0 $ 在该特征向量方向上有非零分量，向量 $ v_k $ 就会收敛到对应于 $ \\lambda_{\\max}(A) $ 的特征向量。\n\n在每一步都可以估计特征值 $ \\lambda_{\\max}(A) $。一个常见的估计是瑞利商 $ \\lambda^{(k)} = v_k^\\top A v_k $。一个更简单、直接从迭代中产生的估计是 $ \\lambda^{(k+1)} = \\|A v_k\\|_2 $。当 $ v_k $ 接近真实特征向量时，$ A v_k \\approx \\lambda_{\\max} v_k $，所以 $ \\|A v_k\\|_2 \\approx |\\lambda_{\\max}| \\|v_k\\|_2 = \\lambda_{\\max} $。\n\n问题的一个关键要求是仅使用与 $ K $ 和 $ K^\\top $ 的乘法，而不显式地构造矩阵 $ A = K^\\top K $，因为对于大规模问题，这在计算上可能非常昂贵或不可行。矩阵-向量乘积 $ A v_k $ 可以通过以下两个操作序列来计算：\n1.  计算向量 $ u_k = K v_k $。\n2.  计算向量 $ w_{k+1} = K^\\top u_k = K^\\top (K v_k) $。\n\n完整的迭代步骤如下：\n$ w_{k+1} = K^\\top K v_k $\n$ \\lambda_{k+1} = \\|w_{k+1}\\|_2 $ (这是我们对 $ \\lambda_{\\max} $ 的估计)\n$ v_{k+1} = \\frac{w_{k+1}}{\\lambda_{k+1}} $ (为下一步进行归一化)\n\n当特征值估计值的相对变化小于指定的容差 $ \\epsilon = 10^{-12} $（即 $ \\frac{|\\lambda_{k+1} - \\lambda_k|}{|\\lambda_{k+1}|}  \\epsilon $），或者达到最大迭代次数（$10000$）时，循环终止。\n\n### 步长选择流程\n\n估计谱范数并选择步长 $ \\tau $ 和 $ \\sigma $ 的完整流程如下：\n\n1.  **估计 $ \\lambda_{\\max}(K^\\top K) $**：\n    -   初始化 $ v_0 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n $ 和一个初始特征值估计，例如 $ \\lambda_0 = 0 $。\n    -   如上所述，迭代计算 $ v_{k+1} $ 和 $ \\lambda_{k+1} $ 直至收敛。设最终估计值为 $ \\hat{\\lambda}_{\\max} $。\n\n2.  **估计 $ \\|K\\| $**：\n    -   谱范数估计为 $ \\widehat{\\|K\\|} = \\sqrt{\\hat{\\lambda}_{\\max}} $。\n\n3.  **选择 $ \\tau $ 和 $ \\sigma $**：\n    -   引入一个安全系数 $ s \\in (0,1) $ 以确保稳定性条件严格成立。\n    -   **情况1：$ \\widehat{\\|K\\|} > 0 $**（典型情况）。对称地选择步长：\n        $$ \\tau = \\frac{s}{\\widehat{\\|K\\|}} \\quad \\text{和} \\quad \\sigma = \\frac{s}{\\widehat{\\|K\\|}} $$\n        通过这种选择，稳定性表达式变为：\n        $$ \\tau \\sigma \\widehat{\\|K\\|}^2 = \\left(\\frac{s}{\\widehat{\\|K\\|}}\\right) \\left(\\frac{s}{\\widehat{\\|K\\|}}\\right) \\widehat{\\|K\\|}^2 = s^2 $$\n        由于 $ s \\in (0,1) $，可以保证 $ s^2  1 $，从而满足条件 $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $。\n    -   **情况2：$ \\widehat{\\|K\\|} $ 在数值上为零**。如果 $ K $ 是零矩阵，则会发生这种情况。问题指定了以下约定：\n        $$ \\tau = 1 \\quad \\text{和} \\quad \\sigma = 1 $$\n        在这种情况下，乘积 $ \\tau \\sigma \\widehat{\\|K\\|}^2 = 1 \\cdot 1 \\cdot 0^2 = 0 $，也满足稳定性条件 $ 0  1 $。\n\n此流程提供了一种稳健且标准的方法来配置PDHG算法的步长，确保稳定性的同时仅使用基本的矩阵运算。最终的实现将把这个完整的逻辑应用于每个给定的测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of selecting PDHG step sizes for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[3.0, 0.0, 0.0], [0.0, 1.0, 2.0]]), 0.9),\n        (np.identity(4), 0.99),\n        (np.zeros((3, 2)), 0.5),\n        (np.array([[1e-3, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 1e-1], [0.0, 0.0, 0.0]]), 0.95),\n    ]\n\n    results = []\n    \n    # Define constants for the power iteration\n    max_iter = 10000\n    rel_tol = 1e-12\n    # Tolerance for checking if a value is numerically zero\n    zero_tol = 1e-14 \n\n    for K, s in test_cases:\n        m, n = K.shape\n\n        # Initialize the vector v0 deterministically.\n        if n > 0:\n            v = np.ones(n) / np.sqrt(n)\n        else: # Handle zero-column matrix\n            v = np.array([])\n            \n        lambda_est = 0.0\n\n        # Power iteration to find the largest eigenvalue of K^T * K\n        # If K is the zero matrix, K.T @ K @ v will be zero, lambda_est will remain 0.\n        if n > 0:\n            for _ in range(max_iter):\n                # Apply operator K^T * K without forming the matrix explicitly\n                K_v = K @ v\n                Kt_K_v = K.T @ K_v\n    \n                lambda_new = np.linalg.norm(Kt_K_v)\n                \n                # Check for convergence\n                if lambda_new  zero_tol:\n                    lambda_est = 0.0\n                    break\n                \n                # Normalize the vector for the next iteration\n                v = Kt_K_v / lambda_new\n    \n                # Relative change stopping criterion\n                relative_change = abs(lambda_new - lambda_est) / lambda_new\n                lambda_est = lambda_new\n    \n                if relative_change  rel_tol:\n                    break\n        \n        # Estimate the spectral norm\n        norm_K_est = np.sqrt(lambda_est)\n\n        # Select tau and sigma based on the estimated norm\n        if norm_K_est > zero_tol:\n            tau = s / norm_K_est\n            sigma = s / norm_K_est\n        else:\n            # Degenerate case rule: if norm is numerically zero\n            norm_K_est = 0.0 # Clean up tiny numerical noise\n            tau = 1.0\n            sigma = 1.0\n\n        # Calculate the stability product and check the condition\n        product = tau * sigma * norm_K_est**2\n        is_stable = product  1.0\n\n        # Append the list of results for this test case\n        results.append([norm_K_est, tau, sigma, product, is_stable])\n\n    # Final print statement in the exact required format.\n    # The string representation of a Python list includes spaces, which is\n    # consistent with the provided template's use of map(str, ...).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在许多现实世界的应用中，例如数据同化，观测数量是巨大的，这使得计算效率至关重要。本问题探讨如何利用优化问题的数学结构来设计高效的并行算法。你将分析数据失配项 $g$ 的可分离性如何允许 PDHG 方法的对偶更新步骤分解为许多可以并行执行的、更小的独立任务，这是将算法扩展到大规模系统的关键策略。",
            "id": "3413769",
            "problem": "考虑一个凸变分数据同化问题，其形式是最小化一个背景项（或模型一致性项）与一个数据失配项之和：\n$$\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + g(Kx),$$\n其中 $f:\\mathbb{R}^{n}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 和 $g:\\mathbb{R}^{m}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 是真、凸、下半连续函数，而 $K \\in \\mathbb{R}^{m\\times n}$ 是由观测算子以及可能的模型线性化产生的线性算子。假设观测数据被分为 $J$ 个块（例如，按传感器或时间划分），从而产生块行结构 $K = \\begin{bmatrix} K_{1} \\\\ \\vdots \\\\ K_{J} \\end{bmatrix}$ 以及 $z = Kx$ 相应地分解为块 $z = (z_{1},\\dots,z_{J})$，其中 $z_{j} = K_{j}x \\in \\mathbb{R}^{m_{j}}$ 且 $\\sum_{j=1}^{J} m_{j} = m$。假设数据失配项在观测块上是可分的，即\n$$g(z) \\;=\\; \\sum_{j=1}^{J} g_{j}(z_{j}), \\qquad z = (z_{1},\\dots,z_{J}).$$\n\n采用对角步长的原始-对偶混合梯度（PDHG）方法使用对偶和原始更新\n$$y^{k+1} \\;=\\; \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right), \\qquad x^{k+1} \\;=\\; \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right),$$\n其中 $\\bar{x}^{k} = x^{k} + \\theta(x^{k} - x^{k-1})$ 为外推，$\\Sigma \\succeq 0$ 和 $T \\succeq 0$ 为对角步长算子，而 $g^{\\ast}$ 是 $g$ 的 Fenchel 共轭。在具有许多观测的高维设置中，$K$ 通常非常稀疏，并且块 $\\{K_{j}\\}_{j=1}^{J}$ 具有与局部化或时间上孤立的测量相对应的小支撑集。\n\n在数据同化中，以下哪项最好地描述了一种利用 $K$ 的稀疏性和块结构来跨观测块并行化对偶更新的正确且收敛安全的策略？\n\nA. 将 $K$ 按块行划分为 $K = \\begin{bmatrix} K_{1} \\\\ \\cdots \\\\ K_{J} \\end{bmatrix}$，与可分分解 $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$ 对齐。使用块对角对偶步长 $\\Sigma = \\operatorname{diag}(\\sigma_{1} I_{m_{1}},\\dots,\\sigma_{J} I_{m_{J}})$ 和原始步长 $T = \\tau I_{n}$，选择它们以满足标准的 PDHG 条件 $\\lVert \\Sigma^{1/2} K T^{1/2} \\rVert  1$（例如，通过基于块范数的对角预处理）。然后，对所有 $j$ 并行计算块残差 $r_{j}^{k} = K_{j}\\bar{x}^{k}$ 和对偶块更新 $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} r_{j}^{k})$。最后，通过归约（reduction）形成 $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$ 并执行原始更新。这将产生每个迭代中带有一个全局归约的易并行（embarrassingly parallel）对偶更新。\n\nB. 按列划分 $K$ 并通过独立计算局部 $x$ 更新来并行化原始步骤，同时保持一个单一的整体对偶向量 $y$。对偶更新必须保持中心化，因为 $g$ 依赖于所有观测，因此试图分离 $y$ 会违反凸性，并且对于收敛来说是不必要的。\n\nC. 通过在每次迭代中使用应用于 $K K^{\\top}$ 的 Jacobi 方法求解 $y$ 的正规方程来替代对偶邻近算子，因为凸性意味着邻近上升与求解相关的最优性条件是等价的。这使得并行化成为可能，而无需 $g$ 的块可分性。\n\nD. 将状态 $x$ 分割成重叠的空间子域，并完全异步地更新 $x$ 和所有对偶块 $y_{j}$，无需同步或全局归约，因为 $K$ 的稀疏性意味着 $K^{\\top} y$ 可以在局部近似而不会影响收敛保证。\n\nE. 用一个块对角近似 $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ 替换 $K$，该近似通过观测者之间的“闲聊”（gossip）算法构建，以消除块间耦合，然后对 $(f,g,\\tilde{K})$ 执行中心化的 PDHG。块间项的损失在实践中可以忽略不计，并且不会影响对原始解的收敛。",
            "solution": "用户要求对问题陈述进行严格验证，然后推导正确解并评估所有提供的选项。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n问题是一个形如下式的凸优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + g(Kx)\n$$\n- **函数**：$f:\\mathbb{R}^{n}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 和 $g:\\mathbb{R}^{m}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 是真、凸、下半连续函数。\n- **算子**：$K \\in \\mathbb{R}^{m\\times n}$ 是一个线性算子。\n- **块结构**：算子 $K$ 具有块行结构 $K = \\begin{bmatrix} K_{1} \\\\ \\vdots \\\\ K_{J} \\end{bmatrix}$，其中 $K_j \\in \\mathbb{R}^{m_j \\times n}$ 且 $\\sum_{j=1}^{J} m_{j} = m$。这导致 $z=Kx$ 也具有块结构 $z = (z_1, \\dots, z_J)$，其中 $z_j = K_j x$。\n- **可分性**：函数 $g$ 关于此块结构是可分的：$g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$。\n- **算法**：考虑使用原始-对偶混合梯度（PDHG）方法，其更新步骤为：\n  $$\n  y^{k+1} \\;=\\; \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right)\n  $$\n  $$\n  x^{k+1} \\;=\\; \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right)\n  $$\n- **算法参数**：\n    - $\\bar{x}^{k} = x^{k} + \\theta(x^{k} - x^{k-1})$ 是一个外推步骤（对于标准算法，$\\theta=1$）。\n    - $\\Sigma \\succeq 0$ 和 $T \\succeq 0$ 是对角步长算子。\n    - $g^{\\ast}$ 是 $g$ 的 Fenchel 共轭。\n- **背景**：设置是具有稀疏算子 $K$ 的高维数据同化。\n- **问题**：任务是确定一种正确且收敛安全的策略，以跨 $J$ 个观测块并行化对偶更新。\n\n**步骤2：使用提取的已知条件进行验证**\n\n1.  **科学依据**：该问题表述在逆问题、数据同化和机器学习领域是标准的。使用PDHG（也称为Chambolle-Pock算法）解决此类问题是现代凸优化的基石。函数（$f, g$）和算子（$K$）的性质是保证算法收敛的标准假设。该问题在数学优化理论中有坚实的基础。\n2.  **适定的**：问题陈述是适定的。它描述了一个标准的优化问题和一个标准的求解算法。问题要求一种有效的并行化策略，这是一个在此背景下定义明确的计算机科学和数值分析问题。\n3.  **客观的**：语言精确且客观。所有术语，如“真、凸、下半连续”、“Fenchel共轭”和“邻近算子”，都有严格的数学定义。\n4.  **完整性**：问题提供了所有必要的信息。问题的结构（$\\min f(x)+\\sum_j g_j(K_j x)$）和PDHG算法的形式都已明确说明，这足以分析并行化策略。\n\n**步骤3：结论与行动**\n\n问题陈述是**有效的**。它科学上合理、适定且完整。我现在将继续推导解决方案并评估各个选项。\n\n### 解法推导\n\n问题的核心在于对偶更新的结构：\n$$\ny^{k+1} = \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right)\n$$\n能否并行化此更新取决于函数 $g$ 的可分性以及步长算子 $\\Sigma$ 的相应结构。\n\n问题陈述指出 $g(z) = \\sum_{j=1}^{J} g_j(z_j)$，其中 $z = (z_1, \\dots, z_J)$ 是 $z$ 的块分解。Fenchel 共轭的一个基本性质是，对于可分函数，其共轭也是可分的：\n$$\ng^{\\ast}(y) = \\left(\\sum_{j=1}^{J} g_j\\right)^{\\ast}(y) = \\sum_{j=1}^{J} g_j^{\\ast}(y_j),\n$$\n其中 $y = (y_1, \\dots, y_J)$ 是对偶变量相应的块分解。\n\n邻近算子定义为：\n$$\n\\operatorname{prox}_{\\Sigma g^{\\ast}}(v) = \\arg\\min_{y} \\left\\{ g^{\\ast}(y) + \\frac{1}{2} \\| y - v \\|_{\\Sigma^{-1}}^2 \\right\\}\n$$\n我们选择一个与 $y$ 的块结构一致的块对角对偶步长算子 $\\Sigma$：\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_1  0  \\cdots  0 \\\\ 0  \\Sigma_2  \\cdots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 0  0  \\cdots  \\Sigma_J \\end{pmatrix},\n$$\n其中 $\\Sigma_j$ 是 $\\mathbb{R}^{m_j}$ 上的半正定算子。通过这种选择，二次惩罚项也变得可分：\n$$\n\\| y - v \\|_{\\Sigma^{-1}}^2 = (y-v)^\\top \\Sigma^{-1} (y-v) = \\sum_{j=1}^{J} (y_j - v_j)^\\top \\Sigma_j^{-1} (y_j - v_j) = \\sum_{j=1}^{J} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2.\n$$\n因此，邻近算子中的最小化问题解耦为 $J$ 个独立的子问题：\n$$\n\\arg\\min_{y} \\left\\{ \\sum_{j=1}^{J} g_j^{\\ast}(y_j) + \\sum_{j=1}^{J} \\frac{1}{2} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2 \\right\\} = \\sum_{j=1}^{J} \\arg\\min_{y_j} \\left\\{ g_j^{\\ast}(y_j) + \\frac{1}{2} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2 \\right\\}.\n$$\n这意味着更新后的对偶变量的第 $j$ 个块 $y_j^{k+1}$ 可以计算为：\n$$\ny_j^{k+1} = \\operatorname{prox}_{\\Sigma_j g_j^{\\ast}}(v_j),\n$$\n其中 $v_j$ 是参数 $v = y^k + \\Sigma K \\bar{x}^k$ 的第 $j$ 个块。块 $v_j$ 由下式给出：\n$$\nv_j = y_j^k + (\\Sigma K \\bar{x}^k)_j = y_j^k + \\Sigma_j K_j \\bar{x}^k.\n$$\n因此，对偶更新可以对每个块 $j=1, \\dots, J$ 并行执行：\n$$\ny_j^{k+1} = \\operatorname{prox}_{\\Sigma_j g_j^{\\ast}}\\!\\left(y_j^{k} + \\Sigma_j K_j \\bar{x}^k\\right).\n$$\n这是一个易并行的计算，前提是所有处理单元都可以获得完整的向量 $\\bar{x}^k$。\n\n现在，考虑原始更新：\n$$\nx^{k+1} = \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right).\n$$\n关键项是 $K^\\top y^{k+1}$。给定 $K$ 和 $y$ 的块结构，这个矩阵向量乘积展开为：\n$$\nK^\\top y^{k+1} = \\begin{bmatrix} K_1^\\top  K_2^\\top  \\cdots  K_J^\\top \\end{bmatrix} \\begin{bmatrix} y_1^{k+1} \\\\ y_2^{k+1} \\\\ \\vdots \\\\ y_J^{k+1} \\end{bmatrix} = \\sum_{j=1}^{J} K_j^\\top y_j^{k+1}.\n$$\n此操作需要一个同步步骤：在并行计算完所有 $y_j^{k+1}$ 之后，必须计算乘积 $K_j^\\top y_j^{k+1}$（这也可以并行完成），然后将它们相加。这是一个经典的 map-reduce 或归约操作。得到的和随后用于 $x^{k+1}$ 的单一、中心化的原始更新。\n\n当原始和对偶步长被选择以使算子范数 $\\|\\Sigma^{1/2} K T^{1/2}\\|  1$ 时，带有矩阵步长 $\\Sigma, T$ 的 PDHG 方法的收敛性得到保证。这是标准的收敛条件。\n\n### 逐项分析\n\n**A. 将 $K$ 按块行划分为 $K = \\begin{bmatrix} K_{1} \\\\ \\dots \\\\ K_{J} \\end{bmatrix}$，与可分分解 $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$ 对齐。使用块对角对偶步长 $\\Sigma = \\operatorname{diag}(\\sigma_{1} I_{m_{1}},\\dots,\\sigma_{J} I_{m_{J}})$ 和原始步长 $T = \\tau I_{n}$，选择它们以满足标准的 PDHG 条件 $\\lVert \\Sigma^{1/2} K T^{1/2} \\rVert  1$（例如，通过基于块范数的对角预处理）。然后，对所有 $j$ 并行计算块残差 $r_{j}^{k} = K_{j}\\bar{x}^{k}$ 和对偶块更新 $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} r_{j}^{k})$。最后，通过归约（reduction）形成 $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$ 并执行原始更新。这将产生每个迭代中带有一个全局归约的易并行（embarrassingly parallel）对偶更新。**\n\n这个选项与上面的推导完全匹配。它正确地指出，$K$ 的块行划分与 $g$ 的可分性以及块对角步长 $\\Sigma$ 相结合，可以解耦对偶更新。具体形式 $\\Sigma_j = \\sigma_j I_{m_j}$ 是一个常见的实用选择。对偶更新公式 $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} K_j \\bar{x}^{k})$ 是正确的。用于原始更新的归约步骤的正确形式 $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$ 也被陈述。引用的收敛条件是此设置下的正确条件。将计算模式描述为“每个迭代中带有一个全局归约的易并行对偶更新”是准确的。\n**结论：正确**\n\n**B. 按列划分 $K$ 并通过独立计算局部 $x$ 更新来并行化原始步骤，同时保持一个单一的整体对偶向量 $y$。对偶更新必须保持中心化，因为 $g$ 依赖于所有观测，因此试图分离 $y$ 会违反凸性，并且对于收敛来说是不必要的。**\n\n这个选项在多个方面存在缺陷。首先，它建议通过按列划分 $K$ 来并行化原始步骤，这是一种不同的策略（一种原始分解形式）。其次，其理由是不正确的。问题明确指出 $g$ 是可分的（$g(z) = \\sum_j g_j(z_j)$），这与它“依赖于所有观测”且不可分的说法相矛盾。这种可分性正是允许并行对偶更新的原因。声称分离 $y$ “会违反凸性”是毫无根据的；$g^*$ 的可分性是 $g$ 的凸性和可分性的直接结果。\n**结论：不正确**\n\n**C. 通过在每次迭代中使用应用于 $K K^{\\top}$ 的 Jacobi 方法求解 $y$ 的正规方程来替代对偶邻近算子，因为凸性意味着邻近上升与求解相关的最优性条件是等价的。这使得并行化成为可能，而无需 $g$ 的块可分性。**\n\n这个选项是不正确的。用求解线性“正规方程”替代邻近算子仅在 $g$（以及因此的 $g^*$）是二次函数（特别是 $\\ell_2$-范数的平方）时才有效。对于一般的凸函数 $g$，邻近算子是一个非线性映射。“凸性意味着等价”这一说法是严重的过度简化；虽然邻近算子求解一个最优性条件，但这个条件通常不是一个线性系统。此外，它错误地暗示这在没有 $g$ 的可分性的情况下也有效。如果 $g$ 不可分，其邻近算子就不可分，无论使用何种方法计算邻近算子，对偶更新的并行化都不是直接的。\n**结论：不正确**\n\n**D. 将状态 $x$ 分割成重叠的空间子域，并完全异步地更新 $x$ 和所有对偶块 $y_{j}$，无需同步或全局归约，因为 $K$ 的稀疏性意味着 $K^{\\top} y$ 可以在局部近似而不会影响收敛保证。**\n\n这个选项提出了一个完全异步的方案，并做出了一些通常是错误的强断言。虽然异步优化算法存在，但它们有严格的理论要求，并且其收敛保证通常与同步对应算法不同。声称 $K^\\top y$ 可以“在局部近似而不会影响收敛保证”是错误的。$K$ 的稀疏性意味着和 $K^\\top y = \\sum_j K_j^\\top y_j$ 的任何分量都只有少数非零项，使其计算高效。但这并*不*意味着可以忽略非局部贡献而仍然收敛到正确的解。这样的近似会改变算法，使其解决一个不同的问题。该策略不是所要求的“收敛安全”的。\n**结论：不正确**\n\n**E. 用一个块对角近似 $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ 替换 $K$，该近似通过观测者之间的“闲聊”（gossip）算法构建，以消除块间耦合，然后对 $(f,g,\\tilde{K})$ 执行中心化的 PDHG。块间项的损失在实践中可以忽略不计，并且不会影响对原始解的收敛。**\n\n这个选项从根本上是有缺陷的，因为它建议解决一个不同的问题。通过用近似 $\\tilde{K}$ 替换 $K$，算法将收敛到 $f(x) + g(\\tilde{K}x)$ 的最小化子，而不是原始目标函数。声称这“不影响对原始解的收敛”是错误的。这种近似在某些应用中可能是一种有效的启发式方法，但它不是解决原始问题的正确策略。$\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ 的构造也是定义不当的，因为每个 $K_j$ 都是一个大小为 $m_j \\times n$ 的“胖”矩阵；块对角排列需要对原始空间 $\\mathbb{R}^n$ 进行划分，而这并未给出。\n**结论：不正确**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}