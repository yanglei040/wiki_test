{
    "hands_on_practices": [
        {
            "introduction": "掌握任何算法的第一步都是理解其核心机制。本练习将通过一个经典的稀疏线性回归问题（LASSO），引导你手动完成一次完整的ADMM迭代。这个过程将揭示该算法的内部运作，展示如何将一个复杂问题分解为多个更简单的子问题——一个二次优化和一个近端算子求值——并通过对偶变量的更新将它们联系起来。",
            "id": "3476975",
            "problem": "考虑带 $\\ell_{1}$ 惩罚项（最小绝对收缩和选择算子）的稀疏线性回归问题，该问题旨在寻找 $\\beta \\in \\mathbb{R}^{p}$ 以最小化目标函数 $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$，其中数据为 $X \\in \\mathbb{R}^{m \\times p}$，$y \\in \\mathbb{R}^{m}$。为了对 $\\ell_{1}$ 项进行变量分裂，引入一个辅助变量 $z \\in \\mathbb{R}^{p}$ 和约束条件 $z = \\beta$，得到等价的约束优化问题 $\\min_{\\beta, z} \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1}$，约束条件为 $z = \\beta$。交替方向乘子法 (ADMM) 通过增广拉格朗日函数以及对原始变量和缩放对偶变量的交替更新来解决此类问题。\n\n利用凸优化和近端算子的基本定义，推导此约束形式下交替方向乘子法 (ADMM) 的单次迭代更新公式，并对以下示例进行一次完整迭代的显式计算。数据矩阵和响应向量为\n$$\nX = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix},\n$$\n正则化参数 $\\lambda = 1$，惩罚参数 $\\rho = 2$，初始迭代值为\n$$\n\\beta^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad z^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n从给定的迭代值开始，执行一次完整的交替方向乘子法 (ADMM) 迭代，以闭合形式精确计算更新后的变量 $\\beta^{k+1}$、 $z^{k+1}$ 和 $u^{k+1}$。将最终答案以单行矩阵的形式报告，按顺序陈列六个标量条目：$\\beta^{k+1}_{1}$、 $\\beta^{k+1}_{2}$、 $z^{k+1}_{1}$、 $z^{k+1}_{2}$、 $u^{k+1}_{1}$、 $u^{k+1}_{2}$。不要四舍五入；将所有有理数值表示为最简分数形式。不涉及物理单位。",
            "solution": "用户要求针对一个稀疏线性回归问题，推导并计算交替方向乘子法 (ADMM) 的一次迭代。\n\n### 第 1 步：提取已知条件\n问题是最小化目标函数 $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$。\n这被重构为一个等价的约束问题：\n$$\n\\min_{\\beta, z} \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{约束条件为} \\quad z = \\beta\n$$\n具体实例的数据和参数如下：\n- 数据矩阵：$X = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}$\n- 响应向量：$y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$\n- 正则化参数：$\\lambda = 1$\n- ADMM 惩罚参数：$\\rho = 2$\n- 初始迭代值（在第 $k$ 步）：$\\beta^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$， $z^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$， $u^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n\n任务是计算下一次的迭代值 $\\beta^{k+1}$、 $z^{k+1}$ 和 $u^{k+1}$。\n\n### 第 2 步：使用提取的已知条件进行验证\n问题陈述在科学上是合理的、良定的和客观的。\n- **科学依据：**该问题使用 LASSO 目标函数，这是高维统计和机器学习中的一种标准技术。变量分裂和 ADMM 是解决此类问题的成熟方法。该公式在数学上是正确的。\n- **良定性：**该问题提供了计算一次迭代唯一解所需的所有数据和初始条件。目标函数是凸函数，确保了 ADMM 中的子问题是良定的。对于 $\\rho  0$，矩阵 $X^T X + \\rho I$ 是可逆的，这保证了 $\\beta$ 更新步骤有唯一解。\n- **客观性：**该问题使用精确的数学语言陈述，没有任何歧义或主观因素。\n\n该问题是有效的。\n\n### 第 3 步：ADMM 更新公式的推导\n将 ADMM 算法应用于该约束形式。约束可以写成 $\\beta - z = 0$。\n其缩放对偶变量形式的增广拉格朗日函数为：\n$$\nL_{\\rho}(\\beta, z, u) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta - z + u\\|_{2}^{2}\n$$\n这里，$u$ 是缩放对偶变量。ADMM 迭代包含三个步骤：\n1. 关于 $\\beta$ 的最小化：$\\beta^{k+1} = \\arg\\min_{\\beta} L_{\\rho}(\\beta, z^k, u^k)$\n2. 关于 $z$ 的最小化：$z^{k+1} = \\arg\\min_{z} L_{\\rho}(\\beta^{k+1}, z, u^k)$\n3. 对偶变量的更新：$u^{k+1} = u^k + \\beta^{k+1} - z^{k+1}$\n\n**$\\beta$-更新的推导：**\n$\\beta$-最小化步骤需要求解：\n$$\n\\beta^{k+1} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - z^k + u^k\\|_{2}^{2} \\right)\n$$\n这是一个关于 $\\beta$ 的二次目标函数。我们通过将其关于 $\\beta$ 的梯度设为零来找到最小值：\n$$\n\\nabla_{\\beta} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - (z^k - u^k)\\|_{2}^{2} \\right) = 0\n$$\n$$\n-X^T(y - X\\beta) + \\rho(\\beta - z^k + u^k) = 0\n$$\n$$\n(X^T X)\\beta - X^T y + \\rho\\beta - \\rho(z^k - u^k) = 0\n$$\n$$\n(X^T X + \\rho I)\\beta = X^T y + \\rho(z^k - u^k)\n$$\n$\\beta$ 的更新是一个线性系统求解问题：\n$$\n\\beta^{k+1} = (X^T X + \\rho I)^{-1} (X^T y + \\rho(z^k - u^k))\n$$\n\n**$z$-更新的推导：**\n$z$-最小化步骤需要求解：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda\\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta^{k+1} - z + u^k\\|_{2}^{2} \\right)\n$$\n这可以重写为：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\frac{1}{2}\\|z - (\\beta^{k+1} + u^k)\\|_{2}^{2} + \\frac{\\lambda}{\\rho}\\|z\\|_{1} \\right)\n$$\n这是 $\\ell_1$-范数的近端算子的定义。其解由阈值为 $\\kappa = \\lambda/\\rho$ 的软阈值算子 $S_{\\kappa}(\\cdot)$ 给出：\n$$\nz^{k+1} = S_{\\lambda/\\rho}(\\beta^{k+1} + u^k)\n$$\n该算子是逐元素应用的：$(S_{\\kappa}(v))_i = \\text{sign}(v_i) \\max(|v_i| - \\kappa, 0)$。\n\n**对偶变量更新：**\n对偶更新步骤是一个简单的加法：\n$$\nu^{k+1} = u^k + \\beta^{k+1} - z^{k+1}\n$$\n\n### 第 4 步：一次迭代的计算\n现在我们用给定的值计算一次迭代：\n$X = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}$，$y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$，$\\lambda = 1$，$\\rho = 2$，以及初始值 $\\beta^k = \\mathbf{0}$， $z^k = \\mathbf{0}$， $u^k = \\mathbf{0}$。索引为 $k=0$ 和 $k+1=1$。\n\n**1. 计算 $\\beta^{1}$：**\n使用推导出的公式，其中 $z^0 = u^0 = \\mathbf{0}$：\n$$\n\\beta^{1} = (X^T X + \\rho I)^{-1} (X^T y)\n$$\n首先，我们计算涉及的矩阵：\n$$\nX^T X = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+2(2)  1(2)+2(1) \\\\ 2(1)+1(2)  2(2)+1(1) \\end{pmatrix} = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix}\n$$\n$$\nX^T X + \\rho I = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix} + 2 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 7  4 \\\\ 4  7 \\end{pmatrix}\n$$\n该矩阵的逆是：\n$$\n(X^T X + \\rho I)^{-1} = \\frac{1}{7(7) - 4(4)} \\begin{pmatrix} 7  -4 \\\\ -4  7 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7  -4 \\\\ -4  7 \\end{pmatrix}\n$$\n接下来，我们计算 $X^T y$：\n$$\nX^T y = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(3)+2(1) \\\\ 2(3)+1(1) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix}\n$$\n最后，我们计算 $\\beta^1$：\n$$\n\\beta^{1} = \\frac{1}{33} \\begin{pmatrix} 7  -4 \\\\ -4  7 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7(5) - 4(7) \\\\ -4(5) + 7(7) \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 35 - 28 \\\\ -20 + 49 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7 \\\\ 29 \\end{pmatrix}\n$$\n所以，$\\beta^1 = \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}$。\n\n**2. 计算 $z^{1}$：**\n我们使用软阈值算子，阈值为 $\\kappa = \\lambda/\\rho = 1/2$。由于 $u^0 = \\mathbf{0}$，其参数为 $\\beta^1 + u^0 = \\beta^1$。\n$$\nz^{1} = S_{1/2}(\\beta^{1}) = S_{1/2}\\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}\n$$\n我们对每个分量应用此算子：\n- 对于第一个分量，$z_1^1 = S_{1/2}(7/33)$。因为 $|7/33| = 7/33$ 且 $1/2 = 16.5/33$，我们有 $7/33  1/2$。因此，$z_1^1 = 0$。\n- 对于第二个分量，$z_2^1 = S_{1/2}(29/33)$。因为 $|29/33| = 29/33$ 且 $1/2 = 16.5/33$，我们有 $29/33  1/2$。因此，$z_2^1 = 29/33 - 1/2 = \\frac{58 - 33}{66} = \\frac{25}{66}$。\n所以，$z^1 = \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}$。\n\n**3. 计算 $u^{1}$：**\n我们使用对偶更新规则：\n$$\nu^{1} = u^0 + \\beta^1 - z^1 = \\mathbf{0} + \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}\n$$\n计算每个分量：\n- $u_1^1 = 7/33 - 0 = 7/33$。\n- $u_2^1 = 29/33 - 25/66 = 58/66 - 25/66 = 33/66 = 1/2$。\n所以，$u^1 = \\begin{pmatrix} 7/33 \\\\ 1/2 \\end{pmatrix}$。\n\n第一次完整 ADMM 迭代的结果是：\n$\\beta^{1} = \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}$， $z^{1} = \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}$， $u^{1} = \\begin{pmatrix} 7/33 \\\\ 1/2 \\end{pmatrix}$。\n\n要求的最终答案是一个包含六个标量值的行矩阵，顺序为 $(\\beta^1_1, \\beta^1_2, z^1_1, z^1_2, u^1_1, u^1_2)$。\n这些值是 $(\\frac{7}{33}, \\frac{29}{33}, 0, \\frac{25}{66}, \\frac{7}{33}, \\frac{1}{2})$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{33}  \\frac{29}{33}  0  \\frac{25}{66}  \\frac{7}{33}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理论上的收敛性并不能保证实践中的高效性，而惩罚参数 $\\rho$ 的选择是影响ADMM收敛速度的关键。本练习  旨在解决这一实际挑战，你将为一个二次规划问题实现一种自适应 $\\rho$ 调整策略。通过编程实现基于原始残差和对偶残差的平衡规则，你将直观地比较固定参数与自适应参数在不同条件下的收敛性能差异，并理解动态调整 $\\rho$ 为何是优化算法性能的关键一步。",
            "id": "3364422",
            "problem": "考虑将交替方向乘子法（ADMM）应用于二维空间中一个凸的、二次连续可微的二次目标函数的一致性分裂问题。目标是设计一种残差平衡规则，该规则能自适应地调整增广拉格朗日惩罚参数，以平衡原始残差和对偶残差的范数，并分析该规则对一个二维二次测试问题的收敛效果。您将编写的程序必须是完全自包含的，并能计算出若干测试用例收敛所需的迭代次数。\n\n从以下基础开始。设优化问题表述为最小化一个可分离的凸函数之和，其等式约束以一致性形式表示：\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\n其中\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}, \\quad g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z},\n$$\n并且矩阵 $\\boldsymbol{Q}$ 和 $\\boldsymbol{R}$ 是对称正定的，向量 $\\boldsymbol{q}$ 和 $\\boldsymbol{r}$ 属于 $\\mathbb{R}^2$。定义惩罚参数为 $\\rho  0$ 且缩放对偶变量为 $\\boldsymbol{u}^k \\in \\mathbb{R}^2$ 的缩放形式 ADMM 迭代 $(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1})$。原始残差为\n$$\n\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1},\n$$\n对偶残差为\n$$\n\\boldsymbol{s}^{k+1} = \\rho\\left(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k}\\right).\n$$\n\n您的任务是：\n- 当 $f$ 和 $g$ 是所述的凸二次函数且约束为 $\\boldsymbol{x} = \\boldsymbol{z}$ 时，从第一性原理和子问题的一阶最优性条件出发，推导出 $\\boldsymbol{x}^{k+1}$ 和 $\\boldsymbol{z}^{k+1}$ 的显式闭式更新式。\n- 实现具有两种模式的 ADMM：固定惩罚参数模式和自适应残差平衡模式。在自适应模式下，使用以下带有参数 $\\mu  0$ 和 $\\kappa  1$ 的残差平衡规则，在迭代过程中调整 $\\rho$：\n$$\n\\text{if } \\|\\boldsymbol{r}^{k+1}\\|_2  \\mu \\|\\boldsymbol{s}^{k+1}\\|_2 \\text{ then set } \\rho \\leftarrow \\kappa \\rho \\text{ and } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}/\\kappa;\n$$\n$$\n\\text{else if } \\|\\boldsymbol{s}^{k+1}\\|_2  \\mu \\|\\boldsymbol{r}^{k+1}\\|_2 \\text{ then set } \\rho \\leftarrow \\rho/\\kappa \\text{ and } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}\\kappa;\n$$\n否则保持 $\\rho$ 不变。$\\boldsymbol{u}^{k+1}$ 的缩放必须在 $\\rho$ 发生变化时保持未缩放的拉格朗日乘子 $\\boldsymbol{y}^{k+1} = \\rho \\boldsymbol{u}^{k+1}$ 不变。\n- 使用基于绝对和相对容差的停止准则。令 $n = 2$。定义\n$$\n\\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right),\n$$\n$$\n\\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\rho\\,\\boldsymbol{u}^{k+1}\\|_2,\n$$\n并在 $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ 和 $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$ 同时满足时终止。仅在检查这些停止准则之后才应用任何惩罚参数自适应调整。\n\n使用以下固定输入和测试套件实现算法。所有向量都是 $\\mathbb{R}^2$ 中的列向量，所有矩阵都是 $2 \\times 2$ 矩阵。\n\n通用初始条件：\n- $\\boldsymbol{x}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\boldsymbol{z}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\boldsymbol{u}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 容差：$\\varepsilon_{\\mathrm{abs}} = 10^{-6}$，$\\varepsilon_{\\mathrm{rel}} = 10^{-6}$。\n- 最大迭代次数：$50000$。\n- 所有残差和阈值计算必须使用欧几里得范数。\n\n测试套件参数集：\n- 案例 1（良态，固定 $\\rho$）：\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 4  1 \\\\ 1  2 \\end{bmatrix}$，$\\boldsymbol{R} = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，$\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$，\n  - 固定 $\\rho = 1$，禁用自适应模式。\n- 案例 2（良态，自适应小初始 $\\rho$）：\n  - 与案例 1 相同的 $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$，\n  - 初始 $\\rho = 10^{-4}$，启用自适应模式，参数为 $\\mu = 10$，$\\kappa = 2$。\n- 案例 3（良态，自适应大初始 $\\rho$）：\n  - 与案例 1 相同的 $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$，\n  - 初始 $\\rho = 10^{2}$，启用自适应模式，参数为 $\\mu = 2$，$\\kappa = 2$。\n- 案例 4（病态，固定 $\\rho$）：\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 1000  0 \\\\ 0  1 \\end{bmatrix}$，$\\boldsymbol{R} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix}$，\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，$\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$，\n  - 固定 $\\rho = 1$，禁用自适应模式。\n- 案例 5（病态，自适应极小初始 $\\rho$）：\n  - 与案例 4 相同的 $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$，\n  - 初始 $\\rho = 10^{-6}$，启用自适应模式，参数为 $\\mu = 3$，$\\kappa = 2$。\n\n对每种情况，计算并返回满足停止准则所需的总迭代次数。您的程序应生成单行输出，其中包含一个方括号内的逗号分隔列表形式的结果，例如 $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$，其中每个 $\\text{result}_i$ 是案例 $i$ 的整数迭代次数。",
            "solution": "用户提供的问题是有效的。这是一个基于交替方向乘子法（ADMM）既定原理的良定数值优化任务。所有数据、参数和条件都已提供，使得问题自成体系，没有矛盾或含糊不清之处。\n\n该问题要求实现 ADMM 来解决一个一致性形式的可分离凸二次优化问题：\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\n其中 $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}$ 且 $g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z}$。矩阵 $\\boldsymbol{Q}$ 和 $\\boldsymbol{R}$ 是对称正定的。\n\n解决方案首先推导 ADMM 算法的显式更新方程，然后描述算法实现，包括自适应惩罚参数方案。\n\n**ADMM 公式化与更新推导**\n\n该问题的缩放形式增广拉格朗日函数 $L_\\rho$（使用缩放对偶变量 $\\boldsymbol{u}$）为：\n$$\nL_\\rho(\\boldsymbol{x}, \\boldsymbol{z}, \\boldsymbol{u}) = f(\\boldsymbol{x}) + g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z} + \\boldsymbol{u}\\|_2^2 - \\frac{\\rho}{2} \\|\\boldsymbol{u}\\|_2^2\n$$\n对于给定的惩罚参数 $\\rho  0$，ADMM 算法在每次迭代 $k$ 中包含三个顺序更新步骤：\n$$\n\\begin{align*}\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} L_\\rho(\\boldsymbol{x}, \\boldsymbol{z}^k, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} L_\\rho(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n\\end{align*}\n$$\n我们推导 $\\boldsymbol{x}$ 和 $\\boldsymbol{z}$ 子问题的闭式解。\n\n**1. $\\boldsymbol{x}$-更新推导**\n$\\boldsymbol{x}$ 的最小化子问题是：\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( f(\\boldsymbol{x}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\n代入 $f(\\boldsymbol{x})$ 的二次形式：\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x} + \\frac{\\rho}{2} \\|\\boldsymbol{x} - (\\boldsymbol{z}^k - \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\n这是一个无约束的凸二次最小化问题。通过将关于 $\\boldsymbol{x}$ 的梯度设为零来找到最小化子。梯度为：\n$$\n\\nabla_{\\boldsymbol{x}} (\\cdot) = \\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q} + \\rho(\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k)\n$$\n将梯度设为零并求解 $\\boldsymbol{x}$：\n$$\n\\boldsymbol{Q}\\boldsymbol{x}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n$$\n(\\boldsymbol{Q} + \\rho\\boldsymbol{I})\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n由于 $\\boldsymbol{Q}$ 是正定的且 $\\rho  0$，矩阵 $(\\boldsymbol{Q} + \\rho\\boldsymbol{I})$ 是对称正定且因此是可逆的。显式更新式为：\n$$\n\\boldsymbol{x}^{k+1} = (\\boldsymbol{Q} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q} \\right)\n$$\n\n**2. $\\boldsymbol{z}$-更新推导**\n$\\boldsymbol{z}$ 的最小化子问题使用新计算出的 $\\boldsymbol{x}^{k+1}$：\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\n代入 $g(\\boldsymbol{z})$ 的二次形式：\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z} + \\frac{\\rho}{2} \\|\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\n同样，我们将关于 $\\boldsymbol{z}$ 的梯度设为零。梯度为：\n$$\n\\nabla_{\\boldsymbol{z}} (\\cdot) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} + \\rho(\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} - \\rho(\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k)\n$$\n将梯度设为零并求解 $\\boldsymbol{z}$：\n$$\n\\boldsymbol{R}\\boldsymbol{z}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{R} + \\rho\\boldsymbol{I})\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n由于 $\\boldsymbol{R}$ 是正定的，$(\\boldsymbol{R} + \\rho\\boldsymbol{I})$ 是可逆的。显式更新式为：\n$$\n\\boldsymbol{z}^{k+1} = (\\boldsymbol{R} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r} \\right)\n$$\n\n**3. $\\boldsymbol{u}$-更新**\n对偶变量的更新由下式给出：\n$$\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n$$\n\n**算法实现**\n该算法迭代这些更新直至收敛。迭代 $k$（以计算迭代 $k+1$ 的值）的过程如下：\n1.  将 $\\boldsymbol{x}^0, \\boldsymbol{z}^0, \\boldsymbol{u}^0$ 初始化为 $\\boldsymbol{0}$，并将 $\\rho_0$ 初始化为其初始值。\n2.  对于 $k=0, 1, \\dots, \\text{max\\_iter}-1$：\n    a.  使用 $\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k$ 和推导出的公式计算 $\\boldsymbol{x}^{k+1}$。\n    b.  使用 $\\boldsymbol{x}^{k+1}, \\boldsymbol{u}^k, \\rho_k$ 和推导出的公式计算 $\\boldsymbol{z}^{k+1}$。\n    c.  计算缩放前的对偶更新：$\\boldsymbol{u}^{k+1}_{\\text{pre}} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$。\n    d.  计算残差：\n        -   原始残差：$\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$。\n        -   对偶残差：$\\boldsymbol{s}^{k+1} = \\rho_k(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k})$。\n    e.  基于绝对容差 $\\varepsilon_{\\mathrm{abs}}$ 和相对容差 $\\varepsilon_{\\mathrm{rel}}$ 计算停止阈值：\n        $$\n        \\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right)\n        $$\n        未缩放的拉格朗日乘子为 $\\boldsymbol{y}^{k+1} = \\rho_k \\boldsymbol{u}^{k+1}_{\\text{pre}}$。对偶阈值为：\n        $$\n        \\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\boldsymbol{y}^{k+1}\\|_2\n        $$\n    f. 检查收敛性：如果 $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ 且 $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$，则终止并返回 $k+1$。\n    g. 如果启用了自适应模式，则更新下一次迭代的 $\\rho$（即 $\\rho_{k+1}$），并相应地缩放 $\\boldsymbol{u}^{k+1}$。令 $\\boldsymbol{u}^{k+1}_{\\text{post}}$ 为下一次迭代更新后的对偶变量。\n        - 默认情况下，设置 $\\rho_{k+1} = \\rho_k$ 和 $\\boldsymbol{u}^{k+1}_{\\text{post}} = \\boldsymbol{u}^{k+1}_{\\text{pre}}$。\n        - 如果 $\\|\\boldsymbol{r}^{k+1}\\|_2  \\mu \\|\\boldsymbol{s}^{k+1}\\|_2$：\n            $\\rho_{k+1} \\leftarrow \\kappa \\rho_k$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} / \\kappa$。\n        - 否则如果 $\\|\\boldsymbol{s}^{k+1}\\|_2  \\mu \\|\\boldsymbol{r}^{k+1}\\|_2$：\n            $\\rho_{k+1} \\leftarrow \\rho_k / \\kappa$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} \\times \\kappa$。\n    h. 为下一次迭代做准备：将状态变量 $(\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k)$ 更新为 $(\\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1}_{\\text{post}}, \\rho_{k+1})$。\n3.  如果循环完成但未收敛，则返回最大迭代次数。\n最终答案中的 Python 代码实现了此逻辑。",
            "answer": "```python\nimport numpy as np\n\ndef admm_solver(Q, R, q, r, rho_init, adaptive_params, max_iter, eps_abs, eps_rel):\n    \"\"\"\n    Solves a 2D quadratic consensus ADMM problem.\n\n    Args:\n        Q (np.array): 2x2 symmetric positive definite matrix for f(x).\n        R (np.array): 2x2 symmetric positive definite matrix for g(z).\n        q (np.array): 2x1 vector for f(x).\n        r (np.array): 2x1 vector for g(z).\n        rho_init (float): Initial penalty parameter.\n        adaptive_params (tuple): (is_adaptive, mu, kappa) for rho adaptation.\n        max_iter (int): Maximum number of iterations.\n        eps_abs (float): Absolute tolerance for stopping criteria.\n        eps_rel (float): Relative tolerance for stopping criteria.\n\n    Returns:\n        int: Number of iterations to converge, or max_iter.\n    \"\"\"\n    is_adaptive, mu, kappa = adaptive_params\n\n    # Initialization\n    n = 2  # Dimension\n    x_k = np.zeros((n, 1))\n    z_k = np.zeros((n, 1))\n    u_k = np.zeros((n, 1))\n    rho_k = float(rho_init)\n\n    I = np.identity(n)\n\n    for k in range(max_iter):\n        # Precompute matrix inverses, which depend on rho\n        try:\n            inv_Q_rhoI = np.linalg.inv(Q + rho_k * I)\n            inv_R_rhoI = np.linalg.inv(R + rho_k * I)\n        except np.linalg.LinAlgError:\n            # In case of numerical issues, though unlikely with SPD matrices\n            return max_iter\n\n        # x-update\n        x_k1 = inv_Q_rhoI @ (rho_k * (z_k - u_k) - q)\n\n        # z-update\n        z_k1 = inv_R_rhoI @ (rho_k * (x_k1 + u_k) - r)\n\n        # u-update (before potential scaling)\n        u_k1_prescale = u_k + x_k1 - z_k1\n\n        # Calculate residuals\n        r_k1 = x_k1 - z_k1\n        s_k1 = rho_k * (z_k1 - z_k)\n\n        # Calculate residual norms\n        norm_r = np.linalg.norm(r_k1)\n        norm_s = np.linalg.norm(s_k1)\n\n        # Calculate stopping thresholds\n        eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x_k1), np.linalg.norm(z_k1))\n        \n        # y^{k+1} = rho_k * u_{k+1}, where u_{k+1} = u_k + r_{k+1}\n        y_k1 = rho_k * u_k1_prescale\n        eps_dual = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(y_k1)\n\n        # Check for convergence\n        if norm_r = eps_pri and norm_s = eps_dual:\n            return k + 1\n\n        # Penalty parameter and dual variable adaptation for the next iteration\n        rho_k1 = rho_k\n        u_k1 = u_k1_prescale\n        if is_adaptive:\n            if norm_r > mu * norm_s:\n                rho_k1 = kappa * rho_k\n                u_k1 = u_k1_prescale / kappa\n            elif norm_s > mu * norm_r:\n                rho_k1 = rho_k / kappa\n                u_k1 = u_k1_prescale * kappa\n        \n        # Update states for next iteration\n        # x_k is not needed, will be recomputed\n        z_k = z_k1\n        u_k = u_k1\n        rho_k = rho_k1\n    \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Common parameters\n    eps_abs = 1e-6\n    eps_rel = 1e-6\n    max_iter = 50000\n    \n    # Common vectors for specified cases\n    q_vec = np.array([[-1.0], [2.0]])\n    r_vec = np.array([[0.5], [-1.0]])\n\n    # Case 1 (well-conditioned, fixed rho)\n    Q1 = np.array([[4.0, 1.0], [1.0, 2.0]])\n    R1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    rho1 = 1.0\n    params1 = (False, 0, 0)\n    \n    # Case 2 (well-conditioned, adaptive small rho)\n    rho2 = 1e-4\n    params2 = (True, 10.0, 2.0)\n\n    # Case 3 (well-conditioned, adaptive large rho)\n    rho3 = 1e2\n    params3 = (True, 2.0, 2.0)\n    \n    # Case 4 (ill-conditioned, fixed rho)\n    Q4 = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    R4 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    rho4 = 1.0\n    params4 = (False, 0, 0)\n    \n    # Case 5 (ill-conditioned, adaptive very small rho)\n    rho5 = 1e-6\n    params5 = (True, 3.0, 2.0)\n\n    test_cases = [\n        (Q1, R1, q_vec, r_vec, rho1, params1),\n        (Q1, R1, q_vec, r_vec, rho2, params2),\n        (Q1, R1, q_vec, r_vec, rho3, params3),\n        (Q4, R4, q_vec, r_vec, rho4, params4),\n        (Q4, R4, q_vec, r_vec, rho5, params5),\n    ]\n\n    results = []\n    for case in test_cases:\n        iter_count = admm_solver(*case, max_iter, eps_abs, eps_rel)\n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理解一个强大算法的适用范围与其能力同样重要。虽然两块ADMM在凸问题上具有可靠的收敛保证，但将其直接推广到三个或更多块的“直接”高斯-赛德尔形式却可能导致发散。本练习  将通过一个简洁的三块二次规划问题，让你构建一个ADMM不收敛的具体反例。通过编程计算迭代矩阵的谱半径，你将从根本上理解多块ADMM发散的内在机制，这对于严谨地应用和扩展优化算法至关重要。",
            "id": "3364446",
            "problem": "考虑一个每块一维的线性约束三块二次优化问题，由变量 $x_1 \\in \\mathbb{R}$、$x_2 \\in \\mathbb{R}$、$x_3 \\in \\mathbb{R}$ 和约束系数标量 $A_1, A_2, A_3 \\in \\mathbb{R}$ 定义：\n$$\n\\min_{x_1,x_2,x_3} \\;\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\quad \\text{约束条件为} \\quad A_1 x_1 + A_2 x_2 + A_3 x_3 = c,\n$$\n其中 $q_1, q_2, q_3 \\ge 0$，且 $c \\in \\mathbb{R}$ 是一个给定的常数。\n\n已知，将交替方向乘子法（ADMM, Alternating Direction Method of Multipliers）以高斯-赛德尔（Gauss-Seidel）方式朴素地应用于超过两个块时，可能会发散。在上述三块问题中，考虑使用罚参数 $\\rho  0$ 和缩放对偶变量 $u$ 的缩放对偶 ADMM 迭代：\n1. 固定 $x_2$、$x_3$、$u$，通过最小化关于 $x_1$ 的增广拉格朗日量来更新 $x_1$。\n2. 固定 $x_1$（刚更新的）、$x_3$、$u$，通过最小化关于 $x_2$ 的增广拉格朗日量来更新 $x_2$。\n3. 固定 $x_1$、$x_2$（刚更新的）、$u$，通过最小化关于 $x_3$ 的增广拉格朗日量来更新 $x_3$。\n4. 使用单步高斯-赛德尔（Gauss-Seidel）步骤更新缩放对偶变量 $u$。\n\n从增广拉格朗日量的定义开始\n$$\n\\mathcal{L}_\\rho(x_1,x_2,x_3,u) \\;=\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\;+\\; \\frac{\\rho}{2}\\left(A_1 x_1 + A_2 x_2 + A_3 x_3 - c + u\\right)^2,\n$$\n每个块更新的一阶最优性条件产生了闭式线性更新。记\n$$\n\\theta_i \\;=\\; \\frac{\\rho A_i}{q_i + \\rho A_i^2}, \\quad i \\in \\{1,2,3\\},\n$$\n假设 $A_i \\neq 0$ 或 $q_i  0$，因此所有分母均为正。那么，对于 $c=0$（这将迭代隔离为齐次线性迭代），朴素的高斯-赛德尔 ADMM 块更新为：\n$$\nx_1^{k+1} = - \\theta_1\\left(A_2 x_2^k + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_2^{k+1} = - \\theta_2\\left(A_1 x_1^{k+1} + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_3^{k+1} = - \\theta_3\\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k\\right),\n$$\n$$\nu^{k+1} = u^k + \\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}\\right).\n$$\n\n这四个方程构成了对状态向量 $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top$ 的线性迭代，可以写成\n$$\nz^{k+1} = J z^k,\n$$\n其中迭代矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 完全由 $(q_1,q_2,q_3)$、$(A_1,A_2,A_3)$ 和 $\\rho$ 决定。其发散机制由 $J$ 的谱半径决定，即\n$$\n\\varrho(J) = \\max\\{|\\lambda| : \\lambda \\text{ 是 } J \\text{ 的一个特征值}\\}。\n$$\n如果 $\\varrho(J)  1$，迭代是线性不稳定的，并会沿着与不稳定特征值相关的特征向量发散；如果 $\\varrho(J)  1$，迭代会收缩并线性收敛到不动点（最优解）；如果 $\\varrho(J) = 1$，迭代至多是临界稳定的，并且可能无法收敛。\n\n任务：\n- 推导与上述齐次情况 $c=0$ 的高斯-赛德尔 ADMM 步骤一致的显式迭代矩阵 $J$。\n- 根据 $\\varrho(J)$ 解释发散机制。\n- 实现一个程序，该程序为给定参数构建 $J$，计算 $\\varrho(J)$，并为一组用于测试收敛、临界稳定和发散情况的参数集报告结果。\n\n您的程序必须：\n1. 通过将线性更新映射应用于 $\\mathbb{R}^4$ 的每个标准基向量来构建迭代矩阵 $J$。\n2. 将谱半径计算为 $J$ 的特征值的最大模。\n3. 对于每个测试用例，返回谱半径，形式为一个四舍五入到六位小数的浮点数。\n\n用于覆盖不同情况的测试套件：\n- 情况1（强凸、良态、预期收缩）：$(q_1,q_2,q_3) = (10.0,10.0,10.0)$，$(A_1,A_2,A_3) = (1.0,1.0,1.0)$，$\\rho = 1.0$，$c = 0.0$。\n- 情况2（无目标二次项、单位系数、临界稳定）：$(q_1,q_2,q_3) = (0.0,0.0,0.0)$，$(A_1,A_2,A_3) = (1.0,1.0,1.0)$，$\\rho = 1.0$，$c = 0.0$。\n- 情况3（产生大块增益的低幅值且符号不定的系数，预期发散）：$(q_1,q_2,q_3) = (0.01,0.01,0.01)$，$(A_1,A_2,A_3) = (0.2,-0.2,0.2)$，$\\rho = 1.0$，$c = 0.0$。\n- 情况4（混合符号与中等缩放，压力测试）：$(q_1,q_2,q_3) = (0.0,0.0,0.0)$，$(A_1,A_2,A_3) = (0.5,0.5,-0.5)$，$\\rho = 1.0$，$c = 0.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的谱半径，形式为方括号内以逗号分隔的列表，每个值都四舍五入到六位小数（例如，\"[0.732101,1.000000,1.284557,0.998340]\"）。\n此问题不涉及物理单位，也没有角度；所有量都是无量纲的实数。",
            "solution": "问题陈述已经过验证，被认为是科学上成立、适定、客观和完整的。所有参数和更新规则都已提供，可以直接、明确地进行推导和分析。该问题探讨了数值优化中的一个标准的、非平凡的主题——交替方向乘子法（ADMM）在超过两个块时的收敛性——并被构建为一个可解决的数学和计算任务。\n\n对三块 ADMM 迭代的分析始于将给定的高斯-赛德尔（Gauss-Seidel）更新步骤表述为对状态向量的单一线性变换。第 $k$ 次迭代的状态由向量 $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top \\in \\mathbb{R}^4$ 表示。迭代过程通过一个由迭代矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 表示的线性算子将此状态映射到下一个状态 $z^{k+1}$，使得 $z^{k+1} = J z^k$。\n\n为了推导矩阵 $J$，我们首先表达齐次情况（$c=0$）的更新方程：\n$$x_1^{k+1} = - \\theta_1(A_2 x_2^k + A_3 x_3^k + u^k)$$\n$$x_2^{k+1} = - \\theta_2(A_1 x_1^{k+1} + A_3 x_3^k + u^k)$$\n$$x_3^{k+1} = - \\theta_3(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k)$$\n$$u^{k+1} = u^k + A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}$$\n\n这些方程构成一个耦合系统。上标为 $k+1$ 的变量是未知数，它们依赖于上标为 $k$ 的变量（前一次迭代的已知数）以及在当前迭代中已经更新的其他变量（高斯-赛德尔依赖关系）。为了形成标准的矩阵方程，我们通过将所有带上标 $k+1$ 的项移到左侧，将所有带上标 $k$ 的项移到右侧，来重新整理这些方程。\n\n1. $1 \\cdot x_1^{k+1} = 0 \\cdot x_1^k - \\theta_1 A_2 x_2^k - \\theta_1 A_3 x_3^k - \\theta_1 u^k$\n2. $\\theta_2 A_1 x_1^{k+1} + 1 \\cdot x_2^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k - \\theta_2 A_3 x_3^k - \\theta_2 u^k$\n3. $\\theta_3 A_1 x_1^{k+1} + \\theta_3 A_2 x_2^{k+1} + 1 \\cdot x_3^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k - \\theta_3 u^k$\n4. $-A_1 x_1^{k+1} - A_2 x_2^{k+1} - A_3 x_3^{k+1} + 1 \\cdot u^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k + 1 \\cdot u^k$\n\n这个线性方程组可以写成矩阵形式 $L z^{k+1} = R z^k$，其中 $L$ 和 $R$ 是 $4 \\times 4$ 矩阵。矩阵 $L$ 包含 $z^{k+1}$ 项的系数，矩阵 $R$ 包含 $z^k$ 项的系数。\n\n从上面的方程，我们可以明确地构建 $L$ 和 $R$：\n$$\nL = \\begin{pmatrix}\n1  0  0  0 \\\\\n\\theta_2 A_1  1  0  0 \\\\\n\\theta_3 A_1  \\theta_3 A_2  1  0 \\\\\n-A_1  -A_2  -A_3  1\n\\end{pmatrix}\n$$\n矩阵 $L$ 是一个对角线上为一的下三角矩阵，这反映了高斯-赛德尔更新的顺序、前馈特性。这种结构保证了 $L$ 是可逆的。\n\n$$\nR = \\begin{pmatrix}\n0  -\\theta_1 A_2  -\\theta_1 A_3  -\\theta_1 \\\\\n0  0  -\\theta_2 A_3  -\\theta_2 \\\\\n0  0  0  -\\theta_3 \\\\\n0  0  0  1\n\\end{pmatrix}\n$$\n矩阵 $R$ 是上三角矩阵。第一列为零，因为 $x_1^k$ 没有出现在任何更新方程的右侧。\n\n迭代矩阵 $J$ 随后通过求解 $z^{k+1}$ 得到：\n$$z^{k+1} = L^{-1} R z^k \\implies J = L^{-1} R$$\n在数值上，这是通过求解线性系统 $LJ = R$ 来计算矩阵 $J$ 的，这比直接计算 $L$ 的逆更稳定、更高效。\n\n迭代 $z^{k+1} = J z^k$ 的收敛性由 $J$ 的谱半径决定，记作 $\\varrho(J)$。谱半径定义为 $J$ 的特征值的最大模：$\\varrho(J) = \\max \\{|\\lambda_i|\\}$，其中 $\\lambda_i$ 是 $J$ 的特征值。\n通过考虑迭代的长期行为，可以理解发散的机制。任何初始状态 $z^0$ 通常可以表示为 $J$ 的特征向量的线性组合（为简化解释，假设 $J$ 可对角化）。设 $J v_i = \\lambda_i v_i$，其中 $v_i$ 为特征向量，$\\lambda_i$ 为特征值。如果 $z^0 = \\sum_i c_i v_i$，那么经过 $k$ 次迭代后，状态为 $z^k = J^k z^0 = \\sum_i c_i \\lambda_i^k v_i$。\n- 如果 $\\varrho(J)  1$，那么对所有 $i$ 都有 $|\\lambda_i|  1$。因此，当 $k \\to \\infty$ 时，$\\lambda_i^k \\to 0$，迭代收敛到零向量，这是不动点（也是 $c=0$ 时的最优解）。\n- 如果 $\\varrho(J)  1$，则至少存在一个特征值 $\\lambda_j$ 满足 $|\\lambda_j|  1$。对应的项 $c_j \\lambda_j^k v_j$ 的模将增长，导致状态向量 $z^k$ 发散，前提是初始状态 $z^0$ 在 $v_j$ 方向上有非零分量（即 $c_j \\neq 0$）。这是线性不稳定的条件。\n- 如果 $\\varrho(J) = 1$，迭代是临界稳定的。状态的某些分量可能不会衰减，导致振荡或缓慢的次线性收敛，如果模为 $1$ 的特征值的几何重数小于其代数重数，甚至可能发散。\n\n下面的程序将实现这个推导。对于每个测试用例，它将：\n1. 定义参数 $q_i, A_i, \\rho$。\n2. 计算中间变量 $\\theta_i = \\frac{\\rho A_i}{q_i + \\rho A_i^2}$。\n3. 根据上面推导的公式构建矩阵 $L$ 和 $R$。\n4. 求解线性矩阵方程 $LJ=R$ 以找到迭代矩阵 $J$。\n5. 计算 $J$ 的特征值。\n6. 通过找到特征值中的最大绝对值来确定谱半径 $\\varrho(J)$。\n7. 报告这个值，四舍五入到六位小数。\n对所有提供的测试用例重复此过程，以探索不同的收敛情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the 3-block ADMM iteration matrix\n    for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple: ((q1, q2, q3), (A1, A2, A3), rho)\n    test_cases = [\n        # Case 1: strongly convex, well-conditioned, expected contraction\n        ((10.0, 10.0, 10.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 2: no objective quadratics, identity coefficients, marginal stability\n        ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 3: low-magnitude coefficients producing large gains, expected divergence\n        ((0.01, 0.01, 0.01), (0.2, -0.2, 0.2), 1.0),\n        # Case 4: mixed signs with moderate scaling, stress test\n        ((0.0, 0.0, 0.0), (0.5, 0.5, -0.5), 1.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        (q1, q2, q3), (A1, A2, A3), rho = case\n        \n        # Calculate theta_i values\n        # theta_i = (rho * A_i) / (q_i + rho * A_i^2)\n        # Denominators are positive as per problem constraints.\n        theta1 = (rho * A1) / (q1 + rho * A1**2)\n        theta2 = (rho * A2) / (q2 + rho * A2**2)\n        theta3 = (rho * A3) / (q3 + rho * A3**2)\n        \n        # Construct the L matrix from the equation L * z^{k+1} = R * z^k\n        # L = [[1, 0, 0, 0],\n        #      [theta2*A1, 1, 0, 0],\n        #      [theta3*A1, theta3*A2, 1, 0],\n        #      [-A1, -A2, -A3, 1]]\n        L = np.array([\n            [1.0, 0.0, 0.0, 0.0],\n            [theta2 * A1, 1.0, 0.0, 0.0],\n            [theta3 * A1, theta3 * A2, 1.0, 0.0],\n            [-A1, -A2, -A3, 1.0]\n        ])\n        \n        # Construct the R matrix\n        # R = [[0, -theta1*A2, -theta1*A3, -theta1],\n        #      [0, 0, -theta2*A3, -theta2],\n        #      [0, 0, 0, -theta3],\n        #      [0, 0, 0, 1]]\n        R = np.array([\n            [0.0, -theta1 * A2, -theta1 * A3, -theta1],\n            [0.0, 0.0, -theta2 * A3, -theta2],\n            [0.0, 0.0, 0.0, -theta3],\n            [0.0, 0.0, 0.0, 1.0]\n        ])\n\n        # The iteration matrix J is defined by z^{k+1} = J * z^k,\n        # where J = inv(L) * R.\n        # This is best solved as L*J = R.\n        J = np.linalg.solve(L, R)\n        \n        # Compute the eigenvalues of the iteration matrix J.\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # The spectral radius is the maximum magnitude of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        results.append(spectral_radius)\n\n    # Format the final output as a comma-separated list in brackets,\n    # with each value rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}