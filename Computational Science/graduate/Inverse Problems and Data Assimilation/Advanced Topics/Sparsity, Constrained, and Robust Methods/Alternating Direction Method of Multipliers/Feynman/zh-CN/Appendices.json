{
    "hands_on_practices": [
        {
            "introduction": "LASSO (最小绝对收缩和选择算子) 是 ADMM 的一个典型应用。本练习将通过一个简单的实例，引导你手动完整计算一次迭代，从而揭开该算法的神秘面纱。这有助于你从根本上理解 $x$-更新 (一个最小二乘问题)、$z$-更新 (邻近算子/软阈值) 以及对偶变量更新的力学原理。",
            "id": "3476975",
            "problem": "考虑带有 $\\ell_{1}$ 惩罚的稀疏线性回归问题（最小绝对收缩和选择算子），该问题旨在寻找 $\\beta \\in \\mathbb{R}^{p}$ 以最小化目标函数 $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$，其中数据为 $X \\in \\mathbb{R}^{m \\times p}$ 和 $y \\in \\mathbb{R}^{m}$。为了对 $\\ell_{1}$ 项进行变量分裂，引入一个辅助变量 $z \\in \\mathbb{R}^{p}$ 和约束条件 $z = \\beta$，从而得到等价的约束优化问题 $\\min_{\\beta, z} \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1}$，约束条件为 $z = \\beta$。交替方向乘子法 (ADMM) 通过增广拉格朗日量以及对原始变量和缩放对偶变量的交替更新来解决此类问题。\n\n使用凸优化和邻近算子的基本定义，推导此约束形式下交替方向乘子法 (ADMM) 的单次迭代更新式，并对以下玩具实例显式地计算一次迭代。数据矩阵和响应向量为\n$$\nX = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix},\n$$\n正则化参数为 $\\lambda = 1$，惩罚参数为 $\\rho = 2$，初始迭代值为\n$$\n\\beta^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad z^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n从给定的迭代值开始，通过执行一次完整的交替方向乘子法 (ADMM) 迭代，精确地以闭式形式计算更新后的变量 $\\beta^{k+1}$、 $z^{k+1}$ 和 $u^{k+1}$。将最终答案表示为一个单行矩阵，按顺序 $\\beta^{k+1}_{1}$、 $\\beta^{k+1}_{2}$、 $z^{k+1}_{1}$、 $z^{k+1}_{2}$、 $u^{k+1}_{1}$、 $u^{k+1}_{2}$ 列出六个标量条目。不要四舍五入；将所有有理数值表示为最简分数形式。不涉及物理单位。",
            "solution": "用户要求推导并计算交替方向乘子法 (ADMM) 应用于一个稀疏线性回归问题时的一次迭代。\n\n### 第 1 步：提取已知条件\n问题是最小化目标函数 $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$。\n这被重构为一个等价的约束问题：\n$$\n\\min_{\\beta, z} \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{subject to} \\quad z = \\beta\n$$\n该特定实例的数据和参数如下：\n- 数据矩阵： $X = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}$\n- 响应向量： $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$\n- 正则化参数： $\\lambda = 1$\n- ADMM 惩罚参数： $\\rho = 2$\n- 初始迭代值（在第 $k$ 步）： $\\beta^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$、 $z^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$、 $u^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n\n任务是计算下一次迭代的值 $\\beta^{k+1}$、 $z^{k+1}$ 和 $u^{k+1}$。\n\n### 第 2 步：使用提取的已知条件进行验证\n问题陈述在科学上是合理的、适定的和客观的。\n- **科学依据：** 该问题使用 LASSO 目标函数，这是高维统计学和机器学习中的一种标准技术。变量分裂和 ADMM 是解决此类问题的既定方法。该公式在数学上是正确的。\n- **适定性：** 该问题提供了计算一次迭代唯一解所需的所有数据和初始条件。目标函数是凸的，确保了 ADMM 中的子问题是适定的。对于 $\\rho > 0$，矩阵 $X^T X + \\rho I$ 是可逆的，这确保了 $\\beta$ 更新的解是唯一的。\n- **客观性：** 该问题使用精确的数学语言陈述，没有任何歧义或主观因素。\n\n问题是有效的。\n\n### 第 3 步：推导 ADMM 更新式\nADMM 算法应用于该约束形式。约束可以写成 $\\beta - z = 0$。\n增广拉格朗日量以其缩放对偶变量形式给出：\n$$\nL_{\\rho}(\\beta, z, u) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta - z + u\\|_{2}^{2}\n$$\n这里，$u$ 是缩放对偶变量。ADMM 迭代包括三个步骤：\n1. 关于 $\\beta$ 的最小化：$\\beta^{k+1} = \\arg\\min_{\\beta} L_{\\rho}(\\beta, z^k, u^k)$\n2. 关于 $z$ 的最小化：$z^{k+1} = \\arg\\min_{z} L_{\\rho}(\\beta^{k+1}, z, u^k)$\n3. 对偶变量的更新：$u^{k+1} = u^k + \\beta^{k+1} - z^{k+1}$\n\n**$\\beta$ 更新的推导：**\n$\\beta$ 最小化步骤涉及求解：\n$$\n\\beta^{k+1} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - z^k + u^k\\|_{2}^{2} \\right)\n$$\n这是一个关于 $\\beta$ 的二次目标函数。我们通过将其关于 $\\beta$ 的梯度设为零来找到最小值：\n$$\n\\nabla_{\\beta} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - (z^k - u^k)\\|_{2}^{2} \\right) = 0\n$$\n$$\n-X^T(y - X\\beta) + \\rho(\\beta - z^k + u^k) = 0\n$$\n$$\n(X^T X)\\beta - X^T y + \\rho\\beta - \\rho(z^k - u^k) = 0\n$$\n$$\n(X^T X + \\rho I)\\beta = X^T y + \\rho(z^k - u^k)\n$$\n$\\beta$ 的更新是一个线性系统求解：\n$$\n\\beta^{k+1} = (X^T X + \\rho I)^{-1} (X^T y + \\rho(z^k - u^k))\n$$\n\n**$z$ 更新的推导：**\n$z$ 最小化步骤涉及求解：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda\\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta^{k+1} - z + u^k\\|_{2}^{2} \\right)\n$$\n这可以重写为：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\frac{1}{2}\\|z - (\\beta^{k+1} + u^k)\\|_{2}^{2} + \\frac{\\lambda}{\\rho}\\|z\\|_{1} \\right)\n$$\n这是 $\\ell_1$ 范数的邻近算子的定义。其解由软阈值算子 $S_{\\kappa}(\\cdot)$ 给出，阈值为 $\\kappa = \\lambda/\\rho$：\n$$\nz^{k+1} = S_{\\lambda/\\rho}(\\beta^{k+1} + u^k)\n$$\n该算子是逐元素应用的：$(S_{\\kappa}(v))_i = \\text{sign}(v_i) \\max(|v_i| - \\kappa, 0)$。\n\n**对偶变量的更新：**\n对偶更新步骤是一个简单的加法：\n$$\nu^{k+1} = u^k + \\beta^{k+1} - z^{k+1}\n$$\n\n### 第 4 步：单次迭代的计算\n我们现在使用给定的值计算一次迭代：\n$X = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}$，$y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$，$\\lambda = 1$，$\\rho = 2$，以及初始值 $\\beta^k = \\mathbf{0}$， $z^k = \\mathbf{0}$， $u^k = \\mathbf{0}$。索引为 $k=0$ 和 $k+1=1$。\n\n**1. 计算 $\\beta^{1}$：**\n使用推导出的公式，其中 $z^0 = u^0 = \\mathbf{0}$：\n$$\n\\beta^{1} = (X^T X + \\rho I)^{-1} (X^T y)\n$$\n首先，我们计算所涉及的矩阵：\n$$\nX^T X = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+2(2)  1(2)+2(1) \\\\ 2(1)+1(2)  2(2)+1(1) \\end{pmatrix} = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix}\n$$\n$$\nX^T X + \\rho I = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix} + 2 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 7  4 \\\\ 4  7 \\end{pmatrix}\n$$\n该矩阵的逆是：\n$$\n(X^T X + \\rho I)^{-1} = \\frac{1}{7(7) - 4(4)} \\begin{pmatrix} 7  -4 \\\\ -4  7 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7  -4 \\\\ -4  7 \\end{pmatrix}\n$$\n接下来，我们计算 $X^T y$：\n$$\nX^T y = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(3)+2(1) \\\\ 2(3)+1(1) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix}\n$$\n最后，我们计算 $\\beta^1$：\n$$\n\\beta^{1} = \\frac{1}{33} \\begin{pmatrix} 7  -4 \\\\ -4  7 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7(5) - 4(7) \\\\ -4(5) + 7(7) \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 35 - 28 \\\\ -20 + 49 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7 \\\\ 29 \\end{pmatrix}\n$$\n所以，$\\beta^1 = \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}$。\n\n**2. 计算 $z^{1}$：**\n我们使用软阈值算子，阈值为 $\\kappa = \\lambda/\\rho = 1/2$。其参数为 $\\beta^1 + u^0 = \\beta^1$，因为 $u^0 = \\mathbf{0}$。\n$$\nz^{1} = S_{1/2}(\\beta^{1}) = S_{1/2}\\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}\n$$\n我们对每个分量应用此算子：\n- 对于第一个分量，$z_1^1 = S_{1/2}(7/33)$。因为 $|7/33| = 7/33$ 且 $1/2 = 16.5/33$，所以有 $7/33  1/2$。因此，$z_1^1 = 0$。\n- 对于第二个分量，$z_2^1 = S_{1/2}(29/33)$。因为 $|29/33| = 29/33$ 且 $1/2 = 16.5/33$，所以有 $29/33  1/2$。因此，$z_2^1 = 29/33 - 1/2 = \\frac{58 - 33}{66} = \\frac{25}{66}$。\n所以，$z^1 = \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}$。\n\n**3. 计算 $u^{1}$：**\n我们使用对偶更新规则：\n$$\nu^{1} = u^0 + \\beta^1 - z^1 = \\mathbf{0} + \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}\n$$\n计算每个分量：\n- $u_1^1 = 7/33 - 0 = 7/33$。\n- $u_2^1 = 29/33 - 25/66 = 58/66 - 25/66 = 33/66 = 1/2$。\n所以，$u^1 = \\begin{pmatrix} 7/33 \\\\ 1/2 \\end{pmatrix}$。\n\n第一次完整 ADMM 迭代的结果是：\n$\\beta^{1} = \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}$， $z^{1} = \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}$， $u^{1} = \\begin{pmatrix} 7/33 \\\\ 1/2 \\end{pmatrix}$。\n\n所要求的最终答案是一个包含六个标量值的行矩阵，顺序为 $(\\beta^1_1, \\beta^1_2, z^1_1, z^1_2, u^1_1, u^1_2)$。\n这些值为 $(\\frac{7}{33}, \\frac{29}{33}, 0, \\frac{25}{66}, \\frac{7}{33}, \\frac{1}{2})$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{33}  \\frac{29}{33}  0  \\frac{25}{66}  \\frac{7}{33}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "ADMM 的收敛速度对惩罚参数 $\\rho$ 的选择高度敏感。本练习将从固定的 $\\rho$ 转向基于残差平衡的自适应策略。通过实现和比较这两种方法，你将获得构建更稳健、更高效 ADMM 求解器的实践技能。",
            "id": "3364422",
            "problem": "考虑将交替方向乘子法（ADMM）应用于二维凸、二次连续可微二次目标函数的一致性分裂。目标是设计一个残差平衡规则，该规则能自适应调整增广拉格朗日惩罚参数，以平衡原始残差和对偶残差的范数，并分析其对一个二维二次测试问题的收敛效果。您将编写的程序必须完全独立，并计算若干测试案例收敛所需的迭代次数。\n\n从以下基础开始。设优化问题表述为最小化一个可分离凸函数之和，并带有一个以一致性形式表示的等式约束：\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\n其中\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}, \\quad g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z},\n$$\n且矩阵 $\\boldsymbol{Q}$ 和 $\\boldsymbol{R}$ 是对称正定的，向量 $\\boldsymbol{q}$ 和 $\\boldsymbol{r}$ 在 $\\mathbb{R}^2$ 中。定义惩罚参数 $\\rho  0$ 且缩放对偶变量为 $\\boldsymbol{u}^k \\in \\mathbb{R}^2$ 的缩放形式 ADMM 迭代 $(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1})$。原始残差为\n$$\n\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1},\n$$\n对偶残差为\n$$\n\\boldsymbol{s}^{k+1} = \\rho\\left(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k}\\right).\n$$\n\n您的任务是：\n- 当 $f$ 和 $g$ 是所述的凸二次函数且约束为 $\\boldsymbol{x} = \\boldsymbol{z}$ 时，从第一性原理和子问题的一阶最优性条件推导出 $\\boldsymbol{x}^{k+1}$ 和 $\\boldsymbol{z}^{k+1}$ 的显式闭式更新公式。\n- 实现具有两种模式的 ADMM：固定惩罚参数模式和自适应残差平衡模式。在自适应模式中，使用以下带有参数 $\\mu  0$ 和 $\\kappa  1$ 的残差平衡规则，在迭代过程中调整 $\\rho$：\n$$\n\\text{若 } \\|\\boldsymbol{r}^{k+1}\\|_2  \\mu \\|\\boldsymbol{s}^{k+1}\\|_2 \\text{ 则设 } \\rho \\leftarrow \\kappa \\rho \\text{ 且 } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}/\\kappa;\n$$\n$$\n\\text{否则若 } \\|\\boldsymbol{s}^{k+1}\\|_2  \\mu \\|\\boldsymbol{r}^{k+1}\\|_2 \\text{ 则设 } \\rho \\leftarrow \\rho/\\kappa \\text{ 且 } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}\\kappa;\n$$\n否则保持 $\\rho$ 不变。$\\boldsymbol{u}^{k+1}$ 的缩放必须在 $\\rho$ 发生变化时保持未缩放的拉格朗日乘子 $\\boldsymbol{y}^{k+1} = \\rho \\boldsymbol{u}^{k+1}$ 不变。\n- 使用基于绝对和相对容差的停止准则。设 $n = 2$。定义\n$$\n\\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right),\n$$\n$$\n\\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\rho\\,\\boldsymbol{u}^{k+1}\\|_2,\n$$\n当 $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ 和 $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$ 同时满足时终止。仅在检查这些停止准则之后应用任何惩罚参数自适应调整。\n\n用以下固定的输入和测试套件实现算法。所有向量均为 $\\mathbb{R}^2$ 中的列向量，所有矩阵均为 $2 \\times 2$ 矩阵。\n\n通用初始条件：\n- $\\boldsymbol{x}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\boldsymbol{z}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\boldsymbol{u}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 容差：$\\varepsilon_{\\mathrm{abs}} = 10^{-6}$，$\\varepsilon_{\\mathrm{rel}} = 10^{-6}$。\n- 最大迭代次数：$50000$。\n- 所有残差和阈值计算必须使用欧几里得范数。\n\n测试套件参数集：\n- 案例 1（良态，固定 $\\rho$）：\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 4  1 \\\\ 1  2 \\end{bmatrix}$，$\\boldsymbol{R} = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，$\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$，\n  - 固定 $\\rho = 1$，禁用自适应模式。\n- 案例 2（良态，自适应，初始 $\\rho$ 较小）：\n  - 与案例 1 相同的 $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$，\n  - 初始 $\\rho = 10^{-4}$，启用自适应模式，$\\mu = 10$，$\\kappa = 2$。\n- 案例 3（良态，自适应，初始 $\\rho$ 较大）：\n  - 与案例 1 相同的 $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$，\n  - 初始 $\\rho = 10^{2}$，启用自适应模式，$\\mu = 2$，$\\kappa = 2$。\n- 案例 4（病态，固定 $\\rho$）：\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 1000  0 \\\\ 0  1 \\end{bmatrix}$，$\\boldsymbol{R} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix}$，\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，$\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$，\n  - 固定 $\\rho = 1$，禁用自适应模式。\n- 案例 5（病态，自适应，初始 $\\rho$ 非常小）：\n  - 与案例 4 相同的 $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$，\n  - 初始 $\\rho = 10^{-6}$，启用自适应模式，$\\mu = 3$，$\\kappa = 2$。\n\n对于每个案例，计算并返回满足停止准则所需的总迭代次数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如 $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$，其中每个 $\\text{result}_i$ 是案例 $i$ 的整数迭代次数。",
            "solution": "用户提供的问题是有效的。这是一个基于交替方向乘子法（ADMM）既定原则、形式完善的数值优化任务。所有数据、参数和条件都已提供，使得问题自成体系，没有矛盾或歧义。\n\n该问题要求实现 ADMM 来解决一个一致性形式的可分离凸二次优化问题：\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\n其中 $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}$ 且 $g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z}$。矩阵 $\\boldsymbol{Q}$ 和 $\\boldsymbol{R}$ 是对称正定的。\n\n解决方案首先推导 ADMM 算法的显式更新方程，然后描述算法实现，包括自适应惩罚参数方案。\n\n**ADMM 公式和更新推导**\n\n对于此问题，使用缩放对偶变量 $\\boldsymbol{u}$ 的缩放形式增广拉格朗日量 $L_\\rho$ 为：\n$$\nL_\\rho(\\boldsymbol{x}, \\boldsymbol{z}, \\boldsymbol{u}) = f(\\boldsymbol{x}) + g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z} + \\boldsymbol{u}\\|_2^2 - \\frac{\\rho}{2} \\|\\boldsymbol{u}\\|_2^2\n$$\nADMM 算法在每次迭代 $k$ 中包含三个顺序更新步骤，对于给定的惩罚参数 $\\rho  0$：\n$$\n\\begin{align*}\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} L_\\rho(\\boldsymbol{x}, \\boldsymbol{z}^k, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} L_\\rho(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n\\end{align*}\n$$\n我们推导 $\\boldsymbol{x}$ 和 $\\boldsymbol{z}$ 子问题的闭式解。\n\n**1. $\\boldsymbol{x}$-更新推导**\n针对 $\\boldsymbol{x}$ 的最小化子问题是：\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( f(\\boldsymbol{x}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\n代入 $f(\\boldsymbol{x})$ 的二次形式：\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x} + \\frac{\\rho}{2} \\|\\boldsymbol{x} - (\\boldsymbol{z}^k - \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\n这是一个无约束凸二次最小化问题。通过将关于 $\\boldsymbol{x}$ 的梯度设为零来找到最小化点。梯度是：\n$$\n\\nabla_{\\boldsymbol{x}} (\\cdot) = \\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q} + \\rho(\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k)\n$$\n将梯度设为零并求解 $\\boldsymbol{x}$：\n$$\n\\boldsymbol{Q}\\boldsymbol{x}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n$$\n(\\boldsymbol{Q} + \\rho\\boldsymbol{I})\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n由于 $\\boldsymbol{Q}$ 是正定的且 $\\rho  0$，矩阵 $(\\boldsymbol{Q} + \\rho\\boldsymbol{I})$ 是对称正定的，因此是可逆的。显式更新为：\n$$\n\\boldsymbol{x}^{k+1} = (\\boldsymbol{Q} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q} \\right)\n$$\n\n**2. $\\boldsymbol{z}$-更新推导**\n针对 $\\boldsymbol{z}$ 的最小化子问题使用新计算的 $\\boldsymbol{x}^{k+1}$：\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\n代入 $g(\\boldsymbol{z})$ 的二次形式：\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z} + \\frac{\\rho}{2} \\|\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\n同样，我们将关于 $\\boldsymbol{z}$ 的梯度设为零。梯度是：\n$$\n\\nabla_{\\boldsymbol{z}} (\\cdot) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} + \\rho(\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} - \\rho(\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k)\n$$\n将梯度设为零并求解 $\\boldsymbol{z}$：\n$$\n\\boldsymbol{R}\\boldsymbol{z}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{R} + \\rho\\boldsymbol{I})\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n由于 $\\boldsymbol{R}$ 是正定的，$(\\boldsymbol{R} + \\rho\\boldsymbol{I})$ 是可逆的。显式更新为：\n$$\n\\boldsymbol{z}^{k+1} = (\\boldsymbol{R} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r} \\right)\n$$\n\n**3. $\\boldsymbol{u}$-更新**\n对偶变量的更新由下式给出：\n$$\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n$$\n\n**算法实现**\n该算法迭代这些更新直到收敛。迭代 $k$（计算迭代 $k+1$ 的值）的过程如下：\n1.  将 $\\boldsymbol{x}^0, \\boldsymbol{z}^0, \\boldsymbol{u}^0$ 初始化为 $\\boldsymbol{0}$，并将 $\\rho_0$ 初始化为其初始值。\n2.  对于 $k=0, 1, \\dots, \\text{max\\_iter}-1$：\n    a.  使用导出的公式以及 $\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k$ 计算 $\\boldsymbol{x}^{k+1}$。\n    b.  使用导出的公式以及 $\\boldsymbol{x}^{k+1}, \\boldsymbol{u}^k, \\rho_k$ 计算 $\\boldsymbol{z}^{k+1}$。\n    c.  计算缩放前的对偶更新：$\\boldsymbol{u}^{k+1}_{\\text{pre}} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$。\n    d.  计算残差：\n        -   原始残差：$\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$。\n        -   对偶残差：$\\boldsymbol{s}^{k+1} = \\rho_k(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k})$。\n    e.  基于绝对容差 $\\varepsilon_{\\mathrm{abs}}$ 和相对容差 $\\varepsilon_{\\mathrm{rel}}$ 计算停止阈值：\n        $$\n        \\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right)\n        $$\n        未缩放的拉格朗日乘子是 $\\boldsymbol{y}^{k+1} = \\rho_k \\boldsymbol{u}^{k+1}_{\\text{pre}}$。对偶阈值为：\n        $$\n        \\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\boldsymbol{y}^{k+1}\\|_2\n        $$\n    f. 检查是否收敛：如果 $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ 且 $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$，则终止并返回 $k+1$。\n    g. 如果启用了自适应模式，则更新下一次迭代的 $\\rho$（即 $\\rho_{k+1}$），并相应地缩放 $\\boldsymbol{u}^{k+1}$。令 $\\boldsymbol{u}^{k+1}_{\\text{post}}$ 为下一次迭代更新后的对偶变量。\n        - 默认情况下，设置 $\\rho_{k+1} = \\rho_k$ 和 $\\boldsymbol{u}^{k+1}_{\\text{post}} = \\boldsymbol{u}^{k+1}_{\\text{pre}}$。\n        - 如果 $\\|\\boldsymbol{r}^{k+1}\\|_2  \\mu \\|\\boldsymbol{s}^{k+1}\\|_2$：\n            $\\rho_{k+1} \\leftarrow \\kappa \\rho_k$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} / \\kappa$。\n        - 否则如果 $\\|\\boldsymbol{s}^{k+1}\\|_2  \\mu \\|\\boldsymbol{r}^{k+1}\\|_2$：\n            $\\rho_{k+1} \\leftarrow \\rho_k / \\kappa$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} \\times \\kappa$。\n    h. 为下一次迭代做准备：将状态变量 $(\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k)$ 更新为 $(\\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1}_{\\text{post}}, \\rho_{k+1})$。\n3.  如果循环完成但未收敛，则返回最大迭代次数。\n最终答案中的 Python 代码实现了此逻辑。",
            "answer": "```python\nimport numpy as np\n\ndef admm_solver(Q, R, q, r, rho_init, adaptive_params, max_iter, eps_abs, eps_rel):\n    \"\"\"\n    Solves a 2D quadratic consensus ADMM problem.\n\n    Args:\n        Q (np.array): 2x2 symmetric positive definite matrix for f(x).\n        R (np.array): 2x2 symmetric positive definite matrix for g(z).\n        q (np.array): 2x1 vector for f(x).\n        r (np.array): 2x1 vector for g(z).\n        rho_init (float): Initial penalty parameter.\n        adaptive_params (tuple): (is_adaptive, mu, kappa) for rho adaptation.\n        max_iter (int): Maximum number of iterations.\n        eps_abs (float): Absolute tolerance for stopping criteria.\n        eps_rel (float): Relative tolerance for stopping criteria.\n\n    Returns:\n        int: Number of iterations to converge, or max_iter.\n    \"\"\"\n    is_adaptive, mu, kappa = adaptive_params\n\n    # Initialization\n    n = 2  # Dimension\n    x_k = np.zeros((n, 1))\n    z_k = np.zeros((n, 1))\n    u_k = np.zeros((n, 1))\n    rho_k = float(rho_init)\n\n    I = np.identity(n)\n\n    for k in range(max_iter):\n        # Precompute matrix inverses, which depend on rho\n        try:\n            inv_Q_rhoI = np.linalg.inv(Q + rho_k * I)\n            inv_R_rhoI = np.linalg.inv(R + rho_k * I)\n        except np.linalg.LinAlgError:\n            # In case of numerical issues, though unlikely with SPD matrices\n            return max_iter\n\n        # x-update\n        x_k1 = inv_Q_rhoI @ (rho_k * (z_k - u_k) - q)\n\n        # z-update\n        z_k1 = inv_R_rhoI @ (rho_k * (x_k1 + u_k) - r)\n\n        # u-update (before potential scaling)\n        u_k1_prescale = u_k + x_k1 - z_k1\n\n        # Calculate residuals\n        r_k1 = x_k1 - z_k1\n        s_k1 = rho_k * (z_k1 - z_k)\n\n        # Calculate residual norms\n        norm_r = np.linalg.norm(r_k1)\n        norm_s = np.linalg.norm(s_k1)\n\n        # Calculate stopping thresholds\n        eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x_k1), np.linalg.norm(z_k1))\n        \n        # y^{k+1} = rho_k * u_{k+1}, where u_{k+1} = u_k + r_{k+1}\n        y_k1 = rho_k * u_k1_prescale\n        eps_dual = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(y_k1)\n\n        # Check for convergence\n        if norm_r = eps_pri and norm_s = eps_dual:\n            return k + 1\n\n        # Penalty parameter and dual variable adaptation for the next iteration\n        rho_k1 = rho_k\n        u_k1 = u_k1_prescale\n        if is_adaptive:\n            if norm_r > mu * norm_s:\n                rho_k1 = kappa * rho_k\n                u_k1 = u_k1_prescale / kappa\n            elif norm_s > mu * norm_r:\n                rho_k1 = rho_k / kappa\n                u_k1 = u_k1_prescale * kappa\n        \n        # Update states for next iteration\n        # x_k is not needed, will be recomputed\n        z_k = z_k1\n        u_k = u_k1\n        rho_k = rho_k1\n    \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Common parameters\n    eps_abs = 1e-6\n    eps_rel = 1e-6\n    max_iter = 50000\n    \n    # Common vectors for specified cases\n    q_vec = np.array([[-1.0], [2.0]])\n    r_vec = np.array([[0.5], [-1.0]])\n\n    # Case 1 (well-conditioned, fixed rho)\n    Q1 = np.array([[4.0, 1.0], [1.0, 2.0]])\n    R1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    rho1 = 1.0\n    params1 = (False, 0, 0)\n    \n    # Case 2 (well-conditioned, adaptive small rho)\n    rho2 = 1e-4\n    params2 = (True, 10.0, 2.0)\n\n    # Case 3 (well-conditioned, adaptive large rho)\n    rho3 = 1e2\n    params3 = (True, 2.0, 2.0)\n    \n    # Case 4 (ill-conditioned, fixed rho)\n    Q4 = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    R4 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    rho4 = 1.0\n    params4 = (False, 0, 0)\n    \n    # Case 5 (ill-conditioned, adaptive very small rho)\n    rho5 = 1e-6\n    params5 = (True, 3.0, 2.0)\n\n    test_cases = [\n        (Q1, R1, q_vec, r_vec, rho1, params1),\n        (Q1, R1, q_vec, r_vec, rho2, params2),\n        (Q1, R1, q_vec, r_vec, rho3, params3),\n        (Q4, R4, q_vec, r_vec, rho4, params4),\n        (Q4, R4, q_vec, r_vec, rho5, params5),\n    ]\n\n    results = []\n    for case in test_cases:\n        iter_count = admm_solver(*case, max_iter, eps_abs, eps_rel)\n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "尽管 ADMM 功能强大，但其收敛性保证并不能简单地从双块问题推广到多块问题。本练习通过一个简单的三块二次问题来展示这种失效模式。通过分析迭代矩阵的谱半径 $\\varrho(J)$，你将理解发散的机制，并学会在将类 ADMM 方法应用于超过两个块的问题时保持谨慎。",
            "id": "3364446",
            "problem": "考虑每个块为一维的线性约束三块二次优化问题，该问题由变量 $x_1 \\in \\mathbb{R}$、$x_2 \\in \\mathbb{R}$、$x_3 \\in \\mathbb{R}$ 和约束系数标量 $A_1, A_2, A_3 \\in \\mathbb{R}$ 定义：\n$$\n\\min_{x_1,x_2,x_3} \\;\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\quad \\text{subject to} \\quad A_1 x_1 + A_2 x_2 + A_3 x_3 = c,\n$$\n其中 $q_1, q_2, q_3 \\ge 0$，且 $c \\in \\mathbb{R}$ 是一个给定的常数。\n\n已知将交替方向乘子法（ADMM, Alternating Direction Method of Multipliers）以 Gauss-Seidel 方式朴素地应用于两个以上的块可能导致发散。在上述三块问题中，考虑使用罚参数 $\\rho  0$ 和尺度对偶变量 $u$ 的尺度对偶 ADMM 迭代：\n1. 固定 $x_2$、$x_3$、$u$，通过最小化关于 $x_1$ 的增广拉格朗日函数来更新 $x_1$。\n2. 固定 $x_1$（刚更新）、$x_3$、$u$，通过最小化关于 $x_2$ 的增广拉格朗日函数来更新 $x_2$。\n3. 固定 $x_1$、$x_2$（刚更新）、$u$，通过最小化关于 $x_3$ 的增广拉格朗日函数来更新 $x_3$。\n4. 使用单个 Gauss-Seidel 步骤更新尺度对偶变量 $u$。\n\n从增广拉格朗日函数的定义开始\n$$\n\\mathcal{L}_\\rho(x_1,x_2,x_3,u) \\;=\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\;+\\; \\frac{\\rho}{2}\\left(A_1 x_1 + A_2 x_2 + A_3 x_3 - c + u\\right)^2,\n$$\n每个块更新的一阶最优性条件产生了闭式线性更新。记\n$$\n\\theta_i \\;=\\; \\frac{\\rho A_i}{q_i + \\rho A_i^2}, \\quad i \\in \\{1,2,3\\},\n$$\n假设 $A_i \\neq 0$ 或 $q_i  0$，因此所有分母均为正。然后，对于 $c=0$（这隔离了齐次线性迭代），朴素的 Gauss-Seidel ADMM 块更新为：\n$$\nx_1^{k+1} = - \\theta_1\\left(A_2 x_2^k + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_2^{k+1} = - \\theta_2\\left(A_1 x_1^{k+1} + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_3^{k+1} = - \\theta_3\\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k\\right),\n$$\n$$\nu^{k+1} = u^k + \\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}\\right).\n$$\n\n这四个方程构成了关于状态向量 $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top$ 的线性迭代，可以写成\n$$\nz^{k+1} = J z^k,\n$$\n其中迭代矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 完全由 $(q_1,q_2,q_3)$、$(A_1,A_2,A_3)$ 和 $\\rho$ 决定。发散机制由 $J$ 的谱半径决定，即\n$$\n\\varrho(J) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } J\\}.\n$$\n如果 $\\varrho(J)  1$，迭代是线性不稳定的，并会沿着与不稳定特征值相关联的特征向量发散；如果 $\\varrho(J)  1$，迭代会收缩并线性收敛到不动点（最优解）；如果 $\\varrho(J) = 1$，迭代充其量是临界稳定的，可能无法收敛。\n\n任务：\n- 推导与上述齐次情况 $c=0$ 的 Gauss-Seidel ADMM 步骤相一致的显式迭代矩阵 $J$。\n- 根据 $\\varrho(J)$ 解释发散的机制。\n- 实现一个程序，该程序为给定的参数构造 $J$，计算 $\\varrho(J)$，并为一组覆盖收敛、临界稳定和发散情况的测试参数集报告该值。\n\n您的程序必须：\n1. 通过将线性更新映射应用于 $\\mathbb{R}^4$ 的每个标准基向量来构建迭代矩阵 $J$。\n2. 计算谱半径，即 $J$ 的特征值的最大模。\n3. 对于每个测试用例，以浮点数形式返回谱半径，并四舍五入到六位小数。\n\n用于覆盖不同情况的测试套件：\n- 情况1（强凸、良态，预期收缩）：$(q_1,q_2,q_3) = (10.0,10.0,10.0)$，$(A_1,A_2,A_3) = (1.0,1.0,1.0)$，$\\rho = 1.0$，$c = 0.0$。\n- 情况2（无目标函数二次项、单位系数，临界稳定）：$(q_1,q_2,q_3) = (0.0,0.0,0.0)$，$(A_1,A_2,A_3) = (1.0,1.0,1.0)$，$\\rho = 1.0$，$c = 0.0$。\n- 情况3（产生大块增益的低幅值和符号不定系数，预期发散）：$(q_1,q_2,q_3) = (0.01,0.01,0.01)$，$(A_1,A_2,A_3) = (0.2,-0.2,0.2)$，$\\rho = 1.0$，$c = 0.0$。\n- 情况4（混合符号与中等缩放，压力测试）：$(q_1,q_2,q_3) = (0.0,0.0,0.0)$，$(A_1,A_2,A_3) = (0.5,0.5,-0.5)$，$\\rho = 1.0$，$c = 0.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的谱半径，形式为逗号分隔的列表，并用方括号括起来，每个值四舍五入到六位小数（例如，“[0.732101,1.000000,1.284557,0.998340]”）。\n此问题不涉及物理单位，也没有角度；所有量都是无量纲实数。",
            "solution": "问题陈述已经过验证，被认为是科学可靠、适定、客观和完整的。所有参数和更新规则都已提供，可以直接明确地进行推导和分析。该问题涉及数值优化中的一个标准的、非平凡的主题——交替方向乘子法（ADMM）在超过两个块时的收敛性——并被构建为一个可解决的数学和计算任务。\n\n三块 ADMM 迭代的分析始于将给定的 Gauss-Seidel 更新步骤表述为对状态向量的单一线性变换。第 $k$ 次迭代的状态由向量 $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top \\in \\mathbb{R}^4$ 表示。迭代过程通过一个由迭代矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 表示的线性算子，将此状态映射到下一个状态 $z^{k+1}$，使得 $z^{k+1} = J z^k$。\n\n为了推导矩阵 $J$，我们首先表达齐次情况（$c=0$）的更新方程：\n$$x_1^{k+1} = - \\theta_1(A_2 x_2^k + A_3 x_3^k + u^k)$$\n$$x_2^{k+1} = - \\theta_2(A_1 x_1^{k+1} + A_3 x_3^k + u^k)$$\n$$x_3^{k+1} = - \\theta_3(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k)$$\n$$u^{k+1} = u^k + A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}$$\n\n这些方程构成一个耦合系统。上标为 $k+1$ 的变量是未知数，它们依赖于上标为 $k$ 的变量（来自上一次迭代的已知数）以及已在当前迭代中更新的其他变量（Gauss-Seidel 依赖关系）。为了形成一个标准的矩阵方程，我们通过将所有带上标 $k+1$ 的项移到左侧，将所有带上标 $k$ 的项移到右侧来重新排列这些方程。\n\n1. $1 \\cdot x_1^{k+1} = 0 \\cdot x_1^k - \\theta_1 A_2 x_2^k - \\theta_1 A_3 x_3^k - \\theta_1 u^k$\n2. $\\theta_2 A_1 x_1^{k+1} + 1 \\cdot x_2^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k - \\theta_2 A_3 x_3^k - \\theta_2 u^k$\n3. $\\theta_3 A_1 x_1^{k+1} + \\theta_3 A_2 x_2^{k+1} + 1 \\cdot x_3^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k - \\theta_3 u^k$\n4. $-A_1 x_1^{k+1} - A_2 x_2^{k+1} - A_3 x_3^{k+1} + 1 \\cdot u^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k + 1 \\cdot u^k$\n\n这个线性方程组可以写成矩阵形式 $L z^{k+1} = R z^k$，其中 $L$ 和 $R$ 是 $4 \\times 4$ 的矩阵。矩阵 $L$ 包含 $z^{k+1}$ 项的系数，矩阵 $R$ 包含 $z^k$ 项的系数。\n\n从以上方程，我们可以明确地构造 $L$ 和 $R$：\n$$\nL = \\begin{pmatrix}\n1  0  0  0 \\\\\n\\theta_2 A_1  1  0  0 \\\\\n\\theta_3 A_1  \\theta_3 A_2  1  0 \\\\\n-A_1  -A_2  -A_3  1\n\\end{pmatrix}\n$$\n矩阵 $L$ 是一个对角线上为一的下三角矩阵，这反映了 Gauss-Seidel 更新的顺序、前馈性质。这种结构保证了 $L$ 是可逆的。\n\n$$\nR = \\begin{pmatrix}\n0  -\\theta_1 A_2  -\\theta_1 A_3  -\\theta_1 \\\\\n0  0  -\\theta_2 A_3  -\\theta_2 \\\\\n0  0  0  -\\theta_3 \\\\\n0  0  0  1\n\\end{pmatrix}\n$$\n矩阵 $R$ 是上三角矩阵。第一列为零，因为 $x_1^k$ 未出现在任何更新方程的右侧。\n\n迭代矩阵 $J$ 随后通过求解 $z^{k+1}$ 得到：\n$$z^{k+1} = L^{-1} R z^k \\implies J = L^{-1} R$$\n在数值计算上，这是通过求解线性系统 $LJ = R$ 来计算矩阵 $J$ 的，这比直接计算 $L$ 的逆矩阵更稳定、更高效。\n\n迭代 $z^{k+1} = J z^k$ 的收敛性由 $J$ 的谱半径决定，记为 $\\varrho(J)$。谱半径定义为 $J$ 的特征值的最大模：$\\varrho(J) = \\max \\{|\\lambda_i|\\}$，其中 $\\lambda_i$ 是 $J$ 的特征值。\n发散的机制可以通过考虑迭代的长期行为来理解。任何初始状态 $z^0$ 通常可以表示为 $J$ 的特征向量的线性组合（为简化说明，假设 $J$ 是可对角化的）。设 $J v_i = \\lambda_i v_i$，其中 $v_i$ 是特征向量，$\\lambda_i$ 是特征值。如果 $z^0 = \\sum_i c_i v_i$，则经过 $k$ 次迭代后，状态为 $z^k = J^k z^0 = \\sum_i c_i \\lambda_i^k v_i$。\n- 如果 $\\varrho(J)  1$，则对所有 $i$ 都有 $|\\lambda_i|  1$。因此，当 $k \\to \\infty$ 时，$\\lambda_i^k \\to 0$，迭代收敛到零向量，这是不动点（对于 $c=0$ 也是最优解）。\n- 如果 $\\varrho(J)  1$，则存在至少一个特征值 $\\lambda_j$ 满足 $|\\lambda_j|  1$。相应的项 $c_j \\lambda_j^k v_j$ 的模将增长，导致状态向量 $z^k$ 发散，前提是初始状态 $z^0$ 在 $v_j$ 方向上有非零分量（即 $c_j \\neq 0$）。这是线性不稳定的条件。\n- 如果 $\\varrho(J) = 1$，迭代是临界稳定的。状态的某些分量可能不会衰减，导致振荡或缓慢的次线性收敛，如果模为 1 的特征值的几何重数小于其代数重数，甚至可能发散。\n\n下面的程序将实现此推导。对于每个测试用例，它将：\n1. 定义参数 $q_i, A_i, \\rho$。\n2. 计算中间变量 $\\theta_i = \\frac{\\rho A_i}{q_i + \\rho A_i^2}$。\n3. 根据上面推导的公式构造矩阵 $L$ 和 $R$。\n4. 求解线性矩阵方程 $LJ=R$ 以找到迭代矩阵 $J$。\n5. 计算 $J$ 的特征值。\n6. 通过找到特征值中的最大绝对值来确定谱半径 $\\varrho(J)$。\n7. 报告此值，四舍五入到六位小数。\n这个过程将对所有提供的测试用例重复进行，以探索不同的收敛情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the 3-block ADMM iteration matrix\n    for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple: ((q1, q2, q3), (A1, A2, A3), rho)\n    test_cases = [\n        # Case 1: strongly convex, well-conditioned, expected contraction\n        ((10.0, 10.0, 10.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 2: no objective quadratics, identity coefficients, marginal stability\n        ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 3: low-magnitude coefficients producing large gains, expected divergence\n        ((0.01, 0.01, 0.01), (0.2, -0.2, 0.2), 1.0),\n        # Case 4: mixed signs with moderate scaling, stress test\n        ((0.0, 0.0, 0.0), (0.5, 0.5, -0.5), 1.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        (q1, q2, q3), (A1, A2, A3), rho = case\n        \n        # Calculate theta_i values\n        # theta_i = (rho * A_i) / (q_i + rho * A_i^2)\n        # Denominators are positive as per problem constraints.\n        theta1 = (rho * A1) / (q1 + rho * A1**2)\n        theta2 = (rho * A2) / (q2 + rho * A2**2)\n        theta3 = (rho * A3) / (q3 + rho * A3**2)\n        \n        # Construct the L matrix from the equation L * z^{k+1} = R * z^k\n        # L = [[1, 0, 0, 0],\n        #      [theta2*A1, 1, 0, 0],\n        #      [theta3*A1, theta3*A2, 1, 0],\n        #      [-A1, -A2, -A3, 1]]\n        L = np.array([\n            [1.0, 0.0, 0.0, 0.0],\n            [theta2 * A1, 1.0, 0.0, 0.0],\n            [theta3 * A1, theta3 * A2, 1.0, 0.0],\n            [-A1, -A2, -A3, 1.0]\n        ])\n        \n        # Construct the R matrix\n        # R = [[0, -theta1*A2, -theta1*A3, -theta1],\n        #      [0, 0, -theta2*A3, -theta2],\n        #      [0, 0, 0, -theta3],\n        #      [0, 0, 0, 1]]\n        R = np.array([\n            [0.0, -theta1 * A2, -theta1 * A3, -theta1],\n            [0.0, 0.0, -theta2 * A3, -theta2],\n            [0.0, 0.0, 0.0, -theta3],\n            [0.0, 0.0, 0.0, 1.0]\n        ])\n\n        # The iteration matrix J is defined by z^{k+1} = J * z^k,\n        # where J = inv(L) * R.\n        # This is best solved as L*J = R.\n        J = np.linalg.solve(L, R)\n        \n        # Compute the eigenvalues of the iteration matrix J.\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # The spectral radius is the maximum magnitude of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        results.append(spectral_radius)\n\n    # Format the final output as a comma-separated list in brackets,\n    # with each value rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}