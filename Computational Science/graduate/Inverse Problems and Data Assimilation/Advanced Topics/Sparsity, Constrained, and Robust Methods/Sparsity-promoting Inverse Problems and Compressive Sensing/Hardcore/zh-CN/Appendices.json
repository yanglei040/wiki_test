{
    "hands_on_practices": [
        {
            "introduction": "本练习将深入探讨基追踪（Basis Pursuit, BP）问题，它是稀疏信号恢复领域的基石。虽然在线性约束下直接最小化 $\\ell_1$ 范数可能颇具挑战，但凸对偶这一强大框架为求解提供了优雅的途径。练习  将指导你推导一个BP问题的对偶问题，并利用最优性条件来找出唯一的稀疏解。通过这个“纸笔”练习，你将能深刻理解控制稀疏逆问题的数学机制，例如次梯度和互补松弛性。",
            "id": "3420230",
            "problem": "考虑一个数据同化情景，其中一个未知的二维状态向量 $x \\in \\mathbb{R}^{2}$ 由单个线性传感器观测。观测算子由矩阵 $A \\in \\mathbb{R}^{1 \\times 2}$ 表示，测量数据为 $y \\in \\mathbb{R}$。为了促进恢复状态的稀疏性，我们求解基追踪（Basis Pursuit, BP）逆问题，该问题在满足观测约束的条件下最小化 $x$ 的 $\\ell_{1}$ 范数。具体来说，取\n$$\nA = \\begin{pmatrix} 2  -1 \\end{pmatrix}, \\quad y = 3,\n$$\n并考虑以下BP问题\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y.\n$$\n从凸分析和对偶性的基本原理出发，使用凸共轭、对偶范数和鞍点最优性条件的定义来：\n1. 通过拉格朗日函数消除原变量，并通过 $\\ell_{\\infty}$ 范数确定对偶可行集，从而推导出与上述BP公式相关的对偶问题。\n2. 显式求解得到的对偶问题，以获得最优乘子。\n3. 通过 $\\ell_{1}$ 范数的次梯度刻画来施加可行性和互补松弛性条件，从而恢复原问题的最小化子。\n\n将您的最终答案表示为原问题最小化子 $x^{\\star}$，并将其精确地写成一个元素为有理数的单行向量。无需四舍五入，也无需报告单位。",
            "solution": "用户希望我使用对偶理论来解决一个基追踪（BP）问题。\n\n**问题分解：**\n\n1.  **原问题：**\n    -   最小化 $\\|x\\|_1$，其中 $x \\in \\mathbb{R}^2$。\n    -   约束条件为 $Ax = y$。\n    -   给定：$A = \\begin{pmatrix} 2  -1 \\end{pmatrix}$ 和 $y = 3$。\n\n2.  **任务：**\n    -   **任务1：推导对偶问题。**\n        -   使用拉格朗日函数。\n        -   消除原变量 $x$。\n        -   使用 $\\ell_\\infty$ 范数确定对偶可行集。这暗示了 $\\ell_1$ 和 $\\ell_\\infty$ 范数互为对偶的关系。\n    -   **任务2：求解对偶问题。**\n        -   找到最优对偶变量（拉格朗日乘子）。\n    -   **任务3：恢复原问题解。**\n        -   使用可行性（$Ax^\\star = y$）。\n        -   使用互补松弛性，这将涉及 $\\ell_1$ 范数的次梯度。\n\n**分步计划：**\n\n**第1部分：推导对偶问题**\n\n1.  **构造拉格朗日函数：**\n    问题的形式为 $\\min_x \\|x\\|_1$ 约束于 $Ax - y = 0$。拉格朗日函数为 $\\mathcal{L}(x, \\lambda) = \\|x\\|_1 + \\lambda^T(Ax - y)$，其中 $x \\in \\mathbb{R}^2$ 是原变量，$\\lambda \\in \\mathbb{R}^1$（一个标量）是对偶变量。\n    $\\mathcal{L}(x, \\lambda) = \\|x\\|_1 + \\langle A^T \\lambda, x \\rangle - y \\lambda$。\n\n2.  **求拉格朗日对偶函数：**\n    对偶函数 $g(\\lambda)$ 是拉格朗日函数关于 $x$ 的下确界：\n    $g(\\lambda) = \\inf_{x \\in \\mathbb{R}^2} \\mathcal{L}(x, \\lambda) = -y \\lambda + \\inf_{x \\in \\mathbb{R}^2} (\\|x\\|_1 + \\langle A^T \\lambda, x \\rangle)$。\n\n3.  **使用凸共轭：**\n    一个函数 $f(x)$ 的凸共轭是 $f^*(z) = \\sup_x (\\langle z, x \\rangle - f(x))$。$\\ell_1$ 范数的凸共轭是 $\\ell_\\infty$ 范数单位球的指示函数：\n    $(\\|\\cdot\\|_1)^*(z) = \\begin{cases} 0  \\text{if } \\|z\\|_{\\infty} \\le 1 \\\\ \\infty  \\text{if } \\|z\\|_{\\infty}  1 \\end{cases}$。\n    下确界项可以表示为 $ -(\\|\\cdot\\|_1)^*(-A^T\\lambda)$。因此，如果 $\\|A^T\\lambda\\|_{\\infty} \\le 1$，则该项为0；否则为 $-\\infty$。\n\n4.  **构建对偶问题：**\n    对偶问题是最大化对偶函数 $g(\\lambda)$：\n    $$\n    \\max_{\\lambda \\in \\mathbb{R}} \\quad -y \\lambda \\quad \\text{subject to} \\quad \\|A^T \\lambda\\|_{\\infty} \\le 1\n    $$\n    代入具体数值：$A^T = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$，$\\|A^T \\lambda\\|_{\\infty} = \\max(|2\\lambda|, |-\\lambda|) = 2|\\lambda|$。\n    约束变为 $2|\\lambda| \\le 1$，即 $|\\lambda| \\le 1/2$。\n    对偶问题是：\n    $$\n    \\max_{\\lambda \\in \\mathbb{R}} \\quad -3\\lambda \\quad \\text{subject to} \\quad -1/2 \\le \\lambda \\le 1/2\n    $$\n\n**第2部分：求解对偶问题**\n\n-   我们需要在区间 $[-1/2, 1/2]$ 上最大化函数 $f(\\lambda) = -3\\lambda$。\n-   这是一个递减线性函数，其最大值在可行集的最左端点取得。\n-   因此，对偶最优解是 $\\lambda^\\star = -1/2$。\n-   对偶问题的最优值是 $-3(-1/2) = 3/2$。根据强对偶性，这也是原问题的最优值。\n\n**第3部分：恢复原问题解**\n\n我们使用最优性条件（KKT条件），特别是平稳性条件：$-A^T \\lambda^\\star \\in \\partial \\|x^\\star\\|_1$。\n我们计算该条件中的向量 $-A^T \\lambda^\\star$。已知 $\\lambda^\\star = -1/2$ 且 $A^T = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$，我们有：\n$$\n-A^T \\lambda^\\star = - \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\left(-\\frac{1}{2}\\right) = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}\n$$\n令这个向量为 $z = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}$。\n根据 $\\ell_1$ 范数的次微分的定义， $z \\in \\partial \\|x^\\star\\|_1$ 意味着：\n-   对于分量 1：$z_1 = 1$。因为 $|z_1|=1$，这要求如果 $x^\\star_1 \\neq 0$，则 $\\text{sgn}(x^\\star_1) = 1$，即 $x^\\star_1 \\ge 0$。\n-   对于分量 2：$z_2 = -1/2$。因为 $|z_2|  1$，这要求 $x^\\star_2 = 0$。\n综上，我们得到解的形式为 $x^\\star = \\begin{pmatrix} x^\\star_1 \\\\ 0 \\end{pmatrix}$，其中 $x^\\star_1 \\ge 0$。\n\n最后，我们施加原问题可行性约束，$Ax^\\star = y$：\n$$\n\\begin{pmatrix} 2  -1 \\end{pmatrix} \\begin{pmatrix} x^\\star_1 \\\\ 0 \\end{pmatrix} = 3\n$$\n这简化为 $2x^\\star_1 = 3$，得出 $x^\\star_1 = \\frac{3}{2}$。\n这个结果与条件 $x^\\star_1 \\ge 0$ 一致。\n因此，原问题最小化子是 $x^\\star = \\begin{pmatrix} 3/2 \\\\ 0 \\end{pmatrix}$。\n最终答案必须是一个元素为有理数的行向量：$\\begin{pmatrix} \\frac{3}{2}  0 \\end{pmatrix}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在分析了基追踪的理论特性后，我们现在转向与之密切相关的LASSO问题的实用数值解法。迭代软阈值算法（Iterative Shrinkage-Thresholding Algorithm, ISTA）是一种基础近端梯度方法，用于求解此类复合凸优化问题。 这个动手编程练习将让你从零开始实现ISTA，包括其核心的软阈值算子，并数值验证其理论上的收敛保证。完成这个实践将帮助你跨越从凸分析理论到在计算科学中无处不在的算法实现之间的鸿沟。",
            "id": "3420203",
            "problem": "考虑复合凸目标函数 $f(x) = g(x) + \\lambda \\|x\\|_{1}$，其中 $g(x)$ 是一个可微凸函数，其梯度 $\\nabla g$ 是 Lipschitz 连续的，且 $\\lambda \\geq 0$。在标准稀疏线性逆问题模型中，令 $g(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$，其中给定 $A \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^{m}$。其梯度为 $\\nabla g(x) = A^{\\top}(A x - b)$，$\\nabla g$ 的一个有效 Lipschitz 常数是 $L = \\lambda_{\\max}(A^{\\top}A)$，其中 $\\lambda_{\\max}$ 表示最大特征值。用于求解 $f(x)$ 的迭代收缩阈值算法 (ISTA)，其步长 $\\alpha$ 满足 $\\alpha \\leq 1/L$，通过以下方式更新 $x$：\n$$\nx^{k+1} = S_{\\lambda \\alpha}\\left(x^{k} - \\alpha \\nabla g(x^{k})\\right),\n$$\n其中 $S_{\\tau}$ 是坐标级软阈值算子，定义为 $S_{\\tau}(u)_{i} = \\operatorname{sign}(u_{i}) \\max(|u_{i}| - \\tau, 0)$。\n\n从零向量 $x^{0} = 0$ 开始，对以下每个测试用例，实现 ISTA 算法并进行固定次数 $T$ 的迭代。对每个测试用例，精确计算 $L = \\lambda_{\\max}(A^{\\top}A)$，使用给定的因子 $\\eta$ 设置 $\\alpha = \\eta/L$，运行 ISTA 进行 $T$ 次迭代，并记录目标值序列 $f(x^{k})$，其中 $k = 0, 1, \\dots, T$。使用欧几里得范数 $\\|\\cdot\\|_{2}$ 和 1-范数 $\\|\\cdot\\|_{1}$。下面的条目均为精确有理数值。不涉及物理单位。\n\n测试套件：\n- 用例 1（通用良态，边界步长）：\n  - $A \\in \\mathbb{R}^{4 \\times 6}$ 为\n    $$\n    A = \\begin{bmatrix}\n    1  0  1  0  2  -1 \\\\\n    0  1  -1  2  0  1 \\\\\n    2  -1  0  1  1  0 \\\\\n    0  2  1  -1  0  1\n    \\end{bmatrix}.\n    $$\n  - $b \\in \\mathbb{R}^{4}$ 为 $b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{bmatrix}$。\n  - $\\lambda = 0.1$。\n  - $T = 10$。\n  - $\\eta = 1.0$（因此 $\\alpha = 1/L$）。\n- 用例 2（秩亏传感，零数据，保守步长）：\n  - $A \\in \\mathbb{R}^{3 \\times 5}$ 为\n    $$\n    A = \\begin{bmatrix}\n    1  0  1  0  2 \\\\\n    0  1  0  1  1 \\\\\n    1  0  1  0  2\n    \\end{bmatrix}.\n    $$\n  - $b \\in \\mathbb{R}^{3}$ 为 $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n  - $\\lambda = 0.2$。\n  - $T = 8$。\n  - $\\eta = 0.9$（因此 $\\alpha = 0.9/L$）。\n- 用例 3（对角传感，强稀疏性惩罚，边界步长）：\n  - $A \\in \\mathbb{R}^{5 \\times 5}$ 为\n    $$\n    A = \\begin{bmatrix}\n    1  0  0  0  0 \\\\\n    0  0.5  0  0  0 \\\\\n    0  0  2  0  0 \\\\\n    0  0  0  1.5  0 \\\\\n    0  0  0  0  0.8\n    \\end{bmatrix}.\n    $$\n  - $b \\in \\mathbb{R}^{5}$ 为 $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ -2 \\\\ 0.5 \\end{bmatrix}$。\n  - $\\lambda = 0.5$。\n  - $T = 8$。\n  - $\\eta = 1.0$（因此 $\\alpha = 1/L$）。\n- 用例 4（无适度正则化，低于边界）：\n  - $A \\in \\mathbb{R}^{4 \\times 4}$ 为\n    $$\n    A = \\begin{bmatrix}\n    2  -1  0  1 \\\\\n    0  1  2  -1 \\\\\n    1  0  -1  2 \\\\\n    -2  1  0  1\n    \\end{bmatrix}.\n    $$\n  - $b \\in \\mathbb{R}^{4}$ 为 $b = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0 \\\\ 3 \\end{bmatrix}$。\n  - $\\lambda = 0$。\n  - $T = 6$。\n  - $\\eta = 0.8$（因此 $\\alpha = 0.8/L$）。\n\n对每个用例，输出两项内容：目标值列表 $[f(x^{0}), f(x^{1}), \\dots, f(x^{T})]$ 和一个布尔值，该布尔值指示序列是否为非增序列（即对所有 $k$ 都满足 $f(x^{k+1}) \\leq f(x^{k})$ 的单调递减）。你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身是一个形式为 $[\\text{objective\\_sequence}, \\text{is\\_monotone}]$ 的双元素列表。例如，总输出格式为 $[[\\cdots, \\cdots],[\\cdots, \\cdots],[\\cdots, \\cdots],[\\cdots, \\cdots]]$，不含任何额外文本。",
            "solution": "用户希望实现迭代收缩阈值算法 (ISTA) 来解决一个稀疏逆问题，并在多个测试用例上分析其行为。\n\n### 第 1 步：提取已知条件\n问题定义了一个复合凸目标函数 $f(x) = g(x) + \\lambda \\|x\\|_{1}$，其中 $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$。\n平滑部分 $g(x)$ 的梯度为 $\\nabla g(x) = A^{\\top}(A x - b)$。\n梯度 $\\nabla g$ 的 Lipschitz 常数由 $L = \\lambda_{\\max}(A^{\\top}A)$ 给出，其中 $\\lambda_{\\max}$ 是最大特征值。\nISTA 的更新规则由下式给出：\n$$x^{k+1} = S_{\\lambda \\alpha}\\left(x^{k} - \\alpha \\nabla g(x^{k})\\right)$$\n其中 $S_{\\tau}$ 是软阈值算子，按坐标定义为 $S_{\\tau}(u)_{i} = \\operatorname{sign}(u_{i}) \\max(|u_{i}| - \\tau, 0)$。\n初始条件为 $x^{0} = 0$。\n步长为 $\\alpha = \\eta/L$。\n任务是计算目标值的序列 $f(x^k)$，其中 $k=0, 1, \\dots, T$，并检查此序列是否非增。\n\n问题提供了四个具有特定参数的测试用例：\n- **用例 1**：\n  - $A = \\begin{bmatrix} 1  0  1  0  2  -1 \\\\ 0  1  -1  2  0  1 \\\\ 2  -1  0  1  1  0 \\\\ 0  2  1  -1  0  1 \\end{bmatrix}$\n  - $b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{bmatrix}$\n  - $\\lambda = 0.1$\n  - $T = 10$\n  - $\\eta = 1.0$\n- **用例 2**：\n  - $A = \\begin{bmatrix} 1  0  1  0  2 \\\\ 0  1  0  1  1 \\\\ 1  0  1  0  2 \\end{bmatrix}$\n  - $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$\n  - $\\lambda = 0.2$\n  - $T = 8$\n  - $\\eta = 0.9$\n- **用例 3**：\n  - $A = \\mathrm{diag}(1, 0.5, 2, 1.5, 0.8)$\n  - $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ -2 \\\\ 0.5 \\end{bmatrix}$\n  - $\\lambda = 0.5$\n  - $T = 8$\n  - $\\eta = 1.0$\n- **用例 4**：\n  - $A = \\begin{bmatrix} 2  -1  0  1 \\\\ 0  1  2  -1 \\\\ 1  0  -1  2 \\\\ -2  1  0  1 \\end{bmatrix}$\n  - $b = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0 \\\\ 3 \\end{bmatrix}$\n  - $\\lambda = 0$\n  - $T = 6$\n  - $\\eta = 0.8$\n\n每个用例的所需输出是一个包含两个元素的列表：目标值序列 $[f(x^0), \\dots, f(x^T)]$ 和一个指示序列是否非增（即对所有 $k$ 满足 $f(x^{k+1}) \\le f(x^k)$）的布尔值。\n\n### 第 2 步：使用提取的已知条件进行验证\n根据指定标准对问题进行验证。\n- **科学基础**：问题陈述基于凸优化和稀疏信号恢复的成熟理论。ISTA 算法、LASSO 目标函数、Lipschitz 连续性概念以及步长的选择都是该领域的标准且正确的元素。\n- **适定性**：问题是一个计算任务，而非关于极小值点存在的理论问题。对于每个测试用例，所有必要组件（$A$、$b$、$\\lambda$、$T$、$\\eta$、$x^0$）都已提供。该算法是确定性的，可以执行指定次数的迭代以产生唯一的计算结果。\n- **客观性**：问题使用精确的数学定义和数值数据来陈述。没有主观或模糊的语言。\n- **完整性与一致性**：问题是自洽的。所有矩阵、向量和参数都已完全指定。没有矛盾之处。例如，在用例 4 中，$\\lambda=0$，$\\ell_1$ 项消失，ISTA 正确地简化为二次函数的梯度下降，这是一个一致的特例。\n- **现实性与可行性**：问题涉及数值模拟。矩阵和向量的维度很小，迭代次数也很低，这使得计算完全可行。数值是此类示例的标准值。\n- **其他缺陷**：问题是标准数值算法的直接且非平凡的应用。它不是同义反复、隐喻性的，也不在科学可验证性范围之外。\n\n### 第 3 步：结论与行动\n问题是有效的。将通过实现所述的 ISTA 算法来提供解决方案。\n\n### 算法设计与实现\n解决方案涉及实现 ISTA 算法并将其应用于四个测试用例中的每一个。每个用例的核心逻辑如下：\n\n1.  **初始化**：对于一个用例，给定矩阵 $A$、$b$ 和参数 $\\lambda$、$T$、$\\eta$，将解向量 $x^k$ 在 $k=0$ 时初始化为零向量，即 $x^0 = 0 \\in \\mathbb{R}^n$。\n\n2.  **L-常数计算**：计算 Lipschitz 常数 $L = \\lambda_{\\max}(A^{\\top}A)$。由于 $A^{\\top}A$ 是一个实对称半正定矩阵，其特征值是实数且非负。我们可以使用数值线性代数库可靠地计算它们。最大特征值即为 $L$。\n\n3.  **步长计算**：确定算法的步长 $\\alpha = \\eta/L$。同时，预先计算软阈值算子的阈值 $\\tau = \\lambda \\alpha$。\n\n4.  **迭代**：对于 $k = 0, 1, \\dots, T$ 进行循环。在循环的每一步中：\n    a.  **目标评估**：计算目标函数值 $f(x^k) = \\frac{1}{2}\\|A x^k - b\\|_{2}^{2} + \\lambda \\|x^k\\|_{1}$。将此值存储在一个序列中。\n    b.  **更新步骤（对于 $k  T$）**：如果不是最后一次迭代，则计算下一个迭代点 $x^{k+1}$。\n        i.  **梯度计算**：计算平滑项的梯度 $\\nabla g(x^k) = A^{\\top}(A x^k - b)$。\n        ii. **梯度下降步骤**：沿负梯度方向迈出一步：$u = x^k - \\alpha \\nabla g(x^k)$。\n        iii. **近端步骤（软阈值）**：应用软阈值算子得到下一个迭代点：$x^{k+1} = S_{\\tau}(u)$。该算子按元素作用。\n\n5.  **单调性检查**：循环结束后，目标值序列 $[f(x^0), f(x^1), \\dots, f(x^T)]$ 就完整了。检查该序列是否非增，即对于所有 $k \\in \\{0, \\dots, T-1\\}$ 是否满足 $f(x^{k+1}) \\leq f(x^k)$。在比较中使用了小容差以处理潜在的浮点不精确性。理论保证是，对于 $\\alpha \\in (0, 1/L]$，ISTA 生成的函数值序列是非增的。所有测试用例都满足这个关于 $\\alpha$ 的条件（因为 $\\eta \\le 1.0$），所以预期序列是非增的。\n\n6.  **输出格式化**：对每个用例，将目标值列表和布尔单调性标志组合成一个双元素列表。然后将这些结果聚合到一个最终列表中，该列表包含所有四个用例的结果。最终输出被格式化为该嵌套列表结构的单行字符串表示，不含空格。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ISTA problem for a suite of test cases.\n    \"\"\"\n\n    def soft_thresholding(u, tau):\n        \"\"\"Coordinate-wise soft-thresholding operator.\"\"\"\n        return np.sign(u) * np.maximum(np.abs(u) - tau, 0.0)\n\n    def objective_function(x, A, b, lam):\n        \"\"\"Composite objective function f(x) = g(x) + lambda*||x||_1.\"\"\"\n        residual = A @ x - b\n        # Using np.dot for ||v||_2^2 is more efficient than np.linalg.norm(v)**2\n        g_x = 0.5 * np.dot(residual, residual)\n        h_x = lam * np.linalg.norm(x, 1)\n        return g_x + h_x\n\n    def gradient_g(x, A, b):\n        \"\"\"Gradient of the smooth part g(x) = 0.5*||Ax-b||_2^2.\"\"\"\n        return A.T @ (A @ x - b)\n    \n    def run_ista(A, b, lam, T, eta):\n        \"\"\"\n        Runs the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n        Returns the sequence of objective values and a boolean for monotonicity.\n        \"\"\"\n        _, n = A.shape\n        \n        # Initialize x\n        x = np.zeros(n, dtype=float)\n        \n        # Compute Lipschitz constant L = lambda_max(A^T A)\n        AtA = A.T @ A\n        # For real symmetric matrices, eigvalsh is numerically stable and efficient.\n        eigenvalues = np.linalg.eigvalsh(AtA)\n        L = np.max(eigenvalues) if eigenvalues.size > 0 else 0.0\n\n        # Handle the case where L is zero or near-zero to avoid division by zero.\n        if L  1e-15:\n            # If L is zero, A must be the zero matrix. The gradient is always zero,\n            # so x will not update from x_0 = 0. The objective is constant.\n            obj_val = objective_function(x, A, b, lam)\n            obj_sequence = [obj_val] * (T + 1)\n            is_monotone = True\n            return obj_sequence, is_monotone\n\n        # Set step size and threshold parameter\n        alpha = eta / L\n        tau = lam * alpha\n        \n        obj_sequence = []\n        \n        for k in range(T + 1):\n            # Calculate and store objective value for the current iterate x^k\n            current_obj = objective_function(x, A, b, lam)\n            obj_sequence.append(current_obj)\n            \n            # Compute the next iterate x^(k+1)\n            if k  T:\n                grad = gradient_g(x, A, b)\n                u = x - alpha * grad\n                x = soft_thresholding(u, tau)\n        \n        # Check for non-increasing property f(x^{k+1}) = f(x^k)\n        # A small tolerance is used for robust floating-point comparisons.\n        is_monotone = True\n        for i in range(len(obj_sequence) - 1):\n            if obj_sequence[i+1] > obj_sequence[i] + 1e-12:\n                is_monotone = False\n                break\n                \n        return obj_sequence, is_monotone\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\"A\": np.array([[1,0,1,0,2,-1],[0,1,-1,2,0,1],[2,-1,0,1,1,0],[0,2,1,-1,0,1]], dtype=float), \"b\": np.array([1,0,2,-1], dtype=float), \"lambda\": 0.1, \"T\": 10, \"eta\": 1.0},\n        # Case 2\n        {\"A\": np.array([[1,0,1,0,2],[0,1,0,1,1],[1,0,1,0,2]], dtype=float), \"b\": np.array([0,0,0], dtype=float), \"lambda\": 0.2, \"T\": 8, \"eta\": 0.9},\n        # Case 3\n        {\"A\": np.array([[1,0,0,0,0],[0,0.5,0,0,0],[0,0,2,0,0],[0,0,0,1.5,0],[0,0,0,0,0.8]], dtype=float), \"b\": np.array([1,-1,2,-2,0.5], dtype=float), \"lambda\": 0.5, \"T\": 8, \"eta\": 1.0},\n        # Case 4\n        {\"A\": np.array([[2,-1,0,1],[0,1,2,-1],[1,0,-1,2],[-2,1,0,1]], dtype=float), \"b\": np.array([1,-2,0,3], dtype=float), \"lambda\": 0.0, \"T\": 6, \"eta\": 0.8}\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        result_seq, result_mono = run_ista(case[\"A\"], case[\"b\"], case[\"lambda\"], case[\"T\"], case[\"eta\"])\n        all_results.append([result_seq, result_mono])\n    \n    # Format the final output string as a compact, space-free list representation.\n    # repr() creates a string representation, and .replace(' ', '') removes all spaces.\n    final_output_string = repr(all_results).replace(\" \", \"\")\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "稀疏性的原理远不止于向量。现在我们将此概念推广到矩阵，目标是恢复一个低秩矩阵。本练习将探索矩阵填充问题，这是现代数据科学中的一个关键问题，它可以通过最小化核范数（秩函数的凸代理）来求解。在练习  中，你将执行近端梯度法的单步迭代，这需要你计算奇异值阈值（Singular Value Thresholding, SVT）算子。SVT算子是向量软阈值算子在矩阵上的直接对应，这展示了核心概念在不同问题领域之间深刻的统一性。",
            "id": "3420172",
            "problem": "考虑一个矩阵补全问题，其形式是最小化一个数据失配项与一个低秩正则化项之和。设观测算子为正交投影算子 $P_{\\Omega} : \\mathbb{R}^{2 \\times 2} \\to \\mathbb{R}^{2 \\times 2}$，其定义为：若 $(i,j) \\in \\Omega$，则 $P_{\\Omega}(X)_{ij} = X_{ij}$；否则 $P_{\\Omega}(X)_{ij} = 0$。其中 $\\Omega = \\{(1,1),(2,2)\\}$。设观测数据为矩阵 $M \\in \\mathbb{R}^{2 \\times 2}$，给定如下：\n$$\nM = \\begin{pmatrix}\n5  0 \\\\\n0  1\n\\end{pmatrix}.\n$$\n定义光滑损失函数 $g(X) = \\frac{1}{2} \\| P_{\\Omega}(X) - P_{\\Omega}(M) \\|_{F}^{2}$，其中 $\\| \\cdot \\|_{F}$ 表示弗罗贝尼乌斯范数；以及非光滑正则化项 $h(X) = \\tau \\| X \\|_{*}$，其中 $\\| X \\|_{*}$ 表示核范数，等于矩阵 $X$ 的奇异值之和。考虑对 $g+h$ 应用一步近端梯度法（也称为前向-后向分裂算法），从零矩阵 $X^{(0)} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$ 开始，步长 $\\mu = \\frac{1}{3}$，正则化权重 $\\tau = 2$。对于一个正常、下半连续的凸函数 $h$ 和参数 $\\lambda  0$，其近端算子定义为：\n$$\n\\operatorname{prox}_{\\lambda h}(Y) = \\arg\\min_{X} \\left\\{ \\lambda h(X) + \\frac{1}{2} \\| X - Y \\|_{F}^{2} \\right\\}.\n$$\n您需要计算 $g$ 的梯度，执行梯度步 $Y = X^{(0)} - \\mu \\nabla g(X^{(0)})$，然后应用近端算子 $\\operatorname{prox}_{\\mu h}(Y)$。在此背景下，核范数的近端算子是奇异值阈值（SVT）算子；您可以在推导中使用奇异值分解（SVD）$Y = U \\operatorname{diag}(\\sigma_{i}) V^{\\top}$。\n\n在执行完这单步近端梯度迭代后，计算目标函数 $J\\big(X^{(1)}\\big)$ 的值：\n$$\nJ\\big(X^{(1)}\\big) = \\frac{1}{2} \\| P_{\\Omega}\\big(X^{(1)}\\big) - P_{\\Omega}(M) \\|_{F}^{2} + \\tau \\| X^{(1)} \\|_{*}.\n$$\n请以一个精确数值的形式给出您的最终答案。不要进行四舍五入。最终答案无需单位。",
            "solution": "该问题经验证具有科学依据、是良定的、客观且信息自洽的。所有必要信息均已提供，任务是在反问题优化领域内执行一个标准计算。因此，我们可以着手求解。\n\n问题要求计算经过一次近端梯度法迭代后目标函数 $J(X^{(1)})$ 的值。目标函数为 $J(X) = g(X) + h(X)$，其中 $g(X) = \\frac{1}{2} \\| P_{\\Omega}(X) - P_{\\Omega}(M) \\|_{F}^{2}$ 是光滑的数据保真项，$h(X) = \\tau \\| X \\|_{*}$ 是非光滑的正则化项。\n\n近端梯度迭代由下式给出：\n$$\nX^{(k+1)} = \\operatorname{prox}_{\\mu h}\\left(X^{(k)} - \\mu \\nabla g(X^{(k)})\\right)\n$$\n我们需要从 $X^{(0)} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$ 开始计算 $X^{(1)}$。参数为步长 $\\mu = \\frac{1}{3}$ 和正则化权重 $\\tau = 2$。观测数据矩阵为 $M = \\begin{pmatrix} 5  0 \\\\ 0  1 \\end{pmatrix}$，观测集为 $\\Omega = \\{(1,1), (2,2)\\}$。\n\n首先，我们计算光滑项 $g(X)$ 的梯度。函数 $g(X)$ 是弗罗贝尼乌斯范数的平方与一个线性算子的复合。$g(X) = \\frac{1}{2} \\| A(X) - B \\|_{F}^{2}$（其中 $A(X) = P_{\\Omega}(X)$ 且 $B = P_{\\Omega}(M)$）的梯度由 $\\nabla g(X) = A^*(A(X)-B)$ 给出，其中 $A^*$ 是算子 $A$ 的伴随算子。算子 $P_{\\Omega}$ 是一个正交投影算子，因此它是自伴的，即 $P_{\\Omega}^* = P_{\\Omega}$。此外，由于 $P_{\\Omega}$ 是一个投影算子，有 $P_{\\Omega} \\circ P_{\\Omega} = P_{\\Omega}$。\n因此，$g(X)$ 的梯度为：\n$$\n\\nabla g(X) = P_{\\Omega}^*(P_{\\Omega}(X) - P_{\\Omega}(M)) = P_{\\Omega}(P_{\\Omega}(X) - P_{\\Omega}(M)) = P_{\\Omega}(X) - P_{\\Omega}(M)\n$$\n我们在初始点 $X^{(0)} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$ 处计算这个梯度：\n$$\n\\nabla g(X^{(0)}) = P_{\\Omega}(X^{(0)}) - P_{\\Omega}(M)\n$$\n已知 $X^{(0)}$ 是零矩阵，则 $P_{\\Omega}(X^{(0)})$ 也是零矩阵。算子 $P_{\\Omega}$ 的作用是将集合 $\\Omega = \\{(1,1), (2,2)\\}$ 之外的元素置为零。对于给定的矩阵 $M$，我们有：\n$$\nP_{\\Omega}(M) = P_{\\Omega}\\left(\\begin{pmatrix} 5  0 \\\\ 0  1 \\end{pmatrix}\\right) = \\begin{pmatrix} 5  0 \\\\ 0  1 \\end{pmatrix} = M\n$$\n因此，在 $X^{(0)}$ 处的梯度为：\n$$\n\\nabla g(X^{(0)}) = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} - \\begin{pmatrix} 5  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} -5  0 \\\\ 0  -1 \\end{pmatrix}\n$$\n接下来，我们执行梯度下降步（“前向”步）来求得中间矩阵 $Y$：\n$$\nY = X^{(0)} - \\mu \\nabla g(X^{(0)}) = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} -5  0 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{3}  0 \\\\ 0  \\frac{1}{3} \\end{pmatrix}\n$$\n现在，我们将近端算子（“后向”步）应用于 $Y$ 来求得 $X^{(1)}$：\n$$\nX^{(1)} = \\operatorname{prox}_{\\mu h}(Y) = \\operatorname{prox}_{\\mu \\tau \\| \\cdot \\|_{*}}(Y)\n$$\n核范数的近端算子是奇异值阈值（SVT）算子。对于一个具有奇异值分解（SVD）$Y = U \\Sigma V^{\\top}$（其中 $\\Sigma = \\operatorname{diag}(\\sigma_i)$）的矩阵 $Y$，阈值为 $\\lambda$ 的SVT算子定义为 $S_{\\lambda}(Y) = U S_{\\lambda}(\\Sigma) V^{\\top}$，其中 $S_{\\lambda}(\\Sigma) = \\operatorname{diag}(\\max(0, \\sigma_i - \\lambda))$。\n在本例中，阈值为 $\\lambda = \\mu \\tau = \\frac{1}{3} \\times 2 = \\frac{2}{3}$。\n矩阵 $Y$ 已经是一个对角矩阵，所以其SVD是平凡的。奇异值是对角元素的绝对值：$\\sigma_1 = \\frac{5}{3}$ 和 $\\sigma_2 = \\frac{1}{3}$。对应的左右奇异向量是标准基向量，因此 $U = V = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n\n我们将软阈值函数应用于奇异值：\n$$\n\\hat{\\sigma}_1 = \\max\\left(0, \\sigma_1 - \\lambda\\right) = \\max\\left(0, \\frac{5}{3} - \\frac{2}{3}\\right) = \\max\\left(0, \\frac{3}{3}\\right) = 1\n$$\n$$\n\\hat{\\sigma}_2 = \\max\\left(0, \\sigma_2 - \\lambda\\right) = \\max\\left(0, \\frac{1}{3} - \\frac{2}{3}\\right) = \\max\\left(0, -\\frac{1}{3}\\right) = 0\n$$\n我们使用经过阈值处理的奇异值重构矩阵 $X^{(1)}$：\n$$\nX^{(1)} = U \\operatorname{diag}(\\hat{\\sigma}_1, \\hat{\\sigma}_2) V^{\\top} = I \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} I^{\\top} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n$$\n最后，我们计算目标函数 $J(X^{(1)})$ 的值：\n$$\nJ\\big(X^{(1)}\\big) = \\frac{1}{2} \\| P_{\\Omega}\\big(X^{(1)}\\big) - P_{\\Omega}(M) \\|_{F}^{2} + \\tau \\| X^{(1)} \\|_{*}\n$$\n首先，我们计算数据保真项：\n$$\n P_{\\Omega}\\big(X^{(1)}\\big) = P_{\\Omega}\\left(\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\\right) = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n$$\n差值项为：\n$$\nP_{\\Omega}\\big(X^{(1)}\\big) - P_{\\Omega}(M) = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} - \\begin{pmatrix} 5  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} -4  0 \\\\ 0  -1 \\end{pmatrix}\n$$\n该差值的弗罗贝尼乌斯范数的平方为：\n$$\n\\| P_{\\Omega}\\big(X^{(1)}\\big) - P_{\\Omega}(M) \\|_{F}^{2} = (-4)^2 + 0^2 + 0^2 + (-1)^2 = 16 + 1 = 17\n$$\n所以，数据保真项为 $\\frac{1}{2} \\times 17 = \\frac{17}{2}$。\n\n接下来，我们计算正则化项 $\\tau \\| X^{(1)} \\|_{*}$。核范数 $\\| X^{(1)} \\|_{*}$ 是其奇异值之和。$X^{(1)} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$ 的奇异值为 $\\sigma_1=1$ 和 $\\sigma_2=0$。\n$$\n\\| X^{(1)} \\|_{*} = 1 + 0 = 1\n$$\n正则化项为 $\\tau \\| X^{(1)} \\|_{*} = 2 \\times 1 = 2$。\n\n目标函数的总值是这两项之和：\n$$\nJ\\big(X^{(1)}\\big) = \\frac{17}{2} + 2 = \\frac{17}{2} + \\frac{4}{2} = \\frac{21}{2}\n$$",
            "answer": "$$\n\\boxed{\\frac{21}{2}}\n$$"
        }
    ]
}