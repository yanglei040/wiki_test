{
    "hands_on_practices": [
        {
            "introduction": "This first exercise breaks down the proximal gradient method into its two core components: a standard gradient descent step on the smooth part of the objective, followed by a \"proximal correction\" step for the non-smooth part. By working through a single iteration for a simple constrained quadratic problem, you will gain concrete, hands-on experience with how these two pieces interact to move an iterate towards a constrained minimum. This practice is fundamental to building intuition for the algorithm's behavior .",
            "id": "2195110",
            "problem": "Consider the optimization problem of finding a point $x = (x_1, x_2) \\in \\mathbb{R}^2$ that minimizes a function $F(x)$ subject to non-negativity constraints on its components, i.e., $x_1 \\ge 0$ and $x_2 \\ge 0$. The function to be minimized is the squared Euclidean distance from $x$ to a target point $a$, given by $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$.\n\nThis problem can be cast into the standard form for proximal algorithms, $\\min_{x} f(x) + g(x)$, by defining the smooth part as $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$ and the non-smooth part $g(x)$ as the indicator function for the non-negative orthant. The indicator function $g(x)$ is zero if $x_1 \\ge 0$ and $x_2 \\ge 0$, and infinity otherwise.\n\nYou are tasked with applying the proximal gradient method to solve this problem. The iterative update rule for the proximal gradient method is given by:\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\nwhere $\\gamma$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator associated with the function $g$.\n\nGiven the target point $a = (5, -4)$, the initial point $x_0 = (1, 1)$, and a step size of $\\gamma = 0.2$, compute the next iterate, $x_1$. Express your answer as a row vector $(x_{1,1}, x_{1,2})$, where $x_{1,1}$ and $x_{1,2}$ are the components of the vector $x_1$.",
            "solution": "We are minimizing $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ over the non-negative orthant. In the proximal gradient decomposition, set $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ and $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$, the indicator of the feasible set $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$.\n\nThe gradient of $f$ is given by\n$$\n\\nabla f(x)=x-a.\n$$\nThe proximal operator of $\\gamma g$ at a point $z$ is the Euclidean projection onto $\\mathbb{R}_{+}^{2}$:\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\nwhich is the componentwise truncation at zero:\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\nWith $a=(5,-4)$, $x_{0}=(1,1)$, and $\\gamma=0.2$, compute the gradient at $x_{0}$:\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\nPerform the gradient step:\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\nApply the proximal map, i.e., the projection onto $\\mathbb{R}_{+}^{2}$:\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\nsince both components are already non-negative.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The convergence of the proximal gradient method is not guaranteed for any arbitrary choice of step size. This exercise explores the critical relationship between the step size, $\\gamma$, the geometry of the smooth function captured by its Lipschitz constant $L$, and the stability of the iteration . By analyzing a case where the method reduces to simple gradient descent, you will derive the famous convergence condition $\\gamma \\in (0, 2/L)$ and see firsthand how violating it leads to instability, providing essential intuition for why adaptive step-size strategies are indispensable in practice.",
            "id": "3415791",
            "problem": "Consider the composite objective associated with a linear inverse problem, defined for a scalar decision variable $x \\in \\mathbb{R}$ by $F(x) = f(x) + g(x)$ with $f(x) = \\frac{1}{2}\\|A x - b\\|^{2}$ and $g(x)$ a proper, lower semicontinuous, convex function. Assume $A = \\sqrt{L}$ with $L  0$ and $b = 0$, so that $f(x) = \\frac{L}{2}x^{2}$ and the gradient $\\nabla f$ is Lipschitz continuous with Lipschitz constant $L$. The Proximal Gradient Method (PGM) is defined by the iteration $x_{k+1} = \\operatorname{prox}_{\\gamma g}(x_{k} - \\gamma \\nabla f(x_{k}))$, where $\\gamma  0$ is a constant step size and $\\operatorname{prox}_{\\gamma g}$ denotes the proximal operator of $\\gamma g$. Consider the special case $g(x) = 0$ and an initialization $x_{0} \\neq 0$.\n\nStarting from first principles and the definition of the proximal operator, derive the explicit iteration map for $x_{k+1}$ in terms of $x_{k}$, and obtain a closed-form expression for $x_{k}$ as a function of $k$, $x_{0}$, $\\gamma$, and $L$. Using this derivation, establish the precise condition on $\\gamma$ for which the iteration converges. Then, construct a concrete counterexample that exhibits divergence with oscillations by selecting $\\gamma = \\frac{5}{2L}$, and compute the spectral radius of the resulting linear iteration operator under this choice of $\\gamma$. Report the spectral radius as your final answer.\n\nFinally, discuss scientifically sound practical safeguards in proximal gradient methods that prevent instability in inverse problems and data assimilation workflows when the Lipschitz constant $L$ is unknown or poorly estimated. Your discussion should address why these safeguards work from first principles and should be consistent with the assumptions stated above. The final reported quantity must be the spectral radius, expressed as a single real-valued number or a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem asks for an analysis of the Proximal Gradient Method (PGM) under a specific set of conditions. The objective function is $F(x) = f(x) + g(x)$ for a scalar variable $x \\in \\mathbb{R}$. The components are given as $f(x) = \\frac{1}{2}\\|Ax - b\\|^2$ and $g(x)$, a proper, lower semicontinuous, convex function. The parameters are specified as $A = \\sqrt{L}$ with $L  0$, and $b = 0$. This simplifies the smooth part of the objective function to:\n$$ f(x) = \\frac{1}{2}\\|(\\sqrt{L})x - 0\\|^2 = \\frac{1}{2}(\\sqrt{L}x)^2 = \\frac{L}{2}x^2 $$\nThe gradient of $f(x)$ is $\\nabla f(x) = \\frac{d}{dx}\\left(\\frac{L}{2}x^2\\right) = Lx$. The second derivative is $f''(x) = L$, which is the Lipschitz constant of the gradient $\\nabla f(x)$.\n\nThe PGM iteration is defined as $x_{k+1} = \\operatorname{prox}_{\\gamma g}(x_{k} - \\gamma \\nabla f(x_{k}))$. We are asked to consider the special case where $g(x) = 0$.\n\nFirst, we must determine the form of the proximal operator $\\operatorname{prox}_{\\gamma g}$ when $g(x) = 0$. By definition, for a point $z \\in \\mathbb{R}$:\n$$ \\operatorname{prox}_{\\gamma g}(z) = \\arg\\min_{u \\in \\mathbb{R}} \\left\\{ \\gamma g(u) + \\frac{1}{2}\\|u - z\\|^2 \\right\\} $$\nSubstituting $g(x)=0$, the expression to be minimized becomes:\n$$ \\arg\\min_{u \\in \\mathbb{R}} \\left\\{ \\gamma \\cdot 0 + \\frac{1}{2}(u - z)^2 \\right\\} = \\arg\\min_{u \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(u - z)^2 \\right\\} $$\nThe quadratic term $\\frac{1}{2}(u - z)^2$ is uniquely minimized when its argument is zero, which occurs at $u = z$. Therefore, the proximal operator for the zero function is the identity operator:\n$$ \\operatorname{prox}_{\\gamma (0)}(z) = z $$\nSubstituting this result into the PGM iteration formula, we find that for $g(x) = 0$, the method simplifies to the standard Gradient Descent method:\n$$ x_{k+1} = \\operatorname{prox}_{\\gamma (0)}(x_{k} - \\gamma \\nabla f(x_{k})) = x_{k} - \\gamma \\nabla f(x_{k}) $$\nNow, we substitute the specific expression for the gradient, $\\nabla f(x_k) = Lx_k$:\n$$ x_{k+1} = x_k - \\gamma (Lx_k) = (1 - \\gamma L)x_k $$\nThis is the explicit iteration map for $x_{k+1}$ in terms of $x_k$. It is a linear recurrence relation.\n\nTo find a closed-form expression for $x_k$, we can unroll the recurrence starting from the initial condition $x_0$:\n$$ x_1 = (1 - \\gamma L)x_0 $$\n$$ x_2 = (1 - \\gamma L)x_1 = (1 - \\gamma L)(1 - \\gamma L)x_0 = (1 - \\gamma L)^2 x_0 $$\nBy induction, the closed-form expression for $x_k$ is:\n$$ x_k = (1 - \\gamma L)^k x_0 $$\nThe sequence $\\{x_k\\}$ converges to the optimizer of $F(x) = \\frac{L}{2}x^2$, which is $x^*=0$. For the sequence to converge to $0$ for any non-zero initialization $x_0 \\neq 0$, the magnitude of the common ratio of the geometric progression must be strictly less than $1$. This gives us the condition for convergence:\n$$ |1 - \\gamma L|  1 $$\nThis absolute value inequality is equivalent to the two inequalities:\n$$ -1  1 - \\gamma L \\quad \\text{and} \\quad 1 - \\gamma L  1 $$\nFrom the first inequality:\n$$ -1  1 - \\gamma L \\implies \\gamma L  2 \\implies \\gamma  \\frac{2}{L} $$\nFrom the second inequality:\n$$ 1 - \\gamma L  1 \\implies -\\gamma L  0 \\implies \\gamma L  0 $$\nSince $L  0$ is given, this implies $\\gamma  0$, which is also given in the problem statement. Thus, the precise condition for convergence is $0  \\gamma  \\frac{2}{L}$.\n\nNext, we construct a counterexample that exhibits divergence with oscillations. We are instructed to use the step size $\\gamma = \\frac{5}{2L}$. This value does not satisfy the convergence condition, as $\\frac{5}{2L}  \\frac{2}{L}$.\nLet's evaluate the iteration factor $1 - \\gamma L$ for this choice of $\\gamma$:\n$$ 1 - \\gamma L = 1 - \\left(\\frac{5}{2L}\\right)L = 1 - \\frac{5}{2} = -\\frac{3}{2} $$\nThe iteration map becomes:\n$$ x_{k+1} = \\left(-\\frac{3}{2}\\right)x_k $$\nThe closed-form solution is $x_k = \\left(-\\frac{3}{2}\\right)^k x_0$. Because the base is negative, the sign of $x_k$ alternates with each iteration, causing oscillations. Because the magnitude of the base is $|-\\frac{3}{2}| = \\frac{3}{2}  1$, the magnitude of the iterates, $|x_k| = (\\frac{3}{2})^k |x_0|$, grows exponentially to infinity. This demonstrates divergence with oscillations.\n\nThe final task is to compute the spectral radius of the resulting linear iteration operator. The iteration is a linear map of the form $x_{k+1} = Gx_k$, where the operator $G$ is multiplication by the scalar $(1 - \\gamma L)$. For a one-dimensional system, this operator can be represented by the $1 \\times 1$ matrix $[1 - \\gamma L]$. The eigenvalues of a $1 \\times 1$ matrix $[c]$ is simply the scalar $c$. In our case, the single eigenvalue is $\\lambda = 1 - \\gamma L$. The spectral radius, $\\rho(G)$, is the maximum of the absolute values of the eigenvalues.\n$$ \\rho(G) = |\\lambda| = |1 - \\gamma L| $$\nFor the specific choice of $\\gamma = \\frac{5}{2L}$, we have:\n$$ \\rho(G) = \\left|1 - \\left(\\frac{5}{2L}\\right)L\\right| = \\left|1 - \\frac{5}{2}\\right| = \\left|-\\frac{3}{2}\\right| = \\frac{3}{2} $$\nThe spectral radius is $\\frac{3}{2}$.\n\nFinally, a discussion on practical safeguards is required. The convergence of PGM with a fixed step size $\\gamma$ critically depends on the condition $0  \\gamma  2/L$, where $L$ is the Lipschitz constant of $\\nabla f$. In many practical inverse problems and data assimilation applications, the operator $A$ can be a large matrix (representing, for example, a discretization of an integral operator), and computing its largest singular value to find $L = \\|A\\|^2$ can be computationally prohibitive or impossible. Using a step size $\\gamma$ that is too large (i.e., $\\gamma \\ge 2/L$) can lead to the kind of instability and divergence demonstrated in the counterexample.\n\nA scientifically sound and widely used safeguard is to employ an adaptive step-size strategy, most commonly a **backtracking line search**. The principle behind this method is to ensure that each step provides a sufficient decrease in the objective function, without needing to know $L$ a priori.\n\nThe procedure works as follows: At each iteration $k$, one starts with a trial step size, say $\\gamma_k^{(0)}$. Then, one computes a candidate next iterate $x_{k+1}^{(j)} = \\operatorname{prox}_{\\gamma_k^{(j)} g}(x_k - \\gamma_k^{(j)} \\nabla f(x_k))$. A condition is then checked to see if this step is acceptable. For PGM, a standard backtracking condition is based on the quadratic upper bound for $f$ that motivates the iteration. We check if the following inequality holds:\n$$ f(x_{k+1}^{(j)}) \\le f(x_k) + \\langle \\nabla f(x_k), x_{k+1}^{(j)} - x_k \\rangle + \\frac{1}{2\\gamma_k^{(j)}} \\|x_{k+1}^{(j)} - x_k\\|^2 $$\nThis condition essentially verifies that our step size $\\gamma_k^{(j)}$ is small enough such that $1/\\gamma_k^{(j)}$ is an adequate local estimate for the Lipschitz constant. If the condition is met, the step is accepted ($x_{k+1} = x_{k+1}^{(j)}$), and we can even use $\\gamma_k^{(j)}$ as the initial guess for the next iteration. If the condition fails, the step size is reduced (e.g., $\\gamma_k^{(j+1)} = \\beta \\gamma_k^{(j)}$ for some reduction factor $\\beta \\in (0,1)$), and the check is repeated.\n\nFrom first principles, the descent lemma for a function with an $L$-Lipschitz gradient guarantees that $f(y) \\le f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{L}{2}\\|y-x\\|^2$. This implies that the backtracking condition will always be satisfied for any $\\gamma_k \\le 1/L$. The backtracking search is thus guaranteed to terminate in a finite number of reductions (for a given $k$) and find a step size that ensures a decrease in a surrogate function for $F$, leading to a provably convergent algorithm even when $L$ is unknown. This prevents numerical instability and makes the algorithm robust for practical application in complex inverse problems.",
            "answer": "$$ \\boxed{\\frac{3}{2}} $$"
        },
        {
            "introduction": "Many real-world problems in data science and inverse problems require solutions to satisfy complex constraints, such as representing a probability distribution. This exercise tackles the proximal operator for the probability simplex, a crucial building block that moves beyond simple component-wise operations . Deriving and computing this projection provides insight into handling structured constraints, while also prompting a deeper reflection on the computational challenges that arise in very-high-dimensional settings and the alternative algorithmic geometries that can overcome them.",
            "id": "3415733",
            "problem": "Consider a variational data assimilation objective that blends model components through a probability vector $x \\in \\mathbb{R}^{n}$ with $n=10$, where $x$ must lie on the probability simplex $C=\\{x \\in \\mathbb{R}^{n}:\\sum_{i=1}^{n} x_{i}=1,\\ x_{i}\\ge 0\\}$. Suppose the smooth data misfit is differentiable and strongly convex, and a proximal gradient step is taken with step size $t0$ to form an unconstrained intermediate vector $y \\in \\mathbb{R}^{n}$, followed by enforcing the simplex constraint through the proximal of the indicator of $C$. The proximal operator of the indicator of a closed convex set coincides with the Euclidean projection onto that set.\n\nThe fundamental base you should use includes: the definition of the Euclidean projection onto a nonempty closed convex set as the unique minimizer of the squared distance, properties of convex optimization optimality including Karush–Kuhn–Tucker (KKT) conditions, and the definition of proximal gradient methods where the nonsmooth part is handled by a proximal map.\n\nTasks:\n1. Starting from the definition of the Euclidean projection onto $C$ as the solution of the strictly convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|x-y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} x_{i}=1,\\ x_{i}\\ge 0,\n$$\nderive the structure of the projection using first principles. In particular, derive the KKT conditions and show that the unique solution has the shrinkage form $x_{i}^{\\star}=\\max(y_{i}-\\tau,0)$ for some scalar $\\tau \\in \\mathbb{R}$ chosen so that $\\sum_{i=1}^{n} x_{i}^{\\star}=1$. Characterize how the active set determines $\\tau$.\n\n2. Compute the projection explicitly for the given unconstrained proximal gradient iterate\n$$\ny=\\bigl(0.8,\\ -0.3,\\ 0.4,\\ 1.2,\\ 0.0,\\ 2.5,\\ -0.1,\\ 0.6,\\ 3.1,\\ -0.2\\bigr).\n$$\nProvide the projected vector $x^{\\star}=\\operatorname{proj}_{C}(y)$.\n\n3. Explain, using convex analysis and algorithmic complexity considerations, why entropy-type priors (for example, the negative Shannon entropy $\\sum_{i=1}^{n} x_{i}\\ln x_{i}$ on the simplex), or equivalently the use of Kullback–Leibler (KL) divergence geometry in mirror-prox updates, may be preferable when the simplex projection becomes an algorithmic bottleneck in very high dimensions or distributed settings. Base your explanation on properties such as separability, avoidance of global sorting, and numerical stability, and do not rely on any shortcut formulas.\n\nYour final answer must be the projected vector $x^{\\star}$ written as a single row matrix. If any numerical rounding is needed, round to eight significant figures; otherwise, give exact decimal values.",
            "solution": "The probability simplex in $\\mathbb{R}^{n}$ is the closed convex set $C = \\{x \\in \\mathbb{R}^{n} \\mid \\sum_{i=1}^{n} x_i = 1, x_i \\ge 0 \\ \\forall i\\}$. The Euclidean projection of a vector $y \\in \\mathbb{R}^{n}$ onto $C$, denoted $\\operatorname{proj}_{C}(y)$, is the unique solution $x^{\\star}$ to a constrained quadratic programming problem.\n\n### Task 1: Derivation of the Projection Structure\n\nThe first task is to derive the structure of the projection of $y$ onto $C$ by solving the optimization problem:\n$$\nx^{\\star} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|x-y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} x_{i}=1,\\ x_{i}\\ge 0.\n$$\nThe objective function $f(x) = \\frac{1}{2}\\|x-y\\|_{2}^{2} = \\frac{1}{2}\\sum_{i=1}^{n} (x_i - y_i)^2$ is strictly convex. The constraints define a nonempty closed convex set. Therefore, a unique minimum exists. We use the Karush–Kuhn–Tucker (KKT) conditions to characterize this solution.\n\nThe Lagrangian for this problem is:\n$$\nL(x, \\mu, \\lambda) = \\frac{1}{2}\\sum_{i=1}^{n} (x_i - y_i)^2 + \\mu\\left(\\sum_{i=1}^{n} x_i - 1\\right) - \\sum_{i=1}^{n} \\lambda_i x_i\n$$\nwhere $\\mu \\in \\mathbb{R}$ is the Lagrange multiplier for the equality constraint $\\sum_{i=1}^{n} x_i = 1$, and $\\lambda_i \\ge 0$ are the Lagrange multipliers for the inequality constraints $x_i \\ge 0$.\n\nThe KKT conditions for an optimal solution $x^{\\star}$ with corresponding multipliers $\\mu^{\\star}$ and $\\lambda^{\\star} = (\\lambda_1^{\\star}, \\dots, \\lambda_n^{\\star})$ are:\n1.  **Stationarity:** For each $i=1, \\dots, n$, the partial derivative of the Lagrangian with respect to $x_i$ must be zero at $x^{\\star}$:\n    $$\n    \\frac{\\partial L}{\\partial x_i} \\bigg|_{x=x^{\\star}} = (x_i^{\\star} - y_i) + \\mu^{\\star} - \\lambda_i^{\\star} = 0 \\implies x_i^{\\star} = y_i - \\mu^{\\star} + \\lambda_i^{\\star}.\n    $$\n2.  **Primal Feasibility:** The solution $x^{\\star}$ must satisfy the constraints:\n    $$\n    \\sum_{i=1}^{n} x_i^{\\star} = 1 \\quad \\text{and} \\quad x_i^{\\star} \\ge 0 \\ \\forall i.\n    $$\n3.  **Dual Feasibility:** The multipliers for the inequality constraints must be non-negative:\n    $$\n    \\lambda_i^{\\star} \\ge 0 \\ \\forall i.\n    $$\n4.  **Complementary Slackness:** The product of each inequality multiplier and its corresponding constraint slack must be zero:\n    $$\n    \\lambda_i^{\\star} x_i^{\\star} = 0 \\ \\forall i.\n    $$\n\nFrom the complementary slackness condition, for each component $i$, we must have either $\\lambda_i^{\\star}=0$ or $x_i^{\\star}=0$. Let's analyze these two cases:\n\nCase A: $x_i^{\\star}  0$. By complementary slackness, we must have $\\lambda_i^{\\star} = 0$. Substituting this into the stationarity equation gives $x_i^{\\star} = y_i - \\mu^{\\star}$. Since $x_i^{\\star}  0$, this implies $y_i - \\mu^{\\star}  0$, or $y_i  \\mu^{\\star}$.\n\nCase B: $x_i^{\\star} = 0$. In this case, the complementary slackness condition $\\lambda_i^{\\star} x_i^{\\star} = 0$ is satisfied for any $\\lambda_i^{\\star} \\ge 0$. The stationarity equation gives $0 = y_i - \\mu^{\\star} + \\lambda_i^{\\star}$, which means $\\lambda_i^{\\star} = \\mu^{\\star} - y_i$. Since we must have $\\lambda_i^{\\star} \\ge 0$ (dual feasibility), this implies $\\mu^{\\star} - y_i \\ge 0$, or $y_i \\le \\mu^{\\star}$.\n\nCombining these two cases, we can express the optimal solution $x_i^{\\star}$ solely in terms of $y_i$ and the single scalar multiplier $\\mu^{\\star}$:\n- If $y_i  \\mu^{\\star}$, then $x_i^{\\star} = y_i - \\mu^{\\star}$.\n- If $y_i \\le \\mu^{\\star}$, then $x_i^{\\star} = 0$.\n\nThis can be written compactly using the positive part function, $\\max(\\cdot, 0)$. Let's define $\\tau = \\mu^{\\star}$. The solution has the form:\n$$\nx_i^{\\star} = \\max(y_i - \\tau, 0).\n$$\nThis is the shrinkage form required by the problem statement. The scalar $\\tau$ is a threshold parameter determined by the primal feasibility condition $\\sum_{i=1}^{n} x_i^{\\star} = 1$. Substituting the structure of $x_i^{\\star}$ into this sum gives the equation that defines $\\tau$:\n$$\n\\sum_{i=1}^{n} \\max(y_i - \\tau, 0) = 1.\n$$\nThe function $f(\\tau) = \\sum_{i=1}^{n} \\max(y_i - \\tau, 0)$ is continuous, piecewise linear, and monotonically non-increasing. A unique solution for $\\tau$ exists and can be found efficiently.\n\nThe active set is the set of indices $I_+ = \\{i \\mid x_i^{\\star}  0\\}$. From our derivation, this corresponds to $I_+ = \\{i \\mid y_i  \\tau\\}$. For indices in this set, $x_i^{\\star} = y_i - \\tau$. The sum constraint then becomes:\n$$\n\\sum_{i \\in I_+} (y_i - \\tau) = 1 \\implies \\sum_{i \\in I_+} y_i - |I_+| \\tau = 1.\n$$\nThis allows us to solve for $\\tau$ once the active set $I_+$ is known:\n$$\n\\tau = \\frac{\\left(\\sum_{i \\in I_+} y_i\\right) - 1}{|I_+|}.\n$$\nFinding the active set $I_+$ and $\\tau$ simultaneously can be done via a sorting-based algorithm.\n\n### Task 2: Explicit Computation\n\nWe are given the vector $y \\in \\mathbb{R}^{10}$:\n$$\ny=\\bigl(0.8,\\ -0.3,\\ 0.4,\\ 1.2,\\ 0.0,\\ 2.5,\\ -0.1,\\ 0.6,\\ 3.1,\\ -0.2\\bigr).\n$$\nWe must find the value of $\\tau$ that satisfies $\\sum_{i=1}^{10} \\max(y_i - \\tau, 0) = 1$. We use an efficient algorithm that avoids solving the piecewise linear equation directly.\n1. Sort the components of $y$ in descending order. Let's call the sorted vector $u$:\n$$\nu = \\bigl(3.1,\\ 2.5,\\ 1.2,\\ 0.8,\\ 0.6,\\ 0.4,\\ 0.0,\\ -0.1,\\ -0.2,\\ -0.3\\bigr).\n$$\n2. Find the largest integer $\\rho \\in \\{1, \\dots, 10\\}$ such that $u_{\\rho}  \\frac{1}{\\rho}\\left(\\sum_{j=1}^{\\rho} u_j - 1\\right)$. This $\\rho$ will be the number of non-zero elements in the projected vector $x^{\\star}$. Let's compute this iteratively. Let $S_k = \\sum_{j=1}^{k} u_j$. We test for $k=1, 2, \\dots$:\n   - $k=1$: $u_1 = 3.1$. $S_1 = 3.1$. Test: $3.1  \\frac{1}{1}(3.1 - 1) = 2.1$. True.\n   - $k=2$: $u_2 = 2.5$. $S_2 = 3.1 + 2.5 = 5.6$. Test: $2.5  \\frac{1}{2}(5.6 - 1) = 2.3$. True.\n   - $k=3$: $u_3 = 1.2$. $S_3 = 5.6 + 1.2 = 6.8$. Test: $1.2  \\frac{1}{3}(6.8 - 1) = \\frac{5.8}{3} \\approx 1.933$. False.\nThe condition is false for $k=3$ and all subsequent values of $k$. The largest integer for which the condition is true is $\\rho=2$.\n3. The threshold $\\tau$ is then calculated using this value of $\\rho$:\n$$\n\\tau = \\frac{1}{\\rho}\\left(\\sum_{j=1}^{\\rho} u_j - 1\\right) = \\frac{1}{2}(S_2 - 1) = \\frac{1}{2}(5.6 - 1) = 2.3.\n$$\n4. Now we compute the projected vector $x^{\\star}$ using $x_i^{\\star} = \\max(y_i - \\tau, 0)$ with $\\tau = 2.3$:\n   - $x_1^{\\star} = \\max(0.8 - 2.3, 0) = 0$.\n   - $x_2^{\\star} = \\max(-0.3 - 2.3, 0) = 0$.\n   - $x_3^{\\star} = \\max(0.4 - 2.3, 0) = 0$.\n   - $x_4^{\\star} = \\max(1.2 - 2.3, 0) = 0$.\n   - $x_5^{\\star} = \\max(0.0 - 2.3, 0) = 0$.\n   - $x_6^{\\star} = \\max(2.5 - 2.3, 0) = 0.2$.\n   - $x_7^{\\star} = \\max(-0.1 - 2.3, 0) = 0$.\n   - $x_8^{\\star} = \\max(0.6 - 2.3, 0) = 0$.\n   - $x_9^{\\star} = \\max(3.1 - 2.3, 0) = 0.8$.\n   - $x_{10}^{\\star} = \\max(-0.2 - 2.3, 0) = 0$.\n\nThe resulting projected vector is $x^{\\star} = \\bigl(0, 0, 0, 0, 0, 0.2, 0, 0, 0.8, 0\\bigr)$. We verify that $\\sum_i x_i^{\\star} = 0.2 + 0.8 = 1$ and $x_i^{\\star} \\ge 0$, so the solution is valid.\n\n### Task 3: Comparison with Entropy-Based Methods\n\nThe third task is to explain why entropy-based priors or Kullback-Leibler (KL) divergence geometry might be preferable to Euclidean projection in very high dimensions.\n\nThe core of the issue lies in the algorithmic complexity and scalability of the projection operation.\n\n1.  **Complexity of Euclidean Projection:** As demonstrated in Task 2, the standard and efficient algorithms for Euclidean projection onto the probability simplex have a computational complexity dominated by sorting the input vector $y$. Sorting an $n$-dimensional vector takes $O(n \\log n)$ time. For problems where the dimension $n$ is very large (e.g., millions or more), this $O(n \\log n)$ cost can become a significant bottleneck for each iteration of a proximal gradient method.\n\n2.  **Complexity in Distributed Settings:** In a distributed computing environment, where the vector $y$ is partitioned across multiple processors, sorting is a non-local operation. It requires substantial inter-processor communication to compare and exchange elements, making it expensive and difficult to scale efficiently. The global coordination needed for a sort acts as a major barrier to performance.\n\n3.  **Entropy-Based Alternatives (Mirror Descent):** An alternative is to change the geometry of the space from Euclidean to one induced by a different distance-like function, a Bregman divergence. Using the negative Shannon entropy $\\Psi(x) = \\sum_{i=1}^n x_i \\ln x_i$ as the \"potential\" function for the simplex gives rise to the Kullback-Leibler (KL) divergence. Proximal methods in this geometry are known as mirror descent or entropic mirror descent.\n\n   The key update step in this framework, which replaces the Euclidean projection, has a remarkably simple and efficient closed-form solution. For example, to project a log-probability vector $z$ onto the simplex, the operation is $x_i = \\exp(z_i) / \\sum_{j=1}^n \\exp(z_j)$. This is a softmax operation. More generally, the iterative update in a mirror descent step for minimizing a function $f(x)$ over the simplex is of the form:\n   $$\n   x_{i}^{(k+1)} = \\frac{x_i^{(k)} \\exp\\left(-t \\nabla_i f(x^{(k)})\\right)}{\\sum_{j=1}^n x_j^{(k)} \\exp\\left(-t \\nabla_j f(x^{(k)})\\right)},\n   $$\n   where $t$ is the step size.\n\n4.  **Algorithmic Advantages:**\n    - **Separability and Complexity:** The main computation in the entropy-based update, $v_i = x_i^{(k)} \\exp\\left(-t \\nabla_i f(x^{(k)})\\right)$, is perfectly **separable**. Each component $v_i$ can be calculated independently of all others. This is followed by a single reduction operation (a sum, $\\sum_j v_j$) and a final component-wise division. The total complexity of this update is $O(n)$, involving $n$ exponentiations, $n$ multiplications, one sum over $n$ elements, and $n$ divisions. This $O(n)$ complexity is asymptotically superior to the $O(n \\log n)$ of Euclidean projection and makes a profound difference in very high dimensions.\n    - **Scalability in Distributed Settings:** The separable nature of the computation is ideal for distributed systems. Each node can compute its local components of $v$ without any communication. The only communication needed is a single, highly optimized all-reduce operation to compute the sum $Z = \\sum_j v_j$, followed by a broadcast of the scalar $Z$. This communication pattern is far more efficient and scalable than that required for a global sort.\n    - **Numerical Stability:** While the use of exponentials can introduce risks of numerical overflow or underflow, these are well-understood and can be effectively managed using standard techniques like the log-sum-exp trick, ensuring numerical stability without changing the low-complexity nature of the algorithm.\n\nIn summary, the transition from Euclidean geometry (leading to $O(n \\log n)$ sorting-based projections) to KL-divergence geometry (leading to $O(n)$ separable updates) is a powerful strategy to overcome the algorithmic bottleneck of simplex projection in very-high-dimensional or distributed optimization settings, trading a global, communication-heavy sort for a local, communication-light computation.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0  0  0  0  0.2  0  0  0.8  0 \\end{pmatrix}}\n$$"
        }
    ]
}