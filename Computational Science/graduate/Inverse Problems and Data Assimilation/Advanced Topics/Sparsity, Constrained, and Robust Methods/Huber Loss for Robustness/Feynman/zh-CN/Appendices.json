{
    "hands_on_practices": [
        {
            "introduction": "这第一个练习提供了一个关于 $L_2$、$L_1$ 和 Huber 估计量的具体、动手实践的比较。通过解决一个带有明显异常值的简单标量问题，您将直接观察到 Huber 损失函数如何在敏感度与稳健性之间取得平衡，从而在最小二乘法受异常值影响和最小绝对偏差法完全拒绝异常值之间提供一种折衷方案 。这为理解 Huber 损失为何是稳健估计中的一个宝贵工具建立了基础直觉。",
            "id": "3389451",
            "problem": "在数据同化（DA）的背景下，考虑一个标量逆问题，其中未知状态是单个实数参数 $x \\in \\mathbb{R}$。通过恒等观测算子可获得两个独立的点观测，因此每个观测都直接测量 $x$。一个观测是干净的，另一个被一个大误差污染。在数值上，观测值为 $y_{1} = 1.2$（干净）和 $y_{2} = 9.0$（被污染）。为了反映不同的信任度，为 $y_{1}$ 分配观测权重 $w_{1} = 2$，为 $y_{2}$ 分配观测权重 $w_{2} = 0.5$。设 Huber 损失阈值为 $\\delta = 1$。\n\n将两个观测的残差定义为 $r_{1}(x) = x - y_{1}$ 和 $r_{2}(x) = x - y_{2}$。考虑与三种数据失配模型相对应的三个估计量：\n- $L_{2}$（最小二乘）估计量，最小化加权二次失配 $\\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$。\n- $L_{1}$（最小绝对偏差）估计量，最小化加权绝对失配 $\\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$。\n- Huber 估计量，最小化加权 Huber 损失 $\\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$，阈值为 $\\delta$，其中 $\\rho_{\\delta}$ 是 Huber 损失。\n\n从这些损失函数的核心定义和凸最小化的一阶最优性条件出发，推导这个双观测标量问题的每个估计量。对给定的数值，明确地计算这三个估计值，并对它们进行数值比较。只报告 Huber 估计值 $\\hat{x}_{\\mathrm{Huber}}$ 的数值。无需四舍五入。未知状态 $x$ 是无量纲的，因此最终答案应表示为不带单位的纯数。",
            "solution": "在尝试求解之前，对问题陈述的有效性进行严格评估。\n\n### 步骤 1：提取已知条件\n- 未知状态：单个实数参数 $x \\in \\mathbb{R}$。\n- 观测值：$y_{1} = 1.2$（干净），$y_{2} = 9.0$（被污染）。\n- 观测算子：恒等算子。\n- 观测权重：$w_{1} = 2$，$w_{2} = 0.5$。\n- Huber 损失阈值：$\\delta = 1$。\n- 残差：$r_{1}(x) = x - y_{1}$，$r_{2}(x) = x - y_{2}$。\n- $L_{2}$ 估计量最小化：$J_2(x) = \\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$。\n- $L_{1}$ 估计量最小化：$J_1(x) = \\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$。\n- Huber 估计量最小化：$J_H(x) = \\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$，其中 $\\rho_{\\delta}$ 是 Huber 损失函数。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据预定义标准对问题进行评估：\n- **科学依据：** 该问题是统计估计和优化中的一个标准练习，特别是在应用于逆问题和数据同化的稳健统计背景下。$L_1$、$L_2$ 和 Huber 损失的概念在这些领域是基础性的。该设置在科学上和数学上都是合理的。\n- **适定性：** 对于标量变量 $x$，所定义的成本函数（$L_1$、$L_2$、Huber）都是凸函数。在 $\\mathbb{R}$ 上的凸函数具有非空的全局最小值点集合，并且对于这些严格凸（或分段严格凸）的函数，最小值点是唯一的。该问题是适定的。\n- **客观性：** 问题陈述使用了精确的数学定义和数值。没有主观或模糊的术语。\n- **完整性和一致性：** 提供了所有必要的数据（$y_1, y_2, w_1, w_2, \\delta$）和定义。设置是自洽的，没有矛盾。\n- **现实性和可行性：** 拥有干净数据和污染数据混合的场景是数据分析中的一个典型问题，这使得该问题具有现实性。所选的数值旨在清晰地说明不同估计量的行为差异。\n\n### 步骤 3：结论和行动\n问题是**有效的**。这是一个适定的、有科学依据的问题，可以使用已建立的数学原理来解决。我现在将继续推导解决方案。\n\n目标是找到最小化各自成本函数的估计值 $\\hat{x}_{L_2}$、$\\hat{x}_{L_1}$ 和 $\\hat{x}_{\\text{Huber}}$，然后报告 $\\hat{x}_{\\text{Huber}}$ 的数值。\n\n**1. $L_2$（最小二乘）估计量**\n$L_2$ 成本函数由下式给出：\n$$J_2(x) = w_1 (x-y_1)^2 + w_2 (x-y_2)^2$$\n这是一个关于 $x$ 的二次函数，其最小值可以通过将其关于 $x$ 的一阶导数设为零来找到。\n$$\\frac{dJ_2(x)}{dx} = 2w_1(x-y_1) + 2w_2(x-y_2) = 0$$\n求解 $x$：\n$$w_1 x - w_1 y_1 + w_2 x - w_2 y_2 = 0$$\n$$(w_1 + w_2)x = w_1 y_1 + w_2 y_2$$\n$L_2$ 估计量 $\\hat{x}_{L_2}$ 是观测值的加权平均：\n$$\\hat{x}_{L_2} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2}$$\n代入给定的数值：$y_1=1.2$，$y_2=9.0$，$w_1=2$ 和 $w_2=0.5$。\n$$\\hat{x}_{L_2} = \\frac{2(1.2) + 0.5(9.0)}{2 + 0.5} = \\frac{2.4 + 4.5}{2.5} = \\frac{6.9}{2.5} = 2.76$$\n\n**2. $L_1$（最小绝对偏差）估计量**\n$L_1$ 成本函数为：\n$$J_1(x) = w_1|x-y_1| + w_2|x-y_2|$$\n该函数是凸函数，但在 $x=y_1$ 和 $x=y_2$ 处不可微。最小值点是观测值的加权中位数。一阶最优性条件是 $0 \\in \\partial J_1(x)$，其中 $\\partial J_1(x)$ 是次梯度。$J_1(x)$ 在其存在处的导数是：\n$$\\frac{dJ_1(x)}{dx} = w_1 \\text{sgn}(x-y_1) + w_2 \\text{sgn}(x-y_2)$$\n让我们根据 $x$ 相对于 $y_1=1.2$ 和 $y_2=9.0$ 的值来分析导数：\n- 对于 $x < 1.2$：$\\frac{dJ_1}{dx} = -w_1 - w_2 = -2 - 0.5 = -2.5 < 0$。函数是递减的。\n- 对于 $1.2 < x < 9.0$：$\\frac{dJ_1}{dx} = w_1 - w_2 = 2 - 0.5 = 1.5 > 0$。函数是递增的。\n- 对于 $x > 9.0$：$\\frac{dJ_1}{dx} = w_1 + w_2 = 2 + 0.5 = 2.5 > 0$。函数是递增的。\n导数在 $x = y_1 = 1.2$ 处从负变正。因此，$J_1(x)$ 的最小值出现在这一点。\n$$\\hat{x}_{L_1} = y_1 = 1.2$$\n\n**3. Huber 估计量**\nHuber 成本函数由 $J_H(x) = w_1 \\rho_{\\delta}(x-y_1) + w_2 \\rho_{\\delta}(x-y_2)$ 给出，其中 Huber 损失函数 $\\rho_{\\delta}(r)$ 定义为：\n$$\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2}r^2 & \\text{if } |r| \\le \\delta \\\\ \\delta|r| - \\frac{1}{2}\\delta^2 & \\text{if } |r| > \\delta \\end{cases}$$\n这个函数是凸的且连续可微。其导数是 Huber 分数函数 $\\psi_{\\delta}(r) = \\rho'_{\\delta}(r)$：\n$$\\psi_{\\delta}(r) = \\begin{cases} r & \\text{if } |r| \\le \\delta \\\\ \\delta \\cdot \\text{sgn}(r) & \\text{if } |r| > \\delta \\end{cases} $$\n$J_H(x)$ 的最小值通过将其导数设为零来找到：\n$$\\frac{dJ_H(x)}{dx} = w_1 \\psi_{\\delta}(x-y_1) + w_2 \\psi_{\\delta}(x-y_2) = 0$$\n使用给定值 $w_1=2$，$w_2=0.5$，$y_1=1.2$，$y_2=9.0$ 和 $\\delta=1$，方程变为：\n$$2 \\cdot \\psi_{1}(x-1.2) + 0.5 \\cdot \\psi_{1}(x-9.0) = 0$$\n根据其他估计值和权重，我们期望解 $\\hat{x}_{\\text{Huber}}$ 接近 $y_1=1.2$ 但不完全相同。这表明第一个观测的残差 $r_1 = x-y_1$ 很小，而第二个观测的残差 $r_2 = x-y_2$ 很大。我们假设 $|x-y_1| \\le \\delta$ 和 $|x-y_2| > \\delta$。\n- 如果 $|r_1| = |x-1.2| \\le 1$，那么 $\\psi_{1}(x-1.2) = x-1.2$。\n- 如果 $|r_2| = |x-9.0| > 1$，那么 $\\psi_{1}(x-9.0) = 1 \\cdot \\text{sgn}(x-9.0)$。由于我们期望 $x$ 接近 $1.2$，所以 $x-9.0$ 将为负，因此 $\\text{sgn}(x-9.0)=-1$。\n将这些代入最优性条件：\n$$2(x-1.2) + 0.5(-1) = 0$$\n$$2x - 2.4 - 0.5 = 0$$\n$$2x = 2.9$$\n$$x = 1.45$$\n我们现在必须验证这个解是否与我们的初始假设一致。\n- 对于 $x=1.45$，第一个残差是 $r_1 = 1.45 - 1.2 = 0.25$。我们检查是否 $|r_1| \\le \\delta$：$|0.25| \\le 1$。这是成立的。\n- 第二个残差是 $r_2 = 1.45 - 9.0 = -7.55$。我们检查是否 $|r_2| > \\delta$：$|-7.55| > 1$。这也是成立的。\n假设成立，因此 Huber 估计值为：\n$$\\hat{x}_{\\text{Huber}} = 1.45$$\n\n**估计值比较**\n- $\\hat{x}_{L_2} = 2.76$：该估计值受到离群值 $y_2=9.0$ 的强烈影响。对大残差 $r_2$ 的二次惩罚将估计值显著地拉离干净观测值 $y_1=1.2$，显示了最小二乘法的非稳健性。\n- $\\hat{x}_{L_1} = 1.2$：该估计值，即加权中位数，完全抛弃了离群值信息，并迅速贴近干净的观测值，因为其权重更大。这显示了极强的稳健性。\n- $\\hat{x}_{\\text{Huber}} = 1.45$：该估计值代表一种折衷。它接近干净的观测值 $y_1=1.2$，显示了对离群值 $y_2=9.0$ 的稳健性。然而，与 $L_1$ 估计不同，它仍然受到离群值的轻微影响，因为 Huber 损失的线性惩罚部分仍然允许离群值有一个小的、恒定的梯度贡献。它位于 $L_1$ 和 $L_2$ 估计值之间。\n\n问题只要求 Huber 估计值的数值。",
            "answer": "$$\n\\boxed{1.45}\n$$"
        },
        {
            "introduction": "从基本概念转向实用算法，本练习深入探讨了用于 Huber M-估计的迭代重加权最小二乘法 (IRLS)。您将为一个线性回归问题分析性地执行单次迭代，计算由 Huber 得分函数产生的权重 。这种实践揭示了 IRLS 如何通过迭代降低异常值的权重，将一个非二次的稳健优化问题转化为一系列可解的加权最小二乘问题，从而使其不再神秘。",
            "id": "3389420",
            "problem": "考虑一个线性逆问题，其中状态向量 $\\theta \\in \\mathbb{R}^{2}$ 参数化了一个线性观测算子 $g(x;\\theta) = \\theta_{1} x + \\theta_{2}$。给定一组 $4$ 个标量观测值 $\\{(x_{i}, d_{i})\\}_{i=1}^{4}$，其设计点和数据为 $(x_{1}, d_{1}) = (0, 1)$，$(x_{2}, d_{2}) = (1, 3)$，$(x_{3}, d_{3}) = (2, 5)$，以及 $(x_{4}, d_{4}) = (3, 40)$。第四个数据点被一个严重误差污染，产生了一个离群值。将使用 Huber 损失进行稳健估计。带有阈值 $\\delta > 0$ 的 Huber 损失 $\\rho_{\\delta}: \\mathbb{R} \\to \\mathbb{R}$ 定义为\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2}, & |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2}, & |r| > \\delta,\n\\end{cases}\n$$\n其中 $r$ 是残差。初始参数为 $\\theta^{(0)} = (\\theta_{1}^{(0)}, \\theta_{2}^{(0)}) = (1.5, 0.5)$，阈值为 $\\delta = 2$。\n\n从稳健 M 估计和 Huber 损失的核心定义出发，推导针对残差 $r_{i}^{(0)} = d_{i} - (\\theta_{1}^{(0)} x_{i} + \\theta_{2}^{(0)})$ 的迭代重加权最小二乘（IRLS; Iteratively Reweighted Least Squares）算法的权重，为该双参数线性模型构建相应的加权正规方程，并精确求解它们以获得下一次迭代的参数 $\\theta^{(1)}$。最后，计算参数更新量 $\\Delta \\theta = \\theta^{(1)} - \\theta^{(0)}$，结果表示为精确有理数。使用 $\\texttt{pmatrix}$ 环境将最终答案表示为一个双分量行向量。无需四舍五入。",
            "solution": "该问题要求使用带有 Huber 损失函数的 M 估计的迭代重加权最小二乘（IRLS）算法，来求解一个线性模型的首次参数更新 $\\Delta \\theta$。\n\n线性模型由 $g(x;\\theta) = \\theta_{1} x + \\theta_{2}$ 给出，其中 $\\theta = (\\theta_{1}, \\theta_{2})^T \\in \\mathbb{R}^{2}$。\n目标是最小化所有残差的 Huber 损失之和：\n$$\nJ(\\theta) = \\sum_{i=1}^{4} \\rho_{\\delta}(r_i) = \\sum_{i=1}^{4} \\rho_{\\delta}(d_i - g(x_i; \\theta))\n$$\nHuber 损失函数 $\\rho_{\\delta}(r)$ 定义为：\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2}, & |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2}, & |r| > \\delta.\n\\end{cases}\n$$\n为了最小化 $J(\\theta)$，我们将其关于 $\\theta$ 的梯度设为零。$\\rho_{\\delta}(r)$ 关于 $r$ 的导数是影响函数 $\\psi_{\\delta}(r)$：\n$$\n\\psi_{\\delta}(r) = \\frac{d\\rho_{\\delta}}{dr}(r) = \n\\begin{cases}\nr, & |r| \\leq \\delta, \\\\\n\\delta \\, \\text{sgn}(r), & |r| > \\delta.\n\\end{cases}\n$$\n代价函数的梯度是：\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{i=1}^{4} \\frac{\\partial \\rho_{\\delta}(r_i)}{\\partial r_i} \\frac{\\partial r_i}{\\partial \\theta} = \\sum_{i=1}^{4} \\psi_{\\delta}(r_i) (-\\nabla_{\\theta} g(x_i; \\theta)) = 0.\n$$\n由于 $g(x_i; \\theta) = \\theta_1 x_i + \\theta_2$，我们有 $\\nabla_{\\theta} g(x_i; \\theta) = (x_i, 1)^T$。令其为设计矩阵 $G$ 的第 $i$ 行的转置。\n方程组变为：\n$$\n\\sum_{i=1}^{4} \\psi_{\\delta}(d_i - g(x_i; \\theta)) \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nIRLS 算法通过定义权重函数 $w(r) = \\psi_{\\delta}(r)/r$ 来线性化这个系统。\n$$\nw(r) = \n\\begin{cases}\n1, & |r| \\leq \\delta, \\\\\n\\frac{\\delta}{|r|}, & |r| > \\delta.\n\\end{cases}\n$$\n使用该函数，方程组可以写为：\n$$\n\\sum_{i=1}^{4} w(r_i) r_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\sum_{i=1}^{4} w(r_i) (d_i - (\\theta_1 x_i + \\theta_2)) \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n在每次迭代 $k$ 中，我们根据前一次的残差 $r_i^{(k-1)} = d_i - g(x_i; \\theta^{(k-1)})$ 计算权重 $w_i^{(k)} = w(r_i^{(k-1)})$，然后为 $\\theta^{(k)}$ 求解以下加权最小二乘问题：\n$$\n\\theta^{(k)} = \\arg\\min_{\\theta} \\sum_{i=1}^{4} w_i^{(k)} (d_i - g(x_i; \\theta))^2.\n$$\n这导出了加权正规方程：\n$$\n(G^T W^{(k)} G) \\theta^{(k)} = G^T W^{(k)} d,\n$$\n其中 $G$ 是设计矩阵，$d$ 是观测向量，$W^{(k)}$ 是由权重 $w_i^{(k)}$ 构成的对角矩阵。\n\n首先，我们使用初始参数 $\\theta^{(0)} = (1.5, 0.5) = (3/2, 1/2)$ 和模型 $g(x_i; \\theta^{(0)}) = \\frac{3}{2}x_i + \\frac{1}{2}$ 计算初始残差 $r_i^{(0)}$。数据点为 $(x_1, d_1) = (0, 1)$，$(x_2, d_2) = (1, 3)$，$(x_3, d_3) = (2, 5)$ 和 $(x_4, d_4) = (3, 40)$。\n$r_1^{(0)} = 1 - (\\frac{3}{2}(0) + \\frac{1}{2}) = 1 - \\frac{1}{2} = \\frac{1}{2} = 0.5$。\n$r_2^{(0)} = 3 - (\\frac{3}{2}(1) + \\frac{1}{2}) = 3 - 2 = 1$。\n$r_3^{(0)} = 5 - (\\frac{3}{2}(2) + \\frac{1}{2}) = 5 - \\frac{7}{2} = \\frac{3}{2} = 1.5$。\n$r_4^{(0)} = 40 - (\\frac{3}{2}(3) + \\frac{1}{2}) = 40 - 5 = 35$。\n\n接下来，我们使用阈值 $\\delta = 2$ 计算第一次迭代的权重 $w_i^{(1)}$。\n$|r_1^{(0)}| = 0.5 \\leq 2 \\implies w_1^{(1)} = 1$。\n$|r_2^{(0)}| = 1 \\leq 2 \\implies w_2^{(1)} = 1$。\n$|r_3^{(0)}| = 1.5 \\leq 2 \\implies w_3^{(1)} = 1$。\n$|r_4^{(0)}| = 35 > 2 \\implies w_4^{(1)} = \\frac{\\delta}{|r_4^{(0)}|} = \\frac{2}{35}$。\n\n现在，我们建立 $\\theta^{(1)}$ 的加权正规方程。设计矩阵 $G$、数据向量 $d$ 和权重矩阵 $W^{(1)}$ 分别为：\n$$\nG = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{pmatrix}, \\quad d = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 40 \\end{pmatrix}, \\quad W^{(1)} = \\text{diag}\\left(1, 1, 1, \\frac{2}{35}\\right).\n$$\n我们需要计算 $G^T W^{(1)} G$ 和 $G^T W^{(1)} d$。\n$$\nG^T W^{(1)} = \\begin{pmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & \\frac{2}{35} \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 2 & \\frac{6}{35} \\\\ 1 & 1 & 1 & \\frac{2}{35} \\end{pmatrix}.\n$$\n然后，\n$$\nG^T W^{(1)} G = \\begin{pmatrix} 0 & 1 & 2 & \\frac{6}{35} \\\\ 1 & 1 & 1 & \\frac{2}{35} \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+4+\\frac{18}{35} & 1+2+\\frac{6}{35} \\\\ 1+2+\\frac{6}{35} & 1+1+1+\\frac{2}{35} \\end{pmatrix}\n\\\\ = \\begin{pmatrix} 5+\\frac{18}{35} & 3+\\frac{6}{35} \\\\ 3+\\frac{6}{35} & 3+\\frac{2}{35} \\end{pmatrix} = \\begin{pmatrix} \\frac{193}{35} & \\frac{111}{35} \\\\ \\frac{111}{35} & \\frac{107}{35} \\end{pmatrix} = \\frac{1}{35}\\begin{pmatrix} 193 & 111 \\\\ 111 & 107 \\end{pmatrix}.\n$$\n并且，\n$$\nG^T W^{(1)} d = \\begin{pmatrix} 0 & 1 & 2 & \\frac{6}{35} \\\\ 1 & 1 & 1 & \\frac{2}{35} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 40 \\end{pmatrix} = \\begin{pmatrix} 3+10+\\frac{240}{35} \\\\ 1+3+5+\\frac{80}{35} \\end{pmatrix} = \\begin{pmatrix} 13+\\frac{48}{7} \\\\ 9+\\frac{16}{7} \\end{pmatrix}\n\\\\ = \\begin{pmatrix} \\frac{91+48}{7} \\\\ \\frac{63+16}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{139}{7} \\\\ \\frac{79}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{695}{35} \\\\ \\frac{395}{35} \\end{pmatrix} = \\frac{1}{35}\\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}.\n$$\n正规方程为 $\\frac{1}{35}\\begin{pmatrix} 193 & 111 \\\\ 111 & 107 \\end{pmatrix} \\theta^{(1)} = \\frac{1}{35}\\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}$，可以简化为：\n$$\n\\begin{pmatrix} 193 & 111 \\\\ 111 & 107 \\end{pmatrix} \\begin{pmatrix} \\theta_1^{(1)} \\\\ \\theta_2^{(1)} \\end{pmatrix} = \\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}.\n$$\n我们求解这个 $2 \\times 2$ 系统以得到 $\\theta^{(1)}$。设该矩阵为 $A$。$A$ 的行列式为：\n$$\n\\det(A) = (193)(107) - (111)^2 = 20651 - 12321 = 8330.\n$$\n$A$ 的逆矩阵为：\n$$\nA^{-1} = \\frac{1}{8330} \\begin{pmatrix} 107 & -111 \\\\ -111 & 193 \\end{pmatrix}.\n$$\n现在我们计算 $\\theta^{(1)} = A^{-1} \\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}$：\n$$\n\\begin{pmatrix} \\theta_1^{(1)} \\\\ \\theta_2^{(1)} \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} (107)(695) - (111)(395) \\\\ -(111)(695) + (193)(395) \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} 74365 - 43845 \\\\ -77145 + 76235 \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} 30520 \\\\ -910 \\end{pmatrix}.\n$$\n我们简化 $\\theta^{(1)}$ 的分量：\n$$\n\\theta_1^{(1)} = \\frac{30520}{8330} = \\frac{3052}{833} = \\frac{436 \\times 7}{119 \\times 7} = \\frac{436}{119}.\n$$\n$$\n\\theta_2^{(1)} = \\frac{-910}{8330} = \\frac{-91}{833} = \\frac{-13 \\times 7}{119 \\times 7} = -\\frac{13}{119}.\n$$\n所以，$\\theta^{(1)} = (\\frac{436}{119}, -\\frac{13}{119})^T$。\n\n最后，我们计算参数更新量 $\\Delta \\theta = \\theta^{(1)} - \\theta^{(0)}$：\n$$\n\\theta^{(0)} = \\left(\\frac{3}{2}, \\frac{1}{2}\\right)^T.\n$$\n$$\n\\Delta \\theta_1 = \\theta_1^{(1)} - \\theta_1^{(0)} = \\frac{436}{119} - \\frac{3}{2} = \\frac{2 \\times 436 - 3 \\times 119}{2 \\times 119} = \\frac{872 - 357}{238} = \\frac{515}{238}.\n$$\n$$\n\\Delta \\theta_2 = \\theta_2^{(1)} - \\theta_2^{(0)} = -\\frac{13}{119} - \\frac{1}{2} = \\frac{-13 \\times 2 - 1 \\times 119}{2 \\times 119} = \\frac{-26 - 119}{238} = -\\frac{145}{238}.\n$$\n参数更新量为 $\\Delta \\theta = (\\frac{515}{238}, -\\frac{145}{238})^T$。\n由于 $515=5 \\times 103$，$145=5 \\times 29$，以及 $238=2 \\times 7 \\times 17$，它们之间没有公因数，因此这些分数已是最简形式。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{515}{238} & -\\frac{145}{238} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在如数据同化这样的大规模应用中，效率和正确性至关重要。本实践将指导您实现并验证一个基于伴随方法的 Huber 成本函数梯度，这是实际应用中的一项关键技能 。通过编写代码执行梯度检查，并明确验证 Huber 导数的饱和特性，您将确保您的实现在遇到大残差时既是正确的，也表现出预期的稳健性。",
            "id": "3389414",
            "problem": "考虑一个反问题和数据同化中的线性观测算子，其中未知状态向量为 $x \\in \\mathbb{R}^n$，观测向量为 $y \\in \\mathbb{R}^m$，正演算子为矩阵 $A \\in \\mathbb{R}^{m \\times n}$。观测模型残差定义为 $r(x) = A x - y$。为了实现对离群值的鲁棒性，数据失配使用阈值为 $\\delta > 0$ 的 Huber 损失来衡量，该损失由函数 $\\rho_{\\delta} : \\mathbb{R} \\to \\mathbb{R}$ 定义，对于小残差，它与二次损失一致，对于大残差，它过渡到线性损失。总目标函数为 $J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x))$，其中 $r_i(x)$ 表示残差的第 $i$ 个分量。\n\n你的任务是从 $r(x)$ 和 Huber 损失 $\\rho_{\\delta}$ 的定义出发，使用链式法则实现一个关于 $x$ 的 $J(x)$ 的基于伴随的梯度，并通过一个基于中心有限差分的鲁棒梯度检验程序来验证其正确性。此外，你必须明确验证 Huber 损失关于残差分量的导数所预期的饱和行为：对于满足 $|r_i(x)| > \\delta$ 的残差分量，其关于 $r_i$ 的对应导数必须取恒定幅值 $\\delta$，符号与 $\\operatorname{sign}(r_i)$ 匹配；对于 $|r_i(x)| \\le \\delta$ 的残差分量，导数应与残差本身一致。你的程序必须在包括中等残差、恰好在 $\\pm \\delta$ 的边界残差以及严重离群值的合成案例上评估这些属性。\n\n从残差 $r(x) = A x - y$ 和 Huber 损失 $\\rho_{\\delta}$（在阈值内为二次，阈值外为线性）的基本定义出发，推导并实现：\n- 通过链式法则得到的 $J(x)$ 关于 $x$ 的伴随梯度，用 $A$ 和 Huber 损失关于 $r$ 的导数表示。\n- 一个使用步长 $\\varepsilon > 0$ 的中心差分梯度检验器，用于计算 $J(x)$ 梯度的每个分量的有限差分近似。\n- 一个饱和验证器，对于每个残差分量 $r_i(x)$，检查当 $|r_i(x)| > \\delta$ 时关于 $r_i$ 的导数是否饱和在 $\\pm \\delta$，当 $|r_i(x)| < \\delta$ 时是否等于 $r_i(x)$，以及当 $r_i(x) = \\pm \\delta$ 时在边界处是否等于 $\\pm \\delta$。\n\n通过要求伴随梯度和中心差分梯度之间的最大绝对差小于给定的容差 $\\tau > 0$ 来从数值上判断梯度的正确性。饱和验证器应在边界处使用严格的数值相等性检查，并在其他地方酌情使用基于容差的检查。\n\n实现以下测试套件，将每个案例的布尔结果计算为以下条件的逻辑与：\n- 在容差标准下梯度检查通过，且\n- 所有残差分量的饱和检查均通过。\n\n测试案例：\n1. 使用单位正演算子的顺利路径：\n   - 维度：$n = 5$，$m = 5$。\n   - $A = I_5$（$5 \\times 5$ 单位矩阵）。\n   - $x = [0.0, 1.0, -2.0, 0.5, -0.5]$。\n   - $y = [0.0, -1.0, 10.0, 0.4, -0.9]$。\n   - $\\delta = 0.7$。\n   - 有限差分步长：$\\varepsilon = 10^{-6}$。\n   - 梯度检查容差：$\\tau = 10^{-7}$。\n\n2. 在拐点处的边界情况：\n   - 维度：$n = 5$，$m = 5$。\n   - $A = I_5$。\n   - 选择残差 $r = [0.5, -0.5, 0.4999999, -0.5000001, 10.0]$ 并令 $x = [0, 0, 0, 0, 0]$，$y = -r$，使得 $r(x) = A x - y = r$ 完全成立。\n   - $\\delta = 0.5$。\n   - $\\varepsilon = 10^{-8}$。\n   - $\\tau = 10^{-6}$。\n\n3. 一般的非单位正演算子：\n   - 维度：$n = 4$，$m = 4$。\n   - $A = \\begin{bmatrix} 1.0 & 0.2 & 0.0 & -0.1 \\\\ 0.0 & 1.5 & -0.5 & 0.0 \\\\ 0.0 & 0.0 & -1.0 & 2.0 \\\\ 0.3 & 0.0 & 0.0 & 0.7 \\end{bmatrix}$。\n   - $x = [0.2, -0.3, 1.0, -1.2]$。\n   - $y = [2.0, -1.0, 10.0, 0.0]$。\n   - $\\delta = 1.0$。\n   - $\\varepsilon = 10^{-6}$。\n   - $\\tau = 10^{-6}$。\n\n对于测试案例 2 中涉及边界 $r_i(x) = \\pm \\delta$ 处相等性的数值检查，按其定义精确处理相等性（无容差），对于其他饱和检查，使用 $10^{-12}$ 的小数值容差以考虑浮点运算。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，\"[result1,result2,result3]\"），每个结果都是一个布尔值，指示相应测试案例的梯度检查和饱和检查条件是否都已通过。不允许外部输入，此问题中不出现物理单位；未使用角度，因此无需指定单位。最终输出必须严格遵循单个 print 语句中指定的格式。",
            "solution": "该问题要求推导和实现一个基于 Huber 损失的目标函数的伴随梯度，然后对其正确性和饱和特性进行数值验证。解决方案分三个阶段进行：数学推导、数值验证程序的描述和实现细节。\n\n首先，我们将问题形式化。状态向量为 $x \\in \\mathbb{R}^n$，观测向量为 $y \\in \\mathbb{R}^m$，线性正演算子为 $A \\in \\mathbb{R}^{m \\times n}$。残差向量定义为 $r(x) = A x - y$。目标函数 $J(x)$ 是应用于残差各分量的 Huber 损失之和：\n$$ J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x)) $$\nHuber 损失函数 $\\rho_{\\delta}(z)$ 的阈值为 $\\delta > 0$，它在小值时的二次（$L_2$）损失和大值时的线性（$L_1$）损失之间提供了一个过渡，增强了对离群值的鲁棒性。其标准定义确保了函数及其一阶导数的连续性，定义如下：\n$$ \\rho_{\\delta}(z) = \\begin{cases} \\frac{1}{2} z^2 & \\text{若 } |z| \\le \\delta \\\\ \\delta |z| - \\frac{1}{2} \\delta^2 & \\text{若 } |z| > \\delta \\end{cases} $$\nHuber 损失对其自变量的导数，记为 $\\psi(z) = \\frac{d\\rho_{\\delta}}{dz}(z)$，是梯度计算的关键组成部分。它由下式给出：\n$$ \\psi(z) = \\begin{cases} z & \\text{若 } |z| \\le \\delta \\\\ \\delta \\cdot \\operatorname{sign}(z) & \\text{若 } |z| > \\delta \\end{cases} $$\n该函数也称为软阈值或收缩函数。\n\n主要任务是求 $J(x)$ 关于 $x$ 的梯度，记为 $\\nabla_x J(x)$。我们应用链式法则。梯度的第 $j$ 个分量是：\n$$ (\\nabla_x J(x))_j = \\frac{\\partial J}{\\partial x_j} = \\sum_{i=1}^m \\frac{d \\rho_{\\delta}}{d r_i}(r_i(x)) \\frac{\\partial r_i}{\\partial x_j} $$\n残差第 $i$ 个分量 $r_i(x) = (Ax - y)_i = \\sum_{k=1}^n A_{ik} x_k - y_i$ 关于 $x_j$ 的偏导数为：\n$$ \\frac{\\partial r_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^n A_{ik} x_k - y_i \\right) = \\sum_{k=1}^n A_{ik} \\frac{\\partial x_k}{\\partial x_j} = \\sum_{k=1}^n A_{ik} \\delta_{kj} = A_{ij} $$\n其中 $\\delta_{kj}$ 是 Kronecker delta。\n将此代入梯度表达式并使用 $\\psi$ 的定义，我们有：\n$$ (\\nabla_x J(x))_j = \\sum_{i=1}^m \\psi(r_i(x)) A_{ij} = \\sum_{i=1}^m (A^T)_{ji} \\psi(r_i(x)) $$\n此方程表示矩阵向量乘积 $A^T \\psi(r(x))$ 的第 $j$ 个分量，其中 $\\psi(r(x))$ 是一个向量，其分量为 $\\psi(r_i(x))$。因此，完整的梯度向量是：\n$$ \\nabla_x J(x) = A^T \\psi(A x - y) $$\n这就是基于伴随的梯度。它在计算上是高效的，因为它需要一次与 $A$ 的矩阵向量乘积来计算残差，然后逐元素应用函数 $\\psi$，接着一次与转置（伴随）矩阵 $A^T$ 的矩阵向量乘积。\n\n为了验证这个解析推导的梯度的正确性，我们采用中心有限差分近似。对于每个分量 $j \\in \\{1, \\dots, n\\}$，偏导数 $\\frac{\\partial J}{\\partial x_j}$ 近似为：\n$$ (\\nabla_x J(x))_j \\approx \\frac{J(x + \\varepsilon e_j) - J(x - \\varepsilon e_j)}{2\\varepsilon} $$\n其中 $e_j$ 是第 $j$ 个标准基向量，$\\varepsilon$ 是一个小的步长。一个鲁棒的实现需要为梯度的每个分量计算两次完整的目标函数 $J$。梯度正确性的检查包括将基于伴随的梯度与此数值近似进行比较。如果两个梯度向量之间的最大绝对差低于指定的容差 $\\tau$，则检查通过：\n$$ \\max_{j} \\left| (\\nabla_x J(x))_{\\text{adjoint}, j} - (\\nabla_x J(x))_{\\text{fd}, j} \\right| < \\tau $$\n尽管在某些 $i$ 使得 $|r_i(x)| = \\delta$ 的点上 $J(x)$ 的导数没有定义，但中心差分近似仍然表现良好，并提供了一个有效的次梯度的良好估计，在我们的公式中，这对应于解析梯度。\n\n最后，我们必须验证 Huber 导数 $\\psi(r_i)$ 的饱和行为。这涉及到检查计算出的 $\\psi(r_i)$ 值是否符合其对所有残差分量 $r_i$ 的分段定义。检查内容如下：\n1. 对于线性区域中的残差分量 $|r_i| > \\delta$，我们验证 $\\psi(r_i)$ 等于 $\\delta \\cdot \\operatorname{sign}(r_i)$，误差在小的数值容差 $10^{-12}$ 之内。\n2. 对于二次区域中的残差分量 $|r_i| < \\delta$，我们验证 $\\psi(r_i)$ 等于 $r_i$，误差在相同的数值容差之内。\n3. 对于恰好在边界上的残差分量 $|r_i| = \\delta$，我们执行严格的相等性检查，即 $\\psi(r_i)$ 等于 $r_i$（即 $\\pm\\delta$）。这是有效的，因为我们对 $\\psi(z)$ 的定义是连续的，并且在二次情况下包含了端点，其中 $\\psi(z)=z$。\n\n每个测试用例的总体结果是一个布尔值，表示梯度检查通过和所有饱和检查通过的逻辑与，从而为实现提供了全面的验证。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy\n#       version: 1.23.5\n#     - name: scipy\n#       version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving, implementing, and verifying the adjoint-based gradient \n    of a Huber loss objective function for a set of predefined test cases.\n    \"\"\"\n\n    def huber_objective(x_vec, A_mat, y_vec, delta):\n        \"\"\"Computes the total Huber loss J(x).\"\"\"\n        residuals = A_mat @ x_vec - y_vec\n        loss = 0.0\n        for r_i in residuals:\n            abs_r_i = np.abs(r_i)\n            if abs_r_i <= delta:\n                loss += 0.5 * r_i**2\n            else:\n                loss += delta * abs_r_i - 0.5 * delta**2\n        return loss\n\n    def huber_derivative(residuals, delta):\n        \"\"\"Computes the derivative of the Huber loss with respect to residuals, psi(r).\"\"\"\n        psi = np.zeros_like(residuals, dtype=float)\n        for i, r_i in enumerate(residuals):\n            if np.abs(r_i) <= delta:\n                psi[i] = r_i\n            else:\n                psi[i] = delta * np.sign(r_i)\n        return psi\n\n    def run_test_case(params):\n        \"\"\"\n        Runs a single test case, performing adjoint gradient calculation, saturation checks, \n        and finite-difference gradient verification.\n        \"\"\"\n        A = params[\"A\"]\n        x = params[\"x\"]\n        y = params[\"y\"]\n        delta = params[\"delta\"]\n        eps = params[\"eps\"]\n        tau = params[\"tau\"]\n        sat_tol = 1e-12\n\n        # 1. Adjoint-based gradient calculation\n        residuals = A @ x - y\n        psi_r = huber_derivative(residuals, delta)\n        grad_adj = A.T @ psi_r\n\n        # 2. Saturation verifier\n        saturation_ok = True\n        for i in range(len(residuals)):\n            r_i = residuals[i]\n            psi_i = psi_r[i]\n            \n            check_passed = False\n            # Strict equality check for boundary cases, as required\n            if r_i == delta:\n                if psi_i == delta:\n                    check_passed = True\n            elif r_i == -delta:\n                if psi_i == -delta:\n                    check_passed = True\n            # Tolerance-based checks for non-boundary cases\n            elif np.abs(r_i) > delta:\n                if np.abs(psi_i - delta * np.sign(r_i)) < sat_tol:\n                    check_passed = True\n            elif np.abs(r_i) < delta:\n                if np.abs(psi_i - r_i) < sat_tol:\n                    check_passed = True\n            \n            if not check_passed:\n                saturation_ok = False\n                break\n        \n        # 3. Central finite-difference gradient checker\n        n = len(x)\n        grad_fd = np.zeros(n)\n        for j in range(n):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[j] += eps\n            x_minus[j] -= eps\n            \n            J_plus = huber_objective(x_plus, A, y, delta)\n            J_minus = huber_objective(x_minus, A, y, delta)\n            \n            grad_fd[j] = (J_plus - J_minus) / (2 * eps)\n            \n        # 4. Gradient correctness check\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        gradient_ok = max_abs_diff < tau\n        \n        # 5. Final result is a conjunction of both checks\n        return gradient_ok and saturation_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 1.0, -2.0, 0.5, -0.5]),\n            \"y\": np.array([0.0, -1.0, 10.0, 0.4, -0.9]),\n            \"delta\": 0.7,\n            \"eps\": 1e-6,\n            \"tau\": 1e-7,\n        },\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"y\": -np.array([0.5, -0.5, 0.4999999, -0.5000001, 10.0]),\n            \"delta\": 0.5,\n            \"eps\": 1e-8,\n            \"tau\": 1e-6,\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.2, 0.0, -0.1],\n                [0.0, 1.5, -0.5, 0.0],\n                [0.0, 0.0, -1.0, 2.0],\n                [0.3, 0.0, 0.0, 0.7]\n            ]),\n            \"x\": np.array([0.2, -0.3, 1.0, -1.2]),\n            \"y\": np.array([2.0, -1.0, 10.0, 0.0]),\n            \"delta\": 1.0,\n            \"eps\": 1e-6,\n            \"tau\": 1e-6,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```"
        }
    ]
}