{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is a foundational calculation designed to build intuition for robust estimation. By working through a simple scalar problem with one clean data point and one outlier, you will see precisely how the Huber loss function provides a compromise between the sensitivity of the squared-error ($L_2$) loss and the sometimes extreme robustness of the absolute-error ($L_1$) loss. This hands-on comparison is key to developing a feel for how different statistical estimators react to contaminated data .",
            "id": "3389451",
            "problem": "Consider a scalar inverse problem in the context of Data Assimilation (DA) where the unknown state is a single real parameter $x \\in \\mathbb{R}$. Two independent point observations are available through the identity observation operator, so each observation directly measures $x$. One observation is clean and one is contaminated by a large error. Numerically, the observations are $y_{1} = 1.2$ (clean) and $y_{2} = 9.0$ (contaminated). To reflect differing trust, assign observation weights $w_{1} = 2$ to $y_{1}$ and $w_{2} = 0.5$ to $y_{2}$. Let the Huber loss threshold be $\\delta = 1$.\n\nDefine the residuals for the two observations as $r_{1}(x) = x - y_{1}$ and $r_{2}(x) = x - y_{2}$. Consider three estimators corresponding to three data misfit models:\n- The $L_{2}$ (least squares) estimator minimizes the weighted quadratic misfit $\\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$.\n- The $L_{1}$ (least absolute deviations) estimator minimizes the weighted absolute misfit $\\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$.\n- The Huber estimator minimizes the weighted Huber loss $\\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$ with threshold $\\delta$, where $\\rho_{\\delta}$ is the Huber loss.\n\nStarting from the core definitions of these loss functions and first-order optimality conditions for convex minimization, derive each estimator for this two-observation scalar problem. Compute the three estimates explicitly for the given numerical values, and compare them numerically. Report only the numerical value of the Huber estimate $\\hat{x}_{\\mathrm{Huber}}$. No rounding is required. The unknown state $x$ is dimensionless, so express the final answer as a pure number without units.",
            "solution": "The problem statement is critically evaluated for validity before attempting a solution.\n\n### Step 1: Extract Givens\n- Unknown state: a single real parameter $x \\in \\mathbb{R}$.\n- Observations: $y_{1} = 1.2$ (clean), $y_{2} = 9.0$ (contaminated).\n- Observation operator: Identity.\n- Observation weights: $w_{1} = 2$, $w_{2} = 0.5$.\n- Huber loss threshold: $\\delta = 1$.\n- Residuals: $r_{1}(x) = x - y_{1}$, $r_{2}(x) = x - y_{2}$.\n- $L_{2}$ estimator minimizes: $J_2(x) = \\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$.\n- $L_{1}$ estimator minimizes: $J_1(x) = \\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$.\n- Huber estimator minimizes: $J_H(x) = \\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$, where $\\rho_{\\delta}$ is the Huber loss function.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed based on the predefined criteria:\n- **Scientifically Grounded:** The problem is a standard exercise in statistical estimation and optimization, specifically in the context of robust statistics as applied to inverse problems and data assimilation. The concepts of $L_1$, $L_2$, and Huber loss are fundamental in these fields. The setup is scientifically and mathematically sound.\n- **Well-Posed:** The cost functions defined ($L_1$, $L_2$, Huber) are all convex for a scalar variable $x$. A convex function on $\\mathbb{R}$ has a non-empty set of global minimizers, and for these strictly convex (or piecewise strictly convex) functions, the minimizer is unique. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical definitions and numerical values. There are no subjective or ambiguous terms.\n- **Completeness and Consistency:** All necessary data ($y_1, y_2, w_1, w_2, \\delta$) and definitions are provided. The setup is self-contained and free of contradictions.\n- **Realism and Feasibility:** The scenario of having a mix of clean and contaminated data is a canonical problem in data analysis, making the problem realistic. The numerical values are chosen to clearly illustrate the differing behaviors of the estimators.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-posed, scientifically grounded problem that can be solved using established mathematical principles. I will now proceed with deriving the solution.\n\nThe goal is to find the estimates $\\hat{x}_{L_2}$, $\\hat{x}_{L_1}$, and $\\hat{x}_{\\text{Huber}}$ that minimize their respective cost functions, and then report the numerical value for $\\hat{x}_{\\text{Huber}}$.\n\n**1. The $L_2$ (Least Squares) Estimator**\nThe $L_2$ cost function is given by:\n$$J_2(x) = w_1 (x-y_1)^2 + w_2 (x-y_2)^2$$\nThis is a quadratic function of $x$, and its minimum can be found by setting its first derivative with respect to $x$ to zero.\n$$\\frac{dJ_2(x)}{dx} = 2w_1(x-y_1) + 2w_2(x-y_2) = 0$$\nSolving for $x$:\n$$w_1 x - w_1 y_1 + w_2 x - w_2 y_2 = 0$$\n$$(w_1 + w_2)x = w_1 y_1 + w_2 y_2$$\nThe $L_2$ estimator, $\\hat{x}_{L_2}$, is the weighted average of the observations:\n$$\\hat{x}_{L_2} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2}$$\nSubstituting the given numerical values: $y_1=1.2$, $y_2=9.0$, $w_1=2$, and $w_2=0.5$.\n$$\\hat{x}_{L_2} = \\frac{2(1.2) + 0.5(9.0)}{2 + 0.5} = \\frac{2.4 + 4.5}{2.5} = \\frac{6.9}{2.5} = 2.76$$\n\n**2. The $L_1$ (Least Absolute Deviations) Estimator**\nThe $L_1$ cost function is:\n$$J_1(x) = w_1|x-y_1| + w_2|x-y_2|$$\nThis function is convex but not differentiable at $x=y_1$ and $x=y_2$. The minimizer is the weighted median of the observations. The first-order optimality condition is $0 \\in \\partial J_1(x)$, where $\\partial J_1(x)$ is the subgradient. The derivative of $J_1(x)$, where it exists, is:\n$$\\frac{dJ_1(x)}{dx} = w_1 \\text{sgn}(x-y_1) + w_2 \\text{sgn}(x-y_2)$$\nLet's analyze the derivative based on the value of $x$ relative to $y_1=1.2$ and $y_2=9.0$:\n- For $x < 1.2$: $\\frac{dJ_1}{dx} = -w_1 - w_2 = -2 - 0.5 = -2.5 < 0$. The function is decreasing.\n- For $1.2 < x < 9.0$: $\\frac{dJ_1}{dx} = w_1 - w_2 = 2 - 0.5 = 1.5 > 0$. The function is increasing.\n- For $x > 9.0$: $\\frac{dJ_1}{dx} = w_1 + w_2 = 2 + 0.5 = 2.5 > 0$. The function is increasing.\nThe derivative changes sign from negative to positive at $x = y_1 = 1.2$. Thus, the minimum of $J_1(x)$ occurs at this point.\n$$\\hat{x}_{L_1} = y_1 = 1.2$$\n\n**3. The Huber Estimator**\nThe Huber cost function is given by $J_H(x) = w_1 \\rho_{\\delta}(x-y_1) + w_2 \\rho_{\\delta}(x-y_2)$, with the Huber loss function $\\rho_{\\delta}(r)$ defined as:\n$$\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2}r^2 & \\text{if } |r| \\le \\delta \\\\ \\delta|r| - \\frac{1}{2}\\delta^2 & \\text{if } |r| > \\delta \\end{cases}$$\nThis function is convex and continuously differentiable. Its derivative is the Huber score function, $\\psi_{\\delta}(r) = \\rho'_{\\delta}(r)$:\n$$\\psi_{\\delta}(r) = \\begin{cases} r & \\text{if } |r| \\le \\delta \\\\ \\delta \\cdot \\text{sgn}(r) & \\text{if } |r| > \\delta \\end{cases} $$\nThe minimum of $J_H(x)$ is found by setting its derivative to zero:\n$$\\frac{dJ_H(x)}{dx} = w_1 \\psi_{\\delta}(x-y_1) + w_2 \\psi_{\\delta}(x-y_2) = 0$$\nWith the given values $w_1=2$, $w_2=0.5$, $y_1=1.2$, $y_2=9.0$, and $\\delta=1$, the equation becomes:\n$$2 \\cdot \\psi_{1}(x-1.2) + 0.5 \\cdot \\psi_{1}(x-9.0) = 0$$\nBased on the other estimates and the weights, we expect the solution $\\hat{x}_{\\text{Huber}}$ to be close to $y_1=1.2$ but not identical to it. This suggests that the residual for the first observation, $r_1 = x-y_1$, is small, while the residual for the second, $r_2 = x-y_2$, is large. Let us hypothesize that $|x-y_1| \\le \\delta$ and $|x-y_2| > \\delta$.\n- If $|r_1| = |x-1.2| \\le 1$, then $\\psi_{1}(x-1.2) = x-1.2$.\n- If $|r_2| = |x-9.0| > 1$, then $\\psi_{1}(x-9.0) = 1 \\cdot \\text{sgn}(x-9.0)$. Since we expect $x$ to be near $1.2$, $x-9.0$ will be negative, so $\\text{sgn}(x-9.0)=-1$.\nSubstituting these into the optimality condition:\n$$2(x-1.2) + 0.5(-1) = 0$$\n$$2x - 2.4 - 0.5 = 0$$\n$$2x = 2.9$$\n$$x = 1.45$$\nWe must now verify if this solution is consistent with our initial hypothesis.\n- For $x=1.45$, the first residual is $r_1 = 1.45 - 1.2 = 0.25$. We check if $|r_1| \\le \\delta$: $|0.25| \\le 1$. This is true.\n- The second residual is $r_2 = 1.45 - 9.0 = -7.55$. We check if $|r_2| > \\delta$: $|-7.55| > 1$. This is also true.\nThe hypothesis holds, and thus the Huber estimate is:\n$$\\hat{x}_{\\text{Huber}} = 1.45$$\n\n**Comparison of Estimates**\n- $\\hat{x}_{L_2} = 2.76$: This estimate is strongly influenced by the outlier $y_2=9.0$. The quadratic penalty on the large residual $r_2$ pulls the estimate significantly away from the clean observation $y_1=1.2$, demonstrating the non-robustness of least squares.\n- $\\hat{x}_{L_1} = 1.2$: This estimate, the weighted median, completely discards the outlier information and snaps to the clean observation because its weight is larger. This demonstrates extreme robustness.\n- $\\hat{x}_{\\text{Huber}} = 1.45$: This estimate represents a compromise. It is close to the clean observation $y_1=1.2$, showing robustness to the outlier $y_2=9.0$. However, unlike the $L_1$ estimate, it is still influenced slightly by the outlier, as the linear penalty part of the Huber loss still allows the outlier to have a small, constant gradient contribution. It lies between the $L_1$ and $L_2$ estimates.\n\nThe problem asks for the numerical value of the Huber estimate only.",
            "answer": "$$\n\\boxed{1.45}\n$$"
        },
        {
            "introduction": "Building on the conceptual understanding of Huber estimation, this practice transitions to a more realistic scenario involving a linear model with multiple parameters. You will implement one of the most common algorithms for M-estimation, Iteratively Reweighted Least Squares (IRLS), by analytically deriving and applying the weights that effectively down-weight outliers in a regression problem. This exercise demonstrates how the abstract principles of robust estimation are translated into a practical, iterative algorithm for solving the resulting nonlinear optimization problem .",
            "id": "3389420",
            "problem": "Consider a linear inverse problem in which a state vector $\\theta \\in \\mathbb{R}^{2}$ parameterizes a linear observation operator $g(x;\\theta) = \\theta_{1} x + \\theta_{2}$. A set of $4$ scalar observations $\\{(x_{i}, d_{i})\\}_{i=1}^{4}$ is provided, with design points and data given by $(x_{1}, d_{1}) = (0, 1)$, $(x_{2}, d_{2}) = (1, 3)$, $(x_{3}, d_{3}) = (2, 5)$, and $(x_{4}, d_{4}) = (3, 40)$. The fourth datum is contaminated by a gross error, producing an outlier. Robust estimation is to be performed using the Huber loss. The Huber loss $\\rho_{\\delta}: \\mathbb{R} \\to \\mathbb{R}$ with threshold $\\delta > 0$ is defined by\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2}, & |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2}, & |r| > \\delta,\n\\end{cases}\n$$\nwhere $r$ is a residual. The initial parameter is $\\theta^{(0)} = (\\theta_{1}^{(0)}, \\theta_{2}^{(0)}) = (1.5, 0.5)$ and the threshold is $\\delta = 2$.\n\nStarting from the core definitions of robust M-estimation and the Huber loss, derive the Iteratively Reweighted Least Squares (IRLS) weights for the residuals $r_{i}^{(0)} = d_{i} - (\\theta_{1}^{(0)} x_{i} + \\theta_{2}^{(0)})$, construct the corresponding weighted normal equations for the two-parameter linear model, and solve them exactly to obtain the next-iterate parameter $\\theta^{(1)}$. Finally, compute the parameter update $\\Delta \\theta = \\theta^{(1)} - \\theta^{(0)}$ as exact rational numbers. Express the final answer as a two-component row vector using the $\\texttt{pmatrix}$ environment. No rounding is required.",
            "solution": "The problem requires finding the first parameter update $\\Delta \\theta$ for a linear model using the Iteratively Reweighted Least Squares (IRLS) algorithm for M-estimation with a Huber loss function.\n\nThe linear model is given by $g(x;\\theta) = \\theta_{1} x + \\theta_{2}$, where $\\theta = (\\theta_{1}, \\theta_{2})^T \\in \\mathbb{R}^{2}$.\nThe objective is to minimize the sum of Huber losses over the residuals:\n$$\nJ(\\theta) = \\sum_{i=1}^{4} \\rho_{\\delta}(r_i) = \\sum_{i=1}^{4} \\rho_{\\delta}(d_i - g(x_i; \\theta))\n$$\nThe Huber loss function $\\rho_{\\delta}(r)$ is defined as:\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2}, & |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2}, & |r| > \\delta.\n\\end{cases}\n$$\nTo minimize $J(\\theta)$, we set its gradient with respect to $\\theta$ to zero. The derivative of $\\rho_{\\delta}(r)$ with respect to $r$ is the influence function $\\psi_{\\delta}(r)$:\n$$\n\\psi_{\\delta}(r) = \\frac{d\\rho_{\\delta}}{dr}(r) = \n\\begin{cases}\nr, & |r| \\leq \\delta, \\\\\n\\delta \\, \\text{sgn}(r), & |r| > \\delta.\n\\end{cases}\n$$\nThe gradient of the cost function is:\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{i=1}^{4} \\frac{\\partial \\rho_{\\delta}(r_i)}{\\partial r_i} \\frac{\\partial r_i}{\\partial \\theta} = \\sum_{i=1}^{4} \\psi_{\\delta}(r_i) (-\\nabla_{\\theta} g(x_i; \\theta)) = 0.\n$$\nSince $g(x_i; \\theta) = \\theta_1 x_i + \\theta_2$, we have $\\nabla_{\\theta} g(x_i; \\theta) = (x_i, 1)^T$. Let this be the $i$-th row of the design matrix $G$, transposed.\nThe system of equations becomes:\n$$\n\\sum_{i=1}^{4} \\psi_{\\delta}(d_i - g(x_i; \\theta)) \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe IRLS algorithm linearizes this system by defining a weight function $w(r) = \\psi_{\\delta}(r)/r$.\n$$\nw(r) = \n\\begin{cases}\n1, & |r| \\leq \\delta, \\\\\n\\frac{\\delta}{|r|}, & |r| > \\delta.\n\\end{cases}\n$$\nUsing this, the system of equations can be written as:\n$$\n\\sum_{i=1}^{4} w(r_i) r_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\sum_{i=1}^{4} w(r_i) (d_i - (\\theta_1 x_i + \\theta_2)) \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nAt each iteration $k$, we compute weights $w_i^{(k)} = w(r_i^{(k-1)})$ based on the previous residuals $r_i^{(k-1)} = d_i - g(x_i; \\theta^{(k-1)})$, and then solve the following weighted least-squares problem for $\\theta^{(k)}$:\n$$\n\\theta^{(k)} = \\arg\\min_{\\theta} \\sum_{i=1}^{4} w_i^{(k)} (d_i - g(x_i; \\theta))^2.\n$$\nThis leads to the weighted normal equations:\n$$\n(G^T W^{(k)} G) \\theta^{(k)} = G^T W^{(k)} d,\n$$\nwhere $G$ is the design matrix, $d$ is the vector of observations, and $W^{(k)}$ is the diagonal matrix of weights $w_i^{(k)}$.\n\nFirst, we compute the initial residuals $r_i^{(0)}$ using the initial parameter $\\theta^{(0)} = (1.5, 0.5) = (3/2, 1/2)$ and the model $g(x_i; \\theta^{(0)}) = \\frac{3}{2}x_i + \\frac{1}{2}$. The data points are $(x_1, d_1) = (0, 1)$, $(x_2, d_2) = (1, 3)$, $(x_3, d_3) = (2, 5)$, and $(x_4, d_4) = (3, 40)$.\n$r_1^{(0)} = 1 - (\\frac{3}{2}(0) + \\frac{1}{2}) = 1 - \\frac{1}{2} = \\frac{1}{2} = 0.5$.\n$r_2^{(0)} = 3 - (\\frac{3}{2}(1) + \\frac{1}{2}) = 3 - 2 = 1$.\n$r_3^{(0)} = 5 - (\\frac{3}{2}(2) + \\frac{1}{2}) = 5 - \\frac{7}{2} = \\frac{3}{2} = 1.5$.\n$r_4^{(0)} = 40 - (\\frac{3}{2}(3) + \\frac{1}{2}) = 40 - 5 = 35$.\n\nNext, we compute the weights $w_i^{(1)}$ for the first iteration using the threshold $\\delta = 2$.\n$|r_1^{(0)}| = 0.5 \\leq 2 \\implies w_1^{(1)} = 1$.\n$|r_2^{(0)}| = 1 \\leq 2 \\implies w_2^{(1)} = 1$.\n$|r_3^{(0)}| = 1.5 \\leq 2 \\implies w_3^{(1)} = 1$.\n$|r_4^{(0)}| = 35 > 2 \\implies w_4^{(1)} = \\frac{\\delta}{|r_4^{(0)}|} = \\frac{2}{35}$.\n\nNow, we set up the weighted normal equations for $\\theta^{(1)}$. The design matrix $G$, data vector $d$, and weight matrix $W^{(1)}$ are:\n$$\nG = \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{pmatrix}, \\quad d = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 40 \\end{pmatrix}, \\quad W^{(1)} = \\text{diag}\\left(1, 1, 1, \\frac{2}{35}\\right).\n$$\nWe need to calculate $G^T W^{(1)} G$ and $G^T W^{(1)} d$.\n$$\nG^T W^{(1)} = \\begin{pmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & \\frac{2}{35} \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & 2 & \\frac{6}{35} \\\\ 1 & 1 & 1 & \\frac{2}{35} \\end{pmatrix}.\n$$\nThen,\n$$\nG^T W^{(1)} G = \\begin{pmatrix} 0 & 1 & 2 & \\frac{6}{35} \\\\ 1 & 1 & 1 & \\frac{2}{35} \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\\\ 2 & 1 \\\\ 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+4+\\frac{18}{35} & 1+2+\\frac{6}{35} \\\\ 1+2+\\frac{6}{35} & 1+1+1+\\frac{2}{35} \\end{pmatrix}\n\\\\ = \\begin{pmatrix} 5+\\frac{18}{35} & 3+\\frac{6}{35} \\\\ 3+\\frac{6}{35} & 3+\\frac{2}{35} \\end{pmatrix} = \\begin{pmatrix} \\frac{193}{35} & \\frac{111}{35} \\\\ \\frac{111}{35} & \\frac{107}{35} \\end{pmatrix} = \\frac{1}{35}\\begin{pmatrix} 193 & 111 \\\\ 111 & 107 \\end{pmatrix}.\n$$\nAnd,\n$$\nG^T W^{(1)} d = \\begin{pmatrix} 0 & 1 & 2 & \\frac{6}{35} \\\\ 1 & 1 & 1 & \\frac{2}{35} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 40 \\end{pmatrix} = \\begin{pmatrix} 3+10+\\frac{240}{35} \\\\ 1+3+5+\\frac{80}{35} \\end{pmatrix} = \\begin{pmatrix} 13+\\frac{48}{7} \\\\ 9+\\frac{16}{7} \\end{pmatrix}\n\\\\ = \\begin{pmatrix} \\frac{91+48}{7} \\\\ \\frac{63+16}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{139}{7} \\\\ \\frac{79}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{695}{35} \\\\ \\frac{395}{35} \\end{pmatrix} = \\frac{1}{35}\\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}.\n$$\nThe normal equations are $\\frac{1}{35}\\begin{pmatrix} 193 & 111 \\\\ 111 & 107 \\end{pmatrix} \\theta^{(1)} = \\frac{1}{35}\\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}$, which simplifies to:\n$$\n\\begin{pmatrix} 193 & 111 \\\\ 111 & 107 \\end{pmatrix} \\begin{pmatrix} \\theta_1^{(1)} \\\\ \\theta_2^{(1)} \\end{pmatrix} = \\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}.\n$$\nWe solve this $2 \\times 2$ system for $\\theta^{(1)}$. Let the matrix be $A$. The determinant of $A$ is:\n$$\n\\det(A) = (193)(107) - (111)^2 = 20651 - 12321 = 8330.\n$$\nThe inverse of $A$ is:\n$$\nA^{-1} = \\frac{1}{8330} \\begin{pmatrix} 107 & -111 \\\\ -111 & 193 \\end{pmatrix}.\n$$\nNow we find $\\theta^{(1)} = A^{-1} \\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}$:\n$$\n\\begin{pmatrix} \\theta_1^{(1)} \\\\ \\theta_2^{(1)} \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} (107)(695) - (111)(395) \\\\ -(111)(695) + (193)(395) \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} 74365 - 43845 \\\\ -77145 + 76235 \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} 30520 \\\\ -910 \\end{pmatrix}.\n$$\nWe simplify the components of $\\theta^{(1)}$:\n$$\n\\theta_1^{(1)} = \\frac{30520}{8330} = \\frac{3052}{833} = \\frac{436 \\times 7}{119 \\times 7} = \\frac{436}{119}.\n$$\n$$\n\\theta_2^{(1)} = \\frac{-910}{8330} = \\frac{-91}{833} = \\frac{-13 \\times 7}{119 \\times 7} = -\\frac{13}{119}.\n$$\nSo, $\\theta^{(1)} = (\\frac{436}{119}, -\\frac{13}{119})^T$.\n\nFinally, we compute the parameter update $\\Delta \\theta = \\theta^{(1)} - \\theta^{(0)}$:\n$$\n\\theta^{(0)} = \\left(\\frac{3}{2}, \\frac{1}{2}\\right)^T.\n$$\n$$\n\\Delta \\theta_1 = \\theta_1^{(1)} - \\theta_1^{(0)} = \\frac{436}{119} - \\frac{3}{2} = \\frac{2 \\times 436 - 3 \\times 119}{2 \\times 119} = \\frac{872 - 357}{238} = \\frac{515}{238}.\n$$\n$$\n\\Delta \\theta_2 = \\theta_2^{(1)} - \\theta_2^{(0)} = -\\frac{13}{119} - \\frac{1}{2} = \\frac{-13 \\times 2 - 1 \\times 119}{2 \\times 119} = \\frac{-26 - 119}{238} = -\\frac{145}{238}.\n$$\nThe parameter update is $\\Delta \\theta = (\\frac{515}{238}, -\\frac{145}{238})^T$.\nThe fractions are irreducible as $515=5 \\times 103$, $145=5 \\times 29$, and $238=2 \\times 7 \\times 17$. There are no common factors.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{515}{238} & -\\frac{145}{238} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In the context of large-scale inverse problems, such as those in operational data assimilation, the efficiency and correctness of gradient computations are paramount for variational minimization. This coding exercise guides you through implementing the gradient of the Huber loss objective using the powerful adjoint method, which avoids the explicit formation of large Jacobian matrices. You will then perform a rigorous verification using finite differences and confirm the essential \"saturation\" property of the Huber score function, a critical skill for developing reliable and scalable scientific software .",
            "id": "3389414",
            "problem": "Consider a linear observation operator in inverse problems and data assimilation, where the unknown state vector is $x \\in \\mathbb{R}^n$, the observation vector is $y \\in \\mathbb{R}^m$, and the forward operator is the matrix $A \\in \\mathbb{R}^{m \\times n}$. The observation-model residual is defined by $r(x) = A x - y$. To achieve robustness to outliers, the data misfit is measured using the Huber loss with threshold $\\delta > 0$, defined by the function $\\rho_{\\delta} : \\mathbb{R} \\to \\mathbb{R}$ that coincides with a quadratic loss for small residuals and transitions to a linear loss for large residuals. The total objective is $J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x))$, where $r_i(x)$ denotes the $i$-th component of the residual.\n\nYour task is to implement an adjoint-based gradient of $J(x)$ with respect to $x$ using the chain rule starting from the definition of $r(x)$ and the Huber loss $\\rho_{\\delta}$, and to verify its correctness via a robust gradient-checking procedure based on central finite differences. In addition, you must explicitly verify the saturation behavior expected of the derivative of the Huber loss with respect to residual components: for residual components satisfying $|r_i(x)| > \\delta$, the corresponding derivative with respect to $r_i$ must take the constant magnitude $\\delta$ with sign matching $\\operatorname{sign}(r_i)$; for residual components with $|r_i(x)| \\le \\delta$, the derivative should coincide with the residual itself. Your program must evaluate these properties on synthetic cases that include moderate residuals, boundary residuals exactly at $\\pm \\delta$, and severe outliers.\n\nStarting from the fundamental definitions of residual $r(x) = A x - y$ and Huber loss $\\rho_{\\delta}$ (quadratic inside the threshold and linear outside), derive and implement:\n- The adjoint gradient of $J(x)$ with respect to $x$ via the chain rule, expressed in terms of $A$ and the derivative of the Huber loss with respect to $r$.\n- A central-difference gradient checker using step size $\\varepsilon > 0$ that computes the finite-difference approximation of each component of the gradient of $J(x)$.\n- A saturation verifier that, for each residual component $r_i(x)$, checks whether the derivative with respect to $r_i$ saturates at $\\pm \\delta$ when $|r_i(x)| > \\delta$, equals $r_i(x)$ when $|r_i(x)| < \\delta$, and equals $\\pm \\delta$ at the boundary when $r_i(x) = \\pm \\delta$.\n\nNumerically decide gradient correctness by requiring that the maximum absolute difference between the adjoint gradient and the central-difference gradient is less than a given tolerance $\\tau > 0$. The saturation verifier should use a strict numerical check for equality at the boundary and a tolerance-based check elsewhere as appropriate.\n\nImplement the following test suite, computing the boolean result for each case as the logical conjunction of:\n- the gradient check passing under the tolerance criterion, and\n- all saturation checks passing for the residual components.\n\nTest cases:\n1. Happy path with identity forward operator:\n   - Dimensions: $n = 5$, $m = 5$.\n   - $A = I_5$ (the $5 \\times 5$ identity matrix).\n   - $x = [0.0, 1.0, -2.0, 0.5, -0.5]$.\n   - $y = [0.0, -1.0, 10.0, 0.4, -0.9]$.\n   - $\\delta = 0.7$.\n   - Finite-difference step: $\\varepsilon = 10^{-6}$.\n   - Gradient-check tolerance: $\\tau = 10^{-7}$.\n\n2. Boundary case at the kink:\n   - Dimensions: $n = 5$, $m = 5$.\n   - $A = I_5$.\n   - Choose residuals $r = [0.5, -0.5, 0.4999999, -0.5000001, 10.0]$ and let $x = [0, 0, 0, 0, 0]$, $y = -r$ so that $r(x) = A x - y = r$ exactly.\n   - $\\delta = 0.5$.\n   - $\\varepsilon = 10^{-8}$.\n   - $\\tau = 10^{-6}$.\n\n3. General non-identity forward operator:\n   - Dimensions: $n = 4$, $m = 4$.\n   - $A = \\begin{bmatrix} 1.0 & 0.2 & 0.0 & -0.1 \\\\ 0.0 & 1.5 & -0.5 & 0.0 \\\\ 0.0 & 0.0 & -1.0 & 2.0 \\\\ 0.3 & 0.0 & 0.0 & 0.7 \\end{bmatrix}$.\n   - $x = [0.2, -0.3, 1.0, -1.2]$.\n   - $y = [2.0, -1.0, 10.0, 0.0]$.\n   - $\\delta = 1.0$.\n   - $\\varepsilon = 10^{-6}$.\n   - $\\tau = 10^{-6}$.\n\nFor numerical checks involving equalities at the boundary $r_i(x) = \\pm \\delta$ in Test Case $2$, treat equality exactly as defined (no tolerance), and for the other saturation checks, use a small numerical tolerance of $10^{-12}$ to account for floating point arithmetic.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean indicating whether both the gradient-check and saturation-check conditions passed for the corresponding test case. No external input is allowed, and no physical units appear in this problem; angles are not used and thus require no unit specification. The final output must strictly follow the specified format in a single print statement.",
            "solution": "The problem requires the derivation and implementation of the adjoint-based gradient for an objective function based on the Huber loss, followed by a numerical verification of its correctness and saturation properties. The solution proceeds in three stages: mathematical derivation, description of the numerical verification procedures, and implementation details.\n\nFirst, we formalize the problem. The state vector is $x \\in \\mathbb{R}^n$, the observation vector is $y \\in \\mathbb{R}^m$, and the linear forward operator is $A \\in \\mathbb{R}^{m \\times n}$. The residual vector is defined as $r(x) = A x - y$. The objective function $J(x)$ is the sum of Huber losses applied to each component of the residual:\n$$ J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x)) $$\nThe Huber loss function $\\rho_{\\delta}(z)$ with threshold $\\delta > 0$ provides a transition between a quadratic ($L_2$) loss for small values and a linear ($L_1$) loss for large values, enhancing robustness to outliers. Its standard definition, which ensures continuity of the function and its first derivative, is:\n$$ \\rho_{\\delta}(z) = \\begin{cases} \\frac{1}{2} z^2 & \\text{if } |z| \\le \\delta \\\\ \\delta |z| - \\frac{1}{2} \\delta^2 & \\text{if } |z| > \\delta \\end{cases} $$\nThe derivative of the Huber loss with respect to its argument, denoted $\\psi(z) = \\frac{d\\rho_{\\delta}}{dz}(z)$, is a key component for gradient computation. It is given by:\n$$ \\psi(z) = \\begin{cases} z & \\text{if } |z| \\le \\delta \\\\ \\delta \\cdot \\operatorname{sign}(z) & \\text{if } |z| > \\delta \\end{cases} $$\nThis function is also known as a soft-thresholding or shrinkage function.\n\nThe primary task is to find the gradient of $J(x)$ with respect to $x$, denoted $\\nabla_x J(x)$. We apply the chain rule. The $j$-th component of the gradient is:\n$$ (\\nabla_x J(x))_j = \\frac{\\partial J}{\\partial x_j} = \\sum_{i=1}^m \\frac{d \\rho_{\\delta}}{d r_i}(r_i(x)) \\frac{\\partial r_i}{\\partial x_j} $$\nThe partial derivative of the $i$-th component of the residual, $r_i(x) = (Ax - y)_i = \\sum_{k=1}^n A_{ik} x_k - y_i$, with respect to $x_j$ is:\n$$ \\frac{\\partial r_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^n A_{ik} x_k - y_i \\right) = \\sum_{k=1}^n A_{ik} \\frac{\\partial x_k}{\\partial x_j} = \\sum_{k=1}^n A_{ik} \\delta_{kj} = A_{ij} $$\nwhere $\\delta_{kj}$ is the Kronecker delta.\nSubstituting this into the gradient expression and using the definition of $\\psi$, we have:\n$$ (\\nabla_x J(x))_j = \\sum_{i=1}^m \\psi(r_i(x)) A_{ij} = \\sum_{i=1}^m (A^T)_{ji} \\psi(r_i(x)) $$\nThis equation represents the $j$-th component of the matrix-vector product $A^T \\psi(r(x))$, where $\\psi(r(x))$ is a vector whose components are $\\psi(r_i(x))$. Therefore, the full gradient vector is:\n$$ \\nabla_x J(x) = A^T \\psi(A x - y) $$\nThis is the adjoint-based gradient. It is computationally efficient as it requires one matrix-vector product with $A$ to compute the residual, followed by applying the element-wise function $\\psi$, and then one matrix-vector product with the transpose (adjoint) matrix $A^T$.\n\nTo verify the correctness of this analytically derived gradient, we employ a central finite-difference approximation. For each component $j \\in \\{1, \\dots, n\\}$, the partial derivative $\\frac{\\partial J}{\\partial x_j}$ is approximated as:\n$$ (\\nabla_x J(x))_j \\approx \\frac{J(x + \\varepsilon e_j) - J(x - \\varepsilon e_j)}{2\\varepsilon} $$\nwhere $e_j$ is the $j$-th canonical basis vector and $\\varepsilon$ is a small step size. A robust implementation requires evaluating the full objective function $J$ twice for each component of the gradient. The check for gradient correctness consists of comparing the adjoint-based gradient with this numerical approximation. The check passes if the maximum absolute difference between the two gradient vectors is below a specified tolerance $\\tau$:\n$$ \\max_{j} \\left| (\\nabla_x J(x))_{\\text{adjoint}, j} - (\\nabla_x J(x))_{\\text{fd}, j} \\right| < \\tau $$\nEven though the derivative of $J(x)$ is not defined at points where $|r_i(x)| = \\delta$ for some $i$, the central difference approximation remains well-behaved and provides a good estimate of a valid subgradient, which in our formulation corresponds to the analytical gradient.\n\nFinally, we must verify the saturation behavior of the Huber derivative $\\psi(r_i)$. This involves checking if the computed values of $\\psi(r_i)$ conform to its piecewise definition for all residual components $r_i$. The checks are:\n1. For residual components in the linear region, $|r_i| > \\delta$, we verify that $\\psi(r_i)$ equals $\\delta \\cdot \\operatorname{sign}(r_i)$, within a small numerical tolerance of $10^{-12}$.\n2. For residual components in the quadratic region, $|r_i| < \\delta$, we verify that $\\psi(r_i)$ equals $r_i$, within the same numerical tolerance.\n3. For residual components exactly at the boundary, $|r_i| = \\delta$, we perform a strict equality check that $\\psi(r_i)$ equals $r_i$ (i.e., $\\pm\\delta$). This is valid because our definition of $\\psi(z)$ is continuous and includes the endpoints in the quadratic case where $\\psi(z)=z$.\n\nThe overall result for each test case is a boolean value representing the logical conjunction of the gradient check passing and all saturation checks passing, thus providing a comprehensive validation of the implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy\n#       version: 1.23.5\n#     - name: scipy\n#       version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving, implementing, and verifying the adjoint-based gradient \n    of a Huber loss objective function for a set of predefined test cases.\n    \"\"\"\n\n    def huber_objective(x_vec, A_mat, y_vec, delta):\n        \"\"\"Computes the total Huber loss J(x).\"\"\"\n        residuals = A_mat @ x_vec - y_vec\n        loss = 0.0\n        for r_i in residuals:\n            abs_r_i = np.abs(r_i)\n            if abs_r_i = delta:\n                loss += 0.5 * r_i**2\n            else:\n                loss += delta * abs_r_i - 0.5 * delta**2\n        return loss\n\n    def huber_derivative(residuals, delta):\n        \"\"\"Computes the derivative of the Huber loss with respect to residuals, psi(r).\"\"\"\n        psi = np.zeros_like(residuals, dtype=float)\n        for i, r_i in enumerate(residuals):\n            if np.abs(r_i) = delta:\n                psi[i] = r_i\n            else:\n                psi[i] = delta * np.sign(r_i)\n        return psi\n\n    def run_test_case(params):\n        \"\"\"\n        Runs a single test case, performing adjoint gradient calculation, saturation checks, \n        and finite-difference gradient verification.\n        \"\"\"\n        A = params[\"A\"]\n        x = params[\"x\"]\n        y = params[\"y\"]\n        delta = params[\"delta\"]\n        eps = params[\"eps\"]\n        tau = params[\"tau\"]\n        sat_tol = 1e-12\n\n        # 1. Adjoint-based gradient calculation\n        residuals = A @ x - y\n        psi_r = huber_derivative(residuals, delta)\n        grad_adj = A.T @ psi_r\n\n        # 2. Saturation verifier\n        saturation_ok = True\n        for i in range(len(residuals)):\n            r_i = residuals[i]\n            psi_i = psi_r[i]\n            \n            check_passed = False\n            # Strict equality check for boundary cases, as required\n            if r_i == delta:\n                if psi_i == delta:\n                    check_passed = True\n            elif r_i == -delta:\n                if psi_i == -delta:\n                    check_passed = True\n            # Tolerance-based checks for non-boundary cases\n            elif np.abs(r_i) > delta:\n                if np.abs(psi_i - delta * np.sign(r_i))  sat_tol:\n                    check_passed = True\n            elif np.abs(r_i)  delta:\n                if np.abs(psi_i - r_i)  sat_tol:\n                    check_passed = True\n            \n            if not check_passed:\n                saturation_ok = False\n                break\n        \n        # 3. Central finite-difference gradient checker\n        n = len(x)\n        grad_fd = np.zeros(n)\n        for j in range(n):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[j] += eps\n            x_minus[j] -= eps\n            \n            J_plus = huber_objective(x_plus, A, y, delta)\n            J_minus = huber_objective(x_minus, A, y, delta)\n            \n            grad_fd[j] = (J_plus - J_minus) / (2 * eps)\n            \n        # 4. Gradient correctness check\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        gradient_ok = max_abs_diff  tau\n        \n        # 5. Final result is a conjunction of both checks\n        return gradient_ok and saturation_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 1.0, -2.0, 0.5, -0.5]),\n            \"y\": np.array([0.0, -1.0, 10.0, 0.4, -0.9]),\n            \"delta\": 0.7,\n            \"eps\": 1e-6,\n            \"tau\": 1e-7,\n        },\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"y\": -np.array([0.5, -0.5, 0.4999999, -0.5000001, 10.0]),\n            \"delta\": 0.5,\n            \"eps\": 1e-8,\n            \"tau\": 1e-6,\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.2, 0.0, -0.1],\n                [0.0, 1.5, -0.5, 0.0],\n                [0.0, 0.0, -1.0, 2.0],\n                [0.3, 0.0, 0.0, 0.7]\n            ]),\n            \"x\": np.array([0.2, -0.3, 1.0, -1.2]),\n            \"y\": np.array([2.0, -1.0, 10.0, 0.0]),\n            \"delta\": 1.0,\n            \"eps\": 1e-6,\n            \"tau\": 1e-6,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```"
        }
    ]
}