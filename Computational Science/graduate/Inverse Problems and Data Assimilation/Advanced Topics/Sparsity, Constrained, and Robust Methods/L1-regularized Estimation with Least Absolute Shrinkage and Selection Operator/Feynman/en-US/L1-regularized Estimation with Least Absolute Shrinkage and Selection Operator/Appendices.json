{
    "hands_on_practices": [
        {
            "introduction": "Before developing algorithms to solve the LASSO problem, it is essential to know how to verify a solution. This exercise provides direct practice with the core mathematics of non-smooth optimization by requiring you to apply the subgradient optimality condition. By manually calculating the subdifferential for a candidate solution, you will gain a concrete understanding of what mathematically defines a minimizer for the $\\ell_1$-regularized objective function. ",
            "id": "3394847",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in linear inverse modeling and data assimilation, which estimates a parameter vector $\\beta \\in \\mathbb{R}^{p}$ by minimizing the convex objective $f(\\beta) = \\tfrac{1}{2}\\|X\\beta - y\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$. Here, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $y \\in \\mathbb{R}^{n}$ is the observation vector, and $\\lambda  0$ is the regularization parameter. The necessary and sufficient optimality condition for convex problems is $0 \\in \\partial f(\\beta)$, where $\\partial f(\\beta)$ is the subdifferential of $f$ at $\\beta$. The subdifferential can be written as $\\partial f(\\beta) = X^{\\top}(X\\beta - y) + \\lambda \\,\\partial \\|\\beta\\|_{1}$, where $\\partial \\|\\beta\\|_{1}$ has components $z_{j} = \\operatorname{sign}(\\beta_{j})$ if $\\beta_{j} \\neq 0$ and $z_{j} \\in [-1,1]$ if $\\beta_{j} = 0$.\n\nGiven\n- $X = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix}$,\n- $y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}$,\n- $\\lambda = 1$,\n- a candidate $\\beta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$,\n\nperform the following:\n- Compute $r = X\\beta - y$ and $g(\\beta; z) = X^{\\top}r + \\lambda z$ for a subgradient $z \\in \\partial \\|\\beta\\|_{1}$ consistent with the given $\\beta$.\n- Verify whether the Karush–Kuhn–Tucker (KKT) stationarity condition $X^{\\top}(X\\beta - y) + \\lambda z = 0$ can hold for any valid $z$. If it does not hold, construct a feasible descent direction from the subdifferential by selecting the minimal-norm subgradient $g_{\\star} \\in \\partial f(\\beta)$, and define $d_{\\star} = -g_{\\star}$.\n- Finally, report the squared Euclidean norm $\\|g_{\\star}\\|_{2}^{2}$ as a single real number.\n\nNo rounding is necessary. Express the final result as a single real number without units.",
            "solution": "The LASSO objective function is $f(\\beta) = \\frac{1}{2}\\|X\\beta - y\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$. The optimality condition for $\\beta$ to be a minimizer is that the zero vector must be an element of the subdifferential $\\partial f(\\beta)$. The subdifferential is given by $\\partial f(\\beta) = X^{\\top}(X\\beta - y) + \\lambda \\partial \\|\\beta\\|_{1}$. The given data are:\n$$\nX = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}, \\quad \\lambda = 1\n$$\nand the candidate solution is\n$$\n\\beta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nFirst, we compute the residual vector $r = X\\beta - y$.\n$$\nX\\beta = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1) + (2)(0) \\\\ (0)(0) + (1)(1) + (-1)(0) \\\\ (1)(0) + (1)(1) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\nr = X\\beta - y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\nNext, we compute the gradient of the least-squares term, which is $g_{smooth} = X^{\\top}r$. The transpose of $X$ is\n$$\nX^{\\top} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  1 \\end{pmatrix}\n$$\nThus,\n$$\ng_{smooth} = X^{\\top}r = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (0)(1) + (1)(-1) \\\\ (0)(-1) + (1)(1) + (1)(-1) \\\\ (2)(-1) + (-1)(1) + (1)(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\\\ -4 \\end{pmatrix}\n$$\nNow, we must characterize the subdifferential of the $L_1$-norm, $\\partial \\|\\beta\\|_{1}$, at the given $\\beta$. The components of a vector $z \\in \\partial \\|\\beta\\|_{1}$ are given by $z_j = \\operatorname{sign}(\\beta_j)$ if $\\beta_j \\neq 0$ and $z_j \\in [-1, 1]$ if $\\beta_j = 0$.\nFor $\\beta = (0, 1, 0)^{\\top}$:\n- For $j=1$, $\\beta_1 = 0$, so $z_1 \\in [-1, 1]$.\n- For $j=2$, $\\beta_2 = 1$, so $z_2 = \\operatorname{sign}(1) = 1$.\n- For $j=3$, $\\beta_3 = 0$, so $z_3 \\in [-1, 1]$.\nA generic subgradient $g \\in \\partial f(\\beta)$ is therefore of the form $g = g_{smooth} + \\lambda z$, where $z = (z_1, 1, z_3)^{\\top}$ with $z_1, z_3 \\in [-1, 1]$. With $\\lambda = 1$:\n$$\ng = \\begin{pmatrix} -2 \\\\ 0 \\\\ -4 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} z_1 \\\\ 1 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} -2 + z_1 \\\\ 1 \\\\ -4 + z_3 \\end{pmatrix}\n$$\nTo verify the KKT stationarity condition, we must check if $0 \\in \\partial f(\\beta)$, which means we must check if there exists a valid $z$ such that $g = 0$.\nSetting $g = 0$ yields the system:\n1.  $-2 + z_1 = 0 \\implies z_1 = 2$\n2.  $1 = 0$\n3.  $-4 + z_3 = 0 \\implies z_3 = 4$\nThe second equation, $1=0$, is a contradiction. Furthermore, the required values for $z_1$ and $z_3$ (which are $2$ and $4$, respectively) are outside the required interval $[-1, 1]$. Thus, the KKT condition is not satisfied, and the candidate $\\beta$ is not an optimal solution.\n\nSince $\\beta$ is not optimal, we are asked to find the minimal-norm subgradient $g_{\\star} \\in \\partial f(\\beta)$. This is the element of the set $\\partial f(\\beta)$ with the smallest Euclidean norm. We must minimize $\\|g\\|_{2}^{2}$ with respect to the free parameters $z_1$ and $z_3$ over their valid intervals.\n$$\n\\|g\\|_{2}^{2} = (-2 + z_1)^2 + 1^2 + (-4 + z_3)^2\n$$\nThis minimization can be performed separately for each component.\nTo minimize $(-2 + z_1)^2$ subject to $z_1 \\in [-1, 1]$, we find the point in the interval $[-1, 1]$ that is closest to the unconstrained minimizer $z_1 = 2$. This point is $z_1 = 1$.\nTo minimize $(-4 + z_3)^2$ subject to $z_3 \\in [-1, 1]$, we find the point in the interval $[-1, 1]$ that is closest to the unconstrained minimizer $z_3 = 4$. This point is $z_3 = 1$.\nTherefore, the minimal-norm subgradient $g_{\\star}$ is obtained by setting $z_1 = 1$ and $z_3 = 1$:\n$$\ng_{\\star} = \\begin{pmatrix} -2 + 1 \\\\ 1 \\\\ -4 + 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -3 \\end{pmatrix}\n$$\nThe problem asks for the squared Euclidean norm of this minimal-norm subgradient, $\\|g_{\\star}\\|_{2}^{2}$.\n$$\n\\|g_{\\star}\\|_{2}^{2} = (-1)^2 + 1^2 + (-3)^2 = 1 + 1 + 9 = 11\n$$\nThis value represents the squared norm of the smallest subgradient, which is non-zero, confirming that the current point is not optimal. The negative of this subgradient, $d_{\\star} = -g_{\\star}$, would be a feasible descent direction.",
            "answer": "$$\n\\boxed{11}\n$$"
        },
        {
            "introduction": "A common and efficient method for solving LASSO is coordinate descent, which optimizes the objective one variable at a time. This practice guides you through implementing this algorithm while also investigating the crucial, and often overlooked, impact of feature scaling. You will discover how different normalization strategies can alter the solution path and final variable selection, bridging the gap between abstract optimization and practical data analysis. ",
            "id": "3394844",
            "problem": "Consider the linear inverse problem with measurements $y \\in \\mathbb{R}^m$ and forward model matrix $A \\in \\mathbb{R}^{m \\times n}$, where the goal is to estimate a sparse parameter vector $x \\in \\mathbb{R}^n$ by solving the Least Absolute Shrinkage and Selection Operator (LASSO), also known as $\\ell_1$-regularized least squares. The weighted LASSO objective is defined by the function\n$$\nJ(x) = \\tfrac{1}{2} \\lVert A x - y \\rVert_2^2 + \\lambda \\sum_{j=1}^n w_j \\lvert x_j \\rvert,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $w_j  0$ are penalty weights. In inverse problems and data assimilation, the geometry of the $\\ell_1$ ball and the scaling of columns of $A$ interact with the penalty weights $w_j$, which can encode physical units and prior information.\n\nLet $D = \\mathrm{diag}(d_1,\\dots,d_n)$ be a diagonal scaling applied to the columns of $A$, producing the scaled matrix $A' = A D$. Define the scaled variables $u \\in \\mathbb{R}^n$ by $x = D u$, so the measurement model remains $y \\approx A x = A D u = A' u$. If a penalty $\\lambda \\sum_{j=1}^n p_j \\lvert u_j \\rvert$ is applied in the scaled variables, then the corresponding physical weights in the original variables are $w_j = p_j / d_j$, and the Karush–Kuhn–Tucker (KKT) subgradient conditions for optimality in the original variables are\n$$\n\\begin{cases}\na_j^\\top (y - A x) = \\lambda w_j \\, \\mathrm{sign}(x_j),  \\text{if } x_j \\neq 0, \\\\\n\\lvert a_j^\\top (y - A x) \\rvert \\le \\lambda w_j,  \\text{if } x_j = 0,\n\\end{cases}\n$$\nwhere $a_j \\in \\mathbb{R}^m$ denotes the $j$-th column of $A$.\n\nYou are to examine the geometry of the LASSO solution under column scaling by $D$, propose normalization strategies aligned with physical units and prior information, and assess their impact on thresholding behavior implied by the KKT conditions.\n\nUse the following specific instance with $m = 6$ and $n = 3$:\n$$\nA = \\begin{bmatrix}\n10  1  0.2 \\\\\n8  -1  0.2 \\\\\n-5  2  0.2 \\\\\n2  -2  0.2 \\\\\n0  1  0.2 \\\\\n0  -1  0.2\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.1 \\\\ -0.2 \\\\ 0.95 \\\\ -1.1 \\\\ 0.6 \\\\ -0.6\n\\end{bmatrix}.\n$$\n\nImplement a solver that, for given $A$, $y$, $\\lambda$, and a chosen $(D, p)$ pair, computes an approximate minimizer $u^\\star$ of\n$$\n\\tfrac{1}{2} \\lVert A' u - y \\rVert_2^2 + \\lambda \\sum_{j=1}^n p_j \\lvert u_j \\rvert,\n$$\nmaps it back to $x^\\star = D u^\\star$, and evaluates the KKT marginal quantities in the original variables\n$$\nm_j = \\lvert a_j^\\top (y - A x^\\star) \\rvert - \\lambda w_j, \\quad \\text{where } w_j = p_j / d_j.\n$$\nThese margins quantify how close each coordinate is to crossing the threshold implied by the KKT conditions: if $x_j^\\star = 0$ then $m_j \\le 0$, and if $x_j^\\star \\neq 0$ then $m_j = 0$ in the limit of exact convergence.\n\nPropose and implement the following normalization strategies:\n- Raw, unit-agnostic: $D = \\mathrm{diag}(1,1,1)$ and $p = (1,1,1)$, which is the unscaled LASSO in original variables with equal weights $w_j = 1$.\n- Unit-norm scaling without unit-aware weights: $d_j = 1/\\lVert a_j \\rVert_2$ so each column of $A'$ has unit $\\ell_2$ norm, and set $p = (1,1,1)$, which implies original-variable weights $w_j = \\lVert a_j \\rVert_2$.\n- Unit-norm scaling with unit-aware weights: $d_j = 1/\\lVert a_j \\rVert_2$ and set $p_j = d_j$, which preserves original-variable weights $w_j = 1$ while working in a numerically balanced scaled space.\n- Physical prior scaling: let prior standard deviations be $\\sigma = (0.1, 1.0, 5.0)$ in the units of $x$, set $D = \\mathrm{diag}(\\sigma)$ and $p = (1,1,1)$, which implies $w_j = 1/\\sigma_j$ in original variables and nondimensionalizes the coefficients before regularization.\n\nUse coordinate descent with soft-thresholding consistent with the KKT subgradient conditions to compute $u^\\star$ for each case. The solver must terminate when the maximum absolute change of $u$ over a full cycle is less than $10^{-12}$ or after at most $5000$ iterations, whichever comes first. Use $0$-based indexing for reporting support sets.\n\nTest suite:\n- Case 1 (happy path): Raw strategy with $\\lambda = 0.2$.\n- Case 2 (bias against large-norm columns): Unit-norm scaling without unit-aware weights with $\\lambda = 0.2$.\n- Case 3 (threshold invariance under scaling): Unit-norm scaling with unit-aware weights with $\\lambda = 0.2$.\n- Case 4 (physically informed prior): Physical prior scaling with $\\lambda = 0.2$ and $\\sigma = (0.1, 1.0, 5.0)$.\n- Case 5 (boundary condition): Raw strategy with a large $\\lambda = 10.0$.\n\nFor each test case, your program must return a result consisting of:\n- The support set of indices $j$ where $\\lvert x_j^\\star \\rvert  10^{-8}$.\n- The list of KKT margins $m_j$ for $j = 0,1,2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,...]\"), where each result is a list of the form \"[support_list,margin_list]\". No physical units are required in the output; all values are pure numbers.",
            "solution": "The core of the problem is to solve the weighted LASSO objective function for a set of scaled variables $u \\in \\mathbb{R}^n$:\n$$\nJ(u) = \\frac{1}{2} \\lVert A' u - y \\rVert_2^2 + \\lambda \\sum_{j=1}^n p_j \\lvert u_j \\rvert\n$$\nwhere $A' = AD$ for a given diagonal scaling matrix $D = \\mathrm{diag}(d_1, \\dots, d_n)$, and $p = (p_1, \\dots, p_n)^\\top$ is a vector of penalty parameters in the scaled space. The solution in the original variables is recovered via $x = Du$.\n\nWe will employ the coordinate descent algorithm, an iterative method that is highly effective for this class of problems. The algorithm minimizes the objective function with respect to a single coordinate at a time, holding all other coordinates fixed. We cycle through all coordinates until the solution converges.\n\nLet us derive the update rule for the $k$-th coordinate, $u_k$. We fix $u_j$ for all $j \\neq k$ and minimize $J(u)$ with respect to $u_k$. The objective function can be written as a function of $u_k$ only:\n$$\nJ(u_k) = \\frac{1}{2} \\left\\lVert \\sum_{j \\neq k} a'_j u_j + a'_k u_k - y \\right\\rVert_2^2 + \\lambda p_k \\lvert u_k \\rvert + \\mathrm{const}\n$$\nwhere $a'_j$ is the $j$-th column of $A'$. To find the minimum, we compute the subgradient of $J(u_k)$ with respect to $u_k$ and set it to contain $0$. The subgradient is:\n$$\n\\partial_{u_k} J(u_k) = (a'_k)^\\top \\left( \\sum_{j \\neq k} a'_j u_j + a'_k u_k - y \\right) + \\lambda p_k \\partial \\lvert u_k \\rvert\n$$\nSetting this to $0$ yields the optimality condition:\n$$\n(a'_k)^\\top a'_k u_k - (a'_k)^\\top \\left( y - \\sum_{j \\neq k} a'_j u_j \\right) \\in -\\lambda p_k \\partial \\lvert u_k \\rvert = \\lambda p_k \\partial \\lvert -u_k \\rvert\n$$\nThis is equivalent to:\n$$\n(a'_k)^\\top a'_k u_k - (a'_k)^\\top \\left( y - \\sum_{j \\neq k} a'_j u_j \\right) \\in \\lambda p_k \\partial \\lvert u_k \\rvert\n$$\nLet $\\rho_k = (a'_k)^\\top (y - \\sum_{j \\neq k} a'_j u_j)$ and $C'_{kk} = (a'_k)^\\top a'_k = \\lVert a'_k \\rVert_2^2$. The condition becomes $C'_{kk} u_k - \\rho_k \\in \\lambda p_k \\partial \\lvert u_k \\rvert$. This is a standard problem whose solution is given by the soft-thresholding operator, $S_\\tau(z) = \\mathrm{sign}(z) \\max(\\lvert z \\rvert - \\tau, 0)$. The update for $u_k$ is therefore:\n$$\nu_k \\leftarrow \\frac{1}{C'_{kk}} S_{\\lambda p_k}(\\rho_k)\n$$\nFor computational efficiency, we can pre-compute the Gram matrix $C' = (A')^\\top A'$ and the vector $g'=(A')^\\top y$. Then, the term $\\rho_k$ can be calculated within the loop as $\\rho_k = g'_k - \\sum_{j \\neq k} C'_{kj} u_j$.\n\nThe algorithm proceeds as follows:\n1.  For a given strategy, define the matrices $D$ and vector $p$.\n2.  Compute the scaled matrix $A' = AD$.\n3.  Pre-compute $C' = (A')^\\top A'$ and $g' = (A')^\\top y$.\n4.  Initialize $u = (0, \\dots, 0)^\\top$.\n5.  Iterate, for each cycle:\n    a. Store the current solution: $u_{\\mathrm{old}} \\leftarrow u$.\n    b. For $k = 0, 1, \\dots, n-1$:\n        i. Compute $\\rho_k = g'_k - (\\sum_{j=0}^{n-1} C'_{kj} u_j - C'_{kk} u_k)$.\n        ii. Update $u_k \\leftarrow \\frac{1}{C'_{kk}} S_{\\lambda p_k}(\\rho_k)$.\n    c. Check for convergence: if $\\max_k |u_k - u_{\\mathrm{old},k}|  10^{-12}$, or if the maximum number of iterations ($5000$) is reached, terminate.\n6.  The final vector from the iteration is the optimal $u^\\star$.\n7.  Transform the solution back to the original parameter space: $x^\\star = D u^\\star$.\n8.  Determine the active set (support) of $x^\\star$ as indices $j$ where $\\lvert x_j^\\star \\rvert  10^{-8}$.\n9.  Evaluate the KKT marginals to verify optimality. The effective weights in the original space are $w_j = p_j / d_j$. The margins are $m_j = \\lvert a_j^\\top (y - A x^\\star) \\rvert - \\lambda w_j$. For $x_j^\\star \\neq 0$, we expect $m_j \\approx 0$. For $x_j^\\star = 0$, we expect $m_j \\le 0$.\n\nThis procedure is implemented for each of the five test cases specified.\n- The **Raw** strategy uses $D=I$ and $p=(1,1,1)^\\top$, corresponding to the standard LASSO on original variables with uniform weights $w_j=1$.\n- The **Unit-norm scaling without unit-aware weights** strategy sets $d_j = 1/\\lVert a_j \\rVert_2$ and $p=(1,1,1)^\\top$. This results in weights $w_j = \\lVert a_j \\rVert_2$, penalizing columns with larger norms more heavily.\n- The **Unit-norm scaling with unit-aware weights** strategy also sets $d_j=1/\\lVert a_j \\rVert_2$ but chooses $p_j = d_j$. This yields $w_j=1$, making the objective function identical to the Raw strategy but solved in a better-conditioned space. The resulting $x^\\star$ should be the same as in the Raw case.\n- The **Physical prior scaling** strategy uses $D=\\mathrm{diag}(\\sigma)$ and $p=(1,1,1)^\\top$. This imposes weights $w_j=1/\\sigma_j$, penalizing variables with smaller prior variance more heavily.\n- The final case demonstrates the effect of a large regularization parameter $\\lambda$, which is expected to shrink all coefficients to zero.\n\nThe following Python program implements this logic.",
            "answer": "```python\nimport numpy as np\n\ndef coordinate_descent(A, y, lambda_val, D, p, tol=1e-12, max_iter=5000):\n    \"\"\"\n    Solves the scaled LASSO problem using coordinate descent.\n\n    J(u) = 0.5 * ||A'u - y||^2 + lambda * sum(p_j * |u_j|)\n    where A' = A @ D.\n\n    Args:\n        A (np.ndarray): Original model matrix.\n        y (np.ndarray): Measurement vector.\n        lambda_val (float): Regularization parameter.\n        D (np.ndarray): Diagonal scaling matrix.\n        p (np.ndarray): Penalty weights for scaled variables.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The optimal scaled parameter vector u_star.\n    \"\"\"\n    m, n = A.shape\n    A_prime = A @ D\n    \n    # Pre-compute Gram matrix and correlation vector\n    C_prime = A_prime.T @ A_prime\n    g_prime = A_prime.T @ y\n    \n    u = np.zeros(n)\n    \n    # Diagonal elements of C_prime are needed for update\n    C_prime_diag = np.diag(C_prime)\n\n    for i in range(max_iter):\n        u_old = u.copy()\n        \n        for k in range(n):\n            # Calculate rho_k = g'_k - sum_{j!=k} C'_{kj} u_j\n            rho_k = g_prime[k] - (C_prime[k, :] @ u - C_prime[k, k] * u[k])\n            \n            # Soft-thresholding\n            threshold = lambda_val * p[k]\n            \n            if abs(rho_k) = threshold:\n                u[k] = 0.0\n            else:\n                u[k] = np.sign(rho_k) * (abs(rho_k) - threshold) / C_prime_diag[k]\n        \n        # Check for convergence\n        max_change = np.max(np.abs(u - u_old))\n        if max_change  tol:\n            break\n            \n    return u\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define problem data\n    A = np.array([\n        [10.0, 1.0, 0.2],\n        [8.0, -1.0, 0.2],\n        [-5.0, 2.0, 0.2],\n        [2.0, -2.0, 0.2],\n        [0.0, 1.0, 0.2],\n        [0.0, -1.0, 0.2]\n    ])\n    y = np.array([1.1, -0.2, 0.95, -1.1, 0.6, -0.6])\n    n = A.shape[1]\n\n    # Pre-calculate column norms for scaling strategies\n    col_norms = np.linalg.norm(A, axis=0)\n\n    # Define the 5 test cases\n    test_params = [\n        # Case 1: Raw\n        {'lambda': 0.2, 'D_diag': np.array([1.0, 1.0, 1.0]), 'p': np.array([1.0, 1.0, 1.0])},\n        # Case 2: Unit-norm scaling without unit-aware weights\n        {'lambda': 0.2, 'D_diag': 1.0 / col_norms, 'p': np.array([1.0, 1.0, 1.0])},\n        # Case 3: Unit-norm scaling with unit-aware weights\n        {'lambda': 0.2, 'D_diag': 1.0 / col_norms, 'p': 1.0 / col_norms},\n        # Case 4: Physical prior scaling\n        {'lambda': 0.2, 'D_diag': np.array([0.1, 1.0, 5.0]), 'p': np.array([1.0, 1.0, 1.0])},\n        # Case 5: Raw, large lambda\n        {'lambda': 10.0, 'D_diag': np.array([1.0, 1.0, 1.0]), 'p': np.array([1.0, 1.0, 1.0])},\n    ]\n\n    results = []\n    \n    for params in test_params:\n        lambda_val = params['lambda']\n        D_diag = params['D_diag']\n        D = np.diag(D_diag)\n        p = params['p']\n\n        # Solve for u_star\n        u_star = coordinate_descent(A, y, lambda_val, D, p)\n        \n        # Map back to x_star\n        x_star = D @ u_star\n        \n        # Calculate effective weights w\n        w = p / D_diag\n        \n        # Calculate KKT margins m_j\n        residual = y - A @ x_star\n        margins = [np.abs(A[:, j].T @ residual) - lambda_val * w[j] for j in range(n)]\n        \n        # Determine support set\n        support = np.where(np.abs(x_star)  1e-8)[0].tolist()\n        \n        results.append([support, margins])\n\n    # Custom formatter to match the strict output format without spaces\n    def format_list_custom(lst):\n        if isinstance(lst, list):\n            return f\"[{','.join(map(format_list_custom, lst))}]\"\n        elif isinstance(lst, np.ndarray):\n             return f\"[{','.join(map(format_list_custom, lst.tolist()))}]\"\n        else:\n            return repr(lst)\n\n    # Use the default str representation as guided by the boilerplate\n    # The boilerplate print statement is very specific.\n    # print(f\"[{','.join(map(str, results))}]\") will produce spaces: '[[...], [...]]'\n    # The example output suggests no spaces. Let's build it manually.\n    result_strings = []\n    for res in results:\n        support_str = f'[{\",\".join(map(str, res[0]))}]'\n        margins_str = f'[{\",\".join(map(str, res[1]))}]'\n        result_strings.append(f'[{support_str},{margins_str}]')\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "For more complex or large-scale inverse problems, the Alternating Direction Method of Multipliers (ADMM) offers a powerful and flexible framework. This practice demystifies this advanced algorithm by breaking it down into its core components: a variable split, a quadratic subproblem, a proximal update, and a dual variable update. Implementing a single ADMM iteration from scratch will equip you with foundational knowledge of a technique central to modern computational data science. ",
            "id": "3394893",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) inverse problem that seeks an estimate $\\;x \\in \\mathbb{R}^n\\;$ of an unknown signal from linear observations $\\;y \\in \\mathbb{R}^m\\;$ under a known forward operator $\\;H \\in \\mathbb{R}^{m \\times n}\\;$ and a sparsifying transform $\\;W \\in \\mathbb{R}^{p \\times n}\\;$ through the objective\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\lambda \\left\\| W x \\right\\|_1,\n$$\nwhere $\\;\\lambda  0\\;$ is the regularization parameter. This fits within inverse problems and data assimilation, balancing data fidelity and sparsity-promoting regularization.\n\nUsing the Alternating Direction Method of Multipliers (ADMM), with the split variable $\\;z = W x\\;$ and the scaled dual variable $\\;u\\;$, the algorithm iterates over three steps per iteration: the $\\;x\\;$-update minimizing a strictly convex quadratic subproblem, the $\\;z\\;$-update minimizing a strictly convex separable subproblem, and the dual update for $\\;u\\;$. The penalty parameter is denoted by $\\;\\rho  0\\;$. The implementation must obtain the $\\;x\\;$-update by solving the first-order optimality condition of the quadratic subproblem, the $\\;z\\;$-update by applying the soft-thresholding proximity operator associated with the $\\;\\ell_1\\;$ norm, and the $\\;u\\;$-update by the standard scaled dual update. Only a single ADMM iteration is required for each test case.\n\nYour task is to implement a complete, runnable program that performs exactly one ADMM iteration for each of the following test cases. For each case, the initial values are $\\;x^{(0)} = 0\\;$, $\\;z^{(0)} = 0\\;$, and $\\;u^{(0)} = 0\\;$ of appropriate dimensions. There are no physical units and no angle units involved. The final result for each case should be the vector $\\;x^{(1)}\\;$ obtained after the single ADMM iteration.\n\nTest Suite:\n- Case $1$ (happy path, well-conditioned, identity sparsifier):\n  - $n = 3$, $m = 3$, $p = 3$,\n  - $H = \\begin{bmatrix} 2  -1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$,\n  - $W = I_3$,\n  - $y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0.5 \\end{bmatrix}$,\n  - $\\lambda = 0.2$,\n  - $\\rho = 1.0$.\n- Case $2$ (ill-conditioned forward operator, difference sparsifier):\n  - $n = 3$, $m = 4$, $p = 2$,\n  - $H = \\begin{bmatrix} 1  1  0.99 \\\\ 0  1  1.01 \\\\ 1  2  2.01 \\\\ 0.5  1  1.005 \\end{bmatrix}$,\n  - $W = \\begin{bmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 0.5 \\\\ -0.2 \\\\ 1.5 \\\\ 0.3 \\end{bmatrix}$,\n  - $\\lambda = 0.5$,\n  - $\\rho = 0.8$.\n- Case $3$ (boundary case $\\lambda = 0$, identity sparsifier):\n  - $n = 3$, $m = 2$, $p = 3$,\n  - $H = \\begin{bmatrix} 1  0  -1 \\\\ 0  2  1 \\end{bmatrix}$,\n  - $W = I_3$,\n  - $y = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.0$,\n  - $\\rho = 1.2$.\n- Case $4$ (large penalty parameter $\\rho$, identity sparsifier):\n  - $n = 3$, $m = 3$, $p = 3$,\n  - $H = \\begin{bmatrix} 3  0  1 \\\\ 0  4  -1 \\\\ 1  -1  2 \\end{bmatrix}$,\n  - $W = I_3$,\n  - $y = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0.5 \\end{bmatrix}$,\n  - $\\lambda = 1.0$,\n  - $\\rho = 100.0$.\n\nAlgorithmic requirements:\n- Implement one ADMM iteration using the variable split $\\;z = W x\\;$ and the scaled dual $\\;u\\;$.\n- For the $\\;x\\;$-update, solve the first-order optimality condition of the quadratic subproblem using a robust linear solver that attempts a direct solve and falls back to a pseudoinverse if necessary.\n- For the $\\;z\\;$-update, apply soft-thresholding with threshold $\\;\\lambda / \\rho\\;$.\n- For the $\\;u\\;$-update, use the scaled dual update $\\;u \\leftarrow u + W x - z\\;$.\n- Return only $\\;x^{(1)}\\;$ for each case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is the list representation of $\\;x^{(1)}\\;$ for the corresponding case with components in decimal form, for example $\\;[[x_{1,1},x_{1,2},x_{1,3}],[x_{2,1},x_{2,2},x_{2,3}],\\ldots]\\;$.\n\nEach test case's final answer must be a list of floats. The program must be self-contained and require no user input.",
            "solution": "The problem requires the implementation of a single iteration of the Alternating Direction Method of Multipliers (ADMM) for solving the LASSO optimization problem. The solution must be derived methodically from first principles.\n\nThe objective function to minimize is:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; f(x) + g(Wx)\n$$\nwhere $f(x) = \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2$ is the data fidelity term (a smooth convex function) and $g(z) = \\lambda \\left\\| z \\right\\|_1$ is the regularization term (a non-smooth convex function). The matrices $H \\in \\mathbb{R}^{m \\times n}$ and $W \\in \\mathbb{R}^{p \\times n}$, the vector $y \\in \\mathbb{R}^m$, and the regularization parameter $\\lambda  0$ are given.\n\nTo apply ADMM, we introduce a splitting variable $z \\in \\mathbb{R}^p$ and reformulate the problem with an equality constraint:\n$$\n\\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\lambda \\left\\| z \\right\\|_1 \\quad \\text{subject to} \\quad Wx - z = 0.\n$$\nThe augmented Lagrangian for this constrained problem, in its scaled form, is given by:\n$$\nL_{\\rho}(x, z, u) = \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\lambda \\left\\| z \\right\\|_1 + \\frac{\\rho}{2} \\left\\| Wx - z + u \\right\\|_2^2 - \\frac{\\rho}{2} \\left\\| u \\right\\|_2^2\n$$\nwhere $\\rho  0$ is the penalty parameter and $u \\in \\mathbb{R}^p$ is the scaled dual variable.\n\nAn ADMM iteration, indexed by $k$, consists of three sequential minimization/update steps:\n1.  $x$-update: $x^{(k+1)} = \\arg\\min_{x} L_{\\rho}(x, z^{(k)}, u^{(k)})$\n2.  $z$-update: $z^{(k+1)} = \\arg\\min_{z} L_{\\rho}(x^{(k+1)}, z, u^{(k)})$\n3.  $u$-update: $u^{(k+1)} = u^{(k)} + Wx^{(k+1)} - z^{(k+1)}$\n\nThe problem specifies starting conditions $x^{(0)}=0$, $z^{(0)}=0$, and $u^{(0)}=0$, and asks for the computation of $x^{(1)}$ after a single complete iteration.\n\n**Step 1: Derivation of the $x$-update**\n\nThe $x$-subproblem at iteration $k+1$ is:\n$$\nx^{(k+1)} = \\arg\\min_{x} \\left( \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| Wx - z^{(k)} + u^{(k)} \\right\\|_2^2 \\right).\n$$\nThis is an unconstrained quadratic minimization problem. The first-order optimality condition is found by setting the gradient with respect to $x$ to zero:\n$$\n\\nabla_x \\left( \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| Wx - z^{(k)} + u^{(k)} \\right\\|_2^2 \\right) = 0\n$$\n$$\nH^T(Hx - y) + \\rho W^T(Wx - z^{(k)} + u^{(k)}) = 0\n$$\n$$\n(H^T H + \\rho W^T W) x = H^T y + \\rho W^T (z^{(k)} - u^{(k)})\n$$\nFor the first iteration ($k=0$), we use $z^{(0)}=0$ and $u^{(0)}=0$. The equation for $x^{(1)}$ simplifies to:\n$$\n(H^T H + \\rho W^T W) x^{(1)} = H^T y\n$$\nThis is a linear system of the form $Ax=b$, where $A = H^T H + \\rho W^T W$ and $b = H^T y$. The matrix $A$ is symmetric and positive semi-definite. A solution is obtained by solving this linear system. If $A$ is singular, the pseudo-inverse is used as specified.\n\n**Step 2: Derivation of the $z$-update**\n\nThe $z$-subproblem at iteration $k+1$ is:\n$$\nz^{(k+1)} = \\arg\\min_{z} \\left( \\lambda \\left\\| z \\right\\|_1 + \\frac{\\rho}{2} \\left\\| Wx^{(k+1)} - z + u^{(k)} \\right\\|_2^2 \\right)\n$$\nThis can be rewritten as:\n$$\nz^{(k+1)} = \\arg\\min_{z} \\left( \\frac{1}{2} \\left\\| z - (Wx^{(k+1)} + u^{(k)}) \\right\\|_2^2 + \\frac{\\lambda}{\\rho} \\left\\| z \\right\\|_1 \\right)\n$$\nThis is the definition of the proximal operator of the $\\ell_1$-norm, which is solved by the element-wise soft-thresholding operator $S_{\\kappa}(\\cdot)$:\n$$\nz^{(k+1)} = S_{\\lambda/\\rho} (Wx^{(k+1)} + u^{(k)})\n$$\nThe soft-thresholding function is defined as $S_{\\kappa}(a) = \\text{sign}(a) \\max(|a| - \\kappa, 0)$.\nFor the first iteration ($k=0$), using $u^{(0)}=0$, the update is:\n$$\nz^{(1)} = S_{\\lambda/\\rho} (Wx^{(1)})\n$$\n\n**Step 3: Derivation of the $u$-update**\n\nThe dual update is a standard gradient ascent step on the dual problem:\n$$\nu^{(k+1)} = u^{(k)} + Wx^{(k+1)} - z^{(k+1)}\n$$\nFor the first iteration, this becomes:\n$$\nu^{(1)} = u^{(0)} + Wx^{(1)} - z^{(1)} = Wx^{(1)} - z^{(1)}\n$$\n\nThe procedure for each test case is to compute $x^{(1)}$ via the linear system solve. Although the problem requires implementing a full iteration (which includes calculating $z^{(1)}$ and $u^{(1)}$), only the vector $x^{(1)}$ is required for the final output. The values of $z^{(1)}$ and $u^{(1)}$ do not influence the calculation of $x^{(1)}$, which only depends on the results from the prior iteration ($k=0$).\n\nThe specific calculations for each test case will be performed numerically according to these derived formulas.\n- For Case $1$ (happy path): $H$ is square and well-conditioned, $W=I$. $A = H^TH + \\rho I$ will be positive definite.\n- For Case $2$ (ill-conditioned): $H$ is ill-conditioned, but the regularization term $\\rho W^T W$ helps stabilize the linear system for the $x$-update.\n- For Case $3$ ($\\lambda=0$): The problem reduces to a form of regularized least squares. The threshold $\\kappa = \\lambda/\\rho = 0$, so the soft-thresholding operation becomes an identity mapping, $S_0(v) = v$. With $y=0$, the solution $x^{(1)}$ is expected to be the zero vector.\n- For Case $4$ (large $\\rho$): The high penalty $\\rho$ will force the solution $x^{(1)}$ towards zero, as the term $\\frac{\\rho}{2}\\|Wx\\|_2^2$ will dominate the $x$-subproblem objective.\nThe implementation will follow these steps precisely.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef run_single_admm_iteration(H: np.ndarray, W: np.ndarray, y: np.ndarray, lambda_val: float, rho: float) - list[float]:\n    \"\"\"\n    Performs exactly one ADMM iteration for the LASSO problem.\n\n    The function implements the three updates (x, z, u) for a single iteration,\n    starting from zero-initialized variables, and returns the computed x vector.\n\n    Args:\n        H: The forward operator matrix.\n        W: The sparsifying transform matrix.\n        y: The observation vector.\n        lambda_val: The regularization parameter lambda.\n        rho: The ADMM penalty parameter.\n\n    Returns:\n        The updated vector x^(1) as a list of floats.\n    \"\"\"\n    if not H.ndim == 2:\n        raise ValueError(\"H must be a 2D matrix.\")\n    if not W.ndim == 2:\n        raise ValueError(\"W must be a 2D matrix.\")\n    if not y.ndim == 1:\n        raise ValueError(\"y must be a 1D vector.\")\n    \n    n = H.shape[1]\n    p = W.shape[0]\n\n    # Initial values for iteration k=0\n    x0 = np.zeros(n)\n    z0 = np.zeros(p)\n    u0 = np.zeros(p)\n\n    # --- Start of ADMM iteration k=1 ---\n\n    # 1. x-update: x^(1) = argmin_x L_rho(x, z^(0), u^(0))\n    # This simplifies to solving the linear system:\n    # (H^T H + rho * W^T W) x^(1) = H^T y + rho * W^T (z^(0) - u^(0))\n    # Since z^(0) and u^(0) are zero, the system is:\n    # (H^T H + rho * W^T W) x^(1) = H^T y\n    \n    A = H.T @ H + rho * (W.T @ W)\n    b = H.T @ y\n    \n    x1 = None\n    try:\n        # Per the problem, attempt a direct solve first.\n        x1 = linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # If the matrix is singular, fall back to the pseudoinverse.\n        A_pinv = linalg.pinv(A)\n        x1 = A_pinv @ b\n\n    # 2. z-update: z^(1) = S_{lambda/rho}(W @ x^(1) + u^(0))\n    # The soft-thresholding operator S_kappa(v) is sign(v) * max(|v| - kappa, 0).\n    # Since u^(0) is zero:\n    v = W @ x1\n    kappa = lambda_val / rho\n\n    # Element-wise soft-thresholding\n    z1 = np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n    \n    # 3. u-update: u^(1) = u^(0) + W @ x^(1) - z^(1)\n    # Since u^(0) is zero:\n    u1 = W @ x1 - z1\n\n    # The problem requires implementing the full iteration (x, z, u updates),\n    # but only the resulting vector x^(1) is to be returned.\n    return x1.tolist()\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the ADMM iteration for each, and prints results.\n    \"\"\"\n    test_cases = [\n        # Case 1: happy path, well-conditioned, identity sparsifier\n        {\n            \"H\": np.array([[2., -1., 0.], [0., 1., 1.], [1., 0., 1.]]),\n            \"W\": np.identity(3),\n            \"y\": np.array([1., 2., 0.5]),\n            \"lambda_val\": 0.2,\n            \"rho\": 1.0\n        },\n        # Case 2: ill-conditioned forward operator, difference sparsifier\n        {\n            \"H\": np.array([[1., 1., 0.99], [0., 1., 1.01], [1., 2., 2.01], [0.5, 1., 1.005]]),\n            \"W\": np.array([[1., -1., 0.], [0., 1., -1.]]),\n            \"y\": np.array([0.5, -0.2, 1.5, 0.3]),\n            \"lambda_val\": 0.5,\n            \"rho\": 0.8\n        },\n        # Case 3: boundary case lambda = 0, identity sparsifier\n        {\n            \"H\": np.array([[1., 0., -1.], [0., 2., 1.]]),\n            \"W\": np.identity(3),\n            \"y\": np.array([0., 0.]),\n            \"lambda_val\": 0.0,\n            \"rho\": 1.2\n        },\n        # Case 4: large penalty parameter rho, identity sparsifier\n        {\n            \"H\": np.array([[3., 0., 1.], [0., 4., -1.], [1., -1., 2.]]),\n            \"W\": np.identity(3),\n            \"y\": np.array([1., -1., 0.5]),\n            \"lambda_val\": 1.0,\n            \"rho\": 100.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x1 = run_single_admm_iteration(\n            case[\"H\"],\n            case[\"W\"],\n            case[\"y\"],\n            case[\"lambda_val\"],\n            case[\"rho\"]\n        )\n        results.append(x1)\n\n    # The final print statement must match the specified format:\n    # [[...],[...],...]\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}