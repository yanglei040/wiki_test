{
    "hands_on_practices": [
        {
            "introduction": "在实现复杂的求解器之前，理解一个解究竟意味着什么是至关重要的。本练习聚焦于 LASSO 这类凸问题的充要最优性条件——卡罗需-库恩-塔克 (Karush-Kuhn-Tucker, KKT) 条件。通过为候选解手动检验这些条件，学习者将对次梯度的作用以及 LASSO 为何能将某些系数精确地驱动为零获得具体的理解。",
            "id": "3394847",
            "problem": "考虑在线性逆向建模和数据同化中的最小绝对收缩和选择算子 (LASSO) 问题，该问题通过最小化凸目标函数 $f(\\beta) = \\tfrac{1}{2}\\|X\\beta - y\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$ 来估计参数向量 $\\beta \\in \\mathbb{R}^{p}$。此处，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是观测向量，$\\lambda  0$ 是正则化参数。对于凸问题，其充要最优性条件是 $0 \\in \\partial f(\\beta)$，其中 $\\partial f(\\beta)$ 是 $f$ 在 $\\beta$ 处的次微分。次微分可以写作 $\\partial f(\\beta) = X^{\\top}(X\\beta - y) + \\lambda \\,\\partial \\|\\beta\\|_{1}$，其中 $\\partial \\|\\beta\\|_{1}$ 的分量为：若 $\\beta_{j} \\neq 0$，则 $z_{j} = \\operatorname{sign}(\\beta_{j})$；若 $\\beta_{j} = 0$，则 $z_{j} \\in [-1,1]$。\n\n给定\n- $X = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix}$，\n- $y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}$，\n- $\\lambda = 1$，\n- 一个候选解 $\\beta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$，\n\n请执行以下操作：\n- 对于一个与给定 $\\beta$ 一致的次梯度 $z \\in \\partial \\|\\beta\\|_{1}$，计算 $r = X\\beta - y$ 和 $g(\\beta; z) = X^{\\top}r + \\lambda z$。\n- 验证 Karush–Kuhn–Tucker (KKT) 平稳性条件 $X^{\\top}(X\\beta - y) + \\lambda z = 0$ 是否对任何有效的 $z$ 成立。若不成立，则通过选择最小范数次梯度 $g_{\\star} \\in \\partial f(\\beta)$ 从次微分构造一个可行下降方向，并定义 $d_{\\star} = -g_{\\star}$。\n- 最后，将欧几里得范数的平方 $\\|g_{\\star}\\|_{2}^{2}$ 作为一个实数报告。\n\n无需四舍五入。将最终结果表示为无单位的单个实数。",
            "solution": "该问题是适定的且科学上合理的，其理论基础是成熟的凸优化和正则化线性回归理论。所有必要信息都已提供，不存在矛盾。我们可以开始求解。\n\nLASSO 目标函数为 $f(\\beta) = \\frac{1}{2}\\|X\\beta - y\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$。$\\beta$ 成为最小化子的最优性条件是零向量必须是次微分 $\\partial f(\\beta)$ 中的一个元素。次微分由 $\\partial f(\\beta) = X^{\\top}(X\\beta - y) + \\lambda \\partial \\|\\beta\\|_{1}$ 给出。给定的数据是：\n$$\nX = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}, \\quad \\lambda = 1\n$$\n以及候选解是\n$$\n\\beta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n首先，我们计算残差向量 $r = X\\beta - y$。\n$$\nX\\beta = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1) + (2)(0) \\\\ (0)(0) + (1)(1) + (-1)(0) \\\\ (1)(0) + (1)(1) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\nr = X\\beta - y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n接下来，我们计算最小二乘项的梯度，即 $g_{smooth} = X^{\\top}r$。$X$ 的转置是\n$$\nX^{\\top} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  1 \\end{pmatrix}\n$$\n因此，\n$$\ng_{smooth} = X^{\\top}r = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (0)(1) + (1)(-1) \\\\ (0)(-1) + (1)(1) + (1)(-1) \\\\ (2)(-1) + (-1)(1) + (1)(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\\\ -4 \\end{pmatrix}\n$$\n现在，我们必须描述在给定 $\\beta$ 处 $L_1$-范数的次微分 $\\partial \\|\\beta\\|_{1}$。向量 $z \\in \\partial \\|\\beta\\|_{1}$ 的分量由以下方式给出：如果 $\\beta_j \\neq 0$，则 $z_j = \\operatorname{sign}(\\beta_j)$；如果 $\\beta_j = 0$，则 $z_j \\in [-1, 1]$。\n对于 $\\beta = (0, 1, 0)^{\\top}$：\n- 对于 $j=1$，$\\beta_1 = 0$，所以 $z_1 \\in [-1, 1]$。\n- 对于 $j=2$，$\\beta_2 = 1$，所以 $z_2 = \\operatorname{sign}(1) = 1$。\n- 对于 $j=3$，$\\beta_3 = 0$，所以 $z_3 \\in [-1, 1]$。\n因此，一个通用的次梯度 $g \\in \\partial f(\\beta)$ 具有 $g = g_{smooth} + \\lambda z$ 的形式，其中 $z = (z_1, 1, z_3)^{\\top}$ 且 $z_1, z_3 \\in [-1, 1]$。当 $\\lambda = 1$ 时：\n$$\ng = \\begin{pmatrix} -2 \\\\ 0 \\\\ -4 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} z_1 \\\\ 1 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} -2 + z_1 \\\\ 1 \\\\ -4 + z_3 \\end{pmatrix}\n$$\n为了验证 KKT 平稳性条件，我们必须检查是否有 $0 \\in \\partial f(\\beta)$，这意味着我们必须检查是否存在一个有效的 $z$ 使得 $g = 0$。\n令 $g = 0$ 得到方程组：\n1.  $-2 + z_1 = 0 \\implies z_1 = 2$\n2.  $1 = 0$\n3.  $-4 + z_3 = 0 \\implies z_3 = 4$\n第二个方程 $1=0$ 是一个矛盾。此外，$z_1$ 和 $z_3$ 所需的值（分别为 $2$ 和 $4$）都在要求的区间 $[-1, 1]$ 之外。因此，KKT 条件不满足，候选解 $\\beta$ 不是最优解。\n\n由于 $\\beta$ 不是最优的，我们被要求找到最小范数次梯度 $g_{\\star} \\in \\partial f(\\beta)$。这是集合 $\\partial f(\\beta)$ 中欧几里得范数最小的元素。我们必须在自由参数 $z_1$ 和 $z_3$ 的有效区间内最小化 $\\|g\\|_{2}^{2}$。\n$$\n\\|g\\|_{2}^{2} = (-2 + z_1)^2 + 1^2 + (-4 + z_3)^2\n$$\n这个最小化可以对每个分量分别进行。\n为了在 $z_1 \\in [-1, 1]$ 的约束下最小化 $(-2 + z_1)^2$，我们在区间 $[-1, 1]$ 中找到离无约束最小化子 $z_1 = 2$ 最近的点。这个点是 $z_1 = 1$。\n为了在 $z_3 \\in [-1, 1]$ 的约束下最小化 $(-4 + z_3)^2$，我们在区间 $[-1, 1]$ 中找到离无约束最小化子 $z_3 = 4$ 最近的点。这个点是 $z_3 = 1$。\n因此，通过设置 $z_1 = 1$ 和 $z_3 = 1$ 可以得到最小范数次梯度 $g_{\\star}$：\n$$\ng_{\\star} = \\begin{pmatrix} -2 + 1 \\\\ 1 \\\\ -4 + 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -3 \\end{pmatrix}\n$$\n问题要求计算这个最小范数次梯度的欧几里得范数的平方，即 $\\|g_{\\star}\\|_{2}^{2}$。\n$$\n\\|g_{\\star}\\|_{2}^{2} = (-1)^2 + 1^2 + (-3)^2 = 1 + 1 + 9 = 11\n$$\n这个值代表了最小次梯度的范数的平方，它不为零，这证实了当前点不是最优的。该次梯度的负值 $d_{\\star} = -g_{\\star}$ 将是一个可行的下降方向。",
            "answer": "$$\n\\boxed{11}\n$$"
        },
        {
            "introduction": "坐标下降法是求解 LASSO 问题的一种直观且高效的算法。本实践不仅仅是简单的实现，它还探讨了特征缩放的关键影响——这是数据同化中一个常见的挑战，因为变量可能具有迥异的物理单位。通过比较不同的缩放策略，你将为 LASSO 如何惩罚系数和选择变量建立更深刻的几何直觉。",
            "id": "3394844",
            "problem": "考虑一个线性逆问题，其测量值为 $y \\in \\mathbb{R}^m$，正演模型矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，目标是通过求解最小绝对收缩和选择算子 (LASSO)（也称为 $\\ell_1$ 正则化最小二乘）来估计一个稀疏参数向量 $x \\in \\mathbb{R}^n$。加权 LASSO 目标由以下函数定义\n$$\nJ(x) = \\tfrac{1}{2} \\lVert A x - y \\rVert_2^2 + \\lambda \\sum_{j=1}^n w_j \\lvert x_j \\rvert,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$w_j  0$ 是惩罚权重。在逆问题和数据同化中，$\\ell_1$ 球的几何形状以及 $A$ 的列缩放与惩罚权重 $w_j$ 相互作用，这些权重可以编码物理单位和先验信息。\n\n令 $D = \\mathrm{diag}(d_1,\\dots,d_n)$ 为应用于 $A$ 的列的对角缩放，生成缩放矩阵 $A' = A D$。通过 $x = D u$ 定义缩放变量 $u \\in \\mathbb{R}^n$，因此测量模型保持为 $y \\approx A x = A D u = A' u$。如果在缩放变量中应用惩罚 $\\lambda \\sum_{j=1}^n p_j \\lvert u_j \\rvert$，则原始变量中对应的物理权重为 $w_j = p_j / d_j$，并且原始变量的最优性 Karush–Kuhn–Tucker (KKT) 次梯度条件为\n$$\n\\begin{cases}\na_j^\\top (y - A x) = \\lambda w_j \\, \\mathrm{sign}(x_j),  \\text{if } x_j \\neq 0, \\\\\n\\lvert a_j^\\top (y - A x) \\rvert \\le \\lambda w_j,  \\text{if } x_j = 0,\n\\end{cases}\n$$\n其中 $a_j \\in \\mathbb{R}^m$ 表示 $A$ 的第 $j$ 列。\n\n您需要检验由 $D$ 进行列缩放后 LASSO 解的几何性质，提出与物理单位和先验信息对齐的归一化策略，并评估它们对 KKT 条件所蕴含的阈值行为的影响。\n\n使用以下 $m = 6$ 和 $n = 3$ 的具体实例：\n$$\nA = \\begin{bmatrix}\n10  1  0.2 \\\\\n8  -1  0.2 \\\\\n-5  2  0.2 \\\\\n2  -2  0.2 \\\\\n0  1  0.2 \\\\\n0  -1  0.2\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1.1 \\\\ -0.2 \\\\ 0.95 \\\\ -1.1 \\\\ 0.6 \\\\ -0.6\n\\end{bmatrix}.\n$$\n\n实现一个求解器，对于给定的 $A$、$y$、$\\lambda$ 和选定的 $(D, p)$ 对，计算下式的近似最小化子 $u^\\star$\n$$\n\\tfrac{1}{2} \\lVert A' u - y \\rVert_2^2 + \\lambda \\sum_{j=1}^n p_j \\lvert u_j \\rvert,\n$$\n将其映射回 $x^\\star = D u^\\star$，并评估原始变量中的 KKT 边际量\n$$\nm_j = \\lvert a_j^\\top (y - A x^\\star) \\rvert - \\lambda w_j, \\quad \\text{where } w_j = p_j / d_j.\n$$\n这些边际量化了每个坐标距离跨越 KKT 条件所蕴含的阈值有多近：如果 $x_j^\\star = 0$，则 $m_j \\le 0$；如果 $x_j^\\star \\neq 0$，则在精确收敛的极限情况下 $m_j = 0$。\n\n提出并实现以下归一化策略：\n- 原始，单位无关：$D = \\mathrm{diag}(1,1,1)$ 和 $p = (1,1,1)$，这是在原始变量上使用相等权重 $w_j = 1$ 的未缩放 LASSO。\n- 无单位感知权重的单位范数缩放：$d_j = 1/\\lVert a_j \\rVert_2$，使得 $A'$ 的每一列都具有单位 $\\ell_2$ 范数，并设置 $p = (1,1,1)$，这意味着原始变量的权重为 $w_j = \\lVert a_j \\rVert_2$。\n- 带单位感知权重的单位范数缩放：$d_j = 1/\\lVert a_j \\rVert_2$ 并设置 $p_j = d_j$，这在数值平衡的缩放空间中工作的同时保留了原始变量的权重 $w_j = 1$。\n- 物理先验缩放：设 $x$ 单位下的先验标准差为 $\\sigma = (0.1, 1.0, 5.0)$，设置 $D = \\mathrm{diag}(\\sigma)$ 和 $p = (1,1,1)$，这意味着原始变量中的权重为 $w_j = 1/\\sigma_j$，并在正则化前对系数进行无量纲化处理。\n\n对每种情况，使用与 KKT 次梯度条件一致的软阈值坐标下降法来计算 $u^\\star$。当 $u$ 在一个完整周期内的最大绝对变化小于 $10^{-12}$ 或达到最多 $5000$ 次迭代时，求解器必须终止，以先到者为准。报告支撑集时使用 0-基索引。\n\n测试套件：\n- 情况1（理想路径）：原始策略，$\\lambda = 0.2$。\n- 情况2（对大范数列的偏置）：无单位感知权重的单位范数缩放，$\\lambda = 0.2$。\n- 情况3（缩放下的阈值不变性）：带单位感知权重的单位范数缩放，$\\lambda = 0.2$。\n- 情况4（物理信息先验）：物理先验缩放，$\\lambda = 0.2$ 且 $\\sigma = (0.1, 1.0, 5.0)$。\n- 情况5（边界条件）：原始策略，使用较大的 $\\lambda = 10.0$。\n\n对于每个测试用例，您的程序必须返回一个结果，该结果包括：\n- 支撑集，即索引 $j$ 的集合，其中 $\\lvert x_j^\\star \\rvert  10^{-8}$。\n- $j=0,1,2$ 的 KKT 边际量 $m_j$ 列表。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,...]”），其中每个结果都是“[support_list,margin_list]”形式的列表。输出中不需要物理单位；所有值都是纯数字。",
            "solution": "问题陈述经过严格审查并被认定为有效。它在凸优化和正则化线性回归的既定理论（特别是最小绝对收缩和选择算子，即LASSO）中有科学依据。该问题是适定的，提供了所有必要的数据（$A$, $y$）、参数（$\\lambda$）以及四种不同缩放策略的定义。任务是客观的、可在数学上形式化的，并允许唯一、可验证的解。它没有矛盾、歧义或事实上的不健全之处。因此，我们可以着手解决。\n\n问题的核心是为一组缩放变量 $u \\in \\mathbb{R}^n$ 求解加权 LASSO 目标函数：\n$$\nJ(u) = \\frac{1}{2} \\lVert A' u - y \\rVert_2^2 + \\lambda \\sum_{j=1}^n p_j \\lvert u_j \\rvert\n$$\n其中 $A' = AD$，对于给定的对角缩放矩阵 $D = \\mathrm{diag}(d_1, \\dots, d_n)$，$p = (p_1, \\dots, p_n)^\\top$ 是缩放空间中的惩罚参数向量。原始变量中的解通过 $x = Du$ 恢复。\n\n我们将采用坐标下降算法，这是一种对此类问题非常有效的迭代方法。该算法一次只对一个坐标最小化目标函数，同时保持所有其他坐标固定。我们循环遍历所有坐标，直到解收敛为止。\n\n让我们推导第 $k$ 个坐标 $u_k$ 的更新规则。我们固定所有 $j \\neq k$ 的 $u_j$，并对 $u_k$ 最小化 $J(u)$。目标函数可以仅写成 $u_k$ 的函数：\n$$\nJ(u_k) = \\frac{1}{2} \\left\\lVert \\sum_{j \\neq k} a'_j u_j + a'_k u_k - y \\right\\rVert_2^2 + \\lambda p_k \\lvert u_k \\rvert + \\mathrm{const}\n$$\n其中 $a'_j$ 是 $A'$ 的第 $j$ 列。为了找到最小值，我们计算 $J(u_k)$ 关于 $u_k$ 的次梯度，并将其设置为包含 $0$。次梯度为：\n$$\n\\partial_{u_k} J(u_k) = (a'_k)^\\top \\left( \\sum_{j \\neq k} a'_j u_j + a'_k u_k - y \\right) + \\lambda p_k \\partial \\lvert u_k \\rvert\n$$\n将其设置为 $0$ 可得到最优性条件：\n$$\n(a'_k)^\\top a'_k u_k - (a'_k)^\\top \\left( y - \\sum_{j \\neq k} a'_j u_j \\right) \\in -\\lambda p_k \\partial \\lvert u_k \\rvert = \\lambda p_k \\partial \\lvert -u_k \\rvert\n$$\n这等价于：\n$$\n(a'_k)^\\top a'_k u_k - (a'_k)^\\top \\left( y - \\sum_{j \\neq k} a'_j u_j \\right) \\in \\lambda p_k \\partial \\lvert u_k \\rvert\n$$\n令 $\\rho_k = (a'_k)^\\top (y - \\sum_{j \\neq k} a'_j u_j)$ 且 $C'_{kk} = (a'_k)^\\top a'_k = \\lVert a'_k \\rVert_2^2$。条件变为 $C'_{kk} u_k - \\rho_k \\in \\lambda p_k \\partial \\lvert u_k \\rvert$。这是一个标准问题，其解由软阈值算子给出，$S_\\tau(z) = \\mathrm{sign}(z) \\max(\\lvert z \\rvert - \\tau, 0)$。因此 $u_k$ 的更新为：\n$$\nu_k \\leftarrow \\frac{1}{C'_{kk}} S_{\\lambda p_k}(\\rho_k)\n$$\n为了计算效率，我们可以预先计算 Gram 矩阵 $C' = (A')^\\top A'$ 和向量 $g'=(A')^\\top y$。然后，可以在循环内将项 $\\rho_k$ 计算为 $\\rho_k = g'_k - \\sum_{j \\neq k} C'_{kj} u_j$。\n\n算法流程如下：\n1.  对于给定的策略，定义矩阵 $D$ 和向量 $p$。\n2.  计算缩放矩阵 $A' = AD$。\n3.  预计算 $C' = (A')^\\top A'$ 和 $g' = (A')^\\top y$。\n4.  初始化 $u = (0, \\dots, 0)^\\top$。\n5.  迭代，每个周期：\n    a. 存储当前解：$u_{\\mathrm{old}} \\leftarrow u$。\n    b. 对于 $k = 0, 1, \\dots, n-1$：\n        i. 计算 $\\rho_k = g'_k - (\\sum_{j=0}^{n-1} C'_{kj} u_j - C'_{kk} u_k)$。\n        ii. 更新 $u_k \\leftarrow \\frac{1}{C'_{kk}} S_{\\lambda p_k}(\\rho_k)$。\n    c. 检查收敛性：如果 $\\max_k |u_k - u_{\\mathrm{old},k}|  10^{-12}$，或达到最大迭代次数（$5000$），则终止。\n6.  迭代得到的最终向量即为最优的 $u^\\star$。\n7.  将解变换回原始参数空间：$x^\\star = D u^\\star$。\n8.  将 $x^\\star$ 的活跃集（支撑集）确定为索引 $j$ 的集合，其中 $\\lvert x_j^\\star \\rvert  10^{-8}$。\n9.  评估 KKT 边际量以验证最优性。原始空间中的有效权重是 $w_j = p_j / d_j$。边际量是 $m_j = \\lvert a_j^\\top (y - A x^\\star) \\rvert - \\lambda w_j$。对于 $x_j^\\star \\neq 0$，我们期望 $m_j \\approx 0$。对于 $x_j^\\star = 0$，我们期望 $m_j \\le 0$。\n\n此过程将为指定的五个测试用例中的每一个实现。\n- **原始**策略使用 $D=I$ 和 $p=(1,1,1)^\\top$，对应于在原始变量上使用统一权重 $w_j=1$ 的标准 LASSO。\n- **无单位感知权重的单位范数缩放**策略设置 $d_j = 1/\\lVert a_j \\rVert_2$ 和 $p=(1,1,1)^\\top$。这导致权重 $w_j = \\lVert a_j \\rVert_2$，从而对范数较大的列施加更重的惩罚。\n- **带单位感知权重的单位范数缩放**策略同样设置 $d_j=1/\\lVert a_j \\rVert_2$ 但选择 $p_j = d_j$。这会产生 $w_j=1$，使得目标函数与原始策略相同，但在一个条件更好的空间中求解。得到的 $x^\\star$ 应与原始情况下的相同。\n- **物理先验缩放**策略使用 $D=\\mathrm{diag}(\\sigma)$ 和 $p=(1,1,1)^\\top$。这施加了权重 $w_j=1/\\sigma_j$，从而对先验方差较小的变量施加更重的惩罚。\n- 最后一个案例演示了大正则化参数 $\\lambda$ 的效果，预计它会将所有系数收缩到零。\n\n以下 Python 程序实现了这一逻辑。",
            "answer": "```python\nimport numpy as np\n\ndef coordinate_descent(A, y, lambda_val, D, p, tol=1e-12, max_iter=5000):\n    \"\"\"\n    Solves the scaled LASSO problem using coordinate descent.\n\n    J(u) = 0.5 * ||A'u - y||^2 + lambda * sum(p_j * |u_j|)\n    where A' = A @ D.\n\n    Args:\n        A (np.ndarray): Original model matrix.\n        y (np.ndarray): Measurement vector.\n        lambda_val (float): Regularization parameter.\n        D (np.ndarray): Diagonal scaling matrix.\n        p (np.ndarray): Penalty weights for scaled variables.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The optimal scaled parameter vector u_star.\n    \"\"\"\n    m, n = A.shape\n    A_prime = A @ D\n    \n    # Pre-compute Gram matrix and correlation vector\n    C_prime = A_prime.T @ A_prime\n    g_prime = A_prime.T @ y\n    \n    u = np.zeros(n)\n    \n    # Diagonal elements of C_prime are needed for update\n    C_prime_diag = np.diag(C_prime)\n\n    for i in range(max_iter):\n        u_old = u.copy()\n        \n        for k in range(n):\n            # Calculate rho_k = g'_k - sum_{j!=k} C'_{kj} u_j\n            rho_k = g_prime[k] - (C_prime[k, :] @ u - C_prime[k, k] * u[k])\n            \n            # Soft-thresholding\n            threshold = lambda_val * p[k]\n            \n            if abs(rho_k) = threshold:\n                u[k] = 0.0\n            else:\n                u[k] = np.sign(rho_k) * (abs(rho_k) - threshold) / C_prime_diag[k]\n        \n        # Check for convergence\n        max_change = np.max(np.abs(u - u_old))\n        if max_change  tol:\n            break\n            \n    return u\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define problem data\n    A = np.array([\n        [10.0, 1.0, 0.2],\n        [8.0, -1.0, 0.2],\n        [-5.0, 2.0, 0.2],\n        [2.0, -2.0, 0.2],\n        [0.0, 1.0, 0.2],\n        [0.0, -1.0, 0.2]\n    ])\n    y = np.array([1.1, -0.2, 0.95, -1.1, 0.6, -0.6])\n    n = A.shape[1]\n\n    # Pre-calculate column norms for scaling strategies\n    col_norms = np.linalg.norm(A, axis=0)\n\n    # Define the 5 test cases\n    test_params = [\n        # Case 1: Raw\n        {'lambda': 0.2, 'D_diag': np.array([1.0, 1.0, 1.0]), 'p': np.array([1.0, 1.0, 1.0])},\n        # Case 2: Unit-norm scaling without unit-aware weights\n        {'lambda': 0.2, 'D_diag': 1.0 / col_norms, 'p': np.array([1.0, 1.0, 1.0])},\n        # Case 3: Unit-norm scaling with unit-aware weights\n        {'lambda': 0.2, 'D_diag': 1.0 / col_norms, 'p': 1.0 / col_norms},\n        # Case 4: Physical prior scaling\n        {'lambda': 0.2, 'D_diag': np.array([0.1, 1.0, 5.0]), 'p': np.array([1.0, 1.0, 1.0])},\n        # Case 5: Raw, large lambda\n        {'lambda': 10.0, 'D_diag': np.array([1.0, 1.0, 1.0]), 'p': np.array([1.0, 1.0, 1.0])},\n    ]\n\n    results = []\n    \n    for params in test_params:\n        lambda_val = params['lambda']\n        D_diag = params['D_diag']\n        D = np.diag(D_diag)\n        p = params['p']\n\n        # Solve for u_star\n        u_star = coordinate_descent(A, y, lambda_val, D, p)\n        \n        # Map back to x_star\n        x_star = D @ u_star\n        \n        # Calculate effective weights w\n        w = p / D_diag\n        \n        # Calculate KKT margins m_j\n        residual = y - A @ x_star\n        margins = [np.abs(A[:, j].T @ residual) - lambda_val * w[j] for j in range(n)]\n        \n        # Determine support set\n        support = np.where(np.abs(x_star) > 1e-8)[0].tolist()\n        \n        results.append([support, margins])\n\n    # Custom formatter to match the strict output format without spaces\n    def format_list_custom(lst):\n        if isinstance(lst, list):\n            return f\"[{','.join(map(format_list_custom, lst))}]\"\n        elif isinstance(lst, np.ndarray):\n             return f\"[{','.join(map(format_list_custom, lst.tolist()))}]\"\n        else:\n            return repr(lst)\n\n    # Use the default str representation as guided by the boilerplate\n    # The boilerplate print statement is very specific.\n    # print(f\"[{','.join(map(str, results))}]\") will produce spaces: '[[...], [...]]'\n    # The example output suggests no spaces. Let's build it manually.\n    result_strings = []\n    for res in results:\n        support_str = f'[{\",\".join(map(str, res[0]))}]'\n        margins_str = f'[{\",\".join(map(str, res[1]))}]'\n        result_strings.append(f'[{support_str},{margins_str}]')\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "交替方向乘子法 (ADMM) 是一个强大且通用的框架，用于解决大规模和复杂的优化问题，包括反问题中常见的 LASSO 变体。本练习通过将单次迭代分解为其基本步骤——求解一个最小二乘问题和应用一个简单的收缩算子——来揭开 ADMM 的神秘面纱。这项实践为掌握现代数据同化中使用的关键优化工具之一提供了实用的切入点。",
            "id": "3394893",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 反问题，该问题旨在通过以下目标函数，从线性观测值 $\\;y \\in \\mathbb{R}^m\\;$、已知的正向算子 $\\;H \\in \\mathbb{R}^{m \\times n}\\;$ 和稀疏变换 $\\;W \\in \\mathbb{R}^{p \\times n}\\;$ 中求解未知信号的一个估计 $\\;x \\in \\mathbb{R}^n\\;$\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\lambda \\left\\| W x \\right\\|_1,\n$$\n其中 $\\;\\lambda  0\\;$ 是正则化参数。这属于反问题和数据同化的范畴，旨在平衡数据保真度和促进稀疏性的正则化。\n\n使用交替方向乘子法 (ADMM)，引入分裂变量 $\\;z = W x\\;$ 和缩放对偶变量 $\\;u\\;$，该算法在每次迭代中包含三个步骤：最小化一个严格凸二次子问题的 $\\;x\\;$-更新，最小化一个严格凸可分离子问题的 $\\;z\\;$-更新，以及对 $\\;u\\;$ 的对偶更新。惩罚参数用 $\\;\\rho  0\\;$ 表示。实现时必须通过求解二次子问题的一阶最优性条件来获得 $\\;x\\;$-更新，通过应用与 $\\;\\ell_1\\;$ 范数相关的软阈值邻近算子来获得 $\\;z\\;$-更新，以及通过标准的缩放对偶更新来获得 $\\;u\\;$-更新。每个测试用例仅需执行一次 ADMM 迭代。\n\n您的任务是实现一个完整的、可运行的程序，为以下每个测试用例执行恰好一次 ADMM 迭代。对于每个用例，初始值为适当维度的 $\\;x^{(0)} = 0\\;$、$\\;z^{(0)} = 0\\;$ 和 $\\;u^{(0)} = 0\\;$。不涉及物理单位和角度单位。每个用例的最终结果应为单次 ADMM 迭代后获得的向量 $\\;x^{(1)}\\;$。\n\n测试套件：\n- 用例 $1$ (理想情况，良态，单位稀疏变换):\n  - $n = 3$, $m = 3$, $p = 3$,\n  - $H = \\begin{bmatrix} 2  -1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$,\n  - $W = I_3$,\n  - $y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0.5 \\end{bmatrix}$,\n  - $\\lambda = 0.2$,\n  - $\\rho = 1.0$.\n- 用例 $2$ (病态正向算子，差分稀疏变换):\n  - $n = 3$, $m = 4$, $p = 2$,\n  - $H = \\begin{bmatrix} 1  1  0.99 \\\\ 0  1  1.01 \\\\ 1  2  2.01 \\\\ 0.5  1  1.005 \\end{bmatrix}$,\n  - $W = \\begin{bmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 0.5 \\\\ -0.2 \\\\ 1.5 \\\\ 0.3 \\end{bmatrix}$,\n  - $\\lambda = 0.5$,\n  - $\\rho = 0.8$.\n- 用例 $3$ (边界情况 $\\lambda = 0$，单位稀疏变换):\n  - $n = 3$, $m = 2$, $p = 3$,\n  - $H = \\begin{bmatrix} 1  0  -1 \\\\ 0  2  1 \\end{bmatrix}$,\n  - $W = I_3$,\n  - $y = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.0$,\n  - $\\rho = 1.2$.\n- 用例 $4$ (大惩罚参数 $\\rho$，单位稀疏变换):\n  - $n = 3$, $m = 3$, $p = 3$,\n  - $H = \\begin{bmatrix} 3  0  1 \\\\ 0  4  -1 \\\\ 1  -1  2 \\end{bmatrix}$,\n  - $W = I_3$,\n  - $y = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0.5 \\end{bmatrix}$,\n  - $\\lambda = 1.0$,\n  - $\\rho = 100.0$.\n\n算法要求：\n- 使用变量分裂 $\\;z = W x\\;$ 和缩放对偶 $\\;u\\;$ 实现一次 ADMM 迭代。\n- 对于 $\\;x\\;$-更新，使用一个鲁棒的线性求解器求解二次子问题的一阶最优性条件，该求解器会尝试直接求解，并在必要时回退到使用伪逆。\n- 对于 $\\;z\\;$-更新，应用阈值为 $\\;\\lambda / \\rho\\;$ 的软阈值处理。\n- 对于 $\\;u\\;$-更新，使用缩放对偶更新 $\\;u \\leftarrow u + W x - z\\;$。\n- 每个用例仅返回 $\\;x^{(1)}\\;$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个结果是对应案例的 $\\;x^{(1)}\\;$ 的列表表示，其分量为十进制形式，例如 $\\;[[x_{1,1},x_{1,2},x_{1,3}],[x_{2,1},x_{2,2},x_{2,3}],\\ldots]\\;$。\n\n每个测试用例的最终答案必须是一个浮点数列表。程序必须是自包含的，并且不需要用户输入。",
            "solution": "该问题要求为求解 LASSO 优化问题实现单次交替方向乘子法 (ADMM) 迭代。解法必须从第一性原理出发进行系统推导。\n\n需要最小化的目标函数是：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; f(x) + g(Wx)\n$$\n其中 $f(x) = \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2$ 是数据保真项（一个光滑凸函数），$g(z) = \\lambda \\left\\| z \\right\\|_1$ 是正则化项（一个非光滑凸函数）。给定了矩阵 $H \\in \\mathbb{R}^{m \\times n}$ 和 $W \\in \\mathbb{R}^{p \\times n}$，向量 $y \\in \\mathbb{R}^m$ 以及正则化参数 $\\lambda  0$。\n\n为了应用 ADMM，我们引入一个分裂变量 $z \\in \\mathbb{R}^p$，并将问题重构为一个带等式约束的形式：\n$$\n\\min_{x \\in \\mathbb{R}^n, z \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\lambda \\left\\| z \\right\\|_1 \\quad \\text{subject to} \\quad Wx - z = 0.\n$$\n这个约束问题的增广拉格朗日函数，以其缩放形式表示为：\n$$\nL_{\\rho}(x, z, u) = \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\lambda \\left\\| z \\right\\|_1 + \\frac{\\rho}{2} \\left\\| Wx - z + u \\right\\|_2^2 - \\frac{\\rho}{2} \\left\\| u \\right\\|_2^2\n$$\n其中 $\\rho  0$ 是惩罚参数，而 $u \\in \\mathbb{R}^p$ 是缩放对偶变量。\n\n一次 ADMM 迭代（以 $k$ 为索引）包含三个顺序的最小化/更新步骤：\n1.  $x$-更新： $x^{(k+1)} = \\arg\\min_{x} L_{\\rho}(x, z^{(k)}, u^{(k)})$\n2.  $z$-更新： $z^{(k+1)} = \\arg\\min_{z} L_{\\rho}(x^{(k+1)}, z, u^{(k)})$\n3.  $u$-更新： $u^{(k+1)} = u^{(k)} + Wx^{(k+1)} - z^{(k+1)}$\n\n问题指定了初始条件 $x^{(0)}=0$、$z^{(0)}=0$ 和 $u^{(0)}=0$，并要求计算一次完整迭代后的 $x^{(1)}$。\n\n**步骤1：推导 $x$-更新**\n\n第 $k+1$ 次迭代的 $x$-子问题是：\n$$\nx^{(k+1)} = \\arg\\min_{x} \\left( \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| Wx - z^{(k)} + u^{(k)} \\right\\|_2^2 \\right).\n$$\n这是一个无约束二次最小化问题。通过将关于 $x$ 的梯度设为零，可以找到一阶最优性条件：\n$$\n\\nabla_x \\left( \\frac{1}{2} \\left\\| H x - y \\right\\|_2^2 + \\frac{\\rho}{2} \\left\\| Wx - z^{(k)} + u^{(k)} \\right\\|_2^2 \\right) = 0\n$$\n$$\nH^T(Hx - y) + \\rho W^T(Wx - z^{(k)} + u^{(k)}) = 0\n$$\n$$\n(H^T H + \\rho W^T W) x = H^T y + \\rho W^T (z^{(k)} - u^{(k)})\n$$\n对于第一次迭代 ($k=0$)，我们使用 $z^{(0)}=0$ 和 $u^{(0)}=0$。$x^{(1)}$ 的方程简化为：\n$$\n(H^T H + \\rho W^T W) x^{(1)} = H^T y\n$$\n这是一个 $Ax=b$ 形式的线性系统，其中 $A = H^T H + \\rho W^T W$，$b = H^T y$。矩阵 $A$ 是对称半正定的。通过求解这个线性系统可以得到一个解。如果 $A$ 是奇异的，则按要求使用伪逆。\n\n**步骤2：推导 $z$-更新**\n\n第 $k+1$ 次迭代的 $z$-子问题是：\n$$\nz^{(k+1)} = \\arg\\min_{z} \\left( \\lambda \\left\\| z \\right\\|_1 + \\frac{\\rho}{2} \\left\\| Wx^{(k+1)} - z + u^{(k)} \\right\\|_2^2 \\right)\n$$\n这可以重写为：\n$$\nz^{(k+1)} = \\arg\\min_{z} \\left( \\frac{1}{2} \\left\\| z - (Wx^{(k+1)} + u^{(k)}) \\right\\|_2^2 + \\frac{\\lambda}{\\rho} \\left\\| z \\right\\|_1 \\right)\n$$\n这是 $\\ell_1$-范数的邻近算子的定义，它可以通过逐元素的软阈值算子 $S_{\\kappa}(\\cdot)$ 求解：\n$$\nz^{(k+1)} = S_{\\lambda/\\rho} (Wx^{(k+1)} + u^{(k)})\n$$\n软阈值函数定义为 $S_{\\kappa}(a) = \\text{sign}(a) \\max(|a| - \\kappa, 0)$。\n对于第一次迭代 ($k=0$)，使用 $u^{(0)}=0$，更新为：\n$$\nz^{(1)} = S_{\\lambda/\\rho} (Wx^{(1)})\n$$\n\n**步骤3：推导 $u$-更新**\n\n对偶更新是对偶问题上的标准梯度上升步骤：\n$$\nu^{(k+1)} = u^{(k)} + Wx^{(k+1)} - z^{(k+1)}\n$$\n对于第一次迭代，这变为：\n$$\nu^{(1)} = u^{(0)} + Wx^{(1)} - z^{(1)} = Wx^{(1)} - z^{(1)}\n$$\n\n每个测试用例的流程是通过求解线性系统来计算 $x^{(1)}$。尽管问题要求实现一次完整的迭代（包括计算 $z^{(1)}$ 和 $u^{(1)}$），但最终输出只需要向量 $x^{(1)}$。$z^{(1)}$ 和 $u^{(1)}$ 的值不影响 $x^{(1)}$ 的计算，后者仅依赖于上一次迭代 ($k=0$) 的结果。\n\n每个测试用例的具体计算将根据这些推导出的公式进行数值计算。\n- 对于用例1（理想情况）：$H$ 是方的且良态，$W=I$。$A = H^TH + \\rho I$ 将是正定的。\n- 对于用例2（病态）：$H$ 是病态的，但正则化项 $\\rho W^T W$ 有助于稳定用于 $x$-更新的线性系统。\n- 对于用例3（$\\lambda=0$）：问题简化为一种正则化最小二乘形式。阈值 $\\kappa = \\lambda/\\rho = 0$，因此软阈值操作变为恒等映射，$S_0(v) = v$。当 $y=0$ 时，解 $x^{(1)}$ 预计为零向量。\n- 对于用例4（大 $\\rho$）：高惩罚 $\\rho$ 将迫使解 $x^{(1)}$ 趋向于零，因为项 $\\frac{\\rho}{2}\\|Wx\\|_2^2$ 将在 $x$-子问题目标函数中占主导地位。\n实现将精确遵循这些步骤。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef run_single_admm_iteration(H: np.ndarray, W: np.ndarray, y: np.ndarray, lambda_val: float, rho: float) -> list[float]:\n    \"\"\"\n    Performs exactly one ADMM iteration for the LASSO problem.\n\n    The function implements the three updates (x, z, u) for a single iteration,\n    starting from zero-initialized variables, and returns the computed x vector.\n\n    Args:\n        H: The forward operator matrix.\n        W: The sparsifying transform matrix.\n        y: The observation vector.\n        lambda_val: The regularization parameter lambda.\n        rho: The ADMM penalty parameter.\n\n    Returns:\n        The updated vector x^(1) as a list of floats.\n    \"\"\"\n    if not H.ndim == 2:\n        raise ValueError(\"H must be a 2D matrix.\")\n    if not W.ndim == 2:\n        raise ValueError(\"W must be a 2D matrix.\")\n    if not y.ndim == 1:\n        raise ValueError(\"y must be a 1D vector.\")\n    \n    n = H.shape[1]\n    p = W.shape[0]\n\n    # Initial values for iteration k=0\n    x0 = np.zeros(n)\n    z0 = np.zeros(p)\n    u0 = np.zeros(p)\n\n    # --- Start of ADMM iteration k=1 ---\n\n    # 1. x-update: x^(1) = argmin_x L_rho(x, z^(0), u^(0))\n    # This simplifies to solving the linear system:\n    # (H^T H + rho * W^T W) x^(1) = H^T y + rho * W^T (z^(0) - u^(0))\n    # Since z^(0) and u^(0) are zero, the system is:\n    # (H^T H + rho * W^T W) x^(1) = H^T y\n    \n    A = H.T @ H + rho * (W.T @ W)\n    b = H.T @ y\n    \n    x1 = None\n    try:\n        # Per the problem, attempt a direct solve first.\n        x1 = linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # If the matrix is singular, fall back to the pseudoinverse.\n        A_pinv = linalg.pinv(A)\n        x1 = A_pinv @ b\n\n    # 2. z-update: z^(1) = S_{lambda/rho}(W @ x^(1) + u^(0))\n    # The soft-thresholding operator S_kappa(v) is sign(v) * max(|v| - kappa, 0).\n    # Since u^(0) is zero:\n    v = W @ x1\n    kappa = lambda_val / rho\n\n    # Element-wise soft-thresholding\n    z1 = np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n    \n    # 3. u-update: u^(1) = u^(0) + W @ x^(1) - z^(1)\n    # Since u^(0) is zero:\n    u1 = W @ x1 - z1\n\n    # The problem requires implementing the full iteration (x, z, u updates),\n    # but only the resulting vector x^(1) is to be returned.\n    return x1.tolist()\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the ADMM iteration for each, and prints results.\n    \"\"\"\n    test_cases = [\n        # Case 1: happy path, well-conditioned, identity sparsifier\n        {\n            \"H\": np.array([[2., -1., 0.], [0., 1., 1.], [1., 0., 1.]]),\n            \"W\": np.identity(3),\n            \"y\": np.array([1., 2., 0.5]),\n            \"lambda_val\": 0.2,\n            \"rho\": 1.0\n        },\n        # Case 2: ill-conditioned forward operator, difference sparsifier\n        {\n            \"H\": np.array([[1., 1., 0.99], [0., 1., 1.01], [1., 2., 2.01], [0.5, 1., 1.005]]),\n            \"W\": np.array([[1., -1., 0.], [0., 1., -1.]]),\n            \"y\": np.array([0.5, -0.2, 1.5, 0.3]),\n            \"lambda_val\": 0.5,\n            \"rho\": 0.8\n        },\n        # Case 3: boundary case lambda = 0, identity sparsifier\n        {\n            \"H\": np.array([[1., 0., -1.], [0., 2., 1.]]),\n            \"W\": np.identity(3),\n            \"y\": np.array([0., 0.]),\n            \"lambda_val\": 0.0,\n            \"rho\": 1.2\n        },\n        # Case 4: large penalty parameter rho, identity sparsifier\n        {\n            \"H\": np.array([[3., 0., 1.], [0., 4., -1.], [1., -1., 2.]]),\n            \"W\": np.identity(3),\n            \"y\": np.array([1., -1., 0.5]),\n            \"lambda_val\": 1.0,\n            \"rho\": 100.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x1 = run_single_admm_iteration(\n            case[\"H\"],\n            case[\"W\"],\n            case[\"y\"],\n            case[\"lambda_val\"],\n            case[\"rho\"]\n        )\n        results.append(x1)\n\n    # The final print statement must match the specified format:\n    # [[...],[...],...]\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}