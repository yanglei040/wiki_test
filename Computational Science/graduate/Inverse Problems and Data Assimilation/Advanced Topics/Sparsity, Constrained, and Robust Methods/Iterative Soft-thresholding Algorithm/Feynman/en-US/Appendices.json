{
    "hands_on_practices": [
        {
            "introduction": "The Iterative Soft-Thresholding Algorithm (ISTA) solves complex optimization problems through a simple, repeated two-step process: a standard gradient descent step followed by a special \"correction\" step. This first practice focuses squarely on that correction step, which is performed by the soft-thresholding operator, the core engine driving sparsity in the solution. By working through this fundamental calculation, you will gain a concrete understanding of how the algorithm selectively shrinks coefficients or eliminates them entirely, which is the key mechanism behind its effectiveness in sparse recovery .",
            "id": "3392980",
            "problem": "Consider the canonical sparse linear inverse problem formulated as minimizing the sum of a quadratic data misfit and an elementwise sparsity-promoting penalty: find $x \\in \\mathbb{R}^{5}$ that minimizes $f(x) + g(x)$, where $f(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. In the iterative soft-thresholding algorithm (ISTA), a forward (gradient) step on $f$ at an iterate $x^{k}$ with step size $1/L$ is followed by a proximal step of $g$. Suppose that at a certain iteration, after the forward step, the intermediate vector is $u \\in \\mathbb{R}^{5}$, and the scalar threshold used in the proximal step is $\\tau  0$. The proximal map of the scaled $\\ell_{1}$-norm at $u$ is denoted by $S_{\\tau}(u)$.\n\nFor the specific values $u = (\\,3,\\,-1,\\,\\tfrac{3}{2},\\,-4,\\,\\tfrac{1}{2}\\,)^{\\top} \\in \\mathbb{R}^{5}$ and $\\tau = \\tfrac{3}{2}$, compute $S_{\\tau}(u)$ exactly, and explain, on the basis of first principles from convex analysis and properties of the proximal operator, which entries are set to zero and which are shrunk (and in what direction). Express your final answer as a single row vector with exact rational entries. Do not round.",
            "solution": "The problem is valid. It is well-posed, scientifically grounded in the principles of convex optimization and inverse problems, and provides all necessary information for a unique, verifiable solution.\n\nThe problem asks for the computation of the proximal operator of the scaled $\\ell_1$-norm, commonly known as the soft-thresholding operator, denoted by $S_{\\tau}(u)$. This operator is central to the Iterative Soft-Thresholding Algorithm (ISTA) used for solving sparse optimization problems.\n\nFrom first principles of convex analysis, the proximal operator of a function $h(x)$ with a scaling parameter $\\gamma  0$ applied to a vector $v$ is defined as the unique minimizer of the following objective function:\n$$\n\\text{prox}_{\\gamma h}(v) = \\arg\\min_{z} \\left( \\gamma h(z) + \\frac{1}{2} \\|z - v\\|_{2}^{2} \\right)\n$$\nIn the context of this problem, the function is the $\\ell_1$-norm, and the operator is denoted $S_{\\tau}(u)$. This implies we are solving for $x = S_{\\tau}(u)$ where:\n$$\nx = \\arg\\min_{x \\in \\mathbb{R}^{5}} \\left( \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2} \\right)\n$$\nThe objective function to be minimized is $J(x) = \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2}$. A key property of this objective function is its separability across the components of the vector $x$. We can rewrite the norms as sums over the components:\n$$\nJ(x) = \\tau \\sum_{i=1}^{5} |x_i| + \\frac{1}{2} \\sum_{i=1}^{5} (x_i - u_i)^2 = \\sum_{i=1}^{5} \\left( \\tau |x_i| + \\frac{1}{2} (x_i - u_i)^2 \\right)\n$$\nSince the overall minimization problem is a sum of terms that each depend on only one component $x_i$, we can minimize the entire sum by minimizing each term independently for each component $i \\in \\{1, 2, 3, 4, 5\\}$. Thus, for each $i$, we solve the scalar minimization problem:\n$$\nx_i = \\arg\\min_{\\xi \\in \\mathbb{R}} \\left( \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2 \\right)\n$$\nLet $J_i(\\xi) = \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2$. Since $J_i(\\xi)$ is a convex function, its minimum is attained where its subgradient contains zero. The subdifferential of $J_i$ at $\\xi$ is given by:\n$$\n\\partial J_i(\\xi) = \\tau \\partial|\\xi| + (\\xi - u_i)\n$$\nThe subdifferential of the absolute value function, $\\partial|\\xi|$, is:\n$$\n\\partial|\\xi| = \\begin{cases} \\{1\\}  \\text{if } \\xi  0 \\\\ \\{-1\\}  \\text{if } \\xi  0 \\\\ [-1, 1]  \\text{if } \\xi = 0 \\end{cases}\n$$\nThe optimality condition is $0 \\in \\partial J_i(x_i)$. We analyze this condition by cases for the optimal value $x_i$:\n\nCase 1: $x_i  0$.\nThe subgradient is $\\partial J_i(x_i) = \\{\\tau + (x_i - u_i)\\}$. Setting this to zero gives $\\tau + x_i - u_i = 0$, which implies $x_i = u_i - \\tau$. For this solution to be consistent with the assumption $x_i  0$, we must have $u_i - \\tau  0$, or $u_i  \\tau$.\n\nCase 2: $x_i  0$.\nThe subgradient is $\\partial J_i(x_i) = \\{-\\tau + (x_i - u_i)\\}$. Setting this to zero gives $-\\tau + x_i - u_i = 0$, which implies $x_i = u_i + \\tau$. For this solution to be consistent with the assumption $x_i  0$, we must have $u_i + \\tau  0$, or $u_i  -\\tau$.\n\nCase 3: $x_i = 0$.\nThe optimality condition becomes $0 \\in \\tau [-1, 1] + (0 - u_i)$, which simplifies to $u_i \\in \\tau [-1, 1]$. This is equivalent to $-\\tau \\le u_i \\le \\tau$, or $|u_i| \\le \\tau$.\n\nCombining these three cases gives the explicit formula for the soft-thresholding operator on a scalar $u_i$:\n$$\nx_i = S_{\\tau}(u_i) = \\begin{cases} u_i - \\tau  \\text{if } u_i  \\tau \\\\ 0  \\text{if } |u_i| \\le \\tau \\\\ u_i + \\tau  \\text{if } u_i  -\\tau \\end{cases}\n$$\nThis rule explains the behavior of the operator. If the magnitude of a component $u_i$ is below or at the threshold $\\tau$, it is set to zero, thus promoting sparsity. If its magnitude exceeds the threshold, it is \"shrunk\" towards zero by an amount $\\tau$ while preserving its sign.\n\nWe now apply this rule to the given vector $u = (\\,3,\\,-1,\\,\\frac{3}{2},\\,-4,\\,\\frac{1}{2}\\,)^{\\top}$ with the threshold $\\tau = \\frac{3}{2}$.\n\nFor the first component, $u_1 = 3$:\nSince $u_1 = 3  \\tau = \\frac{3}{2}$, this component is shrunk.\n$x_1 = u_1 - \\tau = 3 - \\frac{3}{2} = \\frac{6}{2} - \\frac{3}{2} = \\frac{3}{2}$.\n\nFor the second component, $u_2 = -1$:\nSince $|u_2| = |-1| = 1 \\le \\tau = \\frac{3}{2}$, this component is set to zero.\n$x_2 = 0$.\n\nFor the third component, $u_3 = \\frac{3}{2}$:\nSince $|u_3| = |\\frac{3}{2}| = \\frac{3}{2} \\le \\tau = \\frac{3}{2}$, this component is set to zero (it falls on the boundary of the thresholding region).\n$x_3 = 0$.\n\nFor the fourth component, $u_4 = -4$:\nSince $u_4 = -4  -\\tau = -\\frac{3}{2}$, this component is shrunk.\n$x_4 = u_4 + \\tau = -4 + \\frac{3}{2} = -\\frac{8}{2} + \\frac{3}{2} = -\\frac{5}{2}$.\n\nFor the fifth component, $u_5 = \\frac{1}{2}$:\nSince $|u_5| = |\\frac{1}{2}| = \\frac{1}{2} \\le \\tau = \\frac{3}{2}$, this component is set to zero.\n$x_5 = 0$.\n\nCombining these results, the final vector is $S_{\\tau}(u) = (\\frac{3}{2}, 0, 0, -\\frac{5}{2}, 0)^{\\top}$. As requested, this is expressed as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2}  0  0  -\\frac{5}{2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once we understand the mechanics of an ISTA iteration, the next logical question is: when has the algorithm found the solution? For non-smooth optimization problems like LASSO, the familiar condition of a zero gradient is replaced by a more general concept from convex analysis involving subgradients. This exercise provides hands-on practice with this first-order optimality condition, challenging you to construct a valid subgradient to check how \"close\" a candidate vector is to the true minimizer, thereby connecting the algorithm's behavior to its theoretical foundation .",
            "id": "3392935",
            "problem": "Consider the composite convex objective underlying the Iterative Soft-Thresholding Algorithm (ISTA) in inverse problems and data assimilation,\n$$\nF(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\lambda  0$, and $x \\in \\mathbb{R}^{n}$. The first-order optimality condition for this problem can be expressed using the subdifferential of the $\\ell_{1}$ norm. Using only fundamental definitions from convex analysis and linear algebra, derive the stationarity condition for $F$, and then use it to evaluate the quality of a candidate solution.\n\nLet $A = I_{3} \\in \\mathbb{R}^{3 \\times 3}$, $b = \\begin{pmatrix}2 \\\\ 0.4 \\\\ 0\\end{pmatrix}$, $\\lambda = \\frac{1}{2}$, and the candidate vector $x = \\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$. Construct a valid subgradient $s \\in \\partial \\|x\\|_{1}$ at the given $x$ that minimizes the Euclidean norm of the stationarity residual, and compute the minimal possible value of\n$$\n\\|A^{\\top}(A x - b) + \\lambda s\\|_{2}.\n$$\nGive your final numerical answer rounded to four significant figures. No units are required.",
            "solution": "The problem is valid as it is scientifically grounded in convex optimization, well-posed, objective, and internally consistent.\n\nThe objective function to be analyzed is a composite function $F(x) = f(x) + g(x)$, where $f(x)$ is a smooth, convex, differentiable term and $g(x)$ is a convex, non-smooth term. Specifically, we have:\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}\n$$\n$$\ng(x) = \\lambda \\|x\\|_{1}\n$$\nAccording to Fermat's rule in convex analysis, a point $x^*$ is a global minimizer of $F(x)$ if and only if the zero vector is an element of the subdifferential of $F(x)$ at $x^*$. This is the first-order optimality or stationarity condition:\n$$\n0 \\in \\partial F(x^*)\n$$\nSince $f(x)$ is differentiable, the subdifferential of the sum $F(x)$ is the sum of the gradient of $f(x)$ and the subdifferential of $g(x)$:\n$$\n\\partial F(x) = \\nabla f(x) + \\partial g(x)\n$$\nFirst, we find the gradient of $f(x)$. The function is $f(x) = \\frac{1}{2}(Ax - b)^{\\top}(Ax - b) = \\frac{1}{2}(x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b)$. The gradient with respect to $x$ is:\n$$\n\\nabla f(x) = \\frac{1}{2}(2A^{\\top}Ax - 2A^{\\top}b) = A^{\\top}(Ax - b)\n$$\nNext, we characterize the subdifferential of $g(x) = \\lambda \\|x\\|_{1}$. This is $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$. The $\\ell_1$-norm is $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$. Its subdifferential is the Cartesian product of the subdifferentials of its components. A vector $s \\in \\mathbb{R}^n$ is a subgradient of the $\\ell_1$-norm at $x$, denoted $s \\in \\partial \\|x\\|_1$, if its components $s_i$ satisfy:\n$$\ns_i =\n\\begin{cases}\n\\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\\nc_i \\in [-1, 1]  \\text{if } x_i = 0\n\\end{cases}\n$$\nThe stationarity condition for a point $x$ is therefore the existence of a subgradient $s \\in \\partial\\|x\\|_1$ such that:\n$$\nA^{\\top}(Ax - b) + \\lambda s = 0\n$$\nThe vector $R(s) = A^{\\top}(A x - b) + \\lambda s$ is the stationarity residual. The problem requires us to find a subgradient $s$ that minimizes the Euclidean norm of this residual, $\\|R(s)\\|_2$, for the given candidate solution $x$.\n\nWe are given the specific values:\n$A = I_{3}$ (the $3 \\times 3$ identity matrix)\n$b = \\begin{pmatrix} 2 \\\\ 0.4 \\\\ 0 \\end{pmatrix}$\n$\\lambda = \\frac{1}{2}$\n$x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n\nFirst, we compute the gradient term $\\nabla f(x) = A^{\\top}(Ax-b)$. Since $A = I_3$, $A^{\\top}=I_3$ and the expression simplifies to $x-b$:\n$$\n\\nabla f(x) = x - b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -0.4 \\\\ 1 \\end{pmatrix}\n$$\nNext, we characterize the set of valid subgradients $s \\in \\partial \\|x\\|_1$ for the given $x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$:\nFor the first component, $x_1 = 1 \\neq 0$, so $s_1 = \\text{sgn}(1) = 1$.\nFor the second component, $x_2 = 0$, so $s_2$ can be any value in the interval $[-1, 1]$.\nFor the third component, $x_3 = 1 \\neq 0$, so $s_3 = \\text{sgn}(1) = 1$.\nThus, any valid subgradient $s$ must be of the form $s = \\begin{pmatrix} 1 \\\\ s_2 \\\\ 1 \\end{pmatrix}$ where $s_2 \\in [-1, 1]$.\n\nThe stationarity residual is $R(s) = (x-b) + \\lambda s$. Substituting the known quantities:\n$$\nR(s) = \\begin{pmatrix} -1 \\\\ -0.4 \\\\ 1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ s_2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 + 0.5 \\\\ -0.4 + 0.5 s_2 \\\\ 1 + 0.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -0.4 + 0.5 s_2 \\\\ 1.5 \\end{pmatrix}\n$$\nWe need to find the value of $s_2 \\in [-1, 1]$ that minimizes the Euclidean norm of this residual, $\\|R(s)\\|_2$. Minimizing $\\|R(s)\\|_2$ is equivalent to minimizing its square, $\\|R(s)\\|_2^2$:\n$$\n\\|R(s)\\|_2^2 = (-0.5)^2 + (-0.4 + 0.5 s_2)^2 + (1.5)^2\n$$\n$$\n\\|R(s)\\|_2^2 = 0.25 + (-0.4 + 0.5 s_2)^2 + 2.25 = 2.5 + (-0.4 + 0.5 s_2)^2\n$$\nTo minimize this expression, we must minimize the term $(-0.4 + 0.5 s_2)^2$. The minimum value of a squared real number is $0$, which occurs when the term inside the square is zero. We solve for $s_2$:\n$$\n-0.4 + 0.5 s_2 = 0 \\implies 0.5 s_2 = 0.4 \\implies s_2 = \\frac{0.4}{0.5} = 0.8\n$$\nThe value $s_2 = 0.8$ is within the allowed interval $[-1, 1]$. Therefore, the subgradient that minimizes the residual norm is $s = \\begin{pmatrix} 1 \\\\ 0.8 \\\\ 1 \\end{pmatrix}$.\n\nWith this optimal choice of $s_2$, the minimal value of the squared norm of the residual is:\n$$\n\\min \\|R(s)\\|_2^2 = 2.5 + (-0.4 + 0.5 \\times 0.8)^2 = 2.5 + (-0.4 + 0.4)^2 = 2.5 + 0^2 = 2.5\n$$\nThe minimal possible value of the norm is the square root of this value:\n$$\n\\min \\|R(s)\\|_2 = \\sqrt{2.5}\n$$\nThe problem requires a numerical answer rounded to four significant figures.\n$$\n\\sqrt{2.5} \\approx 1.5811388...\n$$\nRounding to four significant figures, we get $1.581$.",
            "answer": "$$\\boxed{1.581}$$"
        },
        {
            "introduction": "The practical success of regularization methods like LASSO hinges on the judicious choice of the regularization parameter, $\\lambda$. This parameter mediates the trade-off between fitting the data and enforcing sparsity, and a poor choice can lead to trivial or misleading results. This final practice explores the crucial challenge of parameter selection, first by demonstrating how an overly large $\\lambda$ can cause the algorithm to stagnate, and then by deriving a principled, data-driven strategy for setting $\\lambda$ based on the statistical properties of the noise . This exercise bridges the gap between abstract algorithmic theory and its robust application in real-world inverse problems.",
            "id": "3392955",
            "problem": "Consider the linear inverse problem with unknown vector $x^{\\star} \\in \\mathbb{R}^{n}$, sensing matrix $A \\in \\mathbb{R}^{n \\times n}$, and observed data $b \\in \\mathbb{R}^{n}$ generated by $b = A x^{\\star} + \\eta$, where $\\eta$ is an additive noise vector. The Iterative Soft-Thresholding Algorithm (ISTA) for the $\\ell_{1}$-regularized least squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ is defined by the iterative map $x^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)$, where $\\tau  0$ is a step size satisfying a standard Lipschitz condition and $\\mathcal{S}_{\\theta}$ denotes the componentwise soft-thresholding operator $\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}$.\n\n(a) Construct a counterexample demonstrating stagnation (plateau at the zero vector) under naive threshold selection when the regularization parameter $\\lambda$ is too large relative to the data term. Specifically, take $n = 3$, $A = I_{3}$, $\\tau = 1$, and $b = (0.12,\\,-0.09,\\,0)^{\\top}$. Starting from the definitions above and no further shortcuts, use first-principles reasoning to show precisely why the ISTA iterates $x^{k}$ become identically zero for all $k \\geq 1$ whenever $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ in this configuration.\n\n(b) In a noisy data assimilation setting, suppose the noise $\\eta$ has independent and identically distributed (i.i.d.) Gaussian entries with zero mean and variance $\\sigma^{2}$, i.e., $\\eta \\sim \\mathcal{N}(0,\\,\\sigma^{2} I_{n})$. Derive, from a probabilistic argument based on the distribution of the maximum of $n$ i.i.d. Gaussian variables and without invoking pre-stated shortcuts, an analytically justified scaling for $\\lambda$ as a function of $\\sigma$ and $n$ that guards against noise-dominated updates while avoiding trivial zero solutions for signals with entries exceeding the typical noise-driven plateaus. Use the resulting scaling to compute a recommended $\\lambda$ for $n = 1024$ and $\\sigma = 0.03$.\n\nRound your final numerical value of $\\lambda$ to four significant figures. Express your answer as a pure number without units.",
            "solution": "The problem presents two distinct tasks related to the Iterative Soft-Thresholding Algorithm (ISTA). Part (a) requires a demonstration of iterate stagnation under specific conditions, while Part (b) calls for the derivation of a principled scaling for the regularization parameter $\\lambda$ in a stochastic setting.\n\n### Part (a): Stagnation at the Zero Vector\n\nWe are given the ISTA update rule for the minimization problem $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$:\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)\n$$\nThe problem specifies the parameters for this part as $n = 3$, the identity matrix $A = I_{3}$, a step size $\\tau = 1$, and the data vector $b = (0.12,\\,-0.09,\\,0)^{\\top}$.\n\nSubstituting $A=I_{3}$ and $\\tau=1$ into the update rule yields a significant simplification. For any iterate $x^k$:\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\cdot 1}\\!\\left(x^{k} + 1 \\cdot I_{3}^{\\top}(b - I_{3} x^{k})\\right) = \\mathcal{S}_{\\lambda}\\!\\left(x^{k} + (b - x^{k})\\right) = \\mathcal{S}_{\\lambda}(b)\n$$\nThis result shows that for any $k \\geq 0$, the next iterate $x^{k+1}$ is solely determined by the action of the soft-thresholding operator $\\mathcal{S}_{\\lambda}$ on the data vector $b$. It is independent of the current iterate $x^k$. Consequently, all iterates from $k=1$ onwards are identical:\n$$\nx^{1} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\nx^{2} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\n\\vdots\n$$\n$$\nx^{k} = \\mathcal{S}_{\\lambda}(b) \\quad \\text{for all } k \\geq 1.\n$$\nThe problem asks us to show that these iterates become identically zero, i.e., $x^k = 0$ for all $k \\geq 1$, under the condition $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$.\n\nFor our specific configuration where $A=I_3$, the condition is $\\lambda \\geq \\|I_3^{\\top} b\\|_{\\infty} = \\|b\\|_{\\infty}$. The $\\ell_{\\infty}$-norm of the vector $b=(0.12,\\,-0.09,\\,0)^{\\top}$ is:\n$$\n\\|b\\|_{\\infty} = \\max\\big(|0.12|, |-0.09|, |0|\\big) = 0.12\n$$\nSo the condition becomes $\\lambda \\geq 0.12$.\n\nWe now analyze the expression for the iterates, $x^k = \\mathcal{S}_{\\lambda}(b)$, under this condition. The soft-thresholding operator $\\mathcal{S}_{\\theta}$ is defined componentwise as:\n$$\n\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}\n$$\nFor an entire vector $\\mathcal{S}_{\\theta}(z)$ to be the zero vector, it is necessary and sufficient that for every component $i$, $\\max\\{|z_{i}| - \\theta, 0\\} = 0$. This, in turn, is equivalent to the condition $|z_i| \\leq \\theta$ for all $i$. This can be expressed compactly using the $\\ell_{\\infty}$-norm as $\\|z\\|_{\\infty} \\leq \\theta$.\n\nApplying this to our problem, the condition for $x^k = \\mathcal{S}_{\\lambda}(b)$ to be the zero vector is that $\\|b\\|_{\\infty} \\leq \\lambda$. This is precisely the condition $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ given in the problem statement for our choice of $A=I_3$.\n\nTo be explicit, with $\\lambda \\geq 0.12$:\nFor the first component, $b_1 = 0.12$: $|b_1|=0.12$. Since $\\lambda \\geq 0.12$, $|b_1| - \\lambda \\leq 0$, so $\\max\\{|b_1|-\\lambda, 0\\}=0$.\nFor the second component, $b_2 = -0.09$: $|b_2|=0.09$. Since $\\lambda \\geq 0.12$, $|b_2| - \\lambda  0$, so $\\max\\{|b_2|-\\lambda, 0\\}=0$.\nFor the third component, $b_3 = 0$: $|b_3|=0$. Since $\\lambda \\geq 0.12$, $|b_3| - \\lambda  0$, so $\\max\\{|b_3|-\\lambda, 0\\}=0$.\n\nSince all components of $\\mathcal{S}_{\\lambda}(b)$ are zero, we have $\\mathcal{S}_{\\lambda}(b) = 0$.\nTherefore, for any choice of initial vector $x^0$, the first iterate is $x^1 = \\mathcal{S}_{\\lambda}(b) = 0$, and all subsequent iterates $x^k$ for $k \\geq 1$ remain at the zero vector. This demonstrates the specified stagnation.\n\n### Part (b): Probabilistic Scaling of the Regularization Parameter\n\nIn this part, we are to derive a scaling for $\\lambda$ that prevents the algorithm from being dominated by noise. The model is $b = Ax^\\star + \\eta$, where $\\eta$ is a vector of i.i.d. Gaussian noise components, $\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nA key principle in regularization is to choose $\\lambda$ large enough to suppress noise but not so large that it erroneously eliminates true signal components. A common strategy is to choose $\\lambda$ to be just above the level of noise that one would expect to see in the term being thresholded.\n\nConsider the first iteration of ISTA starting from the natural initial guess $x^0 = 0$. The argument of the soft-thresholding operator is $x^0 + \\tau A^{\\top}(b - A x^0) = \\tau A^\\top b$. If we consider a scenario with no true signal ($x^\\star=0$), then $b=\\eta$, and this argument becomes $\\tau A^\\top \\eta$. To prevent the algorithm from fitting to noise, we want this term to be thresholded to zero. This requires that every component of $\\tau A^\\top\\eta$ has a magnitude less than or equal to the threshold $\\lambda\\tau$. This can be written as:\n$$\n\\|\\tau A^\\top \\eta\\|_{\\infty} \\leq \\lambda\\tau \\implies \\|A^\\top \\eta\\|_{\\infty} \\leq \\lambda\n$$\nThis gives us a criterion for choosing $\\lambda$: it should be an upper bound on the likely values of $\\|A^\\top \\eta\\|_{\\infty}$.\n\nTo proceed with an analytical derivation, we must characterize the distribution of the vector $v = A^\\top \\eta$. This distribution depends on the matrix $A$. A standard assumption made for this type of universal threshold derivation is that the sensing matrix $A$ is orthonormal, i.e., $A^\\top A = I_n$. Under this assumption, the covariance of $v$ is:\n$$\n\\mathrm{Cov}(v) = E[v v^\\top] = E[A^\\top \\eta \\eta^\\top A] = A^\\top E[\\eta \\eta^\\top] A = A^\\top (\\sigma^2 I_n) A = \\sigma^2 A^\\top A = \\sigma^2 I_n\n$$\nSince $E[v] = A^\\top E[\\eta] = 0$, the components $v_i$ of $v = A^\\top \\eta$ are i.i.d. Gaussian random variables with mean $0$ and variance $\\sigma^2$, just like the original noise components $\\eta_i$.\n\nOur task now reduces to finding a high-probability upper bound on the random variable $M_n = \\|v\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |v_i|$. Let $Z_i = v_i/\\sigma$ be i.i.d. standard normal variables, $Z_i \\sim \\mathcal{N}(0, 1)$. We seek an estimate for $\\max_i |Z_i|$, and then scale the result by $\\sigma$.\n\nLet $Y = \\max_{1 \\leq i \\leq n} |Z_i|$. The cumulative distribution function (CDF) of $|Z_i|$ is $F_{|Z|}(y) = P(|Z_i| \\leq y) = 2\\Phi(y)-1$ for $y \\geq 0$, where $\\Phi$ is the CDF of the standard normal distribution. Since the $|Z_i|$ are i.i.d., the CDF of their maximum is:\n$$\nF_Y(y) = P(Y \\leq y) = [F_{|Z|}(y)]^n = (2\\Phi(y)-1)^n\n$$\nWe want to find a threshold $y_n$ such that $P(Y  y_n)$ is small for large $n$. We can use the union bound as an approximation for this tail probability:\n$$\nP(Y  y) = P(\\exists i: |Z_i|  y) \\leq \\sum_{i=1}^n P(|Z_i|y) = n P(|Z_1|y)\n$$\nThe probability $P(|Z_1|y)$ is $2(1-\\Phi(y))$. For large $y$, we can use the standard Gaussian tail approximation:\n$$\n1-\\Phi(y) \\approx \\frac{\\phi(y)}{y} = \\frac{1}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)\n$$\nThus, $P(Yy) \\approx 2n(1-\\Phi(y)) \\approx \\frac{2n}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)$. We seek a scaling for $y$ with $n$ such that this probability vanishes as $n \\to \\infty$. Let us test the candidate scaling $y = \\sqrt{2\\ln n}$:\n$$\nP(Y  \\sqrt{2\\ln n}) \\approx \\frac{2n}{\\sqrt{2\\pi}\\sqrt{2\\ln n}}\\exp\\left(-\\frac{( \\sqrt{2\\ln n} )^2}{2}\\right) = \\frac{2n}{\\sqrt{4\\pi\\ln n}}\\exp(-\\ln n) = \\frac{2n}{2\\sqrt{\\pi\\ln n}} \\left(\\frac{1}{n}\\right) = \\frac{1}{\\sqrt{\\pi\\ln n}}\n$$\nAs $n \\to \\infty$, this probability tends to $0$. This indicates that for large $n$, it is highly probable that the maximum of $n$ i.i.d. $|Z_i|$ variables will not exceed $\\sqrt{2\\ln n}$. This justifies choosing this value as a threshold.\n\nScaling back by $\\sigma$, we obtain the analytically justified scaling for $\\lambda$:\n$$\n\\lambda = \\sigma \\sqrt{2 \\ln n}\n$$\nThis is famously known as the universal threshold. It provides a parameter choice that adapts to the problem dimension $n$ and the noise level $\\sigma$.\n\nNow, we compute the recommended $\\lambda$ for $n = 1024$ and $\\sigma = 0.03$.\nFirst, calculate $\\ln(1024)$:\n$$\n\\ln(1024) = \\ln(2^{10}) = 10 \\ln(2)\n$$\nUsing a standard value for $\\ln(2) \\approx 0.69314718$:\n$$\n\\ln(1024) \\approx 6.9314718\n$$\nNow, substitute this into the formula for $\\lambda$:\n$$\n\\lambda = 0.03 \\times \\sqrt{2 \\times 6.9314718} = 0.03 \\times \\sqrt{13.8629436} \\approx 0.03 \\times 3.7232974\n$$\n$$\n\\lambda \\approx 0.1116989\n$$\nRounding this value to four significant figures gives:\n$$\n\\lambda \\approx 0.1117\n$$",
            "answer": "$$\n\\boxed{0.1117}\n$$"
        }
    ]
}