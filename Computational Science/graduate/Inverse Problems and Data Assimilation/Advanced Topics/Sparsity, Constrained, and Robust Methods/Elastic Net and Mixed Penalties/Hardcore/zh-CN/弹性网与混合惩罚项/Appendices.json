{
    "hands_on_practices": [
        {
            "introduction": "正则化并非没有代价；它通过引入估计量的偏差（bias）来换取其方差（variance）的降低，从而可能提高模型的整体预测性能。这个练习旨在将这一抽象的权衡过程具体化。我们将通过一个简化的岭回归（Ridge Regression）问题（即弹性网中设 $\\lambda_1=0$ 的特例），动手推导偏差和方差的显式表达式，并观察正则化参数 $\\lambda_2$ 如何调控这两者之间的消长关系 。这项实践为理解所有惩罚方法（penalized methods）的内在机制奠定了重要的直觉基础。",
            "id": "3377910",
            "problem": "考虑一个二维线性逆问题，其观测算子为单位算子。设数据模型为 $y = x^{\\star} + \\varepsilon$，其中 $x^{\\star} \\in \\mathbb{R}^{2}$ 是未知状态，$y \\in \\mathbb{R}^{2}$ 是观测数据，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$ 是加性高斯噪声，其均值为零，协方差为 $\\sigma^{2} I_{2}$。我们使用弹性网络（Elastic Net）进行估计，通过将其混合惩罚中的 $\\ell_{1}$ 参数设置为零，以分离二次惩罚的效果，从而得到岭正则化估计量\n$$\n\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}, \\quad \\lambda_{2} \\ge 0.\n$$\n定义样本内预测为 $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$，无噪声真实值为 $y^{\\star} = x^{\\star}$。预测稳定性由预期样本内预测误差 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ 分解为其偏差平方和方差分量后的方差项来量化。\n\n从数据模型和 $\\widehat{x}_{\\lambda_{2}}$ 的定义出发，推导 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ 的偏差-方差分解，使其成为 $\\lambda_{2}$、$x^{\\star}$ 和 $\\sigma^{2}$ 的显式函数。然后，使用 $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ 和 $\\sigma^{2} = 1$ 实例化该例子，以证明将 $\\lambda_{2}$ 从 $0$ 增加到 $1$ 会以增加偏差为代价来提高预测稳定性（减小方差）。通过计算以下比率来量化这种权衡\n$$\nR \\equiv \\frac{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时方差的减少量}}{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时偏差平方的增加量}}。\n$$\n请将 $R$ 的值作为一个精确的单一数字给出。不要包含单位。不要提供中间值。最终答案必须是一个实数。",
            "solution": "分析始于验证问题陈述的有效性。\n\n### 步骤1：提取已知条件\n- 数据模型：$y = x^{\\star} + \\varepsilon$，其中 $x^{\\star} \\in \\mathbb{R}^{2}$ 是未知的真实状态，$y \\in \\mathbb{R}^{2}$ 是观测值。\n- 噪声模型：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$，其中 $I_{2}$ 是 $2 \\times 2$ 单位矩阵。\n- 估计量：$\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}$，对于 $\\lambda_{2} \\ge 0$。\n- 样本内预测：$\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$。\n- 无噪声真实值：$y^{\\star} = x^{\\star}$。\n- 待分析量：预期样本内预测误差 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ 及其偏差-方差分解。\n- 用于实例化的特定参数：$x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$，$\\sigma^{2} = 1$。\n- 要求计算：比率 $R \\equiv \\frac{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时方差的减少量}}{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时偏差平方的增加量}}$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，是统计学习理论中岭回归（弹性网络的一个特例）的标准表述。该问题是适定的，因为成本函数是严格凸的，保证了唯一最小值的存在。该问题是客观的，使用了精确的数学语言。提供了解出唯一解所需的所有数据和定义，且没有矛盾之处。该问题要求进行严谨的推导和具体的计算，这是逆问题领域的标准练习。该问题并非无足轻重，并且是科学上可验证的。\n\n### 步骤3：结论与行动\n问题有效。我们继续求解。\n\n第一步是找到估计量 $\\widehat{x}_{\\lambda_{2}}$ 的显式闭式解表达式。需要最小化的成本函数是\n$$\nJ(x) = \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2}\n$$\n对于 $\\lambda_{2} \\ge 0$，该函数是严格凸的。通过将关于 $x$ 的梯度设置为零来找到最小值。\n$$\n\\nabla_{x} J(x) = \\nabla_{x} \\left( \\frac{1}{2} (y - x)^{T}(y - x) + \\frac{\\lambda_{2}}{2} x^{T}x \\right) = -(y - x) + \\lambda_{2} x\n$$\n将梯度设置为零：\n$$\n-(y - \\widehat{x}_{\\lambda_{2}}) + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n-y + \\widehat{x}_{\\lambda_{2}} + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n(1 + \\lambda_{2}) \\widehat{x}_{\\lambda_{2}} = y\n$$\n求解 $\\widehat{x}_{\\lambda_{2}}$ 可得：\n$$\n\\widehat{x}_{\\lambda_{2}} = \\frac{1}{1 + \\lambda_{2}} y\n$$\n样本内预测是 $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$。无噪声真实值是 $y^{\\star} = x^{\\star}$。预期预测误差是 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$。我们将此误差分解为其偏差平方和方差分量：\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\underbrace{\\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2}}_{\\text{偏差平方}} + \\underbrace{\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right]}_{\\text{方差}}\n$$\n首先，我们计算预测的期望值。期望是关于噪声 $\\varepsilon$ 的分布计算的。\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\mathbb{E}\\left[ \\frac{1}{1 + \\lambda_{2}} y \\right] = \\frac{1}{1 + \\lambda_{2}} \\mathbb{E}[y]\n$$\n使用数据模型 $y = x^{\\star} + \\varepsilon$ 以及 $\\mathbb{E}[\\varepsilon] = 0$ 这一事实：\n$$\n\\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}\n$$\n因此，预期预测为：\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\frac{1}{1 + \\lambda_{2}} x^{\\star}\n$$\n现在，我们计算偏差平方，记为 $B^2(\\lambda_2)$:\n$$\nB^2(\\lambda_2) = \\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{1}{1 + \\lambda_{2}} x^{\\star} - x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\left(\\frac{1}{1 + \\lambda_{2}} - 1\\right) x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{-\\lambda_{2}}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2}\n$$\n$$\nB^2(\\lambda_2) = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2}\n$$\n接下来，我们计算方差，记为 $V(\\lambda_2)$:\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} y - \\frac{1}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2} \\right]\n$$\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} (y - x^{\\star}) \\right\\|_{2}^{2} \\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} \\varepsilon \\right\\|_{2}^{2} \\right] = \\frac{1}{(1 + \\lambda_{2})^{2}} \\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}]\n$$\n噪声向量为 $\\varepsilon = (\\varepsilon_1, \\varepsilon_2)^T$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立的。$\\varepsilon$ 的期望平方范数为：\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\mathbb{E}[\\varepsilon_1^2 + \\varepsilon_2^2] = \\mathbb{E}[\\varepsilon_1^2] + \\mathbb{E}[\\varepsilon_2^2]\n$$\n对于任何均值为 $\\mu_Z$、方差为 $\\sigma_Z^2$ 的随机变量 $Z$，$\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2$。这里，$\\mathbb{E}[\\varepsilon_i] = 0$ 且 $\\text{Var}(\\varepsilon_i) = \\sigma^2$。所以，$\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2$。\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\n将此代入方差表达式：\n$$\nV(\\lambda_2) = \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\n完整的偏差-方差分解为：\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2} + \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\n现在，我们使用给定的参数实例化此表达式：$x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ 和 $\\sigma^{2} = 1$。\n首先，计算 $\\|x^{\\star}\\|_{2}^{2}$：\n$$\n\\|x^{\\star}\\|_{2}^{2} = 2^2 + 1^2 = 4 + 1 = 5\n$$\n我们评估 $\\lambda_{2} = 0$ 和 $\\lambda_{2} = 1$ 时的偏差平方和方差。\n\n对于 $\\lambda_{2} = 0$：\n偏差平方：$B^2(0) = \\left( \\frac{0}{1 + 0} \\right)^{2} \\times 5 = 0$。\n方差：$V(0) = \\frac{2(1)}{(1 + 0)^{2}} = 2$。\n\n对于 $\\lambda_{2} = 1$：\n偏差平方：$B^2(1) = \\left( \\frac{1}{1 + 1} \\right)^{2} \\times 5 = \\left(\\frac{1}{2}\\right)^{2} \\times 5 = \\frac{5}{4}$。\n方差：$V(1) = \\frac{2(1)}{(1 + 1)^{2}} = \\frac{2}{4} = \\frac{1}{2}$。\n\n当 $\\lambda_2$ 从0变为1时，偏差平方的增加量为：\n$$\n\\Delta B^2 = B^2(1) - B^2(0) = \\frac{5}{4} - 0 = \\frac{5}{4}\n$$\n当 $\\lambda_2$ 从0变为1时，方差的减少量为：\n$$\n\\Delta V = V(0) - V(1) = 2 - \\frac{1}{2} = \\frac{3}{2}\n$$\n问题要求计算比率 $R$：\n$$\nR = \\frac{\\text{方差的减少量}}{\\text{偏差平方的增加量}} = \\frac{\\Delta V}{\\Delta B^2} = \\frac{3/2}{5/4}\n$$\n$$\nR = \\frac{3}{2} \\times \\frac{4}{5} = \\frac{12}{10} = \\frac{6}{5}\n$$",
            "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$"
        },
        {
            "introduction": "在理解了正则化的一般原理后，我们进一步探讨弹性网中 $\\ell_1$ 和 $\\ell_2$ 惩罚项之间独特的相互作用。这个练习将揭示弹性网著名的“分组效应”（grouping effect）：当面对一组相关的预测变量时，弹性网倾向于将它们作为一个整体同时选入或排除出模型，这与可能随机选择其中一个的 LASSO 方法形成对比。我们将构造一个具体的反例，其中 $\\ell_2$ 惩罚项的强度足以阻止算法恢复真实的稀疏解，从而清晰地展示弹性网的这一重要特性 。",
            "id": "3377903",
            "problem": "考虑一个由 $y = A x^{\\star}$ 建模的单标量观测的线性反问题，其中 $A \\in \\mathbb{R}^{1 \\times 2}$，$x^{\\star} \\in \\mathbb{R}^{2}$，且 $y \\in \\mathbb{R}$。设 $A = [\\,1 \\;\\; 1\\,]$，$x^{\\star} = (2,\\,0)^{\\top}$，因此 $y = 2$。估计量 $x \\in \\mathbb{R}^{2}$ 是通过最小化弹性网络目标函数得到的\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|x\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2},\n$$\n其中 $\\lambda_{1} = 1$ 且 $\\lambda_{2} = 10$。从弹性网络目标函数的核心定义和基本的凸优化原理（包括一阶最优性和凸性论证）出发，推导出精确的极小值点 $\\hat{x}$ 并计算其值。然后，运用基于凸分析中水平集和法锥几何学的几何推理，解释为什么当 $\\lambda_{2}$ 相对于 $\\lambda_{1}$ 较大时，这个构造提供了一个关于支撑集恢复失败的反例。\n\n你的最终数值答案应为表示为单行矩阵的极小值点向量 $\\hat{x}$。不要四舍五入，请提供精确值。",
            "solution": "用户要求我求解弹性网络目标函数的极小值点，并对解的特定性质提供几何解释。\n\n### 步骤1：问题验证\n\n**提取的已知条件：**\n- 线性模型：$y = A x^{\\star}$，其中 $A \\in \\mathbb{R}^{1 \\times 2}$，$x^{\\star} \\in \\mathbb{R}^{2}$，$y \\in \\mathbb{R}$。\n- 数据：$A = [\\,1 \\;\\; 1\\,]$，$x^{\\star} = (2,\\,0)^{\\top}$，$y = 2$。\n- 弹性网络目标函数：$J(x) = \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} + \\lambda_{1}\\,\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2}$。\n- 正则化参数：$\\lambda_{1} = 1$，$\\lambda_{2} = 10$。\n- 任务1：推导精确的极小值点 $\\hat{x}$。\n- 任务2：从几何角度解释为何该结果为支撑集恢复提供了一个反例。\n\n**验证：**\n1.  **科学基础：** 该问题是弹性网络正则化的一个标准应用，这是一种在凸优化、统计学和机器学习中成熟的技术。所有概念都具有科学合理性。\n2.  **适定性：** 目标函数 $J(x)$ 是三个凸函数的和：一个凸二次损失函数、一个凸 $L_1$ 范数和一个严格凸的 $L_2$ 范数平方（因为 $\\lambda_2 = 10 > 0$）。因此，它们的和是严格凸的。一个在紧集上或在 $\\mathbb{R}^n$ 上（如果是强制的，如此处所示）的严格凸函数存在唯一的极小值点。该问题是适定的。\n3.  **客观性：** 问题以精确的数学语言陈述，不含主观性。\n4.  **完整性：** 所有必要的值（$A, y, \\lambda_1, \\lambda_2$）都已提供并且是一致的（$A x^{\\star} = [1\\;1](2,0)^\\top = 2 = y$）。\n\n**结论：** 问题是有效的，其所有组成部分都定义良好。\n\n---\n\n### 步骤2：求解推导\n\n要最小化的目标函数是 $J(x) = \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} + \\lambda_{1}\\,\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2}$。\n设 $x = (x_1, x_2)^{\\top}$。代入给定值 $y=2$，$A = [\\,1 \\;\\; 1\\,]$，$\\lambda_1=1$ 和 $\\lambda_2=10$：\n$$\nJ(x_1, x_2) = \\frac{1}{2}(2 - (x_1 + x_2))^2 + |x_1| + |x_2| + \\frac{10}{2}(x_1^2 + x_2^2)\n$$\n由于绝对值项的存在，该函数是凸的但不可微。点 $\\hat{x}$ 是 $J(x)$ 的唯一极小值点的充要条件是零向量属于 $J$ 在 $\\hat{x}$ 处的次微分，即 $0 \\in \\partial J(\\hat{x})$。\n\n目标函数可以分解为一个可微部分 $f(x) = \\frac{1}{2}(y - Ax)^2 + \\frac{\\lambda_2}{2}\\|x\\|_2^2$ 和一个不可微部分 $g(x) = \\lambda_1 \\|x\\|_1$。次微分为 $\\partial J(x) = \\nabla f(x) + \\partial g(x)$。一阶最优性条件是 $0 \\in \\nabla f(\\hat{x}) + \\lambda_1 \\partial \\|\\hat{x}\\|_1$。\n\n光滑部分 $f(x)$ 的梯度是：\n$$\n\\nabla f(x) = -A^{\\top}(y - Ax) + \\lambda_2 x\n$$\n代入给定值：\n$$\n\\nabla f(x) = -\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}(2 - (x_1 + x_2)) + 10 \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -(2 - x_1 - x_2) + 10x_1 \\\\ -(2 - x_1 - x_2) + 10x_2 \\end{pmatrix} = \\begin{pmatrix} 11x_1 + x_2 - 2 \\\\ x_1 + 11x_2 - 2 \\end{pmatrix}\n$$\n$L_1$ 范数的次微分是 $\\partial \\|x\\|_1 = \\bigotimes_{i=1}^2 \\partial |x_i|$，其中当 $z \\neq 0$ 时 $\\partial |z| = \\text{sign}(z)$，当 $z=0$ 时 $\\partial |z| = [-1, 1]$。\n\n当 $\\lambda_1=1$ 时，最优性条件 $0 \\in \\nabla f(\\hat{x}) + \\lambda_1 \\partial \\|\\hat{x}\\|_1$ 变为：\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\begin{pmatrix} 11\\hat{x}_1 + \\hat{x}_2 - 2 \\\\ \\hat{x}_1 + 11\\hat{x}_2 - 2 \\end{pmatrix} + \\begin{pmatrix} \\partial |\\hat{x}_1| \\\\ \\partial |\\hat{x}_2| \\end{pmatrix}\n$$\n这给出了一个由两个次微分包含关系组成的系统：\n1. $-(11\\hat{x}_1 + \\hat{x}_2 - 2) \\in \\partial |\\hat{x}_1|$\n2. $-(\\hat{x}_1 + 11\\hat{x}_2 - 2) \\in \\partial |\\hat{x}_2|$\n\n由于问题设置的对称性（$A=[1,1]$），目标函数 $J(x_1, x_2)$ 关于交换 $x_1$ 和 $x_2$ 是对称的。因为 $J(x)$ 是严格凸的，因此有唯一的极小值点，这个极小值点也必须是对称的，即 $\\hat{x}_1 = \\hat{x}_2$。设 $\\hat{x}_1 = \\hat{x}_2 = \\alpha$。解不可能是 $(0,0)^{\\top}$，因为 $J(0,0)=2$，而对于一个小的扰动，例如对于小的 $\\epsilon>0$，$x=(\\epsilon, \\epsilon)$，有 $J(\\epsilon, \\epsilon) \\approx \\frac{1}{2}(2-2\\epsilon)^2 + 2\\epsilon \\approx 2 - 4\\epsilon + 2\\epsilon^2 + 2\\epsilon = 2-2\\epsilon < 2$。因此 $\\alpha \\neq 0$。\n\n假设 $\\alpha > 0$。那么 $\\partial |\\hat{x}_1| = \\{1\\}$ 且 $\\partial |\\hat{x}_2| = \\{1\\}$。包含关系变为等式：\n1. $-(11\\alpha + \\alpha - 2) = 1 \\implies -12\\alpha + 2 = 1 \\implies 12\\alpha = 1 \\implies \\alpha = \\frac{1}{12}$\n2. $-(\\alpha + 11\\alpha - 2) = 1 \\implies -12\\alpha + 2 = 1 \\implies 12\\alpha = 1 \\implies \\alpha = \\frac{1}{12}$\n\n两个方程都一致地得出 $\\alpha = 1/12$。由于这个值是正的，我们关于 $\\alpha > 0$ 的假设是有效的。我们找到了一个满足一阶最优性条件的点。由于严格凸性，这个点是唯一的全局极小值点。\n极小值点是 $\\hat{x} = (\\frac{1}{12}, \\frac{1}{12})^{\\top}$。\n\n### 步骤3：支撑集恢复失败的几何解释\n\n**支撑集恢复：** 真实的稀疏向量是 $x^{\\star} = (2, 0)^{\\top}$。其支撑集（非零元素的索引集合）是 $\\{1\\}$。估计的向量是 $\\hat{x} = (\\frac{1}{12}, \\frac{1}{12})^{\\top}$。其支撑集是 $\\{1, 2\\}$。由于支撑集不匹配，弹性网络估计量在这种情况下未能实现支撑集恢复。这提供了一个反例。\n\n**几何与分析解释：** 弹性网络估计量 $\\hat{x}$ 是目标函数光滑部分 $f(x) = \\frac{1}{2}\\|y-Ax\\|^2_2 + \\frac{\\lambda_2}{2}\\|x\\|^2_2$ 的一个水平集与非光滑部分 $g(x) = \\lambda_1\\|x\\|_1$ 的一个水平集之间的切点。形式上，这由最优性条件 $-\\nabla f(\\hat{x}) \\in \\partial g(\\hat{x})$ 表示，该条件表明光滑部分的负梯度必须位于点 $\\hat{x}$ 处 $L_1$ 球的法锥内。\n\n我们来研究一下形式为 $\\hat{x} = (c, 0)^{\\top}$（其中 $c>0$）的稀疏解是否可能是最优的。对于这样的点，由 $\\lambda_1$ 缩放的 $L_1$ 球的法锥是 $\\lambda_1 \\left( \\{1\\} \\times [-1, 1] \\right)$。最优性条件 $-\\nabla f(c,0) \\in \\lambda_1 \\left( \\{1\\} \\times [-1, 1] \\right)$ 变为：\n$$\n\\begin{pmatrix} -(11c + 0 - 2) \\\\ -(c + 0 - 2) \\end{pmatrix} = \\begin{pmatrix} 2 - 11c \\\\ 2 - c \\end{pmatrix} \\in \\begin{pmatrix} 1 \\cdot \\{1\\} \\\\ 1 \\cdot [-1, 1] \\end{pmatrix} = \\begin{pmatrix} \\{1\\} \\\\ [-1, 1] \\end{pmatrix}\n$$\n这产生了两个条件：\n1. $2 - 11c = 1 \\implies 11c = 1 \\implies c = \\frac{1}{11}$.\n2. $|2-c| \\le 1$.\n\n将第一个条件得到的 $c=1/11$ 代入第二个条件：\n$$\n|2 - \\frac{1}{11}| = |\\frac{22-1}{11}| = \\frac{21}{11}\n$$\n该条件变为 $\\frac{21}{11} \\le 1$，这是错误的。\n\n从几何角度看，这意味着在 $x_1$ 轴上的点 $(1/11, 0)^{\\top}$ 处，负梯度向量 $-\\nabla f(c,0)$ 指向 $L_1$ 球法锥的外部。梯度的第二个分量 $-(c-2)$ 太大，表明通过使 $x_2$ 非零可以实现目标函数的下降。因此，形如 $(c,0)^{\\top}$ 的稀疏解不可能是最优的。\n\n这种失败是弹性网络“分组效应”的一种体现。预测变量（$A = [\\,1 \\;\\; 1\\,]$的列）是完全相关的。Ridge惩罚项 $\\frac{\\lambda_2}{2}\\|x\\|_2^2$ 鼓励相关预测变量拥有相似的系数值。当 $\\lambda_2$ 相对于 $\\lambda_1$ 较大时，这种分组效应主导了 $L_1$ 惩罚项的稀疏性诱导压力。它迫使 $\\hat{x}_1$ 和 $\\hat{x}_2$ 彼此接近，将 $\\hat{x}_2$ 从零拉开，从而破坏了真实解的稀疏性。问题的对称性（$A_1=A_2=1$）使得这种效应成为绝对的，迫使 $\\hat{x}_1 = \\hat{x}_2$，并使得对于任何 $\\lambda_1, \\lambda_2 > 0$（除非解是原点的平凡解），支撑集恢复都变得不可能。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{12} & \\frac{1}{12} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "掌握了弹性网的统计特性后，我们转向一个实际问题：如何求解其对应的优化问题。现实中，已有许多针对 LASSO 问题的高度优化求解器。这个练习将引导你推导一种强大的代数技巧，它能将弹性网问题等价地重构为一个在“增广”系统上的 LASSO 问题 。掌握这一变换，你便可以利用现有的各种高效软件包来求解弹性网问题，这是连接理论与实践的关键一步。",
            "id": "3377866",
            "problem": "考虑一个线性逆问题，其中未知状态向量 $x \\in \\mathbb{R}^{n}$ 通过已知的线性正向算子 $A \\in \\mathbb{R}^{m \\times n}$ 与观测数据 $y \\in \\mathbb{R}^{m}$ 相关联，其关系为 $y = A x + \\varepsilon$，其中 $\\varepsilon$ 代表加性噪声。在逆问题和数据同化中，一种常见的方法是通过最小化一个在数据保真度与正则化之间取得平衡的复合目标函数来估计 $x$。假设目标函数为\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + g(x),\n$$\n其中分析弹性网络罚项定义为\n$$\ng(x) = \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2},\n$$\n其中 $W \\in \\mathbb{R}^{p \\times n}$ 是一个固定的分析算子（例如，离散梯度或小波变换），$\\lambda_{1} > 0$ 且 $\\lambda_{2} \\ge 0$。从欧几里得范数的基本性质和标准线性代数恒等式（特别是，对于任何维度相容的向量 $u$ 和 $v$，有 $\\|[u; v]\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\|v\\|_{2}^{2}$）出发，推导该最小化问题的一个等价表述形式，其中 $g(x)$ 的二次部分通过增广正向算子和观测向量被吸收到数据失配项中。更准确地说，证明存在一个增广算子 $\\tilde{A} \\in \\mathbb{R}^{(m+n) \\times n}$ 和一个增广观测向量 $\\tilde{b} \\in \\mathbb{R}^{m+n}$，使得\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1},\n$$\n并用 $A$、$y$、$\\lambda_{2}$ 和单位矩阵 $I \\in \\mathbb{R}^{n \\times n}$ 明确写出 $\\tilde{A}$ 和 $\\tilde{b}$ 的表达式。假设 $\\lambda_{2} \\ge 0$，并在计算任何标量平方根时取其主平方根。你的最终答案必须是 $\\tilde{A}$ 和 $\\tilde{b}$ 的明确表达式，形式为单个解析表达式。不需要进行数值近似或四舍五入。",
            "solution": "该问题是有效的，因为它基于标准的优化和线性代数原理，具有科学依据，是适定的且有唯一解，并使用客观、形式化的数学语言陈述。\n\n需要最小化的目标函数由下式给出：\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + g(x)\n$$\n其中混合弹性网络罚项定义为：\n$$\ng(x) = \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\n参数 $\\lambda_{1}$ 和 $\\lambda_{2}$ 是非负的，其中 $\\lambda_{1} > 0$ 且 $\\lambda_{2} \\ge 0$。将 $g(x)$ 的表达式代入 $J(x)$，我们得到：\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\n目标是证明这可以被重新表述为以下形式：\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1}\n$$\n对于某个增广算子 $\\tilde{A}$ 和增广向量 $\\tilde{b}$。这要求我们将两个二次项 $\\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和 $\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$ 合并成一个单一的欧几里得范数的平方。项 $\\lambda_{1}\\|W x\\|_{1}$ 保持不变。\n\n让我们关注二次项的和：\n$$\nQ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\n我们可以通过将标量系数移入范数内部来重写第二项 $\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$。由于 $\\lambda_{2} \\ge 0$，其主平方根 $\\sqrt{\\lambda_{2}}$ 是一个非负实数。向量 $v$ 的欧几里得范数的平方是 $\\|v\\|_{2}^{2} = v^{T}v$。因此，$\\|x\\|_{2}^{2}$ 可以按如下方式处理：\n$$\n\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} (\\sqrt{\\lambda_{2}})^{2} \\|x\\|_{2}^{2} = \\frac{1}{2} \\|\\sqrt{\\lambda_{2}} x\\|_{2}^{2}\n$$\n我们可以使用单位矩阵 $I \\in \\mathbb{R}^{n \\times n}$ 将向量 $\\sqrt{\\lambda_{2}}x$ 表示为矩阵-向量积，即 $\\sqrt{\\lambda_{2}}x = (\\sqrt{\\lambda_{2}}I)x$。为了匹配第一个数据拟合项的结构，我们可以引入一个零向量 $0_n \\in \\mathbb{R}^n$：\n$$\n\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} \\|(\\sqrt{\\lambda_{2}}I)x\\|_{2}^{2} = \\frac{1}{2} \\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2}\n$$\n现在，将其代回到 $Q(x)$ 的表达式中：\n$$\nQ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2}\n$$\n提出因子 $\\frac{1}{2}$，我们得到：\n$$\nQ(x) = \\frac{1}{2} \\left( \\|A x - y\\|_{2}^{2} + \\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2} \\right)\n$$\n问题陈述提供了垂直拼接向量的范数平方恒等式：对于任何维度相容的向量 $u$ 和 $v$，有 $\\|[u; v]\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\|v\\|_{2}^{2}$。让我们反向应用此恒等式。\n令 $u = Ax - y$ 且 $v = (\\sqrt{\\lambda_{2}}I)x - 0_n$。向量 $u \\in \\mathbb{R}^m$ 且向量 $v \\in \\mathbb{R}^n$。它们的范数平方和等于它们垂直拼接后向量的范数平方：\n$$\nQ(x) = \\frac{1}{2} \\left\\| \\begin{pmatrix} Ax - y \\\\ (\\sqrt{\\lambda_{2}}I)x - 0_n \\end{pmatrix} \\right\\|_{2}^{2}\n$$\n这个拼接向量可以通过使用分块矩阵代数将包含 $x$ 的项与常数项分开来重写：\n$$\n\\begin{pmatrix} Ax - y \\\\ (\\sqrt{\\lambda_{2}}I)x - 0_n \\end{pmatrix} = \\begin{pmatrix} Ax \\\\ (\\sqrt{\\lambda_{2}}I)x \\end{pmatrix} - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\n从第一个分块向量中提出向量 $x$ 得到：\n$$\n\\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix} x - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\n通过将此表达式与目标形式 $\\tilde{A}x - \\tilde{b}$ 进行比较，我们可以确定增广算子 $\\tilde{A}$ 和增广观测向量 $\\tilde{b}$。\n增广算子 $\\tilde{A}$ 是一个通过将 $A$ 堆叠在 $\\sqrt{\\lambda_{2}}I$ 之上形成的分块矩阵。由于 $A \\in \\mathbb{R}^{m \\times n}$ 且 $I \\in \\mathbb{R}^{n \\times n}$，得到的矩阵 $\\tilde{A}$ 的维度为 $(m+n) \\times n$：\n$$\n\\tilde{A} = \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix}\n$$\n增广向量 $\\tilde{b}$ 是通过将原始观测向量 $y$ 堆叠在零向量 $0_n$ 之上形成的。由于 $y \\in \\mathbb{R}^m$ 且 $0_n \\in \\mathbb{R}^n$，得到的向量 $\\tilde{b}$ 的维度为 $(m+n) \\times 1$：\n$$\n\\tilde{b} = \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\n将这些定义代回，我们成功地证明了：\n$$\n\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} \\left\\| \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix} x - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix} \\right\\|_{2}^{2} = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2}\n$$\n因此，原始目标函数 $J(x)$ 等价于：\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1}\n$$\n因此，$\\tilde{A}$ 和 $\\tilde{b}$ 的表达式已按要求推导得出。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_2} I \\end{pmatrix} & \\begin{pmatrix} y \\\\ 0 \\end{pmatrix} \\end{pmatrix}}\n$$"
        }
    ]
}