{
    "hands_on_practices": [
        {
            "introduction": "The first step in any function-space Bayesian analysis is to rigorously define the posterior probability measure. This exercise provides practice in this fundamental construction for a common and important scenario: inferring the full time-dependent state of a system governed by a parabolic partial differential equation (PDE) from noisy, discrete-in-time observations. By combining a Gaussian prior, induced by the stochastic PDE, with a Gaussian likelihood from the data, you will characterize the resulting posterior Gaussian measure and its associated Cameron–Martin space, revealing how the data reshapes the geometry of uncertainty on the path space .",
            "id": "3383875",
            "problem": "Consider the following linear parabolic partial differential equation (PDE) posed on a bounded, open domain $\\mathcal{D} \\subset \\mathbb{R}^{d}$ with smooth boundary, with state space $H = L^{2}(\\mathcal{D})$:\n$$\n\\partial_{t} u(t,x) + A u(t,x) = \\xi(t,x), \\quad t \\in (0,T], \\quad x \\in \\mathcal{D}, \\quad u(0,x) = u_{0}(x),\n$$\nwhere $A$ is a densely defined, self-adjoint, positive-definite linear operator on $H$ that generates an analytic contraction semigroup $\\{S(t)\\}_{t \\ge 0}$ on $H$, and $\\xi$ is an additive Gaussian model noise that is temporally white and spatially correlated, represented in mild form by a $Q$-Wiener process on $H$. Assume $Q$ is a self-adjoint, positive, trace-class covariance operator on $H$, and the initial condition $u_{0}$ is a Gaussian random variable on $H$ with mean $m_{0} \\in H$ and covariance $\\Sigma_{0}$, where $\\Sigma_{0}$ is self-adjoint, positive, and trace class. Let the observation times be $0  t_{1}  t_{2}  \\cdots  t_{K} \\le T$, and denote the observation space by $Y$, a separable Hilbert space. Observations are given by\n$$\ny_{i} = C u(t_{i}) + \\eta_{i}, \\quad i = 1,2,\\ldots,K,\n$$\nwhere $C: H \\to Y$ is a bounded linear observation operator and $\\eta_{i}$ are independent, identically distributed Gaussian random variables in $Y$ with mean zero and covariance $R$, where $R$ is self-adjoint, positive-definite on $Y$, and independent of $(u_{0}, \\xi)$.\n\nLet $\\mathcal{X} = C([0,T]; H)$ denote the path space equipped with the Borel $\\sigma$-algebra induced by the uniform topology. The law of the solution $u$ on $\\mathcal{X}$ under the model (with the above assumptions) defines a Gaussian prior measure on paths. Using the likelihood arising from the discrete, linear Gaussian observations, the smoothing posterior over the path $u$ is a Gaussian measure on $\\mathcal{X}$.\n\nStarting from the fundamental definitions of Gaussian measures on separable Banach spaces, the Cameron–Martin space associated with a Gaussian measure as the reproducing kernel Hilbert space (RKHS), and the mild formulation of linear parabolic PDEs driven by additive Gaussian noise via semigroup theory, construct the smoothing posterior on $\\mathcal{X}$ as a Gaussian measure. Then, identify the associated Cameron–Martin space of the posterior and give its norm explicitly as a functional of a direction $h \\in \\mathcal{X}$.\n\nYour final answer should be the explicit analytic expression for the squared Cameron–Martin norm $\\|h\\|_{\\mathrm{CM}}^{2}$ of the posterior Gaussian measure, for a direction $h$ that is absolutely continuous in time with $\\dot{h} + A h \\in L^{2}((0,T); H)$ and $h(0) \\in H$. No numerical evaluation is required; provide the exact closed-form expression. Do not include any physical units.",
            "solution": "The user wants me to solve a problem in the field of Bayesian inverse problems and data assimilation, specifically focusing on function-space methods for a linear parabolic PDE.\n\nHere's my plan:\n1.  **Validate the problem:** I need to check if the problem statement is well-posed, scientifically grounded, and meets all the criteria for a valid problem.\n    *   **Extract Givens:** List all the equations, definitions, and assumptions provided.\n    *   **Validate:** Check for any inconsistencies, missing information, or scientific flaws. The problem seems to be a standard setup in infinite-dimensional filtering and smoothing theory, so it's likely to be valid. It's a theoretical, mathematical problem, so \"scientifically grounded\" means \"mathematically rigorous\".\n        *   **PDE:** Linear parabolic PDE: $\\partial_{t} u + A u = \\xi$.\n        *   **Domain:** Bounded, open $\\mathcal{D} \\subset \\mathbb{R}^{d}$.\n        *   **State Space:** $H = L^{2}(\\mathcal{D})$.\n        *   **Operator A:** Densely defined, self-adjoint, positive-definite on $H$. Generates an analytic contraction semigroup $S(t)$.\n        *   **Noise $\\xi$:** Temporally white, spatially correlated Gaussian noise. Modeled as a $Q$-Wiener process. $Q$ is self-adjoint, positive, trace-class covariance operator on $H$.\n        *   **Initial Condition $u_0$:** Gaussian random variable on $H$. Mean $m_0 \\in H$. Covariance $\\Sigma_{0}$ (self-adjoint, positive, trace-class).\n        *   **Observations:** $y_i = C u(t_i) + \\eta_i$ for $i=1,...,K$.\n        *   **Observation times:** $0  t_1  \\dots  t_K \\le T$.\n        *   **Observation space:** $Y$, a separable Hilbert space.\n        *   **Observation operator C:** Bounded linear operator $C: H \\to Y$.\n        *   **Observation noise $\\eta_i$:** i.i.d. Gaussian on $Y$. Mean 0. Covariance $R$ (self-adjoint, positive-definite on $Y$).\n        *   **Path space:** $\\mathcal{X} = C([0,T]; H)$.\n        *   **Task:** Construct the smoothing posterior Gaussian measure on $\\mathcal{X}$ and identify its Cameron–Martin space and norm. The final answer is the explicit expression for the squared Cameron–Martin norm $\\|h\\|_{\\mathrm{CM}}^{2}$ for a suitable direction $h$.\n\n    The setup is very standard in the literature (e.g., Stuart, \"Posterior consistency for Gaussian priors,\" or the book by Hairer, Stuart, Voss, \"Analysis of SPDEs\"). It is mathematically well-defined and rigorous. All necessary components seem to be present. The problem is far from trivial, but it's a known result in the field. It's not pseudo-profound, as it requires a solid understanding of Gaussian measures on Banach spaces, SPDE theory, and Bayesian inversion. The problem is self-contained. The language is precise. The assumptions on the operators ($A$, $Q$, $\\Sigma_0$, $R$) and the noise are standard for ensuring well-posedness of both the forward problem (existence of a solution $u$) and the inverse problem (existence of a posterior measure). The problem is valid.\n\n2.  **Solve the problem:**\n    *   **Step 2a: Define the Prior Measure.** The problem states that the law of the solution $u$ is the prior measure. Let's call it $\\mu_{\\text{pr}}$. The solution $u$ to the SPDE can be written in its mild form:\n        $$u(t) = S(t) u_0 + \\int_0^t S(t-s) dW(s),$$\n        where $W(t)$ is a $Q$-Wiener process. $u$ is a Gaussian process on $\\mathcal{X} = C([0,T]; H)$. Since $u_0 \\sim \\mathcal{N}(m_0, \\Sigma_0)$ and $W$ is a Gaussian process, $u$ is a Gaussian process. The mean of the prior, let's call it $m_{\\text{pr}}(t)$, is the solution to the deterministic PDE with the mean initial condition:\n        $$\\partial_t m_{\\text{pr}} + A m_{\\text{pr}} = 0, \\quad m_{\\text{pr}}(0) = m_0.$$\n        So, $m_{\\text{pr}}(t) = S(t)m_0$.\n        The covariance of the prior is more complex. Let $v(t) = u(t) - m_{\\text{pr}}(t)$. Then $v$ satisfies:\n        $$\\partial_t v + A v = \\xi, \\quad v(0) = u_0 - m_0.$$\n        The initial condition $v(0)$ is $\\mathcal{N}(0, \\Sigma_0)$. The mild form is:\n        $$v(t) = S(t)v(0) + \\int_0^t S(t-s) dW(s).$$\n        The prior covariance operator, let's call it $C_{\\text{pr}}$, acts on functions in the dual space. Let's focus on the Cameron–Martin space, which is what the question ultimately asks for.\n        The Cameron–Martin space of a Gaussian measure is the range of the square root of its covariance operator. For a process, it's a space of paths. Let $\\mu_0$ be the law of $u(0)$, which is $\\mathcal{N}(m_0, \\Sigma_0)$. The Cameron-Martin space of $\\mu_0$ is $H_0 = \\mathrm{range}(\\Sigma_0^{1/2})$ with inner product $\\langle u, v \\rangle_{H_0} = \\langle \\Sigma_0^{-1/2} u, \\Sigma_0^{-1/2} v \\rangle_{H}$.\n        The law of the Wiener process part, let's call it $w(t) = \\int_0^t S(t-s) dW(s)$, is a Gaussian measure on $C([0,T]; H)$. Its Cameron-Martin space is the set of functions $h(t)$ of the form $h(t) = \\int_0^t S(t-s) \\phi(s) ds$ for $\\phi \\in L^2([0,T]; \\mathrm{range}(Q^{1/2}))$. This can be simplified. A path $h \\in C([0,T];H)$ is in the Cameron-Martin space of the prior (centered at zero) if and only if $h(0) \\in H_0 = \\mathrm{range}(\\Sigma_0^{1/2})$ and $h$ is absolutely continuous with $\\dot{h} + Ah = \\phi$ for some $\\phi \\in L^2([0,T]; H)$ such that $\\int_0^T \\|\\phi(s)\\|_{Q^{-1/2}}^2 ds  \\infty$. The norm is then given by $\\|h(0)\\|_{\\Sigma_0^{-1/2}}^2 + \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_{Q^{-1/2}}^2 ds$. Here, $\\|\\cdot\\|_{C^{-1/2}}$ denotes the norm in the Cameron-Martin space of a Gaussian with covariance $C$. So $\\|x\\|_{C^{-1/2}}^2 = \\|C^{-1/2}x\\|_H^2$. This is a standard result from the theory of SPDEs.\n\n    *   **Step 2b: Define the Likelihood.** The observations are $y_i = C u(t_i) + \\eta_i$. Let $y = (y_1, \\dots, y_K) \\in Y^K$. The vector $y$ is conditionally Gaussian given $u$. The likelihood potential is related to the negative log-likelihood. Let $G(u) = (Cu(t_1), ..., Cu(t_K))$. This is a linear map from $\\mathcal{X}$ to $Y^K$. The observation equation can be written as $y = G(u) + \\eta$, where $\\eta = (\\eta_1, ..., \\eta_K)$. The noise $\\eta$ has distribution $\\mathcal{N}(0, \\mathcal{R})$, where $\\mathcal{R}$ is a block diagonal operator on $Y^K$ with $R$ on each diagonal block.\n        The negative log-likelihood (up to a constant) is given by:\n        $$\\Phi(u) = \\frac{1}{2} \\sum_{i=1}^K \\|y_i - C u(t_i) \\|_R^2,$$\n        where $\\|z\\|_R^2 = \\langle z, R^{-1} z \\rangle_Y$.\n\n    *   **Step 2c: Formulate the Posterior using Bayes' Theorem.** Bayes' theorem on function space states that the posterior measure $\\mu_{\\text{post}}$ is related to the prior measure $\\mu_{\\text{pr}}$ and the likelihood potential $\\Phi(u)$ by:\n        $$\\frac{d\\mu_{\\text{post}}}{d\\mu_{\\text{pr}}}(u) \\propto \\exp(-\\Phi(u)).$$\n        Since the prior is Gaussian and the likelihood potential is quadratic in $u$, the posterior is also Gaussian. This is a key property of linear Gaussian models.\n\n    *   **Step 2d: Find the Cameron–Martin Space of the Posterior.** A general result states that if $\\mu$ is a Gaussian measure with Cameron-Martin space $H_{CM}$ and $\\mu'$ is another measure absolutely continuous with respect to $\\mu$ with Radon-Nikodym derivative $\\frac{d\\mu'}{d\\mu}(u) \\propto \\exp(-\\frac{1}{2} \\langle u, L u \\rangle + \\langle l, u \\rangle)$ for some self-adjoint operator $L$ and vector $l$, then $\\mu'$ is also Gaussian. In our case, the prior is Gaussian, and the likelihood potential is a quadratic form in $u$.\n        The posterior measure $\\mu_{\\text{post}}$ is Gaussian. Its mean $m_{\\text{post}}$ is the minimizer of the functional:\n        $$I(u) = \\frac{1}{2} \\|u - m_{\\text{pr}}\\|_{C_{\\text{pr}}^{-1}}^2 + \\Phi(u).$$\n        This functional is the negative log-posterior. Minimizing it gives the maximum a posteriori (MAP) estimate, which for a Gaussian posterior is its mean. The norm $\\|\\cdot\\|_{C_{\\text{pr}}^{-1}}$ is the Cameron-Martin norm of the prior. So,\n        $$I(u) = \\frac{1}{2} \\|u(0)-m_0\\|_{\\Sigma_0}^2 + \\frac{1}{2} \\int_0^T \\|\\dot{u}(s) + Au(s)\\|_Q^2 ds + \\frac{1}{2} \\sum_{i=1}^K \\|y_i - C u(t_i)\\|_R^2.$$\n        Here, I used the notation $\\|x\\|_C^2$ for $\\langle x, C^{-1} x \\rangle$.\n        The mean of the posterior is the minimizer of $I(u)$. The covariance of the posterior is given by the inverse of the Hessian of $I(u)$. The Cameron–Martin space of the posterior is the same as the Cameron–Martin space of the prior. What changes is the inner product (and thus the norm).\n        Let's reconsider this. A key result (e.g., Theorem 6.20 in \"An Introduction to Infinite-Dimensional Analysis\" by Da Prato and Zabczyk for the finite-time case, or its extension in the Bayesian inverse problems literature like Stuart 2010), says that if $\\mu \\sim \\mathcal{N}(m, C_{pr})$ and the posterior is given by $d\\mu_{post}/d\\mu_{pr} \\propto \\exp(-\\frac{1}{2}\\|y-Gu\\|^2_{\\mathcal{R}})$, where $G$ is a linear operator, then the posterior is Gaussian $\\mathcal{N}(m_{post}, C_{post})$ where $C_{post}^{-1} = C_{pr}^{-1} + G^* \\mathcal{R}^{-1} G$ and $m_{post} = C_{post}(C_{pr}^{-1} m + G^* \\mathcal{R}^{-1} y)$.\n        The Cameron-Martin space of a Gaussian measure is determined by its covariance operator. If two Gaussian measures are mutually absolutely continuous, they have the same Cameron-Martin space. In our case, the prior and posterior are mutually absolutely continuous because the likelihood functional $\\exp(-\\Phi(u))$ is well-behaved on the Cameron-Martin space of the prior. The space itself doesn't change, but the inner product does.\n        The Cameron-Martin norm of the prior, for a path $h \\in \\mathcal{X}$, is:\n        $$\\|h\\|_{\\text{CM,pr}}^2 = \\|h(0)\\|_{\\Sigma_0}^2 + \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds.$$\n        This is defined for $h$ in the Cameron-Martin space $H_{\\text{pr}}$. This space is the set of paths $h \\in C([0,T];H)$ such that $h(0) \\in \\text{range}(\\Sigma_0^{1/2})$ and $h(t) = S(t)h(0) + \\int_0^t S(t-s) \\phi(s) ds$ for some $\\phi \\in L^2([0,T];\\text{range}(Q^{1/2}))$. The condition on $h$ given in the problem statement, `$h$ that is absolutely continuous in time with $\\dot{h} + A h \\in L^{2}((0,T); H)$ and $h(0) \\in H$`, defines a space that is dense in or maybe equal to the Cameron-Martin space, depending on the precise definitions of the operator domains. Let's assume this defines the space we are working with. The norms $\\|\\cdot\\|_{\\Sigma_0}$ and $\\|\\cdot\\|_Q$ are shorthand for the RKHS norms associated with covariances $\\Sigma_0$ and $Q$. Explicitly, $\\|x\\|_{\\Sigma_0}^2 = \\langle x, \\Sigma_0^{-1} x \\rangle_H = \\|\\Sigma_0^{-1/2} x\\|_H^2$, and $\\|\\psi\\|_Q^2 = \\langle \\psi, Q^{-1} \\psi \\rangle_H = \\|Q^{-1/2} \\psi\\|_H^2$. So, for the prior (centered at its mean):\n        $$\\|h\\|_{\\text{CM,pr}}^2 = \\|\\Sigma_0^{-1/2} h(0)\\|_H^2 + \\int_0^T \\|Q^{-1/2}(\\dot{h}(s) + Ah(s))\\|_H^2 ds.$$\n\n    *   **Step 2e: Derive the Posterior Cameron-Martin Norm.** The posterior is formed by conditioning on the data. The negative log-posterior for a path $u$ around the posterior mean $m_{\\text{post}}$ is:\n        $$\\frac{1}{2}\\|u - m_{\\text{post}}\\|_{\\text{CM,post}}^2.$$\n        This is also equal to the negative log-posterior functional $I(u)$ up to a constant.\n        Let's consider the posterior for a centered prior, $m_0=0$. Let the centered posterior have covariance $C_{\\text{post}}$. Then for a path $h$ in the Cameron-Martin space, its norm squared is given by the quadratic form of the inverse covariance operator.\n        The posterior distribution is proportional to $\\exp(-I(u))$ where\n        $$I(u) = \\frac{1}{2} \\|u(0)\\|_{\\Sigma_0}^2 + \\frac{1}{2} \\int_0^T \\|\\dot{u}(s) + Au(s)\\|_Q^2 ds + \\frac{1}{2} \\sum_{i=1}^K \\|y_i - C u(t_i)\\|_R^2.$$\n        To find the posterior Cameron-Martin norm, we can look at the quadratic part of $I(u)$ centered around its minimizer. The posterior mean $m_{\\text{post}}$ minimizes $I(u)$. Let $u = m_{\\text{post}} + h$.\n        $$I(m_{\\text{post}}+h) = \\frac{1}{2} \\|m_{\\text{post}}(0)+h(0)\\|_{\\Sigma_0}^2 + \\frac{1}{2} \\int_0^T \\|\\dot{m}_{\\text{post}}(s) + A m_{\\text{post}}(s) + \\dot{h}(s) + A h(s)\\|_Q^2 ds + \\frac{1}{2} \\sum_{i=1}^K \\|y_i - C (m_{\\text{post}}(t_i)+h(t_i))\\|_R^2.$$\n        Expanding this will give terms quadratic in $h$, linear in $h$, and constant terms. Since $m_{\\text{post}}$ is the minimizer, the linear term in $h$ must be zero. The quadratic part in $h$ defines the posterior Cameron-Martin norm.\n        The quadratic part in $h$ is:\n        $$J(h) = \\frac{1}{2} \\|h(0)\\|_{\\Sigma_0}^2 + \\frac{1}{2} \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds + \\frac{1}{2} \\sum_{i=1}^K \\|C h(t_i)\\|_R^2.$$\n        This must be $\\frac{1}{2} \\|h\\|_{\\text{CM,post}}^2$.\n        So the squared Cameron–Martin norm for the posterior measure is:\n        $$\\|h\\|_{\\text{CM,post}}^2 = \\|h(0)\\|_{\\Sigma_0}^2 + \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds + \\sum_{i=1}^K \\|C h(t_i)\\|_R^2.$$\n\n    *   **Let's formalize the notation for clarity.**\n        -   The space $H$ is $L^2(\\mathcal{D})$. Its inner product is $\\langle \\cdot, \\cdot \\rangle_H$.\n        -   The space $Y$ is a separable Hilbert space. Its inner product is $\\langle \\cdot, \\cdot \\rangle_Y$.\n        -   The operator $\\Sigma_0$ is a covariance operator on $H$. The norm it induces is $\\|v\\|_{\\Sigma_0}^2 = \\langle v, \\Sigma_0^{-1} v \\rangle_H$. This is defined on the Cameron-Martin space of the initial condition measure, which is $\\text{range}(\\Sigma_0^{1/2})$.\n        -   The operator $Q$ is a covariance operator on $H$. The norm it induces is $\\|\\psi\\|_Q^2 = \\langle \\psi, Q^{-1} \\psi \\rangle_H$.\n        -   The operator $R$ is a covariance operator on $Y$. The norm it induces is $\\|z\\|_R^2 = \\langle z, R^{-1} z \\rangle_Y$.\n        -   The problem states $h$ is absolutely continuous with $\\dot{h} + A h \\in L^{2}((0,T); H)$ and $h(0) \\in H$. This is the space where the norm needs to be defined. This implies that the functions $\\dot{h} + Ah$ and $h(0)$ lie in the domains of $Q^{-1/2}$ and $\\Sigma_0^{-1/2}$ respectively for the norm to be finite. The question asks for the expression for the norm, so we can assume $h$ is in the Cameron-Martin space of the posterior.\n\n    *   **The derivation seems correct and follows from the general theory of Gaussian measures under conditioning.**\n        1.  The prior measure $\\mu_{\\text{pr}}$ on $\\mathcal{X}=C([0,T]; H)$ is Gaussian, with a mean path $m_{\\text{pr}}(t) = S(t)m_0$ and a covariance structure whose Cameron-Martin norm squared is $\\|h\\|_{\\text{CM,pr}}^2 = \\|h(0)\\|_{\\Sigma_0}^2 + \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds$. This is the negative of twice the logarithm of the density of the centered prior Gaussian process, evaluated at $h$. Let's call the centered prior measure $\\mu_{\\text{pr},0}$.\n        $$ \\frac{d\\mu_{\\text{pr},0}}{d\\pi}(h) \\propto \\exp\\left(-\\frac{1}{2} \\|h\\|_{\\text{CM,pr}}^2\\right), $$\n        where $\\pi$ is some notional \"flat\" measure on the path space.\n\n        2.  The likelihood of the data $y = \\{y_i\\}_{i=1}^K$ given the path $u$ is\n        $$ p(y|u) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^K \\|y_i - C u(t_i)\\|_R^2\\right) = \\exp(-\\Phi(u;y)). $$\n        The notation $\\Phi(u;y)$ emphasizes dependence on data $y$.\n\n        3.  By Bayes' theorem, the posterior measure $\\mu_{\\text{post}}$ has a density with respect to the prior measure $\\mu_{\\text{pr}}$:\n        $$ \\frac{d\\mu_{\\text{post}}}{d\\mu_{\\text{pr}}}(u) \\propto \\exp(-\\Phi(u;y)). $$\n        For the centered measures (and absorbing the mean into the definition of $y_i$), we can write\n        $$ \\frac{d\\mu_{\\text{post},0}}{d\\mu_{\\text{pr},0}}(u) \\propto \\exp(-\\Phi(u;y)). $$\n        The density of the posterior with respect to the flat measure $\\pi$ is then\n        $$ \\frac{d\\mu_{\\text{post},0}}{d\\pi}(u) \\propto \\frac{d\\mu_{\\text{pr},0}}{d\\pi}(u) \\frac{d\\mu_{\\text{post},0}}{d\\mu_{\\text{pr},0}}(u) \\propto \\exp\\left(-\\frac{1}{2} \\|u\\|_{\\text{CM,pr}}^2 - \\Phi(u;y)\\right). $$\n        The term in the exponent is the negative log-posterior (up to a factor of 2 and constants).\n        $$ I(u) = \\frac{1}{2}\\|u\\|_{\\text{CM,pr}}^2 + \\Phi(u;y) = \\frac{1}{2} \\left( \\|u(0)\\|_{\\Sigma_0}^2 + \\int_0^T \\|\\dot{u}(s) + Au(s)\\|_Q^2 ds \\right) + \\frac{1}{2} \\sum_{i=1}^K \\|y_i - C u(t_i)\\|_R^2. $$\n\n        4.  The posterior is Gaussian. The mean of the posterior $m_{\\text{post}}$ is the unique minimizer of $I(u)$. The posterior covariance structure is determined by the quadratic part of $I(u)$ around the mean.\n        Let $u = m_{\\text{post}} + h$.\n        $$ I(m_{\\text{post}}+h) = I(m_{\\text{post}}) + \\nabla I(m_{\\text{post}}) \\cdot h + \\frac{1}{2} \\nabla^2 I(m_{\\text{post}}) [h,h] + \\dots $$\n        Since $m_{\\text{post}}$ is the minimizer, $\\nabla I(m_{\\text{post}}) \\cdot h = 0$.\n        The quadratic part is $\\frac{1}{2} \\nabla^2 I(m_{\\text{post}}) [h,h]$. This Hessian is independent of the point of evaluation for a quadratic functional.\n        Let's compute the quadratic part by substituting $u=h$ and setting $y_i=0$ in $I(u)$.\n        $$ J(h) = \\frac{1}{2} \\left( \\|h(0)\\|_{\\Sigma_0}^2 + \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds + \\sum_{i=1}^K \\|C h(t_i)\\|_R^2 \\right). $$\n        The Cameron-Martin norm of the posterior is defined such that $\\frac{1}{2}\\|h\\|_{\\text{CM,post}}^2 = J(h)$.\n        Therefore,\n        $$ \\|h\\|_{\\text{CM,post}}^2 = \\|h(0)\\|_{\\Sigma_0}^2 + \\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds + \\sum_{i=1}^K \\|C h(t_i)\\|_R^2. $$\n        Let's expand the norms using inner products to be fully explicit.\n        $\\|h(0)\\|_{\\Sigma_0}^2 = \\langle \\Sigma_0^{-1/2} h(0), \\Sigma_0^{-1/2} h(0) \\rangle_H = \\langle h(0), \\Sigma_0^{-1} h(0) \\rangle_H$.\n        $\\int_0^T \\|\\dot{h}(s) + Ah(s)\\|_Q^2 ds = \\int_0^T \\langle \\dot{h}(s) + Ah(s), Q^{-1}(\\dot{h}(s) + Ah(s)) \\rangle_H ds$.\n        $\\sum_{i=1}^K \\|C h(t_i)\\|_R^2 = \\sum_{i=1}^K \\langle C h(t_i), R^{-1} C h(t_i) \\rangle_Y$.\n        Using the adjoint of $C$, denoted $C^*: Y \\to H$:\n        $\\langle C h(t_i), R^{-1} C h(t_i) \\rangle_Y = \\langle h(t_i), C^* R^{-1} C h(t_i) \\rangle_H$.\n        So the full expression is:\n        $$ \\|h\\|_{\\text{CM,post}}^2 = \\langle h(0), \\Sigma_0^{-1} h(0) \\rangle_H + \\int_0^T \\langle \\dot{h}(s) + Ah(s), Q^{-1}(\\dot{h}(s) + Ah(s)) \\rangle_H ds + \\sum_{i=1}^K \\langle h(t_i), C^* R^{-1} C h(t_i) \\rangle_H.$$\n        This looks correct and complete. The problem asks for the explicit analytic expression for the squared Cameron–Martin norm $\\|h\\|_{\\mathrm{CM}}^{2}$ of the posterior Gaussian measure. The subscript on the norm in the question is just CM, not post, but it's clear from context. I will use the notation $\\|h\\|_{\\mathrm{CM,post}}^2$ in my derivation and then drop the subscript for the final answer as requested.\n\n    *   **Final checks:**\n        *   All symbols used come from the problem statement or are standard mathematical notation.\n        *   The logic follows from standard theory of Gaussian measures on Banach spaces and Bayesian inverse problems for SPDEs.\n        *   The form of the question implies that the Cameron-Martin spaces of the prior and posterior are the same, which is true in this case (they are equivalent measures). The norm is what changes.\n        *   The condition on $h$ (absolutely continuous, etc.) ensures that all terms in the norm expression are well-defined (at least for $h$ within the CM space). For an arbitrary $h$ in the space described, the norm could be infinite. The CM space is precisely the subspace where this norm is finite.\n        *   The problem is purely symbolic, no numerical values.\n        *   The final expression must be in LaTeX.\n\n    *   **Structure of the solution:**\n        1.  Start with the problem validation (which I've done, it's valid).\n        2.  Introduce the concept of a Gaussian measure on a function space and its relation to the mild solution of the SPDE. This defines the prior measure.\n        3.  State the Cameron-Martin norm of the prior measure, derived from the covariance of the initial condition and the driving noise process.\n        4.  Introduce the observation model and the resulting likelihood function. This is a Gaussian likelihood.\n        5.  Apply Bayes' theorem to combine the Gaussian prior and Gaussian likelihood. Explain that the posterior is also Gaussian.\n        6.  Derive the negative log-posterior functional.\n        7.  Identify the quadratic part of this functional with respect to its minimizer (the posterior mean). This quadratic part, evaluated at a function $h$, gives half the squared Cameron-Martin norm of the posterior for the direction $h$.\n        8.  Write down the final expression for the squared norm.\n\nI will write the solution following these steps, ensuring all mathematical entities are in LaTeX. The language should be precise and formal, as befits a professor. The final answer will be enclosed in `\\boxed{...}`.\nThe expression should be:\n$$ \\langle h(0), \\Sigma_0^{-1} h(0) \\rangle_H + \\int_0^T \\langle \\dot{h}(s) + A h(s), Q^{-1} (\\dot{h}(s) + A h(s)) \\rangle_H ds + \\sum_{i=1}^{K} \\langle h(t_i), C^* R^{-1} C h(t_i) \\rangle_H $$\nThis is what I have derived. It's a sum of three terms, corresponding to the initial condition uncertainty, the dynamical noise uncertainty, and the observation uncertainty. The last term is added to the prior's Cameron-Martin norm to get the posterior's. This is intuitively correct: the posterior is 'more certain' (has smaller variance), which corresponds to a stronger norm on the Cameron-Martin space (a larger inverse covariance).\nThe operators $\\Sigma_0^{-1}$, $Q^{-1}$, $R^{-1}$ are well-defined on the ranges of $\\Sigma_0$, $Q$, $R$ respectively. For the norm to be finite, $h(0)$ must be in the domain of $\\Sigma_0^{-1/2}$, $\\dot{h}+Ah$ must have values in the domain of $Q^{-1/2}$, etc. The question specifies $h$ is in a space where these operations are sensible.\nThe notation $\\|h\\|_{\\mathrm{CM}}^{2}$ is requested in the prompt, so I will use that for the final answer. In the derivation it is clearer to distinguish between prior and posterior norms. I will use $\\|h\\|_{\\mathrm{CM, pr}}^2$ and $\\|h\\|_{\\mathrm{CM, post}}^2$.\nThe question asks for the norm of a direction $h$. It's a functional of $h$. So the variable $h$ and its derivatives and evaluations must appear in the final expression.\n\nThe problem formulation asks for $\\|h\\|_{\\mathrm{CM}}^{2}$ for the posterior measure. The steps are clear. I will write them down carefully.\nThe formulation \"construct the smoothing posterior on $\\mathcal{X}$ as a Gaussian measure\" is a key instruction. My approach does exactly this by characterising it via the negative log-posterior functional, which for a Gaussian measure is a quadratic plus a linear term. The quadratic part defines the inverse covariance and hence the Cameron-Martin norm.",
            "answer": "$$ \\boxed{\\langle h(0), \\Sigma_0^{-1} h(0) \\rangle_{H} + \\int_{0}^{T} \\langle \\dot{h}(s) + A h(s), Q^{-1}(\\dot{h}(s) + A h(s)) \\rangle_{H} ds + \\sum_{i=1}^{K} \\langle h(t_i), C^{*}R^{-1}C h(t_i) \\rangle_{H}} $$"
        },
        {
            "introduction": "Once a posterior measure is well-defined, we must ask if our inference is statistically meaningful. A key aspect of well-posedness is \"posterior consistency\": does the posterior distribution concentrate around the true underlying function as we collect more data? This practice delves into this question by analyzing the posterior contraction rate for a canonical linear inverse problem . You will derive how the speed of learning, $r_n$, depends on the interplay between the assumed prior regularity ($\\alpha$), the intrinsic difficulty of the problem (ill-posedness, $\\kappa$), and the number of data points ($n$), offering a sharp, quantitative insight into statistical efficiency in infinite dimensions.",
            "id": "3383876",
            "problem": "Consider a function-space Bayesian inverse problem posed on a separable Hilbert space $X$ with inner product $\\langle \\cdot,\\cdot \\rangle_X$ and norm $\\|\\cdot\\|_X$. Let $Y$ be another separable Hilbert space and let $\\mathcal{G}:X \\to Y$ be a bounded linear forward map. Assume that the unknown $u^\\dagger \\in X$ is observed through independent and identically distributed (i.i.d.) data $y_i \\in Y$ satisfying\n$$\ny_i \\,=\\, \\mathcal{G}(u^\\dagger) \\,+\\, \\eta_i,\\quad i=1,\\dots,n,\n$$\nwhere $\\eta_i \\sim \\mathcal{N}(0,\\Gamma)$ are i.i.d. centered Gaussian noise with known covariance operator $\\Gamma:Y\\to Y$ that is strictly positive and bounded. Assume a centered Gaussian prior on $X$ given by $u\\sim\\mathcal{N}(0,C_0)$, where the covariance operator $C_0:X\\to X$ is self-adjoint, strictly positive, and trace class. Suppose there exists an orthonormal basis $\\{e_j\\}_{j\\ge 1}$ of $X$ that simultaneously diagonalizes the operators $C_0$ and $\\mathcal{G}^*\\Gamma^{-1}\\mathcal{G}$, where $\\mathcal{G}^*:Y\\to X$ denotes the adjoint of $\\mathcal{G}$. In this basis, assume the following spectral regularity and ill-posedness structure:\n- Prior regularity: $C_0 e_j \\,=\\, \\tau_j^2 e_j$ with $\\tau_j^2 \\asymp j^{-(1+2\\alpha)}$ for some $\\alpha0$.\n- Ill-posedness: $(\\mathcal{G}^*\\Gamma^{-1}\\mathcal{G}) e_j \\,=\\, \\lambda_j^2 e_j$ with $\\lambda_j \\asymp j^{-\\kappa}$ for some $\\kappa0$.\nAssume the data are well-specified in the sense that $\\mathcal{G}(u^\\dagger)$ is the true mean and that $u^\\dagger$ has sufficient smoothness so that posterior bias does not dominate posterior variance in the $X$-norm at the rates implied by the prior (for instance, $u^\\dagger$ lies in the Cameron–Martin space of the prior at a regularity level no less than $\\alpha$).\n\nStarting from fundamental properties of Gaussian measures on Hilbert spaces, the conjugacy of the Gaussian prior–likelihood, and the spectral decomposition above, derive the asymptotic posterior contraction rate in $\\|\\cdot\\|_X$ as $n\\to\\infty$; that is, find a sequence $r_n$ such that for any fixed $M0$ the posterior probability of the ball $\\{u\\in X:\\|u-u^\\dagger\\|_X \\le M r_n\\}$ converges to $1$ as $n\\to\\infty$. Express your final answer as a single closed-form analytic expression in $n$, $\\alpha$, and $\\kappa$, ignoring multiplicative constants and any slowly varying factors. No intermediate formulas should be stated in the final answer. No rounding is required, and no physical units are involved. Your final answer must be a calculation: a single explicit expression for $r_n$.",
            "solution": "The user has requested the derivation of the asymptotic posterior contraction rate for a function-space Bayesian inverse problem.\n\n### 1. Problem Formulation and Posterior Distribution\n\nThe problem is defined by the data model for $n$ independent and identically distributed (i.i.d.) observations $y_i \\in Y$:\n$$ y_i = \\mathcal{G}(u^\\dagger) + \\eta_i, \\quad i=1,\\dots,n $$\nwhere $u^\\dagger \\in X$ is the unknown true function, $\\mathcal{G}: X \\to Y$ is a bounded linear forward map, and $\\eta_i \\sim \\mathcal{N}(0, \\Gamma)$ are i.i.d. Gaussian noise instances.\n\nCombining the $n$ observations by taking their mean, $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$, we obtain an effective data model:\n$$ \\bar{y} = \\mathcal{G}(u^\\dagger) + \\bar{\\eta} $$\nwhere the averaged noise $\\bar{\\eta} = \\frac{1}{n}\\sum_{i=1}^n \\eta_i$ follows a Gaussian distribution $\\mathcal{N}(0, \\frac{1}{n}\\Gamma)$. This leads to a likelihood potential proportional to $\\exp(-\\frac{n}{2}\\|\\bar{y} - \\mathcal{G}(u)\\|_{\\Gamma^{-1}}^2)$, where $\\|\\cdot\\|_{\\Gamma^{-1}}^2 = \\langle \\Gamma^{-1}(\\cdot), \\cdot \\rangle_Y$.\n\nThe prior on the unknown is a centered Gaussian measure $u \\sim \\mathcal{N}(0, C_0)$, where $C_0$ is a self-adjoint, strictly positive, trace-class operator. Its potential is proportional to $\\exp(-\\frac{1}{2}\\|u\\|_{C_0^{-1}}^2)$, where $\\|u\\|_{C_0^{-1}}^2 = \\langle C_0^{-1}u, u \\rangle_X$.\n\nDue to the conjugacy of the Gaussian prior and Gaussian likelihood, the posterior distribution $\\Pi_n(\\cdot | \\bar{y})$ is also Gaussian, which we denote as $\\mathcal{N}(u_{post}, C_{post})$. The posterior precision operator is the sum of the prior precision and the data precision (scaled by $n$):\n$$ C_{post}^{-1} = C_0^{-1} + n \\mathcal{G}^* \\Gamma^{-1} \\mathcal{G} $$\nThe posterior covariance operator is the inverse of the precision operator, $C_{post} = (C_0^{-1} + n \\mathcal{G}^* \\Gamma^{-1} \\mathcal{G})^{-1}$. The posterior mean is given by $u_{post} = C_{post}(n\\mathcal{G}^*\\Gamma^{-1}\\bar{y})$.\n\n### 2. Spectral Analysis of the Posterior Covariance\n\nThe problem states the existence of an orthonormal basis $\\{e_j\\}_{j \\ge 1}$ of $X$ that simultaneously diagonalizes $C_0$ and $\\mathcal{G}^*\\Gamma^{-1}\\mathcal{G}$. The eigenvalues are given by:\n- $C_0 e_j = \\tau_j^2 e_j$ with $\\tau_j^2 \\asymp j^{-(1+2\\alpha)}$ for $\\alpha  0$.\n- $(\\mathcal{G}^*\\Gamma^{-1}\\mathcal{G}) e_j = \\lambda_j^2 e_j$ with $\\lambda_j \\asymp j^{-\\kappa}$ for $\\kappa  0$.\n\nHere, $a_j \\asymp b_j$ means that there exist constants $c_1, c_2  0$ such that $c_1 b_j \\le a_j \\le c_2 b_j$ for all sufficiently large $j$. In this basis, the inverse operators are also diagonal with eigenvalues $\\tau_j^{-2}$ and $\\lambda_j^2$ respectively.\n\nThe posterior precision operator $C_{post}^{-1}$ is therefore diagonal in this basis with eigenvalues:\n$$ \\langle C_{post}^{-1} e_j, e_j \\rangle_X = \\tau_j^{-2} + n \\lambda_j^2 $$\nThe posterior covariance operator $C_{post}$ is also diagonal, with eigenvalues $\\sigma_{j,n}^2$:\n$$ \\sigma_{j,n}^2 = \\langle C_{post} e_j, e_j \\rangle_X = (\\tau_j^{-2} + n \\lambda_j^2)^{-1} = \\frac{\\tau_j^2}{1 + n \\lambda_j^2 \\tau_j^2} $$\n\n### 3. Posterior Contraction Rate Analysis\n\nThe posterior contraction rate $r_n$ is determined by the asymptotic behavior of the squared error, averaged over both the posterior distribution and the data-generating distribution. The mean squared error (MSE) is given by:\n$$ \\text{MSE}(n) = \\mathbb{E}_{\\bar{y}|u^\\dagger} \\left[ \\mathbb{E}_{u|\\bar{y}} \\left[ \\|u-u^\\dagger\\|_X^2 \\right] \\right] $$\nThe inner expectation (over the posterior) is:\n$$ \\mathbb{E}_{u|\\bar{y}} \\left[ \\|u-u^\\dagger\\|_X^2 \\right] = \\mathbb{E}_{u|\\bar{y}} \\left[ \\|(u-u_{post}) + (u_{post}-u^\\dagger)\\|_X^2 \\right] = \\|u-u_{post}\\|_X^2 + \\|u_{post}-u^\\dagger\\|_X^2 $$\nThe expectation of the first term is the trace of the posterior covariance: $\\mathbb{E}_{u|\\bar{y}} [\\|u-u_{post}\\|_X^2] = \\text{Tr}(C_{post})$. The second term is the squared norm of the (data-dependent) bias.\nTaking the expectation over the data $\\bar{y}$ gives:\n$$ \\text{MSE}(n) = \\text{Tr}(C_{post}) + \\mathbb{E}_{\\bar{y}|u^\\dagger} \\left[ \\|u_{post} - u^\\dagger\\|_X^2 \\right] $$\nThe second term can be decomposed into squared deterministic bias and variance:\n$$ \\mathbb{E}_{\\bar{y}|u^\\dagger} \\left[ \\|u_{post} - u^\\dagger\\|_X^2 \\right] = \\|\\mathbb{E}_{\\bar{y}|u^\\dagger}[u_{post}] - u^\\dagger\\|_X^2 + \\text{Tr}(\\text{Cov}_{\\bar{y}}(u_{post})) $$\nThe posterior contraction rate $r_n$ is such that $\\text{MSE}(n) \\asymp r_n^2$. We analyze each of the error components.\n\n**a) Integrated Posterior Variance:** $\\text{Tr}(C_{post})$\nThe trace is the sum of the eigenvalues of $C_{post}$:\n$$ \\text{Tr}(C_{post}) = \\sum_{j=1}^{\\infty} \\sigma_{j,n}^2 = \\sum_{j=1}^{\\infty} \\frac{\\tau_j^2}{1 + n \\lambda_j^2 \\tau_j^2} $$\nUsing the asymptotic forms of the eigenvalues, the term in the denominator scales as $n \\lambda_j^2 \\tau_j^2 \\asymp n (j^{-\\kappa})^2 (j^{-(1+2\\alpha)}) = n j^{-(1+2\\alpha+2\\kappa)}$. This term balances with $1$ at a cutoff frequency index $J_n$ where $n J_n^{-(1+2\\alpha+2\\kappa)} \\approx 1$, which gives $J_n \\asymp n^{1/(1+2\\alpha+2\\kappa)}$.\n\nWe split the sum into two parts: $j \\le J_n$ (data-dominated) and $j  J_n$ (prior-dominated).\n- For $j \\le J_n$, $n\\lambda_j^2\\tau_j^2 \\gg 1$, so $\\sigma_{j,n}^2 \\approx \\frac{\\tau_j^2}{n\\lambda_j^2\\tau_j^2} = \\frac{1}{n\\lambda_j^2} \\asymp \\frac{1}{n} j^{2\\kappa}$.\n- For $j  J_n$, $n\\lambda_j^2\\tau_j^2 \\ll 1$, so $\\sigma_{j,n}^2 \\approx \\tau_j^2 \\asymp j^{-(1+2\\alpha)}$.\n\nApproximating the sum with an integral:\n$$ \\text{Tr}(C_{post}) \\approx \\int_1^{J_n} \\frac{1}{n} j^{2\\kappa} dj + \\int_{J_n}^{\\infty} j^{-(1+2\\alpha)} dj $$\nThe first integral is $\\frac{1}{n(2\\kappa+1)}[j^{2\\kappa+1}]_1^{J_n} \\asymp \\frac{1}{n} J_n^{2\\kappa+1} \\asymp n^{-1} (n^{1/(1+2\\alpha+2\\kappa)})^{2\\kappa+1} = n^{-2\\alpha/(1+2\\alpha+2\\kappa)}$.\nThe second integral is $\\frac{1}{2\\alpha}[j^{-2\\alpha}]_{J_n}^{\\infty} \\asymp J_n^{-2\\alpha} \\asymp (n^{1/(1+2\\alpha+2\\kappa)})^{-2\\alpha} = n^{-2\\alpha/(1+2\\alpha+2\\kappa)}$.\nBoth parts scale with the same rate. Thus,\n$$ \\text{Tr}(C_{post}) \\asymp n^{-\\frac{2\\alpha}{1+2\\alpha+2\\kappa}} $$\n\n**b) Posterior Bias:** $\\|\\mathbb{E}[u_{post}] - u^\\dagger\\|_X^2$\nThe problem states that \"$u^\\dagger$ has sufficient smoothness so that posterior bias does not dominate posterior variance\". This is a crucial assumption which implies that the squared bias term decays at least as fast as the integrated variance term we just calculated. Therefore, we do not need to compute its rate explicitly, as it will not determine the final contraction rate.\n$$ \\|\\mathbb{E}[u_{post}] - u^\\dagger\\|_X^2 = O\\left(n^{-\\frac{2\\alpha}{1+2\\alpha+2\\kappa}}\\right) $$\n\n**c) Variance of the Posterior Mean:** $\\text{Tr}(\\text{Cov}_{\\bar{y}}(u_{post}))$\nA similar analysis can be performed for this term. It can be shown that this variance component also scales at the same rate as the integrated variance. For brevity, we note that in this standard linear Gaussian setting, the error is typically balanced across these sources. The dominant contribution to the squared error comes from the modes that are at the transition between being prior-dominated and data-dominated.\n\nCombining the components, the total mean squared error is dominated by the variance terms:\n$$ \\text{MSE}(n) \\asymp n^{-\\frac{2\\alpha}{1+2\\alpha+2\\kappa}} $$\n\nThe posterior contraction rate $r_n$ is the square root of the MSE rate:\n$$ r_n \\asymp \\left( n^{-\\frac{2\\alpha}{1+2\\alpha+2\\kappa}} \\right)^{1/2} = n^{-\\frac{\\alpha}{1+2\\alpha+2\\kappa}} $$\nThis sequence $r_n$ describes the radius of a ball in the $X$-norm centered at the true value $u^\\dagger$ to which the posterior measure concentrates as the number of data points $n$ goes to infinity.",
            "answer": "$$\\boxed{n^{-\\frac{\\alpha}{1+2\\alpha+2\\kappa}}}$$"
        },
        {
            "introduction": "While theoretical guarantees are vital, practical Bayesian inference hinges on our ability to compute with the posterior measure, often via Markov Chain Monte Carlo (MCMC) sampling. This final practice uncovers a critical challenge known as the \"curse of dimensionality,\" which plagues many naive algorithms when applied in function spaces. By analyzing the simple Random-Walk Metropolis algorithm, you will see precisely why its efficiency collapses as the dimension of the problem grows . This analysis highlights the pathological geometry of high-dimensional spaces and motivates the development of sophisticated, function-space aware samplers that remain efficient as the discretization is refined.",
            "id": "3383910",
            "problem": "Consider a Bayesian inverse problem on a separable Hilbert space $\\mathcal{H}$ with Gaussian prior $\\mu_{0} = \\mathcal{N}(0, \\mathcal{C})$. Suppose the likelihood is dominated by the prior and that, for the purpose of analyzing Markov Chain Monte Carlo (MCMC) behavior, we examine the prior-preserving setting where the target is the prior itself. In finite-dimensional projections $\\mathbb{R}^{d}$ with covariance $\\mathcal{C}_{d} = I_{d}$, the Random-Walk Metropolis (RWM) proposal has the form $u' = u + \\epsilon \\,\\xi$ where $u \\in \\mathbb{R}^{d}$ is the current state, $\\xi \\sim \\mathcal{N}(0, I_{d})$, and $\\epsilon  0$ is a step-size parameter. The Metropolis–Hastings (MH) acceptance probability is given by $\\alpha(u,u') = \\min\\{1, \\exp(-\\Delta \\Phi)\\}$, where $\\Delta \\Phi$ is the change in the negative log-target between $u'$ and $u$. \n\nStarting from first principles for Gaussian measures and the Metropolis–Hastings algorithm, do the following:\n\n1. Derive, under the stationarity assumption $u \\sim \\mathcal{N}(0, I_{d})$, an exact expression for the expected acceptance probability of the RWM proposal in dimension $d$, expressed in terms of the standard normal cumulative distribution function $\\Phi$ and the Euclidean norm $\\|\\xi\\|$. Then, use the law of large numbers for $\\|\\xi\\|$ to obtain an explicit asymptotic expression for the expected acceptance probability as $d \\to \\infty$ with fixed $\\epsilon$.\n\n2. Interpret the asymptotic expression to explain why the RWM acceptance probability vanishes as $d \\to \\infty$ with fixed $\\epsilon$, and explain how to rescale $\\epsilon$ with $d$ to obtain a nontrivial limit. Your explanation must connect to the function-space viewpoint, including the role of the Cameron–Martin space and mutual singularity of Gaussian measures in infinite dimensions.\n\nProvide your final result as a single closed-form analytic expression giving the asymptotic expected acceptance probability as a function of $\\epsilon$ and $d$. No rounding is required. Do not include units. Your final answer must be a single expression.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-contained. It is a standard theoretical problem in the analysis of MCMC algorithms in high dimensions, a topic of central importance in function-space inverse problems.\n\nThe solution is developed in two parts as requested by the problem statement.\n\n### Part 1: Derivation of the Expected Acceptance Probability\n\nThe target distribution in $\\mathbb{R}^{d}$ is a standard multivariate normal distribution, $\\pi(u) \\propto \\exp(-\\frac{1}{2}\\|u\\|^2)$, where $\\|u\\|$ is the standard Euclidean norm. The negative log-target density, up to an additive constant, is $\\Phi(u) = \\frac{1}{2}\\|u\\|^2$.\n\nThe Random-Walk Metropolis (RWM) proposal is given by $u' = u + \\epsilon \\xi$, where $u \\in \\mathbb{R}^{d}$ is the current state, $\\xi \\sim \\mathcal{N}(0, I_d)$ is a random perturbation independent of $u$, and $\\epsilon  0$ is the step size.\n\nThe change in the negative log-target density, $\\Delta \\Phi$, is:\n$$\n\\Delta \\Phi = \\Phi(u') - \\Phi(u) = \\frac{1}{2}\\|u'\\|^2 - \\frac{1}{2}\\|u\\|^2\n$$\nSubstituting $u' = u + \\epsilon \\xi$:\n$$\n\\Delta \\Phi = \\frac{1}{2}\\|u + \\epsilon \\xi\\|^2 - \\frac{1}{2}\\|u\\|^2 = \\frac{1}{2}(\\langle u + \\epsilon \\xi, u + \\epsilon \\xi \\rangle - \\|u\\|^2)\n$$\n$$\n\\Delta \\Phi = \\frac{1}{2}(\\|u\\|^2 + 2\\epsilon\\langle u, \\xi \\rangle + \\epsilon^2\\|\\xi\\|^2 - \\|u\\|^2) = \\epsilon\\langle u, \\xi \\rangle + \\frac{1}{2}\\epsilon^2\\|\\xi\\|^2\n$$\nThe proposal distribution $q(u'|u)$ is symmetric, as it depends only on the difference $u' - u$. Therefore, the Metropolis-Hastings acceptance probability is:\n$$\n\\alpha(u, u') = \\min\\left\\{1, \\frac{\\pi(u')}{\\pi(u)}\\right\\} = \\min\\{1, \\exp(-\\Delta \\Phi)\\} = \\min\\left\\{1, \\exp\\left(-\\left(\\epsilon\\langle u, \\xi \\rangle + \\frac{1}{2}\\epsilon^2\\|\\xi\\|^2\\right)\\right)\\right\\}\n$$\nWe need to compute the expected acceptance probability, $E[\\alpha(u, u')]$. The expectation is taken over the joint distribution of $u$ and $\\xi$. By assumption of stationarity, the current state $u$ is drawn from the target distribution, so $u \\sim \\mathcal N(0, I_d)$. The proposal increment is $\\xi \\sim \\mathcal{N}(0, I_d)$, and $u$ and $\\xi$ are independent.\n\nWe can compute the expectation by first conditioning on $\\xi$:\n$$\nE[\\alpha] = E_{\\xi} \\left[ E_{u|\\xi} [\\alpha(u, u')] \\right]\n$$\nFor a fixed $\\xi$, the term $\\langle u, \\xi \\rangle = \\sum_{i=1}^d u_i \\xi_i$ is a linear combination of independent standard normal random variables $u_i$. Thus, $\\langle u, \\xi \\rangle$ is a Gaussian random variable. Its mean is $E[\\langle u, \\xi \\rangle | \\xi] = \\sum_{i=1}^d E[u_i]\\xi_i = 0$. Its variance is $Var(\\langle u, \\xi \\rangle | \\xi) = \\sum_{i=1}^d Var(u_i) \\xi_i^2 = \\sum_{i=1}^d (1) \\xi_i^2 = \\|\\xi\\|^2$.\nSo, conditional on $\\xi$, we have $\\langle u, \\xi \\rangle \\sim \\mathcal{N}(0, \\|\\xi\\|^2)$.\n\nLet $X = \\Delta \\Phi = \\epsilon\\langle u, \\xi \\rangle + \\frac{1}{2}\\epsilon^2\\|\\xi\\|^2$. Conditional on $\\xi$, $X$ is a Gaussian random variable, $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with:\n$$\n\\mu = E[X|\\xi] = \\epsilon E[\\langle u, \\xi \\rangle | \\xi] + \\frac{1}{2}\\epsilon^2\\|\\xi\\|^2 = \\frac{1}{2}\\epsilon^2\\|\\xi\\|^2\n$$\n$$\n\\sigma^2 = Var(X|\\xi) = \\epsilon^2 Var(\\langle u, \\xi \\rangle | \\xi) = \\epsilon^2 \\|\\xi\\|^2\n$$\nThe conditional expected acceptance probability is $E_{u|\\xi}[\\min\\{1, \\exp(-X)\\}]$, for $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. A known result for this expectation is:\n$$\nE[\\min\\{1, \\exp(-X)\\}] = \\Phi\\left(-\\frac{\\mu}{\\sigma}\\right) + \\exp\\left(-\\mu + \\frac{\\sigma^2}{2}\\right) \\Phi\\left(\\frac{\\mu - \\sigma^2}{\\sigma}\\right)\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$.\nSubstituting our expressions for $\\mu$ and $\\sigma$:\n$$\n\\frac{\\mu}{\\sigma} = \\frac{\\frac{1}{2}\\epsilon^2\\|\\xi\\|^2}{\\epsilon\\|\\xi\\|} = \\frac{1}{2}\\epsilon\\|\\xi\\|\n$$\n$$\n\\frac{\\mu - \\sigma^2}{\\sigma} = \\frac{\\frac{1}{2}\\epsilon^2\\|\\xi\\|^2 - \\epsilon^2\\|\\xi\\|^2}{\\epsilon\\|\\xi\\|} = -\\frac{1}{2}\\epsilon\\|\\xi\\|\n$$\n$$\n-\\mu + \\frac{\\sigma^2}{2} = -\\frac{1}{2}\\epsilon^2\\|\\xi\\|^2 + \\frac{\\epsilon^2\\|\\xi\\|^2}{2} = 0\n$$\nSubstituting these into the identity, we get the conditional expectation:\n$$\nE_{u|\\xi}[\\alpha] = \\Phi\\left(-\\frac{1}{2}\\epsilon\\|\\xi\\|\\right) + \\exp(0) \\Phi\\left(-\\frac{1}{2}\\epsilon\\|\\xi\\|\\right) = 2\\Phi\\left(-\\frac{1}{2}\\epsilon\\|\\xi\\|\\right)\n$$\nThis is the exact expression for the conditional expected acceptance probability. The total expected acceptance probability is the expectation of this quantity over $\\xi$:\n$$\nE[\\alpha] = E_{\\xi}\\left[2\\Phi\\left(-\\frac{1}{2}\\epsilon\\|\\xi\\|\\right)\\right]\n$$\nTo find the asymptotic expression for large $d$, we use the law of large numbers. The variable $\\|\\xi\\|^2 = \\sum_{i=1}^d \\xi_i^2$ is a sum of $d$ i.i.d. $\\chi^2_1$ variables, so $\\|\\xi\\|^2 \\sim \\chi^2_d$. The mean of $\\xi_i^2$ is $1$. By the LLN, $\\frac{1}{d}\\|\\xi\\|^2 \\to 1$ in probability as $d \\to \\infty$. This implies that $\\|\\xi\\|/\\sqrt{d} \\to 1$ in probability. For large $d$, $\\|\\xi\\|$ is tightly concentrated around $\\sqrt{d}$.\nSince $\\Phi(\\cdot)$ is a continuous and bounded function, we can approximate the expectation by evaluating the function at the limiting value of its argument.\n$$\nE[\\alpha] \\approx 2\\Phi\\left(-\\frac{1}{2}\\epsilon E[\\|\\xi\\|]\\right) \\approx 2\\Phi\\left(-\\frac{1}{2}\\epsilon \\sqrt{d}\\right)\n$$\nThis is the asymptotic expression for the expected acceptance probability.\n\n### Part 2: Interpretation and Rescaling\n\nThe asymptotic expression for the expected acceptance probability is $\\bar{\\alpha}(d, \\epsilon) \\approx 2\\Phi(-\\frac{1}{2}\\epsilon\\sqrt{d})$.\n\nIf the step size $\\epsilon$ is held fixed as the dimension $d \\to \\infty$, the argument of the CDF, $-\\frac{1}{2}\\epsilon\\sqrt{d}$, goes to $-\\infty$. Since $\\lim_{x \\to -\\infty} \\Phi(x) = 0$, the expected acceptance probability vanishes:\n$$\n\\lim_{d \\to \\infty} \\bar{\\alpha}(d, \\epsilon) = 0 \\quad (\\text{for fixed } \\epsilon  0)\n$$\nThis phenomenon is a manifestation of the \"curse of dimensionality\". The RWM algorithm becomes exceedingly inefficient in high dimensions, with almost all proposals being rejected.\n\nTo obtain a nontrivial acceptance probability (i.e., a limit strictly between $0$ and $1$), the argument of $\\Phi$ must converge to a finite negative constant, say $-c$. This requires:\n$$\n\\frac{1}{2}\\epsilon\\sqrt{d} \\to c \\quad \\text{as } d \\to \\infty\n$$\nThis implies that $\\epsilon\\sqrt{d}$ must be asymptotically constant. We must therefore rescale the step size $\\epsilon$ with dimension $d$ according to:\n$$\n\\epsilon = \\frac{\\ell}{\\sqrt{d}}\n$$\nfor some constant $\\ell  0$. With this scaling, the limiting acceptance probability becomes $\\bar{\\alpha}_{\\infty} = 2\\Phi(-\\frac{\\ell}{2})$, which is a well-defined value in $(0,1)$. The optimal value of $\\ell \\approx 2.38$ yielding $\\bar{\\alpha}_{\\infty} \\approx 0.234$ is a celebrated result in the MCMC literature.\n\nThis scaling has a clear interpretation from the function-space perspective. The MCMC sampler explores a space of functions, which are elements of an infinite-dimensional Hilbert space $\\mathcal{H}$. The RWM proposal is $u' = u + h$, where the perturbation is $h = \\epsilon\\xi$. The \"size\" of this perturbation in the space is measured by its norm. The expected squared norm of the perturbation is:\n$$\nE[\\|h\\|^2] = E[\\epsilon^2 \\|\\xi\\|^2] = \\epsilon^2 E\\left[\\sum_{i=1}^d \\xi_i^2\\right] = \\epsilon^2 d\n$$\nIf $\\epsilon$ is fixed, $E[\\|h\\|^2] \\to \\infty$ as $d \\to \\infty$. This means the proposed step $h$ is not an element of the Hilbert space $\\mathcal{H}$ (it has infinite norm). The algorithm attempts to make infinitely large jumps, which invariably land in regions of zero probability measure under the target distribution, leading to guaranteed rejection. The proposal distribution and the target distribution become mutually singular in the infinite-dimensional limit.\n\nBy rescaling $\\epsilon = \\ell/\\sqrt{d}$, the expected squared norm of the perturbation remains bounded as $d \\to \\infty$:\n$$\nE[\\|h\\|^2] = \\left(\\frac{\\ell}{\\sqrt{d}}\\right)^2 d = \\ell^2\n$$\nThis scaling ensures that the proposal step $h$ is a proper element of the Hilbert space, having a finite, $d$-independent norm. This allows the sampler to make meaningful, finite-sized steps in the function space, leading to a non-vanishing acceptance probability and a well-defined algorithm in the infinite-dimensional limit. The Cameron-Martin space of the prior measure characterizes the directions in which one can translate a Gaussian measure without creating a singular measure. By ensuring the proposal step has a finite norm, we are effectively ensuring it \"lives\" in a space related to the Cameron-Martin space, preventing the automatic rejection that arises from mutual singularity.\n\nThe asymptotic expression for the expected acceptance probability as a function of $\\epsilon$ and $d$ is the final answer.",
            "answer": "$$ \\boxed{2\\Phi\\left(-\\frac{\\epsilon\\sqrt{d}}{2}\\right)} $$"
        }
    ]
}