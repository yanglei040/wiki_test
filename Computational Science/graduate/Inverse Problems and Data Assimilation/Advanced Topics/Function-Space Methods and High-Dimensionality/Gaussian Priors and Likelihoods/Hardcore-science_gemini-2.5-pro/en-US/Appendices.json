{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in Bayesian modeling is the choice of prior. Empirical Bayes offers a data-driven approach to this problem by treating hyperparameters of the prior not as fixed, but as parameters to be estimated from the data. This exercise guides you through one of the most common methods: maximizing the marginal likelihood, or evidence. By finding the hyperparameter value that makes the observed data most probable, you will derive an analytical solution for a prior covariance inflation factor in a simple but illuminating scalar problem. ",
            "id": "3385456",
            "problem": "Consider a linear inverse problem in data assimilation with a Gaussian prior and a Gaussian likelihood. Let the unknown state be $x \\in \\mathbb{R}^{n}$ with prior $x \\sim \\mathcal{N}(m, \\lambda C)$, where $m \\in \\mathbb{R}^{n}$ is known, $C \\in \\mathbb{R}^{n \\times n}$ is a known symmetric positive definite matrix, and $\\lambda \\in \\mathbb{R}_{+}$ is an unknown inflation factor to be determined. Observations $y \\in \\mathbb{R}^{p}$ are related to $x$ by the linear model $y = H x + \\varepsilon$, where $H \\in \\mathbb{R}^{p \\times n}$ is known and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with a known symmetric positive definite matrix $R \\in \\mathbb{R}^{p \\times p}$. The goal is to select the inflation factor $\\lambda$ by maximizing the marginal likelihood (also called the evidence) $p(y \\mid \\lambda)$.\n\nStarting from the definitions of multivariate Gaussian probability density functions and Bayes’ theorem, and using only standard matrix calculus identities that follow from these definitions, derive from first principles the analytic stationarity condition satisfied by the maximizing $\\lambda$ obtained by differentiating $\\ln p(y \\mid \\lambda)$ with respect to $\\lambda$ and setting the derivative to zero. Then, specialize your result to the scalar case $n=p=1$ with parameters $m = m_{0}$, $C = c_{0}$, $H = h$, and $R = r$, and show that the maximizing $\\lambda$ admits a closed-form expression in terms of $y$, $m_{0}$, $c_{0}$, $h$, and $r$.\n\nFinally, evaluate this expression for the concrete scalar instance with $m_{0} = 0.3$, $c_{0} = 0.2$, $h = 1.5$, $r = 0.25$, and observed value $y = 3.0$, and report the resulting $\\lambda$.\n\nRound your answer to four significant figures. The final answer must be a single real number with no units.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded within the standard framework of Bayesian inference for linear inverse problems, is well-posed with a clear objective, and contains all necessary information without any contradictions or ambiguities.\n\nWe begin by defining the components of the Bayesian model. The state variable is $x \\in \\mathbb{R}^{n}$. The prior distribution for $x$ is Gaussian, parameterized by the inflation factor $\\lambda \\in \\mathbb{R}_{+}$:\n$$x \\mid \\lambda \\sim \\mathcal{N}(m, \\lambda C)$$\nwhere $m \\in \\mathbb{R}^{n}$ is the prior mean and $\\lambda C$ is the prior covariance matrix, with $C \\in \\mathbb{R}^{n \\times n}$ being symmetric positive definite. The probability density function (PDF) of the prior is:\n$$p(x \\mid \\lambda) = \\frac{1}{\\sqrt{(2\\pi)^{n} \\det(\\lambda C)}} \\exp\\left(-\\frac{1}{2}(x-m)^T (\\lambda C)^{-1} (x-m)\\right)$$\n\nThe observations $y \\in \\mathbb{R}^{p}$ are related to the state $x$ through a linear model with additive Gaussian noise:\n$$y = H x + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, R)$$\nwhere $H \\in \\mathbb{R}^{p \\times n}$ is the observation operator and $R \\in \\mathbb{R}^{p \\times p}$ is the symmetric positive definite covariance matrix of the noise $\\varepsilon$. This defines the likelihood function, which is the conditional probability of the data $y$ given the state $x$:\n$$p(y \\mid x) = \\mathcal{N}(y \\mid Hx, R) = \\frac{1}{\\sqrt{(2\\pi)^{p} \\det(R)}} \\exp\\left(-\\frac{1}{2}(y-Hx)^T R^{-1} (y-Hx)\\right)$$\n\nThe goal is to find the value of $\\lambda$ that maximizes the marginal likelihood, or evidence, $p(y \\mid \\lambda)$. This can be found by integrating the joint distribution $p(y, x \\mid \\lambda) = p(y \\mid x) p(x \\mid \\lambda)$ over all possible states $x$:\n$$p(y \\mid \\lambda) = \\int_{\\mathbb{R}^n} p(y \\mid x) p(x \\mid \\lambda) dx$$\nSince this is a linear-Gaussian model, the marginal distribution of $y$ is also Gaussian. We can determine its parameters as follows:\nThe mean of $y$ is:\n$$E[y \\mid \\lambda] = E[Hx + \\varepsilon \\mid \\lambda] = H E[x \\mid \\lambda] + E[\\varepsilon] = Hm + 0 = Hm$$\nThe covariance of $y$, noting that $x$ and $\\varepsilon$ are independent, is:\n$$\\text{Cov}(y \\mid \\lambda) = \\text{Cov}(Hx + \\varepsilon) = \\text{Cov}(Hx) + \\text{Cov}(\\varepsilon) = H \\text{Cov}(x) H^T + R = H(\\lambda C)H^T + R = \\lambda HCH^T + R$$\nThus, the marginal likelihood is the PDF of a multivariate normal distribution:\n$$y \\mid \\lambda \\sim \\mathcal{N}(Hm, \\lambda HCH^T + R)$$\nLet's denote the covariance matrix of $y$ as $S(\\lambda) = \\lambda HCH^T + R$. The PDF is:\n$$p(y \\mid \\lambda) = \\frac{1}{\\sqrt{(2\\pi)^{p} \\det(S(\\lambda))}} \\exp\\left(-\\frac{1}{2}(y-Hm)^T S(\\lambda)^{-1} (y-Hm)\\right)$$\n\nTo maximize $p(y \\mid \\lambda)$, it is more convenient to maximize its logarithm, the log-marginal likelihood, denoted by $\\mathcal{L}(\\lambda)$:\n$$\\mathcal{L}(\\lambda) = \\ln p(y \\mid \\lambda) = -\\frac{p}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln \\det(S(\\lambda)) - \\frac{1}{2}(y-Hm)^T S(\\lambda)^{-1} (y-Hm)$$\nWe find the optimal $\\lambda$ by differentiating $\\mathcal{L}(\\lambda)$ with respect to $\\lambda$ and setting the result to zero. We use the following standard matrix calculus identities:\n1. $\\frac{\\partial}{\\partial \\lambda} \\ln \\det(S(\\lambda)) = \\text{tr}\\left(S(\\lambda)^{-1} \\frac{\\partial S(\\lambda)}{\\partial \\lambda}\\right)$\n2. $\\frac{\\partial}{\\partial \\lambda} S(\\lambda)^{-1} = -S(\\lambda)^{-1} \\frac{\\partial S(\\lambda)}{\\partial \\lambda} S(\\lambda)^{-1}$\n\nFirst, we find the derivative of $S(\\lambda)$:\n$$\\frac{\\partial S(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda}(\\lambda HCH^T + R) = HCH^T$$\nNow we differentiate $\\mathcal{L}(\\lambda)$:\n$$\\frac{\\partial \\mathcal{L}(\\lambda)}{\\partial \\lambda} = 0 - \\frac{1}{2} \\frac{\\partial}{\\partial \\lambda}\\ln \\det(S(\\lambda)) - \\frac{1}{2}(y-Hm)^T \\left(\\frac{\\partial}{\\partial \\lambda}S(\\lambda)^{-1}\\right) (y-Hm)$$\nSubstituting the identities:\n$$\\frac{\\partial \\mathcal{L}(\\lambda)}{\\partial \\lambda} = -\\frac{1}{2} \\text{tr}\\left(S(\\lambda)^{-1} HCH^T\\right) - \\frac{1}{2}(y-Hm)^T \\left(-S(\\lambda)^{-1} (HCH^T) S(\\lambda)^{-1}\\right) (y-Hm)$$\n$$\\frac{\\partial \\mathcal{L}(\\lambda)}{\\partial \\lambda} = -\\frac{1}{2} \\text{tr}\\left(S(\\lambda)^{-1} HCH^T\\right) + \\frac{1}{2}(y-Hm)^T S(\\lambda)^{-1} HCH^T S(\\lambda)^{-1} (y-Hm)$$\nSetting the derivative to zero gives the stationarity condition:\n$$\\text{tr}\\left(S(\\lambda)^{-1} HCH^T\\right) = (y-Hm)^T S(\\lambda)^{-1} HCH^T S(\\lambda)^{-1} (y-Hm)$$\nSubstituting back $S(\\lambda) = \\lambda HCH^T + R$, we get the final general form of the condition:\n$$\\text{tr}\\left((\\lambda HCH^T + R)^{-1} HCH^T\\right) = (y-Hm)^T (\\lambda HCH^T + R)^{-1} HCH^T (\\lambda HCH^T + R)^{-1} (y-Hm)$$\n\nNext, we specialize this result to the scalar case where $n=p=1$. The variables and matrices become scalars:\n$x \\in \\mathbb{R}$, $y \\in \\mathbb{R}$, $m \\rightarrow m_0 \\in \\mathbb{R}$, $C \\rightarrow c_0 \\in \\mathbb{R}_{++}$, $H \\rightarrow h \\in \\mathbb{R}$, $R \\rightarrow r \\in \\mathbb{R}_{++}$.\nThe terms in the stationarity condition become:\n$HCH^T \\rightarrow h c_0 h = h^2 c_0$\n$S(\\lambda) \\rightarrow \\lambda h^2 c_0 + r$\n$y-Hm \\rightarrow y - h m_0$\nThe trace of a scalar is the scalar itself. The stationarity condition simplifies to:\n$$(\\lambda h^2 c_0 + r)^{-1} (h^2 c_0) = (y - h m_0) (\\lambda h^2 c_0 + r)^{-1} (h^2 c_0) (\\lambda h^2 c_0 + r)^{-1} (y - h m_0)$$\n$$\\frac{h^2 c_0}{\\lambda h^2 c_0 + r} = \\frac{(y - h m_0)^2 h^2 c_0}{(\\lambda h^2 c_0 + r)^2}$$\nAssuming $h \\neq 0$ and $c_0 > 0$, we can divide both sides by $h^2 c_0$:\n$$\\frac{1}{\\lambda h^2 c_0 + r} = \\frac{(y - h m_0)^2}{(\\lambda h^2 c_0 + r)^2}$$\nSince $\\lambda > 0$, $c_0 > 0$, $h^2 \\ge 0$, and $r > 0$, the denominator $\\lambda h^2 c_0 + r$ is strictly positive. We can multiply by $(\\lambda h^2 c_0 + r)^2$:\n$$\\lambda h^2 c_0 + r = (y - h m_0)^2$$\nSolving for $\\lambda$ yields the closed-form expression:\n$$\\lambda = \\frac{(y - h m_0)^2 - r}{h^2 c_0}$$\nThis solution for $\\lambda \\in \\mathbb{R}_{+}$ is valid provided that the numerator is positive, i.e., $(y - h m_0)^2 > r$.\n\nFinally, we evaluate this expression for the given numerical values: $m_{0} = 0.3$, $c_{0} = 0.2$, $h = 1.5$, $r = 0.25$, and $y = 3.0$.\nFirst, calculate the terms needed for the expression:\n$$h m_0 = 1.5 \\times 0.3 = 0.45$$\n$$y - h m_0 = 3.0 - 0.45 = 2.55$$\n$$(y - h m_0)^2 = (2.55)^2 = 6.5025$$\n$$h^2 c_0 = (1.5)^2 \\times 0.2 = 2.25 \\times 0.2 = 0.45$$\nThe condition for a positive $\\lambda$ is $(y - h m_0)^2 > r$, which is $6.5025 > 0.25$. This condition is satisfied.\nNow, substitute these values into the expression for $\\lambda$:\n$$\\lambda = \\frac{6.5025 - 0.25}{0.45} = \\frac{6.2525}{0.45} \\approx 13.89444...$$\nRounding the result to four significant figures gives $13.89$.",
            "answer": "$$\\boxed{13.89}$$"
        },
        {
            "introduction": "While analytical solutions for evidence maximization are highly instructive, they are often intractable in more complex, high-dimensional settings. The more general and powerful approach is to use numerical optimization. This practice builds directly on the concept of Empirical Bayes by tasking you with deriving the gradient of the log-evidence and implementing a robust root-finding algorithm to solve for the optimal prior scaling parameter. You will use a key result known as Fisher's identity, providing a powerful link between the abstract theory of marginal likelihoods and a concrete numerical implementation. ",
            "id": "3385486",
            "problem": "Consider the linear Gaussian inverse problem defined by the following components. Let the unknown parameter vector be $x \\in \\mathbb{R}^n$, with a Gaussian prior $x \\sim \\mathcal{N}(0, C)$ where the covariance $C$ is scaled by a positive scalar parameter $\\tau$ via $C = \\tau^{-1} \\tilde{C}$. The matrix $\\tilde{C} \\in \\mathbb{R}^{n \\times n}$ is known and symmetric positive definite (SPD). Observations $y \\in \\mathbb{R}^m$ are generated by the linear model $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is known, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with $R \\in \\mathbb{R}^{m \\times m}$ known and SPD. The marginal likelihood (evidence) is $p(y \\mid \\tau)$, and the goal of Empirical Bayes (EB) is to choose $\\tau$ to maximize this evidence.\n\nStarting only from core definitions of Gaussian distributions, the linear model, and Bayes’ rule, derive the expression for the gradient of the log evidence with respect to the scale $\\tau$ in terms of the posterior mean $m(\\tau)$ and posterior covariance $\\Sigma(\\tau)$ of $x \\mid y, \\tau$. Then implement a program that:\n- Computes the posterior covariance $\\Sigma(\\tau)$ and posterior mean $m(\\tau)$ using fundamental Gaussian identities for linear models.\n- Computes the derivative $\\partial_{\\tau} \\log p(y \\mid \\tau)$ using the derived expression.\n- Finds the EB maximizer $\\tau^\\star$ by solving $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$ using a robust bracketed root-finding method over a positive interval.\n\nThe fundamental base you may use includes:\n- The Gaussian density definition and its normalization, including for transformed linear models.\n- Bayes’ rule and standard formulas for conditioning in multivariate Gaussians.\n- The identity for the marginal likelihood obtained by integrating out Gaussian latent variables.\n- The property known as Fisher’s identity: for a latent variable model with prior parameter $\\tau$, $\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ \\partial_{\\tau} \\log p(x \\mid \\tau) \\right]$.\n\nYour program must compute numerical results for the following test suite. In each case, let $n$ denote the dimension of $x$, $m$ denote the dimension of $y$, and define $A$, $R$, $\\tilde{C}$, $y$, and an evaluation point $\\tau_0 > 0$. For each case, return a pair of floating-point numbers: first, the value of $\\partial_{\\tau} \\log p(y \\mid \\tau)$ evaluated at $\\tau_0$, and second, the EB maximizer $\\tau^\\star$ solving $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$.\n\nTest Suite:\n1. Case 1 (happy path, moderate dimensions):\n   - $n = 2$, $m = 2$,\n   - $A = \\begin{bmatrix} 1.0  0.5 \\\\ 0.2  1.0 \\end{bmatrix}$,\n   - $R = \\operatorname{diag}(0.1^2, 0.15^2)$,\n   - $\\tilde{C} = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  2.0 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$,\n   - $\\tau_0 = 1.0$.\n2. Case 2 (correlated prior, larger $n$):\n   - $n = 3$, $m = 3$,\n   - $A = \\begin{bmatrix} 1.0  -0.3  0.0 \\\\ -0.2  0.8  0.5 \\\\ 0.0  0.4  1.2 \\end{bmatrix}$,\n   - $R = \\operatorname{diag}(0.2^2, 0.25^2, 0.3^2)$,\n   - $\\tilde{C} = \\begin{bmatrix} 1.5  0.1  0.0 \\\\ 0.1  1.0  0.2 \\\\ 0.0  0.2  2.0 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 0.8 \\\\ -1.0 \\\\ 0.3 \\end{bmatrix}$,\n   - $\\tau_0 = 0.8$.\n3. Case 3 (boundary, scalar problem):\n   - $n = 1$, $m = 1$,\n   - $A = \\begin{bmatrix} 1.7 \\end{bmatrix}$,\n   - $R = \\begin{bmatrix} 0.05^2 \\end{bmatrix}$,\n   - $\\tilde{C} = \\begin{bmatrix} 0.7 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 0.25 \\end{bmatrix}$,\n   - $\\tau_0 = 0.5$.\n\nDefinitions to use for computation:\n- The prior precision is $C^{-1} = \\tau \\tilde{C}^{-1}$.\n- The data precision is $R^{-1}$.\n- The posterior precision is $\\Lambda(\\tau) = C^{-1} + A^\\top R^{-1} A$.\n- The posterior covariance is $\\Sigma(\\tau) = \\Lambda(\\tau)^{-1}$.\n- The posterior mean is $m(\\tau) = \\Sigma(\\tau) A^\\top R^{-1} y$.\n\nTarget derivation:\n- Derive $\\partial_{\\tau} \\log p(y \\mid \\tau)$ in terms of $n$, $\\tau$, $\\tilde{C}^{-1}$, $\\Sigma(\\tau)$, and $m(\\tau)$ only, without introducing any other auxiliary variables.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6]\"). The six values correspond to the three test cases in order: for each case, first $\\partial_{\\tau} \\log p(y \\mid \\tau_0)$, then $\\tau^\\star$. All outputs must be real numbers (floats). No physical units or angle units are involved in this problem, and no percentages are required.",
            "solution": "The problem is deemed valid upon rigorous inspection. All provided components are scientifically and mathematically sound, self-contained, and well-posed. The problem statement provides a clear objective within the established framework of Empirical Bayes for linear Gaussian models. All definitions, constraints, and test cases are formally specified and consistent.\n\nThe task is to derive the gradient of the log-marginal-likelihood (log-evidence) with respect to a prior scale parameter $\\tau$, and then to implement a numerical procedure to find the value of $\\tau$ that maximizes this evidence.\n\n### 1. Theoretical Derivation of the Log-Evidence Gradient\n\nWe are asked to find $\\partial_{\\tau} \\log p(y \\mid \\tau)$. The problem suggests starting from Fisher's identity, which is given as:\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ \\partial_{\\tau} \\log p(x \\mid \\tau) \\right]\n$$\nThis identity allows us to compute the gradient of the marginal likelihood by taking the expectation of the gradient of the log-prior with respect to the posterior distribution.\n\nFirst, we write the expression for the log-prior, $\\log p(x \\mid \\tau)$. The prior on $x \\in \\mathbb{R}^n$ is a zero-mean Gaussian, $x \\sim \\mathcal{N}(0, C)$, with covariance $C = \\tau^{-1} \\tilde{C}$. The corresponding prior precision matrix is $C^{-1} = \\tau \\tilde{C}^{-1}$. The probability density function is:\n$$\np(x \\mid \\tau) = \\frac{1}{(2\\pi)^{n/2} (\\det(C))^{1/2}} \\exp\\left(-\\frac{1}{2} x^\\top C^{-1} x\\right)\n$$\nThe log-prior is therefore:\n$$\n\\log p(x \\mid \\tau) = -\\frac{1}{2} x^\\top C^{-1} x - \\frac{1}{2} \\log \\det(C) - \\frac{n}{2} \\log(2\\pi)\n$$\nWe substitute $C = \\tau^{-1} \\tilde{C}$ and $C^{-1} = \\tau \\tilde{C}^{-1}$. The determinant term becomes $\\log \\det(\\tau^{-1} \\tilde{C}) = \\log(\\tau^{-n} \\det(\\tilde{C})) = -n \\log \\tau + \\log \\det(\\tilde{C})$.\n$$\n\\log p(x \\mid \\tau) = -\\frac{1}{2} x^\\top (\\tau \\tilde{C}^{-1}) x - \\frac{1}{2} (-n \\log \\tau + \\log \\det(\\tilde{C})) - \\frac{n}{2} \\log(2\\pi)\n$$\n$$\n\\log p(x \\mid \\tau) = -\\frac{\\tau}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2} \\log \\tau - \\frac{1}{2}\\log\\det(\\tilde{C}) - \\frac{n}{2}\\log(2\\pi)\n$$\nNext, we differentiate the log-prior with respect to $\\tau$. The terms involving $\\tilde{C}$ and $2\\pi$ are constants with respect to $\\tau$.\n$$\n\\partial_{\\tau} \\log p(x \\mid \\tau) = \\frac{\\partial}{\\partial \\tau} \\left( -\\frac{\\tau}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2} \\log \\tau \\right) = -\\frac{1}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2\\tau}\n$$\nNow, we apply Fisher's identity by taking the expectation of this expression with respect to the posterior distribution $p(x \\mid y, \\tau) = \\mathcal{N}(x \\mid m(\\tau), \\Sigma(\\tau))$.\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ -\\frac{1}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2\\tau} \\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = -\\frac{1}{2} \\mathbb{E}_{x \\mid y, \\tau} \\left[ x^\\top \\tilde{C}^{-1} x \\right] + \\frac{n}{2\\tau}\n$$\nThe expectation of a quadratic form $z^\\top Q z$ for a random vector $z \\sim \\mathcal{N}(\\mu, \\Sigma_z)$ is given by the identity $\\mathbb{E}[z^\\top Q z] = \\operatorname{Tr}(Q \\Sigma_z) + \\mu^\\top Q \\mu$. In our case, the random variable is $x$ from the posterior distribution, so its mean is the posterior mean $m(\\tau)$ and its covariance is the posterior covariance $\\Sigma(\\tau)$. The matrix of the quadratic form is $Q = \\tilde{C}^{-1}$.\nApplying this identity yields:\n$$\n\\mathbb{E}_{x \\mid y, \\tau} \\left[ x^\\top \\tilde{C}^{-1} x \\right] = \\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau)) + m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau)\n$$\nSubstituting this back into our expression for the gradient, we arrive at the final desired form:\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\frac{n}{2\\tau} - \\frac{1}{2} \\left( \\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau)) + m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau) \\right)\n$$\nThis expression depends only on $n$, $\\tau$, $\\tilde{C}^{-1}$, and the posterior statistics $m(\\tau)$ and $\\Sigma(\\tau)$, as required.\n\n### 2. Algorithmic Design and Implementation\n\nThe problem requires a Python program to perform two tasks for each test case:\n1.  Compute the gradient $\\partial_{\\tau} \\log p(y \\mid \\tau)$ at a given point $\\tau_0$.\n2.  Find the optimal hyperparameter $\\tau^\\star > 0$ that maximizes the log-evidence by solving the equation $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$.\n\nTo implement this, we design a function that calculates the gradient for any given $\\tau > 0$. This function requires the posterior mean $m(\\tau)$ and covariance $\\Sigma(\\tau)$, which are computed using the provided standard formulas for Gaussian linear models:\n- Posterior Precision: $\\Lambda(\\tau) = C^{-1} + A^\\top R^{-1} A = \\tau \\tilde{C}^{-1} + A^\\top R^{-1} A$.\n- Posterior Covariance: $\\Sigma(\\tau) = \\Lambda(\\tau)^{-1}$.\n- Posterior Mean: $m(\\tau) = \\Sigma(\\tau) A^\\top R^{-1} y$.\n\nThe numerical algorithm is as follows:\n1.  For a given test case, pre-compute the matrix inverses $\\tilde{C}^{-1}$ and $R^{-1}$.\n2.  Define a function, `gradient(tau)`, that takes a scalar `tau` and returns the value of $\\partial_{\\tau} \\log p(y \\mid \\tau)$.\n    a. Inside this function, compute $\\Lambda(\\tau)$, $\\Sigma(\\tau)$, and $m(\\tau)$. This involves matrix multiplication, addition, and inversion.\n    b. Use these posterior statistics to compute the three terms of the derived gradient formula: $\\frac{n}{2\\tau}$, $\\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau))$, and $m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau)$.\n    c. Combine these terms to return the final gradient value.\n3.  Evaluate this `gradient` function at the specified $\\tau_0$ to obtain the first required result for the test case.\n4.  To find the root $\\tau^\\star$, we use a robust numerical method. The function $g(\\tau) = \\partial_{\\tau} \\log p(y \\mid \\tau)$ has the property that $g(\\tau) \\to +\\infty$ as $\\tau \\to 0^+$. For a typical, well-behaved problem, the log-evidence is concave or unimodal, meaning the gradient will cross zero from positive to negative exactly once for $\\tau > 0$. This structure is ideal for a bracketed root-finding algorithm like Brent's method, available in `scipy.optimize.brentq`.\n5.  To use `brentq`, we need to find a bracket $[a, b]$ such that $g(a) \\cdot g(b)  0$.\n    a. We select $a$ as a small positive number (e.g., $10^{-8}$), where we expect $g(a) > 0$.\n    b. We find $b$ using an expanding search. We start with a guess, e.g., $b=1.0$, and repeatedly multiply it by a factor (e.g., $2$) until $g(b)$ becomes negative. This ensures a valid bracket for the root-finder.\n6.  Call `scipy.optimize.brentq(gradient, a, b)` to find $\\tau^\\star$.\n7.  The procedure is repeated for all test cases, and the results are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the Empirical Bayes problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 2, \"m\": 2,\n            \"A\": np.array([[1.0, 0.5], [0.2, 1.0]]),\n            \"R\": np.diag([0.1**2, 0.15**2]),\n            \"C_tilde\": np.array([[1.0, 0.2], [0.2, 2.0]]),\n            \"y\": np.array([1.0, -0.5]),\n            \"tau_0\": 1.0\n        },\n        {\n            \"n\": 3, \"m\": 3,\n            \"A\": np.array([[1.0, -0.3, 0.0], [-0.2, 0.8, 0.5], [0.0, 0.4, 1.2]]),\n            \"R\": np.diag([0.2**2, 0.25**2, 0.3**2]),\n            \"C_tilde\": np.array([[1.5, 0.1, 0.0], [0.1, 1.0, 0.2], [0.0, 0.2, 2.0]]),\n            \"y\": np.array([0.8, -1.0, 0.3]),\n            \"tau_0\": 0.8\n        },\n        {\n            \"n\": 1, \"m\": 1,\n            \"A\": np.array([[1.7]]),\n            \"R\": np.array([[0.05**2]]),\n            \"C_tilde\": np.array([[0.7]]),\n            \"y\": np.array([0.25]),\n            \"tau_0\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, A, R, C_tilde, y, tau_0 = case['n'], case['A'], case['R'], case['C_tilde'], case['y'], case['tau_0']\n        \n        # Pre-compute matrix inverses and other constant terms\n        C_tilde_inv = np.linalg.inv(C_tilde)\n        R_inv = np.linalg.inv(R)\n        At_Rinv = A.T @ R_inv\n        At_Rinv_A = At_Rinv @ A\n        At_Rinv_y = At_Rinv @ y\n\n        def get_grad_log_evidence(tau):\n            \"\"\"\n            Computes the derivative of the log evidence wrt tau.\n            d/d(tau) log p(y|tau) = n/(2*tau) - 0.5 * (Tr(C_tilde_inv * Sigma) + m.T @ C_tilde_inv @ m)\n            \"\"\"\n            if tau = 0:\n                return np.inf\n\n            # Posterior precision\n            # Lambda = tau * C_tilde_inv + A.T @ R_inv @ A\n            Lambda = tau * C_tilde_inv + At_Rinv_A\n            \n            # Posterior covariance and mean\n            try:\n                # Use solve for better numerical stability than inv\n                Sigma = np.linalg.solve(Lambda, np.identity(n))\n                m = Sigma @ At_Rinv_y\n            except np.linalg.LinAlgError:\n                # If Lambda is singular, gradient is ill-defined.\n                return np.nan\n\n            # Compute the terms for the gradient expression\n            trace_term = np.trace(C_tilde_inv @ Sigma)\n            quadratic_term = m.T @ C_tilde_inv @ m\n            \n            grad = n / (2 * tau) - 0.5 * (trace_term + quadratic_term)\n            return grad\n\n        # 1. Compute the gradient at tau_0\n        grad_at_tau0 = get_grad_log_evidence(tau_0)\n        \n        # 2. Find the EB maximizer tau_star by finding the root of the gradient\n        # We need a bracket [a, b] such that g(a)  0 and g(b)  0\n        a = 1e-8\n        \n        # Check sign at lower bound\n        g_a = get_grad_log_evidence(a)\n        if g_a = 0:\n            # This is unexpected as the gradient should go to +inf at tau=0+\n            # If it happens, we cannot find a bracket in the expected way.\n            # For this problem, we assume this path is not taken.\n            tau_star = np.nan\n        else:\n            # Find an upper bound b such that get_grad_log_evidence(b)  0\n            b = 1.0 if tau_0  1.0 else tau_0 * 2.0\n            \n            # Exponentially increase b until the sign changes\n            max_iter = 100\n            for _ in range(max_iter):\n                g_b = get_grad_log_evidence(b)\n                if g_b  0:\n                    break\n                b *= 2.0\n            else: # Loop finished without break\n                # Could not find an upper bracket where the function is negative\n                tau_star = np.nan\n            \n            if not np.isnan(g_b):\n                 tau_star = brentq(get_grad_log_evidence, a, b, xtol=1e-12, rtol=1e-12)\n            else:\n                 tau_star = np.nan\n\n        results.extend([grad_at_tau0, tau_star])\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many large-scale applications, such as weather forecasting, explicitly manipulating the covariance matrices of Gaussian priors is computationally infeasible. The Ensemble Kalman Filter (EnKF) provides a practical and powerful solution by representing the prior distribution with a collection, or \"ensemble,\" of state vectors. The statistics of this ensemble, such as its sample mean and sample covariance, are then used as approximations within the Bayesian update step. This exercise provides a concrete walkthrough of the EnKF analysis step, demonstrating how an ensemble is updated using a new observation to produce a new \"analysis\" ensemble that better reflects the true state of the system. ",
            "id": "3425299",
            "problem": "Consider the scalar linear-Gaussian data assimilation problem with state dynamics and observation model given by the random-walk system and a direct noisy observation,\n$$\nx_{k+1} = x_{k} + \\eta_{k}, \\quad \\eta_{k} \\sim \\mathcal{N}(0,1),\n$$\n$$\ny_{k} = x_{k} + \\epsilon_{k}, \\quad \\epsilon_{k} \\sim \\mathcal{N}(0,4),\n$$\nwhere all random variables are mutually independent across time and between process and observation noise. Let the forecast (prior) ensemble at time step $k$ be\n$$\nx^{f} = \\{0,\\,1,\\,2\\},\n$$\nwith ensemble size $N_{e} = 3$, and suppose the observation at time $k$ is $y_{k} = 1$.\n\nYou will perform one analysis step of the stochastic Ensemble Kalman Filter (EnKF), defined as follows. The EnKF (Ensemble Kalman Filter) uses the unbiased sample covariance of the forecast ensemble, computed with denominator $N_{e}-1$, to form the Kalman gain. The observation operator is the identity, so the analysis update for each ensemble member is\n$$\nx^{a}_{i} = x^{f}_{i} + K\\left((y_{k} + \\epsilon_{i}) - x^{f}_{i}\\right),\n$$\nwhere $K$ is the scalar Kalman gain computed from the ensemble, and $\\epsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,4)$ are independent perturbed observation draws, independent of the forecast ensemble. The analysis ensemble mean is the arithmetic mean of $\\{x^{a}_{i}\\}_{i=1}^{N_{e}}$ and the analysis ensemble variance is the unbiased sample variance with denominator $N_{e}-1$.\n\nStarting only from the definitions of Gaussian likelihoods and priors together with the linear model above, derive the Kalman gain used by the EnKF in this setting, then derive the stochastic EnKF analysis update, and compute the expectation (over the perturbed observations $\\{\\epsilon_{i}\\}_{i=1}^{N_{e}}$) of:\n- the analysis ensemble mean, and\n- the analysis ensemble variance (unbiased, denominator $N_{e}-1$),\nconditional on the given forecast ensemble and $y_{k} = 1$.\n\nExpress your final answer as an exact pair consisting of the expected analysis mean and the expected analysis variance, in that order, written as a single row using the $\\texttt{pmatrix}$ notation. No rounding is required and no physical units apply.",
            "solution": "The user has provided a well-posed problem in the context of data assimilation using the Ensemble Kalman Filter (EnKF). The problem is scientifically grounded, formally stated, and complete. All necessary data, models, and definitions are provided. I will proceed with the solution.\n\nThe problem asks for the expectation of the analysis ensemble mean and the analysis ensemble variance, conditional on the given forecast ensemble and observation. The expectation is taken with respect to the random draws used for the perturbed observations.\n\nFirst, I will compute the statistics of the forecast ensemble and the Kalman gain.\nThe given forecast ensemble is $x^{f} = \\{x^{f}_{1}, x^{f}_{2}, x^{f}_{3}\\} = \\{0, 1, 2\\}$, with an ensemble size of $N_{e} = 3$.\nThe forecast ensemble mean, $\\bar{x}^{f}$, is the arithmetic average of the ensemble members:\n$$\n\\bar{x}^{f} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{f}_{i} = \\frac{0 + 1 + 2}{3} = 1.\n$$\nThe problem specifies using the unbiased sample covariance (variance in this scalar case) of the forecast ensemble, denoted $P^{f}$. The denominator for the unbiased sample variance is $N_{e}-1$.\n$$\nP^{f} = \\frac{1}{N_{e}-1} \\sum_{i=1}^{N_{e}} (x^{f}_{i} - \\bar{x}^{f})^{2} = \\frac{1}{3-1} \\left( (0-1)^{2} + (1-1)^{2} + (2-1)^{2} \\right) = \\frac{1}{2} (1 + 0 + 1) = 1.\n$$\nThe observation model is $y_{k} = x_{k} + \\epsilon_{k}$, which is linear with an observation operator $H=1$. The observation noise variance is given as $R = 4$. The Kalman gain $K$ for the EnKF is computed using the sample statistics:\n$$\nK = P^{f} H^{T} (H P^{f} H^{T} + R)^{-1}.\n$$\nIn this scalar case with $H=1$, the formula simplifies to:\n$$\nK = \\frac{P^{f}}{P^{f} + R} = \\frac{1}{1 + 4} = \\frac{1}{5}.\n$$\n\nNext, I will find the expected analysis ensemble mean.\nThe stochastic EnKF analysis update for each ensemble member $x^{f}_{i}$ is given by:\n$$\nx^{a}_{i} = x^{f}_{i} + K\\left((y_{k} + \\epsilon_{i}) - x^{f}_{i}\\right),\n$$\nwhere $y_{k}=1$ is the observation and $\\epsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, R)$ with $R=4$.\nThe analysis ensemble mean, $\\bar{x}^{a}$, is the average of the analysis ensemble members:\n$$\n\\bar{x}^{a} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{a}_{i} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\left[ x^{f}_{i} + K\\left(y_{k} + \\epsilon_{i} - x^{f}_{i}\\right) \\right].\n$$\nRearranging the terms, we get:\n$$\n\\bar{x}^{a} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{f}_{i} + K \\left( y_{k} - \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} x^{f}_{i} \\right) + K \\left( \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\epsilon_{i} \\right) = \\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}) + K\\bar{\\epsilon},\n$$\nwhere $\\bar{\\epsilon} = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\epsilon_{i}$.\nWe need to find the expectation of $\\bar{x}^{a}$ over the random variables $\\{\\epsilon_i\\}$, conditional on the fixed forecast ensemble $\\{x^{f}_{i}\\}$ and observation $y_k$.\n$$\nE[\\bar{x}^{a} | \\{x^{f}_{i}\\}, y_{k}] = E[\\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}) + K\\bar{\\epsilon}].\n$$\nSince $\\bar{x}^{f}$, $K$, and $y_{k}$ are constant with respect to this expectation, we have:\n$$\nE[\\bar{x}^{a}] = \\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}) + K E[\\bar{\\epsilon}].\n$$\nThe perturbations $\\epsilon_{i}$ are drawn i.i.d. from $\\mathcal{N}(0,R)$, so their expectation is $E[\\epsilon_{i}] = 0$. The expectation of their mean is also zero:\n$$\nE[\\bar{\\epsilon}] = E\\left[\\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} \\epsilon_{i}\\right] = \\frac{1}{N_{e}} \\sum_{i=1}^{N_{e}} E[\\epsilon_{i}] = 0.\n$$\nTherefore, the expected analysis mean is:\n$$\nE[\\bar{x}^{a}] = \\bar{x}^{f} + K(y_{k} - \\bar{x}^{f}).\n$$\nSubstituting the numerical values $\\bar{x}^{f}=1$, $y_{k}=1$, and $K=1/5$:\n$$\nE[\\bar{x}^{a}] = 1 + \\frac{1}{5}(1 - 1) = 1.\n$$\n\nFinally, I will find the expected analysis ensemble variance.\nThe analysis ensemble variance, $P^{a}$, is the unbiased sample variance of the analysis ensemble:\n$$\nP^{a} = \\frac{1}{N_{e}-1} \\sum_{i=1}^{N_{e}} (x^{a}_{i} - \\bar{x}^{a})^{2}.\n$$\nFirst, let's find the deviation of each member from the mean, $x^{a}_{i} - \\bar{x}^{a}$:\n$$\nx^{a}_{i} = (1-K)x^{f}_{i} + K y_{k} + K\\epsilon_{i}\n$$\n$$\n\\bar{x}^{a} = (1-K)\\bar{x}^{f} + K y_{k} + K\\bar{\\epsilon}\n$$\nSubtracting the two gives:\n$$\nx^{a}_{i} - \\bar{x}^{a} = (1-K)(x^{f}_{i} - \\bar{x}^{f}) + K(\\epsilon_{i} - \\bar{\\epsilon}).\n$$\nNow we compute the sum of squares:\n$$\n\\sum_{i=1}^{N_{e}} (x^{a}_{i} - \\bar{x}^{a})^{2} = \\sum_{i=1}^{N_{e}} \\left[ (1-K)(x^{f}_{i} - \\bar{x}^{f}) + K(\\epsilon_{i} - \\bar{\\epsilon}) \\right]^{2}\n$$\n$$\n= \\sum_{i=1}^{N_{e}} \\left[ (1-K)^{2}(x^{f}_{i} - \\bar{x}^{f})^{2} + K^{2}(\\epsilon_{i} - \\bar{\\epsilon})^{2} + 2K(1-K)(x^{f}_{i} - \\bar{x}^{f})(\\epsilon_{i} - \\bar{\\epsilon}) \\right]\n$$\n$$\n= (1-K)^{2} \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f})^{2} + K^{2} \\sum_{i=1}^{N_{e}}(\\epsilon_{i} - \\bar{\\epsilon})^{2} + 2K(1-K) \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f})(\\epsilon_{i} - \\bar{\\epsilon}).\n$$\nThe analysis variance is $P^{a} = \\frac{1}{N_{e}-1}\\sum(x^{a}_{i} - \\bar{x}^{a})^{2}$. Using the definitions $P^{f} = \\frac{1}{N_e-1}\\sum(x^{f}_{i} - \\bar{x}^{f})^{2}$ and letting $S^{2}_{\\epsilon} = \\frac{1}{N_e-1}\\sum(\\epsilon_{i} - \\bar{\\epsilon})^{2}$ be the sample variance of the perturbations, we have:\n$$\nP^{a} = (1-K)^{2} P^{f} + K^{2} S^{2}_{\\epsilon} + \\frac{2K(1-K)}{N_{e}-1} \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f})(\\epsilon_{i} - \\bar{\\epsilon}).\n$$\nWe take the expectation of $P^{a}$ over $\\{\\epsilon_i\\}$. The forecast statistics $P^{f}$ and $x^{f}_{i}$ are fixed.\n$$\nE[P^{a}] = (1-K)^{2} P^{f} + K^{2} E[S^{2}_{\\epsilon}] + \\frac{2K(1-K)}{N_{e}-1} \\sum_{i=1}^{N_{e}}(x^{f}_{i} - \\bar{x}^{f}) E[\\epsilon_{i} - \\bar{\\epsilon}].\n$$\nThe expectation of the cross term is zero because $E[\\epsilon_{i} - \\bar{\\epsilon}] = E[\\epsilon_{i}] - E[\\bar{\\epsilon}] = 0 - 0 = 0$.\nThe expectation of the unbiased sample variance $S^{2}_{\\epsilon}$ of i.i.d. variables is the population variance $R$. That is, $E[S^{2}_{\\epsilon}] = R$.\nThus, the expected analysis variance simplifies to:\n$$\nE[P^{a}] = (1-K)^{2} P^{f} + K^{2} R.\n$$\nSubstituting the numerical values $P^{f}=1$, $R=4$, and $K=1/5$:\n$$\nE[P^{a}] = \\left(1 - \\frac{1}{5}\\right)^{2} (1) + \\left(\\frac{1}{5}\\right)^{2} (4) = \\left(\\frac{4}{5}\\right)^{2} + \\frac{1}{25}(4) = \\frac{16}{25} + \\frac{4}{25} = \\frac{20}{25} = \\frac{4}{5}.\n$$\nThe expected analysis mean is $1$ and the expected analysis variance is $4/5$. The final answer is the pair $(1, 4/5)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{4}{5} \\end{pmatrix}}\n$$"
        }
    ]
}