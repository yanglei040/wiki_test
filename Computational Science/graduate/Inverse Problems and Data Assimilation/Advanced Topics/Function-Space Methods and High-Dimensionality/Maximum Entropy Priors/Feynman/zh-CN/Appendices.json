{
    "hands_on_practices": [
        {
            "introduction": "从连续空间的理论推导过渡到离散空间的计算实现，是应用最大熵原理的关键一步。在实践中，我们如何通过数值方法找到满足给定约束的最大熵分布参数？本练习将指导你实现“广义迭代尺度算法”（Generalized Iterative Scaling, GIS），这是一种在离散特征空间中寻找最大熵解的经典且稳健的方法。通过亲手编写代码，你不仅能掌握该算法的细节，还能学会如何处理实际问题中可能出现的冗余或不可行约束，从而将理论知识转化为解决问题的实用工具。",
            "id": "3401753",
            "problem": "在一个具有最大熵先验的离散样本空间上，提出了一个有限逆问题。设离散空间为 $\\mathcal{X} = \\{1, \\dots, n\\}$。给定一个严格为正的基测度 $h : \\mathcal{X} \\to \\mathbb{R}_{>0}$ 和 $m$ 个非负特征的集合 $f_i : \\mathcal{X} \\to \\mathbb{R}_{\\ge 0}$（其中 $i \\in \\{1, \\dots, m\\}$）。考虑 $\\mathcal{X}$ 上由 $\\lambda \\in \\mathbb{R}^m$ 索引的对数线性先验族 $p_\\lambda$，其形式为\n$$\np_\\lambda(x) = \\frac{h(x)\\exp\\left(\\sum_{i=1}^m \\lambda_i f_i(x)\\right)}{Z(\\lambda)}, \\quad Z(\\lambda) = \\sum_{x \\in \\mathcal{X}} h(x)\\exp\\left(\\sum_{i=1}^m \\lambda_i f_i(x)\\right).\n$$\n对于 $i \\in \\{1,\\dots,m\\}$，给定目标特征期望 $c_i \\in \\mathbb{R}_{>0}$，目标是调整参数向量 $\\lambda$，使得对于每个特征，$p_\\lambda$下的模型期望满足\n$$\n\\mathbb{E}_{p_\\lambda}[f_i] = \\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x) = c_i.\n$$\n假设特征是有界的，并且它们在 $\\mathcal{X}$ 上的和是常数，即存在一个常数 $C \\in \\mathbb{R}_{>0}$ 使得\n$$\n\\sum_{i=1}^m f_i(x) = C \\quad \\text{for all } x \\in \\mathcal{X}.\n$$\n在此有界性假设下，实现广义迭代尺度法，该方法更新参数向量 $\\lambda$，以迫使期望值趋近目标 $c_i$。您的实现必须：\n- 按照指定方式构造 $p_\\lambda$。\n- 迭代更新参数，直到最大绝对约束违反量\n$$\nr = \\max_{1\\le i \\le m} \\left| \\mathbb{E}_{p_\\lambda}[f_i] - c_i \\right|\n$$\n小于某个容差或达到最大迭代次数。\n- 对 $p_\\lambda$ 使用数值稳定的归一化。\n- 为每个测试用例返回 $r$ 的最终值。\n\n在您的解决方案中，解释为什么该算法在凸性和有界性假设下收敛，以及为什么即使在特征线性相关的情况下，解在导出的先验 $p_\\lambda$ 方面仍然是唯一的。\n\n实现您的程序以解决以下测试套件。对于每个测试用例，程序必须计算 $r$ 的最终值，并按如下指定格式将结果汇总到单行输出中。\n\n测试用例 $1$ (正常情况):\n- 规模：$n = 5$, $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：将 $\\mathcal{X}$ 划分为三组，使用 one-hot 指示符，得到常数和 $C = 1$。具体地，\n  - 当 $x \\in \\{1,2\\}$ 时，$f_1(x) = 1$，否则 $f_1(x) = 0$，\n  - 当 $x \\in \\{3,4\\}$ 时，$f_2(x) = 1$，否则 $f_2(x) = 0$，\n  - 当 $x \\in \\{5\\}$ 时，$f_3(x) = 1$，否则 $f_3(x) = 0$。\n- 目标：$c = [\\,0.4,\\,0.4,\\,0.2\\,]$。\n\n测试用例 $2$ (具有极端目标质量的边界情况):\n- 规模：$n = 5$, $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：与测试用例 $1$ 相同，因此 $C = 1$。\n- 目标：$c = [\\,10^{-6},\\,0.999,\\,0.000999\\,]$。\n\n测试用例 $3$ (具有恒定和的冗余特征):\n- 规模：$n = 4$, $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：两个 one-hot 指示符和一个冗余特征，确保常数和 $C = 2$：\n  - 当 $x \\in \\{1,2\\}$ 时，$f_1(x) = 1$，否则 $f_1(x) = 0$，\n  - 当 $x \\in \\{3,4\\}$ 时，$f_2(x) = 1$，否则 $f_2(x) = 0$，\n  - 对于所有 $x \\in \\{1,2,3,4\\}$，$f_3(x) = 1$。\n- 目标：$c = [\\,0.5,\\,0.5,\\,1.0\\,]$。\n\n测试用例 $4$ (不可行约束):\n- 规模：$n = 5$, $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：与测试用例 $1$ 相同，因此 $C = 1$。\n- 目标：$c = [\\,0.3,\\,0.3,\\,0.3\\,]$。\n\n实现细节：\n- 将 $\\lambda$ 初始化为 $\\lambda = \\mathbf{0} \\in \\mathbb{R}^m$。\n- 对 $r$ 使用 $10^{-10}$ 的停止容差，最大迭代次数为 $5000$。\n- 使用数值稳定的归一化方法构造 $p_\\lambda$，以避免上溢或下溢。\n- 对于每个测试用例，在迭代后计算最终的 $r$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。例如，如果有四个测试用例，输出形式应为 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是一个浮点数，表示测试用例 $i$ 的最终最大绝对约束违反量。",
            "solution": "当前的问题是，在离散空间 $\\mathcal{X} = \\{1, \\dots, n\\}$ 上，寻找一个最大熵先验分布 $p_\\lambda$ 的参数 $\\lambda \\in \\mathbb{R}^m$，使得一组特征 $\\{f_i\\}_{i=1}^m$ 的模型期望与一组给定的目标期望 $\\{c_i\\}_{i=1}^m$ 相匹配。这是统计推断中的一个经典逆问题。该分布的规定形式为\n$$\np_\\lambda(x) = \\frac{h(x)\\exp\\left(\\sum_{i=1}^m \\lambda_i f_i(x)\\right)}{Z(\\lambda)}\n$$\n其中 $h(x)$ 是基测度，$Z(\\lambda)$ 是归一化常数或配分函数。该分布是优化问题的唯一解，该问题是在线性约束 $\\mathbb{E}_p[f_i] = c_i$（其中 $i=1, \\dots, m$）下，最大化相对熵（或库尔贝克-莱布勒散度）$S(p \\| h) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log(p(x)/h(x))$。\n\n这个约束优化问题可以通过考虑其拉格朗日对偶问题来解决。寻找分布 $p$ 的任务被转化为一个关于拉格朗日乘子 $\\lambda$ 的无约束优化问题。在对偶问题中要最大化的目标函数是对数似然或对数配分函数，不包括与 $\\lambda$ 无关的项。具体来说，我们寻求找到 $\\lambda \\in \\mathbb{R}^m$，以求解由 $m$ 个非线性方程组成的方程组：\n$$\n\\mathbb{E}_{p_\\lambda}[f_i] = \\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x) = c_i, \\quad \\text{for } i=1, \\dots, m.\n$$\n该方程的左侧与对数配分函数 $\\log Z(\\lambda)$ 的梯度有关：\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\log Z(\\lambda) = \\mathbb{E}_{p_\\lambda}[f_i].\n$$\n因此，求解约束方程等价于寻找对偶势函数 $\\Phi(\\lambda) = \\log Z(\\lambda) - \\sum_{i=1}^m \\lambda_i c_i$ 的一个驻点。\n\n解分布 $p_\\lambda$ 的唯一性和迭代算法的收敛性由该势函数的凸性保证。$\\log Z(\\lambda)$ 的 Hessian 矩阵由下式给出：\n$$\n\\frac{\\partial^2 \\log Z(\\lambda)}{\\partial \\lambda_j \\partial \\lambda_i} = \\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x) f_j(x) - \\left(\\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x)\\right)\\left(\\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_j(x)\\right) = \\text{Cov}_{p_\\lambda}(f_i, f_j).\n$$\n这个 Hessian 矩阵是在分布 $p_\\lambda$ 下特征的协方差矩阵。作为协方差矩阵，它总是半正定的。这确保了 $\\log Z(\\lambda)$ 是一个凸函数，因此对偶势函数 $\\Phi(\\lambda)$ 也是凸的。这种凸性保证了如果存在最小值，则最小化子的集合是一个凸集。此外，该集合中的任意两组参数 $\\lambda_A$ 和 $\\lambda_B$ 都将产生相同的期望向量 $(\\mathbb{E}[f_1], \\dots, \\mathbb{E}[f_m])$，从而产生相同的概率分布 $p_\\lambda$。因此，如果约束 $\\{c_i\\}$ 是可行的（即，它们位于特征向量的凸包内），则存在唯一的解分布。\n\n如果特征 $\\{f_i\\}$ 线性相关，即存在一个非零向量 $\\alpha$，使得对于所有 $x$，$\\sum_{i=1}^m \\alpha_i f_i(x)$ 是一个常数，那么 Hessian 矩阵将是奇异的。在这种情况下，$\\Phi(\\lambda)$ 不是严格凸的，求解该方程组的参数向量 $\\lambda$ 不是唯一的。如果 $\\lambda^*$ 是一个解，那么对于任何标量 $t$，$\\lambda^* + t\\alpha$ 也是一个解，因为指数中的变化在归一化过程中被抵消了。这就是测试用例 3 的情况。然而，得到的分布 $p_\\lambda$ 仍然是唯一的。\n\n该问题指定使用广义迭代尺度法（GIS），该算法在给定条件下适用：非负特征 $f_i(x) \\ge 0$ 以及对于所有 $x \\in \\mathcal{X}$，特征和为常数 $\\sum_{i=1}^m f_i(x) = C > 0$。GIS 提供了一个乘性更新，可以在 $\\lambda$ 的对数参数空间中表示为加性更新。给定第 $k$ 次迭代的参数 $\\lambda^{(k)}$，每个分量 $\\lambda_i$ 的更新公式为：\n$$\n\\lambda_i^{(k+1)} = \\lambda_i^{(k)} + \\frac{1}{C} \\log\\left(\\frac{c_i}{\\mathbb{E}_{p_{\\lambda^{(k)}}}[f_i]}\\right).\n$$\n这里，$\\mathbb{E}_{p_{\\lambda^{(k)}}}[f_i]$ 表示特征 $i$ 的当前模型期望。项 $1/C$ 充当学习率的角色。只要约束是可行的，这个迭代过程保证收敛到满足约束的参数，该参数在允许线性相关的情况下是唯一的。其收敛性证明依赖于表明算法的每一步都严格减小当前分布与唯一目标分布之间的库尔贝克-莱布勒散度，这一性质由 Darroch 和 Ratcliff (1972) 以及 Csiszár (1989) 建立。如果约束不可行（如测试用例 4），算法仍将收敛到指数族内的一个定义良好的分布，但最终的约束违反量 $r$ 将保持非零，表示在所选散度度量下与目标约束的最小可能偏差。\n\n对于数值实现，计算 $p_\\lambda(x)$ 需要仔细处理指数项，以防止数值上溢或下溢。log-sum-exp 技巧是实现这一目的的标准方法。设 $s_x = \\log h(x) + \\sum_{i=1}^m \\lambda_i f_i(x)$。我们计算 $S_{\\max} = \\max_{x \\in \\mathcal{X}} s_x$，然后按如下方式计算概率：\n$$\np_\\lambda(x) = \\frac{\\exp(s_x - S_{\\max})}{\\sum_{y \\in \\mathcal{X}} \\exp(s_y - S_{\\max})}.\n$$\n这确保了指数函数的最大参数为 $0$，从而防止上溢，而可能导致下溢的项会正确地计算为 $0$。\n\n该算法的流程是：首先初始化 $\\lambda = \\mathbf{0}$，然后迭代计算当前分布 $p_{\\lambda^{(k)}}$、期望 $\\mathbb{E}_{p_{\\lambda^{(k)}}}[f_i]$、残差 $r$，并更新 $\\lambda^{(k+1)}$，直到 $r$ 小于指定的容差或达到最大迭代次数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of maximum entropy problems using Generalized Iterative Scaling.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"n\": 5, \"m\": 3,\n            \"h\": np.ones(5),\n            \"F\": np.array([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0], [0, 0, 0, 0, 1]], dtype=float),\n            \"c\": np.array([0.4, 0.4, 0.2]),\n            \"C\": 1.0,\n        },\n        {\n            \"n\": 5, \"m\": 3,\n            \"h\": np.ones(5),\n            \"F\": np.array([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0], [0, 0, 0, 0, 1]], dtype=float),\n            \"c\": np.array([1e-6, 0.999, 0.000999]),\n            \"C\": 1.0,\n        },\n        {\n            \"n\": 4, \"m\": 3,\n            \"h\": np.ones(4),\n            \"F\": np.array([[1, 1, 0, 0], [0, 0, 1, 1], [1, 1, 1, 1]], dtype=float),\n            \"c\": np.array([0.5, 0.5, 1.0]),\n            \"C\": 2.0,\n        },\n        {\n            \"n\": 5, \"m\": 3,\n            \"h\": np.ones(5),\n            \"F\": np.array([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0], [0, 0, 0, 0, 1]], dtype=float),\n            \"c\": np.array([0.3, 0.3, 0.3]),\n            \"C\": 1.0,\n        },\n    ]\n\n    results = []\n    tol = 1e-10\n    max_iter = 5000\n\n    for case in test_cases:\n        n, m = case[\"n\"], case[\"m\"]\n        h, F, c, C = case[\"h\"], case[\"F\"], case[\"c\"], case[\"C\"]\n\n        lambda_vec = np.zeros(m)\n        log_h = np.log(h)\n        \n        final_r = -1.0 # Placeholder\n\n        for k in range(max_iter):\n            # Calculate unnormalized log probabilities\n            # lambda_vec: (m,), F: (m, n) -> s: (n,)\n            s = lambda_vec @ F + log_h\n\n            # Numerically stable calculation of p_lambda (softmax)\n            s_max = np.max(s)\n            exp_s = np.exp(s - s_max)\n            p_lambda = exp_s / np.sum(exp_s)\n            \n            # Calculate current expectations\n            # F: (m, n), p_lambda: (n,) -> current_c: (m,)\n            current_c = F @ p_lambda\n            \n            # Calculate maximum absolute constraint violation\n            r = np.max(np.abs(current_c - c))\n            final_r = r\n\n            # Check for convergence\n            if r  tol:\n                break\n            \n            # GIS update step for lambda\n            # Note: current_c components are > 0 because h>0, f_i>=0, c_i>0,\n            # and each f_i is non-zero for at least one x.\n            delta_lambda = (1.0 / C) * np.log(c / current_c)\n            lambda_vec += delta_lambda\n\n        results.append(final_r)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最大熵先验的真正威力体现在其作为贝叶斯反问题中正则化项的应用。本练习构建了一个完整的地球物理反演场景，你需要在一个非线性问题中估计具有正定性约束的物理参数（如渗透率）。你将首先应用最大熵原理为对数变换后的变量推导出高斯先验，然后将其与似然函数结合，构建最大后验（MAP）估计的目标函数。最后，你将实现一个带有回溯线搜索的牛顿法来求解这个复杂的优化问题，从而体验将信息论、贝叶斯推断和数值优化融为一体的完整工作流程。",
            "id": "3401766",
            "problem": "考虑一个正参数向量的逆问题，其中正性通过分量指数变换进行编码。设 $y \\in \\mathbb{R}^n$，并通过 $x_i = \\exp(y_i)$ 对所有 $i \\in \\{1,\\dots,n\\}$ 定义 $x \\in \\mathbb{R}^n$。假设物理正向模型通过 $x$ 的一个线性映射并带有加性噪声来从 $x$ 生成观测值 $d \\in \\mathbb{R}^m$：$d = M x + e$，其中 $M \\in \\mathbb{R}^{m \\times n}$ 是已知的，$e$ 是一个加性噪声向量，被建模为具有零均值和已知的正定协方差矩阵 $R \\in \\mathbb{R}^{m \\times m}$ 的高斯分布的一个实现。目标是在简单的矩约束下，使用最大熵 (MaxEnt) 原理为 $y$ 构建先验，并使用牛顿类方法计算 $y$ 的最大后验 (MAP) 估计。然后报告在 MAP 估计处量化数据失配和先验偏差的诊断指标，以及恢复的正参数 $x$ 的一个代表性分量。\n\n您必须从适用于逆问题和数据同化的有效基本基础出发：\n- $\\mathbb{R}^n$ 上概率密度函数 $p$ 的微分熵定义为 $H[p] = - \\int_{\\mathbb{R}^n} p(y) \\log p(y) \\, dy$。\n- 最大熵 (MaxEnt) 原理 (Jaynes)规定，在已知的期望约束和归一化条件下，选择 $p$ 以最大化香农熵。\n- 高斯似然源于加性高斯噪声：如果 $e \\sim \\mathcal{N}(0,R)$，那么给定 $y$ 时 $d$ 的似然与 $\\exp\\left(-\\tfrac{1}{2}(M \\exp(y) - d)^\\top R^{-1} (M \\exp(y) - d)\\right)$ 成正比，其中指数是分量形式的。\n- 最大后验 (MAP) 估计是负对数后验的最小化子。\n\n任务：\n- 仅使用上面列出的基本原理，当唯一可用的信息是 $E[y] = \\mu$ 和 $\\mathrm{Var}(y) = \\sigma^2 I$ 时，推导 $y$ 的最大熵先验，其中 $\\mu \\in \\mathbb{R}^n$ 和 $\\sigma^2  0$ 是给定的，$I$ 是单位矩阵。在您的推导中，您必须将约束视为分量矩约束，并证明在这些约束下最大熵解所蕴含的独立性是合理的。不要对 $p(y)$ 的函数形式做任何先验假设。\n- 使用推导出的先验，建立变量为 $y$ 的 MAP 问题的负对数后验目标。从第一性原理出发，使用链式法则和矩阵微积分推导其关于 $y$ 的梯度和海森矩阵；所有导数必须用 $M$、$R$、$\\mu$、$\\sigma^2$ 和 $y$ 明确表示。\n- 设计一个具有回溯线搜索和简单正定性保障（如对角阻尼）的全局收敛牛顿类算法来求解 MAP 估计 $\\hat{y}$。您的算法必须：\n  - 使用您推导的完整海森矩阵（不仅仅是高斯-牛顿近似）。\n  - 通过带有 $(0,1)$ 内的回溯因子和 $(0,1)$ 内的小 Armijo 参数的 Armijo 条件来强制充分下降。\n  - 包括一个保障措施，确保求解牛顿步的线性系统时使用的是对称正定矩阵（例如，向海森矩阵添加单位矩阵的倍数，直到 Cholesky 分解成功）。\n- 计算出 $\\hat{y}$ 后，通过 $\\hat{x}_i = \\exp(\\hat{y}_i)$ 恢复 $\\hat{x}$，并计算以下诊断指标：\n  - 失配贡献 $J_{\\mathrm{mis}}(\\hat{y}) = \\tfrac{1}{2} (M \\exp(\\hat{y}) - d)^\\top R^{-1} (M \\exp(\\hat{y}) - d)$。\n  - 先验贡献 $J_{\\mathrm{prior}}(\\hat{y}) = \\tfrac{1}{2} (\\hat{y} - \\mu)^\\top (\\sigma^2 I)^{-1} (\\hat{y} - \\mu)$。\n  - 恢复的正参数的第一个分量 $\\hat{x}_1 = \\exp(\\hat{y}_1)$。\n\n使用以下固定的、完全指定的正向模型和数据实例，它作为一个渗透率反演的玩具模型：\n- 维度：$n = 3$，$m = 2$。\n- 正向矩阵 $M \\in \\mathbb{R}^{2 \\times 3}$：\n  - $M = \\begin{bmatrix} 1.0  0.5  -0.2 \\\\ 0.0  1.0  0.3 \\end{bmatrix}$。\n- 真实的（但算法未知）$y_{\\mathrm{true}} = \\begin{bmatrix} 0.5 \\\\ -0.7 \\\\ 0.2 \\end{bmatrix}$，仅用于构建 $d$；相应的 $x_{\\mathrm{true}} = \\exp(y_{\\mathrm{true}})$ 是分量形式的。\n- 数据中的确定性噪声实现 $e = \\begin{bmatrix} 0.05 \\\\ -0.02 \\end{bmatrix}$。\n- 数据 $d = M \\, x_{\\mathrm{true}} + e$，其数值计算结果为 $d = \\begin{bmatrix} 1.702733370963799 \\\\ 0.8430061312394604 \\end{bmatrix}$。\n- 噪声协方差 $R = \\mathrm{diag}(1.0, 1.0)$，因此 $R^{-1} = \\mathrm{diag}(1.0, 1.0)$。\n\n测试套件：\n- 您必须针对以下四种情况运行 MAP 求解器，仅更改最大熵先验参数 $(\\mu, \\sigma^2)$：\n  1. 情况 A（理想情况，中等离散度）：$\\mu = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$，$\\sigma^2 = 0.25$。\n  2. 情况 B（强先验，小方差）：$\\mu = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$，$\\sigma^2 = 0.01$。\n  3. 情况 C（弱先验，大方差）：$\\mu = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$，$\\sigma^2 = 25.0$。\n  4. 情况 D（均值偏移，中等离散度）：$\\mu = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$，$\\sigma^2 = 0.25$。\n\n程序要求：\n- 实现上述牛顿类求解器，从初始猜测 $y^{(0)} = \\mu$ 开始，为每种情况计算 $\\hat{y}$。\n- 对于每种情况，按此顺序输出一个包含三个浮点数的列表 $[J_{\\mathrm{mis}}(\\hat{y}), J_{\\mathrm{prior}}(\\hat{y}), \\hat{x}_1]$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，列表中的每一项本身是对应于一个测试用例的三个数字的列表，顺序为情况 A、B、C、D。例如，输出必须看起来像 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$，不需要空格，其中每个 $a_i$、$b_i$、$c_i$ 都是十进制浮点数。此问题中不适用物理单位。不使用角度。不使用百分比。",
            "solution": "问题陈述经评估有效。它在科学上基于贝叶斯逆理论的原理，内部一致，并且作为一个计算任务是适定的。因此，我们可以着手提供一个完整的解决方案。\n\n根据要求，解决方案分四个部分呈现：(1) 最大熵先验的推导，(2) MAP 目标函数的构建及其梯度和海森矩阵的推导，(3) 数值算法的描述，以及 (4) 针对指定测试用例计算所需的诊断指标。\n\n### 1. 最大熵先验的推导\n\n任务是找到 $y \\in \\mathbb{R}^n$ 的概率密度函数 $p(y)$，使其在一系列约束条件下最大化微分熵 $H[p] = - \\int_{\\mathbb{R}^n} p(y) \\log p(y) \\, dy$。给定的约束是关于 $y$ 各分量的一阶和二阶矩：\n1.  归一化：$\\int_{\\mathbb{R}^n} p(y) \\, dy = 1$。\n2.  均值：$E[y_i] = \\mu_i$ 对 $i \\in \\{1, \\dots, n\\}$。\n3.  方差：$\\mathrm{Var}(y_i) = \\sigma^2  0$ 对 $i \\in \\{1, \\dots, n\\}$。\n4.  协方差：$\\mathrm{Cov}(y_i, y_j) = 0$ 对 $i \\neq j$。\n\n方差和协方差约束共同等价于矩阵约束 $\\mathrm{Var}(y) = \\sigma^2 I$，其中 $I$ 是 $n \\times n$ 单位矩阵。\n\n最大熵原理指出，对于一组函数 $\\{f_k(y)\\}$ 的期望约束，即 $E[f_k(y)] = c_k$，所得的概率密度具有形式 $p(y) = \\frac{1}{Z} \\exp\\left(-\\sum_k \\lambda_k f_k(y)\\right)$，其中 $\\lambda_k$ 是拉格朗日乘子，$Z$ 是归一化常数。\n\n我们的约束可以表示为期望的形式。方差和协方差约束为 $E[(y_i - \\mu_i)^2] = \\sigma^2$ 和 $E[(y_i - \\mu_i)(y_j - \\mu_j)] = 0$（当 $i \\neq j$ 时）。这是一组关于一阶和二阶中心矩的完整约束。给定这些矩约束，最大熵分布的一般形式是多元高斯分布。一个多元高斯分布由其均值向量和协方差矩阵唯一确定。\n\n给定的约束直接指定了这些参数：\n- 均值向量是 $E[y] = \\mu$。\n- 协方差矩阵是 $\\mathrm{Var}(y) = \\sigma^2 I$。\n\n因此，$y$ 上的最大熵先验分布是多元正态分布 $\\mathcal{N}(\\mu, \\sigma^2 I)$。其概率密度函数为：\n$$\np_{\\mathrm{prior}}(y) = \\frac{1}{(2\\pi)^{n/2} |\\sigma^2 I|^{1/2}} \\exp\\left( -\\frac{1}{2} (y - \\mu)^\\top (\\sigma^2 I)^{-1} (y - \\mu) \\right)\n$$\n$$\np_{\\mathrm{prior}}(y) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu_i)^2 \\right)\n$$\n这个密度可以分解为一维高斯密度的乘积：\n$$\np_{\\mathrm{prior}}(y) = \\prod_{i=1}^n \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mu_i)^2}{2\\sigma^2} \\right) \\right] = \\prod_{i=1}^n p_i(y_i)\n$$\n其中 $p_i(y_i) = \\mathcal{N}(y_i | \\mu_i, \\sigma^2)$。这种可分解的形式表明分量 $y_i$ 在统计上是独立的。这种独立性不是一个先验假设，而是在所有指定的互相关为零的约束条件下应用最大熵原理的结果。\n\n### 2. MAP 目标函数、梯度和海森矩阵\n\n最大后验 (MAP) 估计 $\\hat{y}$ 是通过最大化后验概率密度 $p(y|d)$ 找到的。根据贝叶斯定理，$p(y|d) \\propto p(d|y)p_{\\mathrm{prior}}(y)$。最大化后验等价于最小化其负对数。我们定义目标函数 $J(y)$ 为负对数后验，忽略不依赖于 $y$ 的常数。\n\n$$\nJ(y) = -\\log p(d|y) - \\log p_{\\mathrm{prior}}(y) = J_{\\mathrm{mis}}(y) + J_{\\mathrm{prior}}(y)\n$$\n\n似然项 $J_{\\mathrm{mis}}(y)$ 源于高斯噪声模型 $e \\sim \\mathcal{N}(0, R)$，它给出 $p(d|y) \\propto \\exp\\left(-\\frac{1}{2}(M \\exp(y) - d)^\\top R^{-1} (M \\exp(y) - d)\\right)$。\n$$\nJ_{\\mathrm{mis}}(y) = \\frac{1}{2} (M \\exp(y) - d)^\\top R^{-1} (M \\exp(y) - d)\n$$\n先验项 $J_{\\mathrm{prior}}(y)$ 是推导出的最大熵先验的负对数：\n$$\nJ_{\\mathrm{prior}}(y) = \\frac{1}{2} (y - \\mu)^\\top (\\sigma^2 I)^{-1} (y - \\mu) = \\frac{1}{2\\sigma^2} \\|y - \\mu\\|_2^2\n$$\n需要最小化的总目标函数是：\n$$\nJ(y) = \\frac{1}{2} (M \\exp(y) - d)^\\top R^{-1} (M \\exp(y) - d) + \\frac{1}{2\\sigma^2} (y - \\mu)^\\top (y - \\mu)\n$$\n\n为了使用牛顿类优化方法，我们必须计算梯度 $\\nabla J(y)$ 和海森矩阵 $\\nabla^2 J(y)$。\n\n**梯度 $\\nabla J(y)$**：\n我们分别计算每一项的梯度。\n对于先验项：$\\nabla J_{\\mathrm{prior}}(y) = \\frac{1}{\\sigma^2} (y - \\mu)$。\n对于失配项，我们使用链式法则。令 $g(y) = M \\exp(y) - d$。$g(y)$ 的雅可比矩阵是 $D_y g = M \\cdot D_y(\\exp(y))$。分量指数函数 $\\exp(y)$ 的雅可比矩阵是一个对角矩阵 $E(y) = \\mathrm{diag}(\\exp(y_1), \\dots, \\exp(y_n))$。因此，$D_y g = M E(y)$。\n梯度为 $\\nabla J_{\\mathrm{mis}}(y) = (D_y g)^\\top R^{-1} g(y) = (M E(y))^\\top R^{-1} (M \\exp(y) - d)$。由于 $E(y)$ 是对角矩阵，其转置是其自身。\n$$\n\\nabla J_{\\mathrm{mis}}(y) = E(y) M^\\top R^{-1} (M \\exp(y) - d)\n$$\n总梯度为：\n$$\n\\nabla J(y) = E(y) M^\\top R^{-1} (M \\exp(y) - d) + \\frac{1}{\\sigma^2} (y - \\mu)\n$$\n\n**海森矩阵 $\\nabla^2 J(y)$**：\n先验项的海森矩阵是 $\\nabla^2 J_{\\mathrm{prior}}(y) = \\frac{1}{\\sigma^2} I$。\n对于失配项，我们对其梯度进行微分。使用矩阵微积分的乘法法则和链式法则，$\\frac{1}{2} g(y)^\\top A g(y)$ 的海森矩阵是 $(D_y g)^\\top A (D_y g) + \\sum_{i=1}^m (A g(y))_i \\nabla^2 g_i(y)$。\n第一部分是海森矩阵的高斯-牛顿近似：\n$$\nH_{\\mathrm{GN}} = (M E(y))^\\top R^{-1} (M E(y)) = E(y) M^\\top R^{-1} M E(y)\n$$\n对于第二部分，$g(y)$ 的第 $i$ 个分量是 $g_i(y) = \\sum_{j=1}^n M_{ij} \\exp(y_j) - d_i$。其海森矩阵 $\\nabla^2 g_i(y)$ 是一个对角矩阵，其元素为 $\\frac{\\partial^2 g_i}{\\partial y_k^2} = M_{ik} \\exp(y_k)$。\n完整的第二项是矩阵之和，结果是一个单一的对角矩阵，其第 $k$ 个对角元素是 $\\exp(y_k) \\left( M^\\top R^{-1} (M \\exp(y) - d) \\right)_k$。我们将此矩阵表示为 $\\mathrm{diag}(E(y) M^\\top R^{-1} (M \\exp(y) - d))$。\n总海森矩阵为：\n$$\n\\nabla^2 J(y) = \\underbrace{E(y) M^\\top R^{-1} M E(y)}_{\\text{高斯-牛顿}} + \\underbrace{\\mathrm{diag}\\left(E(y) M^\\top R^{-1} (M \\exp(y) - d)\\right)}_{\\text{二阶项}} + \\underbrace{\\frac{1}{\\sigma^2} I}_{\\text{先验海森矩阵}}\n$$\n二阶项可以有负值，使得完整的海森矩阵不定。这说明了在牛顿算法中需要一种保障机制。\n\n### 3. 牛顿类算法设计\n\n我们实现一个阻尼牛顿法来找到 $J(y)$ 的最小化子 $\\hat{y}$。该算法从初始猜测 $y^{(0)} = \\mu$ 开始，并迭代地改进估计。\n\n**算法：带回溯线搜索的阻尼牛顿法**\n1.  **初始化**：设置 $k = 0$，$y^{(0)} \\leftarrow \\mu$。定义容差 `tol`、Armijo 参数 $c_1 \\in (0,1)$ 和回溯因子 $\\rho \\in (0,1)$。\n2.  **迭代**：对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n    a.  计算梯度 $g_k = \\nabla J(y^{(k)})$ 和海森矩阵 $H_k = \\nabla^2 J(y^{(k)})$。\n    b.  如果 $\\|g_k\\|  \\mathrm{tol}$，终止并返回 $\\hat{y} = y^{(k)}$。\n    c.  **保障海森矩阵**：求解牛顿系统 $(H_k + \\tau I) p_k = -g_k$。\n        i.  从阻尼参数 $\\tau = 0$ 开始。\n        ii. 尝试进行 Cholesky 分解 $L L^\\top = H_k + \\tau I$。\n        iii. 如果分解失败（矩阵非正定），增加 $\\tau$（例如，如果 $\\tau$ 为 $0$，则设为 $10^{-6}$，否则乘以 $10$）并重复。\n    d.  **求解搜索方向**：使用计算出的 Cholesky 因子 $L L^\\top$ 求解线性系统以得到 $p_k$。\n    e.  **回溯线搜索**：找到步长 $\\alpha_k \\in (0, 1]$。\n        i.  从 $\\alpha = 1$ 开始。\n        ii. 当 Armijo 条件 $J(y^{(k)} + \\alpha p_k)  J(y^{(k)}) + c_1 \\alpha g_k^\\top p_k$ 未满足时，更新 $\\alpha \\leftarrow \\rho \\alpha$。\n        iii. 设置 $\\alpha_k = \\alpha$。\n    f.  **更新**：$y^{(k+1)} \\leftarrow y^{(k)} + \\alpha_k p_k$。\n\n该算法保证每一步都是下降的，并收敛到 $J(y)$ 的一个局部最小值。\n\n### 4. 诊断指标与结果\n\n在牛顿求解器收敛到 MAP 估计 $\\hat{y}$ 后，我们计算所需的诊断指标。恢复的正参数为 $\\hat{x} = \\exp(\\hat{y})$。\n-   失配贡献：$J_{\\mathrm{mis}}(\\hat{y}) = \\frac{1}{2} (M \\exp(\\hat{y}) - d)^\\top R^{-1} (M \\exp(\\hat{y}) - d)$。\n-   先验贡献：$J_{\\mathrm{prior}}(\\hat{y}) = \\frac{1}{2\\sigma^2} \\|\\hat{y} - \\mu\\|_2^2$。\n-   恢复参数的第一个分量：$\\hat{x}_1 = \\exp(\\hat{y}_1)$。\n\n下面的实现为四个指定的测试用例计算这些值。结果展示了先验参数 $(\\mu, \\sigma^2)$ 对解的影响。一个小的 $\\sigma^2$（强先验）会迫使解 $\\hat{y}$ 接近先验均值 $\\mu$，但这可能以大的数据失配为代价。一个大的 $\\sigma^2$（弱先验）允许解主要由数据驱动，导致较小的数据失配，但与先验均值的偏差较大。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to solve the MAP estimation problem for all test cases.\n    \"\"\"\n    \n    # Define the fixed model parameters and data from the problem statement.\n    M = np.array([[1.0, 0.5, -0.2], [0.0, 1.0, 0.3]])\n    d = np.array([1.702733370963799, 0.8430061312394604])\n    R_inv = np.identity(2)  # R = I, so R_inv = I\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path, moderate dispersion\n        {'mu': np.array([0.0, 0.0, 0.0]), 'sigma2': 0.25},\n        # Case B: Strong prior, small variance\n        {'mu': np.array([0.0, 0.0, 0.0]), 'sigma2': 0.01},\n        # Case C: Weak prior, large variance\n        {'mu': np.array([0.0, 0.0, 0.0]), 'sigma2': 25.0},\n        # Case D: Shifted mean, moderate dispersion\n        {'mu': np.array([0.1, -0.2, 0.3]), 'sigma2': 0.25},\n    ]\n\n    # Newton solver parameters\n    tol = 1e-9\n    max_iter = 50\n    armijo_c1 = 1e-4\n    backtrack_rho = 0.5\n    \n    results = []\n    for case in test_cases:\n        mu = case['mu']\n        sigma2 = case['sigma2']\n        \n        # Initial guess is the prior mean\n        y0 = np.copy(mu)\n        \n        # Run the Newton solver to find the MAP estimate y_hat\n        y_hat = newton_map_solver(\n            M, d, R_inv, mu, sigma2, y0, \n            tol, max_iter, armijo_c1, backtrack_rho\n        )\n        \n        x_hat = np.exp(y_hat)\n        \n        # Compute diagnostics at the MAP estimate\n        residual = M @ x_hat - d\n        j_mis = 0.5 * residual.T @ R_inv @ residual\n        \n        prior_diff = y_hat - mu\n        j_prior = 0.5 * (prior_diff.T @ prior_diff) / sigma2\n        \n        x1_hat = x_hat[0]\n        \n        results.append([j_mis, j_prior, x1_hat])\n\n    # Final print statement in the exact required format.\n    formatted_results = \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results])\n    print(f\"[{formatted_results}]\")\n\ndef objective_function(y, M, d, R_inv, mu, sigma2):\n    \"\"\"Computes the negative log-posterior objective function J(y).\"\"\"\n    x = np.exp(y)\n    \n    residual = M @ x - d\n    j_mis = 0.5 * residual.T @ R_inv @ residual\n    \n    prior_diff = y - mu\n    j_prior = 0.5 * (prior_diff.T @ prior_diff) / sigma2\n    \n    return j_mis + j_prior\n\ndef gradient(y, M, d, R_inv, mu, sigma2):\n    \"\"\"Computes the gradient of J(y).\"\"\"\n    x = np.exp(y)\n    \n    grad_mis = np.diag(x) @ M.T @ R_inv @ (M @ x - d)\n    grad_prior = (y - mu) / sigma2\n    \n    return grad_mis + grad_prior\n\ndef hessian(y, M, d, R_inv, mu, sigma2):\n    \"\"\"Computes the Hessian of J(y).\"\"\"\n    x = np.exp(y)\n    E_y = np.diag(x)\n    \n    H_gn = E_y @ M.T @ R_inv @ M @ E_y\n    H_so = np.diag(E_y @ M.T @ R_inv @ (M @ x - d))\n    H_prior = np.identity(len(y)) / sigma2\n    \n    return H_gn + H_so + H_prior\n\ndef newton_map_solver(M, d, R_inv, mu, sigma2, y0, tol, max_iter, armijo_c1, backtrack_rho):\n    \"\"\"\n    Solves for the MAP estimate using a safeguarded Newton-type method.\n    \"\"\"\n    y = np.copy(y0)\n    n_dim = len(y)\n    \n    for _ in range(max_iter):\n        J = objective_function(y, M, d, R_inv, mu, sigma2)\n        g = gradient(y, M, d, R_inv, mu, sigma2)\n        \n        if linalg.norm(g)  tol:\n            break\n            \n        H = hessian(y, M, d, R_inv, mu, sigma2)\n        \n        # Safeguard Hessian via diagonal damping until positive-definite.\n        tau = 0.0\n        while True:\n            try:\n                H_reg = H + tau * np.identity(n_dim)\n                L = linalg.cholesky(H_reg, lower=True)\n                break\n            except linalg.LinAlgError:\n                if tau == 0.0:\n                    tau = 1e-6\n                else:\n                    tau *= 10.0\n        \n        # Solve Newton system p = -(H + tau*I)^-1 * g using Cholesky factor.\n        z = linalg.solve_triangular(L, -g, lower=True, check_finite=False)\n        p = linalg.solve_triangular(L.T, z, lower=False, check_finite=False)\n        \n        # Backtracking line search to enforce sufficient decrease (Armijo condition).\n        alpha = 1.0\n        g_dot_p = g.T @ p\n        \n        # Added a check for very small g_dot_p to prevent long loops for flat regions\n        if abs(g_dot_p)  1e-14:\n            break\n            \n        while True:\n            y_new = y + alpha * p\n            J_new = objective_function(y_new, M, d, R_inv, mu, sigma2)\n            \n            if J_new = J + armijo_c1 * alpha * g_dot_p:\n                break\n            \n            alpha *= backtrack_rho\n            # Break if step size becomes excessively small\n            if alpha  1e-12:\n                break\n        \n        y += alpha * p\n        \n    return y\n\nsolve()\n```"
        }
    ]
}