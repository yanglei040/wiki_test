{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Proper Orthogonal Decomposition (POD) is its ability to provide a quantitative, energy-based hierarchy for basis modes. This allows us to make informed decisions about model truncation. This exercise provides a concrete walkthrough of the core mechanics, focusing on how the eigenvalues of the snapshot correlation matrix guide the selection of a reduced basis to meet a specific energy criterion and how to quantify the resulting truncation error. It also introduces the concept of a weighted inner product via the mass matrix $M$, a crucial element when applying POD to solutions from finite element models .",
            "id": "2593061",
            "problem": "Consider a family of linear, coercive, parameterized partial differential equations discretized by the Finite Element Method (FEM). Suppose the solution manifold is sampled at three parameter values $\\mu_{1}$, $\\mu_{2}$, and $\\mu_{3}$, yielding three snapshot vectors in $\\mathbb{R}^{3}$ collected as columns of the snapshot matrix $S \\in \\mathbb{R}^{3 \\times 3}$. Let the finite element mass matrix be the symmetric positive definite matrix $M \\in \\mathbb{R}^{3 \\times 3}$. You will compute a Proper Orthogonal Decomposition (POD) with respect to the $M$-inner product for these snapshots.\n\nUse the following data:\n- The mass matrix is \n$$\nM \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n- The snapshot matrix is \n$$\nS \\;=\\; \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\n\nAdopt the $M$-inner product $\\langle u, v \\rangle_{M} \\equiv u^{\\top} M v$ and the corresponding $M$-norm $\\|u\\|_{M} \\equiv \\sqrt{u^{\\top} M u}$. Define the snapshot correlation matrix by the fundamental POD definition\n$$\nC \\;=\\; S^{\\top} M S \\;\\in\\; \\mathbb{R}^{3 \\times 3}.\n$$\nLet the eigenvalues of $C$ be denoted and ordered nonincreasingly as $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3} \\ge 0$. The POD energy captured by the first $r$ modes is defined as\n$$\n\\mathcal{E}(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\lambda_{i}}{\\sum_{i=1}^{3} \\lambda_{i}}.\n$$\n\nYour tasks are:\n1. Given the target energy threshold $\\eta = 0.85$, determine the minimal integer $r$ such that $\\mathcal{E}(r) \\ge \\eta$.\n2. Using the fundamental POD error identity for snapshots, estimate the total snapshot reconstruction truncation error in the $M$-norm when using $r$ modes, defined as\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=r+1}^{3} \\lambda_{i}.\n$$\n\nReport your final answer as a two-entry row matrix $\\begin{pmatrix} r & E_{\\text{trunc}} \\end{pmatrix}$. No rounding is required, and exact values are expected. No physical units are needed.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the established theory of reduced-order modeling, specifically Proper Orthogonal Decomposition (POD). It is well-posed, with all necessary data and definitions provided to ensure a unique, stable, and meaningful solution. The problem's structure is objective and logically consistent. We proceed with the solution.\n\nThe problem requires the computation of two quantities related to the Proper Orthogonal Decomposition of a given set of snapshots. The first is the minimal number of modes, $r$, required to capture a specified fraction of the total energy. The second is the corresponding truncation error. The computations are to be performed with respect to the inner product induced by the mass matrix $M$.\n\nThe core of the POD method, as described, is the eigenvalue problem for the snapshot correlation matrix $C$. We begin by computing this matrix. The definition provided is\n$$\nC \\;=\\; S^{\\top} M S\n$$\nThe snapshot matrix $S$ is given as the $3 \\times 3$ identity matrix, $S = I = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$. The transpose of the identity matrix is the identity matrix itself, $S^{\\top} = I^{\\top} = I$. Substituting these into the definition of $C$ yields:\n$$\nC \\;=\\; I^{\\top} M I \\;=\\; I M I \\;=\\; M\n$$\nTherefore, the correlation matrix $C$ is identical to the mass matrix $M$:\n$$\nC \\;=\\; M \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & \\tfrac{1}{2}\n\\end{pmatrix}\n$$\nThe next step is to determine the eigenvalues of $C$. Since $C$ is a diagonal matrix, its eigenvalues are precisely its diagonal entries. The problem requires that these eigenvalues be ordered nonincreasingly: $\\lambda_{1} \\ge \\lambda_{2} \\ge \\lambda_{3}$. From the diagonal of $C$, we identify the eigenvalues as $2$, $1$, and $\\frac{1}{2}$. Ordering them gives:\n$$\n\\lambda_{1} = 2\n$$\n$$\n\\lambda_{2} = 1\n$$\n$$\n\\lambda_{3} = \\frac{1}{2}\n$$\nThese eigenvalues represent the squared singular values of the snapshot matrix scaled by the $M$-inner product, and their sum represents the total energy, or variance, contained in the snapshots. The total energy is the trace of the correlation matrix, $\\mathrm{Tr}(C)$:\n$$\n\\sum_{i=1}^{3} \\lambda_{i} = \\lambda_{1} + \\lambda_{2} + \\lambda_{3} = 2 + 1 + \\frac{1}{2} = 3.5 = \\frac{7}{2}\n$$\nNow, we address the first task: to find the minimal integer $r$ such that the captured energy $\\mathcal{E}(r)$ meets or exceeds the threshold $\\eta = 0.85$. The captured energy is defined as:\n$$\n\\mathcal{E}(r) \\;=\\; \\frac{\\sum_{i=1}^{r} \\lambda_{i}}{\\sum_{i=1}^{3} \\lambda_{i}}\n$$\nWe evaluate $\\mathcal{E}(r)$ for $r = 1, 2, \\dots$ until the condition $\\mathcal{E}(r) \\ge 0.85$ is met. The threshold is $\\eta = 0.85 = \\frac{85}{100} = \\frac{17}{20}$.\n\nFor $r=1$:\n$$\n\\mathcal{E}(1) = \\frac{\\lambda_{1}}{\\sum_{i=1}^{3} \\lambda_{i}} = \\frac{2}{\\frac{7}{2}} = \\frac{4}{7}\n$$\nWe compare this to the threshold. Is $\\frac{4}{7} \\ge \\frac{17}{20}$? By cross-multiplication, we compare $4 \\times 20 = 80$ and $7 \\times 17 = 119$. Since $80  119$, we have $\\frac{4}{7}  \\frac{17}{20}$. Thus, $\\mathcal{E}(1)  0.85$. One mode is insufficient.\n\nFor $r=2$:\n$$\n\\mathcal{E}(2) = \\frac{\\lambda_{1} + \\lambda_{2}}{\\sum_{i=1}^{3} \\lambda_{i}} = \\frac{2 + 1}{\\frac{7}{2}} = \\frac{3}{\\frac{7}{2}} = \\frac{6}{7}\n$$\nWe compare this to the threshold. Is $\\frac{6}{7} \\ge \\frac{17}{20}$? By cross-multiplication, we compare $6 \\times 20 = 120$ and $7 \\times 17 = 119$. Since $120 > 119$, we have $\\frac{6}{7} > \\frac{17}{20}$. Thus, $\\mathcal{E}(2) > 0.85$. Two modes are sufficient.\n\nSince $r=1$ is insufficient and $r=2$ is sufficient, the minimal integer is $r=2$.\n\nThe second task is to compute the total snapshot reconstruction truncation error, $E_{\\text{trunc}}$, when using $r=2$ modes. The definition provided is:\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=r+1}^{3} \\lambda_{i}\n$$\nFor $r=2$, this sum becomes:\n$$\nE_{\\text{trunc}} \\;=\\; \\sum_{i=2+1}^{3} \\lambda_{i} \\;=\\; \\sum_{i=3}^{3} \\lambda_{i} \\;=\\; \\lambda_{3}\n$$\nSubstituting the value for $\\lambda_{3}$:\n$$\nE_{\\text{trunc}} = \\frac{1}{2}\n$$\nThis value represents the energy content of the modes that are truncated, which is exactly the portion of total energy not captured by the first $r$ modes.\n\nThe two required values are $r=2$ and $E_{\\text{trunc}} = \\frac{1}{2}$. The final answer must be reported as a two-entry row matrix.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2  \\frac{1}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In practical applications, such as those involving finite element methods on complex geometries, state data is often defined on a nonuniform mesh. In this setting, the standard Euclidean inner product can be misleading, as it fails to account for the varying sizes of grid cells. This hands-on coding practice  directly confronts this issue by asking you to compare a POD basis generated with the standard Euclidean inner product against one generated with a physically meaningful $M$-weighted inner product. By quantifying the difference in projection error, you will gain a crucial, practical understanding of why the choice of inner product is not merely a theoretical detail but a fundamental factor for achieving optimality in reduced-order modeling.",
            "id": "3417044",
            "problem": "Consider a set of state snapshots represented by a real matrix $X \\in \\mathbb{R}^{n \\times m}$ sampled on a one-dimensional, possibly nonuniform mesh with $n$ spatial nodes. Let $M \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite diagonal matrix that defines the $M$-weighted inner product on $\\mathbb{R}^{n}$ by $\\langle a,b\\rangle_{M} = a^{\\top} M b$ and the induced norm $\\|a\\|_{M}^{2} = a^{\\top} M a$. The Proper Orthogonal Decomposition (POD) seeks an $r$-dimensional subspace that minimizes the mean-squared projection error of the snapshots in a chosen inner product. In a nonuniform mesh setting, the physically meaningful metric is often the $M$-weighted inner product associated with discrete quadrature weights.\n\nYou will quantify, for a fixed rank $r$, the difference in $M$-norm projection errors when the $r$-dimensional subspace is obtained by:\n- Euclidean POD, which uses the standard inner product $\\langle a,b\\rangle_{2} = a^{\\top} b$, and\n- $M$-weighted POD, which uses $\\langle a,b\\rangle_{M} = a^{\\top} M b$.\n\nUse the following fundamental definitions:\n- For any column-orthonormal basis $U \\in \\mathbb{R}^{n \\times r}$ spanning a subspace, the $M$-orthogonal projector onto $\\operatorname{span}(U)$ is\n$$\nP_{M}(U) = U\\left(U^{\\top} M U\\right)^{-1}U^{\\top} M.\n$$\n- The $M$-norm projection error of $X$ onto $\\operatorname{span}(U)$ is\n$$\nE(U;M,X) = \\left\\| X - P_{M}(U) X \\right\\|_{M}^{2} = \\operatorname{trace}\\!\\left(\\left(X - P_{M}(U)X\\right)^{\\top} M \\left(X - P_{M}(U)X\\right)\\right).\n$$\n- The Euclidean POD basis $U_{r}^{E} \\in \\mathbb{R}^{n \\times r}$ is obtained by the leading $r$ left singular vectors of $X$ (using the standard inner product).\n- The $M$-weighted POD basis $U_{r}^{M} \\in \\mathbb{R}^{n \\times r}$ is obtained by forming $Y = M^{1/2} X$, computing the leading $r$ left singular vectors $Q_{r}$ of $Y$, and setting $U_{r}^{M} = M^{-1/2} Q_{r}$, which satisfies $(U_{r}^{M})^{\\top} M U_{r}^{M} = I_{r}$.\n\nYour task is to compute, for each test case, the scalar difference\n$$\n\\Delta(r;M,X) = E\\!\\left(U_{r}^{E}; M, X\\right) - E\\!\\left(U_{r}^{M}; M, X\\right),\n$$\nand to use these results to empirically assess when $M$-weighting is essential.\n\nConstruct the snapshots deterministically as follows. Let $n \\in \\mathbb{N}$ be the number of spatial nodes and $m \\in \\mathbb{N}$ be the number of snapshots. Given a mesh $x = (x_{1},\\dots,x_{n})^{\\top}$ on $[0,1]$, define $m$ snapshot columns by\n$$\nX(:,j) = \\sin\\!\\left( 2\\pi j \\, x \\right) + 0.1\\, j \\, \\cos\\!\\left( 2\\pi (j-1) \\, x \\right), \\quad \\text{for } j = 1,2,\\dots,m,\n$$\nwhere the sine and cosine act entrywise on the vector $x$.\n\nDefine the diagonal mass matrix $M = \\operatorname{diag}(w)$ as follows. For a mesh $x_{i}$, set trapezoidal quadrature weights:\n- $w_{1} = \\tfrac{1}{2}\\left(x_{2}-x_{1}\\right)$,\n- $w_{i} = \\tfrac{1}{2}\\left(x_{i+1}-x_{i-1}\\right)$ for $i=2,\\dots,n-1$,\n- $w_{n} = \\tfrac{1}{2}\\left(x_{n}-x_{n-1}\\right)$.\nFor the uniform-metric case, use $M=I_{n}$.\n\nImplement the following test suite. Fix $n = 80$ and $m = 6$. Consider four cases with rank $r$ as specified:\n- Case $\\mathbf{A}$ (happy path, uniform metric): Uniform mesh $x_{i} = \\tfrac{i-1}{n-1}$ and $M = I_{n}$, with $r=2$.\n- Case $\\mathbf{B}$ (mild nonuniformity): Nonuniform mesh $x_{i} = \\left(\\tfrac{i-1}{n-1}\\right)^{1.5}$, trapezoidal weights as above with $r=2$.\n- Case $\\mathbf{C}$ (strong nonuniformity): Nonuniform mesh $x_{i} = \\left(\\tfrac{i-1}{n-1}\\right)^{3.0}$, trapezoidal weights as above with $r=2$.\n- Case $\\mathbf{D}$ (boundary rank case): Same mesh as Case $\\mathbf{C}$, with $r=m=6$.\n\nFor each case, compute $\\Delta(r;M,X)$ as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[d_{A},d_{B},d_{C},d_{D}]$), in the order $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D}$. No physical units are involved. Angles are in radians. Express all numerical outputs as decimal floating-point numbers.\n\nYour solution must start from the definitions of inner products, orthogonal projection in a weighted inner product, and the construction of POD bases, and must justify the algorithm used to compute $\\Delta(r;M,X)$ for each case.",
            "solution": "The problem statement has been critically examined and is determined to be valid. All provided definitions, parameters, and constraints are scientifically sound, mathematically consistent, and form a well-posed problem. The problem is grounded in the established theory of reduced-order modeling, specifically Proper Orthogonal Decomposition (POD) in weighted inner product spaces, which is a standard topic in numerical analysis and scientific computing. The specified test cases, including the boundary cases, are constructed to meaningfully test the core concepts rather than trivialize them. We may therefore proceed with a complete solution.\n\nThe central task is to quantify the difference in projection error that arises from using a standard Euclidean inner product versus a physically motivated weighted inner product for constructing a reduced-order basis. This difference, $\\Delta(r; M, X)$, highlights the importance of selecting an appropriate metric for model reduction, especially when dealing with data on nonuniform grids.\n\nLet us begin by formalizing the concepts involved.\n\nA set of $m$ snapshots of a system state defined on a spatial domain with $n$ nodes is given by the columns of a matrix $X \\in \\mathbb{R}^{n \\times m}$. The goal of POD is to find an $r$-dimensional linear subspace, with $r \\ll n$, that provides the best possible approximation to the columns of $X$. The notion of \"best\" is defined by minimizing the mean-squared projection error, which depends on the choice of inner product on the state space $\\mathbb{R}^{n}$.\n\nGiven a symmetric positive definite matrix $M \\in \\mathbb{R}^{n \\times n}$, the $M$-weighted inner product is $\\langle a,b\\rangle_{M} = a^{\\top} M b$, with the induced norm $\\|a\\|_{M}^{2} = a^{\\top} M a$. The total \"energy\" of the snapshots in this norm is the squared $M$-weighted Frobenius norm of $X$, given by $\\|X\\|_{M,F}^2 = \\operatorname{trace}(X^{\\top} M X)$.\n\nThe orthogonal projector onto the span of a basis $U \\in \\mathbb{R}^{n \\times r}$ (where the columns of $U$ are linearly independent) in this weighted inner product space is given by the provided formula:\n$$\nP_{M}(U) = U\\left(U^{\\top} M U\\right)^{-1}U^{\\top} M\n$$\nThe projection error for the snapshot matrix $X$ onto $\\operatorname{span}(U)$ is:\n$$\nE(U; M, X) = \\left\\| X - P_{M}(U) X \\right\\|_{M,F}^{2} = \\operatorname{trace}\\!\\left(\\left(X - P_{M}(U)X\\right)^{\\top} M \\left(X - P_{M}(U)X\\right)\\right)\n$$\nThe POD problem seeks the basis $U$ of rank $r$ that minimizes this error.\n\nThe solution to this minimization problem is found via a change of variables and the Singular Value Decomposition (SVD). Let $M^{1/2}$ be the unique symmetric positive definite square root of $M$. We define a transformed state vector $z = M^{1/2}a$. Then the $M$-weighted inner product becomes a standard Euclidean inner product in the transformed space:\n$$\n\\langle a,b\\rangle_{M} = a^{\\top} M b = \\left(M^{1/2}a\\right)^{\\top}\\left(M^{1/2}b\\right) = z_a^{\\top} z_b = \\langle z_a, z_b \\rangle_2\n$$\nThe problem of finding the optimal $M$-weighted basis for $X$ is equivalent to finding the optimal Euclidean basis for the transformed snapshot matrix $Y = M^{1/2}X$.\n\nThe solution to the standard Euclidean POD problem is given by the Eckart-Young-Mirsky theorem. The optimal rank-$r$ basis that minimizes $\\|Y - Q_r Q_r^{\\top} Y\\|_F^2$ is spanned by the leading $r$ left singular vectors of $Y$. Let the SVD of $Y$ be $Y=Q \\Sigma V^{\\top}$. The optimal basis for $Y$ is $Q_r$, the first $r$ columns of $Q$.\nThe corresponding optimal basis for the original problem, $U_{r}^{M}$, is found by transforming back: $U_{r}^{M} = M^{-1/2}Q_r$. By construction, this basis is $M$-orthonormal: $(U_{r}^{M})^{\\top}M U_{r}^{M} = (M^{-1/2}Q_r)^{\\top} M (M^{-1/2}Q_r) = Q_r^{\\top}M^{-1/2} M M^{-1/2}Q_r = Q_r^{\\top}Q_r = I_r$.\n\nThe minimum projection error, $E(U_r^M; M, X)$, can be calculated efficiently. It is equal to the minimum Euclidean projection error of $Y$, which is the sum of the squares of the neglected singular values of $Y$. Let $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$ be the singular values of $Y=M^{1/2}X$, where $k=\\min(n, m)$. Then:\n$$\nE(U_{r}^{M}; M, X) = \\sum_{j=r+1}^{k} \\sigma_j^2\n$$\n\nThe Euclidean POD basis, $U_{r}^{E}$, is a special case where $M=I_n$. It is formed by the leading $r$ left singular vectors of $X$ itself. When used to project in the $M$-weighted inner product space (for $M \\neq I_n$), this basis is generally suboptimal. The projection error $E(U_{r}^{E}; M, X)$ must be computed using the general formula, as $U_{r}^{E}$ is not necessarily $M$-orthonormal:\n$$\nE(U_{r}^{E}; M, X) = \\left\\| X - P_{M}(U_r^E) X \\right\\|_{M,F}^{2}\n$$\nwhere $P_{M}(U_r^E) = U_r^E \\left((U_r^E)^{\\top} M U_r^E\\right)^{-1}(U_r^E)^{\\top} M$.\n\nThe quantity of interest is $\\Delta(r; M, X) = E(U_{r}^{E}; M, X) - E(U_{r}^{M}; M, X)$. By the optimality of $U_{r}^{M}$, we must have $\\Delta \\ge 0$.\n\nSpecial Cases:\n- **Case A**: $M=I_n$. The $M$-weighted inner product is the Euclidean inner product. Thus, $M$-weighted POD is identical to Euclidean POD. $U_r^M = U_r^E$, and consequently $E(U_r^E; I_n, X) = E(U_r^M; I_n, X)$. We expect $\\Delta = 0$.\n- **Case D**: $r=m=6$. The number of basis vectors equals the number of snapshots. The column space of $X$, $\\operatorname{span}(X)$, has dimension at most $m$. Both $U_m^E$ and $U_m^M$ will form a basis for $\\operatorname{span}(X)$. Any column of $X$ is therefore in both $\\operatorname{span}(U_m^E)$ and $\\operatorname{span}(U_m^M)$. The projection of a vector onto a subspace containing it is the vector itself. Hence, $P_M(U_m^E)X = X$ and $P_M(U_m^M)X = X$. Both projection errors are zero, and we expect $\\Delta = 0 - 0 = 0$.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Set the parameters $n=80$, $m=6$, and the case-specific rank $r$.\n2.  Generate the spatial mesh vector $x \\in \\mathbb{R}^n$ based on the case definition.\n3.  Construct the diagonal mass matrix $M \\in \\mathbb{R}^{n \\times n}$. For Cases B, C, D, this involves computing the trapezoidal quadrature weights. For Case A, $M=I_n$.\n4.  Construct the snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ using the specified formula.\n5.  Compute the Euclidean POD basis $U_r^E$:\n    a. Perform an SVD on $X$: $X = U S V^\\top$.\n    b. Extract the first $r$ columns of $U$ to form $U_r^E$.\n6.  Compute the error $E(U_{r}^{E}; M, X)$:\n    a. Compute the projection of $X$: $X_{proj} = U_r^E \\left((U_r^E)^{\\top} M U_r^E\\right)^{-1}(U_r^E)^{\\top} M X$.\n    b. Compute the error matrix: $E_{mat} = X - X_{proj}$.\n    c. Calculate the error: $E(U_{r}^{E}; M, X) = \\operatorname{trace}(E_{mat}^{\\top} M E_{mat})$.\n7.  Compute the optimal error $E(U_{r}^{M}; M, X)$:\n    a. Since $M$ is diagonal, compute $M^{1/2} = \\operatorname{diag}(\\sqrt{w_i})$.\n    b. Form the weighted matrix $Y = M^{1/2} X$.\n    c. Perform an SVD on $Y$ to obtain its singular values, $\\{\\sigma_j\\}$.\n    d. Calculate the error: $E(U_{r}^{M}; M, X) = \\sum_{j=r+1}^{m} \\sigma_j^2$.\n8.  Compute the difference $\\Delta(r; M, X) = E(U_{r}^{E}; M, X) - E(U_{r}^{M}; M, X)$.\n\nThis procedure is implemented for each of the four specified cases to obtain the final results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd, inv\n\ndef solve():\n    \"\"\"\n    Computes the difference in M-norm projection errors between Euclidean POD\n    and M-weighted POD for four specified test cases.\n    \"\"\"\n    \n    n = 80\n    m = 6\n\n    def construct_snapshots(x_nodes: np.ndarray) - np.ndarray:\n        \"\"\"Constructs the snapshot matrix X based on the spatial mesh.\"\"\"\n        X = np.zeros((n, m))\n        j_vals = np.arange(1, m + 1)\n        for j_idx, j in enumerate(j_vals):\n            X[:, j_idx] = np.sin(2 * np.pi * j * x_nodes) + \\\n                          0.1 * j * np.cos(2 * np.pi * (j - 1) * x_nodes)\n        return X\n\n    def construct_mass_matrix(x_nodes: np.ndarray) - np.ndarray:\n        \"\"\"Constructs the trapezoidal quadrature weight matrix M.\"\"\"\n        if np.array_equal(x_nodes, np.linspace(0, 1, n)): # Heuristic for uniform case\n             # Check if it could be the uniform case. We know from the problem\n             # statement that only Case A has M=I. This logic is safe.\n             is_uniform_case = True\n             for i, val in enumerate(x_nodes):\n                 if not np.isclose(val, (i)/(n-1)):\n                     is_uniform_case = False\n                     break\n             if is_uniform_case:\n                return np.identity(n)\n\n        w = np.zeros(n)\n        w[0] = 0.5 * (x_nodes[1] - x_nodes[0])\n        w[n - 1] = 0.5 * (x_nodes[n - 1] - x_nodes[n - 2])\n        for i in range(1, n - 1):\n            w[i] = 0.5 * (x_nodes[i + 1] - x_nodes[i - 1])\n        return np.diag(w)\n\n    def compute_delta(r: int, x_nodes: np.ndarray, use_identity_m: bool) - float:\n        \"\"\"\n        Computes Delta(r; M, X) for a given configuration.\n        \"\"\"\n        X = construct_snapshots(x_nodes)\n        \n        if use_identity_m:\n            M = np.identity(n)\n        else:\n            M = construct_mass_matrix(x_nodes)\n\n        # 1. Compute M-weighted POD error (optimal error)\n        M_sqrt = np.sqrt(M) # Works for diagonal M\n        Y = M_sqrt @ X\n        _, s_y, _ = svd(Y, full_matrices=False)\n        \n        # Error is the sum of squares of neglected singular values\n        error_M_pod = np.sum(s_y[r:]**2)\n\n        # 2. Compute Euclidean POD error (suboptimal error in M-norm)\n        U, _, _ = svd(X, full_matrices=False)\n        UrE = U[:, :r]\n\n        # Projector: P_M(U) = U * (U^T * M * U)^-1 * U^T * M\n        U_T_M_U = UrE.T @ M @ UrE\n        inv_U_T_M_U = inv(U_T_M_U)\n        P_M_UrE = UrE @ inv_U_T_M_U @ UrE.T @ M\n        \n        # Error: ||X - P_M(U)X||_M^2 = trace((X-PX)^T * M * (X-PX))\n        error_matrix = X - P_M_UrE @ X\n        error_E_pod = np.trace(error_matrix.T @ M @ error_matrix)\n\n        # 3. Compute the difference\n        delta = error_E_pod - error_M_pod\n        return delta\n\n    # Define test cases\n    p = np.linspace(0, 1, n)\n    test_cases_params = [\n        # Case A: Uniform mesh, M=I, r=2\n        {'r': 2, 'x_nodes': p, 'use_identity_m': True},\n        # Case B: Mild nonuniformity, Trapezoidal M, r=2\n        {'r': 2, 'x_nodes': p**1.5, 'use_identity_m': False},\n        # Case C: Strong nonuniformity, Trapezoidal M, r=2\n        {'r': 2, 'x_nodes': p**3.0, 'use_identity_m': False},\n        # Case D: Boundary rank, same mesh as C, r=6\n        {'r': 6, 'x_nodes': p**3.0, 'use_identity_m': False},\n    ]\n\n    results = []\n    for params in test_cases_params:\n        result = compute_delta(params['r'], params['x_nodes'], params['use_identity_m'])\n        # Handle potential small negative numbers from floating point arithmetic for expected zero results\n        if np.isclose(result, 0):\n            result = 0.0\n        results.append(result)\n\n    # Format the final output string\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Once an optimal basis is generated, the ultimate goal is to build a fast and accurate Reduced-Order Model (ROM) for prediction. This capstone exercise guides you through the full workflow, from simulating a parametric partial differential equation to generate snapshots, to building a POD-Galerkin ROM, and finally, to testing its predictive accuracy. The core challenge explored here is one of generalization: how well does a ROM, trained at a single parameter value, perform when asked to predict system behavior at different parameter values? This practice  provides essential, hands-on experience with the critical issue of parametric robustness, a key consideration in the practical deployment of ROMs.",
            "id": "3265932",
            "problem": "Consider the one-dimensional linear advection-diffusion initial-boundary value problem on the spatial interval $x \\in (0,1)$ with homogeneous Dirichlet boundary conditions, governed by the partial differential equation\n$$\n\\frac{\\partial u}{\\partial t}(x,t;\\mu) \\;=\\; \\mu \\,\\frac{\\partial^2 u}{\\partial x^2}(x,t;\\mu) \\;-\\; a \\,\\frac{\\partial u}{\\partial x}(x,t;\\mu), \\quad u(0,t;\\mu) = 0,\\quad u(1,t;\\mu) = 0,\\quad u(x,0;\\mu) = u_0(x),\n$$\nwhere $\\mu > 0$ is the diffusion parameter and $a \\ge 0$ is the advection speed. The Reduced-Order Model (ROM) will be constructed using a Galerkin projection onto a basis obtained via Proper Orthogonal Decomposition (POD) of snapshots generated at a training parameter $\\mu_{\\text{train}}$, and then applied to test parameters $\\mu_{\\text{test}}$ of the same partial differential equation.\n\nStarting from fundamental definitions:\n- Use the method-of-lines to semi-discretize the spatial derivatives on $N$ interior grid points with uniform spacing $h$, where $h = \\frac{1}{N+1}$ and the interior nodes are $x_i = i h$ for $i = 1,2,\\dots,N$. Approximate $\\frac{\\partial^2 u}{\\partial x^2}$ using the standard second-order centered finite difference, and approximate $\\frac{\\partial u}{\\partial x}$ using the first-order upwind finite difference consistent with $a \\ge 0$. This yields a semi-discrete ordinary differential equation\n$$\n\\frac{d \\mathbf{u}}{dt}(t;\\mu) \\;=\\; A(\\mu)\\,\\mathbf{u}(t;\\mu),\n$$\nwhere $\\mathbf{u}(t;\\mu) \\in \\mathbb{R}^N$ and $A(\\mu) \\in \\mathbb{R}^{N \\times N}$ is the parameter-dependent spatial operator formed from the discretizations.\n- Advance the semi-discrete system in time using the implicit Euler method. With time step $\\Delta t$ and $t_k = k\\,\\Delta t$, the update satisfies\n$$\n\\left(I - \\Delta t\\,A(\\mu)\\right)\\,\\mathbf{u}^{k+1} \\;=\\; \\mathbf{u}^{k},\n$$\nwhere $\\mathbf{u}^k \\approx \\mathbf{u}(t_k;\\mu)$.\n- Generate a snapshot matrix $X \\in \\mathbb{R}^{N \\times (K+1)}$ at the training parameter $\\mu_{\\text{train}}$ by concatenating the state vectors $\\mathbf{u}^k$ over $k = 0,1,\\dots,K$ for final time $T = K\\,\\Delta t$. Compute the Proper Orthogonal Decomposition (POD) basis of rank $r$ using the Singular Value Decomposition (SVD), $X = U\\,\\Sigma\\,V^\\top$, and take the first $r$ columns of $U$ as the orthonormal basis $U_r \\in \\mathbb{R}^{N \\times r}$. Proper Orthogonal Decomposition (POD) is defined here as the computation of an orthonormal basis that maximizes captured energy (in the Euclidean inner product) from the snapshot ensemble via the Singular Value Decomposition (SVD).\n- Form the Galerkin-projected Reduced-Order Model (ROM) for a test parameter $\\mu_{\\text{test}}$ by projecting the full operator onto the POD basis, yielding\n$$\nA_r(\\mu_{\\text{test}}) \\;=\\; U_r^\\top A(\\mu_{\\text{test}}) U_r \\in \\mathbb{R}^{r \\times r},\n$$\nand evolve the reduced system with implicit Euler:\n$$\n\\left(I_r - \\Delta t\\,A_r(\\mu_{\\text{test}})\\right)\\,\\mathbf{y}^{k+1} \\;=\\; \\mathbf{y}^{k}, \\quad \\text{with} \\quad \\mathbf{y}^0 \\;=\\; U_r^\\top \\mathbf{u}^0, \\quad \\mathbf{u}_{\\text{ROM}}^{k} \\;=\\; U_r\\,\\mathbf{y}^{k}.\n$$\n\nYour program must implement the above using the following scientifically consistent and self-contained test suite and parameters:\n- Spatial discretization:\n  - $N = 64$ interior points.\n  - Domain length $L = 1$, grid spacing $h = \\frac{L}{N+1}$, interior nodes $x_i = i\\,h$ for $i=1,\\dots,N$.\n  - Homogeneous Dirichlet boundaries are enforced by the stencil construction on the interior unknowns.\n- Time discretization:\n  - Implicit Euler with $\\Delta t = 10^{-3}$.\n  - Final time $T = 0.2$, so $K = \\frac{T}{\\Delta t} = 200$ time steps.\n- PDE parameters:\n  - Advection speed $a = 1.0$.\n  - Training diffusion parameter $\\mu_{\\text{train}} = 0.01$.\n- Initial condition:\n  - $u_0(x) = \\sin(\\pi x) + \\tfrac{1}{2}\\,\\sin(2\\pi x)$, discretized on the interior nodes to form $\\mathbf{u}^0 \\in \\mathbb{R}^N$.\n- POD basis:\n  - Rank $r = 6$ constructed from snapshots at $\\mu_{\\text{train}}$ using the Singular Value Decomposition (SVD) of the snapshot matrix as described above.\n\nEvaluate the ROM at the following four test parameters to analyze the impact of generating the basis from a different parameter regime:\n- Case $1$: $\\mu_{\\text{test}} = \\mu_{\\text{train}}$.\n- Case $2$: $\\mu_{\\text{test}} = 1.2\\,\\mu_{\\text{train}}$.\n- Case $3$: $\\mu_{\\text{test}} = 5\\,\\mu_{\\text{train}}$.\n- Case $4$: $\\mu_{\\text{test}} = 10^{-4}$.\n\nFor each case, compute the final-time relative error between the full-order model (FOM) and ROM solutions,\n$$\ne_{\\text{final}} \\;=\\; \\frac{\\left\\|\\mathbf{u}^{K} - \\mathbf{u}_{\\text{ROM}}^{K}\\right\\|_2}{\\left\\|\\mathbf{u}^{K}\\right\\|_2},\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases $[\\text{Case 1}, \\text{Case 2}, \\text{Case 3}, \\text{Case 4}]$, with each value printed as a decimal with eight digits after the decimal point (e.g., $[0.00123456,0.00234567,0.01234567,0.12345678]$). No physical units are involved, and all angles, if any, are implicitly in radians, though none are used here. The answers are pure real numbers in decimal form.",
            "solution": "The user-provided problem is a well-defined and scientifically sound exercise in numerical methods for partial differential equations, specifically focusing on reduced-order modeling using Proper Orthogonal Decomposition (POD). The problem is validated as follows:\n\n- **Scientific Grounding**: The problem is based on the one-dimensional linear advection-diffusion equation, a canonical model in fluid dynamics and transport phenomena. The numerical methods specified—Method of Lines, finite differences (central for diffusion, upwind for advection), implicit Euler time integration, and POD via Singular Value Decomposition (SVD)—are all standard and rigorously established techniques in scientific computing. The problem is free of any scientific or factual unsoundness.\n- **Well-Posedness and Completeness**: The problem statement provides all necessary information to construct and solve the problem. This includes the PDE, domain, boundary and initial conditions, all numerical parameters ($N, L, a, \\Delta t, T, r$), the precise definitions of the discretization stencils, the time integration scheme, the procedure for POD basis generation, and the specific test cases. The implicit Euler method ensures numerical stability. The upwind scheme is appropriate for the advective term. The overall problem is well-posed and self-contained.\n- **Objectivity and Clarity**: The problem is stated in precise, unambiguous mathematical language. There is no subjective or opinion-based content. A minor notational ambiguity exists where the POD basis is called $V$ but is defined using the columns of $U$ from the SVD $X=U\\Sigma V^\\top$. However, this is clarified by the explicit instruction \"take the first $r$ columns of $U$ as the orthonormal basis,\" which aligns with the standard convention for POD of state trajectories. This does not impede a unique and correct implementation.\n\nGiven that the problem is valid, a step-by-step solution is developed.\n\n### 1. Full-Order Model (FOM) Discretization\n\nThe partial differential equation is:\n$$\n\\frac{\\partial u}{\\partial t} \\;=\\; \\mu \\,\\frac{\\partial^2 u}{\\partial x^2} \\;-\\; a \\,\\frac{\\partial u}{\\partial x}\n$$\nWe discretize the spatial domain $x \\in (0,1)$ into $N=64$ interior points $x_i = i h$ for $i=1,\\dots,N$, with grid spacing $h = 1/(N+1)$. The state of the system is represented by the vector $\\mathbf{u}(t) \\in \\mathbb{R}^N$, where $(\\mathbf{u})_i = u(x_i, t)$.\n\nThe spatial derivatives are approximated using finite differences as specified:\n- **Diffusion Term (2nd-order centered)**:\n$$\n\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{x_i} \\approx \\frac{u(x_{i+1},t) - 2u(x_i,t) + u(x_{i-1},t)}{h^2} = \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}\n$$\n- **Advection Term (1st-order upwind for $a \\ge 0$)**:\n$$\n\\frac{\\partial u}{\\partial x}\\bigg|_{x_i} \\approx \\frac{u(x_i,t) - u(x_{i-1},t)}{h} = \\frac{u_i - u_{i-1}}{h}\n$$\n\nSubstituting these approximations into the PDE for each interior node $x_i$ yields a system of ordinary differential equations (ODEs), $\\frac{d\\mathbf{u}}{dt} = A(\\mu)\\mathbf{u}$. The $i$-th equation is:\n$$\n\\frac{du_i}{dt} = \\mu \\left( \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} \\right) - a \\left( \\frac{u_i - u_{i-1}}{h} \\right)\n$$\nHomogeneous Dirichlet boundary conditions $u(0,t)=u_0=0$ and $u(1,t)=u_{N+1}=0$ are incorporated into the stencils at the boundaries ($i=1$ and $i=N$). Rearranging the terms for $u_{i-1}$, $u_i$, and $u_{i+1}$ reveals the structure of the system matrix $A(\\mu)$:\n$$\n\\frac{du_i}{dt} = \\left(\\frac{\\mu}{h^2} + \\frac{a}{h}\\right) u_{i-1} + \\left(-\\frac{2\\mu}{h^2} - \\frac{a}{h}\\right) u_i + \\left(\\frac{\\mu}{h^2}\\right) u_{i+1}\n$$\nThis defines a tridiagonal matrix $A(\\mu) \\in \\mathbb{R}^{N \\times N}$ with the following diagonals:\n- **Main diagonal**: $-\\frac{2\\mu}{h^2} - \\frac{a}{h}$\n- **Sub-diagonal (lower)**: $\\frac{\\mu}{h^2} + \\frac{a}{h}$\n- **Super-diagonal (upper)**: $\\frac{\\mu}{h^2}$\n\n### 2. Time Integration and Snapshot Generation\n\nThe semi-discrete system is advanced in time using the implicit Euler method with timestep $\\Delta t = 10^{-3}$:\n$$\n\\frac{\\mathbf{u}^{k+1} - \\mathbf{u}^{k}}{\\Delta t} = A(\\mu) \\mathbf{u}^{k+1} \\implies \\left(I - \\Delta t\\,A(\\mu)\\right)\\,\\mathbf{u}^{k+1} = \\mathbf{u}^{k}\n$$\nTo obtain the solution at step $k+1$, we solve a linear system at each time step.\n\nSnapshots are generated by solving the FOM for the training parameter $\\mu_{\\text{train}} = 0.01$. The initial condition is $\\mathbf{u}^0$, obtained by evaluating $u_0(x) = \\sin(\\pi x) + \\frac{1}{2}\\sin(2\\pi x)$ at the grid points $x_i$. The simulation runs up to time $T=0.2$, which corresponds to $K = T/\\Delta t = 200$ steps. The snapshot matrix $X \\in \\mathbb{R}^{N \\times (K+1)}$ is formed by collecting the state vectors:\n$$\nX = \\begin{bmatrix} |  |   | \\\\ \\mathbf{u}^0  \\mathbf{u}^1  \\dots  \\mathbf{u}^K \\\\ |  |   | \\end{bmatrix}\n$$\n\n### 3. POD Basis Construction\n\nThe POD basis is computed from the Singular Value Decomposition (SVD) of the snapshot matrix $X$:\n$$\nX = U \\Sigma V^\\top\n$$\nThe columns of the matrix $U \\in \\mathbb{R}^{N \\times (K+1)}$ are the left singular vectors, which are orthonormal and represent the spatial modes of the system's dynamics. The optimal rank-$r$ basis, in the sense of capturing the most energy, is formed by the first $r$ columns of $U$. We construct the basis matrix $U_r \\in \\mathbb{R}^{N \\times r}$ using $r=6$:\n$$\nU_r = U[:, 0:r]\n$$\n\n### 4. Reduced-Order Model (ROM) Formulation and Simulation\n\nThe ROM is constructed by projecting the governing FOM equations onto the low-dimensional subspace spanned by $U_r$. An approximate solution is sought in the form $\\mathbf{u}_{\\text{ROM}}(t) = U_r\\mathbf{y}(t)$, where $\\mathbf{y}(t) \\in \\mathbb{R}^r$ are the reduced coordinates.\nSubstituting this into the semi-discrete system and applying Galerkin projection (pre-multiplying by $U_r^\\top$) yields the ROM for a given test parameter $\\mu_{\\text{test}}$:\n$$\nU_r^\\top U_r \\frac{d\\mathbf{y}}{dt} = U_r^\\top A(\\mu_{\\text{test}}) U_r \\mathbf{y}\n$$\nSince $U_r$ is orthonormal ($U_r^\\top U_r = I_r$), this simplifies to:\n$$\n\\frac{d\\mathbf{y}}{dt} = A_r(\\mu_{\\text{test}}) \\mathbf{y}, \\quad \\text{where} \\quad A_r(\\mu_{\\text{test}}) = U_r^\\top A(\\mu_{\\text{test}}) U_r\n$$\nThe reduced operator $A_r \\in \\mathbb{R}^{r \\times r}$. The initial condition for the ROM is the projection of the full-order initial condition: $\\mathbf{y}^0 = U_r^\\top \\mathbf{u}^0$.\nThe ROM is time-stepped with implicit Euler, analogous to the FOM:\n$$\n\\left(I_r - \\Delta t\\,A_r(\\mu_{\\text{test}})\\right)\\,\\mathbf{y}^{k+1} = \\mathbf{y}^{k}\n$$\nAfter simulating to the final time $T$ to find $\\mathbf{y}^K$, the solution is reconstructed back to the full state space: $\\mathbf{u}_{\\text{ROM}}^K = U_r\\mathbf{y}^K$.\n\n### 5. Error Evaluation\n\nFor each test case $\\mu_{\\text{test}}$, we compute both the FOM solution $\\mathbf{u}^K$ and the ROM solution $\\mathbf{u}_{\\text{ROM}}^K$ at the final time $T=0.2$. The relative error is calculated as:\n$$\ne_{\\text{final}} = \\frac{\\|\\mathbf{u}^K - \\mathbf{u}_{\\text{ROM}}^K\\|_2}{\\|\\mathbf{u}^K\\|_2}\n$$\nThis procedure is repeated for all four specified values of $\\mu_{\\text{test}}$, and the resulting errors are reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a POD-Galerkin Reduced-Order Model for a 1D advection-diffusion\n    equation and evaluates its accuracy for several test parameters.\n    \"\"\"\n    # --------------------------------------------------------------------------\n    # 1. Define problem parameters and grid\n    # --------------------------------------------------------------------------\n    N = 64                  # Number of interior spatial points\n    L = 1.0                 # Domain length\n    a = 1.0                 # Advection speed\n    dt = 1.0e-3             # Time step\n    T = 0.2                 # Final time\n    r = 6                   # Rank of POD basis\n    mu_train = 0.01         # Training diffusion parameter\n\n    test_cases_mu = [\n        mu_train,\n        1.2 * mu_train,\n        5.0 * mu_train,\n        1.0e-4\n    ]\n\n    h = L / (N + 1)         # Grid spacing\n    K = int(T / dt)         # Number of time steps\n    x_grid = np.arange(1, N + 1) * h\n\n    # Initial condition\n    u0 = np.sin(np.pi * x_grid) + 0.5 * np.sin(2 * np.pi * x_grid)\n\n    # Memoization caches for efficiency\n    memo_A = {}\n    memo_fom_final_state = {}\n\n    # --------------------------------------------------------------------------\n    # 2. Helper functions for FOM and ROM\n    # --------------------------------------------------------------------------\n    def build_A(mu):\n        \"\"\"Constructs the FOM spatial discretization matrix A(mu).\"\"\"\n        if mu in memo_A:\n            return memo_A[mu]\n\n        h2 = h * h\n        diag_main = -2.0 * mu / h2 - a / h\n        diag_upper = mu / h2\n        diag_lower = mu / h2 + a / h\n        \n        A = np.zeros((N, N))\n        np.fill_diagonal(A, diag_main)\n        np.fill_diagonal(A[1:, :-1], diag_lower)\n        np.fill_diagonal(A[:-1, 1:], diag_upper)\n        \n        memo_A[mu] = A\n        return A\n\n    def run_fom(mu, collect_snapshots=False):\n        \"\"\"Simulates the Full-Order Model (FOM).\"\"\"\n        if not collect_snapshots and mu in memo_fom_final_state:\n            return memo_fom_final_state[mu], None\n\n        A = build_A(mu)\n        # Implicit Euler matrix: M * u^{k+1} = u^k\n        M = np.eye(N) - dt * A\n        u = u0.copy()\n\n        if collect_snapshots:\n            snapshots = np.zeros((N, K + 1))\n            snapshots[:, 0] = u\n            for k in range(K):\n                u = np.linalg.solve(M, u)\n                snapshots[:, k + 1] = u\n            return u, snapshots\n        else:\n            for k in range(K):\n                u = np.linalg.solve(M, u)\n            memo_fom_final_state[mu] = u\n            return u, None\n    \n    # --------------------------------------------------------------------------\n    # 3. Generate POD basis from training data\n    # --------------------------------------------------------------------------\n    _, X_train = run_fom(mu_train, collect_snapshots=True)\n    \n    # Compute SVD: X = U * S * Vh. The POD basis are the left singular vectors (columns of U).\n    # The problem description calls the basis 'V', but it's constructed from 'U'.\n    U, _, _ = np.linalg.svd(X_train, full_matrices=False)\n    V_basis = U[:, :r]\n\n    # --------------------------------------------------------------------------\n    # 4. Evaluate ROM for each test case\n    # --------------------------------------------------------------------------\n    results = []\n    \n    # Project initial condition onto the POD basis\n    y0 = V_basis.T @ u0\n    \n    for mu_test in test_cases_mu:\n        # Get the true FOM solution at the final time for comparison\n        u_fom_final, _ = run_fom(mu_test, collect_snapshots=False)\n\n        # Build and run the ROM\n        A_test = build_A(mu_test)\n        \n        # Project the FOM operator to get the ROM operator\n        A_r = V_basis.T @ A_test @ V_basis\n        \n        # Implicit Euler matrix for the ROM\n        M_r = np.eye(r) - dt * A_r\n        \n        y = y0.copy()\n        for _ in range(K):\n            y = np.linalg.solve(M_r, y)\n        \n        # Reconstruct the full-order solution from the ROM state\n        u_rom_final = V_basis @ y\n        \n        # Calculate final-time relative error\n        error_norm = np.linalg.norm(u_fom_final - u_rom_final)\n        fom_norm = np.linalg.norm(u_fom_final)\n        \n        if fom_norm == 0:\n            # Avoid division by zero, though unlikely for this problem\n            relative_error = error_norm\n        else:\n            relative_error = error_norm / fom_norm\n            \n        results.append(relative_error)\n        \n    # --------------------------------------------------------------------------\n    # 5. Format and print the final output\n    # --------------------------------------------------------------------------\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}