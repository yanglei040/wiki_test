{
    "hands_on_practices": [
        {
            "introduction": "Understanding a probabilistic model often begins with being able to generate samples from it. This practice provides a foundational hands-on skill: drawing samples from a Gaussian Markov random field (GMRF) defined by a precision matrix $Q$ . You will derive and implement the canonical sampling algorithm, which elegantly connects the Cholesky factorization of the precision matrix to the generation of structured random fields from simple white noise, and verify your implementation using Monte Carlo methods.",
            "id": "3384809",
            "problem": "Consider a Gaussian Markov random field (GMRF) prior for an unknown vector $x \\in \\mathbb{R}^n$ defined by a zero-mean multivariate normal distribution with a sparse, symmetric positive definite (SPD) precision matrix $Q \\in \\mathbb{R}^{n \\times n}$, that is, $x \\sim \\mathcal{N}(0, Q^{-1})$. Use only the following fundamental facts as your starting point: (i) the Cholesky factorization of an SPD matrix exists and is unique, meaning there is a lower-triangular matrix $L$ with positive diagonal entries such that $Q = L L^\\top$; (ii) if $z \\sim \\mathcal{N}(0, I)$, then for any fixed matrix $A$, the random vector $A z$ is Gaussian with mean $0$ and covariance $A A^\\top$; and (iii) solving triangular linear systems is numerically stable and efficient for lower-triangular matrices. From these bases, derive a sampling method to generate $x \\sim \\mathcal{N}(0, Q^{-1})$ by transforming an independent and identically distributed (i.i.d.) standard normal random vector through operations involving the Cholesky factor of $Q$. Your derivation must identify the transformation and justify correctness in terms of the resulting covariance.\n\nImplement the derived sampling method in a program that performs Monte Carlo verification on three SPD precision matrices representing GMRF priors. For each case, generate $N_s$ i.i.d. samples, compute the empirical mean and the second-moment matrix, and quantify how well the samples match the target distribution. Specifically, compute the Euclidean norm of the empirical mean and the relative Frobenius-norm error of the moment identity $Q \\, \\mathbb{E}[x x^\\top] = I$ using the empirical second moment in place of the exact expectation. For the scalar case, also compute the absolute error between the empirical second moment and the exact variance $Q^{-1}$. All numerical results must be reported as unitless floating-point numbers.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,...]\") and in the following order for the test suite below: for each matrix, output the empirical mean norm followed by the moment identity relative error, and for the scalar case additionally append the variance absolute error. Concretely, the output must be the list $[m_1, e_1, m_2, e_2, m_3, e_3]$ where $m_i$ is the empirical mean norm and $e_i$ is the specified error for case $i$, with $e_3$ being the scalar variance absolute error.\n\nUse the following test suite with fixed seeds and sample counts:\n\n- Case $1$ (one-dimensional chain graph): Let $n = 5$. Define the graph Laplacian $L_g \\in \\mathbb{R}^{n \\times n}$ with entries $[L_g]_{ii} = \\deg(i)$ for node degree $\\deg(i)$, $[L_g]_{i,i+1} = [L_g]_{i+1,i} = -1$ for edges between consecutive nodes, and zeros otherwise. Form the SPD precision $Q = L_g + \\gamma I$ with $\\gamma = 1$. Use $N_s = 20000$ and a fixed pseudorandom seed $42$.\n\n- Case $2$ (two-dimensional grid graph): Let the grid be $3 \\times 3$, so $n = 9$. Index nodes in row-major order and define the $2$-dimensional grid Laplacian $L_g \\in \\mathbb{R}^{n \\times n}$ with $[L_g]_{ii} = \\deg(i)$ equal to the number of in-grid neighbors of node $i$, off-diagonal entries $-1$ for each adjacency to a horizontal or vertical neighbor, and zeros otherwise. Form $Q = L_g + \\gamma I$ with $\\gamma = 1$. Use $N_s = 20000$ and a fixed pseudorandom seed $123$.\n\n- Case $3$ (scalar precision): Let $n = 1$ and $Q = [3]$. Use $N_s = 20000$ and a fixed pseudorandom seed $7$.\n\nFor each case, compute:\n- The empirical mean $\\hat{\\mu} = \\frac{1}{N_s} \\sum_{k=1}^{N_s} x^{(k)}$ and report its Euclidean norm $\\|\\hat{\\mu}\\|_2$ as a float.\n- The empirical second-moment matrix $\\hat{S} = \\frac{1}{N_s} \\sum_{k=1}^{N_s} x^{(k)} (x^{(k)})^\\top$ and report the relative Frobenius-norm error $\\frac{\\|Q \\hat{S} - I\\|_F}{\\|I\\|_F}$ as a float for cases $1$ and $2$.\n- For case $3$, also report the absolute error $|\\hat{v} - Q^{-1}|$ where $\\hat{v} = \\frac{1}{N_s} \\sum_{k=1}^{N_s} (x^{(k)})^2$ as a float.\n\nYour program must produce a single line $[m_1,e_1,m_2,e_2,m_3,e_3]$ in that exact order and format. Angles and physical units are not applicable; all results are unitless floating-point values.",
            "solution": "It is our objective to derive a method for generating random samples of a vector $x \\in \\mathbb{R}^n$ from a zero-mean multivariate normal distribution defined by its precision matrix, $x \\sim \\mathcal{N}(0, Q^{-1})$. The precision matrix $Q \\in \\mathbb{R}^{n \\times n}$ is given as sparse, symmetric, and positive definite (SPD). The derivation will be constructed from a set of specified fundamental principles.\n\nThe core strategy is to transform a vector of independent and identically distributed (i.i.d.) standard normal random variables into a vector that exhibits the desired distributional properties. Let $z \\in \\mathbb{R}^n$ be a random vector whose components are drawn independently from the standard normal distribution, i.e., $z \\sim \\mathcal{N}(0, I)$, where $I$ is the $n \\times n$ identity matrix. We posit a linear transformation, defined by a constant matrix $A \\in \\mathbb{R}^{n \\times n}$, such that the transformed vector $x = Az$ follows the target distribution $\\mathcal{N}(0, Q^{-1})$.\n\nWe first appeal to the provided fact (ii), which addresses the distribution of a linear transformation of a Gaussian random vector. According to this principle, if $z \\sim \\mathcal{N}(0, I)$, the vector $x = Az$ is also Gaussian. Its mean is given by $\\mathbb{E}[x] = \\mathbb{E}[Az] = A\\mathbb{E}[z] = A \\cdot 0 = 0$. This confirms the zero-mean property. The covariance matrix of $x$ is given by $\\text{Cov}(x) = \\mathbb{E}[xx^\\top] = \\mathbb{E}[(Az)(Az)^\\top] = \\mathbb{E}[Azz^\\top A^\\top] = A\\mathbb{E}[zz^\\top]A^\\top$. Since the components of $z$ are i.i.d. standard normal, its covariance matrix is the identity, $\\mathbb{E}[zz^\\top] = I$. Therefore, the covariance of $x$ is $\\text{Cov}(x) = AIA^\\top = AA^\\top$.\n\nFor $x$ to be distributed as $\\mathcal{N}(0, Q^{-1})$, its covariance matrix must be equal to $Q^{-1}$. This imposes the following condition on our transformation matrix $A$:\n$$AA^\\top = Q^{-1}$$\n\nTo find a suitable matrix $A$, we now utilize fact (i). This fact states that any SPD matrix, such as our precision matrix $Q$, admits a unique Cholesky factorization of the form $Q = LL^\\top$, where $L$ is a lower-triangular matrix with positive diagonal entries. By taking the matrix inverse of this factorization, we obtain an expression for the required covariance matrix $Q^{-1}$:\n$$Q^{-1} = (LL^\\top)^{-1} = (L^\\top)^{-1}L^{-1}$$\n\nBy comparing the condition $AA^\\top = Q^{-1}$ with the derived expression $Q^{-1} = (L^\\top)^{-1}L^{-1}$, we can identify a valid choice for the transformation matrix $A$. By inspection, we can set $A = (L^\\top)^{-1}$. With this choice, the transformation becomes $x = (L^\\top)^{-1}z$.\n\nThis equation for $x$ should not be implemented by explicitly computing the inverse of $L^\\top$. Instead, we can rewrite it as an equivalent linear system:\n$$L^\\top x = z$$\n\nThe final sampling algorithm is thus delineated. To generate one sample $x$ from $\\mathcal{N}(0, Q^{-1})$:\n1.  Compute the Cholesky factor $L$ of the precision matrix $Q$ such that $Q = LL^\\top$.\n2.  Generate a sample vector $z \\in \\mathbb{R}^n$ from the standard normal distribution, $z \\sim \\mathcal{N}(0, I)$.\n3.  Solve the linear system of equations $L^\\top x = z$ for the unknown vector $x$.\n\nThis procedure is computationally sound. Since $L$ is a lower-triangular matrix, its transpose $L^\\top$ is an upper-triangular matrix. As affirmed by fact (iii), solving a triangular linear system is a numerically stable and efficient operation, typically accomplished via back substitution.\n\nThis completes the derivation of a correct and efficient sampling method for a Gaussian Markov random field, founded strictly upon the provided principles. The implementation will follow this derived algorithm for the Monte Carlo verification.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Derives and implements a GMRF sampling method, and runs Monte Carlo verification.\n    \"\"\"\n\n    def analyze_case(Q, n, Ns, seed, case_id):\n        \"\"\"\n        Analyzes a single GMRF case by sampling and computing statistics.\n\n        Args:\n            Q (np.ndarray): The precision matrix.\n            n (int): The dimension of the state vector.\n            Ns (int): The number of samples to generate.\n            seed (int): The seed for the random number generator.\n            case_id (int): An identifier for the case (1, 2, or 3).\n\n        Returns:\n            tuple: A tuple containing the empirical mean norm and the relevant error metric.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Compute the Cholesky factor L of the precision matrix Q such that Q = L L^T.\n        # The method is based on the derivation that if z ~ N(0, I), then x solved from\n        # L.T @ x = z will have the distribution N(0, Q^-1).\n        L = scipy.linalg.cholesky(Q, lower=True)\n\n        # Step 2: Generate Ns samples from a standard normal distribution.\n        # Z is a matrix of size n x Ns, where each column is an independent sample z.\n        Z = rng.standard_normal(size=(n, Ns))\n\n        # Step 3: Solve L.T @ x = z for each sample z to get samples x from N(0, Q^-1).\n        # solve_triangular is used for efficiency as L.T is upper-triangular.\n        X = scipy.linalg.solve_triangular(L.T, Z, lower=False)\n\n        # Step 4: Compute empirical mean and its norm.\n        mu_hat = np.mean(X, axis=1)\n        mean_norm = np.linalg.norm(mu_hat)\n\n        # Step 5: Compute empirical second-moment matrix.\n        S_hat = (X @ X.T) / Ns\n\n        # Step 6: Compute the specified error metric for the case.\n        if case_id in [1, 2]:\n            # For cases 1 and 2, compute the relative Frobenius-norm error of the moment identity.\n            I = np.identity(n)\n            # The identity is Q * E[x x^T] = Q * Q^-1 = I.\n            # We check ||Q @ S_hat - I||_F / ||I||_F\n            error_numerator = np.linalg.norm(Q @ S_hat - I, ord='fro')\n            # ||I||_F = sqrt(n)\n            error_denominator = np.sqrt(n)\n            error = error_numerator / error_denominator\n        elif case_id == 3:\n            # For scalar case 3, compute the absolute error of the variance.\n            v_hat = S_hat[0, 0]\n            # Exact variance is Q^-1.\n            variance_exact = 1.0 / Q[0, 0]\n            error = np.abs(v_hat - variance_exact)\n        else:\n            # This path should not be reached with the given problem statement.\n            error = np.nan\n\n        return mean_norm, error\n\n    def build_q_case1(n, gamma):\n        \"\"\"Constructs the precision matrix Q for Case 1 (1D chain graph).\"\"\"\n        Lg = np.zeros((n, n))\n        diag = 2.0 * np.ones(n)\n        diag[0], diag[-1] = 1.0, 1.0\n        np.fill_diagonal(Lg, diag)\n        off_diag = -1.0 * np.ones(n - 1)\n        Lg += np.diag(off_diag, k=1)\n        Lg += np.diag(off_diag, k=-1)\n        return Lg + gamma * np.identity(n)\n\n    def build_q_case2(n, gamma):\n        \"\"\"Constructs the precision matrix Q for Case 2 (2D grid graph).\"\"\"\n        dim = int(np.sqrt(n))\n        Lg = np.zeros((n, n))\n        for r in range(dim):\n            for c in range(dim):\n                i = r * dim + c\n                degree = 0\n                for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                    nr, nc = r + dr, c + dc\n                    if 0 = nr  dim and 0 = nc  dim:\n                        degree += 1\n                        j = nr * dim + nc\n                        Lg[i, j] = -1.0\n                Lg[i, i] = degree\n        return Lg + gamma * np.identity(n)\n\n    # Define test cases from the problem statement\n    test_cases = [\n        {'id': 1, 'n': 5, 'Ns': 20000, 'seed': 42, 'gamma': 1.0, 'q_builder': build_q_case1},\n        {'id': 2, 'n': 9, 'Ns': 20000, 'seed': 123, 'gamma': 1.0, 'q_builder': build_q_case2},\n        {'id': 3, 'n': 1, 'Ns': 20000, 'seed': 7, 'Q': np.array([[3.0]])},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['id'] in [1, 2]:\n            Q = case['q_builder'](case['n'], case['gamma'])\n        else: # Case 3\n            Q = case['Q']\n        \n        m, e = analyze_case(Q=Q, n=case['n'], Ns=case['Ns'], seed=case['seed'], case_id=case['id'])\n        results.extend([m, e])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While GMRFs are defined by their precision matrix $Q$, a crucial question for practitioners is how to construct a meaningful $Q$ for a specific problem. This exercise connects GMRFs to the physics of continuous fields by guiding you through the construction of a precision matrix from the finite element discretization of a differential operator . You will implement the assembly for a nonuniform mesh with mixed boundary conditions and then use eigenvalue analysis to investigate the posterior nullspace, a critical step in diagnosing what a given experiment can and cannot resolve.",
            "id": "3384822",
            "problem": "You are asked to construct the precision matrix $Q$ induced by discretizing the negative one-dimensional Laplace operator $-\\Delta$ on a nonuniform mesh with mixed Dirichlet and Neumann boundary conditions, and to analyze the posterior nullspace when this precision is used as a Gaussian Markov random field (GMRF) prior in a linear inverse problem. Use the following principles and specifications.\n\nConstruct the discrete precision matrix as follows. Consider a one-dimensional domain $[0, L]$ with mesh nodes at positions $x_0, x_1, \\ldots, x_M$, where $x_0 = 0$, $x_M = L$, and the mesh is nonuniform, meaning the element lengths $h_i = x_{i+1} - x_i$ for $i = 0, 1, \\ldots, M-1$ need not be equal. Discretize $-\\Delta$ with the mixed boundary conditions: Dirichlet at the left boundary, that is, $u(0) = 0$, and Neumann at the right boundary, that is, $u'(L) = 0$. Use the standard linear finite element method to assemble the stiffness matrix $K \\in \\mathbb{R}^{(M+1) \\times (M+1)}$ by summing the contributions of each element. For each element $e_i = [x_i, x_{i+1}]$ with length $h_i$, the local stiffness matrix is\n$$\nK^{(e_i)} = \\frac{1}{h_i}\n\\begin{bmatrix}\n1  -1 \\\\\n-1  1\n\\end{bmatrix},\n$$\nand $K$ is obtained by summing $K^{(e_i)}$ into the appropriate entries for global nodes $i$ and $i+1$. Enforce the Dirichlet boundary condition $u(0)=0$ by removing the row and column of $K$ corresponding to the left boundary node $x_0$, yielding the precision matrix $Q \\in \\mathbb{R}^{M \\times M}$ over the unknowns $x = [u(x_1), \\ldots, u(x_M)]^\\top$. The Neumann boundary at $x_M$ is natural for this operator and requires no explicit modification to the assembled stiffness.\n\nInterpret $Q$ as the prior precision of a Gaussian Markov random field (GMRF) prior on $x$. Consider the linear inverse problem\n$$\ny = H x + \\varepsilon,\n$$\nwhere $y \\in \\mathbb{R}^m$ are observations, $H \\in \\mathbb{R}^{m \\times M}$ is the observation operator, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ is zero-mean Gaussian noise with covariance $R = \\sigma^2 I$. Let $\\alpha  0$ denote a scalar precision scaling for the prior. Under these assumptions, the posterior precision is\n$$\n\\Lambda = \\alpha Q + H^\\top R^{-1} H = \\alpha Q + \\frac{1}{\\sigma^2} H^\\top H.\n$$\n\nDefine the posterior nullspace dimension $d$ as the number of eigenvalues of $\\Lambda$ strictly less than a prescribed numerical threshold $\\tau  0$. This numerical criterion captures directions that are effectively uninformative in the posterior due to exact or near-singular precision. Use $\\tau = 10^{-12}$.\n\nYour program must:\n- Assemble the precision matrix $Q$ from the given nonuniform mesh via the finite element method as described.\n- For each test case, construct $H$, set $\\sigma$ and $\\alpha$, form $\\Lambda$, compute its eigenvalues, and count how many are strictly less than $\\tau$ to obtain $d$.\n\nUse the following test suite, which is designed to probe different behaviors:\n- Test case $1$ (happy path, fully observed): Mesh nodes $[0.0, 0.2, 0.5, 0.75, 1.0]$ so $M = 4$. Use $\\alpha = 1.0$, $\\sigma = 0.2$, and $H = I_{4}$ (the $4 \\times 4$ identity). Expect a strictly positive definite $\\Lambda$ with $d = 0$.\n- Test case $2$ (likelihood-only, rank-deficient): Same mesh. Use $\\alpha = 0.0$, $\\sigma = 0.3$, and $H$ with $m = 2$ rows selecting the first and third components of $x$, specifically $H = \\begin{bmatrix} 1  0  0  0 \\\\ 0  0  1  0 \\end{bmatrix}$. The rank of $H$ is $2$, so the rank of $H^\\top H$ is $2$ and the nullspace dimension should be $M - 2 = 2$ when counted with the threshold $\\tau$.\n- Test case $3$ (near-singular due to vanishing prior, low-rank data): Same mesh. Use $\\alpha = 10^{-16}$, $\\sigma = 0.1$, and $H$ a single-row operator with all ones, $H = \\begin{bmatrix} 1  1  1  1 \\end{bmatrix}$. Then $H^\\top H$ has rank $1$, and the prior contribution $\\alpha Q$ is extremely small, so with the threshold $\\tau$ the effective nullspace dimension should be $M - 1 = 3$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[d_1,d_2,d_3]$, where each $d_i$ is the nullspace dimension for test case $i$. The outputs must be integers and must follow the order of the test cases listed above.",
            "solution": "The problem requires the construction of a prior precision matrix $Q$ from a finite element discretization of the one-dimensional negative Laplace operator $-\\Delta$ on a nonuniform mesh with mixed boundary conditions. This precision matrix is then used in a Bayesian linear inverse problem to form a posterior precision matrix $\\Lambda$. The objective is to determine the dimension of the posterior nullspace, defined numerically as the number of eigenvalues of $\\Lambda$ below a given threshold $\\tau$.\n\nThe solution proceeds in four steps: 1) assembly of the global stiffness matrix $K$ for the operator $-\\Delta$, 2) derivation of the prior precision $Q$ by enforcing the boundary conditions, 3) formation of the posterior precision $\\Lambda$ by combining the prior with the likelihood information, and 4) computation of the posterior nullspace dimension $d$ via eigenvalue analysis.\n\n**Step 1: Assembling the Global Stiffness Matrix $K$**\n\nThe domain is a one-dimensional interval $[0, L]$ discretized by $M+1$ nodes $x_0, x_1, \\ldots, x_M$, with $x_0 = 0$ and $x_M = L$. The mesh is nonuniform, with element lengths $h_i = x_{i+1} - x_i$ for $i = 0, \\ldots, M-1$.\n\nThe negative Laplace operator is discretized using the linear finite element method. The variational or \"weak\" formulation of $-\\Delta u = f$ involves finding $u$ such that for all test functions $v$, $\\int_0^L u'(x) v'(x) dx = \\int_0^L f(x) v(x) dx$, subject to boundary conditions. The left-hand side gives rise to the stiffness matrix.\n\nThe contribution of each element $e_i = [x_i, x_{i+1}]$ to the global stiffness matrix is given by the local stiffness matrix $K^{(e_i)}$. For linear basis functions, this is:\n$$\nK^{(e_i)} = \\frac{1}{h_i}\n\\begin{bmatrix}\n1  -1 \\\\\n-1  1\n\\end{bmatrix}\n$$\nThe global stiffness matrix $K \\in \\mathbb{R}^{(M+1) \\times (M+1)}$ is assembled by initializing it as a zero matrix and summing the contributions from all elements. For each element $e_i$, the entries of $K^{(e_i)}$ are added to the corresponding global locations in $K$. Specifically, the $2 \\times 2$ matrix $K^{(e_i)}$ is added to the submatrix of $K$ corresponding to nodes $i$ and $i+1$.\n\nThis assembly process results in a symmetric, tridiagonal matrix $K$. For an interior node $j \\in \\{1, \\ldots, M-1\\}$, the diagonal entry is $K_{j,j} = \\frac{1}{h_{j-1}} + \\frac{1}{h_j}$, and the off-diagonal entries are $K_{j, j-1} = -\\frac{1}{h_{j-1}}$ and $K_{j, j+1} = -\\frac{1}{h_j}$. At the boundaries, $K_{0,0} = \\frac{1}{h_0}$ and $K_{M,M} = \\frac{1}{h_{M-1}}$.\n\n**Step 2: Constructing the Prior Precision Matrix $Q$**\n\nThe problem specifies mixed boundary conditions: a Dirichlet condition $u(0)=0$ at the left boundary and a Neumann condition $u'(L)=0$ at the right boundary.\n\nThe Dirichlet condition $u(0)=0$ fixes the value at node $x_0$. In the discrete system, this means the degree of freedom associated with $u(x_0)$ is removed. This is accomplished by eliminating the row and column of the global stiffness matrix $K$ that correspond to node $x_0$ (i.e., the first row and first column, index $0$). The resulting matrix is the precision matrix $Q \\in \\mathbb{R}^{M \\times M}$ for the unknown vector of node values $x = [u(x_1), \\ldots, u(x_M)]^\\top$.\n\nThe Neumann condition $u'(L)=0$ is a \"natural\" boundary condition in this finite element formulation. It is satisfied automatically by the assembly process and requires no explicit modification to the matrix at the node $x_M$.\n\nThe resulting matrix $Q$ is symmetric and positive definite, which is a necessary property for a precision matrix of a proper Gaussian distribution. The positive definiteness arises because the Dirichlet condition at one end prevents the \"floating\" or constant null-space mode that would exist with pure Neumann conditions on both ends.\n\n**Step 3: Forming the Posterior Precision Matrix $\\Lambda$**\n\nThe problem is cast within a Bayesian framework. The GMRF prior on the unknown vector $x \\in \\mathbb{R}^M$ is given by the probability density function $p(x) \\propto \\exp(-\\frac{\\alpha}{2} x^\\top Q x)$, where $Q$ is the prior precision matrix derived above and $\\alpha  0$ is a scalar hyperparameter controlling the strength of the prior.\n\nThe data are related to the unknowns via the linear model $y = Hx + \\varepsilon$, where $H \\in \\mathbb{R}^{m \\times M}$ is the observation operator and $\\varepsilon$ is Gaussian noise with $\\varepsilon \\sim \\mathcal{N}(0, R)$, where $R = \\sigma^2 I$. The likelihood function is $p(y|x) \\propto \\exp(-\\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx))$.\n\nAccording to Bayes' theorem, the posterior probability density is proportional to the product of the likelihood and the prior, $p(x|y) \\propto p(y|x)p(x)$. For Gaussian distributions, the exponent of the posterior density is the sum of the exponents of the likelihood and prior. The quadratic term in $x$ in the posterior exponent defines the posterior precision matrix $\\Lambda$.\n$$\n\\text{exponent} = -\\frac{1}{2} \\left( (y-Hx)^\\top R^{-1} (y-Hx) + \\alpha x^\\top Q x \\right)\n$$\nExpanding the term and collecting quadratic terms in $x$ gives:\n$$\n-\\frac{1}{2} \\left( x^\\top H^\\top R^{-1} H x + \\alpha x^\\top Q x - 2y^\\top R^{-1} Hx + y^\\top R^{-1} y \\right) = -\\frac{1}{2} x^\\top (\\alpha Q + H^\\top R^{-1} H) x + \\dots\n$$\nThus, the posterior precision matrix is:\n$$\n\\Lambda = \\alpha Q + H^\\top R^{-1} H = \\alpha Q + \\frac{1}{\\sigma^2} H^\\top H\n$$\nThis matrix $\\Lambda$ is symmetric and positive semi-definite, as it is a sum of two symmetric positive semi-definite matrices (and positive definite if $\\alpha  0$ since $Q$ is positive definite).\n\n**Step 4: Computing the Posterior Nullspace Dimension $d$**\n\nThe posterior nullspace dimension, $d$, is a measure of the number of directions in the parameter space that are not informed by the data or the prior. Numerically, this is defined as the number of eigenvalues of the posterior precision matrix $\\Lambda$ that are strictly less than a small positive threshold $\\tau = 10^{-12}$. An eigenvalue smaller than this threshold indicates a direction in which the posterior distribution is extremely flat, corresponding to very high uncertainty.\n\nThe computation involves:\n1.  Constructing the matrix $\\Lambda$ for each test case using the specified parameters $\\alpha$, $\\sigma$, $Q$, and $H$.\n2.  Computing all eigenvalues of $\\Lambda$. Since $\\Lambda$ is symmetric, specialized and numerically stable algorithms can be used.\n3.  Counting the number of computed eigenvalues $\\lambda_i$ such that $\\lambda_i  \\tau$. This count is the desired dimension $d$.\n\nThe three test cases provided are designed to probe different scenarios: a well-determined system (Test Case 1), a system underdetermined by data alone (Test Case 2), and a system where a very weak prior fails to regularize a rank-deficient data term (Test Case 3). The analysis for each case follows the steps outlined above.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the posterior nullspace dimension for three\n    different linear inverse problem setups with a GMRF prior.\n    \"\"\"\n    \n    # Numerical threshold for counting eigenvalues in the nullspace.\n    tau = 1e-12\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (mesh_nodes, alpha, sigma, H_definition).\n    # H_definition is a lambda function to construct H for a given M.\n    test_cases = [\n        # Test case 1: happy path, fully observed\n        (\n            np.array([0.0, 0.2, 0.5, 0.75, 1.0]),\n            1.0,\n            0.2,\n            lambda M: np.identity(M)\n        ),\n        # Test case 2: likelihood-only, rank-deficient\n        (\n            np.array([0.0, 0.2, 0.5, 0.75, 1.0]),\n            0.0,\n            0.3,\n            lambda M: np.array([[1., 0., 0., 0.], [0., 0., 1., 0.]])\n        ),\n        # Test case 3: near-singular due to vanishing prior, low-rank data\n        (\n            np.array([0.0, 0.2, 0.5, 0.75, 1.0]),\n            1e-16,\n            0.1,\n            lambda M: np.array([[1., 1., 1., 1.]])\n        ),\n    ]\n\n    results = []\n    for nodes, alpha, sigma, get_H in test_cases:\n        # Determine the number of unknown nodes M\n        M = len(nodes) - 1\n\n        # Calculate element lengths h_i\n        h = np.diff(nodes)\n\n        # Assemble the (M+1)x(M+1) global stiffness matrix K\n        K = np.zeros((M + 1, M + 1))\n        for i in range(M):\n            h_i = h[i]\n            local_K = (1 / h_i) * np.array([[1, -1], [-1, 1]])\n            K[i:i+2, i:i+2] += local_K\n\n        # Enforce Dirichlet BC u(0)=0 by removing the first row and column\n        # to get the M x M prior precision matrix Q.\n        Q = K[1:, 1:]\n\n        # Construct the observation operator H for the current M\n        H = get_H(M)\n\n        # Form the posterior precision matrix Lambda\n        # Lambda = alpha * Q + (1/sigma^2) * H.T @ H\n        HTH = H.T @ H\n        Lambda = alpha * Q + (1 / sigma**2) * HTH\n\n        # Compute eigenvalues of the symmetric matrix Lambda.\n        # eigvalsh is efficient for symmetric matrices.\n        eigenvalues = np.linalg.eigvalsh(Lambda)\n\n        # Count the number of eigenvalues strictly less than the threshold tau.\n        # This is the posterior nullspace dimension d.\n        d = np.sum(eigenvalues  tau)\n        \n        results.append(int(d))\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful Bayesian analysis depends on the productive interplay between the prior and the likelihood. This practice delves into a potential pitfall known as aliasing, where the information from the data and the prior are simultaneously non-informative in the same direction, leading to a degenerate posterior . By constructing a specific inverse problem where the observation operator's nullspace aligns with a smooth mode of a GMRF prior, you will investigate how this degeneracy can cause posterior multimodality and explore principled ways to restore a well-behaved, unimodal solution.",
            "id": "3384858",
            "problem": "Consider a linear inverse problem with a latent field $x \\in \\mathbb{R}^n$, an observation operator $H \\in \\mathbb{R}^{m \\times n}$, and observed data $y \\in \\mathbb{R}^m$. The data model is $y = H x + \\eta$, where $\\eta \\sim \\mathcal{N}(0, R)$ with a known, symmetric positive-definite noise covariance $R \\in \\mathbb{R}^{m \\times m}$. The prior for $x$ is a Gaussian Markov random field (GMRF) with precision $Q \\in \\mathbb{R}^{n \\times n}$ constructed from the graph Laplacian $L \\in \\mathbb{R}^{n \\times n}$ of a periodic one-dimensional lattice (ring) of $n$ nodes: $Q = \\tau L + \\varepsilon I_n$, where $\\tau  0$ is a smoothness scale, $\\varepsilon  0$ is a small ridge that renders the GMRF proper, and $I_n$ is the identity matrix. The Laplacian $L$ is defined by the usual second-difference stencil with periodic boundary conditions. Let $v \\in \\mathbb{R}^n$ be the normalized constant vector (all entries equal to $1/\\sqrt{n}$), which is the smoothest mode of $L$ and satisfies $L v = 0$.\n\nYour task is to investigate aliasing between $H$ and $Q$ by constructing an $H$ whose nullspace includes smooth modes of $Q$ and to demonstrate how this produces multimodal posteriors under a non-Gaussian prior. Specifically, consider a mixture-of-GMRFs prior for $x$ with two equally weighted components centered at $\\pm \\alpha v$, both sharing the same precision $Q$:\n$$\np(x) = \\tfrac{1}{2}\\,\\mathcal{N}(x; +\\alpha v, Q^{-1}) + \\tfrac{1}{2}\\,\\mathcal{N}(x; -\\alpha v, Q^{-1}),\n$$\nwhere $\\alpha  0$ controls the separation of the mixture modes along $v$. For a given $(H, Q, R)$, and observed $y$, the posterior is a mixture of Gaussians obtained by Bayes' rule:\n$$\np(x \\mid y) \\propto p(y \\mid x) \\, p(x),\n$$\nwith $p(y \\mid x) = \\mathcal{N}(y; Hx, R)$.\n\nYou must:\n\n1. Derive, from Bayes' rule and elementary properties of multivariate normal distributions, the posterior precision and mean of each Gaussian component in the mixture posterior. Express your derivation entirely in terms of $H$, $Q$, $R$, $y$, and the component means $\\pm \\alpha v$. Do not skip steps and do not use shortcut formulas beyond these fundamental definitions.\n\n2. Construct an $H$ whose nullspace contains $v$, specifically the first-difference operator on the ring defined by $(H x)_i = x_{(i+1) \\bmod n} - x_i$ for $i = 0, \\ldots, n-1$. Explain why $H v = 0$ and show how this aliasing implies that the likelihood $p(y \\mid x)$ is invariant to shifts of $x$ by any scalar multiple of $v$ in the noiseless case, thereby allowing multimodal posterior behavior when the prior is a mixture concentrated along $\\pm \\alpha v$.\n\n3. Propose principled priors to break this degeneracy and explain the mechanism by which they restore unimodality. You must consider at least one unimodal GMRF prior centered at $0$ and optionally discuss augmentations to the prior precision that penalize the nullspace direction.\n\n4. Implement a program that, for the following test suite of parameter values, builds $(H, Q, R)$, simulates noiseless data $y = H x_{\\text{true}}$, computes the mixture posterior component means, the mixture responsibilities of the two components given $y$, and outputs a boolean indicating whether the posterior exhibits multimodality according to the following criterion: both component responsibilities are at least $0.3$ and the Euclidean distance between the two component posterior means is strictly greater than a fixed threshold $d_{\\text{thr}} = 0.5$. If both conditions hold, return true; otherwise, return false.\n\nUse the following test suite, which explores aliasing, a degeneracy-breaking prior, and a non-aliased observation:\n\n- Test Case 1 (Aliasing with mixture prior):\n  - $n = 16$, $\\tau = 1.0$, $\\varepsilon = 10^{-6}$, $\\alpha = 3.0$, $\\sigma^2 = 10^{-3}$, $R = \\sigma^2 I_m$, $H$ is the first-difference operator on the ring (so $m = n$), $x_{\\text{true}} = 0$, $y = H x_{\\text{true}}$.\n\n- Test Case 2 (Degeneracy-breaking prior):\n  - Same $n$, $\\tau$, $\\varepsilon$, $\\sigma^2$, $H$ and $R$ as Test Case 1, but use a single unimodal GMRF prior $\\mathcal{N}(x; 0, Q^{-1})$ (i.e., not a mixture). Let $x_{\\text{true}} = 0$, $y = H x_{\\text{true}}$.\n\n- Test Case 3 (Non-aliased observation with mixture prior):\n  - Same $n$, $\\tau$, $\\varepsilon$, $\\alpha$, $\\sigma^2$ as Test Case 1, but take $H = I_n$ (identity, so $m = n$), $x_{\\text{true}} = +\\alpha v$, $y = H x_{\\text{true}}$.\n\nFor each test case, your program must decide whether the posterior is multimodal according to the stated criterion and output the three booleans for the test cases in order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,false]\").\n\nNo physical units, angle units, or percentages are required in this problem. All quantities are dimensionless and purely mathematical.\n\nYour program must be a complete, runnable implementation that carries out the computations described above and prints the final results in the required format. The program must not rely on external inputs. It must use standard linear algebra techniques based on the definitions and properties specified above, and it must avoid any shortcuts not derived from these fundamental bases.",
            "solution": "The problem requires a three-part theoretical exposition followed by a computational implementation. We shall address each part in sequence.\n\n### 1. Derivation of the Mixture Posterior\n\nWe are given a linear inverse problem with a Gaussian likelihood and a mixture-of-GMRFs prior.\nThe likelihood is specified as $p(y \\mid x) = \\mathcal{N}(y; Hx, R)$, which, in terms of its probability density function, is proportional to:\n$$\np(y \\mid x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\nThe prior is a mixture of two Gaussian components with equal weight $w_1 = w_2 = \\frac{1}{2}$:\n$$\np(x) = \\frac{1}{2}\\,\\mathcal{N}(x; \\mu_1, Q^{-1}) + \\frac{1}{2}\\,\\mathcal{N}(x; \\mu_2, Q^{-1})\n$$\nwhere $\\mu_1 = +\\alpha v$ and $\\mu_2 = -\\alpha v$. The $k$-th prior component, for $k \\in \\{1, 2\\}$, is $p_k(x) = \\mathcal{N}(x; \\mu_k, Q^{-1})$, which is proportional to:\n$$\np_k(x) \\propto \\exp\\left(-\\frac{1}{2}(x - \\mu_k)^T Q (x - \\mu_k)\\right)\n$$\nThe posterior distribution $p(x \\mid y)$ is obtained via Bayes' rule:\n$$\np(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)} \\propto p(y \\mid x) p(x)\n$$\nSubstituting the mixture prior, we get:\n$$\np(x \\mid y) \\propto p(y \\mid x) \\left(\\frac{1}{2} p_1(x) + \\frac{1}{2} p_2(x)\\right) = \\frac{1}{2} p(y \\mid x) p_1(x) + \\frac{1}{2} p(y \\mid x) p_2(x)\n$$\nThis shows the posterior is also a mixture of two components. Let us find the form of the $k$-th unnormalized posterior component, $p(y \\mid x) p_k(x)$. Its logarithm is the sum of the log-likelihood and the log-prior for that component:\n$$\n\\log(p(y \\mid x) p_k(x)) = C - \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx) - \\frac{1}{2}(x - \\mu_k)^T Q (x - \\mu_k)\n$$\nwhere $C$ is a constant. We expand the quadratic forms:\n$$\n= C - \\frac{1}{2}(y^T R^{-1} y - 2y^T R^{-1} Hx + x^T H^T R^{-1} Hx) - \\frac{1}{2}(x^T Qx - 2x^T Q\\mu_k + \\mu_k^T Q\\mu_k)\n$$\nGrouping terms by powers of $x$:\n$$\n= C' - \\frac{1}{2} \\left[ x^T(H^T R^{-1} H + Q)x - 2(y^T R^{-1} H + \\mu_k^T Q)x \\right]\n$$\nThis is a quadratic function of $x$, which indicates that the component $p(y \\mid x) p_k(x)$ is proportional to a Gaussian density. We can identify its precision and mean by completing the square. The form of a Gaussian log-density is $-\\frac{1}{2}(x - \\mu')^T Q' (x - \\mu') + C'' = -\\frac{1}{2}(x^T Q'x - 2x^T Q'\\mu') + C'''$.\n\nBy comparing the quadratic and linear terms in $x$, we find the posterior precision matrix $Q'$ and posterior mean vector $\\mu'_k$ for the $k$-th component.\n\nThe posterior precision $Q'$ is the matrix of the quadratic term $x^T(\\cdot)x$:\n$$\nQ' = H^T R^{-1} H + Q\n$$\nThis precision is common to both posterior components because it does not depend on the prior mean $\\mu_k$.\n\nThe posterior mean $\\mu'_k$ is found by equating the linear terms:\n$$\n2x^T Q' \\mu'_k = 2(y^T R^{-1} H + \\mu_k^T Q)x\n$$\nTaking the transpose of the coefficient of $x^T$:\n$$\nQ' \\mu'_k = (y^T R^{-1} H)^T + ( \\mu_k^T Q)^T = H^T R^{-1} y + Q \\mu_k\n$$\nSolving for $\\mu'_k$ yields the posterior mean for component $k$:\n$$\n\\mu'_k = (Q')^{-1} (H^T R^{-1} y + Q \\mu_k)\n$$\nThus, the posterior is a mixture $p(x \\mid y) = w'_1 \\mathcal{N}(x; \\mu'_1, (Q')^{-1}) + w'_2 \\mathcal{N}(x; \\mu'_2, (Q')^{-1})$, where $w'_1$ and $w'_2$ are the posterior weights, or responsibilities, which we do not need to derive for this part of the problem.\n\n### 2. Aliasing Between $H$ and $Q$\n\nThe problem specifies the construction of an observation operator $H$ as the first-difference operator on a periodic lattice of $n$ nodes:\n$$\n(Hx)_i = x_{(i+1) \\bmod n} - x_i \\quad \\text{for } i = 0, \\dots, n-1\n$$\nThe vector $v \\in \\mathbb{R}^n$ is the normalized constant vector, with all entries equal to $1/\\sqrt{n}$. Let us demonstrate that $v$ lies in the nullspace of $H$:\n$$\n(Hv)_i = v_{(i+1) \\bmod n} - v_i = \\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n}} = 0\n$$\nSince this holds for all $i = 0, \\dots, n-1$, we have $Hv = 0$.\n\nThis property leads to aliasing. The likelihood function $p(y \\mid x)$ quantifies how well a given state $x$ explains the data $y$. Consider a state $x$ and a new state $x_{\\text{shifted}} = x + c v$ for some scalar $c \\in \\mathbb{R}$. The observation operator applied to this shifted state is:\n$$\nH x_{\\text{shifted}} = H(x + c v) = Hx + c(Hv) = Hx + c \\cdot 0 = Hx\n$$\nThe predicted observation $Hx$ is unchanged by shifts of $x$ along the direction $v$. Consequently, the likelihood is also invariant:\n$$\np(y \\mid x_{\\text{shifted}}) = \\mathcal{N}(y; Hx_{\\text{shifted}}, R) = \\mathcal{N}(y; Hx, R) = p(y \\mid x)\n$$\nThe data provide no information to distinguish between $x$ and any point on the line $x+cv$. The operator $H$ is \"blind\" to the component of $x$ along $v$.\n\nThe prior precision is $Q = \\tau L + \\varepsilon I_n$. Given $Lv=0$, we have $Qv = \\tau(Lv) + \\varepsilon(I_n v) = \\varepsilon v$. The precision in the direction $v$ is $\\varepsilon$, which is a small positive number. This means the prior variance in this direction, which is proportional to $1/\\varepsilon$, is very large. The prior is thus very uncertain or \"flat\" along the direction $v$.\n\nThe mixture-of-GMRFs prior is bimodal with centers at $\\mu_1 = +\\alpha v$ and $\\mu_2 = -\\alpha v$. The separation between these modes lies entirely along the vector $v$. When we combine this prior with the likelihood, the likelihood's invariance along $v$ means it cannot favor one mode over the other. If the data $y$ is consistent with the nullspace of $H$ (e.g., $y=0$, which is consistent with both $H\\mu_1=0$ and $H\\mu_2=0$), the posterior distribution will inherit the bimodality of the prior. The data fails to resolve the ambiguity introduced by the prior, a phenomenon known as aliasing.\n\n### 3. Principled Priors to Break Degeneracy\n\nThe aliasing and resulting posterior multimodality arise because the data contains no information in the nullspace of $H$, and the prior is non-informative (or ambiguous) in that same direction. The degeneracy can be broken by using a prior that provides unambiguous information in the nullspace.\n\n**Method 1: Unimodal GMRF Prior**\nThe simplest approach is to replace the bimodal mixture prior with a unimodal prior centered at a single, non-ambiguous value. A standard choice is a zero-mean GMRF:\n$$\np(x) = \\mathcal{N}(x; 0, Q^{-1})\n$$\nThis prior has a single peak at $x=0$. When combined with the Gaussian likelihood, the resulting posterior is also a single Gaussian. The ambiguity between $+\\alpha v$ and $-\\alpha v$ is resolved because the prior expresses a clear preference for solutions that are close to the origin. The posterior mean becomes $\\mu' = (H^T R^{-1} H + Q)^{-1} H^T R^{-1} y$, which is a unique solution (the standard MAP estimate for a linear-Gaussian model). This effectively regularizes the inverse problem by pulling the solution towards the prior mean in directions unconstrained by the data.\n\n**Method 2: Augmenting the Prior Precision**\nA more surgical approach is to modify the precision matrix $Q$ to specifically penalize the nullspace direction $v$. The original precision $Q = \\tau L + \\varepsilon I_n$ is very weak in the direction $v$ as $Qv = \\varepsilon v$. We can introduce a stronger penalty by adding a rank-one update to $Q$:\n$$\nQ_{\\text{new}} = Q + \\gamma v v^T = \\tau L + \\varepsilon I_n + \\gamma v v^T\n$$\nwhere $\\gamma  0$ is a hyperparameter controlling the strength of the penalty. The term $v v^T$ is a projection operator onto the subspace spanned by $v$. The action of this new precision on $v$ is:\n$$\nQ_{\\text{new}} v = (\\tau L + \\varepsilon I_n + \\gamma v v^T) v = \\tau Lv + \\varepsilon v + \\gamma v(v^T v)\n$$\nSince $v$ is normalized ($v^T v = 1$) and $Lv=0$, this simplifies to:\n$$\nQ_{\\text{new}} v = (\\varepsilon + \\gamma) v\n$$\nBy choosing $\\gamma \\gg \\varepsilon$, we can substantially increase the precision (i.e., decrease the variance) of the prior along the direction $v$. Using a zero-mean prior $\\mathcal{N}(x; 0, Q_{\\text{new}}^{-1})$, this prior strongly discourages solutions with large components along $v$. It effectively imposes a \"weak constraint\" that the average value of the field $x$ (which is proportional to $x \\cdot v$) should be close to $0$, thereby resolving the ambiguity between the $\\pm\\alpha v$ modes and restoring posterior unimodality.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1: Aliasing with mixture prior\n        {\n            \"n\": 16, \"tau\": 1.0, \"eps\": 1e-6, \"alpha\": 3.0, \"sigma2\": 1e-3,\n            \"prior_type\": \"mixture\", \"H_type\": \"diff\", \"xtrue_type\": \"zero\"\n        },\n        # Test Case 2: Degeneracy-breaking prior\n        {\n            \"n\": 16, \"tau\": 1.0, \"eps\": 1e-6, \"alpha\": 3.0, \"sigma2\": 1e-3,\n            \"prior_type\": \"unimodal\", \"H_type\": \"diff\", \"xtrue_type\": \"zero\"\n        },\n        # Test Case 3: Non-aliased observation with mixture prior\n        {\n            \"n\": 16, \"tau\": 1.0, \"eps\": 1e-6, \"alpha\": 3.0, \"sigma2\": 1e-3,\n            \"prior_type\": \"mixture\", \"H_type\": \"identity\", \"xtrue_type\": \"alphav\"\n        },\n    ]\n\n    results = []\n    d_thr = 0.5\n    resp_thr = 0.3\n\n    for case in test_cases:\n        # Unpack parameters\n        n, tau, eps, alpha, sigma2 = case[\"n\"], case[\"tau\"], case[\"eps\"], case[\"alpha\"], case[\"sigma2\"]\n        prior_type, H_type, xtrue_type = case[\"prior_type\"], case[\"H_type\"], case[\"xtrue_type\"]\n\n        # --- Problem Setup ---\n        \n        # 1. Define vectors and base matrices\n        v = np.ones(n) / np.sqrt(n)\n        Id_n = np.identity(n)\n\n        # 2. Construct prior precision Q\n        L = np.zeros((n, n))\n        for i in range(n):\n            L[i, i] = 2.0\n            L[i, (i - 1 + n) % n] = -1.0\n            L[i, (i + 1) % n] = -1.0\n        Q = tau * L + eps * Id_n\n\n        # 3. Construct observation operator H and noise covariance R\n        if H_type == \"diff\":\n            m = n\n            H = np.zeros((n, n))\n            for i in range(n):\n                H[i, i] = -1.0\n                H[i, (i + 1) % n] = 1.0\n        elif H_type == \"identity\":\n            m = n\n            H = np.identity(n)\n        \n        R = sigma2 * np.identity(m)\n        R_inv = (1.0 / sigma2) * np.identity(m)\n\n        # 4. Simulate true state and observed data y\n        if xtrue_type == \"zero\":\n            x_true = np.zeros(n)\n        elif xtrue_type == \"alphav\":\n            x_true = alpha * v\n        \n        y = H @ x_true # Noiseless data\n\n        # --- Posterior Analysis ---\n\n        if prior_type == \"unimodal\":\n            # A unimodal prior results in a unimodal posterior.\n            # The criterion for multimodality requires two components.\n            results.append(False)\n            continue\n\n        # For mixture prior:\n        # 1. Define prior component means\n        mu1 = alpha * v\n        mu2 = -alpha * v\n\n        # 2. Compute posterior precision (common to both components)\n        Q_post = H.T @ R_inv @ H + Q\n        Q_post_inv = linalg.inv(Q_post)\n\n        # 3. Compute posterior means for each component\n        mu1_post = Q_post_inv @ (H.T @ R_inv @ y + Q @ mu1)\n        mu2_post = Q_post_inv @ (H.T @ R_inv @ y + Q @ mu2)\n\n        # 4. Compute posterior component responsibilities (weights)\n        # Log of marginal likelihood for component k is proportional to -1/2 * J_k\n        # J_k = y'R_inv*y + mu_k'*Q*mu_k - (H'R_inv*y+Q*mu_k)'*Q_post_inv*(H'R_inv*y+Q*mu_k)\n        \n        term_y = y.T @ R_inv @ y\n        \n        # Component 1\n        term_mu1 = mu1.T @ Q @ mu1\n        linear_term1 = H.T @ R_inv @ y + Q @ mu1\n        term_post1 = linear_term1.T @ Q_post_inv @ linear_term1\n        J1 = term_y + term_mu1 - term_post1\n        \n        # Component 2\n        term_mu2 = mu2.T @ Q @ mu2\n        linear_term2 = H.T @ R_inv @ y + Q @ mu2\n        term_post2 = linear_term2.T @ Q_post_inv @ linear_term2\n        J2 = term_y + term_mu2 - term_post2\n\n        # Responsibilities are computed from the log evidence difference\n        # log(Z1/Z2) = -0.5 * (J1 - J2)\n        # w1 = Z1/(Z1+Z2) = 1 / (1 + Z2/Z1) = 1 / (1 + exp(-log(Z1/Z2)))\n        log_Z_ratio = -0.5 * (J1 - J2)\n        \n        # Use log-sum-exp trick for numerical stability, though simpler form works here\n        w1_post = 1.0 / (1.0 + np.exp(-log_Z_ratio))\n        w2_post = 1.0 - w1_post\n        \n        # --- Check multimodality criterion ---\n        \n        # Condition 1: Responsibilities\n        is_resp_balanced = (w1_post >= resp_thr) and (w2_post >= resp_thr)\n        \n        # Condition 2: Distance between posterior means\n        dist_means = linalg.norm(mu1_post - mu2_post)\n        is_dist_large = dist_means > d_thr\n\n        is_multimodal = is_resp_balanced and is_dist_large\n        results.append(is_multimodal)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}