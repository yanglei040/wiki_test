{
    "hands_on_practices": [
        {
            "introduction": "变分贝叶斯的核心思想在于用一个简单的、可处理的分布来近似复杂的后验分布。这个过程不可避免地会引入近似误差。我们的第一个实践旨在通过一个简单的线性高斯模型，精确地量化这种误差的来源和大小 。通过亲手推导完全分解的高斯变分近似，并将其与真实的后验分布进行比较，您将清晰地看到当真实的后验变量存在相关性时，平均场假设是如何导致信息损失的。",
            "id": "3430192",
            "problem": "考虑由观测模型 $y = A x + \\varepsilon$ 定义的线性逆问题，其中 $x \\in \\mathbb{R}^2$，$A \\in \\mathbb{R}^{2 \\times 2}$，$x \\sim \\mathcal{N}(m_0, C_0)$，以及 $\\varepsilon \\sim \\mathcal{N}(0, R)$。设各量指定如下：$A = I_2$，$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，$C_0 = \\begin{pmatrix} 2  \\frac{6}{5} \\\\ \\frac{6}{5}  1 \\end{pmatrix}$，$R = \\frac{1}{2} I_2$，以及观测数据 $y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n以贝叶斯法则和高斯密度的定义为基本依据，执行以下操作：\n\n$1.$ 推导精确后验分布 $p(x \\mid y)$，并以闭式形式计算其均值和协方差。\n\n$2.$ 考虑一个完全因子分解的高斯分布的变分族 $q(x) = \\mathcal{N}(m, \\operatorname{diag}(s_1^2, s_2^2))$。通过最小化 Kullback–Leibler (KL) 散度 $\\mathrm{KL}(q \\,\\|\\, p(x \\mid y))$，以闭式形式确定最优的 $m$，$s_1^2$ 和 $s_2^2$。\n\n$3.$ 将“近似差距”定义为 Kullback–Leibler 散度 $\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y))$，其中 $q^\\star$ 是在第2部分中找到的最优平均场高斯分布。为此值提供一个单一的闭式解析表达式。\n\n仅报告最终的近似差距作为你的最终答案。不要四舍五入；提供一个精确的解析表达式。不需要单位。",
            "solution": "该问题要求对一个线性贝叶斯逆问题进行三部分分析：首先，找到精确的后验分布；其次，确定最优的平均场变分近似；第三，计算近似分布与真实后验分布之间的 Kullback–Leibler (KL) 散度。\n\n观测模型由 $y = A x + \\varepsilon$ 给出，其中 $x \\in \\mathbb{R}^2$。\n$x$ 上的先验是一个高斯分布：$p(x) = \\mathcal{N}(x \\mid m_0, C_0)$。\n噪声 $\\varepsilon$ 也是高斯分布的：$\\varepsilon \\sim \\mathcal{N}(0, R)$。\n这意味着似然函数为 $p(y \\mid x) = \\mathcal{N}(y \\mid Ax, R)$。\n\n给定的值为：\n$A = I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\n$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n$C_0 = \\begin{pmatrix} 2  \\frac{6}{5} \\\\ \\frac{6}{5}  1 \\end{pmatrix}$\n$R = \\frac{1}{2} I_2 = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$\n$y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n\n这是一个有效的问题，因为它是贝叶斯统计和机器学习中的一个标准的、适定的问题，所有必要信息都已提供，没有内部矛盾或科学缺陷。\n\n**第1部分：精确后验分布**\n\n对于线性高斯模型，后验分布 $p(x \\mid y)$ 也是一个高斯分布，我们将其记为 $\\mathcal{N}(x \\mid m_p, C_p)$。后验协方差 $C_p$ 和均值 $m_p$ 由标准贝叶斯更新公式给出：\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A$$\n$$m_p = C_p (C_0^{-1}m_0 + A^T R^{-1} y)$$\n\n首先，我们计算逆矩阵 $C_0^{-1}$ 和 $R^{-1}$。\n$C_0$ 的行列式是 $\\det(C_0) = (2)(1) - (\\frac{6}{5})^2 = 2 - \\frac{36}{25} = \\frac{50-36}{25} = \\frac{14}{25}$。\n$C_0$ 的逆是：\n$$C_0^{-1} = \\frac{1}{14/25} \\begin{pmatrix} 1  -\\frac{6}{5} \\\\ -\\frac{6}{5}  2 \\end{pmatrix} = \\frac{25}{14} \\begin{pmatrix} 1  -\\frac{6}{5} \\\\ -\\frac{6}{5}  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{25}{7} \\end{pmatrix}$$\n$R$ 的逆是：\n$$R^{-1} = (\\frac{1}{2} I_2)^{-1} = 2 I_2 = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$$\n\n现在我们可以计算后验精度矩阵 $C_p^{-1}$：\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A = C_0^{-1} + I_2 (2I_2) I_2 = C_0^{-1} + 2I_2$$\n$$C_p^{-1} = \\begin{pmatrix} \\frac{25}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{25}{7} \\end{pmatrix} + \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14} + \\frac{28}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{25}{7} + \\frac{14}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{39}{7} \\end{pmatrix}$$\n\n接下来，我们计算后验均值 $m_p$。由于 $m_0 = 0$，公式简化为 $m_p = C_p (A^T R^{-1} y)$。\n让我们首先计算项 $A^T R^{-1} y$：\n$$A^T R^{-1} y = I_2 (2I_2) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$$\n因此，$m_p = C_p \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$，这等价于求解方程组 $C_p^{-1} m_p = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$。\n为了求得 $m_p$，我们首先需要 $C_p = (C_p^{-1})^{-1}$。$C_p^{-1}$ 的行列式是：\n$$\\det(C_p^{-1}) = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) - \\left(-\\frac{15}{7}\\right)^2 = \\frac{2067}{98} - \\frac{225}{49} = \\frac{2067 - 450}{98} = \\frac{1617}{98} = \\frac{33}{2}$$\n后验协方差矩阵 $C_p$ 是：\n$$C_p = \\frac{1}{33/2} \\begin{pmatrix} \\frac{39}{7}  \\frac{15}{7} \\\\ \\frac{15}{7}  \\frac{53}{14} \\end{pmatrix} = \\frac{2}{33} \\begin{pmatrix} \\frac{78}{14}  \\frac{30}{14} \\\\ \\frac{30}{14}  \\frac{53}{14} \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 78  30 \\\\ 30  53 \\end{pmatrix}$$\n现在我们计算后验均值 $m_p$：\n$$m_p = \\frac{1}{231} \\begin{pmatrix} 78  30 \\\\ 30  53 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 156 - 60 \\\\ 60 - 106 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 96 \\\\ -46 \\end{pmatrix} = \\begin{pmatrix} \\frac{32}{77} \\\\ -\\frac{46}{231} \\end{pmatrix}$$\n所以精确后验分布是 $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$，其中 $m_p$ 和 $C_p$ 如上推导。\n\n**第2部分：最优变分近似**\n\n我们考虑一个完全因子分解的高斯分布的变分族 $q(x) = q_1(x_1)q_2(x_2) = \\mathcal{N}(x \\mid m, \\operatorname{diag}(s_1^2, s_2^2))$。我们最小化 $\\mathrm{KL}(q \\,\\|\\, p(x \\mid y))$。对于高斯后验分布 $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$，最小化此 KL 散度的 $q$ 的最优参数已知为：\n$1.$ 变分分布的均值等于真实后验分布的均值：$m^\\star = m_p$。\n$2.$ 各因子的方差是后验精度矩阵 $C_p^{-1}$ 对角元素的倒数。\n所以，$s_1^2 = 1 / (C_p^{-1})_{11}$ 且 $s_2^2 = 1 / (C_p^{-1})_{22}$。\n\n使用第1部分中的后验精度矩阵 $C_p^{-1}$：\n$$(C_p^{-1})_{11} = \\frac{53}{14} \\quad \\text{和} \\quad (C_p^{-1})_{22} = \\frac{39}{7}$$\n最优方差为：\n$$s_1^2 = \\frac{1}{53/14} = \\frac{14}{53}$$\n$$s_2^2 = \\frac{1}{39/7} = \\frac{7}{39}$$\n最优变分分布是 $q^\\star(x) = \\mathcal{N}(x \\mid m^\\star, C_q^\\star)$，其中 $m^\\star = m_p$ 且 $C_q^\\star = \\operatorname{diag}(\\frac{14}{53}, \\frac{7}{39})$。\n\n**第3部分：近似差距**\n\n近似差距定义为 $\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y))$。两个维度为 $d$ 的多元高斯分布 $q = \\mathcal{N}(m_q, C_q)$ 和 $p = \\mathcal{N}(m_p, C_p)$ 之间的 KL 散度通用公式为：\n$$\\mathrm{KL}(q \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q}\\right) - d + \\operatorname{tr}(C_p^{-1} C_q) + (m_p - m_q)^T C_p^{-1} (m_p - m_q) \\right]$$\n在我们的情况下，$d=2$，$q=q^\\star$，且 $p=p(x \\mid y)$。从第2部分我们知道 $m_q = m_p$，所以 $(m_p - m_q)^T C_p^{-1} (m_p - m_q)$ 项为零。公式简化为：\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + \\operatorname{tr}(C_p^{-1} C_q^\\star) \\right]$$\n让我们计算迹项。设 $P = C_p^{-1}$。那么 $C_q^\\star = \\operatorname{diag}(1/P_{11}, 1/P_{22})$。\n$$\\operatorname{tr}(C_p^{-1} C_q^\\star) = \\operatorname{tr}\\left( \\begin{pmatrix} P_{11}  P_{12} \\\\ P_{21}  P_{22} \\end{pmatrix} \\begin{pmatrix} 1/P_{11}  0 \\\\ 0  1/P_{22} \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1  P_{12}/P_{22} \\\\ P_{21}/P_{11}  1 \\end{pmatrix} \\right) = 1+1=2$$\n迹项恰好等于维度 $d$。KL 散度的表达式变得更简单：\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + 2 \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right)$$\n我们有 $\\det C_p = 1/\\det(C_p^{-1})$。$C_q^\\star$ 的行列式是 $(\\det C_q^\\star) = s_1^2 s_2^2 = (1/P_{11})(1/P_{22})$。\n所以行列式之比为：\n$$\\frac{\\det C_p}{\\det C_q^\\star} = \\frac{1/\\det(P)}{1/(P_{11}P_{22})} = \\frac{P_{11}P_{22}}{\\det(P)} = \\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2}$$\n让我们代入 $P_{ij} = (C_p^{-1})_{ij}$ 的值：\n$P_{11} = 53/14$，$P_{22} = 39/7$，以及 $P_{12} = -15/7$。\n$$P_{11}P_{22} = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) = \\frac{2067}{98}$$\n$$P_{12}^2 = \\left(-\\frac{15}{7}\\right)^2 = \\frac{225}{49}$$\n该比率为：\n$$\\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2} = \\frac{2067/98}{2067/98 - 225/49} = \\frac{2067/98}{(2067-450)/98} = \\frac{2067}{1617}$$\n这个分数可以通过将分子和分母同除以它们的最大公约数 $3$ 来化简。\n$$\\frac{2067 \\div 3}{1617 \\div 3} = \\frac{689}{539}$$\n最终的近似差距是：\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y)) = \\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)$$\n这也可以表示为 $-\\frac{1}{2} \\ln(1 - \\rho_P^2)$，其中 $\\rho_P = P_{12}/\\sqrt{P_{11}P_{22}}$ 是与精度矩阵相关的相关系数。这个非零值反映了因忽略 $x_1$ 和 $x_2$ 之间的后验相关性而丢失的信息。",
            "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)}$$"
        },
        {
            "introduction": "在处理非线性逆问题时，变分贝叶斯并非唯一的近似推断工具，拉普拉斯近似是另一种常用的方法。本练习将引导您在一个简洁的非线性模型中，并排推导和比较这两种方法 。通过分析它们对后验协方差的不同估计，您将深入理解这两种方法各自的假设及其对不确定性量化的影响，这对于在实际应用中选择合适的近似方法至关重要。",
            "id": "3430120",
            "problem": "考虑一个贝叶斯逆问题，其未知量为标量 $x \\in \\mathbb{R}$，先验为高斯分布 $x \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$，观测模型为 $y = G(x) + \\eta$，其中正向映射为非线性函数 $G(x) = x^{2}$，噪声为加性高斯噪声 $\\eta \\sim \\mathcal{N}(0,\\sigma^{2})$。令负对数后验（忽略一个加性常数）表示为 $\\Phi(x)$。最大后验（MAP）估计量 $x_{\\mathrm{MAP}}$ 定义为 $\\Phi(x)$ 的最小化子。您可以假设 $0  2y  \\sigma^{2}/\\sigma_{0}^{2}$，从而使 $x_{\\mathrm{MAP}} = 0$ 是 $\\Phi(x)$ 的一个局部最小化子。\n\n从高斯先验和高斯似然的定义出发，仅使用微分的第一性原理和链式法则，执行以下操作：\n\n1. 显式推导 $\\Phi(x)$ 并获得平稳性条件 $\\nabla \\Phi(x) = 0$，以验证 $x = 0$ 是一个平稳点。在给定的关于 $y$ 的假设下，论证 $x_{\\mathrm{MAP}} = 0$ 是一个局部最小化子。\n2. 在 $x_{\\mathrm{MAP}}$ 处推导黑塞矩阵 $\\nabla^{2}\\Phi(x)$，并将后验的拉普拉斯近似写成一个均值为 $x_{\\mathrm{MAP}}$、协方差为 $\\Sigma_{\\mathrm{Lap}} = [\\nabla^{2}\\Phi(x_{\\mathrm{MAP}})]^{-1}$ 的高斯分布。\n3. 考虑一个高斯变分贝叶斯（VB）近似 $q(x) = \\mathcal{N}(\\mu,v)$，该近似通过最小化从 $q(x)$ 到精确后验的 Kullback-Leibler（KL）散度得到。按照标准高斯 VB 的做法，在变分均值 $\\mu$ 附近将 $G(x)$ 线性化为 $G(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)$。使用此线性化方法，推导似然的隐含二次代理和相应的后验高斯近似，并求出在 $\\mu = x_{\\mathrm{MAP}}$ 时的 VB 协方差，记为 $\\Sigma_{\\mathrm{VB}}$。\n4. 计算比率 $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$，并将其表示为关于 $\\sigma_{0}^{2}$、$\\sigma^{2}$ 和 $y$ 的单个闭式函数。\n\n您的最终答案必须是 $\\rho$ 的单个解析表达式。无需四舍五入，不涉及单位。",
            "solution": "该问题要求对一个特定的非线性贝叶斯逆问题，分四部分推导并比较拉普拉斯近似和变分贝叶斯（VB）近似。我们将按要求逐步进行。\n\n$x$ 的先验概率密度函数（PDF）由 $x \\sim \\mathcal{N}(0, \\sigma_{0}^{2})$ 给出，即 $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{x^2}{2\\sigma_{0}^{2}}\\right)$。\n观测模型为 $y = x^2 + \\eta$，噪声为 $\\eta \\sim \\mathcal{N}(0, \\sigma^2)$。这定义了似然概率密度函数为 $p(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - x^2)^2}{2\\sigma^2}\\right)$。\n\n根据贝叶斯定理，后验概率密度函数正比于似然与先验的乘积：$p(x|y) \\propto p(y|x)p(x)$。负对数后验（记为 $\\Phi(x)$）可以通过取负对数并舍去任何不依赖于 $x$ 的项来写出：\n$$\n\\Phi(x) = -\\ln(p(y|x)) - \\ln(p(x)) + \\text{const}\n$$\n$$\n\\Phi(x) = \\frac{(y - x^2)^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\n\n**第一部分：MAP 估计的分析**\n\n为了找到 $\\Phi(x)$ 的平稳点，我们计算它关于 $x$ 的一阶导数，记为 $\\nabla \\Phi(x)$：\n$$\n\\nabla \\Phi(x) = \\frac{d\\Phi}{dx} = \\frac{1}{2\\sigma^2} \\cdot 2(y - x^2) \\cdot (-2x) + \\frac{2x}{2\\sigma_0^2}\n$$\n$$\n\\nabla \\Phi(x) = -\\frac{2x(y - x^2)}{\\sigma^2} + \\frac{x}{\\sigma_0^2} = x \\left( \\frac{2x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)\n$$\n平稳性条件为 $\\nabla \\Phi(x) = 0$。通过观察可知，一个解是 $x=0$。因此，$x=0$ 是一个平稳点。\n为了确定这个平稳点的性质，我们计算 $\\Phi(x)$ 的二阶导数，即黑塞矩阵 $\\nabla^2 \\Phi(x)$：\n$$\n\\nabla^2 \\Phi(x) = \\frac{d^2\\Phi}{dx^2} = \\frac{d}{dx} \\left( \\frac{2x^3 - 2xy}{\\sigma^2} + \\frac{x}{\\sigma_0^2} \\right)\n$$\n$$\n\\nabla^2 \\Phi(x) = \\frac{6x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\n我们在平稳点 $x=0$ 处计算黑塞矩阵：\n$$\n\\nabla^2 \\Phi(0) = \\frac{6(0)^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\n为了使 $x=0$ 成为一个局部最小化子，二阶导数检验要求 $\\nabla^2 \\Phi(0) > 0$。这意味着：\n$$\n\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2} > 0 \\implies \\frac{1}{\\sigma_0^2} > \\frac{2y}{\\sigma^2} \\implies 2y  \\frac{\\sigma^2}{\\sigma_0^2}\n$$\n这正是问题陈述中给出的条件。因此，在给定假设下，$x=0$ 是 $\\Phi(x)$ 的一个局部最小化子，我们将其确定为局部最大后验估计 $x_{\\mathrm{MAP}} = 0$。\n\n**第二部分：拉普拉斯近似协方差 $\\Sigma_{\\mathrm{Lap}}$**\n\n拉普拉斯近似将后验分布建模为一个以 MAP 估计为中心的高斯分布，其精度（协方差的逆）由在 MAP 估计处求值的负对数后验的黑塞矩阵给出。\n对于我们的标量情况，精度为 $\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(x_{\\mathrm{MAP}})$。使用第一部分中 $x_{\\mathrm{MAP}} = 0$ 的结果：\n$$\n\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(0) = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\n拉普拉斯近似的协方差是该精度的逆：\n$$\n\\Sigma_{\\mathrm{Lap}} = \\left( \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2} \\right)^{-1} = \\left( \\frac{\\sigma^2 - 2y\\sigma_0^2}{\\sigma_0^2\\sigma^2} \\right)^{-1} = \\frac{\\sigma_0^2\\sigma^2}{\\sigma^2 - 2y\\sigma_0^2}\n$$\n\n**第三部分：变分贝叶斯协方差 $\\Sigma_{\\mathrm{VB}}$**\n\n高斯变分贝叶斯近似考虑一族高斯分布 $q(x) = \\mathcal{N}(\\mu, v)$，并试图最小化 KL 散度 $\\mathrm{KL}(q(x) || p(x|y))$。该问题指定使用在变分均值 $\\mu$ 附近对正向映射 $G(x) = x^2$ 进行线性化的方法：\n$$\nG(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)\n$$\n由于 $G'(\\mu) = 2\\mu$，线性化为 $G(x) \\approx \\mu^2 + 2\\mu(x-\\mu)$。\n我们将其代入似然项中。近似的负对数后验（我们记为 $\\tilde{\\Phi}(x; \\mu)$）变为：\n$$\n\\tilde{\\Phi}(x; \\mu) = \\frac{(y - (\\mu^2 + 2\\mu(x-\\mu)))^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\n这个表达式是关于 $x$ 的二次型。相应的高斯后验近似的精度由 $\\frac{1}{2}x^2$ 的系数给出。展开似然项：\n$$\n\\frac{(y - \\mu^2 + 2\\mu^2 - 2\\mu x)^2}{2\\sigma^2} = \\frac{((y+\\mu^2) - 2\\mu x)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}((y+\\mu^2)^2 - 4\\mu(y+\\mu^2)x + 4\\mu^2 x^2)\n$$\n包含 $x^2$ 的项是 $\\frac{4\\mu^2 x^2}{2\\sigma^2} = \\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2}\\right)$。\n将近似似然和先验中的二次项相加，$\\tilde{\\Phi}(x; \\mu)$ 中的总二次项为 $\\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)$。因此，此 VB 后验近似的精度（作为线性化点 $\\mu$ 的函数）为：\n$$\nv^{-1}(\\mu) = \\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\n问题要求计算在 $\\mu = x_{\\mathrm{MAP}} = 0$ 处的 VB 协方差。代入 $\\mu=0$：\n$$\n\\Sigma_{\\mathrm{VB}}^{-1} = v^{-1}(0) = \\frac{4(0)^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2}\n$$\nVB 协方差 $\\Sigma_{\\mathrm{VB}}$ 是该精度的逆：\n$$\n\\Sigma_{\\mathrm{VB}} = \\left(\\frac{1}{\\sigma_0^2}\\right)^{-1} = \\sigma_0^2\n$$\n这一结果表明，当这个特定的 VB 近似以 $\\mu=0$ 为中心时，其协方差恢复为先验协方差。这是因为正向映射的导数 $G'(0)=0$，所以线性化模型显示出对 $x$ 没有依赖性，数据没有提供关于 $x$ 精度的信息。\n\n**第四部分：比率 $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$**\n\n我们现在计算两个推导出的协方差的比率。\n$$\n\\rho = \\frac{\\Sigma_{\\mathrm{VB}}}{\\Sigma_{\\mathrm{Lap}}} = \\Sigma_{\\mathrm{VB}} \\cdot \\Sigma_{\\mathrm{Lap}}^{-1}\n$$\n代入我们找到的表达式：\n$$\n\\rho = (\\sigma_0^2) \\cdot \\left(\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\\right)\n$$\n展开 $\\sigma_0^2$：\n$$\n\\rho = \\sigma_0^2 \\cdot \\frac{1}{\\sigma_0^2} - \\sigma_0^2 \\cdot \\frac{2y}{\\sigma^2}\n$$\n$$\n\\rho = 1 - \\frac{2y\\sigma_0^2}{\\sigma^2}\n$$\n这是比率 $\\rho$ 关于 $\\sigma_0^2$、$\\sigma^2$ 和 $y$ 的最终闭式表达式。注意，给定的条件 $0  2y  \\sigma^2/\\sigma_0^2$ 意味着 $0  \\frac{2y\\sigma_0^2}{\\sigma^2}  1$，这确保了 $0  \\rho  1$。这表明对于此问题，拉普拉斯近似正确地估计了比先验方差更小的后验方差（因为数据提供了关于 $x$ 的信息，使其更集中于 0），而 VB 近似错误地估计了等于先验方差的方差。",
            "answer": "$$\\boxed{1 - \\frac{2y\\sigma_{0}^{2}}{\\sigma^{2}}}$$"
        },
        {
            "introduction": "构建了近似后验分布之后，一个关键的步骤是评估其质量，即“模型校准”。本练习将带您从理论走向实践，学习如何使用后验预测检验来诊断变分近似的性能 。您将探索平均场变分贝叶斯众所周知的方差低估问题，是如何在后验预测残差和概率积分变换（PIT）等校准诊断工具中留下明确“指纹”的，从而掌握一套评估和批判近似推断结果的实用技能。",
            "id": "3430181",
            "problem": "考虑一个线性高斯逆问题，其中未知参数向量 $u \\in \\mathbb{R}^n$ 通过观测算子 $A \\in \\mathbb{R}^{m \\times n}$ 从带噪声的观测值 $y \\in \\mathbb{R}^m$ 中推断得出。数据模型为 $y \\mid u \\sim \\mathcal{N}(A u, \\Sigma_{e})$，先验为高斯分布 $u \\sim \\mathcal{N}(m_{0}, \\Sigma_{0})$，其中 $\\Sigma_{e} \\in \\mathbb{R}^{m \\times m}$ 和 $\\Sigma_{0} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。$u$ 的贝叶斯后验为 $p(u \\mid y) \\propto p(y \\mid u) p(u)$，而对于一个重复观测值 $y_{\\mathrm{rep}}$ 的后验预测分布为 $p(y_{\\mathrm{rep}} \\mid y) = \\int p(y_{\\mathrm{rep}} \\mid u) p(u \\mid y) \\,\\mathrm{d}u$。\n\n假设我们基于后验预测均值构造后验预测残差，定义为 $r = y - \\mathbb{E}[y_{\\mathrm{rep}} \\mid y]$。标准化残差是利用后验预测协方差的一个对称因子 $L$ 形成的，定义为 $L L^{\\top} = \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y)$，因此 $z = L^{-1} r$。校准诊断包括检查 $z$ 的分量是否表现得像独立的标准正态变量，以及在后验预测下，观测值的概率积分变换 (Probability Integral Transform, PIT) 是否在 $[0,1]$ 上均匀分布。\n\n在变分贝叶斯 (Variational Bayes, VB) 中，后验 $p(u \\mid y)$ 通过最小化 Kullback-Leibler (KL) 散度 $\\mathrm{KL}(q(u) \\,\\|\\, p(u \\mid y))$（等价于最大化证据下界 (Evidence Lower Bound, ELBO)）被一个易于处理的分布 $q(u)$ 近似。考虑一个平均场高斯 VB 近似 $q(u) = \\mathcal{N}(m_{\\mathrm{VB}}, \\Sigma_{\\mathrm{VB}})$，其中 $\\Sigma_{\\mathrm{VB}}$ 在 $u$ 的典范坐标系中被约束为对角矩阵。那么 VB 后验预测分布为 $\\mathcal{N}(A m_{\\mathrm{VB}}, \\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})$，标准化残差的形成方式类似。\n\n从上述定义以及高斯条件化和边缘化定律出发，推导在精确贝叶斯后验下的后验预测均值和协方差。然后，构造在精确后验下的后验预测残差及其标准化形式的分布。最后，讨论基于标准化残差和概率积分变换的校准诊断如何响应由平均场 VB 近似引起的方差低估。\n\n以下哪个陈述是正确的？\n\nA. 在线性高斯设定中，若先验和噪声被正确指定，则 $r = y - A m_{\\mathrm{post}}$ 是均值为 $0$、协方差为 $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$ 的高斯分布，其中 $m_{\\mathrm{post}}$ 和 $\\Sigma_{\\mathrm{post}}$ 是精确的后验均值和协方差；此外，如果 $L L^{\\top} = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$，则 $z = L^{-1} r$ 具有独立的标准正态分量。\n\nB. 在平均场高斯变分贝叶斯近似下，只要 $A$ 具有正交标准列，VB 后验预测方差 $\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}$ 就是精确后验预测方差 $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$ 的无偏估计。\n\nC. 如果变分贝叶斯后验预测相对于精确后验预测是欠分散的 (under-dispersed)，那么在 VB 后验预测下，观测值的概率积分变换倾向于呈 U 形，在 $0$ 和 $1$ 附近有过多的质量。\n\nD. 对于分量 $i \\in \\{1,\\dots,m\\}$，如果 $z_{i} = r_{i} / \\sqrt{\\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}}$ 是 VB 标准化残差，而在正确模型下 $r \\sim \\mathcal{N}\\left(0, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)$，那么 $\\mathbb{E}[z_{i}^{2}] = \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)_{ii} \\big/ \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}$，因此 VB 中的欠分散意味着 $\\mathbb{E}[z_{i}^{2}]  1$。\n\nE. 用 VB 预测协方差来标准化后验预测残差，可以消除对 VB 协方差低估的敏感性，使得校准诊断对变分近似误差不敏感。\n\n选择所有适用的选项，并从第一性原理出发证明你的选择，包括上述要求的推导。缩写变分贝叶斯 (VB)、Kullback-Leibler (KL) 和证据下界 (ELBO) 必须在首次使用时定义，并且所有数学声明必须由推导或基于所述模型的逻辑上合理的推理来支持。",
            "solution": "问题陈述提出了一个标准的贝叶斯线性逆问题，并要求分析精确的后验预测分布以及使用变分贝叶斯 (Variational Bayes, VB) 近似进行模型校准的后果。缩写词的首次使用将附带其全名：变分贝叶斯 (VB)、Kullback-Leibler (KL) 和证据下界 (ELBO)。我们首先验证问题陈述。\n\n### 问题验证\n**步骤 1：提取给定条件**\n-   未知参数向量：$u \\in \\mathbb{R}^n$\n-   观测向量：$y \\in \\mathbb{R}^m$\n-   观测算子：$A \\in \\mathbb{R}^{m \\times n}$\n-   数据模型（似然）：$y \\mid u \\sim \\mathcal{N}(A u, \\Sigma_{e})$\n-   先验分布：$u \\sim \\mathcal{N}(m_{0}, \\Sigma_{0})$\n-   协方差矩阵：$\\Sigma_{e} \\in \\mathbb{R}^{m \\times m}$ 和 $\\Sigma_{0} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。\n-   后验分布：$p(u \\mid y) \\propto p(y \\mid u) p(u)$\n-   后验预测分布：$p(y_{\\mathrm{rep}} \\mid y) = \\int p(y_{\\mathrm{rep}} \\mid u) p(u \\mid y) \\,\\mathrm{d}u$\n-   后验预测残差：$r = y - \\mathbb{E}[y_{\\mathrm{rep}} \\mid y]$\n-   标准化残差：$z = L^{-1} r$，其中 $L L^{\\top} = \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y)$\n-   VB 近似：$q(u) = \\mathcal{N}(m_{\\mathrm{VB}}, \\Sigma_{\\mathrm{VB}})$，其中 $\\Sigma_{\\mathrm{VB}}$ 是对角的，通过最小化 Kullback-Leibler (KL) 散度 $\\mathrm{KL}(q(u) \\,\\|\\, p(u \\mid y))$（等价于最大化证据下界 (ELBO)）获得。\n-   VB 后验预测：$\\mathcal{N}(A m_{\\mathrm{VB}}, \\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})$\n\n**步骤 2：使用提取的给定条件进行验证**\n该问题在科学上和数学上是合理的。它描述了一个典型的线性高斯模型，这是贝叶斯推断和逆问题的基石。后验预测分布、带有平均场假设的变分贝叶斯以及诸如残差和概率积分变换 (Probability Integral Transform, PIT) 等校准诊断的概念，在统计学和机器学习中都是标准的且定义明确。该问题是自洽的，提供了所有必要的定义。语言精确客观。所提出的问题是适定的 (well-posed)，可以通过基于所提供设置的严格数学推导来回答。没有矛盾、歧义或违反科学原理的地方。\n\n**步骤 3：结论和行动**\n问题是有效的。我们将继续推导所需的量并评估各个选项。\n\n### I. 精确贝叶斯后验的推导\n\n先验和似然都是高斯分布：\n$$p(u) \\propto \\exp\\left(-\\frac{1}{2} (u - m_0)^{\\top} \\Sigma_0^{-1} (u - m_0)\\right)$$\n$$p(y \\mid u) \\propto \\exp\\left(-\\frac{1}{2} (y - Au)^{\\top} \\Sigma_e^{-1} (y - Au)\\right)$$\n后验 $p(u \\mid y)$ 与乘积 $p(y \\mid u) p(u)$ 成正比。指数部分是 $u$ 的二次型：\n$$-\\frac{1}{2} \\left[ (y - Au)^{\\top} \\Sigma_e^{-1} (y - Au) + (u - m_0)^{\\top} \\Sigma_0^{-1} (u - m_0) \\right]$$\n展开并收集关于 $u$ 的项：\n$$-\\frac{1}{2} \\left[ u^{\\top}(A^{\\top}\\Sigma_e^{-1}A + \\Sigma_0^{-1})u - 2u^{\\top}(A^{\\top}\\Sigma_e^{-1}y + \\Sigma_0^{-1}m_0) + \\dots \\right]$$\n这表明后验也是高斯分布，$p(u \\mid y) = \\mathcal{N}(u; m_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}})$，其精度和均值通过配方法给出：\n$$ \\Sigma_{\\mathrm{post}}^{-1} = A^{\\top}\\Sigma_e^{-1}A + \\Sigma_0^{-1} $$\n$$ m_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}}(A^{\\top}\\Sigma_e^{-1}y + \\Sigma_0^{-1}m_0) $$\n\n### II. 精确后验预测分布的推导\n\n后验预测分布 $p(y_{\\mathrm{rep}} \\mid y)$ 是针对一个新观测值 $y_{\\mathrm{rep}}$ 的，该观测值是使用从后验分布中抽取的参数 $u$ 从模型中生成的。也就是说，$y_{\\mathrm{rep}} = Au + \\epsilon$，其中 $u \\sim \\mathcal{N}(m_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}})$ 且 $\\epsilon \\sim \\mathcal{N}(0, \\Sigma_e)$，$u$ 和 $\\epsilon$ 是独立的。\n\n$y_{\\mathrm{rep}}$ 的分布是高斯分布，因为它是两个独立高斯随机变量（$Au$ 和 $\\epsilon$）的和。\n均值为：\n$$ \\mathbb{E}[y_{\\mathrm{rep}} \\mid y] = \\mathbb{E}[Au + \\epsilon \\mid y] = A \\, \\mathbb{E}[u \\mid y] + \\mathbb{E}[\\epsilon] = A m_{\\mathrm{post}} + 0 = A m_{\\mathrm{post}} $$\n协方差为：\n$$ \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y) = \\operatorname{Cov}(Au + \\epsilon \\mid y) = \\operatorname{Cov}(Au \\mid y) + \\operatorname{Cov}(\\epsilon \\mid y) = A \\operatorname{Cov}(u \\mid y) A^{\\top} + \\Sigma_e = A \\Sigma_{\\mathrm{post}} A^{\\top} + \\Sigma_e $$\n因此，精确的后验预测分布为：\n$$ p(y_{\\mathrm{rep}} \\mid y) = \\mathcal{N}(y_{\\mathrm{rep}}; A m_{\\mathrm{post}}, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}) $$\n\n### III. 后验预测残差和校准\n\n对于校准诊断，人们评估观测数据 $y$ 在后验预测分布下是否合理。这是通过将 $y$ 视为从 $p(y_{\\mathrm{rep}} \\mid y)$ 中随机抽取的一个样本来完成的。\n后验预测残差定义为 $r = y - \\mathbb{E}[y_{\\mathrm{rep}} \\mid y] = y - A m_{\\mathrm{post}}$。\n如果将 $y$ 视为从 $p(y_{\\mathrm{rep}} \\mid y)$ 中抽取的随机变量，则相应残差向量 $r$ 的分布为：\n$$ r \\sim \\mathcal{N}(0, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}) $$\n标准化残差是 $z = L^{-1} r$，其中 $L$ 是一个矩阵因子，使得 $L L^{\\top} = \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y) = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$。\n$z$ 的协方差是：\n$$ \\operatorname{Cov}(z) = \\operatorname{Cov}(L^{-1}r) = L^{-1} \\operatorname{Cov}(r) (L^{-1})^{\\top} = L^{-1} (L L^{\\top}) (L^{\\top})^{-1} = I $$\n因此，在一个正确指定的模型下，标准化残差向量 $z$ 服从一个标准多元正态分布，$z \\sim \\mathcal{N}(0, I)$，意味着其分量是独立的标准正态变量。\n\n### IV. 变分贝叶斯和方差低估\n\n变分贝叶斯用一个更简单的分布 $q(u)$ 来近似真实的后验 $p(u \\mid y)$，在本例中是一个具有对角协方差矩阵的多元高斯分布，$q(u) = \\mathcal{N}(m_{\\mathrm{VB}}, \\Sigma_{\\mathrm{VB}})$。众所周知，最小化 $\\mathrm{KL}(q \\,\\|\\, p)$ 会产生一个“过于自信”的近似 $q$，即其方差小于真实后验的方差。这是因为 KL 散度会严厉惩罚 $q$ 在 $p$ 没有质量的地方放置质量，从而迫使 $q$ 拟合在 $p$ 的内部。因此，VB 后验方差 $\\Sigma_{\\mathrm{VB}}$ 系统地低估了真实的后验方差 $\\Sigma_{\\mathrm{post}}$。具体来说，$\\Sigma_{\\mathrm{post}}$ 的非对角元素在 $\\Sigma_{\\mathrm{VB}}$ 中被强制为零，并且通常有 $(\\Sigma_{\\mathrm{VB}})_{ii} \\le (\\Sigma_{\\mathrm{post}})_{ii}$。这导致了对后验预测方差的低估：$\\Sigma_e + A \\Sigma_{\\mathrm{VB}} A^{\\top}$ 的对角元素通常小于 $\\Sigma_e + A \\Sigma_{\\mathrm{post}} A^{\\top}$ 的相应对角元素。\n\n### 逐项分析\n\n**A. 在线性高斯设定中，若先验和噪声被正确指定，则 $r = y - A m_{\\mathrm{post}}$ 是均值为 $0$、协方差为 $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$ 的高斯分布，其中 $m_{\\mathrm{post}}$ 和 $\\Sigma_{\\mathrm{post}}$ 是精确的后验均值和协方差；此外，如果 $L L^{\\top} = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$，则 $z = L^{-1} r$ 具有独立的标准正态分量。**\n\n这个陈述描述了在标准模型检查范式下后验预测残差的属性。正如在第三节中推导的那样，如果模型是正确的，观测数据 $y$ 应该是后验预测分布 $p(y_{\\mathrm{rep}} \\mid y) = \\mathcal{N}(A m_{\\mathrm{post}}, \\Sigma_e + A \\Sigma_{\\mathrm{post}} A^\\top)$ 的一个典型抽样。因此，通过减去该分布的均值形成的残差 $r = y - A m_{\\mathrm{post}}$ 将服从分布 $\\mathcal{N}(0, \\Sigma_e + A \\Sigma_{\\mathrm{post}} A^\\top)$。第二部分是多元正态分布的标准结果：通过乘以 $L^{-1}$（其中 $LL^\\top=C$）对一个协方差为 $C$ 的零均值随机向量进行白化，会产生一个具有单位协方差的随机向量。因此，$z = L^{-1}r$ 的分量是独立的，并且方差为 $1$。\n**结论：正确。**\n\n**B. 在平均场高斯变分贝叶斯近似下，只要 $A$ 具有正交标准列，VB 后验预测方差 $\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}$ 就是精确后验预测方差 $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$ 的无偏估计。**\n\n这里的无偏性意味着相等：$\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top} = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$，这简化为 $A \\Sigma_{\\mathrm{VB}} A^{\\top} = A \\Sigma_{\\mathrm{post}} A^{\\top}$。如果 $A$ 有正交标准列，则 $A^{\\top}A=I_n$。左乘 $A^{\\top}$ 并右乘 $A$ 将得到 $\\Sigma_{\\mathrm{VB}} = \\Sigma_{\\mathrm{post}}$。这个等式仅在 VB 近似是精确的情况下成立，即 $q(u) = p(u \\mid y)$。这种情况发生于真实后验 $p(u \\mid y)$ 本身就具有对角协方差矩阵。后验精度为 $\\Sigma_{\\mathrm{post}}^{-1} = \\Sigma_0^{-1} + A^{\\top}\\Sigma_e^{-1}A$。即使 $A$ 具有正交标准列（使得 $A^\\top A$ 是对角矩阵），如果 $\\Sigma_e$ 不与单位矩阵成比例，矩阵 $A^{\\top}\\Sigma_e^{-1}A$ 也不能保证是对角矩阵。同样，如果 $\\Sigma_0$ 不是对角的，$\\Sigma_{\\mathrm{post}}^{-1}$ 通常也不会是对角的。如果真实后验没有分解，平均场 VB 近似将是不精确的，并且 $\\Sigma_{\\mathrm{VB}} \\neq \\Sigma_{\\mathrm{post}}$。因此，该声明通常是错误的。\n**结论：不正确。**\n\n**C. 如果变分贝叶斯后验预测相对于精确后验预测是欠分散的 (under-dispersed)，那么在 VB 后验预测下，观测值的概率积分变换倾向于呈 U 形，在 $0$ 和 $1$ 附近有过多的质量。**\n\n观测值 $y_i$ 的概率积分变换 (PIT) 是其在预测模型下的累积分布函数 (CDF) 值，$F_{\\mathrm{VB}}(y_i)$。如果模型校准良好，PIT 值应在 $[0, 1]$ 上均匀分布。欠分散意味着 VB 预测分布过窄。这意味着，真实的意外观测（那些位于更宽的真实预测分布尾部的观测）会被狭窄的 VB 模型认为是极其意外的。例如，一个位于真实分布 $2.5\\%$ 分位点的观测值，可能位于更窄的 VB 分布的 $0.5\\%$ 分位点。同样，一个位于真实分布 $97.5\\%$ 分位点的观测值，可能位于 VB 分布的 $99.5\\%$ 分位点。因此，来自真实数据生成过程尾部的观测将被映射到更接近 $0$ 和 $1$ 的 PIT 值。这导致 PIT 值在两端累积，在中间减少，从而形成 U 形的 PIT 值直方图。这是预测欠分散的典型特征。\n**结论：正确。**\n\n**D. 对于分量 $i \\in \\{1,\\dots,m\\}$，如果 $z_{i} = r_{i} / \\sqrt{\\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}}$ 是 VB 标准化残差，而在正确模型下 $r \\sim \\mathcal{N}\\left(0, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)$，那么 $\\mathbb{E}[z_{i}^{2}] = \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)_{ii} \\big/ \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}$，因此 VB 中的欠分散意味着 $\\mathbb{E}[z_{i}^{2}]  1$。**\n\n这个陈述形式化了方差的诊断检查。真实残差 $r_i$ 的均值为 $0$，方差为 $\\sigma_{\\mathrm{post}, i}^2 = (\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top})_{ii}$。VB 标准化残差在分母中使用 VB 预测方差，$\\sigma_{\\mathrm{VB}, i}^2 = (\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})_{ii}$。我们想要计算 $\\mathbb{E}[z_i^2]$：\n$$ \\mathbb{E}[z_i^2] = \\mathbb{E}\\left[ \\left(\\frac{r_i}{\\sigma_{\\mathrm{VB}, i}}\\right)^2 \\right] = \\frac{1}{\\sigma_{\\mathrm{VB}, i}^2} \\mathbb{E}[r_i^2] $$\n由于 $r_i$ 的均值为 $0$，$\\mathbb{E}[r_i^2] = \\operatorname{Var}(r_i) = \\sigma_{\\mathrm{post}, i}^2$。因此，\n$$ \\mathbb{E}[z_i^2] = \\frac{\\sigma_{\\mathrm{post}, i}^2}{\\sigma_{\\mathrm{VB}, i}^2} = \\frac{(\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top})_{ii}}{(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})_{ii}} $$\n这与陈述的第一部分相符。如前所述，VB 中的欠分散意味着 VB 预测方差是真实方差的低估：$\\sigma_{\\mathrm{VB}, i}^2  \\sigma_{\\mathrm{post}, i}^2$。这直接意味着该比率大于 $1$，所以 $\\mathbb{E}[z_i^2] > 1$。标准化残差在平均意义上过大，表明用于标准化的方差过小。\n**结论：正确。**\n\n**E. 用 VB 预测协方差来标准化后验预测残差，可以消除对 VB 协方差低估的敏感性，使得校准诊断对变分近似误差不敏感。**\n\n这个陈述表明标准化是一种纠正措施。这是错误的。标准化是一种诊断工具。如在选项 D 的分析中所示，用一个“不正确”（被低估）的 VB 标准差来标准化一个“真实”的残差，会得到一个量 $z_i$，其平方的期望不是 $1$，而是大于 $1$。通常，VB 标准化残差向量 $z_{\\mathrm{VB}} = L_{\\mathrm{VB}}^{-1} r$ 的协方差将是 $\\operatorname{Cov}(z_{\\mathrm{VB}}) = L_{\\mathrm{VB}}^{-1} \\operatorname{Cov}(r) (L_{\\mathrm{VB}}^{-1})^\\top \\neq I$。该过程非但没有产生一组标准正态值，反而产生了其分布揭示了真实协方差与 VB 近似之间不匹配的值。这远非使诊断对误差不敏感，这个程序正是为了检测此类误差而设计的。\n**结论：不正确。**",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}