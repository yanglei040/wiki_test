{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the nature of an approximation, it is invaluable to apply it to a problem where the exact solution is also attainable. This first exercise provides just such an opportunity, focusing on a simple linear-Gaussian model. By analytically deriving both the exact correlated posterior distribution and its mean-field variational approximation, you will be able to precisely quantify the information lost by ignoring posterior dependencies, a quantity known as the approximation gap ().",
            "id": "3430192",
            "problem": "Consider the linear inverse problem defined by the observation model $y = A x + \\varepsilon$ with $x \\in \\mathbb{R}^2$, where $A \\in \\mathbb{R}^{2 \\times 2}$, $x \\sim \\mathcal{N}(m_0, C_0)$, and $\\varepsilon \\sim \\mathcal{N}(0, R)$. Let the quantities be specified as follows: $A = I_2$, $m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $C_0 = \\begin{pmatrix} 2 & \\frac{6}{5} \\\\ \\frac{6}{5} & 1 \\end{pmatrix}$, $R = \\frac{1}{2} I_2$, and the observed data $y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nUsing Bayes’ rule and the definition of the Gaussian density as the fundamental base, perform the following:\n\n$1.$ Derive the exact posterior distribution $p(x \\mid y)$ and compute its mean and covariance in closed form.\n\n$2.$ Consider a variational family of fully factorized Gaussians $q(x) = \\mathcal{N}(m, \\operatorname{diag}(s_1^2, s_2^2))$. By minimizing the Kullback–Leibler (KL) divergence $\\mathrm{KL}(q \\,\\|\\, p(x \\mid y))$, determine the optimal $m$, $s_1^2$, and $s_2^2$ in closed form.\n\n$3.$ Define the “approximation gap” as the Kullback–Leibler divergence $\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y))$, where $q^\\star$ is the optimal mean-field Gaussian found in part $2$. Provide a single closed-form analytic expression for this value.\n\nReport only the final approximation gap as your final answer. Do not round; provide an exact analytic expression. No units are required.",
            "solution": "The problem requires a three-part analysis of a linear Bayesian inverse problem: first, finding the exact posterior distribution; second, determining the optimal mean-field variational approximation; and third, calculating the Kullback–Leibler (KL) divergence between the approximation and the true posterior.\n\nThe observation model is given by $y = A x + \\varepsilon$, where $x \\in \\mathbb{R}^2$.\nThe prior on $x$ is a Gaussian distribution: $p(x) = \\mathcal{N}(x \\mid m_0, C_0)$.\nThe noise $\\varepsilon$ is also Gaussian: $\\varepsilon \\sim \\mathcal{N}(0, R)$.\nThis implies the likelihood function is $p(y \\mid x) = \\mathcal{N}(y \\mid Ax, R)$.\n\nThe provided values are:\n$A = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n$C_0 = \\begin{pmatrix} 2 & \\frac{6}{5} \\\\ \\frac{6}{5} & 1 \\end{pmatrix}$\n$R = \\frac{1}{2} I_2 = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$\n$y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n\nThe problem is valid as it is a standard, well-posed problem in Bayesian statistics and machine learning, with all necessary information provided and no internal contradictions or scientific flaws.\n\n**Part 1: Exact Posterior Distribution**\n\nFor a linear-Gaussian model, the posterior distribution $p(x \\mid y)$ is also a Gaussian, which we denote as $\\mathcal{N}(x \\mid m_p, C_p)$. The posterior covariance $C_p$ and mean $m_p$ are given by the standard Bayesian update formulas:\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A$$\n$$m_p = C_p (C_0^{-1}m_0 + A^T R^{-1} y)$$\n\nFirst, we compute the inverse matrices $C_0^{-1}$ and $R^{-1}$.\nThe determinant of $C_0$ is $\\det(C_0) = (2)(1) - (\\frac{6}{5})^2 = 2 - \\frac{36}{25} = \\frac{50-36}{25} = \\frac{14}{25}$.\nThe inverse of $C_0$ is:\n$$C_0^{-1} = \\frac{1}{14/25} \\begin{pmatrix} 1 & -\\frac{6}{5} \\\\ -\\frac{6}{5} & 2 \\end{pmatrix} = \\frac{25}{14} \\begin{pmatrix} 1 & -\\frac{6}{5} \\\\ -\\frac{6}{5} & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{25}{7} \\end{pmatrix}$$\nThe inverse of $R$ is:\n$$R^{-1} = (\\frac{1}{2} I_2)^{-1} = 2 I_2 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}$$\n\nNow we can compute the posterior precision matrix $C_p^{-1}$:\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A = C_0^{-1} + I_2 (2I_2) I_2 = C_0^{-1} + 2I_2$$\n$$C_p^{-1} = \\begin{pmatrix} \\frac{25}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{25}{7} \\end{pmatrix} + \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14} + \\frac{28}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{25}{7} + \\frac{14}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{14} & -\\frac{15}{7} \\\\ -\\frac{15}{7} & \\frac{39}{7} \\end{pmatrix}$$\n\nNext, we calculate the posterior mean $m_p$. Since $m_0 = 0$, the formula simplifies to $m_p = C_p (A^T R^{-1} y)$.\nLet's first compute the term $A^T R^{-1} y$:\n$$A^T R^{-1} y = I_2 (2I_2) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$$\nSo, $m_p = C_p \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$, which is equivalent to solving the system $C_p^{-1} m_p = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$.\nTo find $m_p$, we first need $C_p = (C_p^{-1})^{-1}$. The determinant of $C_p^{-1}$ is:\n$$\\det(C_p^{-1}) = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) - \\left(-\\frac{15}{7}\\right)^2 = \\frac{2067}{98} - \\frac{225}{49} = \\frac{2067 - 450}{98} = \\frac{1617}{98} = \\frac{33}{2}$$\nThe posterior covariance matrix $C_p$ is:\n$$C_p = \\frac{1}{33/2} \\begin{pmatrix} \\frac{39}{7} & \\frac{15}{7} \\\\ \\frac{15}{7} & \\frac{53}{14} \\end{pmatrix} = \\frac{2}{33} \\begin{pmatrix} \\frac{78}{14} & \\frac{30}{14} \\\\ \\frac{30}{14} & \\frac{53}{14} \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 78 & 30 \\\\ 30 & 53 \\end{pmatrix}$$\nNow we compute the posterior mean $m_p$:\n$$m_p = \\frac{1}{231} \\begin{pmatrix} 78 & 30 \\\\ 30 & 53 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 156 - 60 \\\\ 60 - 106 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 96 \\\\ -46 \\end{pmatrix} = \\begin{pmatrix} \\frac{32}{77} \\\\ -\\frac{46}{231} \\end{pmatrix}$$\nSo the exact posterior is $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$ with $m_p$ and $C_p$ as derived.\n\n**Part 2: Optimal Variational Approximation**\n\nWe consider a variational family of fully factorized Gaussians $q(x) = q_1(x_1)q_2(x_2) = \\mathcal{N}(x \\mid m, \\operatorname{diag}(s_1^2, s_2^2))$. We minimize $\\mathrm{KL}(q \\,\\|\\, p(x \\mid y))$. For a Gaussian posterior $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$, the optimal parameters for $q$ that minimize this KL divergence are known to be:\n$1.$ The mean of the variational distribution is equal to the mean of the true posterior: $m^\\star = m_p$.\n$2.$ The variances of the factors are the reciprocals of the diagonal elements of the posterior precision matrix $C_p^{-1}$.\nSo, $s_1^2 = 1 / (C_p^{-1})_{11}$ and $s_2^2 = 1 / (C_p^{-1})_{22}$.\n\nUsing the posterior precision matrix $C_p^{-1}$ from Part 1:\n$$(C_p^{-1})_{11} = \\frac{53}{14} \\quad \\text{and} \\quad (C_p^{-1})_{22} = \\frac{39}{7}$$\nThe optimal variances are:\n$$s_1^2 = \\frac{1}{53/14} = \\frac{14}{53}$$\n$$s_2^2 = \\frac{1}{39/7} = \\frac{7}{39}$$\nThe optimal variational distribution is $q^\\star(x) = \\mathcal{N}(x \\mid m^\\star, C_q^\\star)$, where $m^\\star = m_p$ and $C_q^\\star = \\operatorname{diag}(\\frac{14}{53}, \\frac{7}{39})$.\n\n**Part 3: Approximation Gap**\n\nThe approximation gap is defined as $\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y))$. The general formula for the KL divergence between two multivariate Gaussian distributions, $q = \\mathcal{N}(m_q, C_q)$ and $p = \\mathcal{N}(m_p, C_p)$ of dimension $d$, is:\n$$\\mathrm{KL}(q \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q}\\right) - d + \\operatorname{tr}(C_p^{-1} C_q) + (m_p - m_q)^T C_p^{-1} (m_p - m_q) \\right]$$\nIn our case, $d=2$, $q=q^\\star$, and $p=p(x \\mid y)$. From Part 2, we know $m_q = m_p$, so the term $(m_p - m_q)^T C_p^{-1} (m_p - m_q)$ is zero. The formula simplifies to:\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + \\operatorname{tr}(C_p^{-1} C_q^\\star) \\right]$$\nLet's compute the trace term. Let $P = C_p^{-1}$. Then $C_q^\\star = \\operatorname{diag}(1/P_{11}, 1/P_{22})$.\n$$\\operatorname{tr}(C_p^{-1} C_q^\\star) = \\operatorname{tr}\\left( \\begin{pmatrix} P_{11} & P_{12} \\\\ P_{21} & P_{22} \\end{pmatrix} \\begin{pmatrix} 1/P_{11} & 0 \\\\ 0 & 1/P_{22} \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & P_{12}/P_{22} \\\\ P_{21}/P_{11} & 1 \\end{pmatrix} \\right) = 1+1=2$$\nThe trace term is exactly equal to the dimension $d$. The KL divergence expression becomes even simpler:\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + 2 \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right)$$\nWe have $\\det C_p = 1/\\det(C_p^{-1})$. The determinant of $C_q^\\star$ is $(\\det C_q^\\star) = s_1^2 s_2^2 = (1/P_{11})(1/P_{22})$.\nSo the ratio of determinants is:\n$$\\frac{\\det C_p}{\\det C_q^\\star} = \\frac{1/\\det(P)}{1/(P_{11}P_{22})} = \\frac{P_{11}P_{22}}{\\det(P)} = \\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2}$$\nLet's substitute the values for $P_{ij} = (C_p^{-1})_{ij}$:\n$P_{11} = 53/14$, $P_{22} = 39/7$, and $P_{12} = -15/7$.\n$$P_{11}P_{22} = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) = \\frac{2067}{98}$$\n$$P_{12}^2 = \\left(-\\frac{15}{7}\\right)^2 = \\frac{225}{49}$$\nThe ratio is:\n$$\\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2} = \\frac{2067/98}{2067/98 - 225/49} = \\frac{2067/98}{(2067-450)/98} = \\frac{2067}{1617}$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $3$.\n$$\\frac{2067 \\div 3}{1617 \\div 3} = \\frac{689}{539}$$\nThe final approximation gap is:\n$$\\mathrm{KL}(q^\\star \\,\\|\\, p(x \\mid y)) = \\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)$$\nThis can also be expressed as $-\\frac{1}{2} \\ln(1 - \\rho_P^2)$, where $\\rho_P = P_{12}/\\sqrt{P_{11}P_{22}}$ is the correlation coefficient associated with the precision matrix. The non-zero value reflects the information lost by ignoring the posterior correlation between $x_1$ and $x_2$.",
            "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)}$$"
        },
        {
            "introduction": "While linear models offer a clear benchmark, most real-world inverse problems are nonlinear, making exact posterior inference intractable. This practice delves into a simple nonlinear scenario to contrast two powerful approximation techniques: the Laplace approximation and Variational Bayes. You will explore how the Laplace method's reliance on local curvature at the mode compares to a VB approach based on linearizing the forward model, revealing how these different foundational assumptions can lead to distinct estimates of posterior uncertainty ().",
            "id": "3430120",
            "problem": "Consider a Bayesian inverse problem with scalar unknown $x \\in \\mathbb{R}$, Gaussian prior $x \\sim \\mathcal{N}(0,\\sigma_0^2)$, and observation model $y = G(x) + \\eta$ with nonlinear forward map $G(x) = x^2$ and additive Gaussian noise $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$. Let the negative log-posterior (up to an additive constant) be denoted by $\\Phi(x)$. The maximum a posteriori (MAP) estimator $x_{\\mathrm{MAP}}$ is defined as the minimizer of $\\Phi(x)$. You may assume $y > 0$ and $0  2 y  \\sigma^2/\\sigma_0^2$ so that $x_{\\mathrm{MAP}} = 0$ is a local minimizer of $\\Phi(x)$.\n\nStarting from the definitions of the Gaussian prior and Gaussian likelihood, and using only first principles of differentiation and the chain rule, perform the following:\n\n1. Derive $\\Phi(x)$ explicitly and obtain the stationarity condition $\\nabla \\Phi(x) = 0$ to verify that $x = 0$ is a stationary point. Under the stated assumption on $y$, argue that $x_{\\mathrm{MAP}} = 0$ is a local minimizer.\n2. Derive the Hessian $\\nabla^2\\Phi(x)$ at $x_{\\mathrm{MAP}}$ and write the Laplace approximation to the posterior as a Gaussian with mean $x_{\\mathrm{MAP}}$ and covariance $\\Sigma_{\\mathrm{Lap}} = [\\nabla^2\\Phi(x_{\\mathrm{MAP}})]^{-1}$.\n3. Consider a Gaussian Variational Bayes (VB) approximation $q(x) = \\mathcal{N}(\\mu,v)$ obtained by minimizing the Kullback-Leibler divergence (KL) from $q(x)$ to the exact posterior, under the standard Gaussian VB practice of linearizing $G(x)$ around the variational mean $\\mu$ as $G(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)$. Using this linearization, derive the implied quadratic surrogate for the likelihood and the corresponding Gaussian posterior approximation, and obtain the VB covariance at $\\mu = x_{\\mathrm{MAP}}$, denoted $\\Sigma_{\\mathrm{VB}}$.\n4. Compute the ratio $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$ and express it as a single closed-form function of $\\sigma_0^2$, $\\sigma^2$, and $y$.\n\nYour final answer must be the single analytical expression for $\\rho$. No rounding is required and no units are involved.",
            "solution": "The problem requires a four-part derivation comparing the Laplace and Variational Bayes (VB) approximations for a specific nonlinear Bayesian inverse problem. We shall proceed step-by-step as requested.\n\nThe prior probability density function (PDF) for $x$ is given by $x \\sim \\mathcal{N}(0, \\sigma_0^2)$, which is $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{x^2}{2\\sigma_0^2}\\right)$.\nThe observation model is $y = x^2 + \\eta$ with noise $\\eta \\sim \\mathcal{N}(0, \\sigma^2)$. This defines the likelihood PDF as $p(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - x^2)^2}{2\\sigma^2}\\right)$.\n\nAccording to Bayes' theorem, the posterior PDF is proportional to the product of the likelihood and the prior: $p(x|y) \\propto p(y|x)p(x)$. The negative log-posterior, denoted $\\Phi(x)$, can be written by taking the negative logarithm and dropping any terms that do not depend on $x$:\n$$\n\\Phi(x) = -\\ln(p(y|x)) - \\ln(p(x)) + \\text{const}\n$$\n$$\n\\Phi(x) = \\frac{(y - x^2)^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\n\n**Part 1: Analysis of the MAP estimate**\n\nTo find the stationary points of $\\Phi(x)$, we compute its first derivative with respect to $x$, which we denote as $\\nabla \\Phi(x)$:\n$$\n\\nabla \\Phi(x) = \\frac{d\\Phi}{dx} = \\frac{1}{2\\sigma^2} \\cdot 2(y - x^2) \\cdot (-2x) + \\frac{2x}{2\\sigma_0^2}\n$$\n$$\n\\nabla \\Phi(x) = -\\frac{2x(y - x^2)}{\\sigma^2} + \\frac{x}{\\sigma_0^2} = x \\left( \\frac{2x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)\n$$\nThe stationarity condition is $\\nabla \\Phi(x) = 0$. By inspection, one solution is $x=0$. Thus, $x=0$ is a stationary point.\n\nTo determine the nature of this stationary point, we compute the second derivative of $\\Phi(x)$, the Hessian $\\nabla^2 \\Phi(x)$:\n$$\n\\nabla^2 \\Phi(x) = \\frac{d^2\\Phi}{dx^2} = \\frac{d}{dx} \\left( \\frac{2x^3 - 2xy}{\\sigma^2} + \\frac{x}{\\sigma_0^2} \\right)\n$$\n$$\n\\nabla^2 \\Phi(x) = \\frac{6x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\nWe evaluate the Hessian at the stationary point $x=0$:\n$$\n\\nabla^2 \\Phi(0) = \\frac{6(0)^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\nFor $x=0$ to be a local minimizer, the second derivative test requires $\\nabla^2 \\Phi(0)  0$. This implies:\n$$\n\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}  0 \\implies \\frac{1}{\\sigma_0^2}  \\frac{2y}{\\sigma^2} \\implies 2y  \\frac{\\sigma^2}{\\sigma_0^2}\n$$\nThis is precisely the condition provided in the problem statement. Thus, under the given assumption, $x=0$ is a local minimizer of $\\Phi(x)$, and we identify it as the local Maximum A Posteriori estimate, $x_{\\mathrm{MAP}} = 0$.\n\n**Part 2: Laplace Approximation Covariance $\\Sigma_{\\mathrm{Lap}}$**\n\nThe Laplace approximation models the posterior distribution as a Gaussian centered at the MAP estimate, with a precision (inverse covariance) given by the Hessian of the negative log-posterior evaluated at the MAP estimate.\nFor our scalar case, the precision is $\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(x_{\\mathrm{MAP}})$. Using our result from Part 1 with $x_{\\mathrm{MAP}} = 0$:\n$$\n\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(0) = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\nThe covariance of the Laplace approximation is the inverse of this precision:\n$$\n\\Sigma_{\\mathrm{Lap}} = \\left( \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2} \\right)^{-1} = \\left( \\frac{\\sigma^2 - 2y\\sigma_0^2}{\\sigma_0^2\\sigma^2} \\right)^{-1} = \\frac{\\sigma_0^2\\sigma^2}{\\sigma^2 - 2y\\sigma_0^2}\n$$\n\n**Part 3: Variational Bayes Covariance $\\Sigma_{\\mathrm{VB}}$**\n\nThe Gaussian Variational Bayes approximation considers a family of Gaussian distributions $q(x) = \\mathcal{N}(\\mu, v)$ and seeks to minimize the KL divergence $\\mathrm{KL}(q(x) || p(x|y))$. The problem specifies using a linearization of the forward map $G(x) = x^2$ around the variational mean $\\mu$:\n$$\nG(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)\n$$\nSince $G'(\\mu) = 2\\mu$, the linearization is $G(x) \\approx \\mu^2 + 2\\mu(x-\\mu)$.\nWe substitute this into the likelihood term. The approximate negative log-posterior, which we denote $\\tilde{\\Phi}(x; \\mu)$, then becomes:\n$$\n\\tilde{\\Phi}(x; \\mu) = \\frac{(y - (\\mu^2 + 2\\mu(x-\\mu)))^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\nThis expression is quadratic in $x$. The precision of the corresponding Gaussian posterior approximation is given by the coefficient of $\\frac{1}{2}x^2$. Expanding the term from the likelihood:\n$$\n\\frac{(y - \\mu^2 + 2\\mu^2 - 2\\mu x)^2}{2\\sigma^2} = \\frac{((y+\\mu^2) - 2\\mu x)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}((y+\\mu^2)^2 - 4\\mu(y+\\mu^2)x + 4\\mu^2 x^2)\n$$\nThe term containing $x^2$ is $\\frac{4\\mu^2 x^2}{2\\sigma^2} = \\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2}\\right)$.\nSumming the quadratic terms from the approximate likelihood and the prior, the total quadratic term in $\\tilde{\\Phi}(x; \\mu)$ is $\\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)$. The precision of this VB posterior approximation, as a function of the linearization point $\\mu$, is therefore:\n$$\nv^{-1}(\\mu) = \\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\nThe problem asks for the VB covariance evaluated at $\\mu = x_{\\mathrm{MAP}} = 0$. Substituting $\\mu=0$:\n$$\n\\Sigma_{\\mathrm{VB}}^{-1} = v^{-1}(0) = \\frac{4(0)^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2}\n$$\nThe VB covariance $\\Sigma_{\\mathrm{VB}}$ is the inverse of this precision:\n$$\n\\Sigma_{\\mathrm{VB}} = \\left(\\frac{1}{\\sigma_0^2}\\right)^{-1} = \\sigma_0^2\n$$\nThis result shows that when this specific VB approximation is centered at $\\mu=0$, its covariance reverts to the prior covariance. This is because the derivative of the forward map $G'(0)=0$, so the linearized model shows no dependence on $x$, and the data provide no information about the precision of $x$.\n\n**Part 4: Ratio $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$**\n\nWe now compute the ratio of the two derived covariances.\n$$\n\\rho = \\frac{\\Sigma_{\\mathrm{VB}}}{\\Sigma_{\\mathrm{Lap}}} = \\Sigma_{\\mathrm{VB}} \\cdot \\Sigma_{\\mathrm{Lap}}^{-1}\n$$\nSubstituting the expressions we found:\n$$\n\\rho = (\\sigma_0^2) \\cdot \\left(\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\\right)\n$$\nDistributing $\\sigma_0^2$:\n$$\n\\rho = \\sigma_0^2 \\cdot \\frac{1}{\\sigma_0^2} - \\sigma_0^2 \\cdot \\frac{2y}{\\sigma^2}\n$$\n$$\n\\rho = 1 - \\frac{2y\\sigma_0^2}{\\sigma^2}\n$$\nThis is the final closed-form expression for the ratio $\\rho$ as a function of $\\sigma_0^2$, $\\sigma^2$, and $y$. Note that the given condition $0  2y  \\sigma^2/\\sigma_0^2$ implies that $0  \\frac{2y\\sigma_0^2}{\\sigma^2}  1$, which ensures that $0  \\rho  1$. This indicates that for this problem, the Laplace approximation correctly estimates a posterior variance larger than the prior variance (due to the evidence for non-zero $x$), while the VB approximation incorrectly estimates a variance equal to the prior variance.",
            "answer": "$$\\boxed{1 - \\frac{2y\\sigma_0^2}{\\sigma^2}}$$"
        },
        {
            "introduction": "Constructing an approximation is only half the battle; we must also critically assess its quality. This final practice introduces posterior predictive checking, a powerful framework for diagnosing the failures of an approximate posterior. You will learn how to use the approximate posterior from Variational Bayes to generate 'replicated' data and see how its properties compare to the actual data. This process reveals how the characteristic under-dispersion of mean-field VB produces clear, detectable signatures in calibration diagnostics, providing you with the tools to validate your inference ().",
            "id": "3430181",
            "problem": "Consider a linear Gaussian inverse problem in which the unknown parameter vector $u \\in \\mathbb{R}^n$ is inferred from noisy observations $y \\in \\mathbb{R}^m$ via the observation operator $A \\in \\mathbb{R}^{m \\times n}$. The data model is $y \\mid u \\sim \\mathcal{N}(A u, \\Sigma_{e})$ with a Gaussian prior $u \\sim \\mathcal{N}(m_{0}, \\Sigma_{0})$, where $\\Sigma_{e} \\in \\mathbb{R}^{m \\times m}$ and $\\Sigma_{0} \\in \\mathbb{R}^{n \\times n}$ are symmetric positive definite. The Bayesian posterior for $u$ is $p(u \\mid y) \\propto p(y \\mid u) p(u)$, and the posterior predictive distribution for a replicated observation $y_{\\mathrm{rep}}$ is $p(y_{\\mathrm{rep}} \\mid y) = \\int p(y_{\\mathrm{rep}} \\mid u) p(u \\mid y) \\,\\mathrm{d}u$.\n\nSuppose we construct posterior predictive residuals based on the posterior predictive mean, defined by $r = y - \\mathbb{E}[y_{\\mathrm{rep}} \\mid y]$. Standardized residuals are formed using a symmetric factor $L$ of the posterior predictive covariance, defined by $L L^{\\top} = \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y)$, so that $z = L^{-1} r$. Calibration diagnostics include examining whether components of $z$ behave like independent standard normal variables and whether the Probability Integral Transform (PIT) of observed values under the posterior predictive is uniform on $[0,1]$.\n\nIn Variational Bayes (VB), the posterior $p(u \\mid y)$ is approximated by a tractable distribution $q(u)$ by minimizing the Kullback-Leibler (KL) divergence $\\mathrm{KL}(q(u) \\,\\|\\, p(u \\mid y))$, equivalently maximizing the Evidence Lower Bound (ELBO). Consider a mean-field Gaussian VB approximation $q(u) = \\mathcal{N}(m_{\\mathrm{VB}}, \\Sigma_{\\mathrm{VB}})$ with $\\Sigma_{\\mathrm{VB}}$ constrained to be diagonal in the canonical coordinates of $u$. The VB posterior predictive is then $\\mathcal{N}(A m_{\\mathrm{VB}}, \\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})$, with standardized residuals formed analogously.\n\nStarting from the above definitions and the laws of Gaussian conditioning and marginalization, derive the posterior predictive mean and covariance under the exact Bayesian posterior. Then construct the distribution of posterior predictive residuals and their standardized form under the exact posterior. Finally, discuss how calibration diagnostics based on standardized residuals and the Probability Integral Transform respond to variance underestimation induced by the mean-field VB approximation.\n\nWhich of the following statements are correct?\n\nA. In the linear Gaussian setting with correctly specified prior and noise, $r = y - A m_{\\mathrm{post}}$ is Gaussian with mean $0$ and covariance $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$, where $m_{\\mathrm{post}}$ and $\\Sigma_{\\mathrm{post}}$ are the exact posterior mean and covariance; furthermore, if $L L^{\\top} = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$, then $z = L^{-1} r$ has independent standard normal components.\n\nB. Under a mean-field Gaussian Variational Bayes approximation with diagonal $\\Sigma_{\\mathrm{VB}}$, the VB posterior predictive variance $\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}$ is unbiased for the exact posterior predictive variance $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$ whenever $A$ has orthonormal columns.\n\nC. If the Variational Bayes posterior predictive is under-dispersed relative to the exact posterior predictive, then the Probability Integral Transform of observed values under the VB posterior predictive tends to be U-shaped, with excess mass near $0$ and $1$.\n\nD. For component $i \\in \\{1,\\dots,m\\}$, if $z_{i} = r_{i} / \\sqrt{\\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}}$ is the VB-standardized residual while $r \\sim \\mathcal{N}\\left(0, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)$ under the correct model, then $\\mathbb{E}[z_{i}^{2}] = \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)_{ii} \\big/ \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}$, so under-dispersion in VB implies $\\mathbb{E}[z_{i}^{2}]  1$.\n\nE. Standardizing posterior predictive residuals by the VB predictive covariance eliminates sensitivity to VB covariance underestimation, making calibration diagnostics invariant to variational approximation errors.\n\nSelect all that apply, and justify your choices from first principles, including the derivations requested above. The acronyms Variational Bayes (VB), Kullback-Leibler (KL), and Evidence Lower Bound (ELBO) must be defined on first use, and all mathematical claims must be supported by derivations or logically sound reasoning grounded in the stated model.",
            "solution": "The problem statement presents a standard Bayesian linear inverse problem and asks for an analysis of the exact posterior predictive distribution and the consequences of using a Variational Bayes (VB) approximation for model calibration. The first use of acronyms will be accompanied by their full names: Variational Bayes (VB), Kullback-Leibler (KL), and Evidence Lower Bound (ELBO). We begin by validating the problem statement.\n\n### Problem Validation\n**Step 1: Extract Givens**\n-   Unknown parameter vector: $u \\in \\mathbb{R}^n$\n-   Observation vector: $y \\in \\mathbb{R}^m$\n-   Observation operator: $A \\in \\mathbb{R}^{m \\times n}$\n-   Data model (likelihood): $y \\mid u \\sim \\mathcal{N}(A u, \\Sigma_{e})$\n-   Prior distribution: $u \\sim \\mathcal{N}(m_{0}, \\Sigma_{0})$\n-   Covariance matrices: $\\Sigma_{e} \\in \\mathbb{R}^{m \\times m}$ and $\\Sigma_{0} \\in \\mathbb{R}^{n \\times n}$ are symmetric positive definite.\n-   Posterior distribution: $p(u \\mid y) \\propto p(y \\mid u) p(u)$\n-   Posterior predictive distribution: $p(y_{\\mathrm{rep}} \\mid y) = \\int p(y_{\\mathrm{rep}} \\mid u) p(u \\mid y) \\,\\mathrm{d}u$\n-   Posterior predictive residuals: $r = y - \\mathbb{E}[y_{\\mathrm{rep}} \\mid y]$\n-   Standardized residuals: $z = L^{-1} r$, with $L L^{\\top} = \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y)$\n-   VB approximation: $q(u) = \\mathcal{N}(m_{\\mathrm{VB}}, \\Sigma_{\\mathrm{VB}})$ with $\\Sigma_{\\mathrm{VB}}$ diagonal, obtained by minimizing the Kullback-Leibler (KL) divergence $\\mathrm{KL}(q(u) \\,\\|\\, p(u \\mid y))$, equivalent to maximizing the Evidence Lower Bound (ELBO).\n-   VB posterior predictive: $\\mathcal{N}(A m_{\\mathrm{VB}}, \\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It describes a canonical linear-Gaussian model, which is a cornerstone of Bayesian inference and inverse problems. The concepts of posterior predictive distributions, Variational Bayes with a mean-field assumption, and calibration diagnostics like residuals and the Probability Integral Transform (PIT) are all standard and well-defined in statistics and machine learning. The problem is self-contained, with all necessary definitions provided. The language is precise and objective. The questions posed are well-posed and can be answered through rigorous mathematical derivation based on the provided setup. There are no contradictions, ambiguities, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed to derive the required quantities and evaluate the options.\n\n### I. Derivation for the Exact Bayesian Posterior\n\nThe prior and likelihood are Gaussian:\n$$p(u) \\propto \\exp\\left(-\\frac{1}{2} (u - m_0)^{\\top} \\Sigma_0^{-1} (u - m_0)\\right)$$\n$$p(y \\mid u) \\propto \\exp\\left(-\\frac{1}{2} (y - Au)^{\\top} \\Sigma_e^{-1} (y - Au)\\right)$$\nThe posterior $p(u \\mid y)$ is proportional to the product $p(y \\mid u) p(u)$. The exponent is a quadratic form in $u$:\n$$-\\frac{1}{2} \\left[ (y - Au)^{\\top} \\Sigma_e^{-1} (y - Au) + (u - m_0)^{\\top} \\Sigma_0^{-1} (u - m_0) \\right]$$\nExpanding and collecting terms in $u$:\n$$-\\frac{1}{2} \\left[ u^{\\top}(A^{\\top}\\Sigma_e^{-1}A + \\Sigma_0^{-1})u - 2u^{\\top}(A^{\\top}\\Sigma_e^{-1}y + \\Sigma_0^{-1}m_0) + \\dots \\right]$$\nThis shows the posterior is also Gaussian, $p(u \\mid y) = \\mathcal{N}(u; m_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}})$, with precision and mean given by completing the square:\n$$ \\Sigma_{\\mathrm{post}}^{-1} = A^{\\top}\\Sigma_e^{-1}A + \\Sigma_0^{-1} $$\n$$ m_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}}(A^{\\top}\\Sigma_e^{-1}y + \\Sigma_0^{-1}m_0) $$\n\n### II. Derivation for the Exact Posterior Predictive Distribution\n\nThe posterior predictive distribution $p(y_{\\mathrm{rep}} \\mid y)$ is for a new observation $y_{\\mathrm{rep}}$ generated from the model using a parameter $u$ drawn from the posterior. That is, $y_{\\mathrm{rep}} = Au + \\epsilon$, where $u \\sim \\mathcal{N}(m_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}})$ and $\\epsilon \\sim \\mathcal{N}(0, \\Sigma_e)$, with $u$ and $\\epsilon$ being independent.\n\nThe distribution of $y_{\\mathrm{rep}}$ is Gaussian, as it is a sum of two independent Gaussian random variables ($Au$ and $\\epsilon$).\nThe mean is:\n$$ \\mathbb{E}[y_{\\mathrm{rep}} \\mid y] = \\mathbb{E}[Au + \\epsilon \\mid y] = A \\, \\mathbb{E}[u \\mid y] + \\mathbb{E}[\\epsilon] = A m_{\\mathrm{post}} + 0 = A m_{\\mathrm{post}} $$\nThe covariance is:\n$$ \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y) = \\operatorname{Cov}(Au + \\epsilon \\mid y) = \\operatorname{Cov}(Au \\mid y) + \\operatorname{Cov}(\\epsilon \\mid y) = A \\operatorname{Cov}(u \\mid y) A^{\\top} + \\Sigma_e = A \\Sigma_{\\mathrm{post}} A^{\\top} + \\Sigma_e $$\nThus, the exact posterior predictive distribution is:\n$$ p(y_{\\mathrm{rep}} \\mid y) = \\mathcal{N}(y_{\\mathrm{rep}}; A m_{\\mathrm{post}}, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}) $$\n\n### III. Posterior Predictive Residuals and Calibration\n\nFor calibration diagnostics, one assesses if the observed data $y$ is plausible under the posterior predictive distribution. This is done by treating $y$ as a random draw from $p(y_{\\mathrm{rep}} \\mid y)$.\nThe posterior predictive residuals are defined as $r = y - \\mathbb{E}[y_{\\mathrm{rep}} \\mid y] = y - A m_{\\mathrm{post}}$.\nIf $y$ is considered a random variable drawn from $p(y_{\\mathrm{rep}} \\mid y)$, then the distribution of the corresponding residual vector $r$ is:\n$$ r \\sim \\mathcal{N}(0, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}) $$\nThe standardized residuals are $z = L^{-1} r$, where $L$ is a matrix factor such that $L L^{\\top} = \\operatorname{Cov}(y_{\\mathrm{rep}} \\mid y) = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$.\nThe covariance of $z$ is:\n$$ \\operatorname{Cov}(z) = \\operatorname{Cov}(L^{-1}r) = L^{-1} \\operatorname{Cov}(r) (L^{-1})^{\\top} = L^{-1} (L L^{\\top}) (L^{\\top})^{-1} = I $$\nThus, under a correctly specified model, the vector of standardized residuals $z$ follows a standard multivariate normal distribution, $z \\sim \\mathcal{N}(0, I)$, meaning its components are independent standard normal variables.\n\n### IV. Variational Bayes and Variance Underestimation\n\nVariational Bayes approximates the true posterior $p(u \\mid y)$ with a simpler distribution $q(u)$, in this case a multivariate Gaussian with a diagonal covariance matrix, $q(u) = \\mathcal{N}(m_{\\mathrm{VB}}, \\Sigma_{\\mathrm{VB}})$. Minimizing $\\mathrm{KL}(q \\,\\|\\, p)$ is known to yield an approximation $q$ that is \"over-confident,\" i.e., it has smaller variance than the true posterior. This is because the KL divergence heavily penalizes $q$ for placing mass where $p$ has none, forcing $q$ to fit within $p$. Consequently, the VB posterior variance $\\Sigma_{\\mathrm{VB}}$ systematically underestimates the true posterior variance $\\Sigma_{\\mathrm{post}}$. Specifically, the off-diagonal elements of $\\Sigma_{\\mathrm{post}}$ are forced to be zero in $\\Sigma_{\\mathrm{VB}}$, and generally $(\\Sigma_{\\mathrm{VB}})_{ii} \\le (\\Sigma_{\\mathrm{post}})_{ii}$. This leads to underestimation of the posterior predictive variance: the diagonal elements of $\\Sigma_e + A \\Sigma_{\\mathrm{VB}} A^{\\top}$ are typically smaller than the corresponding diagonal elements of $\\Sigma_e + A \\Sigma_{\\mathrm{post}} A^{\\top}$.\n\n### Option-by-Option Analysis\n\n**A. In the linear Gaussian setting with correctly specified prior and noise, $r = y - A m_{\\mathrm{post}}$ is Gaussian with mean $0$ and covariance $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$, where $m_{\\mathrm{post}}$ and $\\Sigma_{\\mathrm{post}}$ are the exact posterior mean and covariance; furthermore, if $L L^{\\top} = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$, then $z = L^{-1} r$ has independent standard normal components.**\n\nThis statement describes the properties of posterior predictive residuals under the standard model-checking paradigm. As derived in Section III, if the model is correct, the observed data $y$ should be a typical draw from the posterior predictive distribution $p(y_{\\mathrm{rep}} \\mid y) = \\mathcal{N}(A m_{\\mathrm{post}}, \\Sigma_e + A \\Sigma_{\\mathrm{post}} A^\\top)$. The residuals, formed by subtracting the mean of this distribution, $r = y - A m_{\\mathrm{post}}$, will thus be distributed as $\\mathcal{N}(0, \\Sigma_e + A \\Sigma_{\\mathrm{post}} A^\\top)$. The second part is a standard result for multivariate normal distributions: whitening a zero-mean random vector with covariance $C$ by multiplying with $L^{-1}$ where $LL^\\top=C$ produces a random vector with identity covariance. Thus, the components of $z = L^{-1}r$ are independent and have variance $1$.\n**Verdict: Correct.**\n\n**B. Under a mean-field Gaussian Variational Bayes approximation with diagonal $\\Sigma_{\\mathrm{VB}}$, the VB posterior predictive variance $\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}$ is unbiased for the exact posterior predictive variance $\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$ whenever $A$ has orthonormal columns.**\n\nUnbiasedness here means equality: $\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top} = \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}$, which simplifies to $A \\Sigma_{\\mathrm{VB}} A^{\\top} = A \\Sigma_{\\mathrm{post}} A^{\\top}$. If $A$ has orthonormal columns, then $A^{\\top}A=I_n$. Premultiplying by $A^{\\top}$ and postmultiplying by $A$ would give $\\Sigma_{\\mathrm{VB}} = \\Sigma_{\\mathrm{post}}$. This equality holds only if the VB approximation is exact, i.e., $q(u) = p(u \\mid y)$. This occurs if the true posterior $p(u \\mid y)$ already has a diagonal covariance matrix. The posterior precision is $\\Sigma_{\\mathrm{post}}^{-1} = \\Sigma_0^{-1} + A^{\\top}\\Sigma_e^{-1}A$. Even if $A$ has orthonormal columns (making $A^\\top A$ diagonal), the matrix $A^{\\top}\\Sigma_e^{-1}A$ is not guaranteed to be diagonal if $\\Sigma_e$ is not proportional to the identity matrix. Similarly, if $\\Sigma_0$ is not diagonal, $\\Sigma_{\\mathrm{post}}^{-1}$ will generally not be diagonal. If the true posterior is not factorized, the mean-field VB approximation will be inexact, and $\\Sigma_{\\mathrm{VB}} \\neq \\Sigma_{\\mathrm{post}}$. Therefore, the claim is false in general.\n**Verdict: Incorrect.**\n\n**C. If the Variational Bayes posterior predictive is under-dispersed relative to the exact posterior predictive, then the Probability Integral Transform of observed values under the VB posterior predictive tends to be U-shaped, with excess mass near $0$ and $1$.**\n\nThe Probability Integral Transform (PIT) of an observation $y_i$ is its CDF value under the predictive model, $F_{\\mathrm{VB}}(y_i)$. If the model is well-calibrated, the PIT values should be uniformly distributed on $[0, 1]$. Under-dispersion means the VB predictive distribution is too narrow. This implies that truly surprising observations (those in the tails of the true, wider predictive distribution) are considered extremely surprising by the narrow VB model. An observation that is, for instance, at the $2.5$-th percentile of the true distribution might be at the $0.5$-th percentile of the narrower VB distribution. Similarly, an observation at the $97.5$-th percentile of the true distribution might be at the $99.5$-th percentile of the VB one. Consequently, observations from the tails of the true data-generating process will be mapped to PIT values closer to $0$ and $1$. This leads to an accumulation of PIT values at the extremes and a depletion in the middle, resulting in a U-shaped histogram of PIT values. This is a classic signature of predictive under-dispersion.\n**Verdict: Correct.**\n\n**D. For component $i \\in \\{1,\\dots,m\\}$, if $z_{i} = r_{i} / \\sqrt{\\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}}$ is the VB-standardized residual while $r \\sim \\mathcal{N}\\left(0, \\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)$ under the correct model, then $\\mathbb{E}[z_{i}^{2}] = \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top}\\right)_{ii} \\big/ \\left(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top}\\right)_{ii}$, so under-dispersion in VB implies $\\mathbb{E}[z_{i}^{2}]  1$.**\n\nThis statement formalizes the diagnostic check for variance. The true residual $r_i$ has mean $0$ and variance $\\sigma_{\\mathrm{post}, i}^2 = (\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top})_{ii}$. The VB-standardized residual uses the VB predictive variance in the denominator, $\\sigma_{\\mathrm{VB}, i}^2 = (\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})_{ii}$. We want to compute $\\mathbb{E}[z_i^2]$:\n$$ \\mathbb{E}[z_i^2] = \\mathbb{E}\\left[ \\left(\\frac{r_i}{\\sigma_{\\mathrm{VB}, i}}\\right)^2 \\right] = \\frac{1}{\\sigma_{\\mathrm{VB}, i}^2} \\mathbb{E}[r_i^2] $$\nSince $r_i$ has mean $0$, $\\mathbb{E}[r_i^2] = \\operatorname{Var}(r_i) = \\sigma_{\\mathrm{post}, i}^2$. Therefore,\n$$ \\mathbb{E}[z_i^2] = \\frac{\\sigma_{\\mathrm{post}, i}^2}{\\sigma_{\\mathrm{VB}, i}^2} = \\frac{(\\Sigma_{e} + A \\Sigma_{\\mathrm{post}} A^{\\top})_{ii}}{(\\Sigma_{e} + A \\Sigma_{\\mathrm{VB}} A^{\\top})_{ii}} $$\nThis matches the first part of the statement. Under-dispersion in VB, as discussed, means that the VB predictive variance is an underestimate of the true one: $\\sigma_{\\mathrm{VB}, i}^2  \\sigma_{\\mathrm{post}, i}^2$. This directly implies that the ratio is greater than $1$, so $\\mathbb{E}[z_i^2]  1$. The standardized residuals are too large on average, indicating the variance used for standardization was too small.\n**Verdict: Correct.**\n\n**E. Standardizing posterior predictive residuals by the VB predictive covariance eliminates sensitivity to VB covariance underestimation, making calibration diagnostics invariant to variational approximation errors.**\n\nThis statement suggests that standardization is a corrective measure. This is false. Standardization is a diagnostic tool. As shown in the analysis of option D, standardizing a \"true\" residual by an \"incorrect\" (underestimated) VB standard deviation results in a quantity $z_i$ whose expected square is not $1$, but greater than $1$. In general, the covariance of the vector of VB-standardized residuals $z_{\\mathrm{VB}} = L_{\\mathrm{VB}}^{-1} r$ will be $\\operatorname{Cov}(z_{\\mathrm{VB}}) = L_{\\mathrm{VB}}^{-1} \\operatorname{Cov}(r) (L_{\\mathrm{VB}}^{-1})^\\top \\neq I$. Instead of producing a set of standard normal values, the process yields values whose distribution reveals the mismatch between the true covariance and the VB approximation. Far from making diagnostics invariant to error, this procedure is designed specifically to detect such errors.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}