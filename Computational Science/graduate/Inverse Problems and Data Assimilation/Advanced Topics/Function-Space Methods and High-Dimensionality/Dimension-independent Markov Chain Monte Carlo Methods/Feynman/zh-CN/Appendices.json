{
    "hands_on_practices": [
        {
            "introduction": "预条件克兰克-尼科尔森（Preconditioned Crank–Nicolson, pCN）算法是维度无关采样方法的基石。本练习旨在通过一个理想化的设定，帮助你深入理解其核心特性。你的任务是解析地证明，在仅有先验信息的情况下，pCN算法的混合性能（通过自相关性来衡量）不会随着问题维度的增加而退化，从而揭示其维度无关性的本质 。这个理论洞察力对于后续构建更复杂的维度无关算法至关重要。",
            "id": "3376428",
            "problem": "考虑一个定义在可分希尔伯特空间 $\\mathcal{H}$ 上的贝叶斯线性反问题，其高斯先验为 $u \\sim \\mathcal{N}(0,\\mathcal{C})$，其中 $\\mathcal{C}:\\mathcal{H}\\to\\mathcal{H}$ 是一个迹类、自伴、正定的协方差算子。假设似然是平坦的（等价地，正向算子是零映射，因此数据不包含任何信息），所以后验等于先验。令 $\\{(\\lambda_{k},e_{k})\\}_{k\\geq 1}$ 表示 $\\mathcal{C}$ 的特征对，其中 $\\lambda_{k}>0$ 且 $\\{e_{k}\\}$ 构成 $\\mathcal{H}$ 的一个标准正交基。对于一个固定的截断水平 $J\\in\\mathbb{N}$，定义 Karhunen–Loève (KL) 截断\n$$\nu^{(J)}=\\sum_{k=1}^{J}\\sqrt{\\lambda_{k}}\\,\\xi_{k}\\,e_{k},\\quad \\xi_{k}\\stackrel{\\text{i.i.d.}}{\\sim}\\mathcal{N}(0,1).\n$$\n令 $\\ell:\\mathcal{H}\\to\\mathbb{R}$ 是一个连续线性泛函，其先验方差 $\\sigma_{\\ell}^{2}=\\langle \\ell,\\mathcal{C}\\ell\\rangle$ 是有限的，其中 $\\langle\\cdot,\\cdot\\rangle$ 表示由 $\\mathcal{H}$ 导出的对偶配对。考虑参数为 $\\beta\\in(0,1)$ 的预条件克兰克-尼科尔森 (pCN) 马尔可夫链，它在截断空间上由以下更新规则定义\n$$\nu_{n+1}^{(J)}=\\sqrt{1-\\beta^{2}}\\,u_{n}^{(J)}+\\beta\\,\\xi_{n+1}^{(J)},\n$$\n其中 $\\xi_{n+1}^{(J)}\\sim\\mathcal{N}(0,\\mathcal{C}^{(J)})$ 独立于 $u_{n}^{(J)}$，且 $\\mathcal{C}^{(J)}$ 是对应于 J-模态截断的协方差。在上述零信息似然设置中，pCN 提议总是被接受，因此这定义了马尔可夫链的精确转移。\n\n令 $s_{n}^{(J)}=\\ell(u_{n}^{(J)})$ 表示通过将 $\\ell$ 应用于该链而得到的标量时间序列。在链平稳的条件下，推导 $s_{n}^{(J)}$ 的滞后-1 自相关，其定义为\n$$\n\\rho_{1}^{(J)}=\\frac{\\operatorname{Cov}\\big(s_{n+1}^{(J)},s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)},\n$$\n作为一个仅含 $\\beta$ 的解析表达式。证明该表达式与截断水平 $J$ 无关。将最终答案表示为一个闭式符号表达式。不需要四舍五入，最终表达式中也不应包含单位。",
            "solution": "该问题被确定为有效，因为它在科学上基于贝叶斯反问题和马尔可夫链蒙特卡洛方法的理论，是适定的，具有明确的目标，并以无歧义的正式语言表达。因此，我们可以进行推导。\n\n我们的目标是计算标量时间序列 $s_{n}^{(J)} = \\ell(u_{n}^{(J)})$ 的滞后-1 自相关，其定义为：\n$$\n\\rho_{1}^{(J)} = \\frac{\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)}\n$$\n问题指明马尔可夫链处于其平稳状态。预条件克兰克-尼科尔森 (pCN) 算法被设计为以目标分布为其不变测度。在这种情况下，由于似然是平坦的，目标分布就是先验分布。对于 J 维截断系统，目标分布是 $\\mathcal{N}(0, \\mathcal{C}^{(J)})$，其中 $\\mathcal{C}^{(J)}$ 是对应于 J-模态截断的协方差算子。因此，在平稳状态下，对于任何时间指标 $n$，状态 $u_{n}^{(J)}$ 是一个从该分布中抽取的随机变量：\n$$\nu_{n}^{(J)} \\sim \\mathcal{N}(0, \\mathcal{C}^{(J)})\n$$\n标量时间序列是通过将连续线性泛函 $\\ell$ 应用于状态 $s_{n}^{(J)} = \\ell(u_{n}^{(J)})$ 得到的。由于 $u_{n}^{(J)}$ 是一个零均值高斯随机变量且 $\\ell$ 是线性的，因此 $s_{n}^{(J)}$ 也是一个零均值高斯随机变量。我们可以通过计算其期望来正式地证明这一点：\n$$\n\\mathbb{E}\\big[s_{n}^{(J)}\\big] = \\mathbb{E}\\big[\\ell(u_{n}^{(J)})\\big] = \\ell\\big(\\mathbb{E}[u_{n}^{(J)}]\\big) = \\ell(0) = 0\n$$\n$\\rho_{1}^{(J)}$ 定义中的分母是 $s_{n}^{(J)}$ 的方差。由于均值为零，方差由下式给出：\n$$\n\\operatorname{Var}\\big(s_{n}^{(J)}\\big) = \\mathbb{E}\\big[(s_{n}^{(J)})^{2}\\big]\n$$\n这是应用于高斯随机变量的线性泛函的方差，由与协方差算子相关的二次型给出：\n$$\n\\operatorname{Var}\\big(s_{n}^{(J)}\\big) = \\langle \\ell, \\mathcal{C}^{(J)}\\ell \\rangle\n$$\n我们假设 $\\ell$ 不是零泛函，以确保对于任何 $J$，只要截断先验的支撑集在 $\\ell$ 上有非平凡投影，就有 $\\operatorname{Var}(s_{n}^{(J)}) > 0$。如果我们假设 $\\ell$ 关于基 $\\{e_k\\}$ 的第一个分量非零，那么对于任何 $J\\ge 1$ 这一点都是有保证的。问题中 $\\sigma_{\\ell}^{2}=\\langle \\ell,\\mathcal{C}\\ell\\rangle$ 是有限的这一条件意味着 $\\ell$ 在一个合适的域中，并且假设 $\\ell \\neq 0$ 意味着 $\\sigma_{\\ell}^2 > 0$。\n\n接下来，我们处理分子，即协方差项 $\\operatorname{Cov}(s_{n+1}^{(J)}, s_{n}^{(J)})$。由于均值为零，这可以简化为：\n$$\n\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big) = \\mathbb{E}\\big[s_{n+1}^{(J)}s_{n}^{(J)}\\big]\n$$\n我们使用问题陈述中给出的 $u_{n+1}^{(J)}$ 的 pCN 更新规则：\n$$\nu_{n+1}^{(J)} = \\sqrt{1-\\beta^{2}}\\,u_{n}^{(J)} + \\beta\\,\\xi_{n+1}^{(J)}\n$$\n其中 $\\beta \\in (0,1)$ 且新息项 $\\xi_{n+1}^{(J)} \\sim \\mathcal{N}(0, \\mathcal{C}^{(J)})$ 独立于 $u_{n}^{(J)}$。将线性泛函 $\\ell$ 应用于更新方程的两边，得到 $s_{n+1}^{(J)}$ 的表达式：\n$$\ns_{n+1}^{(J)} = \\ell\\left(\\sqrt{1-\\beta^{2}}\\,u_{n}^{(J)} + \\beta\\,\\xi_{n+1}^{(J)}\\right) = \\sqrt{1-\\beta^{2}}\\,\\ell(u_{n}^{(J)}) + \\beta\\,\\ell(\\xi_{n+1}^{(J)})\n$$\n认识到 $\\ell(u_{n}^{(J)}) = s_{n}^{(J)}$，我们得到标量时间序列的以下自回归关系：\n$$\ns_{n+1}^{(J)} = \\sqrt{1-\\beta^{2}}\\,s_{n}^{(J)} + \\beta\\,\\ell(\\xi_{n+1}^{(J)})\n$$\n我们现在计算期望 $\\mathbb{E}[s_{n+1}^{(J)}s_{n}^{(J)}]$：\n$$\n\\mathbb{E}\\big[s_{n+1}^{(J)}s_{n}^{(J)}\\big] = \\mathbb{E}\\left[\\left(\\sqrt{1-\\beta^{2}}\\,s_{n}^{(J)} + \\beta\\,\\ell(\\xi_{n+1}^{(J)})\\right) s_{n}^{(J)}\\right]\n$$\n根据期望的线性性：\n$$\n\\mathbb{E}\\big[s_{n+1}^{(J)}s_{n}^{(J)}\\big] = \\sqrt{1-\\beta^{2}}\\,\\mathbb{E}\\big[(s_{n}^{(J)})^{2}\\big] + \\beta\\,\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}\\big]\n$$\n第一项可被识别为 $\\sqrt{1-\\beta^{2}}\\,\\operatorname{Var}(s_{n}^{(J)})$。在第二项中，我们分析期望 $\\mathbb{E}[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}]$。根据构造，新息 $\\xi_{n+1}^{(J)}$ 独立于当前状态 $u_{n}^{(J)}$。因此，$\\xi_{n+1}^{(J)}$ 的任何函数（如 $\\ell(\\xi_{n+1}^{(J)})$）都独立于 $u_{n}^{(J)}$ 的任何函数（如 $s_{n}^{(J)}$）。因此，积的期望等于期望的积：\n$$\n\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}\\big] = \\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})\\big] \\mathbb{E}\\big[s_{n}^{(J)}\\big]\n$$\n我们已经确定 $\\mathbb{E}[s_{n}^{(J)}] = 0$。新息项的期望也为零：\n$$\n\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})\\big] = \\ell\\left(\\mathbb{E}[\\xi_{n+1}^{(J)}]\\right) = \\ell(0) = 0\n$$\n因此，第二项完全消失：\n$$\n\\beta\\,\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}\\big] = \\beta \\cdot 0 \\cdot 0 = 0\n$$\n这就给我们留下了协方差的最终表达式：\n$$\n\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big) = \\sqrt{1-\\beta^{2}}\\,\\operatorname{Var}\\big(s_{n}^{(J)}\\big)\n$$\n我们现在可以组合出滞后-1 自相关：\n$$\n\\rho_{1}^{(J)} = \\frac{\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)} = \\frac{\\sqrt{1-\\beta^{2}}\\,\\operatorname{Var}\\big(s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)}\n$$\n只要 $\\operatorname{Var}(s_{n}^{(J)}) \\neq 0$，我们就可以消去方差项，得到：\n$$\n\\rho_{1}^{(J)} = \\sqrt{1-\\beta^{2}}\n$$\n这个结果仅仅是 pCN 参数 $\\beta$ 的函数，并且确实与截断水平 $J$、协方差算子 $\\mathcal{C}$ 以及线性泛函 $\\ell$ 的选择无关。这一性质是 pCN 方法在这种理想化设置下的一个标志，其中算法的自回归结构直接转化为任何线性可观测量的自回归结构。",
            "answer": "$$\\boxed{\\sqrt{1-\\beta^{2}}}$$"
        },
        {
            "introduction": "现在，我们将理论付诸实践，探索一种更精妙的方法——函数空间Metropolis调整的朗之万算法（MALA），该方法利用了来自似然函数的梯度信息。本练习将指导你为一个由偏微分方程（PDE）约束的逆问题实现此算法，这在科学计算中是一个非常典型的应用场景。关键任务是通过经验验证该算法的维度无关特性，即在不同的离散化水平下，接受率能够保持稳定，这是正确预条件处理的直接体现 。",
            "id": "3376373",
            "problem": "考虑单位区间上的周期性边界条件的一维扩散初值问题，该问题由偏微分方程 (PDE) $\\,\\partial_t y - \\partial_x\\!\\left(u(x)\\,\\partial_x y\\right)=0\\,$ 定义，其中 $\\,x\\in[0,1]\\,$ 且 $\\,t\\ge 0\\,$, $\\,u(x)\\,$ 是待从观测中推断的空间变化的扩散系数。设初始条件为 $\\,y(x,0)=y_0(x)\\,$，其中 $\\,y_0\\,$ 是一个光滑函数。假设在一个很小的时间 $\\,T>0\\,$ 时的观测值可以通过线性化关系近似：\n$$\ny(x,T)\\approx y_0(x) + T\\,\\partial_x\\!\\left(u(x)\\,\\partial_x y_0(x)\\right),\n$$\n因此离散数据向量 $\\,d\\,$ 可建模为\n$$\nd = A u + \\varepsilon,\\quad A u := T\\,D u,\\quad D u := \\partial_x u \\cdot \\partial_x y_0 + u \\cdot \\partial_{xx} y_0,\n$$\n其中 $\\,\\varepsilon\\,$ 是均值为零、协方差为 $\\,\\Gamma = \\sigma^2 I\\,$ 的高斯噪声，$\\,I\\,$ 是离散网格上的单位算子。\n\n对 $\\,u\\,$ 采用高斯先验，其均值为 $\\,m(x)\\,$，协方差算子为 $\\,\\mathcal{C} = \\left(\\alpha I - \\Delta\\right)^{-\\kappa}\\,$，定义在具有周期性边界条件的 $\\,L^2([0,1])\\,$ 上，其中 $\\,\\alpha>0\\,$, $\\,\\kappa\\ge 1\\,$, $\\,\\Delta\\,$ 是拉普拉斯算子。对于间距为 $\\,h\\,$ 的离散网格，负对数后验（不计加性常数）为\n$$\nJ(u) = \\frac{1}{2}\\,\\left\\langle A u - d,\\,\\Gamma^{-1}(A u - d)\\right\\rangle_{L^2} + \\frac{1}{2}\\,\\left\\langle u - m,\\,\\mathcal{C}^{-1}(u - m)\\right\\rangle_{L^2},\n$$\n其中 $\\,L^2\\,$ 内积通过 $\\,\\langle v,w\\rangle_{L^2} \\approx h \\sum_j v_j w_j\\,$ 进行离散化。\n\n构建一个函数空间 Metropolis-Adjusted Langevin 算法 (MALA)，通过预处理提议来更新 $\\,u\\,$：\n$$\nu' = u + \\eta\\,\\mathcal{C}\\,\\nabla \\Phi(u) + \\sqrt{2\\eta}\\,\\mathcal{C}^{1/2}\\,\\xi,\n$$\n其中 $\\,\\Phi(u)\\,$ 是对数后验（因此 $\\,\\nabla \\Phi(u) = -\\nabla J(u)\\,$），$\\,\\eta>0\\,$ 是步长，$\\,\\xi\\sim \\mathcal{N}(0,I)\\,$ 是网格上的白噪声。使用由 $\\,L^2\\,$ 内积定义的伴随算子 $\\,A^*\\,$ 来计算 $\\,\\nabla J(u) = A^*\\Gamma^{-1}(Au-d) + \\mathcal{C}^{-1}(u-m)\\,$。得到的提议密度是均值为 $\\,u + \\eta\\,g(u)\\,$、协方差为 $\\,2\\eta\\,\\mathcal{C}\\,$ 的高斯分布，其中 $\\,g(u)=-\\mathcal{C}\\,\\nabla J(u)\\,$，MALA 的接受概率为\n$$\na(u,u') = \\min\\left\\{1,\\ \\exp\\!\\Big(\\Phi(u')-\\Phi(u) + \\log q(u\\mid u') - \\log q(u'\\mid u)\\Big)\\right\\},\n$$\n其中\n$$\n\\log q(u'\\mid u) = -\\frac{1}{4\\eta}\\,\\left\\langle u' - u - \\eta g(u),\\,\\mathcal{C}^{-1}\\left(u' - u - \\eta g(u)\\right)\\right\\rangle_{L^2} + \\text{const}.\n$$\n\n您的任务是：\n- 从偏微分方程和分部积分出发，在周期性边界条件下推导 $\\,D\\,$ 及其 $\\,L^2\\,$ 伴随算子 $\\,D^*\\,$ 的显式离散形式。\n- 使用这些形式，在一个大小为 $\\,N\\,$、间距为 $\\,h=1/N\\,$ 的均匀网格上实现 $\\,A\\,$ 和 $\\,A^*\\,$。\n- 在周期性边界条件下，使用 $\\,\\alpha I - \\Delta\\,$ 的离散傅里叶变换对角化，实现 $\\,\\mathcal{C}\\,$, $\\,\\mathcal{C}^{-1}\\,$ 和 $\\,\\mathcal{C}^{1/2}\\,$。使用 $\\,\\kappa=1\\,$。\n- 构建并运行具有上述接受规则的函数空间 MALA 采样器，并计算每个测试用例在固定迭代次数下的经验接受率。\n\n识别并论证步长 $\\,\\eta\\,$ 的一种缩放方式，该方式能在网格尺寸 $\\,N\\,$ 增加时保持维度无关 (DI) 行为，即接受率在网格加密后大致保持稳定。通过在所有网格上使用单一恒定的 $\\,\\eta\\,$ 来运行您的采样器，凭经验证明这一点。\n\n测试套件的参数：\n- 使用 $\\,y_0(x) = \\sin(2\\pi x) + \\tfrac{1}{2}\\cos(6\\pi x)\\,$；在网格上将其离散化。\n- 设置 $\\,T = 10^{-2}\\,$, $\\,\\sigma = 5\\times 10^{-2}\\,$, $\\,\\alpha = 1\\,$, $\\,\\kappa = 1\\,$，以及先验均值 $\\,m(x)\\equiv 1\\,$。\n- 通过从先验中抽取一个真实值 $\\,u^\\dagger\\,$（幅度较小以保持 $\\,u^\\dagger>0\\,$）并设置 $\\,d = A u^\\dagger + \\varepsilon\\,$（其中 $\\,\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2 I)\\,$）来生成合成数据。\n- 使用固定的随机种子以确保可复现性。\n- 从初始化 $\\,u_0=m\\,$ 开始，对所有网格使用相同的恒定 $\\,\\eta\\,$ 运行 MALA 链 $\\,K\\,$ 次迭代。\n\n测试用例：\n- 用例 1: $\\,N=32\\,$。\n- 用例 2: $\\,N=64\\,$。\n- 用例 3: $\\,N=128\\,$。\n\n输出规格：\n- 对每个测试用例，计算经验接受率，结果为一个在 $\\,[0,1]\\,$ 范围内的浮点数，四舍五入到六位小数。\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的、按测试用例顺序排列的逗号分隔列表，例如 $\\,\\left[r_1,r_2,r_3\\right]\\,$。\n\n程序必须是一个完整的、可运行的脚本，执行所有计算并仅输出指定的最后一行。",
            "solution": "该问题要求实现并分析一个函数空间 Metropolis-Adjusted Langevin 算法 (MALA)，用于解决涉及一维扩散方程的贝叶斯逆问题。核心任务是推导并实现必要的离散算子，构建 MALA 采样器，并验证其维度无关的收敛特性。\n\n### 1. 离散化与算子推导\n\n我们在一个包含 $N$ 个点 $x_j = j h$（其中 $j=0, \\dots, N-1$）的均匀网格上进行操作，网格间距为 $h=1/N$。函数被表示为其在这些网格点上的值构成的向量。$L^2$ 内积被离散化为 $\\langle v, w \\rangle_{L^2} \\approx h \\sum_{j=0}^{N-1} v_j w_j$。\n\n**前向算子 $A$ 及其伴随算子 $A^*$**\n\n前向算子定义为 $A u = T D u$，其中 $D u = (\\partial_x y_0) (\\partial_x u) + (\\partial_{xx} y_0) u$。\n为了离散化 $D$，我们对导数 $\\partial_x u$ 使用二阶中心差分格式，这适用于周期性边界条件：\n$$\n(\\partial_x u)_j \\approx \\frac{u_{j+1} - u_{j-1}}{2h}\n$$\n其中下标对 $N$ 取模。令 $\\mathbf{u}$、$\\mathbf{\\partial_x y_0}$ 和 $\\mathbf{\\partial_{xx} y_0}$ 为表示这些函数在网格上的向量。作用于 $\\mathbf{u}$ 上的离散算子 $D$ 为：\n$$\n(D \\mathbf{u})_j = (\\mathbf{\\partial_x y_0})_j \\left( \\frac{\\mathbf{u}_{j+1} - \\mathbf{u}_{j-1}}{2h} \\right) + (\\mathbf{\\partial_{xx} y_0})_j \\mathbf{u}_j\n$$\n算子 $A$ 则为 $A = T D$。\n\n为了找到 $L^2$-伴随算子 $D^*$，我们使用定义 $\\langle Dv, w \\rangle_{L^2} = \\langle v, D^*w \\rangle_{L^2}$。在连续层面上，使用分部积分和周期性：\n$$\n\\begin{align*}\n\\langle Du, v \\rangle_{L^2} = \\int_0^1 \\left( (\\partial_x y_0)(\\partial_x u) + (\\partial_{xx} y_0) u \\right) v \\, dx \\\\\n= \\int_0^1 (\\partial_x y_0) v (\\partial_x u) \\, dx + \\int_0^1 u (\\partial_{xx} y_0) v \\, dx \\\\\n= \\left[ u (\\partial_x y_0) v \\right]_0^1 - \\int_0^1 u \\, \\partial_x ((\\partial_x y_0) v) \\, dx + \\int_0^1 u (\\partial_{xx} y_0) v \\, dx \\\\\n= -\\int_0^1 u \\left( (\\partial_{xx} y_0) v + (\\partial_x y_0) (\\partial_x v) \\right) \\, dx + \\int_0^1 u (\\partial_{xx} y_0) v \\, dx \\\\\n= \\int_0^1 u \\left( -(\\partial_x y_0) (\\partial_x v) \\right) \\, dx\n\\end{align*}\n$$\n因此，我们确定伴随算子为 $D^* v = -(\\partial_x y_0) (\\partial_x v)$。其离散形式，对 $\\partial_x v$ 使用相同的中心差分格式，为：\n$$\n(D^* \\mathbf{v})_j = -(\\mathbf{\\partial_x y_0})_j \\left( \\frac{\\mathbf{v}_{j+1} - \\mathbf{v}_{j-1}}{2h} \\right)\n$$\n完整前向算子的伴随算子是 $A^* = (TD)^* = T D^*$，因为 $T$ 是一个实标量。\n\n### 2. 傅里叶空间中的先验协方差算子\n\n先验协方差 $\\mathcal{C} = (\\alpha I - \\Delta)^{-\\kappa}$（其中 $\\kappa=1$）定义在周期函数空间上。这类平移不变算子可被傅里叶基对角化。算子 $\\mathcal{C}$ 及其幂次（$\\mathcal{C}^{-1}$, $\\mathcal{C}^{1/2}$）可以使用快速傅里叶变换 (FFT) 高效地应用。\n\n在周期区间 $[0,1]$ 上，负拉普拉斯算子 $-\\Delta$ 的特征值为 $(2\\pi k)^2$，其中 $k$ 为整数。在一个大小为 $N$、间距为 $h$ 的离散网格上，对应的离散波数为 $q_k = 2\\pi k'$，其中 $k' = \\texttt{fftfreq}(N, h)$。$-\\Delta$ 的离散特征值为 $\\lambda_k = q_k^2$。\n\n算子 $\\mathcal{C}^{-1} = \\alpha I - \\Delta$ 在傅里叶空间中是一个乘法算子，其特征值为 $\\mu_k = \\alpha + \\lambda_k = \\alpha + (2\\pi k')^2$。因此，$\\mathcal{C}$, $\\mathcal{C}^{-1}$ 和 $\\mathcal{C}^{1/2}$ 作用于向量 $\\mathbf{v}$ 的计算过程如下：\n1.  计算离散傅里叶变换：$\\hat{\\mathbf{v}} = \\text{FFT}(\\mathbf{v})$。\n2.  将傅里叶系数 $\\hat{\\mathbf{v}}_k$ 乘以相应的特征值：\n    -   对于 $\\mathcal{C}\\mathbf{v}$：乘以 $\\mu_k^{-\\kappa} = (\\alpha + \\lambda_k)^{-1}$ (因为 $\\kappa=1$)。\n    -   对于 $\\mathcal{C}^{-1}\\mathbf{v}$：乘以 $\\mu_k^{\\kappa} = \\alpha + \\lambda_k$。\n    -   对于 $\\mathcal{C}^{1/2}\\mathbf{v}$：乘以 $\\mu_k^{-\\kappa/2} = (\\alpha + \\lambda_k)^{-1/2}$。\n3.  计算结果的逆离散傅里叶变换。\n\n这个过程提供了一种无矩阵且高效 ($O(N \\log N)$) 的算子实现方法。\n\n### 3. MALA 采样器与接受概率\n\n函数空间 MALA 通过一个预处理的朗之万型更新，从当前状态 $u$ 提议一个新状态 $u'$：\n$$\nu' = u + \\eta\\,g(u) + \\sqrt{2\\eta}\\,\\mathcal{C}^{1/2}\\,\\xi\n$$\n其中 $\\eta$ 是步长，$\\xi \\sim \\mathcal{N}(0,I)$ 是离散白噪声，而 $g(u) = -\\mathcal{C}\\,\\nabla \\Phi(u) = -\\mathcal{C}(-\\nabla J(u)) = \\mathcal{C}\\,\\nabla J(u)$。问题描述中存在一个轻微的符号不一致；提议应该是一个沿对数后验 $\\Phi$ 增加方向的步长。我们使用 MALA 的标准形式，即在 $\\Phi$ 上的梯度上升：\n$u' = u + \\eta\\,\\mathcal{C}\\,\\nabla\\Phi(u) + \\dots = u - \\eta\\,\\mathcal{C}\\,\\nabla J(u) + \\dots$。\n然而，问题将 $g(u)=-\\mathcal{C}\\nabla J(u)$ 并在提议中使用了加号。为了与所给公式保持一致，我们将遵循问题陈述：$u' = u + \\eta\\,g(u) + \\dots$。\n\n负对数后验 $J(u)$ 关于 $L^2$ 内积的梯度为：\n$$\n\\nabla J(u) = A^*\\Gamma^{-1}(Au-d) + \\mathcal{C}^{-1}(u-m) = \\frac{1}{\\sigma^2}A^*(Au-d) + \\mathcal{C}^{-1}(u-m)\n$$\n接受概率为 $a(u,u') = \\min\\{1, \\exp(\\log\\alpha)\\}$，其中对数比率为：\n$$\n\\log\\alpha = \\Phi(u') - \\Phi(u) + \\log q(u\\mid u') - \\log q(u'\\mid u)\n$$\n使用 $\\Phi(u) = -J(u)$，这变为：\n$$\n\\log\\alpha = J(u) - J(u') + \\log q(u\\mid u') - \\log q(u'\\mid u)\n$$\n对数提议密度由下式给出：\n$$\n\\log q(v \\mid w) = -\\frac{1}{4\\eta}\\,\\left\\langle v - w - \\eta g(w),\\,\\mathcal{C}^{-1}\\left(v - w - \\eta g(w)\\right)\\right\\rangle_{L^2} + \\text{const}\n$$\n为了计算这一项，我们计算残差 $r = v - w - \\eta g(w)$，然后评估内积 $\\langle r, \\mathcal{C}^{-1}r \\rangle_{L^2} = \\langle \\mathcal{C}^{-1/2}r, \\mathcal{C}^{-1/2}r \\rangle_{L^2}$，其离散化为 $h \\sum_j (\\mathcal{C}^{-1/2}r)_j^2$。因此，所有分量都使用上面推导的算子进行计算。\n\n### 4. 维度无关步长的论证\n\n标准的高维 MCMC 方法，如随机游走 Metropolis 或未预处理的 MALA，会遭受维度灾难：为了保持恒定的接受率，步长 $\\eta$ 必须随着状态空间维度 $N$ 的增加而缩小（例如，标准 MALA 中 $\\eta \\propto N^{-1/3}$）。\n\n这里实现的函数空间 MALA 专门设计用来克服这个问题。关键在于通过先验协方差算子 $\\mathcal{C}$ 对更新进行预处理。提议由两部分组成：一个漂移项 $\\eta g(u) = -\\eta \\mathcal{C} \\nabla J(u)$ 和一个扩散项 $\\sqrt{2\\eta}\\mathcal{C}^{1/2}\\xi$。这两部分都由与 $\\mathcal{C}$ 相关的算子进行缩放。这相当于在一个“白化”参数 $v$ 上执行标准 MALA，其中 $u = m + \\mathcal{C}^{1/2} v$，而 $v$ 的先验是标准高斯分布 $\\mathcal{N}(0, I)$。\n\n通过将先验结构融入提议机制，该算法提出的步长对于由先验定义的函数空间来说是自然的。采样器实际上是在函数空间中进行探索，而不是在一个高维向量空间中。随着离散化的加密（即 $N$ 增加），只要底层的函数空间问题是适定的，采样器的性能（包括其对固定 $\\eta$ 的接受率）预计将保持稳定。这种维度无关的行为是函数空间 MCMC 方法的一个标志性特征。我们通过对所有网格尺寸（$N=32, 64, 128$）使用单一恒定步长 $\\eta$ 并观察到经验接受率大致恒定来证明这一点。",
            "answer": "```python\nimport numpy as np\nfrom numpy.fft import fft, ifft, fftfreq\n\n# This program implements a function-space Metropolis-Adjusted Langevin Algorithm (MALA)\n# to solve a Bayesian inverse problem for the diffusion coefficient in a 1D PDE.\n\ndef solve():\n    \"\"\"\n    Main function to run the MALA simulation for specified test cases and print the results.\n    \"\"\"\n    # Problem parameters\n    T = 1e-2\n    SIGMA = 5e-2\n    ALPHA = 1.0\n    KAPPA = 1.0\n    PRIOR_MEAN_VAL = 1.0\n    K_ITER = 10000  # Number of MCMC iterations\n    ETA = 0.04      # MALA step size, constant across all dimensions\n    SEED = 42\n\n    def y0_func(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.cos(6 * np.pi * x)\n\n    def dy0dx_func(x):\n        return 2 * np.pi * np.cos(2 * np.pi * x) - 3 * np.pi * np.sin(6 * np.pi * x)\n\n    def d2y0dx2_func(x):\n        return -4 * np.pi**2 * np.sin(2 * np.pi * x) - 18 * np.pi**2 * np.cos(6 * np.pi * x)\n\n    def get_fourier_multipliers(N, h, alpha, kappa):\n        \"\"\"Computes Fourier multipliers for C, C^-1, C^1/2, C^-1/2.\"\"\"\n        fourier_freqs = 2 * np.pi * fftfreq(N, h)\n        lambda_k = fourier_freqs**2  # Eigenvalues of -Delta\n        mu_k = alpha + lambda_k     # Eigenvalues of (alpha*I - Delta)\n        \n        # Regularize to avoid division by zero, although not expected for alpha > 0\n        mu_k[mu_k == 0] = 1e-16\n\n        C_mult = mu_k**(-kappa)\n        C_inv_mult = mu_k**(kappa)\n        C_half_mult = mu_k**(-kappa / 2.0)\n        C_half_inv_mult = mu_k**(kappa / 2.0)\n        return C_mult, C_inv_mult, C_half_mult, C_half_inv_mult\n\n    def apply_fourier_op(v, multipliers):\n        \"\"\"Applies a Fourier-diagonal operator.\"\"\"\n        return np.real(ifft(multipliers * fft(v)))\n\n    def apply_A(u, T, dy0_dx, d2y0_dx2, h):\n        \"\"\"Applies the forward operator A.\"\"\"\n        # Central difference for du/dx with periodic boundary conditions\n        du_dx = (np.roll(u, -1) - np.roll(u, 1)) / (2 * h)\n        D_u = dy0_dx * du_dx + d2y0_dx2 * u\n        return T * D_u\n\n    def apply_A_star(v, T, dy0_dx, h):\n        \"\"\"Applies the adjoint operator A*.\"\"\"\n        # Central difference for dv/dx with periodic boundary conditions\n        dv_dx = (np.roll(v, -1) - np.roll(v, 1)) / (2 * h)\n        D_star_v = -dy0_dx * dv_dx\n        return T * D_star_v\n\n    def compute_J(u, d, m, T, dy0_dx, d2y0_dx2, h, sigma, C_half_inv_mult):\n        \"\"\"Computes the negative log-posterior J(u).\"\"\"\n        Au_minus_d = apply_A(u, T, dy0_dx, d2y0_dx2, h) - d\n        misfit_term = (1.0 / (2.0 * sigma**2)) * h * np.sum(Au_minus_d**2)\n        \n        u_minus_m = u - m\n        C_half_inv_u_minus_m = apply_fourier_op(u_minus_m, C_half_inv_mult)\n        reg_term = 0.5 * h * np.sum(C_half_inv_u_minus_m**2)\n\n        return misfit_term + reg_term\n\n    def run_mala_for_N(N, K, eta):\n        \"\"\"Runs the MALA sampler for a given grid size N.\"\"\"\n        h = 1.0 / N\n        x_grid = np.arange(N) * h\n        \n        dy0_dx = dy0dx_func(x_grid)\n        d2y0_dx2 = d2y0dx2_func(x_grid)\n        m = np.full(N, PRIOR_MEAN_VAL)\n\n        C_mult, C_inv_mult, C_half_mult, C_half_inv_mult = get_fourier_multipliers(N, h, ALPHA, KAPPA)\n\n        rng = np.random.default_rng(SEED)\n        \n        # Generate synthetic data\n        zeta_true = rng.standard_normal(N)\n        u_true = m + apply_fourier_op(zeta_true, C_half_mult)\n        d_true = apply_A(u_true, T, dy0_dx, d2y0_dx2, h)\n        noise = rng.normal(0, SIGMA, N)\n        d = d_true + noise\n\n        # MALA chain initialization\n        u = np.copy(m)\n        accepted_count = 0\n        J_u = compute_J(u, d, m, T, dy0_dx, d2y0_dx2, h, SIGMA, C_half_inv_mult)\n        \n        for _ in range(K):\n            # 1. Compute gradient of J\n            Au_minus_d = apply_A(u, T, dy0_dx, d2y0_dx2, h) - d\n            grad_misfit = apply_A_star(Au_minus_d / SIGMA**2, T, dy0_dx, h)\n            grad_reg = apply_fourier_op(u - m, C_inv_mult)\n            grad_J = grad_misfit + grad_reg\n\n            # 2. Compute proposal drift term g(u)\n            g_u = -apply_fourier_op(grad_J, C_mult)\n\n            # 3. Generate proposal u'\n            xi = rng.standard_normal(N)\n            noise_term = np.sqrt(2 * eta) * apply_fourier_op(xi, C_half_mult)\n            u_proposal = u + eta * g_u + noise_term\n            \n            # 4. Compute acceptance probability\n            J_u_proposal = compute_J(u_proposal, d, m, T, dy0_dx, d2y0_dx2, h, SIGMA, C_half_inv_mult)\n            \n            # log q(u'|u) term\n            log_q_num_term = u_proposal - u - eta * g_u\n            C_half_inv_log_q_num = apply_fourier_op(log_q_num_term, C_half_inv_mult)\n            log_q_u_prime_given_u = -1.0/(4.0*eta) * h * np.sum(C_half_inv_log_q_num**2)\n            \n            # log q(u|u') term requires g(u')\n            Au_prop_minus_d = apply_A(u_proposal, T, dy0_dx, d2y0_dx2, h) - d\n            grad_misfit_prop = apply_A_star(Au_prop_minus_d / SIGMA**2, T, dy0_dx, h)\n            grad_reg_prop = apply_fourier_op(u_proposal - m, C_inv_mult)\n            grad_J_prop = grad_misfit_prop + grad_reg_prop\n            g_u_prime = -apply_fourier_op(grad_J_prop, C_mult)\n            \n            log_q_den_term = u - u_proposal - eta * g_u_prime\n            C_half_inv_log_q_den = apply_fourier_op(log_q_den_term, C_half_inv_mult)\n            log_q_u_given_u_prime = -1.0/(4.0*eta) * h * np.sum(C_half_inv_log_q_den**2)\n            \n            log_alpha = (J_u - J_u_proposal) + log_q_u_given_u_prime - log_q_u_prime_given_u\n            \n            # 5. Accept/Reject\n            if np.log(rng.uniform(0, 1))  log_alpha:\n                u = u_proposal\n                J_u = J_u_proposal\n                accepted_count += 1\n                \n        return accepted_count / K\n\n    test_cases_N = [32, 64, 128]\n    results = []\n    \n    for N_val in test_cases_N:\n        rate = run_mala_for_N(N_val, K_ITER, ETA)\n        results.append(rate)\n        \n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "本练习将介绍一种前沿的混合采样策略，它专门用于解决数据仅在低维子空间中提供信息的问题。你将构建一个维度无关似然信息（DILI）采样器，它巧妙地结合了在“似然信息子空间”（LIS）上的高效MALA更新与在其高维补空间上的稳健pCN更新。你的目标是通过实验证明，即使问题总维度显著增加，这种先进方法仍能为其在两个子空间上的分量保持稳定的混合性质 。",
            "id": "3376410",
            "problem": "给定一个由数据同化（DA）中的大气反演问题所启发的贝叶斯线性逆问题。未知状态 $u \\in \\mathbb{R}^N$ 是一个随高度变化的离散化垂直剖面。数据模型为\n$$\ny = H u + \\eta,\n$$\n其中 $H \\in \\mathbb{R}^{m \\times N}$ 是一个具有 $m$ 个通道的线性化辐射传输算子，$\\eta \\sim \\mathcal{N}(0, R)$ 是加性观测噪声，其协方差为 $R = \\sigma^2 I_m$，$\\sigma  0$ 为已知值。$u$ 的先验是高斯分布 $\\mathcal{N}(0, C)$，在本练习中，为不失一般性，可以取 $C = I_N$。\n\n似然信息子空间 (Likelihood-Informed Subspace, LIS) 定义为由先验预处理的 Gauss–Newton Hessian 矩阵\n$$\n\\mathcal{S} \\equiv C^{1/2} H^\\top R^{-1} H C^{1/2},\n$$\n的优势特征向量所张成的 $\\mathbb{R}^N$ 的线性子空间，因此 LIS 是 $\\mathcal{S}$ 的值域，其维度是 $\\mathcal{S}$ 的严格正特征值的数量。等价地，如果 $J \\equiv R^{-1/2} H C^{1/2}$，那么 $\\mathcal{S} = J^\\top J$，并且 $\\mathcal{S}$ 的非零特征值与 $J$ 的奇异值的平方相同。在实践中，为了定义一个维度为 $k$ 的数值 LIS，我们保留那些特征值超过给定容差 $\\tau  0$ 的特征模态。\n\n你的任务是：\n\n1) 从上述定义出发（不使用快捷公式），推导 LIS 维度如何随线性化辐射传输算子的秩变化。特别地，证明当 $C = I_N$ 时，有\n$$\n\\dim(\\text{LIS}) \\le \\operatorname{rank}(H) \\le m,\n$$\n并且对于任何容差 $\\tau  0$，数值 LIS 维度 $k(\\tau)$ 是 $H^\\top R^{-1} H$ 超过 $\\tau$ 的特征值的数量。\n\n2) 通过实例化一个由随高度变化的平滑权重函数产生的具体的 $H$ 算子族，为大气反演构建一个 DA 驱动的数值 LIS。考虑一个高度域 $[0, Z_{\\max}]$，其中 $Z_{\\max} = 60$（单位 $\\mathrm{km}$），一个有 $N$ 个点的均匀网格，以及在 $(0,Z_{\\max})$ 内均匀分布的通道中心 $z_j$（$j=1,\\dots,m$）。通过归一化高斯权重离散化来定义 $H$ 的每一行：\n$$\nH_{j,i} \\propto \\exp\\Big(-\\tfrac{1}{2}\\big(\\tfrac{z_i - z_j}{L}\\big)^2\\Big),\n$$\n其中 $L0$ 是一个固定宽度（单位 $\\mathrm{km}$），$z_i$ 是网格点。将每一行进行归一化，使其欧几里得范数为 $1$。使用 $C = I_N$ 和 $R = \\sigma^2 I_m$，其中 $\\sigma$ 已指定。\n\n3) 实现一个维度无关似然信息 (Dimension-Independent Likelihood-Informed, DILI) Metropolis–Hastings 提议，具体如下：令 $\\Phi \\in \\mathbb{R}^{N \\times k}$ 是一个矩阵，其列是 $k$ 个标准正交 LIS 基向量。将 $u$ 分解到 LIS 及其正交补空间中，\n$$\nu = \\Phi a + u_\\perp, \\quad a = \\Phi^\\top u, \\quad u_\\perp = u - \\Phi a,\n$$\n并通过组合以下步骤来提议一个新状态：\n- 对 $k$ 维 LIS 坐标 $a$ 应用步长为 $h0$ 的 Metropolis 调整的 Langevin 算法 (MALA) 步骤，使用限制在 LIS 上的完整后验对数密度的梯度，\n- 对正交补空间分量 $u_\\perp$ 应用参数为 $\\beta \\in (0,1)$ 的预条件克兰克-尼科尔森 (pCN) 步骤。\n\n形式上，设 $c = \\sqrt{1-\\beta^2}$ 和 $\\xi \\sim \\mathcal{N}(0,I)$：\n- LIS 更新：提议\n$$\na' = a + \\tfrac{h}{2}\\,\\nabla_a \\log \\pi(u) + \\sqrt{h}\\,\\xi_k, \\quad \\nabla_a \\log \\pi(u) = \\Phi^\\top \\nabla_u \\log \\pi(u),\n$$\n- 补空间更新：提议\n$$\nu_\\perp' = c\\, u_\\perp + \\beta\\, \\xi_\\perp, \\quad \\xi_\\perp = \\xi - \\Phi(\\Phi^\\top \\xi).\n$$\n重新组合 $u' = \\Phi a' + u_\\perp'$，并使用精确的后验密度和精确的联合提议密度（LIS MALA 高斯分布与补空间 pCN 高斯分布的乘积），根据 Metropolis–Hastings 接受概率来接受或拒绝该提议。\n\n4) 维度无关混合性的验证：对于固定的 $m$ 和固定的 LIS 容差 $\\tau$，经验性地证明当 $N$ 增加时，DILI 采样器的混合性保持稳定。通过报告以下内容来量化这一点：\n- 数值 LIS 维度 $k$,\n- 总体接受率,\n- 与 LIS 对齐的标量泛函（第一个 LIS 坐标 $a_1$）的积分自相关时间,\n- 与补空间对齐的标量泛函（正交补空间中的一个单位向量，通过将标准正态向量投影到补空间上构造）的积分自相关时间。\n\n使用以下参数值测试套件：\n\n- 情况 A (理想情况): $N=64$, $m=12$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, 总迭代次数 $T=3000$, 预烧期 $B=1000$, MALA 步长 $h=0.1$, pCN 参数 $\\beta=0.2$.\n- 情况 B (维度增加): $N=128$, $m=12$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, $T=3000$, $B=1000$, $h=0.1$, $\\beta=0.2$.\n- 情况 C (维度增加): $N=256$, $m=12$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, $T=3000$, $B=1000$, $h=0.1$, $\\beta=0.2$.\n- 情况 D (低秩边界): $N=64$, $m=4$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, $T=3000$, $B=1000$, $h=0.1$, $\\beta=0.2$.\n\n在所有情况下，将合成真值 $u^\\dagger$ 构造为两个随高度局部化的高斯凸起之和，并使用固定的随机种子生成数据 $y = H u^\\dagger + \\eta$，其中 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$。\n\n对于每种情况，你的程序必须计算列表 $[k, \\text{acc}, \\text{iact\\_lis}, \\text{iact\\_perp}]$，其中 $k$ 是一个整数，其余条目是浮点值。你的程序应生成单行输出，其中包含所有情况的结果，格式为一个由逗号分隔的四元素列表组成的列表，并用方括号括起来，例如：\n\"[ [k_A,acc_A,iact_lis_A,iact_perp_A],[k_B,acc_B,iact_lis_B,iact_perp_B],[k_C,acc_C,iact_lis_C,iact_perp_C],[k_D,acc_D,iact_lis_D,iact_perp_D] ]\".",
            "solution": "所提供的问题是计算统计学和逆问题领域一个有效、适定且有科学依据的练习。它要求推导、实现并验证一个用于贝叶斯线性逆问题的维度无关 MCMC 采样器。所有必要的参数和定义都已提供，任务逻辑结构清晰且技术上可行。\n\n### 任务1：LIS 维度的理论推导\n\n我们已知似然信息子空间 (LIS) 是先验预处理的 Gauss-Newton Hessian 矩阵 $\\mathcal{S} \\equiv C^{1/2} H^\\top R^{-1} H C^{1/2}$ 的值域。LIS 的维度是 $\\mathcal{S}$ 的严格正特征值的数量，也等于其秩，即 $\\dim(\\text{LIS}) = \\operatorname{rank}(\\mathcal{S})$。\n\n在特定条件 $C = I_N$ 下，我们被要求证明两个性质。\n\n**第一个性质：** 证明 $\\dim(\\text{LIS}) \\le \\operatorname{rank}(H) \\le m$。\n\n当先验协方差 $C = I_N$ 时，矩阵 $\\mathcal{S}$ 简化为：\n$$\n\\mathcal{S} = I_N^{1/2} H^\\top R^{-1} H I_N^{1/2} = H^\\top R^{-1} H\n$$\nLIS 的维度是这个矩阵的秩：$\\dim(\\text{LIS}) = \\operatorname{rank}(H^\\top R^{-1} H)$。\n\n噪声协方差 $R$ 由 $R = \\sigma^2 I_m$ 给出，其中 $\\sigma  0$。其逆为 $R^{-1} = \\frac{1}{\\sigma^2} I_m$。由于 $\\sigma^2  0$，$R^{-1}$ 是一个大小为 $m \\times m$ 的满秩对角矩阵。矩阵乘积的秩最多是它们各自秩的最小值。因此，$R^{-1}H$ 的秩等于 $H$ 的秩，因为乘以一个满秩方阵不会改变秩。\n$$\n\\operatorname{rank}(R^{-1}H) = \\operatorname{rank}(H)\n$$\n现在我们考虑 $\\mathcal{S} = H^\\top (R^{-1} H)$ 的秩。使用基础线性代数恒等式，即对于任意矩阵 $A$，$\\operatorname{rank}(A^\\top A) = \\operatorname{rank}(A)$，我们可以令 $A = R^{-1/2}H$。那么 $A^\\top A = H^\\top (R^{-1/2})^\\top R^{-1/2} H = H^\\top R^{-1} H = \\mathcal{S}$。\n因此，\n$$\n\\dim(\\text{LIS}) = \\operatorname{rank}(\\mathcal{S}) = \\operatorname{rank}((R^{-1/2}H)^\\top (R^{-1/2}H)) = \\operatorname{rank}(R^{-1/2}H) = \\operatorname{rank}(H)\n$$\n算子 $H$ 是一个大小为 $m \\times N$ 的矩阵。任何矩阵的秩都不能超过其行数或列数。特别地，$\\operatorname{rank}(H) \\le m$。\n综合这些结果，我们已经证明了所求的不等式：\n$$\n\\dim(\\text{LIS}) = \\operatorname{rank}(H) \\le m\n$$\n\n**第二个性质：** 证明数值 LIS 维度 $k(\\tau)$ 是 $H^\\top R^{-1} H$ 超过 $\\tau$ 的特征值的数量。\n\n维度为 $k$ 的数值 LIS 是通过保留 $\\mathcal{S}$ 中相应特征值超过容差 $\\tau  0$ 的特征模态来定义的。\n给定 $C=I_N$，我们有 $\\mathcal{S} = H^\\top R^{-1} H$。\n根据定义，我们记为 $k(\\tau)$ 的数值 LIS 维度是 $\\mathcal{S}$ 的特征值 $\\lambda_i$ 中满足 $\\lambda_i  \\tau$ 的数量。\n因此，$k(\\tau)$ 正是 $H^\\top R^{-1} H$ 的严格大于 $\\tau$ 的特征值的数量。这直接源于所提供的定义。\n\n### 任务2：数值模型的构建\n\n我们为一个离散化的一维大气剖面构建前向算子 $H$。\n高度域为 $[0, Z_{\\max}]$，其中 $Z_{\\max} = 60 \\, \\mathrm{km}$。\n该域被离散化为 $N$ 个网格单元，中心点位于 $z_i = (i-0.5) \\Delta z$，$i=1, \\dots, N$，其中 $\\Delta z = Z_{\\max}/N$。\n测量有 $m$ 个通道，中心点位于 $z_j = j \\Delta z_{ch}$，$j=1, \\dots, m$，其中 $\\Delta z_{ch} = Z_{\\max}/(m+1)$，这使它们均匀分布在开区间 $(0, Z_{\\max})$ 内。\n\n算子 $H$ 的每一行代表一个测量通道对不同高度上大气状态的敏感度。条目 $H_{j,i}$ 将第 $j$ 个通道与第 $i$ 个网格点联系起来。它由一个高斯权重函数定义：\n$$\nH_{j,i}' = \\exp\\Big(-\\frac{1}{2}\\left(\\frac{z_i - z_j}{L}\\right)^2\\Big)\n$$\n其中 $L$ 是一个特征宽度。按照要求，每一行随后被归一化，使其欧几里得范数为 $1$：\n$$\nH_{j,i} = \\frac{H_{j,i}'}{\\sqrt{\\sum_{p=1}^N (H_{j,p}')^2}}\n$$\n贝叶斯模型的其他组成部分是先验协方差 $C = I_N$ 和观测误差协方差 $R = \\sigma^2 I_m$。\n\n### 任务3：DILI Metropolis-Hastings 采样器\n\n目标是从后验分布 $\\pi(u|y) \\propto \\pi(y|u) \\pi_0(u)$ 中进行采样，其中 $\\pi_0(u)$ 是先验，$\\pi(y|u)$ 是似然。\n在给定高斯假设的情况下，后验对数密度为：\n$$\n\\log \\pi(u|y) = -\\frac{1}{2} (y - Hu)^\\top R^{-1} (y - Hu) - \\frac{1}{2} u^\\top C^{-1} u + \\text{constant}\n$$\n代入 $R = \\sigma^2 I_m$ 和 $C = I_N$：\n$$\n\\log \\pi(u|y) = -\\frac{1}{2\\sigma^2} \\|y - Hu\\|_2^2 - \\frac{1}{2} \\|u\\|_2^2 + \\text{constant}\n$$\n对数后验关于 $u$ 的梯度是：\n$$\n\\nabla_u \\log \\pi(u) = \\frac{1}{\\sigma^2} H^\\top (y - Hu) - u\n$$\n\nDILI 采样器对状态 $u$ 在 LIS 及其正交补空间中的分量进行操作。\n1.  **LIS 基**: 计算矩阵 $\\mathcal{M} = H^\\top R^{-1} H = \\frac{1}{\\sigma^2}H^\\top H$。找到其特征值 $\\lambda_i$ 和相应的标准正交特征向量 $v_i$。LIS 基由满足 $\\lambda_i  \\tau$ 的特征向量 $\\{v_i\\}$ 构成。假设有 $k$ 个这样的特征向量。基矩阵为 $\\Phi = [v_1, \\dots, v_k] \\in \\mathbb{R}^{N \\times k}$。\n\n2.  **状态分解**: 任何状态 $u \\in \\mathbb{R}^N$ 都被分解为 $u = \\Phi a + u_\\perp$，其中 $a = \\Phi^\\top u$ 是在 LIS 中的坐标，$u_\\perp = (I - \\Phi\\Phi^\\top)u$ 是在正交补空间中的分量。\n\n3.  **提议生成**: 给定当前状态 $u$，提议一个新状态 $u'$。\n    *   **LIS 分量 (MALA)**: 对 LIS 坐标 $a$ 执行一个 Metropolis 调整的 Langevin 算法步骤。投影梯度为 $\\nabla_a \\log \\pi(u) = \\Phi^\\top \\nabla_u \\log \\pi(u)$。提议 $a'$ 为：\n        $$ a' = a + \\frac{h}{2} \\nabla_a \\log \\pi(u) + \\sqrt{h}\\,\\xi_k, \\quad \\xi_k \\sim \\mathcal{N}(0, I_k) $$\n    *   **补空间分量 (pCN)**: 对 $u_\\perp$ 执行一个预条件克兰克-尼科尔森步骤。此步骤保留了补空间上的先验分布。\n        $$ u_\\perp' = \\sqrt{1-\\beta^2}\\, u_\\perp + \\beta\\, \\xi_\\perp, \\quad \\xi_\\perp = (I-\\Phi\\Phi^\\top)\\xi, \\quad \\xi \\sim \\mathcal{N}(0, I_N) $$\n\n4.  **重新组合**: 提议的状态为 $u' = \\Phi a' + u_\\perp'$。\n\n5.  **接受概率**: 提议的移动以概率 $\\alpha = \\min(1, A)$ 被接受，其中比率 $A$ 由下式给出\n    $$ A = \\frac{\\pi(u'|y) q(u|u')}{\\pi(u|y) q(u'|u)} $$\n    联合提议密度为 $q(u'|u) = q_a(a'|u) q_\\perp(u_\\perp'|u_\\perp)$。接受率的对数为：\n    $$\n    \\log A = (\\log \\pi(u'|y) - \\log \\pi(u|y)) + (\\log q_a(a|u') - \\log q_a(a'|u)) + (\\log q_\\perp(u_\\perp|u'_\\perp) - \\log q_\\perp(u'_\\perp|u_\\perp))\n    $$\n    各项为：\n    *   对数后验比: $(\\log\\pi(u'|y) - \\log\\pi(u|y))$\n    *   MALA 提议比: $-\\frac{1}{2h} \\|a - (a' + \\frac{h}{2}\\nabla_{a'}\\log\\pi(u'))\\|^2 + \\frac{1}{2h} \\|a' - (a + \\frac{h}{2}\\nabla_a\\log\\pi(u))\\|^2$\n    *   pCN 提议比: $-\\frac{1}{2\\beta^2}\\|u_\\perp - \\sqrt{1-\\beta^2}u_\\perp'\\|^2 + \\frac{1}{2\\beta^2}\\|u_\\perp' - \\sqrt{1-\\beta^2}u_\\perp\\|^2$\n    必须为 Metropolis-Hastings 检验计算这个完整表达式。\n\n### 任务4：维度无关混合性的验证\n\n为了验证采样器在状态维度 $N$ 增加时的性能，我们针对指定的测试用例运行模拟。对于每种情况，我们构造一个作为两个高斯凸起之和的合成真值 $u^\\dagger$，生成合成数据 $y = H u^\\dagger + \\eta$，然后运行 DILI-MCMC 采样器。我们在丢弃预烧期后分析生成的链。计算以下指标：\n*   **$k$**: 数值 LIS 的维度，由特征值截断值 $\\tau$ 确定。\n*   **接受率**: 被接受的 MCMC 提议移动的比例。\n*   **积分自相关时间 (IACT)**: 获得一个独立样本所需的样本数量的度量。我们为两个特定的可观测量计算它：\n    1.  **IACT LIS**: 第一个 LIS 坐标的时间序列 $t \\mapsto a_1(t) = (\\Phi^\\top u_t)_1$ 的 IACT。\n    2.  **IACT Perp**: 状态在正交补空间中固定随机向量上的投影的时间序列 $t \\mapsto c(t) = \\hat{v}_\\perp^\\top u_t$ 的 IACT，其中 $\\hat{v}_\\perp$ 是从补空间中抽样的归一化随机向量。\n\n预期对于固定的 $m$、$\\sigma$ 和其他采样器参数，随着 $N$ 的增加，接受率和 IACTs 将保持稳定，从而证明采样器混合的维度无关性质。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef calculate_iact(series):\n    \"\"\"\n    Calculates the Integrated Autocorrelation Time (IACT) of a time series.\n    Uses FFT for efficient computation of the autocorrelation function (ACF).\n    \"\"\"\n    n = len(series)\n    if n  2:\n        return 1.0\n    \n    mean = np.mean(series)\n    series_centered = series - mean\n    \n    # Calculate autocovariance using FFT\n    n_fft = 1  (2 * n - 1).bit_length() # Next power of 2 for 2*n-1\n    fft_series = np.fft.fft(series_centered, n_fft)\n    autocov = np.fft.ifft(fft_series * np.conj(fft_series)).real\n    autocov = autocov[:n]\n\n    # ACF is normalized autocovariance\n    if autocov[0]  1e-10: # Avoid division by zero if variance is tiny\n        return 1.0\n    rho = autocov / autocov[0]\n    \n    # Sum up to the first negative value (a simple but common windowing method)\n    iact_val = 1.0\n    for k in range(1, n):\n        if rho[k]  0:\n            break\n        iact_val += 2.0 * rho[k]\n    \n    return iact_val\n\ndef run_case(params):\n    \"\"\"\n    Runs the full simulation for a single parameter case.\n    \"\"\"\n    N, m, sigma, L, tau, T, B, h, beta = params\n    \n    # 2) Construct the numerical model\n    Z_max = 60.0\n    z_grid = np.linspace(0, Z_max, N, endpoint=False) + Z_max / (2 * N)\n    \n    # Place channel centers uniformly in (0, Z_max)\n    ch_centers = np.linspace(0, Z_max, m + 2)[1:-1]\n    \n    H = np.zeros((m, N))\n    for j in range(m):\n        row = np.exp(-0.5 * ((z_grid - ch_centers[j]) / L)**2)\n        norm = np.linalg.norm(row)\n        if norm > 0:\n            H[j, :] = row / norm\n\n    # Generate synthetic truth and data\n    u_true = 1.0 * np.exp(-0.5 * ((z_grid - 15.0) / 5.0)**2) + \\\n             0.8 * np.exp(-0.5 * ((z_grid - 40.0) / 5.0)**2)\n    \n    R_inv = (1.0 / sigma**2) * np.identity(m)\n    \n    # Use pre-seeded randomness for data generation\n    noise = np.random.normal(0, sigma, m)\n    y = H @ u_true + noise\n\n    # --- LIS computation ---\n    # S = C^(1/2) H^T R^(-1) H C^(1/2) with C=I\n    S_matrix = H.T @ R_inv @ H\n    \n    # Find eigenvalues and eigenvectors\n    # Use eigh for symmetric matrices\n    eigenvalues, eigenvectors = eigh(S_matrix)\n    \n    # Sort in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Determine numerical LIS dimension k\n    k = np.sum(eigenvalues > tau)\n    if k == 0:\n        # Handle the case of no dominant eigenvalues to avoid errors\n        Phi = np.zeros((N, 1))\n    else:\n        Phi = eigenvectors[:, :k]\n\n    # --- DILI-MCMC Sampler ---\n    \n    # Posterior log-density and its gradient\n    def log_post(u):\n        residual = y - H @ u\n        log_likelihood = -0.5 * (1/sigma**2) * np.dot(residual, residual)\n        log_prior = -0.5 * np.dot(u, u)\n        return log_likelihood + log_prior\n\n    def grad_log_post(u):\n        return (1/sigma**2) * H.T @ (y - H @ u) - u\n\n    # MCMC initialization\n    u = np.random.randn(N) # Start from a random sample from the prior\n    chain = np.zeros((T, N))\n    accept_count = 0\n    c_beta = np.sqrt(1 - beta**2)\n\n    for i in range(T):\n        # 1. Decompose current state\n        a = Phi.T @ u\n        u_perp = u - Phi @ a\n        \n        # 2. Propose new state\n        # LIS proposal (MALA)\n        grad_u = grad_log_post(u)\n        grad_a = Phi.T @ grad_u\n        xi_k = np.random.randn(k)\n        if k > 0:\n            a_prime = a + (h / 2.0) * grad_a + np.sqrt(h) * xi_k\n        else:\n            a_prime = a\n\n        # Complement proposal (pCN)\n        xi_N = np.random.randn(N)\n        xi_perp = xi_N - Phi @ (Phi.T @ xi_N)\n        u_perp_prime = c_beta * u_perp + beta * xi_perp\n        \n        # 3. Recombine\n        u_prime = Phi @ a_prime + u_perp_prime\n\n        # 4. Acceptance probability\n        log_post_u = log_post(u)\n        log_post_u_prime = log_post(u_prime)\n\n        # MALA proposal log-ratio term\n        grad_u_prime = grad_log_post(u_prime)\n        grad_a_prime = Phi.T @ grad_u_prime\n        \n        log_q_ratio_mala = 0.0\n        if k > 0:\n            rev_term = a - (a_prime + (h/2.0) * grad_a_prime)\n            fwd_term = a_prime - (a + (h/2.0) * grad_a)\n            log_q_ratio_mala = (-0.5/h) * (np.dot(rev_term, rev_term) - np.dot(fwd_term, fwd_term))\n\n        # pCN proposal log-ratio term\n        rev_term_pcn = u_perp - c_beta * u_perp_prime\n        fwd_term_pcn = u_perp_prime - c_beta * u_perp\n        log_q_ratio_pcn = (-0.5/beta**2) * (np.dot(rev_term_pcn, rev_term_pcn) - np.dot(fwd_term_pcn, fwd_term_pcn))\n\n        log_A = (log_post_u_prime - log_post_u) + log_q_ratio_mala + log_q_ratio_pcn\n        \n        if np.log(np.random.rand())  log_A:\n            u = u_prime\n            accept_count += 1\n            \n        chain[i, :] = u\n\n    # --- Analysis ---\n    chain = chain[B:] # Discard burn-in\n\n    # Acceptance rate\n    acc_rate = accept_count / T\n\n    # IACT for LIS component\n    if k > 0:\n        a1_series = chain @ Phi[:, 0]\n        iact_lis = calculate_iact(a1_series)\n    else:\n        iact_lis = 1.0 # Or some other placeholder if LIS is empty\n    \n    # IACT for complement component\n    v_rand = np.random.randn(N)\n    v_perp = v_rand - Phi @ (Phi.T @ v_rand)\n    v_perp_norm = np.linalg.norm(v_perp)\n    if v_perp_norm > 1e-9:\n        v_perp_unit = v_perp / v_perp_norm\n        perp_series = chain @ v_perp_unit\n        iact_perp = calculate_iact(perp_series)\n    else:\n        iact_perp = 1.0 # No component in perp direction\n        \n    return [k, acc_rate, iact_lis, iact_perp]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Fix the seed for reproducibility of the entire process\n    np.random.seed(42)\n\n    # Define test cases from the problem statement.\n    # (N, m, sigma, L, tau, T, B, h, beta)\n    test_cases = [\n        (64, 12, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2), # Case A\n        (128, 12, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2), # Case B\n        (256, 12, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2), # Case C\n        (64, 4, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2),  # Case D\n    ]\n\n    results = []\n    for params in test_cases:\n        # We need to re-seed before data generation and MCMC start for each case\n        # to make them independent experiments on the same footing.\n        # But the problem asks for stability, which is often tested by\n        # letting the PRNG state evolve. We will go with a single seed at the top\n        # as it is a common setup for such scripts.\n        result_list = run_case(params)\n        results.append(result_list)\n    \n    # Format the final output string as specified: \"[[k,val,val,val],[...]]\"\n    # Using str().replace() is a robust way to get the exact format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}