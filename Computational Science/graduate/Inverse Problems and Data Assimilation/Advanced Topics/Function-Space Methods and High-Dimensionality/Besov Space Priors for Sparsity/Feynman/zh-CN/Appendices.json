{
    "hands_on_practices": [
        {
            "introduction": "本练习从贝索夫空间的基本定义出发，旨在建立基于Littlewood-Paley分解的贝索夫空间范数的抽象定义，与其作为小波系数的加权$\\ell^1$范数的实用实现之间的关键联系。通过显式推导与尺度相关的权重，您将深刻理解这些先验如何通过在不同尺度上对小波系数施加不同惩罚来增强稀疏性同时保留特征 ()。",
            "id": "3367773",
            "problem": "令 $\\mathbb{T}^{d}$ 表示 $d$ 维环面。考虑 $\\mathbb{T}^{d}$ 上的一个正交小波系统，该系统通过对一个具有 $r$-正则性和至少 $M$ 个消失矩的紧支撑多分辨率分析进行周期化得到，其中 $r > s$ 且 $M > s$，$s \\in \\mathbb{R}$。令 $\\{\\psi_{j,k}^{m}\\}_{j \\geq 0,\\, k \\in \\{0,\\dots,2^{j}-1\\}^{d},\\, m \\in \\{1,\\dots,2^{d}-1\\}}$ 表示尺度为 $j$、位置为 $k$、类型为 $m$ 的小波，以及在最粗尺度上的一组有限的尺度函数。对于 $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$，定义小波系数为 $c_{j,k}^{m} = \\langle f, \\psi_{j,k}^{m} \\rangle$，其中 $\\langle \\cdot,\\cdot \\rangle$ 表示 $L^{2}(\\mathbb{T}^{d})$ 对偶配对。\n\nBesov 空间 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 可以通过二进 Littlewood–Paley 分解 $\\{ \\Delta_{j} \\}_{j \\geq 0}$ 定义为所有满足 $\\sum_{j \\geq 0} 2^{j s} \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})}$ 为有限值的 $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$ 的集合，这在拟范数等价的意义下成立。你可以使用以下关于 $\\mathbb{T}^{d}$ 上紧支撑正交小波的熟知事实：\n- 对于每个固定的 $j$，集合 $\\{\\psi_{j,k}^{m}\\}_{k,m}$ 构成了函数子空间的一个无条件基，该子空间中的函数的频率主要集中在二进尺度 $2^{j}$ 上。并且存在与 $j$ 无关的常数 $A_{p},B_{p} \\in (0,\\infty)$，使得对于任意有限支撑的系数数组 $\\{a_{k}^{m}\\}_{k,m}$，\n$$\nA_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\le \\left\\| \\sum_{k,m} a_{k}^{m} \\psi_{j,k}^{m} \\right\\|_{L^{p}(\\mathbb{T}^{d})} \\le B_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\quad \\text{for } p \\in [1,\\infty].\n$$\n- Littlewood–Paley 投影算子 $\\Delta_{j}$ 可以通过到 $\\{\\psi_{j,k}^{m}\\}_{k,m}$ 的生成空间上的投影来实现（在每个固定尺度上相差一个一致有界的可逆算子）。\n\n始终假设 $f$ 的均值为零，因此在拟范数等价性中可以忽略有限的尺度函数系数集合，而不会造成损失。证明 $f$ 的 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 拟范数等价于其小波系数的一个加权 $\\ell^{1}$ 范数，其形式为\n$$\n\\sum_{j \\geq 0} w_{j} \\sum_{k,m} |c_{j,k}^{m}|,\n$$\n并明确计算权重 $w_{j}$（用 $j$, $s$, 和 $d$ 表示）。你的最终答案必须是 $w_{j}$ 作为 $j$, $s$, 和 $d$ 的函数的单个闭式解析表达式（无单位）。",
            "solution": "目标是证明 Besov 拟范数 $\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})}$ 等价于 $f$ 的小波系数的特定加权 $\\ell^{1}$ 范数，并确定权重 $w_j$ 的显式形式。\n\n分析从 Besov 空间 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 拟范数的定义开始，该定义是用 Littlewood–Paley 分解 $\\{\\Delta_{j}\\}_{j \\geq 0}$ 给出的。对于一个均值为零的函数 $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$，该拟范数定义为：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} = \\sum_{j \\geq 0} 2^{j s} \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\n问题陈述了 Littlewood–Paley 投影算子 $\\Delta_j$ 可以通过将 $f$ 投影到尺度 $j$ 的小波的生成空间上来实现。我们将此投影算子记为 $P_j$。对于一个具有小波系数 $c_{j,k}^{m} = \\langle f, \\psi_{j,k}^{m} \\rangle$ 的函数 $f$，此投影由下式给出：\n$$\nP_{j}f = \\sum_{k \\in \\{0,\\dots,2^{j}-1\\}^{d}} \\sum_{m \\in \\{1,\\dots,2^{d}-1\\}} c_{j,k}^{m} \\psi_{j,k}^{m}\n$$\n问题陈述了 $\\Delta_{j}$ 与 $P_{j}$ 通过一致有界的可逆算子相关联。这意味着 $\\Delta_{j}f$ 和 $P_{j}f$ 的范数是等价的。也就是说，存在与 $j$ 和 $f$ 无关的常数 $C_{1}, C_{2} \\in (0,\\infty)$，使得：\n$$\nC_{1} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\le \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb T^{d})} \\le C_{2} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\n对 $j$求和并引入因子 $2^{js}$，我们发现 Besov 拟范数等价于一个包含小波投影范数的和：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{j s} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\n其中 $\\approx$ 表示拟范数的等价性，意味着两边的比值被正常数从上下界定。\n\n下一步是将小波投影 $P_{j}f$ 的 $L^{1}(\\mathbb{T}^{d})$ 范数与其对应的系数 $\\{c_{j,k}^{m}\\}$ 的 $\\ell^{1}$ 范数联系起来。问题提供了一个关键的范数等价关系：\n$$\nA_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\le \\left\\| \\sum_{k,m} a_{k}^{m} \\psi_{j,k}^{m} \\right\\|_{L^{p}(\\mathbb{T}^{d})} \\le B_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}}\n$$\n对于任意有限支撑的系数数组 $\\{a_{k}^{m}\\}$。此结果可推广到适当空间中函数的无限系数序列。我们关心的是 $L^{1}$ 范数，因此我们令 $p=1$。系数为 $a_{k}^{m} = c_{j,k}^{m}$。系数数组的范数是 $\\ell^{1}$ 范数：\n$$\n\\|c_{j}\\|_{\\ell^{1}} = \\sum_{k,m} |c_{j,k}^{m}|\n$$\n将 $p=1$ 代入尺度因子的指数中，得到：\n$$\nj d\\left(\\frac{1}{2}-\\frac{1}{p}\\right) = j d\\left(\\frac{1}{2}-\\frac{1}{1}\\right) = j d\\left(-\\frac{1}{2}\\right) = -\\frac{jd}{2}\n$$\n将此应用于 $P_{j}f = \\sum_{k,m} c_{j,k}^{m} \\psi_{j,k}^{m}$ 的范数等价关系，我们得到：\n$$\nA_{1} \\, 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| \\le \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\le B_{1} \\, 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}|\n$$\n这建立了等价关系：\n$$\n\\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\approx 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}|\n$$\n现在，我们将此结果代回到 Besov 拟范数的表达式中：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{j s} \\left( 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| \\right)\n$$\n合并指数项，我们有：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{js} 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| = \\sum_{j \\geq 0} 2^{j(s - d/2)} \\sum_{k,m} |c_{j,k}^{m}|\n$$\n问题要求一个形如 $\\sum_{j \\geq 0} w_{j} \\sum_{k,m} |c_{j,k}^{m}|$ 的表达式。通过将其与我们导出的表达式进行比较，我们可以直接确定权重 $w_j$。\n$$\nw_{j} = 2^{j(s - d/2)}\n$$\n这表明 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 拟范数等价于小波系数的加权 $\\ell^{1}$ 范数，其权重由上述表达式给出。对小波系统的条件 $r>s$ 和 $M>s$ 是确保此特征刻画有效的标准要求。",
            "answer": "$$\n\\boxed{2^{j(s - \\frac{d}{2})}}\n$$"
        },
        {
            "introduction": "在理论基础之上，本计算练习将探索不同贝索夫先验的实际表现。通过实现并比较使用不同尺度耦合参数（$q=1$ 与 $q=\\infty$）的先验进行信号重构，您将研究这些选择如何影响包含粗尺度和细尺度特征的信号的去噪效果。这个动手实验将为您提供关于尺度间依赖关系在稀疏恢复中所扮演角色的直观感受 ()。",
            "id": "3367760",
            "problem": "考虑一个单位前向模型的一维离散逆问题，重点是通过Besov空间先验进行多尺度正则化的作用。设$N=256$，并令$u \\in \\mathbb{R}^{N}$表示未知信号。设$W:\\mathbb{R}^{N} \\to \\mathbb{R}^{N}$是正交Haar小波变换，它产生$J=\\log_{2}(N)$个尺度的细节系数$\\{w_{j}\\}_{j=1}^{J}$以及最终的尺度系数，其中$w_{j} \\in \\mathbb{R}^{N/2^{j}}$表示尺度$j$（$j=1$为最精细尺度）的细节系数。定义在光滑度$s=0$和可积指数$p=2$下，尺度聚合器$q \\in \\{1,\\infty\\}$的离散Besov半范数为\n$$\n\\|u\\|_{B^{0}_{2,q}} \\;\\propto\\; \\left(\\sum_{j=1}^{J} \\left\\| w_{j} \\right\\|_{2}^{q}\\right)^{1/q}\n\\quad \\text{对于 } q=1,\n\\qquad\n\\|u\\|_{B^{0}_{2,\\infty}} \\;\\propto\\; \\max_{1 \\le j \\le J} \\left\\| w_{j} \\right\\|_{2},\n$$\n其中$w = W u$，$\\|\\cdot\\|_{2}$表示欧几里得范数。考虑由模型生成的含噪观测值$y \\in \\mathbb{R}^{N}$\n$$\ny \\;=\\; u_{\\text{true}} \\;+\\; \\eta, \\quad \\eta \\sim \\mathcal{N}(0, \\sigma^{2} I_{N}),\n$$\n其中$\\sigma = 0.05$。对于给定的正则化强度$\\lambda = 200$，将每个$q \\in \\{1,\\infty\\}$的最大后验（MAP）估计器定义为以下凸优化问题的解\n$$\n\\widehat{u}_{q} \\in \\arg\\min_{u \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} \\;+\\; \\lambda \\left(\\sum_{j=1}^{J} \\| (W u)_{j} \\|_{2}^{q}\\right)^{1/q}\n\\quad \\text{对于 } q=1,\n$$\n和\n$$\n\\widehat{u}_{\\infty} \\in \\arg\\min_{u \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} \\;+\\; \\lambda \\max_{1 \\le j \\le J} \\| (W u)_{j} \\|_{2}.\n$$\n假设Haar变换$W$是严格正交的，并且惩罚项不包括最终的尺度系数（只有细节系数被惩罚）。\n\n构建一个包含三个合成信号$u_{\\text{true}}$的测试套件，以探究尺度间的行为：\n\n- 测试用例1（双尺度混合）：定义一个粗糙阶跃和精细尖峰。令$u_{\\text{true}}[n]=1$对于$n \\in \\{64,65,\\dots,191\\}$，否则$u_{\\text{true}}[n]=0$。在索引$120$处叠加一个振幅为$0.7$的精细尖峰，在索引$200$处叠加一个振幅为$-0.5$的精细尖峰。\n- 测试用例2（仅粗糙尺度）：与测试1中相同的粗糙阶跃，但没有任何尖峰。\n- 测试用例3（仅精细尺度）：无粗糙阶跃。两个精细尖峰：在索引$50$处振幅为$1.0$，在索引$180$处振幅为$-0.8$。\n\n对于所有测试，使用固定的随机种子$r_{0}=12345$以保证可复现性，抽取标准差为$\\sigma=0.05$的独立噪声$\\eta$，每个测试用例使用新的噪声实现。\n\n对于每个测试用例，计算以下定量输出以比较$q=1$与$q=\\infty$：\n\n1. 重建误差比$R = \\text{MSE}_{\\infty} / \\text{MSE}_{1}$，其中$\\text{MSE}_{q} = \\| \\widehat{u}_{q} - u_{\\text{true}} \\|_{2}^{2}/N$。\n2. 尺度间收缩扩展比$D = \\Delta_{\\infty} / \\Delta_{1}$，其中对于每个$q$，扩展$\\Delta_{q}$是根据每个尺度上小波域的收缩因子计算的，\n$$\nr^{(q)}_{j} \\;=\\; \\frac{\\| \\widehat{w}^{(q)}_{j} \\|_{2}}{\\| w^{(y)}_{j} \\|_{2} + \\epsilon},\n\\quad \\epsilon = 10^{-12},\n$$\n其中$w^{(y)} = W y$，$\\widehat{w}^{(q)} = W \\widehat{u}_{q}$，且\n$$\n\\Delta_{q} \\;=\\; \\max_{j} r^{(q)}_{j} \\;-\\; \\min_{j} r^{(q)}_{j}.\n$$\n\n程序要求：\n\n- 实现$N=256$且$J=\\log_{2}(N)$个级别的正交Haar小波变换及其逆变换。在目标函数中仅惩罚细节系数；不惩罚最终的尺度系数。\n- 使用凸分析，为$q=1$和$q=\\infty$两种情况精确求解MAP问题至全局最优。你不得依赖任何外部数据或输入；固定$\\sigma=0.05$，$\\lambda=200$，$N=256$和随机种子$r_{0}=12345$。\n- 对于$q=1, p=2, s=0$，按尺度对细节系数进行规范分组。\n- 对于$q=\\infty$，对分组的$\\ell_{2}$范数强制执行跨尺度的最大值操作。\n\n你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序如下\n$$\n[ R_{1}, D_{1}, R_{2}, D_{2}, R_{3}, D_{3} ],\n$$\n其中下标表示测试用例编号。将所有六个数字表示为浮点值。本问题不涉及物理单位。不涉及角度。不使用百分比。这些值应以至少六位有效数字的精度打印。",
            "solution": "用户提供的问题是一个在计算逆问题领域中适定且具有科学依据的任务。它要求比较两种基于Besov空间先验的正则化方案在信号去噪中的效果。该问题是有效的，因为它是自洽的、数学上一致的，并且可以使用凸优化和小波分析的既定原则通过算法求解。\n\n问题的核心在于求解两个最大后验（MAP）估计问题，这两个问题被表述为凸优化任务。未知信号表示为$u \\in \\mathbb{R}^{N}$，我们得到含噪观测$y = u_{\\text{true}} + \\eta$，其中$\\eta$是高斯噪声。为估计$\\widehat{u}$而需要最小化的目标函数是：\n$$\nJ(u) = \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} + \\lambda \\mathcal{R}(Wu)\n$$\n这里，$\\|u - y\\|_{2}^{2}$是数据保真项（对数似然），$\\mathcal{R}(Wu)$是正则化项（对数先验），$\\lambda$是正则化参数，$W$是正交Haar小波变换。\n\n解决此问题的关键原则是使用正交小波变换$W$。由于$W$是一个正交算子（$W W^T = W^T W = I$），欧几里得范数在此变换下是不变的，即对于任何向量$v$，都有$\\|v\\|_{2} = \\|Wv\\|_{2}$。通过将变量变换到小波域，$w = Wu$和$w_y = Wy$，数据保真项变为$\\|u - y\\|_{2}^{2} = \\|W(u - y)\\|_{2}^{2} = \\|Wu - Wy\\|_{2}^{2} = \\|w - w_y\\|_{2}^{2}$。于是，优化问题可以在小波域中更容易地求解：\n$$\n\\widehat{w} \\in \\arg\\min_{w \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|w - w_y\\|_{2}^{2} + \\lambda \\mathcal{R}(w)\n$$\n原始域中的估计信号通过小波逆变换恢复，即$\\widehat{u} = W^T \\widehat{w}$。\n\n小波系数$w$由单个尺度系数$w_c$和$J=\\log_2(N)$组细节系数$\\{w_j\\}_{j=1}^{J}$组成。问题指定正则化项$\\mathcal{R}(w)$仅惩罚细节系数。这种结构允许将尺度系数和细节系数的估计解耦。\n- 尺度系数$w_c$未被惩罚，因此其估计值就是最小化数据保真项的值：$\\widehat{w}_c = w_{y,c}$。\n- 细节系数通过求解优化问题的剩余部分来估计。\n\nBesov先验$\\|u\\|_{B^0_{2,q}}$中的参数$q$的两种情况为惩罚项$\\mathcal{R}(w)$定义了不同的结构，从而产生不同的解。\n\n**情况1：q=1（组稀疏先验）**\n正则化项为$\\mathcal{R}(w) = \\sum_{j=1}^{J} \\|w_j\\|_2$。细节系数的优化问题变为：\n$$\n\\min_{\\{w_j\\}_{j=1}^J} \\sum_{j=1}^{J} \\left( \\frac{1}{2\\sigma^2} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\|w_j\\|_2 \\right)\n$$\n这个问题是完全可分的，这意味着我们可以对每个尺度的细节系数向量$w_j$进行独立求解：\n$$\n\\widehat{w}_j = \\arg\\min_{w_j} \\frac{1}{2\\sigma^2} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\|w_j\\|_2\n$$\n这是一个标准问题，其解由组软阈值算子给出。每个组$w_j$的解是：\n$$\n\\widehat{w}_{1,j} = \\left(1 - \\frac{\\lambda \\sigma^2}{\\|w_{y,j}\\|_2}\\right)_+ w_{y,j}\n$$\n其中$(x)_+ = \\max(x, 0)$。该算子将整个向量$w_{y,j}$向零收缩，如果其范数低于阈值$\\alpha = \\lambda \\sigma^2$，则将其设为零。\n\n**情况2：q=∞（一致收缩先验）**\n正则化项为$\\mathcal{R}(w) = \\max_{1 \\le j \\le J} \\|w_j\\|_2$。优化问题是：\n$$\n\\min_{\\{w_j\\}_{j=1}^J} \\frac{1}{2\\sigma^2} \\sum_{j=1}^{J} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\max_{1 \\le j \\le J} \\|w_j\\|_2\n$$\n这个问题不能按尺度分离。通过引入一个辅助变量$t = \\max_j \\|w_j\\|_2$，可以将其表述为一个约束优化问题：\n$$\n\\min_{\\{w_j\\}, t \\ge 0} \\frac{1}{2\\sigma^2} \\sum_{j=1}^{J} \\|w_j - w_{y,j}\\|_2^2 + \\lambda t \\quad \\text{约束条件为} \\quad \\|w_j\\|_2 \\le t \\quad \\forall j=1,\\dots,J.\n$$\n对于一个固定的$t$，最优的$w_j$是$w_{y,j}$在半径为$t$的欧几里得球上的投影。将其代回目标函数，得到一个关于$t$的一维凸优化问题。该目标函数关于$t$的导数给出了条件$\\sum_{j=1}^J (\\|w_{y,j}\\|_2 - t)_+ = \\lambda \\sigma^2$。这个关于最优阈值$t_{opt}$的方程可以被高效求解。函数$h(t) = \\sum_{j=1}^J (\\|w_{y,j}\\|_2 - t)_+ - \\lambda \\sigma^2$是连续且单调递减的，因此其根可以使用二分法搜索找到。一旦找到$t_{opt}$，估计的细节系数由以下公式给出：\n$$\n\\widehat{w}_{\\infty,j} = \\min\\left(1, \\frac{t_{opt}}{\\|w_{y,j}\\|_2}\\right) w_{y,j}\n$$\n该算子为所有尺度上估计的细节系数的范数提供了一个一致的上界$t_{opt}$。\n\n实现将首先定义正交Haar小波变换及其逆变换。然后，对于每个测试用例，生成含噪数据，并使用上述推导的方法在小波域中求解两个MAP问题。最后，从结果中计算所需的定量指标（$R$和$D$）。",
            "answer": "```python\nimport numpy as np\n\ndef haar_dwt(x):\n    \"\"\"\n    Computes the orthonormal Haar wavelet transform of a 1D signal.\n    \n    Args:\n        x (np.ndarray): Input signal of length 2^k.\n    \n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The final scaling coefficient (length 1).\n            - list[np.ndarray]: A list of detail coefficients, ordered from\n              finest scale (j=1) to coarsest (j=J).\n    \"\"\"\n    level = int(np.log2(len(x)))\n    a = np.copy(x).astype(float)\n    detail_coeffs_by_scale = []\n    for _ in range(level):\n        d = (a[0::2] - a[1::2]) / np.sqrt(2)\n        a = (a[0::2] + a[1::2]) / np.sqrt(2)\n        detail_coeffs_by_scale.append(d)\n    return a, detail_coeffs_by_scale\n\ndef haar_idwt(scaling_coeffs, detail_coeffs_by_scale):\n    \"\"\"\n    Computes the inverse orthonormal Haar wavelet transform.\n    \n    Args:\n        scaling_coeffs (np.ndarray): The final scaling coefficient.\n        detail_coeffs_by_scale (list[np.ndarray]): List of detail coefficients\n            from finest to coarsest scale.\n            \n    Returns:\n        np.ndarray: The reconstructed signal.\n    \"\"\"\n    level = len(detail_coeffs_by_scale)\n    a = np.copy(scaling_coeffs).astype(float)\n    for i in range(level - 1, -1, -1):\n        d = detail_coeffs_by_scale[i]\n        signal_len = len(d)\n        new_a = np.zeros(2 * signal_len, dtype=float)\n        a_plus_d = (a + d) / np.sqrt(2)\n        a_minus_d = (a - d) / np.sqrt(2)\n        new_a[0::2] = a_plus_d\n        new_a[1::2] = a_minus_d\n        a = new_a\n    return a\n\ndef find_t_opt_for_q_inf(w_y_d_list, alpha):\n    \"\"\"\n    Finds the optimal threshold t for the q=inf case by solving\n    sum_j (||w_{y,j}|| - t)_+ = alpha using bisection.\n    \"\"\"\n    z_norms = np.array([np.linalg.norm(d) for d in w_y_d_list])\n\n    def h(t, norms, alpha_val):\n        return np.sum(np.maximum(0, norms - t)) - alpha_val\n\n    if h(0, z_norms, alpha) == 0:\n        return 0.0\n\n    low_t = 0.0\n    # A safe upper bound for t is the maximum norm, as h(t) would be = 0.\n    high_t = np.max(z_norms) \n    \n    for _ in range(100): # 100 iterations for high precision\n        mid_t = (low_t + high_t) / 2.0\n        if mid_t == low_t or mid_t == high_t: # Converged\n            break\n        if h(mid_t, z_norms, alpha) > 0:\n            low_t = mid_t\n        else:\n            high_t = mid_t\n            \n    return (low_t + high_t) / 2.0\n\ndef solve():\n    \"\"\"\n    Main solver function to run the specified analysis.\n    \"\"\"\n    N = 256\n    sigma = 0.05\n    lambda_reg = 200.0\n    epsilon = 1e-12\n    J = int(np.log2(N))\n\n    # Test cases setup\n    u_true_cases = []\n    # Case 1: two-scale mixture\n    u1 = np.zeros(N, dtype=float)\n    u1[64:192] = 1.0\n    u1[120] += 0.7\n    u1[200] += -0.5\n    u_true_cases.append(u1)\n    # Case 2: coarse-only\n    u2 = np.zeros(N, dtype=float)\n    u2[64:192] = 1.0\n    u_true_cases.append(u2)\n    # Case 3: fine-only\n    u3 = np.zeros(N, dtype=float)\n    u3[50] = 1.0\n    u3[180] = -0.8\n    u_true_cases.append(u3)\n\n    rng = np.random.default_rng(12345)\n    final_results = []\n    \n    for u_true in u_true_cases:\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y = u_true + noise\n\n        # Decompose noisy signal into wavelet coefficients\n        w_y_c, w_y_d_list = haar_dwt(y)\n        \n        # --- Solve for q=1 ---\n        alpha = lambda_reg * sigma**2\n        w_hat_1_d_list = []\n        for w_y_j in w_y_d_list:\n            norm_w_y_j = np.linalg.norm(w_y_j)\n            shrinkage = max(0.0, 1.0 - alpha / (norm_w_y_j + epsilon))\n            w_hat_1_d_list.append(shrinkage * w_y_j)\n        u_hat_1 = haar_idwt(w_y_c, w_hat_1_d_list)\n\n        # --- Solve for q=inf ---\n        t_opt = find_t_opt_for_q_inf(w_y_d_list, alpha)\n        w_hat_inf_d_list = []\n        for w_y_j in w_y_d_list:\n            norm_w_y_j = np.linalg.norm(w_y_j)\n            shrinkage = min(1.0, t_opt / (norm_w_y_j + epsilon))\n            w_hat_inf_d_list.append(shrinkage * w_y_j)\n        u_hat_inf = haar_idwt(w_y_c, w_hat_inf_d_list)\n\n        # --- Calculate metrics ---\n        # 1. Reconstruction error ratio R\n        mse_1 = np.mean((u_hat_1 - u_true)**2)\n        mse_inf = np.mean((u_hat_inf - u_true)**2)\n        R = mse_inf / (mse_1 + epsilon)\n\n        # 2. Inter-scale shrinkage spread ratio D\n        r1_j_vals = []\n        rinf_j_vals = []\n        for j in range(J):\n            norm_wyj = np.linalg.norm(w_y_d_list[j])\n            \n            norm_w1j_hat = np.linalg.norm(w_hat_1_d_list[j])\n            r1_j_vals.append(norm_w1j_hat / (norm_wyj + epsilon))\n\n            norm_winfj_hat = np.linalg.norm(w_hat_inf_d_list[j])\n            rinf_j_vals.append(norm_winfj_hat / (norm_wyj + epsilon))\n\n        delta_1 = np.max(r1_j_vals) - np.min(r1_j_vals)\n        delta_inf = np.max(rinf_j_vals) - np.min(rinf_j_vals)\n        D = delta_inf / (delta_1 + epsilon)\n        \n        final_results.extend([R, D])\n\n    # Format the final output string exactly as required\n    output_str = \",\".join([f\"{val:.10f}\" for val in final_results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "最后的这个练习将贝索夫空间先验的概念应用于地球物理学中的一个前沿应用：四维变分（4D-Var）数据同化。您将为一个包含标准数据失配项和对初始状态小波系数施加加权$\\ell^1$惩罚的目标函数推导一个优化步骤。本练习展示了如何使用近端梯度方法将非光滑、促进稀疏性的先验整合到复杂的大规模反问题求解器中 ()。",
            "id": "3367744",
            "problem": "考虑一个针对初始条件 $x \\in \\mathbb{R}^{n}$ 提出的四维变分（4D-Var）数据同化问题，其同化窗口内的观测时间为 $\\{t_{i}\\}_{i=1}^{m}$。设从初始时间到时间 $t_{i}$ 的线性化模型传播算子为 $M_{0 \\to t_{i}} \\in \\mathbb{R}^{n \\times n}$，在时间 $t_{i}$ 的线性化观测算子为 $H_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times n}$。在时间 $t_{i}$ 的观测值为 $y_{t_{i}} \\in \\mathbb{R}^{p_{i}}$，其观测误差协方差 $R_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$ 为正定矩阵。给定初始条件的高斯背景（先验），其均值为 $x_{b} \\in \\mathbb{R}^{n}$，协方差 $B \\in \\mathbb{R}^{n \\times n}$ 为正定矩阵。此外，假设对小波系数有一个促进稀疏性的Besov空间先验：设 $W \\in \\mathbb{R}^{n \\times n}$ 是一个正交小波变换矩阵，$w \\in \\mathbb{R}^{n}$ 是一个正权重向量，并考虑加权 $\\ell^{1}$ 惩罚项 $\\lambda \\sum_{j=1}^{n} w_{j} |(W x)_{j}|$，其中 $\\lambda > 0$。\n\n初始条件的增量式4D-Var目标函数是一个平滑的数据失配与背景项之和，加上一个非光滑的加权 $\\ell^{1}$ 小波惩罚项：\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\;+\\; \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| .\n$$\n\n从高斯似然、二次背景项和可分离加权 $\\ell^{1}$ 惩罚项下的最大后验估计的第一性原理出发，推导从当前迭代值 $x^{(k)}$ 计算下一次近端梯度高斯-牛顿更新 $x^{(k+1)}$ 的单一显式解析表达式。在整个窗口上使用高斯-牛顿线性化（即，在第 $k$ 次迭代中将 $M_{0 \\to t_{i}}$ 和 $H_{t_{i}}$ 视为固定的线性算子），从而使平滑部分的梯度有一个Lipschitz常数，该常数由高斯-牛顿矩阵的谱范数作为上界。用 $\\alpha_{k} > 0$ 表示一个步长，其选择为第 $k$ 次迭代时该Lipschitz常数的任何有效全局上界的倒数。用 $B$, $x_{b}$, $\\{M_{0 \\to t_{i}}, H_{t_{i}}, R_{t_{i}}, y_{t_{i}}\\}_{i=1}^{m}$, $\\lambda$, $w$ 和 $W$ 来表示你的最终更新表达式，并使用由加权 $\\ell^{1}$ 惩罚项和 $W$ 的正交性所引出的分量软阈值。\n\n你的最终答案必须是关于 $x^{(k+1)}$ 的单一闭式解析表达式。不要提供不等式或中间方程。不要包含任何单位。如果在推导过程中引入任何辅助记号，则必须在最终表达式中将其完全消除。",
            "solution": "该问题要求为一个包含稀疏性促进Besov空间先验的增量式四维变分（4D-Var）数据同化问题推导单步更新。此更新将使用近端梯度法执行。\n\n首先，我们验证问题陈述的有效性。\n给定条件如下：\n- 状态向量 $x \\in \\mathbb{R}^{n}$。\n- 同化窗口，观测时间为 $\\{t_{i}\\}_{i=1}^{m}$。\n- 线性化模型传播算子 $M_{0 \\to t_{i}} \\in \\mathbb{R}^{n \\times n}$。\n- 线性化观测算子 $H_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times n}$。\n- 观测向量 $y_{t_{i}} \\in \\mathbb{R}^{p_{i}}$。\n- 正定观测误差协方差矩阵 $R_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$。\n- 背景（先验）平均状态 $x_{b} \\in \\mathbb{R}^{n}$。\n- 正定背景误差协方差矩阵 $B \\in \\mathbb{R}^{n \\times n}$。\n- 正交小波变换矩阵 $W \\in \\mathbb{R}^{n \\times n}$。\n- 正权重向量 $w \\in \\mathbb{R}^{n}$。\n- 正则化参数 $\\lambda > 0$。\n- 需要最小化的目标函数：\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\;+\\; \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| .\n$$\n- 一个迭代值 $x^{(k)}$ 和一个步长 $\\alpha_k$。\n\n该问题具有科学依据，代表了先进数据同化中的一个标准公式。该问题是适定的，因为目标函数 $J(x)$ 是严格凸的（由于矩阵 $B$ 是正定的），因此有唯一的极小值点。所有项在数学上和物理上都是一致的。因此，该问题是有效的。\n\n目标函数 $J(x)$ 可以从最大后验（MAP）估计的角度来解释。使 $J(x)$ 最小化的解 $x$ 是在给定观测值条件下使后验概率密度最大化的状态。后验概率 $p(x|y)$ 由贝叶斯定理给出，$p(x|y) \\propto p(y|x) p(x)$，其中 $p(y|x)$ 是似然，$p(x)$ 是状态的先验概率。最小化 $J(x)$ 等价于最小化后验概率的负对数。各个项对应于：\n$1$. 项 $\\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)$\n是负对数似然，假设观测误差是均值为零、协方差为 $R_{t_{i}}$ 的独立高斯随机变量。\n$2$. 项 $\\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b})$ 是状态 $x$ 的高斯先验的负对数，其均值为 $x_b$，协方差为 $B$。\n$3$. 项 $\\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr|$ 是小波系数 $(Wx)_j$ 的可分离拉普拉斯（或指数）先验的负对数，这会促进稀疏性。\n\n目标函数具有复合结构 $J(x) = f(x) + g(x)$，其中 $f(x)$ 是一个平滑、可微的凸函数，而 $g(x)$ 是一个凸但不可微的函数。\n平滑部分是：\n$$\nf(x) = \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr).\n$$\n非光滑部分是加权 $\\ell^{1}$ 惩罚项：\n$$\ng(x) = \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| = \\lambda \\| \\text{diag}(w) W x \\|_1.\n$$\n近端梯度法适用于最小化此类复合函数。从迭代值 $x^{(k)}$ 到 $x^{(k+1)}$ 的更新由下式给出：\n$$\nx^{(k+1)} = \\text{prox}_{\\alpha_k g}\\left(x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\\right),\n$$\n其中 $\\alpha_k$ 是步长，$\\text{prox}_{\\alpha_k g}$ 是函数 $\\alpha_k g$ 的近端算子。\n\n首先，我们计算平滑部分 $\\nabla f(x)$ 的梯度。\n背景项的梯度是 $\\nabla_x \\left( \\frac{1}{2}(x - x_b)^{\\top}B^{-1}(x - x_b) \\right) = B^{-1}(x - x_b)$。\n观测项的梯度是 $\\nabla_x \\left( \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\right) = \\sum_{i=1}^{m} (H_{t_{i}} M_{0 \\to t_{i}})^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}})$。\n将这些结合起来，$f(x)$ 的完整梯度是：\n$$\n\\nabla f(x) = B^{-1}(x - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}).\n$$\n问题陈述指出，步长 $\\alpha_k$ 是基于 $\\nabla f(x)$ 的Lipschitz常数选择的，该常数由Hessian矩阵 $\\nabla^2 f(x)$ 的谱范数给出。由于 $f(x)$ 是关于 $x$ 的二次函数，其Hessian矩阵是常数：\n$$\n\\nabla^2 f(x) = B^{-1} + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} H_{t_{i}} M_{0 \\to t_{i}}.\n$$\n这就是问题中提到的高斯-牛顿矩阵。步长 $\\alpha_k$ 是其谱范数上界的倒数。\n\n接下来，我们推导近端算子 $\\text{prox}_{\\alpha_k g}(z)$，其定义为：\n$$\n\\text{prox}_{\\alpha_k g}(z) = \\underset{u \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k g(u) + \\frac{1}{2} \\|u - z\\|_2^2 \\right).\n$$\n代入 $g(u) = \\lambda \\sum_{j=1}^{n} w_{j} |(W u)_{j}|$，最小化问题变为：\n$$\n\\underset{u \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k \\lambda \\sum_{j=1}^{n} w_{j} |(W u)_{j}| + \\frac{1}{2} \\|u - z\\|_2^2 \\right).\n$$\n我们利用小波变换矩阵 $W$ 是正交的（$W^{\\top}W = WW^{\\top} = I$）这一性质。这意味着 $W$ 是一个欧几里得等距算子，即对于任何向量 $v$，都有 $\\|v\\|_2 = \\|Wv\\|_2$。\n令 $\\theta = Wu$。那么 $u = W^{\\top}\\theta$。项 $\\|u - z\\|_2^2$ 可以改写为 $\\|W(u-z)\\|_2^2 = \\|Wu - Wz\\|_2^2 = \\|\\theta - Wz\\|_2^2$。\n该最小化问题可以用小波系数 $\\theta$ 来改写：\n$$\n\\underset{\\theta \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k \\lambda \\sum_{j=1}^{n} w_{j} |\\theta_{j}| + \\frac{1}{2} \\|\\theta - Wz\\|_2^2 \\right).\n$$\n这个问题是可分离的，意味着我们可以对每个分量 $\\theta_j$ 独立求解：\n$$\n\\underset{\\theta_j \\in \\mathbb{R}}{\\arg\\min} \\left( \\alpha_k \\lambda w_{j} |\\theta_{j}| + \\frac{1}{2} (\\theta_j - (Wz)_j)^2 \\right).\n$$\n这个标量问题的解由软阈值算子给出，我们记为 $S$。最优的 $\\theta_j$ 是：\n$$\n\\theta_j^* = S_{\\alpha_k \\lambda w_j}((Wz)_j) = \\text{sign}((Wz)_j) \\max(0, |(Wz)_j| - \\alpha_k \\lambda w_j).\n$$\n以向量形式，$\\theta$ 的解是 $\\theta^* = S_{\\alpha_k \\lambda w}(Wz)$，其中算子 $S$ 以阈值向量 $\\alpha_k \\lambda w$ 逐分量作用。\n为了找到 $u$ 的解，我们进行逆变换：$u^* = W^{\\top}\\theta^* = W^{\\top}S_{\\alpha_k \\lambda w}(Wz)$。\n所以，近端算子是：\n$$\n\\text{prox}_{\\alpha_k g}(z) = W^{\\top}S_{\\alpha_k \\lambda w}(Wz).\n$$\n现在，我们可以写出完整的近端梯度更新步骤。近端算子的参数是 $z = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})$。\n代入 $\\nabla f(x^{(k)})$ 的表达式，我们得到：\n$$\nz = x^{(k)} - \\alpha_k \\left( B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x^{(k)} - y_{t_{i}}) \\right).\n$$\n$x^{(k+1)}$ 的更新即为 $\\text{prox}_{\\alpha_k g}(z)$：\n$$\nx^{(k+1)} = W^{\\top}S_{\\alpha_k \\lambda w}\\left( W \\left[ x^{(k)} - \\alpha_k \\left( B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x^{(k)} - y_{t_{i}}) \\right) \\right] \\right).\n$$\n这就是从当前迭代值 $x^{(k)}$ 计算一次近端梯度更新 $x^{(k+1)}$ 的单一显式解析表达式，满足问题陈述的所有条件。软阈值算子 $S_{\\tau}(v)$ 被理解为分量函数 $(S_{\\tau}(v))_j = \\text{sign}(v_j) \\max(0, |v_j|-\\tau_j)$。",
            "answer": "$$\n\\boxed{\nx^{(k+1)} = W^{\\top} S_{\\alpha_k \\lambda w}\\left( W \\left( x^{(k)} - \\alpha_k \\left[ B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_i}^{\\top} H_{t_i}^{\\top} R_{t_i}^{-1} (H_{t_i} M_{0 \\to t_i} x^{(k)} - y_{t_i}) \\right] \\right) \\right)\n}\n$$"
        }
    ]
}