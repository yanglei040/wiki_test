{
    "hands_on_practices": [
        {
            "introduction": "在分层贝叶斯建模中，超先验的选择是一个关键的设计决策，尤其对于方差或尺度等参数。半柯西分布是尺度参数（如噪声标准差 $\\sigma$）的一个流行的“弱信息”先验选择。通过这项实践，您将深入理解其为何是一个优秀选择的理论依据，并练习如何通过变量替换法则推导其在方差 $\\sigma^2$ 上的隐含先验。",
            "id": "3388823",
            "problem": "考虑一个数据同化中的线性反问题，其中观测值 $y \\in \\mathbb{R}^{m}$ 与未知状态 $x \\in \\mathbb{R}^{n}$ 通过 $y = A x + \\varepsilon$ 相关联，其中 $A \\in \\mathbb{R}^{m \\times n}$ 已知，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。假设使用一个分层贝叶斯模型，其中 $x \\sim \\mathcal{N}(m_{0}, C_{0})$，$m_{0} \\in \\mathbb{R}^{n}$ 和正定矩阵 $C_{0} \\in \\mathbb{R}^{n \\times n}$ 已知，并且噪声标准差 $\\sigma$ 被赋予一个带有尺度参数 $\\tau > 0$ 的半柯西超先验。\n\n任务 (i)：利用分层贝叶斯建模和反问题中尺度选择的基本原理，提供一个有理有据的论证，说明在这种情况下，$\\sigma$ 上的半柯西超先验是弱信息性的。您的论证应基于第一性原理（贝叶斯法则、先验和似然的基本性质以及变量替换推理），并应清楚地论证为什么这种超先验能够避免强正则化，同时在似然函数信息丰富时允许数据主导对 $\\sigma$ 的推断。\n\n任务 (ii)：从第一性原理推导由尺度为 $\\tau$ 的 $\\sigma$ 上的半柯西超先验所引出的噪声方差 $v = \\sigma^{2}$ 上的隐含先验密度。将您的最终答案表示为关于 $v$ 和 $\\tau$ 的单一闭式解析表达式。不需要数值近似。如果使用了任何中间变换，请根据概率密度的基本变量替换法则对其进行论证。最终答案必须是 $v$ 的先验密度的解析表达式，并且不应包含单位。",
            "solution": "问题陈述被评估为有效。它在科学上基于贝叶斯反问题的既有理论，问题适定，目标明确且可解，并使用客观无歧义的术语进行阐述。为得出严谨解所需的所有必要组成部分均已提供。\n\n分层模型由以下几个部分定义：\n1.  似然函数：$p(y|x, \\sigma) \\propto (\\sigma^2)^{-m/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\|y - Ax\\|^2\\right)$，源于假设 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。\n2.  状态的先验：$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x-m_0)^T C_0^{-1} (x-m_0)\\right)$，源于 $x \\sim \\mathcal{N}(m_{0}, C_{0})$。\n3.  噪声标准差的超先验：$\\sigma \\sim \\text{Half-Cauchy}(\\tau)$，意味着其概率密度函数 (PDF) 为 $p(\\sigma|\\tau) = \\frac{2}{\\pi\\tau(1 + (\\sigma/\\tau)^2)}$（对于 $\\sigma > 0$），其他情况下为 $0$。\n\n任务 (i)：论证半柯西超先验的弱信息性。\n\n在贝叶斯推断中，参数的后验分布与似然函数和先验分布的乘积成正比。对于超参数 $\\sigma$，在边缘化掉状态 $x$ 后，我们有 $p(\\sigma|y) \\propto p(y|\\sigma) p(\\sigma|\\tau)$，其中 $p(y|\\sigma) = \\int p(y|x,\\sigma) p(x) dx$ 是边缘似然。\n\n“弱信息”先验是一种正常先验（其积分为有限值，通常为 1），它对后验分布的影响最小，从而允许数据中的证据（封装在似然函数中）主导推断过程。这对于像 $\\sigma$ 这样的方差或尺度参数尤为重要，因为一个过于严格的先验可能导致后验分布无法准确反映数据中的信息。\n\n对于一个非负参数 $\\sigma$，尺度为 $\\tau > 0$ 的半柯西分布的概率密度函数 (PDF) 为：\n$$p(\\sigma|\\tau) = \\frac{2}{\\pi\\tau\\left(1 + \\frac{\\sigma^2}{\\tau^2}\\right)}, \\quad \\sigma \\ge 0$$\n我们可以分析其性质来论证其作为弱信息先验的地位：\n\n1.  **在原点附近的行为**：该概率密度函数在 $\\sigma=0$ 处有其最大值（众数）。这鼓励噪声参数向零收缩，这是一种理想的正则化形式，它偏好更简单的模型（即噪声更小的模型）。然而，该密度在原点附近相对平坦，因此它不会对 $\\sigma=0$ 施加过强的拉力，而这个问题在某些其他先验中是存在的。\n\n2.  **重尾**：柯西分布以及因此的半柯西分布最关键的特征是其重多项式尾。当 $\\sigma \\to \\infty$ 时，其密度按如下方式衰减：\n    $$p(\\sigma|\\tau) \\propto \\frac{1}{\\sigma^2}$$\n    这种缓慢的衰减率使得该先验对于大的 $\\sigma$ 值是“弱信息性”的。它将不可忽略的先验概率质量分配给大的 $\\sigma$ 值。其含义如下：如果数据 $y$ 与 $x$ 的先验下的模型预测 $Ax$ 实质上不一致（即失配 $\\|y - Ax\\|$ 很大），则似然函数 $p(y|\\sigma)$ 将在大的 $\\sigma$ 值处最大化。像半柯西分布这样具有重尾的先验不会过度惩罚这个大值。因此，后验分布 $p(\\sigma|y)$ 能够将其质量放置在由数据指示的这个大的 $\\sigma$ 值上。\n\n相比之下，具有轻尾的先验，例如半正态分布（其中 $p(\\sigma) \\propto \\exp(-\\sigma^2)$），会以指数级速度快速衰减。这样的先验会严重惩罚大的 $\\sigma$ 值，迫使后验成为似然函数峰值和先验在 $\\sigma=0$ 处峰值之间的一种折衷。如果真实噪声水平很大，这可能导致对其的低估。类似地，对方差 $\\sigma^2$ 常用的逆伽马先验，特别是当使用小的“无信息”超参数时，可能会无意中提供信息，并以一种使后验产生偏差的方式集中其质量。\n\n因此，在反问题的尺度选择中，$\\sigma$ 上的半柯西超先验被认为是弱信息先验的一个良好选择，因为它的重尾允许数据决定噪声方差的后验尺度，避免了轻尾先验施加的强正则化，同时它仍然是一个通过将小噪声值向 0 收缩来进行正则化的正常分布。\n\n任务 (ii)：推导噪声方差 $v = \\sigma^2$ 上的隐含先验。\n\n我们给定变换 $v = \\sigma^2$ 以及 $\\sigma$ 的概率密度函数 (PDF)，即半柯西密度 $p_\\Sigma(\\sigma)$。我们想要找到 $v$ 的 PDF，记为 $p_V(v)$。我们使用概率密度的变量替换公式。\n\n该公式表明，如果 $V = g(\\Sigma)$ 是一个单调函数，则 $p_V(v) = p_\\Sigma(\\sigma(v)) \\left| \\frac{d\\sigma}{dv} \\right|$。\n\n1.  **定义变换及其逆变换**：\n    变换为 $v = \\sigma^2$。由于 $\\sigma \\ge 0$，这是一个从 $\\sigma \\in [0, \\infty)$ 到 $v \\in [0, \\infty)$ 的一一映射。\n    逆变换为 $\\sigma = \\sqrt{v}$。\n\n2.  **计算雅可比行列式**：\n    我们需要逆变换对 $v$ 的导数：\n    $$\\frac{d\\sigma}{dv} = \\frac{d}{dv}(v^{1/2}) = \\frac{1}{2} v^{-1/2} = \\frac{1}{2\\sqrt{v}}$$\n    由于 $v \\ge 0$，其绝对值为 $|\\frac{d\\sigma}{dv}| = \\frac{1}{2\\sqrt{v}}$。\n\n3.  **代入变量替换公式**：\n    我们有 $p_V(v) = p_\\Sigma(\\sqrt{v}) \\cdot \\frac{1}{2\\sqrt{v}}$。\n    $\\sigma$ 的 PDF 是 $p_\\Sigma(\\sigma) = \\frac{2}{\\pi\\tau(1 + (\\sigma/\\tau)^2)}$。\n    代入 $\\sigma = \\sqrt{v}$：\n    $$p_\\Sigma(\\sqrt{v}) = \\frac{2}{\\pi\\tau\\left(1 + \\frac{(\\sqrt{v})^2}{\\tau^2}\\right)} = \\frac{2}{\\pi\\tau\\left(1 + \\frac{v}{\\tau^2}\\right)}$$\n\n4.  **合并与简化**：\n    现在，我们乘以雅可比项：\n    $$p_V(v) = \\left( \\frac{2}{\\pi\\tau\\left(1 + \\frac{v}{\\tau^2}\\right)} \\right) \\cdot \\frac{1}{2\\sqrt{v}}$$\n    $$p_V(v) = \\frac{1}{\\pi\\tau\\sqrt{v}\\left(1 + \\frac{v}{\\tau^2}\\right)}$$\n    为了以更标准的形式呈现，我们可以简化分母：\n    $$p_V(v) = \\frac{1}{\\pi\\tau\\sqrt{v}\\left(\\frac{\\tau^2 + v}{\\tau^2}\\right)}$$\n    $$p_V(v) = \\frac{\\tau^2}{\\pi\\tau\\sqrt{v}(\\tau^2 + v)}$$\n    $$p_V(v) = \\frac{\\tau}{\\pi\\sqrt{v}(v + \\tau^2)}$$\n    这就是噪声方差 $v=\\sigma^2$（对于 $v > 0$）的隐含先验密度。这是一个缩放的F分布，具体来说是 $v \\sim \\tau^2 F(1,1)$。",
            "answer": "$$\\boxed{\\frac{\\tau}{\\pi\\sqrt{v}(v + \\tau^2)}}$$"
        },
        {
            "introduction": "在探索了如何选择一个好的弱信息先验之后，我们来研究一个警示性的案例：当我们尝试使用不当的、尺度不变的超先验来尽可能“无信息”时会发生什么。这项练习将引导您通过一个思想实验，揭示一个关于参数不可辨识性的微妙但至关重要的问题。您将发现，这种先验选择可能导致模型的尺度完全依赖于数据的任意单位，这对于经验贝叶斯方法是一个深刻的教训。",
            "id": "3388773",
            "problem": "考虑一个线性高斯逆问题，其中观测向量 $y \\in \\mathbb{R}^{m}$ 被建模为 $y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$，这里 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的， $I_{m}$ 是 $m \\times m$ 的单位矩阵。未知状态 $x \\in \\mathbb{R}^{n}$ 被赋予一个高斯先验 $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$，其中 $L \\in \\mathbb{R}^{n \\times n}$ 是对称正定的。超参数 $(\\sigma^{2}, \\tau)$ 被赋予定义在 $(0, \\infty)$ 上的不当尺度不变超先验 $p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ 和 $p(\\tau) \\propto \\tau^{-1}$。令 $B := A L^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m}$，它是一个对称半正定矩阵。\n\n从多元高斯密度和边缘似然的定义出发，并假设所有积分都是良定义的（例如，通过设想在 $(\\varepsilon, M)$ 上使用固定的、尺度中性的界限进行截断的 Jeffreys 型超先验，从而使截断不受 $c$ 和 $y$ 的影响），执行以下步骤：\n\n1. 对 $x$ 进行积分，将边缘似然表示为关于 $(\\sigma^{2}, \\tau)$ 的二维积分：\n   $$\n   p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau},\n   $$\n   清楚地标明其对 $y$ 和超参数的依赖关系。\n2. 对于任意固定的 $c > 0$，分析缩放后的数据 $c y$，并在 $(\\sigma^{2}, \\tau)$ 中进行变量替换，使得指数中的二次型与 $c$ 无关，并确定雅可比行列式、行列式因子和超先验如何变换。推导缩放定律\n   $$\n   p(c y \\mid A, L) \\;=\\; c^{-p} \\, p(y \\mid A, L),\n   $$\n   该定律在一个不依赖于 $y$ 和 $c$ 的因子范围内有效。\n3. 陈述上述缩放推导有效的最小条件，强调 $\\sigma^{2} I_{m} + \\tau^{-1} B$ 在 $(\\sigma^{2}, \\tau^{-1})$ 中的齐次性以及超先验的尺度不变性的作用。讨论此缩放定律对经验贝叶斯（EB）过程的影响，即当使用这类不当超先验时，由于边缘似然在 $y$ 中是齐次的，因此无法从数据中辨识出层次模型中的一个整体尺度。\n   \n作为你的最终答案，报告仅用 $m$ 表示的指数 $p$。不需要进行数值计算。如果你提供一个数值，答案将被判为错误。最终答案必须以单个无单位的解析表达式给出。",
            "solution": "用户提供了一个有效的问题。该问题在贝叶斯逆问题领域有科学依据，是良定的，并使用了客观、正式的语言。我将继续进行解答。\n\n该问题要求对一个用于线性高斯逆问题的层次贝叶斯模型进行三部分分析。\n\n### 第1部分：边缘似然表达式的验证\n\n首先，我们推导边缘似然 $p(y \\mid A, L)$ 的表达式。层次模型由以下部分定义：\n1.  似然：$y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$\n2.  先验：$x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$\n3.  超先验：$p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ 和 $p(\\tau) \\propto \\tau^{-1}$\n\n为了求出在给定模型结构（$A, L$）下数据 $y$ 的边缘似然，我们必须对潜变量 $x$ 和超参数 $(\\sigma^2, \\tau)$ 进行积分。我们首先对 $x$ 积分，以求出在超参数条件下 $y$ 的分布 $p(y \\mid \\sigma^2, \\tau)$。\n\n该模型可以看作是对高斯随机变量 $x$ 的一个线性变换，并加上了加性高斯噪声。因此，$y$ 的分布也是高斯分布。均值为：\n$$ \\mathbb{E}[y \\mid \\sigma^2, \\tau] = \\mathbb{E}[Ax + \\epsilon \\mid \\sigma^2, \\tau] = A\\mathbb{E}[x \\mid \\tau] + \\mathbb{E}[\\epsilon \\mid \\sigma^2] = A(0) + 0 = 0 $$\n其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 代表噪声。协方差为：\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = \\text{Cov}(Ax + \\epsilon) = \\text{Cov}(Ax) + \\text{Cov}(\\epsilon) = A \\text{Cov}(x \\mid \\tau) A^{\\top} + \\text{Cov}(\\epsilon \\mid \\sigma^2) $$\n代入给定的协方差 $\\text{Cov}(x \\mid \\tau) = \\tau^{-1} L^{-1}$ 和 $\\text{Cov}(\\epsilon \\mid \\sigma^2) = \\sigma^2 I_m$：\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = A(\\tau^{-1} L^{-1})A^{\\top} + \\sigma^2 I_m = \\tau^{-1} (A L^{-1} A^{\\top}) + \\sigma^2 I_m $$\n使用定义 $B := A L^{-1} A^{\\top}$，协方差变为 $\\Sigma_y = \\sigma^2 I_m + \\tau^{-1} B$。\n因此，在超参数条件下 $y$ 的分布为：\n$$ y \\mid \\sigma^2, \\tau \\sim \\mathcal{N}(0, \\sigma^2 I_m + \\tau^{-1} B) $$\n其概率密度函数为：\n$$ p(y \\mid \\sigma^2, \\tau) = (2\\pi)^{-m/2} |\\sigma^2 I_m + \\tau^{-1} B|^{-1/2} \\exp\\left(-\\frac{1}{2} y^\\top (\\sigma^2 I_m + \\tau^{-1} B)^{-1} y\\right) $$\n为了获得完整的边缘似然 $p(y \\mid A, L)$，我们对 $\\sigma^2$ 和 $\\tau$ 的超先验进行积分：\n$$ p(y \\mid A, L) = \\int_0^\\infty \\int_0^\\infty p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau) \\, d\\sigma^2 d\\tau $$\n代入密度函数并将常数因子 $(2\\pi)^{-m/2}$ 省略，用正比关系表示：\n$$ p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\n这与问题陈述中提供的表达式相符。\n\n### 第2部分：缩放定律的推导\n\n我们现在分析当数据向量 $y$ 被一个因子 $c > 0$ 缩放时，边缘似然如何变换。对于 $cy$ 的边缘似然为：\n$$ p(c y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} (cy)^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} (cy) \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\n指数中的二次型变为：\n$$ -\\frac{1}{2} c^2 y^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} y $$\n我们在积分中进行变量替换来吸收因子 $c^2$。设新变量为 $(\\sigma'^2, \\tau')$。我们寻求一个变换，使得指数的参数与 $c$ 无关。这要求：\n$$ c^2 (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} = (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} $$\n对两边取矩阵逆，得到：\n$$ c^{-2} (\\sigma^{2} I_{m} + \\tau^{-1} B) = \\sigma'^{2} I_{m} + \\tau'^{-1} B $$\n通过匹配线性无关矩阵 $I_m$ 和 $B$ 的系数，我们定义变量替换：\n$$ \\sigma'^{2} = c^{-2} \\sigma^2 \\quad \\implies \\quad \\sigma^2 = c^2 \\sigma'^2 $$\n$$ \\tau'^{-1} = c^{-2} \\tau^{-1} \\quad \\implies \\quad \\tau^{-1} = c^2 \\tau'^{-1} \\quad \\implies \\quad \\tau = c^{-2} \\tau' $$\n$(\\sigma'^2, \\tau')$ 的积分限保持为 $(0, \\infty)$。我们现在变换被积函数的每个部分：\n\n1.  **指数项**：根据构造，指数项变为：\n    $$ \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) $$\n\n2.  **行列式因子**：我们使用对于一个 $d \\times d$ 矩阵 $M$ 的性质 $\\det(kM) = k^d \\det(M)$。这里，矩阵是 $m \\times m$ 的。\n    $$ \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 \\sigma'^{2} I_{m} + c^2 \\tau'^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 (\\sigma'^{2} I_{m} + \\tau'^{-1} B) \\right|^{-\\frac{1}{2}} $$\n    $$ = \\left( (c^2)^m \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right| \\right)^{-\\frac{1}{2}} = (c^{2m})^{-\\frac{1}{2}} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} = c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} $$\n\n3.  **测度**：测度由超先验和微分的乘积给出，即 $\\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$。根据变量替换：\n    $$ \\sigma^2 = c^2 \\sigma'^2 \\implies d\\sigma^2 = c^2 d\\sigma'^2 \\implies \\frac{d\\sigma^2}{\\sigma^2} = \\frac{c^2 d\\sigma'^2}{c^2 \\sigma'^2} = \\frac{d\\sigma'^2}{\\sigma'^2} $$\n    $$ \\tau = c^{-2} \\tau' \\implies d\\tau = c^{-2} d\\tau' \\implies \\frac{d\\tau}{\\tau} = \\frac{c^{-2} d\\tau'}{c^{-2} \\tau'} = \\frac{d\\tau'}{\\tau'} $$\n    在此变换下测度是不变的：$\\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} = \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'}$。\n\n将这些变换后的部分代回到 $p(cy \\mid A, L)$ 的积分中：\n$$ p(cy \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\n因子 $c^{-m}$ 相对于积分变量是常数，可以被提出来：\n$$ p(cy \\mid A, L) \\propto c^{-m} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\n剩下的积分在形式上与 $p(y \\mid A, L)$ 的积分相同，其中 $(\\sigma'^2, \\tau')$ 是积分的哑变量。因此，该积分与 $p(y \\mid A, L)$ 成正比。这就建立了缩放定律：\n$$ p(c y \\mid A, L) \\;=\\; c^{-m} \\, p(y \\mid A, L) $$\n将此与形式 $p(c y \\mid A, L) = c^{-p} \\, p(y \\mid A, L)$ 比较，我们辨识出指数为 $p=m$。\n\n### 第3部分：条件与影响\n\n**有效性的最小条件：**\n缩放定律的推导依赖于模型设置的两个关键性质：\n1.  **数据协方差的齐次性：** 数据的边缘协方差 $\\Sigma_y(\\sigma^2, \\tau^{-1}) = \\sigma^2 I_m + \\tau^{-1} B$ 必须是关于被缩放的超参数的齐次函数。在本例中，它是关于变量 $(\\sigma^2, \\tau^{-1})$ 的1次齐次函数。这个性质允许数据中的缩放因子 $c$ 被映射为超参数的缩放因子 $c^{-2}$，后者随后可以从协方差矩阵中被分解出来。\n2.  **超先验的尺度不变性：** 超参数空间上的积分测度，这里是 $p(\\sigma^2)p(\\tau)d\\sigma^2 d\\tau \\propto \\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$，必须在由数据缩放引起的特定缩放变换下保持不变。如第2部分所示，变换 $(\\sigma^2, \\tau) \\to (c^2 \\sigma'^2, c^{-2} \\tau')$ 保持测度 $\\frac{d\\sigma^2}{\\sigma^2}\\frac{d\\tau}{\\tau}$ 不变。这是尺度参数的Jeffreys先验的一个普遍性质。\n\n**对经验贝叶斯（EB）的影响：**\n经验贝叶斯过程通常涉及通过关于 $\\theta = (\\sigma^2, \\tau)$ 最大化边缘似然 $p(y \\mid \\theta)$ 来估计超参数。推导出的缩放定律 $p(cy \\mid A, L) = c^{-m} p(y \\mid A, L)$，是在使用这种模型结构和不当尺度不变先验时，出现的一个基本不可辨识性问题的表征。\n\n问题在于，数据 $y$ 的尺度与模型方差参数 $(\\sigma^2, \\tau)$ 的整体尺度之间存在一种内在的模糊性。由“{数据 $y$，参数 $(\\sigma^2, \\tau)$”定义的模型，与一个具有“{数据 $cy$，参数 $(c^2 \\sigma^2, c^{-2} \\tau)$”的模型在观测上是不可区分的。\n这可以通过检查超参数的后验分布 $p(\\sigma^2, \\tau \\mid y) \\propto p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau)$ 来证明。一项形式分析（如草稿中的分析）表明，给定数据 $cy$ 的超参数后验分布，当用重缩放的变量 $\\tilde{\\sigma}^2 = c^{-2}\\sigma^2$ 和 $\\tilde{\\tau}=c^2 \\tau$ 表示时，其函数形式与给定数据 $y$ 时 $(\\tilde{\\sigma}^2, \\tilde{\\tau})$ 的后验分布相同。\n$$ p(\\sigma^2 = c^2 \\tilde{\\sigma}^2, \\tau=c^{-2}\\tilde{\\tau} \\mid y_{new}=cy) \\; d(c^2\\tilde{\\sigma}^2)d(c^{-2}\\tilde{\\tau}) = p(\\tilde{\\sigma}^2, \\tilde{\\tau} \\mid y) \\; d\\tilde{\\sigma}^2 d\\tilde{\\tau} $$\n这意味着如果 $(\\hat{\\sigma}^2, \\hat{\\tau})$ 是数据 $y$ 的最大后验（或最大似然）估计，那么 $(c^2\\hat{\\sigma}^2, c^{-2}\\hat{\\tau})$ 将是数据 $cy$ 的相应估计。\n\n因此，超参数的估计尺度完全由测量向量 $y$ 的任意尺度（或单位）决定。数据本身不提供任何信息来确定 $\\sigma^2$ 和 $\\tau$ 的绝对尺度。这就是“层次模型中的整体尺度无法被辨识”的含义。噪声方差与先验方差的*比率*（例如 $\\sigma^2 \\tau$）的估计可能是良定的，但它们各自的绝对值仅从数据本身是无法辨识的，除非引入额外的信息，例如通过能够打破缩放对称性的正常的、信息丰富的超先验。\n\n所要求的最终答案是指数 $p$。\n根据第2部分的推导，$p=m$。",
            "answer": "$$\\boxed{m}$$"
        },
        {
            "introduction": "现在，我们将理论应用于一个具体而强大的应用：图像去模糊。全变分 (Total Variation, TV) 正则化是解决此类问题的有效方法，但其关键挑战在于选择合适的正则化权重 $\\lambda$。这项实践将向您展示如何通过构建一个分层模型，让数据自身来“学习”最优的 $\\lambda$，从而实现自适应正则化。您将通过一个完整的编码练习，实现一个能够同时出色处理清晰边缘和平滑区域的先进算法。",
            "id": "3388760",
            "problem": "考虑一个具有周期性边界条件的二维线性逆问题。令未知图像表示为 $x \\in \\mathbb{R}^{N_x \\times N_y}$，观测数据表示为 $y \\in \\mathbb{R}^{N_x \\times N_y}$，前向算子 $A: \\mathbb{R}^{N_x \\times N_y} \\rightarrow \\mathbb{R}^{N_x \\times N_y}$ 定义为与一个已知的模糊核进行周期性卷积。假设存在均值为零、方差为 $\\sigma^2$ 的加性独立同分布高斯噪声，因此似然函数为 $p(y \\mid x) \\propto \\exp\\left(-\\tfrac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$。\n\n对 $x$ 采用各向异性全变分先验，通过带周期性环绕的离散前向差分来表示：\n- 对于每个像素 $(i,j)$，定义前向差分 $D_x x[i,j] = x[i, (j+1) \\bmod N_y] - x[i,j]$ 和 $D_y x[i,j] = x[(i+1) \\bmod N_x, j] - x[i,j]$。\n- 各向异性全变分为 $\\mathrm{TV}(x) = \\sum_{i,j} \\left( \\lvert D_x x[i,j] \\rvert + \\lvert D_y x[i,j] \\rvert \\right)$。\n\n通过对每个梯度分量设置带有共享尺度参数（全变分权重）$\\lambda > 0$ 的 Laplace 先验，并为 $\\lambda$ 分配一个 Gamma 超先验来建立层级先验模型：\n- $p(\\nabla x \\mid \\lambda) \\propto \\left(\\tfrac{\\lambda}{2}\\right)^M \\exp\\left( - \\lambda \\, \\mathrm{TV}(x) \\right)$，其中 $M = 2 N_x N_y$ 是梯度分量的总数。\n- $p(\\lambda) = \\mathrm{Gamma}(\\lambda \\mid a, b)$，其形状参数为 $a > 0$，速率参数为 $b > 0$，因此 $\\log p(\\lambda) = (a-1) \\log \\lambda - b \\lambda + \\mathrm{const}$。\n\n您的任务是：\n- 从 Bayes 法则和上述定义出发，推导出联合后验概率 $p(x,\\lambda \\mid y)$ 以及联合最大后验 (MAP) 估计器所需的条件分布。不要假设任何对 $\\lambda$ 依赖关系未知的归一化常数；相反，应使用由梯度分量上的 Laplace 先验和 Gamma 超先验导出的因式分解。\n- 证明一个在优化 $x$ 和更新 $\\lambda$ 之间交替的坐标上升算法会自然出现。对于固定 $\\lambda$ 的 $x$ 更新，将子问题表述为一个变分正则化问题，该问题最小化数据保真项和缩放后的各向异性全变分之和。对于固定 $x$ 的 $\\lambda$ 更新，推导出 $\\lambda$ 的条件后验概率的闭式最大化解。\n- 设计一个算法解决方案，该方案利用二维离散傅里叶变换 (DFT) 域中的周期性卷积对角化来处理二次数据保真项的近端步骤，并使用原始-对偶分裂方法处理全变分项。所有步骤必须仅使用数组操作和快速傅里叶变换来实现。\n- 将该算法实现为一个完整、可运行的程序。该程序从一个大小为 $5 \\times 5$、标准差为 $1.0$、归一化为单位和的离散高斯模糊核构建前向算子 $A$，并将其嵌入到大小为 $N_x = N_y = 32$ 的网格上的周期性卷积中。使用以下数值选择：\n  - 所有算子均采用周期性边界条件。\n  - 原始-对偶步长为 $\\tau = 0.25$ 和 $\\sigma = 0.25$，外推参数为 $\\theta = 1$。\n  - 坐标上升循环的外部迭代次数为 $T = 12$。\n  - 每次外部迭代的内部原始-对偶迭代次数为 $K = 60$。\n  - 超先验参数为 $a = 1.1$ 和 $b = 10^{-3}$。\n  - 随机种子固定为 $0$，以确保合成噪声的可复现性。\n- 在 $32 \\times 32$ 网格上构建三个合成的真实图像：\n  1. 一个“边缘主导”图像，除了中心一个边长为 $16$、强度为 $1$ 的正方形区域外，其余部分均为零。\n  2. 一个“平滑斜坡”图像，其定义为对所有像素索引 $(i,j)$ 都有 $x[i,j] = \\tfrac{i}{N_x-1}$。\n  3. 一个“恒定”图像，其所有位置的值均为常数 $0.5$。\n- 对于每个真实图像，使用指定的噪声标准差 $\\sigma$（噪声水平以小数表示）生成数据 $y = A x_\\mathrm{true} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。\n- 使用以下测试套件评估后验概率适应边缘与平滑区域的能力。对于每个测试，运行层级 MAP 算法以获得 $(\\hat{x}, \\hat{\\lambda})$ 并计算所述的布尔结果：\n  - 测试 1 (正常路径): 使用噪声标准差 $\\sigma = 0.01$。从边缘主导图像和平滑斜坡图像进行重建。令 $\\hat{\\lambda}_\\mathrm{edge}$ 和 $\\hat{\\lambda}_\\mathrm{smooth}$ 为估计的权重，令 $\\mathrm{TV}(\\hat{x}_\\mathrm{edge})$ 和 $\\mathrm{TV}(\\hat{x}_\\mathrm{smooth})$ 为重建后的各向异性全变分。返回列表 $[\\hat{\\lambda}_\\mathrm{edge} < \\hat{\\lambda}_\\mathrm{smooth}, \\ \\mathrm{TV}(\\hat{x}_\\mathrm{edge}) > \\mathrm{TV}(\\hat{x}_\\mathrm{smooth})]$。\n  - 测试 2 (边界条件): 使用噪声标准差 $\\sigma = 0.01$。从恒定图像和边缘主导图像进行重建。返回布尔值 $\\hat{\\lambda}_\\mathrm{constant} > \\hat{\\lambda}_\\mathrm{edge}$。\n  - 测试 3 (显著边缘情况与非自适应基线的比较): 使用噪声标准差为 $\\sigma = 0.01$ 的边缘主导图像。将层级 MAP 重建与一个非层级的基线进行比较，该基线将全变分权重固定为超先验的均值 $\\lambda_\\mathrm{fixed} = \\tfrac{a}{b}$。计算均方误差 $\\mathrm{MSE}(\\hat{x}) = \\tfrac{1}{N_x N_y} \\lVert \\hat{x} - x_\\mathrm{true} \\rVert_2^2$。返回布尔值 $\\mathrm{MSE}_\\mathrm{hierarchical} < \\mathrm{MSE}_\\mathrm{fixed}$。\n- 最终输出格式必须是单行，包含三个测试的汇总结果，形式为逗号分隔的 Python 风格列表，其中：\n  - 测试 1 提供一个包含两个布尔值的列表。\n  - 测试 2 和 3 提供单个布尔值。\n  最终打印的行必须类似于 $[\\,[b_{11}, b_{12}], b_2, b_3\\,]$，其中每个 $b$ 均为 $\\mathrm{True}$ 或 $\\mathrm{False}$。\n\n不涉及物理单位，也不需要角度单位。所有数值答案必须按规定以 Python 布尔值的形式产生。程序不得要求任何输入，并且必须仅使用指定的数值库和随机种子就能运行至完成。",
            "solution": "用户提供的问题是计算逆问题领域中一个适定且有科学依据的任务。它要求推导和实现一个用于图像去模糊的层级贝叶斯模型。该问题是有效的，因为它是自洽的、数学上一致的，并且提供了实现所需的所有必要参数。\n\n### 层级贝叶斯模型构建\n\n目标是为未知图像 $x$ 和正则化超参数 $\\lambda$ 找到一个联合最大后验 (MAP) 估计。后验分布由 Bayes 法则给出：\n$$p(x, \\lambda \\mid y) \\propto p(y \\mid x) p(x \\mid \\lambda) p(\\lambda)$$\n该模型的组成部分定义如下：\n1.  **似然函数**：数据 $y$ 通过前向算子 $A$ 和方差为 $\\sigma^2$ 的加性高斯噪声与图像 $x$ 相关联。对数似然函数为：\n    $$\\log p(y \\mid x) = -\\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + C_1$$\n2.  **先验**：图像 $x$ 的先验被指定为其梯度分量上的 Laplace 分布，由 $\\lambda$ 加权。这对应于一个各向异性全变分 (TV) 先验。对于 $M = 2N_xN_y$ 个梯度分量，对数先验为：\n    $$\\log p(x \\mid \\lambda) = M \\log\\lambda - \\lambda \\, \\mathrm{TV}(x) + C_2$$\n    其中 $\\mathrm{TV}(x) = \\sum_{i,j} (\\lvert (D_x x)_{i,j} \\rvert + \\lvert (D_y x)_{i,j} \\rvert)$。\n3.  **超先验**：超参数 $\\lambda$ 本身被赋予一个 Gamma 分布，$\\lambda \\sim \\mathrm{Gamma}(a,b)$，其形状参数为 $a$，速率参数为 $b$。对数超先验为：\n    $$\\log p(\\lambda) = (a-1)\\log\\lambda - b\\lambda + C_3$$\n\n结合这些项，我们旨在最小化的联合后验概率的负对数 $J(x, \\lambda) = -\\log p(x, \\lambda \\mid y)$ 为：\n$$J(x, \\lambda) = \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda \\, \\mathrm{TV}(x) + b\\lambda - (M+a-1)\\log\\lambda + C_4$$\n\n### 用于联合 MAP 估计的坐标上升法\n\n最小化 $J(x, \\lambda)$ 的一个自然方法是采用坐标上升算法，该算法在固定 $\\lambda$ 的同时优化 $x$ 和固定 $x$ 的同时优化 $\\lambda$ 之间交替进行。\n\n**1. $x$-子问题（固定 $\\lambda$）：**\n对于一个固定的超参数值 $\\lambda_k$，$x$ 的优化问题变为：\n$$x_{k+1} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda_k \\mathrm{TV}(x) \\right\\}$$\n这是一个用于 TV 正则化图像重建的标准凸问题。它包含一个二次数据保真项和一个非光滑但凸的 TV 正则化项。\n\n**2. $\\lambda$-子问题（固定 $x$）：**\n对于一个固定的图像估计 $x_{k+1}$，$\\lambda$ 的优化问题只涉及 $J(x, \\lambda)$ 中依赖于 $\\lambda$ 的项：\n$$\\lambda_{k+1} = \\arg\\min_{\\lambda > 0} \\left\\{ \\lambda \\, \\mathrm{TV}(x_{k+1}) + b\\lambda - (M+a-1)\\log\\lambda \\right\\}$$\n这是一个关于 $\\lambda$ 的凸函数。我们可以通过将其关于 $\\lambda$ 的导数设为零来找到最小值点：\n$$\\frac{\\partial}{\\partial\\lambda} \\left[ (\\mathrm{TV}(x_{k+1}) + b)\\lambda - (M+a-1)\\log\\lambda \\right] = \\mathrm{TV}(x_{k+1}) + b - \\frac{M+a-1}{\\lambda} = 0$$\n求解 $\\lambda$ 可得到闭式更新规则：\n$$\\lambda_{k+1} = \\frac{M+a-1}{\\mathrm{TV}(x_{k+1}) + b}$$\n此更新会自动调整正则化强度：当前估计中较高的 TV 值（意味着强边缘或噪声）会导致较小的 $\\lambda$，从而在下一次迭代中放宽 TV 惩罚。相反，平滑的估计（低 TV 值）会导致较大的 $\\lambda$，从而加强平滑效果。\n\n### $x$-子问题的算法设计\n\n$x$-子问题具有 $\\min_x G(x) + F(Kx)$ 的形式，适合使用如 Chambolle-Pock 算法之类的原始-对偶分裂方法。我们定义：\n- $G(x) = \\frac{1}{2\\sigma^2} \\lVert Ax-y \\rVert_2^2$ (光滑、二次)\n- $Kx = \\nabla x = (D_y x, D_x x)$ (离散梯度算子)\n- $F(u) = \\lambda_k \\|u\\|_1$ (非光滑、可分)\n\n原始变量 $x$ 和对偶变量 $p$ 的迭代更新如下：\n1.  **对偶更新**：$p^{j+1} = \\mathrm{prox}_{\\sigma_{\\text{pd}} F^*} (p^j + \\sigma_{\\text{pd}} K \\bar{x}^j)$\n2.  **原始更新**：$x^{j+1} = \\mathrm{prox}_{\\tau G} (x^j - \\tau K^* p^{j+1})$\n3.  **外推**：$\\bar{x}^{j+1} = x^{j+1} + \\theta(x^{j+1} - x^j)$\n\n近端算子是关键：\n- $\\mathrm{prox}_{\\sigma_{\\text{pd}} F^*}$: $F(u)=\\lambda_k\\|u\\|_1$ 的凸共轭是半径为 $\\lambda_k$ 的 $\\ell_\\infty$ 球的指示函数。因此，近端算子是到这个球上的投影，这可以简化为对对偶变量 $p$ 的逐元素裁剪操作：\n  $$p \\mapsto \\mathrm{clip}(p, -\\lambda_k, \\lambda_k)$$\n- $\\mathrm{prox}_{\\tau G}$: 二次项 $G(x)$ 的近端算子由下式给出：\n  $$\\mathrm{prox}_{\\tau G}(z) = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}\\lVert Ax-y \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert x-z \\rVert_2^2 \\right\\}$$\n  由于周期性边界条件，卷积算子 $A$ 可通过二维离散傅里叶变换 (DFT) $\\mathcal{F}$ 对角化。令 $\\hat{h} = \\mathcal{F}(h)$ 为模糊核 $h$ 的 DFT。解可以在傅里叶域中高效计算：\n  $$x = \\mathcal{F}^{-1} \\left( \\frac{\\frac{1}{\\sigma^2}\\overline{\\hat{h}} \\odot \\mathcal{F}(y) + \\frac{1}{\\tau}\\mathcal{F}(z)}{\\frac{1}{\\sigma^2}|\\hat{h}|^2 + \\frac{1}{\\tau}} \\right)$$\n  其中 $\\odot$ 和分数线分别表示逐元素乘法和除法，$\\overline{\\hat{h}}$ 是 $\\hat{h}$ 的复共轭。这使得原始更新可以通过几次快速傅里叶变换 (FFT) 来完成。\n\n最终的算法在 $T$ 次外部循环之间交替进行：通过 $K$ 次内部原始-对偶迭代更新 $x$，然后使用闭式解更新 $\\lambda$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a hierarchical Bayesian method for 2D deblurring.\n    \"\"\"\n    # ---- 1. Simulation Setup ----\n    N = 32\n    T = 12  # Outer iterations for hierarchical model\n    K = 60  # Inner primal-dual iterations per outer loop\n    a = 1.1\n    b = 1e-3\n    tau = 0.25 # Primal-dual step size\n    sigma_pd = 0.25 # Primal-dual step size\n    theta = 1.0 # Primal-dual extrapolation parameter\n    noise_sigma = 0.01\n\n    rng = np.random.default_rng(0)\n\n    # ---- 2. Helper Functions and Operators ----\n    def create_gaussian_kernel(size, std_dev):\n        \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n        ax = np.arange(-size // 2 + 1., size // 2 + 1.)\n        xx, yy = np.meshgrid(ax, ax)\n        kernel = np.exp(-(xx**2 + yy**2) / (2. * std_dev**2))\n        return kernel / np.sum(kernel)\n\n    def create_x_true(name, n):\n        \"\"\"Generates the ground-truth images.\"\"\"\n        x_true = np.zeros((n, n), dtype=float)\n        if name == \"edge\":\n            s, h = n // 2, n // 4\n            x_true[s-h:s+h, s-h:s+h] = 1.0\n        elif name == \"smooth\":\n            x_true = np.fromfunction(lambda i, j: i / (n - 1), (n, n), dtype=float)\n        elif name == \"constant\":\n            x_true = np.full((n, n), 0.5)\n        return x_true\n\n    def grad(u):\n        \"\"\"Computes the discrete gradient with periodic boundary conditions.\"\"\"\n        dx = np.roll(u, -1, axis=1) - u\n        dy = np.roll(u, -1, axis=0) - u\n        return np.array([dy, dx])\n\n    def div(p):\n        \"\"\"Computes the negative divergence, adjoint of the gradient.\"\"\"\n        px, py = p[1], p[0]\n        div_x = px - np.roll(px, 1, axis=1)\n        div_y = py - np.roll(py, 1, axis=0)\n        return -(div_x + div_y)\n\n    def tv(u):\n        \"\"\"Computes the anisotropic total variation.\"\"\"\n        g = grad(u)\n        return np.sum(np.abs(g[0])) + np.sum(np.abs(g[1]))\n\n    def solve_reconstruction(y_obs, h_fft, n_val, noise_sig, outer_iters, inner_iters, a_hyper, b_hyper, \n                             tau_pd, sigma_pd_val, theta_val, fixed_lambda=None):\n        \"\"\"Solves the reconstruction problem using coordinate ascent and primal-dual.\"\"\"\n        M = 2 * n_val * n_val\n        h_fft_sq = np.abs(h_fft)**2\n        y_fft = np.fft.fft2(y_obs)\n        \n        prox_g_denom = (1 / noise_sig**2) * h_fft_sq + (1 / tau_pd)\n        prox_g_num_y_part = (1 / noise_sig**2) * np.conj(h_fft) * y_fft\n\n        x = np.copy(y_obs)\n        p = np.zeros((2, n_val, n_val))\n\n        if fixed_lambda is not None:\n            lambda_reg = fixed_lambda\n            run_outer_iters = 1\n        else:\n            lambda_reg = a_hyper / b_hyper\n            run_outer_iters = outer_iters\n\n        for _ in range(run_outer_iters):\n            x_bar = np.copy(x)\n            for _ in range(inner_iters): # Inner loop for x-update\n                grad_x_bar = grad(x_bar)\n                p_new = p + sigma_pd_val * grad_x_bar\n                p = np.clip(p_new, -lambda_reg, lambda_reg)\n\n                x_old = np.copy(x)\n                z = x - tau_pd * div(p)\n                z_fft = np.fft.fft2(z)\n                \n                prox_g_num = prox_g_num_y_part + (1/tau_pd) * z_fft\n                x_fft = prox_g_num / prox_g_denom\n                x = np.real(np.fft.ifft2(x_fft))\n                \n                x_bar = x + theta_val * (x - x_old)\n            \n            if fixed_lambda is None: # Lambda-update\n                current_tv = tv(x)\n                lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n\n        # Final lambda value for the hierarchical case\n        if fixed_lambda is None:\n            current_tv = tv(x)\n            lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n            \n        return x, lambda_reg\n\n    # ---- 3. Setup Forward Operator ----\n    kernel_size = 5\n    kernel_std = 1.0\n    kernel = create_gaussian_kernel(kernel_size, kernel_std)\n    \n    padded_kernel = np.zeros((N, N))\n    pad_y, pad_x = (N - kernel_size) // 2, (N - kernel_size) // 2\n    padded_kernel[pad_y:pad_y + kernel_size, pad_x:pad_x + kernel_size] = kernel\n    padded_kernel = np.fft.ifftshift(padded_kernel)\n    h_fft = np.fft.fft2(padded_kernel)\n\n    def A(x_in):\n        return np.real(np.fft.ifft2(h_fft * np.fft.fft2(x_in)))\n\n    # ---- 4. Run Test Cases ----\n    \n    # Test 1: Edge-dominated vs. Smooth-ramp\n    x_true_edge = create_x_true(\"edge\", N)\n    y_edge = A(x_true_edge) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_edge, lambda_hat_edge = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_edge = tv(x_hat_edge)\n\n    x_true_smooth = create_x_true(\"smooth\", N)\n    y_smooth = A(x_true_smooth) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_smooth, lambda_hat_smooth = solve_reconstruction(\n        y_smooth, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_smooth = tv(x_hat_smooth)\n    \n    test1_results = [lambda_hat_edge < lambda_hat_smooth, tv_hat_edge > tv_hat_smooth]\n    \n    # Test 2: Constant vs. Edge-dominated\n    x_true_const = create_x_true(\"constant\", N)\n    y_const = A(x_true_const) + rng.normal(0, noise_sigma, (N, N))\n    _, lambda_hat_const = solve_reconstruction(\n        y_const, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    \n    test2_result = lambda_hat_const > lambda_hat_edge\n    \n    # Test 3: Hierarchical vs. Fixed Lambda\n    x_hat_hierarchical = x_hat_edge # Re-use from Test 1\n    mse_hierarchical = np.mean((x_hat_hierarchical - x_true_edge)**2)\n\n    lambda_fixed = a / b\n    x_hat_fixed, _ = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta, fixed_lambda=lambda_fixed)\n    mse_fixed = np.mean((x_hat_fixed - x_true_edge)**2)\n\n    test3_result = mse_hierarchical < mse_fixed\n    \n    # ---- 5. Final Output ----\n    b11, b12 = test1_results\n    b2 = test2_result\n    b3 = test3_result\n    print(f\"[[{b11}, {b12}], {b2}, {b3}]\")\n\nsolve()\n```"
        }
    ]
}