{
    "hands_on_practices": [
        {
            "introduction": "为了将最优传输的抽象理论付诸实践，我们从其最基本的形式开始：离散概率测度之间的耦合。这个练习将指导你把最优传输问题构建成一个线性规划问题，这是理解其计算结构的基础。通过解决一个具体的小例子，你将亲手计算最优耦合矩阵及其相关成本，从而直观地掌握质量如何从先验分布输运到后验分布。",
            "id": "3408143",
            "problem": "考虑一个离散环境下的贝叶斯推断，其中一个由概率为 $\\{p_i\\}_{i=1}^{m}$ 的粒子 $\\{x_i\\}_{i=1}^{m}$ 支撑的离散先验分布，被更新为一个由概率为 $\\{q_j\\}_{j=1}^{n}$ 的粒子 $\\{y_j\\}_{j=1}^{n}$ 支撑的离散后验分布。在二次运输成本下，这个贝叶斯更新可以通过一个最优运输方案来表示，该方案通过求解一个线性规划问题将先验分布与后验分布耦合起来。\n\n任务 1 (构建公式)：仅从最优运输的 Kantorovich 表述出发，写出寻求耦合矩阵 $\\Gamma=\\{\\gamma_{ij}\\}\\in\\mathbb{R}_{+}^{m\\times n}$ 的线性规划，该规划旨在最小化期望二次成本，并满足关于先验和后验概率的质量守恒约束。请用数学形式清晰地陈述目标和约束。\n\n任务 2 (小算例计算)：设 $m=3$ 和 $n=2$。考虑一维粒子和概率\n- 先验支撑集 $\\{x_1,x_2,x_3\\}=\\{0,1,3\\}$，概率为 $\\{p_1,p_2,p_3\\}=\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}\\right\\}$，\n- 后验支撑集 $\\{y_1,y_2\\}=\\left\\{\\frac{1}{2},\\frac{5}{2}\\right\\}$，概率为 $\\{q_1,q_2\\}=\\left\\{\\frac{2}{3},\\frac{1}{3}\\right\\}$。\n\n在 $\\mathbb{R}$上使用二次成本 $c_{ij}=\\|x_i-y_j\\|^{2}$。通过求解任务1中的线性规划，计算此实例的最优耦合 $\\Gamma^{\\star}$。然后计算所得的最优总二次运输成本 $\\sum_{i=1}^{3}\\sum_{j=1}^{2}\\gamma^{\\star}_{ij}\\,\\|x_i-y_j\\|^{2}$。\n\n答案要求：仅报告最优总二次运输成本，以最简分数形式的精确值给出。不要四舍五入。最终答案中不要包含单位或任何其他附加文本。",
            "solution": "该问题是有效的，因为它在科学上基于最优运输理论，问题本身提法清晰，提供了所有必要信息，并且表述客观。该问题包含两个任务：首先，为离散最优运输问题构建一个线性规划；其次，解决该问题的一个具体实例。\n\n**任务 1：线性规划的构建**\n\n在两个离散概率测度之间寻找最优运输方案的问题被称为 Kantorovich 问题。设先验测度为 $\\mu = \\sum_{i=1}^{m} p_i \\delta_{x_i}$，后验测度为 $\\nu = \\sum_{j=1}^{n} q_j \\delta_{y_j}$，其中 $\\delta_z$ 表示在点 $z$ 处的狄拉克δ测度。概率满足 $\\sum_{i=1}^{m} p_i = 1$ 和 $\\sum_{j=1}^{n} q_j = 1$。\n\n$\\mu$ 和 $\\nu$ 之间的一个耦合（或运输方案）是定义在乘积空间上的一个测度，其边缘分布分别为 $\\mu$ 和 $\\nu$。在离散情况下，这对应于一个矩阵 $\\Gamma = \\{\\gamma_{ij}\\} \\in \\mathbb{R}^{m \\times n}$，其中 $\\gamma_{ij}$ 表示从位置 $x_i$ 运输到位置 $y_j$ 的质量量。要使 $\\Gamma$ 成为一个有效的耦合，它必须满足边缘约束：\n1.  从 $x_i$ 运输出的总质量必须等于该点的质量 $p_i$。这对应于 $\\Gamma$ 的行和：\n    $$ \\sum_{j=1}^{n} \\gamma_{ij} = p_i, \\quad \\text{for } i = 1, 2, \\ldots, m $$\n2.  运输到 $y_j$ 的总质量必须等于该点的质量 $q_j$。这对应于 $\\Gamma$ 的列和：\n    $$ \\sum_{i=1}^{m} \\gamma_{ij} = q_j, \\quad \\text{for } j = 1, 2, \\ldots, n $$\n3.  运输的质量不能为负：\n    $$ \\gamma_{ij} \\ge 0, \\quad \\text{for all } i, j $$\n\n从 $x_i$ 运输单位质量到 $y_j$ 的成本由欧几里得距离的平方给出，即 $c_{ij} = \\|x_i - y_j\\|^2$。总运输成本是关于耦合 $\\Gamma$ 的期望成本。目标是找到最小化此总成本的耦合 $\\Gamma^{\\star}$。\n\n该问题可以构建成一个线性规划：\n\n**目标函数：**\n最小化总运输成本 $Z$：\n$$ \\min_{\\Gamma} Z = \\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{ij} \\gamma_{ij} $$\n\n**约束条件：**\n$$\n\\begin{align*}\n\\sum_{j=1}^{n} \\gamma_{ij} = p_i,  \\forall i \\in \\{1, \\ldots, m\\} \\\\\n\\sum_{i=1}^{m} \\gamma_{ij} = q_j,  \\forall j \\in \\{1, \\ldots, n\\} \\\\\n\\gamma_{ij} \\ge 0,  \\forall i \\in \\{1, \\ldots, m\\}, \\forall j \\in \\{1, \\ldots, n\\}\n\\end{align*}\n$$\n\n**任务 2：小算例计算**\n\n给定以下实例：\n-   先验支撑集 $\\{x_1, x_2, x_3\\} = \\{0, 1, 3\\}$，概率为 $\\{p_1, p_2, p_3\\} = \\{\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\}$。\n-   后验支撑集 $\\{y_1, y_2\\} = \\{\\frac{1}{2}, \\frac{5}{2}\\}$，概率为 $\\{q_1, q_2\\} = \\{\\frac{2}{3}, \\frac{1}{3}\\}$。\n\n首先，我们计算成本矩阵 $C = \\{c_{ij}\\}$，其中 $c_{ij} = (x_i - y_j)^2$：\n$c_{11} = (0 - \\frac{1}{2})^2 = \\frac{1}{4}$\n$c_{12} = (0 - \\frac{5}{2})^2 = \\frac{25}{4}$\n$c_{21} = (1 - \\frac{1}{2})^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n$c_{22} = (1 - \\frac{5}{2})^2 = (-\\frac{3}{2})^2 = \\frac{9}{4}$\n$c_{31} = (3 - \\frac{1}{2})^2 = (\\frac{5}{2})^2 = \\frac{25}{4}$\n$c_{32} = (3 - \\frac{5}{2})^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n\n成本矩阵为：\n$$ C = \\begin{pmatrix} \\frac{1}{4}  \\frac{25}{4} \\\\ \\frac{1}{4}  \\frac{9}{4} \\\\ \\frac{25}{4}  \\frac{1}{4} \\end{pmatrix} $$\n\n线性规划是最小化 $Z = \\frac{1}{4}\\gamma_{11} + \\frac{25}{4}\\gamma_{12} + \\frac{1}{4}\\gamma_{21} + \\frac{9}{4}\\gamma_{22} + \\frac{25}{4}\\gamma_{31} + \\frac{1}{4}\\gamma_{32}$，约束条件为：\n\\begin{align*}\n\\gamma_{11} + \\gamma_{12} = \\frac{1}{2} \\\\\n\\gamma_{21} + \\gamma_{22} = \\frac{1}{3} \\\\\n\\gamma_{31} + \\gamma_{32} = \\frac{1}{6} \\\\\n\\gamma_{11} + \\gamma_{21} + \\gamma_{31} = \\frac{2}{3} \\\\\n\\gamma_{12} + \\gamma_{22} + \\gamma_{32} = \\frac{1}{3} \\\\\n\\gamma_{ij} \\ge 0\n\\end{align*}\n我们使用行和约束来用 $\\gamma_{i1}$ 表示 $\\gamma_{i2}$：$\\gamma_{12} = \\frac{1}{2} - \\gamma_{11}$，$\\gamma_{22} = \\frac{1}{3} - \\gamma_{21}$，$\\gamma_{32} = \\frac{1}{6} - \\gamma_{31}$。将这些代入目标函数：\n$$ Z = \\frac{1}{4}\\gamma_{11} + \\frac{25}{4}(\\frac{1}{2} - \\gamma_{11}) + \\frac{1}{4}\\gamma_{21} + \\frac{9}{4}(\\frac{1}{3} - \\gamma_{21}) + \\frac{25}{4}\\gamma_{31} + \\frac{1}{4}(\\frac{1}{6} - \\gamma_{31}) $$\n$$ Z = (\\frac{1}{4} - \\frac{25}{4})\\gamma_{11} + (\\frac{1}{4} - \\frac{9}{4})\\gamma_{21} + (\\frac{25}{4} - \\frac{1}{4})\\gamma_{31} + \\frac{25}{8} + \\frac{9}{12} + \\frac{1}{24} $$\n$$ Z = -6\\gamma_{11} - 2\\gamma_{21} + 6\\gamma_{31} + \\frac{75+18+1}{24} = -6\\gamma_{11} - 2\\gamma_{21} + 6\\gamma_{31} + \\frac{47}{12} $$\n剩余的独立约束是：\n\\begin{align*}\n\\gamma_{11} + \\gamma_{21} + \\gamma_{31} = \\frac{2}{3} \\\\\n0 \\le \\gamma_{11} \\le \\frac{1}{2} \\\\\n0 \\le \\gamma_{21} \\le \\frac{1}{3} \\\\\n0 \\le \\gamma_{31} \\le \\frac{1}{6}\n\\end{align*}\n为了最小化 $Z$，我们需要使 $\\gamma_{11}$ 和 $\\gamma_{21}$ 尽可能大（因为系数为负，分别为-6和-2），并使 $\\gamma_{31}$ 尽可能小（因为系数为正，为6）。\n我们尝试将 $\\gamma_{31}$ 设置为其最小值，即 $\\gamma_{31} = 0$。约束变为 $\\gamma_{11} + \\gamma_{21} = \\frac{2}{3}$。\n为了最小化目标函数，我们优先使 $\\gamma_{11}$ 最大。我们将 $\\gamma_{11}$ 设置为其上界：$\\gamma_{11} = \\frac{1}{2}$。\n这意味着 $\\gamma_{21} = \\frac{2}{3} - \\gamma_{11} = \\frac{2}{3} - \\frac{1}{2} = \\frac{1}{6}$。\n我们来检查这个解是否可行：\n- $\\gamma_{11} = \\frac{1}{2}$，在 $[0, \\frac{1}{2}]$ 范围内。\n- $\\gamma_{21} = \\frac{1}{6}$，在 $[0, \\frac{1}{3}]$ 范围内。\n- $\\gamma_{31} = 0$，在 $[0, \\frac{1}{6}]$ 范围内。\n该解是可行的，并且根据我们基于目标函数系数的贪心构造，它是最优的。\n\n现在我们求解最优耦合矩阵 $\\Gamma^{\\star}$ 的其余分量：\n$\\gamma^{\\star}_{11} = \\frac{1}{2}$\n$\\gamma^{\\star}_{21} = \\frac{1}{6}$\n$\\gamma^{\\star}_{31} = 0$\n$\\gamma^{\\star}_{12} = p_1 - \\gamma^{\\star}_{11} = \\frac{1}{2} - \\frac{1}{2} = 0$\n$\\gamma^{\\star}_{22} = p_2 - \\gamma^{\\star}_{21} = \\frac{1}{3} - \\frac{1}{6} = \\frac{1}{6}$\n$\\gamma^{\\star}_{32} = p_3 - \\gamma^{\\star}_{31} = \\frac{1}{6} - 0 = \\frac{1}{6}$\n\n最优耦合矩阵为：\n$$ \\Gamma^{\\star} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ \\frac{1}{6}  \\frac{1}{6} \\\\ 0  \\frac{1}{6} \\end{pmatrix} $$\n\n最后，我们将 $\\Gamma^{\\star}$ 代入目标函数，计算最优总运输成本：\n$$ Z^{\\star} = \\sum_{i=1}^{3}\\sum_{j=1}^{2} c_{ij} \\gamma^{\\star}_{ij} $$\n$$ Z^{\\star} = c_{11}\\gamma^{\\star}_{11} + c_{21}\\gamma^{\\star}_{21} + c_{22}\\gamma^{\\star}_{22} + c_{32}\\gamma^{\\star}_{32} $$\n$$ Z^{\\star} = (\\frac{1}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{6}) + (\\frac{9}{4})(\\frac{1}{6}) + (\\frac{1}{4})(\\frac{1}{6}) $$\n$$ Z^{\\star} = \\frac{1}{8} + \\frac{1}{24} + \\frac{9}{24} + \\frac{1}{24} $$\n$$ Z^{\\star} = \\frac{3}{24} + \\frac{1+9+1}{24} = \\frac{3}{24} + \\frac{11}{24} = \\frac{14}{24} $$\n化简分数得到最优成本：\n$$ Z^{\\star} = \\frac{7}{12} $$\n这个结果可以用简化后的目标函数进行验证：\n$Z^{\\star} = -6(\\frac{1}{2}) - 2(\\frac{1}{6}) + 6(0) + \\frac{47}{12} = -3 - \\frac{1}{3} + \\frac{47}{12} = -\\frac{10}{3} + \\frac{47}{12} = -\\frac{40}{12} + \\frac{47}{12} = \\frac{7}{12}$。\n计算结果一致。",
            "answer": "$$\\boxed{\\frac{7}{12}}$$"
        },
        {
            "introduction": "在贝叶斯推断中，我们经常遇到从离散先验（如粒子近似）更新到连续后验的情况。这个练习探讨了在这种情况下经典 Monge 映射的局限性，并阐明了为何必须采用更通用的 Kantorovich 概率耦合。通过推导从离散先验到连续后验不存在确定性映射，你将深入理解这两种最优传输公式的根本区别，并学会如何使用分位数函数计算它们之间的 Wasserstein 距离。",
            "id": "3408183",
            "problem": "考虑一个数据同化中的标量逆问题，其中未知参数为 $x \\in \\mathbb{R}$。初始的先验信念由两点概率测度 $\\mu = p\\,\\delta_{-a} + (1-p)\\,\\delta_{a}$ 表示，其中 $a \\in (0,\\infty)$ 且 $p \\in (0,1)$。假设希望采用最优传输 (Optimal Transport, OT) 方法将先验 $\\mu$ 映射到目标后验分布 $\\nu$，该后验分布关于勒贝格测度是绝对连续的（例如，数据同化中常用的高斯后验近似）。设传输成本为二次成本 $c(x,y) = (x-y)^{2}$。\n\n任务：\n- 仅使用前推测度、狄拉克测度和 Monge 公式的核心定义，证明当 $\\nu$ 具有密度时，不存在确定性可测传输映射 $T:\\mathbb{R}\\to\\mathbb{R}$ 满足 $T_{\\#}\\mu=\\nu$。并由此得出结论：在此情况下，Monge 问题无解，必须转而采用 Kantorovich 传输方案（一种概率耦合）。\n- 为配对 $(\\mu,\\nu)$ 和成本 $c(x,y)=(x-y)^{2}$ 建立 Kantorovich 问题，并回顾该问题的最小值等于 Wasserstein-2 (W2) 距离的平方 $W_{2}^{2}(\\mu,\\nu)$。\n- 将问题具体化到 $p=\\tfrac{1}{2}$，$a=1$，且 $\\nu=\\mathcal{N}(0,1)$（标准正态分布）的情况。仅使用一维最优传输的基本性质（特别是单调重排或分位数表示）和基本微积分，推导出最小 Kantorovich 成本 $W_{2}^{2}(\\mu,\\nu)$ 的闭式解析表达式。\n\n答案规格：\n- 最终答案必须是单一的闭式解析表达式。\n- 无需四舍五入，最终答案中不包含任何单位。",
            "solution": "我们从适用于贝叶斯推断的最优传输的第一性原理出发，从前推测度、Monge 和 Kantorovich 公式的核心定义，以及 Wasserstein-2 (W2) 距离的一维单调重排刻画开始。\n\n1. 从离散先验到连续后验的 Monge 传输映射的不存在性。\n- 设 $\\mu = p\\,\\delta_{-a} + (1-p)\\,\\delta_{a}$，其中 $p \\in (0,1)$ 且 $a \\in (0,\\infty)$。根据定义，狄拉克测度 $\\delta_{x_{0}}$ 将单位质量赋予点 $x_{0}$。测度 $\\mu$ 在可测映射 $T:\\mathbb{R}\\to\\mathbb{R}$下的前推是测度 $T_{\\#}\\mu$，其定义为 $T_{\\#}\\mu(B) = \\mu(T^{-1}(B))$，对所有博雷尔集 $B \\subset \\mathbb{R}$ 成立。\n- 如果 $\\mu$ 是有限支撑的，例如 $\\mu=\\sum_{i=1}^{m}\\alpha_{i}\\,\\delta_{x_{i}}$，其中 $\\alpha_{i}0$，$\\sum_{i}\\alpha_{i}=1$，那么对于任何可测的 $T$，我们有\n$$\nT_{\\#}\\mu \\;=\\; \\sum_{i=1}^{m}\\alpha_{i}\\,\\delta_{T(x_{i})}.\n$$\n因此 $T_{\\#}\\mu$ 也是有限支撑的，最多有 $m$ 个原子。特别地，如果 $m=2$，那么 $T_{\\#}\\mu$ 的支撑集基数最多为 2。\n- 假设目标后验 $\\nu$ 关于勒贝格测度是绝对连续的，并具有密度 $\\rho_{\\nu}(y)$，因此对于每个单点集 $\\{y_{0}\\}$，我们有 $\\nu(\\{y_{0}\\})=0$。一个有限支撑的测度不能等于一个绝对连续的测度。因此，在这种情况下，不存在可测映射 $T$ 使得 $T_{\\#}\\mu=\\nu$。\n- 因此，Monge 问题，即寻找一个确定性可测的 $T$ 满足 $T_{\\#}\\mu=\\nu$ 以最小化传输成本 $\\int (x-T(x))^{2}\\,\\mathrm{d}\\mu(x)$，无解。必须转而使用 Kantorovich 公式，该公式允许通过耦合进行质量分裂。\n\n2. Kantorovich 公式与 Wasserstein-2 距离。\n- Kantorovich 问题旨在寻找一个在 $\\mathbb{R}\\times\\mathbb{R}$ 上的耦合 $\\gamma$，其边际为 $\\mu$ 和 $\\nu$，即 $\\gamma \\in \\Gamma(\\mu,\\nu)$，其中\n$$\n\\Gamma(\\mu,\\nu) \\;=\\; \\left\\{ \\gamma \\in \\mathcal{P}(\\mathbb{R}\\times\\mathbb{R}) \\,:\\, \\gamma(A\\times \\mathbb{R})=\\mu(A),\\; \\gamma(\\mathbb{R}\\times B)=\\nu(B)\\;\\text{for all Borel }A,B \\right\\},\n$$\n以最小化期望成本\n$$\n\\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int_{\\mathbb{R}\\times\\mathbb{R}} (x-y)^{2}\\,\\mathrm{d}\\gamma(x,y).\n$$\n- 根据定义，这个下确界等于 Wasserstein-2 (W2) 距离的平方：\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; \\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int_{\\mathbb{R}\\times\\mathbb{R}} (x-y)^{2}\\,\\mathrm{d}\\gamma(x,y).\n$$\n由于 $\\nu$ 是绝对连续的，最优耦合存在，并且在一维情况下，它由单调重排耦合导出。\n\n3. 计算 $p=\\tfrac{1}{2}$，$a=1$ 且 $\\nu=\\mathcal{N}(0,1)$ 时的 $W_{2}^{2}$。\n- 在一维中，二次成本的最优耦合由递增重排给出，并且可以写作\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; \\int_{0}^{1} \\bigl(F_{\\mu}^{-1}(u) - F_{\\nu}^{-1}(u)\\bigr)^{2}\\,\\mathrm{d}u,\n$$\n其中 $F_{\\mu}^{-1}$ 和 $F_{\\nu}^{-1}$ 是 $\\mu$ 和 $\\nu$ 的分位数函数。这是一维最优传输中一个标准且经过充分检验的结果。\n- 对于 $\\mu=\\tfrac{1}{2}\\delta_{-1}+\\tfrac{1}{2}\\delta_{1}$，其分位数函数为\n$$\nF_{\\mu}^{-1}(u) \\;=\\; \\begin{cases}\n-1,  u \\in [0,\\tfrac{1}{2}),\\\\\n1,  u \\in [\\tfrac{1}{2},1].\n\\end{cases}\n$$\n对于 $\\nu=\\mathcal{N}(0,1)$，$F_{\\nu}^{-1}(u)=\\Phi^{-1}(u)$，其中 $\\Phi$ 是标准正态累积分布函数，$\\Phi^{-1}$ 是其分位数函数。\n- 因此，\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; \\int_{0}^{\\tfrac{1}{2}} \\bigl(-1 - \\Phi^{-1}(u)\\bigr)^{2} \\,\\mathrm{d}u \\;+\\; \\int_{\\tfrac{1}{2}}^{1} \\bigl(1 - \\Phi^{-1}(u)\\bigr)^{2} \\,\\mathrm{d}u.\n$$\n在第二个积分中使用对称性 $\\Phi^{-1}(1-u)=-\\Phi^{-1}(u)$ 和换元 $v=1-u$，两个积分相等，得到\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; 2 \\int_{0}^{\\tfrac{1}{2}} \\bigl(\\Phi^{-1}(u)+1\\bigr)^{2}\\,\\mathrm{d}u.\n$$\n- 进行变量替换 $z=\\Phi^{-1}(u)$，因此 $\\mathrm{d}u=\\phi(z)\\,\\mathrm{d}z$，其中 $\\phi(z)=\\tfrac{1}{\\sqrt{2\\pi}}\\exp(-z^{2}/2)$ 是标准正态密度。当 $u$ 从 $0$ 变到 $\\tfrac{1}{2}$ 时，$z$ 从 $-\\infty$ 变到 $0$。因此\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; 2 \\int_{-\\infty}^{0} (z+1)^{2}\\,\\phi(z)\\,\\mathrm{d}z \\;=\\; 2 \\left[ \\int_{-\\infty}^{0} z^{2}\\phi(z)\\,\\mathrm{d}z \\;+\\; 2 \\int_{-\\infty}^{0} z\\,\\phi(z)\\,\\mathrm{d}z \\;+\\; \\int_{-\\infty}^{0} \\phi(z)\\,\\mathrm{d}z \\right].\n$$\n我们现在使用标准正态分布的基本性质来计算每一项：\n- 由于 $z^{2}\\phi(z)$ 是偶函数且 $\\int_{\\mathbb{R}} z^{2}\\phi(z)\\,\\mathrm{d}z = 1$，我们有\n$$\n\\int_{-\\infty}^{0} z^{2}\\phi(z)\\,\\mathrm{d}z \\;=\\; \\tfrac{1}{2}.\n$$\n- 使用 $\\tfrac{\\mathrm{d}}{\\mathrm{d}z}\\phi(z)=-z\\phi(z)$，我们得到\n$$\n\\int_{-\\infty}^{0} z\\,\\phi(z)\\,\\mathrm{d}z \\;=\\; -\\phi(0) \\;=\\; -\\tfrac{1}{\\sqrt{2\\pi}}.\n$$\n- 根据对称性，\n$$\n\\int_{-\\infty}^{0} \\phi(z)\\,\\mathrm{d}z \\;=\\; \\tfrac{1}{2}.\n$$\n代入得，\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; 2 \\left[ \\tfrac{1}{2} \\;+\\; 2\\left(-\\tfrac{1}{\\sqrt{2\\pi}}\\right) \\;+\\; \\tfrac{1}{2} \\right] \\;=\\; 2 \\left[ 1 \\;-\\; \\tfrac{2}{\\sqrt{2\\pi}} \\right] \\;=\\; 2 \\;-\\; \\tfrac{4}{\\sqrt{2\\pi}}.\n$$\n这个值是在二次成本下将两点先验 $\\mu$ 映射到连续目标 $\\nu$ 的最小 Kantorovich 传输成本，它量化了在 Monge 映射不存在时诉诸概率耦合的必要性（以及代价）。",
            "answer": "$$\\boxed{2-\\frac{4}{\\sqrt{2\\pi}}}$$"
        },
        {
            "introduction": "虽然经典的最优传输提供了深刻的几何见解，但其在处理大规模问题时面临计算瓶颈。这个练习将向你介绍一种现代计算方法——熵正则化最优传输，及其高效的 Sinkhorn 算法。你不仅需要实现该算法来近似贝叶斯后验映射，还需要从理论上分析正则化引入的偏差，从而在计算效率和近似精度之间建立起重要的联系。",
            "id": "3408171",
            "problem": "考虑一个一维空间中的贝叶斯逆问题，其中未知参数为 $\\theta \\in \\mathbb{R}$。设先验为高斯分布 $\\mu = \\mathcal{N}(0,\\tau^2)$，其中 $\\tau  0$，观测模型为 $y_{\\mathrm{obs}} = G(\\theta) + \\eta$，其中 $G(\\theta) = \\theta$，噪声 $\\eta$ 为零均值高斯噪声 $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma  0$。根据贝叶斯法则，后验分布为高斯分布 $\\nu = p(\\theta \\mid y_{\\mathrm{obs}}) = \\mathcal{N}(m,s^2)$，其中 $s^2 = \\left(\\tau^{-2} + \\sigma^{-2}\\right)^{-1}$ 且 $m = s^2 \\, \\sigma^{-2} \\, y_{\\mathrm{obs}}$。我们考虑通过最优输运（OT）方法进行贝叶斯推断，使用从先验到后验的输运映射的熵正则化近似（Sinkhorn）。\n\n使用的基本定义：\n- 两个在 $\\mathbb{R}$ 上的概率测度 $\\mu$ 和 $\\nu$ 之间的一个耦合 $\\pi$ 是一个在 $\\mathbb{R} \\times \\mathbb{R}$ 上的联合概率测度，其边际分别为 $\\mu$ 和 $\\nu$。（Kantorovich）最优输运问题对于成本函数 $c(x,y)$ 旨在寻求 $\\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int c(x,y) \\, d\\pi(x,y)$。\n- 熵正则化OT问题将目标函数替换为 $\\int c(x,y) \\, d\\pi(x,y) + \\varepsilon \\, \\mathrm{KL}(\\pi \\,\\|\\, \\mu \\otimes \\nu)$，其中正则化参数 $\\varepsilon  0$，$\\mathrm{KL}$ 表示 Kullback–Leibler 散度，$\\mu \\otimes \\nu$ 是乘积测度。在离散、等权重的情况下，并使用二次成本 $c(x,y) = \\tfrac{1}{2}\\lVert x - y \\rVert^2$，吉布斯核为 $K_{ij} = \\exp\\left(-C_{ij}/\\varepsilon\\right)$，其中 $C_{ij} = \\tfrac{1}{2}(x_i - y_j)^2$，Sinkhorn 缩放算法产生一个具有指定边际的耦合 $\\pi_\\varepsilon$。\n- 与 $\\pi_\\varepsilon$ 相关的重心投影输运近似是映射 $T_\\varepsilon(x) = \\mathbb{E}[Y \\mid X=x]$，它是从 $\\pi_\\varepsilon$ 计算得出的。相应的近似后验是前推 $T_\\varepsilon \\# \\mu$。\n\n推导任务：\n1. 从贝叶斯法则得到的 $\\nu$、OT 的 Kantorovich 公式以及熵正则化的定义出发，为二次成本 $c(x,y) = \\tfrac{1}{2}\\lVert x - y \\rVert^2$ 下经验测度 $\\hat{\\mu}$ 和 $\\hat{\\nu}$（由从 $\\mu$ 和 $\\nu$ 中抽取的独立同分布样本构成）之间的离散 Sinkhorn 耦合 $\\pi_\\varepsilon$ 的构建提供理由，并定义重心映射 $T_\\varepsilon$。\n2. 对于 Lipschitz 常数为 $L$ 的 Lipschitz 函数 $f:\\mathbb{R} \\to \\mathbb{R}$，推导用 $\\mathbb{E}_{T_\\varepsilon \\# \\mu}[f(\\theta)]$ 估计 $\\mathbb{E}_{\\nu}[f(\\theta)]$ 的偏差的一个界，该界用 $\\varepsilon$ 和 $L$ 表示，在 $\\varepsilon \\to 0$ 时成立。从 Kantorovich–Rubinstein 型不等式 $|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| \\leq L \\, \\mathbb{E}_{\\pi_\\varepsilon}\\big[\\lVert Y - T_\\varepsilon(X)\\rVert\\big]$ 开始，并且仅使用适用于二次成本的标准凸性和集中性论证，获得一个与 $\\varepsilon$ 相关的显式缩放关系。清晰地陈述你的界成立所需的任何条件，并确保每一步都源于一个基本原理或一个经过充分检验的结果（例如，对数凹测度或 Poincaré 不等式的性质）。\n3. 将前面的界具体化到一维情况以及具有已知 Lipschitz 常数的函数 $f$。\n\n数值实验与测试套件：\n- 固定 $d=1$（一维参数），$\\tau = 1$，$\\sigma = 0.3$，以及 $y_{\\mathrm{obs}} = 0.7$。这些参数定义了后验分布 $\\nu = \\mathcal{N}(m,s^2)$，其中 $s^2 = (1/\\tau^2 + 1/\\sigma^2)^{-1}$ 且 $m = s^2(y_{\\mathrm{obs}}/\\sigma^2)$。从 $\\mu = \\mathcal{N}(0,\\tau^2)$ 中抽取 $n=128$ 个独立同分布样本 $\\{x_i\\}_{i=1}^n$，并从 $\\nu = \\mathcal{N}(m,s^2)$ 中抽取 $\\{y_j\\}_{j=1}^n$ 个样本，以形成经验测度 $\\hat{\\mu}$ 和 $\\hat{\\nu}$。\n- 对于每个 $\\varepsilon \\in \\{1.0, 0.5, 0.2\\}$，计算 $\\hat{\\mu}$ 和 $\\hat{\\nu}$ 之间在成本函数 $c(x,y) = \\tfrac{1}{2}(x - y)^2$ 下的 Sinkhorn 耦合 $\\pi_\\varepsilon$，以及相关的重心映射值 $T_\\varepsilon(x_i)$。考虑以下函数：\n  - $f_1(\\theta) = \\theta$，Lipschitz 常数为 $L_1 = 1$。\n  - $f_2(\\theta) = |\\theta|$，Lipschitz 常数为 $L_2 = 1$。\n  - $f_3(\\theta) = \\sin(2\\theta)$，Lipschitz 常数为 $L_3 = 2$。\n- 对于每对 $(\\varepsilon, f_k)$，其中 $k \\in \\{1,2,3\\}$，计算经验偏差 $B(\\varepsilon,f_k) = \\left|\\frac{1}{n}\\sum_{j=1}^n f_k(y_j) - \\frac{1}{n}\\sum_{i=1}^n f_k\\big(T_\\varepsilon(x_i)\\big)\\right|$。利用你推导的一维 $\\varepsilon$-缩放关系，构建一个仅基于 $\\varepsilon$ 和 $L_k$（没有不可观测的常数）的显式的、可计算的上界 $U(\\varepsilon,f_k)$，并测试 $B(\\varepsilon,f_k) \\leq U(\\varepsilon,f_k) + \\delta$ 是否在固定的数值容差 $\\delta = 0.05$ 下成立，以考虑采样误差和有限迭代的数值计算误差。\n- 测试套件包含由 $\\{\\varepsilon \\in \\{1.0, 0.5, 0.2\\}\\}$ 和 $\\{f_1,f_2,f_3\\}$ 的笛卡尔积所产生的 9 个案例。对于每个案例，程序必须输出一个布尔值，指示经验不等式是否满足。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个逗号分隔的列表，内含 9 个布尔值，顺序为 $\\varepsilon = 1.0, 0.5, 0.2$ 和 $f_1, f_2, f_3$（按此嵌套顺序），并用方括号括起来（例如，`[True,False,...]`）。不涉及物理单位，也没有出现角度，因此不需要进行单位转换。",
            "solution": "该问题是有效的。这是一个定义明确且具有科学依据的计算统计学练习，它将贝叶斯推断与计算最优输运联系起来。所有定义都是标准的，数值实验所需的所有参数均已提供。\n\n### 1. 离散 Sinkhorn 方法的合理性证明\n\n任务是从先验分布 $\\mu = \\mathcal{N}(0,\\tau^2)$ 出发，使用最优输运映射来近似贝叶斯后验分布 $\\nu = \\mathcal{N}(m,s^2)$。经典的（Kantorovich）最优输运问题寻求一个在 $\\mu$ 和 $\\nu$ 之间的耦合 $\\pi$，以最小化总输运成本，即 $\\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int c(x,y) \\, d\\pi(x,y)$。对于二次成本 $c(x,y) = \\frac{1}{2}(x-y)^2$，Brenier 定理指出，最优耦合是确定性的，即 $\\pi = (\\mathrm{Id} \\times T_0)_\\# \\mu$，其中 $T_0: \\mathbb{R} \\to \\mathbb{R}$ 是最优输运映射。\n\n该问题的熵正则化引入了一个惩罚项，将目标函数修改为寻找一个耦合 $\\pi_\\varepsilon$ 来最小化 $\\int c(x,y) \\, d\\pi(x,y) + \\varepsilon \\, \\mathrm{KL}(\\pi \\,\\|\\, \\mu \\otimes \\nu)$，其中 $\\varepsilon  0$ 是正则化强度。这个正则化问题是严格凸的，并且总是有唯一解 $\\pi_\\varepsilon$，该解关于 $\\mu \\otimes \\nu$ 绝对连续。最优耦合 $\\pi_\\varepsilon$ 不再是确定性的，而是最优映射 $T_0$ 的一个“模糊”版本。\n\n为了进行数值计算，我们通过从 $\\mu$ 中抽取 $n$ 个独立同分布样本 $\\{x_i\\}_{i=1}^n$ 和从 $\\nu$ 中抽取 $n$ 个独立同分布样本 $\\{y_j\\}_{j=1}^n$ 来离散化测度 $\\mu$ 和 $\\nu$。这得到了经验测度 $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}$ 和 $\\hat{\\nu} = \\frac{1}{n}\\sum_{j=1}^n \\delta_{y_j}$。熵正则化OT问题的离散版本是找到一个输运方案（一个矩阵） $\\pi_\\varepsilon \\in \\mathbb{R}^{n \\times n}$，使其最小化：\n$$\n\\sum_{i,j=1}^n \\pi_{ij} C_{ij} + \\varepsilon \\sum_{i,j=1}^n \\pi_{ij} \\log(n^2 \\pi_{ij})\n$$\n在边际约束条件 $\\sum_{j=1}^n \\pi_{ij} = \\frac{1}{n}$（对所有 $i$）和 $\\sum_{i=1}^n \\pi_{ij} = \\frac{1}{n}$（对所有 $j$）下。成本矩阵为 $C_{ij} = \\frac{1}{2}(x_i - y_j)^2$。\n\n这个最小化问题的解具有一个特定的结构：$\\pi_{ij} = u_i K_{ij} v_j$，其中 $K_{ij} = \\exp(-C_{ij}/\\varepsilon)$ 是吉布斯核，$u, v \\in \\mathbb{R}^n$ 是缩放向量。Sinkhorn 算法是寻找这些向量的迭代过程。给定目标均匀边际 $a=b=\\frac{1}{n}\\mathbf{1}$，该算法交替缩放 $K$ 的行和列：\n$$\nu \\leftarrow \\frac{a}{K v} \\quad \\text{和} \\quad v \\leftarrow \\frac{b}{K^T u}\n$$\n（其中除法是逐元素的）直到收敛。一旦找到最优耦合 $\\pi_\\varepsilon$，重心映射 $T_\\varepsilon$ 被定义为目标变量 $Y$ 在给定源变量 $X$ 条件下的条件期望。对于离散样本，这表示为：\n$$\nT_\\varepsilon(x_i) = \\mathbb{E}_{\\pi_\\varepsilon}[Y | X=x_i] = \\sum_{j=1}^n y_j P(Y=y_j | X=x_i) = \\sum_{j=1}^n y_j \\frac{\\pi_{ij}}{\\sum_{k=1}^n \\pi_{ik}} = \\sum_{j=1}^n y_j \\frac{\\pi_{ij}}{1/n} = n \\sum_{j=1}^n y_j \\pi_{ij}\n$$\n然后，前推测度 $T_\\varepsilon \\# \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n \\delta_{T_\\varepsilon(x_i)}$ 被用作后验分布 $\\hat{\\nu}$ 的基于输运的近似。\n\n### 2. 偏差界的推导\n\n我们的目标是为 Lipschitz 常数为 $L$ 的函数 $f$ 的偏差 $|\\mathbb{E}_{\\nu}[f(\\theta)] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f(\\theta)]|$ 定界。问题提供了出发点：\n$$\n|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| = \\left| \\iint (f(y) - f(T_\\varepsilon(x))) d\\pi_\\varepsilon(x,y) \\right| \\leq L \\iint |y - T_\\varepsilon(x)| d\\pi_\\varepsilon(x,y) = L \\, \\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|]\n$$\n通过对凹函数 $\\phi(z)=\\sqrt{z}$ 应用 Jensen 不等式，我们有 $(\\mathbb{E}[Z])^2 \\leq \\mathbb{E}[Z^2]$。因此：\n$$\n\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|] \\leq \\sqrt{\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|^2]}\n$$\n根据定义，$T_\\varepsilon(X) = \\mathbb{E}_{\\pi_\\varepsilon}[Y|X]$。因此，平方根内的项是 Y 在给定 X 条件下的方差，再对 X 求平均：\n$$\n\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|^2] = \\mathbb{E}_{X \\sim \\mu}[\\mathrm{Var}_{\\pi_\\varepsilon(Y|X)}(Y)]\n$$\n为了界定这个方差，我们分析条件分布 $\\pi_\\varepsilon(y|x)$ 的结构。它关于勒贝格测度的密度正比于 $e^{(-c(x,y) + \\phi(x) + \\psi(y))/\\varepsilon}$，其中 $\\phi, \\psi$ 是对偶 OT 势。一个更直接的论证依赖于以下事实：条件测度 $d\\pi_\\varepsilon(y|x) \\propto e^{-c(x,y)/\\varepsilon} e^{\\psi(y)/\\varepsilon} d\\nu(y)$。其密度的负对数是 $V_x(y) = \\frac{c(x,y)}{\\varepsilon} - \\frac{\\psi(y)}{\\varepsilon} - \\log p_\\nu(y)$，其中 $p_\\nu(y)$ 是 $\\nu=\\mathcal{N}(m,s^2)$ 的密度。$\\nu$ 的负对数密度是 $\\frac{1}{2s^2}(y-m)^2$（不计常数）。所以，\n$$\nV_x(y) = \\frac{1}{2\\varepsilon}(y-x)^2 - \\frac{\\psi(y)}{\\varepsilon} + \\frac{1}{2s^2}(y-m)^2 + \\text{const}\n$$\nOT 理论中的一个关键性质是，对于二次成本和对数凹测度（高斯分布即是），对偶势 $\\psi(y)$ 是一个凹函数。因此，其二阶导数 $\\psi''(y) \\leq 0$。$V_x(y)$ 关于 $y$ 的二阶导数是：\n$$\nV_x''(y) = \\frac{1}{\\varepsilon} - \\frac{\\psi''(y)}{\\varepsilon} + \\frac{1}{s^2} \\geq \\frac{1}{\\varepsilon} + \\frac{1}{s^2}\n$$\n这表明条件分布 $\\pi_\\varepsilon(y|x)$ 是强对数凹的，其参数 $\\lambda \\geq \\frac{1}{\\varepsilon} + \\frac{1}{s^2}$。Brascamp-Lieb 不等式指出，参数为 $\\lambda$ 的强对数凹分布的方差的界为 $1/\\lambda$。因此，\n$$\n\\mathrm{Var}_{\\pi_\\varepsilon(Y|X=x)}(Y) \\leq \\left(\\frac{1}{\\varepsilon} + \\frac{1}{s^2}\\right)^{-1} = \\frac{\\varepsilon s^2}{\\varepsilon + s^2}\n$$\n这个界对 x 是一致的。对 $X \\sim \\mu$ 取期望得到：\n$$\n\\mathbb{E}_{X \\sim \\mu}[\\mathrm{Var}_{\\pi_\\varepsilon(Y|X)}(Y)] \\leq \\frac{\\varepsilon s^2}{\\varepsilon + s^2}\n$$\n将此结果代入我们的不等式链中，我们得到偏差的最终界：\n$$\n|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| \\leq L \\sqrt{\\frac{\\varepsilon s^2}{\\varepsilon + s^2}}\n$$\n\n### 3. 具体化与数值测试\n\n对于一维数值实验，后验方差为 $s^2 = (1/\\tau^2 + 1/\\sigma^2)^{-1}$，后验均值为 $m = s^2(y_{\\mathrm{obs}}/\\sigma^2)$。上面推导出的界是直接适用的。对于 Lipschitz 常数为 $L_k$ 的函数 $f_k$，可计算的上界为：\n$$\nU(\\varepsilon, f_k) = L_k \\sqrt{\\frac{\\varepsilon s^2}{\\varepsilon + s^2}}\n$$\n数值测试包括计算经验偏差 $B(\\varepsilon, f_k) = |\\frac{1}{n}\\sum_j f_k(y_j) - \\frac{1}{n}\\sum_i f_k(T_\\varepsilon(x_i))|$ 并检查不等式 $B(\\varepsilon, f_k) \\leq U(\\varepsilon, f_k) + \\delta$ 是否成立，其中 $\\delta = 0.05$ 是一个容差，用于考虑采样带来的统计波动以及有限迭代 Sinkhorn 算法产生的数值误差。\n\n对于 $f_1(\\theta) = \\theta$ 的特殊情况，理论偏差精确为零，如下所示：\n$$\n\\mathbb{E}_{T_\\varepsilon \\# \\mu}[\\theta] = \\int T_\\varepsilon(x)d\\mu(x) = \\iint y \\, d\\pi_\\varepsilon(y|x)d\\mu(x) = \\iint y \\, d\\pi_\\varepsilon(x,y) = \\int y \\, d\\nu(y) = \\mathbb{E}_\\nu[\\theta]\n$$\n因此，$\\mathbb{E}_{\\nu}[f_1] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f_1] = 0$。经验偏差 $B(\\varepsilon,f_1)$ 应接近于零，仅受数值精度的限制。界 $U(\\varepsilon, f_1)$ 是非零的，因此预期不等式会成立。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal transport problem for Bayesian inference as specified.\n    \"\"\"\n    # Problem Parameters\n    tau = 1.0\n    sigma = 0.3\n    y_obs = 0.7\n    n = 128\n    delta = 0.05\n    \n    # Random number generator for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Posterior distribution parameters\n    tau_sq_inv = 1.0 / (tau**2)\n    sigma_sq_inv = 1.0 / (sigma**2)\n    s_sq = 1.0 / (tau_sq_inv + sigma_sq_inv)\n    m = s_sq * (y_obs / sigma**2)\n    s = np.sqrt(s_sq)\n\n    # Generate samples from prior and posterior\n    # x ~ N(0, tau^2)\n    samples_x = rng.normal(loc=0.0, scale=tau, size=n).reshape(-1, 1)\n    # y ~ N(m, s^2)\n    samples_y = rng.normal(loc=m, scale=s, size=n).reshape(-1, 1)\n\n    # Define test functions and their Lipschitz constants\n    f1 = lambda theta: theta\n    f2 = lambda theta: np.abs(theta)\n    f3 = lambda theta: np.sin(2 * theta)\n    functions = [(f1, 1.0), (f2, 1.0), (f3, 2.0)]\n\n    # Epsilon values for regularization\n    epsilons = [1.0, 0.5, 0.2]\n    \n    # Store results\n    results = []\n    \n    # Helper for Sinkhorn algorithm\n    def compute_sinkhorn_coupling(x, y, epsilon, num_iter=1000, tol=1e-9):\n        \"\"\"\n        Computes the entropic OT coupling using Sinkhorn's algorithm.\n        \"\"\"\n        n_samples = x.shape[0]\n        \n        # Cost matrix C_ij = 0.5 * (x_i - y_j)^2\n        C = 0.5 * (x - y.T)**2\n        \n        # Gibbs kernel\n        K = np.exp(-C / epsilon)\n        \n        # Target marginals (uniform)\n        a = np.ones(n_samples) / n_samples\n        b = np.ones(n_samples) / n_samples\n        \n        # Sinkhorn iterations\n        v = np.ones(n_samples)\n        for _ in range(num_iter):\n            u = a / (K @ v + tol)\n            v = b / (K.T @ u + tol)\n            \n        # Coupling matrix P_ij = u_i * K_ij * v_j\n        pi_eps = np.diag(u) @ K @ np.diag(v)\n        return pi_eps\n\n    # Main loop for experiments\n    for epsilon in epsilons:\n        # 1. Compute Sinkhorn coupling\n        pi_epsilon = compute_sinkhorn_coupling(samples_x, samples_y, epsilon)\n        \n        # 2. Compute barycentric map T_eps(x_i)\n        # T_eps(x_i) = n * sum_j(y_j * pi_ij)\n        T_eps_x = n * (pi_epsilon @ samples_y)\n        \n        for f, L in functions:\n            # 3. Compute empirical bias B(epsilon, f_k)\n            # B = | E_nu[f(y)] - E_{T#mu}[f(T(x))] |\n            # E_nu[f(y)] ~= (1/n) * sum(f(y_j))\n            # E_{T#mu}[f(T(x))] ~= (1/n) * sum(f(T(x_i)))\n            \n            bias_empirical = np.abs(np.mean(f(samples_y)) - np.mean(f(T_eps_x)))\n            \n            # 4. Compute theoretical bound U(epsilon, f_k)\n            # U = L * sqrt( (epsilon * s^2) / (epsilon + s^2) )\n            bound_theoretical = L * np.sqrt((epsilon * s_sq) / (epsilon + s_sq))\n            \n            # 5. Check inequality B = U + delta\n            is_satisfied = bias_empirical = bound_theoretical + delta\n            results.append(is_satisfied)\n            \n    # Print results in the specified format\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n```"
        }
    ]
}