{
    "hands_on_practices": [
        {
            "introduction": "The theoretical foundation of optimal transport is the Kantorovich problem, which seeks a \"coupling\" between two probability measures that minimizes a total transportation cost. To make this abstract concept concrete, this first exercise guides you through the process of translating the Kantorovich formulation into a standard linear program for discrete probability measures . By working through a small, tangible example, you will build intuition for the structure of the optimal transport plan and the nature of the constraints that ensure mass is conserved between the prior and posterior distributions.",
            "id": "3408143",
            "problem": "Consider Bayesian inference in a discrete setting in which a discrete prior distribution supported on particles $\\{x_i\\}_{i=1}^{m}$ with probabilities $\\{p_i\\}_{i=1}^{m}$ is updated to a discrete posterior distribution supported on particles $\\{y_j\\}_{j=1}^{n}$ with probabilities $\\{q_j\\}_{j=1}^{n}$. Under the quadratic transportation cost, the Bayesian update can be represented by an optimal transport plan that couples the prior to the posterior by solving a linear program.\n\nTask 1 (formulation): Starting only from the Kantorovich formulation of optimal transport, write the linear program that seeks a coupling matrix $\\Gamma=\\{\\gamma_{ij}\\}\\in\\mathbb{R}_{+}^{m\\times n}$ minimizing the expected quadratic cost, subject to mass conservation with respect to the prior and posterior probabilities. Clearly state the objective and constraints in mathematical form.\n\nTask 2 (computation on a small example): Let $m=3$ and $n=2$. Consider one-dimensional particles and probabilities\n- prior support $\\{x_1,x_2,x_3\\}=\\{0,1,3\\}$ with probabilities $\\{p_1,p_2,p_3\\}=\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}\\right\\}$,\n- posterior support $\\{y_1,y_2\\}=\\left\\{\\frac{1}{2},\\frac{5}{2}\\right\\}$ with probabilities $\\{q_1,q_2\\}=\\left\\{\\frac{2}{3},\\frac{1}{3}\\right\\}$.\n\nUse the quadratic cost $c_{ij}=\\|x_i-y_j\\|^{2}$ on $\\mathbb{R}$. Compute an optimal coupling $\\Gamma^{\\star}$ for this instance by solving the linear program from Task 1. Then compute the resulting optimal total quadratic transport cost $\\sum_{i=1}^{3}\\sum_{j=1}^{2}\\gamma^{\\star}_{ij}\\,\\|x_i-y_j\\|^{2}$.\n\nAnswer specification: Report only the optimal total quadratic transport cost as an exact value in simplest fractional form. Do not round. Do not include units or any additional text in your final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of optimal transport, is well-posed with all necessary information provided, and is expressed objectively. The problem consists of two tasks: first, to formulate a linear program for the discrete optimal transport problem, and second, to solve a specific instance of this problem.\n\n**Task 1: Formulation of the Linear Program**\n\nThe problem of finding an optimal transport plan between two discrete probability measures is known as the Kantorovich problem. Let the prior measure be $\\mu = \\sum_{i=1}^{m} p_i \\delta_{x_i}$ and the posterior measure be $\\nu = \\sum_{j=1}^{n} q_j \\delta_{y_j}$, where $\\delta_z$ denotes the Dirac delta measure at point $z$. The probabilities satisfy $\\sum_{i=1}^{m} p_i = 1$ and $\\sum_{j=1}^{n} q_j = 1$.\n\nA coupling, or transport plan, between $\\mu$ and $\\nu$ is a measure on the product space whose marginals are $\\mu$ and $\\nu$. In the discrete case, this corresponds to a matrix $\\Gamma = \\{\\gamma_{ij}\\} \\in \\mathbb{R}^{m \\times n}$, where $\\gamma_{ij}$ represents the amount of mass transported from location $x_i$ to location $y_j$. For $\\Gamma$ to be a valid coupling, it must satisfy the marginal constraints:\n1.  The sum of mass transported from $x_i$ must equal the mass $p_i$ at that point. This corresponds to the row sums of $\\Gamma$:\n    $$ \\sum_{j=1}^{n} \\gamma_{ij} = p_i, \\quad \\text{for } i = 1, 2, \\ldots, m $$\n2.  The sum of mass transported to $y_j$ must equal the mass $q_j$ at that point. This corresponds to the column sums of $\\Gamma$:\n    $$ \\sum_{i=1}^{m} \\gamma_{ij} = q_j, \\quad \\text{for } j = 1, 2, \\ldots, n $$\n3.  The transported mass cannot be negative:\n    $$ \\gamma_{ij} \\ge 0, \\quad \\text{for all } i, j $$\n\nThe cost of transporting a unit of mass from $x_i$ to $y_j$ is given by the squared Euclidean distance, $c_{ij} = \\|x_i - y_j\\|^2$. The total transportation cost is the expected cost with respect to the coupling $\\Gamma$. The goal is to find the coupling $\\Gamma^{\\star}$ that minimizes this total cost.\n\nThis problem can be formulated as a linear program:\n\n**Objective Function:**\nMinimize the total transport cost $Z$:\n$$ \\min_{\\Gamma} Z = \\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{ij} \\gamma_{ij} $$\n\n**Constraints:**\nSubject to:\n$$\n\\begin{align*}\n\\sum_{j=1}^{n} \\gamma_{ij} = p_i,  \\forall i \\in \\{1, \\ldots, m\\} \\\\\n\\sum_{i=1}^{m} \\gamma_{ij} = q_j,  \\forall j \\in \\{1, \\ldots, n\\} \\\\\n\\gamma_{ij} \\ge 0,  \\forall i \\in \\{1, \\ldots, m\\}, \\forall j \\in \\{1, \\ldots, n\\}\n\\end{align*}\n$$\n\n**Task 2: Computation on a Small Example**\n\nWe are given the following instance:\n-   Prior support $\\{x_1, x_2, x_3\\} = \\{0, 1, 3\\}$ with probabilities $\\{p_1, p_2, p_3\\} = \\{\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\}$.\n-   Posterior support $\\{y_1, y_2\\} = \\{\\frac{1}{2}, \\frac{5}{2}\\}$ with probabilities $\\{q_1, q_2\\} = \\{\\frac{2}{3}, \\frac{1}{3}\\}$.\n\nFirst, we compute the cost matrix $C = \\{c_{ij}\\}$ where $c_{ij} = (x_i - y_j)^2$:\n$c_{11} = (0 - \\frac{1}{2})^2 = \\frac{1}{4}$\n$c_{12} = (0 - \\frac{5}{2})^2 = \\frac{25}{4}$\n$c_{21} = (1 - \\frac{1}{2})^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n$c_{22} = (1 - \\frac{5}{2})^2 = (-\\frac{3}{2})^2 = \\frac{9}{4}$\n$c_{31} = (3 - \\frac{1}{2})^2 = (\\frac{5}{2})^2 = \\frac{25}{4}$\n$c_{32} = (3 - \\frac{5}{2})^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n\nThe cost matrix is:\n$$ C = \\begin{pmatrix} \\frac{1}{4}  \\frac{25}{4} \\\\ \\frac{1}{4}  \\frac{9}{4} \\\\ \\frac{25}{4}  \\frac{1}{4} \\end{pmatrix} $$\n\nThe linear program is to minimize $Z = \\frac{1}{4}\\gamma_{11} + \\frac{25}{4}\\gamma_{12} + \\frac{1}{4}\\gamma_{21} + \\frac{9}{4}\\gamma_{22} + \\frac{25}{4}\\gamma_{31} + \\frac{1}{4}\\gamma_{32}$ subject to:\n\\begin{align*}\n\\gamma_{11} + \\gamma_{12} = \\frac{1}{2} \\\\\n\\gamma_{21} + \\gamma_{22} = \\frac{1}{3} \\\\\n\\gamma_{31} + \\gamma_{32} = \\frac{1}{6} \\\\\n\\gamma_{11} + \\gamma_{21} + \\gamma_{31} = \\frac{2}{3} \\\\\n\\gamma_{12} + \\gamma_{22} + \\gamma_{32} = \\frac{1}{3} \\\\\n\\gamma_{ij} \\ge 0\n\\end{align*}\nWe use the row sum constraints to express $\\gamma_{i2}$ in terms of $\\gamma_{i1}$: $\\gamma_{12} = \\frac{1}{2} - \\gamma_{11}$, $\\gamma_{22} = \\frac{1}{3} - \\gamma_{21}$, $\\gamma_{32} = \\frac{1}{6} - \\gamma_{31}$. Substituting these into the objective function:\n$$ Z = \\frac{1}{4}\\gamma_{11} + \\frac{25}{4}(\\frac{1}{2} - \\gamma_{11}) + \\frac{1}{4}\\gamma_{21} + \\frac{9}{4}(\\frac{1}{3} - \\gamma_{21}) + \\frac{25}{4}\\gamma_{31} + \\frac{1}{4}(\\frac{1}{6} - \\gamma_{31}) $$\n$$ Z = (\\frac{1}{4} - \\frac{25}{4})\\gamma_{11} + (\\frac{1}{4} - \\frac{9}{4})\\gamma_{21} + (\\frac{25}{4} - \\frac{1}{4})\\gamma_{31} + \\frac{25}{8} + \\frac{9}{12} + \\frac{1}{24} $$\n$$ Z = -6\\gamma_{11} - 2\\gamma_{21} + 6\\gamma_{31} + \\frac{75+18+1}{24} = -6\\gamma_{11} - 2\\gamma_{21} + 6\\gamma_{31} + \\frac{47}{12} $$\nThe remaining independent constraints are:\n\\begin{align*}\n\\gamma_{11} + \\gamma_{21} + \\gamma_{31} = \\frac{2}{3} \\\\\n0 \\le \\gamma_{11} \\le \\frac{1}{2} \\\\\n0 \\le \\gamma_{21} \\le \\frac{1}{3} \\\\\n0 \\le \\gamma_{31} \\le \\frac{1}{6}\n\\end{align*}\nTo minimize $Z$, we need to make $\\gamma_{11}$ and $\\gamma_{21}$ as large as possible (due to negative coefficients $-6$ and $-2$) and $\\gamma_{31}$ as small as possible (due to positive coefficient $6$).\nLet's try to set $\\gamma_{31}$ to its minimum value, $\\gamma_{31} = 0$.\nThe constraint becomes $\\gamma_{11} + \\gamma_{21} = \\frac{2}{3}$.\nTo minimize the objective, we prioritize making $\\gamma_{11}$ largest. We set $\\gamma_{11}$ to its upper bound: $\\gamma_{11} = \\frac{1}{2}$.\nThis implies $\\gamma_{21} = \\frac{2}{3} - \\gamma_{11} = \\frac{2}{3} - \\frac{1}{2} = \\frac{1}{6}$.\nLet's check if this solution is feasible:\n- $\\gamma_{11} = \\frac{1}{2}$, which is in $[0, \\frac{1}{2}]$.\n- $\\gamma_{21} = \\frac{1}{6}$, which is in $[0, \\frac{1}{3}]$.\n- $\\gamma_{31} = 0$, which is in $[0, \\frac{1}{6}]$.\nThe solution is feasible and, by our greedy construction based on the objective function's coefficients, it is optimal.\n\nNow we find the remaining components of the optimal coupling matrix $\\Gamma^{\\star}$:\n$\\gamma^{\\star}_{11} = \\frac{1}{2}$\n$\\gamma^{\\star}_{21} = \\frac{1}{6}$\n$\\gamma^{\\star}_{31} = 0$\n$\\gamma^{\\star}_{12} = p_1 - \\gamma^{\\star}_{11} = \\frac{1}{2} - \\frac{1}{2} = 0$\n$\\gamma^{\\star}_{22} = p_2 - \\gamma^{\\star}_{21} = \\frac{1}{3} - \\frac{1}{6} = \\frac{1}{6}$\n$\\gamma^{\\star}_{32} = p_3 - \\gamma^{\\star}_{31} = \\frac{1}{6} - 0 = \\frac{1}{6}$\n\nThe optimal coupling matrix is:\n$$ \\Gamma^{\\star} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ \\frac{1}{6}  \\frac{1}{6} \\\\ 0  \\frac{1}{6} \\end{pmatrix} $$\n\nFinally, we compute the optimal total transport cost by substituting $\\Gamma^{\\star}$ into the objective function:\n$$ Z^{\\star} = \\sum_{i=1}^{3}\\sum_{j=1}^{2} c_{ij} \\gamma^{\\star}_{ij} $$\n$$ Z^{\\star} = c_{11}\\gamma^{\\star}_{11} + c_{21}\\gamma^{\\star}_{21} + c_{22}\\gamma^{\\star}_{22} + c_{32}\\gamma^{\\star}_{32} $$\n$$ Z^{\\star} = (\\frac{1}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{6}) + (\\frac{9}{4})(\\frac{1}{6}) + (\\frac{1}{4})(\\frac{1}{6}) $$\n$$ Z^{\\star} = \\frac{1}{8} + \\frac{1}{24} + \\frac{9}{24} + \\frac{1}{24} $$\n$$ Z^{\\star} = \\frac{3}{24} + \\frac{1+9+1}{24} = \\frac{3}{24} + \\frac{11}{24} = \\frac{14}{24} $$\nSimplifying the fraction gives the optimal cost:\n$$ Z^{\\star} = \\frac{7}{12} $$\nThis result can be verified using the reduced objective function:\n$Z^{\\star} = -6(\\frac{1}{2}) - 2(\\frac{1}{6}) + 6(0) + \\frac{47}{12} = -3 - \\frac{1}{3} + \\frac{47}{12} = -\\frac{10}{3} + \\frac{47}{12} = -\\frac{40}{12} + \\frac{47}{12} = \\frac{7}{12}$.\nThe calculations are consistent.",
            "answer": "$$\\boxed{\\frac{7}{12}}$$"
        },
        {
            "introduction": "While the Kantorovich formulation seeks a general coupling, in some cases we can construct the optimal transport map explicitly. This exercise focuses on this constructive approach, using a class of lower triangular monotone maps to transform a simple reference distribution into a target Bayesian posterior . By tackling the important special case of a Gaussian posterior, you will discover that the transport map must be affine and is directly related to the Cholesky decomposition of the posterior covariance, a cornerstone technique for generating correlated random variables.",
            "id": "3408105",
            "problem": "You are asked to construct, analyze, and implement a deterministic lower triangular monotone polynomial transport map in two dimensions that pushes a reference Gaussian distribution to a log-concave Bayesian posterior distribution arising from a linear inverse problem with Gaussian prior and Gaussian noise. Your implementation must compute the Jacobian determinant of the transport and verify invertibility by demonstrating strictly positive determinant over the domain induced by the reference distribution. The final program must return a set of quantitative diagnostics for a test suite of parameter settings.\n\nGiven the following fundamental base:\n\n- Bayes’ rule for a linear inverse problem with Gaussian prior and Gaussian noise: let the unknown parameter be $\\theta \\in \\mathbb{R}^2$, the forward operator be $A \\in \\mathbb{R}^{m \\times 2}$, observed data be $y \\in \\mathbb{R}^m$, the prior be $\\theta \\sim \\mathcal{N}(m_0, C_0)$ with $m_0 \\in \\mathbb{R}^2$ and $C_0 \\in \\mathbb{R}^{2 \\times 2}$ symmetric positive definite, and observational noise $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma \\in \\mathbb{R}^{m \\times m}$ symmetric positive definite. Then the posterior $\\pi(\\theta \\mid y)$ is Gaussian $\\mathcal{N}(m_\\pi, C_\\pi)$ with precision and mean given by the well-tested formulas\n$$\nC_\\pi^{-1} \\,=\\, C_0^{-1} + A^\\top \\Gamma^{-1} A, \\qquad\nm_\\pi \\,=\\, C_\\pi \\left( C_0^{-1} m_0 + A^\\top \\Gamma^{-1} y \\right).\n$$\n\n- Pushforward of measures and change-of-variables: if $X \\sim \\rho$ and $T:\\mathbb{R}^2 \\to \\mathbb{R}^2$ is a diffeomorphism with Jacobian $J_T(x)$, then the density of $Z = T(X)$ is given by the change-of-variables formula, and for any sufficiently smooth $f$,\n$$\n\\mathbb{E}_\\rho[f(T(X))] \\,=\\, \\int_{\\mathbb{R}^2} f(T(x)) \\,\\rho(x)\\,dx \\,=\\, \\int_{\\mathbb{R}^2} f(z) \\,\\rho\\bigl(T^{-1}(z)\\bigr)\\,\\bigl|\\det J_{T^{-1}}(z)\\bigr|\\,dz.\n$$\n\n- Monotone triangular maps and invertibility: a lower triangular map $T(x_1,x_2) = \\bigl(T_1(x_1),\\, T_2(x_1,x_2)\\bigr)$ is strictly monotone if $\\partial T_1/\\partial x_1 > 0$ and $\\partial T_2/\\partial x_2 > 0$ everywhere; in this case $T$ is invertible and $\\det J_T(x) > 0$ for all $x$.\n\nYou must implement a two-dimensional lower triangular monotone polynomial transport of the following form:\n- The first component is defined by\n$$\nT_1(x_1) \\,=\\, a_{10} \\;+\\; \\int_{0}^{x_1} \\bigl(b_{10} + b_{11} t\\bigr)^2 \\, dt,\n$$\nso that $\\partial T_1/\\partial x_1 = \\bigl(b_{10} + b_{11} x_1\\bigr)^2 > 0$ for all $x_1 \\in \\mathbb{R}$.\n- The second component is defined by\n$$\nT_2(x_1, x_2) \\,=\\, a_{20} \\;+\\; \\alpha\\, x_1 \\;+\\; \\int_{0}^{x_2} \\bigl(c_{20} + c_{21} t + c_{22} x_1\\bigr)^2 \\, dt,\n$$\nso that $\\partial T_2/\\partial x_2 = \\bigl(c_{20} + c_{21} x_2 + c_{22} x_1\\bigr)^2 > 0$ for all $x_1, x_2 \\in \\mathbb{R}$.\n\nLet the reference density be the standard Gaussian $\\rho(x) = \\mathcal{N}(0, I_2)$, where $I_2$ is the $2 \\times 2$ identity matrix. Your goal is to determine parameters $a_{10}, a_{20}, b_{10}, b_{11}, c_{20}, c_{21}, c_{22}, \\alpha$ using only the fundamental base above so that the pushforward $T_\\sharp \\rho$ equals the Gaussian posterior $\\pi(\\theta \\mid y) = \\mathcal{N}(m_\\pi, C_\\pi)$. Then:\n\n1. Derive an explicit expression for the Jacobian matrix $J_T(x)$ and its determinant $\\det J_T(x)$, and provide the condition under which $T$ is invertible everywhere.\n\n2. Explain how to choose parameters so that $T_\\sharp \\rho$ matches the target posterior $\\mathcal{N}(m_\\pi, C_\\pi)$ exactly. Your derivation must start from the properties of Gaussian measures, pushforward, and change-of-variables, and it must remain within the given class of triangular monotone polynomials.\n\n3. Implement a program that, for each test case below:\n   - Computes $m_\\pi$ and $C_\\pi$ from the given $(A, \\Gamma, m_0, C_0, y)$.\n   - Constructs the map $T$ within the specified class with parameters chosen so that $T_\\sharp \\rho = \\mathcal{N}(m_\\pi, C_\\pi)$.\n   - Computes the Euclidean norm of the mean-matching error $e_{\\text{mean}} = \\lVert \\mathbb{E}_\\rho[T(X)] - m_\\pi \\rVert_2$.\n   - Computes the Frobenius norm of the covariance-matching error $e_{\\text{cov}} = \\lVert \\operatorname{Cov}_\\rho[T(X)] - C_\\pi \\rVert_F$.\n   - Computes the Jacobian determinant $\\det J_T(x)$ and reports its minimum over $x$ drawn from the reference distribution. Since the choice of parameters must yield an $x$-independent determinant in this class for the exact match, you may report the determinant at any $x$ as the minimum (equivalently, report the constant value).\n\nYour implementation must use the following test suite of four cases. All matrices and vectors are given explicitly:\n\n- Test case $1$:\n  - $m_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $C_0 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$.\n  - $A = \\begin{bmatrix} 1  0.5 \\\\ 0.5  1 \\end{bmatrix}$,\n    $\\Gamma = \\begin{bmatrix} 0.25  0 \\\\ 0  0.25 \\end{bmatrix}$.\n  - $y = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n\n- Test case $2$:\n  - $m_0 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix}$,\n    $C_0 = \\begin{bmatrix} 2.0  0 \\\\ 0  0.5 \\end{bmatrix}$.\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n    $\\Gamma = \\begin{bmatrix} 0.01  0 \\\\ 0  0.5 \\end{bmatrix}$.\n  - $y = \\begin{bmatrix} 0.2 \\\\ -1.0 \\end{bmatrix}$.\n\n- Test case $3$:\n  - $m_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $C_0 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$.\n  - $A = \\begin{bmatrix} 1  1 \\end{bmatrix}$,\n    $\\Gamma = \\begin{bmatrix} 0.04 \\end{bmatrix}$.\n  - $y = \\begin{bmatrix} 0.3 \\end{bmatrix}$.\n\n- Test case $4$:\n  - $m_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $C_0 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$.\n  - $A = \\begin{bmatrix} 0  0 \\end{bmatrix}$,\n    $\\Gamma = \\begin{bmatrix} 1.0 \\end{bmatrix}$.\n  - $y = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n\nFor each test case, your program must output a triple $[e_{\\text{mean}}, e_{\\text{cov}}, \\det J_T]$. Aggregate the results for all four test cases into a single line printed as a comma-separated list enclosed in square brackets, where each entry is the triple for one test case. For example, the output format must be\n$$\n\\texttt{[[e1\\_mean,e1\\_cov,det1],[e2\\_mean,e2\\_cov,det2],[e3\\_mean,e3\\_cov,det3],[e4\\_mean,e4\\_cov,det4]]}\n$$\nwith all values represented as decimal floating-point numbers. No additional text should be printed.",
            "solution": "The user-provided problem is validated as sound. It is a well-posed, self-contained, and scientifically grounded problem in mathematical statistics and numerical methods, specifically concerning the application of transport maps in Bayesian computation. All necessary data, definitions, and theoretical foundations are provided.\n\n### Part 1: Jacobian, Determinant, and Invertibility\n\nThe polynomial transport map $T: \\mathbb{R}^2 \\to \\mathbb{R}^2$ is defined by its components $T(x_1, x_2) = \\bigl(T_1(x_1), T_2(x_1, x_2)\\bigr)$.\n\nThe first component is:\n$$\nT_1(x_1) = a_{10} + \\int_{0}^{x_1} \\bigl(b_{10} + b_{11} t\\bigr)^2 \\, dt = a_{10} + b_{10}^2 x_1 + b_{10}b_{11} x_1^2 + \\frac{b_{11}^2}{3} x_1^3\n$$\nThe second component is:\n$$\nT_2(x_1, x_2) = a_{20} + \\alpha x_1 + \\int_{0}^{x_2} \\bigl(c_{20} + c_{21} t + c_{22} x_1\\bigr)^2 \\, dt = a_{20} + \\alpha x_1 + \\bigl(c_{20} + c_{22}x_1\\bigr)^2 x_2 + c_{21}\\bigl(c_{20} + c_{22}x_1\\bigr) x_2^2 + \\frac{c_{21}^2}{3} x_2^3\n$$\n\nThe Jacobian matrix $J_T(x)$ is the matrix of first-order partial derivatives:\n$$\nJ_T(x) = \\begin{bmatrix} \\frac{\\partial T_1}{\\partial x_1}  \\frac{\\partial T_1}{\\partial x_2} \\\\ \\frac{\\partial T_2}{\\partial x_1}  \\frac{\\partial T_2}{\\partial x_2} \\end{bmatrix}\n$$\nBy the Fundamental Theorem of Calculus, the diagonal elements are:\n$$\n\\frac{\\partial T_1}{\\partial x_1}(x_1) = \\bigl(b_{10} + b_{11} x_1\\bigr)^2\n$$\n$$\n\\frac{\\partial T_2}{\\partial x_2}(x_1, x_2) = \\bigl(c_{20} + c_{21} x_2 + c_{22} x_1\\bigr)^2\n$$\nThe off-diagonal elements are:\n$$\n\\frac{\\partial T_1}{\\partial x_2}(x_1) = 0\n$$\n$$\n\\frac{\\partial T_2}{\\partial x_1}(x_1, x_2) = \\frac{\\partial}{\\partial x_1} \\left( a_{20} + \\alpha x_1 + \\int_{0}^{x_2} \\bigl(c_{20} + c_{21} t + c_{22} x_1\\bigr)^2 \\, dt \\right) = \\alpha + \\int_{0}^{x_2} 2\\bigl(c_{20} + c_{21} t + c_{22} x_1\\bigr)c_{22} \\, dt\n$$\nThe Jacobian matrix is lower triangular:\n$$\nJ_T(x) = \\begin{bmatrix}\n\\bigl(b_{10} + b_{11} x_1\\bigr)^2  0 \\\\\n\\frac{\\partial T_2}{\\partial x_1}(x_1, x_2)  \\bigl(c_{20} + c_{21} x_2 + c_{22} x_1\\bigr)^2\n\\end{bmatrix}\n$$\nIts determinant is the product of the diagonal elements:\n$$\n\\det J_T(x) = \\bigl(b_{10} + b_{11} x_1\\bigr)^2 \\cdot \\bigl(c_{20} + c_{21} x_2 + c_{22} x_1\\bigr)^2\n$$\nFor the map $T$ to be a diffeomorphism (and thus invertible) over its entire domain $\\mathbb{R}^2$, its Jacobian determinant must be strictly positive everywhere, i.e., $\\det J_T(x) > 0$ for all $x \\in \\mathbb{R}^2$. This requires that both factors in the determinant are strictly positive.\n1.  $\\bigl(b_{10} + b_{11} x_1\\bigr)^2 > 0$ for all $x_1 \\in \\mathbb{R}$. If $b_{11} \\neq 0$, the expression equals zero at $x_1 = -b_{10}/b_{11}$. Thus, we must have $b_{11} = 0$ and $b_{10} \\neq 0$.\n2.  $\\bigl(c_{20} + c_{21} x_2 + c_{22} x_1\\bigr)^2 > 0$ for all $x_1, x_2 \\in \\mathbb{R}$. If either $c_{21} \\neq 0$ or $c_{22} \\neq 0$, the expression equals zero on the line defined by $c_{20} + c_{21} x_2 + c_{22} x_1 = 0$. Thus, we must have $c_{21} = 0$, $c_{22} = 0$, and $c_{20} \\neq 0$.\n\nThe condition for $T$ to be invertible everywhere is that $b_{11}=c_{21}=c_{22}=0$ and $b_{10}, c_{20}$ are non-zero.\n\n### Part 2: Parameter Selection for Gaussian Pushforward\n\nWe aim to find parameters such that the pushforward of the standard Gaussian reference measure $\\rho = \\mathcal{N}(0, I_2)$ by the map $T$ exactly matches the target Gaussian posterior $\\pi = \\mathcal{N}(m_\\pi, C_\\pi)$. That is, $T_\\sharp\\rho = \\pi$.\n\nA fundamental property of the Gaussian distribution is that only an affine transformation can map a Gaussian random variable to another Gaussian random variable. Our transport map $T$ is a polynomial map. For a random vector $X \\sim \\mathcal{N}(0, I_2)$, the transformed vector $Z=T(X)$ is Gaussian if and only if the map $T$ is affine.\nAs shown by the explicit forms of $T_1$ and $T_2$, the map is affine only if the coefficients of all non-linear terms are zero. This requires:\n- For $T_1(x_1)$: The coefficients of $x_1^2$ and $x_1^3$ must be zero, which means $b_{10}b_{11}=0$ and $b_{11}^2/3=0$. This implies $b_{11} = 0$.\n- For $T_2(x_1, x_2)$: The coefficients of $x_2^2$ and $x_2^3$ must be zero, which implies $c_{21}=0$. Furthermore, the coefficient of the mixed term $x_1 x_2$ must be zero, which in $(c_{20} + c_{22}x_1)^2 x_2$ implies $c_{22}=0$.\n\nThese conditions, $b_{11}=0, c_{21}=0, c_{22}=0$, are identical to the conditions for global invertibility. Under these constraints, the map simplifies to an affine transformation:\n$$\nT_1(x_1) = a_{10} + b_{10}^2 x_1\n$$\n$$\nT_2(x_1, x_2) = a_{20} + \\alpha x_1 + c_{20}^2 x_2\n$$\nThis can be written in matrix form as $T(x) = L x + m$, where:\n$$\nm = \\begin{bmatrix} a_{10} \\\\ a_{20} \\end{bmatrix}, \\quad L = \\begin{bmatrix} b_{10}^2  0 \\\\ \\alpha  c_{20}^2 \\end{bmatrix}\n$$\nIf $X \\sim \\mathcal{N}(0, I_2)$, then the affine transform $Z = L X + m$ is distributed as $\\mathcal{N}(m, L L^\\top)$. To match the target posterior $\\mathcal{N}(m_\\pi, C_\\pi)$, we must match the mean and covariance:\n1.  $m = m_\\pi \\implies a_{10} = m_{\\pi,1}$ and $a_{20} = m_{\\pi,2}$.\n2.  $L L^\\top = C_\\pi$.\n\nThe posterior covariance matrix $C_\\pi$ is symmetric positive definite and has a unique lower-triangular Cholesky decomposition $C_\\pi = K K^\\top$ where $K$ has positive diagonal entries.\n$$\nK = \\begin{bmatrix} K_{11}  0 \\\\ K_{21}  K_{22} \\end{bmatrix}\n$$\nWe equate $L = K$ to determine the remaining parameters:\n$$\n\\begin{bmatrix} b_{10}^2  0 \\\\ \\alpha  c_{20}^2 \\end{bmatrix} = \\begin{bmatrix} K_{11}  0 \\\\ K_{21}  K_{22} \\end{bmatrix}\n$$\nThis yields the parameter values:\n$$\nb_{10}^2 = K_{11} = \\sqrt{C_{\\pi,11}}\n$$\n$$\n\\alpha = K_{21} = \\frac{C_{\\pi,12}}{K_{11}} = \\frac{C_{\\pi,12}}{\\sqrt{C_{\\pi,11}}}\n$$\n$$\nc_{20}^2 = K_{22} = \\sqrt{K_{22}^2} = \\sqrt{C_{\\pi,22} - K_{21}^2} = \\sqrt{\\frac{\\det(C_\\pi)}{C_{\\pi,11}}}\n$$\nWe can choose $b_{10} = (K_{11})^{1/2} = (C_{\\pi,11})^{1/4}$ and $c_{20} = (K_{22})^{1/2} = (\\frac{\\det(C_\\pi)}{C_{\\pi,11}})^{1/4}$ to satisfy the non-zero conditions.\n\n### Part 3: Diagnostic Quantities\n\nThe program will compute the posterior mean $m_\\pi$ and covariance $C_\\pi$ for each test case. Then, it constructs the affine map $T(x) = L x + m$ using the parameters derived above. The diagnostics are evaluated as follows:\n\n- **Mean-matching error $e_{\\text{mean}}$**:\nThe expected value of the transformed variable is $\\mathbb{E}_\\rho[T(X)] = \\mathbb{E}_\\rho[L X + m] = L \\mathbb{E}_\\rho[X] + m = L \\cdot 0 + m = m$. By construction, $m=m_\\pi$.\nTherefore, $e_{\\text{mean}} = \\lVert \\mathbb{E}_\\rho[T(X)] - m_\\pi \\rVert_2 = \\lVert m_\\pi - m_\\pi \\rVert_2 = 0$.\n\n- **Covariance-matching error $e_{\\text{cov}}$**:\nThe covariance of the transformed variable is $\\operatorname{Cov}_\\rho[T(X)] = \\operatorname{Cov}_\\rho[L X + m] = L \\operatorname{Cov}_\\rho[X] L^\\top = L I_2 L^\\top = L L^\\top$. By construction, we set $L L^\\top = C_\\pi$.\nTherefore, $e_{\\text{cov}} = \\lVert \\operatorname{Cov}_\\rho[T(X)] - C_\\pi \\rVert_F = \\lVert C_\\pi - C_\\pi \\rVert_F = 0$.\nNumerically, these errors will be close to zero, subject to floating-point precision.\n\n- **Jacobian Determinant $\\det J_T$**:\nWith the parameter choices that make the map affine, the Jacobian is constant: $J_T(x) = L$.\nThe determinant is thus also constant:\n$$\n\\det J_T(x) = \\det(L) = b_{10}^2 \\cdot c_{20}^2 = K_{11} \\cdot K_{22} = \\det(K)\n$$\nSince $\\det(C_\\pi) = \\det(K K^\\top) = \\det(K) \\det(K^\\top) = (\\det(K))^2$, we have $\\det(K) = \\sqrt{\\det(C_\\pi)}$.\nThus, the Jacobian determinant is $\\det J_T = \\sqrt{\\det(C_\\pi)}$. This value is constant across the domain, so its minimum is the value itself.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transport map problem for a suite of test cases.\n    For each case, it computes the Bayesian posterior, constructs the\n    affine transport map that pushes a standard Gaussian to this posterior,\n    and calculates error and Jacobian determinant diagnostics.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"m0\": np.array([0.0, 0.0]),\n            \"C0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"A\": np.array([[1.0, 0.5], [0.5, 1.0]]),\n            \"Gamma\": np.array([[0.25, 0.0], [0.0, 0.25]]),\n            \"y\": np.array([1.0, -1.0])\n        },\n        {\n            \"m0\": np.array([0.5, -0.5]),\n            \"C0\": np.array([[2.0, 0.0], [0.0, 0.5]]),\n            \"A\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Gamma\": np.array([[0.01, 0.0], [0.0, 0.5]]),\n            \"y\": np.array([0.2, -1.0])\n        },\n        {\n            \"m0\": np.array([0.0, 0.0]),\n            \"C0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"A\": np.array([[1.0, 1.0]]),\n            \"Gamma\": np.array([[0.04]]),\n            \"y\": np.array([0.3])\n        },\n        {\n            \"m0\": np.array([0.0, 0.0]),\n            \"C0\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"A\": np.array([[0.0, 0.0]]),\n            \"Gamma\": np.array([[1.0]]),\n            \"y\": np.array([0.0])\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m0 = case[\"m0\"]\n        C0 = case[\"C0\"]\n        A = case[\"A\"]\n        Gamma = case[\"Gamma\"]\n        y = case[\"y\"]\n\n        # Step 1: Compute posterior mean and covariance\n        C0_inv = np.linalg.inv(C0)\n        Gamma_inv = np.linalg.inv(Gamma)\n\n        C_pi_inv = C0_inv + A.T @ Gamma_inv @ A\n        C_pi = np.linalg.inv(C_pi_inv)\n\n        m_pi = C_pi @ (C0_inv @ m0 + A.T @ Gamma_inv @ y)\n\n        # Step 2: Construct the affine transport map T(x) = L*x + m\n        # The map must be affine to push a Gaussian to a Gaussian exactly.\n        # This implies T(x) = m_pi + K @ x where K is the Cholesky factor of C_pi.\n        # The problem defines T(x) in a specific form, which reduces to T(x) = L_map @ x + m_map\n        # where L_map = [[b_10**2, 0], [alpha, c_20**2]].\n        # We find the parameters by setting L_map = K, the Cholesky factor of C_pi.\n        \n        try:\n            K = np.linalg.cholesky(C_pi)\n        except np.linalg.LinAlgError:\n            # Handle cases where C_pi might not be perfectly SPD due to numerics\n            # For this problem set, this shouldn't happen.\n            results.append([float('nan'), float('nan'), float('nan')])\n            continue\n\n        L_map = K  # As per the derivation, L_map must be the Cholesky factor.\n\n        # The map's mean is m_pi, and the constructed map's mean is also m_pi.\n        # E[T(X)] = E[L_map @ X + m_pi] = m_pi\n        computed_mean = m_pi\n        e_mean = np.linalg.norm(computed_mean - m_pi)\n\n        # The map's covariance is L_map @ L_map.T, which is equal to C_pi by construction.\n        computed_cov = L_map @ L_map.T\n        e_cov = np.linalg.norm(computed_cov - C_pi, 'fro')\n\n        # The Jacobian of the map is constant and equal to L_map.\n        # Its determinant is the product of the diagonal elements.\n        det_J_T = np.linalg.det(L_map)\n\n        results.append([e_mean, e_cov, det_J_T])\n\n    # Format the output exactly as required\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Directly solving the OT problem or finding an explicit map can be computationally prohibitive for large-scale problems. This practice introduces entropic regularization, a powerful technique that approximates the original OT problem with a strictly convex one that can be solved efficiently with the Sinkhorn algorithm . You will not only implement this fundamental algorithm but also analyze its behavior by deriving a theoretical bound on the approximation bias and verifying it through a numerical experiment, providing a complete picture from algorithm design to practical performance analysis.",
            "id": "3408171",
            "problem": "Consider a Bayesian inverse problem in one spatial dimension where the unknown parameter is $\\theta \\in \\mathbb{R}$. Let the prior be Gaussian $\\mu = \\mathcal{N}(0,\\tau^2)$ with $\\tau  0$, and the observation model be $y_{\\mathrm{obs}} = G \\theta + \\eta$ with $G(\\theta) = \\theta$ and zero-mean Gaussian noise $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$, with $\\sigma  0$. By Bayes’ rule, the posterior is Gaussian $\\nu = p(\\theta \\mid y_{\\mathrm{obs}}) = \\mathcal{N}(m,s^2)$, where $s^2 = \\left(\\tau^{-2} + \\sigma^{-2}\\right)^{-1}$ and $m = s^2 \\, \\sigma^{-2} \\, y_{\\mathrm{obs}}$. We consider approaching Bayesian inference via Optimal Transport (OT), using an entropic-regularized approximation (Sinkhorn) to a transport map from the prior to the posterior.\n\nFundamental definitions to use:\n- A coupling $\\pi$ between two probability measures $\\mu$ and $\\nu$ on $\\mathbb{R}$ is a joint probability measure on $\\mathbb{R} \\times \\mathbb{R}$ with marginals $\\mu$ and $\\nu$. The (Kantorovich) optimal transport problem for a cost $c(x,y)$ seeks $\\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int c(x,y) \\, d\\pi(x,y)$.\n- The entropic-regularized OT problem replaces the objective with $\\int c(x,y) \\, d\\pi(x,y) + \\varepsilon \\, \\mathrm{KL}(\\pi \\,\\|\\, \\mu \\otimes \\nu)$ for a regularization parameter $\\varepsilon  0$, where $\\mathrm{KL}$ denotes Kullback–Leibler divergence, and $\\mu \\otimes \\nu$ is the product measure. In the discrete, uniform-weight case and with quadratic cost $c(x,y) = \\tfrac{1}{2}\\lVert x - y \\rVert^2$, the Gibbs kernel is $K_{ij} = \\exp\\!\\left(-C_{ij}/\\varepsilon\\right)$ with $C_{ij} = \\tfrac{1}{2}(x_i - y_j)^2$, and Sinkhorn scaling yields a coupling $\\pi_\\varepsilon$ with the prescribed marginals.\n- The barycentric projection transport approximation associated with $\\pi_\\varepsilon$ is the map $T_\\varepsilon(x) = \\mathbb{E}[Y \\mid X=x]$ computed from $\\pi_\\varepsilon$. The corresponding approximate posterior is the pushforward $T_\\varepsilon \\# \\mu$.\n\nDerivation task:\n1. Starting from Bayes’ rule for $\\nu$, the Kantorovich formulation of OT, and the definition of entropic regularization, justify the construction of the discrete Sinkhorn coupling $\\pi_\\varepsilon$ between empirical measures $\\hat{\\mu}$ and $\\hat{\\nu}$ (formed by independent and identically distributed samples from $\\mu$ and $\\nu$) for the quadratic cost $c(x,y) = \\tfrac{1}{2}\\lVert x - y \\rVert^2$, and define the barycentric map $T_\\varepsilon$.\n2. For a Lipschitz function $f:\\mathbb{R} \\to \\mathbb{R}$ with Lipschitz constant $L$, derive a bound on the bias in estimating $\\mathbb{E}_{\\nu}[f(\\theta)]$ by $\\mathbb{E}_{T_\\varepsilon \\# \\mu}[f(\\theta)]$, expressed in terms of $\\varepsilon$ and $L$, as $\\varepsilon \\to 0$. Begin from the Kantorovich–Rubinstein-type inequality $|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| \\leq L \\, \\mathbb{E}_{\\pi_\\varepsilon}\\big[\\lVert Y - T_\\varepsilon(X)\\rVert\\big]$, and, using only standard convexity and concentration arguments applicable to the quadratic cost, obtain an explicit scaling with $\\varepsilon$. Provide clear statements of any conditions required for your bound to hold, and ensure each step follows from a foundational principle or a well-tested result (for example, properties of log-concave measures or Poincaré inequalities).\n3. Specialize the preceding bound to the one-dimensional case and functions $f$ with known Lipschitz constants.\n\nNumerical experiment and test suite:\n- Fix $d=1$ (one-dimensional parameter), $\\tau = 1$, $\\sigma = 0.3$, and $y_{\\mathrm{obs}} = 0.7$. These define the posterior $\\nu = \\mathcal{N}(m,s^2)$ with $s^2 = (1/\\tau^2 + 1/\\sigma^2)^{-1}$ and $m = s^2(y_{\\mathrm{obs}}/\\sigma^2)$. Draw $n=128$ independent and identically distributed samples $\\{x_i\\}_{i=1}^n$ from $\\mu = \\mathcal{N}(0,\\tau^2)$ and $\\{y_j\\}_{j=1}^n$ from $\\nu = \\mathcal{N}(m,s^2)$ to form empirical measures $\\hat{\\mu}$ and $\\hat{\\nu}$.\n- For each $\\varepsilon \\in \\{1.0, 0.5, 0.2\\}$, compute the Sinkhorn coupling $\\pi_\\varepsilon$ between $\\hat{\\mu}$ and $\\hat{\\nu}$ with cost $c(x,y) = \\tfrac{1}{2}(x - y)^2$, and the associated barycentric map values $T_\\varepsilon(x_i)$. Consider the functions:\n  - $f_1(\\theta) = \\theta$ with Lipschitz constant $L_1 = 1$,\n  - $f_2(\\theta) = |\\theta|$ with Lipschitz constant $L_2 = 1$,\n  - $f_3(\\theta) = \\sin(2\\theta)$ with Lipschitz constant $L_3 = 2$.\n- For each pair $(\\varepsilon, f_k)$ with $k \\in \\{1,2,3\\}$, compute the empirical bias $B(\\varepsilon,f_k) = \\left|\\frac{1}{n}\\sum_{j=1}^n f_k(y_j) - \\frac{1}{n}\\sum_{i=1}^n f_k\\big(T_\\varepsilon(x_i)\\big)\\right|$. Using the one-dimensional $\\varepsilon$-scaling you derived, construct an explicit, computable upper bound $U(\\varepsilon,f_k)$ based only on $\\varepsilon$ and $L_k$ (no unobservable constants), and test whether $B(\\varepsilon,f_k) \\leq U(\\varepsilon,f_k) + \\delta$ holds for a fixed numerical tolerance $\\delta = 0.05$ to account for sampling error and finite-iteration numerics.\n- The test suite consists of the $9$ cases from the Cartesian product of $\\{\\varepsilon \\in \\{1.0, 0.5, 0.2\\}\\}$ and $\\{f_1,f_2,f_3\\}$. For each case, the program must output a boolean indicating whether the empirical inequality is satisfied.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list of the $9$ booleans in the order $\\varepsilon = 1.0, 0.5, 0.2$ and $f_1, f_2, f_3$ nested in that order, enclosed in square brackets (for example, $[{\\tt True},{\\tt False},\\dots]$). No physical units are involved, and no angles appear, so no unit conversions are required.",
            "solution": "The problem is valid. It is a well-posed and scientifically grounded exercise in computational statistics, connecting Bayesian inference with computational optimal transport. All definitions are standard, and all parameters for the numerical experiment are provided.\n\n### 1. Justification of the Discrete Sinkhorn Method\n\nThe task is to approximate the Bayesian posterior $\\nu = \\mathcal{N}(m,s^2)$ starting from the prior $\\mu = \\mathcal{N}(0,\\tau^2)$ using an optimal transport map. The classical (Kantorovich) optimal transport problem seeks a coupling $\\pi$ between $\\mu$ and $\\nu$ that minimizes the total transport cost, $\\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int c(x,y) \\, d\\pi(x,y)$. For the quadratic cost $c(x,y) = \\frac{1}{2}(x-y)^2$, Brenier's theorem states that the optimal coupling is deterministic, $\\pi = (\\mathrm{Id} \\times T_0)_\\# \\mu$, where $T_0: \\mathbb{R} \\to \\mathbb{R}$ is the optimal transport map.\n\nThe entropic regularization of this problem introduces a penalty term, modifying the objective to finding a coupling $\\pi_\\varepsilon$ that minimizes $\\int c(x,y) \\, d\\pi(x,y) + \\varepsilon \\, \\mathrm{KL}(\\pi \\,\\|\\, \\mu \\otimes \\nu)$, where $\\varepsilon  0$ is the regularization strength. This regularized problem is strictly convex and always has a unique solution $\\pi_\\varepsilon$, which is absolutely continuous with respect to $\\mu \\otimes \\nu$. The optimal coupling $\\pi_\\varepsilon$ is no longer deterministic but represents a \"blurred\" version of the optimal map $T_0$.\n\nFor numerical computation, we discretize the measures $\\mu$ and $\\nu$ by drawing $n$ independent and identically distributed samples $\\{x_i\\}_{i=1}^n$ from $\\mu$ and $\\{y_j\\}_{j=1}^n$ from $\\nu$. This yields the empirical measures $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}$ and $\\hat{\\nu} = \\frac{1}{n}\\sum_{j=1}^n \\delta_{y_j}$. The discrete version of the entropic OT problem is to find a transport plan (a matrix) $\\pi_\\varepsilon \\in \\mathbb{R}^{n \\times n}$ that minimizes:\n$$\n\\sum_{i,j=1}^n \\pi_{ij} C_{ij} + \\varepsilon \\sum_{i,j=1}^n \\pi_{ij} \\log(n^2 \\pi_{ij})\n$$\nsubject to the marginal constraints $\\sum_{j=1}^n \\pi_{ij} = \\frac{1}{n}$ for all $i$ and $\\sum_{i=1}^n \\pi_{ij} = \\frac{1}{n}$ for all $j$. The cost matrix is $C_{ij} = \\frac{1}{2}(x_i - y_j)^2$.\n\nThe solution to this minimization problem has a specific structure: $\\pi_{ij} = u_i K_{ij} v_j$, where $K_{ij} = \\exp(-C_{ij}/\\varepsilon)$ is the Gibbs kernel, and $u, v \\in \\mathbb{R}^n$ are scaling vectors. Sinkhorn's algorithm is an iterative procedure to find these vectors. Given target uniform marginals $a=b=\\frac{1}{n}\\mathbf{1}$, the algorithm alternates between scaling the rows and columns of $K$:\n$$\nu \\leftarrow \\frac{a}{K v} \\quad \\text{and} \\quad v \\leftarrow \\frac{b}{K^T u}\n$$\n(where division is element-wise) until convergence. Once the optimal coupling $\\pi_\\varepsilon$ is found, the barycentric map $T_\\varepsilon$ is defined as the conditional expectation of the target variable $Y$ given the source variable $X$. For the discrete samples, this is:\n$$\nT_\\varepsilon(x_i) = \\mathbb{E}_{\\pi_\\varepsilon}[Y | X=x_i] = \\sum_{j=1}^n y_j P(Y=y_j | X=x_i) = \\sum_{j=1}^n y_j \\frac{\\pi_{ij}}{\\sum_{k=1}^n \\pi_{ik}} = \\sum_{j=1}^n y_j \\frac{\\pi_{ij}}{1/n} = n \\sum_{j=1}^n y_j \\pi_{ij}\n$$\nThe pushforward measure $T_\\varepsilon \\# \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n \\delta_{T_\\varepsilon(x_i)}$ is then used as the transport-based approximation of the posterior $\\hat{\\nu}$.\n\n### 2. Derivation of the Bias Bound\n\nWe aim to bound the bias $|\\mathbb{E}_{\\nu}[f(\\theta)] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f(\\theta)]|$ for a function $f$ with Lipschitz constant $L$. The problem provides the starting point:\n$$\n|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| = \\left| \\iint (f(y) - f(T_\\varepsilon(x))) d\\pi_\\varepsilon(x,y) \\right| \\leq L \\iint |y - T_\\varepsilon(x)| d\\pi_\\varepsilon(x,y) = L \\, \\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|]\n$$\nBy applying Jensen's inequality to the concave function $\\phi(z)=\\sqrt{z}$, we have $(\\mathbb{E}[Z])^2 \\leq \\mathbb{E}[Z^2]$. Thus:\n$$\n\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|] \\leq \\sqrt{\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|^2]}\n$$\nBy definition, $T_\\varepsilon(X) = \\mathbb{E}_{\\pi_\\varepsilon}[Y|X]$. Therefore, the term inside the square root is the variance of $Y$ conditioned on $X$, averaged over $X$:\n$$\n\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|^2] = \\mathbb{E}_{X \\sim \\mu}[\\mathrm{Var}_{\\pi_\\varepsilon(Y|X)}(Y)]\n$$\nTo bound this variance, we analyze the structure of the conditional distribution $\\pi_\\varepsilon(y|x)$. Its density with respect to the Lebesgue measure is proportional to $e^{(-c(x,y) + \\phi(x) + \\psi(y))/\\varepsilon}$, where $\\phi, \\psi$ are the dual OT potentials. A more direct argument relies on the fact that the conditional measure $d\\pi_\\varepsilon(y|x) \\propto e^{-c(x,y)/\\varepsilon} e^{\\psi(y)/\\varepsilon} d\\nu(y)$. The negative logarithm of its density is $V_x(y) = \\frac{c(x,y)}{\\varepsilon} - \\frac{\\psi(y)}{\\varepsilon} - \\log p_\\nu(y)$, where $p_\\nu(y)$ is the density of $\\nu=\\mathcal{N}(m,s^2)$. The negative log-density of $\\nu$ is $\\frac{1}{2s^2}(y-m)^2$ (up to a constant). So,\n$$\nV_x(y) = \\frac{1}{2\\varepsilon}(y-x)^2 - \\frac{\\psi(y)}{\\varepsilon} + \\frac{1}{2s^2}(y-m)^2 + \\text{const}\n$$\nA key property in OT theory is that for a quadratic cost and log-concave measures (which a Gaussian is), the dual potential $\\psi(y)$ is a concave function. Therefore, its second derivative $\\psi''(y) \\leq 0$. The second derivative of $V_x(y)$ with respect to $y$ is:\n$$\nV_x''(y) = \\frac{1}{\\varepsilon} - \\frac{\\psi''(y)}{\\varepsilon} + \\frac{1}{s^2} \\geq \\frac{1}{\\varepsilon} + \\frac{1}{s^2}\n$$\nThis shows that the conditional distribution $\\pi_\\varepsilon(y|x)$ is strongly log-concave with a parameter $\\lambda \\geq \\frac{1}{\\varepsilon} + \\frac{1}{s^2}$. The Brascamp-Lieb inequality states that the variance of a strongly log-concave distribution with parameter $\\lambda$ is bounded by $1/\\lambda$. Thus,\n$$\n\\mathrm{Var}_{\\pi_\\varepsilon(Y|X=x)}(Y) \\leq \\left(\\frac{1}{\\varepsilon} + \\frac{1}{s^2}\\right)^{-1} = \\frac{\\varepsilon s^2}{\\varepsilon + s^2}\n$$\nThis bound is uniform in $x$. Taking the expectation over $X \\sim \\mu$ gives:\n$$\n\\mathbb{E}_{X \\sim \\mu}[\\mathrm{Var}_{\\pi_\\varepsilon(Y|X)}(Y)] \\leq \\frac{\\varepsilon s^2}{\\varepsilon + s^2}\n$$\nSubstituting this back into our chain of inequalities, we obtain the final bound on the bias:\n$$\n|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| \\leq L \\sqrt{\\frac{\\varepsilon s^2}{\\varepsilon + s^2}}\n$$\n\n### 3. Specialization and Numerical Test\n\nFor the one-dimensional numerical experiment, the posterior variance is $s^2 = (1/\\tau^2 + 1/\\sigma^2)^{-1}$ and the posterior mean is $m = s^2(y_{\\mathrm{obs}}/\\sigma^2)$. The bound derived above is directly applicable. For a function $f_k$ with Lipschitz constant $L_k$, the computable upper bound is:\n$$\nU(\\varepsilon, f_k) = L_k \\sqrt{\\frac{\\varepsilon s^2}{\\varepsilon + s^2}}\n$$\nThe numerical test involves computing the empirical bias $B(\\varepsilon, f_k) = |\\frac{1}{n}\\sum_j f_k(y_j) - \\frac{1}{n}\\sum_i f_k(T_\\varepsilon(x_i))|$ and checking if the inequality $B(\\varepsilon, f_k) \\leq U(\\varepsilon, f_k) + \\delta$ holds, where $\\delta = 0.05$ is a tolerance to account for statistical fluctuations from sampling and numerical errors from the finite-iteration Sinkhorn algorithm.\n\nFor the special case of $f_1(\\theta) = \\theta$, the theoretical bias is exactly zero, as shown by:\n$$\n\\mathbb{E}_{T_\\varepsilon \\# \\mu}[\\theta] = \\int T_\\varepsilon(x)d\\mu(x) = \\iint y \\, d\\pi_\\varepsilon(y|x)d\\mu(x) = \\iint y \\, d\\pi_\\varepsilon(x,y) = \\int y \\, d\\nu(y) = \\mathbb{E}_\\nu[\\theta]\n$$\nThus, $\\mathbb{E}_{\\nu}[f_1] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f_1] = 0$. The empirical bias $B(\\varepsilon,f_1)$ should be close to zero, limited only by numerical precision. The bound $U(\\varepsilon, f_1)$ is non-zero, so the inequality is expected to hold.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal transport problem for Bayesian inference as specified.\n    \"\"\"\n    # Problem Parameters\n    tau = 1.0\n    sigma = 0.3\n    y_obs = 0.7\n    n = 128\n    delta = 0.05\n    \n    # Random number generator for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Posterior distribution parameters\n    tau_sq_inv = 1.0 / (tau**2)\n    sigma_sq_inv = 1.0 / (sigma**2)\n    s_sq = 1.0 / (tau_sq_inv + sigma_sq_inv)\n    m = s_sq * (y_obs / sigma**2)\n    s = np.sqrt(s_sq)\n\n    # Generate samples from prior and posterior\n    # x ~ N(0, tau^2)\n    samples_x = rng.normal(loc=0.0, scale=tau, size=n).reshape(-1, 1)\n    # y ~ N(m, s^2)\n    samples_y = rng.normal(loc=m, scale=s, size=n).reshape(-1, 1)\n\n    # Define test functions and their Lipschitz constants\n    f1 = lambda theta: theta\n    f2 = lambda theta: np.abs(theta)\n    f3 = lambda theta: np.sin(2 * theta)\n    functions = [(f1, 1.0), (f2, 1.0), (f3, 2.0)]\n\n    # Epsilon values for regularization\n    epsilons = [1.0, 0.5, 0.2]\n    \n    # Store results\n    results = []\n    \n    # Helper for Sinkhorn algorithm\n    def compute_sinkhorn_coupling(x, y, epsilon, num_iter=1000, tol=1e-9):\n        \"\"\"\n        Computes the entropic OT coupling using Sinkhorn's algorithm.\n        \"\"\"\n        n_samples = x.shape[0]\n        \n        # Cost matrix C_ij = 0.5 * (x_i - y_j)^2\n        C = 0.5 * (x - y.T)**2\n        \n        # Gibbs kernel\n        K = np.exp(-C / epsilon)\n        \n        # Target marginals (uniform)\n        a = np.ones(n_samples) / n_samples\n        b = np.ones(n_samples) / n_samples\n        \n        # Sinkhorn iterations\n        v = np.ones(n_samples)\n        for _ in range(num_iter):\n            u = a / (K @ v + tol)\n            v = b / (K.T @ u + tol)\n            \n        # Coupling matrix P_ij = u_i * K_ij * v_j\n        pi_eps = np.diag(u) @ K @ np.diag(v)\n        return pi_eps\n\n    # Main loop for experiments\n    for epsilon in epsilons:\n        # 1. Compute Sinkhorn coupling\n        pi_epsilon = compute_sinkhorn_coupling(samples_x, samples_y, epsilon)\n        \n        # 2. Compute barycentric map T_eps(x_i)\n        # T_eps(x_i) = n * sum_j(y_j * pi_ij)\n        T_eps_x = n * (pi_epsilon @ samples_y)\n        \n        for f, L in functions:\n            # 3. Compute empirical bias B(epsilon, f_k)\n            # B = | E_nu[f(y)] - E_{T#mu}[f(T(x))] |\n            # E_nu[f(y)] ~= (1/n) * sum(f(y_j))\n            # E_{T#mu}[f(T(x))] ~= (1/n) * sum(f(T(x_i)))\n            \n            # For f1(theta)=theta, the bias is analytically 0.\n            # Numerically, it's |mean(y) - mean(T_eps(x))|.\n            # Since sum_i T_eps(x_i) = sum_j y_j due to marginal constraints,\n            # this difference will be due to floating point and convergence error.\n            \n            bias_empirical = np.abs(np.mean(f(samples_y)) - np.mean(f(T_eps_x)))\n            \n            # 4. Compute theoretical bound U(epsilon, f_k)\n            # U = L * sqrt( (epsilon * s^2) / (epsilon + s^2) )\n            bound_theoretical = L * np.sqrt((epsilon * s_sq) / (epsilon + s_sq))\n            \n            # 5. Check inequality B = U + delta\n            is_satisfied = bias_empirical = bound_theoretical + delta\n            results.append(is_satisfied)\n            \n    # Print results in the specified format\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n```"
        }
    ]
}