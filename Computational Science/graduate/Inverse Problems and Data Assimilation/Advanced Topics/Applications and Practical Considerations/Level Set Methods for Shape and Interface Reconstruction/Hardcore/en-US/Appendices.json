{
    "hands_on_practices": [
        {
            "introduction": "Gradient-based optimization is central to modern inverse problems. This first practice provides a foundational exercise in calculating the sensitivity of an objective function with respect to key parameters in a level set model . By explicitly deriving and implementing the gradients for a simple geometric shape, you will gain hands-on experience with the chain rule as it applies to the smoothed Heaviside function, a core component of differentiable level set methods.",
            "id": "3396660",
            "problem": "Consider a two-dimensional shape reconstruction problem using the Level Set Method (LSM). A level set function $\\phi:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ represents an interface by its zero level set. We assume a radially symmetric interface defined by $\\phi(\\mathbf{x}) = \\|\\mathbf{x}\\| - r$, where $\\mathbf{x} = (x,y)$ and $r$ is a positive radius parameter. To map the interface to material properties, we use a smoothed Heaviside function $H_\\epsilon:\\mathbb{R}\\rightarrow\\mathbb{R}$ with smoothing parameter $\\epsilon  0$ defined by\n$$\nH_\\epsilon(s) = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{s}{\\epsilon}\\right).\n$$\nThe predicted property field is modeled as $u(\\mathbf{x};\\epsilon,\\theta,r) = \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}))$, where $\\theta$ is a scalar amplitude parameter. Suppose measurements $y_i$ are collected at grid points $\\mathbf{x}_i$ and modeled as noisy observations of a ground truth interface $\\phi^\\star(\\mathbf{x})=\\|\\mathbf{x}\\| - r^\\star$ with parameters $r^\\star$, $\\epsilon^\\star$, $\\theta^\\star$. The measurement model is\n$$\ny_i = \\theta^\\star H_{\\epsilon^\\star}(\\phi^\\star(\\mathbf{x}_i)) + \\eta_i,\n$$\nwhere $\\eta_i$ are independent identically distributed zero-mean Gaussian random variables with standard deviation $\\sigma$, made deterministic by a fixed pseudo-random seed.\n\nWe consider the standard least-squares data misfit with Tikhonov regularization on $\\theta$:\n$$\nJ(\\epsilon,\\theta;r) = \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 + \\frac{\\lambda}{2}\\,\\theta^2,\n$$\nwhere $\\lambda0$ is the regularization weight, $M$ is the number of grid points, and all sums are taken over the fixed measurement grid. The goal is to study differentiability and compute the gradient of $J$ with respect to the smoothing parameter $\\epsilon$ and the amplitude parameter $\\theta$, holding $r$ fixed per test case. Assume $\\epsilon0$ and $\\theta\\in\\mathbb{R}$.\n\nStarting from core definitions of the Level Set Method and the smoothed Heaviside function, and using the chain rule for differentiation, derive expressions for $\\frac{\\partial J}{\\partial \\epsilon}$ and $\\frac{\\partial J}{\\partial \\theta}$ in terms of $\\epsilon$, $\\theta$, $r$, and the observed data $(\\mathbf{x}_i,y_i)$. Then implement a program that computes these gradients for a specified measurement grid and test suite.\n\nUse the following measurement setup:\n- Domain $\\Omega = [-1,1]\\times[-1,1]$.\n- A uniform Cartesian grid of $N\\times N$ points with $N=61$ in each dimension, i.e., $M=N^2$ points $\\mathbf{x}_i$.\n- Ground truth parameters: $r^\\star = 0.45$, $\\epsilon^\\star = 0.02$, $\\theta^\\star = 0.9$.\n- Noise standard deviation: $\\sigma = 0.01$ with pseudo-random seed fixed at $0$.\n- Ground truth level set: $\\phi^\\star(\\mathbf{x}) = \\|\\mathbf{x}\\| - r^\\star$.\n- Measurement model: $y_i=\\theta^\\star H_{\\epsilon^\\star}(\\phi^\\star(\\mathbf{x}_i)) + \\eta_i$ as above.\n\nFor each test case $(\\epsilon,\\theta,r,\\lambda)$, compute and return the pair $\\left[\\frac{\\partial J}{\\partial \\epsilon},\\frac{\\partial J}{\\partial \\theta}\\right]$ using the derived formulas.\n\nProvide a test suite with the following parameter sets:\n- Test $1$: $(\\epsilon,\\theta,r,\\lambda) = (0.05,1.0,0.5,0.01)$.\n- Test $2$: $(\\epsilon,\\theta,r,\\lambda) = (1\\times 10^{-4},0.8,0.5,0.01)$.\n- Test $3$: $(\\epsilon,\\theta,r,\\lambda) = (0.5,1.2,0.7,0.05)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a sublist with two floats rounded to six decimal places. The final output format must be\n$$\n\\text{\"[[dJ\\_depsilon\\_1,dJ\\_dtheta\\_1],[dJ\\_depsilon\\_2,dJ\\_dtheta\\_2],[dJ\\_depsilon\\_3,dJ\\_dtheta\\_3]]\"}\n$$\nwith each float rounded to six decimal places and no additional text.",
            "solution": "The problem is valid as it is scientifically grounded in the field of inverse problems, well-posed, objective, and provides a complete and consistent set of definitions and data. We can thus proceed with the derivation and solution.\n\nThe objective is to derive analytical expressions for the partial derivatives of the cost functional $J(\\epsilon,\\theta;r)$ with respect to the smoothing parameter $\\epsilon$ and the amplitude parameter $\\theta$.\n\nThe cost functional is given by:\n$$\nJ(\\epsilon,\\theta;r) = \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 + \\frac{\\lambda}{2}\\,\\theta^2\n$$\nwhere $M$ is the total number of measurement points. The predicted property field $u(\\mathbf{x};\\epsilon,\\theta,r)$ is defined as:\n$$\nu(\\mathbf{x};\\epsilon,\\theta,r) = \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}))\n$$\nwith the level set function $\\phi(\\mathbf{x}) = \\|\\mathbf{x}\\| - r$ for a specific test case with radius $r$. The smoothed Heaviside function $H_\\epsilon(s)$ is:\n$$\nH_\\epsilon(s) = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{s}{\\epsilon}\\right)\n$$\n\n### Derivation of the Gradient with respect to $\\theta$\n\nTo find the partial derivative of $J$ with respect to $\\theta$, denoted $\\frac{\\partial J}{\\partial \\theta}$, we differentiate the cost functional term by term.\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 \\right] + \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{\\lambda}{2}\\,\\theta^2 \\right]\n$$\nApplying the chain rule to the summation term, we get:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) \\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\theta} + \\lambda\\theta\n$$\nNext, we find the partial derivative of the model $u$ with respect to $\\theta$:\n$$\n\\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}_i)) \\right] = H_\\epsilon(\\phi(\\mathbf{x}_i))\n$$\nsince $H_\\epsilon(\\phi(\\mathbf{x}_i))$ does not depend on $\\theta$. Substituting this result back into the expression for $\\frac{\\partial J}{\\partial \\theta}$ yields the final formula:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) H_\\epsilon(\\phi(\\mathbf{x}_i)) + \\lambda\\theta\n$$\nFor implementation, we can substitute the definition of $u$:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i=1}^M \\left(\\theta\\,H_\\epsilon(\\phi(\\mathbf{x}_i)) - y_i\\right) H_\\epsilon(\\phi(\\mathbf{x}_i)) + \\lambda\\theta\n$$\n\n### Derivation of the Gradient with respect to $\\epsilon$\n\nTo find the partial derivative of $J$ with respect to $\\epsilon$, denoted $\\frac{\\partial J}{\\partial \\epsilon}$, we differentiate the cost functional. The Tikhonov regularization term $\\frac{\\lambda}{2}\\theta^2$ does not depend on $\\epsilon$, so its derivative is $0$.\n$$\n\\frac{\\partial J}{\\partial \\epsilon} = \\frac{\\partial}{\\partial \\epsilon} \\left[ \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 \\right] = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) \\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\epsilon}\n$$\nWe must now find the partial derivative of the model $u$ with respect to $\\epsilon$:\n$$\n\\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\epsilon} = \\frac{\\partial}{\\partial \\epsilon} \\left[ \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}_i)) \\right] = \\theta \\frac{\\partial H_\\epsilon(s_i)}{\\partial \\epsilon}\n$$\nwhere we let $s_i = \\phi(\\mathbf{x}_i)$ for notational simplicity. The derivative of the smoothed Heaviside function $H_\\epsilon(s)$ with respect to $\\epsilon$ is found using the chain rule. The derivative of $\\arctan(z)$ with respect to $z$ is $\\frac{1}{1+z^2}$.\n$$\n\\frac{\\partial H_\\epsilon(s)}{\\partial \\epsilon} = \\frac{\\partial}{\\partial \\epsilon}\\left[ \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{s}{\\epsilon}\\right) \\right] = \\frac{1}{\\pi} \\frac{\\partial}{\\partial \\epsilon}\\left[\\arctan\\left(\\frac{s}{\\epsilon}\\right)\\right]\n$$\n$$\n\\frac{\\partial H_\\epsilon(s)}{\\partial \\epsilon} = \\frac{1}{\\pi} \\left( \\frac{1}{1 + (s/\\epsilon)^2} \\right) \\cdot \\frac{\\partial}{\\partial \\epsilon}\\left(\\frac{s}{\\epsilon}\\right) = \\frac{1}{\\pi} \\left( \\frac{\\epsilon^2}{\\epsilon^2 + s^2} \\right) \\cdot \\left( -\\frac{s}{\\epsilon^2} \\right) = -\\frac{s}{\\pi(\\epsilon^2 + s^2)}\n$$\nSubstituting this back into the expression for $\\frac{\\partial u}{\\partial \\epsilon}$ at each point $\\mathbf{x}_i$:\n$$\n\\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\epsilon} = \\theta \\left( -\\frac{\\phi(\\mathbf{x}_i)}{\\pi(\\epsilon^2 + \\phi(\\mathbf{x}_i)^2)} \\right)\n$$\nFinally, we obtain the expression for $\\frac{\\partial J}{\\partial \\epsilon}$:\n$$\n\\frac{\\partial J}{\\partial \\epsilon} = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) \\left( -\\frac{\\theta \\phi(\\mathbf{x}_i)}{\\pi(\\epsilon^2 + \\phi(\\mathbf{x}_i)^2)} \\right)\n$$\nAgain, substituting the definition of $u$ for implementation:\n$$\n\\frac{\\partial J}{\\partial \\epsilon} = \\sum_{i=1}^M \\left(\\theta H_\\epsilon(\\phi(\\mathbf{x}_i)) - y_i\\right) \\left( -\\frac{\\theta \\phi(\\mathbf{x}_i)}{\\pi(\\epsilon^2 + \\phi(\\mathbf{x}_i)^2)} \\right)\n$$\n\nThese derived expressions for $\\frac{\\partial J}{\\partial \\theta}$ and $\\frac{\\partial J}{\\partial \\epsilon}$ are implemented in the following program. The program first generates the synthetic measurement data $y_i$ based on the ground truth parameters. Then, for each test case, it computes the gradients using the derived analytical formulae.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing gradients of a least-squares functional\n    in a level-set based inverse problem.\n    \"\"\"\n\n    # --- Measurement Setup ---\n    # Domain and grid\n    N = 61\n    domain = [-1.0, 1.0]\n\n    # Ground truth parameters\n    r_star = 0.45\n    eps_star = 0.02\n    theta_star = 0.9\n\n    # Noise parameters\n    sigma = 0.01\n    seed = 0\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (epsilon, theta, r, lambda)\n        (0.05, 1.0, 0.5, 0.01),\n        (1e-4, 0.8, 0.5, 0.01),\n        (0.5, 1.2, 0.7, 0.05),\n    ]\n\n    def smoothed_heaviside(s, epsilon):\n        \"\"\"Computes the smoothed Heaviside function H_epsilon(s).\"\"\"\n        return 0.5 + (1.0 / np.pi) * np.arctan(s / epsilon)\n\n    def generate_measurement_data():\n        \"\"\"Generates the grid and synthetic measurement data y_i.\"\"\"\n        # Create a uniform Cartesian grid\n        lin_space = np.linspace(domain[0], domain[1], N)\n        xx, yy = np.meshgrid(lin_space, lin_space)\n        grid_points = np.vstack([xx.ravel(), yy.ravel()]).T # Shape (M, 2) where M=N*N\n\n        # Compute ground truth level set function values at grid points\n        phi_star_vals = np.linalg.norm(grid_points, axis=1) - r_star\n\n        # Compute true property field without noise\n        u_true = theta_star * smoothed_heaviside(phi_star_vals, eps_star)\n\n        # Generate deterministic pseudo-random noise\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(loc=0.0, scale=sigma, size=N * N)\n\n        # Generate noisy measurement data\n        y_data = u_true + noise\n        \n        return grid_points, y_data\n\n    def compute_gradients(params, grid_points, y_data):\n        \"\"\"\n        Computes the gradients dJ/d(epsilon) and dJ/d(theta) for a given set of parameters.\n        \"\"\"\n        epsilon, theta, r, lam = params\n        \n        # Calculate level set values for the current test case\n        phi_vals = np.linalg.norm(grid_points, axis=1) - r\n        \n        # Calculate smoothed Heaviside values for the current test case\n        H_eps_vals = smoothed_heaviside(phi_vals, epsilon)\n        \n        # Calculate the predicted property field u\n        u_vals = theta * H_eps_vals\n        \n        # Calculate the residual (u_i - y_i)\n        residual = u_vals - y_data\n        \n        # --- Compute Gradient with respect to theta ---\n        # dJ/d(theta) = sum_i( (u_i - y_i) * d(u_i)/d(theta) ) + lambda * theta\n        # d(u_i)/d(theta) = H_epsilon(phi_i)\n        grad_theta = np.sum(residual * H_eps_vals) + lam * theta\n        \n        # --- Compute Gradient with respect to epsilon ---\n        # dJ/d(epsilon) = sum_i( (u_i - y_i) * d(u_i)/d(epsilon) )\n        # d(u_i)/d(epsilon) = theta * d(H_epsilon)/d(epsilon)\n        # d(H_epsilon)/d(epsilon) = -phi_i / (pi * (epsilon^2 + phi_i^2))\n        \n        dH_deps_vals = -phi_vals / (np.pi * (epsilon**2 + phi_vals**2))\n        du_deps_vals = theta * dH_deps_vals\n        grad_epsilon = np.sum(residual * du_deps_vals)\n        \n        return [grad_epsilon, grad_theta]\n\n    # Generate the measurement data once for all test cases\n    grid_points, y_data = generate_measurement_data()\n    \n    # Process all test cases\n    results_str_list = []\n    for params in test_cases:\n        gradients = compute_gradients(params, grid_points, y_data)\n        \n        # Format the output for each test case as a string \"[val1,val2]\"\n        # with values rounded to six decimal places.\n        formatted_grads = [f\"{g:.6f}\" for g in gradients]\n        sublist_str = f\"[{','.join(formatted_grads)}]\"\n        results_str_list.append(sublist_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many real-world inverse problems involve reconstructing properties within a system governed by a partial differential equation (PDE). Directly calculating the shape gradient in this context can be computationally prohibitive. This exercise introduces the powerful adjoint-state method, the standard technique for efficiently computing gradients in PDE-constrained optimization . You will derive the adjoint equation from first principles and implement it to compute a shape gradient, verifying your result with a finite-difference check to build confidence in this essential technique.",
            "id": "3396641",
            "problem": "Consider a two-dimensional square domain $\\Omega = [0,1] \\times [0,1]$ with homogeneous Dirichlet boundary conditions. A piecewise-constant diffusion coefficient $a(\\phi)$ depends on a level set function $\\phi:\\Omega \\to \\mathbb{R}$ that represents an unknown shape $\\mathcal{D} = \\{ x \\in \\Omega \\mid \\phi(x)  0 \\}$. The forward model is the variable-coefficient Poisson equation\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega,\n$$\nwhere $f:\\Omega \\to \\mathbb{R}$ is a known source. The data assimilation objective is to reconstruct the shape $\\mathcal{D}$ by optimizing the level set function $\\phi$ to minimize a data misfit functional\n$$\nJ(\\phi) = \\tfrac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\nwhere $u_{\\mathrm{obs}}:\\Omega \\to \\mathbb{R}$ is an observed state generated from a different, true shape $\\phi_{\\mathrm{true}}$. The coefficient $a(\\phi)$ is defined using a smoothed Heaviside function to ensure differentiability,\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi),\n$$\nwith\n$$\nH_{\\varepsilon}(s) = \\tfrac{1}{2} + \\frac{1}{\\pi}\\arctan\\!\\Big(\\frac{s}{\\varepsilon}\\Big),\n$$\nwhere $a_{\\mathrm{in}}  0$ and $a_{\\mathrm{out}}  0$ are constants, and $\\varepsilon  0$ is a smoothing parameter. The smoothed Dirac delta associated with $H_{\\varepsilon}$ is\n$$\n\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}.\n$$\nThe goal is to derive, from first principles, an adjoint-state formulation and the shape gradient with respect to $\\phi$ under the level set parameterization. Starting only from the strong form of the forward model, the definition of the data misfit functional, and standard calculus of variations, derive the adjoint equation and a pointwise expression for the shape gradient $G(\\phi)$ such that for any suitably smooth perturbation $\\eta:\\Omega \\to \\mathbb{R}$, the first-order directional derivative satisfies\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x.\n$$\nYou must then implement a program that:\n- Discretizes $\\Omega$ on a uniform Cartesian grid with $N \\times N$ nodes and grid spacing $h = 1/(N-1)$.\n- Assembles the discrete operator in flux form for $-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla (\\cdot)\\big)$ using face-centered arithmetic averages of $a$ and homogeneous Dirichlet boundary conditions.\n- Solves the forward problem to compute $u(\\phi)$.\n- Constructs $u_{\\mathrm{obs}}$ by solving the same forward problem with the true shape $\\phi_{\\mathrm{true}}$.\n- Solves the adjoint problem driven by the data misfit residual to obtain the adjoint variable.\n- Computes the shape gradient $G(\\phi)$ at grid nodes using the derived pointwise expression that is consistent with the level set parameterization and the chain rule for $a(\\phi)$.\n- Verifies the adjoint-state identity by comparing the predicted directional derivative $\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$ to a finite-difference approximation $\\big(J(\\phi + t\\,\\eta) - J(\\phi)\\big)/t$ for a small step size $t$.\n\nUse the following deterministic specifications:\n- Source term $f(x,y) = 1$ for all $(x,y) \\in \\Omega$.\n- Observed state $u_{\\mathrm{obs}}$ is computed with a true shape given by a circle: $\\phi_{\\mathrm{true}}(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_{\\mathrm{true}}$ with $r_{\\mathrm{true}} = 0.25$.\n- The perturbation is $\\eta(x,y) = \\sin(2\\pi x)\\sin(2\\pi y)$.\n- The initial level set function $\\phi$ is either a circle $\\phi(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_0$ or a constant function $\\phi(x,y) \\equiv c_0$ as specified by each test case.\n- The smoothed Heaviside is $H_{\\varepsilon}(-\\phi)$ and its derivative with respect to $\\phi$ is $-\\delta_{\\varepsilon}(\\phi)$, with $\\delta_{\\varepsilon}(\\phi)$ as defined above.\n- The inner products and integrals must be approximated by their standard Riemann sums on the grid using weight $h^2$ per node.\n\nYour program must compute, for each test case, the relative consistency error between the adjoint-predicted directional derivative and the finite-difference approximation,\n$$\nE = \\frac{\\left|\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x - \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t}\\right|}{\\max\\!\\Big(\\lvert \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x \\rvert + \\left\\lvert \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t} \\right\\rvert, \\,10^{-12}\\Big)}.\n$$\n\nTest suite:\n- Case $1$: $N = 64$, $\\varepsilon = 0.02$, $t = 10^{-3}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is a circle with $r_0 = 0.35$.\n- Case $2$: $N = 64$, $\\varepsilon = 0.005$, $t = 10^{-3}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is a circle with $r_0 = 0.35$.\n- Case $3$: $N = 16$, $\\varepsilon = 0.02$, $t = 10^{-3}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is a circle with $r_0 = 0.35$.\n- Case $4$: $N = 64$, $\\varepsilon = 0.02$, $t = 10^{-4}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is the constant $c_0 = 0.5$, which should yield a near-zero directional derivative for sufficiently small $t$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four relative errors for the test suite as a comma-separated list enclosed in square brackets (for example, `[e1,e2,e3,e4]`). No additional text should be printed.",
            "solution": "The user-provided problem is a well-posed and scientifically sound task in the field of PDE-constrained optimization and inverse problems. It requests the derivation of the adjoint-state equations for a shape optimization problem governed by a variable-coefficient Poisson equation, followed by a numerical implementation to verify the derived gradient. The problem is self-contained, with all parameters, governing equations, and numerical specifications clearly defined. It adheres to established principles of calculus of variations and numerical analysis. Therefore, the problem is deemed **valid**.\n\n### Derivation of the Shape Gradient using the Adjoint Method\n\nThe objective is to find the gradient of the functional $J(\\phi)$ with respect to the level set function $\\phi$. The functional is\n$$\nJ(\\phi) = \\frac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\nwhere the state variable $u(\\phi)$ is the solution to the forward problem:\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega.\n$$\nThe weak form of the forward problem is to find $u \\in H^1_0(\\Omega)$ such that for all test functions $v \\in H^1_0(\\Omega)$:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} f\\,v \\,\\mathrm{d}x.\n$$\n\nWe seek the Gâteaux derivative of $J(\\phi)$ in an arbitrary direction $\\eta$, which is defined as\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\lim_{t \\to 0} \\frac{J(\\phi + t\\eta) - J(\\phi)}{t}.\n$$\nApplying the chain rule to the definition of $J(\\phi)$, we get:\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)\\,\\dot{u} \\,\\mathrm{d}x,\n$$\nwhere $\\dot{u} = \\frac{\\mathrm{d}u}{\\mathrm{d}\\phi}[\\eta]$ is the sensitivity of the state $u$ with respect to a change in $\\phi$ in the direction $\\eta$.\n\nTo find an equation for $\\dot{u}$, we differentiate the weak form of the state equation with respect to $\\phi$ in the direction $\\eta$. The right-hand side is independent of $\\phi$, so its derivative is zero. Differentiating the left-hand side gives:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\int_{\\Omega} a(\\phi+t\\eta)\\,\\nabla u(\\phi+t\\eta) \\cdot \\nabla v \\,\\mathrm{d}x \\right) \\bigg|_{t=0} = 0.\n$$\nApplying the product rule, we obtain the tangent equation:\n$$\n\\int_{\\Omega} \\dot{a}\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x + \\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = 0,\n$$\nwhere $\\dot{a} = \\frac{\\mathrm{d}a}{\\mathrm{d}\\phi}[\\eta]$. The term $\\dot{a}$ is found by applying the chain rule to the definition of $a(\\phi)$:\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi).\n$$\nThe derivative of the smoothed Heaviside function $H_{\\varepsilon}(s)$ is the smoothed Dirac delta $\\delta_{\\varepsilon}(s)$. As specified, the derivative of $H_{\\varepsilon}(-\\phi)$ with respect to $\\phi$ is $-\\delta_{\\varepsilon}(\\phi)$, where $\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}$. Thus,\n$$\n\\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} = (a_{\\mathrm{in}} - a_{\\mathrm{out}}) \\frac{\\mathrm{d}}{\\mathrm{d}\\phi} H_{\\varepsilon}(-\\phi) = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi).\n$$\nThis implies $\\dot{a} = \\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} \\eta = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta$.\nSubstituting this into the tangent equation gives:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla v) \\,\\mathrm{d}x.\n$$\n\nThe adjoint method is introduced to avoid explicitly solving for $\\dot{u}$. We define an adjoint state $p \\in H^1_0(\\Omega)$ as the solution to the following adjoint equation: find $p$ such that for all test functions $w \\in H^1_0(\\Omega)$,\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla w \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,w \\,\\mathrm{d}x.\n$$\nThe strong form of the adjoint equation is:\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla p\\big) = u - u_{\\mathrm{obs}} \\quad \\text{in } \\Omega, \\qquad p = 0 \\quad \\text{on } \\partial\\Omega.\n$$\nNote that the differential operator is self-adjoint. The source term for the adjoint equation is the residual of the data misfit.\n\nNow, we strategically choose the test function $w$ in the adjoint equation's weak form to be the state sensitivity, $w=\\dot{u}$. This gives:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,\\dot{u} \\,\\mathrm{d}x.\n$$\nThe right-hand side is exactly the expression for the directional derivative $\\mathrm{D}J(\\phi)[\\eta]$. Therefore,\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x.\n$$\nBecause the bilinear form is symmetric, we can write $\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x$.\nWe can now use the tangent equation, with the test function chosen as the adjoint state, $v=p$:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla p) \\,\\mathrm{d}x.\n$$\nBy equating the expressions for the directional derivative, we arrive at the final form:\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\Big( (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p) \\Big)\\,\\eta \\,\\mathrm{d}x.\n$$\nThis expression is in the required form $\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$. We can therefore identify the pointwise shape gradient $G(\\phi)$ as:\n$$\nG(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).\n$$\n\n### Summary of the Adjoint-State System for Gradient Computation\n1.  **Solve the Forward Problem for $u$**: Given $\\phi$, solve for $u$:\n    $$-\\nabla \\cdot (a(\\phi)\\nabla u) = f, \\quad u|_{\\partial\\Omega}=0.$$\n2.  **Solve the Adjoint Problem for $p$**: Using the solution $u$ from step 1, solve for $p$:\n    $$-\\nabla \\cdot (a(\\phi)\\nabla p) = u - u_{\\mathrm{obs}}, \\quad p|_{\\partial\\Omega}=0.$$\n3.  **Compute the Gradient $G(\\phi)$**: Using $u$ and $p$, compute the gradient:\n    $$G(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).$$\nThis completes the derivation. The subsequent Python code implements this three-step procedure numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main driver function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'N': 64, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 2\n        {'N': 64, 'eps': 0.005, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 3\n        {'N': 16, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 4\n        {'N': 64, 'eps': 0.02, 't': 1e-4, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'const', 'phi_param': 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_case(**params)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\ndef run_case(N, eps, t, a_in, a_out, phi_type, phi_param):\n    \"\"\"\n    Executes a single test case for the adjoint verification.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    y = np.linspace(0, 1, N)\n    X, Y = np.meshgrid(x, y)\n\n    # Helper functions for level set, Heaviside, and delta\n    def get_phi(X, Y, type, param):\n        if type == 'circle':\n            radius = param\n            return np.sqrt((X - 0.5)**2 + (Y - 0.5)**2) - radius\n        elif type == 'const':\n            c0 = param\n            return np.full_like(X, c0)\n        \n    def H_eps(s, eps_val):\n        return 0.5 + (1.0 / np.pi) * np.arctan(s / eps_val)\n\n    def delta_eps(s, eps_val):\n        return (1.0 / np.pi) * eps_val / (eps_val**2 + s**2)\n\n    def get_a(phi, a_in_val, a_out_val, eps_val):\n        return a_out_val + (a_in_val - a_out_val) * H_eps(-phi, eps_val)\n\n    # --- Finite Difference Poisson Solver ---\n    def solve_poisson(a_coeff, rhs, h_val, N_val):\n        num_interior = (N_val - 2)**2\n        A = lil_matrix((num_interior, num_interior), dtype=np.float64)\n        \n        # Assemble matrix A using a 5-point stencil for -div(a grad(u))\n        for i in range(1, N_val - 1):\n            for j in range(1, N_val - 1):\n                k = (i - 1) * (N_val - 2) + (j - 1)\n\n                # Face-centered arithmetic means for 'a'\n                a_south = (a_coeff[i-1, j] + a_coeff[i, j]) / 2.0\n                a_north = (a_coeff[i+1, j] + a_coeff[i, j]) / 2.0\n                a_west  = (a_coeff[i, j-1] + a_coeff[i, j]) / 2.0\n                a_east  = (a_coeff[i, j+1] + a_coeff[i, j]) / 2.0\n                \n                A[k, k] = -(a_north + a_south + a_east + a_west)\n\n                if i  1:\n                    A[k, k - (N_val - 2)] = a_south # South neighbor\n                if i  N_val - 2:\n                    A[k, k + (N_val - 2)] = a_north # North neighbor\n                if j  1:\n                    A[k, k - 1] = a_west           # West neighbor\n                if j  N_val - 2:\n                    A[k, k + 1] = a_east           # East neighbor\n\n        A /= h_val**2\n        \n        # Flatten the right-hand side for interior nodes\n        b = rhs[1:-1, 1:-1].flatten()\n        \n        # Solve the linear system\n        u_interior = spsolve(A.tocsr(), b)\n        \n        # Embed solution back into the full grid with boundary conditions\n        u_full = np.zeros((N_val, N_val))\n        u_full[1:-1, 1:-1] = u_interior.reshape((N_val - 2, N_val - 2))\n        return u_full\n\n    # Function to compute the objective functional J\n    def compute_J(u, u_obs, h_val):\n        return 0.5 * np.sum((u - u_obs)**2) * h_val**2\n\n    # --- Main Calculation Steps ---\n    \n    # 1. Compute observed state u_obs\n    f_source = np.ones((N, N))\n    r_true = 0.25\n    phi_true = get_phi(X, Y, 'circle', r_true)\n    a_true = get_a(phi_true, a_in, a_out, eps)\n    u_obs = solve_poisson(a_true, f_source, h, N)\n    \n    # 2. Define initial state phi and perturbation eta\n    phi = get_phi(X, Y, phi_type, phi_param)\n    eta = np.sin(2 * np.pi * X) * np.sin(2 * np.pi * Y)\n\n    # 3. Solve forward problem for u(phi) and compute J(phi)\n    a_phi = get_a(phi, a_in, a_out, eps)\n    u_phi = solve_poisson(a_phi, f_source, h, N)\n    J_phi = compute_J(u_phi, u_obs, h)\n    \n    # 4. Compute finite difference approximation of the directional derivative\n    phi_pert = phi + t * eta\n    a_pert = get_a(phi_pert, a_in, a_out, eps)\n    u_pert = solve_poisson(a_pert, f_source, h, N)\n    J_pert = compute_J(u_pert, u_obs, h)\n    \n    DJ_fd = (J_pert - J_phi) / t\n\n    # 5. Compute adjoint-based directional derivative\n    # 5a. Solve adjoint equation: -div(a grad(p)) = u - u_obs\n    adjoint_rhs = u_phi - u_obs\n    p = solve_poisson(a_phi, adjoint_rhs, h, N)\n\n    # 5b. Compute gradients of u and p\n    grad_u_y, grad_u_x = np.gradient(u_phi, h)\n    grad_p_y, grad_p_x = np.gradient(p, h)\n    \n    # 5c. Compute the shape gradient G(phi)\n    grad_u_dot_grad_p = grad_u_x * grad_p_x + grad_u_y * grad_p_y\n    G_phi = (a_in - a_out) * delta_eps(phi, eps) * grad_u_dot_grad_p\n    \n    # 5d. Compute the directional derivative via inner product\n    DJ_adj = np.sum(G_phi * eta) * h**2\n    \n    # 6. Calculate the relative consistency error\n    numerator = np.abs(DJ_adj - DJ_fd)\n    denominator = max(np.abs(DJ_adj) + np.abs(DJ_fd), 1e-12)\n    error = numerator / denominator\n\n    return error\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "Having learned to compute shape gradients, the next step is to use them to drive a full reconstruction algorithm. This practice guides you through implementing a gradient descent optimization for a level set function, incorporating a perimeter regularization term to ensure a stable solution . Crucially, you will tackle the challenge of selecting the regularization parameter $\\alpha$ by implementing the Morozov discrepancy principle, a classic and robust method for balancing data fidelity with regularization.",
            "id": "3396597",
            "problem": "Consider a shape reconstruction inverse problem in which an unknown subset of a rectangular domain is represented implicitly as the zero level set of a scalar field. Let the domain be a discrete grid of size $N \\times N$ with $N = 48$. A shape is represented by a level set function $\\phi : \\{1,\\dots,N\\} \\times \\{1,\\dots,N\\} \\to \\mathbb{R}$ such that the reconstructed indicator field is $u = H_{\\varepsilon}(\\phi)$, where $H_{\\varepsilon}$ is a smoothed Heaviside function with smoothing parameter $\\varepsilon > 0$. Observations are generated by a known linear, symmetric forward operator $A$ applied to $u$, contaminated by additive independent Gaussian noise with known standard deviation $\\sigma$. The reconstruction is posed as the minimization of a Tikhonov-like functional with a perimeter prior expressed in the level set framework.\n\nUse the following fundamental base:\n- The level set representation of a binary shape via a Heaviside function $H_{\\varepsilon}(\\phi)$ and its smoothed Dirac delta $\\delta_{\\varepsilon}(\\phi) = H_{\\varepsilon}'(\\phi)$.\n- The Morozov discrepancy principle: choose the regularization parameter so that the data misfit equals the noise level in the appropriate norm.\n- The gradient flow obtained from the variational derivative of the energy functional, using standard calculus of variations.\n\nLet the forward operator $A$ be a Gaussian blur with standard deviation $\\sigma_{b} = 1.2$ grid units and reflective boundary conditions. Let the smoothed Heaviside and smoothed Dirac delta be defined by\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right), \\quad\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\tfrac{\\varepsilon}{\\varepsilon^{2} + x^{2}},\n$$\nwith $\\varepsilon = 1.0$.\n\nLet the true shape be a disk of radius $r_{\\mathrm{true}} = 12$ pixels centered at $(c_x,c_y) = (24,30)$, represented by the signed distance level set $\\phi_{\\mathrm{true}}(i,j) = \\sqrt{(i-c_x)^2 + (j-c_y)^2} - r_{\\mathrm{true}}$. The initial guess is a disk of radius $r_{0} = 10$ pixels centered at $(c_{0x},c_{0y}) = (16,18)$, i.e., $\\phi_{0}(i,j) = \\sqrt{(i-c_{0x})^2 + (j-c_{0y})^2} - r_{0}$. The noise-free data are $y_{\\mathrm{clean}} = A(H_{\\varepsilon}(\\phi_{\\mathrm{true}}))$ and the observed data are $y = y_{\\mathrm{clean}} + \\eta$, where $\\eta$ has independent Gaussian entries with zero mean and standard deviation $\\sigma$. The number of data points is $m = N^{2}$.\n\nFor a given regularization parameter $\\alpha  0$, consider the energy\n$$\nJ(\\phi;\\alpha) = \\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2} + \\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x,\n$$\nwhere $\\Omega$ is the discrete grid and $\\lVert \\cdot \\rVert_{2}$ is the Euclidean norm on $\\mathbb{R}^{m}$. Using the calculus of variations and the chain rule, a gradient descent for $\\phi$ with time step $\\Delta t$ can be written in discrete form as\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\delta_{\\varepsilon}(\\phi^{k}) \\cdot A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) + \\alpha \\, \\delta_{\\varepsilon}(\\phi^{k}) \\, \\kappa(\\phi^{k}) \\right),\n$$\nwhere $A^{\\top} = A$ (symmetry of the Gaussian blur) and $\\kappa(\\phi)$ is the curvature of the zero level set given by\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right),\n$$\nwith a small stabilizer $\\beta = 10^{-8}$. The spatial derivatives are approximated with central differences and reflective boundary conditions. Use $\\Delta t = 0.2$ and perform $K = 80$ iterations per evaluation of $J$ for a given $\\alpha$.\n\nThe Morozov discrepancy principle prescribes choosing $\\alpha$ such that the residual norm equals the noise level up to a multiplicative factor: select $\\alpha$ so that\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} \\approx \\tau \\, \\sigma \\, \\sqrt{m},\n$$\nwhere $\\phi_{\\alpha}^{\\star}$ is the result of the gradient descent after $K$ iterations starting from $\\phi_{0}$. Implement a bracketing and bisection procedure on $\\alpha$ using an initial bracket $[\\alpha_{\\min},\\alpha_{\\max}] = [10^{-6}, 1.0]$, doubling $\\alpha_{\\max}$ until the residual at $\\alpha_{\\max}$ is not smaller than the target discrepancy or until $\\alpha_{\\max}$ exceeds $10^{6}$. Use a relative tolerance $\\rho = 10^{-2}$ on the discrepancy in the bisection; if the target cannot be bracketed, choose the endpoint that yields the closest residual to the target.\n\nTest suite. For each case below, generate the noise with a fixed pseudo-random seed equal to $s_{0} + i$, where $s_{0} = 12345$ and $i$ is the $0$-based index of the test case. For each case, compute the selected regularization parameter $\\alpha^{\\star}$ by the Morozov discrepancy principle as described. The cases are:\n- Case $1$: $(\\sigma,\\tau) = (0.05, 1.0)$.\n- Case $2$: $(\\sigma,\\tau) = (0.01, 1.0)$.\n- Case $3$: $(\\sigma,\\tau) = (0.10, 1.1)$.\n- Case $4$: $(\\sigma,\\tau) = (0.08, 0.9)$.\n\nFinal output format. Your program should produce a single line of output containing the selected $\\alpha^{\\star}$ for each case as a comma-separated list enclosed in square brackets, with each number rounded to $6$ decimal places (e.g., \"[0.123456,0.234567,0.345678,0.456789]\"). No additional text should be printed. All angles, if any, are measured in radians. There are no physical units beyond the discrete grid spacing; report the requested values as dimensionless numbers. Ensure the program is fully deterministic and does not require any user input.",
            "solution": "The user-provided problem is a well-posed and self-contained exercise in computational inverse problems, specifically focusing on shape reconstruction using a level set-based method. The problem is scientifically sound, with all components—the physical model, mathematical formulation, and numerical algorithms—being standard and clearly defined. The task is to determine an optimal regularization parameter, $\\alpha$, for four different scenarios using the Morozov discrepancy principle. This will be achieved by implementing a gradient descent optimization for the level set function and a bisection search for the parameter $\\alpha$.\n\nFirst, we formalize the problem. The unknown shape is represented by the zero level set of a function $\\phi(x,y)$, defined on a discrete grid $\\Omega$ of size $N \\times N$, where $N=48$. The shape's indicator function, $u$, which is $1$ inside the shape and $0$ outside, is approximated by a smoothed Heaviside function $H_{\\varepsilon}(\\phi)$. The problem specifies:\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right)\n$$\nwith a smoothing width of $\\varepsilon = 1.0$. The derivative of this function is the smoothed Dirac delta, $\\delta_{\\varepsilon}(x) = H_{\\varepsilon}'(x)$, given by:\n$$\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\frac{\\varepsilon}{\\varepsilon^{2} + x^{2}}\n$$\nThis function localizes computations to the vicinity of the interface (where $\\phi \\approx 0$).\n\nThe data $y$ are generated by applying a linear forward operator $A$ to the true indicator function $u_{\\mathrm{true}} = H_{\\varepsilon}(\\phi_{\\mathrm{true}})$ and adding Gaussian noise $\\eta$. The operator $A$ is a Gaussian blur with standard deviation $\\sigma_b = 1.2$ grid units and is symmetric ($A=A^\\top$). The true shape is a disk of radius $r_{\\mathrm{true}} = 12$ centered at $(c_x, c_y) = (24, 30)$, and the initial guess for the reconstruction is a disk of radius $r_0 = 10$ centered at $(c_{0x}, c_{0y}) = (16, 18)$. The noise $\\eta$ is drawn from a Gaussian distribution with mean $0$ and standard deviation $\\sigma$.\n\nThe reconstruction $\\phi$ is found by minimizing the Tikhonov-like energy functional $J(\\phi;\\alpha)$:\n$$\nJ(\\phi;\\alpha) = \\underbrace{\\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2}}_{\\text{Data Fidelity Term}} + \\underbrace{\\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x}_{\\text{Perimeter Regularization}}\n$$\nThe first term ensures the reconstructed shape, when blurred, matches the observed data $y$. The second term is a perimeter penalty, which regularizes the problem by favoring shapes with shorter boundary lengths, thus promoting smoother and more compact reconstructions. $\\alpha0$ is the regularization parameter that balances these two competing objectives.\n\nTo minimize $J(\\phi;\\alpha)$, we use a gradient descent algorithm. The evolution of $\\phi$ over a fictitious time $t$ follows the negative gradient of the energy. The problem provides the discrete update rule:\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\frac{\\delta J}{\\delta \\phi}(\\phi^k) \\right)\n$$\nwhere a standard, simplified form of the variational derivative $\\frac{\\delta J}{\\delta \\phi}$ is used, leading to the update equation:\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( -\\delta_{\\varepsilon}(\\phi^{k}) A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) + \\alpha \\delta_{\\varepsilon}(\\phi^{k}) \\kappa(\\phi^{k}) \\right)\n$$\nHere, $\\Delta t = 0.2$ is the time step. The first term inside the parentheses is the data-driven force, pulling the level set to match the data. The second term is the geometric regularization force, which moves the level set according to its mean curvature $\\kappa(\\phi)$, effectively smoothing the boundary. The curvature is given by:\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right)\n$$\nwith a small stabilizer $\\beta=10^{-8}$. All spatial derivatives (gradient $\\nabla$ and divergence $\\nabla \\cdot$) are approximated using central differences with reflective boundary conditions, which is implemented by padding the grid before differentiation.\n\nThe core of the problem is selecting the parameter $\\alpha$. The Morozov discrepancy principle is employed for this purpose. It dictates that the regularization parameter $\\alpha$ should be chosen such that the Euclidian norm of the data residual for the optimal solution $\\phi^{\\star}_{\\alpha}$ matches the expected norm of the noise. The target is formulated as:\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} = \\tau \\, \\sigma \\, \\sqrt{m}\n$$\nwhere $m = N^2 = 48^2$ is the total number of data points (pixels) and $\\tau$ is a given factor close to $1$. The term $\\sigma\\sqrt{m}$ is the expected $L_2$-norm of the noise vector. For each given $(\\sigma, \\tau)$, we must find the corresponding $\\alpha^{\\star}$.\n\nTo find $\\alpha^{\\star}$, we define a function $D(\\alpha) = \\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2}$, where $\\phi_{\\alpha}^{\\star}$ is the result of running the gradient descent for $K=80$ iterations with parameter $\\alpha$. The function $D(\\alpha)$ is generally monotonic increasing with $\\alpha$. The problem of finding $\\alpha^{\\star}$ reduces to solving the scalar equation $D(\\alpha) = \\tau \\sigma \\sqrt{m}$. This is solved numerically using a bracketing and bisection algorithm:\n1.  **Bracketing**: An initial search interval $[\\alpha_{\\min}, \\alpha_{\\max}] = [10^{-6}, 1.0]$ is established. If the discrepancy at $\\alpha_{\\max}$ is less than the target, $\\alpha_{\\max}$ is repeatedly doubled until the target is bracketed (i.e., $D(\\alpha_{\\min})  \\text{target}  D(\\alpha_{\\max})$) or $\\alpha_{\\max}$ exceeds a cap of $10^6$. If bracketing fails, the endpoint of the search range yielding the discrepancy closest to the target is chosen.\n2.  **Bisection**: Once a bracket is found, the bisection method is used to narrow down the interval, iteratively halving it until the discrepancy at the midpoint $\\alpha_m$ is within a relative tolerance $\\rho=10^{-2}$ of the target value.\n\nThe overall algorithm for each test case is as follows:\n1.  Generate the true signed distance function $\\phi_{\\mathrm{true}}$, the true indicator $u_{\\mathrm{true}}$, and the clean data $y_{\\mathrm{clean}} = A(u_{\\mathrm{true}})$.\n2.  Generate the noise vector $\\eta$ with the specified standard deviation $\\sigma$ and random seed, and form the noisy data $y = y_{\\mathrm{clean}} + \\eta$.\n3.  Calculate the target discrepancy $T = \\tau \\sigma \\sqrt{m}$.\n4.  Execute the bracketing-and-bisection search for $\\alpha$ to find the value $\\alpha^{\\star}$ that solves $D(\\alpha) = T$ to the specified tolerance.\n5.  Store the resulting $\\alpha^{\\star}$ and repeat for the next test case.\nFinally, the computed values of $\\alpha^{\\star}$ for all four cases are reported in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import ndimage\n\ndef solve():\n    \"\"\"\n    Solves the level set shape reconstruction problem by finding the optimal\n    regularization parameter alpha using the Morozov discrepancy principle.\n    \"\"\"\n    \n    # Constants from the problem statement\n    N = 48\n    EPSILON = 1.0\n    SIGMA_B = 1.2\n    R_TRUE = 12.0\n    CX_TRUE, CY_TRUE = 23, 29  # 0-based from 1-based (24, 30)\n    R0 = 10.0\n    CX0, CY0 = 15, 17  # 0-based from 1-based (16, 18)\n    BETA = 1e-8\n    DT = 0.2\n    K = 80\n    ALPHA_MIN_INIT = 1e-6\n    ALPHA_MAX_INIT = 1.0\n    ALPHA_MAX_CAP = 1e6\n    RHO = 1e-2\n    S0 = 12345\n    M = N * N\n\n    # 1. Mathematical Functions\n    def smoothed_heaviside(x):\n        \"\"\" Smoothed Heaviside function H_epsilon(x). \"\"\"\n        return 0.5 + (1.0 / np.pi) * np.arctan(x / EPSILON)\n\n    def smoothed_delta(x):\n        \"\"\" Smoothed Dirac delta function delta_epsilon(x). \"\"\"\n        return (1.0 / np.pi) * (EPSILON / (EPSILON**2 + x**2))\n\n    def create_sdf_disk(n_grid, cx, cy, r):\n        \"\"\" Creates a signed distance function for a disk. \"\"\"\n        y_coords, x_coords = np.mgrid[0:n_grid, 0:n_grid]\n        return np.sqrt((x_coords - cx)**2 + (y_coords - cy)**2) - r\n\n    # 2. Operators and Derivatives\n    def forward_operator_A(u):\n        \"\"\" Forward operator A: Gaussian blur with reflective boundaries. \"\"\"\n        return ndimage.gaussian_filter(u, sigma=SIGMA_B, mode='reflect')\n\n    def gradient_cd_reflect(f):\n        \"\"\" Computes gradient (fy, fx) using central differences and reflective padding. \"\"\"\n        f_padded = np.pad(f, pad_width=1, mode='reflect')\n        fy = (f_padded[2:, 1:-1] - f_padded[:-2, 1:-1]) / 2.0\n        fx = (f_padded[1:-1, 2:] - f_padded[1:-1, :-2]) / 2.0\n        return fy, fx\n\n    def divergence_cd_reflect(fy, fx):\n        \"\"\" Computes divergence of a vector field (fy, fx) using central differences. \"\"\"\n        fy_padded = np.pad(fy, pad_width=1, mode='reflect')\n        fx_padded = np.pad(fx, pad_width=1, mode='reflect')\n        fyy = (fy_padded[2:, 1:-1] - fy_padded[:-2, 1:-1]) / 2.0\n        fxx = (fx_padded[1:-1, 2:] - fx_padded[1:-1, :-2]) / 2.0\n        return fxx + fyy\n\n    def curvature(phi):\n        \"\"\" Computes curvature kappa(phi) of the level set. \"\"\"\n        phi_y, phi_x = gradient_cd_reflect(phi)\n        grad_phi_norm = np.sqrt(phi_x**2 + phi_y**2 + BETA)\n        g_x = phi_x / grad_phi_norm\n        g_y = phi_y / grad_phi_norm\n        return divergence_cd_reflect(g_y, g_x)\n\n    # 3. Optimization and Parameter Search\n    def run_gradient_descent(phi_init, y, alpha):\n        \"\"\" Performs K iterations of gradient descent for a given alpha. \"\"\"\n        phi = phi_init.copy()\n        for _ in range(K):\n            u_k = smoothed_heaviside(phi)\n            delta_k = smoothed_delta(phi)\n            residual = forward_operator_A(u_k) - y\n            data_term_grad = -delta_k * forward_operator_A(residual)\n            kappa_k = curvature(phi)\n            reg_term_grad = alpha * delta_k * kappa_k\n            phi += DT * (data_term_grad + reg_term_grad)\n        return phi\n\n    def get_discrepancy(alpha, phi_init, y, memo):\n        \"\"\" Calculates the residual norm for a given alpha, with memoization. \"\"\"\n        if alpha in memo:\n            return memo[alpha]\n        phi_star = run_gradient_descent(phi_init, y, alpha)\n        u_star = smoothed_heaviside(phi_star)\n        discrepancy = np.linalg.norm(forward_operator_A(u_star) - y)\n        memo[alpha] = discrepancy\n        return discrepancy\n\n    def find_alpha_morozov(y, phi_0, sigma, tau):\n        \"\"\" Finds the optimal alpha using Morozov's principle with bisection. \"\"\"\n        target_discrepancy = tau * sigma * np.sqrt(M)\n        memo = {}\n        \n        alpha_min, alpha_max = ALPHA_MIN_INIT, ALPHA_MAX_INIT\n        d_min = get_discrepancy(alpha_min, phi_0, y, memo)\n        \n        if d_min  target_discrepancy:\n            return alpha_min\n\n        d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n\n        while d_max  target_discrepancy:\n            alpha_max *= 2.0\n            if alpha_max  ALPHA_MAX_CAP:\n                # Target cannot be bracketed, choose the one that's closer\n                d_cap = get_discrepancy(ALPHA_MAX_CAP, phi_0, y, memo)\n                if abs(d_cap - target_discrepancy)  abs(d_min - target_discrepancy):\n                    return ALPHA_MAX_CAP\n                else:\n                    return alpha_min\n            d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n            \n        alpha_l, alpha_u = alpha_min, alpha_max\n        for _ in range(50):\n            alpha_m = (alpha_l + alpha_u) / 2.0\n            if alpha_m == alpha_l or alpha_m == alpha_u:\n                break\n            d_m = get_discrepancy(alpha_m, phi_0, y, memo)\n            if abs(d_m - target_discrepancy)  RHO * target_discrepancy:\n                return alpha_m\n            if d_m  target_discrepancy:\n                alpha_l = alpha_m\n            else:\n                alpha_u = alpha_m\n        \n        d_l = get_discrepancy(alpha_l, phi_0, y, memo)\n        d_u = get_discrepancy(alpha_u, phi_0, y, memo)\n        return alpha_l if abs(d_l - target_discrepancy)  abs(d_u - target_discrepancy) else alpha_u\n\n    # 4. Main Execution Loop\n    phi_true = create_sdf_disk(N, CX_TRUE, CY_TRUE, R_TRUE)\n    u_true = smoothed_heaviside(phi_true)\n    y_clean = forward_operator_A(u_true)\n    phi_0 = create_sdf_disk(N, CX0, CY0, R0)\n    \n    test_cases = [\n        (0.05, 1.0), (0.01, 1.0), (0.10, 1.1), (0.08, 0.9)\n    ]\n    \n    results = []\n    for i, (sigma, tau) in enumerate(test_cases):\n        seed = S0 + i\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(0, sigma, (N, N))\n        y = y_clean + noise\n        \n        alpha_star = find_alpha_morozov(y, phi_0, sigma, tau)\n        results.append(\"{:.6f}\".format(alpha_star))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}