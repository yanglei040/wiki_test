{
    "hands_on_practices": [
        {
            "introduction": "The Bayesian Information Criterion (BIC) is more than just a formula; it is a powerful approximation to the Bayesian model evidence. This exercise grounds your understanding by guiding you through the derivation of BIC from the Laplace approximation of the marginal likelihood. By applying this to a concrete example, you will not only calculate BIC values but also connect their difference directly to the Bayes factor, providing a tangible sense of the relative evidence supporting one model over another .",
            "id": "3403899",
            "problem": "Consider an inverse modeling and data assimilation setting in which an analyst must select between two nested Gaussian linear observation models for assimilating a fixed batch of measurements. Let $\\mathcal{M}_1$ be a restricted model with $k_1$ free parameters and $\\mathcal{M}_2$ be an extended model with $k_2$ free parameters, where the nesting implies that the parameter space of $\\mathcal{M}_1$ is a linear subspace of that of $\\mathcal{M}_2$. The data consist of $n$ independent draws, and the observation errors are Gaussian with zero mean and unknown variance. The maximum likelihood fits for the two models yield maximized log-likelihood values $\\ell_1$ and $\\ell_2$.\n\nStarting from the Bayesian definition of model evidence (marginal likelihood) for a given model and the Laplace approximation under standard regularity conditions, derive a large-sample criterion that penalizes model complexity through a term involving the sample size $n$ and the number of free parameters $k$. Use this criterion to compute the values for $\\mathcal{M}_1$ and $\\mathcal{M}_2$ from the reported $\\ell_1$ and $\\ell_2$, and then connect the difference of these criterion values to the logarithm of the Bayes factor in favor of $\\mathcal{M}_2$ against $\\mathcal{M}_1$.\n\nYou are provided with the following numerical outputs from the maximum likelihood fits:\n- Sample size: $n = 150$.\n- Parameter counts: $k_1 = 4$ for $\\mathcal{M}_1$ and $k_2 = 9$ for $\\mathcal{M}_2$.\n- Maximized log-likelihoods: $\\ell_1 = -210.7$ for $\\mathcal{M}_1$ and $\\ell_2 = -195.2$ for $\\mathcal{M}_2$.\n\nAssume that all regularity conditions for the Laplace approximation hold, the models are correctly specified up to parameter values, the priors are positive and sufficiently smooth near the maximum likelihood estimates, and use natural logarithms. Compute the Bayesian Information Criterion (BIC) for both models and then compute the Laplace-approximated Bayes factor in favor of $\\mathcal{M}_2$ against $\\mathcal{M}_1$ by relating the BIC difference to the marginal likelihood ratio. Provide the Bayes factor value as your final answer, rounded to four significant figures. No units are required for the final answer.",
            "solution": "The problem requires the derivation of a large-sample model selection criterion from the Bayesian definition of model evidence, its application to a specific case, and the calculation of a Bayes factor. The problem is scientifically grounded, well-posed, and contains all necessary information for a unique solution. Therefore, it is deemed valid.\n\nLet $\\mathcal{M}$ be a model with a $k$-dimensional parameter vector $\\boldsymbol{\\theta}$. The evidence for model $\\mathcal{M}$, given the data $\\mathbf{y}$, is the marginal likelihood, defined as the integral of the likelihood over the prior distribution of the parameters:\n$$ p(\\mathbf{y} | \\mathcal{M}) = \\int p(\\mathbf{y} | \\boldsymbol{\\theta}, \\mathcal{M}) p(\\boldsymbol{\\theta} | \\mathcal{M}) d\\boldsymbol{\\theta} $$\nHere, $p(\\mathbf{y} | \\boldsymbol{\\theta}, \\mathcal{M})$ is the likelihood function $L(\\boldsymbol{\\theta})$, and $p(\\boldsymbol{\\theta} | \\mathcal{M})$ is the prior probability distribution of the parameters. The integral can be rewritten as:\n$$ p(\\mathbf{y} | \\mathcal{M}) = \\int \\exp\\left( \\ln\\left[ L(\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} | \\mathcal{M}) \\right] \\right) d\\boldsymbol{\\theta} = \\int \\exp\\left( \\ell(\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta} | \\mathcal{M}) \\right) d\\boldsymbol{\\theta} $$\nwhere $\\ell(\\boldsymbol{\\theta}) = \\ln L(\\boldsymbol{\\theta})$ is the log-likelihood.\n\nWe can approximate this integral using the Laplace method. The method is based on a second-order Taylor expansion of the exponent around its maximum. Let $f(\\boldsymbol{\\theta}) = \\ell(\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta} | \\mathcal{M})$. The maximum of $f(\\boldsymbol{\\theta})$ occurs at the maximum a posteriori (MAP) estimate, $\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}$. For a large sample size $n$, the likelihood term $\\ell(\\boldsymbol{\\theta})$ dominates the prior term $\\ln p(\\boldsymbol{\\theta} | \\mathcal{M})$, assuming a sufficiently diffuse prior. Consequently, $\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}$ is well approximated by the maximum likelihood estimate (MLE), $\\hat{\\boldsymbol{\\theta}}$, which maximizes $\\ell(\\boldsymbol{\\theta})$.\n\nThe Taylor expansion of $f(\\boldsymbol{\\theta})$ around $\\hat{\\boldsymbol{\\theta}}$ is:\n$$ f(\\boldsymbol{\\theta}) \\approx f(\\hat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\nabla f(\\hat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathbf{H}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) $$\nwhere $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ is the Hessian matrix of $f$ evaluated at $\\hat{\\boldsymbol{\\theta}}$. By definition of the MLE, $\\nabla \\ell(\\hat{\\boldsymbol{\\theta}}) = \\boldsymbol{0}$. Thus, $\\nabla f(\\hat{\\boldsymbol{\\theta}}) = \\nabla \\ell(\\hat{\\boldsymbol{\\theta}}) + \\nabla \\ln p(\\hat{\\boldsymbol{\\theta}}) = \\nabla \\ln p(\\hat{\\boldsymbol{\\theta}})$. If the prior is flat near $\\hat{\\boldsymbol{\\theta}}$, this gradient term is negligible. The Hessian $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ is also dominated by the Hessian of the log-likelihood, and we can approximate $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}}) \\approx \\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})$. The negative of this matrix is the observed Fisher information matrix, $\\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) = -\\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})$.\n\nThe integral for the evidence becomes:\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\int \\exp\\left( f(\\hat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right) d\\boldsymbol{\\theta} $$\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\exp(f(\\hat{\\boldsymbol{\\theta}})) \\int \\exp\\left( - \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right) d\\boldsymbol{\\theta} $$\nThe integral is the unnormalized form of a $k$-variate Gaussian distribution, which evaluates to $(2\\pi)^{k/2} |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|^{-1/2}$. Substituting this and $f(\\hat{\\boldsymbol{\\theta}}) \\approx \\ell(\\hat{\\boldsymbol{\\theta}}) = \\ell_{\\text{max}}$ (ignoring the prior term $\\ln p(\\hat{\\boldsymbol{\\theta}})$ as a lower-order term), we get:\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\exp(\\ell_{\\text{max}}) (2\\pi)^{k/2} |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|^{-1/2} $$\nTaking the natural logarithm yields the log-evidence:\n$$ \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx \\ell_{\\text{max}} + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})| $$\nFor large $n$, the Fisher information matrix is asymptotically proportional to the sample size, i.e., $|\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|$ is of order $O(n^k)$. Thus, $\\ln|\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})| \\approx k \\ln n + C$, where $C$ is a constant independent of $n$. Dropping all terms that do not grow with $n$, we obtain the large-sample approximation:\n$$ \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx \\ell_{\\text{max}} - \\frac{k}{2} \\ln n $$\nThe Bayesian Information Criterion (BIC) is conventionally defined by multiplying this quantity by $-2$ to create a loss function to be minimized:\n$$ \\text{BIC} = -2 \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx -2 \\ell_{\\text{max}} + k \\ln n $$\nThis is the desired large-sample criterion. A lower BIC value indicates stronger evidence for the model.\n\nWe now apply this to the two models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, using the provided data:\nSample size: $n = 150$.\nFor $\\mathcal{M}_1$: number of parameters $k_1 = 4$, maximized log-likelihood $\\ell_1 = -210.7$.\nFor $\\mathcal{M}_2$: number of parameters $k_2 = 9$, maximized log-likelihood $\\ell_2 = -195.2$.\n\nFirst, we compute the value of $\\ln(150)$:\n$$ \\ln(150) \\approx 5.010635 $$\nNow we compute the BIC for each model:\n$$ \\text{BIC}_1 = -2 \\ell_1 + k_1 \\ln n = -2(-210.7) + 4 \\ln(150) = 421.4 + 4(5.010635) = 421.4 + 20.04254 = 441.44254 $$\n$$ \\text{BIC}_2 = -2 \\ell_2 + k_2 \\ln n = -2(-195.2) + 9 \\ln(150) = 390.4 + 9(5.010635) = 390.4 + 45.095715 = 435.495715 $$\nSince $\\text{BIC}_2 < \\text{BIC}_1$, the BIC favors the more complex model $\\mathcal{M}_2$.\n\nThe final step is to compute the Bayes factor, $B_{21}$, in favor of $\\mathcal{M}_2$ against $\\mathcal{M}_1$. The Bayes factor is the ratio of the marginal likelihoods:\n$$ B_{21} = \\frac{p(\\mathbf{y} | \\mathcal{M}_2)}{p(\\mathbf{y} | \\mathcal{M}_1)} $$\nThe logarithm of the Bayes factor is related to the difference in log-evidence:\n$$ \\ln B_{21} = \\ln p(\\mathbf{y} | \\mathcal{M}_2) - \\ln p(\\mathbf{y} | \\mathcal{M}_1) $$\nUsing our large-sample approximation for the log-evidence:\n$$ \\ln B_{21} \\approx \\left(\\ell_2 - \\frac{k_2}{2} \\ln n \\right) - \\left(\\ell_1 - \\frac{k_1}{2} \\ln n \\right) = (\\ell_2 - \\ell_1) - \\frac{k_2 - k_1}{2} \\ln n $$\nThis can also be expressed in terms of the BIC values:\n$$ \\ln B_{21} \\approx \\frac{1}{2} \\left[ (-2\\ell_1 + k_1 \\ln n) - (-2\\ell_2 + k_2 \\ln n) \\right] = \\frac{\\text{BIC}_1 - \\text{BIC}_2}{2} $$\nUsing the BIC values we computed:\n$$ \\ln B_{21} \\approx \\frac{441.44254 - 435.495715}{2} = \\frac{5.946825}{2} = 2.9734125 $$\nTo calculate the Bayes factor $B_{21}$, we exponentiate this result:\n$$ B_{21} = \\exp(\\ln B_{21}) \\approx \\exp(2.9734125) \\approx 19.5583 $$\nAlternatively, and for better precision, we use the direct formula:\n$$ \\ln B_{21} \\approx (\\ell_2 - \\ell_1) - \\frac{k_2 - k_1}{2} \\ln n $$\nSubstituting the given values:\n$$ \\ell_2 - \\ell_1 = -195.2 - (-210.7) = 15.5 $$\n$$ k_2 - k_1 = 9 - 4 = 5 $$\n$$ \\ln B_{21} \\approx 15.5 - \\frac{5}{2} \\ln(150) = 15.5 - 2.5(5.01063529) = 15.5 - 12.52658823 = 2.97341177 $$\n$$ B_{21} = \\exp(2.97341177) \\approx 19.55829 $$\nRounding the result to four significant figures gives $19.56$. This value indicates very strong evidence in favor of model $\\mathcal{M}_2$ compared to model $\\mathcal{M}_1$, according to standard interpretations of the Bayes factor (e.g., Jeffreys' scale).",
            "answer": "$$\\boxed{19.56}$$"
        },
        {
            "introduction": "While both AIC and BIC penalize model complexity, the effectiveness of the penalty hinges on correctly identifying the number of estimated parameters, $k$. This practice problem sharpens that crucial skill by presenting a scenario where the error variance is first considered known, then unknown and estimated from the data. You will see firsthand how this change affects the parameter count and can even reverse the model preference, highlighting the importance of careful thought over rote application of formulas .",
            "id": "3403909",
            "problem": "Consider a data assimilation setting in which a forecast model is calibrated to a fixed set of $n$ observations by minimizing the sum of squared innovations (residuals). Two candidate models, $\\mathcal{M}_{1}$ and $\\mathcal{M}_{2}$, are fit to the same $n$ innovations under the assumption of independent Gaussian measurement errors. Let the residual sequences for the $n=10$ assimilation cycles be\n$$\n\\boldsymbol{r}_{1} = \\big(1,\\,-2,\\,2,\\,1,\\,3,\\,-2,\\,1,\\,-1,\\,2,\\,-1\\big), \\quad \\boldsymbol{r}_{2} = \\big(2,\\,-1,\\,1,\\,2,\\,-2,\\,1,\\,0,\\,-1,\\,2,\\,-1\\big).\n$$\nModel $\\mathcal{M}_{1}$ estimates a parameter vector of dimension $p_{1}=3$, and model $\\mathcal{M}_{2}$ estimates a parameter vector of dimension $p_{2}=5$. Assume the residuals arise from a Gaussian error model with variance $\\sigma^{2}$ that is either known or unknown, as specified below.\n\nStarting only from the Gaussian likelihood for independent errors and the definition of the Akaike Information Criterion (AIC), perform the following:\n\n1. Compute the residual sums of squares $RSS_{1} = \\sum_{i=1}^{n} r_{1,i}^{2}$ and $RSS_{2} = \\sum_{i=1}^{n} r_{2,i}^{2}$.\n\n2. Under the scenario where the error variance is known and equal to $\\sigma^{2} = 2$, derive the AIC for each model and report the difference $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})$.\n\n3. Under the scenario where the error variance $\\sigma^{2}$ is unknown and estimated by maximum likelihood, derive the AIC for each model and report the difference $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})$.\n\nBriefly discuss, based on your calculations and without using any pre-stated formulas, how the change from known $\\sigma^{2}$ to unknown $\\sigma^{2}$ affects the model preference, and summarize the intuition in relation to the Bayesian Information Criterion (BIC).\n\nYour final reported answer must be the pair of differences $\\big(AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})\\big)$ under known and unknown variance, expressed exactly as a row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides all necessary information to derive the Akaike Information Criterion (AIC) from first principles under two distinct, clearly defined scenarios.\n\nThe fundamental definition of AIC is $AIC = -2 \\ln(\\mathcal{L}_{max}) + 2k$, where $\\mathcal{L}_{max}$ is the value of the likelihood function maximized over the model parameters, and $k$ is the total number of estimated parameters in the model.\n\nThe likelihood function for $n$ independent and identically distributed Gaussian residuals $r_i$ with mean $0$ and variance $\\sigma^2$ is given by:\n$$\n\\mathcal{L}(\\theta, \\sigma^2 | \\boldsymbol{r}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} r_i^2\\right)\n$$\nThe corresponding log-likelihood is:\n$$\n\\ln(\\mathcal{L}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS}{2\\sigma^2}\n$$\nwhere $RSS = \\sum_{i=1}^{n} r_i^2$ is the residual sum of squares.\n\n**1. Compute the Residual Sums of Squares (RSS)**\n\nFor model $\\mathcal{M}_{1}$, with $n=10$ and residual vector $\\boldsymbol{r}_{1} = \\big(1,\\,-2,\\,2,\\,1,\\,3,\\,-2,\\,1,\\,-1,\\,2,\\,-1\\big)$:\n$$\nRSS_{1} = \\sum_{i=1}^{10} r_{1,i}^{2} = 1^2 + (-2)^2 + 2^2 + 1^2 + 3^2 + (-2)^2 + 1^2 + (-1)^2 + 2^2 + (-1)^2\n$$\n$$\nRSS_{1} = 1 + 4 + 4 + 1 + 9 + 4 + 1 + 1 + 4 + 1 = 30\n$$\nFor model $\\mathcal{M}_{2}$, with $n=10$ and residual vector $\\boldsymbol{r}_{2} = \\big(2,\\,-1,\\,1,\\,2,\\,-2,\\,1,\\,0,\\,-1,\\,2,\\,-1\\big)$:\n$$\nRSS_{2} = \\sum_{i=1}^{10} r_{2,i}^{2} = 2^2 + (-1)^2 + 1^2 + 2^2 + (-2)^2 + 1^2 + 0^2 + (-1)^2 + 2^2 + (-1)^2\n$$\n$$\nRSS_{2} = 4 + 1 + 1 + 4 + 4 + 1 + 0 + 1 + 4 + 1 = 21\n$$\n\n**2. AIC with Known Error Variance**\n\nIn this scenario, the error variance is known and fixed at $\\sigma^2 = 2$. It is not an estimated parameter. The number of estimated parameters for each model is solely the dimension of its internal parameter vector.\n- For model $\\mathcal{M}_{1}$, the number of estimated parameters is $k_1 = p_1 = 3$.\n- For model $\\mathcal{M}_{2}$, the number of estimated parameters is $k_2 = p_2 = 5$.\n\nThe log-likelihood for a model $j$ is evaluated with the given $RSS_j$ and the known $\\sigma^2=2$. This value represents the maximized likelihood because the residuals are the result of a precedent optimization process.\n$$\n\\ln(\\mathcal{L}_{max, j}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS_j}{2\\sigma^2}\n$$\nThe AIC for model $j$ is:\n$$\nAIC(\\mathcal{M}_{j}) = -2 \\left( -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS_j}{2\\sigma^2} \\right) + 2k_j = n\\ln(2\\pi\\sigma^2) + \\frac{RSS_j}{\\sigma^2} + 2k_j\n$$\nFor $\\mathcal{M}_{1}$:\n$$\nAIC(\\mathcal{M}_{1}) = 10\\ln(2\\pi \\cdot 2) + \\frac{30}{2} + 2(3) = 10\\ln(4\\pi) + 15 + 6 = 10\\ln(4\\pi) + 21\n$$\nFor $\\mathcal{M}_{2}$:\n$$\nAIC(\\mathcal{M}_{2}) = 10\\ln(2\\pi \\cdot 2) + \\frac{21}{2} + 2(5) = 10\\ln(4\\pi) + \\frac{21}{2} + 10 = 10\\ln(4\\pi) + \\frac{41}{2}\n$$\nThe difference is:\n$$\nAIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = \\left(10\\ln(4\\pi) + \\frac{41}{2}\\right) - (10\\ln(4\\pi) + 21) = \\frac{41}{2} - 21 = \\frac{41 - 42}{2} = -\\frac{1}{2}\n$$\n\n**3. AIC with Unknown Error Variance**\n\nIn this scenario, the error variance $\\sigma^2$ is unknown and must be estimated from the data for each model. This adds one estimated parameter to each model's count.\n- For model $\\mathcal{M}_{1}$, the number of estimated parameters is $k_1 = p_1 + 1 = 3 + 1 = 4$.\n- For model $\\mathcal{M}_{2}$, the number of estimated parameters is $k_2 = p_2 + 1 = 5 + 1 = 6$.\n\nTo find the maximized log-likelihood, we first find the maximum likelihood estimate (MLE) of $\\sigma^2$ for a given $RSS$. We differentiate the log-likelihood with respect to $\\sigma^2$ and set the result to zero:\n$$\n\\frac{\\partial}{\\partial (\\sigma^2)} \\ln(\\mathcal{L}) = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2}\\ln(2\\pi) -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{RSS}{2\\sigma^2} \\right) = -\\frac{n}{2\\sigma^2} + \\frac{RSS}{2(\\sigma^2)^2}\n$$\nSetting this to zero yields the MLE for $\\sigma^2$:\n$$\n\\frac{n}{2\\hat{\\sigma}^2} = \\frac{RSS}{2(\\hat{\\sigma}^2)^2} \\implies \\hat{\\sigma}^2 = \\frac{RSS}{n}\n$$\nNow we substitute this $\\hat{\\sigma}^2$ back into the log-likelihood expression to get the maximized value, $\\ln(\\mathcal{L}_{max})$:\n$$\n\\ln(\\mathcal{L}_{max}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{RSS}{n}\\right) - \\frac{RSS}{2(RSS/n)} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{RSS}{n}\\right) + 1 \\right)\n$$\nThe AIC for model $j$ is then:\n$$\nAIC(\\mathcal{M}_{j}) = -2\\ln(\\mathcal{L}_{max, j}) + 2k_j = n\\left(\\ln(2\\pi) + \\ln\\left(\\frac{RSS_j}{n}\\right) + 1\\right) + 2k_j\n$$\nFor $\\mathcal{M}_{1}$: $RSS_1 = 30$, $k_1 = 4$.\n$$\nAIC(\\mathcal{M}_{1}) = 10\\left(\\ln(2\\pi) + \\ln\\left(\\frac{30}{10}\\right) + 1\\right) + 2(4) = 10(\\ln(2\\pi) + \\ln(3) + 1) + 8 = 10\\ln(6\\pi) + 10 + 8 = 10\\ln(6\\pi) + 18\n$$\nFor $\\mathcal{M}_{2}$: $RSS_2 = 21$, $k_2 = 6$.\n$$\nAIC(\\mathcal{M}_{2}) = 10\\left(\\ln(2\\pi) + \\ln\\left(\\frac{21}{10}\\right) + 1\\right) + 2(6) = 10(\\ln(2\\pi) + \\ln(2.1) + 1) + 12 = 10\\ln(4.2\\pi) + 10 + 12 = 10\\ln\\left(\\frac{21\\pi}{5}\\right) + 22\n$$\nThe difference is:\n$$\nAIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = \\left(10\\ln\\left(\\frac{21\\pi}{5}\\right) + 22\\right) - (10\\ln(6\\pi) + 18)\n$$\n$$\n= 10\\left(\\ln\\left(\\frac{21\\pi}{5}\\right) - \\ln(6\\pi)\\right) + 4 = 10\\ln\\left(\\frac{21\\pi/5}{6\\pi}\\right) + 4 = 10\\ln\\left(\\frac{21}{30}\\right) + 4\n$$\n$$\n= 10\\ln\\left(\\frac{7}{10}\\right) + 4\n$$\n\n**Discussion**\n\nWhen the error variance $\\sigma^2$ is known, $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = -1/2$. A lower AIC value indicates a better model, so $\\mathcal{M}_{2}$ is preferred.\n\nWhen $\\sigma^2$ is unknown and estimated, $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = 4 + 10\\ln(7/10)$. Since $\\ln(7/10) < 0$, we can evaluate this. $\\ln(0.7) \\approx -0.3567$, so the difference is approximately $4 - 3.567 = 0.433 > 0$. In this case, $\\mathcal{M}_{1}$ is preferred.\n\nThe model preference switches from the more complex model ($\\mathcal{M}_{2}$) to the simpler model ($\\mathcal{M}_{1}$) when the variance becomes an estimated parameter. The reason lies in the goodness-of-fit term of the AIC.\n- For known $\\sigma^2$, the log-likelihood term in the $\\Delta AIC$ scales as $(RSS_2 - RSS_1)/\\sigma^2 = (21-30)/2 = -4.5$. This is a linear measure of the improvement in fit.\n- For unknown $\\sigma^2$, the log-likelihood term scales as $n\\ln(RSS_2/RSS_1) = 10\\ln(21/30) = 10\\ln(0.7) \\approx -3.57$. The logarithmic scale dampens the reward for reducing the RSS.\n\nWhen each model is allowed to estimate its own optimal variance ($\\hat{\\sigma}^2 = RSS/n$), the advantage of a lower $RSS$ is partially absorbed into the claim of a lower intrinsic error level. This reduces the effective \"credit\" the model gets for its better fit compared to the case where both models are judged against a common, fixed variance. Consequently, the penalty for higher complexity ($2k$) becomes more influential, leading to the selection of the simpler model $\\mathcal{M}_{1}$.\n\nThis effect has a conceptual parallel to the Bayesian Information Criterion (BIC), defined as $BIC = -2 \\ln(\\mathcal{L}_{max}) + k \\ln(n)$. For $n=10$, $\\ln(10) \\approx 2.3$, so the BIC complexity penalty, $k\\ln(10)$, is stronger than the AIC penalty, $2k$. BIC inherently favors simpler models more strongly than AIC. The shift in preference towards the simpler model $\\mathcal{M}_{1}$ when moving to the unknown-variance case for AIC is thus a move in the direction that BIC would likely favor from the start.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2} & 4 + 10\\ln\\left(\\frac{7}{10}\\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Model selection criteria are powerful tools, but they are not infallible and their validity rests on underlying statistical assumptions. This exercise presents a realistic but challenging scenario where AIC and residual diagnostics provide conflicting advice, with a more complex model yielding a lower AIC but clearly violating the error assumptions. By analyzing this case, you will learn the critical importance of using information criteria as part of a holistic diagnostic process, ensuring that a seemingly \"better\" model is not simply overfitting structured noise .",
            "id": "3403888",
            "problem": "In an inverse problem within a data assimilation workflow, consider a sequence of models $\\mathcal{M}_1$ and $\\mathcal{M}_2$ for a forward map $y = \\mathcal{G}(\\theta) + \\varepsilon$, where $y \\in \\mathbb{R}^n$ are observations, $\\theta \\in \\mathbb{R}^p$ are unknown parameters to be inferred, and $\\varepsilon$ denotes observation errors. The data assimilation step uses a likelihood for the observation errors and a regularization from background priors on $\\theta$; here, focus only on the observation likelihood component for model comparison. Both $\\mathcal{M}_1$ and $\\mathcal{M}_2$ assume independent and identically distributed Gaussian errors with constant variance, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$, and differ by the complexity of the parametric form of $\\mathcal{G}(\\theta)$, with $\\mathcal{M}_2$ having additional basis components, thereby increasing the number of free parameters.\n\nUsing maximum likelihood estimates for $(\\theta, \\sigma^2)$ under these assumptions, the Akaike Information Criterion (AIC) computed from the fitted likelihood decreases by $25$ units when moving from $\\mathcal{M}_1$ to $\\mathcal{M}_2$, while the Bayesian Information Criterion (BIC) increases by $5$ units. However, standardized residual diagnostics for $\\mathcal{M}_2$ exhibit the following changes compared to $\\mathcal{M}_1$: the sample lag-$1$ autocorrelation increases from $0.20$ to $0.60$, the kurtosis increases from approximately $3.0$ to $5.5$, and portmanteau whiteness tests reject the independent and identically distributed Gaussian assumption at the $1\\%$ level for $\\mathcal{M}_2$ but not for $\\mathcal{M}_1$. The data assimilation innovations (observation minus model prediction) under $\\mathcal{M}_2$ also show statistically significant serial correlation relative to $\\mathcal{M}_1$.\n\nWhich option best explains how a decreasing $ \\mathrm{AIC} $ accompanied by worsening residual diagnostics may indicate model misspecification rather than true improvement in predictive fit in this inverse problems and data assimilation setting?\n\nA. A decreasing $ \\mathrm{AIC} $ is definitive evidence of improved expected predictive performance; residual diagnostics are asymptotically irrelevant, so $\\mathcal{M}_2$ should be preferred regardless of autocorrelation and kurtosis increases.\n\nB. A decreasing $ \\mathrm{AIC} $ computed under the assumption $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ can be driven by overfitting the mean structure $\\mathcal{G}(\\theta)$ to serially correlated and heavy-tailed errors, inflating the (mis-specified) likelihood. The worsening residual diagnostics indicate violation of the independent and identically distributed Gaussian assumption, so the $ \\mathrm{AIC} $ comparison is unreliable; one should model the error covariance (e.g., $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$ with nontrivial $\\Sigma$) or use a criterion computed from the correctly specified likelihood. The increase in $ \\mathrm{BIC} $ is consistent with stronger penalization of gratuitous complexity under misspecification.\n\nC. Worsening residual diagnostics unambiguously prove that the true data-generating process is non-Gaussian, so any Gaussian-based criterion will necessarily fail; the observed decrease in $ \\mathrm{AIC} $ is impossible unless there is a coding error.\n\nD. $ \\mathrm{AIC} $ is invariant to residual autocorrelation and tail behavior in Gaussian models, so if residual autocorrelation increases, $ \\mathrm{AIC} $ will necessarily decrease, which indicates a better fit; the increase in $ \\mathrm{BIC} $ is inconsequential because it depends on arbitrary priors, so $\\mathcal{M}_2$ should be selected.",
            "solution": "The user has provided a problem concerning model selection in the context of inverse problems and data assimilation. The task is to validate the problem statement and, if valid, provide a detailed solution, including an evaluation of all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Problem Context:** Inverse problem, data assimilation workflow.\n*   **Forward Model:** $y = \\mathcal{G}(\\theta) + \\varepsilon$, where $y \\in \\mathbb{R}^n$ (observations), $\\theta \\in \\mathbb{R}^p$ (parameters), and $\\varepsilon$ (observation errors).\n*   **Models for Comparison:** $\\mathcal{M}_1$ and $\\mathcal{M}_2$.\n*   **Error Assumption (for both models):** Independent and identically distributed (i.i.d.) Gaussian errors, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n*   **Model Complexity:** $\\mathcal{M}_2$ is more complex than $\\mathcal{M}_1$ (has more free parameters).\n*   **Information Criteria Changes ($\\mathcal{M}_1 \\to \\mathcal{M}_2$):**\n    *   Akaike Information Criterion (AIC) decreases by $25$ units: $\\mathrm{AIC}(\\mathcal{M}_2) - \\mathrm{AIC}(\\mathcal{M}_1) = -25$.\n    *   Bayesian Information Criterion (BIC) increases by $5$ units: $\\mathrm{BIC}(\\mathcal{M}_2) - \\mathrm{BIC}(\\mathcal{M}_1) = 5$.\n*   **Estimation Method:** Maximum likelihood estimates for $(\\theta, \\sigma^2)$.\n*   **Residual Diagnostics Changes ($\\mathcal{M}_1 \\to \\mathcal{M}_2$):**\n    *   Sample lag-$1$ autocorrelation increases from $0.20$ to $0.60$.\n    *   Kurtosis increases from approx. $3.0$ to $5.5$.\n    *   Portmanteau whiteness tests reject the i.i.d. Gaussian assumption at the $1\\%$ level for $\\mathcal{M}_2$, but not for $\\mathcal{M}_1$.\n*   **Innovations:** Data assimilation innovations for $\\mathcal{M}_2$ show statistically significant serial correlation relative to $\\mathcal{M}_1$.\n*   **Question:** Explain how a decreasing AIC with worsening residual diagnostics can indicate model misspecification instead of true improvement.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem is firmly rooted in the established statistical theory of model selection and diagnostics. AIC, BIC, residual analysis, and assumptions of i.i.d. Gaussian errors are fundamental concepts in statistics, econometrics, and data assimilation. The scenario described is a classic illustration of the pitfalls of relying solely on information criteria without checking model assumptions.\n*   **Well-Posed:** The problem provides a-clear, self-contained scenario and asks for a conceptual explanation. The provided quantitative information (changes in AIC, BIC, autocorrelation, kurtosis) is consistent and sufficient to arrive at a logical conclusion.\n*   **Objective:** The problem is stated using objective, quantitative measures and established terminology. It does not contain subjective or opinion-based statements.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness:** None. The scenario is plausible and frequently encountered in practice when a more flexible model overfits the data, capturing noise structure as if it were signal.\n2.  **Non-Formalizable or Irrelevant:** None. The problem is directly relevant to model selection in scientific modeling.\n3.  **Incomplete or Contradictory Setup:** None. The apparent conflict between the AIC result and the residual diagnostics is the central feature of the problem to be explained, not a contradiction in the problem statement itself.\n4.  **Unrealistic or Infeasible:** None. The numerical values are realistic for a modeling problem.\n5.  **Ill-Posed or Poorly Structured:** None. The question asks for the best explanation among a set of choices, which is a well-defined task.\n6.  **Pseudo-Profound, Trivial, or Tautological:** None. The problem addresses a subtle but critical issue in applied statistics, requiring an understanding of the assumptions underlying model selection criteria.\n7.  **Outside Scientific Verifiability:** None. The principles involved are standard and verifiable within statistical science.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. We may proceed with the solution.\n\n### Derivation and Option Analysis\n\n**Theoretical Framework**\n\nThe Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are defined as:\n$$ \\mathrm{AIC} = -2\\ln(\\hat{L}) + 2k $$\n$$ \\mathrm{BIC} = -2\\ln(\\hat{L}) + k\\ln(n) $$\nwhere $\\hat{L}$ is the maximized value of the likelihood function for the model, $k$ is the number of estimated parameters in the model, and $n$ is the number of observations. Both criteria trade off model fit (represented by the maximized log-likelihood $\\ln(\\hat{L})$) against model complexity (represented by the penalty term). A lower value for AIC or BIC is preferred.\n\nThe likelihood for the model $y = \\mathcal{G}(\\theta) + \\varepsilon$ under the assumption of i.i.d. Gaussian errors $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ is:\n$$ L(\\theta, \\sigma^2 | y) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - [\\mathcal{G}(\\theta)]_i)^2\\right) $$\nFor a given $\\hat{\\theta}$, the maximum likelihood estimate of the variance is $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - [\\mathcal{G}(\\hat{\\theta})]_i)^2 = \\frac{\\mathrm{RSS}}{n}$, where $\\mathrm{RSS}$ is the residual sum of squares. Substituting this into the log-likelihood function yields (up to an additive constant):\n$$ -2\\ln(\\hat{L}) \\propto n\\ln(\\mathrm{RSS}) $$\nTherefore, a model that achieves a lower RSS will have a higher maximized log-likelihood and a lower value for the $-2\\ln(\\hat{L})$ term in AIC and BIC.\n\n**Analysis of the Scenario**\n\n1.  **Model Fit vs. Complexity:** We are given that $\\mathcal{M}_2$ is more complex than $\\mathcal{M}_1$, so its parameter count $k_2$ is greater than $k_1$. As a more flexible model, $\\mathcal{M}_2$ can achieve a fit that is at least as good as $\\mathcal{M}_1$ in terms of RSS. Typically, it will achieve a strictly lower RSS, leading to a higher maximized log-likelihood $\\hat{L}_2 > \\hat{L}_1$.\n\n2.  **Interpreting AIC and BIC Changes:**\n    *   $\\mathrm{AIC}(\\mathcal{M}_2) < \\mathrm{AIC}(\\mathcal{M}_1)$: The decrease of $25$ units indicates that the improvement in fit (the increase in $\\ln(\\hat{L})$) for $\\mathcal{M}_2$ is substantial enough to more than compensate for the penalty of having more parameters ($2(k_2-k_1)$). Naively, this suggests $\\mathcal{M}_2$ is a better model.\n    *   $\\mathrm{BIC}(\\mathcal{M}_2) > \\mathrm{BIC}(\\mathcal{M}_1)$: The BIC penalty for complexity, $(k_2-k_1)\\ln(n)$, is stricter than the AIC penalty for any reasonable sample size ($n \\ge 8$). The increase in BIC by $5$ units implies that, from BIC's perspective, the improvement in fit does *not* justify the added complexity. This discrepancy between AIC and BIC is a classic signal of potential overfitting.\n\n3.  **The Role of Residual Diagnostics:** The crucial part of the problem lies here. The theoretical justification for AIC and BIC as estimators of predictive performance relies on the assumption that the specified likelihood function is correct (or at least a good approximation). The problem states this likelihood is based on the assumption $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. The residual diagnostics are designed to test precisely these assumptions.\n    *   The residuals of $\\mathcal{M}_2$ exhibit high autocorrelation ($0.60$) and fail whiteness tests. This directly violates the **independence** assumption in \"$i.i.d$.\".\n    *   The residuals of $\\mathcal{M}_2$ have a kurtosis of $5.5$, which is much higher than the kurtosis of a Gaussian distribution ($3.0$). This violates the **Gaussian** assumption.\n    *   The residuals of $\\mathcal{M}_1$, in contrast, are much more consistent with the i.i.d. Gaussian assumption.\n\n4.  **Synthesis:** The more flexible model $\\mathcal{M}_2$ has likely used its extra parameters to \"fit the noise.\" The true observation errors $\\varepsilon$ are not i.i.d. Gaussian; they appear to be serially correlated and have heavier tails. By having more parameters, $\\mathcal{M}_2$ can contort its mean function $\\mathcal{G}(\\theta)$ to partially absorb this structured noise. This leads to a smaller RSS, which in turn inflates the value of the *misspecified* likelihood function. This inflated likelihood is what drives AIC down.\n\nHowever, because the likelihood function is wrong (the true errors are not $\\sim \\mathcal{N}(0, \\sigma^2 I)$), the resulting AIC value is unreliable. Its theoretical properties as an unbiased estimator of out-of-sample prediction error are lost. The worsening residual diagnostics are the primary evidence of this model misspecification. They reveal that the \"better fit\" of $\\mathcal{M}_2$ is spurious. The increase in BIC, with its stronger penalty, correctly penalizes this \"gratuitous complexity.\" The correct conclusion is not that $\\mathcal{M}_2$ is better, but that the modeling framework itself (specifically the i.i.d. Gaussian error assumption) is flawed, and that $\\mathcal{M}_2$ is simply a more severely misspecified model that overfits the data.\n\n**Evaluation of Options**\n\n*   **A. A decreasing $ \\mathrm{AIC} $ is definitive evidence of improved expected predictive performance; residual diagnostics are asymptotically irrelevant, so $\\mathcal{M}_2$ should be preferred regardless of autocorrelation and kurtosis increases.**\n    This statement is fundamentally incorrect. AIC is an *estimate* of predictive performance, and its validity hinges on the model assumptions. Residual diagnostics are a critical tool for verifying these assumptions. Ignoring strong evidence of misspecification from residuals is a major error in statistical practice. The claim that they are \"asymptotically irrelevant\" is false.\n    **Verdict: Incorrect.**\n\n*   **B. A decreasing $ \\mathrm{AIC} $ computed under the assumption $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ can be driven by overfitting the mean structure $\\mathcal{G}(\\theta)$ to serially correlated and heavy-tailed errors, inflating the (mis-specified) likelihood. The worsening residual diagnostics indicate violation of the independent and identically distributed Gaussian assumption, so the $ \\mathrm{AIC} $ comparison is unreliable; one should model the error covariance (e.g., $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$ with nontrivial $\\Sigma$) or use a criterion computed from the correctly specified likelihood. The increase in $ \\mathrm{BIC} $ is consistent with stronger penalization of gratuitous complexity under misspecification.**\n    This option provides a complete and accurate explanation of the phenomenon. It correctly identifies overfitting to structured noise as the cause, notes that this inflates a misspecified likelihood, correctly concludes that the AIC comparison is therefore unreliable, suggests the appropriate remedy (improving the model for the error structure), and correctly interprets the BIC increase as a sign of penalizing excessive complexity.\n    **Verdict: Correct.**\n\n*   **C. Worsening residual diagnostics unambiguously prove that the true data-generating process is non-Gaussian, so any Gaussian-based criterion will necessarily fail; the observed decrease in $ \\mathrm{AIC} $ is impossible unless there is a coding error.**\n    This statement is flawed. Firstly, statistical tests do not \"unambiguously prove\" anything; they provide evidence against a null hypothesis. Secondly, and more critically, it is false that a decrease in AIC is \"impossible\" under these conditions. As explained above, overfitting to structured noise will mechanically reduce the RSS and thus increase the misspecified log-likelihood, which can easily cause the computed AIC to decrease. This is a known modeling pathology, not a computational error.\n    **Verdict: Incorrect.**\n\n*   **D. $ \\mathrm{AIC} $ is invariant to residual autocorrelation and tail behavior in Gaussian models, so if residual autocorrelation increases, $ \\mathrm{AIC} $ will necessarily decrease, which indicates a better fit; the increase in $ \\mathrm{BIC} $ is inconsequential because it depends on arbitrary priors, so $\\mathcal{M}_2$ should be selected.**\n    This option contains multiple errors. AIC is not \"invariant\" to residual properties; its numerical value is directly computed from the residuals via the RSS. The claim that increased autocorrelation *necessarily* leads to decreased AIC is baseless. Finally, the dismissal of BIC is incorrect. The BIC penalty does not depend on \"arbitrary priors\" in the way implied, and its disagreement with AIC is a highly consequential piece of information, not something to be ignored.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}