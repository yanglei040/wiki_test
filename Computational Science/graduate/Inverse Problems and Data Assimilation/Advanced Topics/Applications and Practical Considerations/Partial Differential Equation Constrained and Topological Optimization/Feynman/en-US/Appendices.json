{
    "hands_on_practices": [
        {
            "introduction": "Efficient gradient computation is the cornerstone of optimizing systems governed by partial differential equations. This first exercise guides you through the fundamental derivation of the adjoint method, a powerful technique for finding the gradient of an objective functional at a computational cost that is remarkably independent of the number of control parameters . Mastering this derivation from first principles is an essential theoretical skill for developing and understanding algorithms in PDE-constrained optimization.",
            "id": "3409520",
            "problem": "Consider a bounded, open, connected domain $\\Omega \\subset \\mathbb{R}^{d}$ with Lipschitz boundary $\\partial \\Omega$, where $d \\in \\{2,3\\}$. Let $y_{d} \\in L^{2}(\\Omega)$ denote a given data field interpreted as a target state in the context of inverse problems and data assimilation. For a control $u \\in L^{2}(\\Omega)$, define the state $y \\in H_{0}^{1}(\\Omega)$ as the weak solution of the elliptic partial differential equation (PDE)\n$$\n-\\Delta y = u \\quad \\text{in } \\Omega, \\qquad y = 0 \\quad \\text{on } \\partial \\Omega,\n$$\nwhere $\\Delta$ is the Laplace operator and $H_{0}^{1}(\\Omega)$ is the Sobolev space of functions with square-integrable gradients and vanishing trace on $\\partial \\Omega$.\n\nConsider the quadratic tracking-type objective functional with Tikhonov regularization parameter $\\alpha > 0$,\n$$\nJ(y,u) = \\frac{1}{2}\\|y - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u\\|_{L^{2}(\\Omega)}^{2}.\n$$\nDefine the reduced functional $j(u) := J(S(u),u)$, where $S : L^{2}(\\Omega) \\to H_{0}^{1}(\\Omega)$ maps $u$ to the weak solution $y$ of the PDE. Starting from first principles of variational calculus and the weak formulation of the governing PDE, derive the adjoint equation associated with the state equation and use it to obtain the gradient of the reduced functional $j(u)$ with respect to $u$. Your derivation must explicitly justify the introduction of the adjoint variable via the inner-product representation of the directional derivative and Green’s identities on $\\Omega$, and it must use only the fundamental definitions of the weak solution for the elliptic PDE and the Fréchet derivative in the Hilbert space $L^{2}(\\Omega)$.\n\nReport, as your final answer, the closed-form analytic expression for the $L^{2}(\\Omega)$-gradient of $j(u)$. No numerical evaluation is required. If your final analytic expression involves any auxiliary variable, it must be specified implicitly by a PDE and boundary conditions derived from the stated principles. The final answer must be a single expression without units.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and complete. It represents a standard problem in the field of PDE-constrained optimization and inverse problems. We may proceed with the derivation.\n\nThe objective is to find the gradient of the reduced functional $j(u) = J(S(u), u)$ with respect to the control variable $u \\in L^{2}(\\Omega)$. The gradient is an element of the Hilbert space $L^{2}(\\Omega)$, which we denote as $\\nabla j(u)$. By the Riesz representation theorem, the gradient is defined via the Fréchet derivative $j'(u)$ as follows:\n$$\nj'(u)h = \\langle \\nabla j(u), h \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} \\nabla j(u) h \\,d\\mathbf{x}\n$$\nfor all perturbations $h \\in L^{2}(\\Omega)$. The Fréchet derivative, acting on a direction $h$, is given by the directional derivative:\n$$\nj'(u)h = \\lim_{\\epsilon \\to 0} \\frac{j(u+\\epsilon h) - j(u)}{\\epsilon}.\n$$\n\nThe reduced functional is given by\n$$\nj(u) = \\frac{1}{2}\\|S(u) - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u\\|_{L^{2}(\\Omega)}^{2}.\n$$\nLet $y = S(u)$ be the state corresponding to the control $u$, which is the weak solution of\n$$\n-\\Delta y = u \\quad \\text{in } \\Omega, \\qquad y=0 \\quad \\text{on } \\partial \\Omega.\n$$\nThe weak formulation of this state equation is: find $y \\in H_{0}^{1}(\\Omega)$ such that for all test functions $v \\in H_{0}^{1}(\\Omega)$,\n$$\n\\int_{\\Omega} \\nabla y \\cdot \\nabla v \\,d\\mathbf{x} = \\int_{\\Omega} u v \\,d\\mathbf{x}.\n$$\nThis equation defines the control-to-state map $S: u \\mapsto y$. Since the governing PDE is linear, the map $S$ is a linear operator.\n\nWe now compute the directional derivative of $j(u)$. Let us consider a perturbation $u + \\epsilon h$ for a small parameter $\\epsilon > 0$ and a direction $h \\in L^{2}(\\Omega)$. The corresponding state is $y_{u+\\epsilon h} = S(u+\\epsilon h)$. Due to the linearity of $S$, we have $y_{u+\\epsilon h} = S(u) + \\epsilon S(h)$. Let us define the sensitivity state $\\delta y := S(h)$. This $\\delta y \\in H_{0}^{1}(\\Omega)$ is the weak solution to the PDE with source term $h$:\n$$\n-\\Delta(\\delta y) = h \\quad \\text{in } \\Omega, \\qquad \\delta y = 0 \\quad \\text{on } \\partial \\Omega.\n$$\nThe weak formulation for the sensitivity equation is: find $\\delta y \\in H_{0}^{1}(\\Omega)$ such that for all $v \\in H_{0}^{1}(\\Omega)$,\n$$\n\\int_{\\Omega} \\nabla (\\delta y) \\cdot \\nabla v \\,d\\mathbf{x} = \\int_{\\Omega} h v \\,d\\mathbf{x}.\n$$\n\nNow we evaluate $j(u+\\epsilon h)$:\n$$\nj(u+\\epsilon h) = \\frac{1}{2}\\|S(u+\\epsilon h) - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u+\\epsilon h\\|_{L^{2}(\\Omega)}^{2} \\\\\n= \\frac{1}{2}\\|(y + \\epsilon \\delta y) - y_{d}\\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2}\\|u+\\epsilon h\\|_{L^{2}(\\Omega)}^{2}.\n$$\nWe expand the squared norms, which are defined by the $L^{2}(\\Omega)$ inner product $\\langle f, g \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} f g \\,d\\mathbf{x}$:\n$$\n\\|(y - y_{d}) + \\epsilon \\delta y\\|_{L^{2}(\\Omega)}^{2} = \\|y-y_{d}\\|^{2} + 2\\epsilon\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} + \\epsilon^{2}\\|\\delta y\\|^{2} \\\\\n\\|u+\\epsilon h\\|_{L^{2}(\\Omega)}^{2} = \\|u\\|^{2} + 2\\epsilon\\langle u, h \\rangle_{L^{2}(\\Omega)} + \\epsilon^{2}\\|h\\|^{2}.\n$$\nSubstituting these expansions into the expression for $j(u+\\epsilon h)$ gives:\n$$\nj(u+\\epsilon h) = \\left(\\frac{1}{2}\\|y-y_{d}\\|^{2} + \\frac{\\alpha}{2}\\|u\\|^{2}\\right) + \\epsilon \\left( \\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} + \\alpha \\langle u, h \\rangle_{L^{2}(\\Omega)} \\right) + O(\\epsilon^{2}).\n$$\nRecognizing that the first term is $j(u)$, we find the directional derivative:\n$$\nj'(u)h = \\lim_{\\epsilon \\to 0} \\frac{j(u+\\epsilon h) - j(u)}{\\epsilon} = \\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} + \\alpha \\langle u, h \\rangle_{L^{2}(\\Omega)}.\n$$\nThis expression currently involves $\\delta y$, which depends on the perturbation direction $h$. To obtain the gradient, we must express this derivative as an inner product of some function with $h$. This is achieved by introducing an adjoint state.\n\nWe introduce the adjoint state $p \\in H_{0}^{1}(\\Omega)$ as the weak solution to an auxiliary problem, the adjoint equation. The purpose of this equation is to replace the term involving $\\delta y$ with a term involving $h$. Let us define the adjoint equation as:\n$$\n-\\Delta p = y - y_d \\quad \\text{in } \\Omega, \\qquad p = 0 \\quad \\text{on } \\partial \\Omega.\n$$\nThe source term for this PDE is chosen precisely as the pre-factor of $\\delta y$ in the inner product term $\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)}$. The weak formulation for the adjoint equation is: find $p \\in H_{0}^{1}(\\Omega)$ such that for all test functions $w \\in H_{0}^{1}(\\Omega)$,\n$$\n\\int_{\\Omega} \\nabla p \\cdot \\nabla w \\,d\\mathbf{x} = \\int_{\\Omega} (y-y_{d}) w \\,d\\mathbf{x}.\n$$\n\nNow, we use the weak formulations to transform the term $\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)}$.\nFrom the definition of the inner product, we have $\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} (y-y_{d}) \\delta y \\,d\\mathbf{x}$.\nIn the weak formulation of the adjoint equation, we can choose the test function $w = \\delta y$, since $\\delta y \\in H_{0}^{1}(\\Omega)$. This yields:\n$$\n\\int_{\\Omega} \\nabla p \\cdot \\nabla (\\delta y) \\,d\\mathbf{x} = \\int_{\\Omega} (y-y_{d}) \\delta y \\,d\\mathbf{x} = \\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)}.\n$$\nNext, we turn to the weak formulation of the sensitivity equation. We are free to choose any test function $v \\in H_{0}^{1}(\\Omega)$. Since the adjoint state $p$ is in $H_{0}^{1}(\\Omega)$, we can choose $v=p$. This gives:\n$$\n\\int_{\\Omega} \\nabla (\\delta y) \\cdot \\nabla p \\,d\\mathbf{x} = \\int_{\\Omega} h p \\,d\\mathbf{x} = \\langle h, p \\rangle_{L^{2}(\\Omega)}.\n$$\nBy the symmetry of the dot product, $\\nabla p \\cdot \\nabla (\\delta y) = \\nabla (\\delta y) \\cdot \\nabla p$. Therefore, we have established the crucial identity:\n$$\n\\langle y-y_{d}, \\delta y \\rangle_{L^{2}(\\Omega)} = \\langle p, h \\rangle_{L^{2}(\\Omega)}.\n$$\nThis justification, using the weak formulations, is the rigorous counterpart to applying Green's identities, as the weak forms are themselves derived via integration by parts (which is a form of Green's identity) and incorporate the boundary conditions by restricting the function spaces to $H_{0}^{1}(\\Omega)$.\n\nSubstituting this identity back into our expression for the directional derivative $j'(u)h$:\n$$\nj'(u)h = \\langle p, h \\rangle_{L^{2}(\\Omega)} + \\alpha \\langle u, h \\rangle_{L^{2}(\\Omega)}.\n$$\nUsing the linearity of the inner product, we combine the terms:\n$$\nj'(u)h = \\langle p + \\alpha u, h \\rangle_{L^{2}(\\Omega)}.\n$$\nThis is now in the required form $j'(u)h = \\langle g, h \\rangle_{L^{2}(\\Omega)}$ for $g = p + \\alpha u$. By the Riesz representation theorem, the gradient of the functional $j(u)$ in the Hilbert space $L^{2}(\\Omega)$ is this function $g$.\nThus, the gradient of the reduced functional is\n$$\n\\nabla j(u) = p + \\alpha u,\n$$\nwhere $p$ is the adjoint state defined as the weak solution in $H_0^1(\\Omega)$ to $-\\Delta p = y - y_d$, and $y$ is the state variable, which is the weak solution in $H_0^1(\\Omega)$ to $-\\Delta y = u$.",
            "answer": "$$\n\\boxed{p + \\alpha u}\n$$"
        },
        {
            "introduction": "Building on the foundational theory, this practice transitions to a major real-world application: four-dimensional variational data assimilation (4D-Var), a technique central to modern weather forecasting. You will explore how the abstract optimization framework is applied to the immense inverse problem of determining the optimal initial state of a dynamic system from sparse observations over time . This exercise highlights the practical considerations for large-scale problems, including the use of the Gauss-Newton approximation and preconditioning to make the computation tractable.",
            "id": "3409490",
            "problem": "Consider a strong-constraint Four-Dimensional Variational (4D-Var) data assimilation problem for a state vector $x \\in \\mathbb{R}^{n}$ governed by a time-discretized Partial Differential Equation (PDE) model over assimilation times $t_{0}, t_{1}, \\dots, t_{N}$. The PDE-constrained discrete model mapping from step $k$ to $k+1$ is denoted by $m_{k}: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$, and the model is assumed perfect (no model error), so that $x_{k+1} = m_{k}(x_{k})$. Observations $y_{k} \\in \\mathbb{R}^{p_{k}}$ at times $t_{k}$ are related to the model state through a (generally nonlinear) observation operator $h_{k}: \\mathbb{R}^{n} \\to \\mathbb{R}^{p_{k}}$, with additive observational error $\\varepsilon_{k}$ that is Gaussian with zero mean and covariance $R_{k} \\in \\mathbb{R}^{p_{k} \\times p_{k}}$ that is symmetric positive definite. The background (also called prior) state $x_{b} \\in \\mathbb{R}^{n}$ has Gaussian error with zero mean and covariance $B \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite. The cost functional for the initial state $x_{0}$ is\n$$\nJ(x_{0}) = \\tfrac{1}{2} \\,(x_{0} - x_{b})^{\\top} B^{-1} (x_{0} - x_{b}) \\;+\\; \\tfrac{1}{2} \\sum_{k=0}^{N} \\big(h_{k}(x_{k}) - y_{k}\\big)^{\\top} R_{k}^{-1} \\big(h_{k}(x_{k}) - y_{k}\\big),\n$$\nwhere $x_{k}$ is obtained by model integration $x_{k} = M_{0,k}(x_{0})$, with $M_{0,k} = m_{k-1} \\circ \\cdots \\circ m_{0}$ for $k \\ge 1$ and $M_{0,0} = \\mathrm{Id}$.\n\nLet $\\{x_{k}^{t}\\}_{k=0}^{N}$ be a reference trajectory obtained by integrating the model from a current iterate $x_{0}^{t}$. Define the tangent linear model matrices $M_{k} = \\nabla m_{k}(x_{k}^{t}) \\in \\mathbb{R}^{n \\times n}$ and their products $M_{0,k} = M_{k-1} \\cdots M_{0}$ for $k \\ge 1$, with $M_{0,0} = I$. Define the linearized observation matrices $H_{k} = \\nabla h_{k}(x_{k}^{t}) \\in \\mathbb{R}^{p_{k} \\times n}$ and the innovations at the trajectory $d_{k} = y_{k} - h_{k}(x_{k}^{t}) \\in \\mathbb{R}^{p_{k}}$. Consider the Gauss–Newton linearization of $J$ around the trajectory, yielding the incremental cost in the control increment $\\delta x_{0} \\in \\mathbb{R}^{n}$:\n$$\nJ_{\\mathrm{inc}}(\\delta x_{0}) \\;=\\; \\tfrac{1}{2}\\,\\delta x_{0}^{\\top} B^{-1} \\delta x_{0} \\;+\\; \\tfrac{1}{2} \\sum_{k=0}^{N} \\big( H_{k} \\, M_{0,k} \\, \\delta x_{0} - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} \\, M_{0,k} \\, \\delta x_{0} - d_{k} \\big).\n$$\nIntroduce the background preconditioning by the symmetric positive definite square root $B^{1/2}$, letting $\\delta x_{0} = B^{1/2} v$ for a preconditioned control increment $v \\in \\mathbb{R}^{n}$.\n\nStarting from the above definitions and the Gauss–Newton approximation (neglecting second-order terms from the observation-model nonlinearity beyond the first-order linearization), derive the normal equations for $v$ and express them in the form of a symmetric positive definite linear system. Your derivation must begin from the stated cost functional and the tangent linearization and proceed through the first-order optimality condition. Then, write the preconditioned normal equations in terms of the known quantities $B^{1/2}$, $M_{0,k}$, $H_{k}$, $R_{k}$, and $d_{k}$ only.\n\nAnswer specification:\n- Your final answer must be the pair consisting of the preconditioned Gauss–Newton Hessian operator and the right-hand side of the normal equations, expressed solely as analytic matrix/vector expressions in the specified symbols. Do not include any equality or assignment signs in the final boxed expression.\n- Provide the pair as a two-entry row matrix using the $\\mathrm{pmatrix}$ environment, with the first entry being the operator and the second entry being the right-hand side vector.\n- No numerical evaluation is required, and no units are involved.",
            "solution": "The user has provided a problem statement that has been validated and found to be scientifically grounded, well-posed, objective, and internally consistent. The problem asks for the derivation of the preconditioned normal equations for a 4D-Var data assimilation problem, starting from a given incremental cost functional.\n\nThe analysis begins with the provided incremental cost functional, $J_{\\mathrm{inc}}(\\delta x_{0})$, which is a quadratic approximation of the full cost functional around a reference trajectory. This functional is to be minimized with respect to the initial state increment, $\\delta x_{0} \\in \\mathbb{R}^{n}$. The functional is:\n$$ J_{\\mathrm{inc}}(\\delta x_{0}) = \\frac{1}{2} \\delta x_{0}^{\\top} B^{-1} \\delta x_{0} + \\frac{1}{2} \\sum_{k=0}^{N} \\big( H_{k} M_{0,k} \\delta x_{0} - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} M_{0,k} \\delta x_{0} - d_{k} \\big) $$\nHere, $\\delta x_{0}$ is the control variable. We introduce a preconditioning to improve the conditioning of the minimization problem. The preconditioning is defined by the change of variables $\\delta x_{0} = B^{1/2} v$, where $v \\in \\mathbb{R}^{n}$ is the new preconditioned control variable and $B^{1/2}$ is the symmetric positive definite square root of the background error covariance matrix $B$.\n\nWe substitute this change of variables into the cost functional $J_{\\mathrm{inc}}$ to obtain the preconditioned cost functional, denoted $J_{p}(v) = J_{\\mathrm{inc}}(B^{1/2} v)$.\n\nThe background term becomes:\n$$ \\frac{1}{2} (B^{1/2} v)^{\\top} B^{-1} (B^{1/2} v) = \\frac{1}{2} v^{\\top} (B^{1/2})^{\\top} B^{-1} B^{1/2} v $$\nSince $B$ is symmetric, $B^{1/2}$ is also symmetric, thus $(B^{1/2})^{\\top} = B^{1/2}$. Furthermore, $B^{-1} = (B^{1/2}B^{1/2})^{-1} = (B^{1/2})^{-1}(B^{1/2})^{-1}$. Let's denote $(B^{1/2})^{-1}$ as $B^{-1/2}$. The expression simplifies to:\n$$ \\frac{1}{2} v^{\\top} B^{1/2} B^{-1/2} B^{-1/2} B^{1/2} v = \\frac{1}{2} v^{\\top} I I v = \\frac{1}{2} v^{\\top} v $$\n\nThe observation term for each time index $k$ becomes:\n$$ \\frac{1}{2} \\big( H_{k} M_{0,k} (B^{1/2} v) - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} M_{0,k} B^{1/2} v - d_{k} \\big) $$\n\nSumming all terms, the preconditioned cost functional $J_{p}(v)$ is:\n$$ J_{p}(v) = \\frac{1}{2} v^{\\top} v + \\frac{1}{2} \\sum_{k=0}^{N} \\big( H_{k} M_{0,k} B^{1/2} v - d_{k} \\big)^{\\top} R_{k}^{-1} \\big( H_{k} M_{0,k} B^{1/2} v - d_{k} \\big) $$\nTo find the minimum of this quadratic functional, we compute its gradient with respect to $v$ and set it to the zero vector. This is the first-order optimality condition. We use the following standard matrix calculus identities: for a vector $u$, symmetric matrix $A$, and vector $c$, $\\nabla_{u}(\\frac{1}{2} u^{\\top} A u) = A u$ and $\\nabla_{u}((Au - c)^{\\top}W(Au-c)) = 2A^{\\top}W(Au-c)$.\n\nThe gradient of the background term $\\frac{1}{2} v^{\\top} v$ with respect to $v$ is simply $v$.\n\nFor the observation part, let's denote the operator multiplying $v$ as $\\mathcal{A}_{k} = H_{k} M_{0,k} B^{1/2}$. The gradient of the $k$-th term in the sum is:\n$$ \\nabla_{v} \\left[ \\frac{1}{2} (\\mathcal{A}_{k} v - d_{k})^{\\top} R_{k}^{-1} (\\mathcal{A}_{k} v - d_{k}) \\right] = \\mathcal{A}_{k}^{\\top} R_{k}^{-1} (\\mathcal{A}_{k} v - d_{k}) $$\n$$ = (\\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k}) v - \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} $$\nThe gradient of the full cost functional $\\nabla_{v} J_{p}(v)$ is the sum of the gradients of all its terms:\n$$ \\nabla_{v} J_{p}(v) = v + \\sum_{k=0}^{N} \\left[ (\\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k}) v - \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} \\right] $$\nWe can group the terms multiplying $v$ and the constant terms:\n$$ \\nabla_{v} J_{p}(v) = \\left( I + \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k} \\right) v - \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} $$\nSetting the gradient to zero, $\\nabla_{v} J_{p}(v) = 0$, gives the normal equations for $v$:\n$$ \\left( I + \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} \\mathcal{A}_{k} \\right) v = \\sum_{k=0}^{N} \\mathcal{A}_{k}^{\\top} R_{k}^{-1} d_{k} $$\nThis equation is of the form $\\mathcal{H}_{p} v = g_{p}$, where $\\mathcal{H}_{p}$ is the preconditioned Hessian operator and $g_{p}$ is the preconditioned right-hand side vector.\n\nWe now substitute back the definition $\\mathcal{A}_{k} = H_{k} M_{0,k} B^{1/2}$ to express these quantities in terms of the given matrices and vectors.\n\nThe preconditioned Hessian operator is:\n$$ \\mathcal{H}_{p} = I + \\sum_{k=0}^{N} (H_{k} M_{0,k} B^{1/2})^{\\top} R_{k}^{-1} (H_{k} M_{0,k} B^{1/2}) $$\nUsing the property $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$ and the symmetry of $B^{1/2}$, i.e., $(B^{1/2})^{\\top} = B^{1/2}$:\n$$ \\mathcal{H}_{p} = I + \\sum_{k=0}^{N} B^{1/2} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} H_{k} M_{0,k} B^{1/2} $$\nWe can factor out $B^{1/2}$ from the summation to obtain the final form:\n$$ \\mathcal{H}_{p} = I + B^{1/2} \\left( \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} H_{k} M_{0,k} \\right) B^{1/2} $$\n\nThe preconditioned right-hand side vector is:\n$$ g_{p} = \\sum_{k=0}^{N} (H_{k} M_{0,k} B^{1/2})^{\\top} R_{k}^{-1} d_{k} = \\sum_{k=0}^{N} B^{1/2} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} d_{k} $$\nFactoring out $B^{1/2}$ gives:\n$$ g_{p} = B^{1/2} \\left( \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} d_{k} \\right) $$\nThese expressions for $\\mathcal{H}_{p}$ and $g_{p}$ are the components of the preconditioned normal equations, expressed solely in terms of the quantities specified in the problem statement. The operator $\\mathcal{H}_{p}$ is symmetric and positive definite as it is the sum of the identity matrix (which is symmetric positive definite) and a symmetric positive semi-definite matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nI + B^{1/2} \\left( \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} H_{k} M_{0,k} \\right) B^{1/2} & B^{1/2} \\sum_{k=0}^{N} M_{0,k}^{\\top} H_{k}^{\\top} R_{k}^{-1} d_{k}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Real-world models are rarely perfect; their parameters are often subject to uncertainty. This advanced practice introduces a risk-averse optimization framework designed to find solutions that are robust against worst-case scenarios, rather than just optimal for a single deterministic model . You will learn to formulate and solve such a problem using the Conditional Value-at-Risk (CVaR), combining the adjoint method with tools from stochastic optimization like sample-average approximation and subgradient calculus for non-smooth objectives.",
            "id": "3409489",
            "problem": "Consider a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$ with $d \\in \\{2,3\\}$, and let the Partial Differential Equation (PDE) state $u(\\cdot;\\xi,m) \\in H_{0}^{1}(\\Omega)$ solve the weak form of the linear elliptic boundary value problem\n$$\n-\\nabla \\cdot \\big(a(x,\\xi)\\nabla u(x;\\xi,m)\\big) \\;=\\; q(x) + m(x) \\quad \\text{in } \\Omega,\\qquad u(\\cdot;\\xi,m)=0 \\quad \\text{on } \\partial\\Omega,\n$$\nwhere $a(\\cdot,\\xi)$ is a random, uniformly elliptic coefficient field, $\\xi$ is a random parameter, $q \\in L^{2}(\\Omega)$ is a deterministic source, and the control $m \\in L^{2}(\\Omega)$ is a distributed forcing. Let $B:H_{0}^{1}(\\Omega)\\to\\mathbb{R}^{p}$ be a bounded linear observation operator, and $d\\in\\mathbb{R}^{p}$ be fixed data. Define the data misfit functional\n$$\nJ(m,\\xi)\\;=\\;\\tfrac{1}{2}\\,\\|B\\,u(\\cdot;\\xi,m)-d\\|_{2}^{2}.\n$$\nAdopt the Conditional Value-at-Risk (CVaR) at level $\\tau\\in(0,1)$ from coherent risk theory as the risk measure for $J(m,\\xi)$. Assume $\\{\\,\\xi_{i}\\,\\}_{i=1}^{N}$ is an independent and identically distributed sample from the law of $\\xi$, and suppose $a(\\cdot,\\xi)$ satisfies the standard uniform ellipticity and boundedness assumptions ensuring well-posedness of the state and adjoint PDEs.\n\nDevelop a stochastic PDE-constrained and robust optimization formulation that is risk-averse with respect to uncertainty in $\\xi$ by performing the following tasks:\n\n- Starting from the foundational definition of Conditional Value-at-Risk, derive a sample-average approximation of the risk-averse objective for the data misfit $J(m,\\xi)$ using the sample $\\{\\,\\xi_{i}\\,\\}_{i=1}^{N}$.\n\n- For the resulting sample-average objective, derive a subgradient with respect to the control $m \\in L^{2}(\\Omega)$, expressed entirely in terms of solutions of PDEs and linear operators, and clearly identify the adjoint PDE that must be solved for each sample.\n\nExpress your final answer as analytic expressions, not numerical values. No external references or shortcut formulas are permitted in the derivation. All mathematical entities must be written in LaTeX. The final boxed answer must contain the sample-average approximation and the subgradient with respect to $m$ in a single row matrix using the $\\mathrm{pmatrix}$ environment.",
            "solution": "### Derivation\n\nThe goal is to formulate a risk-averse optimization problem for the control $m$ and derive the necessary expressions for its solution. The risk aversion is with respect to the uncertainty in the coefficient field $a(x, \\xi)$, captured by the random parameter $\\xi$. The data misfit $J(m, \\xi)$ is a random variable, and we aim to minimize its risk, as measured by the Conditional Value-at-Risk.\n\n#### Task 1: Sample-Average Approximation of the CVaR Objective\n\nThe Conditional Value-at-Risk (CVaR) of a real-valued random variable $X$ at a confidence level $\\tau \\in (0,1)$ is defined as the expected value of $X$ conditional on $X$ exceeding its Value-at-Risk (VaR), where $\\text{VaR}_{\\tau}(X)$ is the $\\tau$-quantile of the distribution of $X$. A fundamental result by Rockafellar and Uryasev provides a more convenient representation for optimization purposes:\n$$\n\\text{CVaR}_{\\tau}(X) = \\inf_{t \\in \\mathbb{R}} \\left( t + \\frac{1}{1-\\tau} \\mathbb{E}\\left[ (X - t)_+ \\right] \\right),\n$$\nwhere $(z)_+ = \\max(0, z)$. The infimum is achieved at $t = \\text{VaR}_{\\tau}(X)$ under certain conditions on the distribution of $X_i$.\n\nIn our problem, the random variable is the data misfit functional $X = J(m,\\xi)$. The risk-averse optimization problem is to find a control $m$ that minimizes the CVaR of $J(m,\\xi)$. This can be formulated as a joint minimization problem over the control $m \\in L^2(\\Omega)$ and the auxiliary scalar variable $t \\in \\mathbb{R}$:\n$$\n\\min_{m \\in L^2(\\Omega), t \\in \\mathbb{R}} \\left( t + \\frac{1}{1-\\tau} \\mathbb{E}\\left[ (J(m,\\xi) - t)_+ \\right] \\right).\n$$\nThe expectation $\\mathbb{E}[\\cdot]$ is taken with respect to the probability measure of $\\xi$. Since we are given a finite i.i.d. sample $\\{\\xi_i\\}_{i=1}^N$, we can approximate the expectation using a sample average. This leads to the Sample Average Approximation (SAA) of the risk-averse objective function, denoted by $\\mathcal{R}_N(m,t)$:\n$$\n\\mathcal{R}_N(m,t) = t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\left( J(m,\\xi_i) - t \\right)_+.\n$$\nLet $u_i = u(\\cdot;m,\\xi_i)$ be the solution of the state PDE for a given control $m$ and sample $\\xi_i$. The misfit is $J(m,\\xi_i) = \\frac{1}{2}\\|B u_i - d\\|_2^2$. Substituting this into the SAA objective, we obtain the final form for the first task:\n$$\n\\mathcal{R}_N(m,t) = t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\max\\left(0, \\frac{1}{2}\\|B u(\\cdot;m,\\xi_i) - d\\|_2^2 - t\\right).\n$$\n\n#### Task 2: Subgradient of the SAA Objective with respect to $m$\n\nTo solve the optimization problem $\\min_{m,t} \\mathcal{R}_N(m,t)$, one typically uses a gradient-based method. Since the function $\\max(0,z)$ is not differentiable at $z=0$, we must work with subgradients. We are asked to derive a subgradient of $\\mathcal{R}_N(m,t)$ with respect to the control $m \\in L^2(\\Omega)$.\n\nThe objective function is $\\mathcal{R}_N(m,t) = t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\max(0, J(m,\\xi_i) - t)$.\nThe term $t$ is constant with respect to $m$. By linearity of subdifferentiation, a subgradient of $\\mathcal{R}_N$ with respect to $m$, denoted $g_m \\in \\partial_m \\mathcal{R}_N(m,t)$, is given by:\n$$\ng_m = \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} g_{m,i},\n$$\nwhere $g_{m,i} \\in \\partial_m \\max(0, J(m,\\xi_i) - t)$.\n\nWe apply the chain rule for subdifferentials. For a composition $f(g(x))$, where $f$ is a convex function and $g$ is a differentiable function, we have $\\partial (f \\circ g)(x) = \\partial f(g(x)) \\cdot \\nabla g(x)$.\nHere, let $f(z) = \\max(0,z)$ and $g_i(m) = J(m,\\xi_i) - t$. The subdifferential of $f(z)$ is:\n$$\n\\partial f(z) = \\partial (\\cdot)_+|_z =\n\\begin{cases}\n    \\{0\\}       & \\text{if } z < 0 \\\\\n    [0, 1]    & \\text{if } z = 0 \\\\\n    \\{1\\}       & \\text{if } z > 0\n\\end{cases}\n$$\nThe gradient of $g_i(m)$ with respect to $m$ is simply $\\nabla_m J(m,\\xi_i)$.\nTherefore, a subgradient $g_{m,i}$ is given by $g_{m,i} = \\lambda_i \\nabla_m J(m,\\xi_i)$, where $\\lambda_i \\in \\partial (\\cdot)_+|_{J(m,\\xi_i)-t}$. That is,\n$$\n\\lambda_i \\in\n\\begin{cases}\n    \\{1\\}       & \\text{if } J(m,\\xi_i) > t \\\\\n    \\{0\\}       & \\text{if } J(m,\\xi_i) < t \\\\\n    [0, 1]    & \\text{if } J(m,\\xi_i) = t\n\\end{cases}\n$$\nA subgradient of the SAA objective with respect to $m$ is thus:\n$$\ng_m = \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\lambda_i \\nabla_m J(m,\\xi_i),\n$$\nfor any choice of $\\lambda_i$ satisfying the conditions above.\n\nThe final step is to derive an expression for the Fréchet derivative (gradient) $\\nabla_m J(m,\\xi_i)$ using the adjoint method. For each sample $i \\in \\{1,\\dots,N\\}$, let $a_i(x) = a(x,\\xi_i)$, $u_i(x) = u(x;m,\\xi_i)$. The state $u_i \\in H_0^1(\\Omega)$ satisfies the weak form of the state equation:\n$$\n\\int_{\\Omega} a_i(x) \\nabla u_i \\cdot \\nabla v \\,dx = \\int_{\\Omega} (q(x)+m(x))v(x) \\,dx, \\quad \\forall v \\in H_0^1(\\Omega).\n$$\nThe misfit is $J(m,\\xi_i) = \\frac{1}{2}\\|B u_i - d\\|_2^2$. Its derivative with respect to $m$ in a direction $\\delta m \\in L^2(\\Omega)$ is given by:\n$$\n\\langle \\nabla_m J(m,\\xi_i), \\delta m \\rangle_{L^2} = \\langle B u_i - d, B\\dot{u}_i \\rangle_{\\mathbb{R}^p},\n$$\nwhere $\\dot{u}_i = \\frac{du_i}{dm}[\\delta m] \\in H_0^1(\\Omega)$ is the state sensitivity. Differentiating the state equation with respect to $m$ in direction $\\delta m$ yields the sensitivity equation for $\\dot{u}_i$:\n$$\n\\int_{\\Omega} a_i(x) \\nabla \\dot{u}_i \\cdot \\nabla v \\,dx = \\int_{\\Omega} \\delta m(x) v(x) \\,dx, \\quad \\forall v \\in H_0^1(\\Omega).\n$$\nTo avoid solving for $\\dot{u}_i$ for every $\\delta m$, we introduce an adjoint state $p_i \\in H_0^1(\\Omega)$. The adjoint equation is defined to relate the misfit derivative to the adjoint variable. We can write the misfit derivative as $\\langle B u_i - d, B\\dot{u}_i \\rangle_{\\mathbb{R}^p} = \\langle B^*(B u_i - d), \\dot{u}_i \\rangle$ where $B^*:\\mathbb{R}^p \\to (H_0^1(\\Omega))^*$ is the adjoint operator of $B$. We define the adjoint equation for $p_i$ such that for any $w \\in H_0^1(\\Omega)$:\n$$\n\\int_{\\Omega} a_i(x) \\nabla w \\cdot \\nabla p_i \\,dx = \\langle B^*(B u_i - d), w \\rangle_{(H_0^1)^*, H_0^1}.\n$$\nThe bilinear form on the left is symmetric, so $\\int a_i \\nabla w \\cdot \\nabla p_i \\,dx = \\int a_i \\nabla p_i \\cdot \\nabla w \\,dx$. The strong form of this adjoint PDE is:\n$$\n-\\nabla \\cdot(a_i(x) \\nabla p_i(x)) = B^*(B u_i - d) \\quad \\text{in } \\Omega, \\qquad p_i = 0 \\quad \\text{on } \\partial\\Omega.\n$$\nNow, by setting $w=\\dot{u}_i$ in the weak form of the adjoint equation, we get:\n$$\n\\langle \\nabla_m J(m,\\xi_i), \\delta m \\rangle_{L^2} = \\langle B^*(B u_i - d), \\dot{u}_i \\rangle = \\int_{\\Omega} a_i(x) \\nabla \\dot{u}_i \\cdot \\nabla p_i \\,dx.\n$$\nAnd by setting $v=p_i$ in the weak form of the sensitivity equation, we get:\n$$\n\\int_{\\Omega} a_i(x) \\nabla \\dot{u}_i \\cdot \\nabla p_i \\,dx = \\int_{\\Omega} \\delta m(x) p_i(x) \\,dx = \\langle p_i, \\delta m \\rangle_{L^2(\\Omega)}.\n$$\nEquating the two expressions, we have $\\langle \\nabla_m J(m,\\xi_i), \\delta m \\rangle_{L^2} = \\langle p_i, \\delta m \\rangle_{L^2}$. Since this holds for all $\\delta m \\in L^2(\\Omega)$, we can identify the gradient as:\n$$\n\\nabla_m J(m,\\xi_i) = p_i.\n$$\nSubstituting this result back into the expression for $g_m$, we obtain a subgradient of $\\mathcal{R}_N(m,t)$ with respect to $m$:\n$$\ng_m = \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\lambda_i p_i,\n$$\nwhere for each $i=1, \\dots, N$, $p_i$ is the solution to the adjoint PDE defined above, which in turn depends on the solution $u_i$ of the state PDE. The coefficients $\\lambda_i$ are chosen from the subdifferential of the $\\max(0,\\cdot)$ function as previously defined.\n\nThis completes the derivation. The results are summarized in the final answer.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} t + \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\max\\left(0, \\frac{1}{2}\\|B u(\\cdot;m,\\xi_i) - d\\|_2^2 - t\\right) & \\frac{1}{N(1-\\tau)} \\sum_{i=1}^{N} \\lambda_i p_i \\end{pmatrix}}\n$$"
        }
    ]
}