{
    "hands_on_practices": [
        {
            "introduction": "A foundational task in observation quality control is to determine if a new observation is statistically consistent with our prior knowledge, encapsulated by the background state and its error statistics. This practice focuses on the chi-square test, a cornerstone of gross error checking. You will compute the expected variance of the innovation—the difference between the observation and the model background—and use it to normalize the innovation, yielding a statistic that can be compared against a known statistical distribution to make an objective decision . This exercise provides a concrete application of error propagation, illustrating how uncertainties from the background model and the observation instrument combine to define a \"believability\" threshold for incoming data.",
            "id": "3406849",
            "problem": "Consider a single-observation quality control task in a variational data assimilation system. Let the model state be of dimension $n=3$, and let the observation operator be differentiable and linearized at the background state $x_b$ with Jacobian (row) vector $H \\in \\mathbb{R}^{1 \\times 3}$. The background error $x - x_b$ is modeled as a zero-mean random vector with covariance matrix $B \\in \\mathbb{R}^{3 \\times 3}$. The observation error is the sum of an instrument error and a representation error, each modeled as zero-mean, mutually independent random variables, and independent of the background error. The innovation (observation-minus-background in observation space) is $d = y - \\mathcal{H}(x_b)$, where $\\mathcal{H}$ is the nonlinear observation operator and $H$ is its Jacobian at $x_b$. Use only the linearized error-propagation rule for variances and the independence assumptions to derive the scalar innovation variance $S = \\operatorname{Var}(d)$ and then compute a chi-square gross error test statistic $z = d^2 / S$.\n\nYou are given:\n- The linearized observation operator Jacobian as $H = \\begin{bmatrix} 0.5 & -1.2 & 0.3 \\end{bmatrix}$.\n- The background error covariance\n$$\nB = \\begin{bmatrix}\n1.0 & 0.2 & -0.1 \\\\\n0.2 & 2.0 & 0.4 \\\\\n-0.1 & 0.4 & 1.5\n\\end{bmatrix}.\n$$\n- The instrument error variance $R = 0.49$.\n- The representation error variance $R_{\\text{rep}} = 0.36$.\n- The realized innovation $d = 5.0$.\n- A prescribed scalar chi-square threshold $\\gamma = 6.635$ for one degree of freedom.\n\nStarting from the stated modeling assumptions and the linearization, first derive the scalar innovation variance $S$ and then compute the statistic $z = d^2 / S$. State whether the residual is consistent with the assumed error statistics by comparing $z$ to $\\gamma$ in your reasoning. Report only the value of $z$ as your final answer. Round your numerical answer to four significant figures. No units are required.",
            "solution": "The problem requires the computation of a chi-square gross error test statistic for a single observation in a data assimilation context. The statistic is defined as $z = d^2 / S$, where $d$ is the innovation and $S$ is the innovation variance.\n\nFirst, we must derive the expression for the innovation variance, $S$. The innovation is the difference between the observation $y$ and the model's forecast of the observation, $\\mathcal{H}(x_b)$, where $\\mathcal{H}$ is the (potentially nonlinear) observation operator and $x_b$ is the background state.\n$$d = y - \\mathcal{H}(x_b)$$\nThe observation $y$ is related to the true state $x_t$ through the observation operator and an observation error $\\epsilon_o$:\n$$y = \\mathcal{H}(x_t) + \\epsilon_o$$\nThe background state $x_b$ is an estimate of the true state, differing by the background error $\\epsilon_b = x_b - x_t$. We can linearize the operator $\\mathcal{H}$ about the background state $x_b$ using a first-order Taylor expansion: $\\mathcal{H}(x_t) \\approx \\mathcal{H}(x_b) + H(x_t - x_b)$.\nSubstituting these into the expression for $d$:\n$$d \\approx (\\mathcal{H}(x_b) + H(x_t - x_b) + \\epsilon_o) - \\mathcal{H}(x_b) = H(x_t - x_b) + \\epsilon_o = -H\\epsilon_b + \\epsilon_o$$\nThe innovation variance $S$ is the expected value of the squared innovation, $S = \\operatorname{Var}(d) = E[d^2]$, since both $\\epsilon_b$ and $\\epsilon_o$ are stated to be zero-mean, which implies $E[d]=0$.\n$$S = E[(-H\\epsilon_b + \\epsilon_o)(-H\\epsilon_b + \\epsilon_o)^T]$$\nExpanding this and using the independence of the background and observation errors (which means the cross-terms $E[\\epsilon_b\\epsilon_o^T]$ are zero), we get:\n$$S = H E[\\epsilon_b\\epsilon_b^T] H^T + E[\\epsilon_o\\epsilon_o^T]$$\nBy definition, the background error covariance matrix is $B = E[\\epsilon_b\\epsilon_b^T]$. The total observation error $\\epsilon_o$ is the sum of an instrument error $\\epsilon_{inst}$ and a representation error $\\epsilon_{rep}$. As these are independent, the total observation error variance $R_o$ is the sum of the individual variances: $R_o = \\operatorname{Var}(\\epsilon_{inst}) + \\operatorname{Var}(\\epsilon_{rep}) = R + R_{\\text{rep}}$.\nThe final formula for the innovation variance is:\n$$S = HBH^T + R + R_{\\text{rep}}$$\nNow, we substitute the given values into this formula.\nGiven:\n$H = \\begin{bmatrix} 0.5 & -1.2 & 0.3 \\end{bmatrix}$\n$B = \\begin{bmatrix} 1.0 & 0.2 & -0.1 \\\\ 0.2 & 2.0 & 0.4 \\\\ -0.1 & 0.4 & 1.5 \\end{bmatrix}$\n$R = 0.49$\n$R_{\\text{rep}} = 0.36$\n$d = 5.0$\n\nFirst, compute the term $HBH^T$.\n$$H B = \\begin{bmatrix} (0.5)(1.0) + (-1.2)(0.2) + (0.3)(-0.1) & (0.5)(0.2) + (-1.2)(2.0) + (0.3)(0.4) & (0.5)(-0.1) + (-1.2)(0.4) + (0.3)(1.5) \\end{bmatrix}$$\n$$H B = \\begin{bmatrix} 0.23 & -2.18 & -0.08 \\end{bmatrix}$$\nNext, we compute $HBH^T = (HB)H^T$:\n$$HBH^T = \\begin{bmatrix} 0.23 & -2.18 & -0.08 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ -1.2 \\\\ 0.3 \\end{bmatrix}$$\n$$HBH^T = (0.23)(0.5) + (-2.18)(-1.2) + (-0.08)(0.3) = 0.115 + 2.616 - 0.024 = 2.707$$\nNow, compute the total observation error variance $R_o$:\n$$R_o = R + R_{\\text{rep}} = 0.49 + 0.36 = 0.85$$\nThe total innovation variance $S$ is:\n$$S = HBH^T + R_o = 2.707 + 0.85 = 3.557$$\nFinally, we compute the chi-square statistic $z$:\n$$z = \\frac{d^2}{S} = \\frac{(5.0)^2}{3.557} = \\frac{25}{3.557} \\approx 7.028394436...$$\nRounding to four significant figures, we get $z = 7.028$.\n\nThe computed statistic $z \\approx 7.028$ is greater than the prescribed threshold $\\gamma = 6.635$. This means the null hypothesis (that the observation is consistent with the error statistics) is rejected. The innovation is statistically too large, and the observation would be flagged as a gross error.",
            "answer": "$$\\boxed{7.028}$$"
        },
        {
            "introduction": "While simply rejecting observations that fail a statistical test is a valid strategy, it can lead to the loss of valuable information, especially in data-sparse regions. A more nuanced approach is offered by robust statistics, which aims to reduce, rather than eliminate, the influence of potential outliers. This exercise introduces the Huber weighting scheme, a classic M-estimation technique, where observations with large residuals are systematically down-weighted in the assimilation process . By calculating these weights and performing a weighted-least-squares update, you will gain hands-on experience with an iterative method that makes the data assimilation more resilient to gross errors.",
            "id": "3406858",
            "problem": "In a one-dimensional variational data assimilation step, consider $n=6$ independent sensors observing the same scalar state $x$ through a linear observational operator $\\mathcal{H}_i(x)=h_i x$. At iteration $t$, the current estimate is $x^{(t)}=1.0$, and the scalar residuals $r_i=y_i-\\mathcal{H}_i(x^{(t)})$ are available for quality control. To perform a robust gross-error check using the Huber weighting scheme, use the following foundational definitions:\n- Residuals are defined as $r_i=y_i-\\mathcal{H}_i(x^{(t)})$.\n- The robust scale $\\sigma$ of the residuals is provided.\n- The Huber threshold is $\\delta=k\\sigma$ for a chosen parameter $k$.\n- The Huber weights $w_i$ follow the standard M-estimation rule $w_i=\\psi(r_i)/r_i$ with the Huber score function $\\psi(r)=r$ if $|r|\\le \\delta$ and $\\psi(r)=\\delta\\operatorname{sign}(r)$ if $|r|>\\delta$, resulting in $w_i=1$ for $|r_i|\\le \\delta$ and $w_i=\\delta/|r_i|$ for $|r_i|>\\delta$.\n- The Weighted Least Squares (WLS) update solves $x^{(t+1)}=\\arg\\min_x \\sum_{i=1}^{n} w_i \\left(y_i-\\mathcal{H}_i(x)\\right)^2$.\n\nSuppose the sensor sensitivities are $(h_1,h_2,h_3,h_4,h_5,h_6)=(1.0,0.8,1.2,0.6,1.5,0.7)$, the residuals at iteration $t$ are $(r_1,r_2,r_3,r_4,r_5,r_6)=(0.2,-0.1,0.5,-0.4,4.0,-3.5)$, and the robust scale is $\\sigma=0.3$. Choose $k=2$, so that $\\delta=k\\sigma$. Use the provided $r_i$, $\\sigma$, $k$, and $h_i$ to:\n\n1. Compute the Huber weights $w_i$.\n2. Recover the observations $y_i$ from $r_i=y_i-h_i x^{(t)}$.\n3. Perform one WLS update to obtain $x^{(t+1)}$.\n\nReport the final $x^{(t+1)}$ as a dimensionless number, rounded to four significant figures.",
            "solution": "The solution follows the three main steps outlined in the problem: computing the Huber weights, recovering the original observations, and performing a Weighted Least Squares (WLS) update.\n\n**1. Compute Huber Weights**\nFirst, calculate the Huber threshold $\\delta$ using the robust scale $\\sigma=0.3$ and parameter $k=2$:\n$\\delta = k \\sigma = 2 \\times 0.3 = 0.6$.\nNext, apply the Huber weighting rule: $w_i=1$ for $|r_i| \\le \\delta$ and $w_i=\\delta/|r_i|$ for $|r_i| > \\delta$.\n- $|r_1|=0.2, |r_2|=0.1, |r_3|=0.5, |r_4|=0.4$. All are $\\le 0.6$, so $w_1=w_2=w_3=w_4=1$.\n- $|r_5|=4.0 > 0.6$, so $w_5 = 0.6/4.0 = 0.15$.\n- $|r_6|=3.5 > 0.6$, so $w_6 = 0.6/3.5 = 6/35$.\n\n**2. Recover Observations**\nThe observations $y_i$ are recovered from the residual definition $r_i = y_i - h_i x^{(t)}$ with $x^{(t)}=1.0$: $y_i = r_i + h_i$.\n- $y_1 = 0.2 + 1.0 = 1.2$\n- $y_2 = -0.1 + 0.8 = 0.7$\n- $y_3 = 0.5 + 1.2 = 1.7$\n- $y_4 = -0.4 + 0.6 = 0.2$\n- $y_5 = 4.0 + 1.5 = 5.5$\n- $y_6 = -3.5 + 0.7 = -2.8$\n\n**3. Perform WLS Update**\nThe WLS update finds the value of $x$ that minimizes the cost function $J(x) = \\sum w_i (y_i - h_i x)^2$. The solution is given by:\n$$\nx^{(t+1)} = \\frac{\\sum_{i=1}^{n} w_i h_i y_i}{\\sum_{i=1}^{n} w_i h_i^2}\n$$\nWe compute the denominator and numerator separately.\n- **Denominator** $\\sum w_i h_i^2$: The terms are $1(1.0^2), 1(0.8^2), 1(1.2^2), 1(0.6^2), 0.15(1.5^2), (6/35)(0.7^2)$, which evaluate to $1.0, 0.64, 1.44, 0.36, 0.3375, 0.084$. Their sum is $3.8615$.\n- **Numerator** $\\sum w_i h_i y_i$: The terms are $1(1.0)(1.2), 1(0.8)(0.7), 1(1.2)(1.7), 1(0.6)(0.2), 0.15(1.5)(5.5), (6/35)(0.7)(-2.8)$, which evaluate to $1.2, 0.56, 2.04, 0.12, 1.2375, -0.336$. Their sum is $4.8215$.\n\nFinally, the updated state is:\n$$\nx^{(t+1)} = \\frac{4.8215}{3.8615} \\approx 1.248608...\n$$\nRounding to four significant figures gives $1.249$.",
            "answer": "$$\n\\boxed{1.249}\n$$"
        },
        {
            "introduction": "The quality control methods based on linear theory are powerful but can be misleading when the observation operator is highly nonlinear. Strong curvature in the operator can cause the linear approximation of the innovation statistics to be a poor guide, potentially leading to the erroneous rejection of good observations. This practical coding exercise delves into this advanced topic by implementing a curvature-aware gross error check . You will develop a test that explicitly accounts for the potential mismatch arising from model nonlinearity, using information from the Hessian matrix to establish a more reliable outlier detection threshold, a crucial skill for developing robust assimilation systems for complex, real-world models.",
            "id": "3406926",
            "problem": "Consider an observation operator mapping a state vector to scalar observations in the setting of inverse problems and data assimilation. Let the state be $\\mathbf{x} \\in \\mathbb{R}^2$ and define three scalar observation functions $h_1, h_2, h_3$ such that the full observation operator is $H(\\mathbf{x}) = \\big(h_1(\\mathbf{x}), h_2(\\mathbf{x}), h_3(\\mathbf{x})\\big) \\in \\mathbb{R}^3$, with\n- $h_1(\\mathbf{x}) = \\exp(x_1) + \\sin(x_2)$,\n- $h_2(\\mathbf{x}) = \\log\\!\\big(1 + x_1^2\\big) + x_1 x_2$,\n- $h_3(\\mathbf{x}) = \\sqrt{1 + x_2^2} + \\tanh(x_1)$.\n\nAll trigonometric functions must be evaluated with angles in radians.\n\nFor a given background (prior) state $\\hat{\\mathbf{x}} \\in \\mathbb{R}^2$ and an increment $\\delta \\mathbf{x} \\in \\mathbb{R}^2$, define the linearized predictor for each scalar observation $h_i$ as $h_i(\\hat{\\mathbf{x}}) + \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x}$. Let the residual for observation $i$ be $r_i = y_i - h_i(\\hat{\\mathbf{x}})$, where $y_i$ is the observed value. The mismatch between the observed residual and the linearized prediction is then $e_i = r_i - \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x}$.\n\nObservation Quality Control (QC) aims to flag outliers due to model nonlinearity (curvature) and gross errors. We consider a curvature-aware gross error check based on the second-order Taylor remainder bound. For each observation $i$, let $C_i$ denote a curvature bound given by\n$$\nC_i = \\frac{1}{2} \\left\\| \\nabla^2 h_i(\\hat{\\mathbf{x}}) \\right\\|_2 \\, \\left\\| \\delta \\mathbf{x} \\right\\|_2^2,\n$$\nwhere $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ is the Hessian of $h_i$ at $\\hat{\\mathbf{x}}$ and $\\|\\cdot\\|_2$ denotes the spectral norm for matrices and the Euclidean norm for vectors. Let $\\sigma_i > 0$ be the observation noise standard deviation for observation $i$, and let $\\tau > 0$ be a fixed decision factor.\n\nDefine the curvature-aware gross error flag for observation $i$ by the rule:\n- Flag observation $i$ as an outlier if and only if $\\left| e_i \\right| > \\tau \\left( \\sigma_i + C_i \\right)$.\n\nYour task is to implement the above QC test in a complete, runnable program. You must:\n- Compute $\\nabla h_i(\\hat{\\mathbf{x}})$ and $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ for $i \\in \\{1,2,3\\}$ exactly from the definitions of $h_i$ given above.\n- Use the Euclidean norm for $\\left\\| \\delta \\mathbf{x} \\right\\|_2$ and the spectral norm (largest singular value) for $\\left\\| \\nabla^2 h_i(\\hat{\\mathbf{x}}) \\right\\|_2$.\n- Construct observations $y_i$ indirectly via prescribed mismatch multipliers $a_i$ according to\n$$\ne_i = a_i \\, \\tau \\left( \\sigma_i + C_i \\right), \\quad\ny_i = h_i(\\hat{\\mathbf{x}}) + \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x} + e_i,\n$$\nso that the test outcome is controlled by the magnitude of $a_i$ relative to $1$.\n\nUse the fixed decision factor $\\tau = 3$.\n\nTest Suite:\nProvide a test suite of five cases. Each case consists of $(\\hat{\\mathbf{x}}, \\delta \\mathbf{x}, \\boldsymbol{\\sigma}, \\mathbf{a})$, where $\\hat{\\mathbf{x}} \\in \\mathbb{R}^2$, $\\delta \\mathbf{x} \\in \\mathbb{R}^2$, $\\boldsymbol{\\sigma} \\in \\mathbb{R}^3$, and $\\mathbf{a} \\in \\mathbb{R}^3$. For each case, compute the outlier flags for the three observations and return them as booleans. The test suite must be:\n- Case $1$ (happy path, small increment, no flags): $\\hat{\\mathbf{x}} = [0.1, 0.2]$, $\\delta \\mathbf{x} = [0.01, -0.02]$, $\\boldsymbol{\\sigma} = [0.05, 0.05, 0.05]$, $\\mathbf{a} = [0.5, -0.2, 0.0]$.\n- Case $2$ (curvature-driven outliers with mixed outcomes): $\\hat{\\mathbf{x}} = [2.0, 1.2]$, $\\delta \\mathbf{x} = [0.5, -0.4]$, $\\boldsymbol{\\sigma} = [0.02, 0.02, 0.02]$, $\\mathbf{a} = [1.1, 0.8, 2.0]$.\n- Case $3$ (boundary condition at threshold equality, no flags): $\\hat{\\mathbf{x}} = [-1.0, 2.0]$, $\\delta \\mathbf{x} = [0.3, 0.3]$, $\\boldsymbol{\\sigma} = [0.03, 0.03, 0.03]$, $\\mathbf{a} = [1.0, 1.0, 1.0]$.\n- Case $4$ (zero increment, pure gross error test): $\\hat{\\mathbf{x}} = [0.0, -0.5]$, $\\delta \\mathbf{x} = [0.0, 0.0]$, $\\boldsymbol{\\sigma} = [0.1, 0.1, 0.1]$, $\\mathbf{a} = [1.2, 0.3, -0.9]$.\n- Case $5$ (angles in radians near high curvature, mixed flags): $\\hat{\\mathbf{x}} = [0.0, \\pi/2]$, $\\delta \\mathbf{x} = [-0.2, 0.1]$, $\\boldsymbol{\\sigma} = [0.02, 0.02, 0.02]$, $\\mathbf{a} = [0.95, 1.05, 1.5]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three booleans in the order of observations $\\big(h_1, h_2, h_3\\big)$. For example, a valid output format is $[[\\text{False},\\text{True},\\text{False}], [\\text{True},\\text{True},\\text{False}], \\dots]$.",
            "solution": "The task is to implement a curvature-aware gross error check for a set of three scalar observations, $h_1, h_2, h_3$, which are functions of a two-dimensional state vector $\\mathbf{x} = [x_1, x_2]^\\top$. The check is designed to flag an observation $y_i$ as an outlier if its deviation from a linearized model prediction is too large, accounting for both observation noise and model nonlinearity.\n\nThe core of the Quality Control (QC) test is the inequality:\n$$\n\\left| e_i \\right| > \\tau \\left( \\sigma_i + C_i \\right)\n$$\nwhere $i \\in \\{1, 2, 3\\}$ is the observation index. The terms are defined as follows:\n- $e_i$ is the mismatch between the observed residual and its linearized prediction.\n- $\\tau = 3$ is a fixed, positive decision factor.\n- $\\sigma_i$ is the standard deviation of the observation noise for the $i$-th observation.\n- $C_i$ is the curvature bound, which estimates the maximum possible error from using a linear approximation of the observation operator $h_i$. It is defined as:\n$$\nC_i = \\frac{1}{2} \\left\\| \\nabla^2 h_i(\\hat{\\mathbf{x}}) \\right\\|_2 \\, \\left\\| \\delta \\mathbf{x} \\right\\|_2^2\n$$\nHere, $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ is the Hessian matrix of $h_i$ evaluated at $\\hat{\\mathbf{x}}$, $\\|\\cdot\\|_2$ denotes the spectral norm for matrices (the largest singular value), and $\\|\\cdot\\|_2$ denotes the Euclidean norm for vectors.\n\nTo perform this check, we first need the Hessian matrices for each observation function.\n\n1.  For $h_1(\\mathbf{x}) = \\exp(x_1) + \\sin(x_2)$:\n    $$\n    \\nabla^2 h_1(\\mathbf{x}) = \\begin{pmatrix} \\exp(x_1) & 0 \\\\ 0 & -\\sin(x_2) \\end{pmatrix}\n    $$\n\n2.  For $h_2(\\mathbf{x}) = \\log(1 + x_1^2) + x_1 x_2$:\n    $$\n    \\nabla^2 h_2(\\mathbf{x}) = \\begin{pmatrix} \\frac{2(1-x_1^2)}{(1+x_1^2)^2} & 1 \\\\ 1 & 0 \\end{pmatrix}\n    $$\n\n3.  For $h_3(\\mathbf{x}) = \\sqrt{1 + x_2^2} + \\tanh(x_1)$:\n    $$\n    \\nabla^2 h_3(\\mathbf{x}) = \\begin{pmatrix} -2\\tanh(x_1)\\text{sech}^2(x_1) & 0 \\\\ 0 & (1+x_2^2)^{-3/2} \\end{pmatrix}\n    $$\n\nThe problem statement provides a specific construction for the mismatch term $e_i$ to facilitate testing: $e_i = a_i \\, \\tau \\left( \\sigma_i + C_i \\right)$, where $a_i$ is a prescribed multiplier. Substituting this into the QC inequality simplifies the test to checking if $|a_i| > 1$.\n\nThe algorithm for each test case is as follows:\nFor each observation $i \\in \\{1, 2, 3\\}$:\n1.  Evaluate the Hessian matrix $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ at the given background state $\\hat{\\mathbf{x}}$.\n2.  Compute the spectral norm $\\|\\nabla^2 h_i(\\hat{\\mathbf{x}})\\|_2$.\n3.  Compute the squared Euclidean norm of the increment, $\\|\\delta \\mathbf{x}\\|_2^2$.\n4.  Calculate the curvature bound $C_i = \\frac{1}{2} \\|\\nabla^2 h_i(\\hat{\\mathbf{x}})\\|_2 \\|\\delta \\mathbf{x}\\|_2^2$.\n5.  Using the given multiplier $a_i$ and standard deviation $\\sigma_i$, calculate the mismatch $e_i = a_i \\tau (\\sigma_i + C_i)$.\n6.  Calculate the flagging threshold $T_i = \\tau (\\sigma_i + C_i)$.\n7.  The flag is set to `True` if $|e_i| > T_i$, and `False` otherwise.\nThis procedure is implemented in the provided code and repeated for all five test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a curvature-aware gross error check for a set of nonlinear\n    observation operators.\n    \"\"\"\n\n    # Helper function for hyperbolic secant, as it's not a direct numpy function.\n    def sech(x):\n        return 1.0 / np.cosh(x)\n\n    # --- Hessian Matrix Definitions ---\n    # These functions compute the Hessian matrix for each observation operator h_i.\n\n    def hessian_h1(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the Hessian of h_1(x) = exp(x_1) + sin(x_2).\"\"\"\n        x1, x2 = x\n        return np.array([\n            [np.exp(x1), 0.0],\n            [0.0, -np.sin(x2)]\n        ])\n\n    def hessian_h2(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the Hessian of h_2(x) = log(1 + x_1^2) + x_1 * x_2.\"\"\"\n        x1, x2 = x\n        term_11 = (2.0 * (1.0 - x1**2)) / (1.0 + x1**2)**2\n        return np.array([\n            [term_11, 1.0],\n            [1.0, 0.0]\n        ])\n\n    def hessian_h3(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the Hessian of h_3(x) = sqrt(1 + x_2^2) + tanh(x_1).\"\"\"\n        x1, x2 = x\n        term_11 = -2.0 * np.tanh(x1) * sech(x1)**2\n        term_22 = (1.0 + x2**2)**(-1.5)\n        return np.array([\n            [term_11, 0.0],\n            [0.0, term_22]\n        ])\n\n    # Fixed decision factor\n    tau = 3.0\n\n    # List of Hessian functions for easy iteration\n    hessian_functions = [hessian_h1, hessian_h2, hessian_h3]\n    \n    # Test suite from the problem statement\n    test_cases = [\n        # Case 1 (happy path, small increment, no flags)\n        ([0.1, 0.2], [0.01, -0.02], [0.05, 0.05, 0.05], [0.5, -0.2, 0.0]),\n        # Case 2 (curvature-driven outliers with mixed outcomes)\n        ([2.0, 1.2], [0.5, -0.4], [0.02, 0.02, 0.02], [1.1, 0.8, 2.0]),\n        # Case 3 (boundary condition at threshold equality, no flags)\n        ([-1.0, 2.0], [0.3, 0.3], [0.03, 0.03, 0.03], [1.0, 1.0, 1.0]),\n        # Case 4 (zero increment, pure gross error test)\n        ([0.0, -0.5], [0.0, 0.0], [0.1, 0.1, 0.1], [1.2, 0.3, -0.9]),\n        # Case 5 (angles in radians near high curvature, mixed flags)\n        ([0.0, np.pi/2], [-0.2, 0.1], [0.02, 0.02, 0.02], [0.95, 1.05, 1.5]),\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        x_hat_list, dx_list, sigma_list, a_list = case\n        x_hat = np.array(x_hat_list)\n        dx = np.array(dx_list)\n        sigma = np.array(sigma_list)\n        a = np.array(a_list)\n        \n        case_flags = []\n        \n        # Pre-compute the squared norm of the increment vector\n        norm_dx_sq = np.linalg.norm(dx)**2\n        \n        for i in range(3):\n            # 1. Compute the Hessian matrix and its spectral norm\n            H_i = hessian_functions[i](x_hat)\n            # The spectral norm is the largest singular value (ord=2)\n            norm_H_i = np.linalg.norm(H_i, ord=2)\n            \n            # 2. Compute the curvature bound C_i\n            C_i = 0.5 * norm_H_i * norm_dx_sq\n            \n            # 3. Compute the mismatch e_i from its definition in the problem\n            e_i = a[i] * tau * (sigma[i] + C_i)\n            \n            # 4. Compute the flagging threshold\n            threshold = tau * (sigma[i] + C_i)\n            \n            # 5. Apply the QC test: |e_i| > threshold\n            is_outlier = np.abs(e_i) > threshold\n            case_flags.append(bool(is_outlier))\n            \n        all_results.append(case_flags)\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str.replace(\" \", \"\").replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n```"
        }
    ]
}