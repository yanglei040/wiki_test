{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of observation quality control is the gross error check, which statistically evaluates whether an observation is consistent with our prior knowledge. This practice guides you through the fundamental chi-square test, where you will calculate the expected variance of the innovation (the difference between the observation and the model background) and use it to normalize the observed residual. By comparing this normalized value to a statistical threshold, you will make a quantitative decision to accept or reject an observation, a critical first step in any data assimilation system .",
            "id": "3406849",
            "problem": "Consider a single-observation quality control task in a variational data assimilation system. Let the model state be of dimension $n=3$, and let the observation operator be differentiable and linearized at the background state $x_b$ with Jacobian (row) vector $H \\in \\mathbb{R}^{1 \\times 3}$. The background error $x - x_b$ is modeled as a zero-mean random vector with covariance matrix $B \\in \\mathbb{R}^{3 \\times 3}$. The observation error is the sum of an instrument error and a representation error, each modeled as zero-mean, mutually independent random variables, and independent of the background error. The innovation (observation-minus-background in observation space) is $d = y - \\mathcal{H}(x_b)$, where $\\mathcal{H}$ is the nonlinear observation operator and $H$ is its Jacobian at $x_b$. Use only the linearized error-propagation rule for variances and the independence assumptions to derive the scalar innovation variance $S = \\operatorname{Var}(d)$ and then compute a chi-square gross error test statistic $z = d^2 / S$.\n\nYou are given:\n- The linearized observation operator Jacobian as $H = \\begin{bmatrix} 0.5  -1.2  0.3 \\end{bmatrix}$.\n- The background error covariance\n$$\nB = \\begin{bmatrix}\n1.0  0.2  -0.1 \\\\\n0.2  2.0  0.4 \\\\\n-0.1  0.4  1.5\n\\end{bmatrix}.\n$$\n- The instrument error variance $R = 0.49$.\n- The representation error variance $R_{\\text{rep}} = 0.36$.\n- The realized innovation $d = 5.0$.\n- A prescribed scalar chi-square threshold $\\gamma = 6.635$ for one degree of freedom.\n\nStarting from the stated modeling assumptions and the linearization, first derive the scalar innovation variance $S$ and then compute the statistic $z = d^2 / S$. State whether the residual is consistent with the assumed error statistics by comparing $z$ to $\\gamma$ in your reasoning. Report only the value of $z$ as your final answer. Round your numerical answer to four significant figures. No units are required.",
            "solution": "The problem requires the computation of a chi-square gross error test statistic for a single observation in a data assimilation context. The statistic is defined as $z = d^2 / S$, where $d$ is the innovation and $S$ is the innovation variance.\n\nFirst, we must derive the expression for the innovation variance, $S$. The innovation is the difference between the observation $y$ and the model's forecast of the observation, $\\mathcal{H}(x_b)$.\n$$d = y - \\mathcal{H}(x_b)$$\nThe observation $y$ is related to the true state $x_t$ through the observation operator and an observation error $\\epsilon_o$:\n$$y = \\mathcal{H}(x_t) + \\epsilon_o$$\nThe background state $x_b$ is an estimate of the true state $x_t$. We define the background error as $\\epsilon_b = x_b - x_t$, which implies $x_t - x_b = -\\epsilon_b$.\n\nSubstituting the expression for $y$ into the definition of $d$:\n$$d = (\\mathcal{H}(x_t) + \\epsilon_o) - \\mathcal{H}(x_b)$$\nWe linearize the operator $\\mathcal{H}$ about the background state $x_b$ using a first-order Taylor expansion:\n$$\\mathcal{H}(x_t) \\approx \\mathcal{H}(x_b) + H(x_t - x_b)$$\nwhere $H$ is the Jacobian of $\\mathcal{H}$ evaluated at $x_b$. Substituting this approximation into the expression for $d$:\n$$d \\approx (\\mathcal{H}(x_b) + H(x_t - x_b) + \\epsilon_o) - \\mathcal{H}(x_b) = H(x_t - x_b) + \\epsilon_o = -H\\epsilon_b + \\epsilon_o$$\nThe innovation variance $S$ is the expected value of the squared innovation, $S = \\operatorname{Var}(d) = E[d^2]$, since both $\\epsilon_b$ and $\\epsilon_o$ are stated to be zero-mean, which implies $E[d]=0$.\n$$S = E[(-H\\epsilon_b + \\epsilon_o)(-H\\epsilon_b + \\epsilon_o)^T]$$\nNote that for a scalar innovation, the transpose is redundant but is included for correctness in the general case. Expanding the expression:\n$$S = E[H\\epsilon_b\\epsilon_b^T H^T - H\\epsilon_b\\epsilon_o^T - \\epsilon_o\\epsilon_b^T H^T + \\epsilon_o\\epsilon_o^T]$$\nUsing the linearity of the expectation operator:\n$$S = H E[\\epsilon_b\\epsilon_b^T] H^T - H E[\\epsilon_b\\epsilon_o^T] - E[\\epsilon_o\\epsilon_b^T] H^T + E[\\epsilon_o\\epsilon_o^T]$$\nThe problem states that the background error and the observation error are independent. As they are also zero-mean, their covariance is zero: $E[\\epsilon_b\\epsilon_o^T] = E[\\epsilon_b]E[\\epsilon_o^T] = 0$. The cross-terms vanish.\n$$S = H E[\\epsilon_b\\epsilon_b^T] H^T + E[\\epsilon_o\\epsilon_o^T]$$\nBy definition, the background error covariance matrix is $B = E[\\epsilon_b\\epsilon_b^T]$. The total observation error variance for a scalar observation is $R_o = E[\\epsilon_o\\epsilon_o^T]$. Thus, the innovation variance is:\n$$S = HBH^T + R_o$$\nThe total observation error $\\epsilon_o$ is the sum of an instrument error $\\epsilon_{inst}$ and a representation error $\\epsilon_{rep}$. These are stated to be zero-mean and mutually independent. Therefore, the total observation error variance $R_o$ is the sum of the individual variances:\n$$R_o = \\operatorname{Var}(\\epsilon_{inst}) + \\operatorname{Var}(\\epsilon_{rep}) = R + R_{\\text{rep}}$$\nThe final formula for the innovation variance is:\n$$S = HBH^T + R + R_{\\text{rep}}$$\nNow, we substitute the given values into this formula.\nGiven:\n$H = \\begin{bmatrix} 0.5  -1.2  0.3 \\end{bmatrix}$\n$B = \\begin{bmatrix} 1.0  0.2  -0.1 \\\\ 0.2  2.0  0.4 \\\\ -0.1  0.4  1.5 \\end{bmatrix}$\n$R = 0.49$\n$R_{\\text{rep}} = 0.36$\n$d = 5.0$\n\nFirst, compute the term $HBH^T$.\n$$H B = \\begin{bmatrix} 0.5  -1.2  0.3 \\end{bmatrix} \\begin{bmatrix} 1.0  0.2  -0.1 \\\\ 0.2  2.0  0.4 \\\\ -0.1  0.4  1.5 \\end{bmatrix}$$\n$$H B = \\begin{bmatrix} (0.5)(1.0) + (-1.2)(0.2) + (0.3)(-0.1)  (0.5)(0.2) + (-1.2)(2.0) + (0.3)(0.4)  (0.5)(-0.1) + (-1.2)(0.4) + (0.3)(1.5) \\end{bmatrix}$$\n$$H B = \\begin{bmatrix} 0.5 - 0.24 - 0.03  0.1 - 2.4 + 0.12  -0.05 - 0.48 + 0.45 \\end{bmatrix}$$\n$$H B = \\begin{bmatrix} 0.23  -2.18  -0.08 \\end{bmatrix}$$\nNext, we compute $HBH^T = (HB)H^T$:\n$$HBH^T = \\begin{bmatrix} 0.23  -2.18  -0.08 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ -1.2 \\\\ 0.3 \\end{bmatrix}$$\n$$HBH^T = (0.23)(0.5) + (-2.18)(-1.2) + (-0.08)(0.3)$$\n$$HBH^T = 0.115 + 2.616 - 0.024 = 2.707$$\nNow, compute the total observation error variance $R_o$:\n$$R_o = R + R_{\\text{rep}} = 0.49 + 0.36 = 0.85$$\nThe total innovation variance $S$ is:\n$$S = HBH^T + R_o = 2.707 + 0.85 = 3.557$$\nFinally, we compute the chi-square statistic $z$:\n$$z = \\frac{d^2}{S} = \\frac{(5.0)^2}{3.557} = \\frac{25}{3.557} \\approx 7.028394436...$$\nRounding to four significant figures, we get $z = 7.028$.\n\nThe problem asks to state whether the residual is consistent with the assumed error statistics by comparing $z$ to the threshold $\\gamma$.\nThe computed statistic is $z \\approx 7.028$.\nThe prescribed threshold is $\\gamma = 6.635$.\nSince $z > \\gamma$, the null hypothesis that the observation is consistent with the background and observation error statistics is rejected at the significance level corresponding to this threshold (which is $1\\%$ for a $\\chi^2$ distribution with one degree of freedom). Therefore, the innovation is considered statistically too large, and the observation would be flagged as a gross error. The residual is not consistent with the assumed error statistics. The final answer required is only the value of $z$.",
            "answer": "$$\\boxed{7.028}$$"
        },
        {
            "introduction": "While simple gross error checks are effective at flagging outliers, outright rejection of data can be suboptimal. Robust statistical methods offer a more nuanced approach by down-weighting, rather than discarding, suspicious observations. This exercise introduces the widely-used Huber weighting scheme, allowing you to calculate weights based on residual magnitudes and perform a re-weighted least-squares update, demonstrating how the influence of outliers can be gracefully mitigated in the estimation process .",
            "id": "3406858",
            "problem": "In a one-dimensional variational data assimilation step, consider $n=6$ independent sensors observing the same scalar state $x$ through a linear observational operator $\\mathcal{H}_i(x)=h_i x$. At iteration $t$, the current estimate is $x^{(t)}=1.0$, and the scalar residuals $r_i=y_i-\\mathcal{H}_i(x^{(t)})$ are available for quality control. To perform a robust gross-error check using the Huber weighting scheme, use the following foundational definitions:\n- Residuals are defined as $r_i=y_i-\\mathcal{H}_i(x^{(t)})$.\n- The robust scale $\\sigma$ of the residuals is provided.\n- The Huber threshold is $\\delta=k\\sigma$ for a chosen parameter $k$.\n- The Huber weights $w_i$ follow the standard M-estimation rule $w_i=\\psi(r_i)/r_i$ with the Huber score function $\\psi(r)=r$ if $|r|\\le \\delta$ and $\\psi(r)=\\delta\\operatorname{sign}(r)$ if $|r|\\delta$, resulting in $w_i=1$ for $|r_i|\\le \\delta$ and $w_i=\\delta/|r_i|$ for $|r_i|\\delta$.\n- The Weighted Least Squares (WLS) update solves $x^{(t+1)}=\\arg\\min_x \\sum_{i=1}^{n} w_i \\left(y_i-\\mathcal{H}_i(x)\\right)^2$.\n\nSuppose the sensor sensitivities are $(h_1,h_2,h_3,h_4,h_5,h_6)=(1.0,0.8,1.2,0.6,1.5,0.7)$, the residuals at iteration $t$ are $(r_1,r_2,r_3,r_4,r_5,r_6)=(0.2,-0.1,0.5,-0.4,4.0,-3.5)$, and the robust scale is $\\sigma=0.3$. Choose $k=2$, so that $\\delta=k\\sigma$. Use the provided $r_i$, $\\sigma$, $k$, and $h_i$ to:\n\n1. Compute the Huber weights $w_i$.\n2. Recover the observations $y_i$ from $r_i=y_i-h_i x^{(t)}$.\n3. Perform one WLS update to obtain $x^{(t+1)}$.\n\nReport the final $x^{(t+1)}$ as a dimensionless number, rounded to four significant figures.",
            "solution": "The solution follows the three steps defined in the problem: compute the Huber weights based on the residuals, recover the original observations, and perform a Weighted Least Squares (WLS) update to find the new state estimate $x^{(t+1)}$.\n\n**1. Compute the Huber weights $w_i$.**\n\nFirst, the Huber threshold $\\delta$ is calculated from the given robust scale $\\sigma=0.3$ and parameter $k=2$.\n$$\n\\delta = k \\sigma = 2 \\times 0.3 = 0.6\n$$\nNext, we compare the absolute value of each residual $|r_i|$ to this threshold $\\delta=0.6$ to determine the corresponding weight $w_i$.\n- For $i=1$: $|r_1| = |0.2| = 0.2 \\le 0.6$, so $w_1 = 1$.\n- For $i=2$: $|r_2| = |-0.1| = 0.1 \\le 0.6$, so $w_2 = 1$.\n- For $i=3$: $|r_3| = |0.5| = 0.5 \\le 0.6$, so $w_3 = 1$.\n- For $i=4$: $|r_4| = |-0.4| = 0.4 \\le 0.6$, so $w_4 = 1$.\n- For $i=5$: $|r_5| = |4.0| = 4.0  0.6$, so $w_5 = \\frac{\\delta}{|r_5|} = \\frac{0.6}{4.0} = 0.15$.\n- For $i=6$: $|r_6| = |-3.5| = 3.5  0.6$, so $w_6 = \\frac{\\delta}{|r_6|} = \\frac{0.6}{3.5} \\approx 0.1714$. We use the exact fraction $w_6 = 6/35$.\n\nThe vector of Huber weights is $(w_1, w_2, w_3, w_4, w_5, w_6) = (1, 1, 1, 1, 0.15, 6/35)$.\n\n**2. Recover the observations $y_i$.**\n\nThe observations $y_i$ are recovered from the definition of the residuals: $y_i = r_i + \\mathcal{H}_i(x^{(t)})$. Given $\\mathcal{H}_i(x) = h_i x$ and $x^{(t)}=1.0$:\n- $y_1 = 0.2 + 1.0 \\times 1.0 = 1.2$.\n- $y_2 = -0.1 + 0.8 \\times 1.0 = 0.7$.\n- $y_3 = 0.5 + 1.2 \\times 1.0 = 1.7$.\n- $y_4 = -0.4 + 0.6 \\times 1.0 = 0.2$.\n- $y_5 = 4.0 + 1.5 \\times 1.0 = 5.5$.\n- $y_6 = -3.5 + 0.7 \\times 1.0 = -2.8$.\n\nThe vector of observations is $(y_1, y_2, y_3, y_4, y_5, y_6) = (1.2, 0.7, 1.7, 0.2, 5.5, -2.8)$.\n\n**3. Perform one WLS update to obtain $x^{(t+1)}$.**\n\nThe updated state estimate $x^{(t+1)}$ is the value of $x$ that minimizes the Weighted Least Squares cost function $J(x) = \\sum_{i=1}^{n} w_i (y_i - h_i x)^2$. Taking the derivative with respect to $x$ and setting it to zero yields the standard normal equation solution for a scalar state:\n$$\nx^{(t+1)} = \\frac{\\sum_{i=1}^{n} w_i h_i y_i}{\\sum_{i=1}^{n} w_i h_i^2}\n$$\nWe compute the numerator and denominator separately.\n\nDenominator: $\\sum_{i=1}^{6} w_i h_i^2$\n- $w_1 h_1^2 = 1 \\cdot (1.0)^2 = 1.0$\n- $w_2 h_2^2 = 1 \\cdot (0.8)^2 = 0.64$\n- $w_3 h_3^2 = 1 \\cdot (1.2)^2 = 1.44$\n- $w_4 h_4^2 = 1 \\cdot (0.6)^2 = 0.36$\n- $w_5 h_5^2 = 0.15 \\cdot (1.5)^2 = 0.15 \\cdot 2.25 = 0.3375$\n- $w_6 h_6^2 = (6/35) \\cdot (0.7)^2 = (6/35) \\cdot 0.49 = 0.084$\nDenominator Sum = $1.0 + 0.64 + 1.44 + 0.36 + 0.3375 + 0.084 = 3.8615$.\n\nNumerator: $\\sum_{i=1}^{6} w_i h_i y_i$\n- $w_1 h_1 y_1 = 1 \\cdot 1.0 \\cdot 1.2 = 1.2$\n- $w_2 h_2 y_2 = 1 \\cdot 0.8 \\cdot 0.7 = 0.56$\n- $w_3 h_3 y_3 = 1 \\cdot 1.2 \\cdot 1.7 = 2.04$\n- $w_4 h_4 y_4 = 1 \\cdot 0.6 \\cdot 0.2 = 0.12$\n- $w_5 h_5 y_5 = 0.15 \\cdot 1.5 \\cdot 5.5 = 1.2375$\n- $w_6 h_6 y_6 = (6/35) \\cdot 0.7 \\cdot (-2.8) = -0.336$\nNumerator Sum = $1.2 + 0.56 + 2.04 + 0.12 + 1.2375 - 0.336 = 4.8215$.\n\nFinally, we calculate $x^{(t+1)}$:\n$$\nx^{(t+1)} = \\frac{4.8215}{3.8615} \\approx 1.24860805...\n$$\nRounding to four significant figures gives $x^{(t+1)} \\approx 1.249$.",
            "answer": "$$\n\\boxed{1.249}\n$$"
        },
        {
            "introduction": "Many real-world systems involve nonlinear relationships between the state and the observations, a reality that can challenge simple linear quality control checks. When model curvature is significant, a large residual may be due to nonlinearity rather than a true gross error. This practice dives into this advanced topic, tasking you with implementing a curvature-aware QC check that explicitly accounts for second-order effects, providing a more reliable method for identifying outliers in complex, nonlinear models .",
            "id": "3406926",
            "problem": "Consider an observation operator mapping a state vector to scalar observations in the setting of inverse problems and data assimilation. Let the state be $\\mathbf{x} \\in \\mathbb{R}^2$ and define three scalar observation functions $h_1, h_2, h_3$ such that the full observation operator is $H(\\mathbf{x}) = \\big(h_1(\\mathbf{x}), h_2(\\mathbf{x}), h_3(\\mathbf{x})\\big) \\in \\mathbb{R}^3$, with\n- $h_1(\\mathbf{x}) = \\exp(x_1) + \\sin(x_2)$,\n- $h_2(\\mathbf{x}) = \\log\\!\\big(1 + x_1^2\\big) + x_1 x_2$,\n- $h_3(\\mathbf{x}) = \\sqrt{1 + x_2^2} + \\tanh(x_1)$.\n\nAll trigonometric functions must be evaluated with angles in radians.\n\nFor a given background (prior) state $\\hat{\\mathbf{x}} \\in \\mathbb{R}^2$ and an increment $\\delta \\mathbf{x} \\in \\mathbb{R}^2$, define the linearized predictor for each scalar observation $h_i$ as $h_i(\\hat{\\mathbf{x}}) + \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x}$. Let the residual for observation $i$ be $r_i = y_i - h_i(\\hat{\\mathbf{x}})$, where $y_i$ is the observed value. The mismatch between the observed residual and the linearized prediction is then $e_i = r_i - \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x}$.\n\nObservation Quality Control (QC) aims to flag outliers due to model nonlinearity (curvature) and gross errors. We consider a curvature-aware gross error check based on the second-order Taylor remainder bound. For each observation $i$, let $C_i$ denote a curvature bound given by\n$$\nC_i = \\frac{1}{2} \\left\\| \\nabla^2 h_i(\\hat{\\mathbf{x}}) \\right\\|_2 \\, \\left\\| \\delta \\mathbf{x} \\right\\|_2^2,\n$$\nwhere $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ is the Hessian of $h_i$ at $\\hat{\\mathbf{x}}$ and $\\|\\cdot\\|_2$ denotes the spectral norm for matrices and the Euclidean norm for vectors. Let $\\sigma_i  0$ be the observation noise standard deviation for observation $i$, and let $\\tau  0$ be a fixed decision factor.\n\nDefine the curvature-aware gross error flag for observation $i$ by the rule:\n- Flag observation $i$ as an outlier if and only if $\\left| e_i \\right|  \\tau \\left( \\sigma_i + C_i \\right)$.\n\nYour task is to implement the above QC test in a complete, runnable program. You must:\n- Compute $\\nabla h_i(\\hat{\\mathbf{x}})$ and $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ for $i \\in \\{1,2,3\\}$ exactly from the definitions of $h_i$ given above.\n- Use the Euclidean norm for $\\left\\| \\delta \\mathbf{x} \\right\\|_2$ and the spectral norm (largest singular value) for $\\left\\| \\nabla^2 h_i(\\hat{\\mathbf{x}}) \\right\\|_2$.\n- Construct observations $y_i$ indirectly via prescribed mismatch multipliers $a_i$ according to\n$$\ne_i = a_i \\, \\tau \\left( \\sigma_i + C_i \\right), \\quad\ny_i = h_i(\\hat{\\mathbf{x}}) + \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x} + e_i,\n$$\nso that the test outcome is controlled by the magnitude of $a_i$ relative to $1$.\n\nUse the fixed decision factor $\\tau = 3$.\n\nTest Suite:\nProvide a test suite of five cases. Each case consists of $(\\hat{\\mathbf{x}}, \\delta \\mathbf{x}, \\boldsymbol{\\sigma}, \\mathbf{a})$, where $\\hat{\\mathbf{x}} \\in \\mathbb{R}^2$, $\\delta \\mathbf{x} \\in \\mathbb{R}^2$, $\\boldsymbol{\\sigma} \\in \\mathbb{R}^3$, and $\\mathbf{a} \\in \\mathbb{R}^3$. For each case, compute the outlier flags for the three observations and return them as booleans. The test suite must be:\n- Case $1$ (happy path, small increment, no flags): $\\hat{\\mathbf{x}} = [0.1, 0.2]$, $\\delta \\mathbf{x} = [0.01, -0.02]$, $\\boldsymbol{\\sigma} = [0.05, 0.05, 0.05]$, $\\mathbf{a} = [0.5, -0.2, 0.0]$.\n- Case $2$ (curvature-driven outliers with mixed outcomes): $\\hat{\\mathbf{x}} = [2.0, 1.2]$, $\\delta \\mathbf{x} = [0.5, -0.4]$, $\\boldsymbol{\\sigma} = [0.02, 0.02, 0.02]$, $\\mathbf{a} = [1.1, 0.8, 2.0]$.\n- Case $3$ (boundary condition at threshold equality, no flags): $\\hat{\\mathbf{x}} = [-1.0, 2.0]$, $\\delta \\mathbf{x} = [0.3, 0.3]$, $\\boldsymbol{\\sigma} = [0.03, 0.03, 0.03]$, $\\mathbf{a} = [1.0, 1.0, 1.0]$.\n- Case $4$ (zero increment, pure gross error test): $\\hat{\\mathbf{x}} = [0.0, -0.5]$, $\\delta \\mathbf{x} = [0.0, 0.0]$, $\\boldsymbol{\\sigma} = [0.1, 0.1, 0.1]$, $\\mathbf{a} = [1.2, 0.3, -0.9]$.\n- Case $5$ (angles in radians near high curvature, mixed flags): $\\hat{\\mathbf{x}} = [0.0, \\pi/2]$, $\\delta \\mathbf{x} = [-0.2, 0.1]$, $\\boldsymbol{\\sigma} = [0.02, 0.02, 0.02]$, $\\mathbf{a} = [0.95, 1.05, 1.5]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three booleans in the order of observations $\\big(h_1, h_2, h_3\\big)$. For example, a valid output format is $[[\\text{False},\\text{True},\\text{False}], [\\text{True},\\text{True},\\text{False}], \\dots]$.",
            "solution": "The task is to implement a curvature-aware gross error check for a set of three scalar observations, $h_1, h_2, h_3$, which are functions of a two-dimensional state vector $\\mathbf{x} = [x_1, x_2]^\\top$. The check is designed to flag an observation $y_i$ as an outlier if its deviation from a linearized model prediction is too large, accounting for both observation noise and model nonlinearity.\n\nThe core of the Quality Control (QC) test is the inequality:\n$$\n\\left| e_i \\right|  \\tau \\left( \\sigma_i + C_i \\right)\n$$\nwhere $i \\in \\{1, 2, 3\\}$ is the observation index. Let us define the terms involved.\n- $e_i$ is the mismatch between the observed residual and its linearized prediction. It is defined as $e_i = (y_i - h_i(\\hat{\\mathbf{x}})) - \\nabla h_i(\\hat{\\mathbf{x}})^\\top \\delta \\mathbf{x}$, where $\\hat{\\mathbf{x}}$ is the background state and $\\delta \\mathbf{x}$ is the analysis increment.\n- $\\tau = 3$ is a fixed, positive decision factor.\n- $\\sigma_i$ is the standard deviation of the observation noise for the $i$-th observation.\n- $C_i$ is the curvature bound, which estimates the maximum possible error from using a linear approximation of the observation operator $h_i$. It is defined as:\n$$\nC_i = \\frac{1}{2} \\left\\| \\nabla^2 h_i(\\hat{\\mathbf{x}}) \\right\\|_2 \\, \\left\\| \\delta \\mathbf{x} \\right\\|_2^2\n$$\nHere, $\\nabla^2 h_i(\\hat{\\mathbf{x}})$ is the Hessian matrix of $h_i$ evaluated at $\\hat{\\mathbf{x}}$, $\\|\\cdot\\|_2$ denotes the spectral norm for matrices (the largest singular value), and $\\|\\cdot\\|_2$ denotes the Euclidean norm for vectors.\n\nTo perform this check, we must first compute the Hessian matrices for each observation function. The state vector is $\\mathbf{x} = [x_1, x_2]^\\top$.\n\n1.  For $h_1(\\mathbf{x}) = \\exp(x_1) + \\sin(x_2)$:\n    The Hessian matrix is:\n    $$\n    \\nabla^2 h_1(\\mathbf{x}) = \\begin{pmatrix} \\exp(x_1)  0 \\\\ 0  -\\sin(x_2) \\end{pmatrix}\n    $$\n\n2.  For $h_2(\\mathbf{x}) = \\log(1 + x_1^2) + x_1 x_2$:\n    The Hessian matrix is:\n    $$\n    \\nabla^2 h_2(\\mathbf{x}) = \\begin{pmatrix} \\frac{2(1-x_1^2)}{(1+x_1^2)^2}  1 \\\\ 1  0 \\end{pmatrix}\n    $$\n\n3.  For $h_3(\\mathbf{x}) = \\sqrt{1 + x_2^2} + \\tanh(x_1)$:\n    The Hessian matrix is:\n    $$\n    \\nabla^2 h_3(\\mathbf{x}) = \\begin{pmatrix} -2\\tanh(x_1)\\text{sech}^2(x_1)  0 \\\\ 0  (1+x_2^2)^{-3/2} \\end{pmatrix}\n    $$\n\nThe problem statement provides a specific construction for the mismatch term $e_i$ to facilitate testing:\n$$\ne_i = a_i \\, \\tau \\left( \\sigma_i + C_i \\right)\n$$\nwhere $a_i$ is a prescribed mismatch multiplier. Substituting this definition into the QC inequality gives:\n$$\n\\left| a_i \\, \\tau \\left( \\sigma_i + C_i \\right) \\right|  \\tau \\left( \\sigma_i + C_i \\right)\n$$\nSince $\\tau  0$, $\\sigma_i  0$, and $C_i \\ge 0$, the term $\\tau(\\sigma_i + C_i)$ is strictly positive. Therefore, we can simplify the inequality:\n$$\n|a_i| \\, \\tau \\left( \\sigma_i + C_i \\right)  \\tau \\left( \\sigma_i + C_i \\right)\n$$\n$$\n|a_i|  1\n$$\nThis reveals that the outcome of the QC test is determined solely by whether the absolute value of the multiplier $a_i$ is strictly greater than $1$.\n\nThe program implementation will follow the full calculation chain as specified. For each test case and each observation:\n1.  Evaluate the Hessian matrix $\\nabla^2 h_i(\\hat{\\mathbf{x}})$.\n2.  Compute its spectral norm $\\|\\nabla^2 h_i(\\hat{\\mathbf{x}})\\|_2$.\n3.  Compute the curvature bound $C_i = \\frac{1}{2} \\|\\nabla^2 h_i(\\hat{\\mathbf{x}})\\|_2 \\|\\delta \\mathbf{x}\\|_2^2$.\n4.  Calculate the mismatch $e_i = a_i \\tau (\\sigma_i + C_i)$.\n5.  Calculate the flagging threshold $T_i = \\tau (\\sigma_i + C_i)$.\n6.  The flag is `True` if $|e_i| > T_i$, and `False` otherwise.\nThis procedure is repeated for all five test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a curvature-aware gross error check for a set of nonlinear\n    observation operators.\n    \"\"\"\n\n    # Helper function for hyperbolic secant, as it's not a direct numpy function.\n    def sech(x):\n        return 1.0 / np.cosh(x)\n\n    # --- Hessian Matrix Definitions ---\n    # These functions compute the Hessian matrix for each observation operator h_i.\n\n    def hessian_h1(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the Hessian of h_1(x) = exp(x_1) + sin(x_2).\"\"\"\n        x1, x2 = x\n        return np.array([\n            [np.exp(x1), 0.0],\n            [0.0, -np.sin(x2)]\n        ])\n\n    def hessian_h2(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the Hessian of h_2(x) = log(1 + x_1^2) + x_1 * x_2.\"\"\"\n        x1, x2 = x\n        term_11 = (2.0 * (1.0 - x1**2)) / (1.0 + x1**2)**2\n        return np.array([\n            [term_11, 1.0],\n            [1.0, 0.0]\n        ])\n\n    def hessian_h3(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the Hessian of h_3(x) = sqrt(1 + x_2^2) + tanh(x_1).\"\"\"\n        x1, x2 = x\n        term_11 = -2.0 * np.tanh(x1) * sech(x1)**2\n        term_22 = (1.0 + x2**2)**(-1.5)\n        return np.array([\n            [term_11, 0.0],\n            [0.0, term_22]\n        ])\n\n    # Fixed decision factor\n    tau = 3.0\n\n    # List of Hessian functions for easy iteration\n    hessian_functions = [hessian_h1, hessian_h2, hessian_h3]\n    \n    # Test suite from the problem statement\n    test_cases = [\n        # Case 1 (happy path, small increment, no flags)\n        ([0.1, 0.2], [0.01, -0.02], [0.05, 0.05, 0.05], [0.5, -0.2, 0.0]),\n        # Case 2 (curvature-driven outliers with mixed outcomes)\n        ([2.0, 1.2], [0.5, -0.4], [0.02, 0.02, 0.02], [1.1, 0.8, 2.0]),\n        # Case 3 (boundary condition at threshold equality, no flags)\n        ([-1.0, 2.0], [0.3, 0.3], [0.03, 0.03, 0.03], [1.0, 1.0, 1.0]),\n        # Case 4 (zero increment, pure gross error test)\n        ([0.0, -0.5], [0.0, 0.0], [0.1, 0.1, 0.1], [1.2, 0.3, -0.9]),\n        # Case 5 (angles in radians near high curvature, mixed flags)\n        ([0.0, np.pi/2], [-0.2, 0.1], [0.02, 0.02, 0.02], [0.95, 1.05, 1.5]),\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        x_hat_list, dx_list, sigma_list, a_list = case\n        x_hat = np.array(x_hat_list)\n        dx = np.array(dx_list)\n        sigma = np.array(sigma_list)\n        a = np.array(a_list)\n        \n        case_flags = []\n        \n        # Pre-compute the squared norm of the increment vector\n        norm_dx_sq = np.linalg.norm(dx)**2\n        \n        for i in range(3):\n            # 1. Compute the Hessian matrix and its spectral norm\n            H_i = hessian_functions[i](x_hat)\n            # The spectral norm is the largest singular value (ord=2)\n            norm_H_i = np.linalg.norm(H_i, ord=2)\n            \n            # 2. Compute the curvature bound C_i\n            C_i = 0.5 * norm_H_i * norm_dx_sq\n            \n            # 3. Compute the mismatch e_i from its definition in the problem\n            e_i = a[i] * tau * (sigma[i] + C_i)\n            \n            # 4. Compute the flagging threshold\n            threshold = tau * (sigma[i] + C_i)\n            \n            # 5. Apply the QC test: |e_i| > threshold\n            # Note: This is logically equivalent to |a[i]| > 1\n            is_outlier = np.abs(e_i) > threshold\n            case_flags.append(bool(is_outlier))\n            \n        all_results.append(case_flags)\n\n    # Format the final output string as specified\n    # The replacement of ' ' with '' ensures no spaces in the final list format\n    # and 'True'/'False' are capitalized as is standard for Python booleans to string.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}