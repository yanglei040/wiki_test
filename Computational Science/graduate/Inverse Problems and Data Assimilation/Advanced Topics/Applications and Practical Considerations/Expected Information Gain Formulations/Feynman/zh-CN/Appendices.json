{
    "hands_on_practices": [
        {
            "introduction": "在设计实验时，“最优”的真正含义是什么？本练习探讨了两种常见的最优性准则之间的根本性冲突。通过一个简单的线性高斯模型示例，您将看到最大化预期信息增益（EIG）——旨在缩小后验不确定性的整体体积——如何可能导致与最小化参数平均后验方差（A-最优性）的实验设计产生不同选择 。这个练习对于理解最优性准则的选择并非中立至关重要；它反映了关于如何减少不确定性的特定偏好。",
            "id": "3380335",
            "problem": "考虑一个二维线性高斯逆问题。设未知状态为 $x \\in \\mathbb{R}^{2}$，其高斯先验为 $x \\sim \\mathcal{N}(0, C_{\\text{prior}})$，其中 $C_{\\text{prior}} = I_{2}$ 是 $2 \\times 2$ 的单位矩阵。观测值由线性正向模型 $y = H x + \\varepsilon$ 给出，其中 $H = I_{2}$，观测噪声为高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, R)$，其中 $R$ 是一个正定的 $2 \\times 2$ 协方差矩阵。考虑两种备选的实验设计：\n- 设计 $\\mathcal{A}$：$R_{\\mathcal{A}} = \\operatorname{diag}(1, 1)$。\n- 设计 $\\mathcal{B}$：$R_{\\mathcal{B}} = \\operatorname{diag}(0.01, 1000)$。\n\n你将使用贝叶斯线性高斯更新和Kullback-Leibler散度（KLD）的定义来计算后验协方差和期望信息增益（EIG）。期望信息增益（EIG）定义为在该设计下 $x$ 和 $y$ 之间的互信息，等价于在由该设计引出的 $y$ 的边际分布上，从先验到后验的期望KLD。\n\n任务：\n1. 根据线性高斯贝叶斯推断的基本原理，推导每种设计 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的后验协方差 $C_{\\text{post}}$。\n2. 计算每种设计的 $\\operatorname{tr}(C_{\\text{post}})$，并确定哪种设计能最小化 $\\operatorname{tr}(C_{\\text{post}})$，即哪种设计是A-最优的。\n3. 根据联合高斯变量的互信息定义，推导每种设计的EIG，并用相关的模型矩阵和协方差表示。\n4. 使用这些结果明确证明，在这种情况下，最小化 $\\operatorname{tr}(C_{\\text{post}})$ 的设计并不能最大化EIG。\n5. 计算数量 $\\Delta = \\mathrm{EIG}_{\\mathcal{B}} - \\mathrm{EIG}_{\\mathcal{A}}$，并将你的最终答案表示为关于自然对数的单一闭式解析表达式。不要四舍五入。EIG是无量纲的（单位为奈特），因此最终的数值表达式中不需要单位。",
            "solution": "该问题陈述经核实具有科学依据、问题适定且客观。这是贝叶斯最优实验设计中的一个标准问题，没有缺陷，可以使用既定原则解决。我们可以开始解题。\n\n问题要求对一个线性高斯逆问题的两种实验设计 $\\mathcal{A}$ 和 $\\mathcal{B}$ 进行分析。状态为 $x \\in \\mathbb{R}^{2}$，其先验为 $x \\sim \\mathcal{N}(0, C_{\\text{prior}})$，其中 $C_{\\text{prior}} = I_2$。观测模型为 $y = Hx + \\varepsilon$，其中 $H=I_2$，噪声为 $\\varepsilon \\sim \\mathcal{N}(0, R)$。两种设计对应不同的噪声协方差矩阵：$R_{\\mathcal{A}} = \\operatorname{diag}(1, 1)$ 和 $R_{\\mathcal{B}} = \\operatorname{diag}(0.01, 1000)$。\n\n**1. 后验协方差的推导**\n\n在线性高斯贝叶斯框架中，后验分布 $p(x|y)$ 也是高斯分布。其协方差矩阵 $C_{\\text{post}}$ 由先验精度矩阵和数据精度矩阵之和的逆给出。精度矩阵是协方差矩阵的逆。\n后验精度矩阵 $C_{\\text{post}}^{-1}$ 为：\n$$C_{\\text{post}}^{-1} = C_{\\text{prior}}^{-1} + H^T R^{-1} H$$\n因此，后验协方差为：\n$$C_{\\text{post}} = (C_{\\text{prior}}^{-1} + H^T R^{-1} H)^{-1}$$\n根据问题的具体设定，$C_{\\text{prior}} = I_2$ 且 $H = I_2$，表达式简化为：\n$$C_{\\text{post}} = (I_2^{-1} + I_2^T R^{-1} I_2)^{-1} = (I_2 + R^{-1})^{-1}$$\n我们现在将此应用于每种设计。\n\n对于设计 $\\mathcal{A}$：\n$R_{\\mathcal{A}} = \\operatorname{diag}(1, 1) = I_2$。其逆矩阵为 $R_{\\mathcal{A}}^{-1} = I_2^{-1} = I_2$。\n$$C_{\\text{post}, \\mathcal{A}} = (I_2 + I_2)^{-1} = (2I_2)^{-1} = \\frac{1}{2}I_2 = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$$\n\n对于设计 $\\mathcal{B}$：\n$R_{\\mathcal{B}} = \\operatorname{diag}(0.01, 1000) = \\operatorname{diag}(10^{-2}, 10^3)$。其逆矩阵为 $R_{\\mathcal{B}}^{-1} = \\operatorname{diag}((10^{-2})^{-1}, (10^3)^{-1}) = \\operatorname{diag}(100, 10^{-3})$。\n$$C_{\\text{post}, \\mathcal{B}}^{-1} = I_2 + R_{\\mathcal{B}}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 100 & 0 \\\\ 0 & 0.001 \\end{pmatrix} = \\begin{pmatrix} 101 & 0 \\\\ 0 & 1.001 \\end{pmatrix}$$\n对该对角矩阵求逆可得后验协方差：\n$$C_{\\text{post}, \\mathcal{B}} = \\begin{pmatrix} \\frac{1}{101} & 0 \\\\ 0 & \\frac{1}{1.001} \\end{pmatrix}$$\n\n**2. A-最优性分析**\n\nA-最优性旨在最小化后验协方差矩阵的迹 $\\operatorname{tr}(C_{\\text{post}})$，该迹表示状态分量的后验方差之和。\n\n对于设计 $\\mathcal{A}$：\n$$\\operatorname{tr}(C_{\\text{post}, \\mathcal{A}}) = \\operatorname{tr}\\left(\\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1$$\n\n对于设计 $\\mathcal{B}$：\n$$\\operatorname{tr}(C_{\\text{post}, \\mathcal{B}}) = \\operatorname{tr}\\left(\\begin{pmatrix} \\frac{1}{101} & 0 \\\\ 0 & \\frac{1}{1.001} \\end{pmatrix}\\right) = \\frac{1}{101} + \\frac{1}{1.001}$$\n为了将其与 $1$ 比较，我们计算其和：\n$$\\frac{1}{101} + \\frac{1}{1.001} = \\frac{1}{101} + \\frac{1}{1001/1000} = \\frac{1}{101} + \\frac{1000}{1001} = \\frac{1001 + 101 \\times 1000}{101 \\times 1001} = \\frac{1001 + 101000}{101101} = \\frac{102001}{101101}$$\n由于 $102001 > 101101$，我们有 $\\frac{102001}{101101} > 1$。\n因此，$\\operatorname{tr}(C_{\\text{post}, \\mathcal{B}}) > \\operatorname{tr}(C_{\\text{post}, \\mathcal{A}})$。设计 $\\mathcal{A}$ 最小化了迹，因此是A-最优的。\n\n**3. 期望信息增益（EIG）的推导**\n\n期望信息增益（EIG）是互信息 $I(x; y)$。对于联合高斯变量，它是从先验到后验的熵减少量：\n$$\\text{EIG} = H(x) - H(x|y)$$\n一个多元高斯分布 $\\mathcal{N}(\\mu, \\Sigma)$ 的微分熵为 $H = \\frac{1}{2}\\ln \\det(2\\pi e \\Sigma)$。\n所以，\n$$\\text{EIG} = \\frac{1}{2}\\ln \\det(2\\pi e C_{\\text{prior}}) - \\frac{1}{2}\\ln \\det(2\\pi e C_{\\text{post}})$$\n$$\\text{EIG} = \\frac{1}{2}\\left( \\ln \\det(C_{\\text{prior}}) - \\ln \\det(C_{\\text{post}}) \\right) = \\frac{1}{2}\\ln\\left(\\frac{\\det(C_{\\text{prior}})}{\\det(C_{\\text{post}})}\\right)$$\n使用 $\\det(C_{\\text{post}}^{-1}) = \\det(C_{\\text{prior}}^{-1} + H^T R^{-1} H)$，我们可以写出：\n$$\\text{EIG} = \\frac{1}{2}\\ln\\left(\\det(C_{\\text{prior}}) \\det(C_{\\text{post}}^{-1})\\right) = \\frac{1}{2}\\ln\\left(\\det(C_{\\text{prior}}(C_{\\text{prior}}^{-1} + H^T R^{-1} H))\\right)$$\n$$\\text{EIG} = \\frac{1}{2}\\ln \\det(I + C_{\\text{prior}} H^T R^{-1} H)$$\n当 $C_{\\text{prior}} = I_2$ 且 $H = I_2$ 时，该式简化为：\n$$\\text{EIG} = \\frac{1}{2}\\ln \\det(I_2 + R^{-1})$$\n\n对于设计 $\\mathcal{A}$：\n$I_2 + R_{\\mathcal{A}}^{-1} = I_2 + I_2 = 2I_2$。\n$$\\text{EIG}_{\\mathcal{A}} = \\frac{1}{2}\\ln \\det(2I_2) = \\frac{1}{2}\\ln(4) = \\ln(2)$$\n\n对于设计 $\\mathcal{B}$：\n$I_2 + R_{\\mathcal{B}}^{-1} = \\begin{pmatrix} 101 & 0 \\\\ 0 & 1.001 \\end{pmatrix}$。\n$$\\det(I_2 + R_{\\mathcal{B}}^{-1}) = 101 \\times 1.001 = 101.101$$\n$$\\text{EIG}_{\\mathcal{B}} = \\frac{1}{2}\\ln(101.101)$$\n\n**4. A-最优性与EIG最大化之间的冲突**\n\n我们已经确定设计 $\\mathcal{A}$ 是A-最优的，因为 $\\operatorname{tr}(C_{\\text{post}, \\mathcal{A}})  \\operatorname{tr}(C_{\\text{post}, \\mathcal{B}})$。为了看这个设计是否也最大化EIG，我们比较 $\\text{EIG}_{\\mathcal{A}}$ 和 $\\text{EIG}_{\\mathcal{B}}$。\n我们需要比较 $\\ln(2)$ 和 $\\frac{1}{2}\\ln(101.101)$。\n这等价于比较 $2\\ln(2) = \\ln(2^2) = \\ln(4)$ 和 $\\ln(101.101)$。\n由于当 $x  0$ 时，$\\ln(x)$ 是一个严格递增函数，并且 $101.101  4$，因此可得：\n$$\\ln(101.101)  \\ln(4)$$\n$$\\frac{1}{2}\\ln(101.101)  \\ln(2)$$\n$$\\text{EIG}_{\\mathcal{B}}  \\text{EIG}_{\\mathcal{A}}$$\n这明确地表明，最小化 $\\operatorname{tr}(C_{\\text{post}})$ 的设计 $\\mathcal{A}$ 并不能最大化EIG。设计 $\\mathcal{B}$ 产生了更大的信息增益。这是一个经典的结果，表明不同的最优性准则可能导致相互冲突的设计选择。A-最优性关注平均方差，而EIG（与最小化 $\\det(C_{\\text{post}})$ 的D-最优性相关）关注后验不确定性椭球的总体积。\n\n**5. $\\Delta = \\mathrm{EIG}_{\\mathcal{B}} - \\mathrm{EIG}_{\\mathcal{A}}$ 的计算**\n\n我们被要求计算两种设计之间的EIG差异。\n$$\\Delta = \\mathrm{EIG}_{\\mathcal{B}} - \\mathrm{EIG}_{\\mathcal{A}} = \\frac{1}{2}\\ln(101.101) - \\ln(2)$$\n为了将其表示为单一的闭式表达式，我们使用对数的性质：\n$$\\Delta = \\frac{1}{2}\\ln(101.101) - \\frac{1}{2}\\ln(4) = \\frac{1}{2}\\left(\\ln(101.101) - \\ln(4)\\right) = \\frac{1}{2}\\ln\\left(\\frac{101.101}{4}\\right)$$\n为了得到精确的解析表达式，我们将小数写成分数形式。$101.101 = \\frac{101101}{1000}$。\n$$\\Delta = \\frac{1}{2}\\ln\\left(\\frac{101101/1000}{4}\\right) = \\frac{1}{2}\\ln\\left(\\frac{101101}{4000}\\right)$$\n这就是期望信息增益差异的最终解析表达式。",
            "answer": "$$\\boxed{\\frac{1}{2}\\ln\\left(\\frac{101101}{4000}\\right)}$$"
        },
        {
            "introduction": "通常，我们的首要目标不是学习模型中的每一个参数，而是精确确定一个由这些参数导出的特定“目标量”（Quantity of Interest, QoI）。本练习将证明，一个为学习完整参数矢量 $\\theta$ 而优化的实验，不一定是学习特定 QoI $\\psi$ 的最高效设计 。您将推导并比较针对这两个目标的最优设计，揭示为何“以参数为中心”和“以 QoI 为中心”的信息增益会产生不同策略的几何原因。这凸显了使实验设计与具体科学问题保持一致的重要性。",
            "id": "3380354",
            "problem": "考虑一个线性高斯贝叶斯逆问题和实验设计设置，其中未知参数是向量 $\\theta \\in \\mathbb{R}^{2}$，其高斯先验为 $\\theta \\sim \\mathcal{N}(0, \\Sigma_{\\theta})$，其中\n$$\n\\Sigma_{\\theta} \\;=\\;\n\\begin{pmatrix}\n4  3 \\\\\n3  4\n\\end{pmatrix}.\n$$\n通过测量模型，使用设计参数 $d \\in [0, 2\\pi)$ 进行单个标量观测\n$$\nY \\,\\big|\\, \\theta, d \\;\\sim\\; \\mathcal{N}\\!\\big(h(d)^{\\top}\\theta,\\, \\sigma^{2}\\big), \\quad \\text{其中} \\quad h(d) \\;=\\; \\begin{pmatrix}\\cos d \\\\ \\sin d\\end{pmatrix}, \\quad \\sigma^{2} \\;=\\; 1.\n$$\n目标量是线性泛函 $\\psi = a^{\\top}\\theta$，其中 $a = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$。\n\n仅使用信息论和高斯概率的基本定义（特别是互信息 $I(X;Y\\,|\\,d) = H(X) - H(X\\,|\\,Y,d)$ 和多元高斯熵的定义），完成以下任务：\n\n1. 推导期望信息增益 $I(\\theta; Y \\mid d)$ 和 $I(\\psi; Y \\mid d)$ 作为 $d$ 的函数。\n2. 对于每个信息目标，在由 $h(d)$ 隐含的单位范数约束下，确定最大化 $I(\\theta; Y \\mid d)$ 的最优设计 $d_{\\theta}^{\\star}$ 和最大化 $I(\\psi; Y \\mid d)$ 的最优设计 $d_{\\psi}^{\\star}$。\n3. 证明对于给定的 $\\Sigma_{\\theta}$ 和 $\\sigma^{2}$，有 $d_{\\theta}^{\\star} \\neq d_{\\psi}^{\\star}$。\n4. 解释为什么当 $\\psi$ 是 $\\theta$ 的不可逆函数时，这两个最优设计通常会不同，其结构性原因需基于定义和高斯推断的几何学。\n\n最后，计算两个单位范数最优测量方向 $h(d_{\\theta}^{\\star})$ 和 $h(d_{\\psi}^{\\star})$ 之间的锐角（以弧度为单位），并以单个精确的解析表达式给出你的答案。角度以弧度表示。不要对答案进行四舍五入。",
            "solution": "该问题在贝叶斯逆问题和最优实验设计的框架内是适定的，并且有科学依据。所有必要的参数和定义都已提供。我们可以开始求解。\n\n解答根据问题陈述中指定的四个任务进行组织，最终计算出两个最优测量方向之间的角度。\n\n### 1. 期望信息增益的推导\n\n两个随机变量 $X$ 和 $Y$ 之间的互信息由 $I(X; Y) = H(X) - H(X|Y)$ 给出，其中 $H(\\cdot)$ 表示微分熵。对于高斯分布，一个等价且通常更方便的表达式是 $I(X; Y) = H(Y) - H(Y|X)$。我们使用一个设计参数 $d$，因此所有的量都以 $d$ 为条件。一个 $k$ 维高斯随机变量 $Z \\sim \\mathcal{N}(\\mu, \\Sigma)$ 的熵是 $H(Z) = \\frac{1}{2}\\ln(\\det(2\\pi e \\Sigma))$。\n\n**$I(\\theta; Y \\mid d)$ 的推导**\n\n我们计算 $I(\\theta; Y \\mid d) = H(Y \\mid d) - H(Y \\mid \\theta, d)$。\n\n首先，我们考虑在参数 $\\theta$ 和设计 $d$ 的条件下，观测 $Y$ 的熵。问题陈述 $Y \\mid \\theta, d \\sim \\mathcal{N}(h(d)^\\top \\theta, \\sigma^2)$。由于这是一个方差为 $\\sigma^2 = 1$ 的标量高斯分布，其熵为：\n$$\nH(Y \\mid \\theta, d) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2) = \\frac{1}{2}\\ln(2\\pi e)\n$$\n\n接下来，我们求仅以 $d$ 为条件的 $Y$ 的边缘分布。由于 $\\theta \\sim \\mathcal{N}(0, \\Sigma_\\theta)$ 并且观测模型关于 $\\theta$ 是线性的，因此 $Y \\mid d$ 也是高斯分布。\n其均值为 $\\mathbb{E}[Y \\mid d] = \\mathbb{E}[h(d)^\\top \\theta] = h(d)^\\top \\mathbb{E}[\\theta] = 0$。\n其方差为 $\\text{Var}(Y \\mid d) = \\text{Var}(h(d)^\\top \\theta + \\epsilon) = \\text{Var}(h(d)^\\top \\theta) + \\text{Var}(\\epsilon)$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 是观测噪声。\n$$\n\\text{Var}(Y \\mid d) = h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2\n$$\n所以，$Y \\mid d \\sim \\mathcal{N}(0, h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)$。其熵为：\n$$\nH(Y \\mid d) = \\frac{1}{2}\\ln(2\\pi e (\\text{Var}(Y \\mid d))) = \\frac{1}{2}\\ln(2\\pi e (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2))\n$$\n期望信息增益是这两个熵的差：\n$$\nI(\\theta; Y \\mid d) = H(Y \\mid d) - H(Y \\mid \\theta, d) = \\frac{1}{2}\\ln\\left(\\frac{2\\pi e (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}{2\\pi e \\sigma^2}\\right) = \\frac{1}{2}\\ln\\left(1 + \\frac{h(d)^\\top \\Sigma_\\theta h(d)}{\\sigma^2}\\right)\n$$\n当 $\\sigma^2=1$ 时，这简化为 $I(\\theta; Y \\mid d) = \\frac{1}{2}\\ln(1 + h(d)^\\top \\Sigma_\\theta h(d))$。\n\n**$I(\\psi; Y \\mid d)$ 的推导**\n\n我们计算 $I(\\psi; Y \\mid d) = H(\\psi) - H(\\psi \\mid Y, d)$，其中 $\\psi = a^\\top \\theta$。最大化这个量等价于最小化后验熵 $H(\\psi \\mid Y, d)$。\n$\\psi$ 的先验方差是 $\\text{Var}(\\psi) = \\text{Var}(a^\\top\\theta) = a^\\top \\Sigma_\\theta a$。\n观测到 $Y$ 后，$\\theta$ 的后验协方差矩阵由线性高斯模型的贝叶斯更新公式给出：\n$$\n\\Sigma_{\\theta|Y,d} = \\left(\\Sigma_\\theta^{-1} + \\frac{h(d)h(d)^\\top}{\\sigma^2}\\right)^{-1}\n$$\n使用 Sherman-Morrison-Woodbury 公式，我们得到：\n$$\n\\Sigma_{\\theta|Y,d} = \\Sigma_\\theta - \\frac{\\Sigma_\\theta h(d) h(d)^\\top \\Sigma_\\theta}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\n$$\n$\\psi = a^\\top \\theta$ 的后验分布是高斯的，$\\psi \\mid Y,d \\sim \\mathcal{N}(a^\\top \\mu_{\\theta|Y,d}, a^\\top \\Sigma_{\\theta|Y,d} a)$，其中 $\\mu_{\\theta|Y,d}$ 是后验均值。$\\psi$ 的后验方差 $\\text{Var}(\\psi \\mid Y, d) = a^\\top \\Sigma_{\\theta|Y,d} a$ 不依赖于测量值 $Y$。\n$$\n\\text{Var}(\\psi \\mid Y, d) = a^\\top \\left(\\Sigma_\\theta - \\frac{\\Sigma_\\theta h(d) h(d)^\\top \\Sigma_\\theta}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\\right) a = a^\\top\\Sigma_\\theta a - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\n$$\n信息增益是 $\\psi$ 从先验到后验的熵减少量：\n$$\nI(\\psi; Y \\mid d) = H(\\psi) - H(\\psi \\mid Y, d) = \\frac{1}{2}\\ln(2\\pi e \\text{Var}(\\psi)) - \\frac{1}{2}\\ln(2\\pi e \\text{Var}(\\psi \\mid Y, d))\n$$\n$$\nI(\\psi; Y \\mid d) = \\frac{1}{2}\\ln\\left(\\frac{\\text{Var}(\\psi)}{\\text{Var}(\\psi \\mid Y, d)}\\right) = \\frac{1}{2}\\ln\\left(\\frac{a^\\top\\Sigma_\\theta a}{a^\\top\\Sigma_\\theta a - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}}\\right)\n$$\n这可以重写为：\n$$\nI(\\psi; Y \\mid d) = -\\frac{1}{2}\\ln\\left(1 - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{(a^\\top \\Sigma_\\theta a) (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}\\right)\n$$\n\n### 2. 最优设计的确定\n\n**最优设计 $d_{\\theta}^{\\star}$**\n\n为了最大化 $I(\\theta; Y \\mid d) = \\frac{1}{2}\\ln(1 + h(d)^\\top \\Sigma_\\theta h(d))$，我们必须在 $h(d)$ 是单位向量（即 $h(d)^\\top h(d) = 1$）的约束下，最大化二次型 $h(d)^\\top \\Sigma_\\theta h(d)$。这是一个瑞利商最大化问题。最大值是 $\\Sigma_\\theta$ 的最大特征值，最优向量 $h(d)$ 是对应的特征向量。\n\n先验协方差为 $\\Sigma_\\theta = \\begin{pmatrix} 4  3 \\\\ 3  4 \\end{pmatrix}$。其特征方程是 $\\det(\\Sigma_\\theta - \\lambda I) = (4-\\lambda)^2 - 9 = 0$，解得特征值为 $\\lambda_1 = 7$ 和 $\\lambda_2 = 1$。最大特征值是 $\\lambda_{\\max} = 7$。\n$\\lambda_{\\max}=7$ 对应的特征向量通过求解 $(\\Sigma_\\theta - 7I)v = 0$ 得到：\n$$\n\\begin{pmatrix} -3  3 \\\\ 3  -3 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies v_1 = v_2\n$$\n单位化的特征向量是 $\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。我们设 $h(d_{\\theta}^{\\star}) = \\begin{pmatrix} \\cos d_{\\theta}^{\\star} \\\\ \\sin d_{\\theta}^{\\star} \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。这得到 $\\cos d_{\\theta}^{\\star} = \\sin d_{\\theta}^{\\star} = 1/\\sqrt{2}$，因此一个最优设计是 $d_{\\theta}^{\\star} = \\pi/4$。\n\n**最优设计 $d_{\\psi}^{\\star}$**\n\n为了最大化 $I(\\psi; Y \\mid d)$，我们必须最大化对数的参数，这等价于最大化 $\\psi$ 和 $Y$ 之间的相关系数的平方（给定 $d$）：\n$$\n\\rho^2(d) = \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{(a^\\top \\Sigma_\\theta a) (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}\n$$\n我们用 $a = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$h(d) = \\begin{pmatrix} \\cos d \\\\ \\sin d \\end{pmatrix}$ 和 $\\sigma^2=1$ 来计算各个组成部分：\n- $a^\\top \\Sigma_\\theta = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 4  3 \\\\ 3  4 \\end{pmatrix} = \\begin{pmatrix} 4  3 \\end{pmatrix}$\n- $a^\\top \\Sigma_\\theta a = 4$\n- $a^\\top \\Sigma_\\theta h(d) = 4\\cos d + 3\\sin d$\n- $h(d)^\\top \\Sigma_\\theta h(d) = 4\\cos^2 d + 6\\sin d \\cos d + 4\\sin^2 d = 4 + 3\\sin(2d)$\n\n需要最大化的目标函数是：\n$$\n\\rho^2(d) = \\frac{(4\\cos d + 3\\sin d)^2}{4(4 + 3\\sin(2d) + 1)} = \\frac{(4\\cos d + 3\\sin d)^2}{4(5 + 3\\sin(2d))}\n$$\n将关于 $d$ 的导数设为零，得到条件 $9\\cos(2d) - 7\\sin(2d) = 4.2$。\n我们从这个方程和恒等式 $\\cos^2(2d) + \\sin^2(2d) = 1$ 中求解 $\\cos(2d)$ 和 $\\sin(2d)$。这会产生两对解：\n1. $\\cos(2d) = 56/65$, $\\sin(2d) = 33/65$\n2. $\\cos(2d) = -7/25$, $\\sin(2d) = -24/25$\n计算目标函数 $\\rho^2(d)$ 的值表明，第二组解给出 $\\rho^2=0$（一个最小值），而第一组解给出最大值。\n对于最优设计，我们使用恒等式 $\\cos^2 d = \\frac{1+\\cos(2d)}{2}$ 和 $\\sin^2 d = \\frac{1-\\cos(2d)}{2}$：\n$$\n\\cos^2(d_{\\psi}^{\\star}) = \\frac{1 + 56/65}{2} = \\frac{121/65}{2} = \\frac{121}{130} \\implies |\\cos(d_{\\psi}^{\\star})| = \\frac{11}{\\sqrt{130}}\n$$\n$$\n\\sin^2(d_{\\psi}^{\\star}) = \\frac{1 - 56/65}{2} = \\frac{9/65}{2} = \\frac{9}{130} \\implies |\\sin(d_{\\psi}^{\\star})| = \\frac{3}{\\sqrt{130}}\n$$\n由于 $\\sin(2d_{\\psi}^{\\star}) = 2\\sin(d_{\\psi}^{\\star})\\cos(d_{\\psi}^{\\star}) = 33/65  0$，$\\sin(d_{\\psi}^{\\star})$ 和 $\\cos(d_{\\psi}^{\\star})$ 必须同号。我们选择正解，这给出了最优测量方向向量：\n$$\nh(d_{\\psi}^{\\star}) = \\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\n这对应于一个角度 $d_{\\psi}^{\\star} = \\arctan(3/11)$。\n\n### 3. 最优设计不相等的证明\n\n总信息的最优设计是 $d_{\\theta}^{\\star} = \\pi/4$。特定于目标量的信息的最优设计是 $d_{\\psi}^{\\star} = \\arctan(3/11)$。由于 $\\tan(d_{\\theta}^{\\star}) = \\tan(\\pi/4) = 1$ 且 $\\tan(d_{\\psi}^{\\star}) = 3/11$，而 $1 \\neq 3/11$，因此证明了 $d_{\\theta}^{\\star} \\neq d_{\\psi}^{\\star}$。\n\n### 4. 差异的结构性原因\n\n最大化 $I(\\theta; Y \\mid d)$ 和最大化 $I(\\psi; Y \\mid d)$ 这两个目标是根本上不同的。\n- 最大化 $I(\\theta; Y \\mid d)$ 旨在减少参数向量 $\\theta$ 的总体不确定性。这通过在先验不确定性最大的方向上进行测量来实现。先验不确定性的几何形状由协方差矩阵 $\\Sigma_\\theta$ 描述。最大方差的方向是 $\\Sigma_\\theta$ 的主特征向量。因此，$h(d_\\theta^\\star)$ 与该特征向量对齐。这是一个非靶向的，或“参数聚焦”的设计准则。\n- 最大化 $I(\\psi; Y \\mid d)$ 旨在减少特定目标量 $\\psi=a^\\top\\theta$ 的不确定性，这是一个 $\\theta$ 的不可逆线性投影。最优设计必须平衡在目标函数 $\\rho^2(d)$ 中可见的两个竞争因素：\n  1. 分子 $(a^\\top \\Sigma_\\theta h)^2 = \\text{Cov}(\\psi, h^\\top\\theta)^2$ 鼓励将测量方向 $h$ 与向量 $\\Sigma_\\theta a$ 对齐。这个向量表示目标量与 $\\theta$ 各分量之间的协方差，有效地映射了 $\\theta$ 的变化如何影响 $\\psi$。与此方向相关的测量对 $\\psi$ 的信息量最大。\n  2. 分母 $h^\\top \\Sigma_\\theta h + \\sigma^2$ 是观测的先验预测方差。该项惩罚在高先验方差方向（即 $h^\\top \\Sigma_\\theta h$ 较大）上进行的测量，因为由于 $\\theta$ 自身的不确定性，这类测量内在地更“嘈杂”。\n因此，最优设计 $h(d_\\psi^\\star)$ 代表一种折衷：它从最大先验方差的方向（由 $d_\\theta^\\star$ 选择）拉开，朝向一个为估计 $\\psi$ 提供更好信噪比的方向。这两个方向仅在特殊情况下才会重合，例如，如果目标量向量 $a$ 恰好是 $\\Sigma_\\theta$ 的一个特征向量。在这个问题中，$a=(1,0)^\\top$ 不是一个特征向量，所以最优设计是不同的。\n\n### 最终计算：最优方向之间的锐角\n\n我们需要求出两个最优单位范数测量向量之间的锐角 $\\phi$：\n$$\nh(d_{\\theta}^{\\star}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\quad \\text{和} \\quad h(d_{\\psi}^{\\star}) = \\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\n两个单位向量之间夹角的余弦是它们的点积：\n$$\n\\cos \\phi = h(d_{\\theta}^{\\star}) \\cdot h(d_{\\psi}^{\\star}) = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\cdot \\left(\\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\\right)\n$$\n$$\n\\cos \\phi = \\frac{1}{\\sqrt{260}} (1 \\cdot 11 + 1 \\cdot 3) = \\frac{14}{\\sqrt{260}} = \\frac{14}{\\sqrt{4 \\cdot 65}} = \\frac{14}{2\\sqrt{65}} = \\frac{7}{\\sqrt{65}}\n$$\n由于 $\\cos \\phi  0$，该角是锐角。该角度是：\n$$\n\\phi = \\arccos\\left(\\frac{7}{\\sqrt{65}}\\right)\n$$\n这就是所要求的精确解析表达式。",
            "answer": "$$\n\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{65}}\\right)}\n$$"
        },
        {
            "introduction": "现实世界中的科学模型通常是分层的，包含我们不直接关心但会影响数据生成过程的“滋扰参数”（nuisance parameters）。本练习从解析解转向计算方法，解决了如何在这种复杂情景下估计 EIG 的问题 。您将推导一个高效的蒙特卡洛估计量，它通过解析方法边缘化滋扰参数，这是一种能显著提高计算准确性和速度的强大技术。该练习为 EIG 理论与其在实际建模场景中的应用之间架起了一座桥梁。",
            "id": "3380344",
            "problem": "考虑一个分层线性高斯数据模型，其中讨厌参数仅通过噪声分布进入模型。令感兴趣的参数为向量 $\\psi \\in \\mathbb{R}^{n_{\\psi}}$，其先验密度为 $p(\\psi)$，并且可以从中采样。对于一个固定的设计 $d$，数据 $Y \\in \\mathbb{R}^{n_{y}}$ 通过以下方式生成：\n- 前向映射 $H(d) \\in \\mathbb{R}^{n_{y} \\times n_{\\psi}}$，即 $Y \\mid \\psi, \\phi, d \\sim \\mathcal{N}\\!\\left(H(d)\\,\\psi,\\;\\phi^{-1}\\,\\Sigma(d)\\right)$，其中 $\\Sigma(d) \\in \\mathbb{R}^{n_{y} \\times n_{y}}$ 是一个已知的正定矩阵，以及\n- 噪声精度 $\\phi$ 服从伽马先验分布 $\\phi \\sim \\mathrm{Gamma}\\!\\left(\\frac{\\nu}{2},\\,\\frac{\\nu}{2}\\right)$，对于一个固定的 $\\nu  0$，其形状参数为 $\\frac{\\nu}{2}$，速率参数为 $\\frac{\\nu}{2}$。\n\n假设 $\\phi$ 是一个我们不感兴趣的讨厌参数，且仅如上所述进入噪声模型，并且 $p(\\psi)$ 与 $\\phi$ 相互独立。期望信息增益 $I(\\psi; Y \\mid d)$ 定义为在该分层模型下，以 $d$ 为条件时 $\\psi$ 和 $Y$ 之间的互信息。\n\n从互信息的定义和概率论的基本原理（例如全概率定律和贝叶斯法则）出发，推导出一个 $I(\\psi; Y \\mid d)$ 的蒙特卡洛估计量。该估计量在似然函数中精确地将讨厌参数 $\\phi$ 边缘化，并避免在合成数据的内循环中对 $\\phi$ 进行采样。您的推导必须：\n- 从定义 $I(\\psi; Y \\mid d) = \\mathbb{E}_{\\psi}\\,\\mathbb{E}_{Y \\mid \\psi, d}\\!\\left[\\ln p(Y \\mid \\psi, d) - \\ln p(Y \\mid d)\\right]$ 开始，\n- 对于给定的分层模型，以闭合形式获得经 $\\phi$ 边缘化的数据似然 $p(Y \\mid \\psi, d)$，以及\n- 产生一个基于 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^{M}$ 的单循环、样本重用的蒙特卡洛估计量，该估计量仅依赖于经 $\\phi$ 边缘化的似然评估，并且不在对 $Y$ 的内循环中采样 $\\phi$。\n\n用 $H(d)$、$\\Sigma(d)$、$\\nu$ 和多元学生t分布密度，将您的最终估计量表示为单个明确的解析表达式，并确保清晰地指定每个数学符号。最终答案必须是闭合形式的表达式。不需要进行数值评估。",
            "solution": "该问题陈述经严格验证，具有科学依据、问题定义良好、客观且完整。它描述了一个标准的分层贝叶斯模型，其期望信息增益是一个明确定义的量。前提条件是一致的，任务是在反问题和数据同化范畴内的形式化推导。因此，该问题被认定为有效。\n\n目标是为期望信息增益（EIG）或互信息 $I(\\psi; Y \\mid d)$ 推导一个单循环、样本重用的蒙特卡洛估计量，该估计量解析地将讨厌参数 $\\phi$ 边缘化。\n\nEIG 定义为：\n$$\nI(\\psi; Y \\mid d) = \\mathbb{E}_{\\psi, Y \\mid d}\\! \\left[ \\ln\\frac{p(Y \\mid \\psi, d)}{p(Y \\mid d)} \\right] = \\mathbb{E}_{\\psi}\\,\\mathbb{E}_{Y \\mid \\psi, d}\\!\\left[\\ln p(Y \\mid \\psi, d) - \\ln p(Y \\mid d)\\right]\n$$\n期望是关于联合分布 $p(\\psi, Y \\mid d) = p(Y \\mid \\psi, d) p(\\psi)$ 的。通过从该联合分布中抽取 $M$ 个样本 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^{M}$ 来构建此期望的蒙特卡洛近似。实现方法是首先抽取 $\\psi^{(i)} \\sim p(\\psi)$，然后抽取 $Y^{(i)} \\sim p(Y \\mid \\psi^{(i)}, d)$。\n\n推导过程主要分为三个步骤：\n1.  推导经 $\\phi$ 边缘化的数据似然 $p(Y \\mid \\psi, d)$ 的解析形式。\n2.  制定避免对 $\\phi$ 采样的蒙特卡洛采样程序。\n3.  使用推导出的似然和针对证据项 $p(Y \\mid d)$ 的样本重用策略，构建 $I(\\psi; Y \\mid d)$ 的最终估计量。\n\n**步骤 1：边缘似然 $p(Y \\mid \\psi, d)$ 的推导**\n\n边缘似然 $p(Y \\mid \\psi, d)$ 是通过对讨厌参数 $\\phi$ 积分完整似然 $p(Y, \\phi \\mid \\psi, d)$ 得到的。使用概率的链式法则以及 $\\psi$ 和 $\\phi$ 的先验独立性，我们有：\n$$\np(Y \\mid \\psi, d) = \\int_{0}^{\\infty} p(Y, \\phi \\mid \\psi, d) \\, d\\phi = \\int_{0}^{\\infty} p(Y \\mid \\psi, \\phi, d) \\, p(\\phi \\mid \\psi, d) \\, d\\phi = \\int_{0}^{\\infty} p(Y \\mid \\psi, \\phi, d) \\, p(\\phi) \\, d\\phi\n$$\n问题指明了这两种密度的形式：\n数据似然是一个多元正态分布：\n$$\np(Y \\mid \\psi, \\phi, d) = \\mathcal{N}(Y; H(d)\\psi, \\phi^{-1}\\Sigma(d)) = \\frac{|\\det(\\phi^{-1}\\Sigma(d))|^{-1/2}}{(2\\pi)^{n_y/2}} \\exp\\left(-\\frac{1}{2} (Y-H(d)\\psi)^T (\\phi^{-1}\\Sigma(d))^{-1} (Y-H(d)\\psi)\\right)\n$$\n$$\np(Y \\mid \\psi, \\phi, d) = \\frac{\\phi^{n_y/2}}{ (2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2}} \\exp\\left(-\\frac{\\phi}{2} (Y-H(d)\\psi)^T \\Sigma(d)^{-1} (Y-H(d)\\psi)\\right)\n$$\n噪声精度 $\\phi$ 的先验是一个伽马分布：\n$$\np(\\phi) = \\mathrm{Gamma}\\left(\\phi; \\frac{\\nu}{2}, \\frac{\\nu}{2}\\right) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\phi^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\phi\\right)\n$$\n现在，我们计算 $p(Y \\mid \\psi, d)$ 的积分：\n$$\np(Y \\mid \\psi, d) = \\int_{0}^{\\infty} \\left[ \\frac{\\phi^{n_y/2} \\exp\\left(-\\frac{\\phi}{2} \\delta^2(Y, \\psi)\\right)}{(2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2}} \\right] \\left[ \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\phi^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\phi\\right) \\right] d\\phi\n$$\n其中 $\\delta^2(Y, \\psi) = (Y-H(d)\\psi)^T \\Sigma(d)^{-1} (Y-H(d)\\psi)$ 是马氏距离的平方。\n合并与 $\\phi$ 相关的项：\n$$\np(Y \\mid \\psi, d) = \\frac{(\\nu/2)^{\\nu/2}}{(2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2} \\Gamma(\\nu/2)} \\int_{0}^{\\infty} \\phi^{\\frac{n_y+\\nu}{2}-1} \\exp\\left(-\\frac{\\phi}{2}(\\delta^2(Y, \\psi) + \\nu)\\right) d\\phi\n$$\n该积分是伽马分布的核。使用恒等式 $\\int_0^\\infty x^{a-1} \\exp(-bx) dx = \\frac{\\Gamma(a)}{b^a}$，其中 $a = \\frac{n_y+\\nu}{2}$ 和 $b = \\frac{\\delta^2(Y, \\psi) + \\nu}{2}$，积分的计算结果为：\n$$\n\\int_{0}^{\\infty} \\dots d\\phi = \\frac{\\Gamma\\left(\\frac{n_y+\\nu}{2}\\right)}{\\left(\\frac{\\delta^2(Y, \\psi) + \\nu}{2}\\right)^{\\frac{n_y+\\nu}{2}}}\n$$\n将此结果代回 $p(Y \\mid \\psi, d)$ 的表达式中：\n$$\np(Y \\mid \\psi, d) = \\frac{(\\nu/2)^{\\nu/2} \\Gamma\\left(\\frac{n_y+\\nu}{2}\\right)}{(2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2} \\Gamma(\\nu/2)} \\frac{2^{\\frac{n_y+\\nu}{2}}}{(\\delta^2(Y, \\psi) + \\nu)^{\\frac{n_y+\\nu}{2}}}\n$$\n简化常数项可得：\n$$\np(Y \\mid \\psi, d) = \\frac{\\Gamma\\left(\\frac{\\nu+n_y}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) (\\nu\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2}} \\left(1 + \\frac{\\delta^2(Y, \\psi)}{\\nu}\\right)^{-\\frac{\\nu+n_y}{2}}\n$$\n这是多元学生t分布的概率密度函数（PDF）。令 $t_{n_y}(y; \\mu, S, \\nu_{df})$ 表示随机向量 $y \\in \\mathbb{R}^{n_y}$ 的PDF，其位置参数为 $\\mu$，尺度矩阵为 $S$，自由度为 $\\nu_{df}$。那么我们可以写作：\n$$\np(Y \\mid \\psi, d) = t_{n_y}(Y; H(d)\\psi, \\Sigma(d), \\nu)\n$$\n\n**步骤 2：蒙特卡洛采样程序**\n\n用于采样的联合分布是 $p(\\psi, Y \\mid d) = p(Y \\mid \\psi, d) p(\\psi)$。我们通过祖先采样生成 $M$ 个样本 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^M$：\n1.  对于 $i=1, \\dots, M$，从其先验分布中抽取一个参数样本 $\\psi^{(i)}$：$\\psi^{(i)} \\sim p(\\psi)$。\n2.  对于每个 $\\psi^{(i)}$，从上面推导出的边缘化似然中抽取一个对应的数据样本 $Y^{(i)}$：$Y^{(i)} \\sim p(Y \\mid \\psi^{(i)}, d) = t_{n_y}(\\cdot; H(d)\\psi^{(i)}, \\Sigma(d), \\nu)$。\n此过程不需要对讨厌参数 $\\phi$ 进行采样。\n\n**步骤 3：单循环估计量的构建**\n\nEIG 可以从生成的样本 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^M$ 中估计如下：\n$$\n\\hat{I}_M(\\psi; Y \\mid d) = \\frac{1}{M} \\sum_{i=1}^M \\left[ \\ln p(Y^{(i)} \\mid \\psi^{(i)}, d) - \\ln p(Y^{(i)} \\mid d) \\right]\n$$\n第一项 $p(Y^{(i)} \\mid \\psi^{(i)}, d)$ 使用学生t分布的PDF直接评估：\n$$\np(Y^{(i)} \\mid \\psi^{(i)}, d) = t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu)\n$$\n第二项，即证据 $p(Y^{(i)} \\mid d)$，难以直接计算。它被定义为似然函数在 $\\psi$ 的先验上的期望：\n$$\np(Y^{(i)} \\mid d) = \\int p(Y^{(i)} \\mid \\psi', d) \\, p(\\psi') \\, d\\psi' = \\mathbb{E}_{\\psi'} \\left[ t_{n_y}(Y^{(i)}; H(d)\\psi', \\Sigma(d), \\nu) \\right]\n$$\n我们可以使用蒙特卡洛近似来估计这个期望。一个高效估计量的关键思想是，重用同一组参数样本 $\\{\\psi^{(j)}\\}_{j=1}^M$ 来估计每个数据点 $Y^{(i)}$ 的证据。这给出了证据的估计量：\n$$\n\\hat{p}(Y^{(i)} \\mid d) = \\frac{1}{M} \\sum_{j=1}^M p(Y^{(i)} \\mid \\psi^{(j)}, d) = \\frac{1}{M} \\sum_{j=1}^M t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu)\n$$\n将这些表达式代回 EIG 估计量，得到最终的单循环、样本重用的蒙特卡洛估计量：\n$$\n\\hat{I}_M(\\psi; Y \\mid d) = \\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\ln t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu) - \\ln \\left( \\frac{1}{M} \\sum_{j=1}^{M} t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu) \\right) \\right]\n$$\n该表达式仅依赖于经 $\\phi$ 边缘化的似然的评估。\n\n为清晰起见，对于向量 $y \\in \\mathbb{R}^{n_y}$，其位置参数为 $\\mu$，尺度矩阵为 $S$，自由度为 $\\nu_{df}$ 的多元学生t分布密度函数 $t_{n_y}(y; \\mu, S, \\nu_{df})$ 定义为：\n$$\nt_{n_y}(y; \\mu, S, \\nu_{df}) = \\frac{\\Gamma\\left(\\frac{\\nu_{df}+n_y}{2}\\right)}{\\Gamma\\left(\\frac{\\nu_{df}}{2}\\right) (\\nu_{df}\\pi)^{n_y/2} |\\det(S)|^{1/2}} \\left(1 + \\frac{1}{\\nu_{df}}(y-\\mu)^T S^{-1} (y-\\mu)\\right)^{-\\frac{\\nu_{df}+n_y}{2}}\n$$\n在我们的例子中，参数为 $y=Y^{(i)}$，$\\mu=H(d)\\psi^{(j)}$，$S=\\Sigma(d)$，以及 $\\nu_{df}=\\nu$。",
            "answer": "$$\n\\boxed{\\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\ln t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu) - \\ln \\left( \\frac{1}{M} \\sum_{j=1}^{M} t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu) \\right) \\right]}\n$$"
        }
    ]
}