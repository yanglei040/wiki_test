{
    "hands_on_practices": [
        {
            "introduction": "To build a robust understanding of Fisher information, it is essential to move beyond textbook Gaussian examples. This practice guides you through deriving the Fisher Information Matrix from first principles for data following a Poisson distribution, a model crucial for photon-limited imaging and event-counting applications . By working through this derivation, you will solidify your grasp of the score function and its connection to information, and explore how information content degrades in practical low-count scenarios.",
            "id": "3381487",
            "problem": "In a photon-limited data assimilation setting, suppose $n$ independent sensor counts $y_{i}$, for $i \\in \\{1,\\dots,n\\}$, follow $y_{i} \\sim \\mathrm{Poisson}(\\lambda_{i}(\\theta))$, where $\\theta \\in \\mathbb{R}^{p}$ is an unknown parameter vector and each forward rate function $\\lambda_{i} : \\mathbb{R}^{p} \\to \\mathbb{R}_{>0}$ is continuously differentiable. Work from the first principles of likelihood-based inference, starting from the definition of the Fisher information matrix as the expectation of the outer product of the score under the data model, and derive a general expression for the Fisher information matrix $I(\\theta)$ in terms of $\\{\\lambda_{i}(\\theta)\\}_{i=1}^{n}$ and their gradients with respect to $\\theta$. Then specialize to the log-linear forward model $\\lambda_{i}(\\theta) = \\exp(x_{i}^{\\top}\\theta)$ with $x_{i} \\in \\mathbb{R}^{p}$ known.\n\nNext, consider $p=2$ and $n=3$ with\n$$\nx_{1}=\\begin{pmatrix}1 \\\\ 0.5\\end{pmatrix},\\quad x_{2}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad x_{3}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix},\n$$\nand the true parameter $\\theta_{0}=\\begin{pmatrix}-2 \\\\ -1\\end{pmatrix}$. Using your expression for the Fisher information, compute the Cramér–Rao lower bound (CRLB) for any unbiased estimator of the linear functional $c^{\\top}\\theta$ with $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, evaluated at $\\theta=\\theta_{0}$. Express your final numerical answer as a single real number, rounded to $4$ significant figures.\n\nFinally, briefly discuss, without altering your numerical result, how the low-count regime (small $\\lambda_{i}(\\theta)$) influences the Fisher information and the practical tightness of the Cramér–Rao lower bound in finite-sample inverse problems and data assimilation.",
            "solution": "We begin from the definition of the Fisher information matrix for a parametric family with parameter $\\theta \\in \\mathbb{R}^{p}$,\n$$\nI(\\theta) \\equiv \\mathbb{E}_{\\theta}\\!\\left[\\,\\nabla_{\\theta} \\ell(\\theta; y)\\,\\nabla_{\\theta} \\ell(\\theta; y)^{\\top}\\right],\n$$\nwhere $\\ell(\\theta; y)$ is the log-likelihood, and the expectation is with respect to the data distribution at parameter $\\theta$. For independent observations $\\{y_{i}\\}_{i=1}^{n}$ with $y_{i} \\sim \\mathrm{Poisson}(\\lambda_{i}(\\theta))$, the joint likelihood is\n$$\nL(\\theta; y)=\\prod_{i=1}^{n}\\frac{\\exp(-\\lambda_{i}(\\theta))\\,\\lambda_{i}(\\theta)^{y_{i}}}{y_{i}!},\\qquad \\ell(\\theta; y)=\\sum_{i=1}^{n}\\left[-\\lambda_{i}(\\theta)+y_{i}\\ln \\lambda_{i}(\\theta)-\\ln(y_{i}!)\\right].\n$$\nDifferentiate to obtain the score:\n$$\n\\nabla_{\\theta}\\ell(\\theta; y)=\\sum_{i=1}^{n}\\left[-\\nabla_{\\theta}\\lambda_{i}(\\theta)+y_{i}\\,\\nabla_{\\theta}\\ln \\lambda_{i}(\\theta)\\right]\n=\\sum_{i=1}^{n}\\left[-\\nabla_{\\theta}\\lambda_{i}(\\theta)+\\frac{y_{i}}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)\\right]\n=\\sum_{i=1}^{n}\\frac{y_{i}-\\lambda_{i}(\\theta)}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta).\n$$\nUsing independence, the expectation of the outer product of the sum is the sum of expectations of the outer products, because cross-terms vanish due to zero covariance across $i \\neq j$:\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\mathbb{E}_{\\theta}\\!\\left[\\left(\\frac{y_{i}-\\lambda_{i}(\\theta)}{\\lambda_{i}(\\theta)}\\right)^{2}\\right]\\nabla_{\\theta}\\lambda_{i}(\\theta)\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)^{\\top}.\n$$\nFor a Poisson random variable, $\\mathrm{Var}(y_{i})=\\lambda_{i}(\\theta)$ and $\\mathbb{E}\\!\\left[(y_{i}-\\lambda_{i}(\\theta))^{2}\\right]=\\lambda_{i}(\\theta)$. Therefore,\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\frac{1}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)^{\\top}.\n$$\n\nSpecialize to the log-linear model $\\lambda_{i}(\\theta)=\\exp(x_{i}^{\\top}\\theta)$ with $x_{i}\\in\\mathbb{R}^{p}$. Then\n$$\n\\nabla_{\\theta}\\lambda_{i}(\\theta)=\\exp(x_{i}^{\\top}\\theta)\\,x_{i}=\\lambda_{i}(\\theta)\\,x_{i},\n$$\nand hence\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\lambda_{i}(\\theta)\\,x_{i}x_{i}^{\\top}.\n$$\n\nNow take $p=2$, $n=3$ with\n$$\nx_{1}=\\begin{pmatrix}1\\\\ 0.5\\end{pmatrix},\\quad x_{2}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix},\\quad x_{3}=\\begin{pmatrix}1\\\\ 2\\end{pmatrix},\\quad \\theta_{0}=\\begin{pmatrix}-2\\\\ -1\\end{pmatrix}.\n$$\nEvaluate $\\lambda_{i}(\\theta_{0})=\\exp(x_{i}^{\\top}\\theta_{0})$:\n$$\nx_{1}^{\\top}\\theta_{0}=-2-0.5=-2.5,\\quad \\lambda_{1}=\\exp(-2.5),\n$$\n$$\nx_{2}^{\\top}\\theta_{0}=-2-1=-3,\\quad \\lambda_{2}=\\exp(-3),\n$$\n$$\nx_{3}^{\\top}\\theta_{0}=-2-2=-4,\\quad \\lambda_{3}=\\exp(-4).\n$$\nThus,\n$$\nI(\\theta_{0})=\\lambda_{1}\\begin{pmatrix}1 & 0.5 \\\\ 0.5 & 0.25\\end{pmatrix}+\\lambda_{2}\\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix}+\\lambda_{3}\\begin{pmatrix}1 & 2 \\\\ 2 & 4\\end{pmatrix}.\n$$\nNumerically,\n$$\n\\lambda_{1}=\\exp(-2.5)\\approx 0.08208499862,\\quad \\lambda_{2}=\\exp(-3)\\approx 0.04978706837,\\quad \\lambda_{3}=\\exp(-4)\\approx 0.01831563889,\n$$\nso\n$$\nI(\\theta_{0})\\approx\n\\begin{pmatrix}\n0.15018770588 & 0.12746084546 \\\\\n0.12746084546 & 0.14357087359\n\\end{pmatrix}.\n$$\n\nWe seek the Cramér–Rao lower bound (CRLB) for any unbiased estimator of $c^{\\top}\\theta$ with $c=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. For a differentiable regular model, the CRLB for a linear functional is\n$$\n\\mathrm{Var}(\\widehat{c^{\\top}\\theta})\\;\\ge\\; c^{\\top}I(\\theta)^{-1}c.\n$$\nHere this equals the $(2,2)$ element of $I(\\theta_{0})^{-1}$. For a symmetric $2\\times 2$ matrix $I=\\begin{pmatrix}a & b\\\\ b & d\\end{pmatrix}$, we have\n$$\nI^{-1}=\\frac{1}{ad-b^{2}}\\begin{pmatrix}d & -b\\\\ -b & a\\end{pmatrix},\\quad \\Rightarrow\\quad \\left[I^{-1}\\right]_{22}=\\frac{a}{ad-b^{2}}.\n$$\nUsing $a=0.15018770588$, $b=0.12746084546$, $d=0.14357087359$, compute\n$$\nad\\approx 0.02156258014,\\quad b^{2}\\approx 0.01624626712,\\quad ad-b^{2}\\approx 0.00531631302,\n$$\nso\n$$\nc^{\\top}I(\\theta_{0})^{-1}c=\\frac{a}{ad-b^{2}}\\approx \\frac{0.15018770588}{0.00531631302}\\approx 28.25035.\n$$\nRounding to $4$ significant figures gives $28.25$.\n\nDiscussion of low-count regime: When $\\lambda_{i}(\\theta)$ are small, $I(\\theta)=\\sum_{i}\\lambda_{i}(\\theta)\\,x_{i}x_{i}^{\\top}$ becomes small in the positive semidefinite order, causing its inverse to inflate and the Cramér–Rao lower bound $c^{\\top}I(\\theta)^{-1}c$ to grow. Thus, low photon counts or sparse events degrade local identifiability and inflate variance lower bounds. Moreover, the asymptotic normal approximations underlying the Cramér–Rao lower bound may be less accurate in finite samples when many $y_{i}$ equal $0$, leading to skewed likelihoods and potentially poor tightness of the bound. In such regimes, exact or nonasymptotic bounds (for example, the Barankin bound or the Chapman–Robbins bound) and regularization-informed priors in Bayesian data assimilation can be more informative, while reparameterizations ensuring positivity, such as the log link used here, maintain differentiability but do not eliminate the fundamental information scarcity induced by small $\\lambda_{i}(\\theta)$.",
            "answer": "$$\\boxed{28.25}$$"
        },
        {
            "introduction": "Physical parameters in data assimilation models are often constrained, such as variances which must be positive. This exercise explores a powerful technique for this by using an exponential map, $\\theta = \\exp(\\psi)$, to enforce positivity on a variance parameter . You will practice deriving the Fisher information for the unconstrained parameter $\\psi$ and then use the delta method to translate the Cramér-Rao bound back to the original parameter $\\theta$, a fundamental skill in statistical modeling.",
            "id": "3381492",
            "problem": "Consider a data assimilation setting in which the model–data misfit consists of $n$ independent scalar residuals $r_{1},\\dots,r_{n}$, modeled as realizations of a zero-mean Gaussian distribution with unknown observation-error variance $\\theta > 0$. Specifically, the residual vector $\\mathbf{r} = (r_{1},\\dots,r_{n})^{\\top}$ has the likelihood\n$$\np(\\mathbf{r} | \\theta) = (2\\pi\\theta)^{-n/2} \\exp\\left( -\\frac{1}{2\\theta}\\sum_{i=1}^{n} r_i^{2} \\right),\n$$\nwhich is the standard Gaussian model–error likelihood in linear Gaussian data assimilation when the mean is known and only the scalar variance parameter $\\theta$ is unknown. To enforce positivity of the variance while enabling unconstrained optimization, introduce the reparameterization\n$$\n\\theta = \\exp(\\psi),\n$$\nso that $\\psi \\in \\mathbb{R}$.\n\nUsing only fundamental definitions of the score, Fisher information, and the Cramér–Rao bound, carry out the following:\n\n1. Derive the Fisher information for $\\psi$, denoted $I_\\psi(\\psi)$, under the above model.\n2. Starting from the Cramér–Rao bound for $\\psi$, use a first-order delta method argument to obtain a corresponding bound for the variance of any approximately unbiased estimator of $\\theta$.\n\nExpress your final result as a two-entry row matrix whose first entry is $I_\\psi(\\psi)$ and whose second entry is the mapped Cramér–Rao bound for $\\theta$ as a closed-form analytic expression in terms of $n$ and $\\psi$. No numerical approximation or rounding is required.",
            "solution": "### Solution Derivation\n\nThe solution proceeds in two parts as requested.\n\n**Part 1: Derivation of the Fisher Information $I_\\psi(\\psi)$**\n\nThe Fisher information for a parameter $\\psi$ is defined as the negative expected value of the second derivative of the log-likelihood function with respect to $\\psi$.\n$$\nI_\\psi(\\psi) = - \\mathbb{E} \\left[ \\frac{\\partial^2 \\ln p(\\mathbf{r}|\\psi)}{\\partial \\psi^2} \\right]\n$$\nFirst, we write the log-likelihood function, $\\mathcal{L}(\\psi) = \\ln p(\\mathbf{r}|\\psi)$. The likelihood is given in terms of $\\theta$, so we begin there.\n$$\n\\ln p(\\mathbf{r}|\\theta) = \\ln \\left[ (2\\pi\\theta)^{-n/2} \\exp\\left( -\\frac{1}{2\\theta}\\sum_{i=1}^{n} r_i^{2} \\right) \\right]\n$$\nLet $S = \\sum_{i=1}^{n} r_i^{2}$. The log-likelihood is:\n$$\n\\ln p(\\mathbf{r}|\\theta) = -\\frac{n}{2} \\ln(2\\pi\\theta) - \\frac{S}{2\\theta} = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\theta - \\frac{S}{2\\theta}\n$$\nNow, we use the reparameterization $\\theta = \\exp(\\psi)$, which implies $\\ln\\theta = \\psi$. Substituting this into the log-likelihood expression gives $\\mathcal{L}(\\psi)$:\n$$\n\\mathcal{L}(\\psi) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\psi - \\frac{S}{2\\exp(\\psi)} = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\psi - \\frac{S}{2}\\exp(-\\psi)\n$$\nNext, we compute the first and second derivatives of $\\mathcal{L}(\\psi)$ with respect to $\\psi$.\nThe first derivative (the score function for $\\psi$) is:\n$$\n\\frac{\\partial \\mathcal{L}(\\psi)}{\\partial \\psi} = \\frac{\\partial}{\\partial \\psi} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\psi - \\frac{S}{2}\\exp(-\\psi) \\right) = -\\frac{n}{2} - \\frac{S}{2} \\cdot (-\\exp(-\\psi)) = \\frac{S \\exp(-\\psi)}{2} - \\frac{n}{2}\n$$\nThe second derivative is:\n$$\n\\frac{\\partial^2 \\mathcal{L}(\\psi)}{\\partial \\psi^2} = \\frac{\\partial}{\\partial \\psi} \\left( \\frac{S \\exp(-\\psi)}{2} - \\frac{n}{2} \\right) = \\frac{S}{2} \\cdot (-\\exp(-\\psi)) = -\\frac{S}{2\\exp(\\psi)}\n$$\nTo find the Fisher information, we take the negative expectation of this second derivative. The expectation is taken over the distribution of the data $\\mathbf{r}$ for a fixed $\\psi$.\n$$\nI_\\psi(\\psi) = - \\mathbb{E} \\left[ -\\frac{S}{2\\exp(\\psi)} \\right] = \\frac{1}{2\\exp(\\psi)} \\mathbb{E}[S]\n$$\nWe need to calculate $\\mathbb{E}[S]$. The residuals $r_i$ are independent and identically distributed draws from a zero-mean Gaussian distribution with variance $\\theta$, i.e., $r_i \\sim N(0, \\theta)$. The expectation of the square of such a random variable is its variance:\n$$\n\\mathbb{E}[r_i^2] = \\text{Var}(r_i) + (\\mathbb{E}[r_i])^2 = \\theta + 0^2 = \\theta\n$$\nBy linearity of expectation, the expectation of the sum $S$ is:\n$$\n\\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{i=1}^{n} r_i^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}[r_i^2] = \\sum_{i=1}^{n} \\theta = n\\theta\n$$\nSubstituting this result back into the expression for $I_\\psi(\\psi)$:\n$$\nI_\\psi(\\psi) = \\frac{1}{2\\exp(\\psi)} (n\\theta)\n$$\nFinally, we replace $\\theta$ with its definition in terms of $\\psi$, $\\theta = \\exp(\\psi)$:\n$$\nI_\\psi(\\psi) = \\frac{n\\exp(\\psi)}{2\\exp(\\psi)} = \\frac{n}{2}\n$$\nThus, the Fisher information for $\\psi$ is a constant, independent of $\\psi$.\n\n**Part 2: Derivation of the Cramér–Rao Bound for $\\theta$**\n\nThe Cramér–Rao bound (CRB) states that for any unbiased estimator $\\hat{\\psi}$ of $\\psi$, its variance is bounded from below by the reciprocal of the Fisher information:\n$$\n\\text{Var}(\\hat{\\psi}) \\ge \\frac{1}{I_\\psi(\\psi)}\n$$\nUsing the result from Part 1, the CRB for $\\psi$ is:\n$$\n\\text{Var}(\\hat{\\psi}) \\ge \\frac{1}{n/2} = \\frac{2}{n}\n$$\nWe are asked to find the corresponding bound for an estimator of $\\theta$ using a first-order delta method argument. Let the transformation be $g(\\psi) = \\theta = \\exp(\\psi)$. An estimator for $\\theta$ can be formed by applying this transformation to an estimator for $\\psi$, i.e., $\\hat{\\theta} = g(\\hat{\\psi}) = \\exp(\\hat{\\psi})$.\n\nThe delta method approximates the variance of a function of a random variable. For an estimator $\\hat{\\theta} = g(\\hat{\\psi})$, the variance is asymptotically given by:\n$$\n\\text{Var}(\\hat{\\theta}) \\approx [g'(\\psi)]^2 \\text{Var}(\\hat{\\psi})\n$$\nwhere $g'(\\psi)$ is the derivative of the transformation function $g$ evaluated at the true parameter value $\\psi$.\nIn our case, $g(\\psi) = \\exp(\\psi)$, so its derivative is:\n$$\ng'(\\psi) = \\frac{d}{d\\psi} \\exp(\\psi) = \\exp(\\psi)\n$$\nThe Cramér–Rao bound for $\\theta$ is obtained by applying this transformation to the bound for $\\psi$. For any approximately unbiased estimator of $\\theta$ that is a function of an efficient estimator of $\\psi$, its variance will be bounded by:\n$$\n\\text{Var}(\\hat{\\theta}) \\ge [g'(\\psi)]^2 \\frac{1}{I_\\psi(\\psi)}\n$$\nSubstituting the expressions we found for $g'(\\psi)$ and $I_\\psi(\\psi)$:\n$$\n\\text{CRB}(\\theta) = [\\exp(\\psi)]^2 \\cdot \\frac{2}{n} = \\exp(2\\psi) \\frac{2}{n} = \\frac{2\\exp(2\\psi)}{n}\n$$\nThis is the Cramér–Rao bound for the variance of any approximately unbiased estimator of $\\theta$, expressed in terms of $n$ and $\\psi$.\n\nThe final result is a two-entry row matrix containing $I_\\psi(\\psi)$ and the derived bound for $\\theta$.\n\nFirst entry: $I_\\psi(\\psi) = \\frac{n}{2}$\nSecond entry: $\\text{CRB}(\\theta) = \\frac{2\\exp(2\\psi)}{n}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{n}{2} & \\frac{2\\exp(2\\psi)}{n} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In practice, our statistical models are convenient approximations rather than perfect descriptions of reality. This exercise confronts the critical issue of model misspecification, where data with heavy tails (from a Student-$t$ distribution with $\\nu$ degrees of freedom) is incorrectly analyzed using a Gaussian likelihood . You will derive the Godambe (or \"sandwich\") information, a robust measure of uncertainty, and see precisely how much a naive analysis can underestimate the true variance of an estimator.",
            "id": "3381484",
            "problem": "Consider a linear inverse observation model used in data assimilation in which a scalar calibration parameter $\\,\\theta\\,$ multiplies a known template $\\,g_{i}\\,$ across $\\,n\\,$ independent observations. The forward model is $\\,y_{i} = \\theta\\, g_{i} + \\varepsilon_{i}\\,$ for $\\,i = 1, \\dots, n\\,$, where the regressors $\\,g_{i}\\,$ are known real numbers. The true observation errors $\\,\\varepsilon_{i}\\,$ are independently and identically distributed according to a Student-$t$ distribution with degrees of freedom $\\,\\nu > 2\\,$, mean $\\,0\\,$, and scale parameter $\\,\\sigma\\,$, so that $\\,\\operatorname{Var}(\\varepsilon_{i})\\,$ exists and is finite.\n\nA quasi-likelihood estimator is constructed by maximizing a Gaussian pseudo-likelihood that assumes $\\,\\varepsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})\\,$, using the same known $\\,\\sigma\\,$ as the true scale but ignoring heavy tails. Work from first principles, using only core definitions of the score function, information, and asymptotic covariance of estimators, to:\n\n- define the quasi-score for $\\,\\theta\\,$ under the Gaussian pseudo-model and identify its sensitivity and variability under the true Student-$t$ data-generating mechanism,\n- construct the Godambe (sandwich) information for $\\,\\theta\\,$ under this misspecification,\n- deduce the asymptotic Cramér-Rao lower bound (CRLB) for unbiased estimators based on the Godambe information, and\n- compute the naive Fisher information and its associated CRLB under the Gaussian misspecification.\n\nFinally, provide a single closed-form analytic expression for the ratio of the Godambe-based CRLB to the naive Fisher-based CRLB, simplified entirely in terms of $\\,\\nu\\,$. No rounding is required. The final answer must be an analytic expression without units.",
            "solution": "The problem requires the derivation of the ratio between the Cramér-Rao lower bound (CRLB) based on the Godambe (sandwich) information and the naive CRLB based on a misspecified Gaussian likelihood. The true data-generating process follows a Student-$t$ distribution, while the estimation is based on a Gaussian pseudo-likelihood. We proceed from first principles.\n\nThe forward model for the $n$ independent observations is given by:\n$$y_{i} = \\theta g_{i} + \\varepsilon_{i}, \\quad i = 1, \\dots, n$$\nwhere $\\theta$ is the scalar parameter of interest, $g_{i}$ are known constants, and $\\varepsilon_{i}$ are the observation errors.\n\nThe true distribution of the errors is independent and identically distributed (i.i.d.) according to a Student-$t$ distribution with $\\nu > 2$ degrees of freedom, mean $0$, and scale parameter $\\sigma$. We denote this true data-generating process by $P$. The condition $\\nu > 2$ ensures that the variance is finite. The variance of $\\varepsilon_{i}$ under $P$ is given by:\n$$\\operatorname{Var}_{P}(\\varepsilon_{i}) = \\sigma^{2} \\frac{\\nu}{\\nu - 2}$$\n\nThe quasi-likelihood estimator for $\\theta$ is constructed by maximizing a pseudo-likelihood that incorrectly assumes the errors are Gaussian, i.e., $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$. Let this pseudo-model be denoted by $\\psi$. Under this assumption, $y_{i} \\sim \\mathcal{N}(\\theta g_{i}, \\sigma^{2})$.\n\nThe pseudo-log-likelihood for a single observation $y_{i}$ is:\n$$l_{i}(\\theta) = \\ln \\psi(y_{i}; \\theta) = -\\frac{1}{2} \\ln(2\\pi\\sigma^{2}) - \\frac{(y_{i} - \\theta g_{i})^{2}}{2\\sigma^{2}}$$\nThe total pseudo-log-likelihood is $L(\\theta) = \\sum_{i=1}^{n} l_{i}(\\theta)$.\n\nThe quasi-score function is the derivative of $L(\\theta)$ with respect to $\\theta$:\n$$S(\\theta) = \\frac{\\partial L(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n} \\frac{\\partial l_{i}(\\theta)}{\\partial \\theta}$$\nThe contribution from the $i$-th observation is:\n$$s_{i}(\\theta) = \\frac{\\partial l_{i}(\\theta)}{\\partial \\theta} = -\\frac{1}{2\\sigma^{2}} \\cdot 2(y_{i} - \\theta g_{i})(-g_{i}) = \\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}$$\nSo, the total quasi-score is $S(\\theta) = \\sum_{i=1}^{n} \\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}$.\n\nTo analyze the estimator, we evaluate the score at the true parameter value, $\\theta_{0}$. From the true model, $y_i - \\theta_0 g_i = \\varepsilon_i$.\n$$S(\\theta_{0}) = \\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}$$\n\nThe Godambe information, $J_{G}(\\theta)$, is constructed from two components: the sensitivity, which we denote $A(\\theta)$, and the variability, which we denote $B(\\theta)$.\n\nFirst, we define and compute the sensitivity, $A(\\theta_0)$. This is the expectation of the negative Hessian of the pseudo-log-likelihood, evaluated under the pseudo-model $\\psi$.\n$$A(\\theta) = -\\mathbb{E}_{\\psi}\\left[\\frac{\\partial^{2} L(\\theta)}{\\partial \\theta^{2}}\\right]$$\nThe second derivative is:\n$$\\frac{\\partial^{2} L(\\theta)}{\\partial \\theta^{2}} = \\sum_{i=1}^{n} \\frac{\\partial s_{i}(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} \\left(\\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}\\right) = \\sum_{i=1}^{n} \\frac{-g_{i}^{2}}{\\sigma^{2}}$$\nSince this expression does not depend on the data $y_{i}$, the expectation is the expression itself.\n$$A(\\theta_{0}) = -\\left(\\sum_{i=1}^{n} \\frac{-g_{i}^{2}}{\\sigma^{2}}\\right) = \\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}$$\nThis quantity, $A(\\theta_{0})$, is also the naive Fisher information, $I_{F, \\text{naive}}(\\theta_{0})$, as it is the Fisher information calculated under the misspecified Gaussian model.\n\nSecond, we define and compute the variability, $B(\\theta_0)$. This is the variance of the quasi-score function, evaluated under the true data-generating process $P$.\n$$B(\\theta_{0}) = \\operatorname{Var}_{P}(S(\\theta_{0})) = \\mathbb{E}_{P}[S(\\theta_{0})^{2}] - (\\mathbb{E}_{P}[S(\\theta_{0})])^{2}$$\nThe expectation of the score under the true model is:\n$$\\mathbb{E}_{P}[S(\\theta_{0})] = \\mathbb{E}_{P}\\left[\\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}\\right] = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} g_{i} \\mathbb{E}_{P}[\\varepsilon_{i}]$$\nSince the true distribution of $\\varepsilon_{i}$ has mean $0$, $\\mathbb{E}_{P}[S(\\theta_{0})] = 0$. Therefore, the variability is the expectation of the squared score:\n$$B(\\theta_{0}) = \\mathbb{E}_{P}[S(\\theta_{0})^{2}] = \\mathbb{E}_{P}\\left[\\left(\\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}\\right)^{2}\\right] = \\frac{1}{\\sigma^{4}} \\mathbb{E}_{P}\\left[\\left(\\sum_{i=1}^{n} \\varepsilon_{i} g_{i}\\right)^{2}\\right]$$\nBecause the errors $\\varepsilon_{i}$ are independent, $\\mathbb{E}_{P}[\\varepsilon_{i}\\varepsilon_{j}] = \\mathbb{E}_{P}[\\varepsilon_{i}]\\mathbb{E}_{P}[\\varepsilon_{j}] = 0$ for $i \\neq j$. The expression simplifies to:\n$$B(\\theta_{0}) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{n} g_{i}^{2} \\mathbb{E}_{P}[\\varepsilon_{i}^{2}]$$\nSince $\\mathbb{E}_{P}[\\varepsilon_{i}] = 0$, we have $\\mathbb{E}_{P}[\\varepsilon_{i}^{2}] = \\operatorname{Var}_{P}(\\varepsilon_{i}) = \\sigma^{2}\\frac{\\nu}{\\nu-2}$. Substituting this in:\n$$B(\\theta_{0}) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{n} g_{i}^{2} \\left(\\sigma^{2}\\frac{\\nu}{\\nu-2}\\right) = \\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu}{\\nu-2}$$\n\nThe Godambe (sandwich) information for a scalar parameter is defined as $J_{G}(\\theta) = A(\\theta)^{2} / B(\\theta)$.\n$$J_{G}(\\theta_{0}) = \\frac{\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right)^{2}}{\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu}{\\nu-2}} = \\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu-2}{\\nu}$$\n\nThe asymptotic CRLB for unbiased estimators under misspecification is the inverse of the Godambe information.\n$$CRLB_{G} = [J_{G}(\\theta_{0})]^{-1} = \\left[\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu-2}{\\nu}\\right]^{-1} = \\left(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}\\right) \\frac{\\nu}{\\nu-2}$$\nThis is equivalent to the asymptotic variance of the quasi-likelihood estimator, given by $B(\\theta_{0}) / A(\\theta_{0})^{2}$.\n\nThe naive Fisher information was already identified as $I_{F, \\text{naive}}(\\theta_{0}) = A(\\theta_{0}) = \\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}$.\nThe associated naive CRLB is its inverse:\n$$CRLB_{\\text{naive}} = [I_{F, \\text{naive}}(\\theta_{0})]^{-1} = \\left[\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right]^{-1} = \\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}$$\nThis is the lower bound one would incorrectly compute by assuming the Gaussian model is true.\n\nFinally, we compute the ratio of the Godambe-based CRLB to the naive CRLB:\n$$\\frac{CRLB_{G}}{CRLB_{\\text{naive}}} = \\frac{\\left(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}\\right) \\frac{\\nu}{\\nu-2}}{\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}}$$\nThe common term $\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}$ cancels out, leaving:\n$$\\frac{CRLB_{G}}{CRLB_{\\text{naive}}} = \\frac{\\nu}{\\nu-2}$$\nThis ratio quantifies the loss of efficiency of the estimator due to using a Gaussian likelihood when the true errors have heavier tails described by a Student-$t$ distribution. As $\\nu \\to \\infty$, the Student-$t$ converges to a Gaussian, and the ratio approaches $1$. As $\\nu \\to 2^{+}$, the variance of the true error distribution approaches infinity, and the ratio approaches infinity, indicating a severe loss of efficiency.",
            "answer": "$$\\boxed{\\frac{\\nu}{\\nu-2}}$$"
        }
    ]
}