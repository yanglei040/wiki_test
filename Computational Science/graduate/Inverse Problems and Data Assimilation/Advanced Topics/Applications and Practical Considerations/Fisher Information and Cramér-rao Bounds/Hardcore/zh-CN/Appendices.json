{
    "hands_on_practices": [
        {
            "introduction": "理论知识只有通过实践才能巩固。本节的第一个练习将带领我们超越标准的高斯假设，为泊松分布数据推导费雪信息矩阵。这个练习不仅强化了我们对数似然函数、得分函数和费雪信息之间基本关系的理解，而且直接关联到光子计数、事件发生率估计等众多实际应用场景。",
            "id": "3381487",
            "problem": "在光子受限的数据同化背景下，假设 $n$ 个独立的传感器计数 $y_{i}$（$i \\in \\{1,\\dots,n\\}$）服从泊松分布 $y_{i} \\sim \\mathrm{Poisson}(\\lambda_{i}(\\theta))$，其中 $\\theta \\in \\mathbb{R}^{p}$ 是一个未知参数矢量，且每个正演率函数 $\\lambda_{i} : \\mathbb{R}^{p} \\to \\mathbb{R}_{0}$ 是连续可微的。从基于似然推断的第一性原理出发，从Fisher信息矩阵的定义（即数据模型下得分向量的外积的期望）开始，推导Fisher信息矩阵 $I(\\theta)$ 关于 $\\{\\lambda_{i}(\\theta)\\}_{i=1}^{n}$ 及其相对于 $\\theta$ 的梯度的一般表达式。然后，将其特化到对数线性正演模型 $\\lambda_{i}(\\theta) = \\exp(x_{i}^{\\top}\\theta)$，其中 $x_{i} \\in \\mathbb{R}^{p}$ 是已知的。\n\n接下来，考虑 $p=2$ 和 $n=3$ 的情况，其中\n$$\nx_{1}=\\begin{pmatrix}1 \\\\ 0.5\\end{pmatrix},\\quad x_{2}=\\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\\quad x_{3}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix},\n$$\n且真实参数为 $\\theta_{0}=\\begin{pmatrix}-2 \\\\ -1\\end{pmatrix}$。使用你推导的Fisher信息表达式，计算在 $\\theta=\\theta_{0}$ 处，对于线性泛函 $c^{\\top}\\theta$ 的任意无偏估计量的 Cramér–Rao 下界 (CRLB)，其中 $c=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。将最终数值答案表示为单个实数，并四舍五入到 $4$ 位有效数字。\n\n最后，在不改变你的数值结果的情况下，简要讨论低计数状态（即 $\\lambda_{i}(\\theta)$ 很小）如何影响Fisher信息以及 Cramér–Rao 下界在有限样本的反演问题和数据同化中的实际紧致性。",
            "solution": "我们从参数为 $\\theta \\in \\mathbb{R}^{p}$ 的参数族的Fisher信息矩阵的定义开始，\n$$\nI(\\theta) \\equiv \\mathbb{E}_{\\theta}\\!\\left[\\,\\nabla_{\\theta} \\ell(\\theta; y)\\,\\nabla_{\\theta} \\ell(\\theta; y)^{\\top}\\right],\n$$\n其中 $\\ell(\\theta; y)$ 是对数似然，期望是关于参数为 $\\theta$ 时的数据分布计算的。对于独立观测值 $\\{y_{i}\\}_{i=1}^{n}$，其中 $y_{i} \\sim \\mathrm{Poisson}(\\lambda_{i}(\\theta))$，其联合似然函数为\n$$\nL(\\theta; y)=\\prod_{i=1}^{n}\\frac{\\exp(-\\lambda_{i}(\\theta))\\,\\lambda_{i}(\\theta)^{y_{i}}}{y_{i}!},\n$$\n其对数似然函数为\n$$\n\\ell(\\theta; y)=\\sum_{i=1}^{n}\\left[-\\lambda_{i}(\\theta)+y_{i}\\ln \\lambda_{i}(\\theta)-\\ln(y_{i}!)\\right].\n$$\n微分得到得分向量：\n$$\n\\nabla_{\\theta}\\ell(\\theta; y)=\\sum_{i=1}^{n}\\left[-\\nabla_{\\theta}\\lambda_{i}(\\theta)+y_{i}\\,\\nabla_{\\theta}\\ln \\lambda_{i}(\\theta)\\right]\n=\\sum_{i=1}^{n}\\left[-\\nabla_{\\theta}\\lambda_{i}(\\theta)+\\frac{y_{i}}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)\\right]\n=\\sum_{i=1}^{n}\\frac{y_{i}-\\lambda_{i}(\\theta)}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta).\n$$\n利用独立性，和的外积的期望等于外积的期望的和，因为当 $i \\neq j$ 时，由于协方差为零，交叉项会消失：\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\mathbb{E}_{\\theta}\\!\\left[\\left(\\frac{y_{i}-\\lambda_{i}(\\theta)}{\\lambda_{i}(\\theta)}\\right)^{2}\\right]\\nabla_{\\theta}\\lambda_{i}(\\theta)\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)^{\\top}.\n$$\n对于泊松随机变量，$\\mathrm{Var}(y_{i})=\\lambda_{i}(\\theta)$ 且 $\\mathbb{E}\\!\\left[(y_{i}-\\lambda_{i}(\\theta))^{2}\\right]=\\lambda_{i}(\\theta)$。因此，\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\frac{1}{\\lambda_{i}(\\theta)}\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)\\,\\nabla_{\\theta}\\lambda_{i}(\\theta)^{\\top}.\n$$\n\n特化到对数线性模型 $\\lambda_{i}(\\theta)=\\exp(x_{i}^{\\top}\\theta)$，其中 $x_{i}\\in\\mathbb{R}^{p}$。那么\n$$\n\\nabla_{\\theta}\\lambda_{i}(\\theta)=\\exp(x_{i}^{\\top}\\theta)\\,x_{i}=\\lambda_{i}(\\theta)\\,x_{i},\n$$\n因此\n$$\nI(\\theta)=\\sum_{i=1}^{n}\\lambda_{i}(\\theta)\\,x_{i}x_{i}^{\\top}.\n$$\n\n现在取 $p=2$, $n=3$ 且\n$$\nx_{1}=\\begin{pmatrix}1\\\\ 0.5\\end{pmatrix},\\quad x_{2}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix},\\quad x_{3}=\\begin{pmatrix}1\\\\ 2\\end{pmatrix},\\quad \\theta_{0}=\\begin{pmatrix}-2\\\\ -1\\end{pmatrix}.\n$$\n计算 $\\lambda_{i}(\\theta_{0})=\\exp(x_{i}^{\\top}\\theta_{0})$：\n$$\nx_{1}^{\\top}\\theta_{0}=-2-0.5=-2.5,\\quad \\lambda_{1}=\\exp(-2.5),\n$$\n$$\nx_{2}^{\\top}\\theta_{0}=-2-1=-3,\\quad \\lambda_{2}=\\exp(-3),\n$$\n$$\nx_{3}^{\\top}\\theta_{0}=-2-2=-4,\\quad \\lambda_{3}=\\exp(-4).\n$$\n因此，\n$$\nI(\\theta_{0})=\\lambda_{1}\\begin{pmatrix}1  0.5\\\\ 0.5  0.25\\end{pmatrix}+\\lambda_{2}\\begin{pmatrix}1  1\\\\ 1  1\\end{pmatrix}+\\lambda_{3}\\begin{pmatrix}1  2\\\\ 2  4\\end{pmatrix}.\n$$\n数值上，\n$$\n\\lambda_{1}=\\exp(-2.5)\\approx 0.08208499862,\\quad \\lambda_{2}=\\exp(-3)\\approx 0.04978706837,\\quad \\lambda_{3}=\\exp(-4)\\approx 0.01831563889,\n$$\n所以\n$$\nI(\\theta_{0})\\approx\n\\begin{pmatrix}\n0.15018770588  0.12746084546\\\\\n0.12746084546  0.14357087359\n\\end{pmatrix}.\n$$\n\n我们寻求 $c^{\\top}\\theta$ 的任意无偏估计量的 Cramér–Rao 下界 (CRLB)，其中 $c=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$。对于一个可微的正则模型，线性泛函的CRLB为\n$$\n\\mathrm{Var}(\\widehat{c^{\\top}\\theta})\\;\\ge\\; c^{\\top}I(\\theta)^{-1}c.\n$$\n在这里，它等于 $I(\\theta_{0})^{-1}$ 的 $(2,2)$ 元素。对于一个对称的 $2\\times 2$ 矩阵 $I=\\begin{pmatrix}a  b\\\\ b  d\\end{pmatrix}$，我们有\n$$\nI^{-1}=\\frac{1}{ad-b^{2}}\\begin{pmatrix}d  -b\\\\ -b  a\\end{pmatrix},\\quad \\Rightarrow\\quad \\left[I^{-1}\\right]_{22}=\\frac{a}{ad-b^{2}}.\n$$\n使用 $a=0.15018770588$, $b=0.12746084546$, $d=0.14357087359$, 计算\n$$\nad\\approx 0.02156258014,\\quad b^{2}\\approx 0.01624626712,\\quad ad-b^{2}\\approx 0.00531631302,\n$$\n所以\n$$\nc^{\\top}I(\\theta_{0})^{-1}c=\\frac{a}{ad-b^{2}}\\approx \\frac{0.15018770588}{0.00531631302}\\approx 28.25035.\n$$\n四舍五入到 $4$ 位有效数字，得到 $28.25$。\n\n关于低计数状态的讨论：当 $\\lambda_{i}(\\theta)$ 很小时，在半正定序下，$I(\\theta)=\\sum_{i}\\lambda_{i}(\\theta)\\,x_{i}x_{i}^{\\top}$ 会变小，导致其逆矩阵膨胀，Cramér–Rao 下界 $c^{\\top}I(\\theta)^{-1}c$ 增大。因此，低光子计数或稀疏事件会降低局部可辨识性并抬高方差下界。此外，当许多 $y_{i}$ 等于 $0$ 时，Cramér–Rao 下界所依赖的渐近正态近似在有限样本中可能不太准确，导致似然函数偏斜，并可能使该界的紧致性变差。在这种情况下，精确或非渐近的界（例如 Barankin 界或 Chapman–Robbins 界）以及贝叶斯数据同化中由正则化提供的先验可能更具信息量，而确保正性的重参数化方法，例如此处使用的对数链接，虽然保持了可微性，但并不能消除由小的 $\\lambda_{i}(\\theta)$ 引起的基本信息稀缺性。",
            "answer": "$$\\boxed{28.25}$$"
        },
        {
            "introduction": "在许多逆问题和数据同化场景中，物理参数（如方差或浓度）必须为正。这个练习介绍了一种优雅而强大的技术——重参数化——来处理这类约束。我们将学习如何为一个新的无约束参数计算费雪信息，并利用“德尔塔方法” (delta method) 将克拉默-拉奥下界转换回我们最初关心的有约束参数上，从而正确量化其不确定性。",
            "id": "3381492",
            "problem": "考虑一个数据同化情景，其中模型与数据的失配由 $n$ 个独立的标量残差 $r_{1},\\dots,r_{n}$ 组成。这些残差被建模为具有未知观测误差方差 $\\theta > 0$ 的零均值高斯分布的实现。具体来说，残差向量 $\\mathbf{r} = (r_{1},\\dots,r_{n})^{\\top}$ 的似然函数为\n$$\np(\\mathbf{r} | \\theta) = (2\\pi\\theta)^{-n/2} \\exp\\left( -\\frac{1}{2\\theta}\\sum_{i=1}^{n} r_i^{2} \\right),\n$$\n当均值已知，仅标量方差参数 $\\theta$ 未知时，这是线性高斯数据同化中的标准高斯模型误差似然。为了在进行无约束优化的同时强制方差为正，引入重参数化\n$$\n\\theta = \\exp(\\psi),\n$$\n使得 $\\psi \\in \\mathbb{R}$。\n\n仅使用得分函数、费雪信息和克拉默-拉奥下界的基本定义，完成以下任务：\n\n1. 在上述模型下，推导 $\\psi$ 的费雪信息，记为 $I_\\psi(\\psi)$。\n2. 从 $\\psi$ 的克拉默-拉奥下界出发，使用一阶 delta 方法推导 $\\theta$ 的任何近似无偏估计量的方差对应的下界。\n\n将您的最终结果表示为一个包含两个元素的行矩阵，其第一个元素是 $I_\\psi(\\psi)$，第二个元素是用 $n$ 和 $\\psi$ 表示的 $\\theta$ 的映射克拉默-拉奥下界的闭式解析表达式。无需进行数值近似或舍入。",
            "solution": "**第1部分：费雪信息 $I_\\psi(\\psi)$ 的推导**\n\n参数 $\\psi$ 的费雪信息定义为对数似然函数关于 $\\psi$ 的二阶导数的负期望值。\n$$\nI_\\psi(\\psi) = - \\mathbb{E} \\left[ \\frac{\\partial^2 \\ln p(\\mathbf{r}|\\psi)}{\\partial \\psi^2} \\right]\n$$\n首先，我们写出对数似然函数 $\\mathcal{L}(\\psi) = \\ln p(\\mathbf{r}|\\psi)$。似然函数是用 $\\theta$ 表示的，所以我们从那里开始。\n$$\n\\ln p(\\mathbf{r}|\\theta) = \\ln \\left[ (2\\pi\\theta)^{-n/2} \\exp\\left( -\\frac{1}{2\\theta}\\sum_{i=1}^{n} r_i^{2} \\right) \\right]\n$$\n令 $S = \\sum_{i=1}^{n} r_i^{2}$。对数似然函数为：\n$$\n\\ln p(\\mathbf{r}|\\theta) = -\\frac{n}{2} \\ln(2\\pi\\theta) - \\frac{S}{2\\theta} = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\theta - \\frac{S}{2\\theta}\n$$\n现在，我们使用重参数化 $\\theta = \\exp(\\psi)$，这意味着 $\\ln\\theta = \\psi$。将此代入对数似然表达式，得到 $\\mathcal{L}(\\psi)$：\n$$\n\\mathcal{L}(\\psi) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\psi - \\frac{S}{2\\exp(\\psi)} = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\psi - \\frac{S}{2}\\exp(-\\psi)\n$$\n接下来，我们计算 $\\mathcal{L}(\\psi)$ 关于 $\\psi$ 的一阶和二阶导数。\n一阶导数（$\\psi$ 的得分函数）是：\n$$\n\\frac{\\partial \\mathcal{L}(\\psi)}{\\partial \\psi} = \\frac{\\partial}{\\partial \\psi} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\psi - \\frac{S}{2}\\exp(-\\psi) \\right) = -\\frac{n}{2} - \\frac{S}{2} \\cdot (-\\exp(-\\psi)) = \\frac{S \\exp(-\\psi)}{2} - \\frac{n}{2}\n$$\n二阶导数是：\n$$\n\\frac{\\partial^2 \\mathcal{L}(\\psi)}{\\partial \\psi^2} = \\frac{\\partial}{\\partial \\psi} \\left( \\frac{S \\exp(-\\psi)}{2} - \\frac{n}{2} \\right) = \\frac{S}{2} \\cdot (-\\exp(-\\psi)) = -\\frac{S}{2\\exp(\\psi)}\n$$\n为了求得费雪信息，我们取这个二阶导数的负期望。期望是在固定 $\\psi$ 的情况下对数据 $\\mathbf{r}$ 的分布求得的。\n$$\nI_\\psi(\\psi) = - \\mathbb{E} \\left[ -\\frac{S}{2\\exp(\\psi)} \\right] = \\frac{1}{2\\exp(\\psi)} \\mathbb{E}[S]\n$$\n我们需要计算 $\\mathbb{E}[S]$。残差 $r_i$ 是从均值为零、方差为 $\\theta$ 的高斯分布中独立同分布地抽取的，即 $r_i \\sim N(0, \\theta)$。这样一个随机变量平方的期望是其方差：\n$$\n\\mathbb{E}[r_i^2] = \\text{Var}(r_i) + (\\mathbb{E}[r_i])^2 = \\theta + 0^2 = \\theta\n$$\n根据期望的线性性质，$S$ 的期望是：\n$$\n\\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{i=1}^{n} r_i^2\\right] = \\sum_{i=1}^{n} \\mathbb{E}[r_i^2] = \\sum_{i=1}^{n} \\theta = n\\theta\n$$\n将此结果代回 $I_\\psi(\\psi)$ 的表达式中：\n$$\nI_\\psi(\\psi) = \\frac{1}{2\\exp(\\psi)} (n\\theta)\n$$\n最后，我们用 $\\psi$ 的定义替换 $\\theta$，即 $\\theta = \\exp(\\psi)$：\n$$\nI_\\psi(\\psi) = \\frac{n\\exp(\\psi)}{2\\exp(\\psi)} = \\frac{n}{2}\n$$\n因此，$\\psi$ 的费雪信息是一个常数，与 $\\psi$ 无关。\n\n**第2部分：$\\theta$ 的克拉默-拉奥下界的推导**\n\n克拉默-拉奥下界 (CRLB) 指出，对于 $\\psi$ 的任何无偏估计量 $\\hat{\\psi}$，其方差的下界是费雪信息的倒数：\n$$\n\\text{Var}(\\hat{\\psi}) \\ge \\frac{1}{I_\\psi(\\psi)}\n$$\n使用第1部分的结果，$\\psi$ 的克拉默-拉奥下界为：\n$$\n\\text{Var}(\\hat{\\psi}) \\ge \\frac{1}{n/2} = \\frac{2}{n}\n$$\n我们被要求使用一阶 delta 方法来找到 $\\theta$ 的估计量的相应下界。设变换为 $g(\\psi) = \\theta = \\exp(\\psi)$。$\\theta$ 的估计量可以通过将此变换应用于 $\\psi$ 的估计量来形成，即 $\\hat{\\theta} = g(\\hat{\\psi}) = \\exp(\\hat{\\psi})$。\n\nDelta 方法近似了随机变量函数的方差。对于估计量 $\\hat{\\theta} = g(\\hat{\\psi})$，其方差渐近地由下式给出：\n$$\n\\text{Var}(\\hat{\\theta}) \\approx [g'(\\psi)]^2 \\text{Var}(\\hat{\\psi})\n$$\n其中 $g'(\\psi)$ 是变换函数 $g$ 在真实参数值 $\\psi$ 处的导数。\n在我们的例子中，$g(\\psi) = \\exp(\\psi)$，所以它的导数是：\n$$\ng'(\\psi) = \\frac{d}{d\\psi} \\exp(\\psi) = \\exp(\\psi)\n$$\n$\\theta$ 的克拉默-拉奥下界是通过将此变换应用于 $\\psi$ 的下界而得到的。对于任何作为 $\\psi$ 的有效估计量的函数的近似无偏估计量 $\\hat{\\theta}$，其方差将受以下界限制：\n$$\n\\text{Var}(\\hat{\\theta}) \\ge [g'(\\psi)]^2 \\frac{1}{I_\\psi(\\psi)}\n$$\n代入我们找到的 $g'(\\psi)$ 和 $I_\\psi(\\psi)$ 的表达式：\n$$\n\\text{CRB}(\\theta) = [\\exp(\\psi)]^2 \\cdot \\frac{2}{n} = \\exp(2\\psi) \\frac{2}{n} = \\frac{2\\exp(2\\psi)}{n}\n$$\n这是 $\\theta$ 的任何近似无偏估计量的方差的克拉默-拉奥下界，用 $n$ 和 $\\psi$ 表示。\n\n最终结果是一个包含两个元素的行矩阵，其第一个元素是 $I_\\psi(\\psi)$，第二个元素是用 $n$ 和 $\\psi$ 表示的 $\\theta$ 的映射克拉默-拉奥下界的闭式解析表达式。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{n}{2}  \\frac{2\\exp(2\\psi)}{n} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "我们构建的模型几乎总是对复杂现实的一种简化，因此模型误设是数据分析中一个不可避免的问题。这个练习直面这一挑战，探讨了当真实误差分布具有比高斯分布更“重”的尾部时，误用高斯似然函数进行估计的后果。我们将构造Godambe（三明治）信息矩阵，这是一个在模型误设下稳健量化不确定性的重要工具，并精确计算由于模型错误而导致的估计精度损失。",
            "id": "3381484",
            "problem": "考虑一个用于数据同化的线性逆观测模型，其中标量校准参数 $\\theta$ 乘以一个已知的模板 $g_{i}$，遍及 $n$ 个独立观测值。正向模型为 $y_{i} = \\theta g_{i} + \\varepsilon_{i}$，对于 $i = 1, \\dots, n$，其中回归量 $g_{i}$ 是已知的实数。真实的观测误差 $\\varepsilon_{i}$ 是独立同分布的，服从自由度为 $\\nu  2$、均值为 $0$、尺度参数为 $\\sigma$ 的学生t分布，因此 $\\operatorname{Var}(\\varepsilon_{i})$ 存在且为有限值。\n\n通过最大化一个高斯伪似然来构建一个拟似然估计量，该伪似然假设 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$，使用了与真实尺度参数相同的已知 $\\sigma$，但忽略了重尾特性。从第一性原理出发，仅使用得分函数、信息和估计量的渐近协方差的核心定义，来：\n\n- 在高斯伪模型下定义 $\\theta$ 的拟得分，并确定其在真实的学生t数据生成机制下的敏感度和变异性，\n- 在此模型误设下构建 $\\theta$ 的 Godambe（三明治）信息，\n- 基于 Godambe 信息推导无偏估计量的渐近克拉默-拉奥下界（CRLB），以及\n- 在高斯模型误设下计算朴素Fisher信息及其相关的CRLB。\n\n最后，给出一个基于 Godambe 的 CRLB 与基于朴素 Fisher 的 CRLB 之比的单一闭式解析表达式，该表达式完全用 $\\nu$ 来简化。无需四舍五入。最终答案必须是一个无单位的解析表达式。",
            "solution": "该问题要求推导基于 Godambe（三明治）信息的克拉默-拉奥下界（CRLB）与基于模型误设的高斯似然的朴素 CRLB 之间的比率。真实的数据生成过程服从学生t分布，而估计是基于高斯伪似然的。我们从第一性原理出发。\n\n$n$ 个独立观测值的正向模型由下式给出：\n$$y_{i} = \\theta g_{i} + \\varepsilon_{i}, \\quad i = 1, \\dots, n$$\n其中 $\\theta$ 是我们关心的标量参数，$g_{i}$ 是已知常数，$\\varepsilon_{i}$ 是观测误差。\n\n误差的真实分布是独立同分布（i.i.d.）的，服从自由度为 $\\nu  2$、均值为 $0$、尺度参数为 $\\sigma$ 的学生t分布。我们将此真实数据生成过程表示为 $P$。条件 $\\nu  2$ 确保了方差是有限的。在 $P$ 下，$\\varepsilon_{i}$ 的方差由下式给出：\n$$\\operatorname{Var}_{P}(\\varepsilon_{i}) = \\sigma^{2} \\frac{\\nu}{\\nu - 2}$$\n\n$\\theta$ 的拟似然估计量是通过最大化一个伪似然来构建的，该伪似然错误地假设误差是高斯的，即 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$。我们将此伪模型表示为 $\\psi$。在此假设下，$y_{i} \\sim \\mathcal{N}(\\theta g_{i}, \\sigma^{2})$。\n\n单个观测值 $y_{i}$ 的伪对数似然为：\n$$l_{i}(\\theta) = \\ln \\psi(y_{i}; \\theta) = -\\frac{1}{2} \\ln(2\\pi\\sigma^{2}) - \\frac{(y_{i} - \\theta g_{i})^{2}}{2\\sigma^{2}}$$\n总伪对数似然为 $L(\\theta) = \\sum_{i=1}^{n} l_{i}(\\theta)$。\n\n拟得分函数是 $L(\\theta)$ 关于 $\\theta$ 的导数：\n$$S(\\theta) = \\frac{\\partial L(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n} \\frac{\\partial l_{i}(\\theta)}{\\partial \\theta}$$\n第 $i$ 个观测值的贡献是：\n$$s_{i}(\\theta) = \\frac{\\partial l_{i}(\\theta)}{\\partial \\theta} = -\\frac{1}{2\\sigma^{2}} \\cdot 2(y_{i} - \\theta g_{i})(-g_{i}) = \\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}$$\n因此，总拟得分为 $S(\\theta) = \\sum_{i=1}^{n} \\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}$。\n\n为了分析该估计量，我们在真实参数值 $\\theta_{0}$ 处评估得分函数。根据真实模型，$y_i - \\theta_0 g_i = \\varepsilon_i$。\n$$S(\\theta_{0}) = \\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}$$\n\nGodambe 信息 $J_{G}(\\theta)$ 由两个部分构成：敏感度（我们记为 $A(\\theta)$）和变异性（我们记为 $B(\\theta)$）。\n\n首先，我们定义并计算敏感度 $A(\\theta_0)$。这是在伪模型 $\\psi$ 下评估的伪对数似然的负Hessian矩阵的期望。\n$$A(\\theta) = -\\mathbb{E}_{\\psi}\\left[\\frac{\\partial^{2} L(\\theta)}{\\partial \\theta^{2}}\\right]$$\n二阶导数是：\n$$\\frac{\\partial^{2} L(\\theta)}{\\partial \\theta^{2}} = \\sum_{i=1}^{n} \\frac{\\partial s_{i}(\\theta)}{\\partial \\theta} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} \\left(\\frac{(y_{i} - \\theta g_{i})g_{i}}{\\sigma^{2}}\\right) = \\sum_{i=1}^{n} \\frac{-g_{i}^{2}}{\\sigma^{2}}$$\n由于此表达式不依赖于数据 $y_{i}$，其期望就是表达式本身。\n$$A(\\theta_{0}) = -\\left(\\sum_{i=1}^{n} \\frac{-g_{i}^{2}}{\\sigma^{2}}\\right) = \\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}$$\n这个量 $A(\\theta_{0})$ 也是朴素Fisher信息 $I_{F, \\text{naive}}(\\theta_{0})$，因为它是在模型误设的高斯模型下计算的Fisher信息。\n\n其次，我们定义并计算变异性 $B(\\theta_0)$。这是在真实数据生成过程 $P$ 下评估的拟得分函数的方差。\n$$B(\\theta_{0}) = \\operatorname{Var}_{P}(S(\\theta_{0})) = \\mathbb{E}_{P}[S(\\theta_{0})^{2}] - (\\mathbb{E}_{P}[S(\\theta_{0})])^{2}$$\n在真实模型下，得分函数的期望是：\n$$\\mathbb{E}_{P}[S(\\theta_{0})] = \\mathbb{E}_{P}\\left[\\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}\\right] = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} g_{i} \\mathbb{E}_{P}[\\varepsilon_{i}]$$\n由于 $\\varepsilon_{i}$ 的真实分布均值为 $0$，所以 $\\mathbb{E}_{P}[S(\\theta_{0})] = 0$。因此，变异性是得分函数平方的期望：\n$$B(\\theta_{0}) = \\mathbb{E}_{P}[S(\\theta_{0})^{2}] = \\mathbb{E}_{P}\\left[\\left(\\sum_{i=1}^{n} \\frac{\\varepsilon_{i} g_{i}}{\\sigma^{2}}\\right)^{2}\\right] = \\frac{1}{\\sigma^{4}} \\mathbb{E}_{P}\\left[\\left(\\sum_{i=1}^{n} \\varepsilon_{i} g_{i}\\right)^{2}\\right]$$\n因为误差 $\\varepsilon_{i}$ 是独立的，所以对于 $i \\neq j$，有 $\\mathbb{E}_{P}[\\varepsilon_{i}\\varepsilon_{j}] = \\mathbb{E}_{P}[\\varepsilon_{i}]\\mathbb{E}_{P}[\\varepsilon_{j}] = 0$。该表达式可简化为：\n$$B(\\theta_{0}) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{n} g_{i}^{2} \\mathbb{E}_{P}[\\varepsilon_{i}^{2}]$$\n由于 $\\mathbb{E}_{P}[\\varepsilon_{i}] = 0$，我们有 $\\mathbb{E}_{P}[\\varepsilon_{i}^{2}] = \\operatorname{Var}_{P}(\\varepsilon_{i}) = \\sigma^{2}\\frac{\\nu}{\\nu-2}$。将其代入：\n$$B(\\theta_{0}) = \\frac{1}{\\sigma^{4}} \\sum_{i=1}^{n} g_{i}^{2} \\left(\\sigma^{2}\\frac{\\nu}{\\nu-2}\\right) = \\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu}{\\nu-2}$$\n\n标量参数的 Godambe（三明治）信息定义为 $J_{G}(\\theta) = A(\\theta)^{2} / B(\\theta)$。\n$$J_{G}(\\theta_{0}) = \\frac{\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right)^{2}}{\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu}{\\nu-2}} = \\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu-2}{\\nu}$$\n\n在模型误设下，无偏估计量的渐近CRLB是 Godambe 信息的倒数。\n$$CRLB_{G} = [J_{G}(\\theta_{0})]^{-1} = \\left[\\left(\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right) \\frac{\\nu-2}{\\nu}\\right]^{-1} = \\left(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}\\right) \\frac{\\nu}{\\nu-2}$$\n这等价于拟似然估计量的渐近方差，由 $B(\\theta_{0}) / A(\\theta_{0})^{2}$ 给出。\n\n朴素Fisher信息已被确定为 $I_{F, \\text{naive}}(\\theta_{0}) = A(\\theta_{0}) = \\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}$。\n相关的朴素CRLB是其倒数：\n$$CRLB_{\\text{naive}} = [I_{F, \\text{naive}}(\\theta_{0})]^{-1} = \\left[\\frac{\\sum_{i=1}^{n} g_{i}^{2}}{\\sigma^{2}}\\right]^{-1} = \\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}$$\n这是在假设高斯模型为真的情况下会错误计算出的下界。\n\n最后，我们计算基于 Godambe 的 CRLB 与朴素 CRLB 的比率：\n$$\\frac{CRLB_{G}}{CRLB_{\\text{naive}}} = \\frac{\\left(\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}\\right) \\frac{\\nu}{\\nu-2}}{\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}}$$\n公共项 $\\frac{\\sigma^{2}}{\\sum_{i=1}^{n} g_{i}^{2}}$ 被消去，剩下：\n$$\\frac{CRLB_{G}}{CRLB_{\\text{naive}}} = \\frac{\\nu}{\\nu-2}$$\n当真实误差具有由学生t分布描述的重尾时，这个比率量化了由于使用高斯似然而导致的估计量效率损失。当 $\\nu \\to \\infty$ 时，学生t分布收敛于高斯分布，该比率趋近于 $1$。当 $\\nu \\to 2^{+}$ 时，真实误差分布的方差趋于无穷大，该比率也趋于无穷大，表明效率严重损失。",
            "answer": "$$\\boxed{\\frac{\\nu}{\\nu-2}}$$"
        }
    ]
}