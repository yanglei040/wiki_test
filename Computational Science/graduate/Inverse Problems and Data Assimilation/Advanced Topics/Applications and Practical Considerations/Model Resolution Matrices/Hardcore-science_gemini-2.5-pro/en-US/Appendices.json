{
    "hands_on_practices": [
        {
            "introduction": "To truly master a concept, it is essential to understand its origins. This first practice takes you back to fundamentals by deriving the model resolution matrix, $\\mathcal{R}$, from first principles within a simple yet illustrative dynamical system. By working through the linear-Gaussian framework, you will solidify the connection between the Bayesian posterior estimate, the true underlying state $x_{0}^{\\star}$, and the definition of the resolution matrix as the operator that maps the 'truth' to our 'best guess' in expectation .",
            "id": "3403416",
            "problem": "Consider a linear discrete-time two-dimensional state model with unknown initial state $x_{0} = \\begin{pmatrix}p_{0} \\\\ v_{0}\\end{pmatrix} \\in \\mathbb{R}^{2}$, where $p_{0}$ denotes an initial position and $v_{0}$ denotes an initial velocity. The deterministic time-evolution over a uniform time step $\\Delta t > 0$ is given by the linear operator $M = \\begin{pmatrix}1 & \\Delta t \\\\ 0 & 1\\end{pmatrix}$ so that $x_{k} = M^{k} x_{0}$ at observation index $k \\in \\mathbb{N}$. A single scalar observation is made at time index $k$ through the linear observation operator $H = \\begin{pmatrix}1 & 0\\end{pmatrix}$, with additive Gaussian noise of zero mean and variance $\\sigma_{y}^{2} > 0$, i.e., $y_{k} = H x_{k} + \\eta_{k}$ with $\\eta_{k} \\sim \\mathcal{N}(0, \\sigma_{y}^{2})$. Assume a Gaussian prior on $x_{0}$ with zero mean and covariance $C_{0} = \\operatorname{diag}(\\sigma_{p}^{2}, \\sigma_{v}^{2})$, where $\\sigma_{p}^{2} > 0$ and $\\sigma_{v}^{2} > 0$ are given.\n\nUsing only the foundational setup described above (linear dynamics, linear observation, Gaussian prior and noise), derive the model resolution matrix, defined as the linear operator $\\mathcal{R}(k)$ that maps the true initial state $x_{0}^{\\star}$ to the expected posterior mean estimate $\\mathbb{E}[\\hat{x}_{0} \\mid x_{0}^{\\star}]$ when assimilating the single observation at time index $k$. Express $\\mathcal{R}(k)$ in closed form as a function of $k$, $\\Delta t$, $\\sigma_{p}^{2}$, $\\sigma_{v}^{2}$, and $\\sigma_{y}^{2}$. Your final answer must be a single exact analytic matrix expression and should be written using standard matrix notation. No numerical rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and self-contained. All necessary information is provided to derive the model resolution matrix for the described linear-Gaussian system. We shall proceed with the solution.\n\nThe problem requires the derivation of the model resolution matrix $\\mathcal{R}(k)$, defined by the relationship $\\mathbb{E}[\\hat{x}_{0} \\mid x_{0}^{\\star}] = \\mathcal{R}(k) x_{0}^{\\star}$, where $x_{0}^{\\star}$ is the true initial state and $\\hat{x}_{0}$ is the data-informed estimate of the initial state. The estimate $\\hat{x}_{0}$ is taken to be the posterior mean, which for a linear-Gaussian system is equivalent to the Maximum A Posteriori (MAP) estimate.\n\nWe begin by establishing the statistical framework. The state to be estimated is the initial state $x_{0} \\in \\mathbb{R}^{2}$.\nThe prior distribution for $x_{0}$ is given as a zero-mean Gaussian with covariance $C_{0}$:\n$$p(x_{0}) \\propto \\exp\\left(-\\frac{1}{2} x_{0}^{T} C_{0}^{-1} x_{0}\\right)$$\nwhere $C_{0} = \\operatorname{diag}(\\sigma_{p}^{2}, \\sigma_{v}^{2})$.\n\nA single scalar observation $y_k$ is made at time index $k$. The observation model is $y_{k} = H x_{k} + \\eta_{k}$, where $x_k = M^k x_0$ and $\\eta_k \\sim \\mathcal{N}(0, \\sigma_y^2)$. We can write the observation directly as a function of the initial state $x_0$:\n$$y_k = H M^k x_0 + \\eta_k$$\nThe forward operator that maps the initial state to the (noiseless) observation is $G_k = H M^k$.\nThe likelihood function, which is the probability of observing $y_k$ given a state $x_0$, is:\n$$p(y_k \\mid x_0) \\propto \\exp\\left(-\\frac{1}{2\\sigma_y^2} (y_k - H M^k x_0)^2\\right)$$\n\nAccording to Bayes' theorem, the posterior probability density for $x_0$ given $y_k$ is proportional to the product of the likelihood and the prior: $p(x_0 \\mid y_k) \\propto p(y_k \\mid x_0) p(x_0)$.\nThe MAP estimate $\\hat{x}_0$ is the value of $x_0$ that maximizes this posterior probability, or equivalently, minimizes the negative log-posterior, which defines the cost function $J(x_0)$:\n$$J(x_0) = \\frac{1}{2\\sigma_y^2}(y_k - H M^k x_0)^2 + \\frac{1}{2}x_0^T C_0^{-1} x_0$$\nNote that since $y_k$ and $H M^k x_0$ are scalars, the term $(y_k - H M^k x_0)^T (\\sigma_y^2)^{-1} (y_k - H M^k x_0)$ simplifies to the expression above.\n\nTo find the minimum, we compute the gradient of $J(x_0)$ with respect to $x_0$ and set it to zero. Let's denote the row vector $H M^k$ as $G_k$. Then $H M^k x_0 = G_k x_0$.\n$$\\nabla_{x_0} J(x_0) = \\frac{1}{2\\sigma_y^2} \\nabla_{x_0} \\left( y_k^2 - 2y_k G_k x_0 + (G_k x_0)^2 \\right) + \\frac{1}{2} \\nabla_{x_0} (x_0^T C_0^{-1} x_0)$$\n$$\\nabla_{x_0} J(x_0) = \\frac{1}{2\\sigma_y^2} \\left( -2y_k G_k^T + 2G_k^T G_k x_0 \\right) + C_0^{-1} x_0$$\nSetting the gradient to zero to find the estimate $\\hat{x}_0$:\n$$\\frac{1}{\\sigma_y^2}(-y_k G_k^T + G_k^T G_k \\hat{x}_0) + C_0^{-1} \\hat{x}_0 = 0$$\n$$\\left( \\frac{1}{\\sigma_y^2} G_k^T G_k + C_0^{-1} \\right) \\hat{x}_0 = \\frac{1}{\\sigma_y^2} G_k^T y_k$$\nSolving for $\\hat{x}_0$:\n$$\\hat{x}_0 = \\left( \\frac{1}{\\sigma_y^2} (H M^k)^T (H M^k) + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (H M^k)^T y_k$$\nThis equation gives the posterior mean estimate $\\hat{x}_0$ as a function of the observation $y_k$.\n\nNow, we use the definition of the model resolution matrix. We let $y_k$ be generated from the true state $x_0^\\star$, so $y_k = H M^k x_0^\\star + \\eta_k$. We then take the expectation of $\\hat{x}_0$ with respect to the noise distribution of $\\eta_k$, conditional on the true state $x_0^\\star$. Since $\\mathbb{E}[\\eta_k] = 0$, we have $\\mathbb{E}[y_k \\mid x_0^\\star] = H M^k x_0^\\star$.\n$$\\mathbb{E}[\\hat{x}_0 \\mid x_0^\\star] = \\mathbb{E}\\left[ \\left( \\frac{1}{\\sigma_y^2} (H M^k)^T H M^k + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (H M^k)^T y_k \\mid x_0^\\star \\right]$$\nThe matrix terms are deterministic, so we can move the expectation operator inside:\n$$\\mathbb{E}[\\hat{x}_0 \\mid x_0^\\star] = \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (M^k)^T H^T \\mathbb{E}[y_k \\mid x_0^\\star]$$\n$$\\mathbb{E}[\\hat{x}_0 \\mid x_0^\\star] = \\left[ \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k \\right] x_0^\\star$$\nBy definition, the expression in the brackets is the model resolution matrix $\\mathcal{R}(k)$:\n$$\\mathcal{R}(k) = \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1} \\right)^{-1} \\left(\\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k \\right)$$\n\nTo obtain the closed-form expression, we must compute each component.\nFirst, we find $M^k$. Given $M = \\begin{pmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{pmatrix}$, we can find a pattern for its powers:\n$M^2 = \\begin{pmatrix} 1 & 2\\Delta t \\\\ 0 & 1 \\end{pmatrix}$, $M^3 = \\begin{pmatrix} 1 & 3\\Delta t \\\\ 0 & 1 \\end{pmatrix}$. By induction, we establish that:\n$$M^k = \\begin{pmatrix} 1 & k \\Delta t \\\\ 0 & 1 \\end{pmatrix}$$\nThe term $(M^k)^T H^T H M^k$ is computed as follows:\n$H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, so $H^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n$(M^k)^T H^T = \\begin{pmatrix} 1 & 0 \\\\ k\\Delta t & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ k\\Delta t \\end{pmatrix}$.\nThen, $(M^k)^T H^T H M^k = \\begin{pmatrix} 1 \\\\ k\\Delta t \\end{pmatrix} \\begin{pmatrix} 1 & k\\Delta t \\end{pmatrix} = \\begin{pmatrix} 1 & k\\Delta t \\\\ k\\Delta t & (k\\Delta t)^2 \\end{pmatrix}$.\n\nThe prior covariance inverse is $C_0^{-1} = \\operatorname{diag}(\\sigma_p^{-2}, \\sigma_v^{-2}) = \\begin{pmatrix} 1/\\sigma_p^2 & 0 \\\\ 0 & 1/\\sigma_v^2 \\end{pmatrix}$.\n\nLet $A_k = \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1}$. This is the matrix to be inverted.\n$$A_k = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1 & k\\Delta t \\\\ k\\Delta t & k^2(\\Delta t)^2 \\end{pmatrix} + \\begin{pmatrix} 1/\\sigma_p^2 & 0 \\\\ 0 & 1/\\sigma_v^2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} & \\frac{k\\Delta t}{\\sigma_y^2} \\\\ \\frac{k\\Delta t}{\\sigma_y^2} & \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2} \\end{pmatrix}$$\nThe determinant of $A_k$ is:\n$$\\det(A_k) = \\left(\\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2}\\right) \\left(\\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2}\\right) - \\left(\\frac{k\\Delta t}{\\sigma_y^2}\\right)^2$$\n$$\\det(A_k) = \\frac{k^2(\\Delta t)^2}{\\sigma_y^4} + \\frac{1}{\\sigma_y^2\\sigma_v^2} + \\frac{k^2(\\Delta t)^2}{\\sigma_y^2\\sigma_p^2} + \\frac{1}{\\sigma_p^2\\sigma_v^2} - \\frac{k^2(\\Delta t)^2}{\\sigma_y^4}$$\n$$\\det(A_k) = \\frac{1}{\\sigma_y^2\\sigma_v^2} + \\frac{k^2(\\Delta t)^2}{\\sigma_y^2\\sigma_p^2} + \\frac{1}{\\sigma_p^2\\sigma_v^2} = \\frac{\\sigma_p^2 + k^2(\\Delta t)^2\\sigma_v^2 + \\sigma_y^2}{\\sigma_y^2\\sigma_p^2\\sigma_v^2}$$\nFor brevity, let $D_k = \\sigma_p^2 + k^2(\\Delta t)^2\\sigma_v^2 + \\sigma_y^2$. Then $\\det(A_k) = \\frac{D_k}{\\sigma_y^2\\sigma_p^2\\sigma_v^2}$.\n\nThe inverse of $A_k$ is:\n$$A_k^{-1} = \\frac{1}{\\det(A_k)} \\begin{pmatrix} \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2} & -\\frac{k\\Delta t}{\\sigma_y^2} \\\\ -\\frac{k\\Delta t}{\\sigma_y^2} & \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} \\end{pmatrix}$$\nNow, we compute $\\mathcal{R}(k) = A_k^{-1} \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k \\right)$.\nLet $B_k = \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1 & k\\Delta t \\\\ k\\Delta t & k^2(\\Delta t)^2 \\end{pmatrix}$.\n$$\\mathcal{R}(k) = \\frac{\\sigma_y^2\\sigma_p^2\\sigma_v^2}{D_k} \\begin{pmatrix} \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2} & -\\frac{k\\Delta t}{\\sigma_y^2} \\\\ -\\frac{k\\Delta t}{\\sigma_y^2} & \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} \\end{pmatrix} \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1 & k\\Delta t \\\\ k\\Delta t & k^2(\\Delta t)^2 \\end{pmatrix}$$\nLet's compute the product of the two matrices:\n$$ \\begin{pmatrix} \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2} & -\\frac{k\\Delta t}{\\sigma_y^2} \\\\ -\\frac{k\\Delta t}{\\sigma_y^2} & \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} \\end{pmatrix} \\begin{pmatrix} 1 & k\\Delta t \\\\ k\\Delta t & k^2(\\Delta t)^2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma_v^2} & \\frac{k\\Delta t}{\\sigma_v^2} \\\\ \\frac{k\\Delta t}{\\sigma_p^2} & \\frac{k^2(\\Delta t)^2}{\\sigma_p^2} \\end{pmatrix} $$\nThe (1,1) entry is $(\\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2})(1) - (\\frac{k\\Delta t}{\\sigma_y^2})(k\\Delta t) = \\frac{1}{\\sigma_v^2}$.\nThe other entries are computed similarly.\nSubstituting this back into the expression for $\\mathcal{R}(k)$:\n$$\\mathcal{R}(k) = \\frac{\\sigma_y^2\\sigma_p^2\\sigma_v^2}{D_k} \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\frac{1}{\\sigma_v^2} & \\frac{k\\Delta t}{\\sigma_v^2} \\\\ \\frac{k\\Delta t}{\\sigma_p^2} & \\frac{k^2(\\Delta t)^2}{\\sigma_p^2} \\end{pmatrix}$$\n$$\\mathcal{R}(k) = \\frac{\\sigma_p^2\\sigma_v^2}{D_k} \\begin{pmatrix} \\frac{1}{\\sigma_v^2} & \\frac{k\\Delta t}{\\sigma_v^2} \\\\ \\frac{k\\Delta t}{\\sigma_p^2} & \\frac{k^2(\\Delta t)^2}{\\sigma_p^2} \\end{pmatrix}$$\nFinally, multiplying the scalar into the matrix yields the closed-form expression for the model resolution matrix:\n$$\\mathcal{R}(k) = \\frac{1}{D_k} \\begin{pmatrix} \\sigma_p^2 & k\\Delta t \\sigma_p^2 \\\\ k\\Delta t \\sigma_v^2 & k^2(\\Delta t)^2 \\sigma_v^2 \\end{pmatrix}$$\nSubstituting the expression for $D_k$:\n$$\\mathcal{R}(k) = \\frac{1}{\\sigma_p^2 + k^2(\\Delta t)^2\\sigma_v^2 + \\sigma_y^2} \\begin{pmatrix} \\sigma_p^2 & k\\Delta t \\sigma_p^2 \\\\ k\\Delta t \\sigma_v^2 & k^2(\\Delta t)^2 \\sigma_v^2 \\end{pmatrix}$$\nThis is the final analytical expression for the model resolution matrix as a function of the given parameters.",
            "answer": "$$\n\\boxed{\n\\frac{1}{\\sigma_{p}^{2} + k^{2} (\\Delta t)^{2} \\sigma_{v}^{2} + \\sigma_{y}^{2}}\n\\begin{pmatrix}\n\\sigma_{p}^{2} & k \\Delta t \\sigma_{p}^{2} \\\\\nk \\Delta t \\sigma_{v}^{2} & k^{2} ( \\Delta t )^{2} \\sigma_{v}^{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from analytical derivation to computational practice, we now explore how the model resolution matrix is used to quantify the performance of an inversion. This exercise introduces the concept of the point-spread function (PSF)—a column of the resolution matrix that shows how a single, localized feature in the true model is \"seen\" by the inversion. You will implement a Tikhonov-regularized inversion and compute the width of the PSF, providing a direct, quantitative measure of how regularization strength, $\\lambda$, creates a trade-off between solution stability and model resolution .",
            "id": "3403453",
            "problem": "Consider a one-dimensional discretized linear inverse problem on a uniform grid of $N$ unknowns $\\{m_k\\}_{k=0}^{N-1}$ with unit grid spacing. Let the forward operator be the identity, so the data equal the model in the absence of noise, and assume unit data covariance. A standard Tikhonov-regularized estimator with a first-difference stabilizer seeks $\\hat{m}$ by minimizing a quadratic objective that penalizes roughness measured by the first-difference operator. The model resolution matrix encodes how the true model is mapped, in expectation for noise-free data, to the estimated model. A point-spread function at index $i$ is the $i$-th row of the model resolution matrix; it quantifies how a unit spike at index $i$ is blurred by the estimator. Your task is to derive, compute, and quantify the width of such point-spread functions as a function of the regularization strength.\n\nStarting from the fundamental definitions of Tikhonov regularization and the model resolution matrix, perform the following:\n\n- Derive the model resolution matrix for the special case where the forward operator is the identity and the data covariance is the identity. The regularization operator $L$ is the first-difference operator with entries $(L)_{r,r}=-1$ and $(L)_{r,r+1}=1$ for $r \\in \\{0,\\dots,N-2\\}$, and all other entries zero. Do not assume any expression for the resolution matrix a priori; begin from the variational formulation that defines the Tikhonov estimator and the definition of the resolution matrix in terms of the mapping from the true model to the expected estimate in the noise-free case.\n\n- Explain how to compute the $i$-th point-spread function, defined as the $i$-th row of the model resolution matrix, without explicitly forming the full inverse, using only linear algebra identities and the symmetry properties of the system.\n\n- Define a quantitative width metric for a discrete point-spread function $p=\\{p_k\\}_{k=0}^{N-1}$ centered at index $i$ using the second central moment. Specifically, with $\\alpha=\\sum_{k=0}^{N-1} p_k$, define the variance $\\sigma^2=\\frac{1}{\\alpha}\\sum_{k=0}^{N-1} (k-i)^2 p_k$ and the width as $w=2\\sqrt{\\sigma^2}$. All distances are in grid-index units, which are dimensionless here.\n\nThen, implement a program that, for each parameter set in the test suite below, constructs $L$, computes the corresponding point-spread function for the given index $i$ and regularization parameter $\\lambda$, and outputs its width $w(\\lambda)$ as defined above. Use the convention that the grid indices are integers from $0$ to $N-1$.\n\nTest suite (each tuple is $(N,i,\\lambda)$):\n\n- Case A (interior index, increasing smoothing):\n  - $(N,i,\\lambda)=(51,25,0)$\n  - $(N,i,\\lambda)=(51,25,0.2)$\n  - $(N,i,\\lambda)=(51,25,1.0)$\n  - $(N,i,\\lambda)=(51,25,5.0)$\n- Case B (left boundary index):\n  - $(N,i,\\lambda)=(51,0,0)$\n  - $(N,i,\\lambda)=(51,0,1.0)$\n  - $(N,i,\\lambda)=(51,0,5.0)$\n- Case C (different grid size, interior index):\n  - $(N,i,\\lambda)=(21,10,0)$\n  - $(N,i,\\lambda)=(21,10,0.5)$\n  - $(N,i,\\lambda)=(21,10,2.0)$\n\nYour program should produce a single line of output containing the $10$ computed widths in the order listed above, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets (for example, $[0.000000,0.123456,\\dots]$). No other text should be printed. All computations are dimensionless; no physical units are required. Angles are not involved.",
            "solution": "The problem is assessed as valid, as it is scientifically grounded in the established principles of linear inverse theory, is well-posed with sufficient and consistent information, and is formulated objectively. We may proceed with the solution.\n\nThe task is to derive and compute the width of a model point-spread function (PSF) for a Tikhonov-regularized inverse problem. The solution involves three primary stages: first, deriving the model resolution matrix from fundamental principles; second, describing an efficient method to compute a single PSF (a row of the resolution matrix); and third, quantifying the width of the computed PSF.\n\n**1. Derivation of the Model Resolution Matrix ($R$)**\n\nThe Tikhonov-regularized estimate $\\hat{m}$ of a model vector $m$ is the vector that minimizes the quadratic objective function $J(m)$:\n$$\nJ(m) = \\| Gm - d \\|^2_{C_d^{-1}} + \\lambda^2 \\| Lm \\|^2\n$$\nHere, $G$ is the forward operator, $d$ is the data vector, $C_d$ is the data covariance matrix, $L$ is the regularization operator (or stabilizer), and $\\lambda$ is the regularization parameter that balances data fidelity against model simplicity (e.g., smoothness). The squared weighted norm is defined as $\\|v\\|^2_W = v^T W v$.\n\nThe problem specifies that the forward operator is the identity, $G=I$, and the data covariance is also the identity, $C_d=I$. Substituting these into the objective function yields:\n$$\nJ(m) = \\| Im - d \\|^2_{I^{-1}} + \\lambda^2 \\| Lm \\|^2 = (m - d)^T (m - d) + \\lambda^2 (Lm)^T (Lm)\n$$\nExpanding this expression, we have:\n$$\nJ(m) = m^T m - 2m^T d + d^T d + \\lambda^2 m^T L^T L m\n$$\nTo find the model estimate $\\hat{m}$ that minimizes $J(m)$, we compute the gradient of $J(m)$ with respect to $m$ and set it to zero. The gradient is:\n$$\n\\nabla_m J(m) = 2m - 2d + 2\\lambda^2 L^T L m\n$$\nSetting $\\nabla_m J(m) = 0$ gives:\n$$\n2\\hat{m} - 2d + 2\\lambda^2 L^T L \\hat{m} = 0\n$$\n$$\n(I + \\lambda^2 L^T L) \\hat{m} = d\n$$\nSolving for the regularized estimate $\\hat{m}$, we obtain:\n$$\n\\hat{m} = (I + \\lambda^2 L^T L)^{-1} d\n$$\nThe model resolution matrix, $R$, is defined by the linear mapping from the true model, $m_{\\text{true}}$, to the expected value of the estimated model, $E[\\hat{m}]$. Under the condition of noise-free data, the observed data vector $d$ is simply $d = G m_{\\text{true}}$. Given $G=I$, this becomes $d = m_{\\text{true}}$. Substituting this into the expression for $\\hat{m}$ gives the relationship for the noise-free case:\n$$\n\\hat{m} = (I + \\lambda^2 L^T L)^{-1} m_{\\text{true}}\n$$\nBy comparing this with the definition $\\hat{m} = R m_{\\text{true}}$, we identify the model resolution matrix as:\n$$\nR = (I + \\lambda^2 L^T L)^{-1}\n$$\nThis is the required expression for the model resolution matrix, derived from the variational formulation. The matrix $L$ is the $(N-1) \\times N$ first-difference operator, and $I$ is the $N \\times N$ identity matrix.\n\n**2. Computation of the Point-Spread Function**\n\nThe point-spread function (PSF) at an index $i$, denoted $p^{(i)}$, is defined as the $i$-th row of the model resolution matrix $R$. A direct computation would require forming the matrix $A = I + \\lambda^2 L^T L$ and then computing its full inverse $R = A^{-1}$, an operation of complexity $O(N^3)$, which is computationally inefficient for large $N$.\n\nA more efficient method exploits the properties of the matrix $A$. The matrix $L^T L$ is symmetric, as $(L^T L)^T = L^T (L^T)^T = L^T L$. Consequently, the matrix $A = I + \\lambda^2 L^T L$ is also symmetric ($A^T=A$). The inverse of a symmetric matrix is also symmetric, so $R$ is symmetric ($R^T=R$).\n\nThe $i$-th row of $R$ can be written as $e_i^T R$, where $e_i$ is a column vector with a $1$ at index $i$ and $0$ elsewhere (the $i$-th standard basis vector). Due to the symmetry of $R$, the $i$-th row is the transpose of the $i$-th column:\n$$\n(i\\text{-th row of } R) = e_i^T R = (R^T e_i)^T = (R e_i)^T\n$$\nThe vector $R e_i$ is the $i$-th column of $R$. Let this column vector be $x$. Then $x = R e_i = A^{-1} e_i$. To find $x$, we can solve the equivalent linear system:\n$$\nAx = e_i\n$$\nThe solution vector $x$ contains the elements of the $i$-th PSF, $p^{(i)}$. This approach, solving a single linear system, has a significantly lower computational cost than inverting the matrix $A$. Since $A$ is symmetric and positive definite (and banded), specialized and highly efficient linear solvers can be employed, often with complexity close to $O(N)$.\n\nThe computational procedure is therefore:\n1. Construct the $(N-1) \\times N$ first-difference matrix $L$.\n2. Form the $N \\times N$ matrix $A = I + \\lambda^2 L^T L$.\n3. Define the $N \\times 1$ basis vector $e_i$.\n4. Solve the linear system of equations $Ax = e_i$ to obtain the PSF vector $x$.\n\n**3. PSF Width Quantification**\n\nThe problem provides a specific metric for the width of a discrete PSF, $p = \\{p_k\\}_{k=0}^{N-1}$, centered at index $i$. The width is defined based on the second central moment (variance) of the distribution of values in $p$.\n\nFirst, we compute the sum of the PSF's components:\n$$\n\\alpha = \\sum_{k=0}^{N-1} p_k\n$$\nThis sum serves as a normalization factor. For $\\lambda=0$, $R=I$, so $p^{(i)} = e_i^T$ and $\\alpha=1$. For $\\lambda > 0$, the PSF broadens, and $\\alpha$ may deviate from $1$.\n\nNext, the variance $\\sigma^2$ is calculated as the weighted average of the squared distances from the center $i$:\n$$\n\\sigma^2 = \\frac{1}{\\alpha} \\sum_{k=0}^{N-1} (k-i)^2 p_k\n$$\nFinally, the width $w$ is defined as twice the standard deviation:\n$$\nw = 2\\sqrt{\\sigma^2}\n$$\nThe distances $(k-i)$ are in dimensionless grid-index units, so the resulting width $w$ is also in these units. For the special case where $\\lambda=0$, the PSF is a perfect spike ($p_k = \\delta_{ik}$), leading to $\\sigma^2=0$ and thus $w=0$, indicating perfect resolution. As $\\lambda$ increases, the regularization effect strengthens, the PSF broadens, and the width $w$ is expected to increase.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the widths of point-spread functions for a series of test cases\n    in a Tikhonov-regularized inverse problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (N, i, lambda), where:\n    # N is the number of grid points (size of the model vector).\n    # i is the index for which the point-spread function is computed.\n    # lambda is the regularization parameter.\n    test_cases = [\n        # Case A (interior index, increasing smoothing)\n        (51, 25, 0.0),\n        (51, 25, 0.2),\n        (51, 25, 1.0),\n        (51, 25, 5.0),\n        # Case B (left boundary index)\n        (51, 0, 0.0),\n        (51, 0, 1.0),\n        (51, 0, 5.0),\n        # Case C (different grid size, interior index)\n        (21, 10, 0.0),\n        (21, 10, 0.5),\n        (21, 10, 2.0),\n    ]\n\n    results = []\n    for N, i, lambda_reg in test_cases:\n        # For lambda = 0, the resolution matrix is the identity.\n        # The PSF is a Kronecker delta, so its width is exactly 0.\n        if lambda_reg == 0.0:\n            results.append(0.0)\n            continue\n            \n        # STEP 1: Construct the (N-1)xN first-difference operator L.\n        L = np.zeros((N - 1, N))\n        rows = np.arange(N - 1)\n        L[rows, rows] = -1.0\n        L[rows, rows + 1] = 1.0\n        \n        # STEP 2: Form the matrix A = I + lambda^2 * L^T * L.\n        # This matrix is symmetric and positive definite.\n        LTL = L.T @ L\n        A = np.identity(N) + (lambda_reg**2) * LTL\n        \n        # STEP 3: Define the standard basis vector e_i.\n        e_i = np.zeros(N)\n        e_i[i] = 1.0\n        \n        # STEP 4: Solve the linear system A*p = e_i to find the PSF p.\n        # np.linalg.solve is efficient for such systems.\n        p = np.linalg.solve(A, e_i)\n        \n        # STEP 5: Calculate the width of the PSF p.\n        # The width is defined as 2 * sqrt(variance).\n        \n        # Sum of the PSF components (normalization factor).\n        alpha = np.sum(p)\n\n        # Vector of grid indices [0, 1, ..., N-1].\n        k = np.arange(N)\n        \n        # Variance of the PSF around its center i.\n        # The sum is a weighted average of squared distances from the center.\n        variance = np.sum(((k - i)**2) * p) / alpha\n        \n        # The variance can be slightly negative due to floating-point inaccuracies\n        # if the true value is near zero. np.abs() provides robustness.\n        width = 2.0 * np.sqrt(np.abs(variance))\n        \n        results.append(width)\n\n    # Print the final results in the specified format:\n    # A single line, comma-separated list of widths, rounded to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond the blurring of individual features, the model resolution matrix $R$ reveals crucial information about the coupling, or 'cross-talk', between different model parameters. This practice delves into the interpretation of the individual entries $R_{ij}$, which quantify how a feature in the true parameter $m_j$ can leak into the estimate of a different parameter, $\\hat{m}_i$. Through a series of carefully designed scenarios, you will investigate how the forward problem physics ($G$), data quality ($C_d$), and regularization ($L$) govern these trade-offs and shape the structure of the entire resolution matrix .",
            "id": "3403399",
            "problem": "Consider a linear inverse problem with a forward operator $G \\in \\mathbb{R}^{m \\times n}$, model vector $m \\in \\mathbb{R}^{n}$, and data vector $d \\in \\mathbb{R}^{m}$ satisfying $d = G m + \\varepsilon$, where $\\varepsilon$ is additive noise with zero mean and covariance matrix $C_d \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. Let the estimator $\\hat{m}$ be defined as the minimizer of the strictly convex quadratic objective\n$$\nJ(m) = \\| C_d^{-1/2} (G m - d)\\|_2^2 + \\| L m \\|_2^2,\n$$\nwhere $L \\in \\mathbb{R}^{r \\times n}$ is a fixed regularization operator (with any desired scaling absorbed into $L$). The matrix $L$ is allowed to be rectangular, and the product $L^\\top L$ is assumed to be well-defined. Assume $G$, $C_d$, and $L$ are such that the minimizer is unique.\n\nStarting from this definition and standard least-squares optimality conditions, define the model resolution matrix $R \\in \\mathbb{R}^{n \\times n}$, interpret the cross-talk coefficient between parameters $i$ and $j$ via the entry $R_{ij}$, and compute the response of the $i$-th estimated model component $\\hat{m}_i$ to a unit perturbation applied to the $j$-th true model component $m_j$ under the idealized noise-free setting $d = G m_{\\text{true}}$. Use a zero-based index convention for the integers $i$ and $j$.\n\nYour program must:\n- Derive an expression for $R$ only from the optimality conditions of the stated objective without assuming any specific structure for $G$, $C_d$, or $L$ beyond the given assumptions.\n- Implement a function that, given $G$, $C_d$, $L$, and indices $(i,j)$, returns the scalar cross-talk coefficient $R_{ij}$, which equals the response of $\\hat{m}_i$ to a unit perturbation at $m_j$ when all other components of $m_{\\text{true}}$ are zero.\n- Use the following test suite, where each case specifies $(G, C_d, L, i, j)$ explicitly as real-valued numeric arrays and integers. All arrays are dimensionless, and no physical units are involved.\n\nTest Suite (zero-based indices):\n1. Case A (identity resolution, on-diagonal):\n   - $G = I_3$\n   - $C_d = I_3$\n   - $L = 0_{3 \\times 3}$ (the $3 \\times 3$ zero matrix)\n   - $i = 1$, $j = 1$\n2. Case B (identity resolution, off-diagonal):\n   - $G = I_3$\n   - $C_d = I_3$\n   - $L = 0_{3 \\times 3}$\n   - $i = 0$, $j = 2$\n3. Case C (blurred forward operator with first-difference regularization):\n   - $G = \\begin{bmatrix}\n     1 & 0.5 & 0 & 0 \\\\\n     0.5 & 1 & 0.5 & 0 \\\\\n     0 & 0.5 & 1 & 0.5 \\\\\n     0 & 0 & 0.5 & 1\n     \\end{bmatrix}$\n   - $C_d = I_4$\n   - $L = \\begin{bmatrix}\n     -1 & 1 & 0 & 0 \\\\\n     0 & -1 & 1 & 0 \\\\\n     0 & 0 & -1 & 1\n     \\end{bmatrix}$\n   - $i = 1$, $j = 2$\n4. Case D (identity forward operator with anisotropic data covariance and first-difference regularization):\n   - $G = I_4$\n   - $C_d = \\mathrm{diag}(0.04, 1.0, 0.25, 9.0)$\n   - $L = \\begin{bmatrix}\n     -1 & 1 & 0 & 0 \\\\\n     0 & -1 & 1 & 0 \\\\\n     0 & 0 & -1 & 1\n     \\end{bmatrix}$\n   - $i = 3$, $j = 0$\n5. Case E (rank-deficient forward operator stabilized by identity regularization):\n   - $G = \\begin{bmatrix}\n     1 & 1 \\\\\n     2 & 2\n     \\end{bmatrix}$\n   - $C_d = I_2$\n   - $L = I_2$\n   - $i = 0$, $j = 1$\n6. Case F (strong identity regularization suppressing resolution):\n   - $G = I_2$\n   - $C_d = I_2$\n   - $L = 100\\, I_2$\n   - $i = 1$, $j = 1$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the six scalar results corresponding to Cases A–F, in order, as a comma-separated list enclosed in square brackets, for example, \"[rA,rB,rC,rD,rE,rF]\". Each entry must be a real number.",
            "solution": "We begin from the stated quadratic objective\n$$\nJ(m) = \\| C_d^{-1/2} (G m - d) \\|_2^2 + \\| L m \\|_2^2,\n$$\nwhere $G \\in \\mathbb{R}^{m \\times n}$, $C_d \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite, and $L \\in \\mathbb{R}^{r \\times n}$ is the regularization operator. The minimizer $\\hat{m}$ satisfies the first-order optimality condition obtained by setting the gradient of $J(m)$ with respect to $m$ to zero. Using standard properties of quadratic forms,\n$$\n\\nabla_m J(m) = 2 G^\\top C_d^{-1} (G m - d) + 2 L^\\top (L m)\n$$\nSetting $\\nabla_m J(\\hat{m}) = 0$ gives the normal equations\n$$\n\\left(G^\\top C_d^{-1} G + L^\\top L \\right) \\hat{m} = G^\\top C_d^{-1} d.\n$$\nDefine the symmetric positive definite matrix\n$A \\equiv G^\\top C_d^{-1} G + L^\\top L$, which is in $\\mathbb{R}^{n \\times n}$,\nwhich is invertible under the given assumptions because $L^\\top L$ provides stabilization even if $G^\\top C_d^{-1} G$ is rank deficient, and is not needed if $G^\\top C_d^{-1} G$ is already positive definite. Then\n$$\n\\hat{m} = A^{-1} G^\\top C_d^{-1} d.\n$$\nTo analyze model resolution, consider the idealized noise-free setting where $d = G m_{\\text{true}}$ for some fixed true model $m_{\\text{true}}$. Substituting $d$,\n$$\n\\hat{m} = A^{-1} G^\\top C_d^{-1} G \\, m_{\\text{true}}.\n$$\nThis shows that the estimator is a linear mapping of $m_{\\text{true}}$. The model resolution matrix is thus defined as\n$R \\equiv A^{-1} G^\\top C_d^{-1} G = \\left(G^\\top C_d^{-1} G + L^\\top L\\right)^{-1} G^\\top C_d^{-1} G$, which is in $\\mathbb{R}^{n \\times n}$.\nBy construction,\n$$\n\\hat{m} = R \\, m_{\\text{true}}.\n$$\nTo interpret cross-talk, consider a unit perturbation in the $j$-th component of the true model while keeping all other components zero:\n$$\nm_{\\text{true}} = e_j,\n$$\nwhere $e_j$ is the $j$-th standard basis vector in $\\mathbb{R}^{n}$. Then\n$$\n\\hat{m} = R e_j,\n$$\nand the $i$-th component of the estimate becomes\n$$\n\\hat{m}_i = e_i^\\top \\hat{m} = e_i^\\top R e_j = R_{ij}.\n$$\nTherefore, the cross-talk coefficient from parameter $j$ into the estimated parameter $i$ is precisely the matrix entry $R_{ij}$. In particular, $R_{ij}$ quantifies the response of $\\hat{m}_i$ to a unit perturbation at $m_j$.\n\nAlgorithmic computation proceeds as follows:\n1. Given $G$, $C_d$, and $L$, form $C_d^{-1}$ by inverting the symmetric positive definite matrix $C_d$.\n2. Form $A = G^\\top C_d^{-1} G + L^\\top L$ and $B = G^\\top C_d^{-1} G$.\n3. Solve the linear matrix equation\n$$\nA R = B\n$$\nfor $R$ using a stable linear solver; this is equivalent to $R = A^{-1} B$ without explicitly inverting $A$.\n4. Extract the scalar $R_{ij}$ at the requested zero-based indices $(i, j)$.\n\nDiscussion of the test suite:\n- In Case A with $G = I_3$, $C_d = I_3$, and $L = 0_{3 \\times 3}$, we have $A = I_3$ and $B = I_3$, hence $R = I_3$. The on-diagonal response $R_{11}$ equals $1$.\n- In Case B with the same operators, the off-diagonal entry $R_{02}$ equals $0$.\n- In Case C, the forward operator $G$ couples neighboring parameters, and the first-difference regularization $L$ penalizes roughness, leading to a nontrivial $R$ with both on- and off-diagonal effects; $R_{12}$ captures cross-talk from parameter $2$ into estimate component $1$.\n- In Case D, even with $G = I_4$, the anisotropic data covariance $C_d$ weights data unequally, and the roughness penalty $L$ couples parameters through $L^\\top L$, producing off-diagonal resolution entries such as $R_{30}$.\n- In Case E, $G$ is rank deficient but stabilization by $L = I_2$ yields a well-defined $R$; the value $R_{01}$ indicates how the second true parameter influences the first estimated parameter.\n- In Case F with strong $L = 100 I_2$, we have $A \\approx 10000 I_2 + I_2$, so $R \\approx (10001 I_2)^{-1} I_2$, making diagonal entries close to $1/10001$ and off-diagonals near zero, reflecting strong suppression of resolution.\n\nThe program implements the outlined steps, computes the six requested cross-talk coefficients $R_{ij}$ for Cases A–F, and prints them on a single line as a comma-separated list enclosed in square brackets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_resolution_entry(G: np.ndarray, Cd: np.ndarray, L: np.ndarray, i: int, j: int) -> float:\n    \"\"\"\n    Compute the cross-talk coefficient R_{ij} for the regularized least-squares estimator:\n        R = (G^T Cd^{-1} G + L^T L)^{-1} G^T Cd^{-1} G\n    Returns the scalar entry at zero-based indices (i, j).\n    \"\"\"\n    # Invert data covariance (assumed symmetric positive definite)\n    Cd_inv = np.linalg.inv(Cd)\n\n    # Assemble A and B\n    GT_CdInv = G.T @ Cd_inv\n    B = GT_CdInv @ G\n    A = B + (L.T @ L)\n\n    # Solve A R = B for R without explicitly inverting A\n    R = np.linalg.solve(A, B)\n\n    return float(R[i, j])\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A: Identity resolution, on-diagonal\n    G_A = np.eye(3)\n    Cd_A = np.eye(3)\n    L_A = np.zeros((3, 3))\n    i_A, j_A = 1, 1\n\n    # Case B: Identity resolution, off-diagonal\n    G_B = np.eye(3)\n    Cd_B = np.eye(3)\n    L_B = np.zeros((3, 3))\n    i_B, j_B = 0, 2\n\n    # Case C: Blurred forward operator with first-difference regularization\n    G_C = np.array([\n        [1.0, 0.5, 0.0, 0.0],\n        [0.5, 1.0, 0.5, 0.0],\n        [0.0, 0.5, 1.0, 0.5],\n        [0.0, 0.0, 0.5, 1.0]\n    ])\n    Cd_C = np.eye(4)\n    L_C = np.array([\n        [-1.0,  1.0,  0.0,  0.0],\n        [ 0.0, -1.0,  1.0,  0.0],\n        [ 0.0,  0.0, -1.0,  1.0]\n    ])\n    i_C, j_C = 1, 2\n\n    # Case D: Identity forward operator, anisotropic Cd, first-difference L\n    G_D = np.eye(4)\n    Cd_D = np.diag([0.04, 1.0, 0.25, 9.0])\n    L_D = np.array([\n        [-1.0,  1.0,  0.0,  0.0],\n        [ 0.0, -1.0,  1.0,  0.0],\n        [ 0.0,  0.0, -1.0,  1.0]\n    ])\n    i_D, j_D = 3, 0\n\n    # Case E: Rank-deficient G, stabilized by identity L\n    G_E = np.array([\n        [1.0, 1.0],\n        [2.0, 2.0]\n    ])\n    Cd_E = np.eye(2)\n    L_E = np.eye(2)\n    i_E, j_E = 0, 1\n\n    # Case F: Strong identity regularization suppressing resolution\n    G_F = np.eye(2)\n    Cd_F = np.eye(2)\n    L_F = 100.0 * np.eye(2)\n    i_F, j_F = 1, 1\n\n    test_cases = [\n        (G_A, Cd_A, L_A, i_A, j_A),\n        (G_B, Cd_B, L_B, i_B, j_B),\n        (G_C, Cd_C, L_C, i_C, j_C),\n        (G_D, Cd_D, L_D, i_D, j_D),\n        (G_E, Cd_E, L_E, i_E, j_E),\n        (G_F, Cd_F, L_F, i_F, j_F),\n    ]\n\n    results = []\n    for G, Cd, L, i, j in test_cases:\n        res_ij = compute_resolution_entry(G, Cd, L, i, j)\n        results.append(res_ij)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}