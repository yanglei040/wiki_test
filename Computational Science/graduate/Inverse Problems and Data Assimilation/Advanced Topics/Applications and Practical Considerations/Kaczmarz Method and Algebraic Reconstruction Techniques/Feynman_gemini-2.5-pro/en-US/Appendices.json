{
    "hands_on_practices": [
        {
            "introduction": "The Kaczmarz method is best understood through its geometric interpretation as a series of projections. This first practice provides a concrete, step-by-step calculation for a simple tomographic system, allowing you to manually execute the Algebraic Reconstruction Technique (ART) and build a solid intuition for how each update moves the solution estimate closer to satisfying one of the measurement equations. By working through this exercise, you will directly engage with the core update rule and see how the system matrix and measurements translate into a geometric update in the solution space.",
            "id": "3393590",
            "problem": "Consider a simple parallel-beam Computed Tomography (CT) discretization on a $3\\times 3$ pixel grid of unit square pixels. Let the unknown image be represented by a vector $x \\in \\mathbb{R}^{9}$ in row-major order, i.e., $x = (x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}, x_{7}, x_{8}, x_{9})$ corresponds to rows from top to bottom and columns from left to right. The forward model is the standard line integral discretization: each measurement is the sum of pixel values weighted by the path length of the ray through that pixel. The measurements are modeled as $A x = b$, where each row of $A$ corresponds to a ray, and $b$ contains the corresponding recorded line integrals.\n\nYou are given four rays:\n- Ray $R_{1}$: horizontal through the middle row, crossing pixels $(x_{4}, x_{5}, x_{6})$ with path lengths $1, 1, 1$.\n- Ray $R_{2}$: vertical through the middle column, crossing pixels $(x_{2}, x_{5}, x_{8})$ with path lengths $1, 1, 1$.\n- Ray $R_{3}$: main diagonal from the top-left to the bottom-right crossing pixels $(x_{1}, x_{5}, x_{9})$ along the diagonal with path length $\\sqrt{2}$ in each crossed pixel.\n- Ray $R_{4}$: anti-diagonal from the top-right to the bottom-left crossing pixels $(x_{3}, x_{5}, x_{7})$ along the diagonal with path length $\\sqrt{2}$ in each crossed pixel.\n\nThe recorded measurements are\n$$\nb = \\big(4,\\ 6,\\ 4\\sqrt{2},\\ 5\\sqrt{2}\\big).\n$$\n\nTasks:\n1. Assemble the system matrix $A \\in \\mathbb{R}^{4\\times 9}$ explicitly from the stated geometry.\n2. Starting from the initial guess $x^{(0)} = 0 \\in \\mathbb{R}^{9}$, perform two successive iterations of the Algebraic Reconstruction Technique (ART) using the Kaczmarz method with relaxation parameter equal to $1$, projecting sequentially onto the hyperplanes defined by $R_{1}$ and then $R_{2}$ (i.e., use the first row of $A$ and its measurement in the first step, and the second row in the second step).\n3. Report the updated pixel vector $x^{(2)}$ after exactly two ART steps as a single row vector.\n\nExpress your final answer in exact form; do not round. Provide the answer as a single row matrix.",
            "solution": "We begin from the standard linear forward model for line-integral tomography, $A x = b$. For each ray indexed by $i$, the measurement is $b_i = a_i^\\top x$, where the vector $a_i \\in \\mathbb{R}^9$ contains the path lengths of ray $i$ through each pixel and forms the $i$-th row of the system matrix $A$. Assembling $A$ requires identifying which pixels are intersected by each ray and setting the corresponding entries of $a_i$ to the path lengths; all other entries are zero.\n\nFrom the given geometry:\n- For ray $R_{1}$ (middle row), the path lengths are $1$ for pixels $(x_{4}, x_{5}, x_{6})$, so the first row of $A$ is\n$$\na_1^\\top = (0,\\ 0,\\ 0,\\ 1,\\ 1,\\ 1,\\ 0,\\ 0,\\ 0).\n$$\n- For ray $R_{2}$ (middle column), the path lengths are $1$ for pixels $(x_{2}, x_{5}, x_{8})$, so the second row is\n$$\na_2^\\top = (0,\\ 1,\\ 0,\\ 0,\\ 1,\\ 0,\\ 0,\\ 1,\\ 0).\n$$\n- For ray $R_{3}$ (main diagonal), the path lengths are $\\sqrt{2}$ for pixels $(x_{1}, x_{5}, x_{9})$, so the third row is\n$$\na_3^\\top = (\\sqrt{2},\\ 0,\\ 0,\\ 0,\\ \\sqrt{2},\\ 0,\\ 0,\\ 0,\\ \\sqrt{2}).\n$$\n- For ray $R_{4}$ (anti-diagonal), the path lengths are $\\sqrt{2}$ for pixels $(x_{3}, x_{5}, x_{7})$, so the fourth row is\n$$\na_4^\\top = (0,\\ 0,\\ \\sqrt{2},\\ 0,\\ \\sqrt{2},\\ 0,\\ \\sqrt{2},\\ 0,\\ 0).\n$$\nThus,\n$$\nA = \n\\begin{pmatrix}\n0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\\\\n\\sqrt{2} & 0 & 0 & 0 & \\sqrt{2} & 0 & 0 & 0 & \\sqrt{2} \\\\\n0 & 0 & \\sqrt{2} & 0 & \\sqrt{2} & 0 & \\sqrt{2} & 0 & 0\n\\end{pmatrix},\n\\quad\nb = \n\\begin{pmatrix}\n4 \\\\ 6 \\\\ 4\\sqrt{2} \\\\ 5\\sqrt{2}\n\\end{pmatrix}.\n$$\n\nNext, we apply the Kaczmarz update for the $i$-th equation, which projects the current estimate $x^{(k)}$ orthogonally onto the hyperplane $\\{x : a_i^\\top x = b_i\\}$. With relaxation parameter $\\omega=1$, the update is:\n$$\nx^{(k+1)} = x^{(k)} + \\frac{b_i - a_i^\\top x^{(k)}}{\\|a_i\\|^2} a_i.\n$$\nWe perform two successive iterations starting from $x^{(0)} = 0$, first using $a_1$ and then $a_2$.\n\nStep $1$ (use $i=1$):\n- Compute the squared norm $\\|a_1\\|^2 = 1^2 + 1^2 + 1^2 = 3$.\n- Compute $a_1^\\top x^{(0)} = 0$ because $x^{(0)} = 0$.\n- Compute the correction factor $c_1 = \\frac{b_1 - a_1^\\top x^{(0)}}{\\|a_1\\|^2} = \\frac{4 - 0}{3} = \\frac{4}{3}$.\n- Update: $x^{(1)} = x^{(0)} + c_1 a_1$. Since $a_1$ has ones at positions $4, 5, 6$ and zeros elsewhere, we obtain\n$$\nx^{(1)} = (0,\\ 0,\\ 0,\\ \\tfrac{4}{3},\\ \\tfrac{4}{3},\\ \\tfrac{4}{3},\\ 0,\\ 0,\\ 0).\n$$\n\nStep $2$ (use $i=2$):\n- Compute the squared norm $\\|a_2\\|^2 = 1^2 + 1^2 + 1^2 = 3$.\n- Compute $a_2^\\top x^{(1)} = x_{2}^{(1)} + x_{5}^{(1)} + x_{8}^{(1)} = 0 + \\tfrac{4}{3} + 0 = \\tfrac{4}{3}$.\n- Compute the correction factor $c_2 = \\frac{b_2 - a_2^\\top x^{(1)}}{\\|a_2\\|^2} = \\frac{6 - \\tfrac{4}{3}}{3} = \\frac{\\tfrac{18}{3} - \\tfrac{4}{3}}{3} = \\frac{\\tfrac{14}{3}}{3} = \\frac{14}{9}$.\n- Update: $x^{(2)} = x^{(1)} + c_2 a_2$. Since $a_2$ has ones at positions $2, 5, 8$ and zeros elsewhere, we obtain\n$$\nx^{(2)} = (0,\\ \\tfrac{14}{9},\\ 0,\\ \\tfrac{4}{3},\\ \\tfrac{4}{3} + \\tfrac{14}{9},\\ \\tfrac{4}{3},\\ 0,\\ \\tfrac{14}{9},\\ 0).\n$$\nCombine the middle element:\n$$\n\\tfrac{4}{3} + \\tfrac{14}{9} = \\tfrac{12}{9} + \\tfrac{14}{9} = \\tfrac{26}{9}.\n$$\nThus,\n$$\nx^{(2)} = \\left(0,\\ \\tfrac{14}{9},\\ 0,\\ \\tfrac{4}{3},\\ \\tfrac{26}{9},\\ \\tfrac{4}{3},\\ 0,\\ \\tfrac{14}{9},\\ 0\\right).\n$$\n\nThis is the exact updated pixel vector after two ART steps in the specified order.",
            "answer": "$$\\boxed{\\begin{pmatrix}0 & \\tfrac{14}{9} & 0 & \\tfrac{4}{3} & \\tfrac{26}{9} & \\tfrac{4}{3} & 0 & \\tfrac{14}{9} & 0\\end{pmatrix}}$$"
        },
        {
            "introduction": "While the basic Kaczmarz method converges, its speed is a critical practical concern, which can often be improved by introducing a relaxation parameter. This exercise moves from mechanics to analysis, guiding you through a derivation to find the optimal relaxation parameter $\\omega$ that maximizes the guaranteed convergence rate for a representative toy problem. By connecting the algorithm's performance to the geometric \"coherence\" between the system's equations, this practice reveals the deep interplay between a system's properties and the optimal tuning of the iterative solver.",
            "id": "3266569",
            "problem": "Consider a consistent linear system $A x = b$ in $\\mathbb{R}^{2}$ with two measurement rows $a_{1}^{\\top} x = b_{1}$ and $a_{2}^{\\top} x = b_{2}$, where $a_{1}, a_{2} \\in \\mathbb{R}^{2}$ satisfy $\\|a_{1}\\| = \\|a_{2}\\| = 1$ and $a_{1}^{\\top} a_{2} = \\mu$ with $0 \\leq \\mu < 1$. Let $x^{\\star}$ denote the exact solution. The Kaczmarz method, also known as the Algebraic Reconstruction Technique (ART), with relaxation parameter $\\omega \\in [0, 2]$ performs updates of the form\n$$\nx_{k+1} = x_{k} + \\omega \\frac{b_{i} - a_{i}^{\\top} x_{k}}{\\|a_{i}\\|^{2}} a_{i},\n$$\nwhere at iteration $k$, the row index $i \\in \\{1, 2\\}$ is selected uniformly at random. Let $e_{k} = x_{k} - x^{\\star}$ denote the error vector.\n\nStarting from fundamental definitions of orthogonal projections and their properties (symmetry and idempotence), derive a bound on the conditional expectation $\\mathbb{E}\\left[\\|e_{k+1}\\|^{2} \\mid e_{k}\\right]$ in terms of $\\omega$, $\\mu$, and $\\|e_{k}\\|^{2}$, by explicitly relating the average projected energy $\\frac{1}{2}\\sum_{i=1}^{2}\\|P_{i} e_{k}\\|^{2}$ to the eigenvalues of the operator $\\sum_{i=1}^{2} P_{i}$, where $P_{i}$ is the orthogonal projector onto $\\text{span}(a_{i})$. Use the coherence parameter $\\mu$ to bound this operator and obtain a guaranteed one-step contraction bound on the expected error.\n\nThen, using this coherence-based bound, choose the relaxation parameter $\\omega \\in [0, 2]$ that optimizes the guaranteed expected convergence (i.e., minimizes the upper bound on $\\mathbb{E}\\left[\\|e_{k+1}\\|^{2} \\mid e_{k}\\right]$). Provide your final choice of $\\omega$ as a single exact value. No rounding is required.",
            "solution": "We analyze the evolution of the error vector $e_k = x_k - x^\\star$. Given that $\\|a_i\\| = 1$ and $a_i^\\top x^\\star = b_i$, the update rule for the error is:\n$$\ne_{k+1} = e_k - \\omega (a_i^\\top e_k) a_i.\n$$\nLet $P_i = a_i a_i^\\top$ be the orthogonal projection operator onto the span of $a_i$. The error update can be written compactly as $e_{k+1} = (I - \\omega P_i) e_k$. We analyze the squared norm of the error, using the symmetry ($P_i^\\top = P_i$) and idempotence ($P_i^2 = P_i$) of orthogonal projectors:\n$$\n\\|e_{k+1}\\|^2 = \\langle (I - \\omega P_i)e_k, (I - \\omega P_i)e_k \\rangle = \\|e_k\\|^2 - (2\\omega - \\omega^2) \\langle e_k, P_i e_k \\rangle = \\|e_k\\|^2 - (2\\omega - \\omega^2) \\|P_i e_k\\|^2.\n$$\nSince the index $i$ is chosen uniformly at random from $\\{1, 2\\}$, we take the conditional expectation given $e_k$:\n$$\n\\mathbb{E}\\left[\\|e_{k+1}\\|^2 \\mid e_k\\right] = \\frac{1}{2} \\sum_{i=1}^{2} \\left( \\|e_k\\|^2 - (2\\omega - \\omega^2)\\|P_i e_k\\|^2 \\right) = \\|e_k\\|^2 - \\frac{1}{2}(2\\omega - \\omega^2) \\sum_{i=1}^{2} \\|P_i e_k\\|^2.\n$$\nLet $S = \\sum_{i=1}^2 P_i = P_1 + P_2$. The sum of squared projected norms can be written as $\\sum_{i=1}^2 \\|P_i e_k\\|^2 = \\langle e_k, S e_k \\rangle$. This yields:\n$$\n\\mathbb{E}\\left[\\|e_{k+1}\\|^2 \\mid e_k\\right] = \\|e_k\\|^2 - \\frac{1}{2}(2\\omega - \\omega^2) \\langle e_k, S e_k \\rangle.\n$$\nTo obtain a guaranteed convergence bound, we use the Rayleigh-Ritz theorem, which states that $\\langle e_k, S e_k \\rangle \\ge \\lambda_{\\min}(S) \\|e_k\\|^2$, where $\\lambda_{\\min}(S)$ is the smallest eigenvalue of $S$. Since $\\omega \\in [0, 2]$, the term $2\\omega - \\omega^2$ is non-negative. This gives the upper bound:\n$$\n\\mathbb{E}\\left[\\|e_{k+1}\\|^2 \\mid e_k\\right] \\le \\|e_k\\|^2 - \\frac{1}{2}(2\\omega - \\omega^2) \\lambda_{\\min}(S) \\|e_k\\|^2 = \\left( 1 - \\frac{1}{2}(2\\omega - \\omega^2) \\lambda_{\\min}(S) \\right) \\|e_k\\|^2.\n$$\nTo find $\\lambda_{\\min}(S)$, we determine the eigenvalues of $S = a_1 a_1^\\top + a_2 a_2^\\top$. The trace of $S$ is $\\text{Tr}(S) = \\text{Tr}(a_1 a_1^\\top) + \\text{Tr}(a_2 a_2^\\top) = \\|a_1\\|^2 + \\|a_2\\|^2 = 1+1=2$. The determinant is $\\det(S) = \\det(A A^\\top)$ where $A = [a_1 \\quad a_2]$. This equals $\\det(A^\\top A) = \\det\\begin{pmatrix} 1 & \\mu \\\\ \\mu & 1 \\end{pmatrix} = 1 - \\mu^2$. The characteristic equation for the eigenvalues $\\lambda$ of $S$ is $\\lambda^2 - \\text{Tr}(S)\\lambda + \\det(S) = 0$, which is $\\lambda^2 - 2\\lambda + (1-\\mu^2) = 0$. The solutions are $\\lambda = 1 \\pm \\mu$. Since $0 \\le \\mu  1$, the smallest eigenvalue is $\\lambda_{\\min}(S) = 1-\\mu$.\n\nThe guaranteed convergence bound is determined by the contraction factor $C(\\omega) = 1 - \\frac{1}{2}(2\\omega - \\omega^2)(1-\\mu)$. To optimize the convergence rate, we must minimize $C(\\omega)$, which is equivalent to maximizing the term being subtracted. As $1-\\mu > 0$, we need to maximize the function $f(\\omega) = 2\\omega - \\omega^2$ on the interval $[0, 2]$. This downward-opening parabola has its vertex, and thus its maximum, at $\\omega = -2/(2(-1)) = 1$. This value lies in the allowed interval. Therefore, the optimal choice for the relaxation parameter is $\\omega=1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Many real-world inverse problems, from medical imaging to geophysical exploration, are fundamentally nonlinear, requiring extensions of methods originally developed for linear systems. This advanced practice challenges you to develop and implement the Newton-Kaczmarz method, a powerful algorithm that combines the component-wise processing of Kaczmarz with the local linearization of Newton's method. Through both a theoretical derivation of local convergence and a hands-on coding implementation, you will explore its behavior in various scenarios, including the presence of noise and strong nonlinearities, gaining insight into its practical power and limitations.",
            "id": "3393632",
            "problem": "Consider a nonlinear system of equations $F(x) = b$ with $F : \\mathbb{R}^n \\to \\mathbb{R}^m$, where $F(x) = \\left(F_1(x), \\dots, F_m(x)\\right)^\\top$ and $b \\in \\mathbb{R}^m$. Assume that $F$ is continuously differentiable in an open neighborhood of a solution $x^\\star \\in \\mathbb{R}^n$ satisfying $F(x^\\star) = b$, and that the Jacobian $J_F(x) \\in \\mathbb{R}^{m \\times n}$ has rows $\\nabla F_i(x)^\\top \\in \\mathbb{R}^n$. The Newton–Kaczmarz approach performs scalarized Gauss–Newton updates by cycling through the component functions $F_i$ and, at each step, linearizing the scalar equation $F_i(x) = b_i$ at the current iterate. A damping factor $\\omega \\in (0,2)$ scales the update. In the presence of potentially vanishing directional derivatives for some $i$, the implementation must skip an update whenever the denominator would be smaller than a given threshold $\\tau  0$ to avoid division by a number too close to zero.\n\nYour tasks are:\n\n- Derive, from first principles, a local convergence result for the damped cyclic Newton–Kaczmarz method under the following assumptions:\n  - The Jacobian $J_F$ is Lipschitz continuous in a neighborhood of $x^\\star$ with Lipschitz constant $L \\ge 0$, meaning $\\|\\nabla F_i(x) - \\nabla F_i(y)\\| \\le L \\|x - y\\|$ for all $i \\in \\{1,\\dots,m\\}$ and all $x,y$ in the neighborhood.\n  - The Jacobian at the solution $J_F(x^\\star)$ has full column rank $n$.\n  - The damping factor satisfies $\\omega \\in (0,2)$.\n  - A cyclic control sequence is used, i.e., the index $i$ increases by $1$ modulo $m$ at each step.\n  - There exists $\\tau  0$ such that updates for which $\\|\\nabla F_i(x)\\|^2  \\tau$ are skipped.\n  Provide a rigorous statement of local convergence in the form of a contraction inequality over one cycle together with a second-order remainder term and specify how the constants depend on problem data. Start from the fundamental definition of Lipschitz continuity of the Jacobian and the mean value theorem applied componentwise; do not use any pre-packaged convergence theorems. Your derivation should clarify what is meant by “local” in terms of a neighborhood around $x^\\star$ and how the damping $\\omega$ influences the contraction factor.\n\n- Implement the damped cyclic Newton–Kaczmarz method that, for a given $F$ and $b$, performs the following at iteration $k$ for index $i = (k \\bmod m) + 1$:\n  - Compute the scalar residual $r_i(x_k) = b_i - F_i(x_k)$ and the gradient $g_i(x_k) = \\nabla F_i(x_k)$.\n  - If $\\|g_i(x_k)\\|^2 \\ge \\tau$, update $x_{k+1} = x_k + \\omega \\, \\frac{r_i(x_k)}{\\|g_i(x_k)\\|^2} g_i(x_k)$; otherwise set $x_{k+1} = x_k$ (skip).\n  The method should run for a prescribed number of full sweeps, where one sweep equals $m$ iterations.\n\nFor numerical experiments, you will use a nonlinear family inspired by algebraic reconstruction techniques. Fix a matrix $A \\in \\mathbb{R}^{m \\times n}$ with rows $a_i^\\top \\in \\mathbb{R}^n$ and a scalar nonlinearity parameter $c \\ge 0$. Define component functions\n$$\nF_i(x) = a_i^\\top x + \\frac{c}{2} \\left(a_i^\\top x\\right)^2 \\quad \\text{for } i \\in \\{1,\\dots,m\\}.\n$$\nThen $\\nabla F_i(x) = \\left(1 + c \\, a_i^\\top x\\right) a_i$. Given a target solution $x^\\star \\in \\mathbb{R}^n$, define $b \\in \\mathbb{R}^m$ by $b_i = F_i(x^\\star)$, except in the inconsistent noisy case below.\n\nYou must implement your program to run the following four test cases. In all cases use the Euclidean norm, a skip threshold $\\tau = 10^{-12}$, cyclic control, and the exact damping indicated.\n\n- Test case $1$ (well-conditioned, modest nonlinearity, near-initialization):\n  - $n = 3$, $m = 6$, $c = 1$.\n  - $A = \\begin{bmatrix}\n  1  2  -1 \\\\\n  0.5  -1  1.5 \\\\\n  2  0  1 \\\\\n  -1.5  1  0.5 \\\\\n  1.2  -0.7  0.3 \\\\\n  0  1  1\n  \\end{bmatrix}$.\n  - $x^\\star = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.05 \\end{bmatrix}$, $b_i = F_i(x^\\star)$.\n  - Initial $x_0 = x^\\star + \\begin{bmatrix} 0.01 \\\\ -0.01 \\\\ 0.005 \\end{bmatrix}$.\n  - Damping $\\omega = 1$.\n  - Perform $8$ full sweeps.\n\n- Test case $2$ (near-zero directional derivative for one component at initialization):\n  - $n = 2$, $m = 3$, $c = 1$.\n  - $A = \\begin{bmatrix}\n  1  0 \\\\\n  0  1 \\\\\n  1  1\n  \\end{bmatrix}$.\n  - $x^\\star = \\begin{bmatrix} 0.1 \\\\ 0.1 \\end{bmatrix}$, $b_i = F_i(x^\\star)$.\n  - Initial $x_0 = \\begin{bmatrix} -1 \\\\ 0.1 \\end{bmatrix}$.\n  - Damping $\\omega = 1$.\n  - Perform $12$ full sweeps.\n\n- Test case $3$ (strong nonlinearity, far initialization, damping required):\n  - $n = 3$, $m = 5$, $c = 10$.\n  - $A = \\begin{bmatrix}\n  1  -0.5  0.3 \\\\\n  -0.7  1.1  0.2 \\\\\n  0.6  0.4  -1.2 \\\\\n  1.5  0.2  0.1 \\\\\n  -0.3  0.8  0.9\n  \\end{bmatrix}$.\n  - $x^\\star = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.1 \\end{bmatrix}$, $b_i = F_i(x^\\star)$.\n  - Initial $x_0 = \\begin{bmatrix} 0.8 \\\\ -0.5 \\\\ 0.7 \\end{bmatrix}$.\n  - Damping $\\omega = 0.3$.\n  - Perform $12$ full sweeps.\n\n- Test case $4$ (inconsistent data due to additive noise):\n  - Use the same configuration as in test case $1$ with $n = 3$, $m = 6$, $c = 1$ and the same $A$ and $x^\\star$.\n  - Define $b$ by $b_i = F_i(x^\\star) + \\epsilon_i$ with fixed noise $\\epsilon = \\begin{bmatrix} 10^{-3} \\\\ -8 \\cdot 10^{-4} \\\\ 5 \\cdot 10^{-4} \\\\ -4 \\cdot 10^{-4} \\\\ 3 \\cdot 10^{-4} \\\\ -2 \\cdot 10^{-4} \\end{bmatrix}$.\n  - Initial $x_0 = x^\\star + \\begin{bmatrix} 0.01 \\\\ -0.01 \\\\ 0.005 \\end{bmatrix}$.\n  - Damping $\\omega = 1$.\n  - Perform $20$ full sweeps.\n\nFor each test case, run the algorithm and compute the final root-mean-square residual\n$$\n\\mathrm{RMS}(x) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^m \\left(F_i(x) - b_i\\right)^2}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, where each entry is the final $\\mathrm{RMS}(x)$ as a floating-point number. For example, an output with four results should look like\n$[r_1,r_2,r_3,r_4]$.",
            "solution": "We analyze the convergence of the damped cyclic Newton–Kaczmarz method for solving a nonlinear system $F(x) = b$, where $F: \\mathbb{R}^n \\to \\mathbb{R}^m$. The system has a solution $x^\\star$ where $F(x^\\star) = b$. The error at iteration $k$ is $e_k = x_k - x^\\star$. For a non-skipped step processing the $i$-th equation, the error update is:\n$$\ne_{k+1} = e_k + \\omega \\frac{F_i(x^\\star) - F_i(x_k)}{\\|\\nabla F_i(x_k)\\|^2} \\nabla F_i(x_k).\n$$\nBy the Mean Value Theorem, there exists a point $\\xi_k$ on the line segment between $x_k$ and $x^\\star$ such that $F_i(x_k) - F_i(x^\\star) = \\nabla F_i(\\xi_k)^\\top e_k$. Substituting this gives:\n$$\ne_{k+1} = e_k - \\omega \\frac{\\nabla F_i(\\xi_k)^\\top e_k}{\\|\\nabla F_i(x_k)\\|^2} \\nabla F_i(x_k).\n$$\nThe analysis is local, assuming $\\|e_k\\|$ is small. Let $g_i(x) = \\nabla F_i(x)$ and $g_i^\\star = g_i(x^\\star)$. The Lipschitz continuity of the Jacobian implies $\\|g_i(x) - g_i(y)\\| \\le L \\|x - y\\|$. We can express the gradients at $x_k$ and $\\xi_k$ as perturbations of the gradient at $x^\\star$: $g_i(x_k) = g_i^\\star + O(\\|e_k\\|)$ and $g_i(\\xi_k) = g_i^\\star + O(\\|e_k\\|)$.\nThis allows us to approximate the update by its linear part. Let $P_{g_i^\\star} = \\frac{g_i^\\star g_i^{\\star\\top}}{\\|g_i^\\star\\|^2}$ be the orthogonal projection onto the span of $g_i^\\star$. The update is approximately:\n$$\ne_{k+1} \\approx e_k - \\omega \\frac{g_i^{\\star\\top} e_k}{\\|g_i^\\star\\|^2} g_i^\\star = (I - \\omega P_{g_i^\\star}) e_k = T_i(\\omega) e_k.\n$$\nThe full update can be written as $e_{k+1} = T_i(\\omega)e_k + S_k$, where the remainder term $S_k$ can be shown to satisfy $\\|S_k\\| = O(\\|e_k\\|^2)$.\n\nAfter one full sweep of $m$ iterations starting with error $e^{(s)}$, the error becomes $e^{(s+1)}$. Let $C(\\omega) = T_m(\\omega)\\dots T_1(\\omega)$ be the linear operator for one sweep. Then:\n$$\ne^{(s+1)} = C(\\omega)e^{(s)} + S^{(s)},\n$$\nwhere the accumulated remainder $\\|S^{(s)}\\| = O(\\|e^{(s)}\\|^2)$. Taking norms gives $\\|e^{(s+1)}\\| \\le \\|C(\\omega)\\| \\|e^{(s)}\\| + C_{sweep} \\|e^{(s)}\\|^2$.\nLet $\\kappa = \\|C(\\omega)\\|$. The assumption that $J_F(x^\\star)$ has full rank implies its rows span $\\mathbb{R}^n$, which ensures that for $\\omega \\in (0,2)$, the operator norm $\\kappa  1$.\n\n**Local Convergence Statement:** There exists a radius $r > 0$ such that if the initial error $\\|x^{(0)} - x^\\star\\|  r$, the sequence of iterates $\\{x^{(s)}\\}$ generated by full sweeps of the damped Newton-Kaczmarz method converges to $x^\\star$. The convergence is at least linear, with the error satisfying:\n$$\n\\|e^{(s+1)}\\| \\le \\kappa \\|e^{(s)}\\| + C_{sweep} \\|e^{(s)}\\|^2,\n$$\nwhere $\\kappa  1$ is the contraction factor of the linearized system and $C_{sweep}\\|e^{(s)}\\|^2$ is a second-order remainder. The constants $\\kappa$ and $C_{sweep}$ depend on the problem data ($J_F(x^\\star)$, $L$) and the damping factor $\\omega$. The radius $r$ of this \"local\" neighborhood must be small enough for the linear part to dominate (e.g., $\\|e^{(s)}\\|  (1-\\kappa)/C_{sweep}$) and for the skip condition to not be triggered. The full rank of $J_F(x^\\star)$ ensures its rows are non-zero, so for a sufficiently small threshold $\\tau$, no skips occur near the solution.",
            "answer": "```python\nimport numpy as np\n\ndef run_newton_kaczmarz(n, m, c, A, x_star, x0, omega, sweeps, noise=None):\n    \"\"\"\n    Runs the damped cyclic Newton-Kaczmarz method for a given test case.\n    \"\"\"\n    tau = 1e-12\n\n    # Define the nonlinear function F and its components\n    def F_func(x_vec, A_mat, c_val):\n        ax = A_mat @ x_vec\n        return ax + 0.5 * c_val * (ax ** 2)\n\n    def Fi(x_vec, ai, c_val):\n        aix = ai @ x_vec\n        return aix + 0.5 * c_val * (aix ** 2)\n\n    # Define the gradient of a component function\n    def grad_Fi(x_vec, ai, c_val):\n        aix = ai @ x_vec\n        return (1.0 + c_val * aix) * ai\n\n    # Set up the problem\n    b = F_func(x_star, A, c)\n    if noise is not None:\n        b += noise\n\n    x = x0.copy()\n\n    # Main iteration loop\n    for _ in range(sweeps):\n        for i in range(m):\n            ai = A[i]\n            \n            # Compute residual and gradient for component i\n            ri = b[i] - Fi(x, ai, c)\n            gi = grad_Fi(x, ai, c)\n            \n            # Compute squared norm of the gradient\n            gi_norm_sq = gi @ gi\n            \n            # Update if norm is above threshold\n            if gi_norm_sq = tau:\n                x += omega * (ri / gi_norm_sq) * gi\n            # Otherwise, skip the update (x_{k+1} = x_k)\n\n    # Compute final RMS residual\n    final_F_vals = F_func(x, A, c)\n    final_residuals = final_F_vals - b\n    rms = np.sqrt(np.mean(final_residuals ** 2))\n    \n    return rms\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    \n    test_cases = [\n        # Test case 1\n        {\n            \"n\": 3, \"m\": 6, \"c\": 1.0,\n            \"A\": np.array([\n                [1.0, 2.0, -1.0],\n                [0.5, -1.0, 1.5],\n                [2.0, 0.0, 1.0],\n                [-1.5, 1.0, 0.5],\n                [1.2, -0.7, 0.3],\n                [0.0, 1.0, 1.0]\n            ]),\n            \"x_star\": np.array([0.2, -0.1, 0.05]),\n            \"x0\": np.array([0.2, -0.1, 0.05]) + np.array([0.01, -0.01, 0.005]),\n            \"omega\": 1.0,\n            \"sweeps\": 8,\n            \"noise\": None\n        },\n        # Test case 2\n        {\n            \"n\": 2, \"m\": 3, \"c\": 1.0,\n            \"A\": np.array([\n                [1.0, 0.0],\n                [0.0, 1.0],\n                [1.0, 1.0]\n            ]),\n            \"x_star\": np.array([0.1, 0.1]),\n            \"x0\": np.array([-1.0, 0.1]),\n            \"omega\": 1.0,\n            \"sweeps\": 12,\n            \"noise\": None\n        },\n        # Test case 3\n        {\n            \"n\": 3, \"m\": 5, \"c\": 10.0,\n            \"A\": np.array([\n                [1.0, -0.5, 0.3],\n                [-0.7, 1.1, 0.2],\n                [0.6, 0.4, -1.2],\n                [1.5, 0.2, 0.1],\n                [-0.3, 0.8, 0.9]\n            ]),\n            \"x_star\": np.array([0.05, -0.02, 0.1]),\n            \"x0\": np.array([0.8, -0.5, 0.7]),\n            \"omega\": 0.3,\n            \"sweeps\": 12,\n            \"noise\": None\n        },\n        # Test case 4\n        {\n            \"n\": 3, \"m\": 6, \"c\": 1.0,\n            \"A\": np.array([\n                [1.0, 2.0, -1.0],\n                [0.5, -1.0, 1.5],\n                [2.0, 0.0, 1.0],\n                [-1.5, 1.0, 0.5],\n                [1.2, -0.7, 0.3],\n                [0.0, 1.0, 1.0]\n            ]),\n            \"x_star\": np.array([0.2, -0.1, 0.05]),\n            \"x0\": np.array([0.2, -0.1, 0.05]) + np.array([0.01, -0.01, 0.005]),\n            \"omega\": 1.0,\n            \"sweeps\": 20,\n            \"noise\": np.array([1e-3, -8e-4, 5e-4, -4e-4, 3e-4, -2e-4])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        rms_val = run_newton_kaczmarz(\n            case[\"n\"], case[\"m\"], case[\"c\"], case[\"A\"],\n            case[\"x_star\"], case[\"x0\"], case[\"omega\"],\n            case[\"sweeps\"], case[\"noise\"]\n        )\n        results.append(rms_val)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}