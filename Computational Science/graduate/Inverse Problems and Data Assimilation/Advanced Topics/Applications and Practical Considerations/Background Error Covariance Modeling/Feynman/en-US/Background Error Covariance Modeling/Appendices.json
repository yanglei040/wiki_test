{
    "hands_on_practices": [
        {
            "introduction": "The performance of a variational data assimilation system depends critically on the formulation of the background error covariance matrix, $\\boldsymbol{B}$. This exercise explores this dependency from a numerical perspective by examining the Hessian of the 3D-Var cost function in a simplified setting. By computing the condition number, you will gain direct insight into how the assumed background error variances influence the shape of the cost function and, consequently, the stability and convergence speed of the minimization algorithms used to find the optimal analysis state .",
            "id": "3366744",
            "problem": "Consider the Three-Dimensional Variational data assimilation (3D-Var) objective for a two-dimensional state vector $\\mathbf{x} \\in \\mathbb{R}^{2}$,\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_{b})^{\\top} \\boldsymbol{B}^{-1} (\\mathbf{x} - \\mathbf{x}_{b}) + \\frac{1}{2}\\,(y - H \\mathbf{x})^{\\top} \\boldsymbol{R}^{-1} (y - H \\mathbf{x}),\n$$\nwhere $\\mathbf{x}_{b}$ is the background state, $\\boldsymbol{B}$ is the background error covariance, $y$ is the observation, $H$ is the linear observation operator, and $\\boldsymbol{R}$ is the observation error covariance. Assume a single scalar observation with $H = [\\,1\\ \\ 0\\,]$, $\\boldsymbol{R} = \\sigma_{o}^{2}$ (a positive scalar), and a diagonal background error covariance $\\boldsymbol{B} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2})$ with $\\lambda_{1} > 0$ and $\\lambda_{2} > 0$. Starting from this definition of $J(\\mathbf{x})$, derive the Hessian $\\nabla^{2} J(\\mathbf{x})$ and compute its spectral condition number, defined as the ratio of the largest to the smallest eigenvalue. Provide your final answer as a closed-form analytic expression in terms of $\\lambda_{1}$, $\\lambda_{2}$, and $\\sigma_{o}^{2}$. Additionally, explain in words how $\\lambda_{1}$ and $\\lambda_{2}$ influence the numerical stability of minimizing $J(\\mathbf{x})$, but the final reported answer must be only the condition number expression. No rounding is required and no units are involved.",
            "solution": "The user-provided problem is assessed to be valid. It is a scientifically grounded, well-posed, and objective problem within the field of data assimilation. All necessary data and definitions are provided, and there are no contradictions or ambiguities. We may therefore proceed with a solution.\n\nThe Three-Dimensional Variational (3D-Var) objective function for a state vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ is given by:\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_{b})^{\\top} \\boldsymbol{B}^{-1} (\\mathbf{x} - \\mathbf{x}_{b}) + \\frac{1}{2}\\,(y - H \\mathbf{x})^{\\top} \\boldsymbol{R}^{-1} (y - H \\mathbf{x})\n$$\nThis is a quadratic function of $\\mathbf{x}$. To find the Hessian, we first compute the gradient of $J(\\mathbf{x})$ with respect to $\\mathbf{x}$. The gradient of a quadratic form $\\frac{1}{2}(\\mathbf{z}-\\mathbf{c})^{\\top}\\boldsymbol{M}(\\mathbf{z}-\\mathbf{c})$ with respect to $\\mathbf{z}$ is $\\boldsymbol{M}(\\mathbf{z}-\\mathbf{c})$ if $\\boldsymbol{M}$ is symmetric. Also, the gradient of $\\frac{1}{2}(d-A\\mathbf{z})^{\\top}N(d-A\\mathbf{z})$ with respect to $\\mathbf{z}$ is $-A^{\\top}N(d-A\\mathbf{z}) = A^{\\top}N(A\\mathbf{z}-d)$.\nThe background term is $J_b(\\mathbf{x}) = \\frac{1}{2}\\,(\\mathbf{x} - \\mathbf{x}_{b})^{\\top} \\boldsymbol{B}^{-1} (\\mathbf{x} - \\mathbf{x}_{b})$. Its gradient is:\n$$\n\\nabla J_b(\\mathbf{x}) = \\boldsymbol{B}^{-1} (\\mathbf{x} - \\mathbf{x}_{b})\n$$\nThe observation term is $J_o(\\mathbf{x}) = \\frac{1}{2}\\,(y - H \\mathbf{x})^{\\top} \\boldsymbol{R}^{-1} (y - H \\mathbf{x})$. Its gradient is:\n$$\n\\nabla J_o(\\mathbf{x}) = -H^{\\top} \\boldsymbol{R}^{-1} (y - H \\mathbf{x}) = H^{\\top} \\boldsymbol{R}^{-1} (H \\mathbf{x} - y)\n$$\nThe total gradient $\\nabla J(\\mathbf{x})$ is the sum of these two terms:\n$$\n\\nabla J(\\mathbf{x}) = \\nabla J_b(\\mathbf{x}) + \\nabla J_o(\\mathbf{x}) = \\boldsymbol{B}^{-1} (\\mathbf{x} - \\mathbf{x}_{b}) + H^{\\top} \\boldsymbol{R}^{-1} (H \\mathbf{x} - y)\n$$\nThe Hessian matrix, $\\nabla^2 J(\\mathbf{x})$, is obtained by differentiating the gradient with respect to $\\mathbf{x}^{\\top}$.\n$$\n\\nabla^2 J(\\mathbf{x}) = \\frac{\\partial}{\\partial \\mathbf{x}^{\\top}} \\left( \\boldsymbol{B}^{-1} \\mathbf{x} - \\boldsymbol{B}^{-1} \\mathbf{x}_{b} + H^{\\top} \\boldsymbol{R}^{-1} H \\mathbf{x} - H^{\\top} \\boldsymbol{R}^{-1} y \\right)\n$$\nThe terms $-\\boldsymbol{B}^{-1} \\mathbf{x}_{b}$ and $-H^{\\top} \\boldsymbol{R}^{-1} y$ are constant with respect to $\\mathbf{x}$, so their derivatives are zero. The derivative of $\\boldsymbol{B}^{-1}\\mathbf{x}$ with respect to $\\mathbf{x}^{\\top}$ is $\\boldsymbol{B}^{-1}$. The derivative of $(H^{\\top} \\boldsymbol{R}^{-1} H)\\mathbf{x}$ with respect to $\\mathbf{x}^{\\top}$ is $H^{\\top} \\boldsymbol{R}^{-1} H$.\nThus, the Hessian is:\n$$\n\\nabla^2 J(\\mathbf{x}) = \\boldsymbol{B}^{-1} + H^{\\top} \\boldsymbol{R}^{-1} H\n$$\nThis is a constant matrix, which is expected for a quadratic objective function. Let us denote the Hessian by $\\mathcal{H}$.\n\nNow, we substitute the specific forms provided in the problem. The state vector is $\\mathbf{x} \\in \\mathbb{R}^{2}$.\nThe background error covariance is $\\boldsymbol{B} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}) = \\begin{pmatrix} \\lambda_{1} & 0 \\\\ 0 & \\lambda_{2} \\end{pmatrix}$. Its inverse is:\n$$\n\\boldsymbol{B}^{-1} = \\begin{pmatrix} \\frac{1}{\\lambda_{1}} & 0 \\\\ 0 & \\frac{1}{\\lambda_{2}} \\end{pmatrix}\n$$\nThe observation operator is $H = [\\,1\\ \\ 0\\,]$, so its transpose is $H^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe observation error covariance is $\\boldsymbol{R} = \\sigma_{o}^{2}$, a scalar, so its inverse is $\\boldsymbol{R}^{-1} = \\frac{1}{\\sigma_{o}^{2}}$.\n\nWe compute the term $H^{\\top} \\boldsymbol{R}^{-1} H$:\n$$\nH^{\\top} \\boldsymbol{R}^{-1} H = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\frac{1}{\\sigma_{o}^{2}} [\\,1\\ \\ 0\\,] = \\frac{1}{\\sigma_{o}^{2}} \\begin{pmatrix} 1 \\cdot 1 & 1 \\cdot 0 \\\\ 0 \\cdot 1 & 0 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma_{o}^{2}} & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe Hessian matrix $\\mathcal{H}$ is the sum of $\\boldsymbol{B}^{-1}$ and $H^{\\top} \\boldsymbol{R}^{-1} H$:\n$$\n\\mathcal{H} = \\begin{pmatrix} \\frac{1}{\\lambda_{1}} & 0 \\\\ 0 & \\frac{1}{\\lambda_{2}} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{\\sigma_{o}^{2}} & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\lambda_{1}} + \\frac{1}{\\sigma_{o}^{2}} & 0 \\\\ 0 & \\frac{1}{\\lambda_{2}} \\end{pmatrix}\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. Therefore, the two eigenvalues of the Hessian $\\mathcal{H}$ are:\n$$\n\\mu_{1} = \\frac{1}{\\lambda_{1}} + \\frac{1}{\\sigma_{o}^{2}}\n$$\n$$\n\\mu_{2} = \\frac{1}{\\lambda_{2}}\n$$\nSince $\\lambda_{1} > 0$, $\\lambda_{2} > 0$, and $\\sigma_{o}^{2} > 0$, both eigenvalues $\\mu_1$ and $\\mu_2$ are positive, confirming that the Hessian is positive definite and the objective function $J(\\mathbf{x})$ has a unique minimum.\n\nThe spectral condition number, $\\kappa(\\mathcal{H})$, is defined as the ratio of the largest eigenvalue to the smallest eigenvalue.\n$$\n\\kappa(\\mathcal{H}) = \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} = \\frac{\\max(\\mu_{1}, \\mu_{2})}{\\min(\\mu_{1}, \\mu_{2})}\n$$\nSubstituting the expressions for $\\mu_{1}$ and $\\mu_{2}$, we get the final expression for the condition number:\n$$\n\\kappa(\\mathcal{H}) = \\frac{\\max\\left(\\frac{1}{\\lambda_{1}} + \\frac{1}{\\sigma_{o}^{2}}, \\frac{1}{\\lambda_{2}}\\right)}{\\min\\left(\\frac{1}{\\lambda_{1}} + \\frac{1}{\\sigma_{o}^{2}}, \\frac{1}{\\lambda_{2}}\\right)}\n$$\n\nRegarding the influence of $\\lambda_{1}$ and $\\lambda_{2}$ on numerical stability:\nThe condition number of the Hessian matrix is a critical factor for the convergence rate of iterative optimization algorithms, such as gradient descent, used to find the minimum of $J(\\mathbf{x})$. A large condition number signifies an ill-conditioned problem, where the level sets of $J(\\mathbf{x})$ are highly elongated, leading to slow convergence. A condition number close to $1$ implies a well-conditioned problem with nearly circular level sets and fast convergence.\n\nThe parameters $\\lambda_{1}$ and $\\lambda_{2}$ represent the variances of the background error for the two components of the state vector. They quantify the uncertainty in the background state $\\mathbf{x}_b$. Their values directly influence the eigenvalues $\\mu_1 = 1/\\lambda_1 + 1/\\sigma_o^2$ and $\\mu_2 = 1/\\lambda_2$.\nThe condition number becomes large if there is a significant disparity between the magnitudes of $\\mu_1$ and $\\mu_2$.\n- A very small value for $\\lambda_1$ or $\\lambda_2$ corresponds to high confidence in the respective component of the background state. This leads to a very large value for $1/\\lambda_1$ or $1/\\lambda_2$, which in turn increases the magnitude of the corresponding eigenvalue ($\\mu_1$ or $\\mu_2$).\n- Conversely, a large value for $\\lambda_1$ or $\\lambda_2$ (low confidence) makes the corresponding inverse term small.\nIll-conditioning, and thus poor numerical stability, arises when the scales of uncertainty across the state space are very different. For instance, if $\\lambda_1$ is very large (weak background constraint on state component $1$) and $\\lambda_2$ is very small (strong background constraint on state component $2$), then $\\mu_1 \\approx 1/\\sigma_o^2$ while $\\mu_2 = 1/\\lambda_2$ is very large. This leads to a large condition number $\\kappa \\approx \\mu_2/\\mu_1$. Similarly, if $\\lambda_1$ is very small, $\\mu_1$ becomes very large, and if $\\lambda_2$ is chosen to make $\\mu_2$ comparatively small (e.g., $\\lambda_2$ is large), the condition number will again be large.\nIn essence, a large mismatch between the background error variances $\\lambda_1$ and $\\lambda_2$ can create a significant disparity in the eigenvalues of the Hessian, leading to an ill-conditioned optimization problem and compromising numerical stability.",
            "answer": "$$\n\\boxed{\\frac{\\max\\left(\\frac{1}{\\lambda_{1}} + \\frac{1}{\\sigma_{o}^{2}}, \\frac{1}{\\lambda_{2}}\\right)}{\\min\\left(\\frac{1}{\\lambda_{1}} + \\frac{1}{\\sigma_{o}^{2}}, \\frac{1}{\\lambda_{2}}\\right)}}\n$$"
        },
        {
            "introduction": "After understanding why the structure of the background error covariance $B$ is critical, the next step is to learn how to construct it. A powerful and widely used method models the inverse of $B$ as a differential operator, which naturally introduces physically-motivated spatial correlations. This hands-on coding exercise  guides you through the process of discretizing a second-order elliptic operator on a grid to build a sparse precision matrix, a fundamental building block for many advanced data assimilation systems.",
            "id": "3366804",
            "problem": "Consider a two-dimensional discrete rectangular grid with $N_x$ nodes in the $x$-direction and $N_y$ nodes in the $y$-direction, and uniform grid spacings $h_x$ and $h_y$. In Bayesian inverse problems and data assimilation, a common way to model background error covariance is to posit a Gaussian prior whose precision (the inverse covariance) is generated by a second-order elliptic operator. Let the operator be defined in continuum form by\n$$\n\\mathcal{L} u = - \\nabla \\cdot (A \\nabla u) + \\mu u,\n$$\nwhere $A = \\mathrm{diag}(a_x, a_y)$ is a constant, symmetric, positive definite diffusion tensor with $a_x > 0$ and $a_y > 0$, and $\\mu \\ge 0$ is a constant zero-order term. On the boundary, impose homogeneous Neumann boundary conditions, i.e., $A \\nabla u \\cdot \\boldsymbol{n} = 0$ where $\\boldsymbol{n}$ is the outward unit normal. You must construct the discrete sparse precision matrix $B^{-1}$ associated with $\\mathcal{L}$ on this grid, using a second-order accurate finite difference scheme derived from the divergence form $-\\nabla \\cdot (A \\nabla u)$ so that the Neumann boundary conditions correspond to zero-flux across the boundary.\n\nBegin from the following fundamental base:\n- The Gaussian prior with precision $B^{-1}$ can be characterized by a quadratic energy functional in the continuum, \n$$\nJ(u) = \\frac{1}{2}\\int_{\\Omega} \\left( a_x \\left|\\frac{\\partial u}{\\partial x}\\right|^2 + a_y \\left|\\frac{\\partial u}{\\partial y}\\right|^2 + \\mu |u|^2 \\right)\\, \\mathrm{d}x \\mathrm{d}y,\n$$\nwhose Eulerâ€“Lagrange equation under homogeneous Neumann boundary conditions yields the operator $\\mathcal{L}$.\n- On a uniform grid, a second-order accurate discretization of $-\\nabla \\cdot (A \\nabla u)$ in divergence form uses flux differences across cell faces. For interior grid points, this results in a five-point stencil. At boundaries, zero-flux removes the missing face contribution, altering the stencil weights at edges and corners but preserving symmetry.\n\nYour tasks:\n1. Derive an algorithm to assemble the sparse matrix $B^{-1}$ in Compressed Sparse Row (CSR) format corresponding to the operator $\\mathcal{L}$ with homogeneous Neumann boundary conditions. Use a node-based discretization where the linear index $k \\in \\{0,1,\\dots,N_xN_y-1\\}$ maps the two-dimensional indices $(i,j)$ via $k = i + j N_x$. For interior points, the discretization should recover the usual five-point stencil of the elliptic operator. For boundary points, enforce zero normal flux by removing the missing face contribution, which modifies the diagonal and the single available neighbor in that direction.\n2. For each assembled matrix, compute the following diagnostics:\n   - The total number of nonzero entries, denoted $\\mathrm{nnz}$.\n   - The smallest algebraic eigenvalue, denoted $\\lambda_{\\min}$, computed from the dense representation of the matrix using standard symmetric eigenvalue routines.\n   - A boolean determining whether the matrix is Symmetric Positive Definite (SPD), defined here as the condition $\\lambda_{\\min} > 10^{-12}$.\n   - The sorted list of distinct row nonzero counts across the matrix, which characterizes the sparsity pattern differences between interior, edge, and corner nodes.\n\nDesign a program that implements this algorithm and computes the diagnostics for the following test suite of parameter values $(N_x,N_y,h_x,h_y,a_x,a_y,\\mu)$:\n- Test case A (happy path, isotropic, singular): $(3,3,1.0,1.0,1.0,1.0,0.0)$.\n- Test case B (happy path, isotropic, SPD via zero-order term): $(3,3,1.0,1.0,1.0,1.0,0.001)$.\n- Test case C (anisotropic and rectangular grid): $(4,3,1.0,2.0,1.0,10.0,0.1)$.\n- Test case D (degenerate one-dimensional limit in $x$): $(1,5,1.0,1.0,1.0,1.0,0.0)$.\n\nFor each test case, your program must produce a result encoded as the list $[\\mathrm{nnz}, \\mathrm{round}(\\lambda_{\\min}, 8), \\mathrm{SPD}, \\mathrm{row\\_counts}]$, where $\\mathrm{round}(\\lambda_{\\min}, 8)$ is the smallest algebraic eigenvalue rounded to eight decimal places, $\\mathrm{SPD}$ is a boolean, and $\\mathrm{row\\_counts}$ is the sorted list of distinct row nonzero counts. Your program should produce a single line of output containing the results aggregated over all test cases as a comma-separated list of these lists enclosed in square brackets, with no spaces, for example, `[[\\dots],[\\dots],[\\dots],[\\dots]]`. No physical units are involved; all quantities are dimensionless.",
            "solution": "The user wants to construct a sparse precision matrix $B^{-1}$ for a Gaussian prior, where the matrix corresponds to a discretized second-order elliptic operator $\\mathcal{L} u = - \\nabla \\cdot (A \\nabla u) + \\mu u$ on a two-dimensional rectangular grid. The discretization must be second-order accurate and handle homogeneous Neumann boundary conditions by enforcing zero flux.\n\nThe implementation below follows a finite difference/finite volume approach on the nodal grid. For each node $(i,j)$, we construct a row of the matrix corresponding to the discretized operator. The stencil coefficients for the neighboring nodes become off-diagonal entries, and the central node's coefficient becomes the diagonal entry. Zero-flux Neumann boundary conditions are handled by not including contributions from neighbors outside the grid, which naturally modifies the stencil at edges and corners and ensures the matrix is symmetric. The resulting sparse matrix is assembled and then diagnosed as per the problem's requirements.\n\n```python\nimport numpy as np\nfrom scipy import sparse\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It iterates through test cases, constructs the precision matrix for each,\n    computes diagnostics, and prints the aggregated results.\n    \"\"\"\n    test_cases = [\n        # (Nx, Ny, hx, hy, ax, ay, mu)\n        (3, 3, 1.0, 1.0, 1.0, 1.0, 0.0),      # Test case A\n        (3, 3, 1.0, 1.0, 1.0, 1.0, 0.001),   # Test case B\n        (4, 3, 1.0, 2.0, 1.0, 10.0, 0.1),    # Test case C\n        (1, 5, 1.0, 1.0, 1.0, 1.0, 0.0),      # Test case D\n    ]\n\n    results = []\n    for params in test_cases:\n        result = build_and_diagnose(params)\n        results.append(str(result).replace(\" \", \"\"))\n\n    # The final print statement must follow the exact format.\n    print(f\"[{','.join(results)}]\")\n\ndef build_and_diagnose(params):\n    \"\"\"\n    Constructs the sparse precision matrix and computes its diagnostics.\n\n    Args:\n        params (tuple): A tuple containing the model parameters\n                        (Nx, Ny, hx, hy, ax, ay, mu).\n\n    Returns:\n        list: A list containing the diagnostics [nnz, lambda_min, SPD, row_counts].\n    \"\"\"\n    Nx, Ny, hx, hy, ax, ay, mu = params\n    N = Nx * Ny\n\n    if N == 0:\n        return [0, 0.0, False, []]\n\n    # Use lil_matrix for efficient sparse matrix construction.\n    B_inv = sparse.lil_matrix((N, N), dtype=np.float64)\n\n    cx = ax / (hx**2)\n    cy = ay / (hy**2)\n\n    # Iterate through each node in the grid.\n    for k in range(N):\n        # Convert linear index k to 2D grid indices (i, j).\n        i = k % Nx\n        j = k // Nx\n\n        diag_val = mu\n\n        # West neighbor\n        if i > 0:\n            kW = k - 1\n            diag_val += cx\n            B_inv[k, kW] = -cx\n\n        # East neighbor\n        if i  Nx - 1:\n            kE = k + 1\n            diag_val += cx\n            B_inv[k, kE] = -cx\n\n        # South neighbor\n        if j > 0:\n            kS = k - Nx\n            diag_val += cy\n            B_inv[k, kS] = -cy\n\n        # North neighbor\n        if j  Ny - 1:\n            kN = k + Nx\n            diag_val += cy\n            B_inv[k, kN] = -cy\n\n        B_inv[k, k] = diag_val\n\n    # Convert to CSR format for efficient row operations and nnz counting.\n    B_inv_csr = B_inv.tocsr()\n\n    # 1. Total number of nonzero entries.\n    nnz = B_inv_csr.nnz\n\n    # 2. Smallest algebraic eigenvalue.\n    # The matrix is symmetric by construction.\n    B_inv_dense = B_inv_csr.toarray()\n    # np.linalg.eigh is for symmetric matrices and returns sorted eigenvalues.\n    eigenvalues = np.linalg.eigh(B_inv_dense)[0]\n    lambda_min = eigenvalues[0]\n\n    # 3. Symmetric Positive Definite (SPD) check.\n    is_spd = bool(lambda_min > 1e-12)\n\n    # 4. Sorted list of distinct row nonzero counts.\n    row_nz_counts = B_inv_csr.getnnz(axis=1)\n    distinct_row_counts = sorted(np.unique(row_nz_counts).tolist())\n\n    return [nnz, round(lambda_min, 8), is_spd, distinct_row_counts]\n\nif __name__ == \"__main__\":\n    solve()\n```",
            "answer": "[[33,0.0,False,[3,4,5]],[33,0.001,True,[3,4,5]],[46,0.1,True,[3,4,5]],[13,0.0,False,[2,3]]]"
        },
        {
            "introduction": "A theoretical covariance model must be validated and tuned against reality. This practice introduces the Desroziers diagnostics, a powerful a posteriori technique for estimating the true error statistics from the outputs of the data assimilation system itself. By using sample statistics of innovations and analysis residuals from a hypothetical experiment, you will solve a least-squares problem to find the optimal scaling factors for the background and observation error covariance matrices . This exercise demonstrates a core principle of operational data assimilation: using the data to diagnose and improve the statistical assumptions of the system.",
            "id": "3366745",
            "problem": "Consider a linear, Gaussian data assimilation setting with a linear observation operator $H \\in \\mathbb{R}^{p \\times n}$ and background and observation error covariance matrices $\\boldsymbol{B} \\in \\mathbb{R}^{n \\times n}$ and $\\boldsymbol{R} \\in \\mathbb{R}^{p \\times p}$, respectively. The innovation is defined as $\\mathbf{v} = \\mathbf{y} - H \\mathbf{x}^{b}$ and the analysis residual is defined as $\\mathbf{w} = \\mathbf{y} - H \\mathbf{x}^{a}$. In the framework of Desroziers diagnostics, the following well-tested relations are used to link second-order statistics of innovations and analysis residuals to the error covariances: the second-moment of innovations approximates $H \\boldsymbol{B} H^{\\top} + \\boldsymbol{R}$, and the cross-second-moment of analysis residuals with innovations approximates $\\boldsymbol{R}$. Suppose the actual error covariances are modeled as scalar multiples of given templates, namely $\\boldsymbol{B} = \\alpha \\boldsymbol{B}_{0}$ and $\\boldsymbol{R} = \\beta \\boldsymbol{R}_{0}$ for unknown scalars $\\alpha, \\beta \\in \\mathbb{R}$ and known symmetric positive-definite templates $\\boldsymbol{B}_{0}$ and $\\boldsymbol{R}_{0}$.\n\nLet the observation dimension be $p = 2$ and the observation operator be $H = \\boldsymbol{I}_{2}$, the $2 \\times 2$ identity matrix. Let the covariance templates be\n$$\n\\boldsymbol{B}_{0} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}, \\qquad \\boldsymbol{R}_{0} = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}.\n$$\nYou are given $m = 4$ paired samples of innovations and analysis residuals, $\\{(\\mathbf{v}_{i}, \\mathbf{w}_{i})\\}_{i=1}^{4}$, with\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{v}_{2} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{v}_{3} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{v}_{4} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n$$\n$$\n\\mathbf{w}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{w}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{w}_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad \\mathbf{w}_{4} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nForm the sample second-moment matrices\n$$\n\\boldsymbol{S}_{vv} = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{v}_{i} \\mathbf{v}_{i}^{\\top}, \\qquad \\boldsymbol{S}_{wv} = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{w}_{i} \\mathbf{v}_{i}^{\\top}.\n$$\nUsing the Desroziers relations as the modeling base, determine the scalars $\\alpha$ and $\\beta$ that best satisfy both relations in a least-squares sense, by minimizing the sum of squared Frobenius-norm residuals\n$$\nJ(\\alpha, \\beta) = \\left\\| \\boldsymbol{S}_{vv} - H (\\alpha \\boldsymbol{B}_{0}) H^{\\top} - \\beta \\boldsymbol{R}_{0} \\right\\|_{F}^{2} + \\left\\| \\boldsymbol{S}_{wv} - \\beta \\boldsymbol{R}_{0} \\right\\|_{F}^{2},\n$$\nwhere $\\| \\boldsymbol{X} \\|_{F}^{2} = \\operatorname{tr}(\\boldsymbol{X}^{\\top} \\boldsymbol{X})$. Compute the exact values of $\\alpha$ and $\\beta$ that minimize $J(\\alpha, \\beta)$, and present your final answer as a $1 \\times 2$ row matrix containing $\\alpha$ and $\\beta$, respectively. No rounding is required, and no units are involved.",
            "solution": "We begin from the linear, Gaussian data assimilation setting and the Desroziers diagnostics, which provide well-tested relations between second-order statistics of innovations and analysis residuals and the error covariances. Specifically, under correct specification and linear assumptions, the second-moment of innovations satisfies $\\mathbb{E}[\\mathbf{v} \\mathbf{v}^{\\top}] \\approx H \\boldsymbol{B} H^{\\top} + \\boldsymbol{R}$, and the cross-second-moment between analysis residuals and innovations satisfies $\\mathbb{E}[\\mathbf{w} \\mathbf{v}^{\\top}] \\approx \\boldsymbol{R}$. When modeling $\\boldsymbol{B} = \\alpha \\boldsymbol{B}_{0}$ and $\\boldsymbol{R} = \\beta \\boldsymbol{R}_{0}$ with scalar multipliers applied to known templates $\\boldsymbol{B}_{0}$ and $\\boldsymbol{R}_{0}$, these relations suggest fitting\n$$\n\\boldsymbol{S}_{vv} \\approx H (\\alpha \\boldsymbol{B}_{0}) H^{\\top} + \\beta \\boldsymbol{R}_{0}, \\qquad \\boldsymbol{S}_{wv} \\approx \\beta \\boldsymbol{R}_{0},\n$$\nin a least-squares sense with the Frobenius norm, leading to the objective\n$$\nJ(\\alpha, \\beta) = \\left\\| \\boldsymbol{S}_{vv} - \\alpha \\boldsymbol{A} - \\beta \\boldsymbol{R}_{0} \\right\\|_{F}^{2} + \\left\\| \\boldsymbol{S}_{wv} - \\beta \\boldsymbol{R}_{0} \\right\\|_{F}^{2},\n$$\nwhere $\\boldsymbol{A} = H \\boldsymbol{B}_{0} H^{\\top}$. In this problem, $H = \\boldsymbol{I}_{2}$, so $\\boldsymbol{A} = \\boldsymbol{B}_{0}$.\n\nStep $1$: Compute the sample second-moment matrices $\\boldsymbol{S}_{vv}$ and $\\boldsymbol{S}_{wv}$ from the provided data.\n\nFor $\\boldsymbol{S}_{vv}$, compute each $\\mathbf{v}_{i} \\mathbf{v}_{i}^{\\top}$:\n$$\n\\mathbf{v}_{1} \\mathbf{v}_{1}^{\\top} = \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} \\mathbf{v}_{2}^{\\top} = \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix}, \\quad\n\\mathbf{v}_{3} \\mathbf{v}_{3}^{\\top} = \\begin{pmatrix} 4  4 \\\\ 4  4 \\end{pmatrix}, \\quad\n\\mathbf{v}_{4} \\mathbf{v}_{4}^{\\top} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nSumming and dividing by $m = 4$ gives\n$$\n\\boldsymbol{S}_{vv} = \\frac{1}{4} \\left( \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} + \\begin{pmatrix} 4  4 \\\\ 4  4 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} \\right)\n= \\frac{1}{4} \\begin{pmatrix} 8  4 \\\\ 4  8 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}.\n$$\nFor $\\boldsymbol{S}_{wv}$, compute each $\\mathbf{w}_{i} \\mathbf{v}_{i}^{\\top}$:\n$$\n\\mathbf{w}_{1} \\mathbf{v}_{1}^{\\top} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}, \\quad\n\\mathbf{w}_{2} \\mathbf{v}_{2}^{\\top} = \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix}, \\quad\n\\mathbf{w}_{3} \\mathbf{v}_{3}^{\\top} = \\begin{pmatrix} 2  2 \\\\ 2  2 \\end{pmatrix}, \\quad\n\\mathbf{w}_{4} \\mathbf{v}_{4}^{\\top} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nSumming and dividing by $m = 4$ gives\n$$\n\\boldsymbol{S}_{wv} = \\frac{1}{4} \\left( \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  2 \\end{pmatrix} + \\begin{pmatrix} 2  2 \\\\ 2  2 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  0 \\endpmatrix} \\right)\n= \\frac{1}{4} \\begin{pmatrix} 4  2 \\\\ 2  4 \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{1}{2} \\\\ \\frac{1}{2}  1 \\end{pmatrix}.\n$$\n\nStep $2$: Set $\\boldsymbol{A} = H \\boldsymbol{B}_{0} H^{\\top} = \\boldsymbol{B}_{0} = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}$ and $\\boldsymbol{R}_{0} = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$. Introduce the Frobenius inner product $\\langle \\boldsymbol{X}, \\boldsymbol{Y} \\rangle = \\operatorname{tr}(\\boldsymbol{X}^{\\top} \\boldsymbol{Y})$, so that $\\| \\boldsymbol{X} \\|_{F}^{2} = \\langle \\boldsymbol{X}, \\boldsymbol{X} \\rangle$. The objective can be expanded as\n$$\nJ(\\alpha, \\beta) = \\left\\langle \\boldsymbol{S}_{vv} - \\alpha \\boldsymbol{A} - \\beta \\boldsymbol{R}_{0},\\, \\boldsymbol{S}_{vv} - \\alpha \\boldsymbol{A} - \\beta \\boldsymbol{R}_{0} \\right\\rangle + \\left\\langle \\boldsymbol{S}_{wv} - \\beta \\boldsymbol{R}_{0},\\, \\boldsymbol{S}_{wv} - \\beta \\boldsymbol{R}_{0} \\right\\rangle.\n$$\nDifferentiating with respect to $\\alpha$ and $\\beta$ and setting the gradients to zero gives the normal equations for the least-squares minimizer:\n$$\n\\frac{\\partial J}{\\partial \\alpha} = -2 \\langle \\boldsymbol{S}_{vv}, \\boldsymbol{A} \\rangle + 2 \\alpha \\langle \\boldsymbol{A}, \\boldsymbol{A} \\rangle + 2 \\beta \\langle \\boldsymbol{A}, \\boldsymbol{R}_{0} \\rangle = 0,\n$$\n$$\n\\frac{\\partial J}{\\partial \\beta} = -2 \\langle \\boldsymbol{S}_{vv}, \\boldsymbol{R}_{0} \\rangle + 2 \\alpha \\langle \\boldsymbol{A}, \\boldsymbol{R}_{0} \\rangle + 2 \\beta \\langle \\boldsymbol{R}_{0}, \\boldsymbol{R}_{0} \\rangle - 2 \\langle \\boldsymbol{S}_{wv}, \\boldsymbol{R}_{0} \\rangle + 2 \\beta \\langle \\boldsymbol{R}_{0}, \\boldsymbol{R}_{0} \\rangle = 0.\n$$\nDividing both equations by $2$ yields the $2 \\times 2$ linear system\n$$\n\\alpha \\langle \\boldsymbol{A}, \\boldsymbol{A} \\rangle + \\beta \\langle \\boldsymbol{A}, \\boldsymbol{R}_{0} \\rangle = \\langle \\boldsymbol{S}_{vv}, \\boldsymbol{A} \\rangle,\n$$\n$$\n\\alpha \\langle \\boldsymbol{A}, \\boldsymbol{R}_{0} \\rangle + 2 \\beta \\langle \\boldsymbol{R}_{0}, \\boldsymbol{R}_{0} \\rangle = \\langle \\boldsymbolS_{vv}, \\boldsymbol{R}_{0} \\rangle + \\langle \\boldsymbol{S}_{wv}, \\boldsymbol{R}_{0} \\rangle.\n$$\n\nStep $3$: Compute the required inner products.\nFirst, compute $\\langle \\boldsymbol{A}, \\boldsymbol{A} \\rangle$:\n$$\n\\langle \\boldsymbol{A}, \\boldsymbol{A} \\rangle = 3^{2} + 1^{2} + 1^{2} + 2^{2} = 9 + 1 + 1 + 4 = 15.\n$$\nNext, compute $\\langle \\boldsymbol{A}, \\boldsymbol{R}_{0} \\rangle$:\n$$\n\\langle \\boldsymbol{A}, \\boldsymbol{R}_{0} \\rangle = 3 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 + 2 \\cdot 2 = 3 + 0 + 0 + 4 = 7.\n$$\nCompute $\\langle \\boldsymbol{S}_{vv}, \\boldsymbol{A} \\rangle$:\n$$\n\\langle \\boldsymbol{S}_{vv}, \\boldsymbol{A} \\rangle = 2 \\cdot 3 + 1 \\cdot 1 + 1 \\cdot 1 + 2 \\cdot 2 = 6 + 1 + 1 + 4 = 12.\n$$\nCompute $\\langle \\boldsymbol{R}_{0}, \\boldsymbol{R}_{0} \\rangle$:\n$$\n\\langle \\boldsymbol{R}_{0}, \\boldsymbol{R}_{0} \\rangle = 1^{2} + 0^{2} + 0^{2} + 2^{2} = 1 + 0 + 0 + 4 = 5.\n$$\nCompute $\\langle \\boldsymbol{S}_{vv}, \\boldsymbol{R}_{0} \\rangle$:\n$$\n\\langle \\boldsymbol{S}_{vv}, \\boldsymbol{R}_{0} \\rangle = 2 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 + 2 \\cdot 2 = 2 + 0 + 0 + 4 = 6.\n$$\nCompute $\\langle \\boldsymbol{S}_{wv}, \\boldsymbol{R}_{0} \\rangle$:\n$$\n\\langle \\boldsymbol{S}_{wv}, \\boldsymbol{R}_{0} \\rangle = 1 \\cdot 1 + \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 0 + 1 \\cdot 2 = 1 + 0 + 0 + 2 = 3.\n$$\n\nStep $4$: Solve the linear system\n$$\n15 \\alpha + 7 \\beta = 12, \\qquad 7 \\alpha + 10 \\beta = 9.\n$$\nSolve for $\\alpha$ and $\\beta$. Multiply the first equation by $10$ and the second equation by $7$:\n$$\n150 \\alpha + 70 \\beta = 120, \\qquad 49 \\alpha + 70 \\beta = 63.\n$$\nSubtract the second from the first:\n$$\n(150 - 49) \\alpha = 120 - 63 \\quad \\Rightarrow \\quad 101 \\alpha = 57 \\quad \\Rightarrow \\quad \\alpha = \\frac{57}{101}.\n$$\nSubstitute into $7 \\alpha + 10 \\beta = 9$:\n$$\n10 \\beta = 9 - 7 \\alpha = 9 - 7 \\cdot \\frac{57}{101} = \\frac{909 - 399}{101} = \\frac{510}{101},\n$$\nthus\n$$\n\\beta = \\frac{51}{101}.\n$$\n\nThese exact rational values minimize the specified least-squares objective and provide the scalar multipliers for $\\boldsymbol{B}$ and $\\boldsymbol{R}$ consistent with the Desroziers relations in the given setting.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{57}{101}  \\frac{51}{101}\\end{pmatrix}}$$"
        }
    ]
}