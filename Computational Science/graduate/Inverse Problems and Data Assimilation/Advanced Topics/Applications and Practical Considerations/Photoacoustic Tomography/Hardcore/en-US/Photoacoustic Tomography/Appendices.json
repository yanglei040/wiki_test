{
    "hands_on_practices": [
        {
            "introduction": "A reliable forward model is the cornerstone of any inverse problem. This practice guides you through the process of verifying a numerical solver for the acoustic wave equation using the method of manufactured solutions, a crucial skill for ensuring your simulations accurately reflect the underlying physics. By implementing a finite-difference scheme and demonstrating its theoretical convergence rate, you will build confidence in your computational tools .",
            "id": "3410161",
            "problem": "Consider the standard forward model for Photoacoustic Tomography (PAT) under the small-signal, lossless, homogeneous medium assumption. The acoustic pressure field satisfies the linear wave equation\n$$\n\\partial_{tt} p(x,t) - c^2 \\partial_{xx} p(x,t) = 0 \\quad \\text{for } x \\in [0,L], \\ t \\ge 0,\n$$\nwith periodic boundary conditions on the spatial interval, initial displacement $p(x,0) = p_0(x)$, and initial velocity $\\partial_t p(x,0) = v_0(x)$. Here $c$ is the (constant) speed of sound. In Photoacoustic Tomography, the initial pressure $p_0(x)$ is proportional to the absorbed optical energy density and $v_0(x)=0$ due to the nature of thermoelastic expansion following short-pulse illumination.\n\nYou will validate a numerical forward solver using the method of manufactured solutions. All quantities are nondimensional (unitless). The solver must be second-order accurate in both space and time. You must start from fundamental conservation laws and constitutive relations as the base for your derivations (mass conservation, momentum conservation, and a linear equation of state), not from pre-derived discretization formulas.\n\nManufactured solutions:\n\n1) Homogeneous wave equation: Choose a smooth periodic solution\n$$\np(x,t) = \\sin(k x)\\cos(c k t),\n$$\nwith $k = 2\\pi m$, where $m$ is a positive integer. This satisfies the homogeneous wave equation with initial data $p(x,0) = \\sin(k x)$ and $\\partial_t p(x,0)=0$.\n\n2) Forced wave equation: To isolate temporal convergence independently of spatial discretization error, use a spatially uniform manufactured solution\n$$\np(x,t) = \\cos(\\omega t),\n$$\nwhich satisfies\n$$\n\\partial_{tt} p(x,t) - c^2 \\partial_{xx} p(x,t) = S(x,t),\n$$\nwith manufactured source\n$$\nS(x,t) = -\\omega^2 \\cos(\\omega t).\n$$\nUse initial data $p(x,0)=\\cos(0)=1$ and $\\partial_t p(x,0)=0$.\n\nYour tasks:\n\n- Derive from the fundamental acoustic equations that the pressure field satisfies the stated wave equation with the given initial data for PAT.\n\n- Construct a consistent second-order accurate numerical method in one spatial dimension with periodic boundary conditions. The method must be explicit in time and second-order accurate in both space and time when applied to smooth data. Use a fixed spatial grid with $N$ points and a uniform time step $\\Delta t$. The final time $T$ must be exactly representable as an integer multiple of $\\Delta t$.\n\n- Implement the solver so that it can handle both the homogeneous equation and the forced equation with a general source term $S(x,t)$, and so that it supports the initial conditions stated above. The program must compute the solution at time $T$ and compare it to the exact manufactured solution to produce an error.\n\n- Use the discrete $L^2$ error defined by\n$$\nE = \\left( \\sum_{i=0}^{N-1} \\left[p_i^{\\mathrm{num}}(T) - p^{\\mathrm{exact}}(x_i,T)\\right]^2 \\Delta x \\right)^{1/2},\n$$\nwhere $\\Delta x = L/N$ and $x_i = i \\Delta x$.\n\n- Empirical convergence rates: Given two errors $E_1$ and $E_2$ corresponding to refinements by a factor of $r$ in the relevant step size (e.g., halving $\\Delta x$ or $\\Delta t$ so $r=2$), compute the observed order\n$$\nq = \\frac{\\log(E_1/E_2)}{\\log(r)}.\n$$\n\nTest suite and required output:\n\nImplement the following test suite, each test producing a float error and, where applicable, an empirical order. All quantities are nondimensional.\n\n- Test A (spatial convergence dominated): Homogeneous case with $c=1$, $L=1$, $m=3$, $k=2\\pi m$, final time $T=0.01$. Use three grids with $N \\in \\{50, 100, 200\\}$. For temporal stepping choose $\\Delta t = \\alpha (\\Delta x)^2$ with $\\alpha=0.2$. This ensures the temporal truncation error is negligible compared to the spatial truncation error at these resolutions. Compute the discrete $L^2$ errors $E_N$ for each $N$, and then compute the observed order using $q_{\\text{space}} = \\frac{1}{2}\\left( \\frac{\\log(E_{50}/E_{100})}{\\log 2} + \\frac{\\log(E_{100}/E_{200})}{\\log 2} \\right)$.\n\n- Test B (temporal convergence only via manufactured source): Forced case with $c=1$, $L=1$, uniform in space manufactured solution $p(x,t)=\\cos(\\omega t)$, $\\omega=7$. Use a fixed spatial grid $N=64$ (so that the spatial Laplacian of a constant field is exactly zero on the periodic grid, eliminating spatial error). Final time $T=0.2$. Use three time steps $\\Delta t \\in \\{0.02, 0.01, 0.005\\}$, each chosen so that $T$ is an integer multiple of $\\Delta t$. Compute the discrete $L^2$ errors $E_{\\Delta t}$ and the observed temporal order\n$$\nq_{\\text{time}} = \\frac{1}{2}\\left( \\frac{\\log(E_{0.02}/E_{0.01})}{\\log 2} + \\frac{\\log(E_{0.01}/E_{0.005})}{\\log 2} \\right).\n$$\n\n- Test C (edge case consistency): Homogeneous case with the same parameters as Test A but with $T=0$. The numerical solution must equal the initial condition, so the discrete $L^2$ error must be numerically zero. Report whether the error is less than a tolerance $\\varepsilon=10^{-12}$ as a boolean.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n- The observed spatial convergence order $q_{\\text{space}}$ from Test A as a float.\n- The observed temporal convergence order $q_{\\text{time}}$ from Test B as a float.\n- A boolean indicating whether $q_{\\text{space}} \\ge 1.8$.\n- A boolean indicating whether $q_{\\text{time}} \\ge 1.8$.\n- A boolean from Test C indicating whether the $T=0$ error is less than $\\varepsilon$.\n\nFor example, the output format must be exactly like\n\"[2.01,1.99,True,True,True]\"\n\nAll computed quantities are nondimensional. Angles, where they appear in trigonometric functions, are in radians. The final answers for the observed orders must be printed as plain decimal floats (not percentages), and booleans must be Python-style True or False.",
            "solution": "The problem is valid. It presents a well-posed, scientifically grounded task in computational physics: to derive, implement, and validate a numerical solver for the one-dimensional acoustic wave equation as a forward model for Photoacoustic Tomography (PAT). The use of the method of manufactured solutions for verification is a standard and rigorous approach in scientific computing. All parameters and test cases are clearly defined and physically and numerically consistent.\n\n### 1. Derivation of the Governing Equation\n\nThe behavior of sound waves in a fluid is described by the fundamental principles of mass and momentum conservation, along with an equation of state relating pressure and density. For a lossless, homogeneous medium under the small-signal assumption, these are expressed as the linearized acoustic equations. In one spatial dimension, $x$, they are:\n\n1.  **Conservation of Mass (Continuity Equation):** This principle states that the rate of change of mass in a volume is equal to the net flux of mass into it. For small perturbations, this linearizes to:\n    $$ \\frac{\\partial \\rho'}{\\partial t} + \\rho_a \\frac{\\partial v}{\\partial x} = 0 $$\n    Here, $\\rho'(x,t)$ is the acoustic density perturbation, $v(x,t)$ is the fluid particle velocity, and $\\rho_a$ is the ambient fluid density.\n\n2.  **Conservation of Momentum (Euler's Equation):** This is Newton's second law applied to a fluid element. The net force due to a pressure gradient accelerates the fluid. The linearized form is:\n    $$ \\rho_a \\frac{\\partial v}{\\partial t} = -\\frac{\\partial p}{\\partial x} $$\n    Here, $p(x,t)$ is the acoustic pressure.\n\n3.  **Equation of State:** For a rapid acoustic process (which is approximately adiabatic), the pressure and density are related. For small perturbations, this relationship is linear:\n    $$ p = c^2 \\rho' $$\n    The constant of proportionality, $c^2$, is defined by the medium's properties, where $c$ is the speed of sound, given by $c = \\sqrt{(\\partial P / \\partial \\rho)_S}$ evaluated at ambient conditions.\n\nTo derive the wave equation for pressure $p$, we combine these three equations. First, we differentiate the momentum equation with respect to $x$ and the continuity equation with respect to $t$:\n$$ \\rho_a \\frac{\\partial^2 v}{\\partial t \\partial x} = -\\frac{\\partial^2 p}{\\partial x^2} $$\n$$ \\frac{\\partial^2 \\rho'}{\\partial t^2} + \\rho_a \\frac{\\partial^2 v}{\\partial x \\partial t} = 0 $$\nAssuming sufficient smoothness, the order of differentiation does not matter ($\\partial_{tx}v = \\partial_{xt}v$). We can substitute the expression for $\\rho_a \\partial_{tx}v$ from the first equation into the second:\n$$ \\frac{\\partial^2 \\rho'}{\\partial t^2} + \\left( -\\frac{\\partial^2 p}{\\partial x^2} \\right) = 0 \\implies \\frac{\\partial^2 \\rho'}{\\partial t^2} = \\frac{\\partial^2 p}{\\partial x^2} $$\nNow, using the equation of state, we replace $\\rho'$ with $p/c^2$. Since $c$ is a constant, we can write $\\partial_{tt}\\rho' = (1/c^2) \\partial_{tt}p$. Substituting this yields:\n$$ \\frac{1}{c^2} \\frac{\\partial^2 p}{\\partial t^2} = \\frac{\\partial^2 p}{\\partial x^2} $$\nRearranging gives the standard homogeneous linear wave equation:\n$$ \\frac{\\partial^2 p}{\\partial t^2} - c^2 \\frac{\\partial^2 p}{\\partial x^2} = 0 $$\nIn Photoacoustic Tomography, tissue is illuminated by a short laser pulse. The absorbed energy causes rapid, localized heating and thermoelastic expansion, which creates an initial pressure distribution $p(x,0) = p_0(x)$. Because this process is assumed to occur before any significant material motion, the initial fluid velocity is zero, i.e., $v(x,0)=0$. From the continuity equation and equation of state, we have $\\partial_t p = c^2 \\partial_t \\rho' = -c^2 \\rho_a \\partial_x v$. Evaluating at $t=0$, we get $\\partial_t p(x,0) = -c^2 \\rho_a \\partial_x v(x,0)$. Since $v(x,0)=0$ for all $x$, its spatial derivative is also zero, which leads to the initial condition $\\partial_t p(x,0) = 0$.\n\n### 2. Numerical Discretization\n\nWe are tasked with constructing a second-order accurate, explicit finite difference scheme for the wave equation, possibly including a source term $S(x,t)$:\n$$ \\partial_{tt} p - c^2 \\partial_{xx} p = S(x,t) $$\nWe define a discrete grid with points $(x_i, t_n)$ where $x_i = i \\Delta x$ for $i=0, \\dots, N-1$ and $t_n = n \\Delta t$. The numerical solution at these points is denoted $p_i^n \\approx p(x_i, t_n)$.\n\nA second-order accurate approximation for the spatial and temporal derivatives is achieved using central differences:\n$$ \\partial_{xx} p \\bigg|_{(x_i, t_n)} \\approx \\frac{p_{i+1}^n - 2p_i^n + p_{i-1}^n}{(\\Delta x)^2} + \\mathcal{O}((\\Delta x)^2) $$\n$$ \\partial_{tt} p \\bigg|_{(x_i, t_n)} \\approx \\frac{p_i^{n+1} - 2p_i^n + p_i^{n-1}}{(\\Delta t)^2} + \\mathcal{O}((\\Delta t)^2) $$\nSubstituting these into the wave equation gives the discretized form:\n$$ \\frac{p_i^{n+1} - 2p_i^n + p_i^{n-1}}{(\\Delta t)^2} - c^2 \\frac{p_{i+1}^n - 2p_i^n + p_{i-1}^n}{(\\Delta x)^2} = S_i^n $$\nSolving for the pressure at the next time step, $p_i^{n+1}$, yields the explicit time-marching formula:\n$$ p_i^{n+1} = 2p_i^n - p_i^{n-1} + \\left(\\frac{c\\Delta t}{\\Delta x}\\right)^2 (p_{i+1}^n - 2p_i^n + p_{i-1}^n) + (\\Delta t)^2 S_i^n $$\nThis is a three-level scheme, requiring data from two previous time levels, $t_n$ and $t_{n-1}$. For numerical stability, the Courant-Friedrichs-Lewy (CFL) condition must be met: $\\sigma = c\\Delta t/\\Delta x \\le 1$.\n\nTo start the simulation, we need $p^0$ and $p^1$. The initial condition $p(x,0)=p_0(x)$ gives $p_i^0 = p_0(x_i)$. To find $p_i^1$ while maintaining second-order accuracy, we use the initial condition $\\partial_t p(x,0) = 0$. A second-order central difference for this derivative at $t=0$ is $(\\frac{p_i^1 - p_i^{-1}}{2\\Delta t}) = 0$, implying $p_i^1 = p_i^{-1}$. Substituting this into the general scheme for $n=0$:\n$$ p_i^1 = 2p_i^0 - p_i^{-1} + \\sigma^2 (p_{i+1}^0 - 2p_i^0 + p_{i-1}^0) + (\\Delta t)^2 S_i^0 $$\n$$ p_i^1 = 2p_i^0 - p_i^1 + \\sigma^2 (p_{i+1}^0 - 2p_i^0 + p_{i-1}^0) + (\\Delta t)^2 S_i^0 $$\nSolving for $p_i^1$ gives the second-order accurate starter formula:\n$$ p_i^1 = p_i^0 + \\frac{1}{2} \\sigma^2 (p_{i+1}^0 - 2p_i^0 + p_{i-1}^0) + \\frac{1}{2}(\\Delta t)^2 S_i^0 $$\nPeriodic boundary conditions on $[0,L]$ imply $p(x,t) = p(x+L,t)$, which on the discrete grid means $p_N^n = p_0^n$ and $p_{-1}^n = p_{N-1}^n$. These are handled using modulo arithmetic on indices.\n\n### 3. Verification Strategy\n\nThe numerical solver is validated using the method of manufactured solutions and a test suite.\n- **Test A (Spatial Convergence):** A homogeneous case with a smooth sinusoidal initial condition is used. The time step is chosen as $\\Delta t = \\alpha (\\Delta x)^2$ with a small $\\alpha=0.2$. This makes the temporal truncation error, which scales as $(\\Delta t)^2$, much smaller than the spatial error, which scales as $(\\Delta x)^2$. This test effectively isolates and measures the spatial order of accuracy. For our scheme, we expect the error $E$ to scale as $(\\Delta x)^2$, so the observed order $q_{\\text{space}}$ should be close to $2$.\n- **Test B (Temporal Convergence):** A forced case with a spatially uniform manufactured solution $p(x,t) = \\cos(\\omega t)$ is used. Since the solution and source term are constant in space, the discrete spatial Laplacian term $(p_{i+1}^n - 2p_i^n + p_{i-1}^n)$ is identically zero for all timesteps. This eliminates spatial discretization error entirely, isolating the temporal error. It also sidesteps the CFL stability constraint, as the part of the scheme responsible for the instability is nullified. We expect the error $E$ to scale as $(\\Delta t)^2$, so the observed order $q_{\\text{time}}$ should be close to $2$.\n- **Test C (Consistency Check):** This tests the solver at $T=0$. The numerical solution must simply be the initial condition, and the error should be zero (or within machine precision). This is a basic sanity check of the implementation.\n\nThe discrete $L^2$ error norm $E = \\left( \\sum_{i=0}^{N-1} \\left[p_i^{\\mathrm{num}}(T) - p^{\\mathrm{exact}}(x_i,T)\\right]^2 \\Delta x \\right)^{1/2}$ is used to quantify the difference between numerical and exact solutions, and from this, the empirical order of convergence $q = \\log(E_1/E_2)/\\log(r)$ is computed for a refinement factor $r$.",
            "answer": "```python\nimport numpy as np\n\ndef wave_solver(L, N, c, T, dt, p0_func, S_func=None):\n    \"\"\"\n    Solves the 1D wave equation partial_tt p - c^2 partial_xx p = S(x,t)\n    using a second-order finite difference scheme with periodic boundary conditions.\n    \"\"\"\n    dx = L / N\n    x = np.arange(N) * dx\n\n    num_steps = int(round(T / dt))\n\n    if num_steps == 0:\n        return p0_func(x), x\n\n    # Initialize pressure fields at t=0 and t=-dt (by convention)\n    # p_curr is p^n, p_prev is p^(n-1)\n    p_curr = p0_func(x)  # p^0\n\n    # Courant number squared\n    sigma_sq = (c * dt / dx)**2\n\n    # First time step (n=0) to calculate p^1 using second-order starter.\n    # p^1 = p^0 + dt*v_0 + 0.5*dt^2*(c^2*laplacian(p^0) + S^0). Here v_0=0.\n    lap_p0 = np.roll(p_curr, -1) - 2 * p_curr + np.roll(p_curr, 1)\n    \n    S_at_0 = np.zeros(N)\n    if S_func:\n        S_at_0 = S_func(x, 0.0)\n    \n    p_next = p_curr + 0.5 * sigma_sq * lap_p0 + 0.5 * dt**2 * S_at_0\n\n    p_prev = p_curr\n    p_curr = p_next\n\n    # Main time-stepping loop (n=1 to num_steps-1)\n    for n in range(1, num_steps):\n        t = n * dt\n        \n        lap_p_curr = np.roll(p_curr, -1) - 2 * p_curr + np.roll(p_curr, 1)\n\n        S_at_t = np.zeros(N)\n        if S_func:\n            S_at_t = S_func(x, t)\n\n        p_next = 2 * p_curr - p_prev + sigma_sq * lap_p_curr + dt**2 * S_at_t\n\n        p_prev = p_curr\n        p_curr = p_next\n\n    return p_curr, x\n\ndef calculate_l2_error(p_num, p_exact, dx):\n    \"\"\"Calculates the discrete L2 error.\"\"\"\n    return np.sqrt(np.sum((p_num - p_exact)**2) * dx)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    results = []\n\n    # --- Test A: Spatial Convergence ---\n    c_A = 1.0\n    L_A = 1.0\n    m_A = 3\n    k_A = 2 * np.pi * m_A\n    T_A = 0.01\n    alpha_A = 0.2\n    Ns_A = [50, 100, 200]\n    errors_A = []\n\n    def p0_A(x):\n        return np.sin(k_A * x)\n\n    for N_val in Ns_A:\n        dx_val = L_A / N_val\n        dt_val = alpha_A * (dx_val)**2\n        \n        p_num, x_grid = wave_solver(L_A, N_val, c_A, T_A, dt_val, p0_A)\n        p_exact = np.sin(k_A * x_grid) * np.cos(c_A * k_A * T_A)\n        \n        error = calculate_l2_error(p_num, p_exact, dx_val)\n        errors_A.append(error)\n\n    # Calculate observed order of convergence\n    q_50_100 = np.log(errors_A[0] / errors_A[1]) / np.log(2)\n    q_100_200 = np.log(errors_A[1] / errors_A[2]) / np.log(2)\n    q_space = 0.5 * (q_50_100 + q_100_200)\n    \n    results.append(q_space)\n\n    # --- Test B: Temporal Convergence ---\n    c_B = 1.0\n    L_B = 1.0\n    N_B = 64\n    omega_B = 7.0\n    T_B = 0.2\n    dts_B = [0.02, 0.01, 0.005]\n    errors_B = []\n\n    def p0_B(x):\n        return np.ones_like(x)\n\n    def S_B(x, t):\n        # x is unused since source is spatially uniform\n        return -omega_B**2 * np.cos(omega_B * t) * np.ones_like(x)\n\n    for dt_val in dts_B:\n        p_num, x_grid = wave_solver(L_B, N_B, c_B, T_B, dt_val, p0_B, S_func=S_B)\n        p_exact = np.cos(omega_B * T_B) * np.ones_like(x_grid)\n        dx_val = L_B / N_B\n        \n        error = calculate_l2_error(p_num, p_exact, dx_val)\n        errors_B.append(error)\n\n    # Calculate observed order of convergence\n    q_1 = np.log(errors_B[0] / errors_B[1]) / np.log(2)\n    q_2 = np.log(errors_B[1] / errors_B[2]) / np.log(2)\n    q_time = 0.5 * (q_1 + q_2)\n\n    results.append(q_time)\n\n    # --- Append boolean checks for Test A and B ---\n    results.append(q_space >= 1.8)\n    results.append(q_time >= 1.8)\n\n    # --- Test C: Edge Case Consistency (T=0) ---\n    N_C = 50 \n    dx_C = L_A / N_C\n    dt_C = alpha_A * dx_C**2\n    \n    p_num_C, x_grid_C = wave_solver(L_A, N_C, c_A, 0.0, dt_C, p0_A)\n    p_exact_C = p0_A(x_grid_C)\n\n    error_C = calculate_l2_error(p_num_C, p_exact_C, dx_C)\n    \n    results.append(error_C  1e-12)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Bridging theory and experiment requires understanding how physical properties constrain measurement design. This exercise explores the relationship between an object's smallest feature size $\\ell$ and the necessary temporal sampling interval $\\Delta t$ for the resulting photoacoustic signals. By applying the acoustic dispersion relation and the Nyquist-Shannon theorem, you will derive a fundamental sampling criterion essential for designing any real-world PAT data acquisition system .",
            "id": "3410220",
            "problem": "Consider photoacoustic tomography (PAT), where an initial pressure distribution $p_{0}(\\mathbf{x})$ is induced by an optical pulse and subsequently propagates as an acoustic field $p(\\mathbf{x}, t)$ in a homogeneous, isotropic, and lossless medium of constant sound speed $c$. The acoustic propagation is governed by the scalar wave equation with initial conditions $p(\\mathbf{x}, 0) = p_{0}(\\mathbf{x})$ and $\\partial_{t} p(\\mathbf{x}, 0) = 0$. A detector located at position $\\mathbf{y}$ on the measurement surface records the time series $p(\\mathbf{y}, t)$, which you intend to sample uniformly in time with sampling interval $\\Delta t$.\n\nStarting from fundamental principles, reason about the frequency content of $p(\\mathbf{y}, t)$ in relation to the spatial bandwidth of $p_{0}(\\mathbf{x})$. Specifically:\n\n- Use the dispersion relation for acoustic waves to connect the temporal frequency content of $p(\\mathbf{y}, t)$ to the spatial wavenumber content of $p_{0}(\\mathbf{x})$.\n- Relate the maximum significant temporal frequency $f_{\\max}$ in $p(\\mathbf{y}, t)$ to the smallest characteristic feature size $\\ell$ in $p_{0}(\\mathbf{x})$.\n- Using the Nyquist–Shannon sampling theorem, determine the minimal uniform temporal sampling interval $\\Delta t_{\\min}$ that avoids aliasing in $p(\\mathbf{y}, t)$.\n\nExpress your final answer for $\\Delta t_{\\min}$ as a closed-form analytic expression in terms of $c$ and $\\ell$. Express $\\Delta t_{\\min}$ in seconds. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the minimal uniform temporal sampling interval, $\\Delta t_{\\min}$, required to sample the photoacoustic signal $p(\\mathbf{y}, t)$ without aliasing, expressed in terms of the sound speed $c$ and the smallest characteristic feature size $\\ell$ of the initial pressure distribution $p_{0}(\\mathbf{x})$.\n\nThe problem will be solved in three steps as guided by the prompt:\n1.  Establish the relationship between temporal and spatial frequencies using the dispersion relation of the acoustic wave equation.\n2.  Relate the maximum temporal frequency $f_{\\max}$ in the measured signal to the smallest feature size $\\ell$ in the initial source.\n3.  Apply the Nyquist–Shannon sampling theorem to determine the required sampling interval.\n\nFirst, the propagation of the acoustic pressure field $p(\\mathbf{x}, t)$ in a homogeneous, isotropic, and lossless medium is governed by the scalar wave equation:\n$$ \\nabla^2 p(\\mathbf{x}, t) - \\frac{1}{c^2} \\frac{\\partial^2 p(\\mathbf{x}, t)}{\\partial t^2} = 0 $$\nwhere $c$ is the constant speed of sound. To analyze the frequency content, we consider a plane wave solution of the form $p(\\mathbf{x}, t) = A \\exp(i(\\mathbf{k} \\cdot \\mathbf{x} - \\omega t))$, where $\\mathbf{k}$ is the spatial wavenumber vector and $\\omega$ is the temporal angular frequency. Substituting this into the wave equation gives:\n$$ \\nabla^2 (A e^{i(\\mathbf{k} \\cdot \\mathbf{x} - \\omega t)}) = (i\\mathbf{k}) \\cdot (i\\mathbf{k}) (A e^{i(\\mathbf{k} \\cdot \\mathbf{x} - \\omega t)}) = -|\\mathbf{k}|^2 p(\\mathbf{x}, t) $$\n$$ \\frac{\\partial^2}{\\partial t^2} (A e^{i(\\mathbf{k} \\cdot \\mathbf{x} - \\omega t)}) = (-i\\omega)^2 (A e^{i(\\mathbf{k} \\cdot \\mathbf{x} - \\omega t)}) = -\\omega^2 p(\\mathbf{x}, t) $$\nThe wave equation thus becomes:\n$$ -|\\mathbf{k}|^2 p(\\mathbf{x}, t) - \\frac{1}{c^2} (-\\omega^2 p(\\mathbf{x}, t)) = 0 $$\nFor a non-trivial solution ($p(\\mathbf{x}, t) \\neq 0$), the coefficients must vanish:\n$$ -|\\mathbf{k}|^2 + \\frac{\\omega^2}{c^2} = 0 $$\nThis yields the dispersion relation for acoustic waves in this medium:\n$$ \\omega^2 = c^2 |\\mathbf{k}|^2 $$\nSince physical frequencies are non-negative, we take the positive root:\n$$ \\omega = c |\\mathbf{k}| $$\nThis fundamental relation directly links the temporal angular frequency $\\omega$ of a wave component to the magnitude of its spatial wavenumber $|\\mathbf{k}|$. This means that the temporal frequency content of the propagating wave field $p(\\mathbf{x}, t)$ is determined entirely by the spatial frequency (wavenumber) content of the initial pressure distribution $p_{0}(\\mathbf{x})$.\n\nSecond, we relate the maximum significant temporal frequency, $f_{\\max}$, in the measured signal $p(\\mathbf{y}, t)$ to the smallest characteristic feature size, $\\ell$, in the initial pressure distribution $p_{0}(\\mathbf{x})$. The smallest feature size $\\ell$ dictates the highest spatial frequency required for its representation. In signal processing and imaging, the spatial resolution, $\\ell$, is related to the maximum required spatial wavenumber, $k_{\\max} = |\\mathbf{k}|_{\\max}$. A standard convention, stemming from the spatial sampling theorem, defines the resolution limit as half the wavelength of the highest frequency component. The spatial wavelength $\\lambda_{\\text{sp}}$ is related to the wavenumber $k$ by $\\lambda_{\\text{sp}} = 2\\pi/k$. Therefore, the minimum wavelength $\\lambda_{\\text{sp, min}}$ corresponds to the maximum wavenumber $k_{\\max}$. The relation is:\n$$ \\ell = \\frac{\\lambda_{\\text{sp, min}}}{2} = \\frac{1}{2} \\frac{2\\pi}{k_{\\max}} = \\frac{\\pi}{k_{\\max}} $$\nFrom this, we can express the maximum significant spatial wavenumber in the initial pressure distribution $p_{0}(\\mathbf{x})$ as:\n$$ k_{\\max} = \\frac{\\pi}{\\ell} $$\nUsing the dispersion relation $\\omega = c |\\mathbf{k}|$, the maximum temporal angular frequency $\\omega_{\\max}$ in the propagating wave is determined by $k_{\\max}$:\n$$ \\omega_{\\max} = c k_{\\max} = c \\frac{\\pi}{\\ell} $$\nThe maximum temporal frequency $f_{\\max}$ is related to the angular frequency by $\\omega_{\\max} = 2\\pi f_{\\max}$. Thus:\n$$ 2\\pi f_{\\max} = c \\frac{\\pi}{\\ell} $$\n$$ f_{\\max} = \\frac{c}{2\\ell} $$\n\nThird, we apply the Nyquist–Shannon sampling theorem to find the required sampling interval. The theorem states that to avoid aliasing, a signal must be sampled at a frequency $f_s$ that is at least twice its maximum frequency component, $f_{\\max}$.\n$$ f_s \\ge 2 f_{\\max} $$\nThe sampling interval $\\Delta t$ is the reciprocal of the sampling frequency, $\\Delta t = 1/f_s$. Substituting this into the inequality gives the condition on the sampling interval:\n$$ \\frac{1}{\\Delta t} \\ge 2 f_{\\max} \\implies \\Delta t \\le \\frac{1}{2 f_{\\max}} $$\nThe problem asks for the \"minimal\" sampling interval $\\Delta t_{\\min}$. In the context of the Nyquist criterion, this is interpreted as the sampling interval corresponding to the minimal sampling rate that avoids aliasing, which is the Nyquist rate $f_s = 2f_{\\max}$. This corresponds to the maximum allowable sampling interval. Any interval smaller than this would also avoid aliasing, but would constitute oversampling. The limiting case for sufficient sampling is:\n$$ \\Delta t_{\\min} = \\frac{1}{2 f_{\\max}} $$\nSubstituting our expression for $f_{\\max}$:\n$$ \\Delta t_{\\min} = \\frac{1}{2 \\left( \\frac{c}{2\\ell} \\right)} = \\frac{1}{\\frac{c}{\\ell}} $$\nThis simplifies to the final expression for the minimal sampling interval:\n$$ \\Delta t_{\\min} = \\frac{\\ell}{c} $$\nThe units of this expression are (length) / (length/time) = time, which is correct for a sampling interval. Physically, this result states that the sampling interval must be no larger than the time it takes for sound to travel a distance equal to the smallest feature size in the object. This is an intuitive and fundamental requirement for resolving that feature.",
            "answer": "$$\\boxed{\\frac{\\ell}{c}}$$"
        },
        {
            "introduction": "The goal of quantitative photoacoustic tomography (QPAT) is not just to reconstruct an image, but also to quantify the uncertainty in the inferred parameters $\\boldsymbol{\\theta}$. This advanced practice immerses you in a full Bayesian inverse problem, from finding the maximum a posteriori (MAP) estimate to approximating the posterior uncertainty using the Laplace method. By computing the Gauss-Newton Hessian $\\mathbf{H}$ and its inverse, which approximates the posterior covariance $\\mathbf{C}$, you will gain hands-on experience with methods that provide confidence measures for your scientific conclusions .",
            "id": "3410206",
            "problem": "You are given a simplified one-dimensional model for Quantitative Photoacoustic Tomography (QPAT) that uses the steady-state diffusion approximation for light transport. The goal is to approximate the uncertainty of the inferred optical absorption field using the Laplace approximation at the Maximum A Posteriori (MAP) estimate by computing the Hessian of the negative log-posterior and relating its inverse to the approximate posterior covariance.\n\nConsider a one-dimensional spatial domain discretized into $n$ interior nodes with grid spacing $h = 1/(n+1)$ and homogeneous Dirichlet boundary conditions. The light fluence $\\Phi \\in \\mathbb{R}^n$ satisfies the discrete diffusion equation\n$$\n\\mathbf{A}(\\boldsymbol{\\theta}) \\Phi = q,\n$$\nwhere $q \\in \\mathbb{R}^n$ is a known source vector, $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ are the log-absorption parameters, and the absorption $\\boldsymbol{\\mu}_a \\in \\mathbb{R}^n$ is given by $\\boldsymbol{\\mu}_a = \\exp(\\boldsymbol{\\theta})$ element-wise. The stiffness matrix $\\mathbf{A}(\\boldsymbol{\\theta})$ is\n$$\n\\mathbf{A}(\\boldsymbol{\\theta}) = \\frac{D}{h^2}\\mathbf{K} + \\operatorname{diag}(\\boldsymbol{\\mu}_a),\n$$\nwhere $D \\in \\mathbb{R}_+$ is the diffusion coefficient and $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ is the standard tridiagonal finite-difference matrix with $2$ on the diagonal and $-1$ on the off-diagonals.\n\nThe initial acoustic pressure $p_0 \\in \\mathbb{R}^n$ is given by\n$$\np_0 = \\Gamma \\boldsymbol{\\mu}_a \\odot \\Phi,\n$$\nwhere $\\Gamma \\in \\mathbb{R}_+$ is the Grüneisen parameter (assumed known) and $\\odot$ denotes the element-wise (Hadamard) product. The measurement operator selects a subset of indices $\\mathcal{S} \\subset \\{1,\\dots,n\\}$, and the measured data are\n$$\n\\mathbf{g}(\\boldsymbol{\\theta}) = \\mathbf{S} p_0,\n$$\nwhere $\\mathbf{S} \\in \\{0,1\\}^{m \\times n}$ selects the $m$ sensor locations. The observed data $\\mathbf{y} \\in \\mathbb{R}^m$ are generated by the forward model at a ground-truth parameter $\\boldsymbol{\\theta}_\\text{true}$, without additional noise, and are treated as a realization of a Gaussian likelihood with covariance $\\sigma^2 \\mathbf{I}_m$.\n\nAssume an independent Gaussian prior for $\\boldsymbol{\\theta}$ with mean $\\boldsymbol{\\theta}_\\text{prior}$ and covariance $\\tau^2 \\mathbf{I}_n$. The negative log-posterior (up to an additive constant) is\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2\\sigma^2} \\|\\mathbf{g}(\\boldsymbol{\\theta}) - \\mathbf{y}\\|_2^2 + \\frac{1}{2\\tau^2} \\|\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prior}}\\|_2^2.\n$$\n\nThe Laplace approximation to the posterior near the MAP estimate $\\boldsymbol{\\theta}_\\text{MAP}$ uses a Gaussian with covariance equal to the inverse of the Hessian of $\\mathcal{L}$ at $\\boldsymbol{\\theta}_\\text{MAP}$. In practice, for nonlinear least squares with Gaussian noise, a Gauss–Newton approximation to the Hessian\n$$\n\\mathbf{H}(\\boldsymbol{\\theta}_\\text{MAP}) \\approx \\mathbf{J}(\\boldsymbol{\\theta}_\\text{MAP})^\\top \\left(\\frac{1}{\\sigma^2} \\mathbf{I}_m\\right) \\mathbf{J}(\\boldsymbol{\\theta}_\\text{MAP}) + \\frac{1}{\\tau^2} \\mathbf{I}_n\n$$\nis often used, where $\\mathbf{J}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{m \\times n}$ is the Jacobian of $\\mathbf{g}(\\boldsymbol{\\theta})$.\n\nTasks:\n1. Compute $\\boldsymbol{\\theta}_\\text{MAP}$ by minimizing $\\mathcal{L}(\\boldsymbol{\\theta})$ using a damped Gauss–Newton (Levenberg–Marquardt) method starting from $\\boldsymbol{\\theta}_\\text{prior}$.\n2. At $\\boldsymbol{\\theta}_\\text{MAP}$, compute the Gauss–Newton Hessian approximation $\\mathbf{H}(\\boldsymbol{\\theta}_\\text{MAP})$ as above.\n3. Compute the approximate posterior covariance $\\mathbf{C} \\approx \\mathbf{H}(\\boldsymbol{\\theta}_\\text{MAP})^{-1}$.\n4. Report the three selected marginal variances $\\{\\mathbf{C}_{i,i}\\}$ at indices $i \\in \\{1,\\lfloor n/2 \\rfloor, n\\}$ (using $1$-based indexing; in code, use $0$, $\\lfloor n/2 \\rfloor$, and $n-1$).\nAll quantities are dimensionless due to nondimensionalization; express the three reported outputs as floating-point numbers.\n\nThe Jacobian $\\mathbf{J}(\\boldsymbol{\\theta})$ must be computed by differentiating through the implicit definition of $\\Phi$ given by $\\mathbf{A}(\\boldsymbol{\\theta}) \\Phi = q$:\n- Use $\\boldsymbol{\\mu}_a = \\exp(\\boldsymbol{\\theta})$ element-wise,\n- Compute $\\Phi$ by solving the linear system,\n- For each component $\\theta_i$, compute $\\partial \\Phi/\\partial \\theta_i$ by solving\n$$\n\\mathbf{A}(\\boldsymbol{\\theta}) \\frac{\\partial \\Phi}{\\partial \\theta_i} = - \\frac{\\partial \\mathbf{A}}{\\partial \\theta_i} \\Phi,\n$$\nand combine with $\\partial p_0/\\partial \\theta_i$ to form the Jacobian columns, before selecting sensor rows via $\\mathbf{S}$.\n\nTest suite:\nUse the following three test cases to exercise different regimes of the inference problem. In each case, the observed data $\\mathbf{y}$ are the forward prediction at the ground-truth parameters without added noise. The ground-truth absorption profile is defined as\n$$\n\\mu_a^{\\text{true}}(x_j) = \\mu_0 + \\delta \\exp\\left(-\\frac{(x_j - 0.5)^2}{2 w^2}\\right),\n$$\nwith $x_j = j h$ for interior nodes $j=1,\\dots,n$, and $\\boldsymbol{\\theta}_{\\text{true}} = \\log( \\mu_a^{\\text{true}}(x_j) )$ element-wise, while the prior mean is the uniform background $\\boldsymbol{\\theta}_{\\text{prior}} = \\log(\\mu_0) \\mathbf{1}$.\n\n- Case A (general, fully observed):\n  - $n = 20$, $D = 10^{-2}$, $\\Gamma = 1$, $q = \\mathbf{1}$,\n  - $\\mu_0 = 0.1$, $\\delta = 0.2$, $w = 0.1$,\n  - sensors: all nodes ($\\mathbf{S} = \\mathbf{I}_{n}$), $\\sigma^2 = (10^{-2})^2$, $\\tau^2 = (0.5)^2$.\n\n- Case B (low noise, strong data):\n  - same as Case A except $\\sigma^2 = (10^{-4})^2$.\n\n- Case C (partial observations, weaker prior):\n  - same as Case A except sensors are every third node starting at index $3$ (i.e., indices $\\{3,6,9,\\dots\\}$ in $1$-based indexing), $\\sigma^2 = (10^{-2})^2$, $\\tau^2 = (1.0)^2$.\n\nYour program must:\n- Implement the forward model and Jacobian computation as specified,\n- Compute $\\boldsymbol{\\theta}_\\text{MAP}$ using a damped Gauss–Newton method,\n- Form $\\mathbf{H}(\\boldsymbol{\\theta}_\\text{MAP})$ and $\\mathbf{C} \\approx \\mathbf{H}^{-1}$,\n- For each test case, output the three marginal variances $\\mathbf{C}_{i,i}$ at indices $i \\in \\{1,\\lfloor n/2 \\rfloor, n\\}$ (using zero-based indices in code),\n- Produce a single line of output containing a list of lists, one per test case, each inner list containing the three floating-point numbers in the order $[i=0, i=\\lfloor n/2 \\rfloor, i=n-1]$, with the entire output formatted as a comma-separated list enclosed in square brackets, e.g., $[[v_{A1},v_{A2},v_{A3}],[v_{B1},v_{B2},v_{B3}],[v_{C1},v_{C2},v_{C3}]]$.\n\nAll quantities are dimensionless; no physical units are required. Angles are not involved. Percentages are not involved; express all values as plain floating-point numbers.",
            "solution": "**Problem Validation**\n\nThe problem statement has been critically evaluated and is deemed valid.\n\n1.  **Givens Extraction**: All required parameters, equations, and conditions are explicitly provided. This includes the model for light diffusion ($\\mathbf{A}(\\boldsymbol{\\theta})\\Phi = q$), acoustic pressure generation ($p_0 = \\Gamma \\boldsymbol{\\mu}_a \\odot \\Phi$), the Bayesian statistical framework (Gaussian likelihood and prior), the negative log-posterior $\\mathcal{L}(\\boldsymbol{\\theta})$, the Gauss-Newton approximation to the Hessian $\\mathbf{H}$, and the method for calculating the Jacobian $\\mathbf{J}$ via implicit differentiation. Three distinct test cases (A, B, C) with all necessary constants ($n, D, \\Gamma, q, \\mu_0, \\delta, w, \\sigma^2, \\tau^2$) are defined.\n\n2.  **Validation Checks**:\n    - **Scientific Grounding**: The model is based on the diffusion approximation to the radiative transfer equation and standard principles of photoacoustic signal generation. The Bayesian inference approach with a Gauss-Newton optimization and Laplace approximation for uncertainty are standard and well-established methods in computational inverse problems. The model is scientifically sound.\n    - **Well-Posedness**: The forward problem is well-posed (the matrix $\\mathbf{A}(\\boldsymbol{\\theta})$ is invertible). The inverse problem is regularized by the prior, leading to a well-posed optimization problem for the MAP estimate. The Gauss-Newton Hessian, being the sum of a positive semi-definite and a positive definite matrix, is invertible, ensuring a unique solution for the approximate posterior covariance.\n    - **Objectivity**: The problem is stated in precise mathematical and algorithmic terms, free from subjective language.\n    - **Completeness**: All information required to implement the solution and reproduce the results is provided. A minor ambiguity exists regarding the indices for reporting results, between the 1-based mathematical description ($\\{1, \\lfloor n/2 \\rfloor, n\\}$) and the explicit instruction for code implementation ($\\{0, \\lfloor n/2 \\rfloor, n-1\\}$). The latter, more direct instruction will be followed.\n\nThe problem is a well-defined and standard exercise in computational inverse problems, requiring the implementation of a forward model, an optimization algorithm, and an uncertainty quantification method.\n\n**Solution Methodology**\n\nThe goal is to compute the marginal posterior variances of the log-absorption parameter field $\\boldsymbol{\\theta}$ at selected spatial locations. This is achieved by first finding the maximum a posteriori (MAP) estimate $\\boldsymbol{\\theta}_{\\text{MAP}}$ and then computing the Laplace approximation to the posterior covariance, which is given by the inverse of the Hessian of the negative log-posterior evaluated at the MAP estimate. A Gauss-Newton approximation is used for the Hessian. The solution proceeds in the following steps.\n\n**1. Forward Model and Observation**\n\nThe physical process is described by a sequence of models.\n\n- **Light Transport**: The steady-state light fluence $\\Phi \\in \\mathbb{R}^n$ for a given log-absorption field $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ is governed by the discrete diffusion equation:\n$$\n\\mathbf{A}(\\boldsymbol{\\theta}) \\Phi = q\n$$\nwhere $\\boldsymbol{\\mu}_a = \\exp(\\boldsymbol{\\theta})$ (element-wise), $\\mathbf{A}(\\boldsymbol{\\theta}) = \\frac{D}{h^2}\\mathbf{K} + \\operatorname{diag}(\\boldsymbol{\\mu}_a)$, $q$ is the source, $D$ is the diffusion coefficient, $h=\\frac{1}{n+1}$ is the grid spacing, and $\\mathbf{K}$ is the $n \\times n$ second-order finite difference matrix (tridiagonal with $2$ on the main diagonal and $-1$ on the first off-diagonals). Since $\\mathbf{K}$ is positive semi-definite and $\\operatorname{diag}(\\boldsymbol{\\mu}_a)$ is a positive definite diagonal matrix (as $\\mu_{a,i} = e^{\\theta_i} > 0$), the matrix $\\mathbf{A}(\\boldsymbol{\\theta})$ is symmetric positive definite and thus invertible. We can compute $\\Phi$ by solving this linear system:\n$$\n\\Phi(\\boldsymbol{\\theta}) = \\mathbf{A}(\\boldsymbol{\\theta})^{-1} q\n$$\n- **Acoustic Generation**: The initial acoustic pressure $p_0 \\in \\mathbb{R}^n$ is proportional to the absorbed optical energy density:\n$$\np_0(\\boldsymbol{\\theta}) = \\Gamma (\\boldsymbol{\\mu}_a \\odot \\Phi(\\boldsymbol{\\theta}))\n$$\nwhere $\\Gamma$ is the Grüneisen parameter and $\\odot$ is the element-wise product.\n- **Measurement**: The observation model $\\mathbf{g}(\\boldsymbol{\\theta})$ maps the parameter space to the measurement space via a linear selection operator $\\mathbf{S} \\in \\{0,1\\}^{m \\times n}$:\n$$\n\\mathbf{g}(\\boldsymbol{\\theta}) = \\mathbf{S} \\, p_0(\\boldsymbol{\\theta})\n$$\nThe observed data $\\mathbf{y} \\in \\mathbb{R}^m$ are synthesized noise-free from a known ground truth $\\boldsymbol{\\theta}_{\\text{true}}$: $\\mathbf{y} = \\mathbf{g}(\\boldsymbol{\\theta}_{\\text{true}})$.\n\n**2. Bayesian Inference and MAP Estimation**\n\nWe adopt a Bayesian framework. The posterior probability distribution of $\\boldsymbol{\\theta}$ given data $\\mathbf{y}$ is proportional to the product of the likelihood and the prior. Assuming a Gaussian likelihood and a Gaussian prior, the negative log-posterior $\\mathcal{L}(\\boldsymbol{\\theta})$ (our objective function to be minimized) is:\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2\\sigma^2} \\|\\mathbf{g}(\\boldsymbol{\\theta}) - \\mathbf{y}\\|_2^2 + \\frac{1}{2\\tau^2} \\|\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prior}}\\|_2^2\n$$\nThe MAP estimate is the minimizer of this functional: $\\boldsymbol{\\theta}_{\\text{MAP}} = \\arg\\min_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta})$. This is a nonlinear least-squares problem. We solve it using a damped Gauss-Newton (Levenberg-Marquardt) iterative method, starting with the prior mean $\\boldsymbol{\\theta}_{\\text{prior}}$ as the initial guess.\n\nThe Gauss-Newton update step $\\delta\\boldsymbol{\\theta}$ is found by solving the linear system:\n$$\n(\\mathbf{H}(\\boldsymbol{\\theta}_k) + \\lambda \\mathbf{I}) \\delta\\boldsymbol{\\theta} = -\\nabla\\mathcal{L}(\\boldsymbol{\\theta}_k)\n$$\nwhere $\\boldsymbol{\\theta}_k$ is the estimate at iteration $k$, $\\lambda$ is a damping parameter, $\\nabla\\mathcal{L}$ is the gradient of the objective function, and $\\mathbf{H}$ is the Gauss-Newton approximation of the Hessian.\nThe gradient is:\n$$\n\\nabla\\mathcal{L}(\\boldsymbol{\\theta}) = \\mathbf{J}(\\boldsymbol{\\theta})^\\top \\frac{1}{\\sigma^2} (\\mathbf{g}(\\boldsymbol{\\theta}) - \\mathbf{y}) + \\frac{1}{\\tau^2} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{\\text{prior}})\n$$\nThe Gauss-Newton Hessian is:\n$$\n\\mathbf{H}(\\boldsymbol{\\theta}) = \\mathbf{J}(\\boldsymbol{\\theta})^\\top \\frac{1}{\\sigma^2} \\mathbf{I}_m \\mathbf{J}(\\boldsymbol{\\theta}) + \\frac{1}{\\tau^2} \\mathbf{I}_n\n$$\nwhere $\\mathbf{J}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{m\\times n}$ is the Jacobian of the forward model $\\mathbf{g}(\\boldsymbol{\\theta})$. The iterative process continues until the norm of the gradient falls below a tolerance.\n\n**3. Jacobian Calculation via Implicit Differentiation**\n\nThe core of the optimization is the computation of the Jacobian $\\mathbf{J}(\\boldsymbol{\\theta})$. Its elements are $J_{ij} = \\frac{\\partial g_i}{\\partial \\theta_j}$. We have $\\mathbf{g} = \\mathbf{S} p_0$, so $\\mathbf{J} = \\mathbf{S} \\frac{\\partial p_0}{\\partial \\boldsymbol{\\theta}}$, where $\\frac{\\partial p_0}{\\partial \\boldsymbol{\\theta}}$ is the full $n \\times n$ Jacobian of $p_0$. We compute the $j$-th column of this matrix, $\\frac{\\partial p_0}{\\partial \\theta_j}$.\nUsing the product rule on $p_0 = \\Gamma (\\boldsymbol{\\mu}_a \\odot \\Phi)$:\n$$\n\\frac{\\partial p_0}{\\partial \\theta_j} = \\Gamma \\left( \\frac{\\partial \\boldsymbol{\\mu}_a}{\\partial \\theta_j} \\odot \\Phi + \\boldsymbol{\\mu}_a \\odot \\frac{\\partial \\Phi}{\\partial \\theta_j} \\right)\n$$\nThe derivative of $\\boldsymbol{\\mu}_a$ is straightforward: $\\frac{\\partial \\boldsymbol{\\mu}_a}{\\partial \\theta_j} = \\mu_{a,j} \\mathbf{e}_j$, where $\\mathbf{e}_j$ is the $j$-th standard basis vector.\nThe derivative of $\\Phi$ requires implicit differentiation of the forward equation $\\mathbf{A} \\Phi = q$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\theta_j} \\Phi + \\mathbf{A} \\frac{\\partial \\Phi}{\\partial \\theta_j} = 0 \\implies \\mathbf{A} \\frac{\\partial \\Phi}{\\partial \\theta_j} = - \\frac{\\partial \\mathbf{A}}{\\partial \\theta_j} \\Phi\n$$\nThe derivative of the stiffness matrix is $\\frac{\\partial \\mathbf{A}}{\\partial \\theta_j} = \\operatorname{diag}(\\frac{\\partial \\boldsymbol{\\mu}_a}{\\partial \\theta_j}) = \\mu_{a,j} \\mathbf{E}_{jj}$, where $\\mathbf{E}_{jj}$ is a matrix with $1$ at entry $(j, j)$ and $0$ elsewhere. Therefore, the right-hand side is $-\\mu_{a,j} \\Phi_j \\mathbf{e}_j$.\nThe sensitivity of the fluence is then:\n$$\n\\frac{\\partial \\Phi}{\\partial \\theta_j} = -\\mathbf{A}(\\boldsymbol{\\theta})^{-1} (\\mu_{a,j} \\Phi_j \\mathbf{e}_j)\n$$\nSubstituting this back, the $j$-th column of the full Jacobian of $p_0$ is:\n$$\n\\frac{\\partial p_0}{\\partial \\theta_j} = \\Gamma \\mu_{a,j} \\Phi_j \\left( \\mathbf{e}_j - \\boldsymbol{\\mu}_a \\odot (\\mathbf{A}^{-1} \\mathbf{e}_j) \\right)\n$$\nTo compute this efficiently, for each $j=1,\\dots,n$, we solve one linear system $\\mathbf{A}\\mathbf{z}_j = \\mathbf{e}_j$ to find $\\mathbf{z}_j=\\mathbf{A}^{-1}\\mathbf{e}_j$ (the $j$-th column of $\\mathbf{A}^{-1}$) and then form the Jacobian column. The final Jacobian is $\\mathbf{J} = \\mathbf{S} \\left[ \\frac{\\partial p_0}{\\partial \\theta_1} | \\dots | \\frac{\\partial p_0}{\\partial \\theta_n} \\right]$.\n\n**4. Uncertainty Quantification via Laplace Approximation**\n\nNear the MAP estimate, the posterior distribution $p(\\boldsymbol{\\theta}|\\mathbf{y})$ is approximated by a multivariate Gaussian:\n$$\np(\\boldsymbol{\\theta}|\\mathbf{y}) \\approx \\mathcal{N}(\\boldsymbol{\\theta}_{\\text{MAP}}, \\mathbf{C})\n$$\nThe covariance matrix $\\mathbf{C}$ is the inverse of the Hessian of the negative log-posterior evaluated at the MAP: $\\mathbf{C} \\approx \\mathbf{H}(\\boldsymbol{\\theta}_{\\text{MAP}})^{-1}$. The diagonal entries of $\\mathbf{C}$, $\\mathbf{C}_{ii}$, are the marginal posterior variances for each parameter $\\theta_i$. These values quantify the uncertainty in our estimate of each log-absorption parameter. The algorithm computes $\\mathbf{H}(\\boldsymbol{\\theta}_{\\text{MAP}})$ using the final Jacobian $\\mathbf{J}(\\boldsymbol{\\theta}_{\\text{MAP}})$, inverts it to obtain $\\mathbf{C}$, and extracts the required diagonal elements.\nSpecifically, for $n=20$, the indices requested by the directive \"in code, use $0, \\lfloor n/2 \\rfloor, n-1$\" are $0$, $10$, and $19$. We report the variances $\\mathbf{C}_{0,0}$, $\\mathbf{C}_{10,10}$, and $\\mathbf{C}_{19,19}$.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the 1D QPAT inverse problem for three test cases and reports\n    marginal posterior variances based on the Laplace approximation.\n    \"\"\"\n\n    test_cases_params = [\n        # Case A (general, fully observed)\n        {\n            \"n\": 20, \"D\": 1e-2, \"Gamma\": 1.0, \"q_val\": 1.0,\n            \"mu0\": 0.1, \"delta\": 0.2, \"w\": 0.1,\n            \"sensor_type\": \"all\",\n            \"sigma2\": (1e-2)**2, \"tau2\": (0.5)**2\n        },\n        # Case B (low noise, strong data)\n        {\n            \"n\": 20, \"D\": 1e-2, \"Gamma\": 1.0, \"q_val\": 1.0,\n            \"mu0\": 0.1, \"delta\": 0.2, \"w\": 0.1,\n            \"sensor_type\": \"all\",\n            \"sigma2\": (1e-4)**2, \"tau2\": (0.5)**2\n        },\n        # Case C (partial observations, weaker prior)\n        {\n            \"n\": 20, \"D\": 1e-2, \"Gamma\": 1.0, \"q_val\": 1.0,\n            \"mu0\": 0.1, \"delta\": 0.2, \"w\": 0.1,\n            \"sensor_type\": \"partial\",\n            \"sigma2\": (1e-2)**2, \"tau2\": (1.0)**2\n        }\n    ]\n\n    results = []\n\n    for params in test_cases_params:\n        n = params[\"n\"]\n        D = params[\"D\"]\n        Gamma = params[\"Gamma\"]\n        q_val = params[\"q_val\"]\n        mu0 = params[\"mu0\"]\n        delta = params[\"delta\"]\n        w = params[\"w\"]\n        sensor_type = params[\"sensor_type\"]\n        sigma2 = params[\"sigma2\"]\n        tau2 = params[\"tau2\"]\n\n        h = 1.0 / (n + 1)\n        x_nodes = np.arange(1, n + 1) * h\n        q_vec = np.full(n, q_val)\n\n        # Standard 1D finite difference matrix K\n        K = 2 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n\n        # Ground truth and prior mean\n        mu_a_true = mu0 + delta * np.exp(-(x_nodes - 0.5)**2 / (2 * w**2))\n        theta_true = np.log(mu_a_true)\n        theta_prior = np.log(mu0) * np.ones(n)\n\n        # Sensor selection matrix S\n        if sensor_type == \"all\":\n            m = n\n            S = np.eye(n)\n        elif sensor_type == \"partial\":\n            # 1-based indices {3, 6, ..., 18} -> 0-based {2, 5, ..., 17}\n            sensor_indices = np.arange(2, n, 3)\n            m = len(sensor_indices)\n            S = np.zeros((m, n))\n            S[np.arange(m), sensor_indices] = 1.0\n        \n        # --- Forward Model: g(theta) ---\n        def forward_model(theta):\n            mu_a = np.exp(theta)\n            A = (D / h**2) * K + np.diag(mu_a)\n            # Solve A * Phi = q for Phi\n            phi = linalg.solve(A, q_vec, assume_a='pos')\n            p0 = Gamma * mu_a * phi\n            g = S @ p0\n            return g\n\n        # --- Jacobian: J(theta) ---\n        def jacobian(theta):\n            mu_a = np.exp(theta)\n            A = (D / h**2) * K + np.diag(mu_a)\n            phi = linalg.solve(A, q_vec, assume_a='pos')\n            \n            # Using scipy.linalg.solve is more stable than np.linalg.inv\n            # We solve A * z_j = e_j for each column j of the inverse\n            A_inv = linalg.inv(A)\n            \n            J_full = np.zeros((n, n))\n            for j in range(n):\n                e_j = np.zeros(n)\n                e_j[j] = 1.0\n                A_inv_col_j = A_inv[:, j]\n                # Formula derived from implicit differentiation\n                # d(p0)/d(theta_j) = Gamma * mu_a_j * Phi_j * (e_j - mu_a * (A_inv * e_j))\n                jac_col = Gamma * mu_a[j] * phi[j] * (e_j - mu_a * A_inv_col_j)\n                J_full[:, j] = jac_col\n            \n            return S @ J_full\n\n        # Generate observed data y\n        y_obs = forward_model(theta_true)\n\n        # --- Damped Gauss-Newton for MAP estimate ---\n        theta_map = np.copy(theta_prior)\n        lambda_damping = 1e-3\n        max_iter = 50\n        tol = 1e-8\n\n        for _ in range(max_iter):\n            g_curr = forward_model(theta_map)\n            J_curr = jacobian(theta_map)\n            \n            residual = g_curr - y_obs\n            \n            grad_L = (1 / sigma2) * J_curr.T @ residual + (1 / tau2) * (theta_map - theta_prior)\n\n            if np.linalg.norm(grad_L)  tol:\n                break\n            \n            # Gauss-Newton Hessian\n            H = (1 / sigma2) * (J_curr.T @ J_curr) + (1 / tau2) * np.eye(n)\n            \n            # Damped step\n            H_damped = H + lambda_damping * np.eye(n)\n            \n            try:\n                # Solve (H + lambda*I) d_theta = -grad_L\n                d_theta = linalg.solve(H_damped, -grad_L, assume_a='pos')\n                theta_map += d_theta\n            except linalg.LinAlgError:\n                # In case of instability, increase damping and retry\n                lambda_damping *= 10\n                continue\n\n            # Simple damping adjustment (optional, but good practice)\n            # A full line search or trust-region method would be more robust,\n            # but a simple update is sufficient for this problem.\n            # Here we just decrease damping for next step if solve was successful.\n            lambda_damping *= 0.1\n        \n        # --- Posterior Covariance Calculation ---\n        J_map = jacobian(theta_map)\n        H_map = (1 / sigma2) * (J_map.T @ J_map) + (1 / tau2) * np.eye(n)\n        \n        # Covariance is the inverse of the Hessian\n        C = linalg.inv(H_map)\n        \n        # Extract marginal variances at specified indices\n        # Per problem: \"in code, use 0, floor(n/2), n-1\"\n        # For n=20, this is 0, 10, 19\n        idx1 = 0\n        idx2 = n // 2\n        idx3 = n - 1\n        \n        variances = [C[idx1, idx1], C[idx2, idx2], C[idx3, idx3]]\n        results.append(variances)\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{v[0]},{v[1]},{v[2]}]\" for v in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}