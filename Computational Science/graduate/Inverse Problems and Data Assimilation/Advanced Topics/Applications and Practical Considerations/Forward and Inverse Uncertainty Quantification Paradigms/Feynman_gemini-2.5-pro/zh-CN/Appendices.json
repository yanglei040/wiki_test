{
    "hands_on_practices": [
        {
            "introduction": "反演不确定性量化的基础在于理解数据如何为我们提供关于未知参数的信息。本练习提供了一个坚实的起点，要求您推导费雪信息（Fisher Information），这是一个关键量，它将前向模型的灵敏度与参数估计的最佳可能精度联系起来，这由克拉默-拉奥下限（Cramér–Rao lower bound）所决定。通过这个计算 ，您将精确地看到实验设计和噪声水平如何控制测量数据中的信息含量。",
            "id": "3382635",
            "problem": "考虑一系列设计点 $\\{x_{i}\\}_{i=1}^{n}$，它们被视为固定的和已知的；以及一系列观测值 $\\{y_{i}\\}_{i=1}^{n}$，它们由正向模型 $y_{i} = \\exp(\\theta x_{i}) + \\eta_{i}$ 生成。其中，$\\theta \\in \\mathbb{R}$ 是一个未知的标量参数，测量误差 $\\{\\eta_{i}\\}_{i=1}^{n}$ 是独立同分布 (i.i.d.) 的，且对于一个已知的噪声方差 $\\sigma^{2} > 0$，有 $\\eta_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$。在正向不确定性量化 (UQ) 范式中，映射 $\\theta \\mapsto \\{\\exp(\\theta x_{i})\\}_{i=1}^{n}$ 将参数不确定性传播到数据空间；而在用于数据同化的逆向UQ范式中，人们通过似然函数中的统计信息从数据中量化参数的不确定性。\n\n仅使用独立高斯噪声的似然函数和费雪信息的定义，推导参数 $\\theta$ 的费雪信息 $I(\\theta)$，并用 $\\{x_{i}\\}_{i=1}^{n}$、$\\sigma^{2}$ 和 $\\theta$ 表示。然后，使用 $I(\\theta)$ 来评估参数的松散性（parameter sloppiness），通过用 $\\{x_{i}\\}_{i=1}^{n}$ 和 $\\theta$ 来刻画信息量何时大、何时小，并解释这如何通过克拉默-拉奥下界 (Cramér–Rao lower bound, CRLB) 将正向灵敏度与逆向不确定性联系起来。\n\n你的最终答案必须是 $\\theta$ 的费雪信息矩阵的唯一特征值的解析表达式（在此标量参数设置中，它等于 $I(\\theta)$）。不包含单位。不需要数值计算，也不需要四舍五入。仅报告该特征值的闭式表达式作为最终答案。",
            "solution": "该问题陈述经证实具有科学依据、是适定且客观的。它为统计推断和不确定性量化中的一个标准问题提供了完整且一致的设定。\n\n该问题要求推导一个非线性回归模型中参数 $\\theta$ 的费雪信息，并基于此信息进行分析。\n\n首先，我们建立观测值的似然函数。正向模型由 $y_i = \\exp(\\theta x_i) + \\eta_i$ 给出，其中误差 $\\eta_i$ 独立同分布于正态分布 $\\mathcal{N}(0, \\sigma^2)$。单个误差项 $\\eta_i$ 的概率密度函数 (PDF) 为：\n$$p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\eta_i^2}{2\\sigma^2}\\right)$$\n由于 $\\eta_i = y_i - \\exp(\\theta x_i)$，给定参数 $\\theta$ 的单个观测值 $y_i$ 的条件概率密度函数为：\n$$p(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\exp(\\theta x_i))^2}{2\\sigma^2}\\right)$$\n观测值 $\\{y_i\\}_{i=1}^{n}$ 是独立的，所以整个数据集的联合似然函数 $L(\\theta)$ 是各个概率密度函数的乘积：\n$$L(\\theta) = \\prod_{i=1}^{n} p(y_i|\\theta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\exp(\\theta x_i))^2}{2\\sigma^2}\\right)$$\n使用对数似然函数 $\\ell(\\theta) = \\ln(L(\\theta))$ 更为方便：\n$$\\ell(\\theta) = \\ln\\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\sum_{i=1}^{n}\\frac{(y_i - \\exp(\\theta x_i))^2}{2\\sigma^2}\\right) \\right)$$\n$$\\ell(\\theta) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\exp(\\theta x_i))^2$$\n对于标量参数 $\\theta$，费雪信息 $I(\\theta)$ 定义为对数似然函数关于 $\\theta$ 的二阶导数的负期望值。期望是针对数据 $\\{y_i\\}$ 的分布来计算的。\n$$I(\\theta) = -E\\left[ \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2} \\right]$$\n我们计算 $\\ell(\\theta)$ 的一阶和二阶导数。一阶导数（得分函数）为：\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} (y_i - \\exp(\\theta x_i))^2$$\n使用链式法则，其中内部项 $(y_i - \\exp(\\theta x_i))$ 关于 $\\theta$ 的导数为 $-x_i \\exp(\\theta x_i)$：\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\exp(\\theta x_i))(-x_i \\exp(\\theta x_i))$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\exp(\\theta x_i))x_i \\exp(\\theta x_i)$$\n现在，我们计算二阶导数：\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\exp(\\theta x_i))x_i \\exp(\\theta x_i) \\right]$$\n对项 $(y_i - \\exp(\\theta x_i))(x_i \\exp(\\theta x_i))$ 应用乘法法则：\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ (-x_i \\exp(\\theta x_i))(x_i \\exp(\\theta x_i)) + (y_i - \\exp(\\theta x_i))(x_i^2 \\exp(\\theta x_i)) \\right]$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ -x_i^2 \\exp(2\\theta x_i) + y_i x_i^2 \\exp(\\theta x_i) - x_i^2 \\exp(2\\theta x_i) \\right]$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ y_i x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right]$$\n接下来，我们对数据 $\\{y_i\\}$ 取期望。根据模型定义，$y_i = \\exp(\\theta x_i) + \\eta_i$ 且 $E[\\eta_i] = 0$，这意味着 $E[y_i] = E[\\exp(\\theta x_i) + \\eta_i] = \\exp(\\theta x_i)$。\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = E\\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ y_i x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right] \\right]$$\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ E[y_i] x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right]$$\n代入 $E[y_i] = \\exp(\\theta x_i)$:\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ (\\exp(\\theta x_i)) x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right]$$\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ x_i^2 \\exp(2\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right] = - \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i^2 \\exp(2\\theta x_i)$$\n最后，费雪信息为 $I(\\theta) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right]$：\n$$I(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i^2 \\exp(2\\theta x_i)$$\n这是费雪信息的表达式。对于标量参数，费雪信息矩阵是一个 $1 \\times 1$ 的矩阵，其唯一的特征值等于 $I(\\theta)$ 本身。\n\n现在，我们评估参数松散性以及正向和逆向UQ之间的联系。\n参数松散性指的是数据对某个参数提供的信息很少，导致难以精确估计的情况。在此背景下，$I(\\theta)$ 的值小表示松散性。\n从 $I(\\theta)$ 的表达式来看，信息量小的情况是：\n1. 噪声方差 $\\sigma^2$ 很大。更多的噪声会掩盖信号，减少信息内容。\n2. 设计点 $\\{x_i\\}$ 的选择使得总和 $\\sum_{i=1}^{n} x_i^2 \\exp(2\\theta x_i)$ 很小。如果 $x_i$ 的量级很小（例如，聚集在 $0$ 附近），就会出现这种情况。如果所有 $x_i = 0$，则 $I(\\theta)=0$，参数不可辨识。实验的信息量关键取决于实验设计 $\\{x_i\\}_{i=1}^n$。\n\n与正向和逆向UQ的联系是通过克拉默-拉奥下界 (CRLB) 和正向灵敏度的概念建立的。CRLB指出，对于 $\\theta$ 的任何无偏估计量 $\\hat{\\theta}$，其方差的下界是费雪信息的倒数：\n$$\\text{Var}(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)}$$\n项 $\\text{Var}(\\hat{\\theta})$ 代表估计参数中的不确定性（一个逆问题概念）。较大的 $I(\\theta)$ 意味着该不确定性的下界较小。\n\n模型预测 $f_i(\\theta) = \\exp(\\theta x_i)$ 相对于参数 $\\theta$ 的正向灵敏度是其导数：\n$$\\frac{\\partial f_i(\\theta)}{\\partial \\theta} = x_i \\exp(\\theta x_i)$$\n该导数量化了参数的微小变化对模型输出的影响程度（一个正向UQ概念）。\n\n将灵敏度代入 $I(\\theta)$ 的表达式中，我们找到了明确的联系：\n$$I(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left( x_i \\exp(\\theta x_i) \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial f_i(\\theta)}{\\partial \\theta}\\right)^2$$\n这个方程表明，费雪信息是正向灵敏度平方和，再按噪声方差的倒数进行缩放。因此：\n- **高正向灵敏度**（$|\\frac{\\partial f_i}{\\partial \\theta}|$ 的值大）导致**大的费雪信息** $I(\\theta)$。\n- **大的费雪信息** $I(\\theta)$ 导致**小的CRLB** $\\frac{1}{I(\\theta)}$，这意味着**低的逆向不确定性**（低的 $\\text{Var}(\\hat{\\theta})$）的潜力。\n\n总之，模型的正向灵敏度直接决定了数据的信息内容，这反过来又为逆问题中参数估计的精度设定了基本限制。当所选设计点的正向灵敏度很小时，就会出现参数松散性，导致费雪信息很小，从而参数估计的不确定性很大。\n问题要求 $\\theta$ 的费雪信息矩阵的唯一特征值。如前所示，这个值就是 $I(\\theta)$。",
            "answer": "$$\n\\boxed{\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} x_{i}^{2} \\exp(2\\theta x_{i})}\n$$"
        },
        {
            "introduction": "虽然费雪信息描述了在表现良好的问题中的不确定性，但许多现实世界的模型都存在不可辨识性（non-identifiability）问题，即不同的参数值会产生相同的输出。本练习通过一个简单但富有启发性的模型探讨了这一根本性挑战，揭示了不可辨识性如何导致复杂的、多峰值的后验分布 。通过分析这个案例，您将更深刻地理解推断的局限性，以及在面对此类不适定性（ill-posedness）时，像方差这样的简单不确定性总结为何会失效。",
            "id": "3382702",
            "problem": "考虑一个在逆问题和数据同化的正向与逆向不确定性量化范式下的标量贝叶斯逆问题。设正向映射为 $G(\\theta) = \\theta^{2}$，其中参数 $\\theta \\in \\mathbb{R}$。通过加性高斯噪声模型 $y = G(\\theta) + \\varepsilon$ 获得单个带噪观测值 $y \\in \\mathbb{R}$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ 且 $\\sigma > 0$ 为已知。参数的先验分布为高斯分布 $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$，其中 $\\tau > 0$ 为已知。所有量均为标量。在整个过程中，使用基于第一性原理的贝叶斯法则和有原则的渐近近似方法；在不可辨识性不成立之处，不要假设其成立。\n\n该设定表现出不可辨识性，因为 $G(\\theta) = G(-\\theta)$，这可能导致不适定性，表现为后验多峰性和较大的后验方差。从第一性原理（贝叶斯法则、高斯分布的性质以及 Jacques Hadamard 意义下的适定性定义）出发，推导在后验分布为双峰的情况下，后验方差 $\\mathrm{Var}[\\theta \\mid y]$ 的一个显式领头阶解析近似。在推导表达式时，请根据 $y$、$\\sigma$ 和 $\\tau$ 证明出现双峰性的条件，并讨论这种不可辨识性对稳定性（解的存在性、唯一性和对数据的连续依赖性）的影响。\n\n你的最终答案必须是关于 $\\mathrm{Var}[\\theta \\mid y]$ 领头阶近似的一个单一闭式解析表达式，该表达式是 $y$、$\\sigma$ 和 $\\tau$ 的函数。无需四舍五入，也不涉及单位。",
            "solution": "该问题被评估为有效，因为它代表了贝叶斯逆问题中一个定义明确的标准练习。它在科学上基于概率论和统计学，内部逻辑一致，并包含进行形式化推导所需的所有必要信息。\n\n我们的任务是分析给定观测值 $y$ 时参数 $\\theta$ 的后验分布。该关系由正向模型 $G(\\theta) = \\theta^{2}$ 和观测模型 $y = \\theta^{2} + \\varepsilon$ 定义，其中噪声为高斯分布，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$。关于参数的先验信念也是高斯分布，$\\theta \\sim \\mathcal{N}(0, \\tau^{2})$。\n\n首先，我们使用贝叶斯法则构建后验概率密度函数（PDF）$p(\\theta \\mid y)$：\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\n似然函数 $p(y \\mid \\theta)$ 源自噪声模型。给定一个 $\\theta$ 值，$y$ 是一个服从 $\\mathcal{N}(\\theta^{2}, \\sigma^{2})$ 分布的随机变量。因此，似然函数为：\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right)\n$$\n先验概率密度函数（PDF）给出如下：\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^{2}}} \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right)\n$$\n将这些结合起来，未归一化的后验概率密度函数为：\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\left[ \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}} \\right] \\right)\n$$\n为了找到后验分布的峰值（modes），我们可以找到负对数后验 $\\Phi(\\theta)$ 的最小值，其中 $p(\\theta \\mid y) \\propto \\exp(-\\Phi(\\theta))$：\n$$\n\\Phi(\\theta) = \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}}\n$$\n峰值是驻点，通过将关于 $\\theta$ 的一阶导数设为零来找到：\n$$\n\\frac{d\\Phi}{d\\theta} = \\frac{2(y - \\theta^{2})(-2\\theta)}{2\\sigma^{2}} + \\frac{2\\theta}{2\\tau^{2}} = -\\frac{2\\theta(y - \\theta^{2})}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} = \\theta \\left( \\frac{2\\theta^{2} - 2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right) = 0\n$$\n该方程在 $\\theta = 0$ 处有解，以及在括号内项为零时有解：\n$$\n\\frac{2\\theta^{2}}{\\sigma^{2}} = \\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}} \\implies \\theta^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}\n$$\n因此，驻点为 $\\theta_{0} = 0$ 以及，在 $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$ 的条件下，$\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$。\n\n为了确定这些点是峰值（PDF的局部极大值，$\\Phi$的局部极小值）还是反峰（PDF的局部极小值，$\\Phi$的局部极大值），我们检查 $\\Phi(\\theta)$ 的二阶导数：\n$$\n\\frac{d^{2}\\Phi}{d\\theta^{2}} = \\frac{d}{d\\theta} \\left( \\frac{2\\theta^{3}}{\\sigma^{2}} - \\frac{2y\\theta}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} \\right) = \\frac{6\\theta^{2}}{\\sigma^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\n$$\n在 $\\theta = 0$ 处：\n$$\n\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} = \\frac{1}{\\tau^{2}} - \\frac{2y}{\\sigma^{2}}\n$$\n要使 $\\theta=0$ 成为一个峰值，我们需要 $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} > 0$，这意味着 $y  \\frac{\\sigma^{2}}{2\\tau^{2}}$。在这种情况下，后验分布是单峰的。\n要使 $\\theta=0$ 成为一个反峰，我们需要 $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0}  0$，这意味着 $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$。\n\n在 $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$ 处，代入 $\\theta_{\\pm}^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}$：\n$$\n\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} = \\frac{6}{\\sigma^{2}}\\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} = \\frac{6y}{\\sigma^{2}} - \\frac{3}{\\tau^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} = \\frac{4y}{\\sigma^{2}} - \\frac{2}{\\tau^{2}} = 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right)\n$$\n点 $\\theta_{\\pm}$ 仅在 $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$ 时存在，这意味着 $\\frac{2y}{\\sigma^{2}} > \\frac{1}{\\tau^{2}}$。因此，$\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} > 0$。\n这证实了当 $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$ 时，后验分布是双峰的，两个峰值位于 $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$，一个反峰位于 $\\theta = 0$。\n\n现在我们推导在这种双峰情况下后验方差 $\\mathrm{Var}[\\theta \\mid y]$ 的领头阶近似。一个常见的有原则的近似是拉普拉斯近似，它将后验建模为以每个峰值为中心的高斯分布的混合。由于正向模型和先验的对称性，后验分布关于 $\\theta=0$ 对称。因此，后验可以近似为一个对称的两个高斯分布的混合：\n$$\np(\\theta \\mid y) \\approx \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{+}, \\sigma_{L}^{2}) + \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{-}, \\sigma_{L}^{2})\n$$\n其中均值 $\\mu_{\\pm}$ 是峰值的位置：\n$$\n\\mu_{\\pm} = \\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}\n$$\n而局部方差 $\\sigma_{L}^{2}$ 是在峰值处评估的 $\\Phi(\\theta)$ 的Hessian矩阵的逆：\n$$\n\\sigma_{L}^{2} = \\left( \\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} \\right)^{-1} = \\left( 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right) \\right)^{-1} = \\left( \\frac{4y\\tau^{2} - 2\\sigma^{2}}{\\sigma^{2}\\tau^{2}} \\right)^{-1} = \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\n这个高斯混合模型的总方差由全方差定律给出：\n$$\n\\mathrm{Var}[\\theta \\mid y] = \\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] + \\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]]\n$$\n其中 $Z$ 是一个指示混合成分的潜变量。\n第一项是各分量方差的平均值：\n$$\n\\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] = \\frac{1}{2}\\sigma_{L}^{2} + \\frac{1}{2}\\sigma_{L}^{2} = \\sigma_{L}^{2}\n$$\n第二项是各分量均值的方差。混合分布的均值为 $\\frac{1}{2}\\mu_{+} + \\frac{1}{2}\\mu_{-} = 0$。\n$$\n\\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]] = \\mathbb{E}[(\\mathbb{E}[\\theta \\mid y, Z])^{2}] - (\\mathbb{E}[\\mathbb{E}[\\theta \\mid y, Z]])^{2} = \\left( \\frac{1}{2}\\mu_{+}^{2} + \\frac{1}{2}\\mu_{-}^{2} \\right) - 0^{2} = \\mu_{+}^{2}\n$$\n结合这些项，总后验方差近似为：\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\mu_{+}^{2} + \\sigma_{L}^{2}\n$$\n代入 $\\mu_{+}^{2}$ 和 $\\sigma_{L}^{2}$ 的表达式：\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\n这个表达式是在双峰情况下（$y > \\frac{\\sigma^{2}}{2\\tau^{2}}$）后验方差的领头阶近似。\n\n最后，我们讨论 Jacques Hadamard 意义下的稳定性影响：\n1.  **存在性：** 在贝叶斯意义上，解是后验分布 $p(\\theta \\mid y)$。由于先验是正常的（proper），且似然函数是良定义的，因此对于任何 $y \\in \\mathbb{R}$，后验分布都存在。\n2.  **唯一性：** 唯一性不成立。在双峰情况下，$\\theta$ 有两个等可能性的解，由峰值 $\\mu_{\\pm}$ 表示。单个点估计，例如最大后验（MAP）估计，不是唯一的。根据对称性，后验均值为 $\\mathbb{E}[\\theta \\mid y] = 0$，但该点具有最小的后验概率，是对解的一个很差的表示。这种缺乏唯一的、有代表性的点估计正是不适定性的一种表现。\n3.  **对数据的连续依赖性（稳定性）：** 稳定性也显著地不成立。从单峰后验到双峰后验的转变发生在临界数据值 $y_{c} = \\frac{\\sigma^{2}}{2\\tau^{2}}$。当数据 $y$ 从上方接近这个临界值时（$y \\to y_{c}^{+}$），代表局部方差的项 $\\sigma_{L}^{2}$ 会发散：\n$$\n\\lim_{y \\to (\\frac{\\sigma^{2}}{2\\tau^{2}})^{+}} \\sigma_{L}^{2} = \\lim_{y \\to (\\frac{\\sigma^{2}}{2\\tau^{2}})^{+}} \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}} = \\frac{\\sigma^{2}\\tau^{2}}{4(\\frac{\\sigma^{2}}{2\\tau^{2}})\\tau^{2} - 2\\sigma^{2}} = \\frac{\\sigma^{2}\\tau^{2}}{2\\sigma^{2} - 2\\sigma^{2}} \\to \\infty\n$$\n这种发散表明后验方差在分岔点处会爆炸。在 $y_c$ 附近对数据 $y$ 的一个微小扰动，就可能导致后验分布形状的剧烈变化及其方差的巨大改变。这种对数据的极端敏感性说明了稳定性的严重缺乏，这是不适定逆问题的一个标志。由正向映射 $G(\\theta) = \\theta^{2}$ 引入的不可辨识性是导致唯一性和稳定性失效的根本原因。",
            "answer": "$$\n\\boxed{y - \\frac{\\sigma^{2}}{2\\tau^{2}} + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}}\n$$"
        },
        {
            "introduction": "真实世界的系统通常涉及多个相互作用的物理过程，导致参数之间相互耦合或难以区分。这个动手练习从单参数问题转向多参数线性模型，从而能够精确分析参数的“纠缠”或后验相关性 。您将计算模型灵敏度矩阵（雅可比矩阵）的结构如何决定后验相关性，并学习量化参数的“解耦”（disentanglement）程度，这对于验证和简化复杂模型是一个至关重要的概念。",
            "id": "3382662",
            "problem": "考虑一个耦合多物理场模型，该模型通过两个子模型的和将参数向量映射到观测数量。设总正向映射定义为 $\\mathcal{G}(m) = \\mathcal{G}_1(m_1) + \\mathcal{G}_2(m_2)$，其中参数向量 $m \\in \\mathbb{R}^3$ 的分量为 $m_s$（两个子模型共享）、$m_a$（第一个子模型独有）和 $m_b$（第二个子模型独有）。观测模型是线性的，由 $y = J m + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^3$ 是由三个独立的观测实验形成的， $J \\in \\mathbb{R}^{3 \\times 3}$ 是雅可比（设计）矩阵，其列对应于对 $m_s$、$m_a$ 和 $m_b$ 的敏感度，而 $\\varepsilon$ 是一个加性噪声项。假设不确定性表征如下：$m$ 的先验为高斯分布，均值为 $0$，对角协方差为 $\\Sigma_m = \\mathrm{diag}(v_s, v_a, v_b)$；观测噪声为高斯分布，均值为 $0$，协方差为 $\\Sigma_\\varepsilon = \\sigma^2 I_3$，其中 $I_3$ 是 $3 \\times 3$ 的单位矩阵。所有量均为无量纲。\n\n您的任务是实现一个程序，对于一组给定的测试案例，执行以下计算：\n- 正向不确定性量化聚合：计算在先验条件下可观测量 $y$ 的聚合预测方差，定义为 $y$ 各分量方差之和。\n- 逆向不确定性量化解耦：计算在线性高斯模型下 $m$ 的后验协方差，并报告两个量：\n  1. 共享参数 $m_s$ 的后验方差。\n  2. 独有参数 $m_a$ 和 $m_b$ 之间的解耦指数，定义为 $m_a$ 和 $m_b$ 之间的后验相关性的绝对值，该值必须在区间 $[0,1]$ 内。\n\n此外，通过确定模型是否能从无噪声数据中局部可辨识来评估结构可辨识性，这在线性设置中等同于设计矩阵 $J$ 具有满列秩。将此结果报告为一个布尔值。\n\n使用以下测试套件，其中每个案例指定了设计矩阵 $J$、先验方差 $(v_s, v_a, v_b)$ 和噪声标准差 $\\sigma$：\n- 案例 1（良态，可辨识）：\n  - $J = \\begin{bmatrix}\n  1.0  0.5  0.1 \\\\\n  0.8  0.2  0.7 \\\\\n  0.3  1.0  0.4\n  \\end{bmatrix}$,\n  - $(v_s, v_a, v_b) = (1.0, 1.0, 1.0)$,\n  - $\\sigma = 0.05$。\n- 案例 2（独有参数近似共线，弱解耦）：\n  - $J = \\begin{bmatrix}\n  0.9  1.0  1.02 \\\\\n  0.3  0.5  0.51 \\\\\n  0.6  0.2  0.21\n  \\end{bmatrix}$,\n  - $(v_s, v_a, v_b) = (1.0, 1.0, 1.0)$,\n  - $\\sigma = 0.05$。\n- 案例 3（因独有参数精确共线而不可辨识）：\n  - $J = \\begin{bmatrix}\n  0.7  1.0  2.0 \\\\\n  0.4  0.0  0.0 \\\\\n  0.1  1.0  2.0\n  \\end{bmatrix}$,\n  - $(v_s, v_a, v_b) = (1.0, 1.0, 1.0)$,\n  - $\\sigma = 0.05$。\n\n您必须使用的基本定义和原理如下：\n- 线性观测模型 $y = J m + \\varepsilon$，具有高斯先验和高斯噪声。\n- 适用于高斯模型的贝叶斯法则，该法则意味着后验分布也是高斯分布。\n- 线性模型中的结构可辨识性由设计矩阵 $J$ 的列秩确定。\n\n程序要求：\n- 对于每个测试案例，计算并返回一个列表，其中按以下顺序包含四个量：\n  1. 可辨识性，作为一个布尔值，指示 $J$ 是否具有满列秩。\n  2. $y$ 的聚合正向预测方差，作为一个浮点数，等于 $y$ 的三个分量方差之和。\n  3. 共享参数 $m_s$ 的后验方差，作为一个浮点数。\n  4. $m_a$ 和 $m_b$ 之间的解耦指数，作为一个浮点数，等于 $m_a$ 和 $m_b$ 之间的后验相关性的绝对值。\n- 您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试案例，并且本身是上述四个量的列表。例如，输出应类似于 \"[[case1_result1,case1_result2,case1_result3,case1_result4],[case2_result1,case2_result2,case2_result3,case2_result4],[case3_result1,case3_result2,case3_result3,case3_result4]]\"。\n\n不涉及物理单位，也不使用角度。所有数值答案均以十进制浮点数或布尔值表示。通过遵守上述线性高斯框架，确保科学真实性和内部一致性。",
            "solution": "问题陈述已经过验证，被认为是合理的。这是一个在反问题和数据同化领域的适定问题，基于线性高斯模型和贝叶斯推断的标准原理。所有必要的数据和定义均已提供，问题没有科学不准确、矛盾或模糊之处。因此，我们可以进行形式化的求解。\n\n该问题要求在三个不同的测试案例中计算四个量，这些案例均在一个线性高斯框架内。模型由 $y = J m + \\varepsilon$ 给出，其中 $m \\in \\mathbb{R}^3$ 是参数向量，$y \\in \\mathbb{R}^3$ 是观测向量，$J \\in \\mathbb{R}^{3 \\times 3}$ 是雅可比或设计矩阵，$\\varepsilon \\in \\mathbb{R}^3$ 是观测噪声。不确定性由 $m$ 的先验和噪声 $\\varepsilon$ 的高斯分布来表征。具体来说，$m \\sim \\mathcal{N}(0, \\Sigma_m)$，其中 $\\Sigma_m = \\mathrm{diag}(v_s, v_a, v_b)$；以及 $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$，其中 $\\Sigma_\\varepsilon = \\sigma^2 I_3$。$J$ 的列和 $m$ 的分量按 $(m_s, m_a, m_b)$ 的顺序排列。我们现在将推导这四个所需量的表达式。\n\n1.  **结构可辨识性**\n\n结构可辨识性评估了模型参数是否可以从无噪声数据中唯一确定。在线性模型 $y = Jm$ 的背景下，这等同于询问从参数 $m$ 到观测 $y$ 的映射是否是单射的。该条件成立当且仅当矩阵 $J$ 具有满列秩。由于 $J$ 是一个 $3 \\times 3$ 的方阵，具有满列秩等同于其可逆、具有非零行列式或秩为 $3$。我们将通过计算每个测试案例中矩阵 $J$ 的秩来确定可辨识性。\n- 如果 $\\mathrm{rank}(J) = 3$，模型是结构可辨识的。\n- 如果 $\\mathrm{rank}(J)  3$，模型不是结构可辨识的。\n\n结果报告为一个布尔值。\n\n2.  **聚合正向预测方差**\n\n此任务是正向不确定性量化的一次练习，我们将不确定性从输入参数 $m$ 通过模型传播到输出可观测量 $y$。$y$ 的预测分布是在 $m$ 的先验下形成的。给定模型 $y = Jm + \\varepsilon$，其中 $m$ 和 $\\varepsilon$ 是独立的零均值高斯随机变量，可观测量 $y$ 也是一个零均值高斯随机变量。其协方差 $\\Sigma_y$ 计算如下：\n$$ \\Sigma_y = \\mathrm{Cov}(Jm + \\varepsilon) $$\n由于 $m$ 和 $\\varepsilon$ 的独立性，和的协方差是协方差的和：\n$$ \\Sigma_y = \\mathrm{Cov}(Jm) + \\mathrm{Cov}(\\varepsilon) $$\n使用线性变换的协方差性质 $\\mathrm{Cov}(Ax) = A \\mathrm{Cov}(x) A^T$，我们得到：\n$$ \\Sigma_y = J \\mathrm{Cov}(m) J^T + \\mathrm{Cov}(\\varepsilon) $$\n代入给定的先验和噪声协方差 $\\Sigma_m$ 和 $\\Sigma_\\varepsilon$：\n$$ \\Sigma_y = J \\Sigma_m J^T + \\Sigma_\\varepsilon $$\n问题要求的是*聚合*预测方差，其定义为 $y$ 各分量方差之和。这是预测协方差矩阵 $\\Sigma_y$ 对角线元素之和，也称为矩阵的迹。\n$$ \\text{Aggregated Variance} = \\mathrm{Tr}(\\Sigma_y) = \\mathrm{Tr}(J \\Sigma_m J^T + \\Sigma_\\varepsilon) $$\n\n3.  **共享参数 $m_s$ 的后验方差**\n\n此任务及后续任务属于逆向不确定性量化。我们使用贝叶斯法则来找到给定数据模型下参数 $m$ 的后验分布。对于线性高斯问题，后验分布也是高斯分布。后验协方差矩阵 $\\Sigma_{post}$ 由标准公式给出：\n$$ \\Sigma_{post} = \\left( \\Sigma_m^{-1} + J^T \\Sigma_\\varepsilon^{-1} J \\right)^{-1} $$\n括号内的项 $H = \\Sigma_m^{-1} + J^T \\Sigma_\\varepsilon^{-1} J$ 是后验精度矩阵，或负对数后验的黑塞矩阵。让我们代入我们协方差矩阵的具体形式：\n- $\\Sigma_m^{-1} = \\mathrm{diag}(v_s^{-1}, v_a^{-1}, v_b^{-1})$\n- $\\Sigma_\\varepsilon^{-1} = (\\sigma^2 I_3)^{-1} = \\frac{1}{\\sigma^2} I_3$\n将这些代入 $H$ 的表达式中：\n$$ H = \\Sigma_m^{-1} + J^T \\left(\\frac{1}{\\sigma^2} I_3\\right) J = \\Sigma_m^{-1} + \\frac{1}{\\sigma^2} J^T J $$\n后验协方差则为 $\\Sigma_{post} = H^{-1}$。至关重要的是，$\\Sigma_m^{-1}$ 是一个正定矩阵（因为 $v_s, v_a, v_b > 0$），而 $J^T J$ 是一个半正定矩阵。一个正定矩阵与一个半正定矩阵之和总是正定的，因此，$H$ 总是可逆的。这确保了即使在 $J$ 是奇异的（不可辨识的情况）情况下，后验协方差也是良定的。\n\n后验协方差矩阵 $\\Sigma_{post}$ 具有以下结构，其中索引 $0, 1, 2$ 对应于 $m_s, m_a, m_b$：\n$$ \\Sigma_{post} = \\begin{bmatrix}\n\\mathrm{Var}(m_s|\\text{data})  \\mathrm{Cov}(m_s, m_a|\\text{data})  \\mathrm{Cov}(m_s, m_b|\\text{data}) \\\\\n\\mathrm{Cov}(m_a, m_s|\\text{data})  \\mathrm{Var}(m_a|\\text{data})  \\mathrm{Cov}(m_a, m_b|\\text{data}) \\\\\n\\mathrm{Cov}(m_b, m_s|\\text{data})  \\mathrm{Cov}(m_b, m_a|\\text{data})  \\mathrm{Var}(m_b|\\text{data})\n\\end{bmatrix} $$\n共享参数 $m_s$ 的后验方差是该矩阵的第一个对角线元素：$(\\Sigma_{post})_{0,0}$。\n\n4.  **$m_a$ 和 $m_b$ 之间的解耦指数**\n\n解耦指数定义为独有参数 $m_a$ 和 $m_b$ 之间后验相关性的绝对值。两个变量 $X$ 和 $Y$ 之间的相关系数 $\\rho_{XY}$ 由它们的协方差除以它们标准差的乘积给出。使用后验协方差矩阵 $\\Sigma_{post}$ 的元素：\n- $m_a$ 的后验方差: $\\mathrm{Var}(m_a|\\text{data}) = (\\Sigma_{post})_{1,1}$\n- $m_b$ 的后验方差: $\\mathrm{Var}(m_b|\\text{data}) = (\\Sigma_{post})_{2,2}$\n- $m_a$ 和 $m_b$ 的后验协方差: $\\mathrm{Cov}(m_a, m_b|\\text{data}) = (\\Sigma_{post})_{1,2}$\n\n因此，后验相关性为：\n$$ \\rho_{m_a, m_b | \\text{data}} = \\frac{(\\Sigma_{post})_{1,2}}{\\sqrt{(\\Sigma_{post})_{1,1} \\cdot (\\Sigma_{post})_{2,2}}} $$\n解耦指数是该量的绝对值：\n$$ \\text{Index} = \\left| \\rho_{m_a, m_b | \\text{data}} \\right| $$\n该指数提供了一个衡量数据能够区分参数 $m_a$ 和 $m_b$ 影响程度的度量。接近 0 的指数意味着良好的解耦（参数在后验上近似独立），而接近 1 的指数则意味着参数之间存在强烈的后验耦合或混淆，这通常是可辨识性差的症状。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_quantities(J, prior_variances, sigma):\n    \"\"\"\n    Computes the four required quantities for a single test case.\n\n    Args:\n        J (np.ndarray): The 3x3 Jacobian (design) matrix.\n        prior_variances (tuple): A tuple (v_s, v_a, v_b) of prior variances.\n        sigma (float): The standard deviation of the observation noise.\n\n    Returns:\n        list: A list containing the four results in order:\n              [identifiability, fwd_variance, post_var_ms, disentanglement_index].\n    \"\"\"\n    # Parameter setup\n    v_s, v_a, v_b = prior_variances\n    m = 3 # Number of parameters\n\n    # --- 1. Structural Identifiability ---\n    # Check if the Jacobian matrix J has full column rank.\n    # For a square matrix, this is equivalent to being non-singular.\n    # np.linalg.matrix_rank is the most robust way to check this numerically.\n    rank_J = np.linalg.matrix_rank(J)\n    is_identifiable = (rank_J == m)\n\n    # --- 2. Aggregated Forward Predictive Variance ---\n    # Construct prior parameter covariance and noise covariance matrices.\n    Sigma_m = np.diag([v_s, v_a, v_b])\n    Sigma_eps = (sigma**2) * np.identity(m)\n    \n    # Compute predictive covariance of the observable y: Sigma_y = J * Sigma_m * J^T + Sigma_eps\n    Sigma_y = J @ Sigma_m @ J.T + Sigma_eps\n    \n    # The aggregated variance is the trace of the predictive covariance matrix.\n    fwd_variance = np.trace(Sigma_y)\n\n    # --- 3.  4. Posterior Covariance and Derived Quantities ---\n    # Compute the posterior covariance matrix: Sigma_post = (Sigma_m^-1 + J^T * Sigma_eps^-1 * J)^-1\n    # Inverse of the prior covariance\n    Sigma_m_inv = np.diag([1/v_s, 1/v_a, 1/v_b])\n    \n    # The term J^T * Sigma_eps^-1 * J simplifies to (1/sigma^2) * J^T * J\n    # This is the Fisher Information Matrix for the parameters.\n    H_data = (1 / sigma**2) * J.T @ J\n    \n    # The posterior precision matrix (Hessian of negative log-posterior)\n    H_post = Sigma_m_inv + H_data\n    \n    # The posterior covariance matrix is the inverse of the precision matrix.\n    # This inversion is numerically stable because the prior regularizes the problem,\n    # ensuring H_post is positive definite even if J is singular.\n    Sigma_post = np.linalg.inv(H_post)\n    \n    # Extract posterior variance of the shared parameter m_s (first parameter)\n    # This corresponds to the top-left element of the posterior covariance matrix.\n    post_var_ms = Sigma_post[0, 0]\n    \n    # Compute the disentanglement index between m_a and m_b (second and third parameters)\n    # This is the absolute value of their posterior correlation.\n    cov_ab = Sigma_post[1, 2] # Cov(m_a, m_b)\n    var_a = Sigma_post[1, 1]  # Var(m_a)\n    var_b = Sigma_post[2, 2]  # Var(m_b)\n    \n    # Correlation formula: corr(a, b) = cov(a, b) / sqrt(var(a) * var(b))\n    # Handle potential division by zero, though variances should be positive.\n    if var_a > 0 and var_b > 0:\n        disentanglement_index = np.abs(cov_ab) / np.sqrt(var_a * var_b)\n    else:\n        disentanglement_index = np.nan # Should not happen in this problem\n\n    return [is_identifiable, fwd_variance, post_var_ms, disentanglement_index]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run computations, and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"J\": np.array([\n                [1.0, 0.5, 0.1],\n                [0.8, 0.2, 0.7],\n                [0.3, 1.0, 0.4]\n            ]),\n            \"prior_variances\": (1.0, 1.0, 1.0),\n            \"sigma\": 0.05\n        },\n        {\n            \"J\": np.array([\n                [0.9, 1.0, 1.02],\n                [0.3, 0.5, 0.51],\n                [0.6, 0.2, 0.21]\n            ]),\n            \"prior_variances\": (1.0, 1.0, 1.0),\n            \"sigma\": 0.05\n        },\n        {\n            \"J\": np.array([\n                [0.7, 1.0, 2.0],\n                [0.4, 0.0, 0.0],\n                [0.1, 1.0, 2.0]\n            ]),\n            \"prior_variances\": (1.0, 1.0, 1.0),\n            \"sigma\": 0.05\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_quantities(case[\"J\"], case[\"prior_variances\"], case[\"sigma\"])\n        all_results.append(result)\n\n    # Format the final output string exactly as required.\n    # The format is a list of lists, represented as a string.\n    # Ex: [[true,1.23,4.56,0.78],[false,2.34,5.67,0.89]]\n    case_strings = []\n    for res in all_results:\n        # The boolean must be lowercase 'true' or 'false'\n        bool_str = str(res[0]).lower()\n        # Create a string for the inner list, e.g., \"[true,1.23,4.56,0.78]\"\n        inner_list_str = f\"[{bool_str},{res[1]},{res[2]},{res[3]}]\"\n        case_strings.append(inner_list_str)\n\n    # Join the inner list strings with commas and enclose in brackets.\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}