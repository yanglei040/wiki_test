{
    "hands_on_practices": [
        {
            "introduction": "本节的第一个实践旨在巩固我们对观测误差协方差矩阵 $R$ 的数学要求的理解。我们将对一个简单的 $2 \\times 2$ 矩阵执行基本的线性代数计算，以验证其关键属性（如正定性）并评估其数值条件。这项练习  为后续更复杂的应用提供了坚实的具体基础。",
            "id": "3406348",
            "problem": "考虑一个变分资料同化系统中的线性观测模型，该模型用于观测一个地球物理状态变量，观测由两个具有相关误差的同位传感器同时进行。设观测误差向量被建模为一个零均值高斯随机向量，其协方差矩阵为 $R \\in \\mathbb{R}^{2 \\times 2}$。在科学上符合实际的观测误差协方差建模中，$R$ 必须是对称正定的，以反映具有物理意义的方差和协方差，并确保在同化代价函数中对 $R$ 求逆时的适定性。假设报告的观测误差协方差矩阵为\n$$\nR = \\begin{pmatrix}\n4  1\\\\\n1  1\n\\end{pmatrix},\n$$\n其中矩阵元素的单位是观测量的平方（为具体起见，设为开尔文的平方，$\\mathrm{K}^{2}$）。\n\n严格从概率论和线性代数的基本定义出发——即高斯随机向量的协方差矩阵的定义、对称矩阵的性质、通过特征值表征正定性以及谱条件数的定义——回答以下问题：\n\n1. 精确计算逆矩阵 $R^{-1}$，确保结果是对称的，正如对称正定矩阵的逆矩阵所要求的那样。\n2. 精确计算 $R$ 的特征值 $\\lambda_{1}$ 和 $\\lambda_{2}$，并用它们来验证正定性。\n3. 使用二范数谱条件数的定义，精确计算谱条件数 $\\kappa_{2}(R)$，以评估与同化代价函数中的求逆相关的数值条件。\n\n所有结果都用精确的符号形式表示（不要四舍五入）。关于单位报告：$R^{-1}$ 的元素单位应理解为 $\\mathrm{K}^{-2}$，特征值 $\\lambda_{1}, \\lambda_{2}$ 的单位应理解为 $\\mathrm{K}^{2}$。提供的最终特征值应按 $\\lambda_{1} \\geq \\lambda_{2} > 0$ 的顺序排列。最终答案必须是一个单一表达式，包含 $R^{-1}$ 的四个元素，后跟 $\\lambda_{1}$、$\\lambda_{2}$ 和 $\\kappa_{2}(R)$，使用 $\\text{pmatrix}$ 环境排成一行，并以精确形式表示。",
            "solution": "按照要求，解答分为三个部分。\n\n### 第 1 部分：计算逆矩阵 $R^{-1}$\n设给定矩阵为 $R = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} = \\begin{pmatrix} 4  1 \\\\ 1  1 \\end{pmatrix}$。由于 $b=c$，该矩阵是对称的。\n一个 $2 \\times 2$ 矩阵的逆由以下公式给出：\n$$\nR^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}\n$$\n首先，我们计算 $R$ 的行列式，记为 $\\det(R)$。\n$$\n\\det(R) = (4)(1) - (1)(1) = 4 - 1 = 3\n$$\n由于 $\\det(R) \\neq 0$，逆矩阵存在。将数值代入公式：\n$$\nR^{-1} = \\frac{1}{3} \\begin{pmatrix} 1  -1 \\\\ -1  4 \\end{pmatrix}\n$$\n将标量因子 $\\frac{1}{3}$ 分配到矩阵中，得到逆矩阵的最终形式：\n$$\nR^{-1} = \\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{4}{3} \\end{pmatrix}\n$$\n正如所要求的，逆矩阵 $R^{-1}$ 也是一个对称矩阵。\n\n### 第 2 部分：计算特征值并验证正定性\n矩阵 $R$ 的特征值 $\\lambda$ 是特征方程 $\\det(R - \\lambda I) = 0$ 的解，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n$$\nR - \\lambda I = \\begin{pmatrix} 4  1 \\\\ 1  1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4-\\lambda  1 \\\\ 1  1-\\lambda \\end{pmatrix}\n$$\n行列式为：\n$$\n\\det(R - \\lambda I) = (4-\\lambda)(1-\\lambda) - (1)(1) = 0\n$$\n展开此方程得到关于 $\\lambda$ 的二次方程：\n$$\n4 - 4\\lambda - \\lambda + \\lambda^2 - 1 = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + 3 = 0\n$$\n我们使用二次公式 $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 求解 $\\lambda$，其中 $a=1, b=-5, c=3$。\n$$\n\\lambda = \\frac{-(-5) \\pm \\sqrt{(-5)^2 - 4(1)(3)}}{2(1)} = \\frac{5 \\pm \\sqrt{25 - 12}}{2} = \\frac{5 \\pm \\sqrt{13}}{2}\n$$\n这给出了两个特征值。按照要求将它们排序为 $\\lambda_{1} \\geq \\lambda_{2}$：\n$$\n\\lambda_{1} = \\frac{5 + \\sqrt{13}}{2}\n$$\n$$\n\\lambda_{2} = \\frac{5 - \\sqrt{13}}{2}\n$$\n为了验证 $R$ 是正定的，我们必须证明其所有特征值都严格为正。\n对于 $\\lambda_{1}$，$5$ 和 $\\sqrt{13}$ 都是正数，所以 $\\lambda_{1} > 0$。\n对于 $\\lambda_{2}$，我们需要检查是否 $5 - \\sqrt{13} > 0$。这等价于检查是否 $5 > \\sqrt{13}$，也就等价于 $5^2 > 13$，即 $25 > 13$。这是成立的。\n由于 $\\lambda_{1}$ 和 $\\lambda_{2}$ 都严格为正，对称矩阵 $R$ 是正定的。这证实了问题陈述的前提。\n\n### 第 3 部分：计算谱条件数 $\\kappa_{2}(R)$\n矩阵 $R$ 的谱条件数（使用矩阵 2-范数）定义为 $\\kappa_{2}(R) = \\|R\\|_{2} \\|R^{-1}\\|_{2}$。\n对于对称正定矩阵，其 2-范数等于其最大特征值（$\\lambda_{\\max}$）。因此，$\\|R\\|_{2} = \\lambda_{\\max}(R)$。\n逆矩阵 $R^{-1}$ 的特征值是 $R$ 的特征值的倒数。因此，$R^{-1}$ 的最大特征值是 $R$ 的最小特征值的倒数，即 $\\lambda_{\\max}(R^{-1}) = 1/\\lambda_{\\min}(R)$。\n因此，对于对称正定矩阵，条件数简化为最大特征值与最小特征值的比值：\n$$\n\\kappa_{2}(R) = \\frac{\\lambda_{\\max}(R)}{\\lambda_{\\min}(R)} = \\frac{\\lambda_{1}}{\\lambda_{2}}\n$$\n代入计算出的特征值：\n$$\n\\kappa_{2}(R) = \\frac{\\frac{5 + \\sqrt{13}}{2}}{\\frac{5 - \\sqrt{13}}{2}} = \\frac{5 + \\sqrt{13}}{5 - \\sqrt{13}}\n$$\n为了以简化形式表示，我们通过将分子和分母同乘以分母的共轭数 $5 + \\sqrt{13}$ 来使分母有理化：\n$$\n\\kappa_{2}(R) = \\frac{5 + \\sqrt{13}}{5 - \\sqrt{13}} \\times \\frac{5 + \\sqrt{13}}{5 + \\sqrt{13}} = \\frac{(5 + \\sqrt{13})^2}{5^2 - (\\sqrt{13})^2}\n$$\n分子展开为：\n$$\n(5 + \\sqrt{13})^2 = 5^2 + 2(5)(\\sqrt{13}) + (\\sqrt{13})^2 = 25 + 10\\sqrt{13} + 13 = 38 + 10\\sqrt{13}\n$$\n分母为：\n$$\n5^2 - (\\sqrt{13})^2 = 25 - 13 = 12\n$$\n结合这些结果：\n$$\n\\kappa_{2}(R) = \\frac{38 + 10\\sqrt{13}}{12}\n$$\n通过将分子和分母除以它们的最大公约数 2 来化简分数：\n$$\n\\kappa_{2}(R) = \\frac{19 + 5\\sqrt{13}}{6}\n$$\n这就是精确的谱条件数。高条件数表明矩阵是病态的，意味着其逆矩阵对矩阵元素的微小扰动很敏感，这会在代价函数最小化过程中带来数值挑战。\n\n最终结果是：$R^{-1}$ 的元素、特征值 $\\lambda_1, \\lambda_2$ 以及条件数 $\\kappa_2(R)$。将这些结果收集到要求的最终格式中。\n$R^{-1}$ 的四个元素是 $(R^{-1})_{11} = \\frac{1}{3}$、$(R^{-1})_{12} = -\\frac{1}{3}$、$(R^{-1})_{21} = -\\frac{1}{3}$ 和 $(R^{-1})_{22} = \\frac{4}{3}$。\n$\\lambda_{1} = \\frac{5+\\sqrt{13}}{2}$。\n$\\lambda_{2} = \\frac{5-\\sqrt{13}}{2}$。\n$\\kappa_{2}(R) = \\frac{19+5\\sqrt{13}}{6}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3}  -\\frac{1}{3}  -\\frac{1}{3}  \\frac{4}{3}  \\frac{5 + \\sqrt{13}}{2}  \\frac{5 - \\sqrt{13}}{2}  \\frac{19 + 5\\sqrt{13}}{6} \\end{pmatrix}}$$"
        },
        {
            "introduction": "在掌握了基本概念之后，这项实践将展示为何精确建模协方差矩阵 $R$ 至关重要。通过比较一个考虑了误差相关性的最优估计器与一个忽略了这些相关性的简单估计器，我们可以量化地衡量估计精度的提升。这个练习  突显了超越对角 $R$ 矩阵假设的实际价值。",
            "id": "3406402",
            "problem": "考虑数据同化背景下的一个线性观测模型，\n$$\ny = H x_{\\text{true}} + \\varepsilon,\n$$\n其中 $x_{\\text{true}} \\in \\mathbb{R}^{2}$ 是一个未知的状态向量，$y \\in \\mathbb{R}^{3}$ 是观测数据，$\\varepsilon \\sim \\mathcal{N}(0, R)$ 表示观测误差，其协方差矩阵 $R$ 已知且非对角。观测算子 $H \\in \\mathbb{R}^{3 \\times 2}$ 和观测误差协方差 $R \\in \\mathbb{R}^{3 \\times 3}$ 由下式给出\n$$\nH = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  1\n\\end{pmatrix}, \\qquad\nR = \\begin{pmatrix}\n2  1  0 \\\\\n1  2  1 \\\\\n0  1  2\n\\end{pmatrix}.\n$$\n观测的一个实现为\n$$\ny = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n从 $\\varepsilon$ 的高斯似然出发，并假设 $x_{\\text{true}}$ 的先验为平坦先验，执行以下操作：\n\n1) 从第一性原理出发，推导使与观测误差的负对数似然相关的二次型最小化的估计量 $\\displaystyle x^{\\ast}$，并为给定的 $H$、$R$ 和 $y$ 计算其值。\n\n2) 在真实观测误差协方差 $R$ 下，使用线性估计量误差传播，推导第 1) 部分中估计量的误差协方差。然后，考虑忽略 $R$ 的非对角结构并使用单位权重的普通最小二乘法 (OLS) 估计量，并在真实 $R$ 下推导其误差协方差。使用这些结果计算广义方差比\n$$\n\\rho \\equiv \\frac{\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)}{\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)}.\n$$\n\n将 $\\rho$ 的最终答案表示为精确分数。无需四舍五入。最终答案必须是一个数字。",
            "solution": "观测模型为 $y = H x_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, R)$。$x_{\\text{true}}$ 的平坦先验意味着最大后验估计等于最大似然估计。在高斯误差下，负对数似然（不计一个与 $x$ 无关的加法常数）由以下二次型给出\n$$\nJ(x) = (y - Hx)^{\\top} R^{-1} (y - Hx).\n$$\n最小化 $J(x)$ 的估计量是广义最小二乘 (GLS) 估计量。我们通过将其梯度设为零来推导它：\n$$\n\\nabla_x J(x) = -2 H^{\\top} R^{-1} (y - Hx) = 0\n\\quad \\Longrightarrow \\quad\nH^{\\top} R^{-1} H \\, x = H^{\\top} R^{-1} y.\n$$\n假设 $H^{\\top} R^{-1} H$ 是可逆的，最小化解为\n$$\nx^{\\ast} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y.\n$$\n\n我们现在为给定的 $H$、$R$ 和 $y$ 计算这个解。\n\n首先，计算 $R^{-1}$。对于\n$$\nR = \\begin{pmatrix}\n2  1  0 \\\\\n1  2  1 \\\\\n0  1  2\n\\end{pmatrix},\n$$\n可以验证\n$$\nR^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3  -2  1 \\\\\n-2  4  -2 \\\\\n1  -2  3\n\\end{pmatrix}.\n$$\n接下来，计算 $H^{\\top} R^{-1} H$。先计算 $R^{-1} H$ 会比较方便。使用\n$$\nH = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n0  1\n\\end{pmatrix},\n\\quad\nR^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3  -2  1 \\\\\n-2  4  -2 \\\\\n1  -2  3\n\\end{pmatrix},\n$$\n我们得到\n$$\nR^{-1} H = \\frac{1}{4}\n\\begin{pmatrix}\n1  -1 \\\\\n2  2 \\\\\n-1  1\n\\end{pmatrix}.\n$$\n因此，\n$$\nH^{\\top} R^{-1} H\n= H^{\\top} (R^{-1} H)\n= \\begin{pmatrix}\n\\frac{3}{4}  \\frac{1}{4} \\\\\n\\frac{1}{4}  \\frac{3}{4}\n\\end{pmatrix}.\n$$\n其逆矩阵为\n$$\n\\big(H^{\\top} R^{-1} H\\big)^{-1}\n= \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{3}{2}\n\\end{pmatrix},\n$$\n因为行列式为 $\\frac{1}{2}$，伴随矩阵为\n$\\begin{pmatrix} \\frac{3}{4}  -\\frac{1}{4} \\\\ -\\frac{1}{4}  \\frac{3}{4} \\end{pmatrix}$。\n\n我们还需要 $H^{\\top} R^{-1} y$。当 $y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 时，\n$$\nR^{-1} y\n= \\frac{1}{4}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{3}{4} \\\\\n-\\frac{1}{2} \\\\\n\\frac{1}{4}\n\\end{pmatrix},\n\\quad\nH^{\\top} R^{-1} y\n= \\begin{pmatrix}\n1  1  0 \\\\\n0  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{3}{4} \\\\\n-\\frac{1}{2} \\\\\n\\frac{1}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n-\\frac{1}{4}\n\\end{pmatrix}.\n$$\n因此，\n$$\nx^{\\ast} = \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{3}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n-\\frac{1}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n-\\frac{1}{2}\n\\end{pmatrix}.\n$$\n\n我们现在在真实观测误差协方差 $R$ 下推导误差协方差。对于一个线性估计量 $x = K y$，估计误差为 $x - x_{\\text{true}} = K(H x_{\\text{true}} + \\varepsilon) - x_{\\text{true}} = (K H - I)x_{\\text{true}} + K \\varepsilon$。为了相对于 $x_{\\text{true}}$ 的无偏性，我们要求 $K H = I$，下面的广义最小二乘 (GLS) 和普通最小二乘 (OLS) 选择都满足这个条件。在 $\\varepsilon \\sim \\mathcal{N}(0, R)$ 的条件下，误差协方差为\n$$\n\\operatorname{Var}(x) = K R K^{\\top}.\n$$\n\n对于 GLS，估计量为\n$$\nx^{\\ast} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y \\equiv K_{\\text{GLS}} y,\n$$\n其中 $K_{\\text{GLS}} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$。那么\n$$\n\\operatorname{Var}(x^{\\ast})\n= K_{\\text{GLS}} R K_{\\text{GLS}}^{\\top}\n= (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} R R^{-1} H (H^{\\top} R^{-1} H)^{-1}\n= (H^{\\top} R^{-1} H)^{-1}.\n$$\n使用计算出的矩阵，我们得到\n$$\n\\operatorname{Var}(x^{\\ast})\n= \\begin{pmatrix}\n\\frac{3}{2}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{3}{2}\n\\end{pmatrix}.\n$$\n\n对于 OLS，它忽略了 $R$ 的非对角结构并使用单位权重，其估计量为\n$$\nx_{\\text{OLS}} = (H^{\\top} H)^{-1} H^{\\top} y \\equiv K_{\\text{OLS}} y,\n\\quad\nK_{\\text{OLS}} = (H^{\\top} H)^{-1} H^{\\top}.\n$$\n那么\n$$\n\\operatorname{Var}(x_{\\text{OLS}})\n= K_{\\text{OLS}} R K_{\\text{OLS}}^{\\top}\n= (H^{\\top} H)^{-1} H^{\\top} R H (H^{\\top} H)^{-1}.\n$$\n我们计算所需的矩阵。首先，\n$$\nH^{\\top} H\n= \\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix},\n\\quad\n(H^{\\top} H)^{-1}\n= \\frac{1}{3}\n\\begin{pmatrix}\n2  -1 \\\\\n-1  2\n\\end{pmatrix}.\n$$\n接下来，\n$$\nR H\n= \\begin{pmatrix}\n3  1 \\\\\n3  3 \\\\\n1  3\n\\end{pmatrix},\n\\quad\nH^{\\top} R H\n= \\begin{pmatrix}\n6  4 \\\\\n4  6\n\\end{pmatrix}.\n$$\n因此，\n$$\n\\operatorname{Var}(x_{\\text{OLS}})\n= \\frac{1}{3}\n\\begin{pmatrix}\n2  -1 \\\\\n-1  2\n\\end{pmatrix}\n\\begin{pmatrix}\n6  4 \\\\\n4  6\n\\end{pmatrix}\n\\frac{1}{3}\n\\begin{pmatrix}\n2  -1 \\\\\n-1  2\n\\end{pmatrix}\n= \\frac{1}{9}\n\\begin{pmatrix}\n14  -4 \\\\\n-4  14\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{14}{9}  -\\frac{4}{9} \\\\\n-\\frac{4}{9}  \\frac{14}{9}\n\\end{pmatrix}.\n$$\n\n我们现在计算广义方差比\n$$\n\\rho \\equiv \\frac{\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)}{\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)}.\n$$\n对于一个 $2 \\times 2$ 的对称矩阵\n$\\begin{pmatrix} a  c \\\\ c  a \\end{pmatrix}$，其行列式为 $a^{2} - c^{2}$。\n\n对于 $\\operatorname{Var}(x^{\\ast})$，我们有 $a = \\frac{3}{2}$ 和 $c = -\\frac{1}{2}$，所以\n$$\n\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)\n= \\left(\\frac{3}{2}\\right)^{2} - \\left(-\\frac{1}{2}\\right)^{2}\n= \\frac{9}{4} - \\frac{1}{4}\n= 2.\n$$\n对于 $\\operatorname{Var}(x_{\\text{OLS}})$，我们有 $a = \\frac{14}{9}$ 和 $c = -\\frac{4}{9}$，所以\n$$\n\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)\n= \\left(\\frac{14}{9}\\right)^{2} - \\left(-\\frac{4}{9}\\right)^{2}\n= \\frac{196}{81} - \\frac{16}{81}\n= \\frac{180}{81}\n= \\frac{20}{9}.\n$$\n因此，\n$$\n\\rho = \\frac{\\frac{20}{9}}{2} = \\frac{10}{9}.\n$$\n该比值大于 1，表明通过非对角的 $R$ 矩阵恰当地考虑相关的观测误差，相比于普通最小二乘法 (OLS)，减小了估计量的广义方差。",
            "answer": "$$\\boxed{\\frac{10}{9}}$$"
        },
        {
            "introduction": "在现实世界的大规模系统中，真实的观测误差协方差通常是未知的或计算上难以处理的。这最后一个实践介绍协方差局部化，这是一种强大且广泛使用的技术，用于构建实用且高效的 $R$ 矩阵。通过一个编程练习 ，我们将实现这种方法并分析其中涉及的权衡，从而在理论与实际数据同化操作之间架起一座桥梁。",
            "id": "3406395",
            "problem": "考虑一个一维空间有限网格上的线性高斯逆问题。设未知状态向量为 $x \\in \\mathbb{R}^n$，其高斯先验为 $x \\sim \\mathcal{N}(x_b, B)$，观测值 $y \\in \\mathbb{R}^m$ 由线性模型 $y = H x + \\varepsilon$ 生成，其中 $H \\in \\mathbb{R}^{m \\times n}$ 是观测算子，观测误差 $\\varepsilon \\sim \\mathcal{N}(0, R)$ 与 $x$ 独立。网格点的位置为 $x_i = i$，其中 $i \\in \\{0,1,\\dots,n-1\\}$。先验协方差 $B \\in \\mathbb{R}^{n \\times n}$ 是具有指数核的平稳协方差，真实的观测误差协方差 $R \\in \\mathbb{R}^{m \\times m}$ 也是具有指数核的平稳协方差，二者都具有有限的相关长度。\n\n基本原理：\n- 线性高斯贝叶斯法则表明后验（分析）分布是高斯的，它由先验二次型和似然二次型的组合产生。由此，可以通过配方法推导出分析协方差。\n- Schur 积定理指出，两个半正定矩阵的逐元素（Hadamard）积是半正定的。\n\n观测误差协方差建模与局地化：\n- 真实观测误差协方差的元素为\n$$\nR_{ij}^{\\text{true}} = \\sigma_r^2 \\exp\\!\\left(-\\frac{|s_i - s_j|}{L_r}\\right),\n$$\n其中 $s_i$ 和 $s_j$ 是观测分量的空间位置，$\\sigma_r^2$ 是方差，$L_r$ 是相关长度。\n- 一个局地化的观测误差协方差构造如下\n$$\nR_{\\text{loc}} = R_{\\text{true}} \\odot C,\n$$\n其中 $\\odot$ 表示 Hadamard 积，$C$ 是由一个紧支集正定函数定义的锥化矩阵。使用 Wendland $C^2$ 锥化函数\n$$\n\\phi(q) =\n\\begin{cases}\n(1 - q)^4 (1 + 4 q),  0 \\le q \\le 1,\\\\\n0,  q > 1,\n\\end{cases}\n$$\n其中 $q = \\frac{|s_i - s_j|}{L_{\\text{loc}}}$ 且矩阵元素为 $C_{ij} = \\phi\\!\\left(\\frac{|s_i - s_j|}{L_{\\text{loc}}}\\right)$。按照惯例，取 $L_{\\text{loc}} = \\infty$ 意味着对所有 $i,j$ 都有 $C_{ij} = 1$（无局地化），取 $L_{\\text{loc}} = 0$ 意味着 $C = I$（对角化）。\n\n任务：\n1. 仅使用上述基本原理，推导使用真实 $R_{\\text{true}}$ 的分析协方差 $A_\\star$，以及使用局地化 $R_{\\text{loc}}$ 的分析协方差 $A_{\\text{loc}}$。\n2. 通过相对 Frobenius 范数来量化由局地化引起的分析协方差偏差\n$$\n\\beta = \\frac{\\|A_{\\text{loc}} - A_\\star\\|_F}{\\|A_\\star\\|_F}。\n$$\n3. 实现一个程序，为下面测试套件中的每个测试用例计算 $\\beta$。该程序应根据给定参数构造 $B$、$H$、$R_{\\text{true}}$ 和 $R_{\\text{loc}}$，然后计算 $A_\\star$ 和 $A_{\\text{loc}}$，最后输出 $\\beta$ 值的列表。\n\n协方差构造细节：\n- 对于先验，\n$$\nB_{ij} = \\sigma_b^2 \\exp\\!\\left(-\\frac{|x_i - x_j|}{L_b}\\right), \\quad i,j \\in \\{0,1,\\dots,n-1\\}。\n$$\n- 对于 $R_{\\text{true}}$，计算观测索引的位置 $s_i$，并使用参数 $(\\sigma_r, L_r)$ 应用与上述相同的指数形式。\n- 对于 $R_{\\text{loc}}$，将前述的 Wendland $C^2$ 锥化函数应用于 $R_{\\text{true}}$。\n\n不涉及角度单位。不需要物理单位。所有输出必须是实数。\n\n测试套件：\n- 用例 $1$（理想情况，无局地化）：\n  - $n = 20$，观测索引 $\\{0,1,2,\\dots,19\\}$。\n  - $\\sigma_b = 1$，$L_b = 2$。\n  - $\\sigma_r = 0.5$，$L_r = 3$。\n  - $L_{\\text{loc}} = \\infty$。\n- 用例 $2$（中等程度局地化）：\n  - $n = 20$，观测索引 $\\{0,1,2,\\dots,19\\}$。\n  - $\\sigma_b = 1$，$L_b = 2$。\n  - $\\sigma_r = 0.5$，$L_r = 3$。\n  - $L_{\\text{loc}} = 2$。\n- 用例 $3$（强局地化至对角阵）：\n  - $n = 20$，观测索引 $\\{0,1,2,\\dots,19\\}$。\n  - $\\sigma_b = 1$，$L_b = 2$。\n  - $\\sigma_r = 0.5$，$L_r = 3$。\n  - $L_{\\text{loc}} = 0$。\n- 用例 $4$（使用局地化的稀疏观测）：\n  - $n = 20$，观测索引 $\\{0,2,4,6,8,10,12,14,16,18\\}$。\n  - $\\sigma_b = 1$，$L_b = 2$。\n  - $\\sigma_r = 0.5$，$L_r = 3$。\n  - $L_{\\text{loc}} = 1$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含按用例 1 到 4 的顺序排列的结果，形式为方括号括起来的逗号分隔列表，例如 $[\\beta_1,\\beta_2,\\beta_3,\\beta_4]$。",
            "solution": "任务是为一个线性高斯逆问题，在两种不同的观测误差协方差假设下，推导其分析协方差矩阵，然后量化由此产生的分析协方差偏差。\n\n### 第 1 部分：分析协方差的推导\n\n求解的基础是贝叶斯定理，它将给定观测值 $y$ 时状态 $x$ 的后验概率 $p(x|y)$ 与似然 $p(y|x)$ 和先验 $p(x)$ 联系起来：\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\n问题指定了在线性模型 $y = Hx + \\varepsilon$ 中，高斯先验为 $x \\sim \\mathcal{N}(x_b, B)$，高斯观测误差为 $\\varepsilon \\sim \\mathcal{N}(0, R)$。对应的概率密度函数为：\n- 先验： $p(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b)\\right)$\n- 似然： $p(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)$\n\n由于两个关于 $x$ 的高斯函数的乘积仍是高斯函数，因此后验 $p(x|y)$ 也是高斯的。我们可以将其写作 $x|y \\sim \\mathcal{N}(x_a, A)$，其中 $x_a$ 是分析状态（后验均值），$A$ 是分析协方差（后验协方差）。\n\n后验密度的指数部分是先验和似然指数部分之和。该指数部分的相反数定义了代价函数 $J(x)$，它与负对数后验成正比：\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\n$$\n对于高斯分布 $\\mathcal{N}(\\mu, \\Sigma)$，指数中的二次型为 $\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu)$。协方差矩阵的逆 $\\Sigma^{-1}$ 是该二次型关于 $x$ 的 Hessian 矩阵。因此，分析协方差的逆 $A^{-1}$ 是 $J(x)$ 关于 $x$ 的 Hessian 矩阵。\n\n为了求 Hessian 矩阵，我们首先展开 $J(x)$：\n$$\nJ(x) = \\frac{1}{2}(x^T B^{-1} x - 2x_b^T B^{-1} x + \\text{const}) + \\frac{1}{2}(x^T H^T R^{-1} H x - 2y^T R^{-1} H x + \\text{const})\n$$\n$J(x)$ 关于 $x$ 的梯度是：\n$$\n\\nabla_x J(x) = B^{-1}(x - x_b) - H^T R^{-1}(y - Hx) = (B^{-1} + H^T R^{-1} H)x - (B^{-1}x_b + H^T R^{-1}y)\n$$\nHessian 矩阵是 $\\nabla_x J(x)$ 关于 $x$ 的梯度：\n$$\n\\nabla_x^2 J(x) = A^{-1} = B^{-1} + H^T R^{-1} H\n$$\n对此表达式求逆，得到分析协方差 $A$ 的公式：\n$$\nA = \\left(B^{-1} + H^T R^{-1} H\\right)^{-1}\n$$\n此推导仅使用了问题陈述中指定的基本原理。\n\n现在我们可以定义所需的两种分析协方差：\n1.  真实的分析协方差 $A_\\star$ 是使用真实观测误差协方差 $R_{\\text{true}}$ 推导得出的：\n    $$\n    A_\\star = \\left(B^{-1} + H^T R_{\\text{true}}^{-1} H\\right)^{-1}\n    $$\n2.  局地化的分析协方差 $A_{\\text{loc}}$ 是使用局地化观测误差协方差 $R_{\\text{loc}} = R_{\\text{true}} \\odot C$ 推导得出的：\n    $$\n    A_{\\text{loc}} = \\left(B^{-1} + H^T R_{\\text{loc}}^{-1} H\\right)^{-1}\n    $$\nSchur 积定理指出，两个半正定矩阵的 Hadamard 积是半正定的，这确保了 $R_{\\text{loc}}$ 是一个有效的协方差矩阵，因为 $R_{\\text{true}}$ 和 $C$ 都是由正定函数构造的，因此它们是正定的（或对于 $C$ 是半正定的）。\n\n### 第 2 部分：偏差量化\n\n使用 $R_{\\text{loc}}$ 而非 $R_{\\text{true}}$ 所引起的偏差，通过所得分析协方差之间差异的相对 Frobenius 范数来量化：\n$$\n\\beta = \\frac{\\|A_{\\text{loc}} - A_\\star\\|_F}{\\|A_\\star\\|_F}\n$$\n一个 $n \\times n$ 矩阵 $M$ 的 Frobenius 范数定义为 $\\|M\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n |M_{ij}|^2}$。\n\n### 第 3 部分：算法实现\n\n每个测试用例的 $\\beta$ 计算过程如下：\n\n1.  **参数初始化**：对于每个用例，设置 $n$、观测索引、$\\sigma_b$、$L_b$、$\\sigma_r$、$L_r$ 和 $L_{\\text{loc}}$ 的值。\n2.  **矩阵构造**：\n    -   生成网格点位置向量 $x_{pos} = [0, 1, \\dots, n-1]$。\n    -   使用公式 $B_{ij} = \\sigma_b^2 \\exp(-\\frac{|x_i - x_j|}{L_b})$ 构造 $n \\times n$ 的先验协方差矩阵 $B$。\n    -   确定观测数量 $m$，并构造 $m \\times n$ 的观测算子 $H$，它是一个将状态向量分量映射到观测分量的选择矩阵。\n    -   根据 $x_{pos}$ 和观测索引生成观测位置向量 $s_{pos}$。\n    -   使用 $R_{ij}^{\\text{true}} = \\sigma_r^2 \\exp(-\\frac{|s_i - s_j|}{L_r})$ 构造 $m \\times m$ 的真实观测误差协方差 $R_{\\text{true}}$。\n    -   构造 $m \\times m$ 的局地化观测误差协方差 $R_{\\text{loc}}$：\n        -   如果 $L_{\\text{loc}} = \\infty$，则设置 $R_{\\text{loc}} = R_{\\text{true}}$。\n        -   如果 $L_{\\text{loc}} = 0$，则将 $R_{\\text{loc}}$ 设置为一个对角矩阵，其对角线元素与 $R_{\\text{true}}$ 的对角线元素相等，这可以简化为 $\\sigma_r^2 I$。\n        -   否则，使用 Wendland $C^2$ 函数构造 $m \\times m$ 的锥化矩阵 $C$，其中 $C_{ij} = \\phi(\\frac{|s_i - s_j|}{L_{\\text{loc}})$，然后计算 Hadamard 积 $R_{\\text{loc}} = R_{\\text{true}} \\odot C$。\n3.  **分析协方差计算**：\n    -   计算矩阵的逆 $B^{-1}$、$R_{\\text{true}}^{-1}$ 和 $R_{\\text{loc}}^{-1}$。\n    -   计算 $A_\\star = (B^{-1} + H^T R_{\\text{true}}^{-1} H)^{-1}$。\n    -   计算 $A_{\\text{loc}} = (B^{-1} + H^T R_{\\text{loc}}^{-1} H)^{-1}$。\n4.  **偏差计算**：\n    -   计算 Frobenius 范数 $\\|A_{\\text{loc}} - A_\\star\\|_F$ 和 $\\|A_\\star\\|_F$。\n    -   计算它们的比值 $\\beta$。对于 $L_{\\text{loc}} = \\infty$ 的情况，$A_{\\text{loc}} = A_\\star$，所以 $\\beta$ 精确为 0。\n5.  **输出**：将所有测试用例的 $\\beta$ 值收集到一个列表中，并按要求格式化输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_beta(n, observed_indices, sigma_b, L_b, sigma_r, L_r, L_loc):\n    \"\"\"\n    Calculates the bias in analysis covariance due to localization.\n\n    Args:\n        n (int): Dimension of the state vector.\n        observed_indices (list): Indices of the observed state components.\n        sigma_b (float): Standard deviation for the prior covariance.\n        L_b (float): Correlation length for the prior covariance.\n        sigma_r (float): Standard deviation for the observation error covariance.\n        L_r (float): Correlation length for the observation error covariance.\n        L_loc (float): Correlation length for the localization taper.\n\n    Returns:\n        float: The relative Frobenius norm of the bias, beta.\n    \"\"\"\n    # 1. Define grid and observation operator H\n    x_pos = np.arange(n, dtype=float)\n    obs_indices_arr = np.array(observed_indices, dtype=int)\n    m = len(obs_indices_arr)\n    H = np.zeros((m, n), dtype=float)\n    if m > 0:\n        H[np.arange(m), obs_indices_arr] = 1.0\n\n    # 2. Construct prior covariance B\n    dist_matrix_b = np.abs(x_pos[:, None] - x_pos[None, :])\n    B = sigma_b**2 * np.exp(-dist_matrix_b / L_b)\n\n    # 3. Construct true observation error covariance R_true\n    s_pos = x_pos[obs_indices_arr]\n    dist_matrix_r = np.abs(s_pos[:, None] - s_pos[None, :])\n    R_true = sigma_r**2 * np.exp(-dist_matrix_r / L_r)\n\n    # 4. Construct localized observation error covariance R_loc\n    if np.isinf(L_loc):\n        R_loc = R_true\n    elif L_loc == 0.0:\n        R_loc = np.diag(np.diag(R_true))\n    else:\n        q = dist_matrix_r / L_loc\n        # Wendland C^2 taper function: phi(q) = (1-q)^4 * (1+4q) for 0 = q = 1\n        phi = np.zeros_like(q, dtype=float)\n        mask = (q >= 0)  (q = 1)\n        q_masked = q[mask]\n        phi[mask] = (1 - q_masked)**4 * (1 + 4 * q_masked)\n        C = phi\n        R_loc = R_true * C  # Hadamard product\n\n    # 5. Compute analysis covariances A_star and A_loc\n    # Formula: A = (B_inv + H.T @ R_inv @ H)^-1\n    try:\n        B_inv = np.linalg.inv(B)\n        H_T = H.T\n\n        R_true_inv = np.linalg.inv(R_true)\n        A_star_inv = B_inv + H_T @ R_true_inv @ H\n        A_star = np.linalg.inv(A_star_inv)\n        \n        # When L_loc=inf, A_loc is A_star, so beta=0.\n        if np.isinf(L_loc):\n            return 0.0\n\n        R_loc_inv = np.linalg.inv(R_loc)\n        A_loc_inv = B_inv + H_T @ R_loc_inv @ H\n        A_loc = np.linalg.inv(A_loc_inv)\n\n    except np.linalg.LinAlgError:\n        # Should not happen with given parameters, but robust code handles it.\n        return np.nan\n\n    # 6. Quantify bias beta\n    diff_norm = np.linalg.norm(A_loc - A_star, 'fro')\n    star_norm = np.linalg.norm(A_star, 'fro')\n    \n    if star_norm == 0:\n        return np.inf if diff_norm > 0 else 0.0\n    \n    beta = diff_norm / star_norm\n    return beta\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, no localization)\n        {'n': 20, 'observed_indices': list(range(20)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': np.inf},\n        # Case 2 (moderate localization)\n        {'n': 20, 'observed_indices': list(range(20)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': 2.0},\n        # Case 3 (strong localization to diagonal)\n        {'n': 20, 'observed_indices': list(range(20)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': 0.0},\n        # Case 4 (sparser observations with localization)\n        {'n': 20, 'observed_indices': list(range(0, 20, 2)), 'sigma_b': 1.0, 'L_b': 2.0, 'sigma_r': 0.5, 'L_r': 3.0, 'L_loc': 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_beta(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}