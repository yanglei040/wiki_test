{
    "hands_on_practices": [
        {
            "introduction": "A central task in experimental design is to choose which measurements will be most informative. This exercise provides a foundational look at this choice by comparing two simple strategies: replication and diversification. By calculating the expected information gain, measured by the Kullback-Leibler divergence, you will see how prior knowledge about the parameters can determine whether it's better to repeat a measurement or to try something new .",
            "id": "3367120",
            "problem": "Consider a linear inverse problem with a two-dimensional parameter vector $\\theta \\in \\mathbb{R}^{2}$, modeled with a Gaussian prior $p(\\theta) = \\mathcal{N}(0,\\Sigma_{0})$ where $\\Sigma_{0} = \\mathrm{diag}(\\alpha,\\beta)$. You can perform $N$ experiments, each producing a scalar observation modeled by $y = X \\theta + \\varepsilon$, where $X \\in \\mathbb{R}^{N \\times 2}$ is the design matrix whose rows are the chosen design vectors, and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I)$ is independent Gaussian noise. Two admissible designs are under consideration:\n\n- Design $\\mathrm{A}$ (replication): choose the same design vector $v_{1} = e_{1}$ twice, so $X_{\\mathrm{A}}$ has rows $e_{1}^{\\top}$ and $e_{1}^{\\top}$.\n- Design $\\mathrm{B}$ (diversification): choose orthogonal design vectors $v_{1} = e_{1}$ and $v_{2} = e_{2}$, so $X_{\\mathrm{B}}$ has rows $e_{1}^{\\top}$ and $e_{2}^{\\top}$.\n\nAdopt as the Bayesian experimental design utility the expected Kullback–Leibler divergence (KLD) from the posterior distribution to the prior distribution of $\\theta$, that is, the mutual information between $\\theta$ and the data $y$ under the specified linear-Gaussian model. Starting from first principles and core definitions, derive the utility for each of the two designs and compute the difference\n$$\\Delta U = U_{\\mathrm{B}} - U_{\\mathrm{A}}$$\nfor the specific parameter and noise settings $\\alpha = 9$, $\\beta = 1$, and $\\sigma^{2} = 1$. Express your final answer as a single closed-form analytic expression involving the natural logarithm, with no rounding and no units.",
            "solution": "The utility of an experimental design, denoted by the design matrix $X$, is given by the expected Kullback–Leibler divergence (KLD) from the posterior distribution of the parameters $\\theta$ to their prior distribution. This quantity is equivalent to the mutual information between the parameters $\\theta$ and the data $y$.\nFor a given design $X$, the utility is $U(X) = I(\\theta; y | X)$.\nThe mutual information can be expressed using differential entropies as $I(\\theta; y) = H(\\theta) - H(\\theta|y)$.\n\nThe prior distribution for $\\theta \\in \\mathbb{R}^{k}$ is given as $p(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0)$. The differential entropy of a multivariate Gaussian distribution is $H(\\theta) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_0)$. In this problem, the dimension of $\\theta$ is $k=2$ and the prior mean is $\\mu_0 = 0$.\n\nThe observation model is $y = X\\theta + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. The likelihood function is $p(y|\\theta) = \\mathcal{N}(y | X\\theta, \\sigma^2 I)$.\nUnder this linear-Gaussian model, the posterior distribution $p(\\theta|y)$ is also Gaussian, $p(\\theta|y) = \\mathcal{N}(\\mu_{post}, \\Sigma_{post})$. The posterior covariance matrix $\\Sigma_{post}$ is given by the inverse of the sum of the inverse prior covariance and the Fisher information from the data:\n$$ \\Sigma_{post}^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^{\\top}X $$\nCrucially, $\\Sigma_{post}$ does not depend on the specific data realization $y$. The entropy of the posterior distribution is $H(p(\\theta|y)) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post})$.\nThe conditional entropy $H(\\theta|y)$ is the expectation of $H(p(\\theta|y))$ over all possible data $y$:\n$$ H(\\theta|y) = \\mathbb{E}_{y}[H(p(\\theta|y))] $$\nSince $H(p(\\theta|y))$ is independent of $y$, the expectation is trivial, and $H(\\theta|y) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post})$.\n\nSubstituting the entropies into the mutual information formula:\n$$ U(X) = H(\\theta) - H(\\theta|y) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_0) - \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post}) $$\n$$ U(X) = \\frac{1}{2} \\left[ \\ln(\\det(\\Sigma_0)) - \\ln(\\det(\\Sigma_{post})) \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det(\\Sigma_0)}{\\det(\\Sigma_{post})}\\right) = \\frac{1}{2} \\ln(\\det(\\Sigma_0 \\Sigma_{post}^{-1})) $$\nSubstituting the expression for $\\Sigma_{post}^{-1}$:\n$$ U(X) = \\frac{1}{2} \\ln\\left(\\det\\left(\\Sigma_0 \\left(\\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^{\\top}X\\right)\\right)\\right) = \\frac{1}{2} \\ln\\left(\\det\\left(I + \\frac{1}{\\sigma^2} \\Sigma_0 X^{\\top}X\\right)\\right) $$\nThis expression provides the utility for any design $X$. We now apply this formula to the two specified designs using the given parameter values: $\\Sigma_0 = \\mathrm{diag}(\\alpha, \\beta) = \\mathrm{diag}(9, 1)$ and $\\sigma^2=1$. The utility simplifies to:\n$$ U(X) = \\frac{1}{2} \\ln(\\det(I + \\Sigma_0 X^{\\top}X)) $$\n\nFor Design A (replication), the design matrix $X_{\\mathrm{A}}$ has two identical rows $e_1^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\n$$ X_{\\mathrm{A}} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix} $$\nWe compute the matrix product $X_{\\mathrm{A}}^{\\top}X_{\\mathrm{A}}$:\n$$ X_{\\mathrm{A}}^{\\top}X_{\\mathrm{A}} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nNow we can compute the utility $U_{\\mathrm{A}}$:\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 18 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\begin{pmatrix} 19 & 0 \\\\ 0 & 1 \\end{pmatrix}\\right) = \\frac{1}{2} \\ln(19) $$\n\nFor Design B (diversification), the design matrix $X_{\\mathrm{B}}$ has rows $e_1^{\\top} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $e_2^{\\top} = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$.\n$$ X_{\\mathrm{B}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nThe matrix product $X_{\\mathrm{B}}^{\\top}X_{\\mathrm{B}}$ is:\n$$ X_{\\mathrm{B}}^{\\top}X_{\\mathrm{B}} = I_2^{\\top} I_2 = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nNow we can compute the utility $U_{\\mathrm{B}}$:\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\left( I_2 + \\Sigma_0 I_2 \\right)\\right) = \\frac{1}{2} \\ln(\\det(I_2 + \\Sigma_0)) $$\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\begin{pmatrix} 10 & 0 \\\\ 0 & 2 \\end{pmatrix}\\right) = \\frac{1}{2} \\ln(20) $$\n\nFinally, we compute the difference in utility $\\Delta U = U_{\\mathrm{B}} - U_{\\mathrm{A}}$:\n$$ \\Delta U = \\frac{1}{2} \\ln(20) - \\frac{1}{2} \\ln(19) $$\nUsing the property of logarithms $\\ln(a) - \\ln(b) = \\ln(a/b)$, we get:\n$$ \\Delta U = \\frac{1}{2} \\left( \\ln(20) - \\ln(19) \\right) = \\frac{1}{2} \\ln\\left(\\frac{20}{19}\\right) $$\nThis is the final closed-form analytical expression.",
            "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{20}{19}\\right)}$$"
        },
        {
            "introduction": "While gathering more data is generally better, the value of each new piece of information is not constant. This practice formalizes the crucial economic principle of diminishing returns within the Bayesian framework . You will derive the information gain objective as a function of experimental effort and use its second derivative—the curvature—to precisely quantify how the benefit of additional investment decreases.",
            "id": "3367052",
            "problem": "Consider a linear-Gaussian Bayesian inverse problem with parameter vector $\\theta \\in \\mathbb{R}^{d}$ endowed with a Gaussian prior $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\text{prior}})$, and a linear observation model $y = H \\theta + \\eta$. The observational noise is Gaussian with covariance $\\Gamma_{\\eta}/u$, where $u > 0$ is a scalar effort parameter (for example, proportional to the number of independent replicates), and $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta}/u)$. Under this model, the Expected Information Gain (Kullback–Leibler divergence from the posterior to the prior) for a given effort $u$ can be expressed in terms of the ratio of posterior and prior covariance determinants. In this problem, use the following data:\n- Dimension $d = 2$.\n- Prior covariance $\\Gamma_{\\text{prior}} = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1.2 \\end{pmatrix}$.\n- Forward model matrix $H = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$.\n- Noise covariance $\\Gamma_{\\eta} = \\begin{pmatrix} 1.0 & 0.2 \\\\ 0.2 & 1.5 \\end{pmatrix}$.\n\nStarting from core definitions of Gaussian conditioning and the log-determinant identity for mutual information, derive the scalar D-optimal design objective $J(u)$ as a function of $u$ and compute its second derivative with respect to $u$ to characterize curvature (which captures diminishing returns). Then, for the specific value $u = 1$, evaluate the curvature $J''(1)$ numerically using the provided matrices.\n\nRound your answer to $4$ significant figures. Express the final answer as a dimensionless real number.",
            "solution": "The problem requires the derivation and evaluation of the curvature of the D-optimal design objective function, $J(u)$, for a linear-Gaussian Bayesian inverse problem. The objective function is the Expected Information Gain (EIG), which for this model is the Kullback-Leibler divergence from the posterior to the prior, averaged over all possible data.\n\nThe prior distribution for the parameter vector $\\theta \\in \\mathbb{R}^{d}$ is given as $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\text{prior}})$. The observation model is $y = H \\theta + \\eta$, where the noise $\\eta$ is distributed as $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta}/u)$. Here, $u$ is a scalar effort parameter.\n\nThe posterior distribution of $\\theta$ given an observation $y$ is also a Gaussian, $p(\\theta|y) = \\mathcal{N}(\\mu_{\\text{post}}, \\Gamma_{\\text{post}})$. The posterior precision (inverse covariance) matrix is the sum of the prior precision and the data-dependent precision from the likelihood:\n$$ \\Gamma_{\\text{post}}^{-1} = \\Gamma_{\\text{prior}}^{-1} + H^T (\\Gamma_{\\eta}/u)^{-1} H = \\Gamma_{\\text{prior}}^{-1} + u H^T \\Gamma_{\\eta}^{-1} H $$\n\nThe EIG, which we denote as $J(u)$, is the mutual information between the parameters $\\theta$ and the data $y$. For a linear-Gaussian system, this quantity is independent of the specific observation $y$ and can be expressed in terms of the prior and posterior covariances:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\Gamma_{\\text{prior}})}{\\det(\\Gamma_{\\text{post}})} \\right) = \\frac{1}{2} \\ln \\left( \\det(\\Gamma_{\\text{post}}^{-1} \\Gamma_{\\text{prior}}) \\right) $$\nSubstituting the expression for $\\Gamma_{\\text{post}}^{-1}$:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\det\\left( (\\Gamma_{\\text{prior}}^{-1} + u H^T \\Gamma_{\\eta}^{-1} H) \\Gamma_{\\text{prior}} \\right) \\right) = \\frac{1}{2} \\ln \\left( \\det(I + u H^T \\Gamma_{\\eta}^{-1} H \\Gamma_{\\text{prior}}) \\right) $$\nUsing the Sylvester determinant identity, $\\det(I + AB) = \\det(I + BA)$, we can write this as:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\det(I + u \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H) \\right) $$\nLet us define the matrix $\\mathcal{I} = \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H$. While $\\mathcal{I}$ is not generally symmetric, its eigenvalues are real, non-negative, and identical to those of the symmetric matrix $\\mathcal{I}_s = \\Gamma_{\\text{prior}}^{1/2} H^T \\Gamma_{\\eta}^{-1} H \\Gamma_{\\text{prior}}^{1/2}$. Let the eigenvalues of $\\mathcal{I}$ (and $\\mathcal{I}_s$) be $\\{\\lambda_i\\}_{i=1}^{d}$. Then the determinant can be expressed as a product over eigenvalues:\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\prod_{i=1}^{d} (1 + u \\lambda_i) \\right) = \\frac{1}{2} \\sum_{i=1}^{d} \\ln(1 + u \\lambda_i) $$\nThis is the derived objective function.\n\nTo find the curvature, we compute the second derivative of $J(u)$ with respect to $u$. The first derivative is:\n$$ J'(u) = \\frac{dJ}{du} = \\frac{1}{2} \\sum_{i=1}^{d} \\frac{\\lambda_i}{1 + u \\lambda_i} $$\nThe second derivative, which represents the curvature, is:\n$$ J''(u) = \\frac{d^2J}{du^2} = \\frac{1}{2} \\sum_{i=1}^{d} \\frac{-(\\lambda_i)(\\lambda_i)}{(1 + u \\lambda_i)^2} = -\\frac{1}{2} \\sum_{i=1}^{d} \\left( \\frac{\\lambda_i}{1 + u \\lambda_i} \\right)^2 $$\nThis expression for $J''(u)$ is always non-positive, which correctly captures the principle of diminishing returns on investment of effort $u$.\n\nNow we evaluate $J''(1)$ using the provided matrices. The dimension is $d=2$.\n- $\\Gamma_{\\text{prior}} = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1.2 \\end{pmatrix}$\n- $H = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n- $\\Gamma_{\\eta} = \\begin{pmatrix} 1.0 & 0.2 \\\\ 0.2 & 1.5 \\end{pmatrix}$\n\nFirst, we compute the matrix $\\mathcal{I} = \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H$ and find its eigenvalues.\n\n1.  Calculate $\\Gamma_{\\eta}^{-1}$:\n    $\\det(\\Gamma_{\\eta}) = (1.0)(1.5) - (0.2)(0.2) = 1.5 - 0.04 = 1.46$.\n    $$ \\Gamma_{\\eta}^{-1} = \\frac{1}{1.46} \\begin{pmatrix} 1.5 & -0.2 \\\\ -0.2 & 1.0 \\end{pmatrix} $$\n\n2.  Calculate the Fisher information matrix $H^T \\Gamma_{\\eta}^{-1} H$:\n    $$ H^T \\Gamma_{\\eta}^{-1} H = \\frac{1}{1.46} \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1.5 & -0.2 \\\\ -0.2 & 1.0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\n    $$ = \\frac{1}{1.46} \\begin{pmatrix} 1.3 & 0.8 \\\\ 1.1 & 1.8 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{1.46} \\begin{pmatrix} 2.1 & 2.9 \\\\ 2.9 & 4.7 \\end{pmatrix} $$\n\n3.  Calculate $\\mathcal{I} = \\Gamma_{\\text{prior}} (H^T \\Gamma_{\\eta}^{-1} H)$:\n    $$ \\mathcal{I} = \\begin{pmatrix} 2 & 0.6 \\\\ 0.6 & 1.2 \\end{pmatrix} \\left( \\frac{1}{1.46} \\begin{pmatrix} 2.1 & 2.9 \\\\ 2.9 & 4.7 \\end{pmatrix} \\right) $$\n    $$ \\mathcal{I} = \\frac{1}{1.46} \\begin{pmatrix} (2)(2.1) + (0.6)(2.9) & (2)(2.9) + (0.6)(4.7) \\\\ (0.6)(2.1) + (1.2)(2.9) & (0.6)(2.9) + (1.2)(4.7) \\end{pmatrix} $$\n    $$ \\mathcal{I} = \\frac{1}{1.46} \\begin{pmatrix} 4.2 + 1.74 & 5.8 + 2.82 \\\\ 1.26 + 3.48 & 1.74 + 5.64 \\end{pmatrix} = \\frac{1}{1.46} \\begin{pmatrix} 5.94 & 8.62 \\\\ 4.74 & 7.38 \\end{pmatrix} $$\n\n4.  Find the eigenvalues of $\\mathcal{I}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(\\mathcal{I} - \\lambda I) = 0$. Let $A = \\begin{pmatrix} 5.94 & 8.62 \\\\ 4.74 & 7.38 \\end{pmatrix}$, so the eigenvalues of $\\mathcal{I}$ are $\\frac{1}{1.46}$ times the eigenvalues of $A$. The characteristic equation for $A$ is $\\Lambda^2 - \\text{tr}(A)\\Lambda + \\det(A) = 0$.\n    $\\text{tr}(A) = 5.94 + 7.38 = 13.32$.\n    $\\det(A) = (5.94)(7.38) - (8.62)(4.74) = 43.8372 - 40.8588 = 2.9784$.\n    So, $\\Lambda^2 - 13.32 \\Lambda + 2.9784 = 0$.\n    Using the quadratic formula, $\\Lambda = \\frac{13.32 \\pm \\sqrt{13.32^2 - 4(2.9784)}}{2} = \\frac{13.32 \\pm \\sqrt{165.5088}}{2}$.\n    $\\sqrt{165.5088} \\approx 12.864983$.\n    $\\Lambda_1 = \\frac{13.32 + 12.864983}{2} \\approx 13.0924915$.\n    $\\Lambda_2 = \\frac{13.32 - 12.864983}{2} \\approx 0.2275085$.\n    The eigenvalues of $\\mathcal{I}$ are:\n    $\\lambda_1 = \\Lambda_1 / 1.46 \\approx 13.0924915 / 1.46 \\approx 8.967460$.\n    $\\lambda_2 = \\Lambda_2 / 1.46 \\approx 0.2275085 / 1.46 \\approx 0.155828$.\n\n5.  Evaluate $J''(u)$ at $u=1$:\n    $$ J''(1) = -\\frac{1}{2} \\left[ \\left( \\frac{\\lambda_1}{1 + \\lambda_1} \\right)^2 + \\left( \\frac{\\lambda_2}{1 + \\lambda_2} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ \\left( \\frac{8.967460}{1 + 8.967460} \\right)^2 + \\left( \\frac{0.155828}{1 + 0.155828} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ \\left( \\frac{8.967460}{9.967460} \\right)^2 + \\left( \\frac{0.155828}{1.155828} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ (0.899679)^2 + (0.134819)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} [ 0.809422 + 0.018176 ] $$\n    $$ J''(1) \\approx -\\frac{1}{2} [ 0.827598 ] \\approx -0.413799 $$\n\nRounding to $4$ significant figures, the curvature is $-0.4138$.",
            "answer": "$$\\boxed{-0.4138}$$"
        },
        {
            "introduction": "Optimal experimental design can often be formulated in an idealized continuous setting, for instance, by distributing measurement effort over a spatial domain. This advanced practice bridges the gap between this theoretical ideal and a practical, discrete implementation . You will first determine an optimal continuous design within a given family and then analyze the loss in information that results from approximating it with a realistic, discrete set of measurement points.",
            "id": "3367035",
            "problem": "Consider a linear inverse problem on the domain $[-1,1]$ with an unknown two-dimensional parameter vector $\\theta = (a,b)^{\\top}$. The forward model evaluates two known basis functions $g_{1}(x) = 1$ and $g_{2}(x) = x$ at measurement locations $s \\in [-1,1]$, resulting in a noisily observed linear functional. The experimenter may select a continuous design measure with a density $w(s) \\ge 0$ over $[-1,1]$, subject to a fixed total experimental budget, expressed as $\\int_{-1}^{1} w(s) \\, ds = m$, where $m>0$ is given. The prior on $\\theta$ is Gaussian with covariance $\\Gamma_{\\mathrm{pr}} = \\tau^{2} I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix and $\\tau^{2} > 0$; the observational noise is independent and Gaussian with variance $\\sigma^{2} > 0$.\n\nStarting from fundamental linear-Gaussian Bayesian data assimilation, and without invoking any pre-derived optimality shortcuts, carry out the following steps:\n\n1. Formulate the continuous design measure and derive the Bayesian $D$-optimality criterion for this problem, expressed as the natural logarithm of the determinant of the posterior precision matrix, as a functional of $w(s)$ and in terms of the moments $\\mu_{0} = \\int_{-1}^{1} w(s) \\, ds$, $\\mu_{1} = \\int_{-1}^{1} s \\, w(s) \\, ds$, and $\\mu_{2} = \\int_{-1}^{1} s^{2} \\, w(s) \\, ds$.\n\n2. Restrict attention to a parametric family of symmetric continuous design densities on $[-1,1]$ given by $w_{k}(s) = m \\, c_{k} \\, \\big(1 - s^{2}\\big)^{k}$ for $k \\ge 0$, where $c_{k}$ is a normalization constant chosen so that $\\int_{-1}^{1} w_{k}(s) \\, ds = m$. Derive $c_{k}$ and express the $D$-optimality criterion in terms of $k$. Using these expressions, derive the choice $k^{\\star}$ that maximizes the $D$-optimality criterion within this family of continuous designs.\n\n3. To study discretization effects, approximate the continuous design corresponding to $k^{\\star}$ by a discrete design with $N$ equally spaced measurement points at the midpoints, $s_{i} = -1 + \\frac{2i-1}{N}$ for $i = 1, 2, \\dots, N$, and assign equal weights $\\frac{m}{N}$ to each point. Compute the resulting discrete moment $\\mu_{2}^{(N)} = \\frac{m}{N} \\sum_{i=1}^{N} s_{i}^{2}$, derive the corresponding discrete $D$-optimality criterion, and then compute the difference\n$$\n\\Delta(N) = \\Phi_{D}^{\\mathrm{cont}} - \\Phi_{D}^{\\mathrm{disc}}(N),\n$$\nwhere $\\Phi_{D}^{\\mathrm{cont}}$ is the $D$-criterion for the continuous design at $k^{\\star}$, while $\\Phi_{D}^{\\mathrm{disc}}(N)$ is for the discretized design with $N$ points as above.\n\nFinally, for the numerical values $m = 10$, $\\tau^{2} = 1$, $\\sigma^{2} = 0.25$, and $N = 5$, evaluate $\\Delta(N)$ as a single real-valued number. Express your final numerical answer as a dimensionless quantity and round to four significant figures.",
            "solution": "The problem concerns a linear inverse problem where an unknown parameter vector $\\theta = (a, b)^{\\top}$ is to be estimated from noisy measurements. The forward model, which maps the parameters to observable data at a measurement location $s \\in [-1, 1]$, is given by $F(\\theta, s) = a \\cdot g_1(s) + b \\cdot g_2(s)$, with basis functions $g_1(s)=1$ and $g_2(s)=x$. This can be expressed in matrix form as $F(\\theta, s) = K(s)\\theta$, where the forward operator is $K(s) = \\begin{pmatrix} 1 & s \\end{pmatrix}$.\n\nThe Bayesian framework incorporates a Gaussian prior on the parameters, $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\mathrm{pr}})$, with covariance $\\Gamma_{\\mathrm{pr}} = \\tau^2 I_2$, where $I_2$ is the $2 \\times 2$ identity matrix and $\\tau^2 > 0$. The prior precision matrix is therefore $\\Gamma_{\\mathrm{pr}}^{-1} = \\frac{1}{\\tau^2} I_2$. The observational noise is independent and Gaussian with variance $\\sigma^2 > 0$.\n\nIn a linear-Gaussian setting, the posterior distribution is also Gaussian. The posterior precision matrix, $\\Gamma_{\\mathrm{post}}^{-1}$, is the sum of the prior precision and the Fisher information matrix from the data, $H$:\n$$\n\\Gamma_{\\mathrm{post}}^{-1} = \\Gamma_{\\mathrm{pr}}^{-1} + H\n$$\n\n**Step 1: The Bayesian $D$-optimality Criterion as a Functional of $w(s)$**\n\nThe problem specifies a continuous design measure with density $w(s) \\ge 0$ on $[-1, 1]$, subject to the constraint $\\int_{-1}^{1} w(s) \\, ds = m$. The Fisher information matrix $H$ for a continuous design is obtained by integrating the information from each infinitesimal measurement over the design space:\n$$\nH = \\int_{-1}^{1} \\frac{1}{\\sigma^2} K(s)^{\\top} K(s) w(s) \\, ds\n$$\nThe kernel $K(s)^{\\top} K(s)$ is:\n$$\nK(s)^{\\top} K(s) = \\begin{pmatrix} 1 \\\\ s \\end{pmatrix} \\begin{pmatrix} 1 & s \\end{pmatrix} = \\begin{pmatrix} 1 & s \\\\ s & s^2 \\end{pmatrix}\n$$\nSubstituting this into the integral for $H$ gives:\n$$\nH = \\frac{1}{\\sigma^2} \\int_{-1}^{1} \\begin{pmatrix} 1 & s \\\\ s & s^2 \\end{pmatrix} w(s) \\, ds = \\frac{1}{\\sigma^2} \\begin{pmatrix} \\int_{-1}^{1} w(s) \\, ds & \\int_{-1}^{1} s \\, w(s) \\, ds \\\\ \\int_{-1}^{1} s \\, w(s) \\, ds & \\int_{-1}^{1} s^2 \\, w(s) \\, ds \\end{pmatrix}\n$$\nUsing the provided moment definitions $\\mu_0 = \\int_{-1}^{1} w(s) \\, ds$, $\\mu_1 = \\int_{-1}^{1} s \\, w(s) \\, ds$, and $\\mu_2 = \\int_{-1}^{1} s^2 \\, w(s) \\, ds$, the information matrix is:\n$$\nH = \\frac{1}{\\sigma^2} \\begin{pmatrix} \\mu_0 & \\mu_1 \\\\ \\mu_1 & \\mu_2 \\end{pmatrix}\n$$\nFrom the problem constraint, we have $\\mu_0 = m$.\nNow, we can write the posterior precision matrix:\n$$\n\\Gamma_{\\mathrm{post}}^{-1} = \\frac{1}{\\tau^2} I_2 + \\frac{1}{\\sigma^2} \\begin{pmatrix} m & \\mu_1 \\\\ \\mu_1 & \\mu_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2} & \\frac{\\mu_1}{\\sigma^2} \\\\ \\frac{\\mu_1}{\\sigma^2} & \\frac{1}{\\tau^2} + \\frac{\\mu_2}{\\sigma^2} \\end{pmatrix}\n$$\nThe Bayesian $D$-optimality criterion, $\\Phi_D$, is the natural logarithm of the determinant of the posterior precision matrix.\n$$\n\\Phi_D[w] = \\ln\\left(\\det(\\Gamma_{\\mathrm{post}}^{-1})\\right) = \\ln\\left( \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{\\mu_2}{\\sigma^2}\\right) - \\left(\\frac{\\mu_1}{\\sigma^2}\\right)^2 \\right)\n$$\nThis expression defines the $D$-optimality criterion as a functional of the design density $w(s)$ through its moments.\n\n**Step 2: Optimization within a Parametric Family of Designs**\n\nWe are given a family of symmetric design densities $w_k(s) = m \\, c_k \\, (1-s^2)^k$ for $k \\ge 0$. First, we find the normalization constant $c_k$ from the constraint $\\int_{-1}^{1} w_k(s) \\, ds = m$:\n$$\nm = \\int_{-1}^{1} m \\, c_k \\, (1-s^2)^k \\, ds \\implies c_k^{-1} = \\int_{-1}^{1} (1-s^2)^k \\, ds\n$$\nThis integral is related to the Beta function, $B(x, y) = \\int_0^1 t^{x-1}(1-t)^{y-1} \\, dt = 2 \\int_0^{\\pi/2} (\\sin\\theta)^{2x-1}(\\cos\\theta)^{2y-1} \\, d\\theta$.\nThe integral is $2 \\int_0^1 (1-s^2)^k \\, ds$. Let $t=s^2$, $s=t^{1/2}$, $ds = \\frac{1}{2}t^{-1/2}dt$.\n$$\nc_k^{-1} = 2 \\int_0^1 (1-t)^k \\frac{1}{2} t^{-1/2} \\, dt = \\int_0^1 t^{1/2-1} (1-t)^{(k+1)-1} \\, dt = B\\left(\\frac{1}{2}, k+1\\right) = \\frac{\\Gamma(1/2)\\Gamma(k+1)}{\\Gamma(k+3/2)}\n$$\nThus, the normalization constant is $c_k = \\frac{\\Gamma(k+3/2)}{\\Gamma(1/2)\\Gamma(k+1)}$.\n\nNext, we calculate the moments $\\mu_1$ and $\\mu_2$.\nFor $\\mu_1$:\n$$\n\\mu_1 = \\int_{-1}^{1} s \\, w_k(s) \\, ds = m c_k \\int_{-1}^{1} s(1-s^2)^k \\, ds\n$$\nThe integrand $s(1-s^2)^k$ is an odd function integrated over a symmetric interval $[-1, 1]$, so the integral is zero. Hence, $\\mu_1 = 0$.\n\nFor $\\mu_2$:\n$$\n\\mu_2 = \\int_{-1}^{1} s^2 \\, w_k(s) \\, ds = m c_k \\int_{-1}^{1} s^2(1-s^2)^k \\, ds\n$$\nThe integral part evaluates to $B(3/2, k+1) = \\frac{\\Gamma(3/2)\\Gamma(k+1)}{\\Gamma(k+5/2)}$.\n$$\n\\mu_2 = m c_k \\frac{\\Gamma(3/2)\\Gamma(k+1)}{\\Gamma(k+5/2)} = m \\frac{\\Gamma(k+3/2)}{\\Gamma(1/2)\\Gamma(k+1)} \\frac{\\Gamma(3/2)\\Gamma(k+1)}{\\Gamma(k+5/2)}\n$$\nUsing the properties $\\Gamma(z+1) = z\\Gamma(z)$, $\\Gamma(1/2)=\\sqrt{\\pi}$, $\\Gamma(3/2) = \\frac{1}{2}\\Gamma(1/2)$, and $\\Gamma(k+5/2) = (k+3/2)\\Gamma(k+3/2)$, we get:\n$$\n\\mu_2 = m \\frac{\\Gamma(k+3/2)}{\\Gamma(1/2)\\Gamma(k+1)} \\frac{\\frac{1}{2}\\Gamma(1/2)\\Gamma(k+1)}{(k+3/2)\\Gamma(k+3/2)} = m \\frac{1/2}{k+3/2} = \\frac{m}{2k+3}\n$$\nNow, substitute the moments ($\\mu_0=m, \\mu_1=0, \\mu_2=\\frac{m}{2k+3}$) into the $D$-optimality criterion:\n$$\n\\Phi_D(k) = \\ln\\left( \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2(2k+3)}\\right) \\right) = \\ln\\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) + \\ln\\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2(2k+3)}\\right)\n$$\nTo maximize $\\Phi_D(k)$ for $k \\ge 0$, we need to maximize the second term, as the first is constant. This is equivalent to maximizing its argument, $\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2(2k+3)}$. Since $m, \\sigma^2, \\tau^2$ are positive, this term is maximized when the denominator $2k+3$ is minimized. For the domain $k \\ge 0$, the minimum of $2k+3$ occurs at $k=0$.\nTherefore, the optimal choice within this family of designs is $k^{\\star} = 0$.\n\n**Step 3: Discretization Effects**\n\nThe optimal continuous design within the given family corresponds to $k^{\\star}=0$, which is the uniform design $w_0(s) = m c_0 (1-s^2)^0 = m c_0$. With $c_0^{-1} = \\int_{-1}^1 ds = 2$, we have $c_0=1/2$, so $w_0(s) = m/2$. The moments for this continuous design are:\n$\\mu_0 = m$, $\\mu_1 = 0$, and $\\mu_2 = \\int_{-1}^1 s^2 (m/2) \\, ds = \\frac{m}{2}[\\frac{s^3}{3}]_{-1}^1 = \\frac{m}{3}$.\nThe D-criterion for this continuous design is:\n$$\n\\Phi_D^{\\text{cont}} = \\ln\\left( \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{m}{3\\sigma^2}\\right) \\right)\n$$\nNow, consider the discrete design with $N$ equally spaced points $s_i = -1 + \\frac{2i-1}{N}$ for $i=1, \\dots, N$, and equal weights $\\frac{m}{N}$.\nThe discrete moments are sums:\n$\\mu_0^{(N)} = \\sum_{i=1}^N \\frac{m}{N} = m$.\n$\\mu_1^{(N)} = \\sum_{i=1}^N s_i \\frac{m}{N} = \\frac{m}{N} \\sum_{i=1}^N s_i = 0$ due to the symmetry of points $s_i$ around $0$.\n$\\mu_2^{(N)} = \\sum_{i=1}^N s_i^2 \\frac{m}{N} = \\frac{m}{N} \\sum_{i=1}^N \\left(-1 + \\frac{2i-1}{N}\\right)^2$.\nLet's evaluate the sum:\n$$\n\\sum_{i=1}^N \\left(-1 + \\frac{2i-1}{N}\\right)^2 = \\sum_{i=1}^N \\left(1 - \\frac{2(2i-1)}{N} + \\frac{(2i-1)^2}{N^2}\\right)\n$$\nUsing the sum identities $\\sum_{i=1}^N (2i-1) = N^2$ and $\\sum_{i=1}^N (2i-1)^2 = \\frac{N(4N^2-1)}{3}$:\n$$\n\\sum_{i=1}^N s_i^2 = N - \\frac{2}{N}(N^2) + \\frac{1}{N^2}\\frac{N(4N^2-1)}{3} = N - 2N + \\frac{4N^2-1}{3N} = -N + \\frac{4N}{3} - \\frac{1}{3N} = \\frac{N}{3} - \\frac{1}{3N}\n$$\nThus, the discrete second moment is:\n$$\n\\mu_2^{(N)} = \\frac{m}{N} \\left(\\frac{N}{3} - \\frac{1}{3N}\\right) = m\\left(\\frac{1}{3} - \\frac{1}{3N^2}\\right)\n$$\nThe D-criterion for the discrete design is:\n$$\n\\Phi_D^{\\text{disc}}(N) = \\ln\\left( \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{\\mu_2^{(N)}}{\\sigma^2}\\right) \\right) = \\ln\\left( \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\left(\\frac{1}{3} - \\frac{1}{3N^2}\\right)\\right) \\right)\n$$\nThe difference $\\Delta(N) = \\Phi_D^{\\text{cont}} - \\Phi_D^{\\text{disc}}(N)$ is then:\n$$\n\\Delta(N) = \\ln\\left(\\frac{ \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{m}{3\\sigma^2}\\right) }{ \\left(\\frac{1}{\\tau^2} + \\frac{m}{\\sigma^2}\\right) \\left(\\frac{1}{\\tau^2} + \\frac{m}{3\\sigma^2}\\left(1 - \\frac{1}{N^2}\\right)\\right) }\\right) = \\ln\\left( \\frac{\\frac{1}{\\tau^2} + \\frac{m}{3\\sigma^2}}{\\frac{1}{\\tau^2} + \\frac{m}{3\\sigma^2} - \\frac{m}{3\\sigma^2 N^2}} \\right)\n$$\nFinally, we evaluate this expression for the given numerical values: $m=10$, $\\tau^2=1$, $\\sigma^2=0.25$, and $N=5$.\nThe term $\\frac{1}{\\tau^2} = \\frac{1}{1} = 1$.\nThe term $\\frac{m}{3\\sigma^2} = \\frac{10}{3(0.25)} = \\frac{10}{0.75} = \\frac{40}{3}$.\n$N^2 = 5^2 = 25$.\nSubstituting these into the expression for $\\Delta(N)$:\n$$\n\\Delta(5) = \\ln\\left( \\frac{1 + \\frac{40}{3}}{1 + \\frac{40}{3} - \\frac{40}{3 \\cdot 25}} \\right) = \\ln\\left( \\frac{\\frac{43}{3}}{1 + \\frac{40}{3}\\left(1 - \\frac{1}{25}\\right)} \\right) = \\ln\\left( \\frac{\\frac{43}{3}}{1 + \\frac{40}{3}\\left(\\frac{24}{25}\\right)} \\right)\n$$\nThe denominator is:\n$$\n1 + \\frac{40 \\cdot 24}{3 \\cdot 25} = 1 + \\frac{960}{75} = 1 + \\frac{64}{5} = \\frac{69}{5}\n$$\nSo, the argument of the logarithm is:\n$$\n\\frac{43/3}{69/5} = \\frac{43}{3} \\cdot \\frac{5}{69} = \\frac{215}{207}\n$$\nThe final numerical value is:\n$$\n\\Delta(5) = \\ln\\left(\\frac{215}{207}\\right) \\approx 0.0379165\n$$\nRounding to four significant figures, the result is $0.03792$.",
            "answer": "$$\\boxed{0.03792}$$"
        }
    ]
}