{
    "hands_on_practices": [
        {
            "introduction": "Blind deconvolution is fundamentally ill-posed, meaning a single observation can correspond to multiple pairs of signal and kernel. This exercise  provides a direct, hands-on experience with this ambiguity. By first constructing multiple valid solutions under a non-negativity constraint, and then showing how additional realistic constraints restore uniqueness, you will gain a foundational understanding of the critical role that priors play in making this problem solvable.",
            "id": "3369032",
            "problem": "Consider the one-dimensional noiseless discrete blind deconvolution model on finite-support sequences, where the observation is given by $y = x * h$ with $x \\in \\mathbb{R}_{\\ge 0}^{2}$ and $h \\in \\mathbb{R}_{\\ge 0}^{2}$, and $*$ denotes discrete convolution. The convolution is defined by $y_{k} = \\sum_{i} x_{i} h_{k-i}$ with indices outside support treated as zero. Suppose the measured data is $y = [2, 5, 2]$, interpreted as the full linear convolution of two length-$2$ sequences $x = [x_{0}, x_{1}]$ and $h = [h_{0}, h_{1}]$. Use the fundamental facts that discrete convolution is bilinear and commutative, and that nonnegativity constraints are convex, to do the following: construct a counterfactual demonstrating that imposing only $x \\ge 0$ and $h \\ge 0$ admits multiple solutions consistent with $y$. Then, propose additional convex constraints on $h$ that are scientifically realistic for a Point Spread Function (PSF), such as the sum-to-one constraint $\\sum_{i} h_{i} = 1$ and a monotonicity constraint $h_{0} \\ge h_{1}$, and explain why these constraints are convex. Under these constraints, determine the unique $(x, h)$ pair that exactly reproduces $y$ via $y = x * h$. Express your final answer as a single row matrix containing $x_{0}$, $x_{1}$, $h_{0}$, $h_{1}$ in that order. No rounding is required and no units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in the established field of blind deconvolution, is well-posed upon the addition of standard constraints, and is expressed in objective, formal language. We may proceed to a solution.\n\nThe one-dimensional discrete convolution of a sequence $x = [x_{0}, x_{1}]$ and a sequence $h = [h_{0}, h_{1}]$ is given by $y = x * h$. The resulting sequence $y$ has length $2+2-1 = 3$. The components of $y$ are:\n$$y_{0} = x_{0}h_{0}$$\n$$y_{1} = x_{0}h_{1} + x_{1}h_{0}$$\n$$y_{2} = x_{1}h_{1}$$\nGiven the measured data $y = [2, 5, 2]$, we establish the following system of nonlinear equations:\n$$\n\\begin{cases}\nx_{0}h_{0} = 2 & (1) \\\\\nx_{0}h_{1} + x_{1}h_{0} = 5 & (2) \\\\\nx_{1}h_{1} = 2 & (3)\n\\end{cases}\n$$\nThe problem initially imposes only non-negativity constraints: $x_{0}, x_{1}, h_{0}, h_{1} \\ge 0$.\n\nFirst, we construct a counterfactual to demonstrate that these constraints alone are insufficient to guarantee a unique solution.\nFrom equations $(1)$ and $(3)$, we can see that $x_{0}h_{0} = x_{1}h_{1} = 2$. Since $y_{0}=2$ and $y_{2}=2$, none of $x_{0}, x_{1}, h_{0}, h_{1}$ can be zero. We can express $x_{0}$ and $x_{1}$ in terms of $h_{0}$ and $h_{1}$:\n$$x_{0} = \\frac{2}{h_{0}}$$\n$$x_{1} = \\frac{2}{h_{1}}$$\nSubstituting these into equation $(2)$:\n$$\\left(\\frac{2}{h_{0}}\\right)h_{1} + \\left(\\frac{2}{h_{1}}\\right)h_{0} = 5$$\nMultiplying by $h_{0}h_{1}$ (which is non-zero) yields:\n$$2h_{1}^{2} + 2h_{0}^{2} = 5h_{0}h_{1}$$\nDividing by $h_{1}^{2}$ (also non-zero) gives a quadratic equation for the ratio $r = h_{0}/h_{1}$:\n$$2\\left(\\frac{h_{0}}{h_{1}}\\right)^{2} - 5\\left(\\frac{h_{0}}{h_{1}}\\right) + 2 = 0$$\n$$2r^2 - 5r + 2 = 0$$\nFactoring this quadratic equation, we get $(2r-1)(r-2) = 0$. The two possible roots are $r=2$ and $r=1/2$. This implies two families of solutions for the kernel $h$.\n\nCase A: $h_{0}/h_{1} = 2$.\nLet us choose a scale for $h$. For instance, let $h_{1}=1$. Then $h_{0}=2$, so $h = [2, 1]$.\nThe corresponding signal $x$ is found as $x_{0} = 2/h_{0} = 2/2 = 1$ and $x_{1} = 2/h_{1} = 2/1 = 2$. So $x = [1, 2]$.\nThis gives a solution pair $(x, h) = ([1, 2], [2, 1])$. Both $x$ and $h$ satisfy the non-negativity constraint. A quick check verifies $x*h = [1, 2] * [2, 1] = [2, 1 \\cdot 1 + 2 \\cdot 2, 2] = [2, 5, 2]$.\n\nCase B: $h_{0}/h_{1} = 1/2$.\nLet us again choose a scale, say $h_{1}=2$. Then $h_{0}=1$, so $h=[1, 2]$.\nThe corresponding signal $x$ is $x_{0} = 2/h_{0} = 2/1 = 2$ and $x_{1} = 2/h_{1} = 2/2 = 1$. So $x = [2, 1]$.\nThis gives a second solution pair $(x, h) = ([2, 1], [1, 2])$. This also satisfies the non-negativity constraint and results in the same observation $y$.\n\nSince we have found at least two distinct solutions, $([1, 2], [2, 1])$ and $([2, 1], [1, 2])$, we have demonstrated that the problem is not unique under only non-negativity constraints.\n\nNext, we propose additional convex constraints on the blurring kernel $h$, which is often interpreted as a Point Spread Function (PSF). Scientifically realistic PSFs are typically normalized to conserve energy and are often peaked at their center.\n1.  **Sum-to-one constraint**: $\\sum_{i} h_{i} = h_{0} + h_{1} = 1$. This enforces energy conservation.\n2.  **Monotonicity constraint**: $h_{0} \\ge h_{1}$. This models a PSF that is centered or decaying from the origin.\n\nThese constraints are convex. A constraint defines a convex set if for any two points in the set, the line segment connecting them is also contained within the set.\n- For the constraint $h_{0} + h_{1} = 1$, let $h_{a}$ and $h_{b}$ be two vectors satisfying it. For any $\\lambda \\in [0, 1]$, the convex combination $h_{c} = \\lambda h_{a} + (1-\\lambda)h_{b}$ has a sum of components $\\sum (h_{c})_i = \\lambda \\sum (h_{a})_i + (1-\\lambda) \\sum (h_{b})_i = \\lambda(1) + (1-\\lambda)(1) = 1$. The set of points satisfying this is an affine subspace, which is convex.\n- For the constraint $h_{0} \\ge h_{1}$, or $h_{0} - h_{1} \\ge 0$, let $h_{a}$ and $h_{b}$ satisfy it. The convex combination $h_{c}$ has components $(h_{c})_{0} - (h_{c})_{1} = \\lambda((h_{a})_{0}-(h_{a})_{1}) + (1-\\lambda)((h_{b})_{0}-(h_{b})_{1})$. Since $\\lambda \\ge 0$, $1-\\lambda \\ge 0$, $(h_{a})_{0}-(h_{a})_{1} \\ge 0$, and $(h_{b})_{0}-(h_{b})_{1} \\ge 0$, the result is non-negative. This confirms $(h_{c})_{0} \\ge (h_{c})_{1}$. This constraint defines a closed half-space, which is a convex set.\n\nThe intersection of these convex sets with the non-negative orthant ($h_{0} \\ge 0$, $h_{1} \\ge 0$) defines a convex feasible set for $h$. Now, we apply these new constraints to find the unique solution.\n\nWe revisit the two cases derived from the algebraic system.\nCase A: $h_{0} = 2h_{1}$.\nApplying the sum-to-one constraint: $2h_{1} + h_{1} = 1 \\implies 3h_{1} = 1 \\implies h_{1} = 1/3$.\nThis gives $h_{0} = 2/3$. The candidate kernel is $h = [2/3, 1/3]$.\nLet's check all constraints on $h$:\n- $h_{0} = 2/3 \\ge 0$, $h_{1} = 1/3 \\ge 0$ (Non-negativity: satisfied).\n- $h_{0} + h_{1} = 2/3 + 1/3 = 1$ (Sum-to-one: satisfied).\n- $h_{0} \\ge h_{1} \\implies 2/3 \\ge 1/3$ (Monotonicity: satisfied).\nThis case is valid. We find the corresponding signal $x$:\n$x_{0} = 2/h_{0} = 2/(2/3) = 3$.\n$x_{1} = 2/h_{1} = 2/(1/3) = 6$.\nThe signal is $x = [3, 6]$. The non-negativity constraints $x_{0} \\ge 0$ and $x_{1} \\ge 0$ are satisfied.\nThus, $(x, h) = ([3, 6], [2/3, 1/3])$ is a valid solution.\n\nCase B: $h_{0} = h_{1}/2$.\nApplying the sum-to-one constraint: $h_{1}/2 + h_{1} = 1 \\implies (3/2)h_{1} = 1 \\implies h_{1} = 2/3$.\nThis gives $h_{0} = (1/2)(2/3) = 1/3$. The candidate kernel is $h = [1/3, 2/3]$.\nLet's check the monotonicity constraint: $h_{0} \\ge h_{1} \\implies 1/3 \\ge 2/3$. This is false.\nTherefore, this case is ruled out by the imposed constraints.\n\nThe only solution satisfying all equations and constraints is the one found in Case A. The unique solution is $x_{0}=3$, $x_{1}=6$, $h_{0}=2/3$, and $h_{1}=1/3$.\nThe final answer is required as a single row matrix containing $x_{0}, x_{1}, h_{0}, h_{1}$ in order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 6 & \\frac{2}{3} & \\frac{1}{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the previous exercise showed how constraints on the kernel can resolve ambiguity, this practice  explores an alternative path to identifiability based on the signal's structure. You will investigate how assuming the signal is sparse—composed of a few isolated spikes—leads to a rigorous condition for unique recovery. Deriving this condition and constructing a counterexample will provide deep insight into the theoretical underpinnings of super-resolution and sparse blind deconvolution.",
            "id": "3369057",
            "problem": "Consider discrete-time, finite-length real sequences with linear convolution under zero-padding. Let $n \\in \\mathbb{N}$ be large enough that boundary effects can be neglected for the analysis below. You observe \n$$\ny = h * x + \\eta,\n$$\nwhere $h \\in \\mathbb{R}^{n}$ is an unknown blur with compact support, $x \\in \\mathbb{R}^{n}$ is an unknown $k$-sparse signal, and $\\eta \\in \\mathbb{R}^{n}$ is additive noise. Assume noiseless data for identifiability analysis, i.e., $\\eta = 0$. Impose a gauge to remove trivial scaling and shift ambiguities by requiring \n$$\n\\operatorname{supp}(h) \\subseteq \\{0,1,\\dots,s-1\\} \\quad \\text{and} \\quad h[0] = 1,\n$$\nfor a known support size $s \\in \\mathbb{N}$ with $s \\geq 2$. The signal $x$ is $k$-sparse with support set $T \\subset \\{0,1,\\dots,n-1\\}$ and minimum separation \n$$\n\\Delta = \\min_{\\substack{t,t' \\in T\\\\ t \\neq t'}} |t - t'|,\n$$\nso that any two nonzero entries of $x$ are at least $\\Delta$ indices apart.\n\nStarting only from the definition of linear convolution, the notion of support, and the above gauge, analyze when $(h,x)$ is identifiable from $y$ in the sense that the forward map $(h,x) \\mapsto y$ is injective on the model class. In particular:\n- Derive, from first principles, a threshold $\\Delta_{\\star} = \\Delta_{\\star}(s)$ such that if $\\Delta \\geq \\Delta_{\\star}$ then $(h,x)$ is uniquely determined by $y$ under the stated gauge.\n- Justify minimality of your threshold by constructing an explicit counterexample that breaks identifiability when the separation condition is violated. Concretely, for the case $s=2$, $k=2$, and $\\Delta=1$, construct two distinct pairs $(h,x)$ and $(\\tilde h,\\tilde x)$ satisfying the same gauge that produce the same $y$ but with $(h,x) \\neq (\\tilde h,\\tilde x)$.\n\nReport your threshold $\\Delta_{\\star}$ as a closed-form analytic expression in $s$. Your final answer must be a single analytic expression. No rounding is required, and no units apply.",
            "solution": "The problem asks for the condition on the minimum separation $\\Delta$ of a sparse signal $x$ that guarantees unique identifiability of a pair $(h,x)$ from its convolution $y=h*x$, given a specific gauge on the filter $h$. We will first derive a sufficient condition for identifiability and then demonstrate its necessity (and thus minimality) by constructing a counterexample where identifiability fails.\n\nLet the problem be defined for discrete-time sequences of length $n$. The observed signal is $y = h * x$, where $*$ denotes linear convolution. The sequences $h \\in \\mathbb{R}^{n}$ and $x \\in \\mathbb{R}^{n}$ are unknown.\n\nThe constraints on $(h,x)$ are as follows:\n1.  The filter $h$ has support size at most $s$, with $\\operatorname{supp}(h) \\subseteq \\{0, 1, \\dots, s-1\\}$ for a known integer $s \\geq 2$.\n2.  A gauge is imposed on $h$: $h[0]=1$.\n3.  The signal $x$ is $k$-sparse, meaning it has exactly $k$ non-zero entries. Let its support be $T = \\{t_1, t_2, \\dots, t_k\\}$, where $0 \\leq t_1 < t_2 < \\dots < t_k \\leq n-1$.\n4.  The minimum separation between non-zero entries of $x$ is $\\Delta = \\min_{i \\neq j} |t_i - t_j|$.\n\nIdentifiability means that the forward map $(h,x) \\mapsto y$ is injective on the set of admissible pairs. Suppose we have two distinct pairs, $(h,x)$ and $(\\tilde{h},\\tilde{x})$, both satisfying the given constraints, that produce the same output:\n$$\nh * x = \\tilde{h} * \\tilde{x}\n$$\nOur goal is to find the condition on $\\Delta$ that forces $(h,x) = (\\tilde{h},\\tilde{x})$.\n\nLet us analyze the structure of the output $y$. By definition of convolution,\n$$\ny[j] = (h * x)[j] = \\sum_{m=0}^{n-1} h[m] x[j-m]\n$$\nSince $x$ is non-zero only on its support $T=\\{t_1, \\dots, t_k\\}$, we can write $x$ as a sum of scaled Dirac impulses: $x = \\sum_{i=1}^{k} x_i \\delta_{t_i}$, where $x_i = x[t_i]$. The convolution is then:\n$$\ny = h * \\left( \\sum_{i=1}^{k} x_i \\delta_{t_i} \\right) = \\sum_{i=1}^{k} x_i (h * \\delta_{t_i}) = \\sum_{i=1}^{k} x_i h(\\cdot - t_i)\n$$\nThis shows that $y$ is a linear combination of $k$ copies of the filter $h$, each shifted by a corresponding location $t_i \\in T$ and scaled by the amplitude $x_i$.\n\nThe support of the $i$-th shifted copy, $h(\\cdot - t_i)$, is $\\{t_i, t_i+1, \\dots, t_i+s-1\\}$, an interval of length $s$. The support of $y$ is the union of these intervals: $\\operatorname{supp}(y) \\subseteq \\bigcup_{i=1}^k [t_i, t_i+s-1]$.\n\nNow, let us impose a condition on the minimum separation $\\Delta$. Let us assume that $\\Delta \\geq s$.\nThe separation between the start of the support of the $i$-th copy and the $(i+1)$-th copy is $t_{i+1} - t_i \\geq \\Delta$.\nIf $\\Delta \\geq s$, then $t_{i+1} - t_i \\geq s$.\nThe support of the $i$-th copy ends at $t_i+s-1$.\nThe support of the $(i+1)$-th copy starts at $t_{i+1}$.\nThe condition $t_{i+1} \\geq t_i+s$ implies that $t_{i+1} > t_i+s-1$.\nThis means that the support interval $[t_i, t_i+s-1]$ and $[t_{i+1}, t_{i+1}+s-1]$ are disjoint for all $i$.\n\nUnder the condition $\\Delta \\geq s$, the output $y$ is a sequence of $k$ disjoint blocks. For any index $j$ in the first block, $j \\in [t_1, t_1+s-1]$, the only non-zero contribution to the sum for $y[j]$ comes from the first term, $i=1$:\n$$\ny[j] = x_1 h[j-t_1] \\quad \\text{for } j \\in \\{t_1, t_1+1, \\dots, t_1+s-1\\}\n$$\nAll other terms $x_i h[j-t_i]$ for $i > 1$ are zero because $j-t_i < (t_1+s)-t_i \\leq (t_1+s)-t_2 \\leq (t_1+s)-(t_1+s) = 0$, so $j-t_i$ is outside the support of $h$.\n\nNow, consider the equation $h*x = \\tilde{h} * \\tilde{x}$. Let $T=\\{t_i\\}$ and $\\tilde{T}=\\{\\tilde{t}_i\\}$ be the supports of $x$ and $\\tilde{x}$ respectively. Both are assumed to satisfy the separation condition $\\Delta \\geq s$. The support of $y=h*x$ is the disjoint union of intervals $\\bigcup_{i=1}^k [t_i, t_i+s-1]$. Similarly, the support of $y=\\tilde{h}*\\tilde{x}$ is the disjoint union $\\bigcup_{i=1}^k [\\tilde{t}_i, \\tilde{t}_i+s-1]$. For these to be equal, the sets of intervals must be identical. This implies $t_i = \\tilde{t}_i$ for all $i=1, \\dots, k$. So, the spike locations are identifiable, i.e., $T = \\tilde{T}$.\n\nNow, let's examine the values. On the first interval $[t_1, t_1+s-1]$:\n$$\ny[j] = x_1 h[j-t_1] \\quad \\text{and} \\quad y[j] = \\tilde{x}_1 \\tilde{h}[j-t_1]\n$$\nAt the first point $j=t_1$, we have:\n$$\ny[t_1] = x_1 h[0] \\quad \\text{and} \\quad y[t_1] = \\tilde{x}_1 \\tilde{h}[0]\n$$\nUsing the gauge $h[0]=1$ and $\\tilde{h}[0]=1$, we find $y[t_1] = x_1$ and $y[t_1] = \\tilde{x}_1$. This implies $x_1 = \\tilde{x}_1$. Since $x_1$ must be non-zero, we can divide by it.\nFor any $j \\in [t_1, t_1+s-1]$, we have $x_1 h[j-t_1] = x_1 \\tilde{h}[j-t_1]$, which implies $h[j-t_1] = \\tilde{h}[j-t_1]$. Letting $l=j-t_1$, this means $h[l] = \\tilde{h}[l]$ for all $l \\in \\{0, 1, \\dots, s-1\\}$. This proves that $h=\\tilde{h}$.\n\nHaving established $h=\\tilde{h}$ and $T=\\tilde{T}$, the original equation becomes $h*x = h*\\tilde{x}$, or $h*(x-\\tilde{x})=0$. In the Z-transform domain, this is $H(z) (X(z) - \\tilde{X}(z)) = 0$. The Z-transform of $h$ is $H(z) = \\sum_{l=0}^{s-1} h[l]z^{-l}$, which is a polynomial in $z^{-1}$. Since $h[0]=1$, $H(z)$ is not the zero polynomial. The ring of polynomials is an integral domain, so if a product is zero, one of the factors must be zero. Thus, $X(z) - \\tilde{X}(z) = 0$, which implies $X(z) = \\tilde{X}(z)$ and consequently $x = \\tilde{x}$.\n\nTherefore, if $\\Delta \\geq s$, the pair $(h,x)$ is uniquely identifiable. This establishes that $\\Delta \\geq s$ is a sufficient condition.\n\nNext, we must prove this threshold is minimal. We need to show that if $\\Delta < s$, identifiability may fail. It suffices to construct a single counterexample. Let us choose $\\Delta = s-1$. The problem specifically requests a counterexample for $s=2$, $k=2$, and $\\Delta=1$. Here, $\\Delta = 1 = 2-1 = s-1$, which violates the sufficient condition $\\Delta \\geq s$.\n\nLet $(h,x)$ and $(\\tilde{h},\\tilde{x})$ be two distinct pairs.\nFor $s=2$, the filter $h$ has support in $\\{0,1\\}$. Gauge gives $h=[1, h_1]$.\nFor $k=2, \\Delta=1$, the signal $x$ has two adjacent non-zero entries. Without loss of generality, let the support be $T=\\{0,1\\}$. So $x$ has non-zero entries $x_0 = x[0]$ and $x_1 = x[1]$.\n\nWe require $h*x = \\tilde{h}*\\tilde{x}$. Let's analyze this in the polynomial domain (equivalent to Z-transform for finite signals).\nLet $H(z) = 1 + h_1 z^{-1}$ and $X(z) = x_0 + x_1 z^{-1}$.\nThe output is $Y(z) = H(z)X(z) = (1+h_1 z^{-1})(x_0+x_1 z^{-1})$.\n\nWe seek a different factorization $Y(z) = \\tilde{H}(z)\\tilde{X}(z)$ where $\\tilde{H}(z)$ and $\\tilde{X}(z)$ correspond to a valid pair $(\\tilde{h},\\tilde{x})$.\n$Y(z) = x_0 + (x_1+h_1 x_0) z^{-1} + h_1 x_1 z^{-2}$.\nA known ambiguity in polynomial products is to exchange factors. We can write $X(z)$ as $x_0(1 + \\frac{x_1}{x_0} z^{-1})$. Then $Y(z) = x_0 (1+h_1 z^{-1})(1+\\frac{x_1}{x_0} z^{-1})$.\nLet us define a new pair by swapping the polynomial factors:\n$$\n\\tilde{H}(z) = 1 + \\frac{x_1}{x_0} z^{-1}\n$$\n$$\n\\tilde{X}(z) = x_0(1 + h_1 z^{-1}) = x_0 + x_0 h_1 z^{-1}\n$$\nBy construction, $\\tilde{H}(z)\\tilde{X}(z) = Y(z)$. Now we check if $(\\tilde{h},\\tilde{x})$ is a valid and distinct pair.\n\nFrom $\\tilde{H}(z)$, we get $\\tilde{h} = [1, x_1/x_0]$. This satisfies the gauge $\\tilde{h}[0]=1$ and $\\operatorname{supp}(\\tilde{h}) \\subseteq \\{0,1\\}$, which is valid for $s=2$.\nFrom $\\tilde{X}(z)$, we get $\\tilde{x}$ with non-zero entries $\\tilde{x}_0 = x_0$ and $\\tilde{x}_1 = x_0 h_1$ at locations $\\{0,1\\}$. This is a $2$-sparse signal with separation $\\Delta=1$, valid for $k=2, \\Delta=1$.\n\nThe pair $(\\tilde{h},\\tilde{x})$ is distinct from $(h,x)$ if $h \\neq \\tilde{h}$ or $x \\neq \\tilde{x}$.\n$h = \\tilde{h} \\implies [1, h_1] = [1, x_1/x_0] \\implies h_1 = x_1/x_0$.\n$x = \\tilde{x} \\implies$ entries $(x_0, x_1)$ equal to $(\\tilde{x}_0, \\tilde{x}_1) = (x_0, x_0 h_1) \\implies x_1 = x_0 h_1$.\nIf we choose $h_1 \\neq x_1/x_0$, the pairs will be distinct.\n\nLet's construct a numerical example.\nLet $h=[1, 2]$, so $h_1=2$. This is a valid filter for $s=2$.\nLet $x$ have support at $\\{0,1\\}$ with values $x_0=3$ and $x_1=3$. So $(h,x) = ([1,2], [3,3,0,\\dots])$. Here $k=2, \\Delta=1$. And $h_1 = 2 \\neq x_1/x_0 = 3/3 = 1$. The pairs will be distinct.\n\nOriginal pair: $(h,x)$\n- $h = [1, 2]$\n- $x$ has entries $(x_0, x_1) = (3, 3)$\nThe convolution $y=h*x$ gives $[1,2]*[3,3] = [3, 3+6, 6] = [3, 9, 6]$.\n\nConstructed pair: $(\\tilde{h},\\tilde{x})$\n- $\\tilde{h} = [1, x_1/x_0] = [1, 3/3] = [1, 1]$\n- $\\tilde{x}$ has entries $(\\tilde{x}_0, \\tilde{x}_1) = (x_0, x_0 h_1) = (3, 3 \\cdot 2) = (3, 6)$\nThis pair is valid: $\\tilde{h}[0]=1$, $\\operatorname{supp}(\\tilde{h}) \\subseteq \\{0,1\\}$; $\\tilde{x}$ is $2$-sparse with $\\Delta=1$.\nThe pair $([1,2], [3,3,\\dots])$ is clearly distinct from $([1,1], [3,6,\\dots])$.\nThe convolution $\\tilde{y}=\\tilde{h}*\\tilde{x}$ gives $[1,1]*[3,6] = [3, 6+3, 6] = [3, 9, 6]$.\n\nSince $y=\\tilde{y}$, we have found two distinct pairs satisfying the problem constraints that produce the same output. This demonstrates non-identifiability for $\\Delta = 1$ and $s=2$. This logic generalizes to show that for any $s \\geq 2$, if $\\Delta \\leq s-1$, a similar polynomial factor-swapping ambiguity can be constructed.\n\nThus, the condition $\\Delta \\geq s$ is both sufficient and necessary. The minimum threshold $\\Delta_\\star$ such that $\\Delta \\geq \\Delta_\\star$ guarantees identifiability is $\\Delta_\\star = s$.",
            "answer": "$$\\boxed{s}$$"
        },
        {
            "introduction": "Having established conditions under which blind deconvolution is well-posed, we now turn to the practical question of how to compute the solution from noisy data. This exercise  guides you through the derivation of a complete algorithm using a Bayesian framework. By formulating the problem probabilistically and applying the mean-field variational approximation, you will derive the iterative update rules that form the core of a powerful and flexible estimation technique.",
            "id": "3369047",
            "problem": "Consider a one-dimensional discrete blind deconvolution model with an unknown signal $x \\in \\mathbb{R}^{n}$ and an unknown blur kernel $k \\in \\mathbb{R}^{m}$. Let the observation $y \\in \\mathbb{R}^{n}$ be generated by linear convolution of $x$ and $k$ under additive Gaussian noise as\n$$\ny = k * x + \\varepsilon,\n$$\nwhere $*$ denotes linear convolution with zero-padding to length $n$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ with known noise variance $\\sigma^{2} > 0$. Equip $x$ and $k$ with independent Gaussian priors,\n$$\nx \\sim \\mathcal{N}(0, \\alpha_{x}^{-1} I_{n}), \\quad k \\sim \\mathcal{N}(0, \\alpha_{k}^{-1} I_{m}),\n$$\nwhere $\\alpha_{x} > 0$ and $\\alpha_{k} > 0$ are known prior precisions. Let $K(k) \\in \\mathbb{R}^{n \\times n}$ be the Toeplitz (or circulant, consistent with the chosen boundary conditions) convolution matrix such that $K(k) x = k * x$, and let $X(x) \\in \\mathbb{R}^{n \\times m}$ be the convolution design matrix such that $X(x) k = k * x$. Both $K(k)$ and $X(x)$ depend linearly on their arguments. Define linear operator bases $\\{B_{j}\\}_{j=1}^{m}$ and $\\{A_{i}\\}_{i=1}^{n}$ such that\n$$\nK(k) = \\sum_{j=1}^{m} k_{j} B_{j}, \\quad X(x) = \\sum_{i=1}^{n} x_{i} A_{i},\n$$\nwhere $B_{j} \\in \\mathbb{R}^{n \\times n}$ maps $x$ to the convolution $e_{j} * x$ with the unit kernel $e_{j} \\in \\mathbb{R}^{m}$, and $A_{i} \\in \\mathbb{R}^{n \\times m}$ maps $k$ to the convolution $k * e_{i}$ with the unit signal $e_{i} \\in \\mathbb{R}^{n}$.\n\nUsing Variational Bayes (VB), specifically the mean-field approximation $q(x) q(k)$ to the posterior $p(x, k \\mid y)$, propose Gaussian factors\n$$\nq(x) = \\mathcal{N}(m_{x}, S_{x}), \\quad q(k) = \\mathcal{N}(m_{k}, S_{k}),\n$$\nand derive the coordinate ascent updates for the mean vectors $m_{x} \\in \\mathbb{R}^{n}$, $m_{k} \\in \\mathbb{R}^{m}$ and covariance matrices $S_{x} \\in \\mathbb{R}^{n \\times n}$, $S_{k} \\in \\mathbb{R}^{m \\times m}$ in closed form, expressed in terms of $y$, $\\sigma^{2}$, $\\alpha_{x}$, $\\alpha_{k}$, and the operator bases $\\{B_{j}\\}$ and $\\{A_{i}\\}$. Your final expressions must be written using expectations of second moments under the current variational factors. Do not assume diagonal covariances.\n\nExpress your final answer as analytic expressions for the four updated quantities $S_{x}$, $m_{x}$, $S_{k}$, $m_{k}$. No numerical evaluation is required.",
            "solution": "We begin by establishing the probabilistic model. The joint probability distribution of all variables is given by the product of the likelihood and the priors:\n$p(y, x, k) = p(y | x, k) p(x) p(k)$.\nGiven the problem statement, the factors are:\n$p(y | x, k) = \\mathcal{N}(y | k*x, \\sigma^2 I_n) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\|y - k*x\\|_2^2\\right)$\n$p(x) = \\mathcal{N}(x | 0, \\alpha_x^{-1} I_n) \\propto \\exp\\left(-\\frac{\\alpha_x}{2} \\|x\\|_2^2\\right)$\n$p(k) = \\mathcal{N}(k | 0, \\alpha_k^{-1} I_m) \\propto \\exp\\left(-\\frac{\\alpha_k}{2} \\|k\\|_2^2\\right)$\nThe log of the joint distribution, ignoring constants, is:\n$\\ln p(y, x, k) = -\\frac{1}{2\\sigma^2} \\|y - k*x\\|_2^2 - \\frac{\\alpha_x}{2} \\|x\\|_2^2 - \\frac{\\alpha_k}{2} \\|k\\|_2^2 + C$.\nIn the Variational Bayes framework with a mean-field approximation $q(x,k) = q(x)q(k)$, the optimal updates for the factors $q(x)$ and $q(k)$ are found via coordinate ascent. The general update rule for a factor $q_i(z_i)$ is given by:\n$\\ln q_i^*(z_i) = \\mathbb{E}_{j \\neq i}[\\ln p(y, x, k)] + \\text{const}_{z_i}$,\nwhere the expectation is taken over all other variables with respect to their current variational distributions.\n\nDerivation of the update for $q(x) = \\mathcal{N}(m_x, S_x)$:\nThe optimal distribution $q^*(x)$ is given by:\n$\\ln q^*(x) = \\mathbb{E}_{q(k)}[\\ln p(y, x, k)] + \\text{const}_x$\n$\\ln q^*(x) = \\mathbb{E}_{q(k)}[\\ln p(y|x,k) + \\ln p(x) + \\ln p(k)] + \\text{const}_x$\nSince $\\ln p(k)$ does not depend on $x$, we have:\n$\\ln q^*(x) = \\mathbb{E}_{q(k)}[-\\frac{1}{2\\sigma^2} \\|y - K(k)x\\|_2^2] - \\frac{\\alpha_x}{2} \\|x\\|_2^2 + \\text{const}_x$\nWe expand the quadratic term within the expectation:\n$\\|y - K(k)x\\|_2^2 = (y - K(k)x)^T(y - K(k)x) = y^T y - 2y^T K(k)x + x^T K(k)^T K(k)x$.\nTaking the expectation with respect to $q(k)$:\n$\\mathbb{E}_{q(k)}[\\|y - K(k)x\\|_2^2] = y^T y - 2y^T \\mathbb{E}_{q(k)}[K(k)]x + x^T \\mathbb{E}_{q(k)}[K(k)^T K(k)]x$.\nSubstituting this back into the expression for $\\ln q^*(x)$ and dropping terms not dependent on $x$:\n$\\ln q^*(x) \\propto -\\frac{1}{2\\sigma^2} \\left( -2y^T \\mathbb{E}_{q(k)}[K(k)]x + x^T \\mathbb{E}_{q(k)}[K(k)^T K(k)]x \\right) - \\frac{\\alpha_x}{2} x^T x$\n$\\ln q^*(x) \\propto x^T \\left(\\frac{1}{\\sigma^2} \\mathbb{E}_{q(k)}[K(k)]^T y \\right) - \\frac{1}{2} x^T \\left(\\frac{1}{\\sigma^2} \\mathbb{E}_{q(k)}[K(k)^T K(k)] + \\alpha_x I_n \\right) x$.\nThis expression is a quadratic form in $x$, which indicates that $q^*(x)$ is a Gaussian distribution. By comparing this to the log-density of a multivariate Gaussian $\\mathcal{N}(x|m_x, S_x)$, which is proportional to $-\\frac{1}{2}(x-m_x)^T S_x^{-1} (x-m_x) = -\\frac{1}{2}x^T S_x^{-1} x + x^T S_x^{-1}m_x + \\text{const}$, we can identify the precision matrix $S_x^{-1}$ and the mean $m_x$.\nThe precision matrix is:\n$S_x^{-1} = \\frac{1}{\\sigma^2} \\mathbb{E}_{q(k)}[K(k)^T K(k)] + \\alpha_x I_n$.\nAnd the mean is determined by:\n$S_x^{-1} m_x = \\frac{1}{\\sigma^2} \\mathbb{E}_{q(k)}[K(k)]^T y$.\nThis yields the update equations for the parameters of $q(x)$:\n$S_x = \\left(\\frac{1}{\\sigma^2}\\mathbb{E}_{q(k)}[K(k)^T K(k)] + \\alpha_x I_n\\right)^{-1}$\n$m_x = S_x \\left(\\frac{1}{\\sigma^2} \\mathbb{E}_{q(k)}[K(k)]^T y\\right)$.\n\nDerivation of the update for $q(k) = \\mathcal{N}(m_k, S_k)$:\nThe derivation is symmetric to the one for $q(x)$. We use the alternative representation of the convolution $k*x = X(x)k$.\n$\\ln q^*(k) = \\mathbb{E}_{q(x)}[\\ln p(y, x, k)] + \\text{const}_k$\n$\\ln q^*(k) = \\mathbb{E}_{q(x)}[-\\frac{1}{2\\sigma^2} \\|y - X(x)k\\|_2^2] - \\frac{\\alpha_k}{2} \\|k\\|_2^2 + \\text{const}_k$.\nFollowing the same procedure of expanding the quadratic term, taking the expectation with respect to $q(x)$, and comparing to the log-density of a Gaussian $\\mathcal{N}(k|m_k, S_k)$, we obtain:\n$S_k^{-1} = \\frac{1}{\\sigma^2} \\mathbb{E}_{q(x)}[X(x)^T X(x)] + \\alpha_k I_m$\n$S_k^{-1} m_k = \\frac{1}{\\sigma^2} \\mathbb{E}_{q(x)}[X(x)]^T y$.\nThis gives the update equations for the parameters of $q(k)$:\n$S_k = \\left(\\frac{1}{\\sigma^2}\\mathbb{E}_{q(x)}[X(x)^T X(x)] + \\alpha_k I_m\\right)^{-1}$\n$m_k = S_k \\left(\\frac{1}{\\sigma^2} \\mathbb{E}_{q(x)}[X(x)]^T y\\right)$.\n\nExpressing expectations using operator bases:\nThe final step is to express the required expectations in terms of the operator bases $\\{A_i\\}$ and $\\{B_j\\}$ and the parameters of the current variational distributions.\nFor the $q(x)$ update, we need expectations over $q(k) = \\mathcal{N}(k|m_k, S_k)$.\n$\\mathbb{E}_{q(k)}[K(k)] = \\mathbb{E}_{q(k)}\\left[\\sum_{j=1}^{m} k_j B_j\\right] = \\sum_{j=1}^{m} \\mathbb{E}_{q(k)}[k_j] B_j$.\n$\\mathbb{E}_{q(k)}[K(k)^T K(k)] = \\mathbb{E}_{q(k)}\\left[\\left(\\sum_{j=1}^{m} k_j B_j\\right)^T \\left(\\sum_{l=1}^{m} k_l B_l\\right)\\right] = \\sum_{j=1}^{m}\\sum_{l=1}^{m} \\mathbb{E}_{q(k)}[k_j k_l] B_j^T B_l$.\nHere, $\\mathbb{E}_{q(k)}[k_j]$ is the $j$-th component of the mean vector $m_k$, and $\\mathbb{E}_{q(k)}[k_j k_l]$ is the $(j,l)$-th element of the second moment matrix $\\mathbb{E}_{q(k)}[kk^T] = S_k + m_k m_k^T$.\n\nFor the $q(k)$ update, we need expectations over $q(x) = \\mathcal{N}(x|m_x, S_x)$.\n$\\mathbb{E}_{q(x)}[X(x)] = \\mathbb{E}_{q(x)}\\left[\\sum_{i=1}^{n} x_i A_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}_{q(x)}[x_i] A_i$.\n$\\mathbb{E}_{q(x)}[X(x)^T X(x)] = \\mathbb{E}_{q(x)}\\left[\\left(\\sum_{i=1}^{n} x_i A_i\\right)^T \\left(\\sum_{p=1}^{n} x_p A_p\\right)\\right] = \\sum_{i=1}^{n}\\sum_{p=1}^{n} \\mathbb{E}_{q(x)}[x_i x_p] A_i^T A_p$.\nHere, $\\mathbb{E}_{q(x)}[x_i]$ is the $i$-th component of the mean vector $m_x$, and $\\mathbb{E}_{q(x)}[x_i x_p]$ is the $(i,p)$-th element of the second moment matrix $\\mathbb{E}_{q(x)}[xx^T] = S_x + m_x m_x^T$.\n\nSubstituting these into the update rules yields the final closed-form expressions as required.\nThe update for $S_x$ is:\n$S_x = \\left(\\frac{1}{\\sigma^2} \\sum_{j=1}^{m}\\sum_{l=1}^{m} \\mathbb{E}_{q(k)}[k_j k_l] B_j^T B_l + \\alpha_x I_n\\right)^{-1}$.\nThe update for $m_x$ is:\n$m_x = S_x \\left(\\frac{1}{\\sigma^2} \\left(\\sum_{j=1}^{m}\\mathbb{E}_{q(k)}[k_j]B_j\\right)^T y\\right)$.\nThe update for $S_k$ is:\n$S_k = \\left(\\frac{1}{\\sigma^2} \\sum_{i=1}^{n}\\sum_{p=1}^{n} \\mathbb{E}_{q(x)}[x_i x_p] A_i^T A_p + \\alpha_k I_m\\right)^{-1}$.\nThe update for $m_k$ is:\n$m_k = S_k \\left(\\frac{1}{\\sigma^2} \\left(\\sum_{i=1}^{n}\\mathbb{E}_{q(x)}[x_i]A_i\\right)^T y\\right)$.\nThese expressions fulfill all conditions of the problem statement. The expectations $\\mathbb{E}_{q(k)}[\\cdot]$ and $\\mathbb{E}_{q(x)}[\\cdot]$ are computed using the parameters of the variational distributions from the previous step of the coordinate ascent algorithm.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_{x} = \\left(\\frac{1}{\\sigma^{2}} \\sum_{j=1}^{m} \\sum_{l=1}^{m} \\mathbb{E}_{q(k)}[k_{j}k_{l}] B_{j}^T B_{l} + \\alpha_{x} I_{n}\\right)^{-1} &\nm_{x} = S_{x} \\left(\\frac{1}{\\sigma^{2}} \\left(\\sum_{j=1}^{m} \\mathbb{E}_{q(k)}[k_{j}] B_{j}\\right)^{T} y\\right) &\nS_{k} = \\left(\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} \\sum_{p=1}^{n} \\mathbb{E}_{q(x)}[x_{i}x_{p}] A_{i}^T A_{p} + \\alpha_{k} I_{m}\\right)^{-1} &\nm_{k} = S_{k} \\left(\\frac{1}{\\sigma^{2}} \\left(\\sum_{i=1}^{n} \\mathbb{E}_{q(x)}[x_{i}] A_{i}\\right)^{T} y\\right)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}