{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in diagnostics is to establish the expected statistical properties of residuals when the data assimilation system is optimal. This exercise guides you through the derivation of one of the most important such properties, the Desroziers diagnostic . This powerful identity reveals that the theoretical cross-covariance between the innovation vector and the analysis residual is exactly equal to the observation error covariance matrix, $R$, providing a data-driven method for tuning and validating this critical parameter.",
            "id": "3390983",
            "problem": "Consider a linear-Gaussian data assimilation setting in which a true state $x \\in \\mathbb{R}^{n}$ is observed through a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ with observation errors $v \\in \\mathbb{R}^{m}$. The observation equation is $y = H x + v$, where $v$ is unbiased and has covariance $R \\in \\mathbb{R}^{m \\times m}$. A forecast $x_{f} \\in \\mathbb{R}^{n}$ is available with forecast error $e_{f} = x - x_{f}$ that is unbiased, has covariance $P_{f} \\in \\mathbb{R}^{n \\times n}$, and is statistically independent of $v$. The analysis is formed by the linear update $x_{a} = x_{f} + K d$, where $K \\in \\mathbb{R}^{n \\times m}$ is a gain matrix and the innovation $d \\in \\mathbb{R}^{m}$ and analysis residual $r_{a} \\in \\mathbb{R}^{m}$ are defined by $d = y - H x_{f}$ and $r_{a} = y - H x_{a}$, respectively. Assume the gain $K$ is chosen according to the optimal linear estimator that minimizes the analysis error covariance under the linear-Gaussian assumptions of the Kalman Filter (KF) and that $e_{f}$ and $v$ are independent and unbiased.\n\nStarting from the definitions above and using only the standard properties of unbiased linear estimators and covariances under the linear-Gaussian KF assumptions, derive a closed-form expression for the cross-covariance $E[d r_{a}^{T}]$ in terms of the problem covariances, and obtain its exact value in analytic form. Then, explain concisely how this identity motivates empirical estimation of the observation error covariance $R$ from sample cross-covariances computed from data pairs $\\{(d_{i}, r_{a,i})\\}_{i=1}^{N}$ produced over $N$ assimilation cycles.\n\nYour final answer must be the single closed-form analytic expression for $E[d r_{a}^{T}]$ specified above. No numerical evaluation is required. If you choose to propose an empirical estimator within your explanation, do not include it in the final answer.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- True state: $x \\in \\mathbb{R}^{n}$\n- Observation operator: $H \\in \\mathbb{R}^{m \\times n}$\n- Observation error: $v \\in \\mathbb{R}^{m}$, with $E[v] = 0$ and covariance $E[v v^T] = R \\in \\mathbb{R}^{m \\times m}$\n- Observation equation: $y = H x + v$\n- Forecast: $x_{f} \\in \\mathbb{R}^{n}$\n- Forecast error: $e_{f} = x - x_{f}$, with $E[e_{f}] = 0$ and covariance $E[e_{f} e_{f}^T] = P_{f} \\in \\mathbb{R}^{n \\times n}$\n- Independence: $e_{f}$ and $v$ are statistically independent, implying $E[e_{f} v^T] = 0$\n- Analysis update equation: $x_{a} = x_{f} + K d$\n- Gain matrix: $K \\in \\mathbb{R}^{n \\times m}$\n- Innovation: $d = y - H x_{f}$\n- Analysis residual: $r_{a} = y - H x_{a}$\n- Key assumption: The gain $K$ is the optimal Kalman Filter gain that minimizes the analysis error covariance.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is set in the standard theoretical framework of linear-Gaussian data assimilation, a cornerstone of modern estimation theory and its applications in physical sciences and engineering. All terms are standard and well-defined. The assumptions (unbiased errors, independence, optimal gain) are typical for deriving foundational results in this field. The problem is self-contained, with all necessary information provided. It is factually and scientifically sound, and its objective is to derive a known, non-trivial identity in residual diagnostics. There are no contradictions, ambiguities, or reliance on pseudoscience.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous derivation can proceed based on the provided givens and standard properties of covariance and linear estimators.\n\n### Derivation\nOur goal is to derive a closed-form expression for the cross-covariance $E[d r_{a}^{T}]$. We begin by expressing the innovation $d$ and the analysis residual $r_{a}$ in terms of the fundamental, uncorrelated error sources: the forecast error $e_{f}$ and the observation error $v$.\n\nThe innovation $d$ is defined as $d = y - H x_{f}$. Substituting the observation equation $y = Hx + v$, we get:\n$$\nd = (Hx + v) - Hx_{f} = H(x - x_{f}) + v\n$$\nUsing the definition of the forecast error, $e_{f} = x - x_{f}$, this simplifies to:\n$$\nd = H e_{f} + v\n$$\nThe analysis residual $r_{a}$ is defined as $r_{a} = y - H x_{a}$. We substitute the analysis update equation, $x_{a} = x_{f} + K d$:\n$$\nr_{a} = y - H(x_{f} + K d) = (y - H x_{f}) - H K d\n$$\nRecognizing that the term in parentheses is the definition of the innovation $d$, we find a direct relationship between the analysis residual and the innovation:\n$$\nr_{a} = d - H K d = (I - H K) d\n$$\nwhere $I$ is the identity matrix of size $m \\times m$.\n\nNow we can write the expression for the cross-covariance $E[d r_{a}^{T}]$:\n$$\nE[d r_{a}^{T}] = E[d ((I - H K) d)^T]\n$$\nUsing the property of the transpose of a product, $(AB)^T = B^T A^T$, we get:\n$$\nE[d r_{a}^{T}] = E[d d^T (I - H K)^T] = E[d d^T (I^T - (HK)^T)] = E[d d^T (I - K^T H^T)]\n$$\nSince $I$, $K$, and $H$ are constant matrices, we can pull the term $(I - K^T H^T)$ outside the expectation:\n$$\nE[d r_{a}^{T}] = E[d d^T] (I - K^T H^T)\n$$\nThe term $E[d d^T]$ is the innovation covariance matrix, which we denote by $S$. Let us compute $S$ using our expression for $d$:\n$$\nS = E[d d^T] = E[(H e_{f} + v)(H e_{f} + v)^T] = E[H e_{f} e_{f}^T H^T + H e_{f} v^T + v e_{f}^T H^T + v v^T]\n$$\nBy linearity of expectation:\n$$\nS = H E[e_{f} e_{f}^T] H^T + H E[e_{f} v^T] + E[v e_{f}^T] H^T + E[v v^T]\n$$\nWe are given that $e_{f}$ and $v$ are independent and have zero mean. This implies that their cross-covariance is zero: $E[e_{f} v^T] = E[e_f]E[v^T] = 0$. Similarly, $E[v e_{f}^T] = 0$. We are also given the definitions of the forecast error covariance $P_{f} = E[e_{f} e_{f}^T]$ and the observation error covariance $R = E[v v^T]$. Substituting these into the expression for $S$ yields the standard formula for the innovation covariance:\n$$\nS = H P_{f} H^T + R\n$$\nNow, returning to our expression for the cross-covariance:\n$$\nE[d r_{a}^{T}] = S (I - K^T H^T) = S - S K^T H^T\n$$\nThe problem statement specifies that the gain matrix $K$ is optimal in the Kalman Filter sense. The optimal Kalman gain is given by:\n$$\nK = P_{f} H^T (H P_{f} H^T + R)^{-1} = P_{f} H^T S^{-1}\n$$\nWe substitute this optimal gain into our expression. First, let's find the transpose of $K$:\n$$\nK^T = (P_{f} H^T S^{-1})^T = (S^{-1})^T (P_{f} H^T)^T = (S^T)^{-1} H P_{f}^T\n$$\nSince covariance matrices are symmetric, $S=S^T$ and $P_{f}=P_{f}^T$. Therefore, $K^T = S^{-1} H P_{f}$. Now we can evaluate the term $S K^T H^T$:\n$$\nS K^T H^T = S (S^{-1} H P_{f}) H^T = (S S^{-1}) H P_{f} H^T = I H P_{f} H^T = H P_{f} H^T\n$$\nFinally, we substitute this result back into the expression for $E[d r_{a}^{T}]$:\n$$\nE[d r_{a}^{T}] = S - H P_{f} H^T\n$$\nSubstituting the expression for $S = H P_{f} H^T + R$:\n$$\nE[d r_{a}^{T}] = (H P_{f} H^T + R) - H P_{f} H^T = R\n$$\nThis derivation yields the closed-form analytic expression for the cross-covariance.\n\n### Motivation for Empirical Estimation of $R$\nThe identity $E[d r_{a}^{T}] = R$ is a cornerstone of data assimilation diagnostics. It states that the theoretical expectation of the cross-covariance between the innovation vector (the forecast residual) and the analysis residual is exactly equal to the observation error covariance matrix $R$. This theoretical result provides a powerful, practical method for estimating $R$ from data.\n\nBy the law of large numbers, a sample mean converges to the true population mean as the sample size grows. If a data assimilation system is run over $N$ cycles, producing a sequence of innovation vectors $\\{d_i\\}_{i=1}^N$ and corresponding analysis residual vectors $\\{r_{a,i}\\}_{i=1}^N$, one can compute an empirical estimate of the expectation $E[d r_{a}^{T}]$ by calculating the sample cross-covariance:\n$$\n\\hat{R} = \\frac{1}{N-1} \\sum_{i=1}^{N} d_i r_{a,i}^T\n$$\nThis method, known as the Desroziers diagnostic, allows for an a posteriori estimation of $R$ using only the outputs of the assimilation system itself. This is immensely valuable in practice, as $R$ (and $P_f$) are often poorly known and must be tuned. This diagnostic provides a data-driven way to check and refine the specification of $R$. It is critical, however, to recognize that the validity of this estimation method hinges on the key assumption that the gain $K$ used in the assimilation is optimal, which in turn requires that the forecast error covariance $P_f$ and observation operator $H$ are correctly specified. Any sub-optimality in the model parameters or gain will invalidate the identity and lead to a biased estimate of $R$.",
            "answer": "$$\\boxed{R}$$"
        },
        {
            "introduction": "After deriving theoretical properties, a common task is to check if the residuals from a real system conform to the model's statistical assumptions. This practice demonstrates the prewhitening technique, a transformation that converts a correlated residual vector with a known covariance matrix $S$ into a \"white\" vector with an identity covariance matrix . This method is fundamental for validating a data assimilation system, as it simplifies complex, correlated residuals into variables that can be evaluated with straightforward statistical tests.",
            "id": "3391019",
            "problem": "Consider a linear observation model in data assimilation with forecast state $x^{f} \\in \\mathbb{R}^{2}$, true state $x \\in \\mathbb{R}^{2}$, and observation $y \\in \\mathbb{R}^{2}$ given by $y = H x + \\epsilon$, where $H \\in \\mathbb{R}^{2 \\times 2}$ is the linear observation operator and $\\epsilon \\in \\mathbb{R}^{2}$ is the observation error. Assume the forecast error $e^{f} = x - x^{f}$ is a zero-mean Gaussian random vector with covariance $P_{f} \\in \\mathbb{R}^{2 \\times 2}$, the observation error $\\epsilon$ is a zero-mean Gaussian random vector with covariance $R \\in \\mathbb{R}^{2 \\times 2}$, and $e^{f}$ and $\\epsilon$ are statistically independent. The innovation (observation-minus-forecast residual) is defined by $d = y - H x^{f}$. Under the correct model assumptions above, $d$ is a zero-mean Gaussian random vector with covariance $S = H P_{f} H^{T} + R$.\n\nIn the context of innovation and residual diagnostics, one seeks a prewhitening transform that maps $d$ to a whitened residual $\\tilde{d}$ whose components are independent and identically distributed standard normal variables. Use the Cholesky factorization $S = L L^{T}$ with $L$ lower triangular and with positive diagonal entries to construct the prewhitening transform. Specifically, take\n$$\nH = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nP_{f} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad\nR = \\begin{pmatrix} 1 & 1 \\\\ 1 & 4 \\end{pmatrix}.\n$$\n\nTasks:\n- Compute $S$ from the given $H$, $P_{f}$, and $R$.\n- Construct the Cholesky factor $L$ such that $S = L L^{T}$ with $L$ lower triangular and positive diagonal entries, and define the prewhitening transform $\\tilde{d} = L^{-1} d$.\n- Starting from the fundamental properties of linear transformations of multivariate Gaussian random vectors, derive the distribution of $\\tilde{d}$ under the correct model. Compute the covariance matrix of $\\tilde{d}$ explicitly in closed form.\n\nYour final answer must be the covariance matrix of $\\tilde{d}$ expressed as a single closed-form analytic expression. No rounding is required, and no units are involved.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. All necessary data and definitions for solving the problem are provided, and there are no internal contradictions or ambiguities.\n\nThe problem asks for the covariance matrix of a whitened residual vector $\\tilde{d}$. The solution proceeds in three steps:\n1.  Compute the innovation covariance matrix $S$.\n2.  Perform the Cholesky factorization of $S$ to find the lower triangular matrix $L$.\n3.  Derive the covariance matrix of the whitened residual $\\tilde{d} = L^{-1} d$.\n\n**Step 1: Compute the innovation covariance matrix $S$**\n\nThe innovation covariance matrix $S$ is given by the formula $S = H P_{f} H^{T} + R$.\nThe given matrices are:\n$H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$, which is the $2 \\times 2$ identity matrix, $I$.\n$P_{f} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 1 \\end{pmatrix}$\n$R = \\begin{pmatrix} 1 & 1 \\\\ 1 & 4 \\end{pmatrix}$\n\nSince $H$ is the identity matrix, $H P_{f} H^{T} = I P_{f} I^{T} = P_{f}$.\nTherefore, the expression for $S$ simplifies to $S = P_{f} + R$.\nSubstituting the given matrices for $P_{f}$ and $R$:\n$$\nS = \\begin{pmatrix} 3 & 1 \\\\ 1 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\\\ 1 & 4 \\end{pmatrix}\n$$\nPerforming the matrix addition:\n$$\nS = \\begin{pmatrix} 3+1 & 1+1 \\\\ 1+1 & 1+4 \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\\\ 2 & 5 \\end{pmatrix}\n$$\nThe matrix $S$ is symmetric. To confirm it is positive definite, we can check its determinant: $\\det(S) = (4)(5) - (2)(2) = 20 - 4 = 16 > 0$. Since the first leading principal minor ($4$) is also positive, $S$ is positive definite, and a unique Cholesky factorization exists.\n\n**Step 2: Construct the Cholesky factor $L$**\n\nWe seek a lower triangular matrix $L$ with positive diagonal entries such that $S = L L^{T}$. Let $L$ be:\n$$\nL = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}\n$$\nThen $L L^{T}$ is:\n$$\nL L^{T} = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix} \\begin{pmatrix} l_{11} & l_{21} \\\\ 0 & l_{22} \\end{pmatrix} = \\begin{pmatrix} l_{11}^{2} & l_{11} l_{21} \\\\ l_{11} l_{21} & l_{21}^{2} + l_{22}^{2} \\end{pmatrix}\n$$\nEquating the elements of $L L^{T}$ with the elements of $S$:\n$$\n\\begin{pmatrix} l_{11}^{2} & l_{11} l_{21} \\\\ l_{11} l_{21} & l_{21}^{2} + l_{22}^{2} \\end{pmatrix} = \\begin{pmatrix} 4 & 2 \\\\ 2 & 5 \\end{pmatrix}\n$$\nFrom the $(1,1)$ element: $l_{11}^{2} = 4$. Since $l_{11}$ must be positive, we have $l_{11} = 2$.\nFrom the $(2,1)$ element: $l_{11} l_{21} = 2$. Substituting $l_{11} = 2$, we get $2 l_{21} = 2$, which gives $l_{21} = 1$.\nFrom the $(2,2)$ element: $l_{21}^{2} + l_{22}^{2} = 5$. Substituting $l_{21} = 1$, we get $1^{2} + l_{22}^{2} = 5$, so $l_{22}^{2} = 4$. Since $l_{22}$ must be positive, we have $l_{22} = 2$.\n\nThus, the Cholesky factor is:\n$$\nL = \\begin{pmatrix} 2 & 0 \\\\ 1 & 2 \\end{pmatrix}\n$$\n\n**Step 3: Derive the covariance matrix of the whitened residual $\\tilde{d}$**\n\nThe innovation vector $d = y - Hx^{f}$ is a linear combination of Gaussian random vectors $e^{f} = x - x^{f}$ and $\\epsilon$. Specifically, $d = H(x-x^{f}) + \\epsilon = H e^{f} + \\epsilon$. As stated in the problem, and verified in the validation, $d$ is a zero-mean Gaussian random vector with covariance matrix $S$, i.e., $d \\sim \\mathcal{N}(0, S)$.\n\nThe whitened residual $\\tilde{d}$ is defined by the linear transformation $\\tilde{d} = L^{-1} d$.\nA fundamental property of multivariate Gaussian random vectors states that if a random vector $z \\sim \\mathcal{N}(\\mu, \\Sigma)$, then a linear transformation $w = A z$ results in a random vector $w$ that is also Gaussian, with distribution $w \\sim \\mathcal{N}(A\\mu, A\\Sigma A^{T})$.\n\nApplying this property to $\\tilde{d}$:\n- The mean of $\\tilde{d}$ is $E[\\tilde{d}] = L^{-1} E[d] = L^{-1} \\cdot 0 = 0$.\n- The covariance matrix of $\\tilde{d}$, let's call it $\\text{Cov}(\\tilde{d})$, is given by:\n$$\n\\text{Cov}(\\tilde{d}) = (L^{-1}) S (L^{-1})^{T}\n$$\nWe substitute $S = L L^{T}$ into this expression:\n$$\n\\text{Cov}(\\tilde{d}) = L^{-1} (L L^{T}) (L^{-1})^{T}\n$$\nUsing the matrix identity $(A^{-1})^{T} = (A^{T})^{-1}$, we can write $(L^{-1})^{T}$ as $(L^{T})^{-1}$.\n$$\n\\text{Cov}(\\tilde{d}) = (L^{-1} L) (L^{T} (L^{T})^{-1})\n$$\nSince $L^{-1}L = I$ and $L^{T}(L^{T})^{-1} = I$, where $I$ is the identity matrix, the expression simplifies to:\n$$\n\\text{Cov}(\\tilde{d}) = I \\cdot I = I\n$$\nThis derivation shows that the covariance matrix of the whitened residual is the identity matrix, which is the expected result of a prewhitening transformation. The components of $\\tilde{d}$ are uncorrelated and have unit variance. Since $\\tilde{d}$ is Gaussian, its components are also independent and are standard normal variables.\n\nTo provide the explicit closed-form answer, the covariance matrix of $\\tilde{d}$ is the $2 \\times 2$ identity matrix:\n$$\n\\text{Cov}(\\tilde{d}) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis result is general. For completeness, we can verify it using the specific matrices calculated earlier.\nFirst, we find $L^{-1}$:\n$$\nL^{-1} = \\frac{1}{(2)(2) - (0)(1)} \\begin{pmatrix} 2 & 0 \\\\ -1 & 2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 & 0 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ -\\frac{1}{4} & \\frac{1}{2} \\end{pmatrix}\n$$\nThen, we calculate $\\text{Cov}(\\tilde{d}) = L^{-1} S (L^{-1})^{T}$:\n$$\nL^{-1} S = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ -\\frac{1}{4} & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 4 & 2 \\\\ 2 & 5 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix}\n$$\nNote that this intermediate result is equal to $L^T$, as expected from $L^{-1}S = L^{-1}LL^T = L^T$.\nNow, we post-multiply by $(L^{-1})^{T}$:\n$$\n(L^{-1})^{T} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{4} \\\\ 0 & \\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\text{Cov}(\\tilde{d}) = (L^{-1}S)(L^{-1})^{T} = \\begin{pmatrix} 2 & 1 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{4} \\\\ 0 & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} (2)(\\frac{1}{2}) + (1)(0) & (2)(-\\frac{1}{4}) + (1)(\\frac{1}{2}) \\\\ (0)(\\frac{1}{2}) + (2)(0) & (0)(-\\frac{1}{4}) + (2)(\\frac{1}{2}) \\end{pmatrix}\n$$\n$$\n\\text{Cov}(\\tilde{d}) = \\begin{pmatrix} 1 & -\\frac{1}{2} + \\frac{1}{2} \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe explicit calculation confirms the general derivation. The covariance matrix of $\\tilde{d}$ is the $2 \\times 2$ identity matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This final practice applies the principles of statistical testing to a real-world algorithmic challenge: deciding when an iterative solver has converged sufficiently. By monitoring the statistical properties of the residual—specifically its squared Mahalanobis norm, which follows a $\\chi^2$ distribution under the correct model—we can design an intelligent stopping criterion that halts the process once the residual is statistically consistent with the noise level . This exercise moves from passive diagnosis to active control, showing how residual statistics can prevent overfitting in iterative algorithms.",
            "id": "3390997",
            "problem": "Consider an iterative inverse problem solver producing at each iteration index $k$ a residual (innovation) vector $v_k \\in \\mathbb{R}^m$ and an associated positive definite residual covariance matrix $S_k \\in \\mathbb{R}^{m \\times m}$. Define the squared Mahalanobis norm of the residual by $r_k = \\|v_k\\|_{S_k^{-1}}^2 = v_k^\\top S_k^{-1} v_k$. Under the null hypothesis of noise-consistent residuals, assume a Gaussian residual model $v_k \\sim \\mathcal{N}(0, S_k)$, which implies $r_k \\sim \\chi^2(m)$, where $m$ is the measurement dimension. Use sequential change-detection based on Cumulative Sum (CUSUM) and Generalized Likelihood Ratio (GLR) on the sequence $\\{r_k\\}_{k=1}^K$ to define an automatic stopping criterion that halts iterations as soon as residuals match the noise level, thereby avoiding overfitting.\n\nStarting from the Gaussian residual assumption and the definition of $r_k$, derive:\n- A one-sided GLR statistic per iteration $k$ that tests for an increase in the residual variance scale relative to the noise-consistent baseline, and describe how to gate the sequence by a small per-iteration threshold to ensure that $r_k$ behaves like $\\chi^2(m)$ for several consecutive iterations before stopping.\n- A one-sided CUSUM designed to detect a downward change in the mean of $r_k$ toward the noise-consistent level, using the fundamental properties of the $\\chi^2(m)$ distribution.\n\nThen implement a combined stopping rule that declares stopping at the smallest iteration index $k$ satisfying both of the following conditions simultaneously:\n- The GLR gate condition holds for the last $W$ iterations, meaning the per-iteration GLR statistic is below a tolerance $\\tau_{\\mathrm{GLR}}$ for $W$ consecutive iterations up to index $k$.\n- The downward CUSUM statistic has crossed a threshold $h_{\\downarrow}$ by or at index $k$.\n\nIf no such $k \\leq K$ exists, return the integer $-1$.\n\nYour program must implement this stopping criterion for the following test suite. For each test case, you are given $m$, a diagonal covariance $S_k$ that is constant in $k$, and a prescribed target sequence of $r_k$ values. To ensure scientific realism, construct $v_k$ so that $r_k$ is exactly achieved under the given $S_k$. A valid construction is to choose $w_k \\in \\mathbb{R}^m$ with $w_k = [\\sqrt{r_k}, 0, \\dots, 0]^\\top$ and set $v_k = L w_k$, where $L$ is the principal square root of $S_k$ (for diagonal $S_k$, $L$ is the diagonal matrix of elementwise square roots). Verify that the computed $r_k = v_k^\\top S_k^{-1} v_k$ equals the prescribed value.\n\nUse the following fixed detection parameters for all test cases: window length $W = 3$, GLR gate tolerance $\\tau_{\\mathrm{GLR}} = 0.25$, and downward CUSUM threshold $h_{\\downarrow} = 0.08$.\n\nTest suite:\n- Case $1$: $m = 3$, $S_k = \\operatorname{diag}(1, 1, 1)$, $K = 15$, with $r_k$ sequence given by $[12, 11, 10, 9, 8, 7, 5, 4, 3.5, 3.2, 3.1, 3.0, 2.9, 3.0, 3.1]$.\n- Case $2$: $m = 2$, $S_k = \\operatorname{diag}(1, 1)$, $K = 12$, with $r_k$ sequence given by $[6.0, 5.8, 5.9, 6.1, 5.7, 5.8, 5.9, 6.0, 5.6, 5.7, 5.8, 6.0]$.\n- Case $3$: $m = 4$, $S_k = \\operatorname{diag}(1, 1, 1, 1)$, $K = 7$, with $r_k$ sequence given by $[3.9, 3.8, 3.95, 4.05, 4.0, 3.98, 4.02]$.\n- Case $4$: $m = 3$, $S_k = \\operatorname{diag}(1, 1, 1)$, $K = 12$, with $r_k$ sequence given by $[9.0, 8.5, 7.5, 6.5, 5.0, 4.0, 3.5, 3.2, 2.9, 2.8, 2.95, 2.9]$.\n\nYour program should produce a single line of output containing the stopping indices for the four cases as a comma-separated list enclosed in square brackets, in the order listed above (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$). The outputs for each case must be integers, with $-1$ indicating no stop within the horizon $K$.",
            "solution": "The problem requires the design and implementation of a sequential change-detection algorithm to serve as a stopping criterion for an iterative inverse problem solver. The core idea is to halt the solver when the residuals become statistically indistinguishable from the measurement noise, thus preventing overfitting. This is achieved by monitoring a sequence of squared Mahalanobis norms of the residuals, $\\{r_k\\}_{k=1}^K$, where $r_k = v_k^\\top S_k^{-1} v_k$. Under the null hypothesis that the residuals $v_k$ are samples from a zero-mean Gaussian distribution with covariance $S_k$, i.e., $v_k \\sim \\mathcal{N}(0, S_k)$, the statistic $r_k$ follows a chi-squared distribution with $m$ degrees of freedom, $r_k \\sim \\chi^2(m)$, where $m$ is the dimension of the measurement vector. The expected value of $r_k$ under this hypothesis is $E[r_k] = m$.\n\nThe stopping criterion is based on two statistical tests applied to the sequence $\\{r_k\\}$: a Generalized Likelihood Ratio (GLR) test and a Cumulative Sum (CUSUM) test.\n\n### Derivation of the One-Sided Generalized Likelihood Ratio (GLR) Statistic\n\nThe GLR test is used as a gating mechanism to ensure that the residual norm $r_k$ is not excessively large, which would indicate that significant signal components still exist in the residual. We test for an increase in the residual variance scale. Let the actual covariance of the residual be $\\sigma^2 S_k$. The null hypothesis $H_0$ states that the residuals are noise-consistent, corresponding to a scale factor $\\sigma^2 = 1$. The alternative hypothesis $H_1$ is that the residuals are larger than the noise level, corresponding to $\\sigma^2 > 1$.\n\nFor a given observation $r_k$, the problem is to test $H_0: \\sigma^2 = 1$ versus $H_1: \\sigma^2 > 1$. If $v_k \\sim \\mathcal{N}(0, \\sigma^2 S_k)$, then $r_k = v_k^\\top S_k^{-1} v_k$ is distributed as $\\sigma^2 \\chi^2(m)$. The probability density function of $r_k$ is given by\n$$p(r_k | \\sigma^2) = \\frac{1}{(\\sigma^2)^{m/2}} \\frac{(r_k)^{m/2 - 1} e^{-r_k/(2\\sigma^2)}}{2^{m/2} \\Gamma(m/2)}$$\nThe log-likelihood function is\n$$\\ell(\\sigma^2; r_k) = -\\frac{m}{2} \\log(\\sigma^2) + (\\frac{m}{2}-1)\\log(r_k) - \\frac{r_k}{2\\sigma^2} - \\log(2^{m/2}\\Gamma(m/2))$$\nThe maximum likelihood estimate (MLE) of $\\sigma^2$ is found by setting the derivative of $\\ell$ with respect to $\\sigma^2$ to zero:\n$$\\frac{\\partial \\ell}{\\partial (\\sigma^2)} = -\\frac{m}{2\\sigma^2} + \\frac{r_k}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{r_k}{m}$$\nThe generalized likelihood ratio statistic is $2 \\log \\Lambda$, where $\\Lambda = \\frac{\\sup_{\\sigma^2 \\in \\Theta_1} p(r_k|\\sigma^2)}{\\sup_{\\sigma^2 \\in \\Theta_0} p(r_k|\\sigma^2)}$. Here $\\Theta_0 = \\{1\\}$ and $\\Theta_1 = \\{\\sigma^2 | \\sigma^2 > 1\\}$. The MLE under the restricted space of $H_1$ is $\\hat{\\sigma}^2_{H_1} = \\max(1, \\hat{\\sigma}^2_{\\text{MLE}}) = \\max(1, r_k/m)$.\n\nThe GLR statistic, denoted $g_k$, is:\n$$g_k = 2 \\left( \\ell(\\hat{\\sigma}^2_{H_1}; r_k) - \\ell(1; r_k) \\right)$$\nIf $r_k/m \\le 1$, then $\\hat{\\sigma}^2_{H_1} = 1$, and $g_k = 0$. This indicates no evidence for an increased variance scale.\nIf $r_k/m > 1$, then $\\hat{\\sigma}^2_{H_1} = r_k/m$. Substituting this into the log-likelihood difference:\n$$g_k = 2 \\left[ \\left(-\\frac{m}{2}\\log(\\frac{r_k}{m}) - \\frac{r_k}{2(r_k/m)}\\right) - \\left(-\\frac{m}{2}\\log(1) - \\frac{r_k}{2}\\right) \\right]$$\n$$g_k = 2 \\left[ -\\frac{m}{2}\\log(\\frac{r_k}{m}) - \\frac{m}{2} + \\frac{r_k}{2} \\right] = r_k - m - m\\log(\\frac{r_k}{m})$$\nThus, the one-sided GLR statistic is:\n$$ g_k = \\begin{cases} r_k - m - m \\log(r_k/m) & \\text{if } r_k > m \\\\ 0 & \\text{if } r_k \\le m \\end{cases} $$\nThe GLR gate condition requires this statistic to be below a tolerance $\\tau_{\\mathrm{GLR}}$ for $W$ consecutive iterations. A small $g_k$ implies $r_k$ is close to its expected value $m$, confirming that residuals behave like $\\chi^2(m)$.\n\n### Derivation of the Downward CUSUM Statistic\n\nThe CUSUM test is designed to detect a downward shift in the mean of the $\\{r_k\\}$ sequence. Initially, $r_k$ is large due to signal contamination in the residuals. As the iterative solver converges, the mean of $r_k$ is expected to decrease towards the noise-consistent level $E[r_k] = m$. We need to detect when this level has been reached.\n\nA one-sided Page's CUSUM statistic for detecting a downward shift is appropriate here. The statistic at each iteration measures the deviation of $r_k$ from this target. The update rule for the downward CUSUM statistic, $D_k^{\\downarrow}$, is defined as a recursive process:\n$$ D_0^{\\downarrow} = 0 $$\n$$ D_k^{\\downarrow} = \\max(0, D_{k-1}^{\\downarrow} + m - r_k) \\quad \\text{for } k=1, 2, \\dots, K $$\nThis statistic, $D_k^{\\downarrow}$, accumulates the \"undershoot\" amounts, $m - r_k$, whenever $r_k$ is less than $m$. If $r_k$ is consistently greater than or equal to $m$, the statistic remains at or is reset to $0$. A sustained period of $r_k  m$ will cause $D_k^{\\downarrow}$ to grow. The detection of the change occurs when $D_k^{\\downarrow}$ crosses a predefined threshold $h_{\\downarrow}$.\n\n### Combined Stopping Criterion\n\nThe final stopping rule combines the GLR gate and the downward CUSUM test. An iteration $k$ is declared as the stopping point if it is the smallest index $k \\in \\{W, \\dots, K\\}$ that simultaneously satisfies two conditions:\n\\begin{enumerate}\n    \\item **GLR Gate Condition**: The GLR statistic has been below the tolerance $\\tau_{\\mathrm{GLR}}$ for the last $W$ consecutive iterations. Formally, $g_j  \\tau_{\\mathrm{GLR}}$ for all $j \\in \\{k-W+1, \\dots, k\\}$.\n    \\item **CUSUM Crossing Condition**: The downward CUSUM statistic has crossed its detection threshold. Formally, $D_k^{\\downarrow} \\ge h_{\\downarrow}$.\n\\end{enumerate}\nIf no such index $k$ exists within the maximum number of iterations $K$, the algorithm indicates no stopping by returning $-1$. The algorithm proceeds by calculating the sequences $\\{g_k\\}$ and $\\{D_k^{\\downarrow}\\}$ and then iterating from $k=W$ to $K$ to check for the first occurrence of both conditions being met.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the combined CUSUM-GLR stopping criterion for a suite of test cases.\n    \"\"\"\n\n    # --- Fixed detection parameters ---\n    W = 3\n    tau_glr = 0.25\n    h_cusum_down = 0.08\n\n    # --- Test suite definition ---\n    test_cases = [\n        {\n            \"m\": 3,\n            \"S_k\": np.diag([1.0, 1.0, 1.0]),\n            \"K\": 15,\n            \"r_k_seq\": np.array([12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 5.0, 4.0, 3.5, 3.2, 3.1, 3.0, 2.9, 3.0, 3.1]),\n        },\n        {\n            \"m\": 2,\n            \"S_k\": np.diag([1.0, 1.0]),\n            \"K\": 12,\n            \"r_k_seq\": np.array([6.0, 5.8, 5.9, 6.1, 5.7, 5.8, 5.9, 6.0, 5.6, 5.7, 5.8, 6.0]),\n        },\n        {\n            \"m\": 4,\n            \"S_k\": np.diag([1.0, 1.0, 1.0, 1.0]),\n            \"K\": 7,\n            \"r_k_seq\": np.array([3.9, 3.8, 3.95, 4.05, 4.0, 3.98, 4.02]),\n        },\n        {\n            \"m\": 3,\n            \"S_k\": np.diag([1.0, 1.0, 1.0]),\n            \"K\": 12,\n            \"r_k_seq\": np.array([9.0, 8.5, 7.5, 6.5, 5.0, 4.0, 3.5, 3.2, 2.9, 2.8, 2.95, 2.9]),\n        },\n    ]\n\n    def compute_stop_index(m, r_k_seq, K, W, tau_glr, h_cusum_down):\n        \"\"\"\n        Computes the stopping index for a single test case.\n        \"\"\"\n        if K  W:\n            return -1\n\n        # --- Step 1: Compute GLR and CUSUM statistics for the whole sequence ---\n        glr_stats = np.zeros(K)\n        cusum_stats = np.zeros(K)\n\n        # GLR statistic calculation\n        for i in range(K):\n            r_k = r_k_seq[i]\n            if r_k > m:\n                # Use np.log for natural logarithm\n                glr_stats[i] = r_k - m - m * np.log(r_k / m)\n            else:\n                glr_stats[i] = 0\n\n        # Downward CUSUM statistic calculation\n        d_cusum_prev = 0.0\n        for i in range(K):\n            r_k = r_k_seq[i]\n            d_cusum_current = max(0, d_cusum_prev + m - r_k)\n            cusum_stats[i] = d_cusum_current\n            d_cusum_prev = d_cusum_current\n\n        # --- Step 2: Find the smallest k that satisfies both conditions ---\n        # Iterate k from W to K (using 1-based indexing for k)\n        for k in range(W, K + 1):\n            # Convert 1-based k to 0-based k_idx\n            k_idx = k - 1\n\n            # Condition 1: GLR Gate\n            # The window is for iterations {k-W+1, ..., k}\n            # In 0-based indices, this corresponds to slice [k-W, k-1] or [k_idx-W+1, k_idx]\n            glr_window = glr_stats[k_idx - W + 1 : k_idx + 1]\n            glr_condition_met = np.all(glr_window  tau_glr)\n\n            # Condition 2: CUSUM Crossing\n            cusum_condition_met = cusum_stats[k_idx] >= h_cusum_down\n            \n            if glr_condition_met and cusum_condition_met:\n                return k # Return the 1-based index\n\n        # If loop completes without finding a stopping point\n        return -1\n\n    results = []\n    for case in test_cases:\n        stop_index = compute_stop_index(\n            m=case[\"m\"],\n            r_k_seq=case[\"r_k_seq\"],\n            K=case[\"K\"],\n            W=W,\n            tau_glr=tau_glr,\n            h_cusum_down=h_cusum_down\n        )\n        results.append(stop_index)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}