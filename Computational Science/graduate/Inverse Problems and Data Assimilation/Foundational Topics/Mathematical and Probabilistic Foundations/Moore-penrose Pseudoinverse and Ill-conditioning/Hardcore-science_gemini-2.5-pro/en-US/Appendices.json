{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex, ill-conditioned systems, it is essential to build a solid foundation by understanding the Moore-Penrose pseudoinverse in its most well-behaved forms. This exercise guides you through the derivation of the explicit formulas for the pseudoinverse for full-rank matrices, starting from the normal equations and the minimum-norm principle. By also examining a rank-deficient counterexample , you will see precisely why these simple formulas fail in the general case, highlighting the necessity of the Singular Value Decomposition (SVD) for a universally applicable definition.",
            "id": "3404346",
            "problem": "Consider a linear inverse problem arising in data assimilation: recover an unknown state vector $x \\in \\mathbb{R}^{n}$ from linear observations $y \\in \\mathbb{R}^{m}$ related by $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator and $\\varepsilon$ is observational error modeled as additive noise. One seeks a linear estimator mapping $y$ to a candidate $x$ based on the principles of least squares and minimal energy, using only the following foundations: the Euclidean norm, the normal equations characterization of least squares minimizers, and the four defining Moore-Penrose conditions for a generalized inverse.\n\nTasks:\n1. Starting from the normal equations and the definition of the least-squares solution for an overdetermined system $y = A x$ with $m \\geq n$ and $A$ having full column rank, derive the unique linear operator from $y$ to the least-squares minimizer. Then, using the four Moore-Penrose conditions, show that this operator equals the Moore-Penrose pseudoinverse of $A$.\n2. For the underdetermined system with $m \\leq n$ and $A$ having full row rank, derive the unique minimal Euclidean norm solution using a variational argument with Lagrange multipliers. Then, verify via the four Moore-Penrose conditions that the resulting linear operator equals the Moore-Penrose pseudoinverse of $A$.\n3. Provide a concrete counterexample where $A$ is rank-deficient and the above derivations fail because the normal equations lead to a singular system. Specifically, consider the matrix\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n2  2 \\\\\n3  3\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}.\n$$\nCompute the Moore-Penrose pseudoinverse $A^{+}$ of this $A$ using the Singular Value Decomposition (SVD), and then compute the Frobenius norm of $A^{+}$.\n\nAnswer requirement:\n- Report only the Frobenius norm of $A^{+}$ for the above counterexample matrix $A$.\n- Provide the final answer as an exact analytic expression. No rounding is required.",
            "solution": "The provided problem is a well-posed and standard exercise in linear algebra, specifically concerning the theory of generalized inverses and their application to linear least-squares problems. The problem is scientifically grounded, objective, and self-contained. It is thus deemed valid. The solution will proceed by addressing the three tasks in the order presented.\n\nThe Moore-Penrose pseudoinverse of a matrix $A$, denoted $A^{+}$, is the unique matrix satisfying the following four Moore-Penrose conditions:\n1. $AXA = A$\n2. $XAX = X$\n3. $(AX)^{T} = AX$ (AX is symmetric)\n4. $(XA)^{T} = XA$ (XA is symmetric)\nFor real matrices, the transpose $T$ is used. For complex matrices, the conjugate transpose would be used.\n\nTask 1: Overdetermined system with full column rank.\nWe seek to find the linear operator that maps the observation vector $y \\in \\mathbb{R}^{m}$ to the least-squares solution $x_{LS}$ of the system $y=Ax$. The solution $x_{LS}$ minimizes the squared Euclidean norm of the residual, $\\|y-Ax\\|_{2}^{2}$. The problem is overdetermined with $A \\in \\mathbb{R}^{m \\times n}$, $m \\geq n$, and $A$ having full column rank, which is $\\mathrm{rank}(A) = n$.\n\nThe minimizer of $\\|y-Ax\\|_{2}^{2} = (y-Ax)^{T}(y-Ax)$ is found by setting the gradient with respect to $x$ to zero.\n$$ \\nabla_{x} (y^{T}y - y^{T}Ax - x^{T}A^{T}y + x^{T}A^{T}Ax) = -2A^{T}y + 2A^{T}Ax = 0 $$\nThis yields the normal equations:\n$$ A^{T}Ax = A^{T}y $$\nSince $A$ has full column rank $n$, the $n \\times n$ matrix $A^{T}A$ is invertible. Therefore, a unique least-squares solution exists and is given by:\n$$ x_{LS} = (A^{T}A)^{-1}A^{T}y $$\nThe linear operator mapping $y$ to $x_{LS}$ is the matrix $X = (A^{T}A)^{-1}A^{T}$. We now verify that this operator is the Moore-Penrose pseudoinverse $A^{+}$ by checking the four conditions.\n\n1.  $AXA = A((A^{T}A)^{-1}A^{T})A = A((A^{T}A)^{-1}(A^{T}A)) = A(I_{n}) = A$. This holds.\n2.  $XAX = ((A^{T}A)^{-1}A^{T})A((A^{T}A)^{-1}A^{T}) = ((A^{T}A)^{-1}(A^{T}A))((A^{T}A)^{-1}A^{T}) = I_{n}((A^{T}A)^{-1}A^{T}) = X$. This holds.\n3.  $AX = A(A^{T}A)^{-1}A^{T}$. We check for symmetry: $(AX)^{T} = (A(A^{T}A)^{-1}A^{T})^{T} = (A^{T})^{T}((A^{T}A)^{-1})^{T}A^{T} = A((A^{T}A)^{T})^{-1}A^{T} = A(A^{T}(A^{T})^{T})^{-1}A^{T} = A(A^{T}A)^{-1}A^{T} = AX$. This holds.\n4.  $XA = (A^{T}A)^{-1}A^{T}A = I_{n}$. The identity matrix $I_n$ is symmetric, so $(XA)^{T} = I_{n}^{T} = I_{n} = XA$. This holds.\n\nAll four conditions are satisfied, so for an overdetermined system with a full column rank matrix $A$, the pseudoinverse is indeed $A^{+} = (A^{T}A)^{-1}A^{T}$.\n\nTask 2: Underdetermined system with full row rank.\nWe seek the unique solution to $Ax=y$ that has the minimum Euclidean norm $\\|x\\|_{2}$. The system is underdetermined with $A \\in \\mathbb{R}^{m \\times n}$, $m \\leq n$, and $A$ having full row rank, which is $\\mathrm{rank}(A) = m$.\nThis is a constrained optimization problem: minimize $\\frac{1}{2}\\|x\\|_{2}^{2}$ subject to $Ax-y=0$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$ \\mathcal{L}(x, \\lambda) = \\frac{1}{2}x^{T}x + \\lambda^{T}(y - Ax) $$\nwhere $\\lambda \\in \\mathbb{R}^{m}$ is the vector of Lagrange multipliers. Setting the gradient with respect to $x$ to zero gives a condition for optimality:\n$$ \\nabla_{x}\\mathcal{L} = x - A^{T}\\lambda = 0 \\implies x = A^{T}\\lambda $$\nSubstituting this into the constraint $Ax=y$:\n$$ A(A^{T}\\lambda) = y \\implies (AA^{T})\\lambda = y $$\nSince $A$ has full row rank $m$, the $m \\times m$ matrix $AA^{T}$ is invertible. We can solve for $\\lambda$:\n$$ \\lambda = (AA^{T})^{-1}y $$\nSubstituting this back into the expression for $x$, we obtain the minimum norm solution:\n$$ x_{MN} = A^{T}(AA^{T})^{-1}y $$\nThe linear operator is $X = A^{T}(AA^{T})^{-1}$. We verify that this is $A^{+}$ by checking the four conditions.\n\n1.  $AXA = A(A^{T}(AA^{T})^{-1})A = (AA^{T})(AA^{T})^{-1}A = I_{m}A = A$. This holds.\n2.  $XAX = (A^{T}(AA^{T})^{-1})A(A^{T}(AA^{T})^{-1}) = A^{T}(AA^{T})^{-1}(AA^{T})(AA^{T})^{-1} = A^{T}I_{m}(AA^{T})^{-1} = A^{T}(AA^{T})^{-1} = X$. This holds.\n3.  $AX = A(A^{T}(AA^{T})^{-1}) = (AA^{T})(AA^{T})^{-1} = I_{m}$. The identity matrix $I_m$ is symmetric, so $(AX)^{T} = I_{m}^{T} = I_{m} = AX$. This holds.\n4.  $XA = A^{T}(AA^{T})^{-1}A$. We check for symmetry: $(XA)^{T} = (A^{T}(AA^{T})^{-1}A)^{T} = A^{T}((AA^{T})^{-1})^{T}(A^{T})^{T} = A^{T}((AA^{T})^{T})^{-1}A = A^{T}((A^{T})^{T}A^{T})^{-1}A = A^{T}(AA^{T})^{-1}A = XA$. This holds.\n\nAll four conditions are satisfied, so for an underdetermined system with a full row rank matrix $A$, the pseudoinverse is indeed $A^{+} = A^{T}(AA^{T})^{-1}$.\n\nTask 3: Rank-deficient counterexample.\nWe are given the matrix $A = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 3  3 \\end{pmatrix}$. This is a $3 \\times 2$ matrix. Its columns are linearly dependent, so its rank is $1$, which is less than the number of columns ($n=2$). Thus, $A$ is rank-deficient and does not have full column rank.\n\nLet's attempt to use the normal equations formulation from Task 1. We compute $A^{T}A$:\n$$ A^{T}A = \\begin{pmatrix} 1  2  3 \\\\ 1  2  3 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 3  3 \\end{pmatrix} = \\begin{pmatrix} 1+4+9  1+4+9 \\\\ 1+4+9  1+4+9 \\end{pmatrix} = \\begin{pmatrix} 14  14 \\\\ 14  14 \\end{pmatrix} $$\nThe determinant of this matrix is $\\det(A^{T}A) = 14 \\times 14 - 14 \\times 14 = 0$. Since the determinant is zero, $A^{T}A$ is singular and its inverse $(A^{T}A)^{-1}$ does not exist. This demonstrates that the derivation from Task 1 fails for rank-deficient matrices.\n\nTo compute the Moore-Penrose pseudoinverse $A^{+}$ for this rank-deficient matrix, we use the Singular Value Decomposition (SVD). For any matrix $A = U\\Sigma V^{T}$, its pseudoinverse is $A^{+} = V\\Sigma^{+}U^{T}$, where $\\Sigma^{+}$ is obtained by taking the transpose of $\\Sigma$ and taking the reciprocal of its non-zero entries.\n\nThe Frobenius norm of $A^{+}$ can be computed directly from the singular values of $A$. The Frobenius norm is defined as $\\|X\\|_{F} = \\sqrt{\\sum_{i,j} |x_{ij}|^2}$. It can also be expressed as $\\|X\\|_{F} = \\sqrt{\\mathrm{Tr}(X^{T}X)} = \\sqrt{\\sum_{k} \\sigma_{k}(X)^2}$, where $\\sigma_{k}(X)$ are the singular values of $X$.\nThe singular values of $A^{+}$ are the reciprocals of the non-zero singular values of $A$. Let the singular values of $A$ be $\\sigma_1, \\sigma_2, \\dots$. The singular values of $A^{+}$ will be $1/\\sigma_1, 1/\\sigma_2, \\dots$ for the non-zero $\\sigma_i$.\n\nThe singular values of $A$ are the square roots of the non-zero eigenvalues of $A^{T}A$. We have already computed $A^{T}A = \\begin{pmatrix} 14  14 \\\\ 14  14 \\end{pmatrix}$. The characteristic equation is $\\det(A^{T}A - \\lambda I) = 0$:\n$$ (14-\\lambda)^{2} - 14^{2} = 0 \\implies (14-\\lambda-14)(14-\\lambda+14) = 0 \\implies (-\\lambda)(28-\\lambda) = 0 $$\nThe eigenvalues of $A^T A$ are $\\lambda_1 = 28$ and $\\lambda_2 = 0$.\nThe non-zero singular value of $A$ is $\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{28} = 2\\sqrt{7}$. The other singular value is $\\sigma_2 = \\sqrt{0} = 0$.\nThe rank of $A$ is the number of non-zero singular values, which is $1$.\n\nThe set of non-zero singular values of $A^{+}$ consists of the reciprocals of the non-zero singular values of $A$. In this case, $A^{+}$ has only one non-zero singular value, $\\sigma_1^{+}$:\n$$ \\sigma_1^{+} = \\frac{1}{\\sigma_1} = \\frac{1}{2\\sqrt{7}} $$\nThe Frobenius norm of $A^{+}$ is the square root of the sum of the squares of its singular values:\n$$ \\|A^{+}\\|_{F} = \\sqrt{(\\sigma_1^{+})^2 + (\\sigma_2^{+})^2 + \\dots} = \\sqrt{\\left(\\frac{1}{2\\sqrt{7}}\\right)^2} = \\frac{1}{2\\sqrt{7}} $$\nTo provide the answer in a rationalized form, we multiply the numerator and denominator by $\\sqrt{7}$:\n$$ \\|A^{+}\\|_{F} = \\frac{1}{2\\sqrt{7}} \\cdot \\frac{\\sqrt{7}}{\\sqrt{7}} = \\frac{\\sqrt{7}}{14} $$\nThis is the required value.",
            "answer": "$$\\boxed{\\frac{\\sqrt{7}}{14}}$$"
        },
        {
            "introduction": "The primary danger of using the pseudoinverse in ill-conditioned problems is its sensitivity to noise in the observations. This practice provides a crucial theoretical workout to quantify this sensitivity by finding the \"worst-case\" scenario for error amplification . By analyzing how the pseudoinverse transforms a noise vector of a fixed magnitude, you will discover that the maximum error occurs when noise aligns with specific directions in the data space, providing a deep insight into the mechanism of ill-conditioning.",
            "id": "3404452",
            "problem": "Consider a linearized observation operator in a variational Data Assimilation (DA) setting, represented by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition (Singular Value Decomposition (SVD)) $A = U \\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r}  0$ and $\\sigma_{i} = 0$ for $i  r$, with $r = \\operatorname{rank}(A) \\geq 1$. The Moore–Penrose pseudoinverse is $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\mathsf{T}}$, where $\\Sigma^{\\dagger}$ is obtained by inverting the nonzero singular values and leaving zeros in place.\n\nAssume the true state $x_{\\star} \\in \\mathbb{R}^{n}$ generates noise-contaminated observations $y = A x_{\\star} + \\eta$, where the observation error $\\eta \\in \\mathbb{R}^{m}$ is deterministic but unknown and satisfies the noise budget $\\|\\eta\\|_{2} = \\delta$ with a known $\\delta  0$. The DA reconstructor computes $\\hat{x} = A^{\\dagger} y$. The reconstruction error attributable to the observation error is $e = \\hat{x} - A^{\\dagger} A x_{\\star} = A^{\\dagger} \\eta$.\n\nUsing only the definitions above and fundamental properties of orthogonal transformations and Euclidean norms, determine the exact maximum possible value of $\\|e\\|_{2}$ over all $\\eta$ with $\\|\\eta\\|_{2} = \\delta$, expressed in closed form in terms of $\\delta$ and the singular values of $A$. You should not assume any result beyond the definitions given and standard properties of orthogonal matrices and Euclidean norms. In your reasoning, indicate which direction in data space and which direction in state space realize this maximum, and explain the relationship to the right singular vector associated with the smallest positive singular value. Interpret the maximizing configuration in the context of DA as the worst-case noise-induced analysis error mechanism.\n\nYour final answer must be a single closed-form analytic expression. Do not include any units in your final answer. Do not provide the maximizing directions or any explanatory text in the final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem in the field of inverse problems and data assimilation.\n\nOur objective is to find the maximum possible value of the Euclidean norm of the reconstruction error, $\\|e\\|_{2}$, given that $e = A^{\\dagger} \\eta$ and the observation error $\\eta$ satisfies $\\|\\eta\\|_{2} = \\delta$. The matrix $A$ has the singular value decomposition (SVD) $A = U \\Sigma V^{\\mathsf{T}}$, and its Moore-Penrose pseudoinverse is $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\mathsf{T}}$.\n\nThe norm of the error vector $e$ is given by:\n$$\n\\|e\\|_{2} = \\|A^{\\dagger} \\eta\\|_{2}\n$$\nSubstituting the SVD expression for $A^{\\dagger}$, we get:\n$$\n\\|e\\|_{2} = \\|V \\Sigma^{\\dagger} U^{\\mathsf{T}} \\eta\\|_{2}\n$$\nThe problem states that $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix. A fundamental property of orthogonal transformations is that they preserve the Euclidean norm. That is, for any vector $z \\in \\mathbb{R}^{n}$, $\\|Vz\\|_{2} = \\|z\\|_{2}$. Applying this property, we can remove the orthogonal matrix $V$ from the norm calculation:\n$$\n\\|e\\|_{2} = \\|\\Sigma^{\\dagger} U^{\\mathsf{T}} \\eta\\|_{2}\n$$\nNow, let us define a new vector $\\tilde{\\eta} \\in \\mathbb{R}^{m}$ as the projection of the noise vector $\\eta$ onto the basis of left singular vectors (the columns of $U$):\n$$\n\\tilde{\\eta} = U^{\\mathsf{T}} \\eta\n$$\nSince $U \\in \\mathbb{R}^{m \\times m}$ is also an orthogonal matrix, its transpose $U^{\\mathsf{T}}$ is orthogonal as well. Therefore, this transformation also preserves the Euclidean norm:\n$$\n\\|\\tilde{\\eta}\\|_{2} = \\|U^{\\mathsf{T}} \\eta\\|_{2} = \\|\\eta\\|_{2} = \\delta\n$$\nThe problem is now transformed into finding the maximum of $\\|\\Sigma^{\\dagger} \\tilde{\\eta}\\|_{2}$ subject to the constraint $\\|\\tilde{\\eta}\\|_{2} = \\delta$.\n\nLet's analyze the term $\\|\\Sigma^{\\dagger} \\tilde{\\eta}\\|_{2}$. The matrix $\\Sigma^{\\dagger}$ is a diagonal matrix of size $n \\times m$. Its first $r$ diagonal entries are the reciprocals of the positive singular values of $A$, i.e., $1/\\sigma_1, 1/\\sigma_2, \\dots, 1/\\sigma_r$. All other entries are zero. Let $\\tilde{\\eta} = (\\tilde{\\eta}_1, \\tilde{\\eta}_2, \\dots, \\tilde{\\eta}_m)^{\\mathsf{T}}$. The product $\\Sigma^{\\dagger} \\tilde{\\eta}$ is an $n$-dimensional vector:\n$$\n\\Sigma^{\\dagger} \\tilde{\\eta} = \\begin{pmatrix}\n1/\\sigma_1  0  \\dots  0  \\dots  0 \\\\\n0  1/\\sigma_2  \\dots  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots   \\vdots \\\\\n0  0  \\dots  1/\\sigma_r  \\dots  0 \\\\\n\\vdots  \\vdots   \\vdots  \\ddots  \\vdots \\\\\n0  0  \\dots  0  \\dots  0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\tilde{\\eta}_1 \\\\ \\tilde{\\eta}_2 \\\\ \\vdots \\\\ \\tilde{\\eta}_r \\\\ \\vdots \\\\ \\tilde{\\eta}_m\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\tilde{\\eta}_1 / \\sigma_1 \\\\\n\\tilde{\\eta}_2 / \\sigma_2 \\\\\n\\vdots \\\\\n\\tilde{\\eta}_r / \\sigma_r \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{pmatrix}\n$$\nThe squared norm of this vector is:\n$$\n\\|e\\|_{2}^{2} = \\|\\Sigma^{\\dagger} \\tilde{\\eta}\\|_{2}^{2} = \\sum_{i=1}^{r} \\left(\\frac{\\tilde{\\eta}_i}{\\sigma_i}\\right)^2 = \\sum_{i=1}^{r} \\frac{1}{\\sigma_i^2} \\tilde{\\eta}_i^2\n$$\nOur goal is to maximize this quantity subject to the constraint $\\|\\tilde{\\eta}\\|_{2}^2 = \\sum_{i=1}^{m} \\tilde{\\eta}_i^2 = \\delta^2$. Since the sum for $\\|e\\|_2^2$ only involves components up to $i=r$, any component $\\tilde{\\eta}_i$ for $i  r$ does not contribute to the error norm. To maximize the sum, we should set $\\tilde{\\eta}_i = 0$ for $ir$. The constraint becomes $\\sum_{i=1}^{r} \\tilde{\\eta}_i^2 = \\delta^2$.\n\nWe have the singular values ordered as $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r}  0$. Consequently, their squared reciprocals are ordered as:\n$$\n0  \\frac{1}{\\sigma_1^2} \\leq \\frac{1}{\\sigma_2^2} \\leq \\cdots \\leq \\frac{1}{\\sigma_r^2}\n$$\nTo maximize the weighted sum $\\sum_{i=1}^{r} (1/\\sigma_i^2) \\tilde{\\eta}_i^2$ subject to $\\sum_{i=1}^{r} \\tilde{\\eta}_i^2 = \\delta^2$, we must allocate the entire \"energy\" $\\delta^2$ of $\\tilde{\\eta}$ to the component $\\tilde{\\eta}_i$ that is multiplied by the largest coefficient. The largest coefficient is $1/\\sigma_r^2$.\n\nTherefore, the maximum is achieved by choosing $\\tilde{\\eta}_r^2 = \\delta^2$ and $\\tilde{\\eta}_i^2 = 0$ for all $i \\neq r$. This configuration satisfies the constraint. For this choice, the squared error norm is:\n$$\n\\|e\\|_{2, \\text{max}}^{2} = \\frac{1}{\\sigma_r^2} \\tilde{\\eta}_r^2 = \\frac{\\delta^2}{\\sigma_r^2}\n$$\nTaking the square root, we find the maximum possible value of the error norm:\n$$\n\\|e\\|_{2, \\text{max}} = \\frac{\\delta}{\\sigma_r}\n$$\n\nTo realize this maximum, we chose $\\tilde{\\eta}$ such that $\\tilde{\\eta}_r = \\pm\\delta$ and all other components are zero. Let's denote the canonical basis vector in $\\mathbb{R}^m$ with a $1$ in the $i$-th position as $e'_i$. Then, the maximizing transformed noise vector is $\\tilde{\\eta}_{\\text{max}} = \\delta e'_r$.\nThe noise in the original data space, $\\eta \\in \\mathbb{R}^m$, is found by transforming back:\n$$\n\\eta_{\\text{max}} = U \\tilde{\\eta}_{\\text{max}} = U (\\delta e'_r) = \\delta (U e'_r) = \\delta u_r\n$$\nwhere $u_r$ is the $r$-th column of $U$, which is the left singular vector of $A$ corresponding to the singular value $\\sigma_r$. Thus, the worst-case noise is aligned with the left singular vector corresponding to the smallest positive singular value $\\sigma_r$.\n\nThe resulting reconstruction error vector $e \\in \\mathbb{R}^n$ is:\n$$\ne_{\\text{max}} = A^{\\dagger} \\eta_{\\text{max}} = V \\Sigma^{\\dagger} U^{\\mathsf{T}} (\\delta u_r)\n$$\nSince the columns of $U$ are orthonormal, $U^{\\mathsf{T}} u_r = e'_r$. The expression becomes:\n$$\ne_{\\text{max}} = V \\Sigma^{\\dagger} (\\delta e'_r) = \\delta (V (\\Sigma^{\\dagger} e'_r))\n$$\nThe vector $\\Sigma^{\\dagger} e'_r$ is a vector in $\\mathbb{R}^n$ with its $r$-th component equal to $1/\\sigma_r$ and all other components zero. Let's call this vector $(1/\\sigma_r) e''_r$, where $e''_r$ is the $r$-th canonical basis vector in $\\mathbb{R}^n$.\n$$\ne_{\\text{max}} = \\delta V \\left(\\frac{1}{\\sigma_r} e''_r\\right) = \\frac{\\delta}{\\sigma_r} (V e''_r) = \\frac{\\delta}{\\sigma_r} v_r\n$$\nwhere $v_r$ is the $r$-th column of $V$, i.e., the right singular vector of $A$ corresponding to $\\sigma_r$.\nThis shows that the worst-case reconstruction error $e_{\\text{max}}$ is aligned with the right singular vector associated with the smallest positive singular value $\\sigma_r$.\n\nIn the context of data assimilation, this result illustrates the mechanism of error amplification in ill-conditioned inverse problems. The right singular vector $v_r$ represents a direction in the state space that is \"least observable,\" because its mapping into the observation space, $A v_r = \\sigma_r u_r$, is scaled by the small singular value $\\sigma_r$. The worst-case observation error $\\eta_{\\text{max}} = \\delta u_r$ has a spatial structure that perfectly mimics the faint observational signature of this poorly-observed state component. The reconstruction process, which involves the inverse operator $A^{\\dagger}$, misinterprets this noise as a very large signal component in the direction of $v_r$. The noise with magnitude $\\delta$ is amplified by the factor $1/\\sigma_r$, leading to a large reconstruction error of magnitude $(\\delta/\\sigma_r)$ aligned with the state space direction $v_r$. This amplification factor $1/\\sigma_r$ is precisely the operator norm $\\|A^{\\dagger}\\|_2$, which quantifies the sensitivity of the solution to perturbations in the data.",
            "answer": "$$\n\\boxed{\\frac{\\delta}{\\sigma_{r}}}\n$$"
        },
        {
            "introduction": "In many scientific applications, such as determining chemical concentrations or population fractions, solutions must satisfy physical constraints like non-negativity. The standard Moore-Penrose pseudoinverse solution, which only minimizes the Euclidean norm, often yields results that are mathematically optimal but physically nonsensical. This problem presents a hypothetical data assimilation scenario to demonstrate this conflict and introduces a practical remedy: projecting the unphysical solution onto the set of feasible states .",
            "id": "3404374",
            "problem": "Consider a linear data assimilation model with state vector $x \\in \\mathbb{R}^{3}$ representing mixing fractions of three chemical species. Physical feasibility requires the bounds $x_{i} \\ge 0$ for all $i$ and conservation $\\sum_{i=1}^{3} x_{i} = 1$. Let the forward operator be the matrix\n$$\nA(\\varepsilon) \\;=\\; \\begin{pmatrix}\n1  1  1 \\\\\n0  \\varepsilon  2\\varepsilon\n\\end{pmatrix},\n$$\nwith a small parameter $\\varepsilon  0$ (so the second row is much weaker than the first), and suppose we assimilate the observation vector\n$$\ny \\;=\\; \\begin{pmatrix} a \\\\ b \\end{pmatrix},\n$$\nwhere $a = \\frac{102}{100}$ and $b = \\frac{1}{4000}$, and $\\varepsilon = \\frac{1}{1000}$. Start from the defining characterization of the Moore–Penrose pseudoinverse as the unique minimum-Euclidean-norm solution,\n$$\nx^{\\dagger} \\;=\\; \\underset{x \\in \\mathbb{R}^{3}}{\\arg\\min}\\;\\|x\\|_{2} \\quad \\text{subject to} \\quad A(\\varepsilon)\\,x = y.\n$$\nUsing only this definition and first principles, derive $x^{\\dagger}$ exactly and identify which physical constraints it violates, if any. As a remedy, enforce feasibility by projecting $x^{\\dagger}$ onto the probability simplex $\\mathcal{S} = \\{x \\in \\mathbb{R}^{3} : x_{i} \\ge 0,\\; \\sum_{i=1}^{3} x_{i} = 1\\}$ via the Euclidean projection\n$$\nx^{\\mathrm{proj}} \\;=\\; \\underset{x \\in \\mathcal{S}}{\\arg\\min}\\;\\|x - x^{\\dagger}\\|_{2}.\n$$\nCompute, in exact arithmetic, the squared Euclidean correction magnitude\n$$\n\\Delta^{2} \\;=\\; \\bigl\\|x^{\\dagger} - x^{\\mathrm{proj}}\\bigr\\|_{2}^{2}.\n$$\nProvide your final answer as a single exact number. No units are required. If you choose to approximate, round your answer to four significant figures.",
            "solution": "The first step is to compute the minimum-Euclidean-norm solution $x^{\\dagger}$ that satisfies the linear system $A(\\varepsilon)x = y$. This is a constrained optimization problem: minimize $f(x) = \\|x\\|_{2}^{2} = x^{T}x$ subject to the constraint $A(\\varepsilon)x - y = 0$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(x, \\lambda) = x^{T}x + \\lambda^{T}(A(\\varepsilon)x - y)\n$$\nwhere $\\lambda \\in \\mathbb{R}^{2}$ is the vector of Lagrange multipliers. To find the minimum, we set the gradient with respect to $x$ to zero:\n$$\n\\nabla_{x} \\mathcal{L} = 2x + A(\\varepsilon)^{T}\\lambda = 0 \\quad \\implies \\quad x = -\\frac{1}{2}A(\\varepsilon)^{T}\\lambda\n$$\nSubstituting this into the constraint equation $A(\\varepsilon)x = y$:\n$$\nA(\\varepsilon) \\left(-\\frac{1}{2}A(\\varepsilon)^{T}\\lambda\\right) = y \\quad \\implies \\quad -\\frac{1}{2}A(\\varepsilon)A(\\varepsilon)^{T}\\lambda = y\n$$\nSince $A(\\varepsilon)$ has full row rank for $\\varepsilon  0$, the matrix $A(\\varepsilon)A(\\varepsilon)^{T}$ is invertible. We can solve for $\\lambda$:\n$$\n\\lambda = -2 \\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1}y\n$$\nSubstituting $\\lambda$ back into the expression for $x$ gives the solution $x^{\\dagger}$:\n$$\nx^{\\dagger} = -\\frac{1}{2}A(\\varepsilon)^{T} \\left(-2 \\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1}y\\right) = A(\\varepsilon)^{T}\\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1}y\n$$\nThis is the standard formula for the Moore-Penrose pseudoinverse $A(\\varepsilon)^{+}$ applied to $y$. Let's compute the matrices:\n$$\nA(\\varepsilon)A(\\varepsilon)^{T} = \\begin{pmatrix} 1  1  1 \\\\ 0  \\varepsilon  2\\varepsilon \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  \\varepsilon \\\\ 1  2\\varepsilon \\end{pmatrix} = \\begin{pmatrix} 3  3\\varepsilon \\\\ 3\\varepsilon  5\\varepsilon^{2} \\end{pmatrix}\n$$\nThe determinant is $\\det(A(\\varepsilon)A(\\varepsilon)^{T}) = (3)(5\\varepsilon^{2}) - (3\\varepsilon)(3\\varepsilon) = 15\\varepsilon^{2} - 9\\varepsilon^{2} = 6\\varepsilon^{2}$. The inverse is:\n$$\n\\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2}  -3\\varepsilon \\\\ -3\\varepsilon  3 \\end{pmatrix}\n$$\nNow we compute $x^{\\dagger}$:\n$$\nx^{\\dagger} = \\begin{pmatrix} 1  0 \\\\ 1  \\varepsilon \\\\ 1  2\\varepsilon \\end{pmatrix} \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2}  -3\\varepsilon \\\\ -3\\varepsilon  3 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix}\n$$\n$$\nx^{\\dagger} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 1  0 \\\\ 1  \\varepsilon \\\\ 1  2\\varepsilon \\end{pmatrix} \\begin{pmatrix} 5\\varepsilon^{2}a - 3\\varepsilon b \\\\ -3\\varepsilon a + 3b \\end{pmatrix}\n$$\n$$\nx^{\\dagger} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2}a - 3\\varepsilon b \\\\ (5\\varepsilon^{2}a - 3\\varepsilon b) + \\varepsilon(-3\\varepsilon a + 3b) \\\\ (5\\varepsilon^{2}a - 3\\varepsilon b) + 2\\varepsilon(-3\\varepsilon a + 3b) \\end{pmatrix} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2}a - 3\\varepsilon b \\\\ 2\\varepsilon^{2}a \\\\ -\\varepsilon^{2}a + 3\\varepsilon b \\end{pmatrix}\n$$\nSimplifying the components of $x^{\\dagger} = (x_{1}^{\\dagger}, x_{2}^{\\dagger}, x_{3}^{\\dagger})^{T}$:\n$$\nx_{1}^{\\dagger} = \\frac{5}{6}a - \\frac{1}{2\\varepsilon}b, \\quad x_{2}^{\\dagger} = \\frac{1}{3}a, \\quad x_{3}^{\\dagger} = -\\frac{1}{6}a + \\frac{1}{2\\varepsilon}b\n$$\nWe substitute the given values $a = \\frac{102}{100}$, $b = \\frac{1}{4000}$, and $\\varepsilon = \\frac{1}{1000}$.\nNote that $\\frac{1}{2\\varepsilon} = \\frac{1}{2(1/1000)} = 500$.\n$$\nx_{1}^{\\dagger} = \\frac{5}{6}\\left(\\frac{102}{100}\\right) - 500\\left(\\frac{1}{4000}\\right) = \\frac{17}{20} - \\frac{1}{8} = \\frac{34}{40} - \\frac{5}{40} = \\frac{29}{40}\n$$\n$$\nx_{2}^{\\dagger} = \\frac{1}{3}\\left(\\frac{102}{100}\\right) = \\frac{34}{100} = \\frac{17}{50}\n$$\n$$\nx_{3}^{\\dagger} = -\\frac{1}{6}\\left(\\frac{102}{100}\\right) + 500\\left(\\frac{1}{4000}\\right) = -\\frac{17}{100} + \\frac{1}{8} = -\\frac{34}{200} + \\frac{25}{200} = -\\frac{9}{200}\n$$\nSo, the unconstrained solution is $x^{\\dagger} = \\left(\\frac{29}{40}, \\frac{17}{50}, -\\frac{9}{200}\\right)^{T}$.\n\nNext, we check this solution against the physical constraints.\n1. Non-negativity ($x_{i} \\ge 0$): $x_{1}^{\\dagger} = \\frac{29}{40}  0$, $x_{2}^{\\dagger} = \\frac{17}{50}  0$, but $x_{3}^{\\dagger} = -\\frac{9}{200}  0$. The non-negativity constraint is violated for the third component.\n2. Conservation ($\\sum x_i = 1$): The first row of the constraint $A(\\varepsilon)x=y$ is $\\sum x_{i} = a$. With $a = \\frac{102}{100} = 1.02$, we have $\\sum x_i^{\\dagger} = 1.02 \\neq 1$. The conservation constraint is violated.\n\nTo enforce feasibility, we project $x^{\\dagger}$ onto the probability simplex $\\mathcal{S}$. We need to compute $x^{\\mathrm{proj}} = \\underset{x \\in \\mathcal{S}}{\\arg\\min}\\;\\|x - x^{\\dagger}\\|_{2}$. The standard algorithm for this projection involves sorting the components of $x^{\\dagger}$ and finding a threshold $\\tau$.\nThe components of $x^{\\dagger}$ are $\\frac{29}{40} = 0.725$, $\\frac{17}{50} = 0.34$, and $-\\frac{9}{200} = -0.045$.\nLet $z = x^{\\dagger}$. The components in descending order are $z_{(1)} = \\frac{29}{40}$, $z_{(2)} = \\frac{17}{50}$, $z_{(3)} = -\\frac{9}{200}$.\nWe find $\\rho = \\max\\left\\{ j \\in \\{1, 2, 3\\} \\, \\Big| \\, z_{(j)} - \\frac{1}{j}\\left(\\sum_{k=1}^{j} z_{(k)} - 1\\right)  0 \\right\\}$.\nFor $j=1$: $z_{(1)} - (z_{(1)} - 1) = 1  0$.\nFor $j=2$: Let's calculate the term.\n$\\sum_{k=1}^{2} z_{(k)} = \\frac{29}{40} + \\frac{17}{50} = \\frac{145}{200} + \\frac{68}{200} = \\frac{213}{200}$.\n$z_{(2)} - \\frac{1}{2}\\left(\\frac{213}{200} - 1\\right) = \\frac{17}{50} - \\frac{1}{2}\\left(\\frac{13}{200}\\right) = \\frac{136}{400} - \\frac{13}{400} = \\frac{123}{400}  0$.\nFor $j=3$:\n$\\sum_{k=1}^{3} z_{(k)} = \\sum_{i=1}^{3} x_{i}^{\\dagger} = a = \\frac{102}{100} = \\frac{204}{200}$.\n$z_{(3)} - \\frac{1}{3}\\left(\\frac{204}{200} - 1\\right) = -\\frac{9}{200} - \\frac{1}{3}\\left(\\frac{4}{200}\\right) = -\\frac{27}{600} - \\frac{4}{600} = -\\frac{31}{600}  0$.\nThe maximum such $j$ is $\\rho = 2$.\nThe threshold $\\tau$ is given by $\\tau = \\frac{1}{\\rho}\\left(\\sum_{k=1}^{\\rho} z_{(k)} - 1\\right)$.\n$\\tau = \\frac{1}{2}\\left(\\frac{213}{200} - 1\\right) = \\frac{13}{400}$.\nThe projected solution $x^{\\mathrm{proj}}$ has components $x_{i}^{\\mathrm{proj}} = \\max(x_{i}^{\\dagger} - \\tau, 0)$.\n$x_{1}^{\\mathrm{proj}} = \\max\\left(\\frac{29}{40} - \\frac{13}{400}, 0\\right) = \\max\\left(\\frac{290-13}{400}, 0\\right) = \\frac{277}{400}$.\n$x_{2}^{\\mathrm{proj}} = \\max\\left(\\frac{17}{50} - \\frac{13}{400}, 0\\right) = \\max\\left(\\frac{136-13}{400}, 0\\right) = \\frac{123}{400}$.\n$x_{3}^{\\mathrm{proj}} = \\max\\left(-\\frac{9}{200} - \\frac{13}{400}, 0\\right) = \\max\\left(\\frac{-18-13}{400}, 0\\right) = \\max\\left(-\\frac{31}{400}, 0\\right) = 0$.\nThe projected solution is $x^{\\mathrm{proj}} = \\left(\\frac{277}{400}, \\frac{123}{400}, 0\\right)^{T}$.\n\nFinally, we compute the squared Euclidean correction magnitude $\\Delta^{2} = \\|x^{\\dagger} - x^{\\mathrm{proj}}\\|_{2}^{2}$.\nThe difference vector is $x^{\\dagger} - x^{\\mathrm{proj}}$:\n$$\nx^{\\dagger} - x^{\\mathrm{proj}} = \\begin{pmatrix} 29/40 \\\\ 17/50 \\\\ -9/200 \\end{pmatrix} - \\begin{pmatrix} 277/400 \\\\ 123/400 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 290/400 - 277/400 \\\\ 136/400 - 123/400 \\\\ -18/400 - 0 \\end{pmatrix} = \\begin{pmatrix} 13/400 \\\\ 13/400 \\\\ -18/400 \\end{pmatrix}\n$$\nThe squared norm of this vector is:\n$$\n\\Delta^{2} = \\left(\\frac{13}{400}\\right)^{2} + \\left(\\frac{13}{400}\\right)^{2} + \\left(-\\frac{18}{400}\\right)^{2}\n$$\n$$\n\\Delta^{2} = \\frac{13^{2} + 13^{2} + 18^{2}}{400^{2}} = \\frac{169 + 169 + 324}{160000} = \\frac{338 + 324}{160000} = \\frac{662}{160000}\n$$\nSimplifying the fraction by dividing the numerator and denominator by $2$:\n$$\n\\Delta^{2} = \\frac{331}{80000}\n$$\nThe numerator $331$ is a prime number, so this fraction is in its simplest form. This is the exact value for the squared correction magnitude.",
            "answer": "$$\\boxed{\\frac{331}{80000}}$$"
        }
    ]
}