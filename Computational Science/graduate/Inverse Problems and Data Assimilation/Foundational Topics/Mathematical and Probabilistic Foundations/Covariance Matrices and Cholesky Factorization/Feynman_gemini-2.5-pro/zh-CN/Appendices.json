{
    "hands_on_practices": [
        {
            "introduction": "在将 Cholesky 分解应用于复杂算法之前，掌握其基本机理至关重要。这个基础练习提供了一个直接的纸笔练习，旨在建立对该算法的直观理解。你将根据一个常见的空间核函数构建一个协方差矩阵，然后逐步应用 Cholesky 分解算法，从而加深对一个对称正定矩阵如何被分解为 $L L^\\top$ 乘积的认识 。",
            "id": "950018",
            "problem": "考虑一条线上的三个不同点，其位置分别为 $x_1 = 0$，$x_2 = 1$ 和 $x_3 = 3$。任意两点 $x_i$ 和 $x_j$ 之间的协方差由核函数 $k(x_i, x_j) = \\exp(-|x_i - x_j|)$ 给出。构建这些点的 $3 \\times 3$ 协方差矩阵 $A$，并计算其 Cholesky 分解，即找到下三角矩阵 $L$ 使得 $A = LL^\\top$。将 $L$ 的所有元素表示为精确的简化形式。",
            "solution": "我们寻求矩阵\n$$A=\\bigl[a_{ij}\\bigr]=\\begin{pmatrix}\n1  e^{-1}  e^{-3}\\\\\ne^{-1}  1  e^{-2}\\\\\ne^{-3}  e^{-2}  1\n\\end{pmatrix}$$\n的 Cholesky 因子 $L$，使得 $A=LL^\\top$，其中\n$$L=\\begin{pmatrix}\n\\ell_{11}  0  0\\\\\n\\ell_{21}  \\ell_{22}  0\\\\\n\\ell_{31}  \\ell_{32}  \\ell_{33}\n\\end{pmatrix}.$$\n1. 从 $a_{11}=\\ell_{11}^2=1$ 我们得到\n   $$\\ell_{11}=1.$$\n2. 从 $a_{21}=\\ell_{21}\\ell_{11}=e^{-1}$ 和 $a_{31}=\\ell_{31}\\ell_{11}=e^{-3}$ 我们得到\n   $$\\ell_{21}=e^{-1},\\quad\\ell_{31}=e^{-3}.$$\n3. 从 $a_{22}=\\ell_{21}^2+\\ell_{22}^2=1$ 我们得到\n   $$\\ell_{22}=\\sqrt{1-e^{-2}}.$$\n4. 从 $a_{32}=\\ell_{31}\\ell_{21}+\\ell_{32}\\ell_{22}=e^{-2}$ 我们解得\n   $$\\ell_{32}=\\frac{e^{-2}-e^{-3}e^{-1}}{\\sqrt{1-e^{-2}}}\n              =\\frac{e^{-2}-e^{-4}}{\\sqrt{1-e^{-2}}}\n              =e^{-2}\\sqrt{1-e^{-2}}.$$\n5. 最后，从\n   $$a_{33}=\\ell_{31}^2+\\ell_{32}^2+\\ell_{33}^2=1$$\n   我们有\n   $$\\ell_{33}\n     =\\sqrt{1-e^{-6}-\\bigl(e^{-2}\\sqrt{1-e^{-2}}\\bigr)^2}\n     =\\sqrt{1-e^{-4}}.$$\n因此 Cholesky 因子为\n$$L=\\begin{pmatrix}\n1  0  0\\\\\ne^{-1}  \\sqrt{1-e^{-2}}  0\\\\\ne^{-3}  e^{-2}\\sqrt{1-e^{-2}}  \\sqrt{1-e^{-4}}\n\\end{pmatrix}.$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\n1  0  0\\\\\ne^{-1}  \\sqrt{1-e^{-2}}  0\\\\\ne^{-3}  e^{-2}\\sqrt{1-e^{-2}}  \\sqrt{1-e^{-4}}\n\\end{pmatrix}}$$"
        },
        {
            "introduction": "Cholesky 分解在数据同化和贝叶斯统计中最强大的应用之一，是高效地从多元正态分布中生成样本。这种从简单的非相关噪声中创建相关随机场的能力，是不确定性量化的基石。在这个计算练习中，你将使用 Cholesky 因子 $L$ 来变换标准正态向量，从而实现一个采样器，并直观地展示 $x = m + L z$ 这一核心原理。这项动手编码任务将巩固代数分解与其在蒙特卡洛方法中实际应用之间的联系 。",
            "id": "3373513",
            "problem": "考虑为线性高斯逆问题和序贯数据同化生成系综，其中必须从具有指定均值和协方差的多元正态分布中抽取样本。令 $m \\in \\mathbb{R}^n$ 为给定的均值向量，令 $C \\in \\mathbb{R}^{n \\times n}$ 为一个对称正定协方差矩阵。一个基本事实是，如果 $F \\in \\mathbb{R}^{n \\times n}$ 满足 $F F^\\top = C$，且 $z \\in \\mathbb{R}^n$ 是一个分量独立同分布的标准正态向量，那么形式为 $x = m + F z$ 的仿射变换就是从均值为 $m$、协方差为 $C$ 的高斯分布中抽取的一个样本。$F$ 的两种值得注意的选择是 Cholesky 因子（它是下三角矩阵且满足 $C = L L^\\top$）和对称平方根（它满足 $C^{1/2} C^{1/2} = C$ 且 $C^{1/2} = (C^{1/2})^\\top$）。对称平方根可以通过谱定理，利用 $C$ 的特征值分解来构建。\n\n您的任务是，通过使用下三角 Cholesky 因子和对称平方根变换标准正态向量，实现一个用于多元正态分布的采样器，并通过一阶矩和二阶矩的蒙特卡洛估计来比较它们的经验性能。推导和实现必须基于以下基本事实：期望的线性性、协方差的定义、对称矩阵的谱定理，以及高斯向量的仿射像仍是高斯的性质。\n\n对于下文定义的每个测试用例，请执行以下操作：\n- 按照规定构建协方差矩阵 $C$。\n- 计算下三角 Cholesky 因子 $L$，使得 $C = L L^\\top$。\n- 使用 $C$ 的特征值分解计算对称平方根 $C^{1/2}$。\n- 在 $\\mathbb{R}^n$ 中生成 $N$ 个独立同分布的标准正态向量 $z$，在给定的测试用例中对两种方法使用相同的固定种子以保证可复现性。对两种方法使用相同的标准正态抽样矩阵，以减少比较中的方差。\n- 形成两组样本：\n  - $x_L = m + L z$,\n  - $x_S = m + C^{1/2} z$,\n  其中 $z$ 被理解为一个 $n \\times N$ 矩阵，而 $m$ 的加法是按列进行的。\n- 使用已知的真实均值 $m$ 计算经验矩误差，以避免在计算协方差时使用样本均值所带来的偏差：\n  - 经验均值误差 $e_{m,L} = \\lVert \\hat{m}_L - m \\rVert_2$ 和 $e_{m,S} = \\lVert \\hat{m}_S - m \\rVert_2$，其中 $\\hat{m}_\\bullet$ 表示相应样本矩阵的按列样本均值。\n  - 相对于真实均值 $m$ 的经验协方差矩阵：$\\hat{C}_L = \\frac{1}{N}\\sum_{k=1}^N (x_{L,k} - m)(x_{L,k} - m)^\\top$ 和 $\\hat{C}_S = \\frac{1}{N}\\sum_{k=1}^N (x_{S,k} - m)(x_{S,k} - m)^\\top$。\n  - 经验协方差误差 $e_{C,L} = \\lVert \\hat{C}_L - C \\rVert_F$ 和 $e_{C,S} = \\lVert \\hat{C}_S - C \\rVert_F$。\n- 为每个测试用例报告单个浮点数 $r = \\frac{e_{C,L}}{e_{C,S}}$。数量 $r$ 比较了基于 Cholesky 的采样器和基于对称平方根的采样器之间二阶矩蒙特卡洛估计的准确性。$r$ 的值接近 $1$ 表示性能相似。\n\n测试套件。使用以下四个测试用例。在所有情况下，在开始处理测试套件时，将用于标准正态抽样的随机数生成器种子设置为 $1729$，并在给定的测试用例中对两种采样方法使用相同的抽样。当需要一个随机矩阵 $A$ 来构造 $C$ 时，请使用为该测试用例指定的独立的固定种子来构造 $A$，并且在处理主要的标准正态抽样时不要更改它。\n\n- 测试用例 1（理想情况，非对角，良态）：\n  - 维度 $n = 4$。\n  - 均值 $m = [1, -1, 0.5, 2]^\\top$。\n  - 通过使用种子 $0$ 抽取独立同分布的标准正态分量来构造 $A \\in \\mathbb{R}^{4 \\times 4}$（根据您的语言默认，按列主序或行主序），并设置 $C = A A^\\top + \\alpha I$，其中 $\\alpha = 0.5$。\n  - 样本数量 $N = 200000$。\n\n- 测试用例 2（边界情况，标量）：\n  - 维度 $n = 1$。\n  - 均值 $m = [0.0]^\\top$。\n  - 协方差 $C = [2.5]$。\n  - 样本数量 $N = 500000$。\n\n- 测试用例 3（病态对角协方差）：\n  - 维度 $n = 5$。\n  - 均值 $m = [-2.0, 0.1, 0.0, 1.5, 0.0]^\\top$。\n  - 协方差 $C = \\operatorname{diag}(2.0, 10^{-3}, 5 \\cdot 10^{-2}, 3.0, 10^{-6})$。\n  - 样本数量 $N = 250000$。\n\n- 测试用例 4（更高维度，近奇异正则化协方差）：\n  - 维度 $n = 8$。\n  - 均值 $m = [0, 0, 0, 0, 0, 0, 0, 0]^\\top$。\n  - 通过使用种子 $123$ 抽取独立同分布的标准正态分量来构造 $A \\in \\mathbb{R}^{8 \\times 8}$，并设置 $C = A A^\\top + \\varepsilon I$，其中 $\\varepsilon = 10^{-6}$。\n  - 样本数量 $N = 200000$。\n\n最终输出格式。您的程序应生成单行输出，其中包含按顺序排列的四个测试用例对应的四个结果，格式为方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$。每个 $r_j$ 都必须是按上述规定计算的浮点数。不应打印任何其他文本。",
            "solution": "该问题要求实现并比较两种从多元正态分布 $N(m, C)$ 生成样本的方法，其中 $m \\in \\mathbb{R}^n$ 是均值向量，$C \\in \\mathbb{R}^{n \\times n}$ 是对称正定协方差矩阵。比较是基于从有限数量样本中经验估计的二阶矩的准确性。\n\n生成多元正态样本的一个关键原则是高斯分布的仿射变换性质。如果 $z \\in \\mathbb{R}^n$ 是一个其分量为独立同分布（i.i.d.）标准正态变量的随机向量（即 $z \\sim N(0, I)$，其中 $I$ 是单位矩阵），那么对于某个矩阵 $F \\in \\mathbb{R}^{n \\times n}$，变换后的向量 $x = m + Fz$ 也服从正态分布。其均值和协方差由以下公式给出：\n- 均值：$E[x] = E[m + Fz] = m + F E[z] = m + F \\cdot 0 = m$。这源于期望算子的线性性。\n- 协方差：$\\text{Cov}(x) = E[(x - E[x])(x - E[x])^\\top] = E[(m + Fz - m)(m + Fz - m)^\\top] = E[(Fz)(Fz)^\\top] = E[Fzz^\\top F^\\top]$。由于期望算子是线性的，我们可以将其写为 $F E[zz^\\top] F^\\top$。$z$ 的协方差矩阵是 $E[zz^\\top] - E[z]E[z]^\\top = E[zz^\\top] = I$。因此，$\\text{Cov}(x) = F I F^\\top = FF^\\top$。\n\n为了从目标分布 $N(m, C)$ 生成样本，我们必须找到一个矩阵 $F$ 使得 $FF^\\top = C$。问题指定了 $F$ 的两种选择：\n\n1.  **Cholesky 因子 ($L$)**：对于任何对称正定矩阵 $C$，存在一个唯一的、对角线元素为正的下三角矩阵 $L$，使得 $C = LL^\\top$。这就是 Cholesky 分解。第一种采样方法使用此因子，通过 $x_L = m + Lz$ 生成样本。\n\n2.  **对称平方根 ($C^{1/2}$)**：对于任何对称正定矩阵 $C$，存在一个唯一的对称正定矩阵，记作 $C^{1/2}$，使得 $C = C^{1/2}C^{1/2}$。由于 $C^{1/2}$ 是对称的，所以 $(C^{1/2})^\\top = C^{1/2}$，因此 $C = C^{1/2}(C^{1/2})^\\top$ 也成立。这个矩阵可以通过谱定理来构建。一个对称矩阵 $C$ 有一个特征分解 $C = VDV^\\top$，其中 $V$ 是一个正交矩阵，其列是 $C$ 的特征向量，$D$ 是对应特征值 $\\lambda_i$ 构成的对角矩阵。由于 $C$ 是正定的，所有的 $\\lambda_i > 0$。对称平方根则由 $C^{1/2} = VD^{1/2}V^\\top$ 给出，其中 $D^{1/2}$ 是对角线元素为 $\\sqrt{\\lambda_i}$ 的对角矩阵。第二种采样方法使用此因子，通过 $x_S = m + C^{1/2}z$ 生成样本。\n\n对于有限数量的样本 $N$，我们可以估计分布的矩。问题指定了计算相对于真实均值 $m$ 的经验协方差矩阵，这是一个无偏估计量：\n$$\n\\hat{C}_\\bullet = \\frac{1}{N}\\sum_{k=1}^N (x_{\\bullet,k} - m)(x_{\\bullet,k} - m)^\\top\n$$\n其中 $\\bullet$ 是 $L$ (Cholesky) 或 $S$ (Symmetric) 的占位符。这些估计的准确性由误差矩阵 $E_\\bullet = \\hat{C}_\\bullet - C$ 的弗罗贝尼乌斯范数来衡量：\n$$\ne_{C,\\bullet} = \\lVert \\hat{C}_\\bullet - C \\rVert_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n (E_{\\bullet, ij})^2}\n$$\n问题的核心是通过计算比率 $r = e_{C,L} / e_{C,S}$ 来计算和比较这些误差。两种方法使用同一组 $N$ 个标准正态向量 $\\{z_k\\}_{k=1}^N$，以确保公平的比较。\n\n采样误差与 $F$ 的选择之间的关系可以通过将 $x_k - m = F z_k$ 代入 $\\hat{C}$ 的估计量中看出：\n$$\n\\hat{C} = \\frac{1}{N} \\sum_{k=1}^N (F z_k)(F z_k)^\\top = F \\left( \\frac{1}{N} \\sum_{k=1}^N z_k z_k^\\top \\right) F^\\top = F \\hat{C}_z F^\\top\n$$\n其中 $\\hat{C}_z$ 是标准正态样本的经验协方差。因此，误差为：\n$$\n\\hat{C} - C = F \\hat{C}_z F^\\top - F F^\\top = F (\\hat{C}_z - I) F^\\top\n$$\n因此，问题比较的是 $\\lVert L(\\hat{C}_z - I)L^\\top \\rVert_F$ 与 $\\lVert C^{1/2}(\\hat{C}_z - I)(C^{1/2})^\\top \\rVert_F$。虽然 $L$ 和 $C^{1/2}$ 通常是不同的矩阵，但在特殊情况下，它们是相同的。对于标量协方差 $C=[\\sigma^2]$，$L=C^{1/2}=[\\sigma]$。对于对角协方差矩阵 $C=\\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$，$L$ 和 $C^{1/2}$ 都等于 $\\operatorname{diag}(\\sigma_1, \\dots, \\sigma_n)$。在这些情况下，我们预期误差会相同，比率 $r$ 会是 $1.0$。对于一般的非对角矩阵 $C$，$L \\neq C^{1/2}$，比率 $r$ 可能会偏离 $1.0$，这反映了 $L$（下三角）和 $C^{1/2}$（稠密对称）的不同结构如何传播有限样本误差 $(\\hat{C}_z - I)$。\n\n实现过程是遍历四个测试用例。对于每个用例：\n1.  设置指定的参数 $n$、$m$、$N$，并按定义构造协方差矩阵 $C$。\n2.  使用 `numpy.linalg.cholesky` 计算下三角 Cholesky 因子 $L$。\n3.  使用 `scipy.linalg.sqrtm` 计算对称平方根 $C^{1/2}$。\n4.  使用固定的随机种子生成一个包含 $N$ 次标准正态抽样的矩阵 $Z \\in \\mathbb{R}^{n \\times N}$，以保证可复现性。\n5.  创建两组样本 $X_L = m + LZ$ 和 $X_S = m + C^{1/2}Z$。使用广播将均值向量 $m$ 加到每一列上。\n6.  经验协方差矩阵 $\\hat{C}_L$ 和 $\\hat{C}_S$ 被高效地计算为 $\\frac{1}{N}(X_\\bullet - m)(X_\\bullet - m)^\\top$。\n7.  使用 `numpy.linalg.norm` 计算误差矩阵的弗罗贝尼乌斯范数 $e_{C,L} = \\lVert \\hat{C}_L - C \\rVert_F$ 和 $e_{C,S} = \\lVert \\hat{C}_S - C \\rVert_F$。\n8.  计算并存储比率 $r = e_{C,L} / e_{C,S}$。\n\n最后，将所有测试用例收集到的比率按指定格式打印出来。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import sqrtm\n\ndef solve():\n    \"\"\"\n    Implements and compares Cholesky and symmetric square root samplers \n    for multivariate normal distributions across four test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1: Well-conditioned\",\n            \"n\": 4,\n            \"m\": np.array([1.0, -1.0, 0.5, 2.0]),\n            \"N\": 200000,\n            \"ctype\": \"random_regularized\",\n            \"cparams\": {\"seed\": 0, \"reg\": 0.5},\n        },\n        {\n            \"name\": \"Case 2: Scalar\",\n            \"n\": 1,\n            \"m\": np.array([0.0]),\n            \"N\": 500000,\n            \"ctype\": \"scalar\",\n            \"cparams\": {\"val\": 2.5},\n        },\n        {\n            \"name\": \"Case 3: Ill-conditioned diagonal\",\n            \"n\": 5,\n            \"m\": np.array([-2.0, 0.1, 0.0, 1.5, 0.0]),\n            \"N\": 250000,\n            \"ctype\": \"diag\",\n            \"cparams\": {\"diag_vals\": np.array([2.0, 1e-3, 5e-2, 3.0, 1e-6])},\n        },\n        {\n            \"name\": \"Case 4: Near-singular regularized\",\n            \"n\": 8,\n            \"m\": np.array([0.0] * 8),\n            \"N\": 200000,\n            \"ctype\": \"random_regularized\",\n            \"cparams\": {\"seed\": 123, \"reg\": 1e-6},\n        },\n    ]\n\n    # Master random number generator for standard normal draws (z).\n    # Seeded once for reproducibility across all test cases.\n    rng_z = np.random.default_rng(1729)\n\n    results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        m = case[\"m\"]\n        N = case[\"N\"]\n\n        # Construct covariance matrix C based on test case specs.\n        if case[\"ctype\"] == \"random_regularized\":\n            # Use an independent RNG for constructing C to not affect the main draws.\n            rng_A = np.random.default_rng(case[\"cparams\"][\"seed\"])\n            A = rng_A.standard_normal((n, n))\n            C = A @ A.T + case[\"cparams\"][\"reg\"] * np.eye(n)\n        elif case[\"ctype\"] == \"scalar\":\n            C = np.array([[case[\"cparams\"][\"val\"]]])\n        elif case[\"ctype\"] == \"diag\":\n            C = np.diag(case[\"cparams\"][\"diag_vals\"])\n\n        # Compute the lower-triangular Cholesky factor L.\n        L = np.linalg.cholesky(C)\n\n        # Compute the symmetric square root C^(1/2).\n        # For a symmetric positive definite matrix, sqrtm returns the unique\n        # symmetric positive definite root, which is real.\n        C_sqrt = sqrtm(C)\n        if np.iscomplexobj(C_sqrt):\n            C_sqrt = C_sqrt.real\n\n        # Generate N i.i.d. standard normal vectors z.\n        # This same set of draws is used for both methods.\n        Z = rng_z.standard_normal((n, N))\n\n        # Reshape mean vector for broadcasting (n, 1).\n        m_col = m.reshape(-1, 1)\n\n        # Generate samples using Cholesky factor.\n        X_L = m_col + L @ Z\n        # Generate samples using symmetric square root.\n        X_S = m_col + C_sqrt @ Z\n\n        # Compute empirical covariance matrices relative to the true mean m.\n        # Deviation matrix: X - m (broadcasting m_col to each column of X)\n        Dev_L = X_L - m_col\n        # C_hat = (1/N) * Sum[(x_k-m)(x_k-m)^T] = (1/N) * Dev @ Dev.T\n        C_hat_L = (Dev_L @ Dev_L.T) / N\n\n        Dev_S = X_S - m_col\n        C_hat_S = (Dev_S @ Dev_S.T) / N\n\n        # Compute covariance error using the Frobenius norm.\n        e_C_L = np.linalg.norm(C_hat_L - C, 'fro')\n        e_C_S = np.linalg.norm(C_hat_S - C, 'fro')\n\n        # Compute the ratio of errors.\n        # e_C_S should be non-zero for any finite N with random draws.\n        if e_C_S == 0.0:\n            # This case is highly improbable. If it occurs, the errors are identical.\n            r = 1.0\n        else:\n            r = e_C_L / e_C_S\n        \n        results.append(r)\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了采样，Cholesky 分解也是贝叶斯推断中不可或缺的工具，尤其是在模型选择和超参数调整等任务中。这些任务通常需要评估边缘似然函数，而直接计算在数值上可能极其不稳定。这个高级练习将指导你推导并实现一种稳定的对数边缘似然计算方法，你将看到 Cholesky 分解如何巧妙地同时解决两个关键问题：计算协方差矩阵的对数行列式，以及求解二次型所需的线性系统，从而有效避免数值上溢或下溢 。",
            "id": "3373567",
            "problem": "给定一个具有分层协方差模型的线性高斯数据同化设置。设状态向量为 $x \\in \\mathbb{R}^{5}$，其高斯先验为 $x \\sim \\mathcal{N}(m_{0}, B(\\theta))$；线性观测模型为 $y \\in \\mathbb{R}^{4}$，定义为 $y = H x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, R(\\theta))$ 且独立于 $x$。先验均值为 $m_{0} = 0 \\in \\mathbb{R}^{5}$。协方差矩阵由超参数 $\\theta = (\\sigma, \\ell, \\tau)$ 参数化，具体如下：\n- 先验协方差 $B(\\theta) \\in \\mathbb{R}^{5 \\times 5}$ 是作用于固定状态位置 $\\{x_{i}\\}_{i=1}^{5}$（其中 $x_{i} \\in \\mathbb{R}$）上的平方指数核，由下式给出\n$$\nB(\\theta)_{ij} = \\sigma^{2} \\exp\\left(-\\frac{(x_{i} - x_{j})^{2}}{2 \\ell^{2}}\\right), \\quad 1 \\leq i,j \\leq 5.\n$$\n- 观测误差协方差为 $R(\\theta) = \\tau^{2} I_{4} \\in \\mathbb{R}^{4 \\times 4}$。\n\n观测算子 $H \\in \\mathbb{R}^{4 \\times 5}$、状态位置 $\\{x_{i}\\}_{i=1}^{5}$ 和观测数据 $y \\in \\mathbb{R}^{4}$ 是固定的，由下式给出\n$$\nH = \\begin{bmatrix}\n1  0  0  0  0 \\\\\n0  \\tfrac{1}{2}  \\tfrac{1}{2}  0  0 \\\\\n0  0  0  1  0 \\\\\n0  0  0  0  1 \\\\\n\\end{bmatrix}, \\quad\n[x_{1}, x_{2}, x_{3}, x_{4}, x_{5}] = [0, 1, 2, 3, 4],\n$$\n$$\ny = \\begin{bmatrix}\n1.0 \\\\\n0.5 \\\\\n-0.25 \\\\\n0.25\n\\end{bmatrix}.\n$$\n\n任务：\n- 仅从多元正态分布的基本性质和线性代数恒等式出发，推导出一个数值稳定的对数边缘似然 $\\log p(y \\mid \\theta)$ 的表达式，该表达式通过积分消去状态 $x$。您的表达式必须基于对一个由 $H$、$B(\\theta)$ 和 $R(\\theta)$ 构成的对称正定矩阵的求值，并且必须使用 Cholesky 分解来稳定地计算相关的二次型和对数行列式。\n- 然后，实现一个程序，使用 Cholesky 分解作为求解线性系统和求值对数行列式的核心数值基元，为一组超参数测试用例计算此对数边缘似然。请勿使用任何直接的矩阵求逆。\n\n测试套件：\n为以下每个超参数设置计算对数边缘似然，其中每个参数都是严格为正的：\n- 情况 1：$(\\sigma, \\ell, \\tau) = (1.2, 1.3, 0.1)$。\n- 情况 2（接近低噪声边界）：$(\\sigma, \\ell, \\tau) = (1.0, 5.0, 10^{-6})$。\n- 情况 3（短相关）：$(\\sigma, \\ell, \\tau) = (0.8, 0.2, 0.5)$。\n- 情况 4（高方差，小噪声）：$(\\sigma, \\ell, \\tau) = (5.0, 1.0, 0.01)$。\n\n要求：\n- 您的程序必须将对数边缘似然值计算为实数，并按上述 4 种情况的指定顺序返回它们。\n- 为确保一致的比较，请将每个结果四舍五入到 6 位小数。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$[r_{1},r_{2},r_{3},r_{4}]$）。\n\n注意：\n- 本问题中没有物理单位。\n- 不使用角度。\n- 不使用百分比。",
            "solution": "## 问题验证\n\n### 步骤 1：提取已知信息\n问题提供了以下信息：\n- 状态向量：$x \\in \\mathbb{R}^{5}$\n- 状态的先验分布：$x \\sim \\mathcal{N}(m_{0}, B(\\theta))$，先验均值为 $m_{0} = 0 \\in \\mathbb{R}^{5}$。\n- 观测向量：$y \\in \\mathbb{R}^{4}$\n- 观测模型：$y = H x + \\varepsilon$，其中观测误差 $\\varepsilon \\sim \\mathcal{N}(0, R(\\theta))$ 独立于 $x$。\n- 超参数：$\\theta = (\\sigma, \\ell, \\tau)$，均为严格正值。\n- 先验协方差矩阵：$B(\\theta) \\in \\mathbb{R}^{5 \\times 5}$ 由平方指数核定义，$B(\\theta)_{ij} = \\sigma^{2} \\exp\\left(-\\frac{(x_{i} - x_{j})^{2}}{2 \\ell^{2}}\\right)$，其中 $1 \\leq i,j \\leq 5$。\n- 观测误差协方差矩阵：$R(\\theta) = \\tau^{2} I_{4} \\in \\mathbb{R}^{4 \\times 4}$，其中 $I_{4}$ 是 $4 \\times 4$ 单位矩阵。\n- 固定状态位置：$[x_{1}, x_{2}, x_{3}, x_{4}, x_{5}] = [0, 1, 2, 3, 4]$。\n- 固定观测算子：$H = \\begin{bmatrix} 1  0  0  0  0 \\\\ 0  \\tfrac{1}{2}  \\tfrac{1}{2}  0  0 \\\\ 0  0  0  1  0 \\\\ 0  0  0  0  1 \\\\ \\end{bmatrix}$。\n- 固定观测数据：$y = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.25 \\\\ 0.25 \\end{bmatrix}$。\n- 任务：使用 Cholesky 分解推导出一个数值稳定的对数边缘似然 $\\log p(y \\mid \\theta)$ 的表达式，并实现一个程序来为四个指定的测试用例计算该值。\n- $(\\sigma, \\ell, \\tau)$ 的测试用例：\n    1. $(1.2, 1.3, 0.1)$\n    2. $(1.0, 5.0, 10^{-6})$\n    3. $(0.8, 0.2, 0.5)$\n    4. $(5.0, 1.0, 0.01)$\n\n### 步骤 2：使用提取的已知信息进行验证\n根据验证标准对问题进行评估：\n- **科学基础**：该问题是贝叶斯推断在线性高斯设置中的一个标准应用，这是数据同化、反问题和机器学习（特别是高斯过程）中的一个基本课题。使用平方指数核作为先验协方差和线性观测模型是常见且成熟的做法。\n- **适定性**：定义模型和计算所需量所需的所有组件都已提供。超参数 $(\\sigma, \\ell, \\tau)$ 是严格正值，确保了协方差矩阵 $B(\\theta)$ 和 $R(\\theta)$ 是对称正定（SPD）的。因此，和 $S(\\theta) = H B(\\theta) H^T + R(\\theta)$ 也是对称正定的，这保证了其 Cholesky 分解、行列式和逆都有明确定义。这确保了对于每个测试用例，都存在一个唯一、稳定且有意义的对数边缘似然解。\n- **客观性**：问题使用精确的数学语言和定义进行陈述。所有量都经过正式定义，没有主观解释的余地。\n\n该问题不存在任何列出的缺陷：\n1. 它在科学上是合理的。\n2. 这是一个与指定主题直接相关的可形式化问题。\n3. 它是完整和一致的。\n4. 所需的计算在数值上是可行的，且该模型是物理系统的标准（尽管简化了）表示。\n5. 它是适定的。\n6. 推导和实现需要对概率论和数值线性代数的原理进行非平凡的应用。\n7. 结果是可数值验证的。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解决方案。\n\n## 推导与求解\n\n目标是计算对数边缘似然 $\\log p(y \\mid \\theta)$，这涉及将状态向量 $x$ 从联合分布 $p(y, x \\mid \\theta)$ 中积分掉。\n$$\np(y \\mid \\theta) = \\int p(y, x \\mid \\theta) \\, dx = \\int p(y \\mid x, \\theta) \\, p(x \\mid \\theta) \\, dx\n$$\n该问题由一个线性高斯模型定义：\n1.  先验：$p(x \\mid \\theta) = \\mathcal{N}(x \\mid m_0, B(\\theta))$。\n2.  似然：$p(y \\mid x, \\theta) = \\mathcal{N}(y \\mid Hx, R(\\theta))$。\n\n鉴于两个分布都是高斯分布，且 $x$ 和 $y$ 之间的关系是线性的，因此 $y$ 的边缘分布也是高斯分布，即 $p(y \\mid \\theta) = \\mathcal{N}(y \\mid \\mu_y, S(\\theta))$。我们需要求出其均值 $\\mu_y$ 和协方差 $S(\\theta)$。\n\n$y$ 的均值可以通过全期望定律求得：\n$$\n\\mu_y = \\mathbb{E}[y \\mid \\theta] = \\mathbb{E}[\\mathbb{E}[y \\mid x, \\theta] \\mid \\theta] = \\mathbb{E}[Hx \\mid \\theta] = H \\mathbb{E}[x \\mid \\theta] = H m_0\n$$\n由于先验均值给定为 $m_0 = 0$，因此边缘分布的均值为 $\\mu_y = 0$。\n\n$y$ 的协方差可以通过全协方差定律求得：\n$$\nS(\\theta) = \\text{Cov}(y \\mid \\theta) = \\mathbb{E}[\\text{Cov}(y \\mid x, \\theta) \\mid \\theta] + \\text{Cov}(\\mathbb{E}[y \\mid x, \\theta] \\mid \\theta)\n$$\n$$\nS(\\theta) = \\mathbb{E}[R(\\theta) \\mid \\theta] + \\text{Cov}(Hx \\mid \\theta) = R(\\theta) + H \\, \\text{Cov}(x \\mid \\theta) \\, H^T\n$$\n将先验协方差 $B(\\theta)$ 代入 $\\text{Cov}(x \\mid \\theta)$，我们得到边缘协方差或证据协方差：\n$$\nS(\\theta) = H B(\\theta) H^T + R(\\theta)\n$$\n因此，观测值 $y$ 的边缘分布为 $\\mathcal{N}(y \\mid 0, S(\\theta))$。\n\n一个 $k$ 维多元正态分布 $\\mathcal{N}(z \\mid \\mu, \\Sigma)$ 的概率密度函数为：\n$$\np(z \\mid \\mu, \\Sigma) = (2\\pi)^{-k/2} (\\det \\Sigma)^{-1/2} \\exp\\left(-\\frac{1}{2}(z - \\mu)^T \\Sigma^{-1} (z - \\mu)\\right)\n$$\n该密度的对数即为对数似然：\n$$\n\\log p(z \\mid \\mu, \\Sigma) = -\\frac{k}{2} \\log(2\\pi) - \\frac{1}{2} \\log(\\det \\Sigma) - \\frac{1}{2}(z - \\mu)^T \\Sigma^{-1} (z - \\mu)\n$$\n对于我们的问题，向量是 $y$，其维度是 $k=4$，均值是 $\\mu_y=0$，协方差是 $S(\\theta)$。因此，对数边缘似然为：\n$$\n\\log p(y \\mid \\theta) = -\\frac{4}{2} \\log(2\\pi) - \\frac{1}{2} \\log(\\det S(\\theta)) - \\frac{1}{2} y^T S(\\theta)^{-1} y\n$$\n$$\n\\log p(y \\mid \\theta) = -2 \\log(2\\pi) - \\frac{1}{2} \\left[ \\log(\\det S(\\theta)) + y^T S(\\theta)^{-1} y \\right]\n$$\n为按要求稳定地计算该值，我们使用对称正定矩阵 $S(\\theta)$ 的 Cholesky 分解。设 $S(\\theta) = L L^T$，其中 $L$ 是一个下三角矩阵。\n\n1.  **对数行列式项**：$\\log(\\det S(\\theta))$\n    $S(\\theta)$ 的行列式是 $\\det(L L^T) = (\\det L)^2$。由于 $L$ 是三角矩阵，其行列式是对角元素的乘积，$\\det L = \\prod_i L_{ii}$。\n    $$\n    \\log(\\det S(\\theta)) = \\log\\left(\\left(\\prod_i L_{ii}\\right)^2\\right) = 2 \\log\\left(\\prod_i L_{ii}\\right) = 2 \\sum_i \\log(L_{ii})\n    $$\n    这避免了在取对数前直接计算行列式可能产生的巨大数值。\n\n2.  **二次型项**：$y^T S(\\theta)^{-1} y$\n    我们必须避免直接计算逆矩阵 $S(\\theta)^{-1}$。二次型可以使用 Cholesky 因子重写：\n    $$\n    y^T S(\\theta)^{-1} y = y^T (L L^T)^{-1} y = y^T (L^T)^{-1} L^{-1} y = (L^{-1} y)^T (L^{-1} y)\n    $$\n    设 $w = L^{-1} y$。向量 $w$ 可以通过前向替换法求解三角系统 $Lw = y$ 来得到，该方法数值稳定且高效。然后，二次型就变成了 $w$ 的 L2 范数的平方：\n    $$\n    y^T S(\\theta)^{-1} y = w^T w = \\|w\\|_2^2\n    $$\n最终的算法如下：\n1.  对于给定的 $\\theta = (\\sigma, \\ell, \\tau)$，构造矩阵 $B(\\theta)$ 和 $R(\\theta)$。\n2.  计算边缘协方差矩阵 $S(\\theta) = H B(\\theta) H^T + R(\\theta)$。\n3.  计算 Cholesky 分解 $S(\\theta) = L L^T$。\n4.  计算对数行列式项为 $2 \\sum_i \\log(L_{ii})$。\n5.  通过前向替换求解线性系统 $Lw = y$ 得到 $w$。\n6.  计算二次型项为 $w^T w$。\n7.  组合这些分量得到对数边缘似然：$\\log p(y \\mid \\theta) = -2 \\log(2\\pi) - \\frac{1}{2} (\\text{对数行列式项} + \\text{二次型项})$。\n\n将为每个提供的测试用例实现此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef calculate_log_marginal_likelihood(theta, H, x_locs, y):\n    \"\"\"\n    Computes the log marginal likelihood for a linear Gaussian model.\n\n    The model is:\n    x ~ N(0, B(theta))\n    y = Hx + e, e ~ N(0, R(theta))\n\n    The log marginal likelihood is:\n    log p(y|theta) = -k/2*log(2*pi) - 1/2*(log|S| + y^T*S^-1*y)\n    where S = H*B*H^T + R and k is the dimension of y.\n\n    Args:\n        theta (tuple): A tuple of hyperparameters (sigma, l, tau).\n        H (np.ndarray): The observation operator matrix.\n        x_locs (np.ndarray): The fixed locations of the state variables.\n        y (np.ndarray): The observation vector.\n\n    Returns:\n        float: The computed log marginal likelihood.\n    \"\"\"\n    sigma, l_corr, tau = theta\n    n_state = len(x_locs)\n    n_obs = len(y)\n\n    # 1. Construct the prior covariance matrix B(theta)\n    # B_ij = sigma^2 * exp(-(x_i - x_j)^2 / (2 * l^2))\n    dist_sq_matrix = np.subtract.outer(x_locs, x_locs)**2\n    B = sigma**2 * np.exp(-dist_sq_matrix / (2 * l_corr**2))\n\n    # 2. Construct the observation error covariance matrix R(theta)\n    # R = tau^2 * I\n    R = tau**2 * np.identity(n_obs)\n\n    # 3. Compute the marginal covariance S(theta) = H*B*H^T + R\n    S = H @ B @ H.T + R\n\n    # 4. Compute the Cholesky factorization S = L*L^T.\n    # np.linalg.cholesky returns the lower triangular matrix L.\n    try:\n        L = np.linalg.cholesky(S)\n    except np.linalg.LinAlgError:\n        # This occurs if S is not positive-definite. Given the problem setup\n        # (sigma, l, tau > 0), this should not happen.\n        return np.nan\n\n    # 5. Compute the log-determinant of S using the Cholesky factor.\n    # log|S| = 2 * sum(log(diag(L)))\n    log_det_S = 2 * np.sum(np.log(np.diag(L)))\n\n    # 6. Compute the quadratic form y^T * S^-1 * y.\n    # First solve Lw = y for w using forward substitution.\n    w = solve_triangular(L, y, lower=True, check_finite=False)\n    # The quadratic form is then w^T * w.\n    quad_form = w.T @ w\n\n    # 7. Assemble the log marginal likelihood.\n    # k is the dimension of y (n_obs).\n    log_ml = -0.5 * n_obs * np.log(2 * np.pi) - 0.5 * (log_det_S + quad_form)\n\n    return log_ml\n\ndef solve():\n    \"\"\"\n    Sets up the problem and computes the log marginal likelihood for all test cases.\n    \"\"\"\n    # Define the fixed model parameters from the problem statement.\n    H = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0]\n    ])\n    x_locs = np.array([0.0, 1.0, 2.0, 3.0, 4.0])\n    y = np.array([1.0, 0.5, -0.25, 0.25])\n\n    # Define the test cases for the hyperparameters (sigma, l, tau).\n    test_cases = [\n        (1.2, 1.3, 0.1),\n        (1.0, 5.0, 1e-6),\n        (0.8, 0.2, 0.5),\n        (5.0, 1.0, 0.01)\n    ]\n\n    results = []\n    for theta in test_cases:\n        log_ml = calculate_log_marginal_likelihood(theta, H, x_locs, y)\n        # Round the result to 6 decimal places as required.\n        results.append(round(log_ml, 6))\n\n    # Print the results in the specified format: [r1,r2,r3,r4]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}