{
    "hands_on_practices": [
        {
            "introduction": "To apply data assimilation to continuous systems, we must first learn to think of functions as vectors in an abstract space. This exercise provides concrete practice with the fundamental geometric tools of a Hilbert space—the inner product, norm, and angle—using the function space $L^2(0,1)$ . By calculating these quantities for specific functions, you will build intuition for how orthogonality represents uncorrelatedness, a cornerstone of projection-based methods in inverse problems.",
            "id": "3430770",
            "problem": "Consider the real Hilbert space $H=L^{2}(0,1)$ with inner product $\\langle f,g\\rangle=\\int_{0}^{1} f(t)\\,g(t)\\,dt$ and induced norm $\\|f\\|=\\sqrt{\\langle f,f\\rangle}$. In the context of linear inverse problems and Data Assimilation (DA), the geometry induced by $\\langle\\cdot,\\cdot\\rangle$ is used to quantify correlations between admissible state directions and to justify projections onto subspaces. Let $f(t)=t-\\tfrac{1}{2}$ and $g(t)=t^{2}-\\tfrac{1}{3}$, viewed as directions in $H$.\n\nTasks:\n- Using only the definitions of inner product, norm, and orthogonality in $H$, compute $\\|f\\|$, $\\|g\\|$, and $\\langle f,g\\rangle$ exactly.\n- Starting from the Cauchy–Schwarz inequality in a real inner product space, derive the definition of the angle $\\theta\\in[0,\\pi]$ between two nonzero elements via the identity $\\cos(\\theta)=\\dfrac{\\langle f,g\\rangle}{\\|f\\|\\,\\|g\\|}$, and then compute the exact value of $\\theta$ for the given $f$ and $g$.\n- Verify explicitly that the Cauchy–Schwarz inequality holds for $f$ and $g$ by comparing $\\langle f,g\\rangle^{2}$ and $\\|f\\|^{2}\\|g\\|^{2}$. Then verify the Pythagoras identity $\\|u+v\\|^{2}=\\|u\\|^{2}+\\|v\\|^{2}$ for the orthogonal pair $u(t)=1$ and $v(t)=f(t)$ in $H$.\n- Briefly interpret the angle computed as a correlation measure between two directions in the state space in a least-squares formulation of an inverse problem, noting how orthogonality encodes uncorrelated directions relevant to projection-based regularization.\n\nExpress the final answer as the exact value of the angle $\\theta$ in radians, without approximation.",
            "solution": "The problem as stated is mathematically well-posed, internally consistent, scientifically grounded in the principles of functional analysis, and provides all necessary information for a unique solution. We may therefore proceed with the calculations and analysis as requested.\n\nThe Hilbert space is $H=L^{2}(0,1)$, equipped with the inner product $\\langle f,g\\rangle=\\int_{0}^{1} f(t)\\,g(t)\\,dt$. The functions are given by $f(t)=t-\\frac{1}{2}$ and $g(t)=t^{2}-\\frac{1}{3}$.\n\nFirst, we compute the norms of $f$ and $g$ and their inner product. The norm squared of $f$ is:\n$$ \\|f\\|^{2} = \\langle f,f \\rangle = \\int_{0}^{1} \\left(t-\\frac{1}{2}\\right)^{2} dt = \\int_{0}^{1} \\left(t^{2}-t+\\frac{1}{4}\\right) dt $$\n$$ = \\left[\\frac{t^{3}}{3} - \\frac{t^{2}}{2} + \\frac{1}{4}t\\right]_{0}^{1} = \\left(\\frac{1}{3} - \\frac{1}{2} + \\frac{1}{4}\\right) - 0 = \\frac{4-6+3}{12} = \\frac{1}{12} $$\nTherefore, the norm of $f$ is $\\|f\\| = \\sqrt{\\frac{1}{12}} = \\frac{1}{\\sqrt{4 \\cdot 3}} = \\frac{1}{2\\sqrt{3}}$.\n\nThe norm squared of $g$ is:\n$$ \\|g\\|^{2} = \\langle g,g \\rangle = \\int_{0}^{1} \\left(t^{2}-\\frac{1}{3}\\right)^{2} dt = \\int_{0}^{1} \\left(t^{4}-\\frac{2}{3}t^{2}+\\frac{1}{9}\\right) dt $$\n$$ = \\left[\\frac{t^{5}}{5} - \\frac{2}{9}t^{3} + \\frac{1}{9}t\\right]_{0}^{1} = \\left(\\frac{1}{5} - \\frac{2}{9} + \\frac{1}{9}\\right) - 0 = \\frac{1}{5} - \\frac{1}{9} = \\frac{9-5}{45} = \\frac{4}{45} $$\nTherefore, the norm of $g$ is $\\|g\\| = \\sqrt{\\frac{4}{45}} = \\frac{2}{\\sqrt{9 \\cdot 5}} = \\frac{2}{3\\sqrt{5}}$.\n\nThe inner product of $f$ and $g$ is:\n$$ \\langle f,g \\rangle = \\int_{0}^{1} \\left(t-\\frac{1}{2}\\right)\\left(t^{2}-\\frac{1}{3}\\right) dt = \\int_{0}^{1} \\left(t^{3} - \\frac{1}{2}t^{2} - \\frac{1}{3}t + \\frac{1}{6}\\right) dt $$\n$$ = \\left[\\frac{t^{4}}{4} - \\frac{t^{3}}{6} - \\frac{t^{2}}{6} + \\frac{t}{6}\\right]_{0}^{1} = \\left(\\frac{1}{4} - \\frac{1}{6} - \\frac{1}{6} + \\frac{1}{6}\\right) - 0 = \\frac{1}{4} - \\frac{1}{6} = \\frac{3-2}{12} = \\frac{1}{12} $$\n\nNext, we derive the definition of the angle $\\theta$ and compute its value. The Cauchy–Schwarz inequality in a real inner product space states that $|\\langle f,g\\rangle| \\le \\|f\\|\\|g\\|$ for any elements $f, g$. For nonzero elements, we can divide by the product of the norms to obtain:\n$$ \\frac{|\\langle f,g\\rangle|}{\\|f\\|\\|g\\|} \\le 1 \\implies -1 \\le \\frac{\\langle f,g\\rangle}{\\|f\\|\\|g\\|} \\le 1 $$\nThe cosine function provides a bijection from the interval $[0, \\pi]$ to $[-1, 1]$. Consequently, for any real value in $[-1, 1]$, there exists a unique angle $\\theta \\in [0, \\pi]$ that corresponds to it. We thus define the angle $\\theta$ between $f$ and $g$ via the identity:\n$$ \\cos(\\theta) = \\frac{\\langle f,g\\rangle}{\\|f\\|\\|g\\|} $$\nUsing the values computed above:\n$$ \\cos(\\theta) = \\frac{\\frac{1}{12}}{\\left(\\frac{1}{2\\sqrt{3}}\\right)\\left(\\frac{2}{3\\sqrt{5}}\\right)} = \\frac{\\frac{1}{12}}{\\frac{2}{6\\sqrt{15}}} = \\frac{\\frac{1}{12}}{\\frac{1}{3\\sqrt{15}}} = \\frac{3\\sqrt{15}}{12} = \\frac{\\sqrt{15}}{4} $$\nThe exact value of the angle is $\\theta = \\arccos\\left(\\frac{\\sqrt{15}}{4}\\right)$.\n\nNow, we explicitly verify the Cauchy–Schwarz inequality for the given $f$ and $g$. We compare $\\langle f,g\\rangle^{2}$ with $\\|f\\|^{2}\\|g\\|^{2}$.\n$$ \\langle f,g\\rangle^{2} = \\left(\\frac{1}{12}\\right)^{2} = \\frac{1}{144} $$\n$$ \\|f\\|^{2}\\|g\\|^{2} = \\left(\\frac{1}{12}\\right)\\left(\\frac{4}{45}\\right) = \\frac{4}{540} = \\frac{1}{135} $$\nThe inequality requires $\\frac{1}{144} \\le \\frac{1}{135}$. Since $144 > 135$, this inequality holds, verifying the Cauchy-Schwarz inequality for this specific case.\n\nNext, we verify the Pythagorean identity $\\|u+v\\|^{2}=\\|u\\|^{2}+\\|v\\|^{2}$ for the pair $u(t)=1$ and $v(t)=f(t)=t-\\frac{1}{2}$. First, we must confirm they are orthogonal:\n$$ \\langle u,v \\rangle = \\int_{0}^{1} (1)\\left(t-\\frac{1}{2}\\right) dt = \\left[\\frac{t^{2}}{2} - \\frac{t}{2}\\right]_{0}^{1} = \\left(\\frac{1}{2}-\\frac{1}{2}\\right) - 0 = 0 $$\nSince the inner product is zero, $u$ and $v$ are orthogonal. Now we check the identity.\nThe left-hand side is $\\|u+v\\|^{2}$. The sum is $u(t)+v(t) = 1 + t - \\frac{1}{2} = t+\\frac{1}{2}$.\n$$ \\|u+v\\|^{2} = \\int_{0}^{1} \\left(t+\\frac{1}{2}\\right)^{2} dt = \\int_{0}^{1} \\left(t^{2}+t+\\frac{1}{4}\\right) dt = \\left[\\frac{t^{3}}{3}+\\frac{t^{2}}{2}+\\frac{t}{4}\\right]_{0}^{1} = \\frac{1}{3}+\\frac{1}{2}+\\frac{1}{4} = \\frac{4+6+3}{12} = \\frac{13}{12} $$\nThe right-hand side is $\\|u\\|^{2}+\\|v\\|^{2}$.\n$$ \\|u\\|^{2} = \\int_{0}^{1} 1^{2} dt = [t]_{0}^{1} = 1 $$\nAs calculated before, $\\|v\\|^{2} = \\|f\\|^{2} = \\frac{1}{12}$.\nThus, $\\|u\\|^{2}+\\|v\\|^{2} = 1 + \\frac{1}{12} = \\frac{13}{12}$.\nSince both sides equal $\\frac{13}{12}$, the Pythagorean identity is verified for this orthogonal pair.\n\nFinally, we provide a brief interpretation. In the context of inverse problems and data assimilation, the state space is a Hilbert space $H$. Its elements, such as $f$ and $g$, represent directions of variation or components of the state vector. The cosine of the angle between two direction vectors, $\\cos(\\theta) = \\frac{\\langle f,g\\rangle}{\\|f\\|\\|g\\|}$, serves as a normalized measure of their statistical correlation. A value of $\\cos(\\theta)$ near $1$ or $-1$ implies the directions are strongly correlated (aligned or anti-aligned), meaning information about one strongly implies information about the other. A value of $\\cos(\\theta)$ near $0$ ($ \\theta \\approx \\frac{\\pi}{2} $) implies the directions are nearly uncorrelated. Orthogonality, $\\langle f,g\\rangle=0$, signifies that the directions are completely uncorrelated.\nIn a least-squares formulation, one often seeks a solution by projecting it onto a basis. Using an orthogonal basis is computationally and conceptually advantageous because the coefficients for each basis vector are independent of one another. Projection-based regularization methods capitalize on this by decomposing the solution along orthogonal directions (e.g., singular vectors) and retaining only the components corresponding to significant, well-determined information, while discarding those associated with noise or instability. Orthogonality ensures that this separation is \"clean,\" as the components do not mix. The angle $\\theta$ quantifies the degree to which two arbitrary directions $f$ and $g$ are \"mixed\" or correlated, with orthogonality being the ideal for independent decomposition. In our case, $\\cos(\\theta) = \\frac{\\sqrt{15}}{4} \\approx 0.968$, which is very close to $1$, indicating that the directions $f(t)=t-\\frac{1}{2}$ and $g(t)=t^{2}-\\frac{1}{3}$ are highly correlated in this space.",
            "answer": "$$\\boxed{\\arccos\\left(\\frac{\\sqrt{15}}{4}\\right)}$$"
        },
        {
            "introduction": "A linear operator, represented by a matrix in finite dimensions, describes how a system's state is transformed. This exercise explores how to quantify the 'size' or 'influence' of such an operator through its norm and spectral radius . Understanding the distinction between the maximum possible amplification (norm) and the long-term iterative behavior (spectral radius) is critical for analyzing the stability of data assimilation schemes.",
            "id": "3430767",
            "problem": "Consider the finite-dimensional Hilbert space $\\mathbb{R}^{2}$ equipped with the standard Euclidean inner product $\\langle x, y \\rangle = x^{\\top} y$ and its associated norm $\\|x\\| = \\sqrt{\\langle x, x \\rangle}$. In a linearized data assimilation step, the mapping from state increments to observation residual increments may be represented by a bounded linear operator $T : \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ given in the standard basis by the real matrix\n$$\nT \\;=\\; \\begin{pmatrix}\n0  3 \\\\\n1  0\n\\end{pmatrix}.\n$$\nUsing only the definitions of operator norm and spectral radius in a Hilbert space, perform the following tasks:\n- Compute the operator norm $\\|T\\|$ induced by the Euclidean norm, defined by\n$$\n\\|T\\| \\;=\\; \\sup_{\\|x\\| = 1} \\|Tx\\|.\n$$\n- Compute the spectral radius $\\rho(T)$, defined by\n$$\n\\rho(T) \\;=\\; \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } T \\}.\n$$\n- Interpret and compare $\\|T\\|$ with $\\sqrt{\\lambda_{\\max}(T^{*} T)}$, where $T^{*}$ denotes the adjoint of $T$ with respect to the given inner product and $\\lambda_{\\max}(T^{*}T)$ is the largest eigenvalue of $T^{*}T$.\n\nReport, as your final answer, the single quantity\n$$\nR \\;=\\; \\frac{\\|T\\|}{\\rho(T)}.\n$$\nExpress the final value of $R$ as a closed-form analytic expression. No rounding is required.",
            "solution": "The problem statement is analyzed and found to be valid. It is a well-posed problem in linear algebra and functional analysis, with all necessary components clearly defined and internally consistent.\n\nThe tasks are to compute the operator norm $\\|T\\|$, the spectral radius $\\rho(T)$, and a related quantity for the operator $T: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ represented in the standard basis by the matrix:\n$$\nT \\;=\\; \\begin{pmatrix} 0  3 \\\\ 1  0 \\end{pmatrix}\n$$\nThe space $\\mathbb{R}^2$ is equipped with the standard Euclidean inner product $\\langle x, y \\rangle = x^{\\top} y$.\n\nFirst, we compute the spectral radius $\\rho(T)$. By definition, $\\rho(T) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } T \\}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(T - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det(T - \\lambda I) \\;=\\; \\det \\begin{pmatrix} 0 - \\lambda  3 \\\\ 1  0 - \\lambda \\end{pmatrix} \\;=\\; \\det \\begin{pmatrix} -\\lambda  3 \\\\ 1  -\\lambda \\end{pmatrix}\n$$\nThe determinant is $(-\\lambda)(-\\lambda) - (3)(1) = \\lambda^2 - 3$. Setting this to zero gives the characteristic equation:\n$$\n\\lambda^2 - 3 = 0\n$$\nThe eigenvalues are the solutions to this equation, which are $\\lambda_1 = \\sqrt{3}$ and $\\lambda_2 = -\\sqrt{3}$. The spectral radius is the maximum of the absolute values of these eigenvalues:\n$$\n\\rho(T) \\;=\\; \\max \\{ |\\sqrt{3}|, |-\\sqrt{3}| \\} \\;=\\; \\sqrt{3}\n$$\n\nNext, we compute the operator norm $\\|T\\|$, which is defined as $\\|T\\| = \\sup_{\\|x\\| = 1} \\|Tx\\|$. Let $x$ be a vector in $\\mathbb{R}^2$ with unit norm. We can represent $x$ as $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ where $\\|x\\|^2 = x_1^2 + x_2^2 = 1$. We apply the operator $T$ to $x$:\n$$\nTx \\;=\\; \\begin{pmatrix} 0  3 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 3x_2 \\\\ x_1 \\end{pmatrix}\n$$\nThe squared norm of the resulting vector $Tx$ is:\n$$\n\\|Tx\\|^2 \\;=\\; \\langle Tx, Tx \\rangle \\;=\\; (3x_2)^2 + (x_1)^2 \\;=\\; 9x_2^2 + x_1^2\n$$\nWe wish to maximize this quantity subject to the constraint $x_1^2 + x_2^2 = 1$. We can substitute $x_1^2 = 1 - x_2^2$ into the expression for $\\|Tx\\|^2$:\n$$\n\\|Tx\\|^2 \\;=\\; 9x_2^2 + (1 - x_2^2) \\;=\\; 8x_2^2 + 1\n$$\nSince $x_1^2 + x_2^2 = 1$, the value of $x_2^2$ must be in the interval $[0, 1]$. To maximize the expression $8x_2^2 + 1$, we must choose the largest possible value for $x_2^2$, which is $x_2^2 = 1$. This occurs when $x_2 = \\pm 1$ and $x_1 = 0$.\nThe maximum value of $\\|Tx\\|^2$ is therefore:\n$$\n\\sup_{\\|x\\|=1} \\|Tx\\|^2 \\;=\\; 8(1) + 1 \\;=\\; 9\n$$\nThe operator norm is the square root of this value:\n$$\n\\|T\\| \\;=\\; \\sqrt{9} \\;=\\; 3\n$$\n\nThe third task is to interpret and compare $\\|T\\|$ with $\\sqrt{\\lambda_{\\max}(T^{*} T)}$. The adjoint operator $T^*$ with respect to the standard Euclidean inner product on $\\mathbb{R}^2$ is the transpose of the matrix $T$.\n$$\nT^{*} \\;=\\; T^{\\top} \\;=\\; \\begin{pmatrix} 0  1 \\\\ 3  0 \\end{pmatrix}\n$$\nWe then compute the operator $T^{*}T$:\n$$\nT^{*}T \\;=\\; \\begin{pmatrix} 0  1 \\\\ 3  0 \\end{pmatrix} \\begin{pmatrix} 0  3 \\\\ 1  0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} (0)(0)+(1)(1)  (0)(3)+(1)(0) \\\\ (3)(0)+(0)(1)  (3)(3)+(0)(0) \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  9 \\end{pmatrix}\n$$\nThe operator $T^{*}T$ is represented by a diagonal matrix. Its eigenvalues are the diagonal entries, which are $1$ and $9$. The largest eigenvalue is $\\lambda_{\\max}(T^{*}T) = 9$. We compute the required quantity:\n$$\n\\sqrt{\\lambda_{\\max}(T^{*}T)} \\;=\\; \\sqrt{9} \\;=\\; 3\n$$\nComparing this with the operator norm computed earlier, we find that $\\|T\\| = 3 = \\sqrt{\\lambda_{\\max}(T^{*}T)}$. This result is an instance of a general theorem in functional analysis, which states that for any bounded linear operator $T$ on a Hilbert space, the operator norm is given by $\\|T\\|^2 = \\|T^{*}T\\| = \\lambda_{\\max}(T^{*}T)$, assuming $T^*T$ is a compact self-adjoint operator (which is always true in finite dimensions). The square roots of the eigenvalues of $T^*T$ are the singular values of $T$, and the operator norm is the largest singular value. The calculation performed here confirms this general principle for the specific operator $T$.\n\nFinally, we are asked to report the ratio $R = \\frac{\\|T\\|}{\\rho(T)}$. Using the values we have computed:\n$$\nR \\;=\\; \\frac{3}{\\sqrt{3}} \\;=\\; \\frac{3\\sqrt{3}}{(\\sqrt{3})(\\sqrt{3})} \\;=\\; \\frac{3\\sqrt{3}}{3} \\;=\\; \\sqrt{3}\n$$\nThe ratio $R$ is equal to $\\sqrt{3}$. This demonstrates that for a non-normal operator ($TT^{*} \\neq T^{*}T$), the operator norm can be strictly greater than the spectral radius. Indeed,\n$$\nTT^* = \\begin{pmatrix} 0  3 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 3  0 \\end{pmatrix} = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} \\neq \\begin{pmatrix} 1  0 \\\\ 0  9 \\end{pmatrix} = T^*T\n$$\nFor any normal operator, the operator norm is equal to the spectral radius. The fact that $\\|T\\| > \\rho(T)$ confirms that $T$ is not a normal operator.",
            "answer": "$$\\boxed{\\sqrt{3}}$$"
        },
        {
            "introduction": "Inverse problems often involve solving linear systems that may not have a unique, stable solution. The Moore-Penrose pseudoinverse provides a powerful and universal way to define the 'best' approximate solution in a least-squares sense . This practice will guide you through both the theoretical properties and the practical computation of the pseudoinverse using the Singular Value Decomposition (SVD), a fundamental tool in data analysis.",
            "id": "3430792",
            "problem": "Let $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$ be finite-dimensional real Hilbert spaces equipped with the standard Euclidean inner product. For a linear operator $A \\in \\mathbb{R}^{m \\times n}$, the adjoint $A^{*}$ is the transpose $A^{\\top}$. An operator $P$ on a Hilbert space is an orthogonal projection if and only if $P^{2} = P$ and $P^{*} = P$. The singular value decomposition (SVD) of $A$ is a factorization $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ diagonal with nonnegative diagonal entries, called the singular values.\n\nTasks:\n- Define the Moore–Penrose pseudoinverse $A^{+}$ of $A$ as the unique operator satisfying the four Moore–Penrose equations. Starting from the definitions above and the properties of the singular value decomposition, prove that these four equations are satisfied by the candidate constructed from the SVD, and argue why they uniquely determine $A^{+}$.\n- Consider the rank-deficient matrix $A \\in \\mathbb{R}^{2 \\times 3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  0 \\\\\n1  -1  0\n\\end{pmatrix}.\n$$\nCompute the singular value decomposition of $A$ from first principles and use it to compute the Moore–Penrose pseudoinverse $A^{+}$. Express your final answer as a single explicit $3 \\times 2$ matrix with exact entries.\n\nAnswer specification:\n- The final answer must be a single closed-form matrix expression.\n- No rounding is required; use exact expressions.\n- Do not include units.",
            "solution": "We begin from core definitions in finite-dimensional real Hilbert spaces. For a matrix $A \\in \\mathbb{R}^{m \\times n}$, the adjoint is $A^{*} = A^{\\top}$. An orthogonal projection $P$ satisfies $P^{2} = P$ and $P^{*} = P$. The singular value decomposition (SVD) asserts that there exist orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$, and a diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ with nonnegative diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r}  0$ and $\\sigma_{r+1} = \\cdots = 0$ such that $A = U \\Sigma V^{\\top}$. The integer $r$ is the rank of $A$.\n\nDefinition of the Moore–Penrose pseudoinverse:\nThe Moore–Penrose pseudoinverse $A^{+} \\in \\mathbb{R}^{n \\times m}$ is defined as the unique matrix satisfying the four Moore–Penrose equations:\n- $(1)$ $A A^{+} A = A$,\n- $(2)$ $A^{+} A A^{+} = A^{+}$,\n- $(3)$ $(A A^{+})^{*} = A A^{+}$,\n- $(4)$ $(A^{+} A)^{*} = A^{+} A$.\n\nWe will prove existence and show these properties using the singular value decomposition, and then argue uniqueness from orthogonal projection characterizations.\n\nExistence via singular value decomposition:\nLet $A = U \\Sigma V^{\\top}$ be the singular value decomposition, with rank $r$. Define $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ by taking reciprocals of the nonzero singular values on the diagonal and leaving zeros where $\\Sigma$ is zero:\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\operatorname{diag}(\\sigma_{1}, \\ldots, \\sigma_{r})  0 \\\\\n0  0\n\\end{pmatrix}\n\\quad\\Longrightarrow\\quad\n\\Sigma^{+} \\;=\\; \\begin{pmatrix}\n\\operatorname{diag}(\\sigma_{1}^{-1}, \\ldots, \\sigma_{r}^{-1})  0 \\\\\n0  0\n\\end{pmatrix}.\n$$\nDefine $B := V \\Sigma^{+} U^{\\top} \\in \\mathbb{R}^{n \\times m}$. We verify the Moore–Penrose equations for $B$.\n\nVerification of $(1)$:\nCompute\n$$\nA B A \\;=\\; U \\Sigma V^{\\top} \\, V \\Sigma^{+} U^{\\top} \\, U \\Sigma V^{\\top}\n\\;=\\; U \\, \\Sigma \\Sigma^{+} \\Sigma \\, V^{\\top}.\n$$\nBecause $\\Sigma$ is diagonal with the first $r$ entries $\\sigma_{i}  0$ and the rest $0$, it follows that $\\Sigma \\Sigma^{+} \\Sigma = \\Sigma$ (on the $r$-dimensional singular subspace this reduces to $\\sigma_{i} \\cdot \\sigma_{i}^{-1} \\cdot \\sigma_{i} = \\sigma_{i}$, and on the nullspace it vanishes). Hence $A B A = U \\Sigma V^{\\top} = A$.\n\nVerification of $(2)$:\nSimilarly,\n$$\nB A B \\;=\\; V \\Sigma^{+} U^{\\top} \\, U \\Sigma V^{\\top} \\, V \\Sigma^{+} U^{\\top}\n\\;=\\; V \\, \\Sigma^{+} \\Sigma \\Sigma^{+} \\, U^{\\top}.\n$$\nOn the $r$-dimensional subspace, $\\sigma_{i}^{-1} \\cdot \\sigma_{i} \\cdot \\sigma_{i}^{-1} = \\sigma_{i}^{-1}$, and it vanishes on the nullspace, so $\\Sigma^{+} \\Sigma \\Sigma^{+} = \\Sigma^{+}$. Consequently, $B A B = V \\Sigma^{+} U^{\\top} = B$.\n\nVerification of $(3)$:\nCompute\n$$\nA B \\;=\\; U \\Sigma V^{\\top} \\, V \\Sigma^{+} U^{\\top}\n\\;=\\; U \\, \\Sigma \\Sigma^{+} \\, U^{\\top}.\n$$\nLet $P_{r} \\in \\mathbb{R}^{m \\times m}$ denote the orthogonal projector onto the span of the first $r$ columns of $U$, i.e., $P_{r} = \\operatorname{diag}(1,\\ldots,1,0,\\ldots,0)$ in the $U$-basis. Then $\\Sigma \\Sigma^{+}$ equals $P_{r}$ in the $U$-basis, and thus\n$$\nA B \\;=\\; U P_{r} U^{\\top}.\n$$\nSince $P_{r}$ is symmetric and idempotent, $U P_{r} U^{\\top}$ is symmetric, hence $(A B)^{*} = A B$.\n\nVerification of $(4)$:\nSimilarly,\n$$\nB A \\;=\\; V \\Sigma^{+} U^{\\top} \\, U \\Sigma V^{\\top}\n\\;=\\; V \\, \\Sigma^{+} \\Sigma \\, V^{\\top}\n\\;=\\; V Q_{r} V^{\\top},\n$$\nwhere $Q_{r}$ is the orthogonal projector onto the span of the first $r$ columns of $V$, i.e., $Q_{r} = \\operatorname{diag}(1,\\ldots,1,0,\\ldots,0)$ in the $V$-basis. Therefore $(B A)^{*} = B A$. This verifies all four Moore–Penrose equations for $B$.\n\nUniqueness:\nAssume $X$ satisfies the four Moore–Penrose equations with $A$. Then $A X$ is idempotent and self-adjoint, hence an orthogonal projector. Moreover, using $A A^{+} A = A$ and $A X A = A$, one deduces that $A X$ projects onto $\\operatorname{ran}(A)$, and $X A$ projects onto $\\operatorname{ran}(A^{\\top})$. In the $U$–$V$ singular vector coordinates, these projectors are precisely $U P_{r} U^{\\top}$ and $V Q_{r} V^{\\top}$, forcing $X$ to act as reciprocal scaling on the nonzero singular directions and to annihilate the null spaces. This matches $V \\Sigma^{+} U^{\\top}$ uniquely. Therefore, $A^{+}$ exists and is unique, and is given by $A^{+} = V \\Sigma^{+} U^{\\top}$.\n\nComputation of $A^{+}$ for a specific rank-deficient matrix:\nLet\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  0 \\\\\n1  -1  0\n\\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}.\n$$\nWe compute its singular value decomposition from first principles.\n\nStep 1: Compute $A A^{\\top}$ and its eigen-decomposition.\nWe have\n$$\nA A^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1  1  0 \\\\\n1  -1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1 \\\\\n1  -1 \\\\\n0  0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n2  0 \\\\\n0  2\n\\end{pmatrix}\n\\;=\\; 2 I_{2}.\n$$\nTherefore, the eigenvalues of $A A^{\\top}$ are $\\sigma_{1}^{2} = 2$ and $\\sigma_{2}^{2} = 2$, so the nonzero singular values are $\\sigma_{1} = \\sqrt{2}$ and $\\sigma_{2} = \\sqrt{2}$. An orthonormal eigenbasis for $A A^{\\top}$ is given by the standard basis, so we may take\n$$\nU \\;=\\; I_{2}, \\qquad \\Sigma \\;=\\; \\begin{pmatrix}\n\\sqrt{2}  0  0 \\\\\n0  \\sqrt{2}  0\n\\end{pmatrix}.\n$$\n\nStep 2: Compute right singular vectors $V$ via $A^{\\top} u_{i} = \\sigma_{i} v_{i}$.\nLet $u_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $u_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Then\n$$\nA^{\\top} u_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\qquad\nv_{1} \\;=\\; \\frac{1}{\\sigma_{1}} A^{\\top} u_{1} \\;=\\; \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nSimilarly,\n$$\nA^{\\top} u_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad\nv_{2} \\;=\\; \\frac{1}{\\sigma_{2}} A^{\\top} u_{2} \\;=\\; \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}.\n$$\nComplete $V$ to an orthogonal matrix by choosing $v_{3}$ orthonormal to $v_{1}$ and $v_{2}$, for example $v_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Thus\n$$\nV \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\n\nStep 3: Form $\\Sigma^{+}$ and compute $A^{+} = V \\Sigma^{+} U^{\\top}$.\nWe have\n$$\n\\Sigma^{+} \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n0  0\n\\end{pmatrix}.\n$$\nSince $U = I_{2}$, we obtain\n$$\nA^{+}\n\\;=\\;\nV \\Sigma^{+} U^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}} \\\\\n0  0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2} \\\\\n\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  0\n\\end{pmatrix}.\n$$\n\nThis $A^{+}$ is the Moore–Penrose pseudoinverse of $A$. It can be directly verified that $A A^{+} = I_{2}$ and $A^{+} A$ is the orthogonal projector onto $\\operatorname{span}\\{ e_{1}, e_{2} \\} \\subset \\mathbb{R}^{3}$, namely $\\operatorname{diag}(1,1,0)$, which confirms the four Moore–Penrose equations in this instance.\n\nTherefore, the requested explicit $3 \\times 2$ matrix is\n$$\nA^{+} \\;=\\; \\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2} \\\\\n\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  0\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  -\\frac{1}{2} \\\\ 0  0\\end{pmatrix}}$$"
        }
    ]
}