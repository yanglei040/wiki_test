{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of variational data assimilation is computing the gradient of a cost functional with respect to control parameters, which are often linked to the state through a Partial Differential Equation (PDE). This first exercise walks you through the essential mechanics of this process, starting from deriving the weak formulation of the governing PDE and then applying the powerful adjoint-state method to find the gradient. Mastering this procedure is fundamental to formulating and solving a vast class of inverse problems.",
            "id": "3383674",
            "problem": "Let $\\Omega \\subset \\mathbb{R}^{d}$ be a bounded Lipschitz domain with boundary $\\partial \\Omega$. Consider the Partial Differential Equation (PDE) $-\\Delta u = f$ in $\\Omega$ with homogeneous Dirichlet boundary condition $u|_{\\partial \\Omega} = 0$, where $f \\in L^{2}(\\Omega)$. \n\nTask 1 (weak formulation): Starting from the core definitions of the Sobolev space $H_{0}^{1}(\\Omega)$ and the Gauss-Green identity, derive the weak formulation by multiplying the PDE by a test function $v \\in H_{0}^{1}(\\Omega)$ and integrating by parts. Define explicitly the bilinear form $a(u,v)$ and the linear functional $\\ell(v)$ that appear in the weak formulation, ensuring all function space membership is rigorously stated.\n\nTask 2 (data assimilation functional and gradient): Define the observation operator $C : H_{0}^{1}(\\Omega) \\to \\mathbb{R}^{m}$ by \n$$\nC(u)_{k} = \\int_{\\Omega} u(x)\\,\\phi_{k}(x)\\,\\mathrm{d}x,\\quad k=1,\\dots,m,\n$$\nwhere $\\phi_{k} \\in L^{2}(\\Omega)$ are given basis functions, and suppose $d = (d_{1},\\dots,d_{m}) \\in \\mathbb{R}^{m}$ are given data. For a fixed regularization parameter $\\lambda  0$, define the least-squares data assimilation objective \n$$\nJ(f) = \\frac{1}{2}\\sum_{k=1}^{m}\\left(C(u(f))_{k} - d_{k}\\right)^{2} + \\frac{\\lambda}{2}\\,\\|f\\|_{L^{2}(\\Omega)}^{2},\n$$\nwhere $u(f) \\in H_{0}^{1}(\\Omega)$ denotes the unique weak solution associated with the source $f \\in L^{2}(\\Omega)$ via Task 1. Derive the $L^{2}(\\Omega)$-gradient of $J$ at a general $f$, expressed solely in terms of $f$, the state $u(f)$, and an adjoint state $p \\in H_{0}^{1}(\\Omega)$ that you must introduce and specify through a weak formulation. \n\nYour final answer must be a single closed-form analytic expression for the $L^{2}(\\Omega)$-gradient of $J(f)$, expressed in terms of $f$ and $p$. No rounding is required and no physical units are involved.",
            "solution": "This problem consists of two tasks. The first is to derive the weak formulation for a Poisson problem. The second is to derive the gradient of a data assimilation objective functional using the adjoint method. We will address each task sequentially.\n\n**Task 1: Weak Formulation**\n\nThe given Partial Differential Equation (PDE) is the Poisson equation with a homogeneous Dirichlet boundary condition:\n$$\n-\\Delta u = f \\quad \\text{in } \\Omega\n$$\n$$\nu|_{\\partial \\Omega} = 0\n$$\nHere, $f$ is a source term belonging to the space $L^{2}(\\Omega)$ of square-integrable functions on the domain $\\Omega$. The solution $u$ is sought in the Sobolev space $H_{0}^{1}(\\Omega)$. The space $H_{0}^{1}(\\Omega)$ is the closure of the space of infinitely differentiable functions with compact support in $\\Omega$, $C_{c}^{\\infty}(\\Omega)$, under the $H^{1}$ norm. Functions in $H_{0}^{1}(\\Omega)$ can be thought of as satisfying the condition $u=0$ on the boundary $\\partial \\Omega$ in a generalized (trace) sense.\n\nTo derive the weak formulation, we multiply the PDE by an arbitrary test function $v \\in H_{0}^{1}(\\Omega)$ and integrate over the domain $\\Omega$:\n$$\n\\int_{\\Omega} (-\\Delta u(x)) v(x) \\, \\mathrm{d}x = \\int_{\\Omega} f(x) v(x) \\, \\mathrm{d}x\n$$\nWe apply the Gauss-Green identity (a form of integration by parts) to the left-hand side. For sufficiently smooth functions $u$ and $v$ on a domain $\\Omega$ with boundary $\\partial\\Omega$ and outward unit normal vector $n$, the identity is:\n$$\n\\int_{\\Omega} (-\\Delta u) v \\, \\mathrm{d}x = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, \\mathrm{d}x - \\int_{\\partial \\Omega} v (\\nabla u \\cdot n) \\, \\mathrm{d}S\n$$\nThis identity extends to functions in Sobolev spaces. Since the test function $v$ is in $H_{0}^{1}(\\Omega)$, its trace on the boundary $\\partial \\Omega$ is zero, i.e., $v|_{\\partial \\Omega} = 0$. Consequently, the boundary integral vanishes:\n$$\n\\int_{\\partial \\Omega} v (\\nabla u \\cdot n) \\, \\mathrm{d}S = 0\n$$\nThus, we obtain the weak formulation: find $u \\in H_{0}^{1}(\\Omega)$ such that\n$$\n\\int_{\\Omega} \\nabla u(x) \\cdot \\nabla v(x) \\, \\mathrm{d}x = \\int_{\\Omega} f(x) v(x) \\, \\mathrm{d}x\n$$\nholds for all test functions $v \\in H_{0}^{1}(\\Omega)$.\n\nThis equation is of the general form $a(u,v) = \\ell(v)$. The bilinear form $a(\\cdot, \\cdot)$ and the linear functional $\\ell(\\cdot)$ are defined as follows:\n1.  The bilinear form $a: H_{0}^{1}(\\Omega) \\times H_{0}^{1}(\\Omega) \\to \\mathbb{R}$ is given by:\n    $$\n    a(u,v) = \\int_{\\Omega} \\nabla u(x) \\cdot \\nabla v(x) \\, \\mathrm{d}x\n    $$\n2.  The linear functional $\\ell: H_{0}^{1}(\\Omega) \\to \\mathbb{R}$ is given by:\n    $$\n    \\ell(v) = \\int_{\\Omega} f(x) v(x) \\, \\mathrm{d}x\n    $$\nThe existence and uniqueness of a solution $u \\in H_{0}^{1}(\\Omega)$ for any given $f \\in L^2(\\Omega)$ is guaranteed by the Lax-Milgram theorem, as $a(\\cdot,\\cdot)$ is continuous and coercive on $H_{0}^{1}(\\Omega)$, and $\\ell(\\cdot)$ is a continuous linear functional on $H_{0}^{1}(\\Omega)$.\n\n**Task 2: Data Assimilation Functional and Gradient**\n\nThe objective functional to be minimized is given by:\n$$\nJ(f) = \\frac{1}{2}\\sum_{k=1}^{m}\\left(C(u(f))_{k} - d_{k}\\right)^{2} + \\frac{\\lambda}{2}\\,\\|f\\|_{L^{2}(\\Omega)}^{2}\n$$\nwhere $u(f)$ is the unique weak solution to the Poisson problem for a given source $f \\in L^{2}(\\Omega)$. We seek the $L^{2}(\\Omega)$-gradient of $J(f)$, denoted $\\nabla_{f} J(f)$, which is an element of $L^{2}(\\Omega)$ defined by the relation:\n$$\nDJ(f)[\\delta f] = \\langle \\nabla_{f} J(f), \\delta f \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} (\\nabla_{f} J(f))(x) \\delta f(x) \\, \\mathrm{d}x\n$$\nfor all perturbations $\\delta f \\in L^{2}(\\Omega)$, where $DJ(f)[\\delta f]$ is the Gâteaux derivative of $J$ at $f$ in the direction $\\delta f$.\n\nThe Gâteaux derivative is calculated as:\n$$\nDJ(f)[\\delta f] = \\lim_{\\epsilon \\to 0} \\frac{J(f + \\epsilon \\delta f) - J(f)}{\\epsilon} = \\frac{d}{d\\epsilon} \\left. J(f + \\epsilon \\delta f) \\right|_{\\epsilon=0}\n$$\nFirst, we analyze the dependence of the state $u$ on the source $f$. The mapping $f \\mapsto u(f)$ is linear due to the linearity of the weak formulation. Let $u(f+\\epsilon\\delta f)$ be the solution corresponding to the source $f+\\epsilon\\delta f$. By linearity, $u(f+\\epsilon\\delta f) = u(f) + \\epsilon u(\\delta f)$. Let us denote $\\delta u := u(\\delta f)$. The function $\\delta u \\in H_{0}^{1}(\\Omega)$ is the unique weak solution to the PDE with source $\\delta f$, i.e., it satisfies:\n$$\na(\\delta u, v) = \\int_{\\Omega} \\delta f(x) v(x) \\, \\mathrm{d}x \\quad \\forall v \\in H_{0}^{1}(\\Omega)\n$$\nNow we compute the derivative of $J(f)$.\nThe derivative of the first term (misfit term) is:\n$$\n\\frac{d}{d\\epsilon} \\left. \\left( \\frac{1}{2}\\sum_{k=1}^{m}\\left(C(u(f+\\epsilon\\delta f))_{k} - d_{k}\\right)^{2} \\right) \\right|_{\\epsilon=0}\n$$\nUsing the chain rule and the linearity of $C$ and $u(f)$:\n$$\n= \\sum_{k=1}^{m} \\left(C(u(f))_{k} - d_{k}\\right) \\left( \\frac{d}{d\\epsilon} \\left. C(u(f) + \\epsilon \\delta u)_{k} \\right|_{\\epsilon=0} \\right)\n$$\n$$\n= \\sum_{k=1}^{m} \\left(C(u(f))_{k} - d_{k}\\right) C(\\delta u)_{k}\n$$\nSubstituting the definition of $C(\\delta u)_k$:\n$$\n= \\sum_{k=1}^{m} \\left(C(u(f))_{k} - d_{k}\\right) \\int_{\\Omega} \\delta u(x) \\phi_{k}(x) \\, \\mathrm{d}x\n$$\n$$\n= \\int_{\\Omega} \\delta u(x) \\left( \\sum_{k=1}^{m} (C(u(f))_{k} - d_{k}) \\phi_{k}(x) \\right) \\, \\mathrm{d}x\n$$\nThe derivative of the second term (regularization term) is:\n$$\n\\frac{d}{d\\epsilon} \\left. \\left( \\frac{\\lambda}{2}\\,\\|f + \\epsilon \\delta f\\|_{L^{2}(\\Omega)}^{2} \\right) \\right|_{\\epsilon=0} = \\frac{\\lambda}{2} \\frac{d}{d\\epsilon} \\left. \\langle f + \\epsilon \\delta f, f + \\epsilon \\delta f \\rangle_{L^2} \\right|_{\\epsilon=0}\n$$\n$$\n= \\frac{\\lambda}{2} \\frac{d}{d\\epsilon} \\left. (\\|f\\|_{L^2}^2 + 2\\epsilon \\langle f, \\delta f \\rangle_{L^2} + \\epsilon^2 \\|\\delta f\\|_{L^2}^2) \\right|_{\\epsilon=0} = \\lambda \\langle f, \\delta f \\rangle_{L^{2}(\\Omega)} = \\lambda \\int_{\\Omega} f(x) \\delta f(x) \\, \\mathrm{d}x\n$$\nCombining both terms, the Gâteaux derivative is:\n$$\nDJ(f)[\\delta f] = \\int_{\\Omega} \\delta u(x) \\left( \\sum_{k=1}^{m} (C(u(f))_{k} - d_{k}) \\phi_{k}(x) \\right) \\, \\mathrm{d}x + \\lambda \\int_{\\Omega} f(x) \\delta f(x) \\, \\mathrm{d}x\n$$\nThis expression depends on $\\delta u$, which in turn depends on $\\delta f$. To find the gradient, we must express $DJ(f)[\\delta f]$ as a single inner product with $\\delta f$. This is achieved by introducing an adjoint state $p \\in H_{0}^{1}(\\Omega)$. We define the adjoint problem to eliminate the term involving $\\delta u$. Let $p \\in H_{0}^{1}(\\Omega)$ be the unique weak solution to the following adjoint equation:\n$$\na(v,p) = \\int_{\\Omega} v(x) \\left( \\sum_{k=1}^{m} (C(u(f))_{k} - d_{k}) \\phi_{k}(x) \\right) \\, \\mathrm{d}x \\quad \\forall v \\in H_{0}^{1}(\\Omega)\n$$\nThe bilinear form $a(\\cdot, \\cdot)$ is symmetric, i.e., $a(v,p) = a(p,v)$. The strong form of the adjoint equation is $-\\Delta p = \\sum_{k=1}^{m}(C(u(f))_k - d_k)\\phi_k$ in $\\Omega$ with $p|_{\\partial\\Omega}=0$.\n\nBy choosing the test function $v = \\delta u \\in H_{0}^{1}(\\Omega)$ in the adjoint weak formulation, we get:\n$$\na(\\delta u, p) = \\int_{\\Omega} \\delta u(x) \\left( \\sum_{k=1}^{m} (C(u(f))_{k} - d_{k}) \\phi_{k}(x) \\right) \\, \\mathrm{d}x\n$$\nThis is precisely the first term in our expression for $DJ(f)[\\delta f]$. So, we can write:\n$$\nDJ(f)[\\delta f] = a(\\delta u, p) + \\lambda \\langle f, \\delta f \\rangle_{L^{2}(\\Omega)}\n$$\nNow, we use the weak formulation for $\\delta u$, which is $a(\\delta u, v) = \\langle \\delta f, v \\rangle_{L^{2}(\\Omega)}$ for all $v \\in H_{0}^{1}(\\Omega)$. Choosing the test function $v=p \\in H_{0}^{1}(\\Omega)$ in this formulation gives:\n$$\na(\\delta u, p) = \\langle \\delta f, p \\rangle_{L^{2}(\\Omega)} = \\int_{\\Omega} \\delta f(x) p(x) \\, \\mathrm{d}x\n$$\nSubstituting this back into the expression for $DJ(f)[\\delta f]$:\n$$\nDJ(f)[\\delta f] = \\int_{\\Omega} p(x) \\delta f(x) \\, \\mathrm{d}x + \\lambda \\int_{\\Omega} f(x) \\delta f(x) \\, \\mathrm{d}x\n$$\n$$\nDJ(f)[\\delta f] = \\int_{\\Omega} (\\lambda f(x) + p(x)) \\delta f(x) \\, \\mathrm{d}x = \\langle \\lambda f + p, \\delta f \\rangle_{L^{2}(\\Omega)}\n$$\nBy the Riesz representation theorem, which defines the gradient in a Hilbert space, we can identify the $L^2(\\Omega)$-gradient of $J$ at $f$ as the term inside the inner product with $\\delta f$.\n\nTherefore, the $L^{2}(\\Omega)$-gradient of $J(f)$ is:\n$$\n\\nabla_{f} J(f) = \\lambda f + p\n$$\nwhere $p \\in H_{0}^{1}(\\Omega)$ is the adjoint state defined by the weak formulation above, which depends on the state $u(f)$ and the data $d$.",
            "answer": "$$\n\\boxed{\\lambda f + p}\n$$"
        },
        {
            "introduction": "The concept of a \"gradient\" is not absolute; its definition depends on the geometry, or inner product, of the space of control variables. This practice explores this crucial idea by contrasting the gradient in the standard $L^2(\\Omega)$ space with the gradient in the Sobolev space $H^1(\\Omega)$, which encodes information about smoothness. You will discover how the Riesz map provides the transformation between these representations and why choosing the right geometry is a key step in designing efficient optimization algorithms.",
            "id": "3383644",
            "problem": "Consider a bounded open interval domain $\\Omega = (0,\\pi)$ and the control space given by the Sobolev space with homogeneous Dirichlet boundary conditions $H^{1}_{0}(\\Omega)$. Let the state $u \\in H^{1}_{0}(\\Omega)$ be determined by a linear elliptic partial differential equation (PDE) with source control $m \\in H^{1}_{0}(\\Omega)$:\n$$\n-\\frac{d^{2}u}{dx^{2}} + \\beta\\, u = m \\quad \\text{in } \\Omega, \\qquad u(0)=u(\\pi)=0,\n$$\nwhere $\\beta  0$ is a fixed constant. Observations are taken to be identity, and the cost functional is a Lebesgue $L^{2}(\\Omega)$ misfit:\n$$\nJ(m) = \\frac{1}{2} \\int_{\\Omega} \\left(u(x) - d(x)\\right)^{2}\\, dx,\n$$\nwith given data $d \\in L^{2}(\\Omega)$. In the context of inverse problems and data assimilation, gradients are defined via the Riesz representation theorem with respect to the geometry of the control space. Specifically, the $L^{2}(\\Omega)$ inner product is given by $\\langle a,b \\rangle_{L^{2}}=\\int_{\\Omega} a\\,b\\,dx$, and the $H^{1}_{0}(\\Omega)$ inner product is given by $\\langle a,b \\rangle_{H^{1}_{0}}=\\int_{\\Omega} \\nabla a \\cdot \\nabla b\\,dx$.\n\nStarting from first principles—namely, the definition of the Gâteaux derivative of $J$, the state equation, and an adjoint method grounded in the variational structure of the PDE—derive the adjoint gradient with respect to the $L^{2}(\\Omega)$ inner product, and then use the Riesz map associated with $H^{1}_{0}(\\Omega)$ to express the gradient in the $H^{1}_{0}(\\Omega)$ geometry. Explain clearly how the Riesz map alters the expression of the gradient and the consequent descent direction in an iterative minimization of $J$.\n\nThen, for the specific choice $d(x)=0$ and $m(x) = \\sin(x) + \\sin(2x)$, carry out the derivation explicitly and provide the final analytic expression for the Sobolev $H^{1}_{0}(\\Omega)$-gradient $\\nabla_{H^{1}} J(m)$ as a function of $x$ and $\\beta$.\n\nYour final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem is valid. It is a well-defined mathematical problem in the field of PDE-constrained optimization and inverse problems, based on established principles of functional analysis and calculus of variations. All necessary components, including the state equation, cost functional, and function space definitions, are provided and are mutually consistent.\n\nWe begin by deriving the gradient of the cost functional $J(m)$ with respect to the control variable $m \\in H^{1}_{0}(\\Omega)$. The derivation proceeds in three main stages: first, we find the Gâteaux derivative of $J(m)$; second, we introduce the adjoint state to express this derivative as an inner product, which yields the gradient in the $L^{2}(\\Omega)$ geometry; and third, we use the Riesz representation theorem in $H^{1}_{0}(\\Omega)$ to find the gradient in the Sobolev geometry.\n\nLet $u(m)$ denote the solution to the state equation for a given control $m$. The cost functional is\n$$\nJ(m) = \\frac{1}{2} \\int_{\\Omega} (u(m)(x) - d(x))^{2}\\, dx\n$$\n\n**1. Gâteaux Derivative**\n\nThe Gâteaux derivative of $J$ at $m$ in the direction $h \\in H^{1}_{0}(\\Omega)$ is defined as\n$$\nJ'(m; h) = \\lim_{\\epsilon \\to 0} \\frac{J(m+\\epsilon h) - J(m)}{\\epsilon}\n$$\nLet $u(m+\\epsilon h)$ be the state corresponding to the perturbed control $m+\\epsilon h$. The state equation is linear, so $u(m+\\epsilon h) = u(m) + \\epsilon u_{h}$, where $u_h$ is the solution to the linearized state equation:\n$$\n-\\frac{d^{2}u_{h}}{dx^{2}} + \\beta u_{h} = h \\quad \\text{in } \\Omega, \\qquad u_{h}(0)=u_{h}(\\pi)=0\n$$\nSubstituting this into the definition of the Gâteaux derivative:\n\\begin{align*}\nJ(m+\\epsilon h) = \\frac{1}{2} \\int_{\\Omega} (u(m)(x) + \\epsilon u_{h}(x) - d(x))^{2}\\, dx \\\\\n= \\frac{1}{2} \\int_{\\Omega} \\left[ (u(m)(x) - d(x))^{2} + 2\\epsilon (u(m)(x) - d(x))u_{h}(x) + \\epsilon^{2} u_{h}(x)^{2} \\right] dx \\\\\n= J(m) + \\epsilon \\int_{\\Omega} (u(m)(x) - d(x))u_{h}(x)\\, dx + O(\\epsilon^{2})\n\\end{align*}\nTherefore, the Gâteaux derivative is\n$$\nJ'(m; h) = \\int_{\\Omega} (u(m)(x) - d(x)) u_{h}(x)\\, dx\n$$\n\n**2. Adjoint Method and the $L^{2}$ Gradient**\n\nThe expression for $J'(m; h)$ depends on $u_h$, which in turn depends on $h$ through a differential equation. The adjoint method provides a way to express the derivative as a direct inner product with $h$. We introduce an adjoint state $p \\in H^{1}_{0}(\\Omega)$ which is the solution to the adjoint equation.\n\nLet $\\mathcal{L} = -\\frac{d^{2}}{dx^{2}} + \\beta$ be the state operator. The state equation for the perturbation is $\\mathcal{L}u_{h} = h$. The operator $\\mathcal{L}$ with homogeneous Dirichlet boundary conditions is self-adjoint with respect to the $L^{2}(\\Omega)$ inner product. That is, for any $v,w \\in H^{1}_{0}(\\Omega)$, $\\langle \\mathcal{L}v, w \\rangle_{L^{2}} = \\langle v, \\mathcal{L}w \\rangle_{L^{2}}$.\n\nWe define the adjoint state $p$ as the solution to:\n$$\n\\mathcal{L}p = u(m) - d \\iff -\\frac{d^{2}p}{dx^{2}} + \\beta p = u(m) - d \\quad \\text{in } \\Omega, \\qquad p(0)=p(\\pi)=0\n$$\nNow, we rewrite the Gâteaux derivative using the adjoint state:\n$$\nJ'(m; h) = \\int_{\\Omega} (u(m) - d) u_{h}\\, dx = \\int_{\\Omega} (\\mathcal{L}p) u_{h}\\, dx = \\langle \\mathcal{L}p, u_{h} \\rangle_{L^{2}}\n$$\nUsing the self-adjoint property of $\\mathcal{L}$:\n$$\nJ'(m; h) = \\langle p, \\mathcal{L}u_{h} \\rangle_{L^{2}} = \\int_{\\Omega} p (\\mathcal{L}u_{h})\\, dx\n$$\nSince $\\mathcal{L}u_{h} = h$, we have:\n$$\nJ'(m; h) = \\int_{\\Omega} p(x) h(x)\\, dx = \\langle p, h \\rangle_{L^{2}}\n$$\nBy the Riesz representation theorem in $L^{2}(\\Omega)$, the gradient with respect to the $L^{2}$ inner product is the adjoint state itself:\n$$\n\\nabla_{L^{2}} J(m) = p\n$$\n\n**3. Riesz Map and the $H^{1}_{0}$ Gradient**\n\nWe now seek the gradient with respect to the $H^{1}_{0}(\\Omega)$ inner product, which we denote by $g = \\nabla_{H^{1}} J(m)$. By definition, $g \\in H^{1}_{0}(\\Omega)$ must satisfy\n$$\nJ'(m; h) = \\langle g, h \\rangle_{H^{1}_{0}} \\quad \\forall h \\in H^{1}_{0}(\\Omega)\n$$\nwhere $\\langle g, h \\rangle_{H^{1}_{0}} = \\int_{\\Omega} \\nabla g \\cdot \\nabla h\\, dx = \\int_{0}^{\\pi} g'(x) h'(x)\\, dx$.\nEquating the two expressions for the derivative $J'(m; h)$:\n$$\n\\langle g, h \\rangle_{H^{1}_{0}} = \\langle p, h \\rangle_{L^{2}} \\implies \\int_{0}^{\\pi} g'(x) h'(x)\\, dx = \\int_{0}^{\\pi} p(x) h(x)\\, dx\n$$\nThis is the weak formulation of a boundary value problem for $g$. To find the strong form, we integrate the left side by parts:\n$$\n\\int_{0}^{\\pi} g'(x) h'(x)\\, dx = - \\int_{0}^{\\pi} g''(x) h(x)\\, dx + [g'(x)h(x)]_{0}^{\\pi}\n$$\nSince $h \\in H^{1}_{0}(\\Omega)$, $h(0) = h(\\pi) = 0$, so the boundary term vanishes. The identity becomes:\n$$\n\\int_{0}^{\\pi} (-g''(x)) h(x)\\, dx = \\int_{0}^{\\pi} p(x) h(x)\\, dx\n$$\nSince this holds for all $h \\in H^{1}_{0}(\\Omega)$, the fundamental lemma of calculus of variations implies that the integrands are equal:\n$$\n-g''(x) = p(x) \\quad \\text{in } \\Omega\n$$\nThe gradient $g=\\nabla_{H^{1}} J(m)$ must be in the control space $H^{1}_{0}(\\Omega)$, so it satisfies homogeneous Dirichlet boundary conditions, $g(0)=g(\\pi)=0$.\n\nThe transformation from the $L^{2}$ gradient $p$ to the $H^{1}_{0}$ gradient $g$ is a Riesz map. It involves solving a Poisson equation, which is an elliptic smoothing operator. In an iterative optimization scheme ($m_{k+1} = m_{k} - \\alpha_{k} \\nabla J(m_k)$), using the $H^{1}_{0}$ gradient $g$ instead of the $L^{2}$ gradient $p$ constitutes a form of preconditioning. The descent direction $g$ is a smoothed version of $p$, which penalizes high-frequency oscillations and often leads to significantly faster convergence by taking steps that are better adapted to the topology of the control space.\n\n**4. Explicit Calculation for the Specific Case**\n\nWe are given $d(x)=0$ and $m(x)=\\sin(x)+\\sin(2x)$. We proceed step-by-step.\n\nFirst, solve for the state $u(x)$ from $-\\frac{d^{2}u}{dx^{2}} + \\beta u = \\sin(x)+\\sin(2x)$ with $u(0)=u(\\pi)=0$. Since $\\{\\sin(kx)\\}_{k=1}^\\infty$ are eigenfunctions of $-\\frac{d^2}{dx^2}$ with eigenvalues $k^2$, we assume a solution of the form $u(x) = c_1 \\sin(x) + c_2 \\sin(2x)$.\n$$\n(1^{2}c_1 \\sin(x) + 2^{2}c_2 \\sin(2x)) + \\beta (c_1 \\sin(x) + c_2 \\sin(2x)) = \\sin(x)+\\sin(2x)\n$$\n$$\n(1+\\beta)c_1 \\sin(x) + (4+\\beta)c_2 \\sin(2x) = \\sin(x)+\\sin(2x)\n$$\nMatching coefficients, we get $c_1 = \\frac{1}{1+\\beta}$ and $c_2 = \\frac{1}{4+\\beta}$. Thus, the state is:\n$$\nu(x) = \\frac{1}{1+\\beta}\\sin(x) + \\frac{1}{4+\\beta}\\sin(2x)\n$$\nSecond, solve for the adjoint state $p(x)$ from $-\\frac{d^{2}p}{dx^{2}} + \\beta p = u(x) - d(x) = u(x)$ with $p(0)=p(\\pi)=0$.\n$$\n-\\frac{d^{2}p}{dx^{2}} + \\beta p = \\frac{1}{1+\\beta}\\sin(x) + \\frac{1}{4+\\beta}\\sin(2x)\n$$\nAgain, assuming a solution $p(x) = p_1 \\sin(x) + p_2 \\sin(2x)$:\n$$\n(1+\\beta)p_1 \\sin(x) + (4+\\beta)p_2 \\sin(2x) = \\frac{1}{1+\\beta}\\sin(x) + \\frac{1}{4+\\beta}\\sin(2x)\n$$\nMatching coefficients, $p_1 = \\frac{1}{(1+\\beta)^{2}}$ and $p_2 = \\frac{1}{(4+\\beta)^{2}}$. The adjoint state (and $L^2$ gradient) is:\n$$\np(x) = \\frac{1}{(1+\\beta)^{2}}\\sin(x) + \\frac{1}{(4+\\beta)^{2}}\\sin(2x)\n$$\nThird, solve for the $H^{1}_{0}$ gradient $g(x) = \\nabla_{H^{1}} J(m)$ from $-g''(x) = p(x)$ with $g(0)=g(\\pi)=0$.\n$$\n-g''(x) = \\frac{1}{(1+\\beta)^{2}}\\sin(x) + \\frac{1}{(4+\\beta)^{2}}\\sin(2x)\n$$\nAssuming a solution $g(x) = g_1 \\sin(x) + g_2 \\sin(2x)$:\n$$\n-(-(1)^{2}g_1 \\sin(x) - (2)^{2}g_2 \\sin(2x)) = g_1 \\sin(x) + 4 g_2 \\sin(2x)\n$$\nEquating this to $p(x)$:\n$$\ng_1 \\sin(x) + 4g_2 \\sin(2x) = \\frac{1}{(1+\\beta)^{2}}\\sin(x) + \\frac{1}{(4+\\beta)^{2}}\\sin(2x)\n$$\nMatching coefficients, $g_1 = \\frac{1}{(1+\\beta)^{2}}$ and $4g_2 = \\frac{1}{(4+\\beta)^{2}}$, which means $g_2 = \\frac{1}{4(4+\\beta)^{2}}$.\nThe final analytic expression for the $H^{1}_{0}$ gradient is:\n$$\ng(x) = \\nabla_{H^{1}} J(m) = \\frac{1}{(1+\\beta)^{2}}\\sin(x) + \\frac{1}{4(4+\\beta)^{2}}\\sin(2x)\n$$",
            "answer": "$$\\boxed{\\frac{\\sin(x)}{(1+\\beta)^{2}} + \\frac{\\sin(2x)}{4(4+\\beta)^{2}}}$$"
        },
        {
            "introduction": "Before applying sophisticated optimization techniques, we must first ensure our inverse problem is well-posed, meaning our mathematical model is sound. This exercise challenges you to analyze the theoretical validity of a common practice: assimilating sparse, pointwise observations. By applying the Sobolev embedding theorems, you will determine why the dimensionality of a problem critically affects whether point measurements are well-defined for states in $H^1(\\Omega)$ and learn about the standard remedies used in practice.",
            "id": "3383671",
            "problem": "Let $\\Omega \\subset \\mathbb{R}^d$ be a bounded Lipschitz domain with $d \\in \\{1,2,3\\}$. In variational data assimilation for partial differential equations, it is common to take the state space to be the first-order Sobolev space $H^1(\\Omega)$, consisting of functions $u \\in L^2(\\Omega)$ whose weak first derivatives also lie in $L^2(\\Omega)$, equipped with the norm $\\|u\\|_{H^1(\\Omega)}^2 = \\|u\\|_{L^2(\\Omega)}^2 + \\|\\nabla u\\|_{L^2(\\Omega)}^2$. Suppose one desires to assimilate sparse interior observations that measure $u(x_i)$ at finitely many distinct points $\\{x_i\\}_{i=1}^m \\subset \\Omega$. Consider the observation operator $H$ defined by $H(u) = (u(x_1), \\dots, u(x_m))$.\n\nUsing only foundational facts about Lebesgue spaces, Sobolev spaces, Sobolev embeddings, and dual spaces, identify the correct statements regarding the well-definedness and continuity of point evaluations $u(x_i)$ when $u \\in H^1(\\Omega)$ in $d=1,2,3$, and the implications for the design of well-posed data assimilation operators. Select all that apply.\n\nA. For $d=1$, each $u \\in H^1(\\Omega)$ is continuous on $\\overline{\\Omega}$, so $u(x_i)$ is well-defined for all $i$, and the map $H: H^1(\\Omega) \\to \\mathbb{R}^m$ given by point evaluation is a bounded linear operator. For $d=2,3$, $u(x_i)$ is also well-defined because $H^1(\\Omega)$ functions have representatives defined almost everywhere, and moreover $\\delta_{x_i} \\in H^{-1}(\\Omega)$ ensures boundedness of the functional $u \\mapsto u(x_i)$.\n\nB. For every $d \\in \\{1,2,3\\}$, the embedding $H^1(\\Omega) \\hookrightarrow L^\\infty(\\Omega)$ holds on bounded Lipschitz domains, so point evaluation $u \\mapsto u(x_i)$ is a bounded linear functional in all three dimensions.\n\nC. If observations are modeled as point samples, the operator $H$ is continuous on $H^1(\\Omega)$ only for $d=1$. In dimensions $d=2$ and $d=3$, to obtain a well-defined and bounded observation operator one must either increase the state space regularity to $H^s(\\Omega)$ with $s  d/2$ (so that $H^s(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$), or replace point observations by local averages, for instance $H(u)_i = \\int_{\\Omega} u(x) \\varphi_i(x) \\,\\mathrm{d}x$ with smooth kernels $\\varphi_i$ supported near $x_i$.\n\nD. In $d=2$, every $u \\in H^1(\\Omega)$ admits a representative that is continuous except on a set of Lebesgue measure zero, so for any fixed interior points $\\{x_i\\}$ the point values $u(x_i)$ are well-defined and independent of the choice of representative.\n\nE. The Dirac distribution at $x_i$, denoted $\\delta_{x_i}$, belongs to the dual space $H^{-1}(\\Omega)$ if and only if $d=1$. Consequently, the dual pairing $\\langle \\delta_{x_i}, u \\rangle = u(x_i)$ defines a bounded linear functional on $H^1(\\Omega)$ in one dimension, while for $d=2,3$ point evaluation is not a bounded functional on $H^1(\\Omega)$; therefore, pointwise observation operators are ill-posed on $H^1(\\Omega)$ in $d=2,3$ unless one increases the state space regularity or mollifies the observations.\n\nChoose all correct options.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   The domain $\\Omega$ is a bounded Lipschitz domain in $\\mathbb{R}^d$.\n-   The spatial dimension is $d \\in \\{1, 2, 3\\}$.\n-   The state space is the first-order Sobolev space $H^1(\\Omega)$, defined as $H^1(\\Omega) = \\{u \\in L^2(\\Omega) : \\nabla u \\in L^2(\\Omega)\\}$.\n-   The norm on $H^1(\\Omega)$ is given by $\\|u\\|_{H^1(\\Omega)}^2 = \\|u\\|_{L^2(\\Omega)}^2 + \\|\\nabla u\\|_{L^2(\\Omega)}^2$.\n-   Observations are taken at a finite set of distinct points $\\{x_i\\}_{i=1}^m \\subset \\Omega$.\n-   The observation operator is defined by point evaluation: $H(u) = (u(x_1), \\dots, u(x_m))$.\n-   The question is to identify correct statements about the well-definedness and continuity of the point evaluation functionals $u \\mapsto u(x_i)$ on the space $H^1(\\Omega)$ for $d=1, 2, 3$, and the related implications for data assimilation.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the mathematical theory of Sobolev spaces and their application to partial differential equations and inverse problems, which is a standard area of study in applied mathematics. The terms used ($L^2$, $H^1$, Sobolev embedding, dual space, Lipschitz domain) are well-defined and standard. The problem is well-posed, asking for a theoretical analysis of mathematical statements based on established theorems. It is objective and free of ambiguity. The provided information is self-contained and sufficient to answer the question. No scientific, logical, or factual flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Principles\nThe core of this problem lies in the Sobolev Embedding Theorem. This theorem describes when a Sobolev space $W^{k,p}(\\Omega)$ continuously embeds into another function space, such as a Lebesgue space $L^q(\\Omega)$ or the space of continuous functions $C^0(\\overline{\\Omega})$.\n\nThe space in question is $H^1(\\Omega)$, which is the Sobolev space $W^{1,2}(\\Omega)$. Therefore, we have $k=1$ and $p=2$.\n\nAn observation operator based on point evaluation, $u \\mapsto u(x_i)$, is well-defined and continuous (i.e., a bounded linear functional) on a function space if and only if that function space embeds continuously into a space of continuous functions, such as $C^0(\\overline{\\Omega})$. An element of $H^1(\\Omega)$ is an equivalence class of functions that are equal almost everywhere. For point evaluation to be well-defined, every function in the equivalence class must have the same value at the point $x_i$, which requires continuity at that point.\n\nThe Sobolev Embedding Theorem states that for a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^d$, the embedding $W^{k,p}(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$ holds if $kp  d$.\n\nLet us apply this condition to our case, $H^1(\\Omega) = W^{1,2}(\\Omega)$:\n-   Condition: $k \\cdot p  d \\implies 1 \\cdot 2  d \\implies 2  d$.\n\nWe analyze this condition for each dimension $d \\in \\{1, 2, 3\\}$:\n-   **Case $d=1$**: The condition $2  1$ is satisfied. Thus, the embedding $H^1(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$ holds. This means that every function $u \\in H^1(\\Omega)$ (which is an equivalence class) contains a unique representative that is continuous on $\\overline{\\Omega}$. Consequently, the point value $u(x_i)$ is well-defined and the functional $u \\mapsto u(x_i)$ is bounded, since $|u(x_i)| \\leq \\|u\\|_{C^0(\\overline{\\Omega})} \\leq C \\|u\\|_{H^1(\\Omega)}$ for some constant $C$. The operator $H: H^1(\\Omega) \\to \\mathbb{R}^m$ is therefore a bounded linear operator.\n\n-   **Case $d=2$**: The condition $2  2$ is false. The embedding $H^1(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$ does not hold. Functions in $H^1(\\Omega)$ for $d=2$ are not necessarily continuous or even bounded. For example, the function $u(x) = \\log(\\log(R/|x|))$ defined on a ball of radius $R/e$ belongs to $H^1$ but is unbounded at the origin. Since functions in $H^1(\\Omega)$ may not be continuous, point evaluation is not a well-defined operation for every element of the space. It is not a bounded functional.\n\n-   **Case $d=3$**: The condition $2  3$ is false. The embedding $H^1(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$ again fails. The situation is \"worse\" than in $d=2$. For $d=3$, $H^1(\\Omega)$ embeds into $L^6(\\Omega)$, but not into $L^\\infty(\\Omega)$, so functions can be unbounded. Point evaluation is not a well-defined, bounded functional on $H^1(\\Omega)$.\n\nAnother perspective is through duality. The point evaluation functional $u \\mapsto u(x_i)$ is represented by the Dirac distribution $\\delta_{x_i}$. This functional is bounded on $H^1(\\Omega)$ if and only if $\\delta_{x_i}$ is in the dual space $(H^1(\\Omega))'$. This is true if and only if the embedding $H^1(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$ holds, which as we have shown, is true only for $d  2$. The common notation for the dual of $H^1_0(\\Omega)$ is $H^{-1}(\\Omega)$. While the dual of $H^1(\\Omega)$ is technically different, the condition for $\\delta_{x_i}$ to be a member is the same.\n\nFor $d \\ge 2$, if the observation operator $H$ is not continuous, the data assimilation cost functional, which often includes a term like $\\|H(u) - y_{\\text{obs}}\\|^2$, is ill-posed because $H(u)$ is not defined for all $u$ in the state space $H^1(\\Omega)$. To remedy this, one must either:\n1.  Increase the regularity of the state space, for example, by working in $H^s(\\Omega)$ with $s  d/2$. By the Sobolev embedding theorem, this ensures $H^s(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$.\n2.  Modify the observation operator to be a continuous functional on $H^1(\\Omega)$. For example, using local averages: $H_i(u) = \\int_\\Omega u(x) \\varphi_i(x) \\,dx$. If $\\varphi_i \\in L^2(\\Omega)$, then by the Cauchy-Schwarz inequality, $|H_i(u)| \\le \\|u\\|_{L^2(\\Omega)} \\|\\varphi_i\\|_{L^2(\\Omega)} \\le \\|u\\|_{H^1(\\Omega)} \\|\\varphi_i\\|_{L^2(\\Omega)}$, which shows the functional is bounded on $H^1(\\Omega)$.\n\n### Option-by-Option Analysis\n\n**A. For $d=1$, each $u \\in H^1(\\Omega)$ is continuous on $\\overline{\\Omega}$, so $u(x_i)$ is well-defined for all $i$, and the map $H: H^1(\\Omega) \\to \\mathbb{R}^m$ given by point evaluation is a bounded linear operator. For $d=2,3$, $u(x_i)$ is also well-defined because $H^1(\\Omega)$ functions have representatives defined almost everywhere, and moreover $\\delta_{x_i} \\in H^{-1}(\\Omega)$ ensures boundedness of the functional $u \\mapsto u(x_i)$.**\n\nThe first sentence concerning $d=1$ is correct, based on our derivation ($H^1(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$ for $d=1$). The second sentence is incorrect. The fact that $H^1$ functions are defined almost everywhere does not make point evaluation well-defined on the equivalence class. More importantly, the statement that $\\delta_{x_i} \\in H^{-1}(\\Omega)$ (the dual space) for $d=2, 3$ is false. It is true only for $d=1$.\n**Verdict: Incorrect.**\n\n**B. For every $d \\in \\{1,2,3\\}$, the embedding $H^1(\\Omega) \\hookrightarrow L^\\infty(\\Omega)$ holds on bounded Lipschitz domains, so point evaluation $u \\mapsto u(x_i)$ is a bounded linear functional in all three dimensions.**\n\nThis statement is false. The embedding $H^1(\\Omega) \\hookrightarrow L^\\infty(\\Omega)$ (and the related $H^1(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$) holds for $d=1$, but it fails for $d=2$ and $d=3$. Therefore, the claim that it holds for all three dimensions is incorrect.\n**Verdict: Incorrect.**\n\n**C. If observations are modeled as point samples, the operator $H$ is continuous on $H^1(\\Omega)$ only for $d=1$. In dimensions $d=2$ and $d=3$, to obtain a well-defined and bounded observation operator one must either increase the state space regularity to $H^s(\\Omega)$ with $s  d/2$ (so that $H^s(\\Omega) \\hookrightarrow C^0(\\overline{\\Omega})$), or replace point observations by local averages, for instance $H(u)_i = \\int_{\\Omega} u(x) \\varphi_i(x) \\,\\mathrm{d}x$ with smooth kernels $\\varphi_i$ supported near $x_i$.**\n\nThis statement is entirely consistent with our derivation.\n-   The operator $H$ is continuous on $H^1(\\Omega)$ only for $d=1$: Correct, as per the Sobolev embedding $H^1 \\hookrightarrow C^0$.\n-   For $d=2,3$, the proposed remedies are standard and correct:\n    1.  Increasing regularity to $H^s(\\Omega)$ with $s  d/2$ correctly invokes the general Sobolev embedding theorem to guarantee continuity.\n    2.  Replacing point samples with local averages is a standard regularization technique that produces bounded functionals on $H^1(\\Omega)$, as shown in the derivation.\n**Verdict: Correct.**\n\n**D. In $d=2$, every $u \\in H^1(\\Omega)$ admits a representative that is continuous except on a set of Lebesgue measure zero, so for any fixed interior points $\\{x_i\\}$ the point values $u(x_i)$ are well-defined and independent of the choice of representative.**\n\nThe premise is trivially true for any measurable function but is misleading. The conclusion does not follow. For a function class defined up to equality almost everywhere, a value at a single point (a set of measure zero) is not well-defined. One can always modify a function at a finite number of points without changing its equivalence class in $H^1(\\Omega)$. Therefore, the point values $u(x_i)$ are not well-defined. There exist functions in $H^1(\\Omega)$ for $d=2$ which are unbounded, making point evaluation impossible at the point of singularity.\n**Verdict: Incorrect.**\n\n**E. The Dirac distribution at $x_i$, denoted $\\delta_{x_i}$, belongs to the dual space $H^{-1}(\\Omega)$ if and only if $d=1$. Consequently, the dual pairing $\\langle \\delta_{x_i}, u \\rangle = u(x_i)$ defines a bounded linear functional on $H^1(\\Omega)$ in one dimension, while for $d=2,3$ point evaluation is not a bounded functional on $H^1(\\Omega)$; therefore, pointwise observation operators are ill-posed on $H^1(\\Omega)$ in $d=2,3$ unless one increases the state space regularity or mollifies the observations.**\n\nThis statement provides the correct analysis from the perspective of duality.\n-   \"The Dirac distribution... belongs to the dual space... if and only if $d=1$\": This is correct. Although $H^{-1}(\\Omega)$ usually denotes the dual of $H^1_0(\\Omega)$, the statement is physically and mathematically correct for the dual of $H^1(\\Omega)$ as well: the functional $u \\mapsto u(x_i)$ is bounded on $H^1(\\Omega)$ if and only if $d=1$.\n-   \"Consequently... defines a bounded linear functional on $H^1(\\Omega)$ in one dimension, while for $d=2,3$ point evaluation is not a bounded functional\": This is a correct deduction.\n-   \"therefore, pointwise observation operators are ill-posed... unless one increases... regularity or mollifies... observations\": This correctly identifies the implication\n(ill-posedness) and the standard remedies, mirroring the correct conclusion of option C.\nThe entire statement is substantively correct and provides a deep, accurate explanation.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{CE}$$"
        }
    ]
}