{
    "hands_on_practices": [
        {
            "introduction": "The Singular Value Decomposition provides a powerful basis for analyzing and solving linear inverse problems. However, a naive inversion that includes components corresponding to small singular values will excessively amplify noise in the data. This practice introduces Truncated SVD (TSVD) as a fundamental regularization method, where the solution is constructed using only the first $k$ singular components. By deriving and minimizing the Mean Squared Error (MSE), you will directly confront the classic bias-variance tradeoff, learning how the choice of the truncation parameter $k$ balances the loss of signal (bias) against the propagation of noise (variance) .",
            "id": "3401167",
            "problem": "Consider a linear inverse problem with a forward operator $A \\in \\mathbb{R}^{m \\times n}$ and observations $y \\in \\mathbb{R}^{m}$ given by $y = A x^{\\dagger} + \\eta$, where $x^{\\dagger} \\in \\mathbb{R}^{n}$ is the unknown state and $\\eta \\in \\mathbb{R}^{m}$ is a zero-mean stochastic noise. Let the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n} \\geq 0$. Define the Truncated Singular Value Decomposition (TSVD) estimator of rank $k$, denoted $x_{k} \\in \\mathbb{R}^{n}$, by $x_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{\\sigma_{i}} v_{i}$, where $u_{i}$ and $v_{i}$ are the left and right singular vectors corresponding to $\\sigma_{i}$. Let the Expected Mean Squared Error (MSE) be $\\mathbb{E}\\|x_{k} - x^{\\dagger}\\|^{2}$, where the expectation is taken with respect to the distribution of $\\eta$. Assume a mode-dependent noise level model $\\mathbb{E}|u_{i}^{\\top} \\eta|^{2} = \\phi_{i}$ for $i = 1, \\dots, n$, and write the true coefficients of $x^{\\dagger}$ in the right singular vector basis as $\\alpha_{i} = v_{i}^{\\top} x^{\\dagger}$.\n\nStarting from the definitions above and without invoking any shortcut formulas, derive a formula for $\\mathbb{E}\\|x_{k} - x^{\\dagger}\\|^{2}$ in terms of $(\\sigma_{i})$, $(\\phi_{i})$, and $(\\alpha_{i})$, and then formulate the discrete optimization problem to choose $k$ that minimizes the expected error.\n\nNow, consider the concrete case with $n = 8$, singular values $\\sigma_{i} = 16 \\cdot 2^{-(i-1)}$ for $i = 1, \\dots, 8$, a white-noise model $\\phi_{i} = 1$ for all $i$, and true coefficients $\\alpha_{i} = \\frac{1}{i}$ for $i = 1, \\dots, 8$. Compute the minimizer $k^{\\star} \\in \\{0, 1, \\dots, 8\\}$ of the expected MSE. Your final answer must be the value of $k^{\\star}$ as a single integer. No rounding is required.",
            "solution": "The problem statement is evaluated as valid, being scientifically grounded, well-posed, and self-contained. It presents a standard exercise in the analysis of regularization methods for linear inverse problems.\n\nThe primary objective is to find the integer $k^{\\star} \\in \\{0, 1, \\dots, 8\\}$ that minimizes the Expected Mean Squared Error (MSE) of the Truncated Singular Value Decomposition (TSVD) estimator. First, we derive the general formula for the MSE, and then we apply it to the specific case provided.\n\nLet the error vector be $e_k = x_k - x^{\\dagger}$. The MSE is defined as $\\mathbb{E}\\|e_k\\|^2 = \\mathbb{E}\\|x_k - x^{\\dagger}\\|^2$. The set of right singular vectors $\\{v_i\\}_{i=1}^n$ forms an orthonormal basis for $\\mathbb{R}^n$. Consequently, the squared norm of the error vector can be expressed as the sum of its squared components in this basis:\n$$\n\\|x_k - x^{\\dagger}\\|^2 = \\sum_{i=1}^{n} \\left(v_i^{\\top}(x_k - x^{\\dagger})\\right)^2\n$$\nThe components are $v_i^{\\top}x_k - v_i^{\\top}x^{\\dagger}$. By definition, we have $\\alpha_i = v_i^{\\top}x^{\\dagger}$.\n\nTo evaluate $v_i^{\\top}x_k$, we use the definition of the TSVD estimator, $x_k = \\sum_{j=1}^{k} \\frac{u_j^{\\top} y}{\\sigma_j} v_j$:\n$$\nv_i^{\\top}x_k = v_i^{\\top}\\left(\\sum_{j=1}^{k} \\frac{u_j^{\\top} y}{\\sigma_j} v_j\\right) = \\sum_{j=1}^{k} \\frac{u_j^{\\top} y}{\\sigma_j} (v_i^{\\top}v_j)\n$$\nDue to the orthonormality of the right singular vectors ($v_i^{\\top}v_j = \\delta_{ij}$), this simplifies to:\n$$\nv_i^{\\top}x_k = \\begin{cases} \\frac{u_i^{\\top} y}{\\sigma_i}  \\text{if } 1 \\leq i \\leq k \\\\ 0  \\text{if } i  k \\end{cases}\n$$\nNext, we expand the projected data term $u_i^{\\top}y$ using the linear model $y = Ax^{\\dagger} + \\eta$ and the SVD of $A$, $A = \\sum_{j=1}^n \\sigma_j u_j v_j^{\\top}$:\n$$\nu_i^{\\top}y = u_i^{\\top}(Ax^{\\dagger} + \\eta) = u_i^{\\top}A x^{\\dagger} + u_i^{\\top}\\eta\n$$\nThe term $u_i^{\\top}A x^{\\dagger}$ becomes:\n$$\nu_i^{\\top}A x^{\\dagger} = u_i^{\\top}\\left(\\sum_{j=1}^n \\sigma_j u_j v_j^{\\top}\\right)x^{\\dagger} = \\sum_{j=1}^n \\sigma_j (u_i^{\\top}u_j)(v_j^{\\top}x^{\\dagger})\n$$\nUsing the orthonormality of the left singular vectors ($u_i^{\\top}u_j = \\delta_{ij}$) and the definition $\\alpha_j = v_j^{\\top}x^{\\dagger}$, we find:\n$$\nu_i^{\\top}A x^{\\dagger} = \\sigma_i \\alpha_i\n$$\nThus, the projected data is $u_i^{\\top}y = \\sigma_i \\alpha_i + u_i^{\\top}\\eta$.\n\nWe can now express the components of the error vector.\nFor $1 \\leq i \\leq k$:\n$$\nv_i^{\\top}(x_k - x^{\\dagger}) = \\frac{u_i^{\\top}y}{\\sigma_i} - \\alpha_i = \\frac{\\sigma_i \\alpha_i + u_i^{\\top}\\eta}{\\sigma_i} - \\alpha_i = \\frac{u_i^{\\top}\\eta}{\\sigma_i}\n$$\nFor $i  k$:\n$$\nv_i^{\\top}(x_k - x^{\\dagger}) = 0 - \\alpha_i = -\\alpha_i\n$$\nSubstituting these back into the expression for the squared norm:\n$$\n\\|x_k - x^{\\dagger}\\|^2 = \\sum_{i=1}^{k} \\left(\\frac{u_i^{\\top}\\eta}{\\sigma_i}\\right)^2 + \\sum_{i=k+1}^{n} (-\\alpha_i)^2 = \\sum_{i=1}^{k} \\frac{|u_i^{\\top}\\eta|^2}{\\sigma_i^2} + \\sum_{i=k+1}^{n} \\alpha_i^2\n$$\nThe MSE is the expectation of this quantity over the noise distribution. As the expectation operator is linear and the terms $\\alpha_i, \\sigma_i$ are deterministic:\n$$\n\\mathbb{E}\\|x_k - x^{\\dagger}\\|^2 = \\sum_{i=1}^{k} \\frac{\\mathbb{E}|u_i^{\\top}\\eta|^2}{\\sigma_i^2} + \\sum_{i=k+1}^{n} \\alpha_i^2\n$$\nUsing the given noise model, $\\mathbb{E}|u_i^{\\top}\\eta|^2 = \\phi_i$, the MSE as a function of $k$ is:\n$$\n\\text{MSE}(k) = \\sum_{i=1}^{k} \\frac{\\phi_i}{\\sigma_i^2} + \\sum_{i=k+1}^{n} \\alpha_i^2\n$$\nThe first term is the variance due to noise propagation, and the second is the squared bias due to truncation. The optimization problem is to find $k^{\\star} = \\arg\\min_{k \\in \\{0, 1, \\dots, n\\}} \\text{MSE}(k)$.\n\nNow we solve the concrete case with $n=8$, $\\sigma_i = 16 \\cdot 2^{-(i-1)}$, $\\phi_i=1$, and $\\alpha_i = \\frac{1}{i}$. Let $f(k) = \\text{MSE}(k)$. We want to find $k^{\\star} = \\arg\\min_{k \\in \\{0, \\dots, 8\\}} f(k)$.\nTo find the minimum, we analyze the increment $f(k) - f(k-1)$ for $k \\in \\{1, \\dots, 8\\}$:\n$$\nf(k) - f(k-1) = \\left(\\sum_{i=1}^{k} \\frac{1}{\\sigma_i^2} + \\sum_{i=k+1}^{8} \\alpha_i^2\\right) - \\left(\\sum_{i=1}^{k-1} \\frac{1}{\\sigma_i^2} + \\sum_{i=k}^{8} \\alpha_i^2\\right)\n$$\n$$\nf(k) - f(k-1) = \\frac{1}{\\sigma_k^2} - \\alpha_k^2\n$$\nThe MSE decreases from $k-1$ to $k$ if this increment is negative, i.e., $\\frac{1}{\\sigma_k^2}  \\alpha_k^2$. The minimum will be attained at the last value of $k$ for which the function was decreasing. We seek the transition point where the increment becomes positive. This occurs when $\\frac{1}{\\sigma_k^2}  \\alpha_k^2$. The optimal truncation level $k^{\\star}$ will be the index just before this transition.\n\nLet's test the condition $\\frac{1}{\\sigma_i^2}  \\alpha_i^2$, which is equivalent to $\\sigma_i^2  \\frac{1}{\\alpha_i^2} = i^2$.\nThe singular values are $\\sigma_i = 16 \\cdot 2^{-(i-1)} = 2^{4} \\cdot 2^{-i+1} = 2^{5-i}$.\nThus, $\\sigma_i^2 = (2^{5-i})^2 = 2^{10-2i}$.\nThe condition to check for each $i \\in \\{1, \\dots, 8\\}$ is $2^{10-2i}  i^2$.\n- For $i=1$: $2^{10-2} = 2^8 = 256$. $1^2 = 1$. $256  1$ is true. $f(1)  f(0)$.\n- For $i=2$: $2^{10-4} = 2^6 = 64$. $2^2 = 4$. $64  4$ is true. $f(2)  f(1)$.\n- For $i=3$: $2^{10-6} = 2^4 = 16$. $3^2 = 9$. $16  9$ is true. $f(3)  f(2)$.\n- For $i=4$: $2^{10-8} = 2^2 = 4$. $4^2 = 16$. $4  16$ is false. $f(4)  f(3)$.\n\nThe increment $f(k) - f(k-1)$ is negative for $k=1, 2, 3$ and becomes positive for $k=4$. This implies that the sequence of MSE values decreases up to $k=3$ and then starts to increase:\n$f(0)  f(1)  f(2)  f(3)  f(4)$.\nThe function $f(k)$ continues to increase for $k4$ because the singular values $\\sigma_k$ decrease rapidly, making the variance term $1/\\sigma_k^2$ grow very large, while the bias term $\\alpha_k^2 = 1/k^2$ decreases more slowly.\nThus, the minimum of the Expected Mean Squared Error is achieved at $k^{\\star}=3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "After developing a regularized solution method like TSVD, it is crucial to assess its performance realistically. A common pitfall in numerical studies is the \"inverse crime,\" where the same mathematical model is used both to generate synthetic data and to invert it, leading to overly optimistic results. This hands-on exercise guides you through constructing a numerical experiment to demonstrate this phenomenon and shows how to use cross-validation—testing your solution against data generated from a different, perturbed operator—to obtain a more honest and robust evaluation of your inversion method's skill .",
            "id": "3401148",
            "problem": "Consider a linear inverse problem in which an unknown vector $x \\in \\mathbb{R}^{n}$ is inferred from linear observations $b \\in \\mathbb{R}^{m}$ governed by a forward operator $A \\in \\mathbb{R}^{m \\times n}$. Assume $m \\ge n$. The data are generated as $b = A x_{\\mathrm{true}} + \\eta$, where $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is fixed but unknown and $\\eta \\in \\mathbb{R}^{m}$ represents additive noise. Let the singular value decomposition (SVD) of a matrix $A \\in \\mathbb{R}^{m \\times n}$ be denoted by $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{n} \\ge 0$. The right singular vectors are the columns of $V$, and the left singular vectors are the columns of $U$. The truncated SVD estimator with truncation parameter $k \\in \\{1,\\dots,n\\}$ is defined by projecting onto the span of the first $k$ right singular vectors of the training operator and inverting the corresponding nonzero singular values.\n\nYour task is to illustrate the phenomenon commonly referred to as “inverse crime” in inverse problems: constructing synthetic data with a forward operator and then inverting with the same operator and basis, which can yield overly optimistic reconstructions. You must also implement practical diagnostics that use cross-validation with a distinct test operator to detect over-optimism.\n\nBase your reasoning on the following fundamental definitions and facts:\n- The singular value decomposition (SVD) exists for any real matrix $A$ and provides orthonormal bases of left and right singular vectors.\n- The truncated SVD solution $x_{k}$ is obtained by restricting inversion to the $k$ largest singular values and corresponding singular vectors of the training operator.\n- Orthogonal transformations preserve Euclidean norms.\n- The Euclidean norm is defined by $\\|z\\|_{2} = \\sqrt{z^{\\top} z}$, and the Frobenius norm of a matrix $M$ is $\\|M\\|_{F} = \\sqrt{\\sum_{i,j} M_{ij}^{2}}$.\n\nConstruct training and test operators by assembling their SVD factors directly to control their spectral content and to induce varying degrees of mismatch between training and test. Use the following deterministic protocol for all cases:\n- Dimensions: $m = 40$, $n = 30$.\n- Singular value profile: for a decay exponent $p  0$, set $\\sigma_{i} = (i)^{-p}$ for $i = 1,\\dots,n$.\n- Construction of an orthogonal matrix of size $d \\times d$: draw a dense matrix with independent standard normal entries and compute its $Q$ factor from a reduced Householder-QR factorization; adjust column signs so the diagonal of $R$ is nonnegative. Use this to construct $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$.\n- Assemble $A$ by $A = U_{:,1:n} \\operatorname{diag}(\\sigma) V_{:,1:n}^{\\top}$.\n- For each case, define a training operator $A_{\\mathrm{tr}}$ and a test operator $A_{\\mathrm{te}}$ by specifying their SVD factors. When a “small rotation” is needed, form $U_{\\mathrm{te}}$ from the $Q$ factor of $U_{\\mathrm{tr}} + \\varepsilon \\tilde{U}$ where $\\tilde{U}$ is an independently drawn orthogonal matrix, and similarly for $V_{\\mathrm{te}}$. When singular values are perturbed, set $\\sigma^{(\\mathrm{te})}_{i} = \\sigma_{i}(1+\\delta_{i})$ with $\\delta_{i}$ independent and uniformly distributed in a symmetric interval with width proportional to $\\varepsilon$.\n- Construct $x_{\\mathrm{true}}$ by drawing $w \\in \\mathbb{R}^{n}$ with independent standard normal entries and setting its components to $w_{i}/i$ for $i = 1,\\dots,n$, followed by normalization to unit Euclidean norm.\n- For a chosen relative noise level $\\alpha  0$, set $b_{\\mathrm{tr}} = A_{\\mathrm{tr}} x_{\\mathrm{true}} + \\eta_{\\mathrm{tr}}$ and $b_{\\mathrm{te}} = A_{\\mathrm{te}} x_{\\mathrm{true}} + \\eta_{\\mathrm{te}}$, where $\\|\\eta_{\\mathrm{tr}}\\|_{2} = \\alpha \\|A_{\\mathrm{tr}} x_{\\mathrm{true}}\\|_{2}$ and $\\|\\eta_{\\mathrm{te}}\\|_{2} = \\alpha \\|A_{\\mathrm{te}} x_{\\mathrm{true}}\\|_{2}$, with directions sampled uniformly at random on the respective spheres.\n\nGiven $A_{\\mathrm{tr}}$ and $b_{\\mathrm{tr}}$, compute the truncated SVD estimate $x_{k}$ using only the top $k$ singular triplets of $A_{\\mathrm{tr}}$. Then evaluate the following diagnostics:\n1. Cross-validation gap ratio $\\rho = \\|A_{\\mathrm{te}} x_{k} - b_{\\mathrm{te}}\\|_{2} \\big/ \\|A_{\\mathrm{tr}} x_{k} - b_{\\mathrm{tr}}\\|_{2}$.\n2. Low-observability energy fraction $f_{\\mathrm{low}}$, defined by expressing $x_{k}$ in the right singular basis of $A_{\\mathrm{te}}$ and reporting the fraction of its squared coefficients that lie in the subspace associated with singular values less than a fixed fraction $\\tau$ of the largest test singular value, i.e., with threshold $\\tau = 10^{-2}$:\n$$\nf_{\\mathrm{low}} = \\frac{\\sum_{i:\\ \\sigma^{(\\mathrm{te})}_{i}  \\tau \\sigma^{(\\mathrm{te})}_{1}} c_{i}^{2}}{\\sum_{i=1}^{n} c_{i}^{2}},\n$$\nwhere $x_{k} = \\sum_{i=1}^{n} c_{i} v^{(\\mathrm{te})}_{i}$ and $v^{(\\mathrm{te})}_{i}$ are the right singular vectors of $A_{\\mathrm{te}}$.\n3. Relative operator discrepancy $d_{A} = \\|A_{\\mathrm{te}} - A_{\\mathrm{tr}}\\|_{F} \\big/ \\|A_{\\mathrm{tr}}\\|_{F}$.\n\nImplement the following four test cases. In all cases set $p = 1$ and $\\tau = 10^{-2}$. For determinism, use a single base integer seed $S$ per case, and derive subsources by adding fixed offsets as specified. Specifically, for an item labeled with seed “$S + c$”, initialize a fresh independent random number generator with integer seed $S + c$.\n\n- Case 1 (inverse crime): $S = 31415$, $\\alpha = 0.02$, $k = 10$. Construct $A_{\\mathrm{tr}}$ from orthogonal factors using seeds $S+1$ and $S+2$ for $U_{\\mathrm{tr}}$ and $V_{\\mathrm{tr}}$. Set $A_{\\mathrm{te}} = A_{\\mathrm{tr}}$. Draw $x_{\\mathrm{true}}$ with seed $S+40$. Draw $\\eta_{\\mathrm{tr}}$ and $\\eta_{\\mathrm{te}}$ with seeds $S+50$ and $S+60$.\n- Case 2 (mild mismatch): $S = 27182$, $\\alpha = 0.02$, $k = 10$. Construct $A_{\\mathrm{tr}}$ as in Case 1 with seeds $S+1$ and $S+2$. Construct small rotations with $\\varepsilon = 0.2$ using seeds $S+10$ and $S+20$ to build $\\tilde{U}$ and $\\tilde{V}$; reorthonormalize to get $U_{\\mathrm{te}}$ and $V_{\\mathrm{te}}$. Perturb singular values with $\\delta_{i}$ drawn uniformly from $[-0.02, 0.02]$ using seed $S+30$. Assemble $A_{\\mathrm{te}}$ using $U_{\\mathrm{te}}$, $V_{\\mathrm{te}}$, and the perturbed singular values. Use seeds $S+40$, $S+50$, $S+60$ for $x_{\\mathrm{true}}$, $\\eta_{\\mathrm{tr}}$, $\\eta_{\\mathrm{te}}$.\n- Case 3 (severe mismatch): $S = 14142$, $\\alpha = 0.02$, $k = 10$. Construct $A_{\\mathrm{tr}}$ with seeds $S+1$ and $S+2$. Construct $A_{\\mathrm{te}}$ independently with seeds $S+1001$ and $S+1002$ and identical singular value profile (no perturbation). Use seeds $S+40$, $S+50$, $S+60$ for $x_{\\mathrm{true}}$, $\\eta_{\\mathrm{tr}}$, $\\eta_{\\mathrm{te}}$.\n- Case 4 (mismatch with aggressive truncation): $S = 17321$, $\\alpha = 0.02$, $k = 25$. Construct $A_{\\mathrm{tr}}$ with seeds $S+1$ and $S+2$. Construct small rotations and perturbed singular values as in Case 2 with the same $\\varepsilon = 0.2$ and seeds $S+10$, $S+20$, $S+30$. Use seeds $S+40$, $S+50$, $S+60$ for $x_{\\mathrm{true}}$, $\\eta_{\\mathrm{tr}}$, $\\eta_{\\mathrm{te}}$.\n\nYour program must:\n- Construct all matrices and vectors as specified.\n- Compute $x_{k}$ from the SVD of $A_{\\mathrm{tr}}$ with the prescribed truncation $k$.\n- Compute and report, for each case, the three real-valued diagnostics $(\\rho, f_{\\mathrm{low}}, d_{A})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[\\rho_{1}, f_{\\mathrm{low},1}, d_{A,1}, \\rho_{2}, f_{\\mathrm{low},2}, d_{A,2}, \\rho_{3}, f_{\\mathrm{low},3}, d_{A,3}, \\rho_{4}, f_{\\mathrm{low},4}, d_{A,4}]$,\nwith each floating-point number formatted to six decimal places.",
            "solution": "A critical validation of the problem statement confirms its validity. The problem is well-posed, scientifically grounded within the established framework of linear inverse problems, and provides a complete, deterministic protocol for all computations. All terms are formally defined, and the setup is free of contradictions or ambiguities. Specifically, the problem provides explicit dimensions ($m=40, n=30$), a deterministic method for constructing all operators ($A_{\\mathrm{tr}}, A_{\\mathrm{te}}$) and vectors ($x_{\\mathrm{true}}, \\eta$), precise formulas for all required diagnostics ($\\rho, f_{\\mathrm{low}}, d_{A}$), and unambiguous parameters and random seeds for four distinct test cases. The task is a standard numerical experiment designed to illustrate a key concept in data assimilation and inverse theory. We may, therefore, proceed with the solution.\n\nThe problem requires an investigation into the \"inverse crime\" phenomenon. This occurs when the same forward operator, $A$, is used both to generate synthetic observation data ($b = A x_{\\mathrm{true}} + \\eta$) and to perform the inversion to estimate $x$ from $b$. This procedure implicitly assumes a perfect model, as the model error $A_{\\mathrm{te}} - A_{\\mathrm{tr}}$ is zero. The resulting reconstruction error can be unrealistically low, fostering a false sense of confidence in the inversion method. A more robust evaluation methodology involves cross-validation, where the operator used for inversion, $A_{\\mathrm{tr}}$, is distinct from the operator that describes the test data, $A_{\\mathrm{te}}$.\n\nThe forward model is a linear system $b = Ax + \\eta$, where we seek to determine $x \\in \\mathbb{R}^{n}$ from observations $b \\in \\mathbb{R}^{m}$. The operator $A$ is ill-conditioned, with singular values $\\sigma_i$ that decay towards zero. This makes the naive solution $x = (A^{\\top}A)^{-1}A^{\\top}b$ highly susceptible to noise amplification. The truncated singular value decomposition (TSVD) is a regularization method that addresses this. Given the SVD of the training operator, $A_{\\mathrm{tr}} = U_{\\mathrm{tr}} \\Sigma_{\\mathrm{tr}} V_{\\mathrm{tr}}^{\\top}$, the TSVD estimate $x_k$ is formed by retaining only the first $k$ components corresponding to the largest singular values:\n$$\nx_k = \\sum_{i=1}^{k} \\frac{u_{\\mathrm{tr},i}^{\\top} b_{\\mathrm{tr}}}{\\sigma_{\\mathrm{tr},i}} v_{\\mathrm{tr},i}\n$$\nwhere $u_{\\mathrm{tr},i}$ and $v_{\\mathrm{tr},i}$ are the $i$-th left and right singular vectors of $A_{\\mathrm{tr}}$, respectively, and $k$ is the truncation parameter. This effectively filters out the contributions from highly noise-sensitive modes associated with small singular values.\n\nTo evaluate $x_k$, we compute three diagnostics:\n\n1.  **Cross-validation gap ratio $\\rho$**: This is the ratio of the test residual norm to the training residual norm, $\\rho = \\|A_{\\mathrm{te}} x_{k} - b_{\\mathrm{te}}\\|_{2} / \\|A_{\\mathrm{tr}} x_{k} - b_{\\mathrm{tr}}\\|_{2}$. In an \"inverse crime\" scenario where $A_{\\mathrm{te}} = A_{\\mathrm{tr}}$, we expect $\\rho \\approx 1$, as the residuals are driven by noise of similar magnitude. When $A_{\\mathrm{te}} \\neq A_{\\mathrm{tr}}$, the model mismatch introduces an additional error term in the test residual, typically causing $\\rho  1$. A large $\\rho$ indicates that the solution $x_k$ is over-fitted to the specific properties of $A_{\\mathrm{tr}}$ and does not generalize well.\n\n2.  **Relative operator discrepancy $d_{A}$**: This is the Frobenius norm of the difference between the test and training operators, normalized by the norm of the training operator, $d_{A} = \\|A_{\\mathrm{te}} - A_{\\mathrm{tr}}\\|_{F} / \\|A_{\\mathrm{tr}}\\|_{F}$. It provides a direct quantification of the model mismatch.\n\n3.  **Low-observability energy fraction $f_{\\mathrm{low}}$**: This diagnostic quantifies how much of the solution's energy lies in subspaces that are poorly observed by the *test* operator $A_{\\mathrm{te}}$. The solution $x_k$ is expanded in the basis of right singular vectors of $A_{\\mathrm{te}}$, $x_{k} = \\sum_{i=1}^{n} c_{i} v^{(\\mathrm{te})}_{i}$. The fraction of energy (squared coefficients) corresponding to singular values $\\sigma_i^{(\\mathrm{te})}$ below a threshold $\\tau \\sigma_1^{(\\mathrm{te})}$ is calculated. A high value of $f_{\\mathrm{low}}$ would suggest that the solution $x_k$, derived from $A_{\\mathrm{tr}}$, relies heavily on components that are nearly in the null-space of $A_{\\mathrm{te}}$, indicating a significant structural mismatch between the operators. For the given parameters ($p=1$, $\\tau=10^{-2}$), the singular values are $\\sigma_i \\approx 1/i$. The smallest singular value is $\\sigma_{30} \\approx 1/30 \\approx 0.0333$. The threshold is $\\tau \\sigma_1 \\approx 10^{-2} \\times 1 = 0.01$. Since all singular values of the constructed operators are greater than this threshold, the low-observability subspace is empty, and we correctly anticipate $f_{\\mathrm{low}} = 0$ for all test cases. This is a valid result of the specified parameterization.\n\nThe computational procedure involves four cases, each defined by a set of seeds and parameters.\n- **Case 1 (Inverse Crime)**: Here, $A_{\\mathrm{te}} = A_{\\mathrm{tr}}$, representing the idealized but unrealistic scenario. We expect $\\rho \\approx 1$ and $d_A=0$.\n- **Case 2 (Mild Mismatch)**: $A_{\\mathrm{te}}$ is a small perturbation of $A_{\\mathrm{tr}}$. This simulates a realistic scenario where the model is a good but imperfect approximation of reality. We expect $\\rho  1$ and a small $d_A  0$.\n- **Case 3 (Severe Mismatch)**: $A_{\\mathrm{te}}$ is constructed independently of $A_{\\mathrm{tr}}$, but with the same spectral profile. This represents a poor model. We expect a large gap ratio $\\rho \\gg 1$ and a large $d_A$.\n- **Case 4 (Mismatch with Aggressive Truncation)**: This is similar to Case 2 but with a much larger truncation parameter $k=25$. By including more ill-conditioned modes in the inversion, the solution becomes more sensitive to both noise and model error. We expect this to exacerbate the cross-validation gap, resulting in a larger $\\rho$ than in Case 2.\n\nThe implementation will deterministically construct all matrices and vectors using the specified seeds, compute the TSVD solution $x_k$ for each case, and then calculate the three diagnostics $(\\rho, d_A, f_{\\mathrm{low}})$. All random orthogonal matrices are constructed via QR decomposition of a matrix with standard normal entries, with column signs adjusted to ensure a unique Q factor for reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef construct_orthogonal_matrix(d, seed):\n    \"\"\"\n    Constructs a d x d orthogonal matrix deterministically using QR factorization.\n    The QR decomposition is made unique by enforcing a non-negative diagonal on R.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    G = rng.standard_normal(size=(d, d))\n    Q, R = linalg.qr(G, mode='full')\n    s = np.diag(np.sign(np.diag(R)))\n    Q = Q @ s\n    return Q\n\ndef construct_true_solution(n, seed):\n    \"\"\"\n    Constructs the true solution vector x_true of size n.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    w = rng.standard_normal(n)\n    x_unscaled = w / np.arange(1, n + 1)\n    x_true = x_unscaled / np.linalg.norm(x_unscaled)\n    return x_true\n\ndef construct_noise_vector(A, x_true, alpha, seed):\n    \"\"\"\n    Constructs a noise vector eta with a specified relative norm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    m = A.shape[0]\n    signal = A @ x_true\n    signal_norm = np.linalg.norm(signal)\n    target_norm = alpha * signal_norm\n    \n    z = rng.standard_normal(m)\n    eta = target_norm * z / np.linalg.norm(z)\n    return eta\n\ndef build_A_and_factors(m, n, p, U_seed, V_seed):\n    \"\"\"\n    Assembles a matrix A from its SVD components constructed deterministically.\n    \"\"\"\n    U_full = construct_orthogonal_matrix(m, U_seed)\n    V_full = construct_orthogonal_matrix(n, V_seed)\n    sigma = np.power(np.arange(1, n + 1, dtype=float), -p)\n    A = U_full[:, :n] @ np.diag(sigma) @ V_full.T\n    return A, U_full, V_full, sigma\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and compute diagnostics.\n    \"\"\"\n    test_cases = [\n        {'S': 31415, 'alpha': 0.02, 'k': 10, 'type': 'crime'},\n        {'S': 27182, 'alpha': 0.02, 'k': 10, 'type': 'mild'},\n        {'S': 14142, 'alpha': 0.02, 'k': 10, 'type': 'severe'},\n        {'S': 17321, 'alpha': 0.02, 'k': 25, 'type': 'aggressive'}\n    ]\n    \n    m, n = 40, 30\n    p = 1.0\n    tau = 1e-2\n    \n    results = []\n\n    for case in test_cases:\n        S, alpha, k = case['S'], case['alpha'], case['k']\n        \n        # --- Operator and Data Construction ---\n        A_tr, U_tr_full, V_tr_full, sigma_base = build_A_and_factors(m, n, p, S + 1, S + 2)\n\n        if case['type'] == 'crime':\n            A_te = A_tr\n        elif case['type'] in ['mild', 'aggressive']:\n            eps_rot = 0.2\n            eps_sv = 0.02\n            \n            U_tilde = construct_orthogonal_matrix(m, S + 10)\n            U_te_unorth = U_tr_full + eps_rot * U_tilde\n            U_te_full, R_u = linalg.qr(U_te_unorth, mode='full')\n            U_te_full = U_te_full @ np.diag(np.sign(np.diag(R_u)))\n\n            V_tilde = construct_orthogonal_matrix(n, S + 20)\n            V_te_unorth = V_tr_full + eps_rot * V_tilde\n            V_te_full, R_v = linalg.qr(V_te_unorth, mode='full')\n            V_te_full = V_te_full @ np.diag(np.sign(np.diag(R_v)))\n            \n            rng_sv = np.random.default_rng(S + 30)\n            delta = rng_sv.uniform(-eps_sv, eps_sv, n)\n            sigma_te = sigma_base * (1 + delta)\n            \n            A_te = U_te_full[:, :n] @ np.diag(sigma_te) @ V_te_full.T\n        elif case['type'] == 'severe':\n            A_te, _, _, _ = build_A_and_factors(m, n, p, S + 1001, S + 1002)\n\n        x_true = construct_true_solution(n, S + 40)\n        \n        eta_tr = construct_noise_vector(A_tr, x_true, alpha, S + 50)\n        eta_te = construct_noise_vector(A_te, x_true, alpha, S + 60)\n        \n        b_tr = A_tr @ x_true + eta_tr\n        b_te = A_te @ x_true + eta_te\n        \n        # --- SVD Inversion ---\n        U_tr_svd, s_tr_svd, Vt_tr_svd = linalg.svd(A_tr, full_matrices=False)\n        \n        U_k = U_tr_svd[:, :k]\n        s_k_inv = 1.0 / s_tr_svd[:k]\n        V_k = Vt_tr_svd[:k, :].T\n        \n        x_k = V_k @ (np.diag(s_k_inv) @ (U_k.T @ b_tr))\n        \n        # --- Diagnostics Calculation ---\n        \n        # 1. Cross-validation gap ratio (rho)\n        resid_te_norm = np.linalg.norm(A_te @ x_k - b_te)\n        resid_tr_norm = np.linalg.norm(A_tr @ x_k - b_tr)\n        rho = resid_te_norm / resid_tr_norm if resid_tr_norm > 0 else np.inf\n        \n        # 2. Low-observability energy fraction (f_low)\n        _, s_te_svd, Vt_te_svd = linalg.svd(A_te, full_matrices=False)\n        V_te_svd = Vt_te_svd.T\n        \n        c = V_te_svd.T @ x_k \n        \n        threshold = tau * s_te_svd[0]\n        low_obs_indices = np.where(s_te_svd  threshold)[0]\n        \n        num = np.sum(c[low_obs_indices]**2)\n        den = np.sum(c**2)\n        f_low = num / den if den > 0 else 0.0\n\n        # 3. Relative operator discrepancy (d_A)\n        d_A = np.linalg.norm(A_te - A_tr, 'fro') / np.linalg.norm(A_tr, 'fro')\n        \n        results.extend([rho, f_low, d_A])\n\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many real-world scenarios, observations alone are insufficient to uniquely determine all parameters of a system, a problem often revealed by closely-spaced singular values in the forward operator $A$. Data assimilation techniques address this by incorporating prior information through a background covariance matrix $B$. This practice explores how a well-chosen prior can break the ambiguities inherent in the operator, improving parameter identifiability. By analyzing the singular values of the \"whitened\" operator $A B^{1/2}$, you will quantify how different prior assumptions can effectively separate the nearly-degenerate modes that the observations alone confound .",
            "id": "3401171",
            "problem": "Consider a linear inverse problem in the context of linear Gaussian data assimilation, where the forward (observation) operator is a matrix $A \\in \\mathbb{R}^{m \\times n}$, the background (prior) covariance is a symmetric positive definite matrix $B \\in \\mathbb{R}^{n \\times n}$, and the observation error covariance is a symmetric positive definite matrix $R \\in \\mathbb{R}^{m \\times m}$. Focus on the structure induced by the Singular Value Decomposition (SVD) on the right singular vectors and singular values, and how near-degeneracy in singular values leads to parameter non-uniqueness along the corresponding right singular subspace. Use the following foundational facts as the starting point: (i) the SVD of a real matrix $M$ exists and can be written as $M = U \\Sigma V^{\\top}$ with $U$ and $V$ orthogonal and $\\Sigma$ diagonal with nonnegative entries; (ii) for symmetric positive definite matrices $C$, a symmetric square root $C^{1/2}$ exists so that $C^{1/2} C^{1/2} = C$; (iii) a symmetric matrix has real eigenvalues and orthonormal eigenvectors; (iv) linear Gaussian data assimilation can be formulated using a quadratic cost and the associated background covariance $B$ defines a metric in parameter space.\n\nConstruct a parameter identifiability study that exhibits near-degeneracy in singular values and shows how carefully chosen background covariances $B$ can separate the ambiguous directions. Work in a fully specified purely mathematical setting with no physical units. Let $m = n = 3$, let $R = I_3$ (the $3 \\times 3$ identity matrix), and define the forward operator\n$$\nA = \\begin{bmatrix}\n1  0.02  0 \\\\\n0.02  1  0 \\\\\n0  0  0.1\n\\end{bmatrix}.\n$$\nThe $2 \\times 2$ leading block has nearly equal eigenvalues $1 \\pm 0.02$, so the two largest singular values of $A$ are close, and the corresponding right singular vectors are nearly non-unique within that $2$-dimensional subspace, which confounds the first two parameters.\n\nDefine the whitened operator for a given $B$ as\n$$\nS(B) = R^{-1/2} \\, A \\, B^{1/2} = A \\, B^{1/2},\n$$\nsince $R = I_3$. For each $B$, let the singular values of $S(B)$ be ordered as $\\sigma_1(B) \\ge \\sigma_2(B) \\ge \\sigma_3(B) \\ge 0$. Quantify the identifiability separation within the near-degenerate subspace by the relative singular-value separation\n$$\n\\rho(B) = \\frac{\\sigma_1(B) - \\sigma_2(B)}{\\sigma_1(B)}.\n$$\nLarger $\\rho(B)$ indicates better separation of the top two right singular directions by the chosen prior $B$.\n\nImplement a program that, for the test suite below, computes $\\rho(B)$ for each case and reports the results on a single output line.\n\nTest suite for $B$:\n- Case $1$ (baseline isotropic): $B_1 = I_3$.\n- Case $2$ (anisotropic axis-aligned, emphasis on parameter $1$): $B_2 = \\mathrm{diag}(4, 1, 1)$.\n- Case $3$ (anisotropic axis-aligned, emphasis on parameter $2$): $B_3 = \\mathrm{diag}(1, 4, 1)$.\n- Case $4$ (anisotropic rotated to align a principal prior axis with the ambiguous subspace): Let $Q$ be the rotation by angle $\\theta = \\pi/4$ (in radians) in the $(x_1,x_2)$-plane,\n$$\nQ = \\begin{bmatrix}\n\\cos(\\theta)  -\\sin(\\theta)  0 \\\\\n\\sin(\\theta)  \\cos(\\theta)  0 \\\\\n0  0  1\n\\end{bmatrix},\n$$\nand define $B_4 = Q \\, \\mathrm{diag}(4,1,1) \\, Q^{\\top}$.\n- Case $5$ (boundary scalar multiple, no directional preference): $B_5 = 9 I_3$.\n\nRequirements:\n- Use the SVD definition to obtain $\\sigma_i(B)$ of $S(B)$, and from these compute $\\rho(B)$ for each case.\n- Ensure that each $B$ is treated via its symmetric positive definite square root $B^{1/2}$, obtained by an orthogonal diagonalization of $B$.\n- The final program must produce a single line of output containing the results as a comma-separated list of floating-point numbers rounded to six decimal places, enclosed in square brackets, and ordered as $[\\rho(B_1), \\rho(B_2), \\rho(B_3), \\rho(B_4), \\rho(B_5)]$.\n\nDesign for coverage:\n- Case $1$ is the happy path baseline with near-degenerate singular values in $A$.\n- Cases $2$ and $3$ probe anisotropic priors aligned with coordinate axes.\n- Case $4$ probes a rotated prior aligned with the nearly degenerate singular subspace, intended to further separate directions.\n- Case $5$ is a boundary condition showing that a scalar multiple $B = \\alpha I_3$ does not change relative separation beyond uniform scaling.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$), with each $\\rho_i$ rounded to six decimal places. No user input or external files are permitted.",
            "solution": "The problem requires an analysis of parameter identifiability in a linear inverse problem governed by the forward operator $A \\in \\mathbb{R}^{3 \\times 3}$, with prior information encoded in a background covariance matrix $B \\in \\mathbb{R}^{3 \\times 3}$, and identity observation error covariance $R=I_3$. The core of the analysis is to compute a metric, $\\rho(B)$, which quantifies the separation of the two largest singular values of a whitened operator $S(B) = A B^{1/2}$. This metric serves as a proxy for how well the prior information, embodied by $B$, resolves the near-degeneracy inherent in the forward operator $A$. The given operator is\n$$\nA = \\begin{bmatrix}\n1  0.02  0 \\\\\n0.02  1  0 \\\\\n0  0  0.1\n\\end{bmatrix}\n$$\nThe near-degeneracy arises from the top-left $2 \\times 2$ block, whose eigenvalues are $1 \\pm 0.02$, which are very close. This translates to two nearly equal singular values for $A$, confounding the first two parameters of the state vector.\n\nThe general procedure to compute the identifiability separation $\\rho(B)$ for a given symmetric positive definite (SPD) background covariance matrix $B$ is as follows:\n1.  Compute the symmetric square root $B^{1/2}$ of $B$. For any SPD matrix $B$, it can be diagonalized by an orthogonal matrix $P$ as $B = P D P^{\\top}$, where $D$ is a diagonal matrix of the positive eigenvalues of $B$. The symmetric square root is then given by $B^{1/2} = P D^{1/2} P^{\\top}$, where $D^{1/2}$ is the diagonal matrix of the square roots of the eigenvalues.\n2.  Construct the whitened operator $S(B) = A B^{1/2}$, since $R = I_3$ and thus $R^{-1/2} = I_3$.\n3.  Compute the singular values of $S(B)$, which are defined as the square roots of the eigenvalues of the matrix $S(B)^{\\top}S(B)$. Let these be ordered as $\\sigma_1(B) \\ge \\sigma_2(B) \\ge \\sigma_3(B)$.\n4.  Calculate the relative singular-value separation $\\rho(B) = \\frac{\\sigma_1(B) - \\sigma_2(B)}{\\sigma_1(B)}$.\n\nWe now apply this procedure to each of the five specified cases for $B$.\n\nCase 1: Baseline isotropic prior, $B_1 = I_3$.\nThe matrix $B_1$ is the $3 \\times 3$ identity matrix. Its square root is also the identity matrix, $B_1^{1/2} = I_3$.\nThe whitened operator is $S(B_1) = A I_3 = A$.\nSince $A$ is a symmetric matrix, its singular values are the absolute values of its eigenvalues. The eigenvalues of $A$ can be determined from its block-diagonal structure. The eigenvalues of the top-left $2 \\times 2$ block $\\begin{pmatrix} 1  0.02 \\\\ 0.02  1 \\end{pmatrix}$ are the roots of $(1-\\lambda)^2 - (0.02)^2 = 0$, which are $\\lambda = 1 \\pm 0.02$. The third eigenvalue is simply the remaining diagonal entry, $0.1$.\nThe eigenvalues are $\\lambda_1 = 1.02$, $\\lambda_2 = 0.98$, and $\\lambda_3 = 0.1$. Since they are all positive, the singular values are $\\sigma_1(B_1) = 1.02$, $\\sigma_2(B_1) = 0.98$, and $\\sigma_3(B_1) = 0.1$.\nThe separation is $\\rho(B_1) = \\frac{1.02 - 0.98}{1.02} = \\frac{0.04}{1.02} \\approx 0.039216$.\n\nCase 2: Anisotropic axis-aligned prior, $B_2 = \\mathrm{diag}(4, 1, 1)$.\nThis prior expresses more confidence in parameters $x_2$ and $x_3$ (smaller variance) relative to parameter $x_1$ (larger variance).\n$B_2$ is diagonal, so its square root is $B_2^{1/2} = \\mathrm{diag}(\\sqrt{4}, \\sqrt{1}, \\sqrt{1}) = \\mathrm{diag}(2, 1, 1)$.\nThe whitened operator is $S(B_2) = A B_2^{1/2} = \\begin{bmatrix} 1  0.02  0 \\\\ 0.02  1  0 \\\\ 0  0  0.1 \\end{bmatrix} \\begin{bmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix} = \\begin{bmatrix} 2  0.02  0 \\\\ 0.04  1  0 \\\\ 0  0  0.1 \\end{bmatrix}$.\nTo find the singular values, we analyze $S(B_2)^{\\top}S(B_2)$:\n$$ S(B_2)^{\\top}S(B_2) = \\begin{bmatrix} 2  0.04  0 \\\\ 0.02  1  0 \\\\ 0  0  0.1 \\end{bmatrix} \\begin{bmatrix} 2  0.02  0 \\\\ 0.04  1  0 \\\\ 0  0  0.1 \\end{bmatrix} = \\begin{bmatrix} 4.0016  0.08  0 \\\\ 0.08  1.0004  0 \\\\ 0  0  0.01 \\end{bmatrix} $$\nOne eigenvalue is $\\lambda_3' = 0.01$. The other two are eigenvalues of the top-left $2 \\times 2$ block. The characteristic equation is $\\lambda^2 - 5.002\\lambda + 3.99680064 = 0$. The roots are $\\lambda'_{1,2} \\approx (5.002 \\pm 3.005462)/2$.\nSo, $\\lambda'_1 \\approx 4.003731$ and $\\lambda'_2 \\approx 0.998269$.\nThe singular values are the square roots: $\\sigma_1(B_2) = \\sqrt{\\lambda'_1} \\approx 2.000933$, $\\sigma_2(B_2) = \\sqrt{\\lambda'_2} \\approx 0.999134$, and $\\sigma_3(B_2) = \\sqrt{0.01} = 0.1$.\nThe separation is $\\rho(B_2) = \\frac{2.000933 - 0.999134}{2.000933} \\approx 0.500667$.\n\nCase 3: Anisotropic axis-aligned prior, $B_3 = \\mathrm{diag}(1, 4, 1)$.\nThis case is symmetric to Case 2, emphasizing parameter $x_2$ instead of $x_1$.\n$B_3^{1/2} = \\mathrm{diag}(1, 2, 1)$.\n$S(B_3) = A B_3^{1/2} = \\begin{bmatrix} 1  0.02  0 \\\\ 0.02  1  0 \\\\ 0  0  0.1 \\end{bmatrix} \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0.04  0 \\\\ 0.02  2  0 \\\\ 0  0  0.1 \\end{bmatrix}$.\n$S(B_3)^{\\top}S(B_3) = \\begin{bmatrix} 1  0.02  0 \\\\ 0.04  2  0 \\\\ 0  0  0.1 \\end{bmatrix} \\begin{bmatrix} 1  0.04  0 \\\\ 0.02  2  0 \\\\ 0  0  0.1 \\end{bmatrix} = \\begin{bmatrix} 1.0004  0.08  0 \\\\ 0.08  4.0016  0 \\\\ 0  0  0.01 \\end{bmatrix}$.\nThe top-left $2 \\times 2$ block of this matrix has the same trace and determinant as that of $S(B_2)^{\\top}S(B_2)$, therefore it has the same eigenvalues. Thus, the singular values of $S(B_3)$ are identical to those of $S(B_2)$.\n$\\rho(B_3) = \\rho(B_2) \\approx 0.500667$.\n\nCase 4: Rotated anisotropic prior, $B_4 = Q \\, \\mathrm{diag}(4,1,1) \\, Q^{\\top}$.\nThe rotation matrix $Q$ for $\\theta=\\pi/4$ is $Q = \\begin{bmatrix} 1/\\sqrt{2}  -1/\\sqrt{2}  0 \\\\ 1/\\sqrt{2}  1/\\sqrt{2}  0 \\\\ 0  0  1 \\end{bmatrix}$.\nThis prior aligns the direction of largest prior variance with the $(1,1,0)/\\sqrt{2}$ direction. This direction is an eigenvector of the problematic block of $A$.\n$B_4$ is already given in its diagonalized form, so $B_4^{1/2} = Q \\, \\mathrm{diag}(\\sqrt{4},\\sqrt{1},\\sqrt{1}) \\, Q^{\\top} = Q \\, \\mathrm{diag}(2,1,1) \\, Q^{\\top}$.\nThe eigensystem of $A$ is $A = Q \\, \\mathrm{diag}(1.02, 0.98, 0.1) \\, Q^{\\top}$.\nThe whitened operator is $S(B_4) = A B_4^{1/2} = (Q \\, \\mathrm{diag}(1.02, 0.98, 0.1) \\, Q^{\\top})(Q \\, \\mathrm{diag}(2,1,1) \\, Q^{\\top})$.\nSince $Q^{\\top}Q=I$, this simplifies to $S(B_4) = Q \\, (\\mathrm{diag}(1.02, 0.98, 0.1) \\mathrm{diag}(2,1,1)) \\, Q^{\\top}$.\n$S(B_4) = Q \\, \\mathrm{diag}(2.04, 0.98, 0.1) \\, Q^{\\top}$.\nThis is the eigendecomposition of the symmetric matrix $S(B_4)$. Its eigenvalues are therefore $2.04$, $0.98$, and $0.1$. Since they are positive, these are also the singular values.\n$\\sigma_1(B_4)=2.04$, $\\sigma_2(B_4)=0.98$, $\\sigma_3(B_4)=0.1$.\nThe separation is $\\rho(B_4) = \\frac{2.04 - 0.98}{2.04} = \\frac{1.06}{2.04} \\approx 0.519608$. This is the best separation achieved, demonstrating the power of aligning the prior covariance with the ambiguous subspace of the forward operator.\n\nCase 5: Isotropic scaled prior, $B_5 = 9 I_3$.\nHere, $B_5^{1/2} = \\sqrt{9} I_3^{1/2} = 3 I_3$.\nThe whitened operator is $S(B_5) = A (3 I_3) = 3A$.\nThe singular values of a matrix scaled by a constant $c$ are scaled by $|c|$. Thus, the singular values of $S(B_5)$ are $3$ times the singular values of $A$.\n$\\sigma_1(B_5)=3 \\times 1.02 = 3.06$, $\\sigma_2(B_5) = 3 \\times 0.98 = 2.94$, $\\sigma_3(B_5) = 3 \\times 0.1 = 0.3$.\nThe separation metric is $\\rho(B_5) = \\frac{3 \\sigma_1(A) - 3 \\sigma_2(A)}{3 \\sigma_1(A)} = \\frac{\\sigma_1(A) - \\sigma_2(A)}{\\sigma_1(A)} = \\rho(B_1)$.\n$\\rho(B_5) = \\rho(B_1) \\approx 0.039216$. This confirms that simple isotropic scaling of the prior variance does not improve the relative separation of near-degenerate singular values.\n\nIn summary, the near-degeneracy of the baseline case ($\\rho \\approx 0.04$) is significantly improved by introducing anisotropic prior information (Cases 2 and 3, $\\rho \\approx 0.50$), and further improved by aligning this anisotropy with the singular vectors of the forward operator (Case 4, $\\rho \\approx 0.52$). Isotropic scaling has no effect on the relative separation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the parameter identifiability separation for five different\n    background covariance matrices in a linear inverse problem.\n    \"\"\"\n\n    # Define the forward operator A\n    A = np.array([\n        [1.0, 0.02, 0.0],\n        [0.02, 1.0, 0.0],\n        [0.0, 0.0, 0.1]\n    ])\n\n    # --- Define the test cases for the background covariance matrix B ---\n\n    # Case 1: B1 = Identity\n    B1 = np.identity(3)\n\n    # Case 2: B2 = diag(4, 1, 1)\n    B2 = np.diag([4.0, 1.0, 1.0])\n\n    # Case 3: B3 = diag(1, 4, 1)\n    B3 = np.diag([1.0, 4.0, 1.0])\n\n    # Case 4: Rotated anisotropic B\n    theta = np.pi / 4.0\n    c, s = np.cos(theta), np.sin(theta)\n    Q = np.array([\n        [c, -s, 0.0],\n        [s,  c, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    D4 = np.diag([4.0, 1.0, 1.0])\n    B4 = Q @ D4 @ Q.T\n\n    # Case 5: B5 = 9 * Identity\n    B5 = 9.0 * np.identity(3)\n\n    test_cases = [B1, B2, B3, B4, B5]\n\n    results = []\n\n    for B in test_cases:\n        # Step 1: Compute the symmetric square root of B, B_sqrt\n        # B is symmetric, so we use eigh for stable eigendecomposition.\n        eigvals, eigvecs = np.linalg.eigh(B)\n        \n        # Check for positive definiteness\n        if np.any(eigvals = 0):\n            # This case should not be reached with the given valid inputs\n            raise ValueError(\"Matrix B must be positive definite.\")\n\n        # Construct B_sqrt = P * D_sqrt * P.T\n        D_sqrt = np.diag(np.sqrt(eigvals))\n        B_sqrt = eigvecs @ D_sqrt @ eigvecs.T\n\n        # Step 2: Form the whitened operator S(B) = A * B_sqrt\n        # Since R = I, R^(-1/2) is also I.\n        S_B = A @ B_sqrt\n\n        # Step 3: Compute the singular values of S(B)\n        # np.linalg.svd returns singular values in descending order.\n        # We don't need the U and Vh matrices, so we use compute_uv=False.\n        singular_values = np.linalg.svd(S_B, compute_uv=False)\n        \n        sigma1 = singular_values[0]\n        sigma2 = singular_values[1]\n\n        # Step 4: Calculate the separation metric rho(B)\n        # Handle case where sigma1 could be zero to avoid division by zero\n        if sigma1 > 1e-12:\n            rho = (sigma1 - sigma2) / sigma1\n        else:\n            rho = 0.0\n        \n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of floats rounded to six decimal places,\n    # enclosed in square brackets.\n    output_str = \",\".join([f\"{r:.6f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}