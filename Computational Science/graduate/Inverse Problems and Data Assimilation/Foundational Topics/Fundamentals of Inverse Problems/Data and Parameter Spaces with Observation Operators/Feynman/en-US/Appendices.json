{
    "hands_on_practices": [
        {
            "introduction": "The stability of an inverse problem is a critical concept, determining how sensitive the estimated parameters are to noise in the observations. This practice explores this concept in the context of a linear observation operator, where the connection is particularly clear. By deriving the relationship between the stability constant and the singular values of the operator matrix, you will gain a fundamental understanding of how the operator's structure dictates the problem's conditioning .",
            "id": "3374182",
            "problem": "Consider a linear inverse problem in which the parameter space is $\\mathbb{R}^{n}$ and the data space is $\\mathbb{R}^{m}$, with a linear observation operator $\\mathcal{O} \\in \\mathbb{R}^{m \\times n}$. Observations are modeled as $y = \\mathcal{O} x$ for $x \\in \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{m}$. Assume $\\mathcal{O}$ has full column rank so that the map from data to parameters defined by the least-squares solution is unique and given by the Moore–Penrose pseudoinverse $\\mathcal{O}^{+}$. Using the Euclidean $2$-norm on both spaces, define stability of the inverse problem as the Lipschitz continuity of the mapping $y \\mapsto x^{\\star}(y)$, where $x^{\\star}(y) = \\mathcal{O}^{+} y$, i.e., there exists a constant $C > 0$ such that $\\|x^{\\star}(y_{1}) - x^{\\star}(y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}$ for all $y_{1}, y_{2} \\in \\mathbb{R}^{m}$.\n\nStarting only from fundamental definitions of the operator norm, the Singular Value Decomposition (SVD) (first appearance: Singular Value Decomposition (SVD)), and the Moore–Penrose pseudoinverse, derive the tightest possible bound constant $C$ in terms of intrinsic spectral properties of $\\mathcal{O}$. Then, for the specific observation operator\n$$\n\\mathcal{O} \\;=\\; \\begin{pmatrix}\n3 & 1 \\\\\n0 & 2 \\\\\n1 & 1\n\\end{pmatrix},\n$$\ncompute the exact value of the optimal stability constant $C$ for the map $y \\mapsto \\mathcal{O}^{+} y$. Your final answer must be a single closed-form expression. If a numerical approximation is used, round your answer to four significant figures; otherwise provide the exact analytical form. No units are required.",
            "solution": "The problem as stated is subject to validation.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n- Parameter space: $\\mathbb{R}^{n}$.\n- Data space: $\\mathbb{R}^{m}$.\n- Observation model: $y = \\mathcal{O} x$, with $\\mathcal{O} \\in \\mathbb{R}^{m \\times n}$ being a linear operator, $x \\in \\mathbb{R}^{n}$, and $y \\in \\mathbb{R}^{m}$.\n- Assumption: The operator $\\mathcal{O}$ has full column rank.\n- Solution map: The least-squares solution is given by $x^{\\star}(y) = \\mathcal{O}^{+} y$, where $\\mathcal{O}^{+}$ is the Moore–Penrose pseudoinverse.\n- Norms: The Euclidean $2$-norm, $\\|\\cdot\\|_{2}$, is used for both $\\mathbb{R}^{n}$ and $\\mathbb{R}^{m}$.\n- Definition of stability: The map $y \\mapsto x^{\\star}(y)$ is Lipschitz continuous, satisfying $\\|x^{\\star}(y_{1}) - x^{\\star}(y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}$ for a constant $C > 0$.\n- Objective 1: Derive the tightest possible constant $C$ from the definitions of the operator norm, SVD, and Moore–Penrose pseudoinverse, expressed in terms of the spectral properties of $\\mathcal{O}$.\n- Objective 2: Compute the exact value of this optimal stability constant $C$ for the specific operator $\\mathcal{O} = \\begin{pmatrix} 3 & 1 \\\\ 0 & 2 \\\\ 1 & 1 \\end{pmatrix}$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, employing standard, well-defined concepts from linear algebra and the theory of inverse problems (SVD, Moore-Penrose pseudoinverse, operator norms). It is well-posed, asking for the derivation of a specific mathematical constant, which is a unique and meaningful value. The language is objective and precise. The setup is complete and consistent; the given matrix $\\mathcal{O}$ is a $3 \\times 2$ matrix, and its columns, $\\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}$ and $\\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$, are linearly independent, confirming it has full column rank ($n=2$), as assumed. The problem is a standard, non-trivial exercise in applied linear algebra and is subject to direct mathematical verification.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Optimal Stability Constant**\n\nThe stability of the inverse problem is characterized by the Lipschitz continuity of the map $y \\mapsto x^{\\star}(y)$. The condition is given by\n$$\n\\|x^{\\star}(y_{1}) - x^{\\star}(y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}\n$$\nfor all $y_{1}, y_{2} \\in \\mathbb{R}^{m}$. Substituting the definition $x^{\\star}(y) = \\mathcal{O}^{+} y$ and using the linearity of the pseudoinverse operator $\\mathcal{O}^{+}$, we have\n$$\n\\|\\mathcal{O}^{+} y_{1} - \\mathcal{O}^{+} y_{2}\\|_{2} = \\|\\mathcal{O}^{+}(y_{1} - y_{2})\\|_{2} \\leq C \\|y_{1} - y_{2}\\|_{2}.\n$$\nLet $z = y_{1} - y_{2}$. Since $y_{1}$ and $y_{2}$ can be any vectors in $\\mathbb{R}^{m}$, $z$ can be any vector in $\\mathbb{R}^{m}$. The inequality becomes\n$$\n\\|\\mathcal{O}^{+} z\\|_{2} \\leq C \\|z\\|_{2}\n$$\nfor all $z \\in \\mathbb{R}^{m}$. The tightest possible constant $C$ that satisfies this inequality for all non-zero $z$ is, by definition, the operator norm of $\\mathcal{O}^{+}$ induced by the Euclidean $2$-norm:\n$$\nC = \\sup_{z \\neq 0} \\frac{\\|\\mathcal{O}^{+} z\\|_{2}}{\\|z\\|_{2}} = \\|\\mathcal{O}^{+}\\|_{2}.\n$$\nThe operator $2$-norm of a matrix is equal to its largest singular value. To find the singular values of $\\mathcal{O}^{+}$, we first consider the Singular Value Decomposition (SVD) of the operator $\\mathcal{O}$. For $\\mathcal{O} \\in \\mathbb{R}^{m \\times n}$, its SVD is given by\n$$\n\\mathcal{O} = U \\Sigma V^T,\n$$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the singular values of $\\mathcal{O}$. Let the rank of $\\mathcal{O}$ be $r$. The singular values $\\sigma_{i}$ are ordered such that $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{r} > 0$, with the remaining singular values being zero (if $m \\ge n$).\n\nThe Moore-Penrose pseudoinverse $\\mathcal{O}^{+}$ is defined from the SVD as\n$$\n\\mathcal{O}^{+} = V \\Sigma^{+} U^T,\n$$\nwhere $\\Sigma^{+} \\in \\mathbb{R}^{n \\times m}$ is the pseudoinverse of $\\Sigma$. If $\\Sigma$ has the form $\\begin{pmatrix} D & 0 \\\\ 0 & 0 \\end{pmatrix}$ with $D = \\text{diag}(\\sigma_{1}, \\dots, \\sigma_{r})$, then $\\Sigma^{+}$ has the form $\\begin{pmatrix} D^{-1} & 0 \\\\ 0 & 0 \\end{pmatrix}$ with $D^{-1} = \\text{diag}(1/\\sigma_{1}, \\dots, 1/\\sigma_{r})$.\n\nThe singular values of $\\mathcal{O}^{+}$ are the non-zero diagonal entries of $\\Sigma^{+}$, which are the reciprocals of the non-zero singular values of $\\mathcal{O}$. Specifically, the singular values of $\\mathcal{O}^{+}$ are $\\{1/\\sigma_{r}, \\dots, 1/\\sigma_{1}\\}$.\n\nThe largest singular value of $\\mathcal{O}^{+}$ is therefore $1/\\sigma_{r}$, which is the reciprocal of the smallest non-zero singular value of $\\mathcal{O}$. We denote this smallest non-zero singular value as $\\sigma_{\\min}(\\mathcal{O})$.\nThus, the tightest stability constant $C$ is\n$$\nC = \\|\\mathcal{O}^{+}\\|_{2} = \\frac{1}{\\sigma_{\\min}(\\mathcal{O})}.\n$$\nThe problem states that $\\mathcal{O}$ has full column rank, which for an $m \\times n$ matrix means the rank $r$ is equal to $n$. Consequently, all $n$ singular values of $\\mathcal{O}$ are positive: $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} > 0$. In this case, the smallest non-zero singular value is simply the smallest singular value, $\\sigma_{n}$. The optimal stability constant is therefore\n$$\nC = \\frac{1}{\\sigma_{n}}.\n$$\n\n**Computation for the Specific Operator**\n\nWe are given the operator\n$$\n\\mathcal{O} = \\begin{pmatrix}\n3 & 1 \\\\\n0 & 2 \\\\\n1 & 1\n\\end{pmatrix}.\n$$\nThis is a $3 \\times 2$ matrix, so $m=3$ and $n=2$. The singular values of $\\mathcal{O}$ are the square roots of the eigenvalues of the matrix $\\mathcal{O}^T \\mathcal{O}$. We compute this matrix product:\n$$\n\\mathcal{O}^T \\mathcal{O} = \\begin{pmatrix}\n3 & 0 & 1 \\\\\n1 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 & 1 \\\\\n0 & 2 \\\\\n1 & 1\n\\end{pmatrix} =\n\\begin{pmatrix}\n(3)(3)+(0)(0)+(1)(1) & (3)(1)+(0)(2)+(1)(1) \\\\\n(1)(3)+(2)(0)+(1)(1) & (1)(1)+(2)(2)+(1)(1)\n\\end{pmatrix} =\n\\begin{pmatrix}\n10 & 4 \\\\\n4 & 6\n\\end{pmatrix}.\n$$\nNext, we find the eigenvalues $\\lambda$ of $\\mathcal{O}^T \\mathcal{O}$ by solving the characteristic equation $\\det(\\mathcal{O}^T \\mathcal{O} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix}\n10-\\lambda & 4 \\\\\n4 & 6-\\lambda\n\\end{pmatrix} = 0\n$$\n$$\n(10-\\lambda)(6-\\lambda) - (4)(4) = 0\n$$\n$$\n60 - 16\\lambda + \\lambda^2 - 16 = 0\n$$\n$$\n\\lambda^2 - 16\\lambda + 44 = 0.\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{-(-16) \\pm \\sqrt{(-16)^2 - 4(1)(44)}}{2(1)} = \\frac{16 \\pm \\sqrt{256 - 176}}{2} = \\frac{16 \\pm \\sqrt{80}}{2}.\n$$\nSimplifying the square root, $\\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5}$.\n$$\n\\lambda = \\frac{16 \\pm 4\\sqrt{5}}{2} = 8 \\pm 2\\sqrt{5}.\n$$\nThe two eigenvalues of $\\mathcal{O}^T \\mathcal{O}$ are $\\lambda_{1} = 8 + 2\\sqrt{5}$ and $\\lambda_{2} = 8 - 2\\sqrt{5}$. Both are positive, as expected. The corresponding singular values of $\\mathcal{O}$ are $\\sigma_{1} = \\sqrt{\\lambda_{1}}$ and $\\sigma_{2} = \\sqrt{\\lambda_{2}}$.\nWe need the smallest singular value, which is $\\sigma_{n} = \\sigma_{2}$:\n$$\n\\sigma_{2} = \\sqrt{8 - 2\\sqrt{5}}.\n$$\nThe optimal stability constant $C$ is the reciprocal of this value:\n$$\nC = \\frac{1}{\\sigma_{2}} = \\frac{1}{\\sqrt{8 - 2\\sqrt{5}}}.\n$$\nTo provide a simplified closed-form expression, we rationalize the denominator by considering $C^2$:\n$$\nC^2 = \\frac{1}{8 - 2\\sqrt{5}} = \\frac{1}{8 - 2\\sqrt{5}} \\cdot \\frac{8 + 2\\sqrt{5}}{8 + 2\\sqrt{5}} = \\frac{8 + 2\\sqrt{5}}{8^2 - (2\\sqrt{5})^2} = \\frac{8 + 2\\sqrt{5}}{64 - 20} = \\frac{8 + 2\\sqrt{5}}{44} = \\frac{2(4 + \\sqrt{5})}{44} = \\frac{4 + \\sqrt{5}}{22}.\n$$\nTaking the square root gives the final expression for $C$:\n$$\nC = \\sqrt{\\frac{4 + \\sqrt{5}}{22}}.\n$$\nThis is the exact analytical form for the optimal stability constant.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{4+\\sqrt{5}}{22}}}\n$$"
        },
        {
            "introduction": "Most real-world inverse problems involve nonlinear observation operators, requiring more sophisticated tools for analysis. This exercise introduces the Gauss-Newton Hessian, a crucial approximation for understanding the local curvature of the data misfit landscape around a potential solution. By computing this quantity and relating it to the Fisher Information Matrix, you will connect the geometric concepts of optimization with the statistical measure of how much information the data provides about the parameters .",
            "id": "3374134",
            "problem": "Consider a data assimilation setting with parameter space $\\mathcal{X}=\\mathbb{R}$ and data space $\\mathcal{Y}=\\mathbb{R}^{3}$. Let the observation operator $\\mathcal{O}:\\mathcal{X}\\to\\mathcal{Y}$ be given componentwise by $\\mathcal{O}_{1}(x)=\\exp(x)$, $\\mathcal{O}_{2}(x)=\\sin(2x)$, and $\\mathcal{O}_{3}(x)=\\frac{x}{1+x^{2}}$. Assume the observation model $y=\\mathcal{O}(x_{\\text{true}})+\\eta$, where $\\eta$ is a zero-mean Gaussian random vector with covariance matrix $\\Gamma=\\mathrm{diag}(0.09,\\,0.04,\\,0.25)$ and the data misfit functional is\n$$\nJ(x)=\\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\big(\\mathcal{O}(x)-y\\big)\\right\\|_{2}^{2}.\n$$\nStarting from the Gaussian likelihood and the definition of $J(x)$, derive the Gauss–Newton approximation to the Hessian of $J(x)$ at a general $x$ by applying first principles (Fréchet differentiation and the chain rule), and interpret its role in quantifying the local curvature of the data misfit. Then evaluate this Gauss–Newton Hessian approximation at the point $x^{\\star}=0.5$ for the given $\\mathcal{O}$ and $\\Gamma$, and present the final value as a single closed-form analytic expression. Finally, explain how the derived quantity relates to the Fisher Information Matrix (FIM) for the Gaussian observation model. Express your final answer as a single closed-form analytic expression.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the theory of inverse problems, well-posed, objective, and self-contained for the required calculations. All necessary functions, constants, and definitions are provided, and there are no contradictions or ambiguities.\n\nThe data misfit functional is given by\n$$\nJ(x)=\\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\big(\\mathcal{O}(x)-y\\big)\\right\\|_{2}^{2}\n$$\nwhere $x \\in \\mathcal{X}=\\mathbb{R}$, $y \\in \\mathcal{Y}=\\mathbb{R}^3$, $\\mathcal{O}:\\mathbb{R} \\to \\mathbb{R}^3$ is the observation operator, and $\\Gamma$ is the $3 \\times 3$ positive-definite covariance matrix of the observation noise. This expression is a weighted Euclidean norm squared, which can be written as a quadratic form:\n$$\nJ(x) = \\frac{1}{2} \\left( \\mathcal{O}(x) - y \\right)^T \\left(\\Gamma^{-1/2}\\right)^T \\Gamma^{-1/2} \\left( \\mathcal{O}(x) - y \\right) = \\frac{1}{2} \\left( \\mathcal{O}(x) - y \\right)^T \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right)\n$$\nsince $\\Gamma$, and therefore $\\Gamma^{-1/2}$, is symmetric.\n\n**Part 1: Derivation of the Gauss-Newton Hessian**\n\nTo find the Hessian of $J(x)$, we first compute its gradient (its Fréchet derivative). Since $x$ is a scalar, the Fréchet derivative of $\\mathcal{O}(x)$ is its Jacobian matrix with respect to $x$, which we denote by $O'(x)$. This is a $3 \\times 1$ column vector:\n$$\nO'(x) = \\frac{d\\mathcal{O}}{dx}(x) = \\begin{pmatrix} \\frac{d\\mathcal{O}_1}{dx}(x) \\\\ \\frac{d\\mathcal{O}_2}{dx}(x) \\\\ \\frac{d\\mathcal{O}_3}{dx}(x) \\end{pmatrix}\n$$\nUsing the chain rule, the gradient of the scalar functional $J(x)$ is:\n$$\n\\nabla J(x) = \\frac{dJ}{dx}(x) = \\left( O'(x) \\right)^T \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right)\n$$\nThis is a $1 \\times 1$ scalar, as expected.\n\nNext, we differentiate $\\nabla J(x)$ with respect to $x$ to find the Hessian, $H(x) = \\nabla^2 J(x)$. We apply the product rule:\n$$\nH(x) = \\frac{d}{dx} \\left[ \\left( O'(x) \\right)^T \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right) \\right]\n$$\n$$\nH(x) = \\left( \\frac{d}{dx} \\left( O'(x) \\right)^T \\right) \\Gamma^{-1} \\left( \\mathcal{O}(x) - y \\right) + \\left( O'(x) \\right)^T \\Gamma^{-1} \\left( \\frac{d}{dx} \\left( \\mathcal{O}(x) - y \\right) \\right)\n$$\nLet $O''(x)$ denote the $3 \\times 1$ vector of second derivatives of the components of $\\mathcal{O}(x)$. The derivative of the transposed Jacobian is $(O''(x))^T$. The derivative of the residual term is simply $O'(x)$. Substituting these into the expression for $H(x)$ gives the exact Hessian:\n$$\nH(x) = (O'(x))^T \\Gamma^{-1} O'(x) + (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - y)\n$$\nThe Gauss-Newton method provides an approximation to the Hessian by neglecting the second term. This approximation is justified when the model is nearly linear (i.e., $O''(x)$ is small) or when the model provides a good fit to the data (i.e., the residual $\\mathcal{O}(x) - y$ is small). The resulting Gauss-Newton Hessian, $H_{GN}(x)$, is:\n$$\nH_{GN}(x) = (O'(x))^T \\Gamma^{-1} O'(x)\n$$\nThis approximation has the practical advantage of always being positive semi-definite, which is beneficial for optimization algorithms that rely on this property to ensure descent directions.\n\nThe Hessian of a function describes its local curvature. For the misfit functional $J(x)$, $H_{GN}(x)$ thus approximates the curvature of the misfit landscape around a point $x$. A large value of $H_{GN}(x)$ implies a sharp, well-defined minimum, indicating that the data are highly sensitive to the parameter $x$ and can constrain it well. A small value implies a flat, wide minimum, indicating low sensitivity and poor constraint.\n\n**Part 2: Evaluation at $x^{\\star}=0.5$**\n\nWe must evaluate $H_{GN}(x^{\\star}) = (O'(x^{\\star}))^T \\Gamma^{-1} O'(x^{\\star})$ at $x^{\\star}=0.5$.\nFirst, we find the components of the Jacobian $O'(x)$:\n$$\n\\mathcal{O}_{1}(x)=\\exp(x) \\implies \\frac{d\\mathcal{O}_{1}}{dx}(x) = \\exp(x)\n$$\n$$\n\\mathcal{O}_{2}(x)=\\sin(2x) \\implies \\frac{d\\mathcal{O}_{2}}{dx}(x) = 2\\cos(2x)\n$$\n$$\n\\mathcal{O}_{3}(x)=\\frac{x}{1+x^{2}} \\implies \\frac{d\\mathcal{O}_{3}}{dx}(x) = \\frac{(1)(1+x^2) - x(2x)}{(1+x^2)^2} = \\frac{1-x^2}{(1+x^2)^2}\n$$\nAt $x^{\\star}=0.5 = 1/2$:\n$$\n\\frac{d\\mathcal{O}_{1}}{dx}(0.5) = \\exp(0.5)\n$$\n$$\n\\frac{d\\mathcal{O}_{2}}{dx}(0.5) = 2\\cos(2 \\cdot 0.5) = 2\\cos(1)\n$$\n$$\n\\frac{d\\mathcal{O}_{3}}{dx}(0.5) = \\frac{1-(0.5)^2}{(1+(0.5)^2)^2} = \\frac{1-1/4}{(1+1/4)^2} = \\frac{3/4}{(5/4)^2} = \\frac{3}{4} \\cdot \\frac{16}{25} = \\frac{12}{25}\n$$\nThe given covariance matrix is $\\Gamma=\\mathrm{diag}(0.09, 0.04, 0.25)$. Its inverse is:\n$$\n\\Gamma^{-1} = \\mathrm{diag}\\left(\\frac{1}{0.09}, \\frac{1}{0.04}, \\frac{1}{0.25}\\right) = \\mathrm{diag}\\left(\\frac{100}{9}, \\frac{100}{4}, \\frac{100}{25}\\right) = \\mathrm{diag}\\left(\\frac{100}{9}, 25, 4\\right)\n$$\nThe Gauss-Newton Hessian is a scalar sum:\n$$\nH_{GN}(x) = \\sum_{i=1}^3 \\frac{1}{\\Gamma_{ii}} \\left( \\frac{d\\mathcal{O}_i}{dx}(x) \\right)^2\n$$\nEvaluating at $x^{\\star}=0.5$:\n$$\nH_{GN}(0.5) = \\frac{100}{9} (\\exp(0.5))^2 + 25 (2\\cos(1))^2 + 4 \\left(\\frac{12}{25}\\right)^2\n$$\n$$\nH_{GN}(0.5) = \\frac{100}{9} \\exp(1) + 25 (4\\cos^2(1)) + 4 \\left(\\frac{144}{625}\\right)\n$$\n$$\nH_{GN}(0.5) = \\frac{100}{9} \\exp(1) + 100\\cos^2(1) + \\frac{576}{625}\n$$\nThis is the final closed-form analytic expression for the Gauss-Newton Hessian at $x^{\\star}=0.5$.\n\n**Part 3: Relation to the Fisher Information Matrix (FIM)**\n\nFor an observation model $y = \\mathcal{O}(x) + \\eta$ with Gaussian noise $\\eta \\sim \\mathcal{N}(0, \\Gamma)$, the likelihood function $p(y|x)$ is:\n$$\np(y|x) = \\frac{1}{\\sqrt{(2\\pi)^3 \\det(\\Gamma)}} \\exp\\left( -\\frac{1}{2} (\\mathcal{O}(x)-y)^T \\Gamma^{-1} (\\mathcal{O}(x)-y) \\right)\n$$\nThe log-likelihood, $\\ell(x; y) = \\ln(p(y|x))$, is:\n$$\n\\ell(x; y) = -\\frac{1}{2} \\ln((2\\pi)^3 \\det(\\Gamma)) - \\frac{1}{2} (\\mathcal{O}(x)-y)^T \\Gamma^{-1} (\\mathcal{O}(x)-y) = -\\text{const} - J(x)\n$$\nThe Fisher Information Matrix (a scalar in this case since $x \\in \\mathbb{R}$) is defined as the negative expected value of the second derivative of the log-likelihood with respect to the parameter $x$:\n$$\nI(x) = -\\mathbb{E}_{y|x} \\left[ \\frac{\\partial^2 \\ell(x;y)}{\\partial x^2} \\right]\n$$\nThe second derivative of the log-likelihood is:\n$$\n\\frac{\\partial^2 \\ell(x;y)}{\\partial x^2} = -\\frac{\\partial^2 J(x)}{\\partial x^2} = -H(x) = -\\left( (O'(x))^T \\Gamma^{-1} O'(x) + (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - y) \\right)\n$$\nNow we take the expectation over all possible observations $y$, given $x$. This means $y$ is treated as a random variable $Y = \\mathcal{O}(x) + \\eta$.\n$$\nI(x) = - \\mathbb{E}_{Y|x} \\left[ -H(x) \\right] = \\mathbb{E}_{Y|x} \\left[ (O'(x))^T \\Gamma^{-1} O'(x) + (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - Y) \\right]\n$$\nUsing the linearity of expectation:\n$$\nI(x) = (O'(x))^T \\Gamma^{-1} O'(x) + \\mathbb{E}_{Y|x} \\left[ (O''(x))^T \\Gamma^{-1} (\\mathcal{O}(x) - Y) \\right]\n$$\nThe term $(\\mathcal{O}(x) - Y)$ is equal to $(\\mathcal{O}(x) - (\\mathcal{O}(x) + \\eta)) = -\\eta$. The expectation of this term is:\n$$\n\\mathbb{E}_{Y|x} [-\\eta] = - \\mathbb{E}[\\eta] = 0\n$$\nSince the noise vector $\\eta$ has zero mean. Therefore, the second term in the expression for $I(x)$ vanishes. This leaves:\n$$\nI(x) = (O'(x))^T \\Gamma^{-1} O'(x)\n$$\nThis demonstrates that for a model with additive Gaussian noise, the Fisher Information Matrix is exactly equal to the Gauss-Newton approximation of the Hessian of the data misfit functional. The FIM quantifies the information that the data provides about the parameter $x$, and its inverse gives the Cramér-Rao lower bound on the variance of any unbiased estimator for $x$.",
            "answer": "$$\n\\boxed{\\frac{100}{9}\\exp(1) + 100\\cos^2(1) + \\frac{576}{625}}\n$$"
        },
        {
            "introduction": "This practice transitions from static problems to the dynamic world of data assimilation, where we aim to reconstruct the state of an evolving system from time-series data. You will implement a numerical experiment to assess the local observability of the famous Hénon map, a simple model for chaotic dynamics. By computing the rank of the sensitivity matrix under different observation scenarios, you will gain hands-on experience with the core concepts of 4D-Var and understand how the choice of an observation operator determines our ability to track a complex system .",
            "id": "3374175",
            "problem": "Consider a discrete-time nonlinear dynamical system with state dimension $n \\in \\mathbb{N}$ and a parameter vector $p \\in \\mathbb{R}^q$ given by the evolution law $x_{t+1} = f(x_t, p)$ for $t \\in \\{0,1,2,\\ldots\\}$. Let $H: \\mathbb{R}^n \\to \\mathbb{R}^m$ be an observation operator. In Four-Dimensional Variational (4D-Var) data assimilation, a central quantity is the sensitivity of observations to perturbations in the initial state $x_0$ along a trajectory, captured by the time-varying linearization. Let $\\Phi_t \\in \\mathbb{R}^{n \\times n}$ denote the state transition (tangent linear) matrix from time $0$ to time $t$, defined recursively by $\\Phi_0 = I_n$ and $\\Phi_{t+1} = f_x(x_t, p)\\,\\Phi_t$, where $f_x(x_t,p) \\in \\mathbb{R}^{n \\times n}$ is the Jacobian of $f$ with respect to the state, evaluated along the trajectory. The Jacobian of the observation operator at time $t$ is $H_x(x_t) \\in \\mathbb{R}^{m \\times n}$. Over an assimilation window of length $T \\in \\mathbb{N}$, define the stacked sensitivity matrix $O_T \\in \\mathbb{R}^{(mT) \\times n}$ by vertically stacking the blocks $H_x(x_t)\\Phi_t$ for $t \\in \\{1,2,\\ldots,T\\}$, and define the time-averaged Jacobian $J_{\\mathrm{avg}} \\in \\mathbb{R}^{m \\times n}$ by $J_{\\mathrm{avg}} = \\frac{1}{T}\\sum_{t=1}^T H_x(x_t)\\Phi_t$. A standard notion of local observability of the initial condition $x_0$ with known $p$ is that $O_T$ has full column rank $n$. Your task is to implement a program that, for a concrete chaotic model and specific sparse observation operators, computes numerical ranks and assesses observability.\n\nUse the Hénon map with state dimension $n=2$ and parameter vector $p = (a,b) \\in \\mathbb{R}^2$, defined by\n$$\n\\begin{pmatrix}\nx_{t+1}^{(1)}\\\\\nx_{t+1}^{(2)}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 - a\\big(x_t^{(1)}\\big)^2 + x_t^{(2)}\\\\\nb\\, x_t^{(1)}\n\\end{pmatrix}.\n$$\nThe Jacobian of $f$ with respect to the state is\n$$\nf_x(x_t,p) =\n\\begin{pmatrix}\n-2 a\\, x_t^{(1)} & 1\\\\\nb & 0\n\\end{pmatrix}.\n$$\nConsider three observation operators $H$ of the form $H(x) = C x$ where $C \\in \\mathbb{R}^{m \\times 2}$ is constant (hence $H_x(x_t) = C$ for all $t$), chosen to represent sparse and dense sensing:\n- Case A (sparse): observe only the first component, $m=1$ with $C = \\begin{pmatrix}1 & 0\\end{pmatrix}$.\n- Case B (dense): observe both components, $m=2$ with $C = I_2$.\n- Case C (null): observe nothing, $m=2$ with $C = \\begin{pmatrix}0 & 0\\\\ 0 & 0\\end{pmatrix}$.\n\nStarting from the initial condition $x_0 = (0.1,\\,0.3)$ and parameters $p=(a,b)=(1.4,\\,0.3)$, propagate the Hénon map to compute the trajectory $\\{x_t\\}_{t=0}^T$ and the tangent linear matrices $\\{\\Phi_t\\}_{t=0}^T$ for each test case. For each test case, form $O_T$ and $J_{\\mathrm{avg}}$ and compute their numerical ranks using the following principle: if $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots$ are the singular values from a singular value decomposition, then the numerical rank is the count of singular values $\\sigma_i$ satisfying $\\sigma_i > \\tau$, where the tolerance is\n$$\n\\tau = \\epsilon \\cdot \\max\\{M,N\\} \\cdot \\sigma_1,\n$$\nwith $M \\times N$ the matrix dimension and $\\epsilon$ the machine epsilon for double-precision floating-point arithmetic. If $\\sigma_1 = 0$, the numerical rank is defined to be $0$.\n\nCompute, for each test case, the triple consisting of:\n- the numerical rank of $O_T$,\n- the numerical rank of $J_{\\mathrm{avg}}$,\n- a boolean indicating whether the initial condition is locally observable, defined as whether the rank of $O_T$ equals $n=2$.\n\nYour program must implement this procedure for the following test suite:\n- Test 1 (happy path, sparse sensing): $T=50$ and Case A.\n- Test 2 (happy path, dense sensing): $T=50$ and Case B.\n- Test 3 (edge case, null sensing): $T=50$ and Case C.\n- Test 4 (boundary window, sparse sensing): $T=1$ and Case A.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\mathrm{rank}(O_T)_{\\mathrm{Test\\,1}},\\ \\mathrm{rank}(J_{\\mathrm{avg}})_{\\mathrm{Test\\,1}},\\ \\mathrm{observable}_{\\mathrm{Test\\,1}},\\ \\mathrm{rank}(O_T)_{\\mathrm{Test\\,2}},\\ \\mathrm{rank}(J_{\\mathrm{avg}})_{\\mathrm{Test\\,2}},\\ \\mathrm{observable}_{\\mathrm{Test\\,2}},\\ \\mathrm{rank}(O_T)_{\\mathrm{Test\\,3}},\\ \\mathrm{rank}(J_{\\mathrm{avg}})_{\\mathrm{Test\\,3}},\\ \\mathrm{observable}_{\\mathrm{Test\\,3}},\\ \\mathrm{rank}(O_T)_{\\mathrm{Test\\,4}},\\ \\mathrm{rank}(J_{\\mathrm{avg}})_{\\mathrm{Test\\,4}},\\ \\mathrm{observable}_{\\mathrm{Test\\,4}}].\n$$\nNo physical units or angle units are involved. The output entries must be of types boolean or integer as specified above, and no additional text should be printed.",
            "solution": "The problem requires an analysis of the local observability of the initial state of the Hénon map, a classical discrete-time chaotic dynamical system. This analysis is performed within the framework of Four-Dimensional Variational (4D-Var) data assimilation. The core of the task is to compute the numerical ranks of two key sensitivity matrices, the stacked sensitivity matrix $O_T$ and the time-averaged Jacobian $J_{\\mathrm{avg}}$, for different observation scenarios and time windows.\n\nThe system is defined by the Hénon map:\n$$\nx_{t+1} = f(x_t, p) = \\begin{pmatrix} 1 - a(x_t^{(1)})^2 + x_t^{(2)} \\\\ b x_t^{(1)} \\end{pmatrix}\n$$\nwhere the state is $x_t = (x_t^{(1)}, x_t^{(2)})^T \\in \\mathbb{R}^2$ (so state dimension $n=2$) and the parameters are $p=(a,b) = (1.4, 0.3)$ starting from an initial state $x_0 = (0.1, 0.3)^T$.\n\nThe sensitivity of the system's state at time $t$ to a small perturbation in the initial state $x_0$ is described by the state transition matrix (or tangent linear model) $\\Phi_t \\in \\mathbb{R}^{2 \\times 2}$. It evolves according to the recursive relation:\n$$\n\\Phi_{t+1} = f_x(x_t, p) \\Phi_t, \\quad \\text{with} \\quad \\Phi_0 = I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix and $f_x(x_t, p)$ is the Jacobian of the map $f$ with respect to the state $x$, evaluated along the trajectory generated from $x_0$:\n$$\nf_x(x_t, p) = \\frac{\\partial f}{\\partial x}\\bigg|_{x_t, p} = \\begin{pmatrix} -2 a x_t^{(1)} & 1 \\\\ b & 0 \\end{pmatrix}\n$$\n\nThe observations are given by a linear operator $y_t = H(x_t) = C x_t$. The Jacobian of the observation operator, $H_x(x_t)$, is therefore the constant matrix $C$. The sensitivity of an observation at time $t$ to the initial state $x_0$ is given by the chain rule: $\\frac{\\partial y_t}{\\partial x_0} = H_x(x_t) \\frac{\\partial x_t}{\\partial x_0} = C \\Phi_t$.\n\nThe problem defines two matrices to be analyzed over an assimilation window of length $T$:\n1.  The **stacked sensitivity matrix** $O_T \\in \\mathbb{R}^{(mT) \\times n}$. This matrix aggregates the sensitivity information from all observations in the window. It is constructed by vertically stacking the individual sensitivity blocks for $t=1, 2, \\ldots, T$:\n    $$\n    O_T = \\begin{pmatrix} C \\Phi_1 \\\\ C \\Phi_2 \\\\ \\vdots \\\\ C \\Phi_T \\end{pmatrix}\n    $$\n2.  The **time-averaged Jacobian** $J_{\\mathrm{avg}} \\in \\mathbb{R}^{m \\times n}$. This matrix represents the average sensitivity over the window:\n    $$\n    J_{\\mathrm{avg}} = \\frac{1}{T} \\sum_{t=1}^{T} C \\Phi_t\n    $$\n\nLocal observability of the initial condition $x_0$ is established if $O_T$ has full column rank, which is $n=2$ in this case. A full column rank implies that the mapping from an initial state perturbation to the sequence of observation perturbations is injective, meaning different initial state perturbations lead to different observation sequences, allowing $x_0$ to be uniquely determined (locally).\n\nThe numerical rank of a matrix $A$ of size $M \\times N$ is determined using its singular value decomposition (SVD). Let the singular values be $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$. The rank is the number of singular values $\\sigma_i$ that exceed a specified tolerance $\\tau$:\n$$\n\\tau = \\epsilon \\cdot \\max\\{M, N\\} \\cdot \\sigma_1\n$$\nwhere $\\epsilon$ is the machine epsilon for double-precision floating-point numbers and $\\sigma_1$ is the largest singular value. If $\\sigma_1=0$, the matrix is null and its rank is $0$.\n\nThe overall algorithm for each test case $(T, C)$ is as follows:\n1.  Initialize the state $x_0 = (0.1, 0.3)^T$ and the state transition matrix $\\Phi_0 = I_2$.\n2.  Iterate from $t=0$ to $T-1$:\n    a. Propagate the state: $x_{t+1} = f(x_t, p)$.\n    b. Propagate the state transition matrix: $\\Phi_{t+1} = f_x(x_t, p) \\Phi_t$.\n    c. Store the matrices $C \\Phi_{t+1}$. This generates the sequence of sensitivity blocks for $t=1, \\ldots, T$.\n3.  Construct $O_T$ by vertically stacking the stored blocks.\n4.  Construct $J_{\\mathrm{avg}}$ by computing the mean of the stored blocks.\n5.  Compute the numerical ranks of $O_T$ and $J_{\\mathrm{avg}}$ using the SVD-based method.\n6.  Determine local observability by checking if the numerical rank of $O_T$ is equal to $n=2$.\n7.  Collect the triple $(\\mathrm{rank}(O_T), \\mathrm{rank}(J_{\\mathrm{avg}}), \\mathrm{observable})$.\n\nThis procedure is applied to the four specified test cases, combining different window lengths ($T=1, 50$) and observation operators (sparse, dense, and null sensing).\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_numerical_rank(matrix: np.ndarray) -> int:\n    \"\"\"\n    Computes the numerical rank of a matrix based on its singular values.\n    The rank is the number of singular values greater than a tolerance tau.\n    tau = epsilon * max(M, N) * sigma_1\n    \"\"\"\n    # An all-zero matrix has rank 0. np.any is a fast check for this.\n    if not np.any(matrix):\n        return 0\n\n    M, N = matrix.shape\n    try:\n        # We only need the singular values, so compute_uv=False for efficiency.\n        s = np.linalg.svd(matrix, compute_uv=False)\n    except np.linalg.LinAlgError:\n        # Graceful handling for non-convergence, though highly unlikely here.\n        return 0\n\n    # The singular values are sorted in descending order. sigma_1 is the largest.\n    sigma_1 = s[0]\n\n    # If the largest singular value is effectively zero, the matrix is null.\n    if np.isclose(sigma_1, 0.0):\n        return 0\n\n    # Epsilon for double-precision floating-point arithmetic.\n    epsilon = np.finfo(float).eps\n    \n    # Calculate the tolerance as specified in the problem statement.\n    tolerance = epsilon * max(M, N) * sigma_1\n\n    # The rank is the number of singular values greater than the tolerance.\n    rank = np.sum(s > tolerance)\n    return int(rank)\n\ndef solve():\n    \"\"\"\n    Main function to run the 4D-Var observability analysis for the Hénon map.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # --- Problem Constants and Initial Conditions ---\n    x0 = np.array([0.1, 0.3])\n    a, b = 1.4, 0.3\n    n_dim = 2  # State dimension n=2\n\n    # --- Test Case Definitions ---\n    # Observation matrices C for each case\n    C_A = np.array([[1.0, 0.0]])  # Case A: Observe x^(1)\n    C_B = np.identity(2)         # Case B: Observe both x^(1) and x^(2)\n    C_C = np.zeros((2, 2))       # Case C: Observe nothing\n\n    # Test suite: (T, C_matrix)\n    test_cases = [\n        (50, C_A), # Test 1: T=50, Case A\n        (50, C_B), # Test 2: T=50, Case B\n        (50, C_C), # Test 3: T=50, Case C\n        (1, C_A),  # Test 4: T=1, Case A\n    ]\n\n    all_results = []\n    \n    for T, C in test_cases:\n        # --- Trajectory and Tangent Linear Model Propagation ---\n        # Initialize state and state transition matrix for the current case.\n        x_t = x0.copy()\n        Phi_t = np.identity(n_dim)\n        \n        # List to store the blocks H_x(x_t) * Phi_t = C @ Phi_t for t = 1, ..., T\n        sensitivity_blocks = []\n\n        for _ in range(T):\n            # Evaluate the Jacobian f_x at the current state x_t\n            f_x = np.array([\n                [-2 * a * x_t[0], 1.0],\n                [b,               0.0]\n            ])\n            \n            # Propagate the state transition matrix: Phi_{t+1} = f_x(x_t) @ Phi_t\n            # We update Phi_t in-place for the next iteration.\n            Phi_t = f_x @ Phi_t\n            \n            # Propagate the state: x_{t+1} = f(x_t)\n            x_t_next_0 = 1.0 - a * x_t[0]**2 + x_t[1]\n            x_t_next_1 = b * x_t[0]\n            x_t = np.array([x_t_next_0, x_t_next_1])\n            \n            # Calculate and store the sensitivity block C @ Phi_t.\n            # The problem asks for blocks for t=1..T.\n            # In iteration `k` (from 0 to T-1), we compute and store C @ Phi_{k+1}.\n            sensitivity_blocks.append(C @ Phi_t)\n\n        # --- Matrix Construction ---\n        # O_T is formed by vertically stacking the blocks C @ Phi_t for t=1..T\n        O_T = np.vstack(sensitivity_blocks)\n        \n        # J_avg is the time-average of these blocks\n        J_avg = np.mean(np.array(sensitivity_blocks), axis=0)\n        \n        # --- Rank Computation and Observability Analysis ---\n        rank_O_T = get_numerical_rank(O_T)\n        rank_J_avg = get_numerical_rank(J_avg)\n        \n        # Local observability is true if the rank of O_T equals the state dimension n\n        is_observable = (rank_O_T == n_dim)\n        \n        all_results.extend([rank_O_T, rank_J_avg, is_observable])\n\n    # Final print statement in the exact required format.\n    # Convert results to strings, with booleans as lowercase 'true'/'false'.\n    output_str = ','.join(str(res).lower() if isinstance(res, bool) else str(res) for res in all_results)\n    \n    print(f\"[{output_str}]\")\n\n# This is called to generate the output string for the answer.\n# solve()\n```",
            "answer": "[2,2,true,2,2,true,0,0,false,1,1,false]"
        }
    ]
}