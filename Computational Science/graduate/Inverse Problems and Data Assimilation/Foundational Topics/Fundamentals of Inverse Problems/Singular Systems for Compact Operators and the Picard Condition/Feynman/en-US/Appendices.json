{
    "hands_on_practices": [
        {
            "introduction": "Even when the Picard condition guarantees that a solution to the equation $Tx=y$ exists, it does not promise the solution is unique. This exercise provides a foundational look into why this is the case by exploring an operator with a non-trivial nullspace . By constructing a specific solution space, you will see firsthand how an entire family of valid solutions can exist and why selecting the unique, minimal-norm solution is a crucial first step in stabilizing inverse problems.",
            "id": "3419607",
            "problem": "Consider the separable Hilbert space $H = \\ell^{2}(\\mathbb{N})$ with its canonical orthonormal basis $\\{e_{k}\\}_{k=1}^{\\infty}$. Let $T : H \\to H$ be a compact linear operator with a singular system $\\{(\\sigma_{k}, u_{k}, v_{k})\\}_{k=1}^{\\infty}$, where $u_{k} = e_{k}$ and $v_{k} = e_{k}$ for all $k$, and singular values defined by\n$$\n\\sigma_{1} = 1,\\quad \\sigma_{2} = \\frac{1}{2},\\quad \\sigma_{3} = \\frac{1}{3},\\quad \\sigma_{k} = 0 \\text{ for all } k \\geq 4.\n$$\nAssume $T$ acts by $T v_{k} = \\sigma_{k} u_{k}$ for all $k$ and that $\\{u_{k}\\}$ and $\\{v_{k}\\}$ are orthonormal bases. The operator $T$ is thus finite rank and compact, with a nontrivial nullspace. The Picard condition, in the context of this singular system, stipulates that a datum $y \\in H$ belongs to the range of $T$ if and only if $y$ is orthogonal to all $u_{k}$ for which $\\sigma_{k} = 0$ and the series\n$$\n\\sum_{k=1}^{\\infty} \\frac{|\\langle y, u_{k} \\rangle|^{2}}{\\sigma_{k}^{2}}\n$$\nis finite, where $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product in $H$.\n\nConstruct a specific right-hand side $y \\in H$ by prescribing its coefficients in the $\\{u_{k}\\}$-basis as\n$$\n\\beta_{k} := \\langle y, u_{k} \\rangle =\n\\begin{cases}\n1,  k = 1, \\\\\n\\frac{1}{2},  k = 2, \\\\\n\\frac{1}{3},  k = 3, \\\\\n0,  k \\geq 4,\n\\end{cases}\n$$\nso that $y = \\sum_{k=1}^{\\infty} \\beta_{k} u_{k}$.\n\nStarting from the fundamental definitions of compact operators on Hilbert spaces, singular systems, and the Picard condition, do the following:\n\n- Verify that the constructed $y$ satisfies the Picard condition for $T$.\n- Characterize the solution set $\\{x \\in H : T x = y\\}$ using the singular system, and explain why the presence of the nullspace of $T$ implies that non-minimal-norm solutions can exhibit unbounded growth in certain components.\n- Derive, from first principles, the minimal-norm solution $x_{\\min}$ in the right singular vector basis.\n\nYou must provide the minimal-norm solution $x_{\\min}$ as a single closed-form analytic expression in terms of $\\{v_{k}\\}$ as your final reported answer. No numerical rounding is required and no physical units are involved. The final answer must be a single expression only, not an equation or inequality.",
            "solution": "The problem is valid. It is a well-posed mathematical problem in the field of functional analysis and inverse problems, containing all necessary information and being free of contradictions or scientific flaws.\n\nWe are tasked with solving the linear operator equation $Tx = y$ for a compact operator $T$ on the Hilbert space $H = \\ell^{2}(\\mathbb{N})$. The operator $T$ is defined via its singular system $\\{(\\sigma_{k}, u_{k}, v_{k})\\}_{k=1}^{\\infty}$, where $\\{u_k\\}$ and $\\{v_k\\}$ are orthonormal bases for $H$, and $T$ acts as $Tv = \\sum_{k=1}^{\\infty} \\sigma_k \\langle v, v_k \\rangle u_k$ for any $v \\in H$. In this specific problem, we are given $u_k = v_k = e_k$ (the canonical basis) for all $k \\geq 1$. The singular values are $\\sigma_{1} = 1$, $\\sigma_{2} = \\frac{1}{2}$, $\\sigma_{3} = \\frac{1}{3}$, and $\\sigma_{k} = 0$ for $k \\geq 4$. The right-hand side $y \\in H$ is given by its components in the $\\{u_k\\}$ basis, $\\beta_k = \\langle y, u_k \\rangle$, as $\\beta_1=1$, $\\beta_2=1/2$, $\\beta_3=1/3$, and $\\beta_k=0$ for $k \\geq 4$. Thus, $y = \\sum_{k=1}^{3} \\beta_k u_k = 1 \\cdot u_1 + \\frac{1}{2} u_2 + \\frac{1}{3} u_3$.\n\nFirst, we verify that the given $y$ satisfies the Picard condition. The condition has two parts:\n1. Orthogonality to the nullspace of the adjoint: The datum $y$ must be in the closure of the range of $T$, which is equivalent to $y$ being orthogonal to the nullspace of the adjoint operator $T^*$. In terms of the singular system, this means $\\langle y, u_k \\rangle = 0$ for all $k$ such that $\\sigma_k = 0$.\nIn our case, $\\sigma_k=0$ for all $k \\geq 4$. We are given the coefficients of $y$ as $\\beta_k = \\langle y, u_k \\rangle$. By construction, $\\beta_k = 0$ for all $k \\ge 4$. Therefore, this part of the condition is satisfied.\n2. Finite weighted norm: The series $\\sum_{k=1}^{\\infty} \\frac{|\\langle y, u_k \\rangle|^2}{\\sigma_k^2}$ must be finite. This sum is understood to be over indices $k$ for which $\\sigma_k \\neq 0$.\nIn our case, $\\sigma_k \\neq 0$ only for $k \\in \\{1, 2, 3\\}$. We compute the sum:\n$$\n\\sum_{k=1}^{3} \\frac{|\\langle y, u_k \\rangle|^2}{\\sigma_k^2} = \\frac{|\\beta_1|^2}{\\sigma_1^2} + \\frac{|\\beta_2|^2}{\\sigma_2^2} + \\frac{|\\beta_3|^2}{\\sigma_3^2} = \\frac{|1|^2}{1^2} + \\frac{|\\frac{1}{2}|^2}{(\\frac{1}{2})^2} + \\frac{|\\frac{1}{3}|^2}{(\\frac{1}{3})^2} = 1 + 1 + 1 = 3.\n$$\nSince $3$ is a finite value, the second part of the Picard condition is also satisfied. Thus, the problem $Tx=y$ has at least one solution.\n\nNext, we characterize the solution set. Let an arbitrary $x \\in H$ be expanded in the orthonormal basis $\\{v_k\\}$:\n$$\nx = \\sum_{j=1}^{\\infty} \\alpha_j v_j, \\quad \\text{where } \\alpha_j = \\langle x, v_j \\rangle.\n$$\nApplying the operator $T$ and using its linearity and continuity, we get:\n$$\nTx = T \\left( \\sum_{j=1}^{\\infty} \\alpha_j v_j \\right) = \\sum_{j=1}^{\\infty} \\alpha_j T v_j.\n$$\nBy the definition of the action of $T$ on its singular vectors, $Tv_j = \\sigma_j u_j$, so\n$$\nTx = \\sum_{j=1}^{\\infty} \\alpha_j \\sigma_j u_j.\n$$\nWe set this equal to $y = \\sum_{k=1}^{\\infty} \\beta_k u_k$:\n$$\n\\sum_{k=1}^{\\infty} \\alpha_k \\sigma_k u_k = \\sum_{k=1}^{\\infty} \\beta_k u_k.\n$$\nSince $\\{u_k\\}$ is an orthonormal basis, we can equate the coefficients for each $k$:\n$$\n\\alpha_k \\sigma_k = \\beta_k \\quad \\text{for all } k \\geq 1.\n$$\nFor $k \\in \\{1, 2, 3\\}$, $\\sigma_k \\neq 0$, so we can uniquely determine the coefficients $\\alpha_k$:\n$\\alpha_1 = \\frac{\\beta_1}{\\sigma_1} = \\frac{1}{1} = 1$.\n$\\alpha_2 = \\frac{\\beta_2}{\\sigma_2} = \\frac{1/2}{1/2} = 1$.\n$\\alpha_3 = \\frac{\\beta_3}{\\sigma_3} = \\frac{1/3}{1/3} = 1$.\nFor $k \\geq 4$, $\\sigma_k = 0$ and $\\beta_k = 0$. The equation becomes $\\alpha_k \\cdot 0 = 0$. This equation holds for any value of $\\alpha_k$.\nThe solution $x$ must be an element of $H = \\ell^2(\\mathbb{N})$, which means its norm must be finite: $\\|x\\|^2 = \\sum_{k=1}^\\infty |\\alpha_k|^2  \\infty$.\nThe general solution is therefore of the form:\n$$\nx = \\alpha_1 v_1 + \\alpha_2 v_2 + \\alpha_3 v_3 + \\sum_{k=4}^{\\infty} \\alpha_k v_k = (v_1 + v_2 + v_3) + \\sum_{k=4}^{\\infty} \\alpha_k v_k,\n$$\nwhere the coefficients $\\alpha_k$ for $k \\geq 4$ are arbitrary, subject to the constraint $\\sum_{k=4}^{\\infty} |\\alpha_k|^2  \\infty$.\n\nThe set of vectors $z = \\sum_{k=4}^{\\infty} \\alpha_k v_k$ with $\\sum_{k=4}^{\\infty} |\\alpha_k|^2  \\infty$ is precisely the nullspace of $T$, $\\ker(T)$, since $Tz = \\sum_{k=4}^{\\infty} \\alpha_k \\sigma_k u_k = \\sum_{k=4}^{\\infty} \\alpha_k \\cdot 0 \\cdot u_k = 0$. The solution set is thus the affine subspace $x_{\\text{part}} + \\ker(T)$, where $x_{\\text{part}} = v_1 + v_2 + v_3$ is a particular solution.\nThe \"unbounded growth\" of components in non-minimal-norm solutions arises because $\\ker(T)$ is an infinite-dimensional subspace. While any individual element $z \\in \\ker(T)$ must have a finite norm, a sequence of its components $\\alpha_k$ must converge to zero. However, there is no upper bound on the magnitude of any single component across the entire solution set. For any real number $M  0$ and any integer $k_0 \\geq 4$, the vector $x = (v_1+v_2+v_3) + M v_{k_0}$ is a valid solution to $Tx=y$. Its $k_0$-th component is $M$, which can be arbitrarily large. This illustrates that while the full vector of coefficients $\\{\\alpha_k\\}_{k=4}^\\infty$ must be square-summable, individual coefficients are not bounded.\n\nFinally, we derive the minimal-norm solution $x_{\\min}$. The norm squared of a general solution $x$ is:\n$$\n\\|x\\|^2 = \\left\\| (v_1 + v_2 + v_3) + \\sum_{k=4}^{\\infty} \\alpha_k v_k \\right\\|^2.\n$$\nThe vector $x_{\\text{part}} = v_1+v_2+v_3$ is in the subspace spanned by $\\{v_1, v_2, v_3\\}$, which is $(\\ker T)^\\perp = \\mathcal{R}(T^*)$. The vector $z = \\sum_{k=4}^{\\infty} \\alpha_k v_k$ is in $\\ker(T)$. These two subspaces are orthogonal. By the Pythagorean theorem in Hilbert spaces:\n$$\n\\|x\\|^2 = \\|v_1 + v_2 + v_3\\|^2 + \\left\\| \\sum_{k=4}^{\\infty} \\alpha_k v_k \\right\\|^2.\n$$\nUsing the orthonormality of $\\{v_k\\}$:\n$$\n\\|x\\|^2 = (|1|^2 + |1|^2 + |1|^2) + \\sum_{k=4}^{\\infty} |\\alpha_k|^2 = 3 + \\sum_{k=4}^{\\infty} |\\alpha_k|^2.\n$$\nTo minimize $\\|x\\|^2$, we must minimize the non-negative term $\\sum_{k=4}^{\\infty} |\\alpha_k|^2$. Its minimum value is $0$, which is achieved if and only if $\\alpha_k = 0$ for all $k \\geq 4$.\nThis choice corresponds to selecting the solution component in the nullspace to be the zero vector. The resulting minimal-norm solution is:\n$$\nx_{\\min} = v_1 + v_2 + v_3 + \\sum_{k=4}^{\\infty} 0 \\cdot v_k = v_1 + v_2 + v_3.\n$$\nThis solution is the unique solution that lies entirely in the orthogonal complement of the nullspace, $(\\ker T)^\\perp$.",
            "answer": "$$\n\\boxed{v_1 + v_2 + v_3}\n$$"
        },
        {
            "introduction": "The degree of ill-posedness in an inverse problem is directly related to how quickly the singular values $\\sigma_i$ decay. This practice explores a case of severe ill-posedness where singular values decay exponentially, causing the Picard condition to fail dramatically for solutions with smoothly decaying coefficients . Your task is to act as a problem-solver by deriving a rule for truncating the SVD expansion, ensuring that the error amplification due to noise remains under a specified tolerance, which is a core technique in regularization.",
            "id": "3419623",
            "problem": "Consider the separable Hilbert space $\\ell^{2}$ of square-summable real sequences, and a compact linear operator $A : \\ell^{2} \\to \\ell^{2}$ that admits a singular system $\\{(\\sigma_{i}, u_{i}, v_{i})\\}_{i \\geq 1}$ with orthonormal bases $\\{u_{i}\\}_{i \\geq 1}$ and $\\{v_{i}\\}_{i \\geq 1}$ such that the singular values are given by $\\sigma_{i} = \\exp(-\\alpha i)$ for a fixed $\\alpha  0$. Let the exact data $y \\in \\ell^{2}$ have expansion coefficients in the left singular basis that decay polynomially,\n$$\\langle y, u_{i} \\rangle = i^{-p}, \\quad i \\geq 1,$$\nfor a fixed $p  \\tfrac{1}{2}$, so that $y \\in \\ell^{2}$ is well-defined. You are given noisy data $y^{\\delta} = y + \\eta$ with $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$, where $\\delta  0$ is a known noise level. The truncated singular value decomposition (TSVD) estimator with truncation level $k \\in \\mathbb{N}$ is\n$$x_{k}^{\\delta} = \\sum_{i=1}^{k} \\frac{\\langle y^{\\delta}, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i} = \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle + \\langle \\eta, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}.$$\nFirst, justify from core definitions why the Picard condition fails for the exact data $y$ relative to the operator $A$ and therefore some form of regularization is necessary to stabilize inversion. Then, focusing on noise-induced error only, define the noise-free truncated estimate $x_{k}^{0} := \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}$ and consider the worst-case deviation $\\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}}$ over all admissible noises with $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$. For a prescribed reconstruction tolerance $E  0$, determine the minimal truncation level $k^{\\star}(\\delta)$ that maximizes information use (i.e., is the largest integer $k$ permitted) while guaranteeing\n$$\\sup_{\\|\\eta\\|_{\\ell^{2}} \\leq \\delta} \\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}} \\leq E.$$\nAssume the noise level satisfies $E/\\delta  \\exp(\\alpha)$ so that $k^{\\star}(\\delta) \\geq 1$ is nontrivial. Provide $k^{\\star}(\\delta)$ as a single closed-form analytic expression in terms of $\\alpha$, $\\delta$, and $E$. No numerical evaluation is required, and no units are involved.",
            "solution": "We begin by recalling two foundational elements for inverse problems with compact operators and singular systems. First, a compact operator $A : \\ell^{2} \\to \\ell^{2}$ admits a singular system $\\{(\\sigma_{i}, u_{i}, v_{i})\\}_{i \\geq 1}$ with singular values $\\sigma_{i} \\downarrow 0$ and orthonormal bases $\\{u_{i}\\}$ and $\\{v_{i}\\}$ such that for any $x \\in \\ell^{2}$,\n$$A x = \\sum_{i=1}^{\\infty} \\sigma_{i} \\langle x, v_{i} \\rangle \\, u_{i}.$$\nSecond, the Picard condition states that the data $y$ are compatible with a stable inverse if and only if\n$$\\sum_{i=1}^{\\infty} \\frac{|\\langle y, u_{i} \\rangle|^{2}}{\\sigma_{i}^{2}}  \\infty,$$\nwhich ensures that the formal inverse $x^{\\dagger} = \\sum_{i=1}^{\\infty} \\frac{\\langle y, u_{i} \\rangle}{\\sigma_{i}} v_{i}$ defines an element of $\\ell^{2}$.\n\nIn our teaching example, the singular values decay exponentially, $\\sigma_{i} = \\exp(-\\alpha i)$ with $\\alpha  0$, while the data coefficients decay polynomially, $\\langle y, u_{i} \\rangle = i^{-p}$ with $p  \\tfrac{1}{2}$. Then the Picard series becomes\n$$\\sum_{i=1}^{\\infty} \\frac{|\\langle y, u_{i} \\rangle|^{2}}{\\sigma_{i}^{2}} = \\sum_{i=1}^{\\infty} \\frac{i^{-2p}}{\\exp(-2\\alpha i)} = \\sum_{i=1}^{\\infty} i^{-2p} \\exp(2\\alpha i).$$\nBecause $\\exp(2\\alpha i)$ grows exponentially and $i^{-2p}$ decays only polynomially, the general term $i^{-2p} \\exp(2\\alpha i)$ does not tend to zero and the series diverges. Therefore, the Picard condition fails, meaning that direct inversion is unstable and regularization is necessary.\n\nWe now analyze the noise-induced error introduced by the truncated singular value decomposition (TSVD). The TSVD estimator with truncation $k$ is\n$$x_{k}^{\\delta} = \\sum_{i=1}^{k} \\frac{\\langle y^{\\delta}, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i} = \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle + \\langle \\eta, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i},$$\nand the corresponding noise-free truncated estimator is\n$$x_{k}^{0} = \\sum_{i=1}^{k} \\frac{\\langle y, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}.$$\nTheir difference isolates the contribution of noise:\n$$x_{k}^{\\delta} - x_{k}^{0} = \\sum_{i=1}^{k} \\frac{\\langle \\eta, u_{i} \\rangle}{\\sigma_{i}} \\, v_{i}.$$\nTaking the norm in $\\ell^{2}$ and using orthonormality of $\\{v_{i}\\}$ yields\n$$\\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}}^{2} = \\sum_{i=1}^{k} \\left|\\frac{\\langle \\eta, u_{i} \\rangle}{\\sigma_{i}}\\right|^{2} = \\sum_{i=1}^{k} |\\langle \\eta, u_{i} \\rangle|^{2} \\exp(2\\alpha i).$$\nWe seek the worst-case deviation over all $\\eta$ with $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$. Writing $a_{i} := |\\langle \\eta, u_{i} \\rangle|^{2}$, we have $a_{i} \\geq 0$ and $\\sum_{i=1}^{\\infty} a_{i} \\leq \\delta^{2}$. The quantity to maximize is a weighted sum\n$$\\sum_{i=1}^{k} a_{i} \\exp(2\\alpha i),$$\nsubject to the budget $\\sum_{i=1}^{\\infty} a_{i} \\leq \\delta^{2}$. Since the weights $\\exp(2\\alpha i)$ are increasing in $i$, the supremum is achieved by concentrating all the available budget on the largest weight within the truncated set, i.e., at index $i = k$. Therefore,\n$$\\sup_{\\|\\eta\\|_{\\ell^{2}} \\leq \\delta} \\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}}^{2} = \\delta^{2} \\exp(2\\alpha k), \\quad \\text{and hence} \\quad \\sup_{\\|\\eta\\|_{\\ell^{2}} \\leq \\delta} \\|x_{k}^{\\delta} - x_{k}^{0}\\|_{\\ell^{2}} = \\delta \\exp(\\alpha k).$$\n\nImposing the reconstruction tolerance $E  0$ requires\n$$\\delta \\exp(\\alpha k) \\leq E.$$\nSolving this inequality for $k$ gives\n$$\\exp(\\alpha k) \\leq \\frac{E}{\\delta} \\quad \\Longrightarrow \\quad \\alpha k \\leq \\ln\\!\\left(\\frac{E}{\\delta}\\right) \\quad \\Longrightarrow \\quad k \\leq \\frac{1}{\\alpha} \\ln\\!\\left(\\frac{E}{\\delta}\\right).$$\nTo maximize information use while meeting the bound (i.e., to choose the largest admissible truncation level), we take the integer part:\n$$k^{\\star}(\\delta) = \\left\\lfloor \\frac{1}{\\alpha} \\ln\\!\\left(\\frac{E}{\\delta}\\right) \\right\\rfloor.$$\nUnder the stated assumption $E/\\delta  \\exp(\\alpha)$, this yields $k^{\\star}(\\delta) \\geq 1$ and thus a nontrivial truncation.\n\nThis $k^{\\star}(\\delta)$ quantifies, in closed form, the minimal TSVD truncation level in the sense of the largest number of singular components that can be retained while ensuring the worst-case noise-induced error remains bounded by $E$ across all noise realizations satisfying $\\|\\eta\\|_{\\ell^{2}} \\leq \\delta$. It explicitly reflects the exponential decay of $\\sigma_{i}$ via the factor $\\alpha$ and captures how stricter noise levels $\\delta$ permit deeper truncations $k^{\\star}(\\delta)$.",
            "answer": "$$\\boxed{\\left\\lfloor \\frac{1}{\\alpha} \\ln\\!\\left(\\frac{E}{\\delta}\\right) \\right\\rfloor}$$"
        },
        {
            "introduction": "Theoretical conditions like the Picard condition must ultimately be tested in the world of discrete data and finite-precision arithmetic. This computational exercise bridges the gap between abstract theory and practical implementation by asking you to design a numerical test to determine if a data vector belongs to the range of an operator . You will discretize an integral operator, compute its singular value decomposition, and analyze the decay rate of the resulting Picard sequence, providing you with a tangible tool for diagnosing the nature of data in inverse problems.",
            "id": "3419606",
            "problem": "You are given a compact linear operator $T : L^2([0,1]) \\to L^2([0,1])$ defined by $(T f)(s) = \\int_0^1 k(s,t) f(t) \\, dt$ with kernel $k(s,t) = \\exp(-|s-t|)$. The operator $T$ admits a singular system $(\\sigma_n, v_n, u_n)$ with singular values $\\sigma_n  0$ in nonincreasing order and singular functions $v_n \\in L^2([0,1])$ and $u_n \\in L^2([0,1])$ such that $T v_n = \\sigma_n u_n$ and $T^\\ast u_n = \\sigma_n v_n$. In numerical computations, $T$ can be approximated by a quadrature-based matrix $A \\in \\mathbb{R}^{m \\times m}$ constructed on an equispaced grid of $m$ points by the composite midpoint rule:\n- Use grid nodes $s_i = (i - \\tfrac{1}{2})/m$ and $t_j = (j - \\tfrac{1}{2})/m$ for $i,j \\in \\{1,\\dots,m\\}$.\n- Define $A_{ij} = k(s_i,t_j) \\cdot (1/m)$ so that $A x \\approx y$ is a discrete approximation of $T f = y$.\n\nLet the singular value decomposition of $A$ be $A = U \\Sigma V^\\top$, where $\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_r)$ with $\\sigma_1 \\ge \\dots \\ge \\sigma_r  0$, $U = [u_1,\\dots,u_r]$, and $V = [v_1,\\dots,v_r]$. The classical Picard condition involves the tail behavior of the sequence $a_n = |\\langle y, u_n \\rangle| / \\sigma_n$ for a given $y \\in L^2([0,1])$. In practice, you can test whether $y \\in \\mathcal{R}(T)$ by examining the empirical tail behavior of $a_n$ computed from the discrete singular system of $A$.\n\nTask: Design and implement a numerical procedure that, for a given discrete right-hand side vector $y \\in \\mathbb{R}^m$, tests whether $y \\in \\mathcal{R}(T)$ by estimating the tail behavior of the sequence $a_n = |\\langle y, u_n \\rangle| / \\sigma_n$ using a slope-based tail-decay criterion on a log-log scale. Your procedure must:\n\n1. Construct the discrete operator $A$ for $m = 120$ using the kernel $k(s,t) = \\exp(-|s-t|)$ and composite midpoint rule as described above.\n2. Compute the singular value decomposition $A = U \\Sigma V^\\top$ in $\\mathbb{R}^{m \\times m}$ and retain only the effective singular triplets with $\\sigma_n \\ge \\tau \\, \\sigma_1$, where $\\tau = 10^{-12}$.\n3. For a nonzero $y$, compute $a_n = |\\langle y, u_n \\rangle| / \\sigma_n$ for the retained singular vectors. Let $r_{\\mathrm{eff}}$ be the number of retained singular values. Define a tail window consisting of the last $\\lceil \\rho \\, r_{\\mathrm{eff}} \\rceil$ indices with $\\rho = 0.3$ and estimate the slope $p$ of the least-squares linear fit of $\\log(a_n)$ versus $\\log(n)$ over this tail window, using natural logarithms. If $\\|y\\|_2 \\le \\varepsilon_0$ with $\\varepsilon_0 = 10^{-14}$, classify $y$ as belonging to $\\mathcal{R}(T)$ without further calculations.\n4. Decision rule: Classify $y$ as belonging to $\\mathcal{R}(T)$ if and only if the estimated slope $p$ is strictly less than the threshold $p_{\\mathrm{thr}} = -0.55$. Otherwise, classify $y$ as not belonging to $\\mathcal{R}(T)$. When computing logarithms, regularize zeros by replacing any zero $a_n$ with $a_n + \\delta$ for $\\delta = 10^{-300}$.\n\nImplement the above procedure and apply it to the following test suite of right-hand sides $y \\in \\mathbb{R}^m$ built using the same $A$ and grid:\n\n- Test case $1$ (clean range element): Let $x_{\\mathrm{true}}(t) = \\sin(\\pi t)$. Define $x \\in \\mathbb{R}^m$ by $x_j = x_{\\mathrm{true}}(t_j)$ and set $y^{(1)} = A x$.\n- Test case $2$ (noisy observation in $Y$): Using the same $x$ as above, let $y^{(2)} = A x + \\epsilon \\eta$, where $\\eta \\in \\mathbb{R}^m$ is a standard normal random vector scaled to unit Euclidean norm $\\|\\eta\\|_2 = 1$, with random seed fixed at $20240501$, and $\\epsilon = 10^{-4}$.\n- Test case $3$ (zero vector): $y^{(3)} = 0 \\in \\mathbb{R}^m$.\n- Test case $4$ (borderline synthetic coefficients): Using the retained singular vectors $U_{\\mathrm{eff}}$ and singular values $\\{\\sigma_n\\}_{n=1}^{r_{\\mathrm{eff}}}$, define coefficients $d_n = 1/\\sqrt{n}$ for $n = 1,\\dots,r_{\\mathrm{eff}}$ and set $y^{(4)} = \\sum_{n=1}^{r_{\\mathrm{eff}}} \\sigma_n d_n u_n$.\n\nYour program must output, in a single line, a comma-separated list enclosed in square brackets containing four boolean values, each indicating the classification for the corresponding test case in the order given above, where “True” denotes “classified as belonging to $\\mathcal{R}(T)$” and “False” denotes “classified as not belonging to $\\mathcal{R}(T)$”. For example, the output format must be exactly like `[True,False,True,False]` (with no additional spaces or text).\n\nThe final output must therefore be a single line containing a list of four boolean values as specified. No physical units are involved in this problem, and all angles (if any were to appear) should be treated implicitly as real numbers in radians. All numerical constants such as $m = 120$, $\\tau = 10^{-12}$, $\\rho = 0.3$, $\\varepsilon_0 = 10^{-14}$, $\\delta = 10^{-300}$, and $\\epsilon = 10^{-4}$ must be used exactly as given. The random number generator must be seeded exactly with the integer seed $20240501$ for reproducibility. The inner product $\\langle \\cdot, \\cdot \\rangle$ is the standard Euclidean inner product on $\\mathbb{R}^m$.",
            "solution": "The user has provided a well-defined computational problem in the field of inverse problems, specifically concerning the numerical verification of the Picard condition for a discretized integral operator. The problem is scientifically grounded, self-contained, and algorithmically specified. It is therefore deemed valid.\n\nThe task is to implement a numerical procedure to classify a given vector $y \\in \\mathbb{R}^m$ as belonging to the range of a compact operator $T$, denoted $\\mathcal{R}(T)$, based on a discretized version of the problem. The classification relies on the decay properties of the coefficients of $y$ projected onto the singular functions of the operator.\n\nThe solution is implemented by following the prescribed steps:\n1.  Discretization of the integral operator $T$.\n2.  Computation of the singular value decomposition (SVD) of the resulting matrix $A$.\n3.  Application of a decision rule based on the tail behavior of the discrete Picard sequence.\n\n### 1. Operator Discretization\nThe linear integral operator $T: L^2([0,1]) \\to L^2([0,1])$ is defined by\n$$\n(T f)(s) = \\int_0^1 k(s,t) f(t) \\, dt\n$$\nwith a symmetric kernel $k(s,t) = \\exp(-|s-t|)$. We discretize this operator using the composite midpoint rule on a uniform grid. The interval $[0,1]$ is divided into $m = 120$ subintervals of width $h = 1/m$. The grid nodes are the midpoints of these subintervals:\n$$\ns_i = \\frac{i - 1/2}{m}, \\quad t_j = \\frac{j - 1/2}{m} \\quad \\text{for } i,j \\in \\{1, \\dots, m\\}\n$$\nThe integral is approximated by a sum:\n$$\n(Tf)(s_i) \\approx \\sum_{j=1}^m k(s_i, t_j) f(t_j) \\frac{1}{m}\n$$\nThis leads to a linear system $y = Ax$, where $y_i \\approx (Tf)(s_i)$, $x_j = f(t_j)$, and the matrix $A \\in \\mathbb{R}^{m \\times m}$ is defined by\n$$\nA_{ij} = k(s_i, t_j) \\cdot \\frac{1}{m} = \\exp\\left(-\\left|\\frac{i - 1/2}{m} - \\frac{j - 1/2}{m}\\right|\\right) \\cdot \\frac{1}{m}\n$$\n\n### 2. Singular System and Filtering\nThe singular value decomposition (SVD) of the matrix $A$ is computed, yielding $A = U \\Sigma V^\\top$. Here, $U$ and $V$ are orthogonal matrices whose columns, $u_n$ and $v_n$ respectively, are the discrete left and right singular vectors. $\\Sigma$ is a diagonal matrix of singular values $\\sigma_n$, sorted in nonincreasing order.\n\nDue to the ill-posed nature of the original problem, the singular values of $A$ decay rapidly towards zero. To work with a numerically stable representation, we filter the singular system. We retain only the \"effective\" singular triplets $(\\sigma_n, u_n, v_n)$ for which the singular value $\\sigma_n$ is not excessively small compared to the largest singular value $\\sigma_1$. The condition for retention is given by:\n$$\n\\sigma_n \\ge \\tau \\sigma_1\n$$\nwhere the relative threshold is $\\tau = 10^{-12}$. Let $r_{\\mathrm{eff}}$ be the number of singular values satisfying this criterion. We denote the truncated SVD components as $U_{\\mathrm{eff}} \\in \\mathbb{R}^{m \\times r_{\\mathrm{eff}}}$, $\\Sigma_{\\mathrm{eff}} \\in \\mathbb{R}^{r_{\\mathrm{eff}} \\times r_{\\mathrm{eff}}}$, and $V_{\\mathrm{eff}} \\in \\mathbb{R}^{m \\times r_{\\mathrm{eff}}}$.\n\n### 3. Picard Condition Test\nThe classical Picard condition for the continuous problem $Tf=g$ requires that the coefficients of the solution, $|\\langle g, u_n \\rangle| / \\sigma_n$, form a square-summable sequence. In the discrete setting, we test whether a given vector $y \\in \\mathbb{R}^m$ lies in the range of $A$ (and by extension, approximately in $\\mathcal{R}(T)$) by analyzing the decay rate of the sequence\n$$\na_n = \\frac{|\\langle y, u_n \\rangle|}{\\sigma_n} \\quad \\text{for } n = 1, \\dots, r_{\\mathrm{eff}}\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ is the standard Euclidean inner product in $\\mathbb{R}^m$.\n\nThe numerical procedure is as follows:\n1.  **Zero Vector Check**: If $\\|y\\|_2 \\le \\varepsilon_0$ for a small tolerance $\\varepsilon_0 = 10^{-14}$, the vector is considered to be the zero vector, which is trivially in $\\mathcal{R}(T)$. The classification is `True`.\n\n2.  **Picard Sequence Calculation**: For a non-zero $y$, compute the coefficients $c_n = \\langle y, u_n \\rangle$ for $n = 1, \\dots, r_{\\mathrm{eff}}$. This is efficiently done via the matrix-vector product $c = U_{\\mathrm{eff}}^\\top y$. The sequence $a_n$ is then calculated as $a_n = |c_n| / \\sigma_n$.\n\n3.  **Tail-Decay Analysis**: The decay behavior is analyzed on a log-log scale. A vector $y$ that satisfies the Picard condition is expected to have $a_n$ that decay to zero. A vector with components outside $\\mathcal{R}(T)$ (e.g., due to noise) will have $a_n$ that stagnate or grow for small $\\sigma_n$. We quantify this decay by fitting a line to the tail of the sequence on a log-log plot.\n    - A tail window is defined, consisting of the last $\\lceil \\rho \\, r_{\\mathrm{eff}} \\rceil$ indices, where $\\rho = 0.3$. Let this size be $N_{\\mathrm{tail}}$.\n    - The indices for the fit are $n \\in \\{r_{\\mathrm{eff}} - N_{\\mathrm{tail}} + 1, \\dots, r_{\\mathrm{eff}}\\}$.\n    - We perform a linear least-squares fit of $\\log(a_n)$ versus $\\log(n)$ for $n$ in this tail window. To handle cases where $a_n=0$, any such value is replaced by $a_n + \\delta$ before taking the logarithm, with $\\delta = 10^{-300}$. The model is $\\log(a_n) \\approx p \\log(n) + q$.\n    - The estimated slope $p$ characterizes the decay rate. A large negative slope indicates rapid decay (satisfying the condition), while a slope near zero or positive indicates a violation.\n\n4.  **Decision Rule**: The vector $y$ is classified as belonging to $\\mathcal{R}(T)$ if and only if the estimated slope $p$ is strictly less than the threshold $p_{\\mathrm{thr}} = -0.55$.\n\n### 4. Evaluation on Test Cases\n\nThis procedure is applied to four test cases:\n- **Test Case 1 ($y^{(1)}$)**: $y^{(1)} = Ax$, where $x_j = \\sin(\\pi t_j)$. This vector is constructed to be in the range of $A$. The coefficients $a_n = |\\langle Ax, u_n \\rangle| / \\sigma_n = |\\sigma_n \\langle x, v_n \\rangle| / \\sigma_n = |\\langle x, v_n \\rangle|$ are the Fourier coefficients of the smooth vector $x$ with respect to the basis $\\{v_n\\}$. These coefficients are expected to decay very rapidly, leading to a large negative slope $p$ and a `True` classification.\n- **Test Case 2 ($y^{(2)}$)**: $y^{(2)} = Ax + \\epsilon \\eta$, with noise level $\\epsilon = 10^{-4}$ and $\\eta$ a normalized random vector. The noise term $\\epsilon \\eta$ is not aligned with the range of $A$ and excites all singular modes. The Picard sequence coefficients are $a_n = |\\sigma_n \\langle x, v_n \\rangle + \\epsilon \\langle \\eta, u_n \\rangle| / \\sigma_n$. For small $\\sigma_n$, the second term $\\epsilon |\\langle \\eta, u_n \\rangle| / \\sigma_n$ dominates and grows, violating the Picard condition. The slope $p$ is expected to be non-negative, resulting in a `False` classification.\n- **Test Case 3 ($y^{(3)}$)**: $y^{(3)} = 0$. The norm check $\\|y^{(3)}\\|_2 = 0 \\le \\varepsilon_0$ will be met, immediately yielding a `True` classification.\n- **Test Case 4 ($y^{(4)}$)**: $y^{(4)} = \\sum_{n=1}^{r_{\\mathrm{eff}}} \\sigma_n d_n u_n$ with $d_n = 1/\\sqrt{n}$. For this vector, $\\langle y^{(4)}, u_k \\rangle = \\sigma_k d_k$. The Picard sequence is $a_k = |\\sigma_k d_k| / \\sigma_k = d_k = k^{-0.5}$. On a log-log scale, $\\log(a_k) = -0.5 \\log(k)$. The theoretical slope is exactly $-0.5$. Since the decision rule is $p  -0.55$, this borderline case is expected to be classified as `False`.\n\nThe implementation will now proceed to execute this validated plan.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Implements and runs the numerical procedure for testing the Picard condition.\n    \"\"\"\n    # Define constants from the problem statement\n    m = 120\n    tau = 1e-12\n    rho = 0.3\n    epsilon_0 = 1e-14\n    delta = 1e-300\n    p_thr = -0.55\n    epsilon_noise = 1e-4\n    random_seed = 20240501\n\n    # Step 1: Construct the discrete operator A\n    # Create grid nodes\n    grid_points = (np.arange(1, m + 1) - 0.5) / m\n    s_grid, t_grid = np.meshgrid(grid_points, grid_points, indexing='ij')\n\n    # Define the kernel and matrix A\n    kernel = np.exp(-np.abs(s_grid - t_grid))\n    A = kernel * (1.0 / m)\n\n    # Step 2: Compute the SVD and filter the singular system\n    U, s, Vt = svd(A, full_matrices=True)\n    \n    sigma_1 = s[0]\n    r_eff = np.sum(s = tau * sigma_1)\n\n    U_eff = U[:, :r_eff]\n    s_eff = s[:r_eff]\n    V_eff = Vt.T[:, :r_eff]\n\n    def test_picard_condition(y, U_eff_p, s_eff_p):\n        \"\"\"\n        Applies the Picard condition test to a given vector y.\n        \"\"\"\n        # Zero vector check\n        if np.linalg.norm(y) = epsilon_0:\n            return True\n\n        # Step 3: Compute Picard sequence and perform tail-decay analysis\n        # Compute coefficients y, u_n\n        coeffs = U_eff_p.T @ y\n        \n        # Compute the Picard sequence a_n\n        a = np.abs(coeffs) / s_eff_p\n        \n        # Regularize zeros for logarithm\n        a[a == 0] = delta\n\n        # Define tail window\n        r_eff_p = len(s_eff_p)\n        tail_size = int(np.ceil(rho * r_eff_p))\n        \n        if tail_size = 1:\n            # Not enough points for a meaningful slope, assume decay\n            return True\n\n        # Select tail for log-log regression\n        tail_indices = np.arange(r_eff_p - tail_size, r_eff_p)\n        n_vals = tail_indices + 1\n        a_vals_tail = a[tail_indices]\n        \n        log_n = np.log(n_vals)\n        log_a = np.log(a_vals_tail)\n\n        # Perform least-squares fit: log(a) = p*log(n) + q\n        X = np.vstack([log_n, np.ones_like(log_n)]).T\n        p, _ = np.linalg.lstsq(X, log_a, rcond=None)[0]\n\n        # Step 4: Decision rule\n        return p  p_thr\n\n    # --- Generate and process test cases ---\n    results = []\n\n    # Test Case 1: Clean range element\n    x_true = np.sin(np.pi * grid_points)\n    y1 = A @ x_true\n    results.append(test_picard_condition(y1, U_eff, s_eff))\n\n    # Test Case 2: Noisy observation\n    rng = np.random.default_rng(random_seed)\n    eta = rng.standard_normal(m)\n    eta /= np.linalg.norm(eta)\n    y2 = y1 + epsilon_noise * eta\n    results.append(test_picard_condition(y2, U_eff, s_eff))\n\n    # Test Case 3: Zero vector\n    y3 = np.zeros(m)\n    results.append(test_picard_condition(y3, U_eff, s_eff))\n\n    # Test Case 4: Borderline synthetic coefficients\n    n_indices = np.arange(1, r_eff + 1)\n    d = 1.0 / np.sqrt(n_indices)\n    y4 = U_eff @ (np.diag(s_eff) @ d)\n    results.append(test_picard_condition(y4, U_eff, s_eff))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}