{
    "hands_on_practices": [
        {
            "introduction": "To develop an intuition for ill-posedness, it is invaluable to see it in action. This first practice explores intrinsic ill-conditioning by examining the Hilbert matrix, a famous example of a matrix that is notoriously difficult to work with numerically. By constructing these matrices and calculating their condition numbers, you will witness firsthand how ill-conditioning can grow exponentially with the size of the problem, rendering the matrix practically singular in finite-precision arithmetic .",
            "id": "3412219",
            "problem": "You are given the task of constructing a sequence of Hilbert matrices and using them to demonstrate a practical source of ill-posedness through the growth behavior of the two-norm condition number. The Hilbert matrix of order $n$, denoted $H_n$, is defined by the rule $$(H_n)_{ij} = \\frac{1}{i + j - 1}$$ for all indices $i,j \\in \\{1,2,\\dots,n\\}$. The two-norm condition number of a nonsingular matrix $A$, denoted $\\kappa_2(A)$, is defined as $$\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2 = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},$$ where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$, respectively. The singular values are defined as the square roots of the eigenvalues of $A^\\top A$. The Cauchy interlacing theorem for symmetric positive definite matrices implies monotonicity properties of singular values for nested principal submatrices, which will be useful for reasoning about how $\\kappa_2(H_n)$ evolves with $n$.\n\nYour program must implement the following:\n\n- Construct $H_n$ for each $n$ in the set $S = \\{2,3,4,8,12\\}$ using the definition above and floating-point arithmetic in the standard double-precision format specified by the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard.\n- Compute $\\kappa_2(H_n)$ for each $n \\in S$ using the definition $\\kappa_2(H_n) = \\sigma_{\\max}(H_n) / \\sigma_{\\min}(H_n)$, where singular values are obtained via the singular value decomposition of $H_n$.\n- Demonstrate superlinear growth by evaluating, for each consecutive pair $(n_i,n_{i+1})$ in the ordered set $S$, the boolean predicate $$\\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})}  \\frac{n_{i+1}}{n_i}.$$ Return a list of these boolean values in the same order of pairs $[(2,3),(3,4),(4,8),(8,12)]$.\n- Illustrate finite-precision ill-posedness by checking whether the smallest singular value $\\sigma_{\\min}(H_{12})$ is below the double-precision machine epsilon $\\epsilon_{\\mathrm{mach}}$ (that is, test whether $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$). Use the canonical value of machine epsilon provided by the runtime environment for $64$-bit floats.\n\nFundamental bases to be used explicitly in your reasoning and implementation include the definitions of the Hilbert matrix $H_n$, the two-norm condition number $\\kappa_2(\\cdot)$, the singular value decomposition, and the concept of machine epsilon for IEEE $754$ double precision arithmetic.\n\nTest Suite:\n- Compute and report $\\kappa_2(H_n)$ for $n \\in \\{2,3,4,8,12\\}$.\n- For $(n_i,n_{i+1}) \\in \\{(2,3),(3,4),(4,8),(8,12)\\}$, compute and report whether $$\\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})}  \\frac{n_{i+1}}{n_i}.$$\n- Compute and report whether $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The first element must be the list of floating-point values $[\\kappa_2(H_2),\\kappa_2(H_3),\\kappa_2(H_4),\\kappa_2(H_8),\\kappa_2(H_{12})]$. The second element must be the list of boolean values corresponding to the superlinearity predicates for consecutive pairs as specified above. The third element must be a single boolean corresponding to the predicate $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$. Precisely, the output must look like $$[\\,[\\kappa_2(H_2),\\kappa_2(H_3),\\kappa_2(H_4),\\kappa_2(H_8),\\kappa_2(H_{12})],\\,[b_1,b_2,b_3,b_4],\\,b_5\\,],$$ where each $b_i$ is a boolean computed as described. No units or angles are involved in this problem, and no percentages should be printed.",
            "solution": "The problem requires an investigation into the numerical ill-posedness of Hilbert matrices, which serve as a canonical example in numerical linear algebra for demonstrating challenges in solving linear systems. The investigation proceeds by computing the two-norm condition number, $\\kappa_2(H_n)$, for a sequence of increasing matrix orders $n$, and analyzing its growth rate and behavior in finite-precision arithmetic.\n\nThe core components of the problem are:\n1.  The Hilbert matrix of order $n$, $H_n$, with entries $(H_n)_{ij} = \\frac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, \\dots, n\\}$.\n2.  The two-norm condition number, $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the maximum and minimum singular values of matrix $A$.\n3.  The concept of machine epsilon, $\\epsilon_{\\mathrm{mach}}$, which for IEEE $754$ double-precision is the smallest number such that $1.0 + \\epsilon_{\\mathrm{mach}} \\neq 1.0$.\n\nThe methodology involves several computational and analytical steps, executed for matrix orders $n$ from the set $S = \\{2, 3, 4, 8, 12\\}$.\n\n**Step 1: Hilbert Matrix Construction**\nFor each order $n \\in S$, an $n \\times n$ Hilbert matrix $H_n$ is constructed. The element at row $i$ and column $j$ (using $1$-based indexing as in the definition) is given by the formula $(H_n)_{ij} = \\frac{1}{i+j-1}$. For implementation with $0$-based indexing, this becomes $(H_n)_{ij} = \\frac{1}{(i+1)+(j+1)-1} = \\frac{1}{i+j+1}$. All computations are performed using standard double-precision floating-point arithmetic.\n\n**Step 2: Condition Number Calculation via Singular Value Decomposition (SVD)**\nThe singular values of each matrix $H_n$ are computed using the Singular Value Decomposition (SVD). Since Hilbert matrices are symmetric and positive-definite, their singular values are identical to their eigenvalues. The SVD provides a vector of singular values, from which the largest, $\\sigma_{\\max}(H_n)$, and smallest, $\\sigma_{\\min}(H_n)$, are extracted. The two-norm condition number is then calculated as their ratio:\n$$ \\kappa_2(H_n) = \\frac{\\sigma_{\\max}(H_n)}{\\sigma_{\\min}(H_n)} $$\nThis process is repeated for each $n \\in S$. The rapid growth of $\\kappa_2(H_n)$ is expected. The problem mentions the Cauchy interlacing theorem, which provides theoretical support for this growth. Since $H_n$ is a leading principal submatrix of $H_{n+1}$, the theorem implies that the singular values of $H_n$ and $H_{n+1}$ interlace. Specifically, $\\sigma_{\\max}(H_{n+1}) \\ge \\sigma_{\\max}(H_n)$ and $\\sigma_{\\min}(H_{n+1}) \\le \\sigma_{\\min}(H_n)$. The combination of an increasing (or non-decreasing) largest singular value and a decreasing (or non-increasing) smallest singular value leads to a monotonically increasing condition number.\n\n**Step 3: Analysis of Superlinear Growth**\nThe problem requires demonstrating a growth rate faster than the growth rate of the matrix dimension. This is tested using the predicate:\n$$ \\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})}  \\frac{n_{i+1}}{n_i} $$\nThis inequality is evaluated for each consecutive pair of orders $(n_i, n_{i+1})$ in the sorted set $S$. The pairs are $(2,3)$, $(3,4)$, $(4,8)$, and $(8,12)$. A boolean result (`True` or `False`) is determined for each check.\n\n**Step 4: Finite-Precision Effects and Numerical Singularity**\nThe final step demonstrates a practical consequence of extreme ill-conditioning. A matrix is considered numerically singular if computations involving it are dominated by floating-point errors. This often occurs when its condition number is on the order of $1/\\epsilon_{\\mathrm{mach}}$. A direct way to diagnose this is to examine the magnitude of its smallest singular value. The problem asks to evaluate the predicate:\n$$ \\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}} $$\nHere, $\\sigma_{\\min}(H_{12})$ is the smallest singular value of the Hilbert matrix of order $12$, and $\\epsilon_{\\mathrm{mach}}$ is the machine epsilon for double-precision arithmetic, which is $2^{-52}$, approximately $2.2204 \\times 10^{-16}$. If this predicate is true, it means the smallest singular value has been rounded to a value so small that it is below the threshold of relative precision for the number format. This effectively renders the matrix singular from a computational standpoint, as inverting it would amplify noise by a factor so large ($ \\kappa_2  10^{16} $) that the result would be meaningless.\n\nThe implementation will consolidate the list of computed condition numbers, the list of boolean results from the superlinearity test, and the single boolean result from the machine epsilon test into a final nested list structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs Hilbert matrices, computes their condition numbers, and analyzes\n    their growth and finite-precision properties as specified in the problem.\n    \"\"\"\n    \n    # Define the set of matrix orders to be evaluated.\n    orders = [2, 3, 4, 8, 12]\n    \n    # Get the machine epsilon for double-precision floating-point numbers.\n    machine_epsilon = np.finfo(float).eps\n\n    cond_numbers = {}\n    singular_values_map = {}\n\n    # Step 1  2: Construct Hilbert matrix and compute condition number for each order.\n    for n in orders:\n        # Construct the Hilbert matrix H_n.\n        # Using 0-based indexing for implementation: (H_n)_ij = 1 / (i+j+1)\n        # where i,j are in {0, 1, ..., n-1}.\n        i = np.arange(1, n + 1).reshape(n, 1)\n        j = np.arange(1, n + 1).reshape(1, n)\n        H = 1.0 / (i + j - 1)\n\n        # Compute singular values using SVD. compute_uv=False is more efficient.\n        sv = np.linalg.svd(H, compute_uv=False)\n        singular_values_map[n] = sv\n        \n        # Calculate the 2-norm condition number.\n        sigma_max = sv[0]\n        sigma_min = sv[-1]\n        cond_numbers[n] = sigma_max / sigma_min\n\n    # Store the list of condition numbers in the required order.\n    k_values = [cond_numbers[n] for n in orders]\n\n    # Step 3: Analyze superlinear growth.\n    superlinearity_checks = []\n    for i in range(len(orders) - 1):\n        n_i = orders[i]\n        n_i_plus_1 = orders[i+1]\n        \n        # Ratio of condition numbers\n        cond_ratio = cond_numbers[n_i_plus_1] / cond_numbers[n_i]\n        \n        # Ratio of dimensions\n        dim_ratio = n_i_plus_1 / n_i\n        \n        # Evaluate the predicate and store the boolean result.\n        superlinearity_checks.append(cond_ratio  dim_ratio)\n\n    # Step 4: Check for finite-precision ill-posedness.\n    # Retrieve the smallest singular value for H_12.\n    sigma_min_h12 = singular_values_map[12][-1]\n    \n    # Evaluate the predicate against machine epsilon.\n    epsilon_check = sigma_min_h12  machine_epsilon\n    \n    # Assemble the final result in the specified format.\n    # To match the no-space-after-comma format, we build the string manually.\n    s1 = '[' + ','.join(map(str, k_values)) + ']'\n    s2 = '[' + ','.join(map(str, superlinearity_checks)) + ']'\n    s3 = str(epsilon_check)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{s1},{s2},{s3}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding that a problem is ill-posed is the first step; the next is learning how to solve it. This exercise tackles a classic ill-posed inverse problem: deblurring a noisy signal. You will implement and compare two powerful regularization techniques, Tikhonov and Total Variation, which stabilize the inversion by incorporating prior knowledge about the solution. This practice highlights the trade-offs involved in regularization, allowing you to quantitatively analyze characteristic artifacts like oversmoothing and staircasing and understand how the choice of regularizer shapes the final result .",
            "id": "3412170",
            "problem": "Consider a one-dimensional discrete inverse problem in which an unknown piecewise constant signal $x_{\\mathrm{true}} \\in \\mathbb{R}^N$ is observed through a circular convolution blur and additive noise. The forward model is $y = K x_{\\mathrm{true}} + \\varepsilon$, where $K$ is the linear operator corresponding to circular convolution with a nonnegative, unit-sum discrete Gaussian kernel $h \\in \\mathbb{R}^N$, and $\\varepsilon$ is independent and identically distributed Gaussian noise with zero mean and known variance. The blur operator $K$ is compact on $\\mathbb{R}^N$ when $N$ is large and its Fourier magnitudes decay for higher frequencies. This decay produces an ill-posed inversion because high-frequency components of $x_{\\mathrm{true}}$ are strongly attenuated, and noise is amplified when attempting deconvolution. You will compare two regularization strategies and quantify the artifacts known as staircasing and oversmoothing.\n\nThe unknown signal $x_{\\mathrm{true}}$ is piecewise constant with three change points. Fix $N = 256$ and define $x_{\\mathrm{true}}[i]$ for $i = 0,1,\\dots,N-1$ by\n$$\nx_{\\mathrm{true}}[i] =\n\\begin{cases}\n0.0,  0 \\le i \\le 63, \\\\\n1.0,  64 \\le i \\le 127, \\\\\n0.5,  128 \\le i \\le 191, \\\\\n1.5,  192 \\le i \\le 255,\n\\end{cases}\n$$\nwith true change index set $C = \\{63,127,191\\}$. The forward blur $K$ is defined by circular convolution with a discrete Gaussian kernel $h$ of the form $h[j] \\propto \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{j'}{\\sigma}\\right)^2\\right)$ for $j = 0,1,\\dots,N-1$, where $j' = \\min(j, N-j)$ and $\\sigma$ is a standard deviation parameter in samples, normalized so that $\\sum_{j=0}^{N-1} h[j] = 1$.\n\nYou will reconstruct $x$ from $y$ by solving the following two optimization problems:\n\n1. Tikhonov regularization with identity penalty:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\lambda_{\\mathrm{T}}}{2}\\|x\\|_2^2,\n$$\nwhere $\\lambda_{\\mathrm{T}} > 0$.\n\n2. Total variation regularization with discrete gradient:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|D x\\|_1,\n$$\nwhere $\\lambda_{\\mathrm{TV}} > 0$, and $D: \\mathbb{R}^N \\to \\mathbb{R}^N$ is the circular forward difference operator $(D x)[i] = x[(i+1) \\bmod N] - x[i]$. You may solve the total variation problem using any convergent and correct method; for example, a splitting method such as the Alternating Direction Method of Multipliers (ADMM) with variable splitting $z = D x$.\n\nDefine the following quantitative artifact measures for any reconstructed signal $\\hat{x} \\in \\mathbb{R}^N$:\n\n- Mean squared error:\n$$\n\\mathrm{MSE}(\\hat{x}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\hat{x}[i] - x_{\\mathrm{true}}[i] \\right)^2.\n$$\n\n- False jump count relative to the true change index set $C$, with threshold $\\tau$:\n$$\nJ_{\\mathrm{false}}(\\hat{x}; \\tau) = \\#\\left\\{ i \\in \\{0,\\dots,N-2\\} \\setminus C \\,:\\, \\left| \\hat{x}[i+1] - \\hat{x}[i] \\right| \\ge \\tau \\right\\}.\n$$\n\n- Mean edge attenuation deficit (oversmoothing artifact), defined at the true change indices:\n$$\n\\mathrm{AD}(\\hat{x}) = \\frac{1}{|C|} \\sum_{t \\in C} \\max\\!\\left( 0, 1 - \\frac{ \\left| \\hat{x}[t+1] - \\hat{x}[t] \\right| }{ \\left| x_{\\mathrm{true}}[t+1] - x_{\\mathrm{true}}[t] \\right| } \\right) \\right).\n$$\nThis quantity lies in $[0,1]$ and is larger when edges are more attenuated.\n\nSet the jump detection threshold $\\tau$ based on the true jumps as $\\tau = \\alpha \\cdot A_{\\mathrm{mean}}$, where\n$$\nA_{\\mathrm{mean}} = \\frac{1}{|C|} \\sum_{t \\in C} \\left| x_{\\mathrm{true}}[t+1] - x_{\\mathrm{true}}[t] \\right|,\n$$\nand fix $\\alpha = 0.25$.\n\nImplement a program that, for each test case in the suite below, constructs $h$, simulates data $y$, computes reconstructions $\\hat{x}_{\\mathrm{T}}$ and $\\hat{x}_{\\mathrm{TV}}$, and evaluates the metrics $\\mathrm{MSE}$, $J_{\\mathrm{false}}$, and $\\mathrm{AD}$ for each method.\n\nTest suite parameter sets are tuples $(\\sigma, \\sigma_{\\varepsilon}, \\lambda_{\\mathrm{T}}, \\lambda_{\\mathrm{TV}}, \\rho, I, \\text{seed})$, where $\\sigma$ is the blur standard deviation in samples, $\\sigma_{\\varepsilon}$ is the noise standard deviation, $\\lambda_{\\mathrm{T}}$ and $\\lambda_{\\mathrm{TV}}$ are regularization strengths, $\\rho$ is the augmented Lagrangian penalty parameter for Alternating Direction Method of Multipliers (ADMM), $I$ is the number of ADMM iterations, and $\\mathrm{seed}$ is the pseudorandom number generator seed for reproducibility. Use the following test suite:\n\n- Case A (moderate blur and noise): $(2.0, 0.01, 0.01, 0.10, 1.0, 200, 0)$.\n- Case B (higher noise): $(2.0, 0.05, 0.05, 0.20, 1.0, 300, 1)$.\n- Case C (strong blur): $(5.0, 0.01, 0.02, 0.15, 1.5, 300, 2)$.\n- Case D (low noise but strong regularization): $(2.0, 0.005, 0.50, 0.80, 1.0, 300, 3)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of six numbers in the order $[\\mathrm{MSE}(\\hat{x}_{\\mathrm{T}}), \\mathrm{MSE}(\\hat{x}_{\\mathrm{TV}}), J_{\\mathrm{false}}(\\hat{x}_{\\mathrm{T}};\\tau), J_{\\mathrm{false}}(\\hat{x}_{\\mathrm{TV}};\\tau), \\mathrm{AD}(\\hat{x}_{\\mathrm{T}}), \\mathrm{AD}(\\hat{x}_{\\mathrm{TV}})]$. For example, the final output format must be of the form\n$$\n\\left[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}, r_{1,5}, r_{1,6}], [r_{2,1}, \\dots, r_{2,6}], [r_{3,1}, \\dots, r_{3,6}], [r_{4,1}, \\dots, r_{4,6}] \\right],\n$$\nwhere each $r_{k,\\ell}$ is a real number or an integer. No other text should be printed.",
            "solution": "The solution is implemented by first setting up the one-dimensional deconvolution problem. This involves defining the true piecewise constant signal $x_{\\mathrm{true}}$, the Gaussian blur kernel $h$, the circular convolution operator $K$, and simulating the noisy measurement data $y$. Subsequently, two distinct regularized inversion algorithms are applied to reconstruct the signal from $y$. Finally, quantitative metrics are computed to assess the quality of each reconstruction and to highlight the characteristic artifacts associated with each regularization method.\n\nFirst, the ground truth signal $x_{\\mathrm{true}} \\in \\mathbb{R}^N$ with $N=256$ is generated as a piecewise constant vector with four distinct levels and three change points at indices $C = \\{63, 127, 191\\}$. The blurring operator $K$ corresponds to a circular convolution. The convolution kernel $h$ is a discrete Gaussian, constructed to be symmetric and normalized to sum to one. The convolution operation $Kx$ is efficiently computed in the Fourier domain using the Fast Fourier Transform (FFT), where it becomes an element-wise product: $(Kx)_{\\text{FFT}} = \\text{FFT}(h) \\odot \\text{FFT}(x)$. The observed data $y$ are then simulated by computing $y = Kx_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon$ is a vector of independent and identically distributed Gaussian noise samples with mean $0$ and a specified standard deviation $\\sigma_{\\varepsilon}$. The random seed is fixed for reproducibility.\n\nThe first reconstruction method is Tikhonov regularization. The objective is to find $\\hat{x}_{\\mathrm{T}}$ that solves:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\lambda_{\\mathrm{T}}}{2}\\|x\\|_2^2\n$$\nThis is a quadratic optimization problem with a closed-form solution derived from the normal equations $(K^T K + \\lambda_{\\mathrm{T}} I) x = K^T y$. Since $K$ is a circulant matrix representing circular convolution, it is diagonalized by the FFT. The operator $K^T K + \\lambda_{\\mathrm{T}} I$ is also circulant and thus easily inverted in the Fourier domain. Let $H = \\text{FFT}(h)$, $Y = \\text{FFT}(y)$, and $\\hat{X}_{\\mathrm{T}} = \\text{FFT}(\\hat{x}_{\\mathrm{T}})$. The solution in the Fourier domain is given by:\n$$\n\\hat{X}_{\\mathrm{T}} = \\frac{\\overline{H} \\odot Y}{|H|^2 + \\lambda_{\\mathrm{T}}}\n$$\nwhere $\\overline{H}$ is the complex conjugate of $H$, $|H|^2$ is the element-wise squared magnitude, and the division is element-wise. The reconstructed signal $\\hat{x}_{\\mathrm{T}}$ is then obtained by applying the inverse FFT. This method is known for producing smooth reconstructions, which can lead to the oversmoothing of sharp edges.\n\nThe second reconstruction method is Total Variation (TV) regularization. The objective is to find $\\hat{x}_{\\mathrm{TV}}$ that solves:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|D x\\|_1\n$$\nwhere $D$ is the circular forward difference operator, $(Dx)[i] = x[(i+1) \\bmod N] - x[i]$, and $\\|\\cdot\\|_1$ is the $\\ell_1$-norm. The non-differentiable $\\ell_1$-norm term promotes sparsity in the gradient of $x$, which favors piecewise constant solutions and preserves sharp edges. This problem is solved using the Alternating Direction Method of Multipliers (ADMM) with the variable splitting $z = Dx$. The augmented Lagrangian is:\n$$\nL_{\\rho}(x, z, u) = \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|z\\|_1 + \\frac{\\rho}{2}\\|Dx - z + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\nwhere $u$ is the scaled dual variable and $\\rho$ is the penalty parameter. The ADMM algorithm iteratively updates $x$, $z$, and $u$:\n1.  **$x$-update**: $\\min_x \\left( \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\rho}{2}\\|Dx - z^k + u^k\\|_2^2 \\right)$. This is a quadratic problem whose solution is efficiently found in the Fourier domain, analogous to the Tikhonov case. The operators $K$ and $D$ are both circulant, so let $\\hat{X}^{k+1} = \\text{FFT}(x^{k+1})$, $Z^k = \\text{FFT}(z^k)$, and $U^k = \\text{FFT}(u^k)$. The solution is:\n    $$\n    \\hat{X}^{k+1} = \\frac{\\overline{H} \\odot Y + \\rho \\overline{\\Delta} \\odot (Z^k - U^k)}{|H|^2 + \\rho |\\Delta|^2}\n    $$\n    where $\\Delta = \\text{FFT}(d)$ with $d$ being the kernel of the operator $D$.\n2.  **$z$-update**: $\\min_z \\left( \\lambda_{\\mathrm{TV}}\\|z\\|_1 + \\frac{\\rho}{2}\\|Dx^{k+1} - z + u^k\\|_2^2 \\right)$. This has a closed-form solution given by the element-wise soft-thresholding operator:\n    $$\n    z^{k+1} = \\mathcal{S}_{\\lambda_{\\mathrm{TV}}/\\rho}(Dx^{k+1} + u^k)\n    $$\n    where $\\mathcal{S}_{\\tau}(v)_i = \\mathrm{sign}(v_i) \\max(|v_i| - \\tau, 0)$.\n3.  **$u$-update**: $u^{k+1} = u^k + Dx^{k+1} - z^{k+1}$.\nThese steps are repeated for a fixed number of iterations $I$. TV regularization is known for preserving edges but can introduce piecewise constant artifacts, known as staircasing, in smoothly varying regions.\n\nFinally, three metrics are evaluated for both reconstructions, $\\hat{x}_{\\mathrm{T}}$ and $\\hat{x}_{\\mathrm{TV}}$:\n1.  **Mean Squared Error (MSE)**: $\\mathrm{MSE}(\\hat{x}) = \\frac{1}{N}\\|\\hat{x} - x_{\\mathrm{true}}\\|_2^2$, a global measure of reconstruction fidelity.\n2.  **False Jump Count ($J_{\\mathrm{false}}$)**: This metric quantifies staircasing by counting the number of significant jumps in the reconstructed signal at locations other than the true change points. A jump at index $i$ is considered significant if its magnitude $|\\hat{x}[i+1] - \\hat{x}[i]|$ exceeds a threshold $\\tau=0.25 \\cdot A_{\\mathrm{mean}}$, where $A_{\\mathrm{mean}}$ is the average magnitude of the true jumps in $x_{\\mathrm{true}}$.\n3.  **Mean Edge Attenuation Deficit ($\\mathrm{AD}$)**: This metric quantifies oversmoothing by measuring the extent to which true edges are attenuated in the reconstruction. It is calculated as the average deficit $1 - |\\Delta \\hat{x}|/|\\Delta x_{\\mathrm{true}}|$ at the true change points, clipped at $0$. A higher $\\mathrm{AD}$ value (closer to $1$) indicates more severe oversmoothing of true edges.\n\nThe entire process is executed for each parameter set in the provided test suite, and the six resulting metric values for each case are collected and formatted into the required output string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the deconvolution comparison experiment.\n    \"\"\"\n    N = 256\n    C = [63, 127, 191]\n\n    # Test suite parameter sets:\n    # (sigma, sigma_eps, lambda_T, lambda_TV, rho, I, seed)\n    test_cases = [\n        (2.0, 0.01, 0.01, 0.10, 1.0, 200, 0),    # Case A\n        (2.0, 0.05, 0.05, 0.20, 1.0, 300, 1),    # Case B\n        (5.0, 0.01, 0.02, 0.15, 1.5, 300, 2),    # Case C\n        (2.0, 0.005, 0.50, 0.80, 1.0, 300, 3)     # Case D\n    ]\n\n    # --- Helper Functions ---\n\n    def create_true_signal(n_samples):\n        x_true = np.zeros(n_samples)\n        x_true[0:64] = 0.0\n        x_true[64:128] = 1.0\n        x_true[128:192] = 0.5\n        x_true[192:256] = 1.5\n        return x_true\n\n    def create_gaussian_kernel(sigma, n_samples):\n        j = np.arange(n_samples)\n        j_centered = np.minimum(j, n_samples - j)\n        h = np.exp(-0.5 * (j_centered / sigma)**2)\n        return h / np.sum(h)\n\n    def solve_tikhonov(y, h, lambda_t):\n        H = np.fft.fft(h)\n        Y = np.fft.fft(y)\n        X_hat_fft = (np.conj(H) * Y) / (np.abs(H)**2 + lambda_t)\n        return np.real(np.fft.ifft(X_hat_fft))\n\n    def soft_threshold(v, kappa):\n        return np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n\n    def solve_tv_admm(y, h, lambda_tv, rho, iterations):\n        n = len(y)\n        # Initialize variables\n        x = np.zeros(n)\n        z = np.zeros(n)\n        u = np.zeros(n)\n\n        # Pre-compute FFTs\n        H = np.fft.fft(h)\n        Y = np.fft.fft(y)\n        \n        # FFT of forward difference operator D\n        d_kernel = np.zeros(n)\n        d_kernel[0] = -1\n        d_kernel[1] = 1\n        Delta = np.fft.fft(d_kernel)\n\n        H_conj = np.conj(H)\n        Delta_conj = np.conj(Delta)\n        H_sq_abs = np.abs(H)**2\n        Delta_sq_abs = np.abs(Delta)**2\n\n        # Pre-compute denominator for x-update\n        x_update_denom = H_sq_abs + rho * Delta_sq_abs\n        \n        # ADMM loop\n        for _ in range(iterations):\n            # x-update\n            Z = np.fft.fft(z)\n            U = np.fft.fft(u)\n            x_update_num = H_conj * Y + rho * Delta_conj * (Z - U)\n            X = x_update_num / x_update_denom\n            x = np.real(np.fft.ifft(X))\n            \n            # z-update\n            Dx = np.roll(x, -1) - x\n            v = Dx + u\n            z = soft_threshold(v, lambda_tv / rho)\n            \n            # u-update\n            u = u + Dx - z\n\n        return x\n\n    def calculate_metrics(x_hat, x_true, change_indices, tau):\n        n_samples = len(x_true)\n        # 1. MSE\n        mse = np.mean((x_hat - x_true)**2)\n\n        # 2. False Jump Count\n        diff_x_hat = np.abs(x_hat[1:] - x_hat[:-1])\n        all_indices = set(range(n_samples - 1))\n        true_jump_indices = set(change_indices)\n        false_jump_check_indices = sorted(list(all_indices - true_jump_indices))\n        \n        j_false = 0\n        for i in false_jump_check_indices:\n            if diff_x_hat[i] = tau:\n                j_false += 1\n\n        # 3. Attenuation Deficit\n        ad_sum = 0\n        for t in change_indices:\n            true_jump_mag = np.abs(x_true[t + 1] - x_true[t])\n            if true_jump_mag  1e-9: # Avoid division by zero\n                recon_jump_mag = np.abs(x_hat[t + 1] - x_hat[t])\n                ratio = recon_jump_mag / true_jump_mag\n                ad_sum += np.maximum(0, 1 - ratio)\n        ad = ad_sum / len(change_indices)\n        \n        return mse, j_false, ad\n\n    # --- Main Loop ---\n    \n    x_true = create_true_signal(N)\n\n    # Calculate jump detection threshold tau\n    true_jump_mags = np.abs([x_true[t+1] - x_true[t] for t in C])\n    A_mean = np.mean(true_jump_mags)\n    alpha = 0.25\n    tau = alpha * A_mean\n\n    all_results = []\n    \n    for params in test_cases:\n        sigma, sigma_eps, lambda_T, lambda_TV, rho, I, seed = params\n        \n        np.random.seed(seed)\n        \n        # 1. Setup problem\n        h = create_gaussian_kernel(sigma, N)\n        Kx_true = np.real(np.fft.ifft(np.fft.fft(h) * np.fft.fft(x_true)))\n        epsilon = np.random.normal(0, sigma_eps, N)\n        y = Kx_true + epsilon\n        \n        # 2. Solve for reconstructions\n        x_hat_T = solve_tikhonov(y, h, lambda_T)\n        x_hat_TV = solve_tv_admm(y, h, lambda_TV, rho, I)\n        \n        # 3. Calculate metrics\n        mse_T, j_false_T, ad_T = calculate_metrics(x_hat_T, x_true, C, tau)\n        mse_TV, j_false_TV, ad_TV = calculate_metrics(x_hat_TV, x_true, C, tau)\n        \n        case_results = [mse_T, mse_TV, j_false_T, j_false_TV, ad_T, ad_TV]\n        all_results.append(case_results)\n        \n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Many real-world inverse problems are nonlinear, requiring iterative optimization methods. This practice delves into how ill-posedness manifests in such scenarios, often causing simple algorithms like the Gauss-Newton method to fail or diverge. By implementing and testing this method against a more robust, regularized approach—the Levenberg-Marquardt algorithm—you will see how globalization strategies handle issues like rank-deficient Jacobians and non-convexity, ensuring stable and reliable convergence to a solution .",
            "id": "3412212",
            "problem": "Consider the nonlinear least-squares objective defined by a residual mapping $r: \\mathbb{R}^n \\to \\mathbb{R}^m$, the measurement vector $y \\in \\mathbb{R}^m$, and the model prediction $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, with residual $r(x) = f(x) - y$. The objective is $\\phi(x) = \\tfrac{1}{2}\\|r(x)\\|_2^2$. Starting from the fundamental definition of the gradient of a composition and the first-order Taylor expansion of $r(x)$, derive a practical iterative method that updates $x$ by solving, at each iteration, a linearized subproblem that uses the Jacobian matrix $J(x) = \\nabla r(x)$ and aims to reduce $\\phi(x)$. This construction should be justified from first principles without relying on any pre-stated algorithmic formula. Then, analyze why this iteration can fail or diverge when the true Hessian of $\\phi(x)$ is indefinite near a saddle point and when the linearized subproblem is ill-posed due to rank-deficiency or near-singularity of $J(x)$, thereby motivating globalization strategies based on trust-region damping.\n\nYour program must implement the following, derived from your analysis:\n\n- A plain Gauss–Newton iteration (no line search, no damping), obtained solely from the first-order linearization of $r(x)$ and the least-squares principle for the linearized subproblem.\n- A Levenberg–Marquardt-style globalization strategy, in which a nonnegative damping parameter $\\mu$ augments the linearized subproblem to improve robustness when $J(x)$ is rank-deficient or when the second-order terms make the true Hessian of $\\phi(x)$ indefinite near a saddle. The strategy must include a step acceptance criterion based on decrease of $\\phi(x)$ and an update rule for $\\mu$ that increases $\\mu$ when a trial step fails to decrease $\\phi(x)$ and decreases $\\mu$ when a step succeeds.\n\nThe program must evaluate both methods on the following test suite. In all tests, use $n=m=2$ unless otherwise specified, and denote $x = (x_1,x_2)^\\top$. There are no physical units involved.\n\n- Test $1$ (happy path, linear full rank):\n  - Mapping $f(x) = A x$ with\n    $$A = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}.$$\n  - Measurement $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n  - Initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n  - Maximum iterations $N = 5$.\n  - This case is well-posed and should converge quickly when the iteration is constructed correctly from first principles.\n\n- Test $2$ (nonconvex residual, indefinite true Hessian near a saddle, classic failure mode for undamped Gauss–Newton):\n  - Residual $r(x) = \\begin{pmatrix} \\sqrt{10}\\,(x_2 - x_1^2) \\\\ 1 - x_1 \\end{pmatrix}$.\n  - Set $f(x) = r(x)$ and $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ so that $\\phi(x) = \\tfrac{1}{2}\\|r(x)\\|_2^2$ is the Rosenbrock least-squares objective.\n  - Initial guess $x_0 = \\begin{pmatrix} -1.2 \\\\ 1.0 \\end{pmatrix}$.\n  - Maximum iterations $N = 20$.\n  - The undamped iteration is expected to exhibit failure (non-monotonic increase of $\\phi(x)$ or divergence), while the damped strategy should produce robust decrease.\n\n- Test $3$ (rank-deficient Jacobian, non-identifiability, ill-posed normal equations):\n  - Mapping\n    $$f(x) = \\begin{pmatrix} x_1 + x_2 \\\\ x_1 + x_2 \\end{pmatrix}.$$\n  - Measurement $y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n  - Initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n  - Maximum iterations $N = 10$.\n  - The set of minimizers satisfies $x_1 + x_2 = 1$, so the problem is underdetermined. The undamped iteration may fail because $J(x)^\\top J(x)$ is singular, whereas the damped strategy (trust-region augmentation) should stabilize the step computation and drive $\\phi(x)$ down.\n\nAlgorithmic specifications your program must satisfy:\n\n- Implement both methods for each test:\n  - Plain Gauss–Newton: At iteration $k$, construct the linearized subproblem based on $r(x_k)$ and $J(x_k)$ and compute an update direction purely from the least-squares principle applied to the linearization. Take the full step $x_{k+1} = x_k + \\Delta x_k$ without line search or damping.\n  - Damped Gauss–Newton (Levenberg–Marquardt): Augment the linearized subproblem with a damping parameter $\\mu_k \\ge 0$ and enforce a step acceptance criterion based on actual decrease of $\\phi(x)$. If a trial step fails to decrease $\\phi(x)$, increase $\\mu_k$ (trust-region contraction) and retry; if it succeeds, decrease $\\mu_k$ (trust-region expansion).\n- Use a convergence check that declares success if either $\\phi(x)$ is below a tolerance $10^{-12}$ or the norm of $J(x)^\\top r(x)$ is below $10^{-8}$, and declare failure otherwise.\n- Detect and handle singular or near-singular linear systems in the undamped method by recording the failure for that test.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test $i \\in \\{1,2,3\\}$, append the list\n$$[\\phi_{\\mathrm{GN},i}, \\ \\phi_{\\mathrm{LM},i}, \\ \\mathrm{conv}_{\\mathrm{GN},i}, \\ \\mathrm{conv}_{\\mathrm{LM},i}, \\ \\mathrm{mono}_{\\mathrm{GN},i}]$$\nwhere $\\phi_{\\mathrm{GN},i}$ and $\\phi_{\\mathrm{LM},i}$ are the final objective values (floats) after running the undamped and damped iterations respectively (with at most the specified $N$ iterations), $\\mathrm{conv}_{\\mathrm{GN},i}$ and $\\mathrm{conv}_{\\mathrm{LM},i}$ are booleans indicating convergence by the above criterion, and $\\mathrm{mono}_{\\mathrm{GN},i}$ is a boolean indicating whether the undamped method produced a monotonically nonincreasing sequence of $\\phi(x)$ over its iterations. The final output must be the list of the three test-result lists in order, for example\n$$[[\\phi_{\\mathrm{GN},1},\\phi_{\\mathrm{LM},1},\\mathrm{conv}_{\\mathrm{GN},1},\\mathrm{conv}_{\\mathrm{LM},1},\\mathrm{mono}_{\\mathrm{GN},1}], [\\phi_{\\mathrm{GN},2},\\phi_{\\mathrm{LM},2},\\mathrm{conv}_{\\mathrm{GN},2},\\mathrm{conv}_{\\mathrm{LM},2},\\mathrm{mono}_{\\mathrm{GN},2}], [\\phi_{\\mathrm{GN},3},\\phi_{\\mathrm{LM},3},\\mathrm{conv}_{\\mathrm{GN},3},\\mathrm{conv}_{\\mathrm{LM},3},\\mathrm{mono}_{\\mathrm{GN},3}]].$$",
            "solution": "The problem asks for the derivation, analysis, and implementation of iterative methods for nonlinear least-squares optimization. We begin by establishing the theoretical foundation from first principles.\n\nLet the nonlinear least-squares problem be defined by the objective function $\\phi: \\mathbb{R}^n \\to \\mathbb{R}$,\n$$ \\phi(x) = \\frac{1}{2} \\|f(x) - y\\|_2^2 = \\frac{1}{2} \\|r(x)\\|_2^2 = \\frac{1}{2} r(x)^T r(x) $$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ is a nonlinear model, $y \\in \\mathbb{R}^m$ is a vector of measurements, and $r(x) = f(x) - y$ is the residual vector. Our goal is to find a parameter vector $x \\in \\mathbb{R}^n$ that minimizes $\\phi(x)$.\n\nThe gradient of the objective function $\\phi(x)$ is found using the chain rule. The components of $\\phi(x)$ are $\\phi(x) = \\frac{1}{2} \\sum_{i=1}^m r_i(x)^2$. The gradient is a vector whose $j$-th component is\n$$ (\\nabla \\phi(x))_j = \\frac{\\partial \\phi}{\\partial x_j} = \\sum_{i=1}^m r_i(x) \\frac{\\partial r_i}{\\partial x_j}. $$\nThis can be expressed in matrix form. Let $J(x)$ be the Jacobian matrix of the residual vector $r(x)$, where its entries are $(J(x))_{ij} = \\frac{\\partial r_i}{\\partial x_j}$. The gradient is then\n$$ \\nabla \\phi(x) = J(x)^T r(x). $$\nThe first-order necessary condition for a minimum is $\\nabla \\phi(x) = 0$, which implies $J(x)^T r(x) = 0$. This is a system of nonlinear equations.\n\nTo develop an iterative method, we seek a sequence of points $x_0, x_1, \\dots, x_k, \\dots$ that converges to a minimizer. Given a current iterate $x_k$, we wish to find a step $\\Delta x$ such that $x_{k+1} = x_k + \\Delta x$ reduces the objective, i.e., $\\phi(x_{k+1})  \\phi(x_k)$.\n\nThe core idea of the Gauss–Newton method is to linearize the residual function $r(x)$ around the current iterate $x_k$. The first-order Taylor expansion of $r(x)$ is:\n$$ r(x_k + \\Delta x) \\approx r(x_k) + J(x_k) \\Delta x. $$\nSubstituting this linear approximation into the objective function gives a quadratic model for $\\phi$ at $x_k$:\n$$ \\phi(x_k + \\Delta x) \\approx \\frac{1}{2} \\|r(x_k) + J(x_k) \\Delta x\\|_2^2 := m_k(\\Delta x). $$\nThe Gauss–Newton method determines the step $\\Delta x$ by minimizing this quadratic model $m_k(\\Delta x)$. This is a linear least-squares problem for the variable $\\Delta x$. The minimizer is found by setting the gradient of $m_k(\\Delta x)$ with respect to $\\Delta x$ to zero:\n$$ \\nabla_{\\Delta x} m_k(\\Delta x) = J(x_k)^T (r(x_k) + J(x_k) \\Delta x) = 0. $$\nThis yields the **Gauss–Newton normal equations**:\n$$ (J(x_k)^T J(x_k)) \\Delta x = -J(x_k)^T r(x_k). $$\nSolving this linear system for $\\Delta x$ gives the Gauss–Newton step. The plain Gauss–Newton iteration is then $x_{k+1} = x_k + \\Delta x$.\n\nThis method can fail for two primary reasons, which we now analyze. To understand these failures, we must examine the true Hessian of $\\phi(x)$. Differentiating the gradient $\\nabla \\phi(x) = \\sum_{i=1}^m r_i(x) \\nabla r_i(x)$, we apply the product rule to get the Hessian matrix $\\nabla^2 \\phi(x)$:\n$$ \\nabla^2 \\phi(x) = \\sum_{i=1}^m \\left( (\\nabla r_i(x)) (\\nabla r_i(x))^T + r_i(x) \\nabla^2 r_i(x) \\right) = J(x)^T J(x) + \\sum_{i=1}^m r_i(x) H_i(x), $$\nwhere $H_i(x) = \\nabla^2 r_i(x)$ is the Hessian of the $i$-th residual component. The Gauss–Newton method approximates the true Hessian by $J(x)^T J(x)$, effectively neglecting the second term involving $r_i$ and $H_i$.\n\n**Failure Mode 1: Non-convexity and Saddle Points.** The Gauss–Newton approximation $J(x)^T J(x)$ is always positive semi-definite. However, the true Hessian $\\nabla^2 \\phi(x)$ may be indefinite, particularly if the problem is highly nonlinear (large $H_i$) or has large residuals at the solution. If $\\nabla^2 \\phi(x_k)$ is indefinite, $x_k$ may be near a saddle point. The Gauss–Newton step, based on a positive semi-definite Hessian approximation, may not be a descent direction for the true objective $\\phi(x)$, leading to an increase in the objective value and potential divergence. Test 2, involving the Rosenbrock function, is a classic example where the undamped Gauss–Newton step can overshoot into a region of higher objective value.\n\n**Failure Mode 2: Ill-posed Subproblem.** The Gauss–Newton step requires solving the linear system with the matrix $A_k = J(x_k)^T J(x_k)$. If the Jacobian $J(x_k)$ does not have full column rank (i.e., its columns are linearly dependent), the matrix $A_k$ is singular. This occurs when the parameters represented by $x$ are not fully identifiable from the data. In this case, the normal equations do not have a unique solution, and the step $\\Delta x$ is not well-defined. Even if $J(x_k)$ is close to rank-deficient, $A_k$ becomes ill-conditioned, and the computed step can be numerically unstable and excessively large. Test 3 demonstrates this scenario with a rank-deficient Jacobian.\n\nTo overcome these failures, globalization strategies are necessary. The Levenberg–Marquardt (LM) method addresses both issues by introducing a damping parameter. This can be derived from a trust-region perspective. The linear approximation for $r(x)$ is only valid within a small neighborhood, or trust region, around $x_k$. We thus seek a step $\\Delta x$ that minimizes the quadratic model $m_k(\\Delta x)$ subject to a constraint on the step length, $\\|\\Delta x\\|_2 \\le \\delta_k$, for some trust-radius $\\delta_k > 0$. The solution to this constrained optimization problem can be shown to satisfy the following system of equations for a Lagrange multiplier $\\mu \\ge 0$:\n$$ (J(x_k)^T J(x_k) + \\mu I) \\Delta x = -J(x_k)^T r(x_k). $$\nHere, $I$ is the identity matrix. This is the LM system.\nThe damping parameter $\\mu$ provides robustness:\n1.  For any $\\mu > 0$, the matrix $(J_k^T J_k + \\mu I)$ is positive definite, even if $J_k^T J_k$ is singular. This guarantees that the linear subproblem for $\\Delta x$ is well-posed and has a unique solution.\n2.  The parameter $\\mu$ interpolates between the Gauss–Newton step and a steepest-descent step. For $\\mu \\to 0$, the step approaches the Gauss–Newton step. For $\\mu \\to \\infty$, the step becomes $\\Delta x \\approx -\\frac{1}{\\mu} J_k^T r_k$, which is a short step in the steepest descent direction $(-\\nabla \\phi(x_k))$. This ensures a local decrease in $\\phi(x)$ away from stationary points.\n\nA practical LM algorithm adapts $\\mu$ based on the quality of each trial step. A trial step $\\Delta x$ is computed for a given $\\mu_k$. If the step leads to a sufficient decrease in the actual objective $\\phi(x)$, the step is accepted, and $\\mu$ is decreased for the next iteration (expanding the trust region). If the step fails to decrease $\\phi(x)$, it is rejected, $\\mu$ is increased (shrinking the trust region), and a new, smaller, more conservative step is computed. This strategy ensures robust progress toward a minimum.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs GN and LM methods on the test suite.\n    \"\"\"\n    \n    # Tolerances for convergence\n    TOL_PHI = 1e-12\n    TOL_GRAD = 1e-8\n\n    # --- Test Case 1: Linear, Full Rank ---\n    def f1(x):\n        A = np.array([[2.0, 0.0], [0.0, 1.0]])\n        return A @ x\n    \n    y1 = np.array([1.0, 2.0])\n    \n    def r1(x):\n        return f1(x) - y1\n        \n    def j1(x):\n        return np.array([[2.0, 0.0], [0.0, 1.0]])\n        \n    test1_params = {\n        \"r\": r1, \"j\": j1, \"x0\": np.array([0.0, 0.0]), \"max_iter\": 5\n    }\n\n    # --- Test Case 2: Rosenbrock, Non-convex ---\n    def r2(x):\n        return np.array([\n            np.sqrt(10.0) * (x[1] - x[0]**2),\n            1.0 - x[0]\n        ])\n        \n    def j2(x):\n        return np.array([\n            [-2.0 * np.sqrt(10.0) * x[0], np.sqrt(10.0)],\n            [-1.0, 0.0]\n        ])\n        \n    test2_params = {\n        \"r\": r2, \"j\": j2, \"x0\": np.array([-1.2, 1.0]), \"max_iter\": 20\n    }\n\n    # --- Test Case 3: Rank Deficient Jacobian ---\n    def f3(x):\n        return np.array([x[0] + x[1], x[0] + x[1]])\n        \n    y3 = np.array([1.0, 1.0])\n    \n    def r3(x):\n        return f3(x) - y3\n    \n    def j3(x):\n        return np.array([[1.0, 1.0], [1.0, 1.0]])\n        \n    test3_params = {\n        \"r\": r3, \"j\": j3, \"x0\": np.array([0.0, 0.0]), \"max_iter\": 10\n    }\n\n    test_cases = [test1_params, test2_params, test3_params]\n\n    def run_gauss_newton(r_func, j_func, x0, max_iter):\n        x = np.copy(x0)\n        phi_history = []\n        converged = False\n        \n        for k in range(max_iter):\n            res_val = r_func(x)\n            jac_val = j_func(x)\n            \n            phi = 0.5 * np.dot(res_val, res_val)\n            phi_history.append(phi)\n            \n            grad_phi = jac_val.T @ res_val\n            \n            if phi  TOL_PHI or np.linalg.norm(grad_phi)  TOL_GRAD:\n                converged = True\n                break\n                \n            JtJ = jac_val.T @ jac_val\n            rhs = -grad_phi\n            \n            try:\n                # Use pinv for robustness against near-singularity, but GN still fails in principle\n                if np.linalg.cond(JtJ)  1 / np.finfo(JtJ.dtype).eps:\n                    raise np.linalg.LinAlgError(\"Singular matrix\")\n                delta_x = np.linalg.solve(JtJ, rhs)\n            except np.linalg.LinAlgError:\n                break # Singular matrix, GN fails\n            \n            x += delta_x\n        \n        # After loop, calculate final phi\n        final_phi = 0.5 * np.dot(r_func(x), r_func(x))\n        if not converged and (final_phi  TOL_PHI or np.linalg.norm(j_func(x).T @ r_func(x))  TOL_GRAD):\n             converged = True\n\n        monotonic = True\n        if len(phi_history)  1:\n            for i in range(len(phi_history) - 1):\n                # Using a small tolerance for floating point comparisons\n                if phi_history[i+1]  phi_history[i] + 1e-12:\n                    monotonic = False\n                    break\n                    \n        return final_phi, converged, monotonic\n\n    def run_levenberg_marquardt(r_func, j_func, x0, max_iter):\n        x = np.copy(x0)\n        converged = False\n        \n        mu = 1e-3\n        nu_up = 2.0\n        nu_down = 3.0\n        \n        k = 0\n        while k  max_iter:\n            res_val = r_func(x)\n            jac_val = j_func(x)\n            \n            phi = 0.5 * np.dot(res_val, res_val)\n            grad_phi = jac_val.T @ res_val\n            \n            if phi  TOL_PHI or np.linalg.norm(grad_phi)  TOL_GRAD:\n                converged = True\n                break\n            \n            JtJ = jac_val.T @ jac_val\n            rhs = -grad_phi\n            \n            accepted_step = False\n            inner_iter_count = 0\n            while not accepted_step and inner_iter_count  10:\n                A = JtJ + mu * np.identity(len(x))\n                try:\n                    delta_x = np.linalg.solve(A, rhs)\n                except np.linalg.LinAlgError:\n                    mu *= nu_up # Increase damping if solver fails\n                    inner_iter_count += 1\n                    continue\n\n                x_trial = x + delta_x\n                phi_trial = 0.5 * np.dot(r_func(x_trial), r_func(x_trial))\n\n                if phi_trial  phi:\n                    x = x_trial\n                    mu /= nu_down\n                    accepted_step = True\n                else:\n                    mu *= nu_up\n                    inner_iter_count += 1\n            \n            if not accepted_step:\n                # Failed to find a descent direction, stop\n                break\n            \n            k += 1\n\n        final_phi = 0.5 * np.dot(r_func(x), r_func(x))\n        if not converged and (final_phi  TOL_PHI or np.linalg.norm(j_func(x).T @ r_func(x))  TOL_GRAD):\n             converged = True\n             \n        return final_phi, converged\n\n    all_results = []\n    for params in test_cases:\n        r, j, x0, N = params[\"r\"], params[\"j\"], params[\"x0\"], params[\"max_iter\"]\n        \n        phi_gn, conv_gn, mono_gn = run_gauss_newton(r, j, x0, N)\n        phi_lm, conv_lm = run_levenberg_marquardt(r, j, x0, N)\n        \n        # Python's default bool representation is 'True'/'False', which is fine.\n        all_results.append([phi_gn, phi_lm, conv_gn, conv_lm, mono_gn])\n    \n    # Format the final output string exactly as requested\n    # The string representation of a list of lists naturally includes spaces, which\n    # is fine per the example format. Removing them is not strictly necessary.\n    print(str(all_results).replace(\" \", \"\").replace(\"'\", \"\"))\n\nsolve()\n\n```"
        }
    ]
}