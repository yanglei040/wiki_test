{
    "hands_on_practices": [
        {
            "introduction": "A primary source of practical ill-posedness in inverse problems is the inherent nature of the forward operator, which often manifests as an ill-conditioned matrix in discrete settings. The Hilbert matrix is a canonical example of a matrix whose condition number, $\\kappa(A) = \\lVert A \\rVert \\lVert A^{-1} \\rVert$, grows astronomically with its size, making it a perfect tool for studying numerical instability. This hands-on exercise  will guide you through constructing these matrices and computationally demonstrating how their properties lead to extreme sensitivity, a hallmark of ill-posedness in finite-precision arithmetic.",
            "id": "3412219",
            "problem": "You are given the task of constructing a sequence of Hilbert matrices and using them to demonstrate a practical source of ill-posedness through the growth behavior of the two-norm condition number. The Hilbert matrix of order $n$, denoted $H_n$, is defined by the rule $(H_n)_{ij} = \\frac{1}{i + j - 1}$ for all indices $i,j \\in \\{1,2,\\dots,n\\}$. The two-norm condition number of a nonsingular matrix $A$, denoted $\\kappa_2(A)$, is defined as $$\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2 = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},$$ where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$, respectively. The singular values are defined as the square roots of the eigenvalues of $A^\\top A$. The Cauchy interlacing theorem for symmetric positive definite matrices implies monotonicity properties of singular values for nested principal submatrices, which will be useful for reasoning about how $\\kappa_2(H_n)$ evolves with $n$.\n\nYour program must implement the following:\n\n- Construct $H_n$ for each $n$ in the set $S = \\{2,3,4,8,12\\}$ using the definition above and floating-point arithmetic in the standard double-precision format specified by the Institute of Electrical and Electronics Engineers (IEEE) 754 standard.\n- Compute $\\kappa_2(H_n)$ for each $n \\in S$ using the definition $\\kappa_2(H_n) = \\sigma_{\\max}(H_n) / \\sigma_{\\min}(H_n)$, where singular values are obtained via the singular value decomposition of $H_n$.\n- Demonstrate superlinear growth by evaluating, for each consecutive pair $(n_i,n_{i+1})$ in the ordered set $S$, the boolean predicate $$\\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})}  \\frac{n_{i+1}}{n_i}.$$ Return a list of these boolean values in the same order of pairs $[(2,3),(3,4),(4,8),(8,12)]$.\n- Illustrate finite-precision ill-posedness by checking whether the smallest singular value $\\sigma_{\\min}(H_{12})$ is below the double-precision machine epsilon $\\epsilon_{\\mathrm{mach}}$ (that is, test whether $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$). Use the canonical value of machine epsilon provided by the runtime environment for $64$-bit floats.\n\nFundamental bases to be used explicitly in your reasoning and implementation include the definitions of the Hilbert matrix $H_n$, the two-norm condition number $\\kappa_2(\\cdot)$, the singular value decomposition, and the concept of machine epsilon for IEEE 754 double precision arithmetic.\n\nTest Suite:\n- Compute and report $\\kappa_2(H_n)$ for $n \\in \\{2,3,4,8,12\\}$.\n- For $(n_i,n_{i+1}) \\in \\{(2,3),(3,4),(4,8),(8,12)\\}$, compute and report whether $$\\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})}  \\frac{n_{i+1}}{n_i}.$$\n- Compute and report whether $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The first element must be the list of floating-point values $[\\kappa_2(H_2), \\kappa_2(H_3), \\kappa_2(H_4), \\kappa_2(H_8), \\kappa_2(H_{12})]$. The second element must be the list of boolean values corresponding to the superlinearity predicates for consecutive pairs as specified above. The third element must be a single boolean corresponding to the predicate $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$. Precisely, the output must look like $$[\\,[\\kappa_2(H_2),\\kappa_2(H_3),\\kappa_2(H_4),\\kappa_2(H_8),\\kappa_2(H_{12})],\\,[b_1,b_2,b_3,b_4],\\,b_5\\,],$$ where each $b_i$ is a boolean computed as described. No units or angles are involved in this problem, and no percentages should be printed.",
            "solution": "The problem requires an investigation into the numerical ill-posedness of Hilbert matrices, which serve as a canonical example in numerical linear algebra for demonstrating challenges in solving linear systems. The investigation proceeds by computing the two-norm condition number, $\\kappa_2(H_n)$, for a sequence of increasing matrix orders $n$, and analyzing its growth rate and behavior in finite-precision arithmetic.\n\nThe core components of the problem are:\n1.  The Hilbert matrix of order $n$, $H_n$, with entries $(H_n)_{ij} = \\frac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, \\dots, n\\}$.\n2.  The two-norm condition number, $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the maximum and minimum singular values of matrix $A$.\n3.  The concept of machine epsilon, $\\epsilon_{\\mathrm{mach}}$, which for IEEE $754$ double-precision is the smallest number such that $1.0 + \\epsilon_{\\mathrm{mach}} \\neq 1.0$.\n\nThe methodology involves several computational and analytical steps, executed for matrix orders $n$ from the set $S = \\{2, 3, 4, 8, 12\\}$.\n\n**Step 1: Hilbert Matrix Construction**\nFor each order $n \\in S$, an $n \\times n$ Hilbert matrix $H_n$ is constructed. The element at row $i$ and column $j$ (using $1$-based indexing as in the definition) is given by the formula $(H_n)_{ij} = \\frac{1}{i+j-1}$. For implementation with $0$-based indexing, this becomes $(H_n)_{ij} = \\frac{1}{(i+1)+(j+1)-1} = \\frac{1}{i+j+1}$. All computations are performed using standard double-precision floating-point arithmetic.\n\n**Step 2: Condition Number Calculation via Singular Value Decomposition (SVD)**\nThe singular values of each matrix $H_n$ are computed using the Singular Value Decomposition (SVD). Since Hilbert matrices are symmetric and positive-definite, their singular values are identical to their eigenvalues. The SVD provides a vector of singular values, from which the largest, $\\sigma_{\\max}(H_n)$, and smallest, $\\sigma_{\\min}(H_n)$, are extracted. The two-norm condition number is then calculated as their ratio:\n$$ \\kappa_2(H_n) = \\frac{\\sigma_{\\max}(H_n)}{\\sigma_{\\min}(H_n)} $$\nThis process is repeated for each $n \\in S$. The rapid growth of $\\kappa_2(H_n)$ is expected. The problem mentions the Cauchy interlacing theorem, which provides theoretical support for this growth. Since $H_n$ is a leading principal submatrix of $H_{n+1}$, the theorem implies that the singular values of $H_n$ and $H_{n+1}$ interlace. Specifically, $\\sigma_{\\max}(H_{n+1}) \\ge \\sigma_{\\max}(H_n)$ and $\\sigma_{\\min}(H_{n+1}) \\le \\sigma_{\\min}(H_n)$. The combination of an increasing (or non-decreasing) largest singular value and a decreasing (or non-increasing) smallest singular value leads to a monotonically increasing condition number.\n\n**Step 3: Analysis of Superlinear Growth**\nThe problem requires demonstrating a growth rate faster than the growth rate of the matrix dimension. This is tested using the predicate:\n$$ \\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})}  \\frac{n_{i+1}}{n_i} $$\nThis inequality is evaluated for each consecutive pair of orders $(n_i, n_{i+1})$ in the sorted set $S$. The pairs are $(2,3)$, $(3,4)$, $(4,8)$, and $(8,12)$. A boolean result (`True` or `False`) is determined for each check.\n\n**Step 4: Finite-Precision Effects and Numerical Singularity**\nThe final step demonstrates a practical consequence of extreme ill-conditioning. A matrix is considered numerically singular if computations involving it are dominated by floating-point errors. This often occurs when its condition number is on the order of $1/\\epsilon_{\\mathrm{mach}}$. A direct way to diagnose this is to examine the magnitude of its smallest singular value. The problem asks to evaluate the predicate:\n$$ \\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}} $$\nHere, $\\sigma_{\\min}(H_{12})$ is the smallest singular value of the Hilbert matrix of order $12$, and $\\epsilon_{\\mathrm{mach}}$ is the machine epsilon for double-precision arithmetic, which is $2^{-52}$, approximately $2.2204 \\times 10^{-16}$. If this predicate is true, it means the smallest singular value has been rounded to a value so small that it is below the threshold of relative precision for the number format. This effectively renders the matrix singular from a computational standpoint, as inverting it would amplify noise by a factor so large ($ \\kappa_2  10^{16} $) that the result would be meaningless.\n\nThe implementation will consolidate the list of computed condition numbers, the list of boolean results from the superlinearity test, and the single boolean result from the machine epsilon test into a final nested list structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs Hilbert matrices, computes their condition numbers, and analyzes\n    their growth and finite-precision properties as specified in the problem.\n    \"\"\"\n    \n    # Define the set of matrix orders to be evaluated.\n    orders = [2, 3, 4, 8, 12]\n    \n    # Get the machine epsilon for double-precision floating-point numbers.\n    machine_epsilon = np.finfo(float).eps\n\n    cond_numbers = {}\n    singular_values_map = {}\n\n    # Step 1  2: Construct Hilbert matrix and compute condition number for each order.\n    for n in orders:\n        # Construct the Hilbert matrix H_n.\n        # Using 0-based indexing for implementation: (H_n)_ij = 1 / (i+j+1)\n        # where i,j are in {0, 1, ..., n-1}.\n        i = np.arange(1, n + 1).reshape(n, 1)\n        j = np.arange(1, n + 1).reshape(1, n)\n        H = 1.0 / (i + j - 1)\n\n        # Compute singular values using SVD. compute_uv=False is more efficient.\n        sv = np.linalg.svd(H, compute_uv=False)\n        singular_values_map[n] = sv\n        \n        # Calculate the 2-norm condition number.\n        sigma_max = sv[0]\n        sigma_min = sv[-1]\n        cond_numbers[n] = sigma_max / sigma_min\n\n    # Store the list of condition numbers in the required order.\n    k_values = [cond_numbers[n] for n in orders]\n\n    # Step 3: Analyze superlinear growth.\n    superlinearity_checks = []\n    for i in range(len(orders) - 1):\n        n_i = orders[i]\n        n_i_plus_1 = orders[i+1]\n        \n        # Ratio of condition numbers\n        cond_ratio = cond_numbers[n_i_plus_1] / cond_numbers[n_i]\n        \n        # Ratio of dimensions\n        dim_ratio = n_i_plus_1 / n_i\n        \n        # Evaluate the predicate and store the boolean result.\n        superlinearity_checks.append(cond_ratio  dim_ratio)\n\n    # Step 4: Check for finite-precision ill-posedness.\n    # Retrieve the smallest singular value for H_12.\n    sigma_min_h12 = singular_values_map[12][-1]\n    \n    # Evaluate the predicate against machine epsilon.\n    epsilon_check = sigma_min_h12  machine_epsilon\n    \n    # Assemble the final result in the specified format.\n    # To match the no-space-after-comma format, we build the string manually.\n    s1 = '[' + ','.join(map(str, k_values)) + ']'\n    s2 = '[' + ','.join(map(str, superlinearity_checks)) + ']'\n    s3 = str(epsilon_check)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{s1},{s2},{s3}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Ill-posedness is not always a matter of decaying singular values; it can also stem from fundamental ambiguities in the structure of the forward model itself, a concept known as non-identifiability. In such cases, distinct sets of parameters can generate identical noiseless data, making a unique inversion impossible without additional information. Through the simple model $y=kx$, this exercise  allows you to analytically diagnose such a structural ambiguity, quantify its dimension by analyzing the Jacobian's nullspace, and explore how scientifically-motivated constraints can restore well-posedness.",
            "id": "3412214",
            "problem": "Consider a linear measurement operator in data assimilation defined by an unknown spatial field and an unknown global gain: a state vector $x \\in \\mathbb{R}^{m}$ and a scalar $k \\in \\mathbb{R}$ together generate noiseless observations $y \\in \\mathbb{R}^{m}$ via the forward model $y = k x$. Both $x$ and $k$ are unknown and must be inferred from $y$. The aim is to quantify a practical source of ill-posedness due to parameter non-identifiability and design constraints that restore well-posedness.\n\nStarting from the definition that parameters are identifiable if no nontrivial transformation of the parameters leaves the data unchanged, and local identifiability is determined by the rank of the Jacobian of the data map with respect to the parameters, do the following:\n\n1. Prove that the transformation $(x, k) \\mapsto (c x, k / c)$ for any nonzero scalar $c \\in \\mathbb{R} \\setminus \\{0\\}$ leaves $y$ invariant. Use this invariance to characterize the equivalence class of parameters that produce the same $y$ for generic $y \\neq 0$.\n2. Compute the dimension of the continuous ambiguity (i.e., the number of continuous degrees of freedom along which $(x, k)$ can vary while keeping $y$ fixed) by analyzing the nullspace of the Jacobian of the map $\\theta \\mapsto y(\\theta)$ at a generic point, where $\\theta = (x, k) \\in \\mathbb{R}^{m} \\times \\mathbb{R}$ and $y(\\theta) = k x$.\n3. Propose constraints that are scientifically reasonable and practically enforceable to break the ambiguity identified in part $2$ and restore local identifiability. Then, demonstrate that these constraints eliminate all continuous non-identifiability by showing that the intersection between the constraint manifold and the ambiguity directions is trivial at a generic point. You may assume the constraints $g_{1}(x, k) = \\|x\\|_{2} - 1 = 0$ and $g_{2}(x, k): k  0$.\n4. Report two quantities: the dimension of the continuous ambiguity before any constraints are imposed and the dimension of the continuous ambiguity after both $g_{1}$ and $g_{2}$ are imposed. Provide your final answer as a row vector, and do not include units. No rounding is required.\n\nYour derivations must be self-consistent and begin from the core definitions of identifiability and local differential analysis. Avoid using shortcut formulas; instead, derive results from the data map and its Jacobian. Ensure that your proposed constraints are justified as practically reasonable within inverse problems and data assimilation.",
            "solution": "The problem asks for an analysis of ill-posedness, specifically parameter non-identifiability, in a simple linear inverse problem. The forward model, which maps a state vector $x \\in \\mathbb{R}^{m}$ and a scalar gain $k \\in \\mathbb{R}$ to a set of noiseless observations $y \\in \\mathbb{R}^{m}$, is given by:\n$$ y = kx $$\nThe total parameter vector is $\\theta = (x, k)$, which resides in the parameter space $\\mathbb{R}^{m} \\times \\mathbb{R} \\cong \\mathbb{R}^{m+1}$. We are asked to characterize the non-identifiability, propose constraints to resolve it, and report the dimension of the ambiguity before and after the constraints are applied.\n\n1.  **Invariance under Transformation and Equivalence Class Characterization**\n\nWe are asked to prove that the transformation $(x, k) \\mapsto (c x, k / c)$ for any nonzero scalar $c \\in \\mathbb{R} \\setminus \\{0\\}$ leaves the observation $y$ invariant. Let the original parameters be $(x, k)$ and the transformed parameters be $(x', k')$, where $x' = cx$ and $k' = k/c$.\nThe observation $y'$ generated by the transformed parameters is:\n$$ y' = k' x' = \\left(\\frac{k}{c}\\right) (c x) $$\nBy associativity of scalar multiplication, we have:\n$$ y' = \\left(\\frac{k}{c} \\cdot c\\right) x = (1) kx = kx $$\nSince $y = kx$, we have $y' = y$. This demonstrates that the observation $y$ is invariant under this transformation for any $c \\in \\mathbb{R} \\setminus \\{0\\}$.\n\nThis invariance defines an equivalence relation on the parameter space. For a generic, non-zero observation $y \\neq 0$, all parameter pairs $(\\tilde{x}, \\tilde{k})$ that produce this observation belong to the same equivalence class. Let $(x, k)$ be one such pair, so $y = kx$. Then any other pair $(\\tilde{x}, \\tilde{k})$ in the same class must satisfy $\\tilde{k}\\tilde{x} = y = kx$.\nSince $y \\neq 0$, neither $k$ nor $x$ can be zero (or the zero vector). Thus, $\\tilde{x}$ must be a scalar multiple of $x$, i.e., $\\tilde{x} = cx$ for some scalar $c \\in \\mathbb{R}$. Since $\\tilde{x}$ must be non-zero (as $\\tilde{k}\\tilde{x} = y \\neq 0$), we must have $c \\neq 0$.\nSubstituting $\\tilde{x} = cx$ into the equation gives:\n$$ \\tilde{k}(cx) = kx $$\n$$ (c\\tilde{k} - k)x = 0 $$\nSince $x \\neq 0$, the scalar factor must be zero: $c\\tilde{k} - k = 0$, which implies $\\tilde{k} = k/c$.\nTherefore, the equivalence class of parameters that produce a given non-zero observation $y$ is the set of all pairs $(cx, k/c)$ for any $c \\in \\mathbb{R} \\setminus \\{0\\}$, defined by a reference pair $(x, k)$. This set forms a one-dimensional hyperbola in the parameter space.\n\n2.  **Dimension of Continuous Ambiguity via Jacobian Analysis**\n\nLocal identifiability is assessed by the Jacobian of the forward map $F(\\theta) = y(\\theta)$, where $\\theta = (x, k)$. The dimension of the continuous ambiguity at a point $\\theta$ is the dimension of the nullspace of the Jacobian matrix $J$ evaluated at $\\theta$.\nThe forward map is $F:(x_1, \\dots, x_m, k) \\mapsto (kx_1, \\dots, kx_m)$. The Jacobian $J$ is an $m \\times (m+1)$ matrix where $J_{ij} = \\frac{\\partial y_i}{\\partial \\theta_j}$. The parameter vector is $\\theta = (x_1, \\dots, x_m, k)^T$.\n\nThe partial derivatives are:\nFor $j \\in \\{1, \\dots, m\\}$, $\\frac{\\partial y_i}{\\partial x_j} = \\frac{\\partial(kx_i)}{\\partial x_j} = k \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\nFor the $(m+1)$-th parameter $k$, $\\frac{\\partial y_i}{\\partial k} = \\frac{\\partial(kx_i)}{\\partial k} = x_i$.\n\nThe Jacobian matrix is:\n$$ J = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1}  \\dots  \\frac{\\partial y_1}{\\partial x_m}  \\frac{\\partial y_1}{\\partial k} \\\\ \\vdots  \\ddots  \\vdots  \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1}  \\dots  \\frac{\\partial y_m}{\\partial x_m}  \\frac{\\partial y_m}{\\partial k} \\end{pmatrix} = \\begin{pmatrix} k  0  \\dots  0  x_1 \\\\ 0  k  \\dots  0  x_2 \\\\ \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\ 0  0  \\dots  k  x_m \\end{pmatrix} $$\nThis can be written in block form as $J = [k I_m \\mid x]$, where $I_m$ is the $m \\times m$ identity matrix.\n\nThe nullspace of $J$ consists of vectors $v = (\\delta x, \\delta k)^T \\in \\mathbb{R}^{m+1}$ such that $Jv = 0$.\n$$ [k I_m \\mid x] \\begin{pmatrix} \\delta x \\\\ \\delta k \\end{pmatrix} = 0 $$\n$$ k \\delta x + x \\delta k = 0 $$\nAt a generic point, we assume $k \\neq 0$. Then we can solve for $\\delta x$:\n$$ \\delta x = -\\frac{\\delta k}{k} x $$\nLet $\\delta k = \\alpha$, where $\\alpha$ is any real number. Then any vector $v$ in the nullspace can be written as:\n$$ v = \\begin{pmatrix} -\\frac{\\alpha}{k} x \\\\ \\alpha \\end{pmatrix} = \\alpha \\begin{pmatrix} -x/k \\\\ 1 \\end{pmatrix} $$\nThe nullspace is spanned by the single vector $v_0 = (-x/k, 1)^T$. The dimension of a vector space is the number of vectors in its basis. Since the nullspace is spanned by a single non-zero vector, its dimension is $1$.\nThis single degree of freedom corresponds to the continuous ambiguity identified in part 1. The tangent vector to the ambiguity curve $\\gamma(c) = (cx, k/c)$ at $c=1$ is $\\gamma'(1) = (x, -k)$, which is a non-zero scalar multiple of $v_0$ (specifically, $-k$ times the version of $v_0$ written as $(x, -k)$) and thus lies in the same nullspace.\nThe dimension of the continuous ambiguity is $\\dim(\\ker(J)) = 1$.\n\n3.  **Restoring Identifiability with Constraints**\n\nWe introduce two constraints: $g_1(x, k) = \\|x\\|_2 - 1 = 0$ and $g_2(x, k): k  0$.\n**Scientific Justification:** The ambiguity $y = (k/c)(cx)$ shows that scale is arbitrary. The constraint $\\|x\\|_2=1$ is a normalization that fixes the scale of the spatial field $x$, forcing any change in overall amplitude to be absorbed by the gain $k$. This is practical when $x$ represents a mode shape or pattern whose absolute magnitude is less important than its form. The constraint $k0$ is common for physical parameters that are inherently positive, such as concentrations, intensities, or absolute temperatures.\n\n**Demonstration of Efficacy:** To eliminate the continuous ambiguity, the ambiguity direction must not be a feasible direction of change on the constraint manifold. An infinitesimal change $(\\delta x, \\delta k)$ is feasible on the manifold $g_1=0$ only if it is tangent to it, i.e., orthogonal to the gradient of $g_1$.\nThe gradient of $g_1(x,k) = (\\sum_{i=1}^m x_i^2)^{1/2} - 1$ with respect to $\\theta=(x, k)$ is:\n$$ \\nabla g_1 = \\left( \\frac{\\partial g_1}{\\partial x_1}, \\dots, \\frac{\\partial g_1}{\\partial x_m}, \\frac{\\partial g_1}{\\partial k} \\right) $$\nThe derivatives are $\\frac{\\partial g_1}{\\partial x_j} = \\frac{x_j}{\\|x\\|_2}$ and $\\frac{\\partial g_1}{\\partial k} = 0$. So, $\\nabla g_1 = (\\frac{x}{\\|x\\|_2}, 0)$.\nOn the constraint manifold itself, $\\|x\\|_2=1$, so the gradient simplifies to $\\nabla g_1 = (x, 0)$.\nThe tangency condition is $\\nabla g_1 \\cdot (\\delta x, \\delta k)^T = 0$.\n$$ (x, 0) \\cdot \\begin{pmatrix} \\delta x \\\\ \\delta k \\end{pmatrix} = x^T \\delta x = 0 $$\nA vector representing an ambiguity direction is of the form $v = (\\alpha x, -\\alpha k)$ for some scalar $\\alpha$ (this is the tangent vector $\\alpha \\gamma'(1)$). For this change to be possible under the constraint, it must satisfy the tangency condition:\n$$ x^T (\\alpha x) = \\alpha (x^T x) = \\alpha \\|x\\|_2^2 = 0 $$\nSince we are on the constraint manifold, $\\|x\\|_2 = 1$, so the condition becomes $\\alpha(1)^2 = \\alpha = 0$.\nAn $\\alpha=0$ implies that the only allowed change along the ambiguity direction is the zero vector $(\\delta x, \\delta k) = (0, 0)$. This means the constraint $g_1$ eliminates all continuous non-identifiability. The intersection of the nullspace of $J$ and the tangent space of the constraint manifold is $\\{0\\}$, which has dimension $0$.\n\nThe constraint $g_1$ alone is not sufficient to guarantee a unique solution. It reduces the equivalence class from a continuous curve to a discrete set of points. If $(x,k)$ is a solution with $\\|x\\|_2 = 1$, then $(\\tilde{x}, \\tilde{k}) = (cx, k/c)$ is an equivalent solution. For $\\|\\tilde{x}\\|_2 = 1$, we need $\\|cx\\|_2 = |c|\\|x\\|_2 = |c| = 1$, which means $c=1$ or $c=-1$.\nThis leaves two possible solutions: $(x, k)$ and $(-x, -k)$.\nThe second constraint, $g_2: k0$, resolves this remaining discrete ambiguity. If we assume our solution $(x,k)$ has $k0$, then the alternative solution $(-x, -k)$ has a gain of $-k  0$, which violates $g_2$. Thus, only one solution remains.\n\nThe combined effect of $g_1$ and $g_2$ is to select a single point from each equivalence class, thereby restoring local (and in this case, global) identifiability. The dimension of any remaining continuous ambiguity is $0$.\n\n4.  **Final Quantities**\n\n- The dimension of the continuous ambiguity before any constraints are imposed is the dimension of the nullspace of the Jacobian, which is $1$.\n- The dimension of the continuous ambiguity after imposing the constraints $g_1(x,k) = \\|x\\|_2 - 1 = 0$ and $g_2(x,k): k  0$ is the dimension of the intersection of the Jacobian's nullspace and the tangent space of the active constraints. We found this to be $0$.\nThe two quantities are $1$ and $0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having diagnosed sources of ill-posedness, we now turn to the practical challenge of solving such problems to obtain meaningful results from noisy, indirect data. Image and signal deblurring is a classic ill-posed inverse problem where naive inversion drastically amplifies noise. This computational exercise  provides a direct comparison between two cornerstone regularization techniques—Tikhonov and Total Variation—allowing you to implement them, witness their power in suppressing noise, and critically evaluate their distinct trade-offs through characteristic artifacts like oversmoothing and staircasing.",
            "id": "3412170",
            "problem": "Consider a one-dimensional discrete inverse problem in which an unknown piecewise constant signal $x_{\\mathrm{true}} \\in \\mathbb{R}^N$ is observed through a circular convolution blur and additive noise. The forward model is $y = K x_{\\mathrm{true}} + \\varepsilon$, where $K$ is the linear operator corresponding to circular convolution with a nonnegative, unit-sum discrete Gaussian kernel $h \\in \\mathbb{R}^N$, and $\\varepsilon$ is independent and identically distributed Gaussian noise with zero mean and known variance. The blur operator $K$ is compact on $\\mathbb{R}^N$ when $N$ is large and its Fourier magnitudes decay for higher frequencies. This decay produces an ill-posed inversion because high-frequency components of $x_{\\mathrm{true}}$ are strongly attenuated, and noise is amplified when attempting deconvolution. You will compare two regularization strategies and quantify the artifacts known as staircasing and oversmoothing.\n\nThe unknown signal $x_{\\mathrm{true}}$ is piecewise constant with three change points. Fix $N = 256$ and define $x_{\\mathrm{true}}[i]$ for $i = 0,1,\\dots,N-1$ by\n$$\nx_{\\mathrm{true}}[i] =\n\\begin{cases}\n0.0,  0 \\le i \\le 63, \\\\\n1.0,  64 \\le i \\le 127, \\\\\n0.5,  128 \\le i \\le 191, \\\\\n1.5,  192 \\le i \\le 255,\n\\end{cases}\n$$\nwith true change index set $C = \\{63,127,191\\}$. The forward blur $K$ is defined by circular convolution with a discrete Gaussian kernel $h$ of the form $h[j] \\propto \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{j'}{\\sigma}\\right)^2\\right)$ for $j = 0,1,\\dots,N-1$, where $j'$ is the centered index $j' = \\min(j, N-j)$ and $\\sigma$ is a standard deviation parameter in samples, normalized so that $\\sum_{j=0}^{N-1} h[j] = 1$.\n\nYou will reconstruct $x$ from $y$ by solving the following two optimization problems:\n\n1. Tikhonov regularization with identity penalty:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\lambda_{\\mathrm{T}}}{2}\\|x\\|_2^2,\n$$\nwhere $\\lambda_{\\mathrm{T}}  0$.\n\n2. Total variation regularization with discrete gradient:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|D x\\|_1,\n$$\nwhere $\\lambda_{\\mathrm{TV}}  0$, and $D: \\mathbb{R}^N \\to \\mathbb{R}^N$ is the circular forward difference operator $(D x)[i] = x[(i+1) \\bmod N] - x[i]$. You may solve the total variation problem using any convergent and correct method; for example, a splitting method such as the Alternating Direction Method of Multipliers (ADMM) with variable splitting $z = D x$.\n\nDefine the following quantitative artifact measures for any reconstructed signal $\\hat{x} \\in \\mathbb{R}^N$:\n\n- Mean squared error:\n$$\n\\mathrm{MSE}(\\hat{x}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\hat{x}[i] - x_{\\mathrm{true}}[i] \\right)^2.\n$$\n\n- False jump count relative to the true change index set $C$, with threshold $\\tau$:\n$$\nJ_{\\mathrm{false}}(\\hat{x}; \\tau) = \\#\\left\\{ i \\in \\{0,\\dots,N-2\\} \\setminus C \\,:\\, \\left| \\hat{x}[i+1] - \\hat{x}[i] \\right| \\ge \\tau \\right\\}.\n$$\n\n- Mean edge attenuation deficit (oversmoothing artifact), defined at the true change indices:\n$$\n\\mathrm{AD}(\\hat{x}) = \\frac{1}{|C|} \\sum_{t \\in C} \\max\\!\\left( 0, 1 - \\frac{ \\left| \\hat{x}[t+1] - \\hat{x}[t] \\right| }{ \\left| x_{\\mathrm{true}}[t+1] - x_{\\mathrm{true}}[t] \\right| } \\right).\n$$\nThis quantity lies in $[0,1]$ and is larger when edges are more attenuated.\n\nSet the jump detection threshold $\\tau$ based on the true jumps as $\\tau = \\alpha \\cdot A_{\\mathrm{mean}}$, where\n$$\nA_{\\mathrm{mean}} = \\frac{1}{|C|} \\sum_{t \\in C} \\left| x_{\\mathrm{true}}[t+1] - x_{\\mathrm{true}}[t] \\right|,\n$$\nand fix $\\alpha = 0.25$.\n\nImplement a program that, for each test case in the suite below, constructs $h$, simulates data $y$, computes reconstructions $\\hat{x}_{\\mathrm{T}}$ and $\\hat{x}_{\\mathrm{TV}}$, and evaluates the metrics $\\mathrm{MSE}$, $J_{\\mathrm{false}}$, and $\\mathrm{AD}$ for each method.\n\nTest suite parameter sets are tuples $(\\sigma, \\sigma_{\\varepsilon}, \\lambda_{\\mathrm{T}}, \\lambda_{\\mathrm{TV}}, \\rho, I, \\mathrm{seed})$, where $\\sigma$ is the blur standard deviation in samples, $\\sigma_{\\varepsilon}$ is the noise standard deviation, $\\lambda_{\\mathrm{T}}$ and $\\lambda_{\\mathrm{TV}}$ are regularization strengths, $\\rho$ is the augmented Lagrangian penalty parameter for Alternating Direction Method of Multipliers (ADMM), $I$ is the number of ADMM iterations, and $\\mathrm{seed}$ is the pseudorandom number generator seed for reproducibility. Use the following test suite:\n\n- Case A (moderate blur and noise): $(2.0, 0.01, 0.01, 0.10, 1.0, 200, 0)$.\n- Case B (higher noise): $(2.0, 0.05, 0.05, 0.20, 1.0, 300, 1)$.\n- Case C (strong blur): $(5.0, 0.01, 0.02, 0.15, 1.5, 300, 2)$.\n- Case D (low noise but strong regularization): $(2.0, 0.005, 0.50, 0.80, 1.0, 300, 3)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of six numbers in the order $[\\mathrm{MSE}(\\hat{x}_{\\mathrm{T}}), \\mathrm{MSE}(\\hat{x}_{\\mathrm{TV}}), J_{\\mathrm{false}}(\\hat{x}_{\\mathrm{T}};\\tau), J_{\\mathrm{false}}(\\hat{x}_{\\mathrm{TV}};\\tau), \\mathrm{AD}(\\hat{x}_{\\mathrm{T}}), \\mathrm{AD}(\\hat{x}_{\\mathrm{TV}})]$. For example, the final output format must be of the form\n$$\n\\left[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}, r_{1,5}, r_{1,6}], [r_{2,1}, \\dots, r_{2,6}], [r_{3,1}, \\dots, r_{3,6}], [r_{4,1}, \\dots, r_{4,6}] \\right],\n$$\nwhere each $r_{k,\\ell}$ is a real number or an integer. No other text should be printed.",
            "solution": "The problem as stated is a well-posed numerical exercise in the field of inverse problems and computational imaging. It is scientifically sound, self-contained, and objective. All necessary parameters, models, and definitions for simulating the data, performing the reconstructions, and evaluating the results are provided. The problem asks for a comparison of two standard regularization techniques, Tikhonov and Total Variation, on a deconvolution problem, which is a canonical task in this field. The specified artifact metrics are standard and directly relevant to the known behaviors of these regularization methods. The problem is therefore deemed valid. A solution will be constructed following the specified methodology.\n\nThe solution is implemented by first setting up the one-dimensional deconvolution problem. This involves defining the true piecewise constant signal $x_{\\mathrm{true}}$, the Gaussian blur kernel $h$, the circular convolution operator $K$, and simulating the noisy measurement data $y$. Subsequently, two distinct regularized inversion algorithms are applied to reconstruct the signal from $y$. Finally, quantitative metrics are computed to assess the quality of each reconstruction and to highlight the characteristic artifacts associated with each regularization method.\n\nFirst, the ground truth signal $x_{\\mathrm{true}} \\in \\mathbb{R}^N$ with $N=256$ is generated as a piecewise constant vector with four distinct levels and three change points at indices $C = \\{63, 127, 191\\}$. The blurring operator $K$ corresponds to a circular convolution. The convolution kernel $h$ is a discrete Gaussian, constructed to be symmetric and normalized to sum to one. The convolution operation $Kx$ is efficiently computed in the Fourier domain using the Fast Fourier Transform (FFT), where it becomes an element-wise product: $(Kx)_{\\mathrm{FFT}} = \\mathrm{FFT}(h) \\odot \\mathrm{FFT}(x)$. The observed data $y$ are then simulated by computing $y = Kx_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon$ is a vector of independent and identically distributed Gaussian noise samples with mean $0$ and a specified standard deviation $\\sigma_{\\varepsilon}$. The random seed is fixed for reproducibility.\n\nThe first reconstruction method is Tikhonov regularization. The objective is to find $\\hat{x}_{\\mathrm{T}}$ that solves:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\lambda_{\\mathrm{T}}}{2}\\|x\\|_2^2\n$$\nThis is a quadratic optimization problem with a closed-form solution derived from the normal equations $(K^T K + \\lambda_{\\mathrm{T}} I) x = K^T y$. Since $K$ is a circulant matrix representing circular convolution, it is diagonalized by the FFT. The operator $K^T K + \\lambda_{\\mathrm{T}} I$ is also circulant and thus easily inverted in the Fourier domain. Let $H = \\mathrm{FFT}(h)$, $Y = \\mathrm{FFT}(y)$, and $\\hat{X}_{\\mathrm{T}} = \\mathrm{FFT}(\\hat{x}_{\\mathrm{T}})$. The solution in the Fourier domain is given by:\n$$\n\\hat{X}_{\\mathrm{T}} = \\frac{\\overline{H} \\odot Y}{|H|^2 + \\lambda_{\\mathrm{T}}}\n$$\nwhere $\\overline{H}$ is the complex conjugate of $H$, $|H|^2$ is the element-wise squared magnitude, and the division is element-wise. The reconstructed signal $\\hat{x}_{\\mathrm{T}}$ is then obtained by applying the inverse FFT. This method is known for producing smooth reconstructions, which can lead to the oversmoothing of sharp edges.\n\nThe second reconstruction method is Total Variation (TV) regularization. The objective is to find $\\hat{x}_{\\mathrm{TV}}$ that solves:\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|D x\\|_1\n$$\nwhere $D$ is the circular forward difference operator, $(Dx)[i] = x[(i+1) \\bmod N] - x[i]$, and $\\|\\cdot\\|_1$ is the $\\ell_1$-norm. The non-differentiable $\\ell_1$-norm term promotes sparsity in the gradient of $x$, which favors piecewise constant solutions and preserves sharp edges. This problem is solved using the Alternating Direction Method of Multipliers (ADMM) with the variable splitting $z = Dx$. The augmented Lagrangian is:\n$$\nL_{\\rho}(x, z, u) = \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|z\\|_1 + \\frac{\\rho}{2}\\|Dx - z + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\nwhere $u$ is the scaled dual variable and $\\rho$ is the penalty parameter. The ADMM algorithm iteratively updates $x$, $z$, and $u$:\n1.  **$x$-update**: $\\min_x \\left( \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\rho}{2}\\|Dx - z^k + u^k\\|_2^2 \\right)$. This is a quadratic problem whose solution is efficiently found in the Fourier domain, analogous to the Tikhonov case. The operators $K$ and $D$ are both circulant, so the solution for $\\hat{X}^{k+1} = \\mathrm{FFT}(x^{k+1})$ is:\n    $$\n    \\hat{X}^{k+1} = \\frac{\\overline{H} \\odot Y + \\rho \\overline{\\Delta} \\odot (Z^k - U^k)}{|H|^2 + \\rho |\\Delta|^2}\n    $$\n    where $\\Delta = \\mathrm{FFT}(d)$ with $d$ being the kernel of the operator $D$.\n2.  **$z$-update**: $\\min_z \\left( \\lambda_{\\mathrm{TV}}\\|z\\|_1 + \\frac{\\rho}{2}\\|Dx^{k+1} - z + u^k\\|_2^2 \\right)$. This has a closed-form solution given by the element-wise soft-thresholding operator:\n    $$\n    z^{k+1} = \\mathcal{S}_{\\lambda_{\\mathrm{TV}}/\\rho}(Dx^{k+1} + u^k)\n    $$\n    where $\\mathcal{S}_{\\tau}(v)_i = \\mathrm{sign}(v_i) \\max(|v_i| - \\tau, 0)$.\n3.  **$u$-update**: $u^{k+1} = u^k + Dx^{k+1} - z^{k+1}$.\nThese steps are repeated for a fixed number of iterations $I$. TV regularization is known for preserving edges but can introduce piecewise constant artifacts, known as staircasing, in smoothly varying regions.\n\nFinally, three metrics are evaluated for both reconstructions, $\\hat{x}_{\\mathrm{T}}$ and $\\hat{x}_{\\mathrm{TV}}$:\n1.  **Mean Squared Error (MSE)**: $\\mathrm{MSE}(\\hat{x}) = \\frac{1}{N}\\|\\hat{x} - x_{\\mathrm{true}}\\|_2^2$, a global measure of reconstruction fidelity.\n2.  **False Jump Count ($J_{\\mathrm{false}}$)**: This metric quantifies staircasing by counting the number of significant jumps in the reconstructed signal at locations other than the true change points. A jump at index $i$ is considered significant if its magnitude $|\\hat{x}[i+1] - \\hat{x}[i]|$ exceeds a threshold $\\tau=0.25 \\cdot A_{\\mathrm{mean}}$, where $A_{\\mathrm{mean}}$ is the average magnitude of the true jumps in $x_{\\mathrm{true}}$.\n3.  **Mean Edge Attenuation Deficit (AD)**: This metric quantifies oversmoothing by measuring the extent to which true edges are attenuated in the reconstruction. It is calculated as the average deficit $1 - |\\Delta \\hat{x}|/|\\Delta x_{\\mathrm{true}}|$ at the true change points, clipped at $0$. A higher AD value (closer to $1$) indicates more severe oversmoothing of true edges.\n\nThe entire process is executed for each parameter set in the provided test suite, and the six resulting metric values for each case are collected and formatted into the required output string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the deconvolution comparison experiment.\n    \"\"\"\n    N = 256\n    C = [63, 127, 191]\n\n    # Test suite parameter sets:\n    # (sigma, sigma_eps, lambda_T, lambda_TV, rho, I, seed)\n    test_cases = [\n        (2.0, 0.01, 0.01, 0.10, 1.0, 200, 0),    # Case A\n        (2.0, 0.05, 0.05, 0.20, 1.0, 300, 1),    # Case B\n        (5.0, 0.01, 0.02, 0.15, 1.5, 300, 2),    # Case C\n        (2.0, 0.005, 0.50, 0.80, 1.0, 300, 3)     # Case D\n    ]\n\n    # --- Helper Functions ---\n\n    def create_true_signal(n_samples):\n        x_true = np.zeros(n_samples)\n        x_true[0:64] = 0.0\n        x_true[64:128] = 1.0\n        x_true[128:192] = 0.5\n        x_true[192:256] = 1.5\n        return x_true\n\n    def create_gaussian_kernel(sigma, n_samples):\n        j = np.arange(n_samples)\n        j_centered = np.minimum(j, n_samples - j)\n        h = np.exp(-0.5 * (j_centered / sigma)**2)\n        return h / np.sum(h)\n\n    def solve_tikhonov(y, h, lambda_t):\n        H = np.fft.fft(h)\n        Y = np.fft.fft(y)\n        X_hat_fft = (np.conj(H) * Y) / (np.abs(H)**2 + lambda_t)\n        return np.real(np.fft.ifft(X_hat_fft))\n\n    def soft_threshold(v, kappa):\n        return np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n\n    def solve_tv_admm(y, h, lambda_tv, rho, iterations):\n        n = len(y)\n        # Initialize variables\n        x = np.zeros(n)\n        z = np.zeros(n)\n        u = np.zeros(n)\n\n        # Pre-compute FFTs\n        H = np.fft.fft(h)\n        Y = np.fft.fft(y)\n        \n        # FFT of forward difference operator D\n        d_kernel = np.zeros(n)\n        d_kernel[0] = -1\n        d_kernel[1] = 1\n        Delta = np.fft.fft(d_kernel)\n\n        H_conj = np.conj(H)\n        Delta_conj = np.conj(Delta)\n        H_sq_abs = np.abs(H)**2\n        Delta_sq_abs = np.abs(Delta)**2\n\n        # Pre-compute denominator for x-update\n        x_update_denom = H_sq_abs + rho * Delta_sq_abs\n        \n        # ADMM loop\n        for _ in range(iterations):\n            # x-update\n            Z = np.fft.fft(z)\n            U = np.fft.fft(u)\n            x_update_num = H_conj * Y + rho * Delta_conj * (Z - U)\n            X = x_update_num / x_update_denom\n            x = np.real(np.fft.ifft(X))\n            \n            # z-update\n            Dx = np.roll(x, -1) - x\n            v = Dx + u\n            z = soft_threshold(v, lambda_tv / rho)\n            \n            # u-update\n            u = u + Dx - z\n\n        return x\n\n    def calculate_metrics(x_hat, x_true, change_indices, tau):\n        n_samples = len(x_true)\n        # 1. MSE\n        mse = np.mean((x_hat - x_true)**2)\n\n        # 2. False Jump Count\n        diff_x_hat = np.abs(x_hat[1:] - x_hat[:-1])\n        all_indices = set(range(n_samples - 1))\n        true_jump_indices = set(change_indices)\n        false_jump_check_indices = sorted(list(all_indices - true_jump_indices))\n        \n        j_false = 0\n        for i in false_jump_check_indices:\n            if diff_x_hat[i] = tau:\n                j_false += 1\n\n        # 3. Attenuation Deficit\n        ad_sum = 0\n        for t in change_indices:\n            true_jump_mag = np.abs(x_true[t + 1] - x_true[t])\n            if true_jump_mag  1e-9: # Avoid division by zero\n                recon_jump_mag = np.abs(x_hat[t + 1] - x_hat[t])\n                ratio = recon_jump_mag / true_jump_mag\n                ad_sum += np.maximum(0, 1 - ratio)\n        ad = ad_sum / len(change_indices)\n        \n        return mse, j_false, ad\n\n    # --- Main Loop ---\n    \n    x_true = create_true_signal(N)\n\n    # Calculate jump detection threshold tau\n    true_jump_mags = np.abs([x_true[t+1] - x_true[t] for t in C])\n    A_mean = np.mean(true_jump_mags)\n    alpha = 0.25\n    tau = alpha * A_mean\n\n    all_results = []\n    \n    for params in test_cases:\n        sigma, sigma_eps, lambda_T, lambda_TV, rho, I, seed = params\n        \n        np.random.seed(seed)\n        \n        # 1. Setup problem\n        h = create_gaussian_kernel(sigma, N)\n        Kx_true = np.real(np.fft.ifft(np.fft.fft(h) * np.fft.fft(x_true)))\n        epsilon = np.random.normal(0, sigma_eps, N)\n        y = Kx_true + epsilon\n        \n        # 2. Solve for reconstructions\n        x_hat_T = solve_tikhonov(y, h, lambda_T)\n        x_hat_TV = solve_tv_admm(y, h, lambda_TV, rho, I)\n        \n        # 3. Calculate metrics\n        mse_T, j_false_T, ad_T = calculate_metrics(x_hat_T, x_true, C, tau)\n        mse_TV, j_false_TV, ad_TV = calculate_metrics(x_hat_TV, x_true, C, tau)\n        \n        case_results = [mse_T, mse_TV, j_false_T, j_false_TV, ad_T, ad_TV]\n        all_results.append(case_results)\n        \n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}