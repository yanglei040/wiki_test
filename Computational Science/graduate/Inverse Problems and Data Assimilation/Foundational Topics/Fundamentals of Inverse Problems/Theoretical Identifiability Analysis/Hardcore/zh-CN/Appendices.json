{
    "hands_on_practices": [
        {
            "introduction": "参数的实际可辨识性不仅是模型的内在属性，也取决于我们收集数据的方式。本练习将通过一个简单的指数衰减模型，阐明测量时间的选取如何导致参数变得实际上不可辨识，即便它们在结构上是可辨识的。我们将通过分析模型灵敏度向量的共线性来探索这一关键概念。",
            "id": "3426673",
            "problem": "考虑一个标量可观测模型 $y(t) = A e^{-\\theta t}$，其中包含未知幅度参数 $A \\in \\mathbb{R}$ 和未知衰减率参数 $\\theta  0$。假设我们在 $t_k = k \\Delta t$（其中 $k = 1, \\dots, K$）处收集了 $K$ 个离散时间观测值，这些观测值带有方差为 $\\sigma^2$ 的独立同分布 (i.i.d.) 加性高斯噪声，因此 $y_k = A e^{-\\theta t_k} + \\varepsilon_k$，其中 $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $t_k = k \\Delta t$。在参数可辨识性的局部（一阶）分析中，灵敏度矩阵 $S \\in \\mathbb{R}^{K \\times 2}$ 汇集了观测模型关于参数的偏导数，这些偏导数是沿着离散采样方案计算的。也就是说，它的两列是 $s_A(k) = \\partial y_k / \\partial A$ 和 $s_{\\theta}(k) = \\partial y_k / \\partial \\theta$。在高斯噪声下，$(A, \\theta)$ 的费雪信息矩阵 (FIM) 在相差一个因子 $1/\\sigma^2$ 的情况下由 $S^\\top S$ 给出。\n\n从这些定义和测量模型出发，根据基本原理进行推理，以确定采样间隔 $\\Delta t$ 的选择如何与由 $\\theta$ 控制的衰减相互作用，从而影响灵敏度列的几何结构（例如，近共线性）以及费雪信息矩阵的条件数。特别地，评估粗采样（大 $\\Delta t$）对 $\\theta$ 的实际可辨识性的影响。\n\n下列哪些陈述是正确的？选择所有适用项。\n\nA. 如果我们定义权重 $w_k = e^{-2 \\theta k \\Delta t}$ 以及相关的归一化离散分布 $p_k = w_k \\big/ \\sum_{j=1}^K w_j$，那么灵敏度列 $s_A$ 和 $s_{\\theta}$ 之间的绝对 Pearson 相关性为 $|\\rho| = \\mu_1 \\big/ \\sqrt{\\mu_2}$，其中 $\\mu_1 = \\sum_{k=1}^K k \\, p_k$ 且 $\\mu_2 = \\sum_{k=1}^K k^2 \\, p_k$。对于大的 $\\theta \\Delta t$（对快速衰减进行粗采样），$p_k$ 集中在小的 $k$ 值上，并且 $|\\rho| \\to 1$，表明存在近共线性。\n\nB. 对于 $w_k = e^{-2 \\theta k \\Delta t}$，费雪信息矩阵的行列式满足 $\\det(S^\\top S) = A^2 \\Delta t^2 \\left(\\sum_{k=1}^K w_k\\right)^2 \\operatorname{Var}_{p}(k)$，其中 $\\operatorname{Var}_{p}(k) = \\sum_{k=1}^K k^2 p_k - \\left(\\sum_{k=1}^K k p_k\\right)^2$。当 $\\theta$ 固定时，随着 $\\Delta t$ 增大，$p_k$ 会集中，且 $\\operatorname{Var}_{p}(k) \\to 0$，这使得 FIM 近奇异，并导致 $\\theta$ 实际上不可辨识。\n\nC. 离散化不会影响可辨识性，因为可辨识性是连续时间模型的一个结构性质；如果 $\\theta$ 在连续时间下是可辨识的，那么在任何离散采样下它都保持同样的可辨识性。\n\nD. 细化采样（减小 $\\Delta t$）总是会增加灵敏度列 $s_A$ 和 $s_{\\theta}$ 之间的相关性，从而恶化 $\\theta$ 的可辨识性。\n\nE. 如果模型中包含一个额外的恒定偏移参数 $B$，使得 $y_k = A e^{-\\theta k \\Delta t} + B + \\varepsilon_k$，那么粗采样会使对 $B$ 的灵敏度，即 $s_B(k) = 1$，与 $s_A(k)$ 正交，从而改善 $\\theta$ 的可辨识性。",
            "solution": "用户需要对问题陈述进行严格验证，然后进行基于基本原理的推导，并对所有选项进行评估。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **模型：** 标量可观测模型 $y(t) = A e^{-\\theta t}$。\n*   **参数：** 幅度 $A \\in \\mathbb{R}$，衰减率 $\\theta  0$。\n*   **离散观测：** $y_k = A e^{-\\theta t_k} + \\varepsilon_k$，其中 $k = 1, \\dots, K$。\n*   **采样方案：** $t_k = k \\Delta t$。\n*   **噪声模型：** $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$，独立同分布 (i.i.d.)。\n*   **灵敏度矩阵：** $S \\in \\mathbb{R}^{K \\times 2}$，其列为 $s_A$ 和 $s_{\\theta}$。\n*   **灵敏度列定义：** 各列的第 $k$ 个元素为 $s_A(k) = \\partial y_k / \\partial A$ 和 $s_{\\theta}(k) = \\partial y_k / \\partial \\theta$。\n*   **费雪信息矩阵 (FIM)：** 在高斯噪声下，$(A, \\theta)$ 的 FIM 在相差一个因子 $1/\\sigma^2$ 的情况下由 $S^\\top S$ 给出。分析应集中于 $S^\\top S$。\n*   **目标：** 分析采样间隔 $\\Delta t$ 和衰减率 $\\theta$ 对灵敏度列的共线性和 FIM 的条件数的影响，特别是对于大的采样间隔 $\\Delta t$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n1.  **科学基础：** 该问题基于标准的指数衰减模型，这是许多 STEM 领域建模的基石。使用灵敏度分析和费雪信息矩阵的分析框架是用于局部参数可辨识性分析和最优实验设计的标准且严谨的方法。所有概念都已广为接受。\n2.  **良态问题：** 该问题提供了一个清晰、自洽的数学设定。目标是分析所得数学对象（$S$ 和 $S^\\top S$）的特定性质，这是一个明确定义的任务。基于所提供的定义，可以进行唯一的分析。\n3.  **目标明确：** 语言精确且数学化。诸如“灵敏度矩阵”、“共线性”和“费雪信息矩阵”等术语在此上下文中具有明确的定义。\n4.  **完整性：** 问题陈述包含了所有必要信息：模型函数、参数定义、采样方案、噪声特性以及待分析数学对象的定义。\n5.  **无其他缺陷：** 该问题没有矛盾、不切实际的条件或含糊之处。它提出了参数估计理论中一个标准的、有深度的问题。\n\n**步骤 3：结论与行动**\n\n问题陈述是有效的。可以继续进行分析。\n\n### 基于原理的推导与求解\n\n**1. 灵敏度向量的推导**\n第 $k$ 次观测的模型是 $y_k$，它基于模型函数 $f(t_k; A, \\theta) = A e^{-\\theta t_k}$。灵敏度向量 $\\mathbf{s}_A$ 和 $\\mathbf{s}_{\\theta}$ 是 $\\mathbb{R}^K$ 中的向量，其分量是模型函数对每个参数的偏导数，在每个采样时间 $t_k = k \\Delta t$ 处求值。\n\n*   **对 A 的灵敏度：** 第 $k$ 个分量是\n    $$ s_A(k) = \\frac{\\partial}{\\partial A} \\left( A e^{-\\theta k \\Delta t} \\right) = e^{-\\theta k \\Delta t} $$\n*   **对 $\\theta$ 的灵敏度：** 第 $k$ 个分量是\n    $$ s_{\\theta}(k) = \\frac{\\partial}{\\partial \\theta} \\left( A e^{-\\theta k \\Delta t} \\right) = A e^{-\\theta k \\Delta t} (-k \\Delta t) = -A k \\Delta t e^{-\\theta k \\Delta t} $$\n\n因此，两个灵敏度列向量为：\n$$ \\mathbf{s}_A = \\begin{pmatrix} e^{-\\theta \\Delta t} \\\\ e^{-2\\theta \\Delta t} \\\\ \\vdots \\\\ e^{-K\\theta \\Delta t} \\end{pmatrix}, \\quad \\mathbf{s}_{\\theta} = -A \\Delta t \\begin{pmatrix} 1 \\cdot e^{-\\theta \\Delta t} \\\\ 2 \\cdot e^{-2\\theta \\Delta t} \\\\ \\vdots \\\\ K \\cdot e^{-K\\theta \\Delta t} \\end{pmatrix} $$\n\n**2. 费雪信息矩阵 ($S^\\top S$) 的推导**\n矩阵 $S^\\top S$ 是一个 $2 \\times 2$ 的 Gram 矩阵，其元素是灵敏度向量的点积。\n$$ S^\\top S = \\begin{pmatrix} \\mathbf{s}_A^\\top \\mathbf{s}_A  \\mathbf{s}_A^\\top \\mathbf{s}_{\\theta} \\\\ \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_A  \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta} \\end{pmatrix} $$\n我们来计算其元素：\n*   $(S^\\top S)_{11} = \\mathbf{s}_A^\\top \\mathbf{s}_A = \\sum_{k=1}^K (e^{-\\theta k \\Delta t})^2 = \\sum_{k=1}^K e^{-2\\theta k \\Delta t}$\n*   $(S^\\top S)_{12} = \\mathbf{s}_A^\\top \\mathbf{s}_{\\theta} = \\sum_{k=1}^K (e^{-\\theta k \\Delta t})(-A k \\Delta t e^{-\\theta k \\Delta t}) = -A \\Delta t \\sum_{k=1}^K k e^{-2\\theta k \\Delta t}$\n*   $(S^\\top S)_{21} = (S^\\top S)_{12}$\n*   $(S^\\top S)_{22} = \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta} = \\sum_{k=1}^K (-A k \\Delta t e^{-\\theta k \\Delta t})^2 = A^2 (\\Delta t)^2 \\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}$\n\n**3. 共线性分析**\n共线性通过向量 $\\mathbf{s}_A$ 和 $\\mathbf{s}_{\\theta}$ 之间夹角 $\\phi$ 的余弦来评估，其中 $\\cos(\\phi) = \\frac{\\mathbf{s}_A^\\top \\mathbf{s}_{\\theta}}{\\|\\mathbf{s}_A\\| \\|\\mathbf{s}_{\\theta}\\|}$。近共线性对应于 $|\\cos(\\phi)| \\approx 1$。\n$$ \\cos(\\phi) = \\frac{-A \\Delta t \\sum_{k=1}^K k e^{-2\\theta k \\Delta t}}{\\sqrt{\\left(\\sum_{k=1}^K e^{-2\\theta k \\Delta t}\\right) \\left(A^2 (\\Delta t)^2 \\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}\\right)}} = -\\mathrm{sgn}(A) \\frac{\\sum_{k=1}^K k e^{-2\\theta k \\Delta t}}{\\sqrt{\\left(\\sum_{k=1}^K e^{-2\\theta k \\Delta t}\\right)\\left(\\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}\\right)}} $$\n我们定义权重 $w_k = e^{-2\\theta k \\Delta t}$ 和总权重 $W = \\sum_{j=1}^K w_j$。我们可以定义一个离散概率分布 $p_k = w_k/W$。余弦的绝对值表达式变为：\n$$ |\\cos(\\phi)| = \\frac{\\sum k w_k}{\\sqrt{(\\sum w_k)(\\sum k^2 w_k)}} = \\frac{W \\sum k p_k}{\\sqrt{W \\cdot W \\sum k^2 p_k}} = \\frac{\\sum k p_k}{\\sqrt{\\sum k^2 p_k}} $$\n遵循问题选项中的记法，令 $\\mu_1 = \\sum_{k=1}^K k p_k$ 和 $\\mu_2 = \\sum_{k=1}^K k^2 p_k$。那么 $|\\cos(\\phi)| = \\mu_1 / \\sqrt{\\mu_2}$。虽然 Pearson 相关系数在技术上有所不同，因为它涉及对向量进行去均值化，但余弦相似度是共线性的直接度量，并且在这类分析中通常是“相关性”的语境含义。\n\n### 逐项分析\n\n**A. 如果我们定义权重 $w_k = e^{-2 \\theta k \\Delta t}$ 以及相关的归一化离散分布 $p_k = w_k \\big/ \\sum_{j=1}^K w_j$，那么灵敏度列 $s_A$ 和 $s_{\\theta}$ 之间的绝对 Pearson 相关性为 $|\\rho| = \\mu_1 \\big/ \\sqrt{\\mu_2}$，其中 $\\mu_1 = \\sum_{k=1}^K k \\, p_k$ 且 $\\mu_2 = \\sum_{k=1}^K k^2 \\, p_k$。对于大的 $\\theta \\Delta t$（对快速衰减进行粗采样），$p_k$ 集中在小的 $k$ 值上，并且 $|\\rho| \\to 1$，表明存在近共线性。**\n\n- **公式评估：** 如上所述，公式 $|\\rho| = \\mu_1 / \\sqrt{\\mu_2}$ 表示灵敏度向量之间余弦相似度的幅值，而不严格是 Pearson 相关系数。然而，它是对它们几何对齐（共线性）的有效且直接的度量。\n- **极限分析：** 考虑“粗采样”的情况，其中乘积 $\\theta \\Delta t$ 很大。令 $x = \\theta \\Delta t \\gg 1$。权重为 $w_k = e^{-2kx}$。连续权重之比为 $w_{k+1}/w_k = e^{-2x} \\ll 1$。这意味着权重衰减极快：$w_1 \\gg w_2 \\gg w_3 \\dots$。总和 $W = \\sum_j w_j$ 由第一项 $w_1$ 主导。\n- 因此，概率质量函数 $p_k = w_k / W$ 将主要集中在最小的索引 $k=1$ 处。在 $\\theta \\Delta t \\to \\infty$ 的极限下，我们有 $p_1 \\to 1$ 且对于 $k  1$ 有 $p_k \\to 0$。\n- 我们现在可以评估矩的极限：\n    - $\\mu_1 = \\sum_{k=1}^K k p_k \\longrightarrow 1 \\cdot p_1 + 2 \\cdot p_2 + \\dots \\longrightarrow 1 \\cdot 1 = 1$。\n    - $\\mu_2 = \\sum_{k=1}^K k^2 p_k \\longrightarrow 1^2 \\cdot p_1 + 2^2 \\cdot p_2 + \\dots \\longrightarrow 1^2 \\cdot 1 = 1$。\n- 因此，共线性度量变为 $|\\rho| = \\mu_1 / \\sqrt{\\mu_2} \\longrightarrow 1 / \\sqrt{1} = 1$。\n- **结论：** $|\\rho| \\to 1$ 的值表示向量 $\\mathbf{s}_A$ 和 $\\mathbf{s}_{\\theta}$ 变得近乎平行。这使得在统计上难以区分改变 $A$ 的效应和改变 $\\theta$ 的效应，这是实际可辨识性差的本质。尽管存在轻微的术语模糊，但该陈述在概念上和数学上都是正确的。\n- **对 A 的裁定：** **正确**。\n\n**B. 对于 $w_k = e^{-2 \\theta k \\Delta t}$，费雪信息矩阵的行列式满足 $\\det(S^\\top S) = A^2 \\Delta t^2 \\left(\\sum_{k=1}^K w_k\\right)^2 \\operatorname{Var}_{p}(k)$，其中 $\\operatorname{Var}_{p}(k) = \\sum_{k=1}^K k^2 p_k - \\left(\\sum_{k=1}^K k p_k\\right)^2$。当 $\\theta$ 固定时，随着 $\\Delta t$ 增大，$p_k$ 会集中，且 $\\operatorname{Var}_{p}(k) \\to 0$，这使得 FIM 近奇异，并导致 $\\theta$ 实际上不可辨识。**\n\n- **公式评估：** 我们来计算 $S^\\top S$ 的行列式：\n    $$ \\det(S^\\top S) = (\\mathbf{s}_A^\\top \\mathbf{s}_A)(\\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta}) - (\\mathbf{s}_A^\\top \\mathbf{s}_{\\theta})^2 $$\n    $$ \\det(S^\\top S) = \\left(\\sum w_k\\right) \\left(A^2 (\\Delta t)^2 \\sum k^2 w_k\\right) - \\left(-A \\Delta t \\sum k w_k\\right)^2 $$\n    $$ \\det(S^\\top S) = A^2 (\\Delta t)^2 \\left[ \\left(\\sum w_k\\right)\\left(\\sum k^2 w_k\\right) - \\left(\\sum k w_k\\right)^2 \\right] $$\n    使用 $p_k=w_k/W$ 和 $W=\\sum w_k$：$\\sum k w_k = W \\mu_1$ 和 $\\sum k^2 w_k = W \\mu_2$。\n    $$ \\det(S^\\top S) = A^2 (\\Delta t)^2 \\left[ W(W \\mu_2) - (W \\mu_1)^2 \\right] = A^2 (\\Delta t)^2 W^2 (\\mu_2 - \\mu_1^2) $$\n    由于 $W = \\sum w_k$ 且 $\\operatorname{Var}_p(k) = \\mu_2 - \\mu_1^2$，该公式为 $\\det(S^\\top S) = A^2 (\\Delta t)^2 \\left(\\sum w_k\\right)^2 \\operatorname{Var}_p(k)$。该公式是正确的。\n- **极限分析：** 随着 $\\Delta t$ 增大，$\\theta \\Delta t \\to \\infty$。正如选项 A 中所确定的，分布 $p_k$ 集中在 $k=1$ 处。对于一个以概率 1 取单个值的随机变量，其方差为零。\n    - $\\operatorname{Var}_p(k) = \\mu_2 - \\mu_1^2 \\longrightarrow 1 - 1^2 = 0$。\n- **结论：** FIM 的行列式趋近于零表明该矩阵近奇异。这是实际可辨识性差的一个标志，因为参数置信区域的体积（与 $\\sqrt{\\det(\\text{FIM})}$ 成反比）变得无限大。该陈述提供了正确的公式，并对可辨识性的影响得出了正确的结论。\n- **对 B 的裁定：** **正确**。\n\n**C. 离散化不会影响可辨识性，因为可辨识性是连续时间模型的一个结构性质；如果 $\\theta$ 在连续时间下是可辨识的，那么在任何离散采样下它都保持同样的可辨识性。**\n\n- 这个陈述混淆了两个不同的概念：*结构可辨识性*和*实际可辨识性*。\n- 结构可辨识性问的是在给定完美的、连续的数据的情况下，参数是否唯一。它确实是模型结构的一个属性。对于 $y(t) = A e^{-\\theta t}$，参数是结构可辨识的。\n- 实际可辨识性问的是能否从一组有限的、带噪声的离散数据中以有限的精度估计参数。它严重依赖于实验设计，包括采样方案（$\\Delta t, K$）和噪声水平（$\\sigma^2$）。\n- 如在 A 和 B 的分析中所证实的，选择一个差的采样方案（例如，大的 $\\Delta t$）会导致 FIM 近奇异，从而无法可靠地估计参数。因此，离散化和采样选择对实际可辨识性有深远的影响。\n- **对 C 的裁定：** **不正确**。\n\n**D. 细化采样（减小 $\\Delta t$）总是会增加灵敏度列 $s_A$ 和 $s_{\\theta}$ 之间的相关性，从而恶化 $\\theta$ 的可辨识性。**\n\n- 这个陈述与事实相反。如 A 和 B 所示，粗采样（大 $\\Delta t$）会增加相关性并恶化可辨识性。我们来考虑细化采样的极限，即 $\\Delta t \\to 0$。\n- 当 $\\Delta t \\to 0$ 时，我们有 $\\theta k \\Delta t \\to 0$，因此对于所有 $k$， $w_k = e^{-2\\theta k \\Delta t} \\to 1$。\n- 分布 $p_k = w_k / \\sum w_j$ 接近离散均匀分布，$p_k \\to 1/K$。\n- 相关性度量 $|\\rho|$ 趋近于一个小于 1 的值（对于 $K1$），这意味着比粗采样情况下的共线性更低。例如，当 $\\Delta t \\to 0$ 时， $|\\rho| \\to \\sqrt{\\frac{3(K+1)}{2(2K+1)}}$，对于 $K \\gg 1$，这个值约等于 $\\sqrt{3/4} \\approx 0.866$。\n- 由于粗采样导致 $|\\rho| \\to 1$，细化采样会*降低*相关性（使其远离 1），从而*改善*实际可辨识性。该陈述是错误的。\n- **对 D 的裁定：** **不正确**。\n\n**E. 如果模型中包含一个额外的恒定偏移参数 $B$，使得 $y_k = A e^{-\\theta k \\Delta t} + B + \\varepsilon_k$，那么粗采样会使对 $B$ 的灵敏度，即 $s_B(k) = 1$，与 $s_A(k)$ 正交，从而改善 $\\theta$ 的可辨识性。**\n\n- 新模型是 $y_k = A e^{-\\theta k \\Delta t} + B$。对 $B$ 的灵敏度向量的分量为 $s_B(k) = \\partial y_k / \\partial B = 1$。向量 $\\mathbf{s}_B$ 是一个全为 1 的向量。\n- 灵敏度向量 $\\mathbf{s}_A$ 保持不变，其分量为 $s_A(k) = e^{-\\theta k \\Delta t}$。\n- 正交性要求点积为零：$\\mathbf{s}_A^\\top \\mathbf{s}_B = \\sum_{k=1}^K s_A(k) s_B(k) = \\sum_{k=1}^K e^{-\\theta k \\Delta t}$。\n- 由于 $\\theta  0$ 且 $\\Delta t  0$，和中的每一项都严格为正。点积总是正的，所以这两个向量永远不会正交。\n- 我们考虑粗采样（$\\theta \\Delta t \\to \\infty$）的极限。在此极限下，对于所有 $k \\ge 1$，$e^{-\\theta k \\Delta t} \\to 0$。因此，向量 $\\mathbf{s}_A \\to \\mathbf{0}$。零向量在技术上与任何其他向量正交。然而，如果一个灵敏度向量变为零，这意味着观测值对该参数没有灵敏度。这使得参数（$A$）完全不可辨识。$\\theta$ 的可辨识性也消失了，因为 $\\mathbf{s}_{\\theta}$ 与 $k e^{-\\theta k \\Delta t}$ 这样的分量成正比，而这些分量也趋于零。\n- “改善 $\\theta$ 的可辨识性”这一结论是不正确的。灵敏度消失是可辨识性最坏的情况。\n- **对 E 的裁定：** **不正确**。",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "在灵敏度概念的基础上，本练习提供了一个用于识别不可辨识参数组合的计算框架。你将构建雅可比（或灵敏度）矩阵，并利用奇异值分解（SVD）来诊断其近秩亏问题，这直接对应于实际上的不可辨识性。通过测试从适定到退化的多种情景，该练习将帮助你建立关于参数可辨识性的直观理解。",
            "id": "3426732",
            "problem": "在一个数据同化场景中，给定一个参数化测量模型，其中时间 $t$ 的标量观测由 $y(t) = \\theta_1 e^{-\\theta_2 t} + \\theta_3$ 建模，参数向量为 $\\theta = (\\theta_1, \\theta_2, \\theta_3) \\in \\mathbb{R}^3$。目标是通过计算雅可比矩阵并分析其奇异结构，围绕一个标称参数向量进行局部理论可辨识性分析。\n\n使用以下基本依据：\n- 向量值函数关于其参数的雅可比矩阵 $J(\\theta)$ 按元素定义为 $J_{k,i}(\\theta) = \\frac{\\partial y(t_k)}{\\partial \\theta_i}$，其中 $\\{t_k\\}_{k=1}^m$ 是一个采样方案。\n- 在一个标称参数向量处的局部可辨识性，可由雅可比矩阵 $J(\\theta)$ 在该向量处具有满列秩来推断；近乎不可辨识的方向与 $J(\\theta)$ 的奇异值分解（SVD, Singular Value Decomposition）中的小奇异值相关联。\n\n您的任务：\n1. 从以上定义和模型 $y(t) = \\theta_1 e^{-\\theta_2 t} + \\theta_3$ 出发，针对给定的采样方案 $\\{t_k\\}$，推导在标称参数向量 $\\theta$ 处求值的雅可比矩阵 $J(\\theta)$。\n2. 计算 SVD 分解 $J(\\theta) = U \\Sigma V^\\top$，并将满足 $\\sigma_j \\leq \\tau \\, \\sigma_{\\max}$ 的奇异值 $\\sigma_j$ 所对应的右奇异向量确定为近零方向，其中 $\\sigma_{\\max}$ 是最大奇异值，$\\tau = 10^{-3}$（无量纲）。\n3. 对下面定义的每个测试用例，计算近零方向的整数数量（即根据上述阈值被分类为近零的奇异值的数量）。时间变量 $t$ 必须解释为秒，并以 $\\mathrm{s}$ 为单位提供。不涉及角度。输出中不需要物理单位。\n\n测试套件：\n- 用例 $1$（一般情况）：$\\theta = (2.0,\\,0.5,\\,0.1)$，采样方案 $t = [0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0]\\,\\mathrm{s}$。\n- 用例 $2$（$t=0$ 时的退化情况）：$\\theta = (1.5,\\,0.4,\\,-0.2)$，采样方案 $t = [0.0,\\,0.0,\\,0.0]\\,\\mathrm{s}$。\n- 用例 $3$（快速衰减，延迟采样）：$\\theta = (1.0,\\,5.0,\\,0.0)$，采样方案 $t = [2.0,\\,3.0,\\,4.0,\\,5.0]\\,\\mathrm{s}$。\n- 用例 $4$（最小采样，良态）：$\\theta = (1.0,\\,0.7,\\,0.2)$，采样方案 $t = [0.2,\\,0.8,\\,1.7]\\,\\mathrm{s}$。\n\n最终输出规范：\n- 您的程序应计算每个测试用例的近零方向数量，并生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按用例 1 到 4 的顺序排列（例如，$[n_1,n_2,n_3,n_4]$）。条目 $n_i$ 必须是整数。",
            "solution": "该问题要求对给定的参数化模型进行局部理论可辨识性分析。该分析通过计算一组测量值相对于模型参数的雅可比矩阵，然后分析其奇异值谱来执行。问题陈述是有效的，解题过程如下。\n\n### 步骤 1：推导雅可比矩阵\n\n测量模型被给出为时间 $t$ 和参数向量 $\\theta = (\\theta_1, \\theta_2, \\theta_3) \\in \\mathbb{R}^3$ 的标量函数：\n$$ y(t) = \\theta_1 e^{-\\theta_2 t} + \\theta_3 $$\n我们考虑在采样方案 $\\{t_k\\}_{k=1}^m$ 指定的离散时间点上获取的 $m$ 个测量值。在时间 $t_k$ 的测量值为 $y(t_k)$。\n\n雅可比矩阵 $J(\\theta)$ 量化了测量值对参数无穷小变化的敏感度。其元素定义为 $J_{k,i}(\\theta) = \\frac{\\partial y(t_k)}{\\partial \\theta_i}$。该矩阵有 $m$ 行（每个时间样本一行）和 3 列（每个参数一列）。\n\n为构建雅可比矩阵，我们计算 $y(t)$ 对每个参数 $\\theta_i$ 的偏导数：\n\n1.  **关于 $\\theta_1$ 的偏导数**：\n    $$ \\frac{\\partial y(t)}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\left( \\theta_1 e^{-\\theta_2 t} + \\theta_3 \\right) = e^{-\\theta_2 t} $$\n\n2.  **关于 $\\theta_2$ 的偏导数**：\n    $$ \\frac{\\partial y(t)}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2} \\left( \\theta_1 e^{-\\theta_2 t} + \\theta_3 \\right) = \\theta_1 \\frac{\\partial}{\\partial \\theta_2} \\left( e^{-\\theta_2 t} \\right) = \\theta_1 ( -t e^{-\\theta_2 t} ) = -t \\theta_1 e^{-\\theta_2 t} $$\n\n3.  **关于 $\\theta_3$ 的偏导数**：\n    $$ \\frac{\\partial y(t)}{\\partial \\theta_3} = \\frac{\\partial}{\\partial \\theta_3} \\left( \\theta_1 e^{-\\theta_2 t} + \\theta_3 \\right) = 1 $$\n\n组合这些偏导数，雅可比矩阵的第 $k$ 行（对应于时间样本 $t_k$）由向量 $\\left[ e^{-\\theta_2 t_k}, -t_k \\theta_1 e^{-\\theta_2 t_k}, 1 \\right]$ 给出。因此，完整的 $m \\times 3$ 雅可比矩阵 $J(\\theta)$ 为：\n$$\nJ(\\theta) =\n\\begin{pmatrix}\n\\frac{\\partial y(t_1)}{\\partial \\theta_1}  \\frac{\\partial y(t_1)}{\\partial \\theta_2}  \\frac{\\partial y(t_1)}{\\partial \\theta_3} \\\\\n\\frac{\\partial y(t_2)}{\\partial \\theta_1}  \\frac{\\partial y(t_2)}{\\partial \\theta_2}  \\frac{\\partial y(t_2)}{\\partial \\theta_3} \\\\\n\\vdots  \\vdots  \\vdots \\\\\n\\frac{\\partial y(t_m)}{\\partial \\theta_1}  \\frac{\\partial y(t_m)}{\\partial \\theta_2}  \\frac{\\partial y(t_m)}{\\partial \\theta_3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ne^{-\\theta_2 t_1}  -t_1 \\theta_1 e^{-\\theta_2 t_1}  1 \\\\\ne^{-\\theta_2 t_2}  -t_2 \\theta_1 e^{-\\theta_2 t_2}  1 \\\\\n\\vdots  \\vdots  \\vdots \\\\\ne^{-\\theta_2 t_m}  -t_m \\theta_1 e^{-\\theta_2 t_m}  1\n\\end{pmatrix}\n$$\n\n### 步骤 2：通过奇异值分解 (SVD) 进行可辨识性分析\n\n在标称参数向量 $\\theta$ 处的局部参数可辨识性取决于雅可比矩阵 $J(\\theta)$ 是否具有满列秩。如果 $J(\\theta)$ 是秩亏的，则其列是线性相关的。这意味着存在一个非零的参数变化线性组合 $\\Delta\\theta$，使得 $J(\\theta) \\Delta\\theta = 0$。这说明参数扰动的某种组合不会导致模型输出发生变化，从而使得这些参数从数据中无法被区分。\n\n奇异值分解（SVD）是分析矩阵的秩和近秩亏性的一个稳健的数值工具。$m \\times 3$ 雅可比矩阵的 SVD 为 $J(\\theta) = U \\Sigma V^\\top$，其中：\n- $U$ 是一个 $m \\times m$ 的正交矩阵，其列是左奇异向量。\n- $\\Sigma$ 是一个 $m \\times 3$ 的对角矩阵，包含非负奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$。\n- $V^\\top$ 是一个 $3 \\times 3$ 的正交矩阵，其行是右奇异向量的转置。\n\n非零奇异值的数量等于矩阵的秩。小的奇异值表明矩阵接近秩亏。相应的右奇异向量（$V$ 的列）指出了参数空间 $(\\theta_1, \\theta_2, \\theta_3)$ 中模型输出不敏感的方向。这些方向被称为“近零方向”。\n\n问题定义了一个用于识别这些方向的定量标准：如果一个奇异值 $\\sigma_j$ 满足不等式：\n$$ \\sigma_j \\le \\tau \\sigma_{\\max} $$\n则认为它足够小，可以指示一个近零方向，其中 $\\sigma_{\\max}$ 是最大奇异值，阈值因子给定为 $\\tau = 10^{-3}$。任务是计算 3 个奇异值中有多少个满足此条件。\n\n### 步骤 3：测试用例的算法流程\n\n通过将推导出的公式应用于每个指定的测试用例来实施解决方案。算法如下：\n\n1.  对于一个给定的测试用例，包含一个标称参数向量 $\\theta = (\\theta_1, \\theta_2, \\theta_3)$ 和一个采样方案 $t = [t_1, t_2, \\ldots, t_m]$，使用步骤 1 中推导的表达式构建数值雅可比矩阵 $J$。\n2.  计算矩阵 $J$ 的奇异值。对于一个 $m \\times 3$ 矩阵，这将产生 $\\min(m, 3)=3$ 个奇异值（因为所有测试用例中 $m \\ge 3$）。用向量 $s = [\\sigma_1, \\sigma_2, \\sigma_3]$ 表示这些奇异值。\n3.  如果所有奇异值都为零，则所有方向都是零方向。计数为 $3$。\n4.  否则，找到最大奇异值 $\\sigma_{\\max} = \\max(s)$。\n5.  计算阈值 $T = \\tau \\sigma_{\\max} = 10^{-3} \\sigma_{\\max}$。\n6.  计算 $s$ 中满足 $\\sigma_j \\le T$ 的奇异值 $\\sigma_j$ 的数量。这个整数计数就是该测试用例的结果。\n\n此流程被系统地应用于问题陈述中提供的所有四个测试用例。\n- **用例 1** 是一个一般的、良态的场景，预期矩阵为满秩，因此近零方向的数量为 $0$。\n- **用例 2** 展示了一个退化的采样方案，其中所有 $t_k=0$。这使得雅可比矩阵的所有行都相同：$[1, 0, 1]$。得到的 $3 \\times 3$ 矩阵的秩为 $1$，其零度为 $3-1=2$。因此，将有 $2$ 个奇异值精确为零，导致计数为 $2$。\n- **用例 3** 涉及快速衰减率 $(\\theta_2 = 5.0)$ 和延迟的采样时间。项 $e^{-\\theta_2 t_k}$ 变得可以忽略不计。依赖于此项的雅可比矩阵的前两列变成了近乎零的向量。雅可比矩阵近似为一个有两列零向量和一列全为1的向量的矩阵，使其秩为 $1$。$\\theta_1$ 和 $\\theta_2$ 的这种实际上的不可辨识性将表现为两个非常小的奇异值，从而导致计数为 $2$。\n- **用例 4** 使用了辨识 $3$ 个参数所需的最少样本数 ($m=3$)。采样时间和参数值的选择使得雅可比矩阵成为一个良态方阵，因此预期没有近零方向（计数为 $0$）。\n\n下面的 Python 程序实现了这一逻辑来计算最终答案。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the theoretical identifiability analysis problem for four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'theta': np.array([2.0, 0.5, 0.1]), 't': np.array([0.0, 0.5, 1.0, 1.5, 2.0])},\n        {'theta': np.array([1.5, 0.4, -0.2]), 't': np.array([0.0, 0.0, 0.0])},\n        {'theta': np.array([1.0, 5.0, 0.0]), 't': np.array([2.0, 3.0, 4.0, 5.0])},\n        {'theta': np.array([1.0, 0.7, 0.2]), 't': np.array([0.2, 0.8, 1.7])}\n    ]\n\n    tau = 1e-3\n    results = []\n\n    for case in test_cases:\n        theta = case['theta']\n        t_samples = case['t']\n        \n        theta1, theta2, _ = theta\n        \n        # 1. Derive the Jacobian matrix J(theta)\n        # J_ki = dy(t_k)/d(theta_i)\n        # The columns of the Jacobian are the partial derivatives with respect to each parameter,\n        # evaluated at each time sample t_k.\n        \n        # Column 1: Partial derivative with respect to theta_1\n        # d/d(theta_1) [theta_1 * exp(-theta_2*t) + theta_3] = exp(-theta_2*t)\n        col1 = np.exp(-theta2 * t_samples)\n        \n        # Column 2: Partial derivative with respect to theta_2\n        # d/d(theta_2) [theta_1 * exp(-theta_2*t) + theta_3] = -t * theta_1 * exp(-theta_2*t)\n        col2 = -t_samples * theta1 * np.exp(-theta2 * t_samples)\n        \n        # Column 3: Partial derivative with respect to theta_3\n        # d/d(theta_3) [theta_1 * exp(-theta_2*t) + theta_3] = 1\n        col3 = np.ones_like(t_samples)\n        \n        jacobian = np.c_[col1, col2, col3]\n        \n        # 2. Compute the SVD and determine near-null directions\n        \n        # The number of singular values is min(m, n) where J is m x n.\n        # Here n = 3 and m = len(t_samples) >= 3, so there are always 3 singular values.\n        singular_values = np.linalg.svd(jacobian, compute_uv=False)\n        \n        if singular_values.size == 0:\n            # This case shouldn't happen with the given problem constraints\n            # but is included for robustness.\n            count = 0\n        else:\n            sigma_max = np.max(singular_values)\n            \n            # If sigma_max is 0, the matrix is a zero matrix. All directions are null.\n            if sigma_max > 0:\n                threshold = tau * sigma_max\n                count = np.sum(singular_values = threshold)\n            else:\n                # All singular values are 0, so all are counted.\n                count = len(singular_values)\n                \n        results.append(int(count))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "线性灵敏度分析之外的另一种方法是直接研究代价函数曲面的几何形状。本练习引入了剖面代价函数（profile cost function）的概念，该函数通过优化掉部分参数来可视化其余参数的代价景观。通过推导剖面代价函数并计算其在最小值点的曲率，你可以直接评估一个参数的实际可辨识性。",
            "id": "3426692",
            "problem": "考虑一个具有独立同分布（IID）高斯测量噪声的标量输出模型的数据同化问题。在已知时间 $t_i \\in \\mathbb{R}_{0}$，您从参数模型 $y(t)=\\theta_1 \\exp(-\\theta_2 t)$ 观测到数据 $\\{(t_i,y_i)\\}_{i=1}^{n}$，其观测噪声建模为 $y_i = \\theta_1 \\exp(-\\theta_2 t_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立同分布的，且 $\\sigma^20$ 已知。考虑负对数似然（不计一个与参数无关的加性常数），等价于最小二乘代价\n$$\nJ(\\theta_1,\\theta_2) \\;=\\; \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2.\n$$\n在理论可辨识性分析中，研究者通过对每个固定的 $\\theta_2$ 就 $\\theta_1$ 最小化 $J(\\theta_1,\\theta_2)$ 来研究 $\\theta_2$ 的剖面代价，然后分析此一维剖面代价在其最小化子处的曲率，以评估局部实际可辨识性。\n\n仅从最小二乘法的定义和基础微积分出发，不引用任何现成的可辨识性公式，完成以下任务：\n- 对于固定的 $\\theta_2$，显式推导使 $J(\\theta_1,\\theta_2)$ 最小化的最优 $\\theta_1^{\\star}(\\theta_2)$。\n- 将 $\\theta_1^{\\star}(\\theta_2)$ 代回 $J$ 中，以数据 $\\{(t_i,y_i)\\}$、噪声方差 $\\sigma^2$ 和 $\\theta_2$ 的形式，获得剖面代价 $J_{\\mathrm{p}}(\\theta_2) \\equiv J\\big(\\theta_1^{\\star}(\\theta_2),\\theta_2\\big)$ 的完全显式闭式表达式。\n- 设 $\\widehat{\\theta}_2$ 是 $J_{\\mathrm{p}}(\\theta_2)$ 在 $\\theta_2 \\in \\mathbb{R}_{0}$ 上的任意最小化子。计算在 $\\theta_2=\\widehat{\\theta}_2$ 处求值的精确二阶导数 $\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)$，并将结果完全用 $\\{(t_i,y_i)\\}$、$\\sigma^2$ 和 $\\widehat{\\theta}_2$ 表示。您的推导过程必须是显式的且自洽的。\n\n以单个解析表达式的形式提供您的最终答案，该表达式由一个包含两个条目的行向量组成：第一个条目是显式的 $J_{\\mathrm{p}}(\\theta_2)$，第二个条目是曲率 $\\left.\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)\\right|_{\\theta_2=\\widehat{\\theta}_2}$。不需要进行数值评估，也不需要四舍五入。最终答案中不要包含单位。",
            "solution": "用户希望解决一个关于给定参数模型的理论可辨识性分析问题。该问题分为三个部分：求一个参数 $\\theta_1$ 作为另一个参数 $\\theta_2$ 的函数时的最优值，推导 $\\theta_2$ 的剖面代价函数，以及计算该剖面代价在其最小值处的曲率。\n\n### 步骤1：提取已知条件\n-   **数据：** 一组在已知时间 $t_i \\in \\mathbb{R}_{0}$ 的 $n$ 个观测值 $\\{(t_i, y_i)\\}_{i=1}^{n}$。\n-   **参数模型：** $y(t) = \\theta_1 \\exp(-\\theta_2 t)$。\n-   **观测模型：** $y_i = \\theta_1 \\exp(-\\theta_2 t_i) + \\varepsilon_i$。\n-   **噪声模型：** 测量误差 $\\varepsilon_i$ 是独立同分布（IID）的，服从高斯分布 $\\mathcal{N}(0, \\sigma^2)$，其中已知方差 $\\sigma^2  0$。\n-   **代价函数：** 负对数似然或最小二乘代价函数由下式给出：\n    $$\n    J(\\theta_1, \\theta_2) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2.\n    $$\n-   **剖面代价：** $J_{\\mathrm{p}}(\\theta_2) \\equiv J\\big(\\theta_1^{\\star}(\\theta_2), \\theta_2\\big)$，其中 $\\theta_1^{\\star}(\\theta_2)$ 是对于固定的 $\\theta_2$ 使 $J(\\theta_1, \\theta_2)$ 最小化的值。\n-   **剖面代价的最小化子：** $\\widehat{\\theta}_2$ 是 $J_{\\mathrm{p}}(\\theta_2)$ 在 $\\theta_2 \\in \\mathbb{R}_{0}$ 上的最小化子。\n\n### 步骤2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估：\n-   **科学基础：** 该问题是非线性回归和参数估计中的一个标准练习，是统计学、数据同化和反问题中的一个基本课题。模型是一个简单的指数衰减模型，代价函数是从高斯噪声假设推导出的标准最小二乘目标函数。它牢固地建立在公认的数学和统计原理之上。\n-   **适定性：** 问题陈述清晰，任务在数学上是适定的。假设了最小化子的存在性，这对此类问题是典型的。目标是推导解析表达式，这是一个适定的数学任务。\n-   **客观性：** 语言精确，使用了标准的数学术语。没有主观或基于观点的陈述。\n-   **完整性：** 问题提供了进行所需推导的所有必要信息（模型、代价函数、定义）。\n-   **未检测到其他缺陷：** 该问题不违反任何其他无效性标准。\n\n### 步骤3：结论与行动\n该问题被判定为**有效**。将提供一个完整的、有理有据的解答。\n\n### 解答推导\n\n将按要求分三部分推导解答。\n\n**第1部分：最优 $\\theta_1^{\\star}(\\theta_2)$ 的推导**\n\n对于固定的 $\\theta_2$ 值，代价函数 $J(\\theta_1, \\theta_2)$ 是关于 $\\theta_1$ 的二次函数。为求其最小化子 $\\theta_1^{\\star}(\\theta_2)$，我们计算 $J$ 对 $\\theta_1$ 的偏导数并令其为零。\n\n$$\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2 \\right]\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2 \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big) \\cdot \\big(-\\exp(-\\theta_2 t_i)\\big)\n$$\n将此导数设为零：\n$$\n-\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\big(y_i \\exp(-\\theta_2 t_i) - \\theta_1 \\exp(-2\\theta_2 t_i)\\big) = 0\n$$\n由于 $\\sigma^2  0$，我们可以乘以 $-\\sigma^2$：\n$$\n\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) - \\theta_1 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) = 0\n$$\n求解 $\\theta_1$ 得到最优值 $\\theta_1^{\\star}(\\theta_2)$：\n$$\n\\theta_1^{\\star}(\\theta_2) = \\frac{\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)}\n$$\n为确认这是一个最小值，我们检查二阶偏导数：\n$$\n\\frac{\\partial^2 J}{\\partial \\theta_1^2} = \\frac{\\partial}{\\partial \\theta_1} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\big(\\theta_1 \\exp(-2\\theta_2 t_i) - y_i \\exp(-\\theta_2 t_i)\\big) \\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\n$$\n由于 $t_i  0$ 并且 $\\theta_2$ 在 $\\mathbb{R}_{0}$ 中寻求，所以 $\\exp(-2\\theta_2 t_i)  0$。因此，该和为正，$\\frac{\\partial^2 J}{\\partial \\theta_1^2}$ 也为正。这证实了 $\\theta_1^{\\star}(\\theta_2)$ 确实是一个最小化子。\n\n**第2部分：剖面代价 $J_{\\mathrm{p}}(\\theta_2)$ 的推导**\n\n我们将 $\\theta_1^{\\star}(\\theta_2)$ 代回代价函数 $J(\\theta_1, \\theta_2)$ 中：\n$$\nJ_{\\mathrm{p}}(\\theta_2) = J\\big(\\theta_1^{\\star}(\\theta_2), \\theta_2\\big) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1^{\\star}(\\theta_2) \\exp(-\\theta_2 t_i)\\big)^2\n$$\n展开平方项：\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left( y_i^2 - 2y_i \\theta_1^{\\star}(\\theta_2) \\exp(-\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\exp(-2\\theta_2 t_i) \\right)\n$$\n分配求和符号：\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2\\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n从 $\\theta_1^{\\star}(\\theta_2)$ 的表达式可知，$\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) = \\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)$。将此代入中间项：\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2\\theta_1^{\\star}(\\theta_2) \\left(\\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\\right) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2(\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n代入 $\\theta_1^{\\star}(\\theta_2)$ 的完整表达式：\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\left(\\frac{\\sum_{j=1}^{n} y_j \\exp(-\\theta_2 t_j)}{\\sum_{k=1}^{n} \\exp(-2\\theta_2 t_k)}\\right)^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n化简后得到剖面代价的显式闭式表达式：\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\frac{\\left(\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)\\right)^2}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)} \\right)\n$$\n\n**第3部分：曲率 $\\left.\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)\\right|_{\\theta_2=\\widehat{\\theta}_2}$ 的推导**\n\n为求剖面代价的二阶导数，我们可以使用基于包络定理的一个结果。剖面代价的一阶导数由原代价函数对 $\\theta_2$ 的偏导数给出，并在最优的 $\\theta_1^{\\star}(\\theta_2)$ 处求值：\n$$\n\\frac{d J_{\\mathrm{p}}}{d\\theta_2} = \\left. \\frac{\\partial J(\\theta_1, \\theta_2)}{\\partial \\theta_2} \\right|_{\\theta_1=\\theta_1^{\\star}(\\theta_2)}\n$$\n使用链式法则再次对 $\\theta_2$ 求导，得到二阶导数：\n$$\n\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} + \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2} \\frac{d\\theta_1^{\\star}}{d\\theta_2}\n$$\n其中所有偏导数都在 $(\\theta_1^{\\star}(\\theta_2), \\theta_2)$ 处求值。导数 $\\frac{d\\theta_1^{\\star}}{d\\theta_2}$ 通过对一阶最优性条件 $\\frac{\\partial J}{\\partial \\theta_1}(\\theta_1^{\\star}(\\theta_2), \\theta_2) = 0$ 关于 $\\theta_2$ 求导得到：\n$$\n\\frac{\\partial^2 J}{\\partial \\theta_1^2} \\frac{d\\theta_1^{\\star}}{d\\theta_2} + \\frac{\\partial^2 J}{\\partial \\theta_2 \\partial \\theta_1} = 0 \\implies \\frac{d\\theta_1^{\\star}}{d\\theta_2} = - \\left(\\frac{\\partial^2 J}{\\partial \\theta_1^2}\\right)^{-1} \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2}\n$$\n将此代回，我们得到以 $J$ 的 Hessian 矩阵分量表示的曲率公式：\n$$\n\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} - \\frac{\\left(\\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2}\\right)^2}{\\frac{\\partial^2 J}{\\partial \\theta_1^2}}\n$$\n我们需要在 $\\theta_2 = \\widehat{\\theta}_2$ 处计算该式。点 $(\\widehat{\\theta}_1, \\widehat{\\theta}_2)$，其中 $\\widehat{\\theta}_1 = \\theta_1^{\\star}(\\widehat{\\theta}_2)$，是完整代价函数 $J(\\theta_1, \\theta_2)$ 的一个最小化子。因此，$J$ 的梯度在该点为零。特别地，$\\left.\\frac{\\partial J}{\\partial \\theta_2}\\right|_{(\\widehat{\\theta}_1, \\widehat{\\theta}_2)} = 0$，这意味着：\n$$\n\\sum_{i=1}^{n} (y_i - \\widehat{\\theta}_1 \\exp(-\\widehat{\\theta}_2 t_i)) t_i \\exp(-\\widehat{\\theta}_2 t_i) = 0 \\implies \\sum_{i=1}^{n} y_i t_i \\exp(-\\widehat{\\theta}_2 t_i) = \\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i)\n$$\n我们现在计算 $J$ 所需的二阶偏导数：\n$$\nJ_{\\theta_1\\theta_1} = \\frac{\\partial^2 J}{\\partial \\theta_1^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\n$$\n$$\nJ_{\\theta_1\\theta_2} = \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i t_i \\exp(-\\theta_2 t_i) - 2\\theta_1 t_i \\exp(-2\\theta_2 t_i))\n$$\n在 $(\\widehat{\\theta}_1, \\widehat{\\theta}_2)$ 处，使用上面推导的最优性条件：\n$$\n\\left.J_{\\theta_1\\theta_2}\\right|_{(\\widehat{\\theta}_1, \\widehat{\\theta}_2)} = \\frac{1}{\\sigma^2} \\left( \\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i) - 2\\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i) \\right) = -\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i)\n$$\n$$\nJ_{\\theta_2\\theta_2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} = \\frac{\\partial}{\\partial \\theta_2} \\left( \\frac{\\theta_1}{\\sigma^2} \\sum_{i=1}^n (y_i t_i e^{-\\theta_2 t_i} - \\theta_1 t_i e^{-2\\theta_2 t_i}) \\right) = -\\frac{\\theta_1}{\\sigma^2} \\sum_{i=1}^n y_i t_i^2 e^{-\\theta_2 t_i} + \\frac{2\\theta_1^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 e^{-2\\theta_2 t_i}\n$$\n现在，组合曲率表达式并在 $\\theta_2 = \\widehat{\\theta}_2$ 处（因此 $\\theta_1 = \\widehat{\\theta}_1$）求值：\n$$\n\\left.\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2}\\right|_{\\widehat{\\theta}_2} = \\left( -\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i} + \\frac{2\\widehat{\\theta}_1^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} \\right) - \\frac{\\left(-\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i}\\right)^2}{\\frac{1}{\\sigma^2} \\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}}\n$$\n$$\n= \\frac{1}{\\sigma^2} \\left( -\\widehat{\\theta}_1 \\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i} + 2\\widehat{\\theta}_1^2 \\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} - \\widehat{\\theta}_1^2 \\frac{\\left( \\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i} \\right)^2}{\\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}} \\right)\n$$\n最后，我们代入 $\\widehat{\\theta}_1 = \\theta_1^{\\star}(\\widehat{\\theta}_2) = \\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}$，将结果完全用数据、$\\sigma^2$ 和 $\\widehat{\\theta}_2$ 表示：\n$$\n\\frac{1}{\\sigma^2} \\left[ \\left(\\frac{\\sum_{j=1}^n y_j e^{-\\widehat{\\theta}_2 t_j}}{\\sum_{k=1}^n e^{-2\\widehat{\\theta}_2 t_k}}\\right) \\left(-\\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i}\\right) + \\left(\\frac{\\sum_{j=1}^n y_j e^{-\\widehat{\\theta}_2 t_j}}{\\sum_{k=1}^n e^{-2\\widehat{\\theta}_2 t_k}}\\right)^2 \\left(2\\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} - \\frac{(\\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i})^2}{\\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}}\\right) \\right]\n$$\n这就是所要求的显式表达式。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\frac{\\left(\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)\\right)^2}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)} \\right)  \\frac{1}{\\sigma^2} \\left[ \\left(\\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}\\right)^2 \\left(2\\sum_{i=1}^n t_i^2 \\exp(-2\\widehat{\\theta}_2 t_i) - \\frac{\\left(\\sum_{i=1}^n t_i \\exp(-2\\widehat{\\theta}_2 t_i)\\right)^2}{\\sum_{i=1}^n \\exp(-2\\widehat{\\theta}_2 t_i)}\\right) - \\left(\\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}\\right) \\sum_{i=1}^n y_i t_i^2 \\exp(-\\widehat{\\theta}_2 t_i) \\right] \\end{pmatrix}}\n$$"
        }
    ]
}