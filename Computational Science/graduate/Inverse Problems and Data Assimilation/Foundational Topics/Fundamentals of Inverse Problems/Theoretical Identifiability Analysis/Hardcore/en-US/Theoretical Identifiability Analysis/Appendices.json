{
    "hands_on_practices": [
        {
            "introduction": "The shape of the likelihood or cost function surface holds the key to understanding parameter uncertainty, where flat directions correspond to non-identifiable parameter combinations. This exercise provides a concrete, first-principles derivation of a profile cost function, a powerful tool for visualizing and analyzing the identifiability of a single parameter in a multi-parameter model . By explicitly calculating the curvature of this profile, you will build a direct connection between the geometry of the cost surface and the statistical precision of a parameter estimate.",
            "id": "3426692",
            "problem": "Consider data assimilation for a scalar output model with independent and identically distributed (IID) Gaussian measurement noise. You observe data $\\{(t_i,y_i)\\}_{i=1}^{n}$ at known times $t_i \\in \\mathbb{R}_{>0}$ from the parametric model $y(t)=\\theta_1 \\exp(-\\theta_2 t)$, with observation noise modeled as $y_i = \\theta_1 \\exp(-\\theta_2 t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are IID and $\\sigma^2>0$ is known. Consider the negative log-likelihood (up to an additive constant independent of parameters), equivalently the least-squares cost\n$$\nJ(\\theta_1,\\theta_2) \\;=\\; \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2.\n$$\nIn theoretical identifiability analysis, one studies the profile cost for $\\theta_2$ by minimizing $J(\\theta_1,\\theta_2)$ over $\\theta_1$ for each fixed $\\theta_2$, and then analyzing the curvature of this one-dimensional profile cost at its minimizer to assess local practical identifiability.\n\nStarting only from the definitions of least squares and basic calculus, and without invoking any pre-packaged identifiability formulas, do the following:\n- Derive explicitly the optimal $\\theta_1^{\\star}(\\theta_2)$ that minimizes $J(\\theta_1,\\theta_2)$ for a fixed $\\theta_2$.\n- Substitute $\\theta_1^{\\star}(\\theta_2)$ back into $J$ to obtain a fully explicit, closed-form expression for the profile cost $J_{\\mathrm{p}}(\\theta_2) \\equiv J\\big(\\theta_1^{\\star}(\\theta_2),\\theta_2\\big)$ in terms of the data $\\{(t_i,y_i)\\}$, the noise variance $\\sigma^2$, and $\\theta_2$.\n- Let $\\widehat{\\theta}_2$ be any minimizer of $J_{\\mathrm{p}}(\\theta_2)$ over $\\theta_2 \\in \\mathbb{R}_{>0}$. Compute the exact second derivative $\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)$ evaluated at $\\theta_2=\\widehat{\\theta}_2$, expressing the result entirely in terms of $\\{(t_i,y_i)\\}$, $\\sigma^2$, and $\\widehat{\\theta}_2$. Your derivation must be explicit and self-contained.\n\nProvide your final answer as a single analytic expression, consisting of a row with two entries: first the explicit $J_{\\mathrm{p}}(\\theta_2)$ and second the curvature $\\left.\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)\\right|_{\\theta_2=\\widehat{\\theta}_2}$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final answer.",
            "solution": "The user wants to solve a problem related to the theoretical identifiability analysis for a given parametric model. The problem is divided into three parts: finding the optimal value of a parameter $\\theta_1$ as a function of another parameter $\\theta_2$, deriving the profile cost function for $\\theta_2$, and calculating the curvature of this profile cost at its minimum.\n\n### Step 1: Extract Givens\n-   **Data:** A set of $n$ observations $\\{(t_i, y_i)\\}_{i=1}^{n}$ with known times $t_i \\in \\mathbb{R}_{>0}$.\n-   **Parametric Model:** $y(t) = \\theta_1 \\exp(-\\theta_2 t)$.\n-   **Observation Model:** $y_i = \\theta_1 \\exp(-\\theta_2 t_i) + \\varepsilon_i$.\n-   **Noise Model:** Measurement errors $\\varepsilon_i$ are independent and identically distributed (IID) as a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$, with known variance $\\sigma^2 > 0$.\n-   **Cost Function:** The negative log-likelihood or least-squares cost function is given by:\n    $$\n    J(\\theta_1, \\theta_2) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2.\n    $$\n-   **Profile Cost:** $J_{\\mathrm{p}}(\\theta_2) \\equiv J\\big(\\theta_1^{\\star}(\\theta_2), \\theta_2\\big)$, where $\\theta_1^{\\star}(\\theta_2)$ minimizes $J(\\theta_1, \\theta_2)$ for a fixed $\\theta_2$.\n-   **Minimizer of Profile Cost:** $\\widehat{\\theta}_2$ is a minimizer of $J_{\\mathrm{p}}(\\theta_2)$ over $\\theta_2 \\in \\mathbb{R}_{>0}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded:** The problem is a standard exercise in nonlinear regression and parameter estimation, a fundamental topic in statistics, data assimilation, and inverse problems. The model is a simple exponential decay, and the cost function is the standard least-squares objective derived from a Gaussian noise assumption. It is firmly based on established mathematical and statistical principles.\n-   **Well-Posed:** The problem statement is clear and the tasks are mathematically well-defined. The existence of minimizers is assumed, which is typical for such problems. The objective is to derive analytical expressions, which is a well-posed mathematical task.\n-   **Objective:** The language is precise, using standard mathematical terminology. There are no subjective or opinion-based statements.\n-   **Completeness:** The problem provides all necessary information (model, cost function, definitions) to perform the required derivations.\n-   **No other flaws detected:** The problem does not violate any of the other invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution will be derived in three parts as requested.\n\n**Part 1: Derivation of the optimal $\\theta_1^{\\star}(\\theta_2)$**\n\nFor a fixed value of $\\theta_2$, the cost function $J(\\theta_1, \\theta_2)$ is a quadratic function of $\\theta_1$. To find the minimizer $\\theta_1^{\\star}(\\theta_2)$, we compute the partial derivative of $J$ with respect to $\\theta_1$ and set it to zero.\n\n$$\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2 \\right]\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2 \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big) \\cdot \\big(-\\exp(-\\theta_2 t_i)\\big)\n$$\nSetting this derivative to zero:\n$$\n-\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\big(y_i \\exp(-\\theta_2 t_i) - \\theta_1 \\exp(-2\\theta_2 t_i)\\big) = 0\n$$\nSince $\\sigma^2 > 0$, we can multiply by $-\\sigma^2$:\n$$\n\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) - \\theta_1 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) = 0\n$$\nSolving for $\\theta_1$ yields the optimal value $\\theta_1^{\\star}(\\theta_2)$:\n$$\n\\theta_1^{\\star}(\\theta_2) = \\frac{\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)}\n$$\nTo confirm this is a minimum, we check the second partial derivative:\n$$\n\\frac{\\partial^2 J}{\\partial \\theta_1^2} = \\frac{\\partial}{\\partial \\theta_1} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\big(\\theta_1 \\exp(-2\\theta_2 t_i) - y_i \\exp(-\\theta_2 t_i)\\big) \\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\n$$\nSince $t_i > 0$ and $\\theta_2$ is sought in $\\mathbb{R}_{>0}$, $\\exp(-2\\theta_2 t_i) > 0$. The sum is therefore positive, and so is $\\frac{\\partial^2 J}{\\partial \\theta_1^2}$. This confirms that $\\theta_1^{\\star}(\\theta_2)$ is indeed a minimizer.\n\n**Part 2: Derivation of the profile cost $J_{\\mathrm{p}}(\\theta_2)$**\n\nWe substitute $\\theta_1^{\\star}(\\theta_2)$ back into the cost function $J(\\theta_1, \\theta_2)$:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = J\\big(\\theta_1^{\\star}(\\theta_2), \\theta_2\\big) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1^{\\star}(\\theta_2) \\exp(-\\theta_2 t_i)\\big)^2\n$$\nExpanding the square:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left( y_i^2 - 2y_i \\theta_1^{\\star}(\\theta_2) \\exp(-\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\exp(-2\\theta_2 t_i) \\right)\n$$\nDistributing the summation:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2\\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\nFrom the expression for $\\theta_1^{\\star}(\\theta_2)$, we have $\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) = \\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)$. Substituting this into the middle term:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2\\theta_1^{\\star}(\\theta_2) \\left(\\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\\right) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2(\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\nSubstituting the full expression for $\\theta_1^{\\star}(\\theta_2)$:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\left(\\frac{\\sum_{j=1}^{n} y_j \\exp(-\\theta_2 t_j)}{\\sum_{k=1}^{n} \\exp(-2\\theta_2 t_k)}\\right)^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\nWhich simplifies to the explicit closed-form expression for the profile cost:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\frac{\\left(\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)\\right)^2}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)} \\right)\n$$\n\n**Part 3: Derivation of the curvature $\\left.\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)\\right|_{\\theta_2=\\widehat{\\theta}_2}$**\n\nTo find the second derivative of the profile cost, we can use a result based on the envelope theorem. The first derivative of the profile cost is given by the partial derivative of the original cost function with respect to $\\theta_2$, evaluated at the optimal $\\theta_1^{\\star}(\\theta_2)$:\n$$\n\\frac{d J_{\\mathrm{p}}}{d\\theta_2} = \\left. \\frac{\\partial J(\\theta_1, \\theta_2)}{\\partial \\theta_2} \\right|_{\\theta_1=\\theta_1^{\\star}(\\theta_2)}\n$$\nDifferentiating this again with respect to $\\theta_2$ using the chain rule gives the second derivative:\n$$\n\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} + \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2} \\frac{d\\theta_1^{\\star}}{d\\theta_2}\n$$\nwhere all partial derivatives are evaluated at $(\\theta_1^{\\star}(\\theta_2), \\theta_2)$. The derivative $\\frac{d\\theta_1^{\\star}}{d\\theta_2}$ is found by differentiating the first-order optimality condition $\\frac{\\partial J}{\\partial \\theta_1}(\\theta_1^{\\star}(\\theta_2), \\theta_2) = 0$ with respect to $\\theta_2$:\n$$\n\\frac{\\partial^2 J}{\\partial \\theta_1^2} \\frac{d\\theta_1^{\\star}}{d\\theta_2} + \\frac{\\partial^2 J}{\\partial \\theta_2 \\partial \\theta_1} = 0 \\implies \\frac{d\\theta_1^{\\star}}{d\\theta_2} = - \\left(\\frac{\\partial^2 J}{\\partial \\theta_1^2}\\right)^{-1} \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2}\n$$\nSubstituting this back, we get the formula for the curvature in terms of the Hessian components of $J$:\n$$\n\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} - \\frac{\\left(\\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2}\\right)^2}{\\frac{\\partial^2 J}{\\partial \\theta_1^2}}\n$$\nWe need to evaluate this at $\\theta_2 = \\widehat{\\theta}_2$. The point $(\\widehat{\\theta}_1, \\widehat{\\theta}_2)$ where $\\widehat{\\theta}_1 = \\theta_1^{\\star}(\\widehat{\\theta}_2)$ is a minimizer of the full cost function $J(\\theta_1, \\theta_2)$. Thus, the gradient of $J$ vanishes at this point. In particular, $\\left.\\frac{\\partial J}{\\partial \\theta_2}\\right|_{(\\widehat{\\theta}_1, \\widehat{\\theta}_2)} = 0$, which implies:\n$$\n\\sum_{i=1}^{n} (y_i - \\widehat{\\theta}_1 \\exp(-\\widehat{\\theta}_2 t_i)) t_i \\exp(-\\widehat{\\theta}_2 t_i) = 0 \\implies \\sum_{i=1}^{n} y_i t_i \\exp(-\\widehat{\\theta}_2 t_i) = \\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i)\n$$\nWe now compute the necessary second partial derivatives of $J$:\n$$\nJ_{\\theta_1\\theta_1} = \\frac{\\partial^2 J}{\\partial \\theta_1^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\n$$\n$$\nJ_{\\theta_1\\theta_2} = \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i t_i \\exp(-\\theta_2 t_i) - 2\\theta_1 t_i \\exp(-2\\theta_2 t_i))\n$$\nAt $(\\widehat{\\theta}_1, \\widehat{\\theta}_2)$, using the optimality condition derived above:\n$$\n\\left.J_{\\theta_1\\theta_2}\\right|_{(\\widehat{\\theta}_1, \\widehat{\\theta}_2)} = \\frac{1}{\\sigma^2} \\left( \\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i) - 2\\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i) \\right) = -\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i)\n$$\n$$\nJ_{\\theta_2\\theta_2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} = \\frac{\\partial}{\\partial \\theta_2} \\left( \\frac{\\theta_1}{\\sigma^2} \\sum_{i=1}^n (y_i t_i e^{-\\theta_2 t_i} - \\theta_1 t_i e^{-2\\theta_2 t_i}) \\right) = -\\frac{\\theta_1}{\\sigma^2} \\sum_{i=1}^n y_i t_i^2 e^{-\\theta_2 t_i} + \\frac{2\\theta_1^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 e^{-2\\theta_2 t_i}\n$$\nNow, assembling the curvature expression and evaluating at $\\theta_2 = \\widehat{\\theta}_2$ (so $\\theta_1 = \\widehat{\\theta}_1$):\n$$\n\\left.\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2}\\right|_{\\widehat{\\theta}_2} = \\left( -\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i} + \\frac{2\\widehat{\\theta}_1^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} \\right) - \\frac{\\left(-\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i}\\right)^2}{\\frac{1}{\\sigma^2} \\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}}\n$$\n$$\n= \\frac{1}{\\sigma^2} \\left( -\\widehat{\\theta}_1 \\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i} + 2\\widehat{\\theta}_1^2 \\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} - \\widehat{\\theta}_1^2 \\frac{\\left( \\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i} \\right)^2}{\\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}} \\right)\n$$\nFinally, we substitute $\\widehat{\\theta}_1 = \\theta_1^{\\star}(\\widehat{\\theta}_2) = \\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}$ to express the result entirely in terms of the data, $\\sigma^2$, and $\\widehat{\\theta}_2$:\n$$\n\\frac{1}{\\sigma^2} \\left[ \\left(\\frac{\\sum_{j=1}^n y_j e^{-\\widehat{\\theta}_2 t_j}}{\\sum_{k=1}^n e^{-2\\widehat{\\theta}_2 t_k}}\\right) \\left(-\\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i}\\right) + \\left(\\frac{\\sum_{j=1}^n y_j e^{-\\widehat{\\theta}_2 t_j}}{\\sum_{k=1}^n e^{-2\\widehat{\\theta}_2 t_k}}\\right)^2 \\left(2\\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} - \\frac{(\\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i})^2}{\\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}}\\right) \\right]\n$$\nThis is the required explicit expression.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\frac{\\left(\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)\\right)^2}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)} \\right) & \\frac{1}{\\sigma^2} \\left[ \\left(\\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}\\right)^2 \\left(2\\sum_{i=1}^n t_i^2 \\exp(-2\\widehat{\\theta}_2 t_i) - \\frac{\\left(\\sum_{i=1}^n t_i \\exp(-2\\widehat{\\theta}_2 t_i)\\right)^2}{\\sum_{i=1}^n \\exp(-2\\widehat{\\theta}_2 t_i)}\\right) - \\left(\\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}\\right) \\sum_{i=1}^n y_i t_i^2 \\exp(-\\widehat{\\theta}_2 t_i) \\right] \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical identifiability is not solely a property of a model's structure; it is deeply intertwined with the experimental design used to collect data. This practice explores the critical link between the sampling schedule and practical identifiability, using the Fisher Information Matrix as an analytical tool . You will analyze how experimental choices, such as the sampling interval $\\Delta t$, can induce near-collinearity in the model's sensitivities and render structurally identifiable parameters practically indistinguishable from noisy, discrete data.",
            "id": "3426673",
            "problem": "Consider a scalar observable model with an unknown amplitude parameter $A \\in \\mathbb{R}$ and unknown decay rate parameter $\\theta > 0$ given by $y(t) = A e^{-\\theta t}$. Suppose we collect $K$ discrete-time observations at $t_k = k \\Delta t$ for $k = 1, \\dots, K$ with independent and identically distributed (i.i.d.) additive Gaussian noise of variance $\\sigma^2$, so that $y_k = A e^{-\\theta t_k} + \\varepsilon_k$ with $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ and $t_k = k \\Delta t$. In a local (first-order) analysis of parameter identifiability, the sensitivity matrix $S \\in \\mathbb{R}^{K \\times 2}$ collects partial derivatives of the observation model with respect to the parameters, evaluated along the discrete sampling schedule. That is, its two columns are $s_A(k) = \\partial y_k / \\partial A$ and $s_{\\theta}(k) = \\partial y_k / \\partial \\theta$. The Fisher Information Matrix (FIM) for $(A, \\theta)$ under Gaussian noise is, up to the factor $1/\\sigma^2$, given by $S^\\top S$.\n\nStarting from these definitions and the measurement model, reason from first principles to determine how the choice of sampling interval $\\Delta t$ interacts with the decay governed by $\\theta$ to affect the geometry (e.g., near-collinearity) of the sensitivity columns and the conditioning of the Fisher Information Matrix. In particular, assess the implications of coarse sampling (large $\\Delta t$) for the practical identifiability of $\\theta$.\n\nWhich of the following statements are correct? Choose all that apply.\n\nA. If we define weights $w_k = e^{-2 \\theta k \\Delta t}$ and the associated normalized discrete distribution $p_k = w_k \\big/ \\sum_{j=1}^K w_j$, then the absolute Pearson correlation between the sensitivity columns $s_A$ and $s_{\\theta}$ is $|\\rho| = \\mu_1 \\big/ \\sqrt{\\mu_2}$, where $\\mu_1 = \\sum_{k=1}^K k \\, p_k$ and $\\mu_2 = \\sum_{k=1}^K k^2 \\, p_k$. For large $\\theta \\Delta t$ (coarse sampling on a fast decay), $p_k$ concentrates on small $k$ and $|\\rho| \\to 1$, indicating near-collinearity.\n\nB. With $w_k = e^{-2 \\theta k \\Delta t}$, the Fisher Information Matrix determinant satisfies $\\det(S^\\top S) = A^2 \\Delta t^2 \\left(\\sum_{k=1}^K w_k\\right)^2 \\operatorname{Var}_{p}(k)$, where $\\operatorname{Var}_{p}(k) = \\sum_{k=1}^K k^2 p_k - \\left(\\sum_{k=1}^K k p_k\\right)^2$. As $\\Delta t$ grows with fixed $\\theta$, $p_k$ concentrates and $\\operatorname{Var}_{p}(k) \\to 0$, making the FIM nearly singular and rendering $\\theta$ practically non-identifiable.\n\nC. Discretization cannot affect identifiability because identifiability is a structural property of the continuous-time model; if $\\theta$ is identifiable in continuous time, it remains equally identifiable under any discrete sampling.\n\nD. Refining the sampling (decreasing $\\Delta t$) always increases the correlation between the sensitivity columns $s_A$ and $s_{\\theta}$, thereby worsening the identifiability of $\\theta$.\n\nE. If an additional constant offset parameter $B$ is included so that $y_k = A e^{-\\theta k \\Delta t} + B + \\varepsilon_k$, then coarse sampling makes the sensitivity to $B$, namely $s_B(k) = 1$, orthogonal to $s_A(k)$, thereby improving the identifiability of $\\theta$.",
            "solution": "The user wants a critical validation of the problem statement, followed by a first-principles derivation and evaluation of all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Model:** Scalar observable $y(t) = A e^{-\\theta t}$.\n*   **Parameters:** Amplitude $A \\in \\mathbb{R}$, decay rate $\\theta > 0$.\n*   **Discrete Observations:** $y_k = A e^{-\\theta t_k} + \\varepsilon_k$ for $k = 1, \\dots, K$.\n*   **Sampling Schedule:** $t_k = k \\Delta t$.\n*   **Noise Model:** $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$, independent and identically distributed (i.i.d.).\n*   **Sensitivity Matrix:** $S \\in \\mathbb{R}^{K \\times 2}$ with columns $s_A$ and $s_{\\theta}$.\n*   **Sensitivity Column Definitions:** The $k$-th element of the columns are $s_A(k) = \\partial y_k / \\partial A$ and $s_{\\theta}(k) = \\partial y_k / \\partial \\theta$.\n*   **Fisher Information Matrix (FIM):** FIM for $(A, \\theta)$ under Gaussian noise is stated to be given by $S^\\top S$ up to a factor of $1/\\sigma^2$. The analysis should focus on $S^\\top S$.\n*   **Objective:** Analyze the effect of the sampling interval $\\Delta t$ and decay rate $\\theta$ on the collinearity of the sensitivity columns and the conditioning of the FIM, particularly for a large sampling interval $\\Delta t$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is based on the standard exponential decay model, a cornerstone of modeling in many STEM fields. The analysis framework using sensitivity analysis and the Fisher Information Matrix is a standard and rigorous method for local parameter identifiability analysis and optimal experimental design. All concepts are well-established.\n2.  **Well-Posed:** The problem provides a clear, self-contained mathematical setup. The goal is to analyze specific properties of the resulting mathematical objects ($S$ and $S^\\top S$), which is a well-defined task. A unique analysis based on the provided definitions is possible.\n3.  **Objective:** The language is precise and mathematical. Terms like \"sensitivity matrix,\" \"collinearity,\" and \"Fisher Information Matrix\" have unambiguous definitions in this context.\n4.  **Completeness:** The problem statement includes all necessary information: the model function, parameter definitions, sampling scheme, noise characteristics, and the definitions of the mathematical objects to be analyzed.\n5.  **No Other Flaws:** The problem is free from contradictions, unrealistic conditions, or vagueness. It presents a standard, non-trivial problem in parameter estimation theory.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The analysis can proceed.\n\n### Principle-Based Derivation and Solution\n\n**1. Derivation of Sensitivity Vectors**\nThe model for the $k$-th observation is $y_k$, which is based on the model function $f(t_k; A, \\theta) = A e^{-\\theta t_k}$. The sensitivity vectors, $\\mathbf{s}_A$ and $\\mathbf{s}_{\\theta}$, are vectors in $\\mathbb{R}^K$ whose components are the partial derivatives of the model function with respect to each parameter, evaluated at each sampling time $t_k = k \\Delta t$.\n\n*   **Sensitivity to A:** The $k$-th component is\n    $$ s_A(k) = \\frac{\\partial}{\\partial A} \\left( A e^{-\\theta k \\Delta t} \\right) = e^{-\\theta k \\Delta t} $$\n*   **Sensitivity to $\\theta$:** The $k$-th component is\n    $$ s_{\\theta}(k) = \\frac{\\partial}{\\partial \\theta} \\left( A e^{-\\theta k \\Delta t} \\right) = A e^{-\\theta k \\Delta t} (-k \\Delta t) = -A k \\Delta t e^{-\\theta k \\Delta t} $$\n\nThus, the two sensitivity column vectors are:\n$$ \\mathbf{s}_A = \\begin{pmatrix} e^{-\\theta \\Delta t} \\\\ e^{-2\\theta \\Delta t} \\\\ \\vdots \\\\ e^{-K\\theta \\Delta t} \\end{pmatrix}, \\quad \\mathbf{s}_{\\theta} = -A \\Delta t \\begin{pmatrix} 1 \\cdot e^{-\\theta \\Delta t} \\\\ 2 \\cdot e^{-2\\theta \\Delta t} \\\\ \\vdots \\\\ K \\cdot e^{-K\\theta \\Delta t} \\end{pmatrix} $$\n\n**2. Derivation of the Fisher Information Matrix ($S^\\top S$)**\nThe matrix $S^\\top S$ is a $2 \\times 2$ Gram matrix whose elements are the dot products of the sensitivity vectors.\n$$ S^\\top S = \\begin{pmatrix} \\mathbf{s}_A^\\top \\mathbf{s}_A & \\mathbf{s}_A^\\top \\mathbf{s}_{\\theta} \\\\ \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_A & \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta} \\end{pmatrix} $$\nLet's compute the elements:\n*   $(S^\\top S)_{11} = \\mathbf{s}_A^\\top \\mathbf{s}_A = \\sum_{k=1}^K (e^{-\\theta k \\Delta t})^2 = \\sum_{k=1}^K e^{-2\\theta k \\Delta t}$\n*   $(S^\\top S)_{12} = \\mathbf{s}_A^\\top \\mathbf{s}_{\\theta} = \\sum_{k=1}^K (e^{-\\theta k \\Delta t})(-A k \\Delta t e^{-\\theta k \\Delta t}) = -A \\Delta t \\sum_{k=1}^K k e^{-2\\theta k \\Delta t}$\n*   $(S^\\top S)_{21} = (S^\\top S)_{12}$\n*   $(S^\\top S)_{22} = \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta} = \\sum_{k=1}^K (-A k \\Delta t e^{-\\theta k \\Delta t})^2 = A^2 (\\Delta t)^2 \\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}$\n\n**3. Analysis of Collinearity**\nCollinearity is assessed by the cosine of the angle $\\phi$ between the vectors $\\mathbf{s}_A$ and $\\mathbf{s}_{\\theta}$, where $\\cos(\\phi) = \\frac{\\mathbf{s}_A^\\top \\mathbf{s}_{\\theta}}{\\|\\mathbf{s}_A\\| \\|\\mathbf{s}_{\\theta}\\|}$. Near-collinearity corresponds to $|\\cos(\\phi)| \\approx 1$.\n$$ \\cos(\\phi) = \\frac{-A \\Delta t \\sum_{k=1}^K k e^{-2\\theta k \\Delta t}}{\\sqrt{\\left(\\sum_{k=1}^K e^{-2\\theta k \\Delta t}\\right) \\left(A^2 (\\Delta t)^2 \\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}\\right)}} = -\\mathrm{sgn}(A) \\frac{\\sum_{k=1}^K k e^{-2\\theta k \\Delta t}}{\\sqrt{\\left(\\sum_{k=1}^K e^{-2\\theta k \\Delta t}\\right)\\left(\\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}\\right)}} $$\nLet's define weights $w_k = e^{-2\\theta k \\Delta t}$ and the total weight $W = \\sum_{j=1}^K w_j$. We can define a discrete probability distribution $p_k = w_k/W$. The expression for the absolute value of the cosine becomes:\n$$ |\\cos(\\phi)| = \\frac{\\sum k w_k}{\\sqrt{(\\sum w_k)(\\sum k^2 w_k)}} = \\frac{W \\sum k p_k}{\\sqrt{W \\cdot W \\sum k^2 p_k}} = \\frac{\\sum k p_k}{\\sqrt{\\sum k^2 p_k}} $$\nFollowing the notation in the problem options, let $\\mu_1 = \\sum_{k=1}^K k p_k$ and $\\mu_2 = \\sum_{k=1}^K k^2 p_k$. Then, $|\\cos(\\phi)| = \\mu_1 / \\sqrt{\\mu_2}$. While the Pearson correlation coefficient is technically different as it involves de-meaning the vectors, the cosine similarity is a direct measure of collinearity and is often what is contextually meant by \"correlation\" in this type of analysis.\n\n### Option-by-Option Analysis\n\n**A. If we define weights $w_k = e^{-2 \\theta k \\Delta t}$ and the associated normalized discrete distribution $p_k = w_k \\big/ \\sum_{j=1}^K w_j$, then the absolute Pearson correlation between the sensitivity columns $s_A$ and $s_{\\theta}$ is $|\\rho| = \\mu_1 \\big/ \\sqrt{\\mu_2}$, where $\\mu_1 = \\sum_{k=1}^K k \\, p_k$ and $\\mu_2 = \\sum_{k=1}^K k^2 \\, p_k$. For large $\\theta \\Delta t$ (coarse sampling on a fast decay), $p_k$ concentrates on small $k$ and $|\\rho| \\to 1$, indicating near-collinearity.**\n\n- **Formula Evaluation:** As derived above, the formula $|\\rho| = \\mu_1 / \\sqrt{\\mu_2}$ represents the magnitude of the cosine similarity between the sensitivity vectors, not strictly the Pearson correlation coefficient. However, it is a valid and direct measure of their geometric alignment (collinearity).\n- **Limit Analysis:** Consider the case of \"coarse sampling,\" where the product $\\theta \\Delta t$ is large. Let $x = \\theta \\Delta t \\gg 1$. The weights are $w_k = e^{-2kx}$. The ratio of successive weights is $w_{k+1}/w_k = e^{-2x} \\ll 1$. This means the weights decay extremely rapidly: $w_1 \\gg w_2 \\gg w_3 \\dots$. The sum $W = \\sum_j w_j$ is dominated by the first term, $w_1$.\n- The probability mass function $p_k = w_k / W$ will therefore be heavily concentrated at the smallest index, $k=1$. In the limit as $\\theta \\Delta t \\to \\infty$, we have $p_1 \\to 1$ and $p_k \\to 0$ for $k > 1$.\n- We can now evaluate the limits of the moments:\n    - $\\mu_1 = \\sum_{k=1}^K k p_k \\longrightarrow 1 \\cdot p_1 + 2 \\cdot p_2 + \\dots \\longrightarrow 1 \\cdot 1 = 1$.\n    - $\\mu_2 = \\sum_{k=1}^K k^2 p_k \\longrightarrow 1^2 \\cdot p_1 + 2^2 \\cdot p_2 + \\dots \\longrightarrow 1^2 \\cdot 1 = 1$.\n- Thus, the collinearity measure becomes $|\\rho| = \\mu_1 / \\sqrt{\\mu_2} \\longrightarrow 1 / \\sqrt{1} = 1$.\n- **Conclusion:** A value of $|\\rho| \\to 1$ indicates that the vectors $\\mathbf{s}_A$ and $\\mathbf{s}_{\\theta}$ become nearly parallel. This makes it statistically difficult to distinguish the effect of changing $A$ from the effect of changing $\\theta$, which is the essence of poor practical identifiability. The statement is conceptually and mathematically correct, despite a minor terminological ambiguity.\n- **Verdict on A:** **Correct**.\n\n**B. With $w_k = e^{-2 \\theta k \\Delta t}$, the Fisher Information Matrix determinant satisfies $\\det(S^\\top S) = A^2 \\Delta t^2 \\left(\\sum_{k=1}^K w_k\\right)^2 \\operatorname{Var}_{p}(k)$, where $\\operatorname{Var}_{p}(k) = \\sum_{k=1}^K k^2 p_k - \\left(\\sum_{k=1}^K k p_k\\right)^2$. As $\\Delta t$ grows with fixed $\\theta$, $p_k$ concentrates and $\\operatorname{Var}_{p}(k) \\to 0$, making the FIM nearly singular and rendering $\\theta$ practically non-identifiable.**\n\n- **Formula Evaluation:** Let's compute the determinant of $S^\\top S$:\n    $$ \\det(S^\\top S) = (\\mathbf{s}_A^\\top \\mathbf{s}_A)(\\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta}) - (\\mathbf{s}_A^\\top \\mathbf{s}_{\\theta})^2 $$\n    $$ \\det(S^\\top S) = \\left(\\sum w_k\\right) \\left(A^2 (\\Delta t)^2 \\sum k^2 w_k\\right) - \\left(-A \\Delta t \\sum k w_k\\right)^2 $$\n    $$ \\det(S^\\top S) = A^2 (\\Delta t)^2 \\left[ \\left(\\sum w_k\\right)\\left(\\sum k^2 w_k\\right) - \\left(\\sum k w_k\\right)^2 \\right] $$\n    Using $p_k=w_k/W$ and $W=\\sum w_k$: $\\sum k w_k = W \\mu_1$ and $\\sum k^2 w_k = W \\mu_2$.\n    $$ \\det(S^\\top S) = A^2 (\\Delta t)^2 \\left[ W(W \\mu_2) - (W \\mu_1)^2 \\right] = A^2 (\\Delta t)^2 W^2 (\\mu_2 - \\mu_1^2) $$\n    Since $W = \\sum w_k$ and $\\operatorname{Var}_p(k) = \\mu_2 - \\mu_1^2$, the formula is $\\det(S^\\top S) = A^2 (\\Delta t)^2 \\left(\\sum w_k\\right)^2 \\operatorname{Var}_p(k)$. The formula is correct.\n- **Limit Analysis:** As $\\Delta t$ grows, $\\theta \\Delta t \\to \\infty$. As established for option A, the distribution $p_k$ concentrates on $k=1$. For a random variable that takes a single value with probability $1$, its variance is zero.\n    - $\\operatorname{Var}_p(k) = \\mu_2 - \\mu_1^2 \\longrightarrow 1 - 1^2 = 0$.\n- **Conclusion:** A determinant of the FIM that approaches zero indicates that the matrix is nearly singular. This is a hallmark of poor practical identifiability, as the volume of the parameter confidence region (which is inversely related to $\\sqrt{\\det(\\text{FIM})}$) becomes infinitely large. The statement provides a correct formula and a correct conclusion about the consequences for identifiability.\n- **Verdict on B:** **Correct**.\n\n**C. Discretization cannot affect identifiability because identifiability is a structural property of the continuous-time model; if $\\theta$ is identifiable in continuous time, it remains equally identifiable under any discrete sampling.**\n\n- This statement conflates two distinct concepts: *structural identifiability* and *practical identifiability*.\n- Structural identifiability asks if parameters are unique given perfect, continuous data. It is indeed a property of the model structure. For $y(t) = A e^{-\\theta t}$, the parameters are structurally identifiable.\n- Practical identifiability asks if parameters can be estimated with finite precision from a finite set of noisy, discrete data. It depends critically on the experimental design, including the sampling schedule ($\\Delta t, K$) and noise level ($\\sigma^2$).\n- As demonstrated in the analysis of A and B, choosing a poor sampling scheme (e.g., large $\\Delta t$) can lead to a nearly singular FIM, making it impossible to estimate parameters reliably. Therefore, discretization and the choice of sampling have a profound effect on practical identifiability.\n- **Verdict on C:** **Incorrect**.\n\n**D. Refining the sampling (decreasing $\\Delta t$) always increases the correlation between the sensitivity columns $s_A$ and $s_{\\theta}$, thereby worsening the identifiability of $\\theta$.**\n\n- This statement is the opposite of the truth. As shown in A and B, coarse sampling (large $\\Delta t$) increases the correlation and worsens identifiability. Let's consider the limit of refining the sampling, $\\Delta t \\to 0$.\n- As $\\Delta t \\to 0$, we have $\\theta k \\Delta t \\to 0$, so $w_k = e^{-2\\theta k \\Delta t} \\to 1$ for all $k$.\n- The distribution $p_k = w_k / \\sum w_j$ approaches the discrete uniform distribution, $p_k \\to 1/K$.\n- The correlation measure $|\\rho|$ approaches a value less than $1$ (for $K>1$), which implies less collinearity than the coarse sampling case. For example, as $\\Delta t \\to 0$, $|\\rho| \\to \\sqrt{\\frac{3(K+1)}{2(2K+1)}}$, which for $K \\gg 1$ is approximately $\\sqrt{3/4} \\approx 0.866$.\n- Since coarse sampling leads to $|\\rho| \\to 1$, refining the sampling *decreases* the correlation (moves it away from $1$) and thus *improves* practical identifiability. The statement is false.\n- **Verdict on D:** **Incorrect**.\n\n**E. If an additional constant offset parameter $B$ is included so that $y_k = A e^{-\\theta k \\Delta t} + B + \\varepsilon_k$, then coarse sampling makes the sensitivity to $B$, namely $s_B(k) = 1$, orthogonal to $s_A(k)$, thereby improving the identifiability of $\\theta$.**\n\n- The new model is $y_k = A e^{-\\theta k \\Delta t} + B$. The sensitivity vector for $B$ has components $s_B(k) = \\partial y_k / \\partial B = 1$. The vector $\\mathbf{s}_B$ is a vector of ones.\n- The sensitivity vector $\\mathbf{s}_A$ is unchanged, with components $s_A(k) = e^{-\\theta k \\Delta t}$.\n- Orthogonality requires the dot product to be zero: $\\mathbf{s}_A^\\top \\mathbf{s}_B = \\sum_{k=1}^K s_A(k) s_B(k) = \\sum_{k=1}^K e^{-\\theta k \\Delta t}$.\n- Since $\\theta > 0$ and $\\Delta t > 0$, every term in the sum is strictly positive. The dot product is always positive, so the vectors are never orthogonal.\n- Let's consider the limit of coarse sampling ($\\theta \\Delta t \\to \\infty$). In this limit, $e^{-\\theta k \\Delta t} \\to 0$ for all $k \\ge 1$. So, the vector $\\mathbf{s}_A \\to \\mathbf{0}$. The zero vector is technically orthogonal to any other vector. However, if a sensitivity vector becomes zero, it means the observations have no sensitivity to that parameter. This makes the parameter ($A$) completely unidentifiable. The identifiability of $\\theta$ also vanishes, since $\\mathbf{s}_{\\theta}$ is proportional to components like $k e^{-\\theta k \\Delta t}$, which also go to zero.\n- The conclusion that this \"improves the identifiability of $\\theta$\" is incorrect. Vanishing sensitivity is the worst possible scenario for identifiability.\n- **Verdict on E:** **Incorrect**.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "In the Bayesian framework of data assimilation, parameter estimation is viewed as an update from a prior to a posterior distribution, conditioned on observations. The possibility of such an update hinges on the flow of information from observations to parameters, which is encoded in the model structure and the prior distribution. This problem presents a carefully constructed linear-Gaussian scenario to illustrate a fundamental principle: without a prior correlation linking an observable state to a parameter, even an informative observation of the state may not update our knowledge of that parameter .",
            "id": "3426691",
            "problem": "Consider a linear-Gaussian data assimilation problem with a scalar state $x$ and a scalar parameter $\\theta$. The forecast (prior) joint distribution of $(x,\\theta)$ is Gaussian with means $m_{x}^{f}$ and $m_{\\theta}^{f}$ and covariance matrix\n$$\n\\Sigma^{f} \\;=\\; \\begin{pmatrix}\np_{x}^{f} & 0 \\\\\n0 & p_{\\theta}^{f}\n\\end{pmatrix},\n$$\nso that $x$ and $\\theta$ are uncorrelated under the forecast measure. An observation $y$ is collected according to the linear model $y = H x + v$, where $H$ is a known scalar and $v$ is independent Gaussian noise with distribution $v \\sim \\mathcal{N}(0,r)$.\n\nA specific instance is given by $m_{x}^{f} = 0$, $m_{\\theta}^{f} = 5$, $p_{x}^{f} = 2$, $p_{\\theta}^{f} = 3$, $H = 1$, $r = 0.1$, and a realized observation $y = 1$. All quantities are dimensionless. In this setting, the observation is informative about the state $x$ in the sense that $r$ is strictly positive and small relative to $p_{x}^{f}$, yet the parameter $\\theta$ is uncorrelated with $x$ under the forecast measure.\n\nStarting from Bayesâ€™s rule and the properties of conditioning in multivariate normal distributions, determine the parameter analysis increment\n$$\n\\delta \\theta \\;=\\; \\mathbb{E}[\\theta \\mid y] \\;-\\; m_{\\theta}^{f}.\n$$\nExpress your final answer as a single real number. No rounding is required, and no units are associated with the answer.",
            "solution": "This problem tests the understanding of Bayesian updates in a linear-Gaussian system, specifically how information propagates between variables. The goal is to calculate the update to a parameter's mean based on an observation of a related state variable.\n\n### Analysis of the System Structure\n\nThe problem can be framed in the context of an augmented state vector $z = (x, \\theta)^T$.\nThe **prior distribution** for this augmented state is Gaussian, $z \\sim \\mathcal{N}(m_z^f, \\Sigma^f)$, with:\n*   Prior mean: $m_z^f = \\begin{pmatrix} m_x^f \\\\ m_\\theta^f \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 5 \\end{pmatrix}$.\n*   Prior covariance: $\\Sigma^f = \\begin{pmatrix} p_x^f & \\mathrm{Cov}(x, \\theta) \\\\ \\mathrm{Cov}(x, \\theta) & p_\\theta^f \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}$.\nThe crucial information here is that the prior covariance between the state $x$ and the parameter $\\theta$ is zero. They are uncorrelated.\n\nThe **observation model** is $y = Hx + v$. We can write this in terms of the augmented state vector as $y = H_{aug} z + v$, where:\n*   Augmented observation operator: $H_{aug} = \\begin{pmatrix} H & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\nThis shows that the observation $y$ depends only on the state $x$ and not directly on the parameter $\\theta$.\n\nThe **observation noise** is $v \\sim \\mathcal{N}(0, R)$ with $R = r = 0.1$.\nThe **observation value** is $y=1$.\n\n### Calculating the Posterior Mean\n\nThe posterior mean (or analysis mean), $m_z^a = \\mathbb{E}[z \\mid y]$, is given by the standard Kalman update formula:\n$$\nm_z^a = m_z^f + K (y - H_{aug} m_z^f)\n$$\nwhere $K$ is the Kalman gain matrix, given by:\n$$\nK = \\Sigma^f H_{aug}^T (H_{aug} \\Sigma^f H_{aug}^T + R)^{-1}\n$$\n\nLet's compute the components of the Kalman gain calculation:\n1.  **Innovation Covariance (a scalar in this case):**\n    $$ S = H_{aug} \\Sigma^f H_{aug}^T + R = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.1 = 2 + 0.1 = 2.1 $$\n2.  **Cross-Covariance between State and Observation:**\n    $$ \\Sigma^f H_{aug}^T = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} $$\n3.  **Kalman Gain (a $2 \\times 1$ vector):**\n    $$ K = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} (2.1)^{-1} = \\begin{pmatrix} 2/2.1 \\\\ 0 \\end{pmatrix} $$\n\nThe Kalman gain has two components, $K = (K_x, K_\\theta)^T$. The gain for the state is $K_x = 2/2.1$, while the gain for the parameter is $K_\\theta = 0$.\n\nNow, we can compute the posterior mean vector $m_z^a$:\n*   **Innovation:** $d = y - H_{aug} m_z^f = 1 - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 5 \\end{pmatrix} = 1 - 0 = 1$.\n*   **Posterior Mean Update:**\n    $$ m_z^a = m_z^f + K d \\implies \\begin{pmatrix} m_x^a \\\\ m_\\theta^a \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 5 \\end{pmatrix} + \\begin{pmatrix} 2/2.1 \\\\ 0 \\end{pmatrix} \\cdot (1) = \\begin{pmatrix} 2/2.1 \\\\ 5 \\end{pmatrix} $$\n\nThe posterior mean of the parameter is $\\mathbb{E}[\\theta \\mid y] = m_\\theta^a = 5$.\n\n### Calculating the Parameter Analysis Increment\n\nThe problem asks for the parameter analysis increment, $\\delta\\theta$:\n$$\n\\delta \\theta = \\mathbb{E}[\\theta \\mid y] - m_{\\theta}^{f} = m_\\theta^a - m_\\theta^f = 5 - 5 = 0\n$$\n\nThe analysis increment for the parameter $\\theta$ is zero. This result occurs because the observation of $y$ provides information about $x$, but since $x$ and $\\theta$ are uncorrelated in the prior (and the model is linear), this new information about $x$ does not propagate to update our belief about $\\theta$. The mean and variance of $\\theta$ remain unchanged after the observation.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}