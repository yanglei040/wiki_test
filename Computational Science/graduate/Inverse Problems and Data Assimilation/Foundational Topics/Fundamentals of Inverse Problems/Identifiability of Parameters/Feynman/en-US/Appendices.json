{
    "hands_on_practices": [
        {
            "introduction": "Non-identifiability often arises from underlying symmetries in a model's parameterization, where different parameter combinations produce identical observations. This first exercise provides a rigorous look at this issue using the canonical example of a simple product, $y = ax$. By formalizing the scaling symmetry as a continuous group action, you will use the concept of an infinitesimal generator to derive precisely which function of the parameters is identifiable, offering a powerful method that applies to more complex systems. ",
            "id": "3390183",
            "problem": "Consider the deterministic observation model with an unknown pair of parameters $(a,x)$ taking values in $\\mathbb{R} \\setminus \\{0\\}$ and a single observed scalar\n$$\ny \\in \\mathbb{R} \\setminus \\{0\\}, \\quad \\text{with} \\quad y = a\\,x.\n$$\nYou are told that only the product is observed and both $a$ and $x$ are unknown. In the sense of parameter identifiability in inverse problems and data assimilation, a parameter (or parameter function) is identifiable if it is uniquely determined by the distribution of the observed data. Assume noiseless observation in the first part, and then consider additive Gaussian noise in the final part.\n\n1. Formalize the scaling invariance by defining a smooth action of the multiplicative group of positive real numbers $\\mathbb{R}_{+}$ on the parameter space $(a,x) \\in (\\mathbb{R}\\setminus\\{0\\}) \\times (\\mathbb{R}\\setminus\\{0\\})$ that leaves the observable $y$ unchanged. Derive the associated infinitesimal generator of this action as a first-order differential operator.\n\n2. Using the infinitesimal generator, derive from first principles the partial differential equation (PDE) that characterizes smooth invariants of this action and solve it to obtain a nontrivial smooth invariant function of $(a,x)$.\n\n3. Propose a smooth reparameterization $\\psi:(a,x)\\mapsto(u,v)$ that separates an identifiable parameter from a non-identifiable one, where $u$ is invariant under the group action you defined and $v$ parameterizes variation along the orbits of the action. Compute the determinant of the Jacobian matrix of $\\psi$ and state where it is nonzero.\n\n4. Now consider a noisy observation model\n$$\nz = y + \\varepsilon = a\\,x + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2}),\n$$\nwith known noise variance $\\sigma^{2} \\in (0,\\infty)$. Explain why only the invariant component of your reparameterization is identifiable from the distribution of $z$ and how the non-identifiable component manifests in the likelihood.\n\nYour final answer must be the single closed-form analytic expression for the identifiable parameter $u(a,x)$ from your reparameterization in part $3$ (do not include any units). Do not round. Express your final answer as a simplified expression.",
            "solution": "The problem is validated as self-contained, consistent, and scientifically sound. It is a standard exercise in the theory of parameter identifiability for inverse problems.\n\n**1. Group Action and Infinitesimal Generator**\n\nThe problem concerns the model $y = a\\,x$, where the parameters $(a,x)$ are unknown elements of the parameter space $M = (\\mathbb{R}\\setminus\\{0\\}) \\times (\\mathbb{R}\\setminus\\{0\\})$. We are given that only the product $y=a\\,x$ is observed. This suggests an invariance under transformations that leave the product $a\\,x$ unchanged.\n\nWe seek a smooth group action of the multiplicative group of positive real numbers, $G = \\mathbb{R}_{+}$, on the parameter space $M$. Let $\\lambda \\in \\mathbb{R}_{+}$ be an element of the group. The action is a map $\\Phi: G \\times M \\to M$, which we denote as $(a', x') = \\Phi_{\\lambda}(a, x)$. The condition is that the action leaves the observable $y$ invariant, which means $a'x' = a\\,x$ for all $\\lambda \\in \\mathbb{R}_{+}$.\n\nA natural choice for such a transformation is a scaling action. Let us define the action as:\n$$\n\\Phi_{\\lambda}(a, x) = (a', x') = (\\lambda^k a, \\lambda^m x)\n$$\nfor some real exponents $k$ and $m$. For this to be a well-defined group action, it must satisfy $\\Phi_{\\lambda_1 \\lambda_2} = \\Phi_{\\lambda_1} \\circ \\Phi_{\\lambda_2}$, which holds for this form. The invariance condition $a'x' = a\\,x$ implies:\n$$\n(\\lambda^k a)(\\lambda^m x) = \\lambda^{k+m} (a\\,x) = a\\,x\n$$\nSince $a\\,x \\neq 0$, we must have $\\lambda^{k+m} = 1$ for all $\\lambda \\in \\mathbb{R}_{+}$. This requires $k+m=0$. A simple-yet-general choice is to set $k=1$, which implies $m=-1$. The group action is therefore:\n$$\n\\Phi_{\\lambda}(a, x) = (\\lambda a, \\lambda^{-1} x) \\quad \\text{for } \\lambda \\in \\mathbb{R}_{+}\n$$\nTo find the infinitesimal generator of this action, we parameterize the group with $s \\in \\mathbb{R}$ such that $\\lambda = \\exp(s)$. The identity element $\\lambda=1$ corresponds to $s=0$. The action becomes $\\Phi_{s}(a, x) = (\\exp(s)a, \\exp(-s)x)$. The infinitesimal generator is the vector field $V$ on $M$ whose components are the derivatives of the transformed coordinates with respect to $s$, evaluated at $s=0$:\n$$\nV = V_a \\frac{\\partial}{\\partial a} + V_x \\frac{\\partial}{\\partial x}\n$$\nThe components are:\n$$\nV_a = \\left. \\frac{d}{ds}(\\exp(s)a) \\right|_{s=0} = \\left. \\exp(s)a \\right|_{s=0} = a\n$$\n$$\nV_x = \\left. \\frac{d}{ds}(\\exp(-s)x) \\right|_{s=0} = \\left. -\\exp(-s)x \\right|_{s=0} = -x\n$$\nThus, the infinitesimal generator represented as a first-order differential operator is:\n$$\nV = a \\frac{\\partial}{\\partial a} - x \\frac{\\partial}{\\partial x}\n$$\n\n**2. PDE for Invariants and its Solution**\n\nA smooth function $I(a,x)$ is an invariant of the group action if it is constant along the orbits of the action. This is equivalent to the condition that its Lie derivative with respect to the generator's vector field is zero, i.e., $V(I)=0$. This gives the partial differential equation (PDE) for the invariants:\n$$\na \\frac{\\partial I}{\\partial a} - x \\frac{\\partial I}{\\partial x} = 0\n$$\nThis is a first-order linear PDE that can be solved using the method of characteristics. The characteristic equations are:\n$$\n\\frac{da}{a} = \\frac{dx}{-x} = \\frac{dI}{0}\n$$\nThe equation $dI=0$ implies that $I$ is constant along the characteristic curves. From the first two parts, we have:\n$$\n\\frac{da}{a} = -\\frac{dx}{x}\n$$\nIntegrating both sides yields:\n$$\n\\int \\frac{da}{a} = -\\int \\frac{dx}{x} \\implies \\ln|a| = -\\ln|x| + C\n$$\nwhere $C$ is a constant of integration. Rearranging gives $\\ln|a| + \\ln|x| = C$, which is equivalent to $\\ln|a\\,x| = C$. This implies $|a\\,x|$ is constant along the characteristics. Since $a$ and $x$ do not cross zero in the parameter space, the sign of $a\\,x$ is also constant on each of the four connected components of the space. Therefore, the quantity $a\\,x$ itself is constant along the characteristic curves.\n\nThe general solution to the PDE is an arbitrary smooth function of this constant, $I(a,x) = F(a\\,x)$. We seek a nontrivial smooth invariant function. The simplest nontrivial choice is to take $F$ to be the identity function, yielding the invariant:\n$$\nI(a,x) = a\\,x\n$$\nThis is precisely the observable quantity $y$, which is expected, as the group action was constructed to preserve it.\n\n**3. Reparameterization**\n\nWe need to find a reparameterization $\\psi:(a,x)\\mapsto(u,v)$ that separates the identifiable parameter $u$ from the non-identifiable parameter $v$. The identifiable parameter $u$ must be an invariant of the action. Based on Part 2, we choose:\n$$\nu(a,x) = a\\,x\n$$\nThe non-identifiable parameter $v$ should parameterize the variation along the orbits (the level sets of $u$). A function that changes along the orbits is required. A convenient choice that is also a rational function of $a$ and $x$ is:\n$$\nv(a,x) = \\frac{a}{x}\n$$\nUnder the group action, $v$ transforms as $v(\\lambda a, \\lambda^{-1} x) = \\frac{\\lambda a}{\\lambda^{-1} x} = \\lambda^2 \\frac{a}{x} = \\lambda^2 v$. As $v$ is not invariant, it serves to parameterize the position along an orbit.\n\nThe proposed reparameterization is $\\psi(a,x) = (u,v) = (a\\,x, a/x)$. To verify that this is a valid local change of coordinates, we compute the Jacobian matrix of $\\psi$:\n$$\nJ_{\\psi} = \\begin{pmatrix} \\frac{\\partial u}{\\partial a} & \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial v}{\\partial a} & \\frac{\\partial v}{\\partial x} \\end{pmatrix} = \\begin{pmatrix} x & a \\\\ \\frac{1}{x} & -\\frac{a}{x^2} \\end{pmatrix}\n$$\nThe determinant of the Jacobian matrix is:\n$$\n\\det(J_{\\psi}) = (x)\\left(-\\frac{a}{x^2}\\right) - (a)\\left(\\frac{1}{x}\\right) = -\\frac{a}{x} - \\frac{a}{x} = -2\\frac{a}{x}\n$$\nThe parameter space is specified as $(a,x) \\in (\\mathbb{R}\\setminus\\{0\\}) \\times (\\mathbb{R}\\setminus\\{0\\})$. For any point in this space, both $a$ and $x$ are nonzero. Consequently, the determinant $\\det(J_{\\psi}) = -2a/x$ is never zero. By the inverse function theorem, this reparameterization is a local diffeomorphism everywhere on the parameter space.\n\n**4. Noisy Observation Model and Identifiability**\n\nWe now consider the noisy observation model:\n$$\nz = a\\,x + \\varepsilon, \\quad \\text{where } \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n$$\nThe observation $z$ is a realization of a random variable $Z$. For a fixed pair of parameters $(a,x)$, the distribution of the observable $Z$ is a normal distribution with mean $E[Z] = a\\,x$ and variance $\\text{Var}[Z] = \\sigma^2$. The probability density function (PDF) for an observation $z$ is:\n$$\np(z | a, x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z - a\\,x)^2}{2\\sigma^2}\\right)\n$$\nAccording to the definition, a parameter is identifiable if it is uniquely determined by the distribution of the observed data. Here, the distribution is fully specified by its mean and variance. Since $\\sigma^2$ is known, the only information about the parameters $(a,x)$ is contained in the mean, $E[Z] = a\\,x$.\n\nAny two pairs of parameters $(a_1, x_1)$ and $(a_2, x_2)$ such that $a_1 x_1 = a_2 x_2$ will result in identical distributions for the observation $Z$. This means that from the data, one can only determine the value of the product $a\\,x$. The set of all parameter pairs $(a,x)$ that yield the same data distribution is precisely an orbit of the group action defined in Part 1. Therefore, only the invariant quantity $u(a,x) = a\\,x$ is identifiable. Any parameter function that is not solely a function of $u=a\\,x$, such as $v=a/x$, is not identifiable from the distribution of $Z$.\n\nThis non-identifiability manifests in the likelihood function, which is proportional to the PDF:\n$$\nL(a,x; z) \\propto p(z | a, x)\n$$\nExpressing the likelihood in terms of the reparameterization $(u,v) = (a\\,x, a/x)$:\n$$\nL(u,v; z) \\propto \\exp\\left(-\\frac{(z - u)^2}{2\\sigma^2}\\right)\n$$\nThe likelihood function depends only on the parameter $u$ and is completely independent of the parameter $v$. This means that for any observed value $z$, the likelihood is constant for all values of $v$. Maximizing the likelihood with respect to the parameters will determine a value for $u$ (the maximum likelihood estimate is $\\hat{u}=z$), but it will provide no information to constrain $v$. The likelihood function is flat along the direction of the non-identifiable parameter $v$, which corresponds to movement along the orbits of the symmetry group.",
            "answer": "$$\\boxed{a\\,x}$$"
        },
        {
            "introduction": "While some models are intrinsically non-identifiable, in many cases, identifiability depends on how the system is observed or stimulated. This practice explores this crucial concept of experimental design within a simple dynamical system. You will discover how the choice of an input function $u(t)$ can either create or resolve parameter ambiguity, learning to design an input that guarantees the joint identifiability of all parameters. ",
            "id": "3390180",
            "problem": "Consider the scalar dynamical system with unknown parameters $\\theta_1$ and $\\theta_2$ given by\n$$\n\\dot{x}(t) \\;=\\; \\theta_1\\,u(t) \\;+\\; \\theta_2,\\quad x(0)=0,\n$$\nand suppose the measured output equals the state,\n$$\ny(t) \\;=\\; x(t),\n$$\nobserved continuously over a fixed finite horizon $[0,T]$ with $T>0$. Assume the measurement is corrupted by additive zero-mean Gaussian white noise with spectral density $r>0$. Work within the framework of inverse problems and data assimilation, and use the core definitions of structural identifiability (injectivity of the parameter-to-output map) and the Fisher Information Matrix (FIM) under Gaussian noise.\n\nYour tasks are:\n- Starting from the solution of the state equation and the definition of structural identifiability, derive a necessary and sufficient condition on the input $u(t)$ under which the parameter pair $(\\theta_1,\\theta_2)$ is structurally identifiable from $y(t)$ on $[0,T]$.\n- Then, using the standard construction of the Fisher Information Matrix (FIM) for continuous-time observations with additive Gaussian noise, express the FIM in terms of parameter sensitivities and time integrals. Use this to certify positive definiteness for your chosen input by showing the determinant is strictly positive for all $T>0$ and $r>0$.\n- Provide one explicit, closed-form input $u(t)$ that satisfies your identifiability condition and makes $(\\theta_1,\\theta_2)$ jointly identifiable from $y(t)$.\n\nProvide, as your final answer, a single explicit expression for $u(t)$ that achieves joint identifiability on $[0,T]$. No numerical rounding is required. No physical units are required.",
            "solution": "The problem asks for an analysis of the parameter identifiability for a simple scalar dynamical system. The analysis will proceed in three stages: first, deriving a condition for structural identifiability from its fundamental definition; second, confirming this condition using the Fisher Information Matrix (FIM); and third, providing a specific input function $u(t)$ that satisfies this condition.\n\nThe state equation is given by:\n$$\n\\dot{x}(t) = \\theta_1 u(t) + \\theta_2\n$$\nwith the initial condition $x(0) = 0$. The parameters $\\theta_1$ and $\\theta_2$ are unknown. The output is the state itself, $y(t) = x(t)$, observed over the interval $[0, T]$.\n\nFirst, we solve the state equation by direct integration from $0$ to $t$:\n$$\nx(t) - x(0) = \\int_0^t (\\theta_1 u(\\tau) + \\theta_2) d\\tau\n$$\nSubstituting $x(0)=0$, we obtain the state trajectory as a function of the parameters $\\theta = (\\theta_1, \\theta_2)$:\n$$\nx(t; \\theta) = \\theta_1 \\int_0^t u(\\tau) d\\tau + \\theta_2 t\n$$\nSince the noise-free output is $y(t; \\theta) = x(t; \\theta)$, this expression defines the parameter-to-output map.\n\n**Part 1: Structural Identifiability Condition**\n\nStructural identifiability requires that the parameter-to-output map be injective. This means that if two different parameter vectors, $\\theta = (\\theta_1, \\theta_2)$ and $\\theta' = (\\theta_1', \\theta_2')$, produce the same output for all time, then the parameter vectors must be identical. Formally, for all $t \\in [0, T]$, if $y(t; \\theta_1, \\theta_2) = y(t; \\theta_1', \\theta_2')$, then it must follow that $\\theta_1 = \\theta_1'$ and $\\theta_2 = \\theta_2'$.\n\nLet's assume $y(t; \\theta_1, \\theta_2) = y(t; \\theta_1', \\theta_2')$ for all $t \\in [0, T]$:\n$$\n\\theta_1 \\int_0^t u(\\tau) d\\tau + \\theta_2 t = \\theta_1' \\int_0^t u(\\tau) d\\tau + \\theta_2' t\n$$\nRearranging the terms, we get:\n$$\n(\\theta_1 - \\theta_1') \\int_0^t u(\\tau) d\\tau + (\\theta_2 - \\theta_2') t = 0\n$$\nLet $\\Delta\\theta_1 = \\theta_1 - \\theta_1'$ and $\\Delta\\theta_2 = \\theta_2 - \\theta_2'$. The equation becomes:\n$$\n\\Delta\\theta_1 \\int_0^t u(\\tau) d\\tau + \\Delta\\theta_2 t = 0\n$$\nFor the parameters to be identifiable, the only solution to this equation for all $t \\in [0, T]$ must be the trivial one, i.e., $\\Delta\\theta_1 = 0$ and $\\Delta\\theta_2 = 0$. This is a condition on the linear independence of the functions $S_1(t) = \\int_0^t u(\\tau) d\\tau$ and $S_2(t) = t$ over the interval $[0, T]$. If these two functions were linearly dependent, there would exist a non-zero pair $(\\Delta\\theta_1, \\Delta\\theta_2)$ satisfying the equation.\n\nLinear dependence means that one function is a constant multiple of the other:\n$$\n\\int_0^t u(\\tau) d\\tau = c \\cdot t\n$$\nfor some constant $c$. Assuming $u(t)$ is continuous, we can differentiate both sides with respect to $t$ (by the Fundamental Theorem of Calculus) to find the condition on $u(t)$:\n$$\nu(t) = c\n$$\nThus, if the input $u(t)$ is a constant, the functions $\\int_0^t u(\\tau) d\\tau$ and $t$ are linearly dependent, and the parameters are not structurally identifiable. For example, if $u(t)=c$, any pair $(\\Delta\\theta_1, \\Delta\\theta_2)$ such that $c\\Delta\\theta_1 + \\Delta\\theta_2 = 0$ would be a valid non-zero solution.\n\nTherefore, the necessary and sufficient condition for the parameter pair $(\\theta_1, \\theta_2)$ to be structurally identifiable is that the input $u(t)$ is not a constant function on the interval $[0, T]$.\n\n**Part 2: Fisher Information Matrix Analysis**\n\nFor a continuous-time observation $z(t) = y(t; \\theta) + \\eta(t)$, where $\\eta(t)$ is zero-mean Gaussian white noise with spectral density $r > 0$, the Fisher Information Matrix (FIM) is given by:\n$$\nF_{ij}(\\theta) = \\frac{1}{r} \\int_0^T \\frac{\\partial y(t; \\theta)}{\\partial \\theta_i} \\frac{\\partial y(t; \\theta)}{\\partial \\theta_j} dt\n$$\nThe functions $\\frac{\\partial y(t; \\theta)}{\\partial \\theta_i}$ are known as the parameter sensitivities. Let's calculate them for our system:\n$$\nS_1(t) = \\frac{\\partial y}{\\partial \\theta_1} = \\int_0^t u(\\tau) d\\tau\n$$\n$$\nS_2(t) = \\frac{\\partial y}{\\partial \\theta_2} = t\n$$\nThese are the same functions encountered in the structural identifiability analysis. The FIM is a $2 \\times 2$ symmetric matrix:\n$$\nF = \\frac{1}{r} \\begin{pmatrix} \\int_0^T S_1(t)^2 dt & \\int_0^T S_1(t)S_2(t) dt \\\\ \\int_0^T S_1(t)S_2(t) dt & \\int_0^T S_2(t)^2 dt \\end{pmatrix}\n$$\nThe parameters are locally identifiable if the FIM is positive definite. A symmetric matrix is positive definite if and only if its determinant is strictly positive (given that its diagonal elements are positive). The diagonal elements are $F_{11} = \\frac{1}{r}\\int_0^T S_1(t)^2 dt$ and $F_{22} = \\frac{1}{r}\\int_0^T S_2(t)^2 dt = \\frac{1}{r}\\int_0^T t^2 dt = \\frac{T^3}{3r}$. Since $T>0$ and $r>0$, $F_{22} > 0$. If $u(t)$ is not identically zero, $F_{11} > 0$. The determinant of $F$ is:\n$$\n\\det(F) = F_{11}F_{22} - F_{12}^2 = \\frac{1}{r^2} \\left[ \\left(\\int_0^T S_1(t)^2 dt\\right) \\left(\\int_0^T S_2(t)^2 dt\\right) - \\left(\\int_0^T S_1(t) S_2(t) dt\\right)^2 \\right]\n$$\nThe expression in the brackets is related to the Cauchy-Schwarz inequality for the inner product $\\langle f, g \\rangle = \\int_0^T f(t)g(t) dt$. The inequality states $\\langle f,g \\rangle^2 \\le \\langle f,f \\rangle \\langle g,g \\rangle$. The equality holds if and only if the functions $f$ and $g$ are linearly dependent.\nIn our case, this means $\\det(F) \\ge 0$. For the FIM to be positive definite, we need $\\det(F) > 0$. This strict inequality holds if and only if the sensitivity functions $S_1(t)$ and $S_2(t)$ are linearly independent on $[0, T]$. As established in Part 1, this is true if and only if $u(t)$ is not a constant function. This confirms the result from the structural identifiability analysis.\n\n**Part 3: An Explicit Input for Identifiability**\n\nTo satisfy the identifiability condition, we need to choose any input $u(t)$ that is not a constant. A simple and effective choice is a linear function of time:\n$$\nu(t) = t\n$$\nThis is clearly not a constant function for any $T>0$. To certify this choice, we will calculate the FIM and show its determinant is strictly positive.\nFor $u(t) = t$:\n$$\nS_1(t) = \\int_0^t \\tau d\\tau = \\frac{1}{2} t^2\n$$\n$$\nS_2(t) = t\n$$\nNow we compute the entries of the FIM:\n$$\nF_{11} = \\frac{1}{r} \\int_0^T \\left(\\frac{1}{2}t^2\\right)^2 dt = \\frac{1}{4r} \\int_0^T t^4 dt = \\frac{1}{4r} \\left[\\frac{t^5}{5}\\right]_0^T = \\frac{T^5}{20r}\n$$\n$$\nF_{12} = \\frac{1}{r} \\int_0^T \\left(\\frac{1}{2}t^2\\right)(t) dt = \\frac{1}{2r} \\int_0^T t^3 dt = \\frac{1}{2r} \\left[\\frac{t^4}{4}\\right]_0^T = \\frac{T^4}{8r}\n$$\n$$\nF_{22} = \\frac{1}{r} \\int_0^T t^2 dt = \\frac{1}{r} \\left[\\frac{t^3}{3}\\right]_0^T = \\frac{T^3}{3r}\n$$\nThe determinant of the FIM is:\n$$\n\\det(F) = F_{11}F_{22} - F_{12}^2 = \\left(\\frac{T^5}{20r}\\right) \\left(\\frac{T^3}{3r}\\right) - \\left(\\frac{T^4}{8r}\\right)^2\n$$\n$$\n\\det(F) = \\frac{T^8}{60r^2} - \\frac{T^8}{64r^2} = \\frac{T^8}{r^2} \\left(\\frac{1}{60} - \\frac{1}{64}\\right) = \\frac{T^8}{r^2} \\left(\\frac{64-60}{60 \\cdot 64}\\right) = \\frac{T^8}{r^2} \\left(\\frac{4}{3840}\\right) = \\frac{T^8}{960r^2}\n$$\nGiven that the problem specifies $T>0$ and $r>0$, it is clear that $T^8 > 0$ and $960r^2 > 0$. Thus, $\\det(F) > 0$. The FIM is positive definite, and the parameters $(\\theta_1, \\theta_2)$ are jointly identifiable with the input $u(t)=t$.\nOther simple examples of valid inputs include $u(t)=t^n$ for any integer $n \\ge 1$, or $u(t) = \\sin(\\omega t)$ for any $\\omega \\ne 0$. The problem asks for one such input.",
            "answer": "$$\n\\boxed{t}\n$$"
        },
        {
            "introduction": "The concept of symmetry-induced non-identifiability extends beyond continuous transformations in deterministic models. This practice delves into the statistical realm by examining the classic 'label switching' problem in mixture models, where the likelihood remains unchanged if we permute the component parameters. You will use the formal language of group theory, this time with a discrete group, to characterize this symmetry and understand why the model parameters are fundamentally only identifiable up to permutation. ",
            "id": "3390205",
            "problem": "Consider a two-component Gaussian mixture model used in a data assimilation setting, where the observation variable $y \\in \\mathbb{R}$ is modeled with probability density function (PDF)\n$$\np(y \\mid \\theta) = \\pi \\, \\varphi(y; \\mu_{1}, \\sigma^{2}) + (1 - \\pi) \\, \\varphi(y; \\mu_{2}, \\sigma^{2}),\n$$\nwhere $\\theta = (\\pi, \\mu_{1}, \\mu_{2})$, $0 < \\pi < 1$, $\\mu_{1}, \\mu_{2} \\in \\mathbb{R}$, and $\\sigma^{2} > 0$ is known and fixed. Here $\\varphi(y; \\mu, \\sigma^{2})$ denotes the Gaussian PDF with mean $\\mu$ and variance $\\sigma^{2}$. Assume a generic parameter regime in which $\\mu_{1} \\neq \\mu_{2}$ and $0 < \\pi < 1$.\n\nUsing the foundational definitions of identifiability in inverse problems, formalize the label switching symmetry in this model as a group action of the symmetric group on two symbols, denoted by $S_{2}$, acting on the parameter space $\\Theta = (0,1) \\times \\mathbb{R} \\times \\mathbb{R}$. Show that the action leaves the data likelihood invariant, and explain why this implies that parameters are only identifiable up to permutation of component labels.\n\nDefine the equivalence relation $\\sim$ on $\\Theta$ by $\\theta \\sim \\theta'$ if and only if there exists a permutation $\\gamma \\in S_{2}$ such that $\\theta' = \\gamma \\cdot \\theta$. Let $\\mathcal{O}(\\theta) = \\{ \\gamma \\cdot \\theta : \\gamma \\in S_{2} \\}$ denote the orbit of a generic parameter point under this action. Your final answer should be the cardinality of $\\mathcal{O}(\\theta)$ for generic $\\theta$ as specified above. Provide your answer as an exact integer. No rounding is required.",
            "solution": "The problem asks for an analysis of the identifiability of parameters in a two-component Gaussian mixture model. We must first formalize the label switching symmetry as a group action, demonstrate that this action leaves the likelihood invariant, and then determine the cardinality of the orbit of a generic parameter vector under this action.\n\nA parameter vector $\\theta$ is structurally identifiable if for any other parameter vector $\\theta' \\neq \\theta$ in the parameter space $\\Theta$, the corresponding probability distributions are distinct, i.e., $p(\\cdot \\mid \\theta) \\neq p(\\cdot \\mid \\theta')$. Equivalently, if $p(y \\mid \\theta) = p(y \\mid \\theta')$ for all possible observations $y$, then it must be that $\\theta = \\theta'$. If this condition is violated, the parameters are non-identifiable.\n\nThe model is a two-component Gaussian mixture with probability density function (PDF):\n$$\np(y \\mid \\theta) = \\pi \\, \\varphi(y; \\mu_{1}, \\sigma^{2}) + (1 - \\pi) \\, \\varphi(y; \\mu_{2}, \\sigma^{2})\n$$\nThe parameter vector to be identified is $\\theta = (\\pi, \\mu_{1}, \\mu_{2})$, which resides in the parameter space $\\Theta = (0,1) \\times \\mathbb{R} \\times \\mathbb{R}$. The variance $\\sigma^2$ is known. The components of the mixture are implicitly labeled `1` and `2`, corresponding to parameters $(\\pi, \\mu_1)$ and $(1-\\pi, \\mu_2)$, respectively.\n\nThe \"label switching\" symmetry corresponds to swapping the identities of these two components. This can be formalized using the action of the symmetric group on two symbols, $S_{2} = \\{e, \\gamma\\}$, where $e$ is the identity element and $\\gamma$ is the transposition (swap) element. We define a group action of $S_2$ on the parameter space $\\Theta$.\n\nLet $\\theta = (\\pi, \\mu_1, \\mu_2) \\in \\Theta$. The action of an element of $S_2$ on $\\theta$ is defined as follows:\n$1$. The identity element $e$ maps $\\theta$ to itself:\n$$\ne \\cdot \\theta = e \\cdot (\\pi, \\mu_1, \\mu_2) = (\\pi, \\mu_1, \\mu_2)\n$$\n$2$. The transposition element $\\gamma$ swaps the parameters of component $1$ with those of component $2$. The new weight for component $1$ becomes the old weight for component $2$, which was $1-\\pi$. The new mean for component $1$ becomes the old mean for component $2$, which was $\\mu_2$. Similarly, the new parameters for component $2$ are the old parameters for component $1$. Thus, the action is:\n$$\n\\gamma \\cdot \\theta = \\gamma \\cdot (\\pi, \\mu_1, \\mu_2) = (1-\\pi, \\mu_2, \\mu_1)\n$$\nThis defines a map $S_2 \\times \\Theta \\to \\Theta$. We can verify that this is a valid group action. The identity property is satisfied by definition. For compatibility, we check for $\\gamma$: $\\gamma \\cdot (\\gamma \\cdot \\theta) = \\gamma \\cdot (1-\\pi, \\mu_2, \\mu_1) = (1 - (1-\\pi), \\mu_1, \\mu_2) = (\\pi, \\mu_1, \\mu_2) = e \\cdot \\theta$. Since $\\gamma^2=e$ in $S_2$, the compatibility condition holds.\n\nNext, we show that this action leaves the likelihood $p(y \\mid \\theta)$ invariant. Let $\\theta' = \\gamma \\cdot \\theta = (1-\\pi, \\mu_2, \\mu_1)$. We evaluate the PDF for this new parameter vector $\\theta'$:\n$$\np(y \\mid \\theta') = \\pi' \\, \\varphi(y; \\mu'_{1}, \\sigma^{2}) + (1 - \\pi') \\, \\varphi(y; \\mu'_{2}, \\sigma^{2})\n$$\nSubstituting $\\pi' = 1-\\pi$, $\\mu'_{1} = \\mu_2$, and $\\mu'_{2} = \\mu_1$:\n$$\np(y \\mid \\theta') = (1-\\pi) \\, \\varphi(y; \\mu_2, \\sigma^{2}) + (1 - (1-\\pi)) \\, \\varphi(y; \\mu_1, \\sigma^{2})\n$$\n$$\np(y \\mid \\theta') = (1-\\pi) \\, \\varphi(y; \\mu_2, \\sigma^{2}) + \\pi \\, \\varphi(y; \\mu_1, \\sigma^{2})\n$$\nBy the commutativity of addition, this is identical to the original PDF:\n$$\np(y \\mid \\theta') = \\pi \\, \\varphi(y; \\mu_1, \\sigma^{2}) + (1-\\pi) \\, \\varphi(y; \\mu_2, \\sigma^{2}) = p(y \\mid \\theta)\n$$\nThis demonstrates that $p(y \\mid \\theta) = p(y \\mid \\gamma \\cdot \\theta)$ for all $y$.\n\nThe problem specifies a generic parameter regime where $\\mu_1 \\neq \\mu_2$ and $0 < \\pi < 1$. In this regime, the parameter vector $\\theta = (\\pi, \\mu_1, \\mu_2)$ is distinct from the permuted vector $\\theta' = \\gamma \\cdot \\theta = (1-\\pi, \\mu_2, \\mu_1)$. They are distinct because their second and third components, $(\\mu_1, \\mu_2)$ and $(\\mu_2, \\mu_1)$, are different since $\\mu_1 \\neq \\mu_2$. Since we have found two distinct parameter vectors $\\theta \\neq \\theta'$ that produce an identical likelihood function for all data $y$, the model parameters are not structurally identifiable. From observational data alone, it is impossible to distinguish between $\\theta$ and $\\theta'$. The parameters are said to be identifiable only up to a permutation of the component labels.\n\nThe equivalence relation $\\sim$ on $\\Theta$ is defined by the group action: $\\theta \\sim \\theta'$ if there exists a $\\gamma' \\in S_{2}$ such that $\\theta' = \\gamma' \\cdot \\theta$. The equivalence classes under this relation are the orbits of the group action. The orbit of a parameter point $\\theta$ is the set of all points that can be reached from $\\theta$ by the group action:\n$$\n\\mathcal{O}(\\theta) = \\{ g \\cdot \\theta : g \\in S_{2} \\}\n$$\nSince $S_2 = \\{e, \\gamma\\}$, the orbit of $\\theta = (\\pi, \\mu_1, \\mu_2)$ is:\n$$\n\\mathcal{O}(\\theta) = \\{ e \\cdot \\theta, \\gamma \\cdot \\theta \\} = \\{ (\\pi, \\mu_1, \\mu_2), (1-\\pi, \\mu_2, \\mu_1) \\}\n$$\nThe final task is to find the cardinality of this orbit, $|\\mathcal{O}(\\theta)|$, for a generic $\\theta$. The cardinality is the number of distinct elements in the set $\\mathcal{O}(\\theta)$. This number can be either $1$ or $2$.\nThe cardinality is $1$ if and only if the two elements in the set are identical, i.e., $(\\pi, \\mu_1, \\mu_2) = (1-\\pi, \\mu_2, \\mu_1)$. This equality holds if and only if $\\pi = 1-\\pi$ and $\\mu_1 = \\mu_2$. This would require $\\pi = \\frac{1}{2}$ and $\\mu_1 = \\mu_2$.\nHowever, the problem specifies a generic parameter regime where $\\mu_1 \\neq \\mu_2$. This condition explicitly prevents the two components from being identical. Consequently, the two elements in the orbit set are always distinct:\n$$\n(\\pi, \\mu_1, \\mu_2) \\neq (1-\\pi, \\mu_2, \\mu_1)\n$$\nThis is because their second components, $\\mu_1$ and $\\mu_2$, are not equal. Therefore, for any $\\theta$ in the specified generic regime, the orbit $\\mathcal{O}(\\theta)$ contains exactly two distinct parameter vectors.\n\nThe cardinality of the orbit is $2$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}