{
    "hands_on_practices": [
        {
            "introduction": "The most effective way to understand an algorithm is to build it. This first exercise guides you through the implementation of a mixed-precision iterative refinement solver, combining a fast, low-precision factorization with high-precision residual computations and solution updatesâ€”a technique central to modern high-performance numerical libraries. This hands-on task solidifies the core mechanics of iterative refinement and demonstrates its power across a variety of test matrices .",
            "id": "3245146",
            "problem": "Implement a program that performs mixed-precision iterative refinement for solving linear systems. You are given a square nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{n}$. The goal is to compute an improved solution to $A x = b$ by using a low-precision factorization and solve together with a high-precision residual computation. Your program must construct a Lower-Upper (LU) factorization with partial pivoting of $A$ in single precision (that is, using $32$-bit floating point arithmetic), use it to obtain an initial solution in single precision, and then iteratively refine the solution in double precision (that is, using $64$-bit floating point arithmetic) using residual correction until a specified stopping criterion is met or a maximum number of iterations is reached.\n\nUse the following foundations and definitions as the base of your design:\n- The residual of an approximate solution $\\hat{x}$ to $A x = b$ is $r = b - A \\hat{x}$.\n- The induced infinity norm of a vector $v$ is $\\lVert v \\rVert_{\\infty} = \\max_{i} |v_{i}|$. The induced infinity norm of a matrix $A$ is $\\lVert A \\rVert_{\\infty} = \\max_{i} \\sum_{j} |a_{ij}|$.\n- The floating-point rounding model is $fl(z) = z (1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff. You will emulate low precision by explicitly using single precision arithmetic for the factorization and the triangular solves, and high precision by using double precision arithmetic for the residual computation and solution updates.\n\nAlgorithmic requirements for your implementation:\n- Construct the LU factorization with partial pivoting $P A = L U$ in single precision for each test matrix $A$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular.\n- Compute the initial approximate solution $\\hat{x}_{0}$ by solving $L U \\hat{x}_{0} = P b$ entirely in single precision and then cast $\\hat{x}_{0}$ to double precision.\n- For refinement iteration $k = 0, 1, 2, \\dots$, compute the residual $r_{k} = b - A \\hat{x}_{k}$ in double precision and the relative backward-error-like measure\n$$\n\\eta_{k} = \\frac{\\lVert r_{k} \\rVert_{\\infty}}{\\lVert A \\rVert_{\\infty} \\, \\lVert \\hat{x}_{k} \\rVert_{\\infty} + \\lVert b \\rVert_{\\infty}}.\n$$\nIf the denominator is $0$ (that is, both $\\lVert \\hat{x}_{k} \\rVert_{\\infty}$ and $\\lVert b \\rVert_{\\infty}$ are $0$), define $\\eta_{k} = 0$.\n- If $\\eta_{k} \\leq \\tau$, stop and report the number of refinement steps taken so far (that is, the number of correction solves actually performed, which is $0$ if the initial solution already satisfies the tolerance).\n- Otherwise, compute a correction $d_{k}$ by solving $L U d_{k} = P r_{k}$ in single precision, cast $d_{k}$ to double precision, and set $\\hat{x}_{k+1} = \\hat{x}_{k} + d_{k}$ in double precision. Repeat until convergence or until reaching a maximum of $K_{\\max}$ refinement steps. If convergence is not achieved within $K_{\\max}$ steps, report failure with the value $-1$.\n\nUse the following fixed parameters for all cases:\n- Tolerance $\\tau = 10^{-12}$.\n- Maximum number of refinement steps $K_{\\max} = 20$.\n\nTest suite:\nImplement and run your solver on the following four test cases. For each case, define $A$ and $b$ exactly as below and apply the same algorithm and parameters.\n\n- Case $1$ (symmetric positive definite, general right-hand side):\n  $$\n  A = \\begin{bmatrix}\n  4 & 1 & 0 \\\\\n  1 & 3 & 1 \\\\\n  0 & 1 & 2\n  \\end{bmatrix},\n  \\quad\n  b = \\begin{bmatrix}\n  1 \\\\ 2 \\\\ 3\n  \\end{bmatrix}.\n  $$\n\n- Case $2$ (zero right-hand side, immediate boundary case):\n  $$\n  A = \\begin{bmatrix}\n  2 & -1 \\\\\n  -1 & 2\n  \\end{bmatrix},\n  \\quad\n  b = \\begin{bmatrix}\n  0 \\\\ 0\n  \\end{bmatrix}.\n  $$\n\n- Case $3$ (Hilbert matrix, ill-conditioned but nonsingular, known exact solution $x = \\mathbf{1}$):\n  For $n = 5$, let $H \\in \\mathbb{R}^{5 \\times 5}$ with entries $h_{ij} = \\frac{1}{i + j + 1}$ for $i, j \\in \\{0, 1, 2, 3, 4\\}$, and set $A = H$ and $b = H \\mathbf{1}$, where $\\mathbf{1}$ is the vector of all ones in $\\mathbb{R}^{5}$.\n\n- Case $4$ (scaled, weakly coupled, mixed magnitudes):\n  $$\n  A = \\begin{bmatrix}\n  10^{-3} & 10^{-6} & 0 & 0 \\\\\n  10^{-6} & 1 & 10^{-3} & 0 \\\\\n  0 & 10^{-3} & 10^{3} & 2 \\\\\n  0 & 0 & 2 & 2 \\times 10^{2}\n  \\end{bmatrix},\n  \\quad\n  b = \\begin{bmatrix}\n  1 \\\\ 2 \\\\ 3 \\\\ 4\n  \\end{bmatrix}.\n  $$\n\nOutput specification:\n- Your program must produce a single line of output containing a list of exactly four integers in the order of the test cases, where each integer is the number of refinement steps used to meet the tolerance for that case, or $-1$ if the method did not converge within $K_{\\max}$ steps. The line must be formatted as a comma-separated list enclosed in square brackets, for example, $[n_{1},n_{2},n_{3},n_{4}]$.\n\nNo external input is provided; all data must be defined within the program. Angles, physical units, or percentages are not involved in this problem. All quantities are purely mathematical and dimensionless.",
            "solution": "The problem requires the implementation of a mixed-precision iterative refinement algorithm for a linear system of equations $A x = b$. The core principle of this method is to leverage the speed of low-precision arithmetic for computationally expensive tasks while using high-precision arithmetic for accuracy-critical computations. Specifically, the computationally intensive LU factorization of the matrix $A$ and the subsequent triangular solves for the solution are performed in single precision ($32$-bit floating-point arithmetic). The residual calculation, which is prone to catastrophic cancellation and thus demands higher accuracy, is performed in double precision ($64$-bit floating-point arithmetic).\n\nThe algorithm proceeds as follows:\n\n1.  **Data Preparation and Precision Management**:\n    The input matrix $A$ and vector $b$ are initially treated as double-precision entities, which we denote as $A_{dp}$ and $b_{dp}$. We compute their infinity norms, $\\lVert A \\rVert_{\\infty}$ and $\\lVert b \\rVert_{\\infty}$, in double precision, as these values are constant throughout the process and are used in the stopping criterion. For the low-precision parts of the algorithm, we create single-precision copies of the matrix and vector, $A_{sp}$ and $b_{sp}$, by casting the double-precision versions. In `NumPy`, this corresponds to using `numpy.float64` for double precision and `numpy.float32` for single precision.\n\n2.  **Single-Precision LU Factorization**:\n    We compute the LU factorization of the single-precision matrix $A_{sp}$ with partial pivoting. This decomposition yields $P A_{sp} = L_{sp} U_{sp}$, where $P$ is a permutation matrix representing row swaps, $L_{sp}$ is a unit lower triangular matrix, and $U_{sp}$ is an upper triangular matrix. Since `NumPy` does not provide a direct function to obtain the $P, L, U$ factors in a convenient form, we must implement this factorization manually. Our implementation will produce a compact matrix containing the elements of both $L_{sp}$ (in the strict lower triangle) and $U_{sp}$ (in the upper triangle including the diagonal), along with a permutation vector that represents $P$.\n\n3.  **Initial Solution**:\n    The first approximate solution, $\\hat{x}_{0}$, is computed entirely in single precision. Using the factorization, the original system $A x = b$ is transformed into $L_{sp} U_{sp} \\hat{x}_{0} = P b_{sp}$. This is solved in two steps:\n    a.  **Forward Substitution**: Solve $L_{sp} y = P b_{sp}$ for the intermediate vector $y$.\n    b.  **Backward Substitution**: Solve $U_{sp} \\hat{x}_{0} = y$ for the initial solution $\\hat{x}_{0}$.\n    The resulting single-precision vector $\\hat{x}_{0}$ is then cast to double precision to be used in the high-precision refinement loop.\n\n4.  **Iterative Refinement Loop**:\n    The refinement process iterates to improve the solution's accuracy. The loop runs for a maximum of $K_{\\max} = 20$ refinement steps. Let $\\hat{x}_k$ be the approximate solution at iteration $k$ (with $\\hat{x}_0$ being the initial solution).\n\n    a.  **Residual Calculation (Double Precision)**: The residual $r_k = b_{dp} - A_{dp} \\hat{x}_k$ is computed using double-precision arithmetic. This step is critical as it recovers the error information that was lost in the single-precision calculations.\n\n    b.  **Convergence Criterion**: We evaluate the relative backward-error-like measure $\\eta_k$:\n        $$\n        \\eta_{k} = \\frac{\\lVert r_{k} \\rVert_{\\infty}}{\\lVert A \\rVert_{\\infty} \\, \\lVert \\hat{x}_{k} \\rVert_{\\infty} + \\lVert b \\rVert_{\\infty}}\n        $$\n        All norms are computed in double precision. If $\\eta_k$ is less than or equal to the specified tolerance $\\tau = 10^{-12}$, the solution is considered converged, and the process terminates. The number of refinement steps performed, $k$, is reported. For the initial check ($k=0$), if $\\eta_0 \\le \\tau$, we report $0$ steps.\n\n    c.  **Correction Step (Single Precision)**: If the solution has not converged, we compute a correction term $d_k$ by solving the system $A d_k = r_k$. We reuse the single-precision LU factors for efficiency: $L_{sp} U_{sp} d_k = P r_k$. The double-precision residual $r_k$ is first cast to single precision. The system is then solved for $d_k$ using forward and backward substitution in single precision.\n\n    d.  **Solution Update (Double Precision)**: The single-precision correction vector $d_k$ is cast back to double precision and added to the current solution: $\\hat{x}_{k+1} = \\hat{x}_k + d_k$. This update is performed in double precision to accumulate the corrections accurately.\n\n5.  **Termination**:\n    The loop terminates upon either satisfying the convergence criterion $\\eta_k \\le \\tau$ or after completing $K_{\\max}$ refinement steps. If the loop completes without convergence after checking the solution $\\hat{x}_{K_{max}}$, the method is considered to have failed, and the value $-1$ is reported. The number of refinements is defined as the number of correction solves performed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lu_pivot(A_in):\n    \"\"\"\n    Computes LU factorization with partial pivoting for a matrix A.\n    The matrix A_in is expected to be of single precision (np.float32).\n    \n    Returns:\n    - LU: A matrix containing U in its upper triangle and L in its strict lower triangle.\n    - p: A permutation vector.\n    \"\"\"\n    n = A_in.shape[0]\n    LU = A_in.copy()\n    p = np.arange(n)\n    \n    for j in range(n - 1):\n        # Find pivot in column j (from row j downwards)\n        max_row_idx_local = np.argmax(np.abs(LU[j:, j]))\n        max_row_idx_global = j + max_row_idx_local\n        \n        # Swap rows in LU and permutation vector p\n        if max_row_idx_global != j:\n            LU[[j, max_row_idx_global], :] = LU[[max_row_idx_global, j], :]\n            p[[j, max_row_idx_global]] = p[[max_row_idx_global, j]]\n            \n        # Elimination\n        pivot_val = LU[j, j]\n        # Use a small tolerance for checking non-singularity\n        if np.abs(pivot_val) > np.finfo(np.float32).tiny:\n            multipliers = LU[j + 1:, j] / pivot_val\n            LU[j + 1:, j] = multipliers\n            # Vectorized update of the submatrix\n            LU[j + 1:, j + 1:] -= np.outer(multipliers, LU[j, j + 1:])\n            \n    return LU, p\n\ndef solve_lu_sp(LU_sp, p_sp, b_sp):\n    \"\"\"\n    Solves the system LUx = Pb using the output of lu_pivot.\n    All computations are in single precision (np.float32).\n    \"\"\"\n    n = LU_sp.shape[0]\n    \n    # Apply permutation to the right-hand side vector\n    b_perm = b_sp[p_sp]\n    \n    # Forward substitution (solves Ly = b_perm)\n    y = np.zeros(n, dtype=np.float32)\n    for i in range(n):\n        y[i] = b_perm[i] - np.dot(LU_sp[i, :i], y[:i])\n        \n    # Backward substitution (solves Ux = y)\n    x = np.zeros(n, dtype=np.float32)\n    for i in range(n - 1, -1, -1):\n        dot_product = np.dot(LU_sp[i, i + 1:], x[i + 1:])\n        diag_val = LU_sp[i, i]\n        if np.abs(diag_val) > np.finfo(np.float32).tiny:\n             x[i] = (y[i] - dot_product) / diag_val\n        else:\n             # This case should ideally not be hit with non-singular matrices\n             # and proper pivoting, but provides a fallback.\n             x[i] = (y[i] - dot_product) / np.finfo(np.float32).tiny\n    return x\n\ndef mixed_precision_refinement(A_in, b_in, K_max, tau):\n    \"\"\"\n    Performs mixed-precision iterative refinement to solve Ax = b.\n    \"\"\"\n    n = A_in.shape[0]\n    \n    # Master copies and norms in double precision (np.float64)\n    A_dp = A_in.astype(np.float64)\n    b_dp = b_in.astype(np.float64)\n    norm_A_inf = np.linalg.norm(A_dp, ord=np.inf)\n    norm_b_inf = np.linalg.norm(b_dp, ord=np.inf)\n    \n    # Factorization in single precision (np.float32)\n    A_sp = A_dp.astype(np.float32)\n    b_sp = b_dp.astype(np.float32)\n    LU_sp, p_sp = lu_pivot(A_sp)\n    \n    # Initial solution computed in single precision\n    x_k_sp = solve_lu_sp(LU_sp, p_sp, b_sp)\n    x_k_dp = x_k_sp.astype(np.float64)\n    \n    # --- Start of the iterative refinement loop ---\n    # k represents the number of refinement steps performed.\n    for k in range(K_max + 1):\n        # Calculate residual in double precision\n        r_k_dp = b_dp - (A_dp @ x_k_dp)\n        \n        # Check stopping criterion\n        norm_r_inf = np.linalg.norm(r_k_dp, ord=np.inf)\n        norm_x_inf = np.linalg.norm(x_k_dp, ord=np.inf)\n        \n        denominator = norm_A_inf * norm_x_inf + norm_b_inf\n        eta_k = norm_r_inf / denominator if denominator != 0.0 else 0.0\n        \n        if eta_k = tau:\n            return k\n            \n        # If max iterations reached, break before next correction\n        if k == K_max:\n            break\n            \n        # Solve for correction in single precision\n        r_k_sp = r_k_dp.astype(np.float32)\n        d_k_sp = solve_lu_sp(LU_sp, p_sp, r_k_sp)\n        \n        # Update solution in double precision\n        d_k_dp = d_k_sp.astype(np.float64)\n        x_k_dp += d_k_dp\n        \n    # If the loop completes without converging\n    return -1\n\ndef solve():\n    # Define fixed parameters\n    tau = 1e-12\n    K_max = 20\n\n    # Define the test cases from the problem statement.\n    # Case 1\n    A1 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 1.0],\n        [0.0, 1.0, 2.0]\n    ])\n    b1 = np.array([1.0, 2.0, 3.0])\n\n    # Case 2\n    A2 = np.array([\n        [2.0, -1.0],\n        [-1.0, 2.0]\n    ])\n    b2 = np.array([0.0, 0.0])\n\n    # Case 3\n    n3 = 5\n    # Use double precision for construction\n    H3 = np.zeros((n3, n3), dtype=np.float64)\n    for i in range(n3):\n        for j in range(n3):\n            H3[i, j] = 1.0 / (i + j + 1.0)\n    A3 = H3\n    b3 = A3 @ np.ones(n3, dtype=np.float64)\n\n    # Case 4\n    A4 = np.array([\n        [1e-3, 1e-6, 0.0, 0.0],\n        [1e-6, 1.0, 1e-3, 0.0],\n        [0.0, 1e-3, 1e3, 2.0],\n        [0.0, 0.0, 2.0, 2e2]\n    ])\n    b4 = np.array([1.0, 2.0, 3.0, 4.0])\n\n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4)\n    ]\n\n    results = []\n    for A, b in test_cases:\n        num_steps = mixed_precision_refinement(A, b, K_max, tau)\n        results.append(num_steps)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having implemented a mixed-precision solver, a crucial question arises: why is the change in precision necessary? This analytical problem delves into the theoretical underpinnings of iterative refinement by examining what happens when all computations are performed in a single, uniform precision. By deriving the error recurrence relation for a specific ill-conditioned system, you will discover how rounding errors in the residual calculation can lead to divergence, providing a rigorous justification for the mixed-precision approach and the condition $\\kappa(A)u \\lt 1$ .",
            "id": "3552174",
            "problem": "Consider solving the linear system $A x = b$ by classical iterative refinement in the same precision as the initial factorization, using Institute of Electrical and Electronics Engineers (IEEE) $754$ single precision arithmetic throughout. Let the unit roundoff be $u_{s} = 2^{-24}$. Classical iterative refinement here means: compute an initial solution $x_{0}$ from a single-precision factorization of $A$, and then for $k = 0, 1, 2, \\dots$ iterate\n$r_{k} = \\operatorname{fl}(b - A x_{k})$ in single precision, solve $A d_{k} = r_{k}$ using the same single-precision factors to obtain $d_{k}$ in single precision, and update $x_{k+1} = \\operatorname{fl}(x_{k} + d_{k})$ in single precision. Assume the standard first-order normwise floating-point model for linear algebraic operations: for any matrix-vector product, $\\operatorname{fl}(A x) = (A + \\Delta A) x$ with $\\| \\Delta A \\|_{2} \\le u_{s} \\|A\\|_{2}$, and for any vector sum or difference, $\\operatorname{fl}(u \\pm v) = u \\pm v + \\delta$ with $\\|\\delta\\|_{2} \\le u_{s} \\|u \\pm v\\|_{2}$. Use the matrix $A_{\\varepsilon} = \\operatorname{diag}(1, \\varepsilon)$ with $\\varepsilon = 2^{-30}$, and let $b = A_{\\varepsilon} x^{\\star}$ for the exact solution $x^{\\star} = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$ so that the true solution is known.\n\nStarting from the above floating-point model only (do not assume any specialized refinement formulas), derive a linearized forward-error recurrence for $e_{k} = x_{k} - x^{\\star}$ in the $2$-norm that separates the contribution depending on $e_{k}$ from the contribution independent of $e_{k}$. Isolate the coefficient that multiplies $\\|e_{k}\\|_{2}$ arising specifically from the residual-roundoff term $-\\Delta A\\, e_{k}$, propagate it through the solve and update, and thereby obtain the first-order linearized error amplification factor $\\rho$ that multiplies $\\|e_{k}\\|_{2}$ in the recurrence. Evaluate this $\\rho$ for $A_{\\varepsilon}$, $u_{s} = 2^{-24}$, and $\\varepsilon = 2^{-30}$, and report it as a single real number. No rounding is required; report the exact value.",
            "solution": "We work with the $2$-norm and the standard first-order normwise floating-point model. The exact residual is $r_{k} = b - A x_{k} = A x^{\\star} - A x_{k} = -A e_{k}$. In single precision, the computed residual satisfies\n$$\n\\tilde{r}_{k} = \\operatorname{fl}(b - A x_{k}) = (b + \\delta b) - (A + \\Delta A) x_{k},\n$$\nwith $\\|\\delta b\\|_{2} \\le u_{s} \\|b\\|_{2}$ and $\\|\\Delta A\\|_{2} \\le u_{s} \\|A\\|_{2}$. Thus\n$$\n\\tilde{r}_{k} = r_{k} + \\underbrace{\\delta b - \\Delta A x_{k}}_{\\Delta r_{k}} = -A e_{k} + \\Delta r_{k}.\n$$\nDecompose $\\Delta r_{k}$ into the part depending on $e_{k}$ and the part independent of $e_{k}$:\n$$\n\\Delta r_{k} = -\\Delta A\\, e_{k} + \\bigl(\\delta b - \\Delta A\\, x^{\\star}\\bigr).\n$$\nThe first term, $-\\Delta A\\, e_{k}$, is the only term proportional to $e_{k}$ and will generate the contraction (or amplification) factor in the linearized recurrence. The second term is a forcing term independent of $e_{k}$ that sets a floor on attainable accuracy but does not contribute to the linear amplification of $\\|e_{k}\\|_{2}$.\n\nNext, the correction $d_{k}$ is computed by solving $A d_{k} = \\tilde{r}_{k}$ using the same single-precision factors. For $A_{\\varepsilon} = \\operatorname{diag}(1, \\varepsilon)$, the factorization is exact in exact arithmetic and the triangular solves reduce to componentwise divisions. Under the first-order model, the computed correction can be written as\n$$\n\\tilde{d}_{k} = (I + S_{k}) A^{-1} \\tilde{r}_{k},\n$$\nwith $\\|S_{k}\\|_{2} \\le u_{s}$, because each componentwise division incurs a relative error bounded by $u_{s}$ and the resulting diagonal scaling matrix has operator norm bounded by $1 + u_{s}$. Substituting $\\tilde{r}_{k}$ gives\n$$\n\\tilde{d}_{k} = (I + S_{k}) A^{-1} \\bigl(-A e_{k} - \\Delta A\\, e_{k} + (\\delta b - \\Delta A\\, x^{\\star})\\bigr) \\\\\n= -(I + S_{k}) e_{k} - (I + S_{k}) A^{-1} \\Delta A\\, e_{k} + (I + S_{k}) A^{-1} (\\delta b - \\Delta A\\, x^{\\star}).\n$$\n\nFinally, the update is computed in single precision:\n$$\nx_{k+1} = \\operatorname{fl}(x_{k} + \\tilde{d}_{k}) = x_{k} + \\tilde{d}_{k} + u_{k},\n$$\nwith $\\|u_{k}\\|_{2} \\le u_{s} \\|x_{k} + \\tilde{d}_{k}\\|_{2}$. Subtracting $x^{\\star}$ and using $e_{k} = x_{k} - x^{\\star}$,\n$$\ne_{k+1} = e_{k} + \\tilde{d}_{k} + u_{k}.\n$$\nSubstitute $\\tilde{d}_{k}$:\n$$\ne_{k+1} = e_{k} - (I + S_{k}) e_{k} - (I + S_{k}) A^{-1} \\Delta A\\, e_{k} + (I + S_{k}) A^{-1} (\\delta b - \\Delta A\\, x^{\\star}) + u_{k}.\n$$\nCollecting the terms proportional to $e_{k}$ and discarding products of perturbations (second order in $u_{s}$), we obtain the linearized recurrence\n$$\ne_{k+1} \\approx \\underbrace{\\bigl(- S_{k} - A^{-1} \\Delta A \\bigr)}_{T_{k}} e_{k} \\;+\\; \\underbrace{A^{-1} (\\delta b - \\Delta A\\, x^{\\star}) + u_{k}}_{\\text{independent of } e_{k}}.\n$$\nHence, to first order in $u_{s}$, the coefficient that multiplies $\\|e_{k}\\|_{2}$ is governed by the operator norm of $T_{k}$:\n$$\n\\|T_{k}\\|_{2} \\le \\|S_{k}\\|_{2} + \\|A^{-1}\\|_{2} \\|\\Delta A\\|_{2} \\le u_{s} + u_{s} \\|A^{-1}\\|_{2} \\|A\\|_{2} = u_{s} + u_{s} \\kappa_{2}(A).\n$$\nThe contribution specifically arising from the residual-roundoff term $-\\Delta A\\, e_{k}$ is the part $\\|A^{-1} \\Delta A\\|_{2}$, which is bounded by $u_{s} \\kappa_{2}(A)$. It is conventional to regard this as the dominant linear amplification factor when $\\kappa_{2}(A)$ is large, since the solve and update rounding contribute only an additive $u_{s}$. Therefore, the first-order linearized error amplification factor attributable to residual roundoff is\n$$\n\\rho = u_{s} \\kappa_{2}(A).\n$$\n\nFor $A_{\\varepsilon} = \\operatorname{diag}(1, \\varepsilon)$ with $\\varepsilon = 2^{-30}$, we have $\\|A\\|_{2} = 1$ and $\\|A^{-1}\\|_{2} = \\varepsilon^{-1}$, so $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = \\varepsilon^{-1} = 2^{30}$. With $u_{s} = 2^{-24}$, the amplification factor is\n$$\n\\rho = u_{s} \\kappa_{2}(A) = 2^{-24} \\cdot 2^{30} = 2^{6} = 64.\n$$\nSince $\\rho  1$, the linearized recurrence predicts that classical single-precision residual refinement will fail to converge (the error component proportional to $\\|e_{k}\\|_{2}$ is amplified by a factor exceeding $1$), and the independent forcing terms prevent reduction below an $O(u_{s} \\kappa_{2}(A))$ floor in any case. This explains the failure: forming the residual in the same precision injects an $e_{k}$-dependent perturbation of size $u_{s} \\|A\\|_{2} \\|e_{k}\\|_{2}$ which, after application of $A^{-1}$, becomes $u_{s} \\kappa_{2}(A) \\|e_{k}\\|_{2}$, yielding $\\rho = 64$ for the given data.",
            "answer": "$$\\boxed{64}$$"
        },
        {
            "introduction": "With a robust implementation and theoretical justification in hand, we now turn to quantifying the practical impact of iterative refinement on a notoriously ill-conditioned problem. This exercise challenges you to apply your understanding to a system involving the Hilbert matrix, tracking the gain in the number of correct significant digits with each refinement step. This provides a clear and intuitive measure of the method's effectiveness and its limitations as the condition number approaches the boundaries of floating-point precision .",
            "id": "3245403",
            "problem": "You are to implement and analyze iterative refinement for solving a linear system with a Hilbert matrix, a classically ill-conditioned matrix. The goal is to quantify, per iteration, how many base-$10$ digits of accuracy are gained in the solution.\n\nFundamental base and definitions to use:\n- A linear system has the form $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$. The Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ is defined by $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1, \\dots, n\\}$.\n- The residual at iteration $k$ is $r^{(k)} = b - A x^{(k)}$.\n- The iterative refinement update at iteration $k$ generates a correction $d^{(k)}$ by solving $A d^{(k)} = r^{(k)}$, and sets $x^{(k+1)} = x^{(k)} + d^{(k)}$.\n- The number of correct base-$10$ digits at iteration $k$ is defined by\n$$\nD^{(k)} = -\\log_{10}\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty}\\right),\n$$\nwhere $x^\\star$ is the exact solution. For numerical stability in double precision arithmetic, report $D^{(k)}$ saturated at $16$ digits, meaning use\n$$\n\\tilde{D}^{(k)} = -\\log_{10}\\!\\left(\\max\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty},\\,10^{-16}\\right)\\right).\n$$\n- The per-iteration gain in correct digits is $G^{(k)} = \\tilde{D}^{(k)} - \\tilde{D}^{(k-1)}$ for $k \\ge 1$. The initial accuracy $\\tilde{D}^{(0)}$ corresponds to the solution obtained by a single direct solve of $A x = b$ before any refinement iterations.\n\nScientific realism and setup:\n- The Hilbert matrix is known to be ill-conditioned, with condition number growing rapidly as $n$ increases. Iterative refinement can improve the solution by correcting accumulated errors via residual solves.\n- Use double precision floating-point arithmetic ($64$-bit) for all computations.\n\nProgram requirements:\n- Construct $A$ as the Hilbert matrix of size $n$.\n- Set the true solution to $x^\\star = \\mathbf{1}$ (the all-ones vector of length $n$). Compute $b = A x^\\star$.\n- Compute an initial solution $x^{(0)}$ by directly solving $A x = b$.\n- Perform $m$ iterative refinement steps as described above.\n- After each iteration $k \\in \\{1, \\dots, m\\}$, compute and record $G^{(k)}$.\n- For each test case, output the list $[G^{(1)}, G^{(2)}, \\dots, G^{(m)}]$.\n\nTest suite:\n- Case $1$: $n = 2$, $m = 5$ (boundary case, relatively well-conditioned).\n- Case $2$: $n = 5$, $m = 5$ (moderately ill-conditioned).\n- Case $3$: $n = 8$, $m = 5$ (challenging ill-conditioning).\n- Case $4$: $n = 12$, $m = 5$ (severe ill-conditioning edge case).\n\nFinal output format:\n- Your program should produce a single line of output containing the per-iteration gains for all test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. For example, the output should look like $[[g_{1,1},\\dots,g_{1,m}],[g_{2,1},\\dots,g_{2,m}],\\dots]$, where $g_{i,k}$ is the gain for iteration $k$ in test case $i$.\n- All numbers should be printed as standard decimal floats, and there are no physical units involved in this problem.",
            "solution": "The user-provided problem statement has been independently validated and is determined to be a well-posed, scientifically grounded, and objective problem in the domain of numerical linear algebra. The problem is free of contradictions, ambiguities, and factual errors. Therefore, a complete solution is provided below.\n\nThe problem requires the implementation and analysis of the iterative refinement algorithm for solving a linear system $A x = b$, where $A$ is the notoriously ill-conditioned Hilbert matrix. The objective is to quantify the gain in solution accuracy, measured in base-$10$ digits, at each refinement step.\n\nThe Hilbert matrix $H$ of size $n \\times n$ is defined by its entries $H_{ij} = \\frac{1}{i + j - 1}$ for row and column indices $i, j$ starting from $1$. Its condition number grows extremely rapidly with $n$, making it a classic test case for numerical stability. For a system $A x = b$, a direct solution using methods like LU decomposition can accumulate significant floating-point error when $A$ is ill-conditioned, leading to an inaccurate computed solution.\n\nIterative refinement is a procedure designed to improve the accuracy of a computed solution. Let $x^{(0)}$ be the initial solution obtained by a direct solver. Due to finite precision arithmetic, $x^{(0)}$ differs from the true solution $x^\\star$ by an error $e^{(0)} = x^\\star - x^{(0)}$. The foundation of the method lies in estimating and correcting this error.\n\nThe residual vector for an approximate solution $x^{(k)}$ is defined as $r^{(k)} = b - A x^{(k)}$. By substituting $b = A x^\\star$, the residual can be related to the true error $e^{(k)} = x^\\star - x^{(k)}$:\n$$\nr^{(k)} = A x^\\star - A x^{(k)} = A (x^\\star - x^{(k)}) = A e^{(k)}\n$$\nThis equation shows that the true error $e^{(k)}$ is the solution to the linear system $A e^{(k)} = r^{(k)}$. Although we cannot compute $e^{(k)}$ exactly (as this would be equivalent to solving the original problem perfectly), we can compute an approximation to it, which we denote $d^{(k)}$, by solving the residual system:\n$$\nA d^{(k)} = r^{(k)}\n$$\nThe vector $d^{(k)}$ serves as a computed correction to the current solution. The next, hopefully more accurate, solution $x^{(k+1)}$ is obtained by applying this correction:\n$$\nx^{(k+1)} = x^{(k)} + d^{(k)}\n$$\nThis process is repeated iteratively. A critical aspect of iterative refinement is that the residual $r^{(k)}$ should ideally be computed with higher precision than the rest of the calculations. This problem, however, specifies the use of standard double precision ($64$-bit floats) for all operations, which allows us to observe the limits of the method when higher precision is not available.\n\nTo quantify the performance of the algorithm, we measure the accuracy of the solution at each step. The number of correct base-$10$ digits in the solution $x^{(k)}$ is defined relative to the true solution $x^\\star$:\n$$\nD^{(k)} = -\\log_{10}\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty}\\right)\n$$\nwhere $\\lVert \\cdot \\rVert_\\infty$ is the infinity norm (maximum absolute value of the vector's components). Since double-precision floating-point arithmetic has a finite precision of approximately $16$ decimal digits, it is practical to use a saturated measure of accuracy that does not exceed this limit and avoids taking the logarithm of zero:\n$$\n\\tilde{D}^{(k)} = -\\log_{10}\\!\\left(\\max\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty},\\,10^{-16}\\right)\\right)\n$$\nThe gain in accuracy at iteration $k$ is the difference in correct digits from the previous step:\n$$\nG^{(k)} = \\tilde{D}^{(k)} - \\tilde{D}^{(k-1)} \\quad \\text{for } k \\ge 1\n$$\nHere, $\\tilde{D}^{(0)}$ is the accuracy of the initial solution $x^{(0)}$ from the direct solve.\n\nThe algorithmic procedure for each test case $(n, m)$ is as follows:\n1.  **System Setup**:\n    *   Construct the $n \\times n$ Hilbert matrix $A$, where the entry at zero-based row $i$ and column $j$ is $A_{ij} = \\frac{1}{(i+1) + (j+1) - 1} = \\frac{1}{i+j+1}$.\n    *   Define the true solution as the all-ones vector, $x^\\star = \\mathbf{1} \\in \\mathbb{R}^n$.\n    *   Calculate the right-hand side vector $b = A x^\\star$. This ensures a known ground truth for error calculation. Since $x^\\star_j = 1$ for all $j$, each component $b_i$ is the $i$-th row sum of $A$: $b_i = \\sum_{j=1}^{n} \\frac{1}{i+j-1}$.\n\n2.  **Initial Solution**:\n    *   Compute the initial approximate solution $x^{(0)}$ by solving the system $A x = b$ using a standard direct numerical solver.\n    *   Calculate the initial accuracy $\\tilde{D}^{(0)}$.\n\n3.  **Iterative Refinement**:\n    *   Initialize the current solution $x \\leftarrow x^{(0)}$ and previous accuracy $D_{prev} \\leftarrow \\tilde{D}^{(0)}$.\n    *   For $k$ from $1$ to $m$:\n        a. Compute the residual: $r = b - A x$.\n        b. Solve for the correction: $A d = r$.\n        c. Update the solution: $x \\leftarrow x + d$.\n        d. Calculate the new accuracy: $D_{current} = \\tilde{D}^{(k)}$.\n        e. Calculate and record the gain: $G^{(k)} = D_{current} - D_{prev}$.\n        f. Update the previous accuracy: $D_{prev} \\leftarrow D_{current}$.\n\n4.  **Output**: Report the list of gains $[G^{(1)}, G^{(2)}, \\dots, G^{(m)}]$ for each test case.\n\nFor matrices with low condition numbers (e.g., $n=2$), the initial solution is already highly accurate, and refinement yields little to no gain. As $n$ increases ($n=5, 8$), the condition number grows, the initial solution degrades, and iterative refinement is expected to provide significant accuracy gains in the first few iterations. For $n=12$, the condition number of the Hilbert matrix exceeds $10^{16}$, which is the approximate precision limit of double-precision numbers. At this point, the computed residual is dominated by noise, and the refinement process is expected to stagnate or fail, yielding minimal or even negative gains.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes iterative refinement for linear systems involving\n    the Hilbert matrix for a suite of test cases.\n    \"\"\"\n    # Test cases are defined as tuples (n, m), where n is the matrix size\n    # and m is the number of refinement iterations.\n    test_cases = [\n        (2, 5),   # Case 1: Well-conditioned\n        (5, 5),   # Case 2: Moderately ill-conditioned\n        (8, 5),   # Case 3: Challenging ill-conditioning\n        (12, 5),  # Case 4: Severe ill-conditioning\n    ]\n\n    all_results = []\n\n    for n, m in test_cases:\n        # Step 1: System Setup\n        # Construct the n x n Hilbert matrix A.\n        # For 0-based indices i, j, the formula is H_ij = 1 / (i + j + 1).\n        A = np.fromfunction(lambda i, j: 1.0 / (i + j + 1), (n, n), dtype=float)\n\n        # Define the true solution as the all-ones vector.\n        x_star = np.ones(n, dtype=float)\n\n        # Calculate the right-hand side vector b = A * x_star.\n        b = A @ x_star\n\n        # Define a helper function to calculate the number of correct digits.\n        def get_saturated_digits(x_approx, x_true):\n            \"\"\"\n            Calculates the saturated number of correct base-10 digits.\n            \"\"\"\n            # The infinity norm of x_star is always 1.0.\n            norm_x_true_inf = 1.0\n            \n            # Calculate relative error using the infinity norm.\n            relative_error = np.linalg.norm(x_approx - x_true, np.inf) / norm_x_true_inf\n            \n            # Apply saturation at 10^-16 to handle finite precision and avoid log(0).\n            effective_error = max(relative_error, 1e-16)\n            \n            return -np.log10(effective_error)\n\n        # Step 2: Initial Solution\n        # Compute the initial solution x^(0) using a direct solver.\n        x_k = np.linalg.solve(A, b)\n\n        # Calculate the initial number of correct digits, D_tilde^(0).\n        D_prev = get_saturated_digits(x_k, x_star)\n\n        # Step 3: Iterative Refinement\n        gains_for_case = []\n        for _ in range(m):\n            # a. Compute the residual in double precision.\n            r_k = b - A @ x_k\n\n            # b. Solve for the correction vector d.\n            d_k = np.linalg.solve(A, r_k)\n\n            # c. Update the solution.\n            x_k = x_k + d_k\n\n            # d. Calculate the new accuracy.\n            D_current = get_saturated_digits(x_k, x_star)\n\n            # e. Calculate and record the gain.\n            gain = D_current - D_prev\n            gains_for_case.append(gain)\n\n            # f. Update the previous accuracy for the next iteration.\n            D_prev = D_current\n        \n        all_results.append(gains_for_case)\n\n    # Final print statement in the exact required format.\n    # Format each sublist of gains into a comma-separated string \"[g1,g2,...]\".\n    formatted_sublists = [f\"[{','.join(map(str, sublist))}]\" for sublist in all_results]\n    # Join all formatted sublists into the final output string.\n    print(f\"[{','.join(formatted_sublists)}]\")\n\nsolve()\n```"
        }
    ]
}