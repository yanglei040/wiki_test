{
    "hands_on_practices": [
        {
            "introduction": "Understanding the stability of Gaussian elimination begins with the growth factor, $\\rho(A)$. This exercise provides direct, hands-on practice in calculating $\\rho(A)$ by performing Gaussian Elimination with Partial Pivoting (GEPP) step-by-step on a parametrized matrix. By working through the mechanics , you will see precisely how pivoting choices influence the magnitudes of intermediate matrix entries and why a growth factor of 1 signifies ideal stability.",
            "id": "3581067",
            "problem": "Consider Gaussian elimination with partial pivoting (GEPP) applied to the matrix $A(\\epsilon) \\in \\mathbb{R}^{3 \\times 3}$ defined by\n$$\nA(\\epsilon) \\;=\\; \\begin{bmatrix}\n1  1  1 \\\\\n1  \\epsilon  1 \\\\\n1  1  \\epsilon\n\\end{bmatrix},\n$$\nwhere $\\epsilon$ is a fixed scalar with $0\\epsilon1$. Use the following deterministic tie-breaking rule for partial pivoting: whenever two or more candidate pivot rows have equal maximal absolute value in the active column, select the one with the smallest row index. Define the growth factor for GEPP as\n$$\n\\rho(A) \\;=\\; \\frac{\\displaystyle \\max_{k \\in \\{0,1,2,3\\}} \\max_{i,j} \\big|a^{(k)}_{ij}\\big|}{\\displaystyle \\max_{i,j} \\big|a^{(0)}_{ij}\\big|},\n$$\nwhere $A^{(0)} = A(\\epsilon)$, and $A^{(k)}$ denotes the matrix of active trailing entries after $k$ elimination steps (so that $A^{(3)}$ is the upper triangular factor $U$). Starting from the core definitions of GEPP and the growth factor, carry out the elimination explicitly for $A(\\epsilon)$ under the stated pivoting rule, and derive a closed-form expression for $\\rho(A)$ as a function of $\\epsilon$. Then, based on your derivation, explain how the parameter $\\epsilon$ influences element growth under GEPP for this family of matrices. Provide your final answer for $\\rho(A)$ as a single closed-form analytic expression in $\\epsilon$. No rounding is required.",
            "solution": "The problem requires the calculation of the growth factor $\\rho(A)$ for Gaussian elimination with partial pivoting (GEPP) applied to a specific $3 \\times 3$ matrix $A(\\epsilon)$. The analysis will proceed by explicitly carrying out the elimination steps.\n\nThe given matrix is:\n$$\nA(\\epsilon) = A^{(0)} = \\begin{bmatrix}\n1  1  1 \\\\\n1  \\epsilon  1 \\\\\n1  1  \\epsilon\n\\end{bmatrix}\n$$\nwhere $\\epsilon$ is a scalar such that $0  \\epsilon  1$.\n\nThe growth factor is defined as:\n$$\n\\rho(A) = \\frac{\\displaystyle \\max_{k \\in \\{0,1,2,3\\}} \\max_{i,j} \\big|a^{(k)}_{ij}\\big|}{\\displaystyle \\max_{i,j} \\big|a^{(0)}_{ij}\\big|}\n$$\nHere, $A^{(0)}$ is the initial matrix. The term $\\max_{i,j} |a^{(k)}_{ij}|$ refers to the maximum absolute value of any entry in the full matrix present after step $k$ of the elimination. The matrix $A^{(k)}$ for $k \\in \\{1, 2\\}$ is the active trailing submatrix, but the numerator of the growth factor definition considers all elements generated up to and including the final upper triangular matrix $U = A^{(3)}$.\n\nFirst, we determine the denominator of the growth factor expression. This is the maximum absolute value of the entries in the initial matrix $A^{(0)}$.\n$$\n\\max_{i,j} \\big|a^{(0)}_{ij}\\big| = \\max \\left\\{ |1|, |\\epsilon| \\right\\}\n$$\nSince $0  \\epsilon  1$, the maximum absolute value is $1$.\n$$\n\\max_{i,j} \\big|a^{(0)}_{ij}\\big| = 1\n$$\nThe maximum element magnitude found so far, which contributes to the numerator, is also $1$.\n\nNow, we perform the steps of GEPP.\n\n**Step 1: Elimination in column 1**\n\nThe first column of $A^{(0)}$ is $\\begin{bmatrix} 1  1  1 \\end{bmatrix}^T$. The entry with the maximum absolute value is $1$, which appears in all three rows. The specified deterministic tie-breaking rule requires selecting the candidate row with the smallest row index. Thus, row $1$ is chosen as the pivot row. The pivot element is $a_{11} = 1$. No row permutation is necessary.\n\nThe multipliers for the elimination are:\n$$\nm_{21} = \\frac{a_{21}}{a_{11}} = \\frac{1}{1} = 1\n$$\n$$\nm_{31} = \\frac{a_{31}}{a_{11}} = \\frac{1}{1} = 1\n$$\n\nThe new rows are computed as $R_2 \\leftarrow R_2 - m_{21}R_1$ and $R_3 \\leftarrow R_3 - m_{31}R_1$:\nRow 2: $[1, \\epsilon, 1] - 1 \\cdot [1, 1, 1] = [0, \\epsilon - 1, 0]$\nRow 3: $[1, 1, \\epsilon] - 1 \\cdot [1, 1, 1] = [0, 0, \\epsilon - 1]$\n\nThe matrix after the first step of elimination, which we denote as $M^{(1)}$, is:\n$$\nM^{(1)} = \\begin{bmatrix}\n1  1  1 \\\\\n0  \\epsilon-1  0 \\\\\n0  0  \\epsilon-1\n\\end{bmatrix}\n$$\nThe active trailing submatrix is $A^{(1)}$:\n$$\nA^{(1)} = \\begin{bmatrix}\n\\epsilon-1  0 \\\\\n0  \\epsilon-1\n\\end{bmatrix}\n$$\nThe new elements generated are $\\epsilon-1$ and $0$. Since $0  \\epsilon  1$, we have $-1  \\epsilon-1  0$, so $|\\epsilon-1| = 1-\\epsilon$. As $1-\\epsilon  1$, the maximum absolute value of any element seen so far ($A^{(0)}$ and $M^{(1)}$) remains $1$.\n\n**Step 2: Elimination in column 2**\n\nThe elimination proceeds on the active submatrix $A^{(1)}$. The first column of $A^{(1)}$ is $\\begin{bmatrix} \\epsilon-1 \\\\ 0 \\end{bmatrix}$. The maximum absolute value is $|\\epsilon-1| = 1-\\epsilon$, which is in the first row of the submatrix (corresponding to row $2$ of the full matrix). This is our pivot element. No row swap is needed.\n\nThe pivot element is $a^{(1)}_{11} = \\epsilon-1$. The multiplier is:\n$$\nm_{32} = \\frac{a^{(1)}_{21}}{a^{(1)}_{11}} = \\frac{0}{\\epsilon-1} = 0\n$$\nSince the multiplier is $0$, the subsequent row update $R_3 \\leftarrow R_3 - m_{32}R_2$ results in no change to row $3$. The matrix after the second step, $M^{(2)}$, is identical to $M^{(1)}$:\n$$\nM^{(2)} = \\begin{bmatrix}\n1  1  1 \\\\\n0  \\epsilon-1  0 \\\\\n0  0  \\epsilon-1\n\\end{bmatrix}\n$$\nNo new element values are generated. The active $1 \\times 1$ submatrix is $A^{(2)} = [\\epsilon-1]$.\n\n**Step 3: Final Matrix**\n\nThe elimination process is complete. The final upper triangular matrix is $U = A^{(3)} = M^{(2)}$.\n$$\nU = A^{(3)} = \\begin{bmatrix}\n1  1  1 \\\\\n0  \\epsilon-1  0 \\\\\n0  0  \\epsilon-1\n\\end{bmatrix}\n$$\n\n**Calculation of the Growth Factor**\n\nThe numerator of $\\rho(A)$ is the maximum absolute value of any element encountered throughout all stages ($k=0, 1, 2, 3$). The set of all entries that appeared in the matrices $A^{(0)}, M^{(1)}, M^{(2)}, U$ is $\\{1, \\epsilon, \\epsilon-1, 0\\}$.\nThe absolute values of these elements are $\\{1, \\epsilon, 1-\\epsilon, 0\\}$.\nSince $0  \\epsilon  1$, both $\\epsilon$ and $1-\\epsilon$ are strictly between $0$ and $1$.\nTherefore, the maximum of these absolute values is $1$.\n$$\n\\max_{k \\in \\{0,1,2,3\\}} \\max_{i,j} \\big|a^{(k)}_{ij}\\big| = 1\n$$\nNow, we can compute the growth factor:\n$$\n\\rho(A) = \\frac{1}{1} = 1\n$$\n\n**Influence of the Parameter $\\epsilon$**\n\nThe derivation shows that the growth factor $\\rho(A)$ is equal to $1$ for any value of $\\epsilon$ in the range $0  \\epsilon  1$. A growth factor of $1$ signifies that no element growth occurred during the elimination process; the largest element in magnitude was present in the original matrix. For this specific family of matrices and the given GEPP procedure, the algorithm is perfectly stable. The parameter $\\epsilon$ influences the values of the intermediate and final matrix entries (e.g., $\\epsilon-1$), but due to the problem's structure and the nature of partial pivoting, these new entries never exceed the magnitude of the largest initial entry. Consequently, $\\epsilon$ has no influence on the element growth for this problem.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "A small backward error is the hallmark of a stable computation, but how we measure this error is critical. This practice problem  uses a carefully constructed ill-scaled matrix to demonstrate the potential pitfalls of relying solely on normwise backward error. By comparing it with the more revealing componentwise backward error, you will gain a deeper appreciation for how to properly diagnose the quality of a numerical solution, especially when dealing with problems involving different physical scales.",
            "id": "3581031",
            "problem": "Consider Gaussian elimination with partial pivoting used to compute an approximate solution to a linear system in exact arithmetic except for a final rounding that yields a nonzero residual. Let $M = 10^{8}$ and $\\eta = \\frac{1}{5}$. Define the matrix and right-hand side\n$$\nA = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} M \\\\ 1 - \\eta \\end{pmatrix}.\n$$\nSuppose the computed solution is\n$$\n\\hat{x} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nLet the residual be $r = b - A \\hat{x}$. Work with the infinity norm $\\|\\cdot\\|_{\\infty}$.\n\n1. Compute the normwise backward error in the infinity norm,\n$$\n\\mu_{\\infty} = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\,\\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}},\n$$\nand the componentwise backward error,\n$$\n\\omega_{\\infty} = \\left\\|\\,|r| \\oslash \\left(|A|\\,|\\hat{x}| + |b|\\right)\\,\\right\\|_{\\infty},\n$$\nwhere $|\\,\\cdot\\,|$ denotes entrywise absolute value and $\\oslash$ denotes entrywise division.\n\n2. Compute the pivot growth factor for partial pivoting on $A$,\n$$\n\\gamma_{\\infty}(A) = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $U$ is the upper triangular factor produced by Gaussian elimination with partial pivoting on $A$.\n\n3. Using the values of $\\mu_{\\infty}$ and $\\omega_{\\infty}$, compute the ratio\n$$\n\\rho = \\frac{\\omega_{\\infty}}{\\mu_{\\infty}}.\n$$\n\nExplain briefly why, in this example, the normwise backward error can be misleadingly small compared with the componentwise backward error, even though the pivot growth factor is minimal.\n\nRound your final numerical answer for $\\rho$ to four significant figures.",
            "solution": "**Part 1: Computation of $\\mu_{\\infty}$ and $\\omega_{\\infty}$**\n\nFirst, we compute the residual vector $r = b - A \\hat{x}$.\nThe product $A\\hat{x}$ is:\n$$\nA\\hat{x} = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} M \\\\ 1 \\end{pmatrix}\n$$\nThe residual is:\n$$\nr = b - A\\hat{x} = \\begin{pmatrix} M \\\\ 1 - \\eta \\end{pmatrix} - \\begin{pmatrix} M \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\eta \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1/5 \\end{pmatrix}\n$$\nNext, we compute the necessary infinity norms for $\\mu_{\\infty}$:\n$$ \\|r\\|_{\\infty} = 1/5 $$\n$$ \\|A\\|_{\\infty} = \\max(|M|+|0|, |0|+|1|) = M = 10^8 $$\n$$ \\|\\hat{x}\\|_{\\infty} = 1 $$\n$$ \\|b\\|_{\\infty} = \\max(|M|, |1-\\eta|) = M = 10^8 $$\nThe normwise backward error is:\n$$\n\\mu_{\\infty} = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\,\\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}} = \\frac{1/5}{M \\cdot 1 + M} = \\frac{1/5}{2M} = \\frac{1}{10M} = 10^{-9}\n$$\nFor the componentwise backward error $\\omega_{\\infty}$, we compute the vector $|r| \\oslash (|A|\\,|\\hat{x}| + |b|)$. The denominator vector is:\n$$\n|A|\\,|\\hat{x}| + |b| = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} M \\\\ 1-\\eta \\end{pmatrix} = \\begin{pmatrix} M \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} M \\\\ 4/5 \\end{pmatrix} = \\begin{pmatrix} 2M \\\\ 9/5 \\end{pmatrix}\n$$\nPerforming the elementwise division with $|r| = (0, 1/5)^T$:\n$$\n|r| \\oslash \\left(|A|\\,|\\hat{x}| + |b|\\right) = \\begin{pmatrix} 0 / (2M) \\\\ (1/5) / (9/5) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/9 \\end{pmatrix}\n$$\nThe componentwise backward error is the infinity norm of this vector:\n$$\n\\omega_{\\infty} = \\left\\| \\begin{pmatrix} 0 \\\\ 1/9 \\end{pmatrix} \\right\\|_{\\infty} = 1/9\n$$\n\n**Part 2: Computation of the pivot growth factor $\\gamma_{\\infty}(A)$**\n\nThe matrix $A$ is diagonal. In Gaussian elimination with partial pivoting, the pivot for the first column is $a_{11}=M$, which is already the largest element, so no row interchange is needed. Since $a_{21}=0$, no elimination operations are performed. The factorization is $A=LU$ where $L=I$ and $U=A$.\nThe pivot growth factor is:\n$$\n\\gamma_{\\infty}(A) = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|} = \\frac{\\max(|M|, |1|)}{\\max(|M|, |1|)} = \\frac{M}{M} = 1\n$$\n\n**Part 3: Computation of $\\rho$ and Explanation**\n\nThe ratio $\\rho$ is:\n$$\n\\rho = \\frac{\\omega_{\\infty}}{\\mu_{\\infty}} = \\frac{1/9}{10^{-9}} = \\frac{10^9}{9} \\approx 1.111 \\times 10^{8}\n$$\n**Explanation:**\nThe normwise backward error $\\mu_{\\infty} \\approx 10^{-9}$ is misleadingly small because its denominator is dominated by the largest matrix entry, $M=10^8$. This masks the error in the second, smaller-scale component of the system. The componentwise backward error $\\omega_{\\infty} = 1/9$ is not small, correctly revealing that the computed solution does not satisfy the second equation well in a relative sense. The minimal growth factor ($\\gamma_{\\infty}(A)=1$) confirms that this large discrepancy is due to the poor scaling of the problem data, not algorithmic instability. For ill-scaled problems, componentwise error is a more reliable measure of solution quality.",
            "answer": "$$\n\\boxed{1.111 \\times 10^8}\n$$"
        },
        {
            "introduction": "Having established the importance of componentwise backward error, this final exercise grounds the concept in a realistic numerical scenario. You are given a computed solution to a linear system, likely affected by floating-point rounding errors, and tasked with quantifying its quality . This practice is fundamental to a posteriori error analysis, allowing you to certify the trustworthiness of a solution produced by a numerical algorithm.",
            "id": "3581065",
            "problem": "Consider solving the upper triangular system $U x = y$ arising as the $U$-solve stage in a lowerâ€“upper (LU) factorization with partial pivoting. The upper triangular matrix $U \\in \\mathbb{R}^{4 \\times 4}$ has ones on the diagonal,\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1  2.5  -3.1  4.0 \\\\\n0  1  0.75  -2.2 \\\\\n0  0  1  1.6 \\\\\n0  0  0  1\n\\end{pmatrix},\n$$\nand the right-hand side is\n$$\ny \\;=\\; \\begin{pmatrix} 5.0 \\\\ -1.3 \\\\ 2.7 \\\\ -0.9 \\end{pmatrix}.\n$$\nBackward substitution is carried out in Institute of Electrical and Electronics Engineers (IEEE) binary64 (double precision) floating-point arithmetic with round-to-nearest, ties-to-even. The computed solution $\\hat{x}$ returned by the backward substitution is observed to be\n$$\n\\hat{x} \\;=\\; \\begin{pmatrix}\n37.39650000000001 \\\\\n-6.385000000000002 \\\\\n4.140000000000001 \\\\\n-0.8999999999999999\n\\end{pmatrix}.\n$$\nUsing the formal definition of componentwise backward error for linear system solves, determine the componentwise backward error of $\\hat{x}$ for this $U$-solve. Express your final answer as a single dimensionless scalar and round your answer to four significant figures.",
            "solution": "The formal definition of the componentwise backward error for a computed solution $\\hat{x}$ to the linear system $Ax=b$ is the smallest non-negative scalar $\\omega$ such that the solution $\\hat{x}$ is the exact solution to a perturbed system $(A + \\Delta A)\\hat{x} = b + \\Delta b$, where the perturbations are bounded componentwise by $|\\Delta A| \\le \\omega|A|$ and $|\\Delta b| \\le \\omega|b|$. The formula for this value is:\n$$ \\omega = \\max_i \\frac{|(b - A\\hat{x})_i|}{(|A||\\hat{x}| + |b|)_i} $$\nIn this problem, the system is given by $U x = y$, so the componentwise backward error $\\omega$ for the computed solution $\\hat{x}$ is:\n$$ \\omega = \\max_i \\frac{|(y - U\\hat{x})_i|}{(|U||\\hat{x}| + |y|)_i} $$\n\n**Step 1: Compute the residual $r = y - U\\hat{x}$**\nFirst, we calculate the product $z = U\\hat{x}$ using sufficient precision to avoid loss of significance.\n$$ z_4 = (U\\hat{x})_4 = 1 \\cdot \\hat{x}_4 = -0.8999999999999999 $$\n$$ z_3 = (U\\hat{x})_3 = 1 \\cdot \\hat{x}_3 + 1.6 \\cdot \\hat{x}_4 = 4.140000000000001 + 1.6(-0.8999999999999999) = 2.7000000000000013 $$\n$$ z_2 = (U\\hat{x})_2 = 1 \\cdot \\hat{x}_2 + 0.75 \\cdot \\hat{x}_3 - 2.2 \\cdot \\hat{x}_4 = -6.385000000000002 + 0.75(4.140000000000001) - 2.2(-0.8999999999999999) = -1.3000000000000018 $$\n$$ z_1 = (U\\hat{x})_1 = 1 \\cdot \\hat{x}_1 + 2.5 \\cdot \\hat{x}_2 - 3.1 \\cdot \\hat{x}_3 + 4.0 \\cdot \\hat{x}_4 = 37.39650000000001 + 2.5(-6.385000000000002) - 3.1(4.140000000000001) + 4.0(-0.8999999999999999) = 5.0 $$\nSo, the product is $U\\hat{x} = \\begin{pmatrix} 5.0 \\\\ -1.3000000000000018 \\\\ 2.7000000000000013 \\\\ -0.8999999999999999 \\end{pmatrix}$.\nThe residual vector $r = y - U\\hat{x}$ is:\n$$ r = \\begin{pmatrix} 5.0 \\\\ -1.3 \\\\ 2.7 \\\\ -0.9 \\end{pmatrix} - \\begin{pmatrix} 5.0 \\\\ -1.3000000000000018 \\\\ 2.7000000000000013 \\\\ -0.8999999999999999 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 1.7763568394002505 \\times 10^{-15} \\\\ -1.3322676295501878 \\times 10^{-15} \\\\ -1.1102230246251565 \\times 10^{-16} \\end{pmatrix} $$\nThe componentwise absolute value is $|r| = \\begin{pmatrix} 0.0 \\\\ 1.7763568394002505 \\times 10^{-15} \\\\ 1.3322676295501878 \\times 10^{-15} \\\\ 1.1102230246251565 \\times 10^{-16} \\end{pmatrix}$.\n\n**Step 2: Compute the denominator $d = |U||\\hat{x}| + |y|$**\nWe take the componentwise absolute values of $U$, $\\hat{x}$, and $y$:\n$$ |U| = \\begin{pmatrix} 1  2.5  3.1  4.0 \\\\ 0  1  0.75  2.2 \\\\ 0  0  1  1.6 \\\\ 0  0  0  1 \\end{pmatrix}, \\;\\; |\\hat{x}| = \\begin{pmatrix} 37.39650000000001 \\\\ 6.385000000000002 \\\\ 4.140000000000001 \\\\ 0.8999999999999999 \\end{pmatrix}, \\;\\; |y| = \\begin{pmatrix} 5.0 \\\\ 1.3 \\\\ 2.7 \\\\ 0.9 \\end{pmatrix} $$\nNext, we calculate the product $v = |U||\\hat{x}|$:\n$$ v_4 = 1 \\cdot |\\hat{x}_4| = 0.8999999999999999 $$\n$$ v_3 = 1 \\cdot |\\hat{x}_3| + 1.6 \\cdot |\\hat{x}_4| = 4.140000000000001 + 1.6(0.8999999999999999) = 5.580000000000001 $$\n$$ v_2 = 1 \\cdot |\\hat{x}_2| + 0.75 \\cdot |\\hat{x}_3| + 2.2 \\cdot |\\hat{x}_4| = 6.385000000000002 + 0.75(4.140000000000001) + 2.2(0.8999999999999999) = 11.470000000000002 $$\n$$ v_1 = 1 \\cdot |\\hat{x}_1| + 2.5 \\cdot |\\hat{x}_2| + 3.1 \\cdot |\\hat{x}_3| + 4.0 \\cdot |\\hat{x}_4| = 37.39650000000001 + 2.5(6.385000000000002) + 3.1(4.140000000000001) + 4.0(0.8999999999999999) = 69.79300000000002 $$\nSo, $|U||\\hat{x}| = \\begin{pmatrix} 69.79300000000002 \\\\ 11.470000000000002 \\\\ 5.580000000000001 \\\\ 0.8999999999999999 \\end{pmatrix}$.\nFinally, the denominator vector is $d = |U||\\hat{x}|+|y|$:\n$$ d = \\begin{pmatrix} 69.79300000000002 \\\\ 11.470000000000002 \\\\ 5.580000000000001 \\\\ 0.8999999999999999 \\end{pmatrix} + \\begin{pmatrix} 5.0 \\\\ 1.3 \\\\ 2.7 \\\\ 0.9 \\end{pmatrix} = \\begin{pmatrix} 74.79300000000002 \\\\ 12.770000000000003 \\\\ 8.280000000000001 \\\\ 1.7999999999999998 \\end{pmatrix} $$\n\n**Step 3: Compute the ratios and find the maximum**\nLet $\\rho_i = |r_i|/d_i$.\n$$ \\rho_1 = \\frac{0.0}{74.79300000000002} = 0.0 $$\n$$ \\rho_2 = \\frac{1.7763568394002505 \\times 10^{-15}}{12.770000000000003} \\approx 1.391039 \\times 10^{-16} $$\n$$ \\rho_3 = \\frac{1.3322676295501878 \\times 10^{-15}}{8.280000000000001} \\approx 1.608995 \\times 10^{-16} $$\n$$ \\rho_4 = \\frac{1.1102230246251565 \\times 10^{-16}}{1.7999999999999998} \\approx 6.167906 \\times 10^{-17} $$\nThe componentwise backward error $\\omega$ is the maximum of these values:\n$$ \\omega = \\max(0.0, 1.391039 \\times 10^{-16}, 1.608995 \\times 10^{-16}, 6.167906 \\times 10^{-17}) $$\nThe maximum value is $\\rho_3$, which is approximately $1.6089947216789706 \\times 10^{-16}$.\n\nRounding the final answer to four significant figures gives $1.609 \\times 10^{-16}$.",
            "answer": "$$\n\\boxed{1.609 \\times 10^{-16}}\n$$"
        }
    ]
}