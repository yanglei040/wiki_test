{
    "hands_on_practices": [
        {
            "introduction": "In theoretical linear algebra, the inverse of a matrix can be found using the adjugate formula. In practice, however, this method is almost never used. This exercise provides a hands-on exploration of why that is, pitting the workhorse of numerical linear algebra—LU decomposition with pivoting—against its theoretical counterpart . By implementing both algorithms and testing them on a variety of matrices, including the notoriously ill-conditioned Hilbert matrix, you will gain a concrete understanding of numerical stability and see firsthand why factorization-based methods are indispensable in scientific computing.",
            "id": "3275824",
            "problem": "You must write a complete, runnable program that, for a given set of square matrices of size $4 \\times 4$, computes the matrix inverse by two different methods and compares the numerical error of each method using a residual-based metric grounded in core definitions of linear algebra. The two methods are: (i) an algorithm that derives from the definition of Gaussian elimination with partial pivoting and the factorization of a nonsingular matrix into a lower-triangular matrix and an upper-triangular matrix, and (ii) a method that constructs the adjugate via cofactor expansion of minors and a scalar determinant computed by recursive expansion of minors. Your program must implement both methods explicitly, without calling a built-in inverse routine. The comparison must be based on the relative Frobenius-norm residual computed from first principles.\n\nFoundational base and definitions to use:\n- The matrix inverse of a nonsingular square matrix $A \\in \\mathbb{R}^{n \\times n}$ is the matrix $X \\in \\mathbb{R}^{n \\times n}$ such that $A X = I$, where $I$ is the identity matrix of size $n \\times n$. The existence of $X$ implies that $A$ is nonsingular.\n- Gaussian elimination with partial pivoting transforms a nonsingular matrix $A$ into a factorization of the permuted matrix $P A = L U$, where $P$ is a permutation matrix, $L$ is a unit lower-triangular matrix (diagonal entries equal to $1$), and $U$ is an upper-triangular matrix. Solving linear systems $A x = b$ uses forward and backward substitution applied to $L$ and $U$ once the permutation is accounted for.\n- The cofactor-based adjugate is formed from signed minors of $A$ computed by recursive expansion of minors (Laplace expansion) of the determinant. Do not use any built-in or closed-form inverse formula; instead, construct the adjugate from cofactors and compute the determinant by recursive minor expansion.\n\nError metric to compute:\n- For each method producing a candidate inverse $X$, define the relative inverse residual error\n$$\ne = \\frac{\\lVert I - A X \\rVert_F}{\\lVert I \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. This metric is derived directly from the inverse definition $A X = I$ and quantifies how close the computed $X$ is to satisfying the identity relation.\n\nAlgorithmic tasks your program must perform for each test matrix $A$:\n1. Compute $X_{\\mathrm{LU}}$ by:\n   - Constructing the factorization $P A = L U$ via Gaussian elimination with partial pivoting using row interchanges and elimination multipliers based solely on arithmetic operations.\n   - Solving $A X = I$ by applying the permutation to the right-hand side columns and then performing forward substitution on $L$ followed by backward substitution on $U$ for all columns of $I$ to obtain $X_{\\mathrm{LU}}$.\n2. Compute $X_{\\mathrm{cof}}$ by:\n   - Computing the determinant of $A$ by recursive minor expansion along a row.\n   - Constructing the cofactor matrix from signed minors, then transposing it to obtain the adjugate. Use this adjugate in conjunction with the scalar determinant to produce $X_{\\mathrm{cof}}$ without employing any built-in inverse function.\n3. For each method, compute $e_{\\mathrm{LU}}$ and $e_{\\mathrm{cof}}$ using the residual formula above.\n\nTest suite:\nEvaluate your program on the following set of five matrices, all in dimension $n = 4$:\n- Case $1$ (identity boundary case):\n$$\nA_1 = I_4 =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n- Case $2$ (well-conditioned integer-valued structure):\n$$\nA_2 =\n\\begin{bmatrix}\n4 & 2 & 0 & 1 \\\\\n0 & 1 & 3 & 2 \\\\\n1 & 0 & 2 & 0 \\\\\n2 & 3 & 1 & 0\n\\end{bmatrix}.\n$$\n- Case $3$ (classical ill-conditioned example, Hilbert matrix):\n$$\nA_3 = H_4, \\quad (A_3)_{i j} = \\frac{1}{i + j - 1}, \\quad i,j \\in \\{1,2,3,4\\}.\n$$\n- Case $4$ (nearly singular block-upper-triangular structure with small determinant):\nLet $\\varepsilon = 10^{-6}$. Define\n$$\nA_4 =\n\\begin{bmatrix}\n1 & 1 & 1 & 1 \\\\\n1 & 1+\\varepsilon & 1 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n- Case $5$ (Vandermonde with wide dynamic range):\nLet $x = [10^{-4}, 1, 2, 5]$. Define the Vandermonde matrix\n$$\nA_5 = V(x), \\quad (A_5)_{i j} = x_i^{j-1}, \\quad i,j \\in \\{1,2,3,4\\}.\n$$\n\nOutput specification:\n- For each test case $k \\in \\{1,2,3,4,5\\}$, compute $e_{\\mathrm{LU}}^{(k)}$ and $e_{\\mathrm{cof}}^{(k)}$ as floating-point numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order\n$$\n[e_{\\mathrm{LU}}^{(1)}, e_{\\mathrm{cof}}^{(1)}, e_{\\mathrm{LU}}^{(2)}, e_{\\mathrm{cof}}^{(2)}, e_{\\mathrm{LU}}^{(3)}, e_{\\mathrm{cof}}^{(3)}, e_{\\mathrm{LU}}^{(4)}, e_{\\mathrm{cof}}^{(4)}, e_{\\mathrm{LU}}^{(5)}, e_{\\mathrm{cof}}^{(5)}].\n$$\n- Each floating-point number must be printed in scientific notation with $6$ digits after the decimal point (for example, $1.234567\\mathrm{e}{-08}$), and there must be no spaces in the output string.\n\nNo physical units are involved. Angles are not involved. All outputs are unitless real numbers.\n\nYour implementation must be entirely self-contained, use only basic arithmetic operations and matrix operations you program yourself for the required steps (do not call any built-in inverse routine), and adhere to the specified printing format. The runtime environment is Python with Numerical Python ($\\mathrm{NumPy}$).",
            "solution": "The problem is evaluated as valid. It is a well-posed, scientifically grounded, and objective problem in numerical linear algebra. The task is to implement and compare two distinct algorithms for matrix inversion—one based on LU factorization and the other on the cofactor/adjugate method—and to quantify their numerical accuracy using a standard residual-based error metric. All definitions, constraints, and test cases are formally specified and mathematically consistent. The solution will proceed by first outlining the theoretical basis for each method and the error metric, and then providing a program that implements these concepts.\n\n### Method 1: Matrix Inversion via LU Factorization with Partial Pivoting\nThe first method computes the inverse $X$ of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ by solving the matrix equation $AX = I$, where $I$ is the identity matrix. The core of this method is the LU factorization of $A$ with partial pivoting. This factorization decomposes a permuted version of $A$ into the product of a unit lower-triangular matrix $L$ and an upper-triangular matrix $U$, such that $PA = LU$. Here, $P$ is a permutation matrix that records the row interchanges performed during Gaussian elimination to ensure numerical stability by avoiding division by small or zero pivots.\n\nThe matrix equation $AX = I$ is rearranged using this factorization:\n$$PAX = PI \\implies LUX = P$$\nThis single matrix equation corresponds to $n$ independent systems of linear equations, one for each column of $X$. Let $x_j$ be the $j$-th column of $X$ and $p_j$ be the $j$-th column of $P$. Then, for each $j \\in \\{1, \\dots, n\\}$, we solve:\n$$LU x_j = p_j$$\nThis is solved in a two-step process for each column $j$:\n1.  **Forward Substitution:** Solve $L y_j = p_j$ for the intermediate vector $y_j$. Since $L$ is unit lower-triangular (with $1$s on the diagonal), this is efficient:\n    $$y_{j,i} = p_{j,i} - \\sum_{k=1}^{i-1} L_{ik} y_{j,k} \\quad \\text{for } i = 1, \\dots, n$$\n2.  **Backward Substitution:** Solve $U x_j = y_j$ for the column vector $x_j$. Since $U$ is upper-triangular, this is also readily solved by back substitution:\n    $$x_{j,i} = \\frac{1}{U_{ii}} \\left( y_{j,i} - \\sum_{k=i+1}^{n} U_{ik} x_{j,k} \\right) \\quad \\text{for } i = n, \\dots, 1$$\nThe collection of the column vectors $x_j$ forms the inverse matrix $X_{\\mathrm{LU}} = [x_1, x_2, \\dots, x_n]$. This method is standard in numerical computing due to its computational efficiency (totaling approximately $\\frac{8}{3}n^3$ floating-point operations for a dense matrix of size $n$) and its good numerical stability when partial pivoting is used.\n\n### Method 2: Matrix Inversion via Adjugate Formula\nThe second method is based on the classical analytical formula for the inverse using the determinant and the adjugate matrix:\n$$A^{-1} = \\frac{1}{\\det(A)} \\mathrm{adj}(A)$$\nThis method requires the computation of two components: the scalar determinant of $A$, $\\det(A)$, and the adjugate of $A$, $\\mathrm{adj}(A)$. Both are computed via cofactor expansion as stipulated.\n\n1.  **Determinant Calculation:** The determinant is computed using Laplace's expansion along a row (e.g., the first row, $i=1$):\n    $$\\det(A) = \\sum_{j=1}^{n} (-1)^{1+j} A_{1j} \\det(M_{1j})$$\n    where $M_{1j}$ is the $(n-1) \\times (n-1)$ submatrix of $A$ (a minor) obtained by deleting row $1$ and column $j$. This definition is recursive; the determinant of each minor $M_{1j}$ is computed by the same expansion, until the base case of a $1 \\times 1$ matrix is reached, where $\\det([a]) = a$.\n\n2.  **Adjugate Matrix Construction:** The adjugate of $A$ is the transpose of its cofactor matrix $C$:\n    $$\\mathrm{adj}(A) = C^T$$\n    The elements of the cofactor matrix, $C_{ij}$, are the signed minors:\n    $$C_{ij} = (-1)^{i+j} \\det(M_{ij})$$\n    where $M_{ij}$ is the submatrix formed by deleting row $i$ and column $j$ from $A$. This requires computing $n^2$ determinants of size $(n-1) \\times (n-1)$.\nCombining these gives the inverse $X_{\\mathrm{cof}}$. This method is computationally very expensive, with a complexity of $O(n!)$ if implemented recursively, and is often numerically unstable. The large number of alternating additions and subtractions can lead to catastrophic cancellation errors in floating-point arithmetic. It is primarily of theoretical and pedagogical importance rather than practical use for larger matrices.\n\n### Error Metric\nTo compare the numerical accuracy of the two computed inverses, $X_{\\mathrm{LU}}$ and $X_{\\mathrm{cof}}$, we use the relative inverse residual error. For a computed inverse $X$, the error $e$ is defined as:\n$$e = \\frac{\\lVert I - A X \\rVert_F}{\\lVert I \\rVert_F}$$\nHere, $\\lVert \\cdot \\rVert_F$ is the Frobenius norm, defined for a matrix $B \\in \\mathbb{R}^{m \\times n}$ as $\\lVert B \\rVert_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |B_{ij}|^2}$. The term $I-AX$ is the residual matrix, which should ideally be the zero matrix if $X$ were the exact inverse. The norm of this residual measures the magnitude of the deviation from the true identity. Normalizing by the norm of the identity matrix, $\\lVert I \\rVert_F$, makes the error metric relative and independent of the matrix dimension scale. For the specified $n=4$ case, $\\lVert I_4 \\rVert_F = \\sqrt{1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{4} = 2$.\nThis metric directly and reliably measures how well the computed inverse $X$ satisfies the fundamental definition $A^{-1}A=I$. The ill-conditioned test cases ($A_3, A_4, A_5$) are expected to show a significant difference in this error metric between the LU-based method and the cofactor-based method, highlighting the superior numerical stability of the former.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lu_decomposition_pivoting(A_in):\n    \"\"\"\n    Performs LU decomposition with partial pivoting: PA = LU.\n    L is unit lower-triangular, U is upper-triangular.\n    \"\"\"\n    n = A_in.shape[0]\n    A = A_in.copy()\n    L = np.zeros((n, n), dtype=float)\n    P = np.identity(n, dtype=float)\n\n    for k in range(n):\n        # Find pivot (largest element in column k below diagonal)\n        pivot_row_index = k + np.argmax(np.abs(A[k:, k]))\n        \n        # Swap rows in A and P\n        if pivot_row_index != k:\n            A[[k, pivot_row_index]] = A[[pivot_row_index, k]]\n            P[[k, pivot_row_index]] = P[[pivot_row_index, k]]\n            # Swap previously computed parts of L\n            if k > 0:\n                L[[k, pivot_row_index], :k] = L[[pivot_row_index, k], :k]\n\n        # Check for singularity\n        if np.isclose(A[k, k], 0.0):\n            # This indicates singularity but we proceed to allow for large errors.\n            continue\n            \n        # Compute multipliers and update the matrix\n        for i in range(k + 1, n):\n            multiplier = A[i, k] / A[k, k]\n            L[i, k] = multiplier\n            A[i, k:] -= multiplier * A[k, k:]\n            \n    np.fill_diagonal(L, 1.0)\n    U = np.triu(A)\n    return P, L, U\n\ndef forward_substitution(L, b):\n    \"\"\"Solves Ly = b for y, where L is lower-triangular.\"\"\"\n    n = L.shape[0]\n    y = np.zeros(n, dtype=float)\n    for i in range(n):\n        y[i] = b[i] - np.dot(L[i, :i], y[:i])\n    return y\n\ndef backward_substitution(U, y):\n    \"\"\"Solves Ux = y for x, where U is upper-triangular.\"\"\"\n    n = U.shape[0]\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        if np.isclose(U[i, i], 0.0):\n            x[i] = np.inf\n        else:\n            x[i] = (y[i] - np.dot(U[i, i + 1:], x[i + 1:])) / U[i, i]\n    return x\n\ndef inverse_lu(A):\n    \"\"\"Computes matrix inverse using LU decomposition.\"\"\"\n    n = A.shape[0]\n    P, L, U = lu_decomposition_pivoting(A)\n    I = np.identity(n)\n    X_LU = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        b_permuted = P @ I[:, j]\n        y = forward_substitution(L, b_permuted)\n        x_col = backward_substitution(U, y)\n        X_LU[:, j] = x_col\n        \n    return X_LU\n\ndef get_minor_matrix(A, i, j):\n    \"\"\"Returns the minor matrix by deleting row i and column j.\"\"\"\n    return np.delete(np.delete(A, i, axis=0), j, axis=1)\n\ndef determinant_recursive(A):\n    \"\"\"Computes determinant by recursive cofactor expansion.\"\"\"\n    n = A.shape[0]\n    if n == 1:\n        return A[0, 0]\n    if n == 2:\n        return A[0, 0] * A[1, 1] - A[0, 1] * A[1, 0]\n    \n    det = 0.0\n    for j in range(n):\n        sign = (-1)**j\n        minor_matrix = get_minor_matrix(A, 0, j)\n        det += sign * A[0, j] * determinant_recursive(minor_matrix)\n    return det\n\ndef inverse_cofactor(A):\n    \"\"\"Computes matrix inverse using the adjugate method.\"\"\"\n    n = A.shape[0]\n    det_A = determinant_recursive(A)\n    \n    if np.isclose(det_A, 0.0):\n        return np.full((n, n), np.nan)\n\n    cofactor_matrix = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(n):\n            sign = (-1)**(i + j)\n            minor_matrix = get_minor_matrix(A, i, j)\n            cofactor_matrix[i, j] = sign * determinant_recursive(minor_matrix)\n    \n    adjugate_matrix = cofactor_matrix.T\n    return adjugate_matrix / det_A\n\ndef calculate_error(A, X):\n    \"\"\"Computes the relative Frobenius norm residual error.\"\"\"\n    n = A.shape[0]\n    if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n        return np.inf\n\n    I = np.identity(n)\n    residual_matrix = I - A @ X\n    norm_residual = np.linalg.norm(residual_matrix, 'fro')\n    norm_identity = np.sqrt(n)\n    \n    return norm_residual / norm_identity\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1 = np.identity(4, dtype=float)\n    \n    A2 = np.array([\n        [4., 2., 0., 1.],\n        [0., 1., 3., 2.],\n        [1., 0., 2., 0.],\n        [2., 3., 1., 0.]\n    ], dtype=float)\n    \n    A3 = np.zeros((4, 4), dtype=float)\n    for i in range(4):\n        for j in range(4):\n            A3[i, j] = 1.0 / (i + j + 1.0)\n            \n    epsilon = 1e-6\n    A4 = np.array([\n        [1., 1., 1., 1.],\n        [1., 1. + epsilon, 1., 1.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]\n    ], dtype=float)\n    \n    x_vals = np.array([1e-4, 1.0, 2.0, 5.0], dtype=float)\n    A5 = np.vander(x_vals, 4, increasing=True)\n\n    test_cases = [A1, A2, A3, A4, A5]\n    results = []\n\n    for A in test_cases:\n        # Method 1: LU Inversion\n        X_lu = inverse_lu(A)\n        e_lu = calculate_error(A, X_lu)\n        results.append(e_lu)\n        \n        # Method 2: Cofactor Inversion\n        X_cof = inverse_cofactor(A)\n        e_cof = calculate_error(A, X_cof)\n        results.append(e_cof)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many problems in science and engineering involve matrices with special structures, such as M-matrices, which guarantee properties like non-negative solutions. This practice explores the crucial interplay between a numerical algorithm and the preservation of such structures. You will investigate whether LU factorization, performed with and without pivoting, maintains the defining properties of M-matrices and their factors . This exercise moves beyond general-purpose stability to highlight a key consideration for advanced applications: an algorithm must not only be accurate but should also respect the theoretical structure of the problem.",
            "id": "3539186",
            "problem": "Design and implement a complete program that, for a family of matrices with a focus on nonsingular monotone matrices known as $M$-matrices, computes the matrix inverse using a factorization-based method and empirically verifies the expected sign patterns of the inverse and factors. Your program must follow these requirements.\n\nFundamental base for reasoning:\n- Use the definition that an $M$-matrix is a real square matrix that is a $Z$-matrix (all off-diagonal entries nonpositive) and is nonsingular with a nonnegative inverse, or equivalently is of the form $A = s I - B$ where $B \\ge 0$ (entrywise) and $s \\ge \\rho(B)$, where $\\rho(B)$ denotes the spectral radius of $B$. Use only these properties as the starting point for your derivations.\n- Use the standard definition of lower–upper factorization: given a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$, a factorization with no pivoting seeks $A = L U$ where $L$ is unit lower triangular and $U$ is upper triangular. With partial pivoting, one seeks $P A = L U$ where $P$ is a permutation matrix.\n\nTasks to perform in the program:\n1. Implement two algorithms based solely on the above foundational definitions:\n   - Algorithm $1$: Compute $A^{-1}$ using an $L U$ factorization without any pivoting. Use forward and backward substitution to solve $A X = I$ column by column, where $I$ is the identity matrix and $X = A^{-1}$.\n   - Algorithm $2$: Compute $A^{-1}$ using an $L U$ factorization with partial (row) pivoting, hence $P A = L U$. Use forward and backward substitution to solve $A X = I$ by first applying the row permutations to the right-hand side and then solving triangular systems.\n\n2. For each algorithm, empirically verify the following properties:\n   - Nonnegativity of the inverse for an $M$-matrix: check that all entries of the computed $\\widehat{A^{-1}}$ are numerically nonnegative within a tolerance.\n   - Preservation of $Z$-matrix structure in the computed upper-triangular factor without pivoting: check that the strictly upper-triangular entries of the computed $\\widehat{U}$ satisfy the sign pattern consistent with a $Z$-matrix, i.e., are numerically nonpositive.\n   - Effect of pivoting on structure: check if partial pivoting performed any row swaps, and empirically assess whether the strictly upper-triangular part of the computed $\\widehat{U}$ retains nonpositivity.\n\n3. Robustness and tolerances:\n   - Let $\\epsilon$ denote machine epsilon for double-precision floating point.\n   - For each matrix $A$, define the verification tolerance as $\\tau = 100 \\cdot \\kappa_1(A) \\cdot \\epsilon \\cdot n$, where $\\kappa_1(A)$ is the condition number of $A$ in the $1$-norm and $n$ is the dimension.\n   - In the no-pivoting factorization, declare a zero pivot breakdown if $|u_{k k}| \\le 10 \\epsilon \\|A\\|_{\\infty}$ at any step $k$. Report whether breakdown occurred.\n   - All sign checks must be conducted with respect to the tolerance $\\tau$, meaning that an entry $x$ is treated as nonnegative if $x \\ge -\\tau$, and an entry is treated as nonpositive if $x \\le \\tau$.\n\nTest suite to cover general, boundary, and edge scenarios:\n- Case $1$ (structured strictly diagonally dominant $M$-matrix): let $n = 5$ and define $A \\in \\mathbb{R}^{n \\times n}$ by $a_{i i} = 2$ for all $i$, $a_{i,i+1} = -1$ for $i = 1,\\dots,n-1$, $a_{i+1,i} = -1$ for $i = 1,\\dots,n-1$, and all other entries $0$.\n- Case $2$ (random dense $M$-matrix with comfortable dominance): let $n = 6$. Generate $B \\in \\mathbb{R}^{n \\times n}$ with a fixed seed equal to the integer $7$, with independent entries uniformly distributed on $[0, 0.2]$, and then set $b_{i i} = 0$ for all $i$. Set $s = 1.5$ and define $A = s I - B$.\n- Case $3$ (nearly singular $M$-matrix): let $n = 7$. Generate $B$ with a fixed seed equal to the integer $11$, with independent entries uniformly distributed on $[0, 0.3]$, and then set $b_{i i} = 0$ for all $i$. Let $r_i = \\sum_{j \\ne i} b_{i j}$ be the off-diagonal row sums and set $s = \\max_i r_i + 10^{-8}$. Define $A = s I - B$.\n- Case $4$ (non-$M$-matrix to probe failure of nonnegativity): let $n = 5$. Define $A = I + \\alpha N$ where $\\alpha = 1/2$, $I$ is the identity, and $N$ is the nilpotent strictly upper bidiagonal matrix with $n-1$ ones on the first superdiagonal and zeros elsewhere.\n\nRequired outputs per case:\n- For each case, produce a list with six entries in the following order:\n  $1$. A boolean indicating whether no-pivot $L U$ factorization completed without zero pivot breakdown.\n  $2$. A boolean indicating whether partial pivoting performed at least one row swap.\n  $3$. A boolean indicating whether the strictly upper-triangular part of $\\widehat{U}$ from no-pivot $L U$ is numerically nonpositive under tolerance $\\tau$.\n  $4$. A boolean indicating whether the strictly upper-triangular part of $\\widehat{U}$ from partial-pivot $L U$ is numerically nonpositive under tolerance $\\tau$.\n  $5$. A boolean indicating whether all entries of the inverse computed by no-pivot $L U$ are numerically nonnegative under tolerance $\\tau$.\n  $6$. A boolean indicating whether all entries of the inverse computed by partial-pivot $L U$ are numerically nonnegative under tolerance $\\tau$.\n\nFinal output format:\n- The program must produce a single line containing a list of four inner lists, one per case, in order of cases $1$ through $4$. The line must be of the form $[\\text{case1}, \\text{case2}, \\text{case3}, \\text{case4}]$, where each $\\text{casek}$ is its six-entry list as specified above with boolean values, for example $[[\\text{True},\\text{False},\\dots],[\\dots],\\dots]$.\n- There are no physical units involved in this task.",
            "solution": "The problem requires the implementation and analysis of two algorithms for computing the matrix inverse, $A^{-1}$, based on Lower-Upper ($LU$) factorization. The analysis focuses on a special class of matrices known as $M$-matrices and verifies their characteristic properties, such as the nonnegativity of the inverse and the sign patterns of the $LU$ factors.\n\n### Theoretical Foundation\n\nAn $n \\times n$ real matrix $A$ is defined as a $Z$-matrix if all its off-diagonal entries are nonpositive, i.e., $a_{ij} \\le 0$ for all $i \\ne j$. An $M$-matrix is a nonsingular $Z$-matrix whose inverse is entrywise nonnegative ($A^{-1} \\ge 0$). An equivalent characterization, which is useful for constructing test cases, is that a matrix $A$ is an $M$-matrix if it can be expressed as $A = sI - B$, where $B$ is a matrix with nonnegative entries ($B \\ge 0$), $I$ is the identity matrix, and $s$ is a scalar greater than or equal to the spectral radius of $B$, $\\rho(B)$, with the nonsingularity condition holding for $s = \\rho(B)$.\n\nA key theoretical result in numerical linear algebra states that if $A$ is a nonsingular $M$-matrix, then its $LU$ factorization without pivoting exists, is numerically stable, and the resulting factors $L$ and $U$ are also nonsingular $M$-matrices. Since $L$ is, by definition, unit lower triangular, its diagonal entries are $1$. For $L$ to be an $M$-matrix, its off-diagonal entries must be nonpositive. Similarly, for the upper triangular factor $U$ to be an $M$-matrix, its diagonal entries must be positive ($u_{kk} > 0$) and its off-diagonal entries must be nonpositive ($u_{ij} \\le 0$ for $i < j$). These properties form the basis for the verification tasks.\n\n### Algorithmic Design and Inversion\n\nThe core task is to solve the matrix equation $AX = I$, where $X$ is the desired inverse $A^{-1}$ and $I$ is the identity matrix. This is solved column by column. For each column $j$ of the identity matrix, $e_j$, we solve the linear system $Ax_j = e_j$ to find the corresponding column $x_j$ of $A^{-1}$. The LU factorization method decouples this system into two simpler triangular systems.\n\n**Algorithm 1: Inversion via $LU$ Factorization without Pivoting**\n\nThis algorithm first decomposes the matrix $A$ into a product of a unit lower triangular matrix $L$ and an upper triangular matrix $U$, such that $A=LU$. This factorization is performed using the Gaussian elimination process. At each step $k$ of the elimination, the pivot element $u_{kk}$ is used to zero out the entries below it in the $k$-th column. The multipliers used are stored in the corresponding entries of $L$. For a general matrix, this process can fail or become numerically unstable if a pivot $u_{kk}$ is zero or very small. The problem specifies a breakdown condition: if $|u_{kk}| \\le 10 \\epsilon \\|A\\|_{\\infty}$ at any step $k$, where $\\epsilon$ is machine epsilon and $\\|A\\|_{\\infty}$ is the infinity norm of $A$, the factorization is considered to have failed. For nonsingular $M$-matrices, theory guarantees that all pivots $u_{kk}$ will be positive, so breakdown should not occur.\n\nOnce the factorization $A=LU$ is obtained, each system $Ax_j=e_j$ becomes $LUx_j=e_j$. This is solved in two steps:\n1.  **Forward Substitution:** Solve $Ly_j = e_j$ for the intermediate vector $y_j$.\n2.  **Backward Substitution:** Solve $Ux_j = y_j$ for the solution vector $x_j$.\n\nThe collection of vectors $\\{x_j\\}_{j=1}^n$ forms the columns of $A^{-1}$.\n\n**Algorithm 2: Inversion via $LU$ Factorization with Partial Pivoting**\n\nTo enhance numerical stability for general matrices, partial pivoting (row swapping) is employed. The algorithm seeks a factorization $PA=LU$, where $P$ is a permutation matrix that represents the row interchanges performed during Gaussian elimination to ensure that the largest-magnitude element in the current column is used as the pivot.\n\nThe system $Ax_j=e_j$ is rewritten using the factorization as $P^{-1}LUx_j = e_j$, which rearranges to $LUx_j = Pe_j$. Note that $Pe_j$ is simply the $j$-th column of the permutation matrix $P$, which corresponds to a reordering of the basis vector $e_j$. The solution process is analogous to the no-pivot case:\n1.  **Forward Substitution:** Solve $Ly_j = Pe_j$ for $y_j$.\n2.  **Backward Substitution:** Solve $Ux_j = y_j$ for $x_j$.\n\nWhile pivoting is crucial for stability with general matrices, it can destroy the special structure of matrices like $M$-matrices. Specifically, the resulting factor $U$ may no longer be a $Z$-matrix (i.e., its strictly upper-triangular entries may not all be nonpositive), even if the original matrix $A$ was an $M$-matrix for which no-pivot LU would have been stable.\n\n### Verification and Tolerances\n\nNumerical computations with floating-point numbers are subject to rounding errors. To perform checks for nonnegativity ($x \\ge 0$) or nonpositivity ($x \\le 0$) robustly, a numerical tolerance $\\tau$ is essential. The problem defines a tolerance that scales with the matrix dimension $n$, machine precision $\\epsilon$, and the matrix condition number in the $1$-norm, $\\kappa_1(A)$:\n$$ \\tau = 100 \\cdot \\kappa_1(A) \\cdot \\epsilon \\cdot n $$\nThis tolerance accounts for the fact that error amplification in solving linear systems is proportional to the condition number. An entry $x$ is considered numerically nonnegative if $x \\ge -\\tau$ and numerically nonpositive if $x \\le \\tau$. The required checks empirically test the theoretical properties of $M$-matrices against the results of these two algorithms, observing how numerical realities and algorithmic choices (pivoting) interact with the theory.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu, solve_triangular\n\ndef analyze_matrix(A: np.ndarray) -> list:\n    \"\"\"\n    Performs LU-based inversion and property verification for a given matrix A.\n    \n    Returns a list of 6 boolean values corresponding to the problem checks.\n    \"\"\"\n    n = A.shape[0]\n    eps = np.finfo(float).eps\n    \n    # Calculate norms and condition number needed for tolerances.\n    # A nearly singular matrix can have an 'inf' condition number.\n    # We cap it to a large number to avoid making tau infinite.\n    cond_A_1 = np.linalg.cond(A, p=1)\n    if np.isinf(cond_A_1) or np.isnan(cond_A_1):\n        cond_A_1 = 1.0 / eps\n\n    tau = 100.0 * cond_A_1 * eps * float(n)\n    norm_A_inf = np.linalg.norm(A, ord=np.inf)\n\n    # --- Algorithm 1: No Pivoting ---\n    \n    # Custom LU implementation to include the pivot breakdown check.\n    U_no_pivot = A.copy().astype(np.float64)\n    L_no_pivot = np.identity(n, dtype=np.float64)\n    no_pivot_completed = True\n    \n    for k in range(n - 1):\n        # Pivot breakdown check\n        if abs(U_no_pivot[k, k]) = 10.0 * eps * norm_A_inf:\n            no_pivot_completed = False\n            break\n        for i in range(k + 1, n):\n            factor = U_no_pivot[i, k] / U_no_pivot[k, k]\n            L_no_pivot[i, k] = factor\n            U_no_pivot[i, k:] -= factor * U_no_pivot[k, k:]\n\n    # Final pivot check for the last element on the diagonal of U\n    if no_pivot_completed and abs(U_no_pivot[n - 1, n - 1]) = 10.0 * eps * norm_A_inf:\n        no_pivot_completed = False\n\n    # Check 1: No-pivot LU completed without zero pivot breakdown.\n    res1 = no_pivot_completed\n\n    res3 = False # Check 3: Is upper-triangular U from no-pivot a Z-matrix?\n    res5 = False # Check 5: Is inverse from no-pivot non-negative?\n\n    if res1:\n        U_strict_upper = np.triu(U_no_pivot, k=1)\n        res3 = np.all(U_strict_upper = tau)\n        \n        # Compute inverse using LU factors\n        I = np.identity(n, dtype=np.float64)\n        A_inv_no_pivot = np.zeros_like(A, dtype=np.float64)\n        for j in range(n):\n            y = solve_triangular(L_no_pivot, I[:, j], lower=True, unit_diagonal=True)\n            A_inv_no_pivot[:, j] = solve_triangular(U_no_pivot, y, lower=False)\n        \n        res5 = np.all(A_inv_no_pivot >= -tau)\n\n    # --- Algorithm 2: Partial Pivoting ---\n    \n    try:\n        P_mat, L_pivot, U_pivot = lu(A)\n        pivoting_successful = True\n    except (np.linalg.LinAlgError, ValueError):\n        # LU may fail for singular matrix, handle gracefully\n        pivoting_successful = False\n\n    res2 = False # Check 2: Partial pivoting performed at least one row swap.\n    res4 = False # Check 4: Is upper-triangular U from partial-pivot a Z-matrix?\n    res6 = False # Check 6: Is inverse from partial-pivot non-negative?\n\n    if pivoting_successful:\n        res2 = not np.array_equal(P_mat, np.identity(n))\n        \n        U_pivot_strict_upper = np.triu(U_pivot, k=1)\n        res4 = np.all(U_pivot_strict_upper = tau)\n        \n        # Compute inverse using PLU factors\n        # The equation is PA = LU => A = P.T @ L @ U\n        # To solve AX=I, it's P.T @ L @ U @ X = I => L @ U @ X = P @ I\n        PI = P_mat @ np.identity(n)\n        A_inv_pivot = np.zeros_like(A, dtype=np.float64)\n        for j in range(n):\n            y = solve_triangular(L_pivot, PI[:, j], lower=True, unit_diagonal=True)\n            A_inv_pivot[:, j] = solve_triangular(U_pivot, y, lower=False)\n        \n        res6 = np.all(A_inv_pivot >= -tau)\n\n    return [res1, res2, res3, res4, res5, res6]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = []\n\n    # Case 1: Structured strictly diagonally dominant M-matrix\n    n1 = 5\n    A1 = np.diag([2.0] * n1) + np.diag([-1.0] * (n1 - 1), k=1) + np.diag([-1.0] * (n1 - 1), k=-1)\n    test_cases.append(A1)\n\n    # Case 2: Random dense M-matrix with comfortable dominance\n    n2 = 6\n    rng2 = np.random.default_rng(7)\n    B2 = rng2.uniform(0, 0.2, size=(n2, n2))\n    np.fill_diagonal(B2, 0)\n    s2 = 1.5\n    A2 = s2 * np.identity(n2) - B2\n    test_cases.append(A2)\n\n    # Case 3: Nearly singular M-matrix\n    n3 = 7\n    rng3 = np.random.default_rng(11)\n    B3 = rng3.uniform(0, 0.3, size=(n3, n3))\n    np.fill_diagonal(B3, 0)\n    r3 = np.sum(B3, axis=1) # off-diagonal row sums\n    s3 = np.max(r3) + 1e-8\n    A3 = s3 * np.identity(n3) - B3\n    test_cases.append(A3)\n\n    # Case 4: Non-M-matrix to probe failure of nonnegativity\n    n4 = 5\n    I4 = np.identity(n4)\n    N4 = np.diag([1.0] * (n4 - 1), k=1)\n    alpha4 = 0.5\n    A4 = I4 + alpha4 * N4\n    test_cases.append(A4)\n\n    results = []\n    for A in test_cases:\n        case_results = analyze_matrix(A)\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "It is well-known that ill-conditioned matrices amplify numerical errors, but how exactly is this error distributed within the solution? This final practice offers a sophisticated framework to answer that question by linking numerical error directly to the matrix's spectrum. By constructing matrices with a known spectral decomposition, $A = Q^T D Q$, you can compute the inverse numerically and analyze the error in the matrix's own eigenbasis . This investigation will help you discover the systematic relationship between eigenvalue magnitudes and error distribution, providing a deeper, more nuanced understanding of conditioning.",
            "id": "3539201",
            "problem": "You are asked to design and implement a complete, runnable program that benchmarks the construction of a matrix inverse via Lower–Upper (LU) factorization for a family of structured matrices and quantitatively explains how the observed numerical error aligns with the spectrum of the matrix. Work entirely in purely mathematical terms and produce numerical outputs that are deterministic and reproducible across executions.\n\nStart from the following foundational base:\n- The LU factorization with partial pivoting of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined by a permutation matrix $P$, a unit lower triangular matrix $L$, and an upper triangular matrix $U$ such that $P A = L U$. Solving $A x = b$ can be achieved stably by forward and back substitution with $(L, U)$ after applying $P$.\n- The inverse matrix $A^{-1}$ is the linear operator that satisfies $A A^{-1} = I$ and $A^{-1} A = I$, where $I$ is the identity matrix. The columns of $A^{-1}$ can be obtained as the unique solutions to the linear systems $A x_j = e_j$, where $e_j$ is the $j$-th canonical basis vector.\n- An orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ satisfies $Q^\\top Q = I$. A diagonal matrix $D = \\mathrm{diag}(d_1,\\dots,d_n)$ has entries $d_i \\neq 0$ on the diagonal and $0$ elsewhere.\n\nProblem setting:\n- For each test case, consider a matrix $A \\in \\mathbb{R}^{n \\times n}$ constructed as $A = Q^\\top D Q$, where $Q$ is orthogonal and $D$ is diagonal with nonzero entries. The inverse $A^{-1}$ is known in closed form from first principles and must be derived using only the properties of orthogonal matrices and diagonal matrices stated above. Do not use any pre-derived formula; reason from the base definitions to obtain the analytical expression.\n- Construct a numerical approximation of $A^{-1}$ by computing an LU factorization of $A$ and solving $A X = I$ for $X$ using triangular solves. This $X$ is your LU-based approximation to $A^{-1}$.\n- Define the global relative error in the Frobenius norm as\n$$\n\\mathrm{frel} = \\frac{\\lVert X - A^{-1}\\rVert_F}{\\lVert A^{-1}\\rVert_F}.\n$$\n- To map error patterns to the spectrum encoded in $D$, transform the error to the eigenbasis of $A$ by computing\n$$\nE = X - A^{-1}, \\quad \\widetilde{E} = Q\\, E\\, Q^\\top,\n$$\nso that $\\widetilde{E}$ is expressed in the basis in which $A$ is diagonal. For each eigendirection $i \\in \\{1,\\dots,n\\}$, define the per-direction relative diagonal error\n$$\n\\epsilon_i = \\frac{\\left| \\widetilde{E}_{ii} \\right|}{\\left| (A^{-1})_{ii} \\text{ in the eigenbasis} \\right|}.\n$$\nAggregate the per-direction trend by computing the Pearson correlation coefficient between $\\log_{10}(|d_i|)$ and $\\log_{10}(\\epsilon_i)$, using a small additive regularizer $\\delta$ inside the logarithm applied to $\\epsilon_i$ to avoid taking $\\log_{10}(0)$. Use $\\delta = 10^{-300}$. If the variance of $\\log_{10}(|d_i|)$ is zero (for example, when all $|d_i|$ are equal), define the correlation to be $0.0$ by convention.\n- Quantify the spectral conditioning by the $2$-norm condition number\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},\n$$\nand for symmetric $A$ this equals $\\max_i |d_i| / \\min_i |d_i|$.\n\nTest suite specification:\nFor all test cases, use dimension $n = 12$. For each case, generate $Q$ by the following procedure to ensure reproducibility and a well-defined distribution: draw a matrix $Z \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries using the specified seed $s$ for the pseudorandom number generator; compute a thin QR factorization $Z = Q R$; let $S = \\mathrm{diag}(\\mathrm{sign}(R_{11}), \\dots, \\mathrm{sign}(R_{nn}))$ with $\\mathrm{sign}(0)$ defined as $1$; set $Q \\leftarrow Q S$. This yields an orthogonal $Q$. Then form $A = Q^\\top D Q$ with the specified diagonal $D$.\n\nProvide the following four cases, covering a happy path, ill-conditioning, indefiniteness, and clustering:\n- Case $1$ (happy path, well-conditioned): seed $s = 0$, $D = \\mathrm{diag}(1,1,\\dots,1)$.\n- Case $2$ (symmetric positive definite, highly ill-conditioned): seed $s = 1$, $D = \\mathrm{diag}(10^{-6}, 10^{-6 + \\Delta}, \\dots, 10^{6})$ with $n$ logarithmically spaced magnitudes from $10^{-6}$ to $10^{6}$.\n- Case $3$ (symmetric indefinite, moderately ill-conditioned with alternating signs): seed $s = 2$, $D = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_n)$ where $|\\sigma_i|$ are logarithmically spaced from $10^{-3}$ to $10^{3}$ and $\\mathrm{sign}(\\sigma_i) = (-1)^{i}$.\n- Case $4$ (clustered spectrum with tiny and moderate scales): seed $s = 3$, $D$ has ten diagonal entries equal to $10^{-6}$ and two entries equal to $1$, with their order randomly permuted using the same seed $s$.\n\nRequired outputs per test case:\n- Compute and return a list of three real numbers in the order $[\\mathrm{frel}, \\mathrm{corr}, \\kappa_2(A)]$, where $\\mathrm{corr}$ is the Pearson correlation coefficient described above.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each element is itself a three-element list in the specified order (for example, $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$). No other text should be printed.\n\nAll numerical quantities must be computed in floating-point arithmetic and reported as bare real numbers with no attached units. Angles, if any appear, must be understood in radians, but no angles are used in this task.",
            "solution": "The problem requires us to analyze the numerical error of computing a matrix inverse using LU factorization for a specific class of matrices, $A = Q^\\top D Q$, and to relate this error to the matrix's spectrum. We must first validate the problem's premises and then, if valid, provide a complete analytical and numerical solution. The problem is well-posed, scientifically grounded in numerical linear algebra, and provides all necessary information for a deterministic and reproducible solution. We may therefore proceed.\n\nThe solution process involves three main stages:\n1.  Deriving the analytical form of the inverse, $A^{-1}$, from first principles. This serves as the ground truth for our error calculations.\n2.  Specifying the numerical algorithm to compute an approximation of the inverse, which we denote as $X$, using LU factorization.\n3.  Defining and computing the specified error metrics ($\\mathrm{frel}$, $\\mathrm{corr}$) and the condition number ($\\kappa_2(A)$).\n\n**1. Analytical Derivation of the Inverse Matrix**\n\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by $A = Q^\\top D Q$, where $Q$ is an orthogonal matrix satisfying $Q^\\top Q = Q Q^\\top = I$, and $D = \\mathrm{diag}(d_1, \\dots, d_n)$ is a diagonal matrix with non-zero diagonal entries $d_i \\neq 0$. The inverse matrix, $A^{-1}$, is defined by the property $A A^{-1} = I$.\n\nTo find the expression for $A^{-1}$, we start with the definition:\n$$\n(Q^\\top D Q) A^{-1} = I\n$$\nWe seek to isolate $A^{-1}$. We can pre-multiply by $Q$ on both sides:\n$$\nQ (Q^\\top D Q) A^{-1} = Q I\n$$\nUsing the associative property of matrix multiplication, we group the terms:\n$$\n(Q Q^\\top) (D Q) A^{-1} = Q\n$$\nSince $Q$ is orthogonal, $Q Q^\\top = I$. The equation simplifies to:\n$$\nI (D Q) A^{-1} = Q \\implies (D Q) A^{-1} = Q\n$$\nNext, we pre-multiply by the inverse of $D$. As $D$ is a diagonal matrix with non-zero entries $d_i$, its inverse $D^{-1}$ is simply the diagonal matrix with entries $1/d_i$, i.e., $D^{-1} = \\mathrm{diag}(1/d_1, \\dots, 1/d_n)$. Pre-multiplying gives:\n$$\nD^{-1} (D Q) A^{-1} = D^{-1} Q\n$$\n$$\n(D^{-1} D) (Q A^{-1}) = D^{-1} Q\n$$\nSince $D^{-1} D = I$, we have:\n$$\nI (Q A^{-1}) = D^{-1} Q \\implies Q A^{-1} = D^{-1} Q\n$$\nFinally, to isolate $A^{-1}$, we pre-multiply by $Q^\\top$. Since $Q^{-1}=Q^\\top$:\n$$\nQ^\\top (Q A^{-1}) = Q^\\top D^{-1} Q\n$$\n$$\n(Q^\\top Q) A^{-1} = Q^\\top D^{-1} Q\n$$\nAs $Q^\\top Q = I$, we arrive at the closed-form expression for the analytical inverse:\n$$\nA^{-1} = Q^\\top D^{-1} Q\n$$\nThis expression will be used as the exact ground truth against which the numerical approximation is compared.\n\n**2. Numerical Computation of the Inverse Matrix**\n\nThe numerical approximation of the inverse, denoted by $X$, is to be found by solving the matrix equation $AX=I$. This is a standard approach where each column of $X$, say $x_j$, is the solution to the linear system $Ax_j = e_j$, with $e_j$ being the $j$-th canonical basis vector (the $j$-th column of the identity matrix $I$).\n\nThe procedure utilizes LU factorization with partial pivoting. First, we factorize $A$ such that $P A = L U$, where $P$ is a permutation matrix, $L$ is a unit lower triangular matrix, and $U$ is an upper triangular matrix. The system $A x_j = e_j$ is then rewritten as $P^{-1} L U x_j = e_j$, or $L U x_j = P e_j$. For each column $e_j$, we solve this system in two steps:\n- Forward substitution: Solve $L y_j = P e_j$ for the intermediate vector $y_j$.\n- Backward substitution: Solve $U x_j = y_j$ for the solution vector $x_j$.\n\nThese steps are performed for all columns $j=1, \\dots, n$ to construct the full matrix $X = [x_1, \\dots, x_n]$.\n\n**3. Error Metrics and Spectral Analysis**\n\nWith both the exact inverse $A^{-1}$ and its numerical approximation $X$, we can quantify the error and its relationship to the spectrum of $A$.\n\n- **Global Relative Error ($\\mathrm{frel}$)**: The total error is measured in the Frobenius norm. The error matrix is $E = X - A^{-1}$. The relative error is:\n$$\n\\mathrm{frel} = \\frac{\\lVert X - A^{-1}\\rVert_F}{\\lVert A^{-1}\\rVert_F} = \\frac{\\lVert E \\rVert_F}{\\lVert A^{-1}\\rVert_F}\n$$\nwhere the Frobenius norm of a matrix $M$ is $\\lVert M \\rVert_F = \\sqrt{\\sum_{i,j} |M_{ij}|^2}$.\n\n- **Error in the Eigenbasis and Correlation ($\\mathrm{corr}$)**: The matrix construction $A = Q^\\top D Q$ is the spectral decomposition of $A$, since $A$ is symmetric ($A^\\top = (Q^\\top D Q)^\\top = Q^\\top D^\\top Q = Q^\\top D Q = A$). The eigenvalues of $A$ are the diagonal entries $d_i$ of $D$, and the corresponding eigenvectors are the columns of $Q^\\top$. To analyze the error in this eigenbasis, we transform the error matrix $E$ into this basis:\n$$\n\\widetilde{E} = Q E Q^\\top\n$$\nIn this basis, the matrix $A$ is diagonal ($Q A Q^\\top = D$) and its inverse is also diagonal ($Q A^{-1} Q^\\top = D^{-1}$). The diagonal entries of $\\widetilde{E}$, denoted $\\widetilde{E}_{ii}$, represent the error component along each eigendirection. The per-direction relative diagonal error $\\epsilon_i$ is defined as the magnitude of the error component relative to the magnitude of the true inverse's component in that direction:\n$$\n\\epsilon_i = \\frac{\\left| \\widetilde{E}_{ii} \\right|}{\\left| (A^{-1})_{ii} \\text{ in the eigenbasis} \\right|} = \\frac{|\\widetilde{E}_{ii}|}{|(D^{-1})_{ii}|} = |\\widetilde{E}_{ii}| |d_i|\n$$\nWe then compute the Pearson correlation coefficient, $\\mathrm{corr}$, between the vectors $u$ and $v$ whose components are $u_i = \\log_{10}(|d_i|)$ and $v_i = \\log_{10}(\\epsilon_i + \\delta)$, where $\\delta=10^{-300}$ is a small regularizer. A positive correlation would suggest that errors are larger for larger eigenvalues, while a negative correlation would suggest errors are larger for smaller eigenvalues. By convention, if the variance of $u_i$ is zero (i.e., all $|d_i|$ are equal), the correlation is defined as $0.0$.\n\n- **Condition Number ($\\kappa_2(A)$)**: The $2$-norm condition number of a matrix is the ratio of its largest to smallest singular values, $\\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$. For a symmetric matrix such as $A$, the singular values are the absolute values of the eigenvalues. Since the eigenvalues of $A$ are the entries $d_i$ of $D$, the singular values are $|d_i|$. Therefore, the condition number is given by:\n$$\n\\kappa_2(A) = \\frac{\\max_i |d_i|}{\\min_i |d_i|}\n$$\nThis quantity measures the sensitivity of the solution of $Ax=b$ to perturbations in $A$ and $b$. A large $\\kappa_2(A)$ indicates an ill-conditioned matrix, for which numerical computations are expected to be less accurate.\n\nThe implementation will proceed by constructing the matrices for each test case, performing the numerical calculations as described, and reporting the three specified quantities: $[\\mathrm{frel}, \\mathrm{corr}, \\kappa_2(A)]$.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Computes and benchmarks matrix inverse via LU factorization for four test cases.\n    \"\"\"\n    \n    n = 12\n    delta = 10**-300.0\n\n    test_cases_spec = [\n        {'s': 0, 'case_type': 'identity'},\n        {'s': 1, 'case_type': 'spd_ill_cond'},\n        {'s': 2, 'case_type': 'indefinite_ill_cond'},\n        {'s': 3, 'case_type': 'clustered_spectrum'},\n    ]\n\n    all_results = []\n\n    for spec in test_cases_spec:\n        s = spec['s']\n        case_type = spec['case_type']\n\n        # 1. Generate the orthogonal matrix Q\n        rng = np.random.default_rng(s)\n        Z = rng.standard_normal((n, n))\n        Q_qr, R_qr = np.linalg.qr(Z)\n        \n        # Ensure a unique Q by forcing the diagonal of R to be positive\n        signs = np.sign(np.diag(R_qr))\n        signs[signs == 0] = 1.0  # As per problem, sign(0) is 1\n        S = np.diag(signs)\n        Q = Q_qr @ S\n\n        # 2. Construct the diagonal matrix D for the current case\n        if case_type == 'identity':\n            d = np.ones(n)\n        elif case_type == 'spd_ill_cond':\n            d = np.logspace(-6, 6, n)\n        elif case_type == 'indefinite_ill_cond':\n            mags = np.logspace(-3, 3, n)\n            sign_vec = (-1.0) ** np.arange(n)\n            d = mags * sign_vec\n        elif case_type == 'clustered_spectrum':\n            d_vals = [10**-6.0] * 10 + [1.0] * 2\n            rng_permute = np.random.default_rng(s)\n            rng_permute.shuffle(d_vals)\n            d = np.array(d_vals)\n        \n        D = np.diag(d)\n        \n        # 3. Construct the matrix A\n        A = Q.T @ D @ Q\n\n        # 4. Compute the analytical inverse A_inv_true\n        D_inv = np.diag(1.0 / d)\n        A_inv_true = Q.T @ D_inv @ Q\n\n        # 5. Compute the numerical inverse X using LU factorization\n        lu, piv = linalg.lu_factor(A)\n        I = np.eye(n)\n        X = linalg.lu_solve((lu, piv), I)\n\n        # 6. Compute the required metrics\n        \n        # frel: Global relative error\n        E = X - A_inv_true\n        frel = np.linalg.norm(E, 'fro') / np.linalg.norm(A_inv_true, 'fro')\n        \n        # kappa_2: Condition number\n        kappa_2 = np.max(np.abs(d)) / np.min(np.abs(d))\n        \n        # corr: Pearson correlation coefficient\n        log_d_abs = np.log10(np.abs(d))\n        \n        if np.var(log_d_abs) == 0:\n            corr = 0.0\n        else:\n            E_tilde = Q @ E @ Q.T\n            E_tilde_diag = np.diag(E_tilde)\n            \n            # Per-direction relative diagonal error\n            epsilon = np.abs(E_tilde_diag) * np.abs(d)\n            \n            log_epsilon = np.log10(epsilon + delta)\n            \n            # Pearson correlation\n            corr_matrix = np.corrcoef(log_d_abs, log_epsilon)\n            corr = corr_matrix[0, 1]\n\n        all_results.append(f\"[{frel},{corr},{kappa_2}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}