## 应用与跨学科关联

在前一章中，我们详细探讨了 Cholesky 分解的原理、计算方法及其卓越的数值稳定性。作为求解[对称正定](@entry_id:145886) (Symmetric Positive Definite, SPD) 线性系统的首选直接方法，Cholesky 分解不仅在理论上优雅，更在实践中扮演着基石性的角色。本章旨在超越其基础理论，展示 Cholesky 分解及其变体如何在众多科学与工程领域中被广泛应用，并与其他学科思想深度融合，解决各种复杂问题。

我们将探索 Cholesky 分解在[统计推断](@entry_id:172747)、机器学习、[大规模优化](@entry_id:168142)、信号处理和[科学计算](@entry_id:143987)等领域的应用。本章的目的不是重复介绍核心原理，而是通过一系列精心设计的应用场景，揭示这些原理在跨学科背景下的强大威力与深刻内涵。读者将看到，从基础的最小二乘问题到[现代机器学习](@entry_id:637169)模型中的梯度计算，Cholesky 分解都是一个不可或缺的计算引擎。

### 科学计算与统计学基础

Cholesky 分解最直接的应用在于高效且稳定地求解[对称正定](@entry_id:145886)线性系统，这是许多计算问题的核心。

#### 求解对称正定[线性系统](@entry_id:147850)：正规方程组

线性最小二乘问题是数据科学和工程计算中的一个基本问题，旨在寻找一个解 $x$ 以最小化残差的 [2-范数](@entry_id:636114) $\|Ax-b\|_2$。当矩阵 $A \in \mathbb{R}^{m \times n}$ ($m \ge n$) 列满秩时，该问题的解满足如下的正规方程组 (Normal Equations)：
$$
(A^{\top}A)x = A^{\top}b
$$
矩阵 $G = A^{\top}A$ 是一个 $n \times n$ 的对称正定矩阵，这为 Cholesky 分解提供了完美的舞台。通过计算 $G$ 的 Cholesky 分解 $G = LL^{\top}$，原方程的求解被转化为两个简单的三角系统求解：首先解 $Lz = A^{\top}b$ 得到 $z$，然后解 $L^{\top}x = z$ 得到最终解 $x$。此方法的计算复杂度主要由 Cholesky 分解决定，约为 $O(n^3)$。

然而，正规方程法的一个显著缺点在于其对矩阵条件的敏感性。$G$ 的条件数是原矩阵 $A$ 条件数的平方，即 $\kappa_2(A^{\top}A) = (\kappa_2(A))^2$。如果 $A$ 本身是病态的 (ill-conditioned)，$\kappa_2(A)$ 很大，那么 $\kappa_2(G)$ 将会极大，这会导致 Cholesky 分解和随后的三角求解过程中的[舍入误差](@entry_id:162651)被严重放大，从而损害解的精度。为了改善这一问题，可以采用[预处理](@entry_id:141204)技术。一个简单而有效的策略是列缩放 (column scaling)，即通过一个[对角矩阵](@entry_id:637782) $D$ 对 $A$ 的列进行缩放，形成新的矩阵 $\tilde{A} = AD$。选择合适的 $D$ (例如，使得 $\tilde{A}$ 的所有列具有单位 [2-范数](@entry_id:636114)) 可以显著降低 $\tilde{A}$ 的[条件数](@entry_id:145150)，进而改善其[正规方程](@entry_id:142238)矩阵 $\tilde{G} = \tilde{A}^{\top}\tilde{A} = D^{\top}GD$ 的条件数，提高 Cholesky 分解求解过程的[数值稳定性](@entry_id:146550)。尽管列缩放无法改变条件数平方化的内在关系，但它通过改善 $A$ 本身的[条件数](@entry_id:145150)，有效地提升了实际计算中的精度 。

#### 多元统计与[蒙特卡洛模拟](@entry_id:193493)

在统计学中，特别是[多元分析](@entry_id:168581)和[贝叶斯推断](@entry_id:146958)中，从[多元正态分布](@entry_id:175229) $\mathcal{N}(\mu, \Sigma)$ 中生成随机样本是一项核心任务。其中，$\mu$ 是[均值向量](@entry_id:266544)，$\Sigma$ 是对称正定的[协方差矩阵](@entry_id:139155)。一个标准方法是利用 Cholesky 分解。首先，计算 $\Sigma$ 的 Cholesky 分解 $\Sigma = LL^{\top}$。然后，生成一个分量独立的标准正态随机向量 $Z \sim \mathcal{N}(0, I)$。通过线性变换 $X = \mu + LZ$，得到的随机向量 $X$ 就服从[目标分布](@entry_id:634522) $\mathcal{N}(\mu, \Sigma)$。这是因为 $X$ 的协[方差](@entry_id:200758)为：
$$
\mathbb{E}[(X-\mu)(X-\mu)^{\top}] = \mathbb{E}[(LZ)(LZ)^{\top}] = L\mathbb{E}[ZZ^{\top}]L^{\top} = LIL^{\top} = LL^{\top} = \Sigma
$$
值得注意的是，任何满足 $MM^{\top} = \Sigma$ 的矩阵 $M$ 都可以用于生成服从该[分布](@entry_id:182848)的样本。Cholesky 因子 $L$ 是其中一个选择，它具有计算高效且数值稳定的优点。另一个重要的选择是[主平方根](@entry_id:180892) (principal square root) $\Sigma^{1/2}$。通过谱分解 $\Sigma = UDU^{\top}$，[主平方根](@entry_id:180892)被唯一定义为 $\Sigma^{1/2} = UD^{1/2}U^{\top}$，它是一个对称正定矩阵。除非 $\Sigma$ 是[对角矩阵](@entry_id:637782)，否则 Cholesky 因子 $L$ (一个下[三角矩阵](@entry_id:636278)) 与[主平方根](@entry_id:180892) $\Sigma^{1/2}$ (一个对称矩阵) 是完全不同的矩阵。然而，它们在生成具有相同[分布](@entry_id:182848)的二次型时是等价的，因为对于任意[对称矩阵](@entry_id:143130) $A$，[随机变量](@entry_id:195330) $Z^{\top}L^{\top}ALZ$ 和 $Z^{\top}(\Sigma^{1/2})A(\Sigma^{1/2})Z$ 具有相同的[分布](@entry_id:182848) 。

当[协方差矩阵](@entry_id:139155) $\Sigma$ 病态或接近奇异时，直接对其进行 Cholesky 分解在有限精度计算中可能失败或产生严重误差。一种常见的稳定化策略是向[协方差矩阵](@entry_id:139155)添加一个小的对角扰动，即“[抖动](@entry_id:200248)” (jittering)，形成正则化后的矩阵 $\Sigma_{\varepsilon} = \Sigma + \varepsilon I$。选择合适的 $\varepsilon  0$ 不仅能保证矩阵的正定性，还能显著改善其[条件数](@entry_id:145150)，从而稳定 Cholesky 分解和后续的采样过程。然而，[抖动](@entry_id:200248)也改变了原始的[概率分布](@entry_id:146404)。最优的 $\varepsilon$ 是在[数值稳定性](@entry_id:146550)和对原始[分布](@entry_id:182848)的忠实度之间的一个权衡。可以通过 Kullback–Leibler (KL) 散度来量化这种[分布](@entry_id:182848)上的偏差，并结合对采样协[方差](@entry_id:200758)误差的分析，来选择一个既能满足[数值精度](@entry_id:173145)要求又能最小化[分布](@entry_id:182848)失真的[抖动](@entry_id:200248)大小 。

### 机器学习与优化中的前沿应用

Cholesky 分解是[现代机器学习](@entry_id:637169)和大规模[数值优化](@entry_id:138060)算法中不可或缺的组成部分，它为高效和稳健的计算提供了保障。

#### [高斯过程回归](@entry_id:276025)

[高斯过程](@entry_id:182192) (Gaussian Process, GP) 是一种功能强大的非参数贝叶斯方法，广泛用于回归和[分类任务](@entry_id:635433)。在 GP 回归中，给定训练数据，预测新数据点的均值和[方差](@entry_id:200758)需要求解一个线性系统并计算一个矩阵的[对数行列式](@entry_id:751430)。具体而言，核心计算涉及形式为 $(K + \sigma_n^2 I)\alpha = y$ 的线性系统，其中 $K$ 是由[核函数](@entry_id:145324)在训练点上计算得到的对称[半正定核](@entry_id:637268)矩阵，$\sigma_n^2 I$ 是代表观测噪声的对角正则项。矩阵 $A = K + \sigma_n^2 I$ 是[对称正定](@entry_id:145886)的，这使得 Cholesky 分解成为理想的求解工具。

使用 Cholesky 分解 $A = LL^{\top}$ 的优势是多方面的：
1.  **高效求解**：求解 $A\alpha=y$ 分解为两个三角求解 $Lz=y$ 和 $L^{\top}\alpha=z$，计算成本远低于直接求逆。
2.  **数值稳定**：对于 SPD 矩阵，Cholesky 分解是数值后向稳定的，无需进行主元选择，这保留了矩阵的对称性并提高了效率。
3.  **[对数行列式](@entry_id:751430)计算**：GP 模型的超参数通常通过最大化对数边缘似然函数来学习，该函数包含 $\log\det(A)$ 项。利用 Cholesky 因子，此项可以非常高效且稳定地计算：$\log\det(A) = \log\det(LL^{\top}) = 2\log\det(L) = 2 \sum_{i} \log(L_{ii})$。

相较于显式计算矩阵的逆 $A^{-1}$，Cholesky 分解在计算成本（具有更小的常数因子）和[数值精度](@entry_id:173145)上都具有明显优势。在实践中，例如在[计算核物理](@entry_id:747629)中用于模拟[基态](@entry_id:150928)[结合能](@entry_id:143405)的[高斯过程](@entry_id:182192)仿真器中，Cholesky 分解是训练和[不确定性量化](@entry_id:138597)过程中的标准计算工具 。

#### 似然函数计算中的数值稳健性

在许多[统计模型](@entry_id:165873)（如[高斯过程](@entry_id:182192)）中计算[对数似然函数](@entry_id:168593)时，[对数行列式](@entry_id:751430)项 $\log\det(A)$ 的稳健计算至关重要。当矩阵 $A$ 的[特征值](@entry_id:154894)跨越多个[数量级](@entry_id:264888)时，直接计算[行列式](@entry_id:142978) $\det(A)$ 很容易导致浮点数的上溢 (overflow) 或[下溢](@entry_id:635171) (underflow)。例如，一个 $64 \times 64$ 的对角矩阵，其对角元素均为 $2^{20}$，其[行列式](@entry_id:142978)为 $(2^{20})^{64} = 2^{1280}$，这个数值远远超出了标准双精度浮点数所能表示的范围 ($\approx 2^{1024}$)。

然而，通过 Cholesky 分解 $A = LL^{\top}$ 计算[对数行列式](@entry_id:751430)，即 $2 \sum_i \log(L_{ii})$，则完全避免了这个问题。在该例子中，Cholesky 因子 $L$ 的对角元素为 $2^{10}$，其对数是适中的数值，它们的求和结果 $1280 \ln(2)$ 也在[浮点数](@entry_id:173316)的表示范围内。这种方法将一个可能溢出的大数的乘积问题，转化为一个数值表现良好的小数的求和问题，极大地扩展了算法的动态范围和数值稳健性 。

更进一步，即使是 $\sum \log(L_{ii})$ 这个求和过程本身，在有限精度下也并非没有误差。当 Cholesky 因子 $L$ 的对角元素 $L_{ii}$ 存在微小的相对[舍入误差](@entry_id:162651)时，通过[非线性](@entry_id:637147)的对数函数传播，会给最终的[对数行列式](@entry_id:751430)带来一个系统性的偏差 (bias)。可以证明，在标准的[浮点误差](@entry_id:173912)模型下，这个偏差的[期望值](@entry_id:153208)约为 $-nu^2/3$，其中 $n$ 是矩阵维度，$u$ 是机器单位舍入。这表明计算结果会倾向于系统性地偏小。当 $L_{ii}$ 的值[分布](@entry_id:182848)范围很广时，求和过程中的吸收误差也可能变得显著。为了缓解这个问题，可以使用[补偿求和](@entry_id:635552)算法（如 Kahan summation）来提高求和的精度，从而使最终误差更接近于这个理论偏差下限 。

#### Cholesky 分解的[自动微分](@entry_id:144512)

在[现代机器学习](@entry_id:637169)框架中，模型参数通常通过[基于梯度的优化](@entry_id:169228)算法进行学习，这依赖于[自动微分](@entry_id:144512) (Automatic Differentiation, AD) 来计算复杂函数（如[损失函数](@entry_id:634569)）的梯度。当 Cholesky 分解作为模型[计算图](@entry_id:636350)的一部[分时](@entry_id:274419)（例如在[高斯过程](@entry_id:182192)中），我们需要能够对这个分解过程进行[微分](@entry_id:158718)，即所谓的“[反向传播](@entry_id:199535)”。

对 Cholesky 分解求导是一个微妙的过程，特别是当输入矩阵接近奇异时。数值不稳定性不仅会影响[前向计算](@entry_id:193086)（即分解本身），还会影响[反向传播](@entry_id:199535)过程中的梯度计算。分析表明，[前向计算](@entry_id:193086)中的[舍入误差](@entry_id:162651) $E$ (即 $\widehat{L}\widehat{L}^{\top} = A+E$) 会通过梯度表达式传播，导致梯度误差。这个误差的大小与矩阵 $A$ 的[条件数](@entry_id:145150) $\kappa(A) = \|A\|_2 \|A^{-1}\|_2$ 密切相关。当 $A$ 病态时，$\|A^{-1}\|_2$ 很大，梯度误差会被急剧放大。

与[前向计算](@entry_id:193086)类似，向矩阵 $A$ 添加一个小的对角正则化项 $\epsilon I$ 是稳定 Cholesky 分解及其梯度的有效方法。这个正则化项 $\epsilon$ 的选择是一个权衡：太小不足以抑制数值误差，太大则会过度偏离原始问题。通过对梯度误差的[上界](@entry_id:274738)进行数学分析，可以推导出关于 $\epsilon$ 的表达式，并找到最小化该误差上界的最优 $\epsilon$ 值。这个过程展示了如何通过理论分析来指导正则化参数的选择，以同时保证[前向计算](@entry_id:193086)（[模型推断](@entry_id:636556)）和反向计算（模型训练）的数值稳定性 。

#### [非凸优化](@entry_id:634396)与[信赖域方法](@entry_id:138393)

在[非凸优化](@entry_id:634396)问题中，牛顿法及其变体是寻找局部最优解的核心算法。这些方法需要求解形式为 $Hs = -g$ 的牛顿方程，其中 $H$ 是目标函数的 Hessian 矩阵，$g$ 是梯度。然而，在非凸区域或[鞍点](@entry_id:142576)附近，$H$可能是 indefnite（不定）的，即同时拥有正负[特征值](@entry_id:154894)。在这种情况下，标准的 Cholesky 分解无法进行。

信赖域 (Trust-Region) 方法是处理此问题的一类强大技术。它通过在当前[点的邻域](@entry_id:144055)（信赖域）内求解一个二次子问题来寻找下一步的 trial step。当 Hessian 矩阵 $H$ 不定时，一个标准的策略是求解正则化的牛顿方程：
$$
(H + \lambda I)s = -g
$$
其中 $\lambda \ge 0$ 是一个自适应选择的 damping 参数。这里的目标是选择足够大的 $\lambda$，使得矩阵 $A = H + \lambda I$ 变为[对称正定](@entry_id:145886)，从而可以用 Cholesky 分解稳定地求解。

$\lambda$ 的选择必须满足两个条件：
1.  **稳健正定性**：在有限精度下，Cholesky 分解的[后向稳定性](@entry_id:140758)意味着它实际上分解的是一个扰动后的矩阵 $A+\Delta A$。为了保证分解过程在数值上可行，必须要求对于所有可能的、由算法引起的扰动 $\Delta A$，矩阵 $A+\Delta A$ 都保持正定。这要求 $A$ 的最小特征值 $\lambda_{\min}(A)$ 必须大于扰动范数 $\|\Delta A\|_2$ 的[上界](@entry_id:274738)。这为 $\lambda$ 提供了一个下界。
2.  **解的精度**：信赖域算法通常对 trial step 的精度有要求。由[后向误差分析](@entry_id:136880)可知，解的相对[前向误差](@entry_id:168661)受 $A$ 的条件数 $\kappa_2(A)$ 控制。为了将[误差控制](@entry_id:169753)在某个容忍度 $\eta$ 之内，需要对 $\kappa_2(A)$ 进行限制，这反过来也为 $\lambda$ 提供了一个下界。

最终，必须选择满足这两个条件中更严格的那个下界，以确保 Cholesky 分解既能成功执行，又能产生满足优化算法精度要求的解 。

### 高性能与专用算法

除了作为通用求解器，Cholesky 分解的思想还催生了多种针对特定结构或应用场景的高效专用算法。

#### [广义特征值问题](@entry_id:151614)

[广义特征值问题](@entry_id:151614) $Ax = \lambda Bx$ 在许多物理和工程领域中都非常重要，例如结构力学中的[振动分析](@entry_id:146266)。当 $A$ 和 $B$ 都是对称矩阵且 $B$ 是正定时，该问题可以通过 Cholesky 分解简化为标准的[特征值问题](@entry_id:142153)。

具体方法是计算 $B$ 的 Cholesky 分解 $B = LL^{\top}$。然后，对原方程进行变换：
$$
Ax = \lambda LL^{\top}x \implies A (L^{\top})^{-1} L^{\top} x = \lambda L (L^{\top}x)
$$
令 $y = L^{\top}x$，并左乘 $L^{-1}$，我们得到：
$$
(L^{-1} A (L^{\top})^{-1}) y = \lambda y
$$
这是一个标准的[对称特征值问题](@entry_id:755714)，因为矩阵 $C = L^{-1} A (L^{\top})^{-1}$ 是对称的。我们可以用标准算法求解出 $C$ 的[特征值](@entry_id:154894) $\lambda$ 和[特征向量](@entry_id:151813) $y$，然后通过[回代](@entry_id:146909) $x = (L^{\top})^{-1}y$ 得到原问题的[广义特征向量](@entry_id:152349)。

在实践中，我们常常需要为一个[子空间](@entry_id:150286)计算一个 $B$-正交基，即一个矩阵 $V$ 使得 $V^{\top}BV = I$。这可以通过一种称为 Cholesky-Gram-Schmidt 的方法实现。该方法首先计算 $B$ 的 Cholesky 因子 $L$，然后对变换后的[基向量](@entry_id:199546) $LW$（其中 $W$ 是原始基）进行标准的 Gram-Schmidt 正交化得到 $Q$，最后通过求解三角系统 $L^{\top}V = Q$ 得到 $B$-[正交基](@entry_id:264024) $V$。这种方法的数值稳定性与 $B$ 的[条件数](@entry_id:145150) $\kappa_2(B)$ 密切相关，其 $B$-正交性的损失在一阶上正比于 $u \cdot \kappa_2(B)$ 。

#### [序列数据](@entry_id:636380)同化与卡尔曼滤波

在卡尔曼滤波 (Kalman filtering) 等序列数据同化应用中，[协方差矩阵](@entry_id:139155)会随着新数据的到来而进行迭代更新。这些更新通常是低秩的，例如 rank-one 更新或 downdate。如果每次更新后都从头重新计算[协方差矩阵](@entry_id:139155)的 Cholesky 分解，计算成本会非常高昂。

幸运的是，存在高效的算法可以直接更新 Cholesky 因子。对于一个 rank-one 更新 $P_{k+1} = P_k + vv^{\top}$，其 Cholesky 因子 $L_{k+1}$ 可以通过一系列 Givens 旋转从 $L_k$ 高效计算得到，其计算复杂度仅为 $O(n^2)$，远低于完全重分解的 $O(n^3)$。类似地，rank-one downdate ($P_{k+1} = P_k - vv^{\top}$) 也有相应的更新算法。

然而，连续进行大量的更新和 downdate 会导致舍入误差的累积。每一次更新都会引入一个小的[后向误差](@entry_id:746645)，这些误差会随着时间累积，可能最终导致计算出的 Cholesky 因子与真实[协方差矩阵](@entry_id:139155)的因子相去甚远，甚至可能失去[正定性](@entry_id:149643)。通过对[误差累积](@entry_id:137710)过程进行分析，可以推导出累积误差的上界。这个上界依赖于更新的次数 $m$、矩阵维度 $n$、机器精度 $u$以及[矩阵范数](@entry_id:139520)的界。基于这个[误差界](@entry_id:139888)，可以制定一个策略：在执行一定数量的快速更新后，进行一次完整的 Cholesky 重分解，以重置累积的数值误差，从而在[计算效率](@entry_id:270255)和[数值精度](@entry_id:173145)之间取得平衡 。

#### [稀疏性](@entry_id:136793)、填充与[矩阵重排](@entry_id:637022)

在许多科学与工程应用中，例如有限元分析和电路模拟，涉及的 SPD 矩阵通常是巨大的[稀疏矩阵](@entry_id:138197)。直接应用 Cholesky 分解到[稀疏矩阵](@entry_id:138197)上会面临一个称为“填充”(fill-in) 的现象：即使原始矩阵 $A$ 非常稀疏，其 Cholesky 因子 $L$ 也可能包含许多新的非零元素，从而大大增加存储和计算成本。

填充的数量和位置与矩阵的非零结构以及消元顺序密切相关。利用图论，我们可以将一个对称矩阵的稀疏模式表示为一个[无向图](@entry_id:270905)，其中节点对应于矩阵的行/列，边对应于非零的非对角元素。Cholesky 分解的消元过程在图上对应于一系列的节点消去操作。当一个节点 $i$ 被消去时，所有与 $i$ 相邻的、且索引大于 $i$ 的节点之间都会被添加一条边（如果尚不存在），形成一个团 (clique)。这个过程产生的最终图的结构精确地预测了 Cholesky 因子 $L$ 的非零模式。通过在分解前对图进行符号分析，我们可以预先知道 $L$ 的[稀疏结构](@entry_id:755138)，并为其分配内存 。

为了最小化填充，可以对矩阵的行和列进行对称重排 (reordering)，这等价于改变图节点的编号顺序。寻找最优重排顺序是一个 NP-难问题，但存在许多高效的启发式算法，例如：
*   **Reverse Cuthill-McKee (RCM)**：一种旨在减小[矩阵带宽](@entry_id:751742)的算法，通常也能有效减少填充。
*   **Nested Dissection (ND)**：一种基于“分治”思想的[递归算法](@entry_id:636816)。它通过寻找小的“节点分隔符”将图切分为两个或多个[子图](@entry_id:273342)，然后递归地对[子图](@entry_id:273342)进行排序。ND 算法对于源于二维或[三维几何](@entry_id:176328)问题的矩阵尤其有效。

有趣的是，这些为减少填充而设计的重排策略，也可能影响分解的数值稳定性。对于接近奇异或不定的[对称矩阵](@entry_id:143130)，不同的消元顺序会导致不同的中间 Schur 补矩阵，从而影响数值行为。例如，在分解一个具有单个小负[特征值](@entry_id:154894)的矩阵时，某些重排顺序可能会比其他顺序更早或更频繁地遇到不稳定的消元步骤（在 $LDL^{\top}$ 分解中表现为负主元）。这揭示了[稀疏矩阵算法](@entry_id:755105)中，优化计算效率（最小化填充）和保证数值稳定性之间的复杂相互作用 。

#### 扩展：主元、分块与不定分解

标准的 Cholesky 分解只适用于[对称正定矩阵](@entry_id:136714)。然而，其核心思想可以被扩展以处理更广泛的矩阵类别。

*   **主元 Cholesky 分解 (Pivoted Cholesky Factorization)**：对于对称半正定 (Positive Semidefinite, PSD) 矩阵，标准的 Cholesky 分解可能因遇到零主元而失败。主元 Cholesky 分解通过在每一步选择对角线上最大的元素作为主元来进行消元。这个过程可以稳定地进行，并且是一种可靠的计算矩阵[数值秩](@entry_id:752818)的方法。当算法进行到第 $k$ 步时，如果剩余对角元素的最大值小于某个预设的容差，算法便终止，此时 $k$ 就是矩阵的[数值秩](@entry_id:752818) 。

*   **分块 Cholesky 分解 (Block Cholesky Factorization)**：Cholesky 分解可以自然地推广到[分块矩阵](@entry_id:148435)。对于一个 $2 \times 2$ 的对称[分块矩阵](@entry_id:148435)，其 Cholesky 因子也是分块下三角的。分解过程递归地包含对角块的 Cholesky 分解、三角系统求解和 Schur 补的形成与分解。这种分块视角不仅是许多高性能[并行算法](@entry_id:271337)的基础，也是理解和推导许多高级矩阵算法（如与 Schur 补相关的算法）的理论基石 。

*   **$LDL^{\top}$ 分解**：对于不定但可逆的对称矩阵，Cholesky 分解 $LL^{\top}$不再适用，但可以采用 $LDL^{\top}$ 分解，其中 $L$ 是单位下三角矩阵，$D$ 是对角矩阵（但其对角元素可正可负）。$LDL^{\top}$ 分解与 Cholesky 分解密切相关，可以看作是无需计算平方根的版本。它在处理[不定系统](@entry_id:750604)时非常重要，例如在前面提到的[非凸优化](@entry_id:634396)中。

### 结论

本章的旅程清晰地表明，Cholesky 分解远不止是一个教科书中的线性代数概念。它是一个强大、灵活且高效的计算工具，其影响渗透到现代科学与工程的各个角落。从[统计建模](@entry_id:272466)中的[参数推断](@entry_id:753157)和随机采样，到机器学习中的核心模型训练；从[大规模优化](@entry_id:168142)中的稳健迭代，到科学计算中的[特征值](@entry_id:154894)和[稀疏系统](@entry_id:168473)求解，Cholesky 分解及其变体始终扮演着关键角色。

理解 Cholesky 分解的特性——特别是其[数值稳定性](@entry_id:146550)、对矩阵结构的高效利用以及与相关数学概念（如[图论](@entry_id:140799)、概率论）的深刻联系——是设计和实现先进计算方法的基础。随着数据规模和模型复杂性的不断增长，对 Cholesky 分解及其高性能实现的依赖只会与日俱增，使其成为每个计算科学家和工程师知识库中不可或缺的一部分。