{
    "hands_on_practices": [
        {
            "introduction": "While the LU factorization provides an elegant framework for solving linear systems, its most basic form without pivoting is not universally applicable. This exercise delves into the fundamental conditions under which the algorithm can fail, linking the theoretical concept of leading principal minors to the practical occurrence of a zero pivot during Gaussian elimination. By constructing a specific counterexample, you will gain a rigorous understanding of why pivoting is often a necessity, not just an enhancement .",
            "id": "3578142",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be a real matrix. Consider the attempt to compute a Lower-Upper (LU) factorization without row permutations, that is, to find a unit lower triangular matrix $L$ and an upper triangular matrix $U$ such that $A = L U$, using the row operations of Gaussian elimination (GE) without pivoting. Denote by $\\Delta_{k}$ the determinant of the leading $k \\times k$ principal submatrix of $A$.\n\n1. Construct explicitly a real $3 \\times 3$ matrix $A$ satisfying $\\Delta_{1} \\neq 0$ and $\\Delta_{2} = 0$, and for which the second elimination step of Gaussian elimination without pivoting would require division by the second pivot. Your construction must be explicit and justified from first principles (namely, the definition of GE and the induced Schur complement at each step).\n\n2. Starting only from the definition of Gaussian elimination and block-partitioned matrix multiplication, derive a necessary condition for the existence of an $A = L U$ factorization without pivoting in terms of the leading principal minors $\\Delta_{k}$ and the elimination pivots $p_{k}$. Your derivation must begin with the first elimination step on a $2 \\times 2$ leading block and proceed via the Schur complement, fully justifying each step.\n\n3. Apply your derivation to your explicit matrix $A$ to explain rigorously why $A = L U$ with unit diagonal $L$ and no row permutations cannot exist. Additionally, determine the numerical value of the second pivot $p_{2}$ that GE without pivoting would attempt to use on your constructed $A$.\n\nProvide the numerical value of $p_{2}$ as your final answer. No rounding is required.",
            "solution": "This problem comprises three parts concerning the existence of a Lower-Upper (LU) factorization of a matrix $A \\in \\mathbb{R}^{3 \\times 3}$ without row permutations. We address each part sequentially.\n\n1. Construction of the matrix $A$.\n\nWe are tasked with constructing a $3 \\times 3$ real matrix $A$ such that the determinant of its leading principal submatrices satisfy $\\Delta_{1} \\neq 0$ and $\\Delta_{2} = 0$, and for which Gaussian elimination (GE) without pivoting fails at the second step.\n\nLet the matrix $A$ be denoted by\n$$A = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}$$\nThe leading $1 \\times 1$ principal submatrix is $[a_{11}]$, and its determinant is $\\Delta_{1} = a_{11}$. The condition $\\Delta_{1} \\neq 0$ implies $a_{11} \\neq 0$. The value $a_{11}$ serves as the first pivot, $p_1$. For simplicity, let us choose $a_{11} = 1$.\n\nThe leading $2 \\times 2$ principal submatrix is $\\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}$, with determinant $\\Delta_{2} = a_{11}a_{22} - a_{12}a_{21}$. The condition is $\\Delta_{2} = 0$. With $a_{11} = 1$, this becomes $a_{22} - a_{12}a_{21} = 0$. We can satisfy this by choosing, for instance, $a_{12} = 1$, $a_{21} = 1$, and $a_{22} = 1$.\n\nSo far, the top-left $2 \\times 2$ block of $A$ is $\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$. Let us choose the remaining entries to create a simple, non-trivial matrix. Let $a_{13}=1$, $a_{23}=2$, $a_{31}=1$, $a_{32}=2$, and $a_{33}=3$. This gives the matrix\n$$A = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix}$$\nLet us verify the conditions. $\\Delta_{1} = \\det([1]) = 1 \\neq 0$.\n$\\Delta_{2} = \\det \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = (1)(1) - (1)(1) = 0$. The conditions are met.\n\nNow, we must justify from first principles that GE fails at the second step. The first step of GE uses the pivot $p_1 = a_{11} = 1$ to eliminate the entries below it.\nThe multiplier for the second row is $l_{21} = \\frac{a_{21}}{p_1} = \\frac{1}{1} = 1$.\nThe multiplier for the third row is $l_{31} = \\frac{a_{31}}{p_1} = \\frac{1}{1} = 1$.\nThe matrix after the first elimination step, $A^{(1)}$, is:\n$$A^{(1)} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 - (1)(1) & 1 - (1)(1) & 2 - (1)(1) \\\\ 1 - (1)(1) & 2 - (1)(1) & 3 - (1)(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 2 \\end{pmatrix}$$\nThe second pivot is the $(2,2)$ entry of $A^{(1)}$, which is $p_2 = a_{22}^{(1)} = 0$. To proceed with the second step of GE, we would need to eliminate the entry $a_{32}^{(1)}=1$ by using the pivot $p_2$. This would require computing a multiplier $l_{32} = \\frac{a_{32}^{(1)}}{p_2} = \\frac{1}{0}$, which involves division by zero. Thus, GE without pivoting fails at the second step.\n\nTo justify this via the Schur complement, we partition $A$:\n$$A = \\begin{pmatrix} a_{11} & \\mathbf{a}_{12}^T \\\\ \\mathbf{a}_{21} & A_{22} \\end{pmatrix} = \\begin{pmatrix} 1 & \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\\\ \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} & \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} \\end{pmatrix}$$\nThe first step of block LU decomposition yields:\n$$A = \\begin{pmatrix} 1 & \\mathbf{0}^T \\\\ \\mathbf{l}_{21} & I \\end{pmatrix} \\begin{pmatrix} a_{11} & \\mathbf{a}_{12}^T \\\\ \\mathbf{0} & A_{22} - \\mathbf{l}_{21} \\mathbf{a}_{12}^T \\end{pmatrix}$$\nwhere $\\mathbf{l}_{21} = \\frac{\\mathbf{a}_{21}}{a_{11}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The lower-right block is the Schur complement $S$ of $a_{11}$ in $A$:\n$$S = A_{22} - \\frac{\\mathbf{a}_{21} \\mathbf{a}_{12}^T}{a_{11}} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} - \\frac{1}{1}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} - \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 2 \\end{pmatrix}$$\nThe matrix after one step of GE has the Schur complement in its lower-right part: $A^{(1)} = \\begin{pmatrix} a_{11} & \\mathbf{a}_{12}^T \\\\ \\mathbf{0} & S \\end{pmatrix}$. The second pivot of GE is the $(1,1)$ entry of this Schur complement, $p_2 = S_{11} = 0$. This confirms the failure.\n\n2. Derivation of the necessary condition.\n\nWe are asked to derive a necessary condition for the existence of $A = LU$ (without pivoting) in terms of the leading principal minors $\\Delta_k$ and the pivots $p_k$. The derivation must start from the definition of GE.\n\nLet $A$ be an $n \\times n$ matrix. Gaussian elimination without pivoting produces a sequence of matrices $A=A^{(0)}, A^{(1)}, \\dots, A^{(n-1)}=U$, where $U$ is upper triangular. The pivots are $p_k = a_{kk}^{(k-1)}$.\n\nFor the first step to be possible, the first pivot $p_1 = a_{11}^{(0)} = a_{11}$ must be non-zero. The first leading principal minor is $\\Delta_1 = \\det([a_{11}]) = a_{11}$. Thus, a necessary condition is $p_1 = \\Delta_1 \\neq 0$.\n\nNow, let us consider the second pivot, $p_2$. This is the $(2,2)$-entry of the matrix $A^{(1)}$ obtained after the first step of elimination. The entries of $A^{(1)}$ are given by $a_{ij}^{(1)} = a_{ij}^{(0)} - \\frac{a_{i1}^{(0)}}{a_{11}^{(0)}} a_{1j}^{(0)}$ for $i,j > 1$. Specifically for the second pivot:\n$$p_2 = a_{22}^{(1)} = a_{22}^{(0)} - \\frac{a_{21}^{(0)}}{a_{11}^{(0)}} a_{12}^{(0)}$$\nUsing the original matrix entries $a_{ij}^{(0)}=a_{ij}$, we have:\n$$p_2 = a_{22} - \\frac{a_{21}a_{12}}{a_{11}} = \\frac{a_{11}a_{22} - a_{12}a_{21}}{a_{11}}$$\nThe numerator is the determinant of the leading $2 \\times 2$ principal submatrix of $A$, which is $\\Delta_2$. The denominator is the first leading principal minor, $\\Delta_1$. Therefore, we have the relation:\n$$p_2 = \\frac{\\Delta_2}{\\Delta_1}$$\nFor the second step of GE to proceed, the pivot $p_2$ must be non-zero. Since we already established the necessity of $\\Delta_1 \\neq 0$, the condition $p_2 \\neq 0$ is equivalent to $\\Delta_2 \\neq 0$.\n\nThis argument can be generalized. If the LU factorization exists, $A=LU$, then for any $k \\in \\{1, \\dots, n\\}$, the leading $k \\times k$ principal submatrix $A_k$ satisfies $A_k = L_k U_k$. Taking determinants, $\\det(A_k) = \\det(L_k) \\det(U_k)$. Since $L$ is unit lower triangular, $L_k$ is also, and $\\det(L_k) = 1$. The determinant of the upper triangular matrix $U_k$ is the product of its diagonal entries, which are the first $k$ pivots: $\\det(U_k) = p_1 p_2 \\cdots p_k$.\nThus, $\\Delta_k = p_1 p_2 \\cdots p_k$.\nThis gives $\\Delta_k = \\Delta_{k-1} p_k$ for $k \\ge 2$, which implies $p_k = \\frac{\\Delta_k}{\\Delta_{k-1}}$. For GE without pivoting to be successful through step $n-1$, all pivots $p_1, \\dots, p_{n-1}$ must be non-zero. This requires $\\Delta_k \\neq 0$ for all $k = 1, \\dots, n-1$. This is the necessary condition for the existence of an LU factorization without pivoting. (Note that $p_n$ can be zero, which corresponds to $\\Delta_n = \\det(A) = 0$; this is permissible as no divisions by $p_n$ are performed).\n\n3. Application to the constructed matrix $A$.\n\nWe apply the derived condition to the constructed matrix $A = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix}$.\nThe necessary condition for a $3 \\times 3$ matrix to have an LU factorization without pivoting is that $\\Delta_1 \\neq 0$ and $\\Delta_2 \\neq 0$.\nLet's compute these minors for our matrix $A$:\n$\\Delta_1 = \\det([1]) = 1$. This satisfies $\\Delta_1 \\neq 0$.\n$\\Delta_2 = \\det\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = (1)(1) - (1)(1) = 0$.\nSince $\\Delta_2 = 0$, the necessary condition is violated. Therefore, an LU factorization with a unit diagonal $L$ and no row permutations cannot exist for this matrix $A$.\n\nFinally, we must determine the numerical value of the second pivot $p_2$. Using the formula derived in Part 2:\n$$p_2 = \\frac{\\Delta_2}{\\Delta_1}$$\nSubstituting the calculated values for our matrix $A$:\n$$p_2 = \\frac{0}{1} = 0$$\nThis is consistent with our direct calculation in Part 1, where the $(2,2)$ entry of the matrix $A^{(1)}$ was found to be $0$. The value of the second pivot that GE without pivoting attempts to use is exactly $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The true power of LU factorization shines when solving a series of linear systems $AX=B$ that share the same coefficient matrix $A$. This practice guides you through a quantitative analysis of this efficiency, where you will model the computational cost and data movement (memory traffic) of the algorithm. By comparing the cost of a single factorization against repeated solves, you will derive the arithmetic intensity and understand why the 'factor-once, solve-many' strategy is a cornerstone of high-performance scientific computing .",
            "id": "3578124",
            "problem": "Consider a dense, nonsingular $n \\times n$ real matrix $A$ and an $n \\times m$ matrix $B$ collecting $m$ Right-Hand Sides (RHS). We want to solve $A X = B$ using the Lower-Upper (LU) factorization with partial pivoting and then two triangular solves per RHS. Use the following cost models and assumptions:\n\n- Floating-point operation (flop) count model: count one flop for each floating-point addition, subtraction, multiplication, or division. Ignore comparisons and data movement overhead in this flop count.\n- LU factorization is performed in-place with a unit diagonal in $L$; the trailing matrix updates are accounted for as multiply-subtract pairs.\n- Triangular solves consist of forward substitution with unit lower-triangular $L$ and back substitution with upper-triangular $U$; include one division per row in back substitution.\n- Memory traffic model: count one word of traffic for each read or write of a matrix or vector element between the processor and Random Access Memory (RAM). For the LU factorization, assume the entire matrix $A$ is streamed in once and overwritten in-place with $L$ and $U$, streaming out once. For the solves, in the \"factor reuse\" strategy assume $L$ and $U$ reside in fast memory after the factorization, so only $B$ (read) and $X$ (write) are streamed. In the \"repeated solves\" strategy, assume the factorization is recomputed for each RHS, streaming $A$ in and $LU$ out each time, and $B$ and $X$ are streamed for each RHS. Ignore traffic for the pivot index vector and any metadata.\n\nDefine the arithmetic intensity (AI) as $\\mathrm{AI} = \\text{flops}/\\text{words moved}$. Let $\\mathrm{AI}_{\\mathrm{reuse}}$ denote the arithmetic intensity for solving $A X = B$ by computing the LU factorization once and reusing it across $m$ RHS, and let $\\mathrm{AI}_{\\mathrm{repeat}}$ denote the arithmetic intensity for solving by recomputing the LU factorization independently for each RHS. Derive, from first principles under the models above, an exact closed-form expression for the improvement factor\n$$\\Phi(n,m) = \\frac{\\mathrm{AI}_{\\mathrm{reuse}}}{\\mathrm{AI}_{\\mathrm{repeat}}}.$$\nProvide your final answer as a single simplified symbolic expression in terms of $n$ and $m$. No rounding is required and no units should be included in the final expression.",
            "solution": "We begin from the definitions of the Lower-Upper (LU) factorization and triangular solves, and the given flop and memory traffic models.\n\n**Flop count for LU factorization:**\nAt elimination step $k$ (for $k=1, \\dots, n-1$), we compute $(n-k)$ multipliers, which costs $(n-k)$ flops (divisions). We then update the trailing $(n-k) \\times (n-k)$ submatrix. Each element update $a_{ij} \\leftarrow a_{ij} - l_{ik} u_{kj}$ costs 2 flops (one multiplication, one subtraction). Thus, the update costs $2(n-k)^2$ flops. The total flop count for LU is the sum over all steps:\n$$ F_{\\mathrm{LU}}(n) = \\sum_{k=1}^{n-1} \\left( (n-k) + 2(n-k)^2 \\right) $$\nLet $r=n-k$. The sum becomes:\n$$ F_{\\mathrm{LU}}(n) = \\sum_{r=1}^{n-1} (r + 2r^2) = \\sum_{r=1}^{n-1} r + 2 \\sum_{r=1}^{n-1} r^2 $$\nUsing the formulas for sums of powers: $\\sum_{r=1}^{N} r = \\frac{N(N+1)}{2}$ and $\\sum_{r=1}^{N} r^2 = \\frac{N(N+1)(2N+1)}{6}$ with $N=n-1$:\n$$ F_{\\mathrm{LU}}(n) = \\frac{(n-1)n}{2} + 2 \\frac{(n-1)n(2n-3)}{6} = \\frac{n(n-1)}{2} + \\frac{n(n-1)(2n-1)}{3} = \\frac{n(n-1)(3 + 2(2n-1))}{6} = \\frac{n(n-1)(4n+1)}{6} $$\n\n**Flop count for triangular solves (one RHS):**\nForward substitution with a unit lower-triangular matrix $L$ computes $y_i = b_i - \\sum_{j=1}^{i-1} L_{ij} y_j$. For each row $i$, this costs $2(i-1)$ flops. The total is:\n$$ F_{\\mathrm{fwd}}(n) = \\sum_{i=1}^{n} 2(i-1) = 2 \\frac{(n-1)n}{2} = n(n-1) $$\nBack substitution with an upper-triangular matrix $U$ computes $x_i = (y_i - \\sum_{j=i+1}^{n} U_{ij} x_j) / U_{ii}$. For each row $i$, this costs $2(n-i)$ flops for the summation and 1 flop for the division. The total is:\n$$ F_{\\mathrm{bwd}}(n) = \\sum_{i=1}^{n} (2(n-i) + 1) = 2 \\sum_{j=0}^{n-1} j + \\sum_{i=1}^{n} 1 = 2 \\frac{(n-1)n}{2} + n = n(n-1) + n = n^2 $$\nThe total flop count per RHS is:\n$$ F_{\\mathrm{solve,per\\_RHS}}(n) = F_{\\mathrm{fwd}}(n) + F_{\\mathrm{bwd}}(n) = n(n-1) + n^2 = 2n^2 - n $$\n\n**Total flop counts for the two strategies:**\n- Factor reuse across $m$ RHS:\n$$ F_{\\mathrm{reuse}}(n,m) = F_{\\mathrm{LU}}(n) + m F_{\\mathrm{solve,per\\_RHS}}(n) = \\frac{n(n-1)(4n+1)}{6} + m(2n^2 - n) $$\n- Repeated factorization for each RHS:\n$$ F_{\\mathrm{repeat}}(n,m) = m \\left( F_{\\mathrm{LU}}(n) + F_{\\mathrm{solve,per\\_RHS}}(n) \\right) = m \\left( \\frac{n(n-1)(4n+1)}{6} + 2n^2 - n \\right) $$\n\n**Memory traffic under the stated model:**\n- For LU factorization: $n^2$ words read (for $A$) + $n^2$ words written (for $LU$) = $2n^2$ words.\n- For solves (one RHS): $n$ words read (for $b$) + $n$ words written (for $x$) = $2n$ words.\n\n**Total memory traffic for the two strategies:**\n- Factor reuse: One factorization plus $m$ solves.\n$$ W_{\\mathrm{reuse}}(n,m) = (2n^2) + m(2n) = 2n(n+m) $$\n- Repeated factorization: $m$ independent factorization-and-solve procedures.\n$$ W_{\\mathrm{repeat}}(n,m) = m (2n^2 + 2n) = 2mn(n+1) $$\n\n**Arithmetic Intensity (AI) for each strategy:**\n$$ \\mathrm{AI}_{\\mathrm{reuse}}(n,m) = \\frac{F_{\\mathrm{reuse}}(n,m)}{W_{\\mathrm{reuse}}(n,m)} = \\frac{\\frac{n(n-1)(4n+1)}{6} + m(2n^2 - n)}{2n(n+m)} $$\n$$ \\mathrm{AI}_{\\mathrm{repeat}}(n,m) = \\frac{F_{\\mathrm{repeat}}(n,m)}{W_{\\mathrm{repeat}}(n,m)} = \\frac{m \\left( \\frac{n(n-1)(4n+1)}{6} + 2n^2 - n \\right)}{2mn(n+1)} = \\frac{\\frac{n(n-1)(4n+1)}{6} + 2n^2 - n}{2n(n+1)} $$\n\n**Improvement factor $\\Phi(n,m)$:**\n$$ \\Phi(n,m) = \\frac{\\mathrm{AI}_{\\mathrm{reuse}}(n,m)}{\\mathrm{AI}_{\\mathrm{repeat}}(n,m)} = \\frac{\\frac{\\frac{n(n-1)(4n+1)}{6} + m(2n^2 - n)}{2n(n+m)}}{\\frac{\\frac{n(n-1)(4n+1)}{6} + 2n^2 - n}{2n(n+1)}} $$\nSimplifying the expression by canceling terms and rearranging:\n$$ \\Phi(n,m) = \\frac{\\frac{n(n-1)(4n+1)}{6} + m(2n^2 - n)}{n+m} \\cdot \\frac{n+1}{\\frac{n(n-1)(4n+1)}{6} + 2n^2 - n} $$\nThis gives the final expression.",
            "answer": "$$\\boxed{\\frac{n+1}{n+m}\\cdot\\frac{\\frac{(n-1)n(4n+1)}{6}+m\\left(2n^{2}-n\\right)}{\\frac{(n-1)n(4n+1)}{6}+2n^{2}-n}}$$"
        },
        {
            "introduction": "An algorithm that works in theory can behave unexpectedly on a real computer due to the limitations of floating-point arithmetic. This advanced exercise confronts this reality by examining how tiny perturbations in a matrix can lead to different pivot choices in LU factorization, causing non-reproducible results across different machines or runs. You will design and test a deterministic pivoting strategy to enforce reproducibility without sacrificing the numerical stability that partial pivoting provides, a crucial skill in developing robust numerical software .",
            "id": "3578078",
            "problem": "Consider the linear system $A x = b$ over the real numbers, where $A \\in \\mathbb{R}^{n \\times n}$ is nonsingular and $b \\in \\mathbb{R}^{n}$. The goal is to solve $A x = b$ using Lower-Upper (LU) factorization with partial pivoting, while analyzing how tiny perturbations in $A$ can cause sensitivity in pivot decisions and lead to nondeterminism across computing architectures. You must also design and implement a deterministic tie-breaking strategy for pivot selection that preserves reproducibility without harming numerical stability.\n\nFundamental base. The factorization with partial pivoting computes a permutation matrix $P \\in \\mathbb{R}^{n \\times n}$, a unit lower triangular matrix $L \\in \\mathbb{R}^{n \\times n}$, and an upper triangular matrix $U \\in \\mathbb{R}^{n \\times n}$ such that $P A = L U$. The standard partial pivoting rule at elimination step $k$ selects a pivot row index $p \\in \\{k, k+1, \\dots, n-1\\}$ that maximizes $|A_{p,k}|$ over the remaining rows. Numerical linear algebra analyses bound the growth factor and backward error under partial pivoting. However, when multiple candidate rows have $|A_{r,k}|$ values that are equal or nearly equal at machine precision, tiny perturbations to $A$ or differences in floating-point evaluation order can lead to different pivot decisions across architectures.\n\nTask. Implement two LU solvers using partial pivoting:\n- A baseline solver following the standard arg-maximum pivot rule, with no explicit tie-handling beyond ordinary comparisons.\n- A reproducible solver that applies a deterministic tie-breaking strategy: when several candidate rows have $|A_{r,k}|$ values within a relative tolerance $\\tau$ of the column maximum at step $k$, select the pivot as the smallest global row index among those candidates. Use a tolerance $\\tau = 8 \\varepsilon_{\\text{mach}}$, where $\\varepsilon_{\\text{mach}}$ is the machine epsilon for double-precision arithmetic. Treat two magnitudes $x$ and $y$ as ties if $|x - y| \\le \\tau \\cdot \\max(|x|, |y|)$, and restrict selection to rows whose magnitude is within $\\tau$ of the column-wise maximum at the current step. The strategy must never select a pivot strictly smaller than the maximum outside this tolerance band.\n\nFor a set of prescribed test cases, quantify sensitivity and reproducibility by comparing pivot sequences and solution vectors under tiny perturbations of $A$. For each test case, define two perturbed matrices $A^{+} = A + \\delta M$ and $A^{-} = A - \\delta M$, where $\\delta > 0$ is tiny and $M$ is a fixed perturbation pattern. For each solver, compute the pivot sequence for $A^{+}$ and $A^{-}$, the solutions $x^{+}$ and $x^{-}$ to $A^{+} x^{+} = b$ and $A^{-} x^{-} = b$, and report the following quantities:\n1. The integer number of positions at which the pivot sequences differ between $A^{+}$ and $A^{-}$ for the baseline solver.\n2. The integer number of positions at which the pivot sequences differ between $A^{+}$ and $A^{-}$ for the reproducible solver.\n3. The $2$-norm $\\|x^{+} - x^{-}\\|_{2}$ for the baseline solver as a floating-point number.\n4. The $2$-norm $\\|x^{+} - x^{-}\\|_{2}$ for the reproducible solver as a floating-point number.\n\nTest suite. Use the following four test cases. In each case, $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$, $M \\in \\mathbb{R}^{n \\times n}$, and $\\delta$ is a tiny scalar. All matrices and vectors are dimensionally consistent and chosen to be nonsingular to ensure solvability.\n\n- Case 1 (happy path, initial-column tie): \n  $$A = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  1 & 0 & 1 \\\\\n  1 & -1 & 0\n  \\end{bmatrix}, \\quad \n  b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \n  M = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  -1 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix}, \\quad\n  \\delta = 10^{-16}.$$\n\n- Case 2 (mid-elimination tie): \n  $$A = \\begin{bmatrix}\n  10^{-12} & 1 & 0.5 \\\\\n  0 & 1 & 0.5 \\\\\n  0 & 1 & -0.5\n  \\end{bmatrix}, \\quad \n  b = \\begin{bmatrix} 10^{-12} \\\\ 1.5 \\\\ -0.5 \\end{bmatrix}, \\quad \n  M = \\begin{bmatrix}\n  0 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & -1 & 0\n  \\end{bmatrix}, \\quad\n  \\delta = 10^{-16}.$$\n\n- Case 3 (larger dimension, multiple near ties):\n  $$A = \\begin{bmatrix}\n  1 & 2 & 0 & 0 & 0 \\\\\n  1 & -2 & 3 & 0 & 0 \\\\\n  1 & 0 & -3 & 4 & 0 \\\\\n  1 & 0 & 0 & -4 & 5 \\\\\n  1 & 0 & 0 & 0 & -5\n  \\end{bmatrix}, \\quad \n  b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\quad \n  M = \\begin{bmatrix}\n  2 & 0 & 0 & 0 & 0 \\\\\n  -2 & 0 & 0 & 0 & 0 \\\\\n  1 & 0 & 0 & 0 & 0 \\\\\n  -1 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 0\n  \\end{bmatrix}, \\quad\n  \\delta = 10^{-16}.$$\n\n- Case 4 (edge case, near-singular first column):\n  $$A = \\begin{bmatrix}\n  10^{-15} & 1 & 0 \\\\\n  10^{-15} & 1 & 0 \\\\\n  2 \\cdot 10^{-15} & 1 & 1\n  \\end{bmatrix}, \\quad \n  b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad \n  M = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  -1 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix}, \\quad\n  \\delta = 5 \\cdot 10^{-16}.$$\n\nAngle units and physical units do not apply. All quantities are dimensionless.\n\nOutput specification. Your program should produce a single line of output containing the results aggregated across all four test cases as a comma-separated list enclosed in square brackets. For each test case, output the four quantities in the order listed above, so the final output contains $16$ values:\n$$[\\text{naive\\_diff}_1,\\text{deterministic\\_diff}_1,\\|x^{+} - x^{-}\\|_{2}^{\\text{naive}}_1,\\|x^{+} - x^{-}\\|_{2}^{\\text{det}}_1,\\dots,\\text{naive\\_diff}_4,\\text{deterministic\\_diff}_4,\\|x^{+} - x^{-}\\|_{2}^{\\text{naive}}_4,\\|x^{+} - x^{-}\\|_{2}^{\\text{det}}_4].$$\n\nYour implementation must solve the systems $A^{+} x^{+} = b$ and $A^{-} x^{-} = b$ using the corresponding LU factors with the pivot sequences generated by each solver, and must adhere to the deterministic tie-breaking rule for the reproducible solver as specified above. The final line must be printed exactly in the specified format.",
            "solution": "The present task is to solve the linear system $Ax = b$ using LU factorization with partial pivoting, and to analyze the numerical sensitivity of pivot selection under small perturbations. We are required to implement two distinct solvers: a baseline solver using the standard pivot selection rule and a reproducible solver employing a deterministic tie-breaking strategy. The performance of these solvers will be evaluated across a suite of test cases by comparing the stability of their pivot sequences and the resulting solutions when the matrix $A$ is slightly perturbed.\n\n### 1. LU Factorization with Partial Pivoting\n\nFor a nonsingular square matrix $A \\in \\mathbb{R}^{n \\times n}$, LU factorization with partial pivoting finds a permutation matrix $P$, a unit lower triangular matrix $L$, and an upper triangular matrix $U$ such that $PA = LU$. This factorization is a cornerstone of numerical linear algebra for solving systems of linear equations.\n\nThe algorithm proceeds iteratively for $k = 0, 1, \\dots, n-2$:\n\n1.  **Pivoting**: At step $k$, to ensure numerical stability and avoid division by a small number, we select a pivot row. The standard partial pivoting strategy involves searching for the element with the largest magnitude in the current column $k$ among rows from $k$ to $n-1$. Let this be row $p$, where $p \\in \\{k, k+1, \\dots, n-1\\}$ is the index that maximizes $|A_{i,k}|$ for $i \\ge k$. The rows $k$ and $p$ of the matrix $A$ are then swapped. This operation is recorded in the permutation matrix $P$. In practice, we maintain a permutation vector, say `piv`, which tracks the original row indices.\n\n2.  **Elimination**: After pivoting, the element $A_{k,k}$ is the pivot. For each row $i$ below the pivot ($i > k$), we compute the multiplier $L_{i,k} = A_{i,k} / A_{k,k}$. This multiplier is stored. The entries of row $i$ are then updated by subtracting the pivot row scaled by the multiplier: $A_{i,j} \\leftarrow A_{i,j} - L_{i,k} A_{k,j}$ for $j=k, \\dots, n-1$. After this, $A_{i,k}$ becomes zero. The multipliers form the matrix $L$, and the final state of the matrix $A$ becomes the matrix $U$. This is typically implemented in-place, with the multipliers stored in the lower-triangular part of the matrix $A$ and $U$ stored in the upper-triangular part.\n\nOnce the factorization $PA=LU$ is complete, the original system $Ax=b$ is transformed into $LUx = Pb$. This system is solved efficiently in two steps:\n1.  **Forward Substitution**: Solve the lower triangular system $Ly = Pb$ for the vector $y$.\n2.  **Backward Substitution**: Solve the upper triangular system $Ux = y$ for the final solution vector $x$.\n\n### 2. Solver Implementations\n\nThe core of the task is to implement two pivoting strategies.\n\n**a) Baseline Solver**\nThis solver adheres strictly to the standard partial pivoting rule. At each step $k$, it finds the index $p \\ge k$ that maximizes $|A_{p,k}|$. If multiple rows yield the same maximum magnitude (a \"tie\"), the choice of `argmax` is implementation-dependent (e.g., `numpy.argmax` returns the index of the first occurrence), making it susceptible to minor variations in floating-point representations across different architectures or even different calculations on the same architecture. This can lead to non-reproducible results.\n\n**b) Reproducible Solver**\nThis solver implements a deterministic tie-breaking rule. The goal is to make the pivot choice robust against tiny perturbations that could flip the result of a simple `argmax` comparison. The rule is as follows:\nAt step $k$, first find the maximum absolute value in the current pivot column, $M_k = \\max_{i \\ge k} |A_{i,k}|$. Then, identify a set of candidate rows $C_k$. A row $i$ is considered a candidate if its corresponding value $|A_{i,k}|$ is \"close\" to $M_k$. The condition for being close is defined by a relative tolerance $\\tau = 8 \\varepsilon_{\\text{mach}}$, where $\\varepsilon_{\\text{mach}}$ is the machine epsilon for double-precision floating-point arithmetic.\nA row index $i \\ge k$ belongs to the candidate set $C_k$ if:\n$$||A_{i,k}| - M_k| \\le \\tau \\cdot \\max(|A_{i,k}|, M_k)$$\nSince $M_k$ is the maximum, this simplifies to:\n$$||A_{i,k}| - M_k| \\le \\tau \\cdot M_k$$\nFrom the set of candidate rows $C_k$, the rule requires selecting the one with the **smallest row index**. This ensures that even if multiple rows have values that are numerically indistinguishable within the tolerance, the choice is always the same, thus making the pivot sequence deterministic and reproducible.\n\n### 3. Analysis Methodology\n\nFor each test case, we are given a matrix $A$, a vector $b$, a perturbation matrix $M$, and a small scalar $\\delta$. We construct two slightly different matrices:\n- $A^{+} = A + \\delta M$\n- $A^{-} = A - \\delta M$\n\nFor both the baseline and the reproducible solvers, we perform the following steps:\n1.  Compute the LU factorization of $A^{+}$ and $A^{-}$, recording the sequence of pivot row indices chosen at each step of the elimination.\n2.  Solve the systems $A^{+} x^{+} = b$ and $A^{-} x^{-} = b$ using the respective factorizations.\n3.  Calculate the required metrics:\n    - The number of positions where the pivot sequences for $A^{+}$ and $A^{-}$ differ.\n    - The Euclidean norm ($2$-norm) of the difference between the solutions, $\\|x^{+} - x^{-}\\|_2$.\n\nThis procedure allows us to quantify the impact of the perturbation on both the internal algorithmic choices (pivoting) and the final output (the solution vector) for each solver strategy. The expectation is that the reproducible solver will exhibit fewer or no differences in its pivot sequence compared to the baseline solver, leading to more stable and predictable behavior.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef lu_decomposition(A_in, reproducible):\n    \"\"\"\n    Performs LU factorization with partial pivoting (PA=LU).\n\n    Args:\n        A_in (np.ndarray): The input matrix.\n        reproducible (bool): If True, use the deterministic tie-breaking rule.\n\n    Returns:\n        tuple: A tuple containing:\n            - LU (np.ndarray): Matrix containing L (without unit diagonal) and U.\n            - p (np.ndarray): The permutation vector.\n            - pivot_sequence (list): Sequence of absolute pivot row indices chosen.\n    \"\"\"\n    n = A_in.shape[0]\n    LU = A_in.copy()\n    p = np.arange(n)\n    pivot_sequence = []\n\n    # Machine epsilon for double precision and tolerance\n    eps = np.finfo(float).eps\n    tau = 8 * eps\n\n    for k in range(n - 1):\n        # --- PIVOTING for column k ---\n        # We are looking for a pivot in rows k to n-1\n        sub_column = LU[k:, k]\n        abs_sub_column = np.abs(sub_column)\n        \n        max_val = np.max(abs_sub_column)\n\n        if max_val == 0.0:\n            # Singular matrix, but problem statement guarantees solvability.\n            # Continue without pivoting for this column.\n            pivot_row_abs = k\n        elif reproducible:\n            # Find indices (relative to sub-column) of candidates\n            is_candidate = np.abs(abs_sub_column - max_val) <= tau * max_val\n            candidate_indices_rel = np.where(is_candidate)[0]\n            \n            # Convert to absolute row indices in the current matrix state\n            candidate_indices_abs = k + candidate_indices_rel\n            \n            # Choose the smallest absolute row index among candidates\n            pivot_row_abs = np.min(candidate_indices_abs)\n        else:  # Baseline solver\n            # Simple argmax, gets the first max in case of a tie\n            pivot_row_rel = np.argmax(abs_sub_column)\n            pivot_row_abs = k + pivot_row_rel\n        \n        pivot_sequence.append(pivot_row_abs)\n        \n        # Swap rows k and pivot_row_abs\n        if pivot_row_abs != k:\n            LU[[k, pivot_row_abs], :] = LU[[pivot_row_abs, k], :]\n            p[[k, pivot_row_abs]] = p[[pivot_row_abs, k]]\n\n        # --- ELIMINATION ---\n        pivot_element = LU[k, k]\n        if pivot_element != 0:\n            # Compute multipliers and store them in the lower part of LU\n            LU[k+1:n, k] /= pivot_element\n            # Update the trailing submatrix\n            LU[k+1:n, k+1:n] -= np.outer(LU[k+1:n, k], LU[k, k+1:n])\n            \n    return LU, p, pivot_sequence\n\ndef solve_lu(LU, p, b):\n    \"\"\"\n    Solves the system Ax=b given the LU factorization and permutation vector.\n\n    Args:\n        LU (np.ndarray): The combined LU matrix.\n        p (np.ndarray): The permutation vector.\n        b (np.ndarray): The right-hand side vector.\n\n    Returns:\n        np.ndarray: The solution vector x.\n    \"\"\"\n    n = LU.shape[0]\n    # Permute b to get Pb\n    Pb = b[p]\n    \n    # Extract L and U matrices. scipy can handle this if we give it the right flags.\n    # Solve Ly = Pb using forward substitution\n    y = scipy.linalg.solve_triangular(LU, Pb, lower=True, unit_diagonal=True)\n    \n    # Solve Ux = y using backward substitution\n    x = scipy.linalg.solve_triangular(LU, y, lower=False, unit_diagonal=False)\n    \n    return x\n\ndef solve():\n    \"\"\"\n    Runs the full analysis across all test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[1, 1, 0], [1, 0, 1], [1, -1, 0]], dtype=float),\n            \"b\": np.array([1, 2, 3], dtype=float),\n            \"M\": np.array([[1, 0, 0], [-1, 0, 0], [0, 0, 0]], dtype=float),\n            \"delta\": 1e-16\n        },\n        {\n            \"A\": np.array([[1e-12, 1, 0.5], [0, 1, 0.5], [0, 1, -0.5]], dtype=float),\n            \"b\": np.array([1e-12, 1.5, -0.5], dtype=float),\n            \"M\": np.array([[0, 0, 0], [0, 1, 0], [0, -1, 0]], dtype=float),\n            \"delta\": 1e-16\n        },\n        {\n            \"A\": np.array([[1, 2, 0, 0, 0], [1, -2, 3, 0, 0], [1, 0, -3, 4, 0],\n                           [1, 0, 0, -4, 5], [1, 0, 0, 0, -5]], dtype=float),\n            \"b\": np.array([1, 2, 3, 4, 5], dtype=float),\n            \"M\": np.array([[2, 0, 0, 0, 0], [-2, 0, 0, 0, 0], [1, 0, 0, 0, 0],\n                           [-1, 0, 0, 0, 0], [0, 0, 0, 0, 0]], dtype=float),\n            \"delta\": 1e-16\n        },\n        {\n            \"A\": np.array([[1e-15, 1, 0], [1e-15, 1, 0], [2e-15, 1, 1]], dtype=float),\n            \"b\": np.array([1, 1, 2], dtype=float),\n            \"M\": np.array([[1, 0, 0], [-1, 0, 0], [0, 0, 0]], dtype=float),\n            \"delta\": 5e-16\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, b, M, delta = case[\"A\"], case[\"b\"], case[\"M\"], case[\"delta\"]\n\n        A_plus = A + delta * M\n        A_minus = A - delta * M\n\n        # --- Baseline Solver ---\n        LU_plus_base, p_plus_base, piv_seq_plus_base = lu_decomposition(A_plus, reproducible=False)\n        LU_minus_base, p_minus_base, piv_seq_minus_base = lu_decomposition(A_minus, reproducible=False)\n        \n        x_plus_base = solve_lu(LU_plus_base, p_plus_base, b)\n        x_minus_base = solve_lu(LU_minus_base, p_minus_base, b)\n        \n        naive_diff = np.sum(np.array(piv_seq_plus_base) != np.array(piv_seq_minus_base))\n        norm_diff_base = np.linalg.norm(x_plus_base - x_minus_base)\n\n        # --- Reproducible Solver ---\n        LU_plus_rep, p_plus_rep, piv_seq_plus_rep = lu_decomposition(A_plus, reproducible=True)\n        LU_minus_rep, p_minus_rep, piv_seq_minus_rep = lu_decomposition(A_minus, reproducible=True)\n\n        x_plus_rep = solve_lu(LU_plus_rep, p_plus_rep, b)\n        x_minus_rep = solve_lu(LU_minus_rep, p_minus_rep, b)\n\n        deterministic_diff = np.sum(np.array(piv_seq_plus_rep) != np.array(piv_seq_minus_rep))\n        norm_diff_rep = np.linalg.norm(x_plus_rep - x_minus_rep)\n\n        results.extend([naive_diff, deterministic_diff, norm_diff_base, norm_diff_rep])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}