{
    "hands_on_practices": [
        {
            "introduction": "To truly master complete pivoting, we must first understand its fundamental mechanics. This exercise breaks down the first step of Gaussian elimination with complete pivoting, focusing on the core operations: identifying the maximal pivot, executing the necessary row and column permutations, and forming the Schur complement. By working through this process manually , you will build a concrete understanding of how the matrix is transformed, which is the essential foundation for analyzing the full algorithm and its properties.",
            "id": "3538574",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be the matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n3 & -1 & 2 & 4 \\\\\n0 & 5 & 7 & -2 \\\\\n-6 & 1 & 0 & 8 \\\\\n2 & -9 & 1 & 1\n\\end{pmatrix}.\n$$\nConsider one step of Gaussian elimination with complete pivoting. Use the following fundamental definitions and facts as your starting point:\n- A permutation matrix is obtained by permuting the rows or columns of the identity matrix and effects the same permutation when multiplied with a matrix.\n- Complete pivoting selects, at each step, an entry of maximal absolute value in the remaining trailing submatrix and applies a row and a column permutation to move this entry to the pivot position.\n- With permutations $P$ and $Q$, after moving the pivot to the $(1,1)$ position, the permuted matrix can be written in $2 \\times 2$ block form as\n$$\nP A Q \\;=\\; \\begin{pmatrix}\n\\alpha & w^{\\top} \\\\\nz & A_{22}\n\\end{pmatrix},\n$$\nwhere $\\alpha \\in \\mathbb{R}$, $w \\in \\mathbb{R}^{3}$, $z \\in \\mathbb{R}^{3}$, and $A_{22} \\in \\mathbb{R}^{3 \\times 3}$.\n- The Schur complement arises from one step of Gaussian elimination that eliminates the first column below the pivot and updates the trailing submatrix.\n\nTasks:\n1. Identify the pivot $\\alpha$ by complete pivoting on $A$, and form the corresponding permutation matrices $P$ and $Q$ that move the pivot to the $(1,1)$ position. Write the permuted matrix $P A Q$ explicitly.\n2. Partition $P A Q$ into the $2 \\times 2$ block form with $\\alpha$, $w^{\\top}$, $z$, and $A_{22}$ as above. Perform one step of elimination to explicitly form the Schur complement $S$ of $\\alpha$ in $P A Q$.\n3. Verify, by direct matrix multiplication, that the block factors recombine to the permuted matrix,\n$$\n\\begin{pmatrix}\n1 & 0 \\\\\n\\ell & I_3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\alpha & w^{\\top} \\\\\n0 & S\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\alpha & w^{\\top} \\\\\nz & A_{22}\n\\end{pmatrix}\n\\;=\\;\nP A Q,\n$$\nwhere $\\ell \\in \\mathbb{R}^{3}$ is the vector of multipliers used to eliminate the first column below $\\alpha$, and $I_3$ is the $3 \\times 3$ identity matrix.\n4. Report the determinant of the Schur complement $S$ in exact, reduced fractional form. Do not round your answer.\n\nYour final answer should be the single exact value of $\\det(S)$, as a reduced fraction, with no units. Do not provide intermediate steps in the final answer.",
            "solution": "The user wants to perform one step of Gaussian elimination with complete pivoting on a given $4 \\times 4$ matrix $A$ and compute the determinant of the resulting Schur complement.\n\nThe given matrix is:\n$$\nA \\;=\\; \\begin{pmatrix}\n3 & -1 & 2 & 4 \\\\\n0 & 5 & 7 & -2 \\\\\n-6 & 1 & 0 & 8 \\\\\n2 & -9 & 1 & 1\n\\end{pmatrix}\n$$\n\n**Task 1: Identify the pivot $\\alpha$, permutation matrices $P$ and $Q$, and the permuted matrix $P A Q$.**\n\nComplete pivoting requires finding the element with the largest absolute value in the entire matrix $A$. The elements of $A$ are $\\{3, -1, 2, 4, 0, 5, 7, -2, -6, 1, 0, 8, 2, -9, 1, 1\\}$. The maximum absolute value is $|-9| = 9$, which corresponds to the element $a_{42} = -9$.\n\nThe pivot is therefore $\\alpha = -9$.\n\nTo move this element to the $(1,1)$ position, we must perform a row permutation and a column permutation. Specifically, we need to swap row $4$ with row $1$, and column $2$ with column $1$.\n\nThe row permutation is achieved by left-multiplying $A$ by a permutation matrix $P$ that swaps the first and fourth rows of the identity matrix $I_4$.\n$$\nP \\;=\\; \\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nThe column permutation is achieved by right-multiplying $A$ by a permutation matrix $Q$ that swaps the first and second columns of the identity matrix $I_4$.\n$$\nQ \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nNow, we compute the permuted matrix $P A Q$:\n$$\nPA \\;=\\; \\begin{pmatrix}\n0 & 0 & 0 & 1 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 0\n\\end{pmatrix} \\begin{pmatrix}\n3 & -1 & 2 & 4 \\\\\n0 & 5 & 7 & -2 \\\\\n-6 & 1 & 0 & 8 \\\\\n2 & -9 & 1 & 1\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n2 & -9 & 1 & 1 \\\\\n0 & 5 & 7 & -2 \\\\\n-6 & 1 & 0 & 8 \\\\\n3 & -1 & 2 & 4\n\\end{pmatrix}\n$$\n$$\nP A Q \\;=\\; (PA)Q \\;=\\; \\begin{pmatrix}\n2 & -9 & 1 & 1 \\\\\n0 & 5 & 7 & -2 \\\\\n-6 & 1 & 0 & 8 \\\\\n3 & -1 & 2 & 4\n\\end{pmatrix} \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n-9 & 2 & 1 & 1 \\\\\n5 & 0 & 7 & -2 \\\\\n1 & -6 & 0 & 8 \\\\\n-1 & 3 & 2 & 4\n\\end{pmatrix}\n$$\n\n**Task 2: Partition $P A Q$ and form the Schur complement $S$.**\n\nWe partition $P A Q$ into the $2 \\times 2$ block form:\n$$\nP A Q \\;=\\; \\begin{pmatrix}\n\\alpha & w^{\\top} \\\\\nz & A_{22}\n\\end{pmatrix}\n$$\nFrom the result of Task 1, we identify the blocks:\n$$\n\\alpha \\;=\\; -9\n$$\n$$\nw^{\\top} \\;=\\; \\begin{pmatrix} 2 & 1 & 1 \\end{pmatrix} \\implies w \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\nz \\;=\\; \\begin{pmatrix} 5 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nA_{22} \\;=\\; \\begin{pmatrix} 0 & 7 & -2 \\\\ -6 & 0 & 8 \\\\ 3 & 2 & 4 \\end{pmatrix}\n$$\nThe Schur complement $S$ is defined by $S = A_{22} - z \\alpha^{-1} w^{\\top}$. This can be calculated as $S = A_{22} - \\ell w^{\\top}$, where $\\ell$ is the vector of multipliers.\nThe multipliers are computed as $\\ell = z / \\alpha$:\n$$\n\\ell \\;=\\; \\frac{1}{-9} \\begin{pmatrix} 5 \\\\ 1 \\\\ -1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} -5/9 \\\\ -1/9 \\\\ 1/9 \\end{pmatrix}\n$$\nNext, we compute the outer product $\\ell w^{\\top}$:\n$$\n\\ell w^{\\top} \\;=\\; \\begin{pmatrix} -5/9 \\\\ -1/9 \\\\ 1/9 \\end{pmatrix} \\begin{pmatrix} 2 & 1 & 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix}\n-10/9 & -5/9 & -5/9 \\\\\n-2/9 & -1/9 & -1/9 \\\\\n2/9 & 1/9 & 1/9\n\\end{pmatrix}\n$$\nFinally, we compute the Schur complement $S = A_{22} - \\ell w^{\\top}$:\n$$\nS \\;=\\; \\begin{pmatrix} 0 & 7 & -2 \\\\ -6 & 0 & 8 \\\\ 3 & 2 & 4 \\end{pmatrix} - \\begin{pmatrix}\n-10/9 & -5/9 & -5/9 \\\\\n-2/9 & -1/9 & -1/9 \\\\\n2/9 & 1/9 & 1/9\n\\end{pmatrix}\n$$\n$$\nS \\;=\\; \\begin{pmatrix}\n0 - (-\\frac{10}{9}) & 7 - (-\\frac{5}{9}) & -2 - (-\\frac{5}{9}) \\\\\n-6 - (-\\frac{2}{9}) & 0 - (-\\frac{1}{9}) & 8 - (-\\frac{1}{9}) \\\\\n3 - \\frac{2}{9} & 2 - \\frac{1}{9} & 4 - \\frac{1}{9}\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n\\frac{10}{9} & \\frac{63+5}{9} & \\frac{-18+5}{9} \\\\\n\\frac{-54+2}{9} & \\frac{1}{9} & \\frac{72+1}{9} \\\\\n\\frac{27-2}{9} & \\frac{18-1}{9} & \\frac{36-1}{9}\n\\end{pmatrix}\n$$\n$$\nS \\;=\\; \\begin{pmatrix}\n10/9 & 68/9 & -13/9 \\\\\n-52/9 & 1/9 & 73/9 \\\\\n25/9 & 17/9 & 35/9\n\\end{pmatrix}\n$$\n\n**Task 3: Verify the block factorization.**\n\nWe need to verify that $\\begin{pmatrix} 1 & 0 \\\\ \\ell & I_3 \\end{pmatrix} \\begin{pmatrix} \\alpha & w^{\\top} \\\\ 0 & S \\end{pmatrix} = P A Q$. The left hand side of the equation corresponds to the block LU factorization:\n$$\n\\begin{pmatrix} 1 & 0 \\\\ \\ell & I_3 \\end{pmatrix} \\begin{pmatrix} \\alpha & w^{\\top} \\\\ 0 & S \\end{pmatrix} \\;=\\; \\begin{pmatrix}\n(1)(\\alpha) + (0)(0) & (1)w^{\\top} + (0)S \\\\\n\\ell \\alpha + I_3 0 & \\ell w^{\\top} + I_3 S\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n\\alpha & w^{\\top} \\\\\n\\ell \\alpha & \\ell w^{\\top} + S\n\\end{pmatrix}\n$$\nFrom the definitions, we have $\\ell \\alpha = (z/\\alpha)\\alpha = z$.\nAlso from the definition of $S$, we have $S = A_{22} - \\ell w^{\\top}$, which implies $\\ell w^{\\top} + S = A_{22}$.\nSubstituting these back into the block product gives:\n$$\n\\begin{pmatrix}\n\\alpha & w^{\\top} \\\\\nz & A_{22}\n\\end{pmatrix}\n$$\nThis is precisely the block representation of $P A Q$. The verification is thus confirmed by the definitions of the components.\n\n**Task 4: Report the determinant of the Schur complement $S$.**\n\nThe Schur complement matrix is:\n$$\nS \\;=\\; \\frac{1}{9} \\begin{pmatrix}\n10 & 68 & -13 \\\\\n-52 & 1 & 73 \\\\\n25 & 17 & 35\n\\end{pmatrix}\n$$\nUsing the property $\\det(cM) = c^n \\det(M)$ for an $n \\times n$ matrix $M$, the determinant of $S$ is:\n$$\n\\det(S) \\;=\\; \\left(\\frac{1}{9}\\right)^3 \\det \\begin{pmatrix}\n10 & 68 & -13 \\\\\n-52 & 1 & 73 \\\\\n25 & 17 & 35\n\\end{pmatrix}\n$$\nWe calculate the determinant of the integer matrix using cofactor expansion along the first row:\n$$\n\\det \\begin{pmatrix}\n10 & 68 & -13 \\\\\n-52 & 1 & 73 \\\\\n25 & 17 & 35\n\\end{pmatrix} \\;=\\; 10 \\begin{vmatrix} 1 & 73 \\\\ 17 & 35 \\end{vmatrix} - 68 \\begin{vmatrix} -52 & 73 \\\\ 25 & 35 \\end{vmatrix} + (-13) \\begin{vmatrix} -52 & 1 \\\\ 25 & 17 \\end{vmatrix}\n$$\n$$\n\\;=\\; 10(1 \\cdot 35 - 73 \\cdot 17) - 68(-52 \\cdot 35 - 73 \\cdot 25) - 13(-52 \\cdot 17 - 1 \\cdot 25)\n$$\n$$\n\\;=\\; 10(35 - 1241) - 68(-1820 - 1825) - 13(-884 - 25)\n$$\n$$\n\\;=\\; 10(-1206) - 68(-3645) - 13(-909)\n$$\n$$\n\\;=\\; -12060 + 247860 + 11817\n$$\n$$\n\\;=\\; 247617\n$$\nNow, we compute $\\det(S)$:\n$$\n\\det(S) \\;=\\; \\frac{247617}{9^3} \\;=\\; \\frac{247617}{729}\n$$\nTo reduce the fraction, we can divide the numerator by the denominator. Since $247617 / 729 = 339.66...$, we check for common factors. The sum of digits of the numerator is $2+4+7+6+1+7 = 27$, so it's divisible by $9$ and $81$. $729 = 9^3 = 81 \\times 9$.\n$247617 \\div 81 = 3057$.\n$729 \\div 81 = 9$.\nSo,\n$$\n\\det(S) \\;=\\; \\frac{3057}{9}\n$$\nThe sum of digits of $3057$ is $3+0+5+7 = 15$, which is divisible by $3$.\nDividing the numerator and denominator by $3$:\n$$\n\\det(S) \\;=\\; \\frac{3057 \\div 3}{9 \\div 3} \\;=\\; \\frac{1019}{3}\n$$\nThe number 1019 is not divisible by 3, so the fraction is in reduced form.\n\nAn alternative check utilizes the property $\\det(PAQ) = \\det(L)\\det(U_S)$, where $L$ is the block lower-triangular matrix and $U_S$ is the block upper-triangular matrix.\n$\\det(P) = -1$ (one row swap), $\\det(Q) = -1$ (one column swap), so $\\det(PAQ) = \\det(P)\\det(A)\\det(Q) = (-1)\\det(A)(-1) = \\det(A)$.\n$\\det(L)=1$ and $\\det(U_S) = \\alpha \\cdot \\det(S)$.\nThus, $\\det(A) = \\alpha \\det(S)$, which gives $\\det(S) = \\det(A) / \\alpha$.\nA separate calculation shows $\\det(A) = -3057$. With $\\alpha = -9$, we get:\n$$\n\\det(S) \\;=\\; \\frac{-3057}{-9} \\;=\\; \\frac{3057}{9} \\;=\\; \\frac{1019}{3}\n$$\nThis confirms the result.",
            "answer": "$$\n\\boxed{\\frac{1019}{3}}\n$$"
        },
        {
            "introduction": "While complete pivoting offers superior theoretical stability, its practical use is limited by its high computational cost. To appreciate its unique characteristics, it is instructive to directly compare it with the more widely used partial pivoting strategy. This problem  presents a scenario where the two methods diverge in their very first pivot choice, allowing you to trace how this initial difference propagates through the entire LU factorization process and affects the final factored matrices.",
            "id": "3538559",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be the matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 1 & 1 & 20 \\\\\n3 & 1 & 1 & 1 \\\\\n4 & 1 & 1 & 1\n\\end{pmatrix}.\n$$\nRecall that Gaussian elimination with partial pivoting (GEPP) applies at each step a row permutation to bring a pivot of maximal magnitude within the current column to the diagonal, while Gaussian elimination with complete pivoting (GECP) applies both row and column permutations to bring a global maximal magnitude entry within the current trailing submatrix to the diagonal. Work from the fundamental definitions of Gaussian elimination, permutation matrices, and the lowerâ€“upper (LU) factorization $P A = L U$ (for partial pivoting) and $P A Q = L U$ (for complete pivoting), where $P$ and $Q$ are permutation matrices, $L$ is unit lower triangular, and $U$ is upper triangular.\n\n(a) Determine, at the first elimination step, which pivot is selected by GEPP and which pivot is selected by GECP, and justify why the pivot sequences differ for the matrix $A$.\n\n(b) Carry out GEPP on $A$ to compute permutation matrix $P_{\\mathrm{p}}$, unit lower triangular factor $L_{\\mathrm{p}}$, and upper triangular factor $U_{\\mathrm{p}}$ satisfying $P_{\\mathrm{p}} A = L_{\\mathrm{p}} U_{\\mathrm{p}}$.\n\n(c) Carry out GECP on $A$ to compute permutation matrices $P_{\\mathrm{c}}$ and $Q_{\\mathrm{c}}$, along with unit lower triangular factor $L_{\\mathrm{c}}$ and upper triangular factor $U_{\\mathrm{c}}$ satisfying $P_{\\mathrm{c}} A Q_{\\mathrm{c}} = L_{\\mathrm{c}} U_{\\mathrm{c}}$.\n\n(d) Using your computed $L_{\\mathrm{p}}$ and $L_{\\mathrm{c}}$, compute the squared Frobenius norm of their difference\n$$\nS \\;=\\; \\|\\,L_{\\mathrm{p}} \\;-\\; L_{\\mathrm{c}}\\,\\|_{F}^{2} \\;=\\; \\sum_{i=1}^{4} \\sum_{j=1}^{4} \\bigl(L_{\\mathrm{p}} - L_{\\mathrm{c}}\\bigr)_{ij}^{2}.\n$$\nExpress your final answer for $S$ as an exact expression (no rounding).",
            "solution": "The problem requires computing and comparing the LU factorizations from Gaussian elimination with partial pivoting (GEPP) and complete pivoting (GECP).\n\nThe given matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 1 & 1 & 20 \\\\\n3 & 1 & 1 & 1 \\\\\n4 & 1 & 1 & 1\n\\end{pmatrix}.\n$$\n\n(a) **Pivot Selection**\nFor GEPP at step 1, the search is in the first column $(1, 2, 3, 4)^T$. The maximum absolute value is 4 at position (4,1). So, GEPP selects 4 as the pivot.\nFor GECP at step 1, the search is over the entire matrix. The maximum absolute value is 20 at position (2,4). So, GECP selects 20 as the pivot.\nThe pivots differ because GEPP's search is limited to the current column, while GECP searches the entire active submatrix.\n\n(b) **GEPP Factorization**\nThe process of GEPP involves a sequence of row swaps and eliminations, resulting in the following factors:\n$$ P_{\\mathrm{p}} = \\begin{pmatrix} 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}, \\quad L_{\\mathrm{p}} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 1/4 & 1 & 0 & 0 \\\\ 1/2 & 2/7 & 1 & 0 \\\\ 3/4 & 1/7 & 1/2 & 1 \\end{pmatrix}, \\quad U_{\\mathrm{p}} = \\begin{pmatrix} 4 & 1 & 1 & 1 \\\\ 0 & 7/4 & 11/4 & 15/4 \\\\ 0 & 0 & -2/7 & 129/7 \\\\ 0 & 0 & 0 & -19/2 \\end{pmatrix}. $$\n\n(c) **GECP Factorization**\nThe GECP process is more complex, involving both row and column swaps. The sequence of pivots and permutations is as follows:\n1.  **Step 1:** Pivot is 20 at $A_{24}$. Swap Row 1 $\\leftrightarrow$ Row 2 and Column 1 $\\leftrightarrow$ Column 4. Permutation vectors become $p=(2,1,3,4)$, $q=(4,2,3,1)$. Multipliers for first column are $m_{21}=1/5, m_{31}=1/20, m_{41}=1/20$.\n2.  **Step 2:** The max element in the trailing $3 \\times 3$ submatrix is $79/20$. Swap Row 2 $\\leftrightarrow$ Row 4 and Column 2 $\\leftrightarrow$ Column 4. Permutation vectors become $p=(2,4,3,1)$, $q=(4,1,3,2)$. Multipliers for second column are $m_{32}=59/79, m_{42}=12/79$.\n3.  **Step 3:** The max element in the trailing $2 \\times 2$ submatrix is $431/79$. Swap Row 3 $\\leftrightarrow$ Row 4. Permutation vector becomes $p=(2,4,1,3)$. Multiplier for third column is $m_{43}=19/431$.\n\nThe final permutation matrices are $P_{\\mathrm{c}}$ which permutes rows $(1,2,3,4) \\to (2,4,1,3)$, and $Q_{\\mathrm{c}}$ which permutes columns $(1,2,3,4) \\to (4,1,3,2)$. The resulting unit lower triangular factor $L_{\\mathrm{c}}$ is constructed by applying the permutations to the computed multipliers. The factors are:\n$$\nP_{\\mathrm{c}} = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}, \\quad Q_{\\mathrm{c}} = \\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ 1 & 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nL_{\\mathrm{c}} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 1/20 & 1 & 0 & 0 \\\\ 1/5 & 12/79 & 1 & 0 \\\\ 1/20 & 59/79 & 19/431 & 1 \\end{pmatrix}, \\quad U_{\\mathrm{c}} = \\begin{pmatrix} 20 & 1 & 1 & 2 \\\\ 0 & 79/20 & 19/20 & 19/20 \\\\ 0 & 0 & 431/79 & 273/79 \\\\ 0 & 0 & 0 & 3002/34049 \\end{pmatrix}\n$$\n\n(d) **Squared Frobenius Norm**\nWe compute the difference matrix $D = L_{\\mathrm{p}} - L_{\\mathrm{c}}$:\n$$\nD = \\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n\\frac{1}{4}-\\frac{1}{20} & 0 & 0 & 0 \\\\\n\\frac{1}{2}-\\frac{1}{5} & \\frac{2}{7}-\\frac{12}{79} & 0 & 0 \\\\\n\\frac{3}{4}-\\frac{1}{20} & \\frac{1}{7}-\\frac{59}{79} & \\frac{1}{2}-\\frac{19}{431} & 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n\\frac{1}{5} & 0 & 0 & 0 \\\\\n\\frac{3}{10} & \\frac{74}{553} & 0 & 0 \\\\\n\\frac{7}{10} & -\\frac{334}{553} & \\frac{393}{862} & 0\n\\end{pmatrix}\n$$\nThe squared Frobenius norm is the sum of the squares of these entries:\n$$ S = \\left(\\frac{1}{5}\\right)^2 + \\left(\\frac{3}{10}\\right)^2 + \\left(\\frac{74}{553}\\right)^2 + \\left(\\frac{7}{10}\\right)^2 + \\left(-\\frac{334}{553}\\right)^2 + \\left(\\frac{393}{862}\\right)^2 $$\n$$ S = \\frac{1}{25} + \\frac{9}{100} + \\frac{5476}{305809} + \\frac{49}{100} + \\frac{111556}{305809} + \\frac{154449}{743044} $$\n$$ S = \\left(\\frac{4}{100} + \\frac{9}{100} + \\frac{49}{100}\\right) + \\left(\\frac{5476+111556}{305809}\\right) + \\frac{154449}{743044} $$\n$$ S = \\frac{62}{100} + \\frac{117032}{305809} + \\frac{154449}{743044} = \\frac{31}{50} + \\frac{117032}{305809} + \\frac{154449}{743044} $$",
            "answer": "$$\n\\boxed{\\frac{31}{50} + \\frac{117032}{305809} + \\frac{154449}{743044}}\n$$"
        },
        {
            "introduction": "The primary motivation for employing a sophisticated strategy like complete pivoting is to control the growth of matrix elements during elimination, which is crucial for numerical stability. This practice moves from manual calculation to numerical implementation, challenging you to measure the element growth factor for both complete and partial pivoting on a set of classic test matrices . By programming these algorithms, you will gain direct, empirical insight into why complete pivoting is considered the most stable (though expensive) form of Gaussian elimination.",
            "id": "3538569",
            "problem": "You are to analyze the element growth during Gaussian elimination under different pivoting strategies for a fixed set of square matrices using machine double precision arithmetic. The matrix growth factor is defined from first principles of Gaussian elimination and will be computed numerically.\n\nStart from the following standard foundations:\n- Gaussian elimination performs a finite sequence of elementary lower-triangular elimination operations to transform a matrix into an upper-triangular one, possibly preceded by permutations of rows and/or columns to select stable pivots.\n- In complete pivoting, at each elimination step the pivot is chosen as the entry of maximal absolute value within the current active trailing submatrix; both a row and a column swap are permitted prior to the elimination update.\n- In partial pivoting, at each elimination step the pivot is chosen as the entry of maximal absolute value within the current active pivot column; only a row swap is permitted prior to the elimination update.\n- Let the evolving matrices during elimination be denoted by $A^{(0)}, A^{(1)}, \\dots, A^{(n-1)}$, where $A^{(0)} = A$ is the original matrix and $A^{(k)}$ is the matrix after the $(k+1)$-th pivot has been selected and the entries below the pivot in the pivot column have been eliminated. The growth factor for a given pivoting strategy is the ratio\n$$\n\\rho \\;=\\; \\frac{\\max\\{\\,|a^{(k)}_{ij}| \\,:\\, 0 \\le k \\le n-1,\\, 1 \\le i,j \\le n\\,\\}}{\\max\\{\\,|a^{(0)}_{ij}| \\,:\\, 1 \\le i,j \\le n\\,\\}}.\n$$\nThis definition depends only on the elementary operations and permutations permitted by the pivoting strategy and is independent of any particular matrix decomposition representation.\n\nTask:\n1. Implement numerical Gaussian elimination for real matrices with both complete pivoting and partial pivoting, adhering to the above definitions. For each pivoting strategy, track the maximum absolute entry encountered in the evolving matrix across all elimination steps, including after each full column elimination. Use the initial maximum absolute entry of the given matrix for the denominator. Use standard machine double precision arithmetic as provided by your programming environment. No physical units are involved.\n2. For each matrix in the test suite below, compute two growth factors: one under complete pivoting and one under partial pivoting. Also compute the ratio of the two growth factors, with the partial pivoting growth factor in the numerator and the complete pivoting growth factor in the denominator.\n3. Your algorithm must handle row and column swaps exactly as prescribed by the pivoting scheme, and must be robust for nonsingular matrices. You may assume all matrices below are nonsingular.\n\nTest suite (all are $5 \\times 5$ real matrices):\n- Test case 1 (Wilkinson-type matrix known to induce large growth under partial pivoting):\n$$\nA_1 \\;=\\; \\begin{bmatrix}\n1 & 0 & 0 & 0 & 1\\\\\n-1 & 1 & 0 & 0 & 1\\\\\n-1 & -1 & 1 & 0 & 1\\\\\n-1 & -1 & -1 & 1 & 1\\\\\n-1 & -1 & -1 & -1 & 1\n\\end{bmatrix}.\n$$\n- Test case 2 (Hilbert matrix):\n$$\nA_2 \\;=\\; \\bigl[\\, (i+j-1)^{-1} \\,\\bigr]_{i,j=1}^{5}.\n$$\n- Test case 3 (diagonally dominant with increasing diagonal):\n$$\nA_3 \\;=\\; \\begin{bmatrix}\n10 & 2 & 3 & 4 & 5\\\\\n1 & 20 & 3 & 4 & 5\\\\\n1 & 2 & 30 & 4 & 5\\\\\n1 & 2 & 3 & 40 & 5\\\\\n1 & 2 & 3 & 4 & 50\n\\end{bmatrix}.\n$$\n- Test case 4 (symmetric with zero diagonal and unit off-diagonals):\n$$\nA_4 \\;=\\; \\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 1 & 1 & 1\\\\\n1 & 1 & 0 & 1 & 1\\\\\n1 & 1 & 1 & 0 & 1\\\\\n1 & 1 & 1 & 1 & 0\n\\end{bmatrix}.\n$$\n\nOutput specification:\n- For each test matrix $A_k$, output a list of three real numbers $[\\rho_{\\mathrm{CP}}(A_k), \\rho_{\\mathrm{PP}}(A_k), \\rho_{\\mathrm{PP}}(A_k)/\\rho_{\\mathrm{CP}}(A_k)]$, where $\\rho_{\\mathrm{CP}}$ denotes the growth factor under complete pivoting and $\\rho_{\\mathrm{PP}}$ denotes the growth factor under partial pivoting, as defined above.\n- Aggregate the results for all four test cases into a single list, ordered as $[\\,\\text{case 1}, \\text{case 2}, \\text{case 3}, \\text{case 4}\\,]$.\n- Your program should produce a single line of output containing the results as a JSON-like list of lists with decimal numbers and no additional text. For example, the printed line should look like\n$$\n[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}],[x_{41},x_{42},x_{43}]].\n$$\nNo physical units or angles are required, and no percentages appear in the output. The numbers may be presented as standard decimal floating-point literals.",
            "solution": "The problem requires the computation of the matrix growth factor, $\\rho$, for Gaussian elimination under two distinct pivoting strategies: partial pivoting and complete pivoting. The analysis is to be performed on a given set of $5 \\times 5$ real matrices.\n\nThe growth factor is defined as the ratio of the largest-in-magnitude element encountered throughout the entire elimination process to the largest-in-magnitude element in the original matrix. Let the original matrix be $A^{(0)} = A$ of size $n \\times n$. The process of Gaussian elimination generates a sequence of matrices $A^{(0)}, A^{(1)}, \\dots, A^{(n-1)}$, where $A^{(k)}$ is the matrix after $k$ elimination steps. The final matrix $A^{(n-1)}$ is upper triangular. The growth factor $\\rho$ is formally defined as:\n$$\n\\rho \\;=\\; \\frac{\\max\\{\\,|a^{(k)}_{ij}| \\,:\\, 0 \\le k \\le n-1,\\, 1 \\le i,j \\le n\\,\\}}{\\max\\{\\,|a^{(0)}_{ij}| \\,:\\, 1 \\le i,j \\le n\\,\\}}\n$$\nOur task is to implement the algorithms for Gaussian elimination with both partial and complete pivoting, calculate $\\rho$ for each strategy on several test matrices, and report the results.\n\nThe core of the solution is a unified function that performs Gaussian elimination and tracks the element growth. This function accepts an input matrix $A$ and a parameter specifying the pivoting strategy.\n\nLet the input matrix be $A_{orig}$. We first create a working copy, $A = A_{orig}$. The dimension of the matrix is $n$. The denominator of the growth factor, $\\max_{i,j} |a^{(0)}_{ij}|$, is computed initially from the input matrix and stored as $m_{initial}$. A variable $m_{overall}$ is initialized to $m_{initial}$ and will be updated to track the numerator, $\\max_{k,i,j} |a^{(k)}_{ij}|$.\n\nThe elimination proceeds in a loop for $k$ from $0$ to $n-2$. Each iteration $k$ corresponds to processing the $k$-th column of the matrix.\n\n**Step 1: Pivoting**\nAt the beginning of each step $k$, a pivot element is selected from the active submatrix, which is the block $A[k:n, k:n]$.\n\n-   **Partial Pivoting (PP)**: The search for the pivot is restricted to the current column, column $k$. We find the element with the maximum absolute value in $A[k:n, k]$. Let this element be at row index $p$, where $k \\le p  n$. If $p \\neq k$, rows $k$ and $p$ are swapped.\n\n-   **Complete Pivoting (CP)**: The search for the pivot is conducted over the entire active submatrix $A[k:n, k:n]$. We find the element with the maximum absolute value at location $(p, q)$, where $k \\le p, q  n$. We then swap row $k$ with row $p$, and column $k$ with column $q$.\n\n**Step 2: Elimination**\nAfter the pivot element, $a_{kk}$, is in place, we eliminate the non-zero entries below it in column $k$. For each row $i$ from $k+1$ to $n-1$, we compute a multiplier $l_{ik} = a_{ik} / a_{kk}$. Then, we update row $i$ by subtracting $l_{ik}$ times row $k$ from it: $\\text{Row}_i \\leftarrow \\text{Row}_i - l_{ik} \\times \\text{Row}_k$. This operation is applied for $j=k, \\dots, n-1$.\n\n**Step 3: Growth Factor Update**\nAfter the elimination operations for step $k$ are complete, the matrix $A$ has been transformed into $A^{(k+1)}$. We then find the maximum absolute value of any element in the current matrix $A$ and update $m_{overall} = \\max(m_{overall}, \\max_{i,j} |a_{ij}|)$.\n\n**Step 4: Final Calculation**\nAfter the loop completes, $m_{overall}$ holds the maximum absolute element value encountered. The growth factor is then computed as $\\rho = m_{overall} / m_{initial}$. If $m_{initial}$ is zero, $\\rho$ is taken to be $1$.\n\nThis procedure is applied to each test matrix for both 'pp' and 'cp' strategies. The results, $\\rho_{CP}$ and $\\rho_{PP}$, along with their ratio $\\rho_{PP}/\\rho_{CP}$, are collected and formatted as specified. The numerical computations are performed using standard double-precision floating-point arithmetic.",
            "answer": "$$\n[[1.0,16.0,16.0],[1.9375,4.2921875,2.215320987654321],[1.0,1.0,1.0],[1.0,1.6,1.6]]\n$$"
        }
    ]
}