{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's ground our understanding with a direct calculation. This first exercise  walks you through the essential mechanics of analyzing a stationary method by constructing the Jacobi iteration matrix for a specific, diagonally dominant system. By computing its spectral radius, you will directly verify the condition for convergence and gain a tangible sense of how this crucial value is determined.",
            "id": "3542428",
            "problem": "Consider the linear system $A x = b$ with \n$$\nA = \\begin{pmatrix}\n5  -1  0 \\\\\n-1  5  -1 \\\\\n0  -1  5\n\\end{pmatrix}.\n$$\nThe matrix $A$ is strictly diagonally dominant. Using the basic splitting of a matrix into its diagonal part $D$ and the remainder $R$, construct the Jacobi stationary iteration matrix for this system, compute its eigenvalues exactly, and determine the spectral radius. Use the following foundational facts only: the Jacobi stationary method is obtained by isolating the diagonal of $A$ and iterating with a fixed linear operator; convergence of a stationary method is governed by the modulus of the eigenvalues of the iteration matrix, called the spectral radius. Express your final answer as a single closed-form analytic expression for the spectral radius. No rounding is required.",
            "solution": "The problem as stated constitutes a valid and well-posed exercise in numerical linear algebra. It provides a specific matrix $A$, specifies the use of the Jacobi stationary method, and asks for the computation of the spectral radius of the corresponding iteration matrix. All data and definitions required for the solution are present and self-consistent. The problem adheres to established scientific principles and is amenable to a unique, exact solution.\n\nThe problem is to solve the linear system $A x = b$ using a stationary iterative method, where the matrix $A$ is given by:\n$$\nA = \\begin{pmatrix}\n5  -1  0 \\\\\n-1  5  -1 \\\\\n0  -1  5\n\\end{pmatrix}\n$$\nThe Jacobi method is defined by splitting the matrix $A$ into its diagonal part, $D$, its strictly lower triangular part, $-L$, and its strictly upper triangular part, $-U$. Thus, $A = D - L - U$. The iterative scheme is derived from $A x = b$ by rewriting it as $(D - L - U)x = b$, which is rearranged to $D x = (L+U)x + b$.\n\nFor a general iteration $k$, this gives the Jacobi iterative update rule:\n$$\nD x^{(k+1)} = (L+U)x^{(k)} + b\n$$\nMultiplying by the inverse of the diagonal matrix, $D^{-1}$, we obtain the explicit form of the iteration:\n$$\nx^{(k+1)} = D^{-1}(L+U)x^{(k)} + D^{-1}b\n$$\nThis is a stationary method of the form $x^{(k+1)} = T x^{(k)} + c$, where the iteration matrix is $T_J = D^{-1}(L+U)$ and the constant vector is $c = D^{-1}b$. The convergence of this method depends on the spectral radius of the iteration matrix $T_J$, denoted $\\rho(T_J)$, which is defined as the maximum absolute value of its eigenvalues. The method converges if and only if $\\rho(T_J)  1$.\n\nFirst, we decompose the given matrix $A$ into its components $D$, $L$, and $U$.\n$$\nD = \\begin{pmatrix}\n5  0  0 \\\\\n0  5  0 \\\\\n0  0  5\n\\end{pmatrix}\n$$\n$$\n-L = \\begin{pmatrix}\n0  0  0 \\\\\n-1  0  0 \\\\\n0  -1  0\n\\end{pmatrix} \\implies L = \\begin{pmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n$$\n$$\n-U = \\begin{pmatrix}\n0  -1  0 \\\\\n0  0  -1 \\\\\n0  0  0\n\\end{pmatrix} \\implies U = \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThe inverse of the diagonal matrix $D$ is:\n$$\nD^{-1} = \\begin{pmatrix}\n\\frac{1}{5}  0  0 \\\\\n0  \\frac{1}{5}  0 \\\\\n0  0  \\frac{1}{5}\n\\end{pmatrix} = \\frac{1}{5}I\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\nThe sum $L+U$ is:\n$$\nL+U = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}\n$$\nNow, we construct the Jacobi iteration matrix $T_J = D^{-1}(L+U)$:\n$$\nT_J = \\frac{1}{5} \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}\n= \\frac{1}{5} \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n0  \\frac{1}{5}  0 \\\\\n\\frac{1}{5}  0  \\frac{1}{5} \\\\\n0  \\frac{1}{5}  0\n\\end{pmatrix}\n$$\nTo find the spectral radius of $T_J$, we must compute its eigenvalues. The eigenvalues, $\\lambda$, are the roots of the characteristic equation $\\det(T_J - \\lambda I) = 0$.\n$$\n\\det(T_J - \\lambda I) = \\det \\begin{pmatrix}\n-\\lambda  \\frac{1}{5}  0 \\\\\n\\frac{1}{5}  -\\lambda  \\frac{1}{5} \\\\\n0  \\frac{1}{5}  -\\lambda\n\\end{pmatrix} = 0\n$$\nWe compute the determinant by cofactor expansion along the first row:\n$$\n-\\lambda \\left| \\begin{matrix} -\\lambda  \\frac{1}{5} \\\\ \\frac{1}{5}  -\\lambda \\end{matrix} \\right| - \\frac{1}{5} \\left| \\begin{matrix} \\frac{1}{5}  \\frac{1}{5} \\\\ 0  -\\lambda \\end{matrix} \\right| + 0 = 0\n$$\n$$\n-\\lambda \\left( (-\\lambda)(-\\lambda) - (\\frac{1}{5})(\\frac{1}{5}) \\right) - \\frac{1}{5} \\left( (\\frac{1}{5})(-\\lambda) - (0)(\\frac{1}{5}) \\right) = 0\n$$\n$$\n-\\lambda \\left( \\lambda^2 - \\frac{1}{25} \\right) - \\frac{1}{5} \\left( -\\frac{\\lambda}{5} \\right) = 0\n$$\n$$\n-\\lambda^3 + \\frac{\\lambda}{25} + \\frac{\\lambda}{25} = 0\n$$\n$$\n-\\lambda^3 + \\frac{2\\lambda}{25} = 0\n$$\n$$\n-\\lambda \\left( \\lambda^2 - \\frac{2}{25} \\right) = 0\n$$\nThis equation yields three eigenvalues for the matrix $T_J$:\n$$\n\\lambda_1 = 0\n$$\n$$\n\\lambda^2 = \\frac{2}{25} \\implies \\lambda_{2,3} = \\pm \\sqrt{\\frac{2}{25}} = \\pm \\frac{\\sqrt{2}}{5}\n$$\nThe eigenvalues are $\\lambda_1 = 0$, $\\lambda_2 = \\frac{\\sqrt{2}}{5}$, and $\\lambda_3 = -\\frac{\\sqrt{2}}{5}$.\n\nThe spectral radius, $\\rho(T_J)$, is the maximum of the absolute values (moduli) of these eigenvalues:\n$$\n\\rho(T_J) = \\max \\left\\{ |\\lambda_1|, |\\lambda_2|, |\\lambda_3| \\right\\}\n$$\n$$\n\\rho(T_J) = \\max \\left\\{ |0|, \\left|\\frac{\\sqrt{2}}{5}\\right|, \\left|-\\frac{\\sqrt{2}}{5}\\right| \\right\\}\n$$\n$$\n\\rho(T_J) = \\max \\left\\{ 0, \\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5} \\right\\}\n$$\nTherefore, the spectral radius of the Jacobi iteration matrix is $\\frac{\\sqrt{2}}{5}$. Since $\\rho(T_J) = \\frac{\\sqrt{2}}{5} \\approx \\frac{1.414}{5} \\approx 0.2828  1$, the Jacobi method converges for this system, which is expected as the matrix $A$ is strictly diagonally dominant.",
            "answer": "$$\\boxed{\\frac{\\sqrt{2}}{5}}$$"
        },
        {
            "introduction": "Building on the foundational calculation, we now move to a more general analysis. This practice  challenges you to derive the spectral radius for the Gauss-Seidel method on a symbolic $2 \\times 2$ symmetric positive definite matrix. Your goal is not just to find the answer, but to interpret it, revealing the explicit relationship between a matrix's diagonal dominance and coupling, and the resulting rate of convergence.",
            "id": "3542479",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be a Symmetric Positive Definite (SPD) matrix with entries\n$$\nA = \\begin{pmatrix}\na  c \\\\\nc  b\n\\end{pmatrix},\n$$\nwhere $a  0$, $b  0$, and $ab - c^{2}  0$. Consider the linear system $A x = b$ and the Gauss–Seidel stationary iterative method, understood as a fixed-point iteration that, within each sweep, uses the most recently updated components of the iterate to produce the next ones.\n\nStarting only from the definitions of stationary iterations and the structure of the Gauss–Seidel method, derive the explicit $2 \\times 2$ matrix for the Gauss–Seidel iteration operator $G_{GS}$ in terms of $a$, $b$, and $c$. Then compute its spectral radius in closed form as a function of $a$, $b$, and $c$. Finally, interpret how the entries $a$, $b$, and $c$ influence the rate of convergence through the spectral radius, explaining the roles of diagonal dominance and coupling.\n\nYour final reported answer must be the single analytic expression for the spectral radius of $G_{GS}$ in terms of $a$, $b$, and $c$. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to derive the Gauss–Seidel iteration matrix and its spectral radius for the given $2 \\times 2$ symmetric positive definite (SPD) matrix $A$. We proceed with the solution.\n\nA stationary iterative method for solving the linear system $A\\mathbf{x} = \\mathbf{b}$ is constructed from a splitting of the matrix $A$ into $A = M - N$, where $M$ is non-singular. The iterative scheme is defined by $M\\mathbf{x}^{(k+1)} = N\\mathbf{x}^{(k)} + \\mathbf{b}$, which can be written as $\\mathbf{x}^{(k+1)} = M^{-1}N\\mathbf{x}^{(k)} + M^{-1}\\mathbf{b}$. The matrix $G = M^{-1}N$ is called the iteration matrix.\n\nThe Gauss-Seidel method is a specific type of stationary iterative method. To define its iteration matrix, we first decompose the matrix $A$ into its diagonal, strictly lower triangular, and strictly upper triangular parts: $A = D + L + U$.\nFor the given matrix $A \\in \\mathbb{R}^{2 \\times 2}$,\n$$\nA = \\begin{pmatrix} a  c \\\\ c  b \\end{pmatrix}\n$$\nthe decomposition is:\n$$\nD = \\begin{pmatrix} a  0 \\\\ 0  b \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  c \\\\ 0  0 \\end{pmatrix}\n$$\nThe Gauss-Seidel method corresponds to the splitting where $M = D + L$ and $N = -U$. The iteration matrix, denoted as $G_{GS}$, is therefore given by $G_{GS} = (D+L)^{-1}(-U) = -(D+L)^{-1}U$.\n\nFirst, we compute the matrix $M = D+L$:\n$$\nM = D+L = \\begin{pmatrix} a  0 \\\\ 0  b \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix} = \\begin{pmatrix} a  0 \\\\ c  b \\end{pmatrix}\n$$\nNext, we find the inverse of $M$. The determinant of $M$ is $\\det(M) = (a)(b) - (0)(c) = ab$. The problem states that $a  0$ and $b  0$, so $\\det(M) = ab \\neq 0$, and thus $M$ is invertible. The inverse is:\n$$\nM^{-1} = (D+L)^{-1} = \\frac{1}{ab} \\begin{pmatrix} b  0 \\\\ -c  a \\end{pmatrix} = \\begin{pmatrix} \\frac{b}{ab}  0 \\\\ -\\frac{c}{ab}  \\frac{a}{ab} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{a}  0 \\\\ -\\frac{c}{ab}  \\frac{1}{b} \\end{pmatrix}\n$$\nNow, we can compute the Gauss-Seidel iteration matrix $G_{GS} = -M^{-1}U$:\n$$\nG_{GS} = - \\begin{pmatrix} \\frac{1}{a}  0 \\\\ -\\frac{c}{ab}  \\frac{1}{b} \\end{pmatrix} \\begin{pmatrix} 0  c \\\\ 0  0 \\end{pmatrix} = - \\begin{pmatrix} (\\frac{1}{a})(0) + (0)(0)  (\\frac{1}{a})(c) + (0)(0) \\\\ (-\\frac{c}{ab})(0) + (\\frac{1}{b})(0)  (-\\frac{c}{ab})(c) + (\\frac{1}{b})(0) \\end{pmatrix}\n$$\n$$\nG_{GS} = - \\begin{pmatrix} 0  \\frac{c}{a} \\\\ 0  -\\frac{c^2}{ab} \\end{pmatrix} = \\begin{pmatrix} 0  -\\frac{c}{a} \\\\ 0  \\frac{c^2}{ab} \\end{pmatrix}\n$$\nThis is the explicit $2 \\times 2$ matrix for the Gauss-Seidel iteration operator $G_{GS}$.\n\nThe next step is to compute the spectral radius of $G_{GS}$, denoted $\\rho(G_{GS})$. The spectral radius is the maximum of the absolute values of the eigenvalues of $G_{GS}$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(G_{GS} - \\lambda I) = 0$. Since $G_{GS}$ is an upper triangular matrix, its eigenvalues are its diagonal entries.\nThe diagonal entries of $G_{GS}$ are $0$ and $\\frac{c^2}{ab}$.\nThus, the eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = \\frac{c^2}{ab}$.\n\nThe spectral radius is then:\n$$\n\\rho(G_{GS}) = \\max(|\\lambda_1|, |\\lambda_2|) = \\max\\left(|0|, \\left|\\frac{c^2}{ab}\\right|\\right)\n$$\nGiven the conditions $a  0$ and $b  0$, the product $ab$ is positive. The term $c^2$ is always non-negative. Therefore, the fraction $\\frac{c^2}{ab}$ is non-negative, and its absolute value is itself.\n$$\n\\rho(G_{GS}) = \\max\\left(0, \\frac{c^2}{ab}\\right) = \\frac{c^2}{ab}\n$$\nThis is the closed-form expression for the spectral radius of $G_{GS}$.\n\nFinally, we interpret this result. A stationary iterative method converges for any initial guess if and only if its spectral radius is less than $1$. For the Gauss-Seidel method applied to this matrix $A$, convergence is guaranteed if $\\rho(G_{GS})  1$, which means:\n$$\n\\frac{c^2}{ab}  1 \\implies c^2  ab \\implies ab - c^2  0\n$$\nThis condition is part of the problem's premise, confirming that $A$ is positive definite. A well-known theorem states that the Gauss-Seidel method converges for any symmetric positive definite matrix, and our explicit calculation confirms this for the $2 \\times 2$ case.\n\nThe asymptotic rate of convergence is given by $R = -\\ln(\\rho(G_{GS}))$. Faster convergence corresponds to a larger rate $R$, which in turn requires a smaller spectral radius $\\rho(G_{GS})$. The expression $\\rho(G_{GS}) = \\frac{c^2}{ab}$ allows us to analyze the influence of the matrix entries on the convergence speed.\n1.  **Coupling ($c$)**: The off-diagonal element $c$ quantifies the coupling between the two variables in the system. The spectral radius is proportional to $c^2$. A larger magnitude of $c$ (stronger coupling) leads to a larger spectral radius, thus slowing down convergence. If $c=0$, the matrix $A$ is diagonal, $\\rho(G_{GS}) = 0$, and the method converges in a single iteration.\n2.  **Diagonal Entries ($a, b$)**: The spectral radius is inversely proportional to the product $ab$ of the diagonal entries. Increasing $a$ or $b$ while keeping $c$ constant decreases the spectral radius, thereby accelerating convergence. This relates to the concept of diagonal dominance. A matrix is strictly diagonally dominant if $|A_{ii}|  \\sum_{j \\neq i} |A_{ij}|$. For our matrix, this means $a  |c|$ and $b  |c|$. If these conditions hold, then $ab  c^2$, which guarantees $\\rho(G_{GS}) = \\frac{c^2}{ab}  1$. The \"strength\" of diagonal dominance, characterized by how much larger $a$ and $b$ are than $|c|$, directly controls the convergence speed. A more strongly diagonally dominant matrix (large $a$ and $b$ relative to $|c|$) will have a smaller $\\rho(G_{GS})$ and thus converge faster.\n\nIn summary, the Gauss-Seidel method converges faster for matrices that are \"closer\" to being diagonal, which means weaker coupling (smaller $|c|$) and stronger diagonal dominance (larger $a$ and $b$).",
            "answer": "$$\n\\boxed{\\frac{c^2}{ab}}\n$$"
        },
        {
            "introduction": "The spectral radius guarantees convergence *asymptotically*, but what happens in the initial steps of an iteration? This final practice  explores the crucial distinction between the spectral radius $\\rho(M)$ and the matrix norm $\\|M\\|$, using carefully constructed examples of normal and non-normal matrices. By analyzing these cases, you will understand why the error in an iteration can temporarily grow even when convergence is assured, a vital insight for robustly implementing stopping criteria.",
            "id": "3542458",
            "problem": "Consider the Stationary Iterative Method (SIM) for solving a linear system, defined by the recurrence $x_{k+1} = M x_k + c$ for a fixed iteration matrix $M \\in \\mathbb{C}^{n \\times n}$ and vector $c \\in \\mathbb{C}^{n}$. Let the error be $e_k = x_k - x^\\star$, where $x^\\star$ is a fixed point satisfying $x^\\star = M x^\\star + c$. Then $e_{k+1} = M e_k$ and $e_k = M^k e_0$. By definition, the spectral radius $\\rho(M)$ is $\\rho(M) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } M\\}$, and the induced $2$-norm is $\\|M\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|M x\\|_2$, equal to the largest singular value of $M$. It is a fundamental fact that for any induced matrix norm, $\\rho(M) \\le \\|M\\|$, and that $e_k \\to 0$ for all $e_0$ if and only if $\\rho(M)  1$. In practice, some stopping rules use $\\|x_{k+1}-x_k\\|_2$ or bounds derived from $\\|M\\|_2$ to infer or estimate convergence.\n\nEvaluate the following options about specific matrices and related convergence assessments. Select all options that are correct.\n\n- Option A: Let $M_A = \\mathrm{diag}\\!\\left(\\frac{3}{5}, \\frac{4}{5}, -\\frac{2}{5}\\right)$. Then $\\|M_A\\|_2  1$ and $\\rho(M_A)  1$, and the SIM converges for all initial guesses $x_0$. In this case, a norm-based contraction argument using $\\|M_A\\|_2$ correctly certifies convergence.\n\n- Option B: Let $M_B = \\begin{pmatrix} \\frac{1}{2}  1 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$. Then $\\rho(M_B)  1$ but $\\|M_B\\|_2  1$, and the SIM converges for all initial guesses $x_0$. A rule of the form “declare divergence if $\\|M\\|_2  1$” would misclassify this case. Moreover, for certain initial errors $e_0$, the sequence $\\|e_k\\|_2$ and the difference $\\|x_{k+1}-x_k\\|_2$ can initially increase before eventually decreasing, so monotonic-decrease-based stopping criteria may misinterpret convergence.\n\n- Option C: Let $M_C = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$. The SIM converges because $\\rho(M_C) = 1$ while $\\|M_C\\|_2 = \\sqrt{2}  1$, and in this case a norm-based stopping rule that requires $\\|M\\|_2  1$ would incorrectly reject a convergent iteration.\n\n- Option D: For any induced matrix norm $\\|\\cdot\\|$, the condition $\\|M\\|  1$ is equivalent to $\\rho(M)  1$ for all matrices $M \\in \\mathbb{C}^{n \\times n}$.\n\n- Option E: If $\\rho(M)  1$, then for every initial error $e_0$ the error norms must be monotonically nonincreasing, specifically $\\|e_{k+1}\\|_2 \\le \\|e_k\\|_2$ for all $k \\ge 0$. Therefore, any stopping rule based on the monotonic decrease of $\\|x_{k+1}-x_k\\|_2$ correctly detects convergence without risk of misinterpretation.\n\nChoose all correct options from A–E.",
            "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   Stationary Iterative Method (SIM): $x_{k+1} = M x_k + c$ for a fixed iteration matrix $M \\in \\mathbb{C}^{n \\times n}$ and vector $c \\in \\mathbb{C}^{n}$.\n-   Error vector: $e_k = x_k - x^\\star$, where $x^\\star$ is a fixed point satisfying $x^\\star = M x^\\star + c$.\n-   Error recurrence relation: $e_{k+1} = M e_k$ and $e_k = M^k e_0$.\n-   Spectral radius definition: $\\rho(M) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } M\\}$.\n-   Induced $2$-norm definition: $\\|M\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|M x\\|_2$, which is equal to the largest singular value of $M$.\n-   Fundamental Fact 1: For any induced matrix norm, $\\rho(M) \\le \\|M\\|$.\n-   Fundamental Fact 2 (Convergence Criterion): The error $e_k \\to 0$ for all initial errors $e_0$ if and only if $\\rho(M)  1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard exposition on the convergence of stationary iterative methods in numerical linear algebra. All definitions, such as the spectral radius and induced $2$-norm, are correct. The provided \"fundamental facts\" are cornerstone theorems of the field. The questions posed in the options are designed to test the understanding of the relationship between the spectral radius, matrix norms, and the transient and asymptotic behavior of the iterative process. The problem is scientifically grounded, well-posed, and objective. It contains no contradictions, ambiguities, or unsound premises.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full analysis of each option is warranted.\n\n### Analysis of Options\n\n**Option A:**\nLet $M_A = \\mathrm{diag}\\!\\left(\\frac{3}{5}, \\frac{4}{5}, -\\frac{2}{5}\\right)$.\nThis is a diagonal matrix, which is a special case of a normal matrix (a matrix $M$ such that $M M^* = M^* M$, where $M^*$ is the conjugate transpose of $M$).\nThe eigenvalues of a diagonal matrix are its diagonal entries: $\\lambda_1 = \\frac{3}{5}$, $\\lambda_2 = \\frac{4}{5}$, and $\\lambda_3 = -\\frac{2}{5}$.\nThe spectral radius is the maximum magnitude of the eigenvalues:\n$$ \\rho(M_A) = \\max\\left\\{\\left|\\frac{3}{5}\\right|, \\left|\\frac{4}{5}\\right|, \\left|-\\frac{2}{5}\\right|\\right\\} = \\max\\left\\{\\frac{3}{5}, \\frac{4}{5}, \\frac{2}{5}\\right\\} = \\frac{4}{5} $$\nSince $\\rho(M_A) = \\frac{4}{5}  1$, the fundamental convergence criterion guarantees that the SIM converges for all initial guesses $x_0$.\nFor any normal matrix, the induced $2$-norm is equal to its spectral radius. Thus,\n$$ \\|M_A\\|_2 = \\rho(M_A) = \\frac{4}{5} $$\nSo, we have $\\|M_A\\|_2  1$ and $\\rho(M_A)  1$, and convergence is guaranteed.\nA norm-based contraction argument is based on showing that the iteration is a contraction mapping with respect to some norm. The error recurrence is $e_{k+1} = M e_k$. Taking the $2$-norm, we get:\n$$ \\|e_{k+1}\\|_2 = \\|M_A e_k\\|_2 \\le \\|M_A\\|_2 \\|e_k\\|_2 $$\nSince $\\|M_A\\|_2 = \\frac{4}{5}  1$, the error norm decreases at each step, i.e., $\\|e_{k+1}\\|_2 \\le \\frac{4}{5} \\|e_k\\|_2$. This proves convergence directly and certifies it, as stated. All parts of the statement are consistent and correct.\n\n**Verdict for Option A: Correct**\n\n**Option B:**\nLet $M_B = \\begin{pmatrix} \\frac{1}{2}  1 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$.\nThis is an upper triangular matrix. Its eigenvalues are its diagonal entries: $\\lambda_1 = \\lambda_2 = \\frac{1}{2}$.\nThe spectral radius is $\\rho(M_B) = \\left|\\frac{1}{2}\\right| = \\frac{1}{2}$.\nSince $\\rho(M_B)  1$, the SIM converges for all initial guesses $x_0$.\nTo find the induced $2$-norm, we calculate the largest singular value of $M_B$. The singular values are the square roots of the eigenvalues of $M_B^* M_B$. Since $M_B$ is real, $M_B^* = M_B^T$.\n$$ M_B^T M_B = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2}  1 \\\\ 0  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{5}{4} \\end{pmatrix} $$\nThe characteristic equation for the eigenvalues $\\mu$ of $M_B^T M_B$ is $\\det(M_B^T M_B - \\mu I) = 0$:\n$$ \\left(\\frac{1}{4} - \\mu\\right)\\left(\\frac{5}{4} - \\mu\\right) - \\frac{1}{4} = 0 \\\\ \\mu^2 - \\frac{6}{4}\\mu + \\frac{5}{16} - \\frac{4}{16} = 0 \\\\ \\mu^2 - \\frac{3}{2}\\mu + \\frac{1}{16} = 0 $$\nThe largest eigenvalue is $\\mu_{\\max} = \\frac{\\frac{3}{2} + \\sqrt{\\frac{9}{4} - \\frac{4}{16}}}{2} = \\frac{\\frac{3}{2} + \\sqrt{\\frac{36}{16} - \\frac{4}{16}}}{2} = \\frac{\\frac{3}{2} + \\sqrt{\\frac{32}{16}}}{2} = \\frac{\\frac{3}{2} + \\sqrt{2}}{2} = \\frac{3 + 2\\sqrt{2}}{4}$.\nThe induced $2$-norm is $\\|M_B\\|_2 = \\sigma_{\\max} = \\sqrt{\\mu_{\\max}} = \\sqrt{\\frac{3+2\\sqrt{2}}{4}} = \\frac{\\sqrt{(1+\\sqrt{2})^2}}{2} = \\frac{1+\\sqrt{2}}{2}$.\nSince $\\sqrt{2} \\approx 1.414$, we have $\\|M_B\\|_2 \\approx \\frac{1+1.414}{2} = 1.207  1$.\nSo, $\\rho(M_B)  1$ (convergence) but $\\|M_B\\|_2  1$. A rule that declares divergence if $\\|M\\|_2  1$ would indeed misclassify this case.\nBecause $\\|M_B\\|_2  1$, there exists an initial error vector $e_0$ such that $\\|e_1\\|_2 = \\|M_B e_0\\|_2  \\|e_0\\|_2$. This demonstrates that the error norm $\\|e_k\\|_2$ can initially increase. This phenomenon is characteristic of non-normal matrices.\nSimilarly, consider the difference vector $\\Delta x_k = x_{k+1}-x_k = (M_B-I)e_k$. Then $\\Delta x_{k+1} = M_B \\Delta x_k$. The norm of this difference behaves like the error norm: $\\|\\Delta x_{k+1}\\|_2 = \\|M_B \\Delta x_k\\|_2$. One can choose an initial guess such that $\\Delta x_0$ is a vector that is maximally stretched by $M_B$, leading to $\\|\\Delta x_1\\|_2  \\|\\Delta x_0\\|_2$. Thus, a stopping criterion based on monotonic decrease of $\\|\\Delta x_k\\|_2$ can prematurely and incorrectly terminate.\n\n**Verdict for Option B: Correct**\n\n**Option C:**\nLet $M_C = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$.\nThis is a Jordan block with eigenvalue $\\lambda=1$. The eigenvalues are $\\lambda_1 = \\lambda_2 = 1$.\nThe spectral radius is $\\rho(M_C) = 1$.\nThe fundamental criterion for convergence for *all* initial conditions is $\\rho(M)  1$. Here, this condition is violated. The statement \"The SIM converges because $\\rho(M_C) = 1$\" is false. To show it diverges, we compute powers of $M_C$. Let $M_C = I+N$ where $N=\\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$ and $N^2=0$.\n$$ M_C^k = (I+N)^k = I + kN = \\begin{pmatrix} 1  k \\\\ 0  1 \\end{pmatrix} $$\nThe error at step $k$ is $e_k = M_C^k e_0$.\n$$ e_k = \\begin{pmatrix} 1  k \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} e_{0,1} \\\\ e_{0,2} \\end{pmatrix} = \\begin{pmatrix} e_{0,1} + k e_{0,2} \\\\ e_{0,2} \\end{pmatrix} $$\nFor $e_k$ to converge to the zero vector, we need both components to go to zero. The first component, $e_{0,1} + k e_{0,2}$, diverges to infinity if $e_{0,2} \\neq 0$. Thus, the method does not converge for arbitrary $e_0$. The first part of the statement is incorrect.\nFurthermore, the statement claims $\\|M_C\\|_2 = \\sqrt{2}$. Let's verify this.\n$$ M_C^T M_C = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} $$\nEigenvalues $\\mu$ of $M_C^T M_C$ satisfy $(1-\\mu)(2-\\mu)-1 = 0$, which is $\\mu^2-3\\mu+1=0$.\nThe largest eigenvalue is $\\mu_{\\max} = \\frac{3+\\sqrt{9-4}}{2} = \\frac{3+\\sqrt{5}}{2}$.\nThe norm is $\\|M_C\\|_2 = \\sqrt{\\mu_{\\max}} = \\sqrt{\\frac{3+\\sqrt{5}}{2}} \\approx 1.618$. This is not equal to $\\sqrt{2} \\approx 1.414$.\nThe statement makes two incorrect claims: one about convergence and one about the value of the norm.\n\n**Verdict for Option C: Incorrect**\n\n**Option D:**\nThe statement is: \"For any induced matrix norm $\\|\\cdot\\|$, the condition $\\|M\\|  1$ is equivalent to $\\rho(M)  1$ for all matrices $M \\in \\mathbb{C}^{n \\times n}$.\"\nThis can be broken into two implications:\n1. $\\|M\\|  1 \\implies \\rho(M)  1$. This is true because of the fundamental fact $\\rho(M) \\le \\|M\\|$ for any induced norm. If $\\|M\\|  1$, then $\\rho(M)$ must also be less than $1$.\n2. $\\rho(M)  1 \\implies \\|M\\|  1$. This is false. A statement \"for any induced matrix norm\" requires it to hold for all of them. We have a direct counterexample from Option B.\nFor $M_B = \\begin{pmatrix} \\frac{1}{2}  1 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$ and the induced $2$-norm, we found $\\rho(M_B) = \\frac{1}{2}  1$, but $\\|M_B\\|_2 = \\frac{1+\\sqrt{2}}{2}  1$.\nSince the reverse implication is false, the equivalence is false.\n\n**Verdict for Option D: Incorrect**\n\n**Option E:**\nThe statement is: \"If $\\rho(M)  1$, then for every initial error $e_0$ the error norms must be monotonically nonincreasing, specifically $\\|e_{k+1}\\|_2 \\le \\|e_k\\|_2$ for all $k \\ge 0$.\"\nThe condition $\\|e_{k+1}\\|_2 \\le \\|e_k\\|_2$ is equivalent to $\\|M e_k\\|_2 \\le \\|e_k\\|_2$. For this to hold for *every* $e_k$ (and thus for any vector in $\\mathbb{C}^n$), it would require that $\\|M v\\|_2 \\le \\|v\\|_2$ for all $v$. By definition, this is equivalent to $\\sup_{\\|v\\|_2=1} \\|M v\\|_2 \\le 1$, which means $\\|M\\|_2 \\le 1$.\nThe statement is therefore claiming that $\\rho(M)  1 \\implies \\|M\\|_2 \\le 1$.\nAs shown in the analysis of Options B and D, this implication is false. The matrix $M_B$ is a counterexample where $\\rho(M_B)  1$ but $\\|M_B\\|_2  1$. For this matrix, there exists an initial error $e_0$ such that $\\|e_1\\|_2  \\|e_0\\|_2$, contradicting the claim of monotonic non-increase.\nThe second sentence, \"Therefore, any stopping rule based on the monotonic decrease of $\\|x_{k+1}-x_k\\|_2$ correctly detects convergence without risk of misinterpretation,\" is a conclusion drawn from this false premise. As also shown in the analysis for Option B, the quantity $\\|x_{k+1}-x_k\\|_2$ can also exhibit transient growth even when the iteration converges. Thus, the conclusion is also false.\n\n**Verdict for Option E: Incorrect**",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}