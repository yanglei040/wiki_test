## Introduction
Solving large systems of linear equations of the form $Ax=b$ is a cornerstone of computational science and engineering. While direct methods can be effective, their computational cost becomes prohibitive for the massive systems arising from modern simulations. Stationary [iterative methods](@entry_id:139472) provide a powerful alternative, generating a sequence of approximate solutions that ideally converge to the true answer. However, this raises a crucial question: under what conditions do these iterations converge, and what determines how quickly they do so? Without a rigorous framework for analysis, choosing or designing an effective iterative solver is a matter of guesswork.

This article provides a comprehensive exploration of the convergence theory for stationary methods. It addresses this knowledge gap by establishing a precise mathematical link between the algebraic properties of an iteration and its performance. Across the following chapters, you will build a robust understanding of this topic. The "Principles and Mechanisms" chapter will dissect the core theory, introducing the iteration matrix and proving that its [spectral radius](@entry_id:138984) is the ultimate arbiter of convergence. Following this, the "Applications and Interdisciplinary Connections" chapter will demonstrate how this theory provides critical insights into problems ranging from discretized PDEs to the PageRank algorithm. Finally, the "Hands-On Practices" section will allow you to apply these concepts directly, cementing your knowledge through targeted exercises. We begin by examining the fundamental principles that govern the behavior of all [stationary iterative methods](@entry_id:144014).

## Principles and Mechanisms

Having established the context of [stationary iterative methods](@entry_id:144014) for [solving linear systems](@entry_id:146035) of the form $A x = b$, we now delve into the core principles that govern their behavior. This chapter will dissect the mechanisms of convergence, explore the pivotal role of the spectral radius, and investigate the subtle yet crucial phenomena that arise from the algebraic properties of the [iteration matrix](@entry_id:637346).

### Stationary Iterative Methods and the Iteration Matrix

A stationary [iterative method](@entry_id:147741) generates a sequence of approximate solutions $\{x^{(k)}\}_{k=0}^{\infty}$ where the rule for advancing from $x^{(k)}$ to $x^{(k+1)}$ is fixed for all $k$. The foundation of these methods lies in the concept of **splitting** the matrix $A$. We express $A$ as the difference of two matrices, $A = M - N$, where $M$ is chosen to be nonsingular and easily invertible.

Substituting this splitting into the original system gives $ (M - N)x = b $, or $Mx = Nx + b$. This algebraic rearrangement inspires an iterative process. Given an approximation $x^{(k)}$, we obtain the next approximation $x^{(k+1)}$ by solving the system:
$$ M x^{(k+1)} = N x^{(k)} + b $$
Since $M$ is nonsingular, we can write the iteration in the explicit fixed-point form:
$$ x^{(k+1)} = M^{-1}N x^{(k)} + M^{-1}b $$
This is the [canonical form](@entry_id:140237) of a linear stationary iteration. We define the **[iteration matrix](@entry_id:637346)** as $G \coloneqq M^{-1}N$ and the constant vector as $c \coloneqq M^{-1}b$, yielding the compact form $x^{(k+1)} = G x^{(k)} + c$. The choice of the splitting $A=M-N$ entirely determines the iteration matrix $G$ and thus the convergence properties of the method.

Let's consider three classical examples derived from the decomposition of $A$ into its diagonal ($D$), strictly lower-triangular ($-L$), and strictly upper-triangular ($-U$) parts, such that $A = D - L - U$.

1.  **The Jacobi Method**: In this method, we choose the most easily invertible part of $A$, its diagonal $D$, for the implicit part of the splitting. We set $M = D$ and $N = L+U$. Assuming all diagonal entries of $A$ are nonzero, $D$ is invertible. The Jacobi iteration matrix is therefore :
    $$ G_{J} = M^{-1}N = D^{-1}(L+U) $$

2.  **The Gauss-Seidel Method**: This method aims to accelerate convergence by using the most recently updated information. At step $i$ of an iteration, the new components $x_1^{(k+1)}, \dots, x_{i-1}^{(k+1)}$ are already computed. The Gauss-Seidel method uses these new values immediately. This corresponds to moving the lower-triangular part of $A$ to the implicit side of the splitting. We set $M = D-L$ and $N = U$. The matrix $M$ is lower-triangular with a nonzero diagonal (assuming the same for $A$), and is thus nonsingular and easily invertible via [forward substitution](@entry_id:139277). The Gauss-Seidel iteration matrix is :
    $$ G_{GS} = M^{-1}N = (D-L)^{-1}U $$

3.  **The Richardson Method**: This method is not based on a structural splitting of $A$ but on a simple correction scheme. The iteration is given by $x^{(k+1)} = x^{(k)} + \tau(b - Ax^{(k)})$, where $\tau > 0$ is a scalar [relaxation parameter](@entry_id:139937). By rearranging this into our [canonical form](@entry_id:140237), $x^{(k+1)} = (I - \tau A)x^{(k)} + \tau b$, we can identify the Richardson [iteration matrix](@entry_id:637346) as :
    $$ G_{R} = I - \tau A $$

### The Spectral Radius and the Fundamental Convergence Criterion

The convergence of a stationary method depends entirely on the properties of its iteration matrix $G$. Let the exact solution to $Ax=b$ be $x^\star$. If the iteration converges, its limit must be a fixed point of the mapping, so $x^\star = Gx^\star + c$. The error at iteration $k$ is defined as $e^{(k)} = x^{(k)} - x^\star$. We can derive the law of [error propagation](@entry_id:136644) by subtracting the [fixed-point equation](@entry_id:203270) from the iteration equation:
$$ x^{(k+1)} - x^\star = (G x^{(k)} + c) - (G x^\star + c) = G(x^{(k)} - x^\star) $$
This gives the simple but profound relationship $e^{(k+1)} = G e^{(k)}$, which implies $e^{(k)} = G^k e^{(0)}$.

The iteration converges for any arbitrary initial vector $x^{(0)}$ if and only if the error $e^{(k)}$ vanishes as $k \to \infty$ for any initial error $e^{(0)}$. This is equivalent to the condition that the [matrix powers](@entry_id:264766) $G^k$ must approach the zero matrix, $\lim_{k \to \infty} G^k = \mathbf{0}$.

A fundamental theorem of [matrix analysis](@entry_id:204325) provides a precise algebraic condition for this limit to hold. The condition is expressed in terms of the **spectral radius** of $G$, denoted $\rho(G)$. The spectral radius is the maximum magnitude among all eigenvalues of $G$:
$$ \rho(G) \coloneqq \max \{ |\lambda| : \lambda \in \sigma(G) \} $$
where $\sigma(G)$ is the spectrum (the set of eigenvalues) of $G$. The central theorem of [stationary iterative methods](@entry_id:144014) states:

*The stationary iteration $x^{(k+1)} = G x^{(k)} + c$ converges for every initial vector $x^{(0)}$ if and only if the [spectral radius](@entry_id:138984) of the [iteration matrix](@entry_id:637346) is strictly less than one: $\rho(G)  1$.* 

The magnitude of the spectral radius determines the asymptotic rate of convergence. The smaller $\rho(G)$ is, the faster the error decays. For example, in the Richardson method ($G_R = I - \tau A$), if we know the eigenvalues $\lambda_j$ of $A$, we can determine the eigenvalues of $G_R$ as $1 - \tau \lambda_j$. The [spectral radius](@entry_id:138984) is then $\rho(G_R) = \max_j |1 - \tau\lambda_j|$. If $A$ is [symmetric positive definite](@entry_id:139466) with eigenvalues $0  \lambda_{\min} \le \lambda_{\max}$, one can choose the parameter $\tau$ to minimize this spectral radius. The optimal choice is $\tau_{opt} = \frac{2}{\lambda_{\min} + \lambda_{\max}}$, which yields the minimal spectral radius $\rho_{min} = \frac{\lambda_{\max} - \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}$. This demonstrates how the convergence criterion can be used not just to check for convergence but to actively optimize it .

### Matrix Norms: A Sufficient Condition and a Practical Bound

While the spectral radius provides a necessary and sufficient condition for convergence, computing the entire spectrum of a large matrix can be prohibitively expensive. Matrix norms offer an alternative, more practical perspective. An **[induced matrix norm](@entry_id:145756)** $\| \cdot \|$, associated with a [vector norm](@entry_id:143228), is defined as $\|G\| = \sup_{x \neq 0} \frac{\|Gx\|}{\|x\|}$.

A direct relationship exists between the [spectral radius](@entry_id:138984) and any [induced matrix norm](@entry_id:145756). For any eigenvalue $\lambda$ of $G$ with corresponding eigenvector $v$, we have $Gv = \lambda v$. Taking norms gives $\|Gv\| = \|\lambda v\| = |\lambda| \|v\|$. From the definition of the [induced norm](@entry_id:148919), $|\lambda| = \frac{\|Gv\|}{\|v\|} \le \sup_{x \neq 0} \frac{\|Gx\|}{\|x\|} = \|G\|$. Since this holds for every eigenvalue, it must hold for the one with the largest magnitude. Thus, we have the fundamental inequality :
$$ \rho(G) \le \|G\| $$

This inequality immediately gives a [sufficient condition](@entry_id:276242) for convergence. From the [error propagation](@entry_id:136644) $e^{(k)} = G^k e^{(0)}$, we can bound the norm of the error: $\|e^{(k)}\| = \|G^k e^{(0)}\| \le \|G^k\| \|e^{(0)}\|$. Using the submultiplicative property of [induced norms](@entry_id:163775), $\|G^k\| \le \|G\|^k$. Therefore, $\|e^{(k)}\| \le \|G\|^k \|e^{(0)}\|$. If we can find any [induced matrix norm](@entry_id:145756) for which $\|G\|  1$, then $\|G\|^k \to 0$, ensuring that the error vanishes and the iteration converges .

However, it is crucial to recognize that $\|G\|  1$ is a *sufficient* condition, not a *necessary* one. An iteration can converge even if the norm of its iteration matrix is greater than one in a particular norm. This happens because the condition for convergence is $\rho(G)  1$, and for many matrices, there can be a significant gap between the spectral radius and the [matrix norm](@entry_id:145006). This discrepancy is a hallmark of **[non-normal matrices](@entry_id:137153)**, for which $G^*G \neq GG^*$. For the special class of [normal matrices](@entry_id:195370), the [2-norm](@entry_id:636114) (or spectral norm) is exactly equal to the [spectral radius](@entry_id:138984): $\|G\|_2 = \rho(G)$. In this case, the condition $\|G\|_2  1$ is both necessary and sufficient.

A classic example illustrating the gap between $\rho(G)$ and $\|G\|$ is the [non-normal matrix](@entry_id:175080) $G_{\alpha} = \begin{pmatrix} 0  \alpha \\ 0  0 \end{pmatrix}$. Its eigenvalues are both 0, so $\rho(G_{\alpha}) = 0$, guaranteeing convergence (in fact, it terminates in two steps as $G_\alpha^2 = \mathbf{0}$). However, its [2-norm](@entry_id:636114) can be calculated as $\|G_{\alpha}\|_2 = |\alpha|$. For $|\alpha| > 1$, we have a situation where $\rho(G_{\alpha})  1$ but $\|G_{\alpha}\|_2 > 1$, demonstrating that a norm exceeding 1 does not preclude convergence .

Despite this, norms provide invaluable practical tools. The easily computable [1-norm](@entry_id:635854) (maximum absolute column sum) and $\infty$-norm (maximum absolute row sum) provide upper bounds for the [spectral radius](@entry_id:138984). For a given matrix $G$, one can compute $\|G\|_1$ and $\|G\|_\infty$ and use the tighter of the two, $\min(\|G\|_1, \|G\|_\infty)$, as an accessible upper bound for $\rho(G)$. If this bound is less than 1, convergence is guaranteed .

### The Anatomy of Non-Normality: Transient Growth and Pseudospectra

The fact that an iteration with $\rho(G)1$ can have $\|G\|>1$ is not merely a theoretical curiosity. It is a direct manifestation of a phenomenon known as **transient growth**, where the error norm $\|e^{(k)}\|$ can increase for a period of time before its eventual asymptotic decay. The behavior of $\|G^k\|$ dictates the [error amplification](@entry_id:142564) at step $k$. While Gelfand's formula, $\rho(G) = \lim_{k \to \infty} \|G^k\|^{1/k}$, ensures that $\|G^k\|$ eventually decays faster than any [geometric sequence](@entry_id:276380) with a ratio greater than $\rho(G)$, it does not forbid $\|G^k\|$ from exceeding 1 for finite $k$.

The mechanism of transient growth is revealed by the Jordan Normal Form of $G$. Any matrix can be written as $G = P J P^{-1}$, where $J$ is a [block-diagonal matrix](@entry_id:145530) of Jordan blocks. A Jordan block of size $m$ for an eigenvalue $\lambda$ is $J_{\lambda,m} = \lambda I + N_m$, where $N_m$ is a [nilpotent matrix](@entry_id:152732) with ones on the first superdiagonal. The $k$-th power of this block is:
$$ (J_{\lambda, m})^k = \sum_{j=0}^{\min(k, m-1)} \binom{k}{j} \lambda^{k-j} N_m^j $$
The norm of this matrix power involves a sum of terms. While the $\lambda^{k-j}$ term provides [exponential decay](@entry_id:136762) (for $|\lambda|1$), the binomial coefficient $\binom{k}{j}$ is a polynomial in $k$ of degree $j$. For $j \ge 1$, this polynomial term causes initial growth. The overall behavior of $\|(J_{\lambda,m})^k\|$ is a competition between this [polynomial growth](@entry_id:177086) and the ultimate [exponential decay](@entry_id:136762). This can lead to a "hump" where the norm initially grows before decaying. When this is combined with the similarity transform $G^k = P J^k P^{-1}$, a poorly conditioned eigenvector matrix $P$ (large $\kappa(P) = \|P\|\|P^{-1}\|$) can further amplify this transient effect .

For example, consider the constructed matrix $G = S J S^{-1}$ where $J = r(I+N)$ is a Jordan block scaled by $r \in (0,1)$ and $S$ is a [scaling matrix](@entry_id:188350). A direct calculation of the matrix power $G^k$ reveals terms that are polynomial in $k$. For a $3 \times 3$ case, the norm $\|G^k\|_1$ can be found to be $r^k (1 + k\alpha + \frac{k(k-1)}{2}\alpha^2)$, explicitly showing the quadratic growth in $k$ multiplied by the [exponential decay](@entry_id:136762) of $r^k$ .

A modern tool for understanding and visualizing the effects of [non-normality](@entry_id:752585) is the **$\varepsilon$-[pseudospectrum](@entry_id:138878)**, $\Lambda_\varepsilon(G)$. It is the set of complex numbers $z$ that are "almost" eigenvalues, defined for $\varepsilon > 0$ as:
$$ \Lambda_\varepsilon(G) \coloneqq \{ z \in \mathbb{C} : \sigma_{\min}(zI - G) \le \varepsilon \} $$
where $\sigma_{\min}$ is the smallest singular value. For a [normal matrix](@entry_id:185943), the $\varepsilon$-pseudospectrum is simply the union of $\varepsilon$-disks around the true eigenvalues. For a highly [non-normal matrix](@entry_id:175080), $\Lambda_\varepsilon(G)$ can be a much larger region that extends far from the spectrum $\sigma(G)$. If the pseudospectrum of a matrix with $\rho(G)  1$ bulges out to cross the unit circle $|z|=1$, it is a strong indicator that the matrix will exhibit significant transient growth, and convergence, while guaranteed, may be extremely slow in practice .

### The Limiting Case: Semiconvergence and Singular Systems

We conclude by examining the boundary case where the spectral radius is not strictly less than one, but equals one. What, if anything, can be said about the convergence of the iteration $x^{(k+1)} = Gx^{(k)} + c$ when $\rho(G) = 1$?

In this regime, the standard convergence theorem does not apply. Instead, we rely on the concept of **[semiconvergence](@entry_id:754688)**. A matrix $G$ is said to be semiconvergent if the limit of its powers, $\lim_{k \to \infty} G^k$, exists (though this limit is not necessarily the [zero matrix](@entry_id:155836)). The [necessary and sufficient conditions](@entry_id:635428) for $G$ to be semiconvergent are :
1.  The [spectral radius](@entry_id:138984) satisfies $\rho(G) \le 1$.
2.  If $\lambda$ is an eigenvalue with $|\lambda|=1$, it must be that $\lambda=1$.
3.  The eigenvalue $\lambda=1$ must be **semisimple**, meaning all its associated Jordan blocks are of size 1. This is equivalent to its [algebraic multiplicity](@entry_id:154240) equaling its [geometric multiplicity](@entry_id:155584).

If the [iteration matrix](@entry_id:637346) $G$ is semiconvergent, the iteration $x^{(k+1)} = Gx^{(k)} + c$ does not necessarily converge for any $c$. A second condition is required: the underlying linear system that the iteration aims to solve, $(I-G)x=c$, must be **consistent**. That is, a solution must exist. If the system is inconsistent, no fixed point exists, and the iteration cannot converge to a solution.

When these two conditions—$G$ is semiconvergent and $(I-G)x=c$ is consistent—are met, the iteration converges for any initial vector $x^{(0)}$. However, the limit generally depends on the starting vector. The limit matrix $\Pi = \lim_{k\to\infty} G^k$ is the spectral projector onto the [null space](@entry_id:151476) of $(I-G)$, which is the [eigenspace](@entry_id:150590) for $\lambda=1$. If $x^\star$ is any [particular solution](@entry_id:149080) to $(I-G)x=c$, the sequence of iterates converges to:
$$ x^{(\infty)} = x^\star + \Pi(x^{(0)} - x^\star) $$
The limit $x^{(\infty)}$ is itself a solution, but it is "pulled" towards the specific solution determined by the projection of the initial error onto the [eigenspace](@entry_id:150590) of $\lambda=1$ .

Let's illustrate this with a concrete example. Consider an iteration with the diagonal matrix $G = \mathrm{diag}(1, 1/2, -1/3)$. Its eigenvalues are $1, 1/2, -1/3$. The [spectral radius](@entry_id:138984) is $\rho(G)=1$. The only eigenvalue on the unit circle is $\lambda=1$, and since it is a simple eigenvalue in a [diagonal matrix](@entry_id:637782), it is semisimple. Thus, $G$ is semiconvergent. Let the associated system be $Ax=b$ where $A = I-G = \mathrm{diag}(0, 1/2, 4/3)$. This matrix is singular. If we choose a vector $b$ that lies in the range of $A$ (e.g., $b=(0, 1, 2)^T$), the system $Ax=b$ is consistent. The iteration $x^{(k+1)} = Gx^{(k)} + b$ will converge. The space of solutions is an affine line. The specific solution to which the iteration converges depends on the initial guess $x^{(0)}$. The limit is composed of two parts: the projection of the initial vector onto the [null space](@entry_id:151476) of $A$, which remains unchanged throughout the iteration, and a fixed vector that is the unique solution to $Ax=b$ within the range of $A$. As $x^{(0)}$ varies, the limit $x^{(\infty)}$ sweeps out the entire affine subspace of solutions .