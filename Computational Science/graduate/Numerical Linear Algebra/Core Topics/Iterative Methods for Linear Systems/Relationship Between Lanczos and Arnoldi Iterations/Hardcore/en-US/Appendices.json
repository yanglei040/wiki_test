{
    "hands_on_practices": [
        {
            "introduction": "The most direct way to grasp the connection between the Arnoldi and Lanczos iterations is to observe them in action. This first exercise provides a concrete computational walkthrough of both algorithms on a small Hermitian matrix. By performing the explicit calculations, you will verify how the general Arnoldi process, with its full orthogonalization, simplifies to the elegant and efficient three-term recurrence of the Lanczos process when the matrix is Hermitian .",
            "id": "3573190",
            "problem": "Let $A \\in \\mathbb{C}^{5 \\times 5}$ be Hermitian, meaning $A = A^{*}$ where $^{*}$ denotes conjugate transpose. Consider the concrete choice\n$$\nA \\;=\\; \\begin{pmatrix}\n2  1  0  0  0 \\\\\n1  3  1  0  0 \\\\\n0  1  4  1  0 \\\\\n0  0  1  3  1 \\\\\n0  0  0  1  2\n\\end{pmatrix},\n$$\nand the initial unit vector $v_{1} = e_{1} = (1,0,0,0,0)^{\\top}$. Using only the core definitions of the Arnoldi process and the Lanczos process for constructing orthonormal Krylov bases, perform exactly two Arnoldi iterations (to obtain $v_{2}$ and $v_{3}$) and two Lanczos steps (to obtain $v_{2}$ and $v_{3}$). Explicitly compute all relevant scalar coefficients that arise in these constructions (in Arnoldi: the Hessenberg entries $h_{11}$, $h_{21}$, $h_{12}$, $h_{22}$, $h_{32}$; in Lanczos: the tridiagonal entries $\\alpha_{1}$, $\\beta_{1}$, $\\alpha_{2}$, $\\beta_{2}$). Then, determine the unimodular scalar $c \\in \\mathbb{C}$ (that is, $|c| = 1$) such that $v_{3}^{\\mathrm{Arnoldi}} = c \\, v_{3}^{\\mathrm{Lanczos}}$.\n\nProvide your final answer as the exact value of $c$ (no rounding required).",
            "solution": "This problem requires us to perform two steps of both the Arnoldi and Lanczos iterations for a given Hermitian matrix and starting vector, and then compare the resulting basis vectors.\n\n**1. Arnoldi Iteration**\n\nThe Arnoldi process constructs an orthonormal basis $\\{v_1, v_2, \\dots\\}$ using a Gram-Schmidt-like procedure. We start with $v_1 = e_1$.\n\n*   **Step 1 (j=1):**\n    1.  Compute $w = Av_1 = A e_1 = [2, 1, 0, 0, 0]^{\\top}$.\n    2.  Orthogonalize $w$ against $v_1$. The coefficient is $h_{11} = v_1^* w = 1 \\cdot 2 = 2$.\n    3.  The orthogonalized vector is $\\tilde{v}_2 = w - h_{11}v_1 = [2, 1, 0, 0, 0]^{\\top} - 2[1, 0, 0, 0, 0]^{\\top} = [0, 1, 0, 0, 0]^{\\top}$.\n    4.  The norm is $h_{21} = \\|\\tilde{v}_2\\|_2 = 1$.\n    5.  Normalize to get the next basis vector: $v_2 = \\tilde{v}_2 / h_{21} = [0, 1, 0, 0, 0]^{\\top} = e_2$.\n\n*   **Step 2 (j=2):**\n    1.  Compute $w = Av_2 = A e_2 = [1, 3, 1, 0, 0]^{\\top}$.\n    2.  Orthogonalize $w$ against $v_1$ and $v_2$. The coefficients are:\n        $h_{12} = v_1^* w = 1 \\cdot 1 = 1$.\n        $h_{22} = v_2^* w = 1 \\cdot 3 = 3$.\n    3.  The orthogonalized vector is $\\tilde{v}_3 = w - h_{12}v_1 - h_{22}v_2 = [1, 3, 1, 0, 0]^{\\top} - 1[1, 0, 0, 0, 0]^{\\top} - 3[0, 1, 0, 0, 0]^{\\top} = [0, 0, 1, 0, 0]^{\\top}$.\n    4.  The norm is $h_{32} = \\|\\tilde{v}_3\\|_2 = 1$.\n    5.  Normalize to get the next basis vector: $v_3^{\\mathrm{Arnoldi}} = \\tilde{v}_3 / h_{32} = [0, 0, 1, 0, 0]^{\\top} = e_3$.\n\nThe computed Arnoldi coefficients are $h_{11}=2, h_{21}=1, h_{12}=1, h_{22}=3, h_{32}=1$.\n\n**2. Lanczos Iteration**\n\nThe Lanczos process is a specialization of Arnoldi for Hermitian matrices, using a three-term recurrence. We start with $v_1 = e_1$, $v_0 = 0$, and $\\beta_0 = 0$.\n\n*   **Step 1 (j=1):**\n    1.  Compute $\\alpha_1 = v_1^* A v_1 = e_1^* A e_1 = 2$.\n    2.  The residual vector is $\\tilde{v}_2 = Av_1 - \\alpha_1 v_1 - \\beta_0 v_0 = [2, 1, 0, 0, 0]^{\\top} - 2[1, 0, 0, 0, 0]^{\\top} = [0, 1, 0, 0, 0]^{\\top}$.\n    3.  The norm is $\\beta_1 = \\|\\tilde{v}_2\\|_2 = 1$.\n    4.  Normalize to get the next basis vector: $v_2 = \\tilde{v}_2 / \\beta_1 = e_2$.\n\n*   **Step 2 (j=2):**\n    1.  Compute $\\alpha_2 = v_2^* A v_2 = e_2^* A e_2 = 3$.\n    2.  The residual vector is $\\tilde{v}_3 = Av_2 - \\alpha_2 v_2 - \\beta_1 v_1 = [1, 3, 1, 0, 0]^{\\top} - 3[0, 1, 0, 0, 0]^{\\top} - 1[1, 0, 0, 0, 0]^{\\top} = [0, 0, 1, 0, 0]^{\\top}$.\n    3.  The norm is $\\beta_2 = \\|\\tilde{v}_3\\|_2 = 1$.\n    4.  Normalize to get the next basis vector: $v_3^{\\mathrm{Lanczos}} = \\tilde{v}_3 / \\beta_2 = e_3$.\n\nThe computed Lanczos coefficients are $\\alpha_1=2, \\beta_1=1, \\alpha_2=3, \\beta_2=1$.\n\n**3. Comparison**\n\nWe have found that $v_3^{\\mathrm{Arnoldi}} = e_3$ and $v_3^{\\mathrm{Lanczos}} = e_3$. The problem asks for the scalar $c$ such that $v_3^{\\mathrm{Arnoldi}} = c \\cdot v_3^{\\mathrm{Lanczos}}$.\nSubstituting our results:\n$$\ne_3 = c \\cdot e_3\n$$\nThis implies $c=1$. This result is expected, as the Lanczos algorithm is mathematically equivalent to the Arnoldi algorithm when applied to a Hermitian matrix, assuming standard (positive real) normalization is used. The long Arnoldi recurrence automatically simplifies to the three-term Lanczos recurrence due to the symmetry of the matrix.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Beyond the mechanical equivalence, a deeper consequence of applying these methods to Hermitian matrices is the phenomenon of \"happy breakdown,\" where the iteration terminates early. This occurs precisely when the Krylov subspace becomes an invariant subspace under the matrix $A$, meaning the algorithm has captured a complete piece of the matrix's structure relative to the starting vector. This practice demonstrates this concept with a carefully chosen matrix and vector, allowing you to determine the exact step at which the underlying invariant subspace is found .",
            "id": "3573173",
            "problem": "Let $A \\in \\mathbb{R}^{5 \\times 5}$ be the real symmetric matrix\n$$\nA = \\operatorname{diag}(1,3,5,7,11),\n$$\nand let $b \\in \\mathbb{R}^{5}$ be\n$$\nb = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix}.\n$$\nConsider the Krylov subspaces $\\mathcal{K}_{k}(A,b) = \\operatorname{span}\\{b, Ab, \\dots, A^{k-1}b\\}$, and the orthonormal bases produced, in exact arithmetic, by the Lanczos iteration (for symmetric $A$) and the Arnoldi iteration (for general $A$) started with $v_{1} = b/\\|b\\|$. Use only foundational definitions from numerical linear algebra, including the definition of a Krylov subspace, the concept of an invariant subspace, and the basic properties that for symmetric $A$, the Arnoldi process reduces to the three-term Lanczos recurrence, generating a symmetric tridiagonal projection.\n\nTasks:\n- Explain why there exists a smallest positive integer $k$ such that the Lanczos iteration produces $v_{k+1} = 0$ in exact arithmetic for the given pair $(A,b)$, and identify the geometric reason in terms of invariant subspace capture by $\\mathcal{K}_{k}(A,b)$.\n- Explain why the Arnoldi iteration exhibits a \"happy breakdown\" at the same step, and relate this to the relationship between Arnoldi and Lanczos for symmetric $A$.\n- Determine the minimal such $k$ explicitly for the given $A$ and $b$.\n\nYour final answer should be the single integer value of $k$ (no units). Do not provide any inequality or equation in the final answer. If you choose to compute any intermediate numerical quantities, retain exact values; no rounding is required. Provide a geometric interpretation as part of your reasoning, but the final answer must be only the integer $k$.",
            "solution": "The Lanczos (or Arnoldi) iteration terminates at step $k$ when the Krylov subspace $\\mathcal{K}_k(A,b)$ becomes an invariant subspace of $A$. This event, known as a \"happy breakdown,\" occurs when the next Krylov vector $A^k b$ is linearly dependent on the previous vectors $\\{b, Ab, \\dots, A^{k-1}b\\}$. The smallest such integer $k$ is the dimension of the maximal Krylov subspace generated by $A$ and $b$.\n\nThe given matrix is $A = \\operatorname{diag}(1,3,5,7,11)$, which is diagonal. Its eigenvectors are the standard basis vectors $e_i$, and its eigenvalues are the diagonal entries.\n\nThe starting vector $b$ can be expressed in the eigenbasis of $A$:\n$$\nb = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix} = 1 \\cdot e_1 + 1 \\cdot e_2\n$$\nThe vector $b$ is a linear combination of only the first two eigenvectors of $A$. This means $b$ lies within the 2-dimensional subspace $\\mathcal{S} = \\operatorname{span}\\{e_1, e_2\\}$. Since $e_1$ and $e_2$ are eigenvectors, this subspace $\\mathcal{S}$ is invariant under $A$.\n\nAny vector generated by applying $A$ to $b$ will remain within this invariant subspace:\n$$\nA^j b = A^j(e_1 + e_2) = A^j e_1 + A^j e_2 = (1^j)e_1 + (3^j)e_2 = e_1 + 3^j e_2\n$$\nAll Krylov vectors $\\{b, Ab, A^2 b, \\dots\\}$ are confined to the 2D subspace $\\mathcal{S}$. Consequently, the dimension of the Krylov subspace $\\mathcal{K}_j(A,b)$ can be at most 2.\n\nTo find the exact dimension, we check for linear independence:\n1.  **k=1**: The first Krylov vector is $b = [1, 1, 0, 0, 0]^\\top$, which is non-zero. The dimension is at least 1.\n2.  **k=2**: The second Krylov vector is $Ab = [1, 3, 0, 0, 0]^\\top$. The set $\\{b, Ab\\}$ is linearly independent, as the two vectors are not scalar multiples of each other. The dimension is at least 2.\n\nSince the dimension is at least 2 and at most 2, the dimension of the maximal Krylov subspace is exactly 2.\nThis means the iteration will generate two orthonormal vectors spanning this subspace. At the next step, the algorithm will find that $A^2 b$ is a linear combination of $b$ and $Ab$. This causes the norm of the residual vector to be zero, leading to a breakdown. The minimal integer $k$ for which this occurs is the dimension of the subspace, which is 2.\n\nTherefore, the Lanczos/Arnoldi process will terminate after exactly $k=2$ steps.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "The primary motivation for building Krylov subspaces is often to solve large-scale eigenvalue problems. This final exercise demonstrates a powerful application: using the generated subspace to find approximations of a matrix's eigenvalues through Rayleigh-Ritz extraction. By constructing a \"refined Ritz vector\" for a specific target value $\\tau$, you will see how the subspace acts as a low-dimensional window into the spectral properties of the matrix and compute the quality of this approximation .",
            "id": "3573179",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be Hermitian with $A = \\operatorname{diag}(1, 2, 4, 8)$, and let $b \\in \\mathbb{R}^{4}$ be $b = [1, 1, 0, 0]^{\\top}$. Consider the Krylov subspace of order $k=2$, $\\mathcal{K}_{2}(A, b) = \\operatorname{span}\\{b, Ab\\}$. Let $V_{2} \\in \\mathbb{R}^{4 \\times 2}$ be any orthonormal basis for $\\mathcal{K}_{2}(A, b)$ produced by the Lanczos process, and let $Q_{2} \\in \\mathbb{R}^{4 \\times 2}$ be any orthonormal basis for the same subspace produced by the Arnoldi process starting from $b$. \n\nFor the target value $\\tau = 4$, define the refined Ritz vector in a subspace $\\mathcal{S}$ with orthonormal basis $W$ as the unit vector $w_{\\mathrm{ref}} \\in \\mathcal{S}$ that minimizes the residual norm $\\|(A - \\tau I) w\\|_{2}$ over all $w \\in \\mathcal{S}$ with $\\|w\\|_{2} = 1$. Using only fundamental definitions of Krylov subspace methods and minimization principles, do the following:\n- Construct the refined Ritz vectors in the subspace $\\mathcal{K}_{2}(A, b)$ using $V_{2}$ and using $Q_{2}$, and justify whether these two refined Ritz vectors in $\\mathbb{R}^{4}$ agree up to a complex unit.\n- Assess and compute exactly the common residual norm $\\|(A - \\tau I) w_{\\mathrm{ref}}\\|_{2}$ attained by these refined Ritz vectors.\n\nYour final answer must be the exact value of this common residual norm as a single real number. No rounding is required.",
            "solution": "The problem asks for the minimum residual norm for a refined Ritz vector in the Krylov subspace $\\mathcal{K}_{2}(A, b)$. Since the Arnoldi and Lanczos processes generate bases for the same subspace, the resulting refined Ritz vector and its residual norm will be identical, regardless of the specific orthonormal basis used.\n\n**Step 1: Define the Krylov Subspace**\nFirst, we construct the basis for $\\mathcal{K}_{2}(A, b) = \\operatorname{span}\\{b, Ab\\}$.\nGiven $A = \\operatorname{diag}(1, 2, 4, 8)$ and $b = [1, 1, 0, 0]^{\\top}$.\nThe first vector is $b = [1, 1, 0, 0]^{\\top}$.\nThe second vector is $Ab = \\operatorname{diag}(1, 2, 4, 8)[1, 1, 0, 0]^{\\top} = [1, 2, 0, 0]^{\\top}$.\n\n**Step 2: Construct an Orthonormal Basis**\nWe apply the Gram-Schmidt process to $\\{b, Ab\\}$ to find an orthonormal basis $W = [w_1, w_2]$ for $\\mathcal{K}_{2}(A, b)$.\n1.  Normalize $b$:\n    $w_1 = \\frac{b}{\\|b\\|_2} = \\frac{1}{\\sqrt{2}}[1, 1, 0, 0]^{\\top}$.\n2.  Orthogonalize $Ab$ against $w_1$:\n    $\\tilde{w}_2 = Ab - (w_1^{\\top}Ab)w_1$.\n    The projection coefficient is $w_1^{\\top}Ab = \\frac{1}{\\sqrt{2}}[1, 1, 0, 0][1, 2, 0, 0]^{\\top} = \\frac{3}{\\sqrt{2}}$.\n    $\\tilde{w}_2 = [1, 2, 0, 0]^{\\top} - \\frac{3}{\\sqrt{2}} \\left( \\frac{1}{\\sqrt{2}}[1, 1, 0, 0]^{\\top} \\right) = [1, 2, 0, 0]^{\\top} - \\frac{3}{2}[1, 1, 0, 0]^{\\top} = [-1/2, 1/2, 0, 0]^{\\top}$.\n3.  Normalize $\\tilde{w}_2$:\n    $w_2 = \\frac{\\tilde{w}_2}{\\|\\tilde{w}_2\\|_2} = \\frac{1}{\\sqrt{1/4 + 1/4}}[-1/2, 1/2, 0, 0]^{\\top} = \\sqrt{2}[-1/2, 1/2, 0, 0]^{\\top} = \\frac{1}{\\sqrt{2}}[-1, 1, 0, 0]^{\\top}$.\nThe orthonormal basis matrix is $W = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\\\ 1  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}$.\n\n**Step 3: Solve the Minimization Problem**\nWe want to find $w_{\\mathrm{ref}} = Wy$ that minimizes $\\|(A-\\tau I)w\\|_2$ with $\\|w\\|_2=1$ and $\\tau=4$. This is equivalent to minimizing $\\|(A-4I)Wy\\|_2^2$ subject to $\\|y\\|_2=1$.\nThe objective function is $y^{\\top} W^{\\top} (A-4I)^2 W y$. The minimum value is the smallest eigenvalue of the matrix $M = W^{\\top} (A-4I)^2 W$. The required residual norm is the square root of this eigenvalue.\n\nFirst, compute $(A-4I)^2$:\n$A - 4I = \\operatorname{diag}(-3, -2, 0, 4)$.\n$(A - 4I)^2 = \\operatorname{diag}(9, 4, 0, 16)$.\n\nNext, compute $M$:\n$$\nM = W^{\\top} (A-4I)^2 W = \\frac{1}{2} \\begin{pmatrix} 1  1  0  0 \\\\ -1  1  0  0 \\end{pmatrix} \\begin{pmatrix} 9  0  0  0 \\\\ 0  4  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  16 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 1  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\n$$\nM = \\frac{1}{2} \\begin{pmatrix} 1  1  0  0 \\\\ -1  1  0  0 \\end{pmatrix} \\begin{pmatrix} 9  -9 \\\\ 4  4 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 13  -5 \\\\ -5  13 \\end{pmatrix}\n$$\n\n**Step 4: Find Eigenvalues of M**\nThe eigenvalues $\\lambda$ of $M$ are roots of the characteristic equation $\\det(M-\\lambda I)=0$:\n$$ \\left(\\frac{13}{2} - \\lambda\\right)^2 - \\left(-\\frac{5}{2}\\right)^2 = 0 $$\n$$ \\lambda^2 - 13\\lambda + \\frac{169}{4} - \\frac{25}{4} = 0 $$\n$$ \\lambda^2 - 13\\lambda + \\frac{144}{4} = 0 $$\n$$ \\lambda^2 - 13\\lambda + 36 = 0 $$\n$$ (\\lambda - 4)(\\lambda - 9) = 0 $$\nThe eigenvalues are $\\lambda_{\\min}=4$ and $\\lambda_{\\max}=9$.\n\nThe minimum squared residual norm is the smallest eigenvalue, $\\lambda_{\\min}=4$. The common residual norm is therefore $\\sqrt{\\lambda_{\\min}} = \\sqrt{4} = 2$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}