{
    "hands_on_practices": [
        {
            "introduction": "The Biconjugate Gradient method begins not with an iteration, but with a critical setup phase. This first practice focuses on the essential starting point: computing the initial residual $r_0$ and a corresponding shadow residual $\\tilde{r}_0$. By calculating the inner product $\\tilde{r}_0^T r_0$, you will verify the non-degenerate biorthogonality condition , which is the gatekeeper that ensures the BiCG algorithm can successfully launch.",
            "id": "3585481",
            "problem": "Consider a nonsymmetric linear system with coefficient matrix $A \\in \\mathbb{R}^{3 \\times 3}$, right-hand side $b \\in \\mathbb{R}^{3}$, and an initial guess $x_{0} \\in \\mathbb{R}^{3}$. The biconjugate gradient method (BiCG) for nonsymmetric systems couples two Krylov processes, one defined by $A$ and the other by $A^T$, and requires two initial residuals $r_{0}$ and $\\tilde{r}_{0}$ such that the coupling $\\tilde{r}_{0}^{T} r_{0}$ is nonzero. Starting from the fundamental definition of a residual for a linear system, the initial residual for the primal system is defined as $r_{0} = b - A x_{0}$. To initialize the shadow process, consider the transposed system $A^T y = \\tilde{b}$ together with an initial guess $y_{0}$, and define the shadow residual $\\tilde{r}_{0} = \\tilde{b} - A^T y_{0}$. Your task is to compute $r_{0}$ and $\\tilde{r}_{0}$ from the provided data and then evaluate the bilinear form $\\tilde{r}_{0}^{T} r_{0}$ to verify the nondegenerate biorthogonality condition required by BiCG. Use the following data:\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 3 & -2 \\\\\n0 & -1 & 1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 2\n\\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\ 1 \\\\ -1\n\\end{pmatrix},\n$$\nand for the shadow system,\n$$\n\\tilde{b} = \\begin{pmatrix}\n0 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\quad\ny_{0} = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nStarting from the definitions of $r_{0}$ and $\\tilde{r}_{0}$, compute $r_{0}$ and $\\tilde{r}_{0}$ exactly, and then evaluate $\\tilde{r}_{0}^{T} r_{0}$. Provide the value of $\\tilde{r}_{0}^{T} r_{0}$ as your final answer.",
            "solution": "The problem is valid as it is scientifically grounded in numerical linear algebra, well-posed with all necessary data provided, and objectively stated. We proceed with the solution.\n\nThe task is to compute the value of the bilinear form $\\tilde{r}_{0}^{T} r_{0}$, which is a crucial quantity for initiating the biconjugate gradient (BiCG) algorithm. This requires the computation of the initial primal residual, $r_{0}$, and the initial shadow residual, $\\tilde{r}_{0}$.\n\nThe given data includes the coefficient matrix $A$, the right-hand side vector $b$, and the initial guess $x_{0}$ for the primal system $A x = b$:\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 3 & -2 \\\\\n0 & -1 & 1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n2\n\\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\\n1 \\\\\n-1\n\\end{pmatrix}\n$$\nFor the auxiliary shadow system, we are given the right-hand side vector $\\tilde{b}$ and the initial guess $y_{0}$:\n$$\n\\tilde{b} = \\begin{pmatrix}\n0 \\\\\n1 \\\\\n1\n\\end{pmatrix}, \\quad\ny_{0} = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\n\nFirst, we compute the initial residual $r_{0}$ for the primal system using its definition, $r_{0} = b - A x_{0}$.\nWe begin by calculating the product $A x_{0}$:\n$$\nA x_{0} = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n1 & 3 & -2 \\\\\n0 & -1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0 \\\\\n1 \\\\\n-1\n\\end{pmatrix}\n= \\begin{pmatrix}\n(2)(0) + (-1)(1) + (0)(-1) \\\\\n(1)(0) + (3)(1) + (-2)(-1) \\\\\n(0)(0) + (-1)(1) + (1)(-1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 - 1 + 0 \\\\\n0 + 3 + 2 \\\\\n0 - 1 - 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n-1 \\\\\n5 \\\\\n-2\n\\end{pmatrix}\n$$\nNow we can find $r_0$ by subtracting this result from $b$:\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n2\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n-1 \\\\\n5 \\\\\n-2\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 - (-1) \\\\\n0 - 5 \\\\\n2 - (-2)\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 \\\\\n-5 \\\\\n4\n\\end{pmatrix}\n$$\n\nNext, we compute the initial residual $\\tilde{r}_{0}$ for the shadow system. The definition is $\\tilde{r}_{0} = \\tilde{b} - A^T y_{0}$.\nFirst, we determine the transpose of the matrix $A$, denoted by $A^T$:\n$$\nA^T = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n-1 & 3 & -1 \\\\\n0 & -2 & 1\n\\end{pmatrix}\n$$\nWe then calculate the product $A^T y_{0}$:\n$$\nA^T y_{0} = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n-1 & 3 & -1 \\\\\n0 & -2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n= \\begin{pmatrix}\n(2)(1) + (1)(0) + (0)(0) \\\\\n(-1)(1) + (3)(0) + (-1)(0) \\\\\n(0)(1) + (-2)(0) + (1)(0)\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n0\n\\end{pmatrix}\n$$\nNow we can find $\\tilde{r}_0$ by subtracting this result from $\\tilde{b}$:\n$$\n\\tilde{r}_{0} = \\tilde{b} - A^T y_{0} = \\begin{pmatrix}\n0 \\\\\n1 \\\\\n1\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n0\n\\end{pmatrix}\n= \\begin{pmatrix}\n0 - 2 \\\\\n1 - (-1) \\\\\n1 - 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n-2 \\\\\n2 \\\\\n1\n\\end{pmatrix}\n$$\n\nFinally, we evaluate the bilinear form $\\tilde{r}_{0}^{T} r_{0}$, which is the dot product of the vectors $\\tilde{r}_{0}$ and $r_{0}$.\n$$\n\\tilde{r}_{0}^{T} r_{0} = \\begin{pmatrix}\n-2 & 2 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\\n-5 \\\\\n4\n\\end{pmatrix}\n$$\n$$\n\\tilde{r}_{0}^{T} r_{0} = (-2)(2) + (2)(-5) + (1)(4) = -4 - 10 + 4 = -10\n$$\nThe condition $\\tilde{r}_{0}^{T} r_{0} \\neq 0$ is satisfied, which ensures that the BiCG algorithm can be initiated without breakdown at the first step. The computed value is $-10$.",
            "answer": "$$\\boxed{-10}$$"
        },
        {
            "introduction": "Once successfully initialized, the BiCG algorithm proceeds through a sequence of iterations that systematically refine the solution. This next exercise takes you through two full cycles of the BiCG machinery, from calculating step sizes like $\\alpha_k$ and recurrence coefficients $\\beta_k$ to updating residuals and search directions. By performing these steps manually , you will gain a concrete understanding of how the coupled short-recurrence relations work in concert to build the solution.",
            "id": "3585465",
            "problem": "Consider the task of solving the nonsymmetric linear system $A x = b$ by performing exactly two iterations of the Biconjugate Gradient (BiCG) method (without preconditioning) in exact arithmetic, starting from $x_{0} = 0$ and taking the shadow residual equal to the initial residual. The system data are\n$$\nA = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n1 & 0 & 2\n\\end{pmatrix},\n\\qquad\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nLet the initial residual be $r_{0} = b - A x_{0}$ and take the shadow residual $s_{0} = r_{0}$. Use the standard real-arithmetic Biconjugate Gradient (BiCG) method, defined through bi-orthogonality of the residuals with respect to $A$ and $A^T$, and coupled short recurrences for search directions driven by three-term relations, to:\n- Construct and report all intermediate scalars and vectors generated during the first two iterations, including $r_{k}$, $\\tilde{r}_{k}$, $p_{k}$, $\\tilde{p}_{k}$, $v_{k} = A p_{k}$, $\\tilde{v}_{k} = A^T \\tilde{p}_{k}$, and the scalars $\\rho_{k}$, $\\alpha_{k}$, $\\beta_{k}$, together with $x_{1}$ and $x_{2}$.\n- Verify nonbreakdown at each step by confirming that all required inner products used as denominators are nonzero.\n\nFinally, provide the exact value of the Euclidean norm $\\|r_{2}\\|_{2}$ as a simplified analytic expression. Do not round; report the exact value.",
            "solution": "The problem is valid as it is a well-defined exercise in numerical linear algebra, based on the standard Biconjugate Gradient (BiCG) method, with all necessary data provided and no internal contradictions or scientific flaws. We proceed with the solution.\n\nThe Biconjugate Gradient (BiCG) method is an iterative algorithm for solving nonsymmetric linear systems $A x = b$. The algorithm generates sequences of residuals $r_k$ and shadow residuals $\\tilde{r}_k$, along with corresponding search directions $p_k$ and $\\tilde{p}_k$. The defining properties are the biorthogonality of the residuals, $\\tilde{r}_j^T r_k = 0$ for $j \\neq k$, and the A-biorthogonality (or biconjugacy) of the search directions, $\\tilde{p}_j^T A p_k = 0$ for $j \\neq k$.\n\nThe algorithm starts with an initial guess $x_0$ and proceeds as follows:\n1.  Initialize:\n    $r_0 = b - A x_0$\n    Choose $\\tilde{r}_0$ (here, $\\tilde{r}_0 = r_0$)\n    $p_0 = r_0$, $\\tilde{p}_0 = \\tilde{r}_0$\n2.  For $k = 0, 1, 2, \\dots$:\n    $\\rho_k = \\tilde{r}_k^T r_k$\n    $v_k = A p_k$\n    $\\alpha_k = \\rho_k / (\\tilde{p}_k^T v_k)$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k v_k$\n    $\\tilde{r}_{k+1} = \\tilde{r}_k - \\alpha_k A^T \\tilde{p}_k$\n    $\\rho_{k+1} = \\tilde{r}_{k+1}^T r_{k+1}$\n    $\\beta_k = \\rho_{k+1} / \\rho_k$\n    $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    $\\tilde{p}_{k+1} = \\tilde{r}_{k+1} + \\beta_k \\tilde{p}_k$\n\nWe are given the system data:\n$$A = \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 1 & 0 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nThe transpose of $A$ is:\n$$A^T = \\begin{pmatrix} 2 & 0 & 1 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$$\n\n**Initialization ($k=0$)**\nThe initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nThe initial shadow residual is taken as $\\tilde{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nThe initial search directions are $p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\tilde{p}_0 = \\tilde{r}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\n**First Iteration ($k=0$)**\nWe compute the scalar $\\rho_0$:\n$$\\rho_0 = \\tilde{r}_0^T r_0 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 1 + 0 \\cdot 0 = 2$$\nThe vector $v_0 = A p_0$ is:\n$$v_0 = A p_0 = \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 1 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix}$$\nThe vector $\\tilde{v}_0 = A^T \\tilde{p}_0$ is:\n$$\\tilde{v}_0 = A^T \\tilde{p}_0 = \\begin{pmatrix} 2 & 0 & 1 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix}$$\nThe denominator for $\\alpha_0$ is $\\tilde{p}_0^T v_0 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} = 3+3 = 6$.\n*Non-breakdown check:* Both $\\rho_0 = 2 \\neq 0$ and $\\tilde{p}_0^T A p_0 = 6 \\neq 0$, so the method can proceed.\nThe step size $\\alpha_0$ is:\n$$\\alpha_0 = \\frac{\\rho_0}{\\tilde{p}_0^T v_0} = \\frac{2}{6} = \\frac{1}{3}$$\nWe update the solution vector to obtain $x_1$:\n$$x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 0 \\end{pmatrix}$$\nThe new residual $r_1$ is:\n$$r_1 = r_0 - \\alpha_0 v_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 1-1 \\\\ 0-1/3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix}$$\nThe new shadow residual $\\tilde{r}_1$ is:\n$$\\tilde{r}_1 = \\tilde{r}_0 - \\alpha_0 \\tilde{v}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-2/3 \\\\ 1-4/3 \\\\ 0-1/3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix}$$\nWe compute $\\rho_1$ for the next step:\n$$\\rho_1 = \\tilde{r}_1^T r_1 = \\begin{pmatrix} 1/3 & -1/3 & -1/3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} = 0 + 0 + \\frac{1}{9} = \\frac{1}{9}$$\nThe scalar $\\beta_0$ is:\n$$\\beta_0 = \\frac{\\rho_1}{\\rho_0} = \\frac{1/9}{2} = \\frac{1}{18}$$\nThe new search directions $p_1$ and $\\tilde{p}_1$ are:\n$$p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} + \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/18 \\\\ 1/18 \\\\ -6/18 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix}$$\n$$\\tilde{p}_1 = \\tilde{r}_1 + \\beta_0 \\tilde{p}_0 = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} + \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6/18 + 1/18 \\\\ -6/18 + 1/18 \\\\ -6/18 \\end{pmatrix} = \\begin{pmatrix} 7/18 \\\\ -5/18 \\\\ -6/18 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 7 \\\\ -5 \\\\ -6 \\end{pmatrix}$$\n\n**Second Iteration ($k=1$)**\nWe start with $\\rho_1 = 1/9$.\nThe vector $v_1 = A p_1$ is:\n$$v_1 = A p_1 = \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 1 & 0 & 2 \\end{pmatrix} \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 2(1)+1(1) \\\\ 3(1)+1(-6) \\\\ 1(1)+2(-6) \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix}$$\nThe vector $\\tilde{v}_1 = A^T \\tilde{p}_1$ is:\n$$\\tilde{v}_1 = A^T \\tilde{p}_1 = \\begin{pmatrix} 2 & 0 & 1 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\frac{1}{18} \\begin{pmatrix} 7 \\\\ -5 \\\\ -6 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 2(7)+1(-6) \\\\ 1(7)+3(-5) \\\\ 1(-5)+2(-6) \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 8 \\\\ -8 \\\\ -17 \\end{pmatrix}$$\nThe denominator for $\\alpha_1$ is $\\tilde{p}_1^T v_1 = \\left( \\frac{1}{18} \\begin{pmatrix} 7 & -5 & -6 \\end{pmatrix} \\right) \\left( \\frac{1}{18} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix} \\right) = \\frac{1}{324} (21+15+66) = \\frac{102}{324} = \\frac{17}{54}$.\n*Non-breakdown check:* We have $\\rho_1 = 1/9 \\neq 0$ and $\\tilde{p}_1^T A p_1 = 17/54 \\neq 0$. The method proceeds without breakdown.\nThe step size $\\alpha_1$ is:\n$$\\alpha_1 = \\frac{\\rho_1}{\\tilde{p}_1^T v_1} = \\frac{1/9}{17/54} = \\frac{1}{9} \\cdot \\frac{54}{17} = \\frac{6}{17}$$\nWe update the solution vector to obtain $x_2$:\n$$x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 0 \\end{pmatrix} + \\frac{6}{17} \\left(\\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix}\\right) = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 0 \\end{pmatrix} + \\frac{1}{51} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 17/51+1/51 \\\\ 17/51+1/51 \\\\ -6/51 \\end{pmatrix} = \\begin{pmatrix} 18/51 \\\\ 18/51 \\\\ -6/51 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 6 \\\\ 6 \\\\ -2 \\end{pmatrix}$$\nThe new residual $r_2$ is:\n$$r_2 = r_1 - \\alpha_1 v_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} - \\frac{6}{17} \\left(\\frac{1}{18} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix}\\right) = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} - \\frac{1}{51} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix} = \\begin{pmatrix} -3/51 \\\\ 3/51 \\\\ -17/51+11/51 \\end{pmatrix} = \\begin{pmatrix} -3/51 \\\\ 3/51 \\\\ -6/51 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix}$$\nWe report the other quantities generated during this iteration as requested. The new shadow residual $\\tilde{r}_2$ is:\n$$\\tilde{r}_2 = \\tilde{r}_1 - \\alpha_1 \\tilde{v}_1 = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} - \\frac{6}{17} \\left(\\frac{1}{18} \\begin{pmatrix} 8 \\\\ -8 \\\\ -17 \\end{pmatrix}\\right) = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} - \\frac{1}{51} \\begin{pmatrix} 8 \\\\ -8 \\\\ -17 \\end{pmatrix} = \\begin{pmatrix} 17/51-8/51 \\\\ -17/51+8/51 \\\\ -17/51+17/51 \\end{pmatrix} = \\begin{pmatrix} 9/51 \\\\ -9/51 \\\\ 0 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 3 \\\\ -3 \\\\ 0 \\end{pmatrix}$$\nFinally, we compute $\\rho_2$ and $\\beta_1$:\n$$\\rho_2 = \\tilde{r}_2^T r_2 = \\left( \\frac{1}{17} \\begin{pmatrix} 3 & -3 & 0 \\end{pmatrix} \\right) \\left( \\frac{1}{17} \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right) = \\frac{1}{289}(-3-3+0) = -\\frac{6}{289}$$\n$$\\beta_1 = \\frac{\\rho_2}{\\rho_1} = \\frac{-6/289}{1/9} = -\\frac{54}{289}$$\n\nThe problem requires the exact value of the Euclidean norm of $r_2$.\n$$\\|r_2\\|_2 = \\left\\| \\frac{1}{17} \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right\\|_2 = \\frac{1}{17} \\left\\| \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right\\|_2 = \\frac{1}{17} \\sqrt{(-1)^2 + 1^2 + (-2)^2} = \\frac{1}{17} \\sqrt{1+1+4} = \\frac{\\sqrt{6}}{17}$$",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{6}}{17}}\n$$"
        },
        {
            "introduction": "Theoretical algorithms must ultimately face the realities of finite-precision arithmetic and practical problem structures. The performance of BiCG is famously sensitive, and its stability depends on the subtle interplay between the system matrix and the initial vectors. This hands-on coding practice  invites you to build a numerical experiment to explore this sensitivity, connecting the abstract theory of eigenvectors and bi-orthogonality to the concrete phenomena of convergence speed and algorithmic breakdown.",
            "id": "3585435",
            "problem": "Consider solving the linear system $A x = b$ for a real, square, nonsymmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $b \\in \\mathbb{R}^n$ using the Biconjugate Gradient (BiCG) method. The BiCG method is a Krylov subspace method characterized by a Petrovâ€“Galerkin condition: residuals are orthogonal with respect to a bilinear form between a right Krylov subspace generated by $A$ and a left Krylov subspace generated by $A^T$. The method evolves $(x_k, r_k, \\tilde{r}_k)$ in iterates, where $r_k = b - A x_k$ is the residual and $\\tilde{r}_k$ is a shadow residual, and makes use of scalar quantities obtained from inner products involving $A$ and $A^T$. The method may break down if certain inner products vanish (either exactly or effectively due to finite precision), leading to undefined steps. Your task is to implement a controlled numerical experiment illustrating sensitive dependence of BiCG convergence on the initial alignment of the residual and shadow residual with left and right eigenvectors of $A$.\n\nYou must construct $A$ as a diagonalizable, non-normal matrix with real spectrum via $A = S \\Lambda S^{-1}$, where $S \\in \\mathbb{R}^{n \\times n}$ is invertible and $\\Lambda \\in \\mathbb{R}^{n \\times n}$ is diagonal with distinct positive entries. Let $n = 10$ and let $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ with $\\lambda_i$ strictly positive and distinct. Use a fixed random seed to generate $S$ so that the experiment is fully reproducible.\n\nDefine the right eigenvector $v_i = S e_i$ and the left eigenvector $w_i$ associated with $\\lambda_i$ as the column of $S^{-T}$ at index $i$, that is $w_i = S^{-T} e_i$, where $e_i$ is the $i$th standard basis vector in $\\mathbb{R}^n$. Choose the index $i^\\star = 3$ and set $b = v_{i^\\star}$, $x_0 = 0$, $r_0 = b$. Consider the following experiment design:\n\n- Perfect alignment case: Set the initial shadow residual $\\tilde{r}_0 = w_{i^\\star}$ and run BiCG until the relative residual $\\|r_k\\|_2 / \\|b\\|_2$ is at most $\\tau = 10^{-12}$ or until a maximum number of iterations $m = 2 n$ is reached. Use a breakdown detection threshold $\\delta = 10^{-15}$ for any scalar denominator produced by BiCG that must be inverted. Record the iteration count at convergence and whether a breakdown occurred.\n\n- Perturbation cases: For $\\varepsilon \\in \\{10^{-12}, 10^{-6}, 10^{-2}\\}$, perform $T = 20$ independent trials per $\\varepsilon$, each trial taking $\\tilde{r}_0 = w_{i^\\star} + \\varepsilon \\xi$, where $\\xi \\in \\mathbb{R}^n$ is a vector with independent standard normal entries generated under the same fixed random seed.\n    - For each trial, run BiCG with the same stopping tolerance $\\tau$, iteration cap $m$, and breakdown threshold $\\delta$.\n    - For each $\\varepsilon$, compute the average number of iterations over the successful runs (those achieving the tolerance without breakdown); if all trials for a given $\\varepsilon$ break down, define the average as $-1.0$. Also record the total number of breakdowns among the $T$ trials.\n\n- True breakdown edge case: Set $\\tilde{r}_0 = w_j$ for $j \\neq i^\\star$ (choose $j = (i^\\star \\bmod n) + 1$ to ensure $j \\neq i^\\star$) and run BiCG once. In this case, $\\tilde{r}_0^T r_0 = 0$ and the method encounters a true breakdown at the first iteration. Record the boolean indicating whether breakdown is detected.\n\nYour program must implement the BiCG method directly in terms of matrix-vector products with $A$ and $A^T$, and must include explicit detection of breakdown when any required scalar denominator is below $\\delta$ in absolute value.\n\nTest suite specification:\n- Matrix size: $n = 10$.\n- Eigenvalues: $\\lambda_i$ are strictly positive and distinct, chosen deterministically.\n- Random seed: fixed and used for constructing $S$ and all perturbations.\n- Tolerances: relative residual tolerance $\\tau = 10^{-12}$, maximum iterations $m = 2 n$, breakdown threshold $\\delta = 10^{-15}$.\n- Cases:\n    1. Perfect alignment: $\\tilde{r}_0 = w_{i^\\star}$.\n    2. Perturbations: $\\varepsilon \\in \\{10^{-12}, 10^{-6}, 10^{-2}\\}$, $T=20$ trials each.\n    3. True breakdown: $\\tilde{r}_0 = w_j$ with $j \\neq i^\\star$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order:\n    1. The integer iteration count for the perfect alignment case.\n    2. The boolean breakdown flag for the perfect alignment case.\n    3. The floating-point average iteration count over successful runs for $\\varepsilon = 10^{-12}$ (or $-1.0$ if all trials break down).\n    4. The integer count of breakdowns among the $T$ trials for $\\varepsilon = 10^{-12}$.\n    5. The floating-point average iteration count over successful runs for $\\varepsilon = 10^{-6}$ (or $-1.0$ if all trials break down).\n    6. The integer count of breakdowns among the $T$ trials for $\\varepsilon = 10^{-6}$.\n    7. The floating-point average iteration count over successful runs for $\\varepsilon = 10^{-2}$ (or $-1.0$ if all trials break down).\n    8. The integer count of breakdowns among the $T$ trials for $\\varepsilon = 10^{-2}$.\n    9. The boolean breakdown flag for the true breakdown edge case.\n\nAll entries in the output list must be of primitive types: integers, floating-point numbers, and booleans. No physical units are involved in this problem. Angles are not involved.",
            "solution": "The problem requires the implementation of a numerical experiment to demonstrate the sensitivity of the Biconjugate Gradient (BiCG) method's convergence to the initial alignment of the residual and shadow residual vectors. This involves constructing a non-normal matrix with a known eigensystem and running BiCG with specific initial vectors designed to probe its stability and breakdown conditions.\n\nThe BiCG method is an iterative algorithm for solving large, sparse, non-symmetric linear systems of equations $A x = b$. It is a Krylov subspace method that generates a sequence of approximate solutions $x_k$ such that the residual $r_k = b - A x_k$ satisfies a Petrov-Galerkin condition. This condition requires the residual $r_k$ to be orthogonal to a Krylov subspace generated by the transpose of the matrix, $A^T$. Specifically, $r_k \\perp \\mathcal{K}_k(A^T, \\tilde{r}_0)$, where $\\mathcal{K}_k(A^T, \\tilde{r}_0) = \\mathrm{span}\\{\\tilde{r}_0, A^T \\tilde{r}_0, \\dots, (A^T)^{k-1} \\tilde{r}_0\\}$ is the \"left\" or \"shadow\" Krylov subspace, and $\\tilde{r}_0$ is a chosen initial shadow residual. The iterates $x_k$ are chosen from the affine Krylov subspace $x_0 + \\mathcal{K}_k(A, r_0)$.\n\nThe standard BiCG algorithm can be summarized as follows. Given an initial guess $x_0$ and an initial shadow residual $\\tilde{r}_0$:\n\n1.  Initialize:\n    $r_0 = b - A x_0$\n    $p_0 = r_0$\n    $\\tilde{p}_0 = \\tilde{r}_0$\n    $\\rho_0 = \\tilde{r}_0^T r_0$\n\n2.  Iterate for $k = 0, 1, 2, \\ldots$:\n    a.  $v_k = A p_k$\n    b.  $\\alpha_k = \\frac{\\rho_k}{\\tilde{p}_k^T v_k}$\n    c.  $x_{k+1} = x_k + \\alpha_k p_k$\n    d.  $r_{k+1} = r_k - \\alpha_k v_k$\n    e.  $\\tilde{r}_{k+1} = \\tilde{r}_k - \\alpha_k A^T\\tilde{p}_k$\n    f.  $\\rho_{k+1} = \\tilde{r}_{k+1}^T r_{k+1}$\n    g.  $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$\n    h.  $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    i.  $\\tilde{p}_{k+1} = \\tilde{r}_{k+1} + \\beta_k \\tilde{p}_k$\n\nThe algorithm requires the inversion of two scalar quantities at each iteration: $\\rho_k = \\tilde{r}_k^T r_k$ and $\\tilde{p}_k^T A p_k$. If either of these quantities is zero (or numerically close to zero), the algorithm breaks down.\n-   A breakdown where $\\rho_k \\approx 0$ is often called a \"true\" breakdown. It implies that the new residual $r_k$ is nearly orthogonal to the shadow residual $\\tilde{r}_k$.\n-   A breakdown where $\\tilde{p}_k^T A p_k \\approx 0$ is a \"Lanczos-type\" breakdown.\n\nThe experiment is designed to investigate these breakdown conditions by carefully selecting the matrix $A$ and the initial vectors. The matrix $A$ is constructed as $A = S \\Lambda S^{-1}$, where $\\Lambda$ is a diagonal matrix of eigenvalues $\\lambda_i$. The columns of $S$, denoted $v_i$, are the right eigenvectors of $A$, and the columns of $S^{-T}$, denoted $w_i$, are the left eigenvectors of $A$. They satisfy $A v_i = \\lambda_i v_i$ and $A^T w_i = \\lambda_i w_i$. A key property of these vectors is their bi-orthogonality: $w_j^T v_i = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nThe experiment sets the initial residual $r_0 = b = v_{i^\\star}$ for a chosen index $i^\\star$. The initial shadow residual $\\tilde{r}_0$ is then varied.\n\n**Perfect Alignment Case:** We set $\\tilde{r}_0 = w_{i^\\star}$. The initial inner product is $\\rho_0 = \\tilde{r}_0^T r_0 = w_{i^\\star}^T v_{i^\\star} = 1$. Since $r_0$ is an eigenvector, the Krylov subspace $\\mathcal{K}_k(A, r_0)$ is one-dimensional for any $k \\geq 1$. The exact solution is $x = (1/\\lambda_{i^\\star}) v_{i^\\star}$, and BiCG should find a solution proportional to $v_{i^\\star}$ in a single iteration. The residual $r_1$ should become zero (or numerically negligible), leading to convergence in one step.\n\n**True Breakdown Edge Case:** We set $\\tilde{r}_0 = w_j$ for $j \\neq i^\\star$. Due to bi-orthogonality, the initial inner product is $\\rho_0 = \\tilde{r}_0^T r_0 = w_j^T v_{i^\\star} = 0$. This forces an immediate breakdown at iteration $0$ when computing $\\beta_{-1}$ (notional) or attempting to compute $\\alpha_0$ in some formulations. In our chosen algorithm, this leads to an initial breakdown since $\\rho_0$ is zero.\n\n**Perturbation Cases:** We set $\\tilde{r}_0 = w_{i^\\star} + \\varepsilon \\xi$, where $\\xi$ is a random vector. For very small $\\varepsilon$, $\\tilde{r}_0$ is nearly aligned with $w_{i^\\star}$ but contains small components of other left eigenvectors.\n$\\rho_0 = (w_{i^\\star} + \\varepsilon \\xi)^T v_{i^\\star} = w_{i^\\star}^T v_{i^\\star} + \\varepsilon \\xi^T v_{i^\\star} = 1 + \\varepsilon (\\xi^T v_{i^\\star})$.\nWhile $\\rho_0$ is not zero, the subsequent iterates $\\rho_k$ can become very small, leading to near-breakdown conditions. This can cause numerical instability, manifesting as erratic convergence or a breakdown after several iterations. As $\\varepsilon$ increases, $\\tilde{r}_0$ becomes a more \"generic\" vector, and the performance of BiCG is expected to become more typical, converging in a number of iterations related to the conditioning of $A$ and the distribution of its eigenvalues, which for $n=10$ should be small. The experiment quantifies this behavior by measuring the average iteration count and the frequency of breakdowns for different perturbation magnitudes $\\varepsilon$.\n\nThe implementation will construct the specified matrix $A \\in \\mathbb{R}^{10 \\times 10}$, execute the BiCG algorithm with careful monitoring of the denominators $\\rho_k$ and $\\tilde{p}_k^T A p_k$, and record the outcomes for each experimental case. This will furnish a quantitative demonstration of the theoretical properties and failure modes of the BiCG method.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef run_bicg(A, b, x0, r0_hat, tol, max_iter, breakdown_thresh):\n    \"\"\"\n    Implements the Biconjugate Gradient (BiCG) method.\n    \n    Returns:\n        (int, bool): A tuple containing the number of iterations and a boolean breakdown flag.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    r_hat = r0_hat.copy()\n\n    norm_b = np.linalg.norm(b)\n    if norm_b == 0.0:\n        return 0, False\n\n    rho = np.dot(r_hat, r)\n    if abs(rho) < breakdown_thresh:\n        return 0, True  # Initial breakdown\n\n    p = r.copy()\n    p_hat = r_hat.copy()\n    A_T = A.T\n\n    for k in range(max_iter):\n        v = A @ p\n        denom_alpha = np.dot(p_hat, v)\n\n        if abs(denom_alpha) < breakdown_thresh:\n            return k, True\n\n        alpha = rho / denom_alpha\n        x += alpha * p\n        r -= alpha * v\n        \n        if np.linalg.norm(r) / norm_b <= tol:\n            return k + 1, False\n\n        r_hat -= alpha * (A_T @ p_hat)\n\n        rho_next = np.dot(r_hat, r)\n        \n        if abs(rho_next) < breakdown_thresh:\n            # Breakdown detected after a successful step\n            return k + 1, True\n\n        beta = rho_next / rho\n        p = r + beta * p\n        p_hat = r_hat + beta * p_hat\n        rho = rho_next\n\n    return max_iter, False # Failed to converge within max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment and print results.\n    \"\"\"\n    # Test suite specification\n    n = 10\n    i_star = 3\n    i_star_idx = i_star - 1\n    tau = 1e-12\n    m = 2 * n\n    delta = 1e-15\n    epsilons = [1e-12, 1e-6, 1e-2]\n    T = 20\n    seed = 42\n    \n    rng = np.random.default_rng(seed)\n\n    # Construct the matrix A\n    Lambda_diag = np.arange(1.0, n + 1.0)\n    Lambda = np.diag(Lambda_diag)\n    S = rng.random((n, n))\n    S_inv = np.linalg.inv(S)\n    A = S @ Lambda @ S_inv\n    \n    # Get right and left eigenvectors\n    S_inv_T = S_inv.T\n    v_istar = S[:, i_star_idx]\n    w_istar = S_inv_T[:, i_star_idx]\n    \n    # Common setup\n    b = v_istar.copy()\n    x0 = np.zeros(n)\n    \n    results = []\n\n    # Case 1: Perfect alignment\n    r0_hat_perfect = w_istar.copy()\n    iters_perfect, breakdown_perfect = run_bicg(A, b, x0, r0_hat_perfect, tau, m, delta)\n    results.extend([iters_perfect, breakdown_perfect])\n\n    # Case 2: Perturbation cases\n    for eps in epsilons:\n        total_iterations = 0\n        successful_runs = 0\n        breakdown_count = 0\n        \n        for _ in range(T):\n            xi = rng.standard_normal(n)\n            r0_hat_pert = w_istar + eps * xi\n            iters, breakdown = run_bicg(A, b, x0, r0_hat_pert, tau, m, delta)\n            \n            if breakdown:\n                breakdown_count += 1\n            else:\n                successful_runs += 1\n                total_iterations += iters\n        \n        if successful_runs > 0:\n            avg_iters = float(total_iterations) / successful_runs\n        else:\n            avg_iters = -1.0\n            \n        results.extend([avg_iters, breakdown_count])\n\n    # Case 3: True breakdown edge case\n    j = (i_star % n) + 1\n    j_idx = j - 1\n    w_j = S_inv_T[:, j_idx]\n    \n    r0_hat_breakdown = w_j.copy()\n    _, breakdown_true = run_bicg(A, b, x0, r0_hat_breakdown, tau, m, delta)\n    results.append(breakdown_true)\n\n    # Format and print the final output\n    print(f\"[{results[0]},{str(results[1]).lower()},{results[2]},{results[3]},{results[4]},{results[5]},{results[6]},{results[7]},{str(results[8]).lower()}]\")\n\nsolve()\n\n```"
        }
    ]
}