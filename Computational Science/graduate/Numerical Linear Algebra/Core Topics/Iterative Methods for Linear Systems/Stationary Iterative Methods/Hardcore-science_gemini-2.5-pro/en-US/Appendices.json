{
    "hands_on_practices": [
        {
            "introduction": "Understanding a numerical method begins with a direct, step-by-step application. This exercise invites you to perform the first few iterations of the Jacobi method from its foundational matrix splitting, $A=M-N$. By calculating the iterates and comparing them to the exact solution , you will gain a concrete feel for how the approximation evolves and how the error behaves in the initial stages of the process.",
            "id": "3581650",
            "problem": "Let $A=\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}$, $b=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, and initial iterate $x^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$. Starting from the definition of a stationary iterative method for solving the linear system $A x = b$ via a splitting $A=M-N$ that produces an affine iteration $x^{k+1}=G x^{k}+c$, specialize this construction to the Jacobi method by choosing an appropriate $M$ and $N$, and derive the corresponding iteration matrix and affine term for this $A$ and $b$. Then, using only this first-principles construction, perform two steps of the Jacobi method to obtain $x^{1}$ and $x^{2}$. Compute the exact solution $x^{\\star}$ of $A x=b$, and hence the error vectors $e^{1}=x^{1}-x^{\\star}$ and $e^{2}=x^{2}-x^{\\star}$. Finally, determine the Euclidean norm $\\|e^{2}\\|_{2}$ in exact form. Give the exact value (no rounding). Your final reported answer must be the single exact value of $\\|e^{2}\\|_{2}$ with no additional commentary.",
            "solution": "The user has provided a well-posed problem in numerical linear algebra. The problem statement is scientifically sound, self-contained, and objective. All data required for the solution are present and consistent. I will proceed with the solution.\n\nThe problem asks to solve the linear system $A x = b$ using the Jacobi method, where\n$$A=\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}, \\quad b=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$$\nwith an initial guess $x^{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nA stationary iterative method is derived from a splitting of the matrix $A$ into $A = M-N$, where $M$ is non-singular. The iteration is given by $M x^{k+1} = N x^{k} + b$, which can be rewritten as\n$$x^{k+1} = M^{-1}N x^{k} + M^{-1}b$$\nThis corresponds to the general affine iteration form $x^{k+1} = Gx^k + c$, with the iteration matrix $G=M^{-1}N$ and the affine term $c=M^{-1}b$.\n\nFor the Jacobi method, we use the standard decomposition of the matrix $A = D - L - U$, where $D$ is the diagonal of $A$, $-L$ is the strictly lower part, and $-U$ is the strictly upper part. This implies that $L$ and $U$ have non-negative entries. For the given matrix $A$, we have:\n$$D = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix}, \\quad L = \\begin{pmatrix}0  0 \\\\ 1  0\\end{pmatrix}, \\quad U = \\begin{pmatrix}0  1 \\\\ 0  0\\end{pmatrix}$$\nThe Jacobi method splitting is defined by choosing $M=D$ and $N=L+U$. The iteration is therefore $D x^{k+1} = (L+U) x^{k} + b$.\nThe corresponding iteration matrix $G_J$ and vector $c_J$ are:\n$$G_J = M^{-1}N = D^{-1}(L+U)$$\n$$c_J = M^{-1}b = D^{-1}b$$\nFirst, we find the inverse of $D$:\n$$D^{-1} = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix}^{-1} = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ 0  \\frac{1}{2}\\end{pmatrix}$$\nNext, we calculate $G_J$:\n$$L+U = \\begin{pmatrix}0  0 \\\\ 1  0\\end{pmatrix} + \\begin{pmatrix}0  1 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix}$$\n$$G_J = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ 0  \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}0  1 \\\\ 1  0\\end{pmatrix} = \\begin{pmatrix}0  \\frac{1}{2} \\\\ \\frac{1}{2}  0\\end{pmatrix}$$\nNow, we calculate $c_J$:\n$$c_J = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ 0  \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix}$$\nThe Jacobi iteration for this specific problem is $x^{k+1} = G_J x^{k} + c_J$:\n$$x^{k+1} = \\begin{pmatrix}0  \\frac{1}{2} \\\\ \\frac{1}{2}  0\\end{pmatrix} x^k + \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix}$$\nWe are asked to perform two steps of this method starting from $x^{0} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nStep 1 (k=0):\n$$x^{1} = G_J x^{0} + c_J = \\begin{pmatrix}0  \\frac{1}{2} \\\\ \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} + \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix} + \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix}$$\nSo, $x^{1} = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix}$.\n\nStep 2 (k=1):\n$$x^{2} = G_J x^{1} + c_J = \\begin{pmatrix}0  \\frac{1}{2} \\\\ \\frac{1}{2}  0\\end{pmatrix} \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} + \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ \\frac{1}{4}\\end{pmatrix} + \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{1}{4}\\end{pmatrix}$$\nSo, $x^{2} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{1}{4}\\end{pmatrix}$.\n\nNext, we compute the exact solution $x^{\\star}$ of $Ax=b$.\n$$\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix} \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$$\nThis gives the system of linear equations:\n$$2x_1 - x_2 = 1$$\n$$-x_1 + 2x_2 = 0$$\nFrom the second equation, we have $x_1 = 2x_2$. Substituting this into the first equation:\n$$2(2x_2) - x_2 = 1 \\implies 4x_2 - x_2 = 1 \\implies 3x_2 = 1 \\implies x_2 = \\frac{1}{3}$$\nThen, $x_1 = 2(\\frac{1}{3}) = \\frac{2}{3}$.\nThe exact solution is $x^{\\star} = \\begin{pmatrix}\\frac{2}{3} \\\\ \\frac{1}{3}\\end{pmatrix}$.\n\nNow, we compute the error vectors $e^{1} = x^{1} - x^{\\star}$ and $e^{2} = x^{2} - x^{\\star}$.\n$$e^{1} = \\begin{pmatrix}\\frac{1}{2} \\\\ 0\\end{pmatrix} - \\begin{pmatrix}\\frac{2}{3} \\\\ \\frac{1}{3}\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{6} - \\frac{4}{6} \\\\ 0 - \\frac{1}{3}\\end{pmatrix} = \\begin{pmatrix}-\\frac{1}{6} \\\\ -\\frac{1}{3}\\end{pmatrix}$$\n$$e^{2} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{1}{4}\\end{pmatrix} - \\begin{pmatrix}\\frac{2}{3} \\\\ \\frac{1}{3}\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{6} - \\frac{4}{6} \\\\ \\frac{3}{12} - \\frac{4}{12}\\end{pmatrix} = \\begin{pmatrix}-\\frac{1}{6} \\\\ -\\frac{1}{12}\\end{pmatrix}$$\nFinally, we compute the Euclidean norm of the error vector $e^{2}$, denoted by $\\|e^{2}\\|_{2}$.\n$$\\|e^{2}\\|_{2} = \\sqrt{\\left(-\\frac{1}{6}\\right)^2 + \\left(-\\frac{1}{12}\\right)^2} = \\sqrt{\\frac{1}{36} + \\frac{1}{144}}$$\nTo sum the fractions, we find a common denominator, which is $144$.\n$$\\|e^{2}\\|_{2} = \\sqrt{\\frac{4}{144} + \\frac{1}{144}} = \\sqrt{\\frac{5}{144}} = \\frac{\\sqrt{5}}{\\sqrt{144}} = \\frac{\\sqrt{5}}{12}$$\nThe exact value of the Euclidean norm of the error after two iterations is $\\frac{\\sqrt{5}}{12}$.",
            "answer": "$$\\boxed{\\frac{\\sqrt{5}}{12}}$$"
        },
        {
            "introduction": "While several iterative methods can solve a given system, their efficiency can vary dramatically. The rate of convergence is governed by the spectral radius, $\\rho(T)$, of the iteration matrix $T$. This practice guides you through deriving the iteration matrices for the Jacobi, Gauss-Seidel, and SOR methods for a model problem and then asks you to compute their respective spectral radii , providing a direct, quantitative comparison of their theoretical performance.",
            "id": "3581651",
            "problem": "Consider the linear system $A x = b$ with\n$$\nA = \\begin{pmatrix}\n3  -1  0 \\\\\n-1  3  -1 \\\\\n0  -1  3\n\\end{pmatrix}.\n$$\nUse the standard splitting $A = D - L - U$, where $D$ is the diagonal of $A$, $-L$ is its strictly lower-triangular part, and $-U$ is its strictly upper-triangular part. Starting from the core definitions of the Jacobi, Gauss–Seidel, and Successive Over-Relaxation (SOR) iteration matrices for solving $A x = b$,\n- Jacobi: $T_{J} = D^{-1} (L + U)$,\n- Gauss–Seidel: $T_{\\mathrm{GS}} = (D - L)^{-1} U$,\n- Successive Over-Relaxation (SOR): $T_{\\mathrm{SOR}} = (D - \\omega L)^{-1} \\bigl( (1 - \\omega) D + \\omega U \\bigr)$ for relaxation factor $\\omega$,\n\nderive the explicit $3 \\times 3$ matrices $T_{J}$, $T_{\\mathrm{GS}}$, and $T_{\\mathrm{SOR}}$ when $\\omega = 1.2$. Then, compute their spectral radii (that is, the maximum absolute value of their eigenvalues) exactly. Report the three spectral radii in a single row matrix in the order $\\rho(T_{J})$, $\\rho(T_{\\mathrm{GS}})$, $\\rho(T_{\\mathrm{SOR}})$. Express your final answer in exact form; do not round.",
            "solution": "The problem as stated is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We begin by decomposing the given matrix $A$ and then proceed to derive the iteration matrices and their spectral radii.\n\nThe linear system is $A x = b$ with the matrix\n$$\nA = \\begin{pmatrix}\n3  -1  0 \\\\\n-1  3  -1 \\\\\n0  -1  3\n\\end{pmatrix}.\n$$\nThe standard splitting $A = D - L - U$ is used, where $D$ is the diagonal of $A$, $-L$ is its strictly lower-triangular part, and $-U$ is its strictly upper-triangular part. This implies $L$ and $U$ have non-negative entries. Based on the structure of $A$, we identify these matrices:\n$$\nD = \\begin{pmatrix}\n3  0  0 \\\\\n0  3  0 \\\\\n0  0  3\n\\end{pmatrix}, \\quad\nL = \\begin{pmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\n\n**Jacobi Iteration**\n\nThe Jacobi iteration matrix is defined as $T_{J} = D^{-1} (L + U)$.\nThe inverse of the diagonal matrix $D$ is trivially found:\n$$\nD^{-1} = \\begin{pmatrix}\n1/3  0  0 \\\\\n0  1/3  0 \\\\\n0  0  1/3\n\\end{pmatrix} = \\frac{1}{3} I.\n$$\nThe sum $L + U$ is:\n$$\nL + U = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}.\n$$\nThus, the Jacobi matrix $T_J$ is:\n$$\nT_{J} = \\frac{1}{3} \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix} = \\begin{pmatrix}\n0  1/3  0 \\\\\n1/3  0  1/3 \\\\\n0  1/3  0\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(T_J)$, we compute the eigenvalues of $T_J$ by solving the characteristic equation $\\det(T_J - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix}\n-\\lambda  1/3  0 \\\\\n1/3  -\\lambda  1/3 \\\\\n0  1/3  -\\lambda\n\\end{pmatrix} = 0.\n$$\nExpanding the determinant along the first row:\n$$\n-\\lambda \\left( (-\\lambda)(-\\lambda) - (1/3)(1/3) \\right) - (1/3) \\left( (1/3)(-\\lambda) - 0 \\right) = 0\n$$\n$$\n-\\lambda (\\lambda^2 - 1/9) + \\lambda/9 = 0\n$$\n$$\n-\\lambda^3 + \\lambda/9 + \\lambda/9 = 0\n$$\n$$\n-\\lambda^3 + \\frac{2}{9}\\lambda = 0\n$$\n$$\n-\\lambda \\left(\\lambda^2 - \\frac{2}{9}\\right) = 0.\n$$\nThe eigenvalues are $\\lambda_1 = 0$, $\\lambda_2 = \\sqrt{2/9} = \\frac{\\sqrt{2}}{3}$, and $\\lambda_3 = -\\sqrt{2/9} = -\\frac{\\sqrt{2}}{3}$.\nThe spectral radius is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(T_J) = \\max \\left\\{ |0|, \\left|\\frac{\\sqrt{2}}{3}\\right|, \\left|-\\frac{\\sqrt{2}}{3}\\right| \\right\\} = \\frac{\\sqrt{2}}{3}.\n$$\n\n**Gauss–Seidel Iteration**\n\nThe Gauss–Seidel iteration matrix is $T_{\\mathrm{GS}} = (D - L)^{-1} U$.\nFirst, we find the matrix $D - L$:\n$$\nD - L = \\begin{pmatrix}\n3  0  0 \\\\\n-1  3  0 \\\\\n0  -1  3\n\\end{pmatrix}.\n$$\nWe compute the inverse, $(D - L)^{-1}$, using forward substitution on the columns of the identity matrix:\n$$\n(D - L)^{-1} = \\begin{pmatrix}\n1/3  0  0 \\\\\n1/9  1/3  0 \\\\\n1/27  1/9  1/3\n\\end{pmatrix}.\n$$\nNow, we compute $T_{\\mathrm{GS}}$:\n$$\nT_{\\mathrm{GS}} = \\begin{pmatrix}\n1/3  0  0 \\\\\n1/9  1/3  0 \\\\\n1/27  1/9  1/3\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n0  1/3  0 \\\\\n0  1/9  1/3 \\\\\n0  1/27  1/9\n\\end{pmatrix}.\n$$\nThe eigenvalues are found from $\\det(T_{\\mathrm{GS}} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix}\n-\\lambda  1/3  0 \\\\\n0  1/9-\\lambda  1/3 \\\\\n0  1/27  1/9-\\lambda\n\\end{pmatrix} = 0.\n$$\nExpanding along the first column:\n$$\n-\\lambda \\left( (1/9 - \\lambda)^2 - (1/3)(1/27) \\right) = 0\n$$\n$$\n-\\lambda \\left( (1/9 - \\lambda)^2 - 1/81 \\right) = 0.\n$$\nOne eigenvalue is $\\lambda_1 = 0$. The others satisfy $(1/9 - \\lambda)^2 = 1/81$, which gives $1/9 - \\lambda = \\pm 1/9$.\nIf $1/9 - \\lambda = 1/9$, then $\\lambda_2 = 0$.\nIf $1/9 - \\lambda = -1/9$, then $\\lambda_3 = 2/9$.\nThe eigenvalues are $0, 0, 2/9$. The spectral radius is:\n$$\n\\rho(T_{\\mathrm{GS}}) = \\max \\left\\{ |0|, |2/9| \\right\\} = \\frac{2}{9}.\n$$\nFor a tridiagonal matrix like $A$, which is consistently ordered, it is known that $\\rho(T_{\\mathrm{GS}}) = (\\rho(T_{J}))^2$. Our results confirm this: $(\\frac{\\sqrt{2}}{3})^2 = \\frac{2}{9}$.\n\n**Successive Over-Relaxation (SOR) Iteration**\n\nThe SOR iteration matrix is $T_{\\mathrm{SOR}} = (D - \\omega L)^{-1} \\bigl( (1 - \\omega) D + \\omega U \\bigr)$, with $\\omega = 1.2$.\nLet's first compute the explicit matrix $T_{\\mathrm{SOR}}$. We use $\\omega = 1.2 = 6/5$.\nThe first term is $(D - \\omega L)$:\n$$\nD - \\omega L = \\begin{pmatrix} 3  0  0 \\\\ -6/5  3  0 \\\\ 0  -6/5  3 \\end{pmatrix}.\n$$\nIts inverse is:\n$$\n(D - \\omega L)^{-1} = \\begin{pmatrix} 1/3  0  0 \\\\ 2/15  1/3  0 \\\\ 4/75  2/15  1/3 \\end{pmatrix}.\n$$\nThe second term is $(1 - \\omega) D + \\omega U = (1 - 6/5) D + (6/5) U = (-1/5)D + (6/5)U$:\n$$\n(-1/5)D + (6/5)U = \\begin{pmatrix} -3/5  0  0 \\\\ 0  -3/5  0 \\\\ 0  0  -3/5 \\end{pmatrix} + \\begin{pmatrix} 0  6/5  0 \\\\ 0  0  6/5 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} -3/5  6/5  0 \\\\ 0  -3/5  6/5 \\\\ 0  0  -3/5 \\end{pmatrix}.\n$$\nNow, we multiply these two matrices to find $T_{\\mathrm{SOR}}$:\n$$\nT_{\\mathrm{SOR}} = \\begin{pmatrix} 1/3  0  0 \\\\ 2/15  1/3  0 \\\\ 4/75  2/15  1/3 \\end{pmatrix} \\begin{pmatrix} -3/5  6/5  0 \\\\ 0  -3/5  6/5 \\\\ 0  0  -3/5 \\end{pmatrix} = \\begin{pmatrix} -1/5  2/5  0 \\\\ -2/25  -1/25  2/5 \\\\ -4/125  -2/125  -1/25 \\end{pmatrix}.\n$$\nTo find the spectral radius, we can find the eigenvalues of $T_{\\mathrm{SOR}}$. However, since $A$ is consistently ordered, we can use the known relationship between the eigenvalues $\\lambda$ of $T_{\\mathrm{SOR}}$ and the eigenvalues $\\mu$ of $T_J$:\n$$\n(\\lambda + \\omega - 1)^2 = \\lambda \\omega^2 \\mu^2.\n$$\nWe have $\\omega = 6/5$, and the eigenvalues of $T_J$ are $\\mu \\in \\{0, \\pm\\sqrt{2}/3\\}$. Thus, $\\mu^2 \\in \\{0, 2/9\\}$.\nCase 1: $\\mu^2 = 0$.\nThe relation becomes $(\\lambda + \\omega - 1)^2 = 0$, which gives $\\lambda = 1 - \\omega$.\nWith $\\omega = 1.2$, we get one eigenvalue $\\lambda_1 = 1 - 1.2 = -0.2 = -1/5$.\n\nCase 2: $\\mu^2 = 2/9$.\nThe relation is $(\\lambda + 1.2 - 1)^2 = \\lambda (1.2)^2 (2/9)$. Using fractions for precision:\n$$\n(\\lambda + 1/5)^2 = \\lambda (6/5)^2 (2/9)\n$$\n$$\n\\lambda^2 + \\frac{2}{5}\\lambda + \\frac{1}{25} = \\lambda \\left(\\frac{36}{25}\\right) \\left(\\frac{2}{9}\\right) = \\lambda \\frac{8}{25}\n$$\n$$\n\\lambda^2 + \\left(\\frac{10}{25} - \\frac{8}{25}\\right)\\lambda + \\frac{1}{25} = 0\n$$\n$$\n\\lambda^2 + \\frac{2}{25}\\lambda + \\frac{1}{25} = 0\n$$\n$$\n25\\lambda^2 + 2\\lambda + 1 = 0.\n$$\nUsing the quadratic formula, we find the other two eigenvalues:\n$$\n\\lambda_{2,3} = \\frac{-2 \\pm \\sqrt{2^2 - 4(25)(1)}}{2(25)} = \\frac{-2 \\pm \\sqrt{4 - 100}}{50} = \\frac{-2 \\pm \\sqrt{-96}}{50}.\n$$\nSimplifying the radical, $\\sqrt{-96} = \\sqrt{16 \\cdot 6 \\cdot (-1)} = 4i\\sqrt{6}$.\n$$\n\\lambda_{2,3} = \\frac{-2 \\pm 4i\\sqrt{6}}{50} = \\frac{-1 \\pm 2i\\sqrt{6}}{25}.\n$$\nThe eigenvalues of $T_{\\mathrm{SOR}}$ are $\\lambda_1 = -1/5$, $\\lambda_2 = \\frac{-1}{25} + i\\frac{2\\sqrt{6}}{25}$, and $\\lambda_3 = \\frac{-1}{25} - i\\frac{2\\sqrt{6}}{25}$.\nWe compute their absolute values:\n$|\\lambda_1| = |-1/5| = 1/5$.\n$|\\lambda_{2,3}| = \\sqrt{\\left(-\\frac{1}{25}\\right)^2 + \\left(\\pm\\frac{2\\sqrt{6}}{25}\\right)^2} = \\sqrt{\\frac{1}{625} + \\frac{4 \\cdot 6}{625}} = \\sqrt{\\frac{1+24}{625}} = \\sqrt{\\frac{25}{625}} = \\sqrt{\\frac{1}{25}} = \\frac{1}{5}$.\nAll three eigenvalues have the same magnitude. The spectral radius is:\n$$\n\\rho(T_{\\mathrm{SOR}}) = \\frac{1}{5}.\n$$\nThe three spectral radii are $\\rho(T_J) = \\frac{\\sqrt{2}}{3}$, $\\rho(T_{\\mathrm{GS}}) = \\frac{2}{9}$, and $\\rho(T_{\\mathrm{SOR}}) = \\frac{1}{5}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{2}}{3}  \\frac{2}{9}  \\frac{1}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A crucial question in numerical analysis is not just *how* a method works, but *when* it is guaranteed to work. This exercise explores the vital role of matrix properties, specifically contrasting the behavior of iterative methods on a symmetric indefinite matrix versus a symmetric positive definite (SPD) one. By analyzing a case where the Jacobi method fails , you will uncover why properties like positive definiteness are central to the convergence theorems that underpin these powerful techniques.",
            "id": "3581622",
            "problem": "Consider the linear system $A x = b$ with \n$$\nA \\;=\\; \\begin{bmatrix} 1  2 \\\\ 2  -1 \\end{bmatrix},\n$$ \nwhich is symmetric and indefinite. The Jacobi method updates each component of $x$ using only values from the previous iterate. The Gauss–Seidel method uses the most recently updated components within the same iteration. Successive Over-Relaxation (SOR) uses a relaxation parameter $\\omega$ to form a weighted update. In stationary iterations, the error propagates linearly through an iteration matrix; convergence is therefore determined by properties of that matrix and does not depend on $b$.\n\nUsing only foundational facts about linear stationary iterations, spectra of matrices, and definiteness, decide which of the following statements are correct. Select all that apply.\n\nA. For the matrix $A$ above, the Jacobi method diverges because its iteration matrix has spectral radius strictly greater than $1$.\n\nB. If $A$ is symmetric positive definite (SPD), then Gauss–Seidel converges for any initial guess; no such general convergence guarantee holds for arbitrary symmetric indefinite $A$.\n\nC. For any symmetric indefinite matrix with nonsingular diagonal, the Jacobi method always diverges.\n\nD. If $A$ is SPD, then SOR with relaxation parameter $\\omega \\in (0,2)$ is guaranteed to converge; no such general guarantee holds for arbitrary symmetric indefinite $A$.\n\nE. The failure of Jacobi for the given $A$ is due to the presence of a negative diagonal entry; if both diagonal entries were positive, Jacobi would converge.",
            "solution": "### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following information:\n- A linear system $Ax = b$.\n- The matrix $A = \\begin{bmatrix} 1  2 \\\\ 2  -1 \\end{bmatrix}$.\n- The matrix $A$ is described as symmetric and indefinite.\n- Definitions of Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) methods are provided in conceptual terms.\n- A key property of stationary iterations is stated: \"error propagates linearly through an iteration matrix; convergence is therefore determined by properties of that matrix and does not depend on $b$.\"\n- The task is to evaluate several statements using \"only foundational facts about linear stationary iterations, spectra of matrices, and definiteness\".\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is situated in numerical linear algebra, a well-established branch of mathematics. The concepts invoked—Jacobi, Gauss-Seidel, SOR methods, spectral radius, matrix definiteness—are standard and rigorously defined. The given matrix $A$ is symmetric. Its determinant is $\\det(A) = (1)(-1) - (2)(2) = -1 - 4 = -5  0$. The eigenvalues $\\lambda$ are given by $(1-\\lambda)(-1-\\lambda)-4=0$, which simplifies to $\\lambda^2 - 5 = 0$, so $\\lambda = \\pm \\sqrt{5}$. Since the eigenvalues have mixed signs, the matrix is indeed indefinite. The problem is factually correct.\n2.  **Well-Posed**: The problem asks to evaluate the truth of several statements based on the provided information and foundational principles. This is a well-defined task with a unique set of correct and incorrect statements.\n3.  **Objective**: The language is precise and devoid of subjective claims. The matrix and the methods are defined mathematically.\n4.  **Complete and Consistent**: The problem provides all necessary information (the matrix $A$ and the context of stationary iterative methods) to evaluate the given statements. There are no internal contradictions.\n5.  **Not Ill-Posed or Trivial**: The statements require specific calculations and application of theorems from numerical analysis, representing a standard conceptual check in this field.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and complete. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe convergence of a stationary iterative method of the form $x_{k+1} = T x_k + c$ is determined by the spectral radius of the iteration matrix $T$. The method converges for any initial guess if and only if the spectral radius $\\rho(T) = \\max_i |\\lambda_i(T)|  1$, where $\\lambda_i(T)$ are the eigenvalues of $T$.\n\nLet the matrix $A$ be decomposed as $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strictly lower-triangular part, and $-U$ is the strictly upper-triangular part.\nFor the given matrix $A = \\begin{bmatrix} 1  2 \\\\ 2  -1 \\end{bmatrix}$:\n- $D = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}$\n- $L = \\begin{bmatrix} 0  0 \\\\ -2  0 \\end{bmatrix}$\n- $U = \\begin{bmatrix} 0  -2 \\\\ 0  0 \\end{bmatrix}$\n\nThe iteration matrices are:\n- Jacobi: $T_J = D^{-1}(L+U)$\n- Gauss-Seidel: $T_{GS} = (D-L)^{-1}U$\n- SOR: $T_{SOR} = (D-\\omega L)^{-1}((1-\\omega)D + \\omega U)$\n\nWe will now analyze each statement.\n\n**A. For the matrix $A$ above, the Jacobi method diverges because its iteration matrix has spectral radius strictly greater than $1$.**\n\nFirst, we construct the Jacobi iteration matrix $T_J$ for the given $A$.\n$D^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}$.\n$L+U = \\begin{bmatrix} 0  -2 \\\\ -2  0 \\end{bmatrix}$.\n$T_J = D^{-1}(L+U) = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix} \\begin{bmatrix} 0  -2 \\\\ -2  0 \\end{bmatrix} = \\begin{bmatrix} 0  -2 \\\\ 2  0 \\end{bmatrix}$.\n\nNext, we find the eigenvalues of $T_J$ by solving the characteristic equation $\\det(T_J - \\lambda I) = 0$.\n$$\n\\det \\begin{bmatrix} -\\lambda  -2 \\\\ 2  -\\lambda \\end{bmatrix} = (-\\lambda)(-\\lambda) - (-2)(2) = \\lambda^2 + 4 = 0.\n$$\nThe eigenvalues are $\\lambda_{1,2} = \\pm \\sqrt{-4} = \\pm 2i$.\nThe spectral radius is $\\rho(T_J) = \\max(|2i|, |-2i|) = 2$.\nSince $\\rho(T_J) = 2 > 1$, the Jacobi method diverges. The statement correctly identifies the reason for divergence as the spectral radius being greater than $1$.\n\nVerdict: **Correct**.\n\n**B. If $A$ is symmetric positive definite (SPD), then Gauss–Seidel converges for any initial guess; no such general convergence guarantee holds for arbitrary symmetric indefinite $A$.**\n\nThis statement consists of two parts.\n1. \"If $A$ is symmetric positive definite (SPD), then Gauss–Seidel converges for any initial guess\". This is a fundamental theorem of numerical linear algebra. For any SPD matrix $A$, the spectral radius of the Gauss-Seidel iteration matrix $T_{GS}$ is strictly less than $1$.\n2. \"no such general convergence guarantee holds for arbitrary symmetric indefinite $A$\". To verify this, we can test the Gauss-Seidel method on the given symmetric indefinite matrix $A$.\n\nWe construct the Gauss-Seidel iteration matrix $T_{GS} = (D-L)^{-1}U$.\n$D-L = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix} - \\begin{bmatrix} 0  0 \\\\ -2  0 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 2  -1 \\end{bmatrix}$.\nThe inverse is $(D-L)^{-1} = \\frac{1}{(1)(-1)-(0)(2)} \\begin{bmatrix} -1  0 \\\\ -2  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 2  -1 \\end{bmatrix}$.\n$U = \\begin{bmatrix} 0  -2 \\\\ 0  0 \\end{bmatrix}$.\n$T_{GS} = (D-L)^{-1}U = \\begin{bmatrix} 1  0 \\\\ 2  -1 \\end{bmatrix} \\begin{bmatrix} 0  -2 \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 0  -2 \\\\ 0  -4 \\end{bmatrix}$.\nSince $T_{GS}$ is a triangular matrix, its eigenvalues are its diagonal entries, which are $\\lambda_1 = 0$ and $\\lambda_2 = -4$.\nThe spectral radius is $\\rho(T_{GS}) = \\max(|0|, |-4|) = 4$.\nSince $\\rho(T_{GS}) = 4 > 1$, the Gauss-Seidel method diverges for this symmetric indefinite matrix. This provides a counterexample to any general convergence guarantee for symmetric indefinite matrices.\nBoth parts of the statement are true.\n\nVerdict: **Correct**.\n\n**C. For any symmetric indefinite matrix with nonsingular diagonal, the Jacobi method always diverges.**\n\nThis is a strong claim (\"always diverges\"). We can test it by attempting to construct a counterexample: a symmetric indefinite matrix with a nonsingular diagonal for which Jacobi converges. Let the matrix be $A = \\begin{bmatrix} a  b \\\\ b  c \\end{bmatrix}$. For $A$ to be indefinite, we need its determinant to be negative, i.e., $ac - b^2  0$. The diagonal is nonsingular if $a \\neq 0$ and $c \\neq 0$.\nThe Jacobi iteration matrix is $T_J = \\begin{bmatrix} 0  -b/a \\\\ -b/c  0 \\end{bmatrix}$. Its eigenvalues $\\lambda$ are given by $\\lambda^2 - (-b/a)(-b/c) = 0$, so $\\lambda^2 = b^2/(ac)$. The spectral radius is $\\rho(T_J) = \\sqrt{|b^2/(ac)|} = |b|/\\sqrt{|ac|}$.\nFor convergence, we need $\\rho(T_J)  1$, which means $b^2  |ac|$.\nLet's choose values satisfying both $ac - b^2  0$ and $b^2  |ac|$. Let $a=2$ and $c=-2$. Then $a, c \\neq 0$ and $ac=-4$. The condition for indefiniteness becomes $-4 - b^2  0$, which is true for any real $b$. The condition for convergence becomes $b^2  |-4| = 4$, or $|b|2$.\nLet's choose $b=1$. The matrix is $A = \\begin{bmatrix} 2  1 \\\\ 1  -2 \\end{bmatrix}$.\nThis matrix is symmetric. Its diagonal entries are non-zero. $\\det(A) = (2)(-2) - (1)^2 = -5  0$, so it is indefinite.\nFor this matrix, $a=2, b=1, c=-2$. The spectral radius of its Jacobi matrix is $\\rho(T_J) = |1|/\\sqrt{|(2)(-2)|} = 1/\\sqrt{4} = 1/2$.\nSince $\\rho(T_J) = 1/2  1$, the Jacobi method converges for this symmetric indefinite matrix. This disproves the claim that it \"always diverges\".\n\nVerdict: **Incorrect**.\n\n**D. If $A$ is SPD, then SOR with relaxation parameter $\\omega \\in (0,2)$ is guaranteed to converge; no such general guarantee holds for arbitrary symmetric indefinite $A$.**\n\nThis statement is analogous to statement B, but for the SOR method.\n1. \"If $A$ is SPD, then SOR with relaxation parameter $\\omega \\in (0,2)$ is guaranteed to converge\". This is the well-known Ostrowski-Reich theorem.\n2. \"no such general guarantee holds for arbitrary symmetric indefinite $A$\". We can use the same counterexample as in B. The Gauss-Seidel method is a special case of SOR with the relaxation parameter $\\omega = 1$. Since $\\omega = 1$ is in the interval $(0, 2)$, the divergence of Gauss-Seidel for the given symmetric indefinite matrix $A = \\begin{bmatrix} 1  2 \\\\ 2  -1 \\end{bmatrix}$ (as shown in the analysis of B) is sufficient to prove that the guarantee for SPD matrices does not extend to the symmetric indefinite case.\n\nBoth parts of the statement are true.\n\nVerdict: **Correct**.\n\n**E. The failure of Jacobi for the given $A$ is due to the presence of a negative diagonal entry; if both diagonal entries were positive, Jacobi would converge.**\n\nThis statement proposes a specific cause for the divergence observed in A and makes a prediction. We can test the prediction by constructing a relevant matrix. Let us modify the given matrix $A$ to have a positive second diagonal entry, for instance, $A' = \\begin{bmatrix} 1  2 \\\\ 2  1 \\end{bmatrix}$.\nThis matrix $A'$ is symmetric and has all positive diagonal entries. Its determinant is $\\det(A') = (1)(1) - (2)(2) = -3  0$, so it is still indefinite.\nLet's check the convergence of Jacobi for $A'$.\n$D' = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = I$.\n$L'+U' = \\begin{bmatrix} 0  -2 \\\\ -2  0 \\end{bmatrix}$.\n$T'_J = (D')^{-1}(L'+U') = I \\begin{bmatrix} 0  -2 \\\\ -2  0 \\end{bmatrix} = \\begin{bmatrix} 0  -2 \\\\ -2  0 \\end{bmatrix}$.\nThe eigenvalues $\\lambda$ are given by $\\det(T'_J - \\lambda I) = 0$:\n$$\n\\det \\begin{bmatrix} -\\lambda  -2 \\\\ -2  -\\lambda \\end{bmatrix} = \\lambda^2 - 4 = 0.\n$$\nThe eigenvalues are $\\lambda_{1,2} = \\pm 2$. The spectral radius is $\\rho(T'_J) = \\max(|2|, |-2|) = 2$.\nSince $\\rho(T'_J) = 2  1$, the Jacobi method still diverges. This falsifies the claim that having positive diagonal entries would ensure convergence. The divergence is related to the lack of diagonal dominance, not simply the sign of the diagonal entries.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}