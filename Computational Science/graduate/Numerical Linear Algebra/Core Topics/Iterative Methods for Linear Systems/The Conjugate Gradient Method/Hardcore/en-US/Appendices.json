{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an iterative algorithm, it's invaluable to trace its execution by hand. This exercise provides a foundational, step-by-step walkthrough of the first iteration of the Conjugate Gradient method for a small, manageable system. By explicitly computing the initial residual, step length, and updated solution (), you will transform the abstract recurrence relations of CG into a concrete sequence of calculations, solidifying your understanding of the algorithm's core mechanics.",
            "id": "3371621",
            "problem": "Consider a linear system arising from a symmetric positive definite discretization of an elliptic operator in computational fluid dynamics, such as the pressure Poisson equation on a minimal control-volume stencil. Let the matrix be $A=\\begin{pmatrix}41\\\\13\\end{pmatrix}$, the right-hand side be $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$, and the initial iterate be $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The Conjugate Gradient method (CG) is derived by minimizing the quadratic functional $\\phi(x)=\\frac{1}{2}x^{\\top}Ax-b^{\\top}x$ over Krylov subspaces, with residuals $r_{k}=b-Ax_{k}$, mutually $A$-conjugate search directions, and step sizes chosen so that the new residual is orthogonal to the current search direction in the Euclidean inner product. Starting from these principles, and without assuming any shortcut formulas, compute the first-step quantities $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$ explicitly for the given $A$, $b$, and $x_{0}$. Provide exact values with no rounding. For reporting, express the final answer as the row $\\left(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0}\\right)$, where $x_{1,i}$ and $r_{1,i}$ denote the components of $x_{1}$ and $r_{1}$, respectively.",
            "solution": "The problem is well-posed and scientifically sound. It requires the computation of the first iteration of the Conjugate Gradient (CG) method for a given linear system, starting from the fundamental principles of the algorithm rather than relying on a pre-packaged algorithm summary.\n\nThe system to be solved is $Ax=b$, where the matrix $A$ is symmetric and positive definite (SPD). The CG method iteratively constructs a solution by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2}x^{\\top}Ax - b^{\\top}x$. The gradient of this functional is $\\nabla\\phi(x) = Ax - b$, which is the negative of the residual, $r(x) = b - Ax$. Thus, minimizing $\\phi(x)$ is equivalent to finding $x$ such that $\\nabla\\phi(x) = 0$, which is the solution to $Ax=b$.\n\nThe givens are:\nThe matrix $A = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$.\nThe right-hand side vector $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial guess for the solution $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm proceeds as follows for iteration $k=0, 1, 2, ...$:\n1. Update the solution: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n2. Update the residual: $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n3. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\nThe parameters $\\alpha_k$ and $\\beta_k$ are derived from core principles.\n\n**Step 0: Initialization**\n\nFirst, we compute the initial residual $r_0$ based on the initial guess $x_0$.\n$$r_{0} = b - Ax_{0}$$\nWith $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the initial residual is simply $b$:\n$$r_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe first search direction $p_0$ is chosen to be the direction of steepest descent, which is the initial residual:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n\n**Step 1: First Iteration ($k=0$)**\n\nWe need to compute $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$.\n\n**Computing the step size $\\alpha_{0}$**\nThe step size $\\alpha_{0}$ is chosen to minimize $\\phi(x_{1}) = \\phi(x_{0} + \\alpha_{0} p_{0})$ along the search direction $p_{0}$. This minimum is achieved when the new residual $r_{1}$ is orthogonal to the current search direction $p_{0}$, i.e., $p_{0}^{\\top}r_{1} = 0$.\nThe new residual is given by $r_{1} = b - Ax_{1} = b - A(x_{0} + \\alpha_{0} p_{0}) = (b - Ax_{0}) - \\alpha_{0}Ap_{0} = r_0 - \\alpha_0 A p_0$.\nSubstituting this into the orthogonality condition:\n$$p_{0}^{\\top}(r_{0} - \\alpha_{0} A p_{0}) = 0$$\n$$p_{0}^{\\top}r_{0} - \\alpha_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\alpha_0$ yields:\n$$\\alpha_{0} = \\frac{p_{0}^{\\top}r_{0}}{p_{0}^{\\top}A p_{0}}$$\nSince $p_0 = r_0$, this becomes:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top}r_{0}}{r_{0}^{\\top}A r_{0}}$$\nWe calculate the necessary quantities:\n$r_{0}^{\\top}r_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1)(1) + (2)(2) = 1 + 4 = 5$.\n$A p_{0} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\n$p_{0}^{\\top}A p_{0} = r_{0}^{\\top}A p_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (1)(6) + (2)(7) = 6 + 14 = 20$.\nSubstituting these values:\n$$\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$$\n\n**Computing the new iterate $x_{1}$**\nThe new solution estimate $x_1$ is found by moving from $x_0$ along the direction $p_0$ by the step size $\\alpha_0$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nSo, $x_{1,1} = \\frac{1}{4}$ and $x_{1,2} = \\frac{1}{2}$.\n\n**Computing the new residual $r_{1}$**\nThe new residual $r_1$ can be computed using the update formula:\n$$r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{6}{4} \\\\ 2 - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{4} - \\frac{6}{4} \\\\ \\frac{8}{4} - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$$\nSo, $r_{1,1} = -\\frac{1}{2}$ and $r_{1,2} = \\frac{1}{4}$.\n\n**Computing the coefficient $\\beta_{0}$**\nThe coefficient $\\beta_0$ is used to construct the next search direction, $p_1 = r_1 + \\beta_0 p_0$. The fundamental principle is that the new search direction $p_1$ must be $A$-conjugate to the previous direction $p_0$, meaning $p_{1}^{\\top}A p_{0} = 0$.\n$$(r_{1} + \\beta_{0} p_{0})^{\\top}A p_{0} = 0$$\n$$r_{1}^{\\top}A p_{0} + \\beta_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = -\\frac{r_{1}^{\\top}A p_{0}}{p_{0}^{\\top}A p_{0}}$$\nWe have the terms from the previous calculations: $A p_0 = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$ and $p_{0}^{\\top}A p_{0} = 20$.\nWe need to calculate the numerator:\n$r_{1}^{\\top}A p_{0} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (-\\frac{1}{2})(6) + (\\frac{1}{4})(7) = -3 + \\frac{7}{4} = -\\frac{12}{4} + \\frac{7}{4} = -\\frac{5}{4}$.\nNow we can compute $\\beta_0$:\n$$\\beta_{0} = - \\frac{-\\frac{5}{4}}{20} = \\frac{5}{4 \\cdot 20} = \\frac{5}{80} = \\frac{1}{16}$$\n\nThe requested quantities are $\\alpha_{0} = \\frac{1}{4}$, $x_{1} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$, $r_{1} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$, and $\\beta_{0} = \\frac{1}{16}$.\nThe final answer is assembled into the specified row vector format $(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0})$.\nThis gives the row vector $(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{16})$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{2}  -\\frac{1}{2}  \\frac{1}{4}  \\frac{1}{16} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The efficiency of the Conjugate Gradient method stems from its construction of $A$-conjugate search directions, which ensures that each new search direction is optimal with respect to all previous ones. This practice moves beyond simple calculation to verify this fundamental principle. By explicitly computing the first two search directions and confirming their $A$-conjugacy (), you will gain a deeper insight into the geometric foundation that guarantees the method's rapid convergence.",
            "id": "3371638",
            "problem": "Consider a linear system arising in Computational Fluid Dynamics (CFD), where a local diagonal block of a preconditioned diffusion operator is modeled as a symmetric positive definite (SPD) matrix. Let the quadratic functional be $f(x) = \\frac{1}{2} x^{T} A x - b^{T} x$ with $A = \\mathrm{diag}(1, 2, 3)$, $b = (1, 1, 1)^{T}$, and the initial guess $x_{0} = (0, 0, 0)^{T}$. Starting from first principles—namely, minimizing $f(x)$ through steepest descent for the initial direction and enforcing $A$-conjugacy for subsequent directions—perform the following:\n\n1. Compute the initial residual $r_{0}$ and the first search direction $p_{0}$ using the steepest descent principle applied to $f(x)$.\n2. Determine the step length $\\alpha_{0}$ by exact line search along $p_{0}$, update $x_{1}$ and compute the new residual $r_{1}$.\n3. Determine the scalar $\\beta_{0}$ by enforcing the $A$-conjugacy condition between the search directions, and form $p_{1}$ accordingly.\n4. Verify explicitly that $p_{0}^{T} A p_{1} = 0$.\n\nReport the value of $p_{0}^{T} A p_{1}$ as your final answer. No rounding is necessary. Express the final answer as a pure number without units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a standard application of the initial steps of the conjugate gradient algorithm, which is a cornerstone of numerical methods for solving linear systems, particularly in contexts like Computational Fluid Dynamics (CFD). The provided matrix $A$ is symmetric and its eigenvalues are the diagonal entries $1$, $2$, and $3$, all of which are positive. Thus, $A$ is symmetric positive definite (SPD) as stated, ensuring the problem is well-posed. We may proceed with the solution.\n\nThe problem asks to perform the first iteration of the conjugate gradient method, starting from first principles. The goal is to find the minimum of the quadratic functional $f(x) = \\frac{1}{2} x^{T} A x - b^{T} x$. The minimizer of $f(x)$ is the solution to the linear system $Ax = b$. The gradient of the functional is $\\nabla f(x) = Ax - b$. The residual of the system is defined as $r(x) = b - Ax$. It follows that the residual is the negative gradient, $r(x) = -\\nabla f(x)$, and thus points in the direction of steepest descent of $f(x)$.\n\nThe given data are:\nThe matrix $A = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}$.\nThe vector $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\nThe initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nWe follow the requested steps.\n\n1.  Compute the initial residual $r_{0}$ and the first search direction $p_{0}$.\n\nThe initial residual $r_0$ is calculated at the initial guess $x_0$:\n$$r_{0} = b - A x_{0}$$\nSubstituting the given values:\n$$r_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nThe first search direction $p_0$ in the conjugate gradient method is taken as the direction of steepest descent, which is the initial residual $r_0$:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n\n2.  Determine the step length $\\alpha_{0}$, update $x_{1}$, and compute the new residual $r_{1}$.\n\nThe next iterate is given by $x_{1} = x_{0} + \\alpha_{0} p_{0}$. The optimal step length $\\alpha_0$ is found by minimizing $f(x_1)$ with respect to $\\alpha_0$. This is an exact line search. We set the derivative of $f(x_0 + \\alpha p_0)$ with respect to $\\alpha$ to zero.\n$$\\frac{d}{d\\alpha} f(x_{0} + \\alpha p_{0}) = \\nabla f(x_{0} + \\alpha p_{0})^{T} p_{0} = 0$$\nUsing $\\nabla f(x) = Ax - b$, we get:\n$$(A(x_{0} + \\alpha_{0} p_{0}) - b)^{T} p_{0} = 0$$\n$$(A x_{0} - b + \\alpha_{0} A p_{0})^{T} p_{0} = 0$$\nSince $r_0 = b - Ax_0$ and $p_0 = r_0$:\n$$(-r_{0} + \\alpha_{0} A p_{0})^{T} p_{0} = 0$$\n$$-r_{0}^{T} p_{0} + \\alpha_{0} p_{0}^{T} A p_{0} = 0$$\nSolving for $\\alpha_0$:\n$$\\alpha_{0} = \\frac{r_{0}^{T} p_{0}}{p_{0}^{T} A p_{0}}$$\nSince $p_0=r_0$, this simplifies to:\n$$\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{r_{0}^{T} A r_{0}}$$\nWe compute the necessary dot products:\n$$r_{0}^{T} r_{0} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 3$$\n$$A r_{0} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\n$$r_{0}^{T} A r_{0} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 3 = 6$$\nThe step length is:\n$$\\alpha_{0} = \\frac{3}{6} = \\frac{1}{2}$$\nNow we update the solution vector $x_1$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nAnd compute the new residual $r_1$:\n$$r_{1} = b - A x_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$$\n\n3.  Determine the scalar $\\beta_{0}$ and form $p_{1}$.\n\nThe next search direction $p_1$ is a linear combination of the new residual $r_1$ and the previous search direction $p_0$:\n$$p_{1} = r_{1} + \\beta_{0} p_{0}$$\nThe scalar $\\beta_0$ is chosen to enforce $A$-conjugacy (also called $A$-orthogonality) between $p_1$ and $p_0$, which is the condition $p_{0}^{T} A p_{1} = 0$:\n$$p_{0}^{T} A (r_{1} + \\beta_{0} p_{0}) = 0$$\n$$p_{0}^{T} A r_{1} + \\beta_{0} p_{0}^{T} A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = - \\frac{p_{0}^{T} A r_{1}}{p_{0}^{T} A p_{0}}$$\nSince A is symmetric, $p_{0}^{T} A r_{1} = (A p_{0})^{T} r_{1}$.\nThe denominator was previously calculated: $p_{0}^{T} A p_{0} = r_{0}^{T} A r_{0} = 6$.\nWe calculate the numerator:\n$$A p_{0} = A r_0 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\n$$(A p_{0})^{T} r_{1} = \\begin{pmatrix} 1  2  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = 1 \\cdot \\frac{1}{2} + 2 \\cdot 0 + 3 \\cdot \\left(-\\frac{1}{2}\\right) = \\frac{1}{2} - \\frac{3}{2} = -1$$\nThus, we find $\\beta_0$:\n$$\\beta_{0} = - \\frac{-1}{6} = \\frac{1}{6}$$\nNow we form the new search direction $p_1$:\n$$p_{1} = r_{1} + \\beta_{0} p_{0} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{6} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} + \\frac{1}{6} \\\\ 0 + \\frac{1}{6} \\\\ -\\frac{1}{2} + \\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{3+1}{6} \\\\ \\frac{1}{6} \\\\ \\frac{-3+1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{6} \\\\ \\frac{1}{6} \\\\ -\\frac{2}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{6} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\n\n4.  Verify explicitly that $p_{0}^{T} A p_{1} = 0$.\n\nThis final step confirms that our calculated value of $\\beta_0$ correctly enforces $A$-conjugacy. We perform the calculation directly with the vectors we have found.\n$$p_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\quad \\text{and} \\quad p_{1} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{6} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\nFirst, compute $A p_1$:\n$$A p_{1} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{6} \\\\ -\\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\frac{2}{3} \\\\ 2 \\cdot \\frac{1}{6} \\\\ 3 \\cdot (-\\frac{1}{3}) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{6} \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{3} \\\\ -1 \\end{pmatrix}$$\nNow, compute the dot product $p_0^T (A p_1)$:\n$$p_{0}^{T} A p_{1} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{3} \\\\ -1 \\end{pmatrix} = 1 \\cdot \\frac{2}{3} + 1 \\cdot \\frac{1}{3} + 1 \\cdot (-1) = \\frac{2}{3} + \\frac{1}{3} - 1 = \\frac{3}{3} - 1 = 1 - 1 = 0$$\nThe calculation confirms that $p_0$ and $p_1$ are $A$-conjugate. The value of the expression $p_{0}^{T} A p_{1}$ is exactly $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "This advanced practice challenges you to transition from theoretical knowledge to practical application by implementing a complete, matrix-free Conjugate Gradient solver. You will tackle a realistic scenario from computational physics—the Poisson equation—and address practical complexities such as periodic boundary conditions and singular systems requiring projection. This comprehensive exercise () not only hones your programming skills but also introduces you to performance analysis through the Roofline model, a crucial concept in modern scientific computing.",
            "id": "3371615",
            "problem": "You are asked to design and implement a matrix-free Conjugate Gradient (CG) solver for the finite-volume discretization of the Poisson equation with periodic boundary conditions on a uniform two-dimensional square grid, and to analyze the arithmetic intensity and roofline-bound performance of the stencil application. The program you produce must be self-contained and runnable without user input.\n\nThe mathematical backbone is the finite-volume formulation derived from the divergence theorem together with the standard central difference approximation of fluxes on a uniform mesh. On a square domain with periodic boundaries and grid spacing $h$, the discrete operator corresponding to the symmetric positive semi-definite operator $-\\nabla^2$ yields a 5-point stencil. On a uniform grid, the finite-volume discretization coincides with the standard 5-point central difference stencil for the Laplacian at cell centers. The operator is symmetric positive semi-definite with a one-dimensional nullspace spanned by constant fields; as such, the linear system is solvable if and only if the right-hand side has zero mean, and the solution is unique up to an additive constant. To employ the Conjugate Gradient method validly, one must restrict the problem to the subspace of zero-mean fields by projecting the right-hand side and all iterates onto the zero-mean subspace.\n\nYour tasks are:\n\n- Implement a matrix-free operator that applies $A = -\\Delta_h$ to a field $u$ stored on an $N \\times N$ uniform grid of cell centers with periodic boundaries. For each interior cell $(i,j)$, the discrete operator is given by\n$$\n(A u)_{i,j} \\;=\\; \\frac{4\\,u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2},\n$$\nwith periodic wrap-around indexing in both directions. The program must use double-precision arithmetic.\n\n- Implement Projected Conjugate Gradient (CG) for the system $A u = b$ restricted to the zero-mean subspace. Let $P$ denote the orthogonal projection onto the zero-mean subspace,\n$$\nP(v) \\;=\\; v - \\overline{v}, \\quad \\text{where}\\;\\; \\overline{v} \\;=\\; \\frac{1}{N^2} \\sum_{i,j} v_{i,j}.\n$$\nUse $b \\leftarrow P(b)$ before starting CG. Assume the initial guess is the zero field. At each iteration, operate within the zero-mean subspace, noting that for the periodic discrete Laplacian $A$, one has $\\sum_{i,j} (A u)_{i,j} = 0$ for any $u$, hence $A$ maps zero-mean fields to zero-mean fields. Use a standard relative residual stopping criterion with respect to the $\\ell^2$ norm. If $\\|b\\|_2 = 0$, the solver should terminate immediately with zero iterations.\n\n- Optimize the matrix-free stencil application in the sense of vectorized periodic neighbor access, avoiding explicit sparse matrices. Then quantify arithmetic intensity (floating-point operations per byte moved) for the stencil application in double precision under two memory models:\n  - Naive model: treat each stencil evaluation per grid point as loading $5$ double values (the center and $4$ neighbors) and storing $1$ double result, for a total of $6$ doubles per point. Count floating-point operations by counting additions, subtractions, and multiplications as one floating-point operation each, and assume $h^{-2}$ is precomputed so there is no division.\n  - Streaming-optimized model: assume perfect reuse so that, amortized per output grid point, one new input double is read and one result double is written.\n\n- Use the Roofline model to bound the achievable performance of the stencil kernel. If $I$ denotes arithmetic intensity in floating-point operations per byte and $B$ denotes sustained memory bandwidth in bytes per second, the bandwidth-bound performance is $I \\cdot B$ floating-point operations per second. The roofline bound is the minimum of the machine peak floating-point rate and $I \\cdot B$.\n\nImplement your program to produce outputs for the following test suite, aggregating all results into a single line as specified below:\n\n- Test T$1$ (discrete manufactured solution at moderate resolution):\n  - Grid: $N = 64$, domain $[0,1)^2$, cell centers at $x_i = (i+\\tfrac{1}{2})h$, $y_j = (j+\\tfrac{1}{2})h$, where $h = 1/N$.\n  - Define $u^\\star(x,y) = \\sin(2\\pi x)\\cos(2\\pi y)$ and sample it at cell centers to obtain $u^\\star_{i,j}$.\n  - Define $b = A u^\\star$ using your matrix-free operator.\n  - Solve $A u = b$ with Projected Conjugate Gradient, using relative tolerance $\\varepsilon = 10^{-12}$ and maximum iterations $K_{\\max} = 1000$.\n  - Output for T$1$: the maximum absolute error $\\max_{i,j} |u_{i,j} - u^\\star_{i,j}|$ as a floating-point number.\n\n- Test T$2$ (discrete manufactured solution probing periodic wrap-around at small $N$):\n  - Grid: $N = 8$, same domain and cell centers.\n  - Define $u^\\star(x,y) = \\cos(2\\pi \\cdot 3\\, x) + \\sin(2\\pi \\cdot 2\\, y)$ and sample it at cell centers.\n  - Define $b = A u^\\star$ and solve $A u = b$ with relative tolerance $\\varepsilon = 10^{-12}$ and maximum iterations $K_{\\max} = 200$.\n  - Output for T$2$: the maximum absolute error $\\max_{i,j} |u_{i,j} - u^\\star_{i,j}|$ as a floating-point number.\n\n- Test T$3$ (nullspace edge case):\n  - Grid: $N = 32$.\n  - Define $b_{i,j} = 1$ for all $i,j$.\n  - Solve $A u = b$ with the same solver; the solver must project $b$ to zero mean prior to iteration. The expected behavior is immediate termination since $P(b) = 0$.\n  - Output for T$3$: the integer number of Conjugate Gradient iterations actually performed.\n\n- Test T$4$ (roofline-bound performance in gigaflops per second for double precision):\n  - Use the arithmetic intensity values you derive for the stencil under the naive and streaming-optimized memory models.\n  - Use the following machine specifications for double precision:\n    - Central Processing Unit (CPU): peak floating-point rate $P_{\\mathrm{CPU}} = 200$ gigaflops per second and sustained memory bandwidth $B_{\\mathrm{CPU}} = 55$ gigabytes per second.\n    - Graphics Processing Unit (GPU): peak floating-point rate $P_{\\mathrm{GPU}} = 14000$ gigaflops per second and sustained memory bandwidth $B_{\\mathrm{GPU}} = 900$ gigabytes per second.\n  - Compute the roofline-bound performance for both the naive and streaming-optimized arithmetic intensities on both machines, taking the minimum of peak and bandwidth-bound values.\n  - Output for T$4$: a list of four floating-point numbers $[G_{\\mathrm{CPU,naive}}, G_{\\mathrm{GPU,naive}}, G_{\\mathrm{CPU,opt}}, G_{\\mathrm{GPU,opt}}]$ in gigaflops per second.\n\nFinal output format requirement:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{T1}, \\text{T2}, \\text{T3}, \\text{T4}]$, where T$1$ and T$2$ are floating-point numbers, T$3$ is an integer, and T$4$ is a list of four floating-point numbers. For example, an output line could look like $[\\text{float},\\text{float},\\text{int},[\\text{float},\\text{float},\\text{float},\\text{float}]]$.\n\nNo physical units other than gigaflops per second and gigabytes per second are involved, and all angles are in radians as implied by trigonometric functions. All computations must use double-precision arithmetic, and the program must be fully deterministic without any external input.",
            "solution": "The user-provided problem has been analyzed and validated.\n\n### Step 1: Extract Givens\n\n- **Problem:** Solve the Poisson equation $-\\nabla^2 u = b$ on a two-dimensional square grid with periodic boundary conditions.\n- **Discretization:** Finite-volume method on a uniform $N \\times N$ grid with spacing $h$, resulting in a $5$-point stencil for the operator $A = -\\Delta_h$.\n- **Operator Formula:** $(A u)_{i,j} = \\frac{4\\,u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2}$, with periodic indexing.\n- **Operator Properties:** $A$ is symmetric positive semi-definite with a one-dimensional nullspace of constant fields.\n- **Solvability Condition:** The system $A u = b$ is solvable iff $b$ has zero mean.\n- **Solver:** Projected Conjugate Gradient (CG) method.\n- **Projection Operator:** $P(v) = v - \\overline{v}$, where $\\overline{v} = \\frac{1}{N^2} \\sum_{i,j} v_{i,j}$.\n- **CG Procedure:**\n    1.  Project the right-hand side: $b \\leftarrow P(b)$.\n    2.  Use initial guess $u_0 = 0$.\n    3.  Iterate within the zero-mean subspace.\n    4.  Stopping criterion: relative residual $\\|r_k\\|_2  \\varepsilon \\|b_{\\text{projected}}\\|_2$.\n    5.  If $\\|b_{\\text{projected}}\\|_2 = 0$, terminate with $0$ iterations.\n- **Arithmetic:** Double precision.\n- **Analysis Task:** Quantify arithmetic intensity and Roofline-bound performance for the stencil application.\n- **Arithmetic Intensity Models:**\n    -   Naive model: Load $5$ doubles, store $1$ double per point.\n    -   Streaming-optimized model: Amortized, read $1$ double, write $1$ double per point.\n- **FLOP Counting:** Add, subtract, multiply count as $1$ FLOP each. $h^{-2}$ is precomputed.\n- **Test Case T1:**\n    -   $N = 64$, domain $[0,1)^2$, $h = 1/N$.\n    -   $u^\\star(x,y) = \\sin(2\\pi x)\\cos(2\\pi y)$.\n    -   $b = A u^\\star$.\n    -   Solve $A u = b$ with $\\varepsilon = 10^{-12}$, $K_{\\max} = 1000$.\n    -   Output: $\\max_{i,j} |u_{i,j} - u^\\star_{i,j}|$.\n- **Test Case T2:**\n    -   $N=8$.\n    -   $u^\\star(x,y) = \\cos(2\\pi \\cdot 3\\, x) + \\sin(2\\pi \\cdot 2\\, y)$.\n    -   $b = A u^\\star$.\n    -   Solve $A u = b$ with $\\varepsilon = 10^{-12}$, $K_{\\max} = 200$.\n    -   Output: $\\max_{i,j} |u_{i,j} - u^\\star_{i,j}|$.\n- **Test Case T3:**\n    -   $N=32$.\n    -   $b_{i,j} = 1$ for all $i,j$.\n    -   Solve $A u = b$.\n    -   Output: Number of CG iterations.\n- **Test Case T4:**\n    -   CPU specs: $P_{\\mathrm{CPU}} = 200$ GFLOP/s, $B_{\\mathrm{CPU}} = 55$ GB/s.\n    -   GPU specs: $P_{\\mathrm{GPU}} = 14000$ GFLOP/s, $B_{\\mathrm{GPU}} = 900$ GB/s.\n    -   Compute Roofline-bound performance for both models on both architectures.\n    -   Output: $[G_{\\mathrm{CPU,naive}}, G_{\\mathrm{GPU,naive}}, G_{\\mathrm{CPU,opt}}, G_{\\mathrm{GPU,opt}}]$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard problem in numerical linear algebra and scientific computing: solving the Poisson equation with a singular operator. The methodology prescribed—using a manufactured solution for verification, projecting the system to handle the nullspace, and applying the Conjugate Gradient method—is entirely correct and standard practice. The performance analysis using the Roofline model is also a standard technique in high-performance computing. All parameters and test cases are well-defined, and there are no contradictions or missing pieces of critical information. The problem is a valid and well-formulated exercise in computational science.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n***\n\n### Solution Design\n\nThe solution is broken down into four parts: the matrix-free operator implementation, the Projected Conjugate Gradient solver, the performance analysis, and the execution of the specified test cases.\n\n**1. Matrix-Free Operator for the Discrete Laplacian**\n\nThe core of the problem is the linear system $A u = b$, where $A = -\\Delta_h$ is the discrete Laplacian operator on an $N \\times N$ grid with periodic boundary conditions. Storing the matrix $A$ explicitly, which would be an $N^2 \\times N^2$ matrix, is computationally prohibitive for large $N$. Instead, we implement a *matrix-free* operator, which is a function that computes the matrix-vector product $v = A u$ without forming $A$.\n\nThe action of the operator on a grid function $u$ at cell $(i,j)$ is given by:\n$$\n(A u)_{i,j} \\;=\\; \\frac{4\\,u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2}\n$$\nThe periodic boundary conditions imply that indices are taken modulo $N$. For example, $u_{N,j} = u_{0,j}$. This operation can be implemented efficiently using array-based shifting operations. For a 2D array `u` representing the field, the neighbors can be accessed using vectorized circular shifts. For example, `numpy.roll(u, shift=-1, axis=0)` provides the field values from the neighbor at $(i+1, j)$ for all $i,j$. The operator can thus be written as:\n```\nAu = (1/h**2) * (4*u - np.roll(u, -1, axis=0) - np.roll(u, 1, axis=0) \n                      - np.roll(u, -1, axis=1) - np.roll(u, 1, axis=1))\n```\nwhere all operations are element-wise on the $N \\times N$ arrays.\n\n**2. Projected Conjugate Gradient Method**\n\nThe operator $A$ is symmetric positive semi-definite. Its nullspace is the set of constant fields, i.e., $A u = 0$ if and only if $u_{i,j} = C$ for all $i,j$. Consequently, a solution to $A u = b$ exists if and only if $b$ is orthogonal to the nullspace. For the standard inner product, this means the sum of the elements of $b$ must be zero. The solution $u$ is then unique up to an additive constant.\n\nTo ensure a unique solution, we restrict the problem to the subspace of zero-mean fields. This is achieved by applying an orthogonal projection $P$ to the right-hand side $b$:\n$$\nb' = P(b) = b - \\overline{b}, \\quad \\text{where} \\quad \\overline{b} = \\frac{1}{N^2} \\sum_{i,j} b_{i,j}\n$$\nWe then solve the system $A u = b'$. Since the initial guess is $u_0=0$ (which is zero-mean) and the operator $A$ maps zero-mean fields to zero-mean fields (i.e., $\\overline{Au}=0$ if $\\overline{u}=0$), all subsequent iterates for the residual $r_k$, search direction $p_k$, and solution $u_k$ will remain within the zero-mean subspace. Thus, no projections are needed inside the CG iteration loop. The resulting solution $u$ will be the unique zero-mean solution.\n\nThe algorithm is as follows:\n1.  Given $A, b, u_0=0, \\varepsilon, K_{\\max}$.\n2.  $b' \\leftarrow b - \\overline{b}$.\n3.  Let $b'_{norm} = \\|b'\\|_2$. If $b'_{norm} = 0$, terminate. The solution is $u=0$ and iterations=$0$.\n4.  $r_0 \\leftarrow b'$.\n5.  $p_0 \\leftarrow r_0$.\n6.  For $k = 0, 1, \\dots, K_{\\max}-1$:\n    a. $v_k \\leftarrow A p_k$.\n    b. $\\alpha_k \\leftarrow \\frac{r_k^T r_k}{p_k^T v_k}$.\n    c. $u_{k+1} \\leftarrow u_k + \\alpha_k p_k$.\n    d. $r_{k+1} \\leftarrow r_k - \\alpha_k v_k$.\n    e. If $\\|r_{k+1}\\|_2  \\varepsilon \\cdot b'_{norm}$, terminate and return $u_{k+1}$.\n    f. $\\beta_k \\leftarrow \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$.\n    g. $p_{k+1} \\leftarrow r_{k+1} + \\beta_k p_k$.\n\nFor the manufactured solution tests (T1, T2), the true solution $u^\\star$ is constructed to have a mean of zero. Therefore, the zero-mean solution $u$ found by the solver should be a highly accurate approximation of $u^\\star$.\n\n**3. Arithmetic Intensity and Roofline Analysis**\n\nThe Roofline model provides an upper bound on performance (in GFLOP/s) based on a kernel's arithmetic intensity and the machine's capabilities.\n$$\n\\text{Performance} = \\min(\\text{Peak Performance}, \\text{Arithmetic Intensity} \\times \\text{Memory Bandwidth})\n$$\nArithmetic intensity ($I$) is the ratio of floating-point operations (FLOPs) to data movement (bytes).\n$$\nI = \\frac{\\text{FLOPs}}{\\text{Bytes}}\n$$\n\nFor applying the stencil operator $Au$, we first count the FLOPs per grid point. Assuming $h_{inv2} = 1/h^2$ is precomputed:\n$$ (A u)_{i,j} = (4.0 \\cdot u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}) \\cdot h_{inv2} $$\nThis expression contains $1$ multiplication ($4.0 \\cdot u_{i,j}$), $4$ subtractions, and $1$ final multiplication by $h_{inv2}$. Total FLOPs per point = $1+4+1 = 6$ FLOPs.\n\nNext, we analyze the data movement in double precision (8 bytes/value):\n-   **Naive Model:** Assumes no data reuse between stencil applications. For each output point $(Au)_{i,j}$, we must load $u_{i,j}$ and its $4$ neighbors (total $5$ doubles) and store the result (1 double).\n    -   Bytes moved = $(5 \\text{ loads} + 1 \\text{ store}) \\times 8 \\text{ bytes/double} = 6 \\times 8 = 48$ bytes.\n    -   $I_{\\text{naive}} = \\frac{6 \\text{ FLOPs}}{48 \\text{ bytes}} = 0.125$ FLOPs/byte.\n-   **Streaming-Optimized Model:** Assumes perfect data reuse through caches. For a large grid traversed in a streaming fashion (e.g., row by row), each point $u_{i,j}$ is read from main memory once, used for computing its own output and its neighbors' outputs, and then discarded from the highest cache levels. Amortized over the grid, we read one input value and write one output value per point.\n    -   Bytes moved = $(1 \\text{ read} + 1 \\text{ write}) \\times 8 \\text{ bytes/double} = 2 \\times 8 = 16$ bytes.\n    -   $I_{\\text{opt}} = \\frac{6 \\text{ FLOPs}}{16 \\text{ bytes}} = 0.375$ FLOPs/byte.\n\nUsing the given machine parameters, we can calculate the Roofline bounds ($G$) for Test T4:\n-   $P_{\\mathrm{CPU}} = 200$ GFLOP/s, $B_{\\mathrm{CPU}} = 55$ GB/s.\n-   $P_{\\mathrm{GPU}} = 14000$ GFLOP/s, $B_{\\mathrm{GPU}} = 900$ GB/s.\n\n-   $G_{\\mathrm{CPU,naive}} = \\min(P_{\\mathrm{CPU}}, I_{\\text{naive}} \\cdot B_{\\mathrm{CPU}}) = \\min(200, 0.125 \\cdot 55) = \\min(200, 6.875) = 6.875$ GFLOP/s.\n-   $G_{\\mathrm{GPU,naive}} = \\min(P_{\\mathrm{GPU}}, I_{\\text{naive}} \\cdot B_{\\mathrm{GPU}}) = \\min(14000, 0.125 \\cdot 900) = \\min(14000, 112.5) = 112.5$ GFLOP/s.\n-   $G_{\\mathrm{CPU,opt}} = \\min(P_{\\mathrm{CPU}}, I_{\\text{opt}} \\cdot B_{\\mathrm{CPU}}) = \\min(200, 0.375 \\cdot 55) = \\min(200, 20.625) = 20.625$ GFLOP/s.\n-   $G_{\\mathrm{GPU,opt}} = \\min(P_{\\mathrm{GPU}}, I_{\\text{opt}} \\cdot B_{\\mathrm{GPU}}) = \\min(14000, 0.375 \\cdot 900) = \\min(14000, 337.5) = 337.5$ GFLOP/s.\n\nThe low arithmetic intensity makes this kernel *memory-bound* on all considered architectures, meaning performance is limited by memory bandwidth, not peak FLOP rate.\n\n**4. Test Case Execution**\n\nThe implemented functions are used to run the four test cases.\n-   **T1  T2:** A grid is created, a manufactured solution $u^\\star$ is sampled, and the RHS $b = A u^\\star$ is computed. The CG solver is called to find a solution $u$. The maximum absolute error, $\\max_{i,j} |u_{i,j} - u^\\star_{i,j}|$, is computed and reported. As the chosen $u^\\star$ functions have (near) zero mean, this error should be small, on the order of the solver tolerance.\n-   **T3:** The RHS is a constant field, $b_{i,j}=1$. The first step in the solver is to project $b$ to the zero-mean subspace, which results in $b' = b - \\overline{b} = 1 - 1 = 0$. The norm of the projected RHS is zero, so the solver is designed to terminate immediately with $0$ iterations.\n-   **T4:** The Roofline bounds are computed as derived above.\n\nThe final code aggregates these results into the specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    # Helper function for the matrix-free discrete Laplacian operator\n    def apply_A(u, h):\n        \"\"\"\n        Applies the 5-point stencil for the negative Laplacian with periodic\n        boundaries to a 2D field `u`.\n        \"\"\"\n        # Precompute 1/h^2 for efficiency\n        h_sq_inv = 1.0 / (h * h)\n        \n        # Apply the stencil using vectorized numpy.roll for periodic shifts\n        # (Au)_{i,j} = (4u_ij - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}) / h^2\n        Au = 4.0 * u\n        Au -= np.roll(u, shift=1, axis=0)\n        Au -= np.roll(u, shift=-1, axis=0)\n        Au -= np.roll(u, shift=1, axis=1)\n        Au -= np.roll(u, shift=-1, axis=1)\n        \n        return Au * h_sq_inv\n\n    # Implementation of the Projected Conjugate Gradient solver\n    def projected_cg(b, h, tol, max_iter):\n        \"\"\"\n        Solves Au = b for a symmetric positive semi-definite operator A\n        using the Projected Conjugate Gradient method.\n        \"\"\"\n        u = np.zeros_like(b, dtype=np.float64)\n        \n        # Project the right-hand side b to the zero-mean subspace\n        b_proj = b - np.mean(b)\n        \n        b_norm = np.linalg.norm(b_proj)\n        \n        # If the projected RHS is zero, the solution is in the nullspace.\n        # Terminate immediately.\n        if b_norm == 0.0:\n            return u, 0\n            \n        r = b_proj  # Initial residual r_0 = b_proj - A*u_0 = b_proj\n        p = r.copy() # Initial search direction\n        rs_old = np.sum(r * r) # r.T @ r\n\n        for i in range(max_iter):\n            Ap = apply_A(p, h)\n            p_dot_Ap = np.sum(p * Ap)\n\n            # If p_dot_Ap is zero, p is in the nullspace, which shouldn't happen\n            # if the setup is correct and b is not in the nullspace.\n            # A small value could lead to instability.\n            if p_dot_Ap  1e-16:\n                break\n            \n            alpha = rs_old / p_dot_Ap\n            \n            u += alpha * p\n            r -= alpha * Ap\n            \n            rs_new = np.sum(r * r)\n            \n            if np.sqrt(rs_new)  tol * b_norm:\n                return u, i + 1\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return u, max_iter\n\n    # --- Test T1: Moderate resolution manufactured solution ---\n    N1 = 64\n    h1 = 1.0 / N1\n    tol1 = 1e-12\n    max_iter1 = 1000\n    \n    # Create grid of cell centers\n    grid_pts = (np.arange(N1) + 0.5) * h1\n    x1, y1 = np.meshgrid(grid_pts, grid_pts, indexing='ij')\n    \n    # Define manufactured solution u_star\n    u_star1 = np.sin(2 * np.pi * x1) * np.cos(2 * np.pi * y1)\n    \n    # Compute the right-hand side b = A*u_star\n    b1 = apply_A(u_star1, h1)\n    \n    # Solve the system\n    u_sol1, iters1 = projected_cg(b1, h1, tol1, max_iter1)\n    \n    # The solver finds the unique zero-mean solution. The u_star chosen\n    # has a mean that is numerically close to zero, so u_sol should be\n    # very close to u_star.\n    error1 = np.max(np.abs(u_sol1 - u_star1))\n    \n    # --- Test T2: Small resolution manufactured solution ---\n    N2 = 8\n    h2 = 1.0 / N2\n    tol2 = 1e-12\n    max_iter2 = 200\n\n    grid_pts2 = (np.arange(N2) + 0.5) * h2\n    x2, y2 = np.meshgrid(grid_pts2, grid_pts2, indexing='ij')\n    \n    u_star2 = np.cos(2 * np.pi * 3.0 * x2) + np.sin(2 * np.pi * 2.0 * y2)\n    \n    b2 = apply_A(u_star2, h2)\n    \n    u_sol2, iters2 = projected_cg(b2, h2, tol2, max_iter2)\n    \n    error2 = np.max(np.abs(u_sol2 - u_star2))\n\n    # --- Test T3: Nullspace edge case ---\n    N3 = 32\n    h3 = 1.0 / N3\n    tol3 = 1e-12\n    max_iter3 = 100\n    \n    # RHS is a constant field, which is in the nullspace of A after projection\n    b3 = np.ones((N3, N3), dtype=np.float64)\n    \n    u_sol3, iters3 = projected_cg(b3, h3, tol3, max_iter3)\n\n    # --- Test T4: Roofline-bound performance analysis ---\n    # Stencil FLOPs per point: (4*u - u_n - u_s - u_e - u_w) / h^2\n    # 1 mult (4*), 4 subs, 1 mult (by 1/h^2) = 6 FLOPs\n    flops_per_point = 6.0\n    bytes_per_double = 8.0\n\n    # Naive model: 5 loads (center + 4 neighbors), 1 store\n    bytes_naive = (5 + 1) * bytes_per_double\n    I_naive = flops_per_point / bytes_naive\n\n    # Streaming-optimized model: 1 read, 1 write (amortized)\n    bytes_opt = (1 + 1) * bytes_per_double\n    I_opt = flops_per_point / bytes_opt\n\n    # Machine specs\n    P_cpu = 200.0  # GFLOP/s\n    B_cpu = 55.0   # GB/s\n    P_gpu = 14000.0 # GFLOP/s\n    B_gpu = 900.0  # GB/s\n    \n    # Calculate Roofline bounds\n    G_cpu_naive = min(P_cpu, I_naive * B_cpu)\n    G_gpu_naive = min(P_gpu, I_naive * B_gpu)\n    G_cpu_opt = min(P_cpu, I_opt * B_cpu)\n    G_gpu_opt = min(P_gpu, I_opt * B_gpu)\n    \n    results_T4 = [G_cpu_naive, G_gpu_naive, G_cpu_opt, G_gpu_opt]\n\n    # --- Aggregate and print results ---\n    final_results = [error1, error2, iters3, results_T4]\n    \n    # Custom formatting to match the required output style\n    # e.g., [float,float,int,[float,float,float,float]]\n    t4_str = f\"[{','.join(map(str, final_results[3]))}]\"\n    print(f\"[{final_results[0]},{final_results[1]},{final_results[2]},{t4_str}]\")\n\nsolve()\n```"
        }
    ]
}