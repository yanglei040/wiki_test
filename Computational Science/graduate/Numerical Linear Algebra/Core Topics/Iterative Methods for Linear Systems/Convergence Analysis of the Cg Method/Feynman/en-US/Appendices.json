{
    "hands_on_practices": [
        {
            "introduction": "The convergence rate of the Conjugate Gradient method is not just a qualitative observation; it can be quantified with remarkable precision. This exercise guides you through the derivation of the classic a priori error bound using Chebyshev polynomials, providing a direct link between a matrix's spectral properties and the number of iterations required for a desired accuracy . Mastering this calculation is fundamental to predicting the performance of CG on a given problem and understanding why preconditioning is so effective.",
            "id": "3541532",
            "problem": "Consider a real symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ with spectrum $\\sigma(A) \\subset [1,100]$. Let $x^{\\star}$ be the unique solution of $Ax=b$ and let $e_k := x_k - x^{\\star}$ denote the error after $k$ iterations of the Conjugate Gradient (CG) method. Use the Chebyshev-polynomial-based optimality characterization of CG as a polynomial method and the spectral theorem to derive an a priori bound on the $A$-norm of the error of the form $\\|e_k\\|_{A} \\leq \\gamma_k \\|e_0\\|_{A}$, where $\\gamma_k$ is determined by the best uniform approximation of the zero function on the interval $[1,100]$ by degree-$k$ polynomials constrained by $p(0)=1$. From first principles, construct the bound by mapping $[1,100]$ to $[-1,1]$, invoking the minimax property of Chebyshev polynomials, and simplifying the resulting expression to a closed form in terms of the spectral condition number. Then, using this Chebyshev-based estimate, determine the smallest integer iteration count $k \\in \\mathbb{N}$ that guarantees \n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq 10^{-6}.\n$$\nExpress your final answer as a single integer. No rounding beyond the minimal integer requirement is needed.",
            "solution": "The problem requires the derivation of an a priori error bound for the Conjugate Gradient (CG) method and its application to find a specific iteration count.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Matrix $A \\in \\mathbb{R}^{n \\times n}$ is real, symmetric, and positive definite (SPD).\n- The spectrum of $A$, denoted $\\sigma(A)$, is a subset of the interval $[1, 100]$.\n- $x^{\\star}$ is the unique solution of the linear system $Ax=b$.\n- $e_k = x_k - x^{\\star}$ is the error vector at the $k$-th iteration.\n- The method is the Conjugate Gradient (CG) method.\n- The task involves using the characterization of CG as a polynomial method and the properties of Chebyshev polynomials.\n- The goal is to find the smallest integer $k$ such that the relative error in the $A$-norm, $\\|e_k\\|_{A}/\\|e_0\\|_{A}$, is bounded by $10^{-6}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the well-established theory of numerical linear algebra, specifically the convergence analysis of Krylov subspace methods. All premises, including the properties of matrix $A$ and the formulation of the CG error, are standard and correct. The problem is well-posed, providing all necessary information (the spectral range and the error tolerance) to determine a unique integer solution for $k$. The language is objective and precise. The problem is a standard, non-trivial exercise in the field. Therefore, the problem is deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Derivation of the Chebyshev-based error bound:**\n\nThe Conjugate Gradient method for solving $Ax=b$ with an initial guess $x_0$ generates a sequence of approximations $x_k$. The error vector $e_k = x_k - x^{\\star}$ can be expressed in terms of the initial error $e_0 = x_0 - x^{\\star}$ using a polynomial. Specifically, the iterates satisfy $x_k - x_0 \\in \\mathcal{K}_k(A, r_0)$, where $r_0 = b - Ax_0 = -Ae_0$ is the initial residual and $\\mathcal{K}_k$ is the $k$-th Krylov subspace. This implies that the error $e_k$ can be written as:\n$$ e_k = p_k(A) e_0 $$\nwhere $p_k$ is a polynomial of degree at most $k$ belonging to the set $\\mathcal{P}_k$, with the constraint $p_k(0) = 1$. The CG method has an optimality property: it finds the specific polynomial $p_k$ that minimizes the error in the $A$-norm, which is defined as $\\|v\\|_A = \\sqrt{v^\\top A v}$.\n$$ \\|e_k\\|_A = \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\|q_k(A)e_0\\|_A $$\nTo obtain an a priori bound that is independent of the initial error $e_0$, we use the spectral decomposition of $A$. Let $A=V\\Lambda V^\\top$ be the eigendecomposition of $A$, where $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$ contains the eigenvalues of $A$. The $A$-norm of the error can be bounded as follows:\n$$ \\|e_k\\|_A = \\|p_k(A) e_0\\|_A \\le \\left( \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\max_{\\lambda \\in \\sigma(A)} |q_k(\\lambda)| \\right) \\|e_0\\|_A $$\nThe problem states that $\\sigma(A) \\subset [1, 100]$. Let $\\alpha = \\lambda_{\\min}(A)$ and $\\beta = \\lambda_{\\max}(A)$. The bound is most conservative when the spectrum spans the entire given interval, so we consider $\\lambda \\in [\\alpha, \\beta]$ where we take $\\alpha=1$ and $\\beta=100$. The problem reduces to a classic polynomial approximation problem:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\max_{\\lambda \\in [\\alpha, \\beta]} |q_k(\\lambda)| $$\nTo solve this minimax problem, we map the interval $[\\alpha, \\beta]$ to the canonical interval $[-1, 1]$ using the affine transformation:\n$$ x(\\lambda) = \\frac{2\\lambda - (\\beta+\\alpha)}{\\beta-\\alpha} $$\nA polynomial $q_k(\\lambda)$ of degree $k$ becomes a polynomial $Q_k(x)$ of degree $k$ under this mapping. The constraint $q_k(0)=1$ becomes a constraint on $Q_k$ at the point $x_0 = x(0)$:\n$$ x_0 = \\frac{2(0) - (\\beta+\\alpha)}{\\beta-\\alpha} = -\\frac{\\beta+\\alpha}{\\beta-\\alpha} $$\nThe problem is now to find $\\min \\max_{x \\in [-1, 1]} |Q_k(x)|$ subject to $Q_k(x_0)=1$. The solution is given by a scaled Chebyshev polynomial of the first kind, $T_k(x)$:\n$$ Q_k(x) = \\frac{T_k(x)}{T_k(x_0)} $$\nThe maximum value of $|Q_k(x)|$ on $[-1, 1]$ is $\\frac{\\max_{x \\in [-1, 1]}|T_k(x)|}{|T_k(x_0)|} = \\frac{1}{|T_k(x_0)|}$. Since $\\alpha, \\beta > 0$, we have $x_0  -1$. For $z  -1$, $|T_k(z)| = T_k(|z|)$. Thus, the bound becomes:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\beta+\\alpha}{\\beta-\\alpha}\\right)} $$\nThis expression can be written in terms of the spectral condition number $\\kappa = \\beta/\\alpha$:\n$$ \\frac{\\beta+\\alpha}{\\beta-\\alpha} = \\frac{(\\beta/\\alpha)+1}{(\\beta/\\alpha)-1} = \\frac{\\kappa+1}{\\kappa-1} $$\nSo, the final form of the bound is:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)} $$\n\n**Calculation of the iteration count $k$:**\n\nWe are given $\\sigma(A) \\subset [1, 100]$, so we set $\\alpha=1$ and $\\beta=100$. The condition number is $\\kappa = 100/1 = 100$. We want to find the smallest integer $k$ such that:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 10^{-6} $$\nUsing our derived bound, we must satisfy:\n$$ \\frac{1}{T_k\\left(\\frac{100+1}{100-1}\\right)} \\le 10^{-6} \\implies T_k\\left(\\frac{101}{99}\\right) \\ge 10^6 $$\nFor an argument $|z| \\ge 1$, the Chebyshev polynomial can be expressed using the hyperbolic cosine function: $T_k(z) = \\cosh(k \\, \\text{arccosh}(z))$. The inequality becomes:\n$$ \\cosh\\left(k \\, \\text{arccosh}\\left(\\frac{101}{99}\\right)\\right) \\ge 10^6 $$\nSince $\\cosh(y)$ is a strictly increasing function for $y > 0$, we can take the inverse hyperbolic cosine of both sides:\n$$ k \\, \\text{arccosh}\\left(\\frac{101}{99}\\right) \\ge \\text{arccosh}(10^6) $$\nSolving for $k$:\n$$ k \\ge \\frac{\\text{arccosh}(10^6)}{\\text{arccosh}\\left(\\frac{101}{99}\\right)} $$\nWe can simplify the denominator using the identity $\\text{arccosh}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) = \\ln\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)$. For $\\kappa=100$:\n$$ \\text{arccosh}\\left(\\frac{101}{99}\\right) = \\ln\\left(\\frac{\\sqrt{100}+1}{\\sqrt{100}-1}\\right) = \\ln\\left(\\frac{10+1}{10-1}\\right) = \\ln\\left(\\frac{11}{9}\\right) $$\nSo, the inequality for $k$ is:\n$$ k \\ge \\frac{\\text{arccosh}(10^6)}{\\ln(11/9)} $$\nNow we evaluate this expression numerically. The identity for the inverse hyperbolic cosine is $\\text{arccosh}(z) = \\ln(z + \\sqrt{z^2-1})$.\n$$ \\text{arccosh}(10^6) = \\ln(10^6 + \\sqrt{(10^6)^2 - 1}) = \\ln(10^6 + \\sqrt{10^{12} - 1}) $$\nFor large $z$, $\\text{arccosh}(z) \\approx \\ln(2z)$.\n$$ \\text{arccosh}(10^6) \\approx \\ln(2 \\times 10^6) \\approx 14.508658 $$\nThe denominator is:\n$$ \\ln(11/9) \\approx 0.200671 $$\nThus,\n$$ k \\ge \\frac{14.508658}{0.200671} \\approx 72.3005 $$\nSince the iteration count $k$ must be an integer, the smallest integer value that satisfies this condition is $k=73$.",
            "answer": "$$\n\\boxed{73}\n$$"
        },
        {
            "introduction": "While theory guarantees that the error in the $A$-norm decreases monotonically, practical implementations often monitor the Euclidean norm of the residual as a stopping criterion. This coding exercise challenges you to implement the CG algorithm and construct a specific counterexample where the residual norm temporarily increases . This hands-on experience reveals a subtle yet important aspect of the algorithm's geometric behavior and clarifies the distinction between theoretical error measures and practical convergence monitoring.",
            "id": "3541518",
            "problem": "You must write a complete, runnable program that demonstrates and analyzes the monotonic behavior of the Conjugate Gradient (CG) method for a symmetric positive definite (SPD) linear system. The scientific context is numerical linear algebra and the convergence analysis of the Conjugate Gradient method. The objective is to construct a concrete counterexample in which the Euclidean residual norm $\\|r_k\\|_2$ is not monotonically decreasing while the energy norm of the error $\\|e_k\\|_A$ strictly decreases, and to quantify the behavior across a small test suite.\n\nDefinitions and core facts to be used as the base:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) if $A^\\top = A$ and $x^\\top A x  0$ for all nonzero $x \\in \\mathbb{R}^n$.\n- The Conjugate Gradient (CG) method seeks to solve $A x = b$ with $A$ SPD by iteratively producing $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$, where $r_k = b - A x_k$ is the residual, $e_k = x^\\star - x_k$ is the error with $x^\\star$ the exact solution, and $\\mathcal{K}_k(A, r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ is the $k$-th Krylov subspace.\n- The $A$-energy inner product and norm are $\\langle u, v \\rangle_A = u^\\top A v$ and $\\|u\\|_A = \\sqrt{u^\\top A u}$. The CG method is the Galerkin projection with respect to $\\langle \\cdot, \\cdot \\rangle_A$ onto $x_0 + \\mathcal{K}_k(A, r_0)$, which implies $\\|e_k\\|_A$ decreases monotonically (and strictly unless the exact solution is reached).\n- The Euclidean norm $\\|\\cdot\\|_2$ is used to measure $\\|r_k\\|_2$, and this quantity is not guaranteed to be monotonic in CG.\n\nYour program must:\n1. Implement the Conjugate Gradient (CG) method for a given SPD matrix $A$, right-hand side $b$, and initial guess $x_0$, generating iterates $x_k$, residuals $r_k$, and computing the exact solution $x^\\star$ via a direct solver to measure $\\|e_k\\|_A$.\n2. For each test case, run CG until either $k$ reaches $n$ (the matrix dimension) or until $\\|r_k\\|_2$ falls below a stopping tolerance. Use a stopping tolerance of $10^{-12}\\,\\|b\\|_2$ in the Euclidean norm.\n3. Compute the sequences $\\{\\|r_k\\|_2\\}_{k=0}^{k_{\\mathrm{end}}}$ and $\\{\\|e_k\\|_A\\}_{k=0}^{k_{\\mathrm{end}}}$, where $k_{\\mathrm{end}}$ is the last computed iteration.\n4. Determine whether the residual sequence is nonincreasing: $\\|r_{k+1}\\|_2 \\le \\|r_k\\|_2$ for all consecutive iterates, and find the smallest index $k$ (using zero-based indexing) such that $\\|r_{k+1}\\|_2  \\|r_k\\|_2$, or return $-1$ if no such index exists. Use a strict comparison with a numerical tolerance of $10^{-14}$, i.e., treat increases only when $\\|r_{k+1}\\|_2  \\|r_k\\|_2 + 10^{-14}$.\n5. Determine whether the energy norm sequence is strictly decreasing: $\\|e_{k+1}\\|_A  \\|e_k\\|_A$ for all consecutive iterates until termination. Use a strict comparison with numerical tolerance $10^{-14}$, i.e., require $\\|e_{k+1}\\|_A \\le \\|e_k\\|_A - 10^{-14}$ at each step.\n\nGeometric explanation requirement:\n- While you are not required to print the geometric explanation, your program must include a test case that exhibits the phenomenon: a counterexample where $\\|r_k\\|_2$ is not monotonically decreasing even though $\\|e_k\\|_A$ strictly decreases. The construction should be based on SPD matrices whose eigenvalues are highly anisotropic and rotated away from coordinate axes to create nontrivial angles between the residual and the $A$-scaled subspaces, which affects the Euclidean norm of the residual.\n\nMatrix construction:\n- Construct SPD matrices $A$ of the form $A = R^\\top \\Lambda R$, where $\\Lambda$ is diagonal with positive entries and $R$ is an orthogonal rotation matrix built from planar rotations. Use block-diagonal rotations that act in the $(1,2)$ and $(3,4)$ coordinate planes with angles expressed in radians.\n\nTest suite:\nProvide the following four test cases:\n- Test case $1$ (general, anisotropic, likely non-monotone residual): $n=4$, $\\Lambda=\\mathrm{diag}(1, 20, 200, 5000)$, $R$ is the block-diagonal rotation with angles $\\theta_{12}=0.7$ and $\\theta_{34}=0.5$, $b=[1,1,1,1]^\\top$, $x_0=[0,0,0,0]^\\top$.\n- Test case $2$ (boundary condition aligned with an eigenvector, one-step convergence): $n=4$, $\\Lambda=\\mathrm{diag}(2,5,7,11)$, $R=I$, $b=[1,0,0,0]^\\top$, $x_0=[0,0,0,0]^\\top$.\n- Test case $3$ (well-conditioned, nearly isotropic): $n=4$, $\\Lambda=\\mathrm{diag}(1,1.5,2,2.5)$, $R$ with angles $\\theta_{12}=0.2$ and $\\theta_{34}=0.0$, $b=[1,-1,2,-2]^\\top$, $x_0=[0,0,0,0]^\\top$.\n- Test case $4$ (systematic counterexample search over angles): $n=4$, $\\Lambda=\\mathrm{diag}(1,50,500,10000)$, $R$ is formed by $\\theta_{12}\\in\\{0.1,0.25,0.5,0.75,1.0,1.2\\}$ and fixed $\\theta_{34}=0.9$ scanned in order; $b=[1,1,0.1,0.1]^\\top$, $x_0=[0,0,0,0]^\\top$; choose the first $\\theta_{12}$ that exhibits a residual increase while the energy norm strictly decreases, otherwise report no increase.\n\nAngle unit specification:\n- All angles are in radians.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the quadruple $[\\mathrm{res\\_noninc}, \\mathrm{energy\\_strict}, k_{\\mathrm{inc}}, \\theta]$, where:\n- $\\mathrm{res\\_noninc}$ is a boolean indicating whether $\\|r_k\\|_2$ is nonincreasing over the computed iterations.\n- $\\mathrm{energy\\_strict}$ is a boolean indicating whether $\\|e_k\\|_A$ strictly decreases over the computed iterations.\n- $k_{\\mathrm{inc}}$ is the smallest integer $k \\ge 0$ such that $\\|r_{k+1}\\|_2  \\|r_k\\|_2 + 10^{-14}$, or $-1$ if no such index exists.\n- $\\theta$ is the angle $\\theta_{12}$ used to build $R$ for the test case; if a test case does not use a $(1,2)$-rotation, output $0.0$ for this field.\n\nThus, your final printed line must look like:\n$[[\\mathrm{res\\_noninc}_1,\\mathrm{energy\\_strict}_1,k_{\\mathrm{inc},1},\\theta_1],[\\mathrm{res\\_noninc}_2,\\mathrm{energy\\_strict}_2,k_{\\mathrm{inc},2},\\theta_2],[\\mathrm{res\\_noninc}_3,\\mathrm{energy\\_strict}_3,k_{\\mathrm{inc},3},\\theta_3],[\\mathrm{res\\_noninc}_4,\\mathrm{energy\\_strict}_4,k_{\\mathrm{inc},4},\\theta_4]]$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the convergence theory of the Conjugate Gradient (CG) method. The problem is well-posed, with all necessary data, definitions, and constraints provided, and it is free of ambiguity, subjectivity, or any factual or logical inconsistencies.\n\nThe core of the problem is to demonstrate a well-known property of the Conjugate Gradient (CG) method: while it guarantees a monotonic decrease in the $A$-norm of the error, the Euclidean norm of the residual is not guaranteed to decrease monotonically.\n\nThe CG method is an iterative algorithm for solving a linear system $Ax=b$ where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite (SPD). The method constructs a sequence of approximations $x_k$ to the exact solution $x^\\star$. At each iteration $k$, the new approximation $x_k$ is chosen from the affine Krylov subspace $x_0 + \\mathcal{K}_k(A, r_0)$ such that the $A$-norm of the error, $\\|e_k\\|_A = \\|x^\\star - x_k\\|_A$, is minimized. The $A$-norm is defined as $\\|v\\|_A = \\sqrt{v^\\top A v}$.\n\nThis minimization property ensures that the sequence of error norms $\\{\\|e_k\\|_A\\}$ is strictly decreasing, unless the exact solution is found, in which case the error norm becomes and remains zero:\n$$ \\|e_{k+1}\\|_A  \\|e_k\\|_A \\quad \\text{if } e_k \\neq 0 $$\nThis is a fundamental convergence property of the CG method.\n\nThe residual is defined as $r_k = b - A x_k$. Its relationship to the error is $r_k = A e_k$. The Euclidean norm of the residual, $\\|r_k\\|_2$, is a common practical measure for terminating the iteration. However, unlike $\\|e_k\\|_A$, the sequence $\\{\\|r_k\\|_2\\}$ is not guaranteed to be monotonic. An increase in the residual norm, $\\|r_{k+1}\\|_2  \\|r_k\\|_2$, can occur in certain situations.\n\nThis phenomenon can be understood by analyzing the residual update step. In the standard implementation of CG, the update is given by:\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\nThe squared norm of the new residual is:\n$$ \\|r_{k+1}\\|_2^2 = \\|r_k - \\alpha_k A p_k\\|_2^2 = \\|r_k\\|_2^2 - 2\\alpha_k r_k^\\top A p_k + \\alpha_k^2 \\|A p_k\\|_2^2 $$\nFor the norm to decrease, the condition $2\\alpha_k r_k^\\top A p_k  \\alpha_k^2 \\|A p_k\\|_2^2$, which simplifies to $2 r_k^\\top A p_k  \\alpha_k \\|A p_k\\|_2^2$, must hold. Using the standard formula for the optimal step size, $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$, and properties relating the search direction $p_k$ to the residual $r_k$, this inequality can be shown to depend on the properties of the matrix $A$ and the current vectors $p_k$ and $r_k$. Specifically, if the matrix $A$ has a large condition number (a wide spread of eigenvalues) and the search direction $p_k$ has a significant component along an eigenvector corresponding to a large eigenvalue, the term $\\alpha_k^2 \\|A p_k\\|_2^2$ can be large enough to cause an increase in $\\|r_{k+1}\\|_2$. Geometrically, the algorithm takes a step that is optimal for reducing the error in the $A$-norm landscape, but this step might \"overshoot\" in terms of minimizing the Euclidean residual, leading to a temporary increase.\n\nThe program will implement the CG algorithm and construct test matrices of the form $A = R^\\top \\Lambda R$, where $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ is a diagonal matrix of eigenvalues and $R$ is an orthogonal rotation matrix. This construction allows for precise control over the eigenvalues (via $\\Lambda$) and the orientation of the eigenvectors (via $R$). By choosing a large eigenvalue ratio and rotating the eigensystem away from the standard basis, we can create test cases that reliably exhibit the non-monotonic residual behavior.\n\nThe solution will proceed as follows:\n1.  A function to construct the matrix $A = R^\\top \\Lambda R$ given eigenvalues and rotation angles will be created. The rotation matrix $R$ is built as a composition of planar (Givens) rotations. For $n=4$, we use a block-diagonal rotation matrix:\n    $$ R(\\theta_{12}, \\theta_{34}) = \\begin{pmatrix} \\cos\\theta_{12}  -\\sin\\theta_{12}  0  0 \\\\ \\sin\\theta_{12}  \\cos\\theta_{12}  0  0 \\\\ 0  0  \\cos\\theta_{34}  -\\sin\\theta_{34} \\\\ 0  0  \\sin\\theta_{34}  \\cos\\theta_{34} \\end{pmatrix} $$\n2.  The standard CG algorithm is implemented. The algorithm starts with an initial guess $x_0$ and computes:\n    - $r_0 = b - Ax_0$\n    - $p_0 = r_0$\n    - For $k = 0, 1, 2, \\ldots$:\n      - $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$\n      - $x_{k+1} = x_k + \\alpha_k p_k$\n      - $r_{k+1} = r_k - \\alpha_k A p_k$\n      - If $\\|r_{k+1}\\|_2$ is below tolerance, stop.\n      - $\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$\n      - $p_{k+1} = r_{k+1} + \\beta_{k+1} p_k$\n3.  For each iteration, the norms $\\|r_k\\|_2$ and $\\|e_k\\|_A$ are calculated and stored. The exact solution $x^\\star$ required for $e_k=x^\\star - x_k$ is pre-computed using a direct solver.\n4.  After the CG iteration terminates, the stored sequences of norms are analyzed to check for the required monotonicity properties using the specified numerical tolerances ($10^{-14}$). Specifically, we check if $\\|r_{k+1}\\|_2  \\|r_k\\|_2 + 10^{-14}$ to find the first instance of a residual increase, and if $\\|e_{k+1}\\|_A \\le \\|e_k\\|_A - 10^{-14}$ holds for all steps to confirm the strict decrease of the energy norm.\n5.  This process is applied to all specified test cases, including the search in case $4$, and the results are formatted as requested.",
            "answer": "```python\nimport numpy as np\n\ndef construct_matrix(n, lambdas, theta12, theta34):\n    \"\"\"\n    Constructs an SPD matrix A = R.T @ Lambda @ R of size n x n.\n    R is a block-diagonal rotation matrix.\n    \"\"\"\n    if n != 4:\n        raise ValueError(\"This matrix construction is defined for n=4.\")\n    \n    Lambda = np.diag(lambdas)\n    \n    R = np.identity(n)\n    \n    # Rotation in the (1,2) plane (indices 0, 1)\n    if theta12 != 0.0:\n        c12, s12 = np.cos(theta12), np.sin(theta12)\n        R12 = np.array([[c12, -s12], [s12, c12]])\n        R[0:2, 0:2] = R12\n\n    # Rotation in the (3,4) plane (indices 2, 3)\n    if theta34 != 0.0:\n        c34, s34 = np.cos(theta34), np.sin(theta34)\n        R34 = np.array([[c34, -s34], [s34, c34]])\n        R[2:4, 2:4] = R34\n\n    A = R.T @ Lambda @ R\n    return A\n\ndef run_cg_analysis(A, b, x0, theta12_val, max_iter_n):\n    \"\"\"\n    Runs the Conjugate Gradient method and analyzes norm behavior.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Tolerances\n    stop_tol = 1e-12 * np.linalg.norm(b)\n    increase_tol = 1e-14\n    decrease_tol = 1e-14\n\n    # Compute exact solution to measure error\n    x_star = np.linalg.solve(A, b)\n\n    r_norms = []\n    e_norms = []\n\n    # CG Initialization\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    # Store initial norms (k=0)\n    e = x_star - x\n    r_norms.append(np.linalg.norm(r))\n    e_norms.append(np.sqrt(e.T @ A @ e))\n\n    if np.linalg.norm(r)  stop_tol:\n        max_iter_n = 0\n\n    for k in range(max_iter_n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Store norms for step k+1\n        e = x_star - x\n        r_norms.append(np.linalg.norm(r))\n        e_norms.append(np.sqrt(e.T @ A @ e))\n        \n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new)  stop_tol:\n            break\n            \n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n\n    # Analyze norm sequences\n    # 1. Residual norm non-increase check\n    k_inc = -1\n    res_noninc = True\n    for k in range(len(r_norms) - 1):\n        if r_norms[k+1]  r_norms[k] + increase_tol:\n            k_inc = k\n            res_noninc = False\n            break\n\n    # 2. Energy norm strict decrease check\n    energy_strict = True\n    # If the method converges exactly, error norm becomes 0 and stays 0.\n    # This is not a violation of strict decrease.\n    for k in range(len(e_norms) - 1):\n        if e_norms[k]  1e-15: # Avoid issues with floating point zero\n             # Check if e_norms[k+1]  e_norms[k] respecting tolerance\n            if e_norms[k+1]  e_norms[k] - decrease_tol:\n                energy_strict = False\n                break\n    \n    return [res_noninc, energy_strict, k_inc, theta12_val]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    results = []\n    n = 4\n\n    # Test case 1\n    lambdas1 = [1., 20., 200., 5000.]\n    theta12_1, theta34_1 = 0.7, 0.5\n    b1 = np.array([1., 1., 1., 1.])\n    x0_1 = np.zeros(n)\n    A1 = construct_matrix(n, lambdas1, theta12_1, theta34_1)\n    results.append(run_cg_analysis(A1, b1, x0_1, theta12_1, n))\n\n    # Test case 2\n    lambdas2 = [2., 5., 7., 11.]\n    theta12_2, theta34_2 = 0.0, 0.0\n    b2 = np.array([1., 0., 0., 0.])\n    x0_2 = np.zeros(n)\n    A2 = construct_matrix(n, lambdas2, theta12_2, theta34_2)\n    results.append(run_cg_analysis(A2, b2, x0_2, theta12_2, n))\n\n    # Test case 3\n    lambdas3 = [1., 1.5, 2., 2.5]\n    theta12_3, theta34_3 = 0.2, 0.0\n    b3 = np.array([1., -1., 2., -2.])\n    x0_3 = np.zeros(n)\n    A3 = construct_matrix(n, lambdas3, theta12_3, theta34_3)\n    results.append(run_cg_analysis(A3, b3, x0_3, theta12_3, n))\n\n    # Test case 4\n    lambdas4 = [1., 50., 500., 10000.]\n    theta12_vals_4 = [0.1, 0.25, 0.5, 0.75, 1.0, 1.2]\n    theta34_4 = 0.9\n    b4 = np.array([1., 1., 0.1, 0.1])\n    x0_4 = np.zeros(n)\n    \n    found_increase = False\n    last_result = None\n    for theta12 in theta12_vals_4:\n        A4 = construct_matrix(n, lambdas4, theta12, theta34_4)\n        current_result = run_cg_analysis(A4, b4, x0_4, theta12, n)\n        last_result = current_result\n        # The result's k_inc is at index 2\n        if current_result[2] != -1:\n            results.append(current_result)\n            found_increase = True\n            break\n    \n    if not found_increase:\n        results.append(last_result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern high-performance computing often employs mixed-precision strategies to accelerate calculations, but this introduces new challenges for numerical stability. This advanced problem asks you to analyze a mixed-precision CG implementation, where different parts of the algorithm use different levels of floating-point precision . By deriving a stability condition on the lower-precision arithmetic, you will explore the trade-offs between computational speed and the attainable accuracy of the solution, a key concern in cutting-edge scientific computing.",
            "id": "3541554",
            "problem": "Consider a symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigenvalues in the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ and spectral condition number $\\kappa(A) \\equiv \\lambda_{\\max}/\\lambda_{\\min}$. You apply the Conjugate Gradient (CG) method to solve $A x = b$ using mixed precision arithmetic as follows: all vector inner products are computed in a lower precision with unit roundoff $u_{\\ell}$, while all matrix-vector products and saxpy-type updates are computed in a higher precision with unit roundoff $u_{h}$. Assume the following floating-point models are valid throughout:\n- For any length-$n$ dot product, the computed result satisfies $\\operatorname{fl}(x^{\\top} y) = x^{\\top} y \\,(1 + \\theta)$ with $|\\theta| \\leq \\gamma_{n}(u)$, where $\\gamma_{n}(u) \\equiv \\dfrac{n\\,u}{1 - n\\,u}$.\n- For any matrix-vector product, the computed result satisfies $\\|\\operatorname{fl}(A x) - A x\\|_{2} \\leq \\gamma_{n}(u)\\,\\|A\\|_{2}\\,\\|x\\|_{2}$.\n\nAssume that $u_{h}$ is sufficiently small compared to $u_{\\ell}$ so that the high-precision errors do not dominate the low-precision inner-product errors, in the sense that\n$$\n\\kappa(A)\\,\\gamma_{n}(u_{h}) \\leq \\tfrac{1}{2}\\,\\gamma_{n}(u_{\\ell}).\n$$\nSuppose you require that the method exhibits near-ideal convergence until the residual norm reaches a noise floor no larger than a prescribed relative level $\\tau \\in (0,1)$ with respect to the initial residual. Work from first principles of the Conjugate Gradient method, the symmetry and positive definiteness of $A$, and the floating-point models above to derive a sufficient stability condition on $u_{\\ell}$ ensuring such near-ideal convergence up to a floor primarily determined by $u_{\\ell}$. In particular, by carefully propagating the mixed-precision perturbations through the CG recurrences and bounding their effect on the residual recurrence, show that the asymptotically attainable floor (in the Euclidean norm) can be bounded in the form\n$$\n\\text{floor} \\;\\leq\\; C\\,\\kappa(A)\\,\\gamma_{n}(u_{\\ell}),\n$$\nfor a universal absolute constant $C$ independent of $n$, $A$, $u_{\\ell}$, and $u_{h}$, provided $n\\,u_{\\ell}  1$.\n\nAssuming the smallest such valid constant is $C=6$ under the model and regime described, determine the largest lower-precision unit roundoff $u_{\\ell,\\max}$ (as a closed-form analytic expression in $n$, $\\kappa(A)$, and $\\tau$) that guarantees the asymptotic floor is at most $\\tau$. Your final answer must be a single analytic expression for $u_{\\ell,\\max}$. Do not provide any inequalities. No numerical rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed question within the domain of numerical linear algebra, specifically concerning the convergence and stability analysis of the Conjugate Gradient (CG) method under mixed-precision arithmetic. The problem is scientifically grounded, free of contradictions, and provides sufficient information to derive the requested expression.\n\nThe objective is to determine the maximum allowable unit roundoff for low-precision computations, $u_{\\ell, \\max}$, that guarantees the asymptotic error floor of the mixed-precision CG method remains below a specified tolerance $\\tau$.\n\nThe Conjugate Gradient method iteratively solves the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix. The standard recurrences for the iterate $x_k$, residual $r_k$, and search direction $p_k$ are:\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\nThe coefficients $\\alpha_k$ and $\\beta_k$ are computed via inner products:\n$$ \\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}, \\quad \\beta_k = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k} $$\nIn the specified mixed-precision setting, the inner products for $\\alpha_k$ and $\\beta_k$ are computed in a lower precision with unit roundoff $u_{\\ell}$, while the matrix-vector product $A p_k$ and the vector updates (saxpy operations) for $x_{k+1}$, $r_{k+1}$, and $p_{k+1}$ are computed in a higher precision with unit roundoff $u_h$.\n\nEach floating-point operation introduces a small error. The propagation of these errors through the CG iterations determines the method's ultimate attainable accuracy. The problem provides standard models for these errors. The error from inner products is bounded using $\\gamma_{n}(u_{\\ell}) = \\frac{n u_{\\ell}}{1-n u_{\\ell}}$, while errors from high-precision operations are related to $u_h$. The given assumption, $\\kappa(A)\\,\\gamma_{n}(u_{h}) \\leq \\frac{1}{2}\\,\\gamma_{n}(u_{\\ell})$, ensures that the errors introduced by the low-precision inner products are dominant.\n\nA rigorous analysis of the effect of these floating-point errors, working from the first principles of the CG algorithm and the given error models, demonstrates that the ideal convergence of the residual norm eventually stagnates. This occurs when the updates to the residual become comparable in magnitude to the accumulated rounding errors. The minimum attainable residual norm, or \"floor,\" is therefore primarily determined by the size of these errors. The problem states that a detailed derivation leads to a bound on this floor. In the context of CG analysis, this bound is typically for the relative residual norm, i.e., $\\frac{\\|\\hat{r}_{\\text{final}}\\|_2}{\\|b\\|_2}$, where $\\hat{r}_{\\text{final}}$ is the final computed residual. The problem provides this bound as:\n$$ \\frac{\\|\\hat{r}_{\\text{final}}\\|_2}{\\|b\\|_2} \\leq C\\,\\kappa(A)\\,\\gamma_{n}(u_{\\ell}) $$\nwhere $C$ is a constant, and $\\kappa(A)$ is the spectral condition number of $A$.\n\nWe are required to ensure that this floor is no larger than a prescribed relative level $\\tau \\in (0,1)$ with respect to the initial residual, $r_0 = b - A x_0$. To obtain a result independent of the initial guess $x_0$, we adopt the standard convention of starting with a zero initial guess, $x_0 = 0$. This implies $r_0 = b$, and thus $\\|r_0\\|_2 = \\|b\\|_2$. The requirement on the floor then becomes:\n$$ \\frac{\\|\\hat{r}_{\\text{final}}\\|_2}{\\|r_0\\|_2} \\leq \\tau $$\nTo guarantee this condition is met, we require that the upper bound on the floor also satisfies this inequality:\n$$ C\\,\\kappa(A)\\,\\gamma_{n}(u_{\\ell}) \\leq \\tau $$\nThe problem specifies that we should use the constant $C=6$. Substituting this value and the definition of $\\gamma_{n}(u_{\\ell})$ yields the specific condition on $u_{\\ell}$:\n$$ 6\\,\\kappa(A)\\,\\left(\\frac{n\\,u_{\\ell}}{1 - n\\,u_{\\ell}}\\right) \\leq \\tau $$\nWe are looking for the largest value of $u_{\\ell}$, which we will call $u_{\\ell, \\max}$, that satisfies this inequality. The function $f(u) = \\frac{nu}{1-nu}$ is monotonically increasing for $n u  1$. Therefore, the largest value of $u_{\\ell}$ is found by solving the equality:\n$$ 6\\,\\kappa(A)\\,\\left(\\frac{n\\,u_{\\ell, \\max}}{1 - n\\,u_{\\ell, \\max}}\\right) = \\tau $$\nWe proceed to solve this equation for $u_{\\ell, \\max}$. All quantities are positive, so we can rearrange the terms:\n$$ \\frac{n\\,u_{\\ell, \\max}}{1 - n\\,u_{\\ell, \\max}} = \\frac{\\tau}{6\\,\\kappa(A)} $$\nTo simplify, let $X = n\\,u_{\\ell, \\max}$. The equation becomes:\n$$ \\frac{X}{1 - X} = \\frac{\\tau}{6\\,\\kappa(A)} $$\nMultiplying both sides by $1 - X$ gives:\n$$ X = \\left(\\frac{\\tau}{6\\,\\kappa(A)}\\right)(1 - X) $$\n$$ X = \\frac{\\tau}{6\\,\\kappa(A)} - \\frac{\\tau}{6\\,\\kappa(A)}X $$\nNow we gather all terms containing $X$ on the left-hand side:\n$$ X + \\frac{\\tau}{6\\,\\kappa(A)}X = \\frac{\\tau}{6\\,\\kappa(A)} $$\nFactoring out $X$:\n$$ X\\left(1 + \\frac{\\tau}{6\\,\\kappa(A)}\\right) = \\frac{\\tau}{6\\,\\kappa(A)} $$\n$$ X\\left(\\frac{6\\,\\kappa(A) + \\tau}{6\\,\\kappa(A)}\\right) = \\frac{\\tau}{6\\,\\kappa(A)} $$\nSolving for $X$:\n$$ X = \\left(\\frac{\\tau}{6\\,\\kappa(A)}\\right) \\left(\\frac{6\\,\\kappa(A)}{6\\,\\kappa(A) + \\tau}\\right) = \\frac{\\tau}{6\\,\\kappa(A) + \\tau} $$\nRecalling that $X = n\\,u_{\\ell, \\max}$, we have:\n$$ n\\,u_{\\ell, \\max} = \\frac{\\tau}{6\\,\\kappa(A) + \\tau} $$\nFinally, solving for $u_{\\ell, \\max}$ by dividing by $n$:\n$$ u_{\\ell, \\max} = \\frac{\\tau}{n(6\\,\\kappa(A) + \\tau)} $$\nThis expression represents the maximum permissible unit roundoff for the low-precision inner product computations that guarantees the asymptotic residual error floor does not exceed the relative tolerance $\\tau$. The requirement $n\\,u_{\\ell}  1$ is satisfied for this value, as $n\\,u_{\\ell, \\max} = \\frac{\\tau}{6\\kappa(A)+\\tau}  1$ since $\\tau  0$ and $\\kappa(A) \\ge 1$.",
            "answer": "$$\\boxed{\\frac{\\tau}{n(6 \\kappa(A) + \\tau)}}$$"
        }
    ]
}