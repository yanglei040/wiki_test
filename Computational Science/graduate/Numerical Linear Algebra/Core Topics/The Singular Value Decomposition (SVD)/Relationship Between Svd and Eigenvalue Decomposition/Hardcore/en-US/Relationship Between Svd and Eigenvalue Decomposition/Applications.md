## Applications and Interdisciplinary Connections

The preceding chapters have established the profound and fundamental algebraic connection between the Singular Value Decomposition (SVD) of a general matrix $A$ and the Eigenvalue Decomposition (EVD) of the related Hermitian matrices $A^{*}A$ and $AA^{*}$. While algebraically equivalent in many respects, these two decompositions offer distinct perspectives and computational pathways. The SVD is a universally applicable tool that reveals a matrix's geometric action and amplification properties, whereas the EVD is most naturally suited to Hermitian or [normal matrices](@entry_id:195370) and is intrinsically linked to the study of dynamical systems, resonance, and stability.

This chapter explores how this duality is leveraged across a wide array of scientific and engineering disciplines. We will demonstrate that the choice between an SVD-based or EVD-based approach is not merely a matter of preference but is often dictated by crucial considerations of computational efficiency, [numerical stability](@entry_id:146550), and the inherent structure of the application. The following sections will illustrate how these decompositions, and the relationship between them, provide the foundation for powerful algorithms and deep theoretical insights.

### Numerical Computation and Algorithm Design

At the heart of numerical linear algebra lies the challenge of computing matrix decompositions accurately and efficiently. The relationship between SVD and EVD provides several strategies for computing the SVD, each with its own profile of advantages and disadvantages.

#### The Normal Equations Approach and Its Numerical Perils

A direct computational path to the SVD of a matrix $A \in \mathbb{C}^{m \times n}$ is through the EVD of the so-called Gram matrices (or normal equations matrices), $A^{*}A \in \mathbb{C}^{n \times n}$ and $AA^{*} \in \mathbb{C}^{m \times m}$. If the SVD of $A$ is $U\Sigma V^{*}$, then the EVD of $A^{*}A$ is given by:

$A^{*}A = (U\Sigma V^{*})^{*}(U\Sigma V^{*}) = V\Sigma^{*}U^{*}U\Sigma V^{*} = V(\Sigma^{*}\Sigma)V^{*}$

This reveals that the [right singular vectors](@entry_id:754365) of $A$ (the columns of $V$) are the eigenvectors of $A^{*}A$, and the eigenvalues of $A^{*}A$ are the squares of the singular values of $A$. Similarly, the EVD of $AA^{*}$ yields the [left singular vectors](@entry_id:751233) and the same squared singular values. This relationship forms the basis of many SVD algorithms. For instance, a classic method for a small $2 \times 2$ real matrix involves forming the [symmetric matrix](@entry_id:143130) $A^{\top}A$ and diagonalizing it exactly using a single Jacobi rotation to find its [eigenvalues and eigenvectors](@entry_id:138808), from which the full SVD of $A$ can be reconstructed .

For large, rectangular matrices where one dimension is much smaller than the other, this approach offers a significant computational advantage. If $m \ll n$, the matrix $AA^{*}$ is of size $m \times m$, whereas $A^{*}A$ is a much larger $n \times n$ matrix. It is far more efficient to compute the EVD of the smaller matrix $AA^{*}$ to find the singular values and [left singular vectors](@entry_id:751233) $U$, and then recover the [right singular vectors](@entry_id:754365) $V$ via the relation $V = A^{*}U\Sigma^{\dagger}$, where $\Sigma^{\dagger}$ is the [pseudoinverse](@entry_id:140762) of $\Sigma$ .

However, this [computational efficiency](@entry_id:270255) comes at a steep price in [numerical stability](@entry_id:146550). The explicit formation of $A^{*}A$ or $AA^{*}$ can be a source of significant numerical error, a problem epitomized by the squaring of the condition number. The $2$-norm condition number of the EVD problem for $A^{*}A$ is related to the ratio of its largest to smallest non-zero eigenvalues, which is $(\sigma_{1}/\sigma_{r})^{2} = (\kappa_{2}(A))^{2}$. For an [ill-conditioned matrix](@entry_id:147408) $A$ where the ratio of its singular values is large, this squaring can amplify the condition number to such an extent that information about the smaller singular values is completely lost in [floating-point arithmetic](@entry_id:146236). A [singular value](@entry_id:171660) $\sigma_{k}$ on the order of the square root of machine precision, $\sqrt{\varepsilon_{\text{mach}}}$, will become $\sigma_{k}^{2} \approx \varepsilon_{\text{mach}}$ upon squaring, potentially underflowing to zero or being washed out by roundoff errors from larger terms. This loss of information is often catastrophic, and it is the primary reason why modern, high-quality SVD routines, such as those based on Golub-Kahan [bidiagonalization](@entry_id:746789), work directly on $A$ and scrupulously avoid forming the Gram matrices  .

#### The Augmented Matrix Method

A more sophisticated technique, particularly valuable for large-scale problems, circumvents the condition number squaring by embedding the SVD problem within a larger, symmetric EVD problem. This is achieved by forming the augmented [block matrix](@entry_id:148435), sometimes called the Jordan-Wielandt matrix:
$$
H = \begin{pmatrix} 0 & A^{*} \\ A & 0 \end{pmatrix}
$$
This $(m+n) \times (m+n)$ matrix is Hermitian. A straightforward calculation shows that if $(\sigma_i, u_i, v_i)$ is a singular triplet of $A$, then $H$ has eigenvalue-eigenvector pairs $(\sigma_i, \begin{pmatrix} v_i \\ u_i \end{pmatrix})$ and $(-\sigma_i, \begin{pmatrix} v_i \\ -u_i \end{pmatrix})$. Thus, the eigenvalues of $H$ are precisely the singular values of $A$ and their negatives, along with some zero eigenvalues.

The crucial advantage of this approach is that the condition number of the eigenvalue problem for $H$ is related to $\sigma_{1}/\sigma_{r} = \kappa_{2}(A)$, not its square. This method is especially powerful for large, sparse matrices $A$. Iterative eigensolvers, such as the Lanczos algorithm, can find the extreme eigenvalues of $H$ efficiently by relying only on matrix-vector products. A matrix-vector product with $H$ can be implemented using only matrix-vector products with $A$ and $A^{*}$, without ever forming the large matrix $H$ explicitly. This "matrix-free" approach combines the numerical stability of avoiding the normal equations with the efficiency of [iterative methods](@entry_id:139472), making it a cornerstone of SVD computation for sparse problems .

#### Iterative Methods and Spectral Gaps

When using [iterative methods](@entry_id:139472) like the Lanczos algorithm to find extreme eigenvalues, the rate of convergence is highly dependent on the relative separation, or "gap," between the desired eigenvalue and the rest of the spectrum. The choice between applying Lanczos directly to a Hermitian matrix $A$ versus applying it to $A^{*}A$ to find singular values brings this issue to the forefront.

For a Hermitian matrix $A$ with eigenvalues $\lambda_{1} \ge \lambda_{2} \ge \dots$, the relevant gap for finding $\lambda_{1}$ is related to $(\lambda_{1} - \lambda_{2})$. When we form $A^{*}A = A^{2}$, the eigenvalues become $\lambda_{1}^{2}, \lambda_{2}^{2}, \dots$. The new gap is related to $\lambda_{1}^{2} - \lambda_{2}^{2} = (\lambda_{1} - \lambda_{2})(\lambda_{1} + \lambda_{2})$. If the leading eigenvalues are clustered (i.e., $\lambda_{1} \approx \lambda_{2}$), the squaring operation can effectively widen the relative gap, potentially accelerating the convergence of the Lanczos algorithm. However, this potential speedup comes at the cost of losing the sign of the eigenvalues; the EVD of $A^{*}A$ cannot distinguish between an eigenvalue of $+7$ and $-7$, as both become $49$. For applications where the sign is critical, this [information loss](@entry_id:271961) is unacceptable .

### Stability, Conditioning, and Perturbation Theory

The distinction between eigenvalues and singular values is sharpest when analyzing the stability of [non-normal matrices](@entry_id:137153)—matrices that do not commute with their conjugate transpose ($AA^{*} \neq A^{*}A$).

For [normal matrices](@entry_id:195370), the magnitudes of the eigenvalues are equal to the singular values ($|\lambda_i| = \sigma_i$). For [non-normal matrices](@entry_id:137153), this correspondence breaks down, and the two sets of values can paint dramatically different pictures of a matrix's behavior. Eigenvalues govern the long-term asymptotic behavior of a system (e.g., whether $A^k x$ grows or decays), defined by the spectral radius $\rho(A) = \max_i |\lambda_i|$. In contrast, singular values govern the amplification properties or norm of a matrix, with $\|A\|_2 = \sigma_1$. For a highly [non-normal matrix](@entry_id:175080), it is possible to have a [spectral radius](@entry_id:138984) less than one, indicating long-term stability, while having a very large operator norm, indicating the potential for massive transient amplification. A canonical example demonstrates that the ratio $\sigma_1(A)/\rho(A)$ can be made arbitrarily large, highlighting this crucial divergence in what eigenvalues and singular values measure .

This dichotomy is formalized by perturbation theory. The celebrated Weyl-Mirsky theorem states that for any matrix $A$ and perturbation $E$, the singular values are perfectly conditioned: $|\sigma_i(A+E) - \sigma_i(A)| \le \|E\|_2$. In stark contrast, the Bauer-Fike theorem for a [diagonalizable matrix](@entry_id:150100) $A=V\Lambda V^{-1}$ bounds the perturbation of eigenvalues by the condition number of the eigenvector matrix, $\kappa(V) = \|V\|_2 \|V^{-1}\|_2$. For a highly [non-normal matrix](@entry_id:175080), $\kappa(V)$ can be enormous, meaning its eigenvalues can be exquisitely sensitive to small perturbations. The singular values, by comparison, remain robust. This stability makes SVD a more reliable tool than EVD for many computational tasks involving [non-normal matrices](@entry_id:137153). The first-order derivatives of eigenvalues and singular values under a rank-one perturbation provide a concrete demonstration of this principle: the sensitivity of an eigenvalue depends on the alignment of the perturbation with its [left and right eigenvectors](@entry_id:173562), while the sensitivity of a [singular value](@entry_id:171660) depends on alignment with its left and [right singular vectors](@entry_id:754365). For a [non-normal matrix](@entry_id:175080), these vector sets are different, allowing for scenarios where singular values are stable while eigenvalues are highly sensitive .

An important application of this concept is in the numerical analysis of polynomial [root-finding](@entry_id:166610). The roots of a [monic polynomial](@entry_id:152311) are precisely the eigenvalues of its associated [companion matrix](@entry_id:148203). While the eigenvalues give the location of the roots, the singular values of the [companion matrix](@entry_id:148203) reveal the conditioning of the [root-finding problem](@entry_id:174994). A small minimum singular value, $\sigma_{\min}(C)$, indicates that the matrix is close to being singular, which in turn implies that the roots (eigenvalues) are highly sensitive to small perturbations in the polynomial's coefficients. Thus, while the EVD of the companion matrix solves the problem, the SVD provides a crucial diagnostic tool for the stability of that solution .

### Applications in Statistics and Data Science

The SVD and its relationship with the EVD of covariance matrices form the bedrock of [multivariate statistics](@entry_id:172773) and modern data science.

#### Principal Component Analysis (PCA)

PCA is a cornerstone of dimensionality reduction, aiming to find the directions of maximal variance in a dataset. For a mean-centered data matrix $X \in \mathbb{R}^{m \times n}$, these directions are given by the eigenvectors of the [sample covariance matrix](@entry_id:163959), $C \propto X^{\top}X$. This is a direct application of the EVD. However, as discussed previously, computing the covariance matrix explicitly squares the condition number of the data matrix, a numerically unstable procedure. A far more robust approach is to compute the SVD of the data matrix $X$ directly. The [right singular vectors](@entry_id:754365) of $X$ are identical to the eigenvectors of $X^{\top}X$, providing the [principal directions](@entry_id:276187) without the numerical risk of forming the covariance matrix .

#### Canonical Correlation Analysis (CCA)

CCA seeks to find the relationships between two sets of variables, represented by random vectors $X$ and $Y$. It identifies pairs of projection vectors, $a$ and $b$, such that the correlation between the projected variables $a^{\top}X$ and $b^{\top}Y$ is maximized. This problem can be elegantly solved using the SVD. When the datasets have been "whitened" (i.e., transformed such that their covariance matrices are identity matrices), the problem of finding the canonical correlations and variates simplifies remarkably. The canonical correlations become precisely the singular values of the cross-covariance matrix $C_{XY} = E[XY^{\top}]$. The corresponding left and [right singular vectors](@entry_id:754365) provide the optimal projection vectors. This provides a beautiful and direct bridge between a fundamental statistical task and the SVD .

#### The Procrustes Problem

In many fields, from [computer vision](@entry_id:138301) to bioinformatics, a common task is to find the optimal [rigid transformation](@entry_id:270247) (rotation and reflection) that aligns one set of points with another. This is known as the Procrustes problem. If two sets of points are represented by matrices $A$ and $B$, the problem is to find a unitary matrix $Q$ that minimizes the distance $\|AQ - B\|_{F}$. The solution is elegantly given by the SVD of the cross-correlation matrix $A^{*}B$. If $A^{*}B = U\Sigma V^{*}$, the optimal rotation is $Q^{\star} = UV^{*}$. This result arises because the problem is equivalent to maximizing $\operatorname{Re}(\operatorname{trace}(Q^{*}A^{*}B))$. While a naive approach might try to align the eigenvectors from the individual EVDs of $A$ and $B$, this fails because EVD does not uniquely determine the phase or sign of eigenvectors. The SVD of the cross-term correctly captures the relative orientation information required for optimal alignment .

### Engineering, Physics, and Network Science

The SVD/EVD relationship is a recurring theme in the [mathematical modeling](@entry_id:262517) of physical and networked systems.

#### Control Theory: Balanced Truncation

In control theory, creating simplified, low-order models of complex high-order systems is a critical task. Balanced truncation is a powerful [model reduction](@entry_id:171175) technique that systematically identifies and discards the least important states of a system. The importance of each state is quantified by the Hankel singular values, $\{\sigma_{H,i}\}$. These values are defined as the square roots of the eigenvalues of the product of the controllability Gramian ($W_c$) and the [observability](@entry_id:152062) Gramian ($W_o$). The Gramians themselves are found by solving Lyapunov equations, which are EVD-related problems. Thus, model reduction hinges on the EVD of the matrix product $W_c W_o$, a direct and impactful application of the interplay between the two decompositions .

#### Continuum Mechanics: Stress Analysis

In [solid mechanics](@entry_id:164042), the state of stress at a point within a material is described by the symmetric Cauchy stress tensor, $\sigma$. The principal stresses—the maximum and minimum [normal stresses](@entry_id:260622) at that point—are the eigenvalues of this tensor. The [principal directions](@entry_id:276187)—the orientations of the surfaces on which these stresses act—are the corresponding eigenvectors. Since the stress tensor is symmetric, its SVD is closely related to its EVD. The singular values are the [absolute values](@entry_id:197463) of the eigenvalues (principal stresses). The sign of each principal stress (tensile or compressive) can be recovered from the SVD by examining the dot product of the corresponding left and [right singular vectors](@entry_id:754365). This allows a unified SVD framework to robustly handle the complete stress state .

#### Spectral Graph Theory

The properties of graphs and networks are deeply connected to the spectra of associated matrices. The graph Laplacian, $L$, is a central object of study. For an [undirected graph](@entry_id:263035) with an [oriented incidence matrix](@entry_id:274962) $B$ (where each row represents an edge), the Laplacian can be constructed as $L = B^{\top}B$. This immediately reveals that the EVD of the Laplacian is linked to the SVD of the [incidence matrix](@entry_id:263683). The eigenvalues of $L$, which encode critical information about the graph's connectivity, partitioning, and structure, are the squares of the singular values of $B$. This relationship allows one to design graphs with specific spectral properties by manipulating the [singular vectors](@entry_id:143538) of the [incidence matrix](@entry_id:263683), subject to the structural constraints of the graph .

#### Signal Processing: Whitening Transformations

In many signal processing and machine learning applications, it is desirable to transform data so that it has an identity covariance matrix—a process known as whitening or sphering. For a data matrix $A$, this can be achieved with a preconditioner $P = (A^{*}A)^{-1/2}$. This specific [preconditioner](@entry_id:137537), the inverse square root of a Gram matrix, is constructed from the EVD of $A^{*}A$. The transformed matrix $AP$ has the remarkable property that its columns are orthonormal, meaning $(AP)^{*}(AP) = I$. Consequently, all of its singular values are equal to one. The EVD of $A^{*}A$ is thus used as a tool to construct a new matrix whose SVD has a perfectly [uniform structure](@entry_id:150536) .

### An Extension to Infinite Dimensions: Compact Operators

The deep connection between SVD and EVD is not confined to the finite-dimensional world of matrices. It extends elegantly to the study of compact operators on infinite-dimensional Hilbert spaces, a cornerstone of [functional analysis](@entry_id:146220). For a general compact operator $T$, which may not be normal or self-adjoint, its singular values are defined as the square roots of the eigenvalues of the positive, self-adjoint, compact operator $T^{*}T$. The spectral theorem for [compact self-adjoint operators](@entry_id:147701) guarantees that $T^{*}T$ has a discrete set of non-negative eigenvalues converging to zero. This allows for a singular value expansion $T = \sum_k \sigma_k \langle \cdot, v_k \rangle u_k$, which is the infinite-dimensional analogue of the SVD.

The Volterra integral operator, $(Tf)(x) = \int_0^x f(t)\,dt$ on $L^2([0,1])$, provides a classic example. This operator is compact but not normal. Its singular values are found by solving the EVD for the self-adjoint operator $T^{*}T$, which can be transformed into a solvable [boundary value problem](@entry_id:138753). This yields an explicit sequence of singular values, $\sigma_k = \frac{2}{(2k-1)\pi}$, whose largest member $\sigma_1 = 2/\pi$ gives the [operator norm](@entry_id:146227) of $T$ . This example beautifully illustrates that the strategy of analyzing a general operator through the EVD of its associated self-adjoint Gram operator is a fundamental principle that transcends dimensionality.