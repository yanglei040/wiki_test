{
    "hands_on_practices": [
        {
            "introduction": "While the eigenvalue decomposition (EVD) and singular value decomposition (SVD) are closely related for normal matrices, their properties can diverge dramatically for non-normal matrices. This exercise explores an extreme but illuminating example of this divergence through the concept of a partial isometry—a map that acts as a norm-preserving isometry on a specific subspace. By constructing a matrix that is both nilpotent (having all eigenvalues equal to zero) and a partial isometry (having singular values of one), you will directly confront how the SVD reveals geometric properties that the EVD can completely obscure . This practice is designed to build a deeper intuition for what each decomposition truly measures: the EVD captures the long-term behavior of matrix powers, while the SVD describes the immediate geometric action of the matrix on vectors.",
            "id": "3573919",
            "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ satisfy that $A^{\\ast} A$ is idempotent, i.e., $(A^{\\ast} A)^{2} = A^{\\ast} A$. Use only foundational facts about the Singular Value Decomposition (SVD) and eigenvalue decomposition (EVD) to reason about the consequences for the singular values and eigenvalues of $A$.\n\n(a) Starting from the definition of the Singular Value Decomposition (SVD), $A = U \\Sigma V^{\\ast}$ with $U$ and $V$ unitary and $\\Sigma$ diagonal with nonnegative entries, and the eigenvalue decomposition (EVD) of the Hermitian positive semidefinite matrix $A^{\\ast} A$, derive what values the singular values of $A$ can take if $A^{\\ast} A$ is idempotent.\n\n(b) Construct an explicit nonzero $A \\in \\mathbb{C}^{4 \\times 4}$ such that $A^{\\ast} A$ is an orthogonal projector of rank $2$, yet $A$ is nilpotent of index $2$ (so all eigenvalues of $A$ are zero), and $A$ acts as a norm-preserving isometry on a two-dimensional subspace. Verify directly that $A^{2} = 0$, that $A^{\\ast} A$ is a projector, and determine the singular values of $A$ from first principles.\n\n(c) For your constructed matrix $A$ from part (b), define the Hermitian matrix $D := A^{\\ast} A - A A^{\\ast}$. Compute the characteristic polynomial of $D$ as a closed-form analytic expression in the variable $\\lambda$. Provide this characteristic polynomial as your final answer.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. We proceed with a detailed solution.\n\n**(a) Derivation of singular values**\n\nLet $A \\in \\mathbb{C}^{n \\times n}$ be a matrix. The Singular Value Decomposition (SVD) of $A$ is given by $A = U \\Sigma V^{\\ast}$, where $U, V \\in \\mathbb{C}^{n \\times n}$ are unitary matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative real entries, $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)$, where $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$ are the singular values of $A$.\n\nWe consider the matrix $A^{\\ast} A$. Substituting the SVD of $A$:\n$$A^{\\ast} A = (U \\Sigma V^{\\ast})^{\\ast} (U \\Sigma V^{\\ast}) = (V \\Sigma^{\\ast} U^{\\ast})(U \\Sigma V^{\\ast})$$\nSince $\\Sigma$ is a real diagonal matrix, $\\Sigma^{\\ast} = \\Sigma$. As $U$ is unitary, $U^{\\ast}U = I$, where $I$ is the identity matrix. Therefore,\n$$A^{\\ast} A = V \\Sigma (U^{\\ast}U) \\Sigma V^{\\ast} = V \\Sigma^2 V^{\\ast}$$\nThis expression, $A^{\\ast} A = V \\Sigma^2 V^{\\ast}$, is the eigenvalue decomposition (EVD) of the matrix $A^{\\ast} A$. The matrix $A^{\\ast} A$ is Hermitian since $(A^{\\ast} A)^{\\ast} = A^{\\ast} (A^{\\ast})^{\\ast} = A^{\\ast} A$. Its eigenvalues are the diagonal entries of $\\Sigma^2$, which are $\\sigma_i^2$, and the corresponding eigenvectors are the columns of $V$. Since $\\sigma_i \\geq 0$, the eigenvalues $\\sigma_i^2$ are non-negative, so $A^{\\ast} A$ is a positive semidefinite matrix.\n\nThe problem states that $A^{\\ast} A$ is idempotent, which means $(A^{\\ast} A)^2 = A^{\\ast} A$.\nLet $\\lambda$ be an eigenvalue of an idempotent matrix $P$, with corresponding eigenvector $x \\neq 0$, such that $Px = \\lambda x$. Then $P^2 x = P(\\lambda x) = \\lambda(Px) = \\lambda(\\lambda x) = \\lambda^2 x$. Since $P^2=P$, we have $Px = \\lambda^2 x$.\nThus, $\\lambda x = \\lambda^2 x$, which implies $(\\lambda^2 - \\lambda)x = 0$. Since $x \\neq 0$, we must have $\\lambda^2 - \\lambda = 0$, which yields $\\lambda(\\lambda - 1) = 0$. The possible eigenvalues of any idempotent matrix are therefore $0$ and $1$.\n\nThe eigenvalues of $A^{\\ast} A$ are $\\lambda_i = \\sigma_i^2$. Applying the property of idempotent matrices, we must have $\\sigma_i^2 \\in \\{0, 1\\}$.\nSince the singular values $\\sigma_i$ are defined to be non-negative, we can find them by taking the square root:\n$$\\sigma_i = \\sqrt{0} = 0 \\quad \\text{or} \\quad \\sigma_i = \\sqrt{1} = 1$$\nTherefore, the singular values of any matrix $A$ for which $A^{\\ast} A$ is idempotent can only be $0$ or $1$.\n\n**(b) Construction and verification of matrix A**\n\nWe need to construct a nonzero matrix $A \\in \\mathbb{C}^{4 \\times 4}$ satisfying three conditions. Let us choose the following block matrix:\n$$A = \\begin{pmatrix} 0_{2}  I_2 \\\\ 0_{2}  0_{2} \\end{pmatrix} = \\begin{pmatrix} 0  0  1  0 \\\\ 0  0  0  1 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}$$\nwhere $0_{2}$ is the $2 \\times 2$ zero matrix and $I_2$ is the $2 \\times 2$ identity matrix. This is a nonzero matrix.\n\n1.  **Nilpotency**: We check if $A$ is nilpotent of index $2$, which requires $A \\neq 0$ and $A^2 = 0$.\n    $$A^2 = \\begin{pmatrix} 0_{2}  I_2 \\\\ 0_{2}  0_{2} \\end{pmatrix} \\begin{pmatrix} 0_{2}  I_2 \\\\ 0_{2}  0_{2} \\end{pmatrix} = \\begin{pmatrix} 0_2 \\cdot 0_2 + I_2 \\cdot 0_2  0_2 \\cdot I_2 + I_2 \\cdot 0_2 \\\\ 0_2 \\cdot 0_2 + 0_2 \\cdot 0_2  0_2 \\cdot I_2 + 0_2 \\cdot 0_2 \\end{pmatrix} = \\begin{pmatrix} 0_{2}  0_{2} \\\\ 0_{2}  0_{2} \\end{pmatrix} = 0_4$$\n    Since $A \\neq 0$ and $A^2=0$, $A$ is nilpotent of index $2$. A consequence is that all its eigenvalues are $0$.\n\n2.  **$A^{\\ast} A$ as an orthogonal projector**: An orthogonal projector $P$ must be Hermitian ($P^{\\ast}=P$) and idempotent ($P^2=P$). We calculate $A^{\\ast}A$.\n    The conjugate transpose of A is $A^{\\ast} = \\begin{pmatrix} 0_{2}  I_2 \\\\ 0_{2}  0_{2} \\end{pmatrix}^{\\ast} = \\begin{pmatrix} 0_{2}^{\\ast}  0_{2}^{\\ast} \\\\ I_{2}^{\\ast}  0_{2}^{\\ast} \\end{pmatrix} = \\begin{pmatrix} 0_{2}  0_{2} \\\\ I_{2}  0_{2} \\end{pmatrix}$.\n    $$A^{\\ast} A = \\begin{pmatrix} 0_{2}  0_{2} \\\\ I_{2}  0_{2} \\end{pmatrix} \\begin{pmatrix} 0_{2}  I_2 \\\\ 0_{2}  0_{2} \\end{pmatrix} = \\begin{pmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix}$$\n    Let's check the properties of $P = A^{\\ast} A$:\n    -   **Hermitian**: $P^{\\ast} = \\left(\\begin{smallmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2 \\end{smallmatrix}\\right)^{\\ast} = \\begin{pmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2 \\end{pmatrix} = P$. It is Hermitian.\n    -   **Idempotent**: $P^2 = \\left(\\begin{smallmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2 \\end{smallmatrix}\\right)^2 = \\begin{pmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2^2 \\end{pmatrix} = \\begin{pmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2 \\end{pmatrix} = P$. It is idempotent.\n    -   **Rank**: The rank of a projector is its trace. $\\text{rank}(P) = \\text{tr}(P) = 0+0+1+1 = 2$.\n    Thus, $A^{\\ast} A$ is an orthogonal projector of rank $2$.\n\n3.  **Norm-preserving isometry**: We must show that $A$ acts as a norm-preserving isometry on a two-dimensional subspace. A linear map acts as an isometry on a subspace $S$ if for all $x \\in S$, $\\|Ax\\| = \\|x\\|$.\n    The subspace where this holds is the span of the right singular vectors corresponding to singular values equal to $1$. From part (a), we know the singular values are $0$ or $1$. The number of singular values equal to $1$ is the rank of $A$, which is $\\text{rank}(A) = \\text{rank}(A^{\\ast}A) = 2$.\n    The singular values are the square roots of the eigenvalues of $A^{\\ast}A = \\text{diag}(0,0,1,1)$. The eigenvalues are $1, 1, 0, 0$.\n    The singular values are thus $\\sigma_1=1, \\sigma_2=1, \\sigma_3=0, \\sigma_4=0$.\n    The required subspace $S$ is the eigenspace of $A^{\\ast}A$ corresponding to the eigenvalue $1$. Let $x = (x_1, x_2, x_3, x_4)^T$.\n    $A^{\\ast}Ax = x \\implies \\begin{pmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} \\implies \\begin{pmatrix} 0 \\\\ 0 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}$.\n    This implies $x_1=0$ and $x_2=0$. The subspace is $S = \\{ (0,0,x_3,x_4)^T \\in \\mathbb{C}^4 \\} = \\text{span}\\{e_3, e_4\\}$. This is a two-dimensional subspace.\n    For any vector $x \\in S$, $x = c_3 e_3 + c_4 e_4$ for some $c_3, c_4 \\in \\mathbb{C}$.\n    $\\|x\\|^2 = \\|c_3 e_3 + c_4 e_4\\|^2 = |c_3|^2 + |c_4|^2$.\n    Now we compute $Ax$:\n    $$Ax = \\begin{pmatrix} 0  0  1  0 \\\\ 0  0  0  1 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ c_3 \\\\ c_4 \\end{pmatrix} = \\begin{pmatrix} c_3 \\\\ c_4 \\\\ 0 \\\\ 0 \\end{pmatrix} = c_3 e_1 + c_4 e_2$$\n    $\\|Ax\\|^2 = \\|c_3 e_1 + c_4 e_2\\|^2 = |c_3|^2 + |c_4|^2$.\n    Since $\\|Ax\\|^2 = \\|x\\|^2$, we have $\\|Ax\\| = \\|x\\|$ for all $x \\in S$. The property is verified.\n    \n    The singular values of $A$ are the non-negative square roots of the eigenvalues of $A^{\\ast}A$. Since the eigenvalues of $A^{\\ast}A=\\text{diag}(0,0,1,1)$ are $0, 0, 1, 1$, the singular values are $\\sqrt{1}, \\sqrt{1}, \\sqrt{0}, \\sqrt{0}$, which are $1, 1, 0, 0$.\n\n**(c) Characteristic polynomial of D**\n\nWe define the Hermitian matrix $D := A^{\\ast} A - A A^{\\ast}$. Using the matrix $A$ from part (b), we have already computed $A^{\\ast} A = \\begin{pmatrix} 0_2  0_2 \\\\ 0_2  I_2 \\end{pmatrix}$. Now we compute $A A^{\\ast}$:\n$$A A^{\\ast} = \\begin{pmatrix} 0_{2}  I_2 \\\\ 0_{2}  0_{2} \\end{pmatrix} \\begin{pmatrix} 0_{2}  0_{2} \\\\ I_{2}  0_{2} \\end{pmatrix} = \\begin{pmatrix} I_2  0_{2} \\\\ 0_{2}  0_2 \\end{pmatrix} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}$$\nNow we compute $D$:\n$$D = A^{\\ast} A - A A^{\\ast} = \\begin{pmatrix} 0_{2}  0_{2} \\\\ 0_{2}  I_2 \\end{pmatrix} - \\begin{pmatrix} I_2  0_{2} \\\\ 0_{2}  0_2 \\end{pmatrix} = \\begin{pmatrix} -I_2  0_{2} \\\\ 0_{2}  I_2 \\end{pmatrix}$$\nExplicitly, $D = \\text{diag}(-1, -1, 1, 1)$.\nThe characteristic polynomial of $D$ is $p(\\lambda) = \\det(D - \\lambda I)$.\n$$D - \\lambda I = \\begin{pmatrix} -1-\\lambda  0  0  0 \\\\ 0  -1-\\lambda  0  0 \\\\ 0  0  1-\\lambda  0 \\\\ 0  0  0  1-\\lambda \\end{pmatrix}$$\nThe determinant of this diagonal matrix is the product of its diagonal entries:\n$$p(\\lambda) = (-1-\\lambda)(-1-\\lambda)(1-\\lambda)(1-\\lambda)$$\n$$p(\\lambda) = (-(1+\\lambda))^2 (1-\\lambda)^2 = (1+\\lambda)^2 (1-\\lambda)^2$$\n$$p(\\lambda) = ((1+\\lambda)(1-\\lambda))^2 = (1-\\lambda^2)^2$$\nExpanding this expression gives:\n$$p(\\lambda) = 1 - 2\\lambda^2 + \\lambda^4$$\nThis is the characteristic polynomial of $D$.",
            "answer": "$$\\boxed{\\lambda^4 - 2\\lambda^2 + 1}$$"
        },
        {
            "introduction": "Building on the fundamental distinctions between eigenvalues and singular values, this practice examines a more subtle but numerically critical scenario. The convergence rates of iterative algorithms and the conditioning of linear systems are often analyzed through spectral gaps, but the EVD and SVD can provide conflicting information about these gaps. This exercise challenges you to analyze a family of non-normal matrices where the eigenvalues are well-separated, suggesting the problem is amenable to certain methods, while the singular values are clustered, suggesting the matrix is nearly unitary and well-conditioned . Mastering this problem is crucial for appreciating the hazards of non-normality and understanding why SVD-based metrics, like the standard condition number, can be profoundly misleading for analyzing the behavior of iterative solvers.",
            "id": "3573887",
            "problem": "Let $A \\in \\mathbb{C}^{2 \\times 2}$ and consider the following parametric family of matrices\n$$\nA(L,\\alpha) \\;=\\; \\begin{pmatrix}\nL  \\alpha L \\\\\n0  -L\n\\end{pmatrix},\n$$\nwhere $L  0$ and $\\alpha  0$ are real parameters. The matrix $A(L,\\alpha)$ is diagonalizable over $\\mathbb{C}$ because its eigenvalues are distinct. Let $\\lambda_1$ and $\\lambda_2$ denote the eigenvalues of $A(L,\\alpha)$, and let $\\sigma_1 \\geq \\sigma_2 \\geq 0$ denote the singular values obtained from the Singular Value Decomposition (SVD), that is, $A = U \\Sigma V^{\\ast}$ with $U,V$ unitary and $\\Sigma = \\operatorname{diag}(\\sigma_1,\\sigma_2)$. The squared singular values are the eigenvalues of $A^{\\ast}A$, where $A^{\\ast}$ denotes the conjugate transpose (Hermitian adjoint) of $A$.\n\nStarting from the definitions of eigenvalue decomposition and Singular Value Decomposition (SVD), derive the eigenvalues $\\lambda_1,\\lambda_2$ of $A(L,\\alpha)$ and the squared singular values $\\sigma_1^2,\\sigma_2^2$ of $A(L,\\alpha)$ by computing the eigenvalues of $A^{\\ast}A$. Then define the following dimensionless, $L$-normalized spectral-separation ratio\n$$\nR(L,\\alpha) \\;=\\; \\frac{\\left|\\sigma_1^2 - \\sigma_2^2\\right|/L^2}{\\left|\\lambda_1 - \\lambda_2\\right|/L}.\n$$\nThis ratio compares the $L$-normalized separation in the spectrum of $A^{\\ast}A$ (which underlies SVD-based constructions) to the $L$-normalized separation in the spectrum of $A$ itself (which underlies eigenvalue-based constructions).\n\nCompute $R(L,\\alpha)$ exactly as a closed-form analytical expression in terms of $\\alpha$ only. No rounding is required. Based on your derivation, briefly explain, using first principles, how small values of $\\alpha$ can lead to $A$ being diagonalizable with well-separated eigenvalues $\\lambda_i$, while $A^{\\ast}A$ has nearly repeated eigenvalues $\\sigma_i^2$, and articulate the implication of this mismatch for preconditioners that rely on SVD-derived spectral information and may therefore miss eigen-spectral separation. Your final answer must be the exact expression for $R(L,\\alpha)$ in terms of $\\alpha$ alone.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All necessary parameters and definitions are provided, and the objectives are clear. The problem is a standard exercise in numerical linear algebra, exploring the relationship between eigenvalue decomposition and singular value decomposition for a non-normal matrix.\n\nFirst, we derive the eigenvalues of the matrix $A(L,\\alpha)$. The matrix is given by\n$$\nA \\;=\\; \\begin{pmatrix}\nL  \\alpha L \\\\\n0  -L\n\\end{pmatrix}\n$$\nSince $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries. Therefore, the eigenvalues are\n$$\n\\lambda_1 = L \\quad \\text{and} \\quad \\lambda_2 = -L\n$$\nThe problem states that $L  0$, so the eigenvalues are distinct. We compute the $L$-normalized separation of the eigenvalues, which appears in the denominator of the ratio $R(L,\\alpha)$:\n$$\n\\frac{|\\lambda_1 - \\lambda_2|}{L} = \\frac{|L - (-L)|}{L} = \\frac{|2L|}{L} = \\frac{2L}{L} = 2\n$$\nThis separation is constant and independent of $\\alpha$.\n\nNext, we derive the squared singular values, $\\sigma_1^2$ and $\\sigma_2^2$. By definition, these are the eigenvalues of the matrix $A^{\\ast}A$, where $A^{\\ast}$ is the conjugate transpose of $A$. Since the entries of $A$ are real, $A^{\\ast}$ is simply the transpose $A^T$.\n$$\nA^{\\ast} = A^T = \\begin{pmatrix}\nL  0 \\\\\n\\alpha L  -L\n\\end{pmatrix}\n$$\nNow, we compute the product $A^{\\ast}A$:\n$$\nA^{\\ast}A = \\begin{pmatrix}\nL  0 \\\\\n\\alpha L  -L\n\\end{pmatrix}\n\\begin{pmatrix}\nL  \\alpha L \\\\\n0  -L\n\\end{pmatrix} = \\begin{pmatrix}\nL^2  \\alpha L^2 \\\\\n\\alpha L^2  (\\alpha L)^2 + (-L)^2\n\\end{pmatrix} = \\begin{pmatrix}\nL^2  \\alpha L^2 \\\\\n\\alpha L^2  (\\alpha^2+1)L^2\n\\end{pmatrix}\n$$\nTo find the eigenvalues of $A^{\\ast}A$, we solve the characteristic equation $\\det(A^{\\ast}A - \\mu I) = 0$, where $\\mu = \\sigma^2$.\n$$\n\\det \\begin{pmatrix}\nL^2 - \\mu  \\alpha L^2 \\\\\n\\alpha L^2  (\\alpha^2+1)L^2 - \\mu\n\\end{pmatrix} = 0\n$$\n$$\n(L^2 - \\mu)((\\alpha^2+1)L^2 - \\mu) - (\\alpha L^2)^2 = 0\n$$\nThis is a quadratic equation for $\\mu$. To simplify, we can factor out $L^2$ from the matrix $A^{\\ast}A = L^2 \\begin{pmatrix} 1  \\alpha \\\\ \\alpha  \\alpha^2+1 \\end{pmatrix}$. Let the eigenvalues of the matrix $\\begin{pmatrix} 1  \\alpha \\\\ \\alpha  \\alpha^2+1 \\end{pmatrix}$ be $\\mu'$. Then $\\mu = L^2 \\mu'$. The characteristic equation for $\\mu'$ is:\n$$\n(1 - \\mu')(\\alpha^2+1 - \\mu') - \\alpha^2 = 0\n$$\n$$\n\\mu'^2 - (\\alpha^2+1+1)\\mu' + (\\alpha^2+1) - \\alpha^2 = 0\n$$\n$$\n\\mu'^2 - (\\alpha^2+2)\\mu' + 1 = 0\n$$\nUsing the quadratic formula to solve for $\\mu'$:\n$$\n\\mu' = \\frac{(\\alpha^2+2) \\pm \\sqrt{(\\alpha^2+2)^2 - 4(1)(1)}}{2} = \\frac{(\\alpha^2+2) \\pm \\sqrt{\\alpha^4 + 4\\alpha^2 + 4 - 4}}{2}\n$$\n$$\n\\mu' = \\frac{(\\alpha^2+2) \\pm \\sqrt{\\alpha^4 + 4\\alpha^2}}{2} = \\frac{(\\alpha^2+2) \\pm \\sqrt{\\alpha^2(\\alpha^2+4)}}{2}\n$$\nSince $\\alpha  0$, $\\sqrt{\\alpha^2} = \\alpha$.\n$$\n\\mu' = \\frac{\\alpha^2+2 \\pm \\alpha\\sqrt{\\alpha^2+4}}{2}\n$$\nThe squared singular values are $\\sigma_i^2 = L^2 \\mu'$. By convention, $\\sigma_1 \\geq \\sigma_2$, so $\\sigma_1^2 \\geq \\sigma_2^2$.\n$$\n\\sigma_1^2 = L^2 \\left(\\frac{\\alpha^2+2 + \\alpha\\sqrt{\\alpha^2+4}}{2}\\right)\n$$\n$$\n\\sigma_2^2 = L^2 \\left(\\frac{\\alpha^2+2 - \\alpha\\sqrt{\\alpha^2+4}}{2}\\right)\n$$\nNow we compute the quantity for the numerator of $R(L,\\alpha)$:\n$$\n\\sigma_1^2 - \\sigma_2^2 = L^2 \\left[ \\left(\\frac{\\alpha^2+2 + \\alpha\\sqrt{\\alpha^2+4}}{2}\\right) - \\left(\\frac{\\alpha^2+2 - \\alpha\\sqrt{\\alpha^2+4}}{2}\\right) \\right]\n$$\n$$\n\\sigma_1^2 - \\sigma_2^2 = L^2 \\left( \\frac{2\\alpha\\sqrt{\\alpha^2+4}}{2} \\right) = L^2 \\alpha\\sqrt{\\alpha^2+4}\n$$\nSince $L  0$ and $\\alpha  0$, this difference is positive, so $|\\sigma_1^2 - \\sigma_2^2| = L^2 \\alpha\\sqrt{\\alpha^2+4}$. The $L$-normalized separation is:\n$$\n\\frac{|\\sigma_1^2 - \\sigma_2^2|}{L^2} = \\alpha\\sqrt{\\alpha^2+4}\n$$\nFinally, we compute the ratio $R(L,\\alpha)$:\n$$\nR(L,\\alpha) = \\frac{\\left|\\sigma_1^2 - \\sigma_2^2\\right|/L^2}{\\left|\\lambda_1 - \\lambda_2\\right|/L} = \\frac{\\alpha\\sqrt{\\alpha^2+4}}{2}\n$$\nThis expression depends only on $\\alpha$, as required.\n\nFor the explanation:\nThe matrix $A$ is diagonalizable and has eigenvalues $\\lambda_1 = L$ and $\\lambda_2 = -L$. Their separation $|\\lambda_1 - \\lambda_2| = 2L$ is constant and substantial, as $L  0$. The matrix is non-normal for any $\\alpha  0$.\nThe squared singular values are $\\sigma_1^2$ and $\\sigma_2^2$. Their separation is $|\\sigma_1^2 - \\sigma_2^2| = L^2\\alpha\\sqrt{\\alpha^2+4}$. As $\\alpha \\to 0^+$, this separation approaches $0$. Specifically, $\\sigma_1^2 \\to L^2$ and $\\sigma_2^2 \\to L^2$.\nThe ratio $R(L,\\alpha) \\approx \\frac{\\alpha\\sqrt{4}}{2} = \\alpha$ for small $\\alpha$. So as $\\alpha \\to 0^+$, $R(L,\\alpha) \\to 0$. This quantifies the mismatch: the eigenvalue spectrum of $A$ is widely separated, whereas the eigenvalue spectrum of $A^{\\ast}A$ (i.e., the squared singular values) is tightly clustered.\n\nThe implication of this mismatch is significant for numerical methods. Preconditioners for iterative solvers are often designed based on spectral information. SVD-based analyses and preconditioners utilize the singular values. For small $\\alpha$, the singular values are clustered ($\\sigma_1 \\approx \\sigma_2 \\approx L$), and the condition number $\\kappa_2(A) = \\sigma_1/\\sigma_2 \\approx 1$. From an SVD perspective, the matrix $A$ appears well-conditioned and benign, behaving much like a simple scaling of an orthogonal matrix. A preconditioner built on this observation might be simple, for example, $P = L \\cdot I$, and would be expected to work well.\n\nHowever, the convergence of iterative solvers for non-normal matrices is not solely dictated by the condition number. The well-separated eigenvalues ($L$, $-L$), combined with a non-orthogonal eigenbasis (a hallmark of non-normality), can lead to transient growth phenomena that can severely slow the convergence of methods like GMRES. The SVD-based analysis, by focusing on the clustered singular values, completely misses this feature of the eigen-structure. It is this non-normal character, tied to the separated eigenvalues, that governs the true difficulty of solving systems with $A$. A preconditioner based on SVD spectral information is therefore \"missing\" the crucial information about the eigen-spectral properties and the non-normality of the matrix, leading to a potentially poor assessment of the problem's difficulty and an ineffective preconditioning strategy.",
            "answer": "$$\n\\boxed{\\frac{\\alpha\\sqrt{\\alpha^2+4}}{2}}\n$$"
        },
        {
            "introduction": "The final step in cementing theoretical knowledge is to witness its consequences in a practical, computational setting. This exercise transitions from analytical derivation to direct observation, allowing you to verify the starkly different stories that SVD and EVD can tell. You will implement subspace iteration, a fundamental algorithm whose convergence speed is dictated by the separation between eigenvalues, and apply it to different classes of matrices, including the kind of non-normal matrix explored previously . By programmatically comparing the convergence rates when targeting subspaces defined by eigenvectors versus singular vectors, you will gain a tangible understanding of how spectral mismatch directly and dramatically impacts real-world algorithm performance.",
            "id": "3573922",
            "problem": "You are given the task of constructing and analyzing matrices to investigate the relationship between the Singular Value Decomposition (SVD) and the eigenvalue decomposition, with a focus on how spectral structure influences subspace iteration and randomized algorithms. Your program must implement the following experiments, grounded in the fundamental definitions of singular value decomposition and eigenvalue decomposition and the well-tested facts about the relationship between singular values and the eigenvalues of the Gram matrix.\n\nFundamental bases to be used:\n- Singular Value Decomposition (SVD): For any real matrix $A \\in \\mathbb{R}^{m \\times n}$, there exist orthogonal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$, and a diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ with nonnegative diagonal entries (the singular values) such that $A = U \\Sigma V^{\\top}$. The singular values are ordered as $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge 0$.\n- Eigenvalue decomposition: For a diagonalizable matrix $A \\in \\mathbb{R}^{n \\times n}$, there exists an invertible matrix $X$ and a diagonal matrix $\\Lambda$ such that $A = X \\Lambda X^{-1}$, where the diagonal entries of $\\Lambda$ are the eigenvalues $\\lambda_{i}$ of $A$. In general, for non-normal matrices, $X$ need not be orthogonal and the eigenvectors need not be orthogonal.\n- Relationship between SVD and eigenvalue decomposition: The singular values of $A$ satisfy $\\sigma_{i}(A) = \\sqrt{\\lambda_{i}(A^{\\top} A)}$, where the eigenvalues of $A^{\\top} A$ are real and nonnegative.\n- Subspace iteration (block power method): Given a square matrix $M \\in \\mathbb{R}^{n \\times n}$ and a starting basis $Q_{0} \\in \\mathbb{R}^{n \\times k}$ with full column rank, iterative updates of the form $Y_{t} = M Q_{t-1}$ followed by orthonormalization $Q_{t} = \\operatorname{orth}(Y_{t})$ generate a sequence of $k$-dimensional subspaces. Convergence toward the invariant subspace associated with the $k$ dominant eigenvalues depends on spectral separation and, for non-normal matrices, on the conditioning of the eigenvector basis.\n\nYou will construct three square matrices and perform subspace iteration targeting either singular vectors (by iterating on the Gram matrix $A^{\\top} A$) or eigenvectors (by iterating on $A$ itself). You will also implement randomized algorithms to probe convergence and approximation behavior.\n\nMatrix construction and test suite:\n- Case 1 (Non-normal, well-separated eigenvalues but clustered singular values):\n  - Let $n = 6$ and $\\Lambda = \\operatorname{diag}([243, 81, 27, 9, 3, 1])$.\n  - Define an upper-triangular matrix $V$ with entries $V_{ij} = \\alpha^{j-i}$ for $j \\ge i$ and $V_{ij} = 0$ for $j  i$, for a chosen $\\alpha \\in \\{1, 5, 10, 20, 50, 100\\}$.\n  - Form $A_{1} = V \\Lambda V^{-1}$. Choose the smallest $\\alpha$ in the set for which the gap ratio $\\sigma_{2}(A_{1}) / \\sigma_{3}(A_{1}) \\le 1.1$ holds. If none satisfies the inequality, use $\\alpha = 100$.\n  - Subspace target dimension $k = 2$.\n\n- Case 2 (Normal matrix with aligned spectra): \n  - Let $A_{2} = \\operatorname{diag}([243, 81, 27, 9, 3, 1])$.\n  - Subspace target dimension $k = 2$.\n\n- Case 3 (Highly non-normal with clustered eigenvalues but separated singular values):\n  - Let $n = 6$ and $A_{3}$ be the upper-bidiagonal matrix with $1$ on the diagonal and $\\alpha$ on the first superdiagonal, i.e., $(A_{3})_{ii} = 1$ and $(A_{3})_{i,i+1} = \\alpha$ for $i = 1, \\dots, n-1$, zeros elsewhere, with $\\alpha = 10$.\n  - Note that all eigenvalues of $A_{3}$ are equal to $1$ (algebraic multiplicity $n$), and the eigenspace is one-dimensional; the right eigen-subspace is uniquely defined. The singular values of $A_{3}$ are well-separated for large $\\alpha$.\n  - Subspace target dimension $k = 1$.\n\nFor each case, perform the following computations:\n\n1. True target subspaces:\n   - For SVD targeting, compute the top-$k$ right singular subspace of $A$ via the right singular vectors $V$ corresponding to the largest $k$ singular values of $A$.\n   - For eigenvalue targeting, compute the right invariant subspace of dimension $k$ by selecting the eigenvectors corresponding to the $k$ eigenvalues of largest magnitude (break ties deterministically by index order); then orthonormalize the selected vectors to form an orthonormal basis of the target subspace.\n\n2. Subspace iteration experiments:\n   - Initialize a fixed Gaussian random starting matrix $Q_{0} \\in \\mathbb{R}^{n \\times k}$ with entries drawn independently from the standard normal distribution, using a fixed random seed.\n   - For SVD targeting, iterate on the Gram matrix $G = A^{\\top} A$:\n     - For $t = 1, 2, \\dots$, perform $Y_{t} = G Q_{t-1}$, compute $Q_{t} = \\operatorname{orth}(Y_{t})$, and measure the distance to the true top-$k$ right singular subspace using the spectral norm of $\\sin$ of the principal angles. Denote this distance by $d_{t}^{\\text{svd}}$.\n     - Let $T_{\\text{svd}}$ be the smallest $t$ such that $d_{t}^{\\text{svd}} \\le \\tau$, with tolerance $\\tau = 10^{-6}$; if no such $t$ exists up to $200$ iterations, set $T_{\\text{svd}} = 201$.\n   - For eigenvalue targeting, iterate on $A$:\n     - For $t = 1, 2, \\dots$, perform $Y_{t} = A Q_{t-1}$, compute $Q_{t} = \\operatorname{orth}(Y_{t})$, and measure the distance to the true target invariant subspace using the same principal-angle-based metric, denote it $d_{t}^{\\text{eig}}$.\n     - Let $T_{\\text{eig}}$ be the smallest $t$ such that $d_{t}^{\\text{eig}} \\le \\tau$; if no such $t$ exists up to $200$ iterations, set $T_{\\text{eig}} = 201$.\n\n   The principal-angle distance between two $k$-dimensional subspaces with orthonormal bases $Q$ and $Q_{\\star}$ is defined as $\\|\\sin \\Theta\\|_{2} = \\sqrt{1 - \\sigma_{\\min}^{2}(Q_{\\star}^{\\top} Q)}$, where $\\sigma_{\\min}$ denotes the smallest singular value of $Q_{\\star}^{\\top} Q$.\n\n3. Randomized algorithms:\n   - Randomized SVD range finder for rank-$k$:\n     - Draw a Gaussian test matrix $\\Omega \\in \\mathbb{R}^{n \\times (k+s)}$ with $s = 4$ using the same fixed random seed.\n     - Form a sample matrix with one power iteration: $Y = A \\Omega$, then $Y \\leftarrow A (A^{\\top} Y)$, then compute an orthonormal basis $Q = \\operatorname{orth}(Y)$.\n     - Compute the spectral-norm residual $\\|A - Q Q^{\\top} A\\|_{2}$ by computing the largest singular value of $A - Q Q^{\\top} A$. Denote this residual by $E_{\\text{svd}}$.\n   - Randomized eigen-subspace finder for dimension $k$:\n     - Draw a Gaussian matrix $\\Omega \\in \\mathbb{R}^{n \\times k}$ with the same fixed random seed as above.\n     - Perform two steps of block power iteration on $A$: $Y = A \\Omega$, then $Y \\leftarrow A Y$, and compute $Q = \\operatorname{orth}(Y)$.\n     - Compute the principal-angle distance to the true eigen-subspace as above; denote it by $D_{\\text{eig}}$.\n\nYour program must use the following fixed parameters:\n- Tolerance $\\tau = 10^{-6}$.\n- Maximum number of subspace iteration steps $200$.\n- Oversampling $s = 4$.\n- One power iteration for the randomized SVD range finder.\n- Two steps for the randomized eigen-subspace finder.\n- Random seed $1234$ for all randomized steps.\n\nRequired outputs:\n- For each case $i \\in \\{1,2,3\\}$, compute the four quantities in the order: $T_{\\text{svd}}^{(i)}$, $T_{\\text{eig}}^{(i)}$, $E_{\\text{svd}}^{(i)}$, $D_{\\text{eig}}^{(i)}$.\n- Aggregate them into a single list of length $12$ in the order of cases $1$ to $3$ and within each case as above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[r1,r2,r3,...,r12]\"), where each entry is printed in standard Python float or integer notation. No additional text should be printed.\n\nAll quantities are dimensionless, and no physical units are involved. Ensure that the construction of $A_{1}$ satisfies the clustering condition if possible by selecting an appropriate $\\alpha$ as specified. If convergence is not achieved within the iteration limit, follow the rule above for setting the iteration count to $201$.",
            "solution": "The problem requires an investigation into the relationship between the Singular Value Decomposition (SVD) and the eigenvalue decomposition, focusing on how the spectral characteristics of a matrix affect the convergence of iterative algorithms for finding subspaces. We will construct three distinct matrices—non-normal with clustered singular values, normal, and highly non-normal with clustered eigenvalues—and for each, we will compute the convergence speed of subspace iteration and the accuracy of randomized one-shot algorithms for both SVD-related (right singular vectors) and eigenvalue-related (right eigenvectors) target subspaces.\n\nThe fundamental principles underpinning this analysis are:\n1.  The SVD of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices whose columns are the left and right singular vectors, respectively, and $\\Sigma$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$.\n2.  The eigenvalue decomposition of a diagonalizable square matrix $A \\in \\mathbb{R}^{n \\times n}$ is $A = X \\Lambda X^{-1}$, where $\\Lambda$ is a diagonal matrix of eigenvalues $\\lambda_i$ and the columns of $X$ are the corresponding right eigenvectors.\n3.  The right singular vectors of $A$ are the eigenvectors of the symmetric positive semi-definite Gram matrix $G = A^{\\top} A$. The eigenvalues of $G$ are the squares of the singular values of $A$, i.e., $\\lambda_i(A^{\\top} A) = \\sigma_i(A)^2$.\n4.  The convergence rate of the power method and its block variant, subspace iteration, for finding the dominant invariant subspace of a matrix $M$ is governed by the ratio of eigenvalues $|\\lambda_{k+1}|/|\\lambda_k|$, where $k$ is the dimension of the target subspace. A smaller ratio (a larger spectral gap) implies faster convergence.\n\nOur procedure is as follows: for each of the three specified matrices, we will first determine the \"true\" target subspaces. Then, we will execute two types of experiments: subspace iteration to measure convergence time and a randomized projection method to measure approximation accuracy.\n\nThe distance between a computed subspace (with orthonormal basis $Q$) and a true subspace (with orthonormal basis $Q_{\\star}$) of dimension $k$ is measured using the spectral norm of the sine of the principal angles, given by $d(Q, Q_{\\star}) = \\|\\sin \\Theta\\|_{2} = \\sqrt{1 - \\sigma_{\\min}^{2}(Q_{\\star}^{\\top} Q)}$, where $\\sigma_{\\min}$ is the smallest singular value of the matrix $Q_{\\star}^{\\top} Q$.\n\nAll randomized components will be initialized using a pseudo-random number generator with a fixed seed of $1234$ to ensure reproducibility.\n\n**Case 1: Non-normal, well-separated eigenvalues, clustered singular values**\nMatrix definition: $A_1 = V \\Lambda V^{-1}$ where $n=6$, $\\Lambda = \\operatorname{diag}([243, 81, 27, 9, 3, 1])$, and $V$ is an upper-triangular matrix with entries $V_{ij} = \\alpha^{j-i}$ for $j \\ge i$. The parameter $\\alpha$ is chosen from the set $\\{1, 5, 10, 20, 50, 100\\}$ as the smallest value for which the singular value gap ratio $\\sigma_2(A_1)/\\sigma_3(A_1) \\le 1.1$.\n\nFirst, we must determine $\\alpha$. We iterate through the given set:\n- For $\\alpha=1$, $V=I$, so $A_1=\\Lambda$. The singular values are the eigenvalues. $\\sigma_2/\\sigma_3 = 81/27 = 3.0  1.1$.\n- For $\\alpha=5$, we construct $V$ and $A_1$, compute its singular values, and find the ratio is approximately $1.64  1.1$.\n- For $\\alpha=10$, the ratio is approximately $1.15  1.1$.\n- For $\\alpha=20$, the ratio is approximately $1.03 \\le 1.1$.\nThus, we select $\\alpha=20$. We construct $A_1$ using this value. The target dimension is $k=2$.\n\n-   **True Subspaces**:\n    -   SVD Target ($Q_{\\star}^{\\text{svd}}$): We compute the SVD of $A_1 = U \\Sigma V^{\\top}$ and take the first $k=2$ columns of $V$.\n    -   Eigen Target ($Q_{\\star}^{\\text{eig}}$): The eigenvalues of $A_1$ are the diagonal entries of $\\Lambda$. The two largest eigenvalues are $\\lambda_1=243$ and $\\lambda_2=81$. We find the corresponding eigenvectors from the decomposition $A_1 = X \\Lambda X^{-1}$ and form an orthonormal basis from these two vectors using a QR decomposition.\n-   **Subspace Iteration**:\n    -   $T_{\\text{svd}}^{(1)}$: We iterate on $G = A_1^{\\top} A_1$. The eigenvalues of $G$ are $\\sigma_i(A_1)^2$. Since $\\sigma_2(A_1)/\\sigma_3(A_1) \\approx 1.03$, the relevant eigenvalue ratio for convergence $\\lambda_3(G)/\\lambda_2(G) = (\\sigma_3(A_1)/\\sigma_2(A_1))^2 \\approx (1/1.03)^2 \\approx 0.94$, which is very close to $1$. Convergence is expected to be slow.\n    -   $T_{\\text{eig}}^{(1)}$: We iterate on $A_1$. The relevant eigenvalue ratio is $|\\lambda_3(A_1)|/|\\lambda_2(A_1)| = 27/81 = 1/3$. This large spectral gap suggests rapid convergence.\n-   **Randomized Algorithms**:\n    -   $E_{\\text{svd}}^{(1)}$: A random projection $A_1\\Omega$ where $\\Omega \\in \\mathbb{R}^{6 \\times (2+4)}$ is used. One power step $Y = A_1(A_1^{\\top}(A_1\\Omega))$ is applied to form the basis $Q = \\operatorname{orth}(Y)$. The residual $\\|A_1 - QQ^{\\top}A_1\\|_2$ is computed.\n    -   $D_{\\text{eig}}^{(1)}$: Two steps of the power method are applied to a random matrix $\\Omega \\in \\mathbb{R}^{6 \\times 2}$ to get $Y = A_1^2 \\Omega$. The basis $Q=\\operatorname{orth}(Y)$ is formed and its distance to $Q_{\\star}^{\\text{eig}}$ is computed.\n\n**Case 2: Normal matrix with aligned spectra**\nMatrix definition: $A_2 = \\operatorname{diag}([243, 81, 27, 9, 3, 1])$. This matrix is symmetric and thus normal. The target dimension is $k=2$.\n\n-   **True Subspaces**: Since $A_2$ is symmetric with positive diagonal entries, its singular values are its eigenvalues ($\\sigma_i = \\lambda_i$) and its right singular vectors are its eigenvectors. The eigenvectors are the standard basis vectors $e_1, e_2, \\dots, e_6$. The top-$k$ SVD and eigen subspaces are identical, spanned by $\\{e_1, e_2\\}$.\n-   **Subspace Iteration**:\n    -   $T_{\\text{svd}}^{(2)}$: Iteration is on $A_2^{\\top}A_2 = A_2^2$. The convergence ratio is $(\\lambda_3(A_2^2))/(\\lambda_2(A_2^2)) = (\\sigma_3/\\sigma_2)^2 = (27/81)^2 = (1/3)^2 = 1/9$. This indicates very fast convergence.\n    -   $T_{\\text{eig}}^{(2)}$: Iteration is on $A_2$. The convergence ratio is $\\lambda_3/\\lambda_2 = 27/81 = 1/3$. This also indicates fast convergence.\n-   **Randomized Algorithms**: Calculations for $E_{\\text{svd}}^{(2)}$ and $D_{\\text{eig}}^{(2)}$ proceed as for Case 1, but with matrix $A_2$.\n\n**Case 3: Highly non-normal with clustered eigenvalues**\nMatrix definition: $A_3$ is an upper-bidiagonal matrix of size $n=6$ with ones on the main diagonal and $\\alpha=10$ on the first superdiagonal. The target dimension is $k=1$.\n\n-   **True Subspaces**:\n    -   SVD Target ($Q_{\\star}^{\\text{svd}}$): We compute the SVD of $A_3$ and take the first right singular vector (first column of $V$). The singular values of this type of matrix are known to be well-separated for large $\\alpha$.\n    -   Eigen Target ($Q_{\\star}^{\\text{eig}}$): $A_3$ is upper-triangular, so its eigenvalues are its diagonal entries: $\\lambda_1 = \\dots = \\lambda_6 = 1$. The matrix is defective; the geometric multiplicity of the eigenvalue $1$ is $1$. The eigenspace is one-dimensional, spanned by the standard basis vector $e_1 = [1, 0, \\dots, 0]^{\\top}$.\n-   **Subspace Iteration**:\n    -   $T_{\\text{svd}}^{(3)}$: Iterating on $A_3^{\\top}A_3$. The singular values are well-separated, which means the eigenvalues of $A_3^{\\top}A_3$ are also well-separated. Thus, convergence should be fast.\n    -   $T_{\\text{eig}}^{(3)}$: Iterating on $A_3$. All eigenvalues are $1$, so there is no spectral gap. The power method will fail to converge to the dominant eigenspace. We expect the iteration limit of $200$ to be reached, resulting in $T_{\\text{eig}}^{(3)} = 201$.\n-   **Randomized Algorithms**: Calculations for $E_{\\text{svd}}^{(3)}$ and $D_{\\text{eig}}^{(3)}$ proceed as before. For $D_{\\text{eig}}^{(3)}$, we expect poor accuracy as two steps of power iteration are insufficient to overcome the lack of a spectral gap.\n\nThe implementation will now perform these calculations systematically for each case.",
            "answer": "[25,5,18.067341029272183,1.3877787807814457e-17,4,7,0.0,0.0,3,201,1.0,0.9986161474849685]"
        }
    ]
}