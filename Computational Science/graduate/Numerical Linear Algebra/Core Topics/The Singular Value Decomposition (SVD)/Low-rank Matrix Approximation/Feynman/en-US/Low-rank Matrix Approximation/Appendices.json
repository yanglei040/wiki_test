{
    "hands_on_practices": [
        {
            "introduction": "The Eckart-Young-Mirsky theorem establishes the singular value decomposition (SVD) as the optimal source for low-rank approximations under unitarily invariant norms like the Frobenius norm. However, is this optimality universal across all possible ways of measuring error? This exercise challenges that assumption by exploring approximation under the entrywise infinity norm, forcing a deeper consideration of how the choice of error metric dictates the best solution .",
            "id": "3557705",
            "problem": "Consider the entrywise infinity norm defined by $\\|A\\|_{\\infty} = \\max_{i,j} |a_{ij}|$ for a real matrix $A$. A widely used approach to low-rank approximation is the truncated singular value decomposition (SVD), which is known to minimize the approximation error in the spectral $2$-norm and the Frobenius norm among all rank-$k$ approximations. However, there is no general optimality guarantee for the truncated SVD under the entrywise infinity norm.\n\nConstruct the $2 \\times 2$ matrix $A_{\\alpha} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix}$ with a parameter $\\alpha \\in (0,1)$ and restrict attention to rank-$1$ approximations. Define the truncated singular value decomposition rank-$1$ approximation $X_{\\mathrm{svd}}$ of $A_{\\alpha}$ to be $\\sigma_{1} u_{1} v_{1}^{\\top}$, where $\\sigma_{1}$ is the largest singular value of $A_{\\alpha}$ and $u_{1}, v_{1}$ are the corresponding left and right singular vectors.\n\nWorking from first principles and using only the definitions specified above, derive the analytical expression for the difference between:\n- the minimal entrywise infinity-norm error $\\min_{\\operatorname{rank}(X)=1} \\|A_{\\alpha} - X\\|_{\\infty}$, and\n- the entrywise infinity-norm error $\\|A_{\\alpha} - X_{\\mathrm{svd}}\\|_{\\infty}$.\n\nYou must justify that the best entrywise infinity-norm rank-$1$ approximation does not coincide with the truncated singular value decomposition approximation for $\\alpha \\in (0,1)$, and then compute the exact gap between the two errors as a function of $\\alpha$. Express your final answer as a single closed-form expression in $\\alpha$. No rounding is required.",
            "solution": "The problem asks for the difference between the minimal entrywise infinity-norm error for a rank-$1$ approximation of the matrix $A_{\\alpha}$, and the error of the rank-$1$ approximation obtained from the truncated singular value decomposition (SVD). The matrix is given by $A_{\\alpha} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix}$ for a parameter $\\alpha \\in (0,1)$. The entrywise infinity norm is defined as $\\|A\\|_{\\infty} = \\max_{i,j} |a_{ij}|$.\n\nFirst, we determine the rank-$1$ approximation from the truncated SVD, denoted by $X_{\\mathrm{svd}}$, and calculate its error, $\\|A_{\\alpha} - X_{\\mathrm{svd}}\\|_{\\infty}$.\n\nThe matrix $A_{\\alpha}$ is a real diagonal matrix. Its singular values are the absolute values of its diagonal entries. Since $\\alpha \\in (0,1)$, the diagonal entries are $1$ and $-\\alpha$. Their absolute values are $1$ and $\\alpha$. Thus, the singular values are $\\sigma_1 = 1$ and $\\sigma_2 = \\alpha$, with $\\sigma_1$ being the largest.\n\nThe SVD of $A_{\\alpha}$ is $A_{\\alpha} = U \\Sigma V^{\\top}$. The right singular vectors $v_i$ are the eigenvectors of $A_{\\alpha}^{\\top} A_{\\alpha}$, and the left singular vectors $u_i$ are the eigenvectors of $A_{\\alpha} A_{\\alpha}^{\\top}$.\n$$A_{\\alpha}^{\\top} A_{\\alpha} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\alpha^2 \\end{pmatrix}$$\nThe eigenvalues are $\\sigma_1^2 = 1$ and $\\sigma_2^2 = \\alpha^2$.\nThe eigenvector for $\\sigma_1^2=1$ is $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe corresponding left singular vector is $u_1 = \\frac{1}{\\sigma_1} A_{\\alpha} v_1 = \\frac{1}{1} \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nThe truncated SVD rank-$1$ approximation is $X_{\\mathrm{svd}} = \\sigma_{1} u_{1} v_{1}^{\\top}$.\n$$X_{\\mathrm{svd}} = 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$$\nThe error matrix for this approximation is:\n$$E_{\\mathrm{svd}} = A_{\\alpha} - X_{\\mathrm{svd}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & -\\alpha \\end{pmatrix}$$\nThe entrywise infinity-norm error is:\n$$\\|A_{\\alpha} - X_{\\mathrm{svd}}\\|_{\\infty} = \\max(|0|, |0|, |0|, |-\\alpha|) = \\alpha$$\n\nNext, we find the minimal error, $\\varepsilon_{\\mathrm{opt}} = \\min_{\\operatorname{rank}(X)=1} \\|A_{\\alpha} - X\\|_{\\infty}$.\nLet $X = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\end{pmatrix}$ be an arbitrary rank-$1$ matrix, which means its determinant is zero: $x_{11}x_{22} - x_{12}x_{21} = 0$.\nLet $\\varepsilon = \\|A_{\\alpha} - X\\|_{\\infty}$. This implies that the entries of the error matrix $E = A_{\\alpha} - X$ are bounded in absolute value by $\\varepsilon$:\n$$A_{\\alpha} - X = \\begin{pmatrix} 1 - x_{11} & -x_{12} \\\\ -x_{21} & -\\alpha - x_{22} \\end{pmatrix} = \\begin{pmatrix} e_{11} & e_{12} \\\\ e_{21} & e_{22} \\end{pmatrix}$$\nwhere $|e_{ij}| \\le \\varepsilon$ for all $i,j \\in \\{1,2\\}$.\nThis gives us the following constraints on the entries of $X$:\n1.  $|1 - x_{11}| \\le \\varepsilon \\implies 1-\\varepsilon \\le x_{11} \\le 1+\\varepsilon$\n2.  $|-x_{12}| \\le \\varepsilon \\implies -\\varepsilon \\le x_{12} \\le \\varepsilon$\n3.  $|-x_{21}| \\le \\varepsilon \\implies -\\varepsilon \\le x_{21} \\le \\varepsilon$\n4.  $|-\\alpha - x_{22}| \\le \\varepsilon \\implies -\\alpha-\\varepsilon \\le x_{22} \\le -\\alpha+\\varepsilon$\n\nWe seek the minimal $\\varepsilon \\ge 0$ for which there exist $x_{ij}$ satisfying these inequalities and the rank-$1$ condition $x_{11}x_{22} = x_{12}x_{21}$.\nFrom the inequalities, the product $x_{12}x_{21}$ must lie in the interval $[-\\varepsilon^2, \\varepsilon^2]$.\nFor $x_{11}$ and $x_{22}$, since $\\alpha \\in (0,1)$, we can assume $\\varepsilon < 1$. Then $1-\\varepsilon > 0$, so $x_{11}$ is positive. Also, if $\\varepsilon$ is small enough, $x_{22}$ will be negative.\nThe range of values for the product $x_{11}x_{22}$ is therefore contained in the interval $[(1+\\varepsilon)(-\\alpha-\\varepsilon), (1-\\varepsilon)(-\\alpha+\\varepsilon)] = [-(1+\\varepsilon)(\\alpha+\\varepsilon), -(1-\\varepsilon)(\\alpha-\\varepsilon)]$.\nFor a solution to exist, the interval for $x_{11}x_{22}$ and the interval for $x_{12}x_{21}$ must overlap. A non-empty intersection requires that the upper bound of the $x_{11}x_{22}$ interval is greater than or equal to the lower bound of the $x_{12}x_{21}$ interval.\n$$-(1-\\varepsilon)(\\alpha-\\varepsilon) \\ge -\\varepsilon^2$$\nMultiplying by $-1$ and reversing the inequality sign gives:\n$$(1-\\varepsilon)(\\alpha-\\varepsilon) \\le \\varepsilon^2$$\nTo find the minimum possible $\\varepsilon$, we consider the boundary case of this condition:\n$$(1-\\varepsilon)(\\alpha-\\varepsilon) = \\varepsilon^2$$\n$$\\alpha - \\varepsilon - \\alpha\\varepsilon + \\varepsilon^2 = \\varepsilon^2$$\n$$\\alpha = \\varepsilon + \\alpha\\varepsilon = \\varepsilon(1+\\alpha)$$\nThis gives the minimal error candidate:\n$$\\varepsilon_{\\mathrm{opt}} = \\frac{\\alpha}{1+\\alpha}$$\nTo justify that this is indeed the minimum error, we must show that a rank-$1$ matrix $X$ exists which achieves this error. When $\\varepsilon = \\frac{\\alpha}{1+\\alpha}$, the intervals for $x_{11}x_{22}$ and $x_{12}x_{21}$ touch at a single point, $-\\varepsilon^2$. Thus, we must have $x_{11}x_{22} = -\\varepsilon^2$ and $x_{12}x_{21} = -\\varepsilon^2$.\nLet's construct such a matrix $X_{\\mathrm{opt}}$. We require all error entries to have magnitude $\\varepsilon_{\\mathrm{opt}}$.\nLet $X_{\\mathrm{opt}} = A_{\\alpha} - E_{\\mathrm{opt}}$, where all entries of $E_{\\mathrm{opt}}$ have magnitude $\\varepsilon_{\\mathrm{opt}}$. A possible choice for $E_{\\mathrm{opt}}$ is:\n$$E_{\\mathrm{opt}} = \\begin{pmatrix} \\varepsilon_{\\mathrm{opt}} & -\\varepsilon_{\\mathrm{opt}} \\\\ \\varepsilon_{\\mathrm{opt}} & -\\varepsilon_{\\mathrm{opt}} \\end{pmatrix} = \\frac{\\alpha}{1+\\alpha} \\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix}$$\nThen the approximation is:\n$$X_{\\mathrm{opt}} = A_{\\alpha} - E_{\\mathrm{opt}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -\\alpha \\end{pmatrix} - \\frac{\\alpha}{1+\\alpha}\\begin{pmatrix} 1 & -1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{\\alpha}{1+\\alpha} & \\frac{\\alpha}{1+\\alpha} \\\\ -\\frac{\\alpha}{1+\\alpha} & -\\alpha + \\frac{\\alpha}{1+\\alpha} \\end{pmatrix}$$\n$$X_{\\mathrm{opt}} = \\begin{pmatrix} \\frac{1}{1+\\alpha} & \\frac{\\alpha}{1+\\alpha} \\\\ -\\frac{\\alpha}{1+\\alpha} & \\frac{-\\alpha(1+\\alpha)+\\alpha}{1+\\alpha} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1+\\alpha} & \\frac{\\alpha}{1+\\alpha} \\\\ -\\frac{\\alpha}{1+\\alpha} & \\frac{-\\alpha^2}{1+\\alpha} \\end{pmatrix}$$\nWe check if this matrix is rank-$1$:\n$$\\det(X_{\\mathrm{opt}}) = \\left(\\frac{1}{1+\\alpha}\\right)\\left(\\frac{-\\alpha^2}{1+\\alpha}\\right) - \\left(\\frac{\\alpha}{1+\\alpha}\\right)\\left(-\\frac{\\alpha}{1+\\alpha}\\right) = \\frac{-\\alpha^2}{(1+\\alpha)^2} + \\frac{\\alpha^2}{(1+\\alpha)^2} = 0$$\nSince the determinant is $0$, $X_{\\mathrm{opt}}$ is a rank-$1$ matrix. The error is $\\|A_{\\alpha} - X_{\\mathrm{opt}}\\|_{\\infty} = \\|E_{\\mathrm{opt}}\\|_{\\infty} = \\varepsilon_{\\mathrm{opt}} = \\frac{\\alpha}{1+\\alpha}$. This confirms that the minimal error is indeed $\\frac{\\alpha}{1+\\alpha}$.\n\nThe problem states that the best entrywise infinity-norm rank-$1$ approximation does not coincide with the SVD approximation. We can see this by comparing their errors.\nError of SVD approximation: $\\|A_{\\alpha} - X_{\\mathrm{svd}}\\|_{\\infty} = \\alpha$.\nMinimal error: $\\min_{\\operatorname{rank}(X)=1} \\|A_{\\alpha} - X\\|_{\\infty} = \\frac{\\alpha}{1+\\alpha}$.\nSince $\\alpha \\in (0,1)$, we have $1+\\alpha > 1$, which implies $\\frac{1}{1+\\alpha}  1$. As $\\alpha > 0$, multiplying by $\\alpha$ gives $\\frac{\\alpha}{1+\\alpha}  \\alpha$.\nThis proves that the truncated SVD does not yield the optimal rank-$1$ approximation in the entrywise infinity norm for this case.\n\nThe final step is to compute the difference, or gap, between these two error values. This is naturally interpreted as the larger error minus the smaller error.\nGap = $\\|A_{\\alpha} - X_{\\mathrm{svd}}\\|_{\\infty} - \\min_{\\operatorname{rank}(X)=1} \\|A_{\\alpha} - X\\|_{\\infty}$\nGap = $\\alpha - \\frac{\\alpha}{1+\\alpha}$\nGap = $\\frac{\\alpha(1+\\alpha) - \\alpha}{1+\\alpha} = \\frac{\\alpha + \\alpha^2 - \\alpha}{1+\\alpha} = \\frac{\\alpha^2}{1+\\alpha}$",
            "answer": "$$\\boxed{\\frac{\\alpha^2}{1+\\alpha}}$$"
        },
        {
            "introduction": "Beyond the choice of norm, the quality of our data is a critical factor, and real-world data is often imperfect. Standard SVD, by minimizing a sum-of-squares error, is notoriously sensitive to large-magnitude outliers that can corrupt the approximation. This practice provides a clear, quantitative demonstration of this vulnerability and contrasts the SVD approach with a more robust $\\ell_{1}$-based formulation, highlighting the crucial concept of robustness in modern data analysis .",
            "id": "3557735",
            "problem": "Consider the following explicit family of data matrices designed to isolate the effect of a single large-magnitude outlier on low-rank approximation. For an integer $n \\geq 3$ and a parameter $M \\geq 0$, define the matrix $A(M) \\in \\mathbb{R}^{2 \\times n}$ by\n$$\nA(M) \\;=\\; \\begin{bmatrix}\n0  0  \\cdots  0  M \\\\\n1  1  \\cdots  1  0\n\\end{bmatrix},\n$$\nthat is, the first $n-1$ columns are equal to the inlier vector $e_{2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, and the last column is the outlier $M e_{1} = \\begin{bmatrix} M \\\\ 0 \\end{bmatrix}$. The “true” underlying low-rank structure (in the absence of the outlier) is the rank-$1$ matrix $L^{\\star} = e_{2} \\mathbf{1}^{\\top}$, where $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of all ones.\n\nYou will analyze two rank-$1$ approximation strategies from first principles:\n\n1. The truncated singular value decomposition (SVD): By the Eckart–Young–Mirsky theorem, the best rank-$1$ approximation in the Frobenius norm is given by the leading singular triplet of $A(M)$. The top left singular vector is the principal eigenvector of $A(M) A(M)^{\\top}$.\n\n2. An $\\ell_{1}$-based rank-$1$ approximation: Minimize the entrywise absolute deviation subject to a rank-$1$ constraint,\n$$\n\\min_{u \\in \\mathbb{R}^{2},\\, v \\in \\mathbb{R}^{n}} \\;\\sum_{i=1}^{2} \\sum_{j=1}^{n} \\left| A(M)_{ij} - (u v^{\\top})_{ij} \\right| \\quad \\text{subject to } \\|u\\|_{2} = 1.\n$$\nThis is a robust, entrywise $\\ell_{1}$ loss criterion.\n\nStarting from the definitions of the singular value decomposition (SVD) and the $\\ell_{1}$ objective, and using only fundamental linear algebra facts such as the relation between singular vectors and the eigenstructure of $A(M) A(M)^{\\top}$, answer the following:\n\n(a) Derive the principal left singular vector of $A(M)$ as a function of $M$ and $n$, and compute the limit of the cosine of the principal angle between this vector and the ground-truth direction $e_{2}$ as $M \\to \\infty$.\n\n(b) Determine the exact magnitude threshold $M_{2}(n)$ at which the truncated SVD’s top left singular vector switches from $e_{2}$ to $e_{1}$.\n\n(c) For the $\\ell_{1}$-based problem, compute the optimal objective values attained by the two candidate directions $u = e_{2}$ (fitting the inliers) and $u = e_{1}$ (fitting the outlier), after optimizing over $v \\in \\mathbb{R}^{n}$ in each case. From these, determine the exact magnitude threshold $M_{1}(n)$ at which the $\\ell_{1}$-based estimator switches from $e_{2}$ to $e_{1}$.\n\nProvide your final answer as a single row vector containing three entries in this order:\n$$\n\\left(\\lim_{M \\to \\infty} \\cos \\theta(M),\\; M_{2}(n),\\; M_{1}(n)\\right),\n$$\nwhere $\\theta(M)$ is the angle between the SVD’s top left singular vector and $e_{2}$. No rounding is required. Express your answer in closed form in terms of $n$ only.",
            "solution": "The user wants to analyze two different rank-$1$ approximation methods for a matrix $A(M)$ containing a single large outlier. The analysis must be conducted from first principles. The problem is divided into three parts: (a) analyzing the Singular Value Decomposition (SVD) based approximation, (b) finding the breakdown threshold for the SVD method, and (c) finding the breakdown threshold for an $\\ell_{1}$-norm based robust method.\n\nFirst, we establish the structure of the matrix $A(M) \\in \\mathbb{R}^{2 \\times n}$. The first $n-1$ columns are $e_{2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ and the last column is $M e_{1} = \\begin{bmatrix} M \\\\ 0 \\end{bmatrix}$. The matrix is explicitly:\n$$\nA(M) = \\begin{bmatrix}\n0  0  \\cdots  0  M \\\\\n1  1  \\cdots  1  0\n\\end{bmatrix}\n$$\nThe ground-truth direction is given as $e_2$.\n\n(a) Derive the principal left singular vector of $A(M)$ and compute a related limit.\n\nThe principal left singular vector of $A(M)$ is the eigenvector corresponding to the largest eigenvalue of the matrix $A(M)A(M)^{\\top}$. Let's compute this $2 \\times 2$ matrix:\n$$\nA(M)A(M)^{\\top} = \\begin{bmatrix}\n0  0  \\cdots  0  M \\\\\n1  1  \\cdots  1  0\n\\end{bmatrix}\n\\begin{bmatrix}\n0  1 \\\\\n0  1 \\\\\n\\vdots  \\vdots \\\\\n0  1 \\\\\nM  0\n\\end{bmatrix}\n$$\nThe entries of the resulting matrix are:\nThe $(1,1)$ entry is the dot product of the first row of $A(M)$ with itself: $(n-1) \\cdot 0^2 + M^2 = M^2$.\nThe $(2,2)$ entry is the dot product of the second row of $A(M)$ with itself: $(n-1) \\cdot 1^2 + 0^2 = n-1$.\nThe off-diagonal entries are the dot product of the first and second rows: $(n-1) \\cdot (0 \\cdot 1) + (M \\cdot 0) = 0$.\nThus, the matrix is diagonal:\n$$\nA(M)A(M)^{\\top} = \\begin{bmatrix}\nM^2  0 \\\\\n0  n-1\n\\end{bmatrix}\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries: $\\lambda_a = M^2$ and $\\lambda_b = n-1$. The corresponding eigenvectors are the standard basis vectors $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, respectively.\n\nThe principal left singular vector, which we denote as $u_{SVD}(M)$, is the eigenvector associated with the largest eigenvalue. We must compare $M^2$ and $n-1$:\n\\begin{itemize}\n    \\item If $M^2  n-1$ (i.e., $M  \\sqrt{n-1}$ since $M \\geq 0$), the largest eigenvalue is $n-1$. The principal left singular vector is $u_{SVD}(M) = e_2$.\n    \\item If $M^2  n-1$ (i.e., $M  \\sqrt{n-1}$), the largest eigenvalue is $M^2$. The principal left singular vector is $u_{SVD}(M) = e_1$.\n    \\item If $M^2 = n-1$, the eigenvalues are equal, and the eigenspace for this eigenvalue is all of $\\mathbb{R}^2$. Any unit vector is a principal left singular vector.\n\\end{itemize}\nSo, the principal left singular vector as a function of $M$ and $n$ is:\n$$\nu_{SVD}(M) = \\begin{cases}\ne_2  \\text{if } M  \\sqrt{n-1} \\\\\ne_1  \\text{if } M  \\sqrt{n-1}\n\\end{cases}\n$$\nThe problem asks for the limit of the cosine of the angle $\\theta(M)$ between $u_{SVD}(M)$ and the ground-truth direction $e_2$ as $M \\to \\infty$. The cosine of the angle between two unit vectors is their dot product.\nAs $M \\to \\infty$, we are in the regime where $M  \\sqrt{n-1}$. For any such $M$, $u_{SVD}(M) = e_1$. Therefore,\n$$\n\\cos(\\theta(M)) = u_{SVD}(M) \\cdot e_2 = e_1 \\cdot e_2 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0\n$$\nThe limit is therefore:\n$$\n\\lim_{M \\to \\infty} \\cos(\\theta(M)) = \\lim_{M \\to \\infty} 0 = 0\n$$\n\n(b) Determine the threshold $M_2(n)$ for the SVD estimator.\n\nThe SVD estimator is the principal left singular vector. It switches from the ground-truth direction $e_2$ to the outlier direction $e_1$ when the dominant eigenvalue of $A(M)A(M)^{\\top}$ changes. As determined above, this occurs when $M^2 = n-1$. Since $M \\geq 0$, the threshold is $M = \\sqrt{n-1}$.\nThus, the switching threshold for the SVD-based approximation is:\n$$\nM_2(n) = \\sqrt{n-1}\n$$\n\n(c) Determine the threshold $M_1(n)$ for the $\\ell_1$-based estimator.\n\nThe problem is to minimize $J(u,v) = \\|A(M) - uv^{\\top}\\|_{\\ell_1} = \\sum_{i,j} |A_{ij} - (uv^{\\top})_{ij}|$ subject to $\\|u\\|_2 = 1$. We are asked to evaluate the minimum cost for two candidate directions, $u=e_2$ and $u=e_1$, and find when one becomes better than the other.\n\nCase 1: $u = e_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nThe approximation is $uv^{\\top} = e_2 v^{\\top} = \\begin{bmatrix} 0  0  \\cdots  0 \\\\ v_1  v_2  \\cdots  v_n \\end{bmatrix}$. The residual matrix is:\n$$\nA(M) - e_2 v^{\\top} = \\begin{bmatrix}\n0  0  \\cdots  0  M \\\\\n1-v_1  1-v_2  \\cdots  1-v_{n-1}  0-v_n\n\\end{bmatrix}\n$$\nThe $\\ell_1$ error is $J(e_2, v) = |M| + \\sum_{j=1}^{n-1} |1-v_j| + |v_n|$.\nTo minimize this cost over $v \\in \\mathbb{R}^n$, we can minimize each term independently. The minimum is achieved when $v_j=1$ for $j=1, \\dots, n-1$ and $v_n=0$. This is because the median of a set of numbers minimizes the sum of absolute differences. The median of $\\{1, 1, \\dots, 1\\}$ is $1$, and the median of $\\{0\\}$ is $0$.\nThe minimum cost for $u=e_2$ is:\n$$\nJ_2 = \\min_v J(e_2, v) = M + \\sum_{j=1}^{n-1} |1-1| + |0| = M\n$$\n(Note: $M \\geq 0$ is given).\n\nCase 2: $u = e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThe approximation is $uv^{\\top} = e_1 v^{\\top} = \\begin{bmatrix} v_1  v_2  \\cdots  v_n \\\\ 0  0  \\cdots  0 \\end{bmatrix}$. The residual matrix is:\n$$\nA(M) - e_1 v^{\\top} = \\begin{bmatrix}\n0-v_1  0-v_2  \\cdots  0-v_{n-1}  M-v_n \\\\\n1  1  \\cdots  1  0\n\\end{bmatrix}\n$$\nThe $\\ell_1$ error is $J(e_1, v) = \\left(\\sum_{j=1}^{n-1} |-v_j| + |M-v_n|\\right) + \\left(\\sum_{j=1}^{n-1} |1| + |0|\\right)$.\n$J(e_1, v) = \\sum_{j=1}^{n-1} |v_j| + |M-v_n| + (n-1)$.\nTo minimize this cost over $v$, we again minimize the terms independently. The minimum is achieved when $v_j=0$ for $j=1, \\dots, n-1$ and $v_n=M$.\nThe minimum cost for $u=e_1$ is:\n$$\nJ_1 = \\min_v J(e_1, v) = \\sum_{j=1}^{n-1} |0| + |M-M| + (n-1) = n-1\n$$\n\nThe $\\ell_1$-based estimator will select the direction $u$ that results in a lower minimal cost. The switch from $e_2$ to $e_1$ occurs when $J_1$ becomes less than $J_2$. The threshold is where the costs are equal:\n$$\nJ_1 = J_2 \\implies n-1 = M\n$$\nIf $M  n-1$, then $J_2  J_1$, and the estimator chooses $u=e_2$.\nIf $M  n-1$, then $J_1  J_2$, and the estimator chooses $u=e_1$.\nThe switching threshold is therefore:\n$$\nM_1(n) = n-1\n$$\n\nThe final answer is the row vector containing the three requested quantities in order:\n1. $\\lim_{M \\to \\infty} \\cos \\theta(M) = 0$\n2. $M_2(n) = \\sqrt{n-1}$\n3. $M_1(n) = n-1$\n\nThe SVD-based method is sensitive to the squared magnitude of the outlier ($M^2$), while the $\\ell_1$-based method is sensitive to the linear magnitude ($M$). This makes the $\\ell_1$ method significantly more robust, as its breakdown threshold $M_1(n)=n-1$ is much larger than the SVD's breakdown threshold $M_2(n)=\\sqrt{n-1}$ for $n2$.\n\nFinal answer assembly: $(\\lim_{M \\to \\infty} \\cos \\theta(M), M_2(n), M_1(n)) = (0, \\sqrt{n-1}, n-1)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\sqrt{n-1}  n-1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having explored some practical limitations of SVD, we now turn to the fundamental question of why low-rank approximation is effective in the first place. The utility stems from the rapid decay of singular values observed in many matrices from science and engineering. This theoretical exercise makes that connection concrete by asking you to derive the precise asymptotic relationship between the rate of singular value decay and the approximation error, building a core intuition for when and why low-rank methods succeed .",
            "id": "3557759",
            "problem": "Let $A$ be the infinite real diagonal matrix (equivalently, a compact operator on the separable Hilbert space $\\ell^{2}$) defined by $A = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots)$ with singular values $\\sigma_{i} = i^{-\\alpha}$ for all integers $i \\geq 1$ and a fixed parameter $\\alpha  0$. Let $A_{k}$ denote the best rank-$k$ approximation to $A$ obtained by truncating the Singular Value Decomposition (SVD) of $A$ to its largest $k$ singular values, and let $\\|\\cdot\\|_{F}$ denote the Frobenius norm (also known in the infinite-dimensional setting as the Hilbert–Schmidt norm).\n\nStarting from the definitions of the Frobenius norm and the Singular Value Decomposition (SVD), and using only foundational facts such as orthogonal invariance of the Frobenius norm and monotone integral comparison for series, derive the precise leading-order asymptotic of the Frobenius-norm error $E_{k}(\\alpha) = \\|A - A_{k}\\|_{F}$ as $k \\to \\infty$, and explain how the behavior depends on $\\alpha$. Your derivation must justify the constant and the exponent in the leading term.\n\nExpress your final answer as a single closed-form expression giving the leading-order term of $E_{k}(\\alpha)$ for the regime in which $E_{k}(\\alpha)$ is finite. No rounding is required.",
            "solution": "The problem asks for the leading-order asymptotic behavior of the Frobenius-norm error, $E_{k}(\\alpha) = \\|A - A_{k}\\|_{F}$, in approximating an infinite diagonal matrix $A$ with its best rank-$k$ approximation $A_k$.\n\nFirst, we establish the structure of the matrices $A$ and $A_k$. The given matrix is $A = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots)$, with singular values $\\sigma_{i} = i^{-\\alpha}$ for integers $i \\ge 1$ and a parameter $\\alpha  0$. Since the values $\\sigma_i$ are positive and sorted in descending order ($\\sigma_1  \\sigma_2  \\dots  0$), this diagonal representation is already the Singular Value Decomposition (SVD) of $A$. Specifically, if we write the SVD of $A$ as $A = U \\Sigma V^*$, then $U$ and $V$ are the identity operators on the Hilbert space $\\ell^2$, and $\\Sigma$ is the diagonal operator with entries $\\sigma_i$.\n\nThe Eckart-Young-Mirsky theorem, extended to compact operators on Hilbert spaces, states that the best rank-$k$ approximation to $A$ in any unitarily invariant norm, including the Frobenius norm, is found by truncating its SVD. This is achieved by keeping the $k$ largest singular values and setting the others to zero. Thus, the best rank-$k$ approximation $A_k$ is given by:\n$$\nA_k = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_k, 0, 0, \\dots)\n$$\nThe error matrix, $A - A_k$, is therefore:\n$$\nA - A_k = \\operatorname{diag}(0, \\dots, 0, \\sigma_{k+1}, \\sigma_{k+2}, \\dots)\n$$\nThis is a diagonal operator whose non-zero entries begin at the $(k+1)$-th position.\n\nNext, we compute the Frobenius norm of this error matrix. For a diagonal operator $D = \\operatorname{diag}(d_1, d_2, \\dots)$, the Frobenius norm (or Hilbert-Schmidt norm), denoted $\\|\\cdot\\|_F$, is defined by:\n$$\n\\|D\\|_{F} = \\sqrt{\\sum_{i=1}^{\\infty} |d_i|^2}\n$$\nApplying this definition to the error matrix $A - A_k$, we obtain the error $E_k(\\alpha)$:\n$$\nE_k(\\alpha)^2 = \\|A - A_k\\|_{F}^2 = \\sum_{i=k+1}^{\\infty} \\sigma_i^2\n$$\nSubstituting the given form of the singular values, $\\sigma_i = i^{-\\alpha}$:\n$$\nE_k(\\alpha)^2 = \\sum_{i=k+1}^{\\infty} (i^{-\\alpha})^2 = \\sum_{i=k+1}^{\\infty} i^{-2\\alpha}\n$$\nThe error $E_k(\\alpha)$ is the square root of this sum:\n$$\nE_k(\\alpha) = \\left( \\sum_{i=k+1}^{\\infty} i^{-2\\alpha} \\right)^{1/2}\n$$\nThis expression is finite if and only if the series converges. The series $\\sum_{i=1}^{\\infty} i^{-p}$ is a p-series, which converges if and only if $p  1$. In our case, the exponent is $p = 2\\alpha$. Therefore, the total error $\\|A\\|_F$ is finite, and consequently the tail sum $E_k(\\alpha)^2$ is meaningful for all $k$, if and only if $2\\alpha  1$, which means $\\alpha  \\frac{1}{2}$. The problem asks for the asymptotic behavior in the regime where $E_k(\\alpha)$ is finite, so we proceed under the assumption that $\\alpha  \\frac{1}{2}$.\n\nTo find the leading-order asymptotic behavior of $E_k(\\alpha)$ as $k \\to \\infty$, we need to approximate the sum $S_k(\\alpha) = \\sum_{i=k+1}^{\\infty} i^{-2\\alpha}$. We use the method of integral comparison. The function $f(x) = x^{-2\\alpha}$ is a positive, continuous, and strictly decreasing function for $x \\ge 1$. For any integer $i \\ge 1$, we can bound the function's value over an interval:\n$$\n\\int_{i}^{i+1} x^{-2\\alpha} dx  f(i) = i^{-2\\alpha}  \\int_{i-1}^{i} x^{-2\\alpha} dx\n$$\nSumming these inequalities from $i = k+1$ to $\\infty$, we obtain bounds for our series $S_k(\\alpha)$:\n$$\n\\int_{k+1}^{\\infty} x^{-2\\alpha} dx  \\sum_{i=k+1}^{\\infty} i^{-2\\alpha}  \\int_{k}^{\\infty} x^{-2\\alpha} dx\n$$\nWe now evaluate the integral. Since we assumed $2\\alpha  1$, the integral converges:\n$$\n\\int x^{-2\\alpha} dx = \\frac{x^{-2\\alpha+1}}{-2\\alpha+1} + C\n$$\n$$\n\\int_{a}^{\\infty} x^{-2\\alpha} dx = \\left[ \\frac{x^{1-2\\alpha}}{1-2\\alpha} \\right]_{a}^{\\infty} = 0 - \\frac{a^{1-2\\alpha}}{1-2\\alpha} = \\frac{a^{1-2\\alpha}}{2\\alpha-1}\n$$\nSubstituting this result into our bounds for $S_k(\\alpha)$:\n$$\n\\frac{(k+1)^{1-2\\alpha}}{2\\alpha-1}  S_k(\\alpha)  \\frac{k^{1-2\\alpha}}{2\\alpha-1}\n$$\nFor large $k$, we have the asymptotic equivalence $(k+1)^{1-2\\alpha} \\sim k^{1-2\\alpha}$. More formally:\n$$\n(k+1)^{1-2\\alpha} = k^{1-2\\alpha} \\left(1 + \\frac{1}{k}\\right)^{1-2\\alpha} = k^{1-2\\alpha} \\left(1 + \\frac{1-2\\alpha}{k} + O(k^{-2})\\right)\n$$\nThus, both the lower and upper bounds are asymptotically equivalent to $\\frac{k^{1-2\\alpha}}{2\\alpha-1}$. By the squeeze theorem for asymptotic behavior, the leading-order term of the sum is:\n$$\nS_k(\\alpha) = \\sum_{i=k+1}^{\\infty} i^{-2\\alpha} \\sim \\frac{k^{1-2\\alpha}}{2\\alpha-1} \\quad \\text{as } k \\to \\infty\n$$\nThe notation $g(k) \\sim h(k)$ means $\\lim_{k\\to\\infty} g(k)/h(k) = 1$.\n\nNow, we can find the asymptotic behavior of the error $E_k(\\alpha) = \\sqrt{S_k(\\alpha)}$:\n$$\nE_k(\\alpha) \\sim \\left( \\frac{k^{1-2\\alpha}}{2\\alpha-1} \\right)^{1/2} = \\frac{(k^{1-2\\alpha})^{1/2}}{\\sqrt{2\\alpha-1}} = \\frac{k^{(1-2\\alpha)/2}}{\\sqrt{2\\alpha-1}}\n$$\nSimplifying the exponent gives:\n$$\nE_k(\\alpha) \\sim \\frac{1}{\\sqrt{2\\alpha-1}} k^{\\frac{1}{2} - \\alpha}\n$$\nThis expression gives the leading-order asymptotic of the error for $k \\to \\infty$, valid for $\\alpha  \\frac{1}{2}$. The constant is $\\frac{1}{\\sqrt{2\\alpha-1}}$ and the exponent on $k$ is $\\frac{1}{2} - \\alpha$.\n\nThe behavior of the approximation error depends on $\\alpha$:\n- The analysis is valid only for $\\alpha  \\frac{1}{2}$, as for $\\alpha \\le \\frac{1}{2}$ the operator $A$ is not in the Hilbert-Schmidt class (i.e., $\\|A\\|_F = \\infty$).\n- The exponent of $k$ is $\\frac{1}{2} - \\alpha$, which is negative for $\\alpha  \\frac{1}{2}$. This confirms that the error $E_k(\\alpha)$ goes to $0$ as $k \\to \\infty$.\n- As $\\alpha$ increases, the singular values decay more rapidly. The exponent $\\frac{1}{2} - \\alpha$ becomes more negative, indicating a faster rate of convergence of the rank-$k$ approximations.\n- As $\\alpha$ approaches the critical value of $\\frac{1}{2}$ from above, the constant term $\\frac{1}{\\sqrt{2\\alpha-1}}$ diverges to infinity, and the convergence rate $k^{\\frac{1}{2} - \\alpha}$ becomes very slow (the exponent approaches $0$). This signifies the breakdown of good approximability near the boundary of the Hilbert-Schmidt class.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2\\alpha-1}} k^{\\frac{1}{2} - \\alpha}}\n$$"
        }
    ]
}