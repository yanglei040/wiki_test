{
    "hands_on_practices": [
        {
            "introduction": "A foundational skill in applying Principal Component Analysis (PCA) is projecting new, out-of-sample data onto the principal subspace learned from a training set. This exercise grounds your understanding in these core mechanics, from deriving the principal loading vectors via the SVD to performing the projection. It critically highlights the importance of consistent data preprocessing, demonstrating how incorrect mean-centering can lead to significant errors in reconstruction .",
            "id": "3566943",
            "problem": "Let $X \\in \\mathbb{R}^{3 \\times 2}$ be a training data matrix with rows\n$$\n\\begin{pmatrix}\n\\frac{5}{3} & -\\frac{4}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n-\\frac{1}{3} & \\frac{2}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n\\frac{5}{3} & \\frac{2}{3}\n\\end{pmatrix}.\n$$\nLet the preprocessing step for Principal Component Analysis (PCA) be column-wise centering by the training mean vector $\\mu \\in \\mathbb{R}^{2}$, and let PCA be carried out via the Singular Value Decomposition (SVD). A new out-of-sample vector is $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}$. Using the one-dimensional principal subspace spanned by the leading PCA loading vector, do the following:\n\n1. Derive, from first principles starting with the definition of the Singular Value Decomposition (SVD), how to obtain the leading loading vector from the centered training matrix, and compute it for the given $X$.\n2. Using the derivation of out-of-sample projection in PCA, compute the squared reconstruction error $\\,\\|y - \\hat{y}\\|^{2}\\,$ when $y$ is handled with the correct mean convention (center $y$ by the training mean $\\mu$ before projection, and de-center by $\\mu$ after reconstruction).\n3. Compute the squared reconstruction error when $y$ is incorrectly handled by centering with its own feature mean (i.e., subtracting $\\mu_{y}\\,\\mathbf{1}$, where $\\mu_{y}$ is the average of the entries of $y$ and $\\mathbf{1}$ is the all-ones vector), while still de-centering by the training mean $\\mu$ after reconstruction.\n4. Provide the difference between the two squared reconstruction errors (incorrect handling minus correct handling) as a single exact value. No rounding is required.\n\nProvide your final answer as a single exact value.",
            "solution": "The problem asks for a series of computations related to Principal Component Analysis (PCA) on a given dataset, focusing on the projection of an out-of-sample data point and the resulting reconstruction error under different handling conventions.\n\nFirst, the validity of the problem statement is established.\n**Step 1: Extract Givens**\n- Data matrix $X \\in \\mathbb{R}^{3 \\times 2}$ with rows:\n$$\n\\begin{pmatrix} \\frac{5}{3} & -\\frac{4}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} \\frac{5}{3} & \\frac{2}{3} \\end{pmatrix}\n$$\n- Preprocessing: Column-wise centering of the training data $X$ by its mean vector $\\mu \\in \\mathbb{R}^2$.\n- Method: PCA carried out via Singular Value Decomposition (SVD).\n- Out-of-sample vector: $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}$.\n- Task dimensions: Use the one-dimensional principal subspace spanned by the leading PCA loading vector.\n- Task 1: Derive how to obtain the leading loading vector from the SVD of the centered training matrix and compute it.\n- Task 2: Compute the squared reconstruction error $\\|y - \\hat{y}\\|^{2}$ with correct handling (center $y$ with $\\mu$, de-center with $\\mu$).\n- Task 3: Compute the squared reconstruction error with incorrect handling (center $y$ with its own feature mean $\\mu_y$, de-center with $\\mu$).\n- Task 4: Compute the difference between the squared errors from Task 3 and Task 2.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard application of PCA and SVD in numerical linear algebra and statistics. It is well-posed, with all necessary data and clear, objective instructions provided. There are no contradictions, ambiguities, or unrealistic conditions. The problem is a formalizable and solvable exercise testing the understanding of PCA mechanics.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nLet the training data matrix be $X \\in \\mathbb{R}^{m \\times n}$, where $m=3$ is the number of data points and $n=2$ is the number of features.\n$$\nX = \\begin{pmatrix}\n\\frac{5}{3} & -\\frac{4}{3} \\\\\n-\\frac{1}{3} & \\frac{2}{3} \\\\\n\\frac{5}{3} & \\frac{2}{3}\n\\end{pmatrix}\n$$\n\nThe first step in PCA is to center the data by subtracting the mean of each feature. The mean vector $\\mu$ is:\n$$\n\\mu = \\frac{1}{3} \\begin{pmatrix} \\frac{5}{3} - \\frac{1}{3} + \\frac{5}{3} \\\\ -\\frac{4}{3} + \\frac{2}{3} + \\frac{2}{3} \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} \\frac{9}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe centered data matrix $B$ is obtained by subtracting $\\mu^T$ from each row of $X$:\n$$\nB = X - \\mathbf{1}\\mu^T = \\begin{pmatrix}\n\\frac{5}{3}-1 & -\\frac{4}{3}-0 \\\\\n-\\frac{1}{3}-1 & \\frac{2}{3}-0 \\\\\n\\frac{5}{3}-1 & \\frac{2}{3}-0\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{2}{3} & -\\frac{4}{3} \\\\\n-\\frac{4}{3} & \\frac{2}{3} \\\\\n\\frac{2}{3} & \\frac{2}{3}\n\\end{pmatrix}\n$$\n\n**1. Derivation and Computation of the Leading Loading Vector**\n\nThe principal component loading vectors are the orthonormal eigenvectors of the sample covariance matrix $C = \\frac{1}{m-1}B^T B$. We need to show how these are obtained from the SVD of $B$.\n\nLet the SVD of the centered matrix $B$ be $B = U\\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix of singular values $\\sigma_i$. The columns of $V$ are the right-singular vectors.\n\nConsider the matrix $B^T B$:\n$$B^T B = (U\\Sigma V^T)^T (U\\Sigma V^T) = V\\Sigma^T U^T U \\Sigma V^T$$\nSince $U$ is orthogonal, $U^T U = I_m$.\n$$B^T B = V\\Sigma^T I_m \\Sigma V^T = V(\\Sigma^T \\Sigma)V^T$$\nThe matrix $\\Sigma^T \\Sigma$ is an $n \\times n$ diagonal matrix with entries $\\sigma_i^2$. This equation shows that $V$ diagonalizes $B^T B$, which means the columns of $V$ are the eigenvectors of $B^T B$, and the diagonal entries of $\\Sigma^T \\Sigma$ (the squared singular values) are the corresponding eigenvalues.\nSince $C = \\frac{1}{m-1}B^T B$, $C$ and $B^T B$ share the same eigenvectors. Therefore, the PCA loading vectors are the columns of $V$, i.e., the right-singular vectors of the centered data matrix $B$. The leading loading vector, $v_1$, corresponds to the largest singular value $\\sigma_1$.\n\nTo compute $v_1$, we find the leading eigenvector of $B^T B$:\n$$\nB^T B = \\begin{pmatrix}\n\\frac{2}{3} & -\\frac{4}{3} & \\frac{2}{3} \\\\\n-\\frac{4}{3} & \\frac{2}{3} & \\frac{2}{3}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{2}{3} & -\\frac{4}{3} \\\\\n-\\frac{4}{3} & \\frac{2}{3} \\\\\n\\frac{2}{3} & \\frac{2}{3}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{4+16+4}{9} & \\frac{-8-8+4}{9} \\\\\n\\frac{-8-8+4}{9} & \\frac{16+4+4}{9}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{24}{9} & -\\frac{12}{9} \\\\\n-\\frac{12}{9} & \\frac{24}{9}\n\\end{pmatrix} = \\frac{4}{3}\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $B^T B$ are found from $\\det(B^T B - \\lambda I) = 0$:\n$$ (\\frac{8}{3}-\\lambda)^2 - (-\\frac{4}{3})^2 = 0 \\implies (\\frac{8}{3}-\\lambda)^2 = \\frac{16}{9} \\implies \\frac{8}{3}-\\lambda = \\pm \\frac{4}{3} $$\nThis gives eigenvalues $\\lambda_1 = \\frac{8}{3} + \\frac{4}{3} = \\frac{12}{3} = 4$ and $\\lambda_2 = \\frac{8}{3} - \\frac{4}{3} = \\frac{4}{3}$.\nThe leading loading vector $v_1$ is the eigenvector corresponding to the largest eigenvalue $\\lambda_1=4$:\n$$ (B^T B - 4I)v_1 = 0 \\implies \\begin{pmatrix} \\frac{8}{3}-4 & -\\frac{4}{3} \\\\ -\\frac{4}{3} & \\frac{8}{3}-4 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} -\\frac{4}{3} & -\\frac{4}{3} \\\\ -\\frac{4}{3} & -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the equation $-\\frac{4}{3}x - \\frac{4}{3}y = 0$, or $x+y=0$. The eigenvector is in the direction $\\begin{pmatrix} 1 & -1 \\end{pmatrix}^T$. Normalizing it to unit length:\n$$ v_1 = \\frac{1}{\\sqrt{1^2+(-1)^2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n\n**2. Squared Reconstruction Error (Correct Handling)**\n\nThe out-of-sample vector is $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}^T$.\nFirst, center $y$ using the training mean $\\mu$:\n$$ y_c = y - \\mu = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $$\nProject the centered vector $y_c$ onto the one-dimensional principal subspace spanned by $v_1$:\n$$ y_{c, \\text{proj}} = (y_c^T v_1) v_1 $$\nThe projection coefficient is:\n$$ y_c^T v_1 = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{2}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} $$\nThe projected centered vector is:\n$$ y_{c, \\text{proj}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\nReconstruct the vector $\\hat{y}$ by de-centering with $\\mu$:\n$$ \\hat{y} = y_{c, \\text{proj}} + \\mu = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\nThe squared reconstruction error is $\\|y - \\hat{y}\\|^2$:\n$$ \\|y - \\hat{y}\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\right\\|^2 = \\left(\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{9}{4} + \\frac{9}{4} = \\frac{18}{4} = \\frac{9}{2} $$\n\n**3. Squared Reconstruction Error (Incorrect Handling)**\n\nNow, center $y$ using the mean of its own entries. The vector is $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}^T$. The mean of its entries is $\\mu_y = \\frac{3+1}{2} = 2$.\nThe incorrectly centered vector $y'_c$ is:\n$$ y'_c = y - \\mu_y \\mathbf{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nProject $y'_c$ onto the subspace spanned by $v_1$:\n$$ y'_{c, \\text{proj}} = ((y'_c)^T v_1) v_1 $$\nThe projection coefficient is:\n$$ (y'_c)^T v_1 = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\nThe projected vector is:\n$$ y'_{c, \\text{proj}} = \\sqrt{2} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nNote that $y'_c$ was already in the direction of $v_1$, so the projection is itself.\nReconstruct the vector $\\hat{y}'$ by de-centering with the training mean $\\mu$:\n$$ \\hat{y}' = y'_{c, \\text{proj}} + \\mu = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\nThe squared reconstruction error is $\\|y - \\hat{y}'\\|^2$:\n$$ \\|y - \\hat{y}'\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\right\\|^2 = 1^2 + 2^2 = 1 + 4 = 5 $$\n\n**4. Difference in Squared Reconstruction Errors**\n\nThe difference is the squared error from incorrect handling minus the squared error from correct handling:\n$$ \\text{Difference} = 5 - \\frac{9}{2} = \\frac{10}{2} - \\frac{9}{2} = \\frac{1}{2} $$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Once the mechanics of PCA are understood, the central practical question becomes: how many principal components should be retained? This coding practice moves beyond basic application to the art of model order selection, a crucial step in any dimensionality reduction task. You will implement and compare three distinct strategies for choosing the rank $k$—an energy-based heuristic, a threshold from random matrix theory, and a method based on Generalized Cross-Validation—gaining a practical toolkit and appreciating the different theoretical motivations behind model selection .",
            "id": "3566986",
            "problem": "You are given the task of implementing component selection and model order determination for Principal Component Analysis (PCA) computed via the Singular Value Decomposition (SVD). Your program must operate purely on matrices and real numbers without any physical units. The underlying method must be derived from first principles in numerical linear algebra, specifically the Singular Value Decomposition and orthogonal projection properties.\n\nYou must implement the following, using only derivations from the foundational base:\n- Perform PCA by computing the Singular Value Decomposition of a column-centered data matrix.\n- Implement three model-order selection rules that decide the number of components to retain:\n  1. An energy-retention criterion based on a user-specified threshold.\n  2. A white-noise bulk-edge threshold derived from asymptotic random matrix theory for independent noise with specified standard deviation.\n  3. A criterion based on Generalized Cross-Validation (GCV) that penalizes model complexity via the effective degrees of freedom of a rank-constrained linear estimator.\n\nYour implementation must start from the following base:\n- The Singular Value Decomposition (SVD) of a real matrix and orthogonality properties.\n- The definition of the sample covariance matrix and its spectral decomposition.\n- The Frobenius norm and its invariance under orthogonal transformations.\n- Dimension counting for rank-constrained matrix manifolds and the concept of effective degrees of freedom for linear estimators.\n\nYou must not use or assume any pre-specified closed-form formulas for the target quantities in your program without derivation from the base principles above. Your program must implement the following for each test case:\n- Construct a synthetic data matrix $X \\in \\mathbb{R}^{m \\times n}$ with $m$ observations (rows) and $n$ variables (columns) as the sum of a low-rank signal matrix and an additive Gaussian noise matrix. To construct the signal, draw random Gaussian matrices and orthonormalize them to obtain orthonormal factors, then scale by specified singular values. The noise is independent and identically distributed with a specified standard deviation. Center the resulting data by subtracting the column means before any PCA computation.\n- Compute the Singular Value Decomposition of the centered data matrix and use the singular values to evaluate the following three component selection rules, all expressed in terms of the singular values of the centered matrix:\n  1. Energy-retention rule: choose the smallest integer $k \\ge 0$ such that the cumulative retained energy fraction meets or exceeds a given threshold $0 \\le \\tau \\le 1$. If the total energy is exactly $0$, define the selected $k$ as $0$.\n  2. White-noise bulk-edge threshold rule: assume the additive noise is independent, identically distributed Gaussian with known standard deviation $\\sigma$. Use an asymptotically justified bulk-edge threshold based on the aspect ratio and noise level to retain singular values that exceed this edge. The selected $k$ is the number of singular values that exceed this edge. This choice must be derived from the SVD properties and the invariance of the Frobenius norm under orthogonal transforms and standard asymptotic behavior of singular values in white-noise matrices.\n  3. Generalized Cross-Validation (GCV) rule: for each integer $k \\ge 0$ not exceeding the maximal identifiable rank after centering, define a GCV score that depends on the residual sum of squares of the rank-$k$ approximation and an effective degrees-of-freedom penalty that correctly counts the number of free parameters of a rank-$k$ matrix under orthogonal invariances. Select the $k$ that minimizes the GCV score, breaking ties by choosing the smallest $k$. Exclude any $k$ where the required denominator would be zero or negative.\n\nYour program must produce the results for a small test suite of parameter sets covering a general case, noise-only case, tied singular values case, a high-noise scenario, and a degenerate zero matrix case. For each test case, your program will return a list of three integers $[k_{\\text{energy}}, k_{\\text{bulk}}, k_{\\text{gcv}}]$, where the entries correspond to the three rules above.\n\nThe test suite consists of the following five cases, each specified by a tuple $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed})$, where:\n- $m$ is the number of rows (observations), an integer.\n- $n$ is the number of columns (variables), an integer.\n- $\\text{singular\\_values}$ is a list of nonnegative real numbers specifying the nonzero singular values of the signal component in nonincreasing order.\n- $\\sigma$ is the standard deviation of the additive Gaussian noise, a nonnegative real number.\n- $\\tau$ is the target cumulative energy fraction in $[0,1]$ for the energy-retention rule.\n- $\\text{seed}$ is a nonnegative integer for pseudorandom number generation to ensure reproducibility.\n\nUse the following test suite:\n- Case $1$: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,60,\\,20,\\,[\\,12,\\,8\\,],\\,1.0,\\,0.9,\\,12345\\,)$.\n- Case $2$: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,60,\\,20,\\,[\\,\\,],\\,1.0,\\,0.9,\\,54321\\,)$.\n- Case $3$: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,50,\\,50,\\,[\\,10,\\,10,\\,10\\,],\\,0.5,\\,0.8,\\,2024\\,)$.\n- Case $4$: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,40,\\,10,\\,[\\,9,\\,7,\\,5,\\,3,\\,1\\,],\\,2.0,\\,0.95,\\,7\\,)$.\n- Case $5$: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,30,\\,15,\\,[\\,\\,],\\,0.0,\\,0.8,\\,4242\\,)$.\n\nYour program must output a single line containing the results for all test cases aggregated into a single list of lists. The required format is a single line with no spaces containing a valid Python-like literal: a list of the five lists in order, where the list for each case is $[k_{\\text{energy}},k_{\\text{bulk}},k_{\\text{gcv}}]$. For example, a valid output has the form \"[[a,b,c],[d,e,f],[g,h,i],[j,k,l],[p,q,r]]\" with integers in place of the letters.\n\nNo external input is allowed. All randomness must be seeded exactly as specified. All singular value computations must be performed on the centered data matrix as described. If centering results in zero total energy, then for the energy-retention rule return $k_{\\text{energy}} = 0$. Ensure the Generalized Cross-Validation rule excludes any $k$ that leads to an undefined denominator.\n\nYour program must implement and report the results for the five specified cases in the required single-line format. The answers must be integers only. The angle unit is not applicable. No physical units are used anywhere in this problem.",
            "solution": "The posed problem is valid. It is a well-defined task in numerical linear algebra and computational statistics, grounded in established scientific principles such as Principal Component Analysis (PCA), Singular Value Decomposition (SVD), random matrix theory, and cross-validation. The problem provides a complete and consistent setup, including all necessary parameters and reproducible synthetic data generation protocols, to implement and test three distinct rules for model order selection in PCA.\n\nThe core of the problem is to determine the optimal number of principal components, denoted by an integer $k$, to retain for representing a data matrix. This is a classic bias-variance trade-off: a small $k$ may lead to a model that is too simple and underfits the data (high bias), while a large $k$ may lead to a model that overfits the noise in the data (high variance). The solution requires deriving and implementing three distinct criteria for selecting $k$ based on the singular values of the column-centered data matrix.\n\nLet the given data matrix be $X \\in \\mathbb{R}^{m \\times n}$, with $m$ observations (rows) and $n$ variables (columns).\n\nFirst, the data must be centered by subtracting the mean of each column. Let $\\boldsymbol{\\mu} \\in \\mathbb{R}^{1 \\times n}$ be the row vector of column means. The centered data matrix $\\bar{X}$ is given by:\n$$ \\bar{X} = X - \\mathbf{1}_m \\boldsymbol{\\mu} $$\nwhere $\\mathbf{1}_m$ is an $m \\times 1$ column vector of ones. The columns of $\\bar{X}$ sum to zero, which implies that the data lies in a subspace of dimension at most $m-1$. Consequently, the rank of $\\bar{X}$ is at most $\\min(m-1, n)$.\n\nPCA is performed via the Singular Value Decomposition (SVD) of the centered matrix $\\bar{X}$. The SVD of $\\bar{X}$ is:\n$$ \\bar{X} = U \\Sigma V^T $$\nHere, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values $s_1 \\ge s_2 \\ge \\dots \\ge s_r > 0$, where $r = \\text{rank}(\\bar{X})$. The columns of $V$ are the principal directions (eigenvectors of the covariance matrix $\\frac{1}{m-1}\\bar{X}^T\\bar{X}$), and the squared singular values $s_i^2$ are proportional to the corresponding eigenvalues, representing the variance captured by each principal component.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$k$ approximation of $\\bar{X}$ in the Frobenius norm is $\\bar{X}_k = U_k \\Sigma_k V_k^T$, where $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ contains the top $k$ singular values. The problem is to select an optimal $k$.\n\n**1. Energy-Retention Rule**\nThis rule is based on retaining a certain fraction $\\tau$ of the total \"energy\" or variance of the data. The total energy is the squared Frobenius norm of the centered data matrix, $\\|\\bar{X}\\|_F^2$. Due to the orthogonality of $U$ and $V$, this is equal to the sum of the squared singular values:\n$$ \\|\\bar{X}\\|_F^2 = \\text{tr}(\\bar{X}^T \\bar{X}) = \\text{tr}(\\Sigma^T \\Sigma) = \\sum_{i=1}^r s_i^2 $$\nThe energy captured by the rank-$k$ approximation $\\bar{X}_k$ is similarly $\\sum_{i=1}^k s_i^2$. The rule is to choose the smallest integer $k \\ge 0$ such that the fraction of retained energy meets or exceeds a threshold $\\tau \\in [0,1]$:\n$$ \\frac{\\sum_{i=1}^k s_i^2}{\\sum_{i=1}^r s_i^2} \\ge \\tau $$\nIf the total energy is zero (i.e., $\\bar{X}$ is the zero matrix), $k$ is defined as $0$.\n\n**2. White-Noise Bulk-Edge Threshold Rule**\nThis rule assumes the data consists of a low-rank signal plus additive i.i.d. Gaussian noise with a known standard deviation $\\sigma$. The singular values corresponding to noise are expected to fall within a predictable range, or \"bulk\". Signal components should produce singular values that protrude beyond the edge of this noise bulk. Asymptotic random matrix theory provides an estimate for this edge. For an $M \\times N$ random matrix with i.i.d. entries of variance $\\sigma^2$, the largest singular value asymptotically approaches $\\sigma(\\sqrt{M} + \\sqrt{N})$. Since we are analyzing the centered matrix $\\bar{X}$, its noise component behaves like that of an $(m-1) \\times n$ random matrix. Thus, a well-justified threshold for separating signal from noise is:\n$$ \\theta_{\\text{bulk}} = \\sigma (\\sqrt{m-1} + \\sqrt{n}) $$\nThe number of components to retain, $k_{\\text{bulk}}$, is the count of singular values $s_i$ of $\\bar{X}$ that are strictly greater than this threshold.\n\n**3. Generalized Cross-Validation (GCV) Rule**\nGCV provides a data-driven method for model selection by approximating leave-one-out cross-validation error. It balances the goodness of fit with model complexity. The GCV score for a rank-$k$ approximation is formulated as:\n$$ GCV(k) = \\frac{\\text{RSS}(k)}{(\\text{Effective number of observations} - \\text{Effective degrees of freedom of model})^2} $$\nThe numerator, $\\text{RSS}(k)$, is the residual sum of squares:\n$$ \\text{RSS}(k) = \\|\\bar{X} - \\bar{X}_k\\|_F^2 = \\sum_{i=k+1}^r s_i^2 $$\nThe denominator penalizes complex models. The \"effective number of observations\" in the $m \\times n$ matrix is $D = mn$. The effective degrees of freedom, $\\text{df}(k)$, for a rank-$k$ model is derived from counting the number of free parameters required to specify a rank-$k$ matrix, which is a manifold of dimension $k(m+n-k)$. This is the principled parameter count indicated by the problem statement. The GCV score to be minimized is therefore:\n$$ GCV(k) = \\frac{\\sum_{i=k+1}^r s_i^2}{(mn - k(m+n-k))^2} $$\nWe must select the integer $k$ in the valid range that minimizes this score. The valid range for $k$ excludes values where the denominator is zero or negative. A tie in scores is broken by choosing the smallest $k$. The range of $k$ to test is from $0$ up to $\\min(m, n)$, excluding any $k$ that invalidates the denominator.\n\nThe implementation will construct the synthetic data, apply centering, perform SVD, and compute $k$ according to each of these three rules for every test case specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for PCA model order selection and print results.\n    \"\"\"\n    # Test suite specified by (m, n, singular_values, sigma, tau, seed)\n    test_cases = [\n        (60, 20, [12, 8], 1.0, 0.9, 12345),\n        (60, 20, [], 1.0, 0.9, 54321),\n        (50, 50, [10, 10, 10], 0.5, 0.8, 2024),\n        (40, 10, [9, 7, 5, 3, 1], 2.0, 0.95, 7),\n        (30, 15, [], 0.0, 0.8, 4242)\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = do_pca_selection(*case_params)\n        results.append(result)\n\n    # The required output is a single-line Python-like literal list of lists with no spaces.\n    # The default str() adds spaces, so we replace them.\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\ndef do_pca_selection(m, n, signal_sv, sigma, tau, seed):\n    \"\"\"\n    Implements PCA model order selection for a single test case.\n    \n    Args:\n        m (int): Number of rows (observations).\n        n (int): Number of columns (variables).\n        signal_sv (list[float]): Non-increasing list of singular values for the signal.\n        sigma (float): Standard deviation of the additive Gaussian noise.\n        tau (float): Target cumulative energy fraction for the energy-retention rule.\n        seed (int): Seed for the pseudorandom number generator.\n        \n    Returns:\n        list[int]: A list of three integers [k_energy, k_bulk, k_gcv].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(signal_sv)\n\n    # 1. Construct the synthetic data matrix X = Signal + Noise\n    signal_matrix = np.zeros((m, n))\n    if d > 0:\n        # Generate random orthonormal factor matrices U_true (m x d) and V_true (n x d)\n        # by performing QR decomposition on random Gaussian matrices.\n        U_true_rand = rng.standard_normal(size=(m, d))\n        U_true, _ = np.linalg.qr(U_true_rand)\n        \n        V_true_rand = rng.standard_normal(size=(n, d))\n        V_true, _ = np.linalg.qr(V_true_rand)\n        \n        signal_matrix = U_true @ np.diag(signal_sv) @ V_true.T\n\n    noise_matrix = rng.normal(loc=0.0, scale=sigma, size=(m, n))\n    X = signal_matrix + noise_matrix\n\n    # 2. Center the data matrix by subtracting column means\n    X_bar = X - X.mean(axis=0, keepdims=True)\n\n    # 3. Compute the Singular Value Decomposition of the centered matrix\n    s = np.linalg.svd(X_bar, compute_uv=False)\n    s_sq = s**2\n    num_sv = len(s)\n\n    # --- Rule 1: Energy-retention rule ---\n    k_energy = 0\n    total_energy = np.sum(s_sq)\n    if total_energy > 1e-15:  # Use a small tolerance for floating-point comparison\n        cumulative_energy = np.cumsum(s_sq)\n        energy_fraction = cumulative_energy / total_energy\n        # Find the smallest k >= 0 (corresponds to index + 1) such that criterion is met.\n        k_candidates = np.where(energy_fraction >= tau)[0]\n        if k_candidates.size > 0:\n            k_energy = k_candidates[0] + 1\n        else:\n            # If tau=1.0 and numerical precision prevents ratio from reaching 1, retain all components.\n            k_energy = num_sv\n\n    # --- Rule 2: White-noise bulk-edge threshold rule ---\n    # The threshold is based on the largest singular value of a centered noise matrix,\n    # whose properties are approximated by an (m-1) x n random matrix.\n    if m = 1:\n        threshold_bulk = 0.0  # Avoid sqrt of negative or zero value if m = 1\n    else:\n        threshold_bulk = sigma * (np.sqrt(m - 1) + np.sqrt(n))\n    k_bulk = np.sum(s > threshold_bulk)\n\n    # --- Rule 3: Generalized Cross-Validation (GCV) rule ---\n    k_gcv = 0\n    min_gcv_score = np.inf\n    \n    # We test k from 0 up to min(m, n).\n    max_k_to_test = min(m, n)\n    rss_total = total_energy\n    \n    rss_cumulative = np.cumsum(s_sq)\n\n    for k in range(max_k_to_test + 1):\n        # Denominator term is (D - df(k)) where D=mn, df(k)=k(m+n-k)\n        den_term = m * n - k * (m + n - k)\n        \n        if den_term = 0:\n            continue\n\n        # Residual Sum of Squares (RSS) for rank-k approximation\n        if k == 0:\n            rss_k = rss_total\n        elif k > num_sv:\n            rss_k = 0.0\n        else:\n            rss_k = rss_total - rss_cumulative[k-1]\n\n        gcv_score = rss_k / (den_term**2)\n\n        # Select k that minimizes GCV. Ties are broken by choosing the smallest k\n        # due to the strict less-than comparison.\n        if gcv_score  min_gcv_score:\n            min_gcv_score = gcv_score\n            k_gcv = k\n    \n    return [int(k_energy), int(k_bulk), int(k_gcv)]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Why can a simple \"elbow\" in a scree plot be a misleading guide for choosing the PCA rank? This advanced theoretical exercise takes you under the hood of model selection to answer that question, providing a rigorous justification for the cross-validation methods explored previously . By deriving an analytical expression for the expected out-of-sample reconstruction error, you will formalize the trade-off between capturing true signal and overfitting to sample-specific noise, revealing the nuanced relationship between a model's in-sample performance and its ability to generalize .",
            "id": "3566961",
            "problem": "Consider a mean-zero data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose rows are independent and identically distributed realizations of a random vector $x \\in \\mathbb{R}^{p}$ with population covariance\n$$\n\\Sigma_{x} \\;=\\; \\sum_{j=1}^{k} \\lambda_{j}\\, u_{j} u_{j}^{\\top} \\;+\\; \\sigma^{2} I_{p},\n$$\nwhere $k \\leq p$, the vectors $u_{1},\\dots,u_{k} \\in \\mathbb{R}^{p}$ are orthonormal, $\\lambda_{1} \\geq \\cdots \\geq \\lambda_{k}  0$ are fixed spike strengths, and $\\sigma^{2}  0$ is the isotropic noise variance. Split the $n$ rows into a training set of size $n_{\\mathrm{tr}}$ and a held-out set of size $n_{\\mathrm{ho}}$ with $n_{\\mathrm{tr}} + n_{\\mathrm{ho}} = n$. Form the singular value decomposition (SVD) of the centered training matrix $X_{\\mathrm{tr}} \\in \\mathbb{R}^{n_{\\mathrm{tr}} \\times p}$ as\n$$\nX_{\\mathrm{tr}} \\;=\\; U \\Sigma V^{\\top},\n$$\nand fix a target rank $r \\in \\{0,1,\\dots,\\min(n_{\\mathrm{tr}},p)\\}$. Let $V_{r} \\in \\mathbb{R}^{p \\times r}$ be the first $r$ right singular vectors and define the corresponding orthogonal projector onto the estimated principal subspace by $\\widehat{P}_{r} \\stackrel{\\mathrm{def}}{=} V_{r} V_{r}^{\\top}$. For a held-out row $x_{\\ast} \\in \\mathbb{R}^{p}$, define its rank-$r$ reconstruction by $\\widehat{x}_{\\ast}^{(r)} \\stackrel{\\mathrm{def}}{=} x_{\\ast} \\widehat{P}_{r}$, and define the per-row held-out reconstruction error\n$$\nL_{r} \\;\\stackrel{\\mathrm{def}}{=}\\; \\mathbb{E}\\!\\left[\\,\\|\\,x_{\\ast} - \\widehat{x}_{\\ast}^{(r)}\\,\\|_{2}^{2}\\,\\right],\n$$\nwhere the expectation is taken jointly over the randomness in the training sample (which determines $\\widehat{P}_{r}$) and the held-out row $x_{\\ast}$.\n\nStarting only from the definitions of the singular value decomposition, the properties of orthogonal projectors, and the identity that for any mean-zero random vector $z$ with covariance $\\Sigma$ and any fixed matrix $A$ one has $\\mathbb{E}\\!\\left[\\|Az\\|_{2}^{2}\\right] = \\operatorname{tr}\\!\\left(A \\Sigma A^{\\top}\\right)$, derive a closed-form analytic expression for $L_{r}$ in terms of the spike strengths $\\lambda_{1},\\dots,\\lambda_{k}$, the noise variance $\\sigma^{2}$, the ambient dimension $p$, the rank $r$, and the training-sample alignment coefficients\n$$\n\\alpha_{j} \\;\\stackrel{\\mathrm{def}}{=}\\; \\mathbb{E}\\!\\left[\\,\\|\\,\\widehat{P}_{r} u_{j}\\,\\|_{2}^{2}\\,\\right], \\qquad j \\in \\{1,\\dots,k\\},\n$$\nwhere the expectation is over the randomness in the training sample only. You may assume that $\\operatorname{tr}(\\widehat{P}_{r}) = r$ almost surely.\n\nIn your written derivation, explain the roles of the projection identity and the trace-of-quadratic-form identity. Then, using your derived expression, briefly justify in words when a naive scree plot of training singular values can be misleading due to sampling noise, relative to the rank that minimizes $L_{r}$. However, for the purposes of grading, provide only the final analytic expression for $L_{r}$ as your answer. No numerical evaluation is required, and no rounding is needed. Express your final answer as a single closed-form expression in the specified symbols.",
            "solution": "The problem is well-posed, scientifically grounded, and self-contained. We proceed with the derivation of the per-row held-out reconstruction error, $L_{r}$.\n\nThe quantity to be computed is\n$$\nL_{r} \\;\\stackrel{\\mathrm{def}}{=}\\; \\mathbb{E}\\!\\left[\\,\\|\\,x_{\\ast} - \\widehat{x}_{\\ast}^{(r)}\\,\\|_{2}^{2}\\,\\right]\n$$\nwhere the expectation is taken over both the training set, which determines the random projector $\\widehat{P}_{r}$, and the held-out sample $x_{\\ast}$. We are given that $\\widehat{x}_{\\ast}^{(r)} = x_{\\ast} \\widehat{P}_{r}$. Substituting this into the definition of $L_r$ gives:\n$$\nL_{r} = \\mathbb{E}\\!\\left[\\,\\|\\,x_{\\ast} - x_{\\ast} \\widehat{P}_{r}\\,\\|_{2}^{2}\\,\\right] = \\mathbb{E}\\!\\left[\\,\\|\\,x_{\\ast} (I_{p} - \\widehat{P}_{r})\\,\\|_{2}^{2}\\,\\right]\n$$\nThe projector $\\widehat{P}_{r} = V_r V_r^\\top$ is an orthogonal projector, and thus so is $I_p - \\widehat{P}_r$. An orthogonal projector is both symmetric, $(I_p - \\widehat{P}_r)^\\top = (I_p - \\widehat{P}_r)$, and idempotent, $(I_p - \\widehat{P}_r)^2 = (I_p - \\widehat{P}_r)$. The term $\\|x_{\\ast} (I_{p} - \\widehat{P}_{r})\\|_{2}^{2}$ represents the squared Euclidean norm of the component of the vector $x_\\ast$ that is orthogonal to the subspace spanned by the first $r$ principal components estimated from the training data. This is the **projection identity** at work: the reconstruction error is the norm of the vector projected onto the orthogonal complement of the signal subspace.\n\nTo handle the joint expectation, we use the law of total expectation, conditioning on the training data (and thus on $\\widehat{P}_{r}$):\n$$\nL_{r} = \\mathbb{E}_{\\widehat{P}_{r}} \\!\\left[ \\mathbb{E}_{x_{\\ast}} \\!\\left[ \\,\\|\\,x_{\\ast} (I_{p} - \\widehat{P}_{r})\\,\\|_{2}^{2} \\, \\middle| \\, \\widehat{P}_{r} \\right] \\right]\n$$\nLet us analyze the inner expectation. Let $z = x_{\\ast}^{\\top}$ be the column vector representation of the held-out sample. The random vector $z$ has mean zero, $\\mathbb{E}[z] = 0$, and covariance matrix $\\mathbb{E}[zz^\\top] = \\Sigma_x$. The norm can be rewritten as $\\|z^\\top(I_p - \\widehat{P}_r)\\|_2^2 = \\|(I_p - \\widehat{P}_r)^\\top z\\|_2^2 = \\|(I_p - \\widehat{P}_r)z\\|_2^2$ since $I_p - \\widehat{P}_r$ is symmetric.\n\nAt this point, we employ the **trace-of-quadratic-form identity** provided: for a mean-zero random vector $z$ with covariance $\\Sigma$ and a fixed matrix $A$, $\\mathbb{E}[\\|Az\\|_2^2] = \\operatorname{tr}(A\\Sigma A^\\top)$. In our case, for a fixed $\\widehat{P}_r$, the matrix is $A = I_p - \\widehat{P}_r$ and the covariance is $\\Sigma = \\Sigma_x$. This identity is the critical step that allows us to replace the expectation over the held-out data $x_\\ast$ with a trace operation involving its population covariance $\\Sigma_x$.\nThe inner conditional expectation becomes:\n$$\n\\mathbb{E}_{x_{\\ast}} \\!\\left[ \\,\\|\\,(I_p - \\widehat{P}_r)x_{\\ast}^{\\top}\\,\\|_{2}^{2} \\, \\middle| \\, \\widehat{P}_{r} \\right] = \\operatorname{tr}\\!\\left( (I_p - \\widehat{P}_r) \\Sigma_x (I_p - \\widehat{P}_r)^{\\top} \\right)\n$$\nUsing the symmetry and idempotency of the projector $I_p - \\widehat{P}_r$, along with the cyclic property of the trace ($\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$):\n$$\n\\operatorname{tr}\\!\\left( (I_p - \\widehat{P}_r) \\Sigma_x (I_p - \\widehat{P}_r) \\right) = \\operatorname{tr}\\!\\left( \\Sigma_x (I_p - \\widehat{P}_r)^2 \\right) = \\operatorname{tr}\\!\\left( \\Sigma_x (I_p - \\widehat{P}_r) \\right)\n$$\nNow, we can take the outer expectation over the randomness in the training data, which determines $\\widehat{P}_r$:\n$$\nL_{r} = \\mathbb{E}_{\\widehat{P}_{r}} \\!\\left[ \\operatorname{tr}\\!\\left( \\Sigma_x (I_p - \\widehat{P}_r) \\right) \\right]\n$$\nSince both trace and expectation are linear operators, they can be commuted:\n$$\nL_{r} = \\operatorname{tr}\\!\\left( \\mathbb{E}_{\\widehat{P}_{r}} \\!\\left[ \\Sigma_x (I_p - \\widehat{P}_r) \\right] \\right) = \\operatorname{tr}\\!\\left( \\Sigma_x (I_p - \\mathbb{E}[\\widehat{P}_r]) \\right)\n$$\nUsing the linearity of the trace, we expand this expression:\n$$\nL_{r} = \\operatorname{tr}(\\Sigma_x) - \\operatorname{tr}(\\Sigma_x \\mathbb{E}[\\widehat{P}_r])\n$$\nWe now evaluate each term separately using the given population covariance $\\Sigma_x = \\sum_{j=1}^{k} \\lambda_{j}\\, u_{j} u_{j}^{\\top} + \\sigma^{2} I_{p}$.\nFirst, $\\operatorname{tr}(\\Sigma_x)$:\n$$\n\\operatorname{tr}(\\Sigma_x) = \\operatorname{tr}\\!\\left(\\sum_{j=1}^{k} \\lambda_{j}\\, u_{j} u_{j}^{\\top} + \\sigma^{2} I_{p}\\right) = \\sum_{j=1}^{k} \\lambda_j \\operatorname{tr}(u_j u_j^\\top) + \\sigma^2 \\operatorname{tr}(I_p)\n$$\nSince $\\operatorname{tr}(u_j u_j^\\top) = u_j^\\top u_j = \\|u_j\\|_2^2=1$ (as $u_j$ are orthonormal) and $\\operatorname{tr}(I_p)=p$, we get:\n$$\n\\operatorname{tr}(\\Sigma_x) = \\sum_{j=1}^{k} \\lambda_j + p\\sigma^2\n$$\nNext, we evaluate $\\operatorname{tr}(\\Sigma_x \\mathbb{E}[\\widehat{P}_r])$:\n$$\n\\operatorname{tr}(\\Sigma_x \\mathbb{E}[\\widehat{P}_r]) = \\operatorname{tr}\\!\\left( \\left(\\sum_{j=1}^{k} \\lambda_{j}\\, u_{j} u_{j}^{\\top} + \\sigma^{2} I_{p}\\right) \\mathbb{E}[\\widehat{P}_r] \\right)\n$$\n$$\n= \\sum_{j=1}^{k} \\lambda_j \\operatorname{tr}(u_j u_j^\\top \\mathbb{E}[\\widehat{P}_r]) + \\sigma^2 \\operatorname{tr}(\\mathbb{E}[\\widehat{P}_r])\n$$\nFor the first part, we again use the cyclic property of trace and commute with expectation:\n$$\n\\operatorname{tr}(u_j u_j^\\top \\mathbb{E}[\\widehat{P}_r]) = \\mathbb{E}[\\operatorname{tr}(u_j u_j^\\top \\widehat{P}_r)] = \\mathbb{E}[\\operatorname{tr}(\\widehat{P}_r u_j u_j^\\top)] = \\mathbb{E}[u_j^\\top \\widehat{P}_r u_j]\n$$\nSince $\\widehat{P}_r$ is a symmetric projector, $u_j^\\top \\widehat{P}_r u_j = u_j^\\top \\widehat{P}_r^\\top \\widehat{P}_r u_j = (\\widehat{P}_r u_j)^\\top (\\widehat{P}_r u_j) = \\|\\widehat{P}_r u_j\\|_2^2$.\nSo, $\\mathbb{E}[u_j^\\top \\widehat{P}_r u_j] = \\mathbb{E}[\\|\\widehat{P}_r u_j\\|_2^2]$, which is exactly the definition of the alignment coefficient $\\alpha_j$. Thus, the first part of the sum is $\\sum_{j=1}^{k} \\lambda_j \\alpha_j$.\nFor the second part, $\\sigma^2 \\operatorname{tr}(\\mathbb{E}[\\widehat{P}_r]) = \\sigma^2 \\mathbb{E}[\\operatorname{tr}(\\widehat{P}_r)]$. The problem states to assume $\\operatorname{tr}(\\widehat{P}_r) = r$ almost surely. Therefore, $\\mathbb{E}[\\operatorname{tr}(\\widehat{P}_r)] = r$. The second part is $\\sigma^2 r$.\nCombining these parts:\n$$\n\\operatorname{tr}(\\Sigma_x \\mathbb{E}[\\widehat{P}_r]) = \\sum_{j=1}^{k} \\lambda_j \\alpha_j + \\sigma^2 r\n$$\nFinally, we substitute these results back into the expression for $L_r$:\n$$\nL_{r} = \\left(\\sum_{j=1}^{k} \\lambda_j + p\\sigma^2\\right) - \\left(\\sum_{j=1}^{k} \\lambda_j \\alpha_j + r\\sigma^2\\right)\n$$\nThis simplifies to the final analytical expression for the held-out error:\n$$\nL_{r} = \\sum_{j=1}^{k} \\lambda_j (1 - \\alpha_j) + \\sigma^2 (p - r)\n$$\nThis expression separates the error into two components: the error due to imperfect alignment with the true signal directions (the sum involving $\\lambda_j$ and $\\alpha_j$), and the error due to projecting away the dimensions that contain isotropic noise (the term $\\sigma^2(p-r)$).\n\nA naive scree plot of the training data's singular values can be misleading because it reflects the in-sample variance explained by each component, not the out-of-sample prediction error $L_r$. The singular values of the training matrix $X_{\\mathrm{tr}}$ are noisy estimates of the population-level spiked eigenvalues. In finite samples, especially when the dimension $p$ is comparable to or larger than the sample size $n_{\\mathrm{tr}}$, sampling noise can significantly distort the estimated principal components. The largest sample singular values might not correspond to the strongest signals, and their directions $V_r$ may be poorly aligned with the true signal directions $u_j$, leading to small $\\alpha_j$. Picking a rank $r$ from an \"elbow\" in the scree plot simply chooses the components that explain the most variance *in the training set*. This can lead to overfitting, where one fits to spurious noise correlations present in the training data that do not generalize. As our formula for $L_r$ shows, increasing $r$ always decreases the $\\sigma^2(p-r)$ term, but it has a complex effect on the alignment term $\\sum \\lambda_j(1-\\alpha_j)$. Including a noisy component (overfitting) might not improve alignment with the true signals enough to offset the loss of $\\sigma^2$ from the second term, thus increasing the true held-out error $L_r$ even while the training error decreases.",
            "answer": "$$\n\\boxed{\\sum_{j=1}^{k} \\lambda_{j} (1 - \\alpha_{j}) + \\sigma^{2} (p - r)}\n$$"
        }
    ]
}