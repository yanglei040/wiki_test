{
    "hands_on_practices": [
        {
            "introduction": "The Singular Value Decomposition (SVD) provides the most stable and insightful pathway to computing the Moore-Penrose pseudoinverse. This first exercise grounds the theory in a concrete calculation, tasking you with constructing the pseudoinverse $A^+$ directly from the SVD components of a given matrix $A$. By working through this process, you will gain hands-on familiarity with the core formula $A^+ = V \\Sigma^+ U^T$ and see how the structure of the pseudoinverse arises from the singular values and vectors .",
            "id": "3592295",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 2}$ have the Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$ with\n$$\nU \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  0  1 \\\\\n-\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0\n\\end{bmatrix},\n\\qquad\n\\Sigma \\;=\\;\n\\begin{bmatrix}\n6  0 \\\\\n0  2 \\\\\n0  0\n\\end{bmatrix},\n\\qquad\nV \\;=\\;\n\\begin{bmatrix}\n\\frac{3}{5}  \\frac{4}{5} \\\\\n-\\frac{4}{5}  \\frac{3}{5}\n\\end{bmatrix}.\n$$\nUsing only the defining properties of the Moore–Penrose pseudoinverse and the definition of the Singular Value Decomposition (SVD), derive the Moore–Penrose pseudoinverse $A^{+}$ explicitly from the provided factors. Then, report the exact value of the $(2,3)$ entry of $A^{+}$ as a single closed-form expression. Do not use any result that is not a direct consequence of these definitions. No rounding is required.",
            "solution": "The problem is valid as it is self-contained, scientifically grounded in numerical linear algebra, and mathematically well-posed. A unique solution exists and can be derived from the provided information.\n\nThe task is to find the $(2,3)$ entry of the Moore-Penrose pseudoinverse $A^{+}$ of a matrix $A$, given its Singular Value Decomposition (SVD), $A = U \\Sigma V^{\\top}$. The derivation must adhere strictly to the defining properties of the pseudoinverse.\n\nThe Moore-Penrose pseudoinverse of a matrix $A$, denoted $A^{+}$, is the unique matrix that satisfies the following four Penrose conditions:\n1.  $A A^{+} A = A$\n2.  $A^{+} A A^{+} = A^{+}$\n3.  $(A A^{+})^{\\top} = A A^{+}$\n4.  $(A^{+} A)^{\\top} = A^{+} A$\n\nWe are given the SVD components of $A \\in \\mathbb{R}^{3 \\times 2}$:\n$$\nU \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  0  1 \\\\\n-\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0\n\\end{bmatrix},\n\\qquad\n\\Sigma \\;=\\;\n\\begin{bmatrix}\n6  0 \\\\\n0  2 \\\\\n0  0\n\\end{bmatrix},\n\\qquad\nV \\;=\\;\n\\begin{bmatrix}\n\\frac{3}{5}  \\frac{4}{5} \\\\\n-\\frac{4}{5}  \\frac{3}{5}\n\\end{bmatrix}.\n$$\nHere, $U \\in \\mathbb{R}^{3 \\times 3}$ and $V \\in \\mathbb{R}^{2 \\times 2}$ are orthogonal matrices ($U^{\\top}U=I$ and $V^{\\top}V=I$), and $\\Sigma$ is a rectangular diagonal matrix.\n\nWe propose the candidate for the pseudoinverse as $A^{+} = V \\Sigma^{+} U^{\\top}$. The matrix $\\Sigma^{+} \\in \\mathbb{R}^{2 \\times 3}$ is the pseudoinverse of $\\Sigma$. It is constructed by taking the reciprocal of each non-zero singular value and transposing the resulting matrix. The non-zero singular values in $\\Sigma$ are $\\sigma_1 = 6$ and $\\sigma_2 = 2$.\n$$\n\\Sigma^{+} \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{6}  0  0 \\\\\n0  \\frac{1}{2}  0\n\\end{bmatrix}.\n$$\nTo rigorously justify the use of $A^{+} = V \\Sigma^{+} U^{\\top}$, we must demonstrate that it satisfies the four Penrose conditions.\n\n1.  $A A^{+} A = (U \\Sigma V^{\\top})(V \\Sigma^{+} U^{\\top})(U \\Sigma V^{\\top}) = U \\Sigma (V^{\\top}V) \\Sigma^{+} (U^{\\top}U) \\Sigma V^{\\top} = U (\\Sigma \\Sigma^{+} \\Sigma) V^{\\top}$.\n    The product $\\Sigma\\Sigma^{+}\\Sigma$ is:\n    $$\n    \\Sigma \\Sigma^{+} \\Sigma = \\left( \\begin{bmatrix} 6  0 \\\\ 0  2 \\\\ 0  0 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{6}  0  0 \\\\ 0  \\frac{1}{2}  0 \\end{bmatrix} \\right) \\Sigma = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{bmatrix} \\begin{bmatrix} 6  0 \\\\ 0  2 \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 6  0 \\\\ 0  2 \\\\ 0  0 \\end{bmatrix} = \\Sigma.\n    $$\n    Therefore, $A A^{+} A = U \\Sigma V^{\\top} = A$. The first condition is satisfied.\n\n2.  $A^{+} A A^{+} = (V \\Sigma^{+} U^{\\top})(U \\Sigma V^{\\top})(V \\Sigma^{+} U^{\\top}) = V \\Sigma^{+} (U^{\\top}U) \\Sigma (V^{\\top}V) \\Sigma^{+} U^{\\top} = V (\\Sigma^{+} \\Sigma \\Sigma^{+}) U^{\\top}$.\n    The product $\\Sigma^{+}\\Sigma\\Sigma^{+}$ is:\n    $$\n    \\Sigma^{+} \\Sigma \\Sigma^{+} = \\left( \\begin{bmatrix} \\frac{1}{6}  0  0 \\\\ 0  \\frac{1}{2}  0 \\end{bmatrix} \\begin{bmatrix} 6  0 \\\\ 0  2 \\\\ 0  0 \\end{bmatrix} \\right) \\Sigma^{+} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{6}  0  0 \\\\ 0  \\frac{1}{2}  0 \\end{bmatrix} = \\Sigma^{+}.\n    $$\n    Therefore, $A^{+} A A^{+} = V \\Sigma^{+} U^{\\top} = A^{+}$. The second condition is satisfied.\n\n3.  $(A A^{+})^{\\top} = A A^{+}$. The product is $A A^{+} = U \\Sigma V^{\\top}V \\Sigma^{+} U^{\\top} = U (\\Sigma \\Sigma^{+}) U^{\\top}$. The matrix $\\Sigma \\Sigma^{+} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{bmatrix}$ is diagonal and thus symmetric, $(\\Sigma \\Sigma^{+})^{\\top} = \\Sigma \\Sigma^{+}$. So, $(A A^{+})^{\\top} = (U (\\Sigma \\Sigma^{+}) U^{\\top})^{\\top} = U (\\Sigma \\Sigma^{+})^{\\top} U^{\\top} = U (\\Sigma \\Sigma^{+}) U^{\\top} = A A^{+}$. The third condition is satisfied.\n\n4.  $(A^{+} A)^{\\top} = A^{+} A$. The product is $A^{+} A = V \\Sigma^{+} U^{\\top}U \\Sigma V^{\\top} = V (\\Sigma^{+} \\Sigma) V^{\\top}$. The matrix $\\Sigma^{+} \\Sigma = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$ is the identity matrix, which is symmetric. So, $(A^{+} A)^{\\top} = (V (\\Sigma^{+} \\Sigma) V^{\\top})^{\\top} = V (\\Sigma^{+} \\Sigma)^{\\top} V^{\\top} = V (\\Sigma^{+} \\Sigma) V^{\\top} = A^{+} A$. The fourth condition is satisfied.\n\nThe derivation confirms that $A^{+} = V \\Sigma^{+} U^{\\top}$ is indeed the Moore-Penrose pseudoinverse. We now compute its $(2,3)$ entry, denoted $A^{+}_{23}$.\n$A^{+} = V \\Sigma^{+} U^{\\top}$. The entry $A^{+}_{23}$ is the dot product of the second row of the product $V\\Sigma^{+}$ and the third column of $U^{\\top}$.\n\nFirst, we find the second row of $V\\Sigma^{+}$:\n$$\n\\left( V\\Sigma^{+} \\right)_{2,:} = \\begin{pmatrix} -\\frac{4}{5}  \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6}  0  0 \\\\ 0  \\frac{1}{2}  0 \\end{pmatrix} = \\begin{pmatrix} \\left(-\\frac{4}{5}\\right)\\left(\\frac{1}{6}\\right)  \\left(-\\frac{4}{5}\\right)(0) + \\left(\\frac{3}{5}\\right)\\left(\\frac{1}{2}\\right)  0 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} -\\frac{4}{30}  \\frac{3}{10}  0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{15}  \\frac{3}{10}  0 \\end{pmatrix}.\n$$\nNext, we determine the third column of $U^{\\top}$. Transposing $U$ gives:\n$$\nU^{\\top} \\;=\\;\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{2}}  0  -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\\n0  1  0\n\\end{bmatrix}.\n$$\nThe third column of $U^{\\top}$ is $\\begin{pmatrix} -\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix}^{\\top}$.\n\nFinally, we compute the dot product to find $A^{+}_{23}$:\n$$\nA^{+}_{23} = \\left(-\\frac{2}{15}\\right) \\left(-\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{3}{10}\\right) \\left(\\frac{1}{\\sqrt{2}}\\right) + (0)(0)\n$$\n$$\nA^{+}_{23} = \\frac{2}{15\\sqrt{2}} + \\frac{3}{10\\sqrt{2}}.\n$$\nTo combine these terms, we use a common denominator of $30\\sqrt{2}$:\n$$\nA^{+}_{23} = \\frac{4}{30\\sqrt{2}} + \\frac{9}{30\\sqrt{2}} = \\frac{13}{30\\sqrt{2}}.\n$$\nRationalizing the denominator yields the final closed-form expression:\n$$\nA^{+}_{23} = \\frac{13\\sqrt{2}}{30 \\times 2} = \\frac{13\\sqrt{2}}{60}.\n$$",
            "answer": "$$\\boxed{\\frac{13\\sqrt{2}}{60}}$$"
        },
        {
            "introduction": "Beyond being a tool for solving equations, the pseudoinverse is deeply connected to the geometry of vector spaces. This practice explores the crucial products $AA^+$ and $A^+A$, which are the orthogonal projectors onto the column space and row space of $A$, respectively. By analyzing a case where $A$ is not a normal matrix, you will demonstrate that these projectors are generally not equal and uncover the distinct subspaces they act upon .",
            "id": "3592297",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the singular matrix\n$$\nA \\;=\\; \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}.\n$$\nUsing only the existence of the singular value decomposition (SVD) for any real matrix and the definition of the Moore–Penrose (MP) pseudoinverse via the SVD, carry out the following steps from first principles:\n\n1. Compute an SVD $A = U \\Sigma V^{\\top}$, identifying the nonzero singular value and corresponding singular vectors. Use this to construct the MP pseudoinverse $A^{+} = V \\Sigma^{+} U^{\\top}$.\n\n2. Compute explicitly the two matrices $A A^{+}$ and $A^{+} A$. Verify directly (by computation) that $A A^{+} \\neq A^{+} A$.\n\n3. Determine the ranges and nullspaces of $A A^{+}$ and $A^{+} A$ by direct linear-algebraic analysis (solve for the column spaces and the solutions to the associated homogeneous systems). State each as a span of vectors in $\\mathbb{R}^{2}$.\n\n4. Let $\\|\\cdot\\|_{F}$ denote the Frobenius norm, defined by $\\|M\\|_{F}^{2} = \\sum_{i,j} M_{ij}^{2}$ for any real matrix $M$. Compute the scalar quantity $\\|A A^{+} - A^{+} A\\|_{F}^{2}$.\n\nYour final answer must be the single real number obtained in step $4$. No rounding is required.",
            "solution": "We proceed from the existence of the singular value decomposition (SVD) and its use in defining the Moore–Penrose (MP) pseudoinverse. For any real matrix $A$, there exist orthogonal matrices $U$ and $V$ and a diagonal matrix $\\Sigma$ with nonnegative diagonal entries (the singular values) such that $A = U \\Sigma V^{\\top}$. If $\\Sigma$ has diagonal entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge 0$, then the MP pseudoinverse is defined by $A^{+} = V \\Sigma^{+} U^{\\top}$, where $\\Sigma^{+}$ replaces each nonzero $\\sigma_{i}$ by $\\sigma_{i}^{-1}$ and leaves zeros as zeros.\n\nStep 1: Compute an SVD of $A$ and then $A^{+}$.\n\nWe have\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}.\n$$\nCompute $A A^{\\top}$ and $A^{\\top} A$:\n$$\nA A^{\\top} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}, \n\\qquad\nA^{\\top} A = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\nThe eigenvalues of $A A^{\\top}$ are $2$ and $0$, so the singular values are $\\sigma_{1} = \\sqrt{2}$ and $\\sigma_{2} = 0$. A unit eigenvector of $A A^{\\top}$ for the eigenvalue $2$ is $u_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, and a unit eigenvector for the eigenvalue $0$ is $u_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Thus we can take\n$$\nU = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \n\\qquad\n\\Sigma = \\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nThe corresponding right singular vector $v_{1}$ is obtained from $v_{1} = \\frac{1}{\\sigma_{1}} A^{\\top} u_{1}$:\n$$\nA^{\\top} u_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \n\\quad\nv_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nChoose $v_{2}$ as a unit vector orthogonal to $v_{1}$, for instance\n$$\nv_{2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nHence\n$$\nV = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nThe pseudoinverse is then\n$$\n\\Sigma^{+} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0 \\end{pmatrix},\n\\qquad\nA^{+} = V \\Sigma^{+} U^{\\top} = V \\Sigma^{+}.\n$$\nCompute $A^{+}$ explicitly:\n$$\nA^{+} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0 \\\\ 0  0 \\end{pmatrix}\n=\n\\begin{pmatrix} \\frac{1}{2}  0 \\\\ \\frac{1}{2}  0 \\end{pmatrix}.\n$$\n\nStep 2: Compute $A A^{+}$ and $A^{+} A$ and verify they are unequal.\n\nFirst,\n$$\nA A^{+} = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{2}  0 \\\\ \\frac{1}{2}  0 \\end{pmatrix}\n=\n\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nNext,\n$$\nA^{+} A = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ \\frac{1}{2}  0 \\end{pmatrix}\n\\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}\n=\n\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}.\n$$\nThese two matrices are clearly not equal, so $A A^{+} \\neq A^{+} A$.\n\nStep 3: Determine ranges and nullspaces of $A A^{+}$ and $A^{+} A$.\n\nFor $A A^{+}$:\n$$\nA A^{+} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nThe range is the column space of $A A^{+}$, namely\n$$\n\\operatorname{range}(A A^{+}) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\}.\n$$\nThe nullspace is the set of $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ such that $(A A^{+}) x = 0$, i.e.,\n$$\n\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\;\\;\\Longrightarrow\\;\\; x_{1} = 0,\n$$\nso\n$$\n\\operatorname{null}(A A^{+}) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right\\}.\n$$\n\nFor $A^{+} A$:\n$$\nA^{+} A = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}.\n$$\nIts range is the span of its columns; both columns are equal to $\\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, hence\n$$\n\\operatorname{range}(A^{+} A) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right\\}.\n$$\nIts nullspace consists of $x$ such that $(A^{+} A) x = 0$, i.e.,\n$$\n\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\;\\;\\Longrightarrow\\;\\; x_{1} + x_{2} = 0,\n$$\nso\n$$\n\\operatorname{null}(A^{+} A) = \\operatorname{span}\\!\\left\\{ \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right\\}.\n$$\n\nStep 4: Compute $\\|A A^{+} - A^{+} A\\|_{F}^{2}$.\n\nForm the difference\n$$\nD \\;=\\; A A^{+} - A^{+} A \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  -\\frac{1}{2} \\end{pmatrix}.\n$$\nThe squared Frobenius norm is the sum of squares of the entries:\n$$\n\\|D\\|_{F}^{2} \\;=\\; \\left(\\frac{1}{2}\\right)^{2} + \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{1}{2}\\right)^{2}\n\\;=\\; \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4}\n\\;=\\; 1.\n$$\nTherefore, the requested scalar is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "The Moore-Penrose pseudoinverse is not the only \"inverse-like\" matrix; it is the unique matrix that satisfies all four Penrose conditions. This exercise illuminates this uniqueness by first characterizing the entire family of \"generalized inverses\" that satisfy only the first condition, $AXA=A$. You will then construct a specific non-Moore-Penrose inverse to see firsthand how it fails to uphold the other crucial properties, thereby appreciating the special role of $A^+$ .",
            "id": "3592273",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the diagonal matrix $A = \\operatorname{diag}(2,0,1)$. A matrix $X \\in \\mathbb{R}^{3 \\times 3}$ is called a generalized inverse of $A$ if it satisfies the equation $A X A = A$. The Moore–Penrose (MP) pseudoinverse $A^{+}$ is the unique matrix satisfying the four Penrose equations: $A A^{+} A = A$, $A^{+} A A^{+} = A^{+}$, $(A A^{+})^{\\top} = A A^{+}$, and $(A^{+} A)^{\\top} = A^{+} A$.\n\nStarting from these definitions and without appealing to any prepackaged formula specific to this matrix, proceed as follows:\n- Determine $A^{+}$ by reasoning from first principles for diagonal matrices.\n- Characterize all generalized inverses $A^{-}$ of $A$ by solving the constraint $A A^{-} A = A$ entrywise.\n- Construct a specific non–Moore–Penrose generalized inverse by selecting one of the free parameters to be nonzero while keeping the others zero as follows: set the $(1,2)$ entry equal to $1$ and all other free parameters equal to $0$. Denote this choice by $A^{-}$.\n- Compute the scalar\n$$\nS \\;=\\; \\|A A^{-} - A A^{+}\\|_{F}^{2} \\;+\\; \\|A^{-} A - A^{+} A\\|_{F}^{2},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm.\n\nExpress the final result for $S$ as an exact real number. No rounding is required.",
            "solution": "We begin from the defining properties. A generalized inverse $A^{-}$ is any matrix satisfying $A A^{-} A = A$. The Moore–Penrose pseudoinverse $A^{+}$ is the unique matrix satisfying the four Penrose equations. For diagonal matrices, these conditions can be analyzed entrywise.\n\nFirst, we determine $A^{+}$. Because $A = \\operatorname{diag}(2,0,1)$ has the singular values equal to $2$, $0$, and $1$, with the standard basis as both left and right singular vectors, the action of the Moore–Penrose pseudoinverse on each singular subspace inverts the nonzero singular values and leaves the zero singular subspace at zero. Thus\n$$\nA^{+} \\;=\\; \\operatorname{diag}\\!\\big(\\tfrac{1}{2},\\,0,\\,1\\big).\n$$\nIndeed, this $A^{+}$ satisfies $A A^{+} = \\operatorname{diag}(1,0,1) = A^{+} A$, which is symmetric, and $A A^{+} A = A$, $A^{+} A A^{+} = A^{+}$.\n\nNext, we characterize all generalized inverses $A^{-}$ by solving $A A^{-} A = A$ entrywise. Write $A = D$ with $D = \\operatorname{diag}(2,0,1)$. Let $X = (x_{ij}) \\in \\mathbb{R}^{3 \\times 3}$ be arbitrary. Then\n$$\n(D X D)_{ij} \\;=\\; d_{i}\\, x_{ij}\\, d_{j},\n$$\nwhere $d_{1} = 2$, $d_{2} = 0$, and $d_{3} = 1$. The constraint $D X D = D$ yields the following equations:\n- For $(i,j) = (1,1)$: $2 \\cdot x_{11} \\cdot 2 = 2$, hence $x_{11} = \\tfrac{1}{2}$.\n- For $(i,j) = (3,3)$: $1 \\cdot x_{33} \\cdot 1 = 1$, hence $x_{33} = 1$.\n- For $(i,j) = (1,3)$: $2 \\cdot x_{13} \\cdot 1 = 0$, hence $x_{13} = 0$.\n- For $(i,j) = (3,1)$: $1 \\cdot x_{31} \\cdot 2 = 0$, hence $x_{31} = 0$.\n- For any index with $i = 2$ or $j = 2$, we have $d_{i} = 0$ or $d_{j} = 0$, so $(D X D)_{ij} = 0$, matching the corresponding zero in $D$ and imposing no constraint.\n\nTherefore, the general solution set of generalized inverses is\n$$\nA^{-} \\;=\\; X \\;=\\;\n\\begin{pmatrix}\n\\tfrac{1}{2}  a  0 \\\\\nb  c  d \\\\\n0  e  1\n\\end{pmatrix},\n$$\nwith $a$, $b$, $c$, $d$, $e \\in \\mathbb{R}$ arbitrary. The Moore–Penrose pseudoinverse corresponds to the choice $a = b = c = d = e = 0$.\n\nWe now construct a specific non–Moore–Penrose generalized inverse by setting the $(1,2)$ entry to $1$ and all other free parameters to $0$. That is,\n$$\nA^{-} \\;=\\;\n\\begin{pmatrix}\n\\tfrac{1}{2}  1  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThis $A^{-}$ satisfies $A A^{-} A = A$ by the above characterization, and it is not the Moore–Penrose pseudoinverse since $a = 1 \\neq 0$. Indeed, it fails the symmetry of one of the oblique projectors, as shown next.\n\nWe compute the associated projectors. First,\n$$\nA A^{+} \\;=\\; \\operatorname{diag}(1,0,1), \\qquad A^{+} A \\;=\\; \\operatorname{diag}(1,0,1).\n$$\nNext,\n$$\nA A^{-} \\;=\\; D X \\;=\\;\n\\begin{pmatrix}\n2  0  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\tfrac{1}{2}  1  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1  2  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix},\n$$\nand\n$$\nA^{-} A \\;=\\; X D \\;=\\;\n\\begin{pmatrix}\n\\tfrac{1}{2}  1  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n2  0  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1  0  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nA A^{-} - A A^{+} \\;=\\;\n\\begin{pmatrix}\n0  2  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix},\n\\qquad\nA^{-} A - A^{+} A \\;=\\;\n\\begin{pmatrix}\n0  0  0 \\\\\n0  0  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nThe Frobenius norm squared is the sum of the squares of all entries. Thus,\n$$\n\\|A A^{-} - A A^{+}\\|_{F}^{2} \\;=\\; 2^{2} \\;=\\; 4,\n\\qquad\n\\|A^{-} A - A^{+} A\\|_{F}^{2} \\;=\\; 0.\n$$\nHence,\n$$\nS \\;=\\; 4 \\;+\\; 0 \\;=\\; 4.\n$$\nThis completes the construction and comparison from first principles, producing the requested scalar.",
            "answer": "$$\\boxed{4}$$"
        }
    ]
}