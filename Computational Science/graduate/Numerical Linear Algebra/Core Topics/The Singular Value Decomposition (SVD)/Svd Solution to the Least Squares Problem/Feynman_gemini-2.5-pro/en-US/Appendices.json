{
    "hands_on_practices": [
        {
            "introduction": "This practice will guide you through the foundational mechanics of using the Singular Value Decomposition (SVD) to find the minimum-norm solution to a least squares problem. When a system is rank-deficient, there exists an entire subspace of solutions that minimize the residual; the SVD provides a natural and stable way to select the unique solution with the smallest Euclidean norm. By working through this exercise , you will solidify your understanding of how transforming the problem into the coordinates of the singular vectors decouples the system and allows for a direct construction of this optimal solution.",
            "id": "3583009",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{3 \\times 3}$ with the singular value decomposition (SVD), meaning $A = U \\Sigma V^{\\top}$ where $U \\in \\mathbb{R}^{3 \\times 3}$ and $V \\in \\mathbb{R}^{3 \\times 3}$ are orthogonal and $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$ is diagonal with nonnegative entries. Let\n$$\nU = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\n\\Sigma = \\begin{pmatrix}\n4 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}.\n$$\nLet $b \\in \\mathbb{R}^{3}$ be given by $b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$ and let $w \\in \\mathbb{R}^{3}$ be given by $w = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$. Using only the core definitions of the least squares problem $\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}$ and the singular value decomposition, explicitly construct the minimum-norm least squares solution $x^{\\star}$ by transforming the problem into the singular vector coordinates, setting coordinates in the null space corresponding to the zero singular values to zero, and solving for the remaining coordinates via the nonzero singular values.\n\nCompute the scalar quantity $w^{\\top} x^{\\star}$ and express your final answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem is to find the minimum-norm least squares solution $x^{\\star} \\in \\mathbb{R}^{3}$ to the problem $\\min_{x \\in \\mathbb{R}^{3}} \\|Ax - b\\|_{2}$, and then compute the scalar quantity $w^{\\top} x^{\\star}$. The matrix $A$ is given by its singular value decomposition (SVD), $A = U \\Sigma V^{\\top}$, where\n$$\nU = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\n\\Sigma = \\begin{pmatrix}\n4 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix}.\n$$\nThe vectors $b$ and $w$ are given as\n$$\nb = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}, \\quad\nw = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}.\n$$\nThe objective is to minimize the Euclidean norm of the residual, $\\|Ax - b\\|_{2}$. We substitute the SVD of $A$ into this expression:\n$$\n\\|Ax - b\\|_{2} = \\|U \\Sigma V^{\\top} x - b\\|_{2}.\n$$\nSince $U$ is an orthogonal matrix, its transpose $U^{\\top}$ is also orthogonal. Multiplication by an orthogonal matrix preserves the Euclidean norm, so we can multiply the term inside the norm by $U^{\\top}$ without changing the value of the norm:\n$$\n\\|U \\Sigma V^{\\top} x - b\\|_{2} = \\|U^{\\top}(U \\Sigma V^{\\top} x - b)\\|_{2} = \\|(U^{\\top}U) \\Sigma V^{\\top} x - U^{\\top}b\\|_{2}.\n$$\nAs $U^{\\top}U = I$ (the identity matrix), this simplifies to:\n$$\n\\|\\Sigma V^{\\top} x - U^{\\top}b\\|_{2}.\n$$\nTo simplify this expression further, we perform a change of variables. Let $y = V^{\\top}x$. Since $V$ is an orthogonal matrix, we can express $x$ as $x = Vy$. This transformation is an isometry, meaning $\\|x\\|_{2} = \\|Vy\\|_{2} = \\|y\\|_{2}$. Therefore, finding the minimum-norm solution $x^{\\star}$ is equivalent to finding the minimum-norm solution $y^{\\star}$ for the transformed problem. The problem now becomes to find $y \\in \\mathbb{R}^{3}$ that minimizes $\\|\\Sigma y - U^{\\top}b\\|_{2}$.\n\nFirst, we compute the vector $c = U^{\\top}b$:\n$$\nc = U^{\\top}b = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}(1) - \\frac{1}{\\sqrt{2}}(-2) \\\\ \\frac{1}{\\sqrt{2}}(1) + \\frac{1}{\\sqrt{2}}(-2) \\\\ 1(3) \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 3 \\end{pmatrix}.\n$$\nNow we express the minimization problem in terms of the components of $y = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}$. We want to minimize the squared norm $\\|\\Sigma y - c\\|_{2}^{2}$:\n$$\n\\|\\Sigma y - c\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 3 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 4y_1 - \\frac{3}{\\sqrt{2}} \\\\ y_2 - (-\\frac{1}{\\sqrt{2}}) \\\\ 0 \\cdot y_3 - 3 \\end{pmatrix} \\right\\|_{2}^{2}.\n$$\nThis squared norm is the sum of the squares of the components:\n$$\n(4y_1 - \\frac{3}{\\sqrt{2}})^2 + (y_2 + \\frac{1}{\\sqrt{2}})^2 + (-3)^2.\n$$\nTo minimize this expression, we must choose $y_1$ and $y_2$ to make the first two terms zero, as they are non-negative.\nFor the first term: $4y_1 - \\frac{3}{\\sqrt{2}} = 0 \\implies y_1 = \\frac{3}{4\\sqrt{2}}$.\nFor the second term: $y_2 + \\frac{1}{\\sqrt{2}} = 0 \\implies y_2 = -\\frac{1}{\\sqrt{2}}$.\nThe third term, $(-3)^2=9$, is a constant and represents the minimum possible squared error. The choice of $y_3$ does not affect the value of the squared error, so any $y_3 \\in \\mathbb{R}$ gives a valid least squares solution. The set of all least squares solutions for $y$ is given by vectors of the form $\\begin{pmatrix} \\frac{3}{4\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ k \\end{pmatrix}$ for any $k \\in \\mathbb{R}$.\n\nTo find the unique minimum-norm solution $x^{\\star}$, we must first find the minimum-norm solution $y^{\\star}$. The squared norm of $y$ is $\\|y\\|_{2}^{2} = y_1^2 + y_2^2 + y_3^2$:\n$$\n\\|y\\|_{2}^{2} = \\left(\\frac{3}{4\\sqrt{2}}\\right)^2 + \\left(-\\frac{1}{\\sqrt{2}}\\right)^2 + y_3^2.\n$$\nTo minimize this norm, we must choose the component $y_3$ to be zero, i.e., $y_3 = 0$. This corresponds to setting the coordinate in the direction of the singular vector associated with the zero singular value to $0$. Thus, the minimum-norm solution $y^{\\star}$ is:\n$$\ny^{\\star} = \\begin{pmatrix} \\frac{3}{4\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}.\n$$\nNow we transform back to the original coordinates to find $x^{\\star} = Vy^{\\star}$:\n$$\nx^{\\star} = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{4\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (0)(\\frac{3}{4\\sqrt{2}}) + (1)(-\\frac{1}{\\sqrt{2}}) + (0)(0) \\\\ (1)(\\frac{3}{4\\sqrt{2}}) + (0)(-\\frac{1}{\\sqrt{2}}) + (0)(0) \\\\ (0)(\\frac{3}{4\\sqrt{2}}) + (0)(-\\frac{1}{\\sqrt{2}}) + (-1)(0) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{3}{4\\sqrt{2}} \\\\ 0 \\end{pmatrix}.\n$$\nFinally, we compute the required scalar quantity $w^{\\top}x^{\\star}$:\n$$\nw^{\\top}x^{\\star} = \\begin{pmatrix} 2 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{3}{4\\sqrt{2}} \\\\ 0 \\end{pmatrix} = (2)\\left(-\\frac{1}{\\sqrt{2}}\\right) + (-1)\\left(\\frac{3}{4\\sqrt{2}}\\right) + (0)(0).\n$$\n$$\nw^{\\top}x^{\\star} = -\\frac{2}{\\sqrt{2}} - \\frac{3}{4\\sqrt{2}} = -\\frac{8}{4\\sqrt{2}} - \\frac{3}{4\\sqrt{2}} = -\\frac{11}{4\\sqrt{2}}.\n$$\nTo write the answer in a standard closed form, we rationalize the denominator:\n$$\nw^{\\top}x^{\\star} = -\\frac{11}{4\\sqrt{2}} \\cdot \\frac{\\sqrt{2}}{\\sqrt{2}} = -\\frac{11\\sqrt{2}}{4 \\cdot 2} = -\\frac{11\\sqrt{2}}{8}.\n$$",
            "answer": "$$ \\boxed{-\\frac{11\\sqrt{2}}{8}} $$"
        },
        {
            "introduction": "Building on the concept of the minimum-norm solution, this exercise explores the full set of solutions for a rank-deficient least squares problem. You will see firsthand that while many vectors can minimize the residual $\\lVert Ax - b \\rVert_2$, they form an affine subspace, and only one of them has the minimum norm. This practice  requires you to compute an SVD from scratch and use the right singular vectors corresponding to zero singular values to navigate this solution space, reinforcing the geometric interpretation of the null space and the unique nature of the Moore-Penrose pseudoinverse solution.",
            "id": "3583032",
            "problem": "Consider the real $m \\times n$ matrix $A$ and vector $b$ given by\n$$\nA = \\begin{pmatrix}\n1 & 2 \\\\\n2 & 4 \\\\\n3 & 6\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n3 \\\\ 1 \\\\ 3\n\\end{pmatrix}.\n$$\nUsing only the foundational definitions of the singular value decomposition (SVD) and the least-squares minimizer, and the properties of orthogonal projection onto a subspace, carry out the following steps.\n\n1. Compute an explicit singular value decomposition of $A$, that is, orthogonal matrices $U \\in \\mathbb{R}^{3 \\times 3}$ and $V \\in \\mathbb{R}^{2 \\times 2}$, and a diagonal matrix $\\Sigma \\in \\mathbb{R}^{3 \\times 2}$ with nonnegative diagonal entries, such that $A = U \\Sigma V^{\\top}$. Identify the nonzero singular value and the corresponding singular vectors, and verify the rank deficiency of $A$.\n\n2. Using the SVD and the Moore–Penrose pseudoinverse, compute the minimum-norm least-squares solution $x^{\\dagger}$ to $\\min_{x \\in \\mathbb{R}^{2}} \\|A x - b\\|_{2}$.\n\n3. Let $v_{2}$ denote the right singular vector associated with the zero singular value of $A$ in your SVD. Construct an alternative least-squares minimizer by setting\n$$\nx_{\\mathrm{alt}} = x^{\\dagger} + \\frac{\\sqrt{5}}{5} \\, v_{2}.\n$$\nExplain why $x_{\\mathrm{alt}}$ is also a least-squares minimizer and why all least-squares minimizers for this $A$ have identical residual vectors.\n\n4. Compute the residual vector $r = b - A x^{\\dagger}$ and its squared $2$-norm $\\|r\\|_{2}^{2}$. Compute the squared $2$-norms $\\|x^{\\dagger}\\|_{2}^{2}$ and $\\|x_{\\mathrm{alt}}\\|_{2}^{2}$, and confirm that the residuals are identical while the solution norms are different.\n\nFinally, define the scalar\n$$\nS \\;=\\; \\|r\\|_{2}^{2} \\;+\\; \\frac{\\|x_{\\mathrm{alt}}\\|_{2}^{2}}{\\|x^{\\dagger}\\|_{2}^{2}}.\n$$\nExpress the final quantity $S$ as an exact value. No rounding is required.",
            "solution": "First, we compute the singular value decomposition (SVD) of the matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{pmatrix}$. The SVD is defined as $A = U \\Sigma V^{\\top}$. To find the singular values and the right singular vectors (columns of $V$), we analyze the matrix $A^{\\top}A$.\n$$ A^{\\top}A = \\begin{pmatrix} 1 & 2 & 3 \\\\ 2 & 4 & 6 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{pmatrix} = \\begin{pmatrix} 1+4+9 & 2+8+18 \\\\ 2+8+18 & 4+16+36 \\end{pmatrix} = \\begin{pmatrix} 14 & 28 \\\\ 28 & 56 \\end{pmatrix} $$\nThe eigenvalues $\\lambda$ of $A^{\\top}A$ are found from the characteristic equation $\\det(A^{\\top}A - \\lambda I) = 0$:\n$$ (14-\\lambda)(56-\\lambda) - 28^2 = \\lambda^2 - 70\\lambda + 784 - 784 = \\lambda(\\lambda - 70) = 0 $$\nThe eigenvalues are $\\lambda_1 = 70$ and $\\lambda_2 = 0$. The singular values are their square roots, $\\sigma_1 = \\sqrt{70}$ and $\\sigma_2 = 0$. Since there is only one non-zero singular value, the rank of $A$ is $1$, confirming its rank deficiency.\n\nThe right singular vectors, $v_1$ and $v_2$, are the normalized eigenvectors of $A^{\\top}A$.\nFor $\\lambda_1 = 70$, the eigenvector equation is $(A^{\\top}A - 70I)v_1 = 0$, which gives $-56x_1 + 28x_2 = 0$, or $x_2 = 2x_1$. We choose the eigenvector $\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. Normalizing gives $v_1 = \\frac{1}{\\sqrt{1^2+2^2}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nFor $\\lambda_2 = 0$, the eigenvector equation is $A^{\\top}Av_2 = 0$, which gives $14x_1 + 28x_2 = 0$, or $x_1 = -2x_2$. We choose $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. Normalizing gives $v_2 = \\frac{1}{\\sqrt{(-2)^2+1^2}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$.\nThe orthogonal matrix $V$ is $V = \\begin{pmatrix} v_1 & v_2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\end{pmatrix}$.\n\nThe left singular vectors (columns of $U$) are found next. For non-zero singular values, $u_i = \\frac{1}{\\sigma_i}Av_i$.\n$$ u_1 = \\frac{1}{\\sqrt{70}} \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{pmatrix} \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{350}} \\begin{pmatrix} 5 \\\\ 10 \\\\ 15 \\end{pmatrix} = \\frac{5}{5\\sqrt{14}} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{14}}\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nThe remaining columns of $U$, $u_2$ and $u_3$, must form an orthonormal basis for the null space of $A^{\\top}$. The null space is defined by $x+2y+3z=0$. Two orthogonal vectors satisfying this are $\\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 3 \\\\ 6 \\\\ -5 \\end{pmatrix}$. Normalizing them, we get $u_2 = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $u_3 = \\frac{1}{\\sqrt{70}}\\begin{pmatrix} 3 \\\\ 6 \\\\ -5 \\end{pmatrix}$.\nThe complete SVD is:\n$A = U \\Sigma V^{\\top}$ with $U = \\begin{pmatrix} \\frac{1}{\\sqrt{14}} & \\frac{-2}{\\sqrt{5}} & \\frac{3}{\\sqrt{70}} \\\\ \\frac{2}{\\sqrt{14}} & \\frac{1}{\\sqrt{5}} & \\frac{6}{\\sqrt{70}} \\\\ \\frac{3}{\\sqrt{14}} & 0 & \\frac{-5}{\\sqrt{70}} \\end{pmatrix}$, $\\Sigma = \\begin{pmatrix} \\sqrt{70} & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$, $V = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\end{pmatrix}$. The non-zero singular value is $\\sigma_1 = \\sqrt{70}$, with corresponding singular vectors $u_1$ and $v_1$.\n\nSecond, we compute the minimum-norm least-squares solution $x^{\\dagger} = A^{\\dagger}b$, where $A^{\\dagger}$ is the Moore-Penrose pseudoinverse, $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$.\nThe pseudoinverse of $\\Sigma$ is $\\Sigma^{\\dagger} = \\begin{pmatrix} 1/\\sqrt{70} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$.\nThe product $U^{\\top}b$ is:\n$$ U^{\\top}b = \\begin{pmatrix} \\frac{1}{\\sqrt{14}} & \\frac{2}{\\sqrt{14}} & \\frac{3}{\\sqrt{14}} \\\\ \\frac{-2}{\\sqrt{5}} & \\frac{1}{\\sqrt{5}} & 0 \\\\ \\frac{3}{\\sqrt{70}} & \\frac{6}{\\sqrt{70}} & \\frac{-5}{\\sqrt{70}} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{14}}(3+2+9) \\\\ \\frac{1}{\\sqrt{5}}(-6+1) \\\\ \\frac{1}{\\sqrt{70}}(9+6-15) \\end{pmatrix} = \\begin{pmatrix} \\sqrt{14} \\\\ -\\sqrt{5} \\\\ 0 \\end{pmatrix} $$\nThen $x^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}b = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{70} & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} \\sqrt{14} \\\\ -\\sqrt{5} \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} \\sqrt{14}/\\sqrt{70} \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{5}}\\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{5} \\\\ 0 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nSo, $x^{\\dagger} = \\begin{pmatrix} 1/5 \\\\ 2/5 \\end{pmatrix}$.\n\nThird, we construct the alternative solution $x_{\\mathrm{alt}} = x^{\\dagger} + \\frac{\\sqrt{5}}{5} \\, v_{2}$. With $c = \\frac{\\sqrt{5}}{5} = \\frac{1}{\\sqrt{5}}$, we have:\n$$ x_{\\mathrm{alt}} = \\begin{pmatrix} 1/5 \\\\ 2/5 \\end{pmatrix} + \\frac{1}{\\sqrt{5}} \\left( \\frac{1}{\\sqrt{5}}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 1/5 \\\\ 2/5 \\end{pmatrix} + \\frac{1}{5}\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1/5 \\\\ 3/5 \\end{pmatrix} $$\nThe set of all least-squares solutions to $\\min_{x} \\|Ax-b\\|_2$ is given by the affine subspace $x^{\\dagger} + \\mathcal{N}(A)$, where $\\mathcal{N}(A)$ is the null space of $A$. The right singular vectors corresponding to zero singular values form an orthonormal basis for $\\mathcal{N}(A)$. Here, $\\mathcal{N}(A) = \\text{span}\\{v_2\\}$. Thus, $x_{\\mathrm{alt}}$ is a valid least-squares solution because it is of the form $x^{\\dagger} + c v_2$ for a scalar $c$. To see why this is true, consider the residual for any such solution: $A(x^{\\dagger} + c v_2) - b = (A x^{\\dagger} - b) + c(A v_2)$. Since $v_2 \\in \\mathcal{N}(A)$, $A v_2 = 0$. The residual vector is $A x^{\\dagger} - b$, identical to that for $x^{\\dagger}$. Since $x^{\\dagger}$ minimizes the residual norm, so does $x_{\\mathrm{alt}}$. The residual vector for any least-squares solution $x_{LS}$ is $r = b - A x_{LS}$. The vector $A x_{LS}$ is the orthogonal projection of $b$ onto the column space of $A$, $\\mathcal{C}(A)$. This projection is unique, regardless of the choice of $x_{LS}$ from the solution set. Therefore, the residual vector $r=b - \\operatorname{proj}_{\\mathcal{C}(A)}(b)$ is identical for all least-squares minimizers.\n\nFourth, we compute the required quantities. The residual vector for $x^{\\dagger}$ is:\n$$ r = b - Ax^{\\dagger} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 6 \\end{pmatrix} \\begin{pmatrix} 1/5 \\\\ 2/5 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1/5+4/5 \\\\ 2/5+8/5 \\\\ 3/5+12/5 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} $$\nIts squared $2$-norm is $\\|r\\|_2^2 = 2^2 + (-1)^2 + 0^2 = 5$.\nThe squared $2$-norm of $x^{\\dagger}$ is $\\|x^{\\dagger}\\|_2^2 = (1/5)^2 + (2/5)^2 = 1/25 + 4/25 = 5/25 = 1/5$.\nThe squared $2$-norm of $x_{\\mathrm{alt}}$ is $\\|x_{\\mathrm{alt}}\\|_2^2 = (-1/5)^2 + (3/5)^2 = 1/25 + 9/25 = 10/25 = 2/5$.\nAs established in the third step, the residual for $x_{\\mathrm{alt}}$ is identical to that for $x^{\\dagger}$. The solution norms are different, with $\\|x_{\\mathrm{alt}}\\|_2^2 > \\|x^{\\dagger}\\|_2^2$, consistent with $x^{\\dagger}$ being the minimum-norm solution.\n\nFinally, we compute the scalar $S$:\n$$ S = \\|r\\|_{2}^{2} + \\frac{\\|x_{\\mathrm{alt}}\\|_{2}^{2}}{\\|x^{\\dagger}\\|_{2}^{2}} = 5 + \\frac{2/5}{1/5} = 5 + 2 = 7 $$\nThe exact value of $S$ is $7$.",
            "answer": "$$\n\\boxed{7}\n$$"
        },
        {
            "introduction": "Why is the SVD-based approach often superior to other methods for solving least squares problems, such as the normal equations? This advanced practice delves into the crucial topic of numerical stability and conditioning, directly comparing the SVD method with the normal equations approach for an ill-conditioned system. By analyzing the spectral condition numbers, you will quantify the dramatic loss of accuracy that can occur when forming the matrix $A^{\\top}A$ and discover the fundamental reason—the squaring of the condition number, $\\kappa_{2}(A^{\\top}A) = \\kappa_{2}(A)^2$—that makes the normal equations method numerically fragile . This exercise provides a compelling demonstration of the robustness that makes the SVD an indispensable tool in numerical computation.",
            "id": "3583055",
            "problem": "Consider the overdetermined least squares problem formulated with the matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and right-hand side $b \\in \\mathbb{R}^{2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 1 \\\\\n0 & \\varepsilon\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ \\varepsilon/2\n\\end{pmatrix},\n\\qquad\n\\varepsilon \\;=\\; 10^{-8}.\n$$\nYou will analyze two algorithms for solving the least squares problem $\\min_{x \\in \\mathbb{R}^{2}} \\|Ax-b\\|_{2}$, and quantify the difference in accuracy due to conditioning effects and rounding errors.\n\nWork from first principles using the following foundational bases:\n- The singular value decomposition (SVD) as a factorization of a real matrix that reveals its singular values.\n- The definition of the spectral (two-norm) condition number $\\kappa_{2}(A) \\equiv \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$ for invertible $A$, and that $\\|A\\|_{2}$ is the largest singular value.\n- The standard floating-point model with unit roundoff $u$ defined by $fl(a \\,\\mathrm{op}\\, b) = (a \\,\\mathrm{op}\\, b)(1+\\delta)$ with $|\\delta| \\le u$, where $u = 2^{-53}$ for IEEE 754 double precision.\n\nTasks:\n1. Argue why the least squares solution coincides with the exact solution to $Ax=b$ for the given data, and determine the exact solution $x_{\\star}$.\n2. Compute the singular values of $A$ by analyzing the eigenvalues of $A^{T}A$, and from these compute the spectral condition number $\\kappa_{2}(A)$ in exact symbolic form as a function of $\\varepsilon$. Then evaluate it numerically for $\\varepsilon = 10^{-8}$.\n3. Determine the spectral condition number of the normal equations matrix $A^{T}A$ in terms of $\\kappa_{2}(A)$, justifying your conclusion from the spectral properties of $A^{T}A$.\n4. Using first-order perturbation analysis under the standard floating-point model and normwise backward stability assumptions appropriate for:\n   - A solver based on the singular value decomposition (SVD),\n   - A solver that forms the normal equations and then uses a backward stable Cholesky factorization,\nderive expressions for the leading-order relative forward error bounds of the computed solution in terms of $\\kappa_{2}(A)$, $\\kappa_{2}(A^{T}A)$, and $u$.\n5. Evaluate these bounds numerically for the given $A$, $\\varepsilon = 10^{-8}$, and $u = 2^{-53}$. Report, as your final answer, the numerical value of the predicted leading-order relative forward error bound for the normal equations–based method. Round your final numerical answer to three significant figures.",
            "solution": "The problem asks for an analysis of two methods for solving the least squares problem $\\min_{x \\in \\mathbb{R}^{2}} \\|Ax-b\\|_{2}$ with the given matrix $A$ and vector $b$.\n\nTask 1: Argue why the least squares solution coincides with the exact solution to $Ax=b$ and determine this solution.\nThe given matrix is $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & \\varepsilon \\end{pmatrix}$, which is a square matrix of size $2 \\times 2$. Its determinant is $\\det(A) = (1)(\\varepsilon) - (1)(0) = \\varepsilon$. Since $\\varepsilon = 10^{-8} \\neq 0$, the matrix $A$ is invertible.\nFor an invertible matrix $A$, the linear system $Ax=b$ has a unique solution given by $x_{\\star} = A^{-1}b$. If we substitute this solution into the objective function of the least squares problem, we get $\\|Ax_{\\star}-b\\|_{2} = \\|A(A^{-1}b)-b\\|_{2} = \\|b-b\\|_{2} = \\|0\\|_{2} = 0$.\nThe norm $\\|Ax-b\\|_{2}$ is always non-negative. Since we have found a vector $x_{\\star}$ for which the norm is $0$, this is the minimum possible value. Therefore, the unique solution to the least squares problem is the same as the unique solution to the linear system $Ax=b$.\nWe find this solution $x_{\\star} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ by solving the system:\n$$\n\\begin{pmatrix}\n1 & 1 \\\\\n0 & \\varepsilon\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\\\\n\\varepsilon/2\n\\end{pmatrix}\n$$\nThis is an upper triangular system, which can be solved by back substitution. From the second row, we have $\\varepsilon x_2 = \\varepsilon/2$, which implies $x_2 = 1/2$.\nSubstituting this into the first row equation, $x_1 + x_2 = 1$, we get $x_1 + 1/2 = 1$, which gives $x_1 = 1/2$.\nThus, the exact solution is $x_{\\star} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$.\n\nTask 2: Compute the singular values and the spectral condition number $\\kappa_{2}(A)$.\nThe singular values of $A$, denoted $\\sigma_i$, are the square roots of the eigenvalues of the matrix $A^{T}A$.\n$$\nA^T = \\begin{pmatrix} 1 & 0 \\\\ 1 & \\varepsilon \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 1 & 0 \\\\ 1 & \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 0 & \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\varepsilon^2 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $A^T A$ are the roots of the characteristic equation $\\det(A^T A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 1-\\lambda & 1 \\\\ 1 & 1+\\varepsilon^2-\\lambda \\end{pmatrix} = (1-\\lambda)(1+\\varepsilon^2-\\lambda) - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\varepsilon^2)\\lambda + (1+\\varepsilon^2) - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\varepsilon^2)\\lambda + \\varepsilon^2 = 0\n$$\nUsing the quadratic formula for $\\lambda$:\n$$\n\\lambda = \\frac{(2+\\varepsilon^2) \\pm \\sqrt{(2+\\varepsilon^2)^2 - 4\\varepsilon^2}}{2} = \\frac{2+\\varepsilon^2 \\pm \\sqrt{4+4\\varepsilon^2+\\varepsilon^4 - 4\\varepsilon^2}}{2} = \\frac{2+\\varepsilon^2 \\pm \\sqrt{4+\\varepsilon^4}}{2}\n$$\nThe eigenvalues of $A^T A$ are $\\lambda_1 = \\sigma_1^2$ and $\\lambda_2 = \\sigma_2^2$, where $\\sigma_1 \\ge \\sigma_2$.\n$$\n\\sigma_1^2 = \\frac{2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4}}{2}, \\qquad \\sigma_2^2 = \\frac{2+\\varepsilon^2 - \\sqrt{4+\\varepsilon^4}}{2}\n$$\nThe singular values are $\\sigma_1 = \\sqrt{\\frac{2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4}}{2}}$ and $\\sigma_2 = \\sqrt{\\frac{2+\\varepsilon^2 - \\sqrt{4+\\varepsilon^4}}{2}}$.\nThe spectral condition number is $\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_2} = \\sqrt{\\frac{\\sigma_1^2}{\\sigma_2^2}} = \\sqrt{\\frac{2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4}}{2+\\varepsilon^2 - \\sqrt{4+\\varepsilon^4}}}$.\nTo simplify, multiply the numerator and denominator inside the square root by the conjugate of the denominator:\n$$\n\\kappa_2(A)^2 = \\frac{(2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4})^2}{(2+\\varepsilon^2)^2 - (4+\\varepsilon^4)} = \\frac{(2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4})^2}{4+4\\varepsilon^2+\\varepsilon^4 - 4 - \\varepsilon^4} = \\frac{(2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4})^2}{4\\varepsilon^2}\n$$\nTaking the square root gives the exact symbolic form:\n$$\n\\kappa_2(A) = \\frac{2+\\varepsilon^2 + \\sqrt{4+\\varepsilon^4}}{2\\varepsilon}\n$$\nFor $\\varepsilon = 10^{-8}$, $\\varepsilon^2=10^{-16}$ and $\\varepsilon^4=10^{-32}$ are very small. We can approximate $\\sqrt{4+\\varepsilon^4} = 2\\sqrt{1+\\varepsilon^4/4} \\approx 2$. Therefore, $\\kappa_2(A) \\approx \\frac{2+0+2}{2\\varepsilon} = \\frac{2}{\\varepsilon}$.\nNumerically, for $\\varepsilon=10^{-8}$:\n$$\n\\kappa_2(A) = \\frac{2+10^{-16} + \\sqrt{4+10^{-32}}}{2 \\times 10^{-8}} \\approx \\frac{4}{2 \\times 10^{-8}} = 2 \\times 10^8\n$$\n\nTask 3: Determine the spectral condition number of $A^T A$.\nLet the singular value decomposition of $A$ be $A=U\\Sigma V^T$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2)$.\nThen $A^T A = (U\\Sigma V^T)^T (U\\Sigma V^T) = V\\Sigma^T U^T U \\Sigma V^T = V(\\Sigma^T \\Sigma)V^T$.\nSince $V$ is orthogonal, this is a similarity transformation, and the eigenvalues of $A^T A$ are the diagonal entries of $\\Sigma^T \\Sigma = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2)$.\nThe matrix $A^T A$ is symmetric and positive definite. Its spectral norm is its largest eigenvalue, $\\|A^T A\\|_2 = \\sigma_1^2 = \\|A\\|_2^2$.\nThe inverse is $(A^T A)^{-1} = V(\\Sigma^T\\Sigma)^{-1}V^T = V \\mathrm{diag}(1/\\sigma_1^2, 1/\\sigma_2^2) V^T$.\nThe spectral norm of the inverse is its largest eigenvalue, which is $1/\\sigma_2^2$. So, $\\|(A^T A)^{-1}\\|_2 = 1/\\sigma_2^2 = \\|A^{-1}\\|_2^2$.\nThe condition number of $A^T A$ is therefore:\n$$\n\\kappa_2(A^T A) = \\|A^T A\\|_2 \\|(A^T A)^{-1}\\|_2 = (\\sigma_1^2) \\left(\\frac{1}{\\sigma_2^2}\\right) = \\left(\\frac{\\sigma_1}{\\sigma_2}\\right)^2 = (\\kappa_2(A))^2\n$$\nThis relationship, $\\kappa_2(A^T A) = \\kappa_2(A)^2$, is a general property.\n\nTask 4: Derive leading-order relative forward error bounds.\nWe analyze the forward error $\\frac{\\|\\hat{x}-x_{\\star}\\|_2}{\\|x_{\\star}\\|_2}$ for a computed solution $\\hat{x}$.\nA) SVD-based solver: Solving the least squares problem (or in this case, the square linear system $Ax=b$) using the SVD is a normwise backward stable method. This means the computed solution $\\hat{x}_{SVD}$ is the exact solution to a perturbed problem $(A+\\Delta A)\\hat{x}_{SVD} = b+\\Delta b$, where $\\|\\Delta A\\|_2 \\le c_1 u \\|A\\|_2$ and $\\|\\Delta b\\|_2 \\le c_2 u \\|b\\|_2$ for small constants $c_1, c_2$ and unit roundoff $u$.\nStandard perturbation theory for linear systems gives the first-order forward error bound:\n$$\n\\frac{\\|\\hat{x}_{SVD}-x_{\\star}\\|_2}{\\|x_{\\star}\\|_2} \\le u \\kappa_2(A) \\left(c_1 + c_2\\frac{\\|b\\|_2}{\\|A\\|_2\\|x_\\star\\|_2}\\right) + O(u^2)\n$$\nFor a leading-order analysis, we consider the dominant term. It is standard to represent this as:\n$$\n\\frac{\\|\\hat{x}_{SVD}-x_{\\star}\\|_2}{\\|x_{\\star}\\|_2} \\approx C_{SVD} u \\kappa_2(A)\n$$\nwhere $C_{SVD}$ is a modest constant of order $O(1)$ related to the problem dimensions and algorithm specifics.\n\nB) Normal equations solver: This method involves two steps:\n1.  Form the matrix $C = A^T A$ and the vector $d = A^T b$.\n2.  Solve the system $Cx=d$ using Cholesky factorization.\nThe critical source of error is the formation of $A^T A$. The computation is subject to roundoff errors, so we compute $\\hat{C} = fl(A^T A) = A^T A + E_C$. The error $E_C$ is bounded by $\\|E_C\\|_2 \\le \\gamma_{n} \\|A^T\\|_2 \\|A\\|_2 = \\gamma_{n} \\|A\\|_2^2$, where $\\gamma_{n} = \\frac{nu}{1-nu}$.\nThe computed system is then solved by a backward stable method (Cholesky), introducing further small perturbations. The dominant error, however, arises from the initial computation of $\\hat{C}$. The relative error on the matrix of the system to be solved is approximately $\\frac{\\|E_C\\|_2}{\\|A^T A\\|_2} \\approx \\frac{c u \\|A\\|_2^2}{\\|A\\|_2^2} = c u$.\nThe forward error in solving $Cx=d$ is bounded by its condition number times the backward error. The governing condition number is that of the normal equations matrix $C = A^T A$.\n$$\n\\frac{\\|\\hat{x}_{NE}-x_{\\star}\\|_2}{\\|x_{\\star}\\|_2} \\approx C_{NE} u \\kappa_2(A^T A)\n$$\nUsing the result from Task 3, we get the leading-order bound:\n$$\n\\frac{\\|\\hat{x}_{NE}-x_{\\star}\\|_2}{\\|x_{\\star}\\|_2} \\approx C_{NE} u \\kappa_2(A)^2\n$$\nwhere $C_{NE}$ is an order-one constant. The squaring of the condition number is the key feature that makes this method numerically unstable for ill-conditioned matrices.\n\nTask 5: Evaluate the bounds numerically.\nWe are given $\\varepsilon = 10^{-8}$ and $u = 2^{-53}$.\nFrom Task 2, $\\kappa_2(A) \\approx 2/\\varepsilon = 2 \\times 10^8$.\nFrom Task 3, $\\kappa_2(A^T A) = \\kappa_2(A)^2 \\approx (2 \\times 10^8)^2 = 4 \\times 10^{16}$.\nFor the leading-order analysis, it is conventional to set the constants $C_{SVD}$ and $C_{NE}$ to $1$ to focus on the scaling with the condition number.\nThe predicted leading-order relative forward error bound for the SVD method is:\n$$\n\\text{Error}_{SVD} \\approx u \\kappa_2(A) \\approx (2^{-53})(2 \\times 10^8) \\approx (1.1102 \\times 10^{-16})(2 \\times 10^8) \\approx 2.22 \\times 10^{-8}\n$$\nThis suggests a loss of about $8$ decimal digits of accuracy.\nThe predicted leading-order relative forward error bound for the normal equations method is:\n$$\n\\text{Error}_{NE} \\approx u \\kappa_2(A^T A) = u \\kappa_2(A)^2 \\approx (2^{-53})(4 \\times 10^{16})\n$$\nUsing $u \\approx 1.110223 \\times 10^{-16}$:\n$$\n\\text{Error}_{NE} \\approx (1.110223 \\times 10^{-16})(4 \\times 10^{16}) = 4.440892\n$$\nAn error bound of this magnitude (greater than $1$) indicates that the computed solution is expected to have no correct significant digits.\nThe problem requests the numerical value of this bound for the normal equations method, rounded to three significant figures.\n$4.440892 \\approx 4.44$.",
            "answer": "$$\n\\boxed{4.44}\n$$"
        }
    ]
}