{
    "hands_on_practices": [
        {
            "introduction": "In practical computation, we need reliable stopping criteria for iterative methods. While the stabilization of the Rayleigh quotient, $\\rho_k = x_k^\\top A x_k$, and the convergence of the iterate vector $x_k$ are related, they are not equivalent. This exercise explores the crucial relationship between these two metrics for symmetric matrices, demonstrating how the spectral gap dictates their connection . By analyzing specific numerical scenarios, you will see how one metric can suggest convergence while the other is still far from the true solution, a vital lesson for designing robust stopping rules.",
            "id": "3541815",
            "problem": "Consider the power iteration applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigenvalues $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ and corresponding orthonormal eigenvectors $v_1, v_2, \\ldots, v_n$. Let $x_k \\in \\mathbb{R}^n$ be the normalized iterate, the Rayleigh quotient be $\\rho_k = x_k^\\top A x_k$, and the principal angle $\\theta_k \\in [0,\\pi/2]$ be the angle between the one-dimensional subspace $\\operatorname{span}\\{x_k\\}$ and the dominant eigenspace $\\mathcal{E}_1 = \\operatorname{span}\\{v_1\\}$. You may assume basic linear algebra facts, such as the spectral theorem for real symmetric matrices and the definition of the Rayleigh quotient and principal angle.\n\nSelect all options that correctly construct explicit scenarios in which the Rayleigh quotient $\\rho_k$ appears converged while the principal angle $\\theta_k$ is not (and vice versa), and that correctly articulate the geometric reasons and sound implications for stopping rules in power iteration.\n\nA. Take $n=2$, $A = \\operatorname{diag}(1, 1 - 10^{-8})$, and $x_0 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$. Then $\\rho_0$ deviates from $\\lambda_1$ by only $5 \\cdot 10^{-9}$, whereas the principal angle $\\theta_0$ equals $\\pi/4$. This illustrates that when the top eigenvalues are tightly clustered, $\\rho_k$ can appear converged even though $\\theta_k$ is not small. A stopping rule based solely on the stabilization of $\\rho_k$ may therefore terminate prematurely; monitoring an angle proxy (e.g., a subspace gap) or a residual norm is advisable.\n\nB. Take $n=2$, $A = \\operatorname{diag}(10^6, 0)$, and $x_0 = (\\cos \\theta, \\sin \\theta)^\\top$ with $\\theta = 10^{-2}$ (in radians). Then $\\theta_0$ is small, yet $\\rho_0$ underestimates $\\lambda_1$ by about $100$, so a stopping rule that demands small absolute eigenvalue error could run much longer even though the direction has already essentially converged. This illustrates that a large spectral gap can make $\\rho_k$ lag in absolute terms while $\\theta_k$ is already tiny; an angle- or residual-based rule can be more faithful to subspace convergence.\n\nC. For real symmetric $A$, the quantities $|\\rho_k - \\lambda_1|$ and $\\sin \\theta_k$ are uniformly equivalent in the sense that there exists a constant $C > 0$ independent of the spectrum such that $C^{-1} |\\rho_k - \\lambda_1| \\le \\sin \\theta_k \\le C |\\rho_k - \\lambda_1|$ for all unit $x_k$. Hence, convergence of $\\rho_k$ always implies convergence of $\\theta_k$, and conversely, at a comparable rate, regardless of the spectral gap.\n\nD. In $n=2$ with $A = \\operatorname{diag}(1, 1 - 10^{-8})$, any unit $x$ satisfying $|\\rho(x) - \\lambda_1| \\le 10^{-9}$ must obey $\\theta \\le 10^{-4}$. Therefore, when the Rayleigh quotient is within $10^{-9}$ of the top eigenvalue, the principal angle is necessarily tiny even under eigenvalue clustering.\n\nChoose all that apply.",
            "solution": "The user-provided problem statement has been validated and found to be sound. It is a well-posed, scientifically-grounded problem within the field of numerical linear algebra. The language is objective and the provided information is internally consistent and sufficient for a rigorous analysis. We may therefore proceed with the solution.\n\nThe core of this problem lies in the relationship between the Rayleigh quotient $\\rho(x) = x^\\top A x$ and the principal angle $\\theta$ for a unit vector $x$ relative to the dominant eigenvector $v_1$ of a real symmetric matrix $A$. Let the eigenvalues of $A \\in \\mathbb{R}^{n \\times n}$ be $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ with a corresponding orthonormal eigenbasis $\\{v_1, v_2, \\ldots, v_n\\}$.\n\nAny unit vector $x \\in \\mathbb{R}^n$ can be expressed in this eigenbasis as $x = \\sum_{i=1}^n c_i v_i$, where $\\sum_{i=1}^n c_i^2 = 1$. The principal angle $\\theta$ between $x$ and the dominant eigenvector $v_1$ is defined by $\\cos \\theta = |x^\\top v_1| = |c_1|$. As $\\theta \\in [0, \\pi/2]$, we have $\\cos \\theta \\ge 0$. The relationship $\\cos^2 \\theta + \\sin^2 \\theta = 1$ gives $\\sin^2 \\theta = 1 - c_1^2 = \\sum_{i=2}^n c_i^2$.\n\nThe Rayleigh quotient for $x$ is:\n$$ \\rho(x) = x^\\top A x = \\left(\\sum_{i=1}^n c_i v_i\\right)^\\top A \\left(\\sum_{j=1}^n c_j v_j\\right) = \\left(\\sum_{i=1}^n c_i v_i\\right)^\\top \\left(\\sum_{j=1}^n c_j \\lambda_j v_j\\right) $$\nDue to the orthonormality of the eigenvectors ($v_i^\\top v_j = \\delta_{ij}$), this simplifies to:\n$$ \\rho(x) = \\sum_{i=1}^n c_i^2 \\lambda_i = c_1^2 \\lambda_1 + \\sum_{i=2}^n c_i^2 \\lambda_i $$\nThe error in the Rayleigh quotient as an approximation to $\\lambda_1$ is:\n$$ \\lambda_1 - \\rho(x) = \\lambda_1 - \\left(c_1^2 \\lambda_1 + \\sum_{i=2}^n c_i^2 \\lambda_i\\right) = (1 - c_1^2)\\lambda_1 - \\sum_{i=2}^n c_i^2 \\lambda_i $$\nSubstituting $1 - c_1^2 = \\sum_{i=2}^n c_i^2$, we obtain the fundamental relation:\n$$ \\lambda_1 - \\rho(x) = \\left(\\sum_{i=2}^n c_i^2\\right)\\lambda_1 - \\sum_{i=2}^n c_i^2 \\lambda_i = \\sum_{i=2}^n c_i^2 (\\lambda_1 - \\lambda_i) $$\nUsing $\\sin^2 \\theta = \\sum_{i=2}^n c_i^2$:\n$$ \\lambda_1 - \\rho(x) = \\sum_{i=2}^n c_i^2 (\\lambda_1 - \\lambda_i) $$\nSince $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_n$, we have $(\\lambda_1 - \\lambda_2) \\ge (\\lambda_1 - \\lambda_i)$ for $i \\ge 2$, but we also have $(\\lambda_1 - \\lambda_i) \\ge (\\lambda_1 - \\lambda_n)$. This allows us to bound the error:\n$$ (\\lambda_1 - \\lambda_2) \\sum_{i=2}^n c_i^2 \\le \\lambda_1 - \\rho(x) \\le (\\lambda_1 - \\lambda_n) \\sum_{i=2}^n c_i^2 $$\nThis yields the well-known inequality relating the Rayleigh quotient error to the angle:\n$$ (\\lambda_1 - \\lambda_2) \\sin^2 \\theta \\le \\lambda_1 - \\rho(x) \\le (\\lambda_1 - \\lambda_n) \\sin^2 \\theta $$\nNote that since $\\lambda_1$ is the maximum eigenvalue, $\\rho(x) \\le \\lambda_1$, so $\\lambda_1 - \\rho(x) = |\\rho(x) - \\lambda_1|$. This inequality is central to evaluating the options.\n\n### Evaluation of Options\n\n**Option A:**\n- **Scenario:** $n=2$, $A = \\operatorname{diag}(1, 1 - 10^{-8})$, $x_0 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$.\n- **Analysis:** We have $\\lambda_1 = 1$ with $v_1 = (1, 0)^\\top$ and $\\lambda_2 = 1 - 10^{-8}$ with $v_2 = (0, 1)^\\top$. The initial vector is $x_0 = \\frac{1}{\\sqrt{2}}v_1 + \\frac{1}{\\sqrt{2}}v_2$.\n- **Principal Angle $\\theta_0$**: The coefficient of $v_1$ is $c_1 = 1/\\sqrt{2}$. Thus, $\\cos \\theta_0 = |c_1| = 1/\\sqrt{2}$, which gives $\\theta_0 = \\pi/4$. This is correct.\n- **Rayleigh Quotient $\\rho_0$**: For a $2 \\times 2$ case, the inequality becomes an equality: $\\lambda_1 - \\rho_0 = (\\lambda_1 - \\lambda_2) \\sin^2 \\theta_0$.\n  The spectral gap is $\\lambda_1 - \\lambda_2 = 1 - (1 - 10^{-8}) = 10^{-8}$.\n  We have $\\sin^2 \\theta_0 = \\sin^2(\\pi/4) = (1/\\sqrt{2})^2 = 1/2$.\n  Therefore, the deviation is $\\lambda_1 - \\rho_0 = (10^{-8})(1/2) = 5 \\cdot 10^{-9}$. This is also correct.\n- **Conclusion:** The scenario demonstrates that with a very small spectral gap ($10^{-8}$), the Rayleigh quotient error ($5 \\cdot 10^{-9}$) can be extremely small, suggesting convergence, while the vector is maximally misaligned ($\\theta_0 = \\pi/4$). The reasoning that stopping rules based solely on $\\rho_k$ can be misleading in cases of eigenvalue clustering is sound and a standard cautionary principle in numerical methods. The recommendation to monitor other quantities is appropriate.\n- **Verdict:** **Correct**.\n\n**Option B:**\n- **Scenario:** $n=2$, $A = \\operatorname{diag}(10^6, 0)$, $x_0 = (\\cos \\theta, \\sin \\theta)^\\top$ with $\\theta = 10^{-2}$ radians.\n- **Analysis:** We have $\\lambda_1 = 10^6$ with $v_1 = (1, 0)^\\top$ and $\\lambda_2 = 0$ with $v_2 = (0, 1)^\\top$. The initial vector $x_0$ forms an angle $\\theta_0 = \\theta = 10^{-2}$ with $v_1$. This angle is small ($\\approx 0.57^\\circ$).\n- **Rayleigh Quotient $\\rho_0$**: Using the same equality as in Option A, $\\lambda_1 - \\rho_0 = (\\lambda_1 - \\lambda_2) \\sin^2 \\theta_0$.\n  The spectral gap is large: $\\lambda_1 - \\lambda_2 = 10^6 - 0 = 10^6$.\n  The angle is $\\theta_0 = 10^{-2}$. For small angles, $\\sin \\theta_0 \\approx \\theta_0$.\n  The deviation is $\\lambda_1 - \\rho_0 = 10^6 \\sin^2(10^{-2}) \\approx 10^6 (10^{-2})^2 = 10^6 \\cdot 10^{-4} = 100$. So $\\rho_0$ underestimates $\\lambda_1$ by approximately $100$. This is correct.\n- **Conclusion:** This scenario correctly illustrates the opposite case: a large spectral gap means the absolute error in the Rayleigh quotient can be large even when the principal angle is small. The vector has nearly converged in direction, but a stopping criterion based on a small absolute error in the eigenvalue would require many more iterations. The reasoning is sound.\n- **Verdict:** **Correct**.\n\n**Option C:**\n- **Statement:** The quantities $|\\rho_k - \\lambda_1|$ and $\\sin \\theta_k$ are uniformly equivalent, with a constant $C > 0$ independent of the spectrum, such that $C^{-1} |\\rho_k - \\lambda_1| \\le \\sin \\theta_k \\le C |\\rho_k - \\lambda_1|$.\n- **Analysis:** This statement is incorrect for two primary reasons.\n  1. The relationship is between $|\\rho_k - \\lambda_1|$ and $\\sin^2 \\theta_k$, not $\\sin \\theta_k$. Specifically, $|\\rho_k - \\lambda_1| = O(\\sin^2 \\theta_k)$. For small $\\theta_k$, this means the convergence of the Rayleigh quotient is quadratic with respect to the convergence of the angle, which is fundamentally different from the linear relationship proposed.\n  2. The fundamental inequality $(\\lambda_1 - \\lambda_2) \\sin^2 \\theta \\le |\\rho(x) - \\lambda_1| \\le (\\lambda_1 - \\lambda_n) \\sin^2 \\theta$ shows that the proportionality \"constants\" are $(\\lambda_1 - \\lambda_2)$ and $(\\lambda_1 - \\lambda_n)$. These quantities, the spectral gaps, are explicitly dependent on the spectrum of $A$. Options A and B provide explicit counterexamples to the claim of a spectrum-independent constant.\n- **Verdict:** **Incorrect**.\n\n**Option D:**\n- **Scenario:** $n=2$, $A = \\operatorname{diag}(1, 1 - 10^{-8})$. The claim is that any unit vector $x$ with $|\\rho(x) - \\lambda_1| \\le 10^{-9}$ must have an angle $\\theta \\le 10^{-4}$.\n- **Analysis:** For this matrix, we have the relation $|\\rho(x) - \\lambda_1| = (\\lambda_1-\\lambda_2)\\sin^2\\theta = 10^{-8} \\sin^2\\theta$.\n  The condition $|\\rho(x) - \\lambda_1| \\le 10^{-9}$ translates to:\n  $$ 10^{-8} \\sin^2\\theta \\le 10^{-9} $$\n  $$ \\sin^2\\theta \\le \\frac{10^{-9}}{10^{-8}} = 0.1 $$\n  Taking the square root, we find $\\sin\\theta \\le \\sqrt{0.1} \\approx 0.316$. Since $\\theta \\in [0, \\pi/2]$, this implies $\\theta \\le \\arcsin(\\sqrt{0.1}) \\approx 0.3217$ radians.\n  The statement claims that $\\theta$ must be less than or equal to $10^{-4}$. However, our calculation shows that $\\theta$ can be as large as $\\approx 0.3217$ radians, which is vastly larger than $10^{-4}$. For example, a vector with $\\theta = 0.3$ radians has $|\\rho(x) - \\lambda_1| = 10^{-8} \\sin^2(0.3) \\approx 8.73 \\cdot 10^{-10}$, which satisfies the condition $\\le 10^{-9}$. Yet, $0.3 \\gg 10^{-4}$.\n- **Conclusion:** The argument is mathematically false. A small Rayleigh quotient error does not guarantee a tiny angle when eigenvalues are tightly clustered. This option contradicts the conclusion of option A.\n- **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "The classical convergence rate of power iteration, governed by the ratio $|\\lambda_2 / \\lambda_1|$, describes the algorithm's long-term behavior. However, for non-normal matrices, the journey to this asymptotic regime can be surprisingly long and complex. This practice investigates the role of the eigenvector basis conditioning, measured by the condition number $\\kappa(V)$ of the eigenvector matrix $V$, in controlling this initial \"transient\" phase of convergence . Through a concrete numerical experiment, you will witness how poor conditioning can trap the iteration for many steps, even when the eigenvalue gap appears favorable.",
            "id": "3541827",
            "problem": "Consider the classical power iteration for a diagonalizable matrix. Let a real matrix $A \\in \\mathbb{R}^{n \\times n}$ be diagonalizable as $A = V \\Lambda V^{-1}$, where $V$ has full column rank with columns equal to the right eigenvectors of $A$, and $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ contains the eigenvalues ordered so that $|\\lambda_1| > |\\lambda_2| \\ge \\cdots \\ge |\\lambda_n| \\ge 0$. Define the power iteration by $x_{k+1} = A x_k / \\|A x_k\\|_2$ for $k \\in \\mathbb{N}$, starting from a nonzero $x_0 \\in \\mathbb{R}^n$. Let the condition number of $V$ in the spectral norm be $\\kappa_2(V) = \\|V\\|_2 \\cdot \\|V^{-1}\\|_2$, where $\\|\\cdot\\|_2$ is the matrix norm induced by the Euclidean vector norm. The Singular Value Decomposition (SVD) refers to the factorization of a real matrix into $U \\Sigma W^\\top$ where the diagonal entries of $\\Sigma$ are the singular values; $\\kappa_2(V)$ equals the ratio of the largest to the smallest singular value of $V$. The classical asymptotic theory of the power iteration states that in exact arithmetic, the $k$th iterate $x_k$ tends to the dominant right eigenvector direction at a rate governed by $|\\lambda_2 / \\lambda_1|$. However, the practical number of iterations needed to reach a useful approximation depends on the initial expansion of $x_0$ in the eigenvector basis, which in turn is sensitive to the conditioning of $V$. Your task is to construct matrices with a large spectral gap $|\\lambda_2 / \\lambda_1| \\ll 1$ but with a very large $\\kappa_2(V)$, and to evaluate how many steps of the power iteration are needed before the non-dominant components, measured in the eigenvector basis, become negligible.\n\nUse the following precise setup, which starts from fundamental definitions and facts only. For each test, you must perform the normalized power iteration $x_{k+1} = A x_k / \\|A x_k\\|_2$. At iteration $k$, expand $x_k$ in the eigenvector basis by computing the coefficient vector $c^{(k)} = V^{-1} x_k \\in \\mathbb{R}^n$. Define the dominant coefficient magnitude $d^{(k)} = |c^{(k)}_1|$, and the worst contaminating magnitude $t^{(k)} = \\max_{j \\ge 2} |c^{(k)}_j|$. Declare convergence when the biorthogonal contamination ratio $r^{(k)} = t^{(k)} / d^{(k)}$ satisfies $r^{(k)} \\le \\tau$, with tolerance $\\tau = 10^{-8}$, or when a cap of $10{,}000$ iterations is reached. The angle unit for any angular quantities is irrelevant here because the criterion is entirely defined in terms of $r^{(k)}$, which is dimensionless.\n\nConstruct the test suite as follows. In each test, take the dominant eigenvalue $\\lambda_1 = 1$, and let all remaining eigenvalues equal the same value $\\lambda_2 = \\cdots = \\lambda_n = \\alpha$, with $\\alpha = 10^{-3}$. Take $\\varepsilon = 10^{-150}$. Let $v_1, \\dots, v_n$ denote the columns of $V$. For the initial vector, use the same concrete choice in standard coordinates for all tests: $x_0$ is the unit-normalized vector whose first two entries are $1 - \\varepsilon$ and $\\varepsilon$, respectively, and all remaining entries are $0$. This initial vector is independent of $V$ and is specified in the standard basis; do not construct $x_0$ from the eigenvectors.\n\nSpecify four test cases covering distinct facets:\n\n- Test A (well-conditioned, dimension $2$): Take $n = 2$ and $V = I_2$, the $2 \\times 2$ identity matrix, so that $\\kappa_2(V) = 1$. Construct $A = V \\Lambda V^{-1}$ with $\\Lambda = \\mathrm{diag}(1, \\alpha)$ and run power iteration starting from the specified $x_0$.\n\n- Test B (ill-conditioned, dimension $2$): Take $n = 2$ and define $V$ to have nearly collinear first two columns, $v_1 = e_1$ and $v_2 = e_1 + \\varepsilon e_2$. Construct $A = V \\Lambda V^{-1}$ with the same $\\Lambda$ as in Test A. This $V$ is invertible and has $\\kappa_2(V)$ on the order of $1/\\varepsilon$. Run power iteration from the same $x_0$ as in Test A.\n\n- Test C (well-conditioned, dimension $5$): Take $n = 5$ and $V = I_5$, the $5 \\times 5$ identity matrix, so that $\\kappa_2(V) = 1$. Let $\\Lambda = \\mathrm{diag}(1, \\alpha, \\alpha, \\alpha, \\alpha)$, and run power iteration from the same $x_0$ (padded with zeros to length $5$) as above.\n\n- Test D (ill-conditioned, dimension $5$): Take $n = 5$ and define $V$ with $v_1 = e_1$, $v_2 = e_1 + \\varepsilon e_2$, and $v_j = e_j$ for $j \\in \\{3,4,5\\}$. Construct $A = V \\Lambda V^{-1}$ with the same $\\Lambda$ as in Test C. This $V$ is invertible and has $\\kappa_2(V)$ on the order of $1/\\varepsilon$. Run power iteration from the same $x_0$ (padded appropriately) as above.\n\nFor each test, compute the minimal integer $k \\ge 0$ for which $r^{(k)} \\le \\tau$, or return $10{,}000$ if such $k$ is not reached within the cap. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"), where each entry is the integer iteration count for the corresponding test in the order A, B, C, D. No other text should be printed. All computations are in pure mathematics with no physical units involved; all reported integers are unitless.",
            "solution": "The user has provided a problem statement that has been validated and found to be scientifically grounded, well-posed, objective, and complete. The problem describes a numerical experiment to investigate the convergence behavior of the power iteration method under different conditioning scenarios of the eigenvector basis. The setup is sound and based on established principles of numerical linear algebra. We may therefore proceed with the solution.\n\nThe task is to determine the number of power iterations, $k$, required to satisfy a specific convergence criterion for four different matrices. The power iteration is defined by the sequence $x_{k+1} = A x_k / \\|A x_k\\|_2$, where $A \\in \\mathbb{R}^{n \\times n}$ is a diagonalizable matrix, $A = V \\Lambda V^{-1}$, and $x_0$ is a starting vector.\n\nThe convergence is measured in the eigenvector basis. At each iteration $k$, the current vector $x_k$ is expressed as a linear combination of the eigenvectors $v_j$ (the columns of $V$): $x_k = \\sum_{j=1}^n c^{(k)}_j v_j$. The coefficient vector is given by $c^{(k)} = V^{-1} x_k$. The criterion for convergence is based on the \"biorthogonal contamination ratio\", defined as $r^{(k)} = t^{(k)} / d^{(k)}$, where $d^{(k)} = |c^{(k)}_1|$ is the magnitude of the coefficient corresponding to the dominant eigenvector, and $t^{(k)} = \\max_{j \\ge 2} |c^{(k)}_j|$ is the largest magnitude among all other \"contaminating\" coefficients. Convergence is declared when $r^{(k)} \\le \\tau$, where the tolerance is $\\tau = 10^{-8}$. The maximum number of iterations is capped at $10,000$.\n\nThe analysis proceeds by constructing the specified matrices and initial vector for each test case and then simulating the iteration process. An analytical examination is useful to predict the outcome. Let's analyze the evolution of the coefficient ratio $r^{(k)}$.\nFrom $x_{k+1} = A x_k / \\|A x_k\\|_2$, we can find the transformation of the coefficients:\n$c^{(k+1)} = V^{-1} x_{k+1} = V^{-1} \\frac{A x_k}{\\|A x_k\\|_2} = \\frac{V^{-1} (V \\Lambda V^{-1}) x_k}{\\|A x_k\\|_2} = \\frac{\\Lambda (V^{-1} x_k)}{\\|A x_k\\|_2} = \\frac{\\Lambda c^{(k)}}{\\|A x_k\\|_2}$.\nThe ratio between two coefficients $c^{(k+1)}_j$ and $c^{(k+1)}_1$ is:\n$$ \\frac{c^{(k+1)}_j}{c^{(k+1)}_1} = \\frac{\\lambda_j c^{(k)}_j / \\|A x_k\\|_2}{\\lambda_1 c^{(k)}_1 / \\|A x_k\\|_2} = \\frac{\\lambda_j}{\\lambda_1} \\frac{c^{(k)}_j}{c^{(k)}_1} $$\nThis shows that at each step, the ratio of any contaminating coefficient to the dominant one is multiplied by the corresponding eigenvalue ratio. By induction, we have:\n$$ \\frac{c^{(k)}_j}{c^{(k)}_1} = \\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k \\frac{c^{(k)}_0}{c^{(k)}_1} $$\nTaking magnitudes and the maximum over $j \\ge 2$:\n$$ r^{(k)} = \\frac{\\max_{j \\ge 2} |c^{(k)}_j|}{|c^{(k)}_1|} = \\max_{j \\ge 2} \\left| \\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k \\frac{c^{(0)}_j}{c^{(0)}_1} \\right| $$\nIn this problem, all non-dominant eigenvalues are equal, $\\lambda_2 = \\dots = \\lambda_n = \\alpha$. Thus, this simplifies to:\n$$ r^{(k)} = \\left|\\frac{\\alpha}{\\lambda_1}\\right|^k \\max_{j \\ge 2} \\left| \\frac{c^{(0)}_j}{c^{(0)}_1} \\right| = \\left|\\frac{\\alpha}{\\lambda_1}\\right|^k r^{(0)} $$\nThe number of iterations is therefore determined by the initial contamination ratio $r^{(0)}$ and the spectral gap. The parameters are $\\lambda_1 = 1$, $\\alpha = 10^{-3}$, $\\varepsilon = 10^{-150}$, and $\\tau = 10^{-8}$.\n\nThe initial vector is the normalization of $x'_0$, where $x'_0$ has first two entries $1-\\varepsilon$ and $\\varepsilon$ and others zero. Since $\\varepsilon$ is extremely small, $\\|x'_0\\|_2 \\approx 1$, so $x_0 \\approx x'_0$.\n\n**Test A & C (Well-Conditioned Cases)**\nFor Test A ($n=2$) and Test C ($n=5$), the eigenvector basis is the standard basis, $V=I_n$. Thus $V^{-1}=I_n$.\nThe initial coefficient vector is $c^{(0)} = V^{-1} x_0 = x_0$.\nFor $n=5$, $x_0$ is the normalization of $[1-\\varepsilon, \\varepsilon, 0, 0, 0]^\\top$. The normalization factor is close to $1$.\n$c^{(0)}_1 \\approx 1-\\varepsilon$, $c^{(0)}_2 \\approx \\varepsilon$, and $c^{(0)}_{j>2}=0$.\nThe initial dominant coefficient magnitude is $d^{(0)} = |c^{(0)}_1| \\approx 1-\\varepsilon$.\nThe worst contaminating magnitude is $t^{(0)} = \\max(|c^{(0)}_2|, |c^{(0)}_3|, \\dots) = \\varepsilon$.\nThe initial ratio is $r^{(0)} = t^{(0)}/d^{(0)} \\approx \\varepsilon/(1-\\varepsilon) \\approx 10^{-150}$.\nSince $r^{(0)} \\approx 10^{-150} \\le 10^{-8} = \\tau$, the convergence condition is met at the start, for $k=0$. The result for both Test A and Test C is $0$.\n\n**Test B & D (Ill-Conditioned Cases)**\nFor Test B ($n=2$), the eigenvector matrix is $V = \\begin{pmatrix} 1 & 1 \\\\ 0 & \\varepsilon \\end{pmatrix}$. Its inverse is $V^{-1} = \\begin{pmatrix} 1 & -1/\\varepsilon \\\\ 0 & 1/\\varepsilon \\end{pmatrix}$.\nThe initial vector is $x_0 \\approx [1-\\varepsilon, \\varepsilon]^\\top$. The initial coefficient vector is:\n$$ c^{(0)} = V^{-1} x_0 \\approx \\begin{pmatrix} 1 & -1/\\varepsilon \\\\ 0 & 1/\\varepsilon \\end{pmatrix} \\begin{pmatrix} 1-\\varepsilon \\\\ \\varepsilon \\end{pmatrix} = \\begin{pmatrix} (1-\\varepsilon) - 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\varepsilon \\\\ 1 \\end{pmatrix} $$\nThe initial magnitudes are $d^{(0)} \\approx \\varepsilon$ and $t^{(0)} \\approx 1$.\nThe initial ratio is $r^{(0)} = t^{(0)}/d^{(0)} \\approx 1/\\varepsilon = 10^{150}$, which is extremely large.\nFor Test D ($n=5$), the matrices $V$ and $V^{-1}$ have the same $2 \\times 2$ block structure affecting the first two components, and are identities otherwise. The analysis for $c^{(0)}$ remains the same for the first two components, with other components being zero. Thus, $r^{(0)} \\approx 10^{150}$ applies here as well.\n\nWe need to find the number of iterations $k$ for these cases. Using the derived formula for $r^{(k)}$:\n$$ r^{(k)} = \\left|\\frac{\\alpha}{\\lambda_1}\\right|^k r^{(0)} \\le \\tau $$\n$$ (10^{-3})^k \\cdot 10^{150} \\le 10^{-8} $$\n$$ 10^{-3k} \\le 10^{-158} $$\nTaking $\\log_{10}$ of both sides:\n$$ -3k \\le -158 $$\n$$ k \\ge \\frac{158}{3} \\approx 52.667 $$\nThe minimal integer $k$ satisfying this is $53$. This result should hold for both Test B and Test D.\n\nThe final program will execute the power iteration numerically to confirm these analytical predictions. The results for tests A, B, C, and D are expected to be $0$, $53$, $0$, and $53$, respectively.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_power_iteration(n, V, Lambda, x0_unnormalized, tau, max_iter):\n    \"\"\"\n    Performs the power iteration for a given test case.\n\n    Args:\n        n (int): The dimension of the matrix.\n        V (np.ndarray): The eigenvector matrix.\n        Lambda (np.ndarray): The diagonal eigenvalue matrix.\n        x0_unnormalized (np.ndarray): The unnormalized initial vector.\n        tau (float): The convergence tolerance.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations to converge, or max_iter if not converged.\n    \"\"\"\n    # Construct the matrix A and pre-compute the inverse of V\n    A = V @ Lambda @ np.linalg.inv(V)\n    V_inv = np.linalg.inv(V)\n\n    # Normalize the initial vector x_0\n    norm_x0 = np.linalg.norm(x0_unnormalized)\n    if norm_x0 == 0:\n        # This case should not occur with the problem's setup\n        return max_iter\n    x_k = x0_unnormalized / norm_x0\n\n    for k in range(max_iter):  # Loop from k = 0 to 9999\n        # 1. Expand x_k in the eigenvector basis to get coefficients c_k\n        c_k = V_inv @ x_k\n        \n        # 2. Compute dominant and contaminating magnitudes\n        d_k = np.abs(c_k[0])\n\n        # Avoid division by zero if dominant component is negligible.\n        # Given the problem setup, d_k will be small but non-zero.\n        if d_k  np.finfo(np.float64).tiny:\n            # Contamination is effectively infinite; continue iterating.\n            pass\n        else:\n            t_k = np.max(np.abs(c_k[1:]))\n            r_k = t_k / d_k\n            \n            # 3. Check for convergence\n            if r_k = tau:\n                return k\n        \n        # 4. Perform the next power iteration step\n        x_next = A @ x_k\n        norm_next = np.linalg.norm(x_next)\n        \n        if norm_next == 0:\n            # This can happen if A is singular and x_k is in its null space.\n            # A is invertible in all test cases, so this is unexpected.\n            return max_iter\n            \n        x_k = x_next / norm_next\n        \n    # If the loop completes without converging, return the cap value\n    return max_iter\n    \ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases as specified in the problem statement.\n    \"\"\"\n    # Define common parameters\n    alpha = 1.0e-3\n    epsilon = 1.0e-150\n    tau = 1.0e-8\n    max_iter = 10000\n\n    results = []\n    \n    # Use float64 for all computations\n    dtype = np.float64\n\n    # --- Test Case A ---\n    n_A = 2\n    V_A = np.identity(n_A, dtype=dtype)\n    Lambda_A = np.diag(np.array([1.0, alpha], dtype=dtype))\n    x0_un_A = np.zeros(n_A, dtype=dtype)\n    x0_un_A[0] = 1.0 - epsilon\n    x0_un_A[1] = epsilon\n    results.append(run_power_iteration(n_A, V_A, Lambda_A, x0_un_A, tau, max_iter))\n\n    # --- Test Case B ---\n    n_B = 2\n    V_B = np.array([[1.0, 1.0], [0.0, epsilon]], dtype=dtype)\n    Lambda_B = np.diag(np.array([1.0, alpha], dtype=dtype))\n    x0_un_B = np.zeros(n_B, dtype=dtype)\n    x0_un_B[0] = 1.0 - epsilon\n    x0_un_B[1] = epsilon\n    results.append(run_power_iteration(n_B, V_B, Lambda_B, x0_un_B, tau, max_iter))\n\n    # --- Test Case C ---\n    n_C = 5\n    V_C = np.identity(n_C, dtype=dtype)\n    Lambda_C = np.diag(np.array([1.0, alpha, alpha, alpha, alpha], dtype=dtype))\n    x0_un_C = np.zeros(n_C, dtype=dtype)\n    x0_un_C[0] = 1.0 - epsilon\n    x0_un_C[1] = epsilon\n    results.append(run_power_iteration(n_C, V_C, Lambda_C, x0_un_C, tau, max_iter))\n\n    # --- Test Case D ---\n    n_D = 5\n    V_D = np.identity(n_D, dtype=dtype)\n    V_D[0, 1] = 1.0\n    V_D[1, 1] = epsilon\n    Lambda_D = np.diag(np.array([1.0, alpha, alpha, alpha, alpha], dtype=dtype))\n    x0_un_D = np.zeros(n_D, dtype=dtype)\n    x0_un_D[0] = 1.0 - epsilon\n    x0_un_D[1] = epsilon\n    results.append(run_power_iteration(n_D, V_D, Lambda_D, x0_un_D, tau, max_iter))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For non-normal matrices, eigenvalues alone do not tell the whole story. It is possible for the eigenvectors to be exquisitely sensitive to perturbations, even when the eigenvalues are stable. This practice provides a hands-on exploration of this phenomenon, a concept closely tied to the idea of pseudospectra . You will construct a special perturbation that leaves the eigenvalues of a matrix unchanged but causes a dramatic rotation in its dominant eigenvector, directly observing the fragility that characterizes many non-normal systems.",
            "id": "3541829",
            "problem": "Consider a square matrix $A \\in \\mathbb{R}^{n \\times n}$ with distinct eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ ordered so that $\\lvert \\lambda_1 \\rvert gt; \\lvert \\lambda_2 \\rvert \\ge \\dots \\ge \\lvert \\lambda_n \\rvert$. The power iteration is the algorithm that, given an initial vector $x_0 \\neq 0$, computes $x_{k+1} = A x_k / \\lVert A x_k \\rVert_2$ for $k = 0, 1, 2, \\dots$, with the aim of converging in direction to a dominant eigenvector $v_1$ associated with $\\lambda_1$. In non-normal matrices (those that do not commute with their transpose), eigenvectors can be highly sensitive to small perturbations, especially when eigenvalue gaps are small (near resonance). The goal is to design perturbations $A + \\delta E$ that leave eigenvalues nearly unchanged while rotating the dominant eigenvector $v_1$ substantially, and then analyze the sensitivity of power iteration outcomes to such perturbations.\n\nFundamental base:\n- Eigenpair definition: an eigenpair $(\\lambda, v)$ of $A$ satisfies $A v = \\lambda v$ and $v \\neq 0$.\n- Power iteration definition: with normalization at each step, $x_{k+1} = A x_k / \\lVert A x_k \\rVert_2$.\n- For triangular matrices, the eigenvalues are the diagonal entries.\n- For diagonalizable $A = V \\Lambda V^{-1}$, the power iteration evolves as $x_k \\propto \\sum_{i=1}^n c_i \\lambda_i^k v_i$, where the coefficients $c_i$ are the coordinates of $x_0$ in the eigenbasis.\n\nTasks:\n1. Construct matrices of the form $A = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\lambda_3)$ with prescribed real diagonal entries and perturbations $E$ that are strictly lower triangular (that is, $E_{ij} = 0$ unless $i gt; j$). For each test case below, use $E$ with $E_{21} = 1$ and $E_{ij} = 0$ for all other indices. Let $\\delta gt; 0$ be a scalar, and define the perturbed matrix $A_\\delta = A + \\delta E$.\n2. Because $A_\\delta$ is lower triangular with the same diagonal as $A$, the eigenvalues of $A_\\delta$ are identical to those of $A$. However, the eigenvectors of $A_\\delta$ can change significantly as functions of $\\delta$ and the eigenvalue gaps.\n3. For each matrix pair $(A, A_\\delta)$, compute:\n   - The dominant eigenvector $v_1(A)$ of $A$ and the dominant eigenvector $v_1(A_\\delta)$ of $A_\\delta$. Normalize both to unit $2$-norm.\n   - The angle in radians between $v_1(A)$ and $v_1(A_\\delta)$, defined as $\\theta_{\\mathrm{evec}} = \\arccos\\left(\\frac{\\lvert v_1(A)^\\top v_1(A_\\delta) \\rvert}{\\lVert v_1(A) \\rVert_2 \\lVert v_1(A_\\delta) \\rVert_2}\\right)$.\n   - The maximum absolute eigenvalue change $\\max\\_i \\lvert \\lambda_i(A) - \\lambda_i(A_\\delta) \\rvert$ using the sets of eigenvalues of $A$ and $A_\\delta$ sorted by descending magnitude.\n   - The power iteration outputs $y_K(A)$ and $y_K(A_\\delta)$ after $K$ steps starting from the same initial vector $x_0$ with all entries equal to $1$. Normalize at each step. Compute the angle in radians $\\theta_{\\mathrm{PI}} = \\arccos\\left(\\frac{\\lvert y_K(A)^\\top y_K(A_\\delta) \\rvert}{\\lVert y_K(A) \\rVert_2 \\lVert y_K(A_\\delta) \\rVert_2}\\right)$.\n\nAngle unit requirement:\n- All angles must be expressed in radians.\n\nTest suite:\n- Use $n = 3$. Let $x_0 = [1, 1, 1]^\\top$ and $K = 12$ power iteration steps. Consider the following three test cases:\n  1. Near-resonant, moderate perturbation:\n     - $\\lambda = [1.0, 0.995, 0.99]$, $\\delta = 0.005$, $E_{21} = 1$ and all other entries of $E$ zero.\n  2. Near-resonant, tiny perturbation:\n     - $\\lambda = [1.0, 0.995, 0.99]$, $\\delta = 10^{-6}$, $E_{21} = 1$ and all other entries of $E$ zero.\n  3. Well-separated spectrum, moderate perturbation:\n     - $\\lambda = [1.0, 0.9, 0.8]$, $\\delta = 0.005$, $E_{21} = 1$ and all other entries of $E$ zero.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one inner list per test case in the order listed above. Each inner list must contain three floating-point numbers rounded to six decimal places in the order $[\\theta_{\\mathrm{evec}}, \\max\\_i \\lvert \\lambda_i(A) - \\lambda_i(A_\\delta) \\rvert, \\theta_{\\mathrm{PI}}]$. For example: \"[[val11,val12,val13],[val21,val22,val23],[val31,val32,val33]]\".",
            "solution": "The posed problem is valid. It is a well-defined and scientifically grounded exercise in numerical linear algebra, exploring the sensitivity of eigenvectors and the power iteration method to perturbations in non-normal matrices. All parameters, conditions, and objectives are specified without ambiguity or contradiction.\n\nThe objective is to analyze the effects of a specific, structured perturbation on the dominant eigenvector and the outcome of the power iteration algorithm for a $3 \\times 3$ matrix. This analysis is performed for cases with near-resonant (closely spaced) and well-separated eigenvalues to highlight the role of eigenvalue gaps in system stability.\n\nLet the set of prescribed eigenvalues be $\\{\\lambda_1, \\lambda_2, \\lambda_3\\}$, ordered such that $\\lvert\\lambda_1\\rvert  \\lvert\\lambda_2\\rvert \\ge \\lvert\\lambda_3\\rvert$. The unperturbed matrix $A$ is diagonal:\n$$\nA = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\lambda_3) = \\begin{pmatrix} \\lambda_1  0  0 \\\\ 0  \\lambda_2  0 \\\\ 0  0  \\lambda_3 \\end{pmatrix}\n$$\nThe perturbation matrix $E$ is given as strictly lower triangular with $E_{21} = 1$ and all other entries being zero:\n$$\nE = \\begin{pmatrix} 0  0  0 \\\\ 1  0  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\nThe perturbed matrix, $A_\\delta$, for a scalar $\\delta  0$ is $A_\\delta = A + \\delta E$:\n$$\nA_\\delta = \\begin{pmatrix} \\lambda_1  0  0 \\\\ \\delta  \\lambda_2  0 \\\\ 0  0  \\lambda_3 \\end{pmatrix}\n$$\nThe matrix $A$ is normal (since it is symmetric), while $A_\\delta$ is non-normal for $\\delta \\neq 0$ because $A_\\delta A_\\delta^\\top \\neq A_\\delta^\\top A_\\delta$.\n\nFirst, we compute the maximum absolute eigenvalue change. Since both $A$ and $A_\\delta$ are triangular matrices, their eigenvalues are their diagonal entries. Thus, for both matrices, the set of eigenvalues is $\\{\\lambda_1, \\lambda_2, \\lambda_3\\}$. Consequently, the eigenvalue change is zero for all perturbations considered:\n$$\n\\max_i \\lvert \\lambda_i(A) - \\lambda_i(A_\\delta) \\rvert = 0\n$$\n\nNext, we determine the dominant eigenvectors and the angle between them.\nFor the diagonal matrix $A$, the eigenvectors are the standard basis vectors. The dominant eigenvalue is $\\lambda_1$, and its corresponding eigenvector is $v_1(A) = e_1 = [1, 0, 0]^\\top$. This vector has a unit $2$-norm.\n\nFor the perturbed matrix $A_\\delta$, we find the eigenvector $v = [v_x, v_y, v_z]^\\top$ corresponding to the eigenvalue $\\lambda_1$ by solving the system $(A_\\delta - \\lambda_1 I)v=0$:\n$$\n\\begin{pmatrix} \\lambda_1 - \\lambda_1  0  0 \\\\ \\delta  \\lambda_2 - \\lambda_1  0 \\\\ 0  0  \\lambda_3 - \\lambda_1 \\end{pmatrix} \\begin{pmatrix} v_x \\\\ v_y \\\\ v_z \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the system of equations:\n$1$. $0 \\cdot v_x = 0$, which is always true.\n$2$. $\\delta v_x + (\\lambda_2 - \\lambda_1) v_y = 0$.\n$3$. $(\\lambda_3 - \\lambda_1) v_z = 0$.\nSince the eigenvalues are distinct, $\\lambda_3 \\neq \\lambda_1$, which implies $v_z=0$. From the second equation, we have $v_y = \\frac{\\delta}{\\lambda_1 - \\lambda_2} v_x$. By choosing $v_x=1$, we obtain an unnormalized eigenvector $[1, \\frac{\\delta}{\\lambda_1 - \\lambda_2}, 0]^\\top$. After normalization, the dominant eigenvector $v_1(A_\\delta)$ is:\n$$\nv_1(A_\\delta) = \\frac{1}{\\sqrt{1 + \\left(\\frac{\\delta}{\\lambda_1 - \\lambda_2}\\right)^2}} \\begin{pmatrix} 1 \\\\ \\frac{\\delta}{\\lambda_1 - \\lambda_2} \\\\ 0 \\end{pmatrix}\n$$\nThe angle $\\theta_{\\mathrm{evec}}$ between the unit-norm vectors $v_1(A) = [1, 0, 0]^\\top$ and $v_1(A_\\delta)$ is given by:\n$$\n\\theta_{\\mathrm{evec}} = \\arccos\\left(\\lvert v_1(A)^\\top v_1(A_\\delta) \\rvert\\right) = \\arccos\\left(\\frac{1}{\\sqrt{1 + \\left(\\frac{\\delta}{\\lambda_1 - \\lambda_2}\\right)^2}}\\right)\n$$\nUsing the trigonometric identity $\\arccos(1/\\sqrt{1+t^2}) = \\arctan(\\lvert t \\rvert)$ for real $t$, this simplifies to:\n$$\n\\theta_{\\mathrm{evec}} = \\arctan\\left(\\left\\lvert \\frac{\\delta}{\\lambda_1 - \\lambda_2} \\right\\rvert\\right)\n$$\nThis formula reveals that the eigenvector sensitivity is governed by the ratio of the perturbation magnitude $\\delta$ to the spectral gap $\\lambda_1 - \\lambda_2$.\n\nFinally, we analyze the power iteration. Starting with $x_0 = [1, 1, 1]^\\top$, we compute $y_K(A)$ and $y_K(A_\\delta)$ after $K=12$ iterations using the rule $x_{k+1} = M x_k / \\lVert M x_k \\rVert_2$. The angle $\\theta_{\\mathrm{PI}}$ is then calculated as $\\arccos(\\lvert y_K(A)^\\top y_K(A_\\delta) \\rvert)$.\n\nThe three test cases are now evaluated.\n\nCase 1: Near-resonant, moderate perturbation.\n$\\lambda = [1.0, 0.995, 0.99]$, $\\delta = 0.005$.\nThe eigenvalue gap is small: $\\lambda_1 - \\lambda_2 = 1.0 - 0.995 = 0.005$.\n$\\theta_{\\mathrm{evec}} = \\arctan\\left(\\frac{0.005}{0.005}\\right) = \\arctan(1) = \\pi/4 \\approx 0.785398$ radians.\nThe eigenvalue change is $0$.\nThe power iteration angle is found to be $\\theta_{\\mathrm{PI}} \\approx 0.697529$ radians.\nThe large rotation in both the true eigenvector and the power iteration output highlights the extreme sensitivity caused by the small eigenvalue gap, which amplifies the effect of the perturbation.\n\nCase 2: Near-resonant, tiny perturbation.\n$\\lambda = [1.0, 0.995, 0.99]$, $\\delta = 10^{-6}$.\nThe eigenvalue gap is still $0.005$.\n$\\theta_{\\mathrm{evec}} = \\arctan\\left(\\frac{10^{-6}}{0.005}\\right) = \\arctan(0.0002) \\approx 0.000200$ radians.\nThe eigenvalue change is $0$.\nThe power iteration angle is $\\theta_{\\mathrm{PI}} \\approx 0.000186$ radians.\nHere, despite the near-resonance, the perturbation is too small to cause a significant rotation. Both angles are very small.\n\nCase 3: Well-separated spectrum, moderate perturbation.\n$\\lambda = [1.0, 0.9, 0.8]$, $\\delta = 0.005$.\nThe eigenvalue gap is large: $\\lambda_1 - \\lambda_2 = 1.0 - 0.9 = 0.1$.\n$\\theta_{\\mathrm{evec}} = \\arctan\\left(\\frac{0.005}{0.1}\\right) = \\arctan(0.05) \\approx 0.049958$ radians.\nThe eigenvalue change is $0$.\nThe power iteration angle is $\\theta_{\\mathrm{PI}} \\approx 0.049618$ radians.\nThe large spectral gap provides robustness, and the same perturbation as in Case 1 now causes only a very small rotation of the eigenvector and the power iteration output. Convergence of the power iteration is also faster, so $\\theta_{\\mathrm{PI}}$ is a closer approximation to $\\theta_{\\mathrm{evec}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_power_iteration(M, x0, K):\n    \"\"\"\n    Performs K steps of the power iteration algorithm.\n    x_{k+1} = M x_k / ||M x_k||_2\n    \"\"\"\n    x = x0.copy().astype(float)\n    for _ in range(K):\n        Mx = M @ x\n        norm_Mx = np.linalg.norm(Mx)\n        if norm_Mx == 0:\n            # This case implies the zero vector is reached, which shouldn't happen here.\n            return x\n        x = Mx / norm_Mx\n    return x\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda_list, delta_value)\n        ([1.0, 0.995, 0.99], 0.005),\n        ([1.0, 0.995, 0.99], 1e-6),\n        ([1.0, 0.9, 0.8], 0.005),\n    ]\n\n    # Shared parameters for all test cases\n    K = 12\n    x0 = np.array([1.0, 1.0, 1.0])\n\n    all_results = []\n    for lambdas, delta in test_cases:\n        # Construct the matrices A (diagonal) and A_delta (perturbed)\n        A = np.diag(lambdas)\n        A_delta = np.array([\n            [lambdas[0], 0.0, 0.0],\n            [delta, lambdas[1], 0.0],\n            [0.0, 0.0, lambdas[2]]\n        ])\n\n        # Task 1: Compute angle between dominant eigenvectors, theta_evec.\n        # The analytical formula derived from the eigenvector structure is used.\n        # v1(A) = e1 = [1,0,0]^T\n        # v1(A_delta) is proportional to [1, delta/(lambda1-lambda2), 0]^T\n        # The angle is arctan(|delta/(lambda1-lambda2)|) in radians.\n        theta_evec = np.arctan(np.abs(delta / (lambdas[0] - lambdas[1])))\n\n        # Task 2: Compute max absolute eigenvalue change.\n        # For triangular matrices, eigenvalues are the diagonal entries.\n        # The diagonals are unchanged by the perturbation, so the change is 0.\n        max_eig_change = 0.0\n\n        # Task 3: Compute angle between power iteration outputs, theta_PI.\n        yK_A = run_power_iteration(A, x0, K)\n        yK_A_delta = run_power_iteration(A_delta, x0, K)\n        \n        # Calculate angle: arccos(|u.v| / (||u|| ||v||)).\n        # Since outputs are unit vectors, the denominator is 1.\n        dot_product = np.abs(np.dot(yK_A, yK_A_delta))\n        # Clamp to 1.0 to prevent domain errors from floating point inaccuracies.\n        dot_product = min(dot_product, 1.0)\n        theta_PI = np.arccos(dot_product)\n\n        all_results.append([theta_evec, max_eig_change, theta_PI])\n\n    # Final print statement in the exact required format.\n    # \"[[val11,val12,val13],[val21,val22,val23],[val31,val32,val33]]\"\n    final_output = f\"[{','.join([f'[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]' for r in all_results])}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}