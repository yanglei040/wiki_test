{
    "hands_on_practices": [
        {
            "introduction": "The Implicit Q theorem fundamentally guarantees that the Hessenberg decomposition of a matrix is essentially unique. This practice explores this core principle by examining two different orthogonal transformations, defined by matrices $Q$ and $\\widehat{Q}$, that both yield a Hessenberg matrix from a starting matrix $A$. You will prove that if the starting vectors are the same and the resulting Hessenberg matrices are unreduced, the two transformation matrices are related by a simple diagonal matrix of signs, a direct consequence of the theorem .",
            "id": "3589427",
            "problem": "Let $A \\in \\mathbb{R}^{5 \\times 5}$ be the upper Hessenberg matrix\n$$\nA=\\begin{pmatrix}\n2 & 3 & 0 & 0 & 0 \\\\\n4 & 1 & -1 & 0 & 0 \\\\\n0 & 2 & 0 & 5 & 0 \\\\\n0 & 0 & -3 & 4 & 6 \\\\\n0 & 0 & 0 & 1 & -2\n\\end{pmatrix},\n$$\nand let $Q=\\mathrm{I}_{5}$ and $\\widehat{Q}=\\mathrm{diag}(1,-1,-1,1,-1) \\in \\mathbb{R}^{5 \\times 5}$. Both $Q$ and $\\widehat{Q}$ are real orthogonal matrices. Define $H=Q^{\\top} A Q$ and $\\widehat{H}=\\widehat{Q}^{\\top} A \\widehat{Q}$. Note that $Q e_{1}=\\widehat{Q} e_{1}=e_{1}$, where $e_{1} \\in \\mathbb{R}^{5}$ is the first standard basis vector. Assume $H$ is unreduced, meaning all its subdiagonal entries are nonzero.\n\nUsing only core definitions about orthogonal similarity and the structural definition of an upper Hessenberg matrix, first derive why, under the conditions that $H$ and $\\widehat{H}$ are both upper Hessenberg and $Q e_{1}=\\widehat{Q} e_{1}$, the matrix $D=Q^{\\top} \\widehat{Q}$ must be diagonal with entries in $\\{\\pm 1\\}$.\n\nThen, for the specific $A$, $Q$, and $\\widehat{Q}$ above, compute $D$ and verify numerically that it is diagonal with entries $\\pm 1$. Finally, provide the value of $\\det(D)$. Express the final numeric answer exactly; no rounding is required.",
            "solution": "The problem is evaluated to be valid. It is scientifically grounded in numerical linear algebra, self-contained, and well-posed. All provided data and conditions are consistent and sufficient for a unique solution.\n\nThe problem asks for three things:\n1.  A theoretical derivation showing why the matrix $D = Q^{\\top} \\widehat{Q}$ must be a diagonal matrix with entries in $\\{\\pm 1\\}$ under the given conditions.\n2.  A numerical computation and verification of this property for the specific matrices provided.\n3.  The calculation of $\\det(D)$.\n\nWe will address these in order.\n\n### 1. Theoretical Derivation\n\nLet $A \\in \\mathbb{R}^{n \\times n}$. We are given two orthogonal similarity transformations of $A$ into upper Hessenberg form, $H$ and $\\widehat{H}$.\n$H = Q^{\\top} A Q$\n$\\widehat{H} = \\widehat{Q}^{\\top} A \\widehat{Q}$\nHere, $Q$ and $\\widehat{Q}$ are $n \\times n$ orthogonal matrices. The matrices $H$ and $\\widehat{H}$ are upper Hessenberg, meaning their entries $h_{ij}$ and $\\widehat{h}_{ij}$ are zero for $i > j+1$.\nWe are also given that $H$ is unreduced, which means all its first subdiagonal entries are non-zero: $h_{j+1, j} \\neq 0$ for $j=1, \\dots, n-1$.\nA crucial condition is that the first columns of $Q$ and $\\widehat{Q}$ are identical. Let $e_1$ be the first standard basis vector. The first column of $Q$ is $Qe_1$ and the first column of $\\widehat{Q}$ is $\\widehat{Q}e_1$. The condition is given as $Qe_1 = \\widehat{Q}e_1$. In the specific problem, this is $Qe_1 = \\widehat{Q}e_1 = e_1$, but the general proof holds for any identical first column.\n\nLet's define the matrix $D = Q^{\\top} \\widehat{Q}$. Since $Q$ and $\\widehat{Q}$ are orthogonal, their product $D$ is also orthogonal:\n$D^{\\top} D = (\\widehat{Q}^{\\top} Q) (Q^{\\top} \\widehat{Q}) = \\widehat{Q}^{\\top} (Q Q^{\\top}) \\widehat{Q} = \\widehat{Q}^{\\top} I \\widehat{Q} = \\widehat{Q}^{\\top} \\widehat{Q} = I$.\n\nFrom the definitions of $H$ and $\\widehat{H}$, we can express $A$ as $A = Q H Q^{\\top}$ and $A = \\widehat{Q} \\widehat{H} \\widehat{Q}^{\\top}$.\nEquating these two expressions for $A$:\n$Q H Q^{\\top} = \\widehat{Q} \\widehat{H} \\widehat{Q}^{\\top}$\nLeft-multiply by $Q^{\\top}$:\n$H Q^{\\top} = (Q^{\\top} \\widehat{Q}) \\widehat{H} \\widehat{Q}^{\\top}$\nRight-multiply by $\\widehat{Q}$:\n$H (Q^{\\top} \\widehat{Q}) = (Q^{\\top} \\widehat{Q}) \\widehat{H} (\\widehat{Q}^{\\top} \\widehat{Q})$\nSubstituting $D = Q^{\\top} \\widehat{Q}$:\n$H D = D \\widehat{H}$. This is known as the Francis-Kublanovskaya equation.\n\nNow, let's analyze the structure of $D$.\nThe condition $Qe_1 = \\widehat{Q}e_1$ can be rewritten by left-multiplying by $Q^{\\top}$:\n$Q^{\\top} Q e_1 = Q^{\\top} \\widehat{Q} e_1$\n$I e_1 = D e_1$\n$e_1 = D e_1$\nThis means the first column of the matrix $D$ is the vector $e_1 = (1, 0, \\dots, 0)^{\\top}$.\n\nLet's proceed by induction on the columns of $D$. Let the columns of $D$ be $d_1, d_2, \\dots, d_n$. We have shown $d_1 = e_1$.\nThe equation $HD = D\\widehat{H}$ can be written column by column. For the $j$-th column:\n$H d_j = (D\\widehat{H})_j = \\sum_{k=1}^{n} d_k \\widehat{h}_{kj}$\nSince $\\widehat{H}$ is upper Hessenberg, $\\widehat{h}_{kj} = 0$ for $k > j+1$. Thus, the sum truncates:\n$H d_j = \\sum_{k=1}^{j+1} d_k \\widehat{h}_{kj} = d_1 \\widehat{h}_{1j} + \\dots + d_j \\widehat{h}_{jj} + d_{j+1} \\widehat{h}_{j+1,j}$\n\nLet's apply this for $j=1$:\n$H d_1 = d_1 \\widehat{h}_{11} + d_2 \\widehat{h}_{21}$\nSince $d_1 = e_1$, this becomes:\n$H e_1 = e_1 \\widehat{h}_{11} + d_2 \\widehat{h}_{21}$\nThe vector $H e_1$ is the first column of $H$. As $H$ is upper Hessenberg, this column is $(h_{11}, h_{21}, 0, \\dots, 0)^{\\top}$.\nSo, $\\begin{pmatrix} h_{11} \\\\ h_{21} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\widehat{h}_{11} + d_2 \\widehat{h}_{21}$.\nThis gives us a system of equations for the components of $d_2$. Let $d_2 = (d_{12}, d_{22}, \\dots, d_{n2})^{\\top}$.\n$h_{11} = \\widehat{h}_{11} + d_{12} \\widehat{h}_{21}$\n$h_{21} = d_{22} \\widehat{h}_{21}$\n$0 = d_{i2} \\widehat{h}_{21}$ for $i > 2$.\n\nBefore we can solve for $d_2$, we must establish that $\\widehat{h}_{21} \\neq 0$.\nWe have $HD = D\\widehat{H}$, so $H = D\\widehat{H}D^{\\top}$.\n$h_{j+1,j} = e_{j+1}^{\\top} H e_j = e_{j+1}^{\\top} (D \\widehat{H} D^{\\top}) e_j$.\nLet's assume for induction that $d_k = \\sigma_k e_k$ for $k=1, \\dots, j$, where $|\\sigma_k|=1$. We know $d_1 = e_1$.\nThen $D^{\\top}e_j = (d_1, \\dots, d_j, \\dots, d_n)^{\\top} e_j = d_j^{\\top} = \\sigma_j e_j^{\\top}$.\nAnd $e_{j+1}^{\\top} D = (e_{j+1}^{\\top} d_1, \\dots, e_{j+1}^{\\top} d_n) = (0, \\dots, 0, e_{j+1}^{\\top} d_{j+1}, \\dots)$.\nLet's continue the argument from $H d_j = \\dots$.\nAssume for induction that for $k \\le j$, $d_k = \\sigma_k e_k$ where $\\sigma_k \\in \\{\\pm 1\\}$ and $\\sigma_1 = 1$.\nThis means $D$ is block diagonal up to column $j$, $D_j = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_j)$.\nLet's analyze the $(j)$-th column relation:\n$\\widehat{h}_{j+1,j} d_{j+1} = H d_j - \\sum_{k=1}^{j} d_k \\widehat{h}_{kj} = H (\\sigma_j e_j) - \\sum_{k=1}^{j} (\\sigma_k e_k) \\widehat{h}_{kj}$\n$\\widehat{h}_{j+1,j} d_{j+1} = \\sigma_j \\sum_{i=1}^{j+1} h_{ij} e_i - \\sum_{k=1}^{j} \\sigma_k \\widehat{h}_{kj} e_k$\n$\\widehat{h}_{j+1,j} d_{j+1} = \\sum_{k=1}^{j} (\\sigma_j h_{kj} - \\sigma_k \\widehat{h}_{kj}) e_k + \\sigma_j h_{j+1,j} e_{j+1}$\n\nSince $D$ is an orthogonal matrix, its columns are orthonormal. Since $d_k = \\sigma_k e_k$ for $k \\le j$, $d_{j+1}$ must be orthogonal to $e_1, \\dots, e_j$. This means the first $j$ components of $d_{j+1}$ must be zero.\nLooking at our expression for $\\widehat{h}_{j+1,j} d_{j+1}$, its components for indices $k \\le j$ must be zero.\nThis implies $\\sigma_j h_{kj} - \\sigma_k \\widehat{h}_{kj} = 0$ for $k \\le j$.\nTherefore, the expression for $d_{j+1}$ simplifies to:\n$\\widehat{h}_{j+1,j} d_{j+1} = \\sigma_j h_{j+1,j} e_{j+1}$\n\nSince $H$ is unreduced, $h_{j+1,j} \\neq 0$. This implies that the RHS is a non-zero vector.\nThus, we must have $\\widehat{h}_{j+1,j} \\neq 0$ (so $\\widehat{H}$ is also unreduced) and we can solve for $d_{j+1}$:\n$d_{j+1} = \\frac{\\sigma_j h_{j+1,j}}{\\widehat{h}_{j+1,j}} e_{j+1}$\nSince $d_{j+1}$ must be a unit vector (column of an orthogonal matrix), the scalar coefficient must have an absolute value of $1$.\nSo, $d_{j+1} = \\sigma_{j+1} e_{j+1}$ where $\\sigma_{j+1} = \\pm 1$.\nThe induction holds for $j=1, \\dots, n-1$. So $d_k = \\sigma_k e_k$ for $k=1, \\dots, n$.\nThis means $D$ is a diagonal matrix whose diagonal entries are $d_{kk} = \\sigma_k \\in \\{\\pm 1\\}$.\nThis completes the theoretical derivation. This result is a key part of the Implicit Q theorem, which establishes the essential uniqueness of the Hessenberg decomposition.\n\n### 2. Numerical Verification\n\nWe are given the specific matrices:\n$Q = \\mathrm{I}_{5} = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\end{pmatrix}$\n$\\widehat{Q} = \\mathrm{diag}(1,-1,-1,1,-1) = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & -1 \\end{pmatrix}$\n\nWe need to compute $D = Q^{\\top} \\widehat{Q}$.\nSince $Q = I_5$, its transpose is also the identity matrix, $Q^{\\top} = I_5$.\nTherefore, $D = I_5 \\widehat{Q} = \\widehat{Q}$.\n$D = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & -1 \\end{pmatrix}$\n\nThis computed matrix $D$ is indeed a diagonal matrix, and all its diagonal entries are either $1$ or $-1$. This numerically verifies the conclusion of the theoretical derivation.\n\n### 3. Computation of $\\det(D)$\n\nThe determinant of a diagonal matrix is the product of its diagonal entries.\nThe diagonal entries of $D$ are $1, -1, -1, 1, -1$.\n$\\det(D) = (1) \\times (-1) \\times (-1) \\times (1) \\times (-1)$\n$\\det(D) = (1) \\times (1) \\times (-1)$\n$\\det(D) = -1$",
            "answer": "$$\\boxed{-1}$$"
        },
        {
            "introduction": "A powerful theorem is often defined by its boundaries, and the Implicit Q theorem is no exception. Its guarantee of uniqueness hinges on the \"unreduced\" nature of the Hessenberg matrix, meaning all its subdiagonal entries must be nonzero. This exercise invites you to construct a direct counterexample, showing that when a subdiagonal entry is zero, a non-trivial family of similarity transformations can preserve the Hessenberg form, thus violating uniqueness .",
            "id": "3589422",
            "problem": "Consider the definitions: an upper Hessenberg matrix is a square matrix in which all entries below the first subdiagonal are zero; an unreduced upper Hessenberg matrix is one in which every subdiagonal entry is nonzero; a reduced upper Hessenberg matrix is one with at least one zero subdiagonal entry; a unitary matrix is a square matrix $\\mathcal{U}$ satisfying $\\mathcal{U}^{*}\\mathcal{U}=I$, where $I$ is the identity matrix and $\\mathcal{U}^{*}$ denotes the conjugate transpose. The implicit $Q$ theorem asserts a uniqueness property for unitary similarities that preserve unreduced Hessenberg form under suitable conditions, and its unreducedness hypothesis is necessary. Your task is to explicitly construct a counterexample to uniqueness when unreducedness fails, and compute a scalar that certifies the construction.\n\nLet\n$$\nH=\\begin{pmatrix}\n0 & 1 & 4 & 0 \\\\\n2 & 0 & 5 & 6 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 1 & 3\n\\end{pmatrix},\n$$\nwhich is an upper Hessenberg matrix with a zero subdiagonal entry at position $(3,2)$, hence reduced. For an angle $\\theta\\in\\mathbb{R}$, define the unitary family\n$$\nU(\\theta)=\\operatorname{diag}\\!\\big(1,\\,1,\\,R(\\theta)\\big),\\quad R(\\theta)=\\begin{pmatrix}\\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta\\end{pmatrix}.\n$$\nClearly, $U(\\theta)e_{1}=e_{1}$ for all $\\theta$. You will show that $U(\\theta)^{*}HU(\\theta)$ is upper Hessenberg for all $\\theta$, yet for a nonzero choice of $\\theta$, $U(\\theta)$ is not diagonal.\n\nDerive from first principles the structural invariance that guarantees upper Hessenberg form is preserved by this similarity, and then determine the unique $\\theta\\in(0,\\pi/2)$ (in radians) such that the $(4,3)$ entry of $U(\\theta)^{*}HU(\\theta)$ equals $0$. Provide your final answer as a closed-form analytic expression for $\\theta$ in radians. No rounding is required, and no units should be included inside the final boxed expression. The details of the construction and derivation must be justified from the stated definitions and fundamental properties of unitary similarity and block-partitioned matrices.",
            "solution": "The problem requires the construction of a counterexample to the uniqueness clause of the implicit $Q$ theorem when the unreduced Hessenberg hypothesis is not met. We are given a reduced upper Hessenberg matrix $H$ and a family of unitary matrices $U(\\theta)$, and we must show that the similarity transformation $U(\\theta)^* H U(\\theta)$ preserves the upper Hessenberg form for all $\\theta$, demonstrating a failure of uniqueness. Finally, we must calculate a specific value of $\\theta$ that results in a particular entry of the transformed matrix being zero.\n\nFirst, we validate the given information. The matrix $H$ is given by\n$$\nH=\\begin{pmatrix}\n0 & 1 & 4 & 0 \\\\\n2 & 0 & 5 & 6 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 1 & 3\n\\end{pmatrix}.\n$$\nThis is an upper Hessenberg matrix because all entries for which $i > j+1$ are zero. Specifically, the entries $h_{31}$, $h_{41}$, and $h_{42}$ are zero. The matrix is reduced because the subdiagonal entry $h_{32}$ is $0$. The family of matrices $U(\\theta)$ is given by\n$$\nU(\\theta)=\\operatorname{diag}\\!\\big(1,\\,1,\\,R(\\theta)\\big) = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & \\cos\\theta & -\\sin\\theta \\\\\n0 & 0 & \\sin\\theta & \\cos\\theta\n\\end{pmatrix}.\n$$\nFor each $\\theta \\in \\mathbb{R}$, $U(\\theta)$ is a real orthogonal matrix, hence unitary. Its conjugate transpose is its transpose, $U(\\theta)^* = U(\\theta)^T$. The condition $U(\\theta)e_1 = e_1$ is satisfied for all $\\theta$, as the first column of $U(\\theta)$ is $(1,0,0,0)^T$.\n\nThe crucial structural feature of this problem arises from the zero subdiagonal entry $h_{32}=0$. This allows us to partition the matrices $H$ and $U(\\theta)$ conformally. Let's use a $2 \\times 2$ block structure, where each block is a $2 \\times 2$ matrix.\n$$\nH = \\begin{pmatrix} H_{11} & H_{12} \\\\ H_{21} & H_{22} \\end{pmatrix}, \\quad U(\\theta) = \\begin{pmatrix} I_2 & 0 \\\\ 0 & R(\\theta) \\end{pmatrix},\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix and $0$ is the $2 \\times 2$ zero matrix. The blocks of $H$ are:\n$$\nH_{11} = \\begin{pmatrix} 0 & 1 \\\\ 2 & 0 \\end{pmatrix}, \\quad H_{12} = \\begin{pmatrix} 4 & 0 \\\\ 5 & 6 \\end{pmatrix}, \\quad H_{21} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad H_{22} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}.\n$$\nThe key observation is that $H_{21}$ is the zero matrix. This is a direct consequence of the entries $h_{31}, h_{41}, h_{32}, h_{42}$ being zero. In particular, the prescribed zero at $h_{32}$ is essential for this block structure.\n\nNow, let's compute the similarity transformation $\\tilde{H}(\\theta) = U(\\theta)^*HU(\\theta)$. Since $U(\\theta)$ is real, $U(\\theta)^* = U(\\theta)^T$.\n$$\n\\tilde{H}(\\theta) = \\begin{pmatrix} I_2 & 0 \\\\ 0 & R(\\theta)^T \\end{pmatrix} \\begin{pmatrix} H_{11} & H_{12} \\\\ H_{21} & H_{22} \\end{pmatrix} \\begin{pmatrix} I_2 & 0 \\\\ 0 & R(\\theta) \\end{pmatrix}.\n$$\nUsing the rules of block matrix multiplication:\n$$\n\\tilde{H}(\\theta) = \\begin{pmatrix} I_2 H_{11} I_2 & I_2 H_{12} R(\\theta) \\\\ R(\\theta)^T H_{21} I_2 & R(\\theta)^T H_{22} R(\\theta) \\end{pmatrix} = \\begin{pmatrix} H_{11} & H_{12} R(\\theta) \\\\ R(\\theta)^T H_{21} & R(\\theta)^T H_{22} R(\\theta) \\end{pmatrix}.\n$$\nSince $H_{21}$ is the zero matrix, $R(\\theta)^T H_{21}$ is also the zero matrix. Therefore, the transformed matrix has the form:\n$$\n\\tilde{H}(\\theta) = \\begin{pmatrix} H_{11} & H_{12} R(\\theta) \\\\ 0 & R(\\theta)^T H_{22} R(\\theta) \\end{pmatrix}.\n$$\nThis is a block upper triangular matrix. The zero in the $(2,1)$ block position means that the entries $\\tilde{h}_{31}, \\tilde{h}_{41}, \\tilde{h}_{32}, \\tilde{h}_{42}$ are all zero. The conditions for an upper Hessenberg matrix are that entries $\\tilde{h}_{ij}$ are zero for $i > j+1$. The zero block ensures this is the case for $(i,j) \\in \\{(3,1), (4,1), (4,2)\\}$. Thus, $\\tilde{H}(\\theta)$ is an upper Hessenberg matrix for all values of $\\theta$. This demonstrates the structural invariance. For any $\\theta \\in (0, \\pi)$, the matrix $U(\\theta)$ is not diagonal, yet $U(\\theta)e_1=e_1$ and $U(\\theta)^*HU(\\theta)$ is Hessenberg. This family of transformations serves as a counterexample to the uniqueness part of the implicit Q theorem, which would have forced $U(\\theta)$ to be diagonal if $H$ were unreduced.\n\nThe next task is to find the unique $\\theta \\in (0, \\pi/2)$ such that the $(4,3)$ entry of $\\tilde{H}(\\theta)$ is zero. The $(4,3)$ entry of the full matrix $\\tilde{H}(\\theta)$ is the $(2,1)$ entry of the block $\\tilde{H}_{22}(\\theta) = R(\\theta)^T H_{22} R(\\theta)$. We must compute this entry and set it to zero.\nLet $c = \\cos\\theta$ and $s = \\sin\\theta$. We have:\n$$\n\\tilde{H}_{22}(\\theta) = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} c & -s \\\\ s & c \\end{pmatrix}.\n$$\nLet's first compute the product $H_{22}R(\\theta)$:\n$$\nH_{22}R(\\theta) = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}\\begin{pmatrix} c & -s \\\\ s & c \\end{pmatrix} = \\begin{pmatrix} 2c+s & -2s+c \\\\ c+3s & -s+3c \\end{pmatrix}.\n$$\nNow, we pre-multiply by $R(\\theta)^T$:\n$$\n\\tilde{H}_{22}(\\theta) = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} 2c+s & -2s+c \\\\ c+3s & -s+3c \\end{pmatrix}.\n$$\nThe $(2,1)$ entry of this product, which corresponds to the $\\tilde{h}_{43}$ entry, is given by the dot product of the second row of the first matrix with the first column of the second matrix:\n$$\n\\tilde{h}_{43}(\\theta) = (-s)(2c+s) + (c)(c+3s) = -2sc - s^2 + c^2 + 3sc = c^2 - s^2 + sc.\n$$\nUsing the double-angle trigonometric identities $\\cos(2\\theta) = \\cos^2\\theta - \\sin^2\\theta$ and $\\sin(2\\theta) = 2\\sin\\theta\\cos\\theta$, we can rewrite this as:\n$$\n\\tilde{h}_{43}(\\theta) = \\cos(2\\theta) + \\frac{1}{2}\\sin(2\\theta).\n$$\nWe need to find $\\theta \\in (0, \\pi/2)$ such that $\\tilde{h}_{43}(\\theta) = 0$.\n$$\n\\cos(2\\theta) + \\frac{1}{2}\\sin(2\\theta) = 0.\n$$\nThis can be rearranged to $\\frac{1}{2}\\sin(2\\theta) = -\\cos(2\\theta)$. For $\\theta \\in (0, \\pi/2)$, $2\\theta \\in (0, \\pi)$. In this interval, $\\cos(2\\theta)$ is only zero at $2\\theta=\\pi/2$, where $\\sin(2\\theta)=1$, so the equation is not satisfied. We can therefore safely divide by $\\cos(2\\theta)$:\n$$\n\\frac{\\sin(2\\theta)}{\\cos(2\\theta)} = \\tan(2\\theta) = -2.\n$$\nWe are looking for a solution for $\\theta$ in the interval $(0, \\pi/2)$, which implies $2\\theta$ is in the interval $(0, \\pi)$. The tangent function is negative in the second quadrant, so $2\\theta$ must be in $(\\pi/2, \\pi)$. The general solution for $\\tan(x) = y$ is $x = \\arctan(y) + k\\pi$ for an integer $k$.\n$$\n2\\theta = \\arctan(-2) + k\\pi.\n$$\nSince $\\arctan(-2)$ lies in $(-\\pi/2, 0)$, we must choose $k=1$ to place $2\\theta$ in the desired interval $(0, \\pi)$.\n$$\n2\\theta = \\arctan(-2) + \\pi.\n$$\nUsing the identity $\\arctan(-x) = -\\arctan(x)$, we have:\n$$\n2\\theta = \\pi - \\arctan(2).\n$$\nThis value lies in $(\\pi/2, \\pi)$, as desired. Finally, solving for $\\theta$:\n$$\n\\theta = \\frac{1}{2}\\left(\\pi - \\arctan(2)\\right).\n$$\nThis value lies in $(\\pi/4, \\pi/2)$, which is within the specified interval $(0, \\pi/2)$.",
            "answer": "$$ \\boxed{\\frac{1}{2}\\left(\\pi - \\arctan(2)\\right)} $$"
        },
        {
            "introduction": "The true power of the Implicit Q theorem is revealed in its application, particularly in the celebrated QR algorithm for eigenvalue computation. This practice delves into the \"bulge-chasing\" mechanism, which is the heart of an implicit QR step. You will prove how a sequence of carefully chosen Givens rotations preserves the Hessenberg structure while implicitly performing a shifted QR iteration, a process whose validity rests entirely on the uniqueness guaranteed by the theorem .",
            "id": "3589432",
            "problem": "Let $H \\in \\mathbb{C}^{n \\times n}$ be an unreduced upper Hessenberg matrix, meaning $H$ is upper Hessenberg and every subdiagonal entry $h_{i+1,i}$ is nonzero for $1 \\leq i \\leq n-1$. Let $\\sigma \\in \\mathbb{C}$ be a given shift and define $x^{(1)} = (H - \\sigma I)e_1 \\in \\mathbb{C}^{n}$, where $e_1$ is the first standard basis vector and $I$ is the identity matrix. A unitary Givens rotation acting on indices $\\{k,k+1\\}$ is a unitary matrix $G_k \\in \\mathbb{C}^{n \\times n}$ that differs from the identity only in the $2 \\times 2$ principal submatrix on rows and columns $\\{k,k+1\\}$, where it takes the form $\\begin{pmatrix} c_k & s_k \\\\ -\\overline{s_k} & c_k \\end{pmatrix}$ with $c_k \\in \\mathbb{R}_{\\ge 0}$, $s_k \\in \\mathbb{C}$, and $c_k^2 + |s_k|^2 = 1$. Consider the sequence $(G_k)_{k=1}^{n-1}$ defined recursively as follows: for each $k = 1,2,\\dots,n-1$, let $x^{(k)} = G_{k-1}^{*}\\cdots G_1^{*}(H - \\sigma I)e_1$ and choose $G_k$ so that the $2 \\times 2$ action on the components $(x^{(k)}_k, x^{(k)}_{k+1})$ maps them to $(\\alpha_k, 0)$ with $\\alpha_k \\in \\mathbb{R}_{\\ge 0}$, i.e., $G_k^{*} x^{(k)}$ has $k$-th component equal to $\\alpha_k$ and $(k+1)$-st component equal to zero. Define the similarity sequence $H^{(j)} = G_j^{*}\\cdots G_1^{*} \\, H \\, G_1 \\cdots G_j$ for $j = 1,2,\\dots,n-1$.\n\nUsing only the definitions of unreduced upper Hessenberg structure, unitary similarity, and Givens rotations, do the following:\n\n1. Prove by induction on $j$ that each $H^{(j)}$ is upper Hessenberg. Your proof must justify that at every step the only possible departure from upper Hessenberg structure is a single subdiagonal bulge that is immediately chased one position down by the next left multiplication, so the structure is preserved.\n\n2. Explain why, for an unreduced upper Hessenberg $H$, the above construction determines the sequence $(G_k)_{k=1}^{n-1}$ uniquely up to multiplication of each $G_k$ on the left by a diagonal unit-modulus scalar on the active $2 \\times 2$ block, thereby connecting with the statement customarily known as the implicit $Q$ theorem (uniqueness of the Hessenberg form under a similarity constructed from $(H - \\sigma I)e_1$).\n\nFinally, determine a closed-form analytic expression for the cosine parameter $c_1$ of the first Givens rotation $G_1$ in terms of the first two components of $x^{(1)}$, denoted $x^{(1)}_1$ and $x^{(1)}_2$. Your final answer must be a single exact expression. No rounding is required and no units are involved.",
            "solution": "The supplied problem will be addressed in three parts: first, a proof regarding the structure of the matrix sequence $H^{(j)}$; second, an explanation of the uniqueness of the involved Givens rotations and its connection to the implicit Q theorem; and third, the derivation of a closed-form expression for the parameter $c_1$.\n\nA preliminary validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning the Francis QR algorithm and the implicit Q theorem. However, there is a minor ambiguity in the phrasing of Part 1, which will be clarified before proceeding with the proof.\n\nThe problem states: \"Prove by induction on $j$ that each $H^{(j)}$ is upper Hessenberg.\" As will be shown, the intermediate matrices $H^{(j)} = G_j^{*}\\cdots G_1^{*} \\, H \\, G_1 \\cdots G_j$ for $j=1, \\dots, n-2$ are in fact not strictly upper Hessenberg but rather upper Hessenberg with an additional non-zero entry (a \"bulge\") below the subdiagonal. The provided description, \"...the only possible departure from upper Hessenberg structure is a single subdiagonal bulge that is immediately chased one position down...\", accurately describes the process, but contradicts the assertion that each intermediate $H^{(j)}$ is itself upper Hessenberg. The proof will therefore demonstrate that this bulge-chasing procedure systematically moves the bulge downwards and outwards, such that the final matrix $H^{(n-1)}$ is restored to the upper Hessenberg form.\n\n### Part 1: Proof of Structure Preservation\n\nWe will prove by induction that the matrix $H^{(j)}$ for $j=1, \\dots, n-2$ is of upper Hessenberg form except for a single non-zero \"bulge\" at position $(j+2, j)$. The process culminates in $H^{(n-1)}$ which is a true upper Hessenberg matrix. The Givens rotation $G_k$ is chosen to act on rows/columns $k$ and $k+1$. In the explicit bulge-chasing algorithm, $G_k$ is defined to eliminate a specific element. As we will discuss in Part 2, the implicit definition of $G_k$ given in the problem is equivalent. For this proof, we consider the effect of these rotations on the matrix structure.\n\nLet the initial matrix be $H^{(0)} = H$, which is unreduced upper Hessenberg.\n\n**Base Case ($j=1$):**\nWe analyze the structure of $H^{(1)} = G_1^{*} H G_1$. The rotation $G_1$ acts on indices $\\{1,2\\}$.\n1.  Consider the matrix $A = G_1^{*} H$. Since $G_1^*$ acts on rows $1$ and $2$, $A_{ik} = H_{ik}$ for all $i > 2$. The rows $i=1,2$ are linear combinations of the original rows $1,2$. An upper Hessenberg matrix remains upper Hessenberg under this operation. So, $A = G_1^* H$ is an upper Hessenberg matrix.\n2.  Now consider $H^{(1)} = A G_1$. The right-multiplication by $G_1$ forms linear combinations of columns $1$ and $2$. For any column $k > 2$, the column is unchanged: $(H^{(1)})_{ik} = A_{ik}$ for $k > 2$. Since $A$ is upper Hessenberg, these columns satisfy the Hessenberg condition.\n3.  We inspect the entries $(H^{(1)})_{i,1}$ for $i > 2$.\n    The first column of $H^{(1)}$ is given by $(H^{(1)})_{\\cdot,1} = c_1 A_{\\cdot,1} - \\overline{s_1} A_{\\cdot,2}$.\n    For an entry $(i,1)$ with $i > 2$, its value is $(H^{(1)})_{i,1} = c_1 A_{i,1} - \\overline{s_1} A_{i,2}$.\n    Since $A=G_1^*H$ and $i>2$, the $i$-th row is not affected by $G_1^*$. Thus, $A_{i,1} = H_{i,1}$ and $A_{i,2} = H_{i,2}$.\n    As $H$ is upper Hessenberg, $H_{i,1} = 0$ for $i > 2$ and $H_{i,2} = 0$ for $i > 3$.\n    -   For $i=3$: $(H^{(1)})_{3,1} = c_1 H_{3,1} - \\overline{s_1} H_{3,2} = c_1 (0) - \\overline{s_1} h_{32} = -\\overline{s_1} h_{32}$.\n        Since $H$ is unreduced, $h_{32} \\neq 0$. The parameter $s_1$ is determined by the need to zero out the second component of $(H-\\sigma I)e_1 = (h_{11}-\\sigma)e_1 + h_{21}e_2$. Since $h_{21} \\neq 0$, $s_1$ will be non-zero. Thus, $(H^{(1)})_{3,1} \\neq 0$. This is the \"bulge\" at position $(3, 1)$.\n    -   For $i>3$: $(H^{(1)})_{i,1} = c_1 H_{i,1} - \\overline{s_1} H_{i,2} = c_1(0) - \\overline{s_1}(0) = 0$.\nThus, $H^{(1)}$ is upper Hessenberg except for the single bulge at $(3,1)$. The base case holds.\n\n**Inductive Step:**\nAssume that for some $j-1$ where $1 \\le j-1 < n-2$, the matrix $H^{(j-1)}$ is upper Hessenberg except for a single bulge at position $(j+1, j-1)$. We want to show that $H^{(j)} = G_j^{*} H^{(j-1)} G_j$ is upper Hessenberg except for a bulge at $(j+2, j)$. The rotation $G_j$ acts on indices $\\{j, j+1\\}$.\n\n1.  Consider $B = G_j^{*} H^{(j-1)}$. This mixes rows $j$ and $j+1$. The bulge in $H^{(j-1)}$ is at $(j+1, j-1)$, so it is located in a row affected by $G_j^*$. The rotation $G_j$ is chosen to annihilate this bulge. That is, $G_j$ is constructed such that $(B)_{j+1, j-1} = 0$. This specific construction defines $G_j$. All other entries below the subdiagonal in $H^{(j-1)}$ are zero, and this operation does not create new non-zero entries below the subdiagonal except possibly in row $j$, column $j-1$. Specifically, $(B)_{j,j-1}$ becomes non-zero. The matrix $B$ is now upper Hessenberg.\n2.  Now consider $H^{(j)} = B G_j$. This mixes columns $j$ and $j+1$.\n    -   The entry at $(j+1, j-1)$ remains zero because column $j-1$ is not affected by $G_j$: $(H^{(j)})_{j+1, j-1} = B_{j+1, j-1} = 0$. The bulge at $(j+1, j-1)$ has been removed.\n    -   Let's check for the creation of a new bulge at $(j+2, j)$. The value is $(H^{(j)})_{j+2, j} = c_j B_{j+2, j} - \\overline{s_j} B_{j+2, j+1}$.\n    -   The left multiplication by $G_j^*$ did not touch row $j+2$, so $B_{j+2,k} = (H^{(j-1)})_{j+2,k}$ for all $k$.\n    -   Therefore, $(H^{(j)})_{j+2, j} = c_j (H^{(j-1)})_{j+2, j} - \\overline{s_j} (H^{(j-1)})_{j+2, j+1}$.\n    -   By the inductive hypothesis, $H^{(j-1)}$ is upper Hessenberg below its bulge, so $(H^{(j-1)})_{j+2, j}=0$.\n    -   The entry $(H^{(j-1)})_{j+2, j+1}$ is on the subdiagonal of $H^{(j-1)}$. Unitary similarity transformations from unreduced Hessenberg matrices preserve the non-zero character of the subdiagonal. Thus, $(H^{(j-1)})_{j+2, j+1} \\neq 0$. Also, $s_j \\neq 0$ because it was used to annihilate a non-zero bulge.\n    -   So, $(H^{(j)})_{j+2, j} = -\\overline{s_j} (H^{(j-1)})_{j+2, j+1} \\neq 0$. This creates the new bulge at $(j+2, j)$.\n    -   A similar detailed check confirms no other entries below the subdiagonal become non-zero.\n\nThe induction holds. For $j=n-2$, $H^{(n-2)}$ has a bulge at $(n, n-2)$. The final step is to compute $H^{(n-1)} = G_{n-1}^{*} H^{(n-2)} G_{n-1}$. $G_{n-1}$ acts on $\\{n-1, n\\}$. It is chosen to annihilate the bulge at $(n, n-2)$. The right multiplication by $G_{n-1}$ would attempt to create a new bulge at $(n+1, n-1)$, but this is outside the matrix dimensions. Therefore, $H^{(n-1)}$ is a true upper Hessenberg matrix.\n\n### Part 2: Uniqueness and the Implicit Q Theorem\n\nThe connection between the construction of the Givens rotations $(G_k)_{k=1}^{n-1}$ and the uniqueness of the Hessenberg form lies at the heart of the implicit Q theorem.\n\nThe theorem on the uniqueness of the (unreduced) Hessenberg reduction states the following: Let $A \\in \\mathbb{C}^{n \\times n}$. If $Q$ and $V$ are two unitary matrices such that $Q^* A Q = H$ and $V^* A V = K$ are both unreduced upper Hessenberg matrices, and if the first columns of $Q$ and $V$ are equal, $Qe_1 = Ve_1$, then $V = QD$ and $K=D^*HD$ for some diagonal unitary matrix $D$.\n\nIn our problem, the sequence of Givens rotations $G_1, \\dots, G_{n-1}$ constructs a unitary matrix $Q = G_1 G_2 \\cdots G_{n-1}$. The result of the similarity transformation is $\\tilde{H} = Q^* H Q = H^{(n-1)}$, which, as shown in Part 1, is an upper Hessenberg matrix.\n\nThe first column of $Q$ is $Qe_1 = (G_1 \\cdots G_{n-1})e_1$. Since $G_k$ acts on indices $\\{k,k+1\\}$, $G_k e_1 = e_1$ for $k > 1$. Therefore, $Qe_1 = G_1 e_1$.\n\nThe problem defines $G_1$ via the vector $x^{(1)} = (H-\\sigma I)e_1$. Specifically, $G_1^*$ is chosen to rotate $x^{(1)}$ into a multiple of $e_1$: $G_1^* x^{(1)} = \\alpha_1 e_1$, with $\\alpha_1 \\in \\mathbb{R}_{\\ge 0}$. This implies $x^{(1)} = \\alpha_1 G_1 e_1$. Thus, the first column of our transformation matrix $Q$ is determined by the first column of $H-\\sigma I$:\n$$Qe_1 = G_1 e_1 = \\frac{1}{\\alpha_1} x^{(1)} = \\frac{1}{\\| (H-\\sigma I)e_1 \\|_2} (H-\\sigma I)e_1$$\nThis vector $Qe_1$ is the starting vector for a new basis. The procedure constructs a matrix $Q$ with this first column, such that $Q^*HQ$ is upper Hessenberg.\n\nThe uniqueness theorem guarantees that for an unreduced $H$, any such unitary matrix $Q$ is determined up to phase factors by its first column $Qe_1$.\n\nThe explicit bulge-chasing algorithm, which we used as a guide in Part 1, also defines a sequence of Givens rotations, let's call them $\\{G'_k\\}$, to restore the Hessenberg form. The first rotation $G'_1$ is chosen to begin the QR factorization of $H-\\sigma I$, which is precisely the same condition used to define $G_1$. Thus, $G'_1 = G_1$. Since the entire sequence of rotations is uniquely determined by the starting conditions (the matrix $H$ and the first column $Qe_1$), the sequence of rotations $G_k$ defined implicitly by the vectors $x^{(k)}$ must be the same as the sequence $G'_k$ defined explicitly by chasing the bulge. This is why the method is called the \"implicit Q\" theorem: the transformation $Q$ is determined implicitly by $(H-\\sigma I)e_1$ without ever forming the matrix $H-\\sigma I$ or its QR factorization explicitly.\n\nRegarding uniqueness of the Givens rotations themselves: The problem requires choosing $G_k$ such that its adjoint $G_k^*$ maps a vector $(a,b)^T$ to $(\\alpha_k, 0)^T$ with $\\alpha_k \\ge 0$, and the rotation has the form $\\begin{pmatrix} c_k & s_k \\\\ -\\overline{s_k} & c_k \\end{pmatrix}$ with $c_k \\in \\mathbb{R}_{\\ge 0}$. This set of conditions uniquely determines $c_k$ and $s_k$. For instance, from the system of equations $-s_k a + c_k b = 0$ and $c_k^2 + |s_k|^2 = 1$, we find $c_k = |a|/\\sqrt{|a|^2+|b|^2}$ and $s_k = c_k(b/a) = \\overline{a}b/(|a|\\sqrt{|a|^2+|b|^2})$. The condition $c_k \\ge 0$ is satisfied automatically, and this choice is unique. The problem's mention of \"uniquely up to multiplication of each $G_k$ on the left by a diagonal unit-modulus scalar\" refers to a more general definition of Givens rotations where $c_k$ is not restricted to be real and non-negative, a freedom which has been removed by the problem's strict definition.\n\n### Part 3: Expression for $c_1$\n\nWe need to find the cosine parameter $c_1$ of the first Givens rotation $G_1$.\nThe rotation $G_1$ is defined by its action on the first two components of the vector $x^{(1)} = (H-\\sigma I)e_1$.\nThe vector $x^{(1)}$ has components:\n$$x^{(1)}_1 = h_{11} - \\sigma$$\n$$x^{(1)}_2 = h_{21}$$\nAll other components $x^{(1)}_i$ are zero for $i > 2$ because $H$ is upper Hessenberg.\n\nThe Givens rotation $G_1$ acts on the subspace spanned by $\\{e_1, e_2\\}$. Its adjoint $G_1^*$ is applied to $x^{(1)}$ to zero out the second component. Let $a = x^{(1)}_1$ and $b = x^{(1)}_2$.\nWe have $G_1^{*} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\alpha_1 \\\\ 0 \\end{pmatrix}$, where the $2 \\times 2$ defining part of $G_1^*$ is $\\begin{pmatrix} c_1 & \\overline{s_1} \\\\ -s_1 & c_1 \\end{pmatrix}$.\nThe condition for the second component of the result to be zero is:\n$$-s_1 a + c_1 b = 0$$\nThe rotation is unitary, so we also have the constraint:\n$$c_1^2 + |s_1|^2 = 1$$\nFrom the first equation, assuming $a \\neq 0$, we have $s_1 = c_1 \\frac{b}{a}$. Substituting this into the second equation:\n$$c_1^2 + \\left|c_1 \\frac{b}{a}\\right|^2 = 1$$\n$$c_1^2 \\left(1 + \\frac{|b|^2}{|a|^2}\\right) = 1$$\n$$c_1^2 \\frac{|a|^2 + |b|^2}{|a|^2} = 1$$\n$$c_1^2 = \\frac{|a|^2}{|a|^2 + |b|^2}$$\nThe problem states that $c_1 \\in \\mathbb{R}_{\\ge 0}$, so we take the positive square root:\n$$c_1 = \\frac{|a|}{\\sqrt{|a|^2 + |b|^2}}$$\nIf $a=0$, then from $-s_1 a + c_1 b = 0$ we get $c_1 b = 0$. Since $H$ is unreduced, $b=h_{21}\\neq 0$, so we must have $c_1=0$. The formula holds in this case as well.\n\nSubstituting $a = x^{(1)}_1$ and $b = x^{(1)}_2$ gives the final expression for $c_1$.\n$$c_1 = \\frac{|x^{(1)}_1|}{\\sqrt{|x^{(1)}_1|^2 + |x^{(1)}_2|^2}}$$",
            "answer": "$$\\boxed{\\frac{|x^{(1)}_1|}{\\sqrt{|x^{(1)}_1|^2 + |x^{(1)}_2|^2}}}$$"
        }
    ]
}