{
    "hands_on_practices": [
        {
            "introduction": "Mastering the standard eigenvalue problem, $A x = \\lambda x$, begins with a firm grasp of its foundational definitions. This first exercise walks you through the essential mechanics of finding eigenvalues and eigenvectors from first principles for a simple $2 \\times 2$ matrix. By computing the characteristic polynomial, determining the multiplicities of the eigenvalues, and constructing the eigenspaces, you will directly confront the crucial concept of diagonalizability and encounter a matrix that is *not* diagonalizable—a so-called defective matrix .",
            "id": "3597590",
            "problem": "Consider the standard eigenvalue problem for a real square matrix, defined by the relation $A x = \\lambda x$, where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$ is a nonzero vector, and $\\lambda \\in \\mathbb{C}$ is a scalar. Let $A = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix}$. Starting from the definition of an eigenvalue as a scalar $\\lambda$ for which there exists a nonzero vector $x$ satisfying $A x = \\lambda x$, and from the definition of the characteristic polynomial as $\\det(A - \\lambda I)$, do the following:\n- Derive the characteristic polynomial of $A$ and determine all eigenvalues along with their algebraic multiplicities.\n- For each eigenvalue, determine the corresponding eigenspace and a basis for it. Using the definition of an eigenvector, explicitly verify that $A x = \\lambda x$ holds for a representative basis vector in each eigenspace.\n- Conclude, with justification based on the algebraic and geometric multiplicities, whether $A$ is diagonalizable over $\\mathbb{R}$.\nFinally, report the spectral radius $\\rho(A)$, defined as $\\rho(A) = \\max\\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}$. Express your final reported quantity as an exact value. No rounding is required, and no units are involved.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- The problem is a standard eigenvalue problem defined by the relation $A x = \\lambda x$.\n- The matrix is $A = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix}$, where $A \\in \\mathbb{R}^{n \\times n}$.\n- The eigenvector $x \\in \\mathbb{R}^{n}$ is a nonzero vector.\n- The eigenvalue $\\lambda$ is a scalar, $\\lambda \\in \\mathbb{C}$.\n- The characteristic polynomial is defined as $\\det(A - \\lambda I)$.\n- The tasks are:\n    1. Derive the characteristic polynomial and find all eigenvalues with their algebraic multiplicities.\n    2. For each eigenvalue, determine the corresponding eigenspace and a basis.\n    3. Explicitly verify $A x = \\lambda x$ for a representative basis vector.\n    4. Conclude if $A$ is diagonalizable over $\\mathbb{R}$, with justification.\n    5. Report the spectral radius $\\rho(A) = \\max\\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } A \\}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical)**: The problem is a standard exercise in linear algebra. All definitions (eigenvalue, eigenvector, characteristic polynomial, diagonalizability, spectral radius) are correct and fundamental to the field. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. A single, specific matrix $A$ is provided, and the tasks required are clear and standard procedures in linear algebra that lead to a unique solution.\n- **Objective (Critical)**: The problem is stated using precise, objective mathematical language.\n- **Completeness**: All necessary information to solve the problem is provided. The matrix $A$ is fully specified.\n- **Other considerations**: The problem is not ill-posed, contradictory, or trivial. It requires the application of standard, well-defined concepts and procedures.\n\n### Step 3: Verdict and Action\n- **Verdict**: The problem is valid.\n- **Action**: Proceed with the solution.\n\nThe solution is now developed as follows.\n\nThe standard eigenvalue problem is given by $A x = \\lambda x$, where $\\lambda$ is an eigenvalue and $x$ is the corresponding non-zero eigenvector. This can be rewritten as $(A - \\lambda I)x = 0$, where $I$ is the identity matrix. For a non-trivial solution ($x \\neq 0$) to exist, the matrix $(A - \\lambda I)$ must be singular, which means its determinant must be zero. This condition gives the characteristic equation: $\\det(A - \\lambda I) = 0$.\n\nFirst, we derive the characteristic polynomial for the given matrix $A = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix}$.\nThe matrix $(A - \\lambda I)$ is:\n$$ A - \\lambda I = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\lambda  -1 \\\\ 1  -\\lambda \\end{pmatrix} $$\nThe characteristic polynomial, $p(\\lambda)$, is the determinant of this matrix:\n$$ p(\\lambda) = \\det(A - \\lambda I) = (2 - \\lambda)(-\\lambda) - (-1)(1) = -2\\lambda + \\lambda^2 + 1 $$\nThus, the characteristic polynomial is $p(\\lambda) = \\lambda^2 - 2\\lambda + 1$.\n\nTo find the eigenvalues, we solve the characteristic equation $p(\\lambda) = 0$:\n$$ \\lambda^2 - 2\\lambda + 1 = 0 $$\nThis polynomial factors as $(\\lambda - 1)^2 = 0$.\nThe equation has a repeated root, $\\lambda_1 = 1$. Therefore, there is only one distinct eigenvalue, $\\lambda = 1$. The algebraic multiplicity of this eigenvalue is the power of the factor $(\\lambda - 1)$ in the characteristic polynomial, which is $2$.\n\nNext, we find the eigenspace corresponding to the eigenvalue $\\lambda = 1$. The eigenspace $E_1$ is the null space of the matrix $(A - 1 \\cdot I)$.\n$$ A - I = \\begin{pmatrix} 2 - 1  -1 \\\\ 1  0 - 1 \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ 1  -1 \\end{pmatrix} $$\nWe need to solve the system $(A - I)x = 0$ for $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$:\n$$ \\begin{pmatrix} 1  -1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis leads to the single independent equation $x_1 - x_2 = 0$, which implies $x_1 = x_2$.\nThe eigenvectors are of the form $x = \\begin{pmatrix} k \\\\ k \\end{pmatrix}$ for any non-zero scalar $k \\in \\mathbb{R}$.\nThe eigenspace $E_1$ is the set of all such vectors:\n$$ E_1 = \\left\\{ k \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} : k \\in \\mathbb{R} \\right\\} = \\text{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right\\} $$\nA basis for this eigenspace is the single vector $\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right\\}$.\nThe geometric multiplicity of the eigenvalue $\\lambda = 1$ is the dimension of its eigenspace, which is $1$.\n\nNow, we explicitly verify the eigenvalue-eigenvector relationship $A x = \\lambda x$ using a representative basis vector from the eigenspace, $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and the eigenvalue $\\lambda=1$.\n$$ A x = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (2)(1) + (-1)(1) \\\\ (1)(1) + (0)(1) \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nAnd for the right-hand side:\n$$ \\lambda x = 1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nSince both sides are equal, the relationship $A x = \\lambda x$ is verified.\n\nA square matrix is diagonalizable if and only if for every distinct eigenvalue, its algebraic multiplicity is equal to its geometric multiplicity. For the matrix $A$:\n- The only eigenvalue is $\\lambda = 1$.\n- The algebraic multiplicity of $\\lambda = 1$ is $2$.\n- The geometric multiplicity of $\\lambda = 1$ is $1$.\nSince the algebraic multiplicity ($2$) is not equal to the geometric multiplicity ($1$), the matrix $A$ is not diagonalizable over $\\mathbb{R}$ (or over $\\mathbb{C}$).\n\nFinally, we determine the spectral radius $\\rho(A)$. The spectrum of $A$, denoted $\\sigma(A)$, is the set of all its eigenvalues. In this case, $\\sigma(A) = \\{1\\}$. The spectral radius is defined as the maximum of the absolute values (or moduli, for complex eigenvalues) of the eigenvalues in the spectrum:\n$$ \\rho(A) = \\max\\{ |\\lambda| : \\lambda \\in \\sigma(A) \\} $$\nFor our matrix $A$, this is:\n$$ \\rho(A) = \\max\\{ |1| \\} = 1 $$\nThe spectral radius of $A$ is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "In contrast to the defective matrix encountered previously, many matrices in physical and engineering applications possess a special structure that makes their eigenvalue problems exceptionally well-behaved. This practice focuses on one such class: Hermitian (or real-symmetric) matrices . By working through this example, you will verify two cornerstone properties of Hermitian matrices—that their eigenvalues are real and their eigenvectors corresponding to distinct eigenvalues are orthogonal—which form the theoretical basis for many stable and efficient numerical algorithms.",
            "id": "3597596",
            "problem": "Consider the standard eigenvalue problem for the Hermitian matrix $A$ given by\n$$\nA=\\begin{pmatrix}4  1 \\\\ 1  2\\end{pmatrix}.\n$$\nStarting from the definition that an eigenpair $(\\lambda, v)$ satisfies $A v=\\lambda v$ with $v\\neq 0$, and that eigenvalues $\\lambda$ are those for which $\\det(A-\\lambda I)=0$, do the following tasks from first principles:\n- Derive the characteristic polynomial of $A$ and compute all eigenvalues.\n- For each eigenvalue, construct a corresponding eigenvector by solving the homogeneous linear system $(A-\\lambda I)v=0$, and scale each eigenvector to have unit $2$-norm.\n- Using the Euclidean inner product, verify that these normalized eigenvectors are orthogonal, explaining why this property follows for Hermitian matrices with distinct eigenvalues.\n\nFinally, determine the spectral condition number with respect to the matrix two-norm, denoted $\\kappa_{2}(A)$, defined as the ratio of the largest singular value of $A$ to the smallest singular value of $A$. Express your final answer in exact form as a single analytic expression. No rounding is required.",
            "solution": "The problem is well-posed and self-contained, presenting a standard exercise in linear algebra. The given matrix is a $2 \\times 2$ real symmetric matrix, which is a special case of a Hermitian matrix. We will proceed with the tasks in the specified order.\n\nThe given matrix is\n$$\nA = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix}\n$$\nThis matrix is Hermitian, as $A = A^T = A^H$, where $A^H$ is the conjugate transpose of $A$.\n\n### Part 1: Characteristic Polynomial and Eigenvalues\n\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\nFirst, we construct the matrix $A - \\lambda I$:\n$$\nA - \\lambda I = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4-\\lambda  1 \\\\ 1  2-\\lambda \\end{pmatrix}\n$$\nNext, we compute the determinant of this matrix to find the characteristic polynomial $p(\\lambda)$:\n$$\np(\\lambda) = \\det(A - \\lambda I) = (4-\\lambda)(2-\\lambda) - (1)(1)\n$$\n$$\np(\\lambda) = 8 - 4\\lambda - 2\\lambda + \\lambda^2 - 1\n$$\n$$\np(\\lambda) = \\lambda^2 - 6\\lambda + 7\n$$\nThis is the characteristic polynomial of $A$. To find the eigenvalues, we set $p(\\lambda) = 0$ and solve for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{-(-6) \\pm \\sqrt{(-6)^2 - 4(1)(7)}}{2(1)} = \\frac{6 \\pm \\sqrt{36 - 28}}{2} = \\frac{6 \\pm \\sqrt{8}}{2}\n$$\nSince $\\sqrt{8} = \\sqrt{4 \\cdot 2} = 2\\sqrt{2}$, the eigenvalues are:\n$$\n\\lambda = \\frac{6 \\pm 2\\sqrt{2}}{2} = 3 \\pm \\sqrt{2}\n$$\nLet us denote the larger eigenvalue as $\\lambda_1$ and the smaller as $\\lambda_2$:\n$$\n\\lambda_1 = 3 + \\sqrt{2}\n$$\n$$\n\\lambda_2 = 3 - \\sqrt{2}\n$$\n\n### Part 2: Eigenvectors and Normalization\n\nFor each eigenvalue, we find a corresponding eigenvector $v$ by solving the homogeneous system $(A - \\lambda I)v = 0$.\n\nFor $\\lambda_1 = 3 + \\sqrt{2}$:\nThe system is $(A - \\lambda_1 I)v_1 = 0$, where $v_1 = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.\n$$\n\\left(\\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} - (3 + \\sqrt{2})\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1 - \\sqrt{2}  1 \\\\ 1  -1 - \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get the equation $(1 - \\sqrt{2})x + y = 0$, which implies $y = (\\sqrt{2} - 1)x$. Choosing $x=1$ gives a non-zero eigenvector $v'_1 = \\begin{pmatrix} 1 \\\\ \\sqrt{2} - 1 \\end{pmatrix}$.\nTo normalize this vector, we compute its $2$-norm:\n$$\n\\|v'_1\\|_2 = \\sqrt{1^2 + (\\sqrt{2} - 1)^2} = \\sqrt{1 + (2 - 2\\sqrt{2} + 1)} = \\sqrt{4 - 2\\sqrt{2}}\n$$\nThe normalized eigenvector $v_1$ is:\n$$\nv_1 = \\frac{1}{\\|v'_1\\|_2} v'_1 = \\frac{1}{\\sqrt{4 - 2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ \\sqrt{2} - 1 \\end{pmatrix}\n$$\n\nFor $\\lambda_2 = 3 - \\sqrt{2}$:\nThe system is $(A - \\lambda_2 I)v_2 = 0$.\n$$\n\\left(\\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} - (3 - \\sqrt{2})\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1 + \\sqrt{2}  1 \\\\ 1  -1 + \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get $(1 + \\sqrt{2})x + y = 0$, which implies $y = -(1 + \\sqrt{2})x$. Choosing $x=1$ gives a non-zero eigenvector $v'_2 = \\begin{pmatrix} 1 \\\\ -(1 + \\sqrt{2}) \\end{pmatrix}$.\nTo normalize this vector, we compute its $2$-norm:\n$$\n\\|v'_2\\|_2 = \\sqrt{1^2 + (-(1 + \\sqrt{2}))^2} = \\sqrt{1 + (1 + 2\\sqrt{2} + 2)} = \\sqrt{4 + 2\\sqrt{2}}\n$$\nThe normalized eigenvector $v_2$ is:\n$$\nv_2 = \\frac{1}{\\|v'_2\\|_2} v'_2 = \\frac{1}{\\sqrt{4 + 2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ -(1 + \\sqrt{2}) \\end{pmatrix}\n$$\n\n### Part 3: Orthogonality Verification and Explanation\n\nTo verify that the eigenvectors are orthogonal, we compute their Euclidean inner product (dot product), $\\langle v_1, v_2 \\rangle = v_1^T v_2$. It is simpler to use the unnormalized eigenvectors $v'_1$ and $v'_2$, as orthogonality is independent of scaling.\n$$\n\\langle v'_1, v'_2 \\rangle = (1)(1) + (\\sqrt{2} - 1)(-(1 + \\sqrt{2})) = 1 - (\\sqrt{2} - 1)(\\sqrt{2} + 1)\n$$\nUsing the difference of squares formula, $(a-b)(a+b) = a^2 - b^2$:\n$$\n\\langle v'_1, v'_2 \\rangle = 1 - ((\\sqrt{2})^2 - 1^2) = 1 - (2 - 1) = 1 - 1 = 0\n$$\nSince the inner product is $0$, the eigenvectors are orthogonal.\n\nThis property is general for Hermitian matrices with distinct eigenvalues. Let $A$ be a Hermitian matrix ($A=A^H$). Let $(\\lambda_1, v_1)$ and $(\\lambda_2, v_2)$ be two eigenpairs with $\\lambda_1 \\neq \\lambda_2$.\nFrom $Av_1 = \\lambda_1 v_1$, we compute the inner product with $v_2$: $\\langle Av_1, v_2 \\rangle = \\langle \\lambda_1 v_1, v_2 \\rangle = \\lambda_1 \\langle v_1, v_2 \\rangle$.\nUsing the adjoint property of the inner product, $\\langle Av_1, v_2 \\rangle = v_2^H A v_1$. Since $A$ is Hermitian, $A=A^H$, so $v_2^H A v_1 = (A^H v_2)^H v_1 = (A v_2)^H v_1$.\nFrom $Av_2 = \\lambda_2 v_2$, we have $(A v_2)^H v_1 = (\\lambda_2 v_2)^H v_1 = \\overline{\\lambda_2} v_2^H v_1 = \\overline{\\lambda_2} \\langle v_1, v_2 \\rangle$.\nThe eigenvalues of a Hermitian matrix are always real, so $\\overline{\\lambda_2} = \\lambda_2$.\nEquating the two expressions for $\\langle Av_1, v_2 \\rangle$:\n$$\n\\lambda_1 \\langle v_1, v_2 \\rangle = \\lambda_2 \\langle v_1, v_2 \\rangle\n$$\n$$\n(\\lambda_1 - \\lambda_2) \\langle v_1, v_2 \\rangle = 0\n$$\nSince the eigenvalues are distinct, $\\lambda_1 - \\lambda_2 \\neq 0$. Therefore, we must have $\\langle v_1, v_2 \\rangle = 0$, which proves the eigenvectors are orthogonal. Our eigenvalues $\\lambda_1 = 3 + \\sqrt{2}$ and $\\lambda_2 = 3 - \\sqrt{2}$ are distinct, confirming the theory.\n\n### Part 4: Spectral Condition Number\n\nThe spectral condition number is defined as $\\kappa_2(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $A$, respectively. The singular values of $A$ are the square roots of the eigenvalues of the matrix $A^H A$.\nSince $A$ is Hermitian ($A^H = A$), we are interested in the eigenvalues of $A^2$. If $\\lambda_i$ are the eigenvalues of $A$, then $\\lambda_i^2$ are the eigenvalues of $A^2$. The singular values are thus $\\sigma_i = \\sqrt{\\lambda_i^2} = |\\lambda_i|$.\nIn our case, the eigenvalues of $A$ are $\\lambda_1 = 3 + \\sqrt{2}$ and $\\lambda_2 = 3 - \\sqrt{2}$. Both are positive, since $3 = \\sqrt{9}$ and $\\sqrt{2}  \\sqrt{9}$.\nTherefore, the singular values of $A$ are equal to its eigenvalues:\n$$\n\\sigma_{\\max} = \\sigma_1 = |\\lambda_1| = \\lambda_1 = 3 + \\sqrt{2}\n$$\n$$\n\\sigma_{\\min} = \\sigma_2 = |\\lambda_2| = \\lambda_2 = 3 - \\sqrt{2}\n$$\nThe spectral condition number is the ratio of these values:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{3 + \\sqrt{2}}{3 - \\sqrt{2}}\n$$\nTo express this in a simplified analytic form, we rationalize the denominator:\n$$\n\\kappa_2(A) = \\frac{3 + \\sqrt{2}}{3 - \\sqrt{2}} \\times \\frac{3 + \\sqrt{2}}{3 + \\sqrt{2}} = \\frac{(3 + \\sqrt{2})^2}{3^2 - (\\sqrt{2})^2} = \\frac{3^2 + 2(3)(\\sqrt{2}) + (\\sqrt{2})^2}{9 - 2}\n$$\n$$\n\\kappa_2(A) = \\frac{9 + 6\\sqrt{2} + 2}{7} = \\frac{11 + 6\\sqrt{2}}{7}\n$$\nThis is the final exact expression for the spectral condition number of $A$.",
            "answer": "$$\n\\boxed{\\frac{11 + 6\\sqrt{2}}{7}}\n$$"
        },
        {
            "introduction": "The existence of defective matrices is not just a theoretical curiosity; it has profound implications for numerical computation. This final practice delves into the crucial concept of eigenvalue sensitivity, or conditioning, by examining how small perturbations to a matrix can affect its eigenvalues . By analyzing a defective matrix and deriving its eigenvalue condition number from first principles, you will discover why such matrices are numerically unstable and gain a deeper appreciation for the challenges that arise when computing eigenvalues in finite-precision arithmetic.",
            "id": "3597594",
            "problem": "Consider the standard eigenvalue problem $A x = \\lambda x$ for the matrix\n$$\nA = \\begin{pmatrix} 1  100 \\\\ 0  1 \\end{pmatrix}.\n$$\nLet $\\lambda = 1$ denote the (algebraically double) eigenvalue of $A$. Work with the operator $2$-norm (also called the spectral norm). Using only foundational definitions—namely, the definition of eigenvalues as roots of the characteristic polynomial, the operator $2$-norm, and the notion of (absolute) condition number as the local Lipschitz constant of the mapping that sends a matrix to its spectrum—compute the absolute condition number of the eigenvalue $\\lambda = 1$ at $A$, defined as\n$$\n\\kappa(\\lambda; A) \\;=\\; \\limsup_{\\varepsilon \\to 0^+} \\;\\sup_{\\|E\\|_2 \\le \\varepsilon} \\;\\frac{\\mathrm{dist}\\!\\left(\\lambda, \\Lambda(A+E)\\right)}{\\varepsilon},\n$$\nwhere $\\Lambda(M)$ denotes the set of eigenvalues of a matrix $M$, and $\\mathrm{dist}(\\lambda, \\Lambda(M)) = \\min_{\\mu \\in \\Lambda(M)} |\\lambda - \\mu|$. Do not invoke any pre-derived perturbation formulas; derive the behavior from first principles by analyzing the characteristic polynomial of $A+E$ for small perturbations $E$. In your reasoning, explain qualitatively why the sensitivity behaves as it does for this matrix.\n\nYour final answer must be a single symbolic expression. No rounding is required and no units are involved.",
            "solution": "The problem asks for the absolute condition number of the eigenvalue $\\lambda = 1$ for the matrix $A = \\begin{pmatrix} 1  100 \\\\ 0  1 \\end{pmatrix}$. The norm is the operator $2$-norm, $\\| \\cdot \\|_2$. The absolute condition number is defined as:\n$$\n\\kappa(\\lambda; A) = \\limsup_{\\varepsilon \\to 0^+} \\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\mathrm{dist}(\\lambda, \\Lambda(A+E))}{\\varepsilon}\n$$\nwhere $\\Lambda(M)$ is the spectrum of a matrix $M$, and $\\mathrm{dist}(\\lambda, \\Lambda(M)) = \\min_{\\mu \\in \\Lambda(M)} |\\lambda - \\mu|$.\n\nLet $E = \\begin{pmatrix} e_{11}  e_{12} \\\\ e_{21}  e_{22} \\end{pmatrix}$ be an arbitrary perturbation matrix such that $\\|E\\|_2 \\le \\varepsilon$. The perturbed matrix is $A+E = \\begin{pmatrix} 1+e_{11}  100+e_{12} \\\\ e_{21}  1+e_{22} \\end{pmatrix}$.\n\nThe eigenvalues $\\mu$ of $A+E$ are the roots of its characteristic polynomial, $\\det(A+E-\\mu I)=0$.\n$$\n\\det\\left( \\begin{pmatrix} 1+e_{11}-\\mu  100+e_{12} \\\\ e_{21}  1+e_{22}-\\mu \\endpmatrix} \\right) = 0\n$$\n$$\n(1+e_{11}-\\mu)(1+e_{22}-\\mu) - e_{21}(100+e_{12}) = 0\n$$\nThe original eigenvalue is $\\lambda=1$. We are interested in the deviation of the perturbed eigenvalues $\\mu$ from $1$. Let $\\delta = \\mu - 1$, which implies $\\mu = 1+\\delta$. Substituting this into the characteristic equation gives:\n$$\n(e_{11}-\\delta)(e_{22}-\\delta) - e_{21}(100+e_{12}) = 0\n$$\nExpanding this, we obtain a quadratic equation for the eigenvalue shift $\\delta$:\n$$\n\\delta^2 - (e_{11}+e_{22})\\delta + (e_{11}e_{22} - 100e_{21} - e_{12}e_{21}) = 0\n$$\nThe solutions for $\\delta$ are given by the quadratic formula:\n$$\n\\delta_{1,2} = \\frac{e_{11}+e_{22} \\pm \\sqrt{(e_{11}+e_{22})^2 - 4(e_{11}e_{22} - 100e_{21} - e_{12}e_{21})}}{2}\n$$\nSimplifying the discriminant:\n$$\n\\delta_{1,2} = \\frac{e_{11}+e_{22} \\pm \\sqrt{(e_{11}-e_{22})^2 + 400e_{21} + 4e_{12}e_{21}}}{2}\n$$\nThe perturbed eigenvalues are $\\mu_1 = 1+\\delta_1$ and $\\mu_2 = 1+\\delta_2$. The distance from $\\lambda=1$ to the set of perturbed eigenvalues is:\n$$\n\\mathrm{dist}(1, \\Lambda(A+E)) = \\min(|\\mu_1-1|, |\\mu_2-1|) = \\min(|\\delta_1|, |\\delta_2|)\n$$\nWe need to evaluate $\\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\min(|\\delta_1|, |\\delta_2|)}{\\varepsilon}$.\n\nTo find this supremum, we seek a \"worst-case\" perturbation $E$ that maximizes this ratio. The term $400e_{21}$ under the square root suggests that the entry $e_{21}$ plays a crucial role. For a general matrix, we have the inequality $|e_{ij}| \\le \\|E\\|_2$. Thus, for $\\|E\\|_2 \\le \\varepsilon$, we have $|e_{ij}| \\le \\varepsilon$.\n\nConsider a specific perturbation designed to make the term $400e_{21}$ dominant. Let us choose the perturbation matrix\n$$\nE_0 = \\begin{pmatrix} 0  0 \\\\ \\varepsilon  0 \\end{pmatrix}\n$$\nThe $2$-norm of this matrix is $\\|E_0\\|_2 = \\sup_{\\|v\\|_2=1} \\|E_0 v\\|_2 = \\sup_{v_1^2+v_2^2=1} \\|\\begin{pmatrix} 0 \\\\ \\varepsilon v_1 \\end{pmatrix}\\|_2 = \\sup \\varepsilon|v_1| = \\varepsilon$. So, this is a valid perturbation with $\\|E_0\\|_2 = \\varepsilon$.\n\nFor this choice of $E_0$, we have $e_{11}=e_{12}=e_{22}=0$ and $e_{21}=\\varepsilon$. The equation for $\\delta$ becomes:\n$$\n\\delta^2 - 0 \\cdot \\delta + (0 - 100\\varepsilon - 0) = 0\n$$\n$$\n\\delta^2 - 100\\varepsilon = 0\n$$\nThe solutions are $\\delta_{1,2} = \\pm\\sqrt{100\\varepsilon} = \\pm 10\\sqrt{\\varepsilon}$.\nThe perturbed eigenvalues are $\\mu_{1,2} = 1 \\pm 10\\sqrt{\\varepsilon}$.\nThe distance from $1$ to the perturbed spectrum is:\n$$\n\\mathrm{dist}(1, \\Lambda(A+E_0)) = \\min(|10\\sqrt{\\varepsilon}|, |-10\\sqrt{\\varepsilon}|) = 10\\sqrt{\\varepsilon}\n$$\nNow we compute the ratio for this specific perturbation:\n$$\n\\frac{\\mathrm{dist}(1, \\Lambda(A+E_0))}{\\varepsilon} = \\frac{10\\sqrt{\\varepsilon}}{\\varepsilon} = \\frac{10}{\\sqrt{\\varepsilon}}\n$$\nThe definition of the condition number involves the supremum over all perturbations $E$ with $\\|E\\|_2 \\le \\varepsilon$. The value of the supremum must be at least the value for any particular instance. Thus:\n$$\n\\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\mathrm{dist}(1, \\Lambda(A+E))}{\\varepsilon} \\ge \\frac{10}{\\sqrt{\\varepsilon}}\n$$\nFinally, we take the limit superior as $\\varepsilon \\to 0^+$:\n$$\n\\kappa(1; A) = \\limsup_{\\varepsilon \\to 0^+} \\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\mathrm{dist}(1, \\Lambda(A+E))}{\\varepsilon} \\ge \\limsup_{\\varepsilon \\to 0^+} \\frac{10}{\\sqrt{\\varepsilon}}\n$$\nAs $\\varepsilon \\to 0^+$, the term $\\frac{10}{\\sqrt{\\varepsilon}}$ tends to infinity. Therefore:\n$$\n\\kappa(1; A) = \\infty\n$$\nQualitative explanation: The large sensitivity of the eigenvalue is a consequence of the algebraic structure of the matrix $A$. The matrix $A$ has an eigenvalue $\\lambda=1$ with algebraic multiplicity $2$, but the dimension of the corresponding eigenspace is $1$ (it is spanned by the vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$). The geometric multiplicity ($1$) is less than the algebraic multiplicity ($2$), which means the matrix is defective and non-diagonalizable. It is similar to a Jordan block of size $2$. Eigenvalues of defective matrices are inherently sensitive to perturbations. For a perturbation of norm $\\varepsilon$, the resulting change in the eigenvalue is of the order $\\varepsilon^{1/m}$, where $m$ is the size of the largest Jordan block associated with that eigenvalue. In our case, $m=2$, so the eigenvalue shift $\\delta$ is of order $\\sqrt{\\varepsilon}$. The condition number, as defined, measures the rate of change $|\\delta|/\\varepsilon$, which behaves like $\\sqrt{\\varepsilon}/\\varepsilon = 1/\\sqrt{\\varepsilon}$. This expression diverges as $\\varepsilon \\to 0$, indicating infinite local sensitivity. The large off-diagonal entry, $100$, further amplifies this effect. As shown in the derivation, the eigenvalue shift is proportional to $\\sqrt{100}=10$, but this finite factor multiplies a term that diverges.",
            "answer": "$$\n\\boxed{\\infty}\n$$"
        }
    ]
}