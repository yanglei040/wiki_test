{
    "hands_on_practices": [
        {
            "introduction": "The study of the standard eigenvalue problem often begins with Hermitian (or real symmetric) matrices, which form a class of operators with exceptionally well-behaved spectral properties. The Spectral Theorem guarantees that their eigenvalues are real and that eigenvectors corresponding to distinct eigenvalues are orthogonal. This exercise provides a hands-on verification of these fundamental principles, guiding you through the process of calculating the eigenpairs for a simple symmetric matrix and confirming their orthogonality. ",
            "id": "3597596",
            "problem": "Consider the standard eigenvalue problem for the Hermitian matrix $A$ given by\n$$\nA = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix}.\n$$\nStarting from the definition that an eigenpair $(\\lambda, v)$ satisfies $A v=\\lambda v$ with $v\\neq 0$, and that eigenvalues $\\lambda$ are those for which $\\det(A-\\lambda I)=0$, do the following tasks from first principles:\n- Derive the characteristic polynomial of $A$ and compute all eigenvalues.\n- For each eigenvalue, construct a corresponding eigenvector by solving the homogeneous linear system $(A-\\lambda I)v=0$, and scale each eigenvector to have unit $2$-norm.\n- Using the Euclidean inner product, verify that these normalized eigenvectors are orthogonal, explaining why this property follows for Hermitian matrices with distinct eigenvalues.\n\nFinally, determine the spectral condition number with respect to the matrix two-norm, denoted $\\kappa_{2}(A)$, defined as the ratio of the largest singular value of $A$ to the smallest singular value of $A$. Express your final answer in exact form as a single analytic expression. No rounding is required.",
            "solution": "The problem is well-posed and self-contained, presenting a standard exercise in linear algebra. The given matrix is a $2 \\times 2$ real symmetric matrix, which is a special case of a Hermitian matrix. We will proceed with the tasks in the specified order.\n\nThe given matrix is\n$$\nA = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix}\n$$\nThis matrix is Hermitian, as $A = A^T = A^H$, where $A^H$ is the conjugate transpose of $A$.\n\n### Part 1: Characteristic Polynomial and Eigenvalues\n\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\nFirst, we construct the matrix $A - \\lambda I$:\n$$\nA - \\lambda I = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4-\\lambda  1 \\\\ 1  2-\\lambda \\end{pmatrix}\n$$\nNext, we compute the determinant of this matrix to find the characteristic polynomial $p(\\lambda)$:\n$$\np(\\lambda) = \\det(A - \\lambda I) = (4-\\lambda)(2-\\lambda) - (1)(1)\n$$\n$$\np(\\lambda) = 8 - 4\\lambda - 2\\lambda + \\lambda^2 - 1\n$$\n$$\np(\\lambda) = \\lambda^2 - 6\\lambda + 7\n$$\nThis is the characteristic polynomial of $A$. To find the eigenvalues, we set $p(\\lambda) = 0$ and solve for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{-(-6) \\pm \\sqrt{(-6)^2 - 4(1)(7)}}{2(1)} = \\frac{6 \\pm \\sqrt{36 - 28}}{2} = \\frac{6 \\pm \\sqrt{8}}{2}\n$$\nSince $\\sqrt{8} = \\sqrt{4 \\cdot 2} = 2\\sqrt{2}$, the eigenvalues are:\n$$\n\\lambda = \\frac{6 \\pm 2\\sqrt{2}}{2} = 3 \\pm \\sqrt{2}\n$$\nLet us denote the larger eigenvalue as $\\lambda_1$ and the smaller as $\\lambda_2$:\n$$\n\\lambda_1 = 3 + \\sqrt{2}\n$$\n$$\n\\lambda_2 = 3 - \\sqrt{2}\n$$\n\n### Part 2: Eigenvectors and Normalization\n\nFor each eigenvalue, we find a corresponding eigenvector $v$ by solving the homogeneous system $(A - \\lambda I)v = 0$.\n\nFor $\\lambda_1 = 3 + \\sqrt{2}$:\nThe system is $(A - \\lambda_1 I)v_1 = 0$, where $v_1 = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.\n$$\n\\left(\\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} - (3 + \\sqrt{2})\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1 - \\sqrt{2}  1 \\\\ 1  -1 - \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get the equation $(1 - \\sqrt{2})x + y = 0$, which implies $y = (\\sqrt{2} - 1)x$. Choosing $x=1$ gives a non-zero eigenvector $v'_1 = \\begin{pmatrix} 1 \\\\ \\sqrt{2} - 1 \\end{pmatrix}$.\nTo normalize this vector, we compute its $2$-norm:\n$$\n\\|v'_1\\|_2 = \\sqrt{1^2 + (\\sqrt{2} - 1)^2} = \\sqrt{1 + (2 - 2\\sqrt{2} + 1)} = \\sqrt{4 - 2\\sqrt{2}}\n$$\nThe normalized eigenvector $v_1$ is:\n$$\nv_1 = \\frac{1}{\\|v'_1\\|_2} v'_1 = \\frac{1}{\\sqrt{4 - 2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ \\sqrt{2} - 1 \\end{pmatrix}\n$$\n\nFor $\\lambda_2 = 3 - \\sqrt{2}$:\nThe system is $(A - \\lambda_2 I)v_2 = 0$.\n$$\n\\left(\\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} - (3 - \\sqrt{2})\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1 + \\sqrt{2}  1 \\\\ 1  -1 + \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get $(1 + \\sqrt{2})x + y = 0$, which implies $y = -(1 + \\sqrt{2})x$. Choosing $x=1$ gives a non-zero eigenvector $v'_2 = \\begin{pmatrix} 1 \\\\ -(1 + \\sqrt{2}) \\end{pmatrix}$.\nTo normalize this vector, we compute its $2$-norm:\n$$\n\\|v'_2\\|_2 = \\sqrt{1^2 + (-(1 + \\sqrt{2}))^2} = \\sqrt{1 + (1 + 2\\sqrt{2} + 2)} = \\sqrt{4 + 2\\sqrt{2}}\n$$\nThe normalized eigenvector $v_2$ is:\n$$\nv_2 = \\frac{1}{\\|v'_2\\|_2} v'_2 = \\frac{1}{\\sqrt{4 + 2\\sqrt{2}}} \\begin{pmatrix} 1 \\\\ -(1 + \\sqrt{2}) \\end{pmatrix}\n$$\n\n### Part 3: Orthogonality Verification and Explanation\n\nTo verify that the eigenvectors are orthogonal, we compute their Euclidean inner product (dot product), $\\langle v_1, v_2 \\rangle = v_1^T v_2$. It is simpler to use the unnormalized eigenvectors $v'_1$ and $v'_2$, as orthogonality is independent of scaling.\n$$\n\\langle v'_1, v'_2 \\rangle = (1)(1) + (\\sqrt{2} - 1)(-(1 + \\sqrt{2})) = 1 - (\\sqrt{2} - 1)(\\sqrt{2} + 1)\n$$\nUsing the difference of squares formula, $(a-b)(a+b) = a^2 - b^2$:\n$$\n\\langle v'_1, v'_2 \\rangle = 1 - ((\\sqrt{2})^2 - 1^2) = 1 - (2 - 1) = 1 - 1 = 0\n$$\nSince the inner product is $0$, the eigenvectors are orthogonal.\n\nThis property is general for Hermitian matrices with distinct eigenvalues. Let $A$ be a Hermitian matrix ($A=A^H$). Let $(\\lambda_1, v_1)$ and $(\\lambda_2, v_2)$ be two eigenpairs with $\\lambda_1 \\neq \\lambda_2$.\nFrom $Av_1 = \\lambda_1 v_1$, we compute the inner product with $v_2$: $\\langle Av_1, v_2 \\rangle = \\langle \\lambda_1 v_1, v_2 \\rangle = \\lambda_1 \\langle v_1, v_2 \\rangle$.\nUsing the adjoint property of the inner product, $\\langle Av_1, v_2 \\rangle = v_2^H A v_1$. Since $A$ is Hermitian, $A=A^H$, so $v_2^H A v_1 = (A^H v_2)^H v_1 = (A v_2)^H v_1$.\nFrom $Av_2 = \\lambda_2 v_2$, we have $(A v_2)^H v_1 = (\\lambda_2 v_2)^H v_1 = \\overline{\\lambda_2} v_2^H v_1 = \\overline{\\lambda_2} \\langle v_1, v_2 \\rangle$.\nThe eigenvalues of a Hermitian matrix are always real, so $\\overline{\\lambda_2} = \\lambda_2$.\nEquating the two expressions for $\\langle Av_1, v_2 \\rangle$:\n$$\n\\lambda_1 \\langle v_1, v_2 \\rangle = \\lambda_2 \\langle v_1, v_2 \\rangle\n$$\n$$\n(\\lambda_1 - \\lambda_2) \\langle v_1, v_2 \\rangle = 0\n$$\nSince the eigenvalues are distinct, $\\lambda_1 - \\lambda_2 \\neq 0$. Therefore, we must have $\\langle v_1, v_2 \\rangle = 0$, which proves the eigenvectors are orthogonal. Our eigenvalues $\\lambda_1 = 3 + \\sqrt{2}$ and $\\lambda_2 = 3 - \\sqrt{2}$ are distinct, confirming the theory.\n\n### Part 4: Spectral Condition Number\n\nThe spectral condition number is defined as $\\kappa_2(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $A$, respectively. The singular values of $A$ are the square roots of the eigenvalues of the matrix $A^H A$.\nSince $A$ is Hermitian ($A^H = A$), we are interested in the eigenvalues of $A^2$. If $\\lambda_i$ are the eigenvalues of $A$, then $\\lambda_i^2$ are the eigenvalues of $A^2$. The singular values are thus $\\sigma_i = \\sqrt{\\lambda_i^2} = |\\lambda_i|$.\nIn our case, the eigenvalues of $A$ are $\\lambda_1 = 3 + \\sqrt{2}$ and $\\lambda_2 = 3 - \\sqrt{2}$. Both are positive, since $3 = \\sqrt{9}$ and $\\sqrt{2}  \\sqrt{9}$.\nTherefore, the singular values of $A$ are equal to its eigenvalues:\n$$\n\\sigma_{\\max} = \\sigma_1 = |\\lambda_1| = \\lambda_1 = 3 + \\sqrt{2}\n$$\n$$\n\\sigma_{\\min} = \\sigma_2 = |\\lambda_2| = \\lambda_2 = 3 - \\sqrt{2}\n$$\nThe spectral condition number is the ratio of these values:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{3 + \\sqrt{2}}{3 - \\sqrt{2}}\n$$\nTo express this in a simplified analytic form, we rationalize the denominator:\n$$\n\\kappa_2(A) = \\frac{3 + \\sqrt{2}}{3 - \\sqrt{2}} \\times \\frac{3 + \\sqrt{2}}{3 + \\sqrt{2}} = \\frac{(3 + \\sqrt{2})^2}{3^2 - (\\sqrt{2})^2} = \\frac{3^2 + 2(3)(\\sqrt{2}) + (\\sqrt{2})^2}{9 - 2}\n$$\n$$\n\\kappa_2(A) = \\frac{9 + 6\\sqrt{2} + 2}{7} = \\frac{11 + 6\\sqrt{2}}{7}\n$$\nThis is the final exact expression for the spectral condition number of $A$.",
            "answer": "$$\n\\boxed{\\frac{11 + 6\\sqrt{2}}{7}}\n$$"
        },
        {
            "introduction": "While Hermitian matrices exhibit ideal numerical stability, many applications involve non-Hermitian matrices whose eigenvalues can be exquisitely sensitive to small perturbations. This practice explores this critical concept of eigenvalue conditioning by examining a defective matrix, where the algebraic multiplicity of an eigenvalue exceeds its geometric multiplicity. By deriving the eigenvalue condition number from first principles, you will uncover why such matrices pose a significant challenge for numerical algorithms and gain a deeper appreciation for the stability properties discussed in the previous exercise. ",
            "id": "3597594",
            "problem": "Consider the standard eigenvalue problem $A x = \\lambda x$ for the matrix\n$$\nA = \\begin{pmatrix} 1  100 \\\\ 0  1 \\end{pmatrix}.\n$$\nLet $\\lambda = 1$ denote the (algebraically double) eigenvalue of $A$. Work with the operator $2$-norm (also called the spectral norm). Using only foundational definitions—namely, the definition of eigenvalues as roots of the characteristic polynomial, the operator $2$-norm, and the notion of (absolute) condition number as the local Lipschitz constant of the mapping that sends a matrix to its spectrum—compute the absolute condition number of the eigenvalue $\\lambda = 1$ at $A$, defined as\n$$\n\\kappa(\\lambda; A) \\;=\\; \\limsup_{\\varepsilon \\to 0^+} \\;\\sup_{\\|E\\|_2 \\le \\varepsilon} \\;\\frac{\\mathrm{dist}\\!\\left(\\lambda, \\Lambda(A+E)\\right)}{\\varepsilon},\n$$\nwhere $\\Lambda(M)$ denotes the set of eigenvalues of a matrix $M$, and $\\mathrm{dist}(\\lambda, \\Lambda(M)) = \\min_{\\mu \\in \\Lambda(M)} |\\lambda - \\mu|$. Do not invoke any pre-derived perturbation formulas; derive the behavior from first principles by analyzing the characteristic polynomial of $A+E$ for small perturbations $E$. In your reasoning, explain qualitatively why the sensitivity behaves as it does for this matrix.\n\nYour final answer must be a single symbolic expression. No rounding is required and no units are involved.",
            "solution": "The problem asks for the absolute condition number of the eigenvalue $\\lambda = 1$ for the matrix $A = \\begin{pmatrix} 1  100 \\\\ 0  1 \\end{pmatrix}$. The norm is the operator $2$-norm, $\\| \\cdot \\|_2$. The absolute condition number is defined as:\n$$\n\\kappa(\\lambda; A) = \\limsup_{\\varepsilon \\to 0^+} \\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\mathrm{dist}(\\lambda, \\Lambda(A+E))}{\\varepsilon}\n$$\nwhere $\\Lambda(M)$ is the spectrum of a matrix $M$, and $\\mathrm{dist}(\\lambda, \\Lambda(M)) = \\min_{\\mu \\in \\Lambda(M)} |\\lambda - \\mu|$.\n\nLet $E = \\begin{pmatrix} e_{11}  e_{12} \\\\ e_{21}  e_{22} \\end{pmatrix}$ be an arbitrary perturbation matrix such that $\\|E\\|_2 \\le \\varepsilon$. The perturbed matrix is $A+E = \\begin{pmatrix} 1+e_{11}  100+e_{12} \\\\ e_{21}  1+e_{22} \\end{pmatrix}$.\n\nThe eigenvalues $\\mu$ of $A+E$ are the roots of its characteristic polynomial, $\\det(A+E-\\mu I)=0$.\n$$\n\\det\\left( \\begin{pmatrix} 1+e_{11}-\\mu  100+e_{12} \\\\ e_{21}  1+e_{22}-\\mu \\end{pmatrix} \\right) = 0\n$$\n$$\n(1+e_{11}-\\mu)(1+e_{22}-\\mu) - e_{21}(100+e_{12}) = 0\n$$\nThe original eigenvalue is $\\lambda=1$. We are interested in the deviation of the perturbed eigenvalues $\\mu$ from $1$. Let $\\delta = \\mu - 1$, which implies $\\mu = 1+\\delta$. Substituting this into the characteristic equation gives:\n$$\n(e_{11}-\\delta)(e_{22}-\\delta) - e_{21}(100+e_{12}) = 0\n$$\nExpanding this, we obtain a quadratic equation for the eigenvalue shift $\\delta$:\n$$\n\\delta^2 - (e_{11}+e_{22})\\delta + (e_{11}e_{22} - 100e_{21} - e_{12}e_{21}) = 0\n$$\nThe solutions for $\\delta$ are given by the quadratic formula:\n$$\n\\delta_{1,2} = \\frac{e_{11}+e_{22} \\pm \\sqrt{(e_{11}+e_{22})^2 - 4(e_{11}e_{22} - 100e_{21} - e_{12}e_{21})}}{2}\n$$\nSimplifying the discriminant:\n$$\n\\delta_{1,2} = \\frac{e_{11}+e_{22} \\pm \\sqrt{(e_{11}-e_{22})^2 + 400e_{21} + 4e_{12}e_{21}}}{2}\n$$\nThe perturbed eigenvalues are $\\mu_1 = 1+\\delta_1$ and $\\mu_2 = 1+\\delta_2$. The distance from $\\lambda=1$ to the set of perturbed eigenvalues is:\n$$\n\\mathrm{dist}(1, \\Lambda(A+E)) = \\min(|\\mu_1-1|, |\\mu_2-1|) = \\min(|\\delta_1|, |\\delta_2|)\n$$\nWe need to evaluate $\\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\min(|\\delta_1|, |\\delta_2|)}{\\varepsilon}$.\n\nTo find this supremum, we seek a \"worst-case\" perturbation $E$ that maximizes this ratio. The term $400e_{21}$ under the square root suggests that the entry $e_{21}$ plays a crucial role. For a general matrix, we have the inequality $|e_{ij}| \\le \\|E\\|_2$. Thus, for $\\|E\\|_2 \\le \\varepsilon$, we have $|e_{ij}| \\le \\varepsilon$.\n\nConsider a specific perturbation designed to make the term $400e_{21}$ dominant. Let us choose the perturbation matrix\n$$\nE_0 = \\begin{pmatrix} 0  0 \\\\ \\varepsilon  0 \\end{pmatrix}\n$$\nThe $2$-norm of this matrix is $\\|E_0\\|_2 = \\sup_{\\|v\\|_2=1} \\|E_0 v\\|_2 = \\sup_{v_1^2+v_2^2=1} \\|\\begin{pmatrix} 0 \\\\ \\varepsilon v_1 \\end{pmatrix}\\|_2 = \\sup \\varepsilon|v_1| = \\varepsilon$. So, this is a valid perturbation with $\\|E_0\\|_2 = \\varepsilon$.\n\nFor this choice of $E_0$, we have $e_{11}=e_{12}=e_{22}=0$ and $e_{21}=\\varepsilon$. The equation for $\\delta$ becomes:\n$$\n\\delta^2 - 0 \\cdot \\delta + (0 - 100\\varepsilon - 0) = 0\n$$\n$$\n\\delta^2 - 100\\varepsilon = 0\n$$\nThe solutions are $\\delta_{1,2} = \\pm\\sqrt{100\\varepsilon} = \\pm 10\\sqrt{\\varepsilon}$.\nThe perturbed eigenvalues are $\\mu_{1,2} = 1 \\pm 10\\sqrt{\\varepsilon}$.\nThe distance from $1$ to the perturbed spectrum is:\n$$\n\\mathrm{dist}(1, \\Lambda(A+E_0)) = \\min(|10\\sqrt{\\varepsilon}|, |-10\\sqrt{\\varepsilon}|) = 10\\sqrt{\\varepsilon}\n$$\nNow we compute the ratio for this specific perturbation:\n$$\n\\frac{\\mathrm{dist}(1, \\Lambda(A+E_0))}{\\varepsilon} = \\frac{10\\sqrt{\\varepsilon}}{\\varepsilon} = \\frac{10}{\\sqrt{\\varepsilon}}\n$$\nThe definition of the condition number involves the supremum over all perturbations $E$ with $\\|E\\|_2 \\le \\varepsilon$. The value of the supremum must be at least the value for any particular instance. Thus:\n$$\n\\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\mathrm{dist}(1, \\Lambda(A+E))}{\\varepsilon} \\ge \\frac{10}{\\sqrt{\\varepsilon}}\n$$\nFinally, we take the limit superior as $\\varepsilon \\to 0^+$:\n$$\n\\kappa(1; A) = \\limsup_{\\varepsilon \\to 0^+} \\sup_{\\|E\\|_2 \\le \\varepsilon} \\frac{\\mathrm{dist}(1, \\Lambda(A+E))}{\\varepsilon} \\ge \\limsup_{\\varepsilon \\to 0^+} \\frac{10}{\\sqrt{\\varepsilon}}\n$$\nAs $\\varepsilon \\to 0^+$, the term $\\frac{10}{\\sqrt{\\varepsilon}}$ tends to infinity. Therefore:\n$$\n\\kappa(1; A) = \\infty\n$$\nQualitative explanation: The large sensitivity of the eigenvalue is a consequence of the algebraic structure of the matrix $A$. The matrix $A$ has an eigenvalue $\\lambda=1$ with algebraic multiplicity $2$, but the dimension of the corresponding eigenspace is $1$ (it is spanned by the vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$). The geometric multiplicity ($1$) is less than the algebraic multiplicity ($2$), which means the matrix is defective and non-diagonalizable. It is similar to a Jordan block of size $2$. Eigenvalues of defective matrices are inherently sensitive to perturbations. For a perturbation of norm $\\varepsilon$, the resulting change in the eigenvalue is of the order $\\varepsilon^{1/m}$, where $m$ is the size of the largest Jordan block associated with that eigenvalue. In our case, $m=2$, so the eigenvalue shift $\\delta$ is of order $\\sqrt{\\varepsilon}$. The condition number, as defined, measures the rate of change $|\\delta|/\\varepsilon$, which behaves like $\\sqrt{\\varepsilon}/\\varepsilon = 1/\\sqrt{\\varepsilon}$. This expression diverges as $\\varepsilon \\to 0$, indicating infinite local sensitivity. The large off-diagonal entry, $100$, further amplifies this effect. As shown in the derivation, the eigenvalue shift is proportional to $\\sqrt{100}=10$, but this finite factor multiplies a term that diverges.",
            "answer": "$$\n\\boxed{\\infty}\n$$"
        },
        {
            "introduction": "Having explored the theoretical properties and numerical sensitivities of eigenvalues, we now turn to a powerful computational method for finding them: the Rayleigh Quotient Iteration (RQI). This algorithm is celebrated for its remarkable speed, exhibiting local cubic convergence for symmetric matrices—a rate rarely seen in numerical analysis. This practice asks you to implement RQI, connecting the theoretical elegance of symmetric matrices to the practical power of a state-of-the-art iterative solver and observing its rapid convergence in practice. ",
            "id": "3597597",
            "problem": "Implement a complete program that, for each matrix in a provided test suite, performs exactly three steps of the Rayleigh quotient iteration (RQI) starting from the initial vector $x_0 = (1,1,1)^{\\top}$ to approximate the largest eigenvalue of a given real symmetric matrix $A \\in \\mathbb{R}^{3 \\times 3}$. After the three iterations, report the approximate eigenvalue and the Euclidean norm of the residual vector.\n\nUse the following fundamental base:\n- The standard eigenvalue problem seeks $(\\lambda, x)$ with $x \\neq 0$ such that $A x = \\lambda x$ for a given real symmetric matrix $A$.\n- The Rayleigh quotient of a nonzero vector $x$ with respect to $A$ is defined by\n$$\n\\rho_A(x) = \\frac{x^{\\top} A x}{x^{\\top} x}.\n$$\n- For a symmetric matrix $A$, stationary points of $\\rho_A(x)$ on the unit sphere solve the eigenvalue equation $A x = \\lambda x$ with $\\lambda = \\rho_A(x)$.\n\nYour program must:\n1. For each test matrix $A$, set $x_0 = (1,1,1)^{\\top}$ and run exactly $3$ Rayleigh quotient iteration steps. At each iteration, you must normalize the iterate to unit Euclidean norm.\n2. After completing the $3$ iterations, compute the approximate eigenvalue $\\widehat{\\lambda}$ as the Rayleigh quotient $\\rho_A(x)$ of the final normalized vector $x$, and compute the residual vector $r = A x - \\widehat{\\lambda} x$.\n3. Report the pair consisting of $\\widehat{\\lambda}$ and $\\|r\\|_2$, where $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nTest suite:\n- Case $1$ (distinct well-separated eigenvalues):\n$$\nA_1 = \\begin{bmatrix}\n4  1  0 \\\\\n1  3  1 \\\\\n0  1  2\n\\end{bmatrix}.\n$$\n- Case $2$ (largest eigenvalue of multiplicity $2$):\n$$\nA_2 = \\begin{bmatrix}\n2  0  0 \\\\\n0  2  0 \\\\\n0  0  1\n\\end{bmatrix}.\n$$\n- Case $3$ (two close dominant eigenvalues in a principal block):\n$$\nA_3 = \\begin{bmatrix}\n10  9  0 \\\\\n9  10  0 \\\\\n0  0  0.1\n\\end{bmatrix}.\n$$\n- Case $4$ (scale disparity and a small eigenvalue):\n$$\nA_4 = \\begin{bmatrix}\n10^{-6}  0  0 \\\\\n0  5  2 \\\\\n0  2  5\n\\end{bmatrix}.\n$$\n\nNumerical requirements and definitions:\n- Treat $x_0$ as a real vector and normalize after each solve so that $\\|x\\|_2 = 1$ at each iteration.\n- Use the Euclidean norm $\\|\\cdot\\|_2$ for all vector norms.\n- For each case, after exactly $3$ iterations, compute and report:\n  - The approximate largest eigenvalue estimate $\\widehat{\\lambda} = \\rho_A(x)$ of the final $x$.\n  - The residual norm $\\|A x - \\widehat{\\lambda} x\\|_2$.\n- There are no physical units or angle units involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces.\n- The result for each case must be a two-element list $[\\widehat{\\lambda}, \\|r\\|_2]$.\n- Aggregate the four case results as a list in the order $A_1, A_2, A_3, A_4$, producing a single line like\n$[\\![\\widehat{\\lambda}_1,\\|r_1\\|_2],[\\widehat{\\lambda}_2,\\|r_2\\|_2],[\\widehat{\\lambda}_3,\\|r_3\\|_2],[\\widehat{\\lambda}_4,\\|r_4\\|_2]\\!]$\nwith each floating-point number rounded to exactly $10$ decimal places and printed without spaces.",
            "solution": "We begin from the standard eigenvalue problem for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$: find $(\\lambda, x)$ with $x \\neq 0$ such that $A x = \\lambda x$. For any nonzero vector $x \\in \\mathbb{R}^n$, the Rayleigh quotient is defined as\n$$\n\\rho_A(x) = \\frac{x^{\\top} A x}{x^{\\top} x}.\n$$\nA fundamental fact is that, for symmetric $A$, the stationary points of $\\rho_A(x)$ constrained to the unit sphere $\\{x \\in \\mathbb{R}^n : x^{\\top} x = 1\\}$ correspond exactly to eigenvectors of $A$, and the Rayleigh quotient at such a stationary point equals the corresponding eigenvalue. Specifically, if $x$ is stationary, then introducing a Lagrange multiplier $\\lambda$ for the constraint yields the stationarity condition\n$$\n\\nabla_x \\left(x^{\\top} A x - \\lambda (x^{\\top} x - 1)\\right) = 0\n\\quad \\Longrightarrow \\quad\n2 A x - 2 \\lambda x = 0 \\quad \\Longrightarrow \\quad (A - \\lambda I) x = 0,\n$$\nso $x$ is an eigenvector and $\\lambda = \\rho_A(x)$.\n\nRayleigh quotient iteration (RQI) can be derived by applying Newton’s method to the nonlinear system\n$$\nF(x, \\lambda) = \\begin{bmatrix} (A - \\lambda I) x \\\\ \\tfrac{1}{2}(x^{\\top} x - 1) \\end{bmatrix} = 0,\n$$\nwhere the second component enforces the unit-norm constraint. Eliminating the Lagrange multiplier variable through the Rayleigh quotient $\\lambda = \\rho_A(x)$ leads to a practical iteration where, given an iterate $x_k$ (normalized to $\\|x_k\\|_2 = 1$), we form the Rayleigh quotient\n$$\n\\mu_k = \\rho_A(x_k) = \\frac{x_k^{\\top} A x_k}{x_k^{\\top} x_k},\n$$\nthen take a Newton step for the equation $(A - \\mu_k I) x = 0$ in the tangent direction by solving\n$$\n(A - \\mu_k I) y_k = x_k,\n$$\nand finally renormalize $x_{k+1} = \\frac{y_k}{\\|y_k\\|_2}$. After the update, one may recompute $\\mu_{k+1} = \\rho_A(x_{k+1})$. For real symmetric $A$, if $x_0$ is sufficiently close to an eigenvector, RQI enjoys locally cubic convergence to the corresponding eigenpair; moreover, the Rayleigh quotient at each step is a refined eigenvalue estimate.\n\nIn this task, we are asked to:\n- Start from $x_0 = (1,1,1)^{\\top}$.\n- Perform exactly $3$ iterations of the above RQI scheme for each test matrix $A$.\n- After completing the $3$ iterations, compute the approximate eigenvalue $\\widehat{\\lambda} = \\rho_A(x)$ at the final normalized iterate $x$, and compute the residual vector $r = A x - \\widehat{\\lambda} x$ with residual norm $\\|r\\|_2$.\n\nAlgorithmic steps for one matrix $A$:\n1. Initialize $x \\leftarrow x_0 = (1,1,1)^{\\top}$ and normalize $x \\leftarrow \\frac{x}{\\|x\\|_2}$.\n2. For $k = 1,2,3$:\n   - Compute the Rayleigh quotient $\\mu = \\frac{x^{\\top} A x}{x^{\\top} x}$.\n   - Solve the shifted linear system $(A - \\mu I) y = x$ for $y$. Since $A$ is symmetric and $\\mu$ should not coincide exactly with an eigenvalue in finite precision for the given scenarios, this system is generically nonsingular. If it is near singular, numerical linear algebra solvers will still produce a solution vector consistent with the floating-point arithmetic model.\n   - Normalize the update $x \\leftarrow \\frac{y}{\\|y\\|_2}$.\n3. After the loop, set $\\widehat{\\lambda} \\leftarrow \\frac{x^{\\top} A x}{x^{\\top} x}$ and compute $r \\leftarrow A x - \\widehat{\\lambda} x$.\n4. Report $\\widehat{\\lambda}$ and $\\|r\\|_2$.\n\nTest suite justification and coverage:\n- Case $1$ uses\n$$\nA_1 = \\begin{bmatrix}\n4  1  0 \\\\\n1  3  1 \\\\\n0  1  2\n\\end{bmatrix},\n$$\na symmetric, positive definite matrix with distinct eigenvalues that are moderately well separated. This is a standard “happy path” case where RQI should rapidly refine the eigenvalue corresponding to the dominant eigenvector component of $x_0$.\n- Case $2$ uses\n$$\nA_2 = \\begin{bmatrix}\n2  0  0 \\\\\n0  2  0 \\\\\n0  0  1\n\\end{bmatrix},\n$$\nwhich has a largest eigenvalue of multiplicity $2$. This probes behavior when the top eigenspace is not one-dimensional; RQI can still converge toward the invariant subspace corresponding to the largest eigenvalue, and the Rayleigh quotient remains a valid eigenvalue estimate.\n- Case $3$ uses\n$$\nA_3 = \\begin{bmatrix}\n10  9  0 \\\\\n9  10  0 \\\\\n0  0  0.1\n\\end{bmatrix},\n$$\nwhere the leading $2 \\times 2$ block has two close eigenvalues, creating a small spectral gap near the top end. This tests sensitivity and the refinement provided by RQI with a challenging cluster.\n- Case $4$ uses\n$$\nA_4 = \\begin{bmatrix}\n10^{-6}  0  0 \\\\\n0  5  2 \\\\\n0  2  5\n\\end{bmatrix},\n$$\nwhich has disparate scales and includes a very small eigenvalue, probing numerical stability and conditioning with respect to the shift-and-invert step.\n\nNumerical outputs:\n- For each case, after $3$ iterations, compute $\\widehat{\\lambda}$ and $\\|r\\|_2$ as described.\n- Rounding and formatting: the program must round each floating-point result to exactly $10$ decimal places and output a single line of the form\n$$\n[\\,[\\widehat{\\lambda}_1,\\|r_1\\|_2],[\\widehat{\\lambda}_2,\\|r_2\\|_2],[\\widehat{\\lambda}_3,\\|r_3\\|_2],[\\widehat{\\lambda}_4,\\|r_4\\|_2]\\,],\n$$\nwith no spaces.\n\nThe algorithm is grounded in the core definition of the Rayleigh quotient and the eigenvalue equation, and it is implemented via the shift-and-invert Newton step intrinsic to Rayleigh quotient iteration. The residual norm $\\|A x - \\widehat{\\lambda} x\\|_2$ quantifies the quality of the computed eigenpair after the prescribed number of iterations.",
            "answer": "```python\nimport numpy as np\n\ndef rayleigh_quotient(A, x):\n    return float(x.T @ A @ x) / float(x.T @ x)\n\ndef rqi_three_steps(A, x0):\n    # Normalize initial vector\n    x = x0.astype(float).copy()\n    x /= np.linalg.norm(x, 2)\n    # Perform exactly 3 iterations\n    for _ in range(3):\n        mu = rayleigh_quotient(A, x)\n        # Solve (A - mu I) y = x\n        M = A - mu * np.eye(A.shape[0])\n        try:\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # In the unlikely event of near singularity, use least squares fallback\n            y, *_ = np.linalg.lstsq(M, x, rcond=None)\n        # Normalize\n        x = y / np.linalg.norm(y, 2)\n    # Final Rayleigh quotient and residual\n    lam = rayleigh_quotient(A, x)\n    r = A @ x - lam * x\n    resid = np.linalg.norm(r, 2)\n    return lam, resid\n\ndef format_results(results, decimals=10):\n    # Format as a single list with inner lists, no spaces, fixed decimals\n    formatted_pairs = []\n    for lam, resid in results:\n        lam_str = f\"{lam:.{decimals}f}\"\n        resid_str = f\"{resid:.{decimals}f}\"\n        formatted_pairs.append(f\"[{lam_str},{resid_str}]\")\n    return \"[\" + \",\".join(formatted_pairs) + \"]\"\n\ndef solve():\n    # Define the test matrices\n    A1 = np.array([[4.0, 1.0, 0.0],\n                   [1.0, 3.0, 1.0],\n                   [0.0, 1.0, 2.0]])\n    A2 = np.array([[2.0, 0.0, 0.0],\n                   [0.0, 2.0, 0.0],\n                   [0.0, 0.0, 1.0]])\n    A3 = np.array([[10.0, 9.0, 0.0],\n                   [9.0, 10.0, 0.0],\n                   [0.0, 0.0, 0.1]])\n    A4 = np.array([[1e-6, 0.0, 0.0],\n                   [0.0, 5.0, 2.0],\n                   [0.0, 2.0, 5.0]])\n\n    test_matrices = [A1, A2, A3, A4]\n    x0 = np.array([1.0, 1.0, 1.0])\n\n    results = []\n    for A in test_matrices:\n        lam, resid = rqi_three_steps(A, x0)\n        results.append((lam, resid))\n\n    print(format_results(results, decimals=10))\n\nsolve()\n```"
        }
    ]
}