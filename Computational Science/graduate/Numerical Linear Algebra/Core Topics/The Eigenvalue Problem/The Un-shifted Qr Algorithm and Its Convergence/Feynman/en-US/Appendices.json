{
    "hands_on_practices": [
        {
            "introduction": "The unshifted QR algorithm is an elegant iterative process that uses a sequence of QR factorizations and similarity transformations to gradually reveal a matrix's eigenvalues. Performing the first few iterations by hand is an invaluable exercise for demystifying the algorithm's mechanics and solidifying your understanding of the underlying QR decomposition. This practice  will have you compute the first two iterates for a simple symmetric matrix, observing how it begins to converge towards a diagonal form.",
            "id": "3598477",
            "problem": "Consider the unshifted orthogonal-triangular (QR) algorithm applied to the real symmetric matrix\n$$\nA_0=\\begin{pmatrix}2 & 1 \\\\ 1 & 2\\end{pmatrix}.\n$$\nThe unshifted QR iteration is defined as follows. At iteration index $k$, compute the orthogonal-triangular (QR) decomposition $A_k=Q_k R_k$, where $Q_k$ is orthogonal (so $Q_k^{\\top}Q_k=I$) and $R_k$ is upper triangular, and then form $A_{k+1}=R_k Q_k$. This procedure is known to produce a sequence of matrices connected by similarity transformations, and it is used to iteratively reveal eigenvalue information.\n\nUsing only the fundamental definition of the orthogonal-triangular (QR) decomposition for full-rank square matrices and the orthogonality property $Q_k^{\\top}Q_k=I$, perform exactly two unshifted QR iterations starting from $A_0$, thereby constructing $A_1$ and $A_2$. At each step, explicitly confirm the similarity relation that connects $A_{k+1}$ to $A_k$.\n\nYou must compute these decompositions and products exactly, without appeal to any pre-stated algorithmic shortcuts other than the defining properties of the orthogonal-triangular (QR) decomposition and orthogonality. Express your final result as an exact rational number.\n\nWhat is the exact value of the $(1,1)$ entry of $A_2$?",
            "solution": "The problem requires performing two iterations of the un-shifted QR algorithm on the matrix $A_0 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$. The algorithm is defined by the sequence $A_k = Q_k R_k$ and $A_{k+1} = R_k Q_k$, where $A_k = Q_k R_k$ is the orthogonal-triangular (QR) decomposition of $A_k$.\n\nFirst, let's analyze the given matrix. The matrix $A_0$ is a $2 \\times 2$ real symmetric matrix.\n$$\nA_0 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThe determinant is $\\det(A_0) = (2)(2) - (1)(1) = 3 \\neq 0$, so the matrix is full-rank and a unique QR decomposition (up to signs) exists.\n\n**Iteration 1: Computation of $A_1$**\n\nWe begin by computing the QR decomposition of $A_0$. Let the columns of $A_0$ be $a_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$. We apply the Gram-Schmidt process to the column vectors of $A_0$ to find the orthogonal matrix $Q_0$.\n\nLet the columns of $Q_0$ be $q_1$ and $q_2$.\nThe first unnormalized orthogonal vector $u_1$ is simply $a_1$:\n$$\nu_1 = a_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe squared norm is $\\|u_1\\|^2 = 2^2 + 1^2 = 5$. The norm is $\\|u_1\\| = \\sqrt{5}$.\nThe first column of $Q_0$ is the normalized vector $q_1$:\n$$\nq_1 = \\frac{u_1}{\\|u_1\\|} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe second unnormalized orthogonal vector $u_2$ is found by subtracting the projection of $a_2$ onto $u_1$:\n$$\nu_2 = a_2 - \\frac{a_2^{\\top} u_1}{u_1^{\\top} u_1} u_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{(1)(2) + (2)(1)}{5} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{4}{5} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{8}{5} \\\\ 2 - \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{5} \\\\ \\frac{6}{5} \\end{pmatrix}\n$$\nThe squared norm is $\\|u_2\\|^2 = (-\\frac{3}{5})^2 + (\\frac{6}{5})^2 = \\frac{9}{25} + \\frac{36}{25} = \\frac{45}{25} = \\frac{9}{5}$. The norm is $\\|u_2\\| = \\sqrt{\\frac{9}{5}} = \\frac{3}{\\sqrt{5}}$.\nThe second column of $Q_0$ is the normalized vector $q_2$:\n$$\nq_2 = \\frac{u_2}{\\|u_2\\|} = \\frac{1}{3/\\sqrt{5}} \\begin{pmatrix} -3/5 \\\\ 6/5 \\end{pmatrix} = \\frac{\\sqrt{5}}{3} \\frac{3}{5} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\n$$\nThus, the orthogonal matrix $Q_0$ is:\n$$\nQ_0 = [q_1 \\ q_2] = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 & -1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThe upper triangular matrix $R_0$ is obtained from $A_0 = Q_0 R_0 \\implies R_0 = Q_0^{\\top} A_0$, since $Q_0$ is orthogonal ($Q_0^{\\top}Q_0 = I$).\n$$\nR_0 = \\left(\\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 & 1 \\\\ -1 & 2 \\end{pmatrix}\\right) \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} (2)(2)+(1)(1) & (2)(1)+(1)(2) \\\\ (-1)(2)+(2)(1) & (-1)(1)+(2)(2) \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 & 4 \\\\ 0 & 3 \\end{pmatrix}\n$$\nHaving found $Q_0$ and $R_0$, we compute the next matrix in the sequence, $A_1$:\n$$\nA_1 = R_0 Q_0 = \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 5 & 4 \\\\ 0 & 3 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 & -1 \\\\ 1 & 2 \\end{pmatrix} \\right) = \\frac{1}{5} \\begin{pmatrix} (5)(2)+(4)(1) & (5)(-1)+(4)(2) \\\\ (0)(2)+(3)(1) & (0)(-1)+(3)(2) \\end{pmatrix}\n$$\n$$\nA_1 = \\frac{1}{5} \\begin{pmatrix} 14 & 3 \\\\ 3 & 6 \\end{pmatrix}\n$$\nTo confirm the similarity relation, we show $A_1 = Q_0^{\\top} A_0 Q_0$. From the definitions, $A_1 = R_0 Q_0$ and $R_0 = Q_0^{\\top} A_0$. Substituting the second into the first gives $A_1 = (Q_0^{\\top} A_0) Q_0$, which is the desired relation. Since $A_0$ is symmetric, $A_1$ must also be symmetric, which our result confirms.\n\n**Iteration 2: Computation of $A_2$**\n\nWe now repeat the process for $A_1 = \\frac{1}{5} \\begin{pmatrix} 14 & 3 \\\\ 3 & 6 \\end{pmatrix}$. Let its columns be $a'_1 = \\frac{1}{5} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}$ and $a'_2 = \\frac{1}{5} \\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix}$. We compute the QR decomposition $A_1 = Q_1 R_1$.\n\nFirst unnormalized vector $u'_1$:\n$$\nu'_1 = a'_1 = \\frac{1}{5} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}\n$$\nIts squared norm is $\\|u'_1\\|^2 = (\\frac{1}{5})^2 (14^2 + 3^2) = \\frac{1}{25}(196+9) = \\frac{205}{25} = \\frac{41}{5}$. The norm is $\\|u'_1\\|=\\sqrt{\\frac{41}{5}} = \\frac{\\sqrt{205}}{5}$.\nThe first column of $Q_1$ is $q'_1$:\n$$\nq'_1 = \\frac{u'_1}{\\|u'_1\\|} = \\frac{\\frac{1}{5} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}}{\\frac{\\sqrt{205}}{5}} = \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix}\n$$\nThe second unnormalized vector $u'_2$ is:\n$$\nu'_2 = a'_2 - \\frac{{a'_2}^{\\top} u'_1}{{u'_1}^{\\top} u'_1} u'_1\n$$\nThe dot product is ${a'_2}^{\\top} u'_1 = (\\frac{1}{5})^2 ((3)(14)+(6)(3)) = \\frac{1}{25}(42+18) = \\frac{60}{25} = \\frac{12}{5}$.\n$$\nu'_2 = \\frac{1}{5}\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix} - \\frac{12/5}{41/5} \\left( \\frac{1}{5}\\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix} \\right) = \\frac{1}{5}\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix} - \\frac{12}{41} \\frac{1}{5}\\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix} = \\frac{1}{205} \\left( 41\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix} - 12\\begin{pmatrix} 14 \\\\ 3 \\end{pmatrix} \\right)\n$$\n$$\nu'_2 = \\frac{1}{205} \\begin{pmatrix} 123-168 \\\\ 246-36 \\end{pmatrix} = \\frac{1}{205}\\begin{pmatrix} -45 \\\\ 210 \\end{pmatrix} = \\frac{15}{205}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix} = \\frac{3}{41}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix}\n$$\nIts squared norm is $\\|u'_2\\|^2 = (\\frac{3}{41})^2 ((-3)^2+14^2) = \\frac{9}{41^2}(9+196) = \\frac{9 \\cdot 205}{41^2} = \\frac{9 \\cdot 5 \\cdot 41}{41^2} = \\frac{45}{41}$. The norm is $\\|u'_2\\| = \\sqrt{\\frac{45}{41}} = \\frac{3\\sqrt{5}}{\\sqrt{41}} = \\frac{3\\sqrt{205}}{41}$.\nThe second column of $Q_1$ is $q'_2$:\n$$\nq'_2 = \\frac{u'_2}{\\|u'_2\\|} = \\frac{\\frac{3}{41}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix}}{\\frac{3\\sqrt{205}}{41}} = \\frac{1}{\\sqrt{205}}\\begin{pmatrix} -3 \\\\ 14 \\end{pmatrix}\n$$\nThe orthogonal matrix $Q_1$ is:\n$$\nQ_1 = [q'_1 \\ q'_2] = \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 & -3 \\\\ 3 & 14 \\end{pmatrix}\n$$\nThe upper triangular matrix $R_1 = Q_1^{\\top} A_1$:\n$$\nR_1 = \\left( \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 & 3 \\\\ -3 & 14 \\end{pmatrix} \\right) \\left( \\frac{1}{5} \\begin{pmatrix} 14 & 3 \\\\ 3 & 6 \\end{pmatrix} \\right) = \\frac{1}{5\\sqrt{205}} \\begin{pmatrix} 14^2+3^2 & (14)(3)+(3)(6) \\\\ (-3)(14)+(14)(3) & (-3)(3)+(14)(6) \\end{pmatrix}\n$$\n$$\nR_1 = \\frac{1}{5\\sqrt{205}} \\begin{pmatrix} 196+9 & 42+18 \\\\ -42+42 & -9+84 \\end{pmatrix} = \\frac{1}{5\\sqrt{205}} \\begin{pmatrix} 205 & 60 \\\\ 0 & 75 \\end{pmatrix} = \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 41 & 12 \\\\ 0 & 15 \\end{pmatrix}\n$$\nNow we compute $A_2 = R_1 Q_1$:\n$$\nA_2 = \\left( \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 41 & 12 \\\\ 0 & 15 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{205}} \\begin{pmatrix} 14 & -3 \\\\ 3 & 14 \\end{pmatrix} \\right) = \\frac{1}{205} \\begin{pmatrix} (41)(14)+(12)(3) & (41)(-3)+(12)(14) \\\\ (0)(14)+(15)(3) & (0)(-3)+(15)(14) \\end{pmatrix}\n$$\n$$\nA_2 = \\frac{1}{205} \\begin{pmatrix} 574+36 & -123+168 \\\\ 45 & 210 \\end{pmatrix} = \\frac{1}{205} \\begin{pmatrix} 610 & 45 \\\\ 45 & 210 \\end{pmatrix}\n$$\nFactoring out common terms (all entries and the denominator $205=5 \\times 41$ are divisible by $5$):\n$$\nA_2 = \\frac{5}{205} \\begin{pmatrix} 122 & 9 \\\\ 9 & 42 \\end{pmatrix} = \\frac{1}{41} \\begin{pmatrix} 122 & 9 \\\\ 9 & 42 \\end{pmatrix}\n$$\nAs before, the similarity relation $A_2 = Q_1^{\\top} A_1 Q_1$ is guaranteed by the construction. Since $A_1$ is symmetric, $A_2$ must also be symmetric, which our result confirms.\n\nThe problem asks for the $(1,1)$ entry of $A_2$. From the matrix $A_2$ derived above, this value is:\n$$\n(A_2)_{11} = \\frac{122}{41}\n$$\nThis fraction is irreducible as $41$ is a prime number and $122 = 2 \\times 61$.",
            "answer": "$$\\boxed{\\frac{122}{41}}$$"
        },
        {
            "introduction": "The practical utility of an iterative algorithm depends heavily on its convergence rate, and the unshifted QR algorithm is no exception. Its convergence slows dramatically for matrices with eigenvalues of nearly equal magnitude, a situation epitomized by nearly-defective matrices. By analyzing a single QR step on a perturbed Jordan block , you can directly quantify this slow convergence and gain insight into the algorithm's performance limitations.",
            "id": "2219199",
            "problem": "The unshifted QR algorithm is a fundamental method for computing the eigenvalues of a matrix. The algorithm generates a sequence of matrices $\\{A_k\\}$ starting with $A_0 = A$. Each step consists of computing the QR decomposition of the current matrix, $A_k = Q_k R_k$, and then reversing the factors to form the next matrix, $A_{k+1} = R_k Q_k$. Here, $Q_k$ is an orthogonal matrix and $R_k$ is an upper-triangular matrix. For a unique decomposition, we enforce the condition that the diagonal entries of $R_k$ must be positive.\n\nThe convergence rate of the algorithm is known to be slow for matrices with eigenvalues of nearly equal magnitude. This behavior is particularly pronounced for matrices that are close to being non-diagonalizable (defective). A canonical example of a defective matrix is a Jordan block.\n\nConsider the matrix $A$ which is a small perturbation of a Jordan block:\n$$\nA = \\begin{pmatrix} \\lambda & 1 \\\\ \\delta^2 & \\lambda \\end{pmatrix}\n$$\nwhere $\\lambda$ is a positive real number and $\\delta$ is a real parameter such that $0 < |\\delta| \\ll \\lambda$. As $\\delta \\to 0$, this matrix approaches a defective Jordan block with a double eigenvalue at $\\lambda$.\n\nLet $A_0 = A$. Perform one full iteration of the unshifted QR algorithm to obtain the matrix $A_1$. The sub-diagonal entry, $(A_1)_{2,1}$, is a key indicator of the algorithm's progress towards finding the eigenvalues. Determine the exact expression for this entry, $(A_1)_{2,1}$, as a function of $\\lambda$ and $\\delta$.",
            "solution": "Let $A_0 = A = QR$ be the QR decomposition of $A$, and $A_1 = RQ$.\nWe write $A$ by columns as $A = [a_1 \\ a_2]$ with $a_1=\\begin{pmatrix}\\lambda \\\\ \\delta^{2}\\end{pmatrix}$ and $a_2=\\begin{pmatrix}1 \\\\ \\lambda\\end{pmatrix}$. The QR decomposition with positive diagonal entries in $R$ is obtained by the Gram–Schmidt process.\n\n1.  The first column of $Q$, denoted $q_1$, and the entry $r_{11}$ of $R$ are found by normalizing $a_1$:\n    $r_{11} = \\|a_1\\|_2 = \\sqrt{\\lambda^2 + \\delta^4}$. Since $\\lambda>0$, $r_{11}>0$.\n    $q_1 = \\frac{a_1}{r_{11}} = \\frac{1}{\\sqrt{\\lambda^2 + \\delta^4}}\\begin{pmatrix}\\lambda \\\\ \\delta^2\\end{pmatrix}$.\n\n2.  The entry $r_{12}$ is the projection of $a_2$ onto $q_1$:\n    $r_{12} = q_1^\\top a_2 = \\frac{1}{\\sqrt{\\lambda^2+\\delta^4}} (\\lambda \\cdot 1 + \\delta^2 \\cdot \\lambda) = \\frac{\\lambda(1+\\delta^2)}{\\sqrt{\\lambda^2+\\delta^4}}$.\n\n3.  The second column of $Q$, $q_2$, and the entry $r_{22}$ are found from the vector $u_2 = a_2 - r_{12} q_1$:\n    $$\n    u_2 = \\begin{pmatrix}1 \\\\ \\lambda\\end{pmatrix} - \\frac{\\lambda(1+\\delta^2)}{\\lambda^2+\\delta^4}\\begin{pmatrix}\\lambda \\\\ \\delta^2\\end{pmatrix} = \\frac{1}{\\lambda^2+\\delta^4} \\left[ \\begin{pmatrix} \\lambda^2+\\delta^4 \\\\ \\lambda(\\lambda^2+\\delta^4) \\end{pmatrix} - \\begin{pmatrix} \\lambda^2(1+\\delta^2) \\\\ \\lambda\\delta^2(1+\\delta^2) \\end{pmatrix} \\right] = \\frac{\\lambda^2-\\delta^2}{\\lambda^2+\\delta^4}\\begin{pmatrix}-\\delta^2 \\\\ \\lambda\\end{pmatrix}.\n    $$\n    $r_{22} = \\|u_2\\|_2 = \\left|\\frac{\\lambda^2-\\delta^2}{\\lambda^2+\\delta^4}\\right|\\sqrt{(-\\delta^2)^2+\\lambda^2} = \\frac{|\\lambda^2-\\delta^2|}{\\sqrt{\\lambda^2+\\delta^4}}$.\n    Since $0  |\\delta| \\ll \\lambda$, we have $\\lambda^2 - \\delta^2 > 0$, so $r_{22} = \\frac{\\lambda^2-\\delta^2}{\\sqrt{\\lambda^2+\\delta^4}} > 0$.\n\nNow, we compute $A_1 = RQ = \\begin{pmatrix} r_{11}  r_{12} \\\\ 0  r_{22} \\end{pmatrix} \\begin{pmatrix} q_{11}  q_{12} \\\\ q_{21}  q_{22} \\end{pmatrix}$.\nThe sub-diagonal entry is $(A_1)_{2,1} = r_{22} q_{21}$.\nHere, $q_{21}$ is the second component of the vector $q_1$. From step 1:\n$q_{21} = \\frac{\\delta^2}{\\sqrt{\\lambda^2+\\delta^4}}$.\n\nTherefore, the sub-diagonal entry of $A_1$ is:\n$$\n(A_1)_{2,1} = r_{22} q_{21} = \\frac{\\lambda^2-\\delta^2}{\\sqrt{\\lambda^2+\\delta^4}} \\cdot \\frac{\\delta^2}{\\sqrt{\\lambda^2+\\delta^4}} = \\frac{\\delta^2(\\lambda^2-\\delta^2)}{\\lambda^2+\\delta^4}.\n$$\nAs stated in the problem, $0  |\\delta| \\ll \\lambda$, ensuring $\\lambda^2 - \\delta^2 > 0$. The final expression is:\n$$\n(A_1)_{2,1} = \\frac{\\delta^2(\\lambda^2-\\delta^2)}{\\lambda^2+\\delta^4}.\n$$",
            "answer": "$$\\boxed{\\frac{\\delta^{2}\\left(\\lambda^{2}-\\delta^{2}\\right)}{\\lambda^{2}+\\delta^{4}}}$$"
        },
        {
            "introduction": "While manual calculations provide foundational insight, a true grasp of a numerical algorithm comes from its implementation and practical application. This practice involves building the unshifted QR algorithm from scratch using the numerically stable Modified Gram-Schmidt (MGS) method. By applying your implementation to a suite of test cases—including symmetric, non-symmetric, and defective matrices—you will see firsthand how the theoretical properties of the algorithm manifest in a computational setting , bridging the gap between abstract theory and practical scientific computing.",
            "id": "3237833",
            "problem": "You are to implement, analyze, and test the unshifted QR factorization algorithm for computing eigenvalues, where each QR factorization step must be performed using the Modified Gram-Schmidt (MGS) process. The implementation must be fully programmatic and produce results in a strictly specified output format. The focus is on deriving the algorithm from core definitions of linear algebra and numerical methods, demonstrating why the procedure preserves eigenvalues and how the convergence behavior emerges when using MGS for numerical stability.\n\nFundamental base for derivation:\n- The standard Euclidean inner product of vectors $u,v \\in \\mathbb{R}^n$ is $u^\\top v$ and the induced $2$-norm is $\\|v\\|_2 = \\sqrt{v^\\top v}$.\n- The Frobenius norm of a matrix $A \\in \\mathbb{R}^{n \\times n}$ is $\\|A\\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^n a_{ij}^2}$, and the strictly lower-triangular part of $A$ is the matrix with entries $\\ell_{ij} = a_{ij}$ for $i  j$ and $\\ell_{ij} = 0$ otherwise.\n- The Modified Gram-Schmidt (MGS) process is a numerically stable variant of Gram-Schmidt that orthonormalizes a set of vectors to produce an orthonormal basis and an upper-triangular coefficient matrix.\n- The QR factorization of $A \\in \\mathbb{R}^{n \\times n}$ is $A = Q R$ with $Q \\in \\mathbb{R}^{n \\times n}$ orthogonal (i.e., $Q^\\top Q = I$) and $R \\in \\mathbb{R}^{n \\times n}$ upper triangular.\n- A similarity transformation $A \\mapsto S^{-1} A S$ preserves eigenvalues. In particular, if $A = Q R$ with $Q$ orthogonal, then $R Q = Q^\\top A Q$ is similar to $A$.\n\nImplement the unshifted QR algorithm:\nGiven $A_0 = A \\in \\mathbb{R}^{n \\times n}$, iterate for $k = 0, 1, 2, \\dots$:\n- Compute the QR factorization $A_k = Q_k R_k$ using Modified Gram-Schmidt (MGS) so that $Q_k^\\top Q_k \\approx I$ and $R_k$ is upper triangular.\n- Form $A_{k+1} = R_k Q_k$.\nStop when the convergence metric $\\|\\operatorname{lower}(A_k)\\|_F \\le \\varepsilon$ or when the iteration count reaches a prescribed maximum. Here, $\\operatorname{lower}(A_k)$ denotes the strictly lower-triangular part of $A_k$, and $\\varepsilon  0$ is a given tolerance.\n\nProgram requirements:\n- Implement MGS to compute $Q$ and $R$ directly from $A$ without using any built-in QR routines.\n- Implement the unshifted QR iteration using the MGS routine, starting from the given $A$.\n- Use the convergence metric $\\|\\operatorname{lower}(A_k)\\|_F$ with a prescribed tolerance $\\varepsilon$.\n- Return, for each test case, a list containing the integer iteration count $k$ at termination (or $-1$ if not converged within the maximum iterations), the final value of $\\|\\operatorname{lower}(A_k)\\|_F$ (a float), followed by the diagonal entries of $A_k$ (each a float) at termination in the order they appear on the diagonal.\n\nTest suite:\nYou must evaluate your implementation on the following four test cases. Angles, where used, must be in radians.\n\n- Test case 1 (symmetric positive definite): Define the matrix $S_1 \\in \\mathbb{R}^{5 \\times 5}$ by\n$$\nS_1 = \\begin{bmatrix}\n2  -1  0  0.5  3 \\\\\n1  0  1.5  -2  0 \\\\\n0  2  -1  1  1 \\\\\n3  0.5  2  -1  -1 \\\\\n1  -3  0  2  0.5\n\\end{bmatrix}.\n$$\nLet $A_1 = S_1^\\top S_1$. Use tolerance $\\varepsilon_1 = 10^{-8}$ and maximum iterations $N_1 = 1000$.\n\n- Test case 2 (non-symmetric with a complex-conjugate pair in the real Schur form): Let\n$$\nB = \\begin{bmatrix}\n0  -2  0 \\\\\n2  \\phantom{-}0  0 \\\\\n0  \\phantom{-}0  3\n\\end{bmatrix}.\n$$\nDefine the $3 \\times 3$ rotation matrices about the $z$-axis and $y$-axis by\n$$\nR_z(\\theta) = \\begin{bmatrix}\n\\cos\\theta  -\\sin\\theta  0 \\\\\n\\sin\\theta  \\phantom{-}\\cos\\theta  0 \\\\\n0  0  1\n\\end{bmatrix}, \\quad\nR_y(\\phi) = \\begin{bmatrix}\n\\cos\\phi  0  \\sin\\phi \\\\\n0  1  0 \\\\\n-\\sin\\phi  0  \\cos\\phi\n\\end{bmatrix}.\n$$\nLet $\\theta = 0.3$ and $\\phi = 0.5$, define $Q = R_z(\\theta) R_y(\\phi)$, and set $A_2 = Q^\\top B Q$. Use tolerance $\\varepsilon_2 = 10^{-10}$ and maximum iterations $N_2 = 1000$.\n\n- Test case 3 (defective upper-triangular Jordan block): Let\n$$\nA_3 = \\begin{bmatrix}\n1  1 \\\\\n0  1\n\\end{bmatrix}.\n$$\nUse tolerance $\\varepsilon_3 = 10^{-12}$ and maximum iterations $N_3 = 1000$.\n\n- Test case 4 (stringent tolerance with limited iterations): Define the matrix $S_2 \\in \\mathbb{R}^{4 \\times 4}$ by\n$$\nS_2 = \\begin{bmatrix}\n1  2  -1  0.5 \\\\\n0  -1  3  2 \\\\\n2  0  1  -2 \\\\\n1  -1.5  0  1\n\\end{bmatrix}.\n$$\nLet $A_4 = S_2^\\top S_2$. Use tolerance $\\varepsilon_4 = 10^{-18}$ and maximum iterations $N_4 = 5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list formatted without spaces. Specifically, the output line must be of the form\n$[r_1,r_2,r_3,r_4]$\nwhere each $r_i$ is a bracket-enclosed comma-separated list without spaces, containing the integer iteration count $k$, the float $\\|\\operatorname{lower}(A_k)\\|_F$, followed by the float diagonal entries of $A_k$ in order. For example, a valid structure is\n$[[k_1,f_1,d_{1,1},\\dots,d_{1,n_1}],[k_2,f_2,d_{2,1},\\dots,d_{2,n_2}],[k_3,f_3,d_{3,1},\\dots,d_{3,n_3}],[k_4,f_4,d_{4,1},\\dots,d_{4,n_4}]]$.\nNo other text must be printed.",
            "solution": "The problem requires the implementation and analysis of the unshifted QR algorithm for eigenvalue computation, with a specific mandate to use the Modified Gram-Schmidt (MGS) method for the QR factorization step. A thorough understanding of the underlying principles is essential for a correct implementation.\n\n### 1. The Unshifted QR Algorithm and Eigenvalue Preservation\n\nThe fundamental iterative procedure of the unshifted QR algorithm is defined for a given square matrix $A_0 = A \\in \\mathbb{R}^{n \\times n}$ as follows:\nFor $k = 0, 1, 2, \\dots$,\n1.  Compute the QR factorization of the current matrix $A_k$: $A_k = Q_k R_k$. Here, $Q_k$ is an orthogonal matrix ($Q_k^\\top Q_k = I$) and $R_k$ is an upper-triangular matrix.\n2.  Define the next matrix in the sequence by reversing the order of multiplication: $A_{k+1} = R_k Q_k$.\n\nA crucial property of this iteration is that it constitutes a sequence of similarity transformations. Since $Q_k$ is orthogonal, its inverse is its transpose, $Q_k^{-1} = Q_k^\\top$. We can rewrite the update step as follows:\nFrom the factorization $A_k = Q_k R_k$, we can express $R_k$ as $R_k = Q_k^\\top A_k$. Substituting this into the definition of $A_{k+1}$:\n$$\nA_{k+1} = R_k Q_k = (Q_k^\\top A_k) Q_k = Q_k^\\top A_k Q_k\n$$\nThis demonstrates that $A_{k+1}$ is orthogonally similar to $A_k$. By induction, every matrix $A_k$ in the sequence is orthogonally similar to the original matrix $A_0 = A$. A similarity transformation preserves eigenvalues. Therefore, the set of eigenvalues of $A_k$ is identical to the set of eigenvalues of $A$ for all $k \\ge 0$.\n\nUnder favorable conditions (specifically, when the eigenvalues of $A$ have distinct magnitudes, $|\\lambda_1|  |\\lambda_2|  \\dots  |\\lambda_n|$), the sequence of matrices $\\{A_k\\}$ converges to an upper-triangular matrix, known as the Schur form of $A$. The diagonal entries of this limiting matrix are the eigenvalues of $A$. Even when some eigenvalues have the same magnitude, the algorithm often converges to a block upper-triangular form (the real Schur form), where diagonal blocks correspond to eigenvalues of equal magnitude or complex-conjugate pairs.\n\nThe convergence of the off-diagonal elements is monitored by computing the Frobenius norm of the strictly lower-triangular part of $A_k$, denoted as $\\operatorname{lower}(A_k)$. The iteration terminates when this norm, $\\|\\operatorname{lower}(A_k)\\|_F$, falls below a specified tolerance $\\varepsilon  0$.\n\n### 2. Modified Gram-Schmidt (MGS) for QR Factorization\n\nThe QR factorization is central to the algorithm. While several methods exist to compute it (e.g., Householder reflections, Givens rotations), the problem specifies the use of the Modified Gram-Schmidt (MGS) process.\n\nLet the matrix $A \\in \\mathbb{R}^{n \\times n}$ have columns $a_1, a_2, \\dots, a_n$. The Gram-Schmidt process aims to find an orthonormal set of vectors $q_1, q_2, \\dots, q_n$ that span the same successive subspaces as the columns of $A$. The MGS algorithm proceeds as follows:\n\nLet $V$ be a working matrix, initialized as a copy of $A$. The columns of $V$ will be progressively orthogonalized. The matrices $Q$ (with columns $q_j$) and $R$ (with entries $r_{ij}$) are constructed.\n\nFor $j = 1, \\dots, n$:\n1.  The $j$-th orthogonal vector is normalized. The norm is stored as the diagonal element of $R$:\n    $$\n    r_{jj} = \\|v_j\\|_2\n    $$\n    The $j$-th orthonormal vector is then:\n    $$\n    q_j = \\frac{v_j}{r_{jj}}\n    $$\n2.  The remaining vectors $v_k$ (for $k  j$) are made orthogonal to the newly computed $q_j$. The components of $v_k$ in the direction of $q_j$ are computed and subtracted. These components form the super-diagonal elements of $R$:\n    For $k = j+1, \\dots, n$:\n    $$\n    r_{jk} = q_j^\\top v_k \\quad (\\text{projection coefficient})\n    $$\n    $$\n    v_k \\leftarrow v_k - r_{jk} q_j \\quad (\\text{orthogonalization})\n    $$\n\nMGS is mathematically equivalent to the classical Gram-Schmidt (CGS) process in exact arithmetic. However, in floating-point arithmetic, MGS is numerically superior. The repeated subtractions in MGS reduce the loss of orthogonality among the computed $q_j$ vectors, which is a significant problem in CGS when the columns of $A$ are nearly linearly dependent. This enhanced stability is why MGS is preferred in numerical practice.\n\n### 3. The Complete Algorithm\n\nThe final algorithm combines the QR iteration with the MGS factorization and the specified convergence criteria.\n\nGiven an initial matrix $A$, a tolerance $\\varepsilon$, and a maximum number of iterations $N_{max}$:\n\n1.  Initialize $A_0 = A$.\n2.  Check for immediate convergence: Calculate the initial norm $f_0 = \\|\\operatorname{lower}(A_0)\\|_F$. If $f_0 \\le \\varepsilon$, the algorithm terminates with an iteration count of $k=0$.\n3.  Iterate for $k = 1, 2, \\dots, N_{max}$:\n    a.  Compute the QR factorization of the previous matrix, $A_{k-1} = Q_{k-1} R_{k-1}$, using the MGS algorithm.\n    b.  Form the next matrix: $A_k = R_{k-1} Q_{k-1}$.\n    c.  Calculate the convergence metric: $f_k = \\|\\operatorname{lower}(A_k)\\|_F$.\n    d.  If $f_k \\le \\varepsilon$, the algorithm has converged. Terminate and return the current iteration count $k$, the norm $f_k$, and the matrix $A_k$.\n4.  If the loop completes without the convergence criterion being met, the algorithm has failed to converge within the allowed iterations. Return a failure indicator (iteration count $k = -1$), the final norm $f_{N_{max}}$, and the final matrix $A_{N_{max}}$.\n\n### 4. Analysis of Test Cases\n\n- **Cases 1 and 4**: The matrices $A_1 = S_1^\\top S_1$ and $A_4 = S_2^\\top S_2$ are symmetric and positive definite (assuming $S_1, S_2$ are non-singular). For such matrices, all eigenvalues are real and positive. The unshifted QR algorithm is guaranteed to converge. The rate of convergence depends on the ratios of the eigenvalues. Case $4$ is designed to fail due to a stringent tolerance combined with a very low iteration limit.\n- **Case 2**: The matrix $A_2$ is similar to $B$, which has one real eigenvalue ($3$) and a complex-conjugate pair ($\\pm 2i$). For a real matrix with complex-conjugate eigenvalues, the unshifted QR iteration generally converges to a real Schur form, which is block upper-triangular. A $2 \\times 2$ block on the diagonal corresponds to the complex-conjugate pair. The convergence criterion, which checks for the strictly lower part to become zero, might not be met, as the entry below the diagonal within the $2 \\times 2$ block may not vanish. This case tests the algorithm's behavior under such conditions, likely resulting in non-convergence within the specified tolerance.\n- **Case 3**: The matrix $A_3 = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}$ is a Jordan block. It is already in upper-triangular form. The strictly lower-triangular part is zero, so $\\|\\operatorname{lower}(A_3)\\|_F = 0$. The algorithm should correctly identify this on the initial check and terminate immediately with an iteration count of $k=0$.",
            "answer": "```python\nimport numpy as np\n\ndef mgs_qr(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Modified Gram-Schmidt.\n    A = QR, where Q is orthogonal and R is upper triangular.\n    \"\"\"\n    n = A.shape[0]\n    Q = np.zeros((n, n))\n    R = np.zeros((n, n))\n    V = A.copy().astype(np.float64)\n\n    for j in range(n):\n        # Norm of the j-th column of V is the diagonal element R[j, j]\n        r_jj = np.linalg.norm(V[:, j])\n        R[j, j] = r_jj\n        \n        # If the column is non-zero, normalize it to get the j-th column of Q\n        if r_jj  1e-16: # A small tolerance to avoid division by zero\n            Q[:, j] = V[:, j] / r_jj\n        else:\n            # If a column is (close to) zero, the matrix is singular.\n            # The basis vector can be chosen arbitrarily, but must be orthogonal to others.\n            # This is an edge case not expected in the test suite. We proceed assuming non-singularity.\n            Q[:, j] = V[:, j] # Will likely be a zero vector\n\n        # Orthogonalize the remaining vectors in V against the new q_j\n        for k in range(j + 1, n):\n            # The projection of V[:, k] onto Q[:, j] gives R[j, k]\n            r_jk = Q[:, j].T @ V[:, k]\n            R[j, k] = r_jk\n            \n            # Subtract the projection from V[:, k]\n            V[:, k] = V[:, k] - r_jk * Q[:, j]\n\n    return Q, R\n\ndef unshifted_qr_eigenvalues(A, tol, max_iter):\n    \"\"\"\n    Implements the unshifted QR algorithm to find eigenvalues of a matrix A.\n    Uses Modified Gram-Schmidt for QR factorization.\n    \"\"\"\n    A_k = A.copy().astype(np.float64)\n    n = A_k.shape[0]\n\n    # Initial check at k=0\n    lower_A = np.tril(A_k, -1)\n    norm = np.linalg.norm(lower_A, 'fro')\n    if norm = tol:\n        return 0, norm, A_k\n\n    # QR iteration loop\n    for k in range(1, max_iter + 1):\n        try:\n            Q, R = mgs_qr(A_k)\n        except np.linalg.LinAlgError:\n            # This might happen if mgs_qr fails on a singular matrix\n            return -1, norm, A_k\n\n        A_k = R @ Q\n        \n        lower_A = np.tril(A_k, -1)\n        norm = np.linalg.norm(lower_A, 'fro')\n\n        if norm = tol:\n            return k, norm, A_k\n\n    # If loop completes, max_iter reached without convergence\n    return -1, norm, A_k\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the QR algorithm implementation.\n    \"\"\"\n    \n    # Test Case 1\n    S1 = np.array([\n        [2, -1, 0, 0.5, 3],\n        [1, 0, 1.5, -2, 0],\n        [0, 2, -1, 1, 1],\n        [3, 0.5, 2, -1, -1],\n        [1, -3, 0, 2, 0.5]\n    ])\n    A1 = S1.T @ S1\n    tol1 = 1e-8\n    N1 = 1000\n\n    # Test Case 2\n    theta = 0.3\n    phi = 0.5\n    B = np.array([\n        [0, -2, 0],\n        [2, 0, 0],\n        [0, 0, 3]\n    ])\n    Rz = np.array([\n        [np.cos(theta), -np.sin(theta), 0],\n        [np.sin(theta), np.cos(theta), 0],\n        [0, 0, 1]\n    ])\n    Ry = np.array([\n        [np.cos(phi), 0, np.sin(phi)],\n        [0, 1, 0],\n        [-np.sin(phi), 0, np.cos(phi)]\n    ])\n    Q_rot = Rz @ Ry\n    A2 = Q_rot.T @ B @ Q_rot\n    tol2 = 1e-10\n    N2 = 1000\n\n    # Test Case 3\n    A3 = np.array([\n        [1, 1],\n        [0, 1]\n    ])\n    tol3 = 1e-12\n    N3 = 1000\n\n    # Test Case 4\n    S2 = np.array([\n        [1, 2, -1, 0.5],\n        [0, -1, 3, 2],\n        [2, 0, 1, -2],\n        [1, -1.5, 0, 1]\n    ])\n    A4 = S2.T @ S2\n    tol4 = 1e-18\n    N4 = 5\n\n    test_cases = [\n        (A1, tol1, N1),\n        (A2, tol2, N2),\n        (A3, tol3, N3),\n        (A4, tol4, N4),\n    ]\n\n    all_results = []\n    for A, tol, max_iter in test_cases:\n        k, final_norm, final_A = unshifted_qr_eigenvalues(A, tol, max_iter)\n        \n        diag_entries = final_A.diagonal().tolist()\n        \n        result_list = [k, final_norm] + diag_entries\n        \n        # Format the result list into a string without spaces\n        result_str = f\"[{','.join(map(str, result_list))}]\"\n        all_results.append(result_str)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}