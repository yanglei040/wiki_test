{
    "hands_on_practices": [
        {
            "introduction": "The power iteration method is elegant in its simplicity, but its success hinges on a critical property of the matrix's spectrum: the existence of a unique dominant eigenvalue. This first practice invites you to explore the method's breaking points. By implementing the iteration and a metric for directional stability, you will numerically verify its convergence for a well-behaved matrix and witness its failure to stabilize for matrices with multiple eigenvalues of maximal modulus . This exercise provides a concrete understanding of the theoretical conditions that govern the algorithm's applicability.",
            "id": "3592899",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real matrix. The standard power iteration generates a sequence of vectors $\\{x_k\\}_{k \\geq 0}$ by the recurrence $x_{k+1} = A x_k$ and, in practice, one studies the normalized sequence $y_k = x_k / \\|x_k\\|_2$ for $k \\geq 0$ to analyze directional behavior. The fundamental base for this problem is the definition of eigenvalues and eigenvectors: a scalar $\\lambda \\in \\mathbb{C}$ and a nonzero vector $v \\in \\mathbb{C}^n$ are called an eigenvalue and eigenvector of $A$ if $A v = \\lambda v$. If $A$ is diagonalizable, any initial vector $x_0$ can be decomposed in the eigenbasis of $A$, and repeated multiplication by $A$ scales each eigencomponent by successive powers of the corresponding eigenvalue. The spectral radius $\\rho(A)$ is defined by $\\rho(A) = \\max_{i} |\\lambda_i|$ over all eigenvalues $\\lambda_i$ of $A$. When there is a unique eigenvalue with modulus equal to the spectral radius, the direction of $y_k$ tends to the eigenvector associated with that eigenvalue for generic $x_0$. However, if there are two or more eigenvalues with equal modulus equal to the spectral radius, different eigencomponents can persist in magnitude and the direction of $y_k$ may fail to stabilize.\n\nIn this problem, you must implement the standard power iteration with normalization and a quantitative test for directional stabilization. Define the stabilization metric as follows. Given the normalized sequence $y_k = x_k / \\|x_k\\|_2$, for $k \\geq 0$, define\n$$\nd_k = \\min\\left(\\|y_k - y_{k-1}\\|_2,\\;\\|y_k + y_{k-1}\\|_2\\right) \\quad \\text{for } k \\geq 1,\n$$\nwhich measures the smallest change in direction between successive normalized iterates, accounting for the global sign ambiguity that is irrelevant for eigenvector directions. For a chosen burn-in window length $L$, define the stability residual\n$$\n\\rho = \\max\\{d_k : k = K-L+1, K-L+2, \\dots, K\\},\n$$\nwhere $K$ is the total number of iterations performed. A sequence is deemed stabilized if $\\rho \\leq \\tau$ for a given tolerance $\\tau  0$.\n\nYour task is to write a complete program that:\n- Implements the normalized power iteration for a given matrix $A$, initial vector $x_0 \\neq 0$, and iteration count $K$, and computes the stabilization residual $\\rho$ over the last $L$ iterates using the metric above.\n- Returns a Boolean value indicating whether stabilization occurred within tolerance $\\tau$.\n\nTest suite and parameters:\n1. Happy path with a unique dominant eigenvalue:\n   - Matrix $A_1 = \\operatorname{diag}(2.5, 1.2, 0.8)$.\n   - Initial vector $x_0 = (1, 1, 1)^\\top$.\n   - Iterations $K = 600$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-8}$.\n   This case should stabilize.\n\n2. Two distinct eigenvalues of equal modulus (rotation; complex-conjugate pair on the unit circle):\n   - Matrix $A_2 = R(\\theta)$ with\n     $$\n     R(\\theta) = \\begin{bmatrix}\n     \\cos \\theta  -\\sin \\theta \\\\\n     \\sin \\theta  \\cos \\theta\n     \\end{bmatrix}, \\quad \\theta = 0.3 \\text{ radians}.\n     $$\n   - Initial vector $x_0 = (1, 0)^\\top$.\n   - Iterations $K = 400$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should fail to stabilize.\n\n3. Two distinct real eigenvalues of equal modulus with sign alternation:\n   - Matrix $A_3 = \\begin{bmatrix}0  1 \\\\ 1  0\\end{bmatrix}$.\n   - Initial vector $x_0 = (1, 2)^\\top$.\n   - Iterations $K = 400$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should fail to stabilize.\n\n4. Diagonal matrix with dominant eigenvalues of equal modulus but opposite sign:\n   - Matrix $A_4 = \\operatorname{diag}(2, -2)$.\n   - Initial vector $x_0 = (1, 1)^\\top$.\n   - Iterations $K = 400$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should fail to stabilize.\n\n5. Near-tie eigenvalues (slow convergence but unique modulus maximum):\n   - Matrix $A_5 = \\operatorname{diag}(1.01, 1.00, 0.99)$.\n   - Initial vector $x_0 = (1, 1, 1)^\\top$.\n   - Iterations $K = 2000$.\n   - Burn-in length $L = 200$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should stabilize.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1, result_2, result_3, result_4, result_5]$), where each $result_i$ is a Boolean indicating stabilization for the corresponding test case.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the power iteration method for finding dominant eigenvalues. The problem is well-posed, providing all necessary matrices, initial vectors, and parameters ($A, x_0, K, L, \\tau$) for each test case. The definitions for the stabilization metric $d_k$ and stability residual $\\rho$ are mathematically precise and objective, allowing for a unique, verifiable solution. The test cases are thoughtfully designed to probe different convergence behaviors of the power iteration method, from simple convergence to various modes of non-convergence.\n\nThe power iteration method is an algorithm to find the eigenvalue with the largest magnitude (the dominant eigenvalue) and its corresponding eigenvector for a given matrix $A \\in \\mathbb{R}^{n \\times n}$. The core idea relies on the iterative application of the matrix to an initial vector $x_0$. If $A$ is diagonalizable with eigenvalues $\\lambda_1, \\dots, \\lambda_n$ and corresponding eigenvectors $v_1, \\dots, v_n$, such that there is a unique dominant eigenvalue $|\\lambda_1|  |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$, then any initial vector $x_0$ with a non-zero component along $v_1$ can be expressed as $x_0 = \\sum_{i=1}^n c_i v_i$ where $c_1 \\neq 0$.\n\nThe $k$-th iterate is given by $x_k = A^k x_0$. This can be expanded in the eigenbasis:\n$$\nx_k = A^k \\sum_{i=1}^n c_i v_i = \\sum_{i=1}^n c_i A^k v_i = \\sum_{i=1}^n c_i \\lambda_i^k v_i\n$$\nFactoring out the dominant eigenvalue term $\\lambda_1^k$ yields:\n$$\nx_k = \\lambda_1^k \\left( c_1 v_1 + \\sum_{i=2}^n c_i \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k v_i \\right)\n$$\nAs $k \\to \\infty$, the terms $(\\lambda_i/\\lambda_1)^k$ for $i \\ge 2$ approach zero because $|\\lambda_i/\\lambda_1|  1$. Consequently, the vector $x_k$ becomes increasingly aligned with the dominant eigenvector $v_1$, i.e., $x_k \\approx c_1 \\lambda_1^k v_1$.\n\nTo prevent the magnitude of $x_k$ from diverging or vanishing, we study the sequence of normalized vectors $y_k = x_k / \\|x_k\\|_2$. For a unique dominant eigenvalue, this sequence $\\{y_k\\}$ converges to a unit vector in the direction of $v_1$.\n\nThe algorithm's convergence fails if there is no unique dominant eigenvalue. If multiple distinct eigenvalues share the same maximal modulus, say $|\\lambda_1| = |\\lambda_2| = \\dots = |\\lambda_m|  |\\lambda_{m+1}|$, then the components corresponding to all these eigenvectors ($v_1, \\dots, v_m$) will persist in the iteration, and the direction of $y_k$ will generally not stabilize.\n\nThis problem requires implementing a quantitative test for this directional stabilization. The proposed metric is for $k \\ge 1$:\n$$\nd_k = \\min\\left(\\|y_k - y_{k-1}\\|_2,\\;\\|y_k + y_{k-1}\\|_2\\right)\n$$\nThis metric shrewdly accounts for the fact that an eigenvector's direction is defined up to a sign. If the sequence $y_k$ converges to a fixed direction $u$, then $\\|y_k - y_{k-1}\\|_2 \\to 0$. If the sequence alternates in sign, e.g., $y_k \\approx (-1)^k u$, then $\\|y_k + y_{k-1}\\|_2 \\to 0$. In either case of directional convergence, $d_k \\to 0$.\n\nThe stability residual $\\rho$ is defined as the maximum value of $d_k$ over a final \"burn-in\" window of $L$ iterations:\n$$\n\\rho = \\max\\{d_k : k = K-L+1, \\dots, K\\}\n$$\nA sequence is deemed stabilized if this maximum directional change over the window is below a specified tolerance $\\tau$, i.e., $\\rho \\le \\tau$.\n\nThe implementation will consist of a function that takes $A, x_0, K, L$, and $\\tau$ as inputs. It will perform the following steps:\n1. Initialize the vector $x$ with the initial vector $x_0$.\n2. Normalize $x$ to get the first vector in the sequence, $y_0$. We will designate this as `y_prev`.\n3. Iterate from $k=1$ to $K$. In each iteration:\n   a. Update the vector: $x \\leftarrow A x$.\n   b. Normalize the new vector: $y_{\\text{curr}} \\leftarrow x / \\|x\\|_2$.\n   c. If the iteration count $k$ is within the final window ($k \\ge K - L + 1$), calculate $d_k = \\min(\\|y_{\\text{curr}} - y_{\\text{prev}}\\|_2, \\|y_{\\text{curr}} + y_{\\text{prev}}\\|_2)$ and store it.\n   d. Update for the next iteration: $y_{\\text{prev}} \\leftarrow y_{\\text{curr}}$.\n4. After the loop completes, find the maximum value, $\\rho$, among the stored $d_k$ values.\n5. Return the boolean result of the comparison $\\rho \\le \\tau$.\n\nThis procedure will be applied to each of the five test cases provided, and the boolean results will be collected and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration_stability_check(A: np.ndarray, x0: np.ndarray, K: int, L: int, tau: float) - bool:\n    \"\"\"\n    Performs normalized power iteration and checks for directional stabilization.\n\n    Args:\n        A (np.ndarray): The matrix for the iteration.\n        x0 (np.ndarray): The initial vector.\n        K (int): The total number of iterations.\n        L (int): The length of the burn-in window for the stability check.\n        tau (float): The tolerance for stabilization.\n\n    Returns:\n        bool: True if the sequence stabilized within the tolerance, False otherwise.\n    \"\"\"\n    if L  K or L = 0:\n        raise ValueError(\"Burn-in length L must be positive and not greater than K.\")\n        \n    x = x0.astype(np.float64)\n\n    # Ensure the initial vector is not a zero vector\n    norm_x = np.linalg.norm(x)\n    if norm_x == 0:\n        return False  # Or raise an error for invalid input\n\n    y_prev = x / norm_x\n    \n    d_values = []\n\n    for k in range(1, K + 1):\n        x = A @ x\n        norm_x = np.linalg.norm(x)\n\n        if norm_x == 0:\n            # Iteration produced a zero vector.\n            # This happens if A is singular and x lies in a subspace mapped to the null space.\n            # The direction is undefined, hence not stabilized.\n            return False\n\n        y_curr = x / norm_x\n\n        if k = K - L + 1:\n            dist_minus = np.linalg.norm(y_curr - y_prev)\n            dist_plus = np.linalg.norm(y_curr + y_prev)\n            d_k = min(dist_minus, dist_plus)\n            d_values.append(d_k)\n        \n        y_prev = y_curr\n\n    if not d_values:\n        # This case is avoided by the initial L  0 check, but as a safeguard:\n        # Not enough iterations to compute stability residual.\n        return False\n\n    rho = max(d_values)\n    \n    return rho = tau\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement, runs the stability check,\n    and prints the results in the required format.\n    \"\"\"\n    # Test Case 1: Happy path with a unique dominant eigenvalue\n    A1 = np.diag([2.5, 1.2, 0.8])\n    x0_1 = np.array([1.0, 1.0, 1.0])\n    params1 = (A1, x0_1, 600, 100, 1e-8)\n\n    # Test Case 2: Rotation matrix (complex-conjugate eigenvalue pair)\n    theta = 0.3\n    c, s = np.cos(theta), np.sin(theta)\n    A2 = np.array([[c, -s], [s, c]])\n    x0_2 = np.array([1.0, 0.0])\n    params2 = (A2, x0_2, 400, 100, 1e-6)\n\n    # Test Case 3: Two distinct real eigenvalues of equal modulus (1 and -1)\n    A3 = np.array([[0.0, 1.0], [1.0, 0.0]])\n    x0_3 = np.array([1.0, 2.0])\n    params3 = (A3, x0_3, 400, 100, 1e-6)\n\n    # Test Case 4: Diagonal matrix with dominant eigenvalues of equal modulus (2 and -2)\n    A4 = np.diag([2.0, -2.0])\n    x0_4 = np.array([1.0, 1.0])\n    params4 = (A4, x0_4, 400, 100, 1e-6)\n\n    # Test Case 5: Near-tie eigenvalues (slow convergence)\n    A5 = np.diag([1.01, 1.00, 0.99])\n    x0_5 = np.array([1.0, 1.0, 1.0])\n    params5 = (A5, x0_5, 2000, 200, 1e-6)\n\n    test_cases = [params1, params2, params3, params4, params5]\n\n    results = []\n    for params in test_cases:\n        A, x0, K, L, tau = params\n        result = power_iteration_stability_check(A, x0, K, L, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In numerical computations, a small residual is often used as a sign of success, indicating that our approximate solution nearly satisfies the defining equation. However, for non-normal matrices, this intuition can be dangerously misleading. This practice presents a classic example of a defective matrix to demonstrate that a candidate vector can have a very small residual norm while being almost orthogonal to the true eigenvector . This calculation will sharpen your critical assessment of numerical results and highlight the important distinction between the residual and the true error.",
            "id": "3592908",
            "problem": "Let $A(\\varepsilon) \\in \\mathbb{R}^{3 \\times 3}$ be the upper-triangular matrix\n$$\nA(\\varepsilon) \\;=\\; \\begin{pmatrix}\n1  \\varepsilon  0 \\\\\n0  1  0 \\\\\n0  0  \\alpha\n\\end{pmatrix},\n$$\nwhere $0  \\alpha  1$ and $0  \\varepsilon \\ll 1$. Consider the dominant eigenvalue $\\lambda_{1} = 1$, whose eigenvector is $v_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$. This matrix is highly non-normal because it has a nontrivial Jordan block of size two at $\\lambda_{1}$.\n\nStarting from the initial vector $x^{(0)} = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}$, define the Rayleigh quotient $\\mu(x)$ by\n$$\n\\mu(x) \\;=\\; \\frac{x^{\\top} A x}{x^{\\top} x},\n$$\nand the residual vector by $r(x) = A x - \\mu(x)\\, x$. Compute the following two quantities for $x^{(0)}$:\n1. The residual norm $\\|r(x^{(0)})\\|_{2}$.\n2. The angle $\\theta$ between $x^{(0)}$ and the eigenvector $v_{1}$, expressed in radians.\n\nYour final result must be a single composite answer consisting of the residual norm and the angle, in that order, written as a row matrix. No rounding is required; express the angle exactly in radians.",
            "solution": "The user has requested the solution to a problem in numerical linear algebra. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n-   The matrix is $A(\\varepsilon) = \\begin{pmatrix} 1  \\varepsilon  0 \\\\ 0  1  0 \\\\ 0  0  \\alpha \\end{pmatrix}$, with $A \\in \\mathbb{R}^{3 \\times 3}$.\n-   The parameters satisfy $0  \\alpha  1$ and $0  \\varepsilon \\ll 1$.\n-   The dominant eigenvalue is $\\lambda_{1} = 1$.\n-   The corresponding eigenvector is $v_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n-   The initial vector is $x^{(0)} = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}$.\n-   The Rayleigh quotient is defined as $\\mu(x) = \\frac{x^{\\top} A x}{x^{\\top} x}$.\n-   The residual vector is defined as $r(x) = A x - \\mu(x)\\, x$.\n-   The tasks are to compute the residual norm $\\|r(x^{(0)})\\|_{2}$ and the angle $\\theta$ between $x^{(0)}$ and $v_{1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in numerical linear algebra, involving standard definitions of eigenvalues, eigenvectors, Rayleigh quotients, and vector norms. The matrix provided is a classic example of a defective matrix used to study the behavior of eigenvalue algorithms. All definitions are standard, and the required data is provided. The problem is well-posed, as the quantities to be computed are uniquely determined by the givens. The language is objective and precise. The problem is therefore valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\nThe problem requires the computation of two quantities: the residual norm $\\|r(x^{(0)})\\|_{2}$ and the angle $\\theta$ between $x^{(0)}$ and the eigenvector $v_{1}$.\n\nFirst, we compute the residual norm associated with the vector $x^{(0)}$. The residual vector is defined as $r(x) = A x - \\mu(x) x$, where $\\mu(x)$ is the Rayleigh quotient. We are given the vector $x^{(0)} = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}$ and the matrix $A = \\begin{pmatrix} 1  \\varepsilon  0 \\\\ 0  1  0 \\\\ 0  0  \\alpha \\end{pmatrix}$.\n\nWe begin by calculating the Rayleigh quotient $\\mu(x^{(0)}) = \\frac{(x^{(0)})^{\\top} A x^{(0)}}{(x^{(0)})^{\\top} x^{(0)}}$.\nThe denominator is the squared Euclidean norm of $x^{(0)}$:\n$$\n(x^{(0)})^{\\top} x^{(0)} = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = (0)(0) + (1)(1) + (0)(0) = 1.\n$$\nTo compute the numerator, we first find the product $A x^{(0)}$:\n$$\nA x^{(0)} = \\begin{pmatrix} 1  \\varepsilon  0 \\\\ 0  1  0 \\\\ 0  0  \\alpha \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (\\varepsilon)(1) + (0)(0) \\\\ (0)(0) + (1)(1) + (0)(0) \\\\ (0)(0) + (0)(1) + (\\alpha)(0) \\end{pmatrix} = \\begin{pmatrix} \\varepsilon \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nNow, we can compute the numerator $(x^{(0)})^{\\top} (A x^{(0)})$:\n$$\n(x^{(0)})^{\\top} A x^{(0)} = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} \\varepsilon \\\\ 1 \\\\ 0 \\end{pmatrix} = (0)(\\varepsilon) + (1)(1) + (0)(0) = 1.\n$$\nThe Rayleigh quotient is therefore:\n$$\n\\mu(x^{(0)}) = \\frac{1}{1} = 1.\n$$\nWith the Rayleigh quotient, we can now find the residual vector $r(x^{(0)})$:\n$$\nr(x^{(0)}) = A x^{(0)} - \\mu(x^{(0)}) x^{(0)} = \\begin{pmatrix} \\varepsilon \\\\ 1 \\\\ 0 \\end{pmatrix} - (1) \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\varepsilon - 0 \\\\ 1 - 1 \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} \\varepsilon \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe first quantity to be computed is the Euclidean norm (or $L_2$-norm) of this residual vector:\n$$\n\\|r(x^{(0)})\\|_{2} = \\left\\| \\begin{pmatrix} \\varepsilon \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{\\varepsilon^2 + 0^2 + 0^2} = \\sqrt{\\varepsilon^2} = |\\varepsilon|.\n$$\nSince the problem states that $0  \\varepsilon \\ll 1$, $\\varepsilon$ is a positive value, so $|\\varepsilon| = \\varepsilon$. Thus, the first result is $\\|r(x^{(0)})\\|_{2} = \\varepsilon$.\n\nSecond, we compute the angle $\\theta$ between the vector $x^{(0)} = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}$ and the eigenvector $v_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe angle $\\theta$ between two non-zero vectors $u$ and $v$ is given by the formula:\n$$\n\\theta = \\arccos\\left(\\frac{u^{\\top} v}{\\|u\\|_{2} \\|v\\|_{2}}\\right).\n$$\nHere, $u = x^{(0)}$ and $v = v_{1}$. We compute the components of this formula.\nThe norms of the vectors are:\n$$\n\\|x^{(0)}\\|_{2} = \\left\\| \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{0^2 + 1^2 + 0^2} = 1.\n$$\n$$\n\\|v_{1}\\|_{2} = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{1^2 + 0^2 + 0^2} = 1.\n$$\nThe dot product of the vectors is:\n$$\n(x^{(0)})^{\\top} v_{1} = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = (0)(1) + (1)(0) + (0)(0) = 0.\n$$\nSubstituting these values into the angle formula gives:\n$$\n\\cos(\\theta) = \\frac{0}{(1)(1)} = 0.\n$$\nThe angle $\\theta$ in the range $[0, \\pi]$ for which $\\cos(\\theta) = 0$ is $\\theta = \\frac{\\pi}{2}$ radians. This indicates that the vectors $x^{(0)}$ and $v_1$ are orthogonal.\n\nThe two requested quantities are the residual norm, $\\varepsilon$, and the angle, $\\frac{\\pi}{2}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\varepsilon  \\frac{\\pi}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having explored the power iteration's theoretical foundations and numerical subtleties, we now apply it to a cornerstone of stochastic modeling: finding the stationary distribution of a Markov chain. This task corresponds to finding the dominant eigenvector of the transposed transition matrix. In this comprehensive exercise, you will implement the power iteration with appropriate diagnostics, calculate the true solution for comparison, and analyze the algorithm's behavior on different types of chains, including one that converges smoothly, one that fails to converge due to periodicity, and one that converges very slowly . This practice solidifies your understanding by connecting the algorithm to a tangible and important application.",
            "id": "3592911",
            "problem": "You are to implement the power iteration for the dominant eigenpair of a small Markov chain and verify convergence to the stationary distribution using residual and Rayleigh quotient diagnostics. Work entirely in purely mathematical terms. A Markov chain is represented by a row-stochastic matrix $P \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries and each row summing to $1$. A stationary distribution is a vector $\\pi \\in \\mathbb{R}^n$ with nonnegative entries summing to $1$ that satisfies $\\pi^\\top P = \\pi^\\top$. The power iteration for the dominant eigenpair of $P^\\top$ applies repeated matrix-vector multiplication with $P^\\top$ to a probability vector, followed by normalization to the $\\ell_1$-norm to retain a distribution. For a Markov chain with a unique stationary distribution and spectral gap, the iteration converges to the stationary distribution. The residual and Rayleigh quotient are defined as follows for a candidate eigenvector $v \\in \\mathbb{R}^n$: the residual is $\\|P^\\top v - v\\|_1$, and the Rayleigh quotient is $(v^\\top P^\\top v)/(v^\\top v)$. For an exact eigenvector associated with the eigenvalue $1$, the residual is $0$ and the Rayleigh quotient equals $1$.\n\nImplement a program that, for each given test case, performs the following steps:\n- Given a row-stochastic matrix $P \\in \\mathbb{R}^{n \\times n}$ and an initial vector $v_0 \\in \\mathbb{R}^n$ with nonnegative entries summing to $1$, run the power iteration $v_{k+1} \\propto P^\\top v_k$ with normalization to the $\\ell_1$-norm. Continue until the residual $\\|P^\\top v_k - v_k\\|_1$ is less than or equal to a given tolerance or until a maximum number of iterations is reached. Use the $\\ell_1$-norm for both normalization of the iterate and residual calculation.\n- Compute a stationary distribution $\\pi^\\star$ by solving the linear system characterized by $P^\\top \\pi^\\star = \\pi^\\star$ and $\\mathbf{1}^\\top \\pi^\\star = 1$, where $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of all ones.\n- After the iteration terminates (either by reaching tolerance or by exhausting the maximum iteration count), compute three diagnostics:\n  1. The $\\ell_1$-distance to stationarity: $\\|\\hat{v} - \\pi^\\star\\|_1$, where $\\hat{v}$ is the final iterate.\n  2. The final residual: $\\|P^\\top \\hat{v} - \\hat{v}\\|_1$.\n  3. The absolute Rayleigh quotient error: $\\left|\\frac{\\hat{v}^\\top P^\\top \\hat{v}}{\\hat{v}^\\top \\hat{v}} - 1\\right|$.\n\nUse the following test suite of three cases. In each case, $P$ is given explicitly and $v_0$ is specified. All numbers below are exact and unitless.\n\n- Case $1$ (ergodic and aperiodic, $n=3$):\n  - $P_1 = \\begin{bmatrix}\n  0.5  0.5  0.0 \\\\\n  0.2  0.3  0.5 \\\\\n  0.3  0.2  0.5\n  \\end{bmatrix}$,\n  - $v_0^{(1)} = \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3} \\end{bmatrix}$,\n  - tolerance $= 10^{-12}$,\n  - maximum iterations $= 10000$.\n\n- Case $2$ (irreducible but periodic of period $2$, $n=2$):\n  - $P_2 = \\begin{bmatrix}\n  0  1 \\\\\n  1  0\n  \\end{bmatrix}$,\n  - $v_0^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n  - tolerance $= 10^{-12}$,\n  - maximum iterations $= 100$.\n\n- Case $3$ (slowly mixing but aperiodic, $n=4$):\n  - $P_3 = \\begin{bmatrix}\n  0.98  0.019  0.001  0.0 \\\\\n  0.019  0.98  0.0  0.001 \\\\\n  0.001  0.0  0.98  0.019 \\\\\n  0.0  0.001  0.019  0.98\n  \\end{bmatrix}$,\n  - $v_0^{(3)} = \\begin{bmatrix} \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\end{bmatrix}$,\n  - tolerance $= 10^{-12}$,\n  - maximum iterations $= 50000$.\n\nYour program must compute, for each case $i \\in \\{1,2,3\\}$, the triple of diagnostics $(\\|\\hat{v}^{(i)} - \\pi^{\\star (i)}\\|_1,\\; \\|P_i^\\top \\hat{v}^{(i)} - \\hat{v}^{(i)}\\|_1,\\; \\left|\\frac{\\hat{v}^{(i)\\top} P_i^\\top \\hat{v}^{(i)}}{\\hat{v}^{(i)\\top} \\hat{v}^{(i)}} - 1\\right|)$. Aggregate the results for all three cases into a single line of output as a comma-separated list enclosed in square brackets in the order $[\\text{case }1\\text{ distance}, \\text{case }1\\text{ residual}, \\text{case }1\\text{ Rayleigh error}, \\text{case }2\\text{ distance}, \\text{case }2\\text{ residual}, \\text{case }2\\text{ Rayleigh error}, \\text{case }3\\text{ distance}, \\text{case }3\\text{ residual}, \\text{case }3\\text{ Rayleigh error}]$.\n\nYour program must be fully self-contained, must not read any input, and must print exactly one line in the specified format. No physical units or angle units are involved, so none should be reported. All numerical outputs must be real numbers.",
            "solution": "The problem requires the implementation of the power iteration method to find the dominant eigenpair of the transpose of a given row-stochastic matrix $P$. For a suitable Markov chain, this corresponds to finding its unique stationary distribution. The process involves iterative application of the matrix, numerical checks for convergence, and final evaluation of diagnostic metrics.\n\nThe stationary distribution of a Markov chain with transition matrix $P \\in \\mathbb{R}^{n \\times n}$ is a probability vector $\\pi \\in \\mathbb{R}^n$ (represented here as a column vector) that is a fixed point of the transposed transition operator, satisfying the equation:\n$$\nP^\\top \\pi = \\pi\n$$\nThis indicates that $\\pi$ is a right-hand eigenvector of the matrix $P^\\top$ corresponding to the eigenvalue $\\lambda = 1$. The Perron-Frobenius theorem guarantees that for a regular Markov chain (irreducible and aperiodic), this eigenvalue is unique in its maximum magnitude, and the corresponding eigenvector (the stationary distribution) is unique and has strictly positive components.\n\n### Method and Implementation Details\n\n#### 1. Power Iteration Algorithm\nThe power iteration method is employed to find the eigenvector associated with the dominant eigenvalue. For a matrix $A$ and an initial vector $v_0$, the iteration is defined by:\n$$\nv_{k+1} = \\frac{A v_k}{\\|A v_k\\|}\n$$\nIn our context, the matrix is $A = P^\\top$, and the problem specifies using the $\\ell_1$-norm for normalization to ensure that each iterate remains a probability vector. The iteration is thus:\n$$\nv_{k+1} = \\frac{P^\\top v_k}{\\|P^\\top v_k\\|_1}\n$$\nThe process starts with an initial probability vector $v_0$ and continues until one of two conditions is met:\n1.  **Convergence:** The iteration stops when the iterate $v_k$ is sufficiently close to being an eigenvector. This is measured by the $\\ell_1$-norm of the residual, defined as $r_k = \\|P^\\top v_k - v_k\\|_1$. The loop terminates if $r_k \\le \\tau$, where $\\tau$ is a given tolerance.\n2.  **Maximum Iterations:** A hard limit on the number of iterations, $k_{\\text{max}}$, is imposed to prevent an infinite loop, particularly in cases where the method does not converge (e.g., for periodic Markov chains).\n\nThe final vector produced by this iterative process is denoted as $\\hat{v}$.\n\n#### 2. Exact Stationary Distribution Calculation\nTo assess the accuracy of the power iteration, a reference stationary distribution, $\\pi^\\star$, must be computed with high precision. This is achieved by directly solving the system of linear equations that define $\\pi^\\star$:\n$$\n\\begin{cases}\nP^\\top \\pi^\\star = \\pi^\\star \\\\\n\\mathbf{1}^\\top \\pi^\\star = 1\n\\end{cases}\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^n$ is a column vector of ones. The first equation can be rewritten as $(P^\\top - I) \\pi^\\star = \\mathbf{0}$, where $I$ is the identity matrix. This is a homogeneous system, and since $\\lambda = 1$ is an eigenvalue, the matrix $(P^\\top - I)$ is singular. To obtain a unique solution, one of the linearly dependent equations in this system is replaced by the normalization constraint $\\mathbf{1}^\\top \\pi^\\star = 1$. We construct a new matrix $A'$ by taking the first $n-1$ rows of $(P^\\top - I)$ and setting the last row to be $\\mathbf{1}^\\top$. We also construct a new right-hand-side vector $b' = [0, 0, \\dots, 0, 1]^\\top$. The resulting non-homogeneous system $A' \\pi^\\star = b'$ is well-posed for an irreducible chain and can be solved using a standard linear solver.\n\n#### 3. Diagnostic Metrics\nOnce the power iteration terminates, yielding the final iterate $\\hat{v}$, and the reference distribution $\\pi^\\star$ is calculated, three diagnostic metrics are computed to evaluate the performance and accuracy of the iteration:\n\n1.  **$\\ell_1$-Distance to Stationarity:** This metric measures the error in the computed eigenvector. It is the $\\ell_1$-norm of the difference between the final iterate and the true stationary distribution:\n    $$\n    d = \\|\\hat{v} - \\pi^\\star\\|_1\n    $$\n\n2.  **Final Residual:** This metric quantifies how well the final iterate $\\hat{v}$ satisfies the eigenvector equation. It is the $\\ell_1$-norm of the residual vector for $\\hat{v}$:\n    $$\n    r = \\|P^\\top \\hat{v} - \\hat{v}\\|_1\n    $$\n    For a perfect eigenvector, this value would be $0$.\n\n3.  **Absolute Rayleigh Quotient Error:** The Rayleigh quotient provides an estimate of the eigenvalue corresponding to an approximate eigenvector. For a candidate eigenvector $v$, it is defined as $R(v) = \\frac{v^\\top P^\\top v}{v^\\top v}$. Since we are seeking the eigenvector for $\\lambda = 1$, the error is the absolute difference between the Rayleigh quotient of the final iterate $\\hat{v}$ and $1$:\n    $$\n    \\epsilon_{RQ} = \\left| \\frac{\\hat{v}^\\top P^\\top \\hat{v}}{\\hat{v}^\\top \\hat{v}} - 1 \\right|\n    $$\n    This metric also approaches $0$ as $\\hat{v}$ converges to the true eigenvector. The denominator $\\hat{v}^\\top \\hat{v}$ is the squared $\\ell_2$-norm of $\\hat{v}$.\n\nThese steps are applied to each of the three test cases, which are designed to probe the behavior of the power iteration under different conditions: a standard ergodic case, a periodic case where convergence fails, and a case where the initial guess is already the solution.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    def solve_case(P, v0, tol, max_iter):\n        \"\"\"\n        Performs power iteration and calculates diagnostics for a single case.\n\n        Args:\n            P (np.ndarray): The row-stochastic transition matrix.\n            v0 (np.ndarray): The initial probability vector.\n            tol (float): The convergence tolerance for the residual.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            tuple: A tuple containing the three diagnostics:\n                   (distance_to_stationary, final_residual, rayleigh_quotient_error)\n        \"\"\"\n        n = P.shape[0]\n\n        # 1. Compute the reference stationary distribution (pi_star)\n        A = P.T - np.identity(n)\n        A[-1, :] = 1.0  # Replace the last row with the normalization constraint\n        b = np.zeros(n)\n        b[-1] = 1.0     # Set the RHS for the normalization constraint\n        pi_star = np.linalg.solve(A, b)\n\n        # 2. Perform power iteration\n        v = v0.copy()\n        for _ in range(max_iter):\n            # Check for convergence before updating the vector\n            residual = np.linalg.norm(P.T @ v - v, ord=1)\n            if residual = tol:\n                break\n            \n            # Update step of the power iteration\n            v_unnorm = P.T @ v\n            v = v_unnorm / np.linalg.norm(v_unnorm, ord=1)\n        \n        hat_v = v\n\n        # 3. Compute the three diagnostics\n        # Diagnostic 1: L1-distance to stationarity\n        dist_to_stat = np.linalg.norm(hat_v - pi_star, ord=1)\n\n        # Diagnostic 2: Final residual\n        final_residual = np.linalg.norm(P.T @ hat_v - hat_v, ord=1)\n\n        # Diagnostic 3: Absolute Rayleigh quotient error\n        numerator = hat_v.T @ P.T @ hat_v\n        denominator = hat_v.T @ hat_v\n        rayleigh_quotient = numerator / denominator\n        rq_error = np.abs(rayleigh_quotient - 1.0)\n        \n        return (dist_to_stat, final_residual, rq_error)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"P\": np.array([\n                [0.5, 0.5, 0.0],\n                [0.2, 0.3, 0.5],\n                [0.3, 0.2, 0.5]\n            ]),\n            \"v0\": np.array([1/3, 1/3, 1/3]),\n            \"tol\": 1e-12,\n            \"max_iter\": 10000\n        },\n        {\n            \"P\": np.array([\n                [0.0, 1.0],\n                [1.0, 0.0]\n            ]),\n            \"v0\": np.array([1.0, 0.0]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        },\n        {\n            \"P\": np.array([\n                [0.98, 0.019, 0.001, 0.0],\n                [0.019, 0.98, 0.0, 0.001],\n                [0.001, 0.0, 0.98, 0.019],\n                [0.0, 0.001, 0.019, 0.98]\n            ]),\n            \"v0\": np.array([1/4, 1/4, 1/4, 1/4]),\n            \"tol\": 1e-12,\n            \"max_iter\": 50000\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        diagnostics = solve_case(case[\"P\"], case[\"v0\"], case[\"tol\"], case[\"max_iter\"])\n        all_results.extend(diagnostics)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}