{
    "hands_on_practices": [
        {
            "introduction": "The power iteration method is a cornerstone of numerical linear algebra, prized for its simplicity and utility. This practice grounds the abstract algorithm in a concrete application: finding the stationary distribution of a Markov chain, which describes the long-term equilibrium state of a dynamic system . By implementing the iterative process and comparing your results against a directly computed solution, you will gain hands-on experience with the method's mechanics and learn to use diagnostic tools like the residual and Rayleigh quotient to assess convergence and accuracy.",
            "id": "3592911",
            "problem": "You are to implement the power iteration for the dominant eigenpair of a small Markov chain and verify convergence to the stationary distribution using residual and Rayleigh quotient diagnostics. Work entirely in purely mathematical terms. A Markov chain is represented by a row-stochastic matrix $P \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries and each row summing to $1$. A stationary distribution is a vector $\\pi \\in \\mathbb{R}^n$ with nonnegative entries summing to $1$ that satisfies $\\pi^\\top P = \\pi^\\top$. The power iteration for the dominant eigenpair of $P^\\top$ applies repeated matrix-vector multiplication with $P^\\top$ to a probability vector, followed by normalization to the $\\ell_1$-norm to retain a distribution. For a Markov chain with a unique stationary distribution and spectral gap, the iteration converges to the stationary distribution. The residual and Rayleigh quotient are defined as follows for a candidate eigenvector $v \\in \\mathbb{R}^n$: the residual is $\\|P^\\top v - v\\|_1$, and the Rayleigh quotient is $(v^\\top P^\\top v)/(v^\\top v)$. For an exact eigenvector associated with the eigenvalue $1$, the residual is $0$ and the Rayleigh quotient equals $1$.\n\nImplement a program that, for each given test case, performs the following steps:\n- Given a row-stochastic matrix $P \\in \\mathbb{R}^{n \\times n}$ and an initial vector $v_0 \\in \\mathbb{R}^n$ with nonnegative entries summing to $1$, run the power iteration $v_{k+1} \\propto P^\\top v_k$ with normalization to the $\\ell_1$-norm. Continue until the residual $\\|P^\\top v_k - v_k\\|_1$ is less than or equal to a given tolerance or until a maximum number of iterations is reached. Use the $\\ell_1$-norm for both normalization of the iterate and residual calculation.\n- Compute a stationary distribution $\\pi^\\star$ by solving the linear system characterized by $P^\\top \\pi^\\star = \\pi^\\star$ and $\\mathbf{1}^\\top \\pi^\\star = 1$, where $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of all ones.\n- After the iteration terminates (either by reaching tolerance or by exhausting the maximum iteration count), compute three diagnostics:\n  1. The $\\ell_1$-distance to stationarity: $\\|\\hat{v} - \\pi^\\star\\|_1$, where $\\hat{v}$ is the final iterate.\n  2. The final residual: $\\|P^\\top \\hat{v} - \\hat{v}\\|_1$.\n  3. The absolute Rayleigh quotient error: $\\left|\\frac{\\hat{v}^\\top P^\\top \\hat{v}}{\\hat{v}^\\top \\hat{v}} - 1\\right|$.\n\nUse the following test suite of three cases. In each case, $P$ is given explicitly and $v_0$ is specified. All numbers below are exact and unitless.\n\n- Case $1$ (ergodic and aperiodic, $n=3$):\n  - $P_1 = \\begin{bmatrix}\n  0.5  0.5  0.0 \\\\\n  0.2  0.3  0.5 \\\\\n  0.3  0.2  0.5\n  \\end{bmatrix}$,\n  - $v_0^{(1)} = \\begin{bmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3} \\end{bmatrix}$,\n  - tolerance $= 10^{-12}$,\n  - maximum iterations $= 10000$.\n\n- Case $2$ (irreducible but periodic of period $2$, $n=2$):\n  - $P_2 = \\begin{bmatrix}\n  0  1 \\\\\n  1  0\n  \\end{bmatrix}$,\n  - $v_0^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n  - tolerance $= 10^{-12}$,\n  - maximum iterations $= 100$.\n\n- Case $3$ (slowly mixing but aperiodic, $n=4$):\n  - $P_3 = \\begin{bmatrix}\n  0.98  0.019  0.001  0.0 \\\\\n  0.019  0.98  0.0  0.001 \\\\\n  0.001  0.0  0.98  0.019 \\\\\n  0.0  0.001  0.019  0.98\n  \\end{bmatrix}$,\n  - $v_0^{(3)} = \\begin{bmatrix} \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\end{bmatrix}$,\n  - tolerance $= 10^{-12}$,\n  - maximum iterations $= 50000$.\n\nYour program must compute, for each case $i \\in \\{1,2,3\\}$, the triple of diagnostics $\\left(\\|\\hat{v}^{(i)} - \\pi^{\\star (i)}\\|_1,\\; \\|P_i^\\top \\hat{v}^{(i)} - \\hat{v}^{(i)}\\|_1,\\; \\left|\\frac{\\hat{v}^{(i)\\top} P_i^\\top \\hat{v}^{(i)}}{\\hat{v}^{(i)\\top} \\hat{v}^{(i)}} - 1\\right|\\right)$. Aggregate the results for all three cases into a single line of output as a comma-separated list enclosed in square brackets in the order $[\\text{case }1\\text{ distance}, \\text{case }1\\text{ residual}, \\text{case }1\\text{ Rayleigh error}, \\text{case }2\\text{ distance}, \\text{case }2\\text{ residual}, \\text{case }2\\text{ Rayleigh error}, \\text{case }3\\text{ distance}, \\text{case }3\\text{ residual}, \\text{case }3\\text{ Rayleigh error}]$.\n\nYour program must be fully self-contained, must not read any input, and must print exactly one line in the specified format. No physical units or angle units are involved, so none should be reported. All numerical outputs must be real numbers.",
            "solution": "The problem requires the implementation of the power iteration method to find the dominant eigenpair of the transpose of a given row-stochastic matrix $P$. For a suitable Markov chain, this corresponds to finding its unique stationary distribution. The process involves iterative application of the matrix, numerical checks for convergence, and final evaluation of diagnostic metrics.\n\nThe stationary distribution of a Markov chain with transition matrix $P \\in \\mathbb{R}^{n \\times n}$ is a probability vector $\\pi \\in \\mathbb{R}^n$ (represented here as a column vector) that is a fixed point of the transposed transition operator, satisfying the equation:\n$$\nP^\\top \\pi = \\pi\n$$\nThis indicates that $\\pi$ is a right-hand eigenvector of the matrix $P^\\top$ corresponding to the eigenvalue $\\lambda = 1$. The Perron-Frobenius theorem guarantees that for a regular Markov chain (irreducible and aperiodic), this eigenvalue is unique in its maximum magnitude, and the corresponding eigenvector (the stationary distribution) is unique and has strictly positive components.\n\n### Method and Implementation Details\n\n#### 1. Power Iteration Algorithm\nThe power iteration method is employed to find the eigenvector associated with the dominant eigenvalue. For a matrix $A$ and an initial vector $v_0$, the iteration is defined by:\n$$\nv_{k+1} = \\frac{A v_k}{\\|A v_k\\|}\n$$\nIn our context, the matrix is $A = P^\\top$, and the problem specifies using the $\\ell_1$-norm for normalization to ensure that each iterate remains a probability vector. The iteration is thus:\n$$\nv_{k+1} = \\frac{P^\\top v_k}{\\|P^\\top v_k\\|_1}\n$$\nThe process starts with an initial probability vector $v_0$ and continues until one of two conditions is met:\n1.  **Convergence:** The iteration stops when the iterate $v_k$ is sufficiently close to being an eigenvector. This is measured by the $\\ell_1$-norm of the residual, defined as $r_k = \\|P^\\top v_k - v_k\\|_1$. The loop terminates if $r_k \\le \\tau$, where $\\tau$ is a given tolerance.\n2.  **Maximum Iterations:** A hard limit on the number of iterations, $k_{\\text{max}}$, is imposed to prevent an infinite loop, particularly in cases where the method does not converge (e.g., for periodic Markov chains).\n\nThe final vector produced by this iterative process is denoted as $\\hat{v}$.\n\n#### 2. Exact Stationary Distribution Calculation\nTo assess the accuracy of the power iteration, a reference stationary distribution, $\\pi^\\star$, must be computed with high precision. This is achieved by directly solving the system of linear equations that define $\\pi^\\star$:\n$$\n\\begin{cases}\nP^\\top \\pi^\\star = \\pi^\\star \\\\\n\\mathbf{1}^\\top \\pi^\\star = 1\n\\end{cases}\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^n$ is a column vector of ones. The first equation can be rewritten as $(P^\\top - I) \\pi^\\star = \\mathbf{0}$, where $I$ is the identity matrix. This is a homogeneous system, and since $\\lambda = 1$ is an eigenvalue, the matrix $(P^\\top - I)$ is singular. To obtain a unique solution, one of the linearly dependent equations in this system is replaced by the normalization constraint $\\mathbf{1}^\\top \\pi^\\star = 1$. We construct a new matrix $A'$ by taking the first $n-1$ rows of $(P^\\top - I)$ and setting the last row to be $\\mathbf{1}^\\top$. We also construct a new right-hand-side vector $b' = [0, 0, \\dots, 0, 1]^\\top$. The resulting non-homogeneous system $A' \\pi^\\star = b'$ is well-posed for an irreducible chain and can be solved using a standard linear solver.\n\n#### 3. Diagnostic Metrics\nOnce the power iteration terminates, yielding the final iterate $\\hat{v}$, and the reference distribution $\\pi^\\star$ is calculated, three diagnostic metrics are computed to evaluate the performance and accuracy of the iteration:\n\n1.  **$\\ell_1$-Distance to Stationarity:** This metric measures the error in the computed eigenvector. It is the $\\ell_1$-norm of the difference between the final iterate and the true stationary distribution:\n    $$\n    d = \\|\\hat{v} - \\pi^\\star\\|_1\n    $$\n\n2.  **Final Residual:** This metric quantifies how well the final iterate $\\hat{v}$ satisfies the eigenvector equation. It is the $\\ell_1$-norm of the residual vector for $\\hat{v}$:\n    $$\n    r = \\|P^\\top \\hat{v} - \\hat{v}\\|_1\n    $$\n    For a perfect eigenvector, this value would be $0$.\n\n3.  **Absolute Rayleigh Quotient Error:** The Rayleigh quotient provides an estimate of the eigenvalue corresponding to an approximate eigenvector. For a candidate eigenvector $v$, it is defined as $R(v) = \\frac{v^\\top P^\\top v}{v^\\top v}$. Since we are seeking the eigenvector for $\\lambda = 1$, the error is the absolute difference between the Rayleigh quotient of the final iterate $\\hat{v}$ and $1$:\n    $$\n    \\epsilon_{RQ} = \\left| \\frac{\\hat{v}^\\top P^\\top \\hat{v}}{\\hat{v}^\\top \\hat{v}} - 1 \\right|\n    $$\n    This metric also approaches $0$ as $\\hat{v}$ converges to the true eigenvector. The denominator $\\hat{v}^\\top \\hat{v}$ is the squared $\\ell_2$-norm of $\\hat{v}$.\n\nThese steps are applied to each of the three test cases, which are designed to probe the behavior of the power iteration under different conditions: a standard ergodic case, a periodic case where convergence fails, and a case where the initial guess is already the solution.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    def solve_case(P, v0, tol, max_iter):\n        \"\"\"\n        Performs power iteration and calculates diagnostics for a single case.\n\n        Args:\n            P (np.ndarray): The row-stochastic transition matrix.\n            v0 (np.ndarray): The initial probability vector.\n            tol (float): The convergence tolerance for the residual.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            tuple: A tuple containing the three diagnostics:\n                   (distance_to_stationary, final_residual, rayleigh_quotient_error)\n        \"\"\"\n        n = P.shape[0]\n\n        # 1. Compute the reference stationary distribution (pi_star)\n        try:\n            A = P.T - np.identity(n)\n            A[-1, :] = 1.0  # Replace the last row with the normalization constraint\n            b = np.zeros(n)\n            b[-1] = 1.0     # Set the RHS for the normalization constraint\n            pi_star = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Handle periodic case where A' may be singular (e.g. reducible chains)\n            # For the given problem, only Case 2 is periodic but it's irreducible.\n            # The matrix A' is not singular. However, for a generic periodic chain,\n            # this solver could fail. For this problem's cases, it is fine.\n            # For the periodic case 2, the true stationary dist is [0.5, 0.5]\n            if n==2 and np.all(P == np.array([[0,1],[1,0]])):\n                 pi_star = np.array([0.5, 0.5])\n            else: # Should not happen for problem's test cases\n                 pi_star = np.full(n, 1/n)\n\n\n        # 2. Perform power iteration\n        v = v0.copy()\n        for _ in range(max_iter):\n            # Check for convergence before updating the vector\n            # The problem asks to check residual of v_k, not v_{k-1}.\n            # Let's compute next vector first and then check its residual\n            v_unnorm = P.T @ v\n            v_next = v_unnorm / np.linalg.norm(v_unnorm, ord=1)\n            \n            residual = np.linalg.norm(P.T @ v_next - v_next, ord=1)\n            v = v_next\n            \n            if residual = tol:\n                break\n        \n        hat_v = v\n\n        # 3. Compute the three diagnostics\n        # Diagnostic 1: L1-distance to stationarity\n        dist_to_stat = np.linalg.norm(hat_v - pi_star, ord=1)\n\n        # Diagnostic 2: Final residual (recompute for clarity)\n        final_residual = np.linalg.norm(P.T @ hat_v - hat_v, ord=1)\n\n        # Diagnostic 3: Absolute Rayleigh quotient error\n        numerator = hat_v.T @ P.T @ hat_v\n        denominator = hat_v.T @ hat_v\n        if denominator == 0:\n             rayleigh_quotient = 1.0 # Avoid division by zero\n        else:\n             rayleigh_quotient = numerator / denominator\n        rq_error = np.abs(rayleigh_quotient - 1.0)\n        \n        return (dist_to_stat, final_residual, rq_error)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"P\": np.array([\n                [0.5, 0.5, 0.0],\n                [0.2, 0.3, 0.5],\n                [0.3, 0.2, 0.5]\n            ]),\n            \"v0\": np.array([1/3, 1/3, 1/3]),\n            \"tol\": 1e-12,\n            \"max_iter\": 10000\n        },\n        {\n            \"P\": np.array([\n                [0.0, 1.0],\n                [1.0, 0.0]\n            ]),\n            \"v0\": np.array([1.0, 0.0]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        },\n        {\n            \"P\": np.array([\n                [0.98, 0.019, 0.001, 0.0],\n                [0.019, 0.98, 0.0, 0.001],\n                [0.001, 0.0, 0.98, 0.019],\n                [0.0, 0.001, 0.019, 0.98]\n            ]),\n            \"v0\": np.array([1/4, 1/4, 1/4, 1/4]),\n            \"tol\": 1e-12,\n            \"max_iter\": 50000\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        diagnostics = solve_case(case[\"P\"], case[\"v0\"], case[\"tol\"], case[\"max_iter\"])\n        all_results.extend(diagnostics)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The convergence of the power iteration is guaranteed only under specific spectral conditions—namely, the existence of a single, unique eigenvalue with the largest magnitude. This exercise focuses on what happens when this fundamental requirement is not met, a crucial aspect of understanding the algorithm's limitations . You will develop a quantitative measure of directional stability to empirically test the algorithm's behavior on matrices that lack a unique dominant eigenvalue, observing firsthand the resulting rotational or oscillatory patterns instead of convergence to a single eigenvector.",
            "id": "3592899",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real matrix. The standard power iteration generates a sequence of vectors $\\{x_k\\}_{k \\geq 0}$ by the recurrence $x_{k+1} = A x_k$ and, in practice, one studies the normalized sequence $y_k = x_k / \\|x_k\\|_2$ for $k \\geq 0$ to analyze directional behavior. The fundamental base for this problem is the definition of eigenvalues and eigenvectors: a scalar $\\lambda \\in \\mathbb{C}$ and a nonzero vector $v \\in \\mathbb{C}^n$ are called an eigenvalue and eigenvector of $A$ if $A v = \\lambda v$. If $A$ is diagonalizable, any initial vector $x_0$ can be decomposed in the eigenbasis of $A$, and repeated multiplication by $A$ scales each eigencomponent by successive powers of the corresponding eigenvalue. The spectral radius $\\rho(A)$ is defined by $\\rho(A) = \\max_{i} |\\lambda_i|$ over all eigenvalues $\\lambda_i$ of $A$. When there is a unique eigenvalue with modulus equal to the spectral radius, the direction of $y_k$ tends to the eigenvector associated with that eigenvalue for generic $x_0$. However, if there are two or more eigenvalues with equal modulus equal to the spectral radius, different eigencomponents can persist in magnitude and the direction of $y_k$ may fail to stabilize.\n\nIn this problem, you must implement the standard power iteration with normalization and a quantitative test for directional stabilization. Define the stabilization metric as follows. Given the normalized sequence $y_k = x_k / \\|x_k\\|_2$, for $k \\geq 0$, define\n$$\nd_k = \\min\\left(\\|y_k - y_{k-1}\\|_2,\\;\\|y_k + y_{k-1}\\|_2\\right) \\quad \\text{for } k \\geq 1,\n$$\nwhich measures the smallest change in direction between successive normalized iterates, accounting for the global sign ambiguity that is irrelevant for eigenvector directions. For a chosen burn-in window length $L$, define the stability residual\n$$\n\\rho = \\max\\{d_k : k = K-L+1, K-L+2, \\dots, K\\},\n$$\nwhere $K$ is the total number of iterations performed. A sequence is deemed stabilized if $\\rho \\leq \\tau$ for a given tolerance $\\tau  0$.\n\nYour task is to write a complete program that:\n- Implements the normalized power iteration for a given matrix $A$, initial vector $x_0 \\neq 0$, and iteration count $K$, and computes the stabilization residual $\\rho$ over the last $L$ iterates using the metric above.\n- Returns a Boolean value indicating whether stabilization occurred within tolerance $\\tau$.\n\nTest suite and parameters:\n1. Happy path with a unique dominant eigenvalue:\n   - Matrix $A_1 = \\operatorname{diag}(2.5, 1.2, 0.8)$.\n   - Initial vector $x_0 = (1, 1, 1)^\\top$.\n   - Iterations $K = 600$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-8}$.\n   This case should stabilize.\n\n2. Two distinct eigenvalues of equal modulus (rotation; complex-conjugate pair on the unit circle):\n   - Matrix $A_2 = R(\\theta)$ with\n     $$\n     R(\\theta) = \\begin{bmatrix}\n     \\cos \\theta  -\\sin \\theta \\\\\n     \\sin \\theta  \\cos \\theta\n     \\end{bmatrix}, \\quad \\theta = 0.3 \\text{ radians}.\n     $$\n   - Initial vector $x_0 = (1, 0)^\\top$.\n   - Iterations $K = 400$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should fail to stabilize.\n\n3. Two distinct real eigenvalues of equal modulus with sign alternation:\n   - Matrix $A_3 = \\begin{bmatrix}0  1 \\\\ 1  0\\end{bmatrix}$.\n   - Initial vector $x_0 = (1, 2)^\\top$.\n   - Iterations $K = 400$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should fail to stabilize.\n\n4. Diagonal matrix with dominant eigenvalues of equal modulus but opposite sign:\n   - Matrix $A_4 = \\operatorname{diag}(2, -2)$.\n   - Initial vector $x_0 = (1, 1)^\\top$.\n   - Iterations $K = 400$.\n   - Burn-in length $L = 100$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should fail to stabilize.\n\n5. Near-tie eigenvalues (slow convergence but unique modulus maximum):\n   - Matrix $A_5 = \\operatorname{diag}(1.01, 1.00, 0.99)$.\n   - Initial vector $x_0 = (1, 1, 1)^\\top$.\n   - Iterations $K = 2000$.\n   - Burn-in length $L = 200$.\n   - Tolerance $\\tau = 10^{-6}$.\n   This case should stabilize.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1, result_2, result_3, result_4, result_5]$), where each $result_i$ is a Boolean indicating stabilization for the corresponding test case.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the power iteration method for finding dominant eigenvalues. The problem is well-posed, providing all necessary matrices, initial vectors, and parameters ($A, x_0, K, L, \\tau$) for each test case. The definitions for the stabilization metric $d_k$ and stability residual $\\rho$ are mathematically precise and objective, allowing for a unique, verifiable solution. The test cases are thoughtfully designed to probe different convergence behaviors of the power iteration method, from simple convergence to various modes of non-convergence.\n\nThe power iteration method is an algorithm to find the eigenvalue with the largest magnitude (the dominant eigenvalue) and its corresponding eigenvector for a given matrix $A \\in \\mathbb{R}^{n \\times n}$. The core idea relies on the iterative application of the matrix to an initial vector $x_0$. If $A$ is diagonalizable with eigenvalues $\\lambda_1, \\dots, \\lambda_n$ and corresponding eigenvectors $v_1, \\dots, v_n$, such that there is a unique dominant eigenvalue $|\\lambda_1|  |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$, then any initial vector $x_0$ with a non-zero component along $v_1$ can be expressed as $x_0 = \\sum_{i=1}^n c_i v_i$ where $c_1 \\neq 0$.\n\nThe $k$-th iterate is given by $x_k = A^k x_0$. This can be expanded in the eigenbasis:\n$$\nx_k = A^k \\sum_{i=1}^n c_i v_i = \\sum_{i=1}^n c_i A^k v_i = \\sum_{i=1}^n c_i \\lambda_i^k v_i\n$$\nFactoring out the dominant eigenvalue term $\\lambda_1^k$ yields:\n$$\nx_k = \\lambda_1^k \\left( c_1 v_1 + \\sum_{i=2}^n c_i \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k v_i \\right)\n$$\nAs $k \\to \\infty$, the terms $(\\lambda_i/\\lambda_1)^k$ for $i \\ge 2$ approach zero because $|\\lambda_i/\\lambda_1|  1$. Consequently, the vector $x_k$ becomes increasingly aligned with the dominant eigenvector $v_1$, i.e., $x_k \\approx c_1 \\lambda_1^k v_1$.\n\nTo prevent the magnitude of $x_k$ from diverging or vanishing, we study the sequence of normalized vectors $y_k = x_k / \\|x_k\\|_2$. For a unique dominant eigenvalue, this sequence $\\{y_k\\}$ converges to a unit vector in the direction of $v_1$.\n\nThe algorithm's convergence fails if there is no unique dominant eigenvalue. If multiple distinct eigenvalues share the same maximal modulus, say $|\\lambda_1| = |\\lambda_2| = \\dots = |\\lambda_m|  |\\lambda_{m+1}|$, then the components corresponding to all these eigenvectors ($v_1, \\dots, v_m$) will persist in the iteration, and the direction of $y_k$ will generally not stabilize.\n\nThis problem requires implementing a quantitative test for this directional stabilization. The proposed metric is for $k \\ge 1$:\n$$\nd_k = \\min\\left(\\|y_k - y_{k-1}\\|_2,\\;\\|y_k + y_{k-1}\\|_2\\right)\n$$\nThis metric shrewdly accounts for the fact that an eigenvector's direction is defined up to a sign. If the sequence $y_k$ converges to a fixed direction $u$, then $\\|y_k - y_{k-1}\\|_2 \\to 0$. If the sequence alternates in sign, e.g., $y_k \\approx (-1)^k u$, then $\\|y_k + y_{k-1}\\|_2 \\to 0$. In either case of directional convergence, $d_k \\to 0$.\n\nThe stability residual $\\rho$ is defined as the maximum value of $d_k$ over a final \"burn-in\" window of $L$ iterations:\n$$\n\\rho = \\max\\{d_k : k = K-L+1, \\dots, K\\}\n$$\nA sequence is deemed stabilized if this maximum directional change over the window is below a specified tolerance $\\tau$, i.e., $\\rho \\le \\tau$.\n\nThe implementation will consist of a function that takes $A, x_0, K, L$, and $\\tau$ as inputs. It will perform the following steps:\n1. Initialize the vector $x$ with the initial vector $x_0$.\n2. Normalize $x$ to get the first vector in the sequence, $y_0$. We will designate this as `y_prev`.\n3. Iterate from $k=1$ to $K$. In each iteration:\n   a. Update the vector: $x \\leftarrow A x$.\n   b. Normalize the new vector: $y_{\\text{curr}} \\leftarrow x / \\|x\\|_2$.\n   c. If the iteration count $k$ is within the final window ($k \\ge K - L + 1$), calculate $d_k = \\min(\\|y_{\\text{curr}} - y_{\\text{prev}}\\|_2, \\|y_{\\text{curr}} + y_{\\text{prev}}\\|_2)$ and store it.\n   d. Update for the next iteration: $y_{\\text{prev}} \\leftarrow y_{\\text{curr}}$.\n4. After the loop completes, find the maximum value, $\\rho$, among the stored $d_k$ values.\n5. Return the boolean result of the comparison $\\rho \\le \\tau$.\n\nThis procedure will be applied to each of the five test cases provided, and the boolean results will be collected and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration_stability_check(A: np.ndarray, x0: np.ndarray, K: int, L: int, tau: float) - bool:\n    \"\"\"\n    Performs normalized power iteration and checks for directional stabilization.\n\n    Args:\n        A (np.ndarray): The matrix for the iteration.\n        x0 (np.ndarray): The initial vector.\n        K (int): The total number of iterations.\n        L (int): The length of the burn-in window for the stability check.\n        tau (float): The tolerance for stabilization.\n\n    Returns:\n        bool: True if the sequence stabilized within the tolerance, False otherwise.\n    \"\"\"\n    if L  K or L = 0:\n        raise ValueError(\"Burn-in length L must be positive and not greater than K.\")\n        \n    x = x0.astype(np.float64)\n\n    # Ensure the initial vector is not a zero vector\n    norm_x = np.linalg.norm(x)\n    if norm_x == 0:\n        return False  # Or raise an error for invalid input\n\n    y_prev = x / norm_x\n    \n    d_values = []\n\n    for k in range(1, K + 1):\n        x = A @ x\n        norm_x = np.linalg.norm(x)\n\n        if norm_x == 0:\n            # Iteration produced a zero vector.\n            # This happens if A is singular and x lies in a subspace mapped to the null space.\n            # The direction is undefined, hence not stabilized.\n            return False\n\n        y_curr = x / norm_x\n\n        if k = K - L + 1:\n            dist_minus = np.linalg.norm(y_curr - y_prev)\n            dist_plus = np.linalg.norm(y_curr + y_prev)\n            d_k = min(dist_minus, dist_plus)\n            d_values.append(d_k)\n        \n        y_prev = y_curr\n\n    if not d_values:\n        # This case is avoided by the initial L > 0 check, but as a safeguard:\n        # Not enough iterations to compute stability residual.\n        return False\n\n    rho = max(d_values)\n    \n    return rho = tau\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement, runs the stability check,\n    and prints the results in the required format.\n    \"\"\"\n    # Test Case 1: Happy path with a unique dominant eigenvalue\n    A1 = np.diag([2.5, 1.2, 0.8])\n    x0_1 = np.array([1.0, 1.0, 1.0])\n    params1 = (A1, x0_1, 600, 100, 1e-8)\n\n    # Test Case 2: Rotation matrix (complex-conjugate eigenvalue pair)\n    theta = 0.3\n    c, s = np.cos(theta), np.sin(theta)\n    A2 = np.array([[c, -s], [s, c]])\n    x0_2 = np.array([1.0, 0.0])\n    params2 = (A2, x0_2, 400, 100, 1e-6)\n\n    # Test Case 3: Two distinct real eigenvalues of equal modulus (1 and -1)\n    A3 = np.array([[0.0, 1.0], [1.0, 0.0]])\n    x0_3 = np.array([1.0, 2.0])\n    params3 = (A3, x0_3, 400, 100, 1e-6)\n\n    # Test Case 4: Diagonal matrix with dominant eigenvalues of equal modulus (2 and -2)\n    A4 = np.diag([2.0, -2.0])\n    x0_4 = np.array([1.0, 1.0])\n    params4 = (A4, x0_4, 400, 100, 1e-6)\n\n    # Test Case 5: Near-tie eigenvalues (slow convergence)\n    A5 = np.diag([1.01, 1.00, 0.99])\n    x0_5 = np.array([1.0, 1.0, 1.0])\n    params5 = (A5, x0_5, 2000, 200, 1e-6)\n\n    test_cases = [params1, params2, params3, params4, params5]\n\n    results = []\n    for params in test_cases:\n        A, x0, K, L, tau = params\n        result = power_iteration_stability_check(A, x0, K, L, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from theoretical mathematics to practical computation introduces the subtle yet profound effects of finite precision arithmetic. This advanced practice explores how the realities of implementing algorithms on a computer can lead to unexpected behaviors that are invisible in exact arithmetic . You will construct a scenario where a dominant eigenvector has components of vastly different scales and, by using a simplified floating-point emulator, investigate how the choice of normalization—specifically the $\\| \\cdot \\|_2$ versus the $\\| \\cdot \\|_\\infty$ norm—can determine whether a critical component of the eigenvector is prematurely lost to underflow, causing the iteration to stagnate on an incorrect result.",
            "id": "3592836",
            "problem": "Consider a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and the classical power iteration that updates a vector sequence $\\{x_k\\}_{k \\ge 0}$ by\n$$\nx_{k+1} \\;=\\; \\frac{A x_k}{\\|A x_k\\|},\n$$\nwhere the normalization uses either the $2$-norm $\\|\\cdot\\|_2$ or the infinity-norm $\\|\\cdot\\|_\\infty$. In finite precision arithmetic, normalization and matrix-vector products do not occur in exact arithmetic. Let the floating point model be the following simplified binary rounding scheme: every computed real number $z$ is replaced by\n$$\n\\mathrm{fl}(z) \\;=\\; \\begin{cases}\n\\operatorname{sign}(z) \\cdot m_r \\cdot 2^{e_r},  \\text{if } e_r \\in [e_{\\min}, e_{\\max}], \\\\\n0,  \\text{if } e_r  e_{\\min}, \\\\\n+\\infty \\cdot \\operatorname{sign}(z),  \\text{if } e_r  e_{\\max},\n\\end{cases}\n$$\nwhere the exact $z$ is decomposed as $z = m \\cdot 2^{e}$ with $m \\in [1,2)$, $e \\in \\mathbb{Z}$, and the rounded mantissa $m_r$ satisfies\n$$\nm_r \\;=\\; \\frac{\\left\\lfloor m \\cdot 2^{p} + \\tfrac{1}{2} \\right\\rfloor}{2^{p}},\n$$\nwith a carry to the exponent if $m_r = 2$. Here $p \\in \\mathbb{N}$ is the number of fraction bits in the mantissa, and $e_{\\min}, e_{\\max} \\in \\mathbb{Z}$ define the minimum and maximum exponents. All arithmetic operations in the algorithm must be rounded by $\\mathrm{fl}(\\cdot)$ after each operation. In particular, for a matrix-vector product $y = A x$, each product $A_{ij} x_j$ is rounded by $\\mathrm{fl}(\\cdot)$, and each accumulation into the sum $\\sum_j A_{ij} x_j$ is rounded by $\\mathrm{fl}(\\cdot)$ after every addition. For normalization by $\\|\\cdot\\|_2$, the sum of squares and the square root must also be rounded by $\\mathrm{fl}(\\cdot)$ at every step, and for normalization by $\\|\\cdot\\|_\\infty$, the denominator is the rounded maximum of the rounded absolute values.\n\nBase your reasoning on the core definitions of the power iteration update, the properties of the $2$-norm and infinity-norm, and the standard finite precision model with rounding and exponent bounds as stated above; do not employ any specialized pre-derived formulas beyond these foundations. Investigate how the choice of normalization $\\|A x_k\\|_2$ versus $\\|A x_k\\|_\\infty$ interacts with finite precision to cause the loss of the smallest nonzero component in $x_k$ when the dominant eigenvector is badly scaled.\n\nConstruct, for a given dimension $n \\ge 2$, a matrix $A$ whose dominant eigenvector $v_1$ has components\n$$\nv_1 \\;=\\; [\\,1,\\,1,\\,\\dots,\\,1,\\,\\alpha\\,]^\\top,\n$$\nwith $\\alpha \\in \\mathbb{R}_{0}$ very small, and all other eigenvectors equal to the standard basis vectors $e_1, e_2, \\dots, e_{n-1}$. Assign the dominant eigenvalue $\\lambda_{\\mathrm{dom}} \\in \\mathbb{R}_{0}$ to $v_1$ and a common subdominant eigenvalue $\\lambda_{\\mathrm{sub}} \\in \\mathbb{R}_{0}$ to each $e_j$ for $1 \\le j \\le n-1$, with $\\lambda_{\\mathrm{dom}}  \\lambda_{\\mathrm{sub}}$. Your construction must be mathematically consistent with these eigen-structure requirements. Initialize the power iteration at $x_0 = v_1$ (not necessarily normalized), and perform a fixed number of iterations. At each iteration, record whether the smallest nonzero component in the true dominant eigenvector, which is the entry at index $n$, becomes exactly zero after rounding and normalization, and whether it remains zero for all subsequent iterations. Define “stagnation” to mean that there exists an iteration index $k_0$ such that the $n$-th component of $x_k$ is exactly zero for all $k \\ge k_0$.\n\nExplain why the choice of normalization can lead to different scaling factors for the smallest component under finite precision, and why, for certain $(n,\\alpha)$ combinations relative to the minimum exponent $e_{\\min}$, normalization by $\\|\\cdot\\|_2$ can force the smallest component below the underflow threshold while normalization by $\\|\\cdot\\|_\\infty$ may not, or vice versa.\n\nYour program must implement:\n- A floating point emulator consistent with the above $\\mathrm{fl}(\\cdot)$ model with parameters $p$, $e_{\\min}$, $e_{\\max}$.\n- A matrix construction procedure for $A$ consistent with the specified eigen-structure.\n- Power iteration with either $\\|A x_k\\|_2$ or $\\|A x_k\\|_\\infty$ normalization, fully rounded at every arithmetic step by $\\mathrm{fl}(\\cdot)$.\n- A stagnation detector that returns a boolean indicating whether the smallest component has been lost and remains at zero from some iteration onward.\n\nTest Suite. Use the following parameter sets to assess the phenomena across different regimes:\n1. Happy path case where normalization by $\\|\\cdot\\|_2$ causes stagnation but normalization by $\\|\\cdot\\|_\\infty$ does not: $n=64$, $\\alpha=2^{-9}$, $\\lambda_{\\mathrm{dom}}=2$, $\\lambda_{\\mathrm{sub}}=0.5$, $p=10$, $e_{\\min}=-10$, $e_{\\max}=20$, iterations $=20$.\n2. Boundary case near the underflow threshold for $\\|\\cdot\\|_2$: $n=64$, $\\alpha=1.1 \\cdot 2^{-10} \\cdot \\sqrt{n-1}$, $\\lambda_{\\mathrm{dom}}=2$, $\\lambda_{\\mathrm{sub}}=0.5$, $p=10$, $e_{\\min}=-10$, $e_{\\max}=20$, iterations $=20$.\n3. Edge case where both normalizations cause stagnation due to extreme $\\alpha$: $n=64$, $\\alpha=2^{-12}$, $\\lambda_{\\mathrm{dom}}=2$, $\\lambda_{\\mathrm{sub}}=0.5$, $p=10$, $e_{\\min}=-10$, $e_{\\max}=20$, iterations $=20$.\n4. Case with no stagnation under either normalization due to moderate scaling: $n=32$, $\\alpha=0.05$, $\\lambda_{\\mathrm{dom}}=2$, $\\lambda_{\\mathrm{sub}}=0.5$, $p=10$, $e_{\\min}=-10$, $e_{\\max}=20$, iterations $=20$.\n\nFor each test case, run two experiments: one with $\\|\\cdot\\|_2$ normalization and one with $\\|\\cdot\\|_\\infty$ normalization. The required final output format is a single line containing the results as a comma-separated list of pairs of booleans in square brackets, where each pair is $[\\text{stagnation}_\\infty,\\text{stagnation}_2]$ for the corresponding test case. For example:\n$$\n[\\,[\\text{True},\\text{False}],\\,[\\text{False},\\text{False}],\\,[\\text{True},\\text{True}],\\,[\\text{False},\\text{False}]\\,]\n$$\nYour program must print exactly one such line, with no additional text.",
            "solution": "The problem is valid as it presents a well-defined numerical experiment grounded in the principles of linear algebra and finite precision arithmetic. It is self-contained, consistent, and poses a non-trivial question about the behavior of the power iteration algorithm under different numerical conditions.\n\nThe core of the problem is to understand how the choice of vector norm in the power iteration method can, under finite precision arithmetic, lead to the loss of small components of the dominant eigenvector. We will first derive the matrix $A$ that satisfies the given eigen-structure, then analyze a single step of the power iteration in the specified floating-point model to explain the phenomenon.\n\n### 1. Matrix Construction\n\nWe are asked to construct a matrix $A \\in \\mathbb{R}^{n \\times n}$ with a dominant eigenvector $v_1 = [1, 1, \\dots, 1, \\alpha]^\\top$ associated with eigenvalue $\\lambda_{\\mathrm{dom}}$, and $n-1$ other eigenvectors given by the standard basis vectors $e_1, e_2, \\dots, e_{n-1}$, all associated with a subdominant eigenvalue $\\lambda_{\\mathrm{sub}}$. Here, $e_j$ is the vector with a $1$ in the $j$-th position and zeros elsewhere. We are given $\\lambda_{\\mathrm{dom}}  \\lambda_{\\mathrm{sub}}  0$ and $\\alpha  0$.\n\nThe conditions $A e_j = \\lambda_{\\mathrm{sub}} e_j$ for $j=1, \\dots, n-1$ force the first $n-1$ columns of $A$ to be $\\lambda_{\\mathrm{sub}} e_j$. This means $A$ is an upper triangular matrix of the form:\n$$A = \\begin{pmatrix} \\lambda_{\\mathrm{sub}}  0  \\cdots  A_{1,n} \\\\ 0  \\lambda_{\\mathrm{sub}}  \\cdots  A_{2,n} \\\\ \\vdots   \\ddots  \\vdots \\\\ 0  \\cdots  0  A_{n,n} \\end{pmatrix}$$\nThe eigenvalues of an upper triangular matrix are its diagonal entries. For $\\lambda_{\\mathrm{dom}}$ to be the dominant eigenvalue, we must have $A_{n,n} = \\lambda_{\\mathrm{dom}}$.\n\nNow we enforce the condition $A v_1 = \\lambda_{\\mathrm{dom}} v_1$.\n$$ A v_1 = \\begin{pmatrix} \\lambda_{\\mathrm{sub}}  \\cdots  0  A_{1,n} \\\\ \\vdots  \\ddots   \\vdots \\\\ 0  \\cdots  \\lambda_{\\mathrm{sub}}  A_{n-1,n} \\\\ 0  \\cdots  0  \\lambda_{\\mathrm{dom}} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\\\ \\alpha \\end{pmatrix} = \\begin{pmatrix} \\lambda_{\\mathrm{sub}} \\cdot 1 + A_{1,n} \\cdot \\alpha \\\\ \\vdots \\\\ \\lambda_{\\mathrm{sub}} \\cdot 1 + A_{n-1,n} \\cdot \\alpha \\\\ \\lambda_{\\mathrm{dom}} \\cdot \\alpha \\end{pmatrix} $$\nWe require this to be equal to $\\lambda_{\\mathrm{dom}} v_1 = [\\lambda_{\\mathrm{dom}}, \\dots, \\lambda_{\\mathrm{dom}}, \\lambda_{\\mathrm{dom}}\\alpha]^\\top$.\nComparing the components:\n- For $i=n$: $\\lambda_{\\mathrm{dom}} \\alpha = \\lambda_{\\mathrm{dom}} \\alpha$. This is satisfied.\n- For $i=1, \\dots, n-1$: $\\lambda_{\\mathrm{sub}} + A_{i,n} \\alpha = \\lambda_{\\mathrm{dom}}$, which implies $A_{i,n} = \\frac{\\lambda_{\\mathrm{dom}} - \\lambda_{\\mathrm{sub}}}{\\alpha}$.\n\nSo, the matrix is fully determined and consistent with the required eigen-structure.\n\n### 2. Power Iteration and Finite Precision Effects\n\nThe power iteration starts with $x_0 = v_1$ and computes the sequence $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|}$. In exact arithmetic, $A v_1 = \\lambda_{\\mathrm{dom}} v_1$, so the iteration would simply produce normalized versions of $v_1$. However, in finite precision arithmetic, designated by $\\mathrm{fl}(\\cdot)$, the behavior is different. Each arithmetic operation is rounded.\n\nLet's analyze the first iteration step, starting with $x_0 = v_1 = [1, \\dots, 1, \\alpha]^\\top$. First, we compute $y_1 = \\mathrm{fl}(A x_0)$.\nIn exact arithmetic, $y_1 = A x_0 = \\lambda_{\\mathrm{dom}} x_0 = [\\lambda_{\\mathrm{dom}}, \\dots, \\lambda_{\\mathrm{dom}}, \\lambda_{\\mathrm{dom}}\\alpha]^\\top$. Due to rounding, the computed value will be close to this. Let's denote the computed vector as $y_1$.\n\nThe next step is to normalize $y_1$ to get $x_1 = \\mathrm{fl}(y_1 / \\mathrm{fl}(\\|y_1\\|))$. The core of the phenomenon lies in how $\\mathrm{fl}(\\|y_1\\|)$ is computed for the $2$-norm versus the infinity-norm.\n\n**Infinity-Norm Normalization ($L_\\infty$)**\nThe infinity-norm of $y_1$ is $\\|y_1\\|_\\infty = \\max_i |(y_1)_i|$. Since $(y_1)_n$ is small, the maximum will be one of the first $n-1$ components.\n$$\\|y_1\\|_\\infty \\approx \\lambda_{\\mathrm{dom}}$$\nThe components of the new vector $x_1$ are computed as $ (x_1)_i = \\mathrm{fl}((y_1)_i / \\mathrm{fl}(\\|y_1\\|_\\infty))$.\nThe last component becomes:\n$$(x_1)_n = \\mathrm{fl}(\\frac{\\mathrm{fl}(\\lambda_{\\mathrm{dom}} \\cdot \\alpha)}{\\mathrm{fl}(\\lambda_{\\mathrm{dom}})}) \\approx \\mathrm{fl}(\\alpha)$$\nThus, with infinity-norm normalization, the magnitude of the small component $\\alpha$ is approximately preserved in each iteration (up to rounding errors in the calculation). It will not underflow to zero unless $\\alpha$ itself is smaller than the smallest representable positive number.\n\n**2-Norm Normalization ($L_2$)**\nThe squared $2$-norm of $y_1$ is $\\|y_1\\|_2^2 = \\sum_{i=1}^n (y_1)_i^2$. This is approximately:\n$$\\|y_1\\|_2^2 \\approx \\sum_{i=1}^{n-1} (\\lambda_{\\mathrm{dom}})^2 + (\\lambda_{\\mathrm{dom}}\\alpha)^2 = (n-1)\\lambda_{\\mathrm{dom}}^2 + \\lambda_{\\mathrm{dom}}^2\\alpha^2 = \\lambda_{\\mathrm{dom}}^2((n-1)+\\alpha^2)$$\nSince $\\alpha$ is small, the $\\|y_1\\|_2$ is dominated by the sum of the large components:\n$$\\|y_1\\|_2 \\approx \\sqrt{(n-1)\\lambda_{\\mathrm{dom}}^2} = \\lambda_{\\mathrm{dom}} \\sqrt{n-1}$$\nThe last component of the new vector $x_1$ becomes:\n$$(x_1)_n = \\mathrm{fl}\\left(\\frac{\\mathrm{fl}(\\lambda_{\\mathrm{dom}} \\cdot \\alpha)}{\\mathrm{fl}(\\lambda_{\\mathrm{dom}}\\sqrt{n-1})}\\right) \\approx \\mathrm{fl}\\left(\\frac{\\alpha}{\\sqrt{n-1}}\\right)$$\nWith $2$-norm normalization, the small component $\\alpha$ is attenuated by a factor of approximately $\\sqrt{n-1}$ in each iteration. For large $n$, this factor can be significant.\n\n### 3. Underflow and Stagnation\n\nThe specified floating-point model has a minimum exponent $e_{\\min}$. A computed value $z$ underflows to $0$ if its representation $z = \\operatorname{sign}(z) \\cdot m_r \\cdot 2^{e_r}$ has an exponent $e_r  e_{\\min}$.\n\nThe key insight is that the repeated attenuation by $\\sqrt{n-1}$ can drive the small component below this underflow threshold. If the exponent of $\\alpha/\\sqrt{n-1}$ falls below $e_{\\min}$, the component $(x_k)_n$ will be rounded to exactly zero.\nThe exponent of a number $z$ is $e = \\lfloor \\log_2|z| \\rfloor$.\nFor the $2$-norm case, the exponent of the small component after one step is approximately $\\lfloor \\log_2(\\alpha/\\sqrt{n-1}) \\rfloor = \\lfloor \\log_2(\\alpha) - \\frac{1}{2}\\log_2(n-1) \\rfloor$. If this value is less than $e_{\\min}$, underflow occurs.\n\nOnce $(x_k)_n$ becomes zero, it stagnates. For any vector $x$ where $x_n=0$, the last component of the product $Ax$ is $(Ax)_n = \\sum_j A_{nj}x_j = A_{nn}x_n = \\lambda_{\\mathrm{dom}} \\cdot 0 = 0$. So, if the last component becomes zero at iteration $k_0$, it will remain zero for all subsequent iterations $k \\ge k_0$.\n\nFor test case 1 ($n=64, \\alpha=2^{-9}, e_{\\min}=-10$):\n- $L_\\infty$: The exponent of the small component remains near $\\log_2(2^{-9}) = -9$. Since $-9  e_{\\min}$, it does not underflow.\n- $L_2$: The exponent of the small component becomes approximately $\\lfloor\\log_2(2^{-9}) - \\frac{1}{2}\\log_2(63)\\rfloor \\approx \\lfloor-9 - 2.99\\rfloor = -12$. Since $-12  e_{\\min}$, it underflows to zero, causing stagnation.\n\nThis analysis explains why $L_2$ normalization can be more prone to losing small, but important, components of an eigenvector in finite precision, especially when the vector dimension $n$ is large.",
            "answer": "```python\nimport math\nimport numpy as np\n\nclass FP_Emulator:\n    \"\"\"\n    A class to emulate a simplified floating-point arithmetic system.\n    \"\"\"\n    def __init__(self, p, e_min, e_max):\n        self.p = p\n        self.e_min = e_min\n        self.e_max = e_max\n        self.two_p = 2**p\n\n    def fl(self, z: float) - float:\n        \"\"\"\n        Rounds a real number z according to the defined floating point model.\n        \"\"\"\n        if not np.isfinite(z) or z == 0.0:\n            return z\n\n        s = 1.0 if z  0 else -1.0\n        z_abs = abs(z)\n\n        # Decompose z_abs = m * 2^e, with m in [1, 2)\n        if z_abs == 1.0:\n            m, e = 1.0, 0\n        else:\n            m_frexp, e_frexp = math.frexp(z_abs)\n            m = 2.0 * m_frexp\n            e = e_frexp - 1\n        \n        e_r = e\n\n        # Round mantissa: round half up\n        m_r_num = math.floor(m * self.two_p + 0.5)\n        m_r = m_r_num / self.two_p\n\n        # Handle carry if rounded mantissa is 2\n        if m_r == 2.0:\n            m_r = 1.0\n            e_r += 1\n\n        # Check for underflow/overflow after potential exponent change\n        if e_r  self.e_min:\n            return 0.0\n        if e_r  self.e_max:\n            return s * float('inf')\n            \n        return s * m_r * (2**e_r)\n\nclass NumericOps:\n    \"\"\"\n    Provides numerical operations using the FP_Emulator.\n    \"\"\"\n    def __init__(self, fl_emulator: FP_Emulator):\n        self.fl = fl_emulator.fl\n\n    def matvec(self, A: np.ndarray, x: np.ndarray) - np.ndarray:\n        n = A.shape[0]\n        y = np.zeros(n)\n        for i in range(n):\n            current_sum = 0.0\n            for j in range(n):\n                prod = self.fl(A[i, j] * x[j])\n                current_sum = self.fl(current_sum + prod)\n            y[i] = current_sum\n        return y\n\n    def norm_inf(self, v: np.ndarray) - float:\n        max_abs = 0.0\n        for val in v:\n            abs_val = self.fl(abs(val))\n            if abs_val  max_abs:\n                max_abs = abs_val\n        return max_abs\n        \n    def norm2(self, v: np.ndarray) - float:\n        sum_sq = 0.0\n        for val in v:\n            sq = self.fl(val * val)\n            sum_sq = self.fl(sum_sq + sq)\n        return self.fl(math.sqrt(sum_sq))\n\n    def vec_scalar_div(self, v: np.ndarray, s: float) - np.ndarray:\n        if s == 0.0:\n            return np.full_like(v, float('inf'))\n        return np.array([self.fl(val / s) for val in v])\n\ndef construct_A(n, l_dom, l_sub, alpha, ops: NumericOps):\n    A = np.zeros((n, n))\n    c_val = (l_dom - l_sub) / alpha\n    c = ops.fl(c_val)\n    \n    for i in range(n - 1):\n        A[i, i] = ops.fl(l_sub)\n        A[i, n - 1] = c\n    A[n - 1, n - 1] = ops.fl(l_dom)\n    return A\n\ndef power_iteration(A, x0, num_iter, norm_type, ops: NumericOps):\n    x = np.copy(x0)\n    stagnation_started_at = -1\n\n    for k in range(num_iter):\n        y = ops.matvec(A, x)\n        \n        if norm_type == 'L2':\n            norm_y = ops.norm2(y)\n        elif norm_type == 'Linf':\n            norm_y = ops.norm_inf(y)\n        else:\n            raise ValueError(\"Unsupported norm type\")\n\n        if norm_y == 0.0: # Vector became zero, it will stagnate\n            if stagnation_started_at == -1:\n                stagnation_started_at = k\n            x = np.zeros_like(x)\n        else:\n            x = ops.vec_scalar_div(y, norm_y)\n        \n        # Stagnation detection logic\n        if x[-1] == 0.0:\n            if stagnation_started_at == -1:\n                stagnation_started_at = k\n        else:\n            # If it was zero and now it's not, reset.\n            if stagnation_started_at != -1:\n                stagnation_started_at = -1\n    \n    return stagnation_started_at != -1\n\ndef solve():\n    test_cases = [\n        {'n': 64, 'alpha_str': \"2**-9\", 'l_dom': 2, 'l_sub': 0.5, 'p': 10, 'e_min': -10, 'e_max': 20, 'iter': 20},\n        {'n': 64, 'alpha_str': \"1.1 * 2**-10 * math.sqrt(63)\", 'l_dom': 2, 'l_sub': 0.5, 'p': 10, 'e_min': -10, 'e_max': 20, 'iter': 20},\n        {'n': 64, 'alpha_str': \"2**-12\", 'l_dom': 2, 'l_sub': 0.5, 'p': 10, 'e_min': -10, 'e_max': 20, 'iter': 20},\n        {'n': 32, 'alpha_str': \"0.05\", 'l_dom': 2, 'l_sub': 0.5, 'p': 10, 'e_min': -10, 'e_max': 20, 'iter': 20},\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case['n']\n        alpha = eval(case['alpha_str'], {\"math\": math})\n        \n        l_dom, l_sub = case['l_dom'], case['l_sub']\n        p, e_min, e_max = case['p'], case['e_min'], case['e_max']\n        num_iter = case['iter']\n\n        fp_emulator = FP_Emulator(p, e_min, e_max)\n        ops = NumericOps(fp_emulator)\n        \n        A = construct_A(n, l_dom, l_sub, alpha, ops)\n        \n        v1 = np.ones(n)\n        v1[-1] = alpha\n        x0 = np.array([ops.fl(val) for val in v1])\n\n        # Run with infinity norm\n        stagnation_inf = power_iteration(A, np.copy(x0), num_iter, 'Linf', ops)\n        \n        # Run with 2-norm\n        stagnation_2 = power_iteration(A, np.copy(x0), num_iter, 'L2', ops)\n        \n        results.append([stagnation_inf, stagnation_2])\n\n    # Format the final output string exactly as specified\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}