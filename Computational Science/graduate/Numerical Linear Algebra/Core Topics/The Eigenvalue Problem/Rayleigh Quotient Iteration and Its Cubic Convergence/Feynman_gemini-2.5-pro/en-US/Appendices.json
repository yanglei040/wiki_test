{
    "hands_on_practices": [
        {
            "introduction": "The claim of cubic convergence for Rayleigh Quotient Iteration is one of its most celebrated features, but it can seem almost magical. This first exercise demystifies this property by guiding you through a symbolic, step-by-step derivation in the simplest non-trivial setting: a $2 \\times 2$ Hermitian matrix . By working through the algebra in the natural eigenbasis, you will see exactly how the error term is cubed at each iteration, turning an abstract theorem into a concrete and memorable result.",
            "id": "3572053",
            "problem": "Consider a $2 \\times 2$ Hermitian matrix $A$ with distinct eigenvalues $\\lambda_{1}$ and $\\lambda_{2}$, and associated orthonormal eigenvectors $v_{1}$ and $v_{2}$. Let the initial vector $x_{0}$ be a generic unit vector expressed in the eigenbasis as $x_{0} = \\cos(\\theta_{0})\\,v_{1} + \\sin(\\theta_{0})\\,v_{2}$, where $0 < \\theta_{0} < \\pi/2$. Define the Rayleigh quotient $\\mu(x) = x^{*} A x$. The Rayleigh Quotient Iteration (RQI) is given by the sequence\n$$\n\\mu_{k} = \\mu(x_{k}), \\quad y_{k} = (A - \\mu_{k} I)^{-1} x_{k}, \\quad x_{k+1} = \\frac{y_{k}}{\\|y_{k}\\|},\n$$\nwhere $I$ is the identity matrix and $\\|\\cdot\\|$ denotes the Euclidean norm. Let $\\theta_{k}$ denote the angle that $x_{k}$ makes with the target eigenvector $v_{1}$, so that $x_{k} = \\cos(\\theta_{k})\\,v_{1} + \\sin(\\theta_{k})\\,v_{2}$.\n\nStarting from $x_{0}$, symbolically carry out two iterations of Rayleigh Quotient Iteration to obtain $x_{1}$ and $x_{2}$. Using only the fundamental properties of Hermitian matrices, unitary diagonalization, and the definition of the Rayleigh quotient, derive the exact update for the tangent of the angle at each step and verify the cubic decrease of the angle to $v_{1}$. Your final task is to express $\\tan(\\theta_{2})$ in closed form as a function of $\\tan(\\theta_{0})$.\n\nProvide the final expression for $\\tan(\\theta_{2})$. No rounding is required.",
            "solution": "The problem asks for the relationship between $\\tan(\\theta_2)$ and $\\tan(\\theta_0)$ after two iterations of the Rayleigh Quotient Iteration (RQI) for a $2 \\times 2$ Hermitian matrix $A$. We will derive a general recurrence relation for $\\tan(\\theta_{k+1})$ in terms of $\\tan(\\theta_k)$ and then apply it twice.\n\nLet the orthonormal eigenvectors of the Hermitian matrix $A$ be $v_1$ and $v_2$, corresponding to the distinct real eigenvalues $\\lambda_1$ and $\\lambda_2$. Thus, we have $A v_1 = \\lambda_1 v_1$, $A v_2 = \\lambda_2 v_2$, and $v_i^* v_j = \\delta_{ij}$ for $i,j \\in \\{1, 2\\}$.\n\nThe state of the iteration at step $k$ is given by a unit vector $x_k$, which is expressed in the eigenbasis as:\n$$x_k = \\cos(\\theta_k) v_1 + \\sin(\\theta_k) v_2$$\nwhere $\\theta_k$ is the angle between $x_k$ and the eigenvector $v_1$. The ratio of the component along $v_2$ to the component along $v_1$ defines the tangent of this angle, $\\tan(\\theta_k) = \\frac{\\sin(\\theta_k)}{\\cos(\\theta_k)}$. The initial condition is $0 < \\theta_0 < \\pi/2$, which implies $\\tan(\\theta_0) > 0$.\n\nThe RQI consists of three steps:\n1.  Compute the Rayleigh quotient: $\\mu_k = \\mu(x_k) = x_k^* A x_k$.\n2.  Solve for the next vector (unnormalized): $y_k = (A - \\mu_k I)^{-1} x_k$.\n3.  Normalize to get the next iterate: $x_{k+1} = \\frac{y_k}{\\|y_k\\|}$.\n\nLet's carry out these steps symbolically for a general step $k$.\n\n**Step 1: Compute the Rayleigh quotient $\\mu_k$.**\nWe have $A x_k = A(\\cos(\\theta_k) v_1 + \\sin(\\theta_k) v_2) = \\cos(\\theta_k) (A v_1) + \\sin(\\theta_k) (A v_2) = \\lambda_1 \\cos(\\theta_k) v_1 + \\lambda_2 \\sin(\\theta_k) v_2$.\nThe Hermitian conjugate of $x_k$ is $x_k^* = \\cos(\\theta_k) v_1^* + \\sin(\\theta_k) v_2^*$.\nNow, we compute $\\mu_k$:\n$$\\mu_k = x_k^* A x_k = (\\cos(\\theta_k) v_1^* + \\sin(\\theta_k) v_2^*)(\\lambda_1 \\cos(\\theta_k) v_1 + \\lambda_2 \\sin(\\theta_k) v_2)$$\nUsing the orthonormality of the eigenvectors ($v_1^* v_1 = 1$, $v_2^* v_2 = 1$, $v_1^* v_2 = 0$, $v_2^* v_1 = 0$), the expression simplifies to:\n$$\\mu_k = \\lambda_1 \\cos^2(\\theta_k) + \\lambda_2 \\sin^2(\\theta_k)$$\n\n**Step 2: Solve for $y_k$.**\nThe vector $y_k$ is obtained by applying the inverse of the shifted matrix $(A - \\mu_k I)$ to $x_k$. The action of this inverse operator is most easily described in the eigenbasis of $A$. For an arbitrary vector $w = c_1 v_1 + c_2 v_2$, we have $(A - \\mu_k I)^{-1} w = \\frac{c_1}{\\lambda_1 - \\mu_k} v_1 + \\frac{c_2}{\\lambda_2 - \\mu_k} v_2$. This is well-defined as long as $\\mu_k$ is not an eigenvalue. If $\\mu_k = \\lambda_1$, then $(\\lambda_1-\\lambda_2)\\sin^2(\\theta_k) = 0$, which for $\\lambda_1 \\neq \\lambda_2$ implies $\\sin(\\theta_k)=0$, so $x_k$ is already the eigenvector $v_1$. The iteration would stop. As we start with $\\theta_0 \\in (0, \\pi/2)$, $x_k$ is not an eigenvector for any finite $k$.\nApplying this to $x_k$:\n$$y_k = (A - \\mu_k I)^{-1} x_k = \\frac{\\cos(\\theta_k)}{\\lambda_1 - \\mu_k} v_1 + \\frac{\\sin(\\theta_k)}{\\lambda_2 - \\mu_k} v_2$$\n\n**Step 3: Determine $\\tan(\\theta_{k+1})$.**\nThe next iterate $x_{k+1}$ is proportional to $y_k$. Therefore, the tangent of the angle $\\theta_{k+1}$ is given by the ratio of the coefficients of $v_2$ and $v_1$ in $y_k$:\n$$\\tan(\\theta_{k+1}) = \\frac{\\sin(\\theta_{k+1})}{\\cos(\\theta_{k+1})} = \\frac{\\text{coeff of } v_2 \\text{ in } y_k}{\\text{coeff of } v_1 \\text{ in } y_k} = \\frac{\\sin(\\theta_k)/(\\lambda_2 - \\mu_k)}{\\cos(\\theta_k)/(\\lambda_1 - \\mu_k)}$$\n$$\\tan(\\theta_{k+1}) = \\tan(\\theta_k) \\left( \\frac{\\lambda_1 - \\mu_k}{\\lambda_2 - \\mu_k} \\right)$$\n\nNow, we substitute the expression for $\\mu_k$ into the fraction:\nThe numerator is:\n$$\\lambda_1 - \\mu_k = \\lambda_1 - (\\lambda_1 \\cos^2(\\theta_k) + \\lambda_2 \\sin^2(\\theta_k)) = \\lambda_1 (1 - \\cos^2(\\theta_k)) - \\lambda_2 \\sin^2(\\theta_k) = \\lambda_1 \\sin^2(\\theta_k) - \\lambda_2 \\sin^2(\\theta_k) = (\\lambda_1 - \\lambda_2)\\sin^2(\\theta_k)$$\nThe denominator is:\n$$\\lambda_2 - \\mu_k = \\lambda_2 - (\\lambda_1 \\cos^2(\\theta_k) + \\lambda_2 \\sin^2(\\theta_k)) = \\lambda_2 (1 - \\sin^2(\\theta_k)) - \\lambda_1 \\cos^2(\\theta_k) = \\lambda_2 \\cos^2(\\theta_k) - \\lambda_1 \\cos^2(\\theta_k) = (\\lambda_2 - \\lambda_1)\\cos^2(\\theta_k)$$\nThe ratio of these two expressions is:\n$$\\frac{\\lambda_1 - \\mu_k}{\\lambda_2 - \\mu_k} = \\frac{(\\lambda_1 - \\lambda_2)\\sin^2(\\theta_k)}{(\\lambda_2 - \\lambda_1)\\cos^2(\\theta_k)} = \\frac{(\\lambda_1 - \\lambda_2)\\sin^2(\\theta_k)}{-(\\lambda_1 - \\lambda_2)\\cos^2(\\theta_k)} = -\\frac{\\sin^2(\\theta_k)}{\\cos^2(\\theta_k)} = -\\tan^2(\\theta_k)$$\n\nSubstituting this back into the equation for $\\tan(\\theta_{k+1})$ gives the recurrence relation:\n$$\\tan(\\theta_{k+1}) = \\tan(\\theta_k) (-\\tan^2(\\theta_k)) = -\\tan^3(\\theta_k)$$\nThis exact relationship for the $2 \\times 2$ case demonstrates the cubic convergence of RQI. The magnitude of the tangent of the angle, which represents the error in the eigenvector approximation, is cubed at each iteration: $|\\tan(\\theta_{k+1})|=|\\tan(\\theta_k)|^3$.\n\nNow we apply this relation for two iterations, starting from $x_0$.\n\n**First Iteration ($k=0$):**\nUsing the recurrence relation for $k=0$, we find $\\tan(\\theta_1)$:\n$$\\tan(\\theta_1) = -\\tan^3(\\theta_0)$$\n\n**Second Iteration ($k=1$):**\nNow we apply the recurrence relation for $k=1$ to find $\\tan(\\theta_2)$:\n$$\\tan(\\theta_2) = -\\tan^3(\\theta_1)$$\nSubstitute the expression for $\\tan(\\theta_1)$ into this equation:\n$$\\tan(\\theta_2) = -(\\tan(\\theta_1))^3 = -\\left( -\\tan^3(\\theta_0) \\right)^3$$\n$$\\tan(\\theta_2) = - \\left( (-1)^3 (\\tan^3(\\theta_0))^3 \\right) = - \\left( -1 \\cdot \\tan^{(3 \\times 3)}(\\theta_0) \\right) = - \\left( -\\tan^9(\\theta_0) \\right)$$\n$$\\tan(\\theta_2) = \\tan^9(\\theta_0)$$\n\nThis is the final expression for $\\tan(\\theta_2)$ as a function of $\\tan(\\theta_0)$.\nThe calculation confirms the exceptionally fast, cubic convergence of the Rayleigh quotient iteration. The error term $\\tan(\\theta_k)$ is raised to the power of $3^k$ (up to a sign) after $k$ steps. After two steps, the exponent is $3^2=9$.",
            "answer": "$$\\boxed{\\tan^{9}(\\theta_{0})}$$"
        },
        {
            "introduction": "While the standard eigenvalue problem is foundational, many challenges in science and engineering, from vibrational analysis to quantum mechanics, manifest as the generalized eigenvalue problem $A \\boldsymbol{x} = \\lambda B \\boldsymbol{x}$. This practice extends the Rayleigh Quotient Iteration to solve this more general and highly practical problem . You will implement the algorithm from first principles, using the generalized Rayleigh quotient and the appropriate normalization, transforming your theoretical knowledge into a robust and versatile computational tool.",
            "id": "3265587",
            "problem": "Implement a complete, runnable program that computes approximate generalized eigenvalues for the symmetric generalized eigenvalue problem defined by $A \\boldsymbol{x} = \\lambda B \\boldsymbol{x}$ using Rayleigh quotient iteration derived from first principles. The pair $(A,B)$ must be symmetric with $B$ symmetric positive definite (SPD). Use the generalized Rayleigh quotient $R(\\boldsymbol{x}) = \\dfrac{\\boldsymbol{x}^{\\mathsf{T}} A \\boldsymbol{x}}{\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x}}$ as the scalar that drives the iteration. The algorithm must be derived from the stationarity condition of $R(\\boldsymbol{x})$ on the $B$-unit sphere and not from any pre-supplied shortcut formulas.\n\nFundamental basis that must be used:\n- The generalized eigenvalue problem $A \\boldsymbol{x} = \\lambda B \\boldsymbol{x}$ for symmetric $A$ and symmetric positive definite $B$ has real eigenvalues and a $B$-orthonormal basis of eigenvectors.\n- The generalized Rayleigh quotient $R(\\boldsymbol{x}) = \\dfrac{\\boldsymbol{x}^{\\mathsf{T}} A \\boldsymbol{x}}{\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x}}$ is stationary at generalized eigenvectors when constrained to the $B$-unit sphere $\\{\\boldsymbol{x} \\in \\mathbb{R}^{n} : \\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x} = 1\\}$.\n- Newtonâ€™s method applied to first-order optimality conditions on this constraint manifold yields a locally cubically convergent iteration for simple eigenpairs.\n\nYour program must:\n- Implement Rayleigh quotient iteration based only on the above foundational definitions and facts.\n- Normalize the iterate in the $B$-norm, where $\\lVert \\boldsymbol{x} \\rVert_{B} = \\sqrt{\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x}}$.\n- Terminate when the $2$-norm residual $\\lVert A \\boldsymbol{x} - \\mu B \\boldsymbol{x} \\rVert_{2}$ is below a tolerance $\\tau = 10^{-12}$ or after a maximum of $50$ iterations, whichever happens first. Here $\\mu$ denotes the current generalized Rayleigh quotient.\n\nTest suite:\nProvide results for the following four test cases. In each case, $A$, $B$, and the initial vector $\\boldsymbol{x}_{0}$ are given explicitly. All matrices are symmetric, and $B$ is symmetric positive definite.\n\n- Case $1$ (scalar boundary case):\n  - $A = \\begin{bmatrix} 7 \\end{bmatrix}$, $B = \\begin{bmatrix} 2 \\end{bmatrix}$, $\\boldsymbol{x}_{0} = \\begin{bmatrix} 1 \\end{bmatrix}$.\n\n- Case $2$ (diagonal generalized pair):\n  - $A = \\mathrm{diag}(2,3,5)$, $B = \\mathrm{diag}(1,4,2)$, $\\boldsymbol{x}_{0} = \\begin{bmatrix} 0.05 \\\\ 0.998 \\\\ 0.05 \\end{bmatrix}$.\n\n- Case $3$ (standard eigenproblem as a special case where $B = I$):\n  - $A = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$, $B = I_{3}$, $\\boldsymbol{x}_{0} = \\begin{bmatrix} 1 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$.\n\n- Case $4$ (fully generalized, symmetric off-diagonal pair):\n  - $A = \\begin{bmatrix} 5 & 2 & 0 \\\\ 2 & 4 & 1 \\\\ 0 & 1 & 3 \\end{bmatrix}$, $B = \\begin{bmatrix} 3 & 1 & 0 \\\\ 1 & 2 & 0.5 \\\\ 0 & 0.5 & 1.5 \\end{bmatrix}$, $\\boldsymbol{x}_{0} = \\begin{bmatrix} 0.3 \\\\ -0.4 \\\\ 0.85 \\end{bmatrix}$.\n\nComputational and output requirements:\n- For each case, run Rayleigh quotient iteration starting from the provided $\\boldsymbol{x}_{0}$, using tolerance $\\tau = 10^{-12}$ and a maximum of $50$ iterations.\n- For each case, report the final approximate generalized eigenvalue $\\widehat{\\lambda}$ computed from the last iterate via the generalized Rayleigh quotient.\n- Express each reported $\\widehat{\\lambda}$ as a floating-point number rounded to $12$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$; for example, the printed string must have the form $[\\widehat{\\lambda}_{1},\\widehat{\\lambda}_{2},\\widehat{\\lambda}_{3},\\widehat{\\lambda}_{4}]$.\n\nNo external input is allowed. All quantities are dimensionless; do not include any physical units. Angles are not used. Percentages are not used. The only accepted data types for the final outputs are floating-point numbers printed as specified.",
            "solution": "The problem requires the implementation of the Rayleigh quotient iteration for the symmetric generalized eigenvalue problem $A \\boldsymbol{x} = \\lambda B \\boldsymbol{x}$, where $A$ is a symmetric matrix and $B$ is a symmetric positive definite (SPD) matrix. The implementation must be derived from first principles involving the stationarity of the generalized Rayleigh quotient on the $B$-unit sphere.\n\n### 1. Problem Validation\nThe problem statement has been validated and is found to be sound, well-posed, and objective.\n*   **Givens**: The problem provides all necessary information: the form of the generalized eigenvalue problem, properties of matrices $A$ and $B$, the definition of the generalized Rayleigh quotient $R(\\boldsymbol{x})$, the foundational principles for the algorithm's derivation, specific algorithmic requirements (normalization, termination criteria), and a complete suite of test cases with explicit matrices, initial vectors, and output formatting rules.\n*   **Scientific Grounding**: The problem is rooted in fundamental concepts of numerical linear algebra. The generalized symmetric eigenvalue problem, Rayleigh quotient iteration, and Newton's method are all standard and well-understood topics. The condition that $B$ is SPD guarantees real eigenvalues and a $B$-orthonormal basis of eigenvectors, ensuring the problem is well-behaved.\n*   **Well-Posedness**: The problem is well-posed. Rayleigh quotient iteration exhibits local cubic convergence for simple eigenvalues, so given a suitable initial guess, it is expected to converge quickly to a unique eigenpair. The termination conditions are unambiguous. The provided test data is numerically valid; in particular, all specified $B$ matrices are confirmed to be symmetric positive definite.\n*   **Conclusion**: The problem is valid and can be solved as stated.\n\n### 2. Derivation of the Generalized Rayleigh Quotient Iteration\n\nThe core of the problem is to find a scalar $\\lambda$ and a non-zero vector $\\boldsymbol{x}$ satisfying $A \\boldsymbol{x} = \\lambda B \\boldsymbol{x}$. This can be formulated as a constrained optimization problem. The generalized Rayleigh quotient is defined as:\n$$ R(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}^{\\mathsf{T}} A \\boldsymbol{x}}{\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x}} $$\nThe generalized eigenvectors are the stationary points of $R(\\boldsymbol{x})$. We can find these by seeking the stationary points of the numerator, $\\boldsymbol{x}^{\\mathsf{T}} A \\boldsymbol{x}$, subject to the constraint that the denominator is constant, e.g., $\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x} = 1$. This constraint defines the \"$B$-unit sphere\".\n\nWe use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ for this constrained optimization problem is:\n$$ \\mathcal{L}(\\boldsymbol{x}, \\lambda) = \\boldsymbol{x}^{\\mathsf{T}} A \\boldsymbol{x} - \\lambda (\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x} - 1) $$\nFor a point $\\boldsymbol{x}$ to be a stationary point, the gradient of the Lagrangian with respect to $\\boldsymbol{x}$ must be the zero vector:\n$$ \\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda) = \\mathbf{0} $$\nGiven that $A$ and $B$ are symmetric, the gradients are $\\nabla_{\\boldsymbol{x}} (\\boldsymbol{x}^{\\mathsf{T}} A \\boldsymbol{x}) = 2A\\boldsymbol{x}$ and $\\nabla_{\\boldsymbol{x}} (\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x}) = 2B\\boldsymbol{x}$. Thus, the stationarity condition is:\n$$ 2A\\boldsymbol{x} - \\lambda(2B\\boldsymbol{x}) = \\mathbf{0} \\implies A\\boldsymbol{x} = \\lambda B\\boldsymbol{x} $$\nThis is precisely the generalized eigenvalue problem. The Lagrange multiplier $\\lambda$ is the generalized eigenvalue. By left-multiplying by $\\boldsymbol{x}^{\\mathsf{T}}$, we see that at an eigenvector $\\boldsymbol{x}$, $\\boldsymbol{x}^{\\mathsf{T}}A\\boldsymbol{x} = \\lambda \\boldsymbol{x}^{\\mathsf{T}}B\\boldsymbol{x}$, confirming that $\\lambda = R(\\boldsymbol{x})$.\n\nThe Rayleigh quotient iteration algorithm can be derived by applying Newton's method to find a root of the system of nonlinear equations defined by the stationarity and constraint conditions:\n$$ F(\\boldsymbol{x}, \\lambda) = \\begin{bmatrix} A\\boldsymbol{x} - \\lambda B\\boldsymbol{x} \\\\ \\frac{1}{2}(\\boldsymbol{x}^{\\mathsf{T}} B \\boldsymbol{x} - 1) \\end{bmatrix} = \\begin{bmatrix} \\mathbf{0} \\\\ 0 \\end{bmatrix} $$\nWhile a direct application and simplification of Newton's method to this $(n+1) \\times (n+1)$ system is possible, it leads to the same iterative scheme that arises from the more intuitive \"shift-and-invert\" viewpoint. This viewpoint combines two ideas:\n1.  The Rayleigh quotient $\\mu_k = R(\\boldsymbol{x}_k)$ provides a highly accurate estimate of the eigenvalue corresponding to the eigenvector approximation $\\boldsymbol{x}_k$.\n2.  Inverse iteration with a shift $\\mu_k$ rapidly converges to the eigenvector whose eigenvalue is closest to $\\mu_k$.\n\nFor the generalized problem $A\\boldsymbol{x} = \\lambda B\\boldsymbol{x}$, the appropriate \"shift-and-invert\" operator is $(A - \\mu B)^{-1}B$. Applying this operator to the current vector approximation $\\boldsymbol{x}_k$ gives the direction for the next approximation.\n\nThis leads to the following algorithm.\n\n### 3. Algorithmic Steps\n\nLet $\\boldsymbol{x}_k$ be the approximate eigenvector at iteration $k$.\n1.  **Initialize**: Start with a given vector $\\boldsymbol{x}_0$. Normalize it with respect to the $B$-norm: $\\boldsymbol{x}_0 \\leftarrow \\boldsymbol{x}_0 / \\sqrt{\\boldsymbol{x}_0^{\\mathsf{T}} B \\boldsymbol{x}_0}$.\n2.  **Iterate**: For $k = 0, 1, 2, \\dots$ up to a maximum number of iterations:\n    a. **Compute Eigenvalue Estimate**: Calculate the generalized Rayleigh quotient. Since $\\boldsymbol{x}_k$ is $B$-normalized, $\\boldsymbol{x}_k^{\\mathsf{T}} B \\boldsymbol{x}_k=1$, so the quotient simplifies to:\n       $$ \\mu_k = \\boldsymbol{x}_k^{\\mathsf{T}} A \\boldsymbol{x}_k $$\n    b. **Check for Convergence**: Compute the residual $\\boldsymbol{r}_k = A\\boldsymbol{x}_k - \\mu_k B\\boldsymbol{x}_k$. If its Euclidean norm $\\lVert\\boldsymbol{r}_k\\rVert_2$ is below a specified tolerance $\\tau$, the iteration has converged. The final eigenpair approximation is $(\\mu_k, \\boldsymbol{x}_k)$.\n    c. **Solve for Next Vector**: Find the next (unnormalized) vector $\\boldsymbol{z}_{k+1}$ by solving the shifted linear system:\n       $$ (A - \\mu_k B) \\boldsymbol{z}_{k+1} = B \\boldsymbol{x}_k $$\n       The matrix $A - \\mu_k B$ becomes nearly singular as $\\mu_k$ approaches an eigenvalue, which is the source of the method's rapid (cubic) convergence.\n    d. **Normalize**: Compute the $B$-norm of the new vector, $\\lVert \\boldsymbol{z}_{k+1} \\rVert_B = \\sqrt{\\boldsymbol{z}_{k+1}^{\\mathsf{T}} B \\boldsymbol{z}_{k+1}}$, and normalize it to obtain the next iterate:\n       $$ \\boldsymbol{x}_{k+1} = \\frac{\\boldsymbol{z}_{k+1}}{\\lVert \\boldsymbol{z}_{k+1} \\rVert_B} $$\n3.  **Terminate**: If the loop finishes due to reaching the maximum number of iterations, the last computed $(\\mu_k, \\boldsymbol{x}_k)$ is returned as the final approximation.\n\nThis algorithm is implemented below to solve the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generalized_rayleigh_quotient_iteration(A, B, x0, tol=1e-12, max_iter=50):\n    \"\"\"\n    Computes an eigenpair of the generalized eigenvalue problem Ax = lambda*Bx.\n\n    Args:\n        A (np.ndarray): A symmetric matrix.\n        B (np.ndarray): A symmetric positive definite matrix.\n        x0 (np.ndarray): The initial guess for the eigenvector.\n        tol (float): The tolerance for the residual norm for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The approximate eigenvalue.\n    \"\"\"\n    x = x0.copy().astype(np.float64)\n\n    # Initial B-normalization of the vector x0\n    try:\n        x_T_B_x = (x.T @ B @ x)[0, 0]\n        norm_B = np.sqrt(x_T_B_x)\n        if norm_B == 0:\n            raise ValueError(\"Initial vector x0 has zero B-norm.\")\n        x = x / norm_B\n    except IndexError: # Handles 1x1 case where result is scalar\n        x_T_B_x = x.T @ B @ x\n        norm_B = np.sqrt(x_T_B_x)\n        if norm_B == 0:\n            raise ValueError(\"Initial vector x0 has zero B-norm.\")\n        x = x / norm_B\n\n\n    mu = 0.0\n    for _ in range(max_iter):\n        # Calculate the generalized Rayleigh quotient.\n        # Since x is B-normalized, x.T @ B @ x is 1.\n        mu_val = (x.T @ A @ x)\n        mu = mu_val[0, 0] if isinstance(mu_val, np.ndarray) and mu_val.ndim > 0 else mu_val\n\n        # Compute the residual vector and check for convergence\n        residual = A @ x - mu * (B @ x)\n        residual_norm = np.linalg.norm(residual)\n\n        if residual_norm < tol:\n            return mu\n\n        # Solve the shifted linear system: (A - mu*B)z = Bx\n        shift_matrix = A - mu * B\n        rhs = B @ x\n        \n        try:\n            # Use np.linalg.solve for numerical stability\n            z = np.linalg.solve(shift_matrix, rhs)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, mu is an excellent approximation of\n            # an eigenvalue. We can consider this as converged.\n            return mu\n        \n        # Normalize the new vector z in the B-norm\n        try:\n            z_T_B_z = (z.T @ B @ z)[0, 0]\n            norm_B_z = np.sqrt(z_T_B_z)\n        except IndexError: # Handles 1x1 case\n            z_T_B_z = z.T @ B @ z\n            norm_B_z = np.sqrt(z_T_B_z)\n\n        if norm_B_z == 0:\n            # This is unlikely with a non-singular B, but as a safeguard:\n            # it might indicate convergence to a trivial solution,\n            # so we stop and return the current best estimate.\n            return mu\n            \n        x = z / norm_B_z\n\n    # If max_iter is reached, compute the final Rayleigh quotient and return\n    mu_val = (x.T @ A @ x)\n    mu = mu_val[0, 0] if isinstance(mu_val, np.ndarray) and mu_val.ndim > 0 else mu_val\n    return mu\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the generalized Rayleigh quotient iteration.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[7.0]]),\n            \"B\": np.array([[2.0]]),\n            \"x0\": np.array([1.0]),\n        },\n        {\n            \"A\": np.diag([2.0, 3.0, 5.0]),\n            \"B\": np.diag([1.0, 4.0, 2.0]),\n            \"x0\": np.array([0.05, 0.998, 0.05]),\n        },\n        {\n            \"A\": np.array([[4.0, 1.0, 0.0], [1.0, 3.0, 1.0], [0.0, 1.0, 2.0]]),\n            \"B\": np.identity(3),\n            \"x0\": np.array([1.0, 0.2, -0.1]),\n        },\n        {\n            \"A\": np.array([[5.0, 2.0, 0.0], [2.0, 4.0, 1.0], [0.0, 1.0, 3.0]]),\n            \"B\": np.array([[3.0, 1.0, 0.0], [1.0, 2.0, 0.5], [0.0, 0.5, 1.5]]),\n            \"x0\": np.array([0.3, -0.4, 0.85]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        B = case[\"B\"]\n        x0 = case[\"x0\"]\n        \n        # Ensure x0 is a column vector for consistent matrix operations\n        x0_col = x0.reshape(-1, 1) if x0.ndim == 1 else x0\n        \n        lambda_approx = generalized_rayleigh_quotient_iteration(A, B, x0_col)\n        \n        # Format the result to 12 decimal places as a string\n        results.append(f\"{lambda_approx:.12f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The remarkable cubic convergence of RQI is a local property, meaning it is only guaranteed for an initial guess \"sufficiently close\" to a true eigenvector. This advanced practice invites you to explore the boundaries of this behavior by conducting a numerical experiment to map the algorithm's basin of attraction . By systematically varying the initial vector and observing the convergence rate, you will gain a deeper, more critical appreciation for how factors like eigenvalue separation define the practical performance and limitations of the iteration.",
            "id": "3572063",
            "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with simple eigenvalues and an orthonormal eigenbasis. The Rayleigh quotient iteration (RQI) is a Newton-type method applied to the Rayleigh quotient on the unit sphere $S^{n-1}$, and under appropriate conditions exhibits local cubic convergence to an eigenpair. Let $v$ be a true unit eigenvector of $A$, and let the initial iterate be constructed on the great circle spanned by $v$ and an orthogonal direction $w$ as $x_0(\\theta_0) = \\cos(\\theta_0) v + \\sin(\\theta_0) w$, with $\\theta_0 \\in [0, \\pi/2]$ in radians. Define the angle error at iteration $k$ as $e_k = \\arccos(|x_k^\\top v|)$ in radians, where $x_k$ is the normalized iterate on $S^{n-1}$. The local order of convergence $p$ can be estimated from three consecutive errors via an empirical model $e_{k+1} \\approx C e_k^p$ by eliminating the unknown constant $C$ to form\n$$\np_k \\approx \\frac{\\log(e_{k+1}) - \\log(e_k)}{\\log(e_k) - \\log(e_{k-1})}\n$$\nwhenever $e_{k-1}, e_k, e_{k+1} > 0$. An initial angle $\\theta_0$ is said to lie in the cubic convergence region for the target eigenvector $v$ if the iteration converges to $v$ and there exist at least two consecutive indices $k$ for which $p_k \\in [3 - \\delta, 3 + \\delta]$ with $e_{k-1}, e_k, e_{k+1} < \\varepsilon$, where $\\delta > 0$ and $\\varepsilon > 0$ are prescribed tolerances. This experiment connects the observed region of cubic convergence to the Newton basin structure on $S^{n-1}$.\n\nImplement a program that:\n- Constructs the following test matrices (deterministically) and selects target eigenvectors $v$:\n  1. Test Case 1 (dimension $n=3$): Let $M^{(1)} \\in \\mathbb{R}^{3 \\times 3}$ be defined by $M^{(1)}_{ij} = \\sin(i + 2j) + 0.1 \\cos(3i - j)$ for indices $i,j \\in \\{1,2,3\\}$. Compute an orthonormal matrix $Q^{(1)}$ via a $QR$ decomposition of $M^{(1)}$. Define $A^{(1)} = Q^{(1)} \\operatorname{diag}(1,3,10) \\left(Q^{(1)}\\right)^\\top$, and select the target eigenvector $v^{(1)}$ as the column of $Q^{(1)}$ corresponding to the eigenvalue $3$ (that is, the second column). Choose $w^{(1)}$ as a unit vector orthogonal to $v^{(1)}$, specifically the next column of $Q^{(1)}$ in cyclic order, i.e., the third column.\n  2. Test Case 2 (dimension $n=3$): Reuse $Q^{(1)}$ to define $A^{(2)} = Q^{(1)} \\operatorname{diag}(1, 1.001, 2) \\left(Q^{(1)}\\right)^\\top$, and select $v^{(2)}$ as the column of $Q^{(1)}$ corresponding to the eigenvalue $1.001$ (that is, the second column). Choose $w^{(2)}$ as the third column of $Q^{(1)}$.\n  3. Test Case 3 (dimension $n=5$): Let $M^{(3)} \\in \\mathbb{R}^{5 \\times 5}$ be defined by $M^{(3)}_{ij} = \\sin(0.3 i + 0.7 j) + 0.05 \\cos(i - 2 j)$ for indices $i,j \\in \\{1,2,3,4,5\\}$. Compute an orthonormal matrix $Q^{(3)}$ via a $QR$ decomposition of $M^{(3)}$. Define $A^{(3)} = Q^{(3)} \\operatorname{diag}(0.5, 1.4, 2.0, 4.5, 9.0) \\left(Q^{(3)}\\right)^\\top$, and select $v^{(3)}$ as the column of $Q^{(3)}$ corresponding to the eigenvalue $2.0$ (that is, the third column). Choose $w^{(3)}$ as the fourth column of $Q^{(3)}$.\n\n- For each test case, form a grid of initial angles $\\theta_0 \\in [0, \\pi/2]$ in radians sampled at $61$ equally spaced points, construct $x_0(\\theta_0)$ on $S^{n-1}$, and run Rayleigh quotient iteration defined by:\n  1. Initialize $x_0$ as given. For $k = 0,1,2,\\dots$ until a maximum number of iterations, compute the Rayleigh quotient $\\mu_k = \\frac{x_k^\\top A x_k}{x_k^\\top x_k}$.\n  2. Solve $(A - \\mu_k I) y_k = x_k$ for $y_k$ (use a stable linear solver), and set $x_{k+1} = \\frac{y_k}{\\|y_k\\|_2}$.\n  3. Record the angle errors $e_k = \\arccos(|x_k^\\top v|)$ in radians.\n  4. Stop when either the sequence has converged in angle (for example $e_k < 10^{-14}$) or a maximum of $50$ iterations is reached.\n\n- Using the recorded errors $\\{e_k\\}$ for each $\\theta_0$, estimate local orders $\\{p_k\\}$ where defined, and declare $\\theta_0$ to be within the cubic convergence region for $v$ if the iteration converges toward $v$ and the criterion above is met with tolerances $\\delta = 0.3$ and $\\varepsilon = 10^{-6}$. If $e_0 < \\varepsilon$, treat $\\theta_0$ as trivially cubic.\n\n- For each test case, compute the fraction (as a decimal number) of angles in the grid that satisfy the cubic criterion. Angles are measured in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each fraction rounded to three decimal places, in the order of the test cases described above. For example: \"[0.850,0.300,0.000]\".",
            "solution": "The Rayleigh quotient iteration (RQI) is derived from applying Newton's method to the stationary conditions of the Rayleigh quotient on the unit sphere $S^{n-1}$. For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with simple eigenvalues $\\lambda_1, \\dots, \\lambda_n$ and an orthonormal eigenbasis $Q = [v_1 \\, v_2 \\, \\dots \\, v_n]$, the Rayleigh quotient $R(x) = \\frac{x^\\top A x}{x^\\top x}$ attains extrema at eigenvectors, with values equal to corresponding eigenvalues. Newton's method applied to the constrained optimization of $R(x)$ on $S^{n-1}$ leads to an iteration that at each step solves a shifted linear system with shift equal to the current Rayleigh quotient. The practical algorithm is:\n1. Normalize $x_k$ to lie on $S^{n-1}$.\n2. Compute the Rayleigh quotient $\\mu_k = \\frac{x_k^\\top A x_k}{x_k^\\top x_k}$.\n3. Solve $(A - \\mu_k I) y_k = x_k$ and set $x_{k+1} = \\frac{y_k}{\\|y_k\\|_2}$.\n\nFor real symmetric matrices with simple eigenvalues, it is well known and can be rigorously proved that if $x_k$ is sufficiently close to an eigenvector $v$, then the iteration converges with local cubic order to $v$ and the corresponding eigenvalue. The convergence is local and depends critically on the initial vector $x_0$ lying in the Newton basin of attraction for the target eigenpair on the manifold $S^{n-1}$.\n\nTo map how the initial angle influences entry into the cubic region, we parameterize the initial vector $x_0(\\theta_0)$ along a geodesic on $S^{n-1}$ through the target eigenvector $v$ in the plane spanned by $v$ and an orthogonal unit vector $w$. Explicitly, we set $x_0(\\theta_0) = \\cos(\\theta_0) v + \\sin(\\theta_0) w$, which traces a great circle arc from $v$ at $\\theta_0 = 0$ to an orthogonal direction at $\\theta_0 = \\frac{\\pi}{2}$.\n\nWe quantify closeness to the target eigenvector $v$ by the angle error $e_k = \\arccos(|x_k^\\top v|)$, ensuring $e_k \\in [0, \\pi/2]$. Under local cubic convergence, the error model $e_{k+1} \\approx C e_k^p$ with $p \\approx 3$ and some constant $C > 0$ holds when errors are sufficiently small. Eliminating the unknown constant $C$ using three consecutive errors yields an estimator for the local order of convergence:\n$$\np_k \\approx \\frac{\\log(e_{k+1}) - \\log(e_k)}{\\log(e_k) - \\log(e_{k-1})},\n$$\nvalid when $e_{k-1}, e_k, e_{k+1} > 0$ and small enough that logarithms are numerically stable. This estimator is standard in numerical analysis for empirically determining the local order without requiring knowledge of the asymptotic constant.\n\nTo declare that an initial angle $\\theta_0$ lies in the cubic convergence region for a target eigenvector $v$, we require both basin membership (the iteration tends toward $v$) and detection of cubic behavior. Basin membership is implicitly checked by the errors $e_k$ decreasing toward $0$ and the final iterate being well aligned with $v$ (equivalently, $|x_k^\\top v|$ close to $1$). For cubic detection, we impose a practical criterion: there must exist at least two consecutive indices $k$ where $p_k \\in [3 - \\delta, 3 + \\delta]$ and simultaneously $e_{k-1}, e_k, e_{k+1} < \\varepsilon$, with tolerances chosen as $\\delta = 0.3$ and $\\varepsilon = 10^{-6}$. These thresholds focus on the local regime where the asymptotic model applies and provide robustness against transient behavior in early iterations. If $\\theta_0$ is trivially at the solution ($e_0 < \\varepsilon$), we count it as cubic as it lies in the Newton basin and needs no iteration.\n\nThe test suite is designed to probe how eigenvalue separation and dimension affect the Newton basin on $S^{n-1}$:\n- Test Case 1 uses $A^{(1)} = Q^{(1)} \\operatorname{diag}(1,3,10) \\left(Q^{(1)}\\right)^\\top$ with $Q^{(1)}$ obtained from a $QR$ decomposition of the deterministic matrix $M^{(1)}_{ij} = \\sin(i + 2j) + 0.1 \\cos(3i - j)$ for $i,j \\in \\{1,2,3\\}$. The target eigenvector is $v^{(1)} = Q^{(1)}_{:,2}$ and $w^{(1)} = Q^{(1)}_{:,3}$. The clear separation of eigenvalues yields a comparatively large basin for the target eigenpair, so we expect a substantial fraction of angles to enter the cubic regime after some iterations.\n- Test Case 2 uses $A^{(2)} = Q^{(1)} \\operatorname{diag}(1, 1.001, 2) \\left(Q^{(1)}\\right)^\\top$ with the same $Q^{(1)}$ and target $v^{(2)} = Q^{(1)}_{:,2}$, $w^{(2)} = Q^{(1)}_{:,3}$. The clustering of $1$ and $1.001$ shrinks and distorts the Newton basin associated with the second eigenpair, so fewer initial angles along the chosen great circle should exhibit cubic convergence to $v^{(2)}$.\n- Test Case 3 uses $A^{(3)} = Q^{(3)} \\operatorname{diag}(0.5, 1.4, 2.0, 4.5, 9.0) \\left(Q^{(3)}\\right)^\\top$ with $Q^{(3)}$ from a $QR$ decomposition of $M^{(3)}_{ij} = \\sin(0.3 i + 0.7 j) + 0.05 \\cos(i - 2 j)$ for $i,j \\in \\{1,2,3,4,5\\}$. The target eigenvector is $v^{(3)} = Q^{(3)}_{:,3}$ and $w^{(3)} = Q^{(3)}_{:,4}$. This higher-dimensional case examines how the Newton basin behaves on $S^{4}$, with eigenvalues moderately separated; we expect an intermediate fraction of angles entering cubic convergence compared to Test Cases 1 and 2.\n\nAlgorithmic details:\n- The initial angle grid is set as $61$ equally spaced points in $[0, \\pi/2]$ radians.\n- At each iteration, the Rayleigh quotient $\\mu_k$ is computed, followed by solving $(A - \\mu_k I) y_k = x_k$. A robust linear solver is used (with a least-squares fallback if necessary) to handle near-singularity of the shifted system, which can occur near an eigenvalue. The iterate is re-normalized: $x_{k+1} = y_k / \\|y_k\\|_2$.\n- The angle error sequence $\\{e_k\\}$ is recorded. The local orders $\\{p_k\\}$ are computed using the logarithmic estimator where defined. The cubic criterion is checked with the specified tolerances, and $\\theta_0$ is counted as cubic if and only if the iteration tends to $v$ and the criterion is met. If $e_0 < \\varepsilon$, $\\theta_0$ is counted as cubic without needing the estimator.\n\nFor each test case, we compute the fraction of the angle grid that satisfies the cubic criterion. These fractions, rounded to three decimal places, are printed as a single line in the format \"[f1,f2,f3]\". This provides a quantitative mapping of the Newton basin structure along the chosen great circle on $S^{n-1}$ by indicating how much of the sampled arc lies in the basin where cubic convergence is observed.",
            "answer": "```python\nimport numpy as np\n\ndef qr_orthonormal(M: np.ndarray) -> np.ndarray:\n    # Use QR decomposition to obtain an orthonormal matrix Q\n    Q, R = np.linalg.qr(M)\n    # Ensure deterministic sign convention: make diagonal of R non-negative\n    d = np.sign(np.diag(R))\n    d[d == 0] = 1.0\n    Q = Q * d\n    return Q\n\ndef build_M1():\n    M = np.zeros((3, 3), dtype=float)\n    for i in range(1, 4):\n        for j in range(1, 4):\n            M[i-1, j-1] = np.sin(i + 2*j) + 0.1 * np.cos(3*i - j)\n    return M\n\ndef build_M3():\n    M = np.zeros((5, 5), dtype=float)\n    for i in range(1, 6):\n        for j in range(1, 6):\n            M[i-1, j-1] = np.sin(0.3 * i + 0.7 * j) + 0.05 * np.cos(i - 2*j)\n    return M\n\ndef rqi_errors(A: np.ndarray, v: np.ndarray, x0: np.ndarray, max_iter: int = 50) -> list:\n    # Run Rayleigh quotient iteration and return angle errors w.r.t. v\n    x = x0 / np.linalg.norm(x0)\n    errors = []\n    for k in range(max_iter):\n        # Angle error e_k = arccos(|x_k^T v|)\n        dot = float(np.clip(np.abs(np.dot(x, v)), 0.0, 1.0))\n        e_k = float(np.arccos(dot))\n        errors.append(e_k)\n        if e_k < 1e-14:\n            break\n        mu = float(np.dot(x, A @ x) / np.dot(x, x))\n        B = A - mu * np.eye(A.shape[0])\n        try:\n            y = np.linalg.solve(B, x)\n        except np.linalg.LinAlgError:\n            # Fallback to least squares if singular or ill-conditioned\n            y = np.linalg.lstsq(B, x, rcond=None)[0]\n        ynorm = np.linalg.norm(y)\n        if ynorm == 0.0 or not np.isfinite(ynorm):\n            # Bail out if numerical failure\n            break\n        x = y / ynorm\n    return errors\n\ndef estimate_orders(errors: list) -> list:\n    # Compute local order estimates p_k using three consecutive errors\n    p_est = []\n    # Use only strictly positive errors for logs\n    for k in range(2, len(errors)):\n        e_km1 = errors[k-1]\n        e_km2 = errors[k-2]\n        e_k = errors[k]\n        if e_km2 > 0.0 and e_km1 > 0.0 and e_k > 0.0:\n            num = np.log(e_k) - np.log(e_km1)\n            den = np.log(e_km1) - np.log(e_km2)\n            if den != 0.0 and np.isfinite(num) and np.isfinite(den):\n                p = num / den\n                p_est.append(p)\n            else:\n                p_est.append(np.nan)\n        else:\n            p_est.append(np.nan)\n    return p_est\n\ndef is_cubic(errors: list, delta: float = 0.3, eps: float = 1e-6, min_consecutive: int = 2) -> bool:\n    # If trivially at the solution, count as cubic\n    if len(errors) > 0 and errors[0] < eps:\n        return True\n    p_est = estimate_orders(errors)\n    # We need at least indices k with e_{k-1}, e_k, e_{k+1} < eps and p_k near 3\n    valid = []\n    # p_est index corresponds to k in [2, len(errors)-1]\n    for idx, p in enumerate(p_est):\n        k = idx + 2\n        if k < len(errors):\n            e_km2 = errors[k-2]\n            e_km1 = errors[k-1]\n            e_k = errors[k]\n            if e_km2 < eps and e_km1 < eps and e_k < eps and np.isfinite(p) and abs(p - 3.0) <= delta:\n                valid.append(k)\n    # Check for at least min_consecutive consecutive k indices\n    if len(valid) == 0:\n        return False\n    # Count consecutive indices\n    count = 1\n    for i in range(1, len(valid)):\n        if valid[i] == valid[i-1] + 1:\n            count += 1\n            if count >= min_consecutive:\n                return True\n        else:\n            count = 1\n    return False\n\ndef run_case(A: np.ndarray, v: np.ndarray, w: np.ndarray, thetas: np.ndarray) -> float:\n    # Compute fraction of angles with cubic convergence to v\n    n_cubic = 0\n    for theta in thetas:\n        x0 = np.cos(theta) * v + np.sin(theta) * w\n        x0 /= np.linalg.norm(x0)\n        errors = rqi_errors(A, v, x0, max_iter=50)\n        if is_cubic(errors, delta=0.3, eps=1e-6, min_consecutive=2):\n            n_cubic += 1\n    return n_cubic / len(thetas)\n\ndef solve():\n    # Build Test Case 1\n    M1 = build_M1()\n    Q1 = qr_orthonormal(M1)\n    A1 = Q1 @ np.diag([1.0, 3.0, 10.0]) @ Q1.T\n    v1 = Q1[:, 1]  # eigenvector for eigenvalue 3\n    w1 = Q1[:, 2]  # orthogonal direction\n\n    # Build Test Case 2 (reuse Q1)\n    A2 = Q1 @ np.diag([1.0, 1.001, 2.0]) @ Q1.T\n    v2 = Q1[:, 1]\n    w2 = Q1[:, 2]\n\n    # Build Test Case 3\n    M3 = build_M3()\n    Q3 = qr_orthonormal(M3)\n    A3 = Q3 @ np.diag([0.5, 1.4, 2.0, 4.5, 9.0]) @ Q3.T\n    v3 = Q3[:, 2]  # eigenvector for eigenvalue 2.0\n    w3 = Q3[:, 3]  # orthogonal direction\n\n    # Angle grid in radians\n    thetas = np.linspace(0.0, np.pi / 2.0, 61)\n\n    # Run cases\n    frac1 = run_case(A1, v1, w1, thetas)\n    frac2 = run_case(A2, v2, w2, thetas)\n    frac3 = run_case(A3, v3, w3, thetas)\n\n    results = [f\"{frac1:.3f}\", f\"{frac2:.3f}\", f\"{frac3:.3f}\"]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}