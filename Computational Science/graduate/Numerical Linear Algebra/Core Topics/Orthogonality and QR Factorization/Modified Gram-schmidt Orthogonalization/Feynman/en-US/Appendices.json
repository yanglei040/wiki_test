{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Modified Gram-Schmidt (MGS) algorithm, we must first master its fundamental mechanics. This exercise provides a concrete opportunity to perform a single, complete MGS step by hand. By calculating the first orthonormal vector, the corresponding projection coefficient, and the updated subsequent vector, you will solidify your grasp of the core operations that distinguish MGS from its classical counterpart .",
            "id": "3560619",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 2}$ be the matrix\n$$\nA=\\begin{pmatrix}\n2 & 1 \\\\\n-2 & 0 \\\\\n1 & 2\n\\end{pmatrix},\n$$\nwith columns $a_{1}$ and $a_{2}$. Using the standard Euclidean inner product on $\\mathbb{R}^{3}$ and the induced $2$-norm, perform one full step of the modified Gram–Schmidt (MGS) orthogonalization (where MGS denotes modified Gram–Schmidt) on the first column $a_{1}$ to produce the first orthonormal vector $q_{1}$, then compute the scalar coefficient $r_{12}$ corresponding to the component of $a_{2}$ in the direction of $q_{1}$, and finally update $a_{2}$ by removing this component.\n\nYour derivation must start from the fundamental definitions of inner product, norm, orthogonal projection, and orthonormality, and proceed to identify the expressions that define $q_{1}$, $r_{12}$, and the updated $a_{2}$. Express all quantities exactly (no numerical rounding) over $\\mathbb{R}$.\n\nReport your final answer as a single row of seven entries $(q_{1,1}, q_{1,2}, q_{1,3}, r_{12}, \\tilde{a}_{2,1}, \\tilde{a}_{2,2}, \\tilde{a}_{2,3})$, where $q_{1}=(q_{1,1},q_{1,2},q_{1,3})^{T}$ and $\\tilde{a}_{2}$ is the updated $a_{2}$ after one MGS step. No rounding is required; provide exact rational values where applicable.",
            "solution": "The problem as stated constitutes a well-posed exercise in numerical linear algebra. We are given a matrix $A \\in \\mathbb{R}^{3 \\times 2}$ with columns $a_1$ and $a_2$, and asked to perform the first step of the modified Gram-Schmidt (MGS) orthogonalization process.\n\nThe given matrix is:\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ -2 & 0 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThe columns are $a_1 = \\begin{pmatrix} 2 \\\\ -2 \\\\ 1 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}$.\n\nThe process is to be conducted using the standard Euclidean inner product and its induced $2$-norm. For any two vectors $u = (u_1, u_2, u_3)^T$ and $v = (v_1, v_2, v_3)^T$ in $\\mathbb{R}^3$, the inner product is defined as:\n$$\n\\langle u, v \\rangle = u^T v = u_1 v_1 + u_2 v_2 + u_3 v_3\n$$\nThe induced $2$-norm of a vector $u$ is defined as:\n$$\n\\|u\\|_2 = \\sqrt{\\langle u, u \\rangle} = \\sqrt{u_1^2 + u_2^2 + u_3^2}\n$$\n\nThe modified Gram-Schmidt (MGS) algorithm produces an orthonormal set of vectors $\\{q_1, q_2, \\dots, q_n\\}$ from a linearly independent set $\\{a_1, a_2, \\dots, a_n\\}$. The first step of the MGS process involves normalizing the first vector $a_1$ to obtain $q_1$, and then updating all subsequent vectors by removing their components in the direction of $q_1$.\n\nStep 1: Compute the first orthonormal vector $q_1$.\nThis is achieved by normalizing the vector $a_1$. First, we compute its norm, which corresponds to the coefficient $r_{11}$ in the $QR$ factorization.\n$$\nr_{11} = \\|a_1\\|_2 = \\sqrt{\\langle a_1, a_1 \\rangle} = \\sqrt{(2)^2 + (-2)^2 + (1)^2} = \\sqrt{4 + 4 + 1} = \\sqrt{9} = 3\n$$\nThe first orthonormal vector $q_1$ is then found by dividing $a_1$ by its norm:\n$$\nq_1 = \\frac{a_1}{\\|a_1\\|_2} = \\frac{1}{3} \\begin{pmatrix} 2 \\\\ -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ -2/3 \\\\ 1/3 \\end{pmatrix}\n$$\nThe components of $q_1$ are $q_{1,1} = \\frac{2}{3}$, $q_{1,2} = -\\frac{2}{3}$, and $q_{1,3} = \\frac{1}{3}$.\n\nStep 2: Compute the coefficient $r_{12}$ and update the vector $a_2$.\nThe coefficient $r_{12}$ represents the component of $a_2$ in the direction of the newly computed orthonormal vector $q_1$. It is calculated as the inner product of $q_1$ and $a_2$:\n$$\nr_{12} = \\langle q_1, a_2 \\rangle = \\left\\langle \\begin{pmatrix} 2/3 \\\\ -2/3 \\\\ 1/3 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix} \\right\\rangle = \\left(\\frac{2}{3}\\right)(1) + \\left(-\\frac{2}{3}\\right)(0) + \\left(\\frac{1}{3}\\right)(2) = \\frac{2}{3} + 0 + \\frac{2}{3} = \\frac{4}{3}\n$$\n\nStep 3: Update the vector $a_2$.\nIn the MGS algorithm, the component parallel to $q_1$ is immediately removed from $a_2$. The updated vector, which we denote as $\\tilde{a}_2$, is orthogonal to $q_1$.\n$$\n\\tilde{a}_2 = a_2 - r_{12} q_1\n$$\nSubstituting the values we have computed:\n$$\n\\tilde{a}_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix} - \\frac{4}{3} \\begin{pmatrix} 2/3 \\\\ -2/3 \\\\ 1/3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} (4/3)(2/3) \\\\ (4/3)(-2/3) \\\\ (4/3)(1/3) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 8/9 \\\\ -8/9 \\\\ 4/9 \\end{pmatrix}\n$$\nPerforming the vector subtraction:\n$$\n\\tilde{a}_2 = \\begin{pmatrix} 1 - 8/9 \\\\ 0 - (-8/9) \\\\ 2 - 4/9 \\end{pmatrix} = \\begin{pmatrix} 9/9 - 8/9 \\\\ 8/9 \\\\ 18/9 - 4/9 \\end{pmatrix} = \\begin{pmatrix} 1/9 \\\\ 8/9 \\\\ 14/9 \\end{pmatrix}\n$$\nThe components of the updated vector $\\tilde{a}_2$ are $\\tilde{a}_{2,1} = \\frac{1}{9}$, $\\tilde{a}_{2,2} = \\frac{8}{9}$, and $\\tilde{a}_{2,3} = \\frac{14}{9}$.\n\nThe problem requires reporting the final answer as a single row vector of seven entries: $(q_{1,1}, q_{1,2}, q_{1,3}, r_{12}, \\tilde{a}_{2,1}, \\tilde{a}_{2,2}, \\tilde{a}_{2,3})$.\nCombining our results, we have:\n$q_{1,1} = \\frac{2}{3}$\n$q_{1,2} = -\\frac{2}{3}$\n$q_{1,3} = \\frac{1}{3}$\n$r_{12} = \\frac{4}{3}$\n$\\tilde{a}_{2,1} = \\frac{1}{9}$\n$\\tilde{a}_{2,2} = \\frac{8}{9}$\n$\\tilde{a}_{2,3} = \\frac{14}{9}$\n\nThe final result vector is therefore $(\\frac{2}{3}, -\\frac{2}{3}, \\frac{1}{3}, \\frac{4}{3}, \\frac{1}{9}, \\frac{8}{9}, \\frac{14}{9})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{3} & -\\frac{2}{3} & \\frac{1}{3} & \\frac{4}{3} & \\frac{1}{9} & \\frac{8}{9} & \\frac{14}{9}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the QR factorization of a matrix is essentially unique, the Gram-Schmidt process that generates it is not; the algorithm's output is sensitive to the order in which the columns of the input matrix are processed. This exercise uses a carefully chosen matrix to explicitly demonstrate how permuting the columns leads to different $Q$ and $R$ factors . This provides critical insight into the path-dependent nature of the algorithm and its implications for numerical performance.",
            "id": "3560580",
            "problem": "Consider the Modified Gram-Schmidt (MGS) orthogonalization for an $m \\times n$ matrix $A$ with full column rank, defined in terms of the Euclidean inner product and orthogonal projection onto previously constructed orthonormal vectors. The MGS process constructs an orthonormal set of vectors $\\{q_1,\\dots,q_n\\}$ and an upper triangular matrix $R$ such that $A = Q R$, where $Q = [q_1 \\ \\cdots \\ q_n]$ has orthonormal columns and $R$ encodes the projection coefficients and residual norms. The MGS construction depends on the order in which the columns of $A$ are presented, because at step $j$ the current column $a_j$ is orthogonalized against the span of $\\{q_1,\\dots,q_{j-1}\\}$, and the diagonal entry $r_{jj}$ is the norm of the residual after these projections.\n\nLet $A \\in \\mathbb{R}^{3 \\times 3}$ have columns $a_1, a_2, a_3$ given by\n$$\nA = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix},\n\\quad\na_1 = \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix},\\ \na_2 = \\begin{bmatrix}1 \\\\ 1 \\\\ 0\\end{bmatrix},\\ \na_3 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}.\n$$\nApply MGS to $A$ twice: first with the natural order $(a_1,a_2,a_3)$, and second with the permuted order $(a_3,a_2,a_1)$, producing $A = Q R$ and $A' = Q' R'$ for the permuted matrix $A' = [a_3 \\ a_2 \\ a_1]$.\n\nWhich of the following statements are correct?\n\nA. For any permutation of the columns of $A$, MGS produces the same $Q$ up to a permutation of its columns, and $R$ changes by the corresponding column permutation; therefore the factors $(Q,R)$ are independent of column order.\n\nB. For the given $A$, MGS with order $(a_1,a_2,a_3)$ yields a $Q$ and $R$ with diagonal entries $(r_{11},r_{22},r_{33}) = (1,1,1)$, whereas MGS with order $(a_3,a_2,a_1)$ yields $Q'$ and $R'$ with diagonal entries $(r'_{11},r'_{22},r'_{33}) = (\\sqrt{3},\\sqrt{2/3},1/\\sqrt{2})$; both factorizations satisfy $A = Q R$ and $A' = Q' R'$; moreover, $Q'$ is not a column permutation of $Q$.\n\nC. The diagonal entries $r_{jj}$ of $R$ equal the singular values of $A$ and are invariant under permutations of the columns of $A$.\n\nD. Permuting the columns of $A$ leaves $Q$ unchanged and permutes only the rows of $R$.\n\nE. In exact arithmetic, MGS applied to a full column rank matrix $A$ produces an orthonormal basis whose specific columns depend on the order of input columns; the diagonal coefficients $r_{jj}$ are the norms of residuals after successive projections. Placing columns that are close to the span of previously processed columns later in the order reduces the corresponding $r_{jj}$, and in floating-point arithmetic this increases sensitivity to roundoff and potential loss of orthogonality.\n\nSelect all correct options.",
            "solution": "The user wants me to solve the problem by following these steps:\n1.  **Validate the problem statement:** I will first verify that the problem is well-posed, scientifically sound, and objective.\n2.  **Derive the solution:** I will perform the Modified Gram-Schmidt (MGS) orthogonalization for the two specified column orderings.\n3.  **Evaluate each option:** Based on the results of the derivation, I will analyze each statement (A-E) to determine its correctness.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- The algorithm under consideration is the Modified Gram-Schmidt (MGS) orthogonalization for an $m \\times n$ matrix $A$ with full column rank.\n- The inner product is the Euclidean inner product.\n- MGS produces a factorization $A = QR$, where $Q$ has orthonormal columns and $R$ is an upper triangular matrix.\n- The matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is given by $A = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix}$.\n- The columns of $A$ are $a_1 = \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix}$, $a_2 = \\begin{bmatrix}1 \\\\ 1 \\\\ 0\\end{bmatrix}$, and $a_3 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}$.\n- Two cases are to be analyzed:\n    1. MGS applied to $A$ with column order $(a_1, a_2, a_3)$, yielding $A=QR$.\n    2. MGS applied to $A' = [a_3 \\ a_2 \\ a_1]$ with column order $(a_3, a_2, a_1)$, yielding $A'=Q'R'$.\n- The task is to identify which of the provided statements are correct.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is set within the standard framework of numerical linear algebra. The Modified Gram-Schmidt process is a fundamental and well-defined algorithm. All mathematical concepts and objects (matrices, vectors, inner products, norms) are standard.\n- **Well-Posed:** The given matrix $A$ is upper triangular with non-zero diagonal entries, so its determinant is $\\det(A) = 1 \\times 1 \\times 1 = 1 \\neq 0$. Thus, $A$ is non-singular and has full column rank. The MGS algorithm is well-defined for such a matrix. The question is precise and asks for an evaluation of specific statements, for which a definitive answer can be found.\n- **Objective:** The problem statement is composed of objective mathematical definitions and asks for a verifiable result. It is free from ambiguity and subjective claims.\n\n**1.3. Verdict and Action**\nThe problem statement is valid. I will proceed with the solution derivation.\n\n### Step 2: Solution Derivation\n\nThe Modified Gram-Schmidt (MGS) algorithm proceeds as follows. Initialize $v_k = a_k$ for $k=1, \\dots, n$.\nFor $j = 1, \\dots, n$:\n1. Normalize: $r_{jj} = \\|v_j\\|_2$ and $q_j = v_j / r_{jj}$.\n2. Orthogonalize remaining vectors: For $k = j+1, \\dots, n$, calculate $r_{jk} = q_j^T v_k$ and update $v_k \\leftarrow v_k - r_{jk} q_j$.\n\n**Case 1: MGS on $A = [a_1, a_2, a_3]$**\n\nInitialize $v_1 = a_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $v_2 = a_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $v_3 = a_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n**j = 1:**\n- $r_{11} = \\|v_1\\|_2 = 1$.\n- $q_1 = v_1/r_{11} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- $r_{12} = q_1^T v_2 = [1 \\ 0 \\ 0] \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = 1$.\n- $v_2 \\leftarrow v_2 - r_{12}q_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} - 1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- $r_{13} = q_1^T v_3 = [1 \\ 0 \\ 0] \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = 1$.\n- $v_3 \\leftarrow v_3 - r_{13}q_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} - 1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n\n**j = 2:**\n- $r_{22} = \\|v_2\\|_2 = \\|[0 \\ 1 \\ 0]^T\\|_2 = 1$.\n- $q_2 = v_2/r_{22} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- $r_{23} = q_2^T v_3 = [0 \\ 1 \\ 0] \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} = 1$.\n- $v_3 \\leftarrow v_3 - r_{23}q_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} - 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n\n**j = 3:**\n- $r_{33} = \\|v_3\\|_2 = \\|[0 \\ 0 \\ 1]^T\\|_2 = 1$.\n- $q_3 = v_3/r_{33} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n\nThe resulting factors are:\n$Q = [q_1 \\ q_2 \\ q_3] = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = I$.\n$R = \\begin{bmatrix} r_{11} & r_{12} & r_{13} \\\\ 0 & r_{22} & r_{23} \\\\ 0 & 0 & r_{33} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix} = A$.\nThe diagonal entries of $R$ are $(r_{11}, r_{22}, r_{33}) = (1, 1, 1)$.\n\n**Case 2: MGS on $A' = [a_3, a_2, a_1]$**\n\nLet $a'_1 = a_3, a'_2 = a_2, a'_3 = a_1$.\nInitialize $v'_1 = a'_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $v'_2 = a'_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $v'_3 = a'_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\n**j = 1:**\n- $r'_{11} = \\|v'_1\\|_2 = \\sqrt{1^2+1^2+1^2} = \\sqrt{3}$.\n- $q'_1 = v'_1/r'_{11} = \\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- $r'_{12} = (q'_1)^T v'_2 = \\frac{1}{\\sqrt{3}}[1 \\ 1 \\ 1]\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\frac{2}{\\sqrt{3}}$.\n- $v'_2 \\leftarrow v'_2 - r'_{12}q'_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\frac{2}{\\sqrt{3}}\\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} - \\frac{2}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1/3 \\\\ 1/3 \\\\ -2/3 \\end{bmatrix}$.\n- $r'_{13} = (q'_1)^T v'_3 = \\frac{1}{\\sqrt{3}}[1 \\ 1 \\ 1]\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\frac{1}{\\sqrt{3}}$.\n- $v'_3 \\leftarrow v'_3 - r'_{13}q'_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - \\frac{1}{\\sqrt{3}}\\frac{1}{\\sqrt{3}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - \\frac{1}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\end{bmatrix}$.\n\n**j = 2:**\n- $r'_{22} = \\|v'_2\\|_2 = \\sqrt{(1/3)^2+(1/3)^2+(-2/3)^2} = \\sqrt{6/9} = \\sqrt{2/3}$.\n- $q'_2 = v'_2/r'_{22} = \\frac{1}{\\sqrt{2/3}} \\begin{bmatrix} 1/3 \\\\ 1/3 \\\\ -2/3 \\end{bmatrix} = \\frac{\\sqrt{3}}{\\sqrt{2}}\\frac{1}{3}\\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix} = \\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix}$.\n- $r'_{23} = (q'_2)^T v'_3 = \\frac{1}{\\sqrt{6}}[1 \\ 1 \\ -2]\\begin{bmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\end{bmatrix} = \\frac{1}{\\sqrt{6}}(\\frac{2}{3} - \\frac{1}{3} + \\frac{2}{3}) = \\frac{3/3}{\\sqrt{6}} = \\frac{1}{\\sqrt{6}}$.\n- $v'_3 \\leftarrow v'_3 - r'_{23}q'_2 = \\begin{bmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\end{bmatrix} - \\frac{1}{\\sqrt{6}}\\frac{1}{\\sqrt{6}}\\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 2/3 \\\\ -1/3 \\\\ -1/3 \\end{bmatrix} - \\frac{1}{6}\\begin{bmatrix} 1 \\\\ 1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 4/6 - 1/6 \\\\ -2/6 - 1/6 \\\\ -2/6 + 2/6 \\end{bmatrix} = \\begin{bmatrix} 3/6 \\\\ -3/6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ 0 \\end{bmatrix}$.\n\n**j = 3:**\n- $r'_{33} = \\|v'_3\\|_2 = \\sqrt{(1/2)^2+(-1/2)^2+0^2} = \\sqrt{1/4+1/4} = \\sqrt{1/2} = 1/\\sqrt{2}$.\n- $q'_3 = v'_3/r'_{33} = \\frac{1}{1/\\sqrt{2}}\\begin{bmatrix} 1/2 \\\\ -1/2 \\\\ 0 \\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n\nThe resulting factors are:\n$Q' = [q'_1 \\ q'_2 \\ q'_3] = \\begin{bmatrix} 1/\\sqrt{3} & 1/\\sqrt{6} & 1/\\sqrt{2} \\\\ 1/\\sqrt{3} & 1/\\sqrt{6} & -1/\\sqrt{2} \\\\ 1/\\sqrt{3} & -2/\\sqrt{6} & 0 \\end{bmatrix}$.\n$R' = \\begin{bmatrix} r'_{11} & r'_{12} & r'_{13} \\\\ 0 & r'_{22} & r'_{23} \\\\ 0 & 0 & r'_{33} \\end{bmatrix} = \\begin{bmatrix} \\sqrt{3} & 2/\\sqrt{3} & 1/\\sqrt{3} \\\\ 0 & \\sqrt{2/3} & 1/\\sqrt{6} \\\\ 0 & 0 & 1/\\sqrt{2} \\end{bmatrix}$.\nThe diagonal entries of $R'$ are $(r'_{11}, r'_{22}, r'_{33}) = (\\sqrt{3}, \\sqrt{2/3}, 1/\\sqrt{2})$.\n\n### Step 3: Option-by-Option Analysis\n\n**A. For any permutation of the columns of $A$, MGS produces the same $Q$ up to a permutation of its columns, and $R$ changes by the corresponding column permutation; therefore the factors $(Q,R)$ are independent of column order.**\n\nThe Gram-Schmidt process is inherently order-dependent. The vector $q_j$ is constructed to be in $\\text{span}\\{a_1, \\dots, a_j\\}$ and orthogonal to $\\text{span}\\{q_1, \\dots, q_{j-1}\\}$. Changing the order of the $a_k$ vectors changes these subspaces at each step. Our calculation shows this explicitly:\n$Q = I$ for order $(a_1, a_2, a_3)$.\n$Q' = \\begin{bmatrix} 1/\\sqrt{3} & 1/\\sqrt{6} & 1/\\sqrt{2} \\\\ 1/\\sqrt{3} & 1/\\sqrt{6} & -1/\\sqrt{2} \\\\ 1/\\sqrt{3} & -2/\\sqrt{6} & 0 \\end{bmatrix}$ for order $(a_3, a_2, a_1)$.\nClearly, $Q'$ is not a column permutation of $Q$. The final claim that the factors are independent of column order is fundamentally incorrect.\n**Verdict: Incorrect.**\n\n**B. For the given $A$, MGS with order $(a_1,a_2,a_3)$ yields a $Q$ and $R$ with diagonal entries $(r_{11},r_{22},r_{33}) = (1,1,1)$, whereas MGS with order $(a_3,a_2,a_1)$ yields $Q'$ and $R'$ with diagonal entries $(r'_{11},r'_{22},r'_{33}) = (\\sqrt{3},\\sqrt{2/3},1/\\sqrt{2})$; both factorizations satisfy $A = Q R$ and $A' = Q' R'$; moreover, $Q'$ is not a column permutation of $Q$.**\n\nOur derivation in Step 2 confirms every part of this statement:\n- For order $(a_1, a_2, a_3)$, the diagonal entries of $R$ are $(1, 1, 1)$.\n- For order $(a_3, a_2, a_1)$, the diagonal entries of $R'$ are $(\\sqrt{3}, \\sqrt{2/3}, 1/\\sqrt{2})$.\n- MGS by definition produces a valid factorization, so $A=QR$ and $A'=Q'R'$.\n- As shown in the analysis of option A, $Q' \\neq Q$ and $Q'$ is not a column permutation of $Q$.\n**Verdict: Correct.**\n\n**C. The diagonal entries $r_{jj}$ of $R$ equal the singular values of $A$ and are invariant under permutations of the columns of $A$.**\n\nThis statement makes two claims.\n1. Are $r_{jj}$ equal to the singular values of $A$? The singular values of $A$ are the square roots of the eigenvalues of $A^T A$. $A^T A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 2 & 2 \\\\ 1 & 2 & 3 \\end{bmatrix}$. The characteristic polynomial is $\\det(A^TA-\\lambda I) = -\\lambda^3 + 6\\lambda^2 - 5\\lambda + 1 = 0$. For the natural ordering of columns, we got $(r_{11}, r_{22}, r_{33}) = (1, 1, 1)$. If these were the singular values, the eigenvalues of $A^TA$ would have to be $1^2=1$, $1^2=1$, $1^2=1$. But plugging $\\lambda=1$ into the polynomial gives $-1+6-5+1 = 1 \\neq 0$. So the singular values are not all $1$. The claim is false.\n2. Are $r_{jj}$ invariant under column permutations? Our calculation shows $(r_{11}, r_{22}, r_{33}) = (1, 1, 1)$ for the first ordering and $(r'_{11}, r'_{22}, r'_{33}) = (\\sqrt{3}, \\sqrt{2/3}, 1/\\sqrt{2})$ for the second ordering. These are not equal, so the diagonal entries are not invariant.\nBoth claims are false.\n**Verdict: Incorrect.**\n\n**D. Permuting the columns of $A$ leaves $Q$ unchanged and permutes only the rows of $R$.**\n\nThis is false. As demonstrated by our calculation, permuting the columns of $A$ results in a completely different $Q$ matrix: $Q=I$ and $Q'$ has no zero entries in its first two columns. Therefore, the claim that $Q$ is unchanged is false. The matrix $R'$ is also not a row-permutation of $R$.\n**Verdict: Incorrect.**\n\n**E. In exact arithmetic, MGS applied to a full column rank matrix $A$ produces an orthonormal basis whose specific columns depend on the order of input columns; the diagonal coefficients $r_{jj}$ are the norms of residuals after successive projections. Placing columns that are close to the span of previously processed columns later in the order reduces the corresponding $r_{jj}$, and in floating-point arithmetic this increases sensitivity to roundoff and potential loss of orthogonality.**\n\nThis is a qualitative statement about the properties of MGS. Let's analyze it piece by piece.\n- \"...produces an orthonormal basis whose specific columns depend on the order of input columns\": This is correct, as explained for option A and demonstrated by our calculation of $Q$ and $Q'$.\n- \"...the diagonal coefficients $r_{jj}$ are the norms of residuals after successive projections\": This is correct by the definition of the MGS algorithm. At step $j$, $r_{jj}$ is the norm of the current vector $v_j$, which is the original vector $a_j$ after its components in the directions of $q_1, \\ldots, q_{j-1}$ have been removed. This residual is precisely the component of $a_j$ orthogonal to $\\text{span}\\{q_1, \\dots, q_{j-1}\\} = \\text{span}\\{a_1, \\dots, a_{j-1}\\}$.\n- \"Placing columns that are close to the span of previously processed columns later in the order reduces the corresponding $r_{jj}$\": If a column vector $a_j$ is nearly linearly dependent on the preceding columns $\\{a_1, \\dots, a_{j-1}\\}$, its component orthogonal to their span will be small. The norm of this component is $r_{jj}$. Thus, a small $r_{jj}$ indicates that $a_j$ is close to $\\text{span}\\{a_1, \\dots, a_{j-1}\\}$. This part is a correct statement about the geometric interpretation of MGS.\n- \"...in floating-point arithmetic this increases sensitivity to roundoff and potential loss of orthogonality\": This is a well-known principle in numerical linear algebra. A small $r_{jj}$ implies that the vector being normalized is small, which can amplify relative errors during the normalization step $q_j = v_j / r_{jj}$. More importantly, it signifies near rank-deficiency, a condition that notoriously leads to a computed $Q$ matrix whose columns are not numerically orthogonal. Column pivoting strategies in QR factorization are designed specifically to mitigate this issue by reordering columns to maximize $r_{jj}$ at each step.\nThe entire statement is a correct and accurate description of the behavior of MGS.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{BE}$$"
        },
        {
            "introduction": "The true test of a numerical algorithm lies in its performance under stress, and in this problem, we move from exact arithmetic to the world of floating-point computation to analyze the numerical stability of MGS. By considering its behavior on standard test matrices with diverse properties—from perfectly orthogonal to pathologically ill-conditioned—we can develop a robust intuition for predicting and understanding the loss of orthogonality in practice . This exercise connects the theoretical algorithm to the practical realities of numerical linear algebra.",
            "id": "3560570",
            "problem": "Consider three square test matrices of order $n$ to be orthogonalized by the modified Gram–Schmidt (MGS) algorithm (one pass, no reorthogonalization, no pivoting) in standard floating-point arithmetic with unit roundoff $\\,\\varepsilon\\,$, modeled by the relation $\\mathrm{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $\\lvert \\delta \\rvert \\le \\varepsilon$ for each arithmetic operation $\\,\\circ\\,$. The spectral norm is denoted by $\\,\\lVert \\cdot \\rVert_2\\,$, and the $2$-norm condition number by $\\,\\kappa_2(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)\\,$. The three matrices are:\n- $A^{(H)} \\in \\mathbb{R}^{n \\times n}$ is the Hilbert matrix with entries $\\,A^{(H)}_{ij} = 1/(i + j - 1)\\,$, known to be extremely ill-conditioned and to have columns with very small principal angles.\n- $A^{(K)} \\in \\mathbb{R}^{n \\times n}$ is a Kahan matrix with parameter $\\,\\theta \\in (0,\\pi/2)\\,$, chosen so that $\\,\\kappa_2(A^{(K)}) \\approx 1\\,$ while many columns are strongly pairwise correlated in the sense that the Gram matrix $\\,\\left(A^{(K)}\\right)^\\top A^{(K)}\\,$ has significant off-diagonal entries; this is a standard test matrix family for Gram–Schmidt behavior.\n- $A^{(U)} \\in \\mathbb{R}^{n \\times n}$ is a Haar-distributed orthogonal matrix, that is, $\\,A^{(U)}\\,$ is uniformly distributed on the orthogonal group $\\,\\mathrm{O}(n)\\,$.\n\nLet $\\,Q^{(X)}\\,$ denote the $\\,Q\\,$ returned by MGS when run on $\\,A^{(X)}\\,$ for $\\,X \\in \\{H,K,U\\}\\,$, and measure loss of orthogonality by $\\,\\lVert (Q^{(X)})^\\top Q^{(X)} - I \\rVert_2\\,$. Assume that all three inputs are scaled so that $\\,\\lVert A^{(X)} \\rVert_2 \\approx 1\\,$ and that $\\,n\\,$ is moderate (for concreteness, take $\\,n = 20\\,$ when a specific $\\,n\\,$ is needed). Using only fundamental definitions (spectral norm, condition number, principal angles/column correlations via the Gram matrix) and the stated floating-point model, reason from first principles about how spectral properties and column correlations influence MGS and select all statements below that correctly predict the expected size and ordering of $\\,\\lVert (Q^{(X)})^\\top Q^{(X)} - I \\rVert_2\\,$.\n\nA. For $\\,A^{(U)}\\,$, since the columns are orthonormal in exact arithmetic, MGS inner products ideally vanish; in floating point, the computed inner products are of magnitude $\\,\\mathcal{O}(\\varepsilon)\\,$ and accumulate over $\\,n\\,$ steps. One therefore expects $\\,\\lVert (Q^{(U)})^\\top Q^{(U)} - I \\rVert_2 = \\mathcal{O}(n \\varepsilon)\\,$ with a modest constant factor.\n\nB. For $\\,A^{(K)}\\,$ with $\\,\\kappa_2(A^{(K)}) \\approx 1\\,$ and small $\\,\\theta\\,$ (so that raw columns are strongly correlated), MGS still yields $\\,\\lVert (Q^{(K)})^\\top Q^{(K)} - I \\rVert_2 = \\mathcal{O}(n \\varepsilon)\\,$, because the column-by-column deflation in MGS prevents amplification by ill-conditioning; the lack of amplification is explained by the well-conditioned spectrum and nonpathological principal angles of the already-orthogonalized subspaces.\n\nC. For $\\,A^{(H)}\\,$ with $\\,n = 20\\,$, the combination of extremely small principal angles between columns and very large $\\,\\kappa_2(A^{(H)})\\,$ implies that rounding errors incurred in projections and subtractions are strongly amplified, so one should expect $\\,\\lVert (Q^{(H)})^\\top Q^{(H)} - I \\rVert_2\\,$ to be many orders of magnitude larger than $\\,\\varepsilon\\,$, often as large as about $\\,10^{-3}\\,$ or more, consistent with a heuristic bound of the form $\\,c\\,\\varepsilon\\,\\kappa_2(A^{(H)})\\,$ for a moderate constant $\\,c\\,$.\n\nD. If each column of $\\,A^{(H)}\\,$ is pre-scaled to have unit $\\,2$-norm before executing MGS, then $\\,\\lVert (Q^{(H)})^\\top Q^{(H)} - I \\rVert_2\\,$ becomes $\\,\\mathcal{O}(\\varepsilon)\\,$ independent of $\\,n\\,$, because rescaling removes the small principal angles that cause error amplification.\n\nSelect all correct options.",
            "solution": "The problem statement has been validated and is deemed sound, well-posed, and an appropriate question in the field of numerical linear algebra. We may proceed with the solution.\n\nThe core of this problem is to analyze the numerical stability of the Modified Gram-Schmidt (MGS) algorithm, specifically its sensitivity to different properties of the input matrix $A$. The loss of orthogonality is measured by $\\lVert \\hat{Q}^\\top \\hat{Q} - I \\rVert_2$, where $\\hat{Q}$ is the matrix of vectors computed in floating-point arithmetic.\n\nThe MGS process for a matrix $A = [a_1, a_2, \\dots, a_n]$ iteratively computes an orthonormal basis $Q = [q_1, q_2, \\dots, q_n]$. At step $k$, it normalizes the current vector to get $q_k$, and then immediately makes all subsequent vectors $a_j$ (for $j > k$) orthogonal to this new $q_k$:\n$$ v_k = a_k $$\n$$ \\text{For } i = 1, \\dots, k-1: \\quad v_k = v_k - (q_i^\\top v_k) q_i \\quad \\text{(Classical GS form)} $$\n$$ q_k = v_k / \\lVert v_k \\rVert_2 $$\nThe Modified GS version updates the vectors $a_{k+1}, \\dots, a_n$ within the main loop:\n$$ \\text{For } k = 1, \\dots, n: $$\n$$ \\quad q_k = a_k / \\lVert a_k \\rVert_2 $$\n$$ \\quad \\text{For } j = k+1, \\dots, n: $$\n$$ \\quad \\quad a_j = a_j - (q_k^\\top a_j) q_k $$\nIn floating-point arithmetic, each operation introduces a small relative error of size at most $\\varepsilon$. The stability of the process depends on how these errors accumulate. A key result from the analysis of MGS is that the loss of orthogonality can be bounded, with a well-known heuristic expression being:\n$$ \\lVert \\hat{Q}^\\top \\hat{Q} - I \\rVert_2 \\le c \\cdot n \\cdot \\varepsilon \\cdot \\kappa_2(A) $$\nfor some modest constant $c$. This bound shows that the condition number $\\kappa_2(A)$ is a critical factor. The condition number $\\kappa_2(A)$ reflects the near-linear dependence of the columns of $A$. If $\\kappa_2(A)$ is large, at least one column $a_k$ is nearly in the span of the others, which also implies that for some $k$, $a_k$ is nearly in the span of the preceding columns, $\\mathrm{span}\\{a_1, \\dots, a_{k-1}\\}$. This leads to catastrophic cancellation when computing the component of $a_k$ orthogonal to that span, amplifying rounding errors.\n\nLet's evaluate each matrix type based on these principles.\n\n### Option-by-Option Analysis\n\n**A. For $A^{(U)}$, a Haar-distributed orthogonal matrix:**\nThe columns of $A^{(U)}$ are orthonormal by definition. Thus, $\\kappa_2(A^{(U)}) = 1$. In exact arithmetic, MGS would find that for each $k$, the inner products $q_k^\\top a_j$ (for $j>k$) are all zero, and the algorithm would simply normalize each column of $A^{(U)}$ to produce $Q = A^{(U)}$.\n\nIn floating-point arithmetic:\n1.  The matrix is perfectly conditioned, so the factor $\\kappa_2(A^{(U)})$ in any stability bound is $1$.\n2.  The computation of an inner product $s = x^\\top y$ between two $n$-vectors in floating point generally produces a result $\\hat{s}$ with an error bound $\\lvert \\hat{s} - s \\rvert \\le n \\varepsilon \\lVert x \\rVert_2 \\lVert y \\rVert_2 + \\mathcal{O}(\\varepsilon^2)$.\n3.  When computing $\\hat{r}_{kj} = \\mathrm{fl}(\\hat{q}_k^\\top \\hat{a}_j)$, the true inner product is nearly zero. The computed value will be a small quantity of order $\\mathcal{O}(n \\varepsilon)$ because the vectors have norms close to $1$.\n4.  This small, non-zero $\\hat{r}_{kj}$ is then used to update $\\hat{a}_j$, introducing a small component in the direction of $\\hat{q}_k$. This process repeats at each step, and the errors accumulate.\n5.  The final loss of orthogonality $\\lVert (\\hat{Q}^{(U)})^\\top \\hat{Q}^{(U)} - I \\rVert_2$ will thus be proportional to the size of these accumulated errors. Since there is no amplification mechanism (as $\\kappa_2(A^{(U)})=1$), the final error is expected to scale with the number of operations. The bound simplifies to $\\mathcal{O}(n \\varepsilon)$.\nThe reasoning provided in the option is a correct first-principles argument.\n\n**Verdict: Correct.**\n\n**B. For $A^{(K)}$, a Kahan matrix with $\\kappa_2(A^{(K)}) \\approx 1$:**\nThe Kahan matrix is constructed to be well-conditioned ($\\kappa_2(A^{(K)}) \\approx 1$) while having some columns that are very close to each other (i.e., small pairwise principal angles, leading to a Gram matrix with large off-diagonal entries). This setup distinguishes the behavior of MGS from that of Classical Gram-Schmidt (CGS).\n\nCGS computes all inner products $r_{kj} = a_k^\\top a_j$ using the original vectors. If $a_k$ and $a_j$ are nearly parallel, CGS can fail dramatically. MGS, however, computes $r_{kj} = q_k^\\top a_j$ using the most up-to-date version of $a_j$, which has already been orthogonalized against $q_1, \\dots, q_{k-1}$. The stability of MGS depends on the condition number of the matrix, which measures the linear independence of a column from the *subspace* spanned by previous columns, not just its relationship with other individual columns.\n\nSince $\\kappa_2(A^{(K)}) \\approx 1$, all of its leading submatrices are also well-conditioned. This means that at every step $j$, the vector $a_j$ is not close to the subspace $\\mathrm{span}\\{a_1, \\dots, a_{j-1}\\}$. Consequently, the catastrophic cancellation that plagues ill-conditioned cases does not occur. MGS behaves as if it were applied to any well-conditioned matrix. The loss of orthogonality is governed by the same analysis as for $A^{(U)}$, yielding a bound of $\\mathcal{O}(n \\varepsilon)$ because $\\kappa_2(A^{(K)}) \\approx 1$. The option correctly states that the lack of amplification is due to the well-conditioned spectrum.\n\n**Verdict: Correct.**\n\n**C. For $A^{(H)}$, the Hilbert matrix:**\nThe Hilbert matrix is the canonical example of a severely ill-conditioned matrix. Its condition number $\\kappa_2(A^{(H)})$ grows exponentially with $n$. For $n=20$, $\\kappa_2(A^{(H)})$ is extremely large (on the order of $10^{14}$ for a standard Hilbert matrix, although scaling to $\\lVert A^{(H)} \\rVert_2 \\approx 1$ might change the exact value, the order of magnitude of `ill-conditioning` remains).\n\nThis extreme ill-conditioning means that for relatively small $j$, the column vector $a_j$ becomes nearly linearly dependent on the preceding columns $\\{a_1, \\dots, a_{j-1}\\}$. The principal angle between $a_j$ and the subspace $\\mathrm{span}\\{q_1, \\dots, q_{j-1}\\}$ is tiny.\nWhen MGS computes the orthogonal component $v_j = a_j - \\sum_{i=1}^{j-1} (q_i^\\top a_j)q_i$, it is subtracting a vector (the projection) that is almost equal to $a_j$ itself. This is a textbook case of catastrophic cancellation. The computed vector $\\hat{v}_j$ will have very few, if any, correct significant digits and will be dominated by floating-point rounding errors. Normalizing this error-laden vector to $\\hat{q}_j$ creates a vector of norm $1$ that has a random orientation relative to the true orthogonal direction. This computed $\\hat{q}_j$ will not be orthogonal to the previous $\\hat{q}_i$.\n\nThe heuristic bound $\\lVert \\hat{Q}^\\top \\hat{Q} - I \\rVert_2 \\approx c \\cdot \\varepsilon \\cdot \\kappa_2(A^{(H)})$ is a reliable estimate in this scenario. Given standard double precision arithmetic ($\\varepsilon \\approx 10^{-16}$) and $\\kappa_2(A^{(H)})$ for $n=20$ being enormous ($\\sim 10^{14}$), the product is $\\varepsilon \\cdot \\kappa_2(A^{(H)}) \\approx 10^{-16} \\cdot 10^{14} = 10^{-2}$. A predicted loss of orthogonality of about $10^{-3}$ is therefore entirely reasonable and represents a near-total loss of orthogonality. The reasoning in the option is physically and numerically sound.\n\n**Verdict: Correct.**\n\n**D. Pre-scaling columns of $A^{(H)}$:**\nThe claim is that scaling each column of $A^{(H)}$ to have unit $2$-norm will resolve the stability issue, making the loss of orthogonality $\\mathcal{O}(\\varepsilon)$. This is fundamentally incorrect.\nThe ill-conditioning of the Hilbert matrix stems from the fact that its column vectors become nearly collinear (i.e., point in very similar directions). For example, the first two columns, $[1, 1/2, 1/3, \\dots]^\\top$ and $[1/2, 1/3, 1/4, \\dots]^\\top$, are almost parallel.\nScaling a vector changes its length but not its direction. Let the column-scaled matrix be $A'$. The columns of $A'$ are $a'_j = a_j / \\lVert a_j \\rVert_2$. If $a_j \\approx \\sum c_i a_i$, then after scaling, we have $a'_j \\approx \\sum c'_i a'_i$ for some modified coefficients $c'_i$. The near-linear dependence, which is a geometric property of the column space, persists.\nConsequently, the condition number of the column-scaled Hilbert matrix, $\\kappa_2(A')$, remains extremely large. It is not reduced to $\\mathcal{O}(1)$. Applying MGS to $A'$ will still lead to catastrophic cancellation and a large loss of orthogonality, on the order of $\\varepsilon \\cdot \\kappa_2(A')$.\nThe statement's reasoning that rescaling removes the small principal angles is false. The angles between the vectors (and between a vector and a subspace) are preserved under individual scaling of the basis vectors of the subspace only if the scaling is uniform. Individual, non-uniform scaling will change the angles, but it will not magically make nearly-dependent vectors orthogonal. The core problem of near-dependence remains. Therefore, this statement is based on a misunderstanding of conditioning.\n\n**Verdict: Incorrect.**\n\nFinal conclusion based on the analysis: Statements A, B, and C are correct descriptions of the behavior of MGS.",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}