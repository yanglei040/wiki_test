{
    "hands_on_practices": [
        {
            "introduction": "Understanding the computational cost of any large-scale algorithm begins with analyzing its most fundamental operation. For Householder QR factorization, this core operation is the application of a single Householder reflector to a block of a matrix. This practice guides you through a first-principles derivation of this cost, establishing the foundational $2ms$ leading-term formula that underpins nearly all efficiency analyses of Householder-based methods . Mastering this calculation is the first step toward evaluating the performance of the full algorithm.",
            "id": "3562610",
            "problem": "Consider a single real Householder reflector $H \\in \\mathbb{R}^{m \\times m}$ of the form $H = I - \\tau v v^{\\top}$, where $v \\in \\mathbb{R}^{m}$ and $\\tau \\in \\mathbb{R}$, applied from the left to a dense block $B \\in \\mathbb{R}^{m \\times s}$. Starting from the identity $H B = B - \\tau v (v^{\\top} B)$ and the definition of a floating-point operation, derive the asymptotic operation count for computing $H B$.\n\nAdopt the following scientifically standard and widely used cost model for Basic Linear Algebra Subprograms (BLAS). In real arithmetic:\n- An inner product of length $m$ is costed as $m$ composite fused multiply-adds (abbreviated as FMA), i.e., each multiply-add pair is counted as a single floating-point operation, for a dominant cost of $m$.\n- A rank-$1$ update of size $m \\times s$ is costed as $m s$ composite FMA operations, i.e., one floating-point operation per entry for the multiply-add pair.\n- Scalar-vector and scalar-matrix scalings over $k$ entries cost $k$ floating-point operations.\n\nUnder this model, compute the total number of floating-point operations required to evaluate $H B$ using the minimal three-step algorithm implied by $H B = B - \\tau v (v^{\\top} B)$:\n1. Compute the row vector $w^{\\top} := v^{\\top} B \\in \\mathbb{R}^{1 \\times s}$.\n2. Scale $w^{\\top} := \\tau w^{\\top}$.\n3. Perform the rank-$1$ update $B := B - v w^{\\top}$.\n\nGive your answer as a single closed-form expression in terms of $m$ and $s$ that captures only the leading-order term in $m$ and $s$ (i.e., lower-order terms in $m$ and $s$ must be omitted). Your final answer must be a single analytic expression with no units. Verify that your result matches the standard leading term reported for the application of a single Householder reflector to an $m \\times s$ block.",
            "solution": "We begin from the defining action of the Householder reflector $H = I - \\tau v v^{\\top}$ on a matrix $B \\in \\mathbb{R}^{m \\times s}$:\n$$\nH B \\;=\\; B - \\tau v (v^{\\top} B).\n$$\nThis expression suggests a natural three-step algorithm: compute the intermediate row vector $w^{\\top} := v^{\\top} B \\in \\mathbb{R}^{1 \\times s}$, scale it by $\\tau$, and then perform a rank-$1$ correction $B := B - v w^{\\top}$.\n\nWe count floating-point operations under the stated model that treats each fused multiply-add as a single operation. This model is standard when deriving leading-term operation counts for Householder-based algorithms and aligns with the organization of computations in Level-$2$ Basic Linear Algebra Subprograms (BLAS), where the dominant costs are attributable to matrix-vector and rank-$1$ operations with multiply-add fusion.\n\nStep-by-step counts:\n1. Compute $w^{\\top} := v^{\\top} B$. This is a left matrix-vector product in block form, equivalently $w = B^{\\top} v$, producing a vector in $\\mathbb{R}^{s}$. It is realized as $s$ inner products of length $m$. Under the composite fused multiply-add model, each inner product of length $m$ costs $m$ floating-point operations (one per multiply-add pair). Therefore, the total dominant cost of this step is\n$$\nm s.\n$$\n\n2. Scale $w^{\\top} := \\tau w^{\\top}$. This is a scalar-vector scaling of $s$ entries, which costs\n$$\ns.\n$$\nThis is lower-order in $m$ and $s$ compared with the dominant bilinear terms.\n\n3. Perform the rank-$1$ update $B := B - v w^{\\top}$. This updates every entry of $B$ as $b_{ij} := b_{ij} - v_{i} w_{j}$. Under the composite fused multiply-add model, each update per entry counts as one operation, and there are $m s$ entries. Thus, the dominant cost of this step is\n$$\nm s.\n$$\n\nSumming the three contributions, the total is\n$$\nm s \\;+\\; s \\;+\\; m s \\;=\\; 2 m s \\;+\\; s.\n$$\nBy the problem’s instruction to report only the leading-order term in $m$ and $s$, we omit the lower-order $s$ term. Hence, the leading-order operation count for applying a single Householder reflector to an $m \\times s$ block under the stated model is\n$$\n2 m s.\n$$\n\nThis matches the standard leading-term count used in operation analyses of Householder-based QR factorization, where applying one reflector to a trailing $m \\times s$ block incurs a dominant cost of $2 m s$ floating-point operations (in the composite fused multiply-add sense), with lower-order terms suppressed. For completeness, we note that under the alternative model that counts a multiply and an add as separate operations, the detailed count would be $(2 m s - s) + s + 2 m s = 4 m s$, which still verifies that the dominant scaling is bilinear in $m$ and $s$, but with a different constant prefactor due to the differing operation model. Under the model specified in the problem, the leading term is $2 m s$.",
            "answer": "$$\\boxed{2 m s}$$"
        },
        {
            "introduction": "After determining the cost of a single step, the next logical task is to aggregate these costs to find the total expense of the full algorithm. This exercise scales up the analysis to the entire unblocked Householder QR factorization, requiring you to sum the costs over all algorithmic stages. More importantly, it provides critical context by directly comparing the resulting computational complexity with that of the classical and modified Gram-Schmidt procedures, highlighting the superior efficiency of the Householder approach in terms of floating-point operations .",
            "id": "3562581",
            "problem": "You are asked to compare the dominant operation counts of three orthogonalization methods in real arithmetic: unblocked Householder orthogonal-triangular (QR) factorization, classical Gram–Schmidt (CGS), and modified Gram–Schmidt (MGS). Use the following modeling assumptions for counting cost: each floating-point operation (FLOP) is a single addition or multiplication counted as $1$ FLOP; scalar divisions and square roots are present but may be neglected when determining the coefficient of $n^3$ in the total cost; memory traffic is not counted. Work from a stepwise, first-principles account of the arithmetic performed by each method.\n\nFor unblocked Householder QR on a full-rank real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, consider iteration $k$, where a Householder reflector is formed from the vector in rows $k{:}m$ of column $k$ and then applied to the trailing submatrix in rows $k{:}m$ and columns $k{+}1{:}n$. Build the operation count by accounting for\n- the cost to apply the reflector to each of the $n-k$ trailing columns via a dot product and a scaled vector subtraction, both of length $m-k+1$, and\n- any costs to form the reflector that could contribute to the $n^3$ term when $m=n$.\n\nThen specialize to the square case $m=n$ and determine the coefficient of $n^3$ in the total FLOP count for unblocked Householder QR.\n\nIndependently, derive the leading (in $n$) FLOP counts for CGS and MGS on a real $n \\times n$ full-rank matrix under the same model by summing the arithmetic required by their inner products and vector updates across all columns, and identify the corresponding coefficients of $n^3$.\n\nWhich statement correctly reports these coefficients for the square case $m=n$?\n\nA. In the square case $m=n$, unblocked Householder QR has leading $n^3$ coefficient $(4/3)$, while both classical Gram–Schmidt and modified Gram–Schmidt have leading $n^3$ coefficient $2$.\n\nB. In the square case $m=n$, unblocked Householder QR has leading $n^3$ coefficient $2$, while both classical Gram–Schmidt and modified Gram–Schmidt have leading $n^3$ coefficient $(4/3)$.\n\nC. In the square case $m=n$, all three methods have the same leading $n^3$ coefficient equal to $2$.\n\nD. In the square case $m=n$, unblocked Householder QR has leading $n^3$ coefficient $(8/3)$, while both classical Gram–Schmidt and modified Gram–Schmidt have leading $n^3$ coefficient $2$.",
            "solution": "We begin with unblocked Householder orthogonal-triangular (QR) factorization. In iteration $k$, a Householder reflector $H_k = I - \\tau v v^{\\top}$ is constructed from the vector $A_{k:m,k}$. The reflector is applied to the trailing submatrix $A_{k:m,k+1:n}$ by performing, for each trailing column $j \\in \\{k+1,\\dots,n\\}$:\n- a dot product $w_j = v^{\\top} A_{k:m,j}$ of length $m-k+1$, which costs approximately $2(m-k+1)$ FLOPs, and\n- an axpy-type update $A_{k:m,j} \\leftarrow A_{k:m,j} - \\tau v w_j$, which costs approximately $2(m-k+1)$ FLOPs.\n\nThus, applying the reflector to a single trailing column costs approximately $4(m-k+1)$ FLOPs, and there are $(n-k)$ such trailing columns. The application cost per iteration $k$ is therefore\n$$\n4\\,(m-k+1)\\,(n-k).\n$$\nThe costs to form the reflector itself within iteration $k$ include computing a vector norm, a few vector scalings, and a few inner products of length $m-k+1$. When summed over $k = 1,\\dots,n$, these reflector-formation costs scale as $\\mathcal{O}(n^2)$ for $m=n$ and thus do not contribute to the coefficient of $n^3$.\n\nSpecializing to $m=n$, the dominant cost is the sum\n$$\n\\sum_{k=1}^{n} 4\\,(n-k+1)\\,(n-k) \\;=\\; 4 \\sum_{j=0}^{n-1} (j+1)\\,j \\;=\\; 4 \\sum_{j=0}^{n-1} \\big(j^2 + j\\big).\n$$\nUsing the well-known sums $\\sum_{j=0}^{n-1} j = \\tfrac{n(n-1)}{2}$ and $\\sum_{j=0}^{n-1} j^2 = \\tfrac{(n-1)n(2n-1)}{6}$, we obtain\n$$\n4 \\left( \\frac{(n-1)n(2n-1)}{6} + \\frac{n(n-1)}{2} \\right)\n= \\frac{4}{6} (n-1)n(2n-1) + 2 n(n-1).\n$$\nThe leading term in $n$ from this expression is\n$$\n\\frac{4}{6} \\cdot 2 n^3 \\;=\\; \\frac{4}{3} n^3,\n$$\nso the coefficient of $n^3$ for unblocked Householder QR in the square case is $(4/3)$.\n\nNext, consider classical Gram–Schmidt (CGS) for a real $n \\times n$ matrix. For column $j$, CGS computes, for each $i \\in \\{1,\\dots,j-1\\}$,\n- an inner product $r_{i,j} = q_i^{\\top} a_j$ costing approximately $2n$ FLOPs, and\n- a vector update $a_j \\leftarrow a_j - q_i r_{i,j}$ costing approximately $2n$ FLOPs.\n\nThus, for fixed $j$, the projection-and-subtraction across the $j-1$ prior vectors costs approximately $4n\\,(j-1)$ FLOPs. Summing over $j=1,\\dots,n$ gives\n$$\n\\sum_{j=1}^{n} 4n\\,(j-1) \\;=\\; 4n \\sum_{j=1}^{n} (j-1) \\;=\\; 4n \\cdot \\frac{n(n-1)}{2} \\;=\\; 2 n^3 - 2 n^2.\n$$\nAdditional per-column costs such as a norm ($\\approx 2n$) and scaling ($\\approx n$) contribute $\\mathcal{O}(n^2)$ overall and do not change the $n^3$ coefficient. Therefore, the coefficient of $n^3$ for CGS is $2$.\n\nFor modified Gram–Schmidt (MGS), for column $j$ one first normalizes $a_j$ to $q_j$, incurring $\\mathcal{O}(n)$ FLOPs, and then for each $k \\in \\{j+1,\\dots,n\\}$ computes\n- $r_{j,k} = q_j^{\\top} a_k$ at cost approximately $2n$, and\n- $a_k \\leftarrow a_k - q_j r_{j,k}$ at cost approximately $2n$.\n\nFor fixed $j$, there are $(n-j)$ such updates, contributing approximately $4n\\,(n-j)$ FLOPs. Summing over $j=1,\\dots,n$ gives\n$$\n\\sum_{j=1}^{n} 4n\\,(n-j) \\;=\\; 4n \\sum_{j=1}^{n} (n-j) \\;=\\; 4n \\cdot \\frac{n(n-1)}{2} \\;=\\; 2 n^3 - 2 n^2.\n$$\nAgain, normalization costs are $\\mathcal{O}(n^2)$ and do not affect the leading coefficient. Thus, the coefficient of $n^3$ for MGS is also $2$.\n\nCollecting these results in the square case $m=n$:\n- Unblocked Householder QR: coefficient $(4/3)$.\n- Classical Gram–Schmidt: coefficient $2$.\n- Modified Gram–Schmidt: coefficient $2$.\n\nWe now assess the options.\n\nOption A states Householder has coefficient $(4/3)$ and both Gram–Schmidt variants have coefficient $2$. This matches the derivation. Correct.\n\nOption B states Householder has $2$ and both Gram–Schmidt variants have $(4/3)$. This inverts the correct comparison; Householder is lower by a factor of $3/2$ in the leading term for the square case. Incorrect.\n\nOption C states all three have coefficient $2$. This neglects the strictly smaller leading coefficient $(4/3)$ for Householder QR. Incorrect.\n\nOption D assigns $(8/3)$ to Householder and $2$ to both Gram–Schmidt variants. The $(8/3)$ arises in complex arithmetic contexts but not in real arithmetic under the present model; the real case coefficient is $(4/3)$. Incorrect.\n\nTherefore, the correct choice is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Theoretical operation counts provide insight, but practical high performance is achieved by structuring algorithms to match modern computer hardware. This practice transitions from the \"unblocked\" algorithm, rich in memory-bound matrix-vector operations (Level-2 BLAS), to the \"blocked\" or \"panel\" factorization approach used in libraries like LAPACK. By deriving the cost of applying a block of reflectors in its compact WY form, you will uncover how the computation is recast into high-performance matrix-matrix multiplications (Level-3 BLAS), a key optimization for achieving speed on today's processors .",
            "id": "3562527",
            "problem": "Consider the blocked Householder representation of an orthogonal panel transformation, in which the panel transform is written in the compact Watson–Yamaguchi (WY) form as $Q_{\\text{panel}} = I - V T V^{\\top}$, with $V \\in \\mathbb{R}^{p \\times b}$ containing $b$ Householder vectors and $T \\in \\mathbb{R}^{b \\times b}$ an upper triangular matrix constructed from these vectors. Let $C \\in \\mathbb{R}^{p \\times q}$ denote a trailing matrix to be updated by a left application of the panel transform, $C \\leftarrow Q_{\\text{panel}} C$. Work in real arithmetic and use the standard floating-point operation (flop) model in which each scalar multiplication and each scalar addition counts as one flop. For dense matrix-matrix multiplication of conforming dimensions $m \\times k$ by $k \\times n$, assume a cost of $2 m n k$ flops, and for triangular (upper or lower) matrix-matrix multiplication of dimensions $b \\times b$ by $b \\times q$, assume a cost of $b^{2} q$ flops. For elementwise addition or subtraction of two $p \\times q$ matrices, assume a cost of $p q$ flops.\n\nStarting from these definitions, derive the total flop count required to apply $Q_{\\text{panel}}$ to $C$ on the left in terms of $p$, $q$, and $b$. Your final answer must be a single closed-form analytic expression in $p$, $q$, and $b$. Also, identify which Level-3 Basic Linear Algebra Subprograms (BLAS) term dominates the cost. Provide the final expression only; no rounding is required.",
            "solution": "The task is to determine the total floating-point operation (flop) count for updating a trailing matrix $C \\in \\mathbb{R}^{p \\times q}$ by a left application of a panel transformation $Q_{\\text{panel}}$. The transformation is given in the compact Watson-Yamaguchi (WY) form as $Q_{\\text{panel}} = I - V T V^{\\top}$, where $V \\in \\mathbb{R}^{p \\times b}$ and $T \\in \\mathbb{R}^{b \\times b}$ is an upper triangular matrix. The update operation is $C \\leftarrow Q_{\\text{panel}} C$.\n\nSubstituting the definition of $Q_{\\text{panel}}$ into the update rule gives:\n$$\nC \\leftarrow (I - V T V^{\\top}) C = I C - (V T V^{\\top}) C = C - V T V^{\\top} C\n$$\nTo compute the term $V T V^{\\top} C$ efficiently, we must use an optimal association of the matrix multiplications. The standard and most efficient approach, which maximizes the use of high-performance Level-$3$ BLAS (Basic Linear Algebra Subprograms) routines, is to group the operations from right to left:\n$$\nV(T(V^{\\top} C))\n$$\nWe will now calculate the flop count for each step in this sequence.\n\nStep 1: Compute the matrix product $W_1 = V^{\\top} C$.\nThe dimensions of the matrices are $V^{\\top} \\in \\mathbb{R}^{b \\times p}$ and $C \\in \\mathbb{R}^{p \\times q}$. The resulting matrix $W_1$ has dimensions $b \\times q$. This is a dense matrix-matrix multiplication. According to the provided cost model for an $m \\times k$ matrix multiplied by a $k \\times n$ matrix (cost of $2mnk$ flops), we set $m=b$, $k=p$, and $n=q$.\nThe cost for this step is:\n$$\n\\text{Cost}_1 = 2bpq \\text{ flops}\n$$\nThis operation corresponds to a `GEMM` (General Matrix-Matrix Multiply) routine in BLAS.\n\nStep 2: Compute the matrix product $W_2 = T W_1$.\nThe dimensions are $T \\in \\mathbb{R}^{b \\times b}$ and $W_1 \\in \\mathbb{R}^{b \\times q}$. The matrix $T$ is specified as upper triangular. The resulting matrix $W_2$ has dimensions $b \\times q$. The problem provides a specific cost for a triangular matrix-matrix multiplication of these dimensions.\nThe cost for this step is:\n$$\n\\text{Cost}_2 = b^{2}q \\text{ flops}\n$$\nThis operation corresponds to a `TRMM` (Triangular Matrix-Matrix Multiply) routine in BLAS.\n\nStep 3: Compute the matrix product $W_3 = V W_2$.\nThe dimensions are $V \\in \\mathbb{R}^{p \\times b}$ and $W_2 \\in \\mathbb{R}^{b \\times q}$. The resulting matrix $W_3$ has dimensions $p \\times q$. This is another dense matrix-matrix multiplication. We use the cost model with $m=p$, $k=b$, and $n=q$.\nThe cost for this step is:\n$$\n\\text{Cost}_3 = 2pbq \\text{ flops}\n$$\nThis is another `GEMM` operation.\n\nStep 4: Compute the final update $C \\leftarrow C - W_3$.\nThis operation is an elementwise subtraction of two matrices of size $p \\times q$. According to the problem statement, this costs $pq$ flops.\nThe cost for this step is:\n$$\n\\text{Cost}_4 = pq \\text{ flops}\n$$\n\nThe total flop count is the sum of the costs of these four steps:\n$$\n\\text{Total Flops} = \\text{Cost}_1 + \\text{Cost}_2 + \\text{Cost}_3 + \\text{Cost}_4\n$$\n$$\n\\text{Total Flops} = 2pqb + b^{2}q + 2pqb + pq\n$$\nCombining the like terms, we obtain the final closed-form expression for the total flop count:\n$$\n\\text{Total Flops} = 4pqb + b^{2}q + pq\n$$\nIn typical applications of blocked algorithms, the matrix dimensions $p$ and $q$ are much larger than the block size $b$ (i.e., $p \\gg b$ and $q \\gg b$). The three matrix multiplication steps are all Level-$3$ BLAS operations. Their costs are $2pqb$ (for $V^{\\top} C$), $b^2 q$ (for $T W_1$), and $2pqb$ (for $V W_2$). The two dense matrix-matrix multiplications (`GEMM` operations) have a cost proportional to $pqb$, whereas the triangular matrix multiplication (`TRMM`) has a cost proportional to $b^2 q$. Since $p \\gg b$, the term $pqb$ is much larger than $b^2 q$. Therefore, the dominant costs arise from the two `GEMM` operations, which constitute the bulk of the computational work.",
            "answer": "$$\n\\boxed{4pqb + b^{2}q + pq}\n$$"
        }
    ]
}