{
    "hands_on_practices": [
        {
            "introduction": "Understanding QR factorization with column pivoting begins with the fundamental skill of applying the algorithm and interpreting its output. This first exercise provides a concrete opportunity to determine the numerical rank of a matrix by examining the diagonal elements of its upper triangular factor, $R$. By working through this foundational example , you will solidify your grasp of how the factorization directly reveals the linear independence of the matrix's columns.",
            "id": "1057080",
            "problem": "Consider the matrix $A$ defined as:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 0 & 1 \\\\\n1 & 0 & 0 & 1 \\\\\n0 & 1 & 1 & 0\n\\end{pmatrix}\n$$\nPerform QR factorization with column pivoting on $A$ to determine its numerical rank. The numerical rank is the number of non-zero diagonal elements in the upper triangular matrix $R$ obtained from the factorization, using exact arithmetic (no tolerance threshold). Report this rank.",
            "solution": "The QR factorization with column pivoting of $ A $ yields an upper triangular matrix $ R $ with diagonal entries $ r_{11} = \\sqrt{2} $, $ r_{22} = \\sqrt{2} $, $ r_{33} = 1 $, and $ r_{44} = 0 $. The numerical rank is the number of non-zero diagonal elements in $ R $.\n\n- $ r_{11} = \\sqrt{2} \\neq 0 $\n- $ r_{22} = \\sqrt{2} \\neq 0 $\n- $ r_{33} = 1 \\neq 0 $\n- $ r_{44} = 0 $\n\nThere are 3 non-zero diagonal entries.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "The true power of rank-revealing QR factorization lies in its ability to handle not just exact rank deficiency, but also near-singularity, a common challenge in numerical computations. This exercise moves beyond simple rank counting to an analytical exploration of a nearly rank-deficient matrix parameterized by a small value, $\\epsilon$. By predicting the pattern of the diagonal entries of $R$ as a function of $\\epsilon$ , you will gain a deeper insight into how column pivoting systematically isolates and quantifies the dependencies among columns.",
            "id": "3549747",
            "problem": "Consider the nearly rank-deficient matrix $A(\\epsilon) \\in \\mathbb{R}^{3 \\times 3}$ defined by\n$$\nA(\\epsilon) = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1+\\epsilon \\\\\n1 & 1+\\epsilon & 1\n\\end{bmatrix},\n$$\nwhere $\\epsilon > 0$ is sufficiently small. Perform Householder-based orthonormal-triangular (QR) factorization with column pivoting, meaning you seek $Q \\in \\mathbb{R}^{3 \\times 3}$ orthonormal, $R \\in \\mathbb{R}^{3 \\times 3}$ upper triangular, and a permutation matrix $P \\in \\mathbb{R}^{3 \\times 3}$ such that\n$$\nQ^{\\top} A(\\epsilon) P = R,\n$$\nwith the pivot selection criterion at each step given by the largest column $2$-norm among the remaining (updated) columns; in the event of an exact tie, break the tie by choosing the smallest column index. Adopt the convention that the diagonal entries of $R$ are nonnegative.\n\nStarting only from the definition of a Householder reflector and the column pivoting criterion, carry out the first pivot decision and then analytically predict the pattern of the diagonal magnitudes $|R_{11}|$, $|R_{22}|$, and $|R_{33}|$ produced by the algorithm, expressed exactly in terms of $\\epsilon$. Your final answer must be a single closed-form analytic expression containing $|R_{11}|$, $|R_{22}|$, and $|R_{33}|$ as functions of $\\epsilon$, arranged as a row matrix. No numerical approximation or rounding is required.",
            "solution": "The problem asks for the magnitudes of the diagonal entries $|R_{11}|$, $|R_{22}|$, and $|R_{33}|$ of the upper triangular matrix $R$ obtained from the Householder QR factorization with column pivoting of the matrix $A(\\epsilon)$. The factorization is of the form $Q^{\\top} A(\\epsilon) P = R$, where $Q$ is orthonormal, $P$ is a permutation matrix, and $R$ is upper triangular with non-negative diagonal entries.\n\nThe given matrix is:\n$$\nA(\\epsilon) = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1+\\epsilon \\\\\n1 & 1+\\epsilon & 1\n\\end{bmatrix}\n$$\nLet the columns of $A(\\epsilon)$ be denoted $a_1, a_2, a_3$.\n\n**Step 1: First Pivot Selection**\nThe column pivoting strategy requires selecting the column with the largest $2$-norm. We compute the squared $2$-norms of the columns of $A(\\epsilon)$:\n$$\n||a_1||_2^2 = 1^2 + 1^2 + 1^2 = 3\n$$\n$$\n||a_2||_2^2 = 1^2 + 1^2 + (1+\\epsilon)^2 = 1 + 1 + 1 + 2\\epsilon + \\epsilon^2 = 3 + 2\\epsilon + \\epsilon^2\n$$\n$$\n||a_3||_2^2 = 1^2 + (1+\\epsilon)^2 + 1^2 = 1 + 1 + 2\\epsilon + \\epsilon^2 + 1 = 3 + 2\\epsilon + \\epsilon^2\n$$\nSince $\\epsilon > 0$, we have $3 + 2\\epsilon + \\epsilon^2 > 3$. Therefore, $||a_2||_2$ and $||a_3||_2$ are both larger than $||a_1||_2$. We have a tie between column $2$ and column $3$. The tie-breaking rule specifies choosing the column with the smallest index, so we select column $a_2$ as the first pivot.\n\nThis requires swapping column $1$ and column $2$. The permutation matrix is $P_1 = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$. The matrix to be transformed in the first step is $A' = A(\\epsilon)P_1$:\n$$\nA' = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1+\\epsilon \\\\\n1+\\epsilon & 1 & 1\n\\end{bmatrix}\n$$\nThe first column of $A'$ is the pivot column, which is the original $a_2$. The first diagonal element of $R$, denoted $R_{11}$, is the $2$-norm of this pivot column. According to the convention, $R_{11} \\ge 0$.\n$$\n|R_{11}| = R_{11} = ||a_2||_2 = \\sqrt{3 + 2\\epsilon + \\epsilon^2}\n$$\n\n**Step 2: Second Pivot Selection**\nThe first Householder transformation $Q_1^{\\top}$ is applied to $A'$, resulting in $A^{(2)} = Q_1^{\\top} A' = Q_1^{\\top} A(\\epsilon) P_1$. This matrix has the structure:\n$$\nA^{(2)} = \\begin{bmatrix}\nR_{11} & R_{12} & R_{13} \\\\\n0 & \\cdot & \\cdot \\\\\n0 & \\cdot & \\cdot\n\\end{bmatrix}\n$$\nwhere $M_{2\\times2}$ is a $2 \\times 2$ submatrix whose columns need to be evaluated for the next pivot selection. Let the columns of the submatrix $M_{2\\times2}$ be $\\tilde{c}_2$ and $\\tilde{c}_3$. These correspond to the updated versions of the second and third columns of $A'$.\n\nThe lengths of these sub-columns can be calculated without explicitly forming the Householder matrix. Since the transformation $Q_1^{\\top}$ is orthogonal, it preserves column norms. The original second and third columns of $A'$ are $a_1$ and $a_3$, respectively. After transformation, let them be $c_2 = Q_1^{\\top} a_1$ and $c_3 = Q_1^{\\top} a_3$. We have $c_2 = [R_{12}, \\tilde{c}_2^\\top]^\\top$ and $c_3 = [R_{13}, \\tilde{c}_3^\\top]^\\top$. From conservation of norm:\n$$\n||\\tilde{c}_2||_2^2 = ||c_2||_2^2 - R_{12}^2 = ||a_1||_2^2 - R_{12}^2\n$$\n$$\n||\\tilde{c}_3||_2^2 = ||c_3||_2^2 - R_{13}^2 = ||a_3||_2^2 - R_{13}^2\n$$\nThe entries $R_{12}$ and $R_{13}$ are the projections of $a_1$ and $a_3$ onto the normalized pivot column $a_2/||a_2||_2$.\n$$\nR_{12} = \\frac{\\langle a_2, a_1 \\rangle}{||a_2||_2} = \\frac{1 \\cdot 1 + 1 \\cdot 1 + (1+\\epsilon) \\cdot 1}{\\sqrt{3+2\\epsilon+\\epsilon^2}} = \\frac{3+\\epsilon}{\\sqrt{3+2\\epsilon+\\epsilon^2}}\n$$\n$$\nR_{13} = \\frac{\\langle a_2, a_3 \\rangle}{||a_2||_2} = \\frac{1 \\cdot 1 + 1 \\cdot (1+\\epsilon) + (1+\\epsilon) \\cdot 1}{\\sqrt{3+2\\epsilon+\\epsilon^2}} = \\frac{1+1+\\epsilon+1+\\epsilon}{\\sqrt{3+2\\epsilon+\\epsilon^2}} = \\frac{3+2\\epsilon}{\\sqrt{3+2\\epsilon+\\epsilon^2}}\n$$\nSubstituting these into the norm equations:\n$$\n||\\tilde{c}_2||_2^2 = 3 - \\left(\\frac{3+\\epsilon}{\\sqrt{3+2\\epsilon+\\epsilon^2}}\\right)^2 = \\frac{3(3+2\\epsilon+\\epsilon^2) - (9+6\\epsilon+\\epsilon^2)}{3+2\\epsilon+\\epsilon^2} = \\frac{2\\epsilon^2}{3+2\\epsilon+\\epsilon^2}\n$$\n$$\n||\\tilde{c}_3||_2^2 = (3+2\\epsilon+\\epsilon^2) - \\left(\\frac{3+2\\epsilon}{\\sqrt{3+2\\epsilon+\\epsilon^2}}\\right)^2 = \\frac{(3+2\\epsilon+\\epsilon^2)^2 - (3+2\\epsilon)^2}{3+2\\epsilon+\\epsilon^2}\n$$\nUsing the difference of squares identity $u^2-v^2=(u-v)(u+v)$ on the numerator with $u=3+2\\epsilon+\\epsilon^2$ and $v=3+2\\epsilon$:\n$$\n||\\tilde{c}_3||_2^2 = \\frac{(\\epsilon^2)(6+4\\epsilon+\\epsilon^2)}{3+2\\epsilon+\\epsilon^2}\n$$\nTo select the second pivot, we compare $||\\tilde{c}_2||_2^2$ and $||\\tilde{c}_3||_2^2$. Since $\\epsilon > 0$, we only need to compare the numerators $2\\epsilon^2$ and $\\epsilon^2(6+4\\epsilon+\\epsilon^2)$, which simplifies to comparing $2$ and $6+4\\epsilon+\\epsilon^2$. As $\\epsilon > 0$, it is clear that $6+4\\epsilon+\\epsilon^2 > 2$.\nThus, $||\\tilde{c}_3||_2 > ||\\tilde{c}_2||_2$, and the second pivot column is $\\tilde{c}_3$. This corresponds to the third column of $A'$, which was the original $a_3$. The second diagonal element of $R$ is therefore:\n$$\n|R_{22}| = R_{22} = ||\\tilde{c}_3||_2 = \\sqrt{\\frac{\\epsilon^2(6+4\\epsilon+\\epsilon^2)}{3+2\\epsilon+\\epsilon^2}} = \\frac{\\epsilon\\sqrt{6+4\\epsilon+\\epsilon^2}}{\\sqrt{3+2\\epsilon+\\epsilon^2}}\n$$\n\n**Step 3: Determining the Third Diagonal Element**\nWe can determine $|R_{33}|$ without carrying out the second Householder transformation explicitly by using the property of determinants. For the factorization $AP=QR$ (or $Q^\\top AP=R$), we have:\n$$\n\\det(A) \\det(P) = \\det(Q) \\det(R)\n$$\nThe determinant of a permutation matrix $\\det(P)$ is $\\pm 1$. The determinant of an orthogonal matrix $\\det(Q)$ is also $\\pm 1$. Taking the absolute value of both sides gives:\n$$\n|\\det(A)| = |\\det(R)|\n$$\nThe determinant of an upper triangular matrix is the product of its diagonal entries: $\\det(R) = R_{11}R_{22}R_{33}$. Since we have the convention $R_{ii} \\ge 0$, this means $|\\det(R)| = R_{11}R_{22}R_{33}$.\nLet's compute the determinant of $A(\\epsilon)$:\n$$\n\\det(A(\\epsilon)) = \\det \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1+\\epsilon \\\\\n1 & 1+\\epsilon & 1\n\\end{bmatrix} = 1(1(1) - (1+\\epsilon)^2) - 1(1(1) - 1(1+\\epsilon)) + 1(1(1+\\epsilon) - 1(1))\n$$\n$$\n= (1 - (1+2\\epsilon+\\epsilon^2)) - (1 - 1 - \\epsilon) + (1+\\epsilon - 1) = (-2\\epsilon - \\epsilon^2) - (-\\epsilon) + \\epsilon = -2\\epsilon - \\epsilon^2 + 2\\epsilon = -\\epsilon^2\n$$\nTherefore, $|\\det(A(\\epsilon))| = \\epsilon^2$. We have:\n$$\nR_{11} R_{22} R_{33} = \\epsilon^2\n$$\nSo, $|R_{33}| = R_{33} = \\frac{\\epsilon^2}{R_{11}R_{22}}$. Substituting the expressions for $R_{11}$ and $R_{22}$:\n$$\nR_{11} R_{22} = \\left(\\sqrt{3+2\\epsilon+\\epsilon^2}\\right) \\left(\\frac{\\epsilon\\sqrt{6+4\\epsilon+\\epsilon^2}}{\\sqrt{3+2\\epsilon+\\epsilon^2}}\\right) = \\epsilon\\sqrt{6+4\\epsilon+\\epsilon^2}\n$$\nFinally,\n$$\n|R_{33}| = R_{33} = \\frac{\\epsilon^2}{\\epsilon\\sqrt{6+4\\epsilon+\\epsilon^2}} = \\frac{\\epsilon}{\\sqrt{6+4\\epsilon+\\epsilon^2}}\n$$\n\nThe magnitudes of the diagonal elements are:\n$|R_{11}| = \\sqrt{3+2\\epsilon+\\epsilon^2}$\n$|R_{22}| = \\frac{\\epsilon\\sqrt{6+4\\epsilon+\\epsilon^2}}{\\sqrt{3+2\\epsilon+\\epsilon^2}}$\n$|R_{33}| = \\frac{\\epsilon}{\\sqrt{6+4\\epsilon+\\epsilon^2}}$\nArranging these in a row matrix as requested gives the final result.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{3+2\\epsilon+\\epsilon^2} & \\frac{\\epsilon\\sqrt{6+4\\epsilon+\\epsilon^2}}{\\sqrt{3+2\\epsilon+\\epsilon^2}} & \\frac{\\epsilon}{\\sqrt{6+4\\epsilon+\\epsilon^2}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having explored the theoretical and analytical aspects, this final practice focuses on a critical application: solving ill-conditioned linear least squares problems. You will implement a computational experiment to contrast the robust, stable solution obtained using QR with column pivoting against the failure of the normal equations method when columns are nearly collinear. This problem  demonstrates why rank-revealing QR is an indispensable tool in scientific computing for producing reliable results from imperfect data.",
            "id": "3275467",
            "problem": "You will implement a program to demonstrate, by construction and measurement, how the normal equations formulation for linear least squares can lose rank numerically when two columns of the design matrix are nearly identical, whereas a rank-revealing factorization based on QR factorization with column pivoting can still identify the correct numerical rank and yield a stable least squares solution. Work entirely in pure numerical linear algebra terms.\n\nConsider the least squares problem of minimizing the Euclidean norm of the residual for an overdetermined linear system,\n$$\n\\min_{x \\in \\mathbb{R}^n} \\| A x - b \\|_2,\n$$\nwith a synthetic matrix $A \\in \\mathbb{R}^{m \\times n}$ constructed as follows. Let $m = 5$ and $n = 3$. Define the canonical basis vectors $e_1, e_2, e_3 \\in \\mathbb{R}^5$. For a given parameter $\\epsilon \\ge 0$, set\n$$\nc_1 = e_1, \\quad c_2 = e_1 + \\epsilon\\, e_2, \\quad c_3 = e_3,\n$$\nand assemble\n$$\nA(\\epsilon) = \\begin{bmatrix} c_1 & c_2 & c_3 \\end{bmatrix} \\in \\mathbb{R}^{5 \\times 3}.\n$$\nTake the right-hand side vector\n$$\nb = \\begin{bmatrix} 1 \\\\ -3 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^5.\n$$\n\nFundamental base for the derivation:\n- By the first-order optimality conditions for least squares, any minimizer $x$ satisfies the normal equations\n$$\nA^\\top A \\, x = A^\\top b.\n$$\n- The condition number in the $2$-norm is defined as $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$ for a full-rank matrix $M$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values.\n- For any $A$, the singular values of $A^\\top A$ are the squares of those of $A$, so $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$ when $A$ has full column rank.\n- A QR factorization with column pivoting is a factorization of the form $A P = Q R$, where $P$ is a permutation matrix, $Q$ has orthonormal columns, and $R$ is upper triangular. The magnitude of the diagonal entries of $R$ reveals a numerical rank $r$ when compared against a tolerance $\\tau = \\max(m,n)\\,\\varepsilon_{\\text{mach}}\\,\\|A\\|_2$, where $\\varepsilon_{\\text{mach}}$ is machine epsilon for floating-point arithmetic.\n\nYour tasks:\n1. Implement a rank-revealing QR with column pivoting using Modified Gramâ€“Schmidt to compute $Q$, $R$, and a permutation that encodes $P$, along with a numerical rank $r$ determined by comparing the diagonal entries of $R$ against the tolerance $\\tau$ given above. Use double-precision floating point arithmetic.\n2. Compute the following quantities for each specified value of $\\epsilon$:\n   - The rank of $A(\\epsilon)$ computed via singular value decomposition, denoted $\\mathrm{rank}(A)$.\n   - The rank of $A(\\epsilon)^\\top A(\\epsilon)$ computed via singular value decomposition, denoted $\\mathrm{rank}(A^\\top A)$.\n   - The numerical rank $r_{\\mathrm{QR}}$ identified by your QR with column pivoting as described above.\n   - The $2$-norm condition numbers $\\kappa_2(A)$ and $\\kappa_2(A^\\top A)$.\n   - A least squares solution computed via the normal equations by explicitly forming $G = A^\\top A$ and solving $G x = A^\\top b$ with a direct linear solver. If $\\mathrm{rank}(G) < n$, treat the system as numerically singular and do not solve; in that case, report a special value as described below.\n   - A least squares solution computed via your QR with column pivoting. Use the numerical rank $r_{\\mathrm{QR}}$ to solve the reduced triangular system $R_{1:r,1:r} y = (Q^\\top b)_{1:r}$, then map back to the original variable order via the permutation to obtain $x_{\\mathrm{QR}}$ with the remaining components set to zero. This yields a minimum-norm least squares solution consistent with the detected numerical rank.\n3. For each case, report the residual norms and solution norms:\n   - The residual norm for the normal equations solution, $\\|A x_{\\mathrm{NE}} - b\\|_2$. If $\\mathrm{rank}(A^\\top A) < n$, report this residual as the floating-point NaN (Not a Number).\n   - The residual norm for the QR solution, $\\|A x_{\\mathrm{QR}} - b\\|_2$.\n   - The Euclidean norm of the QR solution, $\\|x_{\\mathrm{QR}}\\|_2$.\n4. Use the following test suite for $\\epsilon$:\n   - $\\epsilon = 10^{-8}$, representing two columns that differ by $10^{-8}$.\n   - $\\epsilon = 0$, representing two exactly identical columns (a true rank deficiency).\n   - $\\epsilon = 10^{-4}$, representing a milder near-collinearity.\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list of three items, each item being a list for one test case in the order of the test suite above.\n   - For each test case, output the list\n     $$\n     [\\mathrm{rank}(A),\\ \\mathrm{rank}(A^\\top A),\\ r_{\\mathrm{QR}},\\ \\kappa_2(A),\\ \\kappa_2(A^\\top A),\\ \\|A x_{\\mathrm{NE}} - b\\|_2,\\ \\|A x_{\\mathrm{QR}} - b\\|_2,\\ \\|x_{\\mathrm{QR}}\\|_2].\n     $$\n   - If $\\mathrm{rank}(A^\\top A) < n$, output the normal-equations residual as NaN as described earlier. All entries must be basic numeric types (integers or floating-point numbers). The entire output must be a single Python-style list-of-lists on one line, for example: [[...],[...],[...]].\n\nNotes:\n- Angles are not involved; no angle unit is required.\n- There are no physical units; report plain scalar values.\n- Ensure that your implementation is deterministic and uses only double-precision arithmetic for the core computations unless otherwise specified by the language defaults.",
            "solution": "The problem has been validated and is determined to be a valid, well-posed problem in numerical linear algebra. It is scientifically grounded, objective, and contains all necessary information to proceed.\n\nThe task is to demonstrate the superior numerical stability of solving linear least squares problems using a rank-revealing QR factorization with column pivoting compared to the traditional normal equations method, especially when the design matrix $A$ has nearly collinear columns.\n\nThe linear least squares problem is to find $x \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual, $\\|Ax - b\\|_2$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$.\n\nThe specific matrix $A(\\epsilon)$ is constructed for $m=5$ and $n=3$ with columns $c_1, c_2, c_3 \\in \\mathbb{R}^5$ as follows:\n$$\nc_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\nc_2 = \\begin{bmatrix} 1 \\\\ \\epsilon \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\nc_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nThe matrix $A(\\epsilon)$ is formed by these columns, $A(\\epsilon) = \\begin{bmatrix} c_1 & c_2 & c_3 \\end{bmatrix}$. The right-hand side vector is given as $b = [1, -3, 2, 0, 0]^\\top$.\n\nWhen the parameter $\\epsilon$ is close to $0$, the columns $c_1$ and $c_2$ become nearly linearly dependent. This makes the matrix $A(\\epsilon)$ ill-conditioned. The analysis will be performed for $\\epsilon \\in \\{10^{-8}, 0, 10^{-4}\\}$.\n\n### Methodology\n\nTwo methods will be implemented and compared.\n\n**1. Normal Equations (NE)**\nThe solution to the least squares problem must satisfy the normal equations:\n$$\nA^\\top A x = A^\\top b.\n$$\nThis method involves explicitly forming the Gram matrix $G = A^\\top A$ and a modified right-hand side $A^\\top b$, then solving the $n \\times n$ linear system $Gx = A^\\top b$. A crucial issue with this method is that the condition number of $G$ is the square of the condition number of $A$, i.e., $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$. This squaring can lead to a significant loss of numerical precision and can make a computationally full-rank problem appear singular.\n\nFor each $\\epsilon$, we first compute the rank of $G = A^\\top A$ using a standard singular value decomposition (SVD) based method. If $\\mathrm{rank}(G) < n$, the matrix is considered numerically singular, and we will not attempt to solve the system. In this case, the residual norm $\\|A x_{\\mathrm{NE}} - b\\|_2$ will be reported as Not a Number (NaN). Otherwise, we solve the system for $x_{\\mathrm{NE}}$ and compute the corresponding residual norm.\n\n**2. QR Factorization with Column Pivoting**\nA more numerically stable approach is to use a QR factorization of $A$. Specifically, we will use a rank-revealing QR factorization with column pivoting, which takes the form:\n$$\nAP = QR,\n$$\nwhere $P$ is a permutation matrix, $Q$ is a matrix with orthonormal columns, and $R$ is an upper triangular matrix. The least squares problem becomes $\\min_x \\|Q R P^\\top x - b\\|_2$. Letting $y = P^\\top x$, the problem is equivalent to $\\min_y \\|Ry - Q^\\top b\\|_2$, since multiplying by the orthogonal matrix $Q^\\top$ does not change the Euclidean norm.\n\nThe implementation will use the Modified Gram-Schmidt (MGS) algorithm with column pivoting. At each step $k$ of the factorization, the column with the largest remaining Euclidean norm is chosen as the pivot, swapped into the $k$-th position, and then used to form the $k$-th column of $Q$ and $k$-th row of $R$.\n\nA key aspect is the determination of the numerical rank, $r_{\\mathrm{QR}}$. This is done by comparing the magnitude of the diagonal elements of $R$ against a tolerance $\\tau$:\n$$\n\\tau = \\max(m,n)\\,\\varepsilon_{\\text{mach}}\\,\\|A\\|_2,\n$$\nwhere $\\varepsilon_{\\text{mach}}$ is the machine epsilon for double-precision floating-point arithmetic. If $|R_{kk}| < \\tau$, the matrix is considered numerically rank-deficient at step $k$, and the numerical rank is set to $r_{\\mathrm{QR}} = k$.\n\nOnce $Q$, $R$, the permutation $P$ (represented by a vector of indices), and the numerical rank $r_{\\mathrm{QR}}$ are found, the solution $x_{\\mathrm{QR}}$ is computed. We solve the reduced-rank, upper-triangular system:\n$$\nR_{1:r, 1:r} \\, y_{1:r} = (Q^\\top b)_{1:r},\n$$\nwhere $r=r_{\\mathrm{QR}}$, using back substitution. The solution vector $y$ of size $n$ is formed by setting its first $r$ components to $y_{1:r}$ and the remaining $n-r$ components to zero. This corresponds to finding a basic solution. Finally, the permutation is reversed to find the solution in the original coordinates: $x_{\\mathrm{QR}} = Py$. This is achieved by assigning the components of $y$ to $x_{\\mathrm{QR}}$ according to the permutation vector.\n\n### Computed Quantities\nFor each value of $\\epsilon$, the following quantities will be calculated and reported:\n- $\\mathrm{rank}(A)$: Rank of $A(\\epsilon)$ via SVD.\n- $\\mathrm{rank}(A^\\top A)$: Rank of $A(\\epsilon)^\\top A(\\epsilon)$ via SVD.\n- $r_{\\mathrm{QR}}$: Numerical rank from our QR with column pivoting implementation.\n- $\\kappa_2(A)$: The $2$-norm condition number of $A(\\epsilon)$.\n- $\\kappa_2(A^\\top A)$: The $2$-norm condition number of $A(\\epsilon)^\\top A(\\epsilon)$.\n- $\\|A x_{\\mathrm{NE}} - b\\|_2$: The residual norm for the normal equations solution (or NaN).\n- $\\|A x_{\\mathrm{QR}} - b\\|_2$: The residual norm for the QR solution.\n- $\\|x_{\\mathrm{QR}}\\|_2$: The Euclidean norm of the QR solution.\n\nThis comparative analysis is designed to highlight the numerical pitfalls of the normal equations and the robustness of a rank-revealing QR approach for solving least squares problems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef qr_pivot_mgs(A):\n    \"\"\"\n    Computes a rank-revealing QR factorization with column pivoting using\n    Modified Gram-Schmidt. The factorization is of the form AP = QR.\n\n    Args:\n        A (np.ndarray): The matrix to factorize, of size m x n.\n\n    Returns:\n        Q (np.ndarray): m x n matrix with orthonormal columns.\n        R (np.ndarray): n x n upper triangular matrix.\n        p (list): Permutation vector of length n. A_permuted[:, k] is A[:, p[k]].\n        rank (int): Numerical rank of the matrix.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n    p = list(range(n))\n\n    A_norm = np.linalg.norm(A, 2)\n    # Handle the case of a zero matrix for robustness\n    if A_norm == 0:\n        return Q, R, p, 0\n    \n    tol = max(m, n) * np.finfo(float).eps * A_norm\n    \n    numerical_rank = n  # Assume full rank initially\n\n    for k in range(n):\n        # Find the column with the largest 2-norm in the remaining submatrix V[:, k:]\n        col_norms = np.linalg.norm(V[:, k:], axis=0)\n        best_col_idx_local = np.argmax(col_norms)\n        best_col_idx_global = k + best_col_idx_local\n        \n        # Swap columns in V and update the permutation vector p\n        if best_col_idx_global != k:\n            V[:, [k, best_col_idx_global]] = V[:, [best_col_idx_global, k]]\n            p[k], p[best_col_idx_global] = p[best_col_idx_global], p[k]\n            \n        # The diagonal element R[k, k] is the norm of the current pivot column\n        R[k, k] = np.linalg.norm(V[:, k])\n\n        # Check for rank deficiency against the tolerance\n        if R[k, k] < tol:\n            numerical_rank = k\n            break\n\n        # Normalize the k-th column of V to get the k-th column of Q\n        Q[:, k] = V[:, k] / R[k, k]\n\n        # Orthogonalize the remaining columns of V against the new basis vector Q[:, k]\n        for j in range(k + 1, n):\n            R[k, j] = np.dot(Q[:, k], V[:, j])\n            V[:, j] -= R[k, j] * Q[:, k]\n            \n    return Q, R, p, numerical_rank\n\ndef back_substitution(R, c):\n    \"\"\"Solves Rx = c for an upper triangular matrix R.\"\"\"\n    n = R.shape[0]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        if R[i, i] == 0:\n            # This should ideally not be reached if the system is well-posed.\n            # R is from a rank-revealing QR, so R[i,i] > tol > 0.\n            raise np.linalg.LinAlgError(\"Singular matrix in back substitution.\")\n        dot_product = np.dot(R[i, i+1:], x[i+1:])\n        x[i] = (c[i] - dot_product) / R[i, i]\n    return x\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment for the specified epsilon values.\n    \"\"\"\n    test_cases = [1e-8, 0.0, 1e-4]\n    m, n = 5, 3\n    b = np.array([1., -3., 2., 0., 0.])\n    \n    results = []\n\n    for epsilon in test_cases:\n        # 1. Construct the matrix A for the given epsilon\n        A = np.zeros((m, n), dtype=float)\n        A[0, 0] = 1.0\n        A[0, 1] = 1.0\n        A[1, 1] = epsilon\n        A[2, 2] = 1.0\n\n        # 2. Compute ranks and condition numbers\n        rank_A = np.linalg.matrix_rank(A)\n        G = A.T @ A\n        rank_G = np.linalg.matrix_rank(G)\n        \n        # Condition number is Inf for singular matrices\n        cond_A = np.linalg.cond(A, 2)\n        cond_G = np.linalg.cond(G, 2)\n      \n        # 3. Solve via Normal Equations\n        if rank_G < n:\n            res_norm_NE = np.nan\n        else:\n            try:\n                x_NE = np.linalg.solve(G, A.T @ b)\n                res_norm_NE = np.linalg.norm(A @ x_NE - b, 2)\n            except np.linalg.LinAlgError:\n                # Fails if G is singular despite rank check, due to floating point limits\n                res_norm_NE = np.nan\n\n        # 4. Solve via QR with column pivoting\n        Q, R, p_indices, r_QR = qr_pivot_mgs(A)\n        \n        c = Q.T @ b\n        \n        y = np.zeros(n)\n        if r_QR > 0:\n            # Solve the reduced upper-triangular system\n            R_r = R[:r_QR, :r_QR]\n            c_r = c[:r_QR]\n            y_r = back_substitution(R_r, c_r)\n            y[:r_QR] = y_r\n        \n        # Un-permute the solution vector y to get x_QR\n        x_QR = np.zeros(n)\n        x_QR[p_indices] = y\n        \n        res_norm_QR = np.linalg.norm(A @ x_QR - b, 2)\n        sol_norm_QR = np.linalg.norm(x_QR, 2)\n\n        # 5. Assemble and store results for this case\n        case_result = [\n            rank_A,\n            rank_G,\n            r_QR,\n            cond_A,\n            cond_G,\n            res_norm_NE,\n            res_norm_QR,\n            sol_norm_QR\n        ]\n        results.append(case_result)\n        \n    # Print the final list of lists in the required format.\n    # The str() function provides the specified \"Python-style list-of-lists\" format.\n    print(str(results))\n\nsolve()\n```"
        }
    ]
}