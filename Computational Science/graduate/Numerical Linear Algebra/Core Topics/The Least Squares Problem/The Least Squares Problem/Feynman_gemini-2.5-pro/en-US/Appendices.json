{
    "hands_on_practices": [
        {
            "introduction": "Solving the least squares problem $\\min_{x} \\|b - Ax\\|_{2}$ demands an understanding of numerically stable methods. This first practice moves beyond the potentially unstable normal equations to the robust QR factorization method . By deriving the solution from first principles, you will solidify your understanding of how orthogonal transformations and the decomposition $A=QR$ transform the problem into an easily solved triangular system.",
            "id": "3590957",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 2}$ and $b \\in \\mathbb{R}^{4}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1\\\\\n1  0\\\\\n0  1\\\\\n0  0\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix}\n2\\\\\n1\\\\\n0\\\\\n0\n\\end{pmatrix}.\n$$\nConsider the linear least squares (LS) problem of minimizing the Euclidean norm of the residual,\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\; \\|\\,b - A x\\,\\|_{2}.\n$$\nStarting from the definitions of orthonormality and orthogonal projection, and without invoking any pre-stated least squares formula, proceed as follows:\n- Compute a reduced $Q$–$R$ factorization (QR) of $A$, namely $A = Q R$ with $Q \\in \\mathbb{R}^{4 \\times 2}$ having orthonormal columns and $R \\in \\mathbb{R}^{2 \\times 2}$ upper triangular with positive diagonal.\n- Using only the invariance of the Euclidean norm under multiplication by an orthogonal projector and the orthogonality of complementary subspaces associated with $Q$, derive from first principles the characterization of the minimizer in terms of a triangular system arising from the reduced $Q$–$R$ factorization.\n- Then, for the specific $A$ and $b$ above, use your derived characterization to compute the minimal value of the objective function $\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}^{2}$ as an exact number.\n\nYour final answer must be the exact value of the minimal squared residual $\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}^{2}$. Do not round.",
            "solution": "We begin with the linear least squares (LS) problem\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}.\n$$\nLet $A = Q R$ be a reduced $Q$–$R$ factorization (QR) with $Q \\in \\mathbb{R}^{4 \\times 2}$ having orthonormal columns and $R \\in \\mathbb{R}^{2 \\times 2}$ upper triangular with positive diagonal. By definition, the columns of $Q$ are orthonormal, so $Q^{T} Q = I_{2}$, and the matrix $P \\coloneqq Q Q^{T}$ is the orthogonal projector onto $\\mathcal{R}(Q) = \\mathcal{R}(A)$.\n\nFor any $x \\in \\mathbb{R}^{2}$, decompose the residual using the orthogonal splitting of $\\mathbb{R}^{4}$ into $\\mathcal{R}(Q)$ and $\\mathcal{R}(Q)^{\\perp}$:\n$$\nb - A x \\;=\\; b - Q R x \\;=\\; (I - Q Q^{T}) b \\;+\\; Q \\bigl(Q^{T} b - R x\\bigr).\n$$\nThe two terms on the right-hand side lie in orthogonal subspaces: $(I - Q Q^{T}) b \\in \\mathcal{R}(Q)^{\\perp}$ and $Q \\bigl(Q^{T} b - R x\\bigr) \\in \\mathcal{R}(Q)$. Therefore, by the Pythagorean theorem,\n$$\n\\|\\,b - A x\\,\\|_{2}^{2} \\;=\\; \\|\\, (I - Q Q^{T}) b \\,\\|_{2}^{2} \\;+\\; \\|\\, Q \\bigl(Q^{T} b - R x\\bigr) \\,\\|_{2}^{2}.\n$$\nUsing the fact that $Q$ has orthonormal columns, we have $\\|\\,Q y\\,\\|_{2} = \\|\\,y\\,\\|_{2}$ for all $y \\in \\mathbb{R}^{2}$, so\n$$\n\\|\\,b - A x\\,\\|_{2}^{2} \\;=\\; \\|\\, (I - Q Q^{T}) b \\,\\|_{2}^{2} \\;+\\; \\|\\, Q^{T} b - R x \\,\\|_{2}^{2}.\n$$\nThe first term does not depend on $x$, and the second term is minimized if and only if\n$$\nR x \\;=\\; Q^{T} b,\n$$\nwhich is a $2 \\times 2$ upper triangular system. When $A$ has full column rank, $R$ is invertible, and the unique minimizer exists and is given by solving this triangular system. At the minimizer $x_{\\star}$, the minimal squared residual is\n$$\n\\|\\,b - A x_{\\star}\\,\\|_{2}^{2} \\;=\\; \\|\\, (I - Q Q^{T}) b \\,\\|_{2}^{2} \\;=\\; \\|\\,b\\,\\|_{2}^{2} - \\|\\,Q^{T} b\\,\\|_{2}^{2}.\n$$\n\nWe now compute a reduced $Q$–$R$ factorization of the specific $A$ by classical Gram–Schmidt, starting from the definitions.\n\nLet the columns of $A$ be $a_{1} = \\begin{pmatrix}1\\\\1\\\\0\\\\0\\end{pmatrix}$ and $a_{2} = \\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix}$.\n- Compute $r_{11} = \\|\\,a_{1}\\,\\|_{2} = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}$ and $q_{1} = a_{1} / r_{11} = \\begin{pmatrix}1/\\sqrt{2}\\\\1/\\sqrt{2}\\\\0\\\\0\\end{pmatrix}$.\n- Compute $r_{12} = q_{1}^{T} a_{2} = \\begin{pmatrix}1/\\sqrt{2}  1/\\sqrt{2}  0  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix} = \\tfrac{1}{\\sqrt{2}}$.\n- Form $u_{2} = a_{2} - r_{12} q_{1} = \\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix} - \\tfrac{1}{\\sqrt{2}} \\begin{pmatrix}1/\\sqrt{2}\\\\1/\\sqrt{2}\\\\0\\\\0\\end{pmatrix} = \\begin{pmatrix}1/2\\\\-1/2\\\\1\\\\0\\end{pmatrix}$.\n- Compute $r_{22} = \\|\\,u_{2}\\,\\|_{2} = \\sqrt{(1/2)^{2} + (-1/2)^{2} + 1^{2}} = \\sqrt{\\tfrac{3}{2}} = \\tfrac{\\sqrt{3}}{\\sqrt{2}}$ and $q_{2} = u_{2} / r_{22}$.\n\nThus\n$$\nQ \\;=\\; \\begin{pmatrix} q_{1}  q_{2} \\end{pmatrix}, \n\\qquad\nR \\;=\\; \\begin{pmatrix}\nr_{11}  r_{12}\\\\\n0  r_{22}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\sqrt{2}  \\tfrac{1}{\\sqrt{2}}\\\\\n0  \\tfrac{\\sqrt{3}}{\\sqrt{2}}\n\\end{pmatrix}.\n$$\n\nNext compute $Q^{T} b$. First,\n$$\nq_{1}^{T} b \\;=\\; \\begin{pmatrix}1/\\sqrt{2}  1/\\sqrt{2}  0  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\0\\\\0\\end{pmatrix} \\;=\\; \\tfrac{3}{\\sqrt{2}}.\n$$\nAlso,\n$$\nq_{2}^{T} b \\;=\\; \\frac{u_{2}^{T} b}{r_{22}} \\;=\\; \\frac{\\begin{pmatrix}1/2  -1/2  1  0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\0\\\\0\\end{pmatrix}}{r_{22}} \\;=\\; \\frac{1 - \\tfrac{1}{2} + 0}{\\tfrac{\\sqrt{3}}{\\sqrt{2}}} \\;=\\; \\frac{\\tfrac{1}{2}}{\\tfrac{\\sqrt{3}}{\\sqrt{2}}} \\;=\\; \\frac{\\sqrt{2}}{2 \\sqrt{3}} \\;=\\; \\frac{1}{\\sqrt{6}}.\n$$\nTherefore,\n$$\nQ^{T} b \\;=\\; \\begin{pmatrix} \\tfrac{3}{\\sqrt{2}} \\\\ \\tfrac{1}{\\sqrt{6}} \\end{pmatrix}.\n$$\n\nBy the characterization derived earlier, the minimizer $x_{\\star}$ satisfies $R x_{\\star} = Q^{T} b$. Solving the upper triangular system,\n$$\n\\frac{\\sqrt{3}}{\\sqrt{2}} \\, x_{2} \\;=\\; \\frac{1}{\\sqrt{6}} \\;\\;\\Rightarrow\\;\\; x_{2} \\;=\\; \\frac{1}{3},\n$$\nand\n$$\n\\sqrt{2} \\, x_{1} \\;+\\; \\frac{1}{\\sqrt{2}} \\, x_{2} \\;=\\; \\frac{3}{\\sqrt{2}}\n\\;\\;\\Rightarrow\\;\\;\nx_{1} \\;=\\; \\frac{\\tfrac{3}{\\sqrt{2}} - \\tfrac{1}{\\sqrt{2}} \\cdot \\tfrac{1}{3}}{\\sqrt{2}} \n\\;=\\; \\frac{4}{3}.\n$$\n\nFinally, the minimal squared residual is\n$$\n\\|\\,b - A x_{\\star}\\,\\|_{2}^{2} \\;=\\; \\|\\,b\\,\\|_{2}^{2} \\;-\\; \\|\\,Q^{T} b\\,\\|_{2}^{2} \\;=\\; \\left(2^{2} + 1^{2}\\right) \\;-\\; \\left(\\left(\\tfrac{3}{\\sqrt{2}}\\right)^{2} + \\left(\\tfrac{1}{\\sqrt{6}}\\right)^{2}\\right)\n\\;=\\; 5 \\;-\\; \\left(\\tfrac{9}{2} + \\tfrac{1}{6}\\right)\n\\;=\\; 5 \\;-\\; \\tfrac{14}{3}\n\\;=\\; \\tfrac{1}{3}.\n$$\nTherefore, the exact minimal value of the objective function $\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}^{2}$ is $\\tfrac{1}{3}$.",
            "answer": "$$\\boxed{\\tfrac{1}{3}}$$"
        },
        {
            "introduction": "After finding a least squares solution, a critical question arises: is it the only one? This practice delves into the fundamental theory of uniqueness, connecting the geometric picture of orthogonal projection to the algebraic properties of the matrix $A$ . You will derive the conditions for a unique minimizer based on the rank of $A$ and characterize the entire affine subspace of solutions when the minimizer is not unique.",
            "id": "3590981",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times m}$ and $b \\in \\mathbb{R}^{n}$. Consider the least squares problem of minimizing the squared Euclidean norm $x \\mapsto \\|A x - b\\|_{2}^{2}$ over $x \\in \\mathbb{R}^{m}$. Starting from the fundamental definition of least squares as the search for a point in the column space of $A$ closest (in the Euclidean norm) to $b$, and the orthogonality principle for projections onto subspaces in finite-dimensional inner product spaces, do the following:\n\n- Derive a necessary and sufficient optimality condition for $x^{\\star}$ to be a least squares minimizer in terms of the orthogonality of the residual to the column space of $A$, and use it to characterize the entire set of minimizers as an affine subspace of $\\mathbb{R}^{m}$.\n\n- From first principles of linear algebra, identify a necessary and sufficient rank condition on $A$ under which the least squares minimizer is unique.\n\n- Specialize your characterization to the case $A \\in \\mathbb{R}^{7 \\times 5}$ with $\\operatorname{rank}(A)=3$, and compute the dimension of the set of least squares minimizers for an arbitrary $b \\in \\mathbb{R}^{7}$.\n\nProvide as your final answer the dimension you compute in the special case. No rounding is required. The final answer should be a single real number.",
            "solution": "The problem asks for a derivation of the properties of the least squares solution set, a condition for its uniqueness, and a calculation of the dimension of this set for a specific case. We will address each part in sequence, starting from the fundamental principles outlined.\n\nLet $A \\in \\mathbb{R}^{n \\times m}$ and $b \\in \\mathbb{R}^{n}$. The least squares problem is to find a vector $x^{\\star} \\in \\mathbb{R}^{m}$ that minimizes the squared Euclidean norm of the residual, $f(x) = \\|Ax - b\\|_{2}^{2}$. The set of vectors of the form $Ax$ for all $x \\in \\mathbb{R}^{m}$ constitutes the column space of $A$, denoted $\\operatorname{col}(A)$, which is a subspace of $\\mathbb{R}^{n}$. The problem is therefore equivalent to finding a vector $p \\in \\operatorname{col}(A)$ that is closest to $b$ in the Euclidean distance. Once such a $p$ is found, any $x^{\\star}$ satisfying $Ax^{\\star} = p$ is a least squares minimizer.\n\nAccording to the orthogonality principle (also known as the projection theorem) in a finite-dimensional inner product space, a vector $p \\in \\operatorname{col}(A)$ is the unique closest point to $b$ if and only if the error vector, $b-p$, is orthogonal to the subspace $\\operatorname{col}(A)$.\n\n**Optimality Condition and Characterization of the Solution Set**\n\nLet $x^{\\star}$ be a least squares minimizer. Then the vector $p = Ax^{\\star}$ is the orthogonal projection of $b$ onto $\\operatorname{col}(A)$. The orthogonality principle states that the residual vector $r = b - Ax^{\\star}$ must be orthogonal to every vector in $\\operatorname{col}(A)$. This means the inner product of $r$ with any vector $y \\in \\operatorname{col}(A)$ must be zero.\nA spanning set for $\\operatorname{col}(A)$ consists of the columns of $A$, which we can denote as $a_1, a_2, \\ldots, a_m$. Therefore, the condition that $r$ is orthogonal to $\\operatorname{col}(A)$ is equivalent to $r$ being orthogonal to each column of $A$:\n$$a_j^{T} (b - Ax^{\\star}) = 0 \\quad \\text{for all } j = 1, 2, \\ldots, m$$\nThese $m$ equations can be assembled into a single matrix equation. The rows of $A^T$ are the transposes of the columns of $A$. Thus, the system of equations is equivalent to:\n$$A^{T}(b - Ax^{\\star}) = 0$$\nRearranging this equation gives the **normal equations**:\n$$A^{T}A x^{\\star} = A^{T}b$$\nThis is the necessary and sufficient optimality condition for $x^{\\star}$ to be a least squares minimizer. The set of all minimizers is precisely the set of solutions to this system of linear equations.\n\nThe solution set of any linear system $Kx=c$ (where here $K=A^TA$ and $c=A^Tb$) is an affine subspace. If a particular solution $x_p$ exists, the general solution is given by $x = x_p + x_h$, where $x_h$ is any solution to the corresponding homogeneous system $Kx_h=0$, i.e., $A^T A x_h = 0$. The set of all such $x_h$ forms the null space of $A^T A$, denoted $\\operatorname{null}(A^T A)$.\n\nIt is a fundamental result in linear algebra that $\\operatorname{null}(A^T A) = \\operatorname{null}(A)$.\nTo prove this:\n1.  Let $x \\in \\operatorname{null}(A)$. Then $Ax=0$. Multiplying by $A^T$ gives $A^T(Ax) = A^T(0)$, which means $A^T A x = 0$. Thus $x \\in \\operatorname{null}(A^T A)$. So, $\\operatorname{null}(A) \\subseteq \\operatorname{null}(A^T A)$.\n2.  Let $x \\in \\operatorname{null}(A^T A)$. Then $A^T A x = 0$. Left-multiplying by $x^T$ gives $x^T A^T A x = 0$. This can be written as $(Ax)^T(Ax) = \\|Ax\\|_{2}^{2} = 0$. The norm of a vector is zero if and only if the vector itself is the zero vector. Therefore, $Ax=0$, which means $x \\in \\operatorname{null}(A)$. So, $\\operatorname{null}(A^T A) \\subseteq \\operatorname{null}(A)$.\nCombining both inclusions, we have $\\operatorname{null}(A^T A) = \\operatorname{null}(A)$.\n\nTherefore, the set of all least squares minimizers is the affine subspace $x_p + \\operatorname{null}(A)$, where $x_p$ is any particular solution to the normal equations $A^T A x = A^T b$.\n\n**Condition for a Unique Minimizer**\n\nThe least squares minimizer is unique if and only if the solution set to the normal equations contains exactly one vector. Based on our characterization above, this occurs if and only if the null space $\\operatorname{null}(A)$ contains only the zero vector, i.e., $\\operatorname{null}(A) = \\{0\\}$.\n\nA matrix $A \\in \\mathbb{R}^{n \\times m}$ has a trivial null space if and only if its columns are linearly independent. The number of linearly independent columns is the rank of the matrix. For the $m$ columns of $A$ to be linearly independent, the rank must be equal to the number of columns.\nTherefore, the necessary and sufficient condition for the least squares minimizer to be unique is that $\\operatorname{rank}(A) = m$. This is also referred to as $A$ having full column rank.\n\n**Special Case: Dimension Calculation**\n\nWe are given a matrix $A \\in \\mathbb{R}^{7 \\times 5}$ with $\\operatorname{rank}(A) = 3$. Here, the number of rows is $n=7$ and the number of columns is $m=5$.\nThe set of all least squares minimizers is the affine subspace $x_p + \\operatorname{null}(A)$. The dimension of an affine subspace is defined as the dimension of the associated vector subspace, which in this case is $\\operatorname{null}(A)$. So, we need to compute $\\dim(\\operatorname{null}(A))$.\n\nThe rank-nullity theorem for a matrix $A \\in \\mathbb{R}^{n \\times m}$ states that:\n$$\\operatorname{rank}(A) + \\dim(\\operatorname{null}(A)) = m$$\nWe are given $\\operatorname{rank}(A) = 3$ and we know $m=5$. Substituting these values into the theorem:\n$$3 + \\dim(\\operatorname{null}(A)) = 5$$\nSolving for the dimension of the null space:\n$$\\dim(\\operatorname{null}(A)) = 5 - 3 = 2$$\nThus, the dimension of the set of least squares minimizers for this specific case is $2$. This means the solutions form a $2$-dimensional plane in $\\mathbb{R}^{5}$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Real-world data is rarely clean, and outliers can drastically skew the results of standard least squares, which minimizes the $\\ell_2$-norm of the residual. This practice introduces robust estimation by replacing the quadratic loss with the Huber loss function, which is less sensitive to extreme data points . You will derive and implement the Iteratively Reweighted Least Squares (IRLS) algorithm, discovering how a non-linear robust regression problem can be solved through a sequence of familiar weighted least squares problems.",
            "id": "3590989",
            "problem": "Consider the robust estimation of a vector $x \\in \\mathbb{R}^n$ from an overdetermined linear system $A x \\approx b$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and $b \\in \\mathbb{R}^m$. The classical least squares formulation minimizes the squared residual norm, which is sensitive to outliers in $b$. To mitigate outlier influence, consider the robust least squares problem that minimizes the sum of the Huber losses applied to the residuals. Let the residual vector be $r(x) = b - A x$. For a fixed threshold parameter $\\tau  0$, the Huber loss for a scalar residual $r$ is defined by\n$$\n\\rho_{\\tau}(r) = \n\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r| \\le \\tau, \\\\\n\\tau |r| - \\frac{1}{2} \\tau^2,  \\text{if } |r|  \\tau.\n\\end{cases}\n$$\nThe robust least squares objective is then\n$$\nF_{\\tau}(x) = \\sum_{i=1}^m \\rho_{\\tau}(r_i(x)),\n$$\nwhere $r_i(x)$ denotes the $i$-th component of $r(x)$. An approach to minimizing $F_{\\tau}(x)$ is Iteratively Reweighted Least Squares (IRLS), in which, at each iteration, one solves a weighted least squares subproblem with diagonal weights that depend on the current residuals.\n\nYour tasks are:\n- Starting from the definition of the Huber loss and first-order optimality conditions for minimizing $F_{\\tau}(x)$, derive the IRLS update rule and the residual-dependent weight expression to be used at each iteration for the weighted least squares subproblem. Do not use any shortcut formulas; derive the weight expression from the derivative of the Huber loss and the stationarity conditions for the minimizer of $F_{\\tau}(x)$.\n- Implement a complete program that performs IRLS using the derived weights. At each iteration, solve the weighted least squares subproblem by forming a weighted system and solving it stably. The weighted normal equations are given by $A^\\top W A x = A^\\top W b$, where $W = \\operatorname{diag}(w_1, \\dots, w_m)$ is the diagonal matrix of weights. Although your algorithm may solve via a numerically stable approach (for example, by using a weighted formulation with a factorization), you must compute and report the conditioning of the weighted normal equations matrix $A^\\top W A$ at the final iteration.\n- Implement convergence assessment based on the relative change in $x$ and assess whether the robust objective $F_{\\tau}(x)$ is nonincreasing across iterations.\n\nDefine the following quantifiable outputs for each test case:\n- A boolean `conv` indicating whether the algorithm converged within a prescribed tolerance on the relative change in $x$ before reaching the maximum number of iterations.\n- A boolean `mono` indicating whether the sequence of objective values $F_{\\tau}(x^{(k)})$ was nonincreasing for all iterations of the algorithm (allowing for floating-point round-off within a very small tolerance).\n- A float `kappa_WNA` equal to the $2$-norm condition number of the final weighted normal equations matrix, $\\kappa(A^\\top W A)$.\n- A float `ratio_kappa` equal to the ratio of the final weighted normal equations condition number to the unweighted normal equations condition number, $\\frac{\\kappa(A^\\top W A)}{\\kappa(A^\\top A)}$.\n- A float `diff_norm` equal to the Euclidean norm of the difference between the final IRLS solution and the ordinary least squares solution, $\\|x_{\\mathrm{IRLS}} - x_{\\mathrm{LS}}\\|_2$.\n\nThe implementation details and evaluation protocol must be as follows:\n- Use initial guess $x^{(0)} = 0 \\in \\mathbb{R}^n$.\n- Use convergence tolerance on the relative change in $x$ equal to $\\varepsilon = 10^{-10}$ unless otherwise specified, and declare convergence if $\\frac{\\|x^{(k+1)} - x^{(k)}\\|_2}{\\max(1, \\|x^{(k+1)}\\|_2)} \\le \\varepsilon$.\n- Use a maximum iteration count as specified in each test case.\n- Compute the final $2$-norm condition numbers using the singular value decomposition definition $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$ for the appropriate symmetric positive definite matrix $M$, and treat extremely ill-conditioned or singular cases by returning a floating value representing $+\\infty$ when the smallest singular value underflows to zero numerically.\n- The robust objective nonincreasing check must be performed with a tolerance of $10^{-12}$, that is, `mono` is true if for all successive iterates $k$ one has $F_{\\tau}(x^{(k+1)}) \\le F_{\\tau}(x^{(k)}) + 10^{-12}$.\n- For numerical stability in solving the weighted least squares subproblem, construct $\\widetilde{A} = \\sqrt{W} A$ and $\\widetilde{b} = \\sqrt{W} b$, and compute $x^{(k+1)}$ from the least squares solution to the weighted system $\\widetilde{A} x \\approx \\widetilde{b}$.\n\nTest suite specification:\n- Case $1$ (happy path with moderate outliers and well-conditioned $A$):\n    - Dimensions: $m = 50$, $n = 5$.\n    - Random seed: $0$.\n    - Matrix construction: draw $A$ with independent standard normal entries and normalize each column to unit $\\ell_2$ length.\n    - Ground truth: draw $x_{\\star}$ with independent standard normal entries.\n    - Observations: set $b = A x_{\\star} + \\eta$ with $\\eta$ drawn from a normal distribution with standard deviation $0.05$ componentwise.\n    - Outliers: choose $6$ random indices and add a perturbation of magnitude $5.0$ with random sign to those components of $b$.\n    - Huber threshold: $\\tau = 0.5$.\n    - Maximum iterations: $50$.\n    - Convergence tolerance: $\\varepsilon = 10^{-10}$.\n- Case $2$ (boundary case where all residuals lie in the quadratic region):\n    - Dimensions: $m = 40$, $n = 4$.\n    - Random seed: $1$.\n    - Matrix construction: draw $A$ with independent standard normal entries and normalize each column to unit $\\ell_2$ length.\n    - Ground truth: draw $x_{\\star}$ with independent standard normal entries.\n    - Observations: set $b = A x_{\\star} + \\eta$ with $\\eta$ drawn from a normal distribution with standard deviation $0.01$ componentwise.\n    - Outliers: none.\n    - Huber threshold: $\\tau = 10^6$.\n    - Maximum iterations: $10$.\n    - Convergence tolerance: $\\varepsilon = 10^{-12}$.\n- Case $3$ (ill-conditioned $A$ with severe outliers):\n    - Dimensions: $m = 60$, $n = 5$.\n    - Random seed: $2$.\n    - Matrix construction: form highly correlated columns by setting $c_0$ to standard normal, $c_1 = c_0 + 10^{-3}$ times a standard normal vector, $c_2$ to standard normal, $c_3 = c_2 + 10^{-4}$ times a standard normal vector, and $c_4$ to a standard normal vector scaled by $10^{-2}$; then set $A = [c_0, c_1, c_2, c_3, c_4]$ as columns without normalization.\n    - Ground truth: draw $x_{\\star}$ with independent standard normal entries.\n    - Observations: set $b = A x_{\\star} + \\eta$ with $\\eta$ drawn from a normal distribution with standard deviation $0.1$ componentwise.\n    - Outliers: choose $10$ random indices and add a perturbation of magnitude $10.0$ with random sign to those components of $b$.\n    - Huber threshold: $\\tau = 0.3$.\n    - Maximum iterations: $100$.\n    - Convergence tolerance: $\\varepsilon = 10^{-8}$.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must be a list of the five values [`conv`, `mono`, `kappa_WNA`, `ratio_kappa`, `diff_norm`] for each test case, in the order Case $1$, Case $2$, Case $3$. Round all floating-point values to $6$ decimal places in the final output.",
            "solution": "The user has provided a problem on robust regression using the Huber loss function, to be solved via the Iteratively Reweighted Least Squares (IRLS) algorithm. The tasks include deriving the IRLS update rule and implementing the algorithm to solve a series of test cases.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Objective Function:** Minimize $F_{\\tau}(x) = \\sum_{i=1}^m \\rho_{\\tau}(r_i(x))$, where $r(x) = b - A x$.\n- **Huber Loss:** $\\rho_{\\tau}(r) = \\begin{cases} \\frac{1}{2} r^2,  \\text{if } |r| \\le \\tau, \\\\ \\tau |r| - \\frac{1}{2} \\tau^2,  \\text{if } |r|  \\tau. \\end{cases}$\n- **Algorithm:** Iteratively Reweighted Least Squares (IRLS).\n- **Task 1: Derivation:** Derive the IRLS update rule and weight expression from first-order optimality conditions.\n- **Task 2: Implementation:** Implement the IRLS algorithm, compute specified output metrics, including the condition number of the final weighted normal equations matrix $A^\\top W A$.\n- **Implementation Details:** Initial guess $x^{(0)} = 0$, specific convergence criteria and tolerances, stable solution of weighted subproblems using $\\widetilde{A} = \\sqrt{W} A$ and $\\widetilde{b} = \\sqrt{W} b$.\n- **Outputs per Case:** A list containing [`conv`, `mono`, `kappa_WNA`, `ratio_kappa`, `diff_norm`].\n- **Test Cases:** Three fully specified test cases are provided, including matrix/vector dimensions, random seeds, construction procedures, and algorithm parameters ($\\tau$, `max_iter`, $\\varepsilon$).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It deals with the minimization of a convex function (the Huber loss is convex, and so is the sum $F_{\\tau}(x)$), a standard topic in optimization and robust statistics. The proposed IRLS method is a well-established and appropriate algorithm for this class of problems. The problem is well-posed; all necessary data, parameters, and procedures are defined unambiguously. The definitions are consistent, and the tasks are feasible. The problem is not trivial, as it requires a correct derivation and a careful, numerically stable implementation.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will now proceed with the derivation and solution.\n\n### Derivation of the IRLS Update Rule\n\nThe objective is to find $x$ that minimizes the function:\n$$\nF_{\\tau}(x) = \\sum_{i=1}^m \\rho_{\\tau}((b - Ax)_i)\n$$\nSince $F_{\\tau}(x)$ is a convex function of $x$, a minimizer $x^*$ must satisfy the first-order optimality condition, which states that the gradient of $F_{\\tau}(x)$ with respect to $x$ is zero. Let $r_i(x) = (b - Ax)_i$ be the $i$-th residual.\nUsing the chain rule, the gradient is:\n$$\n\\nabla_x F_{\\tau}(x) = \\sum_{i=1}^m \\frac{d\\rho_{\\tau}(r_i)}{dr_i} \\nabla_x r_i(x)\n$$\nThe gradient of the $i$-th residual is $\\nabla_x r_i(x) = \\nabla_x (b_i - a_i^\\top x) = -a_i$, where $a_i^\\top$ is the $i$-th row of $A$. Let $\\psi_{\\tau}(r) = \\frac{d\\rho_{\\tau}(r)}{dr}$ be the derivative of the Huber loss function. This derivative, sometimes called the Huber influence function, is:\n$$\n\\psi_{\\tau}(r) = \n\\begin{cases}\nr,  \\text{if } |r| \\le \\tau, \\\\\n\\tau \\operatorname{sgn}(r),  \\text{if } |r|  \\tau.\n\\end{cases}\n$$\nNote that $\\rho_{\\tau}(r)$ is not differentiable at $|r|=\\tau$, but it has a well-defined subgradient. For the purpose of the algorithm, we can take either of the one-sided limits, which are equal.\n\nSubstituting $\\nabla_x r_i(x)$ and $\\psi_{\\tau}(r_i)$ into the gradient expression:\n$$\n\\nabla_x F_{\\tau}(x) = \\sum_{i=1}^m \\psi_{\\tau}(r_i(x)) (-a_i) = -A^\\top \\Psi_{\\tau}(r(x))\n$$\nwhere $\\Psi_{\\tau}(r(x))$ is a column vector with components $\\psi_{\\tau}(r_i(x))$. The optimality condition is $\\nabla_x F_{\\tau}(x) = 0$, which gives:\n$$\nA^\\top \\Psi_{\\tau}(b - Ax) = 0\n$$\nThis is a system of nonlinear equations for $x$. The IRLS algorithm linearizes this problem by introducing weights. We seek a weight function $w_{\\tau}(r)$ such that $\\psi_{\\tau}(r) = w_{\\tau}(r) \\cdot r$. We can derive $w_{\\tau}(r)$ by considering two cases:\n\n1.  For small residuals, $|r| \\le \\tau$: We have $\\psi_{\\tau}(r) = r$. The relation $w_{\\tau}(r) \\cdot r = r$ implies $w_{\\tau}(r) = 1$. This holds even for $r=0$ by taking the limit $\\lim_{r\\to 0} \\psi_{\\tau}(r)/r = 1$.\n2.  For large residuals, $|r|  \\tau$: We have $\\psi_{\\tau}(r) = \\tau \\operatorname{sgn}(r)$. The relation is $w_{\\tau}(r) \\cdot r = \\tau \\operatorname{sgn}(r)$. Since $\\operatorname{sgn}(r) = r/|r|$, we can solve for the weight: $w_{\\tau}(r) = \\frac{\\tau}{|r|}$.\n\nCombining these, the weight for the $i$-th residual $r_i$ is:\n$$\nw_{\\tau}(r_i) = \n\\begin{cases}\n1,  \\text{if } |r_i| \\le \\tau, \\\\\n\\frac{\\tau}{|r_i|},  \\text{if } |r_i|  \\tau.\n\\end{cases}\n$$\nUsing these weights, the optimality condition can be written as $A^\\top W(x) (b - Ax) = 0$, where $W(x)$ is a diagonal matrix with entries $W_{ii}(x) = w_{\\tau}(r_i(x))$. Rearranging this gives:\n$$\nA^\\top W(x) A x = A^\\top W(x) b\n$$\nSince the weights $W(x)$ depend on the solution $x$, this is still a nonlinear system. The IRLS algorithm proceeds by iterating: given the $k$-th estimate $x^{(k)}$, we compute the weights $W^{(k)} = W(x^{(k)})$ and then solve for the next estimate $x^{(k+1)}$ by treating the weights as fixed constants.\n\n**IRLS Update Rule:**\n1.  Compute residuals: $r^{(k)} = b - Ax^{(k)}$.\n2.  Compute weights: For $i = 1, \\dots, m$, set $w_i^{(k)} = w_{\\tau}(r_i^{(k)})$. Form the diagonal matrix $W^{(k)} = \\operatorname{diag}(w_1^{(k)}, \\dots, w_m^{(k)})$.\n3.  Solve the weighted least squares problem for $x^{(k+1)}$:\n    $$\n    x^{(k+1)} = \\arg\\min_x \\| \\sqrt{W^{(k)}} (b - Ax) \\|_2^2\n    $$\n    This is equivalent to solving the linear system (the weighted normal equations):\n    $$\n    (A^\\top W^{(k)} A) x^{(k+1)} = A^\\top W^{(k)} b\n    $$\nThis iterative process is repeated until convergence. The derivation is now complete. The following section provides the Python implementation based on this derived update rule.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_huber_objective(x, A, b, tau):\n    \"\"\"Computes the sum of Huber losses.\"\"\"\n    r = b - A @ x\n    abs_r = np.abs(r)\n    \n    quad_mask = abs_r = tau\n    lin_mask = ~quad_mask\n    \n    obj_quad = 0.5 * np.sum(r[quad_mask]**2)\n    obj_lin = np.sum(tau * abs_r[lin_mask] - 0.5 * tau**2)\n    \n    return obj_quad + obj_lin\n\ndef run_irls(A, b, tau, max_iter, tol):\n    \"\"\"\n    Performs Iteratively Reweighted Least Squares for Huber regression.\n    \"\"\"\n    m, n = A.shape\n    x_k = np.zeros(n)\n    \n    obj_values = []\n    \n    # Calculate ordinary least squares solution for comparison\n    x_ls = np.linalg.lstsq(A, b, rcond=None)[0]\n    \n    conv = False\n    for k in range(max_iter):\n        x_prev = x_k\n        \n        # Store objective value for monotonicity check\n        obj_values.append(compute_huber_objective(x_prev, A, b, tau))\n        \n        # Calculate residuals and weights\n        r = b - A @ x_prev\n        abs_r = np.abs(r)\n        \n        w = np.ones(m)\n        large_res_mask = abs_r  tau\n        # Avoid division by zero, although abs_r[large_res_mask] should be  tau  0\n        w[large_res_mask] = tau / abs_r[large_res_mask]\n\n        # Solve the weighted least squares subproblem\n        sqrt_w = np.sqrt(w)\n        A_tilde = sqrt_w[:, np.newaxis] * A\n        b_tilde = sqrt_w * b\n        x_k = np.linalg.lstsq(A_tilde, b_tilde, rcond=None)[0]\n        \n        # Check for convergence\n        rel_change = np.linalg.norm(x_k - x_prev) / max(1.0, np.linalg.norm(x_k))\n        if rel_change = tol:\n            conv = True\n            break\n            \n    # Final objective value for monotonicity check\n    obj_values.append(compute_huber_objective(x_k, A, b, tau))\n\n    # Check for non-increasing objective function\n    mono = True\n    mono_tol = 1e-12\n    for i in range(len(obj_values) - 1):\n        if obj_values[i+1]  obj_values[i] + mono_tol:\n            mono = False\n            break\n\n    # Calculate final weights and matrices for condition numbers\n    r_final = b - A @ x_k\n    abs_r_final = np.abs(r_final)\n    w_final = np.ones(m)\n    large_res_mask_final = abs_r_final  tau\n    w_final[large_res_mask_final] = tau / abs_r_final[large_res_mask_final]\n    \n    W_final = np.diag(w_final)\n    \n    M_w = A.T @ W_final @ A\n    M_unweighted = A.T @ A\n    \n    kappa_M_w = np.linalg.cond(M_w)\n    kappa_M_unweighted = np.linalg.cond(M_unweighted)\n    \n    # Ratio of condition numbers\n    ratio_kappa = kappa_M_w / kappa_M_unweighted\n    \n    # Norm of difference between IRLS and LS solutions\n    diff_norm = np.linalg.norm(x_k - x_ls)\n    \n    return [conv, mono, kappa_M_w, ratio_kappa, diff_norm]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases specified in the problem.\n    \"\"\"\n    test_cases_params = [\n        # Case 1\n        {\n            'm': 50, 'n': 5, 'seed': 0, 'tau': 0.5, \n            'max_iter': 50, 'tol': 1e-10, 'case_id': 1\n        },\n        # Case 2\n        {\n            'm': 40, 'n': 4, 'seed': 1, 'tau': 1e6, \n            'max_iter': 10, 'tol': 1e-12, 'case_id': 2\n        },\n        # Case 3\n        {\n            'm': 60, 'n': 5, 'seed': 2, 'tau': 0.3,\n            'max_iter': 100, 'tol': 1e-8, 'case_id': 3\n        }\n    ]\n\n    all_results = []\n\n    for params in test_cases_params:\n        m, n, seed = params['m'], params['n'], params['seed']\n        rng = np.random.default_rng(seed)\n\n        if params['case_id'] == 1:\n            A = rng.standard_normal((m, n))\n            A /= np.linalg.norm(A, axis=0)\n            x_star = rng.standard_normal(n)\n            b = A @ x_star + rng.normal(scale=0.05, size=m)\n            outlier_indices = rng.choice(m, 6, replace=False)\n            perturbation = 5.0 * (2 * rng.integers(0, 2, size=6) - 1)\n            b[outlier_indices] += perturbation\n        \n        elif params['case_id'] == 2:\n            A = rng.standard_normal((m, n))\n            A /= np.linalg.norm(A, axis=0)\n            x_star = rng.standard_normal(n)\n            b = A @ x_star + rng.normal(scale=0.01, size=m)\n        \n        elif params['case_id'] == 3:\n            c0 = rng.standard_normal(m)\n            c1 = c0 + 1e-3 * rng.standard_normal(m)\n            c2 = rng.standard_normal(m)\n            c3 = c2 + 1e-4 * rng.standard_normal(m)\n            c4 = 1e-2 * rng.standard_normal(m)\n            A = np.column_stack([c0, c1, c2, c3, c4])\n            x_star = rng.standard_normal(5)\n            b = A @ x_star + rng.normal(scale=0.1, size=m)\n            outlier_indices = rng.choice(m, 10, replace=False)\n            perturbation = 10.0 * (2 * rng.integers(0, 2, size=10) - 1)\n            b[outlier_indices] += perturbation\n            \n        result = run_irls(A, b, params['tau'], params['max_iter'], params['tol'])\n        all_results.append(result)\n\n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        # res has format [conv, mono, kappa_w, ratio, diff_norm]\n        res_str = (\n            f\"[{res[0]},{res[1]},\"\n            f\"{res[2]:.6f},{res[3]:.6f},{res[4]:.6f}]\"\n        )\n        formatted_results.append(res_str)\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}