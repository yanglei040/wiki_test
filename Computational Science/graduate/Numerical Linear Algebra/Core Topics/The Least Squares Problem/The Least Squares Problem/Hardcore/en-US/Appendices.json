{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the least squares problem, we must go beyond merely applying a formula and connect the solution to its underlying geometric principles. This exercise guides you through solving a least squares problem from first principles using one of the most stable and fundamental methods: the QR factorization . By manually constructing the factorization and deriving the solution, you will solidify your understanding of how orthogonal projections provide the key to minimizing the residual norm $\\|b-Ax\\|_2$.",
            "id": "3590957",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 2}$ and $b \\in \\mathbb{R}^{4}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 1\\\\\n1 & 0\\\\\n0 & 1\\\\\n0 & 0\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix}\n2\\\\\n1\\\\\n0\\\\\n0\n\\end{pmatrix}.\n$$\nConsider the linear least squares (LS) problem of minimizing the Euclidean norm of the residual,\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\; \\|\\,b - A x\\,\\|_{2}.\n$$\nStarting from the definitions of orthonormality and orthogonal projection, and without invoking any pre-stated least squares formula, proceed as follows:\n- Compute a reduced $Q$–$R$ factorization (QR) of $A$, namely $A = Q R$ with $Q \\in \\mathbb{R}^{4 \\times 2}$ having orthonormal columns and $R \\in \\mathbb{R}^{2 \\times 2}$ upper triangular with positive diagonal.\n- Using only the invariance of the Euclidean norm under multiplication by an orthogonal projector and the orthogonality of complementary subspaces associated with $Q$, derive from first principles the characterization of the minimizer in terms of a triangular system arising from the reduced $Q$–$R$ factorization.\n- Then, for the specific $A$ and $b$ above, use your derived characterization to compute the minimal value of the objective function $\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}^{2}$ as an exact number.\n\nYour final answer must be the exact value of the minimal squared residual $\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}^{2}$. Do not round.",
            "solution": "We begin with the linear least squares (LS) problem\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}.\n$$\nLet $A = Q R$ be a reduced $Q$–$R$ factorization (QR) with $Q \\in \\mathbb{R}^{4 \\times 2}$ having orthonormal columns and $R \\in \\mathbb{R}^{2 \\times 2}$ upper triangular with positive diagonal. By definition, the columns of $Q$ are orthonormal, so $Q^{T} Q = I_{2}$, and the matrix $P \\coloneqq Q Q^{T}$ is the orthogonal projector onto $\\mathcal{R}(Q) = \\mathcal{R}(A)$.\n\nFor any $x \\in \\mathbb{R}^{2}$, decompose the residual using the orthogonal splitting of $\\mathbb{R}^{4}$ into $\\mathcal{R}(Q)$ and $\\mathcal{R}(Q)^{\\perp}$:\n$$\nb - A x \\;=\\; b - Q R x \\;=\\; (I - Q Q^{T}) b \\;+\\; Q \\bigl(Q^{T} b - R x\\bigr).\n$$\nThe two terms on the right-hand side lie in orthogonal subspaces: $(I - Q Q^{T}) b \\in \\mathcal{R}(Q)^{\\perp}$ and $Q \\bigl(Q^{T} b - R x\\bigr) \\in \\mathcal{R}(Q)$. Therefore, by the Pythagorean theorem,\n$$\n\\|\\,b - A x\\,\\|_{2}^{2} \\;=\\; \\|\\, (I - Q Q^{T}) b \\,\\|_{2}^{2} \\;+\\; \\|\\, Q \\bigl(Q^{T} b - R x\\bigr) \\,\\|_{2}^{2}.\n$$\nUsing the fact that $Q$ has orthonormal columns, we have $\\|\\,Q y\\,\\|_{2} = \\|\\,y\\,\\|_{2}$ for all $y \\in \\mathbb{R}^{2}$, so\n$$\n\\|\\,b - A x\\,\\|_{2}^{2} \\;=\\; \\|\\, (I - Q Q^{T}) b \\,\\|_{2}^{2} \\;+\\; \\|\\, Q^{T} b - R x \\,\\|_{2}^{2}.\n$$\nThe first term does not depend on $x$, and the second term is minimized if and only if\n$$\nR x \\;=\\; Q^{T} b,\n$$\nwhich is a $2 \\times 2$ upper triangular system. When $A$ has full column rank, $R$ is invertible, and the unique minimizer exists and is given by solving this triangular system. At the minimizer $x_{\\star}$, the minimal squared residual is\n$$\n\\|\\,b - A x_{\\star}\\,\\|_{2}^{2} \\;=\\; \\|\\, (I - Q Q^{T}) b \\,\\|_{2}^{2} \\;=\\; \\|\\,b\\,\\|_{2}^{2} - \\|\\,Q^{T} b\\,\\|_{2}^{2}.\n$$\n\nWe now compute a reduced $Q$–$R$ factorization of the specific $A$ by classical Gram–Schmidt, starting from the definitions.\n\nLet the columns of $A$ be $a_{1} = \\begin{pmatrix}1\\\\1\\\\0\\\\0\\end{pmatrix}$ and $a_{2} = \\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix}$.\n- Compute $r_{11} = \\|\\,a_{1}\\,\\|_{2} = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}$ and $q_{1} = a_{1} / r_{11} = \\begin{pmatrix}1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\\\ 0 \\\\ 0\\end{pmatrix}$.\n- Compute $r_{12} = q_{1}^{T} a_{2} = \\begin{pmatrix}1/\\sqrt{2} & 1/\\sqrt{2} & 0 & 0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix} = \\tfrac{1}{\\sqrt{2}}$.\n- Form $u_{2} = a_{2} - r_{12} q_{1} = \\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix} - \\tfrac{1}{\\sqrt{2}} \\begin{pmatrix}1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1/2 \\\\ -1/2 \\\\ 1 \\\\ 0\\end{pmatrix}$.\n- Compute $r_{22} = \\|\\,u_{2}\\,\\|_{2} = \\sqrt{(1/2)^{2} + (-1/2)^{2} + 1^{2}} = \\sqrt{\\tfrac{3}{2}} = \\tfrac{\\sqrt{3}}{\\sqrt{2}}$ and $q_{2} = u_{2} / r_{22}$.\n\nThus\n$$\nQ \\;=\\; \\begin{pmatrix} q_{1} & q_{2} \\end{pmatrix}, \n\\qquad\nR \\;=\\;\n\\begin{pmatrix}\n\\sqrt{2} & \\tfrac{1}{\\sqrt{2}}\\\\\n0 & \\tfrac{\\sqrt{3}}{\\sqrt{2}}\n\\end{pmatrix}.\n$$\n\nNext compute $Q^{T} b$. First,\n$$\nq_{1}^{T} b \\;=\\; \\begin{pmatrix}1/\\sqrt{2} & 1/\\sqrt{2} & 0 & 0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\0\\\\0\\end{pmatrix} \\;=\\; \\tfrac{3}{\\sqrt{2}}.\n$$\nAlso,\n$$\nq_{2}^{T} b \\;=\\; \\frac{u_{2}^{T} b}{r_{22}} \\;=\\; \\frac{\\begin{pmatrix}1/2 & -1/2 & 1 & 0\\end{pmatrix} \\begin{pmatrix}2\\\\1\\\\0\\\\0\\end{pmatrix}}{r_{22}} \\;=\\; \\frac{1 - \\tfrac{1}{2} + 0}{\\tfrac{\\sqrt{3}}{\\sqrt{2}}} \\;=\\; \\frac{\\tfrac{1}{2}}{\\tfrac{\\sqrt{3}}{\\sqrt{2}}} \\;=\\; \\frac{\\sqrt{2}}{2 \\sqrt{3}} \\;=\\; \\frac{1}{\\sqrt{6}}.\n$$\nTherefore,\n$$\nQ^{T} b \\;=\\; \\begin{pmatrix} \\tfrac{3}{\\sqrt{2}} \\\\ \\tfrac{1}{\\sqrt{6}} \\end{pmatrix}.\n$$\n\nBy the characterization derived earlier, the minimizer $x_{\\star}$ satisfies $R x_{\\star} = Q^{T} b$. Solving the upper triangular system,\n$$\n\\frac{\\sqrt{3}}{\\sqrt{2}} \\, x_{2} \\;=\\; \\frac{1}{\\sqrt{6}} \\;\\;\\Rightarrow\\;\\; x_{2} \\;=\\; \\frac{1}{3},\n$$\nand\n$$\n\\sqrt{2} \\, x_{1} \\;+\\; \\frac{1}{\\sqrt{2}} \\, x_{2} \\;=\\; \\frac{3}{\\sqrt{2}}\n\\;\\;\\Rightarrow\\;\\;\nx_{1} \\;=\\; \\frac{\\tfrac{3}{\\sqrt{2}} - \\tfrac{1}{\\sqrt{2}} \\cdot \\tfrac{1}{3}}{\\sqrt{2}} \n\\;=\\; \\frac{4}{3}.\n$$\n\nFinally, the minimal squared residual is\n$$\n\\|\\,b - A x_{\\star}\\,\\|_{2}^{2} \\;=\\; \\|\\,b\\,\\|_{2}^{2} \\;-\\; \\|\\,Q^{T} b\\,\\|_{2}^{2} \\;=\\; \\left(2^{2} + 1^{2}\\right) \\;-\\; \\left(\\left(\\tfrac{3}{\\sqrt{2}}\\right)^{2} + \\left(\\tfrac{1}{\\sqrt{6}}\\right)^{2}\\right)\n\\;=\\; 5 \\;-\\; \\left(\\tfrac{9}{2} + \\tfrac{1}{6}\\right)\n\\;=\\; 5 \\;-\\; \\tfrac{14}{3}\n\\;=\\; \\tfrac{1}{3}.\n$$\nTherefore, the exact minimal value of the objective function $\\min_{x \\in \\mathbb{R}^{2}} \\|\\,b - A x\\,\\|_{2}^{2}$ is $\\tfrac{1}{3}$.",
            "answer": "$$\\boxed{\\tfrac{1}{3}}$$"
        },
        {
            "introduction": "While finding a solution to the least squares problem is a primary goal, understanding the nature of that solution is equally important for a complete theoretical grasp. This exercise challenges you to explore the conditions for the uniqueness of the least squares minimizer by starting from the fundamental orthogonality principle . You will characterize the entire set of minimizers and determine its dimension when the solution is not unique, connecting the algebraic properties of the matrix $A$ to the geometric structure of the solution space.",
            "id": "3590981",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times m}$ and $b \\in \\mathbb{R}^{n}$. Consider the least squares problem of minimizing the squared Euclidean norm $x \\mapsto \\|A x - b\\|_{2}^{2}$ over $x \\in \\mathbb{R}^{m}$. Starting from the fundamental definition of least squares as the search for a point in the column space of $A$ closest (in the Euclidean norm) to $b$, and the orthogonality principle for projections onto subspaces in finite-dimensional inner product spaces, do the following:\n\n- Derive a necessary and sufficient optimality condition for $x^{\\star}$ to be a least squares minimizer in terms of the orthogonality of the residual to the column space of $A$, and use it to characterize the entire set of minimizers as an affine subspace of $\\mathbb{R}^{m}$.\n\n- From first principles of linear algebra, identify a necessary and sufficient rank condition on $A$ under which the least squares minimizer is unique.\n\n- Specialize your characterization to the case $A \\in \\mathbb{R}^{7 \\times 5}$ with $\\operatorname{rank}(A)=3$, and compute the dimension of the set of least squares minimizers for an arbitrary $b \\in \\mathbb{R}^{7}$.\n\nProvide as your final answer the dimension you compute in the special case. No rounding is required. The final answer should be a single real number.",
            "solution": "The problem asks for a derivation of the properties of the least squares solution set, a condition for its uniqueness, and a calculation of the dimension of this set for a specific case. We will address each part in sequence, starting from the fundamental principles outlined.\n\nLet $A \\in \\mathbb{R}^{n \\times m}$ and $b \\in \\mathbb{R}^{n}$. The least squares problem is to find a vector $x^{\\star} \\in \\mathbb{R}^{m}$ that minimizes the squared Euclidean norm of the residual, $f(x) = \\|Ax - b\\|_{2}^{2}$. The set of vectors of the form $Ax$ for all $x \\in \\mathbb{R}^{m}$ constitutes the column space of $A$, denoted $\\operatorname{col}(A)$, which is a subspace of $\\mathbb{R}^{n}$. The problem is therefore equivalent to finding a vector $p \\in \\operatorname{col}(A)$ that is closest to $b$ in the Euclidean distance. Once such a $p$ is found, any $x^{\\star}$ satisfying $Ax^{\\star} = p$ is a least squares minimizer.\n\nAccording to the orthogonality principle (also known as the projection theorem) in a finite-dimensional inner product space, a vector $p \\in \\operatorname{col}(A)$ is the unique closest point to $b$ if and only if the error vector, $b-p$, is orthogonal to the subspace $\\operatorname{col}(A)$.\n\n**Optimality Condition and Characterization of the Solution Set**\n\nLet $x^{\\star}$ be a least squares minimizer. Then the vector $p = Ax^{\\star}$ is the orthogonal projection of $b$ onto $\\operatorname{col}(A)$. The orthogonality principle states that the residual vector $r = b - Ax^{\\star}$ must be orthogonal to every vector in $\\operatorname{col}(A)$. This means the inner product of $r$ with any vector $y \\in \\operatorname{col}(A)$ must be zero.\nA spanning set for $\\operatorname{col}(A)$ consists of the columns of $A$, which we can denote as $a_1, a_2, \\ldots, a_m$. Therefore, the condition that $r$ is orthogonal to $\\operatorname{col}(A)$ is equivalent to $r$ being orthogonal to each column of $A$:\n$$a_j^{T} (b - Ax^{\\star}) = 0 \\quad \\text{for all } j = 1, 2, \\ldots, m$$\nThese $m$ equations can be assembled into a single matrix equation. The rows of $A^T$ are the transposes of the columns of $A$. Thus, the system of equations is equivalent to:\n$$A^{T}(b - Ax^{\\star}) = 0$$\nRearranging this equation gives the **normal equations**:\n$$A^{T}A x^{\\star} = A^{T}b$$\nThis is the necessary and sufficient optimality condition for $x^{\\star}$ to be a least squares minimizer. The set of all minimizers is precisely the set of solutions to this system of linear equations.\n\nThe solution set of any linear system $Kx=c$ (where here $K=A^TA$ and $c=A^Tb$) is an affine subspace. If a particular solution $x_p$ exists, the general solution is given by $x = x_p + x_h$, where $x_h$ is any solution to the corresponding homogeneous system $Kx_h=0$, i.e., $A^T A x_h = 0$. The set of all such $x_h$ forms the null space of $A^T A$, denoted $\\operatorname{null}(A^T A)$.\n\nIt is a fundamental result in linear algebra that $\\operatorname{null}(A^T A) = \\operatorname{null}(A)$.\nTo prove this:\n1.  Let $x \\in \\operatorname{null}(A)$. Then $Ax=0$. Multiplying by $A^T$ gives $A^T(Ax) = A^T(0)$, which means $A^T A x = 0$. Thus $x \\in \\operatorname{null}(A^T A)$. So, $\\operatorname{null}(A) \\subseteq \\operatorname{null}(A^T A)$.\n2.  Let $x \\in \\operatorname{null}(A^T A)$. Then $A^T A x = 0$. Left-multiplying by $x^T$ gives $x^T A^T A x = 0$. This can be written as $(Ax)^T(Ax) = \\|Ax\\|_{2}^{2} = 0$. The norm of a vector is zero if and only if the vector itself is the zero vector. Therefore, $Ax=0$, which means $x \\in \\operatorname{null}(A)$. So, $\\operatorname{null}(A^T A) \\subseteq \\operatorname{null}(A)$.\nCombining both inclusions, we have $\\operatorname{null}(A^T A) = \\operatorname{null}(A)$.\n\nTherefore, the set of all least squares minimizers is the affine subspace $x_p + \\operatorname{null}(A)$, where $x_p$ is any particular solution to the normal equations $A^T A x = A^T b$.\n\n**Condition for a Unique Minimizer**\n\nThe least squares minimizer is unique if and only if the solution set to the normal equations contains exactly one vector. Based on our characterization above, this occurs if and only if the null space $\\operatorname{null}(A)$ contains only the zero vector, i.e., $\\operatorname{null}(A) = \\{0\\}$.\n\nA matrix $A \\in \\mathbb{R}^{n \\times m}$ has a trivial null space if and only if its columns are linearly independent. The number of linearly independent columns is the rank of the matrix. For the $m$ columns of $A$ to be linearly independent, the rank must be equal to the number of columns.\nTherefore, the necessary and sufficient condition for the least squares minimizer to be unique is that $\\operatorname{rank}(A) = m$. This is also referred to as $A$ having full column rank.\n\n**Special Case: Dimension Calculation**\n\nWe are given a matrix $A \\in \\mathbb{R}^{7 \\times 5}$ with $\\operatorname{rank}(A) = 3$. Here, the number of rows is $n=7$ and the number of columns is $m=5$.\nThe set of all least squares minimizers is the affine subspace $x_p + \\operatorname{null}(A)$. The dimension of an affine subspace is defined as the dimension of the associated vector subspace, which in this case is $\\operatorname{null}(A)$. So, we need to compute $\\dim(\\operatorname{null}(A))$.\n\nThe rank-nullity theorem for a matrix $A \\in \\mathbb{R}^{n \\times m}$ states that:\n$$\\operatorname{rank}(A) + \\dim(\\operatorname{null}(A)) = m$$\nWe are given $\\operatorname{rank}(A) = 3$ and we know $m=5$. Substituting these values into the theorem:\n$$3 + \\dim(\\operatorname{null}(A)) = 5$$\nSolving for the dimension of the null space:\n$$\\dim(\\operatorname{null}(A)) = 5 - 3 = 2$$\nThus, the dimension of the set of least squares minimizers for this specific case is $2$. This means the solutions form a $2$-dimensional plane in $\\mathbb{R}^{5}$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "In practical applications, data is rarely perfect. This practice moves into the essential topic of sensitivity analysis, asking how perturbations in the data vector $b$ affect the computed residual . By deriving the condition numbers for the residual map, you will gain a deeper appreciation for the stability of the least squares problem and discover the critical, geometric insight that relative errors can become unbounded precisely when the model is a very good fit for the data.",
            "id": "3591010",
            "problem": "Consider the overdetermined linear system with matrix $A \\in \\mathbb{R}^{m \\times n}$, $m > n$, full column rank, and data vector $b \\in \\mathbb{R}^{m}$. The least squares solution $x(b)$ satisfies the first-order optimality condition $A^{\\top}(A x(b) - b) = 0$, and the residual is $r(b) = b - A x(b)$. Treat the residual as the output of a mapping $b \\mapsto r(b)$ with $A$ fixed.\n\n1. Starting only from the optimality condition $A^{\\top}(A x(b) - b) = 0$ and basic properties of orthogonal projectors, derive the expression of $r(b)$ in terms of the orthogonal projector onto $\\operatorname{range}(A)$, and use it to obtain sharp bounds on $\\|\\Delta r\\|_{2}$ induced by a perturbation $\\Delta b$ with $A$ fixed. Your bounds must be expressed in terms of $\\|\\Delta b\\|_{2}$ and must identify the perturbations $\\Delta b$ that attain equality.\n\n2. Define the absolute and relative condition numbers of the residual with respect to $b$ at $(A,b)$ as\n$$\n\\kappa_{\\mathrm{abs}}(A,b) \\coloneqq \\left\\| D r(b) \\right\\|_{2}, \n\\quad \n\\kappa_{\\mathrm{rel}}(A,b) \\coloneqq \\lim_{\\varepsilon \\to 0^{+}} \\ \\sup_{\\|\\Delta b\\|_{2} \\le \\varepsilon \\|b\\|_{2}} \\ \\frac{\\|r(b + \\Delta b) - r(b)\\|_{2}/\\|r(b)\\|_{2}}{\\|\\Delta b\\|_{2}/\\|b\\|_{2}} ,\n$$\nwhere $D r(b)$ denotes the Fréchet derivative of the residual mapping at $b$. Derive closed-form expressions for $\\kappa_{\\mathrm{abs}}(A,b)$ and $\\kappa_{\\mathrm{rel}}(A,b)$, and explain the geometric meaning of each.\n\n3. Three standard algorithms for computing least squares residuals are:\n   (a) Normal equations: compute $\\widehat{x}_{\\mathrm{N}}$ by solving $A^{\\top} A \\widehat{x}_{\\mathrm{N}} = A^{\\top} b$, then form $r_{\\mathrm{N}} = b - A \\widehat{x}_{\\mathrm{N}}$.\n   (b) Orthogonal-triangular (QR) factorization: compute a thin $QR$ factorization $A = Q R$ with $Q \\in \\mathbb{R}^{m \\times n}$ having orthonormal columns, then form $r_{\\mathrm{Q}} = (I - Q Q^{\\top}) b$.\n   (c) Singular Value Decomposition (SVD): compute $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ orthogonal, and form $r_{\\mathrm{S}} = U_{2} U_{2}^{\\top} b$, where $U_{2} \\in \\mathbb{R}^{m \\times (m-n)}$ spans the orthogonal complement of $\\operatorname{range}(A)$.\nExplain, in exact arithmetic, how the residual condition numbers in part 2 compare across these algorithms, and justify your conclusion.\n\n4. For the specific data\n$$\nA = \\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & 0\n\\end{pmatrix},\n\\qquad\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n0.9999995\n\\end{pmatrix},\n$$\ncompute the numerical value of the common residual relative condition number $\\kappa_{\\mathrm{rel}}(A,b)$ from part 3. Round your answer to six significant figures. No physical units are involved.",
            "solution": "### Part 1: Derivation of the Residual and Perturbation Bounds\n\nThe least squares solution $x(b)$ minimizes $\\|Ax - b\\|_{2}$. The first-order optimality condition is given by the normal equations:\n$$\nA^{\\top}(A x(b) - b) = 0\n$$\nSince $A \\in \\mathbb{R}^{m \\times n}$ has full column rank ($n$), the matrix $A^{\\top} A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and thus invertible. We can solve for $x(b)$:\n$$\nA^{\\top} A x(b) = A^{\\top} b \\implies x(b) = (A^{\\top} A)^{-1} A^{\\top} b\n$$\nThe expression $(A^{\\top} A)^{-1} A^{\\top}$ is the Moore-Penrose pseudoinverse of $A$, denoted $A^{\\dagger}$.\n\nThe residual vector $r(b)$ is defined as $r(b) = b - A x(b)$. Substituting the expression for $x(b)$:\n$$\nr(b) = b - A \\left( (A^{\\top} A)^{-1} A^{\\top} \\right) b\n$$\nLet us define the matrix $P_A = A (A^{\\top} A)^{-1} A^{\\top}$. This matrix is the orthogonal projector onto the column space of $A$, $\\operatorname{range}(A)$. Therefore, the residual can be expressed as:\n$$\nr(b) = (I - P_A) b\n$$\nThe matrix $I - P_A$ is the orthogonal projector onto the orthogonal complement of $\\operatorname{range}(A)$, which is the null space of $A^{\\top}$, denoted $\\operatorname{null}(A^{\\top})$. The mapping $b \\mapsto r(b)$ is a linear transformation defined by the matrix $I - P_A$.\n\nNow, consider a perturbation $\\Delta b$ in the data vector $b$. The new data vector is $b + \\Delta b$. The corresponding residual is $r(b + \\Delta b)$. The change in the residual, $\\Delta r$, is:\n$$\n\\Delta r = r(b + \\Delta b) - r(b) = (I - P_A)(b + \\Delta b) - (I - P_A)b\n$$\nDue to the linearity of the mapping, this simplifies to:\n$$\n\\Delta r = (I - P_A) \\Delta b\n$$\nTo find bounds on $\\|\\Delta r\\|_{2}$, we take the vector $2$-norm of both sides:\n$$\n\\|\\Delta r\\|_{2} = \\|(I - P_A) \\Delta b\\|_{2}\n$$\nBy the definition of an induced matrix norm, $\\|\\Delta r\\|_{2} \\le \\|I - P_A\\|_{2} \\|\\Delta b\\|_{2}$. The matrix $I-P_A$ is an orthogonal projector. The induced $2$-norm of any non-zero orthogonal projector is $1$. Since $m > n$, $\\operatorname{range}(A)$ is a proper subspace of $\\mathbb{R}^m$, so $I - P_A$ is not the zero matrix. Thus, $\\|I - P_A\\|_{2} = 1$. This gives the upper bound:\n$$\n\\|\\Delta r\\|_{2} \\le \\|\\Delta b\\|_{2}\n$$\nEquality, $\\|\\Delta r\\|_{2} = \\|\\Delta b\\|_{2}$, is attained if and only if the vector $\\Delta b$ lies entirely in the subspace onto which $I - P_A$ projects. That is, when $\\Delta b \\in \\operatorname{range}(I - P_A) = \\operatorname{null}(A^{\\top})$. For such a $\\Delta b$, we have $(I - P_A)\\Delta b = \\Delta b$.\n\nThe lower bound is $\\|\\Delta r\\|_{2} \\ge 0$. Equality $\\|\\Delta r\\|_{2} = 0$ is attained if and only if $(I - P_A) \\Delta b = 0$. This occurs when $\\Delta b$ is in the null space of the projector $I - P_A$, which is $\\operatorname{null}(I - P_A) = \\operatorname{range}(A)$.\n\nIn summary, the sharp bounds are $0 \\le \\|\\Delta r\\|_{2} \\le \\|\\Delta b\\|_{2}$. The upper bound is attained for any $\\Delta b \\in \\operatorname{null}(A^{\\top})$ and the lower bound is attained for any $\\Delta b \\in \\operatorname{range}(A)$.\n\n### Part 2: Condition Numbers of the Residual\n\nThe mapping is $r(b) = (I - P_A) b$. This is a linear function of $b$. The Fréchet derivative of a linear map $L(b) = M b$ at any point $b$ is the matrix $M$ itself. Therefore, the derivative of the residual mapping $r(b)$ is:\n$$\nD r(b) = I - P_A\n$$\nThe absolute condition number is defined as $\\kappa_{\\mathrm{abs}}(A,b) = \\| D r(b) \\|_{2}$. Substituting the derivative:\n$$\n\\kappa_{\\mathrm{abs}}(A,b) = \\|I - P_A\\|_{2} = 1\n$$\nThe geometric meaning of $\\kappa_{\\mathrm{abs}}(A,b) = 1$ is that the residual mapping is non-expansive. An absolute perturbation of size $\\|\\Delta b\\|_2$ in the input data results in an absolute change in the residual of at most the same size. This signifies that the problem of computing the residual is well-conditioned in an absolute sense.\n\nThe relative condition number is defined as:\n$$\n\\kappa_{\\mathrm{rel}}(A,b) \\coloneqq \\lim_{\\varepsilon \\to 0^{+}} \\ \\sup_{\\|\\Delta b\\|_{2} \\le \\varepsilon \\|b\\|_{2}} \\ \\frac{\\|r(b + \\Delta b) - r(b)\\|_{2}/\\|r(b)\\|_{2}}{\\|\\Delta b\\|_{2}/\\|b\\|_{2}}\n$$\nAssuming $r(b) \\ne 0$. Using $\\Delta r = r(b + \\Delta b) - r(b) = (I-P_A)\\Delta b$, we can rewrite the expression as:\n$$\n\\kappa_{\\mathrm{rel}}(A,b) = \\frac{\\|b\\|_{2}}{\\|r(b)\\|_{2}} \\lim_{\\varepsilon \\to 0^{+}} \\ \\sup_{\\|\\Delta b\\|_{2} \\le \\varepsilon \\|b\\|_{2}} \\ \\frac{\\|(I - P_A)\\Delta b\\|_{2}}{\\|\\Delta b\\|_{2}}\n$$\nThe term inside the limit is the definition of the induced $2$-norm of the matrix $I-P_A$:\n$$\n\\sup_{\\Delta b \\ne 0} \\frac{\\|(I - P_A)\\Delta b\\|_{2}}{\\|\\Delta b\\|_{2}} = \\|I - P_A\\|_{2} = 1\n$$\nThis supremum is independent of $\\varepsilon$ and $\\|\\Delta b\\|_2$. Thus, the closed-form expression for the relative condition number is:\n$$\n\\kappa_{\\mathrm{rel}}(A,b) = \\frac{\\|b\\|_{2}}{\\|r(b)\\|_{2}} = \\frac{\\|b\\|_{2}}{\\|(I - P_A) b\\|_{2}}\n$$\nFor the geometric meaning, consider the angle $\\theta$ between the vector $b$ and the subspace $\\operatorname{range}(A)$. The vector $P_A b$ is the orthogonal projection of $b$ onto $\\operatorname{range}(A)$, and $r(b) = b - P_A b$ is the component of $b$ orthogonal to $\\operatorname{range}(A)$. In the right triangle formed by $b$, $P_A b$, and $r(b)$, we have:\n$$\n\\sin(\\theta) = \\frac{\\|r(b)\\|_{2}}{\\|b\\|_{2}}\n$$\nTherefore, the relative condition number is:\n$$\n\\kappa_{\\mathrm{rel}}(A,b) = \\frac{1}{\\sin(\\theta)}\n$$\nThis means the relative condition number is the cosecant of the angle between the data vector $b$ and the column space of $A$. If $b$ is nearly in $\\operatorname{range}(A)$, the angle $\\theta$ is small, $\\sin(\\theta)$ is small, and $\\kappa_{\\mathrm{rel}}(A,b)$ becomes very large. This indicates that the problem of computing the residual is ill-conditioned in a relative sense when the residual itself is small compared to the data vector.\n\n### Part 3: Comparison of Algorithms in Exact Arithmetic\n\nThe question asks to compare the residual condition numbers across three different algorithms, under the constraint of \"exact arithmetic\". The condition numbers $\\kappa_{\\mathrm{abs}}(A,b)$ and $\\kappa_{\\mathrm{rel}}(A,b)$ are properties of the mathematical mapping $b \\mapsto r(b)$, not of the algorithm used to compute it. In exact arithmetic, all valid algorithms must compute the exact same mathematical result. Let's verify this for the three given methods.\n\n1.  **Normal Equations (N):** The solution $\\widehat{x}_{\\mathrm{N}}$ is found from $A^{\\top} A \\widehat{x}_{\\mathrm{N}} = A^{\\top} b$. This gives $\\widehat{x}_{\\mathrm{N}} = (A^{\\top} A)^{-1} A^{\\top} b = A^{\\dagger} b$. The residual is $r_{\\mathrm{N}} = b - A \\widehat{x}_{\\mathrm{N}} = b - A A^{\\dagger} b = (I - P_A) b$.\n\n2.  **QR Factorization (Q):** A thin $QR$ factorization of $A$ gives $A=QR$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular. Since the columns of $Q$ form an orthonormal basis for $\\operatorname{range}(A)$, the projector $P_A$ is given by $P_A = Q Q^{\\top}$. The residual is computed as $r_{\\mathrm{Q}} = (I - Q Q^{\\top}) b = (I - P_A) b$.\n\n3.  **Singular Value Decomposition (SVD):** The full SVD of $A$ is $A = U \\Sigma V^{\\top}$ where $U \\in \\mathbb{R}^{m \\times m}$ is orthogonal. We can partition $U$ as $U = \\begin{pmatrix} U_1 & U_2 \\end{pmatrix}$, where $U_1 \\in \\mathbb{R}^{m \\times n}$ and $U_2 \\in \\mathbb{R}^{m \\times (m-n)}$. The columns of $U_1$ form an orthonormal basis for $\\operatorname{range}(A)$, and the columns of $U_2$ form an orthonormal basis for its orthogonal complement, $\\operatorname{null}(A^\\top)$. The projector onto $\\operatorname{range}(A)$ is $P_A = U_1 U_1^{\\top}$, and the projector onto $\\operatorname{null}(A^\\top)$ is $I - P_A = U_2 U_2^{\\top}$. The residual is computed as $r_{\\mathrm{S}} = U_{2} U_{2}^{\\top} b = (I - P_A) b$.\n\nSince all three algorithms, in exact arithmetic, compute the mathematically identical residual vector $r(b) = (I-P_A)b$, they all represent the exact same mapping from $b$ to $r(b)$. The condition numbers are intrinsic properties of this mapping. Therefore, the residual condition numbers $\\kappa_{\\mathrm{abs}}(A,b)$ and $\\kappa_{\\mathrm{rel}}(A,b)$ are identical for all three algorithms. It is crucial to note that this conclusion holds only for exact arithmetic; in floating-point arithmetic, the stability and accuracy of these algorithms differ significantly.\n\n### Part 4: Numerical Calculation\n\nWe are given the data:\n$$\nA = \\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{3}} & 0\n\\end{pmatrix},\n\\qquad\nb = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 0.9999995\n\\end{pmatrix}\n$$\nWe need to compute $\\kappa_{\\mathrm{rel}}(A,b) = \\frac{\\|b\\|_{2}}{\\|r(b)\\|_{2}}$.\n\nFirst, let's analyze the matrix $A$. Its columns are $a_1 = (1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})^\\top$ and $a_2 = (1/\\sqrt{2}, -1/\\sqrt{2}, 0)^\\top$.\nWe compute their inner product: $a_1^\\top a_2 = \\frac{1}{\\sqrt{6}}(1 - 1 + 0) = 0$. They are orthogonal.\nWe compute their norms: $\\|a_1\\|_2^2 = \\frac{1}{3}(1+1+1)=1$ and $\\|a_2\\|_2^2 = \\frac{1}{2}(1+1+0)=1$.\nThe columns of $A$ are orthonormal. This means $A$ has the property $A^\\top A = I$, where $I$ is the $2 \\times 2$ identity matrix.\nThe projector onto $\\operatorname{range}(A)$ simplifies to $P_A = A(A^\\top A)^{-1} A^\\top = A I A^\\top = AA^\\top$.\nThe residual is $r(b) = (I - P_A)b = (I - AA^\\top)b$.\n\nLet's compute $r(b)$. It is easier to use the geometric interpretation. The columns of $A$, $\\{a_1, a_2\\}$, form an orthonormal basis for $\\operatorname{range}(A)$. A third vector, $a_3$, forming an orthonormal basis for $\\mathbb{R}^3$ with $a_1, a_2$ would span $\\operatorname{null}(A^\\top)$. We can find $a_3$ via the cross product or inspection. Let $u = (1,1,1)^\\top$ and $v = (1,-1,0)^\\top$. $u \\times v = (1, 1, -2)^\\top$. Normalizing this vector gives $a_3 = \\frac{1}{\\sqrt{6}}(1, 1, -2)^\\top$.\nThe projector onto $\\operatorname{null}(A^\\top)$ is $I-P_A = a_3 a_3^\\top$.\nThus, $r(b) = (a_3 a_3^\\top) b = a_3 (a_3^\\top b)$.\n\nLet $\\delta = 5 \\times 10^{-7}$, so $b = (1, 1, 1-\\delta)^\\top$.\nFirst, compute the projection scalar $a_3^\\top b$:\n$$\na_3^\\top b = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 & 1 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1-\\delta \\end{pmatrix} = \\frac{1}{\\sqrt{6}}(1 + 1 - 2(1-\\delta)) = \\frac{1}{\\sqrt{6}}(2 - 2 + 2\\delta) = \\frac{2\\delta}{\\sqrt{6}}\n$$\nNow, compute the residual vector $r(b)$:\n$$\nr(b) = a_3 (a_3^\\top b) = \\frac{1}{\\sqrt{6}}\\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\frac{2\\delta}{\\sqrt{6}} = \\frac{2\\delta}{6}\\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\frac{\\delta}{3}\\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\nNext, compute the norms $\\|r(b)\\|_{2}$ and $\\|b\\|_{2}$.\n$$\n\\|r(b)\\|_{2} = \\left\\| \\frac{\\delta}{3}\\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right\\|_{2} = \\frac{\\delta}{3} \\sqrt{1^2 + 1^2 + (-2)^2} = \\frac{\\delta\\sqrt{6}}{3}\n$$\n$$\n\\|b\\|_{2}^2 = 1^2 + 1^2 + (1-\\delta)^2 = 2 + 1 - 2\\delta + \\delta^2 = 3 - 2\\delta + \\delta^2\n$$\n$$\n\\|b\\|_{2} = \\sqrt{3 - 2\\delta + \\delta^2}\n$$\nFinally, compute the relative condition number:\n$$\n\\kappa_{\\mathrm{rel}}(A,b) = \\frac{\\|b\\|_{2}}{\\|r(b)\\|_{2}} = \\frac{\\sqrt{3 - 2\\delta + \\delta^2}}{\\frac{\\delta\\sqrt{6}}{3}}\n$$\nSubstitute $\\delta = 5 \\times 10^{-7}$:\n$$\n\\|b\\|_2 = \\sqrt{3 - 2(5 \\times 10^{-7}) + (5 \\times 10^{-7})^2} = \\sqrt{3 - 10^{-6} + 2.5 \\times 10^{-13}} = \\sqrt{2.99999900000025}\n$$\n$$\n\\|r(b)\\|_2 = \\frac{(5 \\times 10^{-7})\\sqrt{6}}{3}\n$$\n$$\n\\kappa_{\\mathrm{rel}}(A,b) = \\frac{\\sqrt{2.99999900000025}}{\\frac{5\\sqrt{6}}{3} \\times 10^{-7}} = \\frac{3 \\sqrt{2.99999900000025}}{5\\sqrt{6}} \\times 10^7\n$$\nNumerically evaluating this expression:\n$\\|b\\|_2 \\approx 1.7320505200547$\n$\\|r(b)\\|_2 \\approx 4.0824829046386 \\times 10^{-7}$\n$$\n\\kappa_{\\mathrm{rel}}(A,b) \\approx \\frac{1.7320505200547}{4.0824829046386 \\times 10^{-7}} \\approx 4242639.9829\n$$\nRounding to six significant figures, we get $4.24264 \\times 10^6$.",
            "answer": "$$\n\\boxed{4.24264 \\times 10^{6}}\n$$"
        }
    ]
}