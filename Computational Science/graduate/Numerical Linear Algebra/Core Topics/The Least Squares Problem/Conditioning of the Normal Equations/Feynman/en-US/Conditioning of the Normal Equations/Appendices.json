{
    "hands_on_practices": [
        {
            "introduction": "The stability of the normal equations is fundamentally tied to the geometry of the column space of the matrix $A$. When the columns of $A$ are nearly linearly dependent, they form a \"poor\" basis, and this is where numerical troubles begin. This first exercise provides a direct, hands-on calculation to build this intuition, showing how the condition number of $A^\\top A$ explodes as the angle between two basis vectors shrinks, directly linking geometric near-collinearity to numerical ill-conditioning .",
            "id": "2162074",
            "problem": "In numerical linear algebra, the conditioning of a matrix is a critical measure of its sensitivity to errors in input data. For a linear least-squares problem $A\\mathbf{x} \\approx \\mathbf{b}$, the sensitivity is governed by the condition number of the normal equations matrix, $A^\\top A$. Poor conditioning often arises when the columns of the matrix $A$ are nearly linearly dependent.\n\nConsider a simple $2 \\times 2$ matrix $A$ whose columns represent two basis vectors in a plane. The first column is the vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the second column is the vector $\\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$, where $\\theta$, in radians, is the angle between the two vectors. Assume $\\theta$ is a small positive angle. The matrix $A$ is thus given by $A = \\begin{pmatrix} 1 & \\cos\\theta \\\\ 0 & \\sin\\theta \\end{pmatrix}$.\n\nDetermine the leading-order asymptotic expression for the 2-norm condition number of the associated normal equations matrix, $A^\\top A$, in the limit as $\\theta \\to 0^+$. Your answer should be a simple expression in terms of $\\theta$ that captures the dominant behavior for small angles.",
            "solution": "We are asked for the 2-norm condition number of the normal equations matrix $A^{\\top}A$ for $A=\\begin{pmatrix} 1 & \\cos\\theta \\\\ 0 & \\sin\\theta \\end{pmatrix}$, in the limit $\\theta \\to 0^{+}$. For a symmetric positive definite matrix $M$, the 2-norm condition number is $\\kappa_{2}(M)=\\frac{\\lambda_{\\max}(M)}{\\lambda_{\\min}(M)}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $M$.\n\nCompute $A^{\\top}A$:\n$$\nA^{\\top}=\\begin{pmatrix} 1 & 0 \\\\ \\cos\\theta & \\sin\\theta \\end{pmatrix}, \\quad\nA^{\\top}A=\\begin{pmatrix} 1 & \\cos\\theta \\\\ \\cos\\theta & 1 \\end{pmatrix}.\n$$\nThe matrix $\\begin{pmatrix} 1 & \\cos\\theta \\\\ \\cos\\theta & 1 \\end{pmatrix}$ has eigenvalues\n$$\n\\lambda_{\\pm}=1 \\pm \\cos\\theta.\n$$\nTherefore,\n$$\n\\kappa_{2}(A^{\\top}A)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{1+\\cos\\theta}{1-\\cos\\theta}.\n$$\nFor small $\\theta$, use the Taylor expansion $\\cos\\theta=1-\\frac{\\theta^{2}}{2}+O(\\theta^{4})$. Hence\n$$\n1-\\cos\\theta=\\frac{\\theta^{2}}{2}+O(\\theta^{4}), \\quad 1+\\cos\\theta=2-\\frac{\\theta^{2}}{2}+O(\\theta^{4}).\n$$\nThus\n$$\n\\kappa_{2}(A^{\\top}A)=\\frac{2-\\frac{\\theta^{2}}{2}+O(\\theta^{4})}{\\frac{\\theta^{2}}{2}+O(\\theta^{4})}\n=\\frac{4}{\\theta^{2}}+O(1),\n$$\nso the leading-order asymptotic behavior as $\\theta \\to 0^{+}$ is\n$$\n\\kappa_{2}(A^{\\top}A)\\sim \\frac{4}{\\theta^{2}}.\n$$",
            "answer": "$$\\boxed{\\frac{4}{\\theta^{2}}}$$"
        },
        {
            "introduction": "What happens at the precise boundary between a solvable and an unsolvable system? When the normal equations matrix $A^\\top A$ is singular, the least-squares problem has infinitely many solutions, but an infinitesimal perturbation can restore uniqueness. This practice explores this critical transition, demonstrating how a small change in the problem data can cause a discontinuous jump in the rank of $A^\\top A$ and a dramatic explosion in the norm of the solution, a phenomenon that the condition number is designed to predict .",
            "id": "3540701",
            "problem": "Consider the overdetermined linear least-squares problem with a parameterized matrix family $A(\\varepsilon) \\in \\mathbb{R}^{2 \\times 2}$ and a fixed right-hand side $b \\in \\mathbb{R}^{2}$ given by\n$$\nA(\\varepsilon) \\;=\\; \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}, \n\\quad\nb \\;=\\; \\begin{pmatrix}1 \\\\ 1\\end{pmatrix},\n\\quad\n\\varepsilon \\ge 0.\n$$\nThe least-squares solution is defined as any $x(\\varepsilon) \\in \\mathbb{R}^{2}$ that minimizes $\\|A(\\varepsilon)x - b\\|_{2}$; equivalently, it satisfies the normal equations $A(\\varepsilon)^{\\top} A(\\varepsilon) x(\\varepsilon) = A(\\varepsilon)^{\\top} b$. Using only the foundational definitions of the Euclidean norm, least-squares optimality (orthogonality of the residual to the column space of $A(\\varepsilon)$), and matrix rank, answer the following:\n\n1. Prove that $\\operatorname{rank}\\!\\left(A(0)^{\\top} A(0)\\right) = 1$ and $\\operatorname{rank}\\!\\left(A(\\varepsilon)^{\\top} A(\\varepsilon)\\right) = 2$ for every $\\varepsilon > 0$, thereby constructing a perturbation example where an arbitrarily small change in $A$ alters the rank of $A^{\\top}A$.\n\n2. Characterize the complete solution set of the normal equations when $\\varepsilon = 0$, and derive the unique least-squares solution $x(\\varepsilon)$ for $\\varepsilon > 0$ from first principles (do not invoke any pre-packaged formulas without justification).\n\n3. Analyze the conditioning of the normal equations by determining the two-norm condition number $\\kappa_{2}\\!\\left(A(\\varepsilon)^{\\top} A(\\varepsilon)\\right)$ as a function of $\\varepsilon$, starting from the definition $\\kappa_{2}(M) = \\|M\\|_{2}\\,\\|M^{-1}\\|_{2}$ for an invertible matrix $M$ and the relationship between singular values and the spectral norm.\n\n4. To quantify the discontinuity in the solution set at $\\varepsilon = 0$, define $L$ as the limit\n$$\nL \\;=\\; \\lim_{\\varepsilon \\to 0^{+}} \\; \\varepsilon \\, \\|x(\\varepsilon)\\|_{2}.\n$$\nCompute $L$ exactly. Express the final value of $L$ as a single real number. No rounding is required.",
            "solution": "**Part 1: Rank Analysis**\n\nFirst, we compute the matrix product $A(\\varepsilon)^{\\top} A(\\varepsilon)$. The transpose of $A(\\varepsilon)$ is $A(\\varepsilon)^{\\top} = \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}$.\nTherefore, the matrix of the normal equations is:\n$$\nA(\\varepsilon)^{\\top} A(\\varepsilon) = \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix} \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix} = \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon^{2} \\end{pmatrix}\n$$\nFor the case $\\varepsilon = 0$, the matrix becomes $A(0)^{\\top} A(0) = \\begin{pmatrix}1 & 0 \\\\ 0 & 0 \\end{pmatrix}$. The column space is spanned by $\\begin{pmatrix}1 \\\\ 0 \\end{pmatrix}$, so its dimension is $1$. Thus, $\\operatorname{rank}\\!\\left(A(0)^{\\top} A(0)\\right) = 1$.\nFor the case $\\varepsilon > 0$, the matrix is $A(\\varepsilon)^{\\top} A(\\varepsilon) = \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon^{2} \\end{pmatrix}$. Since $\\varepsilon > 0$, it follows that $\\varepsilon^2 > 0$. The columns are linearly independent, so the matrix has full rank. Thus, $\\operatorname{rank}\\!\\left(A(\\varepsilon)^{\\top} A(\\varepsilon)\\right) = 2$.\nThis demonstrates that an arbitrarily small perturbation from $\\varepsilon=0$ to $\\varepsilon>0$ changes the rank of $A(\\varepsilon)^{\\top} A(\\varepsilon)$ from $1$ to $2$.\n\n**Part 2: Characterization of the Solution Set**\n\nWe first compute the right-hand side of the normal equations, $A(\\varepsilon)^{\\top} b$:\n$$\nA(\\varepsilon)^{\\top} b = \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ \\varepsilon \\end{pmatrix}\n$$\nThe normal equations are $A(\\varepsilon)^{\\top} A(\\varepsilon) x(\\varepsilon) = A(\\varepsilon)^{\\top} b$, which we write as a system for $x(\\varepsilon) = \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix}$:\n$$\n\\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon^{2} \\end{pmatrix} \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix}1 \\\\ \\varepsilon \\end{pmatrix}\n$$\nFor the case $\\varepsilon = 0$, the system becomes $\\begin{pmatrix}1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\end{pmatrix}$. This expands to $x_1 = 1$ and $0=0$. The second equation places no constraint on $x_2$. The complete solution set for $\\varepsilon=0$ is $\\{ x \\in \\mathbb{R}^2 \\mid x = \\begin{pmatrix}1 \\\\ c\\end{pmatrix}, c \\in \\mathbb{R} \\}$.\nFor the case $\\varepsilon > 0$, the matrix is invertible, so a unique solution exists. The system gives $x_1 = 1$ and $\\varepsilon^2 x_2 = \\varepsilon$. Since $\\varepsilon > 0$, we can solve for $x_2 = \\frac{\\varepsilon}{\\varepsilon^2} = \\frac{1}{\\varepsilon}$. The unique least-squares solution is $x(\\varepsilon) = \\begin{pmatrix}1 \\\\ 1/\\varepsilon \\end{pmatrix}$.\n\n**Part 3: Conditioning Analysis**\n\nWe determine the $2$-norm condition number of $M(\\varepsilon) = A(\\varepsilon)^{\\top} A(\\varepsilon) = \\begin{pmatrix}1 & 0 \\\\ 0 & \\varepsilon^{2} \\end{pmatrix}$ for $\\varepsilon > 0$. For a symmetric positive definite matrix, $\\kappa_2(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$. The eigenvalues are the diagonal entries: $\\lambda_1 = 1$ and $\\lambda_2 = \\varepsilon^2$.\nSo, $\\lambda_{\\max} = \\max(1, \\varepsilon^2)$ and $\\lambda_{\\min} = \\min(1, \\varepsilon^2)$.\n$$\n\\kappa_{2}(A(\\varepsilon)^{\\top} A(\\varepsilon)) = \\frac{\\max(1, \\varepsilon^2)}{\\min(1, \\varepsilon^2)}\n$$\nThis can be expressed piecewise:\n- If $0 < \\varepsilon \\le 1$, then $\\varepsilon^2 \\le 1$, so the condition number is $\\frac{1}{\\varepsilon^2}$.\n- If $\\varepsilon > 1$, then $\\varepsilon^2 > 1$, so the condition number is $\\frac{\\varepsilon^2}{1} = \\varepsilon^2$.\n\nThus, $\\kappa_{2}(A(\\varepsilon)^{\\top} A(\\varepsilon)) = \\begin{cases} 1/\\varepsilon^2 & \\text{if } 0 < \\varepsilon \\le 1 \\\\ \\varepsilon^2 & \\text{if } \\varepsilon > 1 \\end{cases}$. As $\\varepsilon \\to 0^{+}$, $\\kappa_{2} \\to \\infty$, indicating ill-conditioning.\n\n**Part 4: Limit Computation**\n\nWe need to compute $L = \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon \\|x(\\varepsilon)\\|_{2}$. From Part 2, the solution for $\\varepsilon > 0$ is $x(\\varepsilon) = \\begin{pmatrix}1 \\\\ 1/\\varepsilon \\end{pmatrix}$.\nFirst, we find the Euclidean norm of $x(\\varepsilon)$:\n$$\n\\|x(\\varepsilon)\\|_{2} = \\left\\| \\begin{pmatrix}1 \\\\ 1/\\varepsilon \\end{pmatrix} \\right\\|_{2} = \\sqrt{1^2 + \\left(\\frac{1}{\\varepsilon}\\right)^2} = \\sqrt{1 + \\frac{1}{\\varepsilon^2}}\n$$\nNext, we form the expression inside the limit. Since $\\varepsilon > 0$, we can bring it inside the square root:\n$$\n\\varepsilon \\|x(\\varepsilon)\\|_{2} = \\varepsilon \\sqrt{1 + \\frac{1}{\\varepsilon^2}} = \\sqrt{\\varepsilon^2 \\left(1 + \\frac{1}{\\varepsilon^2}\\right)} = \\sqrt{\\varepsilon^2 + 1}\n$$\nFinally, we evaluate the limit. The function is continuous at $\\varepsilon=0$:\n$$\nL = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\varepsilon^2 + 1} = \\sqrt{0^2 + 1} = \\sqrt{1} = 1\n$$\nThe value of the limit is exactly $1$.",
            "answer": "$$ \\boxed{1} $$"
        },
        {
            "introduction": "The theoretical concept of a condition number finds its practical importance in predicting how errors propagate. In the real world of finite-precision computing, even representing the Gram matrix $G = A^\\top A$ can introduce a small backward error. This final exercise makes the connection explicit by having you quantify how the large condition number of an ill-conditioned Gram matrix acts as a powerful amplifier, turning a tiny, seemingly harmless backward error in the matrix into a large, and potentially catastrophic, forward error in the final solution .",
            "id": "3540715",
            "problem": "Consider the overdetermined linear least-squares problem defined by the full column-rank matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and vector $b \\in \\mathbb{R}^{2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\tau \\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 2 \\\\ 2+\\tau \\end{pmatrix},\n$$\nwith a small positive parameter $\\tau \\in \\mathbb{R}$, $\\tau > 0$. The least-squares solution $x \\in \\mathbb{R}^{2}$ is obtained by solving the normal equations $A^{\\top}A\\,x = A^{\\top}b$, where the Gram matrix is $G = A^{\\top}A$. Assume one computes an approximate solution $\\widehat{x}$ by solving a perturbed system $(G + \\Delta G)\\,\\widehat{x} = A^{\\top}b$ with a small backward error in the Gram matrix, quantified by $\\|\\Delta G\\|_{2}/\\|G\\|_{2} = \\eta_{G}$ in the spectral norm. \n\nUsing only the following foundational facts:\n- The spectral norm condition number of a nonsingular symmetric positive definite matrix $M$ is $\\kappa_{2}(M) = \\|M\\|_{2}\\,\\|M^{-1}\\|_{2}$, which equals the ratio of its largest to smallest eigenvalues.\n- For $M$ symmetric positive definite, the eigenvalues are real and positive, and for a $2 \\times 2$ matrix they can be expressed solely in terms of its trace and determinant.\n- First-order perturbation theory for linear systems implies that, in the worst case, the relative forward error in solving $M x = c$ scales like $\\kappa_{2}(M)$ times the relative backward error in $M$.\n\nProceed as follows:\n1. Derive the Gram matrix $G = A^{\\top}A$ explicitly in terms of $\\tau$.\n2. Compute the trace and determinant of $G$ and then derive the exact expression of $\\kappa_{2}(G)$ in terms of $\\tau$.\n3. Specialize to the case $\\tau = 10^{-6}$ and a Gram backward error magnitude $\\eta_{G} = 10^{-8}$, and, under the worst-case alignment, compute the predicted amplification factor $\\rho$ of the relative forward error in $x$ with respect to the relative backward error in $G$, namely $\\rho = \\kappa_{2}(G)\\,\\eta_{G}$.\n\nRound your final numerical answer for $\\rho$ to three significant figures and express it in standard scientific notation $a \\times 10^{b}$, where $1 \\leq |a| < 10$ and $b$ is an integer. The final answer must be a single number without units.",
            "solution": "**1. Derivation of the Gram Matrix $G$**\n\nThe Gram matrix is $G = A^{\\top}A$. With $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\tau \\end{pmatrix}$, its transpose is $A^{\\top} = A$ since $A$ is symmetric. Thus, $G = A^2$.\n$$G = A^{\\top}A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\tau \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\tau \\end{pmatrix} = \\begin{pmatrix} 2 & 2+\\tau \\\\ 2+\\tau & 1 + (1+\\tau)^2 \\end{pmatrix} = \\begin{pmatrix} 2 & 2+\\tau \\\\ 2+\\tau & 2+2\\tau+\\tau^2 \\end{pmatrix}$$\n\n**2. Derivation of the Condition Number $\\kappa_{2}(G)$**\n\nThe spectral condition number $\\kappa_{2}(G)$ is the ratio of the largest eigenvalue $\\lambda_{\\max}$ to the smallest eigenvalue $\\lambda_{\\min}$ of $G$. Since $G=A^2$ and $A$ is symmetric, the eigenvalues of $G$ are the squares of the eigenvalues of $A$, denoted $\\mu$. The characteristic equation for $A$ is $\\det(A - \\mu I)=0$:\n$$(1-\\mu)(1+\\tau-\\mu) - 1 = \\mu^2 - (2+\\tau)\\mu + \\tau = 0$$\nThe eigenvalues of $A$ are $\\mu_{1,2} = \\frac{(2+\\tau) \\pm \\sqrt{(2+\\tau)^2 - 4\\tau}}{2} = \\frac{2+\\tau \\pm \\sqrt{4+\\tau^2}}{2}$.\nThe eigenvalues of $G$ are $\\lambda_{1,2} = \\mu_{1,2}^2$. The condition number is the ratio:\n$$\\kappa_{2}(G) = \\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)} = \\left( \\frac{2+\\tau + \\sqrt{4+\\tau^2}}{2+\\tau - \\sqrt{4+\\tau^2}} \\right)^2$$\n\n**3. Computation of the Amplification Factor $\\rho$**\n\nGiven $\\tau = 10^{-6}$ and $\\eta_{G} = 10^{-8}$, we compute $\\rho = \\kappa_{2}(G)\\,\\eta_{G}$.\nFor a small value of $\\tau$, we can use a Taylor series approximation. The expression for the condition number has the leading-order behavior $\\kappa_{2}(G) \\approx 16/\\tau^2$. More accurately, the expansion is $\\kappa_{2}(G) \\approx \\frac{16}{\\tau^2} + \\frac{16}{\\tau} + O(1)$.\nSubstituting $\\tau = 10^{-6}$:\n$$\\kappa_{2}(G) \\approx \\frac{16}{(10^{-6})^2} + \\frac{16}{10^{-6}} = 16 \\times 10^{12} + 16 \\times 10^6 = 1.6 \\times 10^{13} + 1.6 \\times 10^7$$\nThe second term is negligible compared to the first for this calculation.\nNow, we compute the amplification factor $\\rho$:\n$$\\rho = \\kappa_{2}(G) \\eta_{G} \\approx (1.6 \\times 10^{13} + 1.6 \\times 10^7) \\times 10^{-8}$$\n$$\\rho \\approx (1.6 \\times 10^{13} \\times 10^{-8}) + (1.6 \\times 10^7 \\times 10^{-8}) = 1.6 \\times 10^5 + 0.16 = 160000.16$$\nThe problem requires rounding to three significant figures. Expressed in scientific notation, this is $1.6000016 \\times 10^5$.\nRounding to three significant figures, we get $1.60 \\times 10^5$.",
            "answer": "$$\\boxed{1.60 \\times 10^{5}}$$"
        }
    ]
}