{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering Padé approximants is to understand their construction from first principles. This exercise guides you through the fundamental process of deriving the celebrated $[2/2]$ approximant for the exponential function by systematically matching its Taylor series. This hands-on calculation solidifies the connection between the rational function's coefficients and the function it approximates, forming the bedrock for its application to matrices .",
            "id": "3564074",
            "problem": "Let $f(z) = \\exp(z)$ denote the scalar exponential function, defined by its Taylor series $f(z) = \\sum_{k=0}^{\\infty} \\frac{z^{k}}{k!}$, and let $\\exp(A)$ for a square matrix $A$ be defined by the same series with $z$ replaced by $A$. The $[m/n]$ Padé approximant to a function $f(z)$ is the rational function $r_{mn}(z) = \\frac{p_{m}(z)}{q_{n}(z)}$ with $\\deg p_{m} \\leq m$, $\\deg q_{n} \\leq n$, $q_{n}(0) = 1$, such that the Maclaurin series of $q_{n}(z) f(z) - p_{m}(z)$ vanishes up to and including the term of order $z^{m+n}$.\n\nStarting only from these definitions, construct explicitly the $[2/2]$ Padé approximant $r_{22}(z)$ for $f(z) = \\exp(z)$ by determining the coefficients of $p_{2}(z)$ and $q_{2}(z)$ through series matching. Then, verify that the truncation error satisfies $\\exp(z) - r_{22}(z) = O(z^{5})$ as $z \\to 0$ by identifying the first nonzero term in the expansion of $q_{2}(z)\\exp(z) - p_{2}(z)$. Finally, explain why the same rational function $r_{22}(A)$ serves as an $O(\\|A\\|^{5})$ approximation to $\\exp(A)$ for matrices $A$ of sufficiently small norm in any consistent submultiplicative matrix norm, provided $q_{2}(A)$ is invertible.\n\nProvide the explicit rational function $r_{22}(z)$ as your final answer in closed form. No numerical rounding is required.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is based on standard definitions from numerical analysis and approximation theory and presents a formalizable task with a unique solution.\n\nThe problem asks for the construction and verification of the $[2/2]$ Padé approximant to $f(z) = \\exp(z)$, and an explanation of its application to matrix exponentials.\n\nLet the $[2/2]$ Padé approximant be denoted by $r_{22}(z) = \\frac{p_{2}(z)}{q_{2}(z)}$.\nAccording to the problem definition, the degrees of the polynomials are $\\deg p_{2} \\leq 2$ and $\\deg q_{2} \\leq 2$. Let their forms be:\n$p_{2}(z) = a_0 + a_1 z + a_2 z^2$\n$q_{2}(z) = b_0 + b_1 z + b_2 z^2$\n\nThe normalization condition is $q_{2}(0) = 1$, which implies $b_0 = 1$. Thus, the denominator is $q_{2}(z) = 1 + b_1 z + b_2 z^2$.\n\nThe core defining property of the Padé approximant is that the series expansion of the error function $q_{n}(z)f(z) - p_{m}(z)$ has its leading terms equal to zero. For the $[2/2]$ case (with $m=2, n=2$), we require:\n$q_{2}(z) \\exp(z) - p_{2}(z) = C z^{m+n+1} + \\dots = O(z^{5})$\nThis means that the coefficients of the terms $z^0, z^1, z^2, z^3, z^4$ in the Maclaurin series of $q_{2}(z) \\exp(z) - p_{2}(z)$ must all be zero.\n\nFirst, we write out the Maclaurin series for $\\exp(z)$:\n$\\exp(z) = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\frac{z^4}{4!} + \\dots = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24} + O(z^5)$\n\nNext, we expand the product $q_{2}(z) \\exp(z)$:\n$q_{2}(z) \\exp(z) = (1 + b_1 z + b_2 z^2) \\left( 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24} + O(z^5) \\right)$\n\nWe collect the coefficients of the powers of $z$ in this product, up to order $z^4$:\nCoefficient of $z^0$: $1$\nCoefficient of $z^1$: $1 + b_1$\nCoefficient of $z^2$: $\\frac{1}{2} + b_1 + b_2$\nCoefficient of $z^3$: $\\frac{1}{6} + \\frac{b_1}{2} + b_2$\nCoefficient of $z^4$: $\\frac{1}{24} + \\frac{b_1}{6} + \\frac{b_2}{2}$\n\nNow we enforce the condition $q_{2}(z)\\exp(z) - p_{2}(z) = O(z^5)$. This implies that $p_2(z)$ must be the polynomial part of the series for $q_2(z)\\exp(z)$ up to degree $2$, and the subsequent coefficients of $z^3$ and $z^4$ must be zero.\n\nMatching coefficients for $p_2(z) = a_0 + a_1 z + a_2 z^2$:\n- Coeff of $z^0$: $a_0 = 1$\n- Coeff of $z^1$: $a_1 = 1 + b_1$\n- Coeff of $z^2$: $a_2 = \\frac{1}{2} + b_1 + b_2$\n\nSetting the next two coefficients to zero to determine $b_1$ and $b_2$:\n- Coeff of $z^3$: $\\frac{1}{6} + \\frac{b_1}{2} + b_2 = 0$\n- Coeff of $z^4$: $\\frac{1}{24} + \\frac{b_1}{6} + \\frac{b_2}{2} = 0$\n\nThis is a system of two linear equations for $b_1$ and $b_2$:\n1. $\\frac{1}{2} b_1 + b_2 = -\\frac{1}{6}$\n2. $\\frac{1}{6} b_1 + \\frac{1}{2} b_2 = -\\frac{1}{24}$\n\nMultiply the second equation by $2$: $\\frac{1}{3} b_1 + b_2 = -\\frac{1}{12}$.\nSubtract this new equation from the first equation:\n$(\\frac{1}{2} b_1 - \\frac{1}{3} b_1) + (b_2 - b_2) = -\\frac{1}{6} - (-\\frac{1}{12})$\n$\\frac{3-2}{6} b_1 = -\\frac{2}{12} + \\frac{1}{12}$\n$\\frac{1}{6} b_1 = -\\frac{1}{12} \\implies b_1 = -\\frac{6}{12} = -\\frac{1}{2}$\n\nSubstitute $b_1 = -1/2$ into the first equation:\n$\\frac{1}{2} \\left(-\\frac{1}{2}\\right) + b_2 = -\\frac{1}{6}$\n$-\\frac{1}{4} + b_2 = -\\frac{1}{6} \\implies b_2 = \\frac{1}{4} - \\frac{1}{6} = \\frac{3-2}{12} = \\frac{1}{12}$\n\nNow we have the coefficients for $q_2(z)$: $b_1 = -1/2$ and $b_2 = 1/12$.\n$q_{2}(z) = 1 - \\frac{1}{2}z + \\frac{1}{12}z^2$\n\nWe can find the coefficients for $p_2(z)$:\n$a_0 = 1$\n$a_1 = 1 + b_1 = 1 - \\frac{1}{2} = \\frac{1}{2}$\n$a_2 = \\frac{1}{2} + b_1 + b_2 = \\frac{1}{2} - \\frac{1}{2} + \\frac{1}{12} = \\frac{1}{12}$\n\nThus, the numerator polynomial is:\n$p_{2}(z) = 1 + \\frac{1}{2}z + \\frac{1}{12}z^2$\n\nThe $[2/2]$ Padé approximant $r_{22}(z)$ for $\\exp(z)$ is therefore:\n$r_{22}(z) = \\frac{1 + \\frac{1}{2}z + \\frac{1}{12}z^2}{1 - \\frac{1}{2}z + \\frac{1}{12}z^2}$\n\nTo verify the truncation error, we identify the first nonzero term in the expansion of $q_{2}(z)\\exp(z) - p_{2}(z)$. By construction, the terms up to $z^4$ are zero. We compute the coefficient of $z^5$ in the product $q_{2}(z)\\exp(z)$:\nCoeff of $z^5$: (coeff $z^0$ in $q_2$) $\\cdot$ (coeff $z^5$ in $\\exp$) + (coeff $z^1$ in $q_2$) $\\cdot$ (coeff $z^4$ in $\\exp$) + (coeff $z^2$ in $q_2$) $\\cdot$ (coeff $z^3$ in $\\exp$)\n$= 1 \\cdot \\frac{1}{5!} + b_1 \\cdot \\frac{1}{4!} + b_2 \\cdot \\frac{1}{3!}$\n$= 1 \\cdot \\frac{1}{120} + \\left(-\\frac{1}{2}\\right) \\cdot \\frac{1}{24} + \\left(\\frac{1}{12}\\right) \\cdot \\frac{1}{6}$\n$= \\frac{1}{120} - \\frac{1}{48} + \\frac{1}{72}$\nThe least common multiple of $120$, $48$, and $72$ is $720$.\n$= \\frac{6}{720} - \\frac{15}{720} + \\frac{10}{720} = \\frac{6 - 15 + 10}{720} = \\frac{1}{720}$\n\nSo, $q_{2}(z)\\exp(z) - p_{2}(z) = \\frac{1}{720}z^5 + O(z^6)$.\nThe error of the Padé approximant itself is $\\exp(z) - r_{22}(z) = \\frac{q_2(z)\\exp(z) - p_2(z)}{q_2(z)}$.\nSince $q_2(z) \\to 1$ as $z \\to 0$, the error is:\n$\\exp(z) - r_{22}(z) = \\frac{\\frac{1}{720}z^5 + O(z^6)}{1 + O(z)} = \\frac{1}{720}z^5 + O(z^6)$.\nThis confirms that the error is indeed $O(z^5)$.\n\nFinally, we explain why $r_{22}(A)$ is an $O(\\|A\\|^5)$ approximation to $\\exp(A)$ for a square matrix $A$ with sufficiently small norm.\nThe definitions of $\\exp(A)$ and the polynomials $p_2(A)$ and $q_2(A)$ are given by substituting the matrix $A$ for the scalar $z$ in the respective series and polynomial expressions. The scalar identity $q_2(z)\\exp(z) - p_2(z) = \\frac{1}{720}z^5 + O(z^6)$ is an identity between convergent power series. This identity holds true when $z$ is replaced by a matrix $A$:\n$q_2(A)\\exp(A) - p_2(A) = \\frac{1}{720}A^5 + \\sum_{k=6}^{\\infty} c_k A^k$, where the series converges if $\\|A\\|$ is within the radius of convergence.\nSince $p_2(A)$, $q_2(A)$, and $\\exp(A)$ are all power series in $A$, they commute with each other. Thus, $\\exp(A)q_2(A) = q_2(A)\\exp(A)$.\nIf $q_2(A) = I - \\frac{1}{2}A + \\frac{1}{12}A^2$ is invertible, we can define $r_{22}(A) = p_2(A)[q_2(A)]^{-1}$.\nThe error is $\\exp(A) - r_{22}(A)$. We can write:\n$\\exp(A) - r_{22}(A) = [\\exp(A)q_2(A) - p_2(A)][q_2(A)]^{-1} = [q_2(A)\\exp(A) - p_2(A)][q_2(A)]^{-1}$\nSubstituting the series expansion for the error term:\n$\\exp(A) - r_{22}(A) = \\left( \\frac{1}{720}A^5 + \\sum_{k=6}^{\\infty} c_k A^k \\right) [q_2(A)]^{-1}$\nNow we take a consistent, submultiplicative matrix norm $\\| \\cdot \\|$:\n$\\|\\exp(A) - r_{22}(A)\\| \\leq \\left\\| \\frac{1}{720}A^5 + \\sum_{k=6}^{\\infty} c_k A^k \\right\\| \\cdot \\left\\| [q_2(A)]^{-1} \\right\\|$\nFor the first term, as $\\|A\\| \\to 0$:\n$\\left\\| \\frac{1}{720}A^5 + \\dots \\right\\| \\leq \\frac{1}{720}\\|A\\|^5 + \\sum_{k=6}^{\\infty} |c_k| \\|A\\|^k = O(\\|A\\|^5)$\nFor the inverse term, $q_2(A) = I - (\\frac{1}{2}A - \\frac{1}{12}A^2)$. If $\\|\\frac{1}{2}A - \\frac{1}{12}A^2\\| < 1$, which is true for sufficiently small $\\|A\\|$, then $q_2(A)$ is invertible and its inverse can be represented by a convergent Neumann series. As $\\|A\\| \\to 0$, $q_2(A) \\to I$, so $\\|[q_2(A)]^{-1}\\| \\to \\|I\\| = 1$. This means $\\|[q_2(A)]^{-1}\\|$ is bounded for small $\\|A\\|$.\nCombining these results, we get:\n$\\|\\exp(A) - r_{22}(A)\\| \\leq O(\\|A\\|^5) \\cdot (\\text{a bounded quantity})$\nTherefore, $\\|\\exp(A) - r_{22}(A)\\| = O(\\|A\\|^5)$.",
            "answer": "$$\n\\boxed{\\frac{1 + \\frac{1}{2}z + \\frac{1}{12}z^2}{1 - \\frac{1}{2}z + \\frac{1}{12}z^2}}\n$$"
        },
        {
            "introduction": "Evaluating a function on a matrix is straightforward for diagonalizable cases, but the theory's true utility is revealed when dealing with non-diagonalizable (defective) matrices. This practice provides a concrete example using a Jordan block, the fundamental building block for all matrices . You will see precisely how the derivatives of the scalar rational function determine the structure of the resulting matrix, making an abstract concept tangible.",
            "id": "3564125",
            "problem": "Let $A \\in \\mathbb{C}^{3 \\times 3}$ be a defective matrix with a single Jordan block of size $3$ for the eigenvalue $\\lambda = 0$, explicitly $A = J_{3}(0)$, where\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nConsider the rational Padé approximant $r$ of the function $\\log(1+z)$ centered at $z=0$, defined by\n$$\nr(z) = \\frac{2z}{2+z}.\n$$\nUsing the definition of matrix functions via analytic continuation around the eigenvalue and the Jordan canonical structure, compute $r(A)$ and explicitly quantify how the derivatives $r^{(k)}(\\lambda)$ contribute to the entries of $r(A)$ for this $3 \\times 3$ Jordan block. Express your final answer as a single closed-form matrix expression. No rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of matrix functions, well-posed with all necessary information provided, and stated objectively. We can proceed with a formal solution.\n\nThe problem asks for the computation of $r(A)$, where $A$ is a $3 \\times 3$ Jordan block with eigenvalue $\\lambda=0$, and $r(z)$ is a given rational function. The definition of a function $f$ of a Jordan block $J_k(\\lambda) \\in \\mathbb{C}^{k \\times k}$ is given by a specific upper triangular Toeplitz matrix structure. For a Jordan block of size $k=3$ corresponding to an eigenvalue $\\lambda$, this definition is:\n$$\nf(J_3(\\lambda)) =\n\\begin{pmatrix}\nf(\\lambda) & \\frac{f^{(1)}(\\lambda)}{1!} & \\frac{f^{(2)}(\\lambda)}{2!} \\\\\n0 & f(\\lambda) & \\frac{f^{(1)}(\\lambda)}{1!} \\\\\n0 & 0 & f(\\lambda)\n\\end{pmatrix}\n$$\nThe entries of this matrix, $(f(J_3(\\lambda)))_{i,j}$, are determined by the derivatives of the function $f$ evaluated at the eigenvalue $\\lambda$. Specifically, for $j \\ge i$, the entry is given by $\\frac{f^{(j-i)}(\\lambda)}{(j-i)!}$.\n\nIn this problem, the matrix is $A = J_3(0)$, which means the block size is $k=3$ and the eigenvalue is $\\lambda=0$. The function is the rational Padé approximant $r(z) = \\frac{2z}{2+z}$. Therefore, to compute $r(A)$, we must evaluate $r(z)$ and its first two derivatives at $z=\\lambda=0$.\n\nThe matrix $r(A)$ will have the form:\n$$\nr(A) = r(J_3(0)) =\n\\begin{pmatrix}\nr(0) & r'(0) & \\frac{r''(0)}{2!} \\\\\n0 & r(0) & r'(0) \\\\\n0 & 0 & r(0)\n\\end{pmatrix}\n$$\nThis structure explicitly quantifies how the derivatives $r^{(k)}(\\lambda)$ contribute to the entries of $r(A)$. The diagonal entries are determined by $r(0)$, the first superdiagonal by $r'(0)$, and the second superdiagonal by $\\frac{r''(0)}{2}$.\n\nWe now compute the required derivatives of $r(z)$. It is convenient to first rewrite $r(z)$ using polynomial long division or algebraic manipulation:\n$$\nr(z) = \\frac{2z}{2+z} = \\frac{2(z+2) - 4}{z+2} = 2 - \\frac{4}{2+z} = 2 - 4(2+z)^{-1}\n$$\nThis form simplifies differentiation.\n\nFirst, we evaluate the function itself at $z=0$:\n$$\nr(0) = 2 - 4(2+0)^{-1} = 2 - \\frac{4}{2} = 2 - 2 = 0\n$$\n\nNext, we compute the first derivative, $r'(z)$:\n$$\nr'(z) = \\frac{d}{dz} \\left( 2 - 4(2+z)^{-1} \\right) = -4(-1)(2+z)^{-2} = 4(2+z)^{-2} = \\frac{4}{(2+z)^2}\n$$\nEvaluating at $z=0$:\n$$\nr'(0) = \\frac{4}{(2+0)^2} = \\frac{4}{4} = 1\n$$\n\nThen, we compute the second derivative, $r''(z)$:\n$$\nr''(z) = \\frac{d}{dz} \\left( 4(2+z)^{-2} \\right) = 4(-2)(2+z)^{-3} = -8(2+z)^{-3} = \\frac{-8}{(2+z)^3}\n$$\nEvaluating at $z=0$:\n$$\nr''(0) = \\frac{-8}{(2+0)^3} = \\frac{-8}{8} = -1\n$$\n\nNow we have all the components needed to construct the matrix $r(A)$. The required values are:\n- $r(0) = 0$\n- $r'(0) = 1$\n- $r''(0) = -1$\n- $2! = 2$\n\nSubstituting these values into the matrix structure for $r(J_3(0))$:\n$$\nr(A) =\n\\begin{pmatrix}\n0 & 1 & \\frac{-1}{2} \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\n\nThis is the final closed-form matrix expression for $r(A)$. As an alternative check, since $A$ is nilpotent with $A^3=0$, we can use the Taylor series expansion of $r(z)$ around $z=0$:\n$r(A) = r(0)I + r'(0)A + \\frac{r''(0)}{2!}A^2$.\nWith $A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}$ and $A^2 = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$, we get:\n$r(A) = 0 \\cdot I + 1 \\cdot A + \\frac{-1}{2} \\cdot A^2 = A - \\frac{1}{2}A^2$.\n$$\nr(A) = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 & -\\frac{1}{2} \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThis confirms our result.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & 1 & -\\frac{1}{2} \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Bridging theory and practice often requires clever algorithmic design. Instead of evaluating a rational function as $p(A)[q(A)]^{-1}$, a more stable and efficient method is to use its partial fraction expansion, which transforms the problem into solving a series of independent linear systems . This exercise challenges you to implement this powerful technique, which is not only numerically robust but also opens the door to significant parallel acceleration on modern hardware like GPUs.",
            "id": "3564069",
            "problem": "You are asked to design and implement a numerical method to approximate the action of a matrix function on a vector using Padé approximants and to quantify a theoretical performance model for batched shifted linear system solves when the shifts preserve the sparsity pattern. The focus is on the matrix exponential as a representative analytic function, and on a cost model that compares a central processing unit (CPU) sequential strategy against a graphics processing unit (GPU) batched strategy.\n\nThe fundamental base for your design must be the following well-tested definitions and facts in numerical linear algebra:\n- For a square matrix $\\displaystyle A \\in \\mathbb{C}^{n \\times n}$ and an analytic function $\\displaystyle f$, the matrix function $\\displaystyle f(A)$ is defined via the convergent power series when applicable: $\\displaystyle f(A) = \\sum_{k=0}^{\\infty} c_k A^k$, where $\\displaystyle f(z) = \\sum_{k=0}^{\\infty} c_k z^k$ is the Taylor series of $\\displaystyle f$ around $\\displaystyle 0$.\n- A Padé approximant $\\displaystyle r_{m,m}(z) = \\frac{p_m(z)}{q_m(z)}$ of order $\\displaystyle (m,m)$ to $\\displaystyle f(z)$ is the unique rational function whose Maclaurin series matches that of $\\displaystyle f$ up to order $\\displaystyle 2m$, that is, $\\displaystyle f(z) - r_{m,m}(z) = \\mathcal{O}(z^{2m+1})$.\n- Any rational function with denominator having simple roots admits a partial fraction expansion: $\\displaystyle \\frac{p(z)}{q(z)} = c_0 + \\sum_{j=1}^{K} \\frac{\\alpha_j}{z - \\zeta_j}$, where $\\displaystyle \\zeta_j$ are the roots (poles) of $\\displaystyle q$, $\\displaystyle \\alpha_j$ are the residues, and $\\displaystyle c_0$ is a constant if the rational function is not proper. The residues satisfy $\\displaystyle \\alpha_j = \\frac{p(\\zeta_j)}{q'(\\zeta_j)}$ for simple poles.\n- For matrices, the action of $\\displaystyle r(A)$ on a vector $\\displaystyle b$ can be realized using the resolvent: $\\displaystyle r(A)b = c_0 b + \\sum_{j=1}^{K} \\alpha_j (A - \\zeta_j I)^{-1} b$, provided no pole coincides with the spectrum of $\\displaystyle A$.\n- For the matrix exponential, the diagonal Padé approximant $\\displaystyle r_{m,m}$ to $\\displaystyle e^{z}$ can be constructed from the Maclaurin series coefficients $\\displaystyle c_k = \\frac{1}{k!}$ for $\\displaystyle k = 0,1,\\dots,2m$.\n\nYour tasks are:\n1) Construct the diagonal Padé approximant $\\displaystyle r_{m,m}(z)$ to $\\displaystyle e^{z}$ by matching the Maclaurin series coefficients $\\displaystyle c_k = \\frac{1}{k!}$ for $\\displaystyle k = 0,1,\\dots,2m$. Then derive a stable partial fraction form $\\displaystyle r_{m,m}(z) = c_0 + \\sum_{j=1}^{K} \\frac{\\alpha_j}{z - \\zeta_j}$ using the fundamental definition of residues and constants for improper rational functions. Your implementation must compute $\\displaystyle c_0$, the poles $\\displaystyle \\{\\zeta_j\\}_{j=1}^{K}$, and the residues $\\displaystyle \\{\\alpha_j\\}_{j=1}^{K}$ from the polynomials $\\displaystyle p_m$ and $\\displaystyle q_m$, without hard-coding any pre-tabulated Padé coefficients.\n2) Implement a function that, given a sparse matrix $\\displaystyle A \\in \\mathbb{R}^{n \\times n}$ and a vector $\\displaystyle b \\in \\mathbb{R}^{n}$, evaluates $\\displaystyle r_{m,m}(A)b$ using the partial fraction representation by solving the shifted sparse linear systems $\\displaystyle (A - \\zeta_j I)x_j = b$ for $\\displaystyle j = 1,\\dots,K$ and combining them as $\\displaystyle y = c_0 b + \\sum_{j=1}^{K} \\alpha_j x_j$. You must correctly handle complex arithmetic and return the real part of the result (the imaginary part should cancel to numerical roundoff for real $\\displaystyle A$ and $\\displaystyle b$).\n3) For validation, compute a high-accuracy reference $\\displaystyle y_{\\mathrm{ref}} \\approx e^{A} b$ using a standard algorithm for the matrix exponential action and report the relative error $\\displaystyle \\|y - y_{\\mathrm{ref}}\\|_2 / \\|y_{\\mathrm{ref}}\\|_2$.\n4) Develop a theoretical performance model that quantifies a predicted speedup when solving the $\\displaystyle K$ shifted systems in batch on a graphics processing unit (GPU) versus sequentially on a central processing unit (CPU), under the assumption that all shifts $\\displaystyle \\zeta_j$ preserve the sparsity pattern of $\\displaystyle A - \\zeta_j I$. Use the following simple, dimensionally consistent cost model, where $\\displaystyle \\text{nnz}$ denotes the number of nonzeros of $\\displaystyle A$, $\\displaystyle n$ is the dimension, and $\\displaystyle K$ is the number of poles:\n- CPU (sequential per shift): $ T_{\\mathrm{CPU}} = K \\cdot \\text{nnz} ( a_c + b_c \\sqrt{n} + c_c ) $\n- GPU (batched across all shifts, with shared analysis and reduced per-shift constants): $ T_{\\mathrm{GPU}} = a_g \\cdot \\text{nnz} + K \\cdot \\text{nnz} ( \\rho b_g \\sqrt{n} + \\sigma c_g ) $\nUse the fixed constants\n$\\displaystyle a_c = 8 \\times 10^{-9}$, $\\displaystyle b_c = 5 \\times 10^{-8}$, $\\displaystyle c_c = 2 \\times 10^{-9}$, $\\displaystyle a_g = 1 \\times 10^{-9}$, $\\displaystyle b_g = 8 \\times 10^{-9}$, $\\displaystyle c_g = 5 \\times 10^{-10}$, $\\displaystyle \\rho = 0.5$, and $\\displaystyle \\sigma = 0.3$,\nwith times expressed in seconds and $\\displaystyle \\text{nnz}$ unitless.\n5) Apply your method to the following test suite. In each case construct $\\displaystyle A$ as a scaled one-dimensional discrete Laplacian with Dirichlet boundary conditions, i.e., $\\displaystyle L_n = \\operatorname{tridiag}(-1,2,-1) \\in \\mathbb{R}^{n \\times n}$ and $\\displaystyle A = -\\alpha L_n$. Form $\\displaystyle b$ with independent standard normal entries using the stated random seed. For each case compute:\n- The relative error $\\displaystyle \\|y - y_{\\mathrm{ref}}\\|_2 / \\|y_{\\mathrm{ref}}\\|_2$.\n- The predicted speedup $\\displaystyle S = T_{\\mathrm{CPU}} / T_{\\mathrm{GPU}}$.\nThe test suite parameter tuples $\\displaystyle (n,\\alpha,m,\\text{seed})$ are:\n- Case $\\displaystyle 1$: $\\displaystyle (40,\\, 0.10,\\, 6,\\, 12345)$.\n- Case $\\displaystyle 2$: $\\displaystyle (1,\\, 0.00,\\, 4,\\, 2024)$.\n- Case $\\displaystyle 3$: $\\displaystyle (60,\\, 0.08,\\, 8,\\, 31415)$.\n\nAngle units are not applicable. There are no physical units in the answer.\n\nYour program should produce a single line of output containing the results as a comma-separated Python-style list of floats in the order\n$\\displaystyle [\\text{err}_1, \\text{speedup}_1, \\text{err}_2, \\text{speedup}_2, \\text{err}_3, \\text{speedup}_3]$,\nwhere $\\displaystyle \\text{err}_i$ is the relative error and $\\displaystyle \\text{speedup}_i$ is the predicted speedup for case $\\displaystyle i$. Round each float to exactly $\\displaystyle 8$ decimal places before printing. The code must be fully self-contained and must not read any input.",
            "solution": "The user-provided problem is a valid numerical analysis task that is self-contained, scientifically grounded, and well-posed. It requires the design and implementation of a method to approximate the action of the matrix exponential on a vector, $y = e^A b$, using Padé approximants. The core of the method lies in the partial fraction expansion of the rational approximant, which transforms the problem into solving a set of shifted linear systems. The problem also specifies the derivation of a theoretical performance model comparing sequential and batched execution strategies. I will proceed with a full solution.\n\nThe solution is systematically structured according to the tasks defined in the problem statement.\n\n### Step 1: Padé Approximant for $e^z$\n\nA rational function $r_{m,m}(z) = p_m(z)/q_m(z)$, where $p_m$ and $q_m$ are polynomials of degree at most $m$, is the $(m,m)$-Padé approximant to an analytic function $f(z)$ if its Maclaurin series expansion agrees with that of $f(z)$ to the highest possible order. For $f(z) = e^z = \\sum_{k=0}^{\\infty} \\frac{z^k}{k!}$, this condition is $q_m(z)e^z - p_m(z) = \\mathcal{O}(z^{2m+1})$.\n\nBy solving the system of linear equations that arise from matching coefficients of powers of $z$ up to $z^{2m}$, one can derive the coefficients for the numerator $p_m(z) = \\sum_{k=0}^{m} a_k z^k$ and denominator $q_m(z) = \\sum_{k=0}^{m} b_k z^k$. The polynomials are unique up to a scaling factor, and are given by the formulas:\n$$\np_m(z) = \\sum_{k=0}^{m} \\frac{(2m-k)! m!}{(2m)! k! (m-k)!} z^k\n$$\n$$\nq_m(z) = \\sum_{k=0}^{m} \\frac{(2m-k)! m!}{(2m)! k! (m-k)!} (-z)^k\n$$\nNotably, the denominator is related to the numerator by $q_m(z) = p_m(-z)$.\n\n### Step 2: Partial Fraction Decomposition\n\nThe Padé approximant $r_{m,m}(z)$ is an improper rational function since $\\deg(p_m) = \\deg(q_m) = m$. It can be expressed using a partial fraction expansion. First, polynomial long division yields a constant term $c_0$ and a strictly proper rational function:\n$$\nr_{m,m}(z) = \\frac{p_m(z)}{q_m(z)} = c_0 + \\frac{s_m(z)}{q_m(z)}\n$$\nwhere $s_m(z) = p_m(z) - c_0 q_m(z)$ is the remainder polynomial with $\\deg(s_m) < m$. The constant $c_0$ is the ratio of the leading coefficients of $p_m(z)$ and $q_m(z)$, which is found to be $c_0 = (-1)^m$.\n\nThe poles of $r_{m,m}(z)$, denoted $\\{\\zeta_j\\}_{j=1}^{m}$, are the roots of the denominator polynomial $q_m(z)$. It is a known property for the Padé approximants of $e^z$ that these poles are simple and lie in the right half of the complex plane. The partial fraction expansion is then:\n$$\nr_{m,m}(z) = c_0 + \\sum_{j=1}^{m} \\frac{\\alpha_j}{z - \\zeta_j}\n$$\nThe residues $\\{\\alpha_j\\}_{j=1}^{m}$ corresponding to the simple poles are calculated using the formula:\n$$\n\\alpha_j = \\frac{s_m(\\zeta_j)}{q'_m(\\zeta_j)} = \\frac{p_m(\\zeta_j) - c_0 q_m(\\zeta_j)}{q'_m(\\zeta_j)}\n$$\nSince $q_m(\\zeta_j) = 0$ by definition of a pole, this simplifies to $\\alpha_j = p_m(\\zeta_j) / q'_m(zeta)$, where $q'_m(z)$ is the derivative of $q_m(z)$ with respect to $z$.\n\n### Step 3: Algorithm for Approximating $e^A b$\n\nReplacing the scalar variable $z$ with a matrix $A \\in \\mathbb{R}^{n \\times n}$, the action of the rational approximation on a vector $b \\in \\mathbb{R}^n$ is given by:\n$$\ny = r_{m,m}(A)b = \\left(c_0 I + \\sum_{j=1}^{m} \\alpha_j (A - \\zeta_j I)^{-1}\\right) b = c_0 b + \\sum_{j=1}^{m} \\alpha_j (A - \\zeta_j I)^{-1} b\n$$\nThis expression suggests an algorithm where we avoid forming the matrix inverses explicitly. Instead, we solve a series of shifted linear systems. Let $x_j = (A - \\zeta_j I)^{-1} b$. This is equivalent to solving for $x_j$ in:\n$$\n(A - \\zeta_j I) x_j = b, \\quad \\text{for } j=1, \\dots, m\n$$\nThe final approximation $y$ is then a linear combination of these solutions:\n$$\ny = c_0 b + \\sum_{j=1}^{m} \\alpha_j x_j\n$$\nThe matrix $A$ is real, but the poles $\\zeta_j$ and residues $\\alpha_j$ are complex and come in conjugate pairs. Consequently, the intermediate vectors $x_j$ and the final sum $y$ are complex. However, for a real matrix $A$ and real vector $b$, the imaginary part of the final result cancels out to within machine precision, so we take the real part of $y$ as the final answer.\n\nThe numerical implementation proceeds as follows:\n1.  For a given order $m$, compute the coefficients of $p_m(z)$ and $q_m(z)$.\n2.  Compute the constant $c_0$, the poles $\\{\\zeta_j\\}$, and residues $\\{\\alpha_j\\}$.\n3.  Initialize a complex vector $y_{approx} = c_0 b$.\n4.  For each pole-residue pair $(\\zeta_j, \\alpha_j)$:\n    a. Construct the sparse, complex, shifted matrix $M_j = A - \\zeta_j I$.\n    b. Solve the linear system $M_j x_j = b$ for $x_j$ using a sparse direct solver.\n    c. Accumulate the result: $y_{approx} \\leftarrow y_{approx} + \\alpha_j x_j$.\n5.  Return $\\text{Re}(y_{approx})$.\n\n### Step 4: Validation and Error Calculation\n\nTo validate the accuracy of the Padé approximation, the result $y$ is compared against a high-accuracy reference solution $y_{\\mathrm{ref}} \\approx e^A b$. This reference is computed using a state-of-the-art algorithm, specifically the one implemented in `scipy.sparse.linalg.expm_multiply`, which is based on Krylov subspace methods. The relative error is quantified using the Euclidean norm ($L_2$-norm):\n$$\n\\text{err} = \\frac{\\|y - y_{\\mathrm{ref}}\\|_2}{\\|y_{\\mathrm{ref}}\\|_2}\n$$\n\n### Step 5: Performance Model and Speedup\n\nA theoretical performance model is defined to estimate the computation time for solving the $m$ linear systems sequentially on a CPU versus in a batch on a GPU. The number of systems to solve is $K=m$. The matrix $A$ is a scaled discrete Laplacian of size $n \\times n$, which is tridiagonal, so its number of non-zero elements is $\\text{nnz} = 3n-2$ for $n > 1$ and $\\text{nnz}=1$ for $n=1$.\n\nThe model equations are provided:\n-   CPU time: $T_{\\mathrm{CPU}} = K \\cdot \\text{nnz} \\cdot (a_c + b_c \\sqrt{n} + c_c)$\n-   GPU time: $T_{\\mathrm{GPU}} = a_g \\cdot \\text{nnz} + K \\cdot \\text{nnz} ( \\rho b_g \\sqrt{n} + \\sigma c_g )$\n\nThe predicted speedup $S$ is the ratio of these two times:\n$$\nS = \\frac{T_{\\mathrm{CPU}}}{T_{\\mathrm{GPU}}}\n$$\nThe constants $a_c, b_c, c_c, a_g, b_g, c_g, \\rho, \\sigma$ are given fixed values. This model provides an estimate of the performance gain achievable through parallelization in a batched setting.\n\n### Step 6: Application to Test Cases\n\nThe described methodology is applied to each test case $(n, \\alpha, m, \\text{seed})$. For each case, the matrix $A = -\\alpha \\operatorname{tridiag}(-1, 2, -1)$ and vector $b$ (from a standard normal distribution with the specified seed) are constructed. The Padé approximation $y$, reference solution $y_{\\mathrm{ref}}$, relative error, and predicted speedup are then computed following the steps outlined above.",
            "answer": "```python\nimport numpy as np\nimport math\nfrom scipy.sparse import diags, identity\nfrom scipy.sparse.linalg import spsolve, expm_multiply\n\ndef get_pade_coeffs(m):\n    \"\"\"\n    Computes coefficients of the numerator p_m(z) and denominator q_m(z)\n    of the (m,m) Padé approximant to exp(z), in descending power order.\n    The formula for the coefficient of z^k in p_m(z) is:\n    c_k = (m! * (2m-k)!) / ((2m)! * k! * (m-k)!)\n    \"\"\"\n    if m < 0:\n        raise ValueError(\"Order m must be a non-negative integer.\")\n    p_coeffs = np.zeros(m + 1, dtype=float)\n    q_coeffs = np.zeros(m + 1, dtype=float)\n    \n    try:\n        fact_m = math.factorial(m)\n        fact_2m = math.factorial(2 * m)\n    except ValueError:\n        # Fallback for very large m, though not needed for this problem\n        return get_pade_coeffs_stable(m)\n\n    for k in range(m + 1):\n        # Coefficients for p_m(z)\n        coeff_val = (fact_m * math.factorial(2 * m - k)) / \\\n                    (fact_2m * math.factorial(k) * math.factorial(m - k))\n        p_coeffs[m - k] = coeff_val\n        # Coefficients for q_m(z) = p_m(-z)\n        q_coeffs[m - k] = ((-1)**k) * coeff_val\n        \n    return p_coeffs, q_coeffs\n\ndef get_pade_partial_fractions(m):\n    \"\"\"\n    Computes the partial fraction expansion of the (m,m) Padé approximant to exp(z).\n    Returns c0, poles (zeta), and residues (alpha).\n    \"\"\"\n    if m == 0:\n        return 1.0, np.array([]), np.array([])\n        \n    p_coeffs, q_coeffs = get_pade_coeffs(m)\n    \n    # Constant c0 from polynomial long division (ratio of leading coefficients)\n    c0 = p_coeffs[0] / q_coeffs[0] # This equals (-1)^m\n    \n    # Poles are the roots of the denominator polynomial q_m(z)\n    poles = np.roots(q_coeffs)\n    \n    # Residues alpha_j = s_m(zeta_j) / q'_m(zeta_j)\n    # where s_m(z) = p_m(z) - c0*q_m(z) is the remainder. Or, p_m(z)/q'_m(z) since q_m(zeta_j)=0\n    q_prime_coeffs = np.polyder(q_coeffs)\n    \n    p_at_poles = np.polyval(p_coeffs, poles)\n    q_prime_at_poles = np.polyval(q_prime_coeffs, poles)\n    \n    residues = p_at_poles / q_prime_at_poles\n    \n    return c0, poles, residues\n\ndef pade_approx_action(A, b, m):\n    \"\"\"\n    Computes y = r_m,m(A)b using partial fraction expansion.\n    \"\"\"\n    n = A.shape[0]\n    if n == 0:\n        return np.array([])\n        \n    c0, poles, residues = get_pade_partial_fractions(m)\n    \n    # Initialize result vector (must be complex)\n    y = c0 * b.astype(np.complex128)\n    \n    A_complex = A.astype(np.complex128)\n    \n    for pole, res in zip(poles, residues):\n        # Construct the shifted sparse matrix\n        M = A_complex - pole * identity(n, format='csc', dtype=np.complex128)\n        \n        # Solve the shifted linear system\n        x = spsolve(M, b)\n        \n        # Accumulate the result\n        y += res * x\n        \n    # For real A and b, the imaginary part should be close to zero\n    return y.real\n\ndef calculate_speedup(n, nnz, K):\n    \"\"\"\n    Calculates the theoretical speedup S = T_CPU / T_GPU based on the problem's model.\n    \"\"\"\n    # Model constants\n    a_c, b_c, c_c = 8e-9, 5e-8, 2e-9\n    a_g, b_g, c_g = 1e-9, 8e-9, 5e-10\n    rho, sigma = 0.5, 0.3\n    \n    if n == 0:\n        return 1.0\n\n    sqrt_n = math.sqrt(n)\n    \n    # CPU Time\n    T_cpu = K * nnz * (a_c + b_c * sqrt_n + c_c)\n    \n    # GPU Time\n    T_gpu = a_g * nnz + K * nnz * (rho * b_g * sqrt_n + sigma * c_g)\n\n    if T_gpu == 0:\n        return float('inf') if T_cpu > 0 else 1.0\n\n    return T_cpu / T_gpu\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all tasks for the given test cases.\n    \"\"\"\n    test_cases = [\n        (40, 0.10, 6, 12345),\n        (1, 0.00, 4, 2024),\n        (60, 0.08, 8, 31415),\n    ]\n\n    results = []\n    \n    for n, alpha, m, seed in test_cases:\n        # Set up matrix A and vector b\n        np.random.seed(seed)\n        if n > 0:\n             b = np.random.randn(n)\n             if n == 1:\n                 L_n = diags([2], [0], shape=(1, 1), format='csc')\n                 nnz = 1\n             else:\n                 L_n = diags([-1, 2, -1], [-1, 0, 1], shape=(n, n), format='csc')\n                 nnz = 3 * n - 2\n             A = -alpha * L_n\n        else: # Handle n=0 edge case if needed\n            A = diags([],[], shape=(0,0), format='csc')\n            b = np.array([])\n            nnz = 0\n            \n        # 1. Compute approximation using Padé method\n        y_approx = pade_approx_action(A, b, m)\n\n        # 2. Compute reference solution\n        if n > 0:\n            y_ref = expm_multiply(A, b)\n        else:\n            y_ref = np.array([])\n\n        # 3. Calculate relative error\n        norm_y_ref = np.linalg.norm(y_ref)\n        if norm_y_ref > 0:\n            error = np.linalg.norm(y_approx - y_ref) / norm_y_ref\n        elif np.linalg.norm(y_approx) > 0:\n            error = float('inf')\n        else:\n            error = 0.0\n\n        # 4. Calculate predicted speedup\n        K = m\n        speedup = calculate_speedup(n, nnz, K)\n\n        results.extend([error, speedup])\n\n    # Format output as specified\n    formatted_results = [f\"{val:.8f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}