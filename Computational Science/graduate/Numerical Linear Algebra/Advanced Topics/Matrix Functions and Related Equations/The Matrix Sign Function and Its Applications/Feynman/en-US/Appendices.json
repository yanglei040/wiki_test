{
    "hands_on_practices": [
        {
            "introduction": "The most widely used algorithm for computing the matrix sign function is an elegant application of Newton's method. This practice guides you through deriving this iteration and understanding its behavior by examining how it acts on the matrix's eigenvalues . You will also explore a key practical optimization: choosing an initial scaling parameter to accelerate the algorithm's quadratic convergence.",
            "id": "3591965",
            "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ be diagonalizable with spectrum $\\sigma(A)$ disjoint from the imaginary axis $i \\mathbb{R}$. Consider the Newton iteration for the matrix sign function\n$$\nX_{k+1} \\;=\\; \\frac{1}{2}\\big(X_k + X_k^{-1}\\big), \\qquad X_0 \\;=\\; \\alpha^{-1} A,\n$$\nwhere $\\alpha \\in \\mathbb{R}_{0}$ is a scaling parameter to be chosen. The matrix sign function of $A$ is defined by $\\,\\mathrm{sign}(A) = A\\,(A^2)^{-1/2}$, where $(A^2)^{-1/2}$ denotes the principal inverse square root.\n\n1) Starting from the fact that Newton’s method for a matrix equation is obtained by linearizing the residual and solving the linearized correction at each step, derive the above iteration as Newton’s method applied to the equation $X^2 = I$. Justify, using the spectral mapping theorem for diagonalizable matrices and the holomorphic functional calculus, that when $X_0$ is a rational function of $A$, each iterate $X_k$ remains a rational function of $A$ and therefore commutes with $A$.\n\n2) Using the scalar Newton iteration $z \\mapsto \\phi(z) = \\tfrac{1}{2}(z + z^{-1})$ as the iteration induced on eigenvalues, analyze the convergence of $X_k$ to $\\mathrm{sign}(A)$. In particular, argue from first principles that if $A = V \\Lambda V^{-1}$ with $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$, and $z_{0,i} = \\alpha^{-1}\\lambda_i$ satisfies $z_{0,i} \\notin i\\mathbb{R}$ for all $i$, then $z_{k,i} \\to s_i \\in \\{+1,-1\\}$ with quadratic rate, where $s_i = \\mathrm{sign}(\\lambda_i)$ in the sense $s_i = \\lambda_i/\\sqrt{\\lambda_i^2}$ with the principal square root.\n\n3) To accelerate convergence, one often chooses $\\alpha$ so that the initial residual $X_0^2 - I$ is as small as possible. Specialize to the case that $A$ is Hermitian and nonsingular with eigenvalues contained in $[-M,-m] \\cup [m,M]$ for some real $0  m \\leq M$. Using only the characterization of the matrix $2$-norm for normal matrices and properties of extrema on compact intervals, determine the value of $\\alpha$ that minimizes the spectral norm $\\|X_0^2 - I\\|_2$ over $\\alpha  0$. Express your final answer as a closed-form analytic expression in terms of $m$ and $M$ only.\n\nYour final answer must be this minimizing $\\alpha$ written as a single analytic expression. Do not include any inequalities or equations in the final answer.",
            "solution": "The problem as stated is valid. It is scientifically grounded in numerical linear algebra, well-posed with a clear objective, and provides all necessary information for a unique solution. We can proceed with the derivation and analysis.\n\nThe problem is divided into three parts. We will address each in sequence.\n\n**Part 1: Derivation of the Newton Iteration and Commutativity**\n\nWe are asked to derive the iteration $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$ as Newton's method applied to the matrix equation $X^2 = I$. Let the function be $F(X) = X^2 - I$. We seek a root of $F(X) = 0$.\n\nNewton's method for a matrix function $F(X)$ generates a sequence of approximations $X_k$ via the update rule $X_{k+1} = X_k + E_k$, where the correction term $E_k$ is the solution to the linearized equation $F(X_k) + DF_{X_k}(E_k) = 0$. Here, $DF_{X_k}(E_k)$ is the Fréchet derivative of $F$ at $X_k$ applied to the increment $E_k$.\n\nLet's compute the Fréchet derivative of $F(X) = X^2-I$. By definition,\n$$\nF(X+E) = (X+E)^2 - I = X^2 + XE + EX + E^2 - I\n$$\nThe Fréchet derivative is the linear part of $F(X+E) - F(X)$ with respect to $E$.\n$$\nF(X+E) - F(X) = (X^2 + XE + EX + E^2 - I) - (X^2 - I) = XE + EX + E^2\n$$\nThe linear term in $E$ is $DF_X(E) = XE + EX$. Thus, the Newton update step requires solving for $E_k$ in the following equation:\n$$\nF(X_k) + DF_{X_k}(E_k) = 0 \\implies (X_k^2 - I) + (X_k E_k + E_k X_k) = 0\n$$\nThis is a Sylvester equation for the correction $E_k$:\n$$\nX_k E_k + E_k X_k = I - X_k^2\n$$\nSolving this general Sylvester equation can be computationally intensive. However, the iteration given in the problem, $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$, corresponds to a particular choice of $X_{k+1}$. Let's define the correction $E_k$ based on this given iteration:\n$$\nE_k = X_{k+1} - X_k = \\frac{1}{2}(X_k + X_k^{-1}) - X_k = \\frac{1}{2}(X_k^{-1} - X_k)\n$$\nWe now verify if this $E_k$ satisfies the Sylvester equation:\n$$\nX_k E_k + E_k X_k = X_k \\left(\\frac{1}{2}(X_k^{-1} - X_k)\\right) + \\left(\\frac{1}{2}(X_k^{-1} - X_k)\\right) X_k\n$$\n$$\n= \\frac{1}{2}(X_k X_k^{-1} - X_k^2) + \\frac{1}{2}(X_k^{-1}X_k - X_k^2) = \\frac{1}{2}(I - X_k^2) + \\frac{1}{2}(I - X_k^2) = I - X_k^2\n$$\nThis is precisely the right-hand side of the Sylvester equation. Therefore, the iteration $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$ is indeed an instance of Newton's method for solving $X^2=I$. This simplified form of the correction $E_k$ and the update is possible because all iterates $X_k$ commute with each other, as we will now show.\n\nNext, we justify that if $X_0$ is a rational function of $A$, then every iterate $X_k$ is also a rational function of $A$ and commutes with $A$. We proceed by induction.\nThe base case is $k=0$. We are given $X_0 = \\alpha^{-1} A$. This is a polynomial (and hence a rational function) of $A$, let's say $X_0 = r_0(A)$ where $r_0(z) = \\alpha^{-1}z$.\nThe inductive hypothesis is that $X_k = r_k(A)$ for some rational function $r_k(z)$.\nThe next iterate is $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$. Substituting the hypothesis, we get:\n$$\nX_{k+1} = \\frac{1}{2}\\big(r_k(A) + (r_k(A))^{-1}\\big)\n$$\nSince $A$ is diagonalizable, its minimal polynomial has distinct roots, and the holomorphic functional calculus (or its simpler version for diagonalizable matrices) applies. This means that if $f$ is a function holomorphic on a neighborhood of the spectrum of $A$, then $f(A)$ is well-defined. Here, the function is inversion, $f(z)=z^{-1}$. For $(r_k(A))^{-1}$ to be a rational function of $A$, the matrix $r_k(A)$ must be invertible. We will see in Part $2$ that under the given conditions, the eigenvalues of $X_k$ are never zero, so $X_k$ is always invertible. The functional calculus tells us that $(r_k(A))^{-1}$ is given by $q_k(A)$ where $q_k(z) = 1/r_k(z)$. Since $r_k(z)$ is a rational function, so is $q_k(z)$.\nThus, $X_{k+1} = r_{k+1}(A)$ where $r_{k+1}(z) = \\frac{1}{2}(r_k(z) + 1/r_k(z))$ is also a rational function.\nBy induction, every iterate $X_k$ is a rational function of $A$.\nAny matrix which is a rational function of $A$ can be expressed as $p(A)q(A)^{-1}$ for polynomials $p, q$. Since polynomials in $A$ commute with $A$, it follows that any rational function of $A$ commutes with $A$. Hence, $A X_k = X_k A$ for all $k \\geq 0$.\n\n**Part 2: Convergence Analysis**\n\nGiven $A$ is diagonalizable, $A = V\\Lambda V^{-1}$ where $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$. The spectrum $\\sigma(A) = \\{\\lambda_1, \\dots, \\lambda_n\\}$ is disjoint from the imaginary axis $i\\mathbb{R}$, which means $\\mathrm{Re}(\\lambda_i) \\neq 0$ for all $i$.\nThe initial matrix is $X_0 = \\alpha^{-1}A = V(\\alpha^{-1}\\Lambda)V^{-1}$.\nSince each $X_k$ is a rational function of $A$, it is co-diagonalizable with $A$. We can write $X_k = V \\Lambda_k V^{-1}$, where $\\Lambda_k = \\mathrm{diag}(z_{k,1}, \\dots, z_{k,n})$ holds the eigenvalues of $X_k$.\nThe matrix iteration $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$ transforms into an iteration on the diagonal eigenvalue matrices:\n$$\nV\\Lambda_{k+1}V^{-1} = \\frac{1}{2}\\left(V\\Lambda_k V^{-1} + (V\\Lambda_k V^{-1})^{-1}\\right) = V \\left(\\frac{1}{2}(\\Lambda_k + \\Lambda_k^{-1})\\right) V^{-1}\n$$\nThis implies $\\Lambda_{k+1} = \\frac{1}{2}(\\Lambda_k + \\Lambda_k^{-1})$, which decouples into $n$ independent scalar iterations for the eigenvalues:\n$$\nz_{k+1,i} = \\frac{1}{2}(z_{k,i} + z_{k,i}^{-1}) = \\phi(z_{k,i})\n$$\nThe initial values are $z_{0,i} = \\alpha^{-1}\\lambda_i$. Since $\\alpha \\in \\mathbb{R}_{0}$ and $\\lambda_i \\notin i\\mathbb{R}$, we have $z_{0,i} \\notin i\\mathbb{R}$, so $\\mathrm{Re}(z_{0,i}) \\neq 0$.\n\nLet's analyze the scalar map $\\phi(z) = \\frac{1}{2}(z+z^{-1})$. The fixed points are given by $z = \\phi(z)$, which leads to $z = \\frac{1}{2}(z+z^{-1}) \\implies 2z=z+z^{-1} \\implies z=z^{-1} \\implies z^2 = 1$. The fixed points are $z=+1$ and $z=-1$.\n\nConsider the real part of the iterates. If $z_k = x+iy$, then $\\mathrm{Re}(\\phi(z_k)) = \\frac{1}{2}(x + \\frac{x}{x^2+y^2})$. If $x  0$, then $\\mathrm{Re}(\\phi(z_k))  0$. If $x  0$, then $\\mathrm{Re}(\\phi(z_k))  0$. This means that if an iterate $z_{k,i}$ is in the open right half-plane, all subsequent iterates remain there. Similarly for the left half-plane. Since $\\mathrm{Re}(z_{0,i}) \\neq 0$, the sequence $\\{z_{k,i}\\}_{k=0}^\\infty$ is confined to either the right or left half-plane.\n\nIf $\\mathrm{Re}(z_{0,i})  0$, the sequence $\\{z_{k,i}\\}$ converges to the unique fixed point in the right half-plane, which is $+1$.\nIf $\\mathrm{Re}(z_{0,i})  0$, the sequence $\\{z_{k,i}\\}$ converges to the unique fixed point in the left half-plane, which is $-1$.\n\nThe sign of $\\mathrm{Re}(z_{0,i})=\\alpha^{-1}\\mathrm{Re}(\\lambda_i)$ is the same as the sign of $\\mathrm{Re}(\\lambda_i)$. So, $z_{k,i} \\to +1$ if $\\mathrm{Re}(\\lambda_i)  0$ and $z_{k,i} \\to -1$ if $\\mathrm{Re}(\\lambda_i)  0$. This limit is precisely $\\mathrm{sign}(\\lambda_i) = \\lambda_i/\\sqrt{\\lambda_i^2}$ using the principal branch of the square root. Let's call this limit $s_i$.\nAs $k \\to \\infty$, $\\Lambda_k \\to \\mathrm{diag}(s_1, \\dots, s_n) = \\mathrm{sign}(\\Lambda)$. Therefore, $X_k = V\\Lambda_k V^{-1} \\to V\\,\\mathrm{sign}(\\Lambda)V^{-1} = \\mathrm{sign}(A)$.\n\nTo analyze the rate of convergence, let's examine the error. For convergence to $+1$, consider the error $e_{k,i} = z_{k,i} - 1$.\n$$\ne_{k+1,i} = z_{k+1,i} - 1 = \\frac{1}{2}(z_{k,i} + z_{k,i}^{-1}) - 1 = \\frac{z_{k,i}^2 - 2z_{k,i} + 1}{2z_{k,i}} = \\frac{(z_{k,i}-1)^2}{2z_{k,i}} = \\frac{e_{k,i}^2}{2(1+e_{k,i})}\n$$\nAs $k \\to \\infty$, $z_{k,i} \\to 1$ and $e_{k,i} \\to 0$. The error propagation is $|e_{k+1,i}| \\approx \\frac{1}{2}|e_{k,i}|^2$, which demonstrates quadratic convergence. A similar analysis for convergence to $-1$ with error $e_{k,i} = z_{k,i} - (-1)$ gives $e_{k+1,i}=\\frac{e_{k,i}^2}{2(e_{k,i}-1)}$, so $|e_{k+1,i}| \\approx \\frac{1}{2}|e_{k,i}|^2$, also quadratic.\n\n**Part 3: Optimal Scaling Parameter**\n\nWe want to find $\\alpha \\in \\mathbb{R}_{0}$ that minimizes $\\|X_0^2 - I\\|_2$. We have $X_0 = \\alpha^{-1}A$. The quantity to minimize is $\\|\\alpha^{-2}A^2 - I\\|_2$.\nThe matrix $A$ is Hermitian, so its eigenvalues are real, and $A$ is a normal matrix. The matrix $B = \\alpha^{-2}A^2 - I$ is also normal (in fact, Hermitian, as $A^2$ is Hermitian and $\\alpha$ is real). For a normal matrix $B$, its $2$-norm is equal to its spectral radius: $\\|B\\|_2 = \\rho(B) = \\max_{\\mu \\in \\sigma(B)} |\\mu|$.\n\nThe eigenvalues of $A$ are $\\lambda_i$, and they are contained in the set $[-M, -m] \\cup [m, M]$ for $0  m \\leq M$.\nThe eigenvalues of $A^2$ are $\\lambda_i^2$. Squaring the eigenvalues of $A$, we find that $\\sigma(A^2) \\subseteq [m^2, M^2]$.\nThe eigenvalues of $B = \\alpha^{-2}A^2 - I$ are given by $\\mu_i = \\alpha^{-2}\\lambda_i^2 - 1$.\nThe problem is now to minimize the maximum absolute value of these eigenvalues:\n$$\n\\min_{\\alpha  0} \\max_{\\lambda_i \\in \\sigma(A)} |\\alpha^{-2}\\lambda_i^2 - 1|\n$$\nLet $x = \\lambda^2$. The variable $x$ lies in the interval $[m^2, M^2]$. We need to solve the minimax problem:\n$$\n\\min_{\\alpha  0} \\max_{x \\in [m^2, M^2]} |\\alpha^{-2}x - 1|\n$$\nLet $\\beta = \\alpha^{-2}$. Since $\\alpha  0$, we have $\\beta  0$. The problem becomes finding $\\beta0$ that minimizes\n$$\ng(\\beta) = \\max_{x \\in [m^2, M^2]} |\\beta x - 1|\n$$\nThe function $h(x) = \\beta x - 1$ is linear in $x$. The maximum of its absolute value on the interval $[m^2, M^2]$ must be attained at one of the endpoints. So,\n$$\ng(\\beta) = \\max\\left( |\\beta m^2 - 1|, |\\beta M^2 - 1| \\right)\n$$\nTo minimize $\\max(|a|,|b|)$, a common strategy is to choose parameters such that $|a|=|b|$. For the minimum to occur, we generally want the function to have extremal values of equal magnitude and opposite sign at the boundaries. This suggests setting $\\beta m^2 - 1 = -(\\beta M^2 - 1)$.\n$$\n\\beta m^2 - 1 = 1 - \\beta M^2\n$$\n$$\n\\beta m^2 + \\beta M^2 = 2\n$$\n$$\n\\beta (m^2 + M^2) = 2\n$$\n$$\n\\beta = \\frac{2}{m^2 + M^2}\n$$\nThis choice of $\\beta$ ensures that the values at the endpoints are $\\frac{2m^2}{m^2+M^2}-1 = \\frac{m^2-M^2}{m^2+M^2}$ and $\\frac{2M^2}{m^2+M^2}-1 = \\frac{M^2-m^2}{m^2+M^2}$. The magnitudes are equal.\nSince $\\beta = \\alpha^{-2}$, we have:\n$$\n\\alpha^{-2} = \\frac{2}{m^2 + M^2} \\implies \\alpha^2 = \\frac{m^2 + M^2}{2}\n$$\nGiven that $\\alpha  0$, we take the positive square root:\n$$\n\\alpha = \\sqrt{\\frac{m^2 + M^2}{2}}\n$$\nThis value of $\\alpha$ minimizes the spectral norm of the initial residual $X_0^2-I$.",
            "answer": "$$\\boxed{\\sqrt{\\frac{m^{2}+M^{2}}{2}}}$$"
        },
        {
            "introduction": "The matrix sign function is a powerful tool, but its definition relies on a clear separation of eigenvalues from the imaginary axis. This exercise explores the consequences when this condition is violated, particularly for non-diagonalizable (defective) matrices . By analyzing a Jordan block, you will see how the function becomes discontinuous and quantify the extreme sensitivity that arises as eigenvalues approach the imaginary axis.",
            "id": "3591967",
            "problem": "Let $\\lambda \\in \\mathbb{C}$ and consider the $2 \\times 2$ defective Jordan block $J(\\lambda) = \\begin{pmatrix} \\lambda  1 \\\\ 0  \\lambda \\end{pmatrix}$. The matrix sign function $\\mathrm{sign}(A)$ is the primary matrix function defined on matrices $A \\in \\mathbb{C}^{n \\times n}$ whose spectra avoid the imaginary axis, by analytic functional calculus using the scalar function $z \\mapsto \\mathrm{sign}(z)$ that maps complex numbers with positive real part to $+1$ and those with negative real part to $-1$.\n\na) Starting from the definition of $\\mathrm{sign}(A)$ via primary matrix functions and the characterization on Jordan blocks using derivatives of the scalar function, show that $\\mathrm{sign}(J(\\lambda))$ is discontinuous when $\\Re(\\lambda)$ crosses $0$. Provide a principled argument based on the structure of $J(\\lambda)$ and the analyticity domain of $z \\mapsto \\mathrm{sign}(z)$.\n\nb) To quantify the sensitivity, construct the block-diagonal matrix $A(\\varepsilon) = \\mathrm{diag}\\big(J(\\varepsilon), J(-\\varepsilon)\\big) \\in \\mathbb{C}^{4 \\times 4}$ for $\\varepsilon \\in \\mathbb{R} \\setminus \\{0\\}$. Let $E \\in \\mathbb{C}^{4 \\times 4}$ be a perturbation whose only nonzero $2 \\times 2$ block is the $(1,2)$ block $E_{12} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}$, with all other blocks equal to the zero matrix. Using first principles of analytic functional calculus (Cauchy integral representation), derive an explicit expression for the Fréchet derivative $L_{\\mathrm{sign}}(A(\\varepsilon); E)$ and compute the Frobenius norm $\\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F}$.\n\nFinally, compute the limit\n$$\\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{3} \\, \\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F},$$\nand express your answer as an exact value (no rounding required). Your final answer must be a single number.",
            "solution": "The problem is evaluated in two parts. Part (a) requires a demonstration of the discontinuity of the matrix sign function applied to a defective Jordan block. Part (b) requires the calculation of a Fréchet derivative and a subsequent limit to quantify the sensitivity of the function near this discontinuity.\n\na) Discontinuity of $\\mathrm{sign}(J(\\lambda))$\n\nThe matrix sign function, $\\mathrm{sign}(A)$, is a primary matrix function associated with the scalar function $s(z) = \\mathrm{sign}(z)$. This scalar function is defined as $s(z) = 1$ if $\\Re(z)  0$ and $s(z) = -1$ if $\\Re(z)  0$. The matrix function $\\mathrm{sign}(A)$ is well-defined for any matrix $A \\in \\mathbb{C}^{n \\times n}$ that has no eigenvalues on the imaginary axis. For such a matrix, the scalar function $s(z)$ is analytic on the spectrum of $A$.\n\nThe evaluation of a primary matrix function $f(J)$ on a Jordan block $J$ is given by a specific formula involving the derivatives of the scalar function $f$. For the given $2 \\times 2$ defective Jordan block $J(\\lambda) = \\begin{pmatrix} \\lambda  1 \\\\ 0  \\lambda \\end{pmatrix}$, the matrix sign function is given by:\n$$ \\mathrm{sign}(J(\\lambda)) = \\begin{pmatrix} s(\\lambda)  s'(\\lambda) \\\\ 0  s(\\lambda) \\end{pmatrix} $$\nwhere $s'(\\lambda)$ is the derivative of $s(z)$ evaluated at $z=\\lambda$. This formula is valid provided $s(z)$ is analytic in a neighborhood of $\\lambda$.\n\nWe analyze the value of $\\mathrm{sign}(J(\\lambda))$ based on the real part of $\\lambda$.\n\nCase 1: $\\Re(\\lambda)  0$.\nIn this case, $\\lambda$ is in the open right half-plane. The scalar function $s(z)$ is constant and equal to $1$ in a neighborhood of $\\lambda$. Therefore, its derivative $s'(\\lambda)$ is $0$.\nThe matrix function is:\n$$ \\mathrm{sign}(J(\\lambda)) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\nwhere $I_2$ is the $2 \\times 2$ identity matrix.\n\nCase 2: $\\Re(\\lambda)  0$.\nIn this case, $\\lambda$ is in the open left half-plane. The scalar function $s(z)$ is constant and equal to $-1$ in a neighborhood of $\\lambda$. Consequently, its derivative $s'(\\lambda)$ is also $0$.\nThe matrix function is:\n$$ \\mathrm{sign}(J(\\lambda)) = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix} = -I_2 $$\n\nThe discontinuity occurs when $\\Re(\\lambda)$ crosses $0$. Let $\\lambda = x+iy$. As $x \\to 0^{+}$, the limit of $\\mathrm{sign}(J(x+iy))$ is $I_2$. As $x \\to 0^{-}$, the limit is $-I_2$. Since the left-hand and right-hand limits are not equal, the function $\\mathrm{sign}(J(\\lambda))$ is discontinuous at any point on the imaginary axis (i.e., where $\\Re(\\lambda)=0$). The principled reason for this discontinuity lies in the definition of the scalar function $s(z)$ and the requirements of analytic functional calculus. The function $s(z)$ itself is not analytic on the imaginary axis, which is the boundary of its domains of definition. When an eigenvalue $\\lambda$ of the matrix $A$ lies on this boundary of analyticity, the matrix function $\\mathrm{sign}(A)$ is not defined. The limits approaching this boundary from the two sides reveal the discontinuous nature of the function.\n\nb) Fréchet Derivative and Limit Calculation\n\nThe Fréchet derivative of a matrix function $f(A)$ in the direction of a matrix $E$, denoted $L_f(A; E)$, can be computed using the Cauchy integral representation:\n$$ L_f(A; E) = \\frac{1}{2\\pi i} \\oint_{\\Gamma} f(z) (zI-A)^{-1} E (zI-A)^{-1} dz $$\nHere, $f(z) = \\mathrm{sign}(z)$ and the contour $\\Gamma$ must enclose the spectrum of $A$.\n\nThe matrix is $A(\\varepsilon) = \\mathrm{diag}\\big(J(\\varepsilon), J(-\\varepsilon)\\big)$, where $J(\\varepsilon) = \\begin{pmatrix} \\varepsilon  1 \\\\ 0  \\varepsilon \\end{pmatrix}$ and $J(-\\varepsilon) = \\begin{pmatrix} -\\varepsilon  1 \\\\ 0  -\\varepsilon \\end{pmatrix}$. For $\\varepsilon \\in \\mathbb{R} \\setminus \\{0\\}$, the spectrum of $A(\\varepsilon)$ is $\\{\\varepsilon, -\\varepsilon\\}$, with each eigenvalue having algebraic multiplicity $2$. We can choose the contour $\\Gamma$ to be the union of two disjoint simple closed contours: $\\Gamma_+$ enclosing $\\varepsilon$ and lying entirely in the right half-plane, and $\\Gamma_-$ enclosing $-\\varepsilon$ and lying entirely in the left half-plane. On $\\Gamma_+$, $\\mathrm{sign}(z)=1$, and on $\\Gamma_-$, $\\mathrm{sign}(z)=-1$.\n\nThe resolvent $(zI - A(\\varepsilon))^{-1}$ is block-diagonal:\n$$ (zI - A(\\varepsilon))^{-1} = \\mathrm{diag}\\left((zI-J(\\varepsilon))^{-1}, (zI-J(-\\varepsilon))^{-1}\\right) $$\nLet $R_+(z) = (zI-J(\\varepsilon))^{-1} = \\begin{pmatrix} \\frac{1}{z-\\varepsilon}  \\frac{1}{(z-\\varepsilon)^2} \\\\ 0  \\frac{1}{z-\\varepsilon} \\end{pmatrix}$ and $R_-(z) = (zI-J(-\\varepsilon))^{-1} = \\begin{pmatrix} \\frac{1}{z+\\varepsilon}  \\frac{1}{(z+\\varepsilon)^2} \\\\ 0  \\frac{1}{z+\\varepsilon} \\end{pmatrix}$.\n\nThe perturbation is $E = \\begin{pmatrix} 0  E_{12} \\\\ 0  0 \\end{pmatrix}$ with $E_{12} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}$. The product inside the integral becomes:\n$$ (zI-A)^{-1} E (zI-A)^{-1} = \\begin{pmatrix} R_+(z)  0 \\\\ 0  R_-(z) \\end{pmatrix} \\begin{pmatrix} 0  E_{12} \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} R_+(z)  0 \\\\ 0  R_-(z) \\end{pmatrix} = \\begin{pmatrix} 0  R_+(z) E_{12} R_-(z) \\\\ 0  0 \\end{pmatrix} $$\nLet $L_{12}$ be the $(1,2)$ block of $L_{\\mathrm{sign}}(A(\\varepsilon); E)$, which is the only non-zero block.\n$$ L_{12} = \\frac{1}{2\\pi i} \\oint_{\\Gamma} \\mathrm{sign}(z) R_+(z) E_{12} R_-(z) dz $$\nSplitting the integral over $\\Gamma_+$ and $\\Gamma_-$:\n$$ L_{12} = \\frac{1}{2\\pi i} \\oint_{\\Gamma_+} (1) R_+(z) E_{12} R_-(z) dz - \\frac{1}{2\\pi i} \\oint_{\\Gamma_-} (1) R_+(z) E_{12} R_-(z) dz $$\nLet $g(z) = R_+(z) E_{12} R_-(z)$. By the residue theorem, this is:\n$$ L_{12} = \\mathrm{Res}(g(z), \\varepsilon) - \\mathrm{Res}(g(z), -\\varepsilon) $$\nWe first compute the matrix $g(z)$:\n$$ g(z) = \\begin{pmatrix} \\frac{1}{z-\\varepsilon}  \\frac{1}{(z-\\varepsilon)^2} \\\\ 0  \\frac{1}{z-\\varepsilon} \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{z+\\varepsilon}  \\frac{1}{(z+\\varepsilon)^2} \\\\ 0  \\frac{1}{z+\\varepsilon} \\end{pmatrix} $$\n$$ g(z) = \\begin{pmatrix} \\frac{1}{(z-\\varepsilon)^2}  0 \\\\ \\frac{1}{z-\\varepsilon}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{z+\\varepsilon}  \\frac{1}{(z+\\varepsilon)^2} \\\\ 0  \\frac{1}{z+\\varepsilon} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)}  \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)^2} \\\\ \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)}  \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)^2} \\end{pmatrix} $$\nNow we compute the residues for each entry. Let $L_{12} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$.\n- For $a$: $g_{11}(z) = \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)}$.\n  $\\mathrm{Res}(g_{11}, \\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{z+\\varepsilon}\\right)\\Big|_{z=\\varepsilon} = \\frac{-1}{(z+\\varepsilon)^2}\\Big|_{z=\\varepsilon} = \\frac{-1}{4\\varepsilon^2}$.\n  $\\mathrm{Res}(g_{11}, -\\varepsilon) = \\frac{1}{(z-\\varepsilon)^2}\\Big|_{z=-\\varepsilon} = \\frac{1}{4\\varepsilon^2}$.\n  $a = \\frac{-1}{4\\varepsilon^2} - \\frac{1}{4\\varepsilon^2} = -\\frac{1}{2\\varepsilon^2}$.\n- For $b$: $g_{12}(z) = \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)^2}$.\n  $\\mathrm{Res}(g_{12}, \\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{(z+\\varepsilon)^2}\\right)\\Big|_{z=\\varepsilon} = \\frac{-2}{(z+\\varepsilon)^3}\\Big|_{z=\\varepsilon} = \\frac{-2}{8\\varepsilon^3} = -\\frac{1}{4\\varepsilon^3}$.\n  $\\mathrm{Res}(g_{12}, -\\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{(z-\\varepsilon)^2}\\right)\\Big|_{z=-\\varepsilon} = \\frac{-2}{(z-\\varepsilon)^3}\\Big|_{z=-\\varepsilon} = \\frac{-2}{-8\\varepsilon^3} = \\frac{1}{4\\varepsilon^3}$.\n  $b = -\\frac{1}{4\\varepsilon^3} - \\frac{1}{4\\varepsilon^3} = -\\frac{1}{2\\varepsilon^3}$.\n- For $c$: $g_{21}(z) = \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)}$.\n  $\\mathrm{Res}(g_{21}, \\varepsilon) = \\frac{1}{z+\\varepsilon}\\Big|_{z=\\varepsilon} = \\frac{1}{2\\varepsilon}$.\n  $\\mathrm{Res}(g_{21}, -\\varepsilon) = \\frac{1}{z-\\varepsilon}\\Big|_{z=-\\varepsilon} = -\\frac{1}{2\\varepsilon}$.\n  $c = \\frac{1}{2\\varepsilon} - (-\\frac{1}{2\\varepsilon}) = \\frac{1}{\\varepsilon}$.\n- For $d$: $g_{22}(z) = \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)^2}$.\n  $\\mathrm{Res}(g_{22}, \\varepsilon) = \\frac{1}{(z+\\varepsilon)^2}\\Big|_{z=\\varepsilon} = \\frac{1}{4\\varepsilon^2}$.\n  $\\mathrm{Res}(g_{22}, -\\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{z-\\varepsilon}\\right)\\Big|_{z=-\\varepsilon} = \\frac{-1}{(z-\\varepsilon)^2}\\Big|_{z=-\\varepsilon} = -\\frac{1}{4\\varepsilon^2}$.\n  $d = \\frac{1}{4\\varepsilon^2} - (-\\frac{1}{4\\varepsilon^2}) = \\frac{1}{2\\varepsilon^2}$.\n\nSo, the Fréchet derivative is $L_{\\mathrm{sign}}(A(\\varepsilon); E) = \\begin{pmatrix} 0  L_{12} \\\\ 0  0 \\end{pmatrix}$, where\n$$ L_{12} = \\begin{pmatrix} -\\frac{1}{2\\varepsilon^2}  -\\frac{1}{2\\varepsilon^3} \\\\ \\frac{1}{\\varepsilon}  \\frac{1}{2\\varepsilon^2} \\end{pmatrix} $$\nThe full matrix is:\n$$ L_{\\mathrm{sign}}(A(\\varepsilon); E) = \\begin{pmatrix} 0  0  -\\frac{1}{2\\varepsilon^2}  -\\frac{1}{2\\varepsilon^3} \\\\ 0  0  \\frac{1}{\\varepsilon}  \\frac{1}{2\\varepsilon^2} \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix} $$\nNext, we compute its Frobenius norm:\n$$ \\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F}^2 = \\sum_{i,j} |(L_{\\mathrm{sign}})_{ij}|^2 = \\left(-\\frac{1}{2\\varepsilon^2}\\right)^2 + \\left(-\\frac{1}{2\\varepsilon^3}\\right)^2 + \\left(\\frac{1}{\\varepsilon}\\right)^2 + \\left(\\frac{1}{2\\varepsilon^2}\\right)^2 $$\n$$ = \\frac{1}{4\\varepsilon^4} + \\frac{1}{4\\varepsilon^6} + \\frac{1}{\\varepsilon^2} + \\frac{1}{4\\varepsilon^4} = \\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2} $$\nTherefore, $\\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F} = \\sqrt{\\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2}}$.\n\nFinally, we compute the required limit for $\\varepsilon \\to 0^{+}$:\n$$ \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{3} \\, \\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F} = \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{3} \\sqrt{\\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2}} $$\nSince $\\varepsilon  0$, we can bring $\\varepsilon^3$ inside the square root as $\\varepsilon^6$:\n$$ = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\varepsilon^{6} \\left(\\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2}\\right)} $$\n$$ = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\frac{\\varepsilon^6}{4\\varepsilon^6} + \\frac{\\varepsilon^6}{2\\varepsilon^4} + \\frac{\\varepsilon^6}{\\varepsilon^2}} $$\n$$ = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\frac{1}{4} + \\frac{\\varepsilon^2}{2} + \\varepsilon^4} $$\nAs $\\varepsilon \\to 0^{+}$, the terms $\\frac{\\varepsilon^2}{2}$ and $\\varepsilon^4$ both approach $0$.\n$$ = \\sqrt{\\frac{1}{4} + 0 + 0} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2} $$\nThe value of the limit is $\\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Beyond its theoretical elegance, the matrix sign function is a cornerstone of algorithms in control theory and dynamical systems. This hands-on problem demonstrates its application to the stability analysis of Hamiltonian systems, a structure that appears in many areas of physics and engineering . You will use projectors built from the sign function to numerically isolate the stable, unstable, and center subspaces and compute the Krein signatures, which are crucial for determining the stability of imaginary-axis eigenvalues.",
            "id": "3591961",
            "problem": "Given a Hamiltonian matrix construction based on a symmetric matrix, you are asked to design and implement a numerical diagnostic that combines the matrix sign function with a contour-based Evans function count to assess spectral stability and to compute Krein signatures for purely imaginary eigenvalues. Let $n \\in \\mathbb{N}$, and define the $2 \\times 2$ canonical symplectic matrix $J_{1}$ by\n$$\nJ_{1} = \\begin{bmatrix} 0  1 \\\\ -1  0 \\end{bmatrix},\n$$\nand, for $N = 2$, define $J = \\mathrm{diag}(J_{1}, J_{1}) \\in \\mathbb{R}^{4 \\times 4}$. For any symmetric matrix $H \\in \\mathbb{R}^{4 \\times 4}$, define the Hamiltonian matrix\n$$\nA = J H.\n$$\nYou may assume that $H$ is real symmetric and that $A$ is diagonalizable over $\\mathbb{C}$ for the provided test matrices. The matrix sign function $\\mathrm{sign}(M)$ of a square matrix $M$ is defined, via the holomorphic functional calculus, to be the unique primary matrix function satisfying $\\mathrm{sign}(M)^{2} = I$ and mapping each eigenvalue $\\lambda$ of $M$ with $\\mathrm{Re}(\\lambda) \\neq 0$ to $+1$ if $\\mathrm{Re}(\\lambda)  0$ and to $-1$ if $\\mathrm{Re}(\\lambda)  0$; in the diagonalizable case, $\\mathrm{sign}(M)$ acts on the Jordan form by applying the scalar sign to the eigenvalues with nonzero real part. When $M$ has eigenvalues on the imaginary axis, the unshifted $\\mathrm{sign}(M)$ is not defined; a consistent regularization is to introduce a small spectral shift $\\varepsilon  0$ and use $\\mathrm{sign}(M \\pm \\varepsilon I)$.\n\nYou will use two connected principles as a fundamental base:\n\n- Spectral projectors from the matrix sign function: If $M$ has no eigenvalues on the imaginary axis, then the projector onto the invariant subspace for eigenvalues with positive real parts is given by\n$$\nP^{+}(M) = \\tfrac{1}{2}\\bigl(I + \\mathrm{sign}(M)\\bigr).\n$$\nIf $M$ has eigenvalues near the imaginary axis, then for sufficiently small $\\varepsilon  0$, the difference\n$$\nP_{\\mathrm{c}}(M;\\varepsilon) := P^{+}(M + \\varepsilon I) - P^{+}(M - \\varepsilon I)\n$$\nconverges, as $\\varepsilon \\to 0^{+}$, to the Riesz projector onto the spectral subspace corresponding to eigenvalues with real part equal to zero; for small finite $\\varepsilon$, this is a robust numerical approximation to the center projector onto the slab $\\{ \\lambda : |\\mathrm{Re}(\\lambda)|  \\varepsilon \\}$.\n\n- Argument principle for the Evans function: The Evans function $D(\\lambda)$ may be taken, for finite-dimensional problems, as the analytic function $D(\\lambda) = \\det(\\lambda I - A)$. For any positively oriented, simple, closed contour $\\Gamma$ in the complex plane such that $D$ has no zeros on $\\Gamma$, the number of zeros of $D$ inside $\\Gamma$ (counted with multiplicity) is given by\n$$\nN_{\\Gamma} = \\frac{1}{2\\pi} \\Delta_{\\Gamma} \\arg D(\\lambda),\n$$\nwhere $\\Delta_{\\Gamma} \\arg D(\\lambda)$ denotes the net change in the complex argument of $D(\\lambda)$ as $\\lambda$ traverses $\\Gamma$ once. Choosing a rectangular contour that encloses the portion of the right half-plane up to a finite bound allows counting eigenvalues with positive real part.\n\nKrein signatures for purely imaginary eigenvalues of $A = JH$ are defined via the restriction of the energy quadratic form induced by $H$. If $E_{\\mathrm{c}}$ is the spectral subspace of $A$ associated with eigenvalues on the imaginary axis and $V$ is any full-column-rank matrix whose columns form a basis of $E_{\\mathrm{c}}$, then the Hermitian matrix\n$$\nG = V^{*} H V\n$$\nencodes the Krein signatures on $E_{\\mathrm{c}}$: the number of positive eigenvalues of $G$ equals the number of positive Krein signatures, and the number of negative eigenvalues of $G$ equals the number of negative Krein signatures, provided the eigenvalues on the imaginary axis are semisimple and the basis $V$ spans exactly $E_{\\mathrm{c}}$.\n\nTasks:\n\n1. For each provided test matrix $H$, construct $A = JH$ and compute the following diagnostics:\n   - A stable/unstable count from the matrix sign function: choose a regularization parameter $\\varepsilon = 10^{-1}$ and compute\n     $$\n     N_{\\mathrm{sign}} := \\mathrm{trace}\\bigl(P^{+}(A - \\varepsilon I)\\bigr),\n     $$\n     interpreted as the number of eigenvalues with $\\mathrm{Re}(\\lambda)  \\varepsilon$ (counting algebraic multiplicity).\n   - A center spectral projector approximation\n     $$\n     P_{\\mathrm{c}}(A;\\varepsilon) = P^{+}(A + \\varepsilon I) - P^{+}(A - \\varepsilon I),\n     $$\n     and, from any numerically stable orthonormal basis $V$ of $\\mathrm{ran}\\,P_{\\mathrm{c}}(A;\\varepsilon)$, compute $G = V^{*} H V$. Determine the Krein signature counts\n     $$\n     N_{\\mathrm{K}}^{+} = \\#\\{\\mu \\in \\sigma(G) : \\mu  0\\}, \\quad N_{\\mathrm{K}}^{-} = \\#\\{\\mu \\in \\sigma(G) : \\mu  0\\},\n     $$\n     using a fixed numerical tolerance to avoid classifying eigenvalues that are too close to zero.\n   - An Evans-function-based unstable count $N_{\\mathrm{Evans}}$ computed by the argument principle, using the contour $\\Gamma$ that is the rectangle with vertices $\\delta \\pm \\mathrm{i} T$ and $R \\pm \\mathrm{i} T$, traversed counterclockwise, where $\\delta = 2 \\times 10^{-1}$, $R = 1.6$, and $T = 2.5$. Compute\n     $$\n     D(\\lambda) = \\det(\\lambda I - A),\n     $$\n     sample $D$ along $\\Gamma$, numerically unwrap the complex argument, and return the nearest integer to $\\Delta_{\\Gamma} \\arg D(\\lambda) / (2\\pi)$ as $N_{\\mathrm{Evans}}$.\n\n2. The test suite consists of the following three symmetric matrices $H \\in \\mathbb{R}^{4 \\times 4}$, defined via $2 \\times 2$ blocks, with $J = \\mathrm{diag}(J_{1}, J_{1})$ and the block diagonal structure\n$$\nH = \\mathrm{diag}(H_{(1)}, H_{(2)}).\n$$\nUse these explicit blocks:\n   - Case A (no imaginary-axis spectrum expected):\n     $$\n     H_{(1)} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}, \\quad\n     H_{(2)} = \\begin{bmatrix} 2  0 \\\\ 0  -\\tfrac{1}{2} \\end{bmatrix}.\n     $$\n   - Case B (purely imaginary spectrum expected):\n     $$\n     H_{(1)} = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}, \\quad\n     H_{(2)} = \\begin{bmatrix} 1.5  0.1 \\\\ 0.1  1.0 \\end{bmatrix}.\n     $$\n   - Case C (mixed: one block purely imaginary, one block unstable):\n     $$\n     H_{(1)} = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}, \\quad\n     H_{(2)} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}.\n     $$\n\n3. Your program must implement the computations above for each case and return, for each case, a list of four integers $\\bigl[N_{\\mathrm{Evans}},\\, N_{\\mathrm{sign}},\\, N_{\\mathrm{K}}^{+},\\, N_{\\mathrm{K}}^{-}\\bigr]$.\n\nFinal output format requirement:\n\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of lists, enclosed in square brackets, for example, $\\bigl[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]\\bigr]$, with no spaces after commas.\n\nThere are no physical units or angles in this problem. All numerical tolerances must be chosen explicitly by your implementation; you must use a fixed small value $\\varepsilon = 10^{-1}$ for the projector regularization and the specified $(\\delta, R, T)$ for the Evans function contour. The answers for all test cases must be integers as specified.",
            "solution": "The problem requires the implementation of three numerical diagnostics to analyze the spectral stability of a $4 \\times 4$ real Hamiltonian matrix $A = JH$. These diagnostics are: (1) a count of unstable eigenvalues, $N_{\\mathrm{sign}}$, using the matrix sign function; (2) a count of Krein signatures, $N_{\\mathrm{K}}^{+}$ and $N_{\\mathrm{K}}^{-}$, for purely imaginary eigenvalues, also derived from the matrix sign function; and (3) an independent count of unstable eigenvalues, $N_{\\mathrm{Evans}}$, using the argument principle applied to the characteristic polynomial, which serves as the Evans function here.\n\nThe foundation of the problem lies in the spectral properties of Hamiltonian matrices and the utility of specific matrix functions for spectral decomposition. A real matrix $A$ is Hamiltonian with respect to the symplectic matrix $J$ if $A^T J + J A = 0$. Given $J = \\mathrm{diag}(J_1, J_1)$ where $J_1 = \\begin{bmatrix} 0  1 \\\\ -1  0 \\end{bmatrix}$, we have $J^T = -J$ and $J^2 = -I$. For a real symmetric matrix $H = H^T$, the matrix $A = JH$ is verified to be Hamiltonian: $A^TJ + JA = (JH)^T J + J(JH) = H^T J^T J + J^2 H = H(-J)J + (-I)H = -H(-I) - H = H-H=0$. The definition is satisfied. A critical property of Hamiltonian matrices is that their spectrum $\\sigma(A)$ is symmetric with respect to both the real and imaginary axes. That is, if $\\lambda \\in \\sigma(A)$, then $-\\lambda$, $\\bar{\\lambda}$, and $-\\bar{\\lambda}$ are also in $\\sigma(A)$. This implies eigenvalues appear as real pairs $(\\alpha, -\\alpha)$, imaginary pairs $(\\mathrm{i}\\omega, -\\mathrm{i}\\omega)$, or quadruplets $(\\alpha \\pm \\mathrm{i}\\beta, -\\alpha \\pm \\mathrm{i}\\beta)$.\n\nThe solution proceeds by implementing the three required computations for each of the three test matrices.\n\n**1. Unstable Eigenvalue Count via Matrix Sign Function ($N_{\\mathrm{sign}}$)**\n\nThe matrix sign function, $\\mathrm{sign}(M)$, for a matrix $M$ with no eigenvalues on the imaginary axis, provides a spectral decomposition. The matrices $P^{\\pm}(M) = \\frac{1}{2}(I \\pm \\mathrm{sign}(M))$ are projectors onto the invariant subspaces corresponding to eigenvalues in the right half-plane ($\\mathrm{Re}(\\lambda)0$) and left half-plane ($\\mathrm{Re}(\\lambda)0$), respectively. The trace of a projector gives the dimension of its range, so $\\mathrm{trace}(P^+(M))$ counts the number of eigenvalues of $M$ with positive real part.\n\nTo count eigenvalues with $\\mathrm{Re}(\\lambda)  \\varepsilon$ for a given $\\varepsilon  0$, we apply this to a shifted matrix, $M' = A - \\varepsilon I$. The eigenvalues of $M'$ are $\\lambda' = \\lambda - \\varepsilon$. An eigenvalue $\\lambda$ of $A$ satisfies $\\mathrm{Re}(\\lambda)  \\varepsilon$ if and only if $\\mathrm{Re}(\\lambda')  0$. Therefore, the number of such eigenvalues is:\n$$\nN_{\\mathrm{sign}} = \\mathrm{trace}\\bigl(P^{+}(A - \\varepsilon I)\\bigr) = \\frac{1}{2} \\mathrm{trace}\\bigl(I + \\mathrm{sign}(A - \\varepsilon I)\\bigr).\n$$\nFor the computation, we use the specified value of $\\varepsilon = 10^{-1}$. The matrix sign function is computed using `scipy.linalg.signm`. The result, being the dimension of a subspace, must be an integer; we round the numerical result to the nearest integer.\n\n**2. Krein Signature Counts ($N_{\\mathrm{K}}^{+}$, $N_{\\mathrm{K}}^{-}$)**\n\nEigenvalues on the imaginary axis are spectrally stable, but may become unstable under perturbations. The Krein signature provides a finer classification. The procedure involves:\n(a) Isolating the invariant subspace corresponding to eigenvalues on or near the imaginary axis. This is the center subspace, $E_c$. The problem provides a numerical approximation via the center projector $P_c(A;\\varepsilon)$, which projects onto the subspace for eigenvalues with $|\\mathrm{Re}(\\lambda)|  \\varepsilon$.\n$$\nP_{\\mathrm{c}}(A;\\varepsilon) = P^{+}(A + \\varepsilon I) - P^{+}(A - \\varepsilon I) = \\frac{1}{2}\\bigl(\\mathrm{sign}(A + \\varepsilon I) - \\mathrm{sign}(A - \\varepsilon I)\\bigr).\n$$\nWe use $\\varepsilon = 10^{-1}$.\n(b) Finding a numerically stable orthonormal basis for the range of $P_{\\mathrm{c}}(A;\\varepsilon)$. A robust method is to compute the Singular Value Decomposition (SVD) of $P_{\\mathrm{c}} = U\\Sigma V^*$. The columns of $U$ corresponding to singular values significantly different from zero form the desired orthonormal basis. Let this basis be collected in a matrix $V$. The number of such columns gives the dimension of the center subspace.\n(c) Forming the Krein matrix $G = V^{*} H V$. This is a Hermitian matrix representing the energy quadratic form defined by $H$, restricted to the center subspace.\n(d) Counting the positive and negative eigenvalues of $G$. The number of positive eigenvalues of $G$ is $N_{\\mathrm{K}}^{+}$, and the number of negative eigenvalues is $N_{\\mathrm{K}}^{-}$. This is done by computing the eigenvalues of $G$ and counting them based on their sign, using a small tolerance to classify values near zero. If the dimension of the center subspace is zero, both counts are zero.\n\n**3. Unstable Eigenvalue Count via Evans Function ($N_{\\mathrm{Evans}}$)**\n\nAn independent count of unstable eigenvalues is obtained via the argument principle from complex analysis. For a finite-dimensional problem, the Evans function can be taken as the characteristic polynomial $D(\\lambda) = \\det(\\lambda I - A)$. The zeros of $D(\\lambda)$ are the eigenvalues of $A$. The number of zeros $N_{\\Gamma}$ enclosed by a simple closed contour $\\Gamma$ (on which $D(\\lambda) \\neq 0$) is given by the winding number of the path $D(\\Gamma)$ around the origin:\n$$\nN_{\\Gamma} = \\frac{1}{2\\pi} \\Delta_{\\Gamma} \\arg D(\\lambda),\n$$\nwhere $\\Delta_{\\Gamma} \\arg D(\\lambda)$ is the total change in the complex argument of $D(\\lambda)$ as $\\lambda$ traverses $\\Gamma$ once in the counter-clockwise direction.\n\nWe are given a rectangular contour $\\Gamma$ with vertices $\\delta \\pm \\mathrm{i}T$ and $R \\pm \\mathrm{i}T$, with $\\delta = 2 \\times 10^{-1}$, $R=1.6$, and $T=2.5$. This contour encloses a portion of the right half-plane. The computational steps are:\n(a) Discretize the four segments of the rectangular contour.\n(b) Evaluate $D(\\lambda_k) = \\det(\\lambda_k I - A)$ for each point $\\lambda_k$ on the discretized contour.\n(c) Compute the sequence of arguments $\\theta_k = \\arg(D(\\lambda_k))$.\n(d) Use a numerical unwrapping algorithm (like `numpy.unwrap`) on the sequence $\\{\\theta_k\\}$ to find the cumulative change in angle, avoiding $2\\pi$ ambiguities.\n(e) The total change $\\Delta_{\\Gamma} \\arg D(\\lambda)$ is the difference between the unwrapped angle at the end and the start of the path, plus the final angular jump to close the loop.\n(f) $N_{\\mathrm{Evans}}$ is the nearest integer to $\\frac{1}{2\\pi} \\Delta_{\\Gamma} \\arg D(\\lambda)$.\n\nThese three distinct methods are applied to each of the three provided matrices $H$, and the results are aggregated in the specified format. The consistency between $N_{\\mathrm{sign}}$ and $N_{\\mathrm{Evans}}$ serves as a cross-validation of the unstable eigenvalue count.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import signm, block_diag\n\ndef solve_case(H, epsilon, contour_params):\n    \"\"\"\n    Computes the stability diagnostics for a single Hamiltonian matrix H.\n    \n    Args:\n        H (np.ndarray): The symmetric matrix defining the Hamiltonian A = JH.\n        epsilon (float): The regularization parameter for projectors.\n        contour_params (tuple): Parameters (delta, R, T) for the Evans contour.\n    \n    Returns:\n        list: A list of four integers [N_Evans, N_sign, N_K_plus, N_K_minus].\n    \"\"\"\n    # Construct the canonical symplectic and Hamiltonian matrices\n    J1 = np.array([[0, 1], [-1, 0]], dtype=float)\n    J = block_diag(J1, J1)\n    A = J @ H\n    n_dim = A.shape[0]\n    I = np.identity(n_dim)\n\n    # --- 1. Compute N_sign using the matrix sign function ---\n    A_minus_eps = A - epsilon * I\n    sign_A_minus_eps = signm(A_minus_eps)\n    P_plus_shifted = 0.5 * (I + sign_A_minus_eps)\n    N_sign = int(np.round(np.real(np.trace(P_plus_shifted))))\n\n    # --- 2. Compute Krein signatures N_K^+, N_K^- ---\n    A_plus_eps = A + epsilon * I\n    sign_A_plus_eps = signm(A_plus_eps)\n    # The center projector P_c isolates the subspace for |Re(lambda)|  epsilon\n    Pc = 0.5 * (sign_A_plus_eps - sign_A_minus_eps)\n    \n    # Find an orthonormal basis for the range of Pc via SVD\n    # Pc = U @ S @ Vh. Columns of U form the basis.\n    U, s, Vh = np.linalg.svd(Pc)\n    \n    # Determine the rank of the projector from its singular values\n    rank_tol = 1e-8\n    rank = np.sum(s  rank_tol)\n    \n    N_K_plus = 0\n    N_K_minus = 0\n    if rank  0:\n        # Basis matrix V consists of the first 'rank' columns of U\n        V = U[:, :rank]\n        # Krein matrix G\n        G = V.conj().T @ H @ V\n        \n        # Eigenvalues of G determine the Krein signatures\n        g_eigvals = np.linalg.eigvalsh(G)\n        \n        # Count positive and negative eigenvalues using a tolerance\n        eig_tol = 1e-8\n        N_K_plus = np.sum(g_eigvals  eig_tol)\n        N_K_minus = np.sum(g_eigvals  -eig_tol)\n\n    # --- 3. Compute N_Evans using the argument principle ---\n    delta, R, T = contour_params\n    n_points_per_side = 250  # Number of points for discretizing each side of the rectangle\n\n    # Define the 4 segments of the rectangular contour (counter-clockwise)\n    p1 = delta - 1j * T\n    p2 = R - 1j * T\n    p3 = R + 1j * T\n    p4 = delta + 1j * T\n    path1 = np.linspace(p1, p2, n_points_per_side)\n    path2 = np.linspace(p2, p3, n_points_per_side)\n    path3 = np.linspace(p3, p4, n_points_per_side)\n    path4 = np.linspace(p4, p1, n_points_per_side)\n\n    # Concatenate paths, excluding duplicate endpoints\n    contour_points = np.concatenate([path1[:-1], path2[:-1], path3[:-1], path4[:-1]])\n    \n    # Evaluate D(lambda) = det(lambda*I - A) along the contour\n    det_values = np.array([np.linalg.det(lam * I - A) for lam in contour_points])\n    \n    # Compute the argument (angle) of each determinant value\n    angles = np.angle(det_values)\n    \n    # Unwrap the sequence of angles to find the total continuous change\n    unwrapped_angles = np.unwrap(angles)\n    \n    # Total change in argument is the difference between the last and first unwrapped angle\n    total_arg_change = unwrapped_angles[-1] - unwrapped_angles[0]\n    \n    # Add the final jump from the last point back to the first to close the loop\n    total_arg_change += np.angle(det_values[0] / det_values[-1])\n\n    # The number of enclosed zeros is the winding number\n    N_Evans = int(np.round(total_arg_change / (2 * np.pi)))\n    \n    return [N_Evans, N_sign, int(N_K_plus), int(N_K_minus)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # --- Define problem parameters ---\n    epsilon = 1e-1\n    contour_params = (2e-1, 1.6, 2.5)  # (delta, R, T)\n\n    # --- Define test cases ---\n    # Case A: Unstable spectrum\n    H1A = np.array([[1.0, 0.0], [0.0, -1.0]])\n    H2A = np.array([[2.0, 0.0], [0.0, -0.5]])\n    H_A = block_diag(H1A, H2A)\n\n    # Case B: Purely imaginary spectrum\n    H1B = np.array([[2.0, 0.0], [0.0, 2.0]])\n    H2B = np.array([[1.5, 0.1], [0.1, 1.0]])\n    H_B = block_diag(H1B, H2B)\n\n    # Case C: Mixed spectrum\n    H1C = np.array([[2.0, 0.0], [0.0, 2.0]])\n    H2C = np.array([[1.0, 0.0], [0.0, -1.0]])\n    H_C = block_diag(H1C, H2C)\n\n    test_cases = [H_A, H_B, H_C]\n    \n    all_results = []\n    for H in test_cases:\n        case_result = solve_case(H, epsilon, contour_params)\n        all_results.append(case_result)\n        \n    # --- Format final output string ---\n    list_of_lists_str = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(list_of_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}