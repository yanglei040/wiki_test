{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Bartels-Stewart algorithm is the solution of a Sylvester equation where the coefficient matrices are in a triangular or quasi-triangular form. This first practice focuses on the simplest case: a system with strictly upper triangular matrices. By deriving the scalar recursion from first principles, you will gain a foundational understanding of the back-substitution process and the precise data dependencies that dictate a valid computational ordering .",
            "id": "3584637",
            "problem": "Consider the Sylvester equation in the core step of the Bartels–Stewart algorithm (BSA), which reduces a general pair of matrices to real Schur form (RSF): given upper triangular matrices $T \\in \\mathbb{R}^{n \\times n}$ and $S \\in \\mathbb{R}^{m \\times m}$ and a right-hand side $F \\in \\mathbb{R}^{n \\times m}$, solve\n$$\nT Y + Y S = F,\n$$\nfor the unknown matrix $Y \\in \\mathbb{R}^{n \\times m}$.\n\n1. Starting only from the definition of matrix multiplication and the property that $T$ and $S$ are upper triangular, derive an explicit scalar recursion for the entries $y_{ij}$ of $Y$ in terms of entries of $T$, $S$, $F$, and already-computed entries of $Y$. Your derivation must separate out the diagonal terms and systematically reduce the double sums implied by the upper triangular structure; do not invoke any pre-packaged formulas.\n\n2. Using your recursion, propose an ordering on the index pairs $(i,j)$ that guarantees that whenever $y_{ij}$ is computed, all quantities on which it depends have already been computed. Justify your ordering logically from the dependency structure revealed by your derivation.\n\n3. Apply your ordering to the concrete $3 \\times 3$ instance with\n$$\nT=\\begin{pmatrix}\n2  1  -1\\\\\n0  3  2\\\\\n0  0  5\n\\end{pmatrix},\\quad\nS=\\begin{pmatrix}\n-1  2  0\\\\\n0  4  1\\\\\n0  0  6\n\\end{pmatrix},\\quad\nF=\\begin{pmatrix}\n1  0  2\\\\\n3  -1  1\\\\\n4  2  0\n\\end{pmatrix}.\n$$\nCompute the single entry $y_{23}$ exactly. Express your final answer as an exact rational number; do not round.",
            "solution": "The problem is valid as it represents a standard task in numerical linear algebra—the solution of the Sylvester equation for upper triangular matrices, which is the core of the Bartels-Stewart algorithm. The given matrices and equation are well-defined, and the existence of a unique solution is guaranteed because the spectra of $T$ and $-S$ are disjoint. Specifically, the eigenvalues of $T$ are $\\{2, 3, 5\\}$ and the eigenvalues of $S$ are $\\{-1, 4, 6\\}$, so the eigenvalues of $-S$ are $\\{1, -4, -6\\}$. These sets are disjoint, ensuring that $t_{ii} + s_{jj} \\neq 0$ for all $i, j$, which is the condition for the solvability of the recursion we will derive.\n\n### Part 1: Derivation of the Scalar Recursion\n\nThe Sylvester equation is given by\n$$\nTY + YS = F\n$$\nwhere $T \\in \\mathbb{R}^{n \\times n}$ and $S \\in \\mathbb{R}^{m \\times m}$ are upper triangular matrices, and $F \\in \\mathbb{R}^{n \\times m}$, $Y \\in \\mathbb{R}^{n \\times m}$. We seek a scalar formula for the entry $y_{ij}$ of the matrix $Y$.\n\nThe $(i,j)$-th entry of the matrix equation is given by:\n$$\n(TY)_{ij} + (YS)_{ij} = f_{ij}\n$$\nfor $i \\in \\{1, \\dots, n\\}$ and $j \\in \\{1, \\dots, m\\}$.\n\nUsing the definition of matrix multiplication, we expand the two terms on the left-hand side:\n$$\n(TY)_{ij} = \\sum_{k=1}^{n} t_{ik} y_{kj}\n$$\n$$\n(YS)_{ij} = \\sum_{l=1}^{m} y_{il} s_{lj}\n$$\n\nNow, we incorporate the upper triangular structure of $T$ and $S$.\nFor the matrix $T$, $t_{ik} = 0$ for $i > k$. Thus, the summation for $(TY)_{ij}$ can be simplified by starting the index $k$ at $i$:\n$$\n(TY)_{ij} = \\sum_{k=i}^{n} t_{ik} y_{kj}\n$$\nFor the matrix $S$, $s_{lj} = 0$ for $l > j$. Thus, the summation for $(YS)_{ij}$ can be simplified by ending the index $l$ at $j$:\n$$\n(YS)_{ij} = \\sum_{l=1}^{j} y_{il} s_{lj}\n$$\n\nSubstituting these back into the component-wise equation gives:\n$$\n\\sum_{k=i}^{n} t_{ik} y_{kj} + \\sum_{l=1}^{j} y_{il} s_{lj} = f_{ij}\n$$\n\nOur goal is to solve for $y_{ij}$. To do this, we must isolate the terms containing $y_{ij}$ from both summations.\nIn the first sum, $\\sum_{k=i}^{n} t_{ik} y_{kj}$, the term containing $y_{ij}$ occurs when $k=i$.\nIn the second sum, $\\sum_{l=1}^{j} y_{il} s_{lj}$, the term containing $y_{ij}$ occurs when $l=j$.\n\nLet's separate these terms from their respective sums:\n$$\n\\left( t_{ii} y_{ij} + \\sum_{k=i+1}^{n} t_{ik} y_{kj} \\right) + \\left( \\sum_{l=1}^{j-1} y_{il} s_{lj} + y_{ij} s_{jj} \\right) = f_{ij}\n$$\nNote that for $i=n$, the first sum $\\sum_{k=i+1}^{n}$ is empty, and for $j=1$, the second sum $\\sum_{l=1}^{j-1}$ is empty.\n\nNow, we group the terms with $y_{ij}$ on the left side and move all other terms to the right side:\n$$\nt_{ii} y_{ij} + y_{ij} s_{jj} = f_{ij} - \\sum_{k=i+1}^{n} t_{ik} y_{kj} - \\sum_{l=1}^{j-1} y_{il} s_{lj}\n$$\nFactoring out $y_{ij}$:\n$$\n(t_{ii} + s_{jj}) y_{ij} = f_{ij} - \\sum_{k=i+1}^{n} t_{ik} y_{kj} - \\sum_{l=1}^{j-1} y_{il} s_{lj}\n$$\nAs established, the condition for a unique solution ensures that $t_{ii} + s_{jj} \\neq 0$ for all valid pairs $(i,j)$. Therefore, we can divide by this factor to obtain the explicit scalar recursion for $y_{ij}$:\n$$\ny_{ij} = \\frac{1}{t_{ii} + s_{jj}} \\left( f_{ij} - \\sum_{k=i+1}^{n} t_{ik} y_{kj} - \\sum_{l=1}^{j-1} y_{il} s_{lj} \\right)\n$$\n\n### Part 2: Ordering of Computations\n\nThe recursion derived in Part 1 shows the dependencies for calculating $y_{ij}$:\n$y_{ij}$ depends on:\n1.  Entries of $T$, $S$, and $F$. These are given.\n2.  Entries $y_{kj}$ where $k > i$. These are entries of $Y$ in the same column $j$, but in rows below row $i$.\n3.  Entries $y_{il}$ where $l  j$. These are entries of $Y$ in the same row $i$, but in columns to the left of column $j$.\n\nTo ensure a valid computation order, we must compute $y_{ij}$ only after all entries it depends on have been computed. Let's analyze the dependency structure. To compute an entry at position $(i,j)$, we need all entries in the rectangle below it in the same column and all entries in the rectangle to its left in the same row.\n\nThis suggests an ordering that sweeps through the matrix $Y$ such that these dependencies are met. One such valid ordering is to compute the entries of $Y$ column by column, from left to right, and within each column, from bottom to top.\n\nThe proposed ordering is as follows:\nIterate through the columns $j$ from $1$ to $m$. For each column $j$, iterate through the rows $i$ from $n$ down to $1$. The sequence of computed indices $(i,j)$ would be $(n,1), (n-1,1), \\dots, (1,1)$, then $(n,2), (n-1,2), \\dots, (1,2)$, and so on, up to $(n,m), \\dots, (1,m)$.\n\nJustification: When we compute $y_{ij}$ using this order:\n- The term $\\sum_{k=i+1}^{n} t_{ik} y_{kj}$ depends on entries $y_{(i+1)j}, \\dots, y_{nj}$. These are in the same column $j$ but in rows below $i$. Since we are iterating rows from $n$ down to $1$, these entries would have already been computed in the current column sweep.\n- The term $\\sum_{l=1}^{j-1} y_{il} s_{lj}$ depends on entries $y_{i1}, \\dots, y_{i(j-1)}$. These are in the same row $i$ but in columns to the left of $j$. Since we are iterating columns from $1$ to $m$, all entries in columns $1, \\dots, j-1$ have already been fully computed.\n\nTherefore, this ordering guarantees that all required values on the right-hand side of the recursion are known at the time of computing $y_{ij}$. An alternative valid ordering is to iterate through rows $i$ from $n$ down to $1$, and for each row, iterate through columns $j$ from $1$ to $m$.\n\n### Part 3: Computation of $y_{23}$\n\nWe are given the matrices:\n$$\nT=\\begin{pmatrix}\n2  1  -1\\\\\n0  3  2\\\\\n0  0  5\n\\end{pmatrix},\\quad\nS=\\begin{pmatrix}\n-1  2  0\\\\\n0  4  1\\\\\n0  0  6\n\\end{pmatrix},\\quad\nF=\\begin{pmatrix}\n1  0  2\\\\\n3  -1  1\\\\\n4  2  0\n\\end{pmatrix}\n$$\nWe need to find $y_{23}$. Using the formula for $(i,j) = (2,3)$ with $n=3$, $m=3$:\n$$\ny_{23} = \\frac{1}{t_{22} + s_{33}} \\left( f_{23} - \\sum_{k=3}^{3} t_{2k} y_{k3} - \\sum_{l=1}^{2} y_{2l} s_{l3} \\right)\n$$\n$$\ny_{23} = \\frac{1}{t_{22} + s_{33}} \\left( f_{23} - t_{23}y_{33} - (y_{21}s_{13} + y_{22}s_{23}) \\right)\n$$\nThis shows that to compute $y_{23}$, we first need to compute $y_{33}$, $y_{21}$, and $y_{22}$. We will compute these dependencies following the ordering from Part 2.\n\n1.  **Compute $y_{31}$:** Start with column $j=1$, row $i=3$.\n    $y_{31} = \\frac{1}{t_{33}+s_{11}}(f_{31}) = \\frac{1}{5+(-1)}(4) = \\frac{4}{4} = 1$.\n\n2.  **Compute $y_{21}$:** Column $j=1$, row $i=2$.\n    $y_{21} = \\frac{1}{t_{22}+s_{11}}(f_{21} - t_{23}y_{31}) = \\frac{1}{3+(-1)}(3 - (2)(1)) = \\frac{1}{2}(1) = \\frac{1}{2}$.\n\n3.  **Compute $y_{32}$:** Column $j=2$, row $i=3$.\n    $y_{32} = \\frac{1}{t_{33}+s_{22}}(f_{32} - y_{31}s_{12}) = \\frac{1}{5+4}(2 - (1)(2)) = \\frac{1}{9}(0) = 0$.\n\n4.  **Compute $y_{22}$:** Column $j=2$, row $i=2$.\n    $y_{22} = \\frac{1}{t_{22}+s_{22}}(f_{22} - t_{23}y_{32} - y_{21}s_{12}) = \\frac{1}{3+4}(-1 - (2)(0) - (\\frac{1}{2})(2)) = \\frac{1}{7}(-1 - 1) = -\\frac{2}{7}$.\n\n5.  **Compute $y_{33}$:** Column $j=3$, row $i=3$.\n    $y_{33} = \\frac{1}{t_{33}+s_{33}}(f_{33} - (y_{31}s_{13} + y_{32}s_{23})) = \\frac{1}{5+6}(0 - ((1)(0) + (0)(1))) = \\frac{1}{11}(0) = 0$.\n\n6.  **Finally, compute $y_{23}$:** Column $j=3$, row $i=2$.\n    We now have all the necessary values:\n    - $t_{22}=3, s_{33}=6$\n    - $f_{23}=1$\n    - $t_{23}=2$\n    - $s_{13}=0, s_{23}=1$\n    - $y_{33}=0$\n    - $y_{21}=1/2$\n    - $y_{22}=-2/7$\n\n    Substitute these into the formula for $y_{23}$:\n    $$\n    y_{23} = \\frac{1}{3+6} \\left( 1 - (2)(0) - \\left( \\left(\\frac{1}{2}\\right)(0) + \\left(-\\frac{2}{7}\\right)(1) \\right) \\right)\n    $$\n    $$\n    y_{23} = \\frac{1}{9} \\left( 1 - 0 - \\left( 0 - \\frac{2}{7} \\right) \\right)\n    $$\n    $$\n    y_{23} = \\frac{1}{9} \\left( 1 + \\frac{2}{7} \\right)\n    $$\n    $$\n    y_{23} = \\frac{1}{9} \\left( \\frac{7}{7} + \\frac{2}{7} \\right)\n    $$\n    $$\n    y_{23} = \\frac{1}{9} \\left( \\frac{9}{7} \\right)\n    $$\n    $$\n    y_{23} = \\frac{1}{7}\n    $$\nThe value of the entry $y_{23}$ is $\\frac{1}{7}$.",
            "answer": "$$\\boxed{\\frac{1}{7}}$$"
        },
        {
            "introduction": "Having established the substitution method for triangular systems, we now address the structure central to the Bartels-Stewart algorithm: the real Schur form. This form is quasi-upper triangular, containing both $1 \\times 1$ blocks (for real eigenvalues) and $2 \\times 2$ blocks (for complex conjugate eigenpairs). This exercise challenges you to derive the explicit solutions for the small, dense linear systems that arise at each step of the block-wise substitution, revealing the mechanics of handling both real and complex spectral information and the conditions that ensure each subproblem is well-posed .",
            "id": "3584711",
            "problem": "Consider the Sylvester equation $A X + X B = C$ for $A, B \\in \\mathbb{R}^{n \\times n}$ and $C \\in \\mathbb{R}^{n \\times n}$. The Bartels–Stewart algorithm (BSA) reduces $A$ and $B$ to real Schur form, $T_A = Q_A^{\\mathsf{T}} A Q_A$ and $T_B = Q_B^{\\mathsf{T}} B Q_B$, where $Q_A, Q_B$ are orthogonal and $T_A, T_B$ are quasi-upper triangular, i.e., block upper triangular with diagonal blocks of size either $1\\times 1$ or $2\\times 2$. The algorithm then solves $T_A Y + Y T_B = \\widehat{C}$ with $\\widehat{C} = Q_A^{\\mathsf{T}} C Q_B$ by block back substitution on these $1\\times 1$ and $2\\times 2$ diagonal blocks.\n\nStarting from the definitions above and fundamental facts about vectorization and Kronecker products, derive the explicit local triangular Sylvester solves that arise when $T_A$ and $T_B$ have $1\\times 1$ and $2\\times 2$ diagonal blocks, and characterize the conditions under which the $2\\times 2$ blocks yield near-singular local subproblems.\n\nSpecifically:\n\nFirst, derive the scalar local solve when a $1\\times 1$ block $[a]$ from $T_A$ interacts with a $1\\times 1$ block $[b]$ from $T_B$, and express the solution $x$ in terms of $a$, $b$, and $c$.\n\nNext, derive the explicit $2\\times 1$ local solve when a $2\\times 2$ block $A_2$ from $T_A$ interacts with a $1\\times 1$ block $[b]$ from $T_B$, i.e., for $A_2 X + X b = C$ with $X, C \\in \\mathbb{R}^{2\\times 1}$, and similarly derive the explicit $1\\times 2$ local solve when a $1\\times 1$ block $[a]$ from $T_A$ interacts with a $2\\times 2$ block $B_2$ from $T_B$, i.e., for $a X + X B_2 = C$ with $X, C \\in \\mathbb{R}^{1\\times 2}$.\n\nThen, consider the case where both interacting diagonal blocks are $2\\times 2$ real Schur blocks $A_2$ and $B_2$ representing complex conjugate eigenpairs $\\alpha \\pm i \\beta$ and $\\gamma \\pm i \\delta$, with $\\beta > 0$ and $\\delta > 0$. Use the vectorization identity and properties of the Kronecker sum to write the $4\\times 4$ coefficient matrix of the vectorized local Sylvester subproblem $A_2 X + X B_2 = C$, and compute its determinant in closed form as a function of $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$. Based on this expression, state precise spectral conditions under which this $4\\times 4$ local system is near-singular.\n\nFinally, evaluate your closed-form determinant when $\\alpha = 1.13$, $\\beta = 0.92$, $\\gamma = -1.08$, and $\\delta = 0.89$. Round your final numerical answer to $4$ significant figures. Report only the determinant value; do not include units.",
            "solution": "The problem asks for the derivation of local subproblems arising in the Bartels-Stewart algorithm for solving the Sylvester equation $A X + X B = C$, an analysis of their singularity, and a specific numerical calculation. The problem is well-defined, scientifically grounded in numerical linear algebra, and all necessary information is provided. We proceed with the solution.\n\nThe core of the Bartels-Stewart algorithm is to transform the original Sylvester equation into a structurally simpler one. Given $A X + X B = C$, we compute the real Schur decompositions $A = Q_A T_A Q_A^{\\mathsf{T}}$ and $B = Q_B T_B Q_B^{\\mathsf{T}}$, where $Q_A$ and $Q_B$ are orthogonal matrices and $T_A$ and $T_B$ are quasi-upper triangular matrices (real Schur forms). Substituting these into the Sylvester equation gives:\n$$(Q_A T_A Q_A^{\\mathsf{T}}) X + X (Q_B T_B Q_B^{\\mathsf{T}}) = C$$\nMultiplying by $Q_A^{\\mathsf{T}}$ on the left and $Q_B$ on the right yields:\n$$Q_A^{\\mathsf{T}} (Q_A T_A Q_A^{\\mathsf{T}}) X Q_B + Q_A^{\\mathsf{T}} X (Q_B T_B Q_B^{\\mathsf{T}}) Q_B = Q_A^{\\mathsf{T}} C Q_B$$\nUsing $Q_A^{\\mathsf{T}} Q_A = I$ and $Q_B^{\\mathsf{T}} Q_B = I$, and defining $Y = Q_A^{\\mathsf{T}} X Q_B$ and $\\widehat{C} = Q_A^{\\mathsf{T}} C Q_B$, we obtain the transformed Sylvester equation:\n$$T_A Y + Y T_B = \\widehat{C}$$\nSince $T_A$ and $T_B$ are quasi-upper triangular, this system can be solved for the blocks of $Y$ using a process of block back substitution. We now analyze the local linear systems that must be solved at each step of this substitution.\n\nLet $T_A = (T_{A,ij})$ and $T_B = (T_{B,ij})$ be partitioned conformally with $Y=(Y_{ij})$ and $\\widehat{C}=(\\widehat{C}_{ij})$, where the diagonal blocks $T_{A,ii}$ and $T_{B,jj}$ are of size $1 \\times 1$ or $2 \\times 2$. The $(i,j)$ block equation is:\n$$T_{A,ii} Y_{ij} + Y_{ij} T_{B,jj} = \\widehat{C}_{ij} - \\sum_{k=i+1}^{p} T_{A,ik} Y_{kj} - \\sum_{l=1}^{j-1} Y_{il} T_{B,lj}$$\nThe Bartels-Stewart algorithm solves for $Y_{ij}$ for $i=p, \\dots, 1$ and $j=1, \\dots, q$ (or another order). At each step, the right-hand side is known, and we must solve a small Sylvester equation for $Y_{ij}$. We now derive the explicit forms of these local solves.\n\nFirst, consider the case where a $1 \\times 1$ diagonal block of $T_A$, denoted $T_{A,ii} = [a]$, interacts with a $1 \\times 1$ diagonal block of $T_B$, denoted $T_{B,jj} = [b]$. The unknown $Y_{ij}$ is a scalar $x$, and the right-hand side is a scalar $c$. The local equation is:\n$$[a] [x] + [x] [b] = [c]$$\nThis is the scalar equation $a x + x b = c$, or $(a+b)x = c$. The solution is:\n$$x = \\frac{c}{a+b}$$\nThis solution is unique provided $a+b \\neq 0$. This condition corresponds to $\\lambda_i(A) + \\lambda_j(B) \\neq 0$, where $\\lambda_i(A)=a$ and $\\lambda_j(B)=b$ are real eigenvalues of $A$ and $B$.\n\nNext, we derive the $2 \\times 1$ local solve where a $2 \\times 2$ block $T_{A,ii} = A_2$ from $T_A$ interacts with a $1 \\times 1$ block $T_{B,jj} = [b]$ from $T_B$. Here, the unknown $Y_{ij} = X$ is a $2 \\times 1$ column vector, and the right-hand side $\\widehat{C}_{ij}' = C$ is also a $2 \\times 1$ vector. The local equation is:\n$$A_2 X + X b = C$$\nSince $b$ is a scalar, $Xb = bX$. The equation can be rewritten as:\n$$A_2 X + b X = C \\implies (A_2 + b I) X = C$$\nwhere $I$ is the $2 \\times 2$ identity matrix. This is a $2 \\times 2$ linear system for the unknown vector $X$. Let $A_2 = \\begin{pmatrix} a_{11}  a_{12} \\\\ a_{21}  a_{22} \\end{pmatrix}$, $X = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, and $C = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$. The system is explicitly:\n$$\\begin{pmatrix} a_{11}+b  a_{12} \\\\ a_{21}  a_{22}+b \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$$\nThis system has a unique solution if the matrix $A_2+bI$ is non-singular, which is true if $-b$ is not an eigenvalue of $A_2$.\n\nSimilarly, we derive the $1 \\times 2$ local solve where a $1 \\times 1$ block $[a]$ from $T_A$ interacts with a $2 \\times 2$ block $B_2$ from $T_B$. Let $Y_{ij}=X \\in \\mathbb{R}^{1 \\times 2}$ and $\\widehat{C}_{ij}'=C \\in \\mathbb{R}^{1 \\times 2}$. The equation is:\n$$a X + X B_2 = C$$\nLet $X = \\begin{pmatrix} x_1  x_2 \\end{pmatrix}$, $B_2 = \\begin{pmatrix} b_{11}  b_{12} \\\\ b_{21}  b_{22} \\end{pmatrix}$, and $C = \\begin{pmatrix} c_1  c_2 \\end{pmatrix}$. The equation expands to:\n$$a \\begin{pmatrix} x_1  x_2 \\end{pmatrix} + \\begin{pmatrix} x_1  x_2 \\end{pmatrix} \\begin{pmatrix} b_{11}  b_{12} \\\\ b_{21}  b_{22} \\end{pmatrix} = \\begin{pmatrix} c_1  c_2 \\end{pmatrix}$$\n$$\\begin{pmatrix} ax_1 + x_1 b_{11} + x_2 b_{21}  ax_2 + x_1 b_{12} + x_2 b_{22} \\end{pmatrix} = \\begin{pmatrix} c_1  c_2 \\end{pmatrix}$$\nThis yields a system of two linear equations in two variables $x_1, x_2$:\n$$\\begin{cases} (a+b_{11}) x_1 + b_{21} x_2 = c_1 \\\\ b_{12} x_1 + (a+b_{22}) x_2 = c_2 \\end{cases}$$\nThis can be written in matrix form as:\n$$\\begin{pmatrix} a+b_{11}  b_{21} \\\\ b_{12}  a+b_{22} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$$\nThe coefficient matrix is $aI + B_2^{\\mathsf{T}}$. The system has a unique solution if $aI+B_2^{\\mathsf{T}}$ is non-singular, i.e., if $-a$ is not an eigenvalue of $B_2^{\\mathsf{T}}$ (and thus of $B_2$).\n\nNow, we consider the case where both interacting blocks are $2 \\times 2$. Let the local equation be $A_2 X + X B_2 = C$, where $A_2, B_2, X, C \\in \\mathbb{R}^{2 \\times 2}$. The block $A_2$ represents a complex conjugate pair of eigenvalues $\\alpha \\pm i \\beta$ with $\\beta > 0$, and $B_2$ represents a pair $\\gamma \\pm i \\delta$ with $\\delta > 0$. We use the vectorization operator $\\text{vec}(\\cdot)$ and the Kronecker product $\\otimes$. The equation can be vectorized as:\n$$\\text{vec}(A_2 X) + \\text{vec}(X B_2) = \\text{vec}(C)$$\nUsing the identity $\\text{vec}(PQR) = (R^{\\mathsf{T}} \\otimes P) \\text{vec}(Q)$, we have:\n$$(I \\otimes A_2) \\text{vec}(X) + (B_2^{\\mathsf{T}} \\otimes I) \\text{vec}(X) = \\text{vec}(C)$$\nThis is a $4 \\times 4$ linear system for $\\text{vec}(X)$:\n$$(I \\otimes A_2 + B_2^{\\mathsf{T}} \\otimes I) \\text{vec}(X) = \\text{vec}(C)$$\nThe $4 \\times 4$ coefficient matrix is the Kronecker sum $K = A_2 \\oplus B_2^{\\mathsf{T}}$. A standard real Schur form for a matrix with eigenvalues $\\lambda \\pm i\\mu$ ($\\mu \\neq 0$) is $\\begin{pmatrix} \\lambda  \\mu \\\\ -\\mu  \\lambda \\end{pmatrix}$. Without loss of generality, let $A_2 = \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}$ and $B_2 = \\begin{pmatrix} \\gamma  \\delta \\\\ -\\delta  \\gamma \\end{pmatrix}$. Then $B_2^{\\mathsf{T}} = \\begin{pmatrix} \\gamma  -\\delta \\\\ \\delta  \\gamma \\end{pmatrix}$. The coefficient matrix is explicitly:\n$$K = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\otimes \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix} + \\begin{pmatrix} \\gamma  -\\delta \\\\ \\delta  \\gamma \\end{pmatrix} \\otimes \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\alpha+\\gamma  \\beta  -\\delta  0 \\\\ -\\beta  \\alpha+\\gamma  0  -\\delta \\\\ \\delta  0  \\alpha+\\gamma  \\beta \\\\ 0  \\delta  -\\beta  \\alpha+\\gamma \\end{pmatrix}$$\nThe determinant of a Kronecker sum is the product of its eigenvalues. The eigenvalues of $K = A_2 \\oplus B_2^{\\mathsf{T}}$ are all possible sums of eigenvalues of $A_2$ and $B_2^{\\mathsf{T}}$. The eigenvalues of $A_2$ are $\\lambda(A_2) = \\{\\alpha+i\\beta, \\alpha-i\\beta\\}$. The eigenvalues of $B_2^{\\mathsf{T}}$ are the same as $B_2$, so $\\lambda(B_2^{\\mathsf{T}}) = \\{\\gamma+i\\delta, \\gamma-i\\delta\\}$.\nThe four eigenvalues of $K$ are:\n$k_1 = (\\alpha+i\\beta) + (\\gamma+i\\delta) = (\\alpha+\\gamma) + i(\\beta+\\delta)$\n$k_2 = (\\alpha+i\\beta) + (\\gamma-i\\delta) = (\\alpha+\\gamma) + i(\\beta-\\delta)$\n$k_3 = (\\alpha-i\\beta) + (\\gamma+i\\delta) = (\\alpha+\\gamma) - i(\\beta-\\delta)$\n$k_4 = (\\alpha-i\\beta) + (\\gamma-i\\delta) = (\\alpha+\\gamma) - i(\\beta+\\delta)$\nThe determinant is the product of these eigenvalues. Pairing complex conjugates ($k_1$ with $k_4$, and $k_2$ with $k_3$):\n$\\det(K) = (k_1 k_4) \\cdot (k_2 k_3) = (k_1 \\bar{k_1}) \\cdot (k_2 \\bar{k_2}) = |k_1|^2 |k_2|^2$\n$\\det(K) = |(\\alpha+\\gamma) + i(\\beta+\\delta)|^2 \\cdot |(\\alpha+\\gamma) + i(\\beta-\\delta)|^2$\n$\\det(K) = ((\\alpha+\\gamma)^2 + (\\beta+\\delta)^2) ((\\alpha+\\gamma)^2 + (\\beta-\\delta)^2)$\nThis is the closed-form expression for the determinant.\n\nThe local system is singular if $\\det(K) = 0$. Since $\\alpha, \\beta, \\gamma, \\delta$ are real numbers and $\\beta0, \\delta0$, the first factor $(\\alpha+\\gamma)^2 + (\\beta+\\delta)^2$ is always positive (since $\\beta+\\delta  0$). Therefore, singularity occurs if and only if the second factor is zero:\n$$(\\alpha+\\gamma)^2 + (\\beta-\\delta)^2 = 0$$\nThis is true if and only if $\\alpha+\\gamma = 0$ and $\\beta-\\delta = 0$. The spectral condition for singularity is therefore $\\alpha = -\\gamma$ and $\\beta = \\delta$. This means that an eigenvalue pair of $A_2$, $\\alpha \\pm i\\beta$, is the negative of an eigenvalue pair of $B_2$, $-(\\gamma \\pm i\\delta) = \\alpha \\mp i\\beta$.\nThe system is near-singular when the determinant is close to zero. This occurs when $(\\alpha+\\gamma)^2 + (\\beta-\\delta)^2 \\approx 0$, which means the spectral condition is nearly met: $\\alpha \\approx -\\gamma$ and $\\beta \\approx \\delta$.\n\nFinally, we evaluate the determinant for the given values: $\\alpha = 1.13$, $\\beta = 0.92$, $\\gamma = -1.08$, and $\\delta = 0.89$.\nWe compute the intermediate terms:\n$\\alpha+\\gamma = 1.13 + (-1.08) = 0.05$\n$\\beta+\\delta = 0.92 + 0.89 = 1.81$\n$\\beta-\\delta = 0.92 - 0.89 = 0.03$\nSubstituting these into the determinant formula:\n$\\det(K) = ((0.05)^2 + (1.81)^2) ((0.05)^2 + (0.03)^2)$\n$\\det(K) = (0.0025 + 3.2761) (0.0025 + 0.0009)$\n$\\det(K) = (3.2786) (0.0034)$\n$\\det(K) = 0.01114724$\nRounding to $4$ significant figures, as requested:\n$\\det(K) \\approx 0.01115$.",
            "answer": "$$\n\\boxed{0.01115}\n$$"
        },
        {
            "introduction": "We have seen that the algorithm's use of a real Schur form requires treating $2 \\times 2$ diagonal blocks as indivisible units. This final practice provides a compelling justification for this added complexity by exploring its role in numerical stability. By analyzing a carefully constructed case where a naive, block-ignorant approach leads to catastrophic error, you will see firsthand why honoring the block structure is essential for obtaining an accurate solution, especially when the problem is ill-conditioned .",
            "id": "3584620",
            "problem": "Consider the Sylvester equation $A X + X B = C$ where $A \\in \\mathbb{R}^{2 \\times 2}$ is upper triangular and $B \\in \\mathbb{R}^{2 \\times 2}$ is a real $2 \\times 2$ Schur block corresponding to a complex conjugate eigenpair. In the Bartels–Stewart algorithm, one first reduces to real Schur form and then solves the resulting triangular Sylvester equation by block back substitution, treating $2 \\times 2$ diagonal blocks as atomic pivots. In this problem, you will construct a counterexample showing that naively treating the $2 \\times 2$ Schur block in $B$ as two separate $1 \\times 1$ scalars (i.e., discarding the off-diagonal coupling) produces catastrophic amplification, and then quantify the improvement obtained by block-aware pivoting.\n\nLet\n$$\nA = \\begin{pmatrix}\n-1  1 \\\\\n0  -1\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 + \\delta  M \\\\\n- M  1 + \\delta\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n0  0 \\\\\n1  0\n\\end{pmatrix},\n$$\nwith fixed parameters $M  0$ and $0  \\delta \\ll 1$. Note that $B$ is the real Schur form of the complex pair $1 + \\delta \\pm i M$.\n\n1. Using only foundational facts about Sylvester equations and spectra (namely, that $A X + X B = C$ has a unique solution if and only if the spectra of $A$ and $-B$ are disjoint), justify that the true Sylvester equation above is uniquely solvable for all $M  0$ and $\\delta  0$.\n\n2. Define a naive, block-ignorant scheme by replacing $B$ with the diagonal matrix $\\widetilde{B} = \\operatorname{diag}(1 + \\delta, 1 + \\delta)$, thereby discarding the off-diagonal coupling in the $2 \\times 2$ Schur block. In this naive scheme, the second row of $X$ decouples and is computed from the scalar triangular systems $(A + (1 + \\delta) I) X_{:,j} = C_{:,j}$ for $j \\in \\{1,2\\}$. Compute the resulting pair $(x_{21}^{\\mathrm{naive}}, x_{22}^{\\mathrm{naive}})$ and its Euclidean norm in terms of $\\delta$ and $M$.\n\n3. In the correct block-aware pivoting mandated by the Bartels–Stewart algorithm, the variables $(x_{21}, x_{22})$ are solved simultaneously from the $2 \\times 2$ linear system obtained by restricting $A X + X B = C$ to the second row of $X$ and to the $2 \\times 2$ Schur block of $B$. Derive the $2 \\times 2$ system for $(x_{21}, x_{22})$, solve it explicitly, and compute the Euclidean norm $\\| (x_{21}, x_{22}) \\|_{2}$ in terms of $\\delta$ and $M$.\n\n4. Define the improvement factor as the ratio of the naive norm to the block-aware norm for the pair $(x_{21}, x_{22})$. Express this ratio analytically in terms of $\\delta$ and $M$, then evaluate it for the specific numerical values $\\delta = 10^{-6}$ and $M = 1$. Round your final answer to three significant figures. Express the final answer as a single real number without any units.",
            "solution": "The problem asks for an analysis of a specific Sylvester equation $A X + X B = C$ to illustrate a key stability feature of the Bartels-Stewart algorithm. The analysis involves comparing a naive solution method with the correct block-aware method.\n\nThe given matrices and parameters are:\n$$\nA = \\begin{pmatrix}\n-1  1 \\\\\n0  -1\n\\end{pmatrix}, \\quad\nB = \\begin{pmatrix}\n1 + \\delta  M \\\\\n-M  1 + \\delta\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n0  0 \\\\\n1  0\n\\end{pmatrix},\n$$\nwith $M  0$ and $0  \\delta \\ll 1$.\n\n1. Justification of a unique solution:\nA unique solution to the Sylvester equation $A X + X B = C$ exists if and only if the spectra of $A$ and $-B$, denoted $\\lambda(A)$ and $\\lambda(-B)$ respectively, are disjoint, i.e., $\\lambda(A) \\cap \\lambda(-B) = \\emptyset$.\n\nFirst, we determine the spectrum of $A$. Since $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries.\n$$ \\lambda(A) = \\{-1\\} $$\nNext, we find the spectrum of $B$. The eigenvalues of $B$ are the roots of the characteristic equation $\\det(B - \\lambda I) = 0$.\n$$ \\det \\begin{pmatrix} 1 + \\delta - \\lambda  M \\\\ -M  1 + \\delta - \\lambda \\end{pmatrix} = (1 + \\delta - \\lambda)^2 + M^2 = 0 $$\n$$ (1 + \\delta - \\lambda)^2 = -M^2 $$\n$$ 1 + \\delta - \\lambda = \\pm i M $$\n$$ \\lambda = 1 + \\delta \\mp i M $$\nSo, the spectrum of $B$ is $\\lambda(B) = \\{1 + \\delta + i M, 1 + \\delta - i M\\}$.\n\nThe spectrum of $-B$ consists of the negatives of the eigenvalues of $B$.\n$$ \\lambda(-B) = \\{-(1 + \\delta + i M), -(1 + \\delta - i M)\\} = \\{-1 - \\delta - i M, -1 - \\delta + i M\\} $$\nTo check if the spectra $\\lambda(A)$ and $\\lambda(-B)$ are disjoint, we compare the single element of $\\lambda(A)$, which is $-1$, with the elements of $\\lambda(-B)$. If $-1$ were an element of $\\lambda(-B)$, then we would have $-1 = -1 - \\delta \\pm i M$, which simplifies to $\\delta = \\pm i M$. This is a contradiction, because $\\delta$ and $M$ are specified as real and positive numbers. Therefore, $-1 \\notin \\lambda(-B)$, and the spectra are disjoint. This confirms that the Sylvester equation $A X + X B = C$ has a unique solution.\n\n2. Naive, block-ignorant scheme:\nIn this scheme, $B$ is approximated by its diagonal part, $\\widetilde{B} = \\operatorname{diag}(1 + \\delta, 1 + \\delta) = (1 + \\delta)I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix. The Sylvester equation becomes $A X_{\\mathrm{naive}} + X_{\\mathrm{naive}}((1 + \\delta)I_{2}) = C$. Factoring $X_{\\mathrm{naive}}$ gives $(A + (1 + \\delta)I_{2}) X_{\\mathrm{naive}} = C$.\n\nLet's compute the matrix $A + (1+\\delta)I_{2}$:\n$$ A + (1+\\delta)I_{2} = \\begin{pmatrix} -1  1 \\\\ 0  -1 \\end{pmatrix} + \\begin{pmatrix} 1+\\delta  0 \\\\ 0  1+\\delta \\end{pmatrix} = \\begin{pmatrix} \\delta  1 \\\\ 0  \\delta \\end{pmatrix} $$\nThe system to solve is:\n$$ \\begin{pmatrix} \\delta  1 \\\\ 0  \\delta \\end{pmatrix} \\begin{pmatrix} x_{11}  x_{12} \\\\ x_{21}  x_{22} \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix} $$\nWe are asked to find the second row of $X_{\\mathrm{naive}}$, which we call $(x_{21}^{\\mathrm{naive}}, x_{22}^{\\mathrm{naive}})$. This row is determined by the second row of the matrix equation, which decouples from the first row. For the first column $(j=1)$, the equation is $0 \\cdot x_{11} + \\delta \\cdot x_{21} = 1$, which yields $x_{21}^{\\mathrm{naive}} = \\frac{1}{\\delta}$. For the second column $(j=2)$, the equation is $0 \\cdot x_{12} + \\delta \\cdot x_{22} = 0$, which yields $x_{22}^{\\mathrm{naive}} = 0$.\nSo, the naive solution for the second row is $(x_{21}^{\\mathrm{naive}}, x_{22}^{\\mathrm{naive}}) = (\\frac{1}{\\delta}, 0)$.\n\nThe Euclidean norm of this vector is:\n$$ \\|(x_{21}^{\\mathrm{naive}}, x_{22}^{\\mathrm{naive}})\\|_2 = \\sqrt{\\left(\\frac{1}{\\delta}\\right)^2 + 0^2} = \\frac{1}{\\delta} \\quad (\\text{since } \\delta > 0) $$\n\n3. Correct block-aware pivoting:\nThe Bartels-Stewart algorithm solves for $X$ using block back substitution based on the Schur form of $A$. Here, we can partition $A$ and $X$ by rows. Let $X_1 = (x_{11}, x_{12})$ and $X_2 = (x_{21}, x_{22})$ be the first and second rows of $X$. The Sylvester equation $AX+XB=C$ can be written in block form for the rows of $X$:\n$$ \\begin{pmatrix} -1  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} + \\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix} B = \\begin{pmatrix} C_1 \\\\ C_2 \\end{pmatrix} $$\nThis expands to a system of two matrix equations:\n$$ -X_1 + X_2 + X_1 B = C_1 $$\n$$ -X_2 + X_2 B = C_2 $$\nThe second equation, $-X_2 + X_2 B = C_2$, can be solved first for $X_2$. With $X_2 = (x_{21}, x_{22})$ and $C_2 = (1, 0)$ (the second row of $C$), we have:\n$$ (x_{21}, x_{22})(B - I_2) = (1, 0) $$\nThe matrix $(B - I_2)$ is:\n$$ B - I_2 = \\begin{pmatrix} 1 + \\delta  M \\\\ -M  1 + \\delta \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\delta  M \\\\ -M  \\delta \\end{pmatrix} $$\nThe equation for $(x_{21}, x_{22})$ is $(x_{21}, x_{22}) \\begin{pmatrix} \\delta  M \\\\ -M  \\delta \\end{pmatrix} = (1, 0)$. This corresponds to the $2 \\times 2$ linear system:\n$$ \\delta x_{21} - M x_{22} = 1 $$\n$$ M x_{21} + \\delta x_{22} = 0 $$\nFrom the second equation, we find $x_{21} = -\\frac{\\delta}{M} x_{22}$. Substituting this into the first equation:\n$$ \\delta \\left(-\\frac{\\delta}{M} x_{22}\\right) - M x_{22} = 1 \\implies \\left(-\\frac{\\delta^2}{M} - M\\right) x_{22} = 1 $$\n$$ -\\frac{\\delta^2 + M^2}{M} x_{22} = 1 \\implies x_{22} = -\\frac{M}{\\delta^2 + M^2} $$\nSubstituting back to find $x_{21}$:\n$$ x_{21} = -\\frac{\\delta}{M} \\left(-\\frac{M}{\\delta^2 + M^2}\\right) = \\frac{\\delta}{\\delta^2 + M^2} $$\nThe correct solution for the second row is $(x_{21}, x_{22}) = \\left(\\frac{\\delta}{\\delta^2 + M^2}, -\\frac{M}{\\delta^2 + M^2}\\right)$.\n\nThe Euclidean norm of this vector is:\n$$ \\|(x_{21}, x_{22})\\|_2 = \\sqrt{\\left(\\frac{\\delta}{\\delta^2 + M^2}\\right)^2 + \\left(-\\frac{M}{\\delta^2 + M^2}\\right)^2} = \\sqrt{\\frac{\\delta^2 + M^2}{(\\delta^2 + M^2)^2}} = \\frac{1}{\\sqrt{\\delta^2 + M^2}} $$\n\n4. Improvement factor:\nThe improvement factor is defined as the ratio of the naive norm to the block-aware norm.\n$$ \\text{Ratio} = \\frac{\\|(x_{21}^{\\mathrm{naive}}, x_{22}^{\\mathrm{naive}})\\|_2}{\\|(x_{21}, x_{22})\\|_2} = \\frac{1/\\delta}{1/\\sqrt{\\delta^2 + M^2}} = \\frac{\\sqrt{\\delta^2 + M^2}}{\\delta} $$\nFor the specific numerical values $\\delta = 10^{-6}$ and $M = 1$:\n$$ \\text{Ratio} = \\frac{\\sqrt{(10^{-6})^2 + 1^2}}{10^{-6}} = \\frac{\\sqrt{10^{-12} + 1}}{10^{-6}} $$\nThe value of $\\sqrt{1 + 10^{-12}}$ is extremely close to $1$. The calculation gives:\n$$ \\text{Ratio} \\approx \\frac{1.0000000000005}{10^{-6}} = 1000000.0000005 $$\nRounding this result to three significant figures yields $1.00 \\times 10^6$.",
            "answer": "$$\\boxed{1.00 \\times 10^{6}}$$"
        }
    ]
}