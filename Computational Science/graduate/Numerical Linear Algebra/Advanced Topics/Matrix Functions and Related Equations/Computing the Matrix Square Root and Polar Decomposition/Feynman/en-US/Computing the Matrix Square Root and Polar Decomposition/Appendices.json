{
    "hands_on_practices": [
        {
            "introduction": "The polar decomposition is a fundamental factorization that reveals the geometric action of a matrix as a rotation (or reflection) and a pure stretch. This exercise guides you through a direct computation for a simple $2 \\times 2$ case, starting from the definition of the symmetric positive definite factor $S = (A^{\\top} A)^{1/2}$ . By carrying out the explicit diagonalization and construction, you will gain a concrete intuition for how these abstract components are realized and interconnected.",
            "id": "3539529",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given by $A = \\begin{pmatrix} 0 & 2 \\\\ -1 & 0 \\end{pmatrix}$. Work from first principles and core definitions in numerical linear algebra.\n\n(a) Using the spectral theorem for symmetric matrices and the definition of the principal matrix square root, compute the unique symmetric positive definite (SPD) square root $S$ of $A^{\\top} A$ by explicit diagonalization. Then verify $S^{2} = A^{\\top} A$ and that $S$ is SPD.\n\n(b) Using the definition of the real polar decomposition of a nonsingular matrix, verify that there exists a unique orthogonal matrix $Q$ and an SPD matrix $S$ such that $A = Q S$, and use the result of part (a) to construct $Q$ explicitly. Verify that $Q^{\\top} Q = I$ and $A = Q S$.\n\n(c) Interpret $Q$ as a planar rotation. That is, find the unique angle $\\theta \\in (-\\pi, \\pi]$ in radians such that $Q = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}$. Provide the final angle $\\theta$ as a single exact expression. The angle unit must be radians.",
            "solution": "The problem asks for the computation of the symmetric positive definite (SPD) square root of $A^{\\top} A$, followed by the polar decomposition of a given matrix $A$, and finally the interpretation of the orthogonal factor as a rotation.\n\nLet the matrix $A \\in \\mathbb{R}^{2 \\times 2}$ be defined as\n$$\nA = \\begin{pmatrix} 0 & 2 \\\\ -1 & 0 \\end{pmatrix}\n$$\n\n(a) We compute the unique symmetric positive definite (SPD) square root $S$ of $A^{\\top} A$.\nFirst, we compute the matrix product $A^{\\top} A$. The transpose of $A$ is\n$$\nA^{\\top} = \\begin{pmatrix} 0 & -1 \\\\ 2 & 0 \\end{pmatrix}\n$$\nNow, we compute the product $A^{\\top} A$:\n$$\nA^{\\top} A = \\begin{pmatrix} 0 & -1 \\\\ 2 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 2 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} (0)(0) + (-1)(-1) & (0)(2) + (-1)(0) \\\\ (2)(0) + (0)(-1) & (2)(2) + (0)(0) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}\n$$\nLet $M = A^{\\top} A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}$. $M$ is a real symmetric matrix. To find its principal square root $S$, we first perform a spectral decomposition $M = V D V^{\\top}$, where $V$ is an orthogonal matrix of eigenvectors and $D$ is a diagonal matrix of eigenvalues.\n\nSince $M$ is already a diagonal matrix, its eigenvalues are the diagonal entries, $\\lambda_1 = 1$ and $\\lambda_2 = 4$. The corresponding normalized eigenvectors are the standard basis vectors, $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe matrix of eigenvectors $V$ is the identity matrix $I$:\n$$\nV = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe diagonal matrix of eigenvalues $D$ is $M$ itself:\n$$\nD = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}\n$$\nThe decomposition is $M = I D I^{\\top} = D$.\n\nThe principal square root $S$ of an SPD matrix $M$ is defined as $S = V D^{1/2} V^{\\top}$, where $D^{1/2}$ is the diagonal matrix whose entries are the positive square roots of the eigenvalues of $M$.\n$$\nD^{1/2} = \\begin{pmatrix} \\sqrt{\\lambda_1} & 0 \\\\ 0 & \\sqrt{\\lambda_2} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{1} & 0 \\\\ 0 & \\sqrt{4} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\nTherefore, $S$ is given by:\n$$\nS = V D^{1/2} V^{\\top} = I D^{1/2} I^{\\top} = D^{1/2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\nNow, we verify that $S^2 = A^{\\top} A$ and that $S$ is SPD.\nFirst, the square of $S$:\n$$\nS^2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 1^2 & 0 \\\\ 0 & 2^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} = A^{\\top}A\n$$\nThe verification holds.\nSecond, we check if $S$ is SPD.\n1.  Symmetry: $S^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} = S$. Thus, $S$ is symmetric.\n2.  Positive Definiteness: The eigenvalues of $S$ are its diagonal entries, which are $1$ and $2$. Since all eigenvalues are strictly positive, $S$ is positive definite.\nThus, $S$ is indeed the unique SPD square root of $A^{\\top} A$.\n\n(b) We now find the unique polar decomposition $A = QS$, where $Q$ is orthogonal and $S$ is SPD. We use the matrix $S$ found in part (a).\nThe matrix $A$ is nonsingular since $\\det(A) = (0)(0) - (2)(-1) = 2 \\neq 0$. Consequently, $S = \\sqrt{A^{\\top} A}$ is also nonsingular (its eigenvalues $1, 2$ are nonzero), and the orthogonal matrix $Q$ is uniquely determined by $Q = A S^{-1}$.\nFirst, we find the inverse of $S$:\n$$\nS^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1^{-1} & 0 \\\\ 0 & 2^{-1} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}\n$$\nNow we compute $Q$:\n$$\nQ = A S^{-1} = \\begin{pmatrix} 0 & 2 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} (0)(1) + (2)(0) & (0)(0) + (2)(\\frac{1}{2}) \\\\ (-1)(1) + (0)(0) & (-1)(0) + (0)(\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}\n$$\nWe verify that $Q$ is an orthogonal matrix, i.e., $Q^{\\top}Q = I$.\n$$\nQ^{\\top} = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n$$\nQ^{\\top}Q = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} (0)(0)+(-1)(-1) & (0)(1)+(-1)(0) \\\\ (1)(0)+(0)(-1) & (1)(1)+(0)(0) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I\n$$\nThe verification holds.\nFinally, we verify that $A = QS$:\n$$\nQS = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} (0)(1) + (1)(0) & (0)(0) + (1)(2) \\\\ (-1)(1) + (0)(0) & (-1)(0) + (0)(2) \\end{pmatrix} = \\begin{pmatrix} 0 & 2 \\\\ -1 & 0 \\end{pmatrix} = A\n$$\nThe verification holds.\n\n(c) We interpret $Q$ as a planar rotation and find the unique angle $\\theta \\in (-\\pi, \\pi]$.\nA general $2 \\times 2$ rotation matrix has the form:\n$$\nR(\\theta) = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n$$\nWe have found that $Q = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$. Note that $\\det(Q) = (0)(0) - (1)(-1) = 1$, so $Q$ is a special orthogonal matrix, which corresponds to a pure rotation.\nWe equate $Q$ with the general rotation matrix form to find $\\theta$:\n$$\n\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n$$\nThis gives the system of equations:\n1. $\\cos(\\theta) = 0$\n2. $\\sin(\\theta) = -1$\n3. $-\\sin(\\theta) = 1$, which is equivalent to the second equation.\n\nFrom $\\cos(\\theta) = 0$, the solutions for $\\theta$ are of the form $\\theta = \\frac{\\pi}{2} + k\\pi$ for any integer $k$.\nFrom $\\sin(\\theta) = -1$, the solutions for $\\theta$ are of the form $\\theta = -\\frac{\\pi}{2} + 2m\\pi$ for any integer $m$.\n\nWe need to find a value of $\\theta$ that satisfies both conditions simultaneously and lies in the interval $(-\\pi, \\pi]$. The only solution that satisfies both trigonometric equations is $\\theta = -\\frac{\\pi}{2}$.\nLet's check if this value lies in the required interval: $-\\pi < -\\frac{\\pi}{2} \\le \\pi$. This is true.\nThus, the unique angle is $\\theta = -\\frac{\\pi}{2}$ radians.",
            "answer": "$$\\boxed{-\\frac{\\pi}{2}}$$"
        },
        {
            "introduction": "While computing a matrix function via diagonalization is elegant in theory, it can be numerically treacherous for non-normal matrices. This practice demonstrates the critical distinction between backward stability (the algorithm solves a nearby problem) and forward stability (the algorithm gives a solution close to the true one) . By constructing a matrix with ill-conditioned eigenvectors, you will quantify how non-normality can dramatically amplify errors, leading to a computed square root that is far from the true solution despite having a tiny backward error.",
            "id": "3539534",
            "problem": "You are asked to construct a concrete family of matrices and a computation procedure that demonstrates a fundamental discrepancy between backward stability and forward stability for the matrix square root in spectral norm. The goal is to show that a method based on eigen-decomposition can be backward stable while being forward unstable due to non-normality.\n\nStart from the following definitions and facts.\n\n1. For any complex matrix $A \\in \\mathbb{C}^{n \\times n}$ with no eigenvalues on the closed negative real axis, the principal matrix square root $A^{1/2}$ is defined by the primary matrix function associated with the scalar square root and agrees with $V \\,\\mathrm{diag}(\\sqrt{\\lambda_1},\\ldots,\\sqrt{\\lambda_n})\\,V^{-1}$ when $A$ is diagonalizable as $A=V \\,\\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_n)\\,V^{-1}$ with all $\\lambda_i$ in the open right-half plane.\n\n2. The spectral norm of $A$ is $\\|A\\|_2 := \\max_{\\|x\\|_2=1} \\|Ax\\|_2$.\n\n3. The forward error for a computed square root $X$ is the relative spectral-norm error $\\|X - A^{1/2}\\|_2 / \\|A^{1/2}\\|_2$. The backward error is the relative perturbation $\\|X^2 - A\\|_2 / \\|A\\|_2$.\n\n4. For a diagonalizable $A=V D V^{-1}$, the sensitivity of the matrix function $f(A)$ is governed by the Fréchet derivative $L_f(A,\\cdot)$, which for analytic $f$ exhibits an amplification bounded in terms of the condition number $\\kappa(V) := \\|V\\|_2 \\,\\|V^{-1}\\|_2$ and the scalar divided differences of $f$. In particular, non-normality (i.e., $V$ far from unitary) can dramatically worsen forward sensitivity even if the backward perturbation is tiny.\n\nYou must implement the following construction and measurement procedure.\n\n- Define, for fixed diagonal $D = \\mathrm{diag}(\\lambda_1,\\lambda_2)$ with $\\lambda_1 = 1$ and $\\lambda_2 = 4$, and for a chosen invertible matrix $V \\in \\mathbb{R}^{2 \\times 2}$, the matrix $A := V D V^{-1}$.\n\n- Emulate a backward-stable eigen-decomposition based computation of $A^{1/2}$ as follows: pick a diagonal perturbation $\\Delta D = \\mathrm{diag}(\\delta_1,\\delta_2)$ with small entries and form\n$$\nX := V \\,\\mathrm{diag}\\!\\big(\\sqrt{\\lambda_1 + \\delta_1}, \\sqrt{\\lambda_2 + \\delta_2}\\big)\\, V^{-1}.\n$$\nBy construction, $X^2 = V \\,\\mathrm{diag}(\\lambda_1 + \\delta_1, \\lambda_2 + \\delta_2)\\, V^{-1} = A + E$ with $E := V \\Delta D V^{-1}$. Thus $X$ is an exact square root of a nearby matrix $A+E$, which formalizes backward stability. The forward error is measured against the principal square root $A^{1/2} = V \\,\\mathrm{diag}(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2})\\, V^{-1}$.\n\n- Quantify the ratio $r := \\dfrac{\\|X - A^{1/2}\\|_2 / \\|A^{1/2}\\|_2}{\\|X^2 - A\\|_2 / \\|A\\|_2}$ in spectral norm. A large value of $r$ demonstrates forward instability relative to backward error.\n\nImplement a program that computes, for the following test suite, the value of $r$:\n\n- Test $\\mathsf{T}_1$ (normal, well-conditioned “happy path”): let $V=Q(\\alpha)$ be the rotation matrix $Q(\\alpha) = \\begin{bmatrix}\\cos(\\alpha) & -\\sin(\\alpha)\\\\ \\sin(\\alpha) & \\cos(\\alpha)\\end{bmatrix}$ with $\\alpha = 1$ (in radians), $D = \\mathrm{diag}(1,4)$, and $\\Delta D = \\mathrm{diag}(\\delta,-\\delta)$ with $\\delta = 10^{-12}$.\n\n- Test $\\mathsf{T}_2$ (moderately non-normal, ill-conditioned): let $V = \\begin{bmatrix}1 & 1\\\\ 0 & \\varepsilon\\end{bmatrix}$ with $\\varepsilon = 10^{-3}$, $D = \\mathrm{diag}(1,4)$, and $\\Delta D = \\mathrm{diag}(\\delta,-\\delta)$ with $\\delta = 10^{-12}$.\n\n- Test $\\mathsf{T}_3$ (highly non-normal, more ill-conditioned): same construction as $\\mathsf{T}_2$ but with $\\varepsilon = 10^{-6}$.\n\n- Test $\\mathsf{T}_4$ (edge case: exact non-principal square root illustrating zero backward error): same $V$ as in $\\mathsf{T}_2$ with $\\varepsilon = 10^{-3}$ and $D = \\mathrm{diag}(1,4)$, but define $X := V \\,\\mathrm{diag}(-1,2)\\, V^{-1}$, so that $X^2 = V \\,\\mathrm{diag}(1,4)\\, V^{-1} = A$ exactly. In this case, take the ratio $r$ to be $+\\infty$ if the computed backward error numerator is $0$; otherwise compute $r$ in the same way.\n\nFor each test, compute the spectral norm using the $2$-norm. Angles must be in radians. No physical units are involved. The outputs are real numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\mathsf{r}_{1},\\mathsf{r}_{2},\\mathsf{r}_{3},\\mathsf{r}_{4}]$. Each $\\mathsf{r}_{k}$ must be a floating-point number (allowing $+\\infty$ when applicable).",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and internally consistent. It presents a standard construction from numerical linear algebra to explore the relationship between forward and backward stability for the matrix square root function. All provided data and definitions are formal and unambiguous. The problem is solvable as stated.\n\nThe core of the problem is to compute the ratio $r$ for four test cases, where $r$ is defined as the ratio of the relative forward error to the relative backward error:\n$$\nr := \\frac{\\text{relative forward error}}{\\text{relative backward error}} = \\dfrac{\\|X - A^{1/2}\\|_2 / \\|A^{1/2}\\|_2}{\\|X^2 - A\\|_2 / \\|A\\|_2}\n$$\nHere, $A$ is the original matrix, $A^{1/2}$ is its principal square root, and $X$ is a computed approximation of the square root. The norm is the spectral norm, $\\|\\cdot\\|_2$.\n\nFor each test case, we construct the matrices and compute the required norms.\n\n### Test $\\mathsf{T}_1$: Normal Case\nIn this case, the matrix $V$ is the rotation matrix $Q(\\alpha)$ with $\\alpha = 1$.\n$$\nV = Q(1) = \\begin{bmatrix}\\cos(1) & -\\sin(1)\\\\ \\sin(1) & \\cos(1)\\end{bmatrix}\n$$\nThis is an orthogonal matrix, meaning $V^{-1} = V^T$, and for any matrix $M$, $\\|VMV^{-1}\\|_2 = \\|M\\|_2$. The condition number $\\kappa(V) = \\|V\\|_2\\|V^{-1}\\|_2 = 1$. The matrix $A = VDV^{-1}$ is normal.\n\nThe given matrices are $D = \\mathrm{diag}(1, 4)$ and $\\Delta D = \\mathrm{diag}(\\delta, -\\delta)$ with $\\delta = 10^{-12}$.\nThe required components for the ratio $r_1$ are:\n- $A = Q(1) D Q(1)^T$, so $\\|A\\|_2 = \\|D\\|_2 = \\max(|1|,|4|) = 4$.\n- $A^{1/2} = Q(1) D^{1/2} Q(1)^T$, with $D^{1/2} = \\mathrm{diag}(\\sqrt{1}, \\sqrt{4}) = \\mathrm{diag}(1, 2)$. Thus, $\\|A^{1/2}\\|_2 = \\|D^{1/2}\\|_2 = 2$.\n- The computed square root is $X = Q(1) (\\mathrm{diag}(\\sqrt{1+\\delta}, \\sqrt{4-\\delta})) Q(1)^T$.\n- The forward error numerator is $\\|X - A^{1/2}\\|_2 = \\|\\mathrm{diag}(\\sqrt{1+\\delta}-1, \\sqrt{4-\\delta}-2)\\|_2$.\nFor small $\\delta$, $\\sqrt{1+\\delta}-1 \\approx \\frac{\\delta}{2}$ and $\\sqrt{4-\\delta}-2 = 2\\sqrt{1-\\delta/4}-2 \\approx 2(1-\\frac{\\delta}{8})-2 = -\\frac{\\delta}{4}$.\nThe norm is $\\max(|\\sqrt{1+\\delta}-1|, |\\sqrt{4-\\delta}-2|) = \\sqrt{1+\\delta}-1 \\approx \\frac{\\delta}{2}$.\n- The backward error numerator is $\\|X^2 - A\\|_2 = \\|Q(1) \\Delta D Q(1)^T\\|_2 = \\|\\Delta D\\|_2 = \\max(|\\delta|, |-\\delta|) = \\delta$.\n\nThe ratio $r_1$ is:\n$$\nr_1 = \\frac{(\\|\\mathrm{diag}(\\sqrt{1+\\delta}-1, \\sqrt{4-\\delta}-2)\\|_2) / 2}{\\delta / 4} = 2 \\frac{\\sqrt{1+\\delta}-1}{\\delta}\n$$\nUsing the Taylor expansion $\\sqrt{1+\\delta} = 1 + \\frac{\\delta}{2} - \\frac{\\delta^2}{8} + O(\\delta^3)$, we have $\\sqrt{1+\\delta}-1 = \\frac{\\delta}{2} - \\frac{\\delta^2}{8} + \\dots$.\n$$\nr_1 = 2 \\frac{\\frac{\\delta}{2} - \\frac{\\delta^2}{8} + \\dots}{\\delta} = 1 - \\frac{\\delta}{4} + O(\\delta^2)\n$$\nFor $\\delta=10^{-12}$, $r_1$ is negligibly different from $1$.\n\n### Tests $\\mathsf{T}_2$ and $\\mathsf{T}_3$: Non-Normal Cases\nFor these tests, the matrix $V$ is given by $V = \\begin{bmatrix}1 & 1\\\\ 0 & \\varepsilon\\end{bmatrix}$, with $\\varepsilon = 10^{-3}$ for $\\mathsf{T}_2$ and $\\varepsilon = 10^{-6}$ for $\\mathsf{T}_3$. This matrix is non-normal and has a large condition number $\\kappa(V) \\approx 2/\\varepsilon$. Its inverse is $V^{-1} = \\begin{bmatrix}1 & -1/\\varepsilon\\\\ 0 & 1/\\varepsilon\\end{bmatrix}$.\n\nFor any diagonal matrix $M = \\mathrm{diag}(m_1, m_2)$, the transformed matrix is:\n$$\nK(M) = VMV^{-1} = \\begin{bmatrix}1 & 1\\\\ 0 & \\varepsilon\\end{bmatrix} \\begin{bmatrix}m_1 & 0\\\\ 0 & m_2\\end{bmatrix} \\begin{bmatrix}1 & -1/\\varepsilon\\\\ 0 & 1/\\varepsilon\\end{bmatrix} = \\begin{bmatrix}m_1 & (m_2-m_1)/\\varepsilon \\\\ 0 & m_2 \\end{bmatrix}\n$$\nThe spectral norm of such an upper triangular matrix, for small $\\varepsilon$, is dominated by the magnitude of the $(1,2)$ entry.\n$\\|K(M)\\|_2 \\approx |m_2-m_1|/\\varepsilon$.\n\nLet's apply this to the matrices in our ratio calculation:\n1.  $A = K(D) = K(\\mathrm{diag}(1, 4)) = \\begin{bmatrix}1 & 3/\\varepsilon \\\\ 0 & 4 \\end{bmatrix}$. $\\|A\\|_2 \\approx 3/\\varepsilon$.\n2.  $A^{1/2} = K(D^{1/2}) = K(\\mathrm{diag}(1, 2)) = \\begin{bmatrix}1 & 1/\\varepsilon \\\\ 0 & 2 \\end{bmatrix}$. $\\|A^{1/2}\\|_2 \\approx 1/\\varepsilon$.\n3.  $X-A^{1/2} = K(\\mathrm{diag}(\\sqrt{1+\\delta}-1, \\sqrt{4-\\delta}-2))$. The eigenvalues are $m_1 \\approx \\delta/2$ and $m_2 \\approx -\\delta/4$. Then $m_2-m_1 \\approx -\\frac{3\\delta}{4}$. So, $\\|X-A^{1/2}\\|_2 \\approx \\frac{3\\delta}{4\\varepsilon}$.\n4.  $X^2-A = K(\\Delta D) = K(\\mathrm{diag}(\\delta, -\\delta))$. The eigenvalues are $m_1 = \\delta$ and $m_2 = -\\delta$. Then $m_2-m_1 = -2\\delta$. So, $\\|X^2-A\\|_2 \\approx \\frac{2\\delta}{\\varepsilon}$.\n\nThe ratio $r$ is then approximately:\n$$\nr \\approx \\dfrac{ (\\frac{3\\delta}{4\\varepsilon}) / (\\frac{1}{\\varepsilon}) }{ (\\frac{2\\delta}{\\varepsilon}) / (\\frac{3}{\\varepsilon}) } = \\dfrac{3\\delta/4}{2\\delta/3} = \\frac{9}{8} = 1.125\n$$\nThis approximation suggests that the ratio $r$ is independent of $\\varepsilon$. This is because the perturbation $E=X^2-A=V\\Delta D V^{-1}$ commutes with $A=VDV^{-1}$, a special case where the ill-conditioning of the eigenvector matrix $V$ does not amplify the forward error. A more detailed calculation of the norms confirms that any dependence on $\\varepsilon$ is in very small higher-order terms. Thus, the ratio will be practically the same for both $\\mathsf{T}_2$ and $\\mathsf{T}_3$.\n\n### Test $\\mathsf{T}_4$: Non-Principal Square Root\nHere, $V$ is the same as in $\\mathsf{T}_2$ ($\\varepsilon = 10^{-3}$) and $D=\\mathrm{diag}(1,4)$, so $A$ is the same. The square root approximation is defined as $X = V \\,\\mathrm{diag}(-1, 2)\\, V^{-1}$.\nWe compute $X^2$:\n$$\nX^2 = \\left(V \\begin{bmatrix}-1 & 0\\\\0 & 2\\end{bmatrix} V^{-1}\\right)^2 = V \\left(\\begin{bmatrix}-1 & 0\\\\0 & 2\\end{bmatrix}\\right)^2 V^{-1} = V \\begin{bmatrix}(-1)^2 & 0\\\\0 & 2^2\\end{bmatrix} V^{-1} = V \\begin{bmatrix}1 & 0\\\\0 & 4\\end{bmatrix} V^{-1} = A\n$$\nThe backward error numerator is $\\|X^2 - A\\|_2 = \\|A - A\\|_2 = 0$.\nThe problem specifies that if the backward error numerator is zero, the ratio $r_4$ should be taken as $+\\infty$. This is because the forward error numerator, $\\|X - A^{1/2}\\|_2$, is non-zero (since $X$ is not the principal square root $A^{1/2}$), leading to a division by zero in the denominator of $r_4$.\n\nThe final results will be computed numerically to full precision.\n- $r_1 \\approx 1.0$\n- $r_2 \\approx 1.125$\n- $r_3 \\approx 1.125$\n- $r_4 = +\\infty$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of relative forward error to relative backward error\n    for the matrix square root under four different test scenarios.\n    \"\"\"\n    \n    # Define test case parameters\n    test_cases = [\n        # T1: Normal case (V is orthogonal)\n        {'type': 'normal', 'V_param': 1.0, 'delta': 1e-12},\n        # T2: Moderately non-normal\n        {'type': 'non-normal', 'V_param': 1e-3, 'delta': 1e-12},\n        # T3: Highly non-normal\n        {'type': 'non-normal', 'V_param': 1e-6, 'delta': 1e-12},\n        # T4: Exact non-principal square root\n        {'type': 'non-principal', 'V_param': 1e-3}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- Shared matrices ---\n        D = np.diag([1.0, 4.0])\n        D_sqrt = np.diag([np.sqrt(1.0), np.sqrt(4.0)])\n\n        if case['type'] == 'normal':\n            alpha = case['V_param']\n            V = np.array([\n                [np.cos(alpha), -np.sin(alpha)],\n                [np.sin(alpha),  np.cos(alpha)]\n            ])\n            delta = case['delta']\n            Delta_D = np.diag([delta, -delta])\n\n            V_inv = np.linalg.inv(V) # For orthogonal, inv(V) is V.T, but this is general\n            A = V @ D @ V_inv\n            A_sqrt = V @ D_sqrt @ V_inv\n\n            # Construct the computed square root X\n            D_perturbed = D + Delta_D\n            D_perturbed_sqrt = np.diag([np.sqrt(D_perturbed[0, 0]), np.sqrt(D_perturbed[1, 1])])\n            X = V @ D_perturbed_sqrt @ V_inv\n            \n            # Numerator of backward error\n            back_err_num = np.linalg.norm(X @ X - A, 2)\n            \n        elif case['type'] == 'non-normal':\n            eps = case['V_param']\n            V = np.array([\n                [1.0, 1.0],\n                [0.0, eps]\n            ])\n            delta = case['delta']\n            Delta_D = np.diag([delta, -delta])\n            \n            V_inv = np.linalg.inv(V)\n            A = V @ D @ V_inv\n            A_sqrt = V @ D_sqrt @ V_inv\n\n            # Construct the computed square root X\n            D_perturbed = D + Delta_D\n            D_perturbed_sqrt = np.diag([np.sqrt(D_perturbed[0, 0]), np.sqrt(D_perturbed[1, 1])])\n            X = V @ D_perturbed_sqrt @ V_inv\n\n            # Numerator of backward error\n            back_err_num = np.linalg.norm(X @ X - A, 2)\n\n        elif case['type'] == 'non-principal':\n            eps = case['V_param']\n            V = np.array([\n                [1.0, 1.0],\n                [0.0, eps]\n            ])\n            \n            V_inv = np.linalg.inv(V)\n            A = V @ D @ V_inv\n            A_sqrt = V @ D_sqrt @ V_inv # Principal root for comparison\n            \n            # Construct the non-principal square root X\n            X_D = np.diag([-1.0, 2.0])\n            X = V @ X_D @ V_inv\n            \n            # Numerator of backward error\n            back_err_num = np.linalg.norm(X @ X - A, 2)\n            \n            # Per problem spec, if backward error is zero, ratio is infinity\n            if np.isclose(back_err_num, 0.0):\n                results.append(np.inf)\n                continue\n        \n        # Calculate norms for the ratio\n        norm_A = np.linalg.norm(A, 2)\n        norm_A_sqrt = np.linalg.norm(A_sqrt, 2)\n        fwd_err_num = np.linalg.norm(X - A_sqrt, 2)\n\n        # Calculate relative errors\n        relative_forward_error = fwd_err_num / norm_A_sqrt\n        relative_backward_error = back_err_num / norm_A\n\n        # Calculate the final ratio\n        ratio = relative_forward_error / relative_backward_error\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Schur decomposition provides a more stable foundation for computing matrix functions than the eigendecomposition, but it is not without its own numerical pitfalls. This exercise exposes a subtle instability in the recursive algorithm for the square root of a triangular matrix when eigenvalues lie near the negative real axis . By analyzing how the magnitude of off-diagonal elements depends on eigenvalue ordering, you will understand why reordering the Schur form to separate problematic eigenvalues is a critical step for robust software.",
            "id": "3539525",
            "problem": "Consider a full-rank complex matrix $A \\in \\mathbb{C}^{3 \\times 3}$ and the Schur-based computation of the principal matrix square root of the Hermitian factor in the polar decomposition, that is, the principal square root of $A^{*} A$. Let $Q \\in \\mathbb{C}^{3 \\times 3}$ be unitary and $T \\in \\mathbb{C}^{3 \\times 3}$ be upper triangular such that $Q^{*} (A^{*} A) Q = T$ (the complex Schur form), and suppose we seek the principal upper triangular square root $S$ that satisfies $S^{2} = T$ and whose diagonal entries are the principal scalar square roots of the diagonal of $T$. Recall that the principal scalar square root of a complex number $z = r \\exp(i \\theta)$ with $r > 0$ and $-\\pi < \\theta \\le \\pi$ is $\\sqrt{r} \\exp(i \\theta/2)$.\n\nConstruct and analyze the following two triangular Schur forms, which differ only by a reordering of eigenvalues:\n$$\nT_{\\mathrm{bad}}(r,\\delta) = \\begin{pmatrix}\nr \\exp\\!\\big(i(\\pi - \\delta)\\big) & 1 & 0 \\\\\n0 & r \\exp\\!\\big(-i(\\pi - \\delta)\\big) & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\n\\quad\nT_{\\mathrm{good}}(r,\\delta) = \\begin{pmatrix}\nr \\exp\\!\\big(i(\\pi - \\delta)\\big) & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & r \\exp\\!\\big(-i(\\pi - \\delta)\\big)\n\\end{pmatrix},\n$$\nwhere $r > 0$ and $0  \\delta \\ll 1$. For each $T_{\\mathrm{bad}}(r,\\delta)$ and $T_{\\mathrm{good}}(r,\\delta)$, determine the $(1,2)$ entry of the principal triangular square root $S$ by starting from the definition $S^{2} = T$ and equating entries. Use these derivations to obtain an explicit closed-form expression for the ratio\n$$\nR(r,\\delta) = \\frac{\\big|s_{12}(T_{\\mathrm{bad}}(r,\\delta))\\big|}{\\big|s_{12}(T_{\\mathrm{good}}(r,\\delta))\\big|}.\n$$\nThen evaluate $R(r,\\delta)$ at $r = 1$ and $\\delta = 10^{-8}$ and round your final numerical answer to four significant figures. Explain, based on your derivation, why the reordering from $T_{\\mathrm{bad}}$ to $T_{\\mathrm{good}}$ improves numerical stability in the Schur-based square root computation when eigenvalues lie near the negative real axis.",
            "solution": "The problem requires the analysis of the computation of the $(1,2)$ entry of the principal upper triangular square root $S$ for two given upper triangular matrices, $T_{\\mathrm{bad}}(r,\\delta)$ and $T_{\\mathrm{good}}(r,\\delta)$. The principal square root $S$ of an upper triangular matrix $T$ is itself upper triangular, satisfies $S^2 = T$, and has diagonal entries $s_{ii}$ that are the principal scalar square roots of the diagonal entries $t_{ii}$ of $T$.\n\nLet $S$ be an upper triangular matrix. Squaring $S$ gives:\n$$\nS^2 = \\begin{pmatrix} s_{11}  s_{12}  s_{13} \\\\ 0  s_{22}  s_{23} \\\\ 0  0  s_{33} \\end{pmatrix}^2 = \\begin{pmatrix} s_{11}^2  s_{12}(s_{11}+s_{22})  s_{11}s_{13} + s_{12}s_{23} + s_{13}s_{33} \\\\ 0  s_{22}^2  s_{23}(s_{22}+s_{33}) \\\\ 0  0  s_{33}^2 \\end{pmatrix}\n$$\nEquating this with $T = (t_{ij})$ gives a system of equations. For the diagonal entries, $s_{ii}^2 = t_{ii}$, which implies $s_{ii}$ is a square root of $t_{ii}$. By the problem's definition, we take the principal square root. For the off-diagonal entries, we can solve for $s_{ij}$ with $ij$ using the Bartels-Stewart algorithm, which for the $(i, j)$ entry is given by:\n$$ s_{ij} (s_{ii} + s_{jj}) = t_{ij} - \\sum_{k=i+1}^{j-1} s_{ik} s_{kj} $$\nFor the specific entry $s_{12}$, this simplifies to:\n$$ s_{12} = \\frac{t_{12}}{s_{11} + s_{22}} $$\n\nWe will now apply this to $T_{\\mathrm{bad}}$ and $T_{\\mathrm{good}}$.\n\nFirst, we analyze $T_{\\mathrm{bad}}(r,\\delta)$:\n$$\nT_{\\mathrm{bad}}(r,\\delta) = \\begin{pmatrix}\nr \\exp\\!\\big(i(\\pi - \\delta)\\big)  1  0 \\\\\n0  r \\exp\\!\\big(-i(\\pi - \\delta)\\big)  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nThe diagonal entries are $t_{11} = r \\exp(i(\\pi - \\delta))$, $t_{22} = r \\exp(-i(\\pi - \\delta))$, and $t_{33} = 1$. The corresponding diagonal entries of its principal square root, $S_{\\mathrm{bad}}$, are the principal scalar square roots:\n$s_{11} = \\sqrt{r} \\exp(i(\\pi - \\delta)/2) = \\sqrt{r} \\exp(i\\pi/2) \\exp(-i\\delta/2) = i\\sqrt{r} \\exp(-i\\delta/2)$.\n$s_{22} = \\sqrt{r} \\exp(-i(\\pi - \\delta)/2) = \\sqrt{r} \\exp(-i\\pi/2) \\exp(i\\delta/2) = -i\\sqrt{r} \\exp(i\\delta/2)$.\nThe entry $t_{12}$ is $1$. The $(1,2)$ entry of $S_{\\mathrm{bad}}$, denoted $s_{12}(T_{\\mathrm{bad}})$, is:\n$$ s_{12}(T_{\\mathrm{bad}}) = \\frac{t_{12}}{s_{11} + s_{22}} = \\frac{1}{i\\sqrt{r} \\exp(-i\\delta/2) - i\\sqrt{r} \\exp(i\\delta/2)} $$\nUsing Euler's formula, $\\exp(ix) = \\cos(x) + i\\sin(x)$, the denominator becomes:\n$$ i\\sqrt{r} \\left( (\\cos(-\\delta/2) + i\\sin(-\\delta/2)) - (\\cos(\\delta/2) + i\\sin(\\delta/2)) \\right) = i\\sqrt{r} \\left( \\cos(\\delta/2) - i\\sin(\\delta/2) - \\cos(\\delta/2) - i\\sin(\\delta/2) \\right) $$\n$$ = i\\sqrt{r} (-2i\\sin(\\delta/2)) = 2\\sqrt{r}\\sin(\\delta/2) $$\nSo, $s_{12}(T_{\\mathrm{bad}}) = \\frac{1}{2\\sqrt{r}\\sin(\\delta/2)}$.\n\nNext, we analyze $T_{\\mathrm{good}}(r,\\delta)$:\n$$\nT_{\\mathrm{good}}(r,\\delta) = \\begin{pmatrix}\nr \\exp\\!\\big(i(\\pi - \\delta)\\big)  1  0 \\\\\n0  1  0 \\\\\n0  0  r \\exp\\!\\big(-i(\\pi - \\delta)\\big)\n\\end{pmatrix}\n$$\nThe diagonal entries are $t'_{11} = r \\exp(i(\\pi - \\delta))$, $t'_{22} = 1$, and $t'_{33} = r \\exp(-i(\\pi - \\delta))$. The corresponding diagonal entries of its principal square root, $S_{\\mathrm{good}}$, are:\n$s'_{11} = \\sqrt{r} \\exp(i(\\pi - \\delta)/2) = i\\sqrt{r} \\exp(-i\\delta/2)$.\n$s'_{22} = \\sqrt{1} = 1$.\nThe entry $t'_{12}$ is $1$. The $(1,2)$ entry of $S_{\\mathrm{good}}$, denoted $s_{12}(T_{\\mathrm{good}})$, is:\n$$ s_{12}(T_{\\mathrm{good}}) = \\frac{t'_{12}}{s'_{11} + s'_{22}} = \\frac{1}{i\\sqrt{r} \\exp(-i\\delta/2) + 1} $$\n$$ = \\frac{1}{1 + i\\sqrt{r}(\\cos(\\delta/2) - i\\sin(\\delta/2))} = \\frac{1}{(1 + \\sqrt{r}\\sin(\\delta/2)) + i\\sqrt{r}\\cos(\\delta/2)} $$\n\nNow we can compute the ratio $R(r,\\delta)$. We need the magnitudes of $s_{12}(T_{\\mathrm{bad}})$ and $s_{12}(T_{\\mathrm{good}})$.\nSince $r > 0$ and $0  \\delta \\ll 1$, $\\sin(\\delta/2) > 0$, so $|s_{12}(T_{\\mathrm{bad}})| = \\frac{1}{2\\sqrt{r}\\sin(\\delta/2)}$.\nFor the second case:\n$$ |s_{12}(T_{\\mathrm{good}})| = \\frac{1}{|(1 + \\sqrt{r}\\sin(\\delta/2)) + i\\sqrt{r}\\cos(\\delta/2)|} = \\frac{1}{\\sqrt{(1 + \\sqrt{r}\\sin(\\delta/2))^2 + (\\sqrt{r}\\cos(\\delta/2))^2}} $$\nThe term under the square root is:\n$$ (1 + 2\\sqrt{r}\\sin(\\delta/2) + r\\sin^2(\\delta/2)) + r\\cos^2(\\delta/2) = 1 + 2\\sqrt{r}\\sin(\\delta/2) + r(\\sin^2(\\delta/2) + \\cos^2(\\delta/2)) = 1 + r + 2\\sqrt{r}\\sin(\\delta/2) $$\nSo, $|s_{12}(T_{\\mathrm{good}})| = \\frac{1}{\\sqrt{1 + r + 2\\sqrt{r}\\sin(\\delta/2)}}$.\n\nThe ratio is:\n$$\nR(r,\\delta) = \\frac{|s_{12}(T_{\\mathrm{bad}}(r,\\delta))|}{|s_{12}(T_{\\mathrm{good}}(r,\\delta))|} = \\frac{1/(2\\sqrt{r}\\sin(\\delta/2))}{1/\\sqrt{1+r+2\\sqrt{r}\\sin(\\delta/2)}} = \\frac{\\sqrt{1+r+2\\sqrt{r}\\sin(\\delta/2)}}{2\\sqrt{r}\\sin(\\delta/2)}\n$$\nWe evaluate this at $r=1$ and $\\delta=10^{-8}$:\n$$ R(1, 10^{-8}) = \\frac{\\sqrt{1+1+2\\sqrt{1}\\sin(10^{-8}/2)}}{2\\sqrt{1}\\sin(10^{-8}/2)} = \\frac{\\sqrt{2+2\\sin(0.5 \\times 10^{-8})}}{2\\sin(0.5 \\times 10^{-8})} $$\nFor such a small angle $x = 0.5 \\times 10^{-8}$, the approximation $\\sin(x) \\approx x$ is highly accurate.\n$$\nR(1, 10^{-8}) \\approx \\frac{\\sqrt{2+2(0.5 \\times 10^{-8})}}{2(0.5 \\times 10^{-8})} = \\frac{\\sqrt{2+10^{-8}}}{10^{-8}} = \\frac{\\sqrt{2(1+0.5 \\times 10^{-8})}}{10^{-8}}\n$$\nThe term $\\sqrt{1+0.5 \\times 10^{-8}}$ is extremely close to $1$. The numerator is thus extremely close to $\\sqrt{2}$.\n$$\nR(1, 10^{-8}) \\approx \\frac{\\sqrt{2}}{10^{-8}} = \\sqrt{2} \\times 10^8 \\approx 1.41421356 \\times 10^8\n$$\nRounding to four significant figures, we get $1.414 \\times 10^8$.\n\nThe analysis of this ratio explains why the reordering from $T_{\\mathrm{bad}}$ to $T_{\\mathrm{good}}$ improves numerical stability. The computation of off-diagonal entries $s_{ij}$ of the matrix square root $S$ involves division by $s_{ii}+s_{jj} = \\sqrt{t_{ii}} + \\sqrt{t_{jj}}$. This sum can be very small, leading to numerical instability, if the principal square roots $\\sqrt{t_{ii}}$ and $\\sqrt{t_{jj}}$ are nearly opposite. This occurs when the eigenvalues $t_{ii}$ and $t_{jj}$ are close to each other and lie on the negative real axis.\n\nIn $T_{\\mathrm{bad}}$, the eigenvalues $t_{11} = r \\exp(i(\\pi - \\delta))$ and $t_{22} = r \\exp(-i(\\pi - \\delta))$ are adjacent on the diagonal. For small $\\delta$, both are very close to $-r$. Their principal square roots, $s_{11}$ and $s_{22}$, are close to $i\\sqrt{r}$ and $-i\\sqrt{r}$ respectively. Their sum, $s_{11}+s_{22} = 2\\sqrt{r}\\sin(\\delta/2)$, is approximately $\\sqrt{r}\\delta$, which is a very small number for small $\\delta$. Division by this small number results in an extremely large value for $s_{12}$, as demonstrated by our calculation of $R$. This large element can propagate and contaminate other elements of $S$ during the back-substitution process, leading to a loss of accuracy.\n\nIn $T_{\\mathrm{good}}$, the problematic eigenvalues are separated. The computation of $s_{12}$ now involves $t'_{11} = r \\exp(i(\\pi - \\delta))$ and $t'_{22}=1$. The denominator for $s'_{12}$ is $s'_{11}+s'_{22}$. The magnitude of this sum is $|s'_{11}+s'_{22}| = \\sqrt{1+r+2\\sqrt{r}\\sin(\\delta/2)}$, which for small $\\delta$ is approximately $\\sqrt{1+r}$. This is a value of order $1$, not a small number approaching zero. Division by this number is a numerically stable operation. By reordering the eigenvalues in the Schur form to keep pairs of eigenvalues from the vicinity of the negative real axis far apart in the ordering, we avoid small denominators and thus ensure the stability of the Schur-based matrix square root computation.",
            "answer": "$$ \\boxed{1.414 \\times 10^{8}} $$"
        }
    ]
}