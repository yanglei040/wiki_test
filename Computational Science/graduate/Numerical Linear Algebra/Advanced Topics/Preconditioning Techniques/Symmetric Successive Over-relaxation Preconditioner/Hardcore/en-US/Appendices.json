{
    "hands_on_practices": [
        {
            "introduction": "Understanding a numerical method begins with its fundamental properties and its response to challenging problems. This exercise  explores a generalized form of the SSOR preconditioner that uses different relaxation parameters for each row, a technique useful for anisotropic problems. You will first establish the necessary and sufficient conditions for this preconditioner to be symmetric positive definite (SPD), and then analyze its performance on a simple $2 \\times 2$ model to see how parameter choice can optimize convergence for anisotropic systems.",
            "id": "3583756",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite (SPD), with the standard splitting $A = D - L - L^{\\top}$, where $D$ is diagonal with positive entries and $-L$ is the strictly lower triangular part. Consider the variable-relaxation symmetric successive over-relaxation (SSOR) preconditioner\n$$\nM \\;=\\; \\frac{1}{2 - \\bar{\\omega}} \\,\\bigl(D + \\Omega L\\bigr)\\,D^{-1}\\,\\bigl(D + \\Omega L\\bigr)^{\\top},\n$$\nwhere $\\Omega = \\operatorname{diag}(\\omega_{1},\\dots,\\omega_{n})$ collects per-row relaxation parameters and $\\bar{\\omega}$ is the arithmetic mean $\\bar{\\omega} = \\tfrac{1}{n}\\sum_{i=1}^{n} \\omega_{i}$.\n\n1. Starting from the structural properties of $D$ and $L$ stated above, derive necessary and sufficient conditions on $\\bar{\\omega}$ that ensure $M$ is SPD. Justify each step from first principles and do not assume properties beyond the strict lower triangularity of $L$ and positivity of the diagonal of $D$.\n\n2. To analyze anisotropy, use the $2 \\times 2$ model\n$$\nA \\;=\\; \\begin{pmatrix} d_{1} & -\\ell \\\\ -\\ell & d_{2} \\end{pmatrix},\n$$\nwith $d_{1} > 0$, $d_{2} > 0$, and $0 < \\ell^{2} < d_{1} d_{2}$. With the splitting $A=D-L-L^\\top$, we have $D = \\operatorname{diag}(d_{1}, d_{2})$ and $L = \\begin{pmatrix} 0 & 0 \\\\ \\ell & 0 \\end{pmatrix}$. Choose $\\Omega = \\operatorname{diag}(\\omega_{1}, \\omega_{2})$ with $\\bar{\\omega} = (\\omega_{1} + \\omega_{2})/2$. Express the eigenvalues of the preconditioned operator $M^{-1}A$ as the generalized eigenvalues of the pair $(A, C)$ with $C = (D + \\Omega L)D^{-1}(D + \\Omega L)^{\\top}$, and show that for this model they are the roots of a quadratic polynomial whose coefficients depend on the dimensionless anisotropy parameter $\\theta = \\ell^{2}/(d_{1} d_{2}) \\in (0,1)$ and on $\\omega_{2}$.\n\nOver the admissible range $0 \\leq \\omega_{2} < 2$, determine the choice of $\\omega_{2}$ that minimizes the $2$-norm condition number $\\kappa_{2}(M^{-1}A)$ and give the resulting minimal condition number as a closed-form analytic expression in $\\theta$. Provide your final answer only as that single closed-form expression. No rounding is required.",
            "solution": "The user has provided a two-part problem concerning the properties of a variable-relaxation symmetric successive over-relaxation (SSOR) preconditioner.\n\n### Part 1: Condition for the Preconditioner to be Symmetric Positive Definite (SPD)\n\nThe problem defines the preconditioner $M$ as\n$$\nM = \\frac{1}{2 - \\bar{\\omega}} (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top}\n$$\nwhere $A = D - L - L^{\\top}$ is an SPD matrix, $D$ is a diagonal matrix with positive entries, $-L$ is the strictly lower triangular matrix, $\\Omega$ is a diagonal matrix of relaxation parameters $\\omega_i$, and $\\bar{\\omega}$ is the arithmetic mean of the $\\omega_i$. We need to find necessary and sufficient conditions on $\\bar{\\omega}$ for $M$ to be SPD.\n\nA matrix is SPD if it is symmetric and all its eigenvalues are positive, or equivalently, if it is symmetric and for any non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$, the quadratic form $\\mathbf{x}^{\\top} M \\mathbf{x}$ is positive.\n\nFirst, let's examine the symmetry of $M$. Let the scalar prefactor be $k = \\frac{1}{2 - \\bar{\\omega}}$ and the matrix part be $C = (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top}$. The matrix $M$ is symmetric if and only if $C$ is symmetric. Let's compute the transpose of $C$:\n$$\nC^{\\top} = \\left( (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top} \\right)^{\\top}\n$$\nUsing the property $(XYZ)^{\\top} = Z^{\\top}Y^{\\top}X^{\\top}$, we get\n$$\nC^{\\top} = \\left( (D + \\Omega L)^{\\top} \\right)^{\\top} (D^{-1})^{\\top} (D + \\Omega L)^{\\top} = (D + \\Omega L) (D^{-1})^{\\top} (D + \\Omega L)^{\\top}\n$$\nSince $D$ is a diagonal matrix, its inverse $D^{-1}$ is also diagonal and therefore symmetric, so $(D^{-1})^{\\top} = D^{-1}$. Substituting this back gives\n$$\nC^{\\top} = (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top} = C\n$$\nThus, the matrix $C$ is symmetric, which implies $M = kC$ is also symmetric for any scalar $k$.\n\nNext, we establish the condition for $M$ to be positive definite. For any non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$, we examine the quadratic form:\n$$\n\\mathbf{x}^{\\top} M \\mathbf{x} = \\mathbf{x}^{\\top} \\left( \\frac{1}{2 - \\bar{\\omega}} C \\right) \\mathbf{x} = \\frac{1}{2 - \\bar{\\omega}} \\mathbf{x}^{\\top} C \\mathbf{x}\n$$\nLet's analyze the term $\\mathbf{x}^{\\top} C \\mathbf{x}$:\n$$\n\\mathbf{x}^{\\top} C \\mathbf{x} = \\mathbf{x}^{\\top} (D + \\Omega L) D^{-1} (D + \\Omega L)^{\\top} \\mathbf{x}\n$$\nLet us define a vector $\\mathbf{y} = (D + \\Omega L)^{\\top} \\mathbf{x}$. The quadratic form becomes:\n$$\n\\mathbf{x}^{\\top} C \\mathbf{x} = \\mathbf{y}^{\\top} D^{-1} \\mathbf{y}\n$$\nThe problem states that $D$ is a diagonal matrix with positive entries, $d_{ii} > 0$. This means $D$ is a positive definite matrix. Consequently, its inverse $D^{-1}$ is also a diagonal matrix with positive entries $1/d_{ii} > 0$, and is therefore also positive definite. For any non-zero vector $\\mathbf{y}$, we must have $\\mathbf{y}^{\\top} D^{-1} \\mathbf{y} > 0$.\n\nNow we need to ensure that $\\mathbf{y}$ is non-zero whenever $\\mathbf{x}$ is non-zero. The vector $\\mathbf{y}$ is defined as $\\mathbf{y} = (D + \\Omega L)^{\\top} \\mathbf{x}$. The matrix $(D + \\Omega L)^{\\top}$ is $D^{\\top} + (\\Omega L)^{\\top} = D + L^{\\top}\\Omega^{\\top}$. Since $D$ is diagonal and $\\Omega$ is diagonal, they are symmetric ($D=D^\\top, \\Omega=\\Omega^\\top$), so this is $D + L^{\\top}\\Omega$. $D$ is diagonal and $L$ corresponds to the negative of the strictly lower part, so $L^T$ corresponds to the negative of the strictly upper part. The matrix $D + L^{\\top}\\Omega$ is therefore an upper triangular matrix. Its diagonal entries are the diagonal entries of $D$, which are all positive. The determinant of a triangular matrix is the product of its diagonal entries, so $\\det(D + L^{\\top}\\Omega) = \\prod_{i=1}^n d_{ii} > 0$. Since the determinant is non-zero, the matrix is invertible.\nThus, if $\\mathbf{x} \\neq \\mathbf{0}$, then $\\mathbf{y} = (D + \\Omega L)^{\\top} \\mathbf{x} \\neq \\mathbf{0}$.\n\nThis establishes that $\\mathbf{y}^{\\top} D^{-1} \\mathbf{y} > 0$ for all $\\mathbf{x} \\neq \\mathbf{0}$. Therefore, the matrix $C$ is positive definite.\n\nSince $M = \\frac{1}{2 - \\bar{\\omega}} C$ and $C$ is positive definite, the positive definiteness of $M$ depends solely on the sign of the scalar prefactor $\\frac{1}{2 - \\bar{\\omega}}$. For $M$ to be positive definite, this scalar must be positive:\n$$\n\\frac{1}{2 - \\bar{\\omega}} > 0\n$$\nThis inequality holds if and only if the denominator is positive:\n$$\n2 - \\bar{\\omega} > 0 \\quad\\implies\\quad \\bar{\\omega} < 2\n$$\nThis is the necessary and sufficient condition on $\\bar{\\omega}$ for $M$ to be SPD.\n\n### Part 2: Analysis of the 2x2 Model\n\nThe problem gives the $2 \\times 2$ matrix $A = \\begin{pmatrix} d_{1} & -\\ell \\\\ -\\ell & d_{2} \\end{pmatrix}$, with $d_{1} > 0$, $d_{2} > 0$, and $0 < \\ell^2 < d_{1} d_{2}$. The splitting $A=D-L-L^\\top$ gives $D = \\operatorname{diag}(d_{1}, d_{2})$ and $L = \\begin{pmatrix} 0 & 0 \\\\ \\ell & 0 \\end{pmatrix}$. The relaxation matrix is $\\Omega = \\operatorname{diag}(\\omega_{1}, \\omega_{2})$.\n\nWe first compute the matrix $C = (D + \\Omega L)D^{-1}(D + \\Omega L)^{\\top}$.\n$$\nD + \\Omega L = \\begin{pmatrix} d_{1} & 0 \\\\ 0 & d_{2} \\end{pmatrix} + \\begin{pmatrix} \\omega_{1} & 0 \\\\ 0 & \\omega_{2} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ \\ell & 0 \\end{pmatrix} = \\begin{pmatrix} d_{1} & 0 \\\\ 0 & d_{2} \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ \\omega_{2}\\ell & 0 \\end{pmatrix} = \\begin{pmatrix} d_{1} & 0 \\\\ \\omega_{2}\\ell & d_{2} \\end{pmatrix}\n$$\nNote that this matrix does not depend on $\\omega_1$. Now, we compute $C$:\n$$\n\\begin{align*} C &= \\begin{pmatrix} d_{1} & 0 \\\\ \\omega_{2}\\ell & d_{2} \\end{pmatrix} \\begin{pmatrix} 1/d_{1} & 0 \\\\ 0 & 1/d_{2} \\end{pmatrix} \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ 0 & d_{2} \\end{pmatrix} \\\\ &= \\begin{pmatrix} 1 & 0 \\\\ \\omega_{2}\\ell/d_{1} & 1 \\end{pmatrix} \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ 0 & d_{2} \\end{pmatrix} \\\\ &= \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & \\frac{(\\omega_{2}\\ell)^2}{d_{1}} + d_{2} \\end{pmatrix} = \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & d_{2}\\left(1 + \\frac{\\omega_{2}^2\\ell^2}{d_{1}d_{2}}\\right) \\end{pmatrix}\\end{align*}\n$$\nUsing the dimensionless parameter $\\theta = \\ell^2 / (d_{1}d_{2})$, we get\n$$\nC = \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & d_{2}(1 + \\omega_{2}^2\\theta) \\end{pmatrix}\n$$\nThe eigenvalues of the preconditioned operator $M^{-1}A$ are $\\lambda$. The eigenvalue problem is $A\\mathbf{x} = \\lambda M \\mathbf{x}$. Substituting the expression for $M$, we have $A\\mathbf{x} = \\lambda \\frac{1}{2 - \\bar{\\omega}} C \\mathbf{x}$. This can be written as $(2 - \\bar{\\omega}) A\\mathbf{x} = \\lambda C \\mathbf{x}$. The generalized eigenvalues $\\mu$ of the pair $(A, C)$ are solutions to $A\\mathbf{x} = \\mu C \\mathbf{x}$. Thus, the eigenvalues of $M^{-1}A$ are $\\lambda = (2 - \\bar{\\omega})\\mu$.\n\nThe condition number $\\kappa_{2}(M^{-1}A) = \\frac{\\lambda_{max}}{\\lambda_{min}} = \\frac{(2 - \\bar{\\omega})\\mu_{max}}{(2 - \\bar{\\omega})\\mu_{min}} = \\frac{\\mu_{max}}{\\mu_{min}}$, which is the condition number of the generalized eigenvalue problem for $(A, C)$. The characteristic polynomial for $\\mu$ is $\\det(A - \\mu C) = 0$.\n$$\n\\det\\left( \\begin{pmatrix} d_{1} & -\\ell \\\\ -\\ell & d_{2} \\end{pmatrix} - \\mu \\begin{pmatrix} d_{1} & \\omega_{2}\\ell \\\\ \\omega_{2}\\ell & d_{2}(1 + \\omega_{2}^2\\theta) \\end{pmatrix} \\right) = 0\n$$\n$$\n\\det\\begin{pmatrix} d_{1}(1-\\mu) & -\\ell - \\mu\\omega_{2}\\ell \\\\ -\\ell - \\mu\\omega_{2}\\ell & d_{2} - \\mu d_{2}(1 + \\omega_{2}^2\\theta) \\end{pmatrix} = 0\n$$\n$$\nd_{1}d_{2}(1-\\mu)(1 - \\mu(1 + \\omega_{2}^2\\theta)) - \\ell^2(1 + \\mu\\omega_{2})^2 = 0\n$$\nDividing by $d_{1}d_{2}$ and using $\\theta = \\ell^2/(d_{1}d_{2})$:\n$$\n(1-\\mu)(1 - \\mu - \\mu\\omega_{2}^2\\theta) - \\theta(1 + 2\\mu\\omega_{2} + \\mu^2\\omega_{2}^2) = 0\n$$\nExpanding and collecting terms in powers of $\\mu$:\n$$\n1 - \\mu - \\mu\\omega_{2}^2\\theta - \\mu + \\mu^2 + \\mu^2\\omega_{2}^2\\theta - \\theta - 2\\mu\\theta\\omega_{2} - \\mu^2\\theta\\omega_{2}^2 = 0\n$$\n$$\n\\mu^2(1 + \\omega_{2}^2\\theta - \\theta\\omega_{2}^2) - \\mu(2 + \\omega_{2}^2\\theta + 2\\theta\\omega_{2}) + (1-\\theta) = 0\n$$\n$$\n\\mu^2 - \\mu(2 + 2\\theta\\omega_{2} + \\theta\\omega_{2}^2) + (1-\\theta) = 0\n$$\nThis is the quadratic polynomial for the generalized eigenvalues $\\mu$. Let the roots be $\\mu_{1}$ and $\\mu_{2}$. The condition number to minimize is $\\kappa = \\mu_{max}/\\mu_{min}$.\nFrom the polynomial, the sum of the roots is $S = \\mu_{1} + \\mu_{2} = 2 + 2\\theta\\omega_{2} + \\theta\\omega_{2}^2$, and the product of the roots is $P = \\mu_{1}\\mu_{2} = 1-\\theta$. Since $\\theta \\in (0,1)$, $P > 0$. Since $S > 0$ for $\\omega_2 \\ge 0$, both roots are positive.\n\nThe condition number $\\kappa = \\mu_{max}/\\mu_{min}$ is minimized when the roots are as close as possible. We can express $\\kappa$ as a function of $S$ and $P$. Let $\\mu_{min}, \\mu_{max} = (S \\mp \\sqrt{S^2-4P})/2$. The ratio is $\\kappa = \\frac{S + \\sqrt{S^2-4P}}{S - \\sqrt{S^2-4P}}$.\nTo minimize $\\kappa$ (where $\\kappa \\ge 1$), we need to minimize the term $S = S(\\omega_2)$, since $P$ is constant with respect to $\\omega_2$.\nWe need to find the minimum of $S(\\omega_2) = 2 + 2\\theta\\omega_{2} + \\theta\\omega_{2}^2$ on the interval $0 \\le \\omega_2 < 2$.\nTo find the minimum, we compute the derivative with respect to $\\omega_2$:\n$$\n\\frac{dS}{d\\omega_2} = 2\\theta + 2\\theta\\omega_2 = 2\\theta(1+\\omega_2)\n$$\nGiven that $\\theta \\in (0,1)$ and $\\omega_2 \\ge 0$, the derivative $\\frac{dS}{d\\omega_2}$ is always positive. This means that $S(\\omega_2)$ is a strictly increasing function of $\\omega_2$ on its domain $[0, 2)$. The minimum value of $S(\\omega_2)$ is therefore attained at the lower boundary of the interval, i.e., at $\\omega_2 = 0$.\n\nTo find the minimal condition number, we set $\\omega_2 = 0$ in the characteristic polynomial for $\\mu$:\n$$\n\\mu^2 - 2\\mu + (1-\\theta) = 0\n$$\nUsing the quadratic formula, the roots are:\n$$\n\\mu = \\frac{2 \\pm \\sqrt{4 - 4(1-\\theta)}}{2} = 1 \\pm \\sqrt{1 - (1-\\theta)} = 1 \\pm \\sqrt{\\theta}\n$$\nSo, $\\mu_{max} = 1 + \\sqrt{\\theta}$ and $\\mu_{min} = 1 - \\sqrt{\\theta}$.\nThe minimal condition number is the ratio of these eigenvalues:\n$$\n\\kappa_{min} = \\frac{\\mu_{max}}{\\mu_{min}} = \\frac{1 + \\sqrt{\\theta}}{1 - \\sqrt{\\theta}}\n$$",
            "answer": "$$\\boxed{\\frac{1 + \\sqrt{\\theta}}{1 - \\sqrt{\\theta}}}$$"
        },
        {
            "introduction": "The practical value of any numerical tool is best understood by comparing it with its alternatives. In this exercise , the SSOR preconditioner is placed side-by-side with another cornerstone of iterative methods: the Incomplete Cholesky (IC(0)) factorization. By investigating the conditions under which these two preconditioners might be identical and analyzing their trade-offs between setup cost and iteration reduction, you will develop the critical judgment needed to select the appropriate tool for a given computational task.",
            "id": "3583768",
            "problem": "Consider a real symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ with bandwidth $p$, meaning $a_{ij} = 0$ whenever $\\lvert i - j \\rvert > p$ in a given natural ordering. Let the standard splitting be $A = D - L - L^{\\top}$ where $D$ is diagonal, and $-L$ is the strictly lower triangular part. The Symmetric Successive Over-Relaxation (SSOR) preconditioner with relaxation parameter $\\omega \\in (0,2)$ is defined by\n$$\nM_{\\mathrm{SSOR}}(\\omega) = (D - \\omega L) D^{-1} (D - \\omega L^{\\top}),\n$$\nand the Incomplete Cholesky factorization with zero fill (IC(0)) produces a lower-triangular factor $\\widetilde{L}$ that has the same sparsity pattern as the lower part of $A$ such that the preconditioner is $M_{\\mathrm{IC(0)}} = \\widetilde{L}\\,\\widetilde{L}^{\\top}$, obtained by performing Cholesky factorization while discarding any fill-in beyond the sparsity of $A$.\n\nSelect the correct statement regarding:\n(i) the sparsity patterns under which $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ become algebraically identical, and\n(ii) the regime in which SSOR dominates in setup cost versus iteration reduction when used, for example, with Preconditioned Conjugate Gradient (PCG).\n\nA. Under the natural ordering in which the lower and upper bandwidths are both $p$, $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ are algebraically identical if and only if $p = 0$ (equivalently, $L = 0$), in which case both reduce to the diagonal $D$. For $p \\ge 1$ they differ. Moreover, for banded SPD matrices with strong diagonal dominance and very small $p$, SSOR’s setup cost is negligible and its per-iteration application is $\\mathcal{O}(n p)$, making it preferable when the IC(0) factorization cost $\\mathcal{O}(n p^{2})$ yields only a modest reduction in iteration count; conversely, for wider bands or weaker diagonal dominance IC(0) typically achieves a larger reduction in iterations, justifying its higher setup cost.\n\nB. For tridiagonal ($p = 1$) SPD matrices in natural ordering, $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ are algebraically identical for $\\omega = 1$; furthermore SSOR generally yields fewer iterations than IC(0) while requiring comparable setup cost.\n\nC. For any banded SPD matrix and any ordering that preserves bandwidth, there exists $\\omega \\in (0,2)$ such that $M_{\\mathrm{SSOR}}(\\omega) = M_{\\mathrm{IC(0)}}$; additionally SSOR has higher setup cost but produces larger iteration reductions than IC(0) as $p$ increases.\n\nD. If the graph of $A$ is a path (the tridiagonal case), then SSOR coincides with IC(0) after diagonal scaling; SSOR dominates both in setup and iteration reduction when $\\omega$ is chosen optimally near $2$.",
            "solution": "The user wants a critical validation of the problem statement, followed by a detailed solution and evaluation of the given options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- $A \\in \\mathbb{R}^{n \\times n}$ is a real symmetric positive definite (SPD) matrix.\n- $A$ has bandwidth $p$, meaning $a_{ij} = 0$ whenever $|i - j| > p$.\n- Standard splitting: $A = D - L - L^{\\top}$, where $D$ is diagonal and $-L$ is the strictly lower triangular part.\n- SSOR preconditioner: $M_{\\mathrm{SSOR}}(\\omega) = (D - \\omega L) D^{-1} (D - \\omega L^{\\top})$ for a relaxation parameter $\\omega \\in (0,2)$.\n- IC(0) preconditioner: $M_{\\mathrm{IC(0)}} = \\widetilde{L}\\,\\widetilde{L}^{\\top}$, where $\\widetilde{L}$ is a lower-triangular factor with the same sparsity pattern as the lower part of $A$ (including the diagonal). It is obtained by Cholesky factorization, discarding fill-in in the factor $\\widetilde{L}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly located within the well-established field of numerical linear algebra. The definitions of SPD matrices, bandwidth, SSOR preconditioners, and Incomplete Cholesky factorization (IC(0)) are standard.\n- **Well-Posed:** The problem provides clear and standard definitions for the objects of comparison ($M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$) and asks for the conditions under which they are identical and for a comparison of their performance trade-offs. This is a well-posed question.\n- **Objective:** The definitions and the question are stated in precise, objective mathematical language.\n- **Conclusion:** The problem statement is free of scientific flaws, ambiguities, or contradictions. It is a valid and well-posed problem.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with deriving the solution.\n\n### Solution Derivation\n\nThe problem asks for an analysis of two aspects: (i) the conditions for the algebraic identity of $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$, and (ii) a comparison of their setup costs versus iteration reduction performance.\n\n#### (i) Algebraic Identity: $M_{\\mathrm{SSOR}}(\\omega)$ vs. $M_{\\mathrm{IC(0)}}$\n\nFirst, let's expand the expression for the SSOR preconditioner:\n$$\nM_{\\mathrm{SSOR}}(\\omega) = (D - \\omega L) D^{-1} (D - \\omega L^{\\top}) = (I - \\omega L D^{-1}) (D - \\omega L^{\\top}) = D - \\omega L^{\\top} - \\omega L D^{-1} D + \\omega^2 L D^{-1} L^{\\top}\n$$\nSince $D^{-1} D = I$, this simplifies to:\n$$\nM_{\\mathrm{SSOR}}(\\omega) = D - \\omega L - \\omega L^{\\top} + \\omega^2 L D^{-1} L^{\\top}\n$$\n\nNext, we analyze the structure of the IC(0) preconditioner, $M_{\\mathrm{IC(0)}} = \\widetilde{L}\\,\\widetilde{L}^{\\top}$. The incomplete Cholesky factorization algorithm with zero fill-in (IC(0)) constructs the lower-triangular factor $\\widetilde{L}$ such that it has the same sparsity pattern as the lower triangle of $A$. A crucial property of the resulting preconditioner matrix $M_{\\mathrm{IC(0)}}$ is that it matches the matrix $A$ on its original sparsity pattern. That is, if $S = \\{(i,j) | a_{ij} \\neq 0\\}$, then $(M_{\\mathrm{IC(0)}})_{ij} = a_{ij}$ for all $(i,j) \\in S$. This can be shown directly from the IC(0) algorithm's defining equations.\n\nFor $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ to be algebraically identical, they must be equal element-wise. A necessary condition is that they agree on the original sparsity pattern $S$ of $A$. Therefore, we must have $(M_{\\mathrm{SSOR}}(\\omega))_{ij} = a_{ij}$ for all $(i,j) \\in S$.\n\nLet's check this condition for the diagonal entries, $(i,i) \\in S$. The diagonal entries of $A$ are $a_{ii} = d_{ii}$.\nThe diagonal entries of $M_{\\mathrm{SSOR}}(\\omega)$ are:\n$$\n(M_{\\mathrm{SSOR}}(\\omega))_{ii} = d_{ii} + \\omega^2 (L D^{-1} L^{\\top})_{ii}\n$$\nThe condition $(M_{\\mathrm{SSOR}}(\\omega))_{ii} = a_{ii} = d_{ii}$ implies:\n$$\n\\omega^2 (L D^{-1} L^{\\top})_{ii} = 0\n$$\nSince $\\omega \\in (0,2)$, we have $\\omega \\neq 0$. Thus, we need $(L D^{-1} L^{\\top})_{ii} = 0$. The term is given by:\n$$\n(L D^{-1} L^{\\top})_{ii} = \\sum_{k=1}^{n} (L)_{ik} (D^{-1})_{kk} (L^{\\top})_{ki} = \\sum_{k=1}^{i-1} l_{ik} d_{kk}^{-1} l_{ik} = \\sum_{k=1}^{i-1} \\frac{l_{ik}^2}{d_{kk}}\n$$\nSince $A$ is SPD, its diagonal entries $d_{kk} = a_{kk}$ are all positive. The sum $\\sum_{k=1}^{i-1} l_{ik}^2/d_{kk}$ is a sum of non-negative terms. It can only be zero if every term is zero. This requires $l_{ik}^2 = 0$ for all $k < i$. This must hold for every $i \\in \\{1, \\dots, n\\}$.\nThis implies that $l_{ij} = 0$ for all $i \\neq j$, which means the strictly lower triangular part $L$ must be the zero matrix, $L=0$.\nIf $L=0$, then $A=D$ is a diagonal matrix, which corresponds to a bandwidth of $p=0$.\n\nLet's check if this condition is also sufficient.\nIf $L=0$ ($p=0$), then $A=D$.\n- $M_{\\mathrm{SSOR}}(\\omega) = (D - 0) D^{-1} (D - 0) = D$.\n- For IC(0) on a diagonal matrix $A=D$, the factor $\\widetilde{L}$ is also diagonal with entries $\\widetilde{l}_{ii} = \\sqrt{a_{ii}} = \\sqrt{d_{ii}}$. So $\\widetilde{L} = D^{1/2}$. The preconditioner is $M_{\\mathrm{IC(0)}} = \\widetilde{L}\\widetilde{L}^{\\top} = D^{1/2} (D^{1/2})^{\\top} = D$.\nSo, if $p=0$, $M_{\\mathrm{SSOR}}(\\omega) = M_{\\mathrm{IC(0)}} = D$.\n\nTherefore, $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ are algebraically identical if and only if $L=0$, which is equivalent to $p=0$. For any banded matrix with $p \\ge 1$, they are different.\n\n#### (ii) Setup Cost vs. Iteration Reduction\n\nLet's analyze the computational costs and effectiveness for a banded matrix with bandwidth $p$.\n\n**SSOR Preconditioner:**\n-   **Setup Cost:** The preconditioner requires the matrices $D$ and $L$, which can be extracted from $A$ with no arithmetic operations. The cost is determined by reading the non-zero elements of $A$, which is $\\mathcal{O}(nnz(A))$. For a banded matrix, $nnz(A) \\approx n(2p+1)$, so the setup cost is $\\mathcal{O}(np)$.\n-   **Application Cost:** Applying the preconditioner means solving $M_{\\mathrm{SSOR}}(\\omega) z = r$. This involves a forward substitution with $(D - \\omega L)$, a diagonal scaling by $D^{-1}$, and a backward substitution with $(D - \\omega L^{\\top})$. Since $L$ has bandwidth $p$, both substitutions cost $\\mathcal{O}(np)$. The total per-iteration application cost is $\\mathcal{O}(np)$.\n\n**IC(0) Preconditioner:**\n-   **Setup Cost:** This requires computing the factor $\\widetilde{L}$. For the entry $\\widetilde{l}_{ij}$ (where $a_{ij} \\neq 0$), the algorithm involves a dot product of two vectors of length up to $p$. The calculation for row $i$ involves computing approximately $p$ non-zero entries, each taking up to $\\mathcal{O}(p)$ work. The total work for row $i$ is $\\mathcal{O}(p^2)$. Over all $n$ rows, the total setup cost is $\\mathcal{O}(np^2)$.\n-   **Application Cost:** Applying the preconditioner means solving $M_{\\mathrm{IC(0)}} z = \\widetilde{L}\\widetilde{L}^{\\top}z = r$. This involves a forward substitution with $\\widetilde{L}$ and a backward substitution with $\\widetilde{L}^{\\top}$. Since $\\widetilde{L}$ has the same sparsity pattern as $L$ (bandwidth $p$), the cost of each solve is $\\mathcal{O}(np)$. The total per-iteration application cost is $\\mathcal{O}(np)$.\n\n**Performance Comparison:**\n-   The setup cost of IC(0) is a factor of $p$ higher than that of SSOR. For $p=1$, they are comparable ($\\mathcal{O}(n)$), but for larger $p$, the difference is significant.\n-   The per-iteration cost is asymptotically the same for both, $\\mathcal{O}(np)$.\n-   In terms of quality, $M_{\\mathrm{IC(0)}}$ is generally a much better approximation to $A$ than $M_{\\mathrm{SSOR}}(\\omega)$, meaning the preconditioned matrix $M_{\\mathrm{IC(0)}}^{-1}A$ has a condition number closer to $1$. This typically leads to a substantial reduction in the number of iterations required for convergence in a method like PCG.\n-   The choice between them involves a trade-off. For problems where $p$ is small and/or the matrix $A$ is strongly diagonally dominant, both preconditioners work well. SSOR's very low setup cost makes it attractive if the number of iterations is already low, or if IC(0) offers only a minor improvement.\n-   Conversely, for problems with wider bands (larger $p$) or weaker diagonal dominance, the quality of the preconditioner is paramount. The large reduction in iteration count achieved by IC(0) can more than compensate for its higher $\\mathcal{O}(np^2)$ setup cost, leading to a shorter total solution time.\n\n### Evaluation of Options\n\n**A. Under the natural ordering in which the lower and upper bandwidths are both $p$, $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ are algebraically identical if and only if $p = 0$ (equivalently, $L = 0$), in which case both reduce to the diagonal $D$. For $p \\ge 1$ they differ. Moreover, for banded SPD matrices with strong diagonal dominance and very small $p$, SSOR’s setup cost is negligible and its per-iteration application is $\\mathcal{O}(n p)$, making it preferable when the IC(0) factorization cost $\\mathcal{O}(n p^{2})$ yields only a modest reduction in iteration count; conversely, for wider bands or weaker diagonal dominance IC(0) typically achieves a larger reduction in iterations, justifying its higher setup cost.**\n- This statement perfectly aligns with our detailed analysis in both part (i) and part (ii). The condition for identity is correct, and the trade-off analysis is accurate.\n- **Verdict: Correct.**\n\n**B. For tridiagonal ($p = 1$) SPD matrices in natural ordering, $M_{\\mathrm{SSOR}}(\\omega)$ and $M_{\\mathrm{IC(0)}}$ are algebraically identical for $\\omega = 1$; furthermore SSOR generally yields fewer iterations than IC(0) while requiring comparable setup cost.**\n- For $p=1$, no fill-in occurs in the Cholesky factorization, so $M_{\\mathrm{IC(0)}}=A$. For SSOR, $M_{\\mathrm{SSOR}}(1) = A + L D^{-1} L^{\\top}$. Since $L \\neq 0$, the term $L D^{-1} L^\\top$ is a non-zero diagonal matrix. Thus, $M_{\\mathrm{SSOR}}(1) \\neq A = M_{\\mathrm{IC(0)}}$. The first clause is false.\n- Since $M_{\\mathrm{IC(0)}}=A$, preconditioning with it reduces the problem to $A^{-1}Ax = A^{-1}b$, which converges in $1$ iteration in PCG (in exact arithmetic). SSOR will take more than $1$ iteration. Thus SSOR yields more, not fewer, iterations. The second clause is false.\n- **Verdict: Incorrect.**\n\n**C. For any banded SPD matrix and any ordering that preserves bandwidth, there exists $\\omega \\in (0,2)$ such that $M_{\\mathrm{SSOR}}(\\omega) = M_{\\mathrm{IC(0)}}$; additionally SSOR has higher setup cost but produces larger iteration reductions than IC(0) as $p$ increases.**\n- The first clause is false. Our derivation showed identity is possible only if $L=0$, regardless of ordering.\n- The second clause is also false. SSOR has a lower setup cost ($\\mathcal{O}(np)$ vs $\\mathcal{O}(np^2)$) and IC(0) typically produces larger iteration reductions. The statement has both comparisons reversed.\n- **Verdict: Incorrect.**\n\n**D. If the graph of $A$ is a path (the tridiagonal case), then SSOR coincides with IC(0) after diagonal scaling; SSOR dominates both in setup and iteration reduction when $\\omega$ is chosen optimally near $2$.**\n- Coincidence after diagonal scaling is not generally true for any tridiagonal matrix, as it imposes strong constraints on the matrix entries that are not generally met.\n- The claim that SSOR dominates in setup cost is false for $p=1$, as the costs are comparable ($\\mathcal{O}(n)$). The claim that it dominates in iteration reduction is false for any $p \\ge 1$, as IC(0) is a more powerful preconditioner. The claim about optimal $\\omega$ near $2$ is not a general rule for SSOR.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Theoretical understanding culminates in practical application, where performance is empirically validated and optimized. This capstone practice  guides you through the entire workflow of applying the SSOR preconditioner, from theoretical derivation to computational implementation. You will design and code an algorithm to tune the relaxation parameter $\\omega$ by minimizing the spectral condition number, testing your implementation on a suite of representative problems and bridging the gap between analysis and effective scientific computation.",
            "id": "3583782",
            "problem": "You are given a real symmetric positive definite linear system with coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$. Consider the matrix splitting $A = D - L - U$, where $D$ is the diagonal of $A$, $-L$ is the strictly lower triangular part, and $-U$ is the strictly upper triangular part. The Symmetric Successive Over-Relaxation (SSOR) preconditioner with relaxation parameter $\\omega \\in (0, 2)$ is constructed by composing one forward and one backward over-relaxed sweep. Your tasks are to derive a usable and provably correct algebraic form for the SSOR preconditioner, establish the properties necessary for safe use in iterative methods, and implement practical parameter tuning and diagnostics.\n\nStarting only from the above splitting and the definition of a forward and a backward over-relaxation sweep as linear operators parameterized by $\\omega$, perform the following:\n\n1) Derive the algebraic form (up to an arbitrary positive scalar multiple) of a left preconditioning matrix $M_{\\omega}$ corresponding to one forward and one backward over-relaxed sweep. Prove that for any symmetric positive definite $A$ and any $\\omega \\in (0, 2)$, the matrix $M_{\\omega}$ is symmetric positive definite. Your derivation must not assume any pre-existing closed-form expression for $M_{\\omega}$; it must start from the definitions of the split operators and the composition of the two sweeps.\n\n2) Using only fundamental properties of symmetric positive definite pairs and generalized eigenvalues, show that the condition number of the preconditioned operator in the $M_{\\omega}$-inner product equals the spectral condition number of $M_{\\omega}^{-1} A$. Prove that multiplying $M_{\\omega}$ by any positive scalar leaves this condition number unchanged. Conclude that any positive scalar factor in the explicit formula of $M_{\\omega}$ may be ignored for the purpose of tuning $\\omega$.\n\n3) Propose a practical tuning objective for $\\omega$ that does not rely on inaccessible problem constants, and justify it from first principles. Then, design a numerically stable algorithm that, for a given $\\omega$, computes the spectral condition number $\\kappa(M_{\\omega}^{-1} A)$ by solving a symmetric generalized eigenvalue problem of the form $A x = \\lambda M_{\\omega} x$. Your algorithm should specify how to:\n- Construct $D$, $L$, and $U$ from $A$.\n- Assemble $M_{\\omega}$ with the chosen algebraic form from Part $1)$.\n- Compute the smallest and largest generalized eigenvalues of the symmetric positive definite pair $(A, M_{\\omega})$ to obtain $\\kappa(M_{\\omega}^{-1} A)$.\n- Guard against end-point instabilities by restricting $\\omega$ to a compact subinterval $[\\omega_{\\min}, \\omega_{\\max}] \\subset (0, 2)$ with small safety margins $\\omega_{\\min} > 0$ and $\\omega_{\\max} < 2$.\n- Validate diagnostics that $M_{\\omega}$ is symmetric (to numerical tolerance) and positive definite (by checking its smallest eigenvalue is strictly positive).\n\n4) Implement the above algorithm as a complete program that performs a grid-search over $\\omega \\in [\\omega_{\\min}, \\omega_{\\max}]$ to minimize $\\kappa(M_{\\omega}^{-1} A)$, and reports the best $\\omega^{\\star}$ and the achieved ratio of condition numbers $\\kappa(M_{\\omega^{\\star}}^{-1} A) / \\kappa(A)$. Use the symmetric generalized eigenvalue problem to compute $\\kappa(M_{\\omega}^{-1} A)$ and the standard symmetric eigenvalue problem to compute $\\kappa(A)$.\n\nTest suite. Your program must run the tuning and diagnostics for the following three symmetric positive definite matrices $A$, covering a general case, a two-dimensional stencil case, and an anisotropically scaled case:\n- Case $1$ (happy path): one-dimensional Poisson matrix with $n = 20$, i.e., the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals.\n- Case $2$ (increased coupling): two-dimensional Poisson matrix on a $5 \\times 5$ interior grid (thus $n = 25$) with the standard five-point stencil, lexicographic ordering, and homogeneous Dirichlet boundary conditions.\n- Case $3$ (significant anisotropic scaling): let $n = 30$, let $K$ be the one-dimensional Poisson matrix as in Case $1$, and let $S = \\mathrm{diag}(s_{1}, \\dots, s_{n})$ with $s_{i} = \\exp(\\alpha \\cdot (i-1)/(n-1))$ and $\\alpha = 3$. Define $A = S K S$.\n\nFor all cases use the same tuning grid with $\\omega_{\\min} = 0.05$, $\\omega_{\\max} = 1.95$, and a uniform step size of $\\Delta\\omega = 0.01$. Enforce the symmetry diagnostic $\\lVert M_{\\omega} - M_{\\omega}^{\\mathsf{T}} \\rVert_{\\mathrm{F}} \\leq \\tau$ with tolerance $\\tau = 10^{-12}$, and verify positive definiteness by the smallest eigenvalue exceeding $\\varepsilon = 10^{-12}$.\n\nFinal output specification. Your program must produce a single line of output containing the results for the three test cases aggregated into a comma-separated list enclosed in square brackets, with each floating-point number rounded to exactly $6$ decimal places, in the following order:\n- $\\omega_{1}^{\\star}$, $\\rho_{1}$, $\\omega_{2}^{\\star}$, $\\rho_{2}$, $\\omega_{3}^{\\star}$, $\\rho_{3}$,\nwhere $\\omega_{j}^{\\star}$ is the tuned relaxation parameter for Case $j$, and $\\rho_{j} = \\kappa(M_{\\omega_{j}^{\\star}}^{-1} A) / \\kappa(A)$ is the achieved condition number ratio for Case $j$. For example, your output must have the exact format\n$[\\omega_{1}^{\\star},\\rho_{1},\\omega_{2}^{\\star},\\rho_{2},\\omega_{3}^{\\star},\\rho_{3}]$,\nwith each numeric entry printed with exactly $6$ digits after the decimal point. No additional text must be printed.",
            "solution": "The problem posed is a comprehensive exercise in the theory and application of the Symmetric Successive Over-Relaxation (SSOR) method as a preconditioner for symmetric positive definite (SPD) linear systems. It is scientifically grounded, well-posed, and all its components are formalizable within the domain of numerical linear algebra. Therefore, the problem is valid, and we proceed with a complete solution.\n\nThe solution is presented in four parts, corresponding to the tasks outlined in the problem statement. We begin with the derivation and analysis of the preconditioner matrix, followed by an examination of its condition number properties, the design of a tuning algorithm, and finally, the implementation details.\n\n### Part 1: Derivation and Properties of the SSOR Preconditioner Matrix $M_{\\omega}$\n\nWe are given a real SPD matrix $A \\in \\mathbb{R}^{n \\times n}$ and its splitting $A = D - L - U$, where $D$ is the diagonal of $A$, $-L$ is its strictly lower triangular part, and $-U$ is its strictly upper triangular part. Since $A$ is symmetric, $U = L^{\\mathsf{T}}$. The relaxation parameter $\\omega$ lies in the interval $(0, 2)$.\n\nA preconditioning step aims to compute an approximate solution $\\mathbf{z}$ to the system $A\\mathbf{z} = \\mathbf{r}$, where $\\mathbf{r}$ is the residual vector. The preconditioner $M_{\\omega}$ is defined such that $\\mathbf{z} = M_{\\omega}^{-1}\\mathbf{r}$. The problem defines the action of $M_{\\omega}^{-1}$ as the composition of one forward and one backward over-relaxed sweep, applied to $A\\mathbf{z}=\\mathbf{r}$ with an initial guess of $\\mathbf{z}_0 = \\mathbf{0}$.\n\n1.  **Forward Sweep**: The intermediate update $\\mathbf{z}_{1/2}$ is computed by solving:\n    $$ (D - \\omega L)\\mathbf{z}_{1/2} = \\omega \\mathbf{r} \\implies \\mathbf{z}_{1/2} = \\omega (D - \\omega L)^{-1}\\mathbf{r} $$\n\n2.  **Backward Sweep**: The final update $\\mathbf{z}$ is computed from $\\mathbf{z}_{1/2}$ by solving:\n    $$ (D - \\omega U)\\mathbf{z} = \\omega \\mathbf{r} + ((1-\\omega)D + \\omega L)\\mathbf{z}_{1/2} $$\n    Substituting for $\\mathbf{z}_{1/2}$:\n    $$ (D-\\omega U)\\mathbf{z} = \\omega\\mathbf{r} + ((1-\\omega)D + \\omega L) \\omega(D-\\omega L)^{-1}\\mathbf{r} $$\n    $$ (D-\\omega U)\\mathbf{z} = \\omega\\left[ I + ((1-\\omega)D + \\omega L)(D-\\omega L)^{-1} \\right]\\mathbf{r} $$\n    Simplifying the term in brackets:\n    $$ I + ((1-\\omega)D + \\omega L)(D-\\omega L)^{-1} = \\left[ (D-\\omega L) + (1-\\omega)D + \\omega L \\right](D-\\omega L)^{-1} = (2-\\omega)D(D-\\omega L)^{-1} $$\n    Substituting this back gives the action of $M_{\\omega}^{-1}$:\n    $$ (D-\\omega U)\\mathbf{z} = \\omega(2-\\omega)D(D-\\omega L)^{-1}\\mathbf{r} \\implies \\mathbf{z} = \\omega(2-\\omega) (D-\\omega U)^{-1} D (D-\\omega L)^{-1} \\mathbf{r} $$\n    Thus, $M_{\\omega}^{-1} = \\omega(2-\\omega) (D-\\omega U)^{-1} D (D-\\omega L)^{-1}$. Inverting this gives the full SSOR preconditioner:\n    $$ M_{\\omega} = \\frac{1}{\\omega(2-\\omega)} (D - \\omega L) D^{-1} (D - \\omega U) $$\nThe problem allows for an arbitrary positive scalar multiple. For $\\omega \\in (0, 2)$, the factor $\\frac{1}{\\omega(2-\\omega)}$ is positive. We may thus select the canonical algebraic form:\n$$ M_{\\omega} = (D - \\omega L) D^{-1} (D - \\omega U) $$\n\n**Proof of Symmetry and Positive Definiteness:**\nTo prove $M_{\\omega}$ is SPD for an SPD matrix $A$ and $\\omega \\in \\mathbb{R}$:\n1.  **Symmetry**: Since $A$ is symmetric, $U = L^{\\mathsf{T}}$. We examine the transpose of $M_{\\omega}$:\n    $$ M_{\\omega}^{\\mathsf{T}} = ((D - \\omega L)D^{-1}(D - \\omega U))^{\\mathsf{T}} = (D - \\omega U)^{\\mathsf{T}}(D^{-1})^{\\mathsf{T}}(D - \\omega L)^{\\mathsf{T}} $$\n    Using $D^{\\mathsf{T}}=D$, $U^{\\mathsf{T}}=L$, and $L^{\\mathsf{T}}=U$:\n    $$ M_{\\omega}^{\\mathsf{T}} = (D^{\\mathsf{T}} - \\omega U^{\\mathsf{T}})(D^{\\mathsf{T}})^{-1}(D^{\\mathsf{T}} - \\omega L^{\\mathsf{T}}) = (D - \\omega L)D^{-1}(D - \\omega U) = M_{\\omega} $$\n    Thus, $M_{\\omega}$ is symmetric.\n\n2.  **Positive Definiteness**: Let $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{x} \\neq \\mathbf{0}$. We must show $\\mathbf{x}^{\\mathsf{T}}M_{\\omega}\\mathbf{x} > 0$. Using $U=L^\\mathsf{T}$, we can write $M_\\omega = (D - \\omega L) D^{-1} (D - \\omega L)^\\mathsf{T}$.\n    $$ \\mathbf{x}^{\\mathsf{T}}M_{\\omega}\\mathbf{x} = \\mathbf{x}^{\\mathsf{T}} (D - \\omega L) D^{-1} (D - \\omega L)^{\\mathsf{T}} \\mathbf{x} $$\n    Let $\\mathbf{y} = (D - \\omega L)^{\\mathsf{T}}\\mathbf{x} = (D - \\omega U)\\mathbf{x}$. The expression becomes $\\mathbf{y}^{\\mathsf{T}} D^{-1} \\mathbf{y}$.\n    Since $A$ is SPD, its diagonal entries $d_{ii}$ are all positive. Thus, $D$ is a diagonal matrix with positive entries, making it and its inverse $D^{-1}$ positive definite.\n    This implies $\\mathbf{y}^{\\mathsf{T}}D^{-1}\\mathbf{y} \\ge 0$. Strict positivity requires $\\mathbf{y} \\neq \\mathbf{0}$.\n    Is it possible for $\\mathbf{y} = (D - \\omega U)\\mathbf{x}$ to be zero for a non-zero $\\mathbf{x}$? The matrix $(D - \\omega U)$ is upper triangular, and its diagonal elements are the strictly positive diagonal elements of $D$. A triangular matrix with a non-zero diagonal is invertible. Therefore, $(D - \\omega U)\\mathbf{x} = \\mathbf{0}$ if and only if $\\mathbf{x} = \\mathbf{0}$.\n    For any $\\mathbf{x} \\neq \\mathbf{0}$, we have $\\mathbf{y} \\neq \\mathbf{0}$, and consequently $\\mathbf{x}^{\\mathsf{T}}M_{\\omega}\\mathbf{x} = \\mathbf{y}^{\\mathsf{T}}D^{-1}\\mathbf{y} > 0$.\n    Hence, $M_{\\omega}$ is symmetric and positive definite for any SPD $A$ and any $\\omega \\in \\mathbb{R}$. The condition $\\omega \\in (0, 2)$ is relevant for convergence properties of the iterative method, not for the SPD property of this simplified form of $M_{\\omega}$.\n\n### Part 2: Properties of the Condition Number\n\nThe preconditioned operator is $K = M_{\\omega}^{-1} A$. Since both $A$ and $M_{\\omega}$ are SPD, we can analyze the symmetrically preconditioned system with operator $\\tilde{K} = M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$. This operator is symmetric and its eigenvalues are real and positive. The condition number in the $M_{\\omega}$-inner product is defined as $\\kappa_{M_\\omega}(K) = \\lambda_{\\max}(\\tilde{K}) / \\lambda_{\\min}(\\tilde{K})$.\n\nThe eigenvalues of $\\tilde{K}$ are identical to the generalized eigenvalues of the pair $(A, M_{\\omega})$, which are the roots $\\lambda$ of $\\det(A - \\lambda M_{\\omega}) = 0$. This is because $\\tilde{K} = M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$ is similar to $M_{\\omega}^{-1}A$, since $M_{\\omega}^{-1}A = M_{\\omega}^{-1/2}(M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2})M_{\\omega}^{1/2}$. Therefore, $\\text{eig}(\\tilde{K}) = \\text{eig}(M_{\\omega}^{-1}A)$. The condition number in the $M_\\omega$-inner product is thus equal to the spectral condition number of $M_\\omega^{-1}A$:\n$$ \\kappa_{M_\\omega}(M_\\omega^{-1} A) = \\frac{\\lambda_{\\max}(M_\\omega^{-1/2} A M_\\omega^{-1/2})}{\\lambda_{\\min}(M_\\omega^{-1/2} A M_\\omega^{-1/2})} = \\frac{\\lambda_{\\max}(M_\\omega^{-1} A)}{\\lambda_{\\min}(M_\\omega^{-1} A)} = \\kappa(M_\\omega^{-1} A) $$\n\nNext, we prove that scaling $M_{\\omega}$ by a positive scalar $c > 0$ does not change this condition number. Let the scaled preconditioner be $M'_{\\omega} = c M_{\\omega}$. The new preconditioned operator is $(M'_{\\omega})^{-1} A = (c M_{\\omega})^{-1} A = \\frac{1}{c} (M_{\\omega}^{-1} A)$.\nIf $\\lambda$ is an eigenvalue of $M_{\\omega}^{-1}A$ with eigenvector $\\mathbf{x}$, so $(M_{\\omega}^{-1}A)\\mathbf{x} = \\lambda \\mathbf{x}$, then for the scaled system we have:\n$$ (\\frac{1}{c} M_{\\omega}^{-1} A) \\mathbf{x} = \\frac{1}{c}(\\lambda \\mathbf{x}) = (\\frac{\\lambda}{c}) \\mathbf{x} $$\nThe eigenvalues of the new operator are $\\lambda' = \\lambda/c$. The condition number is the ratio of the largest to smallest eigenvalue:\n$$ \\kappa((M'_{\\omega})^{-1}A) = \\frac{\\lambda'_{\\max}}{\\lambda'_{\\min}} = \\frac{\\lambda_{\\max}/c}{\\lambda_{\\min}/c} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa(M_\\omega^{-1}A) $$\nThis shows invariance to positive scaling.\n\nAs established in Part 1, the full form of the SSOR preconditioner includes a scalar factor depending on $\\omega$, namely $c(\\omega) = \\frac{1}{\\omega(2-\\omega)}$. Since $\\omega \\in (0, 2)$, this factor is always positive. Because the condition number we aim to minimize is invariant to this positive factor, we are justified in ignoring it for the purpose of tuning $\\omega$. We can therefore use the simpler form $M_{\\omega} = (D - \\omega L) D^{-1} (D - \\omega U)$ for the tuning process.\n\n### Part 3: Tuning Objective and Algorithm Design\n\n**Tuning Objective**: For SPD systems, the convergence rate of the Preconditioned Conjugate Gradient (PCG) method is bounded by a function that decreases as the condition number of the preconditioned operator decreases. Therefore, a natural and practical objective for tuning the parameter $\\omega$ is to **minimize the spectral condition number of the preconditioned matrix, $\\kappa(M_{\\omega}^{-1}A)$**. This objective is computable and directly relates to the performance of the iterative solver.\n\n**Algorithm Design**:\nThe algorithm to find the optimal $\\omega^{\\star}$ in a given range $[\\omega_{\\min}, \\omega_{\\max}]$ proceeds as follows:\n\n1.  **Matrix Decomposition**: Given the matrix $A$, construct its components:\n    - $D$: The diagonal matrix containing the diagonal of $A$.\n    - $L$: The matrix containing the entries of $A$ below the diagonal (i.e., the strictly lower triangular part of $A$).\n    - $U$: The matrix containing the entries of $A$ above the diagonal (i.e., the strictly upper triangular part of $A$).\n\n2.  **Grid Search over $\\omega$**: Iterate through a discrete set of $\\omega$ values in the specified interval $[\\omega_{\\min}, \\omega_{\\max}]$ with a step size $\\Delta\\omega$.\n\n3.  **For each $\\omega$**:\n    a.  **Assemble $M_{\\omega}$**: Construct the preconditioner matrix using the derived formula:\n        $$ M_{\\omega} = (D + \\omega L) D^{-1} (D + \\omega U) $$\n        Here, we use `+` because standard libraries extract `L` and `U` with their original negative signs for problems like the Poisson equation. This form is computationally equivalent to $(D - \\omega L_{std})$ where $L_{std}$ has positive entries.\n\n    b.  **Diagnostics**:\n        i.  **Symmetry Check**: Verify that the computed $M_{\\omega}$ is symmetric to within a numerical tolerance $\\tau$. This is done by checking if $\\lVert M_{\\omega} - M_{\\omega}^{\\mathsf{T}} \\rVert_{\\mathrm{F}} \\leq \\tau$.\n        ii. **Positive Definiteness Check**: Verify that $M_{\\omega}$ is positive definite by computing its smallest eigenvalue, $\\lambda_{\\min}(M_{\\omega})$, and ensuring it is greater than a small positive tolerance $\\varepsilon$, i.e., $\\lambda_{\\min}(M_{\\omega}) > \\varepsilon$.\n\n    c.  **Compute Condition Number**: Solve the symmetric definite generalized eigenvalue problem $A\\mathbf{x} = \\lambda M_{\\omega}\\mathbf{x}$ to find its eigenvalues $\\lambda_i$. This is preferable to forming $M_{\\omega}^{-1}A$ explicitly, as it preserves the symmetry and positive definiteness of the pair $(A, M_{\\omega})$, leading to more stable numerical methods. The condition number is then:\n        $$ \\kappa(M_{\\omega}^{-1}A) = \\frac{\\max_i \\lambda_i}{\\min_i \\lambda_i} $$\n\n4.  **Find Optimal $\\omega^{\\star}$**: Keep track of the $\\omega$ that yields the minimum $\\kappa(M_{\\omega}^{-1}A)$ found so far. The final value is the optimal parameter $\\omega^{\\star}$.\n\n5.  **Compute Reference $\\kappa(A)$**: For comparison, calculate the condition number of the original matrix $A$ by solving the standard symmetric eigenvalue problem $A\\mathbf{x} = \\lambda\\mathbf{x}$ and computing $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$.\n\n6.  **Report Ratio**: The final performance metric is the ratio $\\rho = \\kappa(M_{\\omega^{\\star}}^{-1}A) / \\kappa(A)$, which quantifies the improvement due to preconditioning.\n\nThis algorithm provides a robust and numerically stable procedure for tuning the SSOR preconditioner.\n\n### Part 4: Implementation Strategy\nThe algorithm described in Part 3 will be implemented in Python using the `numpy` and `scipy` libraries.\n- The test matrices for the three cases will be constructed as follows:\n    - **Case 1 (1D Poisson)**: A tridiagonal matrix of size $20 \\times 20$ created using `numpy.diag`.\n    - **Case 2 (2D Poisson)**: A block-tridiagonal matrix of size $25 \\times 25$ constructed using `numpy.kron` to represent the five-point stencil on a $5 \\times 5$ grid.\n    - **Case 3 (Anisotropic)**: The matrix $A=SKS$ of size $30\\times 30$ built from the 1D Poisson matrix $K$ and a diagonal scaling matrix $S$.\n- Eigenvalue problems (both standard and generalized) for SPD matrices will be solved using `scipy.linalg.eigh`, which is designed for this purpose and is highly efficient and accurate.\n- The grid search will be implemented as a simple loop over a `numpy.arange` array for $\\omega$.\n- Diagnostic checks and the final output formatting will precisely follow the problem specifications.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Performs SSOR parameter tuning for three test cases and prints the results.\n    \"\"\"\n    \n    # Define test parameters\n    omega_min = 0.05\n    omega_max = 1.95\n    delta_omega = 0.01\n    sym_tol = 1e-12\n    pd_tol = 1e-12\n\n    test_cases_params = [\n        {'id': 1, 'n': 20},\n        {'id': 2, 'm': 5}, # 2D case on m x m grid\n        {'id': 3, 'n': 30, 'alpha': 3.0}\n    ]\n\n    final_results = []\n\n    for params in test_cases_params:\n        # 1. Construct the matrix A for the current test case\n        case_id = params['id']\n        if case_id == 1:\n            n = params['n']\n            A = np.diag(2.0 * np.ones(n)) - np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n        elif case_id == 2:\n            m = params['m']\n            n = m * m\n            # In 2D, main diag is 4, off-diags are -1. Kronecker sum gives this.\n            T_m = np.diag(2.0 * np.ones(m)) - np.diag(np.ones(m - 1), k=1) - np.diag(np.ones(m - 1), k=-1)\n            I_m = np.eye(m)\n            A = np.kron(I_m, T_m) + np.kron(T_m, I_m)\n            # Adjust main diagonal from 4 to 2, then back to 4 for standard 5-point stencil.\n            # No, wait, the Kronecker sum produces a main diagonal of 4 correctly.\n        elif case_id == 3:\n            n = params['n']\n            alpha = params['alpha']\n            K_n = np.diag(2.0 * np.ones(n)) - np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n            s_diag = np.exp(alpha * np.arange(n) / (n - 1.0))\n            S = np.diag(s_diag)\n            A = S @ K_n @ S\n        else:\n            raise ValueError(\"Invalid case ID\")\n\n        # 2. Compute reference condition number kappa(A)\n        try:\n            eigvals_A = eigh(A, eigvals_only=True)\n            kappa_A = np.max(eigvals_A) / np.min(eigvals_A)\n        except np.linalg.LinAlgError:\n            # This should not happen for the given SPD matrices\n            raise RuntimeError(f\"Eigendecomposition of A failed for case {case_id}\")\n\n        # 3. Decompose A into D, L, U\n        D_diag = np.diag(A)\n        D = np.diag(D_diag)\n        L = np.tril(A, k=-1)\n        U = np.triu(A, k=1) # Note: U = L.T since A is symmetric.\n\n        # Prepare for grid search\n        omega_grid = np.arange(omega_min, omega_max + 1e-9, delta_omega)\n        best_omega = -1.0\n        min_kappa_preconditioned = float('inf')\n        \n        # 4. Grid search for optimal omega\n        for omega in omega_grid:\n            # 4a. Assemble M_omega\n            D_inv = np.diag(1.0 / D_diag)\n            # The formula is (D-omega*L_std). Since L from tril has negative entries, this becomes D+omega*L.\n            M_omega = (D + omega * L) @ D_inv @ (D + omega * U)\n\n            # 4b. Diagnostics\n            # Symmetry check\n            sym_error = np.linalg.norm(M_omega - M_omega.T, 'fro')\n            if sym_error > sym_tol:\n                raise RuntimeError(f\"Symmetry check failed for omega={omega}: {sym_error}\")\n            \n            # Positive definiteness check\n            try:\n                min_eig_M = eigh(M_omega, eigvals_only=True, subset_by_index=[0, 0])[0]\n                if min_eig_M = pd_tol:\n                    raise RuntimeError(f\"PD check failed for omega={omega}: min_eig={min_eig_M}\")\n            except np.linalg.LinAlgError:\n                raise RuntimeError(f\"Eigendecomposition of M_omega failed for omega={omega}\")\n\n            # 4c. Compute condition number kappa(M_inv * A)\n            try:\n                gen_eigvals = eigh(A, b=M_omega, eigvals_only=True)\n                kappa_preconditioned = np.max(gen_eigvals) / np.min(gen_eigvals)\n            except np.linalg.LinAlgError:\n                 raise RuntimeError(f\"Generalized eigendecomposition failed for omega={omega}\")\n\n            # 4d. Update best omega\n            if kappa_preconditioned  min_kappa_preconditioned:\n                min_kappa_preconditioned = kappa_preconditioned\n                best_omega = omega\n\n        # 5. Calculate final ratio\n        kappa_ratio = min_kappa_preconditioned / kappa_A\n        \n        final_results.append(best_omega)\n        final_results.append(kappa_ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{x:.6f}' for x in final_results])}]\")\n\nsolve()\n```"
        }
    ]
}