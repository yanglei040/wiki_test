{
    "hands_on_practices": [
        {
            "introduction": "The core principle of Sparse Approximate Inverse (SPAI) preconditioning is to find a sparse matrix $M$ that makes the product $AM$ or $MA$ as close to the identity matrix $I$ as possible. This first practice gets to the heart of the construction by asking you to derive the entries of a simple SPAI preconditioner from scratch . By minimizing the Frobenius norm $\\|I - AM\\|_{F}$, you will see how the global optimization problem conveniently decouples into independent, manageable least-squares problems for each column of $M$, a foundational property that makes SPAI algorithms computationally feasible.",
            "id": "1029919",
            "problem": "In the iterative solution of large sparse linear systems of equations of the form $Ax=b$, a preconditioner matrix $M$ is often used to transform the system into a more well-conditioned one, such as $MAx=Mb$. An effective preconditioner $M$ should approximate the inverse of $A$, i.e., $MA \\approx I$, while being inexpensive to compute and apply.\n\nA class of such preconditioners, known as Sparse Approximate Inverses (SAI), is constructed by defining a sparse matrix $M$ with a prescribed pattern of non-zero entries and then determining the values of these entries by minimizing the Frobenius norm of the residual matrix, $\\|I - AM\\|_F$.\n\nConsider a general invertible $3 \\times 3$ real matrix $A = (a_{ij})$. An SAI preconditioner $M$ is sought with the following prescribed sparsity pattern:\n$$\nM = \\begin{pmatrix} m_{11}  0  m_{13} \\\\ 0  m_{22}  0 \\\\ m_{31}  0  m_{33} \\end{pmatrix}\n$$\nThe non-zero entries of $M$ are chosen to minimize the squared Frobenius norm $\\|I - AM\\|_F^2$.\n\nYour task is to find the explicit expression for the entry $m_{11}$ of the optimal preconditioner $M$ in terms of the elements $a_{ij}$ of the matrix $A$.",
            "solution": "The problem is to find the non-zero entries of the matrix $M$ that minimize the objective function $F(M) = \\|I - AM\\|_F^2$. The Frobenius norm of a matrix $B$ is given by $\\|B\\|_F = \\sqrt{\\sum_{i,j} |B_{ij}|^2}$. Minimizing $\\|I - AM\\|_F$ is equivalent to minimizing $F(M) = \\|I - AM\\|_F^2$.\n\nThe squared Frobenius norm is the sum of the squared Euclidean norms of its columns. Let $M_j$ be the $j$-th column of $M$, $A_j$ be the $j$-th column of $A$, and $e_j$ be the $j$-th standard basis vector (the $j$-th column of the identity matrix $I$). The $j$-th column of the matrix $I - AM$ is $e_j - AM_j$.\n\nThe objective function can be written as:\n$$\nF(M) = \\sum_{j=1}^3 \\|e_j - AM_j\\|_2^2\n$$\nA key property of this formulation is that the optimization problem decouples for each column of $M$. We can find the optimal entries for each column $M_j$ independently by solving a separate least-squares problem:\n$$\n\\min_{M_j} \\|e_j - AM_j\\|_2^2 \\quad \\text{for } j=1, 2, 3\n$$\nWe are asked to find the entry $m_{11}$, which is part of the first column of $M$, a.k.a. $M_1$. The first column of $M$ has the form $M_1 = (m_{11}, 0, m_{31})^T$. The optimization variables for this column are $m_{11}$ and $m_{31}$.\n\nLet's expand the term $AM_1$:\n$$\nAM_1 = \\begin{pmatrix} a_{11}  a_{12}  a_{13} \\\\ a_{21}  a_{22}  a_{23} \\\\ a_{31}  a_{32}  a_{33} \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ 0 \\\\ m_{31} \\end{pmatrix} = \\begin{pmatrix} a_{11}m_{11} + a_{13}m_{31} \\\\ a_{21}m_{11} + a_{23}m_{31} \\\\ a_{31}m_{11} + a_{33}m_{31} \\end{pmatrix}\n$$\nThis can be written as a product of a submatrix of $A$ and a vector of the non-zero entries of $M_1$. Let $\\tilde{m}_1 = \\begin{pmatrix} m_{11} \\\\ m_{31} \\end{pmatrix}$. Let $\\tilde{A}_1$ be the matrix formed by the columns of $A$ corresponding to the non-zero entries of $M_1$. These are columns 1 and 3 of $A$.\n$$\n\\tilde{A}_1 = \\begin{pmatrix} a_{11}  a_{13} \\\\ a_{21}  a_{23} \\\\ a_{31}  a_{33} \\end{pmatrix}\n$$\nSo, $AM_1 = \\tilde{A}_1 \\tilde{m}_1$. The least-squares problem for the first column is:\n$$\n\\min_{\\tilde{m}_1} \\|e_1 - \\tilde{A}_1 \\tilde{m}_1\\|_2^2\n$$\nThis is a standard linear least-squares problem. The solution $\\tilde{m}_1$ is found by solving the normal equations:\n$$\n(\\tilde{A}_1^T \\tilde{A}_1) \\tilde{m}_1 = \\tilde{A}_1^T e_1\n$$\nLet's compute the terms in this equation.\n$$\n\\tilde{A}_1^T e_1 = \\begin{pmatrix} a_{11}  a_{21}  a_{31} \\\\ a_{13}  a_{23}  a_{33} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} a_{11} \\\\ a_{13} \\end{pmatrix}\n$$\n$$\nG = \\tilde{A}_1^T \\tilde{A}_1 = \\begin{pmatrix} a_{11}  a_{21}  a_{31} \\\\ a_{13}  a_{23}  a_{33} \\end{pmatrix} \\begin{pmatrix} a_{11}  a_{13} \\\\ a_{21}  a_{23} \\\\ a_{31}  a_{33} \\end{pmatrix} = \\begin{pmatrix} a_{11}^2+a_{21}^2+a_{31}^2  a_{11}a_{13}+a_{21}a_{23}+a_{31}a_{33} \\\\ a_{11}a_{13}+a_{21}a_{23}+a_{31}a_{33}  a_{13}^2+a_{23}^2+a_{33}^2 \\end{pmatrix}\n$$\nLet's denote the components of this $2 \\times 2$ Gram matrix $G$ as:\n$g_{11} = a_{11}^2+a_{21}^2+a_{31}^2$\n$g_{22} = a_{13}^2+a_{23}^2+a_{33}^2$\n$g_{12} = g_{21} = a_{11}a_{13}+a_{21}a_{23}+a_{31}a_{33}$\n\nThe normal equations become:\n$$\n\\begin{pmatrix} g_{11}  g_{12} \\\\ g_{12}  g_{22} \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ m_{31} \\end{pmatrix} = \\begin{pmatrix} a_{11} \\\\ a_{13} \\end{pmatrix}\n$$\nThe solution for $\\tilde{m}_1$ is obtained by inverting $G$:\n$$\n\\begin{pmatrix} m_{11} \\\\ m_{31} \\end{pmatrix} = G^{-1} \\begin{pmatrix} a_{11} \\\\ a_{13} \\end{pmatrix}\n$$\nThe inverse of the $2 \\times 2$ matrix $G$ is:\n$$\nG^{-1} = \\frac{1}{\\det(G)} \\begin{pmatrix} g_{22}  -g_{12} \\\\ -g_{12}  g_{11} \\end{pmatrix} = \\frac{1}{g_{11}g_{22}-g_{12}^2} \\begin{pmatrix} g_{22}  -g_{12} \\\\ -g_{12}  g_{11} \\end{pmatrix}\n$$\nSince $A$ is invertible, its columns are linearly independent. Thus, columns $A_1$ and $A_3$ are linearly independent, which ensures that $G = \\tilde{A}_1^T \\tilde{A}_1$ is invertible and $\\det(G) > 0$.\n\nWe need to find $m_{11}$, which is the first component of the solution vector:\n$$\nm_{11} = \\frac{g_{22}a_{11} - g_{12}a_{13}}{g_{11}g_{22} - g_{12}^2}\n$$\nLet's substitute the expressions for $g_{ij}$:\nThe numerator is:\n$N = (a_{13}^2+a_{23}^2+a_{33}^2)a_{11} - (a_{11}a_{13}+a_{21}a_{23}+a_{31}a_{33})a_{13}$\n$N = a_{11}a_{13}^2 + a_{11}a_{23}^2 + a_{11}a_{33}^2 - a_{11}a_{13}^2 - a_{13}a_{21}a_{23} - a_{13}a_{31}a_{33}$\n$N = a_{11}a_{23}^2 + a_{11}a_{33}^2 - a_{21}a_{13}a_{23} - a_{31}a_{13}a_{33}$\n$N = a_{23}(a_{11}a_{23} - a_{21}a_{13}) + a_{33}(a_{11}a_{33} - a_{31}a_{13})$\n\nThe denominator is $\\det(G) = (a_{11}^2+a_{21}^2+a_{31}^2)(a_{13}^2+a_{23}^2+a_{33}^2) - (a_{11}a_{13}+a_{21}a_{23}+a_{31}a_{33})^2$.\nThis is an instance of Lagrange's identity $(\\sum x_i^2)(\\sum y_i^2) - (\\sum x_i y_i)^2 = \\sum_{ij} (x_i y_j - x_j y_i)^2$.\nHere, the vectors are the first and third columns of $A$, $A_{\\cdot 1}=(a_{11}, a_{21}, a_{31})$ and $A_{\\cdot 3}=(a_{13}, a_{23}, a_{33})$.\nThe denominator is:\n$D = (a_{11}a_{23} - a_{21}a_{13})^2 + (a_{11}a_{33} - a_{31}a_{13})^2 + (a_{21}a_{33} - a_{31}a_{23})^2$.\nNotice that the terms in parentheses are the components of the cross product of the vectors $(a_{11}, a_{21}, a_{31})$ and $(a_{13}, a_{23}, a_{33})$, i.e., $D = \\|A_{\\cdot 1} \\times A_{\\cdot 3}\\|_2^2$.\n\nCombining the simplified numerator and denominator gives the final expression for $m_{11}$:\n$$\nm_{11} = \\frac{a_{23}(a_{11}a_{23} - a_{21}a_{13}) + a_{33}(a_{11}a_{33} - a_{31}a_{13})}{(a_{11}a_{23} - a_{21}a_{13})^2 + (a_{11}a_{33} - a_{31}a_{13})^2 + (a_{21}a_{33} - a_{31}a_{23})^2}\n$$\nThis can be written more compactly by reordering the terms in the second factor of the denominator:\n$m_{11} = \\frac{a_{23}(a_{11}a_{23} - a_{21}a_{13}) + a_{33}(a_{11}a_{33} - a_{31}a_{13})}{(a_{21}a_{33} - a_{31}a_{23})^2 + (a_{31}a_{13} - a_{11}a_{33})^2 + (a_{11}a_{23} - a_{21}a_{13})^2}$",
            "answer": "$$ \\boxed{\\frac{a_{23}(a_{11}a_{23} - a_{21}a_{13}) + a_{33}(a_{11}a_{33} - a_{31}a_{13})}{(a_{21}a_{33} - a_{31}a_{23})^2 + (a_{31}a_{13} - a_{11}a_{33})^2 + (a_{11}a_{23} - a_{21}a_{13})^2}} $$"
        },
        {
            "introduction": "While the first practice showed *how* to construct a SPAI, this exercise explores *why* it is often an effective strategy. The inverse of a sparse matrix is typically dense, so approximating it with another sparse matrix seems counter-intuitive at first glance. This problem reveals the motivation by examining block tridiagonal matrices, which commonly arise from discretizing physical systems . You will derive how the entries of the exact inverse decay away from the diagonal, providing a rigorous justification for why a sparse pattern can capture the most significant part of the inverse and serve as a powerful preconditioner.",
            "id": "3263474",
            "problem": "Consider a block tridiagonal linear system arising from a one-dimensional nearest-neighbor coupling with block size $m$, so the coefficient matrix $T \\in \\mathbb{R}^{(nm)\\times(nm)}$ has the form\n$$\nT \\;=\\; \n\\begin{pmatrix}\nA_{1}  B_{1}  0  \\cdots  0 \\\\\nC_{1}  A_{2}  B_{2}  \\ddots  \\vdots \\\\\n0  C_{2}  A_{3}  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  B_{n-1} \\\\\n0  \\cdots  0  C_{n-1}  A_{n}\n\\end{pmatrix},\n$$\nwhere each $A_{i} \\in \\mathbb{R}^{m\\times m}$ is nonsingular, and $B_{i}, C_{i} \\in \\mathbb{R}^{m\\times m}$. Assume $T$ is symmetric positive definite (SPD), so that $C_{i} = B_{i}^{\\mathsf{T}}$ and each $A_{i}$ is SPD.\n\n- Starting only from the definition of the Schur complement and block Gaussian elimination, derive a block lower–diagonal–upper (LDU) factorization $T = L D U$ with $L$ unit lower block bidiagonal, $U$ unit upper block bidiagonal, and $D$ block diagonal. Express the block recurrences defining the diagonal Schur complements in $D$ and the sub- and super-diagonal factors in $L$ and $U$.\n\n- Using $T^{-1} = U^{-1} D^{-1} L^{-1}$ and the nilpotency of strictly block bidiagonal matrices, write a general formula for the $(i,j)$ block of $T^{-1}$ in terms of products of the bidiagonal factors and the diagonal blocks of $D^{-1}$. Explain why, for SPD problems with nearest-neighbor coupling, the magnitude of off-diagonal blocks $|(T^{-1})_{ij}|$ typically decays as $|i-j|$ increases, thereby motivating a Sparse Approximate Inverse (SAI) preconditioner that retains only a fixed block bandwidth of $T^{-1}$.\n\n- Now specialize to the scalar case $m=1$ and $n=3$ with\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nDefine a Sparse Approximate Inverse (SAI) preconditioner $M^{-1}$ by retaining only the tridiagonal stencil (that is, entries with $|i-j|\\leq 1$) of the exact inverse $A^{-1}$. Compute the Frobenius norm $\\|A^{-1} - M^{-1}\\|_{F}$ and provide your final answer as an exact expression. Do not round.",
            "solution": "The problem is composed of three parts. We will address each part in sequence after validating the entire problem statement.\n\nThe problem statement is a well-defined exercise in numerical linear algebra. It concerns the properties of block tridiagonal matrices, specifically their LDU factorization and the structure of their inverses, culminating in a concrete calculation for a sparse approximate inverse. All terms are standard and the premises are scientifically sound. The problem is self-contained, consistent, and well-posed. Therefore, we proceed with the solution.\n\n### Part 1: Block LDU Factorization\n\nWe are given a block tridiagonal matrix $T \\in \\mathbb{R}^{(nm)\\times(nm)}$:\n$$\nT \\;=\\; \n\\begin{pmatrix}\nA_{1}  B_{1}  0  \\cdots  0 \\\\\nC_{1}  A_{2}  B_{2}  \\ddots  \\vdots \\\\\n0  C_{2}  A_{3}  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  B_{n-1} \\\\\n0  \\cdots  0  C_{n-1}  A_{n}\n\\end{pmatrix}\n$$\nWe seek a factorization $T = LDU$, where $L$ is a unit lower block bidiagonal matrix, $D$ is a block diagonal matrix, and $U$ is a unit upper block bidiagonal matrix. Their forms are:\n$$\nL = \n\\begin{pmatrix}\nI  0  \\cdots  0 \\\\\n\\mathcal{L}_{1}  I  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  0 \\\\\n0  \\cdots  \\mathcal{L}_{n-1}  I\n\\end{pmatrix}, \\quad\nD = \n\\begin{pmatrix}\nD_{1}  0  \\cdots  0 \\\\\n0  D_{2}  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  0 \\\\\n0  \\cdots  0  D_{n}\n\\end{pmatrix}, \\quad\nU = \n\\begin{pmatrix}\nI  \\mathcal{U}_{1}  \\cdots  0 \\\\\n0  I  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  \\mathcal{U}_{n-1} \\\\\n0  \\cdots  0  I\n\\end{pmatrix}\n$$\nwhere $I$ is the $m \\times m$ identity matrix, and $\\mathcal{L}_i, D_i, \\mathcal{U}_i \\in \\mathbb{R}^{m\\times m}$.\n\nWe perform block Gaussian elimination by equating the blocks of $T$ with the blocks of the product $LDU$.\n$$\nLDU = \n\\begin{pmatrix}\nD_{1}  D_{1}\\mathcal{U}_{1}  0  \\cdots \\\\\n\\mathcal{L}_{1}D_{1}  \\mathcal{L}_{1}D_{1}\\mathcal{U}_{1}+D_{2}  D_{2}\\mathcal{U}_{2}  \\cdots \\\\\n0  \\mathcal{L}_{2}D_{2}  \\mathcal{L}_{2}D_{2}\\mathcal{U}_{2}+D_{3}  \\cdots \\\\\n\\vdots  \\vdots  \\vdots  \\ddots\n\\end{pmatrix}\n$$\nBy comparing the blocks of $T$ and $LDU$ iteratively for $i = 1, 2, \\dots, n$:\n\nFor $i=1$:\n\\begin{itemize}\n    \\item Block $(1,1)$: $D_1 = A_1$. Since $A_1$ is nonsingular, $D_1$ is invertible.\n    \\item Block $(1,2)$: $D_1 \\mathcal{U}_1 = B_1 \\implies \\mathcal{U}_1 = D_1^{-1} B_1$.\n    \\item Block $(2,1)$: $\\mathcal{L}_1 D_1 = C_1 \\implies \\mathcal{L}_1 = C_1 D_1^{-1}$.\n\\end{itemize}\n\nFor $i = 2, \\dots, n$:\n\\begin{itemize}\n    \\item Block $(i,i)$: $\\mathcal{L}_{i-1}D_{i-1}\\mathcal{U}_{i-1} + D_i = A_i$. This gives the recurrence for the diagonal blocks of $D$:\n    $$ D_i = A_i - \\mathcal{L}_{i-1}D_{i-1}\\mathcal{U}_{i-1} $$\n    Substituting the expressions for $\\mathcal{L}_{i-1}$ and $\\mathcal{U}_{i-1}$:\n    $$ D_i = A_i - (C_{i-1}D_{i-1}^{-1}) D_{i-1} (D_{i-1}^{-1}B_{i-1}) = A_i - C_{i-1}D_{i-1}^{-1}B_{i-1} $$\n    These diagonal blocks $D_i$ are the Schur complements of the leading principal submatrices. Since $T$ is SPD, all principal submatrices are nonsingular, ensuring that each $D_i$ is invertible.\n    \\item Block $(i,i+1)$ (for $i  n$): $D_i \\mathcal{U}_i = B_i \\implies \\mathcal{U}_i = D_i^{-1} B_i$.\n    \\item Block $(i+1,i)$ (for $i  n$): $\\mathcal{L}_i D_i = C_i \\implies \\mathcal{L}_i = C_i D_i^{-1}$.\n\\end{itemize}\n\nIn summary, the recurrences are:\n\\begin{itemize}\n    \\item $D_1 = A_1$\n    \\item For $i = 2, \\ldots, n$: $D_i = A_i - C_{i-1} D_{i-1}^{-1} B_{i-1}$\n    \\item For $i = 1, \\ldots, n-1$: $\\mathcal{U}_i = D_i^{-1} B_i$ and $\\mathcal{L}_i = C_i D_i^{-1}$\n\\end{itemize}\n\n### Part 2: Structure of the Inverse and Motivation for SAI\n\nWe use the factorization $T=LDU$ to find the inverse $T^{-1} = U^{-1}D^{-1}L^{-1}$. Let's analyze the structure of $L^{-1}$ and $U^{-1}$.\nLet $L = I + \\mathbb{L}$ and $U = I + \\mathbb{U}$, where $\\mathbb{L}$ and $\\mathbb{U}$ are strictly block lower and upper bidiagonal matrices, respectively.\n$$\n\\mathbb{L} = \n\\begin{pmatrix}\n0  0  \\cdots  0 \\\\\n\\mathcal{L}_{1}  0  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  0 \\\\\n0  \\cdots  \\mathcal{L}_{n-1}  0\n\\end{pmatrix}, \\quad\n\\mathbb{U} = \n\\begin{pmatrix}\n0  \\mathcal{U}_{1}  \\cdots  0 \\\\\n0  0  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  \\mathcal{U}_{n-1} \\\\\n0  \\cdots  0  0\n\\end{pmatrix}\n$$\nThese matrices are nilpotent, i.e., $\\mathbb{L}^n = 0$ and $\\mathbb{U}^n = 0$. The inverses $L^{-1}$ and $U^{-1}$ can be expressed using a Neumann series:\n$$ L^{-1} = (I+\\mathbb{L})^{-1} = \\sum_{k=0}^{n-1} (-1)^k \\mathbb{L}^k \\quad \\text{and} \\quad U^{-1} = (I+\\mathbb{U})^{-1} = \\sum_{k=0}^{n-1} (-1)^k \\mathbb{U}^k $$\nThe $(i,j)$-th block of $\\mathbb{L}^k$ is non-zero only if $i=j+k$, and the $(i,j)$-th block of $\\mathbb{U}^k$ is non-zero only if $j=i+k$. Specifically, for $ij$:\n$$ (L^{-1})_{ij} = (-1)^{i-j}(\\mathbb{L}^{i-j})_{ij} = (-1)^{i-j} \\mathcal{L}_{i-1} \\mathcal{L}_{i-2} \\cdots \\mathcal{L}_{j} $$\nAnd for $ji$:\n$$ (U^{-1})_{ij} = (-1)^{j-i}(\\mathbb{U}^{j-i})_{ij} = (-1)^{j-i} \\mathcal{U}_{i} \\mathcal{U}_{i+1} \\cdots \\mathcal{U}_{j-1} $$\nThe $(i,j)$-th block of the inverse is $(T^{-1})_{ij} = \\sum_{p,q} (U^{-1})_{ip} (D^{-1})_{pq} (L^{-1})_{qj}$. Since $D$ is block diagonal, this simplifies to $(T^{-1})_{ij} = \\sum_{k=1}^n (U^{-1})_{ik} D_k^{-1} (L^{-1})_{kj}$.\n\nLet's consider the case $i \\le j$. For $(U^{-1})_{ik}$ to be non-zero, we need $k \\ge i$. For $(L^{-1})_{kj}$ to be non-zero, we need $k \\ge j$. Thus, a non-zero contribution requires $k \\ge j$.\n$$ (T^{-1})_{ij} = \\sum_{k=j}^n (U^{-1})_{ik} D_k^{-1} (L^{-1})_{kj} $$\nThe structure shows that $(T^{-1})_{ij}$ is a sum of products. A more direct derivation shows that for $ij$, $(T^{-1})_{ij} = (U^{-1})_{ij} (T^{-1})_{jj}$, and for $ij$, $(T^{-1})_{ij} = (L^{-1})_{ij} (T^{-1})_{jj}$.\nLet's focus on $ij$:\n$$ (T^{-1})_{ij} = ((-1)^{j-i} \\mathcal{U}_{i} \\mathcal{U}_{i+1} \\cdots \\mathcal{U}_{j-1}) (T^{-1})_{jj} $$\nUsing $\\mathcal{U}_k = D_k^{-1} B_k$, we get:\n$$ (T^{-1})_{ij} = (-1)^{j-i} (D_i^{-1}B_i)(D_{i+1}^{-1}B_{i+1}) \\cdots (D_{j-1}^{-1}B_{j-1}) (T^{-1})_{jj} $$\nFor an SPD matrix $T$ arising from nearest-neighbor coupling (e.g., from a finite difference discretization of an elliptic operator), the diagonal blocks $A_i$ are typically \"dominant\" over the off-diagonal coupling blocks $B_i$ and $C_i$. For the SPD case, $D_i = A_i - B_{i-1}^{\\mathsf{T}} D_{i-1}^{-1} B_{i-1}$ is also SPD. The norm of the factors $\\mathcal{U}_k = D_k^{-1} B_k$ is often less than $1$, i.e., $\\|\\mathcal{U}_k\\|  1$.\nAs a consequence, the norm of the product $\\| \\mathcal{U}_{i} \\cdots \\mathcal{U}_{j-1} \\| \\le \\| \\mathcal{U}_{i} \\| \\cdots \\| \\mathcal{U}_{j-1} \\|$ decays as the number of terms, $j-i$, increases. This implies that the magnitude of the off-diagonal blocks $|(T^{-1})_{ij}|$ decays, typically exponentially, as the block distance $|i-j|$ increases.\n\nThis decay property is the motivation for Sparse Approximate Inverse (SAI) preconditioners. Since the blocks of $T^{-1}$ far from the main diagonal are small, we can construct a sparse approximation $M^{-1}$ by explicitly computing a band of blocks of $T^{-1}$ around the main diagonal and setting all other blocks to zero. For a block bandwidth of $p$, we would define $(M^{-1})_{ij} = (T^{-1})_{ij}$ if $|i-j| \\le p$ and $(M^{-1})_{ij} = 0$ otherwise. This matrix $M^{-1}$ is sparse by construction and serves as a good approximation to $T^{-1}$, making it an effective preconditioner.\n\n### Part 3: Specific Calculation\n\nWe are given the scalar case ($m=1$) with $n=3$ and the matrix:\n$$ A = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} $$\nFirst, we compute the exact inverse $A^{-1}$. The determinant is $\\det(A) = 2(2 \\cdot 2 - (-1)(-1)) - (-1)((-1) \\cdot 2 - 0) = 2(3) + (-2) = 4$.\nThe adjugate matrix is the transpose of the cofactor matrix:\n$$ \\text{adj}(A) = \\begin{pmatrix} 2\\cdot 2 - (-1)(-1)  -((-1)2 - 0)  (-1)(-1) - 0 \\\\ -((-1)2 - 0)  2\\cdot 2 - 0  -((-1)2 - 0) \\\\ (-1)(-1) - 0  -((-1)2 - (-1)(-1))  2\\cdot 2 - (-1)(-1) \\end{pmatrix}^{\\mathsf{T}} = \\begin{pmatrix} 3  2  1 \\\\ 2  4  2 \\\\ 1  2  3 \\end{pmatrix} $$\nSo, the inverse is:\n$$ A^{-1} = \\frac{1}{4} \\begin{pmatrix} 3  2  1 \\\\ 2  4  2 \\\\ 1  2  3 \\end{pmatrix} $$\nThe Sparse Approximate Inverse (SAI) preconditioner $M^{-1}$ is defined by retaining only the tridiagonal stencil of $A^{-1}$, which corresponds to entries $(i,j)$ where $|i-j| \\le 1$. We set the entries $(1,3)$ and $(3,1)$ to zero:\n$$ M^{-1} = \\frac{1}{4} \\begin{pmatrix} 3  2  0 \\\\ 2  4  2 \\\\ 0  2  3 \\end{pmatrix} $$\nNow, we compute the difference matrix $E = A^{-1} - M^{-1}$:\n$$ E = A^{-1} - M^{-1} = \\frac{1}{4} \\begin{pmatrix} 3  2  1 \\\\ 2  4  2 \\\\ 1  2  3 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 3  2  0 \\\\ 2  4  2 \\\\ 0  2  3 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ 1  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  \\frac{1}{4} \\\\ 0  0  0 \\\\ \\frac{1}{4}  0  0 \\end{pmatrix} $$\nFinally, we compute the Frobenius norm $\\|E\\|_F$. The Frobenius norm of a matrix is the square root of the sum of the absolute squares of its elements.\n$$ \\|A^{-1} - M^{-1}\\|_F = \\|E\\|_F = \\sqrt{ \\sum_{i=1}^3 \\sum_{j=1}^3 |E_{ij}|^2 } $$\n$$ \\|E\\|_F = \\sqrt{ 0^2 + 0^2 + \\left(\\frac{1}{4}\\right)^2 + 0^2 + 0^2 + 0^2 + \\left(\\frac{1}{4}\\right)^2 + 0^2 + 0^2 } $$\n$$ \\|E\\|_F = \\sqrt{ \\left(\\frac{1}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 } = \\sqrt{ \\frac{1}{16} + \\frac{1}{16} } = \\sqrt{ \\frac{2}{16} } = \\sqrt{ \\frac{1}{8} } $$\n$$ \\|E\\|_F = \\frac{1}{\\sqrt{8}} = \\frac{1}{2\\sqrt{2}} = \\frac{\\sqrt{2}}{4} $$\nThe Frobenius norm of the difference is $\\frac{\\sqrt{2}}{4}$.",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{2}}{4}}\n$$"
        },
        {
            "introduction": "An effective preconditioner must not only approximate the inverse but also be compatible with the chosen iterative solver. This final practice presents a crucial cautionary tale regarding the celebrated Conjugate Gradient (CG) method, which requires a symmetric positive definite (SPD) operator . You will construct a SPAI for an SPD matrix, apply a seemingly logical symmetrization procedure, and discover through direct computation that the resulting preconditioned operator $\\widehat{M} A$ is not symmetric. This exercise highlights the critical fact that the product of two symmetric matrices is not generally symmetric unless they commute, underscoring the care required to ensure theoretical requirements of iterative methods are met in practice.",
            "id": "3579956",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite (SPD) and suppose one seeks a right Sparse Approximate Inverse (SPAI) preconditioner $M \\in \\mathbb{R}^{n \\times n}$ by minimizing the Frobenius-norm objective $\\| I - M A \\|_{F}$ subject to a prescribed sparsity pattern on $M$. Recall the following foundational facts.\n- The Conjugate Gradient (CG) method requires, in the standard Euclidean inner product, that the coefficient matrix be symmetric positive definite. For left preconditioning, the mathematically correct Preconditioned Conjugate Gradient (PCG) formulation preserves symmetry by working in the $M$-inner product with an SPD preconditioner $M$, so that $M^{-1}A$ is self-adjoint in that inner product.\n- If one instead attempts to run CG in the Euclidean inner product on the left-preconditioned operator $M A$, then one must have $M A$ be symmetric positive definite in the Euclidean sense. This requires $M$ and $A$ to commute if both are symmetric; otherwise $M A$ is generally nonsymmetric and hence not SPD.\n- In the SPAI construction with the Frobenius norm, the right-approximate inverse problem $\\min_{M} \\| I - M A \\|_{F}$ decouples by rows, because $\\| I - M A \\|_{F}^{2} = \\sum_{i=1}^{n} \\| e_{i}^{\\top} - r_{i}^{\\top} A \\|_{2}^{2}$, where $r_{i}^{\\top}$ is the $i$-th row of $M$, and each row can be optimized independently subject to its own sparsity.\n\nConsider the SPD tridiagonal matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nConstruct an SPAI $M$ as follows. Use the right-approximate inverse Frobenius objective $\\min_{M} \\| I - M A \\|_{F}$, solved row-wise with the following row-wise sparsity pattern:\n- Row $1$ of $M$ has support $\\{1,2\\}$.\n- Row $2$ of $M$ has support $\\{2\\}$.\n- Row $3$ of $M$ has support $\\{2,3\\}$.\nLet the minimizing (generally nonsymmetric) $M$ be post-symmetrized to enforce symmetry by defining the symmetric SPAI $\\widehat{M} := \\tfrac{1}{2}(M + M^{\\top})$.\n\nTasks:\n1. Derive from first principles the row-wise least-squares problems implied by $\\min_{M} \\| I - M A \\|_{F}$ under the given sparsity pattern, and solve them exactly to obtain the entries of $M$.\n2. Form the symmetric matrix $\\widehat{M} := \\tfrac{1}{2}(M + M^{\\top})$ and compute the product $\\widehat{M} A$.\n3. Explain, using the fundamental CG requirement of symmetry and positive definiteness in the Euclidean inner product, why applying classical CG directly to $\\widehat{M} A$ is not justified even though both $A$ and $\\widehat{M}$ are symmetric. Your explanation must identify the mechanism by which the post-symmetrization can fail to preserve the symmetry of $\\widehat{M} A$ and hence the effective SPD property required by CG.\n4. As quantitative evidence of the loss of effective symmetry, compute the Frobenius norm of the antisymmetric part of $\\widehat{M} A$, namely\n$$\n\\left\\| \\widehat{M} A - (\\widehat{M} A)^{\\top} \\right\\|_{F}.\n$$\n\nGive your final answer as a single exact reduced fraction (no rounding).",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in numerical linear algebra, well-posed, objective, and self-contained. All necessary data and definitions are provided, and there are no internal contradictions.\n\nThe solution proceeds by carrying out the four specified tasks.\n\n### Task 1: Derive and Solve Row-wise Least-Squares Problems\n\nThe Sparse Approximate Inverse (SPAI) preconditioner $M$ is constructed to minimize the Frobenius norm objective $\\| I - M A \\|_{F}$. This objective function can be decoupled by rows:\n$$\n\\min_{M} \\| I - M A \\|_{F}^{2} = \\min_{M} \\sum_{i=1}^{n} \\| e_{i}^{\\top} - r_{i}^{\\top} A \\|_{2}^{2}\n$$\nwhere $r_{i}^{\\top}$ is the $i$-th row of $M$ and $e_i^\\top$ is the $i$-th row of the identity matrix $I$. The minimization for each row $r_i^\\top$ is an independent linear least-squares problem, constrained by the prescribed sparsity pattern for that row.\n\nLet $J_i$ be the set of column indices of the non-zero entries in row $i$. Let $x_i$ be the column vector of these non-zero values, $x_i = (m_{ij})_{j \\in J_i}$. Let $A(J_i, :)$ be the submatrix of $A$ formed by selecting rows with indices in $J_i$. The $i$-th row problem is to find the vector $x_i$ that minimizes $\\| e_i^\\top - x_i^\\top A(J_i, :) \\|_2^2$. Transposing inside the norm, this is equivalent to minimizing $\\| e_i - A(J_i, :)^\\top x_i \\|_2^2$.\n\nThe solution to this least-squares problem is given by the normal equations:\n$$\n\\left( A(J_i, :) A(J_i, :)^\\top \\right) x_i = A(J_i, :) e_i\n$$\nThe matrix $A$ is given as:\n$$\nA = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix}\n$$\n\n**Row 1:** The sparsity pattern is $J_1 = \\{1, 2\\}$, so $x_1 = \\begin{pmatrix} m_{11} \\\\ m_{12} \\end{pmatrix}$.\nThe submatrix of $A$ is $A(J_1, :) = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\end{pmatrix}$.\nThe coefficient matrix for the normal equations is:\n$$\nA(J_1, :) A(J_1, :)^\\top = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\end{pmatrix} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} 5  -4 \\\\ -4  6 \\end{pmatrix}\n$$\nThe right-hand side is:\n$$\nA(J_1, :) e_1 = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nWe solve the system $\\begin{pmatrix} 5  -4 \\\\ -4  6 \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ m_{12} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\nThe determinant is $(5)(6) - (-4)(-4) = 30 - 16 = 14$. Using Cramer's rule:\n$m_{11} = \\frac{(2)(6) - (-4)(-1)}{14} = \\frac{12-4}{14} = \\frac{8}{14} = \\frac{4}{7}$.\n$m_{12} = \\frac{(5)(-1) - (2)(-4)}{14} = \\frac{-5+8}{14} = \\frac{3}{14}$.\n\n**Row 2:** The sparsity pattern is $J_2 = \\{2\\}$, so $x_2 = (m_{22})$.\nThe submatrix is $A(J_2, :) = \\begin{pmatrix} -1  2  -1 \\end{pmatrix}$.\nThe coefficient matrix is $A(J_2, :) A(J_2, :)^\\top = (-1)^2 + 2^2 + (-1)^2 = 6$.\nThe right-hand side is $A(J_2, :) e_2 = 2$.\nWe solve $6 m_{22} = 2$, which gives $m_{22} = \\frac{1}{3}$.\n\n**Row 3:** The sparsity pattern is $J_3 = \\{2, 3\\}$, so $x_3 = \\begin{pmatrix} m_{32} \\\\ m_{33} \\end{pmatrix}$.\nThe submatrix is $A(J_3, :) = \\begin{pmatrix} -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix}$.\nThe coefficient matrix is:\n$$\nA(J_3, :) A(J_3, :)^\\top = \\begin{pmatrix} -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix} -1  0 \\\\ 2  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} 6  -4 \\\\ -4  5 \\end{pmatrix}\n$$\nThe right-hand side is:\n$$\nA(J_3, :) e_3 = \\begin{pmatrix} -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}\n$$\nWe solve $\\begin{pmatrix} 6  -4 \\\\ -4  5 \\end{pmatrix} \\begin{pmatrix} m_{32} \\\\ m_{33} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$.\nThe determinant is $(6)(5) - (-4)(-4) = 30 - 16 = 14$.\n$m_{32} = \\frac{(-1)(5) - (-4)(2)}{14} = \\frac{-5+8}{14} = \\frac{3}{14}$.\n$m_{33} = \\frac{(6)(2) - (-1)(-4)}{14} = \\frac{12-4}{14} = \\frac{8}{14} = \\frac{4}{7}$.\n\nCombining these results, the matrix $M$ is:\n$$\nM = \\begin{pmatrix}\n\\frac{4}{7}  \\frac{3}{14}  0 \\\\\n0  \\frac{1}{3}  0 \\\\\n0  \\frac{3}{14}  \\frac{4}{7}\n\\end{pmatrix}\n$$\n\n### Task 2: Form $\\widehat{M}$ and Compute $\\widehat{M}A$\n\nThe symmetric preconditioner $\\widehat{M}$ is defined as $\\widehat{M} = \\frac{1}{2}(M + M^{\\top})$.\n$$\nM^{\\top} = \\begin{pmatrix}\n\\frac{4}{7}  0  0 \\\\\n\\frac{3}{14}  \\frac{1}{3}  \\frac{3}{14} \\\\\n0  0  \\frac{4}{7}\n\\end{pmatrix}\n$$\n$$\nM + M^{\\top} = \\begin{pmatrix}\n\\frac{8}{7}  \\frac{3}{14}  0 \\\\\n\\frac{3}{14}  \\frac{2}{3}  \\frac{3}{14} \\\\\n0  \\frac{3}{14}  \\frac{8}{7}\n\\end{pmatrix}\n$$\n$$\n\\widehat{M} = \\frac{1}{2}(M+M^\\top) = \\begin{pmatrix}\n\\frac{4}{7}  \\frac{3}{28}  0 \\\\\n\\frac{3}{28}  \\frac{1}{3}  \\frac{3}{28} \\\\\n0  \\frac{3}{28}  \\frac{4}{7}\n\\end{pmatrix}\n$$\nNow, we compute the product $\\widehat{M} A$:\n$$\n\\widehat{M} A = \\begin{pmatrix}\n\\frac{4}{7}  \\frac{3}{28}  0 \\\\\n\\frac{3}{28}  \\frac{1}{3}  \\frac{3}{28} \\\\\n0  \\frac{3}{28}  \\frac{4}{7}\n\\end{pmatrix}\n\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}\n$$\nThe entries are calculated as:\n$(\\widehat{M} A)_{11} = \\frac{4}{7}(2) + \\frac{3}{28}(-1) = \\frac{8}{7} - \\frac{3}{28} = \\frac{32-3}{28} = \\frac{29}{28}$\n$(\\widehat{M} A)_{12} = \\frac{4}{7}(-1) + \\frac{3}{28}(2) = -\\frac{4}{7} + \\frac{6}{28} = \\frac{-16+6}{28} = -\\frac{10}{28} = -\\frac{5}{14}$\n$(\\widehat{M} A)_{13} = \\frac{3}{28}(-1) = -\\frac{3}{28}$\n$(\\widehat{M} A)_{21} = \\frac{3}{28}(2) + \\frac{1}{3}(-1) = \\frac{6}{28} - \\frac{1}{3} = \\frac{3}{14} - \\frac{1}{3} = \\frac{9-14}{42} = -\\frac{5}{42}$\n$(\\widehat{M} A)_{22} = \\frac{3}{28}(-1) + \\frac{1}{3}(2) + \\frac{3}{28}(-1) = -\\frac{6}{28} + \\frac{2}{3} = -\\frac{3}{14} + \\frac{2}{3} = \\frac{-9+28}{42} = \\frac{19}{42}$\n$(\\widehat{M} A)_{23} = \\frac{1}{3}(-1) + \\frac{3}{28}(2) = -\\frac{1}{3} + \\frac{3}{14} = \\frac{-14+9}{42} = -\\frac{5}{42}$\n$(\\widehat{M} A)_{31} = \\frac{3}{28}(-1) = -\\frac{3}{28}$\n$(\\widehat{M} A)_{32} = \\frac{3}{28}(2) + \\frac{4}{7}(-1) = \\frac{3}{14} - \\frac{4}{7} = \\frac{3-8}{14} = -\\frac{5}{14}$\n$(\\widehat{M} A)_{33} = \\frac{3}{28}(-1) + \\frac{4}{7}(2) = -\\frac{3}{28} + \\frac{8}{7} = \\frac{-3+32}{28} = \\frac{29}{28}$\nSo, the product is:\n$$\n\\widehat{M} A = \\begin{pmatrix}\n\\frac{29}{28}  -\\frac{5}{14}  -\\frac{3}{28} \\\\\n-\\frac{5}{42}  \\frac{19}{42}  -\\frac{5}{42} \\\\\n-\\frac{3}{28}  -\\frac{5}{14}  \\frac{29}{28}\n\\end{pmatrix}\n$$\n\n### Task 3: Justification for not using CG on $\\widehat{M}A$\n\nThe classical Conjugate Gradient (CG) method is designed to solve linear systems $B x = b$ where the operator $B$ is symmetric positive definite (SPD) in the Euclidean inner product. Applying CG to the left-preconditioned system with operator $\\widehat{M} A$ thus requires $\\widehat{M}A$ to be SPD.\n\nA necessary condition for a matrix to be SPD is that it must be symmetric. Let's check if $\\widehat{M} A$ is symmetric.\nThe transpose of the product is $(\\widehat{M} A)^\\top = A^\\top \\widehat{M}^\\top$.\nBy construction, $A$ is symmetric ($A^\\top = A$) and $\\widehat{M}$ is symmetric ($\\widehat{M}^\\top = \\widehat{M}$). Therefore, $(\\widehat{M} A)^\\top = A \\widehat{M}$.\nFor $\\widehat{M} A$ to be symmetric, we must have $\\widehat{M} A = A \\widehat{M}$, which means the matrices $\\widehat{M}$ and $A$ must commute.\n\nThe SPAI construction process aims to find a sparse matrix $M$ such that $MA \\approx I$, by solving independent row-wise least-squares problems. This procedure does not enforce any algebraic properties on the resulting preconditioner that would guarantee its commutation with $A$. The post-symmetrization step $\\widehat{M} = \\frac{1}{2}(M+M^\\top)$ enforces symmetry on the preconditioner itself, but it does not induce the commutation property $\\widehat{M}A = A\\widehat{M}$. In general, the product of two symmetric matrices is not symmetric unless they commute.\n\nFrom the result of Task 2, we can see that $\\widehat{M} A$ is not symmetric. For example, $(\\widehat{M} A)_{12} = -\\frac{5}{14}$ while $(\\widehat{M} A)_{21} = -\\frac{5}{42}$. Since $(\\widehat{M} A)_{12} \\neq (\\widehat{M} A)_{21}$, the matrix $\\widehat{M}A$ is not symmetric.\nBecause $\\widehat{M} A$ is not symmetric, it cannot be SPD. Therefore, applying the classical CG method, which fundamentally relies on the symmetry of the operator to guarantee its short-term recurrences and other properties, is not mathematically justified for the system with operator $\\widehat{M}A$. An iterative method designed for nonsymmetric systems, such as GMRES or BiCGSTAB, would be required.\n\n### Task 4: Quantitative Evidence of Asymmetry\n\nThe degree of non-symmetry can be quantified by the norm of the antisymmetric (or skew-symmetric) part of the matrix. We are asked to compute the Frobenius norm of $S = \\widehat{M} A - (\\widehat{M} A)^{\\top}$.\nUsing the result for $\\widehat{M} A$ from Task 2, and the fact that $(\\widehat{M} A)^{\\top} = A \\widehat{M}$:\n$$\n(\\widehat{M} A)^{\\top} = \\begin{pmatrix}\n\\frac{29}{28}  -\\frac{5}{42}  -\\frac{3}{28} \\\\\n-\\frac{5}{14}  \\frac{19}{42}  -\\frac{5}{14} \\\\\n-\\frac{3}{28}  -\\frac{5}{42}  \\frac{29}{28}\n\\end{pmatrix}\n$$\nThe difference matrix $S$ is:\n$$\nS = \\widehat{M} A - (\\widehat{M} A)^{\\top} = \\begin{pmatrix}\n0  -\\frac{5}{14} - (-\\frac{5}{42})  -\\frac{3}{28} - (-\\frac{3}{28}) \\\\\n-\\frac{5}{42} - (-\\frac{5}{14})  0  -\\frac{5}{42} - (-\\frac{5}{14}) \\\\\n-\\frac{3}{28} - (-\\frac{3}{28})  -\\frac{5}{14} - (-\\frac{5}{42})  0\n\\end{pmatrix}\n$$\nCalculating the non-zero entries:\n$S_{12} = -\\frac{15}{42} + \\frac{5}{42} = -\\frac{10}{42} = -\\frac{5}{21}$.\n$S_{21} = -\\frac{5}{42} + \\frac{15}{42} = \\frac{10}{42} = \\frac{5}{21}$.\n$S_{23} = -\\frac{5}{42} + \\frac{15}{42} = \\frac{10}{42} = \\frac{5}{21}$.\n$S_{32} = -\\frac{15}{42} + \\frac{5}{42} = -\\frac{10}{42} = -\\frac{5}{21}$.\nAll other entries are $0$. The resulting skew-symmetric matrix is:\n$$\nS = \\begin{pmatrix}\n0  -\\frac{5}{21}  0 \\\\\n\\frac{5}{21}  0  \\frac{5}{21} \\\\\n0  -\\frac{5}{21}  0\n\\end{pmatrix}\n$$\nThe Frobenius norm $\\|S\\|_{F}$ is the square root of the sum of the squares of its elements:\n$$\n\\|S\\|_{F}^2 = \\sum_{i,j} S_{ij}^2 = \\left(-\\frac{5}{21}\\right)^2 + \\left(\\frac{5}{21}\\right)^2 + \\left(\\frac{5}{21}\\right)^2 + \\left(-\\frac{5}{21}\\right)^2 = 4 \\left( \\frac{5}{21} \\right)^2\n$$\n$$\n\\|S\\|_{F}^2 = 4 \\left( \\frac{25}{441} \\right) = \\frac{100}{441}\n$$\nTaking the square root gives the Frobenius norm:\n$$\n\\|S\\|_{F} = \\sqrt{\\frac{100}{441}} = \\frac{10}{21}\n$$\nThis non-zero value provides quantitative evidence that $\\widehat{M} A$ is not symmetric.",
            "answer": "$$\n\\boxed{\\frac{10}{21}}\n$$"
        }
    ]
}