{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the mechanics of the Preconditioned Conjugate Gradient (PCG) method, there is no substitute for a direct, hands-on calculation. This first exercise guides you through a single iteration of the algorithm on a small-scale system, using a simple but effective diagonal (or Jacobi) preconditioner . By manually computing the residual, preconditioned residual, search direction, and step length, you will build an intuitive understanding of how these components interact to update the solution.",
            "id": "1029864",
            "problem": "Consider the linear system $ A \\mathbf{x} = \\mathbf{b} $, where  \n$$ A = \\begin{bmatrix} 4  -2  0 \\\\ -2  3  -1 \\\\ 0  -1  2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 2 \\end{bmatrix}. $$  \nApply the preconditioned conjugate gradient (PCG) method with diagonal scaling (Jacobi preconditioner), starting from the initial guess $ \\mathbf{x}_0 = \\mathbf{0} $. After one iteration, compute the 2-norm of the residual vector. The Jacobi preconditioner $ M $ is the diagonal part of $ A $, so $ M = \\operatorname{diag}(4, 3, 2) $.",
            "solution": "1. Initial residual  \n$$r_0 = b - A x_0 = \\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}.$$\n2. Preconditioned residual  \n$$M^{-1} = \\operatorname{diag}\\bigl(\\tfrac14,\\tfrac13,\\tfrac12\\bigr),\\qquad \nz_0 = M^{-1}r_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n3. Search direction  \n$$p_0 = z_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n4. Matrix–vector product  \n$$A p_0 = \\begin{bmatrix}4-20\\\\-23-1\\\\0-12\\end{bmatrix}\n\\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}\n=\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}.$$\n5. Step length  \n$$\\alpha_0 = \\frac{r_0^T z_0}{p_0^T A p_0}\n=\\frac{9}{15}=\\frac35.$$\n6. Updated residual  \n$$r_1 = r_0 - \\alpha_0 A p_0\n=\\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}\n-\\frac35\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}\n=\\begin{bmatrix}0.4\\\\0.6\\\\0.2\\end{bmatrix}.$$\n7. 2-norm of the residual  \n$$\\|r_1\\|_2 = \\sqrt{0.4^2 + 0.6^2 + 0.2^2}\n= \\frac{\\sqrt{14}}{5}.$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{14}}{5}}$$"
        },
        {
            "introduction": "While manual calculations are insightful, the true power of PCG is revealed when solving large-scale systems common in scientific computing. This practice transitions from theory to application by tasking you with implementing the PCG algorithm alongside a highly effective preconditioner: the Incomplete Cholesky factorization with zero fill-in, or $\\mathrm{IC(0)}$ . You will compare its performance against the standard Conjugate Gradient method on various test problems, providing a clear, empirical demonstration of how a well-chosen preconditioner can drastically reduce the number of iterations required for convergence.",
            "id": "3244793",
            "problem": "Consider solving a large, sparse, Symmetric Positive Definite (SPD) linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is sparse and $b \\in \\mathbb{R}^{n}$ is given. From first principles, the Conjugate Gradient (CG) method solves such systems by iteratively minimizing the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$ over the Krylov subspace generated by the residuals. Preconditioning aims to accelerate convergence by transforming the problem into $M^{-1} A x = M^{-1} b$ with a symmetric positive definite preconditioner $M$, thereby improving the spectral properties encountered by CG. One widely used preconditioner for SPD matrices is the Incomplete Cholesky factorization with zero fill-in, denoted $\\mathrm{IC(0)}$, where the factor $L$ obeys $M = L L^{\\top}$ and has the same lower-triangular sparsity pattern as $A$ while discarding all fill-in entries that would appear in a complete Cholesky factorization.\n\nYour task is to write a complete, runnable program that:\n- Implements $\\mathrm{IC(0)}$ for an SPD matrix $A$ by computing a lower-triangular matrix $L$ that adheres strictly to the lower-triangular sparsity pattern of $A$ and satisfies $M \\approx A$ with $M = L L^{\\top}$.\n- Implements the Conjugate Gradient (CG) algorithm and the Preconditioned Conjugate Gradient (PCG) algorithm using $M$ as the preconditioner, with $M$ applied by solving two triangular systems with $L$ and $L^{\\top}$.\n- Uses the same initialization $x_0 = 0$ and the same stopping criterion for CG and PCG, namely, the first iteration $k$ where the relative residual $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$, with tolerance $\\varepsilon = 10^{-8}$, or when the maximum number of iterations $k_{\\max} = 1000$ is reached.\n\nFundamental bases permitted for reasoning and implementation in this task include:\n- The definition and properties of Symmetric Positive Definite (SPD) matrices.\n- The definition of the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$ minimized by CG.\n- The concept that preconditioning seeks to improve the conditioning of $A$ by approximating $A$ with $M$ such that $M^{-1} A$ has more favorable spectral properties.\n- The standard notion of Cholesky factorization $A = L L^{\\top}$ for SPD matrices and the idea of incomplete factorizations that maintain chosen sparsity patterns.\n\nYou must not rely on any external files, user input, or libraries other than the specified ones. The program should be self-contained.\n\nTest Suite:\nImplement and run the following four test cases, each with $b$ set to the all-ones vector of appropriate dimension:\n1. Happy path case: One-dimensional Poisson-type tridiagonal SPD matrix of size $n = 50$ with stencil $[ -1, 2, -1 ]$.\n2. Two-dimensional Poisson-type SPD matrix on a square grid of size $n \\times n$ with $n = 10$ (overall dimension $100$), assembled via the $5$-point stencil using Kronecker sums of the one-dimensional operator.\n3. Ill-conditioned but SPD tridiagonal matrix of size $n = 80$ with off-diagonals $-c_i$ where $c_i$ varies geometrically from $10^{-3}$ to $10^{0}$ and the diagonal set to guarantee strict diagonal dominance and symmetry, thereby ensuring SPD while introducing ill-conditioning.\n4. Boundary case: Scalar SPD system with $A = [4]$ (that is, $n = 1$).\n\nFor all cases, use $x_0 = 0$, tolerance $\\varepsilon = 10^{-8}$, and $k_{\\max} = 1000$.\n\nRequired Outputs:\n- For each test case, compute $k_{\\mathrm{CG}}$ and $k_{\\mathrm{PCG}}$, the number of iterations taken by the Conjugate Gradient and Preconditioned Conjugate Gradient methods, respectively, to satisfy the stopping criterion or to reach $k_{\\max}$.\n- The final output must be a single line containing the list of integer differences $k_{\\mathrm{CG}} - k_{\\mathrm{PCG}}$ for the four test cases, in order, as a comma-separated list enclosed in square brackets, for example, $[d_1,d_2,d_3,d_4]$.\n\nAngle units are not applicable. No physical units are involved. Percentages must not be used; all tolerances and quantities are to be treated numerically as specified.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[d_1,d_2,d_3,d_4]$).",
            "solution": "The user requests the implementation and comparison of the Conjugate Gradient (CG) and Preconditioned Conjugate Gradient (PCG) methods for solving symmetric positive definite (SPD) linear systems. The preconditioner specified is the Incomplete Cholesky factorization with zero fill-in, $\\mathrm{IC(0)}$. The task requires implementing these algorithms from first principles and applying them to a suite of four test cases. The final output must be the difference in the number of iterations, $k_{\\mathrm{CG}} - k_{\\mathrm{PCG}}$, for each case.\n\n### Step 1: Problem Validation\n\nThe problem statement has been critically reviewed and is determined to be **valid**.\n\n- **Givens Extraction**: All parameters, algorithms, and test cases are explicitly and unambiguously defined. The problem requires implementing $\\mathrm{IC(0)}$, CG, and PCG, specifies the initial guess $x_0 = 0$, the stopping criterion $\\|r_k\\|_2 / \\|b\\|_2 \\leq 10^{-8}$, and a maximum iteration count of $1000$. Four distinct test cases for the matrix $A$ and vector $b$ are described in detail.\n- **Scientific Grounding**: The problem is firmly rooted in the established field of numerical linear algebra. CG, PCG, and $\\mathrm{IC(0)}$ are standard, well-documented algorithms for solving large sparse linear systems. The underlying principles, such as the minimization of the quadratic functional $\\phi(x)$ and the role of preconditioning in improving spectral properties, are fundamental concepts.\n- **Well-Posedness and Objectivity**: The problem is well-posed. For SPD matrices, a unique solution exists, and both CG and PCG are guaranteed to converge. The test matrices are constructed to be SPD. The problem is stated with objective, formal language, and all specifications are quantitative and precise.\n- **Conclusion**: The problem is a standard exercise in scientific computing, free of any scientific, logical, or structural flaws. It is complete, consistent, and verifiable.\n\n### Step 2: Algorithmic Design and Implementation\n\nThe solution will be structured into three main algorithmic components and a test framework.\n\n#### 1. Incomplete Cholesky Factorization ($\\mathrm{IC(0)}$)\n\nThe $\\mathrm{IC(0)}$ factorization of an SPD matrix $A$ produces a lower-triangular matrix $L$ such that $M = LL^{\\top}$ approximates $A$. The defining characteristic of $\\mathrm{IC(0)}$ is that the sparsity pattern of $L$ is identical to the lower-triangular part of $A$. Any \"fill-in\"—elements that would become non-zero in a full Cholesky factorization but are zero in $A$—are discarded.\n\nThe algorithm to compute $L$ proceeds as follows, populating $L$ row by row (or column by column). For each element $L_{ij}$ with $i \\geq j$:\n1.  The computation is only performed if $A_{ij}$ is non-zero (respecting the sparsity pattern).\n2.  The value is computed based on previously determined elements of $L$. For an element $L_{ij}$:\n    $$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1, L_{ik}\\neq 0, L_{jk}\\neq 0} L_{ik}L_{jk} \\right) \\quad \\text{for } i  j $$\n    $$ L_{ii} = \\sqrt{A_{ii} - \\sum_{k=0}^{i-1, L_{ik}\\neq 0} L_{ik}^2} $$\nThe sums are restricted to indices $k$ where the corresponding entries in $L$ are non-zero, thereby preventing fill-in. For the SPD matrices in the test suite, this factorization is guaranteed to not break down (i.e., the argument of the square root will remain positive).\n\n#### 2. Conjugate Gradient (CG) Algorithm\n\nThe standard CG algorithm is an iterative method for solving $Ax=b$. It minimizes the A-norm of the error by generating a sequence of A-orthogonal search directions.\n\nThe algorithm is initialized with $x_0 = 0$, $r_0 = b - Ax_0 = b$, and $p_0 = r_0$. For each iteration $k = 0, 1, 2, \\dots$:\n1. Check for convergence: If $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$, terminate.\n2. Calculate step size: $\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$.\n3. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n4. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n5. Update search direction: $\\beta_k = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}$, then $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n\nThe number of iterations, $k$, is returned upon convergence or reaching the maximum limit.\n\n#### 3. Preconditioned Conjugate Gradient (PCG) Algorithm\n\nPCG accelerates CG by using a preconditioner $M \\approx A$ to solve the transformed system $M^{-1}Ax = M^{-1}b$. Here, $M=LL^{\\top}$ from the $\\mathrm{IC(0)}$ factorization. The application of $M^{-1}$ involves solving a system $Mz=r$, which is efficiently done in two steps: a forward substitution to solve $Ly=r$, followed by a backward substitution to solve $L^{\\top}z=y$. `numpy.linalg.solve` is used for this step as it automatically detects triangular matrices and employs the appropriate efficient solver.\n\nThe PCG algorithm modifies CG by incorporating the solution of $Mz_k=r_k$ at each step.\n\nInitialize with $x_0 = 0$, $r_0 = b - Ax_0$. For each iteration $k = 0, 1, 2, \\dots$:\n1. Check for convergence: If $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$, terminate.\n2. Solve preconditioning system: $M z_k = r_k$.\n3. Update search direction: $\\beta_{k-1} = \\frac{r_k^{\\top} z_k}{r_{k-1}^{\\top} z_{k-1}}$, then $p_k = z_k + \\beta_{k-1} p_{k-1}$. For $k=0$, $p_0 = z_0$.\n4. Calculate step size: $\\alpha_k = \\frac{r_k^{\\top} z_k}{p_k^{\\top} A p_k}$.\n5. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n6. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n\nThe iteration count $k$ is tracked and returned consistently with the CG implementation.\n\n### Step 3: Test Cases and Execution\n\nThe program will execute the following four test cases, with $x_0=0$, $\\varepsilon=10^{-8}$, and $k_{\\max}=1000$:\n\n1.  **1D Poisson Matrix**: A tridiagonal matrix of size $n=50$ with diagonal entries of $2$ and off-diagonal entries of $-1$.\n2.  **2D Poisson Matrix**: A matrix of size $N=100$ representing the 5-point stencil on a $10 \\times 10$ grid, constructed using Kronecker products of the 1D Poisson operator.\n3.  **Ill-Conditioned Matrix**: A tridiagonal matrix of size $n=80$. The off-diagonal entries vary geometrically, creating a range of local condition numbers. The diagonal is set to ensure the matrix is strictly diagonally dominant and thus SPD.\n4.  **Scalar Case**: A $1 \\times 1$ system with $A=[4]$, serving as a simple boundary-case check.\n\nFor each case, $b$ is the vector of all ones. The difference in iteration counts, $k_{\\mathrm{CG}} - k_{\\mathrm{PCG}}$, is computed and collected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ic0_factorization(A):\n    \"\"\"\n    Computes the Incomplete Cholesky factorization with zero fill-in (IC(0)).\n    The factor L has the same lower-triangular sparsity pattern as A.\n    \"\"\"\n    n = A.shape[0]\n    L = np.zeros_like(A, dtype=float)\n\n    for i in range(n):\n        for j in range(i + 1):\n            if A[i, j] == 0 and i != j:\n                continue\n\n            s = 0.0\n            # The dot product L[i, :j] @ L[j, :j] is implicitly sparse due to\n            # how L is constructed, respecting the sparsity of A.\n            for k in range(j):\n                if L[i,k] != 0 and L[j,k] != 0:\n                   s += L[i, k] * L[j, k]\n\n            val = A[i, j] - s\n            if i == j:\n                # For the specified SPD matrices, val should be positive.\n                # A small perturbation is added for robustness against floating point errors.\n                if val = 0:\n                    val = 1e-12\n                L[i, j] = np.sqrt(val)\n            else:\n                L[i, j] = val / L[j, j]\n    return L\n\ndef conjugate_gradient(A, b, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax = b using the Conjugate Gradient method.\n    Returns the number of iterations taken.\n    \"\"\"\n    k = 0\n    x = np.copy(x0).astype(float)\n    r = b - A @ x\n    p = np.copy(r)\n    rs_old = r @ r\n\n    norm_b = np.linalg.norm(b)\n    if norm_b == 0:\n        norm_b = 1.0\n    \n    # Check initial residual\n    if np.sqrt(rs_old) / norm_b = tol:\n        return 0\n\n    while k  max_iter:\n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = r @ r\n\n        if np.sqrt(rs_new) / norm_b = tol:\n            return k + 1\n        \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        k += 1\n\n    return max_iter\n\ndef preconditioned_conjugate_gradient(A, b, x0, L, tol, max_iter):\n    \"\"\"\n    Solves Ax = b using the Preconditioned Conjugate Gradient method\n    with IC(0) preconditioner M = LL^T. Returns the number of iterations.\n    \"\"\"\n    k = 0\n    x = np.copy(x0).astype(float)\n    r = b - A @ x\n\n    norm_b = np.linalg.norm(b)\n    if norm_b == 0:\n        norm_b = 1.0\n\n    if np.linalg.norm(r) / norm_b = tol:\n        return 0\n    \n    L_T = L.T\n\n    def solve_M(v):\n        y = np.linalg.solve(L, v)\n        z = np.linalg.solve(L_T, y)\n        return z\n\n    z = solve_M(r)\n    p = np.copy(z)\n    rz_old = r @ z\n\n    while k  max_iter:\n        Ap = A @ p\n        alpha = rz_old / (p @ Ap)\n        x += alpha * p\n        r -= alpha * Ap\n\n        if np.linalg.norm(r) / norm_b = tol:\n            return k + 1\n\n        z = solve_M(r)\n        rz_new = r @ z\n        beta = rz_new / rz_old\n        p = z + beta * p\n        rz_old = rz_new\n        k += 1\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run solvers, and print results.\n    \"\"\"\n    test_cases = []\n\n    # Test Case 1: 1D Poisson matrix\n    n1 = 50\n    A1 = np.diag([2.0] * n1) + np.diag([-1.0] * (n1 - 1), k=1) + np.diag([-1.0] * (n1 - 1), k=-1)\n    b1 = np.ones(n1)\n    test_cases.append({'A': A1, 'b': b1})\n\n    # Test Case 2: 2D Poisson matrix\n    n_grid = 10\n    N2 = n_grid * n_grid\n    T_n = np.diag([2.0] * n_grid) + np.diag([-1.0] * (n_grid - 1), k=1) + np.diag([-1.0] * (n_grid - 1), k=-1)\n    I_n = np.eye(n_grid)\n    A2 = np.kron(I_n, T_n) + np.kron(T_n, I_n)\n    b2 = np.ones(N2)\n    test_cases.append({'A': A2, 'b': b2})\n\n    # Test Case 3: Ill-conditioned tridiagonal matrix\n    n3 = 80\n    c = np.geomspace(1e-3, 1.0, num=n3 - 1)\n    A3 = np.diag(-c, k=1) + np.diag(-c, k=-1)\n    d = np.zeros(n3)\n    delta = 0.1  # For strict diagonal dominance\n    d[0] = c[0] + delta\n    d[n3-1] = c[n3-2] + delta\n    d[1:n3-1] = c[0:n3-2] + c[1:n3-1] + delta\n    A3 += np.diag(d)\n    b3 = np.ones(n3)\n    test_cases.append({'A': A3, 'b': b3})\n    \n    # Test Case 4: Scalar SPD system\n    A4 = np.array([[4.0]])\n    b4 = np.array([1.0])\n    test_cases.append({'A': A4, 'b': b4})\n    \n    # Common parameters\n    tol = 1e-8\n    max_iter = 1000\n\n    results = []\n    for case in test_cases:\n        A = case['A']\n        b = case['b']\n        n = A.shape[0]\n        x0 = np.zeros(n)\n\n        # Run Conjugate Gradient\n        k_cg = conjugate_gradient(A, b, x0, tol, max_iter)\n\n        # Compute IC(0) and run Preconditioned Conjugate Gradient\n        L = ic0_factorization(A)\n        k_pcg = preconditioned_conjugate_gradient(A, b, x0, L, tol, max_iter)\n        \n        results.append(k_cg - k_pcg)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having witnessed the practical benefits of preconditioning, we now explore the fundamental theory that explains *why* it works. The convergence rate of the Conjugate Gradient method is intrinsically linked to the distribution of eigenvalues of the system matrix. This thought experiment challenges you to construct a specific system where the preconditioned operator has a minimal number of distinct eigenvalues . In doing so, you will provide a rigorous justification for the algorithm's guaranteed, rapid convergence, illustrating the core principle that preconditioning aims to cluster eigenvalues.",
            "id": "2427437",
            "problem": "Consider solving a linear system with the Conjugate Gradient (CG) method and its preconditioned variant. Construct explicit matrices $A \\in \\mathbb{R}^{5 \\times 5}$ and $M \\in \\mathbb{R}^{5 \\times 5}$ that are both symmetric positive definite, such that the preconditioned operator $M^{-1}A$ has exactly $2$ distinct eigenvalues. Verify the eigenvalue property by direct reasoning from your construction. Then consider applying the Preconditioned Conjugate Gradient (PCG) method, with preconditioner $M$, to the system $A x = b$ for arbitrary $b \\in \\mathbb{R}^{5}$ and arbitrary initial guess $x_0 \\in \\mathbb{R}^{5}$. Provide a rigorous justification, based on first principles of linear algebra and the algorithm’s defining properties, that PCG terminates with the exact solution in a bounded number of iterations that does not depend on $b$ or $x_0$. Determine the minimal integer $k^\\star$ with this property. \n\nYour final answer must be the value of $k^\\star$ only. No rounding is required.",
            "solution": "The problem requires us to construct specific symmetric positive definite (SPD) matrices $A$ and $M$ of size $5 \\times 5$ such that the preconditioned operator $M^{-1}A$ possesses exactly $2$ distinct eigenvalues. Subsequently, we must provide a rigorous justification for the fact that the Preconditioned Conjugate Gradient (PCG) method for the system $A x = b$ converges to the exact solution in a finite number of iterations, $k^\\star$, which is independent of the choice of the right-hand side $b$ and the initial guess $x_0$. Finally, we must determine this minimal bound $k^\\star$.\n\nFirst, we construct the required matrices $A$ and $M$. A simple and effective construction involves diagonal matrices. Let $M$ be the $5 \\times 5$ identity matrix, $M=I_5$. The identity matrix is symmetric, and all its eigenvalues are $1$, so it is positive definite.\nNext, let us construct the matrix $A$. To ensure $M^{-1}A$ has two distinct eigenvalues, and given our choice of $M=I_5$, the matrix $A$ itself must have two distinct eigenvalues. We also need $A$ to be symmetric and positive definite. A diagonal matrix with positive entries on the diagonal satisfies these requirements. We can choose:\n$$\nA = \\text{diag}(1, 1, 1, 2, 2)\n$$\nThis matrix $A$ is symmetric by construction. Its eigenvalues are its diagonal entries, which are $1$ and $2$. Since all eigenvalues are positive, $A$ is positive definite.\nThe preconditioned operator is $M^{-1}A = I_5^{-1}A = A$. The eigenvalues of $M^{-1}A$ are therefore $\\{1, 1, 1, 2, 2\\}$. The set of distinct eigenvalues is $\\{\\lambda_1, \\lambda_2\\} = \\{1, 2\\}$. Thus, there are exactly $2$ distinct eigenvalues, as required by the problem statement. Our construction of $A$ and $M$ is valid.\n\nNow, we must analyze the convergence of the PCG method. The PCG algorithm for solving the system $A x = b$ with an SPD preconditioner $M$ is mathematically equivalent to applying the standard Conjugate Gradient (CG) algorithm to a transformed linear system. Since $M$ is SPD, it has a unique Cholesky factorization $M = L L^T$, where $L$ is a nonsingular lower triangular matrix.\n\nWe can transform the original system $A x = b$ as follows:\n$$\nA x = b \\implies (L^{-1} A L^{-T}) (L^T x) = L^{-1} b\n$$\nLet us define $\\hat{A} = L^{-1} A L^{-T}$, $\\hat{x} = L^T x$, and $\\hat{b} = L^{-1}b$. The system becomes $\\hat{A} \\hat{x} = \\hat{b}$.\nThe matrix $\\hat{A}$ is SPD. It is symmetric because $A$ is symmetric:\n$$\n\\hat{A}^T = (L^{-1} A L^{-T})^T = (L^{-T})^T A^T (L^{-1})^T = L^{-1} A L^{-T} = \\hat{A}\n$$\nIt is positive definite because $A$ is SPD and $L^{-T}$ is nonsingular. For any non-zero vector $y \\in \\mathbb{R}^5$, let $z = L^{-T}y$. Since $L^{-T}$ is nonsingular, $z \\neq 0$. Then:\n$$\ny^T \\hat{A} y = y^T (L^{-1} A L^{-T}) y = (L^{-T}y)^T A (L^{-T}y) = z^T A z  0\n$$\nThe PCG algorithm applied to $A x = b$ is designed such that the sequence of iterates $x_k$ it generates corresponds to the sequence of iterates $\\hat{x}_k = L^T x_k$ generated by the standard CG algorithm applied to $\\hat{A} \\hat{x} = \\hat{b}$.\n\nA fundamental theorem of the CG method states that the algorithm terminates with the exact solution in at most $m$ iterations, where $m$ is the number of distinct eigenvalues of the system matrix. This property arises because the error $e_k = x - x_k$ can be expressed as $e_k = P_k(A) e_0$ for some polynomial $P_k$ of degree $k$ with $P_k(0)=1$. CG finds the polynomial that minimizes the $A$-norm of the error. If the matrix has $m$ distinct eigenvalues $\\{\\mu_1, \\dots, \\mu_m\\}$, one can construct a polynomial $Q(t) = \\prod_{i=1}^m (1 - t/\\mu_i)$ of degree $m$ that vanishes at all eigenvalues and satisfies $Q(0)=1$. The CG algorithm finds this polynomial by step $m$, resulting in a zero error. This holds for any initial guess $x_0$ and right hand side $b$.\n\nFor our preconditioned system, the relevant system matrix is $\\hat{A}$. We need to determine the number of distinct eigenvalues of $\\hat{A}$. The matrices $\\hat{A}$ and $M^{-1}A$ are similar, which means they share the same eigenvalues. We can show this similarity transformation explicitly:\n$$\n\\hat{A} = L^{-1} A L^{-T} = L^{-1} (M M^{-1}) A L^{-T} = L^{-1} (L L^T) (M^{-1}A) (L^T)^{-1} = (L^{-1}L) L^T (M^{-1}A) (L^T)^{-1} = L^T (M^{-1}A) (L^T)^{-1}\n$$\nSince $\\hat{A}$ is a similarity transformation of $M^{-1}A$, they have the same characteristic polynomial and thus the same eigenvalues.\n\nFrom our construction, the preconditioned matrix $M^{-1}A$ has exactly $2$ distinct eigenvalues. Consequently, the transformed matrix $\\hat{A}$ also has exactly $2$ distinct eigenvalues.\n\nTherefore, applying the standard CG convergence theorem to the system $\\hat{A} \\hat{x} = \\hat{b}$, the algorithm is guaranteed to find the exact solution $\\hat{x}$ in at most $2$ iterations. As $\\hat{x}_k = L^T x_k$ and $L$ is nonsingular, if $\\hat{x}_k = \\hat{x}$, then $x_k = x$. This convergence in a maximum of $2$ steps is guaranteed for any initial guess $\\hat{x}_0 = L^T x_0$ and any right-hand side $\\hat{b} = L^{-1} b$, which is equivalent to any $x_0$ and $b$ since $L$ is invertible. While for specific initial conditions convergence may occur in $1$ iteration (if the initial residual is an eigenvector of $\\hat{A}$), the bound must hold for arbitrary inputs. The worst-case scenario, which dictates the bound, requires that the initial residual has components in the eigenspaces of all distinct eigenvalues.\n\nThe minimal integer $k^\\star$ that bounds the number of iterations for any $b$ and $x_0$ is therefore the number of distinct eigenvalues of the preconditioned operator $M^{-1}A$. In this problem, this number is $2$.\nSo, $k^\\star = 2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        }
    ]
}