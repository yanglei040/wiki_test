{
    "hands_on_practices": [
        {
            "introduction": "To begin, we must ground our understanding of preconditioned GMRES in its fundamental components. This first practice requires the manual construction of a preconditioned Krylov subspace, the very search space from which the algorithm finds its approximate solutions . By explicitly calculating the preconditioned operator $AM^{-1}$ and the first few vectors of the Krylov sequence, you will demystify the abstract definitions and build a concrete intuition for how preconditioning reshapes the problem.",
            "id": "3593967",
            "problem": "Consider the linear system $A x = b$ with a right preconditioner $M$, where the matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is nonsymmetric and the matrix $M \\in \\mathbb{R}^{3 \\times 3}$ is nonsingular. Let\n$$\nA = \\begin{pmatrix}\n4 & -1 & 0 \\\\\n2 & 3 & 1 \\\\\n0 & -2 & 5\n\\end{pmatrix}, \\quad\nM = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n3 \\\\ 7 \\\\ -1\n\\end{pmatrix}, \\quad\nx_0 = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nDefine the initial residual $r_0 = b - A x_0$ and the operator $A M^{-1}$ associated with right preconditioning. Using only fundamental definitions of the Krylov subspace and the Generalized Minimal Residual (GMRES) algorithm, explicitly construct the second Krylov subspace $\\mathcal{K}_2(A M^{-1}, r_0)$ and describe the affine solution space explored by right-preconditioned GMRES up to two iterations. You must derive the subspace elements directly from the data above without invoking any pre-derived shortcut formulas.\n\nAdditionally, determine the dimension of $\\mathcal{K}_2(A M^{-1}, r_0)$ and provide this dimension as your final answer.",
            "solution": "The right-preconditioned GMRES algorithm addresses the linear system $A x = b$ by solving the modified system $(A M^{-1})y = b$ and then recovering the solution via $x = M^{-1}y$. The approximations for $y$ are found within an affine space constructed from Krylov subspaces.\n\nThe first step is to calculate the initial residual $r_0$.\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 4 & -1 & 0 \\\\ 2 & 3 & 1 \\\\ 0 & -2 & 5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix}\n$$\nThis vector $r_0$ is the starting vector for building the Krylov subspace.\n\nNext, we need the operator for the Krylov subspace, which is $A M^{-1}$. We first find the inverse of $M$.\n$$\nM = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe inverse $M^{-1}$ can be found using Gaussian elimination on the augmented matrix $[M|I]$:\n$$\n\\left[\\begin{array}{ccc|ccc} 2 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 1 \\end{array}\\right] \\xrightarrow{R_1 \\to \\frac{1}{2}R_1} \\left[\\begin{array}{ccc|ccc} 1 & 0 & 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 1 \\end{array}\\right] \\xrightarrow{R_2 \\to R_2-R_3} \\left[\\begin{array}{ccc|ccc} 1 & 0 & 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 1 & -1 \\\\ 0 & 0 & 1 & 0 & 0 & 1 \\end{array}\\right]\n$$\nThus, the inverse is:\n$$\nM^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nNow, we compute the operator $A M^{-1}$:\n$$\nA M^{-1} = \\begin{pmatrix} 4 & -1 & 0 \\\\ 2 & 3 & 1 \\\\ 0 & -2 & 5 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4(\\frac{1}{2}) & -1(1) & -1(-1) \\\\ 2(\\frac{1}{2}) & 3(1) & 3(-1)+1(1) \\\\ 0 & -2(1) & -2(-1)+5(1) \\end{pmatrix} = \\begin{pmatrix} 2 & -1 & 1 \\\\ 1 & 3 & -2 \\\\ 0 & -2 & 7 \\end{pmatrix}\n$$\nThe second Krylov subspace, $\\mathcal{K}_2(A M^{-1}, r_0)$, is defined by the span of the first two vectors in the Krylov sequence:\n$$\n\\mathcal{K}_2(A M^{-1}, r_0) = \\text{span}\\{r_0, (A M^{-1}) r_0 \\}\n$$\nThe first vector is $r_0 = \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix}$.\nThe second vector is obtained by applying the operator $A M^{-1}$ to $r_0$:\n$$\n(A M^{-1}) r_0 = \\begin{pmatrix} 2 & -1 & 1 \\\\ 1 & 3 & -2 \\\\ 0 & -2 & 7 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(3) - 1(7) + 1(-1) \\\\ 1(3) + 3(7) - 2(-1) \\\\ 0(3) - 2(7) + 7(-1) \\end{pmatrix} = \\begin{pmatrix} 6 - 7 - 1 \\\\ 3 + 21 + 2 \\\\ -14 - 7 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 26 \\\\ -21 \\end{pmatrix}\n$$\nSo, the second Krylov subspace is explicitly constructed as:\n$$\n\\mathcal{K}_2(A M^{-1}, r_0) = \\text{span}\\left\\{ \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix}, \\begin{pmatrix} -2 \\\\ 26 \\\\ -21 \\end{pmatrix} \\right\\}\n$$\nThe GMRES algorithm finds an approximate solution $x_k$ in the affine space $x_0 + M^{-1}\\mathcal{K}_k(A M^{-1}, r_0)$. For $k=2$, the solution $x_2$ lies in the space:\n$$\nx_2 \\in x_0 + M^{-1}\\mathcal{K}_2(A M^{-1}, r_0)\n$$\nSince $x_0 = 0$, this is the linear subspace $M^{-1}\\mathcal{K}_2(A M^{-1}, r_0)$. This subspace is spanned by the images of the basis vectors of $\\mathcal{K}_2(A M^{-1}, r_0)$ under the transformation $M^{-1}$:\n$$\nM^{-1}\\mathcal{K}_2(A M^{-1}, r_0) = \\text{span}\\{ M^{-1}r_0, M^{-1}((A M^{-1})r_0) \\}\n$$\nLet's compute these two spanning vectors:\n$$\nM^{-1}r_0 = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ 7 - (-1) \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ 8 \\\\ -1 \\end{pmatrix}\n$$\n$$\nM^{-1}((A M^{-1})r_0) = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ 0 & 1 & -1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 26 \\\\ -21 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 26 - (-21) \\\\ -21 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 47 \\\\ -21 \\end{pmatrix}\n$$\nTherefore, the affine solution space explored by right-preconditioned GMRES up to two iterations is the set of all vectors $x$ of the form:\n$$\nx = c_1 \\begin{pmatrix} \\frac{3}{2} \\\\ 8 \\\\ -1 \\end{pmatrix} + c_2 \\begin{pmatrix} -1 \\\\ 47 \\\\ -21 \\end{pmatrix}\n$$\nwhere $c_1, c_2 \\in \\mathbb{R}$ are arbitrary scalars. This set is the linear subspace $\\text{span}\\left\\{ \\begin{pmatrix} \\frac{3}{2} \\\\ 8 \\\\ -1 \\end{pmatrix}, \\begin{pmatrix} -1 \\\\ 47 \\\\ -21 \\end{pmatrix} \\right\\}$.\n\nFinally, we determine the dimension of $\\mathcal{K}_2(A M^{-1}, r_0)$. The dimension is the number of linearly independent vectors in its spanning set $\\{r_0, (A M^{-1})r_0\\}$. Let $v_1 = r_0$ and $v_2 = (A M^{-1})r_0$. The vectors are linearly independent if and only if one is not a scalar multiple of the other. Let's check if there exists a scalar $c$ such that $v_2 = c v_1$:\n$$\n\\begin{pmatrix} -2 \\\\ 26 \\\\ -21 \\end{pmatrix} = c \\begin{pmatrix} 3 \\\\ 7 \\\\ -1 \\end{pmatrix}\n$$\nFrom the first component, we would have $-2 = 3c$, which implies $c = -\\frac{2}{3}$.\nFrom the second component, we would have $26 = 7c$, which implies $c = \\frac{26}{7}$.\nSince $-\\frac{2}{3} \\neq \\frac{26}{7}$, there is no such scalar $c$. The vectors $v_1$ and $v_2$ are linearly independent.\nThus, the dimension of the subspace $\\mathcal{K}_2(A M^{-1}, r_0)$ is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "While the Krylov subspace defines where GMRES searches for a solution, the Arnoldi process dictates *how* that search is conducted. This exercise focuses on the engine of GMRES, guiding you through the first two steps of the Arnoldi iteration for a preconditioned system . Performing this calculation will solidify your understanding of how the algorithm builds an orthonormal basis and the corresponding upper Hessenberg matrix $\\bar{H}_k$, which are the essential ingredients for the residual-minimizing projection step.",
            "id": "3593935",
            "problem": "Consider the Generalized Minimal Residual (GMRES) method with right preconditioning applied to the linear system $A x = b$, where the preconditioner $M$ is nonsingular. Right preconditioning transforms the operator to $\\tilde{A} = A M^{-1}$ and the unknown to $y = M x$, so that the Krylov subspaces are built for $\\tilde{A}$ acting on the initial residual $r_0 = b - A x_0$. The Arnoldi process constructs an orthonormal basis $\\{v_1, v_2, \\dots\\}$ for the Krylov subspaces $\\mathcal{K}_k(\\tilde{A}, r_0)$ and produces an upper Hessenberg matrix $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$ that captures the projection of $\\tilde{A}$ onto the generated subspace.\n\nLet\n$$\nA = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 4\n\\end{pmatrix}, \\quad\nM = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting with $r_0 = b - A x_0$ and $v_1 = r_0 / \\|r_0\\|$, carry out exactly two steps of the Arnoldi process for the right-preconditioned operator $\\tilde{A} = A M^{-1}$. Compute the orthonormal basis vectors $v_1$, $v_2$, and $v_3$, and the upper Hessenberg matrix\n$$\n\\bar{H}_2 = \\begin{pmatrix}\nh_{11} & h_{12} \\\\\nh_{21} & h_{22} \\\\\n0 & h_{32}\n\\end{pmatrix}\n$$\nsatisfying $\\tilde{A} V_2 = V_3 \\bar{H}_2$, where $V_2 = [v_1 \\; v_2]$ and $V_3 = [v_1 \\; v_2 \\; v_3]$.\n\nReport the exact value of the subdiagonal coefficient $h_{32}$ as your final answer. Express your reported value exactly; no rounding is permitted.",
            "solution": "First, we define the components of the problem. The linear system is $A x = b$ with a right preconditioner $M$. The GMRES algorithm will be applied to the equivalent system $\\tilde{A} y = b$, where $\\tilde{A} = A M^{-1}$ and $y = M x$.\n\nThe given matrices and vectors are:\n$$\nA = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 4\n\\end{pmatrix}, \\quad\nM = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe preconditioner $M$ is a diagonal matrix, so its inverse $M^{-1}$ is found by taking the reciprocal of its diagonal entries:\n$$\nM^{-1} = \\begin{pmatrix}\n1^{-1} & 0 & 0 \\\\\n0 & 2^{-1} & 0 \\\\\n0 & 0 & 1^{-1}\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & \\frac{1}{2} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nNow, we compute the preconditioned matrix $\\tilde{A} = A M^{-1}$:\n$$\n\\tilde{A} = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 4\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & \\frac{1}{2} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} =\n\\begin{pmatrix}\n(2)(1) & (1)(\\frac{1}{2}) & (0)(1) \\\\\n(0)(1) & (3)(\\frac{1}{2}) & (1)(1) \\\\\n(0)(1) & (0)(\\frac{1}{2}) & (4)(1)\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 & \\frac{1}{2} & 0 \\\\\n0 & \\frac{3}{2} & 1 \\\\\n0 & 0 & 4\n\\end{pmatrix}\n$$\nThe Arnoldi process begins with the initial residual $r_0$.\n$$\nr_0 = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe first basis vector $v_1$ is the normalized initial residual.\n$$\n\\|r_0\\|_2 = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\n$$\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe Arnoldi process iteratively builds an orthonormal basis $\\{v_1, v_2, \\dots\\}$ for the Krylov subspace $\\mathcal{K}_k(\\tilde{A}, r_0)$ and a Hessenberg matrix $\\bar{H}_k$. The relation is $\\tilde{A} V_k = V_{k+1} \\bar{H}_k$.\n\n**Step 1 of Arnoldi:**\nWe compute $w = \\tilde{A} v_1$.\n$$\nw = \\begin{pmatrix}\n2 & \\frac{1}{2} & 0 \\\\\n0 & \\frac{3}{2} & 1 \\\\\n0 & 0 & 4\n\\end{pmatrix}\n\\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right)\n= \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2 + \\frac{1}{2} \\\\ \\frac{3}{2} + 1 \\\\ 4 \\end{pmatrix}\n= \\frac{1}{\\sqrt{3}} \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{5}{2} \\\\ 4 \\end{pmatrix}\n$$\nNext, we project $w$ onto the existing basis vector $v_1$. The coefficient is $h_{11}$.\n$$\nh_{11} = v_1^T w = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{5}{2} \\\\ 4 \\end{pmatrix} \\right)\n= \\frac{1}{3} \\left( \\frac{5}{2} + \\frac{5}{2} + 4 \\right) = \\frac{1}{3} (5 + 4) = \\frac{9}{3} = 3\n$$\nWe compute the component of $w$ orthogonal to $v_1$. Let this be $\\hat{v}_2$.\n$$\n\\hat{v}_2 = w - h_{11} v_1 = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{5}{2} \\\\ 4 \\end{pmatrix} - 3 \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right)\n= \\frac{1}{\\sqrt{3}} \\left( \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{5}{2} \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} \\right)\n= \\frac{1}{\\sqrt{3}} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix}\n$$\nThe next entry in the Hessenberg matrix is $h_{21} = \\|\\hat{v}_2\\|_2$.\n$$\nh_{21} = \\left\\| \\frac{1}{\\sqrt{3}} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} \\right\\|_2\n= \\frac{1}{\\sqrt{3}} \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + 1^2}\n= \\frac{1}{\\sqrt{3}} \\sqrt{\\frac{1}{4} + \\frac{1}{4} + 1} = \\frac{1}{\\sqrt{3}} \\sqrt{\\frac{3}{2}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}\n$$\nThe second basis vector is $v_2 = \\hat{v}_2 / h_{21}$.\n$$\nv_2 = \\frac{\\frac{1}{\\sqrt{3}} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix}}{\\frac{1}{\\sqrt{2}}}\n= \\frac{\\sqrt{2}}{\\sqrt{3}} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\n\n**Step 2 of Arnoldi:**\nWe compute $w = \\tilde{A} v_2$.\n$$\nw = \\begin{pmatrix}\n2 & \\frac{1}{2} & 0 \\\\\n0 & \\frac{3}{2} & 1 \\\\\n0 & 0 & 4\n\\end{pmatrix}\n\\left( \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -1 \\\\ -1 \\\\ 2 \\end{pmatrix} \\right)\n= \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -2 - \\frac{1}{2} \\\\ -\\frac{3}{2} + 2 \\\\ 8 \\end{pmatrix}\n= \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{5}{2} \\\\ \\frac{1}{2} \\\\ 8 \\end{pmatrix}\n$$\nWe project $w$ onto the basis $\\{v_1, v_2\\}$ to find $h_{12}$ and $h_{22}$.\n$$\nh_{12} = v_1^T w = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{5}{2} \\\\ \\frac{1}{2} \\\\ 8 \\end{pmatrix} \\right)\n= \\frac{1}{\\sqrt{18}} \\left( -\\frac{5}{2} + \\frac{1}{2} + 8 \\right) = \\frac{1}{3\\sqrt{2}} (-2 + 8) = \\frac{6}{3\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n$$\n$$\nh_{22} = v_2^T w = \\left( \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -1 & -1 & 2 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{5}{2} \\\\ \\frac{1}{2} \\\\ 8 \\end{pmatrix} \\right)\n= \\frac{1}{6} \\left( (-1)(-\\frac{5}{2}) + (-1)(\\frac{1}{2}) + (2)(8) \\right)\n= \\frac{1}{6} \\left( \\frac{5}{2} - \\frac{1}{2} + 16 \\right) = \\frac{1}{6} (2 + 16) = \\frac{18}{6} = 3\n$$\nWe compute the component of $w$ orthogonal to the span of $\\{v_1, v_2\\}$. Let this be $\\hat{v}_3$.\n$$\n\\hat{v}_3 = w - h_{12} v_1 - h_{22} v_2\n$$\n$$\n\\hat{v}_3 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{5}{2} \\\\ \\frac{1}{2} \\\\ 8 \\end{pmatrix} - \\sqrt{2} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) - 3 \\left( \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -1 \\\\ -1 \\\\ 2 \\end{pmatrix} \\right)\n$$\nTo simplify, we write all terms with a common denominator of $\\sqrt{6}$. Note that $\\frac{\\sqrt{2}}{\\sqrt{3}} = \\frac{\\sqrt{2}\\sqrt{2}}{\\sqrt{3}\\sqrt{2}} = \\frac{2}{\\sqrt{6}}$.\n$$\n\\hat{v}_3 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{5}{2} \\\\ \\frac{1}{2} \\\\ 8 \\end{pmatrix} - \\frac{2}{\\sqrt{6}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{3}{\\sqrt{6}} \\begin{pmatrix} -1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\n$$\n\\hat{v}_3 = \\frac{1}{\\sqrt{6}} \\left[ \\begin{pmatrix} -\\frac{5}{2} \\\\ \\frac{1}{2} \\\\ 8 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} -3 \\\\ -3 \\\\ 6 \\end{pmatrix} \\right]\n= \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{5}{2} - 2 + 3 \\\\ \\frac{1}{2} - 2 + 3 \\\\ 8 - 2 - 6 \\end{pmatrix}\n= \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{3}{2} \\\\ \\frac{3}{2} \\\\ 0 \\end{pmatrix}\n$$\nThe problem asks for the value of $h_{32}$, which is the norm of this vector.\n$$\nh_{32} = \\|\\hat{v}_3\\|_2 = \\left\\| \\frac{1}{\\sqrt{6}} \\begin{pmatrix} -\\frac{3}{2} \\\\ \\frac{3}{2} \\\\ 0 \\end{pmatrix} \\right\\|_2\n= \\frac{1}{\\sqrt{6}} \\sqrt{\\left(-\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 + 0^2}\n= \\frac{1}{\\sqrt{6}} \\sqrt{\\frac{9}{4} + \\frac{9}{4}}\n= \\frac{1}{\\sqrt{6}} \\sqrt{\\frac{18}{4}}\n= \\frac{1}{\\sqrt{6}} \\frac{\\sqrt{18}}{\\sqrt{4}}\n= \\frac{1}{\\sqrt{6}} \\frac{3\\sqrt{2}}{2}\n= \\frac{3\\sqrt{2}}{2\\sqrt{6}} = \\frac{3\\sqrt{2}}{2\\sqrt{2}\\sqrt{3}}\n= \\frac{3}{2\\sqrt{3}} = \\frac{3\\sqrt{3}}{2 \\cdot 3} = \\frac{\\sqrt{3}}{2}\n$$\nThus, the subdiagonal coefficient $h_{32}$ is $\\frac{\\sqrt{3}}{2}$.\nThe Hessenberg matrix after two steps is\n$$\n\\bar{H}_2 = \\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\\\ 0 & h_{32} \\end{pmatrix} = \\begin{pmatrix} 3 & \\sqrt{2} \\\\ \\frac{1}{\\sqrt{2}} & 3 \\\\ 0 & \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nThe question specifically asks for the exact value of $h_{32}$.\nFinal value: $h_{32} = \\frac{\\sqrt{3}}{2}$.",
            "answer": "$$\\boxed{\\frac{\\sqrt{3}}{2}}$$"
        },
        {
            "introduction": "The practical implementation of GMRES often involves restarting to manage memory usage, but this can lead to unexpected pitfalls. This practice explores the critical phenomenon of stagnation, a scenario where restarted GMRES($m$) fails to make any progress toward the solution, while unrestarted GMRES converges perfectly . By constructing and analyzing a specific case of stagnation, you will gain a deeper appreciation for the geometric conditions and residual polynomial properties that govern GMRES convergence, revealing the theoretical trade-offs inherent in restarting.",
            "id": "3593977",
            "problem": "You are to construct a rigorous example in which the Generalized Minimal Residual method (GMRES) exhibits fundamentally different behaviors under restart. The task concerns the preconditioned GMRES algorithm in numerical linear algebra. Let the preconditioning be left-preconditioning, so that the transformed system is $M^{-1} A x = M^{-1} b$, and the residual minimized by the algorithm is $\\| M^{-1} (b - A x) \\|_2$. The construction and analysis must proceed from first principles about Krylov subspaces and residual polynomials.\n\nStarting base definitions and facts:\n- The Generalized Minimal Residual method (GMRES) constructs iterates $x_k$ such that the preconditioned residual satisfies $r_k = p_k(M^{-1} A) r_0$, where $r_0 = b - A x_0$, $p_k$ is a polynomial of degree at most $k$ with $p_k(0) = 1$, and the Euclidean norm of the preconditioned residual is minimized over all such polynomials.\n- The Krylov subspace generated by a matrix $B$ and a vector $v$ is $\\mathcal{K}_k(B, v) = \\operatorname{span}\\{v, B v, B^2 v, \\dots, B^{k-1} v\\}$.\n- Restarted GMRES($m$) applies a degree-$m$ residual polynomial in successive cycles, so that after $j$ cycles the residual is $r_{jm} = \\left(\\prod_{i=1}^j p_m^{(i)}(M^{-1} A)\\right) r_0$, with each $p_m^{(i)}$ a degree-$m$ polynomial satisfying $p_m^{(i)}(0) = 1$ and minimizing the residual over the corresponding Krylov subspace generated in that cycle.\n\nProblem:\n1. Construct a concrete matrix $A \\in \\mathbb{R}^{n \\times n}$, a left preconditioner $M \\in \\mathbb{R}^{n \\times n}$, a right-hand side $b \\in \\mathbb{R}^n$, an initial guess $x_0 \\in \\mathbb{R}^n$, and a restart length $m$, such that restarted GMRES($m$) stagnates (no reduction in the residual norm across cycles), while unrestarted GMRES converges to the solution in at most $n$ iterations. Justify the behavior using the residual polynomial characterization and geometric arguments about the Krylov subspace and orthogonality.\n2. For a scientifically sound and testable scenario, consider the following explicit test suite using dimension $n = 4$ and left preconditioning:\n   - Define $A$ to be the cyclic down-shift permutation matrix in $\\mathbb{R}^{4 \\times 4}$, i.e., $A e_1 = e_2$, $A e_2 = e_3$, $A e_3 = e_4$, and $A e_4 = e_1$, where $(e_i)_{i=1}^4$ are the standard basis vectors in $\\mathbb{R}^4$.\n   - Define the left preconditioner $M = \\operatorname{diag}(1, 2, 3, 4)$.\n   - Define $b = e_1$ and $x_0 = 0$.\n   - Use restart length $m = 1$ for the stagnation demonstration, and unrestarted GMRES for the convergence demonstration.\n   - Also test $m = 2$ and $m = 4$ to probe intermediate and boundary behavior, and test a modified right-hand side $b = e_1 + e_2$ to break the orthogonality condition that causes stagnation.\n3. Your program must implement left-preconditioned GMRES on the transformed system $M^{-1} A x = M^{-1} b$, measuring the original residual norm $\\| b - A x \\|_2$ for reporting. It must compute the following quantifiable outputs for the specified test suite:\n   - Test 1 (stagnation, restarted GMRES($m=1$)): the ratio $\\rho_1 = \\| r_{\\text{final}} \\|_2 / \\| r_{\\text{initial}} \\|_2$ after $8$ restart cycles with $m = 1$, where $r_{\\text{initial}} = b - A x_0$ and $r_{\\text{final}}$ is the final residual. This is a float.\n   - Test 2 (unrestarted GMRES convergence): the final residual norm $\\rho_2 = \\| r_{\\text{final}} \\|_2$ after at most $n$ iterations without restart. This is a float.\n   - Test 3 (progress for $m = 2$): a boolean indicating whether restarted GMRES with $m = 2$ reduces the residual norm after $4$ restart cycles, i.e., whether the final norm is strictly smaller than the initial norm. This is a boolean.\n   - Test 4 (boundary, $m = n$): the final residual norm $\\rho_4 = \\| r_{\\text{final}} \\|_2$ for restarted GMRES with $m = 4$ after a single cycle. This is a float.\n   - Test 5 (breaking stagnation by changing $b$): a boolean indicating whether restarted GMRES with $m = 1$ for the modified right-hand side $b = e_1 + e_2$ reduces the residual norm after $8$ cycles. This is a boolean.\n4. Angle units and physical units are not applicable to this purely mathematical problem. All norms are the Euclidean norm in $\\mathbb{R}^n$. The matrix and vector entries are unitless real numbers.\n5. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where result1 and result2 are floats, result3 and result5 are booleans, and result4 is a float.\n\nYour solution must justify your construction using the residual polynomial characterization $r_k = p_k(M^{-1} A) r_0$ with $p_k(0) = 1$ and the geometry of the Krylov subspace. The code must be complete, runnable \"as is\", and produce the specified outputs for the given test suite.",
            "solution": "The problem requires the construction of an example in numerical linear algebra where restarted Generalized Minimal Residual method (GMRES) stagnates, while unrestarted GMRES converges. The analysis must be based on first principles of Krylov subspaces and residual polynomials.\n\nThe core of the GMRES algorithm lies in finding an approximate solution $x_k$ to a linear system $Bx=c$ from an affine subspace $x_0 + \\mathcal{K}_k(B, \\tilde{r}_0)$, where $x_0$ is an initial guess, $\\mathcal{K}_k(B, \\tilde{r}_0) = \\operatorname{span}\\{\\tilde{r}_0, B\\tilde{r}_0, \\dots, B^{k-1}\\tilde{r}_0\\}$ is the order-$k$ Krylov subspace generated by the matrix $B$ and the initial residual $\\tilde{r}_0 = c - Bx_0$. The iterate $x_k$ is chosen such that its residual $r_k = c - Bx_k$ has the minimum possible Euclidean norm, $\\| r_k \\|_2$. For a left-preconditioned system $M^{-1}Ax = M^{-1}b$, we set $B = M^{-1}A$ and $c = M^{-1}b$. GMRES then minimizes the norm of the preconditioned residual, $\\| M^{-1}(b-Ax_k) \\|_2$.\n\nA key geometric property of GMRES is that the new preconditioned residual, $\\tilde{r}_k = M^{-1}(b - Ax_k)$, is orthogonal to the subspace $B \\mathcal{K}_k(B, \\tilde{r}_0)$, where $\\tilde{r}_0 = M^{-1}(b - Ax_0)$ is the initial preconditioned residual. Stagnation in restarted GMRES($m$) occurs when the algorithm makes no progress from one cycle to the next. This happens if the preconditioned residual vector at the end of a cycle, $\\tilde{r}_m$, is proportional to the one at the start of the cycle, $\\tilde{r}_0$, and the reduction in norm is zero or negligible. A complete lack of progress (zero reduction) occurs if $\\tilde{r}_m=\\tilde{r}_0$. This happens under a specific condition: if the initial preconditioned residual $\\tilde{r}_0$ is already orthogonal to the search space for the residual update, $B\\mathcal{K}_m(B, \\tilde{r}_0)$. In such a case, the minimality condition is satisfied by a zero update, leading to $\\tilde{r}_m = \\tilde{r}_0$.\n\nWe construct such a case using the specified parameters.\nLet $n=4$. The system matrix is the cyclic down-shift permutation matrix:\n$$A = \\begin{pmatrix} 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}$$\nThe left preconditioner is a diagonal matrix:\n$$M = \\operatorname{diag}(1, 2, 3, 4) = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 4 \\end{pmatrix}$$\nThe right-hand side vector is $b = e_1 = (1, 0, 0, 0)^T$, and the initial guess is $x_0 = 0$.\n\nThe preconditioned matrix is $B = M^{-1}A$:\n$$B = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1/2 & 0 & 0 \\\\ 0 & 0 & 1/3 & 0 \\\\ 0 & 0 & 0 & 1/4 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 1 \\\\ 1/2 & 0 & 0 & 0 \\\\ 0 & 1/3 & 0 & 0 \\\\ 0 & 0 & 1/4 & 0 \\end{pmatrix}$$\nThe initial residual is $r_0 = b - Ax_0 = e_1$. The initial preconditioned residual is $\\tilde{r}_0 = M^{-1}r_0 = M^{-1}e_1 = e_1$.\n\n**1. Analysis of Restarted GMRES(m) Stagnation ($m=1, 2, 3$)**\n\nConsider GMRES($m$) with a restart length $m  4$. The first cycle starts with $\\tilde{r}_0=e_1$. The Krylov subspace is $\\mathcal{K}_m(B, e_1)$.\nLet's compute the basis vectors:\n$e_1$\n$B e_1 = (1/2)e_2$\n$B^2 e_1 = (1/6)e_3$\n$B^3 e_1 = (1/24)e_4$\nThe Krylov subspace is $\\mathcal{K}_m(B, e_1) = \\operatorname{span}\\{e_1, e_2, \\dots, e_m\\}$.\nThe subspace to which the new residual must be orthogonal is $B \\mathcal{K}_m(B, e_1) = \\operatorname{span}\\{B e_1, \\dots, B e_m\\} = \\operatorname{span}\\{e_2, e_3, \\dots, e_{m+1}\\}$.\n\nFor any $m  4$, the subspaces $\\mathcal{K}_m(B, e_1)$ and $B\\mathcal{K}_m(B, e_1)$ are orthogonal with respect to the standard inner product, because they are spanned by disjoint sets of standard basis vectors. For instance, for $m=1$, $\\mathcal{K}_1(B, e_1) = \\operatorname{span}\\{e_1\\}$ and $B\\mathcal{K}_1(B, e_1) = \\operatorname{span}\\{e_2\\}$, which are orthogonal.\nThe initial preconditioned residual $\\tilde{r}_0 = e_1$ lies in $\\mathcal{K}_m(B, e_1)$ and is therefore orthogonal to every vector in $B\\mathcal{K}_m(B, e_1)$.\nThe GMRES condition requires the new residual $\\tilde{r}_m$ to be orthogonal to $B\\mathcal{K}_m(B, e_1)$. Since $\\tilde{r}_0$ already satisfies this orthogonality condition, the algorithm finds that the optimal choice is $\\tilde{r}_m = \\tilde{r}_0$, which corresponds to a zero correction, $x_m = x_0$. The residual norm does not decrease. This is stagnation. This reasoning applies to any restart cycle, and for any restart length $m=1, 2, 3$.\nFrom the residual polynomial perspective, the algorithm seeks a polynomial $p_m(z)$ of degree at most $m$ with $p_m(0)=1$ that minimizes $\\| p_m(B) \\tilde{r}_0 \\|_2$. This norm is $\\| p_m(B) e_1 \\|_2$. Because the vectors $e_1, Be_1, \\dots, B^m e_1$ are mutually orthogonal, we have $\\| p_m(B) e_1 \\|_2^2 = \\sum_{j=0}^m c_j^2 \\| B^j e_1 \\|_2^2$ for $p_m(z)=\\sum c_j z^j$. With $c_0=p_m(0)=1$, the minimum is achieved when all other coefficients are zero, i.e., $p_m(z)=1$. Thus, $\\tilde{r}_m = p_m(B)\\tilde{r}_0 = \\tilde{r}_0$.\n\n**2. Analysis of Unrestarted GMRES Convergence ($m=4$)**\n\nFor unrestarted GMRES, the algorithm runs for up to $n=4$ iterations, building the subspace $\\mathcal{K}_4(B, e_1) = \\operatorname{span}\\{e_1, (1/2)e_2, (1/6)e_3, (1/24)e_4\\} = \\operatorname{span}\\{e_1, e_2, e_3, e_4\\} = \\mathbb{R}^4$. Since the Krylov subspace is the entire solution space, GMRES is guaranteed to find the exact correction $z_4 = x_{exact} - x_0$ that makes the residual zero. Therefore, unrestarted GMRES (which is equivalent to one cycle of GMRES($4$)) converges to the exact solution, and the final residual norm, both preconditioned and original, will be zero (or machine precision).\n\n**3. Analysis of Breaking Stagnation ($b = e_1+e_2$)**\n\nLet's change the right-hand side to $b = e_1 + e_2$. The initial residual is $r_0 = e_1 + e_2$, and the preconditioned initial residual is $\\tilde{r}_0 = M^{-1}(e_1 + e_2) = e_1 + (1/2)e_2$.\nNow consider GMRES($1$). The Krylov subspace is $\\mathcal{K}_1(B, \\tilde{r}_0) = \\operatorname{span}\\{e_1 + (1/2)e_2\\}$.\nThe subspace for the residual update is $B\\mathcal{K}_1(B, \\tilde{r}_0) = \\operatorname{span}\\{B(e_1 + (1/2)e_2)\\}$.\n$B(e_1 + (1/2)e_2) = B e_1 + (1/2)B e_2 = (1/2)e_2 + (1/2)M^{-1}Ae_2 = (1/2)e_2 + (1/2)M^{-1}e_3 = (1/2)e_2 + (1/6)e_3$.\nThe crucial orthogonality condition is checked via the inner product:\n$$ \\tilde{r}_0^T (B \\tilde{r}_0) = (e_1 + (1/2)e_2)^T ((1/2)e_2 + (1/6)e_3) = (1/2)(1/2) = 1/4 \\neq 0 $$\nSince $\\tilde{r}_0$ is not orthogonal to $B\\mathcal{K}_1(B, \\tilde{r}_0)$, the GMRES update will be non-zero. The algorithm will project $\\tilde{r}_0$ onto the orthogonal complement of $B\\mathcal{K}_1(B, \\tilde{r}_0)$, resulting in a new residual $\\tilde{r}_1$ with $\\|\\tilde{r}_1\\|_2  \\|\\tilde{r}_0\\|_2$. Thus, the stagnation is broken, and GMRES($1$) will make progress in each cycle.\n\nThis completes the theoretical justification for the behavior observed in the specified test cases. The constructed example brilliantly isolates the geometric condition that leads to stagnation in restarted GMRES.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse.linalg import gmres\n\ndef solve():\n    \"\"\"\n    Constructs and solves the test cases for GMRES stagnation and convergence.\n    \"\"\"\n    n = 4\n    # Define standard basis vectors\n    e1 = np.zeros(n)\n    e1[0] = 1.0\n    e2 = np.zeros(n)\n    e2[1] = 1.0\n\n    # Define the matrix A as the cyclic down-shift permutation matrix\n    A = np.roll(np.eye(n), 1, axis=0)\n\n    # Define the diagonal preconditioner M\n    M = np.diag(np.arange(1, n + 1).astype(float))\n    \n    # Define the initial guess\n    x0 = np.zeros(n)\n    \n    # Define the inverse of the preconditioner\n    M_inv = np.linalg.inv(M)\n    \n    # Pre-compute the transformed matrix for the left-preconditioned system\n    A_tilde = M_inv @ A\n\n    results = []\n\n    # --- Test 1: Stagnation with m=1 ---\n    # b = e1, m=1, 8 restart cycles\n    b1 = e1\n    r_initial_norm_1 = np.linalg.norm(b1 - A @ x0)\n    b_tilde_1 = M_inv @ b1\n    # For m=1 and 8 cycles, restart=1 and maxiter=8\n    x_final_1, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=1, maxiter=8, atol=1e-12)\n    r_final_1 = b1 - A @ x_final_1\n    rho1 = np.linalg.norm(r_final_1) / r_initial_norm_1\n    results.append(rho1)\n\n    # --- Test 2: Unrestarted GMRES convergence ---\n    # b = e1, unrestarted (m=n=4), max n iterations\n    # For unrestarted GMRES with n=4, restart=4 and maxiter=4\n    x_final_2, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=4, maxiter=4, atol=1e-12)\n    r_final_2 = b1 - A @ x_final_2\n    rho2 = np.linalg.norm(r_final_2)\n    results.append(rho2)\n\n    # --- Test 3: Progress for m=2 ---\n    # b = e1, m=2, 4 restart cycles. Check for any reduction in norm.\n    r_initial_norm_3 = np.linalg.norm(b1 - A @ x0)\n    # For m=2 and 4 cycles, restart=2 and maxiter=8\n    x_final_3, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=2, maxiter=8, atol=1e-12)\n    r_final_norm_3 = np.linalg.norm(b1 - A @ x_final_3)\n    progress_3 = r_final_norm_3  r_initial_norm_3\n    results.append(progress_3)\n\n    # --- Test 4: Boundary case m=n ---\n    # b = e1, m=4, 1 cycle. This is identical to unrestarted GMRES.\n    # For m=4 and 1 cycle, restart=4 and maxiter=4\n    x_final_4, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=4, maxiter=4, atol=1e-12)\n    r_final_4 = b1 - A @ x_final_4\n    rho4 = np.linalg.norm(r_final_4)\n    results.append(rho4)\n\n    # --- Test 5: Breaking stagnation by changing b ---\n    # b = e1+e2, m=1, 8 cycles. Check for any reduction in norm.\n    b5 = e1 + e2\n    r_initial_norm_5 = np.linalg.norm(b5 - A @ x0)\n    b_tilde_5 = M_inv @ b5\n    # For m=1 and 8 cycles, restart=1 and maxiter=8\n    x_final_5, _ = gmres(A_tilde, b_tilde_5, x0=x0, restart=1, maxiter=8, atol=1e-12)\n    r_final_norm_5 = np.linalg.norm(b5 - A @ x_final_5)\n    progress_5 = r_final_norm_5  r_initial_norm_5\n    results.append(progress_5)\n\n    # Format the final output string exactly as specified\n    formatted_results = []\n    for res in results:\n        if isinstance(res, bool):\n            formatted_results.append(str(res).lower())\n        else:\n            # Use fixed-point notation for numbers close to 1.0\n            if abs(res - 1.0)  1e-9:\n                 formatted_results.append(f\"{res:.1f}\")\n            else:\n                 formatted_results.append(f\"{res:.15e}\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\n# solve() # The call is commented out to prevent execution in this context, but the function is complete.\n```"
        }
    ]
}