## 引言
在科学计算和工程分析的众多领域中，求解大规模[线性系统](@entry_id:147850) $Ax=b$ 是一个核心且无处不在的挑战。当矩阵 $A$ 巨大且稀疏时，[迭代法](@entry_id:194857)（如GMRES和共轭梯度法）成为首选的求解工具。然而，这些方法的[收敛速度](@entry_id:636873)对矩阵 $A$ 的谱特性高度敏感，对于[病态系统](@entry_id:137611)，收敛过程可能极其缓慢，甚至停滞。为了克服这一瓶颈，预处理技术应运而生，它通过将原始问题变换为一个更容易求解的等价系统，从而戏剧性地加速收敛。然而，“如何”以及“在何处”应用预处理器——即选择[左预处理](@entry_id:165660)、[右预处理](@entry_id:173546)还是[分裂预处理](@entry_id:755247)——是一个微妙但至关重要的问题，直接影响着算法的效率、稳定性和实现复杂性。

本文旨在系统地阐明这三种基本预处理策略的理论与实践。通过以下三个章节，读者将建立一个全面的理解：

-   在“原理与机制”一章中，我们将深入剖析左、右及[分裂预处理](@entry_id:755247)的代数构造，阐明它们如何通过改善谱条件数来加速收敛，并探讨它们对迭代过程中关键量（如残差）的不同影响。
-   在“应用与跨学科交叉”一章中，我们将展示这些技术在[偏微分方程](@entry_id:141332)、数据科学和[高性能计算](@entry_id:169980)等领域的实际应用，揭示[预处理](@entry_id:141204)策略的选择如何与问题的物理背景和计算环境深度融合。
-   最后，通过“动手实践”中的具体练习，您将有机会亲手实现并验证这些概念，巩固所学知识。

让我们首先从[预处理](@entry_id:141204)的基本原理和机制开始，探索其加速迭代求解的奥秘。

## 原理与机制

在迭代方法中，预处理技术通过将原始线性系统 $A x = b$ 变换为一个等价但更易于求解的系统，从而显著加速收敛。这一[变换的核](@entry_id:149509)心在于引入一个非奇异的**[预处理器](@entry_id:753679)**（preconditioner）矩阵 $M$，该矩阵应近似于 $A$，并且其逆矩阵 $M^{-1}$ 的作用（即求解形如 $Mz=d$ 的[线性系统](@entry_id:147850)）在计算上是廉价的。本章将深入探讨预处理的基本原理、主要形式及其对迭代过程产生的深刻影响。

### [预处理](@entry_id:141204)系统的基本形式

根据[预处理器](@entry_id:753679) $M$ 应用于原始系统的方式，我们可以区分三种主要的[预处理](@entry_id:141204)策略：[左预处理](@entry_id:165660)、[右预处理](@entry_id:173546)和[分裂预处理](@entry_id:755247)。理解它们各自的构造和与原始系统的[等价关系](@entry_id:138275)至关重要 。

#### [左预处理](@entry_id:165660)

**[左预处理](@entry_id:165660)**（left preconditioning）通过将[预处理器](@entry_id:753679)的逆 $M^{-1}$ 从左侧乘上原始方程的两边来实现：
$$
M^{-1} (A x) = M^{-1} b
$$
整理后得到新的[线性系统](@entry_id:147850)：
$$
(M^{-1} A) x = M^{-1} b
$$
在这个变换后的系统中，[系统矩阵](@entry_id:172230)变为 $\hat{A} = M^{-1} A$，右端项变为 $\hat{b} = M^{-1} b$。至关重要的是，该系统求解的未知向量仍然是原始的 $x$。由于 $M$ 是非奇异的，这个变换是可逆的，因此[左预处理](@entry_id:165660)系统的解与原始系统的解完全相同。迭代求解器将作用于矩阵 $M^{-1} A$ 上，其谱特性将决定[收敛速度](@entry_id:636873)。

#### [右预处理](@entry_id:173546)

**[右预处理](@entry_id:173546)**（right preconditioning）则采用了一种不同的策略，它通过变量替换来变换系统。我们引入一个新的未知向量 $y$，并定义 $x = M^{-1} y$。将此关系代入原始系统 $A x = b$ 中：
$$
A (M^{-1} y) = b
$$
整理后得到：
$$
(A M^{-1}) y = b
$$
这个新的系统以 $y$ 为未知量，其[系统矩阵](@entry_id:172230)为 $\hat{A} = A M^{-1}$，右端项则保持不变。当[迭代求解器](@entry_id:136910)找到 $y$ 的解后，我们必须通过变换 $x = M^{-1} y$ 来恢复原始解 $x$。由于 $M$ 是可逆的，从 $y$ 到 $x$ 的映射是一一对应的（[双射](@entry_id:138092)），确保了我们可以唯一地恢复原始解。

#### [分裂预处理](@entry_id:755247)

**[分裂预处理](@entry_id:755247)**（split preconditioning）结合了[左预处理](@entry_id:165660)和[右预处理](@entry_id:173546)的思想。它将预处理器 $M$ 分解为两个矩阵的乘积，即 $M = M_L M_R$，其中 $M_L$ 和 $M_R$ 均为[非奇异矩阵](@entry_id:171829)。变换过程如下：
首先，从左侧乘以 $M_L^{-1}$：
$$
M_L^{-1} A x = M_L^{-1} b
$$
然后，在 $A$ 和 $x$ 之间插入[单位矩阵](@entry_id:156724) $I = M_R^{-1} M_R$：
$$
M_L^{-1} A (M_R^{-1} M_R) x = M_L^{-1} b
$$
通过重新组合项，我们得到：
$$
(M_L^{-1} A M_R^{-1}) (M_R x) = M_L^{-1} b
$$
最后，定义新变量 $y = M_R x$，系统变为：
$$
(M_L^{-1} A M_R^{-1}) y = M_L^{-1} b
$$
该系统的矩阵为 $\hat{A} = M_L^{-1} A M_R^{-1}$，右端项为 $\hat{b} = M_L^{-1} b$。求解得到 $y$ 后，原始解通过 $x = M_R^{-1} y$ 恢复。[分裂预处理](@entry_id:755247)在希望保持原始矩阵某些性质（如对称性）时特别有用。例如，如果 $A$ 是对称正定（SPD）的，选择 $M = L L^T$（例如[不完全Cholesky分解](@entry_id:750589)），并令 $M_L = L, M_R = L^T$，则[预处理](@entry_id:141204)后的矩阵 $\hat{A} = L^{-1} A (L^{-1})^T$ 仍然是SPD的。

### [预处理](@entry_id:141204)的目标：改善谱特性

[预处理](@entry_id:141204)的根本目标是构造一个[预处理](@entry_id:141204)后的系统矩阵（如 $M^{-1}A$），其[谱分布](@entry_id:158779)比原始矩阵 $A$ 更适合迭代求解。对于许多迭代方法，如共轭梯度法（CG）和[广义最小残差法](@entry_id:139566)（GMRES），[收敛速度](@entry_id:636873)与系统矩阵的**谱[条件数](@entry_id:145150)**（spectral condition number）密切相关。对于一个对称正定矩阵 $B$，其条件数定义为 $\kappa_2(B) = \lambda_{\max}(B) / \lambda_{\min}(B)$，其中 $\lambda_{\max}$ 和 $\lambda_{\min}$ 分别是其最大和[最小特征值](@entry_id:177333)。理想的预处理器 $M$ 应使得 $\kappa_2(M^{-1}A) \ll \kappa_2(A)$。

一个好的[预处理器](@entry_id:753679) $M$ 应该近似于 $A$，即 $M^{-1}A \approx I$。在这种情况下，$M^{-1}A$ 的所有[特征值](@entry_id:154894)都将聚集在 $1$ 附近，其[条件数](@entry_id:145150)也接近于 $1$，这将导致迭代方法的快速收敛。

我们可以通过一个具体的例子来量化这种加速效果 。考虑一个对角[SPD矩阵](@entry_id:136714)：
$$
A = \operatorname{diag}(1, 100, 10000)
$$
其[特征值](@entry_id:154894)为 $1, 100, 10000$，因此[条件数](@entry_id:145150) $\kappa_2(A) = 10000 / 1 = 10000$。这是一个[病态系统](@entry_id:137611)。现在，我们选择一个简单的对角[预处理器](@entry_id:753679)：
$$
M = \operatorname{diag}(1, 50, 5000)
$$
[左预处理](@entry_id:165660)后的矩阵为：
$$
M^{-1}A = \operatorname{diag}\left(1, \frac{100}{50}, \frac{10000}{5000}\right) = \operatorname{diag}(1, 2, 2)
$$
这个新矩阵的[特征值](@entry_id:154894)为 $1$ 和 $2$，因此其[条件数](@entry_id:145150) $\kappa_2(M^{-1}A) = 2 / 1 = 2$。[条件数](@entry_id:145150)从 $10000$ 急剧下降到了 $2$。

这种改善对[收敛速度](@entry_id:636873)有巨大影响。对于CG方法，误差的[A-范数](@entry_id:746180)收敛界为：
$$
\frac{\|e_{k}\|_{A}}{\|e_{0}\|_{A}} \leq 2 \left( \frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1} \right)^k
$$
对于上述例子，若要使[A-范数](@entry_id:746180)下的误差满足 $\frac{\|e_{k}\|_{A}}{\|e_{0}\|_{A}} \le 10^{-8}$，对于原始系统（$\kappa = 10000$），所需的迭代次数 $k$ 约为966次。而对于[预处理](@entry_id:141204)后的系统（$\kappa = 2$），迭代次数上界 $k$ 满足 $2 \left( \frac{\sqrt{2} - 1}{\sqrt{2} + 1} \right)^k \le 10^{-8}$，解得 $k \ge 10.84$，即仅需 $11$ 次迭代。这清晰地展示了预处理在减少计算工作量方面的巨大威力。

### [预处理器](@entry_id:753679)的来源：与[定常迭代法](@entry_id:144014)的联系

一个自然的问题是：如何构造有效的预处理器 $M$？一个经典而富有洞察力的途径来自于矩阵分裂和传统的[定常迭代法](@entry_id:144014)（如Jacobi、[Gauss-Seidel法](@entry_id:145727)）。

考虑将矩阵 $A$ 分裂为 $A = M - N$，其中 $M$ 是非奇异的，并且容易求逆 。这个分裂直接导出一个[定常迭代](@entry_id:755385)格式：
$$
A x = b \implies (M-N)x = b \implies Mx = Nx + b \implies x = M^{-1}Nx + M^{-1}b
$$
从而得到迭代式 $x_{k+1} = G x_k + c$，其中[迭代矩阵](@entry_id:637346) $G = M^{-1}N$，仿射项 $c = M^{-1}b$。此迭代法收敛的充要条件是 $G$ 的谱半径 $\rho(G)  1$。

这个分裂中使用的矩阵 $M$ 正是[左预处理](@entry_id:165660)器的天然候选者。[左预处理](@entry_id:165660)后的系统矩阵为 $M^{-1}A$。我们可以探究它与[迭代矩阵](@entry_id:637346) $G$ 的关系：
$$
M^{-1}A = M^{-1}(M - N) = M^{-1}M - M^{-1}N = I - G
$$
这个简洁的关系式揭示了一个深刻的联系：[左预处理](@entry_id:165660)算子 $M^{-1}A$ 与相应[定常迭代](@entry_id:755385)的[迭代矩阵](@entry_id:637346) $G$ 通过单位阵相连。一个好的[定常迭代法](@entry_id:144014)意味着其[迭代矩阵](@entry_id:637346) $G$ 的[谱半径](@entry_id:138984)远小于 $1$，即其[特征值](@entry_id:154894)聚集在 $0$ 附近。根据上述关系，这意味着[预处理](@entry_id:141204)后的矩阵 $M^{-1}A$ 的[特征值](@entry_id:154894)将聚集在 $1$ 附近。这正是我们期望从预处理中获得的效果！

以[Gauss-Seidel法](@entry_id:145727)为例 ，我们将 $A$ 分解为 $A = D - L - U$，其中 $D$ 是对角部分，$-L$ 是严格下三角部分，$-U$ 是严格上三角部分。
- **前向Gauss-Seidel[预处理器](@entry_id:753679)**选择 $M_f = D-L$。[左预处理](@entry_id:165660)算子为 $M_f^{-1}A = (D-L)^{-1}(D-L-U) = I - (D-L)^{-1}U$。这里的 $G_f = (D-L)^{-1}U$ 正是前向[Gauss-Seidel迭代](@entry_id:136271)的[迭代矩阵](@entry_id:637346)。
- **后向Gauss-Seidel[预处理器](@entry_id:753679)**选择 $M_b = D-U$。[左预处理](@entry_id:165660)算子为 $M_b^{-1}A = (D-U)^{-1}(D-L-U) = I - (D-U)^{-1}L$。这里的 $G_b = (D-U)^{-1}L$ 则是后向[Gauss-Seidel迭代](@entry_id:136271)的[迭代矩阵](@entry_id:637346)。

因此，任何经典的[定常迭代法](@entry_id:144014)都隐含地定义了一个预处理器，其有效性可以直接通过预处理后[算子的谱](@entry_id:272027)特性来分析。

### 实践中的考量：[左预处理](@entry_id:165660)与[右预处理](@entry_id:173546)的对比

尽管[左预处理](@entry_id:165660)和[右预处理](@entry_id:173546)在精确算术下是等价的，但在有限精度的迭代算法中，它们之间存在着微妙而重要的差异。这些差异影响着停机准则的设置、[后向误差](@entry_id:746645)的解释以及算法的实现。

#### 残差与停机准制

[迭代求解器](@entry_id:136910)通常通过监控残差的范数来判断收敛，即当[残差范数](@entry_id:754273)小于某个容忍度 $\delta$ 时停止。[左预处理](@entry_id:165660)和[右预处理](@entry_id:173546)在此处的行为截然不同。

- 对于**[右预处理](@entry_id:173546)**，求解器作用于系统 $AM^{-1}y = b$。其残差为 $r_k^R = b - (AM^{-1})y_k$。如果我们定义原始变量的近似解为 $x_k = M^{-1}y_k$，那么原始系统的残差 $r_k = b - Ax_k = b - A(M^{-1}y_k) = r_k^R$。这意味着**[右预处理](@entry_id:173546)保持了原始残差**。因此，在[右预处理](@entry_id:173546)的Krylov方法中监控的残差就是真实的残差，停机准则的解释是直接的。

- 对于**[左预处理](@entry_id:165660)**，求解器作用于系统 $M^{-1}Ax = M^{-1}b$。它所“看到”并最小化的残差是**预处理后的残差** $\hat{r}_k = M^{-1}b - (M^{-1}A)x_k = M^{-1}(b - Ax_k) = M^{-1}r_k$。原始残差 $r_k$ 与预处理残差 $\hat{r}_k$ 的关系是 $r_k = M\hat{r}_k$。

这导致一个重要问题 ：在[左预处理](@entry_id:165660)中，当算法报告 $\|\hat{r}_k\|_2 \le \hat{\delta}$ 时，我们对真实残差的范数 $\|r_k\|_2$ 知道多少？根据范数的性质，我们有如下关系：
$$
\| r_k \|_2 = \| M \hat{r}_k \|_2 \le \| M \|_2 \| \hat{r}_k \|_2
$$
以及
$$
\|\hat{r}_k\|_2 = \|M^{-1} r_k\|_2 \le \|M^{-1}\|_2 \|r_k\|_2 \implies \|r_k\|_2 \ge \frac{\|\hat{r}_k\|_2}{\|M^{-1}\|_2}
$$
结合起来，我们得到一个[区间估计](@entry_id:177880)：
$$
\frac{1}{\|M^{-1}\|_2} \|\hat{r}_k\|_2 \le \|r_k\|_2 \le \|M\|_2 \|\hat{r}_k\|_2
$$
其中 $\|\cdot\|_2$ 是[谱范数](@entry_id:143091)。这意味着真实[残差范数](@entry_id:754273)可能比预处理[残差范数](@entry_id:754273)大一个因子 $\|M\|_2$（即 $M$ 的最大奇异值），或者小一个因子 $1/\|M^{-1}\|_2$（即 $M$ 的最小[奇异值](@entry_id:152907)）。如果我们的目标是确保真实[残差范数](@entry_id:754273) $\|r_k\|_2 \le \delta$，那么我们必须设置一个更严格的停机准则 $\|\hat{r}_k\|_2 \le \hat{\delta}$。为了保证在最坏情况下也能满足条件，我们必须选择 $\hat{\delta}$ 使得 $\|M\|_2 \hat{\delta} \le \delta$，即 $\hat{\delta} \le \delta / \|M\|_2$。例如，如果一个SPD预处理器 $M$ 的最大[特征值](@entry_id:154894) $\lambda_{\max}(M)=7$，而我们希望真实残差容忍度为 $\delta = 4 \times 10^{-8}$，那么我们必须将预处理残差的停机容忍度设置为 $\hat{\delta} \le (4 \times 10^{-8}) / 7 \approx 5.71 \times 10^{-9}$。

#### [后向误差](@entry_id:746645)的保持与扭曲

残差与[后向误差](@entry_id:746645)密切相关。近似解 $x_k$ 的[后向误差](@entry_id:746645)是指使得 $x_k$ 成为一个扰动后系统 $(A+\Delta A)x_k = b+\Delta b$ 的精确解的最小扰动 $(\Delta A, \Delta b)$。这个最小扰动的大小可以被量化为 $\beta(x_k, r_k) = \|r_k\|_2 / \sqrt{1 + \|x_k\|_2^2}$ 。

由于[右预处理](@entry_id:173546)保持了原始残差 $r_k$，它也自然地**保持了[后向误差](@entry_id:746645)的结构**。在[右预处理](@entry_id:173546)的迭代中获得的任何关于残差的信息都直接转化为关于原始问题[后向稳定性](@entry_id:140758)的信息。

相比之下，[左预处理](@entry_id:165660)通过关系式 $r_k = M\hat{r}_k$ **扭曲了残差**。即使预处理后的残差 $\hat{r}_k$ 很小，如果[预处理器](@entry_id:753679) $M$ 在某些方向上具有很大的放大作用（即 $\|M\|_2 \gg 1$），那么真实的残差 $r_k$ 可能会很大，从而导致一个不可接受的大[后向误差](@entry_id:746645)。例如，考虑一个各向异性的[预处理器](@entry_id:753679) $M = \operatorname{diag}(50, 0.02)$。如果预处理后的残差方向为 $u = [1, 0]^T$ 且范数为 $\tau = 10^{-3}$，即 $\hat{r}_k = 10^{-3}[1, 0]^T$，那么真实残差将是 $r_k = M \hat{r}_k = [0.05, 0]^T$。真实残差的范数被放大了50倍，导致了比预处理系统所显示的更大的[后向误差](@entry_id:746645)。反之，如果 $\hat{r}_k$ 的方向是 $M$ 强烈衰减的方向，则真实[后向误差](@entry_id:746645)会比预期的要小。这种扭曲效应是选择[左预处理](@entry_id:165660)时必须考虑的重要因素。

#### Krylov方法中的等价性

一个自然的问题是，[左预处理](@entry_id:165660)和[右预处理](@entry_id:173546)产生的迭代序列有何关系？在一般情况下，它们是不同的。然而，在一个重要条件下，它们是等价的 。

**定理**：对于[GMRES方法](@entry_id:139566)，若初始猜测 $x_0=0$，且预处理器 $M$ 满足交换条件 $[M^{-1}, A] = M^{-1}A - AM^{-1} = 0$，则[左预处理](@entry_id:165660)和[右预处理](@entry_id:173546)（在最小化相同范数的前提下）会产生完全相同的迭代序列 $\{x_k\}$。

这个定理的证明思路是展示两种方法在每一步都求解相同的唯一极小化问题。它们需要有相同的（1）搜索空间和（2）目标函数。
- **[左预处理](@entry_id:165660)**的搜索空间是 $\mathcal{S}_k^L = \mathcal{K}_k(M^{-1}A, M^{-1}b)$。
- **[右预处理](@entry_id:173546)**的搜索空间是 $\mathcal{S}_k^R = M^{-1}\mathcal{K}_k(AM^{-1}, b)$。
当 $M^{-1}$ 与 $A$ 交换时，$AM^{-1} = M^{-1}A$。可以证明，此时两个[子空间](@entry_id:150286)的生成向量是相同的，因此 $\mathcal{S}_k^L = \mathcal{S}_k^R$。如果再假定两种方法最小化的是同一个目标函数（例如，真实残差的某个范数），那么它们在每个$k$步产生的迭代解$x_k$必然是相同的。

在实践中，$[M^{-1}, A] = 0$ 是一个非常强的条件，通常不成立（除非 $M$是 $A$ 的多项式或者两者都有非常特殊的结构，如同为对角阵）。因此，[左预处理](@entry_id:165660)和[右预处理](@entry_id:173546)的GMRES通常会产生不同的收敛轨迹。

### 高级主题与潜在问题

#### 左[预处理GMRES](@entry_id:753677)中[内积](@entry_id:158127)的角色

标准[GMRES算法](@entry_id:749938)在[Arnoldi过程](@entry_id:166662)中使用欧几里得[内积](@entry_id:158127)来构建Krylov[子空间](@entry_id:150286)的[正交基](@entry_id:264024)，并最小化相应残差的欧几里得范数（[2-范数](@entry_id:636114)）。当应用于[左预处理](@entry_id:165660)系统时，这意味着GMRES最小化的是**预处理残差的[2-范数](@entry_id:636114)**，即 $\|r_k^L\|_2 = \|M^{-1}r_k\|_2$ 。

这引出了一个深刻的问题：最小化 $\|M^{-1}r_k\|_2$ 等价于最小化真实残差 $r_k$ 的哪个范数？
$$
\|M^{-1}r_k\|_2^2 = (M^{-1}r_k)^T (M^{-1}r_k) = r_k^T (M^{-1})^T M^{-1} r_k = r_k^T (M^{-T}M^{-1}) r_k
$$
这表明，标准的左[预处理GMRES](@entry_id:753677)实际上最小化的是真实残差在由 $M^{-T}M^{-1}$ 定义的加权范数下的值，即 $\|r_k\|_{M^{-T}M^{-1}}$。

如果我们希望算法直接最小化**真实残差的[2-范数](@entry_id:636114)** $\|r_k\|_2$，应该怎么做？这意味着我们需要找到一个[加权内积](@entry_id:163877) $\langle u, v \rangle_W = u^T W v$，使得用这个[内积](@entry_id:158127)的GMRES在作用于[左预处理](@entry_id:165660)系统时，其最小化的目标 $\|r_k^L\|_W$ 等价于 $\|r_k\|_2$。
$$
\|r_k^L\|_W^2 = r_k^T (M^{-T} W M^{-1}) r_k
$$
为了使它等于 $\|r_k\|_2^2 = r_k^T I r_k$，我们必须有 $M^{-T} W M^{-1} = I$。解出 $W$ 得到：
$$
W = M^T M
$$
因此，为了让左[预处理GMRES](@entry_id:753677)最小化真实残差的[2-范数](@entry_id:636114)，我们必须在[Arnoldi过程](@entry_id:166662)中使用由 $W = M^T M$ 定义的[加权内积](@entry_id:163877)。然而，这在计算上是昂贵的。每次计算[内积](@entry_id:158127) $\langle u, v \rangle_W = (Mu)^T(Mv)$ 都需要两次额外的与预处理器 $M$ 相关的[矩阵向量乘法](@entry_id:140544)。这显著增加了每步迭代的成本。因此，在实践中，人们通常接受最小化 $\|M^{-1}r_k\|_2$ 的标准左[预处理GMRES](@entry_id:753677)，因为它在计算成本和[收敛监控](@entry_id:747855)之间提供了一个合理的折衷。

#### 一个警示：当预处理适得其反时

[预处理](@entry_id:141204)并非总是能带来好处。一个设计不佳的[预处理器](@entry_id:753679)，即使它在某些方面看起来是“好的”（例如，减小了初始残差的范数），也可能严重损害收敛，甚至导致算法停滞 。

GMRES的收敛不仅仅取决于[预处理](@entry_id:141204)后算子的[条件数](@entry_id:145150)，还取决于残差向量与Krylov[子空间](@entry_id:150286)方向的对齐程度。在GMRES的第$k$步，残差的减小与[残差向量](@entry_id:165091) $r_{k-1}$ 和[子空间](@entry_id:150286) $A \mathcal{K}_k(A, r_0)$ 之间的夹角有关。简单来说，在第一步，残差的相对减小由 $\sin(\theta)$ 决定，其中 $\theta$ 是当前残差 $r$ 与下一步搜索方向 $Ar$ 之间的夹角。如果 $\theta \approx \pi/2$，即[残差向量](@entry_id:165091)与搜索方向近乎正交，那么 $\sin(\theta) \approx 1$，残差几乎不会减小，算法停滞。

一个坏的[左预处理](@entry_id:165660)器 $M$ 可能导致这种情况。考虑一个初始残差 $r_0$，范数为 $\|r_0\|_2$。预处理器 $M$ 可能使得 $\|r_0^L\|_2 = \|M^{-1}r_0\|_2 \ll \|r_0\|_2$，这看起来是积极的一步。然而，在变换方向后，新的残差 $r_0^L$ 可能变得与新的搜索方向 $B r_0^L = (M^{-1}A)r_0^L$ 近乎正交。

例如，对于矩阵 $A = \operatorname{diag}(1, 0.5)$ 和初始残差 $r_0 = [1, 0]^T$，向量 $Ar_0 = [1, 0]^T$ 与 $r_0$ 完全共线，夹角为$0$，一步GMRES即可收敛。现在，引入一个[预处理器](@entry_id:753679) $M = \begin{pmatrix} 0  10 \\ 1  0 \end{pmatrix}$，其逆为 $M^{-1} = \begin{pmatrix} 0  1 \\ 0.1  0 \end{pmatrix}$。[预处理](@entry_id:141204)后的残差为 $r_0^L = M^{-1}r_0 = [0, 0.1]^T$，其范数为 $0.1$，小于原始[残差范数](@entry_id:754273) $1$。但是，[预处理](@entry_id:141204)后的算子 $B = M^{-1}A = \begin{pmatrix} 0  0.5 \\ 0.1  0 \end{pmatrix}$，新的搜索方向为 $Br_0^L = [0.05, 0]^T$。新的残差 $r_0^L$ 与新的搜索方向 $Br_0^L$ 是正交的！这意味着夹角 $\theta_{\text{left}} = \pi/2$，$\sin(\theta_{\text{left}}) = 1$，一步GMRES完全无法减小预处理后的残差。

这个例子揭示了一个关键教训：一个有效的[预处理器](@entry_id:753679)不仅要使预处理后[算子的谱](@entry_id:272027)[分布](@entry_id:182848)更优（例如，[特征值](@entry_id:154894)聚集在1附近），还必须避免将[残差向量](@entry_id:165091)旋转到与Krylov[子空间](@entry_id:150286)近乎正交的方向。评估预处理器不能仅仅依赖于条件数或初始残差的减小，而必须综合考虑其对整个迭代动力学的影响。