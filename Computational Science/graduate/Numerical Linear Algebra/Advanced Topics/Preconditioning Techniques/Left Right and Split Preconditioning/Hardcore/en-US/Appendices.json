{
    "hands_on_practices": [
        {
            "introduction": "A preconditioner's effectiveness depends not only on how well it approximates the original matrix but also on the iterative scheme it is paired with. This exercise provides a concrete, quantitative comparison between two uses of a simple diagonal preconditioner. By analyzing the convergence of the stationary Jacobi method versus a preconditioned GMRES method for a classic model problem, you will directly calculate the significant performance improvement gained by coupling a preconditioner with a powerful Krylov subspace method. ",
            "id": "3555541",
            "problem": "Consider the linear system with matrix $A \\in \\mathbb{R}^{n \\times n}$ arising from the second-order centered finite-difference discretization of the one-dimensional Dirichlet Laplacian on a uniform mesh, with $n=7$. Thus, $A$ is the tridiagonal Toeplitz matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals. Let the Jacobi splitting be $A = D - (L + U)$, where $D$ is the diagonal of $A$, and $L$ and $U$ are the strictly lower and upper parts of $A$, respectively.\n\nTreat $D$ as a split preconditioner. First, determine the spectral radius $\\rho(D^{-1}(L+U))$. Next, consider applying the Generalized Minimal Residual method (GMRES) with left preconditioning by $D$, which yields the preconditioned operator $D^{-1}A$. Let the stationary Jacobi method update be $x^{(m+1)} = D^{-1}(L+U)x^{(m)} + D^{-1}b$, and define the Jacobi worst-case per-iteration contraction factor as $r_{J} := \\rho(D^{-1}(L+U))$. For the left-preconditioned GMRES applied to the symmetric positive definite (SPD) matrix $D^{-1}A$, use the optimal Chebyshev-polynomial model and define the effective per-iteration contraction factor as $q := \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$, where $\\kappa$ is the condition number of $D^{-1}A$.\n\nQuantify the improvement of preconditioned GMRES over stationary Jacobi by the ratio of iteration counts required to reduce the residual norm by a factor of $10^{-6}$, namely\n$$\n\\text{Improvement} := \\frac{m_{J}}{m_{\\mathrm{GMRES}}},\n$$\nwhere $m_{J}$ is the smallest integer such that $r_{J}^{m_{J}} \\leq 10^{-6}$ and $m_{\\mathrm{GMRES}}$ is the smallest integer such that $q^{m_{\\mathrm{GMRES}}} \\leq 10^{-6}$. Provide this improvement ratio as your final answer.",
            "solution": "The problem has been validated and is deemed scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning the comparison of iterative methods for solving linear systems.\n\nThe problem asks for the improvement ratio of preconditioned GMRES over the stationary Jacobi method for a specific linear system $A\\mathbf{x} = \\mathbf{b}$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=7$ is the tridiagonal Toeplitz matrix given by the centered finite-difference discretization of the 1D Dirichlet Laplacian, with entries $A_{ii} = 2$ and $A_{i, i\\pm 1} = -1$.\n\nThe Jacobi splitting of $A$ is $A = D - (L+U)$, where $D$ is the diagonal part of $A$, and $L$ and $U$ are the strictly lower and upper triangular parts of $A$, respectively. In this case, $D = 2I$, where $I$ is the $n \\times n$ identity matrix. The Jacobi iteration matrix is $T_J = D^{-1}(L+U) = I - D^{-1}A$.\n\nFirst, we determine the convergence factor for the Jacobi method, $r_J = \\rho(T_J)$, which is the spectral radius of $T_J$. The eigenvalues of the matrix $A$ are known to be\n$$\n\\lambda_k(A) = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad k = 1, 2, \\ldots, n\n$$\nWith $n=7$, we have $n+1=8$. The eigenvalues of $T_J = I - D^{-1}A = I - \\frac{1}{2}A$ are\n$$\n\\mu_k(T_J) = 1 - \\frac{1}{2}\\lambda_k(A) = 1 - \\frac{1}{2}\\left(2 - 2\\cos\\left(\\frac{k\\pi}{8}\\right)\\right) = \\cos\\left(\\frac{k\\pi}{8}\\right)\n$$\nfor $k = 1, 2, \\ldots, 7$. The spectral radius is the maximum absolute value of these eigenvalues.\n$$\nr_J = \\rho(T_J) = \\max_{k \\in \\{1, \\ldots, 7\\}} \\left|\\cos\\left(\\frac{k\\pi}{8}\\right)\\right| = \\cos\\left(\\frac{\\pi}{8}\\right)\n$$\nThis is because $\\cos(x)$ is positive and decreasing for $x \\in [0, \\pi/2)$, and $|\\cos(x)| = |\\cos(\\pi-x)|$. The maximum absolute value is attained at the smallest argument, $k=1$, and its counterpart, $k=7$, since $|\\cos(7\\pi/8)| = |-\\cos(\\pi/8)| = \\cos(\\pi/8)$.\n\nNext, we consider GMRES with left preconditioning by $D=2I$. The preconditioned matrix is $M^{-1}A = D^{-1}A = \\frac{1}{2}A$. Since $A$ is symmetric positive definite (SPD), and $D$ is a scalar multiple of the identity, the preconditioned matrix $D^{-1}A$ is also SPD. Its eigenvalues are\n$$\n\\lambda_k(D^{-1}A) = \\frac{1}{2}\\lambda_k(A) = 1 - \\cos\\left(\\frac{k\\pi}{8}\\right), \\quad k=1, \\ldots, 7\n$$\nThe condition number $\\kappa$ of $D^{-1}A$ is the ratio of its largest to its smallest eigenvalue:\n$$\n\\lambda_{\\min}(D^{-1}A) = \\lambda_1 = 1 - \\cos\\left(\\frac{\\pi}{8}\\right)\n$$\n$$\n\\lambda_{\\max}(D^{-1}A) = \\lambda_7 = 1 - \\cos\\left(\\frac{7\\pi}{8}\\right) = 1 - \\cos\\left(\\pi - \\frac{\\pi}{8}\\right) = 1 + \\cos\\left(\\frac{\\pi}{8}\\right)\n$$\nThus, the condition number is\n$$\n\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1 + \\cos(\\pi/8)}{1 - \\cos(\\pi/8)}\n$$\nThe effective per-iteration contraction factor for preconditioned GMRES is given as $q = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$. Substituting the expression for $\\kappa$:\n$$\n\\sqrt{\\kappa} = \\sqrt{\\frac{1 + \\cos(\\pi/8)}{1 - \\cos(\\pi/8)}}\n$$\nUsing the half-angle trigonometric identity $\\cot(x/2) = \\sqrt{\\frac{1+\\cos(x)}{1-\\cos(x)}}$, we get\n$$\n\\sqrt{\\kappa} = \\cot\\left(\\frac{\\pi}{16}\\right)\n$$\nThen, the contraction factor $q$ becomes\n$$\nq = \\frac{\\cot(\\pi/16) - 1}{\\cot(\\pi/16) + 1}\n$$\nUsing the identity $\\tan(\\pi/4 - x) = \\frac{1-\\tan(x)}{1+\\tan(x)} = \\frac{\\cot(x)-1}{\\cot(x)+1}$, we find\n$$\nq = \\tan\\left(\\frac{\\pi}{4} - \\frac{\\pi}{16}\\right) = \\tan\\left(\\frac{3\\pi}{16}\\right)\n$$\nNow we must find the number of iterations required for each method to reduce the residual norm by a factor of $10^{-6}$.\nFor the Jacobi method, we need to find the smallest integer $m_J$ such that $r_J^{m_J} \\leq 10^{-6}$.\n$$\nm_J \\ln(r_J) \\leq -6 \\ln(10) \\implies m_J \\geq \\frac{-6 \\ln(10)}{\\ln(r_J)}\n$$\nSince $m_J$ must be an integer, $m_J = \\left\\lceil \\frac{-6 \\ln(10)}{\\ln(r_J)} \\right\\rceil = \\left\\lceil \\frac{-6 \\ln(10)}{\\ln(\\cos(\\pi/8))} \\right\\rceil$.\nNumerically, $\\ln(10) \\approx 2.302585$, so $-6 \\ln(10) \\approx -13.81551$. The value of $\\cos(\\pi/8) \\approx 0.92388$, so $\\ln(\\cos(\\pi/8)) \\approx -0.079159$.\n$$\nm_J = \\left\\lceil \\frac{-13.81551}{-0.079159} \\right\\rceil = \\lceil 174.542 \\rceil = 175\n$$\nFor preconditioned GMRES, we need the smallest integer $m_{\\mathrm{GMRES}}$ such that $q^{m_{\\mathrm{GMRES}}} \\leq 10^{-6}$.\n$$\nm_{\\mathrm{GMRES}} = \\left\\lceil \\frac{-6 \\ln(10)}{\\ln(q)} \\right\\rceil = \\left\\lceil \\frac{-6 \\ln(10)}{\\ln(\\tan(3\\pi/16))} \\right\\rceil\n$$\nNumerically, $\\tan(3\\pi/16) \\approx 0.668179$, so $\\ln(\\tan(3\\pi/16)) \\approx -0.403205$.\n$$\nm_{\\mathrm{GMRES}} = \\left\\lceil \\frac{-13.81551}{-0.403205} \\right\\rceil = \\lceil 34.264 \\rceil = 35\n$$\nFinally, the improvement ratio is the ratio of the iteration counts:\n$$\n\\text{Improvement} = \\frac{m_J}{m_{\\mathrm{GMRES}}} = \\frac{175}{35} = 5\n$$",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "The choice between left, right, and split preconditioning is a critical decision with direct practical consequences for an iterative solver's behavior. The key difference lies in how each strategy transforms the system and, consequently, the relationship between the true residual and the residual that the algorithm monitors. This practice problem uses a small, tractable example to illuminate these differences, forcing a careful consideration of how to set meaningful stopping criteria to achieve a desired accuracy for the original, untransformed problem. ",
            "id": "3555570",
            "problem": "Consider the linear system $A x = b$ with\n$$\nA = \\begin{bmatrix}4  1 \\\\ 2  3\\end{bmatrix}, \\quad b = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix},\n$$\nand a classical triangular split of $A$ into diagonal, strictly lower, and strictly upper parts, $A = D + L + U$, where\n$$\nD = \\begin{bmatrix}4  0 \\\\ 0  3\\end{bmatrix}, \\quad L = \\begin{bmatrix}0  0 \\\\ 2  0\\end{bmatrix}, \\quad U = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}.\n$$\nDefine the split preconditioners $M_L = D + L = \\begin{bmatrix}4  0 \\\\ 2  3\\end{bmatrix}$ and $M_R = D + U = \\begin{bmatrix}4  1 \\\\ 0  3\\end{bmatrix}$. Consider three preconditioning strategies for $A x = b$:\n(1) left preconditioning with $M_L$, which uses the system $M_L^{-1} A x = M_L^{-1} b$,\n(2) right preconditioning with $M_R$, which uses the system $A M_R^{-1} y = b$ and sets $x = M_R^{-1} y$,\nand (3) split preconditioning, which uses the system $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$ and sets $x = M_R^{-1} z$.\n\nLet $\\|\\cdot\\|_{\\infty}$ denote the vector infinity norm and the induced matrix infinity norm. Suppose one runs a Krylov subspace iterative method, such as the Generalized Minimal Residual (GMRES) method, in each preconditioning regime. In left and split preconditioning, the method’s internally monitored residual is that of the preconditioned system, whereas in right preconditioning it is the residual with respect to the original $A x = b$ system. Two accuracy goals are considered:\n(i) an absolute backward-error goal $\\|r\\|_{\\infty} \\le \\tau$ with $\\tau = 10^{-6}$, where $r = b - A x$ is the true (unpreconditioned) residual,\nand\n(ii) a relative backward-error goal $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$ with $\\varepsilon = 10^{-6}$.\n\nAnalyze how stopping criteria in the internally monitored residual should be chosen in each strategy to achieve these goals and how to correctly recover the physical solution $x$ from the preconditioned variables. Then, select all statements below that are valid for this test problem and these goals.\n\nA. In right preconditioning with $M_R$, the method’s residual equals the true residual $r = b - A x$, so requiring the internally monitored $\\|r\\|_{\\infty} \\le 10^{-6}$ directly enforces the absolute goal; moreover, the physical solution is recovered by $x = M_R^{-1} y$.\n\nB. In left preconditioning with $M_L$, ensuring the internally monitored preconditioned residual $\\|\\hat{r}\\|_{\\infty} \\le 10^{-6}$, where $\\hat{r} = M_L^{-1}(b - A x)$, guarantees $\\|r\\|_{\\infty} \\le 10^{-6}$, so no scaling is needed to meet the absolute goal.\n\nC. In split preconditioning, the internally monitored residual is $\\hat{r} = M_L^{-1}(b - A x)$ in the system $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$, and to ensure $\\|r\\|_{\\infty} \\le \\tau$ it suffices to require $\\|\\hat{r}\\|_{\\infty} \\le \\tau / \\|M_L\\|_{\\infty}$; the physical solution is recovered as $x = M_R^{-1} z$.\n\nD. For the relative backward-error goal $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$ under left preconditioning with $M_L$, if the method monitors $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty}$ with $\\hat{b} = M_L^{-1} b$, then one should stop when $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty} \\le \\varepsilon / \\kappa_{\\infty}(M_L)$, where $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}$; for this test problem, $\\kappa_{\\infty}(M_L) = 2.5$.\n\nE. Right preconditioning corrupts absolute backward-error measurement, so it cannot be used when an absolute residual tolerance is specified.\n\nF. In split preconditioning, the physical solution must be recovered via $x = M_L^{-1} z$ rather than $x = M_R^{-1} z$; this is essential to maintain consistency with the monitored residual.\n\nSelect all correct options.",
            "solution": "The user has requested a detailed analysis of a problem concerning preconditioned iterative methods for solving a linear system. The primary task is to validate the problem statement and, if valid, evaluate several claims about different preconditioning strategies.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following information:\n- The linear system is $A x = b$.\n- The matrix $A$ is given by $A = \\begin{bmatrix}4  1 \\\\ 2  3\\end{bmatrix}$.\n- The vector $b$ is given by $b = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$.\n- A triangular splitting of $A$ is defined as $A = D + L + U$, where:\n  - $D = \\begin{bmatrix}4  0 \\\\ 0  3\\end{bmatrix}$ (diagonal part)\n  - $L = \\begin{bmatrix}0  0 \\\\ 2  0\\end{bmatrix}$ (strictly lower part)\n  - $U = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}$ (strictly upper part)\n- The left preconditioner is $M_L = D + L = \\begin{bmatrix}4  0 \\\\ 2  3\\end{bmatrix}$.\n- The right preconditioner is $M_R = D + U = \\begin{bmatrix}4  1 \\\\ 0  3\\end{bmatrix}$.\n- Three preconditioning strategies are considered for a Krylov subspace method:\n  1.  **Left preconditioning**: Solve $M_L^{-1} A x = M_L^{-1} b$. The internally monitored residual is that of this preconditioned system.\n  2.  **Right preconditioning**: Solve $A M_R^{-1} y = b$ and set $x = M_R^{-1} y$. The internally monitored residual is that of the original system, $r = b - A x$.\n  3.  **Split preconditioning**: Solve $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$ and set $x = M_R^{-1} z$. The internally monitored residual is that of this preconditioned system.\n- The norm used is the infinity norm, $\\|\\cdot\\|_{\\infty}$.\n- The true (unpreconditioned) residual is defined as $r = b - A x$.\n- Two accuracy goals for the true residual are considered:\n  - (i) Absolute backward-error: $\\|r\\|_{\\infty} \\le \\tau$, with $\\tau = 10^{-6}$.\n  - (ii) Relative backward-error: $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$, with $\\varepsilon = 10^{-6}$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is analyzed against the validation criteria.\n- **Scientifically Grounded**: The problem is set firmly within the established theory of numerical linear algebra. Left, right, and split preconditioning are standard techniques for accelerating Krylov subspace methods like GMRES. The concepts of true and preconditioned residuals, as well as norms and condition numbers, are fundamental to this field.\n- **Well-Posed**: The problem is well-posed. The matrices $A$, $M_L$, and $M_R$ are all nonsingular. The questions asked are about the formal relationships between different quantities, which are mathematically derivable. A unique and meaningful analysis is possible.\n- **Objective**: The problem is stated in precise, objective, and unambiguous mathematical language. All terms are standard or explicitly defined.\n- **Flaw Analysis**:\n  1.  **Scientific/Factual Unsoundness**: None. The matrix splittings and definitions are correct.\n  2.  **Non-Formalizable/Irrelevant**: The problem is directly formalizable and central to the topic of preconditioning.\n  3.  **Incomplete/Contradictory Setup**: The setup is complete and internally consistent. It provides all necessary matrices, definitions, and goals.\n  4.  **Unrealistic/Infeasible**: The problem uses a simple $2 \\times 2$ system, which is a common and valid approach for illustrating theoretical concepts.\n  5.  **Ill-Posed/Poorly Structured**: The structure is clear and logical, leading to a well-defined analysis task.\n  6.  **Pseudo-Profound/Trivial**: The problem requires a careful application of norm inequalities and definitions of preconditioning, testing conceptual understanding rather than being trivial.\n  7.  **Outside Scientific Verifiability**: All claims are mathematically verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The analysis can proceed.\n\n### Derivation and Option Analysis\n\nLet $x_k$ be the $k$-th iterate of the solution. The true residual is $r_k = b - A x_k$. We analyze the relationship between the true residual and the monitored residual for each strategy.\n\n**Strategy 1: Left Preconditioning**\nThe system solved is $(M_L^{-1} A) x = M_L^{-1} b$.\nThe iterative method generates iterates $x_k$ and the corresponding monitored (preconditioned) residual is\n$$\n\\hat{r}_k = (M_L^{-1} b) - (M_L^{-1} A) x_k = M_L^{-1} (b - A x_k) = M_L^{-1} r_k.\n$$\nFrom this relationship, we have $r_k = M_L \\hat{r}_k$. Applying norms, we get the bounds:\n$$\n\\|r_k\\|_{\\infty} = \\|M_L \\hat{r}_k\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}_k\\|_{\\infty}\n$$\nand\n$$\n\\|\\hat{r}_k\\|_{\\infty} = \\|M_L^{-1} r_k\\|_{\\infty} \\le \\|M_L^{-1}\\|_{\\infty} \\|r_k\\|_{\\infty}.\n$$\n\n**Strategy 2: Right Preconditioning**\nThe system solved is $(A M_R^{-1}) y = b$. The iterative method generates iterates $y_k$. The physical solution iterate is $x_k = M_R^{-1} y_k$.\nThe problem states that the monitored residual is that of the original system. Let's verify this is consistent. The residual of the transformed system is $b - (A M_R^{-1}) y_k$.\nSubstituting $y_k = M_R x_k$, this becomes $b - (A M_R^{-1}) (M_R x_k) = b - A x_k = r_k$.\nThus, the monitored residual is indeed the true residual, $\\hat{r}_k = r_k$. This is a key advantage of right preconditioning. The solution is recovered via $x = M_R^{-1} y$.\n\n**Strategy 3: Split Preconditioning**\nThe system solved is $(M_L^{-1} A M_R^{-1}) z = M_L^{-1} b$. The iterative method generates iterates $z_k$. The physical solution is recovered by setting $x_k = M_R^{-1} z_k$.\nThe preconditioned residual is\n$$\n\\hat{r}_k = (M_L^{-1} b) - (M_L^{-1} A M_R^{-1}) z_k = M_L^{-1} (b - A (M_R^{-1} z_k)).\n$$\nSubstituting $x_k = M_R^{-1} z_k$, we get\n$$\n\\hat{r}_k = M_L^{-1} (b - A x_k) = M_L^{-1} r_k.\n$$\nThis is the same relationship as in left preconditioning: $r_k = M_L \\hat{r}_k$.\n\nWith these principles established, we evaluate each option.\n\n**A. In right preconditioning with $M_R$, the method’s residual equals the true residual $r = b - A x$, so requiring the internally monitored $\\|r\\|_{\\infty} \\le 10^{-6}$ directly enforces the absolute goal; moreover, the physical solution is recovered by $x = M_R^{-1} y$.**\nOur analysis of right preconditioning shows that the monitored residual is identical to the true residual, $r = b - A x$. Therefore, a stopping criterion based on the monitored residual, $\\|\\hat{r}\\|_{\\infty} \\le \\tau$, directly enforces the goal on the true residual, $\\|r\\|_{\\infty} \\le \\tau$. The variable transformation is correctly stated as $x = M_R^{-1} y$. The statement is entirely accurate.\n**Verdict: Correct.**\n\n**B. In left preconditioning with $M_L$, ensuring the internally monitored preconditioned residual $\\|\\hat{r}\\|_{\\infty} \\le 10^{-6}$, where $\\hat{r} = M_L^{-1}(b - A x)$, guarantees $\\|r\\|_{\\infty} \\le 10^{-6}$, so no scaling is needed to meet the absolute goal.**\nThe relationship between the true and preconditioned residuals is $r = M_L \\hat{r}$. Taking norms, $\\|r\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}$. The statement implies that $\\|M_L\\|_{\\infty} \\le 1$. Let's calculate $\\|M_L\\|_{\\infty}$.\n$$\nM_L = \\begin{bmatrix}4  0 \\\\ 2  3\\end{bmatrix}\n$$\nThe infinity norm of a matrix is the maximum absolute row sum.\n- Row 1 sum: $|4| + |0| = 4$.\n- Row 2 sum: $|2| + |3| = 5$.\nThus, $\\|M_L\\|_{\\infty} = 5$. Since $\\|M_L\\|_{\\infty}  1$, the condition $\\|\\hat{r}\\|_{\\infty} \\le 10^{-6}$ only guarantees $\\|r\\|_{\\infty} \\le 5 \\times 10^{-6}$. It does not guarantee $\\|r\\|_{\\infty} \\le 10^{-6}$. In fact, the true residual could be larger than the monitored residual. Scaling is necessary.\n**Verdict: Incorrect.**\n\n**C. In split preconditioning, the internally monitored residual is $\\hat{r} = M_L^{-1}(b - A x)$ in the system $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$, and to ensure $\\|r\\|_{\\infty} \\le \\tau$ it suffices to require $\\|\\hat{r}\\|_{\\infty} \\le \\tau / \\|M_L\\|_{\\infty}$; the physical solution is recovered as $x = M_R^{-1} z$.**\nThis statement has three parts.\n1.  **Physical solution recovery**: The system being solved is $A'z = b'$ where $A' = M_L^{-1} A M_R^{-1}$ and $b' = M_L^{-1} b$. Left-multiplying by $M_L$ gives $A M_R^{-1} z = b$. Comparing this with the original system $Ax=b$, we see that $x=M_R^{-1}z$. This is correct.\n2.  **Monitored residual**: As derived earlier for split preconditioning, the monitored residual $\\hat{r}$ (for the system in $z$) is related to the true residual $r = b - Ax$ by $\\hat{r} = M_L^{-1} r$. The statement expresses this as $\\hat{r} = M_L^{-1}(b-Ax)$, which is correct.\n3.  **Stopping criterion**: From $r = M_L \\hat{r}$, we have $\\|r\\|_{\\infty} = \\|M_L \\hat{r}\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}$. To ensure $\\|r\\|_{\\infty} \\le \\tau$, it is sufficient to enforce $\\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty} \\le \\tau$, which rearranges to $\\|\\hat{r}\\|_{\\infty} \\le \\tau / \\|M_L\\|_{\\infty}$. This part is also correct.\nAll parts of the statement are correct.\n**Verdict: Correct.**\n\n**D. For the relative backward-error goal $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$ under left preconditioning with $M_L$, if the method monitors $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty}$ with $\\hat{b} = M_L^{-1} b$, then one should stop when $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty} \\le \\varepsilon / \\kappa_{\\infty}(M_L)$, where $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}$; for this test problem, $\\kappa_{\\infty}(M_L) = 2.5$.**\nLet's analyze the relative error propagation. The true relative residual is $\\|r\\|_{\\infty} / \\|b\\|_{\\infty}$. The monitored relative residual is $\\|\\hat{r}\\|_{\\infty} / \\|\\hat{b}\\|_{\\infty}$.\nWe have the relationships $r = M_L \\hat{r}$ and $b = M_L \\hat{b}$. From these, we get the inequalities:\n$\\|r\\|_{\\infty} \\le \\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}$\n$\\|\\hat{b}\\|_{\\infty} = \\|M_L^{-1} b\\|_{\\infty} \\le \\|M_L^{-1}\\|_{\\infty} \\|b\\|_{\\infty} \\implies \\|b\\|_{\\infty} \\ge \\frac{\\|\\hat{b}\\|_{\\infty}}{\\|M_L^{-1}\\|_{\\infty}}$.\nCombining these to bound the true relative error:\n$$\n\\frac{\\|r\\|_{\\infty}}{\\|b\\|_{\\infty}} \\le \\frac{\\|M_L\\|_{\\infty} \\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty} / \\|M_L^{-1}\\|_{\\infty}} = \\left(\\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}\\right) \\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}} = \\kappa_{\\infty}(M_L) \\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}}.\n$$\nTo guarantee $\\|r\\|_{\\infty} / \\|b\\|_{\\infty} \\le \\varepsilon$, it is sufficient to require $\\kappa_{\\infty}(M_L) \\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}} \\le \\varepsilon$, which means the stopping criterion on the monitored relative residual should be $\\frac{\\|\\hat{r}\\|_{\\infty}}{\\|\\hat{b}\\|_{\\infty}} \\le \\frac{\\varepsilon}{\\kappa_{\\infty}(M_L)}$. This logic is correct.\nNow, we calculate $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty}$.\nWe already found $\\|M_L\\|_{\\infty} = 5$. We need $M_L^{-1}$.\nThe determinant is $\\det(M_L) = (4)(3) - (0)(2) = 12$.\nThe inverse is $M_L^{-1} = \\frac{1}{12}\\begin{bmatrix}3  0 \\\\ -2  4\\end{bmatrix} = \\begin{bmatrix}1/4  0 \\\\ -1/6  1/3\\end{bmatrix}$.\nThe infinity norm of the inverse is the maximum absolute row sum:\n- Row 1 sum: $|1/4| + |0| = 1/4 = 0.25$.\n- Row 2 sum: $|-1/6| + |1/3| = 1/6 + 2/6 = 3/6 = 1/2 = 0.5$.\nSo, $\\|M_L^{-1}\\|_{\\infty} = 1/2$.\nThe condition number is $\\kappa_{\\infty}(M_L) = \\|M_L\\|_{\\infty} \\|M_L^{-1}\\|_{\\infty} = 5 \\times (1/2) = 2.5$.\nThe calculated value matches the value given in the statement. Both parts of the statement are correct.\n**Verdict: Correct.**\n\n**E. Right preconditioning corrupts absolute backward-error measurement, so it cannot be used when an absolute residual tolerance is specified.**\nThis statement is the opposite of the truth. As established in the analysis of option A, the monitored residual in right preconditioning IS the true residual. It provides a direct, uncorrupted measure of the backward error. It is the most reliable of the three methods for enforcing a specific tolerance on the true residual.\n**Verdict: Incorrect.**\n\n**F. In split preconditioning, the physical solution must be recovered via $x = M_L^{-1} z$ rather than $x = M_R^{-1} z$; this is essential to maintain consistency with the monitored residual.**\nAs derived in the analysis for option C, the system being solved is $M_L^{-1} A M_R^{-1} z = M_L^{-1} b$. Multiplying by $M_L$ yields $A (M_R^{-1} z) = b$. Comparing this to the original system $Ax=b$, the correct transformation is $x = M_R^{-1} z$. The statement suggests $x = M_L^{-1} z$, which is incorrect.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Ideal preconditioners are both effective and inexpensive to construct and apply, but their construction can encounter numerical hurdles. A classic example is the potential for breakdown in Incomplete LU (ILU) factorization when a zero (or small) pivot is encountered. This exercise demonstrates how pivoting resolves this instability and then uncovers the profound, and different, implications this has for the algebraic structure of the resulting left- and right-preconditioned operators. ",
            "id": "3555596",
            "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the sparse matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}.\n$$\nConsider performing an Incomplete Lower-Upper (ILU) factorization with zero fill-in, denoted $\\mathrm{ILU}(0)$, under the natural row ordering without pivoting, so that the incomplete factors $L$ and $U$ inherit the sparsity pattern of the strictly lower and upper triangular parts of $A$ respectively, with $L$ unit lower triangular. Define the preconditioning matrix $M$ by $M = L U$.\n\nTasks:\n1. Using only the fundamental definitions of factorization and pivoting, argue whether the $\\mathrm{ILU}(0)$ process on $A$ without pivoting can maintain nonsingularity of $M$ and justify your conclusion by analyzing the first pivot.\n2. Let $P \\in \\mathbb{R}^{3 \\times 3}$ be the permutation matrix that swaps the first and second rows. Perform $\\mathrm{ILU}(0)$ on the permuted matrix $P A$ and construct explicit $L$ and $U$ such that $P A = L U$ with the same sparsity constraints as above. Define the corresponding preconditioner $M := L U$.\n3. Using only the definitions of left and right preconditioning, analyze the consequences of this pivoting for the preconditioned operators. Specifically, define the left-preconditioned operator $T_{\\mathrm{left}} := M^{-1} A$ and the right-preconditioned operator $T_{\\mathrm{right}} := A M^{-1}$, and derive their exact forms in terms of $P$.\n4. Compute the determinant of the left-preconditioned operator $T_{\\mathrm{left}}$. Express your final answer as a single real number. No rounding is required.",
            "solution": "The problem will be addressed by sequentially completing the four tasks as stated.\n\nThe given sparse matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}.\n$$\n\nTask 1: Analysis of $\\mathrm{ILU}(0)$ on $A$ without pivoting.\n\nThe $\\mathrm{ILU}(0)$ factorization constructs an approximation $M=LU \\approx A$, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. The sparsity pattern of $L$ is constrained to that of the strictly lower triangular part of $A$, and the sparsity pattern of $U$ is constrained to that of the upper triangular part of $A$.\n\nThe ILU factorization process is a variant of Gaussian elimination. The first step involves the pivot element $A_{11}$. To compute the entries of the factors, we would typically set $u_{11} = A_{11}$. In this problem, $A_{11} = 0$.\n\nThe algorithm for computing the incomplete factorization would require calculating the multiplier $l_{21}$ as $l_{21} = A_{21} / u_{11} = A_{21} / A_{11}$. Since $A_{21}=1$ and $A_{11}=0$, this computation involves division by zero. Therefore, the $\\mathrm{ILU}(0)$ process on $A$ without pivoting fails at the very first step.\n\nIf the factorization could proceed, the preconditioner would be $M = LU$. The determinant of $M$ is $\\det(M) = \\det(L)\\det(U)$. Since $L$ is unit lower triangular, $\\det(L)=1$. The determinant of $U$ is the product of its diagonal elements, $\\det(U) = u_{11} u_{22} u_{33}$. As established, the algorithm would set $u_{11} = A_{11} = 0$. Consequently, $\\det(U) = 0$, which implies $\\det(M) = 0$.\nA singular preconditioner $M$ is computationally undesirable as its inverse $M^{-1}$ does not exist, making it impossible to solve the preconditioned system.\nConclusion for Task 1: The $\\mathrm{ILU}(0)$ process on $A$ without pivoting fails due to a zero pivot at position $(1,1)$. This failure means a nonsingular preconditioner $M$ cannot be formed; if it were formally constructed, it would be singular because its first diagonal element $u_{11}$ would be zero.\n\nTask 2: $\\mathrm{ILU}(0)$ on the permuted matrix $PA$.\n\nThe permutation matrix $P$ that swaps the first and second rows is:\n$$\nP = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThe permuted matrix $PA$ is:\n$$\nPA = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix} =\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  1  1\n\\end{pmatrix}.\n$$\nLet us denote this permuted matrix as $A' = PA$. We perform $\\mathrm{ILU}(0)$ on $A'$. The problem asks to use the same sparsity constraints, which are interpreted as inheriting the pattern from the matrix being factored, i.e., $A'$.\nThe matrix $A'$ has non-zero entries in its strictly lower part only at position $(3,2)$. Its upper part has non-zeroes at $(1,1), (1,3), (2,2), (3,3)$. The factors $L$ and $U$ for $A'$ will have the forms:\n$$\nL = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  l_{32}  1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\nu_{11}  0  u_{13} \\\\\n0  u_{22}  0 \\\\\n0  0  u_{33}\n\\end{pmatrix}\n$$\nbecause $A'_{21}=0, A'_{31}=0, A'_{12}=0, A'_{23}=0$.\nThe product $LU$ is:\n$$\nLU = \\begin{pmatrix}\nu_{11}  0  u_{13} \\\\\n0  u_{22}  0 \\\\\n0  l_{32}u_{22}  u_{33}\n\\end{pmatrix}.\n$$\nWe equate the non-zero entries of $LU$ to the corresponding entries of $A'$:\n$$\n\\begin{pmatrix}\nu_{11}  0  u_{13} \\\\\n0  u_{22}  0 \\\\\n0  l_{32}u_{22}  u_{33}\n\\end{pmatrix} =\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  1  1\n\\end{pmatrix}.\n$$\nFrom this equality, we find the coefficients:\n$u_{11} = 1$\n$u_{13} = 1$\n$u_{22} = 1$\n$l_{32}u_{22} = 1 \\implies l_{32}(1) = 1 \\implies l_{32} = 1$\n$u_{33} = 1$\n\nThe resulting factors are:\n$$\nL = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  1  1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThe preconditioner is $M=LU$. Let's compute it:\n$$\nM = LU = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} =\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  1  1\n\\end{pmatrix}.\n$$\nWe observe that $M=PA$. In this case, the $\\mathrm{ILU}(0)$ factorization of the permuted matrix $PA$ is exact.\n\nTask 3: Analysis of the preconditioned operators.\n\nWe have the preconditioner $M = LU = PA$. The preconditioned operators are $T_{\\mathrm{left}} = M^{-1}A$ and $T_{\\mathrm{right}} = AM^{-1}$.\n\nFor the right-preconditioned operator $T_{\\mathrm{right}}$:\nWe substitute $M=PA$:\n$T_{\\mathrm{right}} = A M^{-1} = A (PA)^{-1}$.\nUsing the property $(XY)^{-1} = Y^{-1}X^{-1}$, we get:\n$T_{\\mathrm{right}} = A (A^{-1}P^{-1}) = (AA^{-1})P^{-1} = I P^{-1} = P^{-1}$.\nThe permutation matrix $P$ is an involution, meaning $P^2=I$, so $P^{-1}=P$.\nThus, the right-preconditioned operator is exactly the permutation matrix $P$:\n$T_{\\mathrm{right}} = P^{-1} = P$.\n\nFor the left-preconditioned operator $T_{\\mathrm{left}}$:\nWe substitute $M=PA$:\n$T_{\\mathrm{left}} = M^{-1}A = (PA)^{-1}A$.\nAgain, using $(XY)^{-1} = Y^{-1}X^{-1}$:\n$T_{\\mathrm{left}} = (A^{-1}P^{-1})A = A^{-1}P^{-1}A$.\nUsing $P^{-1}=P$, we have:\n$T_{\\mathrm{left}} = A^{-1}PA$.\nThis shows that the left-preconditioned operator $T_{\\mathrm{left}}$ is a similarity transformation of the permutation matrix $P$.\nThe exact forms in terms of $P$ (and $A$) are $T_{\\mathrm{right}} = P^{-1}$ and $T_{\\mathrm{left}} = A^{-1}P^{-1}A$.\n\nTask 4: Determinant of the left-preconditioned operator $T_{\\mathrm{left}}$.\n\nWe use the derived form $T_{\\mathrm{left}} = A^{-1}P^{-1}A$. The determinant of a product of matrices is the product of their determinants:\n$$\n\\det(T_{\\mathrm{left}}) = \\det(A^{-1}P^{-1}A) = \\det(A^{-1}) \\det(P^{-1}) \\det(A).\n$$\nSince $\\det(A^{-1}) = 1/\\det(A)$ and $\\det(P^{-1}) = 1/\\det(P)$, we have:\n$$\n\\det(T_{\\mathrm{left}}) = \\frac{1}{\\det(A)} \\frac{1}{\\det(P)} \\det(A) = \\frac{1}{\\det(P)}.\n$$\nAlternatively and more simply, the determinant is invariant under similarity transformations:\n$$\n\\det(T_{\\mathrm{left}}) = \\det(A^{-1}(P^{-1})A) = \\det(P^{-1}).\n$$\nThe matrix $P$ is obtained from the identity matrix $I$ by swapping two rows (the first and second). A single row swap negates the determinant. Since $\\det(I)=1$, the determinant of $P$ is:\n$$\n\\det(P) = -1.\n$$\nTherefore, $\\det(P^{-1}) = 1/\\det(P) = 1/(-1) = -1$.\nSo, the determinant of the left-preconditioned operator is:\n$$\n\\det(T_{\\mathrm{left}}) = -1.\n$$\nThis result is independent of the matrix $A$, provided $A$ is invertible, which it is ($\\det(A) = 0(0-1) - 1(1-0) + 0(1-0) = -1 \\neq 0$).",
            "answer": "$$\n\\boxed{-1}\n$$"
        }
    ]
}