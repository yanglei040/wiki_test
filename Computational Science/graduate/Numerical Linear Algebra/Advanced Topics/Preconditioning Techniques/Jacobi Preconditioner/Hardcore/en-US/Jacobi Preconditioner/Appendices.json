{
    "hands_on_practices": [
        {
            "introduction": "Understanding any numerical method begins with a deep dive into its behavior on a canonical problem. This exercise explores the Jacobi method in its original role as a stationary iterative solver, applied to the matrix arising from the one-dimensional Poisson equation. By deriving the explicit form of the iteration matrix and its spectral radius from first principles, you will gain fundamental insight into the mechanics that govern its convergence. ",
            "id": "3552896",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be the tridiagonal matrix arising from the one-dimensional Poisson problem with homogeneous Dirichlet boundary conditions, having $2$ on the diagonal and $-1$ on the first sub- and super-diagonals. Let $M = \\operatorname{diag}(A)$ denote the Jacobi (diagonal) preconditioner. \n\n(a) Compute $M$ and $M^{-1}A$ explicitly.\n\n(b) Let $G = I - M^{-1}A$ be the Jacobi iteration matrix for the linear system $A x = b$. Starting from the fundamental definition of eigenvalues and eigenvectors, and using only the structural properties of $A$, derive the eigenpairs of $A$ from first principles by solving the associated second-order linear difference equation with homogeneous boundary conditions. Use this derivation to obtain the eigenvalues of $G$. Finally, determine the spectral radius $\\rho(G)$ and express it as a closed-form analytic expression in terms of $n$. \n\nProvide your final answer as the closed-form analytic expression for $\\rho(G)$ in terms of $n$. No numerical rounding is required.",
            "solution": "The problem is to analyze the Jacobi preconditioner for the one-dimensional discrete Poisson problem. The problem is presented in two parts. We will first validate the problem statement and then proceed to solve it.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- $A \\in \\mathbb{R}^{n \\times n}$ is a tridiagonal matrix.\n- The diagonal entries of $A$ are $2$.\n- The first sub-diagonal and super-diagonal entries of $A$ are $-1$.\n- $A$ arises from the one-dimensional Poisson problem with homogeneous Dirichlet boundary conditions.\n- $M = \\operatorname{diag}(A)$ is the Jacobi (diagonal) preconditioner.\n- $G = I - M^{-1}A$ is the Jacobi iteration matrix.\n- Task (a): Compute $M$ and $M^{-1}A$ explicitly.\n- Task (b): Derive the eigenpairs of $A$ from first principles by solving the associated second-order linear difference equation. Use this to obtain the eigenvalues of $G$ and determine the spectral radius $\\rho(G)$ as a closed-form analytic expression in terms of $n$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a classic exercise in numerical linear algebra. The matrix $A$ is the negative of the discrete one-dimensional Laplacian with a scaling factor, a cornerstone in the numerical solution of partial differential equations. The Jacobi method and its analysis are fundamental topics in iterative methods. The problem is scientifically sound.\n- **Well-Posed:** All terms are well-defined. The matrix $A$ is explicitly described. The tasks are specific and lead to a unique, well-defined mathematical result.\n- **Objective:** The problem is stated using precise mathematical language, free from ambiguity or subjective claims.\n\n**Verdict**\nThe problem is valid. It is a well-posed, standard problem in numerical analysis and linear algebra. We may proceed with the solution.\n\n### Part (a): Computation of $M$ and $M^{-1}A$\n\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by its components: $A_{ii} = 2$, $A_{i,i\\pm 1} = -1$, and $A_{ij} = 0$ otherwise. Explicitly, it is:\n$$\nA = \\begin{pmatrix}\n2  -1  0  \\dots  0 \\\\\n-1  2  -1  \\dots  0 \\\\\n0  -1  2  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1 \\\\\n0  \\dots  0  -1  2\n\\end{pmatrix}\n$$\nThe Jacobi preconditioner $M$ is defined as the diagonal of $A$, $M = \\operatorname{diag}(A)$.\n$$\nM = \\operatorname{diag}(A_{11}, A_{22}, \\dots, A_{nn}) = \\operatorname{diag}(2, 2, \\dots, 2) = 2I\n$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nThe inverse of $M$ is straightforward to compute:\n$$\nM^{-1} = (2I)^{-1} = \\frac{1}{2}I = \\begin{pmatrix}\n1/2  0  \\dots  0 \\\\\n0  1/2  \\dots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\vdots \\\\\n0  \\dots  0  1/2\n\\end{pmatrix}\n$$\nNow, we compute the preconditioned matrix $M^{-1}A$:\n$$\nM^{-1}A = \\left(\\frac{1}{2}I\\right)A = \\frac{1}{2}A = \\frac{1}{2} \\begin{pmatrix}\n2  -1  0  \\dots  0 \\\\\n-1  2  -1  \\dots  0 \\\\\n0  -1  2  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1 \\\\\n0  \\dots  0  -1  2\n\\end{pmatrix}\n$$\nThus,\n$$\nM^{-1}A = \\begin{pmatrix}\n1  -1/2  0  \\dots  0 \\\\\n-1/2  1  -1/2  \\dots  0 \\\\\n0  -1/2  1  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  -1/2 \\\\\n0  \\dots  0  -1/2  1\n\\end{pmatrix}\n$$\n\n### Part (b): Eigenanalysis and Spectral Radius\n\n**Step 1: Derivation of Eigenpairs of $A$**\nLet $(\\lambda, v)$ be an eigenpair of $A$, where $v = (v_1, v_2, \\dots, v_n)^T \\neq 0$. The eigenvalue equation is $Av = \\lambda v$. Writing this component-wise for the $j$-th component gives:\n$$ -v_{j-1} + 2v_j - v_{j+1} = \\lambda v_j, \\quad \\text{for } j=1, \\dots, n $$\nThe problem specifies that $A$ arises from a problem with homogeneous Dirichlet boundary conditions, which implies $v_0 = 0$ and $v_{n+1} = 0$. These conditions are consistent with the first ($j=1$) and last ($j=n$) rows of the matrix equation.\n\nWe can rearrange the equation into a second-order linear homogeneous difference equation:\n$$ v_{j+1} + (\\lambda - 2)v_j + v_{j-1} = 0 $$\nThe characteristic equation for this recurrence relation is $r^2 + (\\lambda - 2)r + 1 = 0$.\nThe matrix $A$ is symmetric and positive definite, so its eigenvalues $\\lambda$ are real and positive. Gerschgorin's circle theorem applied to any row of $A$ implies that all eigenvalues lie in the interval $[2-|-1|-|-1|, 2+|-1|+|-1|] = [0, 4]$. Since $A$ is invertible ($\\det(A)=n+1$), $\\lambda \\neq 0$. Thus, $0  \\lambda  4$, which implies $-2  \\lambda-2  2$, and $(\\lambda-2)^2  4$. The discriminant of the characteristic equation, $(\\lambda-2)^2-4$, is negative. Therefore, the roots are a complex conjugate pair.\n\nLet's make the substitution $\\lambda - 2 = -2\\cos(\\theta)$ for some $\\theta$. The characteristic equation becomes $r^2 - 2\\cos(\\theta)r + 1 = 0$. The roots are $r = \\cos(\\theta) \\pm i\\sin(\\theta) = \\exp(\\pm i\\theta)$.\nThe general solution for $v_j$ is a linear combination of the powers of these roots:\n$$ v_j = c_1 \\exp(ij\\theta) + c_2 \\exp(-ij\\theta) = C_1 \\cos(j\\theta) + C_2 \\sin(j\\theta) $$\nWe apply the boundary conditions to find the constants and the allowed values of $\\theta$.\n1.  $v_0 = 0$: $C_1 \\cos(0) + C_2 \\sin(0) = C_1 = 0$.\n    The solution must be of the form $v_j = C_2 \\sin(j\\theta)$.\n2.  $v_{n+1} = 0$: $C_2 \\sin((n+1)\\theta) = 0$.\n    For a non-trivial eigenvector, we require $C_2 \\neq 0$, which implies $\\sin((n+1)\\theta) = 0$.\n\nThis condition holds if $(n+1)\\theta = k\\pi$ for an integer $k$. Thus, $\\theta_k = \\frac{k\\pi}{n+1}$.\nThe values $k=1, 2, \\dots, n$ give $n$ distinct, non-trivial eigenvectors. For $k=0$ or $k=n+1$, $\\theta_k$ is a multiple of $\\pi$, leading to $v_j = C_2\\sin(j\\pi \\cdot (\\text{integer})) = 0$, the trivial vector. For other integer values of $k$, the resulting eigenvectors are linearly dependent on the first $n$ vectors.\n\nSo, the eigenvectors of $A$, denoted $v^{(k)}$, have components:\n$$ v_j^{(k)} = \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad j = 1, \\dots, n $$\nfor each $k=1, \\dots, n$. (We can set $C_2=1$ as eigenvectors are defined up to a scalar).\n\nThe corresponding eigenvalues, $\\lambda_A^{(k)}$, are found from our substitution $\\lambda = 2 - 2\\cos(\\theta)$:\n$$ \\lambda_A^{(k)} = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right) $$\nfor $k=1, \\dots, n$.\n\n**Step 2: Eigenvalues of the Jacobi Iteration Matrix $G$**\nThe Jacobi iteration matrix is $G = I - M^{-1}A$. From part (a), we know $M=2I$, so $G = I - \\frac{1}{2}A$.\nLet $(\\lambda_A^{(k)}, v^{(k)})$ be an eigenpair of $A$. Then $v^{(k)}$ is also an eigenvector of $G$:\n$$ G v^{(k)} = \\left(I - \\frac{1}{2}A\\right) v^{(k)} = I v^{(k)} - \\frac{1}{2} A v^{(k)} = v^{(k)} - \\frac{1}{2}\\lambda_A^{(k)} v^{(k)} = \\left(1 - \\frac{1}{2}\\lambda_A^{(k)}\\right) v^{(k)} $$\nSo, the eigenvalues of $G$, denoted $\\lambda_G^{(k)}$, are given by $\\lambda_G^{(k)} = 1 - \\frac{1}{2}\\lambda_A^{(k)}$.\nSubstituting the expression for $\\lambda_A^{(k)}$:\n$$ \\lambda_G^{(k)} = 1 - \\frac{1}{2}\\left(2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) = 1 - 1 + \\cos\\left(\\frac{k\\pi}{n+1}\\right) = \\cos\\left(\\frac{k\\pi}{n+1}\\right) $$\nfor $k=1, \\dots, n$.\n\n**Step 3: Spectral Radius of $G$**\nThe spectral radius $\\rho(G)$ is the maximum absolute value of its eigenvalues:\n$$ \\rho(G) = \\max_{k \\in \\{1,\\dots,n\\}} |\\lambda_G^{(k)}| = \\max_{k \\in \\{1,\\dots,n\\}} \\left| \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right| $$\nThe arguments $\\frac{k\\pi}{n+1}$ for $k=1, \\dots, n$ lie in the interval $(0, \\pi)$. The function $|\\cos(x)|$ on $x \\in (0, \\pi)$ achieves its maximum value as $x$ approaches the endpoints of the interval, $0$ and $\\pi$.\nThe set of arguments is $\\left\\{ \\frac{\\pi}{n+1}, \\frac{2\\pi}{n+1}, \\dots, \\frac{n\\pi}{n+1} \\right\\}$.\nThe argument closest to $0$ is $\\frac{\\pi}{n+1}$ (for $k=1$).\nThe argument closest to $\\pi$ is $\\frac{n\\pi}{n+1}$ (for $k=n$).\n\nLet's evaluate the magnitude at these points:\n- For $k=1$: $\\left| \\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right)$, since $\\frac{\\pi}{n+1} \\in (0, \\pi/2)$ for $n \\ge 1$.\n- For $k=n$: $\\left| \\cos\\left(\\frac{n\\pi}{n+1}\\right) \\right| = \\left| \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) \\right| = \\left| -\\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right)$.\n\nSince the function $|\\cos(x)|$ is monotonically decreasing on $[0, \\pi/2]$ and the set of arguments $\\frac{k\\pi}{n+1}$ for $k  (n+1)/2$ are all less than $\\pi/2$, the maximum value is attained at the smallest argument, which is for $k=1$. The arguments are symmetric about $\\pi/2$, so the maximum is attained at both $k=1$ and $k=n$.\nTherefore, the spectral radius is:\n$$ \\rho(G) = \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nThis is a closed-form analytic expression for the spectral radius in terms of $n$.",
            "answer": "$$ \\boxed{\\cos\\left(\\frac{\\pi}{n+1}\\right)} $$"
        },
        {
            "introduction": "While the Jacobi method can be used as a solver, its modern application is often as a simple preconditioner for more powerful methods like the Conjugate Gradient (CG) algorithm. This practice shifts our focus to evaluate its effectiveness in this role. By analyzing the effect of Jacobi preconditioning on the spectral condition number of the 1D Poisson matrix, you will discover a crucial lesson about why the simplest choice is not always a beneficial one for accelerating convergence. ",
            "id": "3552956",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be the standard symmetric positive definite tridiagonal matrix arising from the second-order finite difference discretization of the one-dimensional Poisson problem $-u''=f$ on the unit interval with homogeneous Dirichlet boundary conditions, using $n$ interior grid points with uniform mesh size $h = \\frac{1}{n+1}$. Thus,\n$$\nA \\;=\\; \\frac{1}{h^{2}}\\,\\mathrm{tridiag}(-1,\\,2,\\,-1).\n$$\nLet $D = \\mathrm{diag}(A)$ denote the Jacobi (diagonal) preconditioner. Work in the spectral $2$-norm and use only fundamental facts about eigenvalues of symmetric positive definite matrices, condition numbers, and basic discrete Fourier/sine analysis of Toeplitz tridiagonal operators.\n\n(a) Derive the eigenpairs of $A$ and use them to obtain an exact expression for the condition number $\\kappa_{2}(A)$ in terms of $n$.\n\n(b) Derive the eigenpairs of the preconditioned matrix $D^{-1}A$ and obtain an exact expression for $\\kappa_{2}(D^{-1}A)$ in terms of $n$.\n\n(c) Using your expressions, determine the asymptotic behavior in $n$ of $\\kappa_{2}(A)$ and $\\kappa_{2}(D^{-1}A)$, and compute the exact value of the limit\n$$\nL \\;=\\; \\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1}A)}{\\kappa_{2}(A)}.\n$$\n\n(d) Based on the canonical error reduction bound for the Conjugate Gradient (CG) method (Conjugate Gradient (CG) defined as the Krylov subspace method minimizing the $A$-norm of the error for symmetric positive definite matrices), interpret the practical impact of your findings on the iteration counts needed to achieve a fixed relative reduction in the $A$-norm of the error when using Jacobi preconditioning for this problem, compared with no preconditioning.\n\nProvide as your final answer the exact value of $L$.",
            "solution": "The problem asks for a multi-part analysis of the standard finite difference matrix for the one-dimensional Poisson problem and the effect of Jacobi preconditioning on its properties, particularly the condition number and its implication for the Conjugate Gradient (CG) method. The analysis will proceed in four parts as requested.\n\n(a) Derivation of the eigenpairs and condition number of $A$.\n\nThe matrix $A$ is given by\n$$\nA = \\frac{1}{h^2} \\mathrm{tridiag}(-1, 2, -1),\n$$\nwhere $h = \\frac{1}{n+1}$. Let $T$ be the Toeplitz matrix $T = \\mathrm{tridiag}(-1, 2, -1)$. The matrix $A$ is a scalar multiple of $T$, so they share the same eigenvectors. The eigenpairs $(\\mu_k, v_k)$ of the matrix $T$ for $k=1, \\dots, n$ are well-known from discrete Fourier analysis. The components $(v_k)_j$ of the $k$-th eigenvector $v_k$ are given by\n$$\n(v_k)_j = \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad j=1, \\dots, n.\n$$\nTo find the corresponding eigenvalue $\\mu_k$, we apply the matrix $T$ to $v_k$. For the $j$-th component of the vector $T v_k$, we have:\n$$\n(T v_k)_j = -(v_k)_{j-1} + 2(v_k)_j - (v_k)_{j+1},\n$$\nwhere the homogeneous Dirichlet boundary conditions imply $(v_k)_0 = (v_k)_{n+1} = 0$. Substituting the expression for $(v_k)_j$:\n$$\n(T v_k)_j = -\\sin\\left(\\frac{(j-1)k\\pi}{n+1}\\right) + 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - \\sin\\left(\\frac{(j+1)k\\pi}{n+1}\\right).\n$$\nUsing the trigonometric identity $\\sin(\\alpha-\\beta) + \\sin(\\alpha+\\beta) = 2\\sin(\\alpha)\\cos(\\beta)$, we can simplify the first and third terms:\n$$\n\\sin\\left(\\frac{(j-1)k\\pi}{n+1}\\right) + \\sin\\left(\\frac{(j+1)k\\pi}{n+1}\\right) = 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right)\\cos\\left(\\frac{k\\pi}{n+1}\\right).\n$$\nSubstituting this back, we get:\n$$\n(T v_k)_j = 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right) - 2\\sin\\left(\\frac{jk\\pi}{n+1}\\right)\\cos\\left(\\frac{k\\pi}{n+1}\\right) = \\left(2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right)\\sin\\left(\\frac{jk\\pi}{n+1}\\right).\n$$\nUsing the half-angle identity $2\\sin^2(\\theta) = 1-\\cos(2\\theta)$, this becomes:\n$$\n(T v_k)_j = \\left(4\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right)\\right)(v_k)_j.\n$$\nThus, the eigenvalues of $T$ are $\\mu_k = 4\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right)$ for $k=1, \\dots, n$.\n\nThe eigenvalues $\\lambda_k$ of $A = \\frac{1}{h^2} T = (n+1)^2 T$ are:\n$$\n\\lambda_k = (n+1)^2 \\mu_k = 4(n+1)^2 \\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right), \\quad k=1, \\dots, n.\n$$\nThe matrix $A$ is symmetric and positive definite, so its condition number in the spectral $2$-norm is $\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$. The eigenvalues $\\lambda_k$ are strictly increasing for $k \\in \\{1,\\dots,n\\}$ since the argument $\\frac{k\\pi}{2(n+1)}$ is in the interval $(0, \\frac{\\pi}{2})$. The minimum and maximum eigenvalues correspond to $k=1$ and $k=n$, respectively.\n$$\n\\lambda_{\\min}(A) = \\lambda_1 = 4(n+1)^2 \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\n$$\n\\lambda_{\\max}(A) = \\lambda_n = 4(n+1)^2 \\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right).\n$$\nUsing the identity $\\sin\\left(\\frac{\\pi}{2} - \\theta\\right) = \\cos(\\theta)$, we can rewrite $\\lambda_{\\max}(A)$:\n$$\n\\lambda_{\\max}(A) = 4(n+1)^2 \\sin^2\\left(\\frac{\\pi}{2} - \\frac{\\pi}{2(n+1)}\\right) = 4(n+1)^2 \\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\nThe condition number is then:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\frac{4(n+1)^2 \\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right)}{4(n+1)^2 \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\cot^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\n\n(b) Derivation of the eigenpairs and condition number of $D^{-1}A$.\n\nThe Jacobi preconditioner is $D = \\mathrm{diag}(A)$. From the definition of $A = \\frac{1}{h^2}\\mathrm{tridiag}(-1, 2, -1)$, all diagonal entries are identical: $A_{ii} = \\frac{2}{h^2}$ for all $i=1, \\dots, n$. Thus, $D$ is a scalar multiple of the identity matrix:\n$$\nD = \\frac{2}{h^2}I,\n$$\nwhere $I$ is the $n \\times n$ identity matrix. Its inverse is $D^{-1} = \\frac{h^2}{2}I$. The preconditioned matrix is:\n$$\nD^{-1}A = \\left(\\frac{h^2}{2}I\\right)\\left(\\frac{1}{h^2}T\\right) = \\frac{1}{2}T.\n$$\nThe eigenpairs $(\\gamma_k, v_k)$ of $D^{-1}A$ can be found directly from the eigenpairs of $T$. The eigenvectors $v_k$ are the same, and the eigenvalues $\\gamma_k$ are half of the eigenvalues of $T$:\n$$\n\\gamma_k = \\frac{1}{2}\\mu_k = 2\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right), \\quad k=1, \\dots, n.\n$$\nThe matrix $D^{-1}A$ is symmetric (since $D$ is a multiple of $I$) and has positive eigenvalues, so its condition number is the ratio of its largest to smallest eigenvalue.\n$$\n\\gamma_{\\min} = \\gamma_1 = 2\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\n$$\n\\gamma_{\\max} = \\gamma_n = 2\\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right) = 2\\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\nThe condition number of the preconditioned matrix is:\n$$\n\\kappa_2(D^{-1}A) = \\frac{\\gamma_{\\max}}{\\gamma_{\\min}} = \\frac{2\\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right)}{2\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\cot^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\nThis demonstrates that for this specific matrix $A$ with a constant diagonal, Jacobi preconditioning does not change the condition number, i.e., $\\kappa_2(D^{-1}A) = \\kappa_2(A)$.\n\n(c) Asymptotic behavior and the limit $L$.\n\nTo find the asymptotic behavior as $n \\to \\infty$, we analyze $\\kappa_2(A)$ as $n$ becomes large. Let $x = \\frac{\\pi}{2(n+1)}$. As $n \\to \\infty$, $x \\to 0$. We use the Taylor series expansions for sine and cosine around $0$: $\\sin(x) \\approx x$ and $\\cos(x) \\approx 1$.\n$$\n\\kappa_2(A) = \\cot^2(x) = \\frac{\\cos^2(x)}{\\sin^2(x)} \\approx \\frac{1^2}{x^2} = \\frac{1}{\\left(\\frac{\\pi}{2(n+1)}\\right)^2} = \\frac{4(n+1)^2}{\\pi^2}.\n$$\nThus, the asymptotic behavior of $\\kappa_2(A)$ is $O(n^2)$. Since $\\kappa_2(D^{-1}A) = \\kappa_2(A)$, its asymptotic behavior is also $O(n^2)$.\n\nThe limit $L$ is:\n$$\nL = \\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1}A)}{\\kappa_{2}(A)}.\n$$\nSince $\\kappa_{2}(D^{-1}A) = \\kappa_{2}(A)$ for all $n \\ge 1$, the ratio is always $1$.\n$$\nL = \\lim_{n \\to \\infty} 1 = 1.\n$$\n\n(d) Interpretation of the impact on CG iteration counts.\n\nThe convergence rate of the Conjugate Gradient (CG) method for solving a linear system with a symmetric positive definite matrix $M$ is governed by its condition number $\\kappa_2(M)$. The number of iterations $k$ required to reduce the $A$-norm of the error by a factor $\\varepsilon$ is approximately proportional to the square root of the condition number:\n$$\nk \\propto \\sqrt{\\kappa_2(M)}.\n$$\nFor the unpreconditioned system, $M=A$. The number of iterations, $k_{\\mathrm{UCG}}$, is:\n$$\nk_{\\mathrm{UCG}} \\propto \\sqrt{\\kappa_2(A)} \\approx \\sqrt{\\frac{4(n+1)^2}{\\pi^2}} = \\frac{2(n+1)}{\\pi} \\sim O(n).\n$$\nFor the Jacobi preconditioned system, the effective matrix for the CG algorithm is $D^{-1/2}AD^{-1/2}$. Since $D=cI$ for a scalar $c = 2/h^2$, this matrix is $D^{-1/2}AD^{-1/2} = (c^{-1/2}I)A(c^{-1/2}I) = c^{-1}A = D^{-1}A$. The condition number is $\\kappa_2(D^{-1}A)$. The number of iterations for preconditioned CG, $k_{\\mathrm{PCG}}$, is:\n$$\nk_{\\mathrm{PCG}} \\propto \\sqrt{\\kappa_2(D^{-1}A)}.\n$$\nSince our analysis showed that $\\kappa_2(D^{-1}A) = \\kappa_2(A)$, it follows that\n$$\nk_{\\mathrm{PCG}} \\propto \\sqrt{\\kappa_2(A)} \\sim O(n).\n$$\nThe practical impact is that for this problem, Jacobi preconditioning offers no improvement in the convergence rate of the CG method. The number of iterations required to achieve a desired accuracy scales as $O(n)$ both with and without preconditioning. The preconditioning step, while simple, adds computational overhead to each iteration without reducing the total number of iterations. This specific case highlights that a general-purpose method like Jacobi preconditioning is not universally effective and fails to provide any benefit for matrices with constant diagonals, where the preconditioner is merely a scaling of the identity matrix.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The theoretical effectiveness of a preconditioner, measured by iteration count, is only half the story in high-performance computing; the cost per iteration is equally critical. This final practice bridges the gap between abstract analysis and computational reality by modeling the performance of Jacobi preconditioning on a GPU. You will quantify how different implementation strategies—such as fused kernels or pre-scaling—impact memory traffic and arithmetic intensity, revealing the trade-offs that guide practical algorithm design. ",
            "id": "3552903",
            "problem": "You are given a sparse matrix-vector multiplication scenario on a Graphics Processing Unit (GPU), modeled at the level of memory movement and floating-point operations, with a focus on the Jacobi (diagonal) preconditioner. The matrix is banded and stored in Compressed Sparse Row (CSR) format. Your task is to compute the arithmetic intensity and memory bandwidth proxies for several implementation variants, and to evaluate data layout strategies that exploit diagonal scaling to reduce cache misses, under a simplified but explicit cache-line model. You must produce a complete, runnable program that performs all calculations for the specified test suite and prints the results in the required format.\n\nFoundational base and modeling assumptions:\n- Sparse matrix-vector multiplication (SpMV) in CSR: each nonzero contributes one multiply and one add, i.e., $2$ floating-point operations (FLOPs).\n- Jacobi (diagonal) preconditioner: the matrix $A$ is split as $A = D + R$ with $D$ diagonal. Application of the left preconditioner is the diagonal scaling $D^{-1}$ applied to a vector. If $D^{-1}$ is precomputed, each scaling is modeled as one multiply per vector element.\n- Arithmetic intensity (AI) is defined as $I = \\dfrac{\\text{FLOPs}}{\\text{bytes\\ moved}}$.\n- Memory model for bytes moved:\n  1. CSR arrays: values array of length $\\text{nnz}$ with element size $s$ bytes; column indices array of length $\\text{nnz}$ with element size $I$ bytes (with $I = 4$); and row pointer array of length $n+1$ with element size $I$ bytes. All are read once per SpMV.\n  2. Vector $x$ accesses for AI calculation: assume no reuse across nonzeros on the GPU, i.e., each nonzero causes an independent read of the corresponding element of $x$, modeled as $s$ bytes per nonzero.\n  3. The output vector write (either intermediate $t$ or final $y$) is modeled as $s n$ bytes written per SpMV.\n  4. For separate Jacobi scaling performed after SpMV, the pass reads the intermediate $t$ ($s n$ bytes), reads the diagonal $D$ ($s n$ bytes), and writes the final vector $y$ ($s n$ bytes). When SpMV and Jacobi scaling are fused within one kernel with $D$ read in-kernel, there is an extra read of $D$ ($s n$ bytes) but no extra write beyond the SpMV write. When the matrix is pre-scaled offline (store $A' = D^{-1} A$), there is no additional per-iteration cost beyond SpMV.\n- Cache-line model for cache-miss proxy:\n  1. Cache line size is $L$ bytes. Any contiguous read or write of $B$ bytes touches $\\lceil B/L \\rceil$ cache lines.\n  2. For vector $x$ in SpMV, assume a warp of $T$ consecutive threads each processes one consecutive row. Each row of the banded matrix accesses a contiguous window of $b$ entries of $x$; the union of $x$ indices accessed by the warp is taken as a contiguous block of length $b + (T-1)$ elements. The number of cache lines touched for $x$ by one warp is therefore $\\left\\lceil \\dfrac{(b + T - 1)\\, s}{L} \\right\\rceil$. With $\\lceil n/T \\rceil$ warps, the total cache lines touched for $x$ is $\\left\\lceil \\dfrac{n}{T} \\right\\rceil \\cdot \\left\\lceil \\dfrac{(b + T - 1)\\, s}{L} \\right\\rceil$.\n  3. For other arrays ($\\text{val}$, $\\text{col}$, $\\text{rowptr}$, $D$, $t$, $y$), treat the total bytes moved for the pass and convert to cache lines by $\\lceil \\cdot / L \\rceil$ for each array touched.\n\nThe matrix is banded with bandwidth $b$, so that $\\text{nnz} = n \\cdot b$ (ignore boundary effects for simplicity).\n\nImplementation variants per iteration to evaluate:\n- Baseline SpMV: compute $y \\leftarrow A x$.\n  - FLOPs: $2\\, \\text{nnz}$.\n  - Bytes moved: read $\\text{val}$ ($s\\, \\text{nnz}$), read $\\text{col}$ ($I\\, \\text{nnz}$), read $x$ ($s\\, \\text{nnz}$), read $\\text{rowptr}$ ($I\\,(n+1)$), write $y$ ($s n$).\n- Separate Jacobi scaling (two-pass): compute $t \\leftarrow A x$, then $y \\leftarrow D^{-1} t$.\n  - FLOPs: $2\\, \\text{nnz} + n$.\n  - Additional bytes beyond baseline SpMV: read $t$ ($s n$), read $D$ ($s n$), write $y$ ($s n$). Total bytes is baseline plus $3 s n$.\n- Fused Jacobi in-kernel (read $D$ in SpMV kernel): compute $y_i \\leftarrow (A_i \\cdot x) \\cdot D^{-1}_{ii}$.\n  - FLOPs: $2\\, \\text{nnz} + n$.\n  - Additional bytes beyond baseline SpMV: read $D$ ($s n$). Total bytes is baseline plus $s n$.\n- Pre-scaled matrix (offline scaling): store $A' \\leftarrow D^{-1} A$ once and use baseline SpMV on $A'$ each iteration.\n  - FLOPs per iteration: $2\\, \\text{nnz}$.\n  - Bytes per iteration: equal to baseline SpMV.\n\nFor each variant and each test case, compute:\n- Arithmetic intensity $I$ as FLOPs divided by total bytes moved.\n- Total cache lines touched as the sum over arrays touched in that variant:\n  - Baseline: $\\left\\lceil \\dfrac{s\\, \\text{nnz}}{L} \\right\\rceil + \\left\\lceil \\dfrac{I\\, \\text{nnz}}{L} \\right\\rceil + \\left\\lceil \\dfrac{I\\,(n+1)}{L} \\right\\rceil + \\left\\lceil \\dfrac{s n}{L} \\right\\rceil + \\left\\lceil \\dfrac{n}{T} \\right\\rceil \\cdot \\left\\lceil \\dfrac{(b + T - 1)\\, s}{L} \\right\\rceil$.\n  - Separate Jacobi scaling: baseline plus $\\left\\lceil \\dfrac{s n}{L} \\right\\rceil$ for $D$, plus $\\left\\lceil \\dfrac{s n}{L} \\right\\rceil$ for reading $t$, plus an extra $\\left\\lceil \\dfrac{s n}{L} \\right\\rceil$ for writing $y$.\n  - Fused Jacobi in-kernel: baseline plus $\\left\\lceil \\dfrac{s n}{L} \\right\\rceil$ for $D$.\n  - Pre-scaled matrix: equal to baseline.\n\nData-layout proposal space to be evaluated quantitatively:\n- Separate-pass diagonal scaling versus fused in-kernel scaling versus pre-scaling the matrix offline, interpreted as distinct data layouts for the diagonal relative to the matrix: external diagonal array read in a separate pass, diagonal read interleaved with row computation, or diagonal absorbed into the matrix values. Under the stated model, you will quantify cache-line reductions attributable to absorbing the diagonal into the matrix ($A' = D^{-1} A$), which removes all diagonal-array traffic per iteration, and quantify the reduction from fusing the diagonal read in-kernel relative to a separate pass.\n\nTest suite:\n- Test $1$: $n = 8192$, $b = 17$, $s = 8$, $I = 4$, $L = 128$, $T = 32$.\n- Test $2$: $n = 2048$, $b = 1$, $s = 8$, $I = 4$, $L = 128$, $T = 32$.\n- Test $3$: $n = 4096$, $b = 64$, $s = 4$, $I = 4$, $L = 128$, $T = 32$.\n\nYour program should perform the following for each test:\n- Compute $\\text{nnz} = n \\cdot b$.\n- Compute total bytes and arithmetic intensities for the four variants: baseline, separate Jacobi, fused Jacobi, pre-scaled matrix.\n- Compute total cache lines touched for the four variants under the cache-line model above.\n\nOutput specification:\n- For each test case, output a list of $12$ floating-point values in the following order:\n  1. $I_{\\text{baseline}}$,\n  2. $I_{\\text{separate}}$,\n  3. $I_{\\text{fused}}$,\n  4. $I_{\\text{prescaled}}$,\n  5. $\\text{bytes}_{\\text{baseline}}$,\n  6. $\\text{bytes}_{\\text{separate}}$,\n  7. $\\text{bytes}_{\\text{fused}}$,\n  8. $\\text{bytes}_{\\text{prescaled}}$,\n  9. $\\text{lines}_{\\text{baseline}}$,\n  10. $\\text{lines}_{\\text{separate}}$,\n  11. $\\text{lines}_{\\text{fused}}$,\n  12. $\\text{lines}_{\\text{prescaled}}$.\n- Your program should produce a single line of output containing the results for the three tests as a comma-separated list enclosed in square brackets, where each element is the list for one test. All numbers must be printed as decimal floating-point values rounded to six digits after the decimal point.\n\nAngle units and physical units do not apply. All quantities are unitless counts or ratios. The final output must strictly follow the exact formatting described above.",
            "solution": "The problem requires the calculation of arithmetic intensity and memory bandwidth proxies for four variants of a sparse matrix-vector multiplication (SpMV) combined with a Jacobi preconditioner. The analysis is based on a well-defined, albeit simplified, model of memory movement and floating-point operations (FLOPs) on a GPU. The problem is scientifically grounded in the principles of high-performance computing and numerical linear algebra, is well-posed with all necessary parameters and formulas provided, and is objective in its formulation. Therefore, the problem is deemed valid and a full solution is presented below.\n\nThe core task is to apply the provided performance models to three test cases. We will first formalize the calculation for each performance metric for the four implementation variants.\n\nLet the given parameters be:\n- $n$: the dimension of the square matrix $A$.\n- $b$: the bandwidth of the matrix.\n- $s$: the size of a floating-point number in bytes (e.g., for `double` or `float`).\n- $I$: the size of an integer index in bytes.\n- $L$: the size of a cache line in bytes.\n- $T$: the number of threads in a warp.\n\nFrom these, we derive the number of non-zero elements, $\\text{nnz}$, as $\\text{nnz} = n \\cdot b$, ignoring boundary effects as specified.\n\nThe four variants to be analyzed are:\n1.  **Baseline SpMV**: $y \\leftarrow A x$.\n2.  **Separate Jacobi scaling**: A two-pass approach where $t \\leftarrow A x$ followed by $y \\leftarrow D^{-1} t$.\n3.  **Fused Jacobi in-kernel**: A single-pass approach where the diagonal scaling is fused into the SpMV kernel, i.e., $y_i \\leftarrow (A_i \\cdot x) \\cdot D^{-1}_{ii}$.\n4.  **Pre-scaled matrix**: The matrix is scaled offline ($A' \\leftarrow D^{-1} A$), and at each iteration, a baseline SpMV is performed: $y \\leftarrow A' x$.\n\nWe compute FLOPs, total bytes moved, arithmetic intensity ($I$), and total cache lines touched for each variant. The function $\\lceil \\cdot \\rceil$ denotes the ceiling operation.\n\n**1. Floating-Point Operations (FLOPs)**\n\n- **Baseline  Pre-scaled**: Each of the $\\text{nnz}$ elements requires one multiplication and one addition, totaling $2$ FLOPs per non-zero.\n  $$F_{\\text{baseline}} = F_{\\text{prescaled}} = 2 \\cdot \\text{nnz}$$\n- **Separate  Fused**: These variants perform the SpMV and then scale the resulting vector by the inverse of the diagonal. This adds $n$ multiplications.\n  $$F_{\\text{separate}} = F_{\\text{fused}} = (2 \\cdot \\text{nnz}) + n$$\n\n**2. Bytes Moved from Main Memory**\n\nThe total bytes moved is the sum of bytes for each array read or written.\n- The SpMV operation reads the CSR arrays (`val`, `col`, `rowptr`) and the input vector `x`, and writes the output vector `y`. According to the problem's model, the bytes moved are:\n  - Read `val`: $s \\cdot \\text{nnz}$\n  - Read `col`: $I \\cdot \\text{nnz}$\n  - Read `x`: $s \\cdot \\text{nnz}$ (no cache reuse assumed at this level)\n  - Read `rowptr`: $I \\cdot (n+1)$\n  - Write `y` (or intermediate `t`): $s \\cdot n$\n- **Baseline  Pre-scaled**: The bytes moved are the sum of the components for a single SpMV pass.\n  $$B_{\\text{baseline}} = B_{\\text{prescaled}} = s \\cdot \\text{nnz} + I \\cdot \\text{nnz} + s \\cdot \\text{nnz} + I(n+1) + s \\cdot n$$\n- **Fused Jacobi**: This variant performs a baseline SpMV but additionally reads the diagonal vector $D$ (or its inverse).\n  $$B_{\\text{fused}} = B_{\\text{baseline}} + s \\cdot n$$\n- **Separate Jacobi**: This two-pass method first performs an SpMV to compute an intermediate vector $t$ (memory traffic equal to $B_{\\text{baseline}}$), and then a second pass reads $t$, reads $D$, and writes the final vector $y$. The model specifies the total bytes as the baseline amount plus the traffic for the second pass, which is given as $3 \\cdot s \\cdot n$.\n  $$B_{\\text{separate}} = B_{\\text{baseline}} + 3 \\cdot s \\cdot n$$\n\n**3. Arithmetic Intensity (AI)**\n\nArithmetic intensity $I$ is the ratio of FLOPs to bytes moved.\n$$I_{\\text{variant}} = \\frac{F_{\\text{variant}}}{B_{\\text{variant}}}$$\n\n**4. Cache Lines Touched**\n\nThis metric estimates memory traffic at the cache-line level.\n- Cache lines for contiguous arrays are calculated as $\\lceil \\text{total\\_bytes} / L \\rceil$.\n- Cache lines for vector `x` access are given by a special model: the total number of warps, $\\lceil n/T \\rceil$, multiplied by the cache lines touched per warp, $\\lceil s(b+T-1)/L \\rceil$.\n- Let's define the components:\n  - $L_{\\text{val}} = \\lceil \\frac{s \\cdot \\text{nnz}}{L} \\rceil$\n  - $L_{\\text{col}} = \\lceil \\frac{I \\cdot \\text{nnz}}{L} \\rceil$\n  - $L_{\\text{rowptr}} = \\lceil \\frac{I(n+1)}{L} \\rceil$\n  - $L_{\\text{y}} = \\lceil \\frac{s \\cdot n}{L} \\rceil$\n  - $L_{\\text{x}} = \\left\\lceil \\frac{n}{T} \\right\\rceil \\cdot \\left\\lceil \\frac{s(b+T-1)}{L} \\right\\rceil$\n\n- **Baseline  Pre-scaled**: The total is the sum of the components for a single pass.\n  $$\\text{lines}_{\\text{baseline}} = \\text{lines}_{\\text{prescaled}} = L_{\\text{val}} + L_{\\text{col}} + L_{\\text{rowptr}} + L_{\\text{y}} + L_{\\text{x}}$$\n- **Fused Jacobi**: Adds the cache lines for reading the diagonal vector $D$.\n  $$\\text{lines}_{\\text{fused}} = \\text{lines}_{\\text{baseline}} + \\left\\lceil \\frac{s \\cdot n}{L} \\right\\rceil = \\text{lines}_{\\text{baseline}} + L_{\\text{y}}$$\n- **Separate Jacobi**: The total cache lines are modeled as the baseline amount plus the lines for the second pass (reading $t$, reading $D$, and writing $y$).\n  $$\\text{lines}_{\\text{separate}} = \\text{lines}_{\\text{baseline}} + \\left\\lceil \\frac{s \\cdot n}{L} \\right\\rceil (\\text{read } t) + \\left\\lceil \\frac{s \\cdot n}{L} \\right\\rceil (\\text{read } D) + \\left\\lceil \\frac{s \\cdot n}{L} \\right\\rceil (\\text{write } y)$$\n  $$\\text{lines}_{\\text{separate}} = \\text{lines}_{\\text{baseline}} + 3 \\cdot L_{\\text{y}}$$\n\nThese formulas will be implemented for each test case provided in the problem statement. The results are then formatted as specified. The data-layout proposal evaluation is a qualitative interpretation of these quantitative results, where lower cache line counts for fused and pre-scaled variants indicate a more efficient data layout by reducing memory traffic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes arithmetic intensity and cache-line proxies for SpMV with Jacobi\n    preconditioning variants based on a simplified GPU performance model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, b, s, I, L, T)\n        (8192, 17, 8, 4, 128, 32),\n        (2048, 1, 8, 4, 128, 32),\n        (4096, 64, 4, 4, 128, 32),\n    ]\n\n    all_results_list = []\n    for case in test_cases:\n        n, b, s, I, L, T = case\n\n        # Compute number of non-zeros\n        nnz = n * b\n\n        # 1. Floating-Point Operations (FLOPs)\n        flops_base = 2 * nnz\n        flops_prescaled = flops_base\n        flops_fused = 2 * nnz + n\n        flops_separate = flops_fused\n\n        # 2. Bytes Moved\n        bytes_val = s * nnz\n        bytes_col = I * nnz\n        bytes_x_read = s * nnz  # Per-nonzero access model\n        bytes_rowptr = I * (n + 1)\n        bytes_output_write = s * n\n\n        bytes_base = bytes_val + bytes_col + bytes_x_read + bytes_rowptr + bytes_output_write\n        bytes_prescaled = bytes_base\n        \n        # Fused variant adds a read of the diagonal vector D\n        bytes_fused = bytes_base + s * n\n        \n        # Separate variant adds reads of t and D, and a write of y\n        bytes_separate = bytes_base + 3 * s * n\n\n        # 3. Arithmetic Intensity (AI)\n        I_base = flops_base / bytes_base\n        I_prescaled = flops_prescaled / bytes_prescaled\n        I_fused = flops_fused / bytes_fused\n        I_separate = flops_separate / bytes_separate\n\n        # 4. Cache Lines Touched\n        lines_val = np.ceil(bytes_val / L)\n        lines_col = np.ceil(bytes_col / L)\n        lines_rowptr = np.ceil(bytes_rowptr / L)\n        lines_output_write = np.ceil(bytes_output_write / L)\n        \n        # Special model for vector x access\n        num_warps = np.ceil(n / T)\n        lines_per_warp_for_x = np.ceil((s * (b + T - 1)) / L)\n        lines_x = num_warps * lines_per_warp_for_x\n        \n        lines_base = lines_val + lines_col + lines_rowptr + lines_output_write + lines_x\n        lines_prescaled = lines_base\n\n        # Fused variant adds lines for reading D\n        lines_D_read = np.ceil((s * n) / L)\n        lines_fused = lines_base + lines_D_read\n        \n        # Separate variant adds lines for reading t, D and an extra write of y\n        lines_separate = lines_base + 3 * np.ceil((s * n) / L)\n        \n        # Store results for the current test case in the specified order\n        # The problem asks for separate, then fused, then prescaled for intensity\n        case_results = [\n            I_base, I_separate, I_fused, I_prescaled,\n            float(bytes_base), float(bytes_separate), float(bytes_fused), float(bytes_prescaled),\n            float(lines_base), float(lines_separate), float(lines_fused), float(lines_prescaled)\n        ]\n        \n        all_results_list.append(case_results)\n\n    # Format the final output string as per specifications\n    formatted_tests = []\n    for test_result in all_results_list:\n        formatted_values = [f\"{val:.6f}\" for val in test_result]\n        formatted_tests.append(f\"[{','.join(formatted_values)}]\")\n    \n    final_output = f\"[{','.join(formatted_tests)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}