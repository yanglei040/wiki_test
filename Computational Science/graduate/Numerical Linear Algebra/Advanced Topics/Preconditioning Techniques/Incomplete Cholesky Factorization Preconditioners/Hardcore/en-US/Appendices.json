{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a direct calculation of an incomplete Cholesky factorization. This exercise is designed to give you a feel for the algorithm's mechanics by applying the IC(0) procedure step-by-step to a small, well-structured matrix. By working through the factorization process by hand, you will gain a concrete understanding of how fill-in is prevented and how the approximate factor $L$ is constructed .",
            "id": "3550237",
            "problem": "Consider the symmetric positive definite (SPD) sparse matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n4  -1  -1  0 \\\\\n-1  4  -1  -1 \\\\\n-1  -1  4  -1 \\\\\n0  -1  -1  4\n\\end{pmatrix}.\n$$\nLet the sparsity pattern for the incomplete factor be the lower-triangular sparsity of $A$, that is, the set\n$$\n\\mathcal{S} \\;=\\; \\{(i,j)\\,:\\, i \\ge j,\\; A_{ij} \\neq 0\\}.\n$$\nCompute the incomplete Cholesky (IC) factorization with zero fill, also known as incomplete Cholesky with zero fill (IC(0)), of $A$ in the natural ordering, producing a lower-triangular matrix $L$ constrained to the pattern $\\mathcal{S}$ and no diagonal modification. Form the preconditioner $M = L L^{\\top}$. Using only the foundational definition of Cholesky factorization restricted to the prescribed sparsity pattern (i.e., discarding any contribution that would create fill outside $\\mathcal{S}$), determine the determinant of the preconditioner $M$. Provide your final answer as an exact value (no rounding). The final answer must be a single real number.",
            "solution": "The user has provided a valid problem from the field of numerical linear algebra. The task is to compute the determinant of a preconditioner $M$ derived from an incomplete Cholesky factorization of a given symmetric positive definite (SPD) matrix $A$.\n\nThe given matrix is:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n4  -1  -1  0 \\\\\n-1  4  -1  -1 \\\\\n-1  -1  4  -1 \\\\\n0  -1  -1  4\n\\end{pmatrix}\n$$\nThe problem specifies the incomplete Cholesky factorization with zero fill (IC(0)). This means the lower-triangular factor, denoted by $L$, must conform to the sparsity pattern of the lower-triangular part of $A$. The sparsity pattern $\\mathcal{S}$ is given as $\\mathcal{S} = \\{(i,j) : i \\ge j, A_{ij} \\neq 0\\}$. Examining $A$, the non-zero entries $A_{ij}$ for $i \\ge j$ are at positions $(1,1)$, $(2,1)$, $(2,2)$, $(3,1)$, $(3,2)$, $(3,3)$, $(4,2)$, $(4,3)$, and $(4,4)$. Thus, the matrix $L$ must have the following structure:\n$$\nL = \\begin{pmatrix}\nl_{11}  0  0  0 \\\\\nl_{21}  l_{22}  0  0 \\\\\nl_{31}  l_{32}  l_{33}  0 \\\\\n0  l_{42}  l_{43}  l_{44}\n\\end{pmatrix}\n$$\nThe preconditioner is $M = L L^{\\top}$. We are asked to find the determinant of $M$.\nThe determinant of a product of matrices is the product of their determinants, so $\\det(M) = \\det(L L^{\\top}) = \\det(L) \\det(L^{\\top})$. Since the determinant of a matrix is equal to the determinant of its transpose, $\\det(L) = \\det(L^{\\top})$. Therefore, $\\det(M) = (\\det(L))^2$.\nFor a lower-triangular matrix such as $L$, the determinant is the product of its diagonal entries: $\\det(L) = \\prod_{i=1}^{4} l_{ii}$.\nOur primary task is to compute the diagonal entries of $L$ using the IC(0) algorithm. The algorithm follows the standard Cholesky factorization procedure, but modifications to entries outside the prescribed sparsity pattern $\\mathcal{S}$ are discarded.\n\nLet the entries of $A$ be $a_{ij}$ and the computed entries of $L$ be $l_{ij}$. The standard Cholesky factorization formulas are:\n$$\nl_{ii} = \\sqrt{a_{ii} - \\sum_{k=1}^{i-1} l_{ik}^2}\n$$\n$$\nl_{ji} = \\frac{1}{l_{ii}} \\left( a_{ji} - \\sum_{k=1}^{i-1} l_{jk}l_{ik} \\right) \\quad \\text{for } j  i\n$$\nWe apply these formulas column by column for $i=1, 2, 3, 4$.\n\n**Column 1 ($i=1$):**\nThere are no preceding columns, so the sums are empty.\n$l_{11} = \\sqrt{a_{11}} = \\sqrt{4} = 2$.\nFor $j1$:\n$l_{21} = \\frac{a_{21}}{l_{11}} = \\frac{-1}{2}$. This position $(2,1)$ is in $\\mathcal{S}$.\n$l_{31} = \\frac{a_{31}}{l_{11}} = \\frac{-1}{2}$. This position $(3,1)$ is in $\\mathcal{S}$.\n$l_{41} = 0$, since the position $(4,1)$ is not in $\\mathcal{S}$ ($A_{41}=0$).\n\n**Column 2 ($i=2$):**\nFirst, we find the diagonal entry $l_{22}$.\n$l_{22} = \\sqrt{a_{22} - \\sum_{k=1}^{1} l_{2k}^2} = \\sqrt{a_{22} - l_{21}^2}$.\n$l_{22} = \\sqrt{4 - \\left(\\frac{-1}{2}\\right)^2} = \\sqrt{4 - \\frac{1}{4}} = \\sqrt{\\frac{15}{4}} = \\frac{\\sqrt{15}}{2}$.\nNext, we find the off-diagonal entries $l_{j2}$ for $j2$ that are in $\\mathcal{S}$.\nFor $j=3$: $l_{32} = \\frac{1}{l_{22}}(a_{32} - l_{31}l_{21}) = \\frac{1}{\\frac{\\sqrt{15}}{2}} \\left(-1 - \\left(\\frac{-1}{2}\\right)\\left(\\frac{-1}{2}\\right)\\right) = \\frac{2}{\\sqrt{15}} \\left(-1 - \\frac{1}{4}\\right) = \\frac{2}{\\sqrt{15}} \\left(\\frac{-5}{4}\\right) = \\frac{-5}{2\\sqrt{15}}$.\nFor $j=4$: $l_{42} = \\frac{1}{l_{22}}(a_{42} - l_{41}l_{21}) = \\frac{1}{\\frac{\\sqrt{15}}{2}}(-1 - 0 \\cdot l_{21}) = \\frac{-2}{\\sqrt{15}}$.\n\n**Column 3 ($i=3$):**\nWe find the diagonal entry $l_{33}$.\n$l_{33} = \\sqrt{a_{33} - \\sum_{k=1}^{2} l_{3k}^2} = \\sqrt{a_{33} - (l_{31}^2 + l_{32}^2)}$.\nWe have $l_{31}^2 = (-\\frac{1}{2})^2 = \\frac{1}{4}$ and $l_{32}^2 = (\\frac{-5}{2\\sqrt{15}})^2 = \\frac{25}{4 \\cdot 15} = \\frac{25}{60} = \\frac{5}{12}$.\n$l_{33} = \\sqrt{4 - \\left(\\frac{1}{4} + \\frac{5}{12}\\right)} = \\sqrt{4 - \\left(\\frac{3}{12} + \\frac{5}{12}\\right)} = \\sqrt{4 - \\frac{8}{12}} = \\sqrt{4 - \\frac{2}{3}} = \\sqrt{\\frac{10}{3}}$.\nNext, we find $l_{43}$:\n$l_{43} = \\frac{1}{l_{33}}(a_{43} - (l_{41}l_{31} + l_{42}l_{32}))$. Since $l_{41}=0$, this simplifies.\n$l_{42}l_{32} = \\left(\\frac{-2}{\\sqrt{15}}\\right)\\left(\\frac{-5}{2\\sqrt{15}}\\right) = \\frac{10}{2 \\cdot 15} = \\frac{1}{3}$.\n$l_{43} = \\frac{1}{\\sqrt{\\frac{10}{3}}}\\left(-1 - \\frac{1}{3}\\right) = \\sqrt{\\frac{3}{10}}\\left(\\frac{-4}{3}\\right) = \\frac{-4\\sqrt{3}}{3\\sqrt{10}}$.\n\n**Column 4 ($i=4$):**\nFinally, we find the diagonal entry $l_{44}$.\n$l_{44} = \\sqrt{a_{44} - \\sum_{k=1}^{3} l_{4k}^2} = \\sqrt{a_{44} - (l_{41}^2 + l_{42}^2 + l_{43}^2)}$.\nWe have $l_{41}=0$.\n$l_{42}^2 = (\\frac{-2}{\\sqrt{15}})^2 = \\frac{4}{15}$.\n$l_{43}^2 = \\left(\\frac{-4\\sqrt{3}}{3\\sqrt{10}}\\right)^2 = \\frac{16 \\cdot 3}{9 \\cdot 10} = \\frac{48}{90} = \\frac{8}{15}$.\n$l_{44} = \\sqrt{4 - \\left(0 + \\frac{4}{15} + \\frac{8}{15}\\right)} = \\sqrt{4 - \\frac{12}{15}} = \\sqrt{4 - \\frac{4}{5}} = \\sqrt{\\frac{16}{5}} = \\frac{4}{\\sqrt{5}}$.\n\nNow we have all the diagonal entries of $L$:\n$l_{11} = 2$\n$l_{22} = \\frac{\\sqrt{15}}{2}$\n$l_{33} = \\sqrt{\\frac{10}{3}}$\n$l_{44} = \\frac{4}{\\sqrt{5}}$\n\nWe can now compute the determinant of $L$:\n$\\det(L) = l_{11} \\cdot l_{22} \\cdot l_{33} \\cdot l_{44} = 2 \\cdot \\frac{\\sqrt{15}}{2} \\cdot \\sqrt{\\frac{10}{3}} \\cdot \\frac{4}{\\sqrt{5}}$.\n$\\det(L) = \\sqrt{15} \\cdot \\frac{\\sqrt{10}}{\\sqrt{3}} \\cdot \\frac{4}{\\sqrt{5}} = \\sqrt{3 \\cdot 5} \\cdot \\frac{\\sqrt{2 \\cdot 5}}{\\sqrt{3}} \\cdot \\frac{4}{\\sqrt{5}}$.\n$\\det(L) = \\frac{(\\sqrt{3}\\sqrt{5}) \\cdot (\\sqrt{2}\\sqrt{5}) \\cdot 4}{\\sqrt{3}\\sqrt{5}} = 4\\sqrt{2}\\sqrt{5} = 4\\sqrt{10}$.\n\nFinally, the determinant of the preconditioner $M$ is:\n$\\det(M) = (\\det(L))^2 = (4\\sqrt{10})^2 = 16 \\cdot 10 = 160$.\n\nIt is noteworthy that for the given matrix $A$ and the natural ordering of variables, the graph of the matrix is chordal and the ordering is a perfect elimination ordering. This implies that the exact Cholesky factorization of $A$ exhibits no fill-in; its sparsity pattern is identical to that of $A$. Consequently, the incomplete Cholesky factorization with zero fill (IC(0)) is identical to the exact Cholesky factorization. This means $M=A$, and thus $\\det(M)$ must equal $\\det(A)$. A direct calculation of $\\det(A)$ confirms that $\\det(A)=160$, which validates our result.",
            "answer": "$$\n\\boxed{160}\n$$"
        },
        {
            "introduction": "While the basic IC(0) algorithm is straightforward, its practical application can encounter a critical issue: breakdown. This occurs when a non-positive pivot is generated during factorization, which is possible even for some SPD matrices. This exercise  moves from theory to practice by asking you to implement a robust version of IC(0) that incorporates a diagonal shift $\\alpha$ to prevent breakdown, a common stabilization technique. You will develop a program to find the minimal required shift, a task that mirrors real-world challenges in creating reliable preconditioners.",
            "id": "3550258",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real symmetric matrix with a sparsity pattern given by the positions of its nonzero entries. Assume $A$ is Symmetric Positive Definite (SPD). Consider the Incomplete Cholesky factorization with zero fill (IC(0)), defined as computing a lower triangular matrix $L$ with the following constraints: \n- $L$ has the same lower-triangular sparsity pattern as $A$ (i.e., $L_{ij} = 0$ for $i  j$ whenever $A_{ij} = 0$), \n- The factorization is performed column by column and uses only the pre-existing nonzeros in $A$ without introducing any fill-in.\n\nDefine the diagonal shift $\\alpha \\ge 0$ and the pivot safeguard tolerance $\\tau  0$. IC(0) of $A + \\alpha I$ proceeds as follows. For $k = 1, 2, \\dots, n$,\n- Compute the squared pivot \n$$\ns_k(\\alpha) \\equiv A_{kk} + \\alpha - \\sum_{j  k,\\; A_{kj} \\ne 0} L_{kj}^2.\n$$\nIf $s_k(\\alpha) \\le \\tau$, declare breakdown at step $k$. Otherwise set $L_{kk} = \\sqrt{s_k(\\alpha)}$. \n- For each $i  k$ with $A_{ik} \\ne 0$, compute\n$$\nL_{ik} = \\frac{A_{ik} - \\sum_{j  k,\\; A_{ij} \\ne 0,\\; A_{kj} \\ne 0} L_{ij} L_{kj}}{L_{kk}}.\n$$\nThis algorithm uses the original sparsity pattern of $A$ both to decide which updates to apply and which entries of $L$ to compute.\n\nA breakdown at any step $k$ is defined to occur when $s_k(\\alpha) \\le \\tau$. The diagonal shift $\\alpha$ is said to prevent breakdown if IC(0) completes for all $k = 1, \\dots, n$ with $s_k(\\alpha) > \\tau$. Your task is to implement this simulation and then, for each given test matrix, compute the minimal nonnegative $\\alpha$ such that IC(0) of $A + \\alpha I$ completes without breakdown (i.e., all $s_k(\\alpha) > \\tau$). The minimal $\\alpha$ should be computed numerically via a safeguarded bisection search with an absolute tolerance of $10^{-12}$ on the interval endpoints.\n\nStart from first principles: the definition of Symmetric Positive Definite (SPD), the exact Cholesky factorization $A = LL^\\top$ for SPD matrices, the IC(0) algorithm as a projection of the exact factorization onto the given sparsity pattern, and the effect of adding a diagonal shift $\\alpha I$ to $A$ on the computed pivots $s_k(\\alpha)$. The program must use an implementation of IC(0) consistent with the above definitions, and must detect breakdown exactly as specified.\n\nUse the following test suite with $n = 6$. Let $T \\in \\mathbb{R}^{6 \\times 6}$ be the tridiagonal SPD matrix defined by $T_{ii} = 2$ for $i = 1,\\dots,6$ and $T_{i,i+1} = T_{i+1,i} = -1$ for $i = 1,\\dots,5$, and all other entries zero. Define three sparse SPD matrices by scaling $T$:\n- Test 1: $A^{(1)} = \\beta_1 T$ with $\\beta_1 = 10^{-10}$,\n- Test 2: $A^{(2)} = \\beta_2 T$ with $\\beta_2 = 1$,\n- Test 3: $A^{(3)} = \\beta_3 T$ with $\\beta_3 = 4.9 \\times 10^{-10}$.\n\nUse the fixed pivot tolerance $\\tau = 10^{-9}$ for all tests. For each $A^{(m)}$ ($m = 1,2,3$), compute the minimal nonnegative shift $\\alpha^{(m)}$ such that IC(0) of $A^{(m)} + \\alpha^{(m)} I$ completes without breakdown at any step.\n\nYour program should produce a single line of output containing the three results $\\alpha^{(1)}, \\alpha^{(2)}, \\alpha^{(3)}$ as a comma-separated list enclosed in square brackets, in the order of the tests and expressed as floating-point numbers, for example, \"[a1,a2,a3]\".",
            "solution": "The problem asks for the computation of a minimal non-negative diagonal shift $\\alpha$ required to prevent breakdown in the Incomplete Cholesky factorization with zero fill-in, IC(0), for a set of given matrices. The breakdown condition is tied to a pivot safeguard tolerance $\\tau$. The solution requires implementing the specified IC(0) algorithm and using a bisection search to find the minimal $\\alpha$.\n\nFirst, we establish the theoretical foundation. A matrix $A \\in \\mathbb{R}^{n \\times n}$ is Symmetric Positive Definite (SPD) if it is symmetric ($A = A^\\top$) and satisfies $x^\\top A x  0$ for all non-zero vectors $x \\in \\mathbb{R}^n$. A cornerstone property of SPD matrices is that they admit a unique Cholesky factorization $A = LL^\\top$, where $L$ is a lower triangular matrix with strictly positive diagonal entries.\n\nThe standard Cholesky factorization algorithm computes the entries of $L$ column by column. For $k=1, \\dots, n$:\n$$L_{kk} = \\sqrt{A_{kk} - \\sum_{j=1}^{k-1} L_{kj}^2}$$\nAnd for $i  k$:\n$$L_{ik} = \\frac{1}{L_{kk}}\\left(A_{ik} - \\sum_{j=1}^{k-1} L_{ij}L_{kj}\\right)$$\nFor dense matrices, this process can introduce non-zero entries in $L$ at positions where $A$ originally had zeros. This phenomenon is known as \"fill-in\".\n\nThe Incomplete Cholesky factorization with zero fill-in, IC(0), is a commonly used preconditioner that avoids fill-in by restricting the factorization to the original sparsity pattern of $A$. The matrix $L$ is constrained to have non-zero entries only where $A$ does. This is equivalent to modifying the sums in the Cholesky algorithm to run only over indices corresponding to non-zero entries. The algorithm provided in the problem statement formalizes this:\n1.  The squared pivot $s_k(\\alpha)$ is computed as $A_{kk} + \\alpha - \\sum_{j  k, A_{kj} \\ne 0} L_{kj}^2$.\n2.  The off-diagonal entry $L_{ik}$ is computed as $\\frac{1}{L_{kk}}(A_{ik} - \\sum_{j  k, A_{ij} \\ne 0, A_{kj} \\ne 0} L_{ij}L_{kj})$.\n\nIf $A$ is not \"sufficiently\" positive definite, the argument of the square root, $s_k(\\alpha)$, can become non-positive, causing the factorization to fail. To prevent this, a diagonal shift $\\alpha I$ is added to $A$, and a safeguard tolerance $\\tau  0$ is used. A breakdown is declared if $s_k(\\alpha) \\le \\tau$ for any step $k$.\n\nThe test matrices are of the form $A = \\beta T$, where $T$ is the $n \\times n$ tridiagonal matrix with $T_{ii}=2$ and $T_{i,i+1} = T_{i+1,i} = -1$. For such a matrix, the IC(0) algorithm simplifies considerably.\n-   The sparsity pattern is $A_{ij} \\ne 0$ only if $|i-j| \\le 1$.\n-   The sum for the squared pivot $s_k(\\alpha)$ only has one term corresponding to $j=k-1$:\n    $s_k(\\alpha) = (A_{kk} + \\alpha) - L_{k,k-1}^2$ for $k  1$. For $k=1$, the sum is empty, so $s_1(\\alpha) = A_{11} + \\alpha$.\n-   For the off-diagonal entries $L_{ik}$, the sum $\\sum_{j  k, A_{ij} \\ne 0, A_{kj} \\ne 0} L_{ij}L_{kj}$ is always zero. This is because for $i=k+1$, $A_{k+1,j} \\ne 0$ only for $j=k, k+2$. And $A_{k,j} \\ne 0$ only for $j=k-1, k+1$. For $jk$, the only non-zero $A_{k,j}$ is at $j=k-1$, but $A_{k+1,k-1}=0$. Thus, there is no $jk$ for which both conditions hold.\n-   Therefore, $L_{ik} = A_{ik} / L_{kk}$. Since $A$ is tridiagonal, this applies only to $L_{k+1, k}$.\n\nThis leads to a simple recurrence for the squared pivots $s_k(\\alpha)$, which we can denote $d_k(\\alpha)$:\n1.  $d_1(\\alpha) = A_{11} + \\alpha = 2\\beta + \\alpha$.\n2.  $L_{k,k-1} = A_{k,k-1} / L_{k-1,k-1} = -\\beta / \\sqrt{d_{k-1}(\\alpha)}$.\n3.  $d_k(\\alpha) = (A_{kk} + \\alpha) - L_{k,k-1}^2 = (2\\beta + \\alpha) - (-\\beta)^2 / d_{k-1}(\\alpha)$.\n\nThis yields the recurrence relation for $k=2, \\dots, n$:\n$$d_k(\\alpha) = (2\\beta + \\alpha) - \\frac{\\beta^2}{d_{k-1}(\\alpha)}$$\nThe task is to find the minimum non-negative $\\alpha$ such that $d_k(\\alpha) > \\tau$ for all $k=1, \\dots, n$.\n\nFor any fixed $k$, $d_k(\\alpha)$ is a monotonically increasing function of $\\alpha$. This can be proven by induction. The base case $d_1(\\alpha) = 2\\beta+\\alpha$ is clearly increasing. Assuming $d_{k-1}(\\alpha)$ is increasing, then $1/d_{k-1}(\\alpha)$ is decreasing, so $-\\beta^2/d_{k-1}(\\alpha)$ is increasing. Thus, $d_k(\\alpha) = (2\\beta+\\alpha) - \\beta^2/d_{k-1}(\\alpha)$ is a sum of increasing functions and is therefore increasing.\n\nThis monotonicity allows us to use a bisection search to find the minimal $\\alpha$. We define a boolean function, `check(alpha)`, which returns `True` if the IC(0) factorization for the given `alpha` completes without breakdown (i.e., $d_k(\\alpha) > \\tau$ for all $k$), and `False` otherwise.\nThe bisection algorithm proceeds as follows:\n1.  First, check if `check(0)` is `True`. If so, the minimal non-negative shift is $\\alpha=0$.\n2.  If `check(0)` is `False`, we need to find a suitable search interval $[\\alpha_{low}, \\alpha_{high}]$ such that `check`($\\alpha_{low}$) is `False` and `check`($\\alpha_{high}$) is `True`. We can set $\\alpha_{low}=0$. A sufficiently large $\\alpha_{high}$ can be found (e.g., starting with $\\alpha_{high}=\\tau$ and doubling it until `check`($\\alpha_{high}$) passes), guaranteeing a solution exists in the interval.\n3.  The algorithm iteratively bisects the interval: $\\alpha_{mid} = (\\alpha_{low} + \\alpha_{high})/2$. If `check`($\\alpha_{mid}$) is `True`, it means $\\alpha_{mid}$ is a valid shift, so we may be able to find a smaller one; we set $\\alpha_{high} = \\alpha_{mid}$. If `check`($\\alpha_{mid}$) is `False`, the shift is too small, and we must increase it; we set $\\alpha_{low} = \\alpha_{mid}$.\n4.  This process continues until the interval width $\\alpha_{high} - \\alpha_{low}$ is smaller than the specified absolute tolerance of $10^{-12}$. The final result is $\\alpha_{high}$, which is the smallest value found within the given tolerance that prevents breakdown.\n\nThe implementation will consist of a function for the `check` logic using the derived recurrence, and another function for the bisection search. This procedure is applied to each of the three test cases.",
            "answer": "```python\nimport numpy as np\nimport sys\n\n# Set a higher recursion limit for the check function, although with n=6 it's not strictly necessary.\n# This is a safeguard if the problem were scaled to larger n.\nsys.setrecursionlimit(2000)\n\ndef check(alpha: float, beta: float, tau: float, n: int) - bool:\n    \"\"\"\n    Performs the IC(0) factorization for a given shift alpha and checks for breakdown.\n    For the specific tridiagonal matrix, this simplifies to checking a recurrence relation\n    for the squared pivots d_k  tau for all k.\n\n    Args:\n        alpha: The diagonal shift value.\n        beta: The scaling factor for the matrix T.\n        tau: The pivot safeguard tolerance.\n        n: The dimension of the matrix.\n\n    Returns:\n        True if the factorization completes without breakdown, False otherwise.\n    \"\"\"\n    if n == 0:\n        return True\n\n    # Initial squared pivot d_1\n    d = 2 * beta + alpha\n    if d = tau:\n        return False\n\n    # Recursively check the rest of the pivots\n    for _ in range(2, n + 1):\n        # The recurrence relation for subsequent pivots\n        d = (2 * beta + alpha) - (beta**2) / d\n        if d = tau:\n            return False\n            \n    return True\n\ndef find_minimal_alpha(beta: float, tau: float, n: int, abs_tol: float) - float:\n    \"\"\"\n    Finds the minimal non-negative alpha using a bisection search such that\n    IC(0) of (beta*T + alpha*I) completes without breakdown.\n\n    Args:\n        beta: The scaling factor for the matrix T.\n        tau: The pivot safeguard tolerance.\n        n: The dimension of the matrix.\n        abs_tol: The absolute tolerance for the bisection search.\n\n    Returns:\n        The minimal non-negative alpha.\n    \"\"\"\n    # If alpha = 0 works, it's the minimal non-negative value.\n    if check(0.0, beta, tau, n):\n        return 0.0\n\n    # Establish search interval [low, high] where check(low) is False and check(high) is True.\n    low = 0.0\n    \n    # Find an initial upper bound. A reasonable guess is max(1.0, tau)\n    # to handle different scales of beta and tau.\n    high = max(1.0, tau)\n    while not check(high, beta, tau, n):\n        high *= 2.0\n\n    # Bisection search\n    while (high - low)  abs_tol:\n        mid = (low + high) / 2.0\n        if mid == low or mid == high: # Reached precision limit\n            break\n        if check(mid, beta, tau, n):\n            # mid is a valid shift, try to find a smaller one\n            high = mid\n        else:\n            # mid is too small, need a larger shift\n            low = mid\n\n    # high is the minimal alpha within the tolerance that passes the check.\n    return high\n\ndef solve():\n    \"\"\"\n    Main solver function to run the test cases and print the results.\n    \"\"\"\n    # Problem parameters\n    n = 6\n    tau = 1.0e-9\n    abs_tol = 1.0e-12\n    test_cases = [\n        1.0e-10,          # Test 1: beta_1\n        1.0,              # Test 2: beta_2\n        4.9e-10,          # Test 3: beta_3\n    ]\n\n    results = []\n    for beta in test_cases:\n        min_alpha = find_minimal_alpha(beta, tau, n, abs_tol)\n        results.append(min_alpha)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver\nsolve()\n```"
        },
        {
            "introduction": "Now we arrive at the ultimate goal of developing preconditioners: accelerating iterative solvers for large-scale problems. This comprehensive exercise  puts all the pieces together by having you solve a linear system arising from the discretization of a partial differential equation. You will construct the classic 2D 5-point Laplacian matrix, compute its IC(0) preconditioner, and witness firsthand the dramatic reduction in iteration count when using the Preconditioned Conjugate Gradient (PCG) method compared to the standard CG method.",
            "id": "3550244",
            "problem": "You are to design and implement a complete, runnable program that constructs a symmetric positive definite two-dimensional ($2$D) $5$-point Laplacian matrix $A \\in \\mathbb{R}^{N \\times N}$ on an $n \\times n$ grid with homogeneous Dirichlet boundary conditions, computes the Incomplete Cholesky (IC) factorization with zero fill-in (IC$(0)$) of $A$, and uses it as a left preconditioner $M = L L^{\\top}$ in the Preconditioned Conjugate Gradient (PCG) method to solve the linear system $A x = b$ with the initial guess $x_0 = 0$. You must report, for each test case, the iteration counts and the final residual norms for both the unpreconditioned Conjugate Gradient (CG) and the preconditioned variant (PCG).\n\nStart from the following fundamental base:\n- The $5$-point Laplacian on an $n \\times n$ grid (with lexicographic ordering) is a sparse, symmetric positive definite (SPD) matrix whose nonzeros per row correspond to a discrete stencil with one central coefficient and up to four nearest neighbors.\n- The Conjugate Gradient method for SPD matrices is a Krylov subspace method that minimizes the $A$-norm of the error over an expanding Krylov subspace and produces a monotonically decreasing residual norm when exact arithmetic is assumed.\n- A left preconditioner $M$ that is SPD transforms the system to $M^{-1} A x = M^{-1} b$, improving convergence when $M^{-1} A$ has a smaller spectral condition number. The incomplete Cholesky (IC) factorization constructs a lower-triangular $L$ with a prescribed sparsity pattern to approximate $A \\approx L L^{\\top}$ while maintaining $M = L L^{\\top}$ SPD under standard conditions.\n\nYour program must:\n1. Construct $A$ for a given $n$ using lexicographic indexing, where the mapping from grid coordinates $(i,j)$ with $i,j \\in \\{0,\\dots,n-1\\}$ to the vector index $k \\in \\{0,\\dots,N-1\\}$ is $k = i + n j$, where $N = n^2$. The matrix $A$ must reflect homogeneous Dirichlet boundary conditions on the square, using the standard $5$-point discrete Laplacian stencil with central coefficient and immediate neighbor couplings.\n2. Compute an IC$(0)$ factor $L$ of $A$ by enforcing that the sparsity pattern of $L$ is exactly the lower-triangular part (including the diagonal) of the sparsity pattern of $A$. If a nonpositive pivot arises due to floating-point roundoff, you may add a minimal positive diagonal shift to maintain numerical stability while preserving the zero-fill constraint.\n3. Implement both CG and PCG with the following specifications:\n   - Stopping criterion: stop when $\\lVert r_k \\rVert_2 / \\lVert r_0 \\rVert_2 \\leq \\tau$ with tolerance $\\tau = 10^{-8}$, or when $k$ reaches the maximum number of iterations $k_{\\max} = N$.\n   - Initial guess: $x_0 = 0$.\n   - Preconditioner application in PCG: solve $L z = r$ by forward substitution followed by $L^{\\top} y = z$ by backward substitution, yielding $y = M^{-1} r$.\n4. For each test case, run both CG (no preconditioner) and PCG (with $M = L L^{\\top}$), and report:\n   - The number of iterations $k$ taken to satisfy the stopping criterion.\n   - The final residual $2$-norm $\\lVert r_k \\rVert_2$ at termination.\n\nTest suite. Your program must run the following three test cases:\n- Case A (happy path): $n = 10$, $b \\in \\mathbb{R}^{N}$ with all entries equal to $1$.\n- Case B (larger grid with random forcing): $n = 20$, $b$ has independent entries drawn from a uniform distribution on $[0,1)$ using a pseudorandom number generator seeded with $42$.\n- Case C (edge case with localized forcing): $n = 4$, $b = e_k$ where $k = i_0 + n j_0$ with $i_0 = \\lfloor n/2 \\rfloor$ and $j_0 = \\lfloor n/2 \\rfloor$ in zero-based indexing.\n\nQuantities and units. All quantities are dimensionless real numbers in double precision. No physical units are used.\n\nFinal output format. Your program should produce a single line of output containing a Python-style list of results, one per test case, in order [Case A, Case B, Case C]. Each result must be a list of four numbers in the order $[k_{\\mathrm{CG}}, \\lVert r_{\\mathrm{CG}} \\rVert_2, k_{\\mathrm{PCG}}, \\lVert r_{\\mathrm{PCG}} \\rVert_2]$. For example:\n- $[[12,1.23e{-}10,8,4.56e{-}12],[\\dots],[\\dots]]$.\nYour program must print exactly one line with this aggregated list and nothing else.",
            "solution": "The user has provided a valid problem statement. The problem is well-defined, scientifically sound, and grounded in the established principles of numerical linear algebra. It requests the implementation and comparison of the Conjugate Gradient (CG) and Preconditioned Conjugate Gradient (PCG) methods for solving a linear system derived from a partial differential equation.\n\nThe core of the problem is to solve the linear system of equations $Ax = b$, where the matrix $A \\in \\mathbb{R}^{N \\times N}$ represents the discrete $5$-point Laplacian operator on a two-dimensional $n \\times n$ grid with homogeneous Dirichlet boundary conditions. The total number of unknowns is $N = n^2$. The vector $x \\in \\mathbb{R}^{N}$ represents the solution on the grid, and $b \\in \\mathbb{R}^{N}$ is a given forcing term. The mapping from the grid coordinates $(i,j)$ ($i,j \\in \\{0, \\dots, n-1\\}$) to the linear index $k$ ($k \\in \\{0, \\dots, N-1\\}$) is specified as lexicographic: $k = i + n j$.\n\nThe matrix $A$ resulting from this discretization is sparse, symmetric, and positive definite (SPD). Its structure is defined by the $5$-point stencil, which couples each grid point to its immediate North, South, East, and West neighbors. For a given row $k$ corresponding to grid point $(i,j)$, the non-zero entries are:\n$A_{k,k} = 4$\n$A_{k,k-1} = -1$ (if $i0$)\n$A_{k,k+1} = -1$ (if $in-1$)\n$A_{k,k-n} = -1$ (if $j0$)\n$A_{k,k+n} = -1$ (if $jn-1$)\n\nFor solving such SPD systems, the Conjugate Gradient (CG) method is an optimal choice. It is an iterative Krylov subspace method that, in exact arithmetic, is guaranteed to converge to the exact solution in at most $N$ iterations. The convergence rate of CG is governed by the spectral condition number $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$, where $\\lambda$ denotes an eigenvalue. For the 2D Laplacian, $\\kappa(A) = O(N) = O(n^2)$, which leads to a slow-down in convergence as the grid size $n$ increases.\n\nTo accelerate convergence, we employ a preconditioner $M$. A good preconditioner $M$ is a matrix that is also SPD, inexpensive to invert, and approximates $A$ in some sense, such that the condition number of the preconditioned matrix $M^{-1}A$ is much smaller than $\\kappa(A)$. The problem specifies using the Incomplete Cholesky factorization with zero fill-in, denoted IC$(0)$, as the preconditioner. This method computes an approximate Cholesky factorization $A \\approx L L^{\\top}$, where $L$ is a lower triangular matrix. The \"zero fill-in\" constraint means that the sparsity pattern of $L$ is restricted to be the same as the lower triangular part of the original matrix $A$. That is, if $A_{ij}=0$ for $ij$, then $L_{ij}$ is constrained to be $0$.\n\nThe matrix $A$ is an M-matrix, for which it is known that the IC$(0)$ factorization is stable and the pivots (diagonal elements) remain positive. Therefore, the specified diagonal shift is only a safeguard against floating-point roundoff errors. For the specific 5-point stencil on a lexicographically ordered grid, the IC$(0)$ algorithm simplifies significantly. The non-zero entries of the factor $L$ can be computed via simple recurrences. For each grid point with index $k = i+nj$:\n$$\nl_{k,k-1} = \\frac{a_{k,k-1}}{l_{k-1,k-1}} \\quad (\\text{if } i0)\n$$\n$$\nl_{k,k-n} = \\frac{a_{k,k-n}}{l_{k-n,k-n}} \\quad (\\text{if } j0)\n$$\n$$\nl_{k,k} = \\sqrt{a_{k,k} - l_{k,k-1}^2 - l_{k,k-n}^2}\n$$\nThese formulas exploit the fact that for this structured matrix, the dot product term in the general IC$(0)$ formula, $\\sum_{pk} l_{i,p} l_{j,p}$, becomes zero for all relevant $i,j$. This specialized algorithm is both correct and highly efficient.\n\nThe Preconditioned Conjugate Gradient (PCG) method is then applied to the transformed system. With a left preconditioner $M=LL^{\\top}$, the algorithm requires solving a system $M z_k = r_k$ in each iteration $k$. This is efficiently performed by a two-step triangular solve: first, a forward substitution to solve $Ly = r_k$ for an intermediate vector $y$, followed by a backward substitution to solve $L^{\\top}z_k = y$ for the final vector $z_k$.\n\nThe program will implement the following components:\n1.  A function to construct the sparse matrix $A$ for a given grid size $n$.\n2.  A function to compute the IC$(0)$ factor $L$ using the simplified recurrence relations for the 5-point stencil.\n3.  Implementations of both the standard CG and the PCG algorithms.\n4.  A main routine to iterate through the specified test cases, run both solvers, and collect the required metrics: iteration count and final residual norm.\n\nThe stopping criterion for both solvers is when the relative residual norm falls below a tolerance $\\tau = 10^{-8}$, i.e., $\\lVert r_k \\rVert_2 / \\lVert r_0 \\rVert_2 \\leq \\tau$, or a maximum of $N$ iterations is reached. The initial guess is the zero vector, $x_0 = 0$.",
            "answer": "```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import spsolve_triangular\n\ndef build_laplacian_5pt(n):\n    \"\"\"\n    Constructs the sparse 2D 5-point Laplacian matrix A on an n x n grid\n    with homogeneous Dirichlet boundary conditions. The matrix is returned\n    in Compressed Sparse Column (CSC) format.\n    \"\"\"\n    N = n * n\n    # Use DOK format for intuitive construction, then convert for efficiency.\n    A = sp.dok_matrix((N, N), dtype=np.float64)\n\n    for j in range(n):  # y-coordinate\n        for i in range(n):  # x-coordinate\n            k = i + n * j  # lexicographic index\n\n            A[k, k] = 4.0\n            if i  0:\n                A[k, k - 1] = -1.0\n            if i  n - 1:\n                A[k, k + 1] = -1.0\n            if j  0:\n                A[k, k - n] = -1.0\n            if j  n - 1:\n                A[k, k + n] = -1.0\n\n    return A.tocsc()\n\ndef ic0_5pt(A, n):\n    \"\"\"\n    Computes the IC(0) factorization for the 5-point Laplacian matrix.\n    This specialized implementation exploits the matrix structure for efficiency,\n    avoiding a general-purpose factorization algorithm.\n    The factor L is returned in Compressed Sparse Row (CSR) format.\n    \"\"\"\n    N = n * n\n    # LIL format is efficient for the incremental construction of L.\n    L = sp.lil_matrix((N, N), dtype=np.float64)\n    A_dok = A.todok() # DOK is efficient for single element access by index\n\n    for k in range(N):\n        i, j = k % n, k // n\n\n        l_k_km1_sq = 0.0\n        if i  0:\n            km1_idx = k - 1\n            val = A_dok[k, km1_idx] / L[km1_idx, km1_idx]\n            L[k, km1_idx] = val\n            l_k_km1_sq = val**2\n\n        l_k_kmn_sq = 0.0\n        if j  0:\n            kmn_idx = k - n\n            val = A_dok[k, kmn_idx] / L[kmn_idx, kmn_idx]\n            L[k, kmn_idx] = val\n            l_k_kmn_sq = val**2\n            \n        diag_sq = A_dok[k, k] - l_k_km1_sq - l_k_kmn_sq\n\n        # Add a minimal positive diagonal shift for numerical stability\n        if diag_sq = 1e-12:\n            diag_sq = 1e-10\n\n        L[k, k] = np.sqrt(diag_sq)\n\n    return L.tocsr()\n\ndef conjugate_gradient(A, b, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n\n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return x, 0, 0.0\n\n    rs_old = np.dot(r, r)\n\n    for k in range(max_iter):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n\n        x += alpha * p\n        r -= alpha * Ap\n        \n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm = tol:\n            return x, k + 1, r_norm\n\n        rs_new = np.dot(r, r)\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return x, max_iter, np.linalg.norm(r)\n\ndef get_preconditioner_solver(L):\n    \"\"\"\n    Returns a function that applies the IC(0) preconditioner M = LL^T by solving\n    My = r via forward and backward substitution.\n    \"\"\"\n    Lt = L.T.tocsr()\n    def M_solve(r):\n        z = spsolve_triangular(L, r, lower=True)\n        y = spsolve_triangular(Lt, z, lower=False)\n        return y\n    return M_solve\n\ndef preconditioned_cg(A, b, x0, tol, max_iter, M_solve):\n    \"\"\"\n    Solves Ax=b using the Preconditioned Conjugate Gradient method.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n\n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return x, 0, 0.0\n\n    z = M_solve(r)\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(max_iter):\n        Ap = A @ p\n        alpha = rz_old / np.dot(p, Ap)\n\n        x += alpha * p\n        r -= alpha * Ap\n\n        r_norm = np.linalg.norm(r)\n        if r_norm / r0_norm = tol:\n            return x, k + 1, r_norm\n\n        z = M_solve(r)\n        rz_new = np.dot(r, z)\n        p = z + (rz_new / rz_old) * p\n        rz_old = rz_new\n\n    return x, max_iter, np.linalg.norm(r)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'n': 10, 'b_type': 'ones'},\n        {'n': 20, 'b_type': 'random'},\n        {'n': 4, 'b_type': 'point'}\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        n = case['n']\n        b_type = case['b_type']\n        N = n * n\n\n        A = build_laplacian_5pt(n)\n\n        if b_type == 'ones':\n            b = np.ones(N, dtype=np.float64)\n        elif b_type == 'random':\n            rng = np.random.default_rng(42)\n            b = rng.uniform(0, 1, size=N)\n        elif b_type == 'point':\n            b = np.zeros(N, dtype=np.float64)\n            i0, j0 = n // 2, n // 2\n            k_idx = i0 + n * j0\n            b[k_idx] = 1.0\n\n        x0 = np.zeros(N, dtype=np.float64)\n        tol = 1e-8\n        max_iter = N\n\n        # Run unpreconditioned Conjugate Gradient\n        _, k_cg, norm_r_cg = conjugate_gradient(A, b, x0, tol, max_iter)\n\n        # Compute IC(0) preconditioner and run PCG\n        L = ic0_5pt(A, n)\n        M_solver = get_preconditioner_solver(L)\n        _, k_pcg, norm_r_pcg = preconditioned_cg(A, b, x0, tol, max_iter, M_solver)\n\n        all_results.append([k_cg, norm_r_cg, k_pcg, norm_r_pcg])\n    \n    # Format the final output as a Python-style list of lists, on a single line.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}