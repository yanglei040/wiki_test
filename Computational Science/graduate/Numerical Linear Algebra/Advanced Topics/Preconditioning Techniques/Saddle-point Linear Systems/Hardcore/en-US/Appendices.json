{
    "hands_on_practices": [
        {
            "introduction": "A defining characteristic of saddle-point matrices is their indefiniteness, which distinguishes them from the symmetric positive definite matrices often encountered in optimization and physics. This practice  provides a concrete, constructive exercise to build intuition for this fundamental property. By assembling a simple $3 \\times 3$ saddle-point matrix from specific blocks and then finding vectors $z$ for which the quadratic form $z^T K z$ is both positive and negative, you will gain a hands-on understanding of what makes these systems algebraically unique.",
            "id": "3575826",
            "problem": "Consider the saddle-point block matrix $K \\in \\mathbb{R}^{3 \\times 3}$ of the form\n$$\nK \\;=\\; \\begin{bmatrix} A  B^{T} \\\\ B  -C \\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$ is symmetric, $B \\in \\mathbb{R}^{1 \\times 2}$, and $C \\in \\mathbb{R}^{1 \\times 1}$. Work within the discrete sets\n$$\n\\mathcal{A} \\;=\\; \\left\\{ \\begin{bmatrix} a  b \\\\ b  d \\end{bmatrix} \\,\\middle|\\, a,b,d \\in \\{0,1\\} \\right\\}, \n\\quad\n\\mathcal{B} \\;=\\; \\left\\{ \\begin{bmatrix} b_{1}  b_{2} \\end{bmatrix} \\,\\middle|\\, b_{1}, b_{2} \\in \\{0,1\\} \\right\\},\n\\quad\n\\mathcal{C} \\;=\\; \\{1\\},\n$$\nand impose the constraints that $A$ is symmetric positive definite (SPD) and $B \\neq \\mathbf{0}$. Use lexicographic order on the parameter tuples to remove ambiguity: for $A$ compare $(a,b,d)$; for $B$ compare $(b_{1},b_{2})$; for $C$ there is only $1$. Select the lexicographically minimal triple $(A,B,C) \\in \\mathcal{A} \\times \\mathcal{B} \\times \\mathcal{C}$ satisfying these constraints.\n\nStarting from the definition of definiteness via the quadratic form, and using only fundamental linear algebra facts (such as properties of quadratic forms, completion of the square, and the characterization of minimizers for strictly convex quadratic functions), construct a nonzero vector $z \\in \\mathbb{R}^{3}$ such that $z^{T} K z  0$ and argue that $K$ is symmetric indefinite by also identifying a vector that yields a positive quadratic form.\n\nFinally, for your constructed negative-certifying vector $z$, compute the exact value of the scalar $z^{T} K z$. Express your final answer as an exact integer. No rounding is required.",
            "solution": "The problem is found to be valid as it is mathematically well-defined, self-contained, and consistent.\n\nFirst, we must determine the specific matrix $K$ by selecting the lexicographically minimal triple $(A, B, C)$ from the given sets $\\mathcal{A} \\times \\mathcal{B} \\times \\mathcal{C}$ that satisfies the specified constraints.\n\nThe matrix $A \\in \\mathcal{A}$ is of the form $A = \\begin{bmatrix} a  b \\\\ b  d \\end{bmatrix}$ where $a, b, d \\in \\{0, 1\\}$. The constraint is that $A$ must be symmetric positive definite (SPD). A symmetric matrix is positive definite if and only if all its leading principal minors are positive.\nThe first leading principal minor is $\\det(A_1) = a$. For $A$ to be SPD, we must have $a  0$. Since $a \\in \\{0, 1\\}$, this implies $a=1$.\nThe second leading principal minor is $\\det(A_2) = \\det(A) = ad - b^2$. For $A$ to be SPD, we must have $ad - b^2  0$. Substituting $a=1$, this becomes $d - b^2  0$.\nWe need to find the lexicographically minimal tuple $(a,b,d)$ that satisfies these conditions. We already know $a=1$.\nWe test values for $b \\in \\{0, 1\\}$.\n- If $b=0$, the condition becomes $d - 0^2  0$, so $d  0$. Since $d \\in \\{0, 1\\}$, we must have $d=1$. This gives the tuple $(1,0,1)$.\n- If $b=1$, the condition becomes $d - 1^2  0$, so $d  1$. There is no solution for $d \\in \\{0, 1\\}$.\nTherefore, the only possible tuple is $(1,0,1)$, which is necessarily the lexicographically minimal one.\nThe resulting matrix is $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, the $2 \\times 2$ identity matrix $I_2$.\n\nThe matrix $B \\in \\mathcal{B}$ is of the form $B = \\begin{bmatrix} b_1  b_2 \\end{bmatrix}$ where $b_1, b_2 \\in \\{0, 1\\}$. The constraints are $B \\neq \\mathbf{0}$ and that the tuple $(b_1, b_2)$ must be lexicographically minimal. The set $\\mathcal{B}$ consists of the tuples $(0,0), (0,1), (1,0), (1,1)$. Excluding the zero vector gives the set of possible tuples as $\\{(0,1), (1,0), (1,1)\\}$. The lexicographically minimal of these is $(0,1)$.\nThus, $B = \\begin{bmatrix} 0  1 \\end{bmatrix}$.\n\nThe matrix $C \\in \\mathcal{C}$ is drawn from the singleton set $\\mathcal{C} = \\{1\\}$. Thus, $C = [1]$.\n\nNow we assemble the saddle-point matrix $K$:\n$A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $B = \\begin{bmatrix} 0  1 \\end{bmatrix}$, $C = [1]$.\nWe have $B^T = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ and $-C = [-1]$.\n$$\nK = \\begin{bmatrix} A  B^T \\\\ B  -C \\end{bmatrix} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  1  -1 \\end{bmatrix}.\n$$\nThe problem requires us to show that $K$ is symmetric indefinite. $K$ is symmetric by construction. To show it is indefinite, we must find vectors $z_1, z_2 \\in \\mathbb{R}^3$ such that $z_1^T K z_1  0$ and $z_2^T K z_2  0$.\n\nLet a generic vector $z \\in \\mathbb{R}^3$ be partitioned as $z = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2$ and $y \\in \\mathbb{R}$. The quadratic form associated with $K$ is:\n$$\nz^T K z = \\begin{bmatrix} x^T  y \\end{bmatrix} \\begin{bmatrix} A  B^T \\\\ B  -C \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = x^T A x + x^T B^T y + y B x - y^2 C = x^T A x + 2 y B x - y^2 C.\n$$\nSubstituting the specific matrices $A$, $B$, and $C$:\n$$\nz^T K z = x^T I_2 x + 2y \\begin{bmatrix} 0  1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - y^2(1) = (x_1^2 + x_2^2) + 2yx_2 - y^2.\n$$\nTo find a vector that yields a positive quadratic form, consider vectors of the form $z = \\begin{bmatrix} x \\\\ 0 \\end{bmatrix}$ with $x \\neq \\mathbf{0}$. For such vectors, $y=0$, and the quadratic form simplifies to $z^T K z = x^T A x$. Since $A=I_2$ is SPD, $x^T A x  0$ for all $x \\neq \\mathbf{0}$. For instance, let us choose $x = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. This gives the vector $z_p = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\nThe quadratic form value is $z_p^T K z_p = 1^2 + 0^2 + 2(0)(0) - 0^2 = 1  0$.\n\nNext, we must construct a nonzero vector $z$ such that $z^T K z  0$. We analyze the quadratic form $x_1^2 + x_2^2 + 2yx_2 - y^2$ by completing the square with respect to the $x_2$ variable:\n$$\nz^T K z = x_1^2 + (x_2^2 + 2yx_2 + y^2) - y^2 - y^2 = x_1^2 + (x_2 + y)^2 - 2y^2.\n$$\nTo make this expression negative, we can choose $x_1, x_2, y$ to make the positive terms vanish. We set:\n$x_1 = 0$\n$x_2 + y = 0 \\implies x_2 = -y$\nThis choice gives $z^T K z = 0^2 + 0^2 - 2y^2 = -2y^2$. To ensure $z$ is a nonzero vector, we can choose any nonzero value for $y$. Let us choose $y=1$. This implies $x_1=0$ and $x_2=-1$. The resulting vector is:\n$$\nz = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n$$\nThis vector is nonzero. With this choice of $z$, we have established that $z^T K z  0$. The existence of vectors yielding both positive and negative values confirms that the symmetric matrix $K$ is indefinite.\n\nFinally, we compute the exact value of the scalar $z^T K z$ for this constructed vector $z = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$.\nUsing the completed-square expression:\n$$\nz^T K z = x_1^2 + (x_2 + y)^2 - 2y^2 = 0^2 + (-1 + 1)^2 - 2(1)^2 = 0 + 0 - 2 = -2.\n$$\nAlternatively, by direct matrix multiplication:\n$$\nz^T K z = \\begin{bmatrix} 0  -1  1 \\end{bmatrix} \\begin{bmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  1  -1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} (0)(1) + (-1)(0) + (1)(0)  (0)(0) + (-1)(1) + (1)(1)  (0)(0) + (-1)(1) + (1)(-1) \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} 0  0  -2 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix} = (0)(0) + (0)(-1) + (-2)(1) = -2.\n$$\nBoth methods yield the same result. The value is an exact integer as requested.",
            "answer": "$$\\boxed{-2}$$"
        },
        {
            "introduction": "Moving beyond basic properties, the stability and well-posedness of a saddle-point system are critical for ensuring that solutions are reliable and that iterative solvers perform efficiently. The inf-sup (or LBB) condition provides a key theoretical measure of this stability. This computational exercise  creates a direct link between this abstract theory and practical solver performance by asking you to explore how the inf-sup constant, $\\beta$, influences the eigenvalue distribution of a parameterized saddle-point matrix and, in turn, the robustness of the MINRES algorithm.",
            "id": "3575837",
            "problem": "Consider the saddle-point linear system matrix family $K(\\epsilon) \\in \\mathbb{R}^{(n+m)\\times(n+m)}$ defined by\n$$\nK(\\epsilon) = \\begin{bmatrix} A  B^{\\top} \\\\ B  -\\epsilon I_m \\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{n\\times n}$ is symmetric positive definite, $B \\in \\mathbb{R}^{m\\times n}$, and $\\epsilon \\ge 0$. In a finite-dimensional setting, the stability of the pair $(A,B)$ is quantified by the infimum-supremum constant, which for the choice of the $A$-induced norm on the primal space and the Euclidean norm on the dual space is\n$$\n\\beta = \\inf_{q \\neq 0} \\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}\\,\\|q\\|_2}, \\quad \\text{with } \\|v\\|_{A} = \\sqrt{v^{\\top} A v}.\n$$\nThe goal is to construct a family of examples for which the system can be analyzed in detail and to explore how the constant $\\beta$ controls the extremal eigenvalues of $K(\\epsilon)$ as $\\epsilon \\to 0^{+}$, and to identify parameter regimes under which the Minimum Residual method (MINRES) behaves robustly or deteriorates, based on spectral considerations alone.\n\nStart from the foundational facts of spectral theory for symmetric matrices, singular values for rectangular matrices, and the Schur complement. Avoid using any shortcut formulas that directly give the extremal eigenvalues or ready-made spectral bounds; instead, derive all conclusions from the definitions of $\\beta$ and the structure of $K(\\epsilon)$ as specified.\n\nWork in the special case $m=n$, and let $A=\\mathrm{diag}(a_1,\\dots,a_n)$ with $a_i0$ and $B=\\mathrm{diag}(d_1,\\dots,d_n)$ with $d_i \\ge 0$. In this setting:\n- Derive the relationship between $\\beta$ and the spectral properties of the symmetric positive semidefinite matrix $S = B A^{-1} B^{\\top}$, and explain how $\\beta$ is determined by $\\{a_i\\}$ and $\\{d_i\\}$.\n- Derive how the eigenvalues of $K(\\epsilon)$ decompose into $n$ independent $2\\times 2$ blocks and determine the extremal eigenvalues in terms of $\\epsilon$, $\\{a_i\\}$, and $\\{d_i\\}$.\n- Using these derivations, explain the limiting behavior of the smallest magnitude eigenvalue of $K(\\epsilon)$ as $\\epsilon \\to 0^{+}$, and express this behavior in terms of the constant $\\beta$.\n- Based on spectral theory of optimal Krylov subspace methods for symmetric matrices, justify a simple, quantifiable robustness criterion for MINRES that uses only the spectrum of $K(\\epsilon)$: declare MINRES to be robust if the ratio of the smallest to largest magnitude eigenvalue of $K(\\epsilon)$ exceeds a fixed threshold $\\tau0$, and deteriorating otherwise. For the purposes of this exercise, take $\\tau=10^{-3}$.\n\nImplement a program that, for given parameters $\\{a_i\\}$, $\\{d_i\\}$, and $\\epsilon$, constructs $K(\\epsilon)$, computes the infimum-supremum constant $\\beta$, computes the smallest and largest magnitude eigenvalues of $K(\\epsilon)$, and classifies the MINRES robustness via the above criterion. The final output for each test case must be a list holding three quantities in the order: $[\\beta, \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)), \\mathrm{robust}]$, where $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon))$ denotes the smallest magnitude eigenvalue of $K(\\epsilon)$ and $\\mathrm{robust}$ is a boolean logical value defined by the threshold rule with $\\tau=10^{-3}$.\n\nNo physical units are involved in this problem. Angles and percentages are not applicable. All matrix entries and results are real numbers.\n\nUse the following test suite of parameter values to exercise different regimes:\n- Test case $1$ (happy path, uniformly stable, small $\\epsilon$): $n=4$, $a=[1,2,3,4]$, $d=[1,1,1,1]$, $\\epsilon=10^{-2}$.\n- Test case $2$ (small infimum-supremum constant, very small $\\epsilon$): $n=4$, $a=[1,1,1,1]$, $d=[10^{-3},1,1,1]$, $\\epsilon=10^{-8}$.\n- Test case $3$ (zero infimum-supremum constant, small but nonzero $\\epsilon$): $n=4$, $a=[1,1,1,1]$, $d=[0,1,1,1]$, $\\epsilon=10^{-2}$.\n- Test case $4$ (zero infimum-supremum constant, $\\epsilon=0$, singular system): $n=4$, $a=[1,1,1,1]$, $d=[0,1,1,1]$, $\\epsilon=0$.\n- Test case $5$ (anisotropic but stable, extremely small $\\epsilon$): $n=4$, $a=[1,4,9,16]$, $d=[1,2,3,4]$, $\\epsilon=10^{-9}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). Each $resultj$ should be the list $[\\beta, \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)), \\mathrm{robust}]$ for test case $j$, in the order listed above. All computations must be performed using real arithmetic with standard linear algebra routines.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the field of numerical linear algebra, well-posed for the specified special case, objective, and provides a complete and consistent setup for a rigorous mathematical derivation and subsequent implementation.\n\nThe solution proceeds in four stages as requested: deriving the relationship between the inf-sup constant $\\beta$ and the matrix properties, deriving the eigenvalues of $K(\\epsilon)$, analyzing the limiting spectral behavior as $\\epsilon \\to 0^{+}$, and applying the spectral robustness criterion for the MINRES method.\n\n**1. Relationship between $\\beta$ and the Schur Complement $S$**\n\nThe infimum-supremum constant is defined as\n$$\n\\beta = \\inf_{q \\neq 0} \\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}\\,\\|q\\|_2}\n$$\nwhere $\\|v\\|_{A} = \\sqrt{v^{\\top} A v}$. Since $A \\in \\mathbb{R}^{n\\times n}$ is symmetric positive definite (SPD), its square root $A^{1/2}$ and inverse $A^{-1/2}$ exist and are also SPD. Let us analyze the inner supremum term for a fixed non-zero vector $q \\in \\mathbb{R}^m$. By performing a change of variables $w = A^{1/2}v$, we have $v = A^{-1/2}w$ and $\\|v\\|_A = \\sqrt{(A^{-1/2}w)^{\\top} A (A^{-1/2}w)} = \\sqrt{w^{\\top} A^{-1/2} A A^{-1/2} w} = \\sqrt{w^{\\top}w} = \\|w\\|_2$. The expression becomes:\n$$\n\\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}} = \\sup_{w \\neq 0} \\frac{q^{\\top} B A^{-1/2} w}{\\|w\\|_2}\n$$\nThis is the definition of the Euclidean norm of the vector $(q^{\\top} B A^{-1/2})^{\\top} = A^{-1/2} B^{\\top} q$. Therefore,\n$$\n\\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}} = \\|A^{-1/2} B^{\\top} q\\|_2 = \\sqrt{(A^{-1/2} B^{\\top} q)^{\\top}(A^{-1/2} B^{\\top} q)} = \\sqrt{q^{\\top} B A^{-1} B^{\\top} q}\n$$\nWe define the Schur complement of $A$ in $K(0)$ as $S = B A^{-1} B^{\\top}$. Since $A^{-1}$ is SPD, $S$ is symmetric positive semidefinite (SPSD). The expression simplifies to $\\sqrt{q^{\\top} S q}$. Substituting this back into the definition of $\\beta$:\n$$\n\\beta = \\inf_{q \\neq 0} \\frac{\\sqrt{q^{\\top} S q}}{\\|q\\|_2} = \\inf_{\\|q\\|_2=1} \\sqrt{q^{\\top} S q}\n$$\nThe square of this expression, $\\beta^2 = \\inf_{\\|q\\|_2=1} q^{\\top} S q$, is the Rayleigh quotient of $S$ minimized over the unit sphere, which is precisely the smallest eigenvalue of the SPSD matrix $S$, denoted $\\lambda_{\\min}(S)$. Thus, we have the fundamental relationship $\\beta = \\sqrt{\\lambda_{\\min}(S)}$.\n\nFor the special case where $m=n$, $A=\\mathrm{diag}(a_1,\\dots,a_n)$ with $a_i0$, and $B=\\mathrm{diag}(d_1,\\dots,d_n)$ with $d_i \\ge 0$, the matrices are diagonal.\n$A^{-1} = \\mathrm{diag}(1/a_1, \\dots, 1/a_n)$.\nThe Schur complement $S$ is also diagonal:\n$$\nS = B A^{-1} B^{\\top} = \\mathrm{diag}(d_i) \\mathrm{diag}(1/a_i) \\mathrm{diag}(d_i) = \\mathrm{diag}\\left(\\frac{d_1^2}{a_1}, \\dots, \\frac{d_n^2}{a_n}\\right)\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. Therefore, $\\lambda_{\\min}(S) = \\min_{i \\in \\{1,\\dots,n\\}} \\left\\{\\frac{d_i^2}{a_i}\\right\\}$.\nThe inf-sup constant is then given by:\n$$\n\\beta = \\sqrt{\\min_{i} \\frac{d_i^2}{a_i}} = \\min_{i} \\frac{|d_i|}{\\sqrt{a_i}} = \\min_{i} \\frac{d_i}{\\sqrt{a_i}}\n$$\nsince $d_i \\ge 0$ and $a_i  0$.\n\n**2. Eigenvalue Decomposition of $K(\\epsilon)$**\n\nGiven that $A$ and $B$ are diagonal matrices, the system matrix $K(\\epsilon)$ has a special block structure:\n$$\nK(\\epsilon) = \\begin{bmatrix} \\mathrm{diag}(a_1, \\dots, a_n)  \\mathrm{diag}(d_1, \\dots, d_n) \\\\ \\mathrm{diag}(d_1, \\dots, d_n)  -\\epsilon I_n \\end{bmatrix}\n$$\nThe rows and columns of this matrix can be permuted to group the $i$-th components together. A suitable permutation matrix $P$ transforms $K(\\epsilon)$ into a block-diagonal matrix:\n$$\nP K(\\epsilon) P^{\\top} = \\mathrm{diag}(K_1(\\epsilon), K_2(\\epsilon), \\dots, K_n(\\epsilon))\n$$\nwhere each block $K_i(\\epsilon)$ is a $2\\times 2$ matrix:\n$$\nK_i(\\epsilon) = \\begin{bmatrix} a_i  d_i \\\\ d_i  -\\epsilon \\end{bmatrix}\n$$\nThe spectrum of $K(\\epsilon)$ is the union of the spectra of these smaller matrices $K_i(\\epsilon)$. The eigenvalues of $K_i(\\epsilon)$ are found from its characteristic polynomial $\\det(K_i(\\epsilon) - \\lambda I) = 0$:\n$$\n(a_i - \\lambda)(-\\epsilon - \\lambda) - d_i^2 = 0 \\\\\n\\lambda^2 + (\\epsilon - a_i)\\lambda - (a_i\\epsilon + d_i^2) = 0\n$$\nUsing the quadratic formula, the two eigenvalues for each block $i$ are:\n$$\n\\lambda_{i, \\pm} = \\frac{-( \\epsilon - a_i) \\pm \\sqrt{(\\epsilon - a_i)^2 - 4(1)(-(a_i\\epsilon + d_i^2))}}{2} = \\frac{a_i - \\epsilon \\pm \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2}}{2}\n$$\nThese $2n$ values, $\\{\\lambda_{i, \\pm}\\}_{i=1}^n$, constitute the entire spectrum of $K(\\epsilon)$. Since $a_i  0$, $\\epsilon \\ge 0$, the term under the square root $\\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\ge a_i + \\epsilon$. This implies that for each $i$, $\\lambda_{i, +}  0$ and $\\lambda_{i, -} \\le 0$, confirming that $K(\\epsilon)$ is an indefinite matrix. The extremal eigenvalues are $\\lambda_{\\max}(K(\\epsilon)) = \\max_i \\lambda_{i,+}$ and $\\lambda_{\\min}(K(\\epsilon)) = \\min_i \\lambda_{i,-}$.\n\n**3. Limiting Behavior of the Smallest Magnitude Eigenvalue**\n\nWe analyze the smallest magnitude eigenvalue, $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\min_{i, \\pm} |\\lambda_{i, \\pm}|$, as $\\epsilon \\to 0^{+}$. The behavior depends critically on whether $\\beta$ is zero or positive.\n\nCase 1: $\\beta = 0$. This implies from our derivation that there exists at least one index $k$ for which $d_k=0$. For this block, the matrix is $K_k(\\epsilon) = \\mathrm{diag}(a_k, -\\epsilon)$, and its eigenvalues are simply $\\lambda_{k,+} = a_k$ and $\\lambda_{k,-} = -\\epsilon$. Their magnitudes are $a_k  0$ and $\\epsilon \\ge 0$. For any other block $j$ with $d_j  0$, as $\\epsilon \\to 0^{+}$, the eigenvalues $\\lambda_{j,\\pm}$ approach non-zero limits $\\frac{1}{2}(a_j \\pm \\sqrt{a_j^2+4d_j^2})$. For sufficiently small $\\epsilon  0$, the smallest magnitude eigenvalue across all blocks will be $|\\lambda_{k,-}| = \\epsilon$. Thus, if $\\beta=0$, the smallest magnitude eigenvalue approaches $0$ linearly with $\\epsilon$: $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\epsilon$ for small $\\epsilon$.\n\nCase 2: $\\beta  0$. This implies that $d_i  0$ for all $i \\in \\{1, \\dots, n\\}$. We examine the magnitudes $|\\lambda_{i,\\pm}|$ as $\\epsilon \\to 0^{+}$.\n$$\n|\\lambda_{i,+}| = \\lambda_{i,+} = \\frac{1}{2} \\left( a_i - \\epsilon + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right) \\xrightarrow{\\epsilon \\to 0^+} \\frac{1}{2}\\left(a_i + \\sqrt{a_i^2+4d_i^2}\\right)  0\n$$\n$$\n|\\lambda_{i,-}| = -\\lambda_{i,-} = \\frac{1}{2} \\left( -(a_i - \\epsilon) + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right) \\xrightarrow{\\epsilon \\to 0^+} \\frac{1}{2}\\left(-a_i + \\sqrt{a_i^2+4d_i^2}\\right)  0\n$$\nSince both limits are positive constants, the overall smallest magnitude eigenvalue $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon))$ converges to a positive constant:\n$$\n\\lim_{\\epsilon \\to 0^+} \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\min_{i} \\left\\{ \\frac{1}{2}\\left(-a_i + \\sqrt{a_i^2+4d_i^2}\\right) \\right\\}  0\n$$\nThe value of this constant depends on all $\\{a_i\\}$ and $\\{d_i\\}$, not just on $\\beta$. However, the *behavior* is directly tied to $\\beta$: if $\\beta  0$, the system's spectrum is bounded away from zero uniformly in $\\epsilon$ as $\\epsilon \\to 0^+$, whereas if $\\beta=0$, an eigenvalue approaches zero.\n\n**4. MINRES Robustness Criterion**\n\nThe convergence rate of MINRES depends on the distribution of eigenvalues, and particularly on the condition number of the system matrix. The problem defines a spectral robustness criterion based on the ratio of the smallest to largest magnitude eigenvalues. Let $\\rho = \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) / \\lambda_{\\max}^{\\mathrm{abs}}(K(\\epsilon))$. MINRES is declared \"robust\" if $\\rho  \\tau = 10^{-3}$.\n\nFrom the eigenvalue formulas, we can see that for any $i$, $|\\lambda_{i,+}|  |\\lambda_{i,-}|$. Therefore, the absolute extremal eigenvalues are:\n$$\n\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\min_{i} |\\lambda_{i,-}| = \\min_{i} \\frac{1}{2} \\left( -(a_i - \\epsilon) + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right)\n$$\n$$\n\\lambda_{\\max}^{\\mathrm{abs}}(K(\\epsilon)) = \\max_{i} |\\lambda_{i,+}| = \\max_{i} \\frac{1}{2} \\left( a_i - \\epsilon + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right)\n$$\nFor a given test case, we compute these quantities, calculate their ratio $\\rho$, and compare it to the threshold $\\tau=10^{-3}$ to classify the robustness. If $\\lambda_{\\max}^{\\mathrm{abs}}(K(\\epsilon)) = 0$, which can only happen for a zero matrix, the ratio is taken to be $0$. If $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = 0$ (i.e., the matrix is singular), the ratio is $0$ and the method is deteriorating.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the saddle-point system problem.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: happy path, uniformly stable, small epsilon\n        {'n': 4, 'a': [1, 2, 3, 4], 'd': [1, 1, 1, 1], 'epsilon': 1e-2},\n        # Test case 2: small inf-sup constant, very small epsilon\n        {'n': 4, 'a': [1, 1, 1, 1], 'd': [1e-3, 1, 1, 1], 'epsilon': 1e-8},\n        # Test case 3: zero inf-sup constant, small but nonzero epsilon\n        {'n': 4, 'a': [1, 1, 1, 1], 'd': [0, 1, 1, 1], 'epsilon': 1e-2},\n        # Test case 4: zero inf-sup constant, epsilon=0, singular system\n        {'n': 4, 'a': [1, 1, 1, 1], 'd': [0, 1, 1, 1], 'epsilon': 0},\n        # Test case 5: anisotropic but stable, extremely small epsilon\n        {'n': 4, 'a': [1, 4, 9, 16], 'd': [1, 2, 3, 4], 'epsilon': 1e-9}\n    ]\n\n    results = []\n    tau = 1e-3\n\n    for case in test_cases:\n        a_vec = np.array(case['a'], dtype=float)\n        d_vec = np.array(case['d'], dtype=float)\n        epsilon = float(case['epsilon'])\n        n = case['n']\n\n        # 1. Compute the inf-sup constant beta\n        # beta = min_i(d_i / sqrt(a_i))\n        beta = np.min(d_vec / np.sqrt(a_vec))\n\n        # 2. Compute the eigenvalues of K(epsilon)\n        all_eigenvalues = []\n        for i in range(n):\n            ai = a_vec[i]\n            di = d_vec[i]\n            \n            # lambda_{i, pm} = (a_i - epsilon +/- sqrt((a_i + epsilon)^2 + 4*d_i^2)) / 2\n            sqrt_term = np.sqrt((ai + epsilon)**2 + 4 * di**2)\n            lambda_plus = 0.5 * (ai - epsilon + sqrt_term)\n            lambda_minus = 0.5 * (ai - epsilon - sqrt_term)\n            \n            all_eigenvalues.extend([lambda_plus, lambda_minus])\n        \n        all_eigenvalues = np.array(all_eigenvalues)\n        abs_eigenvalues = np.abs(all_eigenvalues)\n        \n        # 3. Compute smallest and largest magnitude eigenvalues\n        lambda_min_abs = np.min(abs_eigenvalues)\n        lambda_max_abs = np.max(abs_eigenvalues)\n        \n        # 4. Classify MINRES robustness\n        # Handle division by zero for singular/zero matrices\n        if lambda_max_abs > 0:\n            ratio = lambda_min_abs / lambda_max_abs\n        else:\n            ratio = 0.0 # Define ratio for zero matrix\n        \n        robust = ratio > tau\n\n        # 5. Store the results for this case\n        results.append([beta, lambda_min_abs, robust])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) is used to correctly format the list-of-lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Even when a saddle-point system is theoretically well-posed, its numerical solution in finite-precision arithmetic can be fraught with peril, especially when system parameters lead to ill-conditioning. This practice  delves into this crucial aspect of numerical stability by presenting a scenario where the direct formation of the Schur complement is compromised by catastrophic cancellation. You will analyze this instability and then explore a powerful remedy, the augmented Lagrangian method, deriving a stabilized formulation and even optimizing its regularization parameter.",
            "id": "3575855",
            "problem": "Consider a $2\\times 2$ block saddle-point linear system arising from a quadratic program with an equality constraint. Let $n=2$ and $m=1$, and define\n$$\nA=\\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix},\\qquad B=\\begin{pmatrix} 1  1 \\end{pmatrix},\\qquad C=-\\frac{1}{\\epsilon}+\\delta,\n$$\nwhere $\\epsilon\\in(0,1)$ is a small parameter and $\\delta\\in\\mathbb{R}$ is $\\mathcal{O}(1)$. The saddle-point matrix is\n$$\n\\mathcal{K}=\\begin{pmatrix}\nA  B^{T} \\\\\nB  -C\n\\end{pmatrix}.\n$$\nThe Schur complement associated with eliminating the primal variable is defined by\n$$\nS = B A^{-1} B^{T} + C.\n$$\nYou are asked to analyze the numerical instability of explicitly forming $S$ when $A$ is ill-conditioned and to propose a stabilized alternative via an augmented Lagrangian modification.\n\nTasks:\n1. Starting from the definitions of the Schur complement and matrix inverse, derive the exact expression of $S$ in terms of $\\epsilon$ and $\\delta$, and explain why explicitly forming $A^{-1}$ and then evaluating $S$ exhibits catastrophic cancellation as $\\epsilon\\to 0^{+}$.\n\n2. Consider the augmented Lagrangian stabilization with\n$$\nA_{\\gamma}=A+\\gamma B^{T}B,\\qquad \\gamma0.\n$$\nUsing only fundamental identities for rank-one updates, derive a closed-form expression for\n$$\nS_{\\gamma}=B A_{\\gamma}^{-1} B^{T} + C\n$$\nas a rational function of $\\epsilon$, $\\gamma$, and $\\delta$, without forming any explicit matrix inverse beyond what is implied by the rank-one update.\n\n3. To balance the improved numerical behavior of $S_{\\gamma}$ against the cost and stability of inner solves with $A_{\\gamma}$, choose $\\gamma$ to minimize the spectral condition number $\\kappa_{2}(A_{\\gamma})$ of $A_{\\gamma}$ (the ratio of its largest to smallest eigenvalue in the Euclidean norm). Starting from the characteristic polynomial of a $2\\times 2$ symmetric matrix, derive the exact expression for the minimizer $\\gamma^{\\star}(\\epsilon)$ over $\\gamma0$.\n\nYour final reported answer should be the closed-form expression for $\\gamma^{\\star}(\\epsilon)$ as an analytic function of $\\epsilon$. Do not include any units. Do not round your answer.",
            "solution": "### Part 1: Analysis of the Schur Complement $S$\n\nFirst, we derive the exact expression for the Schur complement $S$.\nThe matrix $A$ is given by $A = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix}$. Its inverse is $A^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1/\\epsilon \\end{pmatrix}$.\nThe matrix $B$ is $B = \\begin{pmatrix} 1  1 \\end{pmatrix}$, so its transpose is $B^T = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe term $B A^{-1} B^T$ is calculated as:\n$$\nB A^{-1} B^T = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{\\epsilon} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{1}{\\epsilon} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + \\frac{1}{\\epsilon} \\cdot 1 = 1 + \\frac{1}{\\epsilon}.\n$$\nNow, we add the term $C = -\\frac{1}{\\epsilon} + \\delta$ to find $S$:\n$$\nS = (B A^{-1} B^T) + C = \\left(1 + \\frac{1}{\\epsilon}\\right) + \\left(-\\frac{1}{\\epsilon} + \\delta\\right) = 1 + \\delta.\n$$\nThe exact value of $S$ is $1+\\delta$, which is an $\\mathcal{O}(1)$ quantity.\n\nThe numerical instability arises from the procedure of \"explicitly forming $A^{-1}$ and then evaluating $S$\". In floating-point arithmetic, when $\\epsilon$ is very small (e.g., smaller than the machine epsilon relative to $1$), the number $1/\\epsilon$ becomes very large.\nThe computation proceeds as follows:\n1. Form $B A^{-1} B^T = 1 + 1/\\epsilon$. In floating-point arithmetic, if $1/\\epsilon$ is sufficiently large, the addition $1 + 1/\\epsilon$ results in a value that is computationally indistinguishable from $1/\\epsilon$. Information about the term '$1$' is lost.\n2. Form $C = -1/\\epsilon + \\delta$.\n3. Compute $S = (B A^{-1} B^T) + C$. Numerically, this becomes $(1/\\epsilon) + (-1/\\epsilon + \\delta)$. This subtraction of two large, nearly equal numbers results in a computed value of $\\delta$.\nThe true result is $1+\\delta$, but the computed result is $\\delta$. This phenomenon, where the subtraction of large numbers annihilates the leading significant digits and reveals previously insignificant noise or error, is known as catastrophic cancellation. The relative error is $(\\text{true}-\\text{computed})/\\text{true} = ((1+\\delta)-\\delta)/(1+\\delta) = 1/(1+\\delta)$, which is an $\\mathcal{O}(1)$ relative error, indicating a complete loss of accuracy.\n\n### Part 2: Derivation of the Augmented Schur Complement $S_{\\gamma}$\n\nWe need to derive an expression for $S_{\\gamma} = B A_{\\gamma}^{-1} B^{T} + C$, where $A_{\\gamma} = A + \\gamma B^{T}B$. The matrix $A_{\\gamma}$ is a rank-one update of $A$. We can use the Sherman-Morrison formula for the inverse of a rank-one updated matrix:\n$$\n(M + uv^T)^{-1} = M^{-1} - \\frac{M^{-1}uv^T M^{-1}}{1+v^T M^{-1} u}.\n$$\nIn our case, $M=A$, $u=\\gamma B^T$, and $v^T=B$.\n$$\nA_{\\gamma}^{-1} = (A + \\gamma B^T B)^{-1} = A^{-1} - \\frac{A^{-1}(\\gamma B^T)B A^{-1}}{1+B(\\gamma A^{-1}) B^T} = A^{-1} - \\frac{\\gamma A^{-1} B^T B A^{-1}}{1+\\gamma B A^{-1} B^T}.\n$$\nNow, we pre-multiply by $B$ and post-multiply by $B^T$:\n$$\nB A_{\\gamma}^{-1} B^T = B A^{-1} B^T - \\frac{\\gamma B A^{-1} B^T B A^{-1} B^T}{1+\\gamma B A^{-1} B^T}.\n$$\nLet $X = B A^{-1} B^T$. The expression simplifies to:\n$$\nB A_{\\gamma}^{-1} B^T = X - \\frac{\\gamma X^2}{1+\\gamma X} = \\frac{X(1+\\gamma X) - \\gamma X^2}{1+\\gamma X} = \\frac{X + \\gamma X^2 - \\gamma X^2}{1+\\gamma X} = \\frac{X}{1+\\gamma X}.\n$$\nSubstituting $X = 1 + 1/\\epsilon$:\n$$\nB A_{\\gamma}^{-1} B^T = \\frac{1+1/\\epsilon}{1+\\gamma(1+1/\\epsilon)}.\n$$\nNow we compute $S_{\\gamma}$ by adding $C$:\n$$\nS_{\\gamma} = \\frac{1+1/\\epsilon}{1+\\gamma(1+1/\\epsilon)} + C = \\frac{1+1/\\epsilon}{1+\\gamma(1+1/\\epsilon)} - \\frac{1}{\\epsilon} + \\delta.\n$$\nTo obtain a rational function of $\\epsilon, \\gamma, \\delta$, we first rewrite the fraction by multiplying the numerator and denominator by $\\epsilon$:\n$$\nB A_{\\gamma}^{-1} B^T = \\frac{\\epsilon(1+1/\\epsilon)}{\\epsilon(1+\\gamma(1+1/\\epsilon))} = \\frac{\\epsilon+1}{\\epsilon+\\gamma(\\epsilon+1)} = \\frac{1+\\epsilon}{\\epsilon + \\gamma(1+\\epsilon)}.\n$$\nThen, we combine all terms for $S_{\\gamma}$:\n$$\nS_{\\gamma} = \\frac{1+\\epsilon}{\\epsilon + \\gamma(1+\\epsilon)} - \\frac{1}{\\epsilon} + \\delta = \\frac{\\epsilon(1+\\epsilon) - (\\epsilon + \\gamma(1+\\epsilon))}{\\epsilon(\\epsilon + \\gamma(1+\\epsilon))} + \\delta.\n$$\nThe numerator of the fraction is $\\epsilon + \\epsilon^2 - \\epsilon - \\gamma - \\gamma\\epsilon = \\epsilon^2 - \\gamma(1+\\epsilon)$.\nThe denominator is $\\epsilon^2 + \\gamma\\epsilon(1+\\epsilon)$.\nSo we have:\n$$\nS_{\\gamma} = \\frac{\\epsilon^2 - \\gamma(1+\\epsilon)}{\\epsilon^2 + \\gamma\\epsilon(1+\\epsilon)} + \\delta.\n$$\nFinally, combining $\\delta$ into the fraction gives the desired rational function:\n$$\nS_{\\gamma} = \\frac{\\epsilon^2 - \\gamma(1+\\epsilon) + \\delta(\\epsilon^2 + \\gamma\\epsilon(1+\\epsilon))}{\\epsilon^2 + \\gamma\\epsilon(1+\\epsilon)}.\n$$\nThis expression avoids explicit inversion of $A$ or intermediate calculation of large terms like $1/\\epsilon$.\n\n### Part 3: Minimization of the Condition Number of $A_{\\gamma}$\n\nFirst, we construct the matrix $A_{\\gamma}$:\n$$\nA_{\\gamma} = A + \\gamma B^T B = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} + \\gamma \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\epsilon \\end{pmatrix} + \\gamma \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1+\\gamma  \\gamma \\\\ \\gamma  \\epsilon+\\gamma \\end{pmatrix}.\n$$\nThe eigenvalues $\\lambda$ of $A_{\\gamma}$ are the roots of its characteristic polynomial, $\\det(A_{\\gamma} - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 1+\\gamma-\\lambda  \\gamma \\\\ \\gamma  \\epsilon+\\gamma-\\lambda \\end{pmatrix} = (1+\\gamma-\\lambda)(\\epsilon+\\gamma-\\lambda) - \\gamma^2 = 0.\n$$\nExpanding this gives:\n$$\n\\lambda^2 - (\\epsilon+\\gamma+1+\\gamma)\\lambda + (1+\\gamma)(\\epsilon+\\gamma) - \\gamma^2 = 0\n$$\n$$\n\\lambda^2 - (1+\\epsilon+2\\gamma)\\lambda + (\\epsilon+\\gamma+\\gamma\\epsilon) = 0.\n$$\nThe eigenvalues $\\lambda_{1,2}$ are given by the quadratic formula:\n$$\n\\lambda_{1,2} = \\frac{(1+\\epsilon+2\\gamma) \\pm \\sqrt{(1+\\epsilon+2\\gamma)^2 - 4(\\epsilon+\\gamma+\\gamma\\epsilon)}}{2}.\n$$\nLet's simplify the discriminant $\\Delta$:\n$$\n\\Delta = (1+\\epsilon)^2 + 4\\gamma(1+\\epsilon) + 4\\gamma^2 - 4\\epsilon - 4\\gamma - 4\\gamma\\epsilon\n$$\n$$\n\\Delta = 1+2\\epsilon+\\epsilon^2 + 4\\gamma+4\\gamma\\epsilon + 4\\gamma^2 - 4\\epsilon - 4\\gamma - 4\\gamma\\epsilon = 1-2\\epsilon+\\epsilon^2+4\\gamma^2 = (1-\\epsilon)^2+4\\gamma^2.\n$$\nSo the eigenvalues are $\\lambda_1 = \\frac{1+\\epsilon+2\\gamma + \\sqrt{(1-\\epsilon)^2+4\\gamma^2}}{2}$ and $\\lambda_2 = \\frac{1+\\epsilon+2\\gamma - \\sqrt{(1-\\epsilon)^2+4\\gamma^2}}{2}$.\nThe spectral condition number is $\\kappa_2(A_{\\gamma}) = \\lambda_1/\\lambda_2$.\n$$\n\\kappa_2(\\gamma) = \\frac{1+\\epsilon+2\\gamma + \\sqrt{(1-\\epsilon)^2+4\\gamma^2}}{1+\\epsilon+2\\gamma - \\sqrt{(1-\\epsilon)^2+4\\gamma^2}}.\n$$\nTo minimize $\\kappa_2(\\gamma)$ with respect to $\\gamma  0$, we can set its derivative to zero. It is equivalent and simpler to minimize $\\ln(\\kappa_2(\\gamma))$. Let $u(\\gamma) = 1+\\epsilon+2\\gamma$ and $v(\\gamma) = \\sqrt{(1-\\epsilon)^2+4\\gamma^2}$.\nThen $\\ln(\\kappa_2(\\gamma)) = \\ln(u+v) - \\ln(u-v)$.\n$$\n\\frac{d}{d\\gamma} \\ln(\\kappa_2(\\gamma)) = \\frac{u'+v'}{u+v} - \\frac{u'-v'}{u-v} = \\frac{(u'+v')(u-v) - (u'-v')(u+v)}{u^2-v^2} = \\frac{2(v'u - u'v)}{u^2-v^2}.\n$$\nThe derivative is zero when $v'u - u'v = 0$.\nWe have $u'(\\gamma) = 2$ and $v'(\\gamma) = \\frac{8\\gamma}{2\\sqrt{(1-\\epsilon)^2+4\\gamma^2}} = \\frac{4\\gamma}{v}$.\nThe condition becomes:\n$$\n\\frac{4\\gamma}{v}u - 2v = 0 \\implies 4\\gamma u - 2v^2 = 0 \\implies 2\\gamma u = v^2.\n$$\nSubstituting the expressions for $u$ and $v^2$:\n$$\n2\\gamma(1+\\epsilon+2\\gamma) = (1-\\epsilon)^2+4\\gamma^2.\n$$\n$$\n2\\gamma(1+\\epsilon) + 4\\gamma^2 = (1-\\epsilon)^2 + 4\\gamma^2.\n$$\n$$\n2\\gamma(1+\\epsilon) = (1-\\epsilon)^2.\n$$\nSolving for $\\gamma$ gives the optimal value $\\gamma^{\\star}(\\epsilon)$:\n$$\n\\gamma^{\\star}(\\epsilon) = \\frac{(1-\\epsilon)^2}{2(1+\\epsilon)}.\n$$\nSince $\\epsilon \\in (0,1)$, we have $1-\\epsilon  0$ and $1+\\epsilon  0$, so $\\gamma^{\\star}(\\epsilon)  0$ as required. This value minimizes the condition number of the regularized block $A_{\\gamma}$.",
            "answer": "$$\\boxed{\\frac{(1-\\epsilon)^2}{2(1+\\epsilon)}}$$"
        }
    ]
}