{
    "hands_on_practices": [
        {
            "introduction": "This problem serves as a foundational exercise in understanding the core mechanism of contour-integral eigensolvers. We begin by analytically deriving the explicit form of the rational filter that arises from a simple circular contour and the trapezoidal quadrature rule. This practice is crucial for building intuition about how the filter separates desired from undesired eigenvalues and how this separation, quantified by the distance to the contour, directly impacts the convergence rate of the algorithm .",
            "id": "3541067",
            "problem": "Consider a real symmetric (Hermitian) matrix $A \\in \\mathbb{R}^{n \\times n}$ with real spectrum, and suppose the goal is to compute all eigenpairs whose eigenvalues lie in the real interval $\\left[-r, r\\right]$ with $r \\in (0,1)$. A standard contour-integral eigensolver based on the spectral projector approximates\n$$\nP = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (zI - A)^{-1} \\, dz,\n$$\nwhere $\\Gamma$ is the unit circle centered at the origin in the complex plane, oriented counterclockwise, and the integral is approximated by the $M$-point trapezoidal rule over the parametrization $z(\\theta)=\\exp(i\\theta)$ for $\\theta \\in [0,2\\pi)$. The resulting rational filter $R_M(A)$ is applied as a subspace iteration that maps a subspace $V$ to $R_M(A)V$. In this setting, the asymptotic contraction factor of components associated with unwanted eigenvalues (those outside $\\Gamma$) is bounded above by the ratio of the maximal scalar filter magnitude attained on unwanted eigenvalues to the minimal scalar filter magnitude attained on desired eigenvalues.\n\nAssume all desired eigenvalues lie within $\\left[-r, r\\right]$ and all unwanted eigenvalues lie outside the unit circle, with the closest unwanted eigenvalue $\\lambda_{\\star} = 1 + s$ for some $s>0$. Working from the definitions above, the unit-circle parametrization, and the $M$-point trapezoidal rule, derive the explicit scalar rational filter induced by the quadrature, use it to bound the contraction factor as a function of $s$, $M$, and $r$, and then determine the minimal separation $s_{\\min}$ that guarantees a prescribed contraction factor $q \\in (0,1)$ per iteration, i.e., ensures that the ratio of the maximal magnitude of the filter on unwanted eigenvalues to the minimal magnitude on desired eigenvalues is at most $q$. Give your final answer as a single closed-form analytic expression for $s_{\\min}$ in terms of $M$, $r$, and $q$. No rounding is required.",
            "solution": "The problem asks for the minimal separation $s_{\\min}$ of unwanted eigenvalues from the unit circle to guarantee a specific contraction rate for a contour-integral eigensolver. The derivation proceeds in three main steps: first, we determine the explicit form of the scalar rational filter $R_M(\\lambda)$ induced by the numerical quadrature; second, we establish bounds on the magnitude of this filter for desired and unwanted eigenvalues; and third, we use these bounds to solve for the required separation $s$.\n\n**Step 1: Derivation of the Scalar Rational Filter**\n\nThe spectral projector $P$ for the eigenvalue problem of a matrix $A$ is given by the contour integral\n$$\nP = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (zI - A)^{-1} \\, dz\n$$\nwhere $\\Gamma$ is a contour enclosing the desired eigenvalues. The action of this projector on an eigenvector corresponding to an eigenvalue $\\lambda$ is governed by the scalar function\n$$\np(\\lambda) = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (z - \\lambda)^{-1} \\, dz\n$$\nThe problem specifies that $\\Gamma$ is the unit circle, parametrized by $z(\\theta) = \\exp(i\\theta)$ for $\\theta \\in [0, 2\\pi)$. The differential is $dz = i\\exp(i\\theta)d\\theta$. Substituting this into the integral for $p(\\lambda)$ gives\n$$\np(\\lambda) = \\frac{1}{2\\pi i} \\int_{0}^{2\\pi} \\frac{1}{\\exp(i\\theta) - \\lambda} i\\exp(i\\theta) d\\theta = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\frac{\\exp(i\\theta)}{\\exp(i\\theta) - \\lambda} d\\theta\n$$\nThe problem states that this integral is approximated using the $M$-point trapezoidal rule. The interval is $[0, 2\\pi)$ and the step size is $h = \\frac{2\\pi}{M}$. The quadrature points are $\\theta_k = \\frac{2\\pi k}{M}$ for $k \\in \\{0, 1, \\dots, M-1\\}$. The corresponding points on the unit circle are $z_k = \\exp(i\\theta_k) = \\exp(i \\frac{2\\pi k}{M})$, which are the $M$-th roots of unity.\n\nThe trapezoidal rule approximation, which defines the scalar rational filter $R_M(\\lambda)$, is given by\n$$\nR_M(\\lambda) = \\frac{h}{2\\pi} \\sum_{k=0}^{M-1} \\frac{z_k}{z_k - \\lambda} = \\frac{1}{M} \\sum_{k=0}^{M-1} \\frac{z_k}{z_k - \\lambda}\n$$\nWe can rewrite the term in the sum as $\\frac{z_k}{z_k - \\lambda} = 1 + \\frac{\\lambda}{z_k - \\lambda}$. This gives\n$$\nR_M(\\lambda) = \\frac{1}{M} \\sum_{k=0}^{M-1} \\left(1 + \\frac{\\lambda}{z_k - \\lambda}\\right) = 1 + \\frac{\\lambda}{M} \\sum_{k=0}^{M-1} \\frac{1}{z_k - \\lambda}\n$$\nThe sum can be evaluated using the partial fraction expansion of a related rational function. Consider the polynomial $p(z) = z^M - 1$, whose roots are the points $z_k$. Its logarithmic derivative is\n$$\n\\frac{p'(z)}{p(z)} = \\frac{Mz^{M-1}}{z^M - 1} = \\sum_{k=0}^{M-1} \\frac{1}{z - z_k}\n$$\nEvaluating this at $z=\\lambda$ and rearranging signs yields\n$$\n\\sum_{k=0}^{M-1} \\frac{1}{z_k - \\lambda} = - \\sum_{k=0}^{M-1} \\frac{1}{\\lambda - z_k} = - \\frac{M\\lambda^{M-1}}{\\lambda^M - 1}\n$$\nSubstituting this back into the expression for $R_M(\\lambda)$:\n$$\nR_M(\\lambda) = 1 + \\frac{\\lambda}{M} \\left( - \\frac{M\\lambda^{M-1}}{\\lambda^M - 1} \\right) = 1 - \\frac{\\lambda^M}{\\lambda^M - 1} = \\frac{(\\lambda^M - 1) - \\lambda^M}{\\lambda^M - 1} = \\frac{-1}{\\lambda^M - 1}\n$$\nThus, the explicit scalar rational filter is\n$$\nR_M(\\lambda) = \\frac{1}{1 - \\lambda^M}\n$$\n\n**Step 2: Bounding the Contraction Factor**\n\nThe asymptotic contraction factor is bounded by the ratio of the maximal filter magnitude on unwanted eigenvalues to the minimal filter magnitude on desired eigenvalues. Let this bound be $\\eta$.\n$$\n\\eta \\le \\frac{\\sup_{\\lambda_{\\text{out}}} |R_M(\\lambda_{\\text{out}})|}{\\inf_{\\lambda_{\\text{in}}} |R_M(\\lambda_{\\text{in}})|}\n$$\nThe eigenvalues $\\lambda$ are real. Desired eigenvalues $\\lambda_{\\text{in}}$ are in $[-r, r]$ where $r \\in (0,1)$. Unwanted eigenvalues $\\lambda_{\\text{out}}$ are outside the unit circle, with the closest one being at a distance $s>0$ from it, i.e., $|\\lambda_{\\text{out}}| \\ge 1+s$.\n\nFirst, we analyze the denominator, which is the minimal filter magnitude on the desired eigenvalues:\n$$\n\\inf_{\\lambda_{\\text{in}} \\in [-r, r]} |R_M(\\lambda_{\\text{in}})| = \\inf_{\\lambda \\in [-r, r]} \\left| \\frac{1}{1 - \\lambda^M} \\right| = \\frac{1}{\\sup_{\\lambda \\in [-r, r]} |1 - \\lambda^M|}\n$$\nFor any real $\\lambda$ with $|\\lambda| \\le r$, we have $|\\lambda^M| \\le r^M$. By the triangle inequality, $|1 - \\lambda^M| \\le 1 + |\\lambda^M| \\le 1 + r^M$. This upper bound is achieved if $M$ is odd at $\\lambda=-r$. If $M$ is even, the maximum is $1$. To have a bound that is valid for any $M$, we take the worst-case (largest) value for the denominator of the filter, which is $1+r^M$. Thus, we establish a lower bound for the filter magnitude on desired eigenvalues:\n$$\n\\inf_{\\lambda_{\\text{in}}} |R_M(\\lambda_{\\text{in}})| \\ge \\frac{1}{1 + r^M}\n$$\nNext, we analyze the numerator, which is the maximal filter magnitude on the unwanted eigenvalues:\n$$\n\\sup_{\\lambda_{\\text{out}}} |R_M(\\lambda_{\\text{out}})| = \\sup_{|\\lambda| \\ge 1+s, \\lambda \\in \\mathbb{R}} \\left| \\frac{1}{1 - \\lambda^M} \\right| = \\frac{1}{\\inf_{|\\lambda| \\ge 1+s, \\lambda \\in \\mathbb{R}} |1 - \\lambda^M|}\n$$\nWe need to find the minimum of $|1 - \\lambda^M|$ for real $\\lambda$ such that $\\lambda \\ge 1+s$ or $\\lambda \\le -(1+s)$.\n- If $\\lambda \\ge 1+s$, then $\\lambda^M \\ge (1+s)^M > 1$. The function $|1 - \\lambda^M| = \\lambda^M - 1$ is monotonically increasing. Its minimum on this domain is at $\\lambda = 1+s$, with value $(1+s)^M - 1$.\n- If $\\lambda \\le -(1+s)$, let $\\lambda = -x$ where $x \\ge 1+s$. The function is $|1 - (-x)^M|$.\n  - If $M$ is even: $|1 - x^M| = x^M - 1$. Minimum at $x=1+s$, value is $(1+s)^M - 1$.\n  - If $M$ is odd: $|1 + x^M| = 1 + x^M$. Minimum at $x=1+s$, value is $1 + (1+s)^M$.\nThe overall minimum of $|1 - \\lambda^M|$ over the entire domain of unwanted eigenvalues is $\\min((1+s)^M - 1, 1 + (1+s)^M) = (1+s)^M - 1$.\nTherefore, the maximal filter magnitude for unwanted eigenvalues is\n$$\n\\sup_{\\lambda_{\\text{out}}} |R_M(\\lambda_{\\text{out}})| = \\frac{1}{(1+s)^M - 1}\n$$\nCombining these results, we get the bound on the contraction factor:\n$$\n\\eta \\le \\frac{1/((1+s)^M - 1)}{1/(1+r^M)} = \\frac{1+r^M}{(1+s)^M - 1}\n$$\n\n**Step 3: Determining the Minimal Separation $s_{\\min}$**\n\nWe are given that the desired contraction factor is at most $q$, where $q \\in (0,1)$. We set our bound to be less than or equal to $q$ and solve for $s$.\n$$\n\\frac{1+r^M}{(1+s)^M - 1} \\le q\n$$\nSince both $q$ and $(1+s)^M - 1$ are positive, we can rearrange the inequality:\n$$\n(1+s)^M - 1 \\ge \\frac{1+r^M}{q}\n$$\n$$\n(1+s)^M \\ge 1 + \\frac{1+r^M}{q}\n$$\nTaking the $M$-th root of both sides (which is a monotonic operation for positive bases):\n$$\n1+s \\ge \\left(1 + \\frac{1+r^M}{q}\\right)^{1/M}\n$$\n$$\ns \\ge \\left(1 + \\frac{1+r^M}{q}\\right)^{1/M} - 1\n$$\nThis inequality gives the condition on $s$ to ensure the desired contraction rate. The minimal separation, $s_{\\min}$, is the smallest value of $s$ that satisfies this condition.\n$$\ns_{\\min} = \\left(1 + \\frac{1+r^M}{q}\\right)^{1/M} - 1\n$$\nThis is the required closed-form analytic expression for $s_{\\min}$ in terms of $M$, $r$, and $q$.",
            "answer": "$$\n\\boxed{\\left(1 + \\frac{1+r^M}{q}\\right)^{1/M} - 1}\n$$"
        },
        {
            "introduction": "Building on the analytical foundation, this practice moves into a more realistic computational scenario where the choice of contour is a key design parameter. You will implement a numerical experiment to investigate the trade-offs between an elliptical contour's shape, the conditioning of the linear systems that must be solved, and the overall filter effectiveness. This exercise highlights the practical challenges and design considerations in applying contour-integral methods to a given spectral distribution .",
            "id": "3541079",
            "problem": "Consider the Hermitian (and hence normal) matrix $A \\in \\mathbb{C}^{n \\times n}$ with spectrum $\\{\\lambda_i\\}_{i=1}^n$ explicitly given by the diagonal entries\n$$\n\\operatorname{diag}(A) = \\left[-1.5,\\,-1.2,\\,-0.9,\\,-0.8,\\,-0.3,\\,-0.1,\\,0.0,\\,0.15,\\,0.25,\\,0.8,\\,1.2,\\,1.5\\right].\n$$\nLet $\\Gamma$ be an ellipse in the complex plane centered at $0$ with semi-axes $a$ (along the real axis) and $b$ (along the imaginary axis), and let its parametrization be\n$$\nz(\\theta) = a\\cos\\theta + \\mathrm{i}\\,b\\sin\\theta,\\quad \\theta \\in [0,2\\pi).\n$$\nThe Filter Diagonalization via Contour Integration (FEAST) algorithm is a contour-integral eigensolver that applies a rational filter approximating the spectral projector onto the invariant subspace associated with eigenvalues in the interior of $\\Gamma$. The quality of the filter, and hence the per-iteration convergence of FEAST, depends on the contour geometry and the spectral gap from the spectrum to the contour. The shifted linear systems $(zI - A)x = y$ must be solved for $z \\in \\Gamma$, and their numerical conditioning impacts stability and computational cost. This problem asks you to quantify, for a fixed trapezoidal quadrature on $\\Gamma$, how the ellipse eccentricity and the spectral gap to $\\Gamma$ affect:\n- the worst-case $2$-norm condition number of $(zI-A)$ along $\\Gamma$, and\n- an a priori FEAST convergence-rate proxy derived from the rational filter attenuation between eigenvalues inside and outside $\\Gamma$.\n\nUse the following foundational facts:\n- For a normal matrix $A$, the singular values of $(zI-A)$ are the distances from $z$ to the eigenvalues $\\{\\lambda_i\\}$, so the $2$-norm condition number is\n$$\n\\kappa_2(zI-A) = \\frac{\\max_i |z-\\lambda_i|}{\\min_i |z-\\lambda_i|}.\n$$\n- The spectral projector onto eigenvalues inside $\\Gamma$ is\n$$\nP = \\frac{1}{2\\pi \\mathrm{i}}\\oint_{\\Gamma} (zI-A)^{-1}\\,\\mathrm{d}z,\n$$\nand uniform trapezoidal quadrature in the angle parameter $\\theta$ applied to the analytic periodic integrand yields an exponentially accurate rational approximation when the number of nodes is sufficiently large.\n\nYour tasks:\n1. Implement an approximation of the spectral projector using the uniform trapezoidal rule with $m$ nodes in the angle parameter $\\theta$, with $\\theta_k = 2\\pi k/m$ for $k \\in \\{0,1,\\dots,m-1\\}$. Use the parametrization $z(\\theta)$ given above, and take the corresponding quadrature weights to be the product of the step size in $\\theta$ and the Jacobian $\\mathrm{d}z/\\mathrm{d}\\theta$, scaled by $1/(2\\pi \\mathrm{i})$. From this, define the scalar rational filter response at a real point $\\lambda$ as the quadrature approximation to the Cauchy integral of $(z-\\lambda)^{-1}$ over $\\Gamma$. You must compute the complex filter values $r(\\lambda)$ for all spectral points $\\lambda \\in \\{\\lambda_i\\}$.\n2. Define the interior spectral set for a given ellipse as the eigenvalues $\\lambda$ satisfying $(\\lambda/a)^2  1$ (note that all eigenvalues are real). Define the exterior spectral set as the complement with respect to $\\{\\lambda_i\\}$.\n3. Define the eccentricity of the ellipse as\n$$\ne = \\sqrt{1 - \\frac{b^2}{a^2}},\n$$\nand define the spectral gap to the contour for the exterior set as\n$$\n\\delta_{\\text{out}} = \\min_{\\lambda \\in \\text{exterior}} \\min_{\\theta \\in [0,2\\pi)} |z(\\theta) - \\lambda|.\n$$\nApproximate $\\delta_{\\text{out}}$ by sampling the same quadrature nodes used for the filter.\n4. Compute the worst-case conditioning along the contour as\n$$\n\\kappa_{\\max} = \\max_{k \\in \\{0,\\dots,m-1\\}} \\kappa_2\\big(z(\\theta_k)I - A\\big),\n$$\nusing the normality fact above.\n5. Let $r(\\lambda)$ denote the rational filter evaluated at $\\lambda$. Define an a priori per-iteration FEAST convergence-rate proxy as\n$$\n\\gamma = \\frac{\\max_{\\lambda \\in \\text{exterior}} |r(\\lambda)|}{\\min_{\\lambda \\in \\text{interior}} |r(\\lambda)|}.\n$$\nThis quantity captures the attenuation ratio between the exterior and interior spectral components.\n\nUse $m=64$ nodes for all computations. For numerical robustness, use the exact diagonal of $A$ given above without perturbation.\n\nTest suite:\nEvaluate the tuple $(e,\\delta_{\\text{out}},\\kappa_{\\max},\\gamma)$ for the following ellipses $(a,b)$:\n- Test $1$: $(a,b) = (0.5,\\,0.49)$,\n- Test $2$: $(a,b) = (0.6,\\,0.59)$,\n- Test $3$: $(a,b) = (0.6,\\,0.20)$,\n- Test $4$: $(a,b) = (0.7,\\,0.20)$,\n- Test $5$: $(a,b) = (0.78,\\,0.20)$.\n\nNotes and requirements:\n- All angles are in radians.\n- All outputs are dimensionless real numbers.\n- Your program must produce a single line of output containing a list of the five results, where each result is itself a list of four floating-point numbers in the order $[e, \\delta_{\\text{out}}, \\kappa_{\\max}, \\gamma]$. For example, the required overall format is\n$$\n\\big[\\,[e_1,\\delta_1,\\kappa_1,\\gamma_1],\\,[e_2,\\delta_2,\\kappa_2,\\gamma_2],\\,\\dots,\\,[e_5,\\delta_5,\\kappa_5,\\gamma_5]\\,\\big].\n$$\nNo additional text should be printed.",
            "solution": "The problem statement is valid. It is a well-posed, scientifically grounded, and objective problem in the domain of numerical linear algebra, specifically concerning the analysis of contour-integral eigensolvers like the FEAST algorithm. All data, definitions, and constants required for the computation are provided, and there are no internal contradictions, ambiguities, or violations of scientific principles. The problem asks to compute specific metrics that characterize the performance and stability of the FEAST algorithm for a given matrix and a set of elliptical contours.\n\nThe solution proceeds by implementing the specified calculations for each test case. The Hermitian matrix $A \\in \\mathbb{C}^{n \\times n}$ is diagonal, with its spectrum $\\{\\lambda_i\\}_{i=1}^n$ given as the real-valued entries:\n$$\n\\{\\lambda_i\\} = \\{-1.5, -1.2, -0.9, -0.8, -0.3, -0.1, 0.0, 0.15, 0.25, 0.8, 1.2, 1.5\\}.\n$$\nThe complex contour $\\Gamma$ is an ellipse centered at the origin with semi-axes $a$ and $b$, parametrized by $z(\\theta) = a\\cos\\theta + \\mathrm{i}b\\sin\\theta$ for $\\theta \\in [0, 2\\pi)$. All computations will use a uniform trapezoidal quadrature rule with $m=64$ nodes, $\\theta_k = 2\\pi k/m$ for $k \\in \\{0, 1, \\dots, m-1\\}$.\n\nFor each test case defined by a pair of semi-axes $(a,b)$, we compute a tuple of four values $(e, \\delta_{\\text{out}}, \\kappa_{\\max}, \\gamma)$. The computational steps are as follows:\n\n1.  **Eccentricity, $e$**: This value quantifies the shape of the ellipse. It is computed directly from its definition:\n    $$\n    e = \\sqrt{1 - \\frac{b^2}{a^2}}.\n    $$\n\n2.  **Spectral Sets**: The spectrum $\\{\\lambda_i\\}$ is partitioned into two sets based on the semi-axis $a$. The interior set contains eigenvalues $\\lambda_i$ such that $|\\lambda_i|  a$, and the exterior set contains eigenvalues $\\lambda_i$ such that $|\\lambda_i| \\ge a$. These sets are essential for defining the spectral gap and the convergence proxy.\n\n3.  **Quadrature Nodes and Derivatives**: The $m=64$ quadrature points $z_k = z(\\theta_k)$ on the contour and the values of the derivative $z'(\\theta_k) = \\frac{\\mathrm{d}z}{\\mathrm{d}\\theta}\\big|_{\\theta_k}$ are pre-computed for use in subsequent steps.\n    $$\n    z_k = a\\cos(\\theta_k) + \\mathrm{i}b\\sin(\\theta_k)\n    $$\n    $$\n    z'(\\theta_k) = -a\\sin(\\theta_k) + \\mathrm{i}b\\cos(\\theta_k)\n    $$\n\n4.  **Spectral Gap to Contour, $\\delta_{\\text{out}}$**: This metric measures the minimum distance from the contour $\\Gamma$ to any eigenvalue in the exterior set. A larger gap is generally favorable. It is approximated by finding the minimum distance from any quadrature node $z_k$ to any exterior eigenvalue $\\lambda_{\\text{ext}}$:\n    $$\n    \\delta_{\\text{out}} = \\min_{\\lambda \\in \\text{exterior}} \\min_{k \\in \\{0,\\dots,m-1\\}} |z_k - \\lambda|.\n    $$\n\n5.  **Worst-Case Conditioning, $\\kappa_{\\max}$**: This metric quantifies the numerical stability of the linear systems that must be solved in the FEAST algorithm. For a normal matrix $A$, the $2$-norm condition number of $(zI-A)$ is the ratio of the largest to smallest distance from $z$ to the spectrum. We compute the maximum of this value over all quadrature nodes $z_k$:\n    $$\n    \\kappa_{\\max} = \\max_{k \\in \\{0,\\dots,m-1\\}} \\kappa_2(z_kI - A) = \\max_{k \\in \\{0,\\dots,m-1\\}} \\frac{\\max_i |z_k - \\lambda_i|}{\\min_i |z_k - \\lambda_i|}.\n    $$\n\n6.  **Rational Filter Response, $r(\\lambda)$, and Convergence Proxy, $\\gamma$**: The rational filter $r(\\lambda)$ is the numerical approximation of the Cauchy integral $\\frac{1}{2\\pi \\mathrm{i}} \\oint_\\Gamma (z-\\lambda)^{-1}\\,\\mathrm{d}z$, which resolves to $1$ for $\\lambda$ inside $\\Gamma$ and $0$ for $\\lambda$ outside. Using the trapezoidal rule on the parametrized integral, its value is:\n    $$\n    r(\\lambda) = \\frac{1}{2\\pi \\mathrm{i}} \\sum_{k=0}^{m-1} \\frac{1}{z_k - \\lambda} z'(\\theta_k) \\frac{2\\pi}{m} = \\frac{1}{m\\mathrm{i}} \\sum_{k=0}^{m-1} \\frac{z'(\\theta_k)}{z_k - \\lambda}.\n    $$\n    The convergence-rate proxy, $\\gamma$, is the ratio of the maximum filter response for exterior eigenvalues to the minimum filter response for interior eigenvalues. A smaller $\\gamma$ indicates better filtering and faster convergence.\n    $$\n    \\gamma = \\frac{\\max_{\\lambda \\in \\text{exterior}} |r(\\lambda)|}{\\min_{\\lambda \\in \\text{interior}} |r(\\lambda)|}.\n    $$\n\nThese steps are systematically applied to each of the five test cases, and the resulting four-element tuples are collected. The final implementation is encapsulated in a Python script.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes FEAST algorithm performance metrics for a series of elliptical contours.\n    \"\"\"\n\n    # Given spectrum of the diagonal Hermitian matrix A\n    LAMBDAS = np.array([\n        -1.5, -1.2, -0.9, -0.8, -0.3, -0.1, 0.0, 0.15, 0.25, 0.8, 1.2, 1.5\n    ])\n    M = 64  # Number of quadrature nodes\n\n    def calculate_metrics(a, b):\n        \"\"\"\n        Calculates the four required metrics for a given ellipse (a, b).\n        \n        Args:\n            a (float): Semi-axis of the ellipse along the real axis.\n            b (float): Semi-axis of the ellipse along the imaginary axis.\n\n        Returns:\n            list: A list containing [e, delta_out, kappa_max, gamma].\n        \"\"\"\n        \n        # 1. Eccentricity\n        e = np.sqrt(1 - (b/a)**2)\n\n        # 2. Setup quadrature nodes and derivatives\n        thetas = 2 * np.pi * np.arange(M) / M\n        z_nodes = a * np.cos(thetas) + 1j * b * np.sin(thetas)\n        dz_dtheta_nodes = -a * np.sin(thetas) + 1j * b * np.cos(thetas)\n\n        # 3. Partition spectrum and calculate spectral gap delta_out\n        interior_mask = np.abs(LAMBDAS)  a\n        exterior_mask = ~interior_mask\n        \n        interior_lambdas = LAMBDAS[interior_mask]\n        exterior_lambdas = LAMBDAS[exterior_mask]\n\n        # Calculate distances from exterior eigenvalues to contour nodes\n        # Broadcasting: (num_ext_lambdas, 1) - (M,) - (num_ext_lambdas, M)\n        dist_matrix_ext = np.abs(exterior_lambdas[:, np.newaxis] - z_nodes)\n        \n        if dist_matrix_ext.size  0:\n            delta_out = np.min(dist_matrix_ext)\n        else:\n            delta_out = np.inf # Should not happen with given test cases\n\n        # 4. Worst-case conditioning kappa_max\n        # Broadcasting: (M, 1) - (num_lambdas,) - (M, num_lambdas)\n        dist_matrix_all = np.abs(z_nodes[:, np.newaxis] - LAMBDAS)\n        \n        max_dists_per_zk = np.max(dist_matrix_all, axis=1)\n        min_dists_per_zk = np.min(dist_matrix_all, axis=1)\n        \n        # Avoid division by zero if a node lands on an eigenvalue (unlikely)\n        kappas = np.divide(max_dists_per_zk, min_dists_per_zk, \n                           out=np.full_like(max_dists_per_zk, np.inf), \n                           where=min_dists_per_zk!=0)\n        kappa_max = np.max(kappas)\n\n        # 5. Filter response r(lambda) and convergence proxy gamma\n        # Broadcasting: (M,) / ((num_lambdas, 1) - (M,)) - (num_lambdas, M)\n        # Note the expression z-lambda in the denominator\n        integrand = dz_dtheta_nodes / (z_nodes - LAMBDAS[:, np.newaxis])\n        r_values = (1 / (M * 1j)) * np.sum(integrand, axis=1)\n        \n        r_interior = r_values[interior_mask]\n        r_exterior = r_values[exterior_mask]\n\n        if r_interior.size == 0 or r_exterior.size == 0:\n            gamma = np.inf # Should not happen\n        else:\n            max_r_ext = np.max(np.abs(r_exterior))\n            min_r_int = np.min(np.abs(r_interior))\n            gamma = max_r_ext / min_r_int if min_r_int != 0 else np.inf\n\n        return [e, delta_out, kappa_max, gamma]\n\n    test_cases = [\n        (0.5, 0.49),\n        (0.6, 0.59),\n        (0.6, 0.20),\n        (0.7, 0.20),\n        (0.78, 0.20),\n    ]\n\n    results = []\n    for a_val, b_val in test_cases:\n        result = calculate_metrics(a_val, b_val)\n        results.append(result)\n\n    # Format the final output string to match the problem specification\n    # e.g., [[e1,d1,k1,g1],[e2,d2,k2,g2],...] with no spaces\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "In large-scale applications, the linear systems within each step of a contour-integral eigensolver are typically solved iteratively and thus inexactly. This advanced practice explores the critical interplay between the accuracy of these inner solves and the convergence of the outer subspace iteration. By deriving and testing a sufficient condition for convergence, you will gain insight into how to set solver tolerances to guarantee the overall effectiveness of the FEAST algorithm while managing computational cost .",
            "id": "3541107",
            "problem": "Consider a Hermitian matrix eigenvalue problem for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, and the Filtered Eigensolver by Approximate Spectral Transformation (FEAST) algorithm based on contour integrals. The exact spectral projector onto the invariant subspace associated with eigenvalues inside a closed contour $\\Gamma$ in the complex plane is\n$$\nP \\;=\\; \\frac{1}{2\\pi i}\\int_{\\Gamma} (zI - A)^{-1}\\,dz.\n$$\nIn practice, a quadrature rule with nodes $z_k \\in \\mathbb{C}$ and weights $w_k \\in \\mathbb{C}$ approximates the projector by the rational filter\n$$\nR(A) \\;=\\; \\sum_{k=1}^{m} w_k (z_k I - A)^{-1}.\n$$\nOne FEAST outer iteration applies $R(A)$ to a current subspace basis and then performs Rayleighâ€“Ritz to update the approximate invariant subspace. When the inner linear systems $(z_k I - A) X_k = Y$ are solved inexactly, the computed operator becomes\n$$\n\\widetilde{R}(A) \\;=\\; \\sum_{k=1}^{m} w_k \\left((z_k I - A)^{-1} + E_k\\right),\n$$\nwhere $E_k$ are linear solve error operators induced by nonzero residuals $R_k = Y - (z_k I - A)\\widetilde{X}_k$.\n\nStarting from the fundamental definitions above, and the standard subspace iteration framework, derive a sufficient condition that links the inner solve residual levels to a contraction of the error in the computed FEAST subspace. Your derivation must:\n- Begin from the basic properties of the spectral projector, the quadrature-based rational filter, and the subspace iteration view of FEAST.\n- Introduce the scalar filter $R(\\lambda)$ induced by $R(A)$ on an eigenvalue $\\lambda$ of $A$, and define quantities\n$$\n\\alpha_{\\mathrm{in}} \\;=\\; \\min_{\\lambda \\in \\Lambda_{\\mathrm{in}}} |R(\\lambda)|,\\quad\n\\beta_{\\mathrm{out}} \\;=\\; \\max_{\\lambda \\in \\Lambda_{\\mathrm{out}}} |R(\\lambda)|,\n$$\nwhere $\\Lambda_{\\mathrm{in}}$ and $\\Lambda_{\\mathrm{out}}$ are the sets of eigenvalues of $A$ strictly inside and strictly outside $\\Gamma$, respectively.\n- Bound the operator norm of the perturbation $\\Delta R \\equiv \\widetilde{R}(A)-R(A)$ in terms of the linear system residual levels, showing how a uniform tolerance schedule $0 \\le \\tau_k \\le \\tau$ for the inner residuals can be chosen so that the outer iteration remains a contraction in the subspace error.\n\nThen, design and implement a program that evaluates both your sufficient condition prediction and the actually observed contraction in a concrete instance, as follows.\n\nSet up the following data:\n- Matrix size $n = 40$. Let $A$ be diagonal with entries linearly spaced in the interval $[-1.5, 1.5]$.\n- Target region $\\Gamma$ is a circle centered at $c = 0$ with radius $r = 0.45$. Parameterize $\\Gamma$ by $z(\\theta) = c + r e^{i\\theta}$ for $\\theta \\in [0, 2\\pi]$ and approximate the contour integral with an $m$-point trapezoidal rule. Use quadrature nodes $z_k = c + r e^{i \\theta_k}$ with $\\theta_k = 2\\pi(k-1)/m$ and weights $w_k = \\frac{r}{m} e^{i \\theta_k}$.\n- Define the scalar filter on a real argument $\\lambda$ by $R(\\lambda) = \\sum_{k=1}^{m} \\frac{w_k}{z_k - \\lambda}$.\n- Let $\\Lambda_{\\mathrm{in}} = \\{\\lambda: |\\lambda - c| lt; r\\}$ and $\\Lambda_{\\mathrm{out}} = \\{\\lambda: |\\lambda - c| \\ge r\\}$, where the $\\lambda$ are the eigenvalues of $A$.\n- Define the bound\n$$\nS \\;=\\; \\sum_{k=1}^m |w_k| \\,\\bigl\\|(z_k I - A)^{-1}\\bigr\\|_2,\n$$\nand evaluate $\\bigl\\|(z_k I - A)^{-1}\\bigr\\|_2 = 1/\\min_{\\lambda \\in \\mathrm{spec}(A)} |z_k - \\lambda|$ for the Hermitian case.\n- Define the sufficient-condition predicted contraction factor for a uniform residual tolerance $\\tau$ as a quantity of the form\n$$\n\\rho_{\\mathrm{pred}} \\;=\\; \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} \\;+\\; \\frac{S\\,\\tau}{\\alpha_{\\mathrm{in}}}.\n$$\n- Implement one FEAST outer step with inexact inner solves enforced as follows: for each quadrature node $z_k$, instead of the exact solution $X_k^\\ast = (z_k I - A)^{-1}Y$, use the approximation $\\widetilde{X}_k = (1-\\tau) X_k^\\ast$, which yields a residual $R_k = \\tau Y$ with columnwise relative $2$-norm equal to $\\tau$; assemble $\\widetilde{R}(A)Y = \\sum_{k=1}^{m} w_k \\widetilde{X}_k$; orthonormalize the result to get the updated subspace.\n- Measure the largest principal angle between the target invariant subspace (spanned by the eigenvectors of $A$ with eigenvalues in $\\Lambda_{\\mathrm{in}}$) and the iterate subspace before and after one outer step; report the observed contraction ratio\n$$\n\\rho_{\\mathrm{obs}} \\;=\\; \\frac{\\sin(\\theta_{\\max}^{\\mathrm{new}})}{\\sin(\\theta_{\\max}^{\\mathrm{old}})}.\n$$\n\nYour program must compute, for each test case, the pair $[\\rho_{\\mathrm{pred}}, \\rho_{\\mathrm{obs}}]$ as floats.\n\nTest suite:\n- Use $m = 16$ quadrature nodes.\n- Use an initial block size $p = s + 4$, where $s$ is the number of eigenvalues in $\\Lambda_{\\mathrm{in}}$, and initialize the starting subspace with a fixed-seed Gaussian random matrix orthonormalized by a $QR$ factorization.\n- For the residual tolerance schedule, let $\\tau_{\\max}$ be the largest uniform tolerance such that your sufficient-condition inequality strictly guarantees contraction. Define three test cases with scaling factors $\\sigma \\in \\{0.5, 1.0, 1.5\\}$ and use $\\tau = \\sigma \\tau_{\\max}$ in each case.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the three pairs, each rounded to six decimal places and enclosed in nested square brackets, for example, \"[[x1,y1],[x2,y2],[x3,y3]]\", where each $xj$ is $\\rho_{\\mathrm{pred}}$ and each $yj$ is $\\rho_{\\mathrm{obs}}$ for the $j$-th test case. No physical units are involved. Angles are handled internally in radians. All outputs must be pure numbers without percentage signs.",
            "solution": "The problem requires the derivation of a sufficient condition for the convergence of the FEAST algorithm with inexact inner linear system solves, followed by a numerical implementation to validate the derived theoretical model.\n\n### Derivation of the Sufficient Condition for Contraction\n\nThe FEAST algorithm can be understood as a form of subspace iteration. A single outer step updates a subspace basis $Q_{\\mathrm{old}}$ to a new basis $Q_{\\mathrm{new}}$ by applying a rational filter operator $\\widetilde{R}(A)$ and re-orthogonalizing. The goal is to make the subspace spanned by $Q_{\\mathrm{new}}$ a better approximation of the target invariant subspace $V_{\\mathrm{in}}$, which is spanned by the eigenvectors of $A$ corresponding to eigenvalues $\\Lambda_{\\mathrm{in}}$ inside a contour $\\Gamma$.\n\nThe quality of the approximation of a subspace $Q$ to the target subspace $V_{\\mathrm{in}}$ is measured by the sine of the largest principal angle, $\\sin(\\theta_{\\max})$, between them. Let $P_{\\mathrm{in}}$ be the orthogonal projector onto $V_{\\mathrm{in}}$. For an orthonormal basis $Q$, $\\sin(\\theta_{\\max}(Q, V_{\\mathrm{in}})) = \\|(I - P_{\\mathrm{in}})Q\\|_2$. Contraction of the error means that after one step, the new angle $\\theta_{\\max}^{\\mathrm{new}}$ is smaller than the old angle $\\theta_{\\max}^{\\mathrm{old}}$.\n\nIn the exact version of FEAST, the new (unnormalized) basis is $Y_{\\mathrm{new}} = R(A)Q_{\\mathrm{old}}$, where $R(A) = \\sum_{k=1}^{m} w_k (z_k I - A)^{-1}$ is the rational filter approximating the spectral projector. The operator $R(A)$ approximately maps vectors in $V_{\\mathrm{in}}$ to themselves (scaled by a factor close to $1$) and vectors in the orthogonal complement $V_{\\mathrm{out}}$ to zero. The convergence rate is dictated by the ratio of the filter's magnitude on the unwanted spectrum to its magnitude on the desired spectrum. Specifically, the contraction factor for the exact algorithm is $\\rho_{\\mathrm{exact}} = \\beta_{\\mathrm{out}} / \\alpha_{\\mathrm{in}}$, where $\\alpha_{\\mathrm{in}} = \\min_{\\lambda \\in \\Lambda_{\\mathrm{in}}} |R(\\lambda)|$ and $\\beta_{\\mathrm{out}} = \\max_{\\lambda \\in \\Lambda_{\\mathrm{out}}} |R(\\lambda)|$.\n\nWhen inner linear systems $(z_k I - A)X_k = Y$ are solved inexactly, the computed operator is $\\widetilde{R}(A) = R(A) + \\Delta R$, where $\\Delta R = \\sum_{k=1}^m w_k E_k$ is the perturbation due to the sum of individual linear solve error operators $E_k$. The new unnormalized basis is $\\widetilde{Y}_{\\mathrm{new}} = \\widetilde{R}(A) Q_{\\mathrm{old}}$. The error in this new subspace basis can be analyzed by projecting it onto $V_{\\mathrm{out}}$:\n$$\n(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}} = (I-P_{\\mathrm{in}}) (R(A) + \\Delta R) Q_{\\mathrm{old}}\n$$\nSince $R(A)$ commutes with $P_{\\mathrm{in}}$ (as $A$ is Hermitian), we have $(I-P_{\\mathrm{in}})R(A) = R(A)(I-P_{\\mathrm{in}})$. Thus,\n$$\n(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}} = R(A)(I-P_{\\mathrm{in}})Q_{\\mathrm{old}} + (I-P_{\\mathrm{in}})\\Delta R Q_{\\mathrm{old}}\n$$\nTaking the operator $2$-norm gives a bound on the magnitude of the error component:\n$$\n\\|(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}}\\|_2 \\le \\|R(A)(I-P_{\\mathrm{in}})Q_{\\mathrm{old}}\\|_2 + \\|(I-P_{\\mathrm{in}})\\Delta R Q_{\\mathrm{old}}\\|_2\n$$\nThe first term is bounded by $\\|R(A)|_{V_{\\mathrm{out}}}\\|_2 \\|(I-P_{\\mathrm{in}})Q_{\\mathrm{old}}\\|_2 = \\beta_{\\mathrm{out}} \\sin(\\theta_{\\max}^{\\mathrm{old}})$. The second term is bounded by $\\|\\Delta R\\|_2 \\|Q_{\\mathrm{old}}\\|_2 = \\|\\Delta R\\|_2$. Therefore,\n$$\n\\|(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}}\\|_2 \\le \\beta_{\\mathrm{out}} \\sin(\\theta_{\\max}^{\\mathrm{old}}) + \\|\\Delta R\\|_2\n$$\nSimilarly, the signal component is $P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}$. Its norm can be bounded below by $\\|P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}\\|_2 \\ge \\sigma_{\\min}(P_{\\mathrm{in}}\\widetilde{R}(A)Q_{\\mathrm{old}})$. A first-order approximation assumes this norm is dominated by the action of the exact filter $R(A)$, so $\\|P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}\\|_2 \\approx \\|R(A)|_{V_{\\mathrm{in}}}\\|_2 \\|P_{\\mathrm{in}}Q_{\\mathrm{old}}\\|_2 \\ge \\alpha_{\\mathrm{in}} \\cos(\\theta_{\\max}^{\\mathrm{old}})$. For a reasonably good initial subspace, $\\cos(\\theta_{\\max}^{\\mathrm{old}}) \\approx 1$.\n\nThe new error angle $\\sin(\\theta_{\\max}^{\\mathrm{new}})$ is the norm of the error component of the normalized basis $Q_{\\mathrm{new}}$. This is approximately the ratio of the norms of the error and signal components of $\\widetilde{Y}_{\\mathrm{new}}$:\n$$\n\\sin(\\theta_{\\max}^{\\mathrm{new}}) \\approx \\frac{\\|(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}}\\|_2}{\\|P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}\\|_2} \\lesssim \\frac{\\beta_{\\mathrm{out}}\\sin(\\theta_{\\max}^{\\mathrm{old}}) + \\|\\Delta R\\|_2}{\\alpha_{\\mathrm{in}}}\n$$\nFrom this linearized model, the ratio of successive errors, which defines the observed contraction factor $\\rho_{\\mathrm{obs}}$, can be expressed as:\n$$\n\\rho_{\\mathrm{obs}} = \\frac{\\sin(\\theta_{\\max}^{\\mathrm{new}})}{\\sin(\\theta_{\\max}^{\\mathrm{old}})} \\approx \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{\\|\\Delta R\\|_2}{\\alpha_{\\mathrm{in}}\\sin(\\theta_{\\max}^{\\mathrm{old}})}\n$$\nThe perturbation norm $\\|\\Delta R\\|_2$ is bounded by $\\|\\Delta R\\|_2 \\le \\sum_{k=1}^m |w_k| \\|E_k\\|_2$. The problem specifies an inexact solve model where the relative residual norm is $\\tau$. Although the exact form of $\\|E_k\\|_2$ depends on the solver, a general bound can be related to $\\tau$. The problem introduces the quantity $S = \\sum_{k=1}^m |w_k| \\|(z_k I - A)^{-1}\\|_2$, and a uniform residual tolerance $\\tau$, which allows us to posit the bound $\\|\\Delta R\\|_2 \\le S\\tau$. Substituting this into our expression for $\\rho_{\\mathrm{obs}}$ yields:\n$$\n\\rho_{\\mathrm{obs}} \\approx \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{S\\tau}{\\alpha_{\\mathrm{in}}\\sin(\\theta_{\\max}^{\\mathrm{old}})}\n$$\nThis expression shows that the observed contraction depends on the quality of the current subspace through the term $1/\\sin(\\theta_{\\max}^{\\mathrm{old}})$. The formula for the predicted contraction factor provided in the problem, $\\rho_{\\mathrm{pred}} = \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{S\\tau}{\\alpha_{\\mathrm{in}}}$, represents a simplified model that omits this dependency. This can be interpreted as a worst-case bound where the error introduced by the inexactness has its largest relative effect, or simply a first-order model that separates the effects of the ideal filter and the solver errors.\n\nA sufficient condition for contraction ($\\rho_{\\mathrm{obs}}  1$) can be derived from $\\rho_{\\mathrm{pred}}  1$. This leads to:\n$$\n\\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{S\\tau}{\\alpha_{\\mathrm{in}}}  1 \\implies S\\tau  \\alpha_{\\mathrm{in}} - \\beta_{\\mathrm{out}} \\implies \\tau  \\frac{\\alpha_{\\mathrm{in}} - \\beta_{\\mathrm{out}}}{S}\n$$\nThis gives a threshold $\\tau_{\\max} = (\\alpha_{\\mathrm{in}} - \\beta_{\\mathrm{out}})/S$ for the inner solver tolerance that is sufficient to guarantee contraction under this simplified model. The numerical experiment will evaluate the accuracy of this prediction.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Performs the FEAST algorithm simulation and comparison as described.\n    \"\"\"\n    # 1. Setup\n    n = 40\n    # A is a diagonal matrix with eigenvalues linearly spaced in [-1.5, 1.5]\n    eigenvalues = np.linspace(-1.5, 1.5, n)\n    # A = np.diag(eigenvalues) # Not explicitly needed as we work with eigenvalues\n\n    # Contour parameters\n    c = 0.0\n    r = 0.45\n    m = 16\n\n    # 2. Eigenvalue partitioning\n    is_inside = np.abs(eigenvalues - c)  r\n    lambda_in = eigenvalues[is_inside]\n    lambda_out = eigenvalues[~is_inside]\n    s = len(lambda_in)\n    p = s + 4\n\n    # Target invariant subspace V_in (spanned by columns of identity matrix)\n    in_indices = np.where(is_inside)[0]\n    V_in = np.zeros((n, s))\n    for i, idx in enumerate(in_indices):\n        V_in[idx, i] = 1.0\n\n    # 3. Quadrature rule\n    thetas = 2 * np.pi * np.arange(m) / m\n    z_k = c + r * np.exp(1j * thetas)\n    # The weights per problem statement are w_k = (r/m) * exp(i*theta_k)\n    # This corresponds to the complex velocity dz/dtheta * dtheta / (2*pi*i) summed\n    # My derivation: (1/(2*pi*i)) * sum(f(z_k) * i*r*exp(i*theta_k)*(2*pi/m))\n    # = sum f(z_k) * (r/m)*exp(i*theta_k). Correct.\n    w_k = (r / m) * np.exp(1j * thetas)\n    \n    # 4. Filter analysis\n    def R_scalar(lam, z_nodes, w_nodes):\n        return np.sum(w_nodes / (z_nodes - lam))\n\n    R_in = np.array([R_scalar(lam, z_k, w_k) for lam in lambda_in])\n    R_out = np.array([R_scalar(lam, z_k, w_k) for lam in lambda_out])\n\n    alpha_in = np.min(np.abs(R_in))\n    beta_out = np.max(np.abs(R_out))\n    \n    # 5. Compute S\n    # ||(z_k*I - A)^-1||_2 = 1 / min_i |z_k - lambda_i|\n    norm_inv_z_minus_A = np.array([1.0 / np.min(np.abs(zk - eigenvalues)) for zk in z_k])\n    S = np.sum(np.abs(w_k) * norm_inv_z_minus_A)\n\n    # 6. Determine tau_max\n    # Sufficient condition for contraction from the model: rho_pred  1\n    # beta_out/alpha_in + S*tau/alpha_in  1  = S*tau  alpha_in - beta_out\n    tau_max = (alpha_in - beta_out) / S\n    \n    # 7. Test cases\n    test_cases_sigma = [0.5, 1.0, 1.5]\n    results = []\n\n    # Initial subspace (fixed for all tests)\n    rng = np.random.default_rng(0)\n    Q_old, _ = np.linalg.qr(rng.standard_normal((n, p)))\n\n    # Calculate sin(theta_max_old)\n    # Using scipy.linalg.subspace_angles\n    angles_old = linalg.subspace_angles(V_in, Q_old)\n    sin_theta_max_old = np.sin(angles_old[-1])\n\n    for sigma in test_cases_sigma:\n        tau = sigma * tau_max\n        \n        # Calculate rho_pred\n        rho_pred = beta_out / alpha_in + S * tau / alpha_in\n\n        # Perform one inexact FEAST step\n        Y_new = np.zeros((n, p), dtype=np.complex128)\n        for i in range(m):\n            zk_i = z_k[i]\n            wk_i = w_k[i]\n            \n            # Since A is diagonal, (zI-A)^-1 is diagonal\n            inv_diags = 1.0 / (zk_i - eigenvalues)\n            # Action of (zI-A)^-1 on Q_old\n            Xk_star = inv_diags[:, np.newaxis] * Q_old\n            \n            # Inexact solve\n            Xk_tilde = (1.0 - tau) * Xk_star\n\n            Y_new += wk_i * Xk_tilde\n        \n        # Orthonormalize the new subspace basis. Note: Y_new can be rank deficient,\n        # but qr handles it returning a basis for the column space.\n        Q_new, _ = np.linalg.qr(Y_new)\n\n        # Calculate sin(theta_max_new)\n        angles_new = linalg.subspace_angles(V_in, Q_new)\n        sin_theta_max_new = np.sin(angles_new[-1])\n        \n        # Calculate rho_obs\n        # Avoid division by zero if starting subspace is perfect\n        if sin_theta_max_old  1e-15:\n            rho_obs = sin_theta_max_new / sin_theta_max_old\n        else:\n            rho_obs = 0.0 if sin_theta_max_new  1e-15 else np.inf\n\n        results.append([round(rho_pred, 6), round(rho_obs, 6)])\n\n    # Final print statement\n    print(f\"{results}\")\n\nsolve()\n```"
        }
    ]
}