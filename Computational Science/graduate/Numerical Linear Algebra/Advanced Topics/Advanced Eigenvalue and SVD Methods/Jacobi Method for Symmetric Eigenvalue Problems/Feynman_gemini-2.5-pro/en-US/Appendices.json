{
    "hands_on_practices": [
        {
            "introduction": "The practical success of the Jacobi method hinges on its computational efficiency. A naive implementation involving explicit matrix multiplications would render each step an $O(n^3)$ operation, making the method impractical for all but the smallest matrices. This exercise  challenges you to implement the core engine of the method—a single Jacobi rotation—in an efficient $O(n)$ manner by updating only the affected rows and columns.",
            "id": "3552566",
            "problem": "You are given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and two distinct indices $(p,q)$ with $0 \\leq p  q \\leq n-1$. Consider one Jacobi rotation that annihilates the off-diagonal entry $a_{pq}$ by an orthogonal similarity transformation. Your task is to implement a single step of Jacobi's method that computes the updated matrix $A' \\in \\mathbb{R}^{n \\times n}$ corresponding to $A' \\leftarrow G^\\top A G$, where $G \\in \\mathbb{R}^{n \\times n}$ is an orthogonal rotation acting only on the $(p,q)$-plane and chosen to annihilate $a_{pq}$. The implementation must avoid forming the full product $G^\\top A G$ and must update $A$ in $O(n)$ by reusing temporaries for the affected rows and columns.\n\nFundamental base to be used:\n- A real symmetric matrix $A$ has real eigenvalues and can be orthogonally diagonalized. Orthogonal similarity transformations $A' = Q^\\top A Q$ preserve symmetry and eigenvalues.\n- A Jacobi rotation is an orthogonal matrix $G$ that is the identity except for a $2 \\times 2$ rotation acting on indices $(p,q)$; by an appropriate choice of rotation parameters, it can annihilate the single off-diagonal entry $a_{pq}$.\n\nDefinitions and requirements:\n- Let $a_{ij}$ denote the $(i,j)$-entry of $A$, and let $a'_{ij}$ denote the $(i,j)$-entry of $A'$.\n- The rotation parameters must be chosen to annihilate $a_{pq}$ in a numerically stable manner.\n- Efficiency requirement: update only the rows and columns indexed by $p$ and $q$, reusing temporaries to avoid unnecessary memory operations. Do not construct $G$ explicitly and do not perform dense matrix-matrix multiplication.\n- Correctness requirement: $A'$ must remain symmetric and satisfy $a'_{pq} \\approx 0$ up to Floating Point (FP) rounding error.\n\nFor each test case, compute and report the following three quantities:\n1. The integer $m$ equal to the number of entries $(i,j)$ for which $|a'_{ij} - a_{ij}| > \\varepsilon$, with $\\varepsilon = 10^{-12}$.\n2. The float $r$ equal to $|a'_{pq}|$.\n3. The float $f$ equal to the Frobenius norm $\\|A' - A\\|_F$, where $\\|X\\|_F = \\sqrt{\\sum_{i,j} x_{ij}^2}$.\n\nAngle units: Any internal rotation angle, if formed, must be in radians; however, no angle is part of the required output.\n\nTest suite:\n- Case $1$ (boundary, trivial annihilation): $n=2$, \n$$\nA = \\begin{bmatrix}\n2.0  0.0 \\\\\n0.0  3.0\n\\end{bmatrix},\\quad (p,q) = (0,1).\n$$\n- Case $2$ (general, mixed signs): $n=5$,\n$$\nA = \\begin{bmatrix}\n4  1  2  0  0 \\\\\n1  3  -1  2  0 \\\\\n2  -1  5  0  1 \\\\\n0  2  0  4  -2 \\\\\n0  0  1  -2  3\n\\end{bmatrix},\\quad (p,q) = (1,2).\n$$\n- Case $3$ (edge, equal diagonals): $n=8$,\n$$\nA = \\begin{bmatrix}\n7  2  0  1  0  0  3  0 \\\\\n2  6  1  0  0  2  0  0 \\\\\n0  1  5  0  2  0  0  1 \\\\\n1  0  0  6  0  0  2  0 \\\\\n0  0  2  0  4  1  0  0 \\\\\n0  2  0  0  1  3  1  0 \\\\\n3  0  0  2  0  1  6  0 \\\\\n0  0  1  0  0  0  0  5\n\\end{bmatrix},\\quad (p,q) = (3,6).\n$$\n\nYour program must apply one Jacobi rotation to each test case to produce the updated matrix $A'$, compute $(m,r,f)$ for each test case, and output a single line containing the results as a comma-separated list of lists in the following format:\n$$\n\\text{[}[m_1,r_1,f_1],[m_2,r_2,f_2],[m_3,r_3,f_3]\\text{]}.\n$$\nAll numeric outputs must be in standard Floating Point format (no scientific symbols other than the usual decimal representation) and there are no physical units. The tolerance is fixed at $\\varepsilon = 10^{-12}$.",
            "solution": "The user-provided problem is valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The task is to implement a single, efficient step of the Jacobi method for a symmetric matrix.\n\n### Principle-Based Design\n\nThe core of the problem is to apply an orthogonal similarity transformation $A' = G^\\top A G$ to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. The transformation matrix $G$ is a Jacobi rotation (also known as a Givens rotation) designed to annihilate a specific off-diagonal element $a_{pq}$ (and its symmetric counterpart $a_{qp}$).\n\n**1. The Jacobi Rotation Matrix ($G$)**\n\nThe matrix $G$ is an orthogonal matrix that is identical to the identity matrix except for a $2 \\times 2$ rotation sub-block acting on the indices $(p,q)$, where $0 \\leq p  q \\leq n-1$. It is defined as:\n$$\nG_{ij} =\n\\begin{cases}\n    c = \\cos(\\theta)  \\text{if } i=j=p \\text{ or } i=j=q \\\\\n    s = \\sin(\\theta)  \\text{if } (i,j) = (p,q) \\\\\n    -s = -\\sin(\\theta)  \\text{if } (i,j) = (q,p) \\\\\n    1  \\text{if } i=j \\text{ and } i \\notin \\{p,q\\} \\\\\n    0  \\text{otherwise}\n\\end{cases}\n$$\nThe value of $\\theta$ is chosen to make the element $a'_{pq}$ of the transformed matrix $A'$ equal to zero. Orthogonality ($G^\\top G = I$) is guaranteed by the property $c^2 + s^2 = 1$.\n\n**2. Derivation of Rotation Parameters ($c, s$)**\n\nThe transformation affects the $2 \\times 2$ submatrix of $A$ at indices $(p,q)$ as follows:\n$$\n\\begin{pmatrix} a'_{pp}  a'_{pq} \\\\ a'_{qp}  a'_{qq} \\end{pmatrix} = \\begin{pmatrix} c  -s \\\\ s  c \\end{pmatrix} \\begin{pmatrix} a_{pp}  a_{pq} \\\\ a_{qp}  a_{qq} \\end{pmatrix} \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix}\n$$\nPerforming this matrix multiplication yields the expression for the new off-diagonal element $a'_{pq}$:\n$$ a'_{pq} = (c^2 - s^2) a_{pq} + c s (a_{pp} - a_{qq}) $$\nTo annihilate this element (set $a'_{pq} = 0$), we must have:\n$$ (c^2 - s^2) a_{pq} = -c s (a_{pp} - a_{qq}) $$\nUsing the double-angle identities $c^2-s^2 = \\cos(2\\theta)$ and $2cs = \\sin(2\\theta)$, we find:\n$$ \\cot(2\\theta) = \\frac{\\cos(2\\theta)}{\\sin(2\\theta)} = \\frac{a_{qq} - a_{pp}}{2 a_{pq}} $$\nLet $\\tau = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}$. To compute $c$ and $s$ in a numerically stable way without explicitly finding $\\theta$, we first find $t = \\tan(\\theta)$. The relationship $\\cot(2\\theta) = \\frac{1-t^2}{2t} = \\tau$ leads to the quadratic equation $t^2 + 2\\tau t - 1 = 0$. The smaller-magnitude root, which corresponds to a smaller rotation angle $|\\theta| \\leq \\pi/4$, is chosen for stability. This root is given by:\n$$ t = \\frac{\\text{sgn}(\\tau)}{|\\tau| + \\sqrt{\\tau^2 + 1}} $$\nA special case occurs if $\\tau = 0$ (i.e., $a_{pp}=a_{qq}$), for which we take $t=1$ (a rotation of $\\theta = \\pi/4$). If $a_{pq}=0$, no rotation is needed, so we set $\\theta=0$, which implies $t=0, c=1, s=0$.\nOnce $t$ is known, $c$ and $s$ are found by:\n$$ c = \\frac{1}{\\sqrt{1+t^2}}, \\quad s = t c $$\n\n**3. Efficient $O(n)$ Matrix Update**\n\nThe transformation $A' = G^\\top A G$ only modifies rows and columns $p$ and $q$. A naive implementation involving full matrix multiplication would be $O(n^3)$. An efficient $O(n)$ update is achieved by deriving explicit formulas for the changed elements.\n- For $i,j \\notin \\{p,q\\}$, the element $a_{ij}$ is unchanged: $a'_{ij} = a_{ij}$.\n- For elements in the affected rows and columns (but outside the $2 \\times 2$ block), for $i \\notin \\{p,q\\}$:\n  $$ a'_{pi} = a'_{ip} = c \\cdot a_{pi} - s \\cdot a_{qi} $$\n  $$ a'_{qi} = a'_{iq} = s \\cdot a_{pi} + c \\cdot a_{qi} $$\n  This requires $O(n)$ operations.\n- For the $2 \\times 2$ submatrix at indices $(p,q)$, the updated values are:\n  $$ a'_{pp} = c^2 a_{pp} - 2cs a_{pq} + s^2 a_{qq} $$\n  $$ a'_{qq} = s^2 a_{pp} + 2cs a_{pq} + c^2 a_{qq} $$\n  And $a'_{pq} = a'_{qp}$ is zero by construction. These updates require $O(1)$ operations.\n\n**4. Algorithmic Implementation and Output Calculation**\n\nThe algorithm proceeds as follows:\n1. For a given matrix $A$ and indices $(p,q)$, first handle the trivial case where $a_{pq}$ is already zero.\n2. If $a_{pq} \\neq 0$, calculate $\\tau$, then find $t, c, s$ using the stable formulas.\n3. Create a copy of the original matrix, $A_{orig}$, for later comparison. The working matrix $A$ (or a copy $A'$) is updated.\n4. Iterate from $i=0$ to $n-1$. For each $i \\notin \\{p,q\\}$, update the elements $a'_{pi}, a'_{ip}, a'_{qi}, a'_{iq}$ using the formulas above.\n5. Update the four elements of the $2 \\times 2$ submatrix $a'_{pp}, a'_{qq}, a'_{pq}, a'_{qp}$. The value of $a'_{pq}$ is calculated and stored to find $r$, and then can be set to exactly $0.0$ in the matrix to reflect perfect annihilation.\n6. The required metrics are then computed:\n   - $m$: Count of entries $(i,j)$ where $|a'_{ij} - a_{ij}| > \\varepsilon$. This is found by element-wise comparison between $A'$ and $A_{orig}$.\n   - $r$: The absolute value $|a'_{pq}|$ as computed from the transformation formula, which measures the numerical error in annihilation.\n   - $f$: The Frobenius norm of the difference, $\\|A' - A_{orig}\\|_F = \\sqrt{\\sum_{i,j} (a'_{ij} - a_{ij})^2}$.\n\nThis procedure correctly and efficiently implements one step of the Jacobi rotation, adhering to all specified constraints.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes one step of the Jacobi rotation for multiple test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[2.0, 0.0], [0.0, 3.0]], dtype=float),\n            \"p\": 0, \"q\": 1\n        },\n        {\n            \"A\": np.array([\n                [4.0, 1.0, 2.0, 0.0, 0.0],\n                [1.0, 3.0, -1.0, 2.0, 0.0],\n                [2.0, -1.0, 5.0, 0.0, 1.0],\n                [0.0, 2.0, 0.0, 4.0, -2.0],\n                [0.0, 0.0, 1.0, -2.0, 3.0]\n            ], dtype=float),\n            \"p\": 1, \"q\": 2\n        },\n        {\n            \"A\": np.array([\n                [7.0, 2.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0],\n                [2.0, 6.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0],\n                [0.0, 1.0, 5.0, 0.0, 2.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 6.0, 0.0, 0.0, 2.0, 0.0],\n                [0.0, 0.0, 2.0, 0.0, 4.0, 1.0, 0.0, 0.0],\n                [0.0, 2.0, 0.0, 0.0, 1.0, 3.0, 1.0, 0.0],\n                [3.0, 0.0, 0.0, 2.0, 0.0, 1.0, 6.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 5.0]\n            ], dtype=float),\n            \"p\": 3, \"q\": 6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        p = case[\"p\"]\n        q = case[\"q\"]\n        results.append(jacobi_step(A, p, q))\n\n    # Format the final output string\n    result_strings = []\n    for res in results:\n        # Format list to string '[m,r,f]' without extra spaces\n        result_strings.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\ndef jacobi_step(A, p, q):\n    \"\"\"\n    Performs a single Jacobi rotation on matrix A to annihilate element (p,q).\n    \n    Args:\n        A (np.ndarray): The real symmetric matrix.\n        p (int): The first index (row/column).\n        q (int): The second index (row/column).\n\n    Returns:\n        list: A list containing [m, r, f].\n    \"\"\"\n    A_orig = A.copy()\n    A_prime = A.copy()\n    n = A.shape[0]\n\n    app = A_orig[p, p]\n    aqq = A_orig[q, q]\n    apq = A_orig[p, q]\n\n    # If the element is already zero, no rotation is necessary.\n    if np.isclose(apq, 0.0):\n        return [0, 0.0, 0.0]\n\n    # Calculate rotation parameters c and s\n    tau = (aqq - app) / (2.0 * apq)\n    # The quadratic equation for t=tan(theta) is t^2 + 2*tau*t - 1 = 0\n    # The smaller-magnitude root is chosen for stability.\n    if tau >= 0:\n        t = 1.0 / (tau + np.sqrt(1.0 + tau**2))\n    else:\n        t = -1.0 / (-tau + np.sqrt(1.0 + tau**2))\n    \n    c = 1.0 / np.sqrt(1.0 + t**2)\n    s = t * c\n\n    # Update the matrix A_prime in an O(n) fashion\n    # First, update elements in rows/cols p,q but not the 2x2 block\n    for i in range(n):\n        if i != p and i != q:\n            a_pi = A_orig[p, i]\n            a_qi = A_orig[q, i]\n            \n            val_pi_prime = c * a_pi - s * a_qi\n            val_qi_prime = s * a_pi + c * a_qi\n            \n            A_prime[p, i] = val_pi_prime\n            A_prime[i, p] = val_pi_prime\n            A_prime[q, i] = val_qi_prime\n            A_prime[i, q] = val_qi_prime\n\n    # Second, update the 2x2 submatrix\n    A_prime[p, p] = c**2 * app - 2 * c * s * apq + s**2 * aqq\n    A_prime[q, q] = s**2 * app + 2 * c * s * apq + c**2 * aqq\n    \n    apq_prime = (c**2 - s**2) * apq + c * s * (app - aqq)\n    A_prime[p, q] = apq_prime\n    A_prime[q, p] = apq_prime\n\n    # Compute the required metrics\n    eps = 1e-12\n    m = int(np.sum(np.abs(A_prime - A_orig)  eps))\n    r = float(np.abs(A_prime[p, q]))\n    f = float(np.linalg.norm(A_prime - A_orig, 'fro'))\n    \n    return [m, r, f]\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "Having mastered the single rotation, you can now assemble the complete classical Jacobi algorithm. This practice  guides you through building the full iterative process, from selecting the largest off-diagonal element to applying rotations until convergence. You will then use your implementation as a computational tool to investigate a subtle but important characteristic of the method: does it naturally sort the eigenvalues along the diagonal?",
            "id": "2405313",
            "problem": "You will investigate whether the classical Jacobi rotation method for real symmetric eigenproblems exhibits any intrinsic \"sorting\" behavior of the diagonal entries as they converge to the eigenvalues. The investigation must be done by implementing the method from first principles and validating the ordering of the final diagonal entries against the notion of nondecreasing order.\n\nStart from the following fundamental bases:\n- For any real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, there exists an orthogonal matrix $V$ such that $V^{\\mathsf{T}} A V$ is diagonal. Orthogonal similarity preserves symmetry and the eigenvalue multiset of $A$.\n- A Jacobi rotation is an orthogonal transformation in a $2$-dimensional coordinate plane $(p,q)$ that can be chosen to annihilate the $(p,q)$ and $(q,p)$ entries of a symmetric matrix while preserving symmetry.\n\nTask:\n1. Derive the necessary plane rotation in the $(p,q)$-plane that zeroes a single off-diagonal entry of a real symmetric matrix $A$ by an orthogonal similarity transformation $G^{\\mathsf{T}}(p,q,\\theta) A G(p,q,\\theta)$, where $G$ is the identity except for a $2 \\times 2$ rotation block acting on indices $(p,q)$. Angles must be expressed in radians. Do not assume any pre-given formula for the rotation parameters; derive them from the condition that the transformed off-diagonal element becomes zero and from the orthogonality of $G$.\n2. Design and implement a classical Jacobi algorithm that repeatedly applies such rotations to drive the off-diagonal Frobenius norm to below a tolerance. Use a robust pivoting strategy that at each step selects the largest-in-magnitude off-diagonal element for annihilation. Terminate when the off-diagonal norm is less than a specified tolerance times the Frobenius norm of the current matrix or when a reasonable iteration cap is reached. Angles must be in radians.\n3. For each test matrix listed below, run your implementation to convergence and report whether the final diagonal vector is in nondecreasing order. Define nondecreasing order with a numerical tolerance $\\varepsilon$ by requiring $d_i \\le d_{i+1} + \\varepsilon$ for all adjacent pairs, where $d_i$ denotes the $i$-th diagonal entry of the final matrix. Use $\\varepsilon = 10^{-10}$.\n4. The numerical tolerance for the Jacobi method should be absolute, defined as $\\tau = 10^{-12} \\|A\\|_F$, where $\\|A\\|_F$ is the Frobenius norm of the initial matrix, and angles must be in radians.\n\nTest suite (each matrix is symmetric, values are unitless):\n- Case $1$ (nontrivial $2 \\times 2$): $A_1 = \\begin{bmatrix} 2  1 \\\\ 1  0 \\end{bmatrix}$.\n- Case $2$ (already diagonal, sorted): $A_2 = \\mathrm{diag}(1,2,3)$.\n- Case $3$ (already diagonal, unsorted): $A_3 = \\mathrm{diag}(3,1,2)$.\n- Case $4$ (already diagonal, repeated eigenvalues, sorted): $A_4 = \\mathrm{diag}(1,1,2,2)$.\n- Case $5$ (already diagonal, repeated eigenvalues, unsorted): $A_5 = \\mathrm{diag}(2,2,1)$.\n\nFor each case, your program must output an integer $1$ if the final diagonal is in nondecreasing order (within the tolerance $\\varepsilon$) and $0$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the test cases ordered as above. For example, the format is $[r_1,r_2,r_3,r_4,r_5]$, where each $r_k \\in \\{0,1\\}$.\n\nAngles must be expressed in radians. No physical units are involved in the matrices or outputs.",
            "solution": "The problem statement is evaluated to be **valid**. It is scientifically grounded in the principles of numerical linear algebra, specifically the Jacobi eigenvalue algorithm for symmetric matrices. The problem is well-posed, with all necessary data, constants, and boundary conditions explicitly provided. The terminology is precise, and the objectives are objective and verifiable.\n\nHere follows the complete reasoning and derivation as required.\n\n### 1. Derivation of the Jacobi Rotation\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be a real symmetric matrix. The Jacobi method aims to diagonalize $A$ by applying a sequence of orthogonal similarity transformations. Each transformation, a Jacobi rotation, is designed to annihilate a specific off-diagonal element.\n\nA Jacobi rotation is an orthogonal transformation $A \\to A' = G^{\\mathsf{T}} A G$. The matrix $G \\equiv G(p, q, \\theta)$ is a Givens rotation, which is an identity matrix except for a $2 \\times 2$ block corresponding to indices $p$ and $q$ (we assume $p  q$ without loss of generality). We define this block as:\n$$\n\\begin{pmatrix} g_{pp}  g_{pq} \\\\ g_{qp}  g_{qq} \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta  \\sin\\theta \\\\ -\\sin\\theta  \\cos\\theta \\end{pmatrix} = \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix}\n$$\nThis matrix corresponds to a rotation by an angle $\\theta$. The matrix $G$ is orthogonal, i.e., $G^{\\mathsf{T}}G = I$.\n\nThe transformation $A' = G^{\\mathsf{T}} A G$ affects only the rows and columns $p$ and $q$ of $A$. The elements of the new matrix $A'$ are related to the elements of $A$ as follows. Let's analyze the transformation of the submatrix defined by indices $p$ and $q$:\n$$\n\\begin{pmatrix} a'_{pp}  a'_{pq} \\\\ a'_{qp}  a'_{qq} \\end{pmatrix} = \\begin{pmatrix} c  -s \\\\ s  c \\end{pmatrix} \\begin{pmatrix} a_{pp}  a_{pq} \\\\ a_{pq}  a_{qq} \\end{pmatrix} \\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix}\n$$\nThe new off-diagonal element $a'_{pq}$ is computed as:\n$$\na'_{pq} = (c a_{pp} - s a_{pq})(s) + (c a_{pq} - s a_{qq})c = cs a_{pp} - s^2 a_{pq} + c^2 a_{pq} - cs a_{qq}\n$$\n$$\na'_{pq} = (c^2 - s^2) a_{pq} + cs(a_{pp} - a_{qq})\n$$\nUsing the double-angle trigonometric identities, this becomes:\n$$\na'_{pq} = \\cos(2\\theta) a_{pq} + \\frac{1}{2}\\sin(2\\theta) (a_{pp} - a_{qq})\n$$\nWe require $a'_{pq} = 0$, which leads to the condition for the rotation angle $\\theta$:\n$$\n\\cos(2\\theta) a_{pq} = -\\frac{1}{2}\\sin(2\\theta) (a_{pp} - a_{qq})\n$$\nProvided $a_{pq} \\neq 0$ and $\\cos(2\\theta) \\neq 0$, we can write:\n$$\n\\tan(2\\theta) = \\frac{2 a_{pq}}{a_{qq} - a_{pp}}\n$$\nIf $a_{qq} = a_{pp}$, the equation simplifies to $a_{pq}\\cos(2\\theta) = 0$. For $a_{pq} \\neq 0$, this requires $\\cos(2\\theta) = 0$, so $2\\theta = \\pm \\pi/2$, which means $\\theta = \\pm \\pi/4$.\n\nFor numerical stability, we avoid computing $\\theta$ directly. Instead, we compute $c = \\cos\\theta$ and $s = \\sin\\theta$ from $t = \\tan\\theta$. Let $\\xi = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}$. Then $\\tan(2\\theta) = 1/\\xi$. The identity $\\tan(2\\theta) = \\frac{2t}{1-t^2}$ leads to the quadratic equation $t^2 + 2\\xi t - 1 = 0$. The roots are $t = -\\xi \\pm \\sqrt{\\xi^2 + 1}$. To minimize the rotation angle, we choose the root with the smaller absolute value. This corresponds to $|\\theta| \\le \\pi/4$, and $|t| \\le 1$. The desired root is given by the numerically stable formula:\n$$\nt = \\frac{\\mathrm{sgn}(\\xi)}{|\\xi| + \\sqrt{1 + \\xi^2}}\n$$\nwhere we define $\\mathrm{sgn}(0)=1$.\nFrom $t$, we find $c$ and $s$:\n$$\nc = \\frac{1}{\\sqrt{1 + t^2}}, \\quad s = c \\cdot t\n$$\nThese must be used to update the matrix. The elements of the $2 \\times 2$ block become:\n$$\na'_{pp} = c^2 a_{pp} - 2cs a_{pq} + s^2 a_{qq}\n$$\n$$\na'_{qq} = s^2 a_{pp} + 2cs a_{pq} + c^2 a_{qq}\n$$\nOne can verify that the trace is preserved: $a'_{pp} + a'_{qq} = a_{pp} + a_{qq}$. Also, one may derive that $a'_{pp} - a'_{qq} = \\sqrt{(a_{pp}-a_{qq})^2 + 4a_{pq}^2}$, which implies $a'_{pp} \\geq a'_{qq}$ for our choice of rotation. This means the larger of the two eigenvalues of the submatrix is placed at index $p$, and the smaller at index $q$ (since $pq$ by convention), indicating a tendency to sort the diagonal elements in *decreasing* order.\n\nFor any other index $k \\notin \\{p, q\\}$, the elements in row/column $p$ and $q$ are updated as:\n$$\na'_{pk} = a'_{kp} = c a_{pk} - s a_{qk}\n$$\n$$\na'_{qk} = a'_{kq} = s a_{pk} + c a_{qk}\n$$\n\n### 2. Jacobi Algorithm Design\n\nThe classical Jacobi algorithm iteratively applies these rotations.\n1.  Initialize $A_k = A_0$.\n2.  At each iteration, select the off-diagonal element $(p,q)$ with the largest magnitude for annihilation.\n3.  Compute the rotation parameters $c$ and $s$ as derived above.\n4.  Update the matrix $A_k$ to $A_{k+1}$ using the derived update rules. This zeros out $A_{p,q}$ and $A_{q,p}$, and changes other elements in rows/columns $p$ and $q$.\n5.  Repeat until the Frobenius norm of the off-diagonal elements, given by $\\left(\\sum_{i \\neq j} a_{ij}^2\\right)^{1/2}$, falls below a tolerance threshold. The threshold is set to $\\tau = 10^{-12} \\|A_0\\|_F$. Since orthogonal transformations preserve the Frobenius norm of the matrix, $\\|A_k\\|_F = \\|A_0\\|_F$, this is a well-defined stopping criterion.\n\n### 3. Analysis of Sorting Behavior\n\nThe derivation shows that for a given rotation on indices $(p,q)$ with $pq$, the updated diagonal element $a'_{pp}$ will be greater than or equal to $a'_{qq}$. This introduces a systematic bias, pushing larger eigenvalues toward smaller indices. Consequently, the classical Jacobi algorithm does not inherently sort eigenvalues in nondecreasing order; instead, it tends to sort them in *decreasing* order.\n\n-   **Cases 2, 3, 4, 5**: The matrices are already diagonal. The algorithm will find that all off-diagonal elements are zero and terminate immediately. The final diagonal is the same as the initial diagonal. Thus, the result depends solely on whether the initial diagonal is sorted non-decreasingly.\n    -   $A_2 = \\mathrm{diag}(1,2,3)$: Sorted. Result: $1$.\n    -   $A_3 = \\mathrm{diag}(3,1,2)$: Not sorted. Result: $0$.\n    -   $A_4 = \\mathrm{diag}(1,1,2,2)$: Sorted. Result: $1$.\n    -   $A_5 = \\mathrm{diag(2,2,1)}$: Not sorted. Result: $0$.\n-   **Case 1**: $A_1 = \\begin{bmatrix} 2  1 \\\\ 1  0 \\end{bmatrix}$. Here $p=0, q=1$.\n    The eigenvalues of this matrix are the roots of $\\lambda^2 - 2\\lambda - 1 = 0$, which are $\\lambda = 1 \\pm \\sqrt{2}$.\n    As shown, the rotation will place the larger eigenvalue at index $p=0$ and the smaller at $q=1$. The resulting diagonal will be $(1+\\sqrt{2}, 1-\\sqrt{2})$. This vector is not in nondecreasing order. Result: $0$.\n\nThe expected output is therefore $[0, 1, 0, 1, 0]$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It implements the Jacobi rotation method and checks the ordering of the resulting eigenvalues.\n    \"\"\"\n    \n    test_cases = [\n        np.array([[2.0, 1.0], [1.0, 0.0]]),\n        np.diag([1.0, 2.0, 3.0]),\n        np.diag([3.0, 1.0, 2.0]),\n        np.diag([1.0, 1.0, 2.0, 2.0]),\n        np.diag([2.0, 2.0, 1.0]),\n    ]\n    \n    results = []\n    \n    for A in test_cases:\n        final_diagonal = jacobi_eigen_solver(A)\n        is_sorted_flag = is_nondecreasing(final_diagonal)\n        results.append(is_sorted_flag)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef jacobi_eigen_solver(A, convergence_tol_factor=1e-12, max_iter=100):\n    \"\"\"\n    Computes the eigenvalues of a symmetric matrix A using the classical Jacobi method.\n\n    Args:\n        A (np.ndarray): The symmetric input matrix.\n        convergence_tol_factor (float): Factor for the convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: A vector containing the eigenvalues (the diagonal of the final matrix).\n    \"\"\"\n    A_k = np.copy(A).astype(np.float64)\n    n = A.shape[0]\n    if n = 1:\n        return np.diag(A_k)\n\n    initial_fro_norm = np.linalg.norm(A, 'fro')\n    convergence_tol = convergence_tol_factor * initial_fro_norm\n\n    for _ in range(max_iter):\n        # Calculate the Frobenius norm of off-diagonal elements\n        off_diag_norm = np.sqrt(max(0, np.linalg.norm(A_k, 'fro')**2 - np.linalg.norm(np.diag(A_k))**2))\n\n        if off_diag_norm  convergence_tol:\n            break\n\n        # Find the off-diagonal element with the largest magnitude\n        # We only need to search the upper triangle\n        p, q = np.unravel_index(np.argmax(np.abs(np.triu(A_k, 1))), A_k.shape)\n\n        app, aqq, apq = A_k[p, p], A_k[q, q], A_k[p, q]\n\n        # Compute rotation parameters c and s\n        if apq == 0.0:\n            c, s = 1.0, 0.0\n        else:\n            # Using the convention from the derivation where cot(2*theta) = (app-aqq)/(2*apq)\n            xi = (app - aqq) / (2.0 * apq)\n            # The quadratic for t=tan(theta) is t^2 + 2*xi*t - 1 = 0.\n            # We choose the smaller magnitude root for stability.\n            t_sign = np.sign(xi) if xi != 0 else 1.0\n            t = t_sign / (np.abs(xi) + np.sqrt(1.0 + xi**2))\n            \n            c = 1.0 / np.sqrt(1.0 + t**2)\n            s = c * t\n\n        # Store old values needed for updates\n        app_old, aqq_old = A_k[p, p], A_k[q, q]\n        apq_old = A_k[p, q]\n\n        # Update the 2x2 sub-block diagonal elements according to G^T A G\n        A_k[p, p] = c**2 * app_old + s**2 * aqq_old - 2 * c * s * apq_old\n        A_k[q, q] = s**2 * app_old + c**2 * aqq_old + 2 * c * s * apq_old\n\n        # Update the rest of rows/columns p and q\n        for k in range(n):\n            if k != p and k != q:\n                apk_old = A_k[p, k]\n                aqk_old = A_k[q, k]\n                A_k[p, k] = c * apk_old - s * aqk_old\n                A_k[q, k] = s * apk_old + c * aqk_old\n                # Maintain symmetry\n                A_k[k, p] = A_k[p, k]\n                A_k[k, q] = A_k[q, k]\n\n        # Annihilate the target off-diagonal element\n        A_k[p, q] = 0.0\n        A_k[q, p] = 0.0\n\n    return np.diag(A_k)\n\ndef is_nondecreasing(d, tol=1e-10):\n    \"\"\"\n    Checks if a vector d is in nondecreasing order within a given tolerance.\n    The condition is d_i = d_{i+1} + tol for all i.\n\n    Args:\n        d (np.ndarray): The vector to check.\n        tol (float): The numerical tolerance for comparison.\n\n    Returns:\n        int: 1 if the vector is in nondecreasing order, 0 otherwise.\n    \"\"\"\n    for i in range(len(d) - 1):\n        if d[i]  d[i+1] + tol:\n            return 0\n    return 1\n\nsolve()\n```"
        },
        {
            "introduction": "An iterative algorithm is only as good as its stopping criterion and the accuracy guarantees it provides. This final practice  moves from implementation to analysis, connecting the state of the nearly diagonalized matrix to the accuracy of the computed eigenpairs. You will apply foundational results from matrix perturbation theory, such as the Hoffman-Wielandt and Davis-Kahan theorems, to understand the rigorous error bounds that a simple, norm-based stopping criterion can ensure.",
            "id": "3552523",
            "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and the classical Jacobi method, which applies a sequence of plane rotations to produce an orthogonal matrix $Q_{k}$ after $k$ sweeps such that $D_{k} = Q_{k}^{\\top} A Q_{k}$ is nearly diagonal. Define the Frobenius norm $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} a_{ij}^{2}}$ and the off-diagonal Frobenius seminorm $\\operatorname{off}(A) = \\sqrt{\\sum_{i \\neq j} a_{ij}^{2}}$. Let $D = D_{k}$ at termination, with diagonal entries $d_{ii}$, and let $q_{i}$ denote the $i$-th column of $Q_{k}$. Consider the stopping criterion\n$$\n\\operatorname{off}(D) \\le \\epsilon \\,\\|A\\|_{F},\n$$\nfor a prescribed tolerance $\\epsilon > 0$. The approximate eigenpairs reported by Jacobi at termination are $(d_{ii}, q_{i})$, i.e., Rayleigh–Ritz pairs with respect to the standard basis in the $D$–coordinates.\n\nUsing only the fundamental definitions of orthogonal similarity, Rayleigh quotient, residual, Frobenius norm, and well-tested facts for Hermitian (real symmetric) matrices (including the Hoffman–Wielandt theorem and the Davis–Kahan sin–$\\Theta$ perturbation bound), choose all statements that are valid consequences of the above stopping criterion and justify how they provide relative accuracy in the eigenvalues and computed eigenvectors.\n\nA. There exists a permutation $\\pi$ of $\\{1,\\dots,n\\}$ such that\n$$\n\\sum_{i=1}^{n} \\bigl(\\lambda_{i}(A) - d_{\\pi(i)\\pi(i)}\\bigr)^{2} \\le \\operatorname{off}(D)^{2} \\le \\epsilon^{2}\\,\\|A\\|_{F}^{2},\n$$\nso the root-mean-square eigenvalue error is at most $\\epsilon\\,\\|A\\|_{F}$. Moreover, for each $i$, letting $u_{i}$ be the unit eigenvector of $A$ associated with $\\lambda_{i}(A)$ and $q_{\\pi(i)}$ its matched computed vector, one has\n$$\n\\sin \\angle(u_{i}, q_{\\pi(i)}) \\le \\frac{\\|D - \\operatorname{diag}(d_{11},\\dots,d_{nn})\\|_{2}}{\\min_{j \\neq i} |\\lambda_{i}(A) - \\lambda_{j}(A)|} \\le \\frac{\\epsilon\\,\\|A\\|_{F}}{\\operatorname{sep}_{i}},\n$$\nwhere $\\operatorname{sep}_{i} = \\min_{j \\neq i} |\\lambda_{i}(A) - \\lambda_{j}(A)|$.\n\nB. For every $i$, one has $|\\lambda_{i}(A) - d_{ii}| \\le \\epsilon\\,\\|A\\|_{F}$ and $\\|A q_{i} - d_{ii} q_{i}\\|_{2} = 0$, so the computed vectors $q_{i}$ are exact eigenvectors of $A$.\n\nC. For all indices $i$ with $\\lambda_{i}(A) \\neq 0$, the relative eigenvalue error satisfies\n$$\n\\frac{|\\lambda_{i}(A) - d_{ii}|}{|\\lambda_{i}(A)|} \\le \\epsilon.\n$$\n\nD. Writing $R = D - \\operatorname{diag}(d_{11},\\dots,d_{nn})$ (the strictly off-diagonal part of $D$), one has $\\|R\\|_{2} \\le \\|R\\|_{F} = \\operatorname{off}(D) \\le \\epsilon\\,\\|A\\|_{F}$. Consequently, for any isolated eigenvalue cluster of $A$ separated from the remainder of the spectrum by a gap $g > 0$, the principal angle $\\Theta$ between the computed invariant subspace and the true invariant subspace obeys\n$$\n\\sin \\Theta \\le \\frac{\\|R\\|_{2}}{g} \\le \\frac{\\epsilon\\,\\|A\\|_{F}}{g}.\n$$\n\nE. Because the Jacobi method has quadratic convergence, the stopping criterion implies\n$$\n\\max_{i} |\\lambda_{i}(A) - d_{ii}| \\le \\epsilon^{2}\\,\\|A\\|_{F}.\n$$\n\nSelect all correct options.",
            "solution": "The user wants me to validate and solve a problem concerning the accuracy of the Jacobi eigenvalue method.\n\n### Step 1: Extract Givens\n-   $A$ is a real symmetric matrix, $A \\in \\mathbb{R}^{n \\times n}$.\n-   The classical Jacobi method produces an orthogonal matrix $Q_k$ after $k$ sweeps.\n-   $D_k = Q_k^\\top A Q_k$ is a nearly diagonal matrix.\n-   Frobenius norm is defined as $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} a_{ij}^{2}}$.\n-   Off-diagonal Frobenius seminorm is defined as $\\operatorname{off}(A) = \\sqrt{\\sum_{i \\neq j} a_{ij}^{2}}$.\n-   At termination, we have $D = D_k$, with diagonal entries $d_{ii}$.\n-   The $i$-th column of $Q_k$ is denoted by $q_i$.\n-   The stopping criterion is $\\operatorname{off}(D) \\le \\epsilon \\,\\|A\\|_{F}$ for a tolerance $\\epsilon > 0$.\n-   The computed approximate eigenpairs are $(d_{ii}, q_{i})$.\n-   The analysis should use fundamental definitions and well-tested facts, including the Hoffman–Wielandt theorem and the Davis–Kahan sin–$\\Theta$ perturbation bound.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective. It is a standard problem in numerical linear algebra, analyzing the accuracy of a computed eigendecomposition from a Jacobi-like method. The setup is clear: an orthogonal similarity transform $D = Q^T A Q$ produces a nearly diagonal matrix $D$, and the deviation from diagonal form is bounded by the stopping criterion. The task is to deduce the consequences for the accuracy of the eigenvalues and eigenvectors, which is a classic application of matrix perturbation theory. The problem asks to use specific, well-established theorems (Hoffman-Wielandt, Davis-Kahan), which are the correct tools for this analysis. The problem is complete, consistent, and formalizable. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with the solution derivation and option-by-option analysis.\n\n### Solution Derivation\n\nThe core of the problem lies in the relationship $A = Q_k D_k Q_k^\\top$, where $Q_k$ is orthogonal. For simplicity, let $Q = Q_k$ and $D = D_k$. Since $A$ and $D$ are orthogonally similar, they share the same eigenvalues, which we denote by $\\lambda_1(A), \\dots, \\lambda_n(A)$.\n\nThe matrix $D$ is not perfectly diagonal. We can decompose it into its diagonal and off-diagonal parts:\n$$\nD = \\operatorname{diag}(d_{11}, d_{22}, \\dots, d_{nn}) + R\n$$\nwhere $\\operatorname{diag}(d_{11}, \\dots, d_{nn})$ is a diagonal matrix containing the diagonal entries of $D$, and $R$ is a matrix with zeros on its diagonal and the off-diagonal entries of $D$ elsewhere.\n\nThe stopping criterion is given as $\\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$.\nBy definition, the Frobenius norm of the off-diagonal part $R$ is:\n$$\n\\|R\\|_F = \\sqrt{\\sum_{i \\neq j} d_{ij}^2} = \\operatorname{off}(D)\n$$\nThus, the stopping criterion can be written as $\\|R\\|_F \\le \\epsilon \\|A\\|_F$.\n\nThe computed eigenpairs are $(d_{ii}, q_i)$. These are Rayleigh-Ritz pairs obtained from the subspace spanned by the columns of $Q$ (which is all of $\\mathbb{R}^n$) and expressed in the standard basis. The matrix representation of $A$ in the basis $\\{q_1, \\dots, q_n\\}$ is $D = Q^\\top A Q$.\n\nWe now analyze each option.\n\n### Option-by-Option Analysis\n\n**A. There exists a permutation $\\pi$ of $\\{1,\\dots,n\\}$ such that $\\sum_{i=1}^{n} \\bigl(\\lambda_{i}(A) - d_{\\pi(i)\\pi(i)}\\bigr)^{2} \\le \\operatorname{off}(D)^{2} \\le \\epsilon^{2}\\,\\|A\\|_{F}^{2}$, so the root-mean-square eigenvalue error is at most $\\epsilon\\,\\|A\\|_{F}$. Moreover, for each $i$, letting $u_{i}$ be the unit eigenvector of $A$ associated with $\\lambda_{i}(A)$ and $q_{\\pi(i)}$ its matched computed vector, one has $\\sin \\angle(u_{i}, q_{\\pi(i)}) \\le \\frac{\\|D - \\operatorname{diag}(d_{11},\\dots,d_{nn})\\|_{2}}{\\min_{j \\neq i} |\\lambda_{i}(A) - \\lambda_{j}(A)|} \\le \\frac{\\epsilon\\,\\|A\\|_{F}}{\\operatorname{sep}_{i}}$, where $\\operatorname{sep}_{i} = \\min_{j \\neq i} |\\lambda_{i}(A) - \\lambda_{j}(A)|$.**\n\nThis statement consists of two parts: an eigenvalue error bound and an eigenvector error bound.\n\n1.  **Eigenvalue Error**: The Hoffman-Wielandt theorem for symmetric matrices states that for any two symmetric matrices $M_1$ and $M_2$, their eigenvalues $\\lambda_i(M_1)$ and $\\lambda_i(M_2)$ can be ordered such that $\\sum_{i=1}^n (\\lambda_i(M_1) - \\lambda_i(M_2))^2 \\le \\|M_1 - M_2\\|_F^2$.\n    Let's apply this to the matrices $D$ and $\\operatorname{diag}(D) = \\operatorname{diag}(d_{11}, \\dots, d_{nn})$.\n    The eigenvalues of $D$ are the eigenvalues of $A$, i.e., $\\{\\lambda_1(A), \\dots, \\lambda_n(A)\\}$.\n    The eigenvalues of $\\operatorname{diag}(D)$ are its diagonal entries, $\\{d_{11}, \\dots, d_{nn}\\}$.\n    Applying the theorem with $M_1=D$ and $M_2=\\operatorname{diag}(D)$, there exists a permutation $\\pi$ of $\\{1, \\dots, n\\}$ such that:\n    $$\n    \\sum_{i=1}^n (\\lambda_i(A) - d_{\\pi(i)\\pi(i)})^2 \\le \\|D - \\operatorname{diag}(D)\\|_F^2\n    $$\n    The matrix $D - \\operatorname{diag}(D)$ is the off-diagonal part $R$. Its Frobenius norm squared is $\\|R\\|_F^2 = (\\operatorname{off}(D))^2$.\n    Thus, $\\sum_{i=1}^n (\\lambda_i(A) - d_{\\pi(i)\\pi(i)})^2 \\le \\operatorname{off}(D)^2$.\n    Using the stopping criterion $\\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$, we get $\\operatorname{off}(D)^2 \\le \\epsilon^2 \\|A\\|_F^2$.\n    Combining these, we have the first part of the statement: $\\sum_{i=1}^n (\\lambda_i(A) - d_{\\pi(i)\\pi(i)})^2 \\le \\operatorname{off}(D)^2 \\le \\epsilon^2 \\|A\\|_F^2$. The first part of the option is correct. The phrasing about RMS error is a consequence of this sum of squares bound being at most $(\\epsilon \\|A\\|_F)^2$.\n\n2.  **Eigenvector Error**: This part invokes a form of the Davis-Kahan $\\sin\\Theta$ theorem. Let's analyze the perturbation $D = \\operatorname{diag}(D) + R$. The eigenvectors of $\\operatorname{diag}(D)$ are the standard basis vectors $e_i$. The eigenvectors of $D$ are some vectors $u'_i$. Since $A = QDQ^\\top$, the eigenvectors of $A$ are $u_i = Qu'_i$, while the approximate eigenvectors are $q_i = Qe_i$. Because $Q$ is orthogonal, it preserves angles: $\\angle(u_i, q_{\\pi(i)}) = \\angle(Qu'_{\\pi(i)}, Qe_{\\pi(i)}) = \\angle(u'_{\\pi(i)}, e_{\\pi(i)})$.\n    The Davis-Kahan theorem applied to $D = \\operatorname{diag}(D) + R$ provides a bound on the angle between the eigenvectors $u'_{\\pi(i)}$ of $D$ and $e_{\\pi(i)}$ of $\\operatorname{diag}(D)$. For an isolated eigenvalue $\\lambda_i(A)$, a standard version of this theorem is:\n    $$\n    \\sin \\angle(u'_{\\pi(i)}, e_{\\pi(i)}) \\le \\frac{\\|R\\|_2}{\\min_{j \\neq i} |\\lambda_i(A) - d_{\\pi(j)\\pi(j)}|}\n    $$\n    A slightly simpler, but also standard, version of the bound uses the gap between the true eigenvalues in the denominator:\n    $$\n    \\sin \\angle(u_i, q_{\\pi(i)}) = \\sin \\angle(u'_{\\pi(i)}, e_{\\pi(i)}) \\le \\frac{\\|R\\|_2}{\\min_{j \\neq i}|\\lambda_i(A)-\\lambda_j(A)|} = \\frac{\\|R\\|_2}{\\operatorname{sep}_i}\n    $$\n    The statement uses $\\|D - \\operatorname{diag}(d_{11}, \\dots, d_{nn})\\|_2 = \\|R\\|_2$, which is correct.\n    For the final inequality, we use the standard norm inequality $\\|R\\|_2 \\le \\|R\\|_F$ and the stopping criterion $\\|R\\|_F = \\operatorname{off}(D) \\le \\epsilon\\|A\\|_F$. This gives $\\|R\\|_2 \\le \\epsilon\\|A\\|_F$.\n    Substituting this into the bound gives $\\frac{\\|R\\|_2}{\\operatorname{sep}_i} \\le \\frac{\\epsilon\\|A\\|_F}{\\operatorname{sep}_i}$.\n    Thus, every part of the second statement is a valid consequence of standard perturbation theory.\n\nVerdict: **Correct**.\n\n**B. For every $i$, one has $|\\lambda_{i}(A) - d_{ii}| \\le \\epsilon\\,\\|A\\|_{F}$ and $\\|A q_{i} - d_{ii} q_{i}\\|_{2} = 0$, so the computed vectors $q_{i}$ are exact eigenvectors of $A$.**\n\n1.  **Eigenvalue Error**: From option A, we have the guarantee that *after a suitable permutation $\\pi$*, $|\\lambda_i(A) - d_{\\pi(i)\\pi(i)}| \\le \\operatorname{off}(D) \\le \\epsilon\\|A\\|_F$. However, the Jacobi method does not guarantee that the diagonal entries $d_{ii}$ will be ordered in correspondence with the sorted eigenvalues $\\lambda_i(A)$. For example, $d_{11}$ could be the best approximation to $\\lambda_5(A)$. This statement assumes $\\pi$ is the identity permutation, which is not generally true. Thus, the claim for *every* $i$ without a permutation is false.\n\n2.  **Residual Norm**: The statement claims $\\|A q_{i} - d_{ii} q_{i}\\|_{2} = 0$. This implies that $(d_{ii}, q_i)$ is an exact eigenpair of $A$. Let's compute the residual norm.\n    $r_i = A q_i - d_{ii} q_i$. Since $A = QDQ^\\top$ and $Q$ is orthogonal, $Q^\\top q_i = e_i$ (the $i$-th standard basis vector).\n    $$\n    \\|r_i\\|_2 = \\|Q D Q^\\top q_i - d_{ii} q_i\\|_2 = \\|Q(D Q^\\top q_i - d_{ii} Q^\\top q_i)\\|_2 = \\|D e_i - d_{ii} e_i\\|_2\n    $$\n    The vector $De_i$ is the $i$-th column of $D$. So, $De_i - d_{ii} e_i$ is the $i$-th column of $D$ with its diagonal element set to zero. This is the $i$-th column of the matrix $R$. The squared norm is:\n    $$\n    \\|r_i\\|_2^2 = \\|De_i - d_{ii} e_i\\|_2^2 = \\sum_{j=1, j \\neq i}^{n} |d_{ji}|^2\n    $$\n    This is zero if and only if all off-diagonal elements in the $i$-th column of $D$ are zero. The stopping criterion $\\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$ does not imply $\\operatorname{off}(D) = 0$ for a general $\\epsilon > 0$. Therefore, the residual is generally non-zero. The claim that $q_i$ are exact eigenvectors is false.\n\nVerdict: **Incorrect**.\n\n**C. For all indices $i$ with $\\lambda_{i}(A) \\neq 0$, the relative eigenvalue error satisfies $\\frac{|\\lambda_{i}(A) - d_{ii}|}{|\\lambda_{i}(A)|} \\le \\epsilon$.**\n\nThis statement claims a uniform relative error bound. As established in the analysis of option B, we have an absolute error bound of the form $|\\lambda_i(A) - d_{\\pi(i)\\pi(i)}| \\le \\epsilon \\|A\\|_F$ for some permutation $\\pi$. Let us assume for the sake of argument that the permutation is the identity ($\\pi(i) = i$), which is the most favorable case for this option.\nThe relative error would then be:\n$$\n\\frac{|\\lambda_i(A) - d_{ii}|}{|\\lambda_i(A)|} \\le \\frac{\\epsilon \\|A\\|_F}{|\\lambda_i(A)|}\n$$\nThe statement claims this is less than or equal to $\\epsilon$. This would require $\\|A\\|_F \\le |\\lambda_i(A)|$. This is generally false. The Frobenius norm of a matrix can be much larger than the magnitude of its smallest non-zero eigenvalue. For instance, consider $A = \\begin{pmatrix} 100  0 \\\\ 0  0.1 \\end{pmatrix}$. Then $\\|A\\|_F = \\sqrt{100^2 + 0.1^2} \\approx 100$. For $\\lambda_2 = 0.1$, the condition would be $100 \\le 0.1$, which is false. The stopping criterion is based on an absolute measure of deviation from diagonal form and leads to absolute error bounds for the eigenvalues, not general relative error bounds of this form.\n\nVerdict: **Incorrect**.\n\n**D. Writing $R = D - \\operatorname{diag}(d_{11},\\dots,d_{nn})$ (the strictly off-diagonal part of $D$), one has $\\|R\\|_{2} \\le \\|R\\|_{F} = \\operatorname{off}(D) \\le \\epsilon\\,\\|A\\|_{F}$. Consequently, for any isolated eigenvalue cluster of $A$ separated from the remainder of the spectrum by a gap $g > 0$, the principal angle $\\Theta$ between the computed invariant subspace and the true invariant subspace obeys $\\sin \\Theta \\le \\frac{\\|R\\|_{2}}{g} \\le \\frac{\\epsilon\\,\\|A\\|_{F}}{g}$.**\n\n1.  **Norm Inequalities**: The matrix $R$ is defined as the off-diagonal part of $D$. By definition, $\\|R\\|_F = \\operatorname{off}(D)$. The inequality $\\|R\\|_2 \\le \\|R\\|_F$ is a standard property relating the spectral norm and the Frobenius norm for any matrix. The inequality $\\operatorname{off}(D) \\le \\epsilon \\|A\\|_F$ is the given stopping criterion. Therefore, the chain of inequalities $\\|R\\|_{2} \\le \\|R\\|_{F} = \\operatorname{off}(D) \\le \\epsilon\\,\\|A\\|_{F}$ is entirely correct.\n\n2.  **Subspace Error**: The second part of the statement concerns the accuracy of a computed invariant subspace. This is precisely the domain of the general Davis-Kahan $\\sin\\Theta$ theorem. Let the spectrum of $A$ be partitioned into two sets separated by a gap $g > 0$. Let $U_1$ be the basis for the true invariant subspace corresponding to one part of the spectrum, and let $Q_1$ be the basis for the computed subspace (a selection of columns from $Q$). The theorem states that the sine of the largest principal angle, $\\Theta$, between these two subspaces is bounded by the norm of a residual matrix divided by the gap $g$.\n    A standard and powerful formulation of this theorem gives the bound $\\sin\\Theta \\le \\frac{\\|R_{21}\\|_2}{g}$, where $R_{21}$ is the corresponding off-diagonal block of the residual. A simpler and widely used version gives the bound in terms of the full perturbation:\n    $$\n    \\sin \\Theta \\le \\frac{\\|R\\|_2}{g}\n    $$\n    where $R$ is the perturbation that separates the \"unperturbed\" problem (with eigenvectors $q_i$) from the \"perturbed\" problem (with eigenvectors of $A$). In our context, this perturbation is precisely captured by the off-diagonal part $R$ of $D$. The statement is a correct application of the Davis-Kahan theorem for invariant subspaces. The final inequality $\\frac{\\|R\\|_2}{g} \\le \\frac{\\epsilon \\|A\\|_F}{g}$ follows directly from the first part of the statement.\n\nVerdict: **Correct**.\n\n**E. Because the Jacobi method has quadratic convergence, the stopping criterion implies $\\max_{i} |\\lambda_{i}(A) - d_{ii}| \\le \\epsilon^{2}\\,\\|A\\|_{F}$.**\n\nThis statement conflates the convergence *rate* of the algorithm with the *accuracy* achieved at termination. The quadratic convergence of the Jacobi method means that the off-diagonal norm decreases quadratically from one sweep to the next, i.e., $\\operatorname{off}(D_{k+1}) \\approx C \\cdot (\\operatorname{off}(D_k))^2$ for some constant $C$, once $\\operatorname{off}(D_k)$ is sufficiently small.\nThe algorithm terminates at sweep $k$ when $\\operatorname{off}(D_k) \\le \\epsilon \\|A\\|_F$. The error in the eigenvalues is bounded by the off-diagonal norm at termination. As shown by the Hoffman-Wielandt theorem (Option A), $\\sum (\\lambda_i - d_{\\pi(i)})^2 \\le \\operatorname{off}(D_k)^2$. This implies that the maximum absolute eigenvalue error is bounded by:\n$$\n\\max_i |\\lambda_i(A) - d_{\\pi(i)\\pi(i)}| \\le \\sqrt{\\sum (\\lambda_i(A) - d_{\\pi(i)\\pi(i)})^2} \\le \\operatorname{off}(D_k) \\le \\epsilon \\|A\\|_F\n$$\nThe error is proportional to $\\epsilon$, not $\\epsilon^2$. The quadratic convergence means the algorithm is very efficient at reducing the error, but the final error is still determined by the tolerance $\\epsilon$ in the stopping criterion, not its square.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}