{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, there is no substitute for performing the calculations by hand. This first exercise guides you through the fundamental steps of the Golub-Kahan bidiagonalization process for a small, well-structured matrix. By explicitly computing the basis vectors $u_j$ and $v_j$ and the bidiagonal entries $\\alpha_j$ and $\\beta_j$, you will gain a concrete understanding of the underlying recurrences and verify the core matrix identities that the algorithm maintains .",
            "id": "3548819",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be the lower bidiagonal matrix with main diagonal entries equal to $2$ and subdiagonal entries equal to $1$, that is\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 2\n\\end{pmatrix}.\n$$\nApply three steps of the Golub窶適ahan bidiagonalization to $A$ starting from the unit vector $u_1 = e_1 \\in \\mathbb{R}^4$, where at each step you alternately form\n$$\nr_j \\;=\\; A^{T} u_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j \\;=\\; \\|r_j\\|_2, \\quad v_j \\;=\\; r_j/\\alpha_j,\n$$\nfollowed by\n$$\np_j \\;=\\; A v_j - \\alpha_j u_j, \\quad \\beta_j \\;=\\; \\|p_j\\|_2, \\quad u_{j+1} \\;=\\; p_j/\\beta_j,\n$$\nwith the convention $\\beta_0 = 0$ and $v_0$ unused. Construct the matrices $U_3 = [\\,u_1 \\;\\; u_2 \\;\\; u_3\\,] \\in \\mathbb{R}^{4 \\times 3}$, $V_3 = [\\,v_1 \\;\\; v_2 \\;\\; v_3\\,] \\in \\mathbb{R}^{4 \\times 3}$, and the lower bidiagonal matrix $B_3 \\in \\mathbb{R}^{3 \\times 3}$ with diagonal entries $\\alpha_1,\\alpha_2,\\alpha_3$ and subdiagonal entries $\\beta_1,\\beta_2$. Verify numerically that\n$$\nU_3^{T} A V_3 \\;=\\; B_3\n\\quad\\text{and}\\quad\nA V_3 \\;=\\; U_3 B_3 + \\beta_3\\, u_4 e_3^{T},\n$$\nwhere $u_4$ is the next left Lanczos vector produced by the process and $e_3 \\in \\mathbb{R}^{3}$ is the third standard basis vector. Provide the exact value of $\\beta_3$ as your final answer. Express the final answer exactly; no rounding is required.",
            "solution": "The Golub-Kahan bidiagonalization algorithm is applied step-by-step for $j=1, 2, 3$. The matrix $A$ and its transpose $A^T$ are given by:\n$$\nA = \\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 2\n\\end{pmatrix}, \\quad\nA^T = \\begin{pmatrix}\n2 & 1 & 0 & 0 \\\\\n0 & 2 & 1 & 0 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\nThe starting vector is $u_1 = e_1 = (1, 0, 0, 0)^T$, and we initialize with $\\beta_0 = 0$.\n\n**Step 1 ($j=1$):**\n-   Compute $v_1$:\n    $r_1 = A^T u_1 - \\beta_0 v_0 = A^T e_1 = (2, 0, 0, 0)^T$.\n    $\\alpha_1 = \\|r_1\\|_2 = 2$.\n    $v_1 = r_1/\\alpha_1 = (1, 0, 0, 0)^T = e_1$.\n-   Compute $u_2$:\n    $p_1 = A v_1 - \\alpha_1 u_1 = A e_1 - 2 e_1 = (2, 1, 0, 0)^T - (2, 0, 0, 0)^T = (0, 1, 0, 0)^T$.\n    $\\beta_1 = \\|p_1\\|_2 = 1$.\n    $u_2 = p_1/\\beta_1 = (0, 1, 0, 0)^T = e_2$.\n\n**Step 2 ($j=2$):**\n-   Compute $v_2$:\n    $r_2 = A^T u_2 - \\beta_1 v_1 = A^T e_2 - 1 e_1 = (1, 2, 0, 0)^T - (1, 0, 0, 0)^T = (0, 2, 0, 0)^T$.\n    $\\alpha_2 = \\|r_2\\|_2 = 2$.\n    $v_2 = r_2/\\alpha_2 = (0, 1, 0, 0)^T = e_2$.\n-   Compute $u_3$:\n    $p_2 = A v_2 - \\alpha_2 u_2 = A e_2 - 2 e_2 = (0, 2, 1, 0)^T - (0, 2, 0, 0)^T = (0, 0, 1, 0)^T$.\n    $\\beta_2 = \\|p_2\\|_2 = 1$.\n    $u_3 = p_2/\\beta_2 = (0, 0, 1, 0)^T = e_3$.\n\n**Step 3 ($j=3$):**\n-   Compute $v_3$:\n    $r_3 = A^T u_3 - \\beta_2 v_2 = A^T e_3 - 1 e_2 = (0, 1, 2, 0)^T - (0, 1, 0, 0)^T = (0, 0, 2, 0)^T$.\n    $\\alpha_3 = \\|r_3\\|_2 = 2$.\n    $v_3 = r_3/\\alpha_3 = (0, 0, 1, 0)^T = e_3$.\n-   Compute $\\beta_3$:\n    $p_3 = A v_3 - \\alpha_3 u_3 = A e_3 - 2 e_3 = (0, 0, 2, 1)^T - (0, 0, 2, 0)^T = (0, 0, 0, 1)^T$.\n    $\\beta_3 = \\|p_3\\|_2 = 1$.\n\nThe value of $\\beta_3$ is $1$.\n\nFor verification:\n$U_3 = [e_1, e_2, e_3]$, $V_3 = [e_1, e_2, e_3]$.\n$B_3 = \\begin{pmatrix} \\alpha_1 & 0 & 0 \\\\ \\beta_1 & \\alpha_2 & 0 \\\\ 0 & \\beta_2 & \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$. Note: the problem description for $B_3$ seems to use a non-standard indexing for subdiagonals. The GKB process as defined produces $\\beta_2, \\beta_3$ etc. The problem asks for subdiagonals $\\beta_1, \\beta_2$. We follow the computed values from the algorithm's first two steps: $\\beta_1=1, \\beta_2=1$.\n$U_3^T A V_3$ is the top-left $3 \\times 3$ submatrix of $A$, which is $\\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$, which matches $B_3$.\nThe second relation also holds as per the derivation of the algorithm.\nThe final required value is $\\beta_3=1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "One of the most powerful applications of Golub-Kahan bidiagonalization is in solving large-scale inverse and ill-posed problems. This practice demonstrates how the algorithm provides a framework for Tikhonov regularization, a standard method for stabilizing solutions. You will see how the original, large problem is projected onto a small, bidiagonal system where finding an optimal regularization parameter $\\lambda$ becomes computationally feasible, a technique central to scientific computing and data science .",
            "id": "3548852",
            "problem": "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a right-hand side $b \\in \\mathbb{R}^{m}$, and recall the Golub-Kahan (GK) bidiagonalization process started at $u_{1} = b / \\|b\\|_{2}$, which constructs orthonormal matrices $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_{k} \\in \\mathbb{R}^{n \\times k}$ together with a lower bidiagonal matrix $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ so that $A V_{k}$ is represented on $U_{k+1}$ by $B_{k}$. In classical Tikhonov regularization, one seeks $x$ that minimizes the quadratic functional $\\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$ for a regularization parameter $\\lambda > 0$. In a GK-based projection, one restricts $x$ to the $k$-dimensional subspace $\\mathrm{span}(V_{k})$ by writing $x = V_{k} y$ and studies the corresponding projected Tikhonov problem in the $y$-variables.\n\nStarting from the definitions above and the standard properties of orthogonal projectors and least squares problems:\n1. Derive the $k$-dimensional projected Tikhonov problem in terms of $B_{k}$ and $\\beta e_{1}$, where $\\beta = \\|b\\|_{2}$ and $e_{1} \\in \\mathbb{R}^{k+1}$ is the first canonical basis vector. Express the projected problem as an augmented least squares problem with an appropriate block matrix and right-hand side.\n2. Using only fundamental facts about the singular value decomposition (SVD) and orthogonal transformations, derive the spectral form of the projected Tikhonov solution and an explicit expression for the residual norm $\\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}$ in terms of the singular values and the spectral coefficients of $\\beta e_{1}$ with respect to the left singular vectors of $B_{k}$.\n\nNow specialize to $k = 2$ and suppose that after two GK steps the resulting matrix is\n$$\nB_{2} = \\begin{pmatrix}\n3 & 0 \\\\\n4 & 0 \\\\\n0 & 5\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n$$\nand the data have norm $\\beta = \\|b\\|_{2} = 10$. Study parameter choice by imposing the Morozov discrepancy principle on the projected problem: choose $\\lambda > 0$ so that the projected residual satisfies $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2} = \\delta$ with a prescribed noise level $\\delta = \\sqrt{73}$. Use the singular value decomposition of $B_{2}$ to compute the unique positive $\\lambda$ that meets the discrepancy principle for the projected problem with the given $B_{2}$ and $\\beta$.\n\nRound your final numerical answer for $\\lambda$ to four significant figures. No units are required.",
            "solution": "The Tikhonov regularization problem seeks to minimize $\\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$. Projecting the solution onto the Krylov subspace by setting $x=V_k y$ and using the properties of the Golub-Kahan process ($AV_k = U_{k+1} B_k$, $b=\\beta U_{k+1} e_1$, and orthonormality of $U_{k+1}, V_k$), this large problem is transformed into an equivalent small problem:\n$$ \\min_{y \\in \\mathbb{R}^k} J(y) = \\|B_{k} y - \\beta e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\nThe solution $y(\\lambda)$ is given by the normal equations $(B_k^T B_k + \\lambda^2 I_k)y = B_k^T(\\beta e_1)$. The squared residual norm $\\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}^{2}$ can be found using the Singular Value Decomposition (SVD) of $B_k$. It is given by the secular equation:\n$$ \\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\sum_{j=1}^{k} \\left( \\frac{\\lambda^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{k+1}^{2} $$\nwhere $\\sigma_j$ are the singular values of $B_k$ and $\\gamma_j = \\beta p_j^T e_1$ are the spectral coefficients of the right-hand side with respect to the left singular vectors $p_j$ of $B_k$.\n\nFor the specific case $k=2$, we are given $B_{2} = \\begin{pmatrix} 3 & 0 \\\\ 4 & 0 \\\\ 0 & 5 \\end{pmatrix}$, $\\beta = 10$, and the Morozov discrepancy principle requires the residual norm to be $\\delta = \\sqrt{73}$, so the squared norm is $73$.\n\n1.  **SVD of $B_2$**: We compute $B_{2}^{T} B_{2} = \\begin{pmatrix} 25 & 0 \\\\ 0 & 25 \\end{pmatrix}$. The eigenvalues are $\\sigma_{1}^{2} = 25$ and $\\sigma_{2}^{2} = 25$, so the singular values are $\\sigma_{1} = 5$ and $\\sigma_{2} = 5$. The left singular vectors are $p_{1} = (3/5, 4/5, 0)^T$, $p_{2} = (0, 0, 1)^T$, and the vector spanning the null space of $B_2^T$ is $p_{3} = (4/5, -3/5, 0)^T$.\n\n2.  **Spectral Coefficients**: The coefficients $\\gamma_j$ are computed with $\\beta=10$:\n    $$ \\gamma_{1} = 10 \\cdot p_1^T e_1 = 10 \\cdot (3/5) = 6 $$\n    $$ \\gamma_{2} = 10 \\cdot p_2^T e_1 = 10 \\cdot 0 = 0 $$\n    $$ \\gamma_{3} = 10 \\cdot p_3^T e_1 = 10 \\cdot (4/5) = 8 $$\n\n3.  **Solve for $\\lambda$**: Substitute these values into the secular equation:\n    $$ 73 = \\left( \\frac{\\lambda^{2} \\cdot 6}{25 + \\lambda^{2}} \\right)^{2} + \\left( \\frac{\\lambda^{2} \\cdot 0}{25 + \\lambda^{2}} \\right)^{2} + 8^{2} $$\n    $$ 73 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} + 64 $$\n    $$ 9 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} \\quad \\implies \\quad \\frac{1}{4} = \\left(\\frac{\\lambda^{2}}{25 + \\lambda^{2}}\\right)^{2} $$\n    Taking the positive square root (since $\\lambda>0$):\n    $$ \\frac{1}{2} = \\frac{\\lambda^{2}}{25 + \\lambda^{2}} \\quad \\implies \\quad 25 + \\lambda^{2} = 2\\lambda^{2} $$\n    This simplifies to $\\lambda^2=25$, which for $\\lambda > 0$ gives $\\lambda = 5$. Rounded to four significant figures, the answer is $5.000$.",
            "answer": "$$\\boxed{5.000}$$"
        },
        {
            "introduction": "The Golub-Kahan process is the engine behind one of the most famous iterative algorithms for least-squares problems, LSQR. A key to the efficiency of such methods is the ability to monitor convergence without explicitly forming the solution at each step. This problem asks you to derive and implement the elegant estimators for the residual norm $\\|r_k\\|$ and gradient norm $\\|A^T r_k\\|$, which depend only on the scalars generated during the bidiagonalization, and to test their accuracy on challenging ill-conditioned matrices .",
            "id": "3548830",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{m}$. For iterates $x_k \\in \\mathbb{R}^{n}$ produced by $k$ steps of the Golub窶適ahan Bidiagonalization (GKB), define the least-squares residual $r_k = b - A x_k$ and its normal-equation gradient $g_k = A^\\top r_k$. The Golub窶適ahan Bidiagonalization (GKB) constructs orthonormal bases $\\{u_i\\}$ and $\\{v_i\\}$ for the Krylov subspaces in $\\mathbb{R}^m$ and $\\mathbb{R}^n$, respectively, together with recurrence scalars $\\{\\alpha_i\\}$ and $\\{\\beta_i\\}$, yielding a lower-bidiagonal matrix $B_k \\in \\mathbb{R}^{(k+1)\\times k}$ that encodes the small projected least-squares problem.\n\nYour tasks are:\n- Starting from the fundamental definitions of the Golub窶適ahan Bidiagonalization process and the properties of orthonormal transformations, derive estimators for the norms $\\|r_k\\|$ and $\\|A^\\top r_k\\|$ that depend solely on the bidiagonal matrix $B_k$ and the recurrence scalars $\\{\\alpha_i\\}$ and $\\{\\beta_i\\}$ produced by GKB, without using $A$ itself in the estimator formulas.\n- Implement a program that performs $k$ steps of GKB to build $B_k$, solves the associated small least-squares problem to obtain $x_k$, and then computes the estimators for $\\|r_k\\|$ and $\\|A^\\top r_k\\|$ purely from $B_k$ and the recurrence scalars. For validation, compute the actual norms using $A$ and compare them with the estimators.\n- Quantify reliability by reporting the relative errors of the estimators with respect to the actual norms on a set of highly ill-conditioned test problems. Each reported quantity must be a float. No physical units or angle specifications apply.\n\nYou must use the following test suite of matrices and parameters, chosen to probe general cases, ill-conditioning severity, and boundary iteration counts:\n1. $A$ is the $20 \\times 20$ Hilbert matrix with entries $A_{ij} = \\frac{1}{i+j-1}$, $b$ is the all-ones vector in $\\mathbb{R}^{20}$, $k = 10$.\n2. $A$ is the $20 \\times 20$ Hilbert matrix, $b$ is a fixed pseudorandom vector in $\\mathbb{R}^{20}$ (generated deterministically), $k = 5$.\n3. $A$ is the $20 \\times 20$ Vandermonde matrix with nodes $t_i$ linearly spaced in $[0,1]$, columns are powers $t_i^{j-1}$ for $j=1,\\dots,20$, $b$ is the all-ones vector in $\\mathbb{R}^{20}$, $k = 10$.\n4. $A$ has exponentially decaying singular values: construct $A = U \\Sigma V^\\top$ where $U \\in \\mathbb{R}^{60 \\times 20}$ and $V \\in \\mathbb{R}^{20 \\times 20}$ are orthogonal (obtained by the $Q$ factor from a thin Householder-based QR of Gaussian random matrices), and $\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_{20})$ with $\\sigma_i = 10^{-i/3}$; $b$ is a fixed pseudorandom vector in $\\mathbb{R}^{60}$, $k = 12$.\n5. Boundary case: $A$ is the $10 \\times 10$ Hilbert matrix, $b$ is the all-ones vector in $\\mathbb{R}^{10}$, $k = 1$.\n\nFor each test case, compute:\n- The estimator-based residual norm $\\|r_k\\|$ and its actual counterpart, and report the relative error defined as $\\left|\\|r_k\\|_{\\mathrm{est}} - \\|r_k\\|_{\\mathrm{act}}\\right| / \\max(\\|r_k\\|_{\\mathrm{act}}, \\varepsilon)$ where $\\varepsilon$ is a small positive constant to avoid division by zero.\n- The estimator-based gradient norm $\\|A^\\top r_k\\|$ and its actual counterpart, and report the analogous relative error.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$[\\mathrm{err}_{r}^{(1)}, \\mathrm{err}_{g}^{(1)}, \\mathrm{err}_{r}^{(2)}, \\mathrm{err}_{g}^{(2)}, \\mathrm{err}_{r}^{(3)}, \\mathrm{err}_{g}^{(3)}, \\mathrm{err}_{r}^{(4)}, \\mathrm{err}_{g}^{(4)}, \\mathrm{err}_{r}^{(5)}, \\mathrm{err}_{g}^{(5)}]$,\nwhere $\\mathrm{err}_{r}^{(i)}$ is the residual-norm relative error and $\\mathrm{err}_{g}^{(i)}$ is the gradient-norm relative error for the $i$-th test case.\n\nThe final answer must be runnable code that performs all computations and prints the result in the specified format. No inputs from external sources are permitted, and the run must be self-contained and deterministic.",
            "solution": "The problem requires the derivation and implementation of estimators for the least-squares residual norm, $\\|r_k\\|$, and its corresponding normal-equation gradient norm, $\\|A^\\top r_k\\|$, using quantities generated by the Golub-Kahan Bidiagonalization (GKB) process.\n\n### The Golub-Kahan Bidiagonalization (GKB) Process\nGiven a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$, the GKB process generates two sequences of orthonormal vectors, $\\{u_i\\}_{i=1}^{k+1} \\subset \\mathbb{R}^m$ and $\\{v_i\\}_{i=1}^{k} \\subset \\mathbb{R}^n$, and two sequences of positive scalars, $\\{\\alpha_i\\}_{i=1}^{k}$ and $\\{\\beta_i\\}_{i=1}^{k+1}$. The process is initialized as follows:\n$$\n\\beta_1 = \\|b\\|_2, \\quad u_1 = b / \\beta_1, \\quad v_0 = 0\n$$\nFor $i=1, \\dots, k$, the recurrences are:\n$$\n\\tilde{v}_i = A^\\top u_i - \\beta_i v_{i-1}, \\quad \\alpha_i = \\|\\tilde{v}_i\\|_2, \\quad v_i = \\tilde{v}_i / \\alpha_i\n$$\n$$\n\\tilde{u}_{i+1} = A v_i - \\alpha_i u_i, \\quad \\beta_{i+1} = \\|\\tilde{u}_{i+1}\\|_2, \\quad u_{i+1} = \\tilde{u}_{i+1} / \\beta_{i+1}\n$$\nLet $U_{k+1} = [u_1, \\dots, u_{k+1}] \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_k = [v_1, \\dots, v_k] \\in \\mathbb{R}^{n \\times k}$. By construction, $U_{k+1}^\\top U_{k+1} = I_{k+1}$ and $V_k^\\top V_k = I_k$. The recurrences can be expressed in matrix form:\n$$\nA V_k = U_{k+1} B_k\n$$\nwhere $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ is a lower-bidiagonal matrix:\n$$\nB_k =\n\\begin{pmatrix}\n\\alpha_1 & & & \\\\\n\\beta_2 & \\alpha_2 & & \\\\\n& \\beta_3 & \\ddots & \\\\\n& & \\ddots & \\alpha_k \\\\\n& & & \\beta_{k+1}\n\\end{pmatrix}\n$$\nThe GKB process provides an orthonormal basis $V_k$ for the Krylov subspace $\\mathcal{K}_k(A^\\top A, A^\\top b)$. The approximate solution $x_k$ to the least-squares problem $\\min_x \\|b-Ax\\|_2$ is sought in this subspace, i.e., $x_k = V_k y_k$ for some $y_k \\in \\mathbb{R}^k$.\n\n### Derivation of the Estimator for $\\|r_k\\|$\nThe residual $r_k = b - Ax_k$ can be expressed in terms of the GKB quantities. Substituting $x_k = V_k y_k$ and using the GKB matrix relations:\n$$\nr_k = b - A(V_k y_k) = b - (AV_k)y_k\n$$\nFrom the initialization, $b = \\beta_1 u_1$. Since $U_{k+1}$ is an orthonormal basis that contains $u_1$, we can write $b = U_{k+1}(\\beta_1 e_1)$, where $e_1 \\in \\mathbb{R}^{k+1}$ is the first standard basis vector.\n$$\nr_k = U_{k+1}(\\beta_1 e_1) - (U_{k+1} B_k) y_k = U_{k+1} (\\beta_1 e_1 - B_k y_k)\n$$\nSince $U_{k+1}$ is an orthogonal matrix, taking the Euclidean norm preserves the length:\n$$\n\\|r_k\\|_2 = \\|U_{k+1} (\\beta_1 e_1 - B_k y_k)\\|_2 = \\|\\beta_1 e_1 - B_k y_k\\|_2\n$$\nThe vector $y_k$ is chosen to minimize $\\|r_k\\|_2$, which is equivalent to solving the small projected least-squares problem:\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\nTherefore, the estimator for the residual norm is the norm of the residual of this small problem, which is calculable solely from $B_k$ and $\\beta_1$. In ideal arithmetic, this is an exact identity, not an estimator. In finite precision, it serves as a robust estimator that is often more accurate than computing $\\|b-Ax_k\\|$ directly.\n$$\n\\|r_k\\|_{\\mathrm{est}} = \\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\n\n### Derivation of the Estimator for $\\|A^\\top r_k\\|$\nLet $g_k = A^\\top r_k$ be the normal-equation gradient. Let $s_k = \\beta_1 e_1 - B_k y_k$ be the residual of the projected problem. We have $r_k = U_{k+1} s_k$.\nThe optimality condition for the projected problem is that its residual is orthogonal to the columns of $B_k$, which means $B_k^\\top s_k = 0$. This gives $k$ linear equations:\n$$\n\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1} = 0 \\quad \\text{for } i=1, \\dots, k\n$$\nNow, we express $g_k$ using the GKB recurrences:\n$$\ng_k = A^\\top r_k = A^\\top U_{k+1} s_k = \\sum_{i=1}^{k+1} (A^\\top u_i) (s_k)_i = \\sum_{i=1}^{k} (A^\\top u_i) (s_k)_i + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nUsing $A^\\top u_i = \\alpha_i v_i + \\beta_i v_{i-1}$ (with $v_0=0$):\n$$\ng_k = \\sum_{i=1}^{k} (\\alpha_i v_i + \\beta_i v_{i-1}) (s_k)_i + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nRearranging the sum by grouping terms with the same $v_i$:\n$$\ng_k = v_1(\\alpha_1 (s_k)_1 + \\beta_2 (s_k)_2) + \\dots + v_{k-1}(\\alpha_{k-1} (s_k)_{k-1} + \\beta_k (s_k)_k) + v_k(\\alpha_k (s_k)_k) + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nFrom the optimality condition $B_k^\\top s_k = 0$, each term $(\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1})$ is zero for $i=1, \\dots, k-1$. The expression simplifies to:\n$$\ng_k = v_k(\\alpha_k (s_k)_k) + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nThe $k$-th equation from $B_k^\\top s_k = 0$ is $\\alpha_k (s_k)_k + \\beta_{k+1} (s_k)_{k+1} = 0$, so $\\alpha_k (s_k)_k = -\\beta_{k+1} (s_k)_{k+1}$.\n$$\ng_k = v_k(-\\beta_{k+1} (s_k)_{k+1}) + (A^\\top u_{k+1}) (s_k)_{k+1} = (s_k)_{k+1} (A^\\top u_{k+1} - \\beta_{k+1} v_k)\n$$\nThe term $(A^\\top u_{k+1} - \\beta_{k+1} v_k)$ is precisely the un-normalized vector $\\tilde{v}_{k+1}$ that would be computed in the next GKB step. Thus, its norm is $\\alpha_{k+1}$.\n$$\ng_k = (s_k)_{k+1} \\tilde{v}_{k+1} = (s_k)_{k+1} (\\alpha_{k+1} v_{k+1})\n$$\nTaking the norm and using $\\|v_{k+1}\\|_2=1$:\n$$\n\\|g_k\\|_2 = \\|A^\\top r_k\\|_2 = |(s_k)_{k+1}| \\alpha_{k+1}\n$$\nThe term $(s_k)_{k+1}$ is the $(k+1)$-th component of $s_k = \\beta_1 e_1 - B_k y_k$. From the structure of $B_k$, the last row of $B_k$ is $(0, \\dots, 0, \\beta_{k+1})$. Thus, $(B_k y_k)_{k+1} = \\beta_{k+1} (y_k)_k$. Since $(\\beta_1 e_1)_{k+1}=0$, we have:\n$$\n(s_k)_{k+1} = 0 - \\beta_{k+1} (y_k)_k = -\\beta_{k+1} (y_k)_k\n$$\nSubstituting this gives the final estimator:\n$$\n\\|A^\\top r_k\\|_{\\mathrm{est}} = |-\\beta_{k+1} (y_k)_k| \\alpha_{k+1} = \\alpha_{k+1} \\beta_{k+1} |(y_k)_k|\n$$\nThis estimator depends on $\\alpha_{k+1}$ (requiring one half-step beyond step $k$), $\\beta_{k+1}$ (from step $k$), and the last component of the solution $y_k$ of the projected problem.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Small constant to prevent division by zero in relative error calculation.\n    EPSILON = np.finfo(float).eps\n\n    def create_hilbert_matrix(n):\n        \"\"\"Creates an n x n Hilbert matrix.\"\"\"\n        # A_ij = 1 / (i + j - 1) for 1-based indexing.\n        # This is 1 / (i_0 + j_0 + 1) for 0-based indexing.\n        return scipy.linalg.hilbert(n)\n\n    def create_vandermonde_matrix(n):\n        \"\"\"Creates an n x n Vandermonde matrix with nodes in [0, 1].\"\"\"\n        t = np.linspace(0, 1, n)\n        # increasing=True gives columns t^0, t^1, ..., t^(n-1)\n        return np.vander(t, increasing=True)\n\n    def create_exp_decay_sv_matrix(m, n, seed):\n        \"\"\"\n        Creates an m x n matrix with exponentially decaying singular values.\n        A = U @ Sigma @ V.T\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Create U (m x n) with orthonormal columns\n        rand_U_mat = rng.standard_normal((m, n))\n        U, _ = np.linalg.qr(rand_U_mat)\n        \n        # Create V (n x n) orthogonal matrix\n        rand_V_mat = rng.standard_normal((n, n))\n        V, _ = np.linalg.qr(rand_V_mat)\n        \n        # Create Sigma with singular values sigma_i = 10^(-i/3) for i=1,...,n\n        i = np.arange(1, n + 1)\n        sigmas = 10**(-i / 3.0)\n        Sigma = np.diag(sigmas)\n        \n        return U @ Sigma @ V.T\n\n    def run_gkb_test(A, b, k):\n        \"\"\"\n        Performs k steps of GKB, computes estimators and actual norms, \n        and returns the relative errors.\n        \"\"\"\n        m, n = A.shape\n        \n        # Storage for GKB quantities (using lists for dynamic length)\n        u_vectors = [np.zeros(m)]  # u_1, u_2, ..., u_{k+1}\n        v_vectors = []           # v_1, v_2, ..., v_k\n        alphas = []\n        betas = []\n\n        # --- Step 1: Golub-Kahan Bidiagonalization ---\n        # Initialization\n        beta_1 = np.linalg.norm(b)\n        if np.isclose(beta_1, 0.0):\n            return 0.0, 0.0 # Trivial case\n            \n        u_vectors[0] = b / beta_1\n        betas.append(beta_1)\n        v_prev = np.zeros(n)\n\n        # Main loop for k steps\n        for i in range(k):\n            # Update V\n            p = A.T @ u_vectors[i] - betas[i] * v_prev\n            alpha_i = np.linalg.norm(p)\n            v_i = p / alpha_i\n            \n            # Update U\n            q = A @ v_i - alpha_i * u_vectors[i]\n            beta_i_plus_1 = np.linalg.norm(q)\n            u_i_plus_1 = q / beta_i_plus_1\n\n            # Store results\n            alphas.append(alpha_i)\n            betas.append(beta_i_plus_1)\n            v_vectors.append(v_i)\n            u_vectors.append(u_i_plus_1)\n\n            v_prev = v_i\n\n        # Compute alpha_{k+1} for the gradient norm estimator\n        p_k_plus_1 = A.T @ u_vectors[k] - betas[k] * v_vectors[k-1]\n        alpha_k_plus_1 = np.linalg.norm(p_k_plus_1)\n\n        # --- Step 2: Solve Projected Least-Squares Problem ---\n        # Construct the (k+1) x k bidiagonal matrix B_k\n        B_k = np.zeros((k + 1, k))\n        np.fill_diagonal(B_k, alphas)\n        for i in range(k):\n            B_k[i+1, i] = betas[i+1]\n            \n        # Set up the right-hand side for the small problem\n        rhs_small = np.zeros(k + 1)\n        rhs_small[0] = beta_1\n        \n        # Solve min ||B_k * y - rhs_small||_2\n        y_k, residuals, _, _ = np.linalg.lstsq(B_k, rhs_small, rcond=None)\n\n        # --- Step 3: Compute Estimators ---\n        # ||r_k||_est is the residual norm of the small problem\n        # lstsq returns sum-of-squares, so take sqrt\n        norm_r_k_est = np.sqrt(residuals[0])\n        \n        # ||A^T r_k||_est = alpha_{k+1} * beta_{k+1} * |(y_k)_k|\n        beta_k_plus_1 = betas[k]\n        y_k_last = y_k[-1]\n        norm_g_k_est = alpha_k_plus_1 * beta_k_plus_1 * np.abs(y_k_last)\n        \n        # --- Step 4: Compute Actual Norms for Validation ---\n        V_k = np.array(v_vectors).T  # Shape: (n, k)\n        x_k = V_k @ y_k\n        \n        # Actual residual r_k = b - A*x_k\n        r_k_act = b - A @ x_k\n        norm_r_k_act = np.linalg.norm(r_k_act)\n        \n        # Actual gradient g_k = A^T * r_k\n        g_k_act = A.T @ r_k_act\n        norm_g_k_act = np.linalg.norm(g_k_act)\n\n        # --- Step 5: Compute Relative Errors ---\n        err_r = np.abs(norm_r_k_est - norm_r_k_act) / max(norm_r_k_act, EPSILON)\n        err_g = np.abs(norm_g_k_est - norm_g_k_act) / max(norm_g_k_act, EPSILON)\n        \n        return err_r, err_g\n\n    # --- Test Suite Definition ---\n    # Using a fixed seed for deterministic pseudorandom vectors\n    rng_seed = 42\n    rng = np.random.default_rng(rng_seed)\n\n    test_cases = [\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": rng.standard_normal(20),\n            \"k\": 5\n        },\n        {\n            \"A\": create_vandermonde_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_exp_decay_sv_matrix(60, 20, seed=rng_seed),\n            \"b\": rng.standard_normal(60),\n            \"k\": 12\n        },\n        {\n            \"A\": create_hilbert_matrix(10),\n            \"b\": np.ones(10),\n            \"k\": 1\n        }\n    ]\n\n    # --- Run all tests and collect results ---\n    results = []\n    for case in test_cases:\n        err_r, err_g = run_gkb_test(case[\"A\"], case[\"b\"], case[\"k\"])\n        results.extend([err_r, err_g])\n    \n    # --- Final Output ---\n    # Convert results to string with specified precision format\n    result_strings = [f\"{res:.8e}\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n\n```"
        }
    ]
}