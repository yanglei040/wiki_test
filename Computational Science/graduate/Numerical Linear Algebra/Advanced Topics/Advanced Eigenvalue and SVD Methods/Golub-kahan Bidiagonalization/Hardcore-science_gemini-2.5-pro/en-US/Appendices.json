{
    "hands_on_practices": [
        {
            "introduction": "Mastering an algorithm begins with understanding its fundamental mechanics. This first practice provides a direct, hands-on experience with the Golub-Kahan bidiagonalization process by having you apply the recurrence relations step-by-step to a small, well-structured matrix . This exercise is designed to build intuition for how the orthonormal bases and the bidiagonal matrix are constructed, while verifying the resulting matrix identities will solidify the connection between the iterative process and its elegant matrix representation.",
            "id": "3548819",
            "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be the lower bidiagonal matrix with main diagonal entries equal to $2$ and subdiagonal entries equal to $1$, that is\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 2\n\\end{pmatrix}.\n$$\nApply three steps of the Golub–Kahan bidiagonalization to $A$ starting from the unit vector $u_1 = e_1 \\in \\mathbb{R}^4$, where at each step you alternately form\n$$\nr_j \\;=\\; A^{T} u_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j \\;=\\; \\|r_j\\|_2, \\quad v_j \\;=\\; r_j/\\alpha_j,\n$$\nfollowed by\n$$\np_j \\;=\\; A v_j - \\alpha_j u_j, \\quad \\beta_j \\;=\\; \\|p_j\\|_2, \\quad u_{j+1} \\;=\\; p_j/\\beta_j,\n$$\nwith the convention $\\beta_0 = 0$ and $v_0$ unused. Construct the matrices $U_3 = [\\,u_1 \\;\\; u_2 \\;\\; u_3\\,] \\in \\mathbb{R}^{4 \\times 3}$, $V_3 = [\\,v_1 \\;\\; v_2 \\;\\; v_3\\,] \\in \\mathbb{R}^{4 \\times 3}$, and the lower bidiagonal matrix $B_3 \\in \\mathbb{R}^{3 \\times 3}$ with diagonal entries $\\alpha_1,\\alpha_2,\\alpha_3$ and subdiagonal entries $\\beta_1,\\beta_2$. Verify numerically that\n$$\nU_3^{T} A V_3 \\;=\\; B_3\n\\quad\\text{and}\\quad\nA V_3 \\;=\\; U_3 B_3 + \\beta_3\\, u_4 e_3^{T},\n$$\nwhere $u_4$ is the next left Lanczos vector produced by the process and $e_3 \\in \\mathbb{R}^{3}$ is the third standard basis vector. Provide the exact value of $\\beta_3$ as your final answer. Express the final answer exactly; no rounding is required.",
            "solution": "The user has requested the application of three steps of the Golub-Kahan bidiagonalization algorithm to a given matrix $A \\in \\mathbb{R}^{4 \\times 4}$, starting with the vector $u_1 = e_1$. The goal is to find the value of $\\beta_3$.\n\nThe given matrix is:\n$$\nA = \\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 2\n\\end{pmatrix}\n$$\nIts transpose is:\n$$\nA^T = \\begin{pmatrix}\n2 & 1 & 0 & 0 \\\\\n0 & 2 & 1 & 0 \\\\\n0 & 0 & 2 & 1 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n$$\nThe starting vector is $u_1 = e_1 = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^T$. The algorithm proceeds in steps for $j=1, 2, 3, \\ldots$ as defined:\n$$\nr_j = A^{T} u_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j = \\|r_j\\|_2, \\quad v_j = r_j/\\alpha_j\n$$\n$$\np_j = A v_j - \\alpha_j u_j, \\quad \\beta_j = \\|p_j\\|_2, \\quad u_{j+1} = p_j/\\beta_j\n$$\nWe start with $\\beta_0 = 0$.\n\n**Step 1 ($j=1$):**\nFirst, we compute $r_1$, $\\alpha_1$, and $v_1$.\n$$\nr_1 = A^T u_1 - \\beta_0 v_0 = A^T e_1 - 0 = \\begin{pmatrix} 2 & 1 & 0 & 0 \\\\ 0 & 2 & 1 & 0 \\\\ 0 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_1 = \\|r_1\\|_2 = \\sqrt{2^2 + 0^2 + 0^2 + 0^2} = 2\n$$\n$$\nv_1 = \\frac{r_1}{\\alpha_1} = \\frac{1}{2} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_1\n$$\nNext, we compute $p_1$, $\\beta_1$, and $u_2$.\n$$\np_1 = A v_1 - \\alpha_1 u_1 = A e_1 - 2 e_1 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\beta_1 = \\|p_1\\|_2 = \\sqrt{0^2 + 1^2 + 0^2 + 0^2} = 1\n$$\n$$\nu_2 = \\frac{p_1}{\\beta_1} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_2\n$$\n\n**Step 2 ($j=2$):**\nWe use the results from Step 1: $u_2=e_2$, $v_1=e_1$, $\\beta_1=1$.\nFirst, $r_2$, $\\alpha_2$, and $v_2$.\n$$\nr_2 = A^T u_2 - \\beta_1 v_1 = A^T e_2 - 1 e_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_2 = \\|r_2\\|_2 = \\sqrt{0^2 + 2^2 + 0^2 + 0^2} = 2\n$$\n$$\nv_2 = \\frac{r_2}{\\alpha_2} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_2\n$$\nNext, $p_2$, $\\beta_2$, and $u_3$.\n$$\np_2 = A v_2 - \\alpha_2 u_2 = A e_2 - 2 e_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\beta_2 = \\|p_2\\|_2 = \\sqrt{0^2 + 0^2 + 1^2 + 0^2} = 1\n$$\n$$\nu_3 = \\frac{p_2}{\\beta_2} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = e_3\n$$\n\n**Step 3 ($j=3$):**\nWe use the results from Step 2: $u_3=e_3$, $v_2=e_2$, $\\beta_2=1$.\nFirst, $r_3$, $\\alpha_3$, and $v_3$.\n$$\nr_3 = A^T u_3 - \\beta_2 v_2 = A^T e_3 - 1 e_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_3 = \\|r_3\\|_2 = \\sqrt{0^2 + 0^2 + 2^2 + 0^2} = 2\n$$\n$$\nv_3 = \\frac{r_3}{\\alpha_3} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = e_3\n$$\nNext, we compute $p_3$, $\\beta_3$, and $u_4$. This step will yield the required value $\\beta_3$.\n$$\np_3 = A v_3 - \\alpha_3 u_3 = A e_3 - 2 e_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\beta_3 = \\|p_3\\|_2 = \\sqrt{0^2 + 0^2 + 0^2 + 1^2} = 1\n$$\n$$\nu_4 = \\frac{p_3}{\\beta_3} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = e_4\n$$\nThe value of $\\beta_3$ is $1$.\n\nThe problem also asks to construct the matrices and verify the relations.\n$U_3 = [u_1, u_2, u_3] = [e_1, e_2, e_3] = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}$.\n$V_3 = [v_1, v_2, v_3] = [e_1, e_2, e_3] = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}$.\n$B_3 = \\begin{pmatrix} \\alpha_1 & 0 & 0 \\\\ \\beta_1 & \\alpha_2 & 0 \\\\ 0 & \\beta_2 & \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$.\n\nVerification 1: $U_3^T A V_3 = B_3$\n$$\nU_3^T A V_3 = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 1 & 2 & 0 & 0 \\\\ 0 & 1 & 2 & 0 \\\\ 0 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 1 & 2 & 0 & 0 \\\\ 0 & 1 & 2 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} = B_3\n$$\nThe first relation holds.\n\nVerification 2: $A V_3 = U_3 B_3 + \\beta_3 u_4 e_3^T$\nLeft-hand side:\n$$\nA V_3 = \\begin{pmatrix} 2 & 0 & 0 & 0 \\\\ 1 & 2 & 0 & 0 \\\\ 0 & 1 & 2 & 0 \\\\ 0 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nRight-hand side:\n$$\nU_3 B_3 + \\beta_3 u_4 e_3^T = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} + (1) \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 2 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe left-hand side equals the right-hand side. The second relation also holds.\n\nThe calculation confirms that $\\beta_3 = 1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "One of the primary applications of Golub-Kahan bidiagonalization is as the engine for iterative methods that solve large-scale least-squares problems, such as LSQR. A crucial feature of these methods is the ability to monitor convergence efficiently, a challenge you will tackle in this practice . You will derive and implement estimators for key quantities like the residual norm $\\\\|r_k\\\\|_2$ and the gradient norm $\\\\|A^\\\\top r_k\\\\|_2$, using only the information from the small projected matrix $B_k$. This demonstrates a core principle of modern numerical linear algebra: using projected information to steer and terminate large-scale computations.",
            "id": "3548830",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{m}$. For iterates $x_k \\in \\mathbb{R}^{n}$ produced by $k$ steps of the Golub–Kahan Bidiagonalization (GKB), define the least-squares residual $r_k = b - A x_k$ and its normal-equation gradient $g_k = A^\\top r_k$. The Golub–Kahan Bidiagonalization (GKB) constructs orthonormal bases $\\{u_i\\}$ and $\\{v_i\\}$ for the Krylov subspaces in $\\mathbb{R}^m$ and $\\mathbb{R}^n$, respectively, together with recurrence scalars $\\{\\alpha_i\\}$ and $\\{\\beta_i\\}$, yielding a lower-bidiagonal matrix $B_k \\in \\mathbb{R}^{(k+1)\\times k}$ that encodes the small projected least-squares problem.\n\nYour tasks are:\n- Starting from the fundamental definitions of the Golub–Kahan Bidiagonalization process and the properties of orthonormal transformations, derive estimators for the norms $\\|r_k\\|_2$ and $\\|A^\\top r_k\\|_2$ that depend solely on the bidiagonal matrix $B_k$ and the recurrence scalars $\\{\\alpha_i\\}$ and $\\{\\beta_i\\}$ produced by GKB, without using $A$ itself in the estimator formulas.\n- Implement a program that performs $k$ steps of GKB to build $B_k$, solves the associated small least-squares problem to obtain $x_k$, and then computes the estimators for $\\|r_k\\|_2$ and $\\|A^\\top r_k\\|_2$ purely from $B_k$ and the recurrence scalars. For validation, compute the actual norms using $A$ and compare them with the estimators.\n- Quantify reliability by reporting the relative errors of the estimators with respect to the actual norms on a set of highly ill-conditioned test problems. Each reported quantity must be a float. No physical units or angle specifications apply.\n\nYou must use the following test suite of matrices and parameters, chosen to probe general cases, ill-conditioning severity, and boundary iteration counts:\n1. $A$ is the $20 \\times 20$ Hilbert matrix with entries $A_{ij} = \\frac{1}{i+j-1}$, $b$ is the all-ones vector in $\\mathbb{R}^{20}$, $k = 10$.\n2. $A$ is the $20 \\times 20$ Hilbert matrix, $b$ is a fixed pseudorandom vector in $\\mathbb{R}^{20}$ (generated deterministically), $k = 5$.\n3. $A$ is the $20 \\times 20$ Vandermonde matrix with nodes $t_i$ linearly spaced in $[0,1]$, columns are powers $t_i^{j-1}$ for $j=1,\\dots,20$, $b$ is the all-ones vector in $\\mathbb{R}^{20}$, $k = 10$.\n4. $A$ has exponentially decaying singular values: construct $A = U \\Sigma V^\\top$ where $U \\in \\mathbb{R}^{60 \\times 20}$ and $V \\in \\mathbb{R}^{20 \\times 20}$ are orthogonal (obtained by the $Q$ factor from a thin Householder-based QR of Gaussian random matrices), and $\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_{20})$ with $\\sigma_i = 10^{-i/3}$; $b$ is a fixed pseudorandom vector in $\\mathbb{R}^{60}$, $k = 12$.\n5. Boundary case: $A$ is the $10 \\times 10$ Hilbert matrix, $b$ is the all-ones vector in $\\mathbb{R}^{10}$, $k = 1$.\n\nFor each test case, compute:\n- The estimator-based residual norm $\\|r_k\\|_2$ and its actual counterpart, and report the relative error defined as $\\left|\\|r_k\\|_{\\mathrm{est}} - \\|r_k\\|_{\\mathrm{act}}\\right| / \\max(\\|r_k\\|_{\\mathrm{act}}, \\varepsilon)$ where $\\varepsilon$ is a small positive constant to avoid division by zero.\n- The estimator-based gradient norm $\\|A^\\top r_k\\|_2$ and its actual counterpart, and report the analogous relative error.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$[\\mathrm{err}_{r}^{(1)}, \\mathrm{err}_{g}^{(1)}, \\mathrm{err}_{r}^{(2)}, \\mathrm{err}_{g}^{(2)}, \\mathrm{err}_{r}^{(3)}, \\mathrm{err}_{g}^{(3)}, \\mathrm{err}_{r}^{(4)}, \\mathrm{err}_{g}^{(4)}, \\mathrm{err}_{r}^{(5)}, \\mathrm{err}_{g}^{(5)}]$,\nwhere $\\mathrm{err}_{r}^{(i)}$ is the residual-norm relative error and $\\mathrm{err}_{g}^{(i)}$ is the gradient-norm relative error for the $i$-th test case.\n\nThe final answer must be runnable code that performs all computations and prints the result in the specified format. No inputs from external sources are permitted, and the run must be self-contained and deterministic.",
            "solution": "The problem requires the derivation and implementation of estimators for the least-squares residual norm, $\\|r_k\\|_2$, and its corresponding normal-equation gradient norm, $\\|A^\\top r_k\\|_2$, using quantities generated by the Golub-Kahan Bidiagonalization (GKB) process.\n\n### The Golub-Kahan Bidiagonalization (GKB) Process\nGiven a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$, the GKB process generates two sequences of orthonormal vectors, $\\{u_i\\}_{i=1}^{k+1} \\subset \\mathbb{R}^m$ and $\\{v_i\\}_{i=1}^{k} \\subset \\mathbb{R}^n$, and two sequences of positive scalars, $\\{\\alpha_i\\}_{i=1}^{k}$ and $\\{\\beta_i\\}_{i=1}^{k+1}$. The process is initialized as follows:\n$$\n\\beta_1 = \\|b\\|_2, \\quad u_1 = b / \\beta_1, \\quad v_0 = 0\n$$\nFor $i=1, \\dots, k$, the recurrences are:\n$$\n\\tilde{v}_i = A^\\top u_i - \\beta_i v_{i-1}, \\quad \\alpha_i = \\|\\tilde{v}_i\\|_2, \\quad v_i = \\tilde{v}_i / \\alpha_i\n$$\n$$\n\\tilde{u}_{i+1} = A v_i - \\alpha_i u_i, \\quad \\beta_{i+1} = \\|\\tilde{u}_{i+1}\\|_2, \\quad u_{i+1} = \\tilde{u}_{i+1} / \\beta_{i+1}\n$$\nLet $U_{k+1} = [u_1, \\dots, u_{k+1}] \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_k = [v_1, \\dots, v_k] \\in \\mathbb{R}^{n \\times k}$. By construction, $U_{k+1}^\\top U_{k+1} = I_{k+1}$ and $V_k^\\top V_k = I_k$. The recurrences can be expressed in matrix form:\n$$\nA V_k = U_{k+1} B_k\n$$\nwhere $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ is a lower-bidiagonal matrix:\n$$\nB_k =\n\\begin{pmatrix}\n\\alpha_1 & & & \\\\\n\\beta_2 & \\alpha_2 & & \\\\\n& \\beta_3 & \\ddots & \\\\\n& & \\ddots & \\alpha_k \\\\\n& & & \\beta_{k+1}\n\\end{pmatrix}\n$$\nThe GKB process provides an orthonormal basis $V_k$ for the Krylov subspace $\\mathcal{K}_k(A^\\top A, A^\\top b)$. The approximate solution $x_k$ to the least-squares problem $\\min_x \\|b-Ax\\|_2$ is sought in this subspace, i.e., $x_k = V_k y_k$ for some $y_k \\in \\mathbb{R}^k$.\n\n### Derivation of the Estimator for $\\|r_k\\|_2$\nThe residual $r_k = b - Ax_k$ can be expressed in terms of the GKB quantities. Substituting $x_k = V_k y_k$ and using the GKB matrix relations:\n$$\nr_k = b - A(V_k y_k) = b - (AV_k)y_k\n$$\nFrom the initialization, $b = \\beta_1 u_1$. Since $U_{k+1}$ is an orthonormal basis that contains $u_1$, we can write $b = U_{k+1}(\\beta_1 e_1)$, where $e_1 \\in \\mathbb{R}^{k+1}$ is the first standard basis vector.\n$$\nr_k = U_{k+1}(\\beta_1 e_1) - (U_{k+1} B_k) y_k = U_{k+1} (\\beta_1 e_1 - B_k y_k)\n$$\nSince $U_{k+1}$ has orthonormal columns, taking the Euclidean norm preserves the length:\n$$\n\\|r_k\\|_2 = \\|U_{k+1} (\\beta_1 e_1 - B_k y_k)\\|_2 = \\|\\beta_1 e_1 - B_k y_k\\|_2\n$$\nThe vector $y_k$ is chosen to minimize $\\|r_k\\|_2$, which is equivalent to solving the small projected least-squares problem:\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\nTherefore, the estimator for the residual norm is the norm of the residual of this small problem, which is calculable solely from $B_k$ and $\\beta_1$. In ideal arithmetic, this is an exact identity, not an estimator. In finite precision, it serves as a robust estimator that is often more accurate than computing $\\|b-Ax_k\\|$ directly.\n$$\n\\|r_k\\|_{\\mathrm{est}} = \\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\n\n### Derivation of the Estimator for $\\|A^\\top r_k\\|_2$\nLet $g_k = A^\\top r_k$ be the normal-equation gradient. Let $s_k = \\beta_1 e_1 - B_k y_k$ be the residual of the projected problem. We have $r_k = U_{k+1} s_k$.\nThe optimality condition for the projected problem is that its residual is orthogonal to the columns of $B_k$, which means $B_k^\\top s_k = 0$. This gives $k$ linear equations:\n$$\n\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1} = 0 \\quad \\text{for } i=1, \\dots, k\n$$\nNow, we express $g_k$ using the GKB recurrences:\n$$\ng_k = A^\\top r_k = A^\\top U_{k+1} s_k = \\sum_{i=1}^{k+1} (A^\\top u_i) (s_k)_i = \\sum_{i=1}^{k} (A^\\top u_i) (s_k)_i + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nUsing $A^\\top u_i = \\alpha_i v_i + \\beta_i v_{i-1}$ (with $v_0=0$):\n$$\ng_k = \\sum_{i=1}^{k} (\\alpha_i v_i + \\beta_i v_{i-1}) (s_k)_i + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nRearranging the sum by grouping terms with the same $v_i$:\n$$\ng_k = v_1(\\alpha_1 (s_k)_1 + \\beta_2 (s_k)_2) + \\dots + v_{k-1}(\\alpha_{k-1} (s_k)_{k-1} + \\beta_k (s_k)_k) + v_k(\\alpha_k (s_k)_k) + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nFrom the optimality condition $B_k^\\top s_k = 0$, each term $(\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1})$ is zero for $i=1, \\dots, k-1$. The expression simplifies to:\n$$\ng_k = v_k(\\alpha_k (s_k)_k) + (A^\\top u_{k+1}) (s_k)_{k+1}\n$$\nThe $k$-th equation from $B_k^\\top s_k = 0$ is $\\alpha_k (s_k)_k + \\beta_{k+1} (s_k)_{k+1} = 0$, so $\\alpha_k (s_k)_k = -\\beta_{k+1} (s_k)_{k+1}$.\n$$\ng_k = v_k(-\\beta_{k+1} (s_k)_{k+1}) + (A^\\top u_{k+1}) (s_k)_{k+1} = (s_k)_{k+1} (A^\\top u_{k+1} - \\beta_{k+1} v_k)\n$$\nThe term $(A^\\top u_{k+1} - \\beta_{k+1} v_k)$ is precisely the un-normalized vector $\\tilde{v}_{k+1}$ that would be computed in the next GKB step. Thus, its norm is $\\alpha_{k+1}$.\n$$\ng_k = (s_k)_{k+1} \\tilde{v}_{k+1} = (s_k)_{k+1} (\\alpha_{k+1} v_{k+1})\n$$\nTaking the norm and using $\\|v_{k+1}\\|_2=1$:\n$$\n\\|g_k\\|_2 = \\|A^\\top r_k\\|_2 = |(s_k)_{k+1}| \\alpha_{k+1}\n$$\nThe term $(s_k)_{k+1}$ is the $(k+1)$-th component of $s_k = \\beta_1 e_1 - B_k y_k$.\n$$\n(s_k)_{k+1} = (\\beta_1 e_1)_{k+1} - (B_k y_k)_{k+1} = 0 - \\beta_{k+1} (y_k)_k = -\\beta_{k+1} (y_k)_k\n$$\nwhere $(y_k)_k$ is the last element of the solution vector $y_k$. Substituting this gives the final estimator:\n$$\n\\|A^\\top r_k\\|_{\\mathrm{est}} = |-\\beta_{k+1} (y_k)_k| \\alpha_{k+1} = \\alpha_{k+1} \\beta_{k+1} |(y_k)_k|\n$$\nThis estimator depends on $\\alpha_{k+1}$ (requiring one half-step beyond step $k$), $\\beta_{k+1}$ (from step $k$), and the last component of the solution $y_k$ of the projected problem.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Small constant to prevent division by zero in relative error calculation.\n    EPSILON = np.finfo(float).eps\n\n    def create_hilbert_matrix(n):\n        \"\"\"Creates an n x n Hilbert matrix.\"\"\"\n        # A_ij = 1 / (i + j - 1) for 1-based indexing.\n        # This is 1 / (i_0 + j_0 + 1) for 0-based indexing.\n        return scipy.linalg.hilbert(n)\n\n    def create_vandermonde_matrix(n):\n        \"\"\"Creates an n x n Vandermonde matrix with nodes in [0, 1].\"\"\"\n        t = np.linspace(0, 1, n)\n        # increasing=True gives columns t^0, t^1, ..., t^(n-1)\n        return np.vander(t, increasing=True)\n\n    def create_exp_decay_sv_matrix(m, n, seed):\n        \"\"\"\n        Creates an m x n matrix with exponentially decaying singular values.\n        A = U @ Sigma @ V.T\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Create U (m x n) with orthonormal columns\n        rand_U_mat = rng.standard_normal((m, n))\n        U, _ = np.linalg.qr(rand_U_mat)\n        \n        # Create V (n x n) orthogonal matrix\n        rand_V_mat = rng.standard_normal((n, n))\n        V, _ = np.linalg.qr(rand_V_mat)\n        \n        # Create Sigma with singular values sigma_i = 10^(-i/3) for i=1,...,n\n        i = np.arange(1, n + 1)\n        sigmas = 10**(-i / 3.0)\n        Sigma = np.diag(sigmas)\n        \n        return U @ Sigma @ V.T\n\n    def run_gkb_test(A, b, k):\n        \"\"\"\n        Performs k steps of GKB, computes estimators and actual norms, \n        and returns the relative errors.\n        \"\"\"\n        m, n = A.shape\n        \n        # Storage for GKB quantities (using lists for dynamic length)\n        u_vectors = [np.zeros(m)]  # u_1, u_2, ..., u_{k+1}\n        v_vectors = []           # v_1, v_2, ..., v_k\n        alphas = []\n        betas = []\n\n        # --- Step 1: Golub-Kahan Bidiagonalization ---\n        # Initialization\n        beta_1 = np.linalg.norm(b)\n        if np.isclose(beta_1, 0.0):\n            return 0.0, 0.0 # Trivial case\n            \n        u_vectors[0] = b / beta_1\n        betas.append(beta_1)\n        v_prev = np.zeros(n)\n\n        # Main loop for k steps\n        for i in range(k):\n            # Update V\n            p = A.T @ u_vectors[i] - betas[i] * v_prev\n            alpha_i = np.linalg.norm(p)\n            v_i = p / alpha_i\n            \n            # Update U\n            q = A @ v_i - alpha_i * u_vectors[i]\n            beta_i_plus_1 = np.linalg.norm(q)\n            u_i_plus_1 = q / beta_i_plus_1\n\n            # Store results\n            alphas.append(alpha_i)\n            betas.append(beta_i_plus_1)\n            v_vectors.append(v_i)\n            u_vectors.append(u_i_plus_1)\n\n            v_prev = v_i\n\n        # Compute alpha_{k+1} for the gradient norm estimator\n        p_k_plus_1 = A.T @ u_vectors[k] - betas[k] * v_vectors[k-1]\n        alpha_k_plus_1 = np.linalg.norm(p_k_plus_1)\n\n        # --- Step 2: Solve Projected Least-Squares Problem ---\n        # Construct the (k+1) x k bidiagonal matrix B_k\n        B_k = np.zeros((k + 1, k))\n        np.fill_diagonal(B_k, alphas)\n        for i in range(k):\n            B_k[i+1, i] = betas[i+1]\n            \n        # Set up the right-hand side for the small problem\n        rhs_small = np.zeros(k + 1)\n        rhs_small[0] = beta_1\n        \n        # Solve min ||B_k * y - rhs_small||_2\n        y_k, residuals, _, _ = np.linalg.lstsq(B_k, rhs_small, rcond=None)\n\n        # --- Step 3: Compute Estimators ---\n        # ||r_k||_est is the residual norm of the small problem\n        # lstsq returns sum-of-squares, so take sqrt\n        norm_r_k_est = np.sqrt(residuals[0])\n        \n        # ||A^T r_k||_est = alpha_{k+1} * beta_{k+1} * |(y_k)_k|\n        beta_k_plus_1 = betas[k]\n        y_k_last = y_k[-1]\n        norm_g_k_est = alpha_k_plus_1 * beta_k_plus_1 * np.abs(y_k_last)\n        \n        # --- Step 4: Compute Actual Norms for Validation ---\n        V_k = np.array(v_vectors).T  # Shape: (n, k)\n        x_k = V_k @ y_k\n        \n        # Actual residual r_k = b - A*x_k\n        r_k_act = b - A @ x_k\n        norm_r_k_act = np.linalg.norm(r_k_act)\n        \n        # Actual gradient g_k = A^T * r_k\n        g_k_act = A.T @ r_k_act\n        norm_g_k_act = np.linalg.norm(g_k_act)\n\n        # --- Step 5: Compute Relative Errors ---\n        err_r = np.abs(norm_r_k_est - norm_r_k_act) / max(norm_r_k_act, EPSILON)\n        err_g = np.abs(norm_g_k_est - norm_g_k_act) / max(norm_g_k_act, EPSILON)\n        \n        return err_r, err_g\n\n    # --- Test Suite Definition ---\n    # Using a fixed seed for deterministic pseudorandom vectors\n    rng_seed = 42\n    rng = np.random.default_rng(rng_seed)\n\n    test_cases = [\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": rng.standard_normal(20),\n            \"k\": 5\n        },\n        {\n            \"A\": create_vandermonde_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_exp_decay_sv_matrix(60, 20, seed=rng_seed),\n            \"b\": rng.standard_normal(60),\n            \"k\": 12\n        },\n        {\n            \"A\": create_hilbert_matrix(10),\n            \"b\": np.ones(10),\n            \"k\": 1\n        }\n    ]\n\n    # --- Run all tests and collect results ---\n    results = []\n    for case in test_cases:\n        err_r, err_g = run_gkb_test(case[\"A\"], case[\"b\"], case[\"k\"])\n        results.extend([err_r, err_g])\n    \n    # --- Final Output ---\n    # Convert results to string with specified precision format\n    result_strings = [f\"{res:.8e}\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Beyond standard least-squares, Golub-Kahan bidiagonalization offers a powerful framework for addressing ill-posed inverse problems, where solutions are extremely sensitive to noise. This practice demonstrates how to incorporate Tikhonov regularization directly into the bidiagonalization process by solving a much smaller, projected version of the regularized problem . You will derive this projected problem and then use its spectral properties—its singular value decomposition—to implement a parameter-choice strategy, connecting the iterative process of GKB with the sophisticated theory of regularization.",
            "id": "3548852",
            "problem": "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a right-hand side $b \\in \\mathbb{R}^{m}$, and recall the Golub-Kahan (GK) bidiagonalization process started at $u_{1} = b / \\|b\\|_{2}$, which constructs orthonormal matrices $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_{k} \\in \\mathbb{R}^{n \\times k}$ together with a lower bidiagonal matrix $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ so that $A V_{k}$ is represented on $U_{k+1}$ by $B_{k}$. In classical Tikhonov regularization, one seeks $x$ that minimizes the quadratic functional $\\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$ for a regularization parameter $\\lambda > 0$. In a GK-based projection, one restricts $x$ to the $k$-dimensional subspace $\\mathrm{span}(V_{k})$ by writing $x = V_{k} y$ and studies the corresponding projected Tikhonov problem in the $y$-variables.\n\nStarting from the definitions above and the standard properties of orthogonal projectors and least squares problems:\n1. Derive the $k$-dimensional projected Tikhonov problem in terms of $B_{k}$ and $\\beta e_{1}$, where $\\beta = \\|b\\|_{2}$ and $e_{1} \\in \\mathbb{R}^{k+1}$ is the first canonical basis vector. Express the projected problem as an augmented least squares problem with an appropriate block matrix and right-hand side.\n2. Using only fundamental facts about the singular value decomposition (SVD) and orthogonal transformations, derive the spectral form of the projected Tikhonov solution and an explicit expression for the residual norm $\\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}$ in terms of the singular values and the spectral coefficients of $\\beta e_{1}$ with respect to the left singular vectors of $B_{k}$.\n\nNow specialize to $k = 2$ and suppose that after two GK steps the bidiagonal matrix is\n$$\nB_{2} = \\begin{pmatrix}\n3 & 0 \\\\\n4 & 0 \\\\\n0 & 5\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n$$\nand the data have norm $\\beta = \\|b\\|_{2} = 10$. Study parameter choice by imposing the Morozov discrepancy principle on the projected problem: choose $\\lambda > 0$ so that the projected residual satisfies $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2} = \\delta$ with a prescribed noise level $\\delta = \\sqrt{73}$. Use the singular value decomposition of $B_{2}$ to compute the unique positive $\\lambda$ that meets the discrepancy principle for the projected problem with the given $B_{2}$ and $\\beta$.\n\nRound your final numerical answer for $\\lambda$ to four significant figures. No units are required.",
            "solution": "The problem consists of three parts: deriving the projected Tikhonov problem, expressing its solution and residual norm in spectral form, and finally solving for a specific regularization parameter $\\lambda$.\n\n### 1. Derivation of the Projected Tikhonov Problem\nThe Tikhonov functional is $J(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$. We project the solution $x$ onto the $k$-dimensional Krylov subspace $\\mathrm{span}(V_{k})$ by setting $x = V_{k} y$, where $y \\in \\mathbb{R}^{k}$.\nSubstituting this into the functional gives:\n$$ J(y) = \\|A V_{k} y - b\\|_{2}^{2} + \\lambda^{2} \\|V_{k} y\\|_{2}^{2} $$\nUsing the properties of the Golub-Kahan bidiagonalization, we have $A V_{k} = U_{k+1} B_{k}$. The matrix $V_{k}$ has orthonormal columns, so $\\|V_{k} y\\|_{2}^{2} = y^{T} V_{k}^{T} V_{k} y = y^{T} I_{k} y = \\|y\\|_{2}^{2}$. The starting vector for the bidiagonalization is $u_{1} = b/\\|b\\|_{2}$, which implies $b = \\|b\\|_{2} u_{1}$. We are given $\\beta = \\|b\\|_{2}$. Since $u_{1}$ is the first column of the orthonormal matrix $U_{k+1}$, we can write $u_{1} = U_{k+1} e_{1}$, where $e_{1} \\in \\mathbb{R}^{k+1}$ is the first canonical basis vector. Thus, $b = \\beta U_{k+1} e_{1}$.\nSubstituting these relations into the functional:\n$$ J(y) = \\|U_{k+1} B_{k} y - \\beta U_{k+1} e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\nSince $U_{k+1}$ is a matrix with orthonormal columns, multiplication by it preserves the Euclidean norm, i.e., $\\|U_{k+1}z\\|_{2} = \\|z\\|_{2}$. Applying this property, we get the projected Tikhonov functional:\n$$ J(y) = \\|B_{k} y - \\beta e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\nThis expression is to be minimized with respect to $y \\in \\mathbb{R}^{k}$. This is a linear least squares problem with Tikhonov regularization. It can be written as an unregularized (or augmented) least squares problem:\n$$ \\min_{y} \\left\\| \\begin{pmatrix} B_{k} \\\\ \\lambda I_{k} \\end{pmatrix} y - \\begin{pmatrix} \\beta e_{1} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} $$\nwhere the augmented matrix is in $\\mathbb{R}^{(2k+1) \\times k}$ and the augmented right-hand side is in $\\mathbb{R}^{2k+1}$.\n\n### 2. Spectral Form of the Solution and Residual Norm\nThe solution $y(\\lambda)$ that minimizes $J(y)$ is found by solving the normal equations:\n$$ (B_{k}^{T} B_{k} + \\lambda^{2} I_{k}) y = B_{k}^{T} (\\beta e_{1}) $$\nLet the singular value decomposition (SVD) of $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ be $B_{k} = P \\Sigma Q^{T}$, where $P \\in \\mathbb{R}^{(k+1) \\times (k+1)}$ and $Q \\in \\mathbb{R}^{k \\times k}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{(k+1) \\times k}$ is a rectangular diagonal matrix with singular values $\\sigma_{1}, \\dots, \\sigma_{k}$ on its main diagonal. The residual is $r(\\lambda) = B_{k} y(\\lambda) - \\beta e_{1}$. Let $\\gamma = P^{T} (\\beta e_{1})$; its components are $\\gamma_{j} = \\beta p_{j}^{T} e_{1} = \\beta (p_{j})_{1}$, where $p_j$ are the columns of $P$. The residual vector can be expressed in the basis of the left singular vectors $p_j$ as:\n$$ r(\\lambda) = \\sum_{j=1}^{k} \\left( \\frac{\\sigma_{j}^{2}}{\\sigma_{j}^{2} + \\lambda^{2}} - 1 \\right) \\gamma_{j} p_{j} - \\gamma_{k+1} p_{k+1} = \\sum_{j=1}^{k} \\frac{-\\lambda^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} p_{j} - \\gamma_{k+1} p_{k+1} $$\nSince the vectors $p_{j}$ are orthonormal, the squared norm of the residual is the sum of the squares of the coefficients:\n$$ \\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\sum_{j=1}^{k} \\left( \\frac{\\lambda^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{k+1}^{2} $$\n\n### 3. Calculation of $\\lambda$\nWe specialize to $k=2$, with $B_{2} = \\begin{pmatrix} 3 & 0 \\\\ 4 & 0 \\\\ 0 & 5 \\end{pmatrix}$, $\\beta = 10$, and $\\delta = \\sqrt{73}$. The discrepancy principle requires $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\delta^{2} = 73$.\nFirst, we compute the SVD of $B_{2}$. We need the eigenvalues and eigenvectors of $B_{2}^{T} B_{2}$:\n$$ B_{2}^{T} B_{2} = \\begin{pmatrix} 3 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 3 & 0 \\\\ 4 & 0 \\\\ 0 & 5 \\end{pmatrix} = \\begin{pmatrix} 9+16 & 0 \\\\ 0 & 25 \\end{pmatrix} = \\begin{pmatrix} 25 & 0 \\\\ 0 & 25 \\end{pmatrix} $$\nThe eigenvalues are $\\sigma_{1}^{2} = 25$ and $\\sigma_{2}^{2} = 25$, so the singular values are $\\sigma_{1} = 5$ and $\\sigma_{2} = 5$. The matrix of right singular vectors $Q$ is the identity matrix $Q = I_{2}$.\nThe left singular vectors $p_{1}, p_{2}$ are given by $p_{j} = \\frac{1}{\\sigma_{j}} B_{2} q_{j}$.\n$$ p_{1} = \\frac{1}{5} \\begin{pmatrix} 3 & 0 \\\\ 4 & 0 \\\\ 0 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3/5 \\\\ 4/5 \\\\ 0 \\end{pmatrix} $$\n$$ p_{2} = \\frac{1}{5} \\begin{pmatrix} 3 & 0 \\\\ 4 & 0 \\\\ 0 & 5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{5} \\begin{pmatrix} 0 \\\\ 0 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\nThe third left singular vector $p_3$ must be orthogonal to $p_1$ and $p_2$ and have unit norm. We choose $p_{3} = \\begin{pmatrix} 4/5 \\\\ -3/5 \\\\ 0 \\end{pmatrix}$.\nThe spectral coefficients $\\gamma_{j}$ are $\\gamma_{j} = \\beta (p_{j})_{1}$. With $\\beta=10$:\n$$ \\gamma_{1} = 10 \\cdot (p_{1})_{1} = 10 \\cdot \\frac{3}{5} = 6 $$\n$$ \\gamma_{2} = 10 \\cdot (p_{2})_{1} = 10 \\cdot 0 = 0 $$\n$$ \\gamma_{3} = 10 \\cdot (p_{3})_{1} = 10 \\cdot \\frac{4}{5} = 8 $$\nNow we substitute these values into the residual norm equation:\n$$ 73 = \\left( \\frac{\\lambda^{2} \\gamma_{1}}{\\sigma_{1}^{2} + \\lambda^{2}} \\right)^{2} + \\left( \\frac{\\lambda^{2} \\gamma_{2}}{\\sigma_{2}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{3}^{2} $$\n$$ 73 = \\left( \\frac{\\lambda^{2} \\cdot 6}{25 + \\lambda^{2}} \\right)^{2} + \\left( \\frac{\\lambda^{2} \\cdot 0}{25 + \\lambda^{2}} \\right)^{2} + 8^{2} $$\n$$ 73 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} + 0 + 64 $$\n$$ 73 - 64 = 9 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} $$\nDividing by $36$, we get:\n$$ \\frac{9}{36} = \\frac{1}{4} = \\left(\\frac{\\lambda^{2}}{25 + \\lambda^{2}}\\right)^{2} $$\nTaking the square root of both sides (and noting that $\\lambda>0$ ensures the term in parenthesis is positive):\n$$ \\frac{1}{2} = \\frac{\\lambda^{2}}{25 + \\lambda^{2}} $$\n$$ 25 + \\lambda^{2} = 2\\lambda^{2} $$\n$$ \\lambda^{2} = 25 $$\nSince the regularization parameter $\\lambda$ must be positive, we take the positive root:\n$$ \\lambda = 5 $$\nThe problem asks for the answer to be rounded to four significant figures. Thus, $\\lambda = 5.000$.",
            "answer": "$$\\boxed{5.000}$$"
        }
    ]
}