{
    "hands_on_practices": [
        {
            "introduction": "理论的优雅必须与计算的效率相平衡，Jacobi-Davidson (JD) 方法的核心便体现了这一原则。该方法涉及外部迭代（用于更新特征对）和内部迭代（用于求解校正方程），其整体性能取决于在这两个嵌套循环之间取得明智的平衡。本练习  引导您探索 JD 方法的“非精确牛顿法”本质，通过分析精确求解校正方程与更积极地使用中等精度方向扩展子空间之间的计算权衡，您将掌握为何在迭代初期“过度求解”内部问题通常是低效的，从而获得设计高性能 JD 求解器的关键策略。",
            "id": "3590386",
            "problem": "考虑一个埃尔米特矩阵 $A \\in \\mathbb{C}^{n \\times n}$，其中 $n$ 很大，以及一个与简单的、分离良好的特征值相关的目标特征对 $(\\lambda, x)$。Jacobi-Davidson (JD) 方法通过从 JD 校正方程获得的校正方向 $t$ 重复扩展来构建搜索子空间，并使用 Rayleigh-Ritz 过程提取 Ritz 对 $(\\theta, u)$。设外迭代 $j$ 处的残差为 $r_j = A u_j - \\theta_j u_j$，并假设使用预处理 Krylov 方法（例如，对于合适的校正算子使用预处理共轭梯度法 (PCG)，或对于对称不定情况使用预处理最小残差法 (PMINRES)）来近似求解校正方程，其中内部线性系统的残差容忍度参数表示为 $\\eta_j$。\n\n假设：\n- 预处理校正算子的条件数 $\\kappa$ 大小适中（比如 $\\kappa \\approx 400$），因此内部 Krylov 迭代次数的伸缩方式与良好预处理的埃尔米特系统的经典界限一致。\n- 外迭代中的正交化和 Rayleigh-Ritz 投影产生的计算成本分别近似按 $O(n k_j)$ 和 $O(k_j^3)$ 缩放，其中 $k_j$ 是当前的子空间维度。\n\n要求您分析在选择内部容忍度 $\\eta_j$ 的两种策略之间的计算权衡：\n- 策略 $\\mathrm{H}$ (高精度)：对所有 $j$ 强制执行一个固定的严格容忍度 $\\eta_j \\equiv 10^{-10}$。\n- 策略 $\\mathrm{M}$ (中等精度伴随积极扩展)：强制执行一个与当前外残差成比例的容忍度 $\\eta_j \\equiv c \\lVert r_j \\rVert$（其中 $c = 0.1$），随着 $\\lVert r_j \\rVert$ 的减小而自动收紧。\n\n假设初始外残差范数为 $\\lVert r_0 \\rVert = 10^{-2}$，目标精度为达到 $\\lVert r_j \\rVert \\le 10^{-8}$。请不要使用 Jacobi-Davidson 方法的快捷公式，而是从 Rayleigh 商、投影校正方程和不精确类牛顿行为的定义出发进行推理，以确定下面关于这两种策略的预期计算效率和收敛行为的陈述中哪些是正确的。选择所有适用的选项。\n\nA. 选择与 $\\lVert r_j \\rVert$ 成比例的 $\\eta_j$ 可以保持外迭代的二次局部收敛性，并且与使用固定的严格容忍度相比，可以减少总的内部工作量，特别是当预处理校正算子的条件数 $\\kappa \\gg 1$ 时。\n\nB. 内部 Krylov 迭代次数线性依赖于 $1/\\eta_j$，因此 $\\eta_j = 10^{-10}$ 的容忍度意味着每个外迭代步需要大约 $10^{10}$ 次内迭代；因此，高精度的内求解总是代价过高而不可行。\n\nC. 由于正交化和 Rayleigh-Ritz 的成本随子空间维度的增长而增长，外扩展次数的任何增加（例如，从 2 次增加到 3 次）必然会超过由较宽松的内部容忍度带来的节省，使得固定的高精度在总运行时间上总是更优。\n\nD. 在 Jacobi-Davidson 的不精确类牛顿模型下，可以预期在解附近存在一个形如 $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b \\eta_j \\lVert r_j \\rVert$ 的递推关系，其中 $a, b > 0$ 是常数。对于 $\\eta_j = 10^{-10}$，策略 H 从 $\\lVert r_0 \\rVert = 10^{-2}$ 开始，大约需要 2 个外迭代步达到 $\\lVert r_j \\rVert \\le 10^{-8}$；而对于 $\\eta_j = 0.1 \\lVert r_j \\rVert$，如果内部迭代次数按 $s \\log(1/\\eta_j)$ 的方式伸缩（其中 $s \\approx \\sqrt{\\kappa} \\approx 20$），策略 M 大约需要 3 个外迭代步，但总的内部 Krylov 迭代次数更少。这支持了早期采用中等精度的内求解，后期再收紧，作为一种计算上有利的权衡。",
            "solution": "该问题要求分析 Jacobi-Davidson (JD) 方法在设置内部求解器容忍度 $\\eta_j$ 的两种不同策略下的计算效率和收敛行为。\n\n### 基于原理的推导\n\nJacobi-Davidson 方法在每个外迭代 $j$ 中构建一个近似特征对 $(\\theta_j, u_j)$，并寻求一个校正量 $t_j$ 来改善这个近似。校正量 $t_j$ 是通过近似求解 Jacobi-Davidson 校正方程得到的：\n$$ (I - u_j u_j^*)(A - \\theta_j I)(I - u_j u_j^*) t_j = -r_j, \\quad \\text{with } t_j \\perp u_j $$\n其中 $r_j = A u_j - \\theta_j u_j$ 是 Ritz 对的残差。这个关于 $t_j$ 的线性系统通常使用迭代 Krylov 方法（如 PCG 或 PMINRES）求解，直到达到一定的精度。\n\nJD 方法可以被分析为一种不精确牛顿法。这类方法的收敛性由以下一般关系描述，该关系将步骤 $j+1$ 的误差（或残差）与步骤 $j$ 的误差以及线性系统求解的不精确性联系起来：\n$$ \\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b \\lVert \\delta_j \\rVert $$\n其中 $a$ 和 $b$ 是取决于问题性质（例如，条件数和算子范数）的正常数，而 $\\delta_j$ 是内部线性求解的残差。问题陈述一个预处理 Krylov 方法被用来求解校正方程，其相对容忍度为 $\\eta_j$。这意味着内部求解器在其残差最多为其初始残差的 $\\eta_j$ 倍时终止。校正方程的初始残差是 $-r_j$。因此，最终的内部残差范数 $\\lVert \\delta_j \\rVert$ 受 $\\eta_j \\lVert r_j \\rVert$ 限制。将此代入收敛关系式可得：\n$$ \\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b' \\eta_j \\lVert r_j \\rVert $$\n对于某个常数 $b'$。这就是选项 D 中提供的递推模型。\n\n一个预处理 Krylov 方法（如 PCG）将残差减小一个因子 $\\epsilon$ 所需的迭代次数，经典地由一个函数界定，该函数与预处理算子条件数 $\\kappa$ 的平方根以及期望的缩减因子 $1/\\epsilon$ 的对数成正比。如果内部求解器运行直到相对残差为 $\\eta_j$，则外迭代步 $j$ 的内部迭代次数 $k_{\\text{inner}, j}$ 近似为：\n$$ k_{\\text{inner}, j} \\approx C \\sqrt{\\kappa} \\log(1/\\eta_j) $$\n对于某个常数 $C$。问题给出 $\\kappa \\approx 400$，所以 $\\sqrt{\\kappa} \\approx 20$。这种对数依赖关系至关重要。\n\n给定初始残差范数 $\\lVert r_0 \\rVert = 10^{-2}$，目标是 $\\lVert r_j \\rVert \\le 10^{-8}$。让我们使用这些原理来分析这两种策略。\n\n**策略 H (高精度)：$\\eta_j \\equiv 10^{-10}$**\n-   **收敛性：** 递推关系变为 $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b' (10^{-10}) \\lVert r_j \\rVert$。由于 $\\lVert r_j \\rVert$ 从 $10^{-2}$ 开始减小，项 $b' (10^{-10}) \\lVert r_j \\rVert$ 远小于二次项 $a \\lVert r_j \\rVert^2$。例如，在第一步，我们比较 $a(10^{-2})^2 = a \\cdot 10^{-4}$ 和 $b' \\cdot 10^{-10} \\cdot 10^{-2} = b' \\cdot 10^{-12}$。收敛实际上是二次的。\n    -   $j=0$: $\\lVert r_0 \\rVert = 10^{-2}$。\n    -   $j=1$: $\\lVert r_1 \\rVert \\approx a \\lVert r_0 \\rVert^2 = a (10^{-2})^2 = a \\cdot 10^{-4}$。\n    -   $j=2$: $\\lVert r_2 \\rVert \\approx a \\lVert r_1 \\rVert^2 \\approx a (a \\cdot 10^{-4})^2 = a^3 \\cdot 10^{-8}$。假设 $a \\approx 1$，我们大约在 2 次外迭代后达到目标容忍度。\n-   **工作量：** 每个外迭代步的内迭代次数是恒定且高的：$k_{\\text{inner, H}} \\approx C \\sqrt{400} \\log(1/10^{-10}) = C \\cdot 20 \\cdot (10 \\ln 10) \\approx 460 \\cdot C$。如果我们遵循选项 D 中的伸缩关系 $k_{\\text{inner}} \\approx s \\log_{10}(1/\\eta_j)$ 且 $s=20$，那么 $k_{\\text{inner, H}} \\approx 20 \\log_{10}(10^{10}) = 200$。总的内部工作量是 $2 \\times 200 = 400$ 个缩放单位。\n\n**策略 M (中等精度)：$\\eta_j \\equiv 0.1 \\lVert r_j \\rVert$**\n-   **收敛性：** 递推关系变为 $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b' (0.1 \\lVert r_j \\rVert) \\lVert r_j \\rVert = (a + 0.1b') \\lVert r_j \\rVert^2$。收敛仍然是二次的，但常数可能更大 $a' = a + 0.1 b'$，这意味着与纯二次收敛相比，每一步残差范数减小的因子更小。\n    -   $j=0$: $\\lVert r_0 \\rVert = 10^{-2}$。\n    -   $j=1$: $\\lVert r_1 \\rVert \\approx a' \\lVert r_0 \\rVert^2 = a'(10^{-2})^2 = a' \\cdot 10^{-4}$。\n    -   $j=2$: $\\lVert r_2 \\rVert \\approx a' \\lVert r_1 \\rVert^2 \\approx a'(a' \\cdot 10^{-4})^2 = (a')^3 \\cdot 10^{-8}$。如果 $a' > 1$（例如，如选项 D 的分析中 $a'=1.1$），那么 $\\lVert r_2 \\rVert$ 可能会略大于 $10^{-8}$，需要第三步。我们假设需要 3 次外迭代。\n-   **工作量：** 内迭代次数在每一步都会改变。使用 $s=20$：\n    -   $j=0$: $\\eta_0 = 0.1 \\lVert r_0 \\rVert = 0.1(10^{-2}) = 10^{-3}$。$k_{\\text{inner}, 0} \\approx 20 \\log_{10}(10^3) = 60$。\n    -   $j=1$: $\\lVert r_1 \\rVert \\approx a' \\cdot 10^{-4}$。$\\eta_1 = 0.1 \\lVert r_1 \\rVert \\approx 0.1 a' \\cdot 10^{-4}$。$k_{\\text{inner}, 1} \\approx 20 \\log_{10}(1/(0.1 a' \\cdot 10^{-4})) \\approx 20 \\log_{10}(10^5/a') \\approx 20 \\cdot 5 = 100$。\n    -   $j=2$: $\\lVert r_2 \\rVert \\approx (a')^3 \\cdot 10^{-8}$。$\\eta_2=0.1 \\lVert r_2 \\rVert \\approx 0.1 (a')^3 \\cdot 10^{-8}$。$k_{\\text{inner}, 2} \\approx 20 \\log_{10}(1/(0.1 (a')^3 \\cdot 10^{-8})) \\approx 20 \\cdot 9 = 180$。\n    -   总内部工作量约为 $60 + 100 + 180 = 340$ 个缩放单位。\n\n这个分析表明，策略 M 执行的总内迭代次数更少（$340$ vs $400$），尽管它需要更多的外迭代（$3$ vs $2$）。\n\n### 逐项分析\n\n**A. 选择与 $\\lVert r_j \\rVert$ 成比例的 $\\eta_j$ 可以保持外迭代的二次局部收敛性，并且与使用固定的严格容忍度相比，可以减少总的内部工作量，特别是当预处理校正算子的条件数 $\\kappa \\gg 1$ 时。**\n-   保持二次收敛性：如上所示，设置 $\\eta_j = c \\lVert r_j \\rVert$ 会得到递推关系 $\\lVert r_{j+1} \\rVert \\le (a+b'c) \\lVert r_j \\rVert^2$，这表明了二次收敛性。\n-   减少总内部工作量：我们的分析表明，通过在早期阶段避免“过度求解”（例如，第一步中 60 次迭代 vs 200 次迭代），即使需要额外的外迭代，总的内迭代次数也可以减少。\n-   $\\kappa$ 的作用：内迭代的节省量与 $\\sqrt{\\kappa}$ 成比例。如果 $\\kappa$ 很大，$\\sqrt{\\kappa}$ 也很大，$\\log(1/\\eta_{\\text{loose}})$ 和 $\\log(1/\\eta_{\\text{tight}})$ 之间的差异会转化为大量的节省迭代次数。因此，对于较大的 $\\kappa$，这种好处更加明显。\n-   结论：**正确**\n\n**B. 内部 Krylov 迭代次数线性依赖于 $1/\\eta_j$，因此 $\\eta_j = 10^{-10}$ 的容忍度意味着每个外迭代步需要大约 $10^{10}$ 次内迭代；因此，高精度的内求解总是代价过高而不可行。**\n-   Krylov 迭代次数线性依赖于 $1/\\eta_j$ 的前提对于应用于埃尔米特系统的 PCG 或 PMINRES 等方法是根本错误的。其依赖关系是对数的，即 $O(\\log(1/\\eta_j))$。\n-   线性依赖将意味着成本为 $O(1/\\eta_j)$，这对于某些范数下的梯度下降等慢得多的方法是成立的，但对于指定的现代 Krylov 求解器则不成立。\n-   由于是对数依赖关系，容忍度为 $\\eta_j = 10^{-10}$ 且 $\\sqrt{\\kappa} \\approx 20$ 时，会导致几百次迭代，而不是 $10^{10}$ 次。这个数量在计算上是可行的。\n-   结论：**错误**\n\n**C. 由于正交化和 Rayleigh-Ritz 的成本随子空间维度的增长而增长，外扩展次数的任何增加（例如，从 2 次增加到 3 次）必然会超过由较宽松的内部容忍度带来的节省，使得固定的高精度在总运行时间上总是更优。**\n-   这个选项声称额外一次外迭代的开销 *必然* 超过内部求解器带来的任何节省。这是一个过于强烈的断言。\n-   每个外迭代步的主要成本通常是内部 Krylov 求解，它涉及许多矩阵-向量乘积，每个乘积的成本为 $O(n \\cdot \\text{nnz_per_row})$。正交化 ($O(nk_j)$) 和 Rayleigh-Ritz 矩阵构建 ($O(nk_j^2)$) 成本高昂，但它们随子空间维度 $k_j$ 增长，而 $k_j$ 通常很小。\n-   在我们的分析中，内部工作量的节省是 $(200-60) + (200-100) = 240$ 个缩放单位，而第三次内部求解的成本是 $180$ 个单位。这留下了 $60$ 个单位的内部工作量净节省（即 60 个矩阵-向量乘积）。对于一个小的 $k_2=3$，第三次外迭代的开销（正交化等）大约在几个矩阵-向量乘积的量级。因此，节省的量确实可以超过额外的开销。\n-   该陈述中使用的“必然”和“总是”使其为假，因为成本的平衡取决于问题的具体参数（$\\kappa$, $n$, 矩阵稀疏性等）。\n-   结论：**错误**\n\n**D. 在 Jacobi-Davidson 的不精确类牛顿模型下，可以预期在解附近存在一个形如 $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b \\eta_j \\lVert r_j \\rVert$ 的递推关系，其中 $a, b > 0$ 是常数。对于 $\\eta_j = 10^{-10}$，策略 H 从 $\\lVert r_0 \\rVert = 10^{-2}$ 开始，大约需要 2 个外迭代步达到 $\\lVert r_j \\rVert \\le 10^{-8}$；而对于 $\\eta_j = 0.1 \\lVert r_j \\rVert$，如果内部迭代次数按 $s \\log(1/\\eta_j)$ 的方式伸缩（其中 $s \\approx \\sqrt{\\kappa} \\approx 20$），策略 M 大约需要 3 个外迭代步，但总的内部 Krylov 迭代次数更少。这支持了早期采用中等精度的内求解，后期再收紧，作为一种计算上有利的权衡。**\n-   这个陈述提供了一个定量的演练，与我们从第一性原理的推导一致。\n-   这个递推模型适用于 JD。\n-   预测策略 H 需要 2 个外迭代步，策略 M 需要大约 3 个，这与我们的分析一致。\n-   内迭代次数按 $s \\log(1/\\eta_j)$ 伸缩，其中 $s \\approx \\sqrt{\\kappa}$，这对于 PCG 是正确的。\n-   显示策略 M 总内迭代次数更少（$340$ vs $400$ 单位）的计算是正确的。\n-   最终结论，即这展示了一种有利的权衡，是对分析的合理解释。该陈述在其所有部分都是全面而准确的。\n-   结论：**正确**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "在理想条件下，数值算法的收敛性有理论保证，但在实践中，它们可能会遇到导致停滞的“病态”情况。本编码练习  将让您直面一个棘手的场景：即使精确求解内部校正方程，标准的 JD 方法依然会停滞。通过动手构建这样一个反例，利用主角度等工具诊断问题根源，并实施一种“精化提取”策略来摆脱困境，您将对子空间质量和 Ritz 向量选择的重要性有更深刻的几何直觉，并学会如何增强算法的稳健性。",
            "id": "3590340",
            "problem": "考虑分析用于计算实对称矩阵特征对的雅可比-戴维森方法中的停滞问题。\n\n从以下基本概念开始：特征值问题旨在寻找一个标量-特征向量对 $(\\lambda, x)$，使得 $A x = \\lambda x$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称矩阵；从子空间 $\\mathcal{V} = \\operatorname{range}(V)$ 中提取的 Ritz 对 $(\\theta, u)$ 满足 $V^{\\top} A V y = \\theta y$ 和 $u = V y$；雅可比-戴维森校正方程将残差 $r = A u - \\theta u$ 投影到 $u$ 的正交补上，并使用线性系统 $(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -r$ 在正交约束 $t \\perp u$ 下求解校正量 $t$；以及子空间之间的主角（通过标准正交基矩阵的奇异值分解计算）用于量化失准程度。您将构造一个即使在精确求解校正方程时也会导致停滞的对称矩阵，通过主角诊断其原因，然后通过重新提取策略摆脱停滞。\n\n您的程序必须端到端地实现以下任务，不得依赖任何直接实现雅可比-戴维森方法的库例程。\n\n1) 反例构造与停滞演示：\n   - 构造 $\\mathbb{R}^{n}$ 的一个标准正交基 $\\{u, v, q_{1}, \\dots, q_{n-2}\\}$。固定一个标量 $\\theta \\in \\mathbb{R}$ 和一个非零向量 $c \\in \\mathbb{R}^{n-2}$。定义 $r = Q c$，其中 $Q = [q_{1}, \\dots, q_{n-2}] \\in \\mathbb{R}^{n \\times (n-2)}$。在此基中定义一个对称矩阵\n     $$\n     \\tilde{A} \\;=\\;\n     \\begin{bmatrix}\n     \\theta  0  c^{\\top} \\\\\n     0  \\theta  -c^{\\top} \\\\\n     c  -c  B\n     \\end{bmatrix},\n     $$\n     其中 $B \\in \\mathbb{R}^{(n-2)\\times(n-2)}$ 是任意对称矩阵。令 $G = [u, v, Q] \\in \\mathbb{R}^{n \\times n}$ 是收集了这些基向量的正交矩阵，并设置 $A = G \\tilde{A} G^{\\top}$。考虑子空间 $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$，并选择当前的 Ritz 向量为 $u$，其瑞利商为 $\\theta = u^{\\top} A u$。数值上证明，雅可比-戴维森校正方程 $(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -r$ 在 $t \\perp u$ 条件下的精确解 $t$ 位于 $\\mathcal{V}$ 中，因此在与 $V = [u, v]$ 正交化后，扩张量 $t - V(V^{\\top} t)$ 消失，导致即使内部求解是精确的，外部迭代也会停滞。\n   - 计算一维子空间 $\\operatorname{span}\\{u\\}$ 与同 $A$ 的最接近 $\\theta$ 的特征值相关联的真实一维不变特征子空间之间的主角（以度为单位）。该角度必须以度为单位报告。\n\n2) 诊断与通过重新提取摆脱停滞：\n   - 在当前子空间 $\\mathcal{V}$ 内，通过选择一个单位向量 $w \\in \\mathcal{V}$ 来执行精细化的重新提取，该向量使得残差范数 $\\|A w - \\mu w\\|_{2}$ 最小化，其中 $\\mu = w^{\\top} A w$。数值上验证这种重新提取方法能够识别出一个具有小残差的向量，从而在不扩张子空间的情况下摆脱停滞。\n\n3) 正常路径覆盖：\n   - 对于一个具有良好分离特征值的随机实对称矩阵，以及由随机单位向量生成的初始一维子空间 $\\mathcal{V} = \\operatorname{span}\\{u\\}$，执行一步雅可比-戴维森外部迭代，并精确求解内部的校正方程，确认通过将校正量与当前子空间正交化所获得的扩张方向具有非零范数（即，该步骤没有停滞）。\n\n4) 边缘情况覆盖（极端的近不变失准）：\n   - 重复第1项中的反例构造，但使用极小的 $\\|c\\|_{2}$ 来探测数值敏感性。确认停滞仍然被检测到，并且精细化的重新提取仍然能够摆脱停滞。\n\n角度单位说明：本问题中报告的任何角度都必须以度为单位。\n\n输出格式说明：您的程序应生成单行输出，其中包含三个测试用例的结果，这些结果聚合在一个由方括号括起来的逗号分隔列表中。条目必须按以下顺序排列：\n- 对于构造的停滞情况：一个指示是否检测到停滞的布尔值，以度为单位的主角（浮点数），以及一个指示精细化重新提取是否成功的布尔值。\n- 对于正常路径情况：一个指示扩张是否为非平凡的布尔值。\n- 对于边缘情况：一个指示是否检测到停滞的布尔值，以及一个指示精细化重新提取是否成功的布尔值。\n\n因此，最后一行必须具有\n\"[b1,angle1,b2,b3,b4,b5]\"\n的形式，其中 $b1$、$b2$、$b4$ 和 $b5$ 是布尔值，$angle1$ 是以度为单位的浮点数。\n\n测试套件：\n- 测试 1（构造的停滞）：$n = 8$，$\\theta = 1$，$\\|c\\|_{2} \\approx 10^{-2}$，固定随机种子以生成可复现的标准正交基和具有中等谱的对称矩阵 $B$。使用构造的 $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$ 并选择 $u$ 作为当前的 Ritz 向量。\n- 测试 2（正常路径）：$n = 8$，一个随机对称矩阵，其元素由固定种子下的标准正态分布生成并对称化；初始 $\\mathcal{V}$ 是一维的，含一个随机单位向量；执行一步雅可比-戴维森步骤并检查非零扩张。\n- 测试 3（边缘情况）：与测试 1 相同，但 $\\|c\\|_{2} \\approx 10^{-12}$ 且使用不同的固定种子。\n\n您的实现必须是自包含的，不应读取任何输入。最终输出必须严格遵循上述指定格式的单行文本。",
            "solution": "用户提供了一个关于分析对称特征值问题中雅可比-戴维森方法停滞现象的有效且定义明确的数值线性代数问题。任务是构造一个导致停滞的场景，对其进行诊断，实施解决方案，并覆盖相关的测试用例。\n\n雅可比-戴维森（Jacobi-Davidson, JD）方法是一种迭代算法，用于计算大型矩阵 $A$ 的少数满足 $A x = \\lambda x$ 的特征对 $(\\lambda, x)$。给定一个由矩阵 $V$ 的列张成的搜索子空间 $\\mathcal{V}$，通过求解投影的特征问题 $V^{\\top} A V y = \\theta y$ 并设置 $u = Vy$ 来提取一个近似特征对，称为 Ritz 对 $(\\theta, u)$。JD方法的核心是用一个校正向量 $t$ 来扩张子空间 $\\mathcal{V}$。该向量通过近似求解满足 $t \\perp u$ 的*校正方程*来计算：\n$$\n(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -r\n$$\n其中 $r = A u - \\theta u$ 是 Ritz 对的残差。然后，用 $t$ 的正交于 $\\mathcal{V}$ 的分量来扩张子空间。如果此扩张向量为零或在数值上可忽略不计，则会发生停滞，导致外部迭代收敛停滞。\n\n**1. 停滞反例的构造与分析**\n\n我们构造一个专门设计用来引起停滞的矩阵 $A$。令 $\\{u, v, q_1, \\dots, q_{n-2}\\}$ 为 $\\mathbb{R}^n$ 的一个标准正交基。令 $Q = [q_1, \\dots, q_{n-2}]$，完整的基矩阵为 $G = [u, v, Q]$。我们通过在此基中的一个更简单的表示 $\\tilde{A}$ 来定义对称矩阵 $A$，使得 $A = G \\tilde{A} G^{\\top}$：\n$$\n\\tilde{A} =\n\\begin{bmatrix}\n\\theta  0  c^{\\top} \\\\\n0  \\theta  -c^{\\top} \\\\\nc  -c  B\n\\end{bmatrix}\n$$\n这里，$\\theta$ 是一个标量，$c \\in \\mathbb{R}^{n-2}$ 是一个非零向量，$B$ 是任意对称矩阵。\n\n令搜索子空间为 $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$。可以从此子空间中提取一个有效的 Ritz 对。投影矩阵为 $V^{\\top}AV$，其中 $V = [u, v]$。我们有 $u = Ge_1$ 和 $v = Ge_2$，所以：\n$$\nV^{\\top}AV = \\begin{bmatrix} u^{\\top}Au  u^{\\top}Av \\\\ v^{\\top}Au  v^{\\top}Av \\end{bmatrix} = \\begin{bmatrix} e_1^{\\top}\\tilde{A}e_1  e_1^{\\top}\\tilde{A}e_2 \\\\ e_2^{\\top}\\tilde{A}e_1  e_2^{\\top}\\tilde{A}e_2 \\end{bmatrix} = \\begin{bmatrix} \\theta  0 \\\\ 0  \\theta \\end{bmatrix} = \\theta I_2\n$$\n这个投影的特征问题是退化的：$\\mathcal{V}$ 中的任何向量都是 Ritz 值为 $\\theta$ 的 Ritz 向量。问题指定选择 $(\\theta, u)$ 作为当前的 Ritz 对。\n\n此对的残差为 $r = Au - \\theta u$。在 $G$ 基中：\n$$\nA u - \\theta u = G \\tilde{A} G^{\\top} u - \\theta u = G (\\tilde{A} e_1 - \\theta e_1) = G \\begin{pmatrix} c \\\\ 0 \\end{pmatrix}_Q = Qc\n$$\n其中下标表示 $c$ 对应于基的 $Q$ 块。残差与 $u$ 和 $v$ 均正交。校正方程为 $(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -Qc$，且 $t \\perp u$。\n\n我们来检验是否存在一个解位于 $\\mathcal{V}$ 内。我们寻找 $t = \\alpha u + \\beta v$。约束 $t \\perp u$ 意味着 $\\alpha=0$，所以我们测试 $t = \\beta v$。由于 $v \\perp u$，$(I-uu^\\top)t = t$。方程变为 $(I - u u^{\\top})(A - \\theta I)(\\beta v) = -Qc$。\n项 $(A - \\theta I)v$ 为：\n$$\n(A-\\theta I)v = G(\\tilde{A}e_2 - \\theta e_2) = G \\begin{pmatrix} -c \\\\ 0 \\end{pmatrix}_Q = -Qc\n$$\n将其代回，我们得到 $\\beta (I - u u^{\\top})(-Qc) = -Qc$。由于 $u \\perp Qc$，$(I - u u^{\\top})(Qc) = Qc$。方程简化为 $-\\beta Qc = -Qc$。因为 $c \\neq 0$，这得到 $\\beta = 1$。\n因此，$t=v$ 是校正方程的精确解。\n\n子空间扩张步骤要求将 $t$ 与当前空间 $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$ 正交化。扩张向量是 $t_{\\text{new}} = (I-VV^{\\top})t$。当 $V=[u,v]$ 且 $t=v$ 时：\n$$\nt_{\\text{new}} = (I - [u,v][u,v]^{\\top})v = v - (u(u^{\\top}v) + v(v^{\\top}v)) = v - (u \\cdot 0 + v \\cdot 1) = 0\n$$\n扩张向量为零。JD 算法停滞。\n\n**2. 诊断与摆脱**\n\n停滞的发生是因为 $u$ 是从 $\\mathcal{V}$ 中选出的一个糟糕的近似特征向量。子空间 $\\mathcal{V}$ 实际上包含一个精确的特征向量。考虑向量 $w = \\frac{1}{\\sqrt{2}}(u+v)$：\n$$\nAw = \\frac{1}{\\sqrt{2}}(Au+Av) = \\frac{1}{\\sqrt{2}}((\\theta u + Qc) + (\\theta v - Qc)) = \\frac{\\theta}{\\sqrt{2}}(u+v) = \\theta w\n$$\n所以，$w$ 是一个特征值为 $\\theta$ 的特征向量。所选的 Ritz 向量 $u$ 与同一子空间内这个更优的方向 $w$ 存在显著失准。$\\operatorname{span}\\{u\\}$ 和 $\\operatorname{span}\\{w\\}$ 之间的主角 $\\phi$ 由 $\\cos\\phi = |u^{\\top}w| = |u^{\\top}\\frac{1}{\\sqrt{2}}(u+v)| = \\frac{1}{\\sqrt{2}}$ 给出。这得到 $\\phi = 45^\\circ$，表明存在严重失准。\n\n为摆脱停滞，我们必须识别出这个更好的方向。提示建议进行“精细化重新提取”：找到一个单位向量 $w_{opt} \\in \\mathcal{V}$，它能最小化残差范数 $\\|Aw - (w^{\\top}Aw)w\\|_2$。对于任何 $w \\in \\mathcal{V}$，其瑞利商 $w^{\\top}Aw$ 都是 $\\theta$。因此我们寻求最小化 $\\|(A-\\theta I)w\\|_2$，其中 $w \\in \\mathcal{V}$ 且 $\\|w\\|_2=1$。令 $w = y_1 u + y_2 v$，其中 $y_1^2+y_2^2=1$。\n$$\n\\|(A-\\theta I)w\\|_2 = \\|y_1(A-\\theta I)u + y_2(A-\\theta I)v\\|_2 = \\|y_1(Qc) + y_2(-Qc)\\|_2 = |y_1-y_2| \\|c\\|_2\n$$\n当 $y_1=y_2$ 时，此范数最小化，对于单位向量 $y=[y_1, y_2]^\\top$ 意味着 $y_1=y_2=1/\\sqrt{2}$（或符号相反）。这对应于 $w_{opt} = \\frac{1}{\\sqrt{2}}(u+v)$，即精确的特征向量。这种重新提取过程找到了零残差向量，从而摆脱了停滞。执行此最小化的一个稳健的数值方法是，找到矩阵 $[(A-\\theta I)u, (A-\\theta I)v]$ 的最小奇异值对应的右奇异向量。\n\n**3. 正常路径与边缘情况**\n\n对于一个通用的随机对称矩阵（“正常路径”），特征值通常是各不相同的，特征向量也不会出现病态的对齐。从一个随机的一维子空间 $\\mathcal{V}=\\operatorname{span}\\{u_0\\}$ 开始，校正方程的解 $t$ 通常不会为零（除非 $u_0$ 碰巧已经是特征向量），这将导致子空间的非平凡扩张。\n\n对于 $\\|c\\|_2 \\approx 10^{-12}$ 的极小边缘情况，其分析逻辑与主要的停滞情况相同。$u$ 的残差将非常小，$\\|r\\|_2=\\|c\\|_2 \\approx 10^{-12}$，这可能看起来接近收敛。然而，校正量 $t=v$ 的范数仍然是 $1$，而扩张量 $t_{new}$ 在数学上仍然是零。重新提取仍将识别出方向 $w=(u+v)/\\sqrt{2}$，其残差恰好为零，这表明即使初始近似看起来很好，也实现了真正的摆脱。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh, null_space\n\ndef solve_jd_correction(A, u, r, theta):\n    \"\"\"\n    Solves the Jacobi-Davidson correction equation\n    (I - u u^T)(A - theta I)(I - u u^T) t = -r\n    with the constraint t.T @ u = 0.\n    \"\"\"\n    n = A.shape[0]\n    # Ensure residual is numerically orthogonal to u\n    r_proj = r - u * (u.T @ r)\n\n    # Find an orthonormal basis for the orthogonal complement of u\n    U_ortho = null_space(u.T)\n\n    # Project the operator and the right-hand side into the complement space\n    # The equation to solve is M_proj @ z = rhs_proj, where t = U_ortho @ z\n    M = A - theta * np.eye(n)\n    M_proj = U_ortho.T @ M @ U_ortho\n    rhs_proj = -U_ortho.T @ r_proj\n\n    # Solve the projected linear system\n    try:\n        z = np.linalg.solve(M_proj, rhs_proj)\n        # Reconstruct the correction vector t in the full space\n        t = U_ortho @ z\n    except np.linalg.LinAlgError:\n        # Fallback to least-squares if the projected matrix is singular\n        z, _, _, _ = np.linalg.lstsq(M_proj, rhs_proj, rcond=None)\n        t = U_ortho @ z\n\n    return t\n\ndef refined_reextraction(A, V, theta):\n    \"\"\"\n    Performs refined re-extraction by finding the unit vector w in span(V)\n    that minimizes the norm of the residual, ||Aw - (w.T A w) w||.\n    This is especially for cases where standard Rayleigh-Ritz is degenerate.\n    \"\"\"\n    # Standard Rayleigh-Ritz: solve eigenproblem in subspace V\n    A_sub = V.T @ A @ V\n    sub_vals, sub_vecs = eigh(A_sub)\n\n    # Check for degeneracy: if all eigenvalues of the projected problem are identical\n    if sub_vals.size > 1 and np.allclose(sub_vals, sub_vals[0]):\n        # Degenerate case: minimize ||(A - theta I) w||_2 = ||(A - theta I) @ V @ y||_2\n        # where w = V @ y.\n        # This is equivalent to finding the smallest singular value of R = (A - theta I) @ V.\n        # The minimizer y is the right singular vector for sigma_min(R),\n        # which is the eigenvector for the smallest eigenvalue of R.T @ R.\n        R = (A - theta * np.eye(A.shape[0])) @ V\n        RTR = R.T @ R\n        _, eigvecs = eigh(RTR)\n        best_y = eigvecs[:, 0].reshape(-1, 1)\n        w_best = V @ best_y\n        mu_best = (w_best.T @ A @ w_best)[0,0]\n    else:\n        # Non-degenerate case: choose the Ritz pair with the smallest residual norm\n        best_resid_norm = np.inf\n        w_best, mu_best = None, None\n        for i in range(sub_vecs.shape[1]):\n            y = sub_vecs[:, i].reshape(-1, 1)\n            w = V @ y\n            mu = sub_vals[i]\n            resid = A @ w - mu * w\n            resid_norm = np.linalg.norm(resid)\n            if resid_norm  best_resid_norm:\n                best_resid_norm = resid_norm\n                w_best = w\n                mu_best = mu\n    \n    # Calculate the residual norm of the best vector found\n    final_resid_norm = np.linalg.norm(A @ w_best - mu_best * w_best)\n    return w_best, final_resid_norm\n\ndef run_stagnation_test(n, theta_val, c_norm, seed, tol=1e-10):\n    \"\"\"Implements Test 1 and Test 3: the stagnation counterexample.\"\"\"\n    rng = np.random.RandomState(seed)\n    \n    # 1. Construct the matrix A\n    G, _ = np.linalg.qr(rng.randn(n, n))\n    u, v, Q = G[:, 0:1], G[:, 1:2], G[:, 2:]\n    \n    c_vec = rng.randn(n - 2, 1)\n    c_vec = c_vec / np.linalg.norm(c_vec) * c_norm\n    B_rand = rng.randn(n - 2, n - 2)\n    B = (B_rand + B_rand.T) / 2\n\n    A_tilde = np.block([\n        [theta_val, 0, c_vec.T],\n        [0, theta_val, -c_vec.T],\n        [c_vec, -c_vec, B]\n    ])\n    A = G @ A_tilde @ G.T\n\n    u_k, theta_k = u, theta_val\n    V = np.hstack([u, v])\n\n    # 2. Perform one JD step, check for stagnation\n    r_k = A @ u_k - theta_k * u_k\n    t = solve_jd_correction(A, u_k, r_k, theta_k)\n    t_new = t - V @ (V.T @ t)\n    stagnation_detected = np.linalg.norm(t_new)  tol\n    \n    # 3. Compute principal angle\n    evals, evecs = eigh(A)\n    true_eig_idx = np.argmin(np.abs(evals - theta_k))\n    true_eigvec = evecs[:, true_eig_idx:true_eig_idx+1]\n    \n    cos_angle = np.abs(u_k.T @ true_eigvec)[0,0]\n    angle_rad = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n    principal_angle_deg = np.rad2deg(angle_rad)\n    \n    # 4. Perform refined re-extraction and check for escape\n    _, new_residual_norm = refined_reextraction(A, V, theta_k)\n    escape_successful = new_residual_norm  tol\n\n    return stagnation_detected, principal_angle_deg, escape_successful\n\ndef run_happy_path_test(n, seed, tol=1e-10):\n    \"\"\"Implements Test 2: the \"happy path\" with a random matrix.\"\"\"\n    rng = np.random.RandomState(seed)\n\n    X = rng.randn(n, n)\n    A = (X + X.T) / 2\n    u_0 = rng.randn(n, 1)\n    u_0 = u_0 / np.linalg.norm(u_0)\n    \n    V, u_k = u_0, u_0\n    theta_k = (u_k.T @ A @ u_k)[0,0]\n    r_k = A @ u_k - theta_k * u_k\n    \n    t = solve_jd_correction(A, u_k, r_k, theta_k)\n    \n    # For a 1D starting subspace V=u_k, and t perp u_k, the expansion vector is simply t\n    t_new = t - V @ (V.T @ t)\n    nontrivial_expansion = np.linalg.norm(t_new) > tol\n\n    return nontrivial_expansion\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test 1: Constructed stagnation with moderate perturbation\n    b1, angle1, b2 = run_stagnation_test(n=8, theta_val=1.0, c_norm=1e-2, seed=0)\n    \n    # Test 2: Happy path with a random symmetric matrix\n    b3 = run_happy_path_test(n=8, seed=1)\n    \n    # Test 3: Edge case: stagnation with very small perturbation\n    # As angle is not required for output, it is ignored here.\n    b4, _, b5 = run_stagnation_test(n=8, theta_val=1.0, c_norm=1e-12, seed=2)\n\n    results = [b1, angle1, b2, b3, b4, b5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "从理论探索到构建实用的求解器，是数值算法学习的关键一步。现实世界的科学与工程问题常常需要计算多个特征值，它们可能紧密聚集，甚至是重根。本练习  是一个综合性的编码项目，旨在将单向量 JD 方法提升为一个强大的多目标求解器。您将实现一个共享搜索子空间，并设计和实现至关重要的“锁定”与“放缩”机制，以系统地寻找多个特征对，同时避免重复计算和处理特征值重数问题，最终构建出一个能够应对复杂谱分布的、接近研究级别的工具。",
            "id": "3590393",
            "problem": "考虑一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 以及带有归一化约束 $u^\\top u = 1$ 的特征值问题 $A u = \\lambda u$。Jacobi-Davidson (JD) 方法旨在通过迭代地精化一个子空间并求解一个从第一性原理推导出的校正方程来计算目标特征对 $(\\theta, u)$。在一个多目标变体中，使用共享的搜索子空间同时追踪多个目标位移 $\\{\\sigma_j\\}$，同时强制执行缩减 (deflation) 和锁定 (locking) 规则，以防止多个目标收敛到同一个特征向量（交叉污染）或重新发现已经锁定的特征对。\n\n从特征对、Rayleigh 商、Ritz 对和子空间投影的基本定义出发，推导对称情况下 JD 校正方程的形式。然后，设计数学上合理的缩减和锁定规则，以防止目标之间的交叉污染，并能正确处理特征值的多重性。你编写的程序必须实现一个多目标 JD 方案，该方案使用共享子空间，在 $\\{\\sigma_j\\}$ 附近同时进行 Ritz 提取，对已锁定的向量进行正交投影，并使用 JD 校正方程的解进行块更新。对于每个目标 $j$，确保校正向量 $t_j$ 在当前方向 $u_j$ 和锁定子空间的正交补空间中计算，并且缩减/锁定规则强制共享子空间与所有已锁定的向量正交。\n\n你的程序必须实现以下原则：\n- 对于每个位移 $\\sigma_j$ 的 Ritz 提取必须从共享子空间 $V$ 中进行，通过计算投影算子 $T = V^\\top A V$ 的特征分解，并选择最接近 $\\sigma_j$ 的 Ritz 值。在一次迭代中的多个目标，不允许将相同的 Ritz 索引分配给多个目标，除非子空间维度小于未收敛目标的数量。\n- 校正方程必须在与当前近似特征向量和锁定子空间都正交的子空间内求解，使用该正交补空间中的精确解。当校正算子接近奇异时，必须使用数值稳定的最小二乘法或正则化方法。\n- 当一个目标的残差范数满足收敛阈值，并且新的近似特征向量与任何先前锁定的特征向量都非共线时，必须扩展锁定子空间。如果检测到共线性，则不锁定该目标，并且该目标必须在缩减后的子空间内继续搜索。每次锁定后，共享子空间必须重新投影到锁定子空间的正交补空间上并重新正交化。\n\n你必须实现一个单一程序，在以下测试套件上执行多目标 JD 方法。对于每个测试用例，生成一个近似特征值的有序列表（每个目标位移对应一个），作为浮点数。\n\n测试套件：\n1. 案例 D5：$A = \\mathrm{diag}(1,2,3,4,5)$，位移 $\\{\\sigma_j\\} = \\{1.2, 2.8, 4.9\\}$，容差 $\\varepsilon = 10^{-10}$，最大迭代次数 $150$，最大子空间维度 $20$。预期目标接近特征值 $1$、$3$ 和 $5$。\n2. 案例 T10：$A \\in \\mathbb{R}^{10 \\times 10}$ 为三对角矩阵，其中 $A_{ii} = 2$ 且 $A_{i,i+1} = A_{i+1,i} = -1$（$i=1,\\dots,9$），位移 $\\{\\sigma_j\\} = \\{0.1, 1.9, 3.9\\}$，容差 $\\varepsilon = 10^{-9}$，最大迭代次数 $200$，最大子空间维度 $25$。此案例测试聚集的极值和内部特征值。\n3. 案例 R6：$A = \\mathrm{diag}(1,3,3,3,4,6)$，具有重复特征值 $3$，位移 $\\{\\sigma_j\\} = \\{2.9, 3.05, 3.1\\}$，容差 $\\varepsilon = 10^{-10}$，最大迭代次数 $200$，最大子空间维度 $25$。此案例测试多重性处理；结果应在对应于重复特征值的特征空间中产生三个不同的特征向量，同时避免共线锁定。\n4. 案例 C6：$A = \\mathrm{diag}(1.0, 1.0001, 2.0, 3.0, 4.0, 5.0)$，位移 $\\{\\sigma_j\\} = \\{0.9, 1.0002\\}$，容差 $\\varepsilon = 10^{-10}$，最大迭代次数 $200$，最大子空间维度 $25$。此案例测试处理接近 $1$ 的紧密特征值簇。\n\n算法应使用以下基本原理，不依赖于快捷公式：\n- Ritz 对：给定一个标准正交基 $V$，Ritz 值是 $T = V^\\top A V$ 的特征值，Ritz 向量是 $V y$，其中 $y$ 是 $T$ 的特征向量。\n- Rayleigh 商：对于一个向量 $u$，$\\theta = \\frac{u^\\top A u}{u^\\top u}$。\n- 残差：对于一个近似 $(\\theta, u)$，$r = A u - \\theta u$。\n- 正交投影和子空间补空间：线性代数运算必须保持共享子空间是标准正交的，并且每个校正向量都必须位于当前方向和锁定子空间的正交补空间中。\n\n最终输出规范：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、由逗号分隔的列表的列表。每个内部列表对应一个测试用例的近似特征值的有序列表，其顺序与输入位移的顺序相同。例如：\"[[result_case1_shift1,result_case1_shift2,...],[result_case2_shift1,...],...]]\"。\n- 所有数字必须打印为十进制浮点值。不涉及角度或物理单位；不得打印任何附加文本。\n\n解决方案不得读取任何外部输入，也不得访问任何文件或网络。只能使用指定的库。程序必须是自包含的，并且必须如上所述实现带共享子空间、缩减和锁定的多目标 Jacobi-Davidson 方法。",
            "solution": "该问题要求推导对称矩阵的 Jacobi-Davidson (JD) 校正方程，并设计和实现一个多目标 JD 算法。该算法必须使用共享搜索子空间同时处理多个特征值目标，并具备缩减 (deflation) 和锁定 (locking) 机制，以找到多个特征对，包括那些对应于重复特征值的特征对。\n\n### 1. Jacobi-Davidson 校正方程的推导\n\nJacobi-Davidson 方法的核心在于为给定的特征向量近似找到一个最优校正。我们考虑一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的特征值问题：\n$$A u = \\lambda u, \\quad \\text{with } u^\\top u = 1$$\n设 $(\\theta, u)$ 是特征对 $(\\lambda, u_{true})$ 的一个当前近似，其中 $u$ 被归一化以满足 $u^\\top u = 1$，而 $\\theta$ 是对应的 Rayleigh 商：\n$$\\theta = \\frac{u^\\top A u}{u^\\top u} = u^\\top A u$$\n此近似的质量由残差向量 $r$ 来衡量：\n$$r = A u - \\theta u$$\n对于 Rayleigh 商，残差的一个关键属性是它与近似向量 $u$ 的正交性：\n$$u^\\top r = u^\\top (A u - \\theta u) = u^\\top A u - \\theta (u^\\top u) = \\theta - \\theta(1) = 0$$\n因此，$r \\perp u$。\n\n我们寻求一个校正向量 $t$，使得 $u_{new} = u + t$ 是真实特征向量 $u_{true}$ 的一个更好的近似。Jacobi-Davidson 方法的一个基本原则是在与当前近似 $u$ 正交的子空间中搜索此校正。这由约束条件表示：\n$$t \\perp u \\quad \\iff \\quad u^\\top t = 0$$\n精确的校正 $t$ 会满足真实特征值 $\\lambda$ 的特征值方程：\n$$A(u+t) = \\lambda(u+t)$$\n展开并重排该方程可得：\n$$A u - \\lambda u + (A - \\lambda I)t = 0$$\n代入 $A u = r + \\theta u$：\n$$(r + \\theta u) - \\lambda u + (A - \\lambda I)t = 0$$\n$$r + (A - \\lambda I)t = (\\lambda - \\theta)u$$\n这个方程是精确的，但不能直接解出 $t$，因为真实特征值 $\\lambda$ 是未知的。标准的 JD 方法是用当前最佳近似 $\\theta$ 替换算子 $(A - \\lambda I)$ 中的未知 $\\lambda$。这得到：\n$$(A - \\theta I)t \\approx -r + (\\lambda - \\theta)u$$\n为了在遵守正交性约束 $t \\perp u$ 的同时求解 $t$，我们将此方程投影到子空间 $\\{u\\}^\\perp$上，即 $u$ 的正交补空间。到该子空间的投影算子是 $P_u^{\\perp} = I - u u^\\top$。将此投影算子应用于两侧：\n$$P_u^{\\perp}(A - \\theta I)t = -P_u^{\\perp}r + (\\lambda - \\theta)P_u^{\\perp}u$$\n利用属性 $t \\in \\{u\\}^\\perp \\implies P_u^{\\perp}t=t$、$r \\in \\{u\\}^\\perp \\implies P_u^{\\perp}r=r$ 以及 $u \\notin \\{u\\}^\\perp \\implies P_u^{\\perp}u=0$，方程简化为：\n$$P_u^{\\perp}(A - \\theta I)t = -r$$\n因为我们寻找的解 $t$ 已经在 $\\{u\\}^\\perp$ 中，所以我们可以等价地写出限制在该子空间上的算子的方程：\n$$(I - u u^\\top)(A - \\theta I)(I - u u^\\top) t = -r$$\n这就是单个特征对的 Jacobi-Davidson 校正方程。算子 $(I - u u^\\top)(A - \\theta I)(I - u u^\\top)$ 根据其构造是奇异的（其零空间包含 $u$）。该方程需要在子空间 $\\{u\\}^\\perp$ 内求解 $t$。\n\n### 2. 带缩减和锁定的多目标算法\n\n对于一个带有位移 $\\{\\sigma_j\\}$ 的多目标问题，我们扩展此框架。一个共享子空间 $V$ 被迭代地构建。其中引入了缩减 (deflation) 以防止重新收敛到已找到的特征对。\n\n**关键组件：**\n- **状态管理**：我们维护一组活动目标和一组已锁定（收敛）的特征对。已锁定的特征向量构成一个标准正交基，存储在矩阵 $U_{locked}$ 中。\n- **Rayleigh-Ritz 过程**：在每次迭代中，我们求解一个投影特征值问题。给定搜索子空间的一个标准正交基 $V$，我们构造投影矩阵 $H = V^\\top A V$。其特征对 $(\\omega_k, s_k)$ 给出 Ritz 值 $\\omega_k$ 和 Ritz 向量 $y_k = V s_k$。\n- **配对**：对于每个带有位移 $\\sigma_j$ 的活动目标，我们选择使 $|\\omega_k - \\sigma_j|$ 最小化的 Ritz 对 $(\\omega_k, y_k)$。为防止交叉污染，我们强制执行一条规则：在一次迭代中，只要子空间足够大，单个 Ritz 对就不能分配给多个目标。\n- **带缩减的校正方程**：对于具有当前近似 $(\\theta_j, u_j)$ 的目标 $j$，其校正方程在与 $u_j$ 和 $U_{locked}$ 中所有已锁定特征向量都正交的子空间中求解。设 $P_{lock}^{\\perp} = I - U_{locked}U_{locked}^\\top$ 是到锁定子空间的正交补空间的投影算子。组合投影算子为 $P_j^{\\perp} = P_{lock}^{\\perp}(I-u_j u_j^\\top)$。校正方程是：\n$$P_j^{\\perp}(A - \\theta_j I)P_j^{\\perp} t_j = -r_j$$\n其中 $r_j = A u_j - \\theta_j u_j$。此方程在 $t_j \\in (\\mathrm{span}\\{u_j\\} \\cup \\mathrm{span}\\{U_{locked}\\})^\\perp$ 中求解。对于小矩阵，一个实用的方法是找到该正交补空间的一个显式基 $Q$，并求解简化系统 $Q^\\top (A-\\theta_j I) Q \\hat{t}_j = -Q^\\top r_j$，之后得到 $t_j = Q \\hat{t}_j$。\n- **子空间扩展**：通过添加计算出的校正向量 $\\{t_j\\}$ 来扩展子空间 $V$。这些新向量使用修正的 Gram-Schmidt 过程与现有基 $V$ 和彼此进行正交化，以保持数值稳定性。\n- **收敛和锁定**：如果目标 $j$ 的残差范数 $\\|r_j\\| = \\|A u_j - \\theta_j u_j\\|$ 低于指定的容差 $\\varepsilon$，则认为该目标已收敛。在锁定特征对 $(\\theta_j, u_j)$ 之前，会进行共线性检查，以确保 $u_j$ 与 $U_{locked}$ 中已锁定的向量在数值上不是线性相关的。这对于正确处理特征值的多重性至关重要。如果检查通过，则锁定该特征对。\n- **子空间缩减**：在将新向量添加到 $U_{locked}$ 后，必须通过将整个搜索子空间 $V$ 投影到更新后的锁定子空间的正交补空间上来对其进行缩减：$V_{new} = (I - U_{locked}U_{locked}^\\top) V_{old}$。然后，对得到的 $V_{new}$ 的列进行重新正交化。\n- **重启**：如果 $V$ 的维度超过最大限制 $k_{max}$，则对子空间进行裁剪。一个稳健的策略是使用由所有活动目标的当前最佳 Ritz 向量张成的新子空间重新启动，并对 $U_{locked}$ 进行适当的缩减。\n\n这种全面的设计确保了算法系统地探索向量空间，找到指定目标附近的特征对，通过寻找正交特征向量来处理特征值的多重性，并通过缩减防止重复工作。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, qr, null_space\n\ndef get_correction_vector(A, u, theta, residual, locked_vecs):\n    \"\"\"\n    Solves the Jacobi-Davidson correction equation in the appropriate\n    orthogonal complement.\n    (I-P)(A-theta*I)(I-P)t = -r\n    where P projects onto span(u, locked_vecs).\n    \"\"\"\n    n = A.shape[0]\n    \n    # Define the subspace W to be orthogonal to.\n    W_cols = [u[:, np.newaxis]]\n    if locked_vecs.shape[1] > 0:\n        W_cols.append(locked_vecs)\n    W = np.hstack(W_cols)\n    \n    # Q is an orthonormal basis for the orthogonal complement of span(W).\n    Q = null_space(W.T)\n    \n    if Q.shape[1] == 0:\n        return np.zeros(n)\n        \n    # Project the operator and the residual onto the subspace spanned by Q.\n    A_proj = Q.T @ (A @ Q)\n    r_proj = Q.T @ residual\n    \n    # Define the projected linear system operator.\n    M_proj = A_proj - theta * np.identity(Q.shape[1])\n    \n    try:\n        # Solve the projected system. Use lstsq for robustness against singularity.\n        t_hat, _, _, _ = np.linalg.lstsq(M_proj, -r_proj, rcond=None)\n    except np.linalg.LinAlgError:\n        # If the solver fails, return a zero vector.\n        return np.zeros(n)\n        \n    # Map the solution back to the original n-dimensional space.\n    return Q @ t_hat\n\ndef multi_target_jd(case):\n    \"\"\"\n    Implements the multi-target Jacobi-Davidson method.\n    \"\"\"\n    A, shifts, tol, max_iter, max_dim_v = case\n    n = A.shape[0]\n    num_targets = len(shifts)\n    \n    # State management\n    locked_pairs = {}  # key: original_target_idx, val: (eigval, eigvec)\n    active_targets = {i: {'shift': shifts[i], 'u': None, 'theta': None} for i in range(num_targets)}\n\n    # Initialize the search subspace V with random vectors\n    V_dim_init = min(num_targets, max_dim_v)\n    if V_dim_init == 0:\n        return [np.nan] * num_targets\n    V = np.random.rand(n, V_dim_init)\n    V, _ = qr(V, mode='economic')\n\n    for iteration in range(max_iter):\n        if not active_targets:\n            break\n            \n        # Get orthonormal basis of locked eigenvectors\n        locked_vecs = np.zeros((n, 0))\n        if locked_pairs:\n            locked_vecs = np.hstack([p[1][:, np.newaxis] for p in locked_pairs.values()])\n\n        # Subspace restart mechanism\n        if V.shape[1] > max_dim_v:\n            restart_vecs = []\n            for target_idx in active_targets:\n                if active_targets[target_idx]['u'] is not None:\n                    restart_vecs.append(active_targets[target_idx]['u'])\n            \n            if restart_vecs:\n                V_new = np.hstack([v[:, np.newaxis] for v in restart_vecs])\n                if locked_vecs.shape[1] > 0:\n                    V_new -= locked_vecs @ (locked_vecs.T @ V_new)\n                V, _ = qr(V_new, mode='economic')\n            else: # Fallback if no Ritz vectors are available\n                V_restart_dim = min(len(active_targets), max_dim_v)\n                V = np.random.rand(n, V_restart_dim)\n                if locked_vecs.shape[1] > 0:\n                    V -= locked_vecs @ (locked_vecs.T @ V)\n                V, _ = qr(V, mode='economic')\n        \n        # Rayleigh-Ritz step\n        if V.shape[1] == 0: # Subspace may have collapsed after deflation\n            V = np.random.rand(n, 1) # Re-initialize with one random vector\n            if locked_vecs.shape[1] > 0:\n                V -= locked_vecs @ (locked_vecs.T @ V)\n            V /= np.linalg.norm(V)\n            continue\n        \n        H = V.T @ A @ V\n        ritz_vals, S = eigh(H)\n        ritz_vecs = V @ S\n        \n        # Pairing step: assign best Ritz pairs to active targets\n        assignments = {}\n        available_ritz_indices = set(range(len(ritz_vals)))\n        sorted_target_indices = sorted(list(active_targets.keys()))\n        \n        for target_idx in sorted_target_indices:\n            shift = active_targets[target_idx]['shift']\n            best_ritz_idx, min_dist = -1, np.inf\n            for ritz_idx in available_ritz_indices:\n                dist = abs(ritz_vals[ritz_idx] - shift)\n                if dist  min_dist:\n                    min_dist, best_ritz_idx = dist, ritz_idx\n            \n            if best_ritz_idx != -1:\n                assignments[target_idx] = best_ritz_idx\n                # Prevent multiple targets from claiming the same Ritz pair if avoidable\n                if len(ritz_vals) >= len(active_targets):\n                    available_ritz_indices.remove(best_ritz_idx)\n    \n        # Update approximations and check for convergence\n        converged_this_iter = []\n        for target_idx, ritz_idx in assignments.items():\n            theta, u = ritz_vals[ritz_idx], ritz_vecs[:, ritz_idx]\n            active_targets[target_idx].update({'theta': theta, 'u': u})\n            \n            residual = A @ u - theta * u\n            if np.linalg.norm(residual)  tol:\n                converged_this_iter.append(target_idx)\n                \n        # Locking step\n        newly_locked = False\n        for target_idx in converged_this_iter:\n            theta, u = active_targets[target_idx]['theta'], active_targets[target_idx]['u']\n            \n            is_colinear = False\n            if locked_vecs.shape[1] > 0:\n                proj_u_on_locked = locked_vecs @ (locked_vecs.T @ u)\n                if np.linalg.norm(u - proj_u_on_locked)  1e-8:\n                    is_colinear = True\n            \n            if not is_colinear:\n                locked_pairs[target_idx] = (theta, u)\n                del active_targets[target_idx]\n                newly_locked = True\n        \n        # Deflate subspace V if new eigenvectors were locked\n        if newly_locked:\n            updated_locked_vecs = np.hstack([p[1][:, np.newaxis] for p in locked_pairs.values()])\n            if V.shape[1] > 0:\n                V -= updated_locked_vecs @ (updated_locked_vecs.T @ V)\n                V, _ = qr(V, mode='economic')\n\n        # Subspace expansion step\n        vecs_to_add_to_V = []\n        for target_idx in active_targets:\n            u, theta = active_targets[target_idx]['u'], active_targets[target_idx]['theta']\n            if u is None: continue\n            \n            residual = A @ u - theta * u\n            t = get_correction_vector(A, u, theta, residual, locked_vecs)\n            \n            if np.linalg.norm(t) > 1e-8:\n                vecs_to_add_to_V.append(t)\n        \n        # Augment V using Modified Gram-Schmidt for stability\n        for v_new in vecs_to_add_to_V:\n            if V.shape[1] >= max_dim_v: break\n            \n            v_ortho = v_new\n            if V.shape[1] > 0:\n                v_ortho -= V @ (V.T @ v_ortho)\n            \n            norm_v_ortho = np.linalg.norm(v_ortho)\n            if norm_v_ortho > 1e-8:\n                V = np.hstack([V, (v_ortho / norm_v_ortho)[:, np.newaxis]])\n\n    # Finalize results\n    final_eigvals = [np.nan] * num_targets\n    for i in range(num_targets):\n        if i in locked_pairs:\n            final_eigvals[i] = locked_pairs[i][0]\n        elif i in active_targets and active_targets[i]['theta'] is not None:\n            final_eigvals[i] = active_targets[i]['theta']  # Best effort for non-converged\n            \n    return final_eigvals\n\ndef solve():\n    # Test Suite\n    # Case D5: Diagonal matrix\n    A1 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0])\n    shifts1 = [1.2, 2.8, 4.9]\n    tol1 = 1e-10\n    max_iter1 = 150\n    max_dim_v1 = 20\n    case1 = (A1, shifts1, tol1, max_iter1, max_dim_v1)\n\n    # Case T10: Tridiagonal matrix (1D Laplacian)\n    n2 = 10\n    A2 = np.diag(2 * np.ones(n2)) - np.diag(np.ones(n2 - 1), 1) - np.diag(np.ones(n2 - 1), -1)\n    shifts2 = [0.1, 1.9, 3.9]\n    tol2 = 1e-9\n    max_iter2 = 200\n    max_dim_v2 = 25\n    case2 = (A2, shifts2, tol2, max_iter2, max_dim_v2)\n    \n    # Case R6: Repeated eigenvalue\n    A3 = np.diag([1.0, 3.0, 3.0, 3.0, 4.0, 6.0])\n    shifts3 = [2.9, 3.05, 3.1]\n    tol3 = 1e-10\n    max_iter3 = 200\n    max_dim_v3 = 25\n    case3 = (A3, shifts3, tol3, max_iter3, max_dim_v3)\n\n    # Case C6: Clustered eigenvalues\n    A4 = np.diag([1.0, 1.0001, 2.0, 3.0, 4.0, 5.0])\n    shifts4 = [0.9, 1.0002]\n    tol4 = 1e-10\n    max_iter4 = 200\n    max_dim_v4 = 25\n    case4 = (A4, shifts4, tol4, max_iter4, max_dim_v4)\n\n    test_cases = [case1, case2, case3, case4]\n    \n    all_results = []\n    for case in test_cases:\n        results = multi_target_jd(case)\n        # The order of results should match the order of shifts.\n        # This is handled by the dictionary `locked_pairs` and final list construction.\n        all_results.append(results)\n\n    # Format the final output string\n    result_strings = []\n    for res_list in all_results:\n        # Sort results for repeated roots case by value for consistent output.\n        if len(res_list) > 1 and np.isclose(res_list[0], res_list[1], atol=1e-5):\n            res_list.sort()\n        formatted_list = [f\"{x:.14f}\" for x in res_list]\n        result_strings.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}