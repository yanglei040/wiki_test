{
    "hands_on_practices": [
        {
            "introduction": "The Jacobi-Davidson method's efficiency hinges on a delicate balance. Each outer step requires solving the correction equation, but how accurately must this inner linear system be solved? This exercise  delves into this crucial trade-off, contrasting a high-accuracy strategy with a dynamic approach where the inner tolerance adapts to the outer residual's convergence. By analyzing the method through the lens of an inexact Newton iteration, you will discover the, perhaps counterintuitive, principle that \"over-solving\" can be computationally wasteful, and that a carefully managed inexactness can preserve fast outer convergence while minimizing total work.",
            "id": "3590386",
            "problem": "Consider a Hermitian matrix $A \\in \\mathbb{C}^{n \\times n}$ with $n$ large and a target eigenpair $(\\lambda, x)$ associated with a simple, well-separated eigenvalue. The Jacobi-Davidson method (JD) builds a search subspace by repeatedly expanding with a correction direction $t$ obtained from the JD correction equation, and extracts Ritz pairs $(\\theta, u)$ with the Rayleigh-Ritz procedure. Let the residual at outer iteration $j$ be $r_j = A u_j - \\theta_j u_j$ and suppose that a preconditioned Krylov method (for example, Preconditioned Conjugate Gradient (PCG) for suitable correction operators or Preconditioned Minimal Residual method (PMINRES) for symmetric indefinite cases) is used to approximately solve the correction equation, with the inner linear system residual tolerance parameter denoted by $\\eta_j$.\n\nAssume that:\n- The preconditioned correction operator has condition number $\\kappa$ of moderate size (say $\\kappa \\approx 400$), so that inner Krylov iteration counts scale in a way consistent with classical bounds for well-preconditioned Hermitian systems.\n- The orthonormalization and Rayleigh-Ritz projection in the outer iteration incurs costs that scale approximately like $O(n k_j)$ and $O(k_j^3)$, respectively, where $k_j$ is the current subspace dimension.\n\nYou are asked to analyze the computational trade-offs between two strategies for choosing the inner tolerance $\\eta_j$:\n- Strategy $\\mathrm{H}$ (high-accuracy): enforce a fixed tight tolerance $\\eta_j \\equiv 10^{-10}$ for all $j$.\n- Strategy $\\mathrm{M}$ (moderate-accuracy with aggressive expansion): enforce a tolerance proportional to the current outer residual $\\eta_j \\equiv c \\lVert r_j \\rVert$ with $c = 0.1$, tightening automatically as $\\lVert r_j \\rVert$ decreases.\n\nAssume the initial outer residual norm is $\\lVert r_0 \\rVert = 10^{-2}$ and the target accuracy is to reach $\\lVert r_j \\rVert \\le 10^{-8}$. Without using shortcut formulas for the Jacobi-Davidson method, reason from the definitions of the Rayleigh quotient, the projected correction equation, and inexact Newton-like behavior to determine which statements below are correct about the expected computational efficiency and convergence behavior of the two strategies. Select all that apply.\n\nA. Choosing $\\eta_j$ proportional to $\\lVert r_j \\rVert$ preserves quadratic local convergence of the outer iteration and can reduce total inner work compared to using a fixed tight tolerance, especially when the preconditioned correction operator has condition number $\\kappa \\gg 1$.\n\nB. Inner Krylov iteration counts depend linearly on $1/\\eta_j$, so a tolerance of $\\eta_j = 10^{-10}$ implies on the order of $10^{10}$ inner iterations per outer step; therefore, high-accuracy inner solves are always prohibitive.\n\nC. Since orthonormalization and Rayleigh-Ritz costs grow with the subspace dimension, any increase in the number of outer expansions (for example, from $2$ to $3$) necessarily outweighs savings from looser inner tolerances, making fixed high accuracy always superior in total runtime.\n\nD. Under an inexact Newton-type model for Jacobi-Davidson, one can expect a recursion of the form $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b \\eta_j \\lVert r_j \\rVert$ for constants $a, b > 0$ near the solution. With $\\eta_j = 10^{-10}$, Strategy $\\mathrm{H}$ reaches $\\lVert r_j \\rVert \\le 10^{-8}$ in about $2$ outer steps from $\\lVert r_0 \\rVert = 10^{-2}$, whereas with $\\eta_j = 0.1 \\lVert r_j \\rVert$, Strategy $\\mathrm{M}$ requires about $3$ outer steps but fewer total inner Krylov iterations if the inner iteration count scales like $s \\log(1/\\eta_j)$ with $s \\approx \\sqrt{\\kappa} \\approx 20$. This supports moderately accurate inner solves early and tightening later as a computationally favorable trade-off.",
            "solution": "The problem requires an analysis of the computational efficiency and convergence behavior of the Jacobi-Davidson (JD) method under two different strategies for setting the inner solver tolerance, $\\eta_j$.\n\n### Principle-Based Derivation\n\nThe Jacobi-Davidson method constructs an approximate eigenpair $(\\theta_j, u_j)$ at each outer iteration $j$ and seeks a correction $t_j$ to improve this approximation. The correction $t_j$ is found by approximately solving the Jacobi-Davidson correction equation:\n$$ (I - u_j u_j^*)(A - \\theta_j I)(I - u_j u_j^*) t_j = -r_j, \\quad \\text{with } t_j \\perp u_j $$\nwhere $r_j = A u_j - \\theta_j u_j$ is the residual of the Ritz pair. This linear system for $t_j$ is typically solved with an iterative Krylov method, such as PCG or PMINRES, up to a certain accuracy.\n\nThe JD method can be analyzed as an inexact Newton method. The convergence of such methods is described by the following general relationship, which connects the error (or residual) at step $j+1$ to the error at step $j$ and the inexactness of the linear system solve:\n$$ \\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b \\lVert \\delta_j \\rVert $$\nwhere $a$ and $b$ are positive constants that depend on the properties of the problem (e.g., conditioning and norms of operators), and $\\delta_j$ is the residual of the inner linear solve. The problem states that a preconditioned Krylov method is used to solve the correction equation, with a relative tolerance $\\eta_j$. This means the inner solver terminates when its residual is at most $\\eta_j$ times its initial residual. The initial residual for the correction equation is $-r_j$. Thus, the final inner residual norm $\\lVert \\delta_j \\rVert$ is bounded by $\\eta_j \\lVert r_j \\rVert$. Substituting this into the convergence relation gives:\n$$ \\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b' \\eta_j \\lVert r_j \\rVert $$\nfor some constant $b'$. This is the recursion model provided in option D.\n\nThe number of iterations required by a preconditioned Krylov method, such as PCG, to reduce the residual by a factor of $\\epsilon$ is classically bounded by a function proportional to the square root of the condition number of the preconditioned operator, $\\kappa$, and the logarithm of the desired reduction factor, $1/\\epsilon$. If the inner solver is run until the relative residual is $\\eta_j$, the number of inner iterations $k_{\\text{inner}, j}$ at outer step $j$ is approximately:\n$$ k_{\\text{inner}, j} \\approx C \\sqrt{\\kappa} \\log(1/\\eta_j) $$\nfor some constant $C$. The problem gives $\\kappa \\approx 400$, so $\\sqrt{\\kappa} \\approx 20$. This logarithmic dependence is crucial.\n\nWe are given the initial residual norm $\\lVert r_0 \\rVert = 10^{-2}$ and the target is $\\lVert r_j \\rVert \\le 10^{-8}$. Let's analyze the two strategies using these principles.\n\n**Strategy H (high-accuracy): $\\eta_j \\equiv 10^{-10}$**\n-   **Convergence:** The recurrence becomes $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b' (10^{-10}) \\lVert r_j \\rVert$. Since $\\lVert r_j \\rVert$ is decreasing from $10^{-2}$, the term $b' (10^{-10}) \\lVert r_j \\rVert$ is significantly smaller than the quadratic term $a \\lVert r_j \\rVert^2$. For instance, at the first step, we compare $a(10^{-2})^2 = a \\cdot 10^{-4}$ with $b' \\cdot 10^{-10} \\cdot 10^{-2} = b' \\cdot 10^{-12}$. The convergence is effectively quadratic.\n    -   $j=0$: $\\lVert r_0 \\rVert = 10^{-2}$.\n    -   $j=1$: $\\lVert r_1 \\rVert \\approx a \\lVert r_0 \\rVert^2 = a (10^{-2})^2 = a \\cdot 10^{-4}$.\n    -   $j=2$: $\\lVert r_2 \\rVert \\approx a \\lVert r_1 \\rVert^2 \\approx a (a \\cdot 10^{-4})^2 = a^3 \\cdot 10^{-8}$. Assuming $a \\approx 1$, we reach the target tolerance in approximately $2$ outer iterations.\n-   **Work:** The number of inner iterations per outer step is constant and high: $k_{\\text{inner, H}} \\approx C \\sqrt{400} \\log(1/10^{-10}) = C \\cdot 20 \\cdot (10 \\ln 10) \\approx 460 \\cdot C$. If we follow the scaling in option D with $k_{\\text{inner}} \\approx s \\log_{10}(1/\\eta_j)$ and $s=20$, then $k_{\\text{inner, H}} \\approx 20 \\log_{10}(10^{10}) = 200$. Total inner work is $2 \\times 200 = 400$ scaled units.\n\n**Strategy M (moderate-accuracy): $\\eta_j \\equiv 0.1 \\lVert r_j \\rVert$**\n-   **Convergence:** The recurrence becomes $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b' (0.1 \\lVert r_j \\rVert) \\lVert r_j \\rVert = (a + 0.1b') \\lVert r_j \\rVert^2$. The convergence is still quadratic, but with a potentially larger constant $a' = a + 0.1 b'$, meaning the residual norm decreases by a smaller factor at each step compared to pure quadratic convergence.\n    -   $j=0$: $\\lVert r_0 \\rVert = 10^{-2}$.\n    -   $j=1$: $\\lVert r_1 \\rVert \\approx a' \\lVert r_0 \\rVert^2 = a'(10^{-2})^2 = a' \\cdot 10^{-4}$.\n    -   $j=2$: $\\lVert r_2 \\rVert \\approx a' \\lVert r_1 \\rVert^2 \\approx a'(a' \\cdot 10^{-4})^2 = (a')^3 \\cdot 10^{-8}$. If $a'  1$ (e.g., $a'=1.1$ as in the analysis for option D), then $\\lVert r_2 \\rVert$ could be slightly larger than $10^{-8}$, requiring a third step. Let's assume it takes $3$ outer iterations.\n-   **Work:** The number of inner iterations changes at each step. Using $s=20$:\n    -   $j=0$: $\\eta_0 = 0.1 \\lVert r_0 \\rVert = 0.1(10^{-2}) = 10^{-3}$. $k_{\\text{inner}, 0} \\approx 20 \\log_{10}(10^3) = 60$.\n    -   $j=1$: $\\lVert r_1 \\rVert \\approx a' \\cdot 10^{-4}$. $\\eta_1 = 0.1 \\lVert r_1 \\rVert \\approx 0.1 a' \\cdot 10^{-4}$. $k_{\\text{inner}, 1} \\approx 20 \\log_{10}(1/(0.1 a' \\cdot 10^{-4})) \\approx 20 \\log_{10}(10^5/a') \\approx 20 \\cdot 5 = 100$.\n    -   $j=2$: $\\lVert r_2 \\rVert \\approx (a')^3 \\cdot 10^{-8}$. $\\eta_2=0.1 \\lVert r_2 \\rVert \\approx 0.1 (a')^3 \\cdot 10^{-8}$. $k_{\\text{inner}, 2} \\approx 20 \\log_{10}(1/(0.1 (a')^3 \\cdot 10^{-8})) \\approx 20 \\cdot 9 = 180$.\n    -   Total inner work is approximately $60 + 100 + 180 = 340$ scaled units.\n\nThis analysis shows that Strategy M performs fewer total inner iterations ($340$ vs $400$) despite requiring more outer iterations ($3$ vs $2$).\n\n### Option-by-Option Analysis\n\n**A. Choosing $\\eta_j$ proportional to $\\lVert r_j \\rVert$ preserves quadratic local convergence of the outer iteration and can reduce total inner work compared to using a fixed tight tolerance, especially when the preconditioned correction operator has condition number $\\kappa \\gg 1$.**\n-   Preserving quadratic convergence: As shown above, setting $\\eta_j = c \\lVert r_j \\rVert$ results in the recurrence $\\lVert r_{j+1} \\rVert \\le (a+b'c) \\lVert r_j \\rVert^2$, which demonstrates quadratic convergence.\n-   Reducing total inner work: Our analysis shows that by avoiding \"over-solving\" in the early stages (e.g., $60$ vs $200$ iterations in the first step), the total number of inner iterations can be reduced, even if an extra outer iteration is needed.\n-   The role of $\\kappa$: The savings in inner iterations scale with $\\sqrt{\\kappa}$. If $\\kappa$ is large, $\\sqrt{\\kappa}$ is large, and the difference between $\\log(1/\\eta_{\\text{loose}})$ and $\\log(1/\\eta_{\\text{tight}})$ translates into a large number of saved iterations. Therefore, the benefit is more pronounced for larger $\\kappa$.\n-   Verdict: **Correct**\n\n**B. Inner Krylov iteration counts depend linearly on $1/\\eta_j$, so a tolerance of $\\eta_j = 10^{-10}$ implies on the order of $10^{10}$ inner iterations per outer step; therefore, high-accuracy inner solves are always prohibitive.**\n-   The premise that Krylov iteration counts depend linearly on $1/\\eta_j$ is fundamentally incorrect for methods like PCG or PMINRES applied to Hermitian systems. The dependence is logarithmic, i.e., $O(\\log(1/\\eta_j))$.\n-   A linear dependence would imply a cost of $O(1/\\eta_j)$, which is true for much slower methods like gradient descent under certain norms, but not for the specified modern Krylov solvers.\n-   Due to the logarithmic dependence, a tolerance of $\\eta_j = 10^{-10}$ with $\\sqrt{\\kappa} \\approx 20$ leads to a few hundred iterations, not $10^{10}$. This number is computationally feasible.\n-   Verdict: **Incorrect**\n\n**C. Since orthonormalization and Rayleigh-Ritz costs grow with the subspace dimension, any increase in the number of outer expansions (for example, from $2$ to $3$) necessarily outweighs savings from looser inner tolerances, making fixed high accuracy always superior in total runtime.**\n-   This option claims that the overhead of an extra outer iteration *necessarily* outweighs any savings from the inner solver. This is too strong a claim.\n-   The dominant cost in each outer step is often the inner Krylov solve, which involves many matrix-vector products, each costing $O(n \\cdot \\text{nnz_per_row})$. Orthonormalization ($O(nk_j)$) and Rayleigh-Ritz matrix setup ($O(nk_j^2)$) are expensive but grow with the subspace dimension $k_j$, which is typically small.\n-   In our analysis, the saving in inner work was $(200-60) + (200-100) = 240$ scaled units, while the cost of the third inner solve was $180$ units. This leaves a net saving of $60$ units of inner work (i.e., $60$ matrix-vector products). The overhead of the third outer iteration (orthonormalization, etc.) for a small $k_2=3$ would be on the order of a few matrix-vector products. Thus, the savings can indeed outweigh the extra overhead.\n-   The statement's use of \"necessarily\" and \"always\" makes it false, as the balance of costs depends on the specific parameters of the problem ($\\kappa$, $n$, matrix sparsity, etc.).\n-   Verdict: **Incorrect**\n\n**D. Under an inexact Newton-type model for Jacobi-Davidson, one can expect a recursion of the form $\\lVert r_{j+1} \\rVert \\le a \\lVert r_j \\rVert^2 + b \\eta_j \\lVert r_j \\rVert$ for constants $a, b  0$ near the solution. With $\\eta_j = 10^{-10}$, Strategy H reaches $\\lVert r_j \\rVert \\le 10^{-8}$ in about $2$ outer steps from $\\lVert r_0 \\rVert = 10^{-2}$, whereas with $\\eta_j = 0.1 \\lVert r_j \\rVert$, Strategy M requires about $3$ outer steps but fewer total inner Krylov iterations if the inner iteration count scales like $s \\log(1/\\eta_j)$ with $s \\approx \\sqrt{\\kappa} \\approx 20$. This supports moderately accurate inner solves early and tightening later as a computationally favorable trade-off.**\n-   This statement provides a quantitative walkthrough that is consistent with our derivation from first principles.\n-   The recursion model is appropriate for JD.\n-   The prediction of $2$ outer steps for Strategy H and about $3$ for Strategy M is consistent with our analysis.\n-   The scaling of inner iterations as $s \\log(1/\\eta_j)$ with $s \\approx \\sqrt{\\kappa}$ is correct for PCG.\n-   The calculation showing fewer total inner iterations for Strategy M ($340$ vs $400$ units) is correct.\n-   The final conclusion that this demonstrates a favorable trade-off is a sound interpretation of the analysis. The statement is comprehensive and accurate in all its parts.\n-   Verdict: **Correct**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "A robust numerical algorithm must not only perform well in typical scenarios but also be resilient to pathological cases. The Jacobi-Davidson method can sometimes stagnate, failing to expand its search subspace even when the correction equation is solved exactly. This hands-on coding challenge  guides you to construct a specific scenario where this failure occurs due to a misalignment between the standard Ritz vector and a better approximation hiding within the same subspace. You will then diagnose the problem and implement a \"refined re-extraction\" strategy, a powerful technique to escape such stagnation and deepen your understanding of the method's geometric foundations.",
            "id": "3590340",
            "problem": "Consider the task of analyzing stagnation in the Jacobi-Davidson method for computing an eigenpair of a real symmetric matrix. Begin from the following fundamental base: the eigenvalue problem seeks a scalar-eigenvector pair $(\\lambda, x)$ such that $A x = \\lambda x$ with $A \\in \\mathbb{R}^{n \\times n}$ symmetric; a Ritz pair $(\\theta, u)$ extracted from a subspace $\\mathcal{V} = \\operatorname{range}(V)$ satisfies $V^{\\top} A V y = \\theta y$ and $u = V y$; the Jacobi-Davidson correction equation projects the residual $r = A u - \\theta u$ onto the orthogonal complement of $u$ and solves for a correction $t$ orthogonal to $u$ using the linear system $(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -r$ with the orthogonality constraint $t \\perp u$; and principal angles between subspaces, computed via singular value decomposition of orthonormal basis matrices, quantify misalignment. You will construct a symmetric matrix that causes stagnation even when the correction equation is solved exactly, diagnose the cause via a principal angle, and then escape stagnation via a re-extraction strategy.\n\nYour program must implement the following tasks end-to-end, without reliance on any library routines that directly implement the Jacobi-Davidson method.\n\n1) Counterexample construction and stagnation demonstration:\n   - Construct an orthonormal basis $\\{u, v, q_{1}, \\dots, q_{n-2}\\}$ of $\\mathbb{R}^{n}$. Fix a scalar $\\theta \\in \\mathbb{R}$ and a nonzero vector $c \\in \\mathbb{R}^{n-2}$. Define $r = Q c$, where $Q = [q_{1}, \\dots, q_{n-2}] \\in \\mathbb{R}^{n \\times (n-2)}$. Define a symmetric matrix in this basis by\n     $$\n     \\tilde{A} \\;=\\;\n     \\begin{bmatrix}\n     \\theta  0  c^{\\top} \\\\\n     0  \\theta  -c^{\\top} \\\\\n     c  -c  B\n     \\end{bmatrix},\n     $$\n     where $B \\in \\mathbb{R}^{(n-2)\\times(n-2)}$ is any symmetric matrix. Let $G = [u, v, Q] \\in \\mathbb{R}^{n \\times n}$ be the orthogonal matrix collecting the basis vectors, and set $A = G \\tilde{A} G^{\\top}$. Consider the subspace $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$ and choose the current Ritz vector to be $u$ with Rayleigh quotient $\\theta = u^{\\top} A u$. Show numerically that the exact solution $t$ to the Jacobi-Davidson correction equation $(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -r$ with $t \\perp u$ lies in $\\mathcal{V}$, hence the expansion $t - V(V^{\\top} t)$ vanishes after orthogonalization against $V = [u, v]$, causing stagnation in the outer iteration even with an exact inner solve.\n   - Compute the principal angle in degrees between the one-dimensional subspace $\\operatorname{span}\\{u\\}$ and the true one-dimensional invariant eigenspace associated with the eigenvalue of $A$ nearest to $\\theta$. This angle must be reported in degrees.\n\n2) Diagnosis and escape via re-extraction:\n   - Within the current subspace $\\mathcal{V}$, perform a refined re-extraction by selecting a unit vector $w \\in \\mathcal{V}$ that minimizes the residual norm $\\|A w - \\mu w\\|_{2}$ with $\\mu = w^{\\top} A w$. Verify numerically that this re-extraction identifies a vector with a small residual and thereby escapes the stagnation without expanding the subspace.\n\n3) Happy path coverage:\n   - For a random real symmetric matrix with well-separated eigenvalues and a one-dimensional initial subspace $\\mathcal{V} = \\operatorname{span}\\{u\\}$ spanned by a random unit vector, carry out one Jacobi-Davidson outer step with an exact inner solve of the correction equation and confirm that the expansion direction obtained by orthogonalizing the correction against the current subspace has nonzero norm (i.e., the step does not stagnate).\n\n4) Edge case coverage (near-invariant misalignment of extreme magnitude):\n   - Repeat the counterexample construction from item 1 with $\\|c\\|_{2}$ extremely small to probe numerical sensitivity. Confirm that stagnation is still detected and that the refined re-extraction still escapes.\n\nAngle unit specification: any angle reported in this problem must be given in degrees.\n\nOutput format specification: Your program should produce a single line of output containing the results for the three test cases aggregated into a single comma-separated list enclosed in square brackets. The entries must be, in order:\n- For the constructed stagnation case: a boolean indicating stagnation detected, the principal angle in degrees as a float, and a boolean indicating that the refined re-extraction escaped.\n- For the happy path case: a boolean indicating that the expansion was nontrivial.\n- For the edge case: a boolean indicating stagnation detected and a boolean indicating refined re-extraction escaped.\n\nThus the final line must have the form\n\"[b1,angle1,b2,b3,b4,b5]\"\nwith $b1$, $b2$, $b4$, and $b5$ booleans and $angle1$ a float in degrees.\n\nTest suite:\n- Test 1 (constructed stagnation): $n = 8$, $\\theta = 1$, $\\|c\\|_{2} \\approx 10^{-2}$, random seed fixed to produce a reproducible orthonormal basis and symmetric matrix $B$ with moderate spectrum. Use $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$ as constructed and select $u$ as the current Ritz vector.\n- Test 2 (happy path): $n = 8$, a random symmetric matrix with entries generated from a standard normal distribution under a fixed seed and symmetrized; initial $\\mathcal{V}$ is one-dimensional with a random unit vector; perform one Jacobi-Davidson step and check nonzero expansion.\n- Test 3 (edge case): same as Test 1 but with $\\|c\\|_{2} \\approx 10^{-12}$ and a different fixed seed.\n\nYour implementation must be self-contained and should not read any input. The final output must be exactly one line in the format specified above.",
            "solution": "The user has provided a valid, well-posed problem in numerical linear algebra concerning the analysis of stagnation phenomena in the Jacobi-Davidson method for symmetric eigenvalue problems. The task is to construct a scenario causing stagnation, diagnose it, implement a solution, and cover related test cases.\n\nThe Jacobi-Davidson (JD) method is an iterative algorithm for computing a few eigenpairs $(\\lambda, x)$ of a large matrix $A$ satisfying $A x = \\lambda x$. Given a search subspace $\\mathcal{V}$, spanned by the columns of a matrix $V$, one extracts an approximate eigenpair, called a Ritz pair $(\\theta, u)$, by solving the projected eigenproblem $V^{\\top} A V y = \\theta y$ and setting $u = Vy$. The core of the JD method is the expansion of the subspace $\\mathcal{V}$ with a correction vector $t$. This vector is computed by approximately solving the *correction equation* for $t \\perp u$:\n$$\n(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -r\n$$\nwhere $r = A u - \\theta u$ is the residual of the Ritz pair. The subspace is then expanded with the component of $t$ orthogonal to $\\mathcal{V}$. Stagnation occurs if this expansion vector is zero or numerically negligible, halting the convergence of the outer iteration.\n\n**1. Stagnation Counterexample Construction and Analysis**\n\nWe construct a matrix $A$ specifically designed to cause stagnation. Let $\\{u, v, q_1, \\dots, q_{n-2}\\}$ be an orthonormal basis for $\\mathbb{R}^n$. Let $Q = [q_1, \\dots, q_{n-2}]$ and the full basis matrix be $G = [u, v, Q]$. We define a symmetric matrix $A$ via a simpler representation $\\tilde{A}$ in this basis, such that $A = G \\tilde{A} G^{\\top}$:\n$$\n\\tilde{A} =\n\\begin{bmatrix}\n\\theta  0  c^{\\top} \\\\\n0  \\theta  -c^{\\top} \\\\\nc  -c  B\n\\end{bmatrix}\n$$\nHere, $\\theta$ is a scalar, $c \\in \\mathbb{R}^{n-2}$ is a non-zero vector, and $B$ is an arbitrary symmetric matrix.\n\nLet the search subspace be $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$. A valid Ritz pair can be extracted from this subspace. The projected matrix is $V^{\\top}AV$ where $V = [u, v]$. We have $u = Ge_1$ and $v = Ge_2$, so:\n$$\nV^{\\top}AV = \\begin{bmatrix} u^{\\top}Au  u^{\\top}Av \\\\ v^{\\top}Au  v^{\\top}Av \\end{bmatrix} = \\begin{bmatrix} e_1^{\\top}\\tilde{A}e_1  e_1^{\\top}\\tilde{A}e_2 \\\\ e_2^{\\top}\\tilde{A}e_1  e_2^{\\top}\\tilde{A}e_2 \\end{bmatrix} = \\begin{bmatrix} \\theta  0 \\\\ 0  \\theta \\end{bmatrix} = \\theta I_2\n$$\nThis projected eigenproblem is degenerate: any vector in $\\mathcal{V}$ is a Ritz vector with Ritz value $\\theta$. The problem specifies choosing $(\\theta, u)$ as the current Ritz pair.\n\nThe residual for this pair is $r = Au - \\theta u$. In the $G$ basis:\n$$\nA u - \\theta u = G \\tilde{A} G^{\\top} u - \\theta u = G (\\tilde{A} e_1 - \\theta e_1) = G \\begin{pmatrix} c \\\\ 0 \\end{pmatrix}_Q = Qc\n$$\nwhere the subscript denotes that $c$ corresponds to the $Q$ block of the basis. The residual is orthogonal to both $u$ and $v$. The correction equation is $(I - u u^{\\top})(A - \\theta I)(I - u u^{\\top}) t = -Qc$, with $t \\perp u$.\n\nLet's test if a solution exists within $\\mathcal{V}$. We seek $t = \\alpha u + \\beta v$. The constraint $t \\perp u$ implies $\\alpha=0$, so we test $t = \\beta v$. Since $v \\perp u$, $(I-uu^\\top)t = t$. The equation becomes $(I - u u^{\\top})(A - \\theta I)(\\beta v) = -Qc$.\nThe term $(A - \\theta I)v$ is:\n$$\n(A-\\theta I)v = G(\\tilde{A}e_2 - \\theta e_2) = G \\begin{pmatrix} -c \\\\ 0 \\end{pmatrix}_Q = -Qc\n$$\nSubstituting this back, we get $\\beta (I - u u^{\\top})(-Qc) = -Qc$. Since $u \\perp Qc$, $(I - u u^{\\top})(Qc) = Qc$. The equation simplifies to $-\\beta Qc = -Qc$. As $c \\neq 0$, this yields $\\beta = 1$.\nThus, $t=v$ is the exact solution to the correction equation.\n\nThe subspace expansion step requires orthogonalizing $t$ against the current space $\\mathcal{V} = \\operatorname{span}\\{u, v\\}$. The expansion vector is $t_{\\text{new}} = (I-VV^{\\top})t$. With $V=[u,v]$ and $t=v$:\n$$\nt_{\\text{new}} = (I - [u,v][u,v]^{\\top})v = v - (u(u^{\\top}v) + v(v^{\\top}v)) = v - (u \\cdot 0 + v \\cdot 1) = 0\n$$\nThe expansion vector is zero. The JD algorithm stagnates.\n\n**2. Diagnosis and Escape**\n\nThe stagnation occurs because $u$ is a poor choice of approximate eigenvector from $\\mathcal{V}$. The subspace $\\mathcal{V}$ actually contains an exact eigenvector. Consider the vector $w = \\frac{1}{\\sqrt{2}}(u+v)$:\n$Aw = \\frac{1}{\\sqrt{2}}(Au+Av) = \\frac{1}{\\sqrt{2}}((\\theta u + Qc) + (\\theta v - Qc)) = \\frac{\\theta}{\\sqrt{2}}(u+v) = \\theta w$.\nSo, $w$ is an eigenvector with eigenvalue $\\theta$. The chosen Ritz vector $u$ is significantly misaligned with this superior direction $w$ within the same subspace. The principal angle $\\phi$ between $\\operatorname{span}\\{u\\}$ and $\\operatorname{span}\\{w\\}$ is given by $\\cos\\phi = |u^{\\top}w| = |u^{\\top}\\frac{1}{\\sqrt{2}}(u+v)| = \\frac{1}{\\sqrt{2}}$. This gives $\\phi = 45^\\circ$, indicating a severe misalignment.\n\nTo escape stagnation, we must identify this better direction. The prompt suggests a \"refined re-extraction\": finding a unit vector $w_{opt} \\in \\mathcal{V}$ that minimizes the residual norm $\\|Aw - (w^{\\top}Aw)w\\|_2$. For any $w \\in \\mathcal{V}$, its Rayleigh quotient $w^{\\top}Aw$ is $\\theta$. So we seek to minimize $\\|(A-\\theta I)w\\|_2$ for $w \\in \\mathcal{V}$ with $\\|w\\|_2=1$. Let $w = y_1 u + y_2 v$ with $y_1^2+y_2^2=1$.\n$$\n\\|(A-\\theta I)w\\|_2 = \\|y_1(A-\\theta I)u + y_2(A-\\theta I)v\\|_2 = \\|y_1(Qc) + y_2(-Qc)\\|_2 = |y_1-y_2| \\|c\\|_2\n$$\nThis norm is minimized when $y_1=y_2$, which for a unit vector $y=[y_1, y_2]^\\top$ means $y_1=y_2=1/\\sqrt{2}$ (up to sign). This corresponds to $w_{opt} = \\frac{1}{\\sqrt{2}}(u+v)$, the exact eigenvector. This re-extraction procedure finds the zero-residual vector, escaping stagnation. A robust numerical way to perform this minimization is to find the right singular vector corresponding to the smallest singular value of the matrix $[(A-\\theta I)u, (A-\\theta I)v]$.\n\n**3. Happy Path and Edge Case**\n\nFor a generic random symmetric matrix (the \"happy path\"), eigenvalues are typically distinct and eigenvectors are not pathologically aligned. Starting with a random 1-dimensional subspace $\\mathcal{V}=\\operatorname{span}\\{u_0\\}$, the solution $t$ to the correction equation will generally not be zero (unless $u_0$ was already an eigenvector by chance), leading to a non-trivial expansion of the subspace.\n\nThe edge case with a very small $\\|c\\|_2 \\approx 10^{-12}$ follows the same analytical logic as the primary stagnation case. The residual for $u$ will be tiny, $\\|r\\|_2=\\|c\\|_2 \\approx 10^{-12}$, which may appear close to convergence. However, the correction $t=v$ is still of norm $1$, and the expansion $t_{new}$ is still mathematically zero. The re-extraction will still identify the direction $w=(u+v)/\\sqrt{2}$ which has a residual of exactly zero, demonstrating a true escape even when the initial approximation seems good.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh, null_space\n\ndef solve_jd_correction(A, u, r, theta):\n    \"\"\"\n    Solves the Jacobi-Davidson correction equation\n    (I - u u^T)(A - theta I)(I - u u^T) t = -r\n    with the constraint t.T @ u = 0.\n    \"\"\"\n    n = A.shape[0]\n    # Ensure residual is numerically orthogonal to u\n    r_proj = r - u * (u.T @ r)\n\n    # Find an orthonormal basis for the orthogonal complement of u\n    U_ortho = null_space(u.T)\n\n    # Project the operator and the right-hand side into the complement space\n    # The equation to solve is M_proj @ z = rhs_proj, where t = U_ortho @ z\n    M = A - theta * np.eye(n)\n    M_proj = U_ortho.T @ M @ U_ortho\n    rhs_proj = -U_ortho.T @ r_proj\n\n    # Solve the projected linear system\n    try:\n        z = np.linalg.solve(M_proj, rhs_proj)\n        # Reconstruct the correction vector t in the full space\n        t = U_ortho @ z\n    except np.linalg.LinAlgError:\n        # Fallback to least-squares if the projected matrix is singular\n        z, _, _, _ = np.linalg.lstsq(M_proj, rhs_proj, rcond=None)\n        t = U_ortho @ z\n\n    return t\n\ndef refined_reextraction(A, V, theta):\n    \"\"\"\n    Performs refined re-extraction by finding the unit vector w in span(V)\n    that minimizes the norm of the residual, ||Aw - (w.T A w) w||.\n    This is especially for cases where standard Rayleigh-Ritz is degenerate.\n    \"\"\"\n    # Standard Rayleigh-Ritz: solve eigenproblem in subspace V\n    A_sub = V.T @ A @ V\n    sub_vals, sub_vecs = eigh(A_sub)\n\n    # Check for degeneracy: if all eigenvalues of the projected problem are identical\n    if sub_vals.size  1 and np.allclose(sub_vals, sub_vals[0]):\n        # Degenerate case: minimize ||(A - theta I) w||_2 = ||(A - theta I) @ V @ y||_2\n        # where w = V @ y.\n        # This is equivalent to finding the smallest singular value of R = (A - theta I) @ V.\n        # The minimizer y is the right singular vector for sigma_min(R),\n        # which is the eigenvector for the smallest eigenvalue of R.T @ R.\n        R = (A - theta * np.eye(A.shape[0])) @ V\n        RTR = R.T @ R\n        _, eigvecs = eigh(RTR)\n        best_y = eigvecs[:, 0].reshape(-1, 1)\n        w_best = V @ best_y\n        mu_best = (w_best.T @ A @ w_best)[0,0]\n    else:\n        # Non-degenerate case: choose the Ritz pair with the smallest residual norm\n        best_resid_norm = np.inf\n        w_best, mu_best = None, None\n        for i in range(sub_vecs.shape[1]):\n            y = sub_vecs[:, i].reshape(-1, 1)\n            w = V @ y\n            mu = sub_vals[i]\n            resid = A @ w - mu * w\n            resid_norm = np.linalg.norm(resid)\n            if resid_norm  best_resid_norm:\n                best_resid_norm = resid_norm\n                w_best = w\n                mu_best = mu\n    \n    # Calculate the residual norm of the best vector found\n    final_resid_norm = np.linalg.norm(A @ w_best - mu_best * w_best)\n    return w_best, final_resid_norm\n\ndef run_stagnation_test(n, theta_val, c_norm, seed, tol=1e-10):\n    \"\"\"Implements Test 1 and Test 3: the stagnation counterexample.\"\"\"\n    rng = np.random.RandomState(seed)\n    \n    # 1. Construct the matrix A\n    G, _ = np.linalg.qr(rng.randn(n, n))\n    u, v, Q = G[:, 0:1], G[:, 1:2], G[:, 2:]\n    \n    c_vec = rng.randn(n - 2, 1)\n    c_vec = c_vec / np.linalg.norm(c_vec) * c_norm\n    B_rand = rng.randn(n - 2, n - 2)\n    B = (B_rand + B_rand.T) / 2\n\n    A_tilde = np.block([\n        [theta_val, 0, c_vec.T],\n        [0, theta_val, -c_vec.T],\n        [c_vec, -c_vec, B]\n    ])\n    A = G @ A_tilde @ G.T\n\n    u_k, theta_k = u, theta_val\n    V = np.hstack([u, v])\n\n    # 2. Perform one JD step, check for stagnation\n    r_k = A @ u_k - theta_k * u_k\n    t = solve_jd_correction(A, u_k, r_k, theta_k)\n    t_new = t - V @ (V.T @ t)\n    stagnation_detected = np.linalg.norm(t_new)  tol\n    \n    # 3. Compute principal angle\n    evals, evecs = eigh(A)\n    true_eig_idx = np.argmin(np.abs(evals - theta_k))\n    true_eigvec = evecs[:, true_eig_idx:true_eig_idx+1]\n    \n    cos_angle = np.abs(u_k.T @ true_eigvec)[0,0]\n    angle_rad = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n    principal_angle_deg = np.rad2deg(angle_rad)\n    \n    # 4. Perform refined re-extraction and check for escape\n    _, new_residual_norm = refined_reextraction(A, V, theta_k)\n    escape_successful = new_residual_norm  tol\n\n    return stagnation_detected, principal_angle_deg, escape_successful\n\ndef run_happy_path_test(n, seed, tol=1e-10):\n    \"\"\"Implements Test 2: the \"happy path\" with a random matrix.\"\"\"\n    rng = np.random.RandomState(seed)\n\n    X = rng.randn(n, n)\n    A = (X + X.T) / 2\n    u_0 = rng.randn(n, 1)\n    u_0 = u_0 / np.linalg.norm(u_0)\n    \n    V, u_k = u_0, u_0\n    theta_k = (u_k.T @ A @ u_k)[0,0]\n    r_k = A @ u_k - theta_k * u_k\n    \n    t = solve_jd_correction(A, u_k, r_k, theta_k)\n    \n    # For a 1D starting subspace V=u_k, and t perp u_k, the expansion vector is simply t\n    t_new = t - V @ (V.T @ t)\n    nontrivial_expansion = np.linalg.norm(t_new)  tol\n\n    return nontrivial_expansion\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test 1: Constructed stagnation with moderate perturbation\n    b1, angle1, b2 = run_stagnation_test(n=8, theta_val=1.0, c_norm=1e-2, seed=0)\n    \n    # Test 2: Happy path with a random symmetric matrix\n    b3 = run_happy_path_test(n=8, seed=1)\n    \n    # Test 3: Edge case: stagnation with very small perturbation\n    # As angle is not required for output, it is ignored here.\n    b4, _, b5 = run_stagnation_test(n=8, theta_val=1.0, c_norm=1e-12, seed=2)\n\n    results = [b1, angle1, b2, b3, b4, b5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from theory to practical application often involves scaling an algorithm to handle more complex, real-world demands. This advanced coding practice  challenges you to build a multi-target Jacobi-Davidson solver capable of pursuing several eigenpairs near different shifts simultaneously. You will implement a shared search subspace, sophisticated deflation and locking mechanisms to handle converged eigenpairs (including those with multiplicities), and rules to prevent cross-contamination between targets. This exercise synthesizes the core principles of the method into a robust, production-grade framework.",
            "id": "3590393",
            "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and the eigenvalue problem $A u = \\lambda u$ with the normalization constraint $u^\\top u = 1$. The Jacobi-Davidson (JD) method aims to compute targeted eigenpairs $(\\theta, u)$ by iteratively refining a subspace and solving a correction equation derived from first principles. In a multi-target variant, several target shifts $\\{\\sigma_j\\}$ are pursued simultaneously using a shared search subspace, while deflation and locking rules are enforced to prevent multiple targets from converging to the same eigenvector (cross-contamination) or re-discovering already locked eigenpairs.\n\nStarting from the foundational definitions of eigenpairs, the Rayleigh quotient, Ritz pairs, and subspace projection, derive the form of the JD correction equation for the symmetric case. Then, design mathematically sound deflation and locking rules that prevent cross-contamination among targets and enable correct handling of eigenvalue multiplicities. The program you write must implement a multi-target JD scheme that uses a shared subspace, simultaneous Ritz extractions near $\\{\\sigma_j\\}$, orthogonal projection against locked vectors, and a block update using solutions of the JD correction equation. For each target $j$, ensure that the correction vector $t_j$ is computed in the orthogonal complement of both the current direction $u_j$ and the locked subspace, and that deflation/locking rules enforce orthogonality of the shared subspace to all locked vectors.\n\nYour program must implement the following principles:\n- The Ritz extraction for each shift $\\sigma_j$ must be performed from the shared subspace $V$, by computing the eigen-decomposition of the projected operator $T = V^\\top A V$ and selecting the Ritz value closest to $\\sigma_j$. For multiple targets within one iteration, disallow assigning the same Ritz index to more than one target unless the subspace dimension is smaller than the number of unconverged targets.\n- The correction equation must be solved within the subspace orthogonal to both the current approximate eigenvector and the locked subspace, using an exact solution in that orthogonal complement. When the correction operator is nearly singular, a numerically stable least squares or regularized approach must be used.\n- The locked subspace must be expanded when a residual norm satisfies a convergence threshold for a target and the new approximate eigenvector is not colinear with any previously locked eigenvector. If colinearity is detected, the target is not locked and must continue searching within the deflated subspace. The shared subspace must be reprojected and reorthonormalized against the locked subspace after each lock.\n\nYou must implement a single program that executes the multi-target JD method on the following test suite. For each test case, produce an ordered list of approximated eigenvalues (one per target shift) as floating-point numbers.\n\nTest Suite:\n1. Case D5: $A = \\mathrm{diag}(1,2,3,4,5)$, shifts $\\{\\sigma_j\\} = \\{1.2, 2.8, 4.9\\}$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $150$, maximum subspace dimension $20$. Expected targets are near eigenvalues $1$, $3$, and $5$.\n2. Case T10: $A \\in \\mathbb{R}^{10 \\times 10}$ tridiagonal with $A_{ii} = 2$ and $A_{i,i+1} = A_{i+1,i} = -1$ for $i=1,\\dots,9$, shifts $\\{\\sigma_j\\} = \\{0.1, 1.9, 3.9\\}$, tolerance $\\varepsilon = 10^{-9}$, maximum iterations $200$, maximum subspace dimension $25$. This tests clustered extremal and interior eigenvalues.\n3. Case R6: $A = \\mathrm{diag}(1,3,3,3,4,6)$ with repeated eigenvalue $3$, shifts $\\{\\sigma_j\\} = \\{2.9, 3.05, 3.1\\}$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $200$, maximum subspace dimension $25$. This tests multiplicity handling; the result should produce three distinct eigenvectors in the eigenspace corresponding to the repeated eigenvalue, while avoiding colinear locking.\n4. Case C6: $A = \\mathrm{diag}(1.0, 1.0001, 2.0, 3.0, 4.0, 5.0)$, shifts $\\{\\sigma_j\\} = \\{0.9, 1.0002\\}$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $200$, maximum subspace dimension $25$. This tests handling of a tight cluster of eigenvalues near $1$.\n\nThe algorithm should use the following base principles without relying on shortcut formulas:\n- Ritz pairs: given an orthonormal basis $V$, the Ritz values are eigenvalues of $T = V^\\top A V$ and the Ritz vectors are $V y$ for eigenvectors $y$ of $T$.\n- Rayleigh quotient: for a vector $u$, $\\theta = \\frac{u^\\top A u}{u^\\top u}$.\n- Residual: for an approximation $(\\theta, u)$, $r = A u - \\theta u$.\n- Orthogonal projection and subspace complements: linear algebraic operations must keep the shared subspace orthonormal, and every correction vector must lie in the orthogonal complement of the current direction and the locked subspace.\n\nFinal Output Specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, enclosed in square brackets. Each inner list corresponds to the ordered list of approximated eigenvalues for a test case, in the same order as the input shifts. For example: \"[[result_case1_shift1,result_case1_shift2,...],[result_case2_shift1,...],...]\".\n- All numbers must be printed as decimal floating-point values. No angles or physical units are involved; no additional text may be printed.\n\nThe solution must not read any external input and must not access any files or networks. Use only the libraries specified. The program must be self-contained and must implement the multi-target Jacobi-Davidson method with shared subspace, deflation, and locking as described above.",
            "solution": "The problem requires the derivation of the Jacobi-Davidson (JD) correction equation for symmetric matrices and the design and implementation of a multi-target JD algorithm. The algorithm must handle multiple eigenvalue targets simultaneously using a shared search subspace, with mechanisms for deflation and locking to find multiple eigenpairs, including those corresponding to repeated eigenvalues.\n\n### 1. Derivation of the Jacobi-Davidson Correction Equation\n\nThe core of the Jacobi-Davidson method lies in finding an optimal correction to a given approximation of an eigenvector. Let us consider the eigenvalue problem for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$:\n$$A u = \\lambda u, \\quad \\text{with } u^\\top u = 1$$\nLet $(\\theta, u)$ be a current approximation to an eigenpair $(\\lambda, u_{true})$, where $u$ is normalized such that $u^\\top u = 1$, and $\\theta$ is the corresponding Rayleigh quotient:\n$$\\theta = \\frac{u^\\top A u}{u^\\top u} = u^\\top A u$$\nThe quality of this approximation is measured by the residual vector $r$:\n$$r = A u - \\theta u$$\nA key property of the residual for a Rayleigh quotient is its orthogonality to the approximation vector $u$:\n$$u^\\top r = u^\\top (A u - \\theta u) = u^\\top A u - \\theta (u^\\top u) = \\theta - \\theta(1) = 0$$\nThus, $r \\perp u$.\n\nWe seek a correction vector $t$ such that $u_{new} = u + t$ is a better approximation of the true eigenvector $u_{true}$. A fundamental tenet of the Jacobi-Davidson method is to search for this correction in the subspace orthogonal to the current approximation $u$. This is expressed by the constraint:\n$$t \\perp u \\quad \\iff \\quad u^\\top t = 0$$\nThe exact correction $t$ would satisfy the eigenvalue equation for the true eigenvalue $\\lambda$:\n$$A(u+t) = \\lambda(u+t)$$\nExpanding and rearranging this equation yields:\n$$A u - \\lambda u + (A - \\lambda I)t = 0$$\nSubstituting $A u = r + \\theta u$:\n$$(r + \\theta u) - \\lambda u + (A - \\lambda I)t = 0$$\n$$r + (A - \\lambda I)t = (\\lambda - \\theta)u$$\nThis equation is exact but not directly solvable for $t$ because the true eigenvalue $\\lambda$ is unknown. The standard JD approach is to replace the unknown $\\lambda$ in the operator $(A - \\lambda I)$ with the current best approximation, $\\theta$. This yields:\n$$(A - \\theta I)t \\approx -r + (\\lambda - \\theta)u$$\nTo solve for $t$ while respecting the orthogonality constraint $t \\perp u$, we project this equation onto the subspace $\\{u\\}^\\perp$, which is the orthogonal complement of $u$. The projection operator onto this subspace is $P_u^{\\perp} = I - u u^\\top$. Applying this projector to both sides:\n$$P_u^{\\perp}(A - \\theta I)t = -P_u^{\\perp}r + (\\lambda - \\theta)P_u^{\\perp}u$$\nUsing the properties $t \\in \\{u\\}^\\perp \\implies P_u^{\\perp}t=t$, $r \\in \\{u\\}^\\perp \\implies P_u^{\\perp}r=r$, and $u \\notin \\{u\\}^\\perp \\implies P_u^{\\perp}u=0$, the equation simplifies:\n$$P_u^{\\perp}(A - \\theta I)t = -r$$\nSince we seek a solution $t$ that is already in $\\{u\\}^\\perp$, we can equivalently write the equation for an operator restricted to this subspace:\n$$(I - u u^\\top)(A - \\theta I)(I - u u^\\top) t = -r$$\nThis is the Jacobi-Davidson correction equation for a single eigenpair. The operator $(I - u u^\\top)(A - \\theta I)(I - u u^\\top)$ is singular by construction (its null space contains $u$). The equation is to be solved for $t$ within the subspace $\\{u\\}^\\perp$.\n\n### 2. Multi-Target Algorithm with Deflation and Locking\n\nFor a multi-target problem with shifts $\\{\\sigma_j\\}$, we extend this framework. A shared subspace $V$ is built iteratively. Deflation is incorporated to prevent reconvergence to already found eigenpairs.\n\n**Key Components:**\n- **State Management**: We maintain a set of active targets and a set of locked (converged) eigenpairs. The locked eigenvectors form an orthonormal basis stored in a matrix $U_{locked}$.\n- **Rayleigh-Ritz Procedure**: In each iteration, we solve a projected eigenvalue problem. Given an orthonormal basis $V$ for the search subspace, we form the projected matrix $H = V^\\top A V$. Its eigenpairs $(\\omega_k, s_k)$ give the Ritz values $\\omega_k$ and Ritz vectors $y_k = V s_k$.\n- **Pairing**: For each active target with shift $\\sigma_j$, we select the Ritz pair $(\\omega_k, y_k)$ for which $|\\omega_k - \\sigma_j|$ is minimized. To prevent cross-contamination, we enforce a rule that a single Ritz pair cannot be assigned to multiple targets in one iteration, provided the subspace is large enough.\n- **Correction Equation with Deflation**: The correction equation for a target $j$ with current approximation $(\\theta_j, u_j)$ is solved in the subspace orthogonal to both $u_j$ and all locked eigenvectors in $U_{locked}$. Let $P_{lock}^{\\perp} = I - U_{locked}U_{locked}^\\top$ be the projector onto the orthogonal complement of the locked subspace. The combined projector is $P_j^{\\perp} = P_{lock}^{\\perp}(I-u_j u_j^\\top)$. The correction equation is:\n$$P_j^{\\perp}(A - \\theta_j I)P_j^{\\perp} t_j = -r_j$$\nwhere $r_j = A u_j - \\theta_j u_j$. This equation is solved for $t_j \\in (\\mathrm{span}\\{u_j\\} \\cup \\mathrm{span}\\{U_{locked}\\})^\\perp$. A practical method for small matrices is to find an explicit basis $Q$ for this orthogonal complement and solve the reduced system $Q^\\top (A-\\theta_j I) Q \\hat{t}_j = -Q^\\top r_j$, after which $t_j = Q \\hat{t}_j$.\n- **Subspace Expansion**: The subspace $V$ is expanded by adding the computed correction vectors $\\{t_j\\}$. These new vectors are orthogonalized against the existing basis $V$ and each other using a modified Gram-Schmidt procedure to maintain numerical stability.\n- **Convergence and Locking**: A target $j$ is considered converged if the norm of its residual, $\\|r_j\\| = \\|A u_j - \\theta_j u_j\\|$, falls below a specified tolerance $\\varepsilon$. Before locking the pair $(\\theta_j, u_j)$, a colinearity check is performed to ensure $u_j$ is not numerically linearly dependent on the already locked vectors in $U_{locked}$. This is crucial for handling eigenvalue multiplicities correctly. If the check passes, the pair is locked.\n- **Subspace Deflation**: After a new vector is added to $U_{locked}$, the entire search subspace $V$ must be deflated by projecting it onto the orthogonal complement of the updated locked subspace: $V_{new} = (I - U_{locked}U_{locked}^\\top) V_{old}$. The columns of the resulting $V_{new}$ are then re-orthonormalized.\n- **Restart**: If the dimension of $V$ exceeds a maximum limit $k_{max}$, the subspace is pruned. A robust strategy is to restart with a new subspace spanned by the current best Ritz vectors for all active targets, appropriately deflated against $U_{locked}$.\n\nThis comprehensive design ensures that the algorithm systematically explores the vector space, finds eigenpairs near the specified targets, handles eigenvalue multiplicities by finding orthogonal eigenvectors, and prevents redundant work through deflation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, qr, null_space\n\ndef get_correction_vector(A, u, theta, residual, locked_vecs):\n    \"\"\"\n    Solves the Jacobi-Davidson correction equation in the appropriate\n    orthogonal complement.\n    (I-P)(A-theta*I)(I-P)t = -r\n    where P projects onto span(u, locked_vecs).\n    \"\"\"\n    n = A.shape[0]\n    \n    # Define the subspace W to be orthogonal to.\n    W_cols = [u[:, np.newaxis]]\n    if locked_vecs.shape[1]  0:\n        W_cols.append(locked_vecs)\n    W = np.hstack(W_cols)\n    \n    # Q is an orthonormal basis for the orthogonal complement of span(W).\n    Q = null_space(W.T)\n    \n    if Q.shape[1] == 0:\n        return np.zeros(n)\n        \n    # Project the operator and the residual onto the subspace spanned by Q.\n    A_proj = Q.T @ (A @ Q)\n    r_proj = Q.T @ residual\n    \n    # Define the projected linear system operator.\n    M_proj = A_proj - theta * np.identity(Q.shape[1])\n    \n    try:\n        # Solve the projected system. Use lstsq for robustness against singularity.\n        t_hat, _, _, _ = np.linalg.lstsq(M_proj, -r_proj, rcond=None)\n    except np.linalg.LinAlgError:\n        # If the solver fails, return a zero vector.\n        return np.zeros(n)\n        \n    # Map the solution back to the original n-dimensional space.\n    return Q @ t_hat\n\ndef multi_target_jd(case):\n    \"\"\"\n    Implements the multi-target Jacobi-Davidson method.\n    \"\"\"\n    A, shifts, tol, max_iter, max_dim_v = case\n    n = A.shape[0]\n    num_targets = len(shifts)\n    \n    # State management\n    locked_pairs = {}  # key: original_target_idx, val: (eigval, eigvec)\n    active_targets = {i: {'shift': shifts[i], 'u': None, 'theta': None} for i in range(num_targets)}\n\n    # Initialize the search subspace V with random vectors\n    V_dim_init = min(num_targets, max_dim_v)\n    if V_dim_init == 0:\n        return [np.nan] * num_targets\n    V = np.random.rand(n, V_dim_init)\n    V, _ = qr(V, mode='economic')\n\n    for iteration in range(max_iter):\n        if not active_targets:\n            break\n            \n        # Get orthonormal basis of locked eigenvectors\n        locked_vecs = np.zeros((n, 0))\n        if locked_pairs:\n            locked_vecs = np.hstack([p[1][:, np.newaxis] for p in locked_pairs.values()])\n\n        # Subspace restart mechanism\n        if V.shape[1]  max_dim_v:\n            restart_vecs = []\n            for target_idx in active_targets:\n                if active_targets[target_idx]['u'] is not None:\n                    restart_vecs.append(active_targets[target_idx]['u'])\n            \n            if restart_vecs:\n                V_new = np.hstack([v[:, np.newaxis] for v in restart_vecs])\n                if locked_vecs.shape[1]  0:\n                    V_new -= locked_vecs @ (locked_vecs.T @ V_new)\n                V, _ = qr(V_new, mode='economic')\n            else: # Fallback if no Ritz vectors are available\n                V_restart_dim = min(len(active_targets), max_dim_v)\n                V = np.random.rand(n, V_restart_dim)\n                if locked_vecs.shape[1]  0:\n                    V -= locked_vecs @ (locked_vecs.T @ V)\n                V, _ = qr(V, mode='economic')\n        \n        # Rayleigh-Ritz step\n        if V.shape[1] == 0: # Subspace may have collapsed after deflation\n            V = np.random.rand(n, 1) # Re-initialize with one random vector\n            if locked_vecs.shape[1]  0:\n                V -= locked_vecs @ (locked_vecs.T @ V)\n            V /= np.linalg.norm(V)\n            continue\n        \n        H = V.T @ A @ V\n        ritz_vals, S = eigh(H)\n        ritz_vecs = V @ S\n        \n        # Pairing step: assign best Ritz pairs to active targets\n        assignments = {}\n        available_ritz_indices = set(range(len(ritz_vals)))\n        sorted_target_indices = sorted(list(active_targets.keys()))\n        \n        for target_idx in sorted_target_indices:\n            shift = active_targets[target_idx]['shift']\n            best_ritz_idx, min_dist = -1, np.inf\n            for ritz_idx in available_ritz_indices:\n                dist = abs(ritz_vals[ritz_idx] - shift)\n                if dist  min_dist:\n                    min_dist, best_ritz_idx = dist, ritz_idx\n            \n            if best_ritz_idx != -1:\n                assignments[target_idx] = best_ritz_idx\n                # Prevent multiple targets from claiming the same Ritz pair if avoidable\n                if len(ritz_vals) = len(active_targets):\n                    available_ritz_indices.remove(best_ritz_idx)\n    \n        # Update approximations and check for convergence\n        converged_this_iter = []\n        for target_idx, ritz_idx in assignments.items():\n            theta, u = ritz_vals[ritz_idx], ritz_vecs[:, ritz_idx]\n            active_targets[target_idx].update({'theta': theta, 'u': u})\n            \n            residual = A @ u - theta * u\n            if np.linalg.norm(residual)  tol:\n                converged_this_iter.append(target_idx)\n                \n        # Locking step\n        newly_locked = False\n        for target_idx in converged_this_iter:\n            theta, u = active_targets[target_idx]['theta'], active_targets[target_idx]['u']\n            \n            is_colinear = False\n            if locked_vecs.shape[1]  0:\n                proj_u_on_locked = locked_vecs @ (locked_vecs.T @ u)\n                if np.linalg.norm(u - proj_u_on_locked)  1e-8:\n                    is_colinear = True\n            \n            if not is_colinear:\n                locked_pairs[target_idx] = (theta, u)\n                del active_targets[target_idx]\n                newly_locked = True\n        \n        # Deflate subspace V if new eigenvectors were locked\n        if newly_locked:\n            updated_locked_vecs = np.hstack([p[1][:, np.newaxis] for p in locked_pairs.values()])\n            if V.shape[1]  0:\n                V -= updated_locked_vecs @ (updated_locked_vecs.T @ V)\n                V, _ = qr(V, mode='economic')\n\n        # Subspace expansion step\n        vecs_to_add_to_V = []\n        for target_idx in active_targets:\n            u, theta = active_targets[target_idx]['u'], active_targets[target_idx]['theta']\n            if u is None: continue\n            \n            residual = A @ u - theta * u\n            t = get_correction_vector(A, u, theta, residual, locked_vecs)\n            \n            if np.linalg.norm(t)  1e-8:\n                vecs_to_add_to_V.append(t)\n        \n        # Augment V using Modified Gram-Schmidt for stability\n        for v_new in vecs_to_add_to_V:\n            if V.shape[1] = max_dim_v: break\n            \n            v_ortho = v_new\n            if V.shape[1]  0:\n                v_ortho -= V @ (V.T @ v_ortho)\n            \n            norm_v_ortho = np.linalg.norm(v_ortho)\n            if norm_v_ortho  1e-8:\n                V = np.hstack([V, (v_ortho / norm_v_ortho)[:, np.newaxis]])\n\n    # Finalize results\n    final_eigvals = [np.nan] * num_targets\n    for i in range(num_targets):\n        if i in locked_pairs:\n            final_eigvals[i] = locked_pairs[i][0]\n        elif i in active_targets and active_targets[i]['theta'] is not None:\n            final_eigvals[i] = active_targets[i]['theta']  # Best effort for non-converged\n            \n    return final_eigvals\n\ndef solve():\n    # Test Suite\n    # Case D5: Diagonal matrix\n    A1 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0])\n    shifts1 = [1.2, 2.8, 4.9]\n    tol1 = 1e-10\n    max_iter1 = 150\n    max_dim_v1 = 20\n    case1 = (A1, shifts1, tol1, max_iter1, max_dim_v1)\n\n    # Case T10: Tridiagonal matrix (1D Laplacian)\n    n2 = 10\n    A2 = np.diag(2 * np.ones(n2)) - np.diag(np.ones(n2 - 1), 1) - np.diag(np.ones(n2 - 1), -1)\n    shifts2 = [0.1, 1.9, 3.9]\n    tol2 = 1e-9\n    max_iter2 = 200\n    max_dim_v2 = 25\n    case2 = (A2, shifts2, tol2, max_iter2, max_dim_v2)\n    \n    # Case R6: Repeated eigenvalue\n    A3 = np.diag([1.0, 3.0, 3.0, 3.0, 4.0, 6.0])\n    shifts3 = [2.9, 3.05, 3.1]\n    tol3 = 1e-10\n    max_iter3 = 200\n    max_dim_v3 = 25\n    case3 = (A3, shifts3, tol3, max_iter3, max_dim_v3)\n\n    # Case C6: Clustered eigenvalues\n    A4 = np.diag([1.0, 1.0001, 2.0, 3.0, 4.0, 5.0])\n    shifts4 = [0.9, 1.0002]\n    tol4 = 1e-10\n    max_iter4 = 200\n    max_dim_v4 = 25\n    case4 = (A4, shifts4, tol4, max_iter4, max_dim_v4)\n\n    test_cases = [case1, case2, case3, case4]\n    \n    all_results = []\n    for case in test_cases:\n        results = multi_target_jd(case)\n        # The order of results should match the order of shifts.\n        # This is handled by the dictionary `locked_pairs` and final list construction.\n        all_results.append(results)\n\n    # Format the final output string\n    result_strings = []\n    for res_list in all_results:\n        # Sort results for repeated roots case by value for consistent output.\n        if np.isclose(res_list[0], res_list[1], atol=1e-5):\n            res_list.sort()\n        formatted_list = [f\"{x:.14f}\" for x in res_list]\n        result_strings.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}