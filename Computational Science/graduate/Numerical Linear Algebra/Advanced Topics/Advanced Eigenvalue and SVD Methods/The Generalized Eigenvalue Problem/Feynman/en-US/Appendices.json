{
    "hands_on_practices": [
        {
            "introduction": "Before applying complex numerical algorithms, it is essential to grasp the fundamental algebraic structure of a matrix pencil. The Weierstrass canonical form provides this complete theoretical picture, revealing the nature of all finite and infinite eigenvalues, including their potential defectiveness. This exercise  guides you through an analysis of a specific pencil from first principles, allowing you to build concrete intuition for abstract concepts like Jordan chains and eigenvalues at infinity.",
            "id": "3587871",
            "problem": "Let $A,B \\in \\mathbb{C}^{3\\times 3}$ define a regular matrix pencil $A - \\lambda B$ for the generalized eigenvalue problem $A x = \\lambda B x$. Starting only from the core definitions of generalized eigenvalues, Jordan chains, and the Weierstrass canonical form, analyze the following specific pair\n$$\nA \\;=\\; \\begin{pmatrix}\n2  1  0 \\\\\n0  2  0 \\\\\n0  0  1\n\\end{pmatrix},\n\\qquad\nB \\;=\\; \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nYour tasks are:\n- Verify that the pencil $A - \\lambda B$ is regular and exhibits both a finite defective eigenvalue and an eigenvalue at infinity, using first principles.\n- Determine the complete Weierstrass canonical form of the pencil by explicitly identifying the finite Jordan blocks and the nilpotent blocks associated with the eigenvalue at infinity, together with invertible transformations that realize this canonical form.\n- Identify and justify the size of each Jordan chain for the finite eigenvalue and the nilpotent index of the infinite part.\n\nAs the final calculation, report the monic characteristic polynomial of the finite part of the pencil in its Weierstrass canonical form, written as a single closed-form expression in the indeterminate $\\lambda$.",
            "solution": "The problem requires a detailed analysis of the generalized eigenvalue problem $A x = \\lambda B x$ for the specific $3 \\times 3$ matrices\n$$\nA \\;=\\; \\begin{pmatrix}\n2  1  0 \\\\\n0  2  0 \\\\\n0  0  1\n\\end{pmatrix},\n\\qquad\nB \\;=\\; \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nThe analysis will proceed from first principles, beginning with the validation of the pencil's regularity and the nature of its eigenvalues.\n\nFirst, we verify that the pencil $A - \\lambda B$ is regular. A pencil is regular if the determinant of $A - \\lambda B$ is not identically zero for all $\\lambda \\in \\mathbb{C}$. We form the matrix $A - \\lambda B$:\n$$\nA - \\lambda B = \\begin{pmatrix}\n2  1  0 \\\\\n0  2  0 \\\\\n0  0  1\n\\end{pmatrix} - \\lambda \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  0\n\\end{pmatrix} = \\begin{pmatrix}\n2 - \\lambda  1  0 \\\\\n0  2 - \\lambda  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThe determinant is given by\n$$\n\\det(A - \\lambda B) = (2 - \\lambda) \\det \\begin{pmatrix} 2 - \\lambda  0 \\\\ 0  1 \\end{pmatrix} - (1) \\det \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + 0 = (2 - \\lambda)(2 - \\lambda - 0) = (2 - \\lambda)^2.\n$$\nSince $p(\\lambda) = \\det(A - \\lambda B) = (2 - \\lambda)^2$ is a non-zero polynomial, the pencil $A - \\lambda B$ is regular.\n\nThe finite eigenvalues are the roots of $p(\\lambda) = 0$. Setting $(2 - \\lambda)^2 = 0$ yields a single finite eigenvalue $\\lambda_1 = 2$ with algebraic multiplicity $2$. To determine if this eigenvalue is defective, we find its geometric multiplicity, which is the dimension of the null space of $A - \\lambda_1 B$.\nFor $\\lambda_1 = 2$, we have\n$$\nA - 2B = \\begin{pmatrix}\n0  1  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThe null space is found by solving $(A - 2B)x = 0$:\n$$\n\\begin{pmatrix}\n0  1  0 \\\\\n0  0  0 \\\\\n0  0  1\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies \\begin{cases} x_2 = 0 \\\\ x_3 = 0 \\end{cases}.\n$$\nThe solution is of the form $x = \\begin{pmatrix} x_1 \\\\ 0 \\\\ 0 \\end{pmatrix} = x_1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. The null space, $\\ker(A-2B)$, is spanned by the vector $\\begin{pmatrix} 1  0  0 \\end{pmatrix}^T$. Thus, the geometric multiplicity of $\\lambda_1 = 2$ is $\\dim(\\ker(A-2B)) = 1$. Since the geometric multiplicity ($1$) is less than the algebraic multiplicity ($2$), the finite eigenvalue $\\lambda_1 = 2$ is defective.\n\nAn eigenvalue at infinity exists if the matrix $B$ is singular. We compute its determinant:\n$$\n\\det(B) = \\det \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} = 0.\n$$\nSince $\\det(B) = 0$, there is an eigenvalue at infinity. The algebraic multiplicity of the infinite eigenvalue is given by $n - \\deg(p(\\lambda))$, where $n=3$ is the matrix dimension and $\\deg(p(\\lambda))=2$. So, the algebraic multiplicity is $3 - 2 = 1$. The geometric multiplicity of the infinite eigenvalue is $\\dim(\\ker(B))$.\n$$\nB x = 0 \\implies \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies \\begin{cases} x_1 = 0 \\\\ x_2 = 0 \\end{cases}.\n$$\nThe null space $\\ker(B)$ is spanned by $\\begin{pmatrix} 0  0  1 \\end{pmatrix}^T$, so its dimension is $1$. Since the algebraic and geometric multiplicities are both $1$, the infinite eigenvalue is not defective.\n\nNext, we determine the Weierstrass canonical form (WCF) of the pencil. The WCF of a regular pencil $A - \\lambda B$ consists of two block-diagonal matrices, $P A Q$ and $P B Q$, for some invertible matrices $P$ and $Q$. The form is $P A Q = \\begin{pmatrix} J  0 \\\\ 0  I \\end{pmatrix}$ and $P B Q = \\begin{pmatrix} I  0 \\\\ 0  N \\end{pmatrix}$, where $J$ contains the Jordan blocks for the finite eigenvalues and $N$ contains the nilpotent blocks for the infinite eigenvalue.\nOur analysis indicates:\n- A finite part of size $2 \\times 2$ corresponding to the defective eigenvalue $\\lambda=2$. This must be a single Jordan block $J_2(2)$.\n- An infinite part of size $1 \\times 1$. This corresponds to a non-defective infinite eigenvalue, so the nilpotent block is $N_1 = (0)$.\n\nThus, the canonical structure is given by $J = J_2(2) = \\begin{pmatrix} 2  1 \\\\ 0  2 \\end{pmatrix}$ and $N = (0)$. The full canonical matrices are:\n$$\nA' = \\begin{pmatrix} J  0 \\\\ 0  I_1 \\end{pmatrix} = \\begin{pmatrix} 2  1  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}, \\qquad B' = \\begin{pmatrix} I_2  0 \\\\ 0  N \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix}.\n$$\nWe observe that $A' = A$ and $B' = B$. The given matrices are already in Weierstrass canonical form. This implies that the transformation matrices can be chosen as the identity matrices, $P = I$ and $Q = I$.\n\nTo confirm this, we must identify the Jordan chains, which form the columns of $Q$.\nFor the finite eigenvalue $\\lambda=2$, we require a Jordan chain of length $2$, $\\{q_1, q_2\\}$, satisfying:\n$(A-2B)q_1 = 0$ (eigenvector)\n$(A-2B)q_2 = Bq_1$ (generalized eigenvector)\nFrom our earlier calculation, we can choose $q_1 = \\begin{pmatrix} 1  0  0 \\end{pmatrix}^T = e_1$.\nThen, we must solve $(A-2B)q_2 = B e_1$.\n$B e_1 = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_1$.\nThe equation becomes:\n$$\n\\begin{pmatrix} 0  1  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix} q_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis implies the components of $q_2$ satisfy $q_{2,2} = 1$ and $q_{2,3} = 0$. The component $q_{2,1}$ is arbitrary. We can choose the simplest solution, $q_2 = \\begin{pmatrix} 0  1  0 \\end{pmatrix}^T = e_2$.\nThus, the Jordan chain for $\\lambda=2$ is $\\{e_1, e_2\\}$, and its size is $2$.\n\nFor the eigenvalue at infinity, we need a chain of vectors $\\{q_3, \\dots\\}$ starting with $q_3$ such that $Bq_3=0$. From our earlier calculation, we can choose $q_3 = \\begin{pmatrix} 0  0  1 \\end{pmatrix}^T = e_3$. A chain for an infinite eigenvalue has length $k$ if there exist vectors $q_3, \\dots, q_{k+2}$ s.t. $B q_3=0, B q_4 = A q_3, \\dots, B q_{k+2}=A q_{k+1}$, and $A q_{k+2}$ is not in the range of $B$. Here we check if the chain can be extended to length $2$. This would require solving $B x = A q_3$.\n$A q_3 = A e_3 = \\begin{pmatrix} 2  1  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = e_3$.\nThe equation is $B x = e_3$:\n$$\n\\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nThe last row gives the contradiction $0 = 1$. Thus, there is no solution, and the chain cannot be extended. The chain for the infinite eigenvalue is $\\{e_3\\}$, and its length is $1$. The nilpotent index of the infinite part is the size of the largest nilpotent block, which is $1 \\times 1$. The index is therefore $1$.\n\nThe matrix $Q$ is constructed from these chain vectors: $Q = [q_1, q_2, q_3] = [e_1, e_2, e_3] = I$. With $P=I$ and $Q=I$, we have $PAQ=A$ and $PBQ=B$, confirming that the original matrices are the canonical form.\n\nFinally, we are asked for the monic characteristic polynomial of the finite part of the pencil in its Weierstrass canonical form. The finite part of the pencil is described by the pair $(J, I_2)$, forming the pencil $J - \\lambda I_2$. The finite Jordan block is $J = \\begin{pmatrix} 2  1 \\\\ 0  2 \\end{pmatrix}$. The characteristic polynomial is:\n$$\n\\det(J - \\lambda I_2) = \\det \\begin{pmatrix} 2 - \\lambda  1 \\\\ 0  2 - \\lambda \\end{pmatrix} = (2 - \\lambda)^2.\n$$\nTo make this polynomial monic, we multiply by $(-1)^2 = 1$:\n$$\n(\\lambda - 2)^2 = \\lambda^2 - 4\\lambda + 4.\n$$\nThis is the required monic characteristic polynomial.",
            "answer": "$$\n\\boxed{(\\lambda-2)^2}\n$$"
        },
        {
            "introduction": "A key motivation for specialized algorithms for the generalized eigenvalue problem $A x = \\lambda B x$ is that the naive approach of solving the standard problem $B^{-1}A x = \\lambda x$ can be numerically disastrous. This practice  has you explore a classic example where two well-conditioned matrices $A$ and $B$ produce an extremely sensitive, or ill-conditioned, standard eigenvalue problem. By deriving the generalized eigenvalue condition number, you will understand why stability analysis must consider the pencil $(A, B)$ directly.",
            "id": "3587876",
            "problem": "Let $n=2$ and let $\\varepsilon \\in (0,1)$ be a real parameter. Consider the generalized eigenvalue problem $A_{\\varepsilon} x = \\lambda B x$ with\n$$\nA_{\\varepsilon} \\;=\\; \\begin{pmatrix} 1  1 \\\\ \\varepsilon  1 \\end{pmatrix}, \n\\qquad\nB \\;=\\; I_{2}.\n$$\nBy definition, the generalized eigenvalues are the roots of $\\det(A_{\\varepsilon} - \\lambda B) = 0$, equivalently the eigenvalues of $B^{-1}A_{\\varepsilon} = A_{\\varepsilon}$. Work from first principles in numerical linear algebra: start from the definition of the generalized eigenvalue problem and the first-order perturbation model for $(A,B)$, and use norm-based reasoning to arrive at a condition number for a simple generalized eigenvalue in terms of its left and right eigenvectors.\n\nYour tasks are:\n\n- Justify, using only foundational definitions, that $B$ is well-conditioned in the matrix $2$-norm for every $\\varepsilon \\in (0,1)$, and that $A_{\\varepsilon}$ remains well-conditioned as $\\varepsilon \\to 0^{+}$ in the matrix $2$-norm. You may quantify the conditioning of $A_{\\varepsilon}$ by analyzing the eigenvalues of $A_{\\varepsilon}^{*}A_{\\varepsilon}$.\n\n- Let $\\lambda_{+}(\\varepsilon)$ denote the larger generalized eigenvalue of the pencil $(A_{\\varepsilon},B)$. Derive, from the perturbation model for $A x = \\lambda B x$, a closed-form expression for the eigenvalue condition number of $\\lambda_{+}(\\varepsilon)$ in the $2$-norm in terms of $\\varepsilon$, expressed using the left and right eigenvectors of the pencil $(A_{\\varepsilon},B)$.\n\nProvide as your final answer the exact analytic expression you obtain for this eigenvalue condition number as a function of $\\varepsilon$. No numerical rounding is required, and no units apply.",
            "solution": "The problem consists of two main tasks. First, to analyze the conditioning of the matrices $B$ and $A_{\\varepsilon}$. Second, to derive the condition number for the larger eigenvalue of the pencil $(A_{\\varepsilon}, B)$.\n\n_**Part 1: Conditioning of Matrices $B$ and $A_{\\varepsilon}$**_\n\nThe condition number of an invertible matrix $M$ in the matrix $2$-norm is defined as $\\kappa_2(M) = \\|M\\|_2 \\|M^{-1}\\|_2$. A matrix is considered well-conditioned if its condition number is close to $1$.\n\n**Conditioning of $B$:**\nThe matrix $B$ is the $2 \\times 2$ identity matrix, $B = I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$. The matrix $2$-norm, $\\|M\\|_2$, is the largest singular value of $M$, $\\sigma_{\\max}(M)$. The singular values are the square roots of the eigenvalues of $M^*M$.\nFor $B = I_2$, we have $B^*B = I_2^T I_2 = I_2$. The eigenvalues of $I_2$ are $\\lambda_1 = 1$ and $\\lambda_2 = 1$. The singular values of $B$ are thus $\\sigma_1 = \\sqrt{1} = 1$ and $\\sigma_2 = \\sqrt{1} = 1$.\nTherefore, $\\|B\\|_2 = \\sigma_{\\max}(B) = 1$.\nThe inverse of $B$ is $B^{-1} = I_2^{-1} = I_2$. Consequently, $\\|B^{-1}\\|_2 = \\|I_2\\|_2 = 1$.\nThe condition number of $B$ is $\\kappa_2(B) = \\|B\\|_2 \\|B^{-1}\\|_2 = 1 \\times 1 = 1$.\nA condition number of $1$ is the minimum possible value, which signifies that the matrix $B=I_2$ is perfectly well-conditioned. This holds for any value of the parameter $\\varepsilon$, as $B$ is constant.\n\n**Conditioning of $A_{\\varepsilon}$:**\nThe matrix $A_{\\varepsilon}$ is given by $A_{\\varepsilon} = \\begin{pmatrix} 1  1 \\\\ \\varepsilon  1 \\end{pmatrix}$. Its condition number is $\\kappa_2(A_{\\varepsilon}) = \\|A_{\\varepsilon}\\|_2 \\|A_{\\varepsilon}^{-1}\\|_2$. Since $A_\\varepsilon$ is invertible for $\\varepsilon \\in (0,1)$ ($\\det(A_\\varepsilon) = 1-\\varepsilon \\neq 0$), we can express the condition number as the ratio of the largest to the smallest singular value: $\\kappa_2(A_{\\varepsilon}) = \\frac{\\sigma_{\\max}(A_{\\varepsilon})}{\\sigma_{\\min}(A_{\\varepsilon})}$.\nThe singular values of $A_{\\varepsilon}$ are the square roots of the eigenvalues of $A_{\\varepsilon}^* A_{\\varepsilon} = A_{\\varepsilon}^T A_{\\varepsilon}$.\nWe compute $A_{\\varepsilon}^T A_{\\varepsilon}$:\n$$\nA_{\\varepsilon}^T A_{\\varepsilon} = \\begin{pmatrix} 1  \\varepsilon \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ \\varepsilon  1 \\end{pmatrix} = \\begin{pmatrix} 1+\\varepsilon^2  1+\\varepsilon \\\\ 1+\\varepsilon  2 \\end{pmatrix}\n$$\nLet $\\mu$ be an eigenvalue of $A_{\\varepsilon}^T A_{\\varepsilon}$. The eigenvalues are the roots of the characteristic equation $\\det(A_{\\varepsilon}^T A_{\\varepsilon} - \\mu I) = 0$:\n$$\n\\det \\begin{pmatrix} 1+\\varepsilon^2-\\mu  1+\\varepsilon \\\\ 1+\\varepsilon  2-\\mu \\end{pmatrix} = (1+\\varepsilon^2-\\mu)(2-\\mu) - (1+\\varepsilon)^2 = 0\n$$\nExpanding this gives:\n$$\n\\mu^2 - (3+\\varepsilon^2)\\mu + (2+2\\varepsilon^2) - (1+2\\varepsilon+\\varepsilon^2) = 0\n$$\n$$\n\\mu^2 - (3+\\varepsilon^2)\\mu + (1-2\\varepsilon+\\varepsilon^2) = 0\n$$\n$$\n\\mu^2 - (3+\\varepsilon^2)\\mu + (1-\\varepsilon)^2 = 0\n$$\nTo analyze the conditioning as $\\varepsilon \\to 0^{+}$, we examine the limit of the eigenvalues $\\mu$. As $\\varepsilon \\to 0^{+}$, the matrix $A_{\\varepsilon}$ approaches $A_0 = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$, and the characteristic equation for the eigenvalues of $A_0^T A_0$ becomes:\n$$\n\\mu^2 - 3\\mu + 1 = 0\n$$\nThe roots of this quadratic equation are $\\mu = \\frac{3 \\pm \\sqrt{9-4}}{2} = \\frac{3 \\pm \\sqrt{5}}{2}$. Both roots are positive and finite. Let them be $\\mu_{\\max,0} = \\frac{3+\\sqrt{5}}{2}$ and $\\mu_{\\min,0} = \\frac{3-\\sqrt{5}}{2}$.\nThe singular values of $A_0$ are $\\sigma_{\\max}(A_0) = \\sqrt{\\mu_{\\max,0}}$ and $\\sigma_{\\min}(A_0) = \\sqrt{\\mu_{\\min,0}}$.\nThe condition number of $A_0$ is:\n$$\n\\kappa_2(A_0) = \\frac{\\sigma_{\\max}(A_0)}{\\sigma_{\\min}(A_0)} = \\sqrt{\\frac{\\mu_{\\max,0}}{\\mu_{\\min,0}}} = \\sqrt{\\frac{3+\\sqrt{5}}{3-\\sqrt{5}}} = \\sqrt{\\frac{(3+\\sqrt{5})^2}{(3-\\sqrt{5})(3+\\sqrt{5})}} = \\sqrt{\\frac{9+6\\sqrt{5}+5}{9-5}} = \\sqrt{\\frac{14+6\\sqrt{5}}{4}} = \\frac{3+\\sqrt{5}}{2}\n$$\nSince the eigenvalues $\\mu$ of $A_{\\varepsilon}^T A_{\\varepsilon}$ are continuous functions of $\\varepsilon$, the condition number $\\kappa_2(A_{\\varepsilon})$ is also a continuous function of $\\varepsilon$. Therefore, as $\\varepsilon \\to 0^{+}$,\n$$\n\\lim_{\\varepsilon \\to 0^{+}} \\kappa_2(A_{\\varepsilon}) = \\kappa_2(A_0) = \\frac{3+\\sqrt{5}}{2} \\approx 2.618\n$$\nSince the condition number of $A_{\\varepsilon}$ approaches a small, finite value as $\\varepsilon \\to 0^{+}$, the matrix $A_{\\varepsilon}$ remains well-conditioned in this limit.\n\n_**Part 2: Eigenvalue Condition Number**_\n\nWe start from the first-order perturbation model for the generalized eigenvalue problem $Ax = \\lambda Bx$. Let $\\lambda$ be a simple eigenvalue with corresponding right eigenvector $x$ and left eigenvector $y$, where $y^*A = \\lambda y^*B$. Consider a small perturbation to the matrices, $A \\to A+\\delta A$ and $B \\to B+\\delta B$, which results in perturbations to the eigenvalue and eigenvector: $\\lambda \\to \\lambda+\\delta\\lambda$ and $x \\to x+\\delta x$.\nThe perturbed system is $(A+\\delta A)(x+\\delta x) = (\\lambda+\\delta\\lambda)(B+\\delta B)(x+\\delta x)$.\nExpanding this and retaining only first-order terms in $\\delta A, \\delta B, \\delta\\lambda, \\delta x$:\n$$\nAx + A\\delta x + \\delta A x \\approx \\lambda Bx + \\lambda B\\delta x + \\lambda\\delta Bx + \\delta\\lambda Bx\n$$\nUsing the unperturbed equation $Ax = \\lambda Bx$, we simplify to:\n$$\n(A-\\lambda B)\\delta x + (\\delta A - \\lambda\\delta B)x \\approx \\delta\\lambda Bx\n$$\nLeft-multiplying by $y^*$:\n$$\ny^*(A-\\lambda B)\\delta x + y^*(\\delta A - \\lambda\\delta B)x \\approx y^*(\\delta\\lambda Bx)\n$$\nSince $y^*$ is the left eigenvector, $y^*(A-\\lambda B) = y^*A - \\lambda y^*B = 0$. The first term vanishes.\n$$\ny^*(\\delta A - \\lambda\\delta B)x = \\delta\\lambda (y^*Bx)\n$$\nAssuming $\\lambda$ is a simple eigenvalue, it can be shown that $y^*Bx \\neq 0$. Thus, we can express the eigenvalue perturbation $\\delta \\lambda$ as:\n$$\n\\delta\\lambda = \\frac{y^*(\\delta A - \\lambda\\delta B)x}{y^*Bx}\n$$\nFrom this expression, we can define the absolute condition number of the eigenvalue $\\lambda$ as the quantity that bounds the magnitude of $\\delta\\lambda$ in terms of the magnitudes of the perturbations. Using the triangle inequality and properties of matrix and vector norms:\n$$\n|\\delta\\lambda| \\le \\frac{|y^*(\\delta A)x| + |\\lambda||y^*(\\delta B)x|}{|y^*Bx|} \\le \\frac{\\|\\delta A\\|_2 \\|y\\|_2 \\|x\\|_2 + |\\lambda|\\|\\delta B\\|_2 \\|y\\|_2 \\|x\\|_2}{|y^*Bx|}\n$$\nThe expression $k(\\lambda) = \\frac{\\|y\\|_2\\|x\\|_2}{|y^*Bx|}$ is the standard definition of the condition number for a simple generalized eigenvalue $\\lambda$ of the pencil $(A,B)$.\n\nNow, we apply this to the specific problem. Here, $A = A_{\\varepsilon} = \\begin{pmatrix} 1  1 \\\\ \\varepsilon  1 \\end{pmatrix}$ and $B = I_2$. The generalized eigenvalue problem is equivalent to the standard one, $A_{\\varepsilon}x = \\lambda x$. The eigenvalues are the roots of $\\det(A_{\\varepsilon} - \\lambda I) = 0$:\n$$\n(1-\\lambda)^2 - \\varepsilon = 0 \\implies \\lambda = 1 \\pm \\sqrt{\\varepsilon}\n$$\nWe are interested in the larger eigenvalue, $\\lambda_{+}(\\varepsilon) = 1 + \\sqrt{\\varepsilon}$.\n\n**Right eigenvector for $\\lambda_+$**: We solve $(A_{\\varepsilon} - \\lambda_+ I)x = 0$:\n$$\n\\begin{pmatrix} 1-(1+\\sqrt{\\varepsilon})  1 \\\\ \\varepsilon  1-(1+\\sqrt{\\varepsilon}) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -\\sqrt{\\varepsilon}  1 \\\\ \\varepsilon  -\\sqrt{\\varepsilon} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe first row implies $-\\sqrt{\\varepsilon}x_1 + x_2 = 0$, so $x_2 = \\sqrt{\\varepsilon} x_1$. We can choose the right eigenvector $x$ as:\n$$\nx = \\begin{pmatrix} 1 \\\\ \\sqrt{\\varepsilon} \\end{pmatrix}\n$$\nThe $2$-norm of $x$ is $\\|x\\|_2 = \\sqrt{1^2 + (\\sqrt{\\varepsilon})^2} = \\sqrt{1+\\varepsilon}$.\n\n**Left eigenvector for $\\lambda_+$**: We solve $y^*A_{\\varepsilon} = \\lambda_+ y^*$, which is equivalent to $A_{\\varepsilon}^T y = \\lambda_+ y$:\n$$\n\\left( \\begin{pmatrix} 1  \\varepsilon \\\\ 1  1 \\end{pmatrix} - (1+\\sqrt{\\varepsilon})I \\right) y = \\begin{pmatrix} -\\sqrt{\\varepsilon}  \\varepsilon \\\\ 1  -\\sqrt{\\varepsilon} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe second row implies $y_1 - \\sqrt{\\varepsilon}y_2 = 0$, so $y_1 = \\sqrt{\\varepsilon} y_2$. We can choose the left eigenvector $y$ as:\n$$\ny = \\begin{pmatrix} \\sqrt{\\varepsilon} \\\\ 1 \\end{pmatrix}\n$$\nThe $2$-norm of $y$ is $\\|y\\|_2 = \\sqrt{(\\sqrt{\\varepsilon})^2 + 1^2} = \\sqrt{1+\\varepsilon}$.\n\n**Denominator term**: We compute $y^* B x$. Since $B = I_2$ and the vectors are real, this is $y^T x$:\n$$\ny^T x = \\begin{pmatrix} \\sqrt{\\varepsilon}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\sqrt{\\varepsilon} \\end{pmatrix} = (\\sqrt{\\varepsilon})(1) + (1)(\\sqrt{\\varepsilon}) = 2\\sqrt{\\varepsilon}\n$$\nSince $\\varepsilon \\in (0,1)$, this term is non-zero, as expected for a simple eigenvalue.\n\n**Condition number**: We substitute these components into the formula for $k(\\lambda_+)$:\n$$\nk(\\lambda_{+}) = \\frac{\\|y\\|_2\\|x\\|_2}{|y^* B x|} = \\frac{(\\sqrt{1+\\varepsilon})(\\sqrt{1+\\varepsilon})}{|2\\sqrt{\\varepsilon}|}\n$$\nSince $\\varepsilon  0$, $|2\\sqrt{\\varepsilon}| = 2\\sqrt{\\varepsilon}$.\n$$\nk(\\lambda_{+}) = \\frac{1+\\varepsilon}{2\\sqrt{\\varepsilon}}\n$$\nThis is the closed-form expression for the eigenvalue condition number of $\\lambda_{+}(\\varepsilon)$.",
            "answer": "$$\\boxed{\\frac{1+\\varepsilon}{2\\sqrt{\\varepsilon}}}$$"
        },
        {
            "introduction": "The industry-standard QZ algorithm transforms a pencil $(A, B)$ into a generalized Schur form $(S, T)$, where eigenvalues are read from the diagonal pairs. In the real world of finite-precision arithmetic, interpreting this output requires careful numerical judgment, especially when identifying infinite eigenvalues which theoretically correspond to zero diagonal entries in $T$. This hands-on coding task  challenges you to design a robust, scale-invariant criterion to distinguish true infinite eigenvalues from finite ones with large magnitudes, a critical step in building reliable scientific computing tools.",
            "id": "3587883",
            "problem": "You are tasked with building a principled detector and separator for eigenvalues at infinity in the generalized eigenvalue problem, starting from the matrix pencil definition and the generalized Schur decomposition. Let $A,B\\in\\mathbb{C}^{n\\times n}$ define a matrix pencil $A-\\lambda B$. A generalized eigenvalue is any $\\lambda\\in\\mathbb{C}\\cup\\{\\infty\\}$ for which there exists a nonzero vector $x$ such that $(A-\\lambda B)x=0$. When $B$ is singular, the pencil can have eigenvalues at $\\lambda=\\infty$. The generalized Schur decomposition (QZ) produces unitary matrices $Q$ and $Z$ and upper triangular matrices $S$ and $T$ such that $Q^{*}AZ=S$ and $Q^{*}BZ=T$. In the complex generalized Schur form, diagonal pairs $(\\alpha,\\beta)=(S_{ii},T_{ii})$ parameterize the pencilâ€™s spectral information. Your task is to develop, justify, and implement a robust, scale-invariant criterion to decide whether an individual pair $(\\alpha,\\beta)$ encodes an eigenvalue at $\\lambda=\\infty$, and then to separate the spectrum into two disjoint lists: finite eigenvalues and infinite eigenvalues. The criterion must be grounded in backward error and machine precision considerations, without relying on absolute thresholds that are not scale-invariant.\n\nFundamental base to use for your derivation and algorithm:\n- The generalized eigenvalue problem definition: $(A-\\lambda B)x=0$.\n- The generalized Schur decomposition existence: there exist unitary $Q$ and $Z$ with $Q^{*}AZ=S$ and $Q^{*}BZ=T$, where $S$ and $T$ are upper triangular.\n- Numerical backward stability at the level of machine precision $\\epsilon_{\\text{mach}}$ for unitary transformations and triangularization.\n- Scale invariance of the representation of generalized eigenvalues by diagonal pairs $(\\alpha,\\beta)$.\n\nDesign and implement a program that:\n- Computes the complex generalized Schur form $(S,T)$ for each test pencil $(A,B)$ and extracts the diagonal pairs $(\\alpha_i,\\beta_i)$.\n- Proposes and uses a scale-invariant decision rule based on normalizing $(\\alpha_i,\\beta_i)$ by their magnitude and comparing the normalized $\\lvert\\beta_i\\rvert$ to a tolerance that depends on $n$ and $\\epsilon_{\\text{mach}}$, to decide whether $\\lambda_i=\\infty$.\n- Separates the spectrum into two lists: the finite eigenvalues $\\lambda_i=\\alpha_i/\\beta_i$ for indices deemed finite, and the infinite eigenvalues for indices deemed infinite. For the finite list, require the values to be returned as real numbers when the imaginary part is within numerical noise; you must discard eigenvalues whose imaginary part exceeds a validation tolerance and instead return their real parts only if the imaginary part is smaller than the tolerance (justify your tolerance in the solution).\n- Validates that the finite eigenvalues you return are real to within a small tolerance consistent with $\\epsilon_{\\text{mach}}$.\n\nTest suite. Use the following four $n=4$ pencils that exercise rank deficiency, full rank, and near-singularity:\n- Case $1$: $A=\\operatorname{diag}(1,2,3,4)$, $B=\\operatorname{diag}(1,0,0,2)$.\n- Case $2$: $A=\\operatorname{diag}(4,3,2,1)$, $B=\\operatorname{diag}(5,4,3,2)$.\n- Case $3$: $A=\\operatorname{diag}(1,2,3,4)$, $B=\\operatorname{diag}(10^{-16},2,3,4)$.\n- Case $4$: $A=\\begin{bmatrix}2100\\\\1200\\\\0031\\\\0013\\end{bmatrix}$, $B=\\operatorname{diag}(0,2,2,2)$.\n\nFor each case, your program must produce:\n- The integer count of indices detected as infinite eigenvalues.\n- The integer count of indices detected as finite eigenvalues.\n- A list of the finite eigenvalues as floats, sorted in nondecreasing order, each rounded to $6$ decimal places.\n- A boolean asserting that all detected finite eigenvalues have imaginary parts bounded by your validation tolerance and that all detected infinite indices satisfy your criterion.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a nested list in the form $[k_{\\infty},k_{\\mathrm{fin}},[\\lambda_1,\\dots],\\text{boolean}]$. For example, the output should look like $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$. No external input is permitted, and there are no physical units involved in this task.",
            "solution": "The problem of detecting and separating infinite eigenvalues in a generalized eigenvalue problem is a fundamental task in numerical linear algebra. We begin by establishing the theoretical framework and then derive a numerically robust algorithm as required.\n\nThe generalized eigenvalue problem (GEP) for a pair of matrices, or a matrix pencil, $A-\\lambda B$ with $A, B \\in \\mathbb{C}^{n\\times n}$, is to find scalars $\\lambda$ and non-zero vectors $x$ such that $(A-\\lambda B)x=0$.\nIf $B$ is nonsingular, the problem can be converted to a standard eigenvalue problem $B^{-1}Ax = \\lambda x$. However, if $B$ is singular, this transformation is not possible. A singular $B$ matrix can lead to a new phenomenon: infinite eigenvalues. To properly handle this, we represent the eigenvalue using a homogeneous pair of coordinates $(\\alpha, \\beta)$. The GEP is rewritten as $\\beta A x = \\alpha B x$, or $(\\beta A - \\alpha B)x = 0$. An eigenvalue is now represented by any pair $(\\alpha, \\beta) \\neq (0,0)$ that allows for a non-zero solution vector $x$. If $\\beta \\neq 0$, we recover the finite eigenvalue $\\lambda = \\alpha / \\beta$. If $\\beta = 0$ and $\\alpha \\neq 0$, the equation becomes $\\alpha B x = 0$, which for $\\alpha \\neq 0$ implies $B x = 0$. This means $x$ is in the null space of $B$. The full GEP equation for this case, $(A-\\lambda B)x=0$, can only be satisfied for an infinitely large $\\lambda$. Thus, a pair $(\\alpha, 0)$ with $\\alpha \\neq 0$ corresponds to an eigenvalue at infinity.\n\nThe standard numerically stable method for solving the GEP is the generalized Schur (or QZ) decomposition. It states that for any pair $(A, B)$, there exist unitary matrices $Q$ and $Z$ such that $S=Q^*AZ$ and $T=Q^*BZ$ are both upper triangular. The pencil $A-\\lambda B$ is therefore unitarily equivalent to $S-\\lambda T$. The generalized eigenvalues are preserved under this transformation. For the triangular pencil $S-\\lambda T$, the eigenvalues are determined by the diagonal elements. The condition $\\text{det}(S-\\lambda T) = 0$ becomes $\\prod_{i=1}^{n} (S_{ii} - \\lambda T_{ii}) = 0$. This implies that the generalized eigenvalues are given by the pairs $(\\alpha_i, \\beta_i) = (S_{ii}, T_{ii})$ for $i=1, \\dots, n$. In exact arithmetic, an infinite eigenvalue corresponds precisely to a pair where $\\beta_i = 0$ and $\\alpha_i \\neq 0$.\n\nIn finite-precision floating-point arithmetic, a true zero is rarely computed. A value that is theoretically zero will be represented as a small number of the order of machine precision, scaled by the problem's norms. Therefore, a simple check like $\\beta_i = 0$ is not robust. Furthermore, the magnitudes of $\\alpha_i$ and $\\beta_i$ depend on the scaling of the input matrices $A$ and $B$. A criterion based on an absolute threshold for $|\\beta_i|$ would not be scale-invariant. For instance, replacing $B$ with $c B$ scales all $\\beta_i$ by $c$, which would break a fixed threshold.\n\nTo devise a robust and scale-invariant criterion, we must consider the pair $(\\alpha_i, \\beta_i)$ as representing a ratio. The scale-invariance of the eigenvalue $\\lambda_i = \\alpha_i/\\beta_i$ suggests that the \"infiniteness\" of an eigenvalue should only depend on the ratio of $\\beta_i$ to $\\alpha_i$, or more robustly, on the proportion of $\\beta_i$ in the pair. We can normalize the pair by its Euclidean norm:\n$$ (\\alpha'_i, \\beta'_i) = \\frac{(\\alpha_i, \\beta_i)}{\\sqrt{|\\alpha_i|^2 + |\\beta_i|^2}} $$\nThis normalization ensures that $|\\alpha'_i|^2 + |\\beta'_i|^2 = 1$. The normalized value $|\\beta'_i|$ is now scale-invariant. An infinite eigenvalue, corresponding to a theoretical pair $(\\alpha_i, 0)$, will result in a computed pair where $|\\beta_i|$ is small relative to $|\\alpha_i|$, and thus $|\\beta'_i|$ will be close to zero.\nThe question becomes: how small must $|\\beta'_i|$ be to declare the eigenvalue infinite? The QZ algorithm is backward stable, meaning the computed Schur forms $S$ and $T$ are exact for slightly perturbed matrices $A+E_A$ and $B+E_B$, where $\\|E_A\\|$ and $\\|E_B\\|$ are on the order of $\\epsilon_{\\text{mach}} \\|A\\|$ and $\\epsilon_{\\text{mach}} \\|B\\|$, respectively. This suggests that the perturbations in the elements of $S$ and $T$ are on the order of $\\epsilon_{\\text{mach}}$ times some relevant norm. A widely accepted heuristic in numerical linear algebra is to use a tolerance proportional to the problem size $n$ and machine epsilon $\\epsilon_{\\text{mach}}$. We thus propose the following criterion: an eigenvalue $\\lambda_i$ corresponding to the pair $(\\alpha_i, \\beta_i)$ is considered infinite if\n$$ |\\beta'_i| = \\frac{|\\beta_i|}{\\sqrt{|\\alpha_i|^2 + |\\beta_i|^2}}  n \\cdot \\epsilon_{\\text{mach}} $$\nThis tolerance, $\\tau_{\\infty} = n \\cdot \\epsilon_{\\text{mach}}$, is independent of the input matrix scaling and robustly distinguishes between numerically small and theoretically zero values of $\\beta_i$.\n\nFor the indices $i$ that are identified as corresponding to finite eigenvalues (i.e., $|\\beta'_i| \\ge \\tau_{\\infty}$), the eigenvalue is computed as $\\lambda_i = \\alpha_i/\\beta_i$. The problem specifies that the input matrices are real. In theory, all eigenvalues should therefore be real or occur in complex conjugate pairs. The test cases are structured such that all true eigenvalues are real. However, numerical computation may introduce small, spurious imaginary parts. The problem requires us to filter the computed finite eigenvalues, retaining only those that are \"real to within numerical noise\". A finite eigenvalue $\\lambda_i$ is accepted if its imaginary part is sufficiently small. We establish a validation tolerance for this check. Error propagation in the division $\\alpha_i/\\beta_i$ can amplify initial errors, so a slightly more generous tolerance is warranted. A conservative but safe choice is another heuristic tolerance based on problem size and machine precision:\n$$ |\\text{Im}(\\lambda_i)|  \\tau_{\\text{imag}} $$\nwhere we define $\\tau_{\\text{imag}} = 100 \\cdot n \\cdot \\epsilon_{\\text{mach}}$. If this condition is met, we accept the eigenvalue as numerically real and retain its real part, $\\text{Re}(\\lambda_i)$. If the condition is not met, the eigenvalue is discarded from the final list.\n\nThe final algorithm proceeds as follows for each test case $(A,B)$:\n1. Set the dimension $n$ and machine epsilon $\\epsilon_{\\text{mach}}$. Define the tolerances $\\tau_{\\infty} = n \\cdot \\epsilon_{\\text{mach}}$ and $\\tau_{\\text{imag}} = 100 \\cdot n \\cdot \\epsilon_{\\text{mach}}$.\n2. Compute the complex generalized Schur decomposition $S = Q^*AZ$ and $T = Q^*BZ$ using a library function.\n3. Extract the diagonal pairs $(\\alpha_i, \\beta_i) = (S_{ii}, T_{ii})$ for $i=1, \\dots, n$.\n4. Initialize a counter for infinite eigenvalues, $k_{\\infty}$, and a list for finite eigenvalues, $\\Lambda_{\\text{fin}}$.\n5. For each pair $(\\alpha_i, \\beta_i)$:\n   a. Compute the normalized $|\\beta'_i| = |\\beta_i| / \\sqrt{|\\alpha_i|^2 + |\\beta_i|^2}$. Handle the case where the norm is zero.\n   b. If $|\\beta'_i|  \\tau_{\\infty}$, increment $k_{\\infty}$.\n   c. Otherwise, compute $\\lambda_i = \\alpha_i / \\beta_i$. If $|\\text{Im}(\\lambda_i)|  \\tau_{\\text{imag}}$, append $\\text{Re}(\\lambda_i)$ to $\\Lambda_{\\text{fin}}$.\n6. The count of finite indices is $k_{\\text{fin}} = n - k_{\\infty}$.\n7. Sort the list $\\Lambda_{\\text{fin}}$ in nondecreasing order and round its elements to $6$ decimal places.\n8. The validation boolean asserts that the implemented logic is self-consistent: all indices classified as infinite satisfy the infinity criterion, and all eigenvalues included in the final list satisfy the imaginary part check. By construction, this boolean will be `True`.\nThis procedure provides a principled and robust method for separating and validating the eigenvalues of a generalized eigenvalue problem.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qz\n\ndef solve():\n    \"\"\"\n    Solves the generalized eigenvalue problem for several test cases,\n    detecting and separating infinite eigenvalues based on a robust,\n    scale-invariant criterion derived from the generalized Schur decomposition.\n    \"\"\"\n    # Test suite of matrix pencils (A, B)\n    test_cases = [\n        # Case 1: A=diag(1,2,3,4), B=diag(1,0,0,2)\n        (np.diag([1., 2., 3., 4.]), np.diag([1., 0., 0., 2.])),\n        # Case 2: A=diag(4,3,2,1), B=diag(5,4,3,2)\n        (np.diag([4., 3., 2., 1.]), np.diag([5., 4., 3., 2.])),\n        # Case 3: A=diag(1,2,3,4), B=diag(1e-16,2,3,4)\n        (np.diag([1., 2., 3., 4.]), np.diag([1e-16, 2., 3., 4.])),\n        # Case 4: A=[[2,1,0,0],[1,2,0,0],[0,0,3,1],[0,0,1,3]], B=diag(0,2,2,2)\n        (np.array([[2., 1., 0., 0.], [1., 2., 0., 0.], [0., 0., 3., 1.], [0., 0., 1., 3.]]),\n         np.diag([0., 2., 2., 2.]))\n    ]\n\n    results = []\n    for A, B in test_cases:\n        n = A.shape[0]\n        eps = np.finfo(float).eps\n\n        # Define the scale-invariant tolerance for infinity detection\n        inf_tol = n * eps\n        # Define the validation tolerance for the imaginary part of finite eigenvalues\n        imag_tol = 100 * n * eps\n\n        # Compute the complex generalized Schur (QZ) decomposition\n        S, T, _, _ = qz(A, B, output='complex')\n        \n        # Extract diagonal pairs (alpha, beta)\n        alpha = np.diag(S)\n        beta = np.diag(T)\n\n        k_inf = 0\n        finite_eigs = []\n        is_classification_valid = True\n\n        for i in range(n):\n            alpha_i, beta_i = alpha[i], beta[i]\n            \n            # Normalize the pair (alpha_i, beta_i)\n            norm = np.sqrt(np.abs(alpha_i)**2 + np.abs(beta_i)**2)\n\n            # A zero norm corresponds to an ambiguous (0,0) pair, which is ill-defined.\n            # We treat it as finite by setting norm_beta large enough.\n            if norm == 0:\n                norm_beta = 1.0\n            else:\n                norm_beta = np.abs(beta_i) / norm\n\n            # Apply the criterion to detect infinite eigenvalues\n            if norm_beta  inf_tol:\n                k_inf += 1\n            else:\n                # This is a finite eigenvalue\n                # The check for beta_i != 0 is implicitly handled by norm_beta = inf_tol\n                lam = alpha_i / beta_i\n                \n                # Validate if the eigenvalue is numerically real\n                if np.abs(lam.imag)  imag_tol:\n                    finite_eigs.append(lam.real)\n                else:\n                    # This case would indicate a complex eigenvalue for a real pencil,\n                    # beyond numerical noise. Per problem, we discard it,\n                    # and the validation boolean, which checks the final output, remains True.\n                    pass\n\n        k_fin = n - k_inf\n\n        # Sort and round the valid finite eigenvalues\n        finite_eigs.sort()\n        rounded_eigs = [round(eig, 6) for eig in finite_eigs]\n\n        # By construction, the classification is self-consistent.\n        # All detected infinite eigenvalues satisfied the criterion,\n        # and all retained finite eigenvalues passed the imaginary part check.\n        is_valid = True\n\n        results.append([k_inf, k_fin, rounded_eigs, is_valid])\n\n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}