{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for using low-rank tensor formats is to overcome the curse of dimensionality by reducing computational and storage costs. This exercise provides a direct, quantitative comparison between the Tensor-Train (TT) and Hierarchical Tucker (HT) formats by analyzing the costs of a fundamental operation: forming Gram matrices for orthogonality checks. By working through the specific structural differences, you will gain a deeper appreciation for how the underlying topology—a linear chain for TT versus a binary tree for HT—translates into distinct complexity scaling. ",
            "id": "3583915",
            "problem": "Consider a $d$-way tensor represented in the tensor-train (TT) format and, alternatively, in the hierarchical Tucker (HT) format. You will analyze the computational cost (floating-point operations, counted as additions plus multiplications) of forming Gram matrices used for orthogonality checks, along with the storage required to hold these Gram matrices, and then relate those costs to the overall complexity by comparing TT and HT under a set of scientifically standard assumptions.\n\nFor the tensor-train (TT) format: The TT representation consists of $d$ three-dimensional cores $\\mathcal{G}_{k} \\in \\mathbb{R}^{r_{k-1} \\times n \\times r_{k}}$ for $k = 1, \\dots, d$, with $r_{0} = r_{d} = 1$ and internal ranks $r_{1}, \\dots, r_{d-1}$ assumed equal to a uniform value $r \\geq 1$. The physical dimension size is the same for all modes, $n_{1} = \\dots = n_{d} = n \\geq 1$. To check left-orthogonality of the TT cores, one considers the left unfolding matrix $X_{k} \\in \\mathbb{R}^{(r_{k-1} n) \\times r_{k}}$ obtained from $\\mathcal{G}_{k}$, and forms the Gram matrix $G_{k} = X_{k}^{\\top} X_{k} \\in \\mathbb{R}^{r_{k} \\times r_{k}}$. Assume the Gram matrix is formed by straightforward dense matrix multiplication without exploiting symmetry beyond the usual arithmetic count for $X_{k}^{\\top} X_{k}$. You must analyze the total arithmetic cost of forming all Gram matrices $G_{k}$ for the left-orthogonality check across cores $k = 1, \\dots, d-1$, and the total number of scalar entries needed to store these Gram matrices.\n\nFor the hierarchical Tucker (HT) format: Consider a balanced binary dimension tree with $d$ leaves (assume $d$ is a power of $2$), where each leaf node corresponds to a physical dimension of size $n$ and is equipped with a basis matrix $U_{\\ell} \\in \\mathbb{R}^{n \\times r_{\\ell}}$ having $r_{\\ell} \\geq 1$ columns. Every interior node $t$ has a transfer tensor $B_{t} \\in \\mathbb{R}^{r_{t} \\times r_{t_{\\mathrm{left}}} \\times r_{t_{\\mathrm{right}}}}$ with uniform interior rank $r_{t} = r$ and child ranks $r_{t_{\\mathrm{left}}} = r_{t_{\\mathrm{right}}} = r$. Under the standard HT construction, the node basis $U_{t}$ at an interior node is obtained by combining its children’s bases and the transfer tensor. For orthogonality checks, the Gram matrix at a leaf is $G_{\\ell} = U_{\\ell}^{\\top} U_{\\ell} \\in \\mathbb{R}^{r_{\\ell} \\times r_{\\ell}}$, and at an interior node $t$, assuming child bases are orthonormal, the Gram matrix equals a contraction over the child indices of $B_{t}$ that yields $G_{t} \\in \\mathbb{R}^{r \\times r}$. You must analyze the total arithmetic cost of forming the Gram matrices at all nodes (leaves and interiors) in the balanced binary tree, and the total number of scalar entries needed to store these Gram matrices.\n\nUse the following scientifically standard bases for your derivations:\n- The definition of the TT cores and their left unfoldings for orthogonality checks.\n- The definition of HT node bases with transfer tensors on a balanced binary tree and the contraction that yields Gram matrices at interior nodes when child bases are orthonormal.\n- The well-tested arithmetic costs for dense matrix multiplication (forming $A^{\\top} A$ for $A \\in \\mathbb{R}^{m \\times p}$) and for dense tensor contractions counted as the total number of scalar multiply-adds.\n\nAfter deriving both TT and HT total arithmetic costs and total storage counts (in scalars), relate these to overall complexity by comparing the TT and HT arithmetic costs. Your final answer should be a single closed-form analytic expression for the ratio\n$$R(d,n,r,r_{\\ell}) = \\frac{\\text{total TT flop count for Gram formation}}{\\text{total HT flop count for Gram formation}}.$$\nDo not round; provide an exact expression for $R(d,n,r,r_{\\ell})$ without units.",
            "solution": "The objective of this problem is to analyze and compare the computational costs, measured in floating-point operations (flops), for forming Gram matrices used in orthogonality checks for tensors represented in the tensor-train (TT) and hierarchical Tucker (HT) formats. This analysis will culminate in the derivation of the ratio of these costs.\n\nFirst, we address the tensor-train (TT) format. A $d$-way tensor is represented by $d$ three-dimensional cores $\\mathcal{G}_{k} \\in \\mathbb{R}^{r_{k-1} \\times n \\times r_{k}}$ for $k = 1, \\dots, d$. The boundary ranks are $r_{0} = r_{d} = 1$, and all internal ranks are uniform, $r_{k} = r$ for $k = 1, \\dots, d-1$. The physical dimension size is uniform, $n_{k}=n$. To check for left-orthogonality, one forms the Gram matrix $G_{k} = X_{k}^{\\top} X_{k}$ for each core's left unfolding $X_{k} \\in \\mathbb{R}^{(r_{k-1} n) \\times r_{k}}$. The analysis is performed for cores indexed by $k = 1, \\dots, d-1$.\n\nThe standard cost for computing the product $A^{\\top}A$ for a matrix $A \\in \\mathbb{R}^{m \\times p}$ is approximately $2mp^{2}$ flops (counting both additions and multiplications).\nFor a generic TT core $\\mathcal{G}_{k}$, its unfolding $X_{k}$ has dimensions $m = r_{k-1}n$ and $p = r_{k}$. The flop count for computing its Gram matrix $G_{k}$ is therefore $2(r_{k-1}n)r_{k}^{2}$.\n\nWe must sum these costs over the specified range $k = 1, \\dots, d-1$.\nFor the first core ($k=1$), the ranks are $r_{0}=1$ and $r_{1}=r$. The computational cost is:\n$$ C_{1} = 2(r_{0}n)r_{1}^{2} = 2(1 \\cdot n)r^{2} = 2nr^{2} $$\nFor the subsequent internal cores ($k=2, \\dots, d-1$), the ranks are $r_{k-1}=r$ and $r_{k}=r$. The cost for each such core is:\n$$ C_{k} = 2(r_{k-1}n)r_{k}^{2} = 2(rn)r^{2} = 2nr^{3} $$\nThere are $(d-1) - 2 + 1 = d-2$ such cores.\nThe total flop count for the TT format, denoted $C_{\\text{TT}}$, is the sum of the cost for the first core and the costs for the $d-2$ other internal cores:\n$$ C_{\\text{TT}} = C_{1} + \\sum_{k=2}^{d-1} C_{k} = 2nr^{2} + (d-2)2nr^{3} $$\nFactoring out the common term $2nr^{2}$ gives:\n$$ C_{\\text{TT}} = 2nr^{2}(1 + (d-2)r) $$\nFor completeness, the total storage required for these $d-1$ Gram matrices, each of size $r \\times r$, is $(d-1)r^2$ scalars.\n\nNext, we analyze the hierarchical Tucker (HT) format. The tensor is defined on a balanced binary tree with $d$ leaves, where $d$ is a power of $2$. Such a tree has $d-1$ interior nodes.\nAt each of the $d$ leaf nodes, denoted by $\\ell$, there is a basis matrix $U_{\\ell} \\in \\mathbb{R}^{n \\times r_{\\ell}}$. The corresponding Gram matrix is $G_{\\ell} = U_{\\ell}^{\\top} U_{\\ell} \\in \\mathbb{R}^{r_{\\ell} \\times r_{\\ell}}$. The flop count for forming one such matrix is $2nr_{\\ell}^{2}$. Since there are $d$ leaves, the total cost for all leaf nodes is:\n$$ C_{\\text{leaves}} = d \\cdot (2nr_{\\ell}^{2}) = 2dnr_{\\ell}^{2} $$\nAt each of the $d-1$ interior nodes, denoted by $t$, there is a transfer tensor $B_{t} \\in \\mathbb{R}^{r_{t} \\times r_{t_{\\mathrm{left}}} \\times r_{t_{\\mathrm{right}}}}$. The problem states uniform ranks $r_{t} = r_{t_{\\mathrm{left}}} = r_{t_{\\mathrm{right}}} = r$, so $B_{t} \\in \\mathbb{R}^{r \\times r \\times r}$. The Gram matrix $G_t \\in \\mathbb{R}^{r \\times r}$ is formed by a contraction over the child indices. With $B_t$ having indices $(i,j,k)$, this corresponds to computing:\n$$ G_{t}(i, i') = \\sum_{j=1}^{r} \\sum_{k=1}^{r} B_{t}(i, j, k) B_{t}(i', j, k) $$\nThis is equivalent to forming the matrix product $M M^{\\top}$, where $M \\in \\mathbb{R}^{r \\times r^2}$ is the mode-$1$ matricization of $B_t$. The cost is $2 \\cdot r \\cdot r^2 \\cdot r = 2r^4$ flops. This cost is incurred for each of the $d-1$ interior nodes. The total cost for all interior nodes is:\n$$ C_{\\text{interiors}} = (d-1)2r^{4} $$\nThe total flop count for the HT format, $C_{\\text{HT}}$, is the sum of the costs from the leaf and interior nodes:\n$$ C_{\\text{HT}} = C_{\\text{leaves}} + C_{\\text{interiors}} = 2dnr_{\\ell}^{2} + (d-1)2r^{4} = 2(dnr_{\\ell}^{2} + (d-1)r^{4}) $$\nFor completeness, the total storage for the HT Gram matrices comprises $d$ matrices of size $r_{\\ell} \\times r_{\\ell}$ and $d-1$ matrices of size $r \\times r$, for a total of $dr_{\\ell}^{2} + (d-1)r^{2}$ scalars.\n\nFinally, we formulate the required ratio, $R(d, n, r, r_{\\ell})$, of the total TT flop count to the total HT flop count:\n$$ R(d, n, r, r_{\\ell}) = \\frac{C_{\\text{TT}}}{C_{\\text{HT}}} = \\frac{2nr^{2}(1 + (d-2)r)}{2(dnr_{\\ell}^{2} + (d-1)r^{4})} $$\nCanceling the common factor of $2$ yields the final expression:\n$$ R(d, n, r, r_{\\ell}) = \\frac{nr^{2}(1 + (d-2)r)}{dnr_{\\ell}^{2} + (d-1)r^{4}} $$",
            "answer": "$$\\boxed{\\frac{nr^{2}(1 + (d-2)r)}{dnr_{\\ell}^{2} + (d-1)r^{4}}}$$"
        },
        {
            "introduction": "The efficiency of a Tensor-Train (TT) representation depends critically on the ordering of the tensor's modes, as this determines the ranks required for a given accuracy. This practice delves into this crucial aspect of TT decomposition, guiding you to first prove how mode permutations affect TT-ranks and then to design a heuristic for finding an ordering that minimizes them. This exercise demonstrates a key practical challenge and solution in applying TT formats, combining theoretical insight with algorithmic thinking. ",
            "id": "3583892",
            "problem": "Let $d \\in \\mathbb{N}$ and let $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times \\cdots \\times n_d}$ be a real, $d$-mode tensor. Consider a permutation $\\pi \\in S_d$ acting on the $d$ modes, and let $X^\\pi$ denote the permuted tensor whose axes have been re-ordered according to $\\pi$. The Tensor-Train (TT) decomposition of $X$ is defined by a sequence of cores whose minimal TT ranks $\\{r_k\\}_{k=1}^{d-1}$ are equal to the ranks of the matrix unfoldings of $X$ across contiguous mode cuts. Specifically, for each $k \\in \\{1,\\ldots,d-1\\}$, define the unfolding that groups the first $k$ modes versus the remaining $d-k$ modes, and note that the TT rank $r_k$ equals the matrix rank of that unfolding. This problem asks you to start from foundational definitions in numerical linear algebra and multilinear algebra to establish the effect of permuting modes on TT ranks, and to design and evaluate a computational heuristic to choose a mode permutation that minimizes the maximum TT rank.\n\nYour tasks are:\n1. Using the definition of matrix rank and matricization (unfolding), show that for any permutation $\\pi \\in S_d$, the TT ranks of $X^\\pi$ satisfy\n$$\nr_k^\\pi \\;=\\; \\mathrm{rank}\\!\\left(\\mathrm{unfold}_{\\{\\pi(1),\\ldots,\\pi(k)\\}|\\{\\pi(k+1),\\ldots,\\pi(d)\\}}(X)\\right), \\quad k=1,\\ldots,d-1,\n$$\nwhere $\\mathrm{unfold}_{S|\\bar S}(X)$ denotes the matrix obtained by grouping the modes in $S$ as rows and the modes in $\\bar S$ as columns, preserving the order induced by $\\pi$. Your derivation must rely only on fundamental definitions of tensor matricization and matrix rank, and well-tested facts such as invariance of algebraic rank under permutation of rows and columns and the characterization of minimal TT ranks by ranks of unfoldings.\n\n2. Design a heuristic to choose a permutation $\\pi$ that attempts to minimize $\\max_{k \\in \\{1,\\ldots,d-1\\}} r_k^\\pi$. The heuristic must be computable from the data of $X$ alone and should be justified from first principles. Your heuristic must avoid using any target formula as a shortcut and should articulate the rationale for why the chosen strategy tends to reduce the maximum TT rank.\n\n3. Implement the following in a complete, runnable program:\n   - A function to compute the list of TT ranks $\\{r_k^\\pi\\}_{k=1}^{d-1}$ for a given tensor $X$ and permutation $\\pi$ by forming the appropriate matrix unfoldings and computing their ranks over $\\mathbb{R}$.\n   - An exhaustive search over all permutations $\\pi \\in S_d$ for the provided small test tensors to find the optimal minimal possible value of $\\max_k r_k^\\pi$; this establishes the ground truth for each test case.\n   - Your heuristic to choose a permutation $\\pi_{\\mathrm{heur}}$, and the computation of $\\max_k r_k^{\\pi_{\\mathrm{heur}}}$ for each test case.\n   - For each test case, output a boolean indicating whether the heuristic achieved the optimal minimal maximum TT rank (i.e., whether $\\max_k r_k^{\\pi_{\\mathrm{heur}}}$ equals $\\min_{\\pi \\in S_d} \\max_k r_k^\\pi$).\n\nYou must use the following test suite of tensors, all over $\\mathbb{R}$, with precisely defined entries:\n- Test Case $1$ (generic dense, $d=5$): $X \\in \\mathbb{R}^{2 \\times 3 \\times 2 \\times 3 \\times 2}$ with entries $X[i_1,i_2,i_3,i_4,i_5]$ drawn deterministically from a pseudorandom generator with a fixed seed, ensuring generic full ranks subject to dimensional limits.\n- Test Case $2$ (Kronecker structure, $d=4$): $X \\in \\mathbb{R}^{3 \\times 2 \\times 3 \\times 2}$ defined by $X[i,j,k,\\ell] = M[i,k] \\cdot N[j,\\ell]$, where $M \\in \\mathbb{R}^{3 \\times 3}$ is rank-$1$ (constructed as $M = u v^\\top$ for nonzero $u \\in \\mathbb{R}^3$, $v \\in \\mathbb{R}^3$) and $N \\in \\mathbb{R}^{2 \\times 2}$ is full rank (e.g., the identity).\n- Test Case $3$ (rank-$1$ separable, $d=6$): $X \\in \\mathbb{R}^{2 \\times 2 \\times 2 \\times 2 \\times 2 \\times 2}$ defined by $X = g^{(1)} \\otimes g^{(2)} \\otimes g^{(3)} \\otimes g^{(4)} \\otimes g^{(5)} \\otimes g^{(6)}$ for nonzero vectors $g^{(m)} \\in \\mathbb{R}^{2}$, $m=1,\\ldots,6$.\n- Test Case $4$ (structured Canonical Polyadic sum, $d=4$): $X \\in \\mathbb{R}^{3 \\times 3 \\times 2 \\times 2}$ defined by $X = a \\otimes b \\otimes c \\otimes d \\;+\\; a \\otimes b \\otimes c' \\otimes d'$, with $a \\in \\mathbb{R}^3$, $b \\in \\mathbb{R}^3$ fixed and identical in both terms, and $c,c' \\in \\mathbb{R}^2$, $d,d' \\in \\mathbb{R}^2$ chosen to be linearly independent pairs.\n\nAll ranks must be computed as algebraic ranks over $\\mathbb{R}$ via Singular Value Decomposition (SVD)-based numerical rank; angle units are not applicable; no physical units appear. Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_t$ is a boolean indicating whether the heuristic achieved the optimal minimal maximum TT rank on test case $t$.\n\nYour derivations and justifications must be based on:\n- Fundamental definitions of matrix rank and tensor matricization.\n- The characterization of minimal Tensor-Train ranks by ranks of contiguous unfoldings.\n- Well-tested facts about the invariance of matrix rank under permutations of rows and columns.\n\nNo other shortcut formulas are permitted. The implementation must be self-contained and require no user input, external files, or network access. The final output must be exactly a single line in the specified format.",
            "solution": "The problem as stated is valid. It is scientifically grounded in numerical linear algebra, well-posed, and objective. It contains a complete and consistent setup for a standard, albeit challenging, problem in tensor analysis. Proceeding to a solution.\n\n### Part 1: Derivation of TT Ranks for a Permuted Tensor\n\nLet $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times \\cdots \\times n_d}$ be a $d$-mode tensor. Let $\\pi \\in S_d$ be a permutation of the mode indices $\\{1, \\ldots, d\\}$. We define the permuted tensor $X^\\pi$ as the tensor of size $n_{\\pi(1)} \\times \\cdots \\times n_{\\pi(d)}$ whose entries are given by\n$$ (X^\\pi)_{i_1, \\ldots, i_d} = X_{j_1, \\ldots, j_d} \\quad \\text{where} \\quad j_{\\pi(k)} = i_k \\text{ for } k=1, \\ldots, d. $$\nThis definition states that the $k$-th mode of $X^\\pi$ corresponds to the $\\pi(k)$-th mode of $X$.\n\nBy the definition of minimal Tensor-Train (TT) ranks, the $k$-th TT rank of the tensor $X^\\pi$, which we denote by $r_k^\\pi$, is the rank of the matricization (unfolding) of $X^\\pi$ that groups its first $k$ modes against the remaining $d-k$ modes.\n$$ r_k^\\pi \\triangleq \\mathrm{rank}\\left(\\mathrm{unfold}_{\\{1, \\ldots, k\\} | \\{k+1, \\ldots, d\\}}(X^\\pi)\\right). $$\nLet us denote the matrix unfolding on the right-hand side as $M_\\pi$. The matrix $M_\\pi$ has dimensions $(\\prod_{m=1}^k n_{\\pi(m)}) \\times (\\prod_{m=k+1}^d n_{\\pi(m)})$. Its rows are indexed by a multi-index $(i_1, \\ldots, i_k)$ and its columns by a multi-index $(i_{k+1}, \\ldots, i_d)$. The element at matrix position corresponding to these multi-indices is $(X^\\pi)_{i_1, \\ldots, i_d}$.\n\nWe are asked to show that this rank is equivalent to the rank of a different unfolding, one applied directly to the original tensor $X$:\n$$ r_k^\\pi = \\mathrm{rank}\\left(\\mathrm{unfold}_{\\{\\pi(1), \\ldots, \\pi(k)\\} | \\{\\pi(k+1), \\ldots, \\pi(d)\\}}(X)\\right). $$\nLet us denote the matrix on the right-hand side as $M_X$. The row modes for this unfolding are $S = \\{\\pi(1), \\ldots, \\pi(k)\\}$ and the column modes are $\\bar{S} = \\{\\pi(k+1), \\ldots, \\pi(d)\\}$. Let the indices of $X$ be $(j_1, \\ldots, j_d)$, where $j_m \\in \\{1, \\ldots, n_m\\}$. The matrix $M_X$ has dimensions $(\\prod_{m \\in S} n_m) \\times (\\prod_{m \\in \\bar{S}} n_m)$. The problem statement specifies that the internal ordering of modes in the unfolding is \"induced by $\\pi$\". This means the row multi-index can be identified with the tuple $(j_{\\pi(1)}, \\ldots, j_{\\pi(k)})$ and the column multi-index with $(j_{\\pi(k+1)}, \\ldots, j_{\\pi(d)})$. The element at the matrix position corresponding to these multi-indices is $X_{j_1, \\ldots, j_d}$.\n\nLet's establish the correspondence between the matrices $M_\\pi$ and $M_X$.\nAn arbitrary element of $M_\\pi$ corresponds to an entry $(X^\\pi)_{i_1, \\ldots, i_d}$, with $(i_1, \\ldots, i_k)$ determining the row and $(i_{k+1}, \\ldots, i_d)$ determining the column. By definition, this value is equal to $X_{j_1, \\ldots, j_d}$ where $j_{\\pi(m)} = i_m$ for $m=1, \\ldots, d$.\n\nAn arbitrary element of $M_X$ corresponds to an entry $X_{j_1, \\ldots, j_d}$. Its row is determined by the indices $\\{j_m\\}_{m \\in S} = \\{j_{\\pi(1)}, \\ldots, j_{\\pi(k)}\\}$ and its column is determined by $\\{j_m\\}_{m \\in \\bar{S}} = \\{j_{\\pi(k+1)}, \\ldots, j_{\\pi(d)}\\}$.\n\nIf we substitute $i_m = j_{\\pi(m)}$ into the description of $M_\\pi$, we see the following:\n1.  The set of values contained in both matrices is identical, namely all the elements of the tensor $X$.\n2.  The row of an element in $M_\\pi$ is determined by $(i_1, \\ldots, i_k)$, which is identical to $(j_{\\pi(1)}, \\ldots, j_{\\pi(k)})$. This is precisely the set of indices that determines the row of the corresponding element in $M_X$.\n3.  The column of an element in $M_\\pi$ is determined by $(i_{k+1}, \\ldots, i_d)$, which is identical to $(j_{\\pi(k+1)}, \\ldots, j_{\\pi(d)})$. This is precisely the set of indices that determines the column of the corresponding element in $M_X$.\n\nGiven that the problem specifies \"preserving the order induced by $\\pi$\", the lexicographical mapping from multi-index to single matrix index is identical for both matrices. For instance, the row index $I$ for $M_\\pi$ corresponding to $(i_1, \\dots, i_k)$ is $I = \\sum_{m=1}^k (i_m-1) \\prod_{l=1}^{m-1} n_{\\pi(l)}$. A row index for $M_X$ corresponding to $(j_{\\pi(1)}, \\dots, j_{\\pi(k)})$ is $I'=\\sum_{m=1}^k (j_{\\pi(m)}-1) \\prod_{l=1}^{m-1} n_{\\pi(l)}$. Since $i_m = j_{\\pi(m)}$, we have $I = I'$. A similar argument holds for the column indices.\n\nTherefore, the matrices are not just permutations of one another; they are identical: $M_\\pi = M_X$.\nConsequently, their ranks must be equal:\n$$ \\mathrm{rank}(M_\\pi) = \\mathrm{rank}(M_X). $$\nThis completes the derivation, proving that\n$$ r_k^\\pi = \\mathrm{rank}\\left(\\mathrm{unfold}_{\\{\\pi(1), \\ldots, \\pi(k)\\} | \\{\\pi(k+1), \\ldots, \\pi(d)\\}}(X)\\right), \\quad k=1, \\ldots, d-1. $$\nThis result is fundamental, as it allows us to determine the TT ranks for any mode permutation by performing unfoldings on the original tensor $X$, rather than explicitly forming $X^\\pi$.\n\n### Part 2: Heuristic Design for Minimizing Maximum TT Rank\n\nOur objective is to find a permutation $\\pi \\in S_d$ that minimizes $\\max_{k \\in \\{1,\\ldots,d-1\\}} r_k^\\pi$. From Part 1, this is equivalent to minimizing $\\max_{k} \\mathrm{rank}(\\mathrm{unfold}_{\\{\\pi(1),\\ldots,\\pi(k)\\}|\\{\\pi(k+1),\\ldots,\\pi(d)\\}}(X))$.\n\nThe rank of any matrix is bounded by its dimensions. For the unfolding $\\mathrm{unfold}_{S|\\bar{S}}(X)$, the rank is bounded by:\n$$ \\mathrm{rank} \\le \\min\\left( \\prod_{i \\in S} n_i, \\prod_{j \\in \\bar{S}} n_j \\right). $$\nFor a generic tensor, this bound is often tight. A sound heuristic is therefore to find a permutation $\\pi$ that minimizes the maximum of these bounds over all cuts $k \\in \\{1, \\ldots, d-1\\}$:\n$$ \\min_{\\pi \\in S_d} \\max_{k \\in \\{1, \\ldots, d-1\\}} \\min\\left( \\prod_{m=1}^k n_{\\pi(m)}, \\prod_{m=k+1}^d n_{\\pi(m)} \\right). $$\nThis is a combinatorial optimization problem. A greedy strategy can provide a high-quality solution. The core idea is to balance the cumulative products of dimensions on either side of the TT chain. The TT-ranks are determined by a sequence of bisections of the modes. To keep the maximum rank low, we must avoid any bisection that results in two partitions with very large dimension products. This implies that the cumulative product of dimensions, $P_k(\\pi) = \\prod_{m=1}^k n_{\\pi(m)}$, should grow as moderately as possible.\nThis suggests a greedy algorithm that builds the permutation from the outside-in. At each step, we place one of the remaining modes at one of the available ends of the permutation chain. To keep the cumulative products balanced, we select from the available modes and place it on the \"lighter\" side of the chain (the side with the smaller current product of dimensions).\n\n**Heuristic Algorithm:**\n1.  Initialize an empty permutation list $\\pi$ of length $d$, with left and right pointers $l=0, r=d-1$.\n2.  Maintain a list of unassigned mode indices. Let their dimensions be $\\{n_i\\}$.\n3.  Initialize cumulative products for the left and right sides of the chain: $P_{left} = 1$, $P_{right} = 1$.\n4.  Sort the mode indices in ascending order of their dimensions.\n5.  Iterate through the sorted mode indices. For each mode index $m$:\n    a. If $P_{left} \\le P_{right}$, place mode $m$ at the current left position: $\\pi[l] = m$. Update $P_{left} \\leftarrow P_{left} \\cdot n_m$ and increment $l$.\n    b. Otherwise, place mode $m$ at the current right position: $\\pi[r] = m$. Update $P_{right} \\leftarrow P_{right} \\cdot n_m$ and decrement $r$.\n6.  The resulting list $\\pi$ is the desired permutation.\n\nThis heuristic is computable from the tensor dimensions alone and is justified by the principle of minimizing the rank bounds across all TT cuts by balancing the products of dimensions.\n\n### Part 3: Implementation\n\nThe implementation consists of creating the specified test tensors, a function to compute TT ranks for a given permutation, the heuristic function, and an exhaustive search to find the optimal solution for comparison. For each test case, we determine if the heuristic achieves the true minimum maximum TT rank.",
            "answer": "```python\nimport numpy as np\nimport itertools\nfrom functools import reduce\n\ndef compute_tt_ranks(X, pi):\n    \"\"\"\n    Computes the TT ranks for a tensor X with modes permuted by pi.\n    As derived, this is equivalent to computing the ranks of contiguous\n    unfoldings of the transposed tensor X_perm = transpose(X, pi).\n\n    Args:\n        X (np.ndarray): The input d-mode tensor.\n        pi (tuple): A tuple of 0-indexed integers representing the mode permutation.\n\n    Returns:\n        list: A list of the d-1 TT ranks.\n    \"\"\"\n    d = X.ndim\n    if d <= 1:\n        return []\n\n    # Permute the tensor's modes according to pi.\n    # This conceptually creates the tensor X^pi whose TT ranks are to be computed.\n    X_perm = np.transpose(X, pi)\n    \n    ranks = []\n    # Iterate through the d-1 contiguous cuts of the permuted tensor.\n    for k in range(1, d):\n        shape = X_perm.shape\n        # Unfold the permuted tensor across the k-th cut by reshaping into a matrix.\n        dim_rows = np.prod(shape[:k]).astype(int)\n        dim_cols = np.prod(shape[k:]).astype(int)\n        matrix = np.reshape(X_perm, (dim_rows, dim_cols))\n        \n        # Compute the algebraic rank of the resulting matrix.\n        # np.linalg.matrix_rank uses SVD and a tolerance, which is appropriate.\n        rank = np.linalg.matrix_rank(matrix)\n        ranks.append(rank)\n        \n    return ranks\n\ndef heuristic_permutation(dims):\n    \"\"\"\n    Computes a permutation pi that aims to minimize the maximum TT rank, based on\n    a greedy strategy of balancing the products of dimensions across the TT chain.\n\n    Args:\n        dims (tuple): The dimensions of the tensor.\n\n    Returns:\n        tuple: The heuristically chosen permutation.\n    \"\"\"\n    d = len(dims)\n    mode_indices = list(range(d))\n    \n    # Sort mode indices in ascending order of their corresponding dimension size.\n    # Python's `sorted` is stable, which provides deterministic tie-breaking.\n    sorted_modes = sorted(mode_indices, key=lambda i: dims[i])\n    \n    pi = [0] * d\n    left_ptr, right_ptr = 0, d - 1\n    prod_left, prod_right = 1, 1\n    \n    # Greedily build the permutation from the outside in.\n    # At each step, select the available mode with the smallest dimension.\n    for mode_idx in sorted_modes:\n        # Place this mode on the side of the chain with the smaller cumulative product.\n        if prod_left <= prod_right:\n            pi[left_ptr] = mode_idx\n            prod_left *= dims[mode_idx]\n            left_ptr += 1\n        else:\n            pi[right_ptr] = mode_idx\n            prod_right *= dims[mode_idx]\n            right_ptr -= 1\n            \n    return tuple(pi)\n\ndef evaluate_case(X):\n    \"\"\"\n    Finds the optimal maximum TT rank via exhaustive search over all permutations\n    and compares it with the result from the heuristic.\n\n    Args:\n        X (np.ndarray): The test tensor.\n\n    Returns:\n        bool: True if the heuristic finds an optimal permutation, False otherwise.\n    \"\"\"\n    dims = X.shape\n    d = X.ndim\n    \n    if d <= 1:\n        return True # Trivial case, no ranks to compute.\n\n    # --- Exhaustive search for ground truth ---\n    optimal_min_max_rank = float('inf')\n    all_permutations = itertools.permutations(range(d))\n\n    for pi in all_permutations:\n        ranks = compute_tt_ranks(X, pi)\n        current_max_rank = max(ranks) if ranks else 0\n        if current_max_rank < optimal_min_max_rank:\n            optimal_min_max_rank = current_max_rank\n\n    # --- Heuristic ---\n    pi_heuristic = heuristic_permutation(dims)\n    ranks_heuristic = compute_tt_ranks(X, pi_heuristic)\n    heuristic_max_rank = max(ranks_heuristic) if ranks_heuristic else 0\n\n    # --- Comparison ---\n    is_optimal = (heuristic_max_rank == optimal_min_max_rank)\n    return is_optimal\n\ndef define_test_cases():\n    \"\"\"Generates the four test tensors as specified in the problem.\"\"\"\n    test_suite = []\n\n    # Test Case 1: Generic dense tensor, d=5\n    rng = np.random.default_rng(seed=42)\n    X1 = rng.random(size=(2, 3, 2, 3, 2))\n    test_suite.append(X1)\n\n    # Test Case 2: Kronecker structure, d=4\n    u = np.array([1., 2., 3.])\n    v = np.array([4., 5., 6.])\n    M = np.outer(u, v)  # 3x3 rank-1 matrix\n    N = np.eye(2)       # 2x2 full-rank matrix\n    X2 = np.einsum('ik,jl->ijkl', M, N, dtype=np.float64)\n    test_suite.append(X2)\n\n    # Test Case 3: Rank-1 separable tensor, d=6\n    g_vectors = [\n        np.array([1.1, 2.2]), np.array([3.3, 1.1]), np.array([1.1, 4.4]),\n        np.array([2.2, 1.1]), np.array([1.1, 1.1]), np.array([2.2, 3.3])\n    ]\n    X3 = reduce(np.multiply.outer, g_vectors)\n    test_suite.append(X3)\n\n    # Test Case 4: Structured Canonical Polyadic sum, d=4\n    a = np.array([1., 2., 3.])\n    b = np.array([4., 5., 6.])\n    c = np.array([1., 0.])\n    d_vec = np.array([1., 1.])\n    c_p = np.array([0., 1.])\n    d_p = np.array([1., -1.])\n    T1 = reduce(np.multiply.outer, [a, b, c, d_vec])\n    T2 = reduce(np.multiply.outer, [a, b, c_p, d_p])\n    X4 = T1 + T2\n    test_suite.append(X4)\n    \n    return test_suite\n\ndef solve():\n    \"\"\"\n    Main function to run the evaluation on all test cases and print the results.\n    \"\"\"\n    test_cases = define_test_cases()\n    results = []\n\n    for X in test_cases:\n        is_optimal = evaluate_case(X)\n        results.append(is_optimal)\n\n    # The final print statement must follow the specified format exactly.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Implementing an algorithm successfully requires not only understanding its structure but also ensuring its numerical stability. This problem focuses on the Alternating Least Squares (ALS) algorithm for Tensor-Train (TT) decomposition, a workhorse method that can suffer from ill-conditioning in its subproblems. You will analyze how the condition number of the system matrix impacts the solution's sensitivity and explore Tikhonov regularization as a standard technique to improve the algorithm's robustness, a vital skill for any computational scientist. ",
            "id": "3583954",
            "problem": "Consider a $d$-way tensor $\\mathcal{A} \\in \\mathbb{R}^{n_1 \\times \\cdots \\times n_d}$ represented in the Tensor-Train (TT) format, with TT-cores $\\mathcal{G}_1,\\ldots,\\mathcal{G}_d$ and TT-ranks $r_0=1, r_1,\\ldots,r_d=1$. In an Alternating Least Squares (ALS) step for the TT core at position $k$, the update of the vectorized core $x_k = \\operatorname{vec}(\\mathcal{G}_k)$ solves a linear least-squares problem\n$$\n\\min_{x \\in \\mathbb{R}^{p_k}} \\| U_k x - b_k \\|_2,\n$$\nwhere $U_k \\in \\mathbb{R}^{m_k \\times p_k}$ is the $k$-th unfolding matrix formed by contracting the fixed left and right TT interfaces, and $b_k \\in \\mathbb{R}^{m_k}$ is the contracted right-hand side that includes the data tensor $\\mathcal{A}$. Assume $U_k$ has full column rank and let its singular values be $\\sigma_1(U_k) \\ge \\cdots \\ge \\sigma_{r_k}(U_k) > 0$ (with $r_k = p_k$). Define the unfolding condition number\n$$\n\\kappa_k \\;=\\; \\frac{\\sigma_1(U_k)}{\\sigma_{r_k}(U_k)}.\n$$\nStarting only from core definitions in numerical linear algebra (Singular Value Decomposition (SVD), Moore–Penrose pseudoinverse, operator norm), do the following:\n\n1. Derive a worst-case bound that relates the relative perturbation of the ALS core update solution to a perturbation in the right-hand side $b_k$, and express the bound purely in terms of $\\kappa_k$. Assume the exact data relation $b_k = U_k x_k^\\star$ for some $x_k^\\star$ and consider perturbations of the form $b_k \\mapsto b_k + \\delta b_k$.\n\n2. Consider Tikhonov regularization for the ALS update,\n$$\nx_{k,\\lambda} \\;=\\; \\arg\\min_{x \\in \\mathbb{R}^{p_k}} \\left( \\| U_k x - b_k \\|_2^2 + \\lambda^2 \\|x\\|_2^2 \\right),\n$$\nand derive the condition number of the regularized normal equations operator, stated as a function of $\\lambda$, $\\sigma_1(U_k)$, and $\\sigma_{r_k}(U_k)$.\n\n3. Suppose $\\sigma_1(U_k) = 1200$, $\\sigma_{r_k}(U_k) = 0.75$, and you are tasked with choosing a regularization parameter $\\lambda \\ge 0$ so that the regularized condition number is at most $\\alpha = 10^3$. Compute the smallest $\\lambda$ that guarantees this regularized condition number bound. Round your answer to four significant figures.\n\nYour final answer must be a single real number.",
            "solution": "The problem is divided into three parts. We will address each part sequentially. The problem is validated as self-contained, scientifically grounded, and well-posed.\n\n**Part 1: Worst-case bound for the unregularized least-squares problem**\n\nThe Alternating Least Squares (ALS) subproblem is to find the core update $x_k$ that solves the linear least-squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^{p_k}} \\| U_k x - b_k \\|_2\n$$\nGiven that the matrix $U_k \\in \\mathbb{R}^{m_k \\times p_k}$ has full column rank, the unique solution to this problem is given by the normal equations:\n$$\nU_k^T U_k x_k = U_k^T b_k\n$$\nThe solution can be expressed using the Moore-Penrose pseudoinverse, $U_k^\\dagger$:\n$$\nx_k = (U_k^T U_k)^{-1} U_k^T b_k = U_k^\\dagger b_k\n$$\nWe are given an exact underlying solution $x_k^\\star$ such that $b_k = U_k x_k^\\star$. A perturbation $\\delta b_k$ in the right-hand side leads to a new right-hand side $b_k' = b_k + \\delta b_k$. The corresponding new solution is:\n$$\nx_k' = U_k^\\dagger (b_k + \\delta b_k) = U_k^\\dagger b_k + U_k^\\dagger \\delta b_k = x_k^\\star + \\delta x_k\n$$\nThe perturbation in the solution is $\\delta x_k = x_k' - x_k^\\star = U_k^\\dagger \\delta b_k$.\nTo find the worst-case relative error, we analyze the norms. Taking the 2-norm, we get:\n$$\n\\|\\delta x_k\\|_2 = \\|U_k^\\dagger \\delta b_k\\|_2 \\le \\|U_k^\\dagger\\|_2 \\|\\delta b_k\\|_2\n$$\nThe operator norm $\\|U_k^\\dagger\\|_2$ is the largest singular value of $U_k^\\dagger$. Let the SVD of $U_k$ be $U_k = P \\Sigma Q^T$, where $\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_{p_k})$. Since $U_k$ has full column rank, $p_k=r_k$, the pseudoinverse is $U_k^\\dagger = Q \\Sigma^{-1} P^T$. The singular values of $U_k^\\dagger$ are the reciprocals of the singular values of $U_k$. The largest singular value of $U_k^\\dagger$ is the reciprocal of the smallest singular value of $U_k$.\n$$\n\\|U_k^\\dagger\\|_2 = \\frac{1}{\\sigma_{\\min}(U_k)} = \\frac{1}{\\sigma_{r_k}(U_k)}\n$$\nSubstituting this into the inequality for $\\|\\delta x_k\\|_2$ gives:\n$$\n\\|\\delta x_k\\|_2 \\le \\frac{1}{\\sigma_{r_k}(U_k)} \\|\\delta b_k\\|_2\n$$\nTo obtain the relative error in $x_k$, we divide by $\\|x_k^\\star\\|_2$. We need a bound for $1/\\|x_k^\\star\\|_2$. From the relation $b_k = U_k x_k^\\star$, we have:\n$$\n\\|b_k\\|_2 = \\|U_k x_k^\\star\\|_2 \\le \\|U_k\\|_2 \\|x_k^\\star\\|_2 = \\sigma_1(U_k) \\|x_k^\\star\\|_2\n$$\nThis gives us a lower bound on $\\|x_k^\\star\\|_2$:\n$$\n\\|x_k^\\star\\|_2 \\ge \\frac{\\|b_k\\|_2}{\\sigma_1(U_k)}\n$$\nTherefore, $1/\\|x_k^\\star\\|_2 \\le \\sigma_1(U_k)/\\|b_k\\|_2$.\n\nCombining these inequalities, we derive the bound on the relative perturbation:\n$$\n\\frac{\\|\\delta x_k\\|_2}{\\|x_k^\\star\\|_2} \\le \\frac{1}{\\|x_k^\\star\\|_2} \\left( \\frac{1}{\\sigma_{r_k}(U_k)} \\|\\delta b_k\\|_2 \\right) \\le \\left( \\frac{\\sigma_1(U_k)}{\\|b_k\\|_2} \\right) \\left( \\frac{1}{\\sigma_{r_k}(U_k)} \\|\\delta b_k\\|_2 \\right) = \\frac{\\sigma_1(U_k)}{\\sigma_{r_k}(U_k)} \\frac{\\|\\delta b_k\\|_2}{\\|b_k\\|_2}\n$$\nUsing the definition of the unfolding condition number $\\kappa_k = \\sigma_1(U_k)/\\sigma_{r_k}(U_k)$, we obtain the final worst-case bound:\n$$\n\\frac{\\|\\delta x_k\\|_2}{\\|x_k^\\star\\|_2} \\le \\kappa_k \\frac{\\|\\delta b_k\\|_2}{\\|b_k\\|_2}\n$$\n\n**Part 2: Condition number of the regularized normal equations**\n\nThe Tikhonov-regularized least-squares problem is defined as:\n$$\nx_{k,\\lambda} = \\arg\\min_{x \\in \\mathbb{R}^{p_k}} \\left( \\| U_k x - b_k \\|_2^2 + \\lambda^2 \\|x\\|_2^2 \\right)\n$$\nThe objective function is $J(x) = (U_k x - b_k)^T(U_k x - b_k) + \\lambda^2 x^T x$. To find the minimum, we compute the gradient with respect to $x$ and set it to zero:\n$$\n\\nabla_x J(x) = 2 U_k^T(U_k x - b_k) + 2 \\lambda^2 x = 0\n$$\n$$\n(U_k^T U_k) x - U_k^T b_k + \\lambda^2 x = 0\n$$\nThis yields the regularized normal equations:\n$$\n(U_k^T U_k + \\lambda^2 I) x = U_k^T b_k\n$$\nwhere $I$ is the identity matrix of size $p_k \\times p_k$. The operator for this linear system is the matrix $C = U_k^T U_k + \\lambda^2 I$.\nThe condition number of a symmetric positive-definite matrix $C$ in the $2$-norm is the ratio of its largest eigenvalue to its smallest eigenvalue:\n$$\n\\kappa_{2}(C) = \\frac{\\lambda_{\\max}(C)}{\\lambda_{\\min}(C)}\n$$\nThe eigenvalues of the matrix $U_k^T U_k$ are the squares of the singular values of $U_k$, i.e., $\\sigma_i(U_k)^2$. The matrix $C$ is a sum of $U_k^T U_k$ and a scaled identity matrix $\\lambda^2 I$. Thus, the eigenvalues of $C$ are $\\lambda_i(C) = \\sigma_i(U_k)^2 + \\lambda^2$.\nThe largest eigenvalue of $C$ corresponds to the largest singular value of $U_k$:\n$$\n\\lambda_{\\max}(C) = \\sigma_1(U_k)^2 + \\lambda^2\n$$\nThe smallest eigenvalue of $C$ corresponds to the smallest singular value of $U_k$:\n$$\n\\lambda_{\\min}(C) = \\sigma_{r_k}(U_k)^2 + \\lambda^2\n$$\nThe condition number of the regularized normal equations operator is therefore:\n$$\n\\kappa_{\\text{reg}} = \\kappa_{2}(C) = \\frac{\\sigma_1(U_k)^2 + \\lambda^2}{\\sigma_{r_k}(U_k)^2 + \\lambda^2}\n$$\n\n**Part 3: Calculation of the regularization parameter $\\lambda$**\n\nWe are given the values $\\sigma_1(U_k) = 1200$, $\\sigma_{r_k}(U_k) = 0.75$, and a target upper bound on the regularized condition number, $\\alpha = 10^3$. We need to find the smallest $\\lambda \\ge 0$ such that $\\kappa_{\\text{reg}} \\le \\alpha$.\nThe inequality is:\n$$\n\\frac{\\sigma_1(U_k)^2 + \\lambda^2}{\\sigma_{r_k}(U_k)^2 + \\lambda^2} \\le \\alpha\n$$\nLet's analyze the function $f(\\lambda) = \\frac{\\sigma_1(U_k)^2 + \\lambda^2}{\\sigma_{r_k}(U_k)^2 + \\lambda^2}$ for $\\lambda \\ge 0$. Its derivative is:\n$$\nf'(\\lambda) = \\frac{2\\lambda(\\sigma_{r_k}(U_k)^2 + \\lambda^2) - 2\\lambda(\\sigma_1(U_k)^2 + \\lambda^2)}{(\\sigma_{r_k}(U_k)^2 + \\lambda^2)^2} = \\frac{2\\lambda(\\sigma_{r_k}(U_k)^2 - \\sigma_1(U_k)^2)}{(\\sigma_{r_k}(U_k)^2 + \\lambda^2)^2}\n$$\nSince $\\sigma_1(U_k) > \\sigma_{r_k}(U_k)$, the term $(\\sigma_{r_k}(U_k)^2 - \\sigma_1(U_k)^2)$ is negative. Thus, for $\\lambda > 0$, $f'(\\lambda) < 0$, which means $f(\\lambda)$ is a monotonically decreasing function of $\\lambda$. The maximum value of the condition number occurs at $\\lambda=0$ and is $(\\sigma_1(U_k)/\\sigma_{r_k}(U_k))^2 = (1200/0.75)^2 = 1600^2 = 2.56 \\times 10^6$, which is much larger than $\\alpha=10^3$. As $\\lambda \\to \\infty$, $f(\\lambda) \\to 1$.\nBecause $f(\\lambda)$ is decreasing, the inequality $\\kappa_{\\text{reg}} \\le \\alpha$ will be satisfied for all $\\lambda$ greater than or equal to the value where equality holds. To find the smallest such $\\lambda$, we solve the equation:\n$$\n\\frac{\\sigma_1(U_k)^2 + \\lambda^2}{\\sigma_{r_k}(U_k)^2 + \\lambda^2} = \\alpha\n$$\nRearranging the terms to solve for $\\lambda^2$:\n$$\n\\sigma_1(U_k)^2 + \\lambda^2 = \\alpha (\\sigma_{r_k}(U_k)^2 + \\lambda^2)\n$$\n$$\n\\sigma_1(U_k)^2 - \\alpha \\sigma_{r_k}(U_k)^2 = \\alpha \\lambda^2 - \\lambda^2\n$$\n$$\n\\lambda^2 = \\frac{\\sigma_1(U_k)^2 - \\alpha \\sigma_{r_k}(U_k)^2}{\\alpha - 1}\n$$\nNow, we substitute the given numerical values: $\\sigma_1(U_k) = 1200$, $\\sigma_{r_k}(U_k) = 0.75$, and $\\alpha = 1000$.\n$$\n\\lambda^2 = \\frac{(1200)^2 - 1000 \\times (0.75)^2}{1000 - 1} = \\frac{1440000 - 1000 \\times 0.5625}{999}\n$$\n$$\n\\lambda^2 = \\frac{1440000 - 562.5}{999} = \\frac{1439437.5}{999} \\approx 1440.878378\n$$\nTaking the square root to find $\\lambda$:\n$$\n\\lambda = \\sqrt{\\frac{1439437.5}{999}} \\approx 37.9589035\n$$\nRounding the result to four significant figures, we get $37.96$.",
            "answer": "$$\\boxed{37.96}$$"
        }
    ]
}