{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental application of the Kronecker product and $\\operatorname{vec}$ operator is to transform linear matrix equations into standard vector linear systems of the form $M\\mathbf{x} = \\mathbf{b}$. This exercise provides direct practice with this core technique. You will solve a discrete-time Lyapunov equation, a type of equation that arises frequently in control theory and the analysis of dynamical systems, by converting it into a system that can be solved using standard linear algebra methods. ",
            "id": "1073003",
            "problem": "An important class of linear matrix equations is the Sylvester equation, which for given matrices $A, B, C$ seeks a matrix $X$ satisfying $AX + XB = C$. A related equation is the discrete-time Lyapunov equation $X - A^T X A = Q$. This problem concerns a variant of these equations.\n\nTo solve such equations, the Kronecker product and the $\\operatorname{vec}$ operator are powerful tools. For an $m \\times n$ matrix $M$ and a $p \\times q$ matrix $N$, the Kronecker product $M \\otimes N$ is the $mp \\times nq$ block matrix:\n$$\nM \\otimes N = \\begin{pmatrix}\nM_{11}N & \\cdots & M_{1n}N \\\\\n\\vdots & \\ddots & \\vdots \\\\\nM_{m1}N & \\cdots & M_{mn}N\n\\end{pmatrix}\n$$\nThe $\\operatorname{vec}$ operator, $\\operatorname{vec}(A)$, transforms an $m \\times n$ matrix $A$ into an $mn \\times 1$ column vector by stacking its columns:\n$$\n\\operatorname{vec}(A) = \\begin{pmatrix} A_{\\cdot,1} \\\\ \\vdots \\\\ A_{\\cdot,n} \\end{pmatrix}\n$$\nA fundamental identity connecting these concepts is $\\operatorname{vec}(AXB) = (B^T \\otimes A) \\operatorname{vec}(X)$.\n\nConsider the linear matrix equation for an unknown $2 \\times 2$ real matrix $X$:\n$$\nX - A^T X B = I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix, and the matrices $A$ and $B$ are defined in terms of a real parameter $\\alpha$ as:\n$$\nA = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix}, \\quad B = \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix}\n$$\nAssume that the parameter $\\alpha$ is such that $|\\alpha| \\neq 1$, which guarantees the existence of a unique solution for $X$.\n\nUsing the Kronecker product formalism, find a closed-form expression for the trace of the solution matrix, $\\operatorname{tr}(X)$, as a function of $\\alpha$.",
            "solution": "The given matrix equation is $X - A^T X B = I_2$. We are asked to solve for the trace of $X$ using the Kronecker product formalism.\n\nFirst, we apply the $\\operatorname{vec}$ operator to the entire equation. Using the linearity of the $\\operatorname{vec}$ operator, we get:\n$$\n\\operatorname{vec}(X - A^T X B) = \\operatorname{vec}(I_2)\n$$\n$$\n\\operatorname{vec}(X) - \\operatorname{vec}(A^T X B) = \\operatorname{vec}(I_2)\n$$\nNow, we use the identity $\\operatorname{vec}(AXB) = (B^T \\otimes A) \\operatorname{vec}(X)$. In our case, the matrices in the triple product are $A^T$, $X$, and $B$. Thus we have $A \\to A^T$ and $B \\to B$. Applying the identity:\n$$\n\\operatorname{vec}(A^T X B) = (B^T \\otimes A^T) \\operatorname{vec}(X)\n$$\nSubstituting this back into the vectorized equation gives:\n$$\n\\operatorname{vec}(X) - (B^T \\otimes A^T) \\operatorname{vec}(X) = \\operatorname{vec}(I_2)\n$$\nFactoring out $\\operatorname{vec}(X)$, which is identified as $I \\operatorname{vec}(X)$, where $I$ is the identity matrix of appropriate size ($4 \\times 4$ in this case), we obtain a standard linear system:\n$$\n(I_4 - B^T \\otimes A^T) \\operatorname{vec}(X) = \\operatorname{vec}(I_2)\n$$\nLet's compute the matrices involved. The matrices $A$ and $B$ are given by:\n$$\nA = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix}, \\quad B = \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix}\n$$\nTheir transposes are:\n$$\nA^T = \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix}, \\quad B^T = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix}\n$$\nNow, we compute the Kronecker product $B^T \\otimes A^T$:\n$$\nB^T \\otimes A^T = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix} \\otimes \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} = \\begin{pmatrix} \\alpha A^T & 1 A^T \\\\ 0 \\cdot A^T & \\alpha A^T \\end{pmatrix}\n$$\n$$\nB^T \\otimes A^T = \\begin{pmatrix} \\alpha \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} & \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} \\\\ \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} & \\alpha \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 & 0 & \\alpha & 0 \\\\ \\alpha & \\alpha^2 & 1 & \\alpha \\\\ 0 & 0 & \\alpha^2 & 0 \\\\ 0 & 0 & \\alpha & \\alpha^2 \\end{pmatrix}\n$$\nThe coefficient matrix of our linear system is $M = I_4 - B^T \\otimes A^T$:\n$$\nM = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\alpha^2 & 0 & \\alpha & 0 \\\\ \\alpha & \\alpha^2 & 1 & \\alpha \\\\ 0 & 0 & \\alpha^2 & 0 \\\\ 0 & 0 & \\alpha & \\alpha^2 \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha^2 & 0 & -\\alpha & 0 \\\\ -\\alpha & 1-\\alpha^2 & -1 & -\\alpha \\\\ 0 & 0 & 1-\\alpha^2 & 0 \\\\ 0 & 0 & -\\alpha & 1-\\alpha^2 \\end{pmatrix}\n$$\nLet the unknown matrix $X$ be $X = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\end{pmatrix}$. Its vectorization is $\\operatorname{vec}(X) = (x_{11}, x_{21}, x_{12}, x_{22})^T$. The vectorization of the identity matrix $I_2$ is $\\operatorname{vec}(I_2) = (1, 0, 0, 1)^T$.\n\nThe linear system $M \\operatorname{vec}(X) = \\operatorname{vec}(I_2)$ is:\n$$\n\\begin{pmatrix} 1-\\alpha^2 & 0 & -\\alpha & 0 \\\\ -\\alpha & 1-\\alpha^2 & -1 & -\\alpha \\\\ 0 & 0 & 1-\\alpha^2 & 0 \\\\ 0 & 0 & -\\alpha & 1-\\alpha^2 \\end{pmatrix} \\begin{pmatrix} x_{11} \\\\ x_{21} \\\\ x_{12} \\\\ x_{22} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThis represents a system of four linear equations:\n1. $(1-\\alpha^2)x_{11} - \\alpha x_{12} = 1$\n2. $-\\alpha x_{11} + (1-\\alpha^2)x_{21} - x_{12} - \\alpha x_{22} = 0$\n3. $(1-\\alpha^2)x_{12} = 0$\n4. $-\\alpha x_{12} + (1-\\alpha^2)x_{22} = 1$\n\nWe solve this system for the elements of $X$. From equation (3), since $|\\alpha| \\neq 1$, we have $1-\\alpha^2 \\neq 0$, which implies:\n$$\nx_{12} = 0\n$$\nSubstitute $x_{12}=0$ into equation (4):\n$$\n(1-\\alpha^2)x_{22} = 1 \\implies x_{22} = \\frac{1}{1-\\alpha^2}\n$$\nSubstitute $x_{12}=0$ into equation (1):\n$$\n(1-\\alpha^2)x_{11} = 1 \\implies x_{11} = \\frac{1}{1-\\alpha^2}\n$$\nWe do not need to solve for $x_{21}$ to find the trace. The trace of $X$ is $\\operatorname{tr}(X) = x_{11} + x_{22}$.\n$$\n\\operatorname{tr}(X) = \\frac{1}{1-\\alpha^2} + \\frac{1}{1-\\alpha^2} = \\frac{2}{1-\\alpha^2}\n$$\nThis gives the trace of the solution matrix $X$ as a function of $\\alpha$.",
            "answer": "$$ \\boxed{\\frac{2}{1-\\alpha^2}} $$"
        },
        {
            "introduction": "This practice problem encourages a more flexible application of the $\\operatorname{vec}$-Kronecker identity. Instead of starting with a matrix equation and vectorizing it, we begin with a linear system involving a large Kronecker product and use the identity in reverse to simplify it into a more manageable matrix equation. This exercise  not only reinforces the primary identity but also involves the practical skill of inverting structured matrices, demonstrating how these concepts connect to broader topics in numerical linear algebra.",
            "id": "1092486",
            "problem": "Consider the equation $(F \\otimes G) \\mathbf{x} = \\mathbf{b}$, where $F$ and $G$ are $3 \\times 3$ matrices defined by  \n\n$$\nF = \\begin{bmatrix} a & 1 & 0 \\\\ 0 & a & 1 \\\\ 0 & 0 & a \\end{bmatrix}, \\quad G = \\begin{bmatrix} b & 0 & 0 \\\\ 1 & b & 0 \\\\ 0 & 1 & b \\end{bmatrix},\n$$\n  \nand $a$ and $b$ are non-zero real numbers. The vector $\\mathbf{b}$ is given by $\\mathbf{b} = \\operatorname{vec}(I)$, where $I$ is the $3 \\times 3$ identity matrix. Here, $\\operatorname{vec}(X)$ denotes the vectorization of a matrix $X$, formed by stacking its columns into a single column vector. Solve for $\\mathbf{x}$, and then determine the entry in the second row and second column of the matrix $X$ such that $\\mathbf{x} = \\operatorname{vec}(X)$. Express your answer in terms of $a$ and $b$.",
            "solution": "We wish to solve \n$$\n(F\\otimes G)\\,\\operatorname{vec}(X)=\\operatorname{vec}(I)\\,.\n$$\nUsing the identity \n$$(A\\otimes B)\\,\\operatorname{vec}(X)=\\operatorname{vec}\\bigl(B\\,X\\,A^T\\bigr)$$\nwe obtain\n$$\n\\operatorname{vec}\\bigl(G\\,X\\,F^T\\bigr)=\\operatorname{vec}(I)\n\\quad\\Longrightarrow\\quad \nG\\,X\\,F^T=I.\n$$\nHence\n$$\nX=G^{-1}\\,(F^T)^{-1}=G^{-1}F^{-T}.\n$$\nNext we compute $F^{-1}$ and $G^{-1}$.  Write\n$$\nF=aI+R,\\quad R^3=0,\\quad \nR=\\begin{bmatrix}0&1&0\\\\0&0&1\\\\0&0&0\\end{bmatrix},\n$$\nso\n$$\nF^{-1}=(aI+R)^{-1}\n=a^{-1}\\sum_{k=0}^2(-R/a)^k\n=a^{-1}I-a^{-2}R+a^{-3}R^2.\n$$\nThus\n$$\nF^{-1}=\\begin{bmatrix}\na^{-1}&-a^{-2}&a^{-3}\\\\\n0&a^{-1}&-a^{-2}\\\\\n0&0&a^{-1}\n\\end{bmatrix},\n\\quad\nF^{-T}=(F^{-1})^T\n=\\begin{bmatrix}\na^{-1}&0&0\\\\\n-\\,a^{-2}&a^{-1}&0\\\\\na^{-3}&-\\,a^{-2}&a^{-1}\n\\end{bmatrix}.\n$$\nSimilarly write \n$$\nG=bI+S,\\quad S^3=0,\\quad \nS=\\begin{bmatrix}0&0&0\\\\1&0&0\\\\0&1&0\\end{bmatrix},\n$$\nso\n$$\nG^{-1}=b^{-1}I-b^{-2}S+b^{-3}S^2\n=\\begin{bmatrix}\nb^{-1}&0&0\\\\\n-\\,b^{-2}&b^{-1}&0\\\\\nb^{-3}&-\\,b^{-2}&b^{-1}\n\\end{bmatrix}.\n$$\nFinally\n$$\nX=G^{-1}F^{-T},\n$$\nand the $(2,2)$â€“entry is\n$$\nX_{22}\n=\\bigl[\\,-b^{-2},\\,b^{-1},\\,0\\bigr]\n\\begin{bmatrix}0\\\\a^{-1}\\\\-a^{-2}\\end{bmatrix}\n=b^{-1}a^{-1}\n=\\frac1{ab}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{ab}}$$"
        },
        {
            "introduction": "Real-world problems often lead to linear systems that are consistent but singular, meaning they have infinite solutions. In such cases, we often seek the unique solution with the minimum norm. This advanced exercise  tackles a generalized Sylvester equation where the vectorized form results in a singular system matrix. Solving it requires moving beyond simple matrix inversion and employing the Moore-Penrose pseudoinverse to find the physically or mathematically meaningful minimum-norm solution.",
            "id": "1092345",
            "problem": "Let $\\mathbb{R}^{m \\times n}$ be the space of $m \\times n$ real matrices. We introduce the following definitions and properties:\n\n1.  **Kronecker Product:** For $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{p \\times q}$, the Kronecker product $A \\otimes B$ is the $mp \\times nq$ block matrix:\n    $$\n    A \\otimes B = \\begin{pmatrix} a_{11}B & \\cdots & a_{1n}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1}B & \\cdots & a_{mn}B \\end{pmatrix}\n    $$\n2.  **Vectorization Operator:** For a matrix $X = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix} \\in \\mathbb{R}^{m \\times n}$, where $x_i$ are the columns of $X$, the $\\operatorname{vec}$ operator stacks the columns into a single $mn \\times 1$ column vector:\n    $$\n    \\operatorname{vec}(X) = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n    $$\n3.  **Key Property:** A crucial link between the Kronecker product and the vec operator is the identity $\\operatorname{vec}(AXB) = (B^T \\otimes A)\\operatorname{vec}(X)$ for matrices $A, X, B$ of compatible dimensions.\n4.  **Frobenius Norm:** The Frobenius norm of a matrix $X \\in \\mathbb{R}^{m \\times n}$ is defined as $\\|X\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |x_{ij}|^2} = \\sqrt{\\operatorname{tr}(X^T X)}$. This is equivalent to the standard Euclidean norm of its vectorized form, i.e., $\\|X\\|_F = \\|\\operatorname{vec}(X)\\|_2$.\n\nConsider the generalized Sylvester equation $AXB + CXD = E$. It is known that if this system of linear equations for the entries of $X$ is consistent (i.e., has at least one solution), there exists a unique solution $X_0$ that has the minimum Frobenius norm.\n\n**Problem:**\n\nLet $e_1$ and $e_2$ be the standard basis vectors in $\\mathbb{R}^2$. Let two other vectors be defined as $u = e_1 + e_2$ and $v = e_1 - e_2$. The matrices in the Sylvester equation are defined as follows:\n- $A = uu^T$\n- $B = e_1e_1^T$\n- $C = e_2e_2^T$\n- $D = vv^T$\n- $E$ is the $2 \\times 2$ identity matrix $I_2$.\n\nGiven that the equation $AXB + CXD = E$ is consistent for an unknown matrix $X \\in \\mathbb{R}^{2 \\times 2}$, determine the squared Frobenius norm, $\\|X_0\\|_F^2$, of the unique minimum-norm solution $X_0$.\n\n**Find:** $\\|X_0\\|_F^2$.",
            "solution": "**1. Formulating the Linear System**\n\nThe given generalized Sylvester equation is $AXB + CXD = E$. We can transform this matrix equation into a standard vector-form linear system using the $\\operatorname{vec}$ operator. Applying the $\\operatorname{vec}$ operator to both sides, we get:\n$$\n\\operatorname{vec}(AXB + CXD) = \\operatorname{vec}(E)\n$$\nBy linearity of the $\\operatorname{vec}$ operator, this becomes:\n$$\n\\operatorname{vec}(AXB) + \\operatorname{vec}(CXD) = \\operatorname{vec}(E)\n$$\nUsing the identity $\\operatorname{vec}(PQR) = (R^T \\otimes P)\\operatorname{vec}(Q)$, we can rewrite the equation as:\n$$\n(B^T \\otimes A)\\operatorname{vec}(X) + (D^T \\otimes C)\\operatorname{vec}(X) = \\operatorname{vec}(E)\n$$\nThis can be expressed as a single linear system $M\\mathbf{x} = \\mathbf{e}$, where:\n- $\\mathbf{x} = \\operatorname{vec}(X)$\n- $\\mathbf{e} = \\operatorname{vec}(E)$\n- $M = (B^T \\otimes A) + (D^T \\otimes C)$\n\n**2. Constructing the Matrices**\n\nFirst, let's write out the vectors and matrices explicitly. The standard basis vectors in $\\mathbb{R}^2$ are $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe other vectors are $u = e_1 + e_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v = e_1 - e_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe matrices are constructed as outer products:\n$A = uu^T = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$\n$B = e_1e_1^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$\n$C = e_2e_2^T = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$D = vv^T = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$\n$E = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n\nAll matrices $A, B, C, D$ are symmetric, so $B^T = B$ and $D^T = D$.\n\nThe vectorized form of $E$ is $\\mathbf{e} = \\operatorname{vec}(I_2) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n**3. Constructing the System Matrix M**\n\nThe system matrix is $M = (B \\otimes A) + (D \\otimes C)$.\nFirst term:\n$B \\otimes A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot A & 0 \\cdot A \\\\ 0 \\cdot A & 0 \\cdot A \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$\nSecond term:\n$D \\otimes C = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} \\otimes \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot C & -1 \\cdot C \\\\ -1 \\cdot C & 1 \\cdot C \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix}$\nAdding them together:\n$M = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix}$\n\nThe third row of $M$ is zero, so $\\det(M)=0$. The matrix is singular, as expected from the problem statement. The problem states the system is consistent, meaning $\\mathbf{e}$ is in the column space of $M$, a fact we don't need to re-verify but could.\n\n**4. Finding the Minimum Norm Solution**\n\nThe minimum norm solution to a consistent linear system $M\\mathbf{x} = \\mathbf{e}$ is given by $\\mathbf{x}_0 = M^\\dagger \\mathbf{e}$, where $M^\\dagger$ is the Moore-Penrose pseudoinverse of $M$. We can compute $M^\\dagger$ using its singular value decomposition or a full-rank decomposition. Let's use the latter.\n\nLet $M=CR$ be a full-rank decomposition, where the columns of $C$ form a basis for the column space of $M$, and $R$ consists of the coefficients to express the columns of $M$ in that basis.\nThe columns of $M$ are $m_1=(1,1,0,0)^T$, $m_2=(1,2,0,-1)^T$, $m_3=(0,0,0,0)^T$, $m_4=(0,-1,0,1)^T$.\nThe column space $C(M)$ is spanned by $m_1$ and $m_4$. We note that $m_2 = m_1 - m_4$ and $m_3 = 0$.\nSo we can choose the basis for $C(M)$ to be $\\{m_1, m_4\\}$. Let $C = \\begin{pmatrix} m_1 & m_4 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe coefficient matrix $R$ is $R=\\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix}$.\nThe pseudoinverse is then $M^\\dagger = R^\\dagger C^\\dagger = R^T(RR^T)^{-1}(C^TC)^{-1}C^T$.\n\nCalculate the components:\n$C^TC = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$\n$(C^TC)^{-1} = \\frac{1}{2(2)-(-1)^2}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n\n$RR^T = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$\n$(RR^T)^{-1} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n\nNow we assemble $M^\\dagger$:\n$C^\\dagger = (C^TC)^{-1}C^T = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 & 0 & 1 \\\\ 1 & -1 & 0 & 2 \\end{pmatrix}$\n$R^\\dagger = R^T(RR^T)^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 1 & 2 \\end{pmatrix}$\n\n$M^\\dagger = R^\\dagger C^\\dagger = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 1 & 2 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2 & 1 & 0 & 1 \\\\ 1 & -1 & 0 & 2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 5 & 1 & 0 & 4 \\\\ 1 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 4 & -1 & 0 & 5 \\end{pmatrix}$\n\nNow we compute the minimum norm solution vector $\\mathbf{x}_0 = \\operatorname{vec}(X_0)$:\n$\\mathbf{x}_0 = M^\\dagger \\mathbf{e} = \\frac{1}{9}\\begin{pmatrix} 5 & 1 & 0 & 4 \\\\ 1 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 4 & -1 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 5 \\cdot 1 + 4 \\cdot 1 \\\\ 1 \\cdot 1 - 1 \\cdot 1 \\\\ 0 \\\\ 4 \\cdot 1 + 5 \\cdot 1 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 9 \\\\ 0 \\\\ 0 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n\n**5. Reconstructing X_0 and Calculating its Norm**\n\nThe vector $\\mathbf{x}_0 = \\operatorname{vec}(X_0)$ is $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We un-vectorize it to find the matrix $X_0$:\n$\\operatorname{vec}(X_0) = \\begin{pmatrix} x_{11} \\\\ x_{21} \\\\ x_{12} \\\\ x_{22} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\implies X_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2$\n\nThe problem asks for the squared Frobenius norm of $X_0$.\n$\\|X_0\\|_F^2 = \\operatorname{tr}(X_0^T X_0) = \\operatorname{tr}(I_2^T I_2) = \\operatorname{tr}(I_2) = 1+1=2$.\nAlternatively, using the vectorized form:\n$\\|X_0\\|_F^2 = \\|\\mathbf{x}_0\\|_2^2 = \\left\\|\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\|_2^2 = 1^2 + 0^2 + 0^2 + 1^2 = 2$.",
            "answer": "$$ \\boxed{2} $$"
        }
    ]
}