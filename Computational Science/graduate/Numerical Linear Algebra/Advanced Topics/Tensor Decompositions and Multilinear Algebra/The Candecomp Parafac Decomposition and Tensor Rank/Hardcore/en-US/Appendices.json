{
    "hands_on_practices": [
        {
            "introduction": "The concept of tensor rank is central to multilinear algebra, yet its calculation is notoriously difficult. This first exercise provides foundational practice by asking you to determine the rank of a small tensor directly from first principles . The key challenge lies not just in constructing a CANDECOMP/PARAFAC (CP) decomposition, but in rigorously proving that the number of components is the minimum possible, a task that solidifies the definition of rank.",
            "id": "3586498",
            "problem": "Consider a third-order tensor of size $2 \\times 2 \\times 2$ over the real field, denoted by $\\mathcal{X} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$, and let $e_1, e_2 \\in \\mathbb{R}^2$ denote the standard basis vectors. Suppose $\\mathcal{X}$ is defined as\n$$\n\\mathcal{X} \\;=\\; e_1 \\otimes e_1 \\otimes e_1 \\;+\\; e_2 \\otimes e_2 \\otimes e_2,\n$$\nwhere $\\otimes$ denotes the outer product. Starting from the fundamental definitions of a rank-$1$ tensor and the CANonical DECOMPosition/PARAFAC (CP) decomposition, construct an explicit CP representation of $\\mathcal{X}$ by specifying a number $R \\in \\mathbb{N}$, scalars $\\lambda_1,\\dots,\\lambda_R \\in \\mathbb{R}$, and factor vectors $a_r, b_r, c_r \\in \\mathbb{R}^2$ for $r = 1,\\dots,R$ such that\n$$\n\\mathcal{X} \\;=\\; \\sum_{r=1}^{R} \\lambda_r \\, a_r \\otimes b_r \\otimes c_r.\n$$\nUsing only the core definitions of rank-$1$ tensors and tensor rank, determine the CP rank (also called tensor rank) of $\\mathcal{X}$ by proving minimality of $R$ in your representation. Provide the CP rank as the final answer. No rounding is required, and no units are involved.",
            "solution": "The problem asks for an explicit CANonical DECOMPosition/PARAFAC (CP) representation of a given third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ and the determination of its CP rank by proving the minimality of the representation.\n\nFirst, let us state the fundamental definitions.\nA third-order tensor $\\mathcal{T} \\in \\mathbb{R}^{I \\times J \\times K}$ is called a rank-$1$ tensor if it can be written as the outer product of three vectors, $a \\in \\mathbb{R}^I$, $b \\in \\mathbb{R}^J$, and $c \\in \\mathbb{R}^K$. That is, $\\mathcal{T} = a \\otimes b \\otimes c$, which has elements $\\mathcal{T}_{ijk} = a_i b_j c_k$.\n\nThe CP decomposition of a tensor $\\mathcal{X}$ expresses it as a sum of rank-$1$ tensors. The problem statement gives the form as:\n$$\n\\mathcal{X} = \\sum_{r=1}^{R} \\lambda_r \\, a_r \\otimes b_r \\otimes c_r\n$$\nwhere $R \\in \\mathbb{N}$ is the number of rank-$1$ components, $\\lambda_r \\in \\mathbb{R}$ are scalars, and $a_r, b_r, c_r$ are vectors of appropriate dimensions.\n\nThe CP rank, or simply rank, of a tensor $\\mathcal{X}$, denoted $\\text{rank}(\\mathcal{X})$, is the minimum integer $R$ for which such a decomposition exists.\n\nThe tensor in question is $\\mathcal{X} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$, defined as:\n$$\n\\mathcal{X} = e_1 \\otimes e_1 \\otimes e_1 + e_2 \\otimes e_2 \\otimes e_2\n$$\nwhere $e_1, e_2 \\in \\mathbb{R}^2$ are the standard basis vectors, $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nWe begin by constructing a CP representation for $\\mathcal{X}$. The given definition of $\\mathcal{X}$ is already a sum of two tensors. Let us examine each term:\nThe first term is $\\mathcal{T}_1 = e_1 \\otimes e_1 \\otimes e_1$. By definition, this is a rank-$1$ tensor.\nThe second term is $\\mathcal{T}_2 = e_2 \\otimes e_2 \\otimes e_2$. This is also a rank-$1$ tensor by definition.\n\nThus, the expression for $\\mathcal{X}$ is a sum of two rank-$1$ tensors. This naturally provides a CP representation. We can match this to the general form $\\sum_{r=1}^{R} \\lambda_r \\, a_r \\otimes b_r \\otimes c_r$ by setting:\n- $R = 2$\n- For $r=1$: $\\lambda_1 = 1$, $a_1 = e_1$, $b_1 = e_1$, $c_1 = e_1$\n- For $r=2$: $\\lambda_2 = 1$, $a_2 = e_2$, $b_2 = e_2$, $c_2 = e_2$\n\nThis gives the decomposition $\\mathcal{X} = 1 \\cdot (e_1 \\otimes e_1 \\otimes e_1) + 1 \\cdot (e_2 \\otimes e_2 \\otimes e_2)$, which is precisely the definition of $\\mathcal{X}$. Since we have found a representation with $R=2$ components, we have established an upper bound on the rank: $\\text{rank}(\\mathcal{X}) \\leq 2$.\n\nNext, to determine the exact CP rank, we must prove that this representation is minimal. That is, we must show that $\\mathcal{X}$ cannot be represented by a smaller number of rank-$1$ components. The only smaller non-trivial number of components is $1$. We will show that $\\text{rank}(\\mathcal{X}) \\neq 1$ by contradiction.\n\nAssume, for the sake of contradiction, that $\\text{rank}(\\mathcal{X}) = 1$. If this were true, $\\mathcal{X}$ could be written as a single rank-$1$ tensor. Using the requested formulation, this means there exist a scalar $\\lambda \\in \\mathbb{R}$ and vectors $a, b, c \\in \\mathbb{R}^2$ such that $\\mathcal{X} = \\lambda \\, a \\otimes b \\otimes c$. We can absorb the scalar $\\lambda$ into one of the vectors (e.g., let $a' = \\lambda a$) without loss of generality. So, the assumption is equivalent to $\\mathcal{X} = a \\otimes b \\otimes c$ for some $a, b, c \\in \\mathbb{R}^2$.\n\nLet's write out the components of $\\mathcal{X}$. Let the elements of $\\mathcal{X}$ be denoted by $\\mathcal{X}_{ijk}$ for $i,j,k \\in \\{1, 2\\}$.\nThe components of the first term $e_1 \\otimes e_1 \\otimes e_1$ are non-zero only for $(e_1 \\otimes e_1 \\otimes e_1)_{111} = (e_1)_1 (e_1)_1 (e_1)_1 = 1 \\cdot 1 \\cdot 1 = 1$.\nThe components of the second term $e_2 \\otimes e_2 \\otimes e_2$ are non-zero only for $(e_2 \\otimes e_2 \\otimes e_2)_{222} = (e_2)_2 (e_2)_2 (e_2)_2 = 1 \\cdot 1 \\cdot 1 = 1$.\nTherefore, the tensor $\\mathcal{X}$ has exactly two non-zero elements:\n$$\n\\mathcal{X}_{111} = 1 \\quad \\text{and} \\quad \\mathcal{X}_{222} = 1\n$$\nAll other elements $\\mathcal{X}_{ijk}$ are $0$.\n\nNow, let us equate these components with our rank-$1$ assumption. Let $a = \\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix}$, $b = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix}$, and $c = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$. The elements of the assumed rank-$1$ tensor are $\\mathcal{X}_{ijk} = a_i b_j c_k$.\nFrom the non-zero elements, we have:\n1. $\\mathcal{X}_{111} = a_1 b_1 c_1 = 1$. This implies that $a_1 \\neq 0$, $b_1 \\neq 0$, and $c_1 \\neq 0$.\n2. $\\mathcal{X}_{222} = a_2 b_2 c_2 = 1$. This implies that $a_2 \\neq 0$, $b_2 \\neq 0$, and $c_2 \\neq 0$.\n\nNow, let us consider one of the zero elements, for example, $\\mathcal{X}_{112} = 0$.\nAccording to our rank-$1$ assumption, $\\mathcal{X}_{112} = a_1 b_1 c_2$.\nSo, we must have $a_1 b_1 c_2 = 0$.\nFrom implication (1) above, we know that $a_1 \\neq 0$ and $b_1 \\neq 0$. For the product $a_1 b_1 c_2$ to be zero, it must be the case that $c_2 = 0$.\n\nHowever, implication (2) above states that $c_2 \\neq 0$. This is a direct contradiction. The statement $c_2 = 0$ and $c_2 \\neq 0$ cannot both be true.\nTherefore, the initial assumption that $\\text{rank}(\\mathcal{X}) = 1$ must be false.\n\nWe have established two facts:\n- $\\text{rank}(\\mathcal{X}) \\leq 2$ (from the explicit construction)\n- $\\text{rank}(\\mathcal{X})  1$ (from the proof by contradiction)\n\nSince the rank must be an integer, the only possible value for the rank of $\\mathcal{X}$ is $2$.\n\nThe CP rank of $\\mathcal{X}$ is the minimum $R$ such that $\\mathcal{X} = \\sum_{r=1}^{R} \\lambda_r \\, a_r \\otimes b_r \\otimes c_r$. We have demonstrated that $R=2$ is a possible value and $R=1$ is not. Thus, the minimum possible value for $R$ is $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "After finding a CP decomposition, a crucial question arises: is this representation unique? This practice delves into the conditions for uniqueness by examining a case where Kruskal's celebrated sufficient condition fails to hold . By explicitly constructing a family of alternative factorizations for the same tensor, you will gain a concrete understanding of the algebraic indeterminacies that can arise and appreciate why uniqueness is a prized, but not guaranteed, property of tensor decompositions.",
            "id": "3586512",
            "problem": "Let $\\mathcal{T} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ be a third-order tensor with Canonical Decomposition/Parallel Factors (CANDECOMP/PARAFAC (CP)) rank $R = 2$ specified by the following factor matrices:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\nThe CP model is $\\mathcal{T} = \\sum_{r=1}^{2} a_r \\otimes b_r \\otimes c_r$ where $a_r$, $b_r$, and $c_r$ denote the $r$-th columns of $A$, $B$, and $C$, respectively, and $\\otimes$ denotes the outer product. The $k$-rank of a matrix $X$ (also called Kruskal rank) is the largest integer $k_X$ such that every subset of $k_X$ columns of $X$ is linearly independent.\n\nTasks:\n- Using only the definitions of CP decomposition, $k$-rank, and the fact that for matrices $U$ and $V$ with matching column dimension one has $U V^{\\top} = \\sum_{r}$ column$_r(U)$ column$_r(V)^{\\top}$, proceed from first principles to argue whether Kruskalâ€™s sufficient condition for uniqueness can hold for this specific $\\mathcal{T}$.\n- Then, derive a one-parameter family of alternative CP factorizations of $\\mathcal{T}$ of the form $(A',B',C')$ defined by\n$$\nA' = A R(t), \\quad B' = B \\big(R(t)^{-T}\\big), \\quad C' = C,\n$$\nwhere $R(t) \\in \\mathbb{R}^{2 \\times 2}$ is the upper-triangular matrix\n$$\nR(t) = \\begin{pmatrix} 1  t \\\\ 0  1 \\end{pmatrix},\n$$\nand $t \\in \\mathbb{R}$ is a scalar parameter. Justify from the CP definition why $\\mathcal{T} = \\sum_{r=1}^{2} a'_r \\otimes b'_r \\otimes c'_r$ still holds for all $t$ with $R(t)$ invertible.\n- Impose the normalization constraint that the first entry of the first column of $B' = B \\big(R(t)^{-T}\\big)$ equals $0$, and determine the exact value of $t$ that satisfies this constraint.\n\nYour final answer should be the exact value of $t$ (no units). Do not provide intermediate steps in the final answer.",
            "solution": "The problem is divided into three parts. First, we assess the uniqueness of the given CP decomposition using Kruskal's sufficient condition. Second, we demonstrate the existence of a family of alternative factorizations. Third, we find a specific parameter value that satisfies a given normalization constraint.\n\nPart 1: Kruskal's Sufficient Condition for Uniqueness\n\nKruskal's theorem provides a sufficient condition for the essential uniqueness of a rank-$R$ CP decomposition defined by factor matrices $A$, $B$, and $C$. The condition is given by the inequality:\n$$k_A + k_B + k_C \\ge 2R + 2$$\nwhere $k_X$ is the $k$-rank (or Kruskal rank) of a matrix $X$. The $k$-rank is the largest integer $k$ such that any set of $k$ columns of $X$ is linearly independent. The rank of the tensor is given as $R=2$. We must compute the $k$-ranks for the given factor matrices $A, B, C$.\n\nThe factor matrices are:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nB = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\n\n-   For matrix $A \\in \\mathbb{R}^{2 \\times 2}$: The columns are $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Any single column is non-zero, so it forms a linearly independent set. The set of both columns is linearly independent because $\\det(A) = 1(1) - 0(0) = 1 \\ne 0$. Therefore, the largest number of columns that can be selected to form a linearly independent set is $2$. The $k$-rank is $k_A = 2$.\n\n-   For matrix $B \\in \\mathbb{R}^{2 \\times 2}$: The columns are $b_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $b_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. Any single column is non-zero. The set of both columns is linearly independent because $\\det(B) = 1(-1) - 1(1) = -2 \\ne 0$. Therefore, the $k$-rank is $k_B = 2$.\n\n-   For matrix $C \\in \\mathbb{R}^{2 \\times 2}$: The columns are $c_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $c_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Any single column is non-zero, so any subset of size $1$ is linearly independent. However, the set of both columns is linearly dependent since $c_1 = c_2$. Thus, the largest number of columns that can be selected to form a linearly independent set is $1$. The $k$-rank is $k_C = 1$.\n\nNow we check Kruskal's condition with $R=2$, $k_A=2$, $k_B=2$, and $k_C=1$:\n$$k_A + k_B + k_C \\ge 2R + 2$$\n$$2 + 2 + 1 \\ge 2(2) + 2$$\n$$5 \\ge 6$$\nThis inequality is false. Therefore, Kruskal's sufficient condition for uniqueness is not satisfied for this decomposition. This implies that the decomposition is not guaranteed to be unique and alternative factorizations may exist.\n\nPart 2: Justification of the Alternative Factorization\n\nWe are asked to show that the tensor $\\mathcal{T}$ can also be represented by a new set of factor matrices $(A', B', C')$ defined as:\n$$\nA' = A R(t), \\quad B' = B \\big(R(t)^{-T}\\big), \\quad C' = C\n$$\nwhere $R(t) = \\begin{pmatrix} 1  t \\\\ 0  1 \\end{pmatrix}$.\nThe CP decomposition is defined element-wise as $\\mathcal{T}_{ijk} = \\sum_{r=1}^{R} A_{ir} B_{jr} C_{kr}$. Let $\\mathcal{T'}$ be the tensor generated by the new factors. Its elements are given by:\n$$\n\\mathcal{T}'_{ijk} = \\sum_{r=1}^{R} A'_{ir} B'_{jr} C'_{kr}\n$$\nThe columns of the new factor matrices are linear combinations of the old ones. Let's substitute the definitions of $A'$, $B'$, and $C'$ into the formula for $\\mathcal{T}'_{ijk}$. Let $R$ denote $R(t)$.\n$$A'_{ir} = (AR)_{ir} = \\sum_{s=1}^{R} A_{is} R_{sr}$$\n$$B'_{jr} = (BR^{-T})_{jr} = \\sum_{p=1}^{R} B_{jp} (R^{-T})_{pr} = \\sum_{p=1}^{R} B_{jp} (R^{-1})_{rp}$$\n$$C'_{kr} = C_{kr} \\quad (\\text{since } C' = C)$$\nSubstituting these into the expression for $\\mathcal{T}'_{ijk}$:\n$$\n\\mathcal{T}'_{ijk} = \\sum_{r=1}^{R} \\left(\\sum_{s=1}^{R} A_{is} R_{sr}\\right) \\left(\\sum_{p=1}^{R} B_{jp} (R^{-1})_{rp}\\right) C_{kr}\n$$\nWe can rearrange the summations:\n$$\n\\mathcal{T}'_{ijk} = \\sum_{s=1}^{R} \\sum_{p=1}^{R} A_{is} B_{jp} \\left( \\sum_{r=1}^{R} R_{sr} (R^{-1})_{rp} C_{kr} \\right)\n$$\nLet's analyze the term in the parentheses. This identity holds due to a special property of the matrix $C$. The rows of $C$ are $(1,1)$ and $(1,1)$. This means that for a fixed row index $k$, the value of $C_{kr}$ is constant for all column indices $r$. Let us denote this constant value by $c_k^*$. In this problem, $C_{11}=C_{12}=1$ so $c_1^*=1$, and $C_{21}=C_{22}=1$ so $c_2^*=1$.\nWe can factor $C_{kr} = c_k^*$ out of the innermost summation over $r$:\n$$\n\\sum_{r=1}^{R} R_{sr} (R^{-1})_{rp} C_{kr} = c_k^* \\sum_{r=1}^{R} R_{sr} (R^{-1})_{rp}\n$$\nThe summation $\\sum_{r=1}^{R} R_{sr} (R^{-1})_{rp}$ is the $(s,p)$-th element of the matrix product $RR^{-1}$, which is the identity matrix $I$. Therefore, $\\sum_{r=1}^{R} R_{sr} (R^{-1})_{rp} = \\delta_{sp}$, where $\\delta_{sp}$ is the Kronecker delta.\nThe expression in parentheses simplifies to $c_k^* \\delta_{sp}$. Substituting this back:\n$$\n\\mathcal{T}'_{ijk} = \\sum_{s=1}^{R} \\sum_{p=1}^{R} A_{is} B_{jp} (c_k^* \\delta_{sp})\n$$\nThe Kronecker delta $\\delta_{sp}$ is zero unless $s=p$, so the summation over $p$ collapses:\n$$\n\\mathcal{T}'_{ijk} = \\sum_{s=1}^{R} A_{is} B_{js} c_k^*\n$$\nSince $c_k^*$ is the constant value of the $k$-th row of $C$, we have $c_k^* = C_{ks}$ for any $s$. So, we can write:\n$$\n\\mathcal{T}'_{ijk} = \\sum_{s=1}^{R} A_{is} B_{js} C_{ks} = \\mathcal{T}_{ijk}\n$$\nThis holds for all $i,j,k$, proving that $\\mathcal{T}' = \\mathcal{T}$. This is valid for any $t$ for which $R(t)$ is invertible. Since $\\det(R(t)) = 1 \\ne 0$ for all $t \\in \\mathbb{R}$, this family of alternative factorizations is always valid.\n\nPart 3: Determination of the parameter $t$\n\nWe need to find the value of $t$ that satisfies the normalization constraint that the first entry of the first column of $B'$ equals $0$. First, we compute the matrix $B' = B(R(t)^{-T})$.\n\nThe matrix $R(t)$ and its inverse are:\n$$\nR(t) = \\begin{pmatrix} 1  t \\\\ 0  1 \\end{pmatrix}, \\quad R(t)^{-1} = \\begin{pmatrix} 1  -t \\\\ 0  1 \\end{pmatrix}\n$$\nThe required matrix $R(t)^{-T}$ is the transpose of the inverse:\n$$\nR(t)^{-T} = (R(t)^{-1})^T = \\begin{pmatrix} 1  0 \\\\ -t  1 \\end{pmatrix}\n$$\nNow, we compute $B'$ by post-multiplying $B$ by $R(t)^{-T}$:\n$$\nB' = B R(t)^{-T} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ -t  1 \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nB' = \\begin{pmatrix} 1(1) + 1(-t)  1(0) + 1(1) \\\\ 1(1) + (-1)(-t)  1(0) + (-1)(1) \\end{pmatrix} = \\begin{pmatrix} 1-t  1 \\\\ 1+t  -1 \\end{pmatrix}\n$$\nThe first column of $B'$ is $\\begin{pmatrix} 1-t \\\\ 1+t \\end{pmatrix}$. The first entry of this column is $1-t$. The normalization constraint requires this entry to be $0$:\n$$\n1-t = 0\n$$\nSolving for $t$, we find:\n$$\nt = 1\n$$\nThus, the specific value of the parameter that satisfies the normalization constraint is $t=1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Tensors exhibit subtle behaviors not seen in matrices, most notably the distinction between rank and border rank. This advanced exercise guides you through the construction of a classic example that illuminates this concept, showing how a tensor of a certain rank can be approximated with arbitrary precision by tensors of a lower rank . This phenomenon has profound implications, demonstrating that the problem of finding a best low-rank approximation is not always guaranteed to have a solution.",
            "id": "3586501",
            "problem": "Let $X \\in \\mathbb{R}^{I \\times J \\times K}$ be a third-order tensor. A CANonical DECOMPosition/PARAllel FACtors (CP) decomposition of $X$ of rank $R$ is a representation $X = \\sum_{r=1}^{R} \\lambda_{r} \\, a_{r} \\otimes b_{r} \\otimes c_{r}$ with $a_{r} \\in \\mathbb{R}^{I}$, $b_{r} \\in \\mathbb{R}^{J}$, $c_{r} \\in \\mathbb{R}^{K}$ and $\\lambda_{r} \\in \\mathbb{R}$. The CP rank of $X$ is the smallest $R$ for which such a representation exists. The border rank of $X$ is the smallest $R$ such that $X$ is a limit (in the Frobenius norm) of tensors of CP rank at most $R$. The Frobenius norm $\\|X\\|_{F}$ is the square root of the sum of squares of all entries of $X$.\n\nWork over $\\mathbb{R}$ and consider the canonical basis vectors $e_{1}, e_{2} \\in \\mathbb{R}^{2}$. Define the target tensor $X_{0} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ implicitly by its nonzero entries $X_{2,1,1} = 1$, $X_{1,2,1} = 1$, and $X_{1,1,2} = 1$, i.e., $X_{0} = e_{2} \\otimes e_{1} \\otimes e_{1} + e_{1} \\otimes e_{2} \\otimes e_{1} + e_{1} \\otimes e_{1} \\otimes e_{2}$.\n\nUsing only the fundamental definitions above and elementary properties of outer products, do the following:\n\n- Construct explicitly a one-parameter family $\\{X_{\\epsilon}\\}_{\\epsilon  0} \\subset \\mathbb{R}^{2 \\times 2 \\times 2}$ with CP rank at most $2$ for each $\\epsilon  0$, such that the factors in a CP representation of each $X_{\\epsilon}$ necessarily diverge in norm as $\\epsilon \\to 0$, yet $X_{\\epsilon} \\to X_{0}$ in Frobenius norm. Your construction must make clear, by choice of normalization, where the divergence occurs (e.g., in scalar weights or in factor vectors), and you must verify convergence $X_{\\epsilon} \\to X_{0}$.\n- Prove that $X_{0}$ has CP rank strictly greater than $2$, and hence that the best rank-$2$ approximation problem to $X_{0}$ in Frobenius norm is not attained, even though the infimum error is $0$.\n\nFinally, compute the Frobenius norm $\\|X_{0}\\|_{F}$. Provide your final answer as a single exact expression. No rounding is required.",
            "solution": "We proceed with the solution, which is divided into three parts as requested.\n\nFirst, we construct the one-parameter family of tensors $\\{X_{\\epsilon}\\}_{\\epsilon  0}$ with CP rank at most $2$ that converges to $X_{0}$.\nLet us define the family $\\{X_{\\epsilon}\\}$ for $\\epsilon  0$ as a sum of two rank-$1$ tensors. A suitable construction is inspired by a finite difference approximation of a derivative. Consider the two rank-$1$ tensors $T_1(\\epsilon) = (e_1 + \\epsilon e_2) \\otimes (e_1 + \\epsilon e_2) \\otimes (e_1 + \\epsilon e_2)$ and $T_2 = e_1 \\otimes e_1 \\otimes e_1$.\nWe define $X_{\\epsilon}$ as:\n$$\nX_{\\epsilon} = \\frac{1}{\\epsilon} \\left( T_1(\\epsilon) - T_2 \\right) = \\frac{1}{\\epsilon} \\left( (e_1 + \\epsilon e_2) \\otimes (e_1 + \\epsilon e_2) \\otimes (e_1 + \\epsilon e_2) - e_1 \\otimes e_1 \\otimes e_1 \\right)\n$$\nBy construction, for any $\\epsilon  0$, $X_{\\epsilon}$ is the sum of two rank-$1$ tensors (with coefficients $\\frac{1}{\\epsilon}$ and $-\\frac{1}{\\epsilon}$). Thus, the CP rank of $X_{\\epsilon}$ is at most $2$. We note that for $\\epsilon  0$, the two rank-$1$ tensors are linearly independent, so the rank is exactly $2$.\nThis can be written in the form $X_\\epsilon = \\lambda_1 a_1 \\otimes b_1 \\otimes c_1 + \\lambda_2 a_2 \\otimes b_2 \\otimes c_2$ with:\n$\\lambda_1 = \\frac{1}{\\epsilon}$, $a_1 = b_1 = c_1 = e_1 + \\epsilon e_2$\n$\\lambda_2 = -\\frac{1}{\\epsilon}$, $a_2 = b_2 = c_2 = e_1$\nClearly, the scalar weights $\\lambda_1$ and $\\lambda_2$ diverge as $\\epsilon \\to 0$.\n\nNow, we verify that $X_{\\epsilon} \\to X_{0}$ as $\\epsilon \\to 0$. We expand the outer product in $T_1(\\epsilon)$ using the distributive property:\n\\begin{align*} T_1(\\epsilon) = e_1\\otimes e_1\\otimes e_1 \\\\ + \\epsilon (e_2\\otimes e_1\\otimes e_1 + e_1\\otimes e_2\\otimes e_1 + e_1\\otimes e_1\\otimes e_2) \\\\ + \\epsilon^2 (e_2\\otimes e_2\\otimes e_1 + e_2\\otimes e_1\\otimes e_2 + e_1\\otimes e_2\\otimes e_2) \\\\ + \\epsilon^3 (e_2\\otimes e_2\\otimes e_2) \\end{align*}\nSubstituting this into the expression for $X_{\\epsilon}$ and recalling the definition of $X_0$:\n\\begin{align*} X_{\\epsilon} = \\frac{1}{\\epsilon} \\left( T_1(\\epsilon) - e_1\\otimes e_1\\otimes e_1 \\right) \\\\ = (e_2\\otimes e_1\\otimes e_1 + e_1\\otimes e_2\\otimes e_1 + e_1\\otimes e_1\\otimes e_2) \\\\ + \\epsilon (e_2\\otimes e_2\\otimes e_1 + e_2\\otimes e_1\\otimes e_2 + e_1\\otimes e_2\\otimes e_2) \\\\ + \\epsilon^2 (e_2\\otimes e_2\\otimes e_2) \\\\ = X_0 + \\epsilon (e_2\\otimes e_2\\otimes e_1 + e_2\\otimes e_1\\otimes e_2 + e_1\\otimes e_2\\otimes e_2) + \\epsilon^2 (e_2\\otimes e_2\\otimes e_2)\\end{align*}\nThe difference is $E_{\\epsilon} = X_{\\epsilon} - X_0 = \\epsilon (e_2\\otimes e_2\\otimes e_1 + e_2\\otimes e_1\\otimes e_2 + e_1\\otimes e_2\\otimes e_2) + \\epsilon^2 (e_2\\otimes e_2\\otimes e_2)$.\nThe elementary tensors $e_i \\otimes e_j \\otimes e_k$ form an orthonormal basis for the space of tensors with respect to the Frobenius inner product. Therefore, the square of the Frobenius norm is the sum of squares of the coefficients of these basis tensors.\n$$\n\\| E_{\\epsilon} \\|_F^2 = \\epsilon^2 (1^2 + 1^2 + 1^2) + (\\epsilon^2)^2 (1^2) = 3\\epsilon^2 + \\epsilon^4\n$$\nAs $\\epsilon \\to 0$, $\\| E_{\\epsilon} \\|_F^2 \\to 0$, which implies $\\|X_{\\epsilon} - X_0\\|_F \\to 0$. This confirms the convergence. We have thus shown that $X_0$ has border rank at most $2$.\n\nSecond, we prove that the CP rank of $X_{0}$ is strictly greater than $2$.\nWe proceed by contradiction. Assume that the CP rank of $X_0$ is 2. Then, there exist vectors $u_1, u_2, v_1, v_2, w_1, w_2 \\in \\mathbb{R}^2$ such that\n$$\nX_0 = u_1 \\otimes v_1 \\otimes w_1 + u_2 \\otimes v_2 \\otimes w_2\n$$\nThe entries of $X_0$ are given by $(X_0)_{ijk} = (u_1)_i(v_1)_j(w_1)_k + (u_2)_i(v_2)_j(w_2)_k$.\nA third-order tensor can be viewed as a collection of matrix \"slices\". Let $M_k = X_0(:,:,k)$ be the $k$-th frontal slice.\nFor $k=1$, we have $M_1 = \\begin{pmatrix} (X_0)_{111}  (X_0)_{121} \\\\ (X_0)_{211}  (X_0)_{221} \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$.\nFor $k=2$, we have $M_2 = \\begin{pmatrix} (X_0)_{112}  (X_0)_{122} \\\\ (X_0)_{212}  (X_0)_{222} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\nFrom the rank-$2$ decomposition, the slices are given by\n$$\nM_k = (w_1)_k (u_1 v_1^T) + (w_2)_k (u_2 v_2^T)\n$$\nLet $A_1 = u_1 v_1^T$ and $A_2 = u_2 v_2^T$. Both $A_1$ and $A_2$ are rank-$1$ matrices (or the zero matrix, which is a trivial case we can exclude).\nThis means that both $M_1$ and $M_2$ are in the two-dimensional vector space of matrices spanned by $A_1$ and $A_2$. Let this space be $\\mathcal{S} = \\text{span}\\{A_1, A_2\\}$.\nThe matrices $M_1$ and $M_2$ are linearly independent, since if $c_1 M_1 + c_2 M_2 = 0$, then $\\begin{pmatrix} c_2  c_1 \\\\ c_1  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$, which implies $c_1=c_2=0$. Therefore, $\\{M_1, M_2\\}$ forms a basis for $\\mathcal{S}$.\nThis implies that $A_1$ and $A_2$ must themselves be in $\\mathcal{S}$, and can be expressed as linear combinations of $M_1$ and $M_2$. Since $A_1$ and $A_2$ are rank-$1$ matrices, the space $\\mathcal{S}$ must contain rank-$1$ matrices.\nLet's find all matrices in $\\mathcal{S}$ that have rank at most $1$. A general matrix in $\\mathcal{S}$ has the form $M = c_1 M_1 + c_2 M_2 = \\begin{pmatrix} c_2  c_1 \\\\ c_1  0 \\end{pmatrix}$ for some scalars $c_1, c_2 \\in \\mathbb{R}$.\nThe rank of $M$ is at most $1$ if and only if its determinant is zero.\n$$\n\\det(M) = (c_2)(0) - (c_1)(c_1) = -c_1^2\n$$\nFor $\\det(M) = 0$, we must have $c_1=0$. This means the only matrices in $\\mathcal{S}$ with rank at most $1$ are of the form $c_2 M_2 = \\begin{pmatrix} c_2  0 \\\\ 0  0 \\end{pmatrix}$. This is a $1$-dimensional subspace of $\\mathcal{S}$.\nSince $A_1$ and $A_2$ must be rank-$1$ matrices in $\\mathcal{S}$, both must belong to this subspace. Thus, $A_1 = k_1 M_2$ and $A_2 = k_2 M_2$ for some scalars $k_1, k_2$.\nThis implies that $A_1$ and $A_2$ are linearly dependent.\nIf $A_1$ and $A_2$ are linearly dependent, the space they span, $\\mathcal{S}$, is at most $1$-dimensional.\nBut we have already established that $\\mathcal{S}$ is $2$-dimensional, as it is spanned by the linearly independent matrices $M_1$ and $M_2$.\nThis is a contradiction. Therefore, the initial assumption that the CP rank of $X_0$ is $2$ must be false. Hence, $\\operatorname{rank}(X_0) > 2$.\nFrom its definition, $X_0$ is a sum of three rank-$1$ tensors, so its rank is at most $3$. Thus, $\\operatorname{rank}(X_0) = 3$.\n\nThe consequence is that the best rank-$2$ approximation problem for $X_0$ is not attained. The problem is to find $\\min_{\\text{rank}(Y) \\le 2} \\|X_0 - Y\\|_F$.\nWe have shown that $X_0 = \\lim_{\\epsilon \\to 0} X_{\\epsilon}$ where $\\operatorname{rank}(X_{\\epsilon}) = 2$. This implies that the infimum of the error is zero:\n$$\n\\inf_{\\text{rank}(Y) \\le 2} \\|X_0 - Y\\|_F \\le \\lim_{\\epsilon \\to 0} \\|X_0 - X_{\\epsilon}\\|_F = 0\n$$\nSince the norm is non-negative, the infimum must be exactly $0$. If this minimum were attained, there would exist a tensor $Y_0$ with $\\operatorname{rank}(Y_0) \\le 2$ such that $\\|X_0 - Y_0\\|_F = 0$. This implies $X_0 = Y_0$, and thus $\\operatorname{rank}(X_0) \\le 2$. This contradicts our finding that $\\operatorname{rank}(X_0) = 3$. Therefore, the minimum is not attained.\n\nFinally, we compute the Frobenius norm of $X_{0}$.\nThe tensor $X_0$ has three non-zero entries, all equal to $1$: $X_{2,1,1}=1$, $X_{1,2,1}=1$, and $X_{1,1,2}=1$. All other $2^3 - 3 = 5$ entries are zero.\nBy definition, the squared Frobenius norm is the sum of the squares of all entries.\n$$\n\\|X_0\\|_F^2 = \\sum_{i=1}^2 \\sum_{j=1}^2 \\sum_{k=1}^2 (X_0)_{ijk}^2 = (X_{2,1,1})^2 + (X_{1,2,1})^2 + (X_{1,1,2})^2 = 1^2 + 1^2 + 1^2 = 1+1+1 = 3\n$$\nThe Frobenius norm is the square root of this value.\n$$\n\\|X_0\\|_F = \\sqrt{3}\n$$\nThis is the final numerical value requested.",
            "answer": "$$\\boxed{\\sqrt{3}}$$"
        }
    ]
}