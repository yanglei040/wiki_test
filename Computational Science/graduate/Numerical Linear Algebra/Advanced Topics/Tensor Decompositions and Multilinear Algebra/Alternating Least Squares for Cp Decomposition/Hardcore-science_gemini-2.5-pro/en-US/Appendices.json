{
    "hands_on_practices": [
        {
            "introduction": "Before delving into algorithms for decomposing tensors, it is crucial to have a concrete understanding of the CANDECOMP/PARAFAC (CP) structure itself. This first exercise works in reverse, starting from a given set of factor matrices to reconstruct the full tensor they represent. By calculating the tensor entries directly and verifying the slice-wise formulation, you will build a tangible intuition for how rank-one components sum to form a low-rank tensor model .",
            "id": "3533244",
            "problem": "Consider the CANDECOMP/PARAFAC (CP) model for a three-way tensor, and its use within Alternating Least Squares (ALS), where one fixes two factor matrices and solves a least-squares subproblem for the remaining factor. Work with a concrete rank-$R$ CP representation for a tensor of size $I \\times J \\times K$ given by the sum of $R$ rank-one outer products. Let $I=J=K=2$ and $R=2$. Specify the factor matrices\n$$\nA=\\begin{pmatrix}\n1 & 2\\\\\n3 & 1\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}\n0 & 1\\\\\n2 & -1\n\\end{pmatrix},\\quad\nC=\\begin{pmatrix}\n1 & 0\\\\\n1 & 2\n\\end{pmatrix},\n$$\nwhere each column corresponds to a rank-one component. Using the CP definition, the reconstructed tensor $\\mathcal{X}\\in\\mathbb{R}^{2\\times 2\\times 2}$ has entries $x_{ijk}$ obtained from the factors.\n\nTasks:\n- Starting from the rank-one outer-product structure that defines CP, derive the formula for the entries $x_{ijk}$ in terms of $A$, $B$, and $C$, and compute all eight entries of the reconstructed tensor $\\mathcal{X}$ for the specified $A$, $B$, and $C$.\n- Express each frontal slice $\\mathcal{X}(:,:,k)$ as a matrix in terms of $A$, $B$, and the diagonal matrix formed from the $k$-th row of $C$, and verify consistency with the computed entries.\n- Compute the Frobenius norm $\\|\\mathcal{X}\\|_{F}$ of the reconstructed tensor. If you decide to approximate, round your answer to four significant figures; otherwise, provide the exact value.\n\nYour final reported quantity must be the Frobenius norm $\\|\\mathcal{X}\\|_{F}$ as a single real number or a single closed-form analytic expression.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of multilinear algebra, specifically the CANDECOMP/PARAFAC (CP) decomposition, and is well-posed with all necessary information provided for a unique solution.\n\nThe CANDECOMP/PARAFAC (CP) decomposition of a three-way tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ of rank $R$ is given by a sum of $R$ rank-one tensors. Each rank-one tensor is the outer product of three vectors. Let the factor matrices be $A \\in \\mathbb{R}^{I \\times R}$, $B \\in \\mathbb{R}^{J \\times R}$, and $C \\in \\mathbb{R}^{K \\times R}$. Let the columns of these matrices be $\\mathbf{a}_r$, $\\mathbf{b}_r$, and $\\mathbf{c}_r$ for $r=1, \\dots, R$. The tensor $\\mathcal{X}$ is then expressed as:\n$$\n\\mathcal{X} = \\sum_{r=1}^{R} \\mathbf{a}_r \\circ \\mathbf{b}_r \\circ \\mathbf{c}_r\n$$\nwhere $\\circ$ denotes the outer product.\n\nThe first task is to derive the formula for the entries $x_{ijk}$ of the tensor $\\mathcal{X}$. From the definition of the outer product, an element $(i,j,k)$ of the tensor $\\mathbf{a}_r \\circ \\mathbf{b}_r \\circ \\mathbf{c}_r$ is given by the product of the corresponding vector elements, $(a_r)_i (b_r)_j (c_r)_k$. In matrix notation, these are $A_{ir}$, $B_{jr}$, and $C_{kr}$. Thus, the entry $x_{ijk}$ of the tensor $\\mathcal{X}$ is obtained by summing over the $R$ components:\n$$\nx_{ijk} = \\sum_{r=1}^{R} A_{ir} B_{jr} C_{kr}\n$$\nIn this problem, the dimensions are $I=J=K=2$ and the rank is $R=2$. The given factor matrices are:\n$$\nA=\\begin{pmatrix} 1 & 2\\\\ 3 & 1 \\end{pmatrix},\\quad\nB=\\begin{pmatrix} 0 & 1\\\\ 2 & -1 \\end{pmatrix},\\quad\nC=\\begin{pmatrix} 1 & 0\\\\ 1 & 2 \\end{pmatrix}\n$$\nThe entries of the reconstructed tensor $\\mathcal{X} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ are calculated using the formula for $R=2$:\n$$\nx_{ijk} = A_{i1}B_{j1}C_{k1} + A_{i2}B_{j2}C_{k2}\n$$\nWe compute all $8$ entries, organized into the two frontal slices, $\\mathcal{X}(:,:,1)$ and $\\mathcal{X}(:,:,2)$.\n\nFor the first frontal slice ($k=1$):\nThe first row of $C$ is $(C_{11}, C_{12}) = (1, 0)$.\n$x_{111} = A_{11}B_{11}C_{11} + A_{12}B_{12}C_{12} = (1)(0)(1) + (2)(1)(0) = 0$\n$x_{121} = A_{11}B_{21}C_{11} + A_{12}B_{22}C_{12} = (1)(2)(1) + (2)(-1)(0) = 2$\n$x_{211} = A_{21}B_{11}C_{11} + A_{22}B_{12}C_{12} = (3)(0)(1) + (1)(1)(0) = 0$\n$x_{221} = A_{21}B_{21}C_{11} + A_{22}B_{22}C_{12} = (3)(2)(1) + (1)(-1)(0) = 6$\nSo, the first frontal slice is the matrix $\\mathcal{X}(:,:,1) = \\begin{pmatrix} 0 & 2 \\\\ 0 & 6 \\end{pmatrix}$.\n\nFor the second frontal slice ($k=2$):\nThe second row of $C$ is $(C_{21}, C_{22}) = (1, 2)$.\n$x_{112} = A_{11}B_{11}C_{21} + A_{12}B_{12}C_{22} = (1)(0)(1) + (2)(1)(2) = 4$\n$x_{122} = A_{11}B_{21}C_{21} + A_{12}B_{22}C_{22} = (1)(2)(1) + (2)(-1)(2) = 2 - 4 = -2$\n$x_{212} = A_{21}B_{11}C_{21} + A_{22}B_{12}C_{22} = (3)(0)(1) + (1)(1)(2) = 2$\n$x_{222} = A_{21}B_{21}C_{21} + A_{22}B_{22}C_{22} = (3)(2)(1) + (1)(-1)(2) = 6 - 2 = 4$\nSo, the second frontal slice is the matrix $\\mathcal{X}(:,:,2) = \\begin{pmatrix} 4 & -2 \\\\ 2 & 4 \\end{pmatrix}$.\n\nThe second task is to express each frontal slice $\\mathcal{X}(:,:,k)$, which we denote by the matrix $X_k$, in terms of $A$, $B$, and a diagonal matrix formed from the $k$-th row of $C$. The formula for the $k$-th frontal slice is $X_k = A D_k(C) B^T$, where $D_k(C)$ is a diagonal matrix whose diagonal entries are the elements of the $k$-th row of $C$, i.e., $(D_k(C))_{rr} = C_{kr}$.\nLet's verify this formula by computing the $(i,j)$-th entry of $A D_k(C) B^T$:\n$$\n(A D_k(C) B^T)_{ij} = \\sum_{r=1}^{R} (A D_k(C))_{ir} (B^T)_{rj} = \\sum_{r=1}^{R} \\left( \\sum_{s=1}^{R} A_{is} (D_k(C))_{sr} \\right) B_{jr}\n$$\nSince $D_k(C)$ is diagonal, $(D_k(C))_{sr} = C_{kr}\\delta_{sr}$, where $\\delta_{sr}$ is the Kronecker delta.\n$$\n(A D_k(C) B^T)_{ij} = \\sum_{r=1}^{R} \\left( \\sum_{s=1}^{R} A_{is} C_{ks}\\delta_{sr} \\right) B_{jr} = \\sum_{r=1}^{R} A_{ir} C_{kr} B_{jr} = \\sum_{r=1}^{R} A_{ir}B_{jr}C_{kr} = x_{ijk}\n$$\nThis confirms the slice-wise formula. Now we verify it for the given matrices.\n\nFor $k=1$, the first row of $C$ is $(1,0)$, so $D_1(C) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n$$\nX_1 = A D_1(C) B^T = \\begin{pmatrix} 1 & 2 \\\\ 3 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 2 & -1 \\end{pmatrix}^T\n= \\begin{pmatrix} 1 & 0 \\\\ 3 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 2 \\\\ 1 & -1 \\end{pmatrix}\n= \\begin{pmatrix} 0 & 2 \\\\ 0 & 6 \\end{pmatrix}\n$$\nThis matches our computed $\\mathcal{X}(:,:,1)$.\n\nFor $k=2$, the second row of $C$ is $(1,2)$, so $D_2(C) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$.\n$$\nX_2 = A D_2(C) B^T = \\begin{pmatrix} 1 & 2 \\\\ 3 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 2 & -1 \\end{pmatrix}^T\n= \\begin{pmatrix} 1 & 4 \\\\ 3 & 2 \\end{pmatrix} \\begin{pmatrix} 0 & 2 \\\\ 1 & -1 \\end{pmatrix}\n= \\begin{pmatrix} 4 & 2-4 \\\\ 2 & 6-2 \\end{pmatrix}\n= \\begin{pmatrix} 4 & -2 \\\\ 2 & 4 \\end{pmatrix}\n$$\nThis matches our computed $\\mathcal{X}(:,:,2)$.\n\nThe third task is to compute the Frobenius norm $\\|\\mathcal{X}\\|_{F}$ of the reconstructed tensor. The squared Frobenius norm is the sum of the squares of all its elements:\n$$\n\\|\\mathcal{X}\\|_{F}^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} x_{ijk}^2\n$$\nUsing the entries we have calculated:\n$$\n\\|\\mathcal{X}\\|_{F}^2 = \\underbrace{(0^2 + 2^2 + 0^2 + 6^2)}_{\\text{Slice 1}} + \\underbrace{(4^2 + (-2)^2 + 2^2 + 4^2)}_{\\text{Slice 2}}\n$$\n$$\n\\|\\mathcal{X}\\|_{F}^2 = (0 + 4 + 0 + 36) + (16 + 4 + 4 + 16)\n$$\n$$\n\\|\\mathcal{X}\\|_{F}^2 = 40 + 40 = 80\n$$\nThe Frobenius norm is the square root of this value:\n$$\n\\|\\mathcal{X}\\|_{F} = \\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5}\n$$\nThe problem specifies to provide the exact value or an approximation to four significant figures. The exact value is $4\\sqrt{5}$.",
            "answer": "$$\n\\boxed{4\\sqrt{5}}\n$$"
        },
        {
            "introduction": "The 'least squares' in Alternating Least Squares (ALS) refers to the subproblem solved at each iteration to update one factor matrix. This exercise probes the numerical core of that step, comparing two classic methods: solving the normal equations versus using an orthogonal (QR) factorization. By analyzing how each approach handles the condition number of the design matrix, you will discover why the choice of linear solver is not merely a detail, but a critical factor that can dramatically affect the accuracy of the computed decomposition .",
            "id": "3533190",
            "problem": "Consider a single Alternating Least Squares (ALS) update in the Canonical Polyadic (CP) decomposition. To update the factor matrix $A \\in \\mathbb{R}^{I_{A} \\times R}$, one solves, row-wise, the overdetermined least-squares problems with common design matrix $Z \\in \\mathbb{R}^{(I_{B} I_{C}) \\times R}$ given by the Khatri-Rao product $Z = C \\odot B$, where $B \\in \\mathbb{R}^{I_{B} \\times R}$ and $C \\in \\mathbb{R}^{I_{C} \\times R}$. The right-hand side is obtained by the Matricized-Tensor Times Khatri-Rao Product (MTTKRP). Assume $Z$ has full column rank and computations are performed in floating-point arithmetic with unit roundoff $u$. Two classical approaches are used:\n\n- Normal equations with Cholesky factorization: form $G = Z^{\\top} Z$ and solve $G x = Z^{\\top} b$ via Cholesky.\n- Orthogonal-Triangular (QR) factorization: compute a thin QR factorization $Z = Q R$ with $Q^{\\top} Q = I$ and solve $R x = Q^{\\top} b$.\n\nStarting from the singular value decomposition definition of the $2$-norm condition number $\\kappa_{2}(Z) = \\sigma_{\\max}(Z)/\\sigma_{\\min}(Z)$, the relationship between the singular values of $Z$ and those of $Z^{\\top} Z$, and standard backward stability properties of Cholesky and QR factorizations, compare the forward accuracy of the two approaches for solving the overdetermined least-squares problems with design matrix $Z = C \\odot B$. In your comparison, use only the following base facts: (i) the spectral norm is submultiplicative, (ii) the singular values of $Z^{\\top} Z$ are the squares of the singular values of $Z$, (iii) QR factorization of a full-rank matrix is backward stable, and (iv) Cholesky factorization of a symmetric positive definite matrix is backward stable. You may also use that the Khatri-Rao product satisfies $Z = C \\odot B$ and is a column-submatrix of the Kronecker product $C \\otimes B$ with known norm identities for the Kronecker product.\n\nQuantify the accuracy gap in terms of $u$ and $\\kappa_{2}(Z)$, and then evaluate it in the following concrete scenario: suppose $\\kappa_{2}(B) = 10^{4}$, $\\kappa_{2}(C) = 10^{3}$, and IEEE double precision with $u \\approx 10^{-16}$ is used. Select all statements that are correct.\n\nA. In floating-point arithmetic, the normal-equations-plus-Cholesky route yields a forward relative error on the least-squares solution on the order of $u \\,\\kappa_{2}(Z)^{2}$ per right-hand side, while the Orthogonal-Triangular (QR) route yields a forward relative error on the order of $u \\,\\kappa_{2}(Z)$. Consequently, relative to QR, normal equations lose about an additional $\\log_{10}\\!\\big(\\kappa_{2}(Z)\\big)$ correct decimal digits. For the given numerical values, one may bound $\\kappa_{2}(Z) \\le \\kappa_{2}(C)\\,\\kappa_{2}(B) = 10^{7}$, so the error magnitudes are approximately $10^{-2}$ (normal equations) versus $10^{-9}$ (QR), i.e., about $7$ extra digits lost by normal equations.\n\nB. Because $(C \\odot B)^{\\top} (C \\odot B) = (C^{\\top} C) * (B^{\\top} B)$ is a Hadamard product, the condition number of the normal equations is at most $\\max\\{\\kappa_{2}(B)^{2}, \\kappa_{2}(C)^{2}\\}$, implying no asymptotic accuracy loss relative to QR.\n\nC. If the columns of $B$ and $C$ are scaled to unit norm, then $\\kappa_{2}(C \\odot B) = 1$ regardless of column correlations, so both approaches have comparable accuracy up to constants.\n\nD. When $Z$ is very tall and skinny, with $(I_{B} I_{C}) \\gg R$, both approaches achieve forward relative error on the order of $u$ independent of $\\kappa_{2}(Z)$, so there is no meaningful accuracy gap between them in that regime.",
            "solution": "This problem requires a comparison of the forward accuracy of two methods for solving the overdetermined least-squares problems that arise in the Alternating Least Squares (ALS) algorithm: the normal equations method and the QR factorization method. The analysis hinges on how the condition number of the design matrix affects the stability of each approach.\n\n### Derivation and Analysis\n\nThe core of the problem is to compare the forward error of the least-squares solution computed via the normal equations versus the QR factorization. Let the exact solution to $\\min_{x} \\|Zx - b\\|_{2}$ be $x_{LS} = (Z^{\\top}Z)^{-1}Z^{\\top}b$. The relative forward error for a computed solution $\\hat{x}$ is given by $\\frac{\\|\\hat{x} - x_{LS}\\|_{2}}{\\|x_{LS}\\|_{2}}$.\n\n**1. Analysis of the Normal Equations (NE) Approach**\n\nThis approach involves forming $G = Z^{\\top}Z$ and solving the linear system $Gx = Z^{\\top}b$. The primary source of numerical instability is the formation of $G$, which explicitly squares the condition number of the problem.\n- The $2$-norm condition number of the design matrix $Z$ is $\\kappa_{2}(Z) = \\sigma_{\\max}(Z)/\\sigma_{\\min}(Z)$.\n- From fact (ii), the singular values of $G = Z^{\\top}Z$ are the squares of the singular values of $Z$. Since $G$ is symmetric positive definite, its condition number is the ratio of its largest to smallest eigenvalue:\n$$ \\kappa_{2}(G) = \\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)} = \\frac{\\sigma_{\\max}(Z)^{2}}{\\sigma_{\\min}(Z)^{2}} = \\kappa_{2}(Z)^{2} $$\n- Standard forward error analysis for solving a linear system $Gx = Z^{\\top}b$ shows that the relative forward error is bounded by a quantity proportional to the condition number of the matrix, $\\kappa_{2}(G)$. Thus, the relative forward error for the normal equations approach is on the order of $\\mathcal{O}(u \\, \\kappa_{2}(G)) = \\mathcal{O}(u \\, \\kappa_{2}(Z)^{2})$.\n\n**2. Analysis of the QR Factorization Approach**\n\nThis approach avoids forming $Z^{\\top}Z$ by computing a QR factorization $Z=QR$. The problem is transformed into solving the well-conditioned triangular system $Rx = Q^{\\top}b$.\n- The QR factorization of a full-rank matrix is backward stable (fact (iii)). Standard analysis shows that the forward error bound for the QR method is proportional to the condition number of the original design matrix $Z$.\n- The relative forward error for the QR approach is on the order of $\\mathcal{O}(u \\, \\kappa_{2}(Z))$.\n\n**3. Comparison and Quantification**\nThe accuracy gap is the factor $\\kappa_{2}(Z)$. The number of additional decimal digits lost by the normal equations method compared to the QR method is approximately $\\log_{10}(\\kappa_{2}(Z))$.\n\n**4. Evaluation of the Concrete Scenario**\n- Given: $\\kappa_{2}(B) = 10^{4}$, $\\kappa_{2}(C) = 10^{3}$, $u \\approx 10^{-16}$.\n- Using the standard inequality for the condition number of a Khatri-Rao product:\n$$ \\kappa_{2}(Z) = \\kappa_{2}(C \\odot B) \\le \\kappa_{2}(C)\\kappa_{2}(B) = 10^{3} \\times 10^{4} = 10^{7} $$\n- Using this upper bound as an estimate, $\\kappa_{2}(Z) \\approx 10^{7}$:\n  - **NE Error Estimate**: $u \\, \\kappa_{2}(Z)^{2} \\approx 10^{-16} \\times (10^{7})^{2} = 10^{-16} \\times 10^{14} = 10^{-2}$.\n  - **QR Error Estimate**: $u \\, \\kappa_{2}(Z) \\approx 10^{-16} \\times 10^{7} = 10^{-9}$.\n- **Extra digits lost by NE**: $\\log_{10}(\\kappa_{2}(Z)) \\approx \\log_{10}(10^{7}) = 7$.\n\n### Option-by-Option Analysis\n\n**A.** This statement correctly identifies the forward relative errors for the normal equations and QR methods as being on the order of $u \\,\\kappa_{2}(Z)^{2}$ and $u \\,\\kappa_{2}(Z)$, respectively. It correctly quantifies the loss of precision as $\\log_{10}(\\kappa_{2}(Z))$ decimal digits. The application to the numerical scenario is also correct: it uses the standard bound $\\kappa_{2}(Z) \\le \\kappa_{2}(C)\\,\\kappa_{2}(B) = 10^{7}$ and correctly calculates the approximate error magnitudes ($10^{-2}$ for NE, $10^{-9}$ for QR) and the number of lost digits (7). This statement is fully consistent with our derivation. **Therefore, statement A is correct.**\n\n**B.** The identity $(C \\odot B)^{\\top} (C \\odot B) = (C^{\\top} C) * (B^{\\top} B)$ is correct, where $*$ denotes the Hadamard product. However, the claim that the condition number of this Hadamard product is bounded by $\\max\\{\\kappa_{2}(B)^{2}, \\kappa_{2}(C)^{2}\\}$ is incorrect. There is no such general theorem. The conclusion of \"no asymptotic accuracy loss\" is based on this false premise. **Therefore, statement B is incorrect.**\n\n**C.** If the columns of $B$ and $C$ have unit norm, the columns of their Khatri-Rao product $Z = C \\odot B$ will also have unit norm. However, this does not imply that $Z$ has orthonormal columns or that $\\kappa_{2}(Z) = 1$. If columns within $B$ or $C$ are highly correlated (nearly parallel), the columns of $Z$ will also be correlated, leading to a large condition number. The claim that $\\kappa_{2}(Z)=1$ \"regardless of column correlations\" is false. **Therefore, statement C is incorrect.**\n\n**D.** The forward error bounds for least-squares solvers are fundamentally dependent on the condition number $\\kappa_{2}(Z)$, regardless of the matrix dimensions. A \"tall and skinny\" geometry does not eliminate this intrinsic sensitivity of the solution to perturbations in the data. The error is not on the order of $u$ independent of $\\kappa_2(Z)$. **Therefore, statement D is incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "With a solid grasp of the CP model's structure and the numerical mechanics of a single update, the final step is to analyze the algorithm's overall efficiency. This practice challenges you to perform a rigorous computational cost analysis for a complete CP-ALS sweep, accounting for both floating-point operations (flops) and memory usage. Deriving these cost formulas is a fundamental skill for any numerical algorithmist, as it provides the basis for predicting performance, identifying bottlenecks, and making informed implementation decisions for large-scale problems .",
            "id": "3586499",
            "problem": "Consider a dense order-$N$ data tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times \\cdots \\times I_N}$ and its Canonical Decomposition/Parallel Factors (CANDECOMP/PARAFAC, CP) model of rank $R$, represented by factor matrices $\\{A^{(n)} \\in \\mathbb{R}^{I_n \\times R}\\}_{n=1}^{N}$, so that the model approximation is $\\sum_{r=1}^{R} a^{(1)}_{:,r} \\circ \\cdots \\circ a^{(N)}_{:,r}$. One Alternating Least Squares (ALS) sweep visits each mode $n \\in \\{1,\\dots,N\\}$ and updates $A^{(n)}$ by solving a least-squares subproblem built from the mode-$n$ matricization $X_{(n)}$ and the Khatri–Rao product of the other factor matrices. Assume the following implementation and accounting rules, which you must use as the fundamental base for your derivation:\n\n- The Matricized Tensor Times Khatri–Rao Product (MTTKRP) for mode $n$ is computed without explicitly forming any Khatri–Rao products. It is evaluated as a dense matrix–matrix product between $X_{(n)} \\in \\mathbb{R}^{I_n \\times \\left(\\prod_{m \\neq n} I_m\\right)}$ and a $ \\left(\\prod_{m \\neq n} I_m\\right) \\times R$ operand implicit in the contraction. Count this as $2\\,I_n \\left(\\prod_{m \\neq n} I_m\\right) R$ floating-point operations (flops).\n- For each mode $n$, form the normal-equations coefficient $H^{(n)} \\in \\mathbb{R}^{R \\times R}$ as the Hadamard (elementwise) product $H^{(n)} = \\underset{m \\neq n}{\\bigodot}\\, S^{(m)}$, where $S^{(m)} = A^{(m)\\top} A^{(m)} \\in \\mathbb{R}^{R \\times R}$ is the Gram matrix of the $m$th factor. Compute each $S^{(m)}$ via a dense matrix–matrix multiply and count it as $2\\,I_m R^2$ flops. Assemble $H^{(n)}$ naively for each mode without reuse across modes by chaining $(N-1)$ elementwise matrix products, which costs $(N-2)R^2$ flops per mode.\n- Solve the normal equations for updating $A^{(n)}$ by a Cholesky factorization of $H^{(n)}$ followed by two triangular solves with $I_n$ right-hand sides (i.e., update $A^{(n)} = M^{(n)} H^{(n)^{-1}}$, where $M^{(n)}$ is the MTTKRP output). Count the Cholesky factorization of an $R \\times R$ symmetric positive-definite matrix as $\\tfrac{1}{3}R^3$ flops and each triangular solve with $I_n$ right-hand sides as $I_n R^2$ flops, for a total of $2 I_n R^2$ flops for the two solves.\n- After updating $A^{(n)}$, recompute its Gram matrix $S^{(n)} = A^{(n)\\top} A^{(n)}$ at cost $2\\,I_n R^2$ flops to be used in subsequent modes within the same sweep.\n- All dense multiplications use the standard cost rule: multiplying an $m \\times k$ matrix by a $k \\times n$ matrix costs $2 m k n$ flops.\n- Memory model (peak storage, measured in scalar words): store the dense tensor $\\mathcal{X}$ explicitly, all factor matrices $\\{A^{(n)}\\}_{n=1}^{N}$, all Gram matrices $\\{S^{(n)}\\}_{n=1}^{N}$, a single temporary right-hand side $M^{(n)} \\in \\mathbb{R}^{I_n \\times R}$ for one mode at a time, and a single $R \\times R$ temporary for $H^{(n)}$. Do not store any Khatri–Rao products or tensor unfoldings explicitly.\n\nStarting only from these rules and the definitions of the CP decomposition and ALS, derive an exact, closed-form expression for:\n- the total flop count of one complete ALS sweep over all $N$ modes, and\n- the peak number of stored scalar words under the specified memory model,\n\nboth expressed in terms of $N$, $R$, and the mode sizes $\\{I_n\\}_{n=1}^{N}$. Your final answer must be a single row matrix whose first entry is the flop count expression and whose second entry is the peak memory expression, with no units. Do not use asymptotic notation. Do not round or approximate.",
            "solution": "This problem requires the derivation of closed-form expressions for the computational cost (in flops) and peak memory usage for one full sweep of the CP-ALS algorithm, based on a specific set of accounting rules. We will derive each quantity by summing the costs of the constituent operations over all $N$ modes.\n\n### Part 1: Total Flop Count Derivation\n\nThe total flop count, $C_{\\text{total}}$, is the sum of the costs for updating each mode $n \\in \\{1,\\dots,N\\}$. Let $C_n$ be the cost for updating mode $n$. An ALS sweep consists of performing this update for each mode.\nThe procedure for updating $A^{(n)}$ consists of several steps as per the rules.\n\n1.  **MTTKRP Computation**: The cost for computing the MTTKRP for mode $n$ is given as $2\\,I_n \\left(\\prod_{m \\neq n} I_m\\right) R$. Let $P = \\prod_{k=1}^{N} I_k$. Then this cost is $2PR$.\n    $$ C_{\\text{MTTKRP}, n} = 2R \\prod_{k=1}^{N} I_k $$\n\n2.  **Formation of Normal Matrix $H^{(n)}$**: The matrix $H^{(n)}$ is formed by the element-wise product of $N-1$ pre-computed Gram matrices $S^{(m)}$ for all $m \\neq n$. This requires $N-2$ Hadamard products of $R \\times R$ matrices. The cost is:\n    $$ C_{H, n} = (N-2)R^2 $$\n\n3.  **Solving the Linear System**: Updating $A^{(n)}$ requires solving the normal equations. This involves a Cholesky factorization of $H^{(n)}$ and two triangular solves.\n    $$ C_{\\text{solve}, n} = \\frac{1}{3}R^3 + 2I_n R^2 $$\n\n4.  **Gram Matrix Update**: After $A^{(n)}$ is updated, its Gram matrix $S^{(n)} = A^{(n)\\top}A^{(n)}$ is recomputed. The cost is:\n    $$ C_{S, n} = 2I_n R^2 $$\n\nThe total cost for updating mode $n$ is the sum of these components:\n$$ C_n = C_{\\text{MTTKRP}, n} + C_{H, n} + C_{\\text{solve}, n} + C_{S, n} $$\n$$ C_n = \\left(2R \\prod_{k=1}^{N} I_k\\right) + (N-2)R^2 + \\left(\\frac{1}{3}R^3 + 2I_n R^2\\right) + 2I_n R^2 $$\n$$ C_n = 2R \\prod_{k=1}^{N} I_k + \\frac{1}{3}R^3 + (N-2)R^2 + 4I_n R^2 $$\n\nTo find the total cost for one sweep, we sum $C_n$ over $n=1, \\dots, N$:\n$$ C_{\\text{total}} = \\sum_{n=1}^{N} C_n = \\sum_{n=1}^{N} \\left( 2R \\prod_{k=1}^{N} I_k + \\frac{1}{3}R^3 + (N-2)R^2 + 4I_n R^2 \\right) $$\n$$ C_{\\text{total}} = N \\left(2R \\prod_{k=1}^{N} I_k\\right) + N\\frac{1}{3}R^3 + N(N-2)R^2 + 4R^2\\sum_{n=1}^{N} I_n $$\nGrouping terms, the total flop count is:\n$$ C_{\\text{total}} = 2NR\\prod_{k=1}^{N} I_k + \\left(N(N-2) + 4\\sum_{k=1}^{N} I_k\\right)R^2 + \\frac{N}{3}R^3 $$\n\n### Part 2: Peak Memory Usage Derivation\n\nThe peak memory usage, $M_{\\text{peak}}$, is the total number of scalar words required to store all specified data structures, both permanent and temporary.\n\n1.  **Dense Tensor $\\mathcal{X}$**: Storage is $\\prod_{k=1}^{N} I_k$.\n2.  **Factor Matrices $\\{A^{(n)}\\}_{n=1}^{N}$**: Each $A^{(n)}$ is of size $I_n \\times R$. Total size is $\\sum_{n=1}^{N} I_n R = R\\sum_{n=1}^{N} I_n$.\n3.  **Gram Matrices $\\{S^{(n)}\\}_{n=1}^{N}$**: Each $S^{(n)}$ is of size $R \\times R$. Total size is $NR^2$.\n4.  **Temporary for MTTKRP $M^{(n)}$**: A buffer is needed for the $I_n \\times R$ matrix $M^{(n)}$. Its size must be large enough for the largest possible mode, which is $\\max_{k \\in \\{1,\\dots,N\\}} (I_k R) = R \\max_{k \\in \\{1,\\dots,N\\}} I_k$.\n5.  **Temporary for $H^{(n)}$**: A buffer of size $R \\times R$ is required, corresponding to $R^2$ scalars.\n\nThe total peak memory usage is the sum of these components:\n$$ M_{\\text{peak}} = \\left(\\prod_{k=1}^{N} I_k\\right) + \\left(R\\sum_{k=1}^{N} I_k\\right) + NR^2 + \\left(R \\max_{k \\in \\{1,\\dots,N\\}} I_k\\right) + R^2 $$\nCombining terms:\n$$ M_{\\text{peak}} = \\prod_{k=1}^{N} I_k + R\\left(\\sum_{k=1}^{N} I_k + \\max_{k \\in \\{1,\\dots,N\\}} I_k\\right) + (N+1)R^2 $$",
            "answer": "$$ \\boxed{\\pmatrix{ 2NR\\prod_{k=1}^{N} I_k + \\left(N(N-2) + 4\\sum_{k=1}^{N} I_k\\right)R^2 + \\frac{N}{3}R^3 & \\prod_{k=1}^{N} I_k + R\\left( \\sum_{k=1}^{N} I_k + \\max_{k \\in \\{1,\\dots,N\\}} I_k \\right) + (N+1)R^2 }} $$"
        }
    ]
}