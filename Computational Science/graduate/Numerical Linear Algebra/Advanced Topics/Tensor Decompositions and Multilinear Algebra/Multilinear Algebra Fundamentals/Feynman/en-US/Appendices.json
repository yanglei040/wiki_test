{
    "hands_on_practices": [
        {
            "introduction": "A core skill in multilinear algebra is confidently manipulating and tracking the dimensions of tensors as they undergo fundamental operations. This first practice focuses on two of the most essential transformations: the Tensor-Times-Matrix (TTM) product and mode-n unfolding (or matricization). By working through this exercise , you will solidify your understanding of how these operations alter a tensor's shape, a foundational step for building and analyzing more complex tensor algorithms.",
            "id": "3561343",
            "problem": "Let $X \\in \\mathbb{R}^{3 \\times 4 \\times 2}$ be a third-order tensor and let $U \\in \\mathbb{R}^{5 \\times 3}$ be a matrix. Define the mode-$1$ Tensor-Times-Matrix (TTM) product $Y = X \\times_{1} U$, where multiplication is applied along the first mode of $X$ by left-multiplying each mode-$1$ fiber with $U$. Adopt the Kolda–Bader convention for mode-$n$ unfolding (columns are mode-$n$ fibers), and denote by $Y_{(3)}$ the mode-$3$ unfolding of $Y$. Using only fundamental definitions of tensors, fibers, slices, mode-$n$ products, and mode-$n$ unfolding, determine the dimensions of $Y$ and the dimensions of $Y_{(3)}$. Express your final answer as a single row matrix listing, in order, the three dimensions of $Y$ followed by the two dimensions of $Y_{(3)}$. No rounding is required.",
            "solution": "The user wants me to solve the problem by following these steps:\n1.  First, validate the problem.\n2.  If it is valid, solve it.\n3.  If it is not valid, explain why.\n\nHere is the validation of the problem.\n\n### Step 1: Extract Givens\n- A third-order tensor $X \\in \\mathbb{R}^{3 \\times 4 \\times 2}$.\n- A matrix $U \\in \\mathbb{R}^{5 \\times 3}$.\n- The operation is the mode-$1$ Tensor-Times-Matrix (TTM) product, $Y = X \\times_{1} U$.\n- The multiplication is defined by left-multiplying each mode-$1$ fiber with $U$.\n- The Kolda–Bader convention is adopted for mode-$n$ unfolding, where columns are mode-$n$ fibers.\n- $Y_{(3)}$ denotes the mode-$3$ unfolding of $Y$.\n- The objective is to determine the dimensions of $Y$ and the dimensions of $Y_{(3)}$.\n- The answer format is a single row matrix containing the three dimensions of $Y$ followed by the two dimensions of $Y_{(3)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is checked against the validation criteria.\n- **Scientifically Grounded**: The problem is based on fundamental and well-defined concepts in multilinear algebra, namely tensors, the Tensor-Times-Matrix (TTM) product, and tensor unfolding (matricization). These are standard topics in numerical linear algebra and data analysis.\n- **Well-Posed**: The problem is well-posed. The dimensions of the tensor $X$ and the matrix $U$ are specified. The TTM product $X \\times_{1} U$ is well-defined because the size of the first mode of $X$, which is $3$, matches the number of columns of $U$, which is also $3$. The Kolda-Bader convention for unfolding is a standard, unambiguous definition. Therefore, a unique, meaningful solution exists.\n- **Objective**: The problem is stated in precise mathematical language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed:\n1.  **Scientific or Factual Unsoundness**: No violations of mathematical logic. The dimensional compatibility for the TTM product is satisfied.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formalizable question directly related to the specified topic of multilinear algebra fundamentals.\n3.  **Incomplete or Contradictory Setup**: All necessary information is provided.\n4.  **Unrealistic or Infeasible**: Not applicable as it is a purely mathematical problem.\n5.  **Ill-Posed or Poorly Structured**: The definitions are standard and unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: While the problem is an application of definitions, it requires a correct understanding of the concepts and is not trivial.\n7.  **Outside Scientific Verifiability**: Not applicable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n***\n\nThe problem requires us to determine the dimensions of a tensor $Y$ resulting from a Tensor-Times-Matrix (TTM) product, and then to find the dimensions of the mode-$3$ unfolding of $Y$.\n\nLet the tensor $X$ have dimensions $I_1 \\times I_2 \\times I_3$. From the problem statement, we have $X \\in \\mathbb{R}^{3 \\times 4 \\times 2}$, so $I_1 = 3$, $I_2 = 4$, and $I_3 = 2$.\nLet the matrix $U$ have dimensions $J \\times K$. From the problem statement, we have $U \\in \\mathbb{R}^{5 \\times 3}$, so $J = 5$ and $K = 3$.\n\n**Part 1: Determine the dimensions of $Y$**\n\nThe tensor $Y$ is defined by the mode-$1$ TTM product with matrix $U$:\n$$Y = X \\times_{1} U$$\nThe mode-$n$ product of a tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times \\dots \\times I_n \\times \\dots \\times I_N}$ with a matrix $U \\in \\mathbb{R}^{J \\times I_n}$ results in a tensor $\\mathcal{Y} \\in \\mathbb{R}^{I_1 \\times \\dots \\times I_{n-1} \\times J \\times I_{n+1} \\times \\dots \\times I_N}$. The operation replaces the dimension $I_n$ of the $n$-th mode with the dimension $J$ from the matrix $U$.\n\nIn our case, $n=1$. The tensor $X$ has dimensions $I_1 \\times I_2 \\times I_3$, and the matrix $U$ has dimensions $J \\times I_1$. The condition for the product to be well-defined is that the number of columns of $U$ must be equal to the size of the first mode of $X$. Here, the number of columns of $U$ is $K=3$, and the size of the first mode of $X$ is $I_1=3$. Since $K=I_1$, the operation is well-defined.\n\nThe resulting tensor $Y$ will have dimensions $J \\times I_2 \\times I_3$.\nSubstituting the given values:\n- The first dimension of $Y$ is $J = 5$.\n- The second dimension of $Y$ is $I_2 = 4$.\n- The third dimension of $Y$ is $I_3 = 2$.\n\nTherefore, the tensor $Y$ is in $\\mathbb{R}^{5 \\times 4 \\times 2}$, and its dimensions are $(5, 4, 2)$.\n\n**Part 2: Determine the dimensions of $Y_{(3)}$**\n\nThe notation $Y_{(3)}$ represents the mode-$3$ unfolding of the tensor $Y$. The tensor $Y \\in \\mathbb{R}^{5 \\times 4 \\times 2}$ has dimensions we can denote as $J_1 \\times J_2 \\times J_3$, where $J_1 = 5$, $J_2 = 4$, and $J_3 = 2$.\n\nThe mode-$n$ unfolding of a tensor $\\mathcal{Y} \\in \\mathbb{R}^{J_1 \\times J_2 \\times \\dots \\times J_N}$ is a matrix, denoted $\\mathcal{Y}_{(n)}$, which reshapes the tensor's elements. According to the Kolda–Bader convention, the columns of the matrix $\\mathcal{Y}_{(n)}$ are the mode-$n$ fibers of the tensor $\\mathcal{Y}$.\n\nA mode-$n$ fiber is a vector obtained by fixing all indices except the $n$-th index. For a third-order tensor $Y \\in \\mathbb{R}^{J_1 \\times J_2 \\times J_3}$, a mode-$3$ fiber is a column vector of length $J_3$, obtained by fixing the first two indices, i.e., $Y_{j_1, j_2, :}$ for some fixed $j_1$ and $j_2$.\n\nThe mode-$3$ unfolding $Y_{(3)}$ arranges these mode-$3$ fibers as its columns.\n- The number of rows of $Y_{(3)}$ is the length of each mode-$3$ fiber, which is the size of the $3$-rd mode of $Y$. This is $J_3 = 2$.\n- The number of columns of $Y_{(3)}$ is the total number of mode-$3$ fibers. This is determined by the number of possible combinations of the other indices, which is the product of the sizes of the other modes. In this case, it is $J_1 \\times J_2$.\n\nSo, the dimensions of the matrix $Y_{(3)}$ are $J_3 \\times (J_1 \\times J_2)$.\nSubstituting the dimensions of $Y$:\n- The number of rows is $J_3 = 2$.\n- The number of columns is $J_1 \\times J_2 = 5 \\times 4 = 20$.\n\nTherefore, the matrix $Y_{(3)}$ is in $\\mathbb{R}^{2 \\times 20}$, and its dimensions are $(2, 20)$.\n\n**Conclusion**\n\nThe three dimensions of the tensor $Y$ are $5, 4, 2$.\nThe two dimensions of the matrix $Y_{(3)}$ are $2, 20$.\nThe problem requests the final answer as a single row matrix listing these five numbers in order.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n5  4  2  2  20\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond basic definitions, a deeper understanding of tensors comes from appreciating the subtle structural information encoded in their various representations. This practice  explores the critical relationship between the ordering of a tensor's slices and the column structure of its matricization. By constructing two tensors that contain the same set of slices but in a different order, you will discover how their unfoldings are related by a specific column permutation, providing a key insight into how the unfolding process linearizes a tensor's structure in an order-dependent fashion.",
            "id": "3561306",
            "problem": "Let $X \\in \\mathbb{R}^{I \\times J \\times K}$ be a third-order tensor. A mode-$n$ fiber is obtained by fixing all indices except the $n$th; for example, a mode-$1$ fiber of $X$ is $X(:,j,k) \\in \\mathbb{R}^{I}$ for fixed $j$ and $k$. A mode-$3$ slice (also called a frontal slice) is the matrix $X(:,:,k) \\in \\mathbb{R}^{I \\times J}$ obtained by fixing the third index $k$. The mode-$1$ unfolding (also called matricization) $X_{(1)} \\in \\mathbb{R}^{I \\times JK}$ is defined by arranging all mode-$1$ fibers $X(:,j,k)$ as the columns of $X_{(1)}$ in lexicographic order on the pair $(j,k)$ where $j$ varies fastest: that is, the column order is $(j,k) = (1,1),(2,1),\\dots,(J,1),(1,2),(2,2),\\dots,(J,K)$.\n\nConsider the tensor $X \\in \\mathbb{R}^{2 \\times 3 \\times 2}$ with frontal slices\n$$\nX(:,:,1) \\;=\\; \\begin{pmatrix} 1  4  7 \\\\ 2  5  8 \\end{pmatrix}, \n\\qquad\nX(:,:,2) \\;=\\; \\begin{pmatrix} 3  6  9 \\\\ 0  1  2 \\end{pmatrix}.\n$$\nDefine $Y \\in \\mathbb{R}^{2 \\times 3 \\times 2}$ by reordering the mode-$3$ slices of $X$:\n$$\nY(:,:,1) \\;=\\; X(:,:,2), \\qquad Y(:,:,2) \\;=\\; X(:,:,1).\n$$\nThus $X$ and $Y$ have the same collection of mode-$3$ slices but in a different order along the third index.\n\n1. Using the lexicographic fiber-ordering rule specified above, explicitly write down the mode-$1$ unfoldings $X_{(1)}$ and $Y_{(1)}$.\n2. Show there exists a unique column permutation matrix $\\Pi \\in \\mathbb{R}^{JK \\times JK}$ such that $Y_{(1)} = X_{(1)} \\Pi$, and identify $\\Pi$ explicitly.\n3. Compute $\\det(\\Pi)$.\n\nExplain, within your derivation, how the fiber ordering in the mode-$1$ unfolding encodes implicit permutations that are not directly visible when one only compares the set of mode-$3$ slices. Express your final answer as a single real number with no units. No rounding is required.",
            "solution": "The problem requires us to analyze the effect of permuting the slices of a third-order tensor on its mode-$1$ unfolding. We are given a tensor $X \\in \\mathbb{R}^{I \\times J \\times K}$ with dimensions $I=2$, $J=3$, and $K=2$. Its mode-$3$ slices (frontal slices) are specified as:\n$$\nX(:,:,1) = \\begin{pmatrix} 1  4  7 \\\\ 2  5  8 \\end{pmatrix}, \\qquad X(:,:,2) = \\begin{pmatrix} 3  6  9 \\\\ 0  1  2 \\end{pmatrix}.\n$$\nA second tensor $Y \\in \\mathbb{R}^{2 \\times 3 \\times 2}$ is defined by reordering the mode-$3$ slices of $X$:\n$$\nY(:,:,1) = X(:,:,2) = \\begin{pmatrix} 3  6  9 \\\\ 0  1  2 \\end{pmatrix}, \\qquad Y(:,:,2) = X(:,:,1) = \\begin{pmatrix} 1  4  7 \\\\ 2  5  8 \\end{pmatrix}.\n$$\nThis means the elements of the tensors are related by $Y_{ijk} = X_{ij\\tau(k)}$, where $\\tau$ is a permutation on the third index $\\{1, 2\\}$, defined by $\\tau(1)=2$ and $\\tau(2)=1$. While the *set* of mode-$3$ slices $\\{X(:,:,1), X(:,:,2)\\}$ is identical to the set $\\{Y(:,:,1), Y(:,:,2)\\}$, the tensors $X$ and $Y$ are distinct because the slices appear in a different order.\n\nOur first task is to compute the mode-$1$ unfoldings $X_{(1)}$ and $Y_{(1)}$. The mode-$1$ unfolding, $X_{(1)}$, is a matrix of size $I \\times (JK)$, which is $2 \\times (3 \\times 2) = 2 \\times 6$. Its columns are the mode-$1$ fibers $X(:,j,k)$, arranged according to the lexicographic order of the index pair $(j,k)$, where the first index $j$ varies fastest. The order of the columns is determined by the index pairs $(1,1), (2,1), (3,1), (1,2), (2,2), (3,2)$.\n\nLet's extract the mode-$1$ fibers of $X$:\nThe first $J=3$ fibers are from the slice $k=1$:\n- Fiber for $(j,k)=(1,1)$: $X(:,1,1) = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$\n- Fiber for $(j,k)=(2,1)$: $X(:,2,1) = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}$\n- Fiber for $(j,k)=(3,1)$: $X(:,3,1) = \\begin{pmatrix} 7 \\\\ 8 \\end{pmatrix}$\nThe next $J=3$ fibers are from the slice $k=2$:\n- Fiber for $(j,k)=(1,2)$: $X(:,1,2) = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n- Fiber for $(j,k)=(2,2)$: $X(:,2,2) = \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix}$\n- Fiber for $(j,k)=(3,2)$: $X(:,3,2) = \\begin{pmatrix} 9 \\\\ 2 \\end{pmatrix}$\n\nAssembling these column vectors in the specified order gives the mode-$1$ unfolding of $X$:\n$$\nX_{(1)} = \\begin{pmatrix} 1  4  7  3  6  9 \\\\ 2  5  8  0  1  2 \\end{pmatrix}\n$$\n\nNext, we construct $Y_{(1)}$ using the same procedure. The mode-$1$ fibers of $Y$ are $Y(:,j,k)$. From the definition of $Y$, we have $Y(:,j,1) = X(:,j,2)$ and $Y(:,j,2) = X(:,j,1)$.\nThe first $J=3$ fibers for $Y_{(1)}$ are from its slice $k=1$:\n- Fiber for $(j,k)=(1,1)$: $Y(:,1,1) = X(:,1,2) = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n- Fiber for $(j,k)=(2,1)$: $Y(:,2,1) = X(:,2,2) = \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix}$\n- Fiber for $(j,k)=(3,1)$: $Y(:,3,1) = X(:,3,2) = \\begin{pmatrix} 9 \\\\ 2 \\end{pmatrix}$\nThe next $J=3$ fibers for $Y_{(1)}$ are from its slice $k=2$:\n- Fiber for $(j,k)=(1,2)$: $Y(:,1,2) = X(:,1,1) = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$\n- Fiber for $(j,k)=(2,2)$: $Y(:,2,2) = X(:,2,1) = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}$\n- Fiber for $(j,k)=(3,2)$: $Y(:,3,2) = X(:,3,1) = \\begin{pmatrix} 7 \\\\ 8 \\end{pmatrix}$\n\nAssembling these fibers gives the mode-$1$ unfolding of $Y$:\n$$\nY_{(1)} = \\begin{pmatrix} 3  6  9  1  4  7 \\\\ 0  1  2  2  5  8 \\end{pmatrix}\n$$\n\nThe second task is to find a unique column permutation matrix $\\Pi \\in \\mathbb{R}^{JK \\times JK} = \\mathbb{R}^{6 \\times 6}$ such that $Y_{(1)} = X_{(1)} \\Pi$.\nBy comparing the columns of $Y_{(1)}$ with the columns of $X_{(1)}$, we can identify the permutation. Let $x_p$ and $y_q$ be the $p$-th and $q$-th columns of $X_{(1)}$ and $Y_{(1)}$ respectively.\n- $y_1 = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = x_4$\n- $y_2 = \\begin{pmatrix} 6 \\\\ 1 \\end{pmatrix} = x_5$\n- $y_3 = \\begin{pmatrix} 9 \\\\ 2 \\end{pmatrix} = x_6$\n- $y_4 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = x_1$\n- $y_5 = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix} = x_2$\n- $y_6 = \\begin{pmatrix} 7 \\\\ 8 \\end{pmatrix} = x_3$\n\nThis shows that the columns of $Y_{(1)}$ are a permutation of the columns of $X_{(1)}$. The first block of $J=3$ columns of $X_{(1)}$ (corresponding to slice $X(:,:,1)$) has moved to become the second block of columns in $Y_{(1)}$, and the second block of $J=3$ columns of $X_{(1)}$ (from slice $X(:,:,2)$) has become the first block in $Y_{(1)}$. This column shuffling is the \"implicit permutation\" that is not apparent when only considering the set of slices. The rigid fiber ordering rule of the unfolding process makes the operation sensitive to the slice ordering along modes not being unfolded.\n\nThe permutation of column indices $\\sigma: \\{1, ..., 6\\} \\to \\{1, ..., 6\\}$ is given by the mapping from the new position to the old position: $y_q = x_{\\sigma(q)}$.\n$\\sigma(1) = 4, \\sigma(2) = 5, \\sigma(3) = 6, \\sigma(4) = 1, \\sigma(5) = 2, \\sigma(6) = 3$.\nThe permutation matrix $\\Pi$ is constructed such that right-multiplication by $\\Pi$ permutes the columns of a matrix according to $\\sigma$. This means the $q$-th column of $\\Pi$ is the standard basis vector $e_{\\sigma(q)}$.\n$$\n\\Pi = \\begin{pmatrix} e_{\\sigma(1)}  e_{\\sigma(2)}  e_{\\sigma(3)}  e_{\\sigma(4)}  e_{\\sigma(5)}  e_{\\sigma(6)} \\end{pmatrix} = \\begin{pmatrix} e_4  e_5  e_6  e_1  e_2  e_3 \\end{pmatrix}\n$$\nExplicitly, this gives the $6 \\times 6$ matrix:\n$$\n\\Pi = \\begin{pmatrix}\n0  0  0  1  0  0 \\\\\n0  0  0  0  1  0 \\\\\n0  0  0  0  0  1 \\\\\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  1  0  0  0\n\\end{pmatrix}\n$$\nThis matrix has a block structure $\\Pi = \\begin{pmatrix} 0_{3 \\times 3}  I_{3 \\times 3} \\\\ I_{3 \\times 3}  0_{3 \\times 3} \\end{pmatrix}$, where $I_{3 \\times 3}$ is the $3 \\times 3$ identity matrix and $0_{3 \\times 3}$ is the $3 \\times 3$ zero matrix. This structure directly reflects the swapping of the two blocks of $J=3$ columns. Since all columns of $X_{(1)}$ are distinct, this permutation matrix is unique.\n\nThe third task is to compute the determinant of $\\Pi$. The determinant of a permutation matrix is equal to the sign of the corresponding permutation, $\\operatorname{sgn}(\\sigma)$. The permutation $\\sigma$ can be decomposed into disjoint cycles: $\\sigma = (1 \\ 4)(2 \\ 5)(3 \\ 6)$. This is a product of $3$ transpositions (swaps). The sign of the permutation is $(-1)^k$, where $k$ is the number of transpositions.\nThus, $\\operatorname{sgn}(\\sigma) = (-1)^3 = -1$.\nTherefore, the determinant of the permutation matrix is:\n$$\n\\det(\\Pi) = -1\n$$\nAlternatively, for a block matrix of the form $M = \\begin{pmatrix} 0  B \\\\ C  0 \\end{pmatrix}$, where $B$ and $C$ are square matrices of size $n \\times n$, its determinant is $\\det(M) = (-1)^{n^2} \\det(B) \\det(C)$. In our case, $B=I_{3 \\times 3}$ and $C=I_{3 \\times 3}$, so $n=3$.\n$$\n\\det(\\Pi) = (-1)^{3 \\times 3} \\det(I_{3 \\times 3}) \\det(I_{3 \\times 3}) = (-1)^9 \\cdot 1 \\cdot 1 = -1\n$$\nBoth methods yield the same result. The final answer is the value of the determinant.",
            "answer": "$$\\boxed{-1}$$"
        },
        {
            "introduction": "The abstract concepts of multilinear algebra, such as tensor decompositions and their relationship with unfoldings, find their true power in the design of efficient numerical algorithms. This final practice  bridges theory and computation, challenging you to exploit a tensor's low-rank Canonical Polyadic (CP) structure to accelerate a fundamental linear algebra operation. By implementing both a naive method and a structure-aware algorithm, you will gain practical experience with the computational advantages that make tensor methods indispensable in modern data science and scientific computing.",
            "id": "3561330",
            "problem": "Consider an order-$3$ tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}$ constructed to have low matrix rank in its mode-$1$ unfolding and full matrix rank in the other unfoldings. Use only core definitions from multilinear algebra and numerical linear algebra. Definitions: A tensor fiber is a vector obtained by fixing all indices except one. A slice is a matrix obtained by fixing one index and letting two vary. The mode-$n$ unfolding (also called matricization) of a tensor $\\mathcal{X}$, denoted $\\mathbf{X}_{(n)}$, arranges fibers along mode $n$ as columns of a matrix. The column-wise Khatri–Rao product is defined for matrices $\\mathbf{B} \\in \\mathbb{R}^{I_2 \\times r}$ and $\\mathbf{C} \\in \\mathbb{R}^{I_3 \\times r}$ with the same number of columns $r$ as the matrix $\\mathbf{C} \\odot \\mathbf{B} \\in \\mathbb{R}^{(I_2 I_3) \\times r}$ whose $k$-th column equals the Kronecker product $\\mathbf{b}_k \\otimes \\mathbf{c}_k$, where $\\mathbf{b}_k$ and $\\mathbf{c}_k$ are the $k$-th columns of $\\mathbf{B}$ and $\\mathbf{C}$. The Canonical Polyadic (CP) decomposition writes a tensor as a sum of rank-$1$ outer products: $\\mathcal{X} = \\sum_{k=1}^{r} \\mathbf{a}_k \\circ \\mathbf{b}_k \\circ \\mathbf{c}_k$, with factor matrices $\\mathbf{A} = [\\mathbf{a}_1 \\ \\ldots \\ \\mathbf{a}_r] \\in \\mathbb{R}^{I_1 \\times r}$, $\\mathbf{B} = [\\mathbf{b}_1 \\ \\ldots \\ \\mathbf{b}_r] \\in \\mathbb{R}^{I_2 \\times r}$, and $\\mathbf{C} = [\\mathbf{c}_1 \\ \\ldots \\ \\mathbf{c}_r] \\in \\mathbb{R}^{I_3 \\times r}$.\n\nTask: For each parameter set $(I_1, I_2, I_3, r)$ in the test suite below, construct deterministic factor matrices $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$ such that $\\mathbf{A}$ has full column rank $r$ and $\\mathbf{B}$, $\\mathbf{C}$ have full row rank $I_2$ and $I_3$ respectively, with $r  I_1$ and $r \\ge I_2$, $r \\ge I_3$. Define the tensor $\\mathcal{X}$ using the CP decomposition with these factors. Let $\\mathbf{w} \\in \\mathbb{R}^{I_2 I_3}$ be a deterministic vector. Consider the linear operator induced by the mode-$1$ unfolding $\\mathbf{X}_{(1)} \\in \\mathbb{R}^{I_1 \\times (I_2 I_3)}$ acting on $\\mathbf{w}$ to produce $\\mathbf{y} = \\mathbf{X}_{(1)} \\mathbf{w} \\in \\mathbb{R}^{I_1}$.\n\nImplement two algorithms to compute $\\mathbf{y}$:\n- Baseline algorithm: form $\\mathbf{X}_{(1)}$ explicitly from $\\mathcal{X}$ and compute $\\mathbf{y} = \\mathbf{X}_{(1)} \\mathbf{w}$ by dense matrix-vector multiplication.\n- Structure-exploiting algorithm: use only the factor matrices and core definitions to compute $\\mathbf{y}$, avoiding explicit construction of $\\mathbf{X}_{(1)}$ or $\\mathcal{X}$ when possible. Base this algorithm on the CP structure and the relation between unfolding and the column-wise Khatri–Rao product that follows from the definitions, and compute $\\mathbf{y}$ by contracting along modes $2$ and $3$ first.\n\nFor each test case, compute the numerical ranks of $\\mathbf{X}_{(1)}$, $\\mathbf{X}_{(2)}$, and $\\mathbf{X}_{(3)}$ using singular value decomposition and a standard tolerance rule. Verify that $\\operatorname{rank}(\\mathbf{X}_{(1)}) = r$, $\\operatorname{rank}(\\mathbf{X}_{(2)}) = I_2$, and $\\operatorname{rank}(\\mathbf{X}_{(3)}) = I_3$.\n\nCompute the scalar operation counts (sum of multiplications and additions) for both algorithms under the following assumptions:\n- Baseline: computing $\\mathbf{y} = \\mathbf{X}_{(1)} \\mathbf{w}$ by dense matrix-vector multiplication costs $I_1 (I_2 I_3)$ multiplications and $I_1 (I_2 I_3 - 1)$ additions.\n- Structure-exploiting: first reshape $\\mathbf{w}$ into a matrix $\\mathbf{W} \\in \\mathbb{R}^{I_2 \\times I_3}$ and for each $k \\in \\{1, \\ldots, r\\}$ compute $\\mathbf{t}_k = \\mathbf{W} \\mathbf{c}_k$ (costs $I_2 I_3$ multiplications and $I_2(I_3 - 1)$ additions), then $s_k = \\mathbf{b}_k^\\top \\mathbf{t}_k$ (costs $I_2$ multiplications and $I_2 - 1$ additions), and finally accumulate $\\mathbf{y} \\leftarrow \\mathbf{y} + s_k \\mathbf{a}_k$ (costs $I_1$ multiplications and $I_1$ additions). Sum these costs over $k = 1, \\ldots, r$ to obtain the total count.\n\nYour program must, for each test case, return a list of four items: \n$[\\text{eq}_{\\text{baseline,struct}}, \\text{eq}_{\\text{baseline,KR}}, \\text{ranks\\_ok}, \\rho]$, where $\\text{eq}_{\\text{baseline,struct}}$ and $\\text{eq}_{\\text{baseline,KR}}$ are boolean values indicating whether the baseline and the structure-exploiting outputs agree within absolute tolerance $10^{-12}$ and relative tolerance $10^{-10}$, $\\text{ranks\\_ok}$ is a boolean indicating whether the rank conditions hold, and $\\rho$ is the float ratio of baseline scalar operation count divided by the structure-exploiting count.\n\nTest suite parameter values:\n- Case $1$: $(I_1, I_2, I_3, r) = (50, 10, 12, 12)$.\n- Case $2$: $(I_1, I_2, I_3, r) = (40, 8, 8, 8)$.\n- Case $3$: $(I_1, I_2, I_3, r) = (60, 6, 10, 10)$.\n- Case $4$: $(I_1, I_2, I_3, r) = (100, 9, 7, 9)$.\n- Case $5$: $(I_1, I_2, I_3, r) = (30, 1, 20, 20)$.\n\nFinal output format: Your program should produce a single line of output containing the results aggregated over all test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list as described above. For example, the output should look like $[[\\text{True}, \\text{True}, \\text{True}, 3.14], \\ldots]$.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of multilinear algebra, well-posed, and objective. It provides a complete and consistent set of definitions, constraints, and tasks.\n\nThe objective is to compute the vector $\\mathbf{y} \\in \\mathbb{R}^{I_1}$ resulting from the product of the mode-$1$ unfolding of a tensor, $\\mathbf{X}_{(1)} \\in \\mathbb{R}^{I_1 \\times (I_2 I_3)}$, and a vector $\\mathbf{w} \\in \\mathbb{R}^{I_2 I_3}$. The tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}$ is defined by its Canonical Polyadic (CP) decomposition with a specified rank structure. We will implement two algorithms for this computation, verify the rank properties of the tensor unfoldings, and analyze the computational cost savings.\n\nFirst, we construct the deterministic factor matrices $\\mathbf{A} \\in \\mathbb{R}^{I_1 \\times r}$, $\\mathbf{B} \\in \\mathbb{R}^{I_2 \\times r}$, and $\\mathbf{C} \\in \\mathbb{R}^{I_3 \\times r}$ as per the problem specifications. For determinism and the required rank properties, we use a seeded pseudo-random number generator to populate the matrices with values from a continuous uniform distribution. The constraints are $r  I_1$, $r \\ge I_2$, and $r \\ge I_3$.\n- $\\mathbf{A}$ is an $I_1 \\times r$ matrix. Since $I_1  r$, a random matrix will have full column rank $r$ with probability $1$.\n- $\\mathbf{B}$ is an $I_2 \\times r$ matrix. Since $I_2 \\le r$, a random matrix will have full row rank $I_2$ with probability $1$.\n- $\\mathbf{C}$ is an $I_3 \\times r$ matrix. Since $I_3 \\le r$, a random matrix will have full row rank $I_3$ with probability $1$.\nA deterministic vector $\\mathbf{w} \\in \\mathbb{R}^{I_2 I_3}$ is also generated similarly.\n\nThe tensor $\\mathcal{X}$ is given by its CP decomposition:\n$$\n\\mathcal{X} = \\sum_{k=1}^{r} \\mathbf{a}_k \\circ \\mathbf{b}_k \\circ \\mathbf{c}_k\n$$\nIn element-wise form, this is $\\mathcal{X}_{i_1, i_2, i_3} = \\sum_{k=1}^{r} A_{i_1, k} B_{i_2, k} C_{i_3, k}$.\n\nThe core task is to compute $\\mathbf{y} = \\mathbf{X}_{(1)} \\mathbf{w}$. There is ambiguity in how a tensor's indices are mapped to the columns of its unfolding. We adopt a consistent convention based on C-style (row-major) memory layout, as implied by the definition of the structure-exploiting algorithm and standard numerical library functions. An element $\\mathcal{X}_{i_1, i_2, i_3}$ (using $0$-based indexing) maps to the element $(\\mathbf{X}_{(1)})_{i_1, j}$ where $j = i_2 \\cdot I_3 + i_3$.\n\n**Algorithm 1: Baseline**\nThis approach involves the explicit construction of the tensor and its unfolding.\n1.  Construct the full tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}$ from its factor matrices $\\mathbf{A}$, $\\mathbf{B}$, $\\mathbf{C}$. This can be computed efficiently using Einstein summation notation: $\\mathcal{X}_{i_1, i_2, i_3} = \\sum_k A_{i_1,k} B_{i_2,k} C_{i_3,k}$.\n2.  Form the mode-$1$ unfolding $\\mathbf{X}_{(1)} \\in \\mathbb{R}^{I_1 \\times (I_2 I_3)}$ by reshaping $\\mathcal{X}$. Following our C-style convention, this is a direct reshape of the $I_1 \\times I_2 \\times I_3$ array into an $I_1 \\times (I_2 I_3)$ matrix.\n3.  Compute the matrix-vector product $\\mathbf{y}_{\\text{baseline}} = \\mathbf{X}_{(1)} \\mathbf{w}$.\n\n**Algorithm 2: Structure-Exploiting**\nThis method leverages the CP structure to avoid forming the large tensor $\\mathcal{X}$ or its unfolding $\\mathbf{X}_{(1)}$. We start with the definition of the product in terms of tensor elements:\n$$\ny_{i_1} = \\sum_{i_2=1}^{I_2} \\sum_{i_3=1}^{I_3} \\mathcal{X}_{i_1, i_2, i_3} W_{i_2, i_3}\n$$\nwhere $\\mathbf{W} \\in \\mathbb{R}^{I_2 \\times I_3}$ is the matrix obtained by reshaping the vector $\\mathbf{w}$. Substituting the CP decomposition of $\\mathcal{X}$:\n$$\ny_{i_1} = \\sum_{i_2=1}^{I_2} \\sum_{i_3=1}^{I_3} \\left( \\sum_{k=1}^{r} A_{i_1, k} B_{i_2, k} C_{i_3, k} \\right) W_{i_2, i_3}\n$$\nBy reordering the summations, we get:\n$$\ny_{i_1} = \\sum_{k=1}^{r} A_{i_1, k} \\left( \\sum_{i_2=1}^{I_2} B_{i_2, k} \\left( \\sum_{i_3=1}^{I_3} W_{i_2, i_3} C_{i_3, k} \\right) \\right)\n$$\nThis expression defines a sequence of efficient operations as prescribed by the problem:\n1.  For each $k \\in \\{1, \\ldots, r\\}$, compute the vector $\\mathbf{t}_k = \\mathbf{W} \\mathbf{c}_k \\in \\mathbb{R}^{I_2}$. This corresponds to the inner sum.\n2.  For each $k$, compute the scalar $s_k = \\mathbf{b}_k^\\top \\mathbf{t}_k$. This is the middle summation.\n3.  The final result is the linear combination of the columns of $\\mathbf{A}$: $\\mathbf{y}_{\\text{struct}} = \\sum_{k=1}^{r} s_k \\mathbf{a}_k = \\mathbf{A}\\mathbf{s}$, where $\\mathbf{s} = [s_1, \\ldots, s_r]^\\top$.\n\nThis procedure can be vectorized for efficiency. Let $\\mathbf{T} = \\mathbf{W}\\mathbf{C} \\in \\mathbb{R}^{I_2 \\times r}$. The columns of $\\mathbf{T}$ are the vectors $\\mathbf{t}_k$. The vector $\\mathbf{s}$ can be computed as the sum of the element-wise product of $\\mathbf{B}$ and $\\mathbf{T}$ along the first dimension. Then $\\mathbf{y}_{\\text{struct}} = \\mathbf{A}\\mathbf{s}$.\n\nThis algorithm is an efficient implementation of the mathematical identity $\\mathbf{X}_{(1)} = \\mathbf{A} (\\mathbf{C} \\odot \\mathbf{B})^\\top$, where $\\odot$ denotes the column-wise Khatri-Rao product. The product $\\mathbf{y} = \\mathbf{X}_{(1)}\\mathbf{w}$ becomes $\\mathbf{y} = \\mathbf{A} ((\\mathbf{C} \\odot \\mathbf{B})^\\top \\mathbf{w})$. The structured algorithm calculates the term $(\\mathbf{C} \\odot \\mathbf{B})^\\top \\mathbf{w}$ (which is our vector $\\mathbf{s}$) without explicitly forming the large $(I_2 I_3) \\times r$ Khatri-Rao matrix. The problem asks for two boolean comparisons, $\\text{eq}_{\\text{baseline,struct}}$ and $\\text{eq}_{\\text{baseline,KR}}$. Since the structured method is the efficient evaluation of the Khatri-Rao based formula, we compute a single $\\mathbf{y}_{\\text{struct}}$ and use it for both comparisons against $\\mathbf{y}_{\\text{baseline}}$.\n\n**Rank Verification**\nThe numerical ranks of the three unfoldings, $\\mathbf{X}_{(1)}$, $\\mathbf{X}_{(2)}$, and $\\mathbf{X}_{(3)}$, are computed using Singular Value Decomposition (SVD). The rank is the number of singular values above a standard tolerance.\n-   $\\mathbf{X}_{(1)} \\in \\mathbb{R}^{I_1 \\times (I_2 I_3)}$. Expected rank: $\\operatorname{rank}(\\mathbf{X}_{(1)}) = r$. This is because $\\mathbf{A}$ has rank $r$ and $(\\mathbf{C} \\odot \\mathbf{B})^\\top$ has rank $r$, and the product of full-rank matrices preserves the smaller rank.\n-   $\\mathbf{X}_{(2)} \\in \\mathbb{R}^{I_2 \\times (I_1 I_3)}$. Formed by permuting the tensor axes to $(1, 0, 2)$ and reshaping. Expected rank: $\\operatorname{rank}(\\mathbf{X}_{(2)}) = I_2$, as $\\mathbf{B}$ has full row rank $I_2$ and $I_2 \\le r$.\n-   $\\mathbf{X}_{(3)} \\in \\mathbb{R}^{I_3 \\times (I_1 I_2)}$. Formed by permuting axes to $(2, 0, 1)$ and reshaping. Expected rank: $\\operatorname{rank}(\\mathbf{X}_{(3)}) = I_3$, as $\\mathbf{C}$ has full row rank $I_3$ and $I_3 \\le r$.\nThe boolean $\\text{ranks\\_ok}$ is true if all three rank conditions hold.\n\n**Operation Count Ratio**\nThe ratio $\\rho$ is computed using the operation counts (additions + multiplications) provided in the problem statement.\n-   Baseline cost: $C_{\\text{baseline}} = I_1 (I_2 I_3) + I_1 (I_2 I_3 - 1) = 2 I_1 I_2 I_3 - I_1$.\n-   Structure-exploiting cost: $C_{\\text{struct}} = \\sum_{k=1}^{r} (\\text{costs for } \\mathbf{t}_k, s_k, \\text{and accumulation})$.\n    -   $\\mathbf{t}_k = \\mathbf{W}\\mathbf{c}_k$: $I_2 I_3$ mults, $I_2(I_3-1)$ adds.\n    -   $s_k = \\mathbf{b}_k^\\top\\mathbf{t}_k$: $I_2$ mults, $I_2-1$ adds.\n    -   $\\mathbf{y} \\leftarrow \\mathbf{y} + s_k \\mathbf{a}_k$: $I_1$ mults, $I_1$ adds.\n    -   $C_{\\text{struct}} = r \\left( (I_2 I_3 + I_2 + I_1) + (I_2(I_3-1) + (I_2-1) + I_1) \\right) = r(2 I_2 I_3 + I_2 + 2 I_1 - 1)$.\n-   Ratio: $\\rho = C_{\\text{baseline}} / C_{\\text{struct}}$. This ratio is expected to be significantly greater than $1$, demonstrating the efficiency of exploiting the tensor's low-rank structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the tensor computation problem for a suite of test cases.\n    \"\"\"\n    # Test cases: (I1, I2, I3, r)\n    test_cases = [\n        (50, 10, 12, 12),\n        (40, 8, 8, 8),\n        (60, 6, 10, 10),\n        (100, 9, 7, 9),\n        (30, 1, 20, 20)\n    ]\n    \n    # Tolerances for floating point comparisons\n    tol_abs = 1e-12\n    tol_rel = 1e-10\n\n    # Seed for deterministic random number generation\n    rng = np.random.default_rng(0)\n\n    results_all_cases = []\n\n    for case in test_cases:\n        I1, I2, I3, r = case\n\n        # 1. Deterministic construction of factor matrices and vector w\n        # A should have full column rank r (since r  I1)\n        A = rng.random((I1, r))\n        # B should have full row rank I2 (since r = I2)\n        B = rng.random((I2, r))\n        # C should have full row rank I3 (since r = I3)\n        C = rng.random((I3, r))\n        \n        # Deterministic vector w\n        w = rng.random(I2 * I3)\n\n        # --- Baseline Algorithm ---\n        # Explicitly construct the tensor X using the CP decomposition.\n        # X_{i,j,l} = sum_k A_{i,k} * B_{j,k} * C_{l,k}\n        X = np.einsum('ik,jk,lk-ijl', A, B, C)\n        \n        # Unfold X to X_(1) using C-style (row-major) ordering.\n        # This maps X(i1, i2, i3) to X1(i1, i2*I3 + i3).\n        X1 = X.reshape(I1, -1)\n        \n        # Compute y using dense matrix-vector multiplication.\n        y_baseline = X1 @ w\n\n        # --- Structure-Exploiting Algorithm ---\n        # Reshape w into a matrix W.\n        W = w.reshape(I2, I3)\n        \n        # Vectorized implementation of the structured computation:\n        # y = A @ s, where s_k = b_k^T @ (W @ c_k)\n        T = W @ C           # T has shape (I2, r), columns are t_k\n        s = np.sum(B * T, axis=0) # s has shape (r,), elements are s_k\n        y_struct = A @ s    # y has shape (I1,)\n\n        # --- Comparisons and Checks ---\n        # Compare results from baseline and structured algorithms.\n        # As reasoned in the solution, the \"KR\" method is an algebraic\n        # view of the structured method. So we perform the same comparison twice.\n        eq_baseline_struct = np.allclose(y_baseline, y_struct, rtol=tol_rel, atol=tol_abs)\n        eq_baseline_KR = eq_baseline_struct\n\n        # --- Rank Verification ---\n        # Unfold X along all three modes using C-style ordering.\n        # X1 is already computed.\n        # Mode-2 unfolding:\n        X2 = np.transpose(X, (1, 0, 2)).reshape(I2, -1)\n        # Mode-3 unfolding:\n        X3 = np.transpose(X, (2, 0, 1)).reshape(I3, -1)\n\n        # Compute numerical ranks.\n        rank1 = np.linalg.matrix_rank(X1)\n        rank2 = np.linalg.matrix_rank(X2)\n        rank3 = np.linalg.matrix_rank(X3)\n        \n        # Check if rank conditions hold.\n        ranks_ok = (rank1 == r) and (rank2 == I2) and (rank3 == I3)\n\n        # --- Operation Count Ratio ---\n        # Use the formulas provided in the problem description.\n        cost_baseline = float(2 * I1 * I2 * I3 - I1)\n        \n        cost_struct_mult = float(r * (I2 * I3 + I2 + I1))\n        cost_struct_add = float(r * (I2 * (I3 - 1) + (I2 - 1) + I1))\n        cost_struct = cost_struct_mult + cost_struct_add\n        \n        rho = cost_baseline / cost_struct\n\n        # Append results for the current case.\n        results_all_cases.append([eq_baseline_struct, eq_baseline_KR, ranks_ok, rho])\n\n    # Format the output as a single string.\n    output_str = \"[\"\n    for i, res in enumerate(results_all_cases):\n        # Format: [bool, bool, bool, float]\n        output_str += f\"[{res[0]}, {res[1]}, {res[2]}, {res[3]:.14f}]\" # Using .14f to avoid scientific notation\n        if i  len(results_all_cases) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Capitalize booleans for final output per example format\n    output_str = output_str.replace('True', 'True').replace('False', 'False')\n\n    print(output_str)\n\n\nsolve()\n```"
        }
    ]
}