{
    "hands_on_practices": [
        {
            "introduction": "Understanding the theoretical differences between sparse matrix formats is one thing; quantifying their impact on performance is another. This exercise guides you through building a simple but powerful performance model to predict the runtime of a Sparse Matrix-Vector multiplication (SpMV) kernel. By analyzing the memory traffic associated with the Compressed Sparse Row (CSR) and Coordinate (COO) formats, you will gain a concrete understanding of why CSR is often preferred in high-performance applications and how performance is frequently bound by memory bandwidth rather than computational speed.",
            "id": "3271435",
            "problem": "You are asked to design and implement a program that predicts the performance of sparse matrix-vector multiplication for two common sparse storage formats: Compressed Sparse Row (CSR) and Coordinate List (COO). The task focuses on foundational scientific computing principles, modeling the computational cost and memory traffic, and applying a performance model to estimate runtime. Your program must compute predicted runtimes for a given test suite and output the results in a single specified format.\n\nStart from the following fundamental bases:\n\n- Sparse matrix-vector multiplication computes $y = A x$ where $A$ is sparse. For each nonzero entry $a_{ij}$, the computation involves one multiplication and one addition, which is a total of $2$ floating point operations per nonzero.\n- In a simple performance model aligned with the roofline approach, the total time $T$ is estimated by\n$$\nT = \\max\\left(\\frac{\\text{FLOPs}}{P_{\\mathrm{peak}}}, \\frac{\\text{Bytes}}{B_{\\mathrm{peak}}}\\right),\n$$\nwhere $\\text{FLOPs}$ is the number of floating point operations, $P_{\\mathrm{peak}}$ is the peak floating point throughput in $\\mathrm{FLOP/s}$, $\\text{Bytes}$ is the total data traffic in bytes, and $B_{\\mathrm{peak}}$ is the peak memory bandwidth in $\\mathrm{byte/s}$.\n- For CSR and COO formats, under a streaming and unsorted-COO accumulation assumption (no reuse of vector entries due to caching and COO accumulates directly into $y$ for each nonzero), the total bytes moved can be derived from the algorithmic loops and data structures:\n  - CSR format stores arrays for values, column indices, and a row pointer. The implementation traverses rows, accumulating $y_i$ in registers and writing once per row. The bytes moved are modeled as\n  $$\n  \\text{Bytes}_{\\mathrm{CSR}} = \\underbrace{n n z \\cdot v_{\\mathrm{bytes}}}_{\\text{values}} + \\underbrace{n n z \\cdot i_{\\mathrm{bytes}}}_{\\text{column indices}} + \\underbrace{(n+1) \\cdot i_{\\mathrm{bytes}}}_{\\text{row pointer}} + \\underbrace{n n z \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } x} + \\underbrace{n \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } y \\text{ stores}}.\n  $$\n  - COO format stores arrays for values, row indices, and column indices. The implementation accumulates directly into $y$, reading and writing $y$ for every nonzero. The bytes moved are modeled as\n  $$\n  \\text{Bytes}_{\\mathrm{COO}} = \\underbrace{n n z \\cdot v_{\\mathrm{bytes}}}_{\\text{values}} + \\underbrace{n n z \\cdot i_{\\mathrm{bytes}}}_{\\text{column indices}} + \\underbrace{n n z \\cdot i_{\\mathrm{bytes}}}_{\\text{row indices}} + \\underbrace{n n z \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } x} + \\underbrace{2 \\cdot n n z \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } y \\text{ read and write}}.\n  $$\n- The floating point operations for $y = A x$ are\n$$\n\\text{FLOPs} = 2 \\cdot n n z.\n$$\n\nImplement the above model in code and compute the predicted runtime in seconds for both CSR and COO for each test case, assuming double precision values unless specified.\n\nUse the following hardware parameters for all test cases:\n- Peak floating point throughput $P_{\\mathrm{peak}} = 100 \\times 10^9$ $\\mathrm{FLOP/s}$.\n- Peak memory bandwidth $B_{\\mathrm{peak}} = 50 \\times 10^9$ $\\mathrm{byte/s}$.\n\nYour program must process the following test suite, where each test case is a tuple $(n, m, n n z, i_{\\mathrm{bytes}}, v_{\\mathrm{bytes}})$:\n- Test $1$: $(1000, 1000, 50000, 4, 8)$.\n- Test $2$: $(5, 5, 7, 8, 8)$.\n- Test $3$: $(10000, 10000, 10000, 4, 8)$.\n- Test $4$: $(20000, 2000, 80000, 4, 8)$.\n- Test $5$: $(3000, 3000, 600000, 8, 8)$.\n\nFor each test case:\n- Compute $\\text{FLOPs}$.\n- Compute $\\text{Bytes}_{\\mathrm{CSR}}$ and $\\text{Bytes}_{\\mathrm{COO}}$.\n- Compute $T_{\\mathrm{CSR}} = \\max\\left(\\frac{\\text{FLOPs}}{P_{\\mathrm{peak}}}, \\frac{\\text{Bytes}_{\\mathrm{CSR}}}{B_{\\mathrm{peak}}}\\right)$ and $T_{\\mathrm{COO}} = \\max\\left(\\frac{\\text{FLOPs}}{P_{\\mathrm{peak}}}, \\frac{\\text{Bytes}_{\\mathrm{COO}}}{B_{\\mathrm{peak}}}\\right)$.\n\nScientific realism constraints:\n- All values $n$, $m$, and $n n z$ are strictly positive integers.\n- Index sizes $i_{\\mathrm{bytes}}$ are either $4$ or $8$ bytes.\n- Value sizes $v_{\\mathrm{bytes}}$ are either $4$ or $8$ bytes; if $v_{\\mathrm{bytes}} = 8$, interpret the matrix as double precision.\n\nAngle units are not applicable. No physical units beyond seconds are used. Express all runtimes in seconds as floating point numbers rounded to six decimal places.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with one pair per test case in order, where each pair is $[T_{\\mathrm{CSR}}, T_{\\mathrm{COO}}]$.\n- For example, the output must look like\n$$\n\\text{[[t_{1,\\mathrm{CSR}},t_{1,\\mathrm{COO}}],[t_{2,\\mathrm{CSR}},t_{2,\\mathrm{COO}}],\\dots]}\n$$\nwith each $t$ given in seconds, rounded to six decimal places.",
            "solution": "The problem requires the formulation and implementation of a performance model to predict the runtime of sparse matrix-vector multiplication (SpMV) for two distinct sparse matrix storage formats: Compressed Sparse Row (CSR) and Coordinate List (COO). The analysis is grounded in a simplified roofline model, a fundamental concept in high-performance computing for estimating the performance bounds of an algorithm based on the hardware's peak floating-point throughput and memory bandwidth.\n\nThe core of the performance model is the estimation of the total execution time, $T$, as the maximum of the time required for computation, $T_{\\text{compute}}$, and the time required for memory access, $T_{\\text{memory}}$:\n$$T = \\max\\left(T_{\\text{compute}}, T_{\\text{memory}}\\right)$$\nThese components are determined by the total number of floating-point operations ($\\text{FLOPs}$), the total bytes of data transferred between the processor and main memory ($\\text{Bytes}$), and the machine's peak performance characteristics:\n$$T_{\\text{compute}} = \\frac{\\text{FLOPs}}{P_{\\mathrm{peak}}}$$\n$$T_{\\text{memory}} = \\frac{\\text{Bytes}}{B_{\\mathrm{peak}}}$$\nThe problem provides the following hardware parameters, which are constant across all test cases:\n- Peak floating-point throughput: $P_{\\mathrm{peak}} = 100 \\times 10^9$ $\\mathrm{FLOP/s}$.\n- Peak memory bandwidth: $B_{\\mathrm{peak}} = 50 \\times 10^9$ $\\mathrm{byte/s}$.\n\nThe SpMV operation computes $y = A x$, where $A$ is an $n \\times m$ sparse matrix with $nnz$ non-zero elements. For each non-zero element $a_{ij}$, one multiplication ($a_{ij} \\cdot x_j$) and one addition (to the accumulator for $y_i$) are performed. Thus, the total FLOP count is:\n$$\\text{FLOPs} = 2 \\cdot nnz$$\n\nThe memory traffic models are based on specific algorithmic assumptions for each format.\nFor the CSR format, which stores non-zero values, column indices, and a row pointer array, the assumed algorithm accumulates results for each row $y_i$ in a register and writes the final value to memory once per row. The total memory traffic, $\\text{Bytes}_{\\mathrm{CSR}}$, is the sum of reads from the three CSR data arrays (values, column indices, row pointers), reads from the input vector $x$, and writes to the output vector $y$:\n$$\\text{Bytes}_{\\mathrm{CSR}} = \\underbrace{nnz \\cdot v_{\\mathrm{bytes}}}_{\\text{values}} + \\underbrace{nnz \\cdot i_{\\mathrm{bytes}}}_{\\text{column indices}} + \\underbrace{(n+1) \\cdot i_{\\mathrm{bytes}}}_{\\text{row pointer}} + \\underbrace{nnz \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } x} + \\underbrace{n \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } y \\text{ stores}}$$\nHere, $v_{\\mathrm{bytes}}$ is the byte size of a value and $i_{\\mathrm{bytes}}$ is the byte size of an index. This simplifies to:\n$$\\text{Bytes}_{\\mathrm{CSR}} = nnz \\cdot (2 v_{\\mathrm{bytes}} + i_{\\mathrm{bytes}}) + (n+1)i_{\\mathrm{bytes}} + n \\cdot v_{\\mathrm{bytes}}$$\n\nFor the COO format, which stores values, row indices, and column indices in three separate arrays, the assumed algorithm performs an atomic read-modify-write operation on the output vector $y$ for each non-zero element. The total memory traffic, $\\text{Bytes}_{\\mathrm{COO}}$, is:\n$$\\text{Bytes}_{\\mathrm{COO}} = \\underbrace{nnz \\cdot v_{\\mathrm{bytes}}}_{\\text{values}} + \\underbrace{nnz \\cdot i_{\\mathrm{bytes}}}_{\\text{column indices}} + \\underbrace{nnz \\cdot i_{\\mathrm{bytes}}}_{\\text{row indices}} + \\underbrace{nnz \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } x} + \\underbrace{2 \\cdot nnz \\cdot v_{\\mathrm{bytes}}}_{\\text{vector } y \\text{ read/write}}$$\nThis simplifies to:\n$$\\text{Bytes}_{\\mathrm{COO}} = nnz \\cdot (4 v_{\\mathrm{bytes}} + 2 i_{\\mathrm{bytes}})$$\n\nWe now apply these models to each test case. All runtimes are reported in seconds, rounded to six decimal places.\n\n**Test Case 1:** $(n, m, nnz, i_{\\mathrm{bytes}}, v_{\\mathrm{bytes}}) = (1000, 1000, 50000, 4, 8)$\n- $\\text{FLOPs} = 2 \\cdot 50000 = 100000$.\n- $T_{\\text{compute}} = \\frac{100000}{100 \\times 10^9} = 1.0 \\times 10^{-6} \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{CSR}} = 50000 \\cdot (2 \\cdot 8 + 4) + (1000+1) \\cdot 4 + 1000 \\cdot 8 = 1012004$ bytes.\n- $T_{\\text{mem, CSR}} = \\frac{1012004}{50 \\times 10^9} \\approx 2.024008 \\times 10^{-5} \\text{ s}$.\n- $T_{\\mathrm{CSR}} = \\max(1.0 \\times 10^{-6}, 2.024008 \\times 10^{-5}) \\approx 0.000020 \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{COO}} = 50000 \\cdot (4 \\cdot 8 + 2 \\cdot 4) = 2000000$ bytes.\n- $T_{\\text{mem, COO}} = \\frac{2000000}{50 \\times 10^9} = 4.0 \\times 10^{-5} \\text{ s}$.\n- $T_{\\mathrm{COO}} = \\max(1.0 \\times 10^{-6}, 4.0 \\times 10^{-5}) = 0.000040 \\text{ s}$.\n\n**Test Case 2:** $(n, m, nnz, i_{\\mathrm{bytes}}, v_{\\mathrm{bytes}}) = (5, 5, 7, 8, 8)$\n- $\\text{FLOPs} = 2 \\cdot 7 = 14$.\n- $T_{\\text{compute}} = \\frac{14}{100 \\times 10^9} = 1.4 \\times 10^{-10} \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{CSR}} = 7 \\cdot (2 \\cdot 8 + 8) + (5+1) \\cdot 8 + 5 \\cdot 8 = 256$ bytes.\n- $T_{\\text{mem, CSR}} = \\frac{256}{50 \\times 10^9} = 5.12 \\times 10^{-9} \\text{ s}$.\n- $T_{\\mathrm{CSR}} = \\max(1.4 \\times 10^{-10}, 5.12 \\times 10^{-9}) \\approx 0.000000 \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{COO}} = 7 \\cdot (4 \\cdot 8 + 2 \\cdot 8) = 336$ bytes.\n- $T_{\\text{mem, COO}} = \\frac{336}{50 \\times 10^9} = 6.72 \\times 10^{-9} \\text{ s}$.\n- $T_{\\mathrm{COO}} = \\max(1.4 \\times 10^{-10}, 6.72 \\times 10^{-9}) \\approx 0.000000 \\text{ s}$.\n\n**Test Case 3:** $(n, m, nnz, i_{\\mathrm{bytes}}, v_{\\mathrm{bytes}}) = (10000, 10000, 10000, 4, 8)$\n- $\\text{FLOPs} = 2 \\cdot 10000 = 20000$.\n- $T_{\\text{compute}} = \\frac{20000}{100 \\times 10^9} = 2.0 \\times 10^{-7} \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{CSR}} = 10000 \\cdot (2 \\cdot 8 + 4) + (10000+1) \\cdot 4 + 10000 \\cdot 8 = 320004$ bytes.\n- $T_{\\text{mem, CSR}} = \\frac{320004}{50 \\times 10^9} \\approx 6.40008 \\times 10^{-6} \\text{ s}$.\n- $T_{\\mathrm{CSR}} = \\max(2.0 \\times 10^{-7}, 6.40008 \\times 10^{-6}) \\approx 0.000006 \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{COO}} = 10000 \\cdot (4 \\cdot 8 + 2 \\cdot 4) = 400000$ bytes.\n- $T_{\\text{mem, COO}} = \\frac{400000}{50 \\times 10^9} = 8.0 \\times 10^{-6} \\text{ s}$.\n- $T_{\\mathrm{COO}} = \\max(2.0 \\times 10^{-7}, 8.0 \\times 10^{-6}) = 0.000008 \\text{ s}$.\n\n**Test Case 4:** $(n, m, nnz, i_{\\mathrm{bytes}}, v_{\\mathrm{bytes}}) = (20000, 2000, 80000, 4, 8)$\n- $\\text{FLOPs} = 2 \\cdot 80000 = 160000$.\n- $T_{\\text{compute}} = \\frac{160000}{100 \\times 10^9} = 1.6 \\times 10^{-6} \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{CSR}} = 80000 \\cdot (2 \\cdot 8 + 4) + (20000+1) \\cdot 4 + 20000 \\cdot 8 = 1840004$ bytes.\n- $T_{\\text{mem, CSR}} = \\frac{1840004}{50 \\times 10^9} \\approx 3.680008 \\times 10^{-5} \\text{ s}$.\n- $T_{\\mathrm{CSR}} = \\max(1.6 \\times 10^{-6}, 3.680008 \\times 10^{-5}) \\approx 0.000037 \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{COO}} = 80000 \\cdot (4 \\cdot 8 + 2 \\cdot 4) = 3200000$ bytes.\n- $T_{\\text{mem, COO}} = \\frac{3200000}{50 \\times 10^9} = 6.4 \\times 10^{-5} \\text{ s}$.\n- $T_{\\mathrm{COO}} = \\max(1.6 \\times 10^{-6}, 6.4 \\times 10^{-5}) = 0.000064 \\text{ s}$.\n\n**Test Case 5:** $(n, m, nnz, i_{\\mathrm{bytes}}, v_{\\mathrm{bytes}}) = (3000, 3000, 600000, 8, 8)$\n- $\\text{FLOPs} = 2 \\cdot 600000 = 1200000$.\n- $T_{\\text{compute}} = \\frac{1200000}{100 \\times 10^9} = 1.2 \\times 10^{-5} \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{CSR}} = 600000 \\cdot (2 \\cdot 8 + 8) + (3000+1) \\cdot 8 + 3000 \\cdot 8 = 14448008$ bytes.\n- $T_{\\text{mem, CSR}} = \\frac{14448008}{50 \\times 10^9} \\approx 2.8896016 \\times 10^{-4} \\text{ s}$.\n- $T_{\\mathrm{CSR}} = \\max(1.2 \\times 10^{-5}, 2.8896016 \\times 10^{-4}) \\approx 0.000289 \\text{ s}$.\n- $\\text{Bytes}_{\\mathrm{COO}} = 600000 \\cdot (4 \\cdot 8 + 2 \\cdot 8) = 28800000$ bytes.\n- $T_{\\text{mem, COO}} = \\frac{28800000}{50 \\times 10^9} = 5.76 \\times 10^{-4} \\text{ s}$.\n- $T_{\\mathrm{COO}} = \\max(1.2 \\times 10^{-5}, 5.76 \\times 10^{-4}) = 0.000576 \\text{ s}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes predicted runtimes for sparse matrix-vector multiplication (SpMV)\n    for CSR and COO formats based on a simplified roofline performance model.\n    \"\"\"\n\n    # Define hardware parameters from the problem statement.\n    P_peak = 100e9  # Peak floating point throughput in FLOP/s\n    B_peak = 50e9   # Peak memory bandwidth in byte/s\n\n    # Define the test suite. Each tuple is (n, m, nnz, i_bytes, v_bytes).\n    test_cases = [\n        (1000, 1000, 50000, 4, 8),\n        (5, 5, 7, 8, 8),\n        (10000, 10000, 10000, 4, 8),\n        (20000, 2000, 80000, 4, 8),\n        (3000, 3000, 600000, 8, 8),\n    ]\n\n    # List to store the results for each test case as a [T_csr, T_coo] pair.\n    results = []\n\n    for case in test_cases:\n        n, m, nnz, i_bytes, v_bytes = case\n\n        # 1. Compute FLOPs\n        # Each nonzero element results in one multiplication and one addition.\n        flops = 2 * nnz\n\n        # 2. Compute compute time\n        t_compute = flops / P_peak\n\n        # 3. Compute Bytes transferred for CSR format\n        # Bytes_CSR = (nnz*v_bytes) + (nnz*i_bytes) + ((n+1)*i_bytes) + (nnz*v_bytes) + (n*v_bytes)\n        bytes_csr = nnz * (2 * v_bytes + i_bytes) + (n + 1) * i_bytes + n * v_bytes\n        \n        # 4. Compute Bytes transferred for COO format\n        # Bytes_COO = (nnz*v_bytes) + (nnz*i_bytes) + (nnz*i_bytes) + (nnz*v_bytes) + (2*nnz*v_bytes)\n        bytes_coo = nnz * (4 * v_bytes + 2 * i_bytes)\n\n        # 5. Compute memory-bound times\n        t_mem_csr = bytes_csr / B_peak\n        t_mem_coo = bytes_coo / B_peak\n\n        # 6. Compute final predicted runtimes using the roofline model\n        t_csr = max(t_compute, t_mem_csr)\n        t_coo = max(t_compute, t_mem_coo)\n\n        results.append((t_csr, t_coo))\n\n    # Format the output as specified: [[t_1_csr,t_1_coo],[t_2_csr,t_2_coo],...]\n    # Runtimes are rounded to six decimal places.\n    output_pairs = [f\"[{t_csr:.6f},{t_coo:.6f}]\" for t_csr, t_coo in results]\n    final_output_string = f\"[{','.join(output_pairs)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "Correctness is paramount in scientific computing, and subtle bugs can arise from misunderstandings of data structure semantics. Sparse matrices are often assembled by adding contributions to the same entry, leading to duplicate coordinates in formats like COO. This practice presents a thought experiment to diagnose a common implementation error: failing to 'coalesce'—or sum—these duplicates when converting to a format like CSR. By working through this counterexample, you will solidify your understanding of the algebraic requirements of format conversion and the structural differences between COO and CSR.",
            "id": "3580366",
            "problem": "Consider a Sparse Matrix-Vector multiply (SpMV) defined by $y \\leftarrow A x$, where $A \\in \\mathbb{R}^{m \\times n}$ and $x \\in \\mathbb{R}^{n}$, with $y \\in \\mathbb{R}^{m}$. In Coordinate list (COO) format, the matrix $A$ is specified by triplets $(i_k, j_k, v_k)$ for $k = 1, \\ldots, K$, which represent entries $a_{i_k j_k}$ that are to be accumulated into the algebraic matrix $A$ by summation when multiple triplets share the same index pair $(i, j)$. In Compressed Sparse Row (CSR) format, the matrix is represented by three arrays that encode, for each row $i$, a contiguous segment of column indices and values, and structurally the CSR representation holds a single stored value per structural position $(i, j)$.\n\nFrom first principles, SpMV computes $y_i = \\sum_{j=1}^{n} a_{ij} x_j$ for each $i \\in \\{1, \\ldots, m\\}$, where $a_{ij}$ denotes the algebraic matrix entry obtained after assembling all COO triplets by summation over duplicates. In finite element assembly and other additive discretizations, duplicates in COO are common and must be coalesced into a single $a_{ij}$ prior to forming CSR to reflect the true algebraic matrix.\n\nSuppose an implementation converts COO to CSR but does not explicitly coalesce duplicates by summing values for repeated $(i, j)$ pairs. Instead, it constructs CSR by sorting triplets by $(i, j)$ and retaining only the last occurrence for each $(i, j)$ (i.e., overwriting previous values rather than summing), thereby violating the algebraic accumulation rule required to form the correct matrix.\n\nWhich option provides a concrete counterexample demonstrating that this failure to coalesce duplicates leads to incorrect SpMV results, and correctly identifies the structural reason?\n\nA. Let $A \\in \\mathbb{R}^{3 \\times 3}$ be given in COO by the triplets $(0, 1, 1)$, $(0, 1, -2)$, $(0, 1, 3)$, $(1, 0, 4)$, $(1, 2, 5)$, $(2, 2, -1)$, and let $x = [1, 2, 3]^{\\top}$. The algebraic coalesced matrix has $a_{0,1} = 1 + (-2) + 3 = 2$, so the correct SpMV yields $y = [4, 19, -3]^{\\top}$ because $y_0 = 2 \\cdot x_1 = 4$, $y_1 = 4 \\cdot x_0 + 5 \\cdot x_2 = 19$, and $y_2 = -1 \\cdot x_2 = -3$. If CSR is formed by retaining only the last occurrence per $(i, j)$ so that $a_{0,1}$ is set to $3$ (discarding the earlier contributions), the computed SpMV is $y = [6, 19, -3]^{\\top}$, which is incorrect. Structural reason: CSR stores exactly one value per structural index pair $(i, j)$ in the row segment; if duplicates are not summed prior to forming CSR and a last-wins overwrite is applied, then the numerical value stored for $(i, j)$ does not equal the algebraic sum required by COO assembly, changing $a_{ij}$ and thus changing $y_i = \\sum_j a_{ij} x_j$.\n\nB. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given in COO by $(0, 0, 2)$ and $(0, 0, 3)$, and let $x = [1, 1]^{\\top}$. Even if CSR is formed without coalescing duplicates, SpMV will be correct because duplicates are inherently added by the row-wise accumulation in CSR; therefore no explicit coalescing is ever needed. Structural reason: CSR’s row pointers guarantee that any duplicates will be added correctly during SpMV since they appear as repeated entries.\n\nC. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given in COO by $(0, 1, 1)$ and $(0, 1, -1)$, and let $x = [2, 5]^{\\top}$. Without coalescing, CSR SpMV is correct because the duplicates cancel out during multiplication, yielding $y_0 = (1 + (-1)) \\cdot x_1 = 0$. Structural reason: Since the duplicates sum to zero, the failure to coalesce has no effect and CSR can safely ignore earlier entries.\n\nD. Let $A \\in \\mathbb{R}^{3 \\times 3}$ be given in COO by $(0, 2, 4)$, $(1, 2, -4)$, $(1, 2, 1)$, $(2, 0, 7)$, and let $x = [1, 2, 3]^{\\top}$. If CSR is formed by counting row occurrences but not coalescing duplicates, SpMV becomes numerically unstable because the row pointers will be misaligned and the algorithm will read wrong columns. Structural reason: Duplicates necessarily corrupt the row pointer structure in CSR, making any SpMV incorrect regardless of how values are handled.\n\nSelect the single correct option.",
            "solution": "The problem statement poses a question regarding sparse matrix storage formats and the consequences of a specific, flawed implementation of a Coordinate (COO) to Compressed Sparse Row (CSR) conversion. The core of the problem lies in the definition of the matrix $A$ from a list of COO triplets, which requires summation of values for duplicate $(i, j)$ indices to form the true algebraic matrix. The flawed implementation bypasses this summation and instead performs a \"last-wins\" overwrite.\n\nThe problem is to identify a counterexample that correctly demonstrates an erroneous result from this flawed implementation and gives the correct structural reason for the failure.\n\nFirst, let us formalize the definitions provided.\n- The matrix-vector product is defined as $y \\leftarrow Ax$, computed for each row $i$ as $y_i = \\sum_{j=1}^{n} a_{ij} x_j$.\n- The COO format is a list of triplets $(i_k, j_k, v_k)$. The algebraic entry $a_{ij}$ of the matrix $A$ is given by the sum over all values $v_k$ whose corresponding indices $(i_k, j_k)$ equal $(i, j)$: $a_{ij} = \\sum_{k | (i_k, j_k) = (i,j)} v_k$. This process is known as coalescing.\n- The CSR format cannot represent duplicate $(i, j)$ entries within its structure; it stores a single value for each non-zero position.\n- The flawed conversion from COO to CSR sorts the triplets lexicographically by $(i,j)$ and for each unique index pair $(i,j)$, it stores only the value from the last triplet in the sorted list, effectively overwriting any previous values for that same $(i,j)$. Let us denote the matrix resulting from this flawed process as $A'$.\n\nWe must evaluate each option to see if it provides a valid counterexample and a correct explanation.\n\n### Option-by-Option Analysis\n\n**A. Let $A \\in \\mathbb{R}^{3 \\times 3}$ be given in COO by the triplets $(0, 1, 1)$, $(0, 1, -2)$, $(0, 1, 3)$, $(1, 0, 4)$, $(1, 2, 5)$, $(2, 2, -1)$, and let $x = [1, 2, 3]^{\\top}$.**\n\n1.  **Correct Algebraic SpMV:**\n    First, we determine the entries of the true algebraic matrix $A$ by coalescing the COO triplets by summation.\n    - For the index pair $(0, 1)$, we have three triplets: $(0, 1, 1)$, $(0, 1, -2)$, and $(0, 1, 3)$. The coalesced value is $a_{0,1} = 1 + (-2) + 3 = 2$.\n    - For the index pair $(1, 0)$, we have one triplet: $(1, 0, 4)$. Thus, $a_{1,0} = 4$.\n    - For the index pair $(1, 2)$, we have one triplet: $(1, 2, 5)$. Thus, $a_{1,2} = 5$.\n    - For the index pair $(2, 2)$, we have one triplet: $(2, 2, -1)$. Thus, $a_{2,2} = -1$.\n    The resulting algebraic matrix $A$ is:\n    $$ A = \\begin{pmatrix} 0 & 2 & 0 \\\\ 4 & 0 & 5 \\\\ 0 & 0 & -1 \\end{pmatrix} $$\n    Now, we compute the correct SpMV result, $y = Ax$, with $x = [1, 2, 3]^{\\top}$.\n    - $y_0 = (0 \\cdot 1) + (2 \\cdot 2) + (0 \\cdot 3) = 4$.\n    - $y_1 = (4 \\cdot 1) + (0 \\cdot 2) + (5 \\cdot 3) = 4 + 15 = 19$.\n    - $y_2 = (0 \\cdot 1) + (0 \\cdot 2) + (-1 \\cdot 3) = -3$.\n    So, the correct result is $y = [4, 19, -3]^{\\top}$. This matches the calculation in the option.\n\n2.  **Incorrect SpMV from Flawed CSR Conversion:**\n    The flawed implementation sorts the triplets by $(i, j)$ and retains the last value for each pair. For the index pair $(0, 1)$, the triplets are $(0, 1, 1)$, $(0, 1, -2)$, $(0, 1, 3)$. The last value is $3$.\n    The entries of the flawed matrix $A'$ are:\n    - $a'_{0,1} = 3$ (last-wins overwrite).\n    - $a'_{1,0} = 4$ (only one entry).\n    - $a'_{1,2} = 5$ (only one entry).\n    - $a'_{2,2} = -1$ (only one entry).\n    The flawed matrix $A'$ is:\n    $$ A' = \\begin{pmatrix} 0 & 3 & 0 \\\\ 4 & 0 & 5 \\\\ 0 & 0 & -1 \\end{pmatrix} $$\n    Now, we compute the incorrect SpMV result, $y' = A'x$.\n    - $y'_0 = (0 \\cdot 1) + (3 \\cdot 2) + (0 \\cdot 3) = 6$.\n    - $y'_1 = (4 \\cdot 1) + (0 \\cdot 2) + (5 \\cdot 3) = 4 + 15 = 19$.\n    - $y'_2 = (0 \\cdot 1) + (0 \\cdot 2) + (-1 \\cdot 3) = -3$.\n    So, the incorrect result is $y' = [6, 19, -3]^{\\top}$. This also matches the calculation in the option.\n\n3.  **Evaluation of Reason:**\n    The correct result $y$ and the incorrect result $y'$ are different, so this is a valid counterexample. The provided reason is: \"CSR stores exactly one value per structural index pair $(i, j)$... if duplicates are not summed prior to forming CSR and a last-wins overwrite is applied, then the numerical value stored for $(i, j)$ does not equal the algebraic sum required by COO assembly, changing $a_{ij}$ and thus changing $y_i = \\sum_j a_{ij} x_j$.\" This explanation is precise and correct. The structural constraint of CSR (one entry per $(i, j)$) necessitates a coalescing step to correctly represent a matrix defined by additive COO semantics. The flawed \"overwrite\" method populates the CSR structure with incorrect numerical values, which leads to an erroneous SpMV result.\n\n**Verdict:** Correct.\n\n**B. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given in COO by $(0, 0, 2)$ and $(0, 0, 3)$, and let $x = [1, 1]^{\\top}$.**\n\nThe option claims that SpMV will be correct because \"duplicates are inherently added by the row-wise accumulation in CSR.\" This premise is fundamentally flawed. A standard CSR format does not have a mechanism to store, let alone sum, duplicate entries. The structure of CSR consists of three arrays: `row_ptr`, `col_ind`, and `values`. For a given row $i$, the segment `col_ind[row_ptr[i] : row_ptr[i+1]]` is expected to contain unique column indices. If a non-standard CSR were created with duplicate column indices (e.g., `col_ind` for row $0$ is `[0, 0]`), a custom SpMV kernel would be needed to handle it. A standard kernel would simply iterate and perform two separate multiplications, effectively summing them, but this relies on a malformed CSR. The problem statement's definition of CSR (\"holds a single stored value per structural position\") and the specified flaw (overwrite) contradict the logic in this option. The structural reason provided (\"CSR’s row pointers guarantee that any duplicates will be added correctly\") is a misrepresentation of the function of row pointers.\n\n**Verdict:** Incorrect.\n\n**C. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given in COO by $(0, 1, 1)$ and $(0, 1, -1)$, and let $x = [2, 5]^{\\top}$.**\n\nThis option claims the result is correct because the duplicates cancel.\n1.  **Correct Algebraic SpMV:**\n    The COO triplets are $(0, 1, 1)$ and $(0, 1, -1)$. Coalescing gives $a_{0,1} = 1 + (-1) = 0$.\n    The correct SpMV result is $y_0 = a_{0,1} x_1 = 0 \\cdot 5 = 0$.\n2.  **Incorrect SpMV from Flawed CSR Conversion:**\n    The flawed implementation takes the last value. The last triplet is $(0, 1, -1)$, so $a'_{0,1} = -1$.\n    The incorrect SpMV result is $y'_0 = a'_{0,1} x_1 = -1 \\cdot 5 = -5$.\nSince $0 \\neq -5$, the assertion that the SpMV result is correct is false. The option misinterprets the flawed algorithm; it assumes the values are summed, when the problem explicitly states they are overwritten. The special numerical property that the values sum to zero is irrelevant to the outcome of the specified flawed algorithm.\n\n**Verdict:** Incorrect.\n\n**D. Let $A \\in \\mathbb{R}^{3 \\times 3}$ be given in COO by $(0, 2, 4)$, $(1, 2, -4)$, $(1, 2, 1)$, $(2, 0, 7)$, and let $x = [1, 2, 3]^{\\top}$.**\n\nThis option introduces a flaw mechanism (\"counting row occurrences but not coalescing duplicates\") that is different from the one defined in the problem statement (\"retaining only the last occurrence\"). This makes the option non-responsive to the question. Furthermore, its reasoning is flawed. It claims \"row pointers will be misaligned\" and the SpMV becomes \"numerically unstable.\" These are imprecise and incorrect descriptions of the error. The error is a logical one: representing the wrong matrix. With the problem's specified \"overwrite\" flaw, the resulting CSR structure would be perfectly well-formed, not \"corrupt,\" and the SpMV would be numerically stable; it would simply compute the product with the wrong matrix $A'$. The term \"numerical instability\" refers to the sensitivity of an algorithm to small changes in input data (e.g., rounding errors), which is not the issue here. The structural reason given (\"Duplicates necessarily corrupt the row pointer structure in CSR\") is false for the overwrite-based flaw specified in the problem.\n\n**Verdict:** Incorrect.\n\nBased on the thorough analysis, only option A provides a numerically correct counterexample that is consistent with the problem's definition of the flawed algorithm, and it provides a clear, accurate structural explanation for the resulting error.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Optimizing memory usage is a primary motivation for using sparse formats, and exploiting matrix structure, like symmetry, is a common technique. However, these optimizations can introduce errors if not applied carefully. This problem explores a scenario where a matrix is structurally symmetric (the locations of nonzeros are symmetric) but not numerically symmetric. By tracing a naive SpMV algorithm that assumes full symmetry, you will not only calculate the resulting error but also derive the precise mathematical conditions under which such an optimization is valid, sharpening your analytical skills for algorithm verification.",
            "id": "3580379",
            "problem": "Consider the linear operator represented by a real sparse matrix $A \\in \\mathbb{R}^{n \\times n}$ stored in Compressed Sparse Column (CSC) format, where Sparse Matrix-Vector multiplication (SpMV) computes $y \\leftarrow A x$ by iterating over columns $j$ and, for each stored nonzero $a_{ij}$ in column $j$, performing the scatter update $y_i \\leftarrow y_i + a_{ij} x_j$. Assume $A$ has structurally symmetric sparsity, meaning that for each pair $(i,j)$ with $i \\neq j$, the nonzero pattern satisfies $a_{ij} \\neq 0$ if and only if $a_{ji} \\neq 0$, but the values may be nonsymmetric. A common storage optimization stores only the upper triangular part (including the diagonal), i.e., entries with $i \\leq j$. A naive routine intended for symmetric matrices then executes, for each stored upper-triangular entry $a_{ij}$:\n- if $i < j$, the updates $y_i \\leftarrow y_i + a_{ij} x_j$ and $y_j \\leftarrow y_j + a_{ij} x_i$;\n- if $i = j$, the same two-update pattern without any special handling, i.e., it performs $y_i \\leftarrow y_i + a_{ii} x_i$ twice.\n\n1) Construct an explicit counterexample by taking\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2 & 3 & -5 \\\\\n8 & -1 & 7 \\\\\n1 & -6 & 4\n\\end{pmatrix}\n\\in \\mathbb{R}^{3 \\times 3},\n\\qquad\nx \\;=\\; \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} \\in \\mathbb{R}^{3}.\n$$\nHere $A$ has symmetric sparsity but is not value-symmetric. Using only the CSC-stored upper triangular part of $A$ and the naive symmetric SpMV routine described above (with no special handling of the diagonal), compute the SpMV output $\\tilde{y}$ and the true output $y = A x$, and then form the error $e = \\tilde{y} - y$. Report the Euclidean norm $\\|e\\|_{2}$, rounded to six significant figures.\n\n2) Starting from the linearity of SpMV and the definition of matrix-vector multiplication, derive and state a necessary and sufficient condition on the entries of $A$ under which the naive symmetric SpMV from upper-triangular CSC storage (with no special diagonal handling) produces the exact product $y = A x$ for all $x \\in \\mathbb{R}^{n}$. Your condition should be an explicit algebraic relation among the entries $\\{a_{ij}\\}$ that must hold entrywise.\n\nProvide only the numerical value requested in part $1$ as the final answer. Round your answer to six significant figures.",
            "solution": "The user has provided a two-part problem concerning a naive implementation of sparse matrix-vector multiplication (SpMV) for a matrix with structurally symmetric sparsity, where only the upper triangular part is stored. The task is to first compute the error of this naive algorithm for a specific matrix and vector, and second, to derive the general conditions under which this algorithm is exact.\n\n### **Part 1: Counterexample Calculation**\n\nWe are given the matrix $A$ and vector $x$:\n$$\nA = \\begin{pmatrix}\n2 & 3 & -5 \\\\\n8 & -1 & 7 \\\\\n1 & -6 & 4\n\\end{pmatrix}\n, \\quad\nx = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}\n$$\nThe problem specifies that the naive SpMV routine operates on the upper triangular part of $A$, which includes the main diagonal. The entries of $A$ with $i \\leq j$ are:\n$a_{11} = 2$, $a_{12} = 3$, $a_{13} = -5$\n$a_{22} = -1$, $a_{23} = 7$\n$a_{33} = 4$\n\nThe algorithm is defined as follows, for each stored upper triangular entry $a_{ij}$:\n- If $i < j$: perform $y_i \\leftarrow y_i + a_{ij} x_j$ and $y_j \\leftarrow y_j + a_{ij} x_i$.\n- If $i = j$: perform $y_i \\leftarrow y_i + a_{ii} x_i$ twice.\n\nWe will compute the output of this naive routine, denoted $\\tilde{y}$, by simulating the process. The process uses Compressed Sparse Column (CSC) format, so we iterate through columns. We initialize $\\tilde{y} = (0, 0, 0)^T$.\n\n**Column $j=1$:**\n- The only stored entry is $a_{11} = 2$. Since $i=j=1$, we apply the diagonal rule.\n- $\\tilde{y}_1 \\leftarrow \\tilde{y}_1 + a_{11} x_1 = 0 + (2)(1) = 2$.\n- $\\tilde{y}_1 \\leftarrow \\tilde{y}_1 + a_{11} x_1 = 2 + (2)(1) = 4$.\nAfter this column, $\\tilde{y} = (4, 0, 0)^T$.\n\n**Column $j=2$:**\n- Stored entry $a_{12} = 3$: Here $i=1, j=2$, so $i<j$.\n- $\\tilde{y}_1 \\leftarrow \\tilde{y}_1 + a_{12} x_2 = 4 + (3)(-2) = -2$.\n- $\\tilde{y}_2 \\leftarrow \\tilde{y}_2 + a_{12} x_1 = 0 + (3)(1) = 3$.\n- Stored entry $a_{22} = -1$: Here $i=j=2$.\n- $\\tilde{y}_2 \\leftarrow \\tilde{y}_2 + a_{22} x_2 = 3 + (-1)(-2) = 5$.\n- $\\tilde{y}_2 \\leftarrow \\tilde{y}_2 + a_{22} x_2 = 5 + (-1)(-2) = 7$.\nAfter this column, $\\tilde{y} = (-2, 7, 0)^T$.\n\n**Column $j=3$:**\n- Stored entry $a_{13} = -5$: Here $i=1, j=3$, so $i<j$.\n- $\\tilde{y}_1 \\leftarrow \\tilde{y}_1 + a_{13} x_3 = -2 + (-5)(3) = -17$.\n- $\\tilde{y}_3 \\leftarrow \\tilde{y}_3 + a_{13} x_1 = 0 + (-5)(1) = -5$.\n- Stored entry $a_{23} = 7$: Here $i=2, j=3$, so $i<j$.\n- $\\tilde{y}_2 \\leftarrow \\tilde{y}_2 + a_{23} x_3 = 7 + (7)(3) = 28$.\n- $\\tilde{y}_3 \\leftarrow \\tilde{y}_3 + a_{23} x_2 = -5 + (7)(-2) = -19$.\n- Stored entry $a_{33} = 4$: Here $i=j=3$.\n- $\\tilde{y}_3 \\leftarrow \\tilde{y}_3 + a_{33} x_3 = -19 + (4)(3) = -7$.\n- $\\tilde{y}_3 \\leftarrow \\tilde{y}_3 + a_{33} x_3 = -7 + (4)(3) = 5$.\nAfter this final column, the computed vector is $\\tilde{y} = (-17, 28, 5)^T$.\n\nNext, we compute the true product $y = Ax$.\n$$\ny = \\begin{pmatrix}\n2 & 3 & -5 \\\\\n8 & -1 & 7 \\\\\n1 & -6 & 4\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}\n= \\begin{pmatrix}\n(2)(1) + (3)(-2) + (-5)(3) \\\\\n(8)(1) + (-1)(-2) + (7)(3) \\\\\n(1)(1) + (-6)(-2) + (4)(3)\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 - 6 - 15 \\\\\n8 + 2 + 21 \\\\\n1 + 12 + 12\n\\end{pmatrix}\n= \\begin{pmatrix} -19 \\\\ 31 \\\\ 25 \\end{pmatrix}\n$$\nThe error vector is $e = \\tilde{y} - y$.\n$$\ne = \\begin{pmatrix} -17 \\\\ 28 \\\\ 5 \\end{pmatrix} - \\begin{pmatrix} -19 \\\\ 31 \\\\ 25 \\end{pmatrix} = \\begin{pmatrix} -17 - (-19) \\\\ 28 - 31 \\\\ 5 - 25 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -3 \\\\ -20 \\end{pmatrix}\n$$\nFinally, we compute the Euclidean norm $\\|e\\|_2$.\n$$\n\\|e\\|_2 = \\sqrt{2^2 + (-3)^2 + (-20)^2} = \\sqrt{4 + 9 + 400} = \\sqrt{413}\n$$\nNumerically, $\\sqrt{413} \\approx 20.3224014$. Rounding to six significant figures gives $20.3224$.\n\n### **Part 2: Derivation of the Exactness Condition**\n\nTo derive the necessary and sufficient condition for the naive algorithm to be exact for all input vectors $x \\in \\mathbb{R}^n$, we must have $\\tilde{y} = Ax$. This means the effective matrix $\\tilde{A}$ implicitly computed by the algorithm must be identical to $A$. Let us determine the entries of $\\tilde{A}$ by analyzing the computation of each component $\\tilde{y}_i$.\n\nThe total value of $\\tilde{y}_i$ is the sum of all updates to it. We analyze the contributions from each stored upper triangular entry $a_{kj}$ (where $k \\le j$):\n\n1.  **Contribution from diagonal entries:** For a stored entry $a_{ii}$ (where $k=j=i$), the algorithm performs $y_i \\leftarrow y_i + a_{ii}x_i$ twice. This contributes $2a_{ii}x_i$ to the sum for $\\tilde{y}_i$.\n\n2.  **Contribution from strictly upper triangular entries in row $i$:** For a stored entry $a_{ij}$ with $i < j$ (i.e., $k=i, j>i$), the algorithm performs $y_i \\leftarrow y_i + a_{ij}x_j$. This contributes $\\sum_{j>i} a_{ij}x_j$ to $\\tilde{y}_i$.\n\n3.  **Contribution from strictly upper triangular entries in column $i$:** For a stored entry $a_{ji}$ with $j < i$ (i.e., $k=j, j<i$), the algorithm performs $y_i \\leftarrow y_i + a_{ji}x_j$. This contributes $\\sum_{j<i} a_{ji}x_j$ to $\\tilde{y}_i$.\n\nSumming these contributions, the $i$-th component of the computed vector is:\n$$\n\\tilde{y}_i = \\left( \\sum_{j<i} a_{ji}x_j \\right) + 2a_{ii}x_i + \\left( \\sum_{j>i} a_{ij}x_j \\right)\n$$\nThe $i$-th component of the true product $y = Ax$ is:\n$$\ny_i = \\sum_{j=1}^n a_{ij}x_j = \\left( \\sum_{j<i} a_{ij}x_j \\right) + a_{ii}x_i + \\left( \\sum_{j>i} a_{ij}x_j \\right)\n$$\nFor the algorithm to be exact, we must have $\\tilde{y}_i = y_i$ for all $i \\in \\{1,\\dots,n\\}$ and for all $x \\in \\mathbb{R}^n$. Equating the two expressions:\n$$\n\\left( \\sum_{j<i} a_{ji}x_j \\right) + 2a_{ii}x_i + \\left( \\sum_{j>i} a_{ij}x_j \\right) = \\left( \\sum_{j<i} a_{ij}x_j \\right) + a_{ii}x_i + \\left( \\sum_{j>i} a_{ij}x_j \\right)\n$$\nThe term $\\sum_{j>i} a_{ij}x_j$ cancels from both sides. Rearranging the remaining terms, we get:\n$$\n\\left( 2a_{ii} - a_{ii} \\right)x_i + \\sum_{j<i} (a_{ji} - a_{ij})x_j = 0\n$$\n$$\na_{ii}x_i + \\sum_{j<i} (a_{ji} - a_{ij})x_j = 0\n$$\nThis linear equation in the variables $x_1, \\dots, x_i$ must hold for any choice of these variables. This is only possible if all coefficients are zero. This gives us two conditions for each $i$:\n\n1.  The coefficient of $x_i$ must be zero: $a_{ii} = 0$.\n2.  For all $j < i$, the coefficient of $x_j$ must be zero: $a_{ji} - a_{ij} = 0$, which implies $a_{ji} = a_{ij}$.\n\nThese conditions must hold for all $i$ from $1$ to $n$.\n- The condition $a_{ii}=0$ for all $i$ means the matrix $A$ must have a zero diagonal.\n- The condition $a_{ji} = a_{ij}$ for all $j < i$ and all $i$ is equivalent to the condition $a_{ij} = a_{ji}$ for all $i \\neq j$. This is the definition of a symmetric matrix.\n\nTherefore, the necessary and sufficient condition is that the matrix $A$ must be symmetric ($A=A^T$) and have a zero diagonal. Expressed as entrywise relations, the conditions are:\n- $a_{ij} = a_{ji}$ for all $i, j \\in \\{1, \\dots, n\\}$.\n- $a_{ii} = 0$ for all $i \\in \\{1, \\dots, n\\}$.",
            "answer": "$$\\boxed{20.3224}$$"
        }
    ]
}