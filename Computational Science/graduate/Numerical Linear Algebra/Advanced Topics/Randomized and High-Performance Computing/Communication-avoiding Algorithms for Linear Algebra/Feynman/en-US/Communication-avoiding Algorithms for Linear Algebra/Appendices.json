{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of distributed algorithms often hinges on the choice of collective communication operations. This exercise  provides a practical application of the Hockney model to analyze the performance of two distinct strategies for data reduction. By deriving the communication costs for `allreduce` and `reduce-scatter`, you will develop a quantitative understanding of the trade-off between latency and bandwidth, a fundamental skill for performance engineering in high-performance computing.",
            "id": "3537871",
            "problem": "Consider the Cholesky-based QR factorization (CholeskyQR) of a tall-and-skinny matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$ on a distributed-memory system with $P$ processors organized under the Message Passing Interface (MPI) programming model. Assume $A$ is block-row distributed so that processor $p$ owns $A_p \\in \\mathbb{R}^{m_p \\times n}$ and forms its local Gram matrix $G_p = A_p^{T} A_p \\in \\mathbb{R}^{n \\times n}$. The global Gram matrix is $G = \\sum_{p=1}^{P} G_p = R^{T} R$, where $R$ is the upper-triangular Cholesky factor of $G$. Each $G_p$ is symmetric; assume it is stored in upper-triangular packed format of size $s = \\frac{n(n+1)}{2}$ doubles per processor.\n\nYou are to compare two collective communication strategies for forming $G$ from the $\\{G_p\\}$, using a standard two-parameter communication-time model (the Hockney model): the time to send a message of $w$ doubles is $T_{\\text{msg}}(w) = \\alpha + \\beta w$, where $\\alpha$ is the latency (in seconds per message) and $\\beta$ is the inverse bandwidth (in seconds per double). Ignore any computational costs and any overlap between communication and computation. Assume the following algorithmic choices for the collectives:\n\n- The allreduce (sum) is implemented by the Rabenseifner method, i.e., recursive halving for reduce-scatter followed by recursive doubling for allgather, each incurring $\\log_{2} P$ communication steps.\n- The reduce-scatter (sum-reduce with equal-size scatter of the reduced result) is implemented by the ring algorithm, i.e., it uses $P-1$ nearest-neighbor steps with evenly partitioned message size per step.\n\nAssume the end goal of this stage is to make the globally reduced Gram matrix available in a distributed form (not replicated), so that replication performed by allreduce is strictly more communication than necessary for this stage.\n\nStarting only from the Hockney model and the stated algorithmic structures, derive symbolic expressions for the total communication times of the two strategies as functions of $n$, $P$, $\\alpha$, and $\\beta$, and then find the exact closed-form expression for the crossover column dimension $n^{\\ast}$ at which the two strategies have equal communication time. Your final answer must be this $n^{\\ast}$ expressed in terms of $P$, $\\alpha$, and $\\beta$. Do not include units in the final expression. If the expression involves a logarithm of $P$, use base-$2$ explicitly. No numerical evaluation is required.",
            "solution": "We begin from the Hockney model, which states that sending a message of $w$ doubles incurs time $T_{\\text{msg}}(w) = \\alpha + \\beta w$, where $\\alpha$ is latency and $\\beta$ is inverse bandwidth. We will compute the communication time of each collective by counting the number of communication steps and the volume per step under the given algorithms, and then we will compare the resulting expressions.\n\nEach processor holds $s = \\frac{n(n+1)}{2}$ doubles representing the upper-triangular packed storage of $G_p$. The goal is to form $G = \\sum_{p=1}^{P} G_p$ in distributed form across $P$ processors.\n\nFirst, consider the reduce-scatter implemented by the ring algorithm. In the ring reduce-scatter for equal-size partitions, there are $P-1$ communication steps. In each step, each processor sends and receives a contiguous block consisting of $\\frac{s}{P}$ doubles. By the Hockney model, each step costs $\\alpha + \\beta \\cdot \\frac{s}{P}$, and there are $P-1$ steps. Therefore, the total time is\n$$\nT_{\\text{RS,ring}}(n,P,\\alpha,\\beta) \\;=\\; (P-1)\\,\\alpha \\;+\\; (P-1)\\,\\beta \\cdot \\frac{s}{P}\n\\;=\\; (P-1)\\,\\alpha \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}.\n$$\n\nSecond, consider the allreduce implemented by the Rabenseifner method, which consists of a reduce-scatter by recursive halving followed by an allgather by recursive doubling. Under recursive halving (and doubling), the number of steps is $\\log_{2} P$ for each phase. For bandwidth cost, the total number of doubles communicated per processor over the entire reduce-scatter phase equals $\\frac{P-1}{P}\\,s$, and the same holds for the allgather phase. Thus, the total allreduce time is the sum of the two phases:\n$$\nT_{\\text{AR,Rab}}(n,P,\\alpha,\\beta) \\;=\\; \\underbrace{\\log_{2} P \\cdot \\alpha}_{\\text{reduce-scatter latency}} \\;+\\; \\underbrace{\\frac{P-1}{P}\\,\\beta\\,s}_{\\text{reduce-scatter bandwidth}}\n\\;+\\; \\underbrace{\\log_{2} P \\cdot \\alpha}_{\\text{allgather latency}} \\;+\\; \\underbrace{\\frac{P-1}{P}\\,\\beta\\,s}_{\\text{allgather bandwidth}}.\n$$\nCollecting terms and substituting $s = \\frac{n(n+1)}{2}$, we get\n$$\nT_{\\text{AR,Rab}}(n,P,\\alpha,\\beta) \\;=\\; 2\\,\\alpha\\,\\log_{2} P \\;+\\; 2\\,\\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}\n\\;=\\; 2\\,\\alpha\\,\\log_{2} P \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot n(n+1).\n$$\n\nWe now find the crossover $n^{\\ast}$ where the two times are equal. Set $T_{\\text{RS,ring}} = T_{\\text{AR,Rab}}$:\n$$\n(P-1)\\,\\alpha \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}\n\\;=\\; 2\\,\\alpha\\,\\log_{2} P \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot n(n+1).\n$$\nRearrange to isolate the term in $n(n+1)$:\n$$\n(P-1)\\,\\alpha \\;-\\; 2\\,\\alpha\\,\\log_{2} P \\;=\\; \\frac{P-1}{P}\\,\\beta \\left( n(n+1) \\;-\\; \\frac{n(n+1)}{2} \\right)\n\\;=\\; \\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}.\n$$\nSolve for $n(n+1)$:\n$$\n\\frac{n(n+1)}{2} \\;=\\; \\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) \\;-\\; 2\\,\\log_{2} P \\bigr).\n$$\nDefine\n$$\nC \\;=\\; \\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) \\;-\\; 2\\,\\log_{2} P \\bigr).\n$$\nThen the equality condition becomes $\\frac{n(n+1)}{2} = C$. Solving the quadratic equation $n^{2} + n - 2C = 0$ yields\n$$\nn^{\\ast} \\;=\\; \\frac{-1 + \\sqrt{\\,1 + 8C\\,}}{2}\n\\;=\\; \\frac{-1 + \\sqrt{\\,1 + 8\\,\\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) - 2\\,\\log_{2} P \\bigr)\\,}}{2}.\n$$\n\nThis $n^{\\ast}$ is the unique nonnegative crossover in the Hockney model. For $n < n^{\\ast}$, the allreduce with lower latency term is faster; for $n > n^{\\ast}$, the reduce-scatter with lower bandwidth coefficient is faster. When $\\bigl( (P-1) - 2\\,\\log_{2} P \\bigr) \\le 0$, the quantity $C \\le 0$ and the crossover occurs at a nonpositive $s$, indicating that, in this latency-dominated regime, the allreduce remains preferable for all nonnegative $n$ within this model; conversely, for sufficiently large $n$, the reduce-scatter becomes preferable when $C > 0$.",
            "answer": "$$\\boxed{\\frac{-1 + \\sqrt{\\,1 + 8\\,\\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl((P-1) - 2\\,\\log_{2} P\\bigr)\\,}}{2}}$$"
        },
        {
            "introduction": "Theoretical computational cost, measured in floating-point operations, is not the only metric for performance on modern parallel architectures. This practice  guides you through a comparative analysis of four distinct algorithms for QR factorization, from classical methods to communication-avoiding variants. By evaluating their respective synchronization requirements and computational workloads, you will gain insight into the critical trade-offs that motivate the design of communication-avoiding algorithms, where minimizing data movement can be more important than minimizing arithmetic.",
            "id": "3537877",
            "problem": "Consider a tall-skinny matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$, distributed in a block-row layout across $p$ processes in a distributed-memory system using Message Passing Interface (MPI). The goal is to compute a thin $QR$ factorization, $A = QR$, using one of the following algorithms: Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), CGS with reorthogonalization (CGS2), and CholeskyQR. Assume the following standard implementation choices that are widely used as a baseline for communication analysis in numerical linear algebra:\n\n- For CGS on column $k$, the inner products with previously computed $k-1$ columns of $Q$ are computed locally and aggregated with a single MPI Allreduce on a vector of length $k-1$, followed by a local update, and then a separate MPI Allreduce to compute the Euclidean norm.\n- For MGS on column $k$, the algorithm orthogonalizes sequentially against $q_{1}, q_{2}, \\dots, q_{k-1}$, performing a global dot product per step (each as an MPI Allreduce on a scalar) and finally a separate MPI Allreduce for the Euclidean norm; no pipelining or communication-aggregation variants are assumed.\n- For CGS2 on column $k$, two CGS passes are performed (each with a single MPI Allreduce on a vector of length $k-1$) followed by an MPI Allreduce for the Euclidean norm.\n- For CholeskyQR, the Gram matrix $G = A^{T} A$ is formed and reduced once globally (e.g., via a single MPI Allreduce on the symmetric $n \\times n$ matrix or its packed upper triangle of $\\frac{n(n+1)}{2}$ scalars), then $R = \\text{chol}(G)$ is computed locally, and $Q = A R^{-1}$ is formed via a local triangular solve. No reorthogonalization step is used.\n\nUsing only first principles about distributed dot products and norms, and standard floating-point operation (flop) counts for matrix–vector and matrix–matrix operations, select the option that correctly characterizes, for each method: the leading-order flop count in $m$ and $n$, the number of global synchronizations, and the typical MPI collective sizes. You may assume that $m \\gg n$ so that lower-order terms may be stated but do not dominate.\n\nA. CGS and MGS each cost approximately $2 m n^{2}$ flops; CGS2 costs approximately $4 m n^{2}$ flops; CholeskyQR costs approximately $2 m n^{2} + \\frac{1}{3} n^{3}$ flops. Per column $k$, CGS performs $2$ synchronizations (one MPI Allreduce on a vector of length $k-1$ for inner products, and one MPI Allreduce on a scalar for the norm); MGS performs $k$ synchronizations (one scalar MPI Allreduce for each of the $k-1$ dot products plus one scalar MPI Allreduce for the norm); CGS2 performs $3$ synchronizations (two vector MPI Allreduces of length $k-1$ for the two CGS passes, plus one scalar MPI Allreduce for the norm). CholeskyQR performs $1$ global synchronization total for the whole factorization (one MPI Allreduce on the symmetric $n \\times n$ Gram matrix, e.g., $\\frac{n(n+1)}{2}$ scalars), after which all computations are local.\n\nB. CGS and MGS each cost approximately $2 m n^{2}$ flops; CGS2 costs approximately $2 m n^{2}$ flops; CholeskyQR costs approximately $m n^{2}$ flops. Per column $k$, both CGS and MGS can be implemented with $2$ synchronizations (one vector MPI Allreduce of length $k-1$ and one scalar MPI Allreduce for the norm) by batching the dot products; CGS2 also needs only $2$ synchronizations per column; CholeskyQR requires $n$ synchronizations (one scalar MPI Allreduce per column) to accumulate the Gram matrix.\n\nC. CGS costs approximately $2 m n^{2}$ flops; MGS costs approximately $4 m n^{2}$ flops; CGS2 costs approximately $2 m n^{2}$ flops; CholeskyQR costs approximately $2 m n^{2}$ flops. Per column $k$, CGS requires $k$ synchronizations (one scalar MPI Allreduce per inner product plus one for the norm), MGS requires $2$ synchronizations (one vector MPI Allreduce and one scalar MPI Allreduce), CGS2 requires $2$ synchronizations, and CholeskyQR requires $O(n)$ synchronizations because Cholesky needs iterative refinement.\n\nD. CGS and MGS each cost approximately $2 m n^{2}$ flops; CGS2 costs approximately $4 m n^{2}$ flops; CholeskyQR costs approximately $2 m n^{2} + \\frac{1}{3} n^{3}$ flops. Per column $k$, CGS performs $1$ synchronization (one vector MPI Allreduce of length $k-1$, folding the norm into the same collective); MGS performs $2$ synchronizations (one vector MPI Allreduce and one scalar MPI Allreduce); CGS2 performs $2$ synchronizations (two vector MPI Allreduces); CholeskyQR requires $2$ synchronizations for the whole factorization (one MPI Allreduce for $A^{T} A$ and one for distributing $R$).",
            "solution": "The problem requires a critical analysis of four different algorithms for computing the thin $QR$ factorization of a tall-skinny matrix $A \\in \\mathbb{R}^{m \\times n}$ where $m \\gg n$. The matrix $A$ is distributed in a block-row fashion across $p$ processes. The analysis must be based on the specific implementation details provided for each algorithm: Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), CGS with reorthogonalization (CGS2), and CholeskyQR. We will evaluate the leading-order floating-point operation (flop) count, the number of global synchronizations, and the size of data in MPI collectives for each method.\n\nLet $A = [a_1, a_2, \\dots, a_n]$ be the input matrix and $Q=[q_1, q_2, \\dots, q_n]$ and $R \\in \\mathbb{R}^{n \\times n}$ be the output matrices. Each of the $p$ processes holds $m/p$ rows of $A$ and $Q$. A global dot product or norm calculation requires each process to compute a local sum and then participate in a collective `MPI_Allreduce` operation to obtain the global result. A synchronization is counted as one `MPI_Allreduce` call.\n\n**Classical Gram-Schmidt (CGS)**\n\nThe algorithm processes one column $a_k$ at a time for $k=1, \\dots, n$.\n1.  **Orthogonalization:** The vector $a_k$ is orthogonalized against the previously computed orthonormal vectors $q_1, \\dots, q_{k-1}$. The coefficients are $R_{jk} = q_j^T a_k$ for $j=1, \\dots, k-1$. The problem states these $k-1$ dot products are computed via \"a single MPI Allreduce on a vector of length $k-1$\". This constitutes $1$ synchronization. The flop count for these dot products is $(k-1) \\times (2m-1) \\approx 2m(k-1)$. The subsequent update $v_k = a_k - \\sum_{j=1}^{k-1} R_{jk} q_j$ costs $\\approx 2m(k-1)$ flops and is a local operation. Total flops for orthogonalization per column $k$ is $\\approx 4m(k-1)$.\n2.  **Normalization:** The resulting vector $v_k$ is normalized. This requires computing its Euclidean norm $R_{kk} = \\|v_k\\|_2$. The problem states this is done with \"a separate MPI Allreduce\". This is the 2nd synchronization for column $k$, operating on a scalar value. The flop count for normalization is minor ($m$ multiplications and $m-1$ additions for the dot product, plus $m$ divisions).\n3.  **Total Flops:** Summing the dominant cost over all columns: $\\sum_{k=1}^n 4m(k-1) = 4m \\frac{(n-1)n}{2} \\approx 2mn^2$.\n4.  **Total Synchronizations:** For each column $k$, there are $2$ synchronizations.\n5.  **Collective Sizes:** Per column $k$, one `MPI_Allreduce` on a vector of length $k-1$, and one `MPI_Allreduce` on a scalar.\n\n**Modified Gram-Schmidt (MGS)**\n\nThe problem specifies a particular implementation: \"on column $k$, the algorithm orthogonalizes sequentially against $q_{1}, q_{2}, \\dots, q_{k-1}$, performing a global dot product per step (each as an MPI Allreduce on a scalar)\". Let the vector being processed be $v$, initialized to $a_k$.\n1.  **Orthogonalization:** For $j=1, \\dots, k-1$, the algorithm computes $R_{jk} = q_j^T v$ and updates $v \\leftarrow v - R_{jk} q_j$. Each dot product requires an `MPI_Allreduce` on a scalar, so this loop involves $k-1$ synchronizations. The flop cost for each step $j$ is $\\approx 2m$ for the dot product and $\\approx 2m$ for the update, totaling $\\approx 4m$. Over the loop, this is $\\approx 4m(k-1)$ flops.\n2.  **Normalization:** A final `MPI_Allreduce` on a scalar is performed for the norm $R_{kk} = \\|v\\|_2$. This is the $k$-th synchronization for column $k$.\n3.  **Total Flops:** The flop count is identical to CGS: $\\sum_{k=1}^{n} 4m(k-1) \\approx 2mn^2$.\n4.  **Total Synchronizations:** For each column $k$, there are $k-1+1 = k$ synchronizations.\n5.  **Collective Sizes:** All $k$ synchronizations for column $k$ operate on scalars.\n\n**CGS with Reorthogonalization (CGS2)**\n\nThis algorithm performs two passes of CGS for each column to improve numerical stability.\n1.  **Orthogonalization:** The problem states \"two CGS passes are performed (each with a single MPI Allreduce on a vector of length $k-1$)\". This means for column $k$, we perform a CGS step, and then another CGS step on the result.\n    - Pass 1: $v' = a_k - Q_{1:k-1}(Q_{1:k-1}^T a_k)$. This involves $1$ synchronization (vector `MPI_Allreduce`) and $\\approx 4m(k-1)$ flops.\n    - Pass 2: $v'' = v' - Q_{1:k-1}(Q_{1:k-1}^T v')$. This involves another synchronization (vector `MPI_Allreduce`) and another $\\approx 4m(k-1)$ flops.\n2.  **Normalization:** The final normalization $R_{kk} = \\|v''\\|_2$ requires a third synchronization (scalar `MPI_Allreduce`).\n3.  **Total Flops:** The flop count is double that of CGS: $\\approx 2 \\times (2mn^2) = 4mn^2$.\n4.  **Total Synchronizations:** For each column $k$, there are $2+1 = 3$ synchronizations.\n5.  **Collective Sizes:** Per column $k$, two `MPI_Allreduce` calls on vectors of length $k-1$, and one `MPI_Allreduce` on a scalar.\n\n**CholeskyQR**\n\nThis algorithm avoids orthogonalizing vectors directly.\n1.  **Gram Matrix Formation:** First, the Gram matrix $G = A^T A$ is computed. This $n \\times n$ matrix is formed by taking all $n(n+1)/2$ unique dot products between columns of $A$. The cost to compute the upper triangle of $G$ is $\\frac{n(n+1)}{2} \\times 2m \\approx mn^2$ flops. The problem states that the local contributions $A_p^T A_p$ are summed via \"a single MPI Allreduce on the symmetric $n \\times n$ matrix or its packed upper triangle of $\\frac{n(n+1)}{2}$ scalars\". This is $1$ global synchronization for the entire factorization process.\n2.  **Cholesky Factorization:** The Cholesky factorization $G=R^T R$ is computed. The matrix $G$ is small ($n \\times n$) and available on all processes after the reduction. This step is performed locally (or replicated everywhere) and costs $\\approx \\frac{1}{3}n^3$ flops. It requires no communication.\n3.  **Forming Q:** The matrix $Q$ is computed as $Q = AR^{-1}$. This is a triangular solve with $n$ columns of $A$ as multiple right-hand sides. Since $A$ is distributed by rows ($A_p$), each process computes its corresponding rows of $Q$ ($Q_p$) via the local operation $Q_p=A_p R^{-1}$. This costs $\\approx mn^2$ flops in total over all processes and requires no communication, as $R$ is replicated.\n4.  **Total Flops:** The total flop count is the sum of the steps: $mn^2$ (for $A^T A$) $+ \\frac{1}{3}n^3$ (for Cholesky) $+ mn^2$ (for $AR^{-1}$), which simplifies to $2mn^2 + \\frac{1}{3}n^3$.\n5.  **Total Synchronizations:** There is only $1$ global synchronization for the entire algorithm.\n6.  **Collective Size:** The single `MPI_Allreduce` operates on $\\approx n^2/2$ scalars.\n\n**Evaluation of Options**\n\n*   **Option A:**\n    - **Flops:** States CGS/MGS cost $\\approx 2mn^2$, CGS2 costs $\\approx 4mn^2$, CholeskyQR costs $\\approx 2mn^2 + \\frac{1}{3}n^3$. This matches our derivation.\n    - **Synchronizations & Sizes:** States CGS has $2$ syncs per column (vector of $k-1$, scalar), MGS has $k$ syncs per column (all scalar), CGS2 has $3$ syncs per column (two vectors of $k-1$, one scalar), and CholeskyQR has $1$ total sync (matrix of $\\frac{n(n+1)}{2}$ scalars). This perfectly matches our analysis based on the problem's explicit assumptions.\n    - **Verdict:** **Correct**.\n\n*   **Option B:**\n    - **Flops:** States CGS2 costs $\\approx 2mn^2$ and CholeskyQR costs $\\approx mn^2$. Both are incorrect.\n    - **Synchronizations:** States MGS can be done with $2$ synchronizations by batching, which contradicts the problem's premise for MGS. It also states CholeskyQR requires $n$ synchronizations, which contradicts the premise for CholeskyQR.\n    - **Verdict:** **Incorrect**.\n\n*   **Option C:**\n    - **Flops:** States MGS costs $\\approx 4mn^2$ and CGS2 costs $\\approx 2mn^2$. Both are incorrect.\n    - **Synchronizations:** Swaps the synchronization counts for CGS and MGS, and even then, the descriptions are inconsistent with the problem statement. The count for CGS2 is wrong ($2$ instead of $3$). The count for CholeskyQR is wrong ($O(n)$ instead of $1$).\n    - **Verdict:** **Incorrect**.\n\n*   **Option D:**\n    - **Flops:** The flop counts are stated correctly.\n    - **Synchronizations:** States CGS has $1$ sync per column, MGS has $2$, CGS2 has $2$, and CholeskyQR has $2$ total. All of these counts are incorrect and conflict with the derivations based on the explicit problem statement. For example, it assumes an optimized CGS implementation not described in the problem, misrepresents MGS, and incompletely counts syncs for CGS2 and CholeskyQR.\n    - **Verdict:** **Incorrect**.\n\nBased on a thorough analysis from first principles and adhering strictly to the implementation details provided in the problem statement, only Option A provides a completely accurate characterization of all four algorithms.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "In numerical linear algebra, speed is not the only virtue; numerical stability is equally paramount. This exercise  explores this crucial tension in the context of LU factorization, where pivoting strategies are essential for controlling error propagation. By comparing a communication-avoiding pivoting strategy with a more traditional but communication-heavy approach, you will confront the complex interplay between minimizing latency and guaranteeing a reliable, accurate solution, a central challenge in designing robust high-performance algorithms.",
            "id": "3537861",
            "problem": "Consider distributed-memory implementations of Lower-Upper (LU) factorization with pivoting for a dense matrix $A \\in \\mathbb{R}^{n \\times n}$ on a two-dimensional (2D) process grid of size $p_r \\times p_c$ using a block-cyclic distribution with panel width $b$. We compare two communication-avoiding pivoting strategies: Communication-Avoiding LU with tournament pivoting (CALU-TP) and an implementation that attempts to mimic classical rook pivoting (RP) in a distributed setting. We use the standard $\\alpha$–$\\beta$ communication model but focus solely on counting messages (i.e., the latency term associated with reductions and broadcasts), and we assume reduction trees of height $\\Theta(\\log p)$ for collectives over $p$ processes.\n\nDefinitions and modeling assumptions:\n- The growth factor $\\rho(A)$ for LU factorization is defined as $\\rho(A) = \\dfrac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|}$, where $U$ is the computed upper-triangular factor in exact arithmetic and $A = P L U$ with $P$ a permutation matrix and $L$ unit lower-triangular.\n- Classical partial pivoting is known to admit an adversarial family $W_n$ (for example, the Wilkinson matrix family) with $\\rho(W_n) = 2^{n-1}$.\n- Communication-Avoiding LU (CALU) with tournament pivoting selects $b$ pivot rows per panel by a hierarchical reduction of locally chosen candidates across the $p_r$ processes in the owning process column, using a reduction tree of height $\\Theta(\\log p_r)$; the selection is completed before applying the panel updates.\n- In a distributed realization of rook pivoting for a panel, the pivot search alternates between column-maximum and row-maximum queries until a “rook pivot” is found, i.e., an entry that is maximal in magnitude in its column and in its row within the current Schur complement. A global column-maximum requires a collective over $p_r$ processes in the process column, and a global row-maximum requires a collective over $p_c$ processes in the process row. We count each such collective as incurring $\\Theta(\\log p_r)$ or $\\Theta(\\log p_c)$ messages, respectively.\n- We call a matrix “adversarial for rook search” within a panel if the alternating row/column search requires a constant number $\\Theta(1)$ of alternations per pivot on average across the $b$ pivots in the panel (so the total number of alternations in the panel is $\\Theta(b)$), and we call a matrix “adversarial for tournament stability” if it triggers the same worst-case growth behavior known for partial pivoting.\n\nUnder these assumptions, evaluate the following statements about message counts and growth factors for CALU-TP and distributed rook pivoting (RP). Select all statements that are correct.\n\nA. For a single panel of width $b$, CALU-TP selects all $b$ pivots using a reduction tree across the $p_r$ processes in the owning process column, leading to a pivot-selection message count of $\\Theta(\\log p_r)$ per panel, independent of $b$.\n\nB. For a single panel of width $b$ on a matrix that is adversarial for rook search as defined above, any distributed RP implementation that follows the classical alternating search without speculative lookahead must perform at least $\\Omega\\!\\big(b \\, (\\log p_r + \\log p_c)\\big)$ messages for pivot selection in that panel.\n\nC. On the adversarial family $W_n$ with $\\rho(W_n) = 2^{n-1}$ under partial pivoting, CALU-TP is guaranteed to achieve strictly subexponential growth, i.e., $\\rho(W_n) = o(2^{n})$, because the hierarchical tournament avoids worst-case pivot choices.\n\nD. For every $n \\times n$ matrix $A$, rook pivoting guarantees a polynomial growth factor bound $\\rho(A) \\le n^{c}$ for some absolute constant $c$ independent of $A$ and $n$; hence, RP always improves the worst-case bound of partial pivoting asymptotically.\n\nAnswer choices:\n- A\n- B\n- C\n- D",
            "solution": "This problem requires an evaluation of four statements regarding the communication cost (latency) and numerical stability (growth factor) of two pivoting strategies for LU factorization: Communication-Avoiding LU with tournament pivoting (CALU-TP) and a distributed version of rook pivoting (RP). We analyze each statement based on the provided models and definitions.\n\n**Analysis of Statement A:**\nThis statement concerns the message count for pivot selection in CALU-TP. The algorithm is defined to select all $b$ pivots for a panel via a single hierarchical reduction across the $p_r$ processes in the owning column. Such a reduction, implemented with a reduction tree, has a number of communication steps proportional to the tree's height. For $p_r$ processes, the height is $\\Theta(\\log p_r)$. Since we are only counting messages (latency), the cost is determined by the number of steps in the collective operation. This number depends on $p_r$, not on the panel width $b$. Thus, the message count per panel is $\\Theta(\\log p_r)$.\nVerdict: **Correct**.\n\n**Analysis of Statement B:**\nThis statement concerns the message count for distributed rook pivoting. RP finds pivots one by one. To find a single rook pivot, it alternates between column- and row-maximum searches. A column search is a collective over $p_r$ processes, costing $\\Theta(\\log p_r)$ messages. A row search is a collective over $p_c$ processes, costing $\\Theta(\\log p_c)$ messages. For an \"adversarial for rook search\" matrix, finding one pivot takes $\\Theta(1)$ such alternations, for a message cost of $\\Theta(\\log p_r + \\log p_c)$. Since the classical approach finds $b$ pivots sequentially for the panel, the total message count is $b$ times the cost per pivot, which is $\\Theta(b (\\log p_r + \\log p_c))$. The statement claims a lower bound of $\\Omega(b (\\log p_r + \\log p_c))$, which is consistent with this analysis.\nVerdict: **Correct**.\n\n**Analysis of Statement C:**\nThis statement claims that CALU-TP is guaranteed to be more stable than classical partial pivoting (GEPP) on the adversarial matrix family $W_n$. Tournament pivoting is a block version of the GEPP heuristic, selecting pivots based on magnitude. There is no theoretical proof that this block strategy fundamentally avoids the worst-case pivot choices that lead to exponential growth in GEPP. In fact, the stability analysis for CALU-TP shows that it has the same worst-case growth factor bound of $2^{n-1}$ as GEPP. The assertion that it is \"guaranteed\" to achieve \"strictly subexponential\" growth is a strong claim that is not supported by established theory and is scientifically incorrect.\nVerdict: **Incorrect**.\n\n**Analysis of Statement D:**\nThis statement claims that rook pivoting is \"guaranteed\" to have a polynomial growth factor bound. Whether rook pivoting has a polynomial growth factor is a famous, long-standing open problem in numerical linear algebra. It is a conjecture, not a proven fact. Current proven bounds are weaker than polynomial, and no proof of a polynomial bound exists. Stating an open conjecture as a guaranteed property is a significant scientific inaccuracy.\nVerdict: **Incorrect**.\n\nBased on the analysis, only statements A and B are correct descriptions of the algorithms under the given model.",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}