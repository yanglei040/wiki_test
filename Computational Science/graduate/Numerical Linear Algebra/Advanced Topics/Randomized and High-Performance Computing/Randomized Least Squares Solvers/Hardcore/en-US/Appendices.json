{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for employing randomized algorithms is often a dramatic reduction in computational cost. This first practice provides a concrete analysis of this trade-off by comparing a modern 'sketch-and-solve' approach against a classical deterministic method. By deriving the exact floating-point operation counts for both pipelines , you will gain a quantitative understanding of when and why randomized solvers can offer a significant performance advantage, especially for large-scale, sparse problems.",
            "id": "3570172",
            "problem": "Consider a tall-and-skinny least squares problem with matrix $A \\in \\mathbb{R}^{n \\times d}$, $n \\gg d$, and vector $b \\in \\mathbb{R}^{n}$. Let $\\operatorname{nnz}(A)$ denote the number of nonzero entries of $A$. You employ a sketch-and-solve pipeline using an oblivious sparse subspace embedding $S \\in \\mathbb{R}^{m \\times n}$ that has exactly $s$ nonzeros per column, with signs in $\\{-1,+1\\}$ and uniformly random row locations. You form the sketched design matrix $SA \\in \\mathbb{R}^{m \\times d}$ and solve the reduced problem $\\min_{x \\in \\mathbb{R}^{d}} \\|SAx - Sb\\|_{2}$ by Householder orthogonal-triangular (QR) factorization on $SA$. Assume the following well-tested performance models and facts from numerical linear algebra:\n- Each nonzero of $A$ contributes $s$ updates to the corresponding entries of $SA$ when computing $SA = SA(A)$ via the sparse embedding application, so the arithmetic cost to form $SA$ is proportional to $s \\cdot \\operatorname{nnz}(A)$.\n- The Householder orthogonal-triangular (QR) factorization of a dense matrix in $\\mathbb{R}^{m \\times d}$ with $m \\ge d$ costs $2md^{2} - \\tfrac{2}{3}d^{3}$ floating-point operations.\n- A classical Householder orthogonal-triangular (QR) factorization on the original (generally dense after fill-in) $A \\in \\mathbb{R}^{n \\times d}$ costs $2nd^{2} - \\tfrac{2}{3}d^{3}$ floating-point operations.\n\nUsing these bases, derive the exact arithmetic operation counts, as symbolic expressions in terms of $n$, $d$, $m$, $s$, and $\\operatorname{nnz}(A)$, for:\n1. The total number of floating-point operations to form $SA$ and solve the reduced problem by Householder orthogonal-triangular (QR) on $SA$.\n2. The number of floating-point operations for the classical Householder orthogonal-triangular (QR) on $A$.\n\nExpress your final answer as a single row matrix containing two entries corresponding to items 1 and 2 above. Your answer should be a closed-form analytic expression in terms of $n$, $d$, $m$, $s$, and $\\operatorname{nnz}(A)$, in units of floating-point operations. No rounding is required.",
            "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- A tall-and-skinny least squares problem with matrix $A \\in \\mathbb{R}^{n \\times d}$ and vector $b \\in \\mathbb{R}^{n}$.\n- Condition: $n \\gg d$.\n- Notation: $\\operatorname{nnz}(A)$ is the number of nonzero entries of $A$.\n- Method: A sketch-and-solve pipeline using an oblivious sparse subspace embedding $S \\in \\mathbb{R}^{m \\times n}$.\n- Properties of $S$: Exactly $s$ nonzeros per column, with values in $\\{-1,+1\\}$ and uniformly random row locations.\n- Sketched problem: $\\min_{x \\in \\mathbb{R}^{d}} \\|SAx - Sb\\|_{2}$.\n- Solution method for the sketched problem: Householder orthogonal-triangular (QR) factorization on the matrix $SA \\in \\mathbb{R}^{m \\times d}$.\n- Cost Model 1: The arithmetic cost to form $SA$ is proportional to $s \\cdot \\operatorname{nnz}(A)$, based on the fact that each nonzero of $A$ contributes $s$ updates.\n- Cost Model 2: The Householder QR factorization of a dense matrix in $\\mathbb{R}^{m \\times d}$ with $m \\ge d$ costs $2md^{2} - \\frac{2}{3}d^{3}$ floating-point operations.\n- Cost Model 3: The classical Householder QR factorization on $A \\in \\mathbb{R}^{n \\times d}$ costs $2nd^{2} - \\frac{2}{3}d^{3}$ floating-point operations.\n- Required: Derive exact arithmetic operation counts for:\n  1. Forming $SA$ and solving the reduced problem by Householder QR on $SA$.\n  2. The classical Householder QR on $A$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically sound. It is grounded in the established principles of randomized numerical linear algebra, specifically the \"sketch-and-solve\" method for overdetermined least squares problems. The provided cost models for Householder QR factorization are standard, representing the dominant term in the operation count. The description of the sparse embedding and the cost model for its application are consistent with state-of-the-art literature (e.g., Clarkson-Woodruff sketch). All necessary variables ($n, d, m, s, \\operatorname{nnz}(A)$) are defined. The problem is objective and formalizable.\n\nThe phrase \"solve the reduced problem\" requires careful interpretation. The full solution process involves forming $Sb$, performing the QR factorization of $SA$, applying the orthogonal transformation to $Sb$, and solving the resulting triangular system. The problem provides a specific cost model only for the factorization step ($2md^2 - \\frac{2}{3}d^3$). The parallel structure of the questions, where the cost for the classical method is also given just for the factorization, strongly implies that \"solve by QR\" refers to this dominant factorization stage. However, setting up the reduced problem also requires forming $Sb$. The cost for forming $Sb$ is not explicitly given but can be directly derived from the properties of $S$ in a manner analogous to the formation of $SA$. A competent student is expected to deduce this cost. The problem is therefore deemed complete and well-posed.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived based on the provided models and logical deductions.\n\n### Derivation of Operation Counts\n\nThe problem requires the calculation of two distinct operation counts.\n\n**1. Total operations for the sketch-and-solve method**\n\nThe total number of floating-point operations for this method is the sum of three components:\n(a) The cost of forming the sketched matrix $SA$.\n(b) The cost of forming the sketched vector $Sb$.\n(c) The cost of solving the reduced least squares problem using Householder QR factorization on $SA$.\n\n(a) Cost to form $SA$:\nThe sketching matrix $S \\in \\mathbb{R}^{m \\times n}$ has $s$ non-zero entries per column, which are either $+1$ or $-1$. The formation of the product $SA$ can be viewed from the perspective of the non-zero entries of $A$. For each non-zero entry $A_{ij}$, it participates in the computation of the $j$-th column of $SA$. Specifically, the value $A_{ij}$ is multiplied by the $i$-th column of $S$. Since the $i$-th column of $S$ has $s$ non-zero entries, $A_{ij}$ contributes to $s$ entries in the $j$-th column of $SA$. Each such contribution is an update of the form $(SA)_{k,j} \\leftarrow (SA)_{k,j} + S_{k,i}A_{ij}$. Since $S_{k,i} \\in \\{-1, +1\\}$, this is an addition or a subtraction, which counts as one floating-point operation. As there are $\\operatorname{nnz}(A)$ non-zero entries in $A$, the total cost is:\n$$ \\text{Cost}(SA) = s \\cdot \\operatorname{nnz}(A) $$\n\n(b) Cost to form $Sb$:\nThis cost must be included as part of setting up the reduced problem. The vector $b \\in \\mathbb{R}^n$ is typically dense. The product $Sb$ is computed by multiplying the sparse matrix $S$ by the vector $b$. For each element $b_i$ of the vector $b$ (for $i=1, \\dots, n$), it is multiplied by the $i$-th column of $S$. This column has $s$ non-zero entries. Thus, $b_i$ is added or subtracted to $s$ entries in the resulting vector $Sb$. This process is repeated for all $n$ elements of $b$. The total number of floating-point operations is:\n$$ \\text{Cost}(Sb) = s \\cdot n $$\n\n(c) Cost to solve by QR on $SA$:\nThe matrix $SA$ is in $\\mathbb{R}^{m \\times d}$. The problem explicitly provides the cost for the Householder QR factorization of such a matrix as $2md^{2} - \\frac{2}{3}d^{3}$. As reasoned during validation, we use this as the cost to \"solve\" the system via QR.\n$$ \\text{Cost}(\\text{QR on } SA) = 2md^{2} - \\frac{2}{3}d^{3} $$\n\nThe total cost for the first part is the sum of these three components:\n$$ \\text{Total Cost}_1 = \\text{Cost}(SA) + \\text{Cost}(Sb) + \\text{Cost}(\\text{QR on } SA) = s \\cdot \\operatorname{nnz}(A) + sn + 2md^{2} - \\frac{2}{3}d^{3} $$\n\n**2. Operations for the classical Householder QR method**\n\nThe problem asks for the number of floating-point operations for the classical Householder QR factorization on the original matrix $A \\in \\mathbb{R}^{n \\times d}$. This cost is given directly in the problem statement as:\n$$ \\text{Total Cost}_2 = 2nd^{2} - \\frac{2}{3}d^{3} $$\n\nThese two expressions represent the final answers for the two parts of the problem. They will be presented as a row matrix as requested.",
            "answer": "$$ \\boxed{\\begin{pmatrix} s \\operatorname{nnz}(A) + sn + 2md^{2} - \\frac{2}{3}d^{3} & 2nd^{2} - \\frac{2}{3}d^{3} \\end{pmatrix}} $$"
        },
        {
            "introduction": "Having established that sketching can be computationally cheaper, we now explore the theoretical underpinnings that guarantee accuracy. The choice of the sketching distribution is critical, and this exercise contrasts naive uniform sampling with the more sophisticated leverage-score sampling. By deriving the sufficient sample complexities for both methods from foundational principles , you will uncover how data-dependent probabilities exploit the geometric structure of the matrix to achieve superior efficiency.",
            "id": "3570168",
            "problem": "Consider a tall matrix $A \\in \\mathbb{R}^{n \\times d}$ with full column rank $d$ and $n \\geq d$. Let $A = Q R$ be the thin QR factorization, where $Q \\in \\mathbb{R}^{n \\times d}$ has orthonormal columns and $R \\in \\mathbb{R}^{d \\times d}$ is invertible. Define the leverage scores $\\ell_i = \\|Q_{i,:}\\|_2^2$ for $i \\in \\{1, \\dots, n\\}$ and the coherence $\\mu = \\max_{i} \\ell_i$. In a randomized sketch-and-solve least squares method, form a sketch by sampling $m$ rows with replacement using a distribution $p = (p_1, \\dots, p_n)$ and reweighting so that the sketching matrix $S \\in \\mathbb{R}^{m \\times n}$ has rows $e_{i_j}^{\\top} / \\sqrt{m p_{i_j}}$, where $e_{i_j}$ is the $i_j$-th standard basis vector and $i_j \\sim p$ independently for $j = 1, \\dots, m$. A sufficient condition for accurate solution is that $S$ be a $(1 \\pm \\varepsilon)$ subspace embedding for the column space of $A$, i.e., for all $x \\in \\mathbb{R}^d$,\n$$(1 - \\varepsilon)\\|A x\\|_2^2 \\leq \\|S A x\\|_2^2 \\leq (1 + \\varepsilon)\\|A x\\|_2^2,$$\nwith failure probability at most $\\delta$.\n\nStarting from the following foundational facts:\n- $Q^{\\top} Q = I_d$,\n- $\\sum_{i=1}^{n} \\ell_i = d$,\n- $\\mathbb{E}\\!\\left[Q^{\\top} S^{\\top} S Q\\right] = I_d$,\n- The matrix Chernoff inequality controls the concentration of sums of independent positive semidefinite random matrices in terms of uniform bounds on their spectral norms and their expectations,\n\nderive, for a universal constant $c > 0$, sufficient sample sizes $m$ ensuring the subspace embedding property for the following two choices of the sampling distribution $p$:\n(a) Leverage-score sampling: $p_i = \\ell_i / \\sum_{j=1}^{n} \\ell_j$,\n(b) Uniform sampling: $p_i = 1/n$.\n\nAssume a matrix with high coherence in the sense that $\\mu$ is near $1$, and express the dependence of the sample sizes on $\\varepsilon$, $\\delta$, $n$, $d$, and $\\mu$. Finally, compute and provide the simplified symbolic expression for the ratio $m_{\\mathrm{uniform}} / m_{\\mathrm{lev}}$ as a function of $n$, $d$, and $\\mu$. The final answer must be the single expression for this ratio. No numerical approximation is required.",
            "solution": "The problem asks for the derivation of sufficient sample sizes for randomized least squares solvers using two different sampling distributions, and then to compute the ratio of these sample sizes. The analysis relies on the concept of a subspace embedding and is carried out using a matrix Chernoff concentration inequality.\n\nFirst, we formalize the subspace embedding condition. A matrix $S \\in \\mathbb{R}^{m \\times n}$ is a $(1 \\pm \\varepsilon)$ subspace embedding for the column space of $A \\in \\mathbb{R}^{n \\times d}$ if, for all $x \\in \\mathbb{R}^d$, the following holds:\n$$ (1 - \\varepsilon)\\|A x\\|_2^2 \\leq \\|S A x\\|_2^2 \\leq (1 + \\varepsilon)\\|A x\\|_2^2 $$\nGiven the thin QR factorization $A=QR$, where $Q \\in \\mathbb{R}^{n \\times d}$ has orthonormal columns and $R \\in \\mathbb{R}^{d \\times d}$ is invertible, we can make a change of variables. Let $y = Rx$. Since $R$ is invertible, $y$ spans $\\mathbb{R}^d$ as $x$ spans $\\mathbb{R}^d$. The condition can be rewritten in terms of $Q$ and $y$:\n$$ \\|Ax\\|_2^2 = \\|QRx\\|_2^2 = \\|Qy\\|_2^2 = y^{\\top}Q^{\\top}Qy = y^{\\top}I_d y = \\|y\\|_2^2 $$\n$$ \\|SAx\\|_2^2 = \\|SQRx\\|_2^2 = \\|SQy\\|_2^2 = y^{\\top}Q^{\\top}S^{\\top}SQy $$\nSubstituting these into the inequality gives:\n$$ (1 - \\varepsilon)\\|y\\|_2^2 \\leq y^{\\top}Q^{\\top}S^{\\top}SQy \\leq (1 + \\varepsilon)\\|y\\|_2^2 $$\nThis must hold for all $y \\in \\mathbb{R}^d$. This is equivalent to stating that all eigenvalues of the matrix $X = Q^{\\top}S^{\\top}SQ$ lie in the interval $[1-\\varepsilon, 1+\\varepsilon]$. In terms of the spectral norm, this is equivalent to $\\|X - I_d\\|_2 \\le \\varepsilon$.\n\nNext, we analyze the structure of the random matrix $X$. The sketching matrix $S$ is formed by $m$ independent draws. Specifically, its $j$-th row (for $j \\in \\{1, \\dots, m\\}$) is given by $\\frac{1}{\\sqrt{m p_{i_j}}} e_{i_j}^{\\top}$, where $i_j$ is an index sampled from $\\{1, \\dots, n\\}$ according to the probability distribution $p=(p_1, \\dots, p_n)$. The matrix $S^{\\top}S$ can be written as a sum over the outer products of its rows:\n$$ S^{\\top}S = \\sum_{j=1}^{m} \\left( \\frac{1}{\\sqrt{m p_{i_j}}} e_{i_j} \\right) \\left( \\frac{1}{\\sqrt{m p_{i_j}}} e_{i_j}^{\\top} \\right) = \\frac{1}{m} \\sum_{j=1}^{m} \\frac{e_{i_j}e_{i_j}^{\\top}}{p_{i_j}} $$\nThen, the matrix $X$ is:\n$$ X = Q^{\\top}S^{\\top}SQ = Q^{\\top} \\left( \\frac{1}{m} \\sum_{j=1}^{m} \\frac{e_{i_j}e_{i_j}^{\\top}}{p_{i_j}} \\right) Q = \\frac{1}{m} \\sum_{j=1}^{m} \\frac{Q^{\\top}e_{i_j}e_{i_j}^{\\top}Q}{p_{i_j}} $$\nLet $Q_{k,:}$ denote the $k$-th row of $Q$. Then $e_k^{\\top}Q = Q_{k,:}$. So, we can write $X$ as a sum of independent, identically distributed random matrices:\n$$ X = \\sum_{j=1}^{m} X_j, \\quad \\text{where} \\quad X_j = \\frac{1}{m p_{i_j}} (Q_{i_j,:})^{\\top} Q_{i_j,:} $$\nThe problem states that $\\mathbb{E}[X] = I_d$, which is a crucial property we can verify:\n$$ \\mathbb{E}[X_j] = \\sum_{k=1}^{n} p_k \\left( \\frac{1}{m p_k} (Q_{k,:})^{\\top} Q_{k,:} \\right) = \\frac{1}{m} \\sum_{k=1}^{n} (Q_{k,:})^{\\top} Q_{k,:} = \\frac{1}{m} Q^{\\top}Q = \\frac{1}{m} I_d $$\nThe expectation of the sum is $\\mathbb{E}[X] = \\sum_{j=1}^m \\mathbb{E}[X_j] = m \\left( \\frac{1}{m} I_d \\right) = I_d$.\n\nTo bound the deviation of $X$ from its mean $I_d$, we use a matrix Chernoff concentration inequality. A standard version of this inequality for a sum of independent positive semidefinite (PSD) random matrices $X_j$ states that for a failure probability of at most $\\delta$, it is sufficient to choose the number of samples $m$ such that:\n$$ m \\ge c \\cdot \\frac{K}{\\varepsilon^2} \\ln(2d/\\delta) $$\nfor some universal constant $c > 0$, where $K$ is a parameter that depends on the properties of the random matrices. In this context, the parameter $K$ is related to the maximum spectral norm of the individual random matrices being summed. Let's analyze $X = \\frac{1}{m}\\sum_j Z_j$ where $Z_j = \\frac{1}{p_{i_j}} (Q_{i_j,:})^{\\top}Q_{i_j,:}$ are i.i.d. with $\\mathbb{E}[Z_j]=I_d$. The relevant parameter for the Chernoff bound is the maximum spectral norm of any single term $Z_j$. Let this bound be $R$.\n$$ R = \\max_{i \\in \\{1,\\dots,n\\}} \\left\\| \\frac{1}{p_i} (Q_{i,:})^{\\top}Q_{i,:} \\right\\|_2 $$\nThe matrix $(Q_{i,:})^{\\top}Q_{i,:}$ is a rank-$1$ matrix. Its only non-zero eigenvalue is $\\operatorname{Tr}((Q_{i,:})^{\\top}Q_{i,:}) = Q_{i,:}(Q_{i,:})^{\\top} = \\|Q_{i,:}\\|_2^2 = \\ell_i$. Thus, its spectral norm is $\\ell_i$.\nSo, the parameter $R$ is given by:\n$$ R = \\max_{i \\in \\{1,\\dots,n\\}} \\frac{\\ell_i}{p_i} $$\nA sufficient sample size is therefore $m = O\\left(\\frac{R \\ln(d/\\delta)}{\\varepsilon^2}\\right)$. Using the constant $c$ provided in the problem prompt:\n$$ m \\ge c \\cdot (\\max_i \\frac{\\ell_i}{p_i}) \\cdot \\frac{\\ln(d/\\delta)}{\\varepsilon^2} $$\n(Note: The constant $2$ in $\\ln(2d/\\delta)$ is absorbed into the universal constant $c$). We now apply this formula to the two specified sampling distributions.\n\n(a) Leverage-score sampling:\nThe sampling probabilities are defined as $p_i = \\ell_i / \\sum_{j=1}^n \\ell_j$. From the given facts, $\\sum_{j=1}^n \\ell_j = d$. Thus, $p_i = \\ell_i / d$.\nWe compute the parameter $R_{\\mathrm{lev}}$:\n$$ R_{\\mathrm{lev}} = \\max_i \\frac{\\ell_i}{p_i} = \\max_i \\frac{\\ell_i}{\\ell_i / d} = \\max_i d = d $$\nThe sufficient sample size $m_{\\mathrm{lev}}$ is:\n$$ m_{\\mathrm{lev}} = c \\frac{d \\ln(d/\\delta)}{\\varepsilon^2} $$\n\n(b) Uniform sampling:\nThe sampling probabilities are $p_i = 1/n$ for all $i \\in \\{1, \\dots, n\\}$.\nWe compute the parameter $R_{\\mathrm{uniform}}$:\n$$ R_{\\mathrm{uniform}} = \\max_i \\frac{\\ell_i}{p_i} = \\max_i \\frac{\\ell_i}{1/n} = n \\max_i \\ell_i $$\nThe problem defines the coherence as $\\mu = \\max_i \\ell_i$. Therefore, $R_{\\mathrm{uniform}} = n\\mu$.\nThe sufficient sample size $m_{\\mathrm{uniform}}$ is:\n$$ m_{\\mathrm{uniform}} = c \\frac{n \\mu \\ln(d/\\delta)}{\\varepsilon^2} $$\nThe problem statement notes to consider a case of high coherence where $\\mu$ is near $1$, which highlights the scenario where uniform sampling can be inefficient compared to leverage score sampling. The dependence on $\\mu$ captures this.\n\nFinally, we compute the ratio $m_{\\mathrm{uniform}} / m_{\\mathrm{lev}}$:\n$$ \\frac{m_{\\mathrm{uniform}}}{m_{\\mathrm{lev}}} = \\frac{c \\frac{n \\mu \\ln(d/\\delta)}{\\varepsilon^2}}{c \\frac{d \\ln(d/\\delta)}{\\varepsilon^2}} $$\nThe terms $c$, $\\varepsilon^2$, and $\\ln(d/\\delta)$ cancel out, leaving:\n$$ \\frac{m_{\\mathrm{uniform}}}{m_{\\mathrm{lev}}} = \\frac{n\\mu}{d} $$\nThis ratio quantifies the improvement in sample complexity gained by using leverage score sampling over uniform sampling. Since $\\sum_i \\ell_i = d$, we have $d = \\sum_i \\ell_i \\le n \\max_i \\ell_i = n\\mu$, which implies $\\mu \\ge d/n$. Consequently, the ratio $\\frac{n\\mu}{d} \\ge \\frac{n(d/n)}{d} = 1$, confirming that leverage score sampling is, in this theoretical bound, always at least as efficient as uniform sampling. The improvement is significant when the coherence $\\mu$ is much larger than its minimum possible value of $d/n$.",
            "answer": "$$\n\\boxed{\\frac{n\\mu}{d}}\n$$"
        },
        {
            "introduction": "Theoretical concepts like leverage scores and coherence become much clearer when explored through a concrete example. This practice challenges you to construct a specific 'worst-case' design matrix where the leverage is concentrated on a single data point, a classic scenario where uniform sampling performs poorly. By calculating the leverage scores and coherence from first principles for this matrix , you will see firsthand how these abstract geometric measures manifest in a tangible way and directly quantify the advantage of using a data-aware sampling strategy.",
            "id": "3570188",
            "problem": "Consider the overdetermined least squares problem with design matrix $A \\in \\mathbb{R}^{n \\times k}$ and response vector $b \\in \\mathbb{R}^{n}$, where the goal is to compute $x^{\\star} \\in \\arg\\min_{x \\in \\mathbb{R}^{k}} \\|A x - b\\|_{2}$. Assume a sketch-and-solve approach in which a sampling-and-reweighting matrix $S \\in \\mathbb{R}^{m \\times n}$ is used to form a sketched problem $\\min_{x \\in \\mathbb{R}^{k}} \\|S A x - S b\\|_{2}$. The sampling probabilities are denoted by $p_{i}$ for $i \\in \\{1,2,\\dots,n\\}$, and each selected row is reweighted by the factor $(m p_{i})^{-1/2}$.\n\nDefine the row leverage scores $\\ell_{i}$ as the diagonal entries of the orthogonal projector onto the column space of $A$, equivalently $\\ell_{i} = \\|U_{i,:}\\|_{2}^{2}$ where $A = U R$ is the thin factorization with $U \\in \\mathbb{R}^{n \\times k}$ having orthonormal columns and $R \\in \\mathbb{R}^{k \\times k}$ upper triangular. The coherence of $A$ is defined as $\\mu(A) = \\frac{n}{k} \\max_{i} \\ell_{i}$.\n\nConstruct a worst-case coherent design using polynomial features: take $k = 2$ and let $A$ be formed from the degree-$1$ monomial features, that is, $A$ has two columns corresponding to the functions $x \\mapsto 1$ and $x \\mapsto x$. Let the $n$ input locations be $x_{1}, \\dots, x_{n} \\in \\{-1, +1\\}$ with exactly one location equal to $-1$ and the remaining $n-1$ locations equal to $+1$. Using the fundamental definitions of least squares, leverage scores, and coherence, determine the coherence $\\mu(A)$ for this design.\n\nUsing the notion of subspace embedding for $(1 \\pm \\varepsilon)$-distortion with failure probability at most $\\delta$, and standard sampling-and-reweighting constructions, argue from first principles that the smallest sufficient sample size for uniform sampling (i.e., $p_{i} = 1/n$ for all $i$) scales as $m_{\\mathrm{unif}} \\propto k \\ln(k/\\delta) \\mu(A) \\varepsilon^{-2}$, while the smallest sufficient sample size for leverage-score sampling (i.e., $p_{i} = \\ell_{i}/k$) scales as $m_{\\mathrm{lev}} \\propto k \\ln(k/\\delta) \\varepsilon^{-2}$. Conclude the exact asymptotic gain factor, defined as the ratio $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$ with identical $(\\varepsilon, \\delta)$, for the above polynomial design, and express your final answer as a single closed-form analytic expression in terms of $n$ only. No physical units are involved. No rounding is required.",
            "solution": "The user wants to find the asymptotic gain factor of leverage-score sampling over uniform sampling for a specific least squares problem.\n\n### Step 1: Extract Givens\n- Problem type: Overdetermined least squares, $\\min_{x \\in \\mathbb{R}^{k}} \\|A x - b\\|_{2}$.\n- Design matrix: $A \\in \\mathbb{R}^{n \\times k}$.\n- Response vector: $b \\in \\mathbb{R}^{n}$.\n- Sketching matrix: $S \\in \\mathbb{R}^{m \\times n}$, a sampling-and-reweighting matrix.\n- Sketched problem: $\\min_{x \\in \\mathbb{R}^{k}} \\|S A x - S b\\|_{2}$.\n- Sampling probabilities: $p_{i}$ for $i \\in \\{1, 2, \\dots, n\\}$.\n- Reweighting factor: $(m p_{i})^{-1/2}$.\n- Leverage scores: $\\ell_{i} = \\|U_{i,:}\\|_{2}^{2}$, where $A = U R$ is the thin QR factorization of $A$, with $U \\in \\mathbb{R}^{n \\times k}$ having orthonormal columns.\n- Coherence: $\\mu(A) = \\frac{n}{k} \\max_{i} \\ell_{i}$.\n- Specific design:\n    - $k=2$.\n    - Columns of $A$ are basis functions $x \\mapsto 1$ and $x \\mapsto x$.\n    - Input locations $x_{1}, \\dots, x_{n} \\in \\{-1, +1\\}$.\n    - Exactly one location is $-1$, the other $n-1$ are $+1$. Let $x_{1}=-1$ and $x_{i}=+1$ for $i=2, \\dots, n$.\n- Sample size scalings for $(1 \\pm \\varepsilon)$-distortion with failure probability $\\delta$:\n    - Uniform sampling ($p_{i} = 1/n$): $m_{\\mathrm{unif}} \\propto k \\ln(k/\\delta) \\mu(A) \\varepsilon^{-2}$.\n    - Leverage-score sampling ($p_{i} = \\ell_{i}/k$): $m_{\\mathrm{lev}} \\propto k \\ln(k/\\delta) \\varepsilon^{-2}$.\n- Goal: Determine the exact asymptotic gain factor, defined as the ratio $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It is based on standard, well-established concepts in numerical linear algebra and randomized algorithms, including least squares, QR factorization, statistical leverage scores, coherence, and sketch-and-solve methods (specifically sampling and reweighting). The definitions provided are standard in the field. The setup for the design matrix $A$ is explicit and allows for direct computation. The question is well-posed, asking for a specific, computable quantity (the coherence $\\mu(A)$) and a subsequent ratio based on established theoretical results. The problem is self-contained and free of contradictions or ambiguities. Therefore, it is a valid problem.\n\n### Step 3: Proceed to Solution\n\nThe solution involves three main parts:\n1.  Construct the matrix $A$ and compute its thin QR factorization to find the matrix $U$.\n2.  Use the rows of $U$ to compute the leverage scores $\\ell_{i}$ and then the coherence $\\mu(A)$.\n3.  Use the provided scaling laws for sample sizes to determine the asymptotic gain factor $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$.\n\n**Part 1: Constructing $A$ and finding $U$**\n\nThe matrix $A$ has $n$ rows and $k=2$ columns. The first column is a vector of all ones, corresponding to the basis function $x \\mapsto 1$. The second column corresponds to the basis function $x \\mapsto x$ evaluated at the points $x_1 = -1, x_2 = 1, \\dots, x_n = 1$.\n$$\nA = \\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n\\vdots & \\vdots \\\\\n1 & 1\n\\end{pmatrix}\n$$\nLet the columns of $A$ be $a_1$ and $a_2$. We find the orthonormal basis for the column space of $A$ using the Gram-Schmidt process to obtain the columns $u_1, u_2$ of the matrix $U$.\n\nThe first column of $A$ is $a_1 = [1, 1, \\dots, 1]^T$. Its squared norm is $\\|a_1\\|_2^2 = \\sum_{i=1}^n 1^2 = n$.\nThe first orthonormal vector $u_1$ is:\n$$\nu_1 = \\frac{a_1}{\\|a_1\\|_2} = \\frac{1}{\\sqrt{n}} \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n$$\nThe second column of $A$ is $a_2 = [-1, 1, \\dots, 1]^T$. We first find the component of $a_2$ orthogonal to $u_1$, denoted $w_2$.\n$$\nw_2 = a_2 - \\langle a_2, u_1 \\rangle u_1\n$$\nThe inner product is:\n$$\n\\langle a_2, u_1 \\rangle = a_2^T u_1 = \\frac{1}{\\sqrt{n}} \\left( (-1)(1) + (n-1)(1)(1) \\right) = \\frac{n-2}{\\sqrt{n}}\n$$\nSo, $w_2$ is:\n$$\nw_2 = a_2 - \\frac{n-2}{\\sqrt{n}} u_1 = a_2 - \\frac{n-2}{n} a_1 = \\begin{pmatrix} -1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} - \\frac{n-2}{n} \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 - \\frac{n-2}{n} \\\\ 1 - \\frac{n-2}{n} \\\\ \\vdots \\\\ 1 - \\frac{n-2}{n} \\end{pmatrix} = \\begin{pmatrix} \\frac{-n-n+2}{n} \\\\ \\frac{n-n+2}{n} \\\\ \\vdots \\\\ \\frac{2}{n} \\end{pmatrix} = \\frac{2}{n} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n$$\nNext, we normalize $w_2$ to get $u_2$. The squared norm of $w_2$ is:\n$$\n\\|w_2\\|_2^2 = \\left(\\frac{2}{n}\\right)^2 \\left( (1-n)^2 + \\sum_{i=2}^n 1^2 \\right) = \\frac{4}{n^2} \\left( (n-1)^2 + n-1 \\right) = \\frac{4}{n^2} (n-1)(n-1+1) = \\frac{4(n-1)n}{n^2} = \\frac{4(n-1)}{n}\n$$\nThe norm is $\\|w_2\\|_2 = \\sqrt{\\frac{4(n-1)}{n}} = \\frac{2\\sqrt{n-1}}{\\sqrt{n}}$.\nThe second orthonormal vector $u_2$ is:\n$$\nu_2 = \\frac{w_2}{\\|w_2\\|_2} = \\frac{\\frac{2}{n} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}}{\\frac{2\\sqrt{n-1}}{\\sqrt{n}}} = \\frac{\\sqrt{n}}{n\\sqrt{n-1}} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{n(n-1)}} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n$$\nThe matrix $U$ with orthonormal columns is $U = [u_1 | u_2]$.\n\n**Part 2: Computing Leverage Scores and Coherence**\n\nThe leverage score $\\ell_i$ is the squared Euclidean norm of the $i$-th row of $U$, denoted $U_{i,:}$.\nThe first row of $U$, corresponding to $x_1 = -1$, is:\n$$\nU_{1,:} = \\left( \\frac{1}{\\sqrt{n}}, \\frac{1-n}{\\sqrt{n(n-1)}} \\right)\n$$\nIts leverage score $\\ell_1$ is:\n$$\n\\ell_1 = \\|U_{1,:}\\|_2^2 = \\left(\\frac{1}{\\sqrt{n}}\\right)^2 + \\left(\\frac{1-n}{\\sqrt{n(n-1)}}\\right)^2 = \\frac{1}{n} + \\frac{(1-n)^2}{n(n-1)} = \\frac{1}{n} + \\frac{(n-1)^2}{n(n-1)} = \\frac{1}{n} + \\frac{n-1}{n} = \\frac{n}{n} = 1\n$$\nFor any other row $i \\in \\{2, \\dots, n\\}$, corresponding to $x_i = 1$, the row of $U$ is:\n$$\nU_{i,:} = \\left( \\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n(n-1)}} \\right)\n$$\nThe leverage score $\\ell_i$ for $i > 1$ is:\n$$\n\\ell_i = \\|U_{i,:}\\|_2^2 = \\left(\\frac{1}{\\sqrt{n}}\\right)^2 + \\left(\\frac{1}{\\sqrt{n(n-1)}}\\right)^2 = \\frac{1}{n} + \\frac{1}{n(n-1)} = \\frac{n-1+1}{n(n-1)} = \\frac{n}{n(n-1)} = \\frac{1}{n-1}\n$$\nAs a sanity check, the sum of leverage scores must equal the rank $k=2$: $\\sum_{i=1}^n \\ell_i = \\ell_1 + \\sum_{i=2}^n \\ell_i = 1 + (n-1) \\left(\\frac{1}{n-1}\\right) = 1+1=2$. This is correct.\n\nNow, we compute the coherence $\\mu(A)$.\nFor $n \\ge 3$, we have $n-1 \\ge 2$, so $\\frac{1}{n-1} \\le \\frac{1}{2} < 1$. For $n=2$, $\\frac{1}{n-1}=1$. Thus, for any $n \\ge 2$, $\\max_i \\ell_i = \\ell_1 = 1$.\n$$\n\\mu(A) = \\frac{n}{k} \\max_{i} \\ell_{i} = \\frac{n}{2} \\cdot 1 = \\frac{n}{2}\n$$\n\n**Part 3: Determining the Asymptotic Gain Factor**\n\nThe problem provides the scaling for sufficient sample sizes for uniform and leverage-score sampling. We are asked to argue for them from first principles. The core of the argument rests on matrix concentration inequalities. For a sampling-and-reweighting scheme, the sample complexity $m$ required to achieve a subspace embedding is dominated by the term $m \\propto (\\max_i \\frac{\\ell_i}{p_i}) \\frac{\\log(k/\\delta)}{\\varepsilon^2}$.\n\nFor uniform sampling, $p_i = 1/n$ for all $i$. The complexity factor becomes:\n$$\n\\max_i \\frac{\\ell_i}{p_i} = \\max_i \\frac{\\ell_i}{1/n} = n \\max_i \\ell_i\n$$\nUsing the definition of coherence $\\mu(A) = \\frac{n}{k} \\max_i \\ell_i$, we can write $n \\max_i \\ell_i = k \\, \\mu(A)$. Thus, $m_{\\mathrm{unif}} \\propto k \\, \\mu(A) \\frac{\\log(k/\\delta)}{\\varepsilon^2}$, which matches the problem statement.\n\nFor leverage-score sampling, $p_i = \\ell_i / \\sum_j \\ell_j = \\ell_i/k$. The complexity factor becomes:\n$$\n\\max_i \\frac{\\ell_i}{p_i} = \\max_i \\frac{\\ell_i}{\\ell_i/k} = \\max_i k = k\n$$\nThus, $m_{\\mathrm{lev}} \\propto k \\frac{\\log(k/\\delta)}{\\varepsilon^2}$, which also matches the problem statement.\n\nThe asymptotic gain factor is the ratio $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$. Assuming the constants of proportionality are identical (which is standard in this type of analysis), we have:\n$$\n\\frac{m_{\\mathrm{unif}}}{m_{\\mathrm{lev}}} = \\frac{k \\ln(k/\\delta) \\mu(A) \\varepsilon^{-2}}{k \\ln(k/\\delta) \\varepsilon^{-2}} = \\mu(A)\n$$\nSubstituting the value of $\\mu(A)$ we calculated for the specified polynomial design:\n$$\n\\frac{m_{\\mathrm{unif}}}{m_{\\mathrm{lev}}} = \\mu(A) = \\frac{n}{2}\n$$\nThis ratio represents the factor by which uniform sampling requires more samples than the optimal leverage-score sampling to achieve the same approximation guarantee, for this worst-case coherent design.",
            "answer": "$$\n\\boxed{\\frac{n}{2}}\n$$"
        }
    ]
}