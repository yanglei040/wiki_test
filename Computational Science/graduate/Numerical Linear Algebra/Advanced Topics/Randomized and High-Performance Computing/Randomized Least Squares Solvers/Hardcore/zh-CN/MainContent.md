## 引言
在数据规模呈指数级增长的时代，求解线性[最小二乘问题](@entry_id:164198)是科学计算和数据分析中的一项基本任务。然而，对于拥有海量观测数据（即“高瘦”矩阵）的场景，诸如QR分解或SVD分解之类的经典确定性算法，其高昂的计算成本往往令人望而却步。随机最小二乘求解器作为一种革命性的替代方案应运而生，它旨在通过引入随机性来大幅降低计算复杂度，同时提供具有理论保证的近似解。

本文旨在全面而深入地探讨这一前沿领域。我们面临的核心问题是：如何设计算法，使其能够在远低于传统方法所需的时间和空间资源下，有效解决超大规模的最小二乘问题？本文将系统性地回答这一问题，为读者构建一个关于[随机化算法](@entry_id:265385)的坚实知识框架。

在接下来的内容中，我们将通过三个章节展开论述。第一章 **“原理与机制”** 将深入剖析随机求解器的理论基石，从“降维与求解”这一核心[范式](@entry_id:161181)出发，阐明[子空间嵌入](@entry_id:755615)的关键作用，并详细对比两种主流技术——[随机投影](@entry_id:274693)与数据采样——的工作原理与优劣。第二章 **“应用与跨学科联系”** 将展示这些工具的惊人通用性，探讨其如何扩展到正则化、[约束优化](@entry_id:635027)等复杂问题，并揭示其与统计学、信号处理等领域的深刻联系。最后，在 **“动手实践”** 章节中，您将通过具体的计算和分析任务，亲身体验这些算法的计算优势与潜在挑战。

现在，让我们一同开启对随机最小二乘求解器这一强大计算工具的探索之旅。

## 原理与机制

本章旨在深入探讨随机最小二乘求解器的核心原理与作用机制。我们将从[最小二乘问题](@entry_id:164198)的基本几何性质出发，建立分析近似算法的基础。随后，我们将引入“[降维](@entry_id:142982)与求解”(sketch-and-solve)这一核心[范式](@entry_id:161181)，并阐明其成功的关键条件——[子空间嵌入](@entry_id:755615)特性。本章的重点在于剖析两种主要的降维方法：与数据无关的[随机投影](@entry_id:274693)和与数据相关的[采样方法](@entry_id:141232)。我们将通过理论分析和具体实例，揭示这些方法的工作原理、性能权衡以及在面对特定数据结构时的优缺点。最后，我们将讨论如何在这一框架下处理带异常值数据的[稳健回归](@entry_id:139206)问题。

### [最小二乘解](@entry_id:152054)的基本性质与[误差分析](@entry_id:142477)

在深入研究近似算法之前，我们必须首先牢固掌握标[准线性](@entry_id:637689)[最小二乘问题](@entry_id:164198)的精确解及其几何特性。给定一个“高瘦”矩阵 $A \in \mathbb{R}^{n \times d}$ (其中 $n \gg d$ 且 $A$ 列满秩) 和一个向量 $b \in \mathbb{R}^{n}$，最小二乘问题旨在寻找一个向量 $x \in \mathbb{R}^{d}$，使得[残差向量](@entry_id:165091) $r = Ax - b$ 的[欧几里得范数](@entry_id:172687)最小化。其[目标函数](@entry_id:267263)为：
$$ \min_{x \in \mathbb{R}^{d}} \|Ax - b\|_{2}^{2} $$

令 $x^{\star}$ 为该问题的最优解。通过设置目标函数关于 $x$ 的梯度为零，我们可以推导出著名的 **[正规方程](@entry_id:142238) (normal equations)**：
$$ A^{\top} A x^{\star} = A^{\top} b $$
由于 $A$ 是列满秩的，$A^{\top} A$ 是一个 $d \times d$ 的[对称正定矩阵](@entry_id:136714)，因此是可逆的。这保证了[最小二乘问题](@entry_id:164198)存在唯一的解：
$$ x^{\star} = (A^{\top} A)^{-1} A^{\top} b $$
矩阵 $(A^{\top} A)^{-1} A^{\top}$ 通常被称为 $A$ 的 **Moore-Penrose [伪逆](@entry_id:140762)**，记作 $A^{\dagger}$。

除了正规方程，我们还可以通过 $A$ 的 **[奇异值分解](@entry_id:138057) (Singular Value Decomposition, SVD)** 来刻画 $x^{\star}$。对于列满秩的矩阵 $A$，其简化SVD形式为 $A = U \Sigma V^{\top}$，其中：
- $U \in \mathbb{R}^{n \times d}$ 的列是标准正交的 ($U^{\top}U = I_d$)，构成了 $A$ [列空间](@entry_id:156444)的一组[标准正交基](@entry_id:147779)。
- $\Sigma \in \mathbb{R}^{d \times d}$ 是一个[对角矩阵](@entry_id:637782)，其对角线上的元素为 $A$ 的[奇异值](@entry_id:152907) $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_d > 0$。
- $V \in \mathbb{R}^{d \times d}$ 是一个正交矩阵 ($V^{\top}V = VV^{\top} = I_d$)。

将SVD代入 $x^{\star}$ 的表达式，经过化简可得：
$$ x^{\star} = V \Sigma^{-1} U^{\top} b $$

[最小二乘解](@entry_id:152054)的一个核心几何性质是，最优[残差向量](@entry_id:165091) $r^{\star} = Ax^{\star} - b$ 与 $A$ 的[列空间](@entry_id:156444) $\operatorname{col}(A)$ 正交。这可以从正规方程直接看出：$A^{\top}r^{\star} = A^{\top}(Ax^{\star} - b) = A^{\top}b - A^{\top}b = 0$。

这个正交性是分析近似解误差的关键。对于任意一个近似解 $x$，其对应的残差向量为 $r = Ax - b$。我们可以将其分解为：
$$ r = Ax - b = A(x - x^{\star}) + (Ax^{\star} - b) = A(x - x^{\star}) + r^{\star} $$
其中，向量 $A(x - x^{\star})$ 位于 $A$ 的列空间内，而 $r^{\star}$ 与该空间正交。根据勾股定理，我们得到一个精确的恒等式：
$$ \|r\|_{2}^{2} = \|A(x - x^{\star})\|_{2}^{2} + \|r^{\star}\|_{2}^{2} $$
移项后，我们得到[目标函数](@entry_id:267263)值的增量与解的误差之间的关系：
$$ \|Ax - b\|_{2}^{2} - \|Ax^{\star} - b\|_{2}^{2} = \|A(x - x^{\star})\|_{2}^{2} $$

这个关系告诉我们，一个近似解 $x$ 在目标函数上造成的误差，完全由向量 $A(x - x^{\star})$ 的范数决定。为了将这个“目标误差”与“解误差” $\|x - x^{\star}\|_{2}$ 联系起来，我们需要借助 $A$ 的奇异值。对于任意向量 $y \in \mathbb{R}^d$，我们有如下界：
$$ \sigma_{\min}(A) \|y\|_{2} \le \|Ay\|_{2} \le \sigma_{\max}(A) \|y\|_{2} $$
其中 $\sigma_{\max}(A) = \sigma_1$ 是 $A$ 的最大奇异值，$\sigma_{\min}(A) = \sigma_d$ 是最小[奇异值](@entry_id:152907)。将此不等式应用于 $y = x - x^{\star}$ 并平方，我们得到：
$$ \sigma_{\min}(A)^{2} \|x - x^{\star}\|_{2}^{2} \le \|A(x - x^{\star})\|_{2}^{2} \le \sigma_{\max}(A)^{2} \|x - x^{\star}\|_{2}^{2} $$
结合之前的恒等式，我们最终建立了目标误差与解误差之间的不等式关系 ：
$$ \sigma_{\min}(A)^{2} \|x - x^{\star}\|_{2}^{2} \le \|Ax - b\|_{2}^{2} - \|Ax^{\star} - b\|_{2}^{2} \le \sigma_{\max}(A)^{2} \|x - x^{\star}\|_{2}^{2} $$
这个不等式揭示了一个深刻的道理：矩阵 $A$ 的 **条件数** $\kappa_{2}(A) = \sigma_{\max}(A) / \sigma_{\min}(A)$ 控制着从目标误差推断解误差的不确定性。如果 $\kappa_{2}(A)$ 很大（即 $A$ 是病态的），那么即使一个解的目标误差很小，其解误差也可能非常大。这是设计和分析所有[最小二乘问题](@entry_id:164198)求解器（包括随机算法）时必须面对的基本现实。

### “[降维](@entry_id:142982)与求解”[范式](@entry_id:161181)与[子空间嵌入](@entry_id:755615)

对于 $n$ 极大的问题，直接求解正规方程或计算SVD的成本（例如，对于密集矩阵 $A$，QR分解的成本为 $\mathcal{O}(nd^2)$）可能过高。随机算法的核心思想是通过一个 **降维矩阵 (sketching matrix)** $S \in \mathbb{R}^{m \times n}$（其中 $m \ll n$）将原问题转化为一个规模小得多的问题：
$$ \min_{x \in \mathbb{R}^{d}} \|SAx - Sb\|_{2} $$
这个过程被称为 **[降维](@entry_id:142982)与求解 (sketch-and-solve)**。这种方法的有效性取决于一个关键问题：[降维](@entry_id:142982)矩阵 $S$ 需要满足什么性质，才能保证[降维](@entry_id:142982)后的问题解 $\tilde{x}$ 是原问题解 $x^{\star}$ 的一个良好近似？

答案是 $S$ 必须能近似保持原问题相关几何结构的完整性。具体而言，我们需要 $S$ 能够近似保持所有形如 $Ax - b$ 的[残差向量](@entry_id:165091)的欧几里得范数。这些[向量张成](@entry_id:152883)了一个至多 $d+1$ 维的[子空间](@entry_id:150286)，即[增广矩阵](@entry_id:150523) $[A \ b]$ 的列空间 $\operatorname{span}([A \ b])$。如果一个线性映射能近似保持一个[子空间](@entry_id:150286)中所有[向量的范数](@entry_id:154882)，我们就称之为 **[子空间嵌入](@entry_id:755615) (Subspace Embedding, SE)**。

**定义 ([子空间嵌入](@entry_id:755615))**：对于一个[线性子空间](@entry_id:151815) $\mathcal{T} \subseteq \mathbb{R}^{n}$ 和一个误差参数 $\varepsilon \in (0,1)$，如果一个矩阵 $S \in \mathbb{R}^{m \times n}$ 对所有 $y \in \mathcal{T}$ 都满足：
$$ (1 - \varepsilon)\|y\|_{2} \le \|Sy\|_{2} \le (1 + \varepsilon)\|y\|_{2} $$
我们就称 $S$ 是 $\mathcal{T}$ 的一个 $(1 \pm \varepsilon)$-[子空间嵌入](@entry_id:755615)。

如果 $S$ 是 $\operatorname{span}([A \ b])$ 的一个[子空间嵌入](@entry_id:755615)，那么对于任意 $x \in \mathbb{R}^d$，我们有 $\|S(Ax - b)\|_2 \approx \|Ax - b\|_2$。这意味着降维后的[目标函数](@entry_id:267263)与原目标函数非常接近，因此它们的[最小值点](@entry_id:634980)也应该很接近。可以证明，如果 $S$ (经过适当缩放) 是一个 $\varepsilon$-[子空间嵌入](@entry_id:755615)，那么降维问题的解 $\tilde{x}$ 满足：
$$ \|A\tilde{x} - b\|_2 \le \sqrt{\frac{1+\varepsilon}{1-\varepsilon}} \|Ax^{\star} - b\|_2 $$
这为[降维](@entry_id:142982)与求解方法的正确性提供了坚实的理论保证。

[子空间嵌入](@entry_id:755615)性质为何如此重要？如果一个降维映射不具备此性质，它可能会严重扭曲空间的几何结构，导致灾难性的后果。例如，考虑一个[降维](@entry_id:142982)矩阵 $S$ 和一个矩阵 $A$，存在一个非零向量 $y \in \operatorname{col}(A)$ 使得 $Sy=0$。这意味着 $S$ 将 $A$ 列空间中的一个“信号”方向完全湮灭了。在这种情况下，降维问题 $\min \|SAx - Sb\|_2$ 将无法分辨沿该方向的解分量，可能导致解产生巨大偏差 。

那么，如何构造有效的[子空间嵌入](@entry_id:755615)呢？一个确定性的[线性映射](@entry_id:185132)，如简单地选取前 $m$ 行，总会存在一个固定的、维度为 $n-m$ 的零空间。一个“聪明”的对手总能构造一个矩阵 $A$，使其列空间与这个零空间有非平凡的交集，从而让该确定性映射失效 。这正是 **随机化** 发挥关键作用的地方。一个随机生成的降维矩阵，其[零空间](@entry_id:171336)也是随机的。对于任何一个固定的低维[子空间](@entry_id:150286)（如 $\operatorname{span}([A \ b])$），该随机[零空间](@entry_id:171336)与其不相交的概率非常高。这便是随机降维[算法鲁棒性](@entry_id:635315)的根本来源。

主要存在两大类构建随机[子空间嵌入](@entry_id:755615)的方法：
1.  **与数据无关的[随机投影](@entry_id:274693)**：降维矩阵 $S$ 的生成过程完全独立于数据 $A$ 和 $b$。
2.  **与数据相关的[随机采样](@entry_id:175193)**：降维矩阵 $S$ 通过对 $A$ 的行或列进行智能采样来构建，其采样概率依赖于数据本身。

接下来的章节将详细探讨这两种机制。

### 机制一：与数据无关的[随机投影](@entry_id:274693)

这类方法的核心思想源于 Johnson-Lindenstrauss (JL) 引理，该引理指出，将一组点从高维空间投影到一个随机的低维[子空间](@entry_id:150286)，可以以高概率近似保持所有点对之间的距离。通过将此思想扩展到保持整个[子空间](@entry_id:150286)的几何结构，我们得到了多种与数据无关的[子空间嵌入](@entry_id:755615)构造方法。

#### [子采样随机哈达玛变换 (SRHT)](@entry_id:755609)

SRHT 是一种[计算效率](@entry_id:270255)极高的[随机投影](@entry_id:274693)方法。其[降维](@entry_id:142982)矩阵 $S_{\mathrm{H}}$ 的构造如下：
$$ S_{\mathrm{H}} = \sqrt{\frac{n}{m}} R H D $$
其中 $n$ 是 $2$ 的幂，$D \in \mathbb{R}^{n \times n}$ 是一个[对角矩阵](@entry_id:637782)，对角[线元](@entry_id:196833)素为独立的随机符号（Rademacher变量，即等概率取 $\pm 1$），$H \in \mathbb{R}^{n \times n}$ 是归一化的 Walsh-Hadamard 变换矩阵，$R \in \mathbb{R}^{m \times n}$ 是一个均匀[随机采样](@entry_id:175193) $m$ 行的算子。

这个构造的每一步都有其深刻的用意 ：
- **随机符号矩阵 $D$**：它的作用是[随机化](@entry_id:198186)输入向量的符号模式。这一步至关重要，因为它能打破输入数据可能存在的任何特殊结构，使其在哈达玛变换后表现得更像一个“随机”向量，从而避免了最坏情况的发生。如果缺少 $D$，对手可以构造一个与哈达玛[基向量](@entry_id:199546)高度相关的输入向量，使得变换后的能量高度集中在少数几个坐标上，导致后续的随机采样失效 。
- **哈达玛变换 $H$**：这是一个正交变换（$H^{\top}H = I_n$），其所有元素的模为 $1/\sqrt{n}$。它可以通过快速算法（类似FFT）在 $\mathcal{O}(n \log n)$ 时间内应用，而非 $\mathcal{O}(n^2)$。它的作用是将输入向量的能量“摊平”到所有坐标上。
- **随机采样矩阵 $R$**：在能量被摊平后，均匀[随机采样](@entry_id:175193) $m$ 个坐标就足以捕获整个向量能量的绝大部分。
- **缩放因子 $\sqrt{n/m}$**：这个因子的选择是为了确保[降维](@entry_id:142982)映射在期望意义上是保范数的，即 $\mathbb{E}[S_{\mathrm{H}}^{\top} S_{\mathrm{H}}] = I_n$。从数值实现的角度看，由于 $H$ 的元素是 $\pm 1/\sqrt{n}$，经过所有变换后，$S_{\mathrm{H}}$ 的非零元素的模恰好是 $1/\sqrt{m}$，与 $n$ 无关。这避免了当 $n$ 很大时可能出现的数值[溢出](@entry_id:172355)或下溢问题，具有良好的数值稳定性。

对于任意固定的 $d$ 维[子空间](@entry_id:150286)，SRHT 只需要 $m = \mathcal{O}(d \log d / \varepsilon^2)$ 的[降维](@entry_id:142982)维度就能以高概率成为一个 $\varepsilon$-[子空间嵌入](@entry_id:755615)。

#### CountSketch

CountSketch 是另一种重要的与数据无关的投影方法，尤其适用于[稀疏数据](@entry_id:636194)。它通过两个哈希函数来工作：一个位置[哈希函数](@entry_id:636237) $h: \{1, \dots, n\} \to \{1, \dots, m\}$ 将原始的第 $i$ 个[坐标映射](@entry_id:747874)到[降维](@entry_id:142982)后的 $m$ 个坐标之一；一个符号哈希函数 $\sigma: \{1, \dots, n\} \to \{\pm 1\}$ 为其分配一个随机符号。$S_{\mathrm{C}}$ 矩阵的每一列只有一个非零元，其值为 $\sigma(i)$，位于第 $h(i)$ 行。

CountSketch 的一个显著优点是其应用速度极快。对于一个有 $\operatorname{nnz}(A)$ 个非零元素的矩阵 $A$，计算 $S_{\mathrm{C}}A$ 只需要 $\mathcal{O}(\operatorname{nnz}(A))$ 的时间。

#### [随机投影](@entry_id:274693)的局限性

虽然[随机投影](@entry_id:274693)方法因其简洁和普适性而极具吸[引力](@entry_id:175476)，但“与数据无关”也可能成为其弱点。某些投影方法自身的结构可能会与特定类型的[数据结构](@entry_id:262134)发生“共振”，导致性能下降。一个经典的例子是，如果矩阵 $A$ 的列本身就是从哈达玛矩阵的列中选取的，那么SRHT的性能会急剧恶化。这是因为 $A$ 的列与SRHT内部使用的哈达玛基底高度相关，破坏了能量“摊平”的前提，导致所谓的 **结构性[混叠](@entry_id:146322) (structured aliasing)**。在这种最坏情况下，SRHT需要更大的 $m$ 才能保证[子空间嵌入](@entry_id:755615)性质。相比之下，基于哈希的CountSketch对这种算术结构不敏感，因此能够成功应对这类对抗性输入 。这表明，没有一种与数据无关的方法是万能的，理解其内在机制和潜在的失效模式至关重要。

### 机制二：与数据相关的采样与杠杆分数

与[随机投影](@entry_id:274693)不同，另一大类方法通过直接从原矩阵 $A$ 中采样行来构造降维矩阵 $S$。这种方法的直觉是，矩阵的某些行可能比其他行更“重要”。如果我们能识别并优先采样这些重要的行，也许可以用更少的样本构建出更有效的降维。

那么，如何衡量一行的“重要性”呢？答案是 **杠杆分数 (leverage scores)**。第 $i$ 行的杠杆分数 $\ell_i$ 定义为 $A$ 的[列空间](@entry_id:156444)上的正交投影矩阵的第 $i$ 个对角元素。如果 $U$ 是 $A$ 的[列空间](@entry_id:156444)的一组标准正交基（即 $A$ 的SVD中的 $U$ 矩阵），则：
$$ \ell_i = \|(UU^{\top})_{i,:}\|_{2}^{2} = \|U_{i,:}\|_{2}^{2} $$
其中 $U_{i,:}$ 表示 $U$ 的第 $i$ 行。杠杆分数 $\ell_i$ 的取值在 $0$ 和 $1$ 之间，所有杠杆分数的总和为 $\sum_{i=1}^n \ell_i = \operatorname{rank}(A) = d$。它量化了第 $i$ 行的响应 $b_i$ 对其自身拟合值 $(Ax^{\star})_i$ 的影响程度，或者说，第 $i$ 行数据点在确定最优解 $x^{\star}$ 时的“影响力”。一个高杠杆分数的行是一个“离群”的行，它对解的结构有不成比例的影响。

**杠杆分数的重要性** 可以通过一个精巧的例子来阐明 。我们可以构造两个矩阵 $A_1$ 和 $A_2$，它们拥有完全相同的[奇异值](@entry_id:152907)（即相同的谱结构），但杠杆分数[分布](@entry_id:182848)截然不同。$A_1$ 的杠杆分数高度集中在少数几行上，而 $A_2$ 的杠杆分数[均匀分布](@entry_id:194597)在所有行上。实验表明，对于均匀行采样（即不考虑杠杆分数），在处理 $A_1$ 时很容易失败（因为可能采样不到关键的几行），但在处理 $A_2$ 时则几乎总能成功。这有力地证明了，对于行[采样方法](@entry_id:141232)，起决定性作用的不是[奇异值](@entry_id:152907)，而是杠杆分数[分布](@entry_id:182848)。

基于杠杆分数的采样算法如下：构造一个 $m \times n$ 的采样与重[缩放矩阵](@entry_id:188350) $S$，其中第 $k$ 次采样（共 $m$ 次，[独立同分布](@entry_id:169067)）以概率 $p_i$ 选取第 $i$ 行，并将其缩放 $1/\sqrt{mp_i}$。理论表明，为了最小化[估计误差](@entry_id:263890)的[方差](@entry_id:200758)，最优的采样概率 $p_i$ 应该正比于杠杆分数 $\ell_i$。

这种[采样方法](@entry_id:141232)的一个优美特性是，它能生成一个对 $A^{\top}A$ 的[无偏估计](@entry_id:756289)。具体来说，可以证明 ：
$$ \mathbb{E}[A^{\top}S^{\top}SA] = A^{\top}A $$
其[方差](@entry_id:200758)由 $\frac{1}{m}(\sum_{i=1}^{n}\frac{\|a_{i}\|_{2}^{4}}{p_{i}}-\|A^{\top}A\|_{F}^{2})$ 给出，其中 $a_i^{\top}$ 是 $A$ 的第 $i$ 行。选择 $p_i \propto \|a_i\|_2^2$（这是一个易于计算的杠杆分数近似）可以有效地控制这个[方差](@entry_id:200758)。

通过矩阵Chernoff界等更精细的分析工具，可以证明，基于（近似）杠杆分数进行采样，仅需 $m = \mathcal{O}(d \log d / \varepsilon^2)$ 的样本量，就能以高概率构造出一个 $\varepsilon$-[子空间嵌入](@entry_id:755615)。这达到了与SRHT等密集投影方法相媲美的样本复杂度，但生成的[降维](@entry_id:142982)矩阵 $S$ 却是稀疏的。

**挑战与解决方案**：[杠杆分数采样](@entry_id:751254)方法的主要障碍在于杠杆分数自身的计算。直接根据定义计算所有杠杆分数需要 $\mathcal{O}(nd^2)$ 的时间，这与我们试图避免的计算瓶颈相当。幸运的是，研究人员开发了快速[近似算法](@entry_id:139835)。这些算法利用两阶段的[随机投影](@entry_id:274693)，可以在近乎线性的时间 $\mathcal{O}(\operatorname{nnz}(A)\log d + \operatorname{poly}(d/\varepsilon))$ 内，计算出所有杠杆分数的足够精确的乘法近似值 。这使得基于杠杆分数的[采样方法](@entry_id:141232)在实践中是完全可行的。

### 综合比较与高级主题

#### 方法权衡总结

现在我们可以对两类方法进行总结与比较 ：
- **与数据无关的投影 (Oblivious Projections)**：
    - **优点**：预处理成本低（只需生成随机矩阵），无需访问数据即可设计。像CountSketch这样的稀疏投影应用速度极快 ($\mathcal{O}(\operatorname{nnz}(A))$)。
    - **缺点**：为了保证对[任意子](@entry_id:143753)空间的嵌入性质，通常需要更大的[降维](@entry_id:142982)维度 $m$（例如，CountSketch需要 $m=\tilde{\mathcal{O}}(d^2/\varepsilon^2)$）。对于具有特定结构的“对抗性”数据，其性能可能下降。

- **与数据相关的采样 (Data-Dependent Sampling)**：
    - **优点**：样本复杂度更优，仅需 $m=\mathcal{O}(d\log d/\varepsilon^2)$ 即可。由于其适应数据的特性，对于杠杆分数[分布](@entry_id:182848)不均（即高“[相干性](@entry_id:268953)”）的矩阵，其性能通常比与数据无关的方法更稳健。
    - **缺点**：需要一个显著的[预处理](@entry_id:141204)步骤来计算（或近似计算）杠杆分数。虽然存在快速近似算法，但这仍是一个额外的计算开销和实现复杂性。

#### 对异常值的稳健性

到目前为止，我们讨论的都是如何近似 $\ell_2$ 范数下的[最小二乘解](@entry_id:152054)。然而，$\ell_2$ 范数对 **异常值 (outliers)** 极其敏感，因为残差是平方后相加的，一个巨大的残差就能主导整个[目标函数](@entry_id:267263)。因此，如果数据 $(A,b)$ 中有少数几行被任意大的值污染，$\ell_2$ [最小二乘解](@entry_id:152054)可能会被完全破坏。

由于标准的 $\ell_2$ 降维与求解方法旨在忠实地近似原始的 $\ell_2$ 问题，它同样继承了这种对异常值的不稳健性。一个在原始空间中产生巨大 $\ell_2$ 范数的异常值，在降维后的空间中同样会产生巨大的 $\ell_2$ 范数，从而污染[降维](@entry_id:142982)问题的解。

为了实现对异常值的稳健性，我们必须从根本上改变目标函数，从 $\ell_2$ 回归转向 **$\ell_1$ 回归 (Least Absolute Deviations, LAD)**：
$$ \min_{x \in \mathbb{R}^{d}} \|Ax - b\|_{1} = \sum_{i=1}^{n} |(Ax)_i - b_i| $$
在 $\ell_1$ [目标函数](@entry_id:267263)中，残差的贡献是线性的而非二次的，这大大削弱了巨大异常值的影响。

有趣的是，“降维与求解”的[范式](@entry_id:161181)可以被推广到 $\ell_1$ 回归。这需要我们构造一个 **$\ell_1$ [子空间嵌入](@entry_id:755615)**，即一个能够近似保持相关[子空间](@entry_id:150286)中所有向量的 $\ell_1$ 范数的降维矩阵 $S$。通过Lewis权重采样或基于柯西分布的投影等技术，可以构造出这样的 $\ell_1$ 嵌入。然后，我们求解[降维](@entry_id:142982)后的 $\ell_1$ 回归问题 $\min_x \|SAx - Sb\|_1$。其解 $\hat{x}$ 可以被证明满足如下的近似保证 ：
$$ \|A\hat{x} - b\|_{1} \le \frac{1+\varepsilon}{1-\varepsilon} \min_{x} \|Ax - b\|_{1} $$
这个保证的强大之处在于，它不依赖于异常值的大小，从而为处理受污染数据提供了一个计算上高效且理论上可靠的框架。