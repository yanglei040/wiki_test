{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for employing randomized algorithms is their remarkable computational efficiency, especially for large-scale matrices. This first practice provides a foundational analysis of this advantage by guiding you through a detailed breakdown of the computational cost of a standard randomized SVD pipeline. By deriving the total floating-point operation count and comparing it asymptotically to that of a classical deterministic method, you will gain a quantitative understanding of why these randomized approaches are transformative for modern data analysis .",
            "id": "3569802",
            "problem": "Consider a dense real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, a target rank $k$ with $1 \\leq k < n$, oversampling parameter $p \\geq 0$, and power iteration parameter $q \\geq 0$. Define $\\ell = k + p$. A standard randomized singular value decomposition (SVD) pipeline proceeds as follows: draw a Gaussian test matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$, form the sample matrix $Y = (A A^{\\top})^{q} A \\Omega$, compute an orthonormal basis $Q \\in \\mathbb{R}^{m \\times \\ell}$ for the columns of $Y$ via a thin QR factorization, form the compressed matrix $B = Q^{\\top} A \\in \\mathbb{R}^{\\ell \\times n}$, compute a deterministic SVD $B = \\widetilde{U} \\Sigma V^{\\top}$, and set $U = Q \\widetilde{U}$. Assume the following operation cost model for floating-point operation counts:\n- Dense matrix-matrix multiply of sizes $(m \\times n)$ by $(n \\times r)$ costs $2 m n r$ flops.\n- Householder thin QR of an $(m \\times r)$ matrix with $m \\geq r$ costs $2 m r^{2} - \\tfrac{2}{3} r^{3}$ flops.\n- Deterministic SVD of an $(n \\times r)$ matrix with $n \\geq r$ (via bidiagonalization and diagonal SVD) costs $4 n r^{2} + \\tfrac{8}{3} r^{3}$ flops.\n- Dense matrix-vector multiply of sizes $(m \\times n)$ by $(n \\times 1)$ costs $2 m n$ flops.\n\nFor a deterministic truncated SVD that computes the leading $k$ singular triplets by $k$ steps of the Golub–Kahan bidiagonalization with full reorthogonalization, use the standard model\n$$\nF_{\\mathrm{det}}(m,n,k) \\;=\\; 4 m n k \\;+\\; 2 (m + n) k^{2} \\;+\\; \\tfrac{8}{3} k^{3}.\n$$\n\n(a) Using only the foregoing cost model and the algorithmic steps, derive the total operation count $F_{\\mathrm{rand}}(m,n,k,p,q)$ for the randomized SVD pipeline described above, expressed in terms of $m$, $n$, $k$, $p$, and $q$, including the costs for forming $A \\Omega$, the $q$ power iterations, the QR of $Y$, forming $B = Q^{\\top} A$, the small SVD of $B$, and forming $U = Q \\widetilde{U}$.\n\n(b) Consider the asymptotic regime where $m \\to \\infty$ and $n \\to \\infty$ with $m / n \\to \\gamma \\in (1, \\infty)$, while $k$, $p$, and $q$ are fixed and independent of $m$ and $n$. In this regime, compute the limit of the ratio\n$$\nR \\;=\\; \\lim_{\\substack{m,n \\to \\infty \\\\ m/n \\to \\gamma}} \\frac{F_{\\mathrm{rand}}(m,n,k,p,q)}{F_{\\mathrm{det}}(m,n,k)}.\n$$\nProvide your final answer for $R$ as a single simplified analytic expression in terms of $k$, $p$, and $q$. No numerical approximation is required, and no units are involved. The final answer must be a single closed-form expression.",
            "solution": "### Part (a): Derivation of the Total Operation Count $F_{\\mathrm{rand}}$\n\nThe total operation count, or flop count, $F_{\\mathrm{rand}}(m,n,k,p,q)$, is the sum of the costs of each step in the described randomized SVD pipeline. We will analyze each step using the provided cost models and the parameter $\\ell = k+p$.\n\n1.  **Forming the sample matrix $Y = (A A^{\\top})^{q} A \\Omega$**:\n    This computation is performed iteratively. The test matrix is $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$. The matrix $A$ is in $\\mathbb{R}^{m \\times n}$.\n    -   First, we form an initial sample matrix $Y_0 = A \\Omega$. This is a multiplication of an $(m \\times n)$ matrix by an $(n \\times \\ell)$ matrix. The cost is $2mn\\ell$ flops. The resulting matrix $Y_0$ is of size $(m \\times \\ell)$.\n    -   Next, we apply the power iteration operator $(A A^{\\top})$ a total of $q$ times. Each application on a matrix $Y_{i-1} \\in \\mathbb{R}^{m \\times \\ell}$ to produce $Y_i = A A^{\\top} Y_{i-1}$ is done in two steps:\n        a. Compute the intermediate matrix $Z_{i-1} = A^{\\top} Y_{i-1}$. This is a multiplication of $A^{\\top} \\in \\mathbb{R}^{n \\times m}$ by $Y_{i-1} \\in \\mathbb{R}^{m \\times \\ell}$. The cost is $2nm\\ell = 2mn\\ell$ flops.\n        b. Compute $Y_i = A Z_{i-1}$. This is a multiplication of $A \\in \\mathbb{R}^{m \\times n}$ by $Z_{i-1} \\in \\mathbb{R}^{n \\times \\ell}$. The cost is $2mn\\ell$ flops.\n    -   The cost of one power iteration step is thus $2mn\\ell + 2mn\\ell = 4mn\\ell$ flops. For $q$ iterations, the cost is $q \\times (4mn\\ell) = 4qmn\\ell$ flops.\n    -   The total cost for forming the final sample matrix $Y$ is the sum of the initial step and the $q$ power iterations:\n        $$\n        \\text{Cost}_Y = 2mn\\ell + 4qmn\\ell = (2+4q)mn\\ell\n        $$\n\n2.  **Compute an orthonormal basis $Q$ for $Y$**:\n    The matrix $Y$ is of size $(m \\times \\ell)$. We perform a thin QR factorization of $Y$ to obtain $Q \\in \\mathbb{R}^{m \\times \\ell}$. The cost model for a Householder thin QR of an $(m \\times r)$ matrix (with $m \\geq r$) is $2mr^2 - \\frac{2}{3}r^3$. Here, $r=\\ell$.\n    $$\n    \\text{Cost}_{QR} = 2m\\ell^2 - \\frac{2}{3}\\ell^3\n    $$\n\n3.  **Form the compressed matrix $B = Q^{\\top} A$**:\n    This step involves multiplying the matrix $Q^{\\top} \\in \\mathbb{R}^{\\ell \\times m}$ by the matrix $A \\in \\mathbb{R}^{m \\times n}$.\n    $$\n    \\text{Cost}_B = 2\\ell mn = 2mn\\ell\n    $$\n\n4.  **Compute the SVD of $B = \\widetilde{U} \\Sigma V^{\\top}$**:\n    The matrix $B$ is of size $(\\ell \\times n)$. The cost model provided is for an $(N \\times R)$ matrix where $N \\geq R$. The asymptotic regime specified in part (b) has $m, n \\to \\infty$ while $k, p$ (and thus $\\ell=k+p$) are fixed. In this limit, we will have $n > \\ell$, so $B$ is a \"short-and-fat\" matrix. The SVD of $B$ is typically computed via the SVD of $B^{\\top} \\in \\mathbb{R}^{n \\times \\ell}$, which is a \"tall-and-skinny\" matrix. For $B^{\\top}$, the dimensions match the cost model with $N=n$ and $R=\\ell$.\n    $$\n    \\text{Cost}_{SVD} = 4n\\ell^2 + \\frac{8}{3}\\ell^3\n    $$\n\n5.  **Form the final left singular vectors $U = Q \\widetilde{U}$**:\n    The matrix $Q$ is of size $(m \\times \\ell)$. The matrix of left singular vectors of $B$, $\\widetilde{U}$, is of size $(\\ell \\times \\ell)$. The product $U=Q\\widetilde{U}$ is an $(m \\times \\ell)$ matrix. The cost for this matrix-matrix multiplication is:\n    $$\n    \\text{Cost}_U = 2m\\ell\\ell = 2m\\ell^2\n    $$\n\nThe total operation count $F_{\\mathrm{rand}}$ is the sum of these costs:\n$$\nF_{\\mathrm{rand}} = \\text{Cost}_Y + \\text{Cost}_{QR} + \\text{Cost}_B + \\text{Cost}_{SVD} + \\text{Cost}_U\n$$\n$$\nF_{\\mathrm{rand}} = (2+4q)mn\\ell + \\left(2m\\ell^2 - \\frac{2}{3}\\ell^3\\right) + 2mn\\ell + \\left(4n\\ell^2 + \\frac{8}{3}\\ell^3\\right) + 2m\\ell^2\n$$\nWe group the terms by their dependence on $m$ and $n$:\n$$\nF_{\\mathrm{rand}} = ((2+4q)mn\\ell + 2mn\\ell) + (2m\\ell^2 + 2m\\ell^2) + (4n\\ell^2) + \\left(-\\frac{2}{3}\\ell^3 + \\frac{8}{3}\\ell^3\\right)\n$$\n$$\nF_{\\mathrm{rand}} = (4+4q)mn\\ell + 4m\\ell^2 + 4n\\ell^2 + \\frac{6}{3}\\ell^3\n$$\n$$\nF_{\\mathrm{rand}} = (4+4q)mn\\ell + 4m\\ell^2 + 4n\\ell^2 + 2\\ell^3\n$$\nThis can be written more compactly as:\n$$\nF_{\\mathrm{rand}}(m,n,k,p,q) = (4+4q)mn(k+p) + 4(m+n)(k+p)^2 + 2(k+p)^3\n$$\n\n### Part (b): Asymptotic Ratio of Operation Counts\n\nWe are asked to compute the limit of the ratio $R = F_{\\mathrm{rand}} / F_{\\mathrm{det}}$ in the asymptotic regime where $m, n \\to \\infty$ with $m/n \\to \\gamma \\in (1, \\infty)$, while $k$, $p$, and $q$ are fixed constants.\n\nThe expressions for the two costs are:\n$$\nF_{\\mathrm{rand}}(m,n,k,p,q) = (4+4q)mn(k+p) + 4(m+n)(k+p)^2 + 2(k+p)^3\n$$\n$$\nF_{\\mathrm{det}}(m,n,k) = 4mnk + 2(m+n)k^2 + \\frac{8}{3}k^3\n$$\nTo evaluate the limit of the ratio, we identify the dominant terms in the numerator and the denominator as $m, n \\to \\infty$. Since $k, p, q$ are fixed, the parameters $\\ell=k+p$ and the factors involving them are constant with respect to $m$ and $n$.\n\nThe term with the fastest growth in both expressions is the one proportional to $mn$. Other terms grow as $m$ or $n$, which is asymptotically smaller than $mn$.\n- The leading term of $F_{\\mathrm{rand}}$ is $(4+4q)mn(k+p)$.\n- The leading term of $F_{\\mathrm{det}}$ is $4mnk$.\n\nThe limit of the ratio is the ratio of the coefficients of the leading terms.\nA more formal way is to divide both the numerator and the denominator by $mn$:\n$$\nR = \\lim_{\\substack{m,n \\to \\infty \\\\ m/n \\to \\gamma}} \\frac{(4+4q)mn(k+p) + 4(m+n)(k+p)^2 + 2(k+p)^3}{4mnk + 2(m+n)k^2 + \\frac{8}{3}k^3}\n$$\n$$\nR = \\lim_{m,n \\to \\infty} \\frac{(4+4q)(k+p) + 4\\left(\\frac{1}{n}+\\frac{1}{m}\\right)(k+p)^2 + \\frac{2(k+p)^3}{mn}}{4k + 2\\left(\\frac{1}{n}+\\frac{1}{m}\\right)k^2 + \\frac{8k^3}{3mn}}\n$$\nAs $m \\to \\infty$ and $n \\to \\infty$, all terms containing $1/m$, $1/n$, or $1/mn$ approach zero:\n$$\n\\lim_{m,n \\to \\infty} \\left(\\frac{1}{n}+\\frac{1}{m}\\right) = 0 \\quad \\text{and} \\quad \\lim_{m,n \\to \\infty} \\frac{1}{mn} = 0\n$$\nThus, the limit simplifies to:\n$$\nR = \\frac{(4+4q)(k+p) + 0 + 0}{4k + 0 + 0} = \\frac{(4+4q)(k+p)}{4k}\n$$\nSimplifying the expression, we get:\n$$\nR = \\frac{4(1+q)(k+p)}{4k} = \\frac{(1+q)(k+p)}{k}\n$$\nThis can also be written as $R=(1+q)(1+p/k)$. The result is independent of the aspect ratio limit $\\gamma$, as expected from the problem framing.",
            "answer": "$$\\boxed{\\frac{(1+q)(k+p)}{k}}$$"
        },
        {
            "introduction": "While computational speed is a major benefit, it is meaningless without a guarantee of accuracy. This exercise tackles the other crucial aspect of randomized algorithms: their error characteristics. You will derive an analytical bound on the expected approximation error, revealing how the algorithm's performance is tied to the interplay between the chosen oversampling parameter $p$ and the intrinsic decay rate of the matrix's singular values . This practice is essential for developing an intuition for the trade-offs between computational resources and approximation quality.",
            "id": "3569805",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq 0$. Suppose the singular values of $A$ decay geometrically, i.e., there exist constants $s > 0$ and $\\rho \\in (0,1)$ such that $\\sigma_{j} = s \\rho^{j-1}$ for all $j \\geq 1$. Let $k \\geq 1$ be a target rank and $p \\geq 2$ be an oversampling parameter. Define $\\ell = k + p$, and draw a test matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$ whose entries are independent standard normal random variables (mean $0$, variance $1$). Form the sample matrix $Y = A \\Omega$, compute an orthonormal basis $Q \\in \\mathbb{R}^{m \\times \\ell}$ for the range of $Y$ via a thin QR factorization, and consider the orthogonal projector $QQ^{\\top}$ onto the column space of $Q$. \n\nStarting from the singular value decomposition, rotational invariance of the standard normal distribution, and fundamental properties of orthogonal projectors and the Frobenius norm, derive an analytic expression that quantifies the effect of choosing $\\ell = k + p$ with oversampling $p$ on the expected approximation error $\\mathbb{E}\\|A - QQ^{\\top}A\\|_{F}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. Assume only the geometric decay model for the singular values and standard facts about Gaussian matrices and orthogonal projectors; do not assume any specific formula for the expected error a priori. Your final result must be a single closed-form expression in terms of $s$, $\\rho$, $k$, and $p$ that upper bounds $\\mathbb{E}\\|A - QQ^{\\top}A\\|_{F}$ under the stated model. No numerical rounding is required.",
            "solution": "The problem asks for an analytic expression that upper bounds the expected approximation error $\\mathbb{E}\\|A - QQ^{\\top}A\\|_{F}$ for a randomized range finding algorithm, under a specific model for the singular values of the matrix $A$. The derivation must be based on first principles.\n\nLet $P_Y = QQ^{\\top}$ be the orthogonal projector onto the range of the sample matrix $Y = A\\Omega$. The approximation error is given by the matrix $E = A - P_Y A = (I - P_Y)A$. We are interested in an upper bound for the expected Frobenius norm of this error, $\\mathbb{E}\\|E\\|_{F}$.\n\nBy Jensen's inequality, we have $\\mathbb{E}\\|E\\|_{F} \\leq \\left(\\mathbb{E}\\|E\\|_{F}^2\\right)^{1/2}$. We will therefore proceed by deriving a bound on the expected squared Frobenius norm, $\\mathbb{E}\\|A - P_Y A\\|_{F}^2$.\n\nLet the singular value decomposition (SVD) of $A \\in \\mathbb{R}^{m \\times n}$ be $A = U\\Sigma V^{\\top}$. We partition $A$ into its best rank-$k$ approximation and a residual part: $A = A_k + A_{k\\perp}$, where $A_k = U_k \\Sigma_k V_k^{\\top}$ and $A_{k\\perp} = U_{k\\perp} \\Sigma_{k\\perp} V_{k\\perp}^{\\top}$. Here, $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ is the diagonal matrix of the first $k$ singular values, $\\sigma_1, \\dots, \\sigma_k$. The matrices $U_{k\\perp}, V_{k\\perp}, \\Sigma_{k\\perp}$ are defined analogously for the singular values from $k+1$ onwards. The column spaces of $A_k$ and $A_{k\\perp}$ are orthogonal, which implies $A_k^{\\top}A_{k\\perp} = 0$ and $A_{k\\perp}^{\\top}A_k = 0$.\n\nThe squared error is $\\|(I - P_Y)A\\|_{F}^2 = \\|(I - P_Y)(A_k + A_{k\\perp})\\|_{F}^2$. A standard result in the analysis of randomized algorithms, based on the orthogonality of the subspaces and the symmetry of the Gaussian distribution, states that the expectation of the cross-term vanishes:\n$$ \\mathbb{E}\\|(I - P_Y)A\\|_{F}^2 = \\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2 + \\mathbb{E}\\|(I - P_Y)A_{k\\perp}\\|_{F}^2 $$\nWe will bound each of these two terms separately.\n\nFor the second term, since $I - P_Y$ is an orthogonal projector, its operator norm is at most $1$. Thus, $\\|(I - P_Y)A_{k\\perp}\\|_{F} \\leq \\|I - P_Y\\|_{2} \\|A_{k\\perp}\\|_{F} \\leq \\|A_{k\\perp}\\|_{F}$. Therefore,\n$$ \\mathbb{E}\\|(I - P_Y)A_{k\\perp}\\|_{F}^2 \\leq \\mathbb{E}[\\|A_{k\\perp}\\|_{F}^2] = \\|A_{k\\perp}\\|_{F}^2 = \\sum_{j=k+1}^{\\min(m,n)} \\sigma_j^2 $$\nHere, we used the fact that $A_{k\\perp}$ is a fixed matrix.\n\nThe first term, $\\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2$, quantifies how much of the dominant part of $A$ is lost due to the projection. To bound this term, we construct an auxiliary matrix $Z$ in the range of $Y$ that approximates $A_k$. Let $\\Omega_k = V_k^{\\top}\\Omega \\in \\mathbb{R}^{k \\times \\ell}$ and $\\Omega_{k\\perp} = V_{k\\perp}^{\\top}\\Omega \\in \\mathbb{R}^{(n-k) \\times \\ell}$. Since $\\Omega$ is a standard Gaussian matrix and $V$ is orthogonal, $\\Omega_k$ and $\\Omega_{k\\perp}$ are independent standard Gaussian matrices. The sample matrix $Y$ can be written as:\n$$ Y = A\\Omega = (A_k + A_{k\\perp})\\Omega = A_k V V^{\\top} \\Omega + A_{k\\perp}V V^{\\top} \\Omega = U_k\\Sigma_k\\Omega_k + U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp} $$\nSince $\\ell = k+p > k$, the $k \\times \\ell$ matrix $\\Omega_k$ has rank $k$ with probability $1$. Thus, its Moore-Penrose pseudoinverse is $\\Omega_k^{\\dagger} = \\Omega_k^{\\top}(\\Omega_k \\Omega_k^{\\top})^{-1}$, and $\\Omega_k \\Omega_k^{\\dagger} = I_k$.\nConsider the matrix $Z = Y\\Omega_k^{\\dagger}V_k^{\\top}$. The columns of $Z$ are linear combinations of the columns of $Y$, so $\\text{range}(Z) \\subseteq \\text{range}(Y)$. The projection $P_Y A_k$ is the best approximation to $A_k$ from $\\text{range}(Y)$, so $\\|A_k - P_Y A_k\\|_{F} \\leq \\|A_k - Z\\|_{F}$.\nLet us analyze $A_k - Z$:\n\\begin{align*}\nA_k - Z &= A_k - (U_k\\Sigma_k\\Omega_k + U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp})\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= A_k - U_k\\Sigma_k(\\Omega_k\\Omega_k^{\\dagger})V_k^{\\top} - U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= A_k - U_k\\Sigma_k I_k V_k^{\\top} - U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= A_k - A_k - U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= -U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top}\n\\end{align*}\nUsing the unitary invariance of the Frobenius norm ($U_{k\\perp}$ has orthonormal columns), we get:\n$$ \\|(I - P_Y)A_k\\|_{F}^2 \\leq \\|A_k - Z\\|_{F}^2 = \\|-U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top}\\|_{F}^2 = \\|\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top}\\|_{F}^2 $$\nNow we take the expectation. Since $\\Omega_k$ and $\\Omega_{k\\perp}$ are independent, we can condition on $\\Omega_k$:\n$$ \\mathbb{E}_{\\Omega_{k\\perp}}[\\|\\Sigma_{k\\perp}\\Omega_{k\\perp}(\\Omega_k^{\\dagger}V_k^{\\top})\\|_{F}^2 | \\Omega_k] $$\nFor a fixed matrix $B$ and a standard Gaussian matrix $G$, $\\mathbb{E}\\|G B\\|_F^2 = \\|G\\|_F^2 \\|B\\|_F^2$ is not right. Instead, let $B = \\Omega_k^{\\dagger}V_k^{\\top}$. $\\mathbb{E}_{\\Omega_{k\\perp}}[\\|\\Sigma_{k\\perp}\\Omega_{k\\perp}B\\|_{F}^2] = \\text{Tr}(\\mathbb{E}[B^{\\top}\\Omega_{k\\perp}^{\\top}\\Sigma_{k\\perp}^2\\Omega_{k\\perp}B])$. A standard property of Gaussian matrices states that $\\mathbb{E}[\\Omega_{k\\perp}^{\\top}\\Sigma_{k\\perp}^2\\Omega_{k\\perp}] = \\text{Tr}(\\Sigma_{k\\perp}^2) I_{\\ell} = (\\sum_{j=k+1}^n \\sigma_j^2) I_{\\ell}$.\nThus, the conditional expectation is $(\\sum_{j=k+1}^n \\sigma_j^2) \\text{Tr}(B^{\\top}B) = (\\sum_{j=k+1}^n \\sigma_j^2) \\|B\\|_{F}^2$.\nHere, $\\|B\\|_{F}^2 = \\|\\Omega_k^{\\dagger}V_k^{\\top}\\|_{F}^2 = \\|\\Omega_k^{\\dagger}\\|_{F}^2$, since $V_k^{\\top}$ has orthonormal rows.\nTaking the outer expectation over $\\Omega_k$:\n$$ \\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2 \\leq \\left(\\sum_{j=k+1}^n \\sigma_j^2\\right) \\mathbb{E}[\\|\\Omega_k^{\\dagger}\\|_{F}^2] $$\nThe term $\\mathbb{E}[\\|\\Omega_k^{\\dagger}\\|_{F}^2]$ is the expected trace of an inverse Wishart matrix. For a $k \\times \\ell$ standard Gaussian matrix $\\Omega_k$ with $\\ell = k+p$ and $p > 1$, this expectation is known:\n$$ \\mathbb{E}[\\|\\Omega_k^{\\dagger}\\|_{F}^2] = \\mathbb{E}[\\text{Tr}((\\Omega_k \\Omega_k^{\\top})^{-1})] = \\frac{k}{p-1} $$\nThis requires $p \\ge 2$, as given in the problem statement.\nSo, we have the bound:\n$$ \\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2 \\leq \\frac{k}{p-1} \\sum_{j=k+1}^n \\sigma_j^2 $$\nCombining the bounds for the two terms:\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F}^2 \\leq \\frac{k}{p-1} \\sum_{j=k+1}^n \\sigma_j^2 + \\sum_{j=k+1}^n \\sigma_j^2 = \\left(1 + \\frac{k}{p-1}\\right) \\sum_{j=k+1}^n \\sigma_j^2 $$\nFinally, we substitute the given model for singular values, $\\sigma_j = s \\rho^{j-1}$. The sum of the squared tail singular values becomes a geometric series. We consider the sum to infinity, which provides an upper bound for any finite $n$:\n$$ \\sum_{j=k+1}^{\\infty} \\sigma_j^2 = \\sum_{j=k+1}^{\\infty} (s \\rho^{j-1})^2 = s^2 \\sum_{j=k+1}^{\\infty} (\\rho^2)^{j-1} $$\nLet $i=j-1$. The sum is $s^2 \\sum_{i=k}^{\\infty} (\\rho^2)^i$. This is a geometric series with first term $a = s^2(\\rho^2)^k = s^2\\rho^{2k}$ and ratio $r=\\rho^2$. Since $\\rho \\in (0,1)$, the sum is:\n$$ \\sum_{j=k+1}^{\\infty} \\sigma_j^2 = \\frac{s^2\\rho^{2k}}{1-\\rho^2} $$\nSubstituting this into our bound for the expected squared error:\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F}^2 \\leq \\left(1 + \\frac{k}{p-1}\\right) \\frac{s^2\\rho^{2k}}{1-\\rho^2} $$\nBy applying Jensen's inequality to the original error, we take the square root of this upper bound:\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F} \\leq \\left(\\mathbb{E}\\|A - P_Y A\\|_{F}^2\\right)^{1/2} \\leq \\left( \\left(1 + \\frac{k}{p-1}\\right) \\frac{s^2\\rho^{2k}}{1-\\rho^2} \\right)^{1/2} $$\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F} \\leq \\left(1 + \\frac{k}{p-1}\\right)^{1/2} \\frac{s\\rho^k}{\\sqrt{1-\\rho^2}} $$\nThis is the desired closed-form expression.",
            "answer": "$$\\boxed{s \\left(1 + \\frac{k}{p-1}\\right)^{\\frac{1}{2}} \\frac{\\rho^k}{\\sqrt{1-\\rho^2}}}$$"
        },
        {
            "introduction": "The power of randomized methods lies in their adaptability to specific data structures. Many real-world matrices are sparse, and leveraging this structure is key to achieving maximum efficiency. This practice moves from the general dense-matrix setting to this important special case, challenging you to analyze a randomized SVD variant designed specifically for sparse inputs using a CountSketch operator . By examining the modified cost and accuracy trade-offs, you will learn how to tailor these powerful algorithms for optimal performance in practical applications.",
            "id": "3569838",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ whose $n$ columns are each $s$-sparse, meaning every column contains exactly $s$ nonzero entries, so that the total number of nonzeros satisfies $\\mathrm{nnz}(A) = n s$. Fix a target rank $k$ and an oversampling parameter $p \\geq 2$, and define $\\ell = k + p$. Design a randomized singular value decomposition (SVD) variant that exploits the column sparsity via a sparse subspace embedding, and from first principles derive a symbolic expression for its total arithmetic cost and the Frobenius-norm accuracy tradeoff.\n\nYour algorithm must use the following steps:\n- Construct a CountSketch (CS) matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$ with exactly $1$ nonzero per row: for each $j \\in \\{1,\\dots,n\\}$, choose a hash $h(j) \\in \\{1,\\dots,\\ell\\}$ and a sign $d(j) \\in \\{-1,+1\\}$, and set $\\Omega_{j,h(j)} = d(j)$ and $\\Omega_{j,r} = 0$ for $r \\neq h(j)$.\n- Form the sample matrix $Y = A \\Omega \\in \\mathbb{R}^{m \\times \\ell}$.\n- Compute a thin QR factorization $Y = Q R$ with $Q \\in \\mathbb{R}^{m \\times \\ell}$ having orthonormal columns.\n- Form $B = Q^{\\top} A \\in \\mathbb{R}^{\\ell \\times n}$.\n- Form the Gram matrix $W = B B^{\\top} \\in \\mathbb{R}^{\\ell \\times \\ell}$, compute its eigen-decomposition $W = \\widetilde{U} \\Sigma^{2} \\widetilde{U}^{\\top}$, and set the approximate left singular vectors $U_{k} = Q \\widetilde{U}_{k}$, where $\\widetilde{U}_{k}$ are the first $k$ columns of $\\widetilde{U}$. Recover the right singular vectors via $V_{k} = B^{\\top} \\widetilde{U}_{k} \\Sigma_{k}^{-1}$, where $\\Sigma_{k}$ is the leading $k \\times k$ principal block of $\\Sigma$.\n\nWork under the foundational base that:\n- Sparse matrix–dense matrix multiplication with a matrix of $t$ columns costs $2 \\, \\mathrm{nnz}(A) \\, t$ floating-point operations (flops) when each nonzero contributes one multiply and one add per output column it touches, and that multiplying by a CS matrix with one nonzero per row costs $2 \\, \\mathrm{nnz}(A)$ flops since each nonzero contributes exactly one multiply-and-add into a single bucket.\n- Householder QR factorization of an $m \\times \\ell$ matrix with $m \\geq \\ell$ costs $2 m \\ell^{2} - \\frac{2}{3} \\ell^{3}$ flops.\n- Forming $B B^{\\top}$ for $B \\in \\mathbb{R}^{\\ell \\times n}$ costs $2 n \\ell^{2}$ flops.\n- Symmetric eigen-decomposition of an $\\ell \\times \\ell$ matrix via tridiagonal reduction plus the implicit-shifted QR algorithm costs $c_{\\mathrm{eig}} \\, \\ell^{3}$ flops for a method-dependent constant $c_{\\mathrm{eig}} > 0$ that does not depend on $m$, $n$, $s$, $k$, or $p$.\n- Multiplying an $m \\times \\ell$ matrix by an $\\ell \\times k$ matrix costs $2 m \\ell k$ flops, and multiplying an $n \\times \\ell$ matrix by an $\\ell \\times k$ matrix costs $2 n \\ell k$ flops.\n\nFor the accuracy tradeoff, take as base the subspace embedding principle originating in the Johnson–Lindenstrauss (JL) lemma: if $\\Omega$ is a subspace embedding for all $k$-dimensional subspaces of $\\mathbb{R}^{n}$ with distortion parameter $\\epsilon \\in (0,1)$, then projecting $A$ onto the range of $Y = A \\Omega$ yields a rank-$k$ approximation whose Frobenius-norm error is within a factor $(1 + \\epsilon)$ of the optimal error. For CountSketch, assume $\\ell \\geq C k^{2} \\epsilon^{-2}$ with a universal constant $C > 0$ so that the embedding condition holds with high probability.\n\nDerive a single closed-form analytical expression for the total flop count $T(m,n,s,k,p)$ of the above procedure and for the Frobenius-norm error $E(k,p,\\epsilon,\\{\\sigma_{j}\\})$, where $\\{\\sigma_{j}\\}$ are the singular values of $A$ in nonincreasing order and $A_{k}$ denotes the best rank-$k$ approximation to $A$ from the Eckart–Young theorem. Express your final answer as a row matrix containing $(T, E)$, with $\\ell = k + p$ substituted. No inequalities or big-$\\mathcal{O}$ notation may appear in the final answer. The final answer must be a single analytical expression. If you introduce any method-dependent constant in your derivation, it must be kept symbolic in the final expression. There is no rounding requirement for this problem.",
            "solution": "This solution calculates the total arithmetic cost, $T(m,n,s,k,p)$, for the specified sparse randomized SVD algorithm and derives the corresponding Frobenius-norm error bound, $E(k,p,C,\\{\\sigma_{j}\\})$.\n\nLet $\\ell = k+p$.\n\n**Part 1: Derivation of the Total Arithmetic Cost**\n\nThe total flop count $T$ is the sum of the costs of the individual algorithmic steps.\n\n1.  **Form the sample matrix $Y = A \\Omega$**:\n    The matrix $A \\in \\mathbb{R}^{m \\times n}$ has $\\mathrm{nnz}(A) = ns$ nonzeros. The sketch matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$ is a CountSketch matrix with $1$ nonzero per row. The problem states that the cost of this multiplication is $2 \\, \\mathrm{nnz}(A)$ flops.\n    $$C_1 = 2 \\, \\mathrm{nnz}(A) = 2ns$$\n\n2.  **Compute the QR factorization $Y = QR$**:\n    The matrix $Y \\in \\mathbb{R}^{m \\times \\ell}$ is factorized into $Q \\in \\mathbb{R}^{m \\times \\ell}$ and $R \\in \\mathbb{R}^{\\ell \\times \\ell}$. Assuming $m \\geq \\ell$, the cost of Householder QR is given as $2m\\ell^2 - \\frac{2}{3}\\ell^3$.\n    $$C_2 = 2m\\ell^2 - \\frac{2}{3}\\ell^3$$\n\n3.  **Form the projected matrix $B = Q^{\\top} A$**:\n    This is a dense-sparse matrix multiplication. The operation can be viewed as $(A^{\\top} Q)^{\\top}$. $A^{\\top} \\in \\mathbb{R}^{n \\times m}$ is a sparse matrix with $\\mathrm{nnz}(A^{\\top}) = \\mathrm{nnz}(A) = ns$. $Q \\in \\mathbb{R}^{m \\times \\ell}$ is a dense matrix with $\\ell$ columns. The cost for a sparse-matrix-dense-matrix multiplication with a matrix of $t$ columns is given as $2 \\, \\mathrm{nnz}(\\text{sparse matrix}) \\, t$. Here, the sparse matrix is $A^{\\top}$ and $t=\\ell$.\n    $$C_3 = 2 \\, \\mathrm{nnz}(A^{\\top}) \\, \\ell = 2(ns)\\ell = 2ns\\ell$$\n\n4.  **Form the Gram matrix $W = B B^{\\top}$**:\n    The matrix $B \\in \\mathbb{R}^{\\ell \\times n}$. The cost to form the $\\ell \\times \\ell$ Gram matrix $W$ is given as $2n\\ell^2$.\n    $$C_4 = 2n\\ell^2$$\n\n5.  **Compute the eigen-decomposition of $W$**:\n    The matrix $W \\in \\mathbb{R}^{\\ell \\times \\ell}$ is symmetric. Its eigen-decomposition cost is given as $c_{\\mathrm{eig}} \\ell^3$ for a constant $c_{\\mathrm{eig}}$.\n    $$C_5 = c_{\\mathrm{eig}}\\ell^3$$\n\n6.  **Form the approximate left singular vectors $U_k = Q \\widetilde{U}_k$**:\n    This is a multiplication of an $m \\times \\ell$ matrix ($Q$) by an $\\ell \\times k$ matrix ($\\widetilde{U}_k$). The cost is given as $2m\\ell k$.\n    $$C_6 = 2m\\ell k$$\n\n7.  **Recover the approximate right singular vectors $V_k = B^{\\top} \\widetilde{U}_k \\Sigma_k^{-1}$**:\n    This step involves two sub-steps:\n    a. Matrix multiplication $Z = B^{\\top} \\widetilde{U}_k$: $B^{\\top}$ is an $n \\times \\ell$ matrix and $\\widetilde{U}_k$ is an $\\ell \\times k$ matrix. The cost is $2n\\ell k$.\n    b. Column scaling $V_k = Z \\Sigma_k^{-1}$: The matrix $\\Sigma_k^{-1}$ is diagonal. This operation scales each of the $k$ columns of $Z$ (of length $n$) by a scalar. This requires $nk$ multiplications, costing $nk$ flops.\n    The total cost for this step is the sum:\n    $$C_7 = 2n\\ell k + nk$$\n\nThe total cost $T$ is the sum of these partial costs:\n$$T = C_1 + C_2 + C_3 + C_4 + C_5 + C_6 + C_7$$\n$$T = 2ns + \\left(2m\\ell^2 - \\frac{2}{3}\\ell^3\\right) + 2ns\\ell + 2n\\ell^2 + c_{\\mathrm{eig}}\\ell^3 + 2m\\ell k + (2n\\ell k + nk)$$\nWe group terms by dependencies on $m$, $n$, and terms independent of them:\n$$T = (2ns + 2ns\\ell + 2n\\ell^2 + 2n\\ell k + nk) + (2m\\ell^2 + 2m\\ell k) + \\left(c_{\\mathrm{eig}}\\ell^3 - \\frac{2}{3}\\ell^3\\right)$$\nFactoring out common terms:\n$$T = n(2s(1+\\ell) + 2\\ell^2 + 2\\ell k + k) + m(2\\ell^2 + 2\\ell k) + \\left(c_{\\mathrm{eig}} - \\frac{2}{3}\\right)\\ell^3$$\nFurther factorization yields a more compact form:\n$$T = n(2s(1+\\ell) + 2\\ell(\\ell+k) + k) + m(2\\ell(\\ell+k)) + \\left(c_{\\mathrm{eig}} - \\frac{2}{3}\\right)\\ell^3$$\n$$T = 2ns(1+\\ell) + 2(n+m)\\ell(\\ell+k) + nk + \\left(c_{\\mathrm{eig}} - \\frac{2}{3}\\right)\\ell^3$$\nFinally, we substitute $\\ell = k+p$:\n$$T(m,n,s,k,p) = 2ns(k+p+1) + 2(n+m)(k+p)(2k+p) + nk + \\left(c_{\\mathrm{eig}} - \\frac{2}{3}\\right)(k+p)^3$$\n\n**Part 2: Derivation of the Frobenius-Norm Error**\n\nThe problem provides the principle for the error analysis: if $\\Omega$ is a suitable subspace embedding, the resulting rank-$k$ approximation has a Frobenius-norm error $\\|A - A'_k\\|_F$ that is within a factor $(1+\\epsilon)$ of the optimal error $\\|A-A_k\\|_F$. The optimal rank-$k$ approximation error, from the Eckart-Young-Mirsky theorem, is given by the sum of the squares of the tail singular values:\n$$\\|A-A_k\\|_F = \\left( \\sum_{j=k+1}^{\\min(m,n)} \\sigma_j^2 \\right)^{1/2}$$\nThe problem states that for CountSketch, the subspace embedding property holds with distortion $\\epsilon$ if the sketch size $\\ell$ satisfies $\\ell \\geq C k^2 \\epsilon^{-2}$ for some universal constant $C>0$.\nTo obtain a specific expression for the error rather than an inequality, as required by the prompt (\"No inequalities... may appear in the final answer\"), we consider the boundary condition that defines the trade-off:\n$$\\ell = C k^2 \\epsilon^{-2}$$\nSolving for $\\epsilon$, we get:\n$$\\epsilon^2 = \\frac{C k^2}{\\ell} \\implies \\epsilon = k \\sqrt{\\frac{C}{\\ell}}$$\nThe problem states the error of the algorithm's approximation is within $(1+\\epsilon)$ of the optimal one. We formulate this as an equality representing the worst-case bound:\n$$E = \\|A - A'_k\\|_F = (1+\\epsilon) \\|A-A_k\\|_F$$\nSubstituting the expression for $\\epsilon$:\n$$E = \\left(1 + k \\sqrt{\\frac{C}{\\ell}}\\right) \\left( \\sum_{j=k+1}^{\\min(m,n)} \\sigma_j^2 \\right)^{1/2}$$\nLastly, substituting $\\ell = k+p$ gives the final expression for the error:\n$$E(k,p,C,\\{\\sigma_j\\}) = \\left(1 + k \\sqrt{\\frac{C}{k+p}}\\right) \\left( \\sum_{j=k+1}^{\\min(m,n)} \\sigma_j^2 \\right)^{1/2}$$\nThe expression depends on the constant $C$, which is permissible as it is an introduced method-dependent constant.\n\nThe final answer is a row matrix containing the derived expressions for the total cost $T$ and the error $E$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2ns(k+p+1) + 2(n+m)(k+p)(2k+p) + nk + \\left(c_{\\mathrm{eig}} - \\frac{2}{3}\\right)(k+p)^3 & \\left(1 + k \\sqrt{\\frac{C}{k+p}}\\right) \\left( \\sum_{j=k+1}^{\\min(m,n)} \\sigma_j^2 \\right)^{1/2}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}