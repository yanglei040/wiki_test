{
    "hands_on_practices": [
        {
            "introduction": "The foundation of solving any least squares problem lies in a robust numerical method. This first practice focuses on the weighted least squares (WLS) problem, demonstrating from first principles how to derive the solution using the Singular Value Decomposition (SVD) . By comparing this approach to the more naive normal equations, you will gain a crucial, hands-on understanding of numerical stability and why avoiding the formation of $A^T W A$ is paramount in high-precision scientific computing.",
            "id": "3601195",
            "problem": "Consider a weighted least squares problem in which one seeks to minimize the weighted residual norm $\\|A x - b\\|_{W}$ with respect to a symmetric positive definite weight matrix $W \\in \\mathbb{R}^{m \\times m}$. Let $W = L^{T} L$ be a Cholesky factorization. It is well known that this problem is equivalent to the unweighted least squares problem $\\min_{x} \\|L(Ax - b)\\|_{2}$, which depends only on the matrices $L A$ and $L b$. You are given direct access to $L A \\in \\mathbb{R}^{3 \\times 2}$ and $L b \\in \\mathbb{R}^{3}$:\n$$\nL A \\;=\\; \\begin{bmatrix}\n200 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix},\n\\qquad\nL b \\;=\\; \\begin{bmatrix}\n100 \\\\\n3 \\\\\n7\n\\end{bmatrix}.\n$$\n\n(a) Starting from the foundational definitions of least squares and the singular value decomposition (SVD), derive the weighted least squares minimizer $x^{\\star}$ without forming $A^{T} W A$. Your derivation should proceed by modeling the problem as $\\min_{x} \\|C x - d\\|_{2}$ with $C = L A$ and $d = L b$, then using the singular value decomposition (SVD) of $C$ to express $x^{\\star}$ in terms of the SVD factors. Do not assume any pre-stated “shortcut” formulas; instead, reason from the SVD orthogonality and optimality properties.\n\n(b) Use your SVD-based expression to compute the numerical value of $x^{\\star}$ for the given $L A$ and $L b$.\n\n(c) For numerical stability comparison, use the spectral condition number $\\kappa_{2}(\\cdot)$ to quantify the sensitivity of the normal equations approach versus the SVD-based approach. Compute\n$$\nR \\;=\\; \\frac{\\kappa_{2}\\!\\left((L A)^{T} (L A)\\right)}{\\kappa_{2}(L A)}.\n$$\nExplain why $R$ captures the amplification in conditioning induced by forming the normal equations, and evaluate $R$ for the given data.\n\nProvide your final numeric summary as a row vector $[\\,x_1^{\\star},\\, x_2^{\\star},\\, R\\,]$. No rounding is necessary for this problem.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical linear algebra, well-posed with a unique solution, and objectively formulated. All data required for the solution are provided, and there are no internal contradictions.\n\nThe task is to solve a weighted least squares problem, which is formulated as an equivalent unweighted least squares problem:\n$$\n\\min_{x \\in \\mathbb{R}^2} \\|C x - d\\|_{2}\n$$\nwhere $C = LA$ and $d = Lb$. The given matrices are:\n$$\nC \\;=\\; \\begin{bmatrix}\n200 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix},\n\\qquad\nd \\;=\\; \\begin{bmatrix}\n100 \\\\\n3 \\\\\n7\n\\end{bmatrix}\n$$\n\n(a) Derivation of the SVD-based minimizer $x^{\\star}$:\n\nWe want to find the vector $x^{\\star}$ that minimizes the Euclidean norm of the residual, $\\|Cx - d\\|_{2}$. Minimizing the norm is equivalent to minimizing its square, $\\|Cx - d\\|_{2}^{2}$.\n\nLet the singular value decomposition (SVD) of the matrix $C \\in \\mathbb{R}^{m \\times n}$ be $C = U \\Sigma V^{T}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values $\\sigma_i$.\nSubstituting the SVD into the squared norm expression, we get:\n$$\n\\|Cx - d\\|_{2}^{2} = \\|U \\Sigma V^{T} x - d\\|_{2}^{2}\n$$\nSince the Euclidean norm is invariant under orthogonal transformations, we can multiply the vector inside the norm by $U^{T}$ from the left without changing the value of the norm. This is because $\\|y\\|_2^2 = y^T y$ and for an orthogonal matrix $U$, $(U^T y)^T (U^T y) = y^T U U^T y = y^T I y = y^T y = \\|y\\|_2^2$.\n$$\n\\|Cx - d\\|_{2}^{2} = \\|U^{T}(U \\Sigma V^{T} x - d)\\|_{2}^{2} = \\|(U^{T}U) \\Sigma V^{T} x - U^{T} d\\|_{2}^{2} = \\|\\Sigma V^{T} x - U^{T} d\\|_{2}^{2}\n$$\nWe perform a change of variables by letting $y = V^{T} x$. Since $V$ is orthogonal, $V^T V = I$, and we can recover $x$ via $x = V y$. This transformation maps the space $\\mathbb{R}^n$ onto itself, so minimizing over $x$ is equivalent to minimizing over $y$. Let $\\tilde{d} = U^{T} d$. The problem is now transformed into:\n$$\n\\min_{y \\in \\mathbb{R}^n} \\|\\Sigma y - \\tilde{d}\\|_{2}^{2}\n$$\nLet $r$ be the rank of $C$. The singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r > 0$, and $\\sigma_i = 0$ for $i > r$. The matrix $\\Sigma$ has the block structure $\\Sigma = \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix}$, where $\\Sigma_r = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_r)$. We partition the vectors $y$ and $\\tilde{d}$ accordingly:\n$$\ny = \\begin{pmatrix} y_r \\\\ y_{n-r} \\end{pmatrix}, \\quad \\tilde{d} = \\begin{pmatrix} \\tilde{d}_r \\\\ \\tilde{d}_{m-r} \\end{pmatrix}\n$$\nwhere $y_r, \\tilde{d}_r \\in \\mathbb{R}^r$. The squared norm becomes:\n$$\n\\|\\Sigma y - \\tilde{d}\\|_{2}^{2} = \\left\\| \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} y_r \\\\ y_{n-r} \\end{pmatrix} - \\begin{pmatrix} \\tilde{d}_r \\\\ \\tilde{d}_{m-r} \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} \\Sigma_r y_r - \\tilde{d}_r \\\\ - \\tilde{d}_{m-r} \\end{pmatrix} \\right\\|_{2}^{2} = \\|\\Sigma_r y_r - \\tilde{d}_r\\|_{2}^{2} + \\|\\tilde{d}_{m-r}\\|_{2}^{2}\n$$\nTo minimize this expression, we must choose $y_r$ to minimize the first term, $\\|\\Sigma_r y_r - \\tilde{d}_r\\|_{2}^{2}$. The second term, $\\|\\tilde{d}_{m-r}\\|_{2}^{2}$, is the squared norm of the residual and is independent of $y$. Since $\\Sigma_r$ is a diagonal matrix with positive diagonal entries, it is invertible. The minimum value of the first term is $0$, achieved when $\\Sigma_r y_r - \\tilde{d}_r = 0$, which gives $y_r = \\Sigma_r^{-1} \\tilde{d}_r$.\n\nThe components $y_{n-r}$ of $y$ do not affect the residual norm. For a unique solution, one typically seeks the minimum norm solution for $x$, which corresponds to the minimum norm solution for $y$. This is obtained by setting the free components to zero: $y_{n-r} = 0$.\n\nThe solution for $y$ is thus $y^{\\star}$ where its first $r$ components are given by $y_r = \\Sigma_r^{-1} \\tilde{d}_r$ and its remaining $n-r$ components are $0$. This can be expressed compactly using the pseudoinverse of $\\Sigma$, denoted $\\Sigma^{\\dagger}$, as $y^{\\star} = \\Sigma^{\\dagger} \\tilde{d}$.\n\nFinally, transforming back to the original variable $x$, we obtain the minimizer $x^{\\star}$:\n$$\nx^{\\star} = V y^{\\star} = V \\Sigma^{\\dagger} \\tilde{d} = V \\Sigma^{\\dagger} (U^{T} d) = (V \\Sigma^{\\dagger} U^{T}) d\n$$\nThe expression $V \\Sigma^{\\dagger} U^{T}$ is the definition of the Moore-Penrose pseudoinverse of $C$, denoted $C^{\\dagger}$. Thus, the solution is $x^{\\star} = C^{\\dagger} d$.\n\n(b) Numerical computation of $x^{\\star}$:\n\nFirst, we find the SVD of $C = \\begin{bmatrix} 200 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}$. The singular values are the square roots of the eigenvalues of $C^T C$.\n$$\nC^{T}C = \\begin{bmatrix} 200 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 200 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 40000 & 0 \\\\ 0 & 1 \\end{bmatrix}\n$$\nThe eigenvalues are $\\lambda_1 = 40000$ and $\\lambda_2 = 1$. The singular values are $\\sigma_1 = \\sqrt{40000} = 200$ and $\\sigma_2 = \\sqrt{1} = 1$. Since $C$ is a $3 \\times 2$ matrix, the SVD form of $\\Sigma$ is:\n$$\n\\Sigma = \\begin{bmatrix} 200 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n$$\nThe right singular vectors (columns of $V$) are the eigenvectors of $C^T C$. Since $C^T C$ is diagonal, the eigenvectors are the standard basis vectors, so $V = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I_2$.\nThe left singular vectors (columns of $U$) are given by $u_i = \\frac{1}{\\sigma_i} C v_i$ for $i=1, 2$.\n$$\nu_1 = \\frac{1}{200} \\begin{bmatrix} 200 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\n$$\nu_2 = \\frac{1}{1} \\begin{bmatrix} 200 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n$$\nTo complete the orthogonal matrix $U \\in \\mathbb{R}^{3 \\times 3}$, we need a third vector $u_3$ orthogonal to $u_1$ and $u_2$. The clear choice is $u_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$. Thus, $U = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = I_3$.\nThe SVD of $C$ is $C = I_3 \\Sigma I_2^T$.\n\nNow we compute $x^{\\star} = V \\Sigma^{\\dagger} U^T d$.\nThe pseudoinverse $\\Sigma^{\\dagger} \\in \\mathbb{R}^{2 \\times 3}$ is found by taking the reciprocal of the non-zero singular values and transposing the resulting matrix.\n$$\n\\Sigma^{\\dagger} = \\begin{bmatrix} 1/200 & 0 & 0 \\\\ 0 & 1/1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1/200 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}\n$$\nUsing $V=I_2$ and $U=I_3$, the expression for $x^{\\star}$ simplifies:\n$$\nx^{\\star} = I_2 \\Sigma^{\\dagger} I_3^T d = \\Sigma^{\\dagger} d\n$$\nSubstituting the values of $\\Sigma^{\\dagger}$ and $d$:\n$$\nx^{\\star} = \\begin{bmatrix} 1/200 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 100 \\\\ 3 \\\\ 7 \\end{bmatrix} = \\begin{bmatrix} (1/200) \\times 100 \\\\ 1 \\times 3 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ 3 \\end{bmatrix}\n$$\nSo, $x_1^{\\star} = 1/2$ and $x_2^{\\star} = 3$.\n\n(c) Condition number analysis:\n\nThe spectral condition number of a matrix $M$ is $\\kappa_2(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $M$, respectively. For $C=LA$, the singular values are $\\sigma_1 = 200$ and $\\sigma_2 = 1$. Therefore,\n$$\n\\kappa_2(C) = \\frac{200}{1} = 200\n$$\nThe normal equations approach requires solving $(C^T C) x = C^T d$. The sensitivity of this system is determined by $\\kappa_2(C^T C)$. For a symmetric positive definite matrix like $C^T C$, the singular values are its eigenvalues, so $\\kappa_2(C^T C) = \\frac{\\lambda_{\\max}(C^T C)}{\\lambda_{\\min}(C^T C)}$.\nThe eigenvalues of $C^T C$ are the squares of the singular values of $C$. Thus, $\\lambda_{\\max}(C^T C) = \\sigma_{\\max}(C)^2$ and $\\lambda_{\\min}(C^T C) = \\sigma_{\\min}(C)^2$.\nSo, $\\kappa_2(C^T C) = \\frac{\\sigma_{\\max}(C)^2}{\\sigma_{\\min}(C)^2} = \\left(\\frac{\\sigma_{\\max}(C)}{\\sigma_{\\min}(C)}\\right)^2 = (\\kappa_2(C))^2$.\nFor the given matrix $C$, we have:\n$$\n\\kappa_2(C^T C) = (200)^2 = 40000\n$$\nThe ratio $R$ is:\n$$\nR = \\frac{\\kappa_{2}(C^{T} C)}{\\kappa_{2}(C)} = \\frac{(\\kappa_2(C))^2}{\\kappa_2(C)} = \\kappa_2(C)\n$$\nFor the given data, $R = 200$.\n\nThe quantity $R$ captures the amplification in conditioning because it is the ratio of the condition number of the matrix in the normal equations approach to the condition number of the matrix in the SVD (or QR) approach. The normal equations method solves a linear system involving $C^T C$, whose condition number is $\\kappa_2(C^T C)$. Methods that work directly with $C$, such as SVD or QR factorization, confront a problem whose numerical sensitivity is governed by $\\kappa_2(C)$.\nSince $R = \\kappa_2(C)$, the ratio tells us that $\\kappa_2(C^T C) = R \\times \\kappa_2(C) = (\\kappa_2(C))^2$. This squaring of the condition number is a hallmark of the normal equations. If $C$ is ill-conditioned (i.e., $\\kappa_2(C)$ is large), forming $C^T C$ results in a problem that is substantially more ill-conditioned. This can lead to a severe loss of numerical precision when solving the system, as the potential magnification of relative errors in the data scales with the condition number. In this case, forming the normal equations creates a system that is $200$ times more sensitive to perturbations than the original least squares problem's matrix implied.\n\nSummary of numeric results: $x_1^{\\star} = 1/2$, $x_2^{\\star} = 3$, $R = 200$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} & 3 & 200 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Many real-world models require that the solution adhere to strict linear equality constraints. This exercise introduces the null-space method, an elegant and geometrically intuitive technique for solving equality-constrained least squares (ECLS) problems . By parameterizing the entire feasible set of solutions, you will learn to reduce a constrained problem into a smaller, unconstrained one, a powerful skill applicable to a wide range of optimization challenges.",
            "id": "3601196",
            "problem": "Consider the Equality-Constrained Least Squares (ECLS) problem within the framework of Weighted Least Squares (WLS): minimize the weighted residual norm of the linear model subject to linear equality constraints. The weighted residual norm is defined by the Euclidean norm of the residual premultiplied by the square root of the weighting matrix. Specifically, let the design matrix be $A \\in \\mathbb{R}^{4 \\times 3}$, the observation vector be $b \\in \\mathbb{R}^{4}$, and the symmetric positive definite weighting matrix be $W \\in \\mathbb{R}^{4 \\times 4}$. Let the equality constraint be given by $C \\in \\mathbb{R}^{2 \\times 3}$ and $d \\in \\mathbb{R}^{2}$.\n\nUse the data\n$$\nA=\\begin{pmatrix}\n2 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 0 \\\\\n0 & 2 & 3\n\\end{pmatrix},\\quad\nb=\\begin{pmatrix}\n3 \\\\ 1 \\\\ 2 \\\\ 5\n\\end{pmatrix},\\quad\nW=\\operatorname{diag}(1,2,3,1),\n$$\nand the constraints\n$$\nC=\\begin{pmatrix}\n1 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix},\\quad\nd=\\begin{pmatrix}\n0 \\\\ 1\n\\end{pmatrix}.\n$$\n\nStarting from the fundamental definition of the weighted objective $f(x)=\\|W^{1/2}(A x-b)\\|_{2}^{2}$ and the equality constraints $C x=d$, carry out the following steps without invoking any pre-derived shortcut formulas: construct a basis $Z$ for the null space of $C$ satisfying $C Z=0$, find a particular feasible point $x_{p}$ with $C x_{p}=d$, parameterize the feasible set as $x=x_{p}+Z y$ for an appropriate parameter $y$, reduce the constrained problem to an unconstrained WLS in $y$, and solve for the unique minimizer $y^{\\star}$. Then compute the minimized weighted residual sum of squares\n$$\nJ^{\\star}=\\min_{C x=d}\\|W^{1/2}(A x-b)\\|_{2}^{2}.\n$$\n\nExpress your final answer for $J^{\\star}$ as an exact value. No rounding is required. The final answer must be a single real number.",
            "solution": "The problem is to solve an Equality-Constrained Least Squares (ECLS) problem. Specifically, we want to find the solution $x \\in \\mathbb{R}^{3}$ that minimizes the weighted objective function $J(x) = \\|W^{1/2}(A x-b)\\|_{2}^{2}$ subject to the linear equality constraints $C x=d$.\n\n### Step 1: Extract Givens\nThe data provided are:\n- Design matrix: $A=\\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 2 & 3 \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3}$\n- Observation vector: $b=\\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\\\ 5 \\end{pmatrix} \\in \\mathbb{R}^{4}$\n- Weighting matrix: $W=\\operatorname{diag}(1, 2, 3, 1) \\in \\mathbb{R}^{4 \\times 4}$\n- Constraint matrix: $C=\\begin{pmatrix} 1 & -1 & 0 \\\\ 1 & 1 & 1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}$\n- Constraint vector: $d=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined instance of an Equality-Constrained Weighted Least Squares problem in numerical linear algebra.\n- **Scientifically Grounded**: The problem is based on established principles of linear algebra and optimization. The weighting matrix $W$ is diagonal with positive entries, hence it is symmetric and positive definite, which is a standard requirement for a weighted least squares problem.\n- **Well-Posed**: The dimensions of the matrices and vectors are consistent for the operations involved. The constraint matrix $C$ is a $2 \\times 3$ matrix. Its rows are linearly independent, so its rank is $2$. Since $C$ has full row rank, the system of constraints $Cx=d$ is consistent and a feasible solution exists. The method outlined (null space parameterization) leads to a unique solution because the resulting unconstrained problem will have a matrix $AZ$ which has full column rank.\n- **Objective**: The problem is stated with precise mathematical definitions and numerical data. There is no ambiguity or subjectivity.\n\nThe problem is valid.\n\n### Step 3: Action\nThe problem is valid, so I will proceed with the solution. The solution will follow the method prescribed in the problem statement. The overall objective is to find $x^{\\star} = \\arg\\min_{Cx=d} (A x-b)^T W (A x-b)$ and then compute the minimum objective value, $J^{\\star}$.\n\n**1. Characterize the Feasible Set**\nThe set of all solutions to the constraint equation $Cx=d$ can be written as $x = x_p + z_h$, where $x_p$ is any particular solution to $Cx_p=d$, and $z_h$ is any vector in the null space of $C$ (i.e., $Cz_h = 0$).\n\n**1a. Find a basis $Z$ for the null space of $C$.**\nWe need to find vectors $z = \\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix}$ such that $Cz=0$.\n$$\n\\begin{pmatrix}\n1 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix} z_1 \\\\ z_2 \\\\ z_3 \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the system of equations:\n1. $z_1 - z_2 = 0 \\implies z_1 = z_2$\n2. $z_1 + z_2 + z_3 = 0$\n\nSubstituting the first equation into the second gives $z_1 + z_1 + z_3 = 0 \\implies 2z_1 + z_3 = 0 \\implies z_3 = -2z_1$.\nAny vector in the null space is of the form $\\begin{pmatrix} s \\\\ s \\\\ -2s \\end{pmatrix}$ for some scalar $s \\in \\mathbb{R}$. The null space is one-dimensional. We can choose $s=1$ to form a basis vector.\nA basis for the null space of $C$ is given by the columns of the matrix $Z$:\n$$\nZ = \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\n\n**1b. Find a particular solution $x_p$.**\nWe need to find any vector $x_p = \\begin{pmatrix} x_{p1} \\\\ x_{p2} \\\\ x_{p3} \\end{pmatrix}$ that satisfies $Cx_p=d$.\n$$\n\\begin{pmatrix}\n1 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix} x_{p1} \\\\ x_{p2} \\\\ x_{p3} \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThis gives the system:\n1. $x_{p1} - x_{p2} = 0 \\implies x_{p1} = x_{p2}$\n2. $x_{p1} + x_{p2} + x_{p3} = 1$\n\nThis is an underdetermined system. We can set one variable to a convenient value. Let $x_{p3}=0$.\nThen $x_{p1} + x_{p2} = 1$. Substituting $x_{p1} = x_{p2}$, we get $2x_{p1} = 1 \\implies x_{p1} = \\frac{1}{2}$.\nThus, $x_{p1} = \\frac{1}{2}$ and $x_{p2} = \\frac{1}{2}$. A particular solution is:\n$$\nx_p = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\n$$\n\n**2. Reduce to an Unconstrained WLS Problem**\nAny feasible solution $x$ can be parameterized as $x = x_p + Zy$ for some $y \\in \\mathbb{R}$ (since the null space is one-dimensional, $y$ is a scalar). We substitute this into the objective function:\n$$\nJ(y) = (A(x_p + Zy) - b)^T W (A(x_p + Zy)-b)\n$$\nRearranging terms, we get:\n$$\nJ(y) = (AZy - (b-Ax_p))^T W (AZy - (b-Ax_p))\n$$\nThis is an unconstrained Weighted Least Squares problem for the variable $y$. Let $A_y = AZ$ and $b_y = b-Ax_p$. The problem becomes minimizing $J(y) = \\|W^{1/2}(A_y y - b_y)\\|_2^2$.\n\n**3. Compute $A_y$ and $b_y$.**\nFirst, we compute $A_y = AZ$:\n$$\nA_y = \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2(1)+0(1)+1(-2) \\\\ 0(1)+1(1)+1(-2) \\\\ 1(1)+1(1)+0(-2) \\\\ 0(1)+2(1)+3(-2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 2 \\\\ -4 \\end{pmatrix}\n$$\nNext, we compute $b_y = b - Ax_p$. First, find $Ax_p$:\n$$\nAx_p = \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(1/2)+0(1/2)+1(0) \\\\ 0(1/2)+1(1/2)+1(0) \\\\ 1(1/2)+1(1/2)+0(0) \\\\ 0(1/2)+2(1/2)+3(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nNow, compute $b_y$:\n$$\nb_y = b - Ax_p = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\\\ 5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1/2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1/2 \\\\ 1 \\\\ 4 \\end{pmatrix}\n$$\n\n**4. Solve for the optimal parameter $y^{\\star}$.**\nThe unconstrained WLS problem for $y$ is solved via the normal equations:\n$$\n(A_y^T W A_y) y = A_y^T W b_y\n$$\nLet's compute the terms.\nThe scalar term $A_y^T W A_y$:\n$$\nA_y^T W A_y = \\begin{pmatrix} 0 & -1 & 2 & -4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 2 \\\\ -4 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 0 & -2 & 6 & -4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 2 \\\\ -4 \\end{pmatrix} = 0(0)+(-2)(-1)+6(2)+(-4)(-4) = 0+2+12+16 = 30\n$$\nThe scalar term $A_y^T W b_y$:\n$$\nA_y^T W b_y = \\begin{pmatrix} 0 & -2 & 6 & -4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1/2 \\\\ 1 \\\\ 4 \\end{pmatrix} = 0(2)+(-2)(1/2)+6(1)+(-4)(4) = 0-1+6-16 = -11\n$$\nThe normal equation is $30 y = -11$. The optimal parameter is:\n$$\ny^{\\star} = -\\frac{11}{30}\n$$\n\n**5. Compute the minimized weighted residual sum of squares $J^{\\star}$.**\nThe minimum value of the objective function is $J^{\\star} = J(y^{\\star}) = \\|W^{1/2}(A_y y^{\\star} - b_y)\\|_2^2$. Let the unconstrained residual be $r_y^{\\star} = A_y y^{\\star} - b_y$.\n$$\nr_y^{\\star} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 2 \\\\ -4 \\end{pmatrix} \\left(-\\frac{11}{30}\\right) - \\begin{pmatrix} 2 \\\\ 1/2 \\\\ 1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 11/30 \\\\ -22/30 \\\\ 44/30 \\end{pmatrix} - \\begin{pmatrix} 60/30 \\\\ 15/30 \\\\ 30/30 \\\\ 120/30 \\end{pmatrix} = \\frac{1}{30} \\begin{pmatrix} -60 \\\\ -4 \\\\ -52 \\\\ -76 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -4/30 \\\\ -52/30 \\\\ -76/30 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -2/15 \\\\ -26/15 \\\\ -38/15 \\end{pmatrix}\n$$\nNow we compute $J^{\\star} = (r_y^{\\star})^T W r_y^{\\star}$:\n$$\nJ^{\\star} = \\begin{pmatrix} -2 & -2/15 & -26/15 & -38/15 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 3 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -2/15 \\\\ -26/15 \\\\ -38/15 \\end{pmatrix}\n$$\n$$\nJ^{\\star} = 1(-2)^2 + 2\\left(-\\frac{2}{15}\\right)^2 + 3\\left(-\\frac{26}{15}\\right)^2 + 1\\left(-\\frac{38}{15}\\right)^2\n$$\n$$\nJ^{\\star} = 4 + 2\\left(\\frac{4}{225}\\right) + 3\\left(\\frac{676}{225}\\right) + \\frac{1444}{225}\n$$\n$$\nJ^{\\star} = 4 + \\frac{8}{225} + \\frac{2028}{225} + \\frac{1444}{225}\n$$\n$$\nJ^{\\star} = \\frac{4 \\cdot 225}{225} + \\frac{8 + 2028 + 1444}{225} = \\frac{900}{225} + \\frac{3480}{225}\n$$\n$$\nJ^{\\star} = \\frac{900 + 3480}{225} = \\frac{4380}{225}\n$$\nTo simplify the fraction, we can divide the numerator and denominator by their greatest common divisor. Both are divisible by $5$:\n$$\n\\frac{4380 \\div 5}{225 \\div 5} = \\frac{876}{45}\n$$\nBoth are divisible by $3$:\n$$\n\\frac{876 \\div 3}{45 \\div 3} = \\frac{292}{15}\n$$\nThe numerator $292 = 4 \\times 73$ and the denominator $15 = 3 \\times 5$ share no common factors. Thus, the fraction is in simplest form.\n\nThe minimized weighted residual sum of squares is $J^{\\star} = \\frac{292}{15}$.",
            "answer": "$$\n\\boxed{\\frac{292}{15}}\n$$"
        },
        {
            "introduction": "Going beyond simple equalities, many problems involve inequality constraints, such as non-negativity, which are fundamental in fields from statistics to finance. This practice delves into a weighted least squares problem over a simplex, requiring you to apply the Karush-Kuhn-Tucker (KKT) conditions to find the optimal solution . You will investigate how changing the weights in the objective function can alter which constraints become active, providing deep insight into the combinatorial nature of active-set methods and parametric sensitivity.",
            "id": "3601243",
            "problem": "Consider the weighted least squares problem with simplex constraints: minimize the quadratic objective subject to nonnegativity and unit-sum,\n$$\\min_{x \\in \\mathbb{R}^{3}} \\ (x - y)^{\\top} W (x - y) \\quad \\text{subject to} \\quad x \\ge 0, \\ \\mathbf{1}^{\\top} x = 1,$$\nwhere $y \\in \\mathbb{R}^{3}$ is the given target and $W \\in \\mathbb{R}^{3 \\times 3}$ is a diagonal positive definite matrix. In this instance, take\n$$y = \\begin{pmatrix} \\frac{9}{10} \\\\ \\frac{3}{10} \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad W = \\operatorname{diag}\\!\\left(2,\\ 1,\\ t\\right),$$\nwith $t > 0$ being a scalar parameter. The solution $x^{\\star}(t)$ is unique due to strict convexity, and depends on the Karush–Kuhn–Tucker (KKT) system, whose structure determines which components of $x^{\\star}(t)$ lie on the boundary $x_i^{\\star}(t) = 0$. \n\nStarting from first principles of constrained convex optimization, \n- derive the KKT system for this problem, \n- explain how the heteroskedastic diagonal weights in $W$ cause different coordinates to become active (hit the boundary) when $t$ varies,\n- and determine the exact critical value $t^{\\star} > 0$ at which the third coordinate $x_{3}^{\\star}(t)$ becomes active at the boundary, that is, $x_{3}^{\\star}(t^{\\star}) = 0$ while the other KKT conditions are satisfied.\n\nProvide your final answer as the exact value of $t^{\\star}$. No rounding is required.",
            "solution": "The problem as stated is a constrained quadratic optimization problem, specifically a weighted least squares minimization over the standard $2$-simplex. The problem is well-posed and scientifically grounded. The objective function is strictly convex, and the feasible set is a compact convex polytope. Therefore, a unique solution exists for any given parameter $t > 0$. The problem is valid.\n\nWe are asked to find the critical value of the parameter $t$, denoted $t^{\\star}$, at which the third component of the optimal solution, $x_3^{\\star}(t)$, becomes zero. This corresponds to the constraint $x_3 \\ge 0$ becoming active.\n\nThe optimization problem is:\n$$ \\min_{x \\in \\mathbb{R}^{3}} f(x) = (x - y)^{\\top} W (x - y) $$\nsubject to the constraints:\n$$ \\sum_{i=1}^{3} x_i = 1 $$\n$$ x_i \\ge 0 \\quad \\text{for } i \\in \\{1, 2, 3\\} $$\nwith the given data:\n$$ y = \\begin{pmatrix} \\frac{9}{10} \\\\ \\frac{3}{10} \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad W = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & t \\end{pmatrix}, \\quad t > 0 $$\n\nTo solve this constrained optimization problem, we use the Karush–Kuhn–Tucker (KKT) conditions. First, we formulate the Lagrangian function $L(x, \\lambda, \\mu)$, incorporating the objective function, the non-negativity inequality constraints $g_i(x) = -x_i \\le 0$, and the simplex equality constraint $h(x) = \\mathbf{1}^{\\top} x - 1 = 0$. Let $\\lambda = (\\lambda_1, \\lambda_2, \\lambda_3)^{\\top}$ be the Lagrange multipliers for the inequality constraints and $\\mu$ be the Lagrange multiplier for the equality constraint.\n\nThe Lagrangian is:\n$$ L(x, \\lambda, \\mu) = (x - y)^{\\top} W (x - y) - \\sum_{i=1}^{3} \\lambda_i x_i + \\mu \\left( \\sum_{i=1}^{3} x_i - 1 \\right) $$\n\nThe KKT conditions for an optimal solution $x^{\\star}$ are:\n$1$. **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero at $x=x^{\\star}$.\n$$ \\nabla_x L(x^{\\star}, \\lambda, \\mu) = 2W(x^{\\star} - y) - \\lambda + \\mu\\mathbf{1} = 0 $$\n$2$. **Primal Feasibility**: The solution $x^{\\star}$ must satisfy all constraints.\n$$ \\sum_{i=1}^{3} x_i^{\\star} = 1 \\quad \\text{and} \\quad x^{\\star} \\ge 0 $$\n$3$. **Dual Feasibility**: The multipliers for the inequality constraints must be non-negative.\n$$ \\lambda_i \\ge 0 \\quad \\text{for } i \\in \\{1, 2, 3\\} $$\n$4$. **Complementary Slackness**: The product of each inequality multiplier and its corresponding constraint slack must be zero.\n$$ \\lambda_i x_i^{\\star} = 0 \\quad \\text{for } i \\in \\{1, 2, 3\\} $$\n\nThe stationarity condition, written component-wise, is:\n$$ 2w_i(x_i^{\\star} - y_i) - \\lambda_i + \\mu = 0 \\quad \\text{for } i \\in \\{1, 2, 3\\} $$\n\nThe diagonal weights $w_i$ in the matrix $W$ represent the confidence in the corresponding components of the target vector $y$. A larger weight $w_i$ implies a higher penalty for the squared deviation $|x_i - y_i|^2$, thereby pulling the optimal solution component $x_i^{\\star}$ closer to $y_i$. This is characteristic of heteroskedastic models where observations have non-uniform variance. The parameter $t = w_3$ controls the penalty for the third component. As $t$ varies, the solution $x^{\\star}(t)$ traces a path on the simplex. A constraint $x_i \\ge 0$ becomes active (i.e., $x_i^{\\star} = 0$) when the competing influences of the other components and the simplex constraints dominate the pull towards a positive $y_i$.\n\nWe are seeking the critical value $t^{\\star}$ at which $x_3^{\\star}(t^{\\star}) = 0$. At such a transition point, a constraint is \"just\" becoming active. A generic assumption for this state is that the corresponding dual variable is zero. Therefore, we will search for a solution that satisfies the KKT conditions with $x_3^{\\star}=0$ and $\\lambda_3=0$. Furthermore, we assume that for this specific critical point, the other components are not on their boundaries, i.e., $x_1^{\\star} > 0$ and $x_2^{\\star} > 0$.\n\nUnder these assumptions, the complementary slackness conditions $\\lambda_1 x_1^{\\star} = 0$ and $\\lambda_2 x_2^{\\star} = 0$ imply that $\\lambda_1 = 0$ and $\\lambda_2 = 0$.\nSo, at the critical point $t^{\\star}$, we have the following set of conditions:\n- $x_1^{\\star} + x_2^{\\star} = 1$ (since $x_3^{\\star}=0$)\n- $x_1^{\\star}>0, x_2^{\\star}>0$\n- $\\lambda_1 = 0, \\lambda_2 = 0, \\lambda_3 = 0$\n\nWe substitute these into the stationarity equations:\nFor $i=1$: $w_1=2, y_1=\\frac{9}{10}$.\n$$ 2(2)(x_1^{\\star} - \\frac{9}{10}) - 0 + \\mu = 0 \\implies 4x_1^{\\star} - \\frac{36}{10} + \\mu = 0 \\implies x_1^{\\star} = \\frac{1}{4}\\left(\\frac{18}{5} - \\mu\\right) $$\nFor $i=2$: $w_2=1, y_2=\\frac{3}{10}$.\n$$ 2(1)(x_2^{\\star} - \\frac{3}{10}) - 0 + \\mu = 0 \\implies 2x_2^{\\star} - \\frac{6}{10} + \\mu = 0 \\implies x_2^{\\star} = \\frac{1}{2}\\left(\\frac{3}{5} - \\mu\\right) $$\nFor $i=3$: $w_3=t, y_3=\\frac{1}{2}$.\n$$ 2(t)(x_3^{\\star} - \\frac{1}{2}) - 0 + \\mu = 0 $$\nWith our condition $x_3^{\\star}=0$, this becomes:\n$$ 2t(0 - \\frac{1}{2}) + \\mu = 0 \\implies -t + \\mu = 0 \\implies \\mu = t $$\nThis gives us the value of the multiplier $\\mu$ at the critical point $t=t^{\\star}$. Now we substitute $\\mu = t^{\\star}$ into the expressions for $x_1^{\\star}$ and $x_2^{\\star}$:\n$$ x_1^{\\star} = \\frac{1}{4}\\left(\\frac{18}{5} - t^{\\star}\\right) $$\n$$ x_2^{\\star} = \\frac{1}{2}\\left(\\frac{3}{5} - t^{\\star}\\right) $$\nUsing the constraint $x_1^{\\star} + x_2^{\\star} = 1$:\n$$ \\frac{1}{4}\\left(\\frac{18}{5} - t^{\\star}\\right) + \\frac{1}{2}\\left(\\frac{3}{5} - t^{\\star}\\right) = 1 $$\nMultiplying by $4$ to clear the denominators:\n$$ \\left(\\frac{18}{5} - t^{\\star}\\right) + 2\\left(\\frac{3}{5} - t^{\\star}\\right) = 4 $$\n$$ \\frac{18}{5} - t^{\\star} + \\frac{6}{5} - 2t^{\\star} = 4 $$\n$$ \\frac{24}{5} - 3t^{\\star} = 4 $$\n$$ 3t^{\\star} = \\frac{24}{5} - 4 = \\frac{24}{5} - \\frac{20}{5} = \\frac{4}{5} $$\n$$ t^{\\star} = \\frac{4}{15} $$\nWe must verify that our assumptions $x_1^{\\star} > 0$ and $x_2^{\\star} > 0$ hold for this value of $t^{\\star}$.\nAt $t^{\\star} = \\frac{4}{15}$:\n$$ \\mu = t^{\\star} = \\frac{4}{15} $$\n$$ x_1^{\\star} = \\frac{1}{4}\\left(\\frac{18}{5} - \\frac{4}{15}\\right) = \\frac{1}{4}\\left(\\frac{54-4}{15}\\right) = \\frac{1}{4}\\left(\\frac{50}{15}\\right) = \\frac{1}{4}\\left(\\frac{10}{3}\\right) = \\frac{5}{6} $$\n$$ x_2^{\\star} = \\frac{1}{2}\\left(\\frac{3}{5} - \\frac{4}{15}\\right) = \\frac{1}{2}\\left(\\frac{9-4}{15}\\right) = \\frac{1}{2}\\left(\\frac{5}{15}\\right) = \\frac{1}{2}\\left(\\frac{1}{3}\\right) = \\frac{1}{6} $$\nSince $x_1^{\\star} = \\frac{5}{6} > 0$ and $x_2^{\\star} = \\frac{1}{6} > 0$, our assumptions are consistent. The solution is $x^{\\star}(t^{\\star}) = (\\frac{5}{6}, \\frac{1}{6}, 0)^{\\top}$, which satisfies all KKT conditions with $\\lambda = (0,0,0)^{\\top}$ and $\\mu = \\frac{4}{15}$. Thus, the critical value of $t$ is indeed $\\frac{4}{15}$. For $t > t^{\\star}$, the solution $x^{\\star}(t)$ will have $x_3^{\\star}(t) > 0$. For $t < t^{\\star}$, the solution will be pinned at the boundary with $x_3^{\\star}(t) = 0$ and a corresponding multiplier $\\lambda_3 > 0$.",
            "answer": "$$\\boxed{\\frac{4}{15}}$$"
        }
    ]
}