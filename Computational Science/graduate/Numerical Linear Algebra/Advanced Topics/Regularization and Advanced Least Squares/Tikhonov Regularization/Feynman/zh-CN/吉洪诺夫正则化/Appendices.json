{
    "hands_on_practices": [
        {
            "introduction": "首个实践将对吉洪诺夫正则化与另一种基石方法——截断奇异值分解（TSVD）进行动手比较。两种技术都通过滤波问题矩阵的奇异值谱来处理不适定问题，但其哲学思想不同：吉洪诺夫正则化使用平滑衰减，而TSVD则采用“一刀切”的硬截断。通过这个计算练习 ，您将发现这些不同方法的表现，并学会识别在何种场景下（例如存在“谱隙”时）一种方法可能优于另一种。",
            "id": "3599470",
            "problem": "考虑在数值线性代数的意义上求解一个线性逆问题：给定一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$、一个未知向量 $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$、确定性数据噪声 $\\eta \\in \\mathbb{R}^{n}$ 以及观测数据 $b = A x_{\\mathrm{true}} + \\eta$，比较两种正则化策略：Tikhonov 正则化和截断奇异值分解。目标是计算两种策略在一组测试用例上的相对解误差，并突显一个由于谱隙的存在，截断奇异值分解可以优于 Tikhonov 正则化的设计。\n\n基本定义和要求：\n- 对于正则化参数为 $\\lambda > 0$ 的 Tikhonov 正则化，其解是严格凸目标函数 $\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ 的最小化子，也就是正规方程 $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$ 的唯一解。\n- 对于截断奇异值分解 (TSVD)，定义奇异值分解为 $A = U \\Sigma V^{\\top}$，其中奇异值为 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$。对于截断指数 $k \\in \\{0,1,\\dots,n\\}$，TSVD 解为 $x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}$。在本问题中，截断指数 $k$ 由规则 $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$ 确定，并约定如果该集合为空，则 $k = 0$。\n- 候选解 $\\widehat{x}$ 相对于 $x_{\\mathrm{true}}$ 的相对误差为 $\\|\\widehat{x} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n\n为可测试性而设的实现约束和特化：\n- 在所有测试用例中，取 $n = 10$ 并选择 $A$ 为对角矩阵，其对角元为降序正数，因此 $U = I$，$V = I$，奇异值即为 $A$ 的对角元。这使得 $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ 且 $A^{\\top} A = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{n}^{2})$。\n- 对于每个测试用例，通过 $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$（其中 $i = 1,2,\\dots,n$）来确定性地定义噪声向量。\n- 对于 Tikhonov 正则化，在这种对角设定下，从正规方程导出的显式分量表达式为 $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$（其中 $i = 1,2,\\dots,n$）。\n- 对于 TSVD，给定由上述选择规则确定的 $k$，在这种对角设定下的显式分量表达式为 $x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i},  i \\le k, \\\\ 0,  i > k. \\end{cases}$\n\n测试套件：\n- 测试用例 #1（谱隙；旨在有利于截断奇异值分解）：\n  - $n = 10$。\n  - 奇异值 $\\sigma = [1.0, 0.6, 0.36, 0.216, 0.1296, 10^{-3}, 5 \\cdot 10^{-4}, 2 \\cdot 10^{-4}, 10^{-4}, 5 \\cdot 10^{-5}]$。\n  - 真实解 $x_{\\mathrm{true}} = [1, -\\tfrac{1}{2}, \\tfrac{1}{4}, -\\tfrac{1}{8}, \\tfrac{1}{16}, 0, 0, 0, 0, 0]$。\n  - 正则化参数 $\\lambda = 10^{-5}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #2（平滑谱；无明显谱隙）：\n  - $n = 10$。\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{-4}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #3（边界条件：正则化参数非常小）：\n  - $n = 10$。\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{-12}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #4（边界条件：正则化参数非常大）：\n  - $n = 10$。\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{1}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n\n每个测试用例需要实现的任务：\n- 构造 $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$、$x_{\\mathrm{true}}$、$\\eta$（其中 $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$）以及 $b = A x_{\\mathrm{true}} + \\eta$。\n- 使用 $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$ 计算 Tikhonov 解 $x_{\\mathrm{tik}}$。\n- 计算截断指数 $k = \\max\\{ i : \\sigma_{i}^{2} \\ge \\lambda \\}$，并约定若集合为空则 $k = 0$。\n- 使用 $x_{\\mathrm{tsvd},i}^{(k)} = b_{i}/\\sigma_{i}$（当 $i \\le k$ 时）和 $x_{\\mathrm{tsvd},i}^{(k)} = 0$（当 $i > k$ 时）计算截断奇异值分解解 $x_{\\mathrm{tsvd}}^{(k)}$。\n- 计算相对 $2$-范数误差 $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ 和 $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n- 同时计算布尔值 $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$，以指示在该测试中截断奇异值分解是否严格优于 Tikhonov 正则化。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个以方括号括起来的逗号分隔列表形式的结果。每个测试用例贡献一个形式为 $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$ 的列表。因此，最终输出应为一个包含四个内部列表的列表，每个内部列表对应一个测试用例，按测试用例 #1 到 #4 的顺序排列，例如 $[[k_{1}, e_{\\mathrm{tsvd},1}, e_{\\mathrm{tik},1}, b_{\\mathrm{adv},1}], [k_{2}, e_{\\mathrm{tsvd},2}, e_{\\mathrm{tik},2}, b_{\\mathrm{adv},2}], [k_{3}, e_{\\mathrm{tsvd},3}, e_{\\mathrm{tik},3}, b_{\\mathrm{adv},3}], [k_{4}, e_{\\mathrm{tsvd},4}, e_{\\mathrm{tik},4}, b_{\\mathrm{adv},4}]]$。\n\n注：\n- 没有物理单位；所有计算均在无量纲浮点运算中进行。\n- 不使用角度。\n- 将所有布尔值表示为语言原生的布尔值，所有误差表示为浮点数。",
            "solution": "所提出的问题要求比较两种用于求解不适定线性逆问题的标准正则化技术：Tikhonov 正则化和截断奇异值分解 (TSVD)。验证证实了该问题在科学上是合理的、适定的，并为数值实验提供了一套清晰、自包含的指令和数据。所有定义和公式均与数值线性代数领域的既有文献一致。因此，我们可以着手求解。\n\n问题的核心在于求解线性系统 $A x = b$，其中矩阵 $A$ 是病态的，数据向量 $b$ 被噪声 $\\eta$ 污染。模型由 $b = A x_{\\mathrm{true}} + \\eta$ 给出，其中 $x_{\\mathrm{true}}$ 是我们试图逼近的真实解。通过 $x = A^{-1} b$ 求解 $x$ 的简单尝试将导致 $x = x_{\\mathrm{true}} + A^{-1} \\eta$。由于 $A$ 是病态的，其逆矩阵 $A^{-1}$ 具有非常大的范数，导致噪声项 $\\eta$ 被极度放大。正则化方法旨在通过在解中引入受控的偏差，来换取因噪声引起的方差的显著减小，从而解决这个问题。\n\n该问题通过考虑一个对角矩阵 $A = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$ 来简化分析，其中对角元 $\\sigma_i > 0$ 是 $A$ 的奇异值。在一般情况下，任何矩阵 $A$ 都有一个奇异值分解 (SVD) $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$。选择对角矩阵 $A$ 等效于在一个 SVD 是平凡的（$U=V=I$）基中工作，这使我们能够专注于每种正则化方法如何处理奇异值本身。\n\n**Tikhonov 正则化**\n\nTikhonov 正则化将问题重构为一个优化问题，寻求一个解 $x$ 来最小化数据保真项和解范数惩罚项的组合：\n$$ x_{\\mathrm{tik}} = \\arg\\min_{x} \\left( \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2} \\right) $$\n参数 $\\lambda > 0$ 控制着权衡。唯一的最小化子 $x_{\\mathrm{tik}}$ 通过求解相关的正规方程得到：$(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$。对于我们的对角矩阵 $A=\\mathrm{diag}(\\sigma_i)$，这个系统解耦为 $n$ 个标量方程：\n$$ (\\sigma_{i}^{2} + \\lambda) x_{\\mathrm{tik},i} = \\sigma_{i} b_{i} \\quad \\text{for } i \\in \\{1, \\dots, n\\} $$\n这给出了 Tikhonov 解的显式分量公式：\n$$ x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i} $$\n项 $f_{i}^{\\mathrm{tik}} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$ 可被视为一个“滤波因子”。它平滑地衰减与小奇异值相关的分量。如果 $\\sigma_i^2 \\gg \\lambda$，则 $f_{i}^{\\mathrm{tik}} \\approx 1$，分量基本保持不变。如果 $\\sigma_i^2 \\ll \\lambda$，则 $f_{i}^{\\mathrm{tik}} \\approx 0$，分量被抑制。\n\n**截断奇异值分解 (TSVD)**\n\nTSVD 采用更直接的方法，仅使用“重要的”奇异分量来构造解。一般的 TSVD 解由一个截断和给出：\n$$ x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\n其中 $k$ 是截断指数，决定了包含多少个分量。在我们的对角情况下（$U=I, V=I$），这简化为：\n$$ x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}  \\text{if } i \\le k \\\\ 0  \\text{if } i > k \\end{cases} $$\n这对应于构成一个阶跃函数的滤波因子 $f_{i}^{\\mathrm{tsvd}}$：当 $i \\le k$ 时 $f_{i}^{\\mathrm{tsvd}} = 1$，当 $i > k$ 时 $f_{i}^{\\mathrm{tsvd}} = 0$。问题通过规则 $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$（如果集合为空则 $k=0$）将 $k$ 的选择与 Tikhonov 参数 $\\lambda$ 联系起来。该规则基本上保留了所有 Tikhonov 滤波因子至少为 $1/2$ 的分量。\n\n**比较与谱隙的作用**\n\n根本区别在于它们的滤波函数：Tikhonov 的是平滑的，而 TSVD 的是陡峭的。测试用例 #1 专门设计用于突显 TSVD 的陡峭截断具有优势的场景。它具有一个“谱隙”，即奇异值在 $\\sigma_5$ 和 $\\sigma_6$ 之间出现大幅下降。正则化参数 $\\lambda=10^{-5}$ 被选择落在这个谱隙内（即 $\\sigma_5^2 \\gg \\lambda \\gg \\sigma_6^2$）。此外，真实解 $x_{\\mathrm{true}}$ 的信息内容仅限于前 $5$ 个分量。\n\n在这些条件下，TSVD 的截断规则得出 $k=5$。因此，TSVD 保留了前 $5$ 个分量（信号所在处），并完全丢弃了其余分量（由于当 $i>5$ 时 $x_{\\mathrm{true},i}=0$，这些分量仅包含噪声）。对于这种特定的问题结构，这起到了完美滤波器的作用。相比之下，Tikhonov 正则化将其平滑滤波器应用于所有分量。虽然它严重抑制了 $i > 5$ 的分量，但仍允许少量经过滤波的噪声通过。更重要的是，它也轻微地抑制了 $i \\le 5$ 的分量，引入了 TSVD 在这些分量上没有的正则化误差。这导致 TSVD 的性能优于 Tikhonov。\n\n对于其他谱衰减更平滑的测试用例，TSVD 的陡峭截断可能是有害的。如果 $x_{\\mathrm{true}}$ 在被 TSVD 截断的分量（因为它们的 $\\sigma_i$ 很小）中包含显著能量，它将产生较大的正则化误差。Tikhonov 对这些分量的温和抑制可以得到更好的整体近似效果。\n\n**计算步骤**\n\n对于四个测试用例中的每一个，都执行以下步骤：\n1.  初始化参数：$n=10$、奇异值 $\\sigma$、真实解 $x_{\\mathrm{true}}$、正则化参数 $\\lambda$ 和噪声水平。\n2.  构造噪声向量 $\\eta$，其中对于从 0 开始的索引 $j \\in \\{0, \\dots, 9\\}$，$\\eta_{j} = \\mathrm{noise\\_level} \\cdot (-1)^{j+1}$。\n3.  计算数据向量 $b = \\sigma \\odot x_{\\mathrm{true}} + \\eta$，其中 $\\odot$ 表示逐元素乘法。\n4.  使用其分量公式计算 Tikhonov 解向量 $x_{\\mathrm{tik}}$。\n5.  根据提供的规则确定 TSVD 截断指数 $k$。\n6.  计算 TSVD 解向量 $x_{\\mathrm{tsvd}}^{(k)}$。\n7.  计算两个解的相对 $2$-范数误差：$e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ 和 $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n8.  评估布尔条件 $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$。\n然后报告为每个案例收集的结果 $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(sigma_vals, xtrue_vals, lam, noise_level):\n    \"\"\"\n    Runs a single test case for comparing Tikhonov and TSVD regularization.\n    \"\"\"\n    n = 10\n    sigma = np.array(sigma_vals, dtype=float)\n    xtrue = np.array(xtrue_vals, dtype=float)\n\n    # Construct the noise vector eta and observed data b\n    # The problem uses 1-based indexing i=1,...,n. Python uses 0-based j=0,...,n-1.\n    # eta_i = noise_level * (-1)^i translates to eta[j] = noise_level * (-1)**(j+1)\n    indices_1_based = np.arange(1, n + 1)\n    eta = noise_level * ((-1) ** indices_1_based)\n    b = sigma * xtrue + eta\n\n    # Compute the Tikhonov solution\n    xtik = (sigma / (sigma**2 + lam)) * b\n\n    # Determine the TSVD truncation index k\n    # k = max{ i in {1..n} : sigma_i^2 >= lam }\n    # np.where returns 0-based indices. k needs to be a 1-based count.\n    valid_indices = np.where(sigma**2 >= lam)[0]\n    if len(valid_indices) == 0:\n        k = 0\n    else:\n        k = int(np.max(valid_indices) + 1)\n\n    # Compute the TSVD solution\n    xtsvd = np.zeros(n)\n    if k > 0:\n        # Slicing with :k works correctly for 0-based index up to k-1.\n        xtsvd[:k] = b[:k] / sigma[:k]\n\n    # Compute the relative 2-norm errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    # This problem guarantees norm_xtrue > 0, so no division-by-zero check is needed.\n    e_tsvd = np.linalg.norm(xtsvd - xtrue) / norm_xtrue\n    e_tik = np.linalg.norm(xtik - xtrue) / norm_xtrue\n\n    # Determine if TSVD has a strictly smaller error\n    b_adv = bool(e_tsvd  e_tik)\n    \n    return [k, e_tsvd, e_tik, b_adv]\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    # Test case #1: Spectral gap\n    case1 = {\n        \"sigma_vals\": [1.0, 0.6, 0.36, 0.216, 0.1296, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5],\n        \"xtrue_vals\": [1.0, -0.5, 0.25, -0.125, 0.0625, 0, 0, 0, 0, 0],\n        \"lam\": 1e-5,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #2: Smooth spectrum\n    n = 10\n    j_indices = np.arange(n)\n    sigma_smooth = 10**(-j_indices / 3.0)\n    xtrue_smooth = 0.8**j_indices\n    case2 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-4,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #3: Small regularization parameter\n    case3 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-12,\n        \"noise_level\": 1e-6\n    }\n\n    # Test case #4: Large regularization parameter\n    case4 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e1,\n        \"noise_level\": 1e-6\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            case[\"sigma_vals\"],\n            case[\"xtrue_vals\"],\n            case[\"lam\"],\n            case[\"noise_level\"]\n        )\n        results.append(result)\n\n    # Print in the specified format: [[k1, e_tsvd1, e_tik1, b_adv1], [k2, ...]]\n    # Python's default string representation for a list of lists matches the required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "吉洪诺夫正则化的有效性在很大程度上取决于正则化算子 $L$ 的选择，因为它应反映我们对解的结构的先验知识。本练习将探讨“模型偏差”这一概念，即当所选正则化项与信号的真实属性不匹配时产生的系统性误差。在这个问题中 ，您将研究对一个在小波基下具有内在稀疏性的信号使用标准正则化项（$L=I$）所带来的后果，通过量化由此产生的误差，加深您对如何选择合适正则化项的理解。",
            "id": "3599516",
            "problem": "考虑数值线性代数中的 Tikhonov 正则化问题。给定一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$、一个数据向量 $y \\in \\mathbb{R}^{n}$ 和一个正则化算子 $L \\in \\mathbb{R}^{n \\times n}$，带有参数 $\\alpha > 0$ 的 Tikhonov 估计量 $\\hat{x}_{\\alpha}$ 定义为目标函数的唯一最小化子\n$$\nJ_{\\alpha}(x) = \\|A x - y\\|_{2}^{2} + \\alpha^{2} \\|L x\\|_{2}^{2}.\n$$\n您将研究未知向量 $x$ 的真实正则性与所选正则化算子 $L$ 之间的不匹配如何导致模型偏差。构建一个合成案例，其中基准真相 $x_{\\mathrm{true}}$ 在小波基中是稀疏的，但应用了 $L = I$（单位算子）的 Tikhonov 正则化，并量化由此引起的失配。\n\n您必须精确实现以下规范并计算所需的输出。\n\n1. 信号维度与小波基。\n   - 令 $n = 64$ 并考虑 $\\mathbb{R}^{n}$ 上的标准正交 Haar 小波基。\n   - 对于前向 Haar 变换 $W$，使用以下规范系数排序：小波系数向量排序为 $[a_{L}, d_{L}, d_{L-1}, \\dots, d_{1}]$，其中 $L = \\log_{2}(n)$，$a_{L} \\in \\mathbb{R}$ 是最终的粗略近似系数，$d_{j} \\in \\mathbb{R}^{2^{L-j}}$ 是尺度 $j$ 处的细节块，对于 $j = L, L-1, \\dots, 1$。\n   - 逆 Haar 变换 $W^{-1}$ 从此排序重构信号。\n\n2. 基准真相构建（在小波基中稀疏）。\n   - 令 $w_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ 全为零，除了位于细节块中的五个特定位置（使用上述排序，其中 $L = 6$，因为 $n = 64$）：\n     - 在 $d_{6}$ 的位置 $0$ 处，设置值为 $+1.0$。\n     - 在 $d_{5}$ 的位置 $1$ 处，设置值为 $-0.8$。\n     - 在 $d_{3}$ 的位置 $3$ 处，设置值为 $+0.5$。\n     - 在 $d_{2}$ 的位置 $5$ 处，设置值为 $-1.1$。\n     - 在 $d_{1}$ 的位置 $17$ 处，设置值为 $+0.9$。\n   - 所有其他系数，包括 $a_{6}$，均为零。\n   - 定义 $x_{\\mathrm{true}} = W^{-1} w_{\\mathrm{true}}$。\n\n3. 前向算子 $A$ 和数据 $y$。\n   - 考虑两种前向算子：\n     - 情况 $\\mathrm{blur}$：$A$ 是一个循环卷积矩阵，由一个离散高斯核 $g \\in \\mathbb{R}^{n}$ 生成，该核的奇数截断长度为 $9$，其元素为\n       $$\n       g_{k} = \\frac{\\exp\\!\\left(-\\frac{(k-0)^{2}}{2 \\sigma^{2}}\\right)}{\\sum_{j=-4}^{4} \\exp\\!\\left(-\\frac{j^{2}}{2 \\sigma^{2}}\\right)}, \\quad k \\in \\{-4,-3,\\dots,4\\},\n       $$\n       通过将这 $9$ 个权重居中放置，其余位置为零，并以环绕方式定义循环结构，从而周期性地嵌入到长度为 $n$ 的向量中。使用 $\\sigma = 2.0$。\n     - 情况 $\\mathrm{id}$：$A = I$（单位矩阵）。\n   - 在所有情况下，生成无噪声数据 $y = A x_{\\mathrm{true}}$。\n\n4. 正则化与估计量。\n   - 固定 $L = I$，并对每个测试用例，计算给定 $A$ 和 $y$ 下最小化 $J_{\\alpha}(x)$ 的 Tikhonov 估计量 $\\hat{x}_{\\alpha}$。\n   - 该设置为无噪声环境；重点在于由小波稀疏的真实值与各向同性二次惩罚之间的不匹配所引起的确定性模型偏差。\n\n5. 失配量化指标。\n   对于每个测试用例，计算以下两个标量值：\n   - 信号域中的归一化平方模型偏差：\n     $$\n     b = \\frac{\\|\\hat{x}_{\\alpha} - x_{\\mathrm{true}}\\|_{2}^{2}}{\\|x_{\\mathrm{true}}\\|_{2}^{2}}.\n     $$\n   - 小波泄漏率，用于量化放置在 $w_{\\mathrm{true}}$ 中为零的系数上的能量：\n     $$\n     \\ell = \\frac{\\| (W \\hat{x}_{\\alpha})_{S^{c}} \\|_{2}^{2}}{\\| w_{\\mathrm{true}} \\|_{2}^{2}},\n     $$\n     其中 $S$ 是 $w_{\\mathrm{true}}$ 非零项的支撑集（索引集），$S^{c}$ 是其补集。分子使用重构信号 $\\hat{x}_{\\alpha}$ 的前向 Haar 变换。\n\n6. 测试套件。\n   使用以下四个测试用例，每个用例由 $(A\\text{-type}, \\alpha)$ 指定：\n   - 测试 $1$：$(\\mathrm{blur}, 10^{-1})$。\n   - 测试 $2$：$(\\mathrm{blur}, 1)$。\n   - 测试 $3$：$(\\mathrm{blur}, 10^{-3})$。\n   - 测试 $4$：$(\\mathrm{id}, 3 \\times 10^{-1})$。\n\n7. 程序输出格式。\n   - 您的程序必须生成单行输出，其中包含聚合为列表的列表的结果。对于每个测试用例（按 $1, 2, 3, 4$ 的顺序），以浮点数形式输出数对 $[b, \\ell]$。\n   - 因此，最终输出必须是以下格式的单行文本\n     $$\n     [[b_{1}, \\ell_{1}], [b_{2}, \\ell_{2}], [b_{3}, \\ell_{3}], [b_{4}, \\ell_{4}]].\n     $$\n\n约束与指导：\n- 使用与指定排序一致的标准正交 Haar 变换的正确实现。\n- 在这两种情况下，都将 $A$ 显式地构建为稠密矩阵。\n- 通过最小化 $J_{\\alpha}(x)$ 来求解 $\\hat{x}_{\\alpha}$，不要使用任何迭代优化方法；期望采用直接线性代数方法。\n- 不涉及任何物理单位。\n- 禁止所有随机化；结果必须是确定性的。",
            "solution": "经评估，用户提供的问题是有效的。它在科学上基于数值线性代数和信号处理，定义了所有必要参数，问题设定良好，并且表述客观。它要求对 Tikhonov 正则化中的模型偏差进行定量分析，这是一个标准课题。因此，我们提供了一个合理的解决方案。\n\n问题的核心是为给定的线性系统计算 Tikhonov 正则化估计 $\\hat{x}_{\\alpha}$，然后相对于已知的基准真相 $x_{\\mathrm{true}}$ 量化其误差或模型偏差。产生偏差的原因是正则化项 $\\alpha^2 \\|L x\\|_2^2$ 施加了一个惩罚，该惩罚使解偏向于那些 $\\|L x\\|_2^2$ 较小的向量 $x$。在此问题中，正则化算子是 $L=I$（单位算子），它偏好具有小欧几里得范数的解。然而，真实信号 $x_{\\mathrm{true}}$ 被构造成在小波基中是稀疏的，其范数本身并不小。真实信号的性质与正则化器中隐含的假设之间的这种不匹配，正是我们需要研究的模型偏差的来源。\n\n对于每个测试用例，解决方案按以下步骤进行：\n1.  构建必要的数学对象：标准正交 Haar 小波变换矩阵 $W$、基准真相信号 $x_{\\mathrm{true}}$ 以及前向算子 $A$。\n2.  通过求解相应的正规方程组来计算 Tikhonov 估计量 $\\hat{x}_{\\alpha}$。\n3.  计算两个指定的误差指标：归一化平方模型偏差 $b$ 和小波泄漏率 $\\ell$。\n\n### 步骤 1：构建数学对象\n\n**1.1. 标准正交 Haar 小波变换矩阵**\n问题指定了用于维度 $n=64$ 的信号的标准正交 Haar 小波变换。维度 $n=2^L$（其中 $L=6$）是 2 的幂，符合要求。小波系数排序为 $[a_L, d_L, d_{L-1}, \\dots, d_1]$，其中 $a_L$ 是最终近似系数（长度为 1），$d_j$ 是尺度 $j$ 处的细节系数（长度为 $2^{L-j}$）。这是一种可以通过递归算法生成的“非标准”或解包排序。前向变换 $w = Wx$ 将信号 $x \\in \\mathbb{R}^n$ 映射到其小波系数 $w \\in \\mathbb{R}^n$。逆变换为 $x = W^{-1}w$。由于该基是标准正交的，变换矩阵 $W$ 是正交矩阵，意味着 $W^{-1} = W^T$。\n\n矩阵 $W$ 可以通过识别其行是 Haar 基向量来构建。等价地，其转置矩阵 $W^T$ 的列是基向量。第 $i$ 个基向量是对应于小波域中位置 $i$ 处单个非零系数的信号。因此，我们可以通过对每个规范基向量 $e_i \\in \\mathbb{R}^n$ 应用逆 Haar 变换来构建 $W^T$。然后通过转置结果得到矩阵 $W$。\n\n**1.2. 基准真相信号 $x_{\\mathrm{true}}$**\n基准真相信号 $x_{\\mathrm{true}}$ 由其小波系数 $w_{\\mathrm{true}}$ 定义。向量 $w_{\\mathrm{true}} \\in \\mathbb{R}^{64}$ 是稀疏的，在细节块内的指定位置有五个非零项：\n- 系数向量的结构为 $[a_6 (1), d_6 (1), d_5 (2), d_4 (4), d_3 (8), d_2 (16), d_1 (32)]$，其中括号中的数字是块的长度。\n- 这些块的索引为：$a_6: [0]$，$d_6: [1]$，$d_5: [2,3]$，$d_4: [4-7]$，$d_3: [8-15]$，$d_2: [16-31]$，$d_1: [32-63]$。\n- 非零值设置如下：\n  - $w_{\\mathrm{true}}[1] = +1.0$ (在 $d_6$ 中)\n  - $w_{\\mathrm{true}}[3] = -0.8$ (在 $d_5$ 中)\n  - $w_{\\mathrm{true}}[11] = +0.5$ (在 $d_3$ 中)\n  - $w_{\\mathrm{true}}[21] = -1.1$ (在 $d_2$ 中)\n  - $w_{\\mathrm{true}}[49] = +0.9$ (在 $d_1$ 中)\n然后使用逆 Haar 变换合成信号：$x_{\\mathrm{true}} = W^T w_{\\mathrm{true}}$。\n\n**1.3. 前向算子 $A$ 和数据 $y$**\n考虑两种类型的前向算子：\n- **情况 `id`**：$A=I_{64}$，即 $64 \\times 64$ 的单位矩阵。\n- **情况 `blur`**：$A$ 是一个循环卷积矩阵。生成核是一个长度为 $9$、标准差为 $\\sigma=2.0$ 的离散高斯核。对于 $k \\in \\{-4, \\dots, 4\\}$ 的核权重 $g_k$ 被归一化以使其总和为 $1$。该核被嵌入到一个周期向量 $c \\in \\mathbb{R}^{64}$ 中，该向量成为循环矩阵 $A$ 的第一列。具体来说，$c_0=g_0$，$c_k = g_k$ 对于 $k \\in \\{1,2,3,4\\}$，以及 $c_{n+k} = g_k$ 对于 $k \\in \\{-1,-2,-3,-4\\}$。\n无噪声数据向量生成为 $y = A x_{\\mathrm{true}}$。\n\n### 步骤 2：Tikhonov 估计量\nTikhonov 估计量 $\\hat{x}_{\\alpha}$ 是最小化目标函数 $J_{\\alpha}(x) = \\|A x - y\\|_{2}^{2} + \\alpha^{2} \\|I x\\|_{2}^{2}$ 的唯一向量 $x$。通过将梯度 $\\nabla_x J_{\\alpha}(x)$ 设置为零来找到最小化子，这会产生正规方程组：\n$$\n(A^T A + \\alpha^2 I) x = A^T y.\n$$\n这是一个线性方程组，可以求解 $\\hat{x}_{\\alpha}$：\n$$\n\\hat{x}_{\\alpha} = (A^T A + \\alpha^2 I)^{-1} A^T y.\n$$\n该系统使用直接数值线性代数求解器进行求解，以确保稳健性并满足问题规范。\n\n### 步骤 3：失配量化\n模型偏差使用两个指标进行量化：\n\n**3.1. 归一化平方模型偏差 ($b$)**\n该指标测量信号域中的平方误差，并按真实信号的能量进行归一化：\n$$\nb = \\frac{\\|\\hat{x}_{\\alpha} - x_{\\mathrm{true}}\\|_{2}^{2}}{\\|x_{\\mathrm{true}}\\|_{2}^{2}}.\n$$\n\n**3.2. 小波泄漏率 ($\\ell$)**\n该指标量化了正则化解有多少能量“泄漏”到本应为零的小波系数上。令 $S$ 为 $w_{\\mathrm{true}}$ 中五个非零项的索引集，$S^c$ 为其补集。估计的小波系数为 $\\hat{w}_{\\alpha} = W \\hat{x}_{\\alpha}$。泄漏率为：\n$$\n\\ell = \\frac{\\| (\\hat{w}_{\\alpha})_{S^{c}} \\|_{2}^{2}}{\\| w_{\\mathrm{true}} \\|_{2}^{2}}.\n$$\n分母 $\\|w_{\\mathrm{true}}\\|_{2}^{2}$ 等于 $\\|x_{\\mathrm{true}}\\|_{2}^{2}$，因为 $W$ 是一个正交矩阵。分子是在 $w_{\\mathrm{true}}$ 为零的索引处 $\\hat{w}_{\\alpha}$ 分量的平方范数。\n\n对由数对 $(A\\text{-type}, \\alpha)$ 指定的四个测试用例中的每一个都执行这些计算。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import circulant\n\ndef solve():\n    \"\"\"\n    Main function to solve the Tikhonov regularization problem and compute misfit metrics.\n    \"\"\"\n\n    def _inverse_haar_recursive(w: np.ndarray) - np.ndarray:\n        \"\"\"\n        Recursively computes the inverse Haar transform for a 1D vector.\n        This implementation assumes the standard recursive structure where the input `w`\n        consists of an approximation part and a detail part of equal length, i.e.,\n        `w = [approx_coeffs, detail_coeffs]`. This naturally leads to the \n        '[a_L, d_L, d_{L-1}, ...]' coefficient ordering specified in the problem\n        when the synthesis process (inverse transform) starts from the coarsest level.\n        \"\"\"\n        n = len(w)\n        if n == 1:\n            return w\n        \n        m = n // 2\n        w_approx_unpacked = w[:m]\n        w_detail = w[m:]\n        \n        x_approx_reconstructed = _inverse_haar_recursive(w_approx_unpacked)\n        \n        x = np.zeros(n)\n        # Reconstruct from averages and differences\n        x[0::2] = (x_approx_reconstructed + w_detail) / np.sqrt(2.0)\n        x[1::2] = (x_approx_reconstructed - w_detail) / np.sqrt(2.0)\n        \n        return x\n\n    def get_haar_matrix(n: int) - np.ndarray:\n        \"\"\"\n        Constructs the n x n orthonormal Haar wavelet transform matrix W.\n        The transform is defined as w = W @ x.\n        The rows of W are the wavelet basis vectors.\n        \"\"\"\n        if np.log2(n) % 1 != 0:\n            raise ValueError(\"n must be a power of 2.\")\n            \n        # W_T has the basis vectors as columns. Each basis vector is the synthesis\n        # of a canonical vector in the wavelet domain.\n        W_T = np.zeros((n, n))\n        I = np.identity(n)\n        for i in range(n):\n            # To handle the specified coefficient ordering, we must map the\n            # canonical vector e_i to the structure expected by the recursive function.\n            # The structure is [aL, [dL, [dL-1, ...]]].\n            # Example n=4: [a2, d2, d1_1, d1_2].\n            # rec_fn([a2,d2]) -> [a1_1, a1_2].\n            # rec_fn([a1_1, a1_2, d1_1, d1_2]) -> signal.\n            # The recursive function implicitly handles this structure.\n            W_T[:, i] = _inverse_haar_recursive(I[:, i])\n        \n        # W is the transpose of W_T, so its rows are the basis vectors.\n        W = W_T.T\n        return W\n\n    def construct_ground_truth(n: int, W: np.ndarray):\n        \"\"\"Constructs the ground-truth wavelet coefficients and signal.\"\"\"\n        w_true = np.zeros(n)\n        \n        # As per problem spec, n=64, L=6.\n        # Block structure: a6(1), d6(1), d5(2), d4(4), d3(8), d2(16), d1(32).\n        # Offsets: a6->0, d6->1, d5->2, d4->4, d3->8, d2->16, d1->32.\n        \n        # In d6 at position 0 (overall index 1)\n        w_true[1] = 1.0\n        # In d5 at position 1 (overall index 2 + 1 = 3)\n        w_true[3] = -0.8\n        # In d3 at position 3 (overall index 8 + 3 = 11)\n        w_true[11] = 0.5\n        # In d2 at position 5 (overall index 16 + 5 = 21)\n        w_true[21] = -1.1\n        # In d1 at position 17 (overall index 32 + 17 = 49)\n        w_true[49] = 0.9\n        \n        # Synthesize the signal x_true = W^{-1} w_true = W^T w_true\n        x_true = W.T @ w_true\n        \n        return w_true, x_true\n\n    def construct_forward_operator(A_type: str, n: int):\n        \"\"\"Constructs the forward operator A.\"\"\"\n        if A_type == 'id':\n            return np.identity(n)\n        elif A_type == 'blur':\n            sigma = 2.0\n            kernel_len = 9\n            k = np.arange(-(kernel_len//2), (kernel_len//2) + 1)\n            \n            g_unnorm = np.exp(-k**2 / (2.0 * sigma**2))\n            g = g_unnorm / np.sum(g_unnorm)\n            \n            # Embed the kernel into the first column of the circulant matrix\n            c = np.zeros(n)\n            center_idx = kernel_len // 2\n            \n            # Positive shifts\n            c[0:center_idx + 1] = g[center_idx:]\n            # Negative shifts (wrap around)\n            c[n-center_idx:] = g[:center_idx]\n            \n            return circulant(c)\n        else:\n            raise ValueError(f\"Unknown A_type: {A_type}\")\n\n    # ===== Main execution logic =====\n    \n    n = 64\n    \n    # 1. Construct universal components\n    W = get_haar_matrix(n)\n    w_true, x_true = construct_ground_truth(n, W)\n    \n    x_true_norm_sq = np.linalg.norm(x_true)**2\n    w_true_norm_sq = np.linalg.norm(w_true)**2\n    \n    support_mask = (w_true != 0)\n    complement_mask = ~support_mask\n    \n    # 2. Define test suite\n    test_cases = [\n        ('blur', 1e-1),\n        ('blur', 1.0),\n        ('blur', 1e-3),\n        ('id', 3e-1),\n    ]\n\n    results = []\n    \n    # 3. Iterate through test cases\n    for A_type, alpha in test_cases:\n        # Construct the forward operator\n        A = construct_forward_operator(A_type, n)\n        \n        # Generate the data\n        y = A @ x_true\n        \n        # Compute the Tikhonov estimator\n        # Solve (A^T A + alpha^2 I) x = A^T y\n        M = A.T @ A + (alpha**2) * np.identity(n)\n        rhs = A.T @ y\n        x_hat = np.linalg.solve(M, rhs)\n        \n        # Compute misfit metrics\n        # Metric b: Normalized squared model bias\n        bias_sq_norm = np.linalg.norm(x_hat - x_true)**2\n        b = bias_sq_norm / x_true_norm_sq\n        \n        # Metric l: Wavelet leakage ratio\n        w_hat = W @ x_hat\n        leakage_num = np.linalg.norm(w_hat[complement_mask])**2\n        l = leakage_num / w_true_norm_sq\n        \n        results.append([b, l])\n        \n    # 4. Format and print the final output\n    print(f\"{results}\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的应用不仅需要稳定一个不适定系统，还常常要施加物理约束并处理具有不同特性的解分量。这个高级实践通过将吉洪诺夫正则化与线性等式约束相结合，来应对这种复杂场景。在本练习中 ，您将解决一个解向量被划分为多个块的优化问题，并对每个块施加不同强度的平滑度正则化，从而掌握一种解决结构化逆问题的强大而灵活的方法。",
            "id": "3599473",
            "problem": "考虑以下带等式约束和块可分 Tikhonov 正则化的分块线性逆问题。设未知向量分块为 $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$，其中 $x_1 \\in \\mathbb{R}^{n_1}$ 且 $x_2 \\in \\mathbb{R}^{n_2}$。取 $n_1 = 3$，$n_2 = 2$，因此总维度为 $n = 5$。数据矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 已给出，其中 $m = 6$，其具体条目为\n$$\nA =\n\\begin{bmatrix}\n1  0  0.5  0  0.2 \\\\\n0  1  0.3  0.4  0 \\\\\n0.2  0.1  1  0.5  0.3 \\\\\n0.5  0.2  0.1  1  0.6 \\\\\n0  0.3  0.2  0.1  1 \\\\\n0.4  0  0.5  0.2  0.1\n\\end{bmatrix}.\n$$\n定义一个满足约束条件的真实向量\n$$\nx_{\\mathrm{true}} =\n\\begin{bmatrix}\n0.3 \\\\ 0.4 \\\\ 0.3 \\\\ 0.1 \\\\ -0.1\n\\end{bmatrix},\n$$\n并构成无噪声数据向量 $b = A x_{\\mathrm{true}}$。施加两个等式约束，记为 $C x = d$，其中 $C \\in \\mathbb{R}^{q \\times n}$，$q = 2$，且\n$$\nC = \\begin{bmatrix}\n1  1  1  0  0 \\\\\n0  0  0  1  1\n\\end{bmatrix}, \\qquad\nd = \\begin{bmatrix}\n1 \\\\ 0\n\\end{bmatrix}.\n$$\n正则化是块可分的，并使用一阶差分算子作用于每个分块。具体来说，令 $L_1 \\in \\mathbb{R}^{2 \\times 3}$ 和 $L_2 \\in \\mathbb{R}^{1 \\times 2}$ 为\n$$\nL_1 = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}, \\qquad\nL_2 = \\begin{bmatrix}\n-1  1\n\\end{bmatrix}.\n$$\n对于非负正则化参数 $\\lambda_1$ 和 $\\lambda_2$，考虑带约束的 Tikhonov 正则化目标函数\n$$\n\\Phi(x;\\lambda_1,\\lambda_2) = \\|A x - b\\|_2^2 + \\lambda_1 \\|L_1 x_1\\|_2^2 + \\lambda_2 \\|L_2 x_2\\|_2^2\n$$\n并服从等式约束 $C x = d$。矩阵 $A$ 通过其非对角子块耦合了分块 $x_1$ 和 $x_2$，而惩罚项仅在块内起作用。\n\n你的任务是，对于每个指定的参数对 $(\\lambda_1,\\lambda_2)$，计算该约束目标函数的唯一最小化子 $x^\\star(\\lambda_1,\\lambda_2)$，然后量化数据拟合与块光滑性之间的权衡。对于每个测试用例，按顺序输出以下三个浮点数：\n- 残差平方 $\\|A x^\\star - b\\|_2^2$，\n- 加权的块1光滑性贡献 $\\lambda_1 \\|L_1 x_1^\\star\\|_2^2$，\n- 加权的块2光滑性贡献 $\\lambda_2 \\|L_2 x_2^\\star\\|_2^2$。\n\n使用以下正则化参数对的测试套件，这些参数对旨在探索不同的情况：\n1. $(\\lambda_1,\\lambda_2) = (0, 0)$，代表无正则化的等式约束最小二乘解。\n2. $(\\lambda_1,\\lambda_2) = (1000, 0.01)$，代表对块 $x_1$ 强正则化，对块 $x_2$ 弱正则化。\n3. $(\\lambda_1,\\lambda_2) = (0.01, 1000)$，代表对块 $x_1$ 弱正则化，对块 $x_2$ 强正则化。\n4. $(\\lambda_1,\\lambda_2) = (1, 1)$，代表对两个块进行平衡的正则化。\n\n你的程序必须生成单行输出，其中包含所有测试用例的聚合结果，形式为方括号括起来的逗号分隔列表。排序必须是上述序列的串联，即 $[\\text{residual}_1, \\text{block1}_1, \\text{block2}_1, \\text{residual}_2, \\text{block1}_2, \\text{block2}_2, \\text{residual}_3, \\text{block1}_3, \\text{block2}_3, \\text{residual}_4, \\text{block1}_4, \\text{block2}_4]$。不涉及物理单位，结果应以原始浮点数形式打印。程序必须是自包含的，使用确定性计算，并且不需要用户输入。",
            "solution": "对问题陈述的初步验证证实，这是一个数值线性代数中的适定问题，具体来说是一个等式约束二次规划（ECQP）。所有矩阵、向量和参数都已完全指定，并且它们的维度相互一致。该问题具有科学依据，是客观的，并包含足够的信息来为每个测试用例产生唯一的解。\n\n核心任务是找到向量 $x \\in \\mathbb{R}^5$，使其最小化 Tikhonov 正则化目标函数\n$$\n\\Phi(x;\\lambda_1,\\lambda_2) = \\|A x - b\\|_2^2 + \\lambda_1 \\|L_1 x_1\\|_2^2 + \\lambda_2 \\|L_2 x_2\\|_2^2\n$$\n同时服从线性等式约束 $C x = d$。在这里，向量 $x$ 被分块为 $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$，其中 $x_1 \\in \\mathbb{R}^{n_1}$ ($n_1=3$) 且 $x_2 \\in \\mathbb{R}^{n_2}$ ($n_2=2$)。拉格朗日乘子法为求解提供了一条直接而严谨的途径。\n\n首先，我们将目标函数表示为标准的二次型。欧几里得范数的平方 $\\|v\\|_2^2$ 等价于点积 $v^T v$。\n$$\n\\Phi(x) = (Ax - b)^T(Ax - b) + \\lambda_1 (L_1 x_1)^T (L_1 x_1) + \\lambda_2 (L_2 x_2)^T (L_2 x_2)\n$$\n展开各项，我们得到：\n$$\n\\Phi(x) = (x^T A^T - b^T)(Ax - b) + \\lambda_1 x_1^T L_1^T L_1 x_1 + \\lambda_2 x_2^T L_2^T L_2 x_2\n$$\n$$\n\\Phi(x) = x^T A^T A x - x^T A^T b - b^T A x + b^T b + \\lambda_1 x_1^T L_1^T L_1 x_1 + \\lambda_2 x_2^T L_2^T L_2 x_2\n$$\n由于标量项 $b^T A x$ 等于其转置 $x^T A^T b$，表达式简化为：\n$$\n\\Phi(x) = x^T A^T A x - 2b^T A x + b^T b + \\lambda_1 x_1^T L_1^T L_1 x_1 + \\lambda_2 x_2^T L_2^T L_2 x_2\n$$\n为了紧凑地处理块可分的正则化项，我们引入一个块对角矩阵 $\\Gamma \\in \\mathbb{R}^{n \\times n}$，其中 $n = n_1 + n_2 = 5$。该矩阵定义为：\n$$\n\\Gamma = \\begin{bmatrix} \\lambda_1 L_1^T L_1  0_{n_1 \\times n_2} \\\\ 0_{n_2 \\times n_1}  \\lambda_2 L_2^T L_2 \\end{bmatrix}\n$$\n其中 $0_{i \\times j}$ 是一个大小为 $i \\times j$ 的零矩阵。根据此定义，正则化惩罚项之和可以写为 $x^T \\Gamma x$。完整的目标函数则为：\n$$\n\\Phi(x) = x^T(A^T A + \\Gamma)x - 2b^T A x + b^T b\n$$\n现在问题是在线性约束 $Cx=d$ 的条件下最小化这个关于 $x$ 的二次函数。\n\n我们构造拉格朗日函数 $\\mathcal{L}(x, \\mu)$，其中 $\\mu \\in \\mathbb{R}^q$ 是对应于 $q=2$ 个约束的拉格朗日乘子向量：\n$$\n\\mathcal{L}(x, \\mu) = \\Phi(x) + \\mu^T(Cx - d)\n$$\nKarush-Kuhn-Tucker (KKT) 最优性条件要求拉格朗日函数相对于 $x$ 和 $\\mu$ 的梯度均为零。\n\n相对于 $x$ 的梯度是：\n$$\n\\nabla_x \\mathcal{L}(x, \\mu) = \\nabla_x \\left( x^T(A^T A + \\Gamma)x - 2b^T A x + b^T b + \\mu^T C x - \\mu^T d \\right) = 0\n$$\n使用标准矩阵微积分恒等式，并注意到 $A^T A + \\Gamma$ 是一个对称矩阵，可得：\n$$\n2(A^T A + \\Gamma)x - 2A^T b + C^T \\mu = 0\n$$\n相对于 $\\mu$ 的梯度是：\n$$\n\\nabla_\\mu \\mathcal{L}(x, \\mu) = \\nabla_\\mu \\left( ... + \\mu^T(Cx - d) \\right) = 0\n$$\n$$\nCx - d = 0\n$$\n第二个条件只是恢复了原始的等式约束。\n\n这两个线性矩阵方程可以合并成一个单一的块矩阵系统，称为 KKT 系统，用于求解组合的未知向量 $\\begin{bmatrix} x \\\\ \\mu \\end{bmatrix}$：\n$$\n\\begin{bmatrix}\n2(A^T A + \\Gamma)  C^T \\\\\nC  0\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\n\\mu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 A^T b \\\\\nd\n\\end{bmatrix}\n$$\n这是一个包含 $n+q=7$ 个未知数的 $7$ 个线性方程的方阵系统。对问题给定条件的分析确保了对于所有指定的测试用例，左侧的 KKT 矩阵都是非奇异的，从而保证了唯一的解 $x^\\star$（最小化子）和 $\\mu^\\star$（对应的拉格朗日乘子）。\n\n对于每对正则化参数 $(\\lambda_1, \\lambda_2)$ 的计算算法如下：\n1. 构造矩阵 $\\Gamma = \\text{blockdiag}(\\lambda_1 L_1^T L_1, \\lambda_2 L_2^T L_2)$。\n2. 构造 KKT 矩阵 $K = \\begin{bmatrix} 2(A^T A + \\Gamma)  C^T \\\\ C  0 \\end{bmatrix}$ 和右侧向量 $RHS = \\begin{bmatrix} 2A^T b \\\\ d \\end{bmatrix}$。\n3. 求解线性系统 $Kz = RHS$ 得到 $z = \\begin{bmatrix} x^\\star \\\\ \\mu^\\star \\end{bmatrix}$。所需的解 $x^\\star$ 是由 $z$ 的前 $n=5$ 个分量组成的向量。\n4. 将解 $x^\\star$ 分块为 $x_1^\\star$（前 $n_1=3$ 个分量）和 $x_2^\\star$（后 $n_2=2$ 个分量）。\n5. 计算三个所需的度量：数据残差平方 $\\|A x^\\star - b\\|_2^2$，加权的块1惩罚项 $\\lambda_1 \\|L_1 x_1^\\star\\|_2^2$，以及加权的块2惩罚项 $\\lambda_2 \\|L_2 x_2^\\star\\|_2^2$。\n\n此过程通过数值计算实现，以获得每个测试用例的结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained Tikhonov regularization problem for a suite of test cases.\n    \"\"\"\n    # Define problem dimensions and constants as per the statement.\n    n1 = 3\n    n2 = 2\n    n = n1 + n2\n    q = 2\n\n    # Define the data matrix A.\n    A = np.array([\n        [1.0, 0.0, 0.5, 0.0, 0.2],\n        [0.0, 1.0, 0.3, 0.4, 0.0],\n        [0.2, 0.1, 1.0, 0.5, 0.3],\n        [0.5, 0.2, 0.1, 1.0, 0.6],\n        [0.0, 0.3, 0.2, 0.1, 1.0],\n        [0.4, 0.0, 0.5, 0.2, 0.1]\n    ])\n\n    # Define the ground-truth vector x_true.\n    x_true = np.array([0.3, 0.4, 0.3, 0.1, -0.1])\n\n    # Define the constraint matrix C and vector d.\n    C = np.array([\n        [1.0, 1.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 1.0]\n    ])\n    d = np.array([1.0, 0.0])\n\n    # Define the regularization operators L1 and L2.\n    L1 = np.array([\n        [-1.0, 1.0, 0.0],\n        [0.0, -1.0, 1.0]\n    ])\n    L2 = np.array([[-1.0, 1.0]])\n\n    # Define the test suite of regularization parameter pairs (lambda1, lambda2).\n    test_cases = [\n        (0.0, 0.0),\n        (1000.0, 0.01),\n        (0.01, 1000.0),\n        (1.0, 1.0)\n    ]\n\n    # Pre-computation of constant terms\n    b = A @ x_true\n    AtA = A.T @ A\n    Atb = A.T @ b\n    L1tL1 = L1.T @ L1\n    L2tL2 = L2.T @ L2\n\n    results = []\n    for lambda1, lambda2 in test_cases:\n        # 1. Construct the ingredients for the KKT system.\n        \n        # Build the block-diagonal regularization matrix Gamma.\n        Gamma = np.zeros((n, n))\n        Gamma[:n1, :n1] = lambda1 * L1tL1\n        Gamma[n1:, n1:] = lambda2 * L2tL2\n\n        # Build the Hessian of the objective function.\n        H = 2 * (AtA + Gamma)\n\n        # Assemble the full KKT matrix.\n        K = np.zeros((n + q, n + q))\n        K[:n, :n] = H\n        K[:n, n:] = C.T\n        K[n:, :n] = C\n        \n        # Assemble the right-hand-side vector.\n        rhs = np.zeros(n + q)\n        rhs[:n] = 2 * Atb\n        rhs[n:] = d\n\n        # 2. Solve the KKT system K*z = rhs for z = [x_star, mu_star].\n        z = np.linalg.solve(K, rhs)\n        x_star = z[:n]\n\n        # 3. Partition the solution and compute the required output quantities.\n        x1_star = x_star[:n1]\n        x2_star = x_star[n1:]\n        \n        # Calculate squared residual, and weighted smoothness contributions.\n        # np.linalg.norm(v)**2 is a numerically stable way to compute v.T @ v.\n        residual_sq = np.linalg.norm(A @ x_star - b)**2\n        smoothness1 = lambda1 * np.linalg.norm(L1 @ x1_star)**2\n        smoothness2 = lambda2 * np.linalg.norm(L2 @ x2_star)**2\n        \n        results.extend([residual_sq, smoothness1, smoothness2])\n\n    # Final print statement must match the specified format exactly.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}