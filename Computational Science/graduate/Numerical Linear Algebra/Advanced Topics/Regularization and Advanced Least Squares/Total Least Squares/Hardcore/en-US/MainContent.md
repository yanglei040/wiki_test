## Introduction
In scientific and engineering disciplines, fitting a linear model to observed data is a fundamental task. The go-to method for this is often Ordinary Least Squares (OLS), celebrated for its simplicity and clear geometric interpretation of projecting a data vector onto a fixed subspace. However, the OLS framework rests on a critical assumption: that predictor variables are measured perfectly, with all error confined to the observations. In the real world, from calibrating physics experiments to processing signals in robotics, this assumption frequently breaks down. When both predictors and responses are noisy—a scenario known as the Errors-in-Variables (EIV) model—OLS yields biased and inconsistent results. This gap between the idealized model and empirical reality necessitates a more robust approach.

This article explores Total Least Squares (TLS), a powerful generalization of OLS designed specifically for the EIV problem. Instead of minimizing vertical errors, TLS minimizes the orthogonal distances from data points to the fitted model, thereby acknowledging and correcting for errors in all variables. We will begin in **Principles and Mechanisms** by formally defining TLS, contrasting its core assumptions with those of OLS, and deriving its elegant solution using the Singular Value Decomposition (SVD). Next, in **Applications and Interdisciplinary Connections**, we will survey the broad utility of TLS across diverse fields, from geometric fitting and computer vision to [system identification](@entry_id:201290) and [analytical chemistry](@entry_id:137599), illustrating its role as a unifying concept. Finally, **Hands-On Practices** will provide opportunities to implement and explore TLS, bridging the gap between theory and practical computation. Let's begin by shifting our perspective from the fixed-predictor world of OLS to the more realistic and geometrically richer framework of Total Least Squares.

## Principles and Mechanisms

### From Ordinary to Total Least Squares: A Shift in Perspective

In data analysis, we frequently seek to model a [linear relationship](@entry_id:267880) between a set of predictor variables, represented by a matrix $A \in \mathbb{R}^{m \times n}$, and a response vector $b \in \mathbb{R}^{m}$. The most common method for this task is **Ordinary Least Squares (OLS)**, which aims to find a vector $x \in \mathbb{R}^{n}$ that minimizes the sum of squared vertical residuals, expressed as the minimization of the Euclidean norm of the [residual vector](@entry_id:165091): $\min_{x} \|Ax - b\|_2^2$.

The OLS framework is built on a crucial, and often unstated, assumption: the predictor variables in matrix $A$ are known exactly, and all [measurement error](@entry_id:270998) is confined to the response vector $b$. Geometrically, OLS finds the vector within the column space of $A$, $\mathcal{R}(A)$, that is closest to $b$. This is achieved by orthogonally projecting $b$ onto the fixed subspace $\mathcal{R}(A)$. The solution is famously given by the **Moore-Penrose pseudoinverse**, $x_{\mathrm{LS}} = A^{+} b$.

However, in many scientific and engineering applications, this assumption is untenable. Measurements of both the predictor variables and the response are subject to noise. This scenario is known as the **Errors-in-Variables (EIV)** model. When predictors in $A$ are noisy, the OLS estimator is no longer optimal and, more importantly, it becomes biased. For instance, in a simple regression model where the true relationship is $b = A_{\star}x_{\star}$ but we observe a noisy matrix $A = A_{\star} + E$, the OLS estimate for $x$ will be systematically underestimated in magnitude. This phenomenon is known as **[attenuation bias](@entry_id:746571)**.

To address the shortcomings of OLS in the EIV context, we turn to **Total Least Squares (TLS)**. Instead of minimizing only the vertical distances, TLS seeks to find a solution that minimizes the sum of squared **orthogonal** (or perpendicular) distances from the data points to the fitted model. This fundamental shift in the objective function acknowledges that errors can exist in all measured variables. Geometrically, TLS does not project onto a fixed subspace; instead, it seeks a "nearby" subspace that best fits the data, allowing the model itself (represented by the columns of $A$) to be adjusted.

Consider fitting a line to a set of points $(a_i, b_i)$ in a 2D plane. OLS minimizes $\sum (b_i - (xa_i + c))^2$, the sum of squared vertical distances. TLS, in contrast, finds the line that minimizes the sum of squared perpendicular distances from each point to the line. This makes TLS the more appropriate choice when both $x$ and $y$ coordinates are known to be noisy. Because their objective functions differ, OLS and TLS will, in general, produce different results when errors are present in the predictors.

### Formal Definition and Probabilistic Justification

The Total Least Squares problem can be formally stated as a constrained optimization problem. Given $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^{m}$, we seek to find perturbations $\Delta A \in \mathbb{R}^{m \times n}$ and $\Delta b \in \mathbb{R}^{m}$, along with a solution vector $x \in \mathbb{R}^{n}$, such that:
$$ \min_{\Delta A, \Delta b, x} \|[\Delta A \;\; \Delta b]\|_F \quad \text{subject to} \quad (A + \Delta A)x = b + \Delta b $$
Here, $[\Delta A \;\; \Delta b]$ is the augmented perturbation matrix, and $\|\cdot\|_F$ is the **Frobenius norm**, defined as the square root of the sum of the squares of all matrix entries.

The choice of the Frobenius norm is not arbitrary; it has deep roots in [statistical modeling](@entry_id:272466) and invariance principles. If we assume that the errors affecting each entry of the augmented data matrix $[A \;\; b]$ are [independent and identically distributed](@entry_id:169067) (i.i.d.) from a zero-mean Gaussian distribution with a common variance $\sigma^2$, then the **Maximum Likelihood Estimator (MLE)** for the "true" underlying data corresponds precisely to the solution of the TLS problem. Maximizing the likelihood of observing the data is equivalent to minimizing the [negative log-likelihood](@entry_id:637801), which, for this error model, is proportional to the squared Frobenius norm of the perturbation, $\|[\Delta A \;\; \Delta b]\|_F^2$.

This formulation can be generalized. If the errors are not i.i.d. but are drawn from a multivariate Gaussian distribution with a [general covariance](@entry_id:159290) matrix $\Sigma$, the MLE would minimize a weighted norm, $\| \Sigma^{-1/2} \mathrm{vec}([\Delta A \;\; \Delta b]) \|_2^2$, known as the Mahalanobis distance. The standard TLS problem arises as the special case where the covariance is isotropic, i.e., $\Sigma = \sigma^2 I$. Similarly, if errors in $A$ and $b$ are isotropic within each block but have different variances, $\sigma_A^2$ and $\sigma_b^2$, the appropriate objective becomes a weighted sum, $\|\Delta A\|_F^2 / \sigma_A^2 + \|\Delta b\|_2^2 / \sigma_b^2$. This reduces to the standard TLS objective only when the variances are equal.

### The SVD-Based Solution Method

The brilliance of the TLS formulation is that it transforms a complex non-linear estimation problem into a standard problem in [numerical linear algebra](@entry_id:144418): [low-rank approximation](@entry_id:142998). The constraint $(A + \Delta A)x = b + \Delta b$ can be rewritten as:
$$ [A + \Delta A \;\; b + \Delta b] \begin{pmatrix} x \\ -1 \end{pmatrix} = \mathbf{0} $$
This means that the [augmented matrix](@entry_id:150523) $[(A + \Delta A) \;\; (b + \Delta b)]$ must be rank-deficient; its columns are linearly dependent. Let $C = [A \;\; b]$ be the original augmented data matrix. The TLS problem is equivalent to finding the smallest perturbation $\Delta C = [\Delta A \;\; \Delta b]$ such that the perturbed matrix $C + \Delta C$ has rank less than or equal to $n$.

The **Eckart-Young-Mirsky theorem** states that the closest rank-$k$ approximation to a matrix $C$ in the Frobenius norm is obtained from its **Singular Value Decomposition (SVD)**. Let the SVD of $C \in \mathbb{R}^{m \times (n+1)}$ be $C = U \Sigma V^T$, with singular values $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_{n+1} \ge 0$. Assuming the problem is not an exact fit (i.e., $\sigma_{n+1} > 0$), the closest rank-$n$ matrix to $C$ is obtained by setting the smallest singular value $\sigma_{n+1}$ to zero. The minimal perturbation required to achieve this is $\Delta C = -\sigma_{n+1} u_{n+1} v_{n+1}^T$, and the magnitude of this perturbation is $\|\Delta C\|_F = \sigma_{n+1}$.

The [null space](@entry_id:151476) of the resulting matrix $C + \Delta C$ is spanned by the right [singular vector](@entry_id:180970) $v_{n+1}$ corresponding to $\sigma_{n+1}$. Therefore, the vector $\begin{pmatrix} x \\ -1 \end{pmatrix}$ must be proportional to $v_{n+1}$. Let us partition this right [singular vector](@entry_id:180970) $v_{n+1} \in \mathbb{R}^{n+1}$ as:
$$ v_{n+1} = \begin{pmatrix} \hat{v} \\ v_b \end{pmatrix} $$
where $\hat{v} \in \mathbb{R}^n$ and $v_b \in \mathbb{R}$. For a solution to exist in the form we require, we must have a non-zero last component, so we can scale the vector. If the condition $v_b \neq 0$ holds, we can write:
$$ \begin{pmatrix} x_{\mathrm{TLS}} \\ -1 \end{pmatrix} \propto v_{n+1} \implies x_{\mathrm{TLS}} = -\frac{\hat{v}}{v_b} $$
This provides a direct, elegant algorithm for computing the TLS solution. If the condition $v_b = 0$ occurs, no finite solution for $x_{\mathrm{TLS}}$ exists. This special case corresponds to a situation where the best rank-reducing perturbation can be achieved by only modifying $A$ and leaving $b$ unchanged.

It is important to note that if the system has an exact solution (i.e., $b \in \mathcal{R}(A)$), then the OLS and TLS problems are both solved with zero error, and their solutions coincide: $x_{\mathrm{LS}} = x_{\mathrm{TLS}} = A^{+} b$. In all other cases, the two solutions generally differ.

### Geometric Interpretation and Connection to PCA

The SVD-based solution has a powerful geometric interpretation that connects TLS to **Principal Component Analysis (PCA)**. Consider the rows of the [augmented matrix](@entry_id:150523) $[A \;\; b]$ as a cloud of $m$ points in an $(n+1)$-dimensional space. The TLS problem of fitting a linear hyperplane $y_1 x_1 + \dots + y_n x_n - y_{n+1} = 0$ is equivalent to finding the hyperplane that minimizes the sum of squared orthogonal distances to these data points.

It is a fundamental result that this best-fit hyperplane is defined by the direction of minimum variance in the data cloud. This direction is given precisely by the eigenvector of the data's covariance matrix corresponding to the smallest eigenvalue. For zero-mean data, this eigenvector is identical to the right [singular vector](@entry_id:180970) of the data matrix corresponding to the smallest [singular value](@entry_id:171660).

Therefore, the vector $v_{n+1}$ from the SVD of $[A \;\; b]$ (or, more generally, the centered data matrix for an affine fit) gives the coefficients of the implicit equation of the TLS [hyperplane](@entry_id:636937). This provides a profound link: TLS is essentially performing PCA on the combined data $[A \;\; b]$ and identifying the component with the least variance as the normal to the solution [hyperplane](@entry_id:636937). The minimum sum of squared orthogonal distances itself is equal to the square of the smallest [singular value](@entry_id:171660), $\sigma_{n+1}^2$.

### Practical Implementation and Extensions

#### Affine Models and Data Centering
The basic TLS formulation finds a homogeneous model, a [hyperplane](@entry_id:636937) passing through the origin. Most real-world models are affine, containing an intercept term: $b \approx A\beta + \beta_0\mathbf{1}$. There are two primary ways to correctly handle this intercept within the TLS framework.

1.  **Data Centering**: The most common approach is to first center the data by subtracting the column means. Let $J = I - \frac{1}{n}\mathbf{1}\mathbf{1}^T$ be the centering matrix. Applying this projector to the affine model annihilates the intercept term: $Jb \approx J(A\beta) + \beta_0(J\mathbf{1}) \implies b_c \approx A_c \beta$, where $A_c=JA$ and $b_c=Jb$. One then solves this homogeneous TLS problem for the slope vector $\beta$. The intercept is recovered afterward by ensuring the final hyperplane passes through the [centroid](@entry_id:265015) of the data $(\bar{A}, \bar{b})$: $\beta_0 = \bar{b} - \bar{A}\beta$.

2.  **Augmentation with an Exact Column**: An alternative is to rewrite the model as $b \approx [A \;\; \mathbf{1}] \begin{pmatrix} \beta \\ \beta_0 \end{pmatrix}$. To solve this, one must use a **mixed** or **constrained TLS** algorithm that treats the appended column of ones as exact (i.e., not subject to perturbation). This approach is equivalent to the centering method and yields the same slope estimate. It is crucial, however, *not* to apply standard TLS naively to the [augmented matrix](@entry_id:150523) $[A \;\; \mathbf{1} \;\; b]$, as this would allow the algorithm to perturb the column of ones, fundamentally distorting the meaning of the intercept.

#### Numerical Stability
The TLS solution is found from the SVD of the [augmented matrix](@entry_id:150523) $C = [A \;\; b]$. One could also find the required right [singular vector](@entry_id:180970) $v_{n+1}$ by first forming the cross-product matrix $C^T C$ and then solving for its smallest eigenvalue and corresponding eigenvector. In exact arithmetic, these two methods are identical.

However, in [floating-point arithmetic](@entry_id:146236), they are worlds apart. The process of explicitly forming $C^T C$ squares the spectral condition number of the matrix $C$. If $C$ is ill-conditioned, the condition number of $C^T C$ can become so large that information related to the smallest singular values is completely lost to [roundoff error](@entry_id:162651). Modern SVD algorithms work directly on $C$ and are backward stable; they avoid this numerical pitfall. For this reason, the direct SVD-based approach is strongly preferred in any practical implementation for its superior numerical accuracy and robustness.