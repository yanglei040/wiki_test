{
    "hands_on_practices": [
        {
            "introduction": "The L-curve is constructed by plotting the logarithm of the residual norm against the logarithm of the solution (semi)norm. This first practice explores a fundamental justification for this log-log representation. By analyzing how the curve transforms under scaling of the data weights and the regularization operator, you will demonstrate a crucial scale-invariance property of the L-curve's geometry . This exercise reveals that the optimal regularization parameter, as identified by the point of maximum curvature, corresponds to the same underlying solution regardless of arbitrary choices in problem weighting, highlighting the robustness of the method.",
            "id": "3554621",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $W \\in \\mathbb{R}^{m \\times m}$ be a positive-definite weighting matrix on the residual, and $L \\in \\mathbb{R}^{p \\times n}$ be a full row-rank regularization operator. For a regularization parameter $\\lambda > 0$, consider Tikhonov regularization defined by the minimizer $x_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\| W (A x - b) \\|_{2}^{2} + \\lambda^{2} \\| L x \\|_{2}^{2}$. Define the L-curve in logarithmic coordinates as the parametric curve $C(\\lambda) = \\big( u(\\lambda), v(\\lambda) \\big)$ with $u(\\lambda) = \\ln \\| W (A x_{\\lambda} - b) \\|_{2}$ and $v(\\lambda) = \\ln \\| L x_{\\lambda} \\|_{2}$.\n\nNow consider scaled weights $W_{\\alpha} = \\alpha W$ and $L_{\\beta} = \\beta L$ with $\\alpha > 0$ and $\\beta > 0$, and define the scaled regularized solution $y_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\| W_{\\alpha} (A x - b) \\|_{2}^{2} + \\lambda^{2} \\| L_{\\beta} x \\|_{2}^{2}$. Let the corresponding scaled L-curve be $C_{\\mathrm{scaled}}(\\lambda) = \\big( \\tilde{u}(\\lambda), \\tilde{v}(\\lambda) \\big)$ with $\\tilde{u}(\\lambda) = \\ln \\| W_{\\alpha} (A y_{\\lambda} - b) \\|_{2}$ and $\\tilde{v}(\\lambda) = \\ln \\| L_{\\beta} y_{\\lambda} \\|_{2}$.\n\nStarting from first principles in numerical linear algebra and the definitions above, derive how $C_{\\mathrm{scaled}}(\\lambda)$ is related to $C(\\lambda)$, including any reparametrization of the regularization parameter induced by $(\\alpha, \\beta)$, and determine the conditions under which corner detection based on geometric curvature in the logarithmic L-curve is invariant to the scaling $(\\alpha, \\beta)$.\n\nFinally, compute the exact value of the ratio of geometric curvature of the scaled logarithmic L-curve to that of the original logarithmic L-curve at corresponding points under the induced reparametrization. Your final answer must be a single real-valued number or a single closed-form analytic expression. No rounding is required.",
            "solution": "The analysis begins by formalizing the original and scaled Tikhonov regularization problems. The objective is to establish a relationship between their respective solutions and L-curves, and subsequently analyze the geometric properties of these curves.\n\nFirst, consider the original Tikhonov regularization problem. The solution $x_{\\lambda}$ minimizes the functional\n$$J(x, \\lambda) = \\| W (A x - b) \\|_{2}^{2} + \\lambda^{2} \\| L x \\|_{2}^{2}$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $W \\in \\mathbb{R}^{m \\times m}$ is positive-definite, and $L \\in \\mathbb{R}^{p \\times n}$ has full row rank. This is a quadratic optimization problem. The minimizer $x_\\lambda$ is found by setting the gradient of $J(x, \\lambda)$ with respect to $x$ to zero:\n$$\\nabla_x J(x, \\lambda) = 2 A^{T} W^{T} W (A x - b) + 2 \\lambda^{2} L^{T} L x = 0$$\nThis yields the normal equations for the regularized solution $x_{\\lambda}$:\n$$(A^{T} W^{T} W A + \\lambda^{2} L^{T} L) x_{\\lambda} = A^{T} W^{T} W b$$\nThe matrix $(A^{T} W^{T} W A + \\lambda^{2} L^{T} L)$ is invertible for any $\\lambda > 0$, ensuring a unique solution $x_{\\lambda}$.\n\nNext, we consider the scaled Tikhonov problem. The scaled weighting matrix and regularization operator are $W_{\\alpha} = \\alpha W$ and $L_{\\beta} = \\beta L$ for scalar constants $\\alpha > 0$ and $\\beta > 0$. The solution $y_{\\lambda}$ minimizes the scaled functional:\n$$J_{\\mathrm{scaled}}(x, \\lambda) = \\| W_{\\alpha} (A x - b) \\|_{2}^{2} + \\lambda^{2} \\| L_{\\beta} x \\|_{2}^{2}$$\nSubstituting the definitions of $W_{\\alpha}$ and $L_{\\beta}$:\n$$J_{\\mathrm{scaled}}(x, \\lambda) = \\| \\alpha W (A x - b) \\|_{2}^{2} + \\lambda^{2} \\| \\beta L x \\|_{2}^{2} = \\alpha^{2} \\| W (A x - b) \\|_{2}^{2} + (\\lambda \\beta)^{2} \\| L x \\|_{2}^{2}$$\nThe minimizer of $J_{\\mathrm{scaled}}(x, \\lambda)$ is identical to the minimizer of $\\frac{1}{\\alpha^2} J_{\\mathrm{scaled}}(x, \\lambda)$ since $\\alpha^2 > 0$. Let us define this rescaled functional:\n$$\\frac{1}{\\alpha^2} J_{\\mathrm{scaled}}(x, \\lambda) = \\| W (A x - b) \\|_{2}^{2} + \\frac{(\\lambda \\beta)^{2}}{\\alpha^2} \\| L x \\|_{2}^{2} = \\| W (A x - b) \\|_{2}^{2} + \\left(\\frac{\\lambda \\beta}{\\alpha}\\right)^{2} \\| L x \\|_{2}^{2}$$\nThis functional has the exact same form as the original functional $J(x, \\mu)$ if we define a new regularization parameter $\\mu$ such that $\\mu^2 = (\\lambda \\beta / \\alpha)^2$. Since $\\lambda, \\alpha, \\beta$ are all positive, we have the reparametrization:\n$$\\mu(\\lambda) = \\lambda \\frac{\\beta}{\\alpha}$$\nTherefore, the solution $y_{\\lambda}$ of the scaled problem for a given $\\lambda$ is identical to the solution $x_{\\mu}$ of the original problem with the parameter $\\mu = \\lambda \\beta / \\alpha$. That is,\n$$y_{\\lambda} = x_{\\mu(\\lambda)} = x_{\\lambda \\beta / \\alpha}$$\n\nNow we can relate the scaled L-curve $C_{\\mathrm{scaled}}(\\lambda) = (\\tilde{u}(\\lambda), \\tilde{v}(\\lambda))$ to the original L-curve $C(\\lambda) = (u(\\lambda), v(\\lambda))$.\nThe components of the original curve are $u(\\lambda) = \\ln \\| W (A x_{\\lambda} - b) \\|_{2}$ and $v(\\lambda) = \\ln \\| L x_{\\lambda} \\|_{2}$.\nThe components of the scaled curve are $\\tilde{u}(\\lambda) = \\ln \\| W_{\\alpha} (A y_{\\lambda} - b) \\|_{2}$ and $\\tilde{v}(\\lambda) = \\ln \\| L_{\\beta} y_{\\lambda} \\|_{2}$.\nLet's express $\\tilde{u}(\\lambda)$ and $\\tilde{v}(\\lambda)$ in terms of $u$ and $v$. Using $y_{\\lambda} = x_{\\mu(\\lambda)}$:\n$$\\tilde{u}(\\lambda) = \\ln \\| \\alpha W (A y_{\\lambda} - b) \\|_{2} = \\ln \\left( \\alpha \\| W (A x_{\\mu(\\lambda)} - b) \\|_{2} \\right) = \\ln(\\alpha) + \\ln \\| W (A x_{\\mu(\\lambda)} - b) \\|_{2} = \\ln(\\alpha) + u(\\mu(\\lambda))$$\nSimilarly for the solution seminorm component:\n$$\\tilde{v}(\\lambda) = \\ln \\| \\beta L y_{\\lambda} \\|_{2} = \\ln \\left( \\beta \\| L x_{\\mu(\\lambda)} \\|_{2} \\right) = \\ln(\\beta) + \\ln \\| L x_{\\mu(\\lambda)} \\|_{2} = \\ln(\\beta) + v(\\mu(\\lambda))$$\nThus, the scaled L-curve is related to the original L-curve by:\n$$C_{\\mathrm{scaled}}(\\lambda) = \\big( \\tilde{u}(\\lambda), \\tilde{v}(\\lambda) \\big) = \\big( \\ln(\\alpha) + u(\\mu(\\lambda)), \\ln(\\beta) + v(\\mu(\\lambda)) \\big)$$\nwhere $\\mu(\\lambda) = \\lambda \\beta / \\alpha$. This shows that the scaled L-curve is a translation of the original L-curve by the vector $(\\ln \\alpha, \\ln \\beta)$, with its parameter $\\lambda$ related to the original parameter $\\mu$ by a linear scaling.\n\nThe \"corner\" of the L-curve is found at the point of maximum geometric curvature. A translation of a curve does not change its shape or any intrinsic geometric properties, such as curvature. Therefore, the shape of the L-curve is invariant to the scaling factors $\\alpha$ and $\\beta$.\nLet $\\kappa(\\mu)$ be the curvature of the original curve $C(\\mu)$ at parameter value $\\mu$. Let $\\kappa_{\\mathrm{scaled}}(\\lambda)$ be the curvature of the scaled curve $C_{\\mathrm{scaled}}(\\lambda)$ at parameter value $\\lambda$.\nThe position vector for the original curve is $\\vec{r}(\\mu) = (u(\\mu), v(\\mu))$.\nThe position vector for the scaled curve is $\\vec{r}_{\\mathrm{scaled}}(\\lambda) = (\\ln \\alpha, \\ln \\beta) + \\vec{r}(\\mu(\\lambda))$.\nThe geometric curvature is invariant under reparametrization in the sense that the maximum curvature corresponds to the same geometric point on the curve. If the original curve has a corner (maximum curvature) at parameter value $\\mu^*$, which identifies the optimal solution $x_{\\mu^*}$, then the scaled curve will have a corner at a parameter value $\\lambda^*$ such that $\\mu(\\lambda^*) = \\mu^*$. This gives $\\lambda^* = \\mu^* \\frac{\\alpha}{\\beta}$. The solution obtained from the scaled problem is $y_{\\lambda^*} = x_{\\mu(\\lambda^*)} = x_{\\mu^*}$, which is the same optimal solution.\nTherefore, corner detection based on maximizing geometric curvature is invariant to the scaling $(\\alpha, \\beta)$ for all $\\alpha > 0$ and $\\beta > 0$, in the sense that it identifies the same optimal regularized solution vector.\n\nFinally, we compute the ratio of the geometric curvatures at corresponding points. The \"corresponding points\" are those related by the reparametrization, i.e., the point on the scaled curve with parameter $\\lambda$ and the point on the original curve with parameter $\\mu = \\lambda \\beta / \\alpha$.\nThe geometric curvature for a parametric curve $\\vec{p}(t) = (x(t), y(t))$ is given by $\\kappa(t) = \\frac{|x'y'' - y'x''|}{(x'^2 + y'^2)^{3/2}}$, where primes denote derivatives with respect to $t$. In vector notation, $\\kappa(t) = \\frac{|\\det(\\vec{p}'(t), \\vec{p}''(t))|}{\\|\\vec{p}'(t)\\|_{2}^{3}}$.\n\nLet dots denote derivatives with respect to $\\mu$ and primes denote derivatives with respect to $\\lambda$.\n$\\vec{r}_{\\mathrm{scaled}}'(\\lambda) = \\frac{d}{d\\lambda} \\vec{r}(\\mu(\\lambda)) = \\dot{\\vec{r}}(\\mu) \\frac{d\\mu}{d\\lambda} = \\dot{\\vec{r}}(\\mu) \\frac{\\beta}{\\alpha}$.\n$\\vec{r}_{\\mathrm{scaled}}''(\\lambda) = \\frac{d}{d\\lambda} \\left( \\dot{\\vec{r}}(\\mu) \\frac{\\beta}{\\alpha} \\right) = \\ddot{\\vec{r}}(\\mu) \\left(\\frac{d\\mu}{d\\lambda}\\right)^2 + \\dot{\\vec{r}}(\\mu) \\frac{d^2\\mu}{d\\lambda^2}$.\nSince $\\mu(\\lambda) = \\lambda \\beta / \\alpha$ is a linear function of $\\lambda$, its second derivative is zero: $\\frac{d^2\\mu}{d\\lambda^2} = 0$.\nSo, $\\vec{r}_{\\mathrm{scaled}}''(\\lambda) = \\ddot{\\vec{r}}(\\mu) \\left(\\frac{\\beta}{\\alpha}\\right)^2$.\n\nNow we compute the curvature $\\kappa_{\\text{scaled}}(\\lambda)$:\nThe numerator term is $|\\det(\\vec{r}_{\\mathrm{scaled}}', \\vec{r}_{\\mathrm{scaled}}'')| = \\left| \\det\\left( \\dot{\\vec{r}}(\\mu) \\frac{\\beta}{\\alpha}, \\ddot{\\vec{r}}(\\mu) \\left(\\frac{\\beta}{\\alpha}\\right)^2 \\right) \\right|$.\nUsing the property $\\det(c_1 \\mathbf{v}_1, c_2 \\mathbf{v}_2) = c_1 c_2 \\det(\\mathbf{v}_1, \\mathbf{v}_2)$, this becomes:\n$\\left| \\left(\\frac{\\beta}{\\alpha}\\right) \\left(\\frac{\\beta}{\\alpha}\\right)^2 \\det(\\dot{\\vec{r}}(\\mu), \\ddot{\\vec{r}}(\\mu)) \\right| = \\left(\\frac{\\beta}{\\alpha}\\right)^3 |\\det(\\dot{\\vec{r}}(\\mu), \\ddot{\\vec{r}}(\\mu))|$, since $\\alpha, \\beta > 0$.\n\nThe denominator term is $\\|\\vec{r}_{\\mathrm{scaled}}'(\\lambda)\\|_{2}^{3} = \\left\\| \\dot{\\vec{r}}(\\mu) \\frac{\\beta}{\\alpha} \\right\\|_{2}^{3}$.\nUsing the property $\\|\\mathbf{v}c\\|_2 = |c|\\|\\mathbf{v}\\|_2$, this becomes:\n$\\left( \\left| \\frac{\\beta}{\\alpha} \\right| \\|\\dot{\\vec{r}}(\\mu)\\|_{2} \\right)^3 = \\left(\\frac{\\beta}{\\alpha}\\right)^3 \\|\\dot{\\vec{r}}(\\mu)\\|_{2}^{3}$.\n\nCombining these, the curvature of the scaled curve is:\n$$\\kappa_{\\mathrm{scaled}}(\\lambda) = \\frac{\\left(\\frac{\\beta}{\\alpha}\\right)^3 |\\det(\\dot{\\vec{r}}(\\mu), \\ddot{\\vec{r}}(\\mu))|}{\\left(\\frac{\\beta}{\\alpha}\\right)^3 \\|\\dot{\\vec{r}}(\\mu)\\|_{2}^{3}} = \\frac{|\\det(\\dot{\\vec{r}}(\\mu), \\ddot{\\vec{r}}(\\mu))|}{\\|\\dot{\\vec{r}}(\\mu)\\|_{2}^{3}}$$\nThe right-hand side is precisely the definition of the curvature of the original curve, evaluated at parameter $\\mu$.\n$$\\kappa_{\\mathrm{scaled}}(\\lambda) = \\kappa(\\mu) = \\kappa\\left(\\lambda \\frac{\\beta}{\\alpha}\\right)$$\nThe problem asks for the ratio of the geometric curvature of the scaled curve to that of the original curve at corresponding points. The corresponding points are those indexed by $\\lambda$ for the scaled curve and $\\mu = \\mu(\\lambda)$ for the original curve. The ratio is therefore:\n$$\\frac{\\kappa_{\\mathrm{scaled}}(\\lambda)}{\\kappa(\\mu(\\lambda))} = \\frac{\\kappa(\\mu(\\lambda))}{\\kappa(\\mu(\\lambda))} = 1$$\nThis ratio is exactly $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The success of the L-curve method hinges on the existence of a distinct, sharp \"corner\" that clearly demarcates the optimal trade-off between the residual and solution norms. This practice investigates a scenario where this assumption breaks down. You will analytically derive the solution, the L-curve, and its curvature for the limiting case of a problem with a perfectly flat singular value spectrum . This exercise provides a valuable lesson on the method's limitations, showing precisely why a lack of spectral variation leads to an ambiguous, gently bending curve and thus an unreliable parameter choice.",
            "id": "3554649",
            "problem": "Consider the linear inverse problem in $\\mathbb{R}^{n}$ defined by the Tikhonov-regularized least-squares objective\n$$\nJ_{\\lambda}(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda^{2} \\|x\\|_{2}^{2},\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^{n}$ is nonzero, and $\\lambda > 0$ is the regularization parameter. The L-curve is the parametric log-log plot of the residual norm against the solution norm,\n$$\n\\big(\\ln \\rho(\\lambda), \\, \\ln \\eta(\\lambda)\\big), \\quad \\text{with} \\quad \\rho(\\lambda) := \\|A x_{\\lambda} - b\\|_{2}, \\quad \\eta(\\lambda) := \\|x_{\\lambda}\\|_{2},\n$$\nwhere $x_{\\lambda}$ minimizes $J_{\\lambda}(x)$. Construct a matrix $A$ whose singular values are nearly flat by taking the analytically tractable limiting case $A = \\sigma I_{n}$, with $\\sigma > 0$, and analyze the resulting L-curve.\n\nStarting only from fundamental definitions—namely, the normal equations for the minimizer $x_{\\lambda}$ and the definition of curvature for a plane curve—derive:\n1. The expressions for $\\rho(\\lambda)$ and $\\eta(\\lambda)$ in terms of $\\sigma$, $\\lambda$, and $\\|b\\|_{2}$.\n2. The parametric plane curve $c(\\lambda) = \\big(\\ln \\rho(\\lambda), \\ln \\eta(\\lambda)\\big)$ and its curvature $\\kappa(\\lambda)$ (as a function of $\\lambda$) computed via the standard curvature definition for a parametrically represented plane curve.\n3. The value $\\lambda_{\\star} > 0$ that maximizes $\\kappa(\\lambda)$, together with the maximal curvature $\\kappa_{\\max} := \\max_{\\lambda > 0} \\kappa(\\lambda)$.\n\nExplain, on the basis of your analytical expression for $\\kappa(\\lambda)$, why the L-curve lacks a sharp corner in this flat-singular-value setting and thus leads to ambiguous selection of $\\lambda$.\n\nProvide your final result as the pair $\\big(\\lambda_{\\star}, \\kappa_{\\max}\\big)$ in exact analytic form. No rounding is required.",
            "solution": "We begin from first principles for Tikhonov regularization. The minimizer $x_{\\lambda}$ of\n$$\nJ_{\\lambda}(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda^{2} \\|x\\|_{2}^{2}\n$$\nsatisfies the normal equations\n$$\nA^{\\top} (A x_{\\lambda} - b) \\;+\\; \\lambda^{2} x_{\\lambda} \\;=\\; 0,\n$$\nwhich can be rearranged to\n$$\n\\big(A^{\\top} A + \\lambda^{2} I\\big) x_{\\lambda} \\;=\\; A^{\\top} b.\n$$\nIn the analytically tractable limiting case of nearly flat singular values, we set $A = \\sigma I_{n}$ with $\\sigma > 0$. Then $A^{\\top} A = \\sigma^{2} I_{n}$ and $A^{\\top} b = \\sigma b$, so the normal equations reduce to\n$$\n\\big(\\sigma^{2} I_{n} + \\lambda^{2} I_{n}\\big) x_{\\lambda} \\;=\\; \\sigma b\n\\quad\\Longrightarrow\\quad\nx_{\\lambda} \\;=\\; \\frac{\\sigma}{\\sigma^{2} + \\lambda^{2}} \\, b.\n$$\nThe residual is\n$$\nr_{\\lambda} \\;=\\; A x_{\\lambda} - b\n\\;=\\; \\sigma x_{\\lambda} - b\n\\;=\\; \\left( \\frac{\\sigma^{2}}{\\sigma^{2} + \\lambda^{2}} - 1 \\right) b\n\\;=\\; - \\frac{\\lambda^{2}}{\\sigma^{2} + \\lambda^{2}} \\, b.\n$$\nTherefore, the residual norm and solution norm are\n$$\n\\rho(\\lambda) \\;=\\; \\|r_{\\lambda}\\|_{2}\n\\;=\\; \\frac{\\lambda^{2}}{\\sigma^{2} + \\lambda^{2}} \\, \\|b\\|_{2},\n\\qquad\n\\eta(\\lambda) \\;=\\; \\|x_{\\lambda}\\|_{2}\n\\;=\\; \\frac{\\sigma}{\\sigma^{2} + \\lambda^{2}} \\, \\|b\\|_{2}.\n$$\nDefine the L-curve in log-log coordinates\n$$\ns(\\lambda) \\;=\\; \\ln \\rho(\\lambda)\n\\;=\\; \\ln \\|b\\|_{2} + \\ln \\lambda^{2} - \\ln (\\sigma^{2} + \\lambda^{2}),\n\\qquad\nt(\\lambda) \\;=\\; \\ln \\eta(\\lambda)\n\\;=\\; \\ln \\|b\\|_{2} + \\ln \\sigma - \\ln (\\sigma^{2} + \\lambda^{2}).\n$$\nNext, compute derivatives needed for the curvature. We have\n$$\ns'(\\lambda) \\;=\\; \\frac{2}{\\lambda} - \\frac{2\\lambda}{\\sigma^{2} + \\lambda^{2}}\n\\;=\\; \\frac{2 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})},\n$$\n$$\nt'(\\lambda) \\;=\\; - \\frac{2\\lambda}{\\sigma^{2} + \\lambda^{2}}.\n$$\nDifferentiating again gives\n$$\ns''(\\lambda) \\;=\\; - \\frac{2}{\\lambda^{2}} - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}},\n\\qquad\nt''(\\lambda) \\;=\\; - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}}.\n$$\nFor a planar parametric curve $(x(\\lambda), y(\\lambda))$, the curvature as a function of the parameter $\\lambda$ is\n$$\n\\kappa(\\lambda) \\;=\\;\n\\frac{\\left| x'(\\lambda) y''(\\lambda) - y'(\\lambda) x''(\\lambda) \\right|}\n{\\left( x'(\\lambda)^{2} + y'(\\lambda)^{2} \\right)^{3/2}}.\n$$\nApplying this to $x(\\lambda) = s(\\lambda)$ and $y(\\lambda) = t(\\lambda)$, we compute the numerator\n$$\nN(\\lambda) \\;=\\; s'(\\lambda) t''(\\lambda) - t'(\\lambda) s''(\\lambda).\n$$\nUsing the expressions above,\n\\begin{align*}\ns'(\\lambda) t''(\\lambda)\n&=\\; \\frac{2 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})} \\cdot \\left( - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}} \\right)\n\\;=\\; - \\frac{4 \\sigma^{2} (\\sigma^{2} - \\lambda^{2})}{\\lambda (\\sigma^{2} + \\lambda^{2})^{3}}, \\\\\nt'(\\lambda) s''(\\lambda)\n&=\\; \\left( - \\frac{2 \\lambda}{\\sigma^{2} + \\lambda^{2}} \\right) \\left( - \\frac{2}{\\lambda^{2}} - \\frac{2(\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{2}} \\right) \\\\\n&=\\; \\frac{4}{\\lambda (\\sigma^{2} + \\lambda^{2})} + \\frac{4 \\lambda (\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{3}}.\n\\end{align*}\nSubtracting yields\n\\begin{align*}\nN(\\lambda)\n&=\\; - \\frac{4 \\sigma^{2} (\\sigma^{2} - \\lambda^{2})}{\\lambda (\\sigma^{2} + \\lambda^{2})^{3}}\n\\;-\\; \\frac{4}{\\lambda (\\sigma^{2} + \\lambda^{2})}\n\\;-\\; \\frac{4 \\lambda (\\sigma^{2} - \\lambda^{2})}{(\\sigma^{2} + \\lambda^{2})^{3}} \\\\\n&=\\; - \\frac{8 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})^{2}}.\n\\end{align*}\nHence $\\left|N(\\lambda)\\right| = \\dfrac{8 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})^{2}}$.\n\nThe denominator is\n\\begin{align*}\nD(\\lambda)\n&=\\; \\left( s'(\\lambda)^{2} + t'(\\lambda)^{2} \\right)^{3/2}\n\\;=\\; \\left( \\frac{4 \\sigma^{4}}{\\lambda^{2} (\\sigma^{2} + \\lambda^{2})^{2}} + \\frac{4 \\lambda^{2}}{(\\sigma^{2} + \\lambda^{2})^{2}} \\right)^{3/2} \\\\\n&=\\; \\left( \\frac{4}{(\\sigma^{2} + \\lambda^{2})^{2}} \\left( \\frac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right) \\right)^{3/2}\n\\;=\\; \\frac{8}{(\\sigma^{2} + \\lambda^{2})^{3}} \\left( \\frac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right)^{3/2}.\n\\end{align*}\nTherefore, the curvature is\n\\begin{align*}\n\\kappa(\\lambda)\n&=\\; \\frac{\\left|N(\\lambda)\\right|}{D(\\lambda)}\n\\;=\\; \\frac{\\dfrac{8 \\sigma^{2}}{\\lambda (\\sigma^{2} + \\lambda^{2})^{2}}}{\\dfrac{8}{(\\sigma^{2} + \\lambda^{2})^{3}} \\left( \\dfrac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right)^{3/2}} \\\\\n&=\\; \\frac{\\sigma^{2} (\\sigma^{2} + \\lambda^{2})}{\\lambda \\left( \\dfrac{\\sigma^{4}}{\\lambda^{2}} + \\lambda^{2} \\right)^{3/2}}\n\\;=\\; \\frac{\\sigma^{2} (\\sigma^{2} + \\lambda^{2}) \\lambda^{2}}{(\\sigma^{4} + \\lambda^{4})^{3/2}}.\n\\end{align*}\nIntroduce the dimensionless variable $y := \\lambda^{2} / \\sigma^{2}$ (with $y > 0$). Then\n$$\n\\kappa(\\lambda) \\;=\\; \\frac{y(1 + y)}{(1 + y^{2})^{3/2}}.\n$$\nTo maximize $\\kappa(\\lambda)$ over $\\lambda > 0$, we maximize $g(y) := \\dfrac{y(1 + y)}{(1 + y^{2})^{3/2}}$ over $y > 0$. Differentiate,\n\\begin{align*}\ng'(y)\n&=\\; (1 + 2 y) (1 + y^{2})^{-3/2} - 3 y (y + y^{2}) (1 + y^{2})^{-5/2} \\\\\n&=\\; (1 + y^{2})^{-5/2} \\left[ (1 + 2 y)(1 + y^{2}) - 3 y (y + y^{2}) \\right] \\\\\n&=\\; (1 + y^{2})^{-5/2} \\left( 1 + 2 y - 2 y^{2} - y^{3} \\right).\n\\end{align*}\nSetting $g'(y) = 0$ yields the cubic equation\n$$\n1 + 2 y - 2 y^{2} - y^{3} \\;=\\; 0,\n$$\nequivalently\n$$\ny^{3} + 2 y^{2} - 2 y - 1 \\;=\\; 0.\n$$\nOne root is $y = 1$, and the remaining roots of $y^{2} + 3 y + 1 = 0$ are negative. Since $y > 0$, the unique critical point is $y = 1$. As $y \\to 0^{+}$, $g(y) \\sim y \\to 0$, and as $y \\to \\infty$, $g(y) \\sim y^{2}/y^{3} = 1/y \\to 0$, so $y = 1$ is the global maximizer. Therefore,\n$$\n\\lambda_{\\star}^{2} / \\sigma^{2} \\;=\\; 1\n\\quad\\Longrightarrow\\quad\n\\lambda_{\\star} \\;=\\; \\sigma,\n\\qquad\n\\kappa_{\\max} \\;=\\; g(1) \\;=\\; \\frac{2}{(1 + 1)^{3/2}} \\;=\\; \\frac{1}{\\sqrt{2}}.\n$$\nThe curvature $\\kappa(\\lambda)$ is bounded by $\\dfrac{1}{\\sqrt{2}}$ and attains its maximum at $\\lambda = \\sigma$, but this maximum is modest and broad rather than sharply peaked. Consequently, the L-curve does not exhibit a pronounced corner in this flat-singular-value setting, leading to ambiguous selection of $\\lambda$ by the L-curve criterion: there is no sharply distinguished parameter value that stands out geometrically.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\sigma & \\frac{1}{\\sqrt{2}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "While idealized problems often yield a single, well-defined L-curve corner, practical applications can produce more complex shapes, including multiple apparent corners that create ambiguity in parameter selection. This exercise delves into the spectral conditions that give rise to such phenomena, linking them to a structured distribution of singular values and a data vector that excites multiple spectral bands . By exploring this common challenge, you will learn to diagnose the cause of these multiple corners and apply other principled criteria, such as the discrepancy principle or effective degrees of freedom, to disambiguate the choice and select a physically meaningful regularization parameter.",
            "id": "3554620",
            "problem": "Consider a linear inverse problem in numerical linear algebra with a compact operator $A \\in \\mathbb{R}^{m \\times n}$ and observed data $b \\in \\mathbb{R}^{m}$. One seeks a stable solution $x \\in \\mathbb{R}^{n}$ via Tikhonov regularization with identity prior, defined as the minimizer of $\\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2$ for a regularization parameter $\\lambda > 0$. The L-curve is the parametric curve in the plane tracing $(\\log \\lVert A x_\\lambda - b \\rVert_2, \\log \\lVert x_\\lambda \\rVert_2)$ as $\\lambda$ varies. It is well-known that, for many ill-posed problems, the L-curve exhibits a single prominent corner where the trade-off between solution norm and residual norm transitions from under- to over-regularization. However, in some problems, multiple apparent corners may be observed, complicating parameter selection.\n\nWhich of the following statements correctly identify spectral/data conditions under which multiple apparent corners can arise on the L-curve, and propose a principled criterion to disambiguate among them?\n\nA. If the singular values $\\{\\sigma_i\\}$ of $A$ cluster into two well-separated bands and the data coefficients $\\{|u_i^\\top b|\\}$ have significant energy in both bands (where $A = U \\Sigma V^\\top$ is the singular value decomposition), then each cluster can induce its own transition in the filter factors as $\\lambda$ sweeps, producing two prominent bends on the L-curve. A principled disambiguation is to impose Morozov’s discrepancy principle, selecting the corner for which $\\lVert A x_\\lambda - b \\rVert_2$ matches the known noise level and the corresponding filter factors leave the large-$\\sigma$ band largely unattenuated while suppressing the small-$\\sigma$ band.\n\nB. Multiple corners are an artifact of plotting $\\log$ in base $10$ rather than natural logarithm; if one always uses the natural logarithm, the L-curve has at most one corner. The disambiguation is to replot with natural logarithm and take the unique corner.\n\nC. When $\\{\\sigma_i\\}$ follow a smooth power-law decay and $b$ is white noise, the L-curve generically has multiple corners. A robust disambiguation is to choose the corner with the smallest curvature to avoid overfitting.\n\nD. If $A$ is rank-deficient with repeated singular values and $b$ aligns with one repeated singular subspace, the L-curve must exhibit multiple corners. A principled choice is to set $\\lambda$ equal to the geometric mean of two adjacent singular values in the repeated block.\n\nE. If the right-hand side exhibits a multimodal Picard plot, meaning $\\{|u_i^\\top b|\\}$ has two dominant groups aligned with two separated spectral bands in $\\{\\sigma_i\\}$, and $A$ is severely ill-posed so that $\\{\\sigma_i\\}$ decay rapidly with a gap between the bands, then as $\\lambda$ passes through each band, the effective number of parameters changes abruptly, yielding multiple apparent corners. A principled disambiguation is to select the corner that coincides with the estimated Picard break index $k_*$, for which the noise begins to dominate, equivalently choosing $\\lambda$ such that the effective degrees of freedom $\\mathrm{df}(\\lambda) = \\sum_{i} \\sigma_i^2 / (\\sigma_i^2 + \\lambda^2)$ is approximately $k_*$.\n\nSelect all correct options.",
            "solution": "The problem asks for conditions under which the L-curve for Tikhonov regularization exhibits multiple corners and for principled methods to disambiguate them. The L-curve method is used to select a regularization parameter $\\lambda$ for the problem of minimizing $\\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2$. The solution to this minimization problem, denoted $x_\\lambda$, can be expressed using the Singular Value Decomposition (SVD) of the matrix $A$.\n\nLet the SVD of $A$ be $A = U \\Sigma V^\\top$, where $U = [u_1, u_2, \\dots, u_m]$ and $V = [v_1, v_2, \\dots, v_n]$ are orthogonal matrices, and $\\Sigma$ is a diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, with $r = \\text{rank}(A)$. The Tikhonov-regularized solution is given by:\n$$ x_\\lambda = \\sum_{i=1}^{r} \\phi_i(\\lambda) \\frac{u_i^\\top b}{\\sigma_i} v_i $$\nwhere $\\phi_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$ are the filter factors. These factors determine the degree to which each singular component is included in the solution. For a given component $i$, if $\\lambda \\ll \\sigma_i$, then $\\phi_i(\\lambda) \\approx 1$, and the component is preserved. If $\\lambda \\gg \\sigma_i$, then $\\phi_i(\\lambda) \\approx 0$, and the component is filtered out. The transition occurs around $\\lambda \\approx \\sigma_i$.\n\nThe L-curve is a log-log plot of the solution norm $\\lVert x_\\lambda \\rVert_2$ versus the residual norm $\\lVert A x_\\lambda - b \\rVert_2$. These norms can also be expressed using the SVD:\n$$ \\lVert x_\\lambda \\rVert_2^2 = \\sum_{i=1}^{r} \\left( \\frac{\\sigma_i (u_i^\\top b)}{\\sigma_i^2 + \\lambda^2} \\right)^2 = \\sum_{i=1}^{r} \\left( \\phi_i(\\lambda) \\frac{u_i^\\top b}{\\sigma_i} \\right)^2 $$\n$$ \\lVert A x_\\lambda - b \\rVert_2^2 = \\sum_{i=1}^{r} \\left( (\\phi_i(\\lambda)-1) (u_i^\\top b) \\right)^2 + \\sum_{i=r+1}^{m} (u_i^\\top b)^2 = \\sum_{i=1}^{r} \\left( \\frac{\\lambda^2 (u_i^\\top b)}{\\sigma_i^2 + \\lambda^2} \\right)^2 + \\sum_{i=r+1}^{m} (u_i^\\top b)^2 $$\n\nA \"corner\" on the L-curve corresponds to a value of $\\lambda$ where the character of the solution changes significantly, representing a transition in the trade-off. This occurs when $\\lambda$ sweeps across a singular value $\\sigma_i$. If the singular values $\\{\\sigma_i\\}$ are clustered into distinct, well-separated groups, and the data vector $b$ has significant energy projected onto the singular vectors $\\{u_i\\}$ corresponding to more than one of these groups, then the L-curve can exhibit multiple corners. As $\\lambda$ decreases, it sequentially crosses these clusters of singular values, causing abrupt changes in the solution and residual norms at each transition, each of which can manifest as a bend or corner on the log-log plot.\n\nLet's evaluate each option based on this principle.\n\n**A. If the singular values $\\{\\sigma_i\\}$ of $A$ cluster into two well-separated bands and the data coefficients $\\{|u_i^\\top b|\\}$ have significant energy in both bands (where $A = U \\Sigma V^\\top$ is the singular value decomposition), then each cluster can induce its own transition in the filter factors as $\\lambda$ sweeps, producing two prominent bends on the L-curve. A principled disambiguation is to impose Morozov’s discrepancy principle, selecting the corner for which $\\lVert A x_\\lambda - b \\rVert_2$ matches the known noise level and the corresponding filter factors leave the large-$\\sigma$ band largely unattenuated while suppressing the small-$\\sigma$ band.**\n\nThis statement accurately describes the primary mechanism for the formation of multiple corners. The condition of having separated clusters of singular values, combined with data that excites components in each cluster, is the classic scenario for this phenomenon. The proposed disambiguation method, Morozov's discrepancy principle, is a standard and principled approach in regularization theory. It requires an estimate of the noise level $\\delta$ in the data $b$ and selects $\\lambda$ such that the residual norm $\\lVert A x_\\lambda - b \\rVert_2$ is approximately equal to $\\delta$. This provides a clear, objective criterion for choosing among the possible $\\lambda$ values associated with the corners. The description of the filter factors' behavior for a good choice of $\\lambda$ is also correct: it should separate the \"signal\" (associated with large $\\sigma_i$) from the \"noise\" (associated with small $\\sigma_i$).\n\nVerdict: **Correct**.\n\n**B. Multiple corners are an artifact of plotting $\\log$ in base $10$ rather than natural logarithm; if one always uses the natural logarithm, the L-curve has at most one corner. The disambiguation is to replot with natural logarithm and take the unique corner.**\n\nThis statement is mathematically incorrect. The relationship between logarithms of different bases is a simple scaling factor: $\\log_{10}(z) = \\frac{\\ln(z)}{\\ln(10)}$. Plotting the L-curve using base-$10$ logarithms versus natural logarithms amounts to a uniform scaling of the coordinate axes. A uniform scaling of a parametric curve $(x(t), y(t))$ to $(cx(t), cy(t))$ scales its curvature by a factor of $1/c$ but does not change the parameter values $t$ where the curvature is maximized or minimized. Therefore, the number and location of the corners are independent of the choice of logarithm base.\n\nVerdict: **Incorrect**.\n\n**C. When $\\{\\sigma_i\\}$ follow a smooth power-law decay and $b$ is white noise, the L-curve generically has multiple corners. A robust disambiguation is to choose the corner with the smallest curvature to avoid overfitting.**\n\nThis statement is incorrect on two counts. First, a smooth decay of singular values, even for a \"difficult\" right-hand side like white noise, typically leads to a classic L-curve with a single, well-defined corner. Multiple corners are associated with *gaps* or *discontinuities* in the spectrum of singular values, not a smooth decay. Second, the L-curve criterion is precisely to select the point of *maximum* curvature, as this point represents the best compromise in the trade-off. Choosing the point of minimum curvature would correspond to the flat, non-informative parts of the curve, representing extreme under- or over-regularization. This is the antithesis of the method's purpose.\n\nVerdict: **Incorrect**.\n\n**D. If $A$ is rank-deficient with repeated singular values and $b$ aligns with one repeated singular subspace, the L-curve must exhibit multiple corners. A principled choice is to set $\\lambda$ equal to the geometric mean of two adjacent singular values in the repeated block.**\n\nThe premise of this statement is flawed. If a singular value $\\sigma_k$ is repeated (e.g., $\\sigma_k = \\sigma_{k+1}$), the corresponding filter factors are identical: $\\phi_k(\\lambda) = \\phi_{k+1}(\\lambda)$. These components are attenuated in exactly the same way as $\\lambda$ varies. The presence of a repeated singular value does not introduce a new transition scale for $\\lambda$ to cross; it is the *separation* between distinct singular values or clusters of them that causes multiple bends. Therefore, repeated singular values do not, in themselves, cause multiple corners. The proposed disambiguation is also moot since the geometric mean of two identical values is just the value itself.\n\nVerdict: **Incorrect**.\n\n**E. If the right-hand side exhibits a multimodal Picard plot, meaning $\\{|u_i^\\top b|\\}$ has two dominant groups aligned with two separated spectral bands in $\\{\\sigma_i\\}$, and $A$ is severely ill-posed so that $\\{\\sigma_i\\}$ decay rapidly with a gap between the bands, then as $\\lambda$ passes through each band, the effective number of parameters changes abruptly, yielding multiple apparent corners. A principled disambiguation is to select the corner that coincides with the estimated Picard break index $k_*$, for which the noise begins to dominate, equivalently choosing $\\lambda$ such that the effective degrees of freedom $\\mathrm{df}(\\lambda) = \\sum_{i} \\sigma_i^2 / (\\sigma_i^2 + \\lambda^2)$ is approximately $k_*$.**\n\nThis statement provides a detailed and accurate description of the phenomenon, consistent with the analysis for option A. The term \"multimodal Picard plot\" is an excellent way to characterize the data condition where the coefficients $|u_i^\\top b|$ have distinct bumps corresponding to separated spectral bands of $A$. The explanation that the effective number of parameters, or degrees of freedom $\\mathrm{df}(\\lambda) = \\sum_i \\phi_i(\\lambda)$, changes abruptly at each band is a precise and insightful physical interpretation. The proposed disambiguation method is also principled. It involves estimating the dimension of the \"signal\" subspace, denoted $k_*$, which is the number of coefficients that satisfy the discrete Picard condition before noise dominates. Then, $\\lambda$ is chosen to make the effective degrees of freedom of the solution match this dimension, i.e., $\\mathrm{df}(\\lambda) \\approx k_*$. This is a well-established statistical criterion, related to methods like GCV, for choosing the regularization parameter, especially when the noise level is unknown.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}