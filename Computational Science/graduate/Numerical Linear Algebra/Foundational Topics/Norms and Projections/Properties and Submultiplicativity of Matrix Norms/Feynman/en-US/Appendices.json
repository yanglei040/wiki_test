{
    "hands_on_practices": [
        {
            "introduction": "To see the power of matrix norms in a practical setting, we will begin by analyzing the stability of an iterative algorithm. This exercise asks you to implement the Jacobi method for solving a linear system and investigate how a small error in your initial guess propagates through the iterations. By applying the properties of matrix norms, specifically submultiplicativity, you will derive and verify a quantitative bound on this error growth, connecting abstract theory to concrete computational outcomes .",
            "id": "3272411",
            "problem": "You are given a square linear system with a real, nonsingular coefficient matrix and asked to quantify how sensitive a Jacobi iterative approximation after $k$ steps is to perturbations in the initial guess. Your task is to start from the definition of the Jacobi method as an iterative procedure that isolates the diagonal of the coefficient matrix and, at each step, solves a diagonal system after moving off-diagonal contributions to the right-hand side. From this base, reason about how the difference between two iterates produced from two different initial guesses evolves in time. In particular, reason from the properties of linear operators on finite-dimensional vector spaces and the submultiplicativity of induced norms to obtain an explicit expression for how a perturbation in the initial guess propagates through $k$ steps. Then implement a program that uses this understanding to quantitatively evaluate the sensitivity in specific test cases.\n\nFor each test case, do the following tasks:\n- Construct the Jacobi iteration by isolating the diagonal of the coefficient matrix $A$ and using only that diagonal to solve at each step. You must represent the iteration as an affine mapping of the form $x^{(k+1)} = T(x^{(k)})$ and identify a constant vector that depends on $A$ and $b$ only.\n- Using the fundamental linearity of the update with respect to $x^{(k)}$ and the fixed-point interpretation of the iteration, derive, from first principles, an explicit formula that associates a perturbation in the initial guess $e^{(0)}$ with the perturbation after $k$ steps $e^{(k)}$. Your derivation must start from the iteration definition and properties of linear mappings and induced matrix norms; do not assume any specialized shortcut identities beyond these bases.\n- For a given $k$, compute the following four quantities:\n  1. The amplification factor $r = \\lVert e^{(k)} \\rVert_{2} / \\lVert e^{(0)} \\rVert_{2}$, with the convention $r = 0$ if $\\lVert e^{(0)} \\rVert_{2} = 0$.\n  2. An induced $2$-norm upper bound on the amplification, given by $\\lVert B \\rVert_{2}^{k}$, where $B$ is the linear operator that maps $x^{(k)}$ to the linear part of $x^{(k+1)}$. Use the principal (spectral) norm induced by the Euclidean norm, and set the bound to $1$ when $k=0$.\n  3. The spectral radius $\\rho(B)$.\n  4. A boolean indicator of the linear convergence condition based on the spectral radius, namely whether $\\rho(B)  1$.\n- Implement the Jacobi iteration numerically to evaluate $x^{(k)}$ starting from $x^{(0)}$, and also starting from $x^{(0)} + e^{(0)}$, and take the difference to obtain $e^{(k)}$. Use the Euclidean norm for vectors and the induced $2$-norm for matrices. For the induced $2$-norm of a matrix, use the largest singular value. For the spectral radius, use the largest magnitude of the eigenvalues.\n\nUse the following test suite. Each test case is specified by $(A,b,x^{(0)},e^{(0)},k)$.\n- Test case $1$:\n  - $A = \\begin{bmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n    $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n    $e^{(0)} = \\begin{bmatrix} 10^{-3} \\\\ -2 \\cdot 10^{-3} \\\\ 10^{-3} \\end{bmatrix}$,\n    $k = 10$.\n- Test case $2$:\n  - $A = \\begin{bmatrix} 2  1 \\\\ 1  2 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n    $x^{(0)} = \\begin{bmatrix} \\tfrac{1}{2} \\\\ -\\tfrac{1}{2} \\end{bmatrix}$,\n    $e^{(0)} = \\begin{bmatrix} 10^{-6} \\\\ -10^{-6} \\end{bmatrix}$,\n    $k = 25$.\n- Test case $3$:\n  - $A = \\begin{bmatrix} 1  0.99 \\\\ 0.99  1 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$,\n    $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $e^{(0)} = \\begin{bmatrix} 10^{-4} \\\\ 0 \\end{bmatrix}$,\n    $k = 50$.\n- Test case $4$:\n  - $A = \\begin{bmatrix} 1  1.2 \\\\ 1.2  1 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$,\n    $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $e^{(0)} = \\begin{bmatrix} 10^{-6} \\\\ 2 \\cdot 10^{-6} \\end{bmatrix}$,\n    $k = 5$.\n- Test case $5$ (boundary step count $k=0$):\n  - $A = \\begin{bmatrix} 3  -1  0 \\\\ -1  3  -1 \\\\ 0  -1  3 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $x^{(0)} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\\\ 0.05 \\end{bmatrix}$,\n    $e^{(0)} = \\begin{bmatrix} 2 \\cdot 10^{-3} \\\\ -10^{-3} \\\\ 3 \\cdot 10^{-3} \\end{bmatrix}$,\n    $k = 0$.\n- Test case $6$ (zero perturbation $e^{(0)}$):\n  - $A = \\begin{bmatrix} 4  -1 \\\\ -2  5 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$,\n    $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $e^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n    $k = 7$.\n\nFinal output specification:\n- For each test case, your program must produce a list with the four entries $[r,\\ \\lVert B \\rVert_{2}^{k},\\ \\rho(B),\\ \\text{conv}]$, where $\\text{conv}$ is a boolean equal to $\\text{True}$ if and only if $\\rho(B)  1$.\n- All floating-point outputs must be rounded to $8$ decimal places.\n- The program must print a single line consisting of a list of these per-case lists, in order from test case $1$ to test case $6$, with no other text. For example, the printed line must look like\n  $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$,\n  where each $\\cdot$ is a decimal number with exactly $8$ digits after the decimal point or a boolean $\\text{True}/\\text{False}$.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of iterative methods for linear systems, a core topic in numerical analysis. The problem is well-posed, providing all necessary data $(A, b, x^{(0)}, e^{(0)}, k)$ for each test case. The matrices $A$ are confirmed to be nonsingular and to have non-zero diagonal elements, ensuring the Jacobi iteration is well-defined. The terminology is precise, and the objectives are quantifiable and unambiguous.\n\n### 1. Derivation of the Jacobi Iteration\n\nThe problem concerns a square linear system of equations, which can be written in matrix form as:\n$$\nA \\mathbf{x} = \\mathbf{b}\n$$\nwhere $A$ is a real, nonsingular $n \\times n$ coefficient matrix, $\\mathbf{x}$ is the $n \\times 1$ vector of unknowns, and $\\mathbf{b}$ is the $n \\times 1$ right-hand side vector.\n\nThe Jacobi method is an iterative algorithm for solving such systems. The core idea is to decompose the matrix $A$ into the sum of its diagonal part, $D$, its strictly lower-triangular part, $L$, and its strictly upper-triangular part, $U$.\n$$\nA = D + L + U\n$$\nSubstituting this decomposition into the original equation gives:\n$$\n(D + L + U) \\mathbf{x} = \\mathbf{b}\n$$\nThe Jacobi method is derived by rearranging this equation to isolate the term with the diagonal matrix $D$ on the left-hand side, and using this to define an iterative update rule. Let $\\mathbf{x}^{(k)}$ be the approximation of the solution $\\mathbf{x}$ at iteration $k$. The next approximation, $\\mathbf{x}^{(k+1)}$, is computed by solving the following system:\n$$\nD \\mathbf{x}^{(k+1)} = - (L + U) \\mathbf{x}^{(k)} + \\mathbf{b}\n$$\nThis is a diagonal system, which is trivial to solve, provided that $D$ is invertible. The matrix $D$ is invertible if and only if all diagonal entries of $A$ are non-zero, a condition which holds for all test cases provided. We can therefore pre-multiply by the inverse of $D$, $D^{-1}$, to obtain the explicit iteration formula:\n$$\n\\mathbf{x}^{(k+1)} = -D^{-1}(L + U) \\mathbf{x}^{(k)} + D^{-1}\\mathbf{b}\n$$\nThis equation is an affine mapping of the form $\\mathbf{x}^{(k+1)} = T(\\mathbf{x}^{(k)})$, where the transformation $T$ is given by $T(\\mathbf{z}) = B\\mathbf{z} + \\mathbf{c}$. The matrix $B$ and vector $\\mathbf{c}$ are constant and defined as:\n- The Jacobi iteration matrix: $B = -D^{-1}(L + U)$\n- The constant vector: $\\mathbf{c} = D^{-1}\\mathbf{b}$\n\nThus, the Jacobi iteration can be written compactly as:\n$$\n\\mathbf{x}^{(k+1)} = B\\mathbf{x}^{(k)} + \\mathbf{c}\n$$\n\n### 2. Propagation of Perturbations\n\nWe wish to analyze the sensitivity of the method to a perturbation in the initial guess. Let $\\mathbf{x}^{(0)}$ be an initial guess and let $\\tilde{\\mathbf{x}}^{(0)}$ be a perturbed initial guess, defined as:\n$$\n\\tilde{\\mathbf{x}}^{(0)} = \\mathbf{x}^{(0)} + \\mathbf{e}^{(0)}\n$$\nwhere $\\mathbf{e}^{(0)}$ is the initial perturbation vector.\n\nLet $\\{\\mathbf{x}^{(k)}\\}_{k=0}^{\\infty}$ be the sequence of iterates generated starting from $\\mathbf{x}^{(0)}$, and let $\\{\\tilde{\\mathbf{x}}^{(k)}\\}_{k=0}^{\\infty}$ be the sequence generated from $\\tilde{\\mathbf{x}}^{(0)}$. Both sequences follow the same Jacobi update rule:\n$$\n\\mathbf{x}^{(k+1)} = B\\mathbf{x}^{(k)} + \\mathbf{c}\n$$\n$$\n\\tilde{\\mathbf{x}}^{(k+1)} = B\\tilde{\\mathbf{x}}^{(k)} + \\mathbf{c}\n$$\nLet the perturbation at step $k$ be defined as the difference between the two iterates: $\\mathbf{e}^{(k)} = \\tilde{\\mathbf{x}}^{(k)} - \\mathbf{x}^{(k)}$. To find how this perturbation propagates, we subtract the first iterative equation from the second:\n$$\n\\tilde{\\mathbf{x}}^{(k+1)} - \\mathbf{x}^{(k+1)} = (B\\tilde{\\mathbf{x}}^{(k)} + \\mathbf{c}) - (B\\mathbf{x}^{(k)} + \\mathbf{c})\n$$\nThe constant vector $\\mathbf{c}$ cancels out.\n$$\n\\mathbf{e}^{(k+1)} = B\\tilde{\\mathbf{x}}^{(k)} - B\\mathbf{x}^{(k)}\n$$\nDue to the linearity of matrix-vector multiplication, we can factor out the matrix $B$:\n$$\n\\mathbf{e}^{(k+1)} = B(\\tilde{\\mathbf{x}}^{(k)} - \\mathbf{x}^{(k)})\n$$\nSubstituting the definition of $\\mathbf{e}^{(k)}$ gives the recursive relationship for the perturbation:\n$$\n\\mathbf{e}^{(k+1)} = B \\mathbf{e}^{(k)}\n$$\nBy unrolling this recursion back to the initial perturbation $\\mathbf{e}^{(0)}$, we obtain the explicit expression for the perturbation after $k$ steps:\n$$\n\\mathbf{e}^{(k)} = B \\mathbf{e}^{(k-1)} = B(B \\mathbf{e}^{(k-2)}) = \\cdots = B^k \\mathbf{e}^{(0)}\n$$\nThis fundamental result, $\\mathbf{e}^{(k)} = B^k \\mathbf{e}^{(0)}$, shows that the perturbation at step $k$ is obtained by applying the $k$-th power of the Jacobi iteration matrix to the initial perturbation.\n\n### 3. Sensitivity Analysis and an Upper Bound\n\nTo quantify the sensitivity, we examine the amplification factor $r$, defined as the ratio of the magnitude of the perturbation at step $k$ to the magnitude of the initial perturbation. Using the Euclidean vector norm ($\\|\\cdot\\|_2$), this is:\n$$\nr = \\frac{\\lVert \\mathbf{e}^{(k)} \\rVert_2}{\\lVert \\mathbf{e}^{(0)} \\rVert_2}\n$$\n(with the convention $r=0$ if $\\lVert \\mathbf{e}^{(0)} \\rVert_2 = 0$).\n\nUsing the formula $\\mathbf{e}^{(k)} = B^k \\mathbf{e}^{(0)}$, we can establish an upper bound on this factor. We take the $2$-norm of both sides:\n$$\n\\lVert \\mathbf{e}^{(k)} \\rVert_2 = \\lVert B^k \\mathbf{e}^{(0)} \\rVert_2\n$$\nBy the definition of an induced matrix norm, $\\lVert M\\mathbf{v} \\rVert \\le \\lVert M \\rVert \\lVert \\mathbf{v} \\rVert$. Applying this, we get:\n$$\n\\lVert \\mathbf{e}^{(k)} \\rVert_2 \\le \\lVert B^k \\rVert_2 \\lVert \\mathbf{e}^{(0)} \\rVert_2\n$$\nFurthermore, induced matrix norms are submultiplicative, meaning $\\lVert M_1 M_2 \\rVert \\le \\lVert M_1 \\rVert \\lVert M_2 \\rVert$. Applying this property $k-1$ times gives:\n$$\n\\lVert B^k \\rVert_2 = \\lVert B \\cdot B \\cdots B \\rVert_2 \\le \\lVert B \\rVert_2 \\cdot \\lVert B \\rVert_2 \\cdots \\lVert B \\rVert_2 = (\\lVert B \\rVert_2)^k\n$$\nCombining these inequalities, we arrive at an upper bound for the norm of the perturbation:\n$$\n\\lVert \\mathbf{e}^{(k)} \\rVert_2 \\le (\\lVert B \\rVert_2)^k \\lVert \\mathbf{e}^{(0)} \\rVert_2\n$$\nDividing by $\\lVert \\mathbf{e}^{(0)} \\rVert_2$ yields an upper bound on the amplification factor:\n$$\nr = \\frac{\\lVert \\mathbf{e}^{(k)} \\rVert_2}{\\lVert \\mathbf{e}^{(0)} \\rVert_2} \\le (\\lVert B \\rVert_2)^k\n$$\nThis inequality provides a worst-case bound on how much an initial perturbation can be amplified after $k$ iterations. The induced $2$-norm of $B$, $\\lVert B \\rVert_2$, is its largest singular value (the spectral norm). The condition for convergence of the Jacobi method for any initial guess is that the spectral radius of the iteration matrix, $\\rho(B)$, is less than $1$. The spectral radius is the largest absolute value of the eigenvalues of $B$.\n\n### 4. Numerical Implementation\n\nThe program will execute the following steps for each test case:\n1.  Construct the Jacobi iteration matrix $B = -D^{-1}(L+U)$ and the constant vector $\\mathbf{c} = D^{-1}\\mathbf{b}$ from the given matrix $A$ and vector $\\mathbf{b}$.\n2.  Simulate two parallel Jacobi iterations for $k$ steps: one starting from $\\mathbf{x}^{(0)}$ to get $\\mathbf{x}^{(k)}$, and another from $\\tilde{\\mathbf{x}}^{(0)} = \\mathbf{x}^{(0)} + \\mathbf{e}^{(0)}$ to get $\\tilde{\\mathbf{x}}^{(k)}$.\n3.  Calculate the final perturbation $\\mathbf{e}^{(k)} = \\tilde{\\mathbf{x}}^{(k)} - \\mathbf{x}^{(k)}$.\n4.  Compute the amplification factor $r = \\lVert \\mathbf{e}^{(k)} \\rVert_2 / \\lVert \\mathbf{e}^{(0)} \\rVert_2$, handling the case $\\mathbf{e}^{(0)} = \\mathbf{0}$.\n5.  Compute the theoretical upper bound $(\\lVert B \\rVert_2)^k$. The value of $\\lVert B \\rVert_2$ is found using `numpy.linalg.norm(B, 2)`. For $k=0$, the bound is $1$.\n6.  Compute the spectral radius $\\rho(B)$ by finding the maximum absolute value of the eigenvalues of $B$, obtained via `numpy.linalg.eigvals(B)`.\n7.  Determine the convergence indicator, which is `True` if $\\rho(B)  1$ and `False` otherwise.\n8.  Format the four resulting quantities into a list, with floating-point numbers rounded to $8$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Jacobi sensitivity problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[4, 1, 0], [1, 3, 1], [0, 1, 2]], dtype=float),\n            \"b\": np.array([1, 2, 3], dtype=float),\n            \"x0\": np.array([0, 0, 0], dtype=float),\n            \"e0\": np.array([1e-3, -2e-3, 1e-3], dtype=float),\n            \"k\": 10,\n        },\n        {\n            \"A\": np.array([[2, 1], [1, 2]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n            \"x0\": np.array([0.5, -0.5], dtype=float),\n            \"e0\": np.array([1e-6, -1e-6], dtype=float),\n            \"k\": 25,\n        },\n        {\n            \"A\": np.array([[1, 0.99], [0.99, 1]], dtype=float),\n            \"b\": np.array([2, -1], dtype=float),\n            \"x0\": np.array([0, 0], dtype=float),\n            \"e0\": np.array([1e-4, 0], dtype=float),\n            \"k\": 50,\n        },\n        {\n            \"A\": np.array([[1, 1.2], [1.2, 1]], dtype=float),\n            \"b\": np.array([1, 0], dtype=float),\n            \"x0\": np.array([0, 0], dtype=float),\n            \"e0\": np.array([1e-6, 2e-6], dtype=float),\n            \"k\": 5,\n        },\n        {\n            \"A\": np.array([[3, -1, 0], [-1, 3, -1], [0, -1, 3]], dtype=float),\n            \"b\": np.array([1, 1, 1], dtype=float),\n            \"x0\": np.array([0.1, -0.1, 0.05], dtype=float),\n            \"e0\": np.array([2e-3, -1e-3, 3e-3], dtype=float),\n            \"k\": 0,\n        },\n        {\n            \"A\": np.array([[4, -1], [-2, 5]], dtype=float),\n            \"b\": np.array([0, 1], dtype=float),\n            \"x0\": np.array([0, 0], dtype=float),\n            \"e0\": np.array([0, 0], dtype=float),\n            \"k\": 7,\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A, b, x0, e0, k = case[\"A\"], case[\"b\"], case[\"x0\"], case[\"e0\"], case[\"k\"]\n\n        # 1. Construct the Jacobi iteration matrix B and vector c.\n        D = np.diag(np.diag(A))\n        D_inv = np.diag(1 / np.diag(D))\n        L_plus_U = A - D\n        B = -D_inv @ L_plus_U\n        c = D_inv @ b\n\n        # 2. Simulate iterations to find e^{(k)}.\n        x_k = x0.copy()\n        xtilde_k = (x0 + e0).copy()\n        for _ in range(k):\n            x_k = B @ x_k + c\n            xtilde_k = B @ xtilde_k + c\n        e_k = xtilde_k - x_k\n\n        # 3. Compute amplification factor r.\n        norm_e0 = np.linalg.norm(e0, 2)\n        if norm_e0 == 0:\n            r = 0.0\n        else:\n            norm_ek = np.linalg.norm(e_k, 2)\n            r = norm_ek / norm_e0\n\n        # 4. Compute the upper bound on amplification.\n        if k == 0:\n            bound = 1.0\n        else:\n            norm_B_2 = np.linalg.norm(B, 2)\n            bound = norm_B_2**k\n\n        # 5. Compute the spectral radius rho(B).\n        eigenvalues = np.linalg.eigvals(B)\n        rho_B = np.max(np.abs(eigenvalues))\n\n        # 6. Determine convergence condition.\n        conv = rho_B  1.0\n\n        all_results.append([r, bound, rho_B, conv])\n\n    # Format the final output string exactly as specified.\n    formatted_results = []\n    for R in all_results:\n        r_str = f\"{R[0]:.8f}\"\n        bound_str = f\"{R[1]:.8f}\"\n        rho_str = f\"{R[2]:.8f}\"\n        conv_str = str(R[3])\n        formatted_results.append(f\"[{r_str},{bound_str},{rho_str},{conv_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The submultiplicativity property, $\\|AB\\|_{2} \\le \\|A\\|_{2}\\|B\\|_{2}$, gives us a powerful way to bound the amplification of signals or errors. This practice challenges you to explore the \"worst-case\" scenario for this inequality in the context of matrix inversion sensitivity. You will use the singular value decomposition (SVD) to construct a specific perturbation that maximizes its effect, thereby demonstrating how the bound can be achieved and revealing the fundamental role of the matrix condition number in determining system sensitivity .",
            "id": "3568440",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix. Consider the induced operator norm from the Euclidean norm (spectral norm) defined for any matrix $M$ by $\\|M\\|_{2} \\coloneqq \\sup_{\\|x\\|_{2}=1} \\|Mx\\|_{2}$. Let the singular value decomposition of $A$ be $A = U \\Sigma V^{\\top}$, with $U, V \\in \\mathbb{R}^{n \\times n}$ orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$. The spectral condition number is defined by $\\kappa_{2}(A) \\coloneqq \\|A\\|_{2} \\, \\|A^{-1}\\|_{2}$.\n\nUsing only the definitions above and the submultiplicativity of induced operator norms, construct a rank-one perturbation direction $E \\in \\mathbb{R}^{n \\times n}$ with the constraint $\\|E\\|_{2} = \\|A\\|_{2}$ that maximizes $\\|A^{-1} E\\|_{2}$ and thereby nearly achieves equality in the first-order inverse sensitivity bound $\\|A^{-1} \\delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\, \\|\\delta A\\|_{2}$ for small $\\delta A$. Express $E$ directly in terms of the singular vectors of $A$, and determine the resulting value of $\\|A^{-1} E\\|_{2}$ as a closed-form analytic expression in the singular values of $A$. Your final answer should be the expression for $\\|A^{-1} E\\|_{2}$; no rounding is required.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- $A \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix.\n- The induced operator norm (spectral norm) is defined as $\\|M\\|_{2} := \\sup_{\\|x\\|_{2}=1} \\|Mx\\|_{2}$ for any matrix $M$.\n- The Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$.\n- The singular values are ordered as $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$. The condition $\\sigma_{n}  0$ is consistent with $A$ being invertible.\n- The task is to construct a rank-one perturbation matrix $E \\in \\mathbb{R}^{n \\times n}$ subject to the constraint $\\|E\\|_{2} = \\|A\\|_{2}$.\n- The objective is to maximize the quantity $\\|A^{-1} E\\|_{2}$.\n- The final answer must be the maximized value of $\\|A^{-1} E\\|_{2}$ expressed in terms of the singular values of $A$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is located squarely within numerical linear algebra. All definitions—SVD, spectral norm, condition number—are standard and correctly stated. The inequality $\\|A^{-1} \\delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\, \\|\\delta A\\|_{2}$ is a direct result of the submultiplicativity of induced operator norms, which is a fundamental property.\n- **Well-Posedness**: The problem asks to maximize a continuous function, $\\|A^{-1} E\\|_{2}$, over a compact set of matrices (the set of rank-one matrices $E$ with a fixed spectral norm). A solution is guaranteed to exist. The objective and constraints are clear and unambiguous.\n- **Objectivity**: The problem is stated in precise mathematical language without any subjectivity.\n- **Conclusion**: The problem is valid as it is self-contained, scientifically sound, and well-posed.\n\n### Step 3: Proceed with Solution\n\nOur goal is to find a rank-one matrix $E$ that maximizes $\\|A^{-1} E\\|_{2}$ under the constraint $\\|E\\|_{2} = \\|A\\|_{2}$.\n\nFirst, let us express the properties of $A$ using its SVD, $A = U \\Sigma V^{\\top}$. The spectral norm of a matrix is equal to its largest singular value. Therefore, $\\|A\\|_{2} = \\sigma_{1}$. The constraint on $E$ is thus $\\|E\\|_{2} = \\sigma_{1}$.\n\nThe inverse of $A$ is given by $A^{-1} = (U \\Sigma V^{\\top})^{-1} = (V^{\\top})^{-1} \\Sigma^{-1} U^{-1} = V \\Sigma^{-1} U^{\\top}$, since $U$ and $V$ are orthogonal matrices ($U^{-1} = U^{\\top}$, $V^{-1} = V^{\\top}$). The singular values of $A^{-1}$ are the reciprocals of the singular values of $A$. Given the ordering $\\sigma_{1} \\geq \\dots \\geq \\sigma_{n}  0$, the singular values of $A^{-1}$ are $1/\\sigma_{n} \\geq \\dots \\geq 1/\\sigma_{1}  0$. The spectral norm of $A^{-1}$ is its largest singular value, so $\\|A^{-1}\\|_{2} = 1/\\sigma_{n}$.\n\nThe problem requires $E$ to be a rank-one matrix. Any rank-one matrix can be expressed as an outer product of two vectors, $E = uv^{\\top}$, for some non-zero vectors $u, v \\in \\mathbb{R}^{n}$. The spectral norm of such a matrix is given by $\\|E\\|_{2} = \\|uv^{\\top}\\|_{2} = \\|u\\|_{2}\\|v\\|_{2}$. The constraint on $E$ becomes $\\|u\\|_{2}\\|v\\|_{2} = \\sigma_{1}$.\n\nNow, we analyze the objective function $\\|A^{-1} E\\|_{2}$. Substituting $E = uv^{\\top}$, we have:\n$$\n\\|A^{-1} E\\|_{2} = \\|A^{-1} (uv^{\\top})\\|_{2} = \\|(A^{-1}u)v^{\\top}\\|_{2}\n$$\nThis is the norm of another rank-one matrix. Using the same property as before, we get:\n$$\n\\|(A^{-1}u)v^{\\top}\\|_{2} = \\|A^{-1}u\\|_{2} \\|v^{\\top}\\|_{2} = \\|A^{-1}u\\|_{2} \\|v\\|_{2}\n$$\nSo, our objective is to maximize $\\|A^{-1}u\\|_{2} \\|v\\|_{2}$ subject to the constraint $\\|u\\|_{2}\\|v\\|_{2} = \\sigma_{1}$. We can express $\\|v\\|_{2}$ from the constraint as $\\|v\\|_{2} = \\sigma_{1} / \\|u\\|_{2}$ (since $u$ is non-zero) and substitute it into the objective function:\n$$\n\\text{maximize } \\|A^{-1}u\\|_{2} \\left( \\frac{\\sigma_{1}}{\\|u\\|_{2}} \\right) = \\sigma_{1} \\frac{\\|A^{-1}u\\|_{2}}{\\|u\\|_{2}}\n$$\nThis reduces the problem to maximizing the Rayleigh quotient $\\frac{\\|A^{-1}u\\|_{2}}{\\|u\\|_{2}}$ over all non-zero vectors $u \\in \\mathbb{R}^{n}$. By the definition of an induced norm, the maximum value of this quotient is the norm of the operator itself:\n$$\n\\sup_{u \\neq 0} \\frac{\\|A^{-1}u\\|_{2}}{\\|u\\|_{2}} = \\|A^{-1}\\|_{2} = \\frac{1}{\\sigma_{n}}\n$$\nThis maximum is achieved when $u$ is a right singular vector of $A^{-1}$ corresponding to its largest singular value, $1/\\sigma_{n}$. The SVD of $A^{-1}$ is $V (\\Sigma^{-1}) U^{\\top}$. The left singular vectors of $A^{-1}$ are the columns of $V$, and the right singular vectors are the columns of $U$. The right singular vector corresponding to the singular value $1/\\sigma_{n}$ is the $n$-th column of $U$, which we denote by $u_{n}$.\n\nThus, to maximize our objective function, we must choose $u$ to be a multiple of $u_{n}$, i.e., $u = c u_{n}$ for some scalar $c \\neq 0$.\n\nHaving determined the optimal direction for $u$, we can now construct the matrix $E$. Let's select $u = u_{n}$. Then $\\|u\\|_{2} = \\|u_{n}\\|_{2} = 1$. The constraint $\\|u\\|_{2}\\|v\\|_{2} = \\sigma_{1}$ implies that $\\|v\\|_{2} = \\sigma_{1}$. The direction of $v$ does not affect the magnitude of $\\|A^{-1}E\\|_{2}$, so we have some freedom. A logical choice is to use a singular vector of $A$. Let us choose $v$ to be proportional to $v_{1}$, the right singular vector of $A$ corresponding to its largest singular value $\\sigma_{1}$. We set $v = \\sigma_{1} v_{1}$. This choice satisfies the norm constraint: $\\|v\\|_{2} = \\|\\sigma_{1} v_{1}\\|_{2} = \\sigma_{1}\\|v_{1}\\|_{2} = \\sigma_{1} \\cdot 1 = \\sigma_{1}$.\n\nWith these choices, our rank-one perturbation matrix is:\n$$\nE = uv^{\\top} = u_{n} (\\sigma_{1} v_{1})^{\\top} = \\sigma_{1} u_{n} v_{1}^{\\top}\n$$\nLet us verify the norm of this $E$:\n$$\n\\|E\\|_{2} = \\|\\sigma_{1} u_{n} v_{1}^{\\top}\\|_{2} = \\sigma_{1} \\|u_{n}\\|_{2} \\|v_{1}\\|_{2} = \\sigma_{1} \\cdot 1 \\cdot 1 = \\sigma_{1} = \\|A\\|_{2}\n$$\nThe constraint is satisfied. Now we compute the value of $\\|A^{-1} E\\|_{2}$ for this specific $E$:\n$$\nA^{-1} E = (V \\Sigma^{-1} U^{\\top}) (\\sigma_{1} u_{n} v_{1}^{\\top}) = \\sigma_{1} (V \\Sigma^{-1} U^{\\top} u_{n}) v_{1}^{\\top}\n$$\nSince $u_{n}$ is the $n$-th column of $U$, $U^{\\top}u_{n}$ is the $n$-th standard basis vector, $e_{n}$.\n$$\nA^{-1} E = \\sigma_{1} (V \\Sigma^{-1} e_{n}) v_{1}^{\\top}\n$$\nThe product $\\Sigma^{-1}e_{n}$ results in a vector with $1/\\sigma_{n}$ in the $n$-th position and zeros elsewhere.\n$$\nA^{-1} E = \\sigma_{1} V \\left( \\frac{1}{\\sigma_{n}} e_{n} \\right) v_{1}^{\\top} = \\frac{\\sigma_{1}}{\\sigma_{n}} (V e_{n}) v_{1}^{\\top}\n$$\nThe vector $Ve_{n}$ is the $n$-th column of $V$, which is denoted by $v_{n}$.\n$$\nA^{-1} E = \\frac{\\sigma_{1}}{\\sigma_{n}} v_{n} v_{1}^{\\top}\n$$\nThe norm of this resulting matrix is:\n$$\n\\|A^{-1} E\\|_{2} = \\left\\| \\frac{\\sigma_{1}}{\\sigma_{n}} v_{n} v_{1}^{\\top} \\right\\|_{2} = \\frac{\\sigma_{1}}{\\sigma_{n}} \\|v_{n} v_{1}^{\\top}\\|_{2} = \\frac{\\sigma_{1}}{\\sigma_{n}} \\|v_{n}\\|_{2} \\|v_{1}\\|_{2}\n$$\nSince $v_{1}$ and $v_{n}$ are columns of the orthogonal matrix $V$, they are unit vectors, so $\\|v_{1}\\|_{2} = 1$ and $\\|v_{n}\\|_{2} = 1$. Therefore, the maximized value is:\n$$\n\\|A^{-1} E\\|_{2} = \\frac{\\sigma_{1}}{\\sigma_{n}}\n$$\nThis value is precisely the spectral condition number of $A$, $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = \\sigma_{1} (1/\\sigma_{n})$. This shows that our constructed perturbation $E$ saturates the submultiplicativity inequality $\\|A^{-1} E\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|E\\|_{2}$, achieving equality.",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{1}}{\\sigma_{n}}}\n$$"
        },
        {
            "introduction": "Having seen that the submultiplicative bound can be sharp, we now investigate the underlying mechanism that governs the outcome of a matrix product. This exercise guides you through a thought experiment comparing an \"aligned\" matrix product with a \"misaligned\" one, revealing why the norm of the product can either equal the product of the norms or be significantly smaller. This exploration illuminates the geometric role of singular vectors in matrix multiplication and introduces the crucial concept of pseudoresonance, which explains transient growth in complex systems .",
            "id": "3568447",
            "problem": "Consider the induced Euclidean matrix norm (also called the spectral norm), defined for any real matrix $A \\in \\mathbb{R}^{n \\times n}$ by $\\|A\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|A x\\|_{2}$, and recall the submultiplicativity property $\\|A B\\|_{2} \\le \\|A\\|_{2}\\,\\|B\\|_{2}$ and the singular value decomposition (SVD) representation $A = U \\Sigma V^{\\top}$ with $U, V \\in \\mathbb{R}^{n \\times n}$ orthogonal and $\\Sigma$ diagonal with nonnegative diagonal entries (the singular values). Let $n=2$ and let $u, w, v, z, y, t \\in \\mathbb{R}^{2}$ be unit vectors such that $U=[u\\; w]$, $V=[v\\; z]$, and $W=[y\\; t]$ are orthogonal matrices. For a parameter $\\varepsilon \\in (0,1)$, define\n$$\nA_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}, \\qquad B_{\\varepsilon} = V \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}.\n$$\nThus $A_{\\varepsilon}$ and $B_{\\varepsilon}$ have singular values $1$ and $\\varepsilon$ and are near-singular for small $\\varepsilon$ (their condition numbers are $1/\\varepsilon$). Consider also the misaligned product formed with $V' = [z\\; v]$ (which swaps the columns of $V$),\n$$\nB'_{\\varepsilon} = V' \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}.\n$$\nAnswer the following multiple-choice question, selecting all options that are correct. Your reasoning should build from the core definitions above, without assuming any unstated properties.\n\nWhich of the following statements about $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2}$, $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2}$, and the mechanisms behind near-equality in $\\|A B\\|_{2} \\le \\|A\\|_{2} \\|B\\|_{2}$ are correct?\n\nA. In the induced Euclidean norm, if the dominant left singular vector of $B$ equals the dominant right singular vector of $A$, then $\\|A B\\|_{2} = \\|A\\|_{2}\\, \\|B\\|_{2}$.\n\nB. For the aligned factors $A_{\\varepsilon}$ and $B_{\\varepsilon}$ defined above, one has $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2} = \\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2}$ for all $\\varepsilon \\in (0,1)$, even though both $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are near-singular (with condition number $1/\\varepsilon$).\n\nC. For the misaligned factor $B'_{\\varepsilon}$, one has $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} = \\varepsilon$, so the ratio $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} / (\\|A_{\\varepsilon}\\|_{2}\\, \\|B'_{\\varepsilon}\\|_{2})$ equals $\\varepsilon$.\n\nD. For any unitarily invariant norm (including the spectral norm), equality $\\|A B\\| = \\|A\\|\\, \\|B\\|$ cannot hold unless both $A$ and $B$ are normal matrices.\n\nE. The aligned case above exemplifies an amplification mechanism in non-normal products often termed pseudoresonance: near-resonant alignment of left and right singular vectors yields near-equality in the submultiplicative bound and, consequently, transient growth comparable to the product of the individual amplifications, despite each factor being near-singular.\n\nSelect all correct options.",
            "solution": "The problem statement will first be validated for scientific soundness, consistency, and well-posedness.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Norm Definition**: The induced Euclidean matrix norm (spectral norm) for $A \\in \\mathbb{R}^{n \\times n}$ is $\\|A\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|A x\\|_{2}$.\n-   **Property**: Submultiplicativity, $\\|A B\\|_{2} \\le \\|A\\|_{2}\\,\\|B\\|_{2}$.\n-   **Matrix Representation**: Singular Value Decomposition (SVD) is $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal and $\\Sigma$ is a diagonal matrix with non-negative diagonal entries (singular values).\n-   **Parameters**: The dimension is $n=2$. The parameter $\\varepsilon$ is in the interval $(0,1)$.\n-   **Vector and Matrix Definitions**:\n    -   $u, w, v, z, y, t \\in \\mathbb{R}^{2}$ are unit vectors.\n    -   $U=[u\\; w]$, $V=[v\\; z]$, and $W=[y\\; t]$ are orthogonal matrices.\n    -   $A_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}$.\n    -   $B_{\\varepsilon} = V \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}$.\n    -   The problem notes that the singular values of $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are $1$ and $\\varepsilon$, and their condition numbers are $1/\\varepsilon$.\n    -   $V' = [z\\; v]$.\n    -   $B'_{\\varepsilon} = V' \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}$.\n-   **Question**: Evaluate the correctness of five statements concerning $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2}$, $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2}$, and the conditions for near-equality in the submultiplicativity inequality.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness**: The problem is situated in numerical linear algebra. All definitions—spectral norm, SVD, orthogonal matrices, condition number—are standard and mathematically correct. The construction of the matrices $A_{\\varepsilon}$, $B_{\\varepsilon}$, and $B'_{\\varepsilon}$ is a common and valid method for illustrating properties of matrix products. The problem is scientifically sound.\n-   **Well-Posedness**: The question requires the calculation of specific matrix norms and the evaluation of theoretical statements. The setup provides all necessary information to perform these tasks, leading to a unique and meaningful answer.\n-   **Objectivity**: The problem is stated in precise mathematical language, free from subjective or ambiguous terms.\n-   **Completeness and Consistency**: The definitions are self-contained. The fact that $U=[u\\; w]$, $V=[v\\; z]$, and $W=[y\\; t]$ are orthogonal matrices implies that their respective column vectors are orthonormal, which is consistent with the initial statement that they are unit vectors. The construction of $V'=[z\\; v]$ from $V=[v\\; z]$ correctly yields another orthogonal matrix, as it is just a column permutation of $V$. There are no internal contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. The analysis may proceed.\n\n### Solution Derivation\n\nFirst, we establish the spectral norms of the given matrices. The spectral norm, $\\|A\\|_2$, is equal to the largest singular value of $A$, $\\sigma_{\\max}(A)$. The singular values are invariant under multiplication by orthogonal matrices.\n\nFor $A_{\\varepsilon} = U \\Sigma_{\\varepsilon} V^{\\top}$, where $\\Sigma_{\\varepsilon} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}$, the singular values are $1$ and $\\varepsilon$. Since $\\varepsilon \\in (0,1)$, the largest singular value is $1$.\nThus, $\\|A_{\\varepsilon}\\|_2 = 1$.\n\nFor $B_{\\varepsilon} = V \\Sigma_{\\varepsilon} W^{\\top}$, the singular values are also $1$ and $\\varepsilon$.\nThus, $\\|B_{\\varepsilon}\\|_2 = 1$.\n\nFor $B'_{\\varepsilon} = V' \\Sigma_{\\varepsilon} W^{\\top}$, since $V'$ is also an orthogonal matrix, the singular values are again $1$ and $\\varepsilon$.\nThus, $\\|B'_{\\varepsilon}\\|_2 = 1$.\n\nNow, we evaluate each option.\n\n**A. In the induced Euclidean norm, if the dominant left singular vector of $B$ equals the dominant right singular vector of $A$, then $\\|A B\\|_{2} = \\|A\\|_{2}\\, \\|B\\|_{2}$.**\n\nLet the SVD of $A$ be $A = U_A \\Sigma_A V_A^{\\top}$ and of $B$ be $B = U_B \\Sigma_B V_B^{\\top}$. The singular values are ordered such that $\\sigma_1(A) \\ge \\sigma_2(A) \\ge \\dots$ and $\\sigma_1(B) \\ge \\sigma_2(B) \\ge \\dots$. The norms are $\\|A\\|_2 = \\sigma_1(A)$ and $\\|B\\|_2 = \\sigma_1(B)$.\n\nThe dominant right singular vector of $A$ is the first column of $V_A$, denoted $v_{A1}$.\nThe dominant left singular vector of $B$ is the first column of $U_B$, denoted $u_{B1}$.\nThe hypothesis is $v_{A1} = u_{B1}$.\n\nLet's consider the action of $AB$ on the specific unit vector $x = v_{B1}$, which is the dominant right singular vector of $B$.\nFirst, $Bx = B v_{B1} = (U_B \\Sigma_B V_B^{\\top}) v_{B1} = U_B \\Sigma_B e_1 = U_B (\\sigma_1(B)e_1) = \\sigma_1(B)u_{B1} = \\|B\\|_2 u_{B1}$.\nNext, we compute $A(Bx)$:\n$A(Bx) = A(\\|B\\|_2 u_{B1}) = \\|B\\|_2 A u_{B1}$.\nUsing the hypothesis $u_{B1} = v_{A1}$:\n$A u_{B1} = A v_{A1} = (U_A \\Sigma_A V_A^{\\top}) v_{A1} = U_A \\Sigma_A e_1 = U_A (\\sigma_1(A)e_1) = \\sigma_1(A) u_{A1} = \\|A\\|_2 u_{A1}$.\nSubstituting this back, we get:\n$A(Bx) = \\|B\\|_2 (\\|A\\|_2 u_{A1}) = \\|A\\|_2 \\|B\\|_2 u_{A1}$.\n\nNow, we take the norm:\n$\\|ABv_{B1}\\|_2 = \\| \\|A\\|_2 \\|B\\|_2 u_{A1} \\|_2 = \\|A\\|_2 \\|B\\|_2 \\|u_{A1}\\|_2$.\nSince $u_{A1}$ is a column of an orthogonal matrix, it is a unit vector, so $\\|u_{A1}\\|_2 = 1$.\nTherefore, for the specific unit vector $x=v_{B1}$, we have $\\|ABx\\|_2 = \\|A\\|_2 \\|B\\|_2$.\n\nThe definition of the matrix norm is $\\|AB\\|_2 = \\max_{\\|x\\|_2=1} \\|ABx\\|_2$. Since we have found a vector for which the norm $\\|ABx\\|_2$ reaches the value $\\|A\\|_2 \\|B\\|_2$, we must have $\\|AB\\|_2 \\ge \\|A\\|_2 \\|B\\|_2$.\nCombined with the submultiplicativity property, $\\|AB\\|_2 \\le \\|A\\|_2 \\|B\\|_2$, this proves that $\\|AB\\|_2 = \\|A\\|_2 \\|B\\|_2$.\n\nVerdict: **Correct**.\n\n**B. For the aligned factors $A_{\\varepsilon}$ and $B_{\\varepsilon}$ defined above, one has $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2} = \\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2}$ for all $\\varepsilon \\in (0,1)$, even though both $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are near-singular (with condition number $1/\\varepsilon$).**\n\nLet's compute the product $A_{\\varepsilon} B_{\\varepsilon}$:\n$A_{\\varepsilon} B_{\\varepsilon} = \\left(U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}\\right) \\left(V \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}\\right)$.\nSince $V$ is an orthogonal matrix, $V^{\\top}V = I$, where $I$ is the identity matrix.\n$A_{\\varepsilon} B_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} I \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top} = U \\left(\\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}\\right)^2 W^{\\top}$.\nThe squared diagonal matrix is:\n$\\left(\\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}\\right)^2 = \\begin{pmatrix} 1^2  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix}$.\nSo, $A_{\\varepsilon} B_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix} W^{\\top}$.\nThis expression is the SVD of $A_{\\varepsilon} B_{\\varepsilon}$. The singular values are $1$ and $\\varepsilon^2$.\nThe spectral norm is the largest singular value: $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_2 = 1$.\nThe product of the individual norms is $\\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2} = 1 \\cdot 1 = 1$.\nThus, $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2} = \\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2}$ holds.\nThe statement correctly notes that the matrices are near-singular, with condition number $\\kappa_2(A_\\varepsilon) = \\sigma_{\\max}/\\sigma_{\\min} = 1/\\varepsilon$. This is a factual observation.\nThis case is a specific example of the principle in option A. The dominant right singular vector of $A_{\\varepsilon}$ is the first column of $V$, which is $v$. The dominant left singular vector of $B_{\\varepsilon}$ is the first column of $V$, which is also $v$. They are identical.\n\nVerdict: **Correct**.\n\n**C. For the misaligned factor $B'_{\\varepsilon}$, one has $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} = \\varepsilon$, so the ratio $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} / (\\|A_{\\varepsilon}\\|_{2}\\, \\|B'_{\\varepsilon}\\|_{2})$ equals $\\varepsilon$.**\n\nLet's compute the product $A_{\\varepsilon} B'_{\\varepsilon}$:\n$A_{\\varepsilon} B'_{\\varepsilon} = \\left(U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}\\right) \\left(V' \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}\\right)$.\nWe need to evaluate the matrix product $V^{\\top}V'$.\n$V = [v\\; z]$, so $V^{\\top} = \\begin{pmatrix} v^{\\top} \\\\ z^{\\top} \\end{pmatrix}$. $V' = [z\\; v]$.\n$V^{\\top}V' = \\begin{pmatrix} v^{\\top} \\\\ z^{\\top} \\end{pmatrix} [z\\; v] = \\begin{pmatrix} v^{\\top}z  v^{\\top}v \\\\ z^{\\top}z  z^{\\top}v \\end{pmatrix}$.\nSince $V$ is orthogonal, its columns $\\{v, z\\}$ are orthonormal. Thus, $v^{\\top}v=1$, $z^{\\top}z=1$, and $v^{\\top}z=z^{\\top}v=0$.\n$V^{\\top}V' = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$, which is a permutation matrix. Let's call it $P$.\nThe product becomes $A_{\\varepsilon} B'_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} P \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}$.\nLet's compute the central matrix:\n$\\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ \\varepsilon  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix}$.\nLet $M = \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix}$. The product is $A_{\\varepsilon} B'_{\\varepsilon} = U M W^{\\top}$.\nThe norm is $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_2 = \\|U M W^{\\top}\\|_2 = \\|M\\|_2$ due to the unitary invariance of the norm.\nThe singular values of $M$ are the square roots of the eigenvalues of $M^{\\top}M$.\n$M^{\\top}M = \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix} \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix} = \\begin{pmatrix} \\varepsilon^2  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix}$.\nThe eigenvalues of $M^{\\top}M$ are $\\lambda_1 = \\varepsilon^2$ and $\\lambda_2 = \\varepsilon^2$.\nThe singular values of $M$ are $\\sigma_1 = \\sqrt{\\varepsilon^2} = \\varepsilon$ and $\\sigma_2 = \\sqrt{\\varepsilon^2} = \\varepsilon$.\nThe largest singular value is $\\varepsilon$. So, $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_2 = \\|M\\|_2 = \\varepsilon$.\nThe first part of the statement is correct.\nNow for the ratio:\n$\\frac{\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2}}{\\|A_{\\varepsilon}\\|_{2}\\, \\|B'_{\\varepsilon}\\|_{2}} = \\frac{\\varepsilon}{1 \\cdot 1} = \\varepsilon$.\nThe entire statement is correct.\n\nVerdict: **Correct**.\n\n**D. For any unitarily invariant norm (including the spectral norm), equality $\\|A B\\| = \\|A\\|\\, \\|B\\|$ cannot hold unless both $A$ and $B$ are normal matrices.**\n\nA matrix $X$ is normal if $X X^{*} = X^{*} X$. For real matrices, this is $X X^{\\top} = X^{\\top} X$.\nThis statement is a strong claim that can be disproven with a counterexample.\nLet's examine the matrices from option B, for which we proved $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_2 = \\|A_{\\varepsilon}\\|_2 \\|B_{\\varepsilon}\\|_2$. Let's check if $A_{\\varepsilon}$ is normal.\n$A_{\\varepsilon} = U \\Sigma_{\\varepsilon} V^{\\top}$ with $\\Sigma_{\\varepsilon} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}$.\n$A_{\\varepsilon}^{\\top} = V \\Sigma_{\\varepsilon} U^{\\top}$.\n$A_{\\varepsilon} A_{\\varepsilon}^{\\top} = (U \\Sigma_{\\varepsilon} V^{\\top})(V \\Sigma_{\\varepsilon} U^{\\top}) = U \\Sigma_{\\varepsilon}^2 U^{\\top}$.\n$A_{\\varepsilon}^{\\top} A_{\\varepsilon} = (V \\Sigma_{\\varepsilon} U^{\\top})(U \\Sigma_{\\varepsilon} V^{\\top}) = V \\Sigma_{\\varepsilon}^2 V^{\\top}$.\nFor $A_{\\varepsilon}$ to be normal, we must have $A_{\\varepsilon} A_{\\varepsilon}^{\\top} = A_{\\varepsilon}^{\\top} A_{\\varepsilon}$, which implies $U \\Sigma_{\\varepsilon}^2 U^{\\top} = V \\Sigma_{\\varepsilon}^2 V^{\\top}$.\nSince $\\varepsilon \\in (0,1)$, $\\Sigma_{\\varepsilon}^2 = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix}$ is not a multiple of the identity matrix. The matrix $U \\Sigma_{\\varepsilon}^2 U^{\\top}$ has eigenspaces spanned by the columns of $U$, while $V \\Sigma_{\\varepsilon}^2 V^{\\top}$ has eigenspaces spanned by the columns of $V$. If $U \\ne V$ (and $V$ is not trivially related to $U$ by column swaps or sign changes), these matrices will not be equal. The problem does not state any relation between $U$ and $V$ other than that they are orthogonal, so we cannot assume they are equal.\nTherefore, $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are, in general, not normal matrices. Yet, they satisfy the equality condition. This serves as a direct counterexample to the statement.\n\nVerdict: **Incorrect**.\n\n**E. The aligned case above exemplifies an amplification mechanism in non-normal products often termed pseudoresonance: near-resonant alignment of left and right singular vectors yields near-equality in the submultiplicative bound and, consequently, transient growth comparable to the product of the individual amplifications, despite each factor being near-singular.**\n\nThis statement provides a conceptual interpretation of the preceding calculations. Let's analyze its components.\n-   \"The aligned case above exemplifies an amplification mechanism in non-normal products...\": As shown in D, $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are generally non-normal. The \"amplification\" refers to achieving the maximum possible norm for the product, $\\|A\\|_2 \\|B\\|_2$, as opposed to the cancellation seen in the misaligned case (option C), where the norm was a much smaller value, $\\varepsilon$.\n-   \"...often termed pseudoresonance:\": This is the standard technical term in the study of non-normal matrices for the phenomenon where the output of one operator strongly excites the input of a subsequent operator, leading to a large overall response.\n-   \"...near-resonant alignment of left and right singular vectors...\": In the case of $A_{\\varepsilon}$ and $B_{\\varepsilon}$, the alignment is perfect (resonant), not just near-resonant. The dominant right singular vector of $A_{\\varepsilon}$ ($v$) is identical to the dominant left singular vector of $B_{\\varepsilon}$ ($v$). This is the mechanism for the multiplicativity of the norm, as shown in A.\n-   \"...yields near-equality in the submultiplicative bound...\": In our case, it yields exact equality, which is the limit of near-equality.\n-   \"...and, consequently, transient growth comparable to the product of the individual amplifications...\": The \"individual amplifications\" are the norms $\\|A_{\\varepsilon}\\|_2=1$ and $\\|B_{\\varepsilon}\\|_2=1$. The product is $1$. The norm of the product, $\\|A_{\\varepsilon}B_{\\varepsilon}\\|_2=1$, is equal to this product. The term \"transient growth\" describes the behavior of products or powers of matrices where the norm can be large even if the long-term behavior (governed by the spectral radius) is decay. This example is a two-step illustration of the mechanism causing such growth.\n-   \"...despite each factor being near-singular.\": This is true, the condition number is $1/\\varepsilon$, which is large for small $\\varepsilon$. This highlights that while each matrix strongly contracts vectors in certain directions, the resonant alignment ensures that these contractive directions are avoided.\n\nThe statement is a physically and conceptually accurate description of the mathematical phenomenon demonstrated in options A, B, and C, using established terminology from the field.\n\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ABCE}$$"
        }
    ]
}