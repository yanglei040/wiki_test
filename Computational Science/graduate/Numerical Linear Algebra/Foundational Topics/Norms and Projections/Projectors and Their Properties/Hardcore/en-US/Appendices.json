{
    "hands_on_practices": [
        {
            "introduction": "A fundamental task in many applications is to find the common part of two different models or datasets, a task that mathematically translates to projecting onto the intersection of two subspaces. This practice  contrasts a direct, geometric construction based on the identity $(\\mathcal{U} \\cap \\mathcal{V})^\\perp = \\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ with the celebrated Dykstra's alternating projection algorithm. By implementing both, you will gain practical insight into the design and comparison of direct versus iterative numerical methods for a core linear algebra problem.",
            "id": "3567687",
            "problem": "You are given two linear subspaces $\\mathcal{U} \\subset \\mathbb{R}^n$ and $\\mathcal{V} \\subset \\mathbb{R}^n$, each specified by a matrix of basis vectors as columns. Your task is to design a program that, for each test case below, constructs the orthogonal projector onto the intersection $\\mathcal{U} \\cap \\mathcal{V}$ by reasoning from first principles, and compares it to a projector obtained via an iterative scheme based on Dykstra’s algorithm for alternating projections.\n\nFundamental definitions and facts that you may use as the starting point:\n- A matrix with orthonormal columns $Q \\in \\mathbb{R}^{n \\times k}$ spans a subspace $\\mathcal{S} = \\operatorname{range}(Q) \\subset \\mathbb{R}^n$. The orthogonal projector onto $\\mathcal{S}$ is the linear map $P_{\\mathcal{S}} : \\mathbb{R}^n \\to \\mathbb{R}^n$ satisfying $P_{\\mathcal{S}}^2 = P_{\\mathcal{S}}$ and $P_{\\mathcal{S}}^\\top = P_{\\mathcal{S}}$, and $P_{\\mathcal{S}} x$ is the unique closest point to $x$ in $\\mathcal{S}$ with respect to the Euclidean norm. \n- For a subspace $\\mathcal{S} \\subset \\mathbb{R}^n$, the orthogonal complement is $\\mathcal{S}^\\perp = \\{ x \\in \\mathbb{R}^n : x^\\top y = 0 \\ \\forall y \\in \\mathcal{S} \\}$. The direct-sum relation $(\\mathcal{U} \\cap \\mathcal{V})^\\perp = \\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ holds for all subspaces $\\mathcal{U}, \\mathcal{V} \\subset \\mathbb{R}^n$. \n- The Moore–Penrose pseudoinverse is a linear-algebraic operator that generalizes inversion to rank-deficient matrices and can be defined via Singular Value Decomposition (SVD). It yields the orthogonal projector onto a column space when multiplied as $A A^{+}$ for any matrix $A$.\n\nYour program must:\n1. From the provided basis matrices for $\\mathcal{U}$ and $\\mathcal{V}$, construct numerically stable orthonormal bases for $\\mathcal{U}$ and $\\mathcal{V}$ using a robust procedure grounded in the Singular Value Decomposition (SVD), discarding directions with singular values below a user-chosen tolerance.\n2. Construct orthonormal bases for $\\mathcal{U}^\\perp$ and $\\mathcal{V}^\\perp$ (the orthogonal complements), for example by computing the null spaces of $U^\\top$ and $V^\\top$ using a method based on the Singular Value Decomposition.\n3. Using only the fundamental facts above (do not assume any closed-form formula not derivable from these), derive and implement a numerically stable expression for the orthogonal projector onto $\\mathcal{U} \\cap \\mathcal{V}$ in terms of the orthogonal projector onto $\\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ and the Moore–Penrose pseudoinverse.\n4. Implement the two-set Dykstra alternating projections procedure to project an arbitrary vector $x \\in \\mathbb{R}^n$ onto $\\mathcal{U} \\cap \\mathcal{V}$ using only the orthogonal projectors onto $\\mathcal{U}$ and onto $\\mathcal{V}$. To compare a linear operator against the closed-form projector, apply the iterative projection to each standard basis vector to assemble an approximate projector matrix column-by-column. Use a stopping tolerance of $10^{-12}$ in Euclidean norm with a maximum of $2000$ iterations.\n5. For each test case, compute:\n   - The Frobenius norm of the difference between the closed-form projector and the Dykstra-assembled projector, which should be a nonnegative real number.\n   - The dimension of $\\mathcal{U} \\cap \\mathcal{V}$, computed as an integer using an appropriate numerically stable method grounded in projector properties.\n   - The Frobenius-norm idempotence residual of the closed-form projector, namely $\\lVert P^2 - P \\rVert_F$, which should be a nonnegative real number.\n\nTest suite:\nProvide results for the following five test cases. In each, the columns of the matrices are basis vectors for the subspaces; convert them to orthonormal bases before forming projectors.\n\n- Case $1$ ($n = 5$): \n  - $\\mathcal{U}$ is spanned by the columns of $U_1 = [u_{1,1}\\ u_{1,2}]$ with $u_{1,1} = [1,1,0,0,0]^\\top$ and $u_{1,2} = [0,0,1,0,0]^\\top$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_1 = [v_{1,1}\\ v_{1,2}\\ v_{1,3}]$ with $v_{1,1} = [1,0,0,0,0]^\\top$, $v_{1,2} = [0,1,1,0,0]^\\top$, and $v_{1,3} = [0,0,0,1,0]^\\top$.\n\n- Case $2$ ($n = 4$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_2 = [e_1\\ e_2]$ where $e_i$ denotes the $i$-th column of the identity in $\\mathbb{R}^4$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_2 = [e_1\\ e_2\\ e_3]$ in $\\mathbb{R}^4$.\n\n- Case $3$ ($n = 3$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_3 = [e_1]$ in $\\mathbb{R}^3$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_3 = [e_2]$ in $\\mathbb{R}^3$.\n\n- Case $4$ ($n = 3$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_4 = [e_1\\ e_2\\ e_3]$ in $\\mathbb{R}^3$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_4 = [e_1\\ e_2\\ e_3]$ in $\\mathbb{R}^3$.\n\n- Case $5$ ($n = 3$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_5 = [e_1\\ e_2]$ in $\\mathbb{R}^3$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_5 = [w_1\\ w_2]$ with $w_1 = [1,\\varepsilon,0]^\\top$ and $w_2 = [0,1,0]^\\top$ for $\\varepsilon = 10^{-8}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of sublists, one per test case, in the order of cases $1$ through $5$. For each case, output the sublist $[e, d, i]$ where $e$ is the Frobenius norm difference between the two projector constructions (a float), $d$ is the computed dimension of $\\mathcal{U} \\cap \\mathcal{V}$ (an integer), and $i$ is the Frobenius-norm idempotence residual of the closed-form projector (a float). For example, a valid output shape is $[[e_1,d_1,i_1],[e_2,d_2,i_2],\\dots,[e_5,d_5,i_5]]$ with numeric values in place of symbols. No additional text should be printed.",
            "solution": "The objective is to construct and compare two representations of the orthogonal projector onto the intersection of two linear subspaces, $\\mathcal{U} \\cap \\mathcal{V} \\subset \\mathbb{R}^n$. The first method is a closed-form expression derived from fundamental principles of linear algebra. The second is an iterative operator constructed using Dykstra's alternating projection algorithm.\n\nLet the two subspaces be $\\mathcal{U}$ and $\\mathcal{V}$, specified by matrices $U_{basis} \\in \\mathbb{R}^{n \\times k_U}$ and $V_{basis} \\in \\mathbb{R}^{n \\times k_V}$ whose columns form bases for $\\mathcal{U}$ and $\\mathcal{V}$ respectively. The solution procedure is as follows:\n\n**1. Numerically Stable Orthonormal Basis Construction**\n\nA robust method for finding an orthonormal basis for a subspace $\\mathcal{S} = \\operatorname{range}(A)$ from a matrix of spanning vectors $A \\in \\mathbb{R}^{n \\times k}$ is the Singular Value Decomposition (SVD). The SVD of $A$ is given by $A = S \\Sigma V^\\top$, where $S \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{k \\times k}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times k}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values.\n\nThe rank $r$ of the matrix $A$ is the number of its singular values that are greater than a small tolerance $\\tau > 0$. The first $r$ columns of $S$ form an orthonormal basis for the column space of $A$, i.e., for $\\mathcal{S}$. Let this basis matrix be $Q_\\mathcal{S} = S[:, 1:r]$. The remaining $n-r$ columns of $S$, denoted $Q_{\\mathcal{S}^\\perp} = S[:, (r+1):n]$, form an orthonormal basis for the orthogonal complement of the column space, $\\mathcal{S}^\\perp$, which is equivalent to the null space of $A^\\top$.\n\nThis procedure is applied to both $U_{basis}$ and $V_{basis}$ to obtain orthonormal bases $Q_\\mathcal{U}$ for $\\mathcal{U}$ and $Q_\\mathcal{V}$ for $\\mathcal{V}$, as well as bases $Q_{\\mathcal{U}^\\perp}$ for $\\mathcal{U}^\\perp$ and $Q_{\\mathcal{V}^\\perp}$ for $\\mathcal{V}^\\perp$. For numerical stability and reproducibility, a tolerance $\\tau = 10^{-12}$ is used.\n\n**2. Closed-Form Projector for the Intersection $\\mathcal{U} \\cap \\mathcal{V}$**\n\nThe derivation of the orthogonal projector onto $\\mathcal{U} \\cap \\mathcal{V}$, denoted $P_{\\mathcal{U} \\cap \\mathcal{V}}$, relies on the following fundamental properties:\n- For any subspace $\\mathcal{S}$, the projector onto its orthogonal complement is $P_{\\mathcal{S}^\\perp} = I - P_\\mathcal{S}$, where $I$ is the identity matrix.\n- De Morgan's law for subspaces states that $(\\mathcal{U} \\cap \\mathcal{V})^\\perp = \\mathcal{U}^\\perp + \\mathcal{V}^\\perp$.\n\nFrom the first property, the projector onto the intersection $\\mathcal{W} = \\mathcal{U} \\cap \\mathcal{V}$ can be expressed in terms of the projector onto its complement $\\mathcal{W}^\\perp$:\n$$P_{\\mathcal{U} \\cap \\mathcal{V}} = I - P_{(\\mathcal{U} \\cap \\mathcal{V})^\\perp}$$\nUsing the second property, we can substitute the expression for the complement:\n$$P_{\\mathcal{U} \\cap \\mathcal{V}} = I - P_{\\mathcal{U}^\\perp + \\mathcal{V}^\\perp}$$\nThe sum of two subspaces $\\mathcal{S}_1 + \\mathcal{S}_2$ is the space spanned by the union of their bases. If $Q_1$ and $Q_2$ are matrices whose columns form orthonormal bases for $\\mathcal{S}_1$ and $\\mathcal{S}_2$ respectively, then $\\mathcal{S}_1 + \\mathcal{S}_2 = \\operatorname{range}([Q_1, Q_2])$. Let $C = [Q_{\\mathcal{U}^\\perp}, Q_{\\mathcal{V}^\\perp}]$ be the matrix formed by concatenating the basis vectors for $\\mathcal{U}^\\perp$ and $\\mathcal{V}^\\perp$.\nThe orthogonal projector onto the range of any matrix $A$ is given by $A A^{+}$, where $A^{+}$ is the Moore-Penrose pseudoinverse of $A$. Therefore, the projector onto $\\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ is:\n$$P_{\\mathcal{U}^\\perp + \\mathcal{V}^\\perp} = C C^{+}$$\nCombining these results yields the final closed-form expression for the projector onto the intersection:\n$$P_{\\mathcal{U} \\cap \\mathcal{V}} = I - [Q_{\\mathcal{U}^\\perp}, Q_{\\mathcal{V}^\\perp}] [Q_{\\mathcal{U}^\\perp}, Q_{\\mathcal{V}^\\perp}]^{+}$$\nThis constitutes the primary, or \"closed-form\", projector, which we denote $P_{CF}$.\n\n**3. Iterative Projector via Dykstra's Algorithm**\n\nDykstra's algorithm is an iterative method for finding the projection of a vector onto the intersection of two or more convex sets. Since subspaces are convex, it can be applied to find $P_{\\mathcal{U} \\cap \\mathcal{V}}x$ for any vector $x \\in \\mathbb{R}^n$. For two subspaces $\\mathcal{U}$ and $\\mathcal{V}$ with projectors $P_\\mathcal{U}$ and $P_\\mathcal{V}$, the algorithm to project a point $x$ is:\nInitialize: $y_0 = x$, and correction terms $p_0 = \\mathbf{0}$, $q_0 = \\mathbf{0}$.\nFor $k=1, 2, \\dots$:\n1. $x_k = P_\\mathcal{U}(y_{k-1} + p_{k-1})$\n2. $p_k = (y_{k-1} + p_{k-1}) - x_k$\n3. $y_k = P_\\mathcal{V}(x_k + q_{k-1})$\n4. $q_k = (x_k + q_{k-1}) - y_k$\nThe sequence $y_k$ converges to $P_{\\mathcal{U} \\cap \\mathcal{V}}x$. Iteration proceeds until the change $\\lVert y_k - y_{k-1} \\rVert_2$ falls below a tolerance of $10^{-12}$, or a maximum of $2000$ iterations is reached.\n\nTo construct the full projector matrix, $P_{Dykstra}$, this procedure is applied to each of the $n$ standard basis vectors $e_i \\in \\mathbb{R}^n$. The resulting projected vector forms the $i$-th column of $P_{Dykstra}$:\n$$P_{Dykstra} = [ \\operatorname{Dykstra}(e_1), \\operatorname{Dykstra}(e_2), \\dots, \\operatorname{Dykstra}(e_n) ]$$\nwhere $\\operatorname{Dykstra}(x)$ denotes the output of the algorithm for input $x$.\n\n**4. Evaluation Metrics**\n\nFor each test case, three quantities are computed to evaluate and compare the projectors:\n1.  **Frobenius Norm Difference ($e$)**: This measures the discrepancy between the two constructed projectors, $e = \\lVert P_{CF} - P_{Dykstra} \\rVert_F$. This value should be close to zero, reflecting the convergence of Dykstra's algorithm.\n2.  **Dimension of Intersection ($d$)**: For an orthogonal projector $P$, its rank equals its trace, $\\operatorname{rank}(P) = \\operatorname{tr}(P)$, which gives the dimension of the subspace it projects onto. Thus, the dimension of the intersection is calculated as $d = \\operatorname{dim}(\\mathcal{U} \\cap \\mathcal{V}) = \\operatorname{tr}(P_{CF})$. Since numerical computations are involved, the result is rounded to the nearest integer.\n3.  **Idempotence Residual ($i$)**: A matrix $P$ is a projector if and only if it is idempotent, i.e., $P^2 = P$. The Frobenius norm of the residual, $i = \\lVert P_{CF}^2 - P_{CF} \\rVert_F$, measures how closely the computed closed-form projector satisfies this fundamental property. This value should be near machine precision.",
            "answer": "```python\nimport numpy as np\n\ndef get_orthonormal_bases(A, tol):\n    \"\"\"\n    Computes orthonormal bases for the column space of A and its orthogonal complement.\n    Uses SVD for numerical stability.\n    \"\"\"\n    if A.size == 0 or A.shape[1] == 0:\n        return np.empty((A.shape[0], 0)), np.identity(A.shape[0])\n\n    U, s, _ = np.linalg.svd(A, full_matrices=True)\n    rank = np.sum(s > tol)\n    \n    Q = U[:, :rank]\n    Q_perp = U[:, rank:]\n    return Q, Q_perp\n\ndef get_intersection_projector(Q_U_perp, Q_V_perp, n, tol):\n    \"\"\"\n    Constructs the orthogonal projector onto the intersection U_cap_V\n    using the formula P = I - P_{U_perp + V_perp}.\n    \"\"\"\n    # Concatenate the bases for the orthogonal complements\n    if Q_U_perp.shape[1] == 0 and Q_V_perp.shape[1] == 0:\n        C = np.empty((n, 0))\n    elif Q_U_perp.shape[1] == 0:\n        C = Q_V_perp\n    elif Q_V_perp.shape[1] == 0:\n        C = Q_U_perp\n    else:\n        C = np.hstack((Q_U_perp, Q_V_perp))\n\n    # Projector onto the sum of the complements\n    if C.shape[1] == 0:\n        # Sum of complements is the {0} subspace, projector is zero matrix\n        P_sum_perp = np.zeros((n, n))\n    else:\n        # P = A A^+, numerically stable via pinv\n        P_sum_perp = C @ np.linalg.pinv(C, rcond=tol)\n    \n    # P_intersection = I - P_sum_perp\n    P_CF = np.identity(n) - P_sum_perp\n    return P_CF\n\ndef dykstra_projection(x, P_U, P_V, tol, max_iter):\n    \"\"\"\n    Projects a vector x onto the intersection of two subspaces U and V\n    using Dykstra's alternating projection algorithm.\n    \"\"\"\n    y = x.copy()\n    p = np.zeros_like(x)\n    q = np.zeros_like(x)\n\n    for _ in range(max_iter):\n        y_prev = y\n        \n        x_k = P_U @ (y_prev + p)\n        p = (y_prev + p) - x_k\n\n        y = P_V @ (x_k + q)\n        q = (x_k + q) - y\n\n        if np.linalg.norm(y - y_prev) < tol:\n            break\n            \n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    epsilon = 1e-8\n    e1_3, e2_3, e3_3 = np.identity(3).T\n    e1_4, e2_4, e3_4, e4_4 = np.identity(4).T\n\n    test_cases = [\n        # Case 1 (n=5)\n        (np.array([[1, 0], [1, 0], [0, 1], [0, 0], [0, 0]]), \n         np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0]])),\n        # Case 2 (n=4)\n        (np.vstack([e1_4, e2_4]).T, np.vstack([e1_4, e2_4, e3_4]).T),\n        # Case 3 (n=3)\n        (e1_3.reshape(-1, 1), e2_3.reshape(-1, 1)),\n        # Case 4 (n=3)\n        (np.identity(3), np.identity(3)),\n        # Case 5 (n=3), with epsilon\n        (np.vstack([e1_3, e2_3]).T, np.array([[1, 0], [epsilon, 1], [0, 0]]))\n    ]\n\n    SVD_TOL = 1e-12\n    DYKSTRA_TOL = 1e-12\n    DYKSTRA_MAX_ITER = 2000\n\n    results = []\n\n    for U_basis, V_basis in test_cases:\n        n = U_basis.shape[0]\n\n        # 1 & 2. Get orthonormal bases for subspaces and their complements\n        Q_U, Q_U_perp = get_orthonormal_bases(U_basis, SVD_TOL)\n        Q_V, Q_V_perp = get_orthonormal_bases(V_basis, SVD_TOL)\n        \n        # 3. Construct the closed-form projector onto the intersection\n        P_CF = get_intersection_projector(Q_U_perp, Q_V_perp, n, SVD_TOL)\n\n        # 4. Construct projector using Dykstra's algorithm\n        P_U = Q_U @ Q_U.T\n        P_V = Q_V @ Q_V.T\n        P_Dykstra = np.zeros((n, n))\n        for i in range(n):\n            e_i = np.zeros(n)\n            e_i[i] = 1.0\n            col = dykstra_projection(e_i, P_U, P_V, DYKSTRA_TOL, DYKSTRA_MAX_ITER)\n            P_Dykstra[:, i] = col\n\n        # 5. Compute the required metrics\n        # Frobenius norm difference\n        e = np.linalg.norm(P_CF - P_Dykstra, 'fro')\n        \n        # Dimension of intersection (rank of projector = trace of projector)\n        d = int(np.round(np.trace(P_CF)))\n        \n        # Idempotence residual of the closed-form projector\n        i = np.linalg.norm(P_CF @ P_CF - P_CF, 'fro')\n\n        results.append([e, d, i])\n    \n    # Format and print the final output\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "While orthogonal projectors are numerically robust, many problems in data analysis and physics are inherently non-orthogonal, requiring the use of oblique projectors. This practice  delves into the numerical stability of these operators, establishing a concrete link between the geometry of subspaces—measured by principal angles—and the conditioning of the projection problem. You will see firsthand how ill-conditioning manifests as a loss of complementarity in finite precision, a crucial diagnostic for the reliability of your computations.",
            "id": "3567682",
            "problem": "Let $n$ be a positive integer and consider two subspaces $\\mathcal{U} \\subset \\mathbb{R}^n$ and $\\mathcal{V} \\subset \\mathbb{R}^n$ that form a direct sum, meaning $\\mathbb{R}^n = \\mathcal{U} \\oplus \\mathcal{V}$ and $\\mathcal{U} \\cap \\mathcal{V} = \\{0\\}$. An oblique projector $P$ onto $\\mathcal{U}$ along $\\mathcal{V}$ is a linear operator satisfying $P^2 = P$, $\\operatorname{Range}(P) = \\mathcal{U}$, and $\\operatorname{Null}(P) = \\mathcal{V}$. The complementary oblique projector $Q$ onto $\\mathcal{V}$ along $\\mathcal{U}$ also satisfies $Q^2 = Q$, $\\operatorname{Range}(Q) = \\mathcal{V}$, and $\\operatorname{Null}(Q) = \\mathcal{U}$. In exact arithmetic, complementary oblique projectors satisfy $P + Q = I$ and $PQ = QP = 0$. In floating-point computation, loss of complementarity can be diagnosed by the spectral norms $\\lVert P + Q - I \\rVert_2$ and $\\lVert PQ \\rVert_2$.\n\nStarting from the fundamental definitions of direct sum decompositions and linear projectors (no pre-derived projector formulas may be assumed), derive a constructive method to compute $P$ and $Q$ from given bases of $\\mathcal{U}$ and $\\mathcal{V}$. Use the definitions to justify the construction. Then, for each constructed pair $(P,Q)$, compute the diagnostics $\\lVert P + Q - I \\rVert_2$ and $\\lVert PQ \\rVert_2$ and relate their magnitudes to the principal angles between $\\mathcal{U}$ and $\\mathcal{V}$.\n\nPrincipal angles $\\{\\varphi_i\\}$ between two subspaces with orthonormal bases quantify their relative orientation; they can be computed via the Singular Value Decomposition (SVD), where $\\cos(\\varphi_i)$ are the singular values of the matrix formed by the inner products of orthonormal basis vectors. Spectral norm $\\lVert \\cdot \\rVert_2$ refers to the largest singular value of a matrix. In this problem, angles are to be expressed in radians.\n\nYour program must implement the following test suite. In all cases, let $k = 3$ and $n = 2k = 6$. Define $\\mathcal{U}$ with an orthonormal basis given by the first $k$ standard basis vectors in $\\mathbb{R}^n$, namely $U = [e_1, e_2, e_3]$. Define $\\mathcal{V}$ by an orthonormal basis $V = [v_1, v_2, v_3]$, where for a specified triplet of angles $(\\theta_1, \\theta_2, \\theta_3)$,\n$$\nv_i = \\cos(\\theta_i)\\, e_i + \\sin(\\theta_i)\\, e_{i+k}, \\quad i \\in \\{1,2,3\\}.\n$$\nThis construction ensures that the principal angles between $\\mathcal{U}$ and $\\mathcal{V}$ are $(\\theta_1, \\theta_2, \\theta_3)$ and that $\\mathcal{U} \\oplus \\mathcal{V} = \\mathbb{R}^n$ whenever $0 < \\theta_i \\leq \\pi/2$ for all $i$.\n\nTest suite parameters:\n- Case A (exact orthogonal complement): $(\\theta_1,\\theta_2,\\theta_3) = (\\pi/2, \\pi/2, \\pi/2)$.\n- Case B (well-conditioned oblique split): $(\\theta_1,\\theta_2,\\theta_3) = (\\pi/4, \\pi/3, \\pi/6)$.\n- Case C (nearly non-complementary in one direction): $(\\theta_1,\\theta_2,\\theta_3) = (10^{-8}, 0.2, 0.4)$.\n- Case D (mixed conditioning): $(\\theta_1,\\theta_2,\\theta_3) = (10^{-4}, \\pi/5, 0.3)$.\n\nFor each case:\n1. Construct matrices $U \\in \\mathbb{R}^{n \\times k}$ and $V \\in \\mathbb{R}^{n \\times k}$ as described.\n2. From first principles, construct complementary oblique projectors $P \\in \\mathbb{R}^{n \\times n}$ onto $\\mathcal{U}$ along $\\mathcal{V}$ and $Q \\in \\mathbb{R}^{n \\times n}$ onto $\\mathcal{V}$ along $\\mathcal{U}$.\n3. Compute the diagnostics $\\lVert P + Q - I \\rVert_2$ and $\\lVert PQ \\rVert_2$, the smallest principal angle $\\min_i \\varphi_i$ between $\\mathcal{U}$ and $\\mathcal{V}$ (in radians), and the spectral condition number $\\kappa_2(W)$ of the concatenated basis $W = [U\\ V] \\in \\mathbb{R}^{n \\times n}$, where $\\kappa_2(W) = \\sigma_{\\max}(W)/\\sigma_{\\min}(W)$ with $\\sigma_{\\max}$ and $\\sigma_{\\min}$ denoting the largest and smallest singular values respectively.\n\nYour program should produce a single line of output containing, for each case in the given order, a list of four floats $[\\lVert P + Q - I \\rVert_2, \\lVert PQ \\rVert_2, \\min_i \\varphi_i, \\kappa_2(W)]$. Aggregate the four-case results into one list, printed as a comma-separated list enclosed in square brackets. For example, the output format must be:\n\"[[dA1,dA2,dA3,dA4],[dB1,dB2,dB3,dB4],[dC1,dC2,dC3,dD4]]\".",
            "solution": "The problem is scientifically and mathematically sound, well-posed, and all terms are formally defined. It presents a standard, albeit advanced, problem in numerical linear algebra concerning the construction and numerical stability of oblique projectors. All data and conditions are provided to allow for a unique and verifiable solution.\n\nThe problem is valid.\n\n### 1. Derivation of the Oblique Projector Construction\n\nLet $\\mathcal{U}$ and $\\mathcal{V}$ be two subspaces of $\\mathbb{R}^n$ such that $\\mathbb{R}^n = \\mathcal{U} \\oplus \\mathcal{V}$. This direct sum decomposition implies that any vector $x \\in \\mathbb{R}^n$ can be uniquely written as $x = u+v$, where $u \\in \\mathcal{U}$ and $v \\in \\mathcal{V}$.\n\nThe oblique projector $P$ onto $\\mathcal{U}$ along $\\mathcal{V}$ is defined by the mapping $Px = u$. Similarly, the complementary oblique projector $Q$ onto $\\mathcal{V}$ along $\\mathcal{U}$ is defined by $Qx = v$.\n\nWe are given a basis for $\\mathcal{U}$ as the columns of a matrix $U \\in \\mathbb{R}^{n \\times k}$ and a basis for $\\mathcal{V}$ as the columns of a matrix $V \\in \\mathbb{R}^{n \\times k}$, where $n=2k$.\nAny vector $u \\in \\mathcal{U}$ can be expressed as a linear combination of the basis vectors of $\\mathcal{U}$, so $u = Uc$ for some coefficient vector $c \\in \\mathbb{R}^k$.\nSimilarly, any vector $v \\in \\mathcal{V}$ can be expressed as $v = Vd$ for some $d \\in \\mathbb{R}^k$.\n\nThe decomposition of $x$ can thus be written as:\n$$x = Uc + Vd$$\nThis can be formulated as a linear system by concatenating the basis matrices:\n$$x = [U \\quad V] \\begin{pmatrix} c \\\\ d \\end{pmatrix}$$\nLet $W = [U \\quad V] \\in \\mathbb{R}^{n \\times n}$. Since the columns of $U$ and $V$ together form a basis for $\\mathbb{R}^n$ (a consequence of the direct sum), the matrix $W$ is invertible. We can solve for the coefficient vectors $c$ and $d$:\n$$\\begin{pmatrix} c \\\\ d \\end{pmatrix} = W^{-1}x$$\nThe vector of coefficients $c$ corresponds to the first $k$ rows of the product $W^{-1}x$, and $d$ corresponds to the last $k$ rows. We can use block-matrix notation to extract these parts. Let $I_k$ be the $k \\times k$ identity matrix and $0$ be a $k \\times k$ zero matrix.\n$$c = [I_k \\quad 0] \\begin{pmatrix} c \\\\ d \\end{pmatrix} = [I_k \\quad 0] W^{-1} x$$\n$$d = [0 \\quad I_k] \\begin{pmatrix} c \\\\ d \\end{pmatrix} = [0 \\quad I_k] W^{-1} x$$\nNow, we can write the formulas for the projections $u$ and $v$ in terms of $x$:\n$$u = Uc = U([I_k \\quad 0] W^{-1})x$$\n$$v = Vd = V([0 \\quad I_k] W^{-1})x$$\nFrom the definitions $Px=u$ and $Qx=v$, we identify the projector matrices as:\n$$P = U[I_k \\quad 0] W^{-1}$$\n$$Q = V[0 \\quad I_k] W^{-1}$$\nThis is the constructive method derived from first principles.\n\n### 2. Justification of the Construction\n\nWe verify that the constructed $P$ satisfies the required properties. Let $W^{-1}$ be block-partitioned by rows: $W^{-1} = \\begin{pmatrix} U_L \\\\ V_L \\end{pmatrix}$, where $U_L = [I_k \\quad 0]W^{-1} \\in \\mathbb{R}^{k \\times n}$ and $V_L = [0 \\quad I_k]W^{-1} \\in \\mathbb{R}^{k \\times n}$. Then $P=UU_L$ and $Q=VV_L$.\nThe product $W^{-1}W = I_n$ gives block-wise identities:\n$$W^{-1}W = \\begin{pmatrix} U_L \\\\ V_L \\end{pmatrix} [U \\quad V] = \\begin{pmatrix} U_L U & U_L V \\\\ V_L U & V_L V \\end{pmatrix} = \\begin{pmatrix} I_k & 0 \\\\ 0 & I_k \\end{pmatrix}$$\nThis yields $U_L U = I_k$, $U_L V = 0$, $V_L U = 0$, and $V_L V = I_k$.\n\n-   **Idempotence ($P^2 = P$):**\n    $P^2 = (UU_L)(UU_L) = U(U_L U)U_L = U(I_k)U_L = UU_L = P$. The property holds.\n\n-   **Range ($\\operatorname{Range}(P) = \\mathcal{U}$):**\n    For any $x \\in \\mathbb{R}^n$, $Px = U(U_L x)$, which is a linear combination of the columns of $U$. Thus, $\\operatorname{Range}(P) \\subseteq \\mathcal{U}$.\n    For any $u \\in \\mathcal{U}$, let $u=Uc$. Then $Pu = P(Uc) = (UU_L)(Uc) = U(U_L U)c = U(I_k)c = Uc = u$. This shows that every vector in $\\mathcal{U}$ is in the range of $P$. Thus, $\\mathcal{U} \\subseteq \\operatorname{Range}(P)$. Therefore, $\\operatorname{Range}(P) = \\mathcal{U}$.\n\n-   **Null Space ($\\operatorname{Null}(P) = \\mathcal{V}$):**\n    For any $v \\in \\mathcal{V}$, let $v=Vd$. Then $Pv = P(Vd) = (UU_L)(Vd) = U(U_L V)d = U(0)d = 0$. So $\\mathcal{V} \\subseteq \\operatorname{Null}(P)$.\n    For any $x \\in \\operatorname{Null}(P)$, $Px=0$, so $UU_Lx=0$. Since the columns of $U$ are linearly independent, this implies $U_Lx=0$. Any $x$ can be written as $x=u+v=Uc+Vd$. We also know $x = W W^{-1} x = [U \\quad V] \\begin{pmatrix} U_L \\\\ V_L \\end{pmatrix} x = U(U_L x) + V(V_L x)$. Since $U_L x = 0$, we have $x = V(V_L x)$, which means $x$ is a linear combination of the columns of $V$, so $x \\in \\mathcal{V}$. Thus, $\\operatorname{Null}(P) \\subseteq \\mathcal{V}$. Therefore, $\\operatorname{Null}(P) = \\mathcal{V}$.\n\nThe justification for $Q$ follows analogously.\n\n### 3. Relation of Diagnostics to Principal Angles\n\nThe smallest principal angle $\\varphi_{min} = \\min_i \\theta_i$ is a measure of the \"nearness\" of the subspaces $\\mathcal{U}$ and $\\mathcal{V}$. A small $\\varphi_{min}$ indicates that the subspaces are nearly collinear in some direction. This geometric property is directly linked to the numerical stability of the projection, as quantified by the diagnostics.\n\n**Condition Number $\\kappa_2(W)$:** The matrix $W = [U \\quad V]$ is the basis for $\\mathbb{R}^n$ used to decompose vectors. Its condition number $\\kappa_2(W) = \\sigma_{\\max}(W)/\\sigma_{\\min}(W)$ measures the sensitivity of this decomposition. For the specific bases $U$ and $V$ in this problem, we have $W=\\begin{pmatrix} I_k & C \\\\ 0 & S \\end{pmatrix}$ where $C=\\operatorname{diag}(\\cos\\theta_i)$ and $S=\\operatorname{diag}(\\sin\\theta_i)$.\nThe largest and smallest singular values are:\n$$\\sigma_{\\max}(W) = \\max_i \\sqrt{1+\\cos\\theta_i} = \\sqrt{1+\\cos(\\theta_{min})}$$\n$$\\sigma_{\\min}(W) = \\min_i \\sqrt{1-\\cos\\theta_i} = \\sqrt{1-\\cos\\theta_{\\min}}$$\nTherefore, the condition number is:\n$$\\kappa_2(W) = \\frac{\\sigma_{\\max}(W)}{\\sigma_{\\min}(W)} = \\frac{\\sqrt{1+\\cos(\\theta_{min})}}{\\sqrt{1-\\cos(\\theta_{min})}} = \\cot\\left(\\frac{\\theta_{min}}{2}\\right)$$\nAs $\\theta_{min} \\to 0$, $\\cot(\\theta_{min}/2) \\to \\infty$. So, a small minimal principal angle leads to an extremely ill-conditioned basis matrix $W$.\n\n**Diagnostic Norms:** The loss-of-complementarity diagnostics $\\lVert P+Q-I \\rVert_2$ and $\\lVert PQ \\rVert_2$ are zero in exact arithmetic. In floating-point arithmetic, their non-zero values arise from numerical errors.\nThe computation of $P$ and $Q$ involves inverting $W$. The relative error in the computed inverse $W_{inv}$ is bounded by approximately $\\kappa_2(W)\\epsilon_{mach}$, where $\\epsilon_{mach}$ is machine epsilon. This error propagates to $P$ and $Q$.\nMoreover, the norms of the projectors themselves are large for small $\\theta_{min}$. The norm of $P$ is:\n$$\\lVert P \\rVert_2 = \\sigma_{\\max}(P) = \\max_i \\sqrt{1+\\cot^2\\theta_i} = \\max_i |\\csc\\theta_i| = \\csc(\\theta_{min})$$\nAs $\\theta_{min} \\to 0$, $\\lVert P \\rVert_2 \\to \\infty$. The same holds for $\\lVert Q \\rVert_2$.\nThe absolute error in computed quantities like $P+Q$ and $PQ$ is influenced by both the condition number of the problem and the magnitude of the quantities involved.\nThe error in $P+Q-I$ is affected by the error in $W_inv$, so we expect $\\lVert P+Q-I \\rVert_2 \\approx \\mathcal{O}(\\kappa_2(W)\\epsilon_{mach})$.\nThe error in $PQ = P-P^2$ is affected by the large norm of $P$, so we expect $\\lVert PQ \\rVert_2 \\approx \\mathcal{O}(\\lVert P \\rVert_2 \\epsilon_{mach}) = \\mathcal{O}(\\csc(\\theta_{min})\\epsilon_{mach})$.\nSince for small $\\theta_{min}$, $\\cot(\\theta_{min}/2) \\approx 2/\\theta_{min}$ and $\\csc(\\theta_{min}) \\approx 1/\\theta_{min}$, both diagnostics are expected to grow inversely with $\\theta_{min}$, indicating severe loss of numerical complementarity when subspaces are close.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the oblique projector problem for a suite of test cases.\n    \"\"\"\n    # Define problem constants\n    k = 3\n    n = 2 * k\n\n    # Define test cases with angles in radians\n    test_cases = [\n        # Case A: Orthogonal complement\n        (np.pi/2, np.pi/2, np.pi/2),\n        # Case B: Well-conditioned oblique split\n        (np.pi/4, np.pi/3, np.pi/6),\n        # Case C: Nearly non-complementary\n        (1e-8, 0.2, 0.4),\n        # Case D: Mixed conditioning\n        (1e-4, np.pi/5, 0.3)\n    ]\n\n    all_results = []\n\n    for thetas in test_cases:\n        # Convert tuple of angles to a numpy array for vectorized operations\n        thetas_arr = np.array(thetas)\n\n        # 1. Construct matrices U and V\n        # U is the first k standard basis vectors\n        U = np.zeros((n, k))\n        U[:k, :k] = np.eye(k)\n\n        # V is constructed from the given angles\n        cos_thetas = np.cos(thetas_arr)\n        sin_thetas = np.sin(thetas_arr)\n        \n        # C and S are diagonal matrices\n        C = np.diag(cos_thetas)\n        S = np.diag(sin_thetas)\n        \n        V = np.zeros((n, k))\n        V[:k, :] = C\n        V[k:, :] = S\n\n        # Concatenate U and V to form the basis matrix W for R^n\n        W = np.concatenate((U, V), axis=1)\n\n        # 2. Construct oblique projectors P and Q from first principles\n        # The derivation shows P = U * U_L and Q = V * V_L\n        # where U_L and V_L are the top and bottom k-row blocks of W_inv.\n        try:\n            W_inv = np.linalg.inv(W)\n        except np.linalg.LinAlgError:\n            # This case is not expected for the given test parameters\n            # as sin(theta_i) > 0, making W invertible.\n            all_results.append([np.nan, np.nan, np.nan, np.nan])\n            continue\n        \n        U_L = W_inv[:k, :]\n        V_L = W_inv[k:, :]\n\n        P = U @ U_L\n        Q = V @ V_L\n\n        # 3. Compute diagnostics\n        # Diagnostic 1: ||P + Q - I||_2\n        I_n = np.eye(n)\n        diag_1 = np.linalg.norm(P + Q - I_n, ord=2)\n        \n        # Diagnostic 2: ||PQ||_2\n        diag_2 = np.linalg.norm(P @ Q, ord=2)\n        \n        # Diagnostic 3: Smallest principal angle (given by construction)\n        min_angle = np.min(thetas_arr)\n        \n        # Diagnostic 4: Spectral condition number of W\n        cond_W = np.linalg.cond(W, p=2)\n        \n        case_results = [diag_1, diag_2, min_angle, cond_W]\n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In the idealized world of linear algebra, projectors are perfectly idempotent and, if orthogonal, perfectly symmetric. In floating-point reality, these defining properties are often slightly violated by roundoff error. This practice  tackles the essential task of \"denoising\" a numerical approximation of a projector by finding the closest valid orthogonal projector. The solution elegantly uses the spectral theorem to threshold eigenvalues, demonstrating a powerful and widely applicable technique for enforcing structural constraints on numerical data.",
            "id": "3567643",
            "problem": "Consider a real matrix $P \\in \\mathbb{R}^{n \\times n}$ that arises as a numerically computed approximation to a projector. In finite precision arithmetic, such a $P$ may be slightly non-symmetric and slightly non-idempotent. Let the symmetric part of $P$ be $P_s = \\frac{1}{2}(P + P^{\\top})$. A projector is a matrix $X$ satisfying $X^2 = X$. An orthogonal projector is a projector that is also symmetric, i.e., $X = X^{\\top}$ and $X^2 = X$. The Frobenius norm of a matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$.\n\nYour task is to design a program that, for each provided test matrix $P$, evaluates the quality of two repair strategies:\n- the simple symmetrization $P_s$, and\n- the optimized repair by the nearest orthogonal projector in Frobenius norm.\n\nThe program must be derived from and implement the following fundamental and well-tested principles without relying on prepackaged shortcut formulas:\n- The spectral theorem for real symmetric matrices: any real symmetric matrix $S$ admits an eigendecomposition $S = Q \\Lambda Q^{\\top}$ with $Q$ orthogonal and $\\Lambda$ real diagonal.\n- The Frobenius norm is unitarily invariant and induced by the inner product $\\langle A, B \\rangle = \\mathrm{trace}(A^{\\top} B)$.\n- If $A = S + K$ with $S = S^{\\top}$ and $K = -K^{\\top}$, then $\\|A\\|_F^2 = \\|S\\|_F^2 + \\|K\\|_F^2$ and $\\langle S, K \\rangle = 0$.\n\nYou must, from these principles, deduce an algorithm to compute the nearest orthogonal projector to a given real matrix $P$ in Frobenius norm, and then compute the following three quantities for each test case:\n- the idempotency defect of the simple symmetrization: $\\|P_s^2 - P_s\\|_F$,\n- the distance from the original matrix to its symmetrization: $\\|P - P_s\\|_F$,\n- the distance from the original matrix to the nearest orthogonal projector $P_{\\star}$ in Frobenius norm: $\\|P - P_{\\star}\\|_F$.\n\nYour program must implement your derived method for computing $P_{\\star}$ using only the listed principles and standard numerical linear algebra operations.\n\nTest Suite:\nCompute the specified three quantities for each of the following explicitly given matrices. All entries are in plain real numbers. Ensure to use the exact numerical values as written.\n\n- Test case $1$ ($4 \\times 4$):\n$$\nP^{(1)} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0.001 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n-0.001 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\n\n- Test case $2$ ($4 \\times 4$):\n$$\nP^{(2)} =\n\\begin{bmatrix}\n0.5 & 0 & 0.5 & 0.001 \\\\\n0 & 0.5 & 0.0005 & 0.5 \\\\\n0.5 & -0.0005 & 0.5 & 0 \\\\\n0.001 & 0.5 & 0 & 0.5\n\\end{bmatrix}.\n$$\n\n- Test case $3$ ($4 \\times 4$):\n$$\nP^{(3)} =\n\\begin{bmatrix}\n0.49 & 0.001 & 0 & 0 \\\\\n-0.001 & 0.51 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n\n- Test case $4$ ($5 \\times 5$):\n$$\nP^{(4)} =\n\\begin{bmatrix}\n0.5 & 0 & 0 & 0 & 0.0002 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0.5 & 0 \\\\\n-0.0002 & 0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must yield a list of three floating-point numbers in the order\n$[\\|P_s^2 - P_s\\|_F,\\ \\|P - P_s\\|_F,\\ \\|P - P_{\\star}\\|_F]$,\nand the final output must be a list of these per-test lists in the same order as above. For example, the output must have the form\n$[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$\nwith no additional text. Angles and physical units are not applicable; report all real numbers as standard decimal floating-point values. The program must not read any input and must be fully deterministic.",
            "solution": "The problem requires the design of a program to evaluate two methods for repairing a numerically computed matrix $P \\in \\mathbb{R}^{n \\times n}$ that is an approximation of an orthogonal projector. An orthogonal projector $X$ is a matrix that is both symmetric ($X = X^{\\top}$) and idempotent ($X^2 = X$). The two repair strategies are:\n1.  A simple symmetrization, resulting in $P_s = \\frac{1}{2}(P + P^{\\top})$.\n2.  A more sophisticated repair by finding the nearest orthogonal projector $P_{\\star}$ in the Frobenius norm.\n\nThe core of the task is to derive, from first principles, an algorithm to find $P_{\\star}$. This involves solving the optimization problem:\n$$\n\\min_{X} \\|P - X\\|_F \\quad \\text{subject to} \\quad X = X^{\\top} \\text{ and } X^2 = X.\n$$\nThe derivation must utilize the provided principles: the spectral theorem for real symmetric matrices, the properties of the Frobenius norm, and the orthogonality of symmetric and skew-symmetric matrices.\n\nFirst, let us formalize the derivation of the nearest orthogonal projector $P_{\\star}$. We seek to minimize the objective function $f(X) = \\|P - X\\|_F^2$. Every real square matrix $P$ can be uniquely decomposed into its symmetric part $P_s = \\frac{1}{2}(P + P^{\\top})$ and its skew-symmetric part $P_k = \\frac{1}{2}(P - P^{\\top})$, such that $P = P_s + P_k$. The candidate matrix $X$, being an orthogonal projector, must be symmetric. Therefore, the difference $P_s - X$ is also symmetric. We can rewrite the objective function as:\n$$\n\\|P - X\\|_F^2 = \\|(P_s + P_k) - X\\|_F^2 = \\|(P_s - X) + P_k\\|_F^2.\n$$\nThe problem statement provides the principle that for any matrix decomposed into its symmetric part $S$ and skew-symmetric part $K$, the squared Frobenius norm is $\\|S+K\\|_F^2 = \\|S\\|_F^2 + \\|K\\|_F^2$, which follows from the fact that symmetric and skew-symmetric matrices are orthogonal under the Frobenius inner product, i.e., $\\langle S, K \\rangle = \\mathrm{trace}(S^{\\top}K) = 0$. Applying this principle to the expression $(P_s - X) + P_k$, where $(P_s - X)$ is symmetric and $P_k$ is skew-symmetric, we get:\n$$\n\\|(P_s - X) + P_k\\|_F^2 = \\|P_s - X\\|_F^2 + \\|P_k\\|_F^2.\n$$\nThe term $\\|P_k\\|_F^2$ is determined solely by the input matrix $P$ and is constant with respect to the choice of $X$. Therefore, minimizing $\\|P-X\\|_F^2$ is equivalent to minimizing $\\|P_s - X\\|_F^2$. This insight simplifies the problem: the nearest orthogonal projector to $P$ is the same as the nearest orthogonal projector to its symmetric part, $P_s$. Our optimization problem is now:\n$$\n\\min_{X} \\|P_s - X\\|_F \\quad \\text{subject to} \\quad X = X^{\\top} \\text{ and } X^2 = X.\n$$\nThe next step employs the spectral theorem for real symmetric matrices, which states that any such matrix $S$ can be diagonalized by an orthogonal matrix. For $P_s$, we have the eigendecomposition $P_s = Q \\Lambda Q^{\\top}$, where $Q$ is an orthogonal matrix ($Q^{\\top}Q = I$) whose columns are the eigenvectors of $P_s$, and $\\Lambda$ is a real diagonal matrix whose diagonal entries $\\lambda_1, \\ldots, \\lambda_n$ are the corresponding eigenvalues.\nWe use a further principle provided: the Frobenius norm is unitarily invariant. This means $\\|A\\|_F = \\|UAV\\|_F$ for any orthogonal matrices $U$ and $V$. Applying this to our objective function with $U=Q^{\\top}$ and $V=Q$:\n$$\n\\|P_s - X\\|_F^2 = \\|Q \\Lambda Q^{\\top} - X\\|_F^2 = \\|Q^{\\top}(Q \\Lambda Q^{\\top} - X)Q\\|_F^2 = \\|\\Lambda - Q^{\\top}XQ\\|_F^2.\n$$\nLet us define a new variable $Y = Q^{\\top}XQ$. Since $X$ is an orthogonal projector and $Q$ is orthogonal, $Y$ is also an orthogonal projector:\n-   Symmetry: $Y^{\\top} = (Q^{\\top}XQ)^{\\top} = Q^{\\top}X^{\\top}Q = Q^{\\top}XQ = Y$.\n-   Idempotence: $Y^2 = (Q^{\\top}XQ)(Q^{\\top}XQ) = Q^{\\top}X(QQ^{\\top})XQ = Q^{\\top}X^2Q = Q^{\\top}XQ = Y$.\nThe problem is transformed into finding an orthogonal projector $Y$ that minimizes $\\|\\Lambda - Y\\|_F^2$.\n$$\n\\|\\Lambda - Y\\|_F^2 = \\sum_{i,j=1}^{n} (\\Lambda_{ij} - Y_{ij})^2.\n$$\nSince $\\Lambda$ is diagonal, this is $\\sum_{i=1}^{n} (\\lambda_i - Y_{ii})^2 + \\sum_{i \\neq j} Y_{ij}^2$. To minimize this sum, any non-zero off-diagonal elements $Y_{ij}$ would strictly increase the value. Thus, the optimal $Y$ must be a diagonal matrix. A diagonal matrix $Y$ that is also a projector ($Y^2=Y$) must have diagonal entries $Y_{ii}$ that satisfy $Y_{ii}^2 = Y_{ii}$, meaning $Y_{ii} \\in \\{0, 1\\}$.\nThe problem thus reduces to choosing, for each $i$, a value $Y_{ii} \\in \\{0, 1\\}$ that minimizes $(\\lambda_i - Y_{ii})^2$. This is achieved by simple thresholding:\n-   If $\\lambda_i \\ge 0.5$, choosing $Y_{ii}=1$ minimizes the term $(\\lambda_i-1)^2$.\n-   If $\\lambda_i < 0.5$, choosing $Y_{ii}=0$ minimizes the term $(\\lambda_i-0)^2$.\nLet's call this optimal diagonal projector $\\Lambda_{\\star}$. Its diagonal entries are $(\\Lambda_{\\star})_{ii} = 1$ if $\\lambda_i \\ge 0.5$ and $0$ otherwise.\nHaving found the optimal $\\Lambda_{\\star}$, we transform back to find the nearest orthogonal projector $P_{\\star}$ in the original basis:\n$$\nP_{\\star} = Q \\Lambda_{\\star} Q^{\\top}.\n$$\nThe algorithm for computing $P_{\\star}$ from a given matrix $P$ is:\n1.  Compute the symmetric part $P_s = \\frac{1}{2}(P + P^{\\top})$.\n2.  Perform an eigendecomposition of $P_s$ to find its eigenvalues $\\Lambda$ and eigenvectors $Q$, such that $P_s = Q \\Lambda Q^{\\top}$.\n3.  Construct a diagonal matrix $\\Lambda_{\\star}$ by thresholding the eigenvalues: $(\\Lambda_{\\star})_{ii} = 1$ if $\\lambda_i \\ge 0.5$, and $0$ otherwise.\n4.  Compute the nearest orthogonal projector $P_{\\star} = Q \\Lambda_{\\star} Q^{\\top}$.\n\nFor each test matrix $P$, we will compute the following three quantities:\n1.  The idempotency defect of the simple symmetrization: $\\|P_s^2 - P_s\\|_F$. This measures how far $P_s$ is from being idempotent.\n2.  The distance from the original matrix to its symmetrization: $\\|P - P_s\\|_F$. This is simply the norm of the skew-symmetric part, $\\|P_k\\|_F$.\n3.  The distance from the original matrix to the nearest orthogonal projector: $\\|P - P_{\\star}\\|_F$. This is the minimal possible distance from $P$ to any orthogonal projector in the Frobenius norm.\n\nThe following Python code implements this derived procedure for each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the projector repair problem for a series of test matrices.\n    For each matrix P, it computes:\n    1. Idempotency defect of the symmetric part Ps: ||Ps^2 - Ps||_F\n    2. Distance from P to its symmetric part: ||P - Ps||_F\n    3. Distance from P to the nearest orthogonal projector P_star: ||P - P_star||_F\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        np.array([\n            [1.0, 0.0, 0.0, 0.001],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [-0.001, 0.0, 0.0, 0.0]\n        ]),\n        # Test case 2\n        np.array([\n            [0.5, 0.0, 0.5, 0.001],\n            [0.0, 0.5, 0.0005, 0.5],\n            [0.5, -0.0005, 0.5, 0.0],\n            [0.001, 0.5, 0.0, 0.5]\n        ]),\n        # Test case 3\n        np.array([\n            [0.49, 0.001, 0.0, 0.0],\n            [-0.001, 0.51, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 1.0]\n        ]),\n        # Test case 4\n        np.array([\n            [0.5, 0.0, 0.0, 0.0, 0.0002],\n            [0.0, 1.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.5, 0.0],\n            [-0.0002, 0.0, 0.0, 0.0, 1.0]\n        ])\n    ]\n\n    results = []\n    for P in test_cases:\n        # Compute the symmetric part Ps\n        Ps = 0.5 * (P + P.T)\n\n        # 1. Idempotency defect of Ps\n        idempotency_defect_ps = np.linalg.norm(Ps @ Ps - Ps, 'fro')\n\n        # 2. Distance from P to Ps\n        dist_p_ps = np.linalg.norm(P - Ps, 'fro')\n\n        # 3. Distance from P to the nearest orthogonal projector P_star\n        # The derivation shows we need to find the eigendecomposition of Ps\n        eigenvalues, Q = np.linalg.eigh(Ps)\n\n        # Threshold the eigenvalues to get the eigenvalues of the nearest projector\n        # in the diagonalized basis\n        lambda_star_diag = np.where(eigenvalues >= 0.5, 1.0, 0.0)\n        Lambda_star = np.diag(lambda_star_diag)\n\n        # Construct the nearest orthogonal projector P_star\n        P_star = Q @ Lambda_star @ Q.T\n        \n        # Compute the distance from P to P_star\n        dist_p_pstar = np.linalg.norm(P - P_star, 'fro')\n        \n        results.append([idempotency_defect_ps, dist_p_ps, dist_p_pstar])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```"
        }
    ]
}