{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of low-rank matrices, we must first master their fundamental building block: the rank-one outer product $A = uv^T$. This exercise guides you through deriving the essential geometric properties of such a matrix from first principles. By determining its column and row spaces and constructing the orthogonal projectors onto them, you will solidify the connection between the algebraic form of an outer product and its geometric meaning .",
            "id": "3563738",
            "problem": "Let $m,n \\in \\mathbb{N}$, let $u \\in \\mathbb{R}^{m}$ and $v \\in \\mathbb{R}^{n}$ be nonzero vectors, and let $A \\in \\mathbb{R}^{m \\times n}$ be the rank-one outer product $A = u v^{T}$. Work with the standard Euclidean inner product on $\\mathbb{R}^{m}$ and $\\mathbb{R}^{n}$. The orthogonal projector onto a linear subspace $S$ of a finite-dimensional inner product space is the unique linear map $P$ such that $P^{2} = P$, $P$ is self-adjoint, $\\operatorname{range}(P) = S$, and for every $y$ the vector $Py$ is the unique element of $S$ minimizing the Euclidean distance to $y$.\n\nStarting from these definitions and the fundamental properties of outer products, do the following:\n- Determine $\\operatorname{col}(A)$ and $\\operatorname{row}(A)$, where $\\operatorname{col}(A)$ is the column space in $\\mathbb{R}^{m}$ and $\\operatorname{row}(A)$ is the row space in $\\mathbb{R}^{n}$.\n- Derive, from first principles via the defining minimization property, the explicit formulas of the orthogonal projectors $P_{\\operatorname{col}(A)} \\in \\mathbb{R}^{m \\times m}$ and $P_{\\operatorname{row}(A)} \\in \\mathbb{R}^{n \\times n}$ onto these subspaces, and verify that each such projector is symmetric and idempotent.\n- Using only these properties and basic matrix identities, compute the ranks and traces of $P_{\\operatorname{col}(A)}$ and $P_{\\operatorname{row}(A)}$.\n\nProvide your final answer as a single row matrix with entries in the following order:\n`[\\operatorname{rank}(P_{\\operatorname{col}(A)}), \\operatorname{tr}(P_{\\operatorname{col}(A)}), \\operatorname{rank}(P_{\\operatorname{row}(A)}), \\operatorname{tr}(P_{\\operatorname{row}(A)})]`.\n\nNo rounding is required. Do not include any units. The final answer must be a single row matrix as specified above.",
            "solution": "The problem is valid as it is mathematically well-defined, self-contained, and grounded in the standard principles of linear algebra.\n\nFirst, we determine the column space, $\\operatorname{col}(A)$, and the row space, $\\operatorname{row}(A)$, of the matrix $A = u v^{T}$, where $u \\in \\mathbb{R}^{m}$ and $v \\in \\mathbb{R}^{n}$ are nonzero vectors.\n\nThe matrix $A$ is given by the outer product $A = u v^{T}$. Writing the vector $v$ in terms of its components, $v = [v_1, v_2, \\dots, v_n]^T$, the matrix $A$ can be expressed as:\n$$\nA = u [v_1, v_2, \\dots, v_n] = [v_1 u, v_2 u, \\dots, v_n u]\n$$\nThe columns of $A$ are $a_j = v_j u$ for $j = 1, \\dots, n$. Each column is a scalar multiple of the vector $u$. The column space, $\\operatorname{col}(A)$, is the span of these columns. Any vector $x \\in \\operatorname{col}(A)$ can be written as a linear combination of the columns of $A$:\n$$\nx = \\sum_{j=1}^{n} c_j (v_j u) = \\left( \\sum_{j=1}^{n} c_j v_j \\right) u\n$$\nThis demonstrates that any vector in $\\operatorname{col}(A)$ is a scalar multiple of $u$. Therefore, $\\operatorname{col}(A) \\subseteq \\operatorname{span}(u)$.\nSince $v$ is a nonzero vector, there exists at least one component $v_k \\neq 0$. The $k$-th column of $A$ is $a_k = v_k u$. Since $u$ is also a nonzero vector, $a_k$ is a non-zero vector in $\\operatorname{col}(A)$. We can write $u = (1/v_k) a_k$, which shows that $u \\in \\operatorname{col}(A)$. Thus, $\\operatorname{span}(u) \\subseteq \\operatorname{col}(A)$. Combining the two inclusions, we conclude that $\\operatorname{col}(A) = \\operatorname{span}(u)$.\n\nThe row space, $\\operatorname{row}(A)$, is the column space of the transpose matrix $A^T$.\n$$\nA^T = (u v^T)^T = (v^T)^T u^T = v u^T\n$$\nBy an identical argument, the columns of $A^T$ are all scalar multiples of the vector $v$. Since $u \\neq 0$, at least one column is a non-zero multiple of $v$. Thus, the column space of $A^T$ is the span of $v$. Therefore, $\\operatorname{row}(A) = \\operatorname{col}(A^T) = \\operatorname{span}(v)$.\n\nNext, we derive the explicit formulas for the orthogonal projectors $P_{\\operatorname{col}(A)}$ and $P_{\\operatorname{row}(A)}$ from the defining minimization property.\n\nFor the projector $P_{\\operatorname{col}(A)} \\in \\mathbb{R}^{m \\times m}$ onto $S = \\operatorname{col}(A) = \\operatorname{span}(u)$, we seek for any vector $y \\in \\mathbb{R}^m$ the unique vector $p \\in S$ that minimizes the Euclidean distance $\\|y - p\\|_2$. Any vector $p \\in S$ can be written as $p = c u$ for some scalar $c \\in \\mathbb{R}$. We want to find the value of $c$ that minimizes the squared distance $f(c) = \\|y - c u\\|_2^2$.\nUsing the inner product (dot product), for which $\\langle x, z \\rangle = x^T z$:\n$$\nf(c) = \\langle y - c u, y - c u \\rangle = \\langle y, y \\rangle - 2c \\langle u, y \\rangle + c^2 \\langle u, u \\rangle = \\|y\\|_2^2 - 2c (u^T y) + c^2 (u^T u)\n$$\nThis is a quadratic function of $c$. To find the minimum, we differentiate with respect to $c$ and set the derivative to zero:\n$$\n\\frac{df}{dc} = -2(u^T y) + 2c (u^T u) = 0\n$$\nSince $u \\neq 0$, its norm squared $\\|u\\|_2^2 = u^T u$ is non-zero. We can solve for $c$:\n$$\nc = \\frac{u^T y}{u^T u}\n$$\nThe projection of $y$ onto $S$ is $p = c u = \\left(\\frac{u^T y}{u^T u}\\right)u$. This can be rewritten using matrix multiplication:\n$$\np = u \\left(\\frac{u^T y}{u^T u}\\right) = \\frac{u(u^T y)}{u^T u} = \\frac{(u u^T)y}{u^T u}\n$$\nThe projection is a linear transformation $p = P_{\\operatorname{col}(A)} y$. Thus, the matrix representation of the projector is:\n$$\nP_{\\operatorname{col}(A)} = \\frac{u u^T}{u^T u}\n$$\nAn analogous derivation for the projector $P_{\\operatorname{row}(A)} \\in \\mathbb{R}^{n \\times n}$ onto $S' = \\operatorname{row}(A) = \\operatorname{span}(v)$ yields:\n$$\nP_{\\operatorname{row}(A)} = \\frac{v v^T}{v^T v}\n$$\nNow, we verify that these projectors are symmetric (self-adjoint) and idempotent ($P^2 = P$).\nFor $P_{\\operatorname{col}(A)}$:\nSymmetry: $P_{\\operatorname{col}(A)}^T = \\left(\\frac{u u^T}{u^T u}\\right)^T = \\frac{(u u^T)^T}{u^T u} = \\frac{(u^T)^T u^T}{u^T u} = \\frac{u u^T}{u^T u} = P_{\\operatorname{col}(A)}$. It is symmetric.\nIdempotency:\n$$\nP_{\\operatorname{col}(A)}^2 = \\left(\\frac{u u^T}{u^T u}\\right) \\left(\\frac{u u^T}{u^T u}\\right) = \\frac{(u u^T)(u u^T)}{(u^T u)^2} = \\frac{u(u^T u)u^T}{(u^T u)^2}\n$$\nSince $u^T u$ is a scalar, it commutes, leading to:\n$$\nP_{\\operatorname{col}(A)}^2 = \\frac{(u^T u)(u u^T)}{(u^T u)^2} = \\frac{u u^T}{u^T u} = P_{\\operatorname{col}(A)}\n$$\nIt is idempotent. The verification for $P_{\\operatorname{row}(A)}$ is identical, replacing $u$ with $v$.\n\nFinally, we compute the ranks and traces of these projectors.\nThe rank of an orthogonal projector is the dimension of the subspace onto which it projects.\n$$\n\\operatorname{rank}(P_{\\operatorname{col}(A)}) = \\dim(\\operatorname{col}(A)) = \\dim(\\operatorname{span}(u))\n$$\nSince $u$ is a nonzero vector, the set $\\{u\\}$ is a basis for its span. Thus, $\\dim(\\operatorname{span}(u)) = 1$.\nSo, $\\operatorname{rank}(P_{\\operatorname{col}(A)}) = 1$.\nSimilarly, $\\operatorname{rank}(P_{\\operatorname{row}(A)}) = \\dim(\\operatorname{row}(A)) = \\dim(\\operatorname{span}(v)) = 1$, since $v \\neq 0$.\n\nThe trace of a matrix can be computed using the property $\\operatorname{tr}(XY) = \\operatorname{tr}(YX)$. For $P_{\\operatorname{col}(A)} \\in \\mathbb{R}^{m \\times m}$:\n$$\n\\operatorname{tr}(P_{\\operatorname{col}(A)}) = \\operatorname{tr}\\left(\\frac{u u^T}{u^T u}\\right) = \\frac{1}{u^T u} \\operatorname{tr}(u u^T)\n$$\nUsing the cyclic property, $\\operatorname{tr}(u u^T) = \\operatorname{tr}(u^T u)$. The term $u^T u$ is a $1 \\times 1$ matrix (a scalar), so its trace is the scalar itself, $\\operatorname{tr}(u^T u) = u^T u$.\n$$\n\\operatorname{tr}(P_{\\operatorname{col}(A)}) = \\frac{1}{u^T u} (u^T u) = 1\n$$\nBy the same reasoning, for $P_{\\operatorname{row}(A)} \\in \\mathbb{R}^{n \\times n}$:\n$$\n\\operatorname{tr}(P_{\\operatorname{row}(A)}) = \\operatorname{tr}\\left(\\frac{v v^T}{v^T v}\\right) = \\frac{1}{v^T v} \\operatorname{tr}(v^T v) = \\frac{1}{v^T v} (v^T v) = 1\n$$\nThis confirms the general property that for any orthogonal projector $P$, its rank is equal to its trace.\n\nThe requested quantities are:\n$\\operatorname{rank}(P_{\\operatorname{col}(A)}) = 1$\n$\\operatorname{tr}(P_{\\operatorname{col}(A)}) = 1$\n$\\operatorname{rank}(P_{\\operatorname{row}(A)}) = 1$\n$\\operatorname{tr}(P_{\\operatorname{row}(A)}) = 1$\nAssembling these into a single row matrix gives $[1, 1, 1, 1]$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Low-rank matrices are powerful because they can be stored and manipulated in a factored form, such as $A = U_A V_A^T$, leading to significant computational savings. This practice explores the cornerstone of these fast algorithms: the multiplication of two low-rank matrices. By leveraging the associativity of matrix multiplication, you can avoid forming the large matrices explicitly and prove the crucial bound on the rank of the product, $\\operatorname{rank}(A B) \\le \\min(k_A, k_B)$, which is a fundamental result in numerical linear algebra .",
            "id": "3563735",
            "problem": "You are given two matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$ presented in low-rank factorizations $A = U_A V_A^T$ and $B = U_B V_B^T$, where $U_A \\in \\mathbb{R}^{m \\times k_A}$, $V_A \\in \\mathbb{R}^{n \\times k_A}$, $U_B \\in \\mathbb{R}^{n \\times k_B}$, and $V_B \\in \\mathbb{R}^{p \\times k_B}$. Design an algorithm to compute $A B$ without explicitly forming $A$ or $B$, by exploiting associativity of matrix multiplication to use the identity $A B = U_A \\left(V_A^T U_B\\right) V_B^T$. Justify correctness from first principles using the definition of matrix multiplication and associativity. Derive a bound on the rank of $A B$ in terms of $k_A$ and $k_B$, using only the submultiplicative rank inequality $\\operatorname{rank}(X Y) \\le \\min\\{\\operatorname{rank}(X), \\operatorname{rank}(Y)\\}$ and the definition of rank for products of rectangular matrices. Provide a clear argument that $\\operatorname{rank}(A B) \\le \\min(k_A, k_B)$.\n\nYour program must:\n- Implement the algorithm that computes $A B$ as $U_A \\left(V_A^T U_B\\right) V_B^T$ using only the low-rank factors.\n- For verification, also compute $A B$ naively by first forming $A = U_A V_A^T$ and $B = U_B V_B^T$ and then multiplying them.\n- Compute the Frobenius norm of the difference between the two results, $\\lVert A B_{\\text{fast}} - A B_{\\text{naive}} \\rVert_F$.\n- Compute the numerical rank of $A B$ using the singular value decomposition with a tolerance $\\tau = \\max\\{m, p\\} \\, \\epsilon \\, \\sigma_{\\max}$, where $\\epsilon$ is machine epsilon for double precision and $\\sigma_{\\max}$ is the largest singular value of $A B$. The numerical rank is the number of singular values strictly greater than $\\tau$.\n- For each test case, return a quadruple consisting of: the Frobenius error as a floating-point number, the numerical rank of $A B$ as an integer, the bound $\\min(k_A, k_B)$ as an integer, and a boolean indicating whether the inequality $\\operatorname{rank}(A B) \\le \\min(k_A, k_B)$ holds.\n\nBase your derivation solely on:\n- The definition of matrix multiplication and associativity.\n- The representation of a low-rank matrix as a sum of outer products.\n- The inequality $\\operatorname{rank}(X Y) \\le \\min\\{\\operatorname{rank}(X), \\operatorname{rank}(Y)\\}$.\n- The definition of numerical rank using the singular value decomposition.\n\nYour program must be self-contained and use no input. Use the following test suite, which defines the dimensions and constructions for $U_A$, $V_A$, $U_B$, $V_B$:\n- Test case $1$ (general): $m=7$, $n=5$, $p=6$, $k_A=3$, $k_B=2$. Construct $U_A$, $V_A$, $U_B$, $V_B$ with independent standard normal entries using random seed $0$.\n- Test case $2$ (rank-$1$ factors): $m=8$, $n=4$, $p=3$, $k_A=1$, $k_B=1$. Construct $U_A$, $V_A$, $U_B$, $V_B$ with independent standard normal entries using random seed $1$.\n- Test case $3$ (zero product via orthogonality): $m=6$, $n=4$, $p=5$, $k_A=2$, $k_B=2$. Construct $U_A$ and $V_B$ with independent standard normal entries using random seed $2$. Construct $U_B \\in \\mathbb{R}^{n \\times k_B}$ with independent standard normal entries and full column rank. Let $Q \\in \\mathbb{R}^{n \\times n}$ be an orthogonal matrix whose first $k_B$ columns span the column space of $U_B$ (for example, from a complete QR factorization). Set $V_A = Q[:, k_B:(k_B+k_A)]$ so that $V_A^T U_B = 0$ and hence $A B = 0$ by construction.\n- Test case $4$ (general, $k_A > k_B$): $m=5$, $n=5$, $p=5$, $k_A=4$, $k_B=3$. Construct $U_A$, $V_A$, $U_B$, $V_B$ with independent standard normal entries using random seed $3$.\n\nFor each test case $i \\in \\{1,2,3,4\\}$, compute:\n- $e_i = \\lVert A B_{\\text{fast}} - A B_{\\text{naive}} \\rVert_F$,\n- $r_i = \\operatorname{rank}_{\\text{num}}(A B)$ under the specified tolerance,\n- $b_i = \\min(k_A, k_B)$,\n- $q_i =$ the boolean value of the predicate $r_i \\le b_i$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list must be the concatenation $[e_1, r_1, b_1, q_1, e_2, r_2, b_2, q_2, e_3, r_3, b_3, q_3, e_4, r_4, b_4, q_4]$.\n- All floating-point values must be printed in standard Python format; no special rounding is required.\n- No physical units, angles, or percentages are involved in this problem.",
            "solution": "The problem requires the design and validation of an efficient algorithm for multiplying two matrices, $A$ and $B$, that are given in low-rank factored form. We must justify the algorithm's correctness, derive a bound on the rank of the product matrix $A B$, and implement the algorithm for numerical verification.\n\n### Correctness of the Algorithm\n\nWe are given two matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$ with their respective low-rank factorizations:\n$$\nA = U_A V_A^T \\quad \\text{where } U_A \\in \\mathbb{R}^{m \\times k_A}, V_A \\in \\mathbb{R}^{n \\times k_A}\n$$\n$$\nB = U_B V_B^T \\quad \\text{where } U_B \\in \\mathbb{R}^{n \\times k_B}, V_B \\in \\mathbb{R}^{p \\times k_B}\n$$\nThe product $A B$ can be written by substituting these factorizations:\n$$\nA B = (U_A V_A^T) (U_B V_B^T)\n$$\nMatrix multiplication is associative, a fundamental property which states that for any three conformable matrices $X$, $Y$, and $Z$, the equality $(XY)Z = X(YZ)$ holds. We can apply this property to the four-matrix product $U_A V_A^T U_B V_B^T$. We can group the matrices as follows:\n$$\nA B = (U_A V_A^T) (U_B V_B^T) = U_A (V_A^T U_B V_B^T) = U_A (V_A^T U_B) V_B^T\n$$\nThis confirms the correctness of the proposed algorithm, which computes the product by first evaluating the central, smaller matrix product $C = V_A^T U_B$, and then computing $A B = U_A C V_B^T$.\n\nThe dimensions of the matrices involved are: $U_A \\in \\mathbb{R}^{m \\times k_A}$, $V_A^T \\in \\mathbb{R}^{k_A \\times n}$, $U_B \\in \\mathbb{R}^{n \\times k_B}$, and $V_B^T \\in \\mathbb{R}^{k_B \\times p}$.\nThe naive approach involves first forming the full matrices $A$ (costing $O(mnk_A)$ operations) and $B$ (costing $O(npk_B)$ operations), and then multiplying them (costing $O(mnp)$ operations). The total complexity is dominated by $O(mnp)$ for large dimensions.\nThe proposed \"fast\" algorithm computes:\n$1$. $C = V_A^T U_B$. This is a $(k_A \\times n) \\times (n \\times k_B)$ product, resulting in a $k_A \\times k_B$ matrix, costing $O(k_A n k_B)$ operations.\n$2$. $D = U_A C$. This is a $(m \\times k_A) \\times (k_A \\times k_B)$ product, resulting in a $m \\times k_B$ matrix, costing $O(m k_A k_B)$ operations.\n$3$. $AB = D V_B^T$. This is a $(m \\times k_B) \\times (k_B \\times p)$ product, costing $O(m k_B p)$ operations.\nThe total complexity is $O(k_A n k_B + m k_A k_B + m k_B p)$. When the ranks $k_A$ and $k_B$ are much smaller than the matrix dimensions $m, n, p$, this approach is significantly more efficient than the naive method.\n\n### Derivation of the Rank Bound\n\nWe are tasked to prove that $\\operatorname{rank}(A B) \\le \\min(k_A, k_B)$ using the submultiplicative property of rank, $\\operatorname{rank}(XY) \\le \\min\\{\\operatorname{rank}(X), \\operatorname{rank}(Y)\\}$, and the definition of rank.\n\nThe rank of a matrix is the dimension of its column space. The column space of a product of matrices $XY$ is a subspace of the column space of $X$, i.e., $\\operatorname{Col}(XY) \\subseteq \\operatorname{Col}(X)$. This directly implies $\\operatorname{rank}(XY) \\le \\operatorname{rank}(X)$.\n\nConsider the matrix $A = U_A V_A^T$. The columns of $A$ are linear combinations of the columns of $U_A$. Therefore, the column space of $A$ is a subspace of the column space of $U_A$.\n$$\n\\operatorname{Col}(A) \\subseteq \\operatorname{Col}(U_A)\n$$\nThis implies that the dimension of $\\operatorname{Col}(A)$ is at most the dimension of $\\operatorname{Col}(U_A)$.\n$$\n\\operatorname{rank}(A) \\le \\operatorname{rank}(U_A)\n$$\nThe matrix $U_A \\in \\mathbb{R}^{m \\times k_A}$ has $k_A$ columns. The space spanned by these columns can have a dimension of at most $k_A$. Thus, $\\operatorname{rank}(U_A) \\le k_A$.\nCombining these, we get:\n$$\n\\operatorname{rank}(A) \\le k_A\n$$\nThis can also be understood by viewing $A$ as a sum of $k_A$ rank-$1$ matrices (outer products), $A = \\sum_{i=1}^{k_A} u_i v_i^T$, where $u_i$ and $v_i$ are the columns of $U_A$ and $V_A$ respectively. The rank of a sum of matrices is at most the sum of their ranks, so $\\operatorname{rank}(A) \\le \\sum_{i=1}^{k_A} \\operatorname{rank}(u_i v_i^T) = \\sum_{i=1}^{k_A} 1 = k_A$.\n\nBy the same reasoning for matrix $B = U_B V_B^T$, where $U_B \\in \\mathbb{R}^{n \\times k_B}$, we have:\n$$\n\\operatorname{rank}(B) \\le \\operatorname{rank}(U_B) \\le k_B\n$$\nNow, we apply the given submultiplicative rank inequality to the product $A B$:\n$$\n\\operatorname{rank}(A B) \\le \\min\\{\\operatorname{rank}(A), \\operatorname{rank}(B)\\}\n$$\nSubstituting the bounds we derived for $\\operatorname{rank}(A)$ and $\\operatorname{rank}(B)$:\n$$\n\\operatorname{rank}(A B) \\le \\min\\{k_A, k_B\\}\n$$\nThis completes the derivation.\n\n### Numerical Rank Calculation\n\nThe numerical rank of a matrix is determined via its singular value decomposition (SVD). For a matrix $C \\in \\mathbb{R}^{m \\times p}$ with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$, the numerical rank is the number of singular values that are greater than a specified tolerance $\\tau$. The problem defines this tolerance as:\n$$\n\\tau = \\max\\{m, p\\} \\cdot \\epsilon \\cdot \\sigma_{\\max}\n$$\nwhere $\\sigma_{\\max} = \\sigma_1$ is the largest singular value of $C$ and $\\epsilon$ is the machine epsilon for double-precision floating-point arithmetic. The numerical rank $r$ is then calculated as the count of indices $i$ for which $\\sigma_i > \\tau$. This procedure provides a robust way to estimate the effective rank of a matrix in the presence of floating-point inaccuracies.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _process_case(m, n, p, k_A, k_B, seed, special_case_3=False):\n    \"\"\"\n    Generates matrices for a test case, computes the product AB via two methods,\n    and returns the required analysis quadruple.\n    \n    Args:\n        m, n, p (int): Dimensions of matrices A (m,n) and B (n,p).\n        k_A, k_B (int): Ranks of the low-rank factors.\n        seed (int): Random seed for reproducibility.\n        special_case_3 (bool): Flag to trigger special construction for Test Case 3.\n        \n    Returns:\n        tuple: (error, num_rank, rank_bound, inequality_holds)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Construct the low-rank factors U_A, V_A, U_B, V_B\n    if not special_case_3:\n        U_A = rng.standard_normal(size=(m, k_A))\n        V_A = rng.standard_normal(size=(n, k_A))\n        U_B = rng.standard_normal(size=(n, k_B))\n        V_B = rng.standard_normal(size=(p, k_B))\n    else:\n        # Special construction for Test Case 3 to ensure V_A.T @ U_B = 0.\n        U_A = rng.standard_normal(size=(m, k_A))\n        V_B = rng.standard_normal(size=(p, k_B))\n        U_B = rng.standard_normal(size=(n, k_B))\n        \n        # Use complete QR decomposition of U_B to find an orthogonal basis for R^n.\n        # np.linalg.qr with mode='complete' returns a square orthogonal matrix Q.\n        # The first k_B columns of Q form an orthonormal basis for Col(U_B).\n        Q, _ = np.linalg.qr(U_B, mode='complete')\n        \n        # V_A's columns are chosen from the orthogonal complement of Col(U_B).\n        # This ensures the columns of V_A are orthogonal to columns of U_B.\n        if n  k_B + k_A:\n            # This case should not happen with the given test parameters.\n            raise ValueError(\n                f\"Cannot construct V_A for special case 3: n={n} must be >= k_A+k_B={k_A+k_B}\"\n            )\n        V_A = Q[:, k_B : k_B + k_A]\n\n    # Fast computation: AB_fast = U_A @ (V_A.T @ U_B) @ V_B.T\n    intermediate_product = V_A.T @ U_B\n    AB_fast = U_A @ intermediate_product @ V_B.T\n\n    # Naive computation: AB_naive = (U_A @ V_A.T) @ (U_B @ V_B.T)\n    A = U_A @ V_A.T\n    B = U_B @ V_B.T\n    AB_naive = A @ B\n\n    # 1. Compute Frobenius norm of the difference\n    e = np.linalg.norm(AB_fast - AB_naive, 'fro')\n\n    # 2. Compute the numerical rank of AB\n    singular_values = np.linalg.svd(AB_fast, compute_uv=False)\n    \n    sigma_max = 0.0\n    if singular_values.size > 0:\n        sigma_max = singular_values[0]\n\n    epsilon = np.finfo(float).eps\n    tolerance = max(m, p) * epsilon * sigma_max\n    \n    r = np.sum(singular_values > tolerance)\n\n    # 3. Compute the rank bound\n    b = min(k_A, k_B)\n\n    # 4. Check if the inequality rank(AB) = min(k_A, k_B) holds\n    q = (r = b)\n\n    return e, r, b, q\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (m, n, p, k_A, k_B, seed, special_case_3)\n        (7, 5, 6, 3, 2, 0, False),  # Test case 1\n        (8, 4, 3, 1, 1, 1, False),  # Test case 2\n        (6, 4, 5, 2, 2, 2, True),   # Test case 3\n        (5, 5, 5, 4, 3, 3, False),  # Test case 4\n    ]\n\n    results = []\n    for params in test_cases:\n        m, n, p, k_A, k_B, seed, special = params\n        result_tuple = _process_case(m, n, p, k_A, k_B, seed, special)\n        results.extend(list(result_tuple))\n\n    # Format the final output string as per requirements.\n    # The boolean values are converted to their string representations ('True'/'False').\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In practical applications, we often rely on fast heuristics to find low-rank approximations, as computing the full singular value decomposition (SVD) can be expensive. However, these heuristics are not always optimal. This problem presents a carefully designed thought experiment to expose the limitations of a common greedy method known as cross/skeleton approximation . By analyzing a specific rank-2 matrix where the heuristic fails dramatically, you will gain a deeper appreciation for the difference between a theoretically optimal approximation and a practically computed one.",
            "id": "3563748",
            "problem": "Consider the following construction designed to probe the limitations of cross or skeleton approximation driven by a maximum-volume heuristic. Let $n \\geq 3$ be an integer, and define vectors $u \\in \\mathbb{R}^{n}$ and $v \\in \\mathbb{R}^{n}$ by\n$$\nu \\;=\\; \\frac{1}{\\sqrt{n-1}}\\begin{pmatrix}0 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{pmatrix}, \n\\qquad\nv \\;=\\; \\frac{1}{\\sqrt{n-1}}\\begin{pmatrix}1 \\\\ \\vdots \\\\ 1 \\\\ 0\\end{pmatrix},\n$$\nso that $u$ has zero in its first entry and $v$ has zero in its $n$-th entry, with all other entries equal to $1/\\sqrt{n-1}$. Let $e_{1}, e_{n} \\in \\mathbb{R}^{n}$ denote the first and the $n$-th standard basis vectors, respectively. For scalars $s0$ and $t0$ satisfying\n$$\ns \\;\\; t \\;\\; \\frac{s}{\\,n-1\\,},\n$$\ndefine the matrix\n$$\nA \\;=\\; s\\,u\\,v^{\\top} \\;+\\; t\\,e_{1}\\,e_{n}^{\\top} \\;\\in\\; \\mathbb{R}^{n \\times n}.\n$$\nAssume a cross/skeleton rank-$1$ approximation is constructed by selecting one row and one column via the maximum-volume heuristic, which for $1 \\times 1$ submatrices reduces to choosing the entry of $A$ with maximal absolute value. Let $(i^{\\star}, j^{\\star})$ denote the index of that entry, and let the skeleton approximant be\n$$\n\\widetilde{A} \\;=\\; A(:, j^{\\star})\\,\\big(A(i^{\\star}, j^{\\star})\\big)^{-1}\\,A(i^{\\star}, :),\n$$\nwhere $A(:, j)$ denotes the $j$-th column of $A$ and $A(i, :)$ denotes the $i$-th row of $A$. Let $A_{1}$ denote a best rank-$1$ approximation to $A$ in the operator norm (spectral norm), understood through the theory of the singular value decomposition (SVD), without using any explicit formula in the problem statement.\n\nStarting from fundamental definitions of outer products, the operator norm, and the singular value decomposition (SVD), and using only the construction provided above, determine the error inflation factor\n$$\n\\mathcal{E}(n,s,t) \\;=\\; \\frac{\\|A - \\widetilde{A}\\|_{2}}{\\|A - A_{1}\\|_{2}},\n$$\nas a closed-form expression in $s$ and $t$. Your final answer must be a single analytic expression. Do not provide any inequalities or intermediate equations as the final answer. No rounding or numerical evaluation is required.",
            "solution": "We begin by analyzing the structure of the matrix $A = s\\,u\\,v^{\\top} + t\\,e_{1}\\,e_{n}^{\\top}$. By construction, $u$ and $e_{1}$ are orthogonal, since $u$ has first component equal to $0$, and $e_{1}$ has all components zero except the first. Similarly, $v$ and $e_{n}$ are orthogonal, since $v$ has $n$-th component equal to $0$, and $e_{n}$ has all components zero except the $n$-th. Moreover, both $u$ and $v$ are unit vectors because each has $(n-1)$ entries equal to $1/\\sqrt{n-1}$ and one entry equal to $0$, so $\\|u\\|_{2} = \\|v\\|_{2} = 1$. Likewise, $\\|e_{1}\\|_{2} = \\|e_{n}\\|_{2} = 1$.\n\nThe matrix $A$ is thus a sum of two rank-$1$ outer products whose left singular vectors are orthogonal ($u \\perp e_{1}$) and whose right singular vectors are orthogonal ($v \\perp e_{n}$). This orthogonality implies that the singular values of $A$ are precisely the singular values of each term independently, namely $s$ and $t$, together with zeros for the remaining $n-2$ singular values. This can be seen by considering orthonormal bases of $\\mathbb{R}^{n}$ for the left and right spaces that begin with $\\{u, e_{1}\\}$ and $\\{v, e_{n}\\}$, respectively, and extend to full orthonormal bases. In these bases, $A$ acts as\n$A\\,v = s\\,u, \\qquad A\\,e_{n} = t\\,e_{1},$\nand maps vectors orthogonal to $\\operatorname{span}\\{v, e_{n}\\}$ to the zero vector. Consequently, the two nonzero singular values of $A$ are $s$ and $t$.\n\nNext, we examine the maximum-volume heuristic for $1 \\times 1$ submatrices, which selects the largest-magnitude entry of $A$. The entries of $A$ arising from the term $s\\,u\\,v^{\\top}$ are\n$$\n(s\\,u\\,v^{\\top})_{ij} \\;=\\; s\\,u_{i}\\,v_{j} \\;=\\; \n\\begin{cases}\n\\displaystyle \\frac{s}{n-1},  \\text{if } i \\geq 2 \\text{ and } j \\leq n-1,\\\\[6pt]\n0,  \\text{if } i = 1 \\text{ or } j = n.\n\\end{cases}\n$$\nThe entries of $A$ arising from the term $t\\,e_{1}\\,e_{n}^{\\top}$ are zero everywhere except at $(1,n)$, where $A_{1n} = t$. Therefore, the largest-magnitude entry of $A$ is at $(1,n)$ provided $t > s/(n-1)$, which is guaranteed by the assumed inequality $s > t > s/(n-1)$. Hence, the maximum-volume heuristic selects $(i^{\\star}, j^{\\star}) = (1,n)$.\n\nThe skeleton approximant constructed from $(i^{\\star}, j^{\\star}) = (1,n)$ is\n$$\n\\widetilde{A} \\;=\\; A(:, n)\\,\\big(A(1, n)\\big)^{-1}\\,A(1, :) .\n$$\nWe compute these factors explicitly. Since $v_{n} = 0$, the column $A(:, n)$ receives no contribution from $s\\,u\\,v^{\\top}$, and the only contribution comes from $t\\,e_{1}\\,e_{n}^{\\top}$:\n$$\nA(:, n) \\;=\\; t\\,e_{1}.\n$$\nSimilarly, since $u_{1} = 0$, the row $A(1, :)$ receives no contribution from $s\\,u\\,v^{\\top}$ and only from $t\\,e_{1}\\,e_{n}^{\\top}$:\n$$\nA(1, :) \\;=\\; t\\,e_{n}^{\\top}.\n$$\nAlso, $A(1, n) = t$. Therefore,\n$$\n\\widetilde{A} \\;=\\; \\big(t\\,e_{1}\\big)\\,\\frac{1}{t}\\,\\big(t\\,e_{n}^{\\top}\\big) \\;=\\; t\\,e_{1}\\,e_{n}^{\\top}.\n$$\nWe see that the cross/skeleton approximant exactly reproduces the spike component $t\\,e_{1}\\,e_{n}^{\\top}$ and discards the dominant low-rank component $s\\,u\\,v^{\\top}$. Consequently, the approximation error is\n$$\nA - \\widetilde{A} \\;=\\; s\\,u\\,v^{\\top}.\n$$\nIts operator norm equals the singular value of $s\\,u\\,v^{\\top}$, which is $s$, because $\\|u\\|_{2} = \\|v\\|_{2} = 1$ and a rank-$1$ outer product $s\\,u\\,v^{\\top}$ has operator norm $s$.\n\nNow consider the best rank-$1$ approximation $A_{1}$ in operator norm. Since the singular values of $A$ are $s$ and $t$ with $s > t$, the best rank-$1$ approximation is obtained by retaining the singular value $s$ and its corresponding singular vectors, and the optimal error in operator norm equals the next singular value, namely $t$. Thus,\n$$\n\\|A - A_{1}\\|_{2} \\;=\\; t.\n$$\nFinally, the error inflation factor is\n$$\n\\mathcal{E}(n,s,t) \\;=\\; \\frac{\\|A - \\widetilde{A}\\|_{2}}{\\|A - A_{1}\\|_{2}} \\;=\\; \\frac{s}{t}.\n$$\nThis factor can be made arbitrarily large by choosing $s/t$ arbitrarily large, subject to the constraint $t > s/(n-1)$; for instance, for fixed $t$, taking $n$ sufficiently large allows $s$ to grow while maintaining $t > s/(n-1)$. The requested closed-form expression is therefore $\\mathcal{E}(n,s,t) = s/t$.",
            "answer": "$$\\boxed{\\frac{s}{t}}$$"
        }
    ]
}