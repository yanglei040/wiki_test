{
    "hands_on_practices": [
        {
            "introduction": "Many large-scale problems in scientific computing, such as those arising from the discretization of partial differential equations, result in linear systems with a special block structure. This exercise explores one of the most fundamental of these, the block bidiagonal system. By extending the familiar logic of scalar forward and backward substitution to matrix blocks, we can construct highly efficient direct solvers tailored to this structure. This practice builds foundational skills in deriving block algorithms and performing rigorous computational cost analysis, which is crucial for predicting and understanding algorithm performance .",
            "id": "3535162",
            "problem": "Let $p \\in \\mathbb{N}$ and consider a block lower bidiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n = \\sum_{i=1}^{p} n_i$, where each diagonal block $A_i \\in \\mathbb{R}^{n_i \\times n_i}$ is square and nonsingular, and each subdiagonal block $B_{i} \\in \\mathbb{R}^{n_{i+1} \\times n_{i}}$ is arbitrary and dense for $i = 1, \\dots, p-1$. The matrix $A$ has the block structure\n$$\nA \\;=\\;\n\\begin{pmatrix}\nA_1 \\\\\nB_1 & A_2 \\\\\n& B_2 & A_3 \\\\\n& & \\ddots & \\ddots \\\\\n& & & B_{p-1} & A_p\n\\end{pmatrix},\n$$\nand we wish to solve the block linear system $A x = b$ with $x = \\begin{pmatrix} x_1^{\\top} & \\cdots & x_p^{\\top} \\end{pmatrix}^{\\top}$ and $b = \\begin{pmatrix} b_1^{\\top} & \\cdots & b_p^{\\top} \\end{pmatrix}^{\\top}$, where $x_i, b_i \\in \\mathbb{R}^{n_i}$.\n\n1. Using only block partitioning and the definition of matrix-vector multiplication, derive the forward substitution recurrences for $x_i$ that solve $A x = b$ without forming $A^{-1}$.\n\n2. Assume that each dense block solve with $A_i$ is carried out by first computing a Gaussian elimination (LU) factorization without pivoting of $A_i$, followed by forward substitution and backward substitution to apply $A_i^{-1}$ to a single right-hand side. Adopt the following computational cost model: count one floating-point addition or multiplication as one floating-point operation (flop) and ignore the cost of divisions and comparisons. Derive, from first principles, an exact expression that upper-bounds the total number of flops required to compute $x$ in terms of the block sizes $n_1, \\dots, n_p$.\n\n3. Consider instead an upper block bidiagonal system with the same diagonal blocks $A_i \\in \\mathbb{R}^{n_i \\times n_i}$ and arbitrary dense superdiagonal blocks $C_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}$, and derive the backward substitution recurrences that solve $A x = b$. Do not assume any special structure other than the stated block positions.\n\nYour final answer must be a single closed-form analytic expression for the flop bound requested in item $2$, expressed solely in terms of $n_1, \\dots, n_p$. Do not provide intermediate steps in the final answer.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, complete, and consistent. It represents a standard analysis in numerical linear algebra. Therefore, the problem is valid, and a solution will be provided.\n\nThe problem consists of three parts. We will address them in sequence.\n\n**Part 1: Forward Substitution Recurrences for a Block Lower Bidiagonal System**\n\nWe are given the block linear system $A x = b$, where $A$ is a block lower bidiagonal matrix of the form:\n$$\nA \\;=\\;\n\\begin{pmatrix}\nA_1 & 0 & \\cdots & & 0 \\\\\nB_1 & A_2 & 0 & \\cdots & 0 \\\\\n0 & B_2 & A_3 & & \\vdots \\\\\n\\vdots & & \\ddots & \\ddots & 0 \\\\\n0 & \\cdots & 0 & B_{p-1} & A_p\n\\end{pmatrix}\n$$\nThe vectors $x$ and $b$ are partitioned conformally with $A$:\n$$\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p \\end{pmatrix}, \\quad b = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_p \\end{pmatrix}\n$$\nwhere $A_i \\in \\mathbb{R}^{n_i \\times n_i}$, $B_i \\in \\mathbb{R}^{n_{i+1} \\times n_i}$, $x_i, b_i \\in \\mathbb{R}^{n_i}$, and $n = \\sum_{i=1}^{p} n_i$.\n\nBy performing the block matrix-vector multiplication $A x$, we obtain a system of block equations:\n\\begin{align*}\nA_1 x_1 &= b_1 \\\\\nB_1 x_1 + A_2 x_2 &= b_2 \\\\\nB_2 x_2 + A_3 x_3 &= b_3 \\\\\n&\\vdots \\\\\nB_{i-1} x_{i-1} + A_i x_i &= b_i \\\\\n&\\vdots \\\\\nB_{p-1} x_{p-1} + A_p x_p &= b_p\n\\end{align*}\n\nThis system exhibits a sequential dependency, allowing for a forward substitution procedure to solve for the block vectors $x_i$.\n\nFrom the first equation, since $A_1$ is nonsingular, we can solve for $x_1$:\n$$ x_1 = A_1^{-1} b_1 $$\n\nUsing this result for $x_1$, we can substitute it into the second equation and solve for $x_2$:\n$$ B_1 x_1 + A_2 x_2 = b_2 \\implies A_2 x_2 = b_2 - B_1 x_1 $$\nSince $A_2$ is nonsingular, we have:\n$$ x_2 = A_2^{-1} (b_2 - B_1 x_1) $$\n\nThis pattern continues. Assuming $x_1, x_2, \\dots, x_{i-1}$ are known, we can solve for $x_i$ using the $i$-th equation:\n$$ B_{i-1} x_{i-1} + A_i x_i = b_i \\implies A_i x_i = b_i - B_{i-1} x_{i-1} $$\nSince $A_i$ is nonsingular for all $i=1, \\dots, p$, we can write the general recurrence relation for $x_i$:\n$$ x_i = A_i^{-1} (b_i - B_{i-1} x_{i-1}) \\quad \\text{for } i=2, \\dots, p $$\n\nThe complete forward substitution recurrence is:\n1.  Compute $x_1 = A_1^{-1} b_1$.\n2.  For $i = 2, \\dots, p$, compute $x_i = A_i^{-1} (b_i - B_{i-1} x_{i-1})$.\n\n**Part 2: Flop Count Analysis**\n\nWe derive an expression for the total number of floating-point operations (flops) required to solve the system using the recurrences from Part 1, following the specified cost model (add/mult = $1$ flop, divisions/comparisons are free). This requires deriving the flop counts for the constituent operations from first principles.\n\n**Elementary Operation Costs:**\nLet $M$ be a dense $m \\times m$ matrix.\n1.  **LU Factorization of $M$ (without pivoting):** The factorization proceeds in $m-1$ steps. At step $k$ ($k=1, \\dots, m-1$), we update an $(m-k) \\times (m-k)$ submatrix. Each of the $(m-k)^2$ elements is updated via one multiplication and one subtraction ($M_{ij} \\leftarrow M_{ij} - L_{ik} M_{kj}$). This requires $2(m-k)^2$ flops. The divisions to compute the multipliers $L_{ik}$ are ignored. The total flop count is:\n    $$ F_{LU}(m) = \\sum_{k=1}^{m-1} 2(m-k)^2 = 2 \\sum_{j=1}^{m-1} j^2 = 2 \\frac{(m-1)m(2m-1)}{6} = \\frac{m(m-1)(2m-1)}{3} = \\frac{2m^3 - 3m^2 + m}{3} $$\n2.  **Forward Substitution ($Ly=c$):** For a unit lower triangular $L$, $y_i = c_i - \\sum_{j=1}^{i-1} L_{ij} y_j$. For each $i=2, \\dots, m$, this involves $i-1$ multiplications and $i-1$ additions/subtractions. The total flop count is:\n    $$ F_{FS}(m) = \\sum_{i=2}^{m} 2(i-1) = 2 \\sum_{j=1}^{m-1} j = 2 \\frac{(m-1)m}{2} = m(m-1) = m^2-m $$\n3.  **Backward Substitution ($Ux=y$):** For an upper triangular $U$, $x_i = (y_i - \\sum_{j=i+1}^{m} U_{ij} x_j)/U_{ii}$. For each $i=m-1, \\dots, 1$, this involves $m-i$ multiplications and $m-i$ subtractions. Divisions by $U_{ii}$ are ignored. The total flop count is:\n    $$ F_{BS}(m) = \\sum_{i=1}^{m-1} 2(m-i) = 2 \\sum_{j=1}^{m-1} j = m(m-1) = m^2-m $$\n4.  **Matrix-Vector Product ($z=Qv$):** Let $Q$ be $m \\times k$ and $v$ be $k \\times 1$. Each element $z_i = \\sum_{j=1}^{k} Q_{ij} v_j$ requires $k$ multiplications and $k-1$ additions. Total for one element is $2k-1$ flops. For all $m$ elements of $z$, the count is:\n    $$ F_{MV}(m, k) = m(2k-1) $$\n5.  **Vector Subtraction ($u-w$):** For $u, w \\in \\mathbb{R}^m$, this requires $m$ flops.\n\n**Total Flop Count Derivation:**\nThe forward substitution algorithm involves a sequence of steps for $i=1, \\dots, p$.\n\nFor $i=1$: We solve $A_1 x_1 = b_1$. The cost involves LU factorization of $A_1$ followed by forward and backward substitution.\n$$\n\\text{Cost(1)} = F_{LU}(n_1) + F_{FS}(n_1) + F_{BS}(n_1) = \\frac{n_1(n_1-1)(2n_1-1)}{3} + n_1(n_1-1) + n_1(n_1-1)\n$$\n$$\n\\text{Cost(1)} = n_1(n_1-1)\\left(\\frac{2n_1-1}{3} + 2\\right) = \\frac{n_1(n_1-1)(2n_1+5)}{3} = \\frac{2n_1^3+3n_1^2-5n_1}{3}\n$$\n\nFor $i=2, \\dots, p$: We compute $x_i = A_i^{-1}(b_i - B_{i-1}x_{i-1})$. This breaks down into:\n(a) Compute the matrix-vector product $v = B_{i-1} x_{i-1}$. Here, $B_{i-1}$ is $n_i \\times n_{i-1}$.\n    Cost: $F_{MV}(n_i, n_{i-1}) = n_i(2n_{i-1}-1)$ flops.\n(b) Compute the updated right-hand side $\\tilde{b}_i = b_i - v$. This is a vector subtraction.\n    Cost: $n_i$ flops.\n(c) Solve the system $A_i x_i = \\tilde{b}_i$. The cost is identical in form to the $i=1$ case.\n    Cost: $F_{LU}(n_i) + F_{FS}(n_i) + F_{BS}(n_i) = \\frac{2n_i^3+3n_i^2-5n_i}{3}$ flops.\n\nThe total cost for step $i$ (where $i \\ge 2$) is the sum of these costs:\n$$\n\\text{Cost(i)} = \\left(n_i(2n_{i-1}-1) + n_i\\right) + \\frac{2n_i^3+3n_i^2-5n_i}{3} = 2n_i n_{i-1} + \\frac{2n_i^3+3n_i^2-5n_i}{3}\n$$\n\nThe total number of flops is the sum of costs for all steps:\n$$ F = \\text{Cost(1)} + \\sum_{i=2}^{p} \\text{Cost(i)} $$\n$$ F = \\left(\\frac{2n_1^3+3n_1^2-5n_1}{3}\\right) + \\sum_{i=2}^{p} \\left(2n_i n_{i-1} + \\frac{2n_i^3+3n_i^2-5n_i}{3}\\right) $$\nWe can group the terms:\n$$ F = \\sum_{i=1}^{p} \\left(\\frac{2n_i^3+3n_i^2-5n_i}{3}\\right) + \\sum_{i=2}^{p} (2n_i n_{i-1}) $$\nRe-indexing the second sum gives the final expression:\n$$ F = \\frac{1}{3}\\sum_{i=1}^{p} (2n_i^3 + 3n_i^2 - 5n_i) + 2\\sum_{i=1}^{p-1} n_{i+1} n_i $$\nThis is an exact expression for the number of flops under the specified model and for dense blocks, not an upper bound.\n\n**Part 3: Backward Substitution Recurrences for a Block Upper Bidiagonal System**\n\nNow consider an upper block bidiagonal matrix $A$:\n$$\nA \\;=\\;\n\\begin{pmatrix}\nA_1 & C_1 & 0 & \\cdots & 0 \\\\\n0 & A_2 & C_2 & & \\vdots \\\\\n\\vdots & & \\ddots & \\ddots & 0 \\\\\n0 & \\cdots & 0 & A_{p-1} & C_{p-1} \\\\\n0 & \\cdots & \\cdots & 0 & A_p\n\\end{pmatrix}\n$$\nwhere $C_i \\in \\mathbb{R}^{n_i \\times n_{i+1}}$. The system $A x = b$ expands to:\n\\begin{align*}\nA_1 x_1 + C_1 x_2 &= b_1 \\\\\nA_2 x_2 + C_2 x_3 &= b_2 \\\\\n&\\vdots \\\\\nA_i x_i + C_i x_{i+1} &= b_i \\\\\n&\\vdots \\\\\nA_{p-1} x_{p-1} + C_{p-1} x_p &= b_{p-1} \\\\\nA_p x_p &= b_p\n\\end{align*}\n\nThis structure suggests a backward substitution procedure, starting from the last block equation and working upwards.\n\nFrom the last equation, since $A_p$ is nonsingular, we solve for $x_p$:\n$$ x_p = A_p^{-1} b_p $$\n\nUsing this result for $x_p$, we can substitute it into the $(p-1)$-th equation to solve for $x_{p-1}$:\n$$ A_{p-1} x_{p-1} + C_{p-1} x_p = b_{p-1} \\implies A_{p-1} x_{p-1} = b_{p-1} - C_{p-1} x_p $$\nSince $A_{p-1}$ is nonsingular, we have:\n$$ x_{p-1} = A_{p-1}^{-1} (b_{p-1} - C_{p-1} x_p) $$\n\nThis reveals the general pattern. Assuming $x_{p}, x_{p-1}, \\dots, x_{i+1}$ are known, we can solve for $x_i$ using the $i$-th equation:\n$$ A_i x_i + C_i x_{i+1} = b_i \\implies A_i x_i = b_i - C_i x_{i+1} $$\nSince $A_i$ is nonsingular, the general recurrence is:\n$$ x_i = A_i^{-1} (b_i - C_i x_{i+1}) \\quad \\text{for } i=p-1, \\dots, 1 $$\n\nThe complete backward substitution recurrence is:\n1.  Compute $x_p = A_p^{-1} b_p$.\n2.  For $i = p-1$ down to $1$, compute $x_i = A_i^{-1} (b_i - C_i x_{i+1})$.",
            "answer": "$$\n\\boxed{\\frac{1}{3}\\sum_{i=1}^{p} (2n_i^3 + 3n_i^2 - 5n_i) + 2\\sum_{i=1}^{p-1} n_{i+1} n_i}\n$$"
        },
        {
            "introduction": "Often, the initial representation of a matrix problem obscures a simpler, underlying structure that can be exploited for efficient computation. This exercise reveals the profound connection between the sparsity pattern of a matrix and the connectivity of its associated directed graph, where a matrix is reducible to block triangular form if its graph is not strongly connected. By identifying these graph components, you will learn how to strategically reorder a matrix to reveal a block triangular form, a transformation that can dramatically reduce the computational cost of solving linear systems .",
            "id": "3535135",
            "problem": "Let $A \\in \\mathbb{R}^{6 \\times 6}$ be the sparse matrix depending on a small parameter $\\varepsilon > 0$,\n$$\nA \\;=\\;\n\\begin{pmatrix}\n4 & \\varepsilon & 1 & 0 & 2\\varepsilon & 0 \\\\\n0 & 5 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 3 & \\varepsilon & 0 & 3\\varepsilon \\\\\n0 & 1 & 0 & 6 & 2 & 0 \\\\\n0 & 0 & 0 & 2 & 7 & 3 \\\\\n0 & 0 & 0 & 0 & 3 & 8\n\\end{pmatrix}.\n$$\nThis matrix encodes a directed graph via the rule: there is a directed edge $i \\to j$ if and only if $A_{ij} \\neq 0$. Consider the decomposition of this directed graph into strongly connected components, and the condensation into a Directed Acyclic Graph (DAG), which is the graph obtained by contracting each strongly connected component into a single vertex and retaining edges between components induced by edges of the original graph.\n\n- Using only the definitions of the directed graph of a matrix, strongly connected components, and the Directed Acyclic Graph arising from graph condensation, derive the necessary and sufficient conditions on the sparsity pattern of $A$ under which there exists a permutation matrix $P$ such that $P A P^{\\top}$ is block upper triangular. Interpret these conditions via the condensation DAG.\n\n- For the particular matrix $A$ above, identify the strongly connected components of its directed graph, construct an explicit permutation matrix $P$ that renders $P A P^{\\top}$ block upper triangular, and exhibit the block structure\n$$\nP A P^{\\top} \\;=\\;\n\\begin{pmatrix}\nB & E \\\\\n0 & C\n\\end{pmatrix},\n$$\nwith $B \\in \\mathbb{R}^{2 \\times 2}$, $C \\in \\mathbb{R}^{4 \\times 4}$, and $E \\in \\mathbb{R}^{2 \\times 4}$.\n\n- In numerical linear algebra, one often compares the operation counts of different factorization strategies. Consider two strategies for factorizing $A$: dense Gaussian elimination with partial pivoting on the full $6 \\times 6$ matrix, and a block strategy that, for a nearly reducible matrix with couplings of magnitude at most $\\varepsilon$, performs separate dense factorizations of the diagonal blocks $B$ and $C$ and neglects the strictly upper block $E$ at first order. Using well-tested flop count formulas for dense LU factorization, compute the leading-order flop count reduction factor $R$ in the limit $\\varepsilon \\to 0$, defined as\n$$\nR \\;=\\; \\frac{\\text{flops of block-diagonal factorization of } B \\text{ and } C}{\\text{flops of dense LU on } A}.\n$$\nExpress $R$ as a single exact number. No rounding is required.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n-   A sparse matrix $A \\in \\mathbb{R}^{6 \\times 6}$ depending on a small parameter $\\varepsilon > 0$:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n4 & \\varepsilon & 1 & 0 & 2\\varepsilon & 0 \\\\\n0 & 5 & 0 & 1 & 0 & 0 \\\\\n1 & 0 & 3 & \\varepsilon & 0 & 3\\varepsilon \\\\\n0 & 1 & 0 & 6 & 2 & 0 \\\\\n0 & 0 & 0 & 2 & 7 & 3 \\\\\n0 & 0 & 0 & 0 & 3 & 8\n\\end{pmatrix}.\n$$\n-   Rule for directed graph construction: an edge $i \\to j$ exists if and only if $A_{ij} \\neq 0$.\n-   Concepts to be used: strongly connected components (SCCs), condensation into a Directed Acyclic Graph (DAG).\n-   Task 1: Derive necessary and sufficient conditions on the sparsity pattern of $A$ for a permutation matrix $P$ to exist such that $P A P^{\\top}$ is block upper triangular, and interpret these conditions via the condensation DAG.\n-   Task 2: For the given matrix $A$, identify SCCs, construct an explicit permutation matrix $P$, and exhibit the block structure $P A P^{\\top} = \\begin{pmatrix} B & E \\\\ 0 & C \\end{pmatrix}$ with $B \\in \\mathbb{R}^{2 \\times 2}$, $C \\in \\mathbb{R}^{4 \\times 4}$, $E \\in \\mathbb{R}^{2 \\times 4}$.\n-   Task 3: Compute the leading-order flop count reduction factor $R$, defined as the ratio of flops for block-diagonal factorization of $B$ and $C$ to the flops for dense LU factorization on the full matrix $A$. The flop count formula for dense LU on an $n \\times n$ matrix is taken as $\\frac{2}{3}n^3$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is based on well-established principles of numerical linear algebra and graph theory, including matrix-graph duality, strong connectivity, and complexity analysis of algorithms like LU factorization. All concepts are standard and scientifically sound.\n-   **Well-Posed**: The problem is clearly stated and structured. Each task requests a specific, derivable result. The data provided is sufficient to complete all tasks.\n-   **Objective**: The problem is stated in precise, formal mathematical language, free from any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution is presented in three parts, corresponding to the tasks in the problem statement.\n\n**Part 1: Conditions for Block Triangularization**\n\nA square matrix $M \\in \\mathbb{R}^{n \\times n}$ is called **reducible** if there exists a permutation matrix $P$ such that $P M P^{\\top}$ is block upper triangular. If no such permutation exists, the matrix is **irreducible**.\n\nThe key to understanding reducibility lies in the directed graph $G(M)$ associated with the matrix. The vertices of $G(M)$ are the integers $\\{1, 2, \\dots, n\\}$, and a directed edge exists from vertex $i$ to vertex $j$ (denoted $i \\to j$) if and only if the entry $M_{ij} \\neq 0$. The set of non-zero entries defines the **sparsity pattern** of the matrix.\n\nA fundamental theorem in numerical linear algebra states that a matrix $M$ is irreducible if and only if its associated directed graph $G(M)$ is **strongly connected**. A directed graph is strongly connected if for every ordered pair of vertices $(i, j)$, there is a path of directed edges from $i$ to $j$.\n\nConsequently, a matrix $M$ is reducible if and only if its directed graph $G(M)$ is **not** strongly connected. Therefore, the necessary and sufficient condition on the sparsity pattern of a matrix for it to be permutable to a block upper triangular form is that the directed graph defined by its non-zero entries is not strongly connected.\n\nThe interpretation of this condition via the condensation DAG is as follows. If a graph $G(M)$ is not strongly connected, its vertex set can be uniquely partitioned into a set of strongly connected components (SCCs), say $\\{S_1, S_2, \\dots, S_k\\}$ with $k > 1$. The **condensation graph** is formed by contracting each SCC $S_i$ into a single super-vertex. A directed edge exists from super-vertex $S_i$ to $S_j$ if and only if there is an edge in the original graph $G(M)$ from a vertex in $S_i$ to a vertex in $S_j$. By construction, this condensation graph is a Directed Acyclic Graph (DAG).\n\nSince the condensation graph is a DAG and has more than one vertex (as $k>1$), its vertices can be topologically sorted. A topological sort provides an ordering of the super-vertices, say $(S_{\\pi(1)}, S_{\\pi(2)}, \\dots, S_{\\pi(k)})$, such that if there is an edge from $S_{\\pi(i)}$ to $S_{\\pi(j)}$, then $i < j$.\n\nA permutation matrix $P$ that transforms $M$ into block upper triangular form is constructed by reordering the basis vectors (and thus the rows and columns of $M$) according to this topological sort. Specifically, one groups the vertices of $G(M)$ belonging to $S_{\\pi(1)}$, followed by those in $S_{\\pi(2)}$, and so on. The resulting permuted matrix $P M P^{\\top}$ will have zero blocks in its strict lower triangle because the topological sort ensures there are no edges from a later component $S_{\\pi(j)}$ to an earlier component $S_{\\pi(i)}$ (where $j>i$).\n\n**Part 2: Analysis of the Specific Matrix $A$**\n\nFirst, we identify the directed graph $G(A)$ for the given matrix $A$. The vertices are $\\{1, 2, 3, 4, 5, 6\\}$. The edges, determined by the non-zero entries $A_{ij}$ for $\\varepsilon > 0$, are:\n$1 \\to 1, 1 \\to 2, 1 \\to 3, 1 \\to 5$\n$2 \\to 2, 2 \\to 4$\n$3 \\to 1, 3 \\to 3, 3 \\to 4, 3 \\to 6$\n$4 \\to 2, 4 \\to 4, 4 \\to 5$\n$5 \\to 4, 5 \\to 5, 5 \\to 6$\n$6 \\to 5, 6 \\to 6$\n\nNext, we find the SCCs.\n- There is a path from vertex $1$ to $3$ ($1 \\to 3$ since $A_{13}=1$) and a path from $3$ to $1$ ($3 \\to 1$ since $A_{31}=1$). Thus, vertices $\\{1, 3\\}$ are in the same SCC. There are no paths from $\\{1, 3\\}$ to any other vertex and back. For instance, $1 \\to 2$, but there is no path from any other vertex back to $1$ or $3$. So, one SCC is $S_1 = \\{1, 3\\}$.\n- For the remaining vertices $\\{2, 4, 5, 6\\}$:\n    - $2 \\to 4$ ($A_{24}=1$) and $4 \\to 2$ ($A_{42}=1$), so $2$ and $4$ are mutually reachable.\n    - $4 \\to 5$ ($A_{45}=2$) and $5 \\to 4$ ($A_{54}=2$), so $4$ and $5$ are mutually reachable.\n    - $5 \\to 6$ ($A_{56}=3$) and $6 \\to 5$ ($A_{65}=3$), so $5$ and $6$ are mutually reachable.\n- By transitivity, all vertices in $\\{2, 4, 5, 6\\}$ are mutually reachable. Thus, the second SCC is $S_2 = \\{2, 4, 5, 6\\}$.\nThe SCCs are $S_1 = \\{1, 3\\}$ and $S_2 = \\{2, 4, 5, 6\\}$.\n\nNow, we construct the condensation DAG. There are edges from $S_1$ to $S_2$ (e.g., $1 \\to 2$ from $A_{12}=\\varepsilon$). There are no edges from $S_2$ to $S_1$. The condensation DAG is $S_1 \\to S_2$.\n\nA topological sort of the condensation DAG is $(S_1, S_2)$. To achieve a block upper triangular form, we reorder the vertices by grouping those in $S_1$ first, then those in $S_2$. A valid new ordering is $(1, 3, 2, 4, 5, 6)$.\nThe permutation that maps the standard basis $(e_1, e_2, e_3, e_4, e_5, e_6)$ to the new basis $(e_1, e_3, e_2, e_4, e_5, e_6)$ is achieved by a permutation matrix $P$ whose rows are the basis vectors of the new ordering. Specifically, the first row of $P$ is $e_1^\\top$, the second is $e_3^\\top$, the third is $e_2^\\top$, etc.\n$$\nP = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThis matrix swaps the 2nd and 3rd rows. Performing the similarity transformation $P A P^{\\top}$ (which swaps rows 2 and 3, and then columns 2 and 3) yields:\n$$\nP A P^{\\top} \\;=\\; \\begin{pmatrix}\n4 & 1 & \\varepsilon & 0 & 2\\varepsilon & 0 \\\\\n1 & 3 & 0 & \\varepsilon & 0 & 3\\varepsilon \\\\\n0 & 0 & 5 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 6 & 2 & 0 \\\\\n0 & 0 & 0 & 2 & 7 & 3 \\\\\n0 & 0 & 0 & 0 & 3 & 8\n\\end{pmatrix}\n$$\nThis matrix is in the required block upper triangular form:\n$$\nP A P^{\\top} \\;=\\;\n\\begin{pmatrix}\nB & E \\\\\n0 & C\n\\end{pmatrix}\n$$\nwhere the blocks are:\n$B = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n$C = \\begin{pmatrix} 5 & 1 & 0 & 0 \\\\ 1 & 6 & 2 & 0 \\\\ 0 & 2 & 7 & 3 \\\\ 0 & 0 & 3 & 8 \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 4}$\n$E = \\begin{pmatrix} \\varepsilon & 0 & 2\\varepsilon & 0 \\\\ 0 & \\varepsilon & 0 & 3\\varepsilon \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 4}$\nThe bottom-left $4 \\times 2$ block is a zero matrix, confirming the block upper triangular structure.\n\n**Part 3: Flop Count Reduction**\n\nThe number of floating-point operations (flops) for dense LU factorization of an $n \\times n$ matrix is dominated by the term $\\frac{2}{3}n^3$.\n\nFor the first strategy, dense LU factorization on the full $6 \\times 6$ matrix $A$, the flop count is:\n$$\n\\text{flops}_A = \\frac{2}{3} \\times 6^3 = \\frac{2}{3} \\times 216 = 2 \\times 72 = 144\n$$\n\nFor the second strategy, a block-diagonal factorization, we perform separate LU factorizations on the diagonal blocks $B$ and $C$.\n- The block $B$ is a $2 \\times 2$ matrix. The flop count for its factorization is:\n$$\n\\text{flops}_B = \\frac{2}{3} \\times 2^3 = \\frac{2}{3} \\times 8 = \\frac{16}{3}\n$$\n- The block $C$ is a $4 \\times 4$ matrix. The flop count for its factorization is:\n$$\n\\text{flops}_C = \\frac{2}{3} \\times 4^3 = \\frac{2}{3} \\times 64 = \\frac{128}{3}\n$$\nThe total flop count for the block strategy is the sum of the flops for each block:\n$$\n\\text{flops}_{\\text{block}} = \\text{flops}_B + \\text{flops}_C = \\frac{16}{3} + \\frac{128}{3} = \\frac{144}{3} = 48\n$$\n\nThe reduction factor $R$ is the ratio of the block factorization flops to the full matrix factorization flops:\n$$\nR = \\frac{\\text{flops}_{\\text{block}}}{\\text{flops}_A} = \\frac{48}{144} = \\frac{1}{3}\n$$",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "In many applications, one must solve a linear system $AX=B$ for multiple right-hand sides simultaneously, and subsequently refine the accuracy of the computed solution. This practice guides you in designing an advanced block iterative refinement scheme that elegantly handles this challenge. The key innovation is to compute the block residual $R = B - AX$, compress its structure using a low-rank factorization, and then solve a much smaller system for the correction, thereby accelerating the refinement for all solutions at once. This exercise demonstrates the power of synthesizing concepts like mixed-precision arithmetic, iterative methods, and low-rank approximation to create a robust and efficient modern numerical algorithm .",
            "id": "3535155",
            "problem": "Consider the block linear system $A X = B$ with $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$, where $m \\geq 1$ represents multiple right-hand sides. Iterative refinement is a procedure that seeks to improve an approximate solution $X$ by repeatedly computing and correcting the block residual $R = B - A X$. In this problem, you will design a block iterative refinement method that acts on all right-hand sides simultaneously and exploits block low-rank structure in the residuals to reduce computational cost and improve numerical robustness.\n\nThe fundamental base you must use includes: the definition of block matrices and their operations; the definition of the block residual $R = B - A X$; the concept of the Singular Value Decomposition (SVD), defined as follows. For any matrix $R \\in \\mathbb{R}^{n \\times m}$, there exist orthonormal matrices $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m \\times m}$, and a diagonal matrix $S \\in \\mathbb{R}^{n \\times m}$ with nonnegative diagonal entries (the singular values), such that $R = U S V^\\top$. You may also use the well-tested fact that solving linear systems via a precomputed factorization (such as a lower-upper decomposition) is more efficient than refactorizing in each iteration.\n\nYour task is to:\n- Derive from first principles how a block correction can be computed from a low-rank factorization of the block residual so that computing with all right-hand sides is reduced to a smaller number of solves with $A$.\n- Explain why compressing the residual improves robustness and can reduce cost when the residual has rapidly decaying singular values.\n- Implement a complete algorithm that:\n  1. Computes and stores a factorization of $A$ once, using lower numerical precision, and performs iterative refinement in higher precision on the residuals.\n  2. In each iteration, compresses the block residual by a rank-revealing factorization and applies a correction based on the compressed representation.\n  3. Uses a backward error-based stopping criterion. For the $j$-th column $x_j$ of $X$ and $b_j$ of $B$, the backward error is defined as $\\eta_j = \\dfrac{\\lVert r_j \\rVert_2}{\\lVert A \\rVert_2 \\lVert x_j \\rVert_2 + \\lVert b_j \\rVert_2}$, with $\\lVert A \\rVert_2$ the spectral norm and $r_j = b_j - A x_j$. The algorithm should stop when $\\max_j \\eta_j$ is below a prescribed tolerance.\n- Ensure all block operations are mathematically consistent and numerically well-posed.\n\nNo physical units or angle units are involved. All outputs must be purely numerical.\n\nYour program must implement the above algorithm and run the following test suite. For reproducibility, use the specified random seeds. In every case, compute and report the final maximum backward error $\\max_j \\eta_j$ after the algorithm terminates.\n\nTest suite:\n- Case 1 (happy path, low-rank right-hand sides and well-conditioned $A$):\n  - Dimensions: $n = 50$, $m = 20$.\n  - Matrix $A$: let $M \\in \\mathbb{R}^{n \\times n}$ have entries drawn from the standard normal distribution with seed $1$, and define $A = M^\\top M + n I$, where $I$ is the identity matrix.\n  - Block right-hand sides $B$: let $Q \\in \\mathbb{R}^{n \\times n}$ be the $Q$ factor from the QR decomposition of a standard normal random matrix with seed $2$. Let $U_b \\in \\mathbb{R}^{n \\times r}$ be the first $r = 3$ columns of $Q$. Let $V \\in \\mathbb{R}^{m \\times r}$ have entries drawn from the standard normal distribution with seed $3$. Define $B = U_b V^\\top$.\n  - Algorithm parameters: rank cap $k_{\\max} = 5$, SVD relative truncation threshold $\\tau = 10^{-12}$, stopping tolerance $\\varepsilon = 10^{-14}$, maximum iterations $N_{\\text{it}} = 5$.\n\n- Case 2 (boundary case, full-rank right-hand sides and moderately conditioned non-symmetric $A$):\n  - Dimensions: $n = 60$, $m = 60$.\n  - Matrix $A$: generate $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ as orthonormal matrices via QR decompositions of standard normal random matrices with seeds $7$ and $8$, respectively. Let the singular values $s_i$ be linearly spaced in $[0.05, 30]$ for $i = 1, \\dots, n$. Define $A = U \\operatorname{diag}(s) V^\\top$.\n  - Block right-hand sides $B$: entries drawn from the standard normal distribution with seed $4$.\n  - Algorithm parameters: rank cap $k_{\\max} = 15$, SVD relative truncation threshold $\\tau = 10^{-2}$, stopping tolerance $\\varepsilon = 10^{-12}$, maximum iterations $N_{\\text{it}} = 8$.\n\n- Case 3 (edge case, ill-conditioned $A$ and low-rank right-hand sides):\n  - Dimensions: $n = 20$, $m = 10$.\n  - Matrix $A$: Hilbert matrix, $A_{i j} = \\dfrac{1}{i + j - 1}$ for $i, j = 1, \\dots, n$.\n  - Block right-hand sides $B$: let $Q \\in \\mathbb{R}^{n \\times n}$ be the $Q$ factor from the QR decomposition of a standard normal random matrix with seed $5$. Let $U_b \\in \\mathbb{R}^{n \\times r}$ be the first $r = 2$ columns of $Q$. Let $V \\in \\mathbb{R}^{m \\times r}$ have entries drawn from the standard normal distribution with seed $6$. Define $B = U_b V^\\top$.\n  - Algorithm parameters: rank cap $k_{\\max} = 3$, SVD relative truncation threshold $\\tau = 10^{-6}$, stopping tolerance $\\varepsilon = 10^{-8}$, maximum iterations $N_{\\text{it}} = 20$.\n\nAlgorithmic requirements:\n- Compute and store a factorization of $A$ in lower precision and use it to perform the solves throughout the refinement. Compute residuals and rank-revealing factorizations in higher precision.\n- In each iteration, compress the block residual by a rank-revealing factorization and update the approximation in a way that reduces the number of solves with $A$ from $m$ to at most a truncated rank $k$.\n- Use the backward error criterion for stopping as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the final maximum backward error for one test case in the order Case 1, Case 2, Case 3 (e.g., \"[result_case1,result_case2,result_case3]\").",
            "solution": "We consider the block linear system $A X = B$ with $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$. The block residual is defined as $R = B - A X \\in \\mathbb{R}^{n \\times m}$. Iterative refinement proceeds by computing a correction $\\Delta X$ from the residual via the auxiliary system $A \\Delta X = R$ and updating $X \\leftarrow X + \\Delta X$. We present a block algorithm that compresses the residual via a low-rank factorization to reduce the number of solves, leveraging the coupling across right-hand sides.\n\nPrinciples and derivation:\n1. For each iteration, we start from the fundamental definition $R = B - A X$. The correction problem is $A \\Delta X = R$. If we were to solve with all $m$ right-hand sides directly, we would solve the system with $m$ columns at each iteration.\n\n2. To reduce cost, we use a rank-revealing factorization of the residual. A principled choice is the Singular Value Decomposition (SVD), which for $R \\in \\mathbb{R}^{n \\times m}$ yields $R = U S V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m \\times m}$ are orthonormal, and $S \\in \\mathbb{R}^{n \\times m}$ is diagonal with nonnegative singular values. Let us denote by $k$ the numerical rank chosen by truncation according to a criterion; that is, we write an approximation $R \\approx U_k S_k V_k^\\top$ where $U_k \\in \\mathbb{R}^{n \\times k}$ has orthonormal columns, $S_k \\in \\mathbb{R}^{k \\times k}$ is diagonal, and $V_k \\in \\mathbb{R}^{m \\times k}$ has orthonormal columns. This uses the well-tested fact that truncating small singular values yields a near-optimal low-rank approximation in the spectral or Frobenius norm.\n\n3. Substituting this compressed representation in the correction problem $A \\Delta X = R$ gives the approximate correction model $A \\Delta X \\approx U_k S_k V_k^\\top$. We use block operations to avoid $m$ solves. If we define $Y \\in \\mathbb{R}^{n \\times k}$ via the $k$-right-hand-side system $A Y = U_k$, then the correction can be assembled by the block multiplication $\\Delta X \\approx Y S_k V_k^\\top$. This follows from substituting $A Y = U_k$ into $A \\Delta X \\approx U_k S_k V_k^\\top$, yielding $A \\Delta X \\approx A Y S_k V_k^\\top$, hence $\\Delta X \\approx Y S_k V_k^\\top$.\n\n4. The computational benefit arises because solving $A Y = U_k$ requires only $k$ solves, and if $k \\ll m$ due to low-rank residual structure, we reduce per-iteration cost while still correcting all $m$ right-hand sides simultaneously. The robustness benefit comes from the SVD’s stability and the suppression of small singular components that can amplify rounding errors when applied directly.\n\n5. Iterative refinement often uses mixed precision to improve an approximate solution computed in lower precision by refining with higher precision residuals. We employ a lower precision factorization of $A$ to perform the solves and compute residuals and SVD in higher precision. This conforms to the well-tested strategy in numerical linear algebra where higher precision residual computation can correct lower precision solve errors.\n\n6. We use a backward error-based stopping criterion. For each column $j$, $r_j = b_j - A x_j$. The global backward error is $\\max_j \\eta_j$ with\n   $$\\eta_j = \\frac{\\lVert r_j \\rVert_2}{\\lVert A \\rVert_2 \\lVert x_j \\rVert_2 + \\lVert b_j \\rVert_2}.$$\n   Here $\\lVert A \\rVert_2$ is the spectral norm, which we compute as the largest singular value of $A$. This criterion ensures that the computed solution is an exact solution to a nearby problem with small relative perturbations in a block sense.\n\nAlgorithmic design:\n- Precompute a lower-upper (LU) factorization of $A$ in lower precision, then use this factorization for all solves to avoid refactorization. This is justified by the fact that the LU factorization enables efficient triangular solves for multiple right-hand sides.\n- Initialize $X$ by solving $A X^{(0)} \\approx B$ with the lower precision factorization.\n- For iterations $t = 0, 1, \\dots$, compute the high precision residual $R^{(t)} = B - A X^{(t)}$.\n- Compute an SVD of $R^{(t)}$ and choose a numerical rank $k \\leq k_{\\max}$ via truncation based on a relative threshold on singular values. This yields $U_k$, $S_k$, $V_k$.\n- Solve $A Y^{(t)} = U_k$ using the lower precision factorization. Assemble $\\Delta X^{(t)} = Y^{(t)} S_k V_k^\\top$ via block multiplications.\n- Update $X^{(t+1)} = X^{(t)} + \\Delta X^{(t)}$.\n- Evaluate the backward error and stop when $\\max_j \\eta_j \\leq \\varepsilon$ or when the iteration count reaches $N_{\\text{it}}$.\n\nNumerical considerations:\n- The rank truncation threshold should be relative, for instance retaining singular values $\\sigma_i$ such that $\\sigma_i \\geq \\tau \\sigma_1$, where $\\sigma_1$ is the largest singular value of the residual. This guards against dropping significant subspace components.\n- If the residual is numerically zero, the SVD will reveal all singular values near zero and the backward error will already be below tolerance.\n- Using the LU factorization computed once, solving $A Y = U_k$ for multiple columns $U_k$ is efficiently handled with block triangular solves.\n- The spectral norm $\\lVert A \\rVert_2$ is computed via a singular value decomposition of $A$ once, to avoid re-computation.\n\nTest suite realization:\n- Case 1 constructs a symmetric positive definite $A$ with good conditioning and a rank-$3$ block structure for $B$, expecting rapid convergence and effective low-rank compression.\n- Case 2 constructs a non-symmetric $A$ with controlled singular values and a full random $B$; the residual’s singular values are expected to decay moderately, and aggressive truncation tests the boundary case.\n- Case 3 uses a Hilbert matrix $A$, which is ill-conditioned, and rank-$2$ $B$. The lower precision factorization combined with higher precision residuals exercises robustness, and the refinement iterations attempt to reduce the backward error despite the conditioning.\n\nThe program implements the above, computes the final maximum backward errors for each case, and prints a single line with the three values in the specified format. All computations are unitless and purely numerical.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve, qr\n\ndef spectral_norm(A: np.ndarray) -> float:\n    # Compute spectral norm as largest singular value\n    # Use full SVD for robustness; for moderate n this is fine.\n    s = np.linalg.svd(A, compute_uv=False)\n    return float(s[0])\n\ndef build_case1():\n    # Case 1: n=50, m=20, SPD A, low-rank B\n    n, m = 50, 20\n    rng_M = np.random.default_rng(1)\n    M = rng_M.standard_normal((n, n))\n    A = M.T @ M + n * np.eye(n, dtype=np.float64)\n\n    rng_Q = np.random.default_rng(2)\n    Q_rand = rng_Q.standard_normal((n, n))\n    Q, _ = qr(Q_rand, mode='economic')\n    r = 3\n    U_b = Q[:, :r]\n\n    rng_V = np.random.default_rng(3)\n    V = rng_V.standard_normal((m, r))\n    B = U_b @ V.T\n\n    params = {\n        \"rank_cap\": 5,\n        \"svd_tol\": 1e-12,\n        \"tol\": 1e-14,\n        \"max_iter\": 5,\n    }\n    return A, B, params\n\ndef build_case2():\n    # Case 2: n=60, m=60, general A with controlled singular values, full-rank B\n    n, m = 60, 60\n\n    rng_U = np.random.default_rng(7)\n    U_rand = rng_U.standard_normal((n, n))\n    U_q, _ = qr(U_rand, mode='economic')\n\n    rng_V = np.random.default_rng(8)\n    V_rand = rng_V.standard_normal((n, n))\n    V_q, _ = qr(V_rand, mode='economic')\n\n    # Singular values linearly spaced between 0.05 and 30\n    svals = np.linspace(0.05, 30.0, n)\n    A = (U_q @ (np.diag(svals) @ V_q.T)).astype(np.float64)\n\n    rng_B = np.random.default_rng(4)\n    B = rng_B.standard_normal((n, m))\n\n    params = {\n        \"rank_cap\": 15,\n        \"svd_tol\": 1e-2,\n        \"tol\": 1e-12,\n        \"max_iter\": 8,\n    }\n    return A, B, params\n\ndef build_case3():\n    # Case 3: n=20, m=10, Hilbert A, low-rank B\n    n, m = 20, 10\n    # Hilbert matrix\n    i = np.arange(1, n + 1, dtype=np.float64)\n    j = np.arange(1, n + 1, dtype=np.float64)\n    A = 1.0 / (i[:, None] + j[None, :] - 1.0)\n\n    rng_Q = np.random.default_rng(5)\n    Q_rand = rng_Q.standard_normal((n, n))\n    Q, _ = qr(Q_rand, mode='economic')\n    r = 2\n    U_b = Q[:, :r]\n\n    rng_V = np.random.default_rng(6)\n    V = rng_V.standard_normal((m, r))\n    B = U_b @ V.T\n\n    params = {\n        \"rank_cap\": 3,\n        \"svd_tol\": 1e-6,\n        \"tol\": 1e-8,\n        \"max_iter\": 20,\n    }\n    return A, B, params\n\ndef block_iterative_refinement(A: np.ndarray,\n                               B: np.ndarray,\n                               rank_cap: int,\n                               svd_tol: float,\n                               tol: float,\n                               max_iter: int) -> float:\n    \"\"\"\n    Perform block iterative refinement with low-rank residual compression.\n\n    A: (n,n) float64\n    B: (n,m) float64\n    Returns the final maximum backward error across RHSs.\n    \"\"\"\n    n, m = B.shape\n    # Lower precision factorization\n    A32 = A.astype(np.float32)\n    lu, piv = lu_factor(A32)\n\n    # Initial solve in lower precision, cast to float64 for accumulation\n    X = lu_solve((lu, piv), B.astype(np.float32)).astype(np.float64)\n\n    # Precompute spectral norm for backward error\n    normA2 = spectral_norm(A)\n\n    # Iterative refinement\n    for it in range(max_iter):\n        R = B - A @ X  # residual in double precision\n\n        # Backward error computation\n        # Compute columnwise norms\n        r_norms = np.linalg.norm(R, axis=0)\n        x_norms = np.linalg.norm(X, axis=0)\n        b_norms = np.linalg.norm(B, axis=0)\n        # Avoid division by zero: if denominator is zero, set error as norm(r)\n        denom = normA2 * x_norms + b_norms\n        with np.errstate(divide='ignore', invalid='ignore'):\n            eta = np.where(denom > 0, r_norms / denom, r_norms)\n        max_eta = float(np.max(eta))\n        if max_eta <= tol:\n            break\n\n        # SVD of residual\n        # Use economical SVD to get U (n x min(n,m)), S (min(n,m)), Vh (min(n,m) x m)\n        U, S, Vh = np.linalg.svd(R, full_matrices=False)\n        # Determine truncated rank k: keep singular values above relative threshold, cap by rank_cap\n        if S.size == 0 or S[0] == 0.0:\n            # Residual is (near) zero; no correction needed\n            break\n        k_rel = int(np.sum(S >= svd_tol * S[0]))\n        k = max(1, min(rank_cap, k_rel))\n        U_k = U[:, :k]\n        S_k = S[:k]\n        Vh_k = Vh[:k, :]\n\n        # Solve A Y = U_k with lower precision factorization\n        Y = lu_solve((lu, piv), U_k.astype(np.float32)).astype(np.float64)\n        # Assemble correction: ΔX = Y S_k Vh_k\n        # Compute W = S_k[:,None] * Vh_k (k x m), then ΔX = Y (n x k) @ W (k x m)\n        W = (S_k[:, None] * Vh_k)\n        dX = Y @ W\n\n        # Update\n        X += dX\n\n    # Final backward error\n    R = B - A @ X\n    r_norms = np.linalg.norm(R, axis=0)\n    x_norms = np.linalg.norm(X, axis=0)\n    b_norms = np.linalg.norm(B, axis=0)\n    denom = normA2 * x_norms + b_norms\n    with np.errstate(divide='ignore', invalid='ignore'):\n        eta = np.where(denom > 0, r_norms / denom, r_norms)\n    max_eta = float(np.max(eta))\n    return max_eta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = []\n    cases.append(build_case1())\n    cases.append(build_case2())\n    cases.append(build_case3())\n\n    results = []\n    for (A, B, params) in cases:\n        res = block_iterative_refinement(\n            A=A,\n            B=B,\n            rank_cap=params[\"rank_cap\"],\n            svd_tol=params[\"svd_tol\"],\n            tol=params[\"tol\"],\n            max_iter=params[\"max_iter\"],\n        )\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}