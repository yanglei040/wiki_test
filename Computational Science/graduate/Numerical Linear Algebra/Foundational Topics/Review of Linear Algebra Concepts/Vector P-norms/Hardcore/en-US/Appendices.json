{
    "hands_on_practices": [
        {
            "introduction": "While we know that all norms on a finite-dimensional space are topologically equivalent, the specific constants governing their relationships are essential in numerical analysis and optimization. This exercise provides a hands-on derivation of these relationships by asking you to determine the optimal Lipschitz constants for the $\\ell_1$ and $\\ell_\\infty$ norm functions with respect to the $\\ell_1$ and $\\ell_2$ metrics. Successfully completing this will solidify your understanding of norm equivalence and the continuity properties of these fundamental functions. ",
            "id": "3600685",
            "problem": "Let $n \\in \\mathbb{N}$ and consider $\\mathbb{R}^{n}$ equipped with the vector $p$-norms $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$, and $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$. Define $f(x) = \\|x\\|_{1}$ and $g(x) = \\|x\\|_{\\infty}$. For arbitrary $x,y \\in \\mathbb{R}^{n}$, develop upper bounds on the absolute differences $|\\|x\\|_{1} - \\|y\\|_{1}|$ and $|\\|x\\|_{\\infty} - \\|y\\|_{\\infty}|$ expressed in terms of $\\|x-y\\|_{2}$ and $\\|x-y\\|_{1}$. Starting only from the axioms of norms and well-tested inequalities applicable to $\\mathbb{R}^{n}$ (such as the triangle inequality and the Cauchy–Schwarz inequality), prove that for each bound there exists a smallest constant $L$ (depending only on $n$) such that, for all $x,y \\in \\mathbb{R}^{n}$,\n$$\n|\\|x\\|_{1} - \\|y\\|_{1}| \\leq L \\,\\|x-y\\|_{p} \\quad \\text{and} \\quad |\\|x\\|_{\\infty} - \\|y\\|_{\\infty}| \\leq L \\,\\|x-y\\|_{p},\n$$\nwith $p \\in \\{1,2\\}$ as appropriate. Determine these optimal Lipschitz constants explicitly as functions of $n$, and justify that they are best possible (no smaller constant works for all $x,y$). Report your final answer as the ordered row $(L_{1,1}, L_{1,2}, L_{\\infty,1}, L_{\\infty,2})$, where $L_{\\alpha,p}$ denotes the smallest constant such that $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\,\\|x-y\\|_{p}$ holds for all $x,y \\in \\mathbb{R}^{n}$. Provide the exact expressions; no rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed question in numerical linear algebra concerning the Lipschitz continuity of vector norms, with all terms being formally defined and consistent with established mathematical principles. No scientific, logical, or factual flaws were detected.\n\nThe problem requires finding the smallest constants $L$, known as Lipschitz constants, for the functions $f(x) = \\|x\\|_{1}$ and $g(x) = \\|x\\|_{\\infty}$ on $\\mathbb{R}^{n}$. These constants are sought for inequalities of the form $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\|x-y\\|_{p}$, where $\\alpha \\in \\{1, \\infty\\}$ and $p \\in \\{1, 2\\}$.\n\nA fundamental property of any norm $\\|\\cdot\\|$ on a vector space is the reverse triangle inequality, which states that for any vectors $x$ and $y$:\n$$ |\\|x\\| - \\|y\\|| \\leq \\|x-y\\| $$\nThis is derived directly from the triangle inequality axiom of norms:\n$\\|x\\| = \\|x-y+y\\| \\leq \\|x-y\\| + \\|y\\| \\implies \\|x\\| - \\|y\\| \\leq \\|x-y\\|$.\n$\\|y\\| = \\|y-x+x\\| \\leq \\|y-x\\| + \\|x\\| \\implies \\|y\\| - \\|x\\| \\leq \\|y-x\\| = \\|x-y\\|$.\nCombining these two results yields $|\\|x\\| - \\|y\\|| \\leq \\|x-y\\|$.\n\nApplying this general principle to our specific norms, we have:\n$$ |\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq \\|x-y\\|_{\\alpha} $$\nThe problem is to find the best constant $L_{\\alpha,p}$ such that $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\|x-y\\|_{p}$. Using the reverse triangle inequality, we need to find the smallest constant $L_{\\alpha,p}$ such that:\n$$ \\|x-y\\|_{\\alpha} \\leq L_{\\alpha,p} \\|x-y\\|_{p} $$\nLet $z = x-y$. Since $x$ and $y$ are arbitrary vectors in $\\mathbb{R}^n$, $z$ can be any vector in $\\mathbb{R}^n$. The problem is therefore equivalent to finding the smallest constant $L_{\\alpha,p}$ such that for all $z \\in \\mathbb{R}^n$:\n$$ \\|z\\|_{\\alpha} \\leq L_{\\alpha,p} \\|z\\|_{p} $$\nThis constant is the operator norm of the identity map from the normed space $(\\mathbb{R}^n, \\|\\cdot\\|_p)$ to $(\\mathbb{R}^n, \\|\\cdot\\|_\\alpha)$. It is formally defined as:\n$$ L_{\\alpha,p} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\alpha}}{\\|z\\|_{p}} $$\nTo prove that a constant $L$ is indeed the supremum (i.e., the smallest possible constant), we must first establish that $\\|z\\|_{\\alpha} \\leq L \\|z\\|_{p}$ for all $z$, and then demonstrate that this bound is tight by finding a specific non-zero vector $z_0$ for which $\\|z_0\\|_{\\alpha} = L \\|z_0\\|_{p}$.\n\nWe will now determine the four constants $L_{1,1}$, $L_{1,2}$, $L_{\\infty,1}$, and $L_{\\infty,2}$.\n\n1.  **Determination of $L_{1,1}$**\n    We seek the constant $L_{1,1} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{1}}{\\|z\\|_{1}}$.\n    For any non-zero vector $z \\in \\mathbb{R}^n$, the ratio $\\frac{\\|z\\|_{1}}{\\|z\\|_{1}}$ is identically $1$.\n    Therefore, $L_{1,1} = 1$. The inequality is $|\\|x\\|_{1} - \\|y\\|_{1}| \\leq \\|x-y\\|_{1}$, which is the reverse triangle inequality for the $1$-norm itself. To confirm it is the optimal constant, let $x=(1,0,...,0)$ and $y=0$. Then $|\\|x\\|_1 - \\|y\\|_1| = 1$ and $\\|x-y\\|_1 = 1$, so any constant less than $1$ would fail. Thus, $L_{1,1}=1$.\n\n2.  **Determination of $L_{1,2}$**\n    We seek the constant $L_{1,2} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{1}}{\\|z\\|_{2}}$.\n    Let $z = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$. We have $\\|z\\|_1 = \\sum_{i=1}^n |z_i|$ and $\\|z\\|_2 = \\left(\\sum_{i=1}^n z_i^2\\right)^{1/2}$.\n    We apply the Cauchy–Schwarz inequality to the vectors $u = (1, 1, \\dots, 1)$ and $v = (|z_1|, |z_2|, \\dots, |z_n|)$.\n    The inequality states $(u \\cdot v)^2 \\leq \\|u\\|_2^2 \\|v\\|_2^2$.\n    Here, $u \\cdot v = \\sum_{i=1}^n 1 \\cdot |z_i| = \\|z\\|_1$.\n    $\\|u\\|_2^2 = \\sum_{i=1}^n 1^2 = n$.\n    $\\|v\\|_2^2 = \\sum_{i=1}^n |z_i|^2 = \\sum_{i=1}^n z_i^2 = \\|z\\|_2^2$.\n    Substituting these into the Cauchy-Schwarz inequality gives:\n    $$ (\\|z\\|_1)^2 \\leq n \\|z\\|_2^2 $$\n    Taking the square root of both sides, we obtain $\\|z\\|_1 \\leq \\sqrt{n} \\|z\\|_2$.\n    This implies that $\\frac{\\|z\\|_{1}}{\\|z\\|_{2}} \\leq \\sqrt{n}$ for all $z \\neq 0$, so $L_{1,2} \\leq \\sqrt{n}$.\n    To show this bound is optimal, we must find a vector for which equality holds. Equality in the Cauchy-Schwarz inequality holds if and only if the vectors are linearly dependent, i.e., $v = c \\cdot u$ for some scalar $c$. This means $|z_1| = |z_2| = \\dots = |z_n|$.\n    Let's choose $z_0 = (1, 1, \\dots, 1)$.\n    For this vector, $\\|z_0\\|_1 = \\sum_{i=1}^n |1| = n$.\n    And $\\|z_0\\|_2 = \\left(\\sum_{i=1}^n 1^2\\right)^{1/2} = \\sqrt{n}$.\n    The ratio for this vector is $\\frac{\\|z_0\\|_{1}}{\\|z_0\\|_{2}} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$.\n    Since we have found a vector that achieves the bound $\\sqrt{n}$, the supremum must be $\\sqrt{n}$.\n    Thus, $L_{1,2} = \\sqrt{n}$.\n\n3.  **Determination of $L_{\\infty,1}$**\n    We seek the constant $L_{\\infty,1} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\infty}}{\\|z\\|_{1}}$.\n    By definition, $\\|z\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |z_i|$ and $\\|z\\|_1 = \\sum_{i=1}^n |z_i|$.\n    Let $k$ be an index such that $|z_k| = \\|z\\|_{\\infty}$.\n    The sum is $\\|z\\|_1 = |z_1| + \\dots + |z_k| + \\dots + |z_n|$.\n    Since each $|z_i| \\geq 0$, the sum must be greater than or equal to any single term. In particular, $\\|z\\|_1 \\geq |z_k| = \\|z\\|_{\\infty}$.\n    So, $\\|z\\|_{\\infty} \\leq \\|z\\|_{1}$.\n    This implies $\\frac{\\|z\\|_{\\infty}}{\\|z\\|_{1}} \\leq 1$ for all $z \\neq 0$, so $L_{\\infty,1} \\leq 1$.\n    To show this bound is optimal, consider a standard basis vector, for example, $z_0 = e_1 = (1, 0, \\dots, 0)$.\n    For this vector, $\\|z_0\\|_{\\infty} = \\max\\{1, 0, \\dots, 0\\} = 1$.\n    And $\\|z_0\\|_1 = |1| + |0| + \\dots + |0| = 1$.\n    The ratio is $\\frac{\\|z_0\\|_{\\infty}}{\\|z_0\\|_{1}} = \\frac{1}{1} = 1$.\n    Since the bound is achieved, the supremum is $1$.\n    Thus, $L_{\\infty,1} = 1$.\n\n4.  **Determination of $L_{\\infty,2}$**\n    We seek the constant $L_{\\infty,2} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\infty}}{\\|z\\|_{2}}$.\n    Let $k$ be an index such that $|z_k| = \\|z\\|_{\\infty}$.\n    We have $\\|z\\|_{\\infty}^2 = |z_k|^2 = z_k^2$.\n    The squared $2$-norm is $\\|z\\|_2^2 = \\sum_{i=1}^n z_i^2 = z_1^2 + \\dots + z_k^2 + \\dots + z_n^2$.\n    Since each $z_i^2 \\geq 0$, the sum is greater than or equal to any single term: $\\|z\\|_2^2 \\geq z_k^2 = \\|z\\|_{\\infty}^2$.\n    Taking the square root (norms are non-negative), we get $\\|z\\|_2 \\geq \\|z\\|_{\\infty}$.\n    This implies $\\frac{\\|z\\|_{\\infty}}{\\|z\\|_{2}} \\leq 1$ for all $z \\neq 0$, so $L_{\\infty,2} \\leq 1$.\n    To show this bound is optimal, consider again the standard basis vector $z_0 = e_1 = (1, 0, \\dots, 0)$.\n    For this vector, $\\|z_0\\|_{\\infty} = \\max\\{1, 0, \\dots, 0\\} = 1$.\n    And $\\|z_0\\|_{2} = (1^2 + 0^2 + \\dots + 0^2)^{1/2} = 1$.\n    The ratio is $\\frac{\\|z_0\\|_{\\infty}}{\\|z_0\\|_{2}} = \\frac{1}{1} = 1$.\n    Since the bound is achieved, the supremum is $1$.\n    Thus, $L_{\\infty,2} = 1$.\n\nThe four optimal Lipschitz constants are:\n$L_{1,1} = 1$\n$L_{1,2} = \\sqrt{n}$\n$L_{\\infty,1} = 1$\n$L_{\\infty,2} = 1$\n\nThe final answer is the ordered row $(L_{1,1}, L_{1,2}, L_{\\infty,1}, L_{\\infty,2})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  \\sqrt{n}  1  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond continuity, the differentiability of norms is a property of paramount importance, especially in gradient-based optimization. This practice explores the critical difference between smooth and non-smooth norms by contrasting the infinitely differentiable squared Euclidean norm with the non-smooth $\\ell_1$-norm. You will derive the gradient for the smooth case and characterize the subdifferential for the non-smooth case, a concept that generalizes the derivative and is foundational to modern convex analysis. ",
            "id": "3600719",
            "problem": "Let $n \\in \\mathbb{N}$ and consider the function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$ defined by $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$, where $\\|x\\|_{2}$ denotes the Euclidean (or $2$-) norm and the underlying inner product on $\\mathbb{R}^{n}$ is the standard dot product. Using only the fundamental definitions of differentiability, the gradient, and the vector $p$-norms, derive the explicit form of the gradient $\\nabla f(x)$ and prove that the gradient mapping $x \\mapsto \\nabla f(x)$ is $L$-Lipschitz with respect to the $2$-norm for the smallest possible constant $L$. Separately, let $g : \\mathbb{R}^{n} \\to \\mathbb{R}$ be defined by $g(x) = \\|x\\|_{1}$, where $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. Analyze the differentiability properties of $g$ at points $x$ where one or more coordinates $x_{i}$ vanish, and characterize the set of all subgradients of $g$ at such points. Report the minimal Lipschitz constant $L$ for the gradient mapping of $f$ as your final answer. No rounding is required.",
            "solution": "The problem presents two distinct tasks concerning two functions, $f(x)$ and $g(x)$, defined on $\\mathbb{R}^{n}$. The first task is to find the gradient of $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$, and then determine the minimal Lipschitz constant for its gradient mapping. The second task is to analyze the differentiability of $g(x)=\\|x\\|_{1}$ and characterize its subdifferential. We will address each part systematically.\n\nFirst, let's analyze the function $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$.\nThe problem requires deriving the gradient $\\nabla f(x)$ using its fundamental definition. A function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ is differentiable at a point $x \\in \\mathbb{R}^{n}$ if there exists a vector, denoted $\\nabla f(x)$, such that for any displacement vector $h \\in \\mathbb{R}^{n}$, the following holds:\n$$f(x+h) = f(x) + \\langle \\nabla f(x), h \\rangle + o(\\|h\\|_{2})$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ is the standard dot product on $\\mathbb{R}^{n}$ and $o(\\|h\\|_{2})$ is a term that satisfies $\\lim_{\\|h\\|_{2} \\to 0} \\frac{o(\\|h\\|_{2})}{\\|h\\|_{2}} = 0$.\n\nLet's expand $f(x+h)$:\n$$f(x+h) = \\frac{1}{2}\\|x+h\\|_{2}^{2}$$\nBy the definition of the Euclidean norm in terms of the inner product, $\\|v\\|_{2}^{2} = \\langle v, v \\rangle$.\n$$f(x+h) = \\frac{1}{2}\\langle x+h, x+h \\rangle$$\nUsing the bilinearity of the inner product:\n$$f(x+h) = \\frac{1}{2}(\\langle x, x \\rangle + \\langle x, h \\rangle + \\langle h, x \\rangle + \\langle h, h \\rangle)$$\nSince the standard dot product is symmetric, $\\langle x, h \\rangle = \\langle h, x \\rangle$.\n$$f(x+h) = \\frac{1}{2}(\\langle x, x \\rangle + 2\\langle x, h \\rangle + \\langle h, h \\rangle)$$\n$$f(x+h) = \\frac{1}{2}\\|x\\|_{2}^{2} + \\langle x, h \\rangle + \\frac{1}{2}\\|h\\|_{2}^{2}$$\nRecognizing that $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$, we have:\n$$f(x+h) = f(x) + \\langle x, h \\rangle + \\frac{1}{2}\\|h\\|_{2}^{2}$$\nComparing this expression with the definition of differentiability, $f(x+h) = f(x) + \\langle \\nabla f(x), h \\rangle + o(\\|h\\|_{2})$, we can identify the linear term in $h$ as $\\langle x, h \\rangle$. This suggests that $\\nabla f(x) = x$. The remainder term is $\\frac{1}{2}\\|h\\|_{2}^{2}$. To confirm this identification, we must verify that this remainder is indeed $o(\\|h\\|_{2})$. We check the limit:\n$$\\lim_{\\|h\\|_{2} \\to 0} \\frac{\\frac{1}{2}\\|h\\|_{2}^{2}}{\\|h\\|_{2}} = \\lim_{\\|h\\|_{2} \\to 0} \\frac{1}{2}\\|h\\|_{2} = 0$$\nThe limit is $0$, so the remainder term is $o(\\|h\\|_{2})$. Thus, the gradient of $f(x)$ is $\\nabla f(x) = x$.\n\nNext, we must prove that the gradient mapping, which we denote as $G(x) = \\nabla f(x) = x$, is $L$-Lipschitz with respect to the $2$-norm for the smallest possible constant $L$. A mapping $G: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is $L$-Lipschitz continuous with respect to a norm $\\|\\cdot\\|$ if for all $x, y \\in \\mathbb{R}^{n}$, the following inequality holds:\n$$\\|G(x) - G(y)\\| \\le L \\|x - y\\|$$\nIn our case, the mapping is $G(x)=x$ and the norm is the $2$-norm. So we need to find the smallest $L \\ge 0$ such that:\n$$\\|x - y\\|_{2} \\le L \\|x - y\\|_{2}$$\nfor all $x, y \\in \\mathbb{R}^{n}$.\nIf $x=y$, the inequality becomes $0 \\le 0$, which is true for any $L$.\nIf $x \\neq y$, we have $\\|x-y\\|_{2} > 0$, and we can divide both sides by it to get:\n$$1 \\le L$$\nThis shows that the inequality holds for any constant $L \\ge 1$. The smallest such constant is $L=1$. To show that $L=1$ is minimal, we must show that for any $L'  1$, the condition is violated. If we choose $L'  1$, we can pick any two distinct points, e.g., $x=(1, 0, \\dots, 0)$ and $y=(0, \\dots, 0)$. Then $\\|x-y\\|_{2}=1$ and $\\|G(x)-G(y)\\|_{2} = \\|x-y\\|_{2} = 1$. The Lipschitz condition would require $1 \\le L' \\cdot 1$, which is $1 \\le L'$, contradicting $L'1$. Therefore, the smallest possible Lipschitz constant is $L=1$.\n\nNow, we turn to the second part of the problem, concerning the function $g(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\nFirst, let's analyze its differentiability. The function $g(x)$ is a sum of functions of the form $\\phi_i(x) = |x_i|$. The scalar function $\\phi(t)=|t|$ is differentiable everywhere except at $t=0$.\nA function of multiple variables is differentiable at a point if all its partial derivatives exist and are continuous in a neighborhood of that point (a sufficient, but not necessary condition) or, more fundamentally, if it is well-approximated by a linear function at that point. If any partial derivative fails to exist at a point, the function is not differentiable at that point.\nLet's compute the partial derivative of $g(x)$ with respect to $x_k$ at a point $x$.\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{g(x + h e_k) - g(x)}{h}$$\nwhere $e_k$ is the $k$-th standard basis vector.\n$$g(x + h e_k) = \\sum_{i \\neq k} |x_i| + |x_k + h|$$\nSo, the limit becomes:\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{\\left(\\sum_{i \\neq k} |x_i| + |x_k + h|\\right) - \\left(\\sum_{i=1}^{n} |x_i|\\right)}{h} = \\lim_{h \\to 0} \\frac{|x_k + h| - |x_k|}{h}$$\nIf $x_k \\neq 0$, then for sufficiently small $h$, $x_k+h$ has the same sign as $x_k$. In this case, the derivative of $|t|$ is $\\text{sgn}(t)$, so $\\frac{\\partial g}{\\partial x_k}(x) = \\text{sgn}(x_k)$. Thus, if all $x_i \\neq 0$ for $i=1, \\dots, n$, $g$ is differentiable and its gradient is $\\nabla g(x) = (\\text{sgn}(x_1), \\dots, \\text{sgn}(x_n))$.\nNow, consider a point $x$ where at least one coordinate, say $x_k$, is zero. The partial derivative expression becomes:\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{|0 + h| - |0|}{h} = \\lim_{h \\to 0} \\frac{|h|}{h}$$\nThis limit does not exist. The right-sided limit is $\\lim_{h \\to 0^+} \\frac{h}{h} = 1$, while the left-sided limit is $\\lim_{h \\to 0^-} \\frac{-h}{h} = -1$. Since the partial derivative $\\frac{\\partial g}{\\partial x_k}$ does not exist, the function $g(x)$ is not differentiable at any point $x$ having one or more zero coordinates.\n\nFinally, we characterize the set of subgradients of $g(x)$. For a convex function like $g(x)$, a vector $v \\in \\mathbb{R}^n$ is a subgradient at $x$ if for all $y \\in \\mathbb{R}^n$:\n$$g(y) \\ge g(x) + \\langle v, y-x \\rangle$$\nThe set of all such subgradients is the subdifferential, denoted $\\partial g(x)$.\nSince $g(x) = \\sum_{i=1}^{n} |x_i|$ is a sum of separable convex functions, its subdifferential is the Cartesian product of the subdifferentials of its component functions. Let $\\phi_i(x_i) = |x_i|$. Then $\\partial g(x) = \\partial \\phi_1(x_1) \\times \\dots \\times \\partial \\phi_n(x_n)$.\nWe need to find the subdifferential of the scalar absolute value function $\\phi(t)=|t|$.\n\\begin{itemize}\n    \\item If $t_0 > 0$, $\\phi(t)$ is differentiable with derivative $1$. The subdifferential is a singleton set: $\\partial \\phi(t_0) = \\{1\\} = \\{\\text{sgn}(t_0)\\}$.\n    \\item If $t_0  0$, $\\phi(t)$ is differentiable with derivative $-1$. The subdifferential is a singleton set: $\\partial \\phi(t_0) = \\{-1\\} = \\{\\text{sgn}(t_0)\\}$.\n    \\item If $t_0=0$, we seek $v$ such that $|t| \\ge |0| + v(t-0)$, i.e., $|t| \\ge vt$ for all $t \\in \\mathbb{R}$.\n      If $t > 0$, we need $t \\ge vt$, which implies $v \\le 1$.\n      If $t  0$, we need $-t \\ge vt$, which implies $-1 \\le v$ (since we divide by a negative $t$).\n      Combining these, we must have $v \\in [-1, 1]$. Thus, at $t_0=0$, $\\partial \\phi(0) = [-1, 1]$.\n\\end{itemize}\nThe problem asks to characterize the subgradients of $g$ at points where one or more coordinates $x_i$ vanish. For any $x \\in \\mathbb{R}^n$, a vector $v = (v_1, \\dots, v_n)$ is a subgradient of $g$ at $x$ if and only if each component $v_i$ belongs to the subdifferential of $|x_i|$ at $x_i$.\nSo, the set of all subgradients $\\partial g(x)$ is the set of all vectors $v \\in \\mathbb{R}^n$ such that:\n$$v_i = \\begin{cases} \\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\ c_i \\in [-1, 1]  \\text{if } x_i = 0 \\end{cases}$$\nThis completes the characterization. The final answer required is the minimal Lipschitz constant $L$ for the gradient mapping of $f(x)$, which we found to be $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "The non-smoothness of norms like the $\\ell_1$ and $\\ell_\\infty$ norms, as explored previously, poses a challenge for algorithms that require gradients. This practice introduces you to two powerful and widely-used techniques for creating smooth approximations: the log-sum-exp function for the $\\ell_\\infty$-norm and the Moreau envelope for the $\\ell_1$-norm. By deriving the tight uniform error bounds for these approximations, you will gain insight into a key strategy for modern computational science: replacing a difficult non-smooth problem with a tractable smooth one. ",
            "id": "3600694",
            "problem": "Let $n \\in \\mathbb{N}$ with $n \\geq 2$ and let $\\alpha  0$. For a vector $x \\in \\mathbb{R}^{n}$, consider the following two smooth approximations to the vector norms. First, define the smooth approximation to the $\\ell_{\\infty}$-norm by\n$$\nf_{\\alpha}(x) \\coloneqq \\frac{1}{\\alpha}\\,\\ln\\!\\left(\\sum_{i=1}^{n} \\exp\\!\\left(\\alpha\\,|x_{i}|\\right)\\right).\n$$\nSecond, define the smooth approximation to the $\\ell_{1}$-norm via the Moreau envelope of the function $x \\mapsto \\|x\\|_{1}$ with parameter $\\lambda = 1/\\alpha$, namely\n$$\ng_{\\alpha}(x) \\coloneqq \\inf_{y \\in \\mathbb{R}^{n}} \\left\\{ \\|y\\|_{1} + \\frac{\\alpha}{2}\\,\\|y - x\\|_{2}^{2} \\right\\}.\n$$\nStarting only from the definitions of the norms, the exponential and logarithm functions, and the definition of the Moreau envelope for a proper lower semicontinuous convex function, perform the following.\n\n1. Establish that $f_{\\alpha}(x) \\geq \\|x\\|_{\\infty}$ for all $x \\in \\mathbb{R}^{n}$ and determine the exact value of the tight uniform approximation error\n$$\nB_{\\infty}(\\alpha,n) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\left(f_{\\alpha}(x) - \\|x\\|_{\\infty}\\right)\n$$\nas a closed-form expression in $\\alpha$ and $n$.\n\n2. Establish that $g_{\\alpha}(x) \\leq \\|x\\|_{1}$ for all $x \\in \\mathbb{R}^{n}$ and determine the exact value of the tight uniform approximation error\n$$\nB_{1}(\\alpha,n) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\left(\\|x\\|_{1} - g_{\\alpha}(x)\\right)\n$$\nas a closed-form expression in $\\alpha$ and $n$.\n\nReport your final answer as the two-component row matrix $\\bigl[B_{\\infty}(\\alpha,n),\\,B_{1}(\\alpha,n)\\bigr]$ in exact analytic form. No rounding is required.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   $n \\in \\mathbb{N}$ with $n \\geq 2$.\n-   $\\alpha  0$.\n-   $x \\in \\mathbb{R}^{n}$.\n-   Smooth approximation to the $\\ell_{\\infty}$-norm: $f_{\\alpha}(x) \\coloneqq \\frac{1}{\\alpha}\\,\\ln\\!\\left(\\sum_{i=1}^{n} \\exp\\!\\left(\\alpha\\,|x_{i}|\\right)\\right)$.\n-   Smooth approximation to the $\\ell_{1}$-norm: $g_{\\alpha}(x) \\coloneqq \\inf_{y \\in \\mathbb{R}^{n}} \\left\\{ \\|y\\|_{1} + \\frac{\\alpha}{2}\\,\\|y - x\\|_{2}^{2} \\right\\}$.\n-   Task 1: Establish $f_{\\alpha}(x) \\geq \\|x\\|_{\\infty}$ and find $B_{\\infty}(\\alpha,n) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\left(f_{\\alpha}(x) - \\|x\\|_{\\infty}\\right)$.\n-   Task 2: Establish $g_{\\alpha}(x) \\leq \\|x\\|_{1}$ and find $B_{1}(\\alpha,n) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\left(\\|x\\|_{1} - g_{\\alpha}(x)\\right)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents two standard smoothing techniques for non-differentiable vector norms, namely the log-sum-exp trick for the max function (related to $\\|x\\|_\\infty$) and the Moreau envelope for the $\\ell_1$-norm. These are fundamental and correct concepts in convex analysis and optimization. The tasks are to prove inequalities and find the tightest uniform bounds, which are well-defined mathematical problems. The problem is self-contained and free of any logical contradictions or factual errors.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Part 1: Analysis of the smooth $\\ell_{\\infty}$-norm approximation\n\nFirst, we establish the inequality $f_{\\alpha}(x) \\geq \\|x\\|_{\\infty}$. The $\\ell_{\\infty}$-norm is defined as $\\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_i|$. Let $k$ be an index such that $|x_k| = \\|x\\|_{\\infty}$.\n\nThe sum inside the logarithm can be bounded from below by any single term in the summation. In particular,\n$$\n\\sum_{i=1}^{n} \\exp(\\alpha |x_i|) \\geq \\exp(\\alpha |x_k|) = \\exp(\\alpha \\|x\\|_{\\infty}).\n$$\nSince the natural logarithm function $\\ln(z)$ is monotonically increasing for $z  0$, and we are given $\\alpha  0$, we can apply $\\frac{1}{\\alpha}\\ln(\\cdot)$ to both sides of the inequality without changing its direction:\n$$\n\\frac{1}{\\alpha} \\ln\\left(\\sum_{i=1}^{n} \\exp(\\alpha |x_i|)\\right) \\geq \\frac{1}{\\alpha} \\ln\\left(\\exp(\\alpha \\|x\\|_{\\infty})\\right).\n$$\nThis simplifies to:\n$$\nf_{\\alpha}(x) \\geq \\frac{1}{\\alpha} (\\alpha \\|x\\|_{\\infty}) = \\|x\\|_{\\infty}.\n$$\nThe inequality is thus established for all $x \\in \\mathbb{R}^n$.\n\nNext, we determine the tight uniform approximation error, $B_{\\infty}(\\alpha,n)$. We analyze the difference $f_{\\alpha}(x) - \\|x\\|_{\\infty}$:\n$$\nf_{\\alpha}(x) - \\|x\\|_{\\infty} = \\frac{1}{\\alpha} \\ln\\left(\\sum_{i=1}^{n} \\exp(\\alpha |x_i|)\\right) - \\|x\\|_{\\infty}.\n$$\nWe can rewrite $\\|x\\|_{\\infty}$ as $\\frac{1}{\\alpha}\\ln(\\exp(\\alpha\\|x\\|_{\\infty}))$ since $\\ln(\\exp(z))=z$.\n$$\nf_{\\alpha}(x) - \\|x\\|_{\\infty} = \\frac{1}{\\alpha} \\ln\\left(\\sum_{i=1}^{n} \\exp(\\alpha |x_i|)\\right) - \\frac{1}{\\alpha}\\ln(\\exp(\\alpha\\|x\\|_{\\infty})).\n$$\nUsing the property $\\ln(a) - \\ln(b) = \\ln(a/b)$, this becomes:\n$$\nf_{\\alpha}(x) - \\|x\\|_{\\infty} = \\frac{1}{\\alpha} \\ln\\left(\\frac{\\sum_{i=1}^{n} \\exp(\\alpha |x_i|)}{\\exp(\\alpha \\|x\\|_{\\infty})}\\right) = \\frac{1}{\\alpha} \\ln\\left(\\sum_{i=1}^{n} \\exp(\\alpha |x_i| - \\alpha \\|x\\|_{\\infty})\\right) = \\frac{1}{\\alpha} \\ln\\left(\\sum_{i=1}^{n} \\exp(\\alpha(|x_i| - \\|x\\|_{\\infty}))\\right).\n$$\nBy definition, $\\|x\\|_{\\infty} \\ge |x_i|$ for all $i = 1, \\dots, n$. Therefore, the exponents $(|x_i| - \\|x\\|_{\\infty})$ are all non-positive. This implies $\\exp(\\alpha(|x_i| - \\|x\\|_{\\infty})) \\le \\exp(0) = 1$.\nThe sum can be bounded from above:\n$$\n\\sum_{i=1}^{n} \\exp(\\alpha(|x_i| - \\|x\\|_{\\infty})) \\le \\sum_{i=1}^{n} 1 = n.\n$$\nSince $\\ln(z)$ is an increasing function, we have:\n$$\nf_{\\alpha}(x) - \\|x\\|_{\\infty} \\le \\frac{1}{\\alpha} \\ln(n).\n$$\nThis upper bound is independent of $x$. To show that it is the supremum, we must show that it can be approached arbitrarily closely or attained. Consider a vector $x$ where all components have the same absolute value, e.g., $x = (c, c, \\dots, c)$ for some non-zero constant $c \\in \\mathbb{R}$. For such a vector, $\\|x\\|_{\\infty} = |c|$.\nThen for each $i$, we have $|x_i| - \\|x\\|_{\\infty} = |c| - |c| = 0$. The sum becomes:\n$$\n\\sum_{i=1}^{n} \\exp(\\alpha(0)) = \\sum_{i=1}^{n} 1 = n.\n$$\nFor this choice of $x$, the difference is exactly:\n$$\nf_{\\alpha}(x) - \\|x\\|_{\\infty} = \\frac{1}{\\alpha}\\ln(n).\n$$\nSince we found an upper bound and a vector for which this bound is achieved, this value is the supremum.\n$$\nB_{\\infty}(\\alpha,n) = \\sup_{x \\in \\mathbb{R}^{n}} (f_{\\alpha}(x) - \\|x\\|_{\\infty}) = \\frac{\\ln(n)}{\\alpha}.\n$$\n\n### Part 2: Analysis of the smooth $\\ell_{1}$-norm approximation\n\nFirst, we establish the inequality $g_{\\alpha}(x) \\leq \\|x\\|_{1}$. The function $g_{\\alpha}(x)$ is defined as the infimum of a set of values:\n$$\ng_{\\alpha}(x) = \\inf_{y \\in \\mathbb{R}^{n}} \\left\\{ \\|y\\|_{1} + \\frac{\\alpha}{2}\\,\\|y - x\\|_{2}^{2} \\right\\}.\n$$\nBy the definition of an infimum, the value of $g_{\\alpha}(x)$ must be less than or equal to the value of the expression for any specific choice of $y \\in \\mathbb{R}^n$. Let us choose the specific vector $y=x$.\nSubstituting $y=x$ into the expression gives:\n$$\n\\|x\\|_{1} + \\frac{\\alpha}{2}\\|x - x\\|_{2}^{2} = \\|x\\|_{1} + \\frac{\\alpha}{2}\\|0\\|_{2}^{2} = \\|x\\|_{1}.\n$$\nTherefore, $g_{\\alpha}(x) \\leq \\|x\\|_{1}$, and the inequality is established.\n\nNext, we determine the tight uniform approximation error, $B_{1}(\\alpha,n)$. We need to find $g_{\\alpha}(x)$ in closed form. This requires solving the minimization problem for $y$. The objective function is $F(y) = \\|y\\|_{1} + \\frac{\\alpha}{2}\\|y-x\\|_2^2$, which can be written as a sum of functions of individual components:\n$$\nF(y) = \\sum_{i=1}^{n} \\left( |y_i| + \\frac{\\alpha}{2}(y_i - x_i)^2 \\right).\n$$\nThe minimization problem is separable, so we can minimize for each component $y_i$ independently by finding the minimum of $F_i(y_i) = |y_i| + \\frac{\\alpha}{2}(y_i-x_i)^2$. $F_i(y_i)$ is a strictly convex function, so it has a unique minimum. The minimum is found where the subgradient of $F_i(y_i)$ with respect to $y_i$ contains $0$.\nThe subgradient is $\\partial F_i(y_i) = \\partial|y_i| + \\alpha(y_i-x_i)$. The optimality condition is $0 \\in \\partial|y_i^*| + \\alpha(y_i^*-x_i)$, which is equivalent to $x_i-y_i^* \\in \\frac{1}{\\alpha}\\partial|y_i^*|$.\n\nWe analyze this condition in three cases based on the value of $y_i^*$:\n1. If $y_i^*  0$, then $\\partial|y_i^*| = \\{1\\}$. The condition becomes $x_i-y_i^* = 1/\\alpha$, so $y_i^* = x_i - 1/\\alpha$. This is consistent only if $x_i-1/\\alpha  0$, i.e., $x_i  1/\\alpha$.\n2. If $y_i^*  0$, then $\\partial|y_i^*| = \\{-1\\}$. The condition becomes $x_i-y_i^* = -1/\\alpha$, so $y_i^* = x_i + 1/\\alpha$. This is consistent only if $x_i+1/\\alpha  0$, i.e., $x_i  -1/\\alpha$.\n3. If $y_i^* = 0$, then $\\partial|y_i^*| = [-1, 1]$. The condition becomes $x_i-0 \\in [-1/\\alpha, 1/\\alpha]$, i.e., $|x_i| \\le 1/\\alpha$.\n\nThis piecewise function for the optimal $y_i^*$ is known as the soft-thresholding operator, $y_i^* = \\mathrm{sgn}(x_i)\\max(0, |x_i|-1/\\alpha)$.\nNow we compute $g_{\\alpha}(x) = \\sum_{i=1}^{n} F_i(y_i^*)$. Let's find the value of $F_i(y_i^*)$ for each case:\n- If $|x_i|  1/\\alpha$: $y_i^* = x_i - \\frac{1}{\\alpha}\\mathrm{sgn}(x_i)$. Then $|y_i^*| = |x_i| - 1/\\alpha$ and $y_i^*-x_i = -\\frac{1}{\\alpha}\\mathrm{sgn}(x_i)$.\nThe value is $F_i(y_i^*) = (|x_i| - 1/\\alpha) + \\frac{\\alpha}{2}(-\\frac{1}{\\alpha}\\mathrm{sgn}(x_i))^2 = |x_i| - 1/\\alpha + \\frac{\\alpha}{2\\alpha^2} = |x_i| - \\frac{1}{2\\alpha}$.\n- If $|x_i| \\le 1/\\alpha$: $y_i^*=0$.\nThe value is $F_i(y_i^*) = |0| + \\frac{\\alpha}{2}(0-x_i)^2 = \\frac{\\alpha}{2}x_i^2$.\n\nWe want to find $B_1(\\alpha,n) = \\sup_{x \\in \\mathbb{R}^n} (\\|x\\|_1 - g_{\\alpha}(x))$. Let's analyze the difference component-wise, $d_i(x_i) = |x_i| - F_i(y_i^*)$.\n- If $|x_i|  1/\\alpha$: $d_i(x_i) = |x_i| - (|x_i| - \\frac{1}{2\\alpha}) = \\frac{1}{2\\alpha}$.\n- If $|x_i| \\le 1/\\alpha$: $d_i(x_i) = |x_i| - \\frac{\\alpha}{2}x_i^2 = |x_i| - \\frac{\\alpha}{2}|x_i|^2$.\nTo find the supremum, we must maximize $\\|x\\|_1 - g_\\alpha(x) = \\sum_{i=1}^n d_i(x_i)$. This is equivalent to maximizing each $d_i(x_i)$ independently.\nFor the case $|x_i| \\le 1/\\alpha$, let $t = |x_i|$. We want to maximize $h(t) = t - \\frac{\\alpha}{2}t^2$ on the interval $t \\in [0, 1/\\alpha]$. The derivative is $h'(t) = 1-\\alpha t$. Setting $h'(t)=0$ gives $t=1/\\alpha$. Since $h''(t)=-\\alpha  0$, this is a local maximum. The maximum value of $h(t)$ on $[0,1/\\alpha]$ is attained at $t = 1/\\alpha$, and is $h(1/\\alpha) = 1/\\alpha - \\frac{\\alpha}{2}(1/\\alpha)^2 = \\frac{1}{2\\alpha}$.\nThis means that for any $x_i$, the maximum value of $d_i(x_i)$ is $\\frac{1}{2\\alpha}$. This maximum is achieved for any $x_i$ such that $|x_i| \\ge 1/\\alpha$.\nThe supremum of the total difference is the sum of the suprema of the individual components:\n$$\nB_{1}(\\alpha,n) = \\sup_{x \\in \\mathbb{R}^{n}} \\sum_{i=1}^{n} d_i(x_i) = \\sum_{i=1}^{n} \\sup_{x_i \\in \\mathbb{R}} d_i(x_i) = \\sum_{i=1}^{n} \\frac{1}{2\\alpha} = \\frac{n}{2\\alpha}.\n$$\n\nThe final answers are $B_{\\infty}(\\alpha,n) = \\frac{\\ln(n)}{\\alpha}$ and $B_{1}(\\alpha,n) = \\frac{n}{2\\alpha}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\ln(n)}{\\alpha}  \\frac{n}{2\\alpha}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}