{
    "hands_on_practices": [
        {
            "introduction": "Understanding how a function's output changes in response to small input perturbations is fundamental to numerical analysis. This practice explores the Lipschitz continuity of the $\\|\\cdot\\|_1$ and $\\|\\cdot\\|_\\infty$ norms, challenging you to find the tightest bounds on their variation in terms of the $\\|\\cdot\\|_1$ and $\\|\\cdot\\|_2$ distances. By deriving these optimal constants, you will gain a deeper appreciation for the relationships between different $p$-norms and the stability of norm-based calculations .",
            "id": "3600685",
            "problem": "Let $n \\in \\mathbb{N}$ and consider $\\mathbb{R}^{n}$ equipped with the vector $p$-norms $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$, and $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$. Define $f(x) = \\|x\\|_{1}$ and $g(x) = \\|x\\|_{\\infty}$. For arbitrary $x,y \\in \\mathbb{R}^{n}$, develop upper bounds on the absolute differences $|\\|x\\|_{1} - \\|y\\|_{1}|$ and $|\\|x\\|_{\\infty} - \\|y\\|_{\\infty}|$ expressed in terms of $\\|x-y\\|_{2}$ and $\\|x-y\\|_{1}$. Starting only from the axioms of norms and well-tested inequalities applicable to $\\mathbb{R}^{n}$ (such as the triangle inequality and the Cauchy–Schwarz inequality), prove that for each bound there exists a smallest constant $L$ (depending only on $n$) such that, for all $x,y \\in \\mathbb{R}^{n}$,\n$$\n|\\|x\\|_{1} - \\|y\\|_{1}| \\leq L \\,\\|x-y\\|_{p} \\quad \\text{and} \\quad |\\|x\\|_{\\infty} - \\|y\\|_{\\infty}| \\leq L \\,\\|x-y\\|_{p},\n$$\nwith $p \\in \\{1,2\\}$ as appropriate. Determine these optimal Lipschitz constants explicitly as functions of $n$, and justify that they are best possible (no smaller constant works for all $x,y$). Report your final answer as the ordered row $(L_{1,1}, L_{1,2}, L_{\\infty,1}, L_{\\infty,2})$, where $L_{\\alpha,p}$ denotes the smallest constant such that $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\,\\|x-y\\|_{p}$ holds for all $x,y \\in \\mathbb{R}^{n}$. Provide the exact expressions; no rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed question in numerical linear algebra concerning the Lipschitz continuity of vector norms, with all terms being formally defined and consistent with established mathematical principles. No scientific, logical, or factual flaws were detected.\n\nThe problem requires finding the smallest constants $L$, known as Lipschitz constants, for the functions $f(x) = \\|x\\|_{1}$ and $g(x) = \\|x\\|_{\\infty}$ on $\\mathbb{R}^{n}$. These constants are sought for inequalities of the form $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\|x-y\\|_{p}$, where $\\alpha \\in \\{1, \\infty\\}$ and $p \\in \\{1, 2\\}$.\n\nA fundamental property of any norm $\\|\\cdot\\|$ on a vector space is the reverse triangle inequality, which states that for any vectors $x$ and $y$:\n$$ |\\|x\\| - \\|y\\|| \\leq \\|x-y\\| $$\nThis is derived directly from the triangle inequality axiom of norms:\n$\\|x\\| = \\|x-y+y\\| \\leq \\|x-y\\| + \\|y\\| \\implies \\|x\\| - \\|y\\| \\leq \\|x-y\\|$.\n$\\|y\\| = \\|y-x+x\\| \\leq \\|y-x\\| + \\|x\\| \\implies \\|y\\| - \\|x\\| \\leq \\|y-x\\| = \\|x-y\\|$.\nCombining these two results yields $|\\|x\\| - \\|y\\|| \\leq \\|x-y\\|$.\n\nApplying this general principle to our specific norms, we have:\n$$ |\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq \\|x-y\\|_{\\alpha} $$\nThe problem is to find the best constant $L_{\\alpha,p}$ such that $|\\|x\\|_{\\alpha} - \\|y\\|_{\\alpha}| \\leq L_{\\alpha,p} \\|x-y\\|_{p}$. Using the reverse triangle inequality, we need to find the smallest constant $L_{\\alpha,p}$ such that:\n$$ \\|x-y\\|_{\\alpha} \\leq L_{\\alpha,p} \\|x-y\\|_{p} $$\nLet $z = x-y$. Since $x$ and $y$ are arbitrary vectors in $\\mathbb{R}^n$, $z$ can be any vector in $\\mathbb{R}^n$. The problem is therefore equivalent to finding the smallest constant $L_{\\alpha,p}$ such that for all $z \\in \\mathbb{R}^n$:\n$$ \\|z\\|_{\\alpha} \\leq L_{\\alpha,p} \\|z\\|_{p} $$\nThis constant is the operator norm of the identity map from the normed space $(\\mathbb{R}^n, \\|\\cdot\\|_p)$ to $(\\mathbb{R}^n, \\|\\cdot\\|_\\alpha)$. It is formally defined as:\n$$ L_{\\alpha,p} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\alpha}}{\\|z\\|_{p}} $$\nTo prove that a constant $L$ is indeed the supremum (i.e., the smallest possible constant), we must first establish that $\\|z\\|_{\\alpha} \\leq L \\|z\\|_{p}$ for all $z$, and then demonstrate that this bound is tight by finding a specific non-zero vector $z_0$ for which $\\|z_0\\|_{\\alpha} = L \\|z_0\\|_{p}$.\n\nWe will now determine the four constants $L_{1,1}$, $L_{1,2}$, $L_{\\infty,1}$, and $L_{\\infty,2}$.\n\n1.  **Determination of $L_{1,1}$**\n    We seek the constant $L_{1,1} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{1}}{\\|z\\|_{1}}$.\n    For any non-zero vector $z \\in \\mathbb{R}^n$, the ratio $\\frac{\\|z\\|_{1}}{\\|z\\|_{1}}$ is identically $1$.\n    Therefore, $L_{1,1} = 1$. The inequality is $|\\|x\\|_{1} - \\|y\\|_{1}| \\leq \\|x-y\\|_{1}$, which is the reverse triangle inequality for the $1$-norm itself. To confirm it is the optimal constant, let $x=(1,0,...,0)$ and $y=0$. Then $|\\|x\\|_1 - \\|y\\|_1| = 1$ and $\\|x-y\\|_1 = 1$, so any constant less than $1$ would fail. Thus, $L_{1,1}=1$.\n\n2.  **Determination of $L_{1,2}$**\n    We seek the constant $L_{1,2} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{1}}{\\|z\\|_{2}}$.\n    Let $z = (z_1, z_2, \\dots, z_n) \\in \\mathbb{R}^n$. We have $\\|z\\|_1 = \\sum_{i=1}^n |z_i|$ and $\\|z\\|_2 = \\left(\\sum_{i=1}^n z_i^2\\right)^{1/2}$.\n    We apply the Cauchy–Schwarz inequality to the vectors $u = (1, 1, \\dots, 1)$ and $v = (|z_1|, |z_2|, \\dots, |z_n|)$.\n    The inequality states $(u \\cdot v)^2 \\leq \\|u\\|_2^2 \\|v\\|_2^2$.\n    Here, $u \\cdot v = \\sum_{i=1}^n 1 \\cdot |z_i| = \\|z\\|_1$.\n    $\\|u\\|_2^2 = \\sum_{i=1}^n 1^2 = n$.\n    $\\|v\\|_2^2 = \\sum_{i=1}^n |z_i|^2 = \\sum_{i=1}^n z_i^2 = \\|z\\|_2^2$.\n    Substituting these into the Cauchy-Schwarz inequality gives:\n    $$ (\\|z\\|_1)^2 \\leq n \\|z\\|_2^2 $$\n    Taking the square root of both sides, we obtain $\\|z\\|_1 \\leq \\sqrt{n} \\|z\\|_2$.\n    This implies that $\\frac{\\|z\\|_{1}}{\\|z\\|_{2}} \\leq \\sqrt{n}$ for all $z \\neq 0$, so $L_{1,2} \\leq \\sqrt{n}$.\n    To show this bound is optimal, we must find a vector for which equality holds. Equality in the Cauchy-Schwarz inequality holds if and only if the vectors are linearly dependent, i.e., $v = c \\cdot u$ for some scalar $c$. This means $|z_1| = |z_2| = \\dots = |z_n|$.\n    Let's choose $z_0 = (1, 1, \\dots, 1)$.\n    For this vector, $\\|z_0\\|_1 = \\sum_{i=1}^n |1| = n$.\n    And $\\|z_0\\|_2 = \\left(\\sum_{i=1}^n 1^2\\right)^{1/2} = \\sqrt{n}$.\n    The ratio for this vector is $\\frac{\\|z_0\\|_{1}}{\\|z_0\\|_{2}} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$.\n    Since we have found a vector that achieves the bound $\\sqrt{n}$, the supremum must be $\\sqrt{n}$.\n    Thus, $L_{1,2} = \\sqrt{n}$.\n\n3.  **Determination of $L_{\\infty,1}$**\n    We seek the constant $L_{\\infty,1} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\infty}}{\\|z\\|_{1}}$.\n    By definition, $\\|z\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |z_i|$ and $\\|z\\|_1 = \\sum_{i=1}^n |z_i|$.\n    Let $k$ be an index such that $|z_k| = \\|z\\|_{\\infty}$.\n    The sum is $\\|z\\|_1 = |z_1| + \\dots + |z_k| + \\dots + |z_n|$.\n    Since each $|z_i| \\geq 0$, the sum must be greater than or equal to any single term. In particular, $\\|z\\|_1 \\geq |z_k| = \\|z\\|_{\\infty}$.\n    So, $\\|z\\|_{\\infty} \\leq \\|z\\|_{1}$.\n    This implies $\\frac{\\|z\\|_{\\infty}}{\\|z\\|_{1}} \\leq 1$ for all $z \\neq 0$, so $L_{\\infty,1} \\leq 1$.\n    To show this bound is optimal, consider a standard basis vector, for example, $z_0 = e_1 = (1, 0, \\dots, 0)$.\n    For this vector, $\\|z_0\\|_{\\infty} = \\max\\{1, 0, \\dots, 0\\} = 1$.\n    And $\\|z_0\\|_1 = |1| + |0| + \\dots + |0| = 1$.\n    The ratio is $\\frac{\\|z_0\\|_{\\infty}}{\\|z_0\\|_{1}} = \\frac{1}{1} = 1$.\n    Since the bound is achieved, the supremum is $1$.\n    Thus, $L_{\\infty,1} = 1$.\n\n4.  **Determination of $L_{\\infty,2}$**\n    We seek the constant $L_{\\infty,2} = \\sup_{z \\neq 0} \\frac{\\|z\\|_{\\infty}}{\\|z\\|_{2}}$.\n    Let $k$ be an index such that $|z_k| = \\|z\\|_{\\infty}$.\n    We have $\\|z\\|_{\\infty}^2 = |z_k|^2 = z_k^2$.\n    The squared $2$-norm is $\\|z\\|_2^2 = \\sum_{i=1}^n z_i^2 = z_1^2 + \\dots + z_k^2 + \\dots + z_n^2$.\n    Since each $z_i^2 \\geq 0$, the sum is greater than or equal to any single term: $\\|z\\|_2^2 \\geq z_k^2 = \\|z\\|_{\\infty}^2$.\n    Taking the square root (norms are non-negative), we get $\\|z\\|_2 \\geq \\|z\\|_{\\infty}$.\n    This implies $\\frac{\\|z\\|_{\\infty}}{\\|z\\|_{2}} \\leq 1$ for all $z \\neq 0$, so $L_{\\infty,2} \\leq 1$.\n    To show this bound is optimal, consider again the standard basis vector $z_0 = e_1 = (1, 0, \\dots, 0)$.\n    For this vector, $\\|z_0\\|_{\\infty} = \\max\\{1, 0, \\dots, 0\\} = 1$.\n    And $\\|z_0\\|_{2} = (1^2 + 0^2 + \\dots + 0^2)^{1/2} = 1$.\n    The ratio is $\\frac{\\|z_0\\|_{\\infty}}{\\|z_0\\|_{2}} = \\frac{1}{1} = 1$.\n    Since the bound is achieved, the supremum is $1$.\n    Thus, $L_{\\infty,2} = 1$.\n\nThe four optimal Lipschitz constants are:\n$L_{1,1} = 1$\n$L_{1,2} = \\sqrt{n}$\n$L_{\\infty,1} = 1$\n$L_{\\infty,2} = 1$\n\nThe final answer is the ordered row $(L_{1,1}, L_{1,2}, L_{\\infty,1}, L_{\\infty,2})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & \\sqrt{n} & 1 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The choice of norm profoundly impacts the mathematical properties of optimization problems, particularly regarding differentiability. This exercise contrasts the smoothly differentiable function $f(x) = \\frac{1}{2}\\|x\\|_2^2$ with the non-differentiable $\\|x\\|_1$ norm, whose 'sharp corners' are key to promoting sparsity. You will derive the gradient for the former and explore the concept of subgradients for the latter, uncovering the analytical tools needed to work with these foundational functions in optimization .",
            "id": "3600719",
            "problem": "Let $n \\in \\mathbb{N}$ and consider the function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$ defined by $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$, where $\\|x\\|_{2}$ denotes the Euclidean (or $2$-) norm and the underlying inner product on $\\mathbb{R}^{n}$ is the standard dot product. Using only the fundamental definitions of differentiability, the gradient, and the vector $p$-norms, derive the explicit form of the gradient $\\nabla f(x)$ and prove that the gradient mapping $x \\mapsto \\nabla f(x)$ is $L$-Lipschitz with respect to the $2$-norm for the smallest possible constant $L$. Separately, let $g : \\mathbb{R}^{n} \\to \\mathbb{R}$ be defined by $g(x) = \\|x\\|_{1}$, where $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. Analyze the differentiability properties of $g$ at points $x$ where one or more coordinates $x_{i}$ vanish, and characterize the set of all subgradients of $g$ at such points. Report the minimal Lipschitz constant $L$ for the gradient mapping of $f$ as your final answer. No rounding is required.",
            "solution": "The problem presents two distinct tasks concerning two functions, $f(x)$ and $g(x)$, defined on $\\mathbb{R}^{n}$. The first task is to find the gradient of $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$, and then determine the minimal Lipschitz constant for its gradient mapping. The second task is to analyze the differentiability of $g(x)=\\|x\\|_{1}$ and characterize its subdifferential. We will address each part systematically.\n\nFirst, let's analyze the function $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$.\nThe problem requires deriving the gradient $\\nabla f(x)$ using its fundamental definition. A function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ is differentiable at a point $x \\in \\mathbb{R}^{n}$ if there exists a vector, denoted $\\nabla f(x)$, such that for any displacement vector $h \\in \\mathbb{R}^{n}$, the following holds:\n$$f(x+h) = f(x) + \\langle \\nabla f(x), h \\rangle + o(\\|h\\|_{2})$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ is the standard dot product on $\\mathbb{R}^{n}$ and $o(\\|h\\|_{2})$ is a term that satisfies $\\lim_{\\|h\\|_{2} \\to 0} \\frac{o(\\|h\\|_{2})}{\\|h\\|_{2}} = 0$.\n\nLet's expand $f(x+h)$:\n$$f(x+h) = \\frac{1}{2}\\|x+h\\|_{2}^{2}$$\nBy the definition of the Euclidean norm in terms of the inner product, $\\|v\\|_{2}^{2} = \\langle v, v \\rangle$.\n$$f(x+h) = \\frac{1}{2}\\langle x+h, x+h \\rangle$$\nUsing the bilinearity of the inner product:\n$$f(x+h) = \\frac{1}{2}(\\langle x, x \\rangle + \\langle x, h \\rangle + \\langle h, x \\rangle + \\langle h, h \\rangle)$$\nSince the standard dot product is symmetric, $\\langle x, h \\rangle = \\langle h, x \\rangle$.\n$$f(x+h) = \\frac{1}{2}(\\langle x, x \\rangle + 2\\langle x, h \\rangle + \\langle h, h \\rangle)$$\n$$f(x+h) = \\frac{1}{2}\\|x\\|_{2}^{2} + \\langle x, h \\rangle + \\frac{1}{2}\\|h\\|_{2}^{2}$$\nRecognizing that $f(x) = \\frac{1}{2}\\|x\\|_{2}^{2}$, we have:\n$$f(x+h) = f(x) + \\langle x, h \\rangle + \\frac{1}{2}\\|h\\|_{2}^{2}$$\nComparing this expression with the definition of differentiability, $f(x+h) = f(x) + \\langle \\nabla f(x), h \\rangle + o(\\|h\\|_{2})$, we can identify the linear term in $h$ as $\\langle x, h \\rangle$. This suggests that $\\nabla f(x) = x$. The remainder term is $\\frac{1}{2}\\|h\\|_{2}^{2}$. To confirm this identification, we must verify that this remainder is indeed $o(\\|h\\|_{2})$. We check the limit:\n$$\\lim_{\\|h\\|_{2} \\to 0} \\frac{\\frac{1}{2}\\|h\\|_{2}^{2}}{\\|h\\|_{2}} = \\lim_{\\|h\\|_{2} \\to 0} \\frac{1}{2}\\|h\\|_{2} = 0$$\nThe limit is $0$, so the remainder term is $o(\\|h\\|_{2})$. Thus, the gradient of $f(x)$ is $\\nabla f(x) = x$.\n\nNext, we must prove that the gradient mapping, which we denote as $G(x) = \\nabla f(x) = x$, is $L$-Lipschitz with respect to the $2$-norm for the smallest possible constant $L$. A mapping $G: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is $L$-Lipschitz continuous with respect to a norm $\\|\\cdot\\|$ if for all $x, y \\in \\mathbb{R}^{n}$, the following inequality holds:\n$$\\|G(x) - G(y)\\| \\le L \\|x - y\\|$$\nIn our case, the mapping is $G(x)=x$ and the norm is the $2$-norm. So we need to find the smallest $L \\ge 0$ such that:\n$$\\|x - y\\|_{2} \\le L \\|x - y\\|_{2}$$\nfor all $x, y \\in \\mathbb{R}^{n}$.\nIf $x=y$, the inequality becomes $0 \\le 0$, which is true for any $L$.\nIf $x \\neq y$, we have $\\|x-y\\|_{2} > 0$, and we can divide both sides by it to get:\n$$1 \\le L$$\nThis shows that the inequality holds for any constant $L \\ge 1$. The smallest such constant is $L=1$. To show that $L=1$ is minimal, we must show that for any $L' < 1$, the condition is violated. If we choose $L' < 1$, we can pick any two distinct points, e.g., $x=(1, 0, \\dots, 0)$ and $y=(0, \\dots, 0)$. Then $\\|x-y\\|_{2}=1$ and $\\|G(x)-G(y)\\|_{2} = \\|x-y\\|_{2} = 1$. The Lipschitz condition would require $1 \\le L' \\cdot 1$, which is $1 \\le L'$, contradicting $L'<1$. Therefore, the smallest possible Lipschitz constant is $L=1$.\n\nNow, we turn to the second part of the problem, concerning the function $g(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\nFirst, let's analyze its differentiability. The function $g(x)$ is a sum of functions of the form $\\phi_i(x) = |x_i|$. The scalar function $\\phi(t)=|t|$ is differentiable everywhere except at $t=0$.\nA function of multiple variables is differentiable at a point if all its partial derivatives exist and are continuous in a neighborhood of that point (a sufficient, but not necessary condition) or, more fundamentally, if it is well-approximated by a linear function at that point. If any partial derivative fails to exist at a point, the function is not differentiable at that point.\nLet's compute the partial derivative of $g(x)$ with respect to $x_k$ at a point $x$.\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{g(x + h e_k) - g(x)}{h}$$\nwhere $e_k$ is the $k$-th standard basis vector.\n$$g(x + h e_k) = \\sum_{i \\neq k} |x_i| + |x_k + h|$$\nSo, the limit becomes:\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{\\left(\\sum_{i \\neq k} |x_i| + |x_k + h|\\right) - \\left(\\sum_{i=1}^{n} |x_i|\\right)}{h} = \\lim_{h \\to 0} \\frac{|x_k + h| - |x_k|}{h}$$\nIf $x_k \\neq 0$, then for sufficiently small $h$, $x_k+h$ has the same sign as $x_k$. In this case, the derivative of $|t|$ is $\\text{sgn}(t)$, so $\\frac{\\partial g}{\\partial x_k}(x) = \\text{sgn}(x_k)$. Thus, if all $x_i \\neq 0$ for $i=1, \\dots, n$, $g$ is differentiable and its gradient is $\\nabla g(x) = (\\text{sgn}(x_1), \\dots, \\text{sgn}(x_n))$.\nNow, consider a point $x$ where at least one coordinate, say $x_k$, is zero. The partial derivative expression becomes:\n$$\\frac{\\partial g}{\\partial x_k}(x) = \\lim_{h \\to 0} \\frac{|0 + h| - |0|}{h} = \\lim_{h \\to 0} \\frac{|h|}{h}$$\nThis limit does not exist. The right-sided limit is $\\lim_{h \\to 0^+} \\frac{h}{h} = 1$, while the left-sided limit is $\\lim_{h \\to 0^-} \\frac{-h}{h} = -1$. Since the partial derivative $\\frac{\\partial g}{\\partial x_k}$ does not exist, the function $g(x)$ is not differentiable at any point $x$ having one or more zero coordinates.\n\nFinally, we characterize the set of subgradients of $g(x)$. For a convex function like $g(x)$, a vector $v \\in \\mathbb{R}^n$ is a subgradient at $x$ if for all $y \\in \\mathbb{R}^n$:\n$$g(y) \\ge g(x) + \\langle v, y-x \\rangle$$\nThe set of all such subgradients is the subdifferential, denoted $\\partial g(x)$.\nSince $g(x) = \\sum_{i=1}^{n} |x_i|$ is a sum of separable convex functions, its subdifferential is the Cartesian product of the subdifferentials of its component functions. Let $\\phi_i(x_i) = |x_i|$. Then $\\partial g(x) = \\partial \\phi_1(x_1) \\times \\dots \\times \\partial \\phi_n(x_n)$.\nWe need to find the subdifferential of the scalar absolute value function $\\phi(t)=|t|$.\n\\begin{itemize}\n    \\item If $t_0 > 0$, $\\phi(t)$ is differentiable with derivative $1$. The subdifferential is a singleton set: $\\partial \\phi(t_0) = \\{1\\} = \\{\\text{sgn}(t_0)\\}$.\n    \\item If $t_0 < 0$, $\\phi(t)$ is differentiable with derivative $-1$. The subdifferential is a singleton set: $\\partial \\phi(t_0) = \\{-1\\} = \\{\\text{sgn}(t_0)\\}$.\n    \\item If $t_0=0$, we seek $v$ such that $|t| \\ge |0| + v(t-0)$, i.e., $|t| \\ge vt$ for all $t \\in \\mathbb{R}$.\n      If $t > 0$, we need $t \\ge vt$, which implies $v \\le 1$.\n      If $t < 0$, we need $-t \\ge vt$, which implies $-1 \\le v$ (since we divide by a negative $t$).\n      Combining these, we must have $v \\in [-1, 1]$. Thus, at $t_0=0$, $\\partial \\phi(0) = [-1, 1]$.\n\\end{itemize}\nThe problem asks to characterize the subgradients of $g$ at points where one or more coordinates $x_i$ vanish. For any $x \\in \\mathbb{R}^n$, a vector $v = (v_1, \\dots, v_n)$ is a subgradient of $g$ at $x$ if and only if each component $v_i$ belongs to the subdifferential of $|x_i|$ at $x_i$.\nSo, the set of all subgradients $\\partial g(x)$ is the set of all vectors $v \\in \\mathbb{R}^n$ such that:\n$$v_i = \\begin{cases} \\text{sgn}(x_i) & \\text{if } x_i \\neq 0 \\\\ c_i \\in [-1, 1] & \\text{if } x_i = 0 \\end{cases}$$\nThis completes the characterization. The final answer required is the minimal Lipschitz constant $L$ for the gradient mapping of $f(x)$, which we found to be $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "The geometric structure of norm balls is central to constrained optimization, and the ability to project a point onto these sets is a fundamental algorithmic primitive. This practice challenges you to translate theory into practice by designing and analyzing algorithms to compute the Euclidean projection onto the $\\ell_1$-ball $B_1(\\tau)$ and the $\\ell_\\infty$-ball $B_\\infty(\\tau)$. By deriving these projections from first principles, you will gain hands-on experience with the building blocks of many advanced optimization and machine learning methods .",
            "id": "3600683",
            "problem": "Let $n\\in\\mathbb{N}$, and let $y\\in\\mathbb{R}^n$ and $\\tau\\in\\mathbb{R}$ with $\\tau\\ge 0$. For $p\\in\\{1,2,\\infty\\}$, the vector $p$-norm is defined by $\\|y\\|_1=\\sum_{i=1}^n |y_i|$, $\\|y\\|_2=\\left(\\sum_{i=1}^n y_i^2\\right)^{1/2}$, and $\\|y\\|_\\infty=\\max_{1\\le i\\le n}|y_i|$. The closed $\\ell_p$-ball of radius $\\tau$ is the convex set $B_p(\\tau)=\\{x\\in\\mathbb{R}^n:\\|x\\|_p\\le \\tau\\}$. The Euclidean projection (that is, the metric projection in $\\ell_2$) of $y$ onto a nonempty closed convex set $C\\subset\\mathbb{R}^n$ is the unique point $x^\\star\\in C$ that minimizes the Euclidean distance $\\|x-y\\|_2$ over $x\\in C$. In this problem, you will:\n- Design algorithms from first principles (starting from the above definitions and standard optimality conditions for convex minimization) to compute the Euclidean projection $P_{B_1(\\tau)}(y)$ and $P_{B_\\infty(\\tau)}(y)$.\n- Prove that these projection mappings are nonexpansive in the Euclidean norm: for all $u,v\\in\\mathbb{R}^n$, $\\|P_{B_1(\\tau)}(u)-P_{B_1(\\tau)}(v)\\|_2\\le \\|u-v\\|_2$ and $\\|P_{B_\\infty(\\tau)}(u)-P_{B_\\infty(\\tau)}(v)\\|_2\\le \\|u-v\\|_2$.\n- Compare their computational tradeoffs by identifying the dominant operation counts and asymptotic complexity when implemented in a numerically stable way.\n\nYour program must implement your derived algorithms and produce verifiable outputs on the following test suite. Use a numerical tolerance $\\varepsilon=10^{-12}$ when comparing real numbers.\n\nSingle-vector projection tests (feasibility checks):\n1. $y^{(1)}=(3,-1,2,0.5)$ with $\\tau^{(1)}=2$. Verify that $\\|P_{B_1(\\tau^{(1)})}(y^{(1)})\\|_1\\le \\tau^{(1)}+\\varepsilon$ and $\\|P_{B_\\infty(\\tau^{(1)})}(y^{(1)})\\|_\\infty\\le \\tau^{(1)}+\\varepsilon$.\n2. $y^{(2)}=(0,0,0,0)$ with $\\tau^{(2)}=0$. Verify $\\|P_{B_1(\\tau^{(2)})}(y^{(2)})\\|_1\\le \\tau^{(2)}+\\varepsilon$ and $\\|P_{B_\\infty(\\tau^{(2)})}(y^{(2)})\\|_\\infty\\le \\tau^{(2)}+\\varepsilon$.\n3. $y^{(3)}=(1,1,1,1)$ with $\\tau^{(3)}=2$. Verify $\\|P_{B_1(\\tau^{(3)})}(y^{(3)})\\|_1\\le \\tau^{(3)}+\\varepsilon$ and $\\|P_{B_\\infty(\\tau^{(3)})}(y^{(3)})\\|_\\infty\\le \\tau^{(3)}+\\varepsilon$.\n4. $y^{(4)}=(10,-10,0.1,-0.1,5)$ with $\\tau^{(4)}=7$. Verify $\\|P_{B_1(\\tau^{(4)})}(y^{(4)})\\|_1\\le \\tau^{(4)}+\\varepsilon$ and $\\|P_{B_\\infty(\\tau^{(4)})}(y^{(4)})\\|_\\infty\\le \\tau^{(4)}+\\varepsilon$.\n\nPairwise nonexpansiveness tests (Euclidean norm):\nA. $y_a^{(A)}=(3,-1,2)$ and $y_b^{(A)}=(2.5,0,-1)$ with $\\tau^{(A)}=2$. Verify $\\|P_{B_1(\\tau^{(A)})}(y_a^{(A)})-P_{B_1(\\tau^{(A)})}(y_b^{(A)})\\|_2\\le \\|y_a^{(A)}-y_b^{(A)}\\|_2+\\varepsilon$ and $\\|P_{B_\\infty(\\tau^{(A)})}(y_a^{(A)})-P_{B_\\infty(\\tau^{(A)})}(y_b^{(A)})\\|_2\\le \\|y_a^{(A)}-y_b^{(A)}\\|_2+\\varepsilon$.\nB. $y_a^{(B)}=(-5,4,-3,2)$ and $y_b^{(B)}=(1,-2,3,-4)$ with $\\tau^{(B)}=3$. Verify the same two inequalities with $\\tau^{(B)}$.\nC. $y_a^{(C)}=(0.2,-0.3,0.4,-0.5,0.6)$ and $y_b^{(C)}=(-0.1,0.1,-0.2,0.2,-0.3)$ with $\\tau^{(C)}=0.7$. Verify the same two inequalities with $\\tau^{(C)}$.\n\nAdditional norm-specific check:\n- For the $\\ell_\\infty$-ball projection, also verify nonexpansiveness in the $\\ell_\\infty$ norm for each pair above: $\\|P_{B_\\infty(\\tau)}(y_a)-P_{B_\\infty(\\tau)}(y_b)\\|_\\infty\\le \\|y_a-y_b\\|_\\infty+\\varepsilon$.\n\nComputational tradeoff quantification:\n- Using a proxy for dominant operation counts, report the ratios\n$$\nR(n)=\\frac{n\\log_2 n}{n}=\\log_2 n\n$$\nfor $n\\in\\{20,1000,100000\\}$, interpreting $n\\log_2 n$ as the dominant cost for the $\\ell_1$-ball projection algorithm (due to sorting) and $n$ as the dominant cost for the $\\ell_\\infty$-ball projection algorithm (due to componentwise clipping).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain, in order:\n  - Eight booleans from the four single-vector feasibility checks (two per test: $\\ell_1$ then $\\ell_\\infty$).\n  - Six booleans from the three Euclidean nonexpansiveness pair checks (two per pair: $\\ell_1$ then $\\ell_\\infty$).\n  - Three booleans from the $\\ell_\\infty$-norm nonexpansiveness checks (one per pair).\n  - Three floats for $R(20)$, $R(1000)$, and $R(100000)$, in that order.\nThus the final output list has $20$ entries: $17$ booleans followed by $3$ floats.",
            "solution": "The problem is found to be valid. It is a well-posed problem in convex optimization, resting on standard definitions and established principles of numerical linear algebra. All data and conditions are self-contained and consistent.\n\n### 1. Euclidean Projection onto the $\\ell_\\infty$-ball\n\nThe Euclidean projection of a vector $y \\in \\mathbb{R}^n$ onto the closed $\\ell_\\infty$-ball $B_\\infty(\\tau) = \\{x \\in \\mathbb{R}^n : \\|x\\|_\\infty \\le \\tau\\}$ is the solution $x^\\star$ to the optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|x-y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_\\infty \\le \\tau\n$$\nThe objective function is $f(x) = \\frac{1}{2}\\sum_{i=1}^n (x_i - y_i)^2$. The constraint $\\|x\\|_\\infty \\le \\tau$ is equivalent to the set of component-wise constraints $|x_i| \\le \\tau$ for all $i \\in \\{1, \\dots, n\\}$, which can be written as $-\\tau \\le x_i \\le \\tau$.\n\nSince both the objective function and the constraints are separable by component, the $n$-dimensional problem decouples into $n$ independent scalar problems:\n$$\n\\min_{x_i \\in [-\\tau, \\tau]} \\frac{1}{2}(x_i - y_i)^2 \\quad \\text{for } i=1, \\dots, n\n$$\nFor each component $i$, the unconstrained minimizer of $(x_i - y_i)^2$ is $x_i = y_i$.\n- If $|y_i| \\le \\tau$, then $y_i$ is within the feasible interval $[-\\tau, \\tau]$, so the optimal solution is $x_i^\\star = y_i$.\n- If $y_i > \\tau$, the quadratic function $(x_i - y_i)^2$ is strictly decreasing on $[-\\tau, \\tau]$. The minimum is achieved at the right endpoint, so $x_i^\\star = \\tau$.\n- If $y_i < -\\tau$, the quadratic function is strictly increasing on $[-\\tau, \\tau]$. The minimum is achieved at the left endpoint, so $x_i^\\star = -\\tau$.\n\nThese three cases can be compactly written as $x_i^\\star = \\text{sign}(y_i)\\min(|y_i|, \\tau)$, or equivalently as $x_i^\\star = \\text{median}(-\\tau, y_i, \\tau)$. This operation is also known as clipping. The projection is thus given by:\n$$\nP_{B_\\infty(\\tau)}(y)_i = \\max(-\\tau, \\min(y_i, \\tau))\n$$\n\n### 2. Euclidean Projection onto the $\\ell_1$-ball\n\nThe Euclidean projection of $y \\in \\mathbb{R}^n$ onto the closed $\\ell_1$-ball $B_1(\\tau) = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}$ is the solution $x^\\star$ to:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|x-y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le \\tau\n$$\nIf $y$ is already in the ball, i.e., $\\|y\\|_1 \\le \\tau$, then the minimum distance is $0$, achieved at $x^\\star = y$.\n\nIf $\\|y\\|_1 > \\tau$, the projection $x^\\star$ must lie on the boundary of the ball, meaning $\\|x^\\star\\|_1 = \\tau$. This is a constrained convex optimization problem. We form the Lagrangian:\n$$\nL(x, \\lambda) = \\frac{1}{2}\\|x-y\\|_2^2 + \\lambda (\\|x\\|_1 - \\tau)\n$$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier. The Karush-Kuhn-Tucker (KKT) conditions for optimality are:\n1. Primal feasibility: $\\|x\\|_1 \\le \\tau$\n2. Dual feasibility: $\\lambda \\ge 0$\n3. Complementary slackness: $\\lambda (\\|x\\|_1 - \\tau) = 0$\n4. Stationarity: $0 \\in \\partial_x L(x, \\lambda) = x - y + \\lambda \\partial \\|x\\|_1$\n\nThe stationarity condition implies $y - x \\in \\lambda \\partial \\|x\\|_1$, where $\\partial \\|x\\|_1$ is the subgradient of the $\\ell_1$-norm. For each component $i$, this means $y_i - x_i = \\lambda s_i$ where $s_i \\in \\partial |x_i|$. The subgradient of the absolute value function is $\\text{sign}(t)$ for $t \\ne 0$ and $[-1, 1]$ for $t=0$. This gives $x_i = y_i - \\lambda s_i$. A case analysis reveals that the solution must have the form $x_i = \\text{sign}(y_i)\\max(0, |y_i| - \\lambda)$. This operation is known as the soft-thresholding operator, $S_\\lambda(y)$.\n\nSince we assumed $\\|y\\|_1 > \\tau$, the projection must be on the boundary, so $\\|x\\|_1 = \\tau$. This means we must find a $\\lambda > 0$ such that $f(\\lambda) := \\|S_\\lambda(y)\\|_1 = \\sum_{i=1}^n \\max(0, |y_i| - \\lambda) = \\tau$. The function $f(\\lambda)$ is continuous, piecewise-linear, and monotonically decreasing in $\\lambda$. Since $f(0) = \\|y\\|_1 > \\tau$ and $f(\\lambda) \\to 0$ as $\\lambda \\to \\infty$, a unique solution $\\lambda > 0$ exists.\n\nTo find $\\lambda$ efficiently, we can use the following algorithm:\n1. If $\\|y\\|_1 \\le \\tau$, return $y$.\n2. For $\\tau=0$, the projection is the zero vector.\n3. Let $u_i = |y_i|$ and sort these values in descending order: $u_{(1)} \\ge u_{(2)} \\ge \\dots \\ge u_{(n)}$.\n4. Find the integer $\\rho = \\max\\left\\{k \\in \\{1, \\dots, n\\} \\mid u_{(k)} > \\frac{1}{k}\\left(\\sum_{j=1}^k u_{(j)} - \\tau\\right)\\right\\}$. This can be found via a linear scan over $k=1, \\dots, n$.\n5. Set the threshold $\\lambda = \\frac{1}{\\rho}\\left(\\sum_{j=1}^\\rho u_{(j)} - \\tau\\right)$.\n6. The projection is $x^\\star = S_\\lambda(y) = \\text{sign}(y) \\odot \\max(0, |y|-\\lambda)$, where $\\odot$ is the element-wise product and vector operations are element-wise.\n\n### 3. Proof of Nonexpansiveness in the Euclidean Norm\n\nA projection operator $P_C$ onto a nonempty, closed, convex set $C$ in a Hilbert space (such as $\\mathbb{R}^n$ with the Euclidean inner product) is known to be nonexpansive. The sets $B_1(\\tau)$ and $B_\\infty(\\tau)$ are nonempty (for $\\tau \\ge 0$), closed, and convex, hence this general property applies.\n\nProof: Let $x_u = P_C(u)$ and $x_v = P_C(v)$ for any $u,v \\in \\mathbb{R}^n$. The variational inequality characterizing the projection $x_u$ is $\\langle u - x_u, z - x_u \\rangle \\le 0$ for all $z \\in C$.\nSince $x_v \\in C$, we can set $z=x_v$, yielding:\n$$\n\\langle u - x_u, x_v - x_u \\rangle \\le 0\n$$\nSimilarly, for the projection $x_v$, we have $\\langle v - x_v, z' - x_v \\rangle \\le 0$ for all $z' \\in C$. Setting $z' = x_u$ gives:\n$$\n\\langle v - x_v, x_u - x_v \\rangle \\le 0\n$$\nAdding these two inequalities:\n$$\n\\langle u - x_u, x_v - x_u \\rangle + \\langle v - x_v, x_u - x_v \\rangle \\le 0\n$$\nRearranging terms:\n$$\n\\langle u - x_u, x_v - x_u \\rangle - \\langle v - x_v, x_v - x_u \\rangle \\le 0\n$$\n$$\n\\langle (u - v) - (x_u - x_v), x_v - x_u \\rangle \\le 0\n$$\n$$\n-\\langle u - v, x_u - x_v \\rangle + \\|x_u - x_v\\|_2^2 \\le 0\n$$\nThis gives the firm nonexpansiveness property: $\\|x_u - x_v\\|_2^2 \\le \\langle u - v, x_u - x_v \\rangle$.\nBy the Cauchy-Schwarz inequality, $\\langle u - v, x_u - x_v \\rangle \\le \\|u-v\\|_2 \\|x_u - x_v\\|_2$.\nSubstituting this into the previous inequality:\n$$\n\\|x_u - x_v\\|_2^2 \\le \\|u-v\\|_2 \\|x_u - x_v\\|_2\n$$\nIf $\\|x_u - x_v\\|_2 \\ne 0$, we can divide by it to obtain the nonexpansiveness property:\n$$\n\\|P_C(u) - P_C(v)\\|_2 \\le \\|u - v\\|_2\n$$\nThe inequality holds trivially if $\\|x_u - x_v\\|_2 = 0$. This proof applies to both $P_{B_1(\\tau)}$ and $P_{B_\\infty(\\tau)}$.\n\n### 4. Nonexpansiveness of $P_{B_\\infty(\\tau)}$ in the $\\ell_\\infty$-norm\n\nThe projection onto $B_\\infty(\\tau)$ is also nonexpansive in the $\\ell_\\infty$-norm.\nLet $x_u = P_{B_\\infty(\\tau)}(u)$ and $x_v = P_{B_\\infty(\\tau)}(v)$. The projection is given by the scalar clipping function $f(t) = \\text{median}(-\\tau, t, \\tau)$ applied component-wise. The function $f$ is $1$-Lipschitz, since its derivative (where it exists) is either $0$ or $1$. Therefore, for any scalars $t_a, t_b$, we have $|f(t_a) - f(t_b)| \\le |t_a - t_b|$.\nApplying this to each component $i$:\n$$\n|(x_u)_i - (x_v)_i| = |f(u_i) - f(v_i)| \\le |u_i - v_i|\n$$\nTaking the maximum over all components $i$:\n$$\n\\|x_u - x_v\\|_\\infty = \\max_i |(x_u)_i - (x_v)_i| \\le \\max_i |u_i - v_i| = \\|u-v\\|_\\infty\n$$\nThus, $\\|P_{B_\\infty(\\tau)}(u) - P_{B_\\infty(\\tau)}(v)\\|_\\infty \\le \\|u-v\\|_\\infty$.\n\n### 5. Computational Tradeoffs\n\n- **$P_{B_\\infty(\\tau)}$**: The algorithm involves a component-wise clipping operation. For a vector of dimension $n$, this requires a constant number of operations per component (two comparisons and one assignment). The total computational cost is therefore linear in $n$, with an asymptotic complexity of $O(n)$.\n- **$P_{B_1(\\tau)}$**: The algorithm's main computational steps are:\n    1. Calculating $\\|y\\|_1$: $O(n)$ operations.\n    2. Sorting $|y|$: $O(n \\log n)$ operations.\n    3. Finding $\\rho$ via a linear scan: $O(n)$ operations.\n    4. Computing the final projection via soft-thresholding: $O(n)$ operations.\nThe dominant step is sorting, so the overall asymptotic complexity is $O(n \\log n)$.\n\nFor large $n$, the $O(n)$ complexity of the $\\ell_\\infty$-ball projection is significantly more efficient than the $O(n \\log n)$ complexity of the $\\ell_1$-ball projection. The ratio of dominant operation counts, as defined in the problem, is $R(n) = \\frac{n \\log_2 n}{n} = \\log_2 n$, which grows with $n$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef proj_l_inf_ball(y: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of a vector y onto the L_infinity-ball B_inf(tau).\n    x_i = median(-tau, y_i, tau)\n    \"\"\"\n    if tau < 0:\n        raise ValueError(\"Radius tau must be non-negative.\")\n    return np.clip(y, -tau, tau)\n\ndef proj_l1_ball(y: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of a vector y onto the L1-ball B_1(tau).\n    The algorithm is based on Duchi et al., \"Efficient Projections onto the L1-Ball...\" (2008).\n    \"\"\"\n    if tau < 0:\n        raise ValueError(\"Radius tau must be non-negative.\")\n    \n    # If tau is 0, the L1 ball is just the origin.\n    if tau == 0:\n        return np.zeros_like(y)\n    \n    y_abs = np.abs(y)\n    l1_norm = np.sum(y_abs)\n    \n    # If the vector is already in the ball, protection is the identity.\n    if l1_norm <= tau:\n        return y\n    \n    # The core of the algorithm for vectors outside the ball.\n    n = len(y)\n    \n    # Sort absolute values in descending order.\n    # We could sort y_abs directly, but we only need the sorted values.\n    u = np.sort(y_abs)[::-1]\n    \n    # Compute cumulative sums of the sorted values.\n    s = np.cumsum(u)\n    \n    # Find rho, the number of non-zero elements in the projected vector.\n    # This corresponds to finding the largest k such that u_k - (1/k)(s_k - tau) > 0.\n    k_array = np.arange(1, n + 1)\n    condition = u > (s - tau) / k_array\n    \n    # rho is the index of the last True value in 'condition'\n    # np.where returns a tuple, we take the first element (the array of indices).\n    # The indices in `where` correspond to the sorted array `u`.\n    rho_candidates = np.where(condition)[0]\n    rho = rho_candidates[-1] + 1\n    \n    # Compute the Lagrange multiplier lambda.\n    lambda_val = (s[rho - 1] - tau) / rho\n    \n    # Apply soft thresholding.\n    return np.sign(y) * np.maximum(y_abs - lambda_val, 0)\n\ndef solve():\n    \"\"\"\n    Main function to run all tests and generate the final output.\n    \"\"\"\n    eps = 1e-12\n    results = []\n\n    # Test cases definition\n    tests_single = [\n        (np.array([3, -1, 2, 0.5]), 2.0),\n        (np.array([0, 0, 0, 0]), 0.0),\n        (np.array([1, 1, 1, 1]), 2.0),\n        (np.array([10, -10, 0.1, -0.1, 5]), 7.0)\n    ]\n\n    tests_pair = [\n        (np.array([3, -1, 2]), np.array([2.5, 0, -1]), 2.0),\n        (np.array([-5, 4, -3, 2]), np.array([1, -2, 3, -4]), 3.0),\n        (np.array([0.2, -0.3, 0.4, -0.5, 0.6]), np.array([-0.1, 0.1, -0.2, 0.2, -0.3]), 0.7)\n    ]\n    \n    n_vals_ratio = [20, 1000, 100000]\n\n    # --- Single-vector projection tests ---\n    for y, tau in tests_single:\n        # L1 projection feasibility\n        p_l1 = proj_l1_ball(y, tau)\n        results.append(np.linalg.norm(p_l1, 1) <= tau + eps)\n        \n        # L-inf projection feasibility\n        p_linf = proj_l_inf_ball(y, tau)\n        results.append(np.linalg.norm(p_linf, ord=np.inf) <= tau + eps)\n\n    # --- Pairwise nonexpansiveness tests (Euclidean norm) ---\n    for ya, yb, tau in tests_pair:\n        dist_y = np.linalg.norm(ya - yb, 2)\n        \n        # L1 projection nonexpansiveness\n        pa_l1 = proj_l1_ball(ya, tau)\n        pb_l1 = proj_l1_ball(yb, tau)\n        dist_p_l1 = np.linalg.norm(pa_l1 - pb_l1, 2)\n        results.append(dist_p_l1 <= dist_y + eps)\n        \n        # L-inf projection nonexpansiveness\n        pa_linf = proj_l_inf_ball(ya, tau)\n        pb_linf = proj_l_inf_ball(yb, tau)\n        dist_p_linf = np.linalg.norm(pa_linf - pb_linf, 2)\n        results.append(dist_p_linf <= dist_y + eps)\n        \n    # --- Additional norm-specific check (L-inf nonexpansiveness of L-inf projection) ---\n    for ya, yb, tau in tests_pair:\n        dist_y_inf = np.linalg.norm(ya - yb, ord=np.inf)\n        pa_linf = proj_l_inf_ball(ya, tau)\n        pb_linf = proj_l_inf_ball(yb, tau)\n        dist_p_linf_inf = np.linalg.norm(pa_linf - pb_linf, ord=np.inf)\n        results.append(dist_p_linf_inf <= dist_y_inf + eps)\n\n    # --- Computational tradeoff quantification ---\n    for n in n_vals_ratio:\n        # R(n) = (n log2 n) / n = log2 n\n        results.append(np.log2(n))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}