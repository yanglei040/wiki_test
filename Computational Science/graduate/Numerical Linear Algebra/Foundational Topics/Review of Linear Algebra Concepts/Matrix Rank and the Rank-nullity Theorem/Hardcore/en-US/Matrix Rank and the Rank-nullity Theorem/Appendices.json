{
    "hands_on_practices": [
        {
            "introduction": "Before delving into the complexities of numerical computation, it is crucial to solidify our understanding of the rank-nullity theorem in its pure, theoretical form. This exercise provides a direct, hands-on verification of the theorem for a small, well-structured matrix. By manually computing the rank via Gaussian elimination and the nullity by solving the corresponding homogeneous system , you will reinforce the fundamental definitions and witness the elegant balance described by the theorem in a clean, uncluttered setting.",
            "id": "1061213",
            "problem": "Consider the $3 \\times 3$ Toeplitz matrix given by:\n\n$$\nA = \\begin{bmatrix} \n2 & 1 & 0 \\\\ \n1 & 2 & 1 \\\\ \n0 & 1 & 2 \n\\end{bmatrix}.\n$$\n\nThe rank-nullity theorem states that for any matrix $A$, the sum of the rank of $A$ and the nullity of $A$ equals the number of columns of $A$. Compute this sum for the matrix $A$.",
            "solution": "To compute the sum of the rank and nullity of $A$, we first determine the rank by row-reducing $A$ to row-echelon form.\n\nStarting with:\n\n$$\nA = \\begin{bmatrix} \n2 & 1 & 0 \\\\ \n1 & 2 & 1 \\\\ \n0 & 1 & 2 \n\\end{bmatrix},\n$$\n\nperform the row operation $R_2 \\leftarrow 2R_2 - R_1$:\n\n$$\n2R_2 = \\begin{bmatrix} 2 & 4 & 2 \\end{bmatrix}, \\quad 2R_2 - R_1 = \\begin{bmatrix} 2-2 & 4-1 & 2-0 \\end{bmatrix} = \\begin{bmatrix} 0 & 3 & 2 \\end{bmatrix}.\n$$\n\nThe matrix becomes:\n\n$$\n\\begin{bmatrix} \n2 & 1 & 0 \\\\ \n0 & 3 & 2 \\\\ \n0 & 1 & 2 \n\\end{bmatrix}.\n$$\n\nNext, perform $R_3 \\leftarrow 3R_3 - R_2$:\n\n$$\n3R_3 = \\begin{bmatrix} 0 & 3 & 6 \\end{bmatrix}, \\quad 3R_3 - R_2 = \\begin{bmatrix} 0 & 3-3 & 6-2 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 4 \\end{bmatrix}.\n$$\n\nThe row-echelon form is:\n\n$$\n\\begin{bmatrix} \n2 & 1 & 0 \\\\ \n0 & 3 & 2 \\\\ \n0 & 0 & 4 \n\\end{bmatrix}.\n$$\n\nThis has three non-zero rows, so the rank of $A$ is $3$.\n\nTo find the nullity, solve $A\\mathbf{x} = \\mathbf{0}$ using the row-echelon form. The system is:\n\\begin{align*}\n2x + y &= 0, \\\\\n3y + 2z &= 0, \\\\\n4z &= 0.\n\\end{align*}\nFrom $4z = 0$, $z = 0$. Substituting into $3y + 2z = 0$ gives $3y = 0$, so $y = 0$. Then $2x + y = 0$ implies $2x = 0$, so $x = 0$. The only solution is the trivial solution, so the null space is $\\{ \\mathbf{0} \\}$, and the nullity is $0$.\n\nThe sum of the rank and nullity is $3 + 0 = 3$.",
            "answer": "\\[\n\\boxed{3}\n\\]"
        },
        {
            "introduction": "Moving from exact arithmetic to the world of floating-point computation, we encounter the challenge of creating robust algorithms for concepts like the null space. This practice guides you through the development of a practical method to estimate a basis for the null space using the column-pivoted QR factorization, a cornerstone of numerical linear algebra. You will not only implement the algorithm from first principles but also investigate the sensitivity of the result to column pivot ordering , providing crucial insight into how algorithmic choices impact the accuracy and reliability of numerical solutions.",
            "id": "3558928",
            "problem": "You are to implement a complete, runnable program that, for given real matrices, estimates a basis for the null space $\\ker(A)$ using only the upper-triangular factor $R$ obtained from a Column-Pivoted Orthogonal-Triangular (QR) factorization, and then analyzes the sensitivity of the estimation to the column pivot order. The investigation must be grounded in the core definitions of rank, null space, and the rank-nullity theorem, and must proceed from these principles without relying on any shortcut formulas in the problem statement.\n\nFundamental base for the derivation:\n- For a matrix $A \\in \\mathbb{R}^{m \\times n}$, the rank-nullity theorem states that $\\operatorname{rank}(A) + \\dim \\ker(A) = n$.\n- The null space $\\ker(A)$ is the set of all vectors $x \\in \\mathbb{R}^{n}$ such that $A x = 0$.\n- A Column-Pivoted Orthogonal-Triangular (QR) factorization produces a permutation of the columns and orthogonal-triangular factors, where the factorization satisfies $A P = Q R$, with $P$ a permutation matrix, $Q$ orthogonal, and $R$ upper-triangular (or upper-trapezoidal).\n\nTasks to perform:\n1. For each test matrix $A$, compute a Column-Pivoted QR factorization of $A$ with a prescribed column pre-permutation. Use the diagonal of the resulting $R$ to estimate the numerical rank $\\hat{r}$ via a threshold rule: treat $R_{ii}$ values with magnitude below a threshold as numerically zero. Let the threshold be specified by a dimensionless parameter $\\tau$ scaled relative to the largest diagonal magnitude of $R$.\n2. Using only $R$ and the pivot information, construct an estimate of a basis for $\\ker(A)$ of dimension $n - \\hat{r}$, and map it back to the original column ordering. Do not use Singular Value Decomposition (SVD) in this construction; use only $R$ and the pivot information.\n3. Quantify the quality of the estimated basis by computing the Frobenius norm of the residual $A N$, where $N$ is the matrix whose columns form the estimated basis for the null space. Report this norm as a floating-point number.\n4. To analyze sensitivity to pivot order, repeat the construction of the null space basis under a second column pre-permutation of $A$. Compute the principal angles between the two estimated null space subspaces, and report the maximum principal angle. The angle unit must be radians.\n5. Additionally, report whether the two runs (with different pre-permutations) agree on the estimated nullity $\\widehat{\\nu} = n - \\hat{r}$, as a boolean.\n\nOutput specification for each test case:\n- Produce a list containing five items:\n  - The estimated rank $\\hat{r}$ as an integer.\n  - The estimated nullity $\\widehat{\\nu} = n - \\hat{r}$ as an integer.\n  - The Frobenius norm $\\|A N\\|_F$ as a float.\n  - The maximum principal angle between the two estimated null spaces (in radians) as a float.\n  - A boolean indicating whether the two runs agree on $\\widehat{\\nu}$.\n- Aggregate the results for all test cases into a single line of output containing the lists as a comma-separated list enclosed in square brackets, for example, $[\\text{case1},\\text{case2},\\ldots]$.\n\nTest suite:\nImplement the following four scientifically sound and diverse test cases. All random draws must be reproducible by using the specified seeds.\n\n- Test case 1 (happy path, full row rank, rectangular):\n  - $A \\in \\mathbb{R}^{4 \\times 7}$ with entries drawn independently from a standard normal distribution using seed $12345$.\n  - Threshold parameter $\\tau = 10^{-12}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4,5,6\\}$.\n  - Second pre-permutation: reversed order $\\{6,5,4,3,2,1,0\\}$.\n\n- Test case 2 (near rank-deficient with controlled singular spectrum):\n  - $A \\in \\mathbb{R}^{6 \\times 8}$ constructed as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{6 \\times 6}$ and $V \\in \\mathbb{R}^{8 \\times 8}$ are orthogonal matrices obtained by QR factorizations of standard normal matrices using seed $2024$, and $\\Sigma \\in \\mathbb{R}^{6 \\times 8}$ has diagonal singular values $\\{1, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-12}, 10^{-12}\\}$ on its first $6$ diagonal entries (zeros elsewhere).\n  - Threshold parameter $\\tau = 10^{-8}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4,5,6,7\\}$.\n  - Second pre-permutation: random permutation derived from seed $4242$ applied to $\\{0,\\ldots,7\\}$.\n\n- Test case 3 (equal-norm columns to induce pivot-order sensitivity):\n  - $A \\in \\mathbb{R}^{5 \\times 9}$ with columns normalized to unit $2$-norm. Construct $G \\in \\mathbb{R}^{5 \\times 9}$ with standard normal entries using seed $99$, and define $A$ by $A[:,j] = G[:,j] / \\|G[:,j]\\|_2$ for each column index $j$.\n  - Threshold parameter $\\tau = 10^{-10}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4,5,6,7,8\\}$.\n  - Second pre-permutation: reversed order $\\{8,7,6,5,4,3,2,1,0\\}$.\n\n- Test case 4 (structured deficiency via duplicate and dependent columns):\n  - $A \\in \\mathbb{R}^{5 \\times 5}$ constructed as follows: draw $B \\in \\mathbb{R}^{5 \\times 3}$ with standard normal entries using seed $555$, and set $A = [b_1, b_2, b_3, b_1 + b_2, b_2]$, where $b_1$, $b_2$, $b_3$ are the columns of $B$.\n  - Threshold parameter $\\tau = 10^{-12}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4\\}$.\n  - Second pre-permutation: permutation $\\{2,1,0,4,3\\}$.\n\nAngle unit requirement:\n- All reported angles must be expressed in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself being a list of the five items described above, for example, $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$.",
            "solution": "The problem requires the implementation of an algorithm to estimate the null space of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ using its Column-Pivoted QR factorization. The sensitivity of this estimation to the column pivot order must also be analyzed. The entire procedure is to be derived from first principles, namely the definitions of matrix rank, null space, the rank-nullity theorem, and QR factorization.\n\n### Theoretical Foundation\n\nThe rank-nullity theorem states that for any linear map represented by a matrix $A \\in \\mathbb{R}^{m \\times n}$, the dimension of its domain ($n$) is the sum of its rank and the dimension of its null space (kernel). Formally:\n$$ \\operatorname{rank}(A) + \\dim(\\ker(A)) = n $$\nThe null space, $\\ker(A)$, is the set of all vectors $x \\in \\mathbb{R}^{n}$ that are mapped to the zero vector: $\\ker(A) = \\{x \\in \\mathbb{R}^{n} \\mid Ax = 0\\}$. Our goal is to find a basis for this vector subspace.\n\nA Column-Pivoted QR factorization of a matrix $A_{p} \\in \\mathbb{R}^{m \\times n}$ provides a decomposition $A_{p}P = QR$, where $P$ is a permutation matrix, $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($Q^T Q = I$), and $R \\in \\mathbb{R}^{m \\times n}$ is an upper-trapezoidal matrix. The problem involves a prescribed pre-permutation of columns of $A$ before this factorization. Let this pre-permutation be represented by a permutation matrix $P_{pre}$. We first form the matrix $A' = AP_{pre}$. Then, we compute the column-pivoted QR factorization of $A'$, which yields $A'P_{qr} = QR$. Combining these, we have:\n$$ (AP_{pre})P_{qr} = QR \\implies A(P_{pre}P_{qr}) = QR $$\nLet $P_{total} = P_{pre}P_{qr}$ be the total permutation matrix. The original problem of finding $x$ such that $Ax=0$ can be transformed. Let $x = P_{total}y$ for some vector $y \\in \\mathbb{R}^n$. Substituting this into the null space equation gives:\n$$ A(P_{total}y) = 0 \\implies (AP_{total})y = 0 \\implies QRy = 0 $$\nSince $Q$ is orthogonal, it is invertible, and we can multiply by $Q^T$ from the left:\n$$ Q^TQRy = Q^T0 \\implies Iy = 0 \\implies Ry = 0 $$\nThus, finding the null space of $A$ is equivalent to finding the null space of $R$ and then mapping the basis vectors back using the total permutation $P_{total}$.\n\n### Null Space Construction from $R$\n\nThe matrix $R$ is upper-trapezoidal. Due to column pivoting, the diagonal elements $|R_{ii}|$ are sorted in a non-increasing order, which facilitates rank estimation. The numerical rank $\\hat{r}$ can be estimated by counting the number of diagonal elements whose magnitude is above a certain threshold. This threshold is defined as $\\delta = \\tau \\cdot \\max_i(|R_{ii}|)$, where $\\tau$ is a given tolerance parameter. All $|R_{ii}| < \\delta$ are considered numerically zero. The estimated rank $\\hat{r}$ is the count of diagonal entries satisfying $|R_{ii}| \\ge \\delta$. Consequently, the estimated nullity is $\\widehat{\\nu} = n - \\hat{r}$.\n\nTo find a basis for $\\ker(R)$, we solve $Ry=0$. We partition the matrix $R$ and the vector $y$ according to the estimated rank $\\hat{r}$:\n$$ R = \\begin{pmatrix} R_{11} & R_{12} \\\\ 0 & R_{22} \\end{pmatrix}, \\quad y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} $$\nHere, $R_{11} \\in \\mathbb{R}^{\\hat{r} \\times \\hat{r}}$ is an upper-triangular matrix with non-zero diagonal entries (by definition of $\\hat{r}$), making it invertible. $R_{12} \\in \\mathbb{R}^{\\hat{r} \\times \\widehat{\\nu}}$, and $R_{22} \\in \\mathbb{R}^{(m-\\hat{r}) \\times \\widehat{\\nu}}$ is a matrix whose entries are all numerically small. The vector $y$ is partitioned into $y_1 \\in \\mathbb{R}^{\\hat{r}}$ and $y_2 \\in \\mathbb{R}^{\\widehat{\\nu}}$.\n\nThe equation $Ry=0$ expands to:\n$$ R_{11}y_1 + R_{12}y_2 = 0 $$\n$$ R_{22}y_2 = 0 $$\nApproximating $R_{22}$ as a zero matrix, the second equation is satisfied for any choice of $y_2$. This confirms that the dimension of the null space is indeed $\\widehat{\\nu}$. To construct a basis, we can choose $\\widehat{\\nu}$ linearly independent vectors for $y_2$. The standard choice is the set of columns of the identity matrix $I_{\\widehat{\\nu}} \\in \\mathbb{R}^{\\widehat{\\nu} \\times \\widehat{\\nu}}$.\n\nFor each basis vector $e_k$ selected for $y_2$, we solve for the corresponding $y_1$ from the first equation:\n$$ R_{11}y_1 = -R_{12}y_2 $$\nSince $R_{11}$ is invertible, we have a unique solution for $y_1$:\n$$ y_1 = -R_{11}^{-1}R_{12}y_2 $$\nThis system can be solved efficiently for all $\\widehat{\\nu}$ basis vectors of $y_2$ at once by solving the matrix equation $R_{11}X = -R_{12}$, where the columns of $X \\in \\mathbb{R}^{\\hat{r} \\times \\widehat{\\nu}}$ will correspond to the $y_1$ parts of the null space basis vectors. This is a triangular system that can be solved via back substitution.\n\nThe resulting basis for $\\ker(R)$ is represented by the columns of the matrix $Z \\in \\mathbb{R}^{n \\times \\widehat{\\nu}}$:\n$$ Z = \\begin{pmatrix} X \\\\ I_{\\widehat{\\nu}} \\end{pmatrix} = \\begin{pmatrix} -R_{11}^{-1} R_{12} \\\\ I_{\\widehat{\\nu}} \\end{pmatrix} $$\nThe columns of $Z$ are the vectors $y^{(k)}$. To obtain the basis for $\\ker(A)$, we must apply the permutation $P_{total}$. The basis vectors for $\\ker(A)$ are the columns of $N = P_{total}Z$. If $p_{total}$ is the integer vector representing the permutation $P_{total}$, this operation rearranges the rows of $Z$: $N[p_{total}, :] = Z$.\n\n### Basis Orthonormalization and Comparison\n\nThe constructed basis $N$ is not guaranteed to be orthonormal. For robust numerical comparisons, it is beneficial to convert it to an orthonormal basis spanning the same subspace. This is achieved via a QR factorization of $N$ itself, $N = \\tilde{N}R_N$. The columns of $\\tilde{N}$ form the desired orthonormal basis.\n\nTo analyze the sensitivity to pivot order, we generate two different null space bases, $\\tilde{N}_1$ and $\\tilde{N}_2$, using two different initial column pre-permutations. We compare them by:\n1.  **Nullity Agreement**: A boolean check if the estimated nullities $\\widehat{\\nu}_1$ and $\\widehat{\\nu}_2$ are equal.\n2.  **Maximum Principal Angle**: The principal angles $\\{\\theta_k\\}$ between the subspaces spanned by $\\tilde{N}_1$ and $\\tilde{N}_2$ quantify their geometric alignment. The cosines of these angles are the singular values of the matrix $\\tilde{N}_1^T \\tilde{N}_2$. The maximum principal angle, $\\max_k \\theta_k$, indicates the largest deviation between the two subspaces. This is computed efficiently using `scipy.linalg.subspace_angles`.\n\n### Algorithmic Implementation\n\nThe overall algorithm for each test case is as follows:\n1.  For each of the two prescribed pre-permutations:\n    a. Permute the columns of the input matrix $A$.\n    b. Perform column-pivoted QR factorization on the permuted matrix to obtain $Q$, $R$, and an additional pivot order.\n    c. Combine the pre-permutation and the QR pivot order to get the total permutation.\n    d. Estimate the rank $\\hat{r}$ and nullity $\\widehat{\\nu}$ from the diagonal of $R$ using the specified tolerance $\\tau$.\n    e. If $\\widehat{\\nu} > 0$, construct the basis $N$ for the null space of $A$ as derived above. This involves solving a triangular system with $R_{11}$ and permuting the basis vectors back to the original ordering.\n    f. Orthonormalize the basis $N$ to get $\\tilde{N}$.\n2.  Report the rank $\\hat{r}_1$ and nullity $\\widehat{\\nu}_1$ from the first run.\n3.  Calculate the Frobenius norm of the residual, $\\|A\\tilde{N}_1\\|_F$, to assess the quality of the first estimated basis.\n4.  Calculate the maximum principal angle between the subspaces spanned by $\\tilde{N}_1$ and $\\tilde{N}_2$.\n5.  Report a boolean indicating if $\\widehat{\\nu}_1 = \\widehat{\\nu}_2$.\nThese five results are collected for each test case as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    def analyze_null_space(A, tau, pre_permutation):\n        \"\"\"\n        Analyzes a matrix A to find its null space basis using QR factorization.\n\n        Args:\n            A (np.ndarray): The input matrix.\n            tau (float): The threshold parameter for rank estimation.\n            pre_permutation (np.ndarray): The initial column permutation indices.\n\n        Returns:\n            tuple: A tuple containing:\n                - est_rank (int): Estimated numerical rank.\n                - est_nullity (int): Estimated numerical nullity.\n                - ortho_null_basis (np.ndarray): Orthonormal basis for the null space.\n                - total_perm_indices (np.ndarray): Total permutation indices.\n        \"\"\"\n        m, n = A.shape\n        if n == 0:\n            return 0, 0, np.empty((0, 0)), np.array([], dtype=int)\n\n        A_perm = A[:, pre_permutation]\n        Q, R, qr_pivot_indices = linalg.qr(A_perm, pivoting=True)\n\n        k = min(m, n)\n        diag_R = np.abs(np.diag(R[:k, :k]))\n        \n        # Handle case where diag_R is empty\n        if diag_R.size == 0:\n            max_r_diag = 0.0\n        else:\n            max_r_diag = np.max(diag_R)\n\n        # To avoid threshold being 0 if max_r_diag is 0\n        if max_r_diag > 0:\n            threshold = tau * max_r_diag\n        else:\n            threshold = tau\n            \n        est_rank = np.sum(diag_R >= threshold)\n        est_nullity = n - est_rank\n\n        total_perm_indices = pre_permutation[qr_pivot_indices]\n\n        if est_nullity == 0:\n            return est_rank, est_nullity, np.empty((n, 0)), total_perm_indices\n\n        # Partition R\n        # R is (m,n), we need its upper-left (r x n) part for computation\n        R_rank_part = R[:est_rank, :]\n        R11 = R_rank_part[:, :est_rank]\n        R12 = R_rank_part[:, est_rank:]\n\n        # Solve R11 * X = -R12 using back-substitution\n        X = linalg.solve_triangular(R11, -R12, lower=False)\n\n        # Construct the basis for ker(R)\n        Z = np.vstack((X, np.eye(est_nullity)))\n        \n        # Apply inverse of total permutation to get basis for ker(A)\n        # N[total_perm_indices, :] = Z\n        null_basis = np.zeros_like(Z)\n        null_basis[total_perm_indices] = Z\n        \n        # Orthonormalize the basis for stability in angle calculations\n        ortho_null_basis, _ = np.linalg.qr(null_basis)\n\n        return est_rank, est_nullity, ortho_null_basis, total_perm_indices\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Happy path\n        {\n            \"name\": \"Full row rank, rectangular\",\n            \"seed\": 12345,\n            \"shape\": (4, 7),\n            \"tau\": 1e-12,\n            \"perm1\": np.arange(7),\n            \"perm2\": np.arange(7)[::-1],\n            \"builder\": lambda shape, seed: np.random.default_rng(seed).standard_normal(shape)\n        },\n        # Case 2: Near rank-deficient\n        {\n            \"name\": \"Near rank-deficient\",\n            \"seed\": 2024,\n            \"shape\": (6, 8),\n            \"tau\": 1e-8,\n            \"perm1\": np.arange(8),\n            \"perm2\": np.random.default_rng(4242).permutation(8),\n            \"builder\": lambda shape, seed: None # Patched by special handling block below.\n        },\n        # Case 3: Equal-norm columns\n        {\n            \"name\": \"Equal-norm columns\",\n            \"seed\": 99,\n            \"shape\": (5, 9),\n            \"tau\": 1e-10,\n            \"perm1\": np.arange(9),\n            \"perm2\": np.arange(9)[::-1],\n            \"builder\": lambda shape, seed: (\n                lambda G: G / np.linalg.norm(G, axis=0)\n            )(np.random.default_rng(seed).standard_normal(shape))\n        },\n        # Case 4: Structured deficiency\n        {\n            \"name\": \"Structured deficiency\",\n            \"seed\": 555,\n            \"shape\": (5, 5),\n            \"tau\": 1e-12,\n            \"perm1\": np.arange(5),\n            \"perm2\": np.array([2, 1, 0, 4, 3]),\n            \"builder\": lambda shape, seed: (\n                lambda B: np.column_stack((B[:, 0], B[:, 1], B[:, 2], B[:, 0] + B[:, 1], B[:, 1]))\n            )(np.random.default_rng(seed).standard_normal((5, 3)))\n        }\n    ]\n\n    # Special handling for test case 2 builder\n    rng_u_v = np.random.default_rng(2024)\n    U_tc2, _ = linalg.qr(rng_u_v.standard_normal((6, 6)))\n    V_tc2, _ = linalg.qr(rng_u_v.standard_normal((8, 8)))\n    S_tc2 = np.zeros((6, 8))\n    s_vals = [1, 1e-1, 1e-2, 1e-3, 1e-12, 1e-12]\n    np.fill_diagonal(S_tc2, s_vals)\n    A_tc2 = U_tc2 @ S_tc2 @ V_tc2.T\n    test_cases[1]['builder'] = lambda shape, seed: A_tc2\n\n\n    results = []\n    for case in test_cases:\n        A = case[\"builder\"](case[\"shape\"], case[\"seed\"])\n        tau = case[\"tau\"]\n        perm1 = case[\"perm1\"]\n        perm2 = case[\"perm2\"]\n        m, n = A.shape\n\n        r1, nu1, N1, _ = analyze_null_space(A, tau, perm1)\n        r2, nu2, N2, _ = analyze_null_space(A, tau, perm2)\n\n        # Calculate Frobenius norm of the residual\n        if nu1 > 0:\n            fro_norm = np.linalg.norm(A @ N1, 'fro')\n        else:\n            fro_norm = 0.0\n\n        # Calculate max principal angle\n        # Only meaningful if both subspaces are non-trivial\n        if nu1 > 0 and nu2 > 0:\n            # subspace_angles returns angles sorted, max is the last one.\n            angles = linalg.subspace_angles(N1, N2)\n            max_angle = angles[-1] if angles.size > 0 else 0.0\n        else:\n            max_angle = 0.0\n\n        # Check for nullity agreement\n        nullity_agrees = (nu1 == nu2)\n\n        results.append([r1, nu1, fro_norm, max_angle, nullity_agrees])\n    \n    # Custom formatting to match the sample output exactly.\n    # Python's default float formatting may differ.\n    final_output_str = \"[\"\n    for i, res_list in enumerate(results):\n        final_output_str += \"[\"\n        final_output_str += f\"{res_list[0]},\"\n        final_output_str += f\"{res_list[1]},\"\n        final_output_str += f\"{res_list[2]:.15e},\"\n        final_output_str += f\"{res_list[3]:.15e},\"\n        final_output_str += f\"{str(res_list[4]).lower()}\"\n        final_output_str += \"]\"\n        if i  len(results) - 1:\n            final_output_str += \",\"\n    final_output_str += \"]\"\n    \n    print(final_output_str)\n\n\nsolve()\n```"
        },
        {
            "introduction": "In advanced applications, simply estimating the rank of a matrix is often insufficient; we must understand the *nature* of its potential deficiency. This practice takes you to the frontier of numerical analysis, applying rank concepts to the study of Krylov subspaces. You will design a sophisticated numerical diagnostic to distinguish between a \"true\" rank deficiency, which is a stable structural property, and \"slow growth\" or ill-conditioning, which is common in nearly defective matrices . Developing this stability-based test highlights the critical difference between a matrix being singular and being nearly singular, a distinction vital for the analysis of iterative methods and system dynamics.",
            "id": "3558881",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$. Define the $k$-step Krylov subspace generated by $A$ and $b$ as $\\mathcal{K}_k(A,b) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{k-1} b\\}$. Consider the square $n \\times n$ Krylov matrix $K_n(A,b) = [b, Ab, \\dots, A^{n-1} b]$ formed by concatenating the Krylov vectors as columns.\n\nStarting from core definitions and well-tested facts, derive a precise relationship between the dimension of the Krylov subspace and the rank of the Krylov matrix. Then, design and implement a numerical diagnostic that: (i) estimates the dimension of the Krylov subspace by computing the numerical rank of $K_n(A,b)$ via the Singular Value Decomposition (SVD; Singular Value Decomposition), and (ii) distinguishes a true rank deficiency (exact linear dependence among the Krylov columns) from slow Krylov growth due to near-dependence caused by defective or nearly defective $A$.\n\nUse as fundamental base:\n- The definition of rank, nullspace, and column space of a matrix.\n- The rank-nullity theorem for linear maps $T : \\mathbb{R}^n \\to \\mathbb{R}^n$, stating that $\\mathrm{rank}(T) + \\mathrm{nullity}(T) = n$.\n- The Singular Value Decomposition (SVD) characterizes the Euclidean operator norm and the distance of a matrix to the set of rank-deficient matrices via its smallest singular value.\n\nYour program must:\n1. Construct $K_n(A,b)$.\n2. Estimate the numerical rank $r$ of $K_n(A,b)$ using the criterion $r = \\#\\{ \\sigma_i : \\sigma_i  \\tau \\}$, where $\\sigma_i$ are the singular values of $K_n(A,b)$ and $\\tau = n \\cdot \\epsilon \\cdot \\|K_n(A,b)\\|_2$, with $\\epsilon$ the machine precision for double precision floating point.\n3. Compute the smallest singular value $\\sigma_{\\min}(K_n(A,b))$.\n4. Implement a stability-based diagnostic to classify the observed low rank as either a true deficiency or slow growth. The diagnostic must perform the following:\n   - Compute the baseline rank $r_0$ with threshold $\\tau_0 = n \\cdot \\epsilon \\cdot \\|K_n(A,b)\\|_2$.\n   - Vary the threshold across the set $\\{\\tau_0/10, \\tau_0, 10\\tau_0\\}$ and recompute the rank.\n   - Apply small, norm-scaled perturbations of magnitude $\\delta_A = 10^{-12} \\cdot \\max\\{1, \\|A\\|_2\\}$ to $A$ and $\\delta_b = 10^{-12} \\cdot \\max\\{1, \\|b\\|_2\\}$ to $b$, and recompute the rank under the same three thresholds for each perturbation case: $(A,b)$, $(A,b+\\Delta b)$, $(A+\\Delta A, b)$, and $(A+\\Delta A, b+\\Delta b)$ with randomly generated perturbations $\\Delta A$ and $\\Delta b$ using a fixed random seed for reproducibility.\n   - Define the stability score $s$ as the fraction of all recomputed ranks equal to $r_0$. Classify as \"true deficiency\" if $r_0  n$ and $s \\geq 0.9$, and as \"slow growth\" otherwise.\n\nProve the relationship between $\\dim \\mathcal{K}_n(A,b)$ and $\\mathrm{rank}(K_n(A,b))$ from the definitions and rank-nullity, and justify the numerical diagnostic design based on SVD properties and perturbation stability.\n\nYou must implement the above as a complete, runnable program. No user input is permitted.\n\nTest suite:\n- Case $1$ (structural deficiency in diagonal $A$): $n=6$, $A = \\mathrm{diag}(1,2,3,4,5,6)$, $b = [1, 1, 0, 0, 0, 0]^\\top$.\n- Case $2$ (defective $A$ with a single Jordan block, slow growth): $n=6$, $A = J_6(1)$, where $J_6(1)$ has ones on the diagonal and ones on the superdiagonal, zeros elsewhere; $b = e_1$.\n- Case $3$ (nearly defective, clustered spectrum, full rank but ill-conditioned): $n=6$, $A = \\mathrm{diag}(1, 1+10^{-12}, 1+2\\cdot 10^{-12}, 1+3\\cdot 10^{-12}, 1+4\\cdot 10^{-12}, 1+5\\cdot 10^{-12}) + U$, where $U$ is strictly upper bidiagonal with $10^{-12}$ on the superdiagonal; $b$ is a fixed random vector with entries drawn from the standard normal distribution and then scaled to unit norm using a fixed seed.\n- Case $4$ (boundary case): $n=6$, $A = \\mathrm{diag}(1,2,3,4,5,6)$, $b = 0$.\n\nAnswer specification:\n- For each case, output a list $[r, \\sigma_{\\min}, f]$ where $r$ is the estimated numerical rank (an integer), $\\sigma_{\\min}$ is the smallest singular value of $K_n(A,b)$ (a float), and $f$ is the classification flag (an integer, with $1$ indicating \"true deficiency\" and $0$ indicating \"slow growth or no deficiency\").\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the four cases, in order. For example, the format must be $[[r_1,\\sigma_{\\min,1},f_1],[r_2,\\sigma_{\\min,2},f_2],[r_3,\\sigma_{\\min,3},f_3],[r_4,\\sigma_{\\min,4},f_4]]$.\n- No physical units or angles are involved; all quantities are pure numbers.",
            "solution": "The problem is assessed as valid. It is scientifically grounded in numerical linear algebra, well-posed with a clear objective and a deterministic procedure, and is formally expressed. It presents a non-trivial challenge in distinguishing true rank deficiency from numerical ill-conditioning within the context of Krylov subspaces.\n\n### Theoretical Derivation\n\nThe problem asks for a precise relationship between the dimension of the Krylov subspace $\\mathcal{K}_n(A,b)$ and the rank of the Krylov matrix $K_n(A,b)$, based on fundamental definitions.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$.\nThe Krylov subspace is defined as the span of the first $n$ Krylov vectors:\n$$\n\\mathcal{K}_n(A,b) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{n-1} b\\}\n$$\nThe dimension of this subspace, $\\dim(\\mathcal{K}_n(A,b))$, is the size of the largest linearly independent subset of the generating vectors $\\{b, Ab, \\dots, A^{n-1} b\\}$.\n\nThe Krylov matrix is formed by concatenating these vectors as its columns:\n$$\nK_n(A,b) = [b | Ab | A^2 b | \\dots | A^{n-1} b]\n$$\nThe column space of a matrix is the subspace spanned by its column vectors. By definition, the column space of $K_n(A,b)$, denoted $\\mathrm{col}(K_n(A,b))$, is:\n$$\n\\mathrm{col}(K_n(A,b)) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{n-1} b\\}\n$$\nBy direct comparison of their definitions, the Krylov subspace and the column space of the Krylov matrix are identical:\n$$\n\\mathcal{K}_n(A,b) = \\mathrm{col}(K_n(A,b))\n$$\nThe rank of a matrix is defined as the dimension of its column space. That is:\n$$\n\\mathrm{rank}(K_n(A,b)) = \\dim(\\mathrm{col}(K_n(A,b)))\n$$\nCombining these two equalities yields the direct relationship:\n$$\n\\dim(\\mathcal{K}_n(A,b)) = \\mathrm{rank}(K_n(A,b))\n$$\nThis establishes that the dimension of the subspace is precisely the rank of the matrix whose columns generate that subspace. The rank-nullity theorem, $\\mathrm{rank}(T) + \\mathrm{nullity}(T) = n$, applies to the linear map $T_K: \\mathbb{R}^n \\to \\mathbb{R}^n$ represented by the matrix $K_n(A,b)$. The rank of this map is the dimension of its image, which is the column space. The nullity is the dimension of the kernel (or nullspace), which consists of coefficient vectors $c$ such that $\\sum_{i=0}^{n-1} c_i A^i b = 0$. Thus, the rank-nullity theorem connects the dimension of the Krylov subspace to the dimension of the space of linear dependencies among the Krylov vectors.\n\n### Justification of the Numerical Diagnostic\n\nThe diagnostic is designed to distinguish between two scenarios that can lead to a numerically low-rank Krylov matrix.\n\n1.  **Numerical Rank via SVD**: In finite-precision arithmetic, true mathematical rank is obscured by round-off errors. A matrix that is theoretically rank-deficient will have singular values that are computationally small but not exactly zero. The Singular Value Decomposition (SVD), $K_n(A,b) = U\\Sigma V^\\top$, where $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$, is the ideal tool for this analysis. The singular values $\\sigma_i$ measure the \"energy\" of the matrix in different orthogonal directions. A sharp drop in the magnitude of singular values indicates near rank-deficiency. The threshold $\\tau = n \\cdot \\epsilon \\cdot \\|K_n(A,b)\\|_2$ provides a standard, dynamically scaled cutoff to separate significant singular values from those considered to be numerical artifacts. Here, $\\|K_n(A,b)\\|_2 = \\sigma_1$ is the largest singular value, and $\\epsilon$ is machine precision.\n\n2.  **Stability-Based Classification**: The core of the diagnostic rests on the principle of structural stability.\n    *   **True Rank Deficiency**: This arises from an algebraic constraint, for instance, if the minimal polynomial of $A$ with respect to $b$ has a degree $d  n$. This means $\\dim(\\mathcal{K}_n(A,b)) = d$. The Krylov matrix $K_n(A,b)$ is exactly rank-deficient, possessing $n-d$ singular values that are mathematically zero. This is a structurally stable property. Small perturbations to $A$ and $b$ will only lead to small perturbations of these zero singular values. Similarly, modest variations in the rank-determination threshold (e.g., from $\\tau_0/10$ to $10\\tau_0$) are unlikely to cross the significant gap between the small and large singular values. The computed rank $r_0$ should therefore be highly consistent across these tests. The stability score $s$, defined as the fraction of tests yielding rank $r_0$, will be close to $1$. The criterion $s \\geq 0.9$ formalizes this expectation.\n    *   **Slow Growth (Ill-Conditioning)**: This occurs when the Krylov vectors $A^k b$ become nearly linearly dependent, but are not exactly so. This is typical for defective or nearly defective matrices (e.g., those with clustered eigenvalues). The Krylov matrix $K_n(A,b)$ is theoretically full-rank but severely ill-conditioned, meaning its condition number $\\kappa_2(K_n) = \\sigma_1/\\sigma_n$ is very large. The singular values decay gradually without a clear gap. Such a matrix is close to a rank-deficient one, as quantified by the Eckart-Young-Mirsky theorem: $\\min_{\\mathrm{rank}(X)n} \\|K_n-X\\|_2 = \\sigma_n$. In this scenario, the computed numerical rank is highly sensitive to the choice of threshold $\\tau$. Furthermore, the singular values of an ill-conditioned matrix are themselves sensitive to perturbations in the matrix entries. Therefore, small changes to $A$ and $b$ will cause significant fluctuations in the small singular values, leading to a computed rank that varies under perturbation. The stability score $s$ will be low, captured by the condition $s  0.9$. This instability is the hallmark of \"slow growth\".\n\nThe diagnostic systematically probes this stability. By varying both the rank-testing threshold and the input data $(A,b)$, it distinguishes the stable, gapped singular value spectrum of a truly deficient matrix from the unstable, gradually decaying spectrum of an ill-conditioned one.",
            "answer": "```python\nimport numpy as np\n\ndef construct_krylov(A, b, n):\n    \"\"\"\n    Constructs the n x n Krylov matrix K_n(A,b).\n    \"\"\"\n    K = np.zeros((n, n), dtype=float)\n    if np.linalg.norm(b) == 0:\n        return K\n    \n    current_vec = b.copy()\n    K[:, 0] = current_vec\n    for i in range(1, n):\n        current_vec = A @ current_vec\n        K[:, i] = current_vec\n    return K\n\ndef run_diagnostic(A, b, n):\n    \"\"\"\n    Performs the numerical diagnostic on K_n(A,b).\n    Returns [rank, sigma_min, classification_flag].\n    \"\"\"\n    # 1. Construct the unperturbed Krylov matrix and compute its properties.\n    K = construct_krylov(A, b, n)\n    \n    s_vals = np.linalg.svd(K, compute_uv=False)\n    \n    # Handle the zero matrix case (e.g., b=0).\n    if K.any(): # K is not the zero matrix\n        norm_K = s_vals[0]\n        sigma_min = s_vals[-1]\n    else:\n        norm_K = 0.0\n        sigma_min = 0.0\n\n    eps = np.finfo(float).eps\n    tau_0 = n * eps * norm_K\n    \n    r_0 = np.sum(s_vals > tau_0)\n    \n    # 2. Perform stability analysis.\n    rng = np.random.default_rng(seed=42)\n    \n    # Define perturbation magnitudes.\n    norm_A = np.linalg.norm(A, 2)\n    norm_b = np.linalg.norm(b, 2)\n    delta_A_mag = 1e-12 * max(1.0, norm_A)\n    delta_b_mag = 1e-12 * max(1.0, norm_b)\n\n    # Generate and scale random perturbations.\n    dA_rand = rng.standard_normal(size=(n, n))\n    db_rand = rng.standard_normal(size=n)\n    \n    dA = delta_A_mag * dA_rand / np.linalg.norm(dA_rand, 2)\n    \n    norm_db_rand = np.linalg.norm(db_rand)\n    db = delta_b_mag * db_rand / norm_db_rand if norm_db_rand > 0 else np.zeros(n)\n    \n    # Define systems and thresholds to test.\n    perturbed_systems = [\n        (A, b),\n        (A, b + db),\n        (A + dA, b),\n        (A + dA, b + db)\n    ]\n    \n    thresholds = [tau_0 / 10.0, tau_0, tau_0 * 10.0] if tau_0 > 0 else [0.0, 0.0, 0.0]\n\n    all_ranks = []\n    \n    for A_p, b_p in perturbed_systems:\n        K_p = construct_krylov(A_p, b_p, n)\n        s_p = np.linalg.svd(K_p, compute_uv=False)\n        for tau in thresholds:\n            rank_p = np.sum(s_p > tau)\n            all_ranks.append(rank_p)\n            \n    # 3. Calculate stability score and classify.\n    stable_count = all_ranks.count(r_0)\n    stability_score = stable_count / len(all_ranks)\n\n    is_deficient = (r_0  n)\n    is_stable = (stability_score >= 0.9)\n\n    # Classification flag: 1 for true deficiency, 0 otherwise.\n    f = 1 if (is_deficient and is_stable) else 0\n\n    return [int(r_0), sigma_min, f]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the diagnostic for each.\n    \"\"\"\n    n = 6\n    \n    # Case 1: Structural deficiency in a diagonal matrix.\n    A1 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    b1 = np.array([1.0, 1.0, 0.0, 0.0, 0.0, 0.0])\n    \n    # Case 2: Defective matrix (Jordan block), slow growth.\n    A2 = np.diag(np.ones(n)) + np.diag(np.ones(n - 1), k=1)\n    b2 = np.zeros(n)\n    b2[0] = 1.0\n    \n    # Case 3: Nearly defective matrix, ill-conditioned.\n    eps_m = 1e-12\n    diag_vals = 1.0 + np.arange(n) * eps_m\n    A3 = np.diag(diag_vals) + np.diag(np.full(n - 1, eps_m), k=1)\n    rng_b = np.random.default_rng(seed=123) # Fixed seed for b reproducibility\n    b3_rand = rng_b.standard_normal(n)\n    b3 = b3_rand / np.linalg.norm(b3_rand)\n\n    # Case 4: Boundary case with zero vector.\n    A4 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    b4 = np.zeros(n)\n    \n    test_cases = [\n        (A1, b1, n),\n        (A2, b2, n),\n        (A3, b3, n),\n        (A4, b4, n)\n    ]\n\n    results = []\n    for A, b, n_val in test_cases:\n        result = run_diagnostic(A, b, n_val)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}