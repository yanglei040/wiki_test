{
    "hands_on_practices": [
        {
            "introduction": "The power of linear algebra stems from its abstract axiomatic foundation, which allows its principles to be applied across diverse scientific fields. This exercise invites you to test the boundaries of these axioms by examining a structure that satisfies all but one of them. By identifying the specific axiom that fails and quantifying the failure, you will gain a deeper appreciation for the precise formulation of a vector space and why every axiom is indispensable. ",
            "id": "3600932",
            "problem": "Let $n \\geq 1$ be a fixed integer and consider the set $\\mathbb{R}^{n}$ of all $n$-tuples of real numbers equipped with the usual vector addition $+$ defined by $(u_{1},\\dots,u_{n}) + (v_{1},\\dots,v_{n}) = (u_{1}+v_{1},\\dots,u_{n}+v_{n})$. Define a modified scalar multiplication $\\odot$ by declaring, for each scalar $\\alpha \\in \\mathbb{R}$ and vector $v \\in \\mathbb{R}^{n}$, that $\\alpha \\odot v = \\alpha^{2} v$, where juxtaposition denotes the usual scalar multiplication in $\\mathbb{R}^{n}$. \n\nStarting from the standard axioms that characterize a vector space over the real numbers $\\mathbb{R}$ (closure and abelian group structure under $+$, compatibility of scalar multiplication with field multiplication, identity action of the scalar $1$, and both distributivity axioms), analyze whether $(\\mathbb{R}^{n}, +, \\odot)$ satisfies each axiom. Identify precisely which axiom fails and provide a rigorous explanation of the failure.\n\nFor arbitrary scalars $a, b \\in \\mathbb{R}$ and arbitrary nonzero $v \\in \\mathbb{R}^{n}$, define the residual $r(a,b,v) = (a+b) \\odot v - \\big(a \\odot v + b \\odot v\\big)$. It can be written uniquely in the form $r(a,b,v) = c(a,b) \\, v$ for a scalar function $c(a,b)$. Compute the closed-form expression for $c(a,b)$ as a function of $a$ and $b$. The final answer must be a single analytic expression. No rounding is required.",
            "solution": "The problem asks us to determine if the set $\\mathbb{R}^{n}$ for a fixed integer $n \\geq 1$, equipped with the standard vector addition $+$ and a modified scalar multiplication $\\odot$ defined by $\\alpha \\odot v = \\alpha^{2} v$, forms a vector space over the real numbers $\\mathbb{R}$. We must identify which vector space axiom fails and then compute a specific coefficient related to this failure.\n\nFirst, we will systematically examine the axioms required for $(\\mathbb{R}^{n}, +, \\odot)$ to be a vector space over $\\mathbb{R}$. A structure $(V, +, \\cdot)$ is a vector space over a field $F$ if it satisfies the following ten axioms for all $u, v, w \\in V$ and all scalars $\\alpha, \\beta \\in F$. In our case, $V = \\mathbb{R}^{n}$ and $F = \\mathbb{R}$.\n\nThe first five axioms concern the structure of $(V, +)$ as an abelian group:\n1.  **Closure of addition**: $u+v \\in \\mathbb{R}^{n}$.\n2.  **Associativity of addition**: $(u+v)+w = u+(v+w)$.\n3.  **Existence of an additive identity**: There exists a zero vector $0 \\in \\mathbb{R}^{n}$ such that $v+0=v$.\n4.  **Existence of additive inverses**: For each $v \\in \\mathbb{R}^{n}$, there exists $-v \\in \\mathbb{R}^{n}$ such that $v+(-v)=0$.\n5.  **Commutativity of addition**: $u+v=v+u$.\n\nThe problem states that the vector addition $+$ is the usual component-wise addition in $\\mathbb{R}^{n}$. The set $\\mathbb{R}^{n}$ with this standard addition is well-known to form an abelian group. Therefore, axioms $1$ through $5$ are all satisfied. The zero vector is $(0, \\dots, 0)$, and the additive inverse of $(v_1, \\dots, v_n)$ is $(-v_1, \\dots, -v_n)$.\n\nThe remaining five axioms concern the scalar multiplication $\\odot$ and its interaction with vector addition and the field operations in $\\mathbb{R}$.\n\n6.  **Closure of scalar multiplication**: For any scalar $\\alpha \\in \\mathbb{R}$ and any vector $v \\in \\mathbb{R}^{n}$, the product $\\alpha \\odot v$ must be in $\\mathbb{R}^{n}$.\n    By definition, $\\alpha \\odot v = \\alpha^{2} v$. Since $\\alpha \\in \\mathbb{R}$, its square $\\alpha^{2}$ is also a real number. The expression $\\alpha^{2} v$ denotes the standard scalar multiplication of the vector $v \\in \\mathbb{R}^{n}$ by the scalar $\\alpha^{2}$. The result is a vector in $\\mathbb{R}^{n}$. Thus, this axiom is satisfied.\n\n7.  **Distributivity of scalar multiplication with respect to vector addition**: $\\alpha \\odot (u+v) = \\alpha \\odot u + \\alpha \\odot v$.\n    Let's evaluate both sides of the equation.\n    Left-hand side (LHS): $\\alpha \\odot (u+v) = \\alpha^{2} (u+v) = \\alpha^{2} u + \\alpha^{2} v$. Here we used the distributive property of the standard scalar multiplication.\n    Right-hand side (RHS): $\\alpha \\odot u + \\alpha \\odot v = \\alpha^{2} u + \\alpha^{2} v$.\n    Since LHS = RHS, this axiom is satisfied.\n\n8.  **Distributivity of scalar multiplication with respect to scalar addition**: $(\\alpha+\\beta) \\odot v = \\alpha \\odot v + \\beta \\odot v$.\n    Let's evaluate both sides for arbitrary scalars $\\alpha, \\beta \\in \\mathbb{R}$ and an arbitrary vector $v \\in \\mathbb{R}^{n}$.\n    LHS: $(\\alpha+\\beta) \\odot v = (\\alpha+\\beta)^{2} v = (\\alpha^{2} + 2\\alpha\\beta + \\beta^{2}) v$.\n    RHS: $\\alpha \\odot v + \\beta \\odot v = \\alpha^{2} v + \\beta^{2} v = (\\alpha^{2} + \\beta^{2}) v$.\n    For this axiom to hold, the equality $(\\alpha^{2} + 2\\alpha\\beta + \\beta^{2}) v = (\\alpha^{2} + \\beta^{2}) v$ must be true for all $\\alpha, \\beta \\in \\mathbb{R}$ and all $v \\in \\mathbb{R}^{n}$.\n    Subtracting $(\\alpha^{2} + \\beta^{2}) v$ from both sides, we get $(2\\alpha\\beta) v = 0$.\n    This equation must hold universally. However, if we choose non-zero values, for instance $\\alpha = 1$, $\\beta = 1$, and any non-zero vector $v \\in \\mathbb{R}^{n}$, the equation becomes $2v = 0$, which implies $v = 0$. This contradicts our choice of a non-zero $v$. Therefore, the equality does not hold in general.\n    This axiom fails.\n\n9.  **Compatibility of scalar multiplication with field multiplication**: $(\\alpha\\beta) \\odot v = \\alpha \\odot (\\beta \\odot v)$.\n    LHS: $(\\alpha\\beta) \\odot v = (\\alpha\\beta)^{2} v = \\alpha^{2}\\beta^{2} v$.\n    RHS: $\\alpha \\odot (\\beta \\odot v) = \\alpha \\odot (\\beta^{2} v) = \\alpha^{2}(\\beta^{2} v) = (\\alpha^{2}\\beta^{2})v$.\n    Since LHS = RHS, this axiom is satisfied.\n\n10. **Identity element of scalar multiplication**: $1 \\odot v = v$.\n    LHS: $1 \\odot v = 1^{2} v = 1v = v$.\n    Since LHS = $v$, this axiom is satisfied.\n\nThe analysis shows that exactly one axiom fails: the distributive law of scalar multiplication with respect to scalar addition.\n\nNow, we compute the closed-form expression for $c(a,b)$. The residual is defined as $r(a,b,v) = (a+b) \\odot v - \\big(a \\odot v + b \\odot v\\big)$ for arbitrary scalars $a, b \\in \\mathbb{R}$ and an arbitrary non-zero vector $v \\in \\mathbb{R}^{n}$. We are given that $r(a,b,v) = c(a,b) v$.\n\nWe substitute the definition of the modified scalar multiplication $\\odot$ into the expression for the residual:\n$r(a,b,v) = (a+b)^{2} v - \\left( a^{2} v + b^{2} v \\right)$\nUsing the distributive property of standard scalar multiplication over vector addition, we can factor out the vector $v$:\n$r(a,b,v) = \\left( (a+b)^{2} - (a^{2} + b^{2}) \\right) v$\nNext, we expand the term $(a+b)^{2}$:\n$r(a,b,v) = \\left( (a^{2} + 2ab + b^{2}) - (a^{2} + b^{2}) \\right) v$\nSimplifying the scalar coefficient:\n$r(a,b,v) = (a^{2} + 2ab + b^{2} - a^{2} - b^{2}) v$\n$r(a,b,v) = (2ab) v$\nWe are given that $r(a,b,v) = c(a,b) v$. By comparing this with our derived expression, we have:\n$c(a,b) v = (2ab) v$\nSince this must hold for any non-zero vector $v \\in \\mathbb{R}^{n}$, the scalar coefficients must be equal:\n$c(a,b) = 2ab$\nThis expression for $c(a,b)$ quantifies the extent to which the distributive axiom fails. The axiom holds only if $c(a,b)=0$, which only occurs if $a=0$ or $b=0$.",
            "answer": "$$\\boxed{2ab}$$"
        },
        {
            "introduction": "Moving from abstract definitions to concrete applications, this practice addresses two fundamental questions in vector space analysis: determining if a vector belongs to a given subspace and finding its closest approximation within that subspace. You will use standard tools of linear algebra, such as rank analysis and orthogonal projection, to solve a tangible problem in $\\mathbb{R}^{4}$. This exercise solidifies the link between the algebraic concept of a span and the geometric operation of projection, a crucial skill for countless applications in data analysis and approximation theory. ",
            "id": "3600936",
            "problem": "Let $n \\in \\mathbb{N}$ and consider the real vector space $\\mathbb{R}^{n}$ equipped with the standard Euclidean inner product. The span $\\operatorname{span}\\{v_{1},\\dots,v_{k}\\}$ of vectors $v_{1},\\dots,v_{k} \\in \\mathbb{R}^{n}$ is the set of all finite linear combinations of these vectors. A vector $w \\in \\mathbb{R}^{n}$ belongs to $\\operatorname{span}\\{v_{1},\\dots,v_{k}\\}$ if and only if there exist scalars $c_{1},\\dots,c_{k} \\in \\mathbb{R}$ such that $w = \\sum_{j=1}^{k} c_{j} v_{j}$, equivalently if the linear system formed by taking the matrix with columns $v_{1},\\dots,v_{k}$ has a solution.\n\nStarting from these definitions and the well-tested facts of linear algebra concerning linear systems, ranks, and orthogonal projections, carry out the following for the specific data\n$$\nv_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nv_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nv_{3} = \\begin{pmatrix}1 \\\\ 1 \\\\ 2 \\\\ 0\\end{pmatrix},\\quad\nw = \\begin{pmatrix}2 \\\\ -1 \\\\ 1 \\\\ 3\\end{pmatrix} \\in \\mathbb{R}^{4}.\n$$\n1. Formulate the linear system that encodes the condition $w \\in \\operatorname{span}\\{v_{1},v_{2},v_{3}\\}$ and determine, by principled reasoning from the definitions and properties of linear systems and ranks, whether this system is consistent.\n2. Using foundational facts about orthogonal projections onto subspaces defined as spans of vectors, solve for the orthogonal projection of $w$ onto the subspace $V = \\operatorname{span}\\{v_{1},v_{2},v_{3}\\}$ and deduce the Euclidean distance from $w$ to $V$.\n\nExpress your final answer as the exact value of the Euclidean distance (the $2$-norm) from $w$ to $V$, with no rounding.",
            "solution": "The problem consists of two parts. First, we determine if the vector $w$ lies in the subspace $V = \\operatorname{span}\\{v_{1}, v_{2}, v_{3}\\}$. Second, we compute the orthogonal projection of $w$ onto $V$ and find the Euclidean distance from $w$ to $V$.\n\n**Part 1: Consistency of the Linear System**\n\nA vector $w$ belongs to the span of a set of vectors $\\{v_{1}, v_{2}, v_{3}\\}$ if and only if it can be expressed as a linear combination of these vectors. That is, there must exist scalars $c_{1}, c_{2}, c_{3} \\in \\mathbb{R}$ such that:\n$$ c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} = w $$\nThis vector equation can be written as a linear system in matrix form, $Ac=w$, where the matrix $A$ has the vectors $v_{1}, v_{2}, v_{3}$ as its columns, and $c$ is the vector of coefficients.\nGiven the vectors:\n$$ v_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad v_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad v_{3} = \\begin{pmatrix}1 \\\\ 1 \\\\ 2 \\\\ 0\\end{pmatrix},\\quad w = \\begin{pmatrix}2 \\\\ -1 \\\\ 1 \\\\ 3\\end{pmatrix} $$\nThe matrix $A$ and the vector $c$ are:\n$$ A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix}, \\quad c = \\begin{pmatrix} c_{1} \\\\ c_{2} \\\\ c_{3} \\end{pmatrix} $$\nThe linear system is consistent if and only if the rank of the coefficient matrix $A$ is equal to the rank of the augmented matrix $[A|w]$.\n\nFirst, we determine the rank of $A$ by reducing it to row echelon form.\n$$ A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\\\ 0 & 0 & 0 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_1} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_2} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nThe row echelon form of $A$ has two non-zero rows, so $\\operatorname{rank}(A) = 2$. The fact that the rank is less than the number of vectors ($2 < 3$) demonstrates that the set $\\{v_{1}, v_{2}, v_{3}\\}$ is linearly dependent. We can observe that $v_{3} = v_{1} + v_{2}$.\n\nNext, we determine the rank of the augmented matrix $[A|w]$.\n$$ [A|w] = \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & -1 \\\\ 1 & 1 & 2 & 1 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} $$\nWe perform the same row operations:\n$$ \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & -1 \\\\ 1 & 1 & 2 & 1 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_1} \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & -1 \\\\ 0 & 1 & 1 & -1 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_2} \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 3 \\end{pmatrix} $$\nSwapping rows $R_3$ and $R_4$ to obtain a row echelon form:\n$$ \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & -1 \\\\ 0 & 0 & 0 & 3 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} $$\nThe row echelon form of $[A|w]$ has three non-zero rows, thus $\\operatorname{rank}([A|w]) = 3$.\nSince $\\operatorname{rank}(A) = 2 \\neq \\operatorname{rank}([A|w]) = 3$, the linear system is inconsistent. This proves that $w \\notin \\operatorname{span}\\{v_{1}, v_{2}, v_{3}\\}$.\n\n**Part 2: Orthogonal Projection and Distance**\n\nThe subspace $V$ is given by $V = \\operatorname{span}\\{v_{1}, v_{2}, v_{3}\\}$. As established, $v_{3} = v_{1} + v_{2}$, so $v_{3}$ is redundant. A basis for $V$ is $\\{v_{1}, v_{2}\\}$. Therefore, $V = \\operatorname{span}\\{v_{1}, v_{2}\\}$.\n\nThe orthogonal projection of $w$ onto $V$, which we denote as $p = \\operatorname{proj}_{V}(w)$, is the unique vector in $V$ such that the vector $w - p$ is orthogonal to every vector in $V$. The Euclidean distance from $w$ to $V$ is then $\\|w - p\\|$.\n\nThe projection $p$ can be found by solving the normal equations. Let $A'$ be the matrix whose columns form a basis for $V$, i.e., $A' = [v_{1}|v_{2}]$.\n$$ A' = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 0 & 0 \\end{pmatrix} $$\nThe projection $p$ is given by the formula $p = A'(A'^{T}A')^{-1}A'^{T}w$.\nFirst, we compute the Gram matrix $A'^{T}A'$:\n$$ A'^{T}A' = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\nNext, we find the inverse of this matrix:\n$$ (A'^{T}A')^{-1} = \\frac{1}{(2)(2) - (1)(1)} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} $$\nNow we compute $A'^{T}w$:\n$$ A'^{T}w = \\begin{pmatrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot2+0\\cdot(-1)+1\\cdot1+0\\cdot3 \\\\ 0\\cdot2+1\\cdot(-1)+1\\cdot1+0\\cdot3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} $$\nWe can now find the projection $p$. The coefficients of the linear combination of the basis vectors are $c' = (A'^{T}A')^{-1}A'^{T}w$:\n$$ c' = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}(6) \\\\ \\frac{1}{3}(-3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\nThe projection vector is $p = A'c' = 2v_{1} - 1v_{2}$:\n$$ p = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\nThe vector representing the shortest distance from $w$ to the subspace $V$ is the component of $w$ orthogonal to $V$, which is $w - p$:\n$$ w - p = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 3 \\end{pmatrix} $$\nThe Euclidean distance from $w$ to $V$ is the norm of this orthogonal vector:\n$$ \\|w - p\\| = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 3 \\end{pmatrix} \\right\\| = \\sqrt{0^{2} + 0^{2} + 0^{2} + 3^{2}} = \\sqrt{9} = 3 $$\nThus, the exact Euclidean distance from $w$ to the subspace $V$ is $3$.",
            "answer": "$$ \\boxed{3} $$"
        },
        {
            "introduction": "This practice elevates our analysis from individual vectors to the complete structure of a linear transformation, as described by the Fundamental Theorem of Linear Algebra. You will implement a computational algorithm using the Singular Value Decomposition (SVD) to extract orthonormal bases for the four fundamental subspaces associated with a given matrix. This exercise provides hands-on experience with one of the most powerful tools in numerical linear algebra, revealing the full geometric action of a matrix and laying the groundwork for advanced applications like data compression and principal component analysis (PCA). ",
            "id": "3600957",
            "problem": "You are given the task of programmatically extracting orthonormal bases for the four fundamental subspaces associated with a real matrix $A$, and numerically confirming the orthogonality relations that arise from the fundamental theorem of linear algebra. The four fundamental subspaces are the column space $\\mathcal{R}(A)$, the null space $\\mathcal{N}(A)$, the row space $\\mathcal{R}(A^\\top)$, and the left null space $\\mathcal{N}(A^\\top)$. The definitions to start from are: a subspace of a vector space over $\\mathbb{R}$ is any subset closed under addition and scalar multiplication; the column space $\\mathcal{R}(A)$ is $\\{A x : x \\in \\mathbb{R}^n\\}$; the null space $\\mathcal{N}(A)$ is $\\{x \\in \\mathbb{R}^n : A x = 0\\}$; the row space $\\mathcal{R}(A^\\top)$ is $\\{A^\\top y : y \\in \\mathbb{R}^m\\}$; and the left null space $\\mathcal{N}(A^\\top)$ is $\\{y \\in \\mathbb{R}^m : A^\\top y = 0\\}$. You may use the Singular Value Decomposition (SVD) or the QR decomposition (QR), but the extraction of orthonormal bases must be numerically justified by those decompositions.\n\nFrom the Singular Value Decomposition (SVD), if $A \\in \\mathbb{R}^{m \\times n}$, then $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ have orthonormal columns (orthogonal matrices), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_{\\min(m,n)} \\ge 0$. Partition $U = [U_r \\; U_0]$ and $V = [V_r \\; V_0]$ according to the numerical rank $r$ determined by a tolerance. Then $\\mathcal{R}(A)$ is spanned by the columns of $U_r$, $\\mathcal{R}(A^\\top)$ is spanned by the columns of $V_r$, $\\mathcal{N}(A)$ is spanned by the columns of $V_0$, and $\\mathcal{N}(A^\\top)$ is spanned by the columns of $U_0$. The fundamental orthogonality relations are $\\mathcal{N}(A)$ orthogonal to $\\mathcal{R}(A^\\top)$ and $\\mathcal{N}(A^\\top)$ orthogonal to $\\mathcal{R}(A)$.\n\nYour program must:\n- For each test matrix $A$, compute an SVD $A = U \\Sigma V^\\top$ and decide the numerical rank $r$ using the tolerance\n$$\\tau = \\max(m,n) \\, \\sigma_{\\max} \\, \\epsilon,$$\nwhere $m \\times n$ is the shape of $A$, $\\sigma_{\\max}$ is the largest singular value of $A$, and $\\epsilon$ is double-precision machine epsilon ($\\epsilon \\approx 2.22 \\times 10^{-16}$).\n- Form orthonormal bases for the four fundamental subspaces:\n    1. $\\mathcal{R}(A)$ basis: columns of $U_r \\in \\mathbb{R}^{m \\times r}$.\n    2. $\\mathcal{R}(A^\\top)$ basis: columns of $V_r \\in \\mathbb{R}^{n \\times r}$.\n    3. $\\mathcal{N}(A)$ basis: columns of $V_0 \\in \\mathbb{R}^{n \\times (n-r)}$.\n    4. $\\mathcal{N}(A^\\top)$ basis: columns of $U_0 \\in \\mathbb{R}^{m \\times (m-r)}$.\n- Numerically confirm, using the Frobenius norm, both the orthonormality of each basis and the orthogonality relations:\n    - For any basis matrix $Q$ with $k$ columns, check $Q^\\top Q \\approx I_k$ by computing $\\|Q^\\top Q - I_k\\|_F$.\n    - Check $\\|V_r^\\top V_0\\|_F \\approx 0$ and $\\|U_r^\\top U_0\\|_F \\approx 0$.\n- Additionally, confirm that the truncated reconstruction $A_r = U_r \\Sigma_r V_r^\\top$ approximates $A$ with small relative residual $\\|A - A_r\\|_F / \\|A\\|_F$ (use absolute residual $\\|A - A_r\\|_F$ if $\\|A\\|_F = 0$).\n- Verify the dimension equalities $r + \\dim(\\mathcal{N}(A)) = n$ and $r + \\dim(\\mathcal{N}(A^\\top)) = m$ as exact integer equalities.\n\nFor each test matrix, aggregate a single float equal to the maximum over all residual norms described above (orthonormality residuals, orthogonality residuals, and reconstruction residual), along with a boolean indicating whether the dimension equalities hold. The final output must aggregate the results of all provided test cases into a single line formatted as a comma-separated list enclosed in square brackets, in the order\n$$[e_1, b_1, e_2, b_2, e_3, b_3, e_4, b_4],$$\nwhere $e_k$ is the maximum residual float for the $k$-th test matrix, and $b_k$ is the corresponding boolean.\n\nTest suite matrices (to be used exactly as specified):\n1. A tall, full column rank case ($4 \\times 3$):\n$$\nA_1 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n0 & 1 & 3 \\\\\n4 & -2 & 1 \\\\\n1 & 0 & -1\n\\end{bmatrix}.\n$$\n2. A wide, rank-deficient case ($3 \\times 4$) with row dependencies:\n$$\nA_2 = \\begin{bmatrix}\n1 & 0 & 1 & 1 \\\\\n0 & 1 & 1 & 2 \\\\\n1 & 1 & 2 & 3\n\\end{bmatrix}.\n$$\n3. A square, rank $3$ case ($5 \\times 5$) constructed so that two columns are linear combinations of the first three:\nLet the columns be\n$$\nc_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix},\\quad\nc_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix},\\quad\nc_3 = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\\quad\nc_4 = c_1 + c_2,\\quad\nc_5 = c_2 - c_3,\n$$\nso that\n$$\nA_3 = \\begin{bmatrix}\n1 & 0 & 2 & 1 & -2 \\\\\n0 & 1 & -1 & 1 & 2 \\\\\n1 & 2 & 0 & 3 & 2 \\\\\n2 & -1 & 1 & 1 & -2 \\\\\n3 & 0 & 1 & 3 & -1\n\\end{bmatrix}.\n$$\n4. A near rank-deficient diagonal case ($4 \\times 4$) where one singular value is below the tolerance:\n$$\nA_4 = \\operatorname{diag}\\!\\left(10,\\; 1\\times 10^{-16},\\; 3,\\; 1\\right).\n$$\n\nAngle units are not applicable. No physical units appear, so none are required. Your program must produce exactly one output line containing the results as a comma-separated list enclosed in square brackets as described above, with no extra spaces or text.",
            "solution": "The problem requires the programmatic extraction and numerical validation of orthonormal bases for the four fundamental subspaces of a real matrix $A \\in \\mathbb{R}^{m \\times n}$: the column space $\\mathcal{R}(A)$, the null space $\\mathcal{N}(A)$, the row space $\\mathcal{R}(A^\\top)$, and the left null space $\\mathcal{N}(A^\\top)$. This task is a direct application of the Fundamental Theorem of Linear Algebra, which establishes the dimensions and orthogonality relationships between these subspaces. The Singular Value Decomposition (SVD) provides a powerful and numerically stable tool to compute these bases.\n\nThe SVD of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is a factorization $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values, sorted in descending order: $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_p \\ge 0$, where $p = \\min(m, n)$.\n\nThe core principle is that the columns of $U$ and $V$ (the singular vectors) provide orthonormal bases for the four fundamental subspaces. The distinction between vectors spanning the spaces and their orthogonal complements is determined by the numerical rank, $r$. The rank $r$ is the number of singular values that are \"significantly\" greater than zero. In numerical computation, we define this using a tolerance $\\tau$. The problem specifies a standard choice for this tolerance:\n$$\n\\tau = \\max(m, n) \\cdot \\sigma_{\\max} \\cdot \\epsilon,\n$$\nwhere $\\sigma_{\\max}$ is the largest singular value $\\sigma_1$, and $\\epsilon$ is the machine epsilon for double-precision floating-point arithmetic. The numerical rank $r$ is then the count of singular values $\\sigma_i > \\tau$.\n\nGiven the rank $r$, we partition the orthogonal matrices $U$ and $V$. Let the columns of $U$ be $u_1, \\dots, u_m$ and the columns of $V$ be $v_1, \\dots, v_n$. We form the partitions:\n- $U_r = [u_1, \\dots, u_r] \\in \\mathbb{R}^{m \\times r}$\n- $U_0 = [u_{r+1}, \\dots, u_m] \\in \\mathbb{R}^{m \\times (m-r)}$\n- $V_r = [v_1, \\dots, v_r] \\in \\mathbb{R}^{n \\times r}$\n- $V_0 = [v_{r+1}, \\dots, v_n] \\in \\mathbb{R}^{n \\times (n-r)}$\n\nThe SVD directly gives us the orthonormal bases for the four fundamental subspaces:\n1.  **Column Space $\\mathcal{R}(A)$**: An orthonormal basis is given by the columns of $U_r$. Its dimension is $r$.\n2.  **Left Null Space $\\mathcal{N}(A^\\top)$**: An orthonormal basis is given by the columns of $U_0$. Its dimension is $m-r$.\n3.  **Row Space $\\mathcal{R}(A^\\top)$**: An orthonormal basis is given by the columns of $V_r$. Its dimension is $r$.\n4.  **Null Space $\\mathcal{N}(A)$**: An orthonormal basis is given by the columns of $V_0$. Its dimension is $n-r$.\n\nThe Fundamental Theorem of Linear Algebra establishes that $\\mathcal{R}(A)$ is the orthogonal complement of $\\mathcal{N}(A^\\top)$ in $\\mathbb{R}^m$, and $\\mathcal{R}(A^\\top)$ is the orthogonal complement of $\\mathcal{N}(A)$ in $\\mathbb{R}^n$. Our algorithm will numerically verify these properties.\n\nThe algorithmic procedure for each test matrix $A$ is as follows:\n1.  Compute the full SVD $A = U \\Sigma V^\\top$. The matrices $U$ and $V$ must be square (orthogonal), not semi-orthogonal.\n2.  Determine the numerical rank $r$ by counting singular values $s_i$ that exceed the tolerance $\\tau$.\n3.  Partition $U$ and $V$ into $U_r, U_0, V_r, V_0$ based on the rank $r$.\n4.  Perform numerical validations and compute their residuals using the Frobenius norm, $\\| \\cdot \\|_F$:\n    a. **Orthonormality of bases**: For each non-empty basis matrix $Q \\in \\{U_r, U_0, V_r, V_0\\}$, compute the residual $\\|Q^\\top Q - I\\|_F$. Here $I$ is the identity matrix of the appropriate size.\n    b. **Orthogonality of subspaces**: Compute the residuals $\\|U_r^\\top U_0\\|_F$ and $\\|V_r^\\top V_0\\|_F$. These represent the dot products between basis vectors of a space and its orthogonal complement, which should be nearly zero.\n    c. **Reconstruction accuracy**: Compute the truncated SVD reconstruction $A_r = U_r \\Sigma_r V_r^\\top$, where $\\Sigma_r$ is the $r \\times r$ diagonal matrix of the first $r$ singular values. Calculate the relative reconstruction error $\\|A - A_r\\|_F / \\|A\\|_F$.\n5.  Verify the dimension theorem (rank-nullity theorem) identities as exact integer equalities: $r + \\dim(\\mathcal{N}(A)) = n$ and $r + \\dim(\\mathcal{N}(A^\\top)) = m$. By our construction, $\\dim(\\mathcal{N}(A))$ is the number of columns in $V_0$, which is $n-r$, and $\\dim(\\mathcal{N}(A^\\top))$ is the number of columns in $U_0$, which is $m-r$. Thus, the checks become $r + (n-r) = n$ and $r + (m-r) = m$, which are algebraic tautologies. Their programmatic verification confirms correct implementation of the matrix partitioning.\n6.  Aggregate the results for matrix $A_k$ into a pair $(e_k, b_k)$, where $e_k$ is the maximum of all computed floating-point residuals, and $b_k$ is the boolean result of the dimension equality checks.\n\nThis procedure will be applied to each of the four provided test matrices. The final output will be a list concatenating the $(e_k, b_k)$ pairs for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef analyze_matrix(A):\n    \"\"\"\n    Analyzes a matrix A to find its four fundamental subspaces,\n    validates their properties, and returns aggregated error metrics.\n    \"\"\"\n    m, n = A.shape\n    \n    # Step 1: Compute SVD\n    # full_matrices=True is essential to get square U and V for partitioning.\n    try:\n        U, s, Vt = np.linalg.svd(A, full_matrices=True)\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD does not converge, though unlikely for these examples.\n        return float('inf'), False\n\n    # Initialize a list to store all computed numerical residuals.\n    residuals = []\n\n    # Step 2: Determine numerical rank\n    # s is sorted in descending order.\n    # Handle the case of a zero matrix where s would be empty or all zeros.\n    s_max = s[0] if len(s) > 0 else 0.0\n    eps = np.finfo(float).eps\n    tau = max(m, n) * s_max * eps\n    r = np.sum(s > tau)\n    \n    # Step 3: Partition U and V based on rank r\n    V = Vt.T\n    Ur = U[:, :r]\n    U0 = U[:, r:]\n    Vr = V[:, :r]\n    V0 = V[:, r:]\n\n    # Step 4a: Check orthonormality of each basis\n    # Q^T Q should be close to I_k.\n    # The check is only performed if the basis is non-empty (k > 0).\n    if Ur.shape[1] > 0:\n        residuals.append(np.linalg.norm(Ur.T @ Ur - np.eye(r), 'fro'))\n    if U0.shape[1] > 0:\n        residuals.append(np.linalg.norm(U0.T @ U0 - np.eye(m - r), 'fro'))\n    if Vr.shape[1] > 0:\n        residuals.append(np.linalg.norm(Vr.T @ Vr - np.eye(r), 'fro'))\n    if V0.shape[1] > 0:\n        residuals.append(np.linalg.norm(V0.T @ V0 - np.eye(n - r), 'fro'))\n\n    # Step 4b: Check orthogonality between subspaces\n    # These products should be close to zero matrices. numpy.linalg.norm handles\n    # products involving empty matrices correctly (norm is 0).\n    residuals.append(np.linalg.norm(Ur.T @ U0, 'fro'))\n    residuals.append(np.linalg.norm(Vr.T @ V0, 'fro'))\n\n    # Step 4c: Check reconstruction accuracy\n    # Ar = Ur @ diag(s_r) @ Vr.T\n    if r > 0:\n        Ar = Ur @ np.diag(s[:r]) @ Vr.T\n    else: # If rank is 0, A is the zero matrix\n        Ar = np.zeros_like(A, dtype=float)\n        \n    norm_A = np.linalg.norm(A, 'fro')\n    if norm_A == 0:\n        reconstruction_resid = np.linalg.norm(A - Ar, 'fro')\n    else:\n        reconstruction_resid = np.linalg.norm(A - Ar, 'fro') / norm_A\n    residuals.append(reconstruction_resid)\n\n    # Step 5: Verify dimension equalities\n    dim_N_A = V0.shape[1]\n    dim_N_At = U0.shape[1]\n    dim_check1 = (r + dim_N_A == n)\n    dim_check2 = (r + dim_N_At == m)\n    b_k = dim_check1 and dim_check2\n\n    # Step 6: Aggregate results\n    # If residuals list is empty (can happen for trivial cases), max would error.\n    e_k = max(residuals) if residuals else 0.0\n\n    return e_k, b_k\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [2, -1, 0],\n        [0, 1, 3],\n        [4, -2, 1],\n        [1, 0, -1]\n    ], dtype=float)\n\n    A2 = np.array([\n        [1, 0, 1, 1],\n        [0, 1, 1, 2],\n        [1, 1, 2, 3]\n    ], dtype=float)\n\n    c1 = np.array([1, 0, 1, 2, 3])\n    c2 = np.array([0, 1, 2, -1, 0])\n    c3 = np.array([2, -1, 0, 1, 1])\n    c4 = c1 + c2\n    c5 = c2 - c3\n    A3 = np.vstack([c1, c2, c3, c4, c5]).T\n\n    A4 = np.diag([10.0, 1e-16, 3.0, 1.0])\n\n    test_cases = [A1, A2, A3, A4]\n\n    results = []\n    for case in test_cases:\n        e_k, b_k = analyze_matrix(case)\n        results.append(e_k)\n        results.append(b_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}