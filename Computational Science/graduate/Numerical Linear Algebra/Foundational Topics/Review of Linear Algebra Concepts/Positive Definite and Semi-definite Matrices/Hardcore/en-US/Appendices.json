{
    "hands_on_practices": [
        {
            "introduction": "One of the most powerful tests for positive definiteness is Sylvester's criterion, which states that a symmetric matrix is positive definite if and only if all its leading principal minors are positive. A common pitfall is to apply this criterion to non-symmetric matrices. This practice provides a crucial hands-on demonstration of why the symmetry requirement is not merely a technicality but a fundamental necessity, by constructing a non-symmetric matrix with positive leading principal minors that fails to be positive definite . Working through this example solidifies the core definition of positive definiteness ($x^{\\top}Ax > 0$) and its relationship to the matrix's symmetric part.",
            "id": "3566008",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be called positive definite if $x^{\\top} A x > 0$ for all nonzero $x \\in \\mathbb{R}^{n}$. For symmetric matrices, it is a well-tested fact (Sylvester’s criterion) that all leading principal minors being positive is equivalent to positive definiteness. This problem asks you to illustrate the necessity of symmetry in that criterion by explicit construction and analysis, starting only from the definition of positive definiteness and standard linear algebraic identities.\n\nConsider the one-parameter family of real upper-triangular matrices $A_{t} \\in \\mathbb{R}^{3 \\times 3}$ given by\n$$\nA_{t} \\;=\\; \\begin{pmatrix}\n1  t  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix},\n$$\nwhere $t \\in \\mathbb{R}$.\n\nTasks:\n- Using only basic properties of determinants and triangular matrices, analyze the leading principal minors of $A_{t}$ and determine their signs as functions of $t$.\n- Using only the definition of positive definiteness and the identity\n$$\nx^{\\top} A x \\;=\\; x^{\\top} \\left( \\tfrac{A + A^{\\top}}{2} \\right) x \\quad \\text{for all } x \\in \\mathbb{R}^{n},\n$$\ndetermine the smallest integer $t$ strictly greater than $2$ for which $A_{t}$ is not positive definite. Your reasoning should make explicit why the non-symmetric $A_{t}$ can fail to be positive definite even when all of its leading principal minors are positive.\n- With that value of $t$ and the specific vector $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$, compute the quadratic form $q = x_{0}^{\\top} A_{t} x_{0}$ exactly.\n\nAnswer specification:\n- Provide as your final answer the exact value of $q$ as a single real number with no units.\n- No rounding is required.",
            "solution": "The problem is evaluated to be valid as it is scientifically grounded in linear algebra, well-posed with a unique answer, and stated objectively. All provided definitions and statements are standard and correct. The problem constitutes a formal exercise to demonstrate a key theoretical point in numerical linear algebra.\n\nThe problem is addressed in three parts as requested.\n\n### Part 1: Analysis of Leading Principal Minors\n\nThe given one-parameter family of matrices is\n$$\nA_{t} = \\begin{pmatrix}\n1  t  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nwhere $t \\in \\mathbb{R}$. The leading principal minors are the determinants of the leading principal submatrices of $A_t$. Let's denote the $k \\times k$ leading principal minor as $\\Delta_k$.\n\nThe first leading principal minor, $\\Delta_1$, is the determinant of the $1 \\times 1$ top-left submatrix:\n$$\n\\Delta_1 = \\det \\begin{pmatrix} 1 \\end{pmatrix} = 1\n$$\n\nThe second leading principal minor, $\\Delta_2$, is the determinant of the $2 \\times 2$ top-left submatrix:\n$$\n\\Delta_2 = \\det \\begin{pmatrix} 1  t \\\\ 0  1 \\end{pmatrix} = (1)(1) - (t)(0) = 1\n$$\n\nThe third leading principal minor, $\\Delta_3$, is the determinant of the matrix $A_t$ itself. Since $A_t$ is an upper-triangular matrix, its determinant is the product of its diagonal entries:\n$$\n\\Delta_3 = \\det(A_t) = (1)(1)(1) = 1\n$$\n\nFor any value of $t \\in \\mathbb{R}$, all three leading principal minors are equal to $1$, which is a positive number. If Sylvester's criterion were applicable to non-symmetric matrices, this would imply that $A_t$ is positive definite for all $t$.\n\n### Part 2: Condition for Positive Definiteness\n\nA matrix $A \\in \\mathbb{R}^{n \\times n}$ is positive definite if, by definition, the quadratic form $x^{\\top} A x > 0$ for all nonzero vectors $x \\in \\mathbb{R}^{n}$. The problem provides the identity $x^{\\top} A x = x^{\\top} \\left( \\frac{A + A^{\\top}}{2} \\right) x$. The matrix $S = \\frac{A + A^{\\top}}{2}$ is the symmetric part of $A$. This identity shows that the quadratic form associated with any square matrix $A$ is identical to the quadratic form associated with its symmetric part $S$. Therefore, a matrix $A$ is positive definite if and only if its symmetric part $S$ is positive definite.\n\nFor the given matrix $A_t$, its transpose is\n$$\nA_{t}^{\\top} = \\begin{pmatrix}\n1  0  0 \\\\\nt  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nThe symmetric part of $A_t$, which we denote as $S_t$, is:\n$$\nS_t = \\frac{A_t + A_t^{\\top}}{2} = \\frac{1}{2} \\left[ \\begin{pmatrix}\n1  t  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} + \\begin{pmatrix}\n1  0  0 \\\\\nt  1  0 \\\\\n0  0  1\n\\end{pmatrix} \\right] = \\frac{1}{2} \\begin{pmatrix}\n2  t  0 \\\\\nt  2  0 \\\\\n0  0  2\n\\end{pmatrix} = \\begin{pmatrix}\n1  \\frac{t}{2}  0 \\\\\n\\frac{t}{2}  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nSince $S_t$ is a symmetric matrix, we can apply Sylvester's criterion to it to determine the conditions under which it (and therefore $A_t$) is positive definite. The leading principal minors of $S_t$ must all be positive.\n\nThe first leading principal minor of $S_t$ is $\\det(1) = 1 > 0$.\n\nThe second leading principal minor of $S_t$ is:\n$$\n\\det \\begin{pmatrix} 1  \\frac{t}{2} \\\\ \\frac{t}{2}  1 \\end{pmatrix} = (1)(1) - \\left(\\frac{t}{2}\\right)\\left(\\frac{t}{2}\\right) = 1 - \\frac{t^2}{4}\n$$\nFor this to be positive, we must have $1 - \\frac{t^2}{4} > 0$, which implies $t^2  4$, or $-2  t  2$.\n\nThe third leading principal minor of $S_t$ is $\\det(S_t)$:\n$$\n\\det(S_t) = \\det \\begin{pmatrix} 1  \\frac{t}{2}  0 \\\\ \\frac{t}{2}  1  0 \\\\ 0  0  1 \\end{pmatrix} = (1) \\det \\begin{pmatrix} 1  \\frac{t}{2} \\\\ \\frac{t}{2}  1 \\end{pmatrix} = 1 - \\frac{t^2}{4}\n$$\nThis yields the same condition, $-2  t  2$.\n\nThus, the matrix $A_t$ is positive definite if and only if $-2  t  2$. This explicitly demonstrates that even though the leading principal minors of the non-symmetric matrix $A_t$ are always positive, the matrix itself is not positive definite for $|t| \\ge 2$. This failure arises because the criterion applies only to symmetric matrices, and the definiteness of a non-symmetric matrix is governed by its symmetric part, whose minors can be non-positive.\n\nWe are asked to find the smallest integer $t$ strictly greater than $2$ for which $A_t$ is not positive definite. From our analysis, $A_t$ is not positive definite for any $t$ such that $t \\ge 2$ or $t \\le -2$. The integers strictly greater than $2$ are $3, 4, 5, \\dots$. The smallest of these is $t=3$.\n\n### Part 3: Calculation of the Quadratic Form\n\nWe must now compute the quadratic form $q = x_{0}^{\\top} A_{t} x_{0}$ for $t=3$ and the specific vector $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n\nThe matrix $A_3$ is:\n$$\nA_{3} = \\begin{pmatrix}\n1  3  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nThe vector and its transpose are:\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad x_{0}^{\\top} = \\begin{pmatrix} 1  -1  0 \\end{pmatrix}\n$$\nWe first compute the product $A_3 x_0$:\n$$\nA_3 x_0 = \\begin{pmatrix}\n1  3  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 3(-1) + 0(0) \\\\ 0(1) + 1(-1) + 0(0) \\\\ 0(1) + 0(-1) + 1(0) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nNow, we compute the final value of $q$:\n$$\nq = x_{0}^{\\top} (A_3 x_0) = \\begin{pmatrix} 1  -1  0 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\\\ 0 \\end{pmatrix} = (1)(-2) + (-1)(-1) + (0)(0) = -2 + 1 + 0 = -1\n$$\nThe value of the quadratic form is $q=-1$. Since $q  0$ for a nonzero vector $x_0$, this confirms that the matrix $A_3$ is not positive definite.",
            "answer": "$$\\boxed{-1}$$"
        },
        {
            "introduction": "While computing the full spectrum of a matrix provides a definitive test for positive definiteness, it is often computationally intensive or impractical. This exercise demonstrates how to certify positive definiteness and estimate key numerical properties using analytical tools that avoid direct eigenvalue computation. By applying the Gershgorin Circle Theorem to a structured symmetric matrix, you will learn to derive rigorous bounds on the eigenvalues and, from them, provide a guarantee of positive definiteness and an upper bound on the spectral condition number $\\kappa_2(A)$ .",
            "id": "3565998",
            "problem": "Consider the real symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n \\geq 3$ defined by the following entries:\n- $a_{11} = a_{nn} = 4$,\n- $a_{ii} = 6$ for all $i$ with $2 \\leq i \\leq n-1$,\n- $a_{i,i+1} = a_{i+1,i} = -1$ for all $i$ with $1 \\leq i \\leq n-1$,\n- all other entries are $0$.\n\nUsing only foundational facts from numerical linear algebra, including the definition of positive definiteness, the spectral theorem for real symmetric matrices, and the Gershgorin Circle Theorem, proceed as follows:\n1. Use Gershgorin bounds to certify that $A$ is positive definite.\n2. From these same bounds, derive an explicit scalar upper bound on the spectral condition number $\\kappa_{2}(A)$.\n\nReport as your final answer the single real number equal to the tightest upper bound on $\\kappa_{2}(A)$ that can be obtained directly from Gershgorin row bounds for $A$, in exact form (no rounding). Do not use any diagonal scaling or preconditioning. Your answer must be a single exact fraction. No units are required.",
            "solution": "The problem is to first demonstrate that a given real symmetric tridiagonal matrix $A$ is positive definite, and second, to derive an upper bound for its spectral condition number $\\kappa_2(A)$. The analysis must be based on the Gershgorin Circle Theorem.\n\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined for $n \\geq 3$ with the following non-zero entries:\n- Diagonal entries: $a_{11} = 4$, $a_{nn} = 4$, and $a_{ii} = 6$ for $2 \\leq i \\leq n-1$.\n- Off-diagonal entries: $a_{i,i+1} = a_{i+1,i} = -1$ for $1 \\leq i \\leq n-1$.\nAll other entries are $0$.\n\nThe Gershgorin Circle Theorem states that every eigenvalue of a matrix $A \\in \\mathbb{C}^{n \\times n}$ lies within at least one of the Gershgorin discs $G_i$ in the complex plane, where for each row $i=1, \\dots, n$, the disc is defined as:\n$$ G_i = \\{z \\in \\mathbb{C} : |z - a_{ii}| \\leq R_i\\}, \\quad \\text{where } R_i = \\sum_{j \\neq i} |a_{ij}| $$\nThe union of these discs, $G = \\bigcup_{i=1}^n G_i$, contains the entire spectrum of $A$, denoted $\\sigma(A)$.\n\nSince the given matrix $A$ is real and symmetric, the spectral theorem for real symmetric matrices guarantees that all its eigenvalues, $\\lambda_j$, are real numbers. Therefore, the eigenvalues must lie in the union of the real intervals that result from intersecting the Gershgorin discs with the real axis. Let's calculate the centers $a_{ii}$ and radii $R_i$ for the matrix $A$.\n\nFor the first row, $i=1$:\nThe center is $a_{11}=4$.\nThe radius is $R_1 = |a_{12}| = |-1| = 1$.\nThe corresponding real interval is $[\\, a_{11} - R_1, a_{11} + R_1 \\,] = [\\, 4-1, 4+1 \\,] = [3, 5]$.\n\nFor the interior rows, $2 \\leq i \\leq n-1$:\nThe center is $a_{ii}=6$.\nThe radius is $R_i = |a_{i,i-1}| + |a_{i,i+1}| = |-1| + |-1| = 2$.\nThe corresponding real interval is $[\\, a_{ii} - R_i, a_{ii} + R_i \\,] = [\\, 6-2, 6+2 \\,] = [4, 8]$.\n\nFor the last row, $i=n$:\nThe center is $a_{nn}=4$.\nThe radius is $R_n = |a_{n,n-1}| = |-1| = 1$.\nThe corresponding real interval is $[\\, a_{nn} - R_n, a_{nn} + R_n \\,] = [\\, 4-1, 4+1 \\,] = [3, 5]$.\n\nThe set of all eigenvalues of $A$, $\\sigma(A)$, must be contained within the union of these intervals:\n$$ \\sigma(A) \\subseteq [3, 5] \\cup [4, 8] \\cup [3, 5] = [3, 8] $$\n\n1. Certification of Positive Definiteness:\nA real symmetric matrix is positive definite if and only if all its eigenvalues are strictly positive. Let $\\lambda_{min}$ be the smallest eigenvalue of $A$. From the Gershgorin analysis, we have established that all eigenvalues $\\lambda$ of $A$ must satisfy $3 \\leq \\lambda \\leq 8$.\nThis implies that $\\lambda_{min} \\geq 3$. Since $3 > 0$, all eigenvalues of $A$ are strictly positive. Thus, the matrix $A$ is positive definite.\n\n2. Upper Bound on the Spectral Condition Number $\\kappa_2(A)$:\nThe spectral condition number of a matrix $A$ is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. For a symmetric positive definite matrix, this simplifies to the ratio of its largest and smallest eigenvalues:\n$$ \\kappa_2(A) = \\frac{\\lambda_{max}}{\\lambda_{min}} $$\nwhere $\\lambda_{max}$ is the largest eigenvalue and $\\lambda_{min}$ is the smallest eigenvalue of $A$.\n\nThe Gershgorin Circle Theorem provides bounds on the spectrum of $A$. From the inclusion $\\sigma(A) \\subseteq [3, 8]$, we can deduce:\n- An upper bound for the largest eigenvalue: $\\lambda_{max} \\leq 8$.\n- A lower bound for the smallest eigenvalue: $\\lambda_{min} \\geq 3$.\n\nUsing these bounds, we can establish an upper bound for the condition number:\n$$ \\kappa_2(A) = \\frac{\\lambda_{max}}{\\lambda_{min}} \\leq \\frac{\\text{upper bound on } \\lambda_{max}}{\\text{lower bound on } \\lambda_{min}} $$\nSubstituting the values derived from the Gershgorin row bounds:\n$$ \\kappa_2(A) \\leq \\frac{8}{3} $$\nThis is the tightest upper bound that can be derived directly from the application of the Gershgorin Circle Theorem to the rows of $A$ without any further transformations. Since $A$ is symmetric, the column-based Gershgorin bounds are identical and yield the same result.\n\nThe final answer is the value of this upper bound.",
            "answer": "$$\n\\boxed{\\frac{8}{3}}\n$$"
        },
        {
            "introduction": "In the world of numerical computation, theoretical properties do not always translate perfectly due to the limitations of floating-point arithmetic. A symmetric matrix that is theoretically positive definite might yield a small non-positive eigenvalue in a numerical computation, leading to failures in subsequent algorithms like Cholesky factorization. This advanced practice guides you through the development of a robust numerical procedure to counteract this issue by calibrating the smallest diagonal \"jitter\" $\\delta$ required to guarantee that $A + \\delta I$ is numerically positive definite . The derivation is grounded in the principles of backward error analysis and Weyl's inequality, bridging the gap between abstract theory and practical, reliable software implementation.",
            "id": "3566014",
            "problem": "Design and implement a program that, for each given real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, calibrates the smallest nonnegative diagonal jitter $\\delta$ such that $A + \\delta I \\succ 0$ is guaranteed despite uncertainty in the smallest eigenvalue due to floating-point rounding. The derivation and algorithm must start from the following fundamental base:\n\n- Core definitions:\n  - A real symmetric matrix $A$ is positive definite if and only if its smallest eigenvalue $\\lambda_{\\min}(A)$ satisfies $\\lambda_{\\min}(A) > 0$.\n  - The spectral norm is $\\lVert A \\rVert_2 = \\max_{\\lVert x \\rVert_2 = 1} \\lVert A x \\rVert_2$, which equals the largest singular value; for real symmetric $A$, $\\lVert A \\rVert_2 = \\max_i \\lvert \\lambda_i(A) \\rvert$.\n  - The spectral condition number is $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2$ when $A$ is invertible (and $\\kappa_2(A) = +\\infty$ otherwise).\n  - The Rayleigh quotient is $x^\\top A x$ for vectors $x$ with $\\lVert x \\rVert_2 = 1$, and $\\lambda_{\\min}(A) = \\min_{\\lVert x \\rVert_2 = 1} x^\\top A x$.\n\n- Well-tested perturbation facts:\n  - For symmetric $A$ and any symmetric perturbation $E$, Weyl’s inequality states $\\lvert \\lambda_i(A + E) - \\lambda_i(A) \\rvert \\le \\lVert E \\rVert_2$ for all eigenvalues $\\lambda_i(\\cdot)$.\n  - Backward stability of standard symmetric eigensolvers implies that the computed eigenvalues are exact eigenvalues of a nearby matrix $\\widehat{A} = A + \\Delta A$ with a normwise bound $\\lVert \\Delta A \\rVert_2 \\le \\gamma_n \\lVert A \\rVert_2$, where $\\gamma_n$ is proportional to $n u$ and $u$ is the unit roundoff.\n\nModel the uncertainty of the smallest eigenvalue as follows. Assume the execution environment is IEEE binary64 arithmetic with unit roundoff $u = 2^{-53}$. Adopt the following conservative decomposition of the uncertainty radius:\n- A normwise backward error bound $\\varepsilon_{\\text{back}} = c \\, n \\, u \\, \\lVert A \\rVert_2$, with $c = 10$ a modest constant.\n- An additional condition-sensitivity margin $\\varepsilon_{\\text{cond}} = c' \\, n \\, u \\, \\kappa_2(A) \\, \\lVert A \\rVert_2$ to guard the decision boundary when $A$ is used in subsequent operations whose sensitivity scales like $\\kappa_2(A)$, with $c' = 2$. If $A$ is treated as numerically singular (see invertibility test below), then set $\\varepsilon_{\\text{cond}} = 0$.\n\nDefine the total uncertainty radius as $\\varepsilon_{\\text{tot}} = \\varepsilon_{\\text{back}} + \\varepsilon_{\\text{cond}}$. Use Weyl’s inequality and the Rayleigh-quotient characterization to derive a computable and guaranteed lower bound on $\\lambda_{\\min}(A)$ from its floating-point estimate, and from it determine the smallest nonnegative $\\delta$ that guarantees $A + \\delta I \\succ 0$ for all symmetric perturbations of $A$ with spectral norm at most $\\varepsilon_{\\text{tot}}$.\n\nAlgorithmic requirements:\n- Compute $\\lVert A \\rVert_2$ and the smallest eigenvalue estimate $\\widehat{\\lambda}_{\\min}(A)$ using eigenvalues of $A$ (valid for symmetric $A$).\n- Decide invertibility for forming $\\kappa_2(A)$ using the criterion $\\min_i \\lvert \\lambda_i(A) \\rvert \\le \\tau_{\\text{sing}}$, where $\\tau_{\\text{sing}} = 100 \\, n \\, u \\, \\lVert A \\rVert_2$. If this inequality holds, treat $A$ as numerically singular and set $\\kappa_2(A) = +\\infty$ and $\\varepsilon_{\\text{cond}} = 0$. Otherwise, for symmetric invertible $A$, use $\\kappa_2(A) = \\dfrac{\\max_i \\lvert \\lambda_i(A) \\rvert}{\\min_i \\lvert \\lambda_i(A) \\rvert}$.\n- Combine the bounds to produce the smallest nonnegative $\\delta$ that guarantees strict positive definiteness in the sense above.\n\nTest suite:\nUse the following four test matrices, each provided as a list of lists of decimal entries. For each matrix $A^{(k)}$, compute the required $\\delta^{(k)}$ implied by the model with the fixed constants $u = 2^{-53}$, $c = 10$, $c' = 2$, and the invertibility threshold described above.\n\n- Case $1$ (nearly indefinite, tiny negative eigenvalue):\n  $$\n  A^{(1)} =\n  \\begin{bmatrix}\n  1.0  0.99 \\\\\n  0.99  0.98\n  \\end{bmatrix}.\n  $$\n- Case $2$ (positive semidefinite with zero eigenvalue):\n  $$\n  A^{(2)} =\n  \\begin{bmatrix}\n  1.0  1.0 \\\\\n  1.0  1.0\n  \\end{bmatrix}.\n  $$\n- Case $3$ (ill-conditioned positive definite, tiny smallest eigenvalue):\n  $$\n  A^{(3)} =\n  \\begin{bmatrix}\n  10^{-12}  0  0 \\\\\n  0  1.0  0.999 \\\\\n  0  0.999  2.0\n  \\end{bmatrix}.\n  $$\n- Case $4$ (well-conditioned positive definite):\n  $$\n  A^{(4)} = 5.0 \\, I_3.\n  $$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., \"[$r_1$,$r_2$,$r_3$,$r_4$]\".\n- Each $r_k$ must be a floating-point decimal corresponding to the calibrated $\\delta^{(k)}$ for the corresponding matrix $A^{(k)}$ in the order given above.\n\nNo user input is required; all data and constants must be hard-coded exactly as specified.",
            "solution": "The problem is deemed valid as it is scientifically grounded in numerical linear algebra, well-posed, objective, and self-contained with all necessary data and constants provided.\n\nThe objective is to find the smallest non-negative diagonal jitter, $\\delta \\ge 0$, such that the matrix $A + \\delta I$ is guaranteed to be strictly positive definite ($A + \\delta I \\succ 0$) in the face of two sources of uncertainty: the inherent backward error of the eigenvalue computation and a specified margin for perturbations. A matrix $M$ is positive definite if and only if its smallest eigenvalue, $\\lambda_{\\min}(M)$, is strictly positive.\n\nThe problem states that we must guarantee positive definiteness for all symmetric perturbations $E$ of $A$ where the spectral norm of the perturbation is bounded by a total uncertainty radius, $\\lVert E \\rVert_2 \\le \\varepsilon_{\\text{tot}}$. This means we require the matrix $(A+E) + \\delta I$ to be positive definite for all such $E$.\n\nUsing the Rayleigh quotient characterization of the minimum eigenvalue, a matrix $M$ is positive definite if and only if $x^\\top M x > 0$ for all non-zero vectors $x \\in \\mathbb{R}^n$. We apply this to $(A+E) + \\delta I$:\n$$x^\\top (A+E+\\delta I) x > 0$$\nDividing by $\\lVert x \\rVert_2^2$ and considering only unit vectors ($ \\lVert x \\rVert_2 = 1 $), this is equivalent to:\n$$x^\\top A x + x^\\top E x + \\delta > 0$$\nThis inequality must hold for all unit vectors $x$ and all symmetric perturbations $E$ with $\\lVert E \\rVert_2 \\le \\varepsilon_{\\text{tot}}$. To guarantee this, we must ensure it holds for the worst-case choice of $x$ and $E$ that minimizes the left-hand side.\n$$ \\min_{\\lVert x \\rVert_2=1, \\lVert E \\rVert_2 \\le \\varepsilon_{\\text{tot}}} (x^\\top A x + x^\\top E x) + \\delta > 0 $$\nThe term $x^\\top E x$ is a Rayleigh quotient for $E$. Its value is bounded by the eigenvalues of $E$: $\\lambda_{\\min}(E) \\le x^\\top E x \\le \\lambda_{\\max}(E)$. Since $\\lVert E \\rVert_2 = \\max(|\\lambda_{\\min}(E)|, |\\lambda_{\\max}(E)|)$, the minimum possible value for any eigenvalue of $E$ is $-\\varepsilon_{\\text{tot}}$. This minimum for $x^\\top E x$ is achieved by choosing $x$ to be the eigenvector corresponding to $\\lambda_{\\min}(E)$ and choosing an $E$ (e.g., $E = -\\varepsilon_{\\text{tot}} x x^\\top$) such that $\\lambda_{\\min}(E) = -\\varepsilon_{\\text{tot}}$. Therefore, $\\min_{\\lVert E \\rVert_2 \\le \\varepsilon_{\\text{tot}}} x^\\top E x = -\\varepsilon_{\\text{tot}}$.\n\nSubstituting this minimum value, our condition becomes:\n$$ \\min_{\\lVert x \\rVert_2=1} (x^\\top A x - \\varepsilon_{\\text{tot}}) + \\delta > 0 $$\n$$ \\left( \\min_{\\lVert x \\rVert_2=1} x^\\top A x \\right) - \\varepsilon_{\\text{tot}} + \\delta > 0 $$\nBy definition, $\\lambda_{\\min}(A) = \\min_{\\lVert x \\rVert_2=1} x^\\top A x$. So, the condition on $\\delta$ is:\n$$ \\lambda_{\\min}(A) - \\varepsilon_{\\text{tot}} + \\delta > 0 \\implies \\delta > \\varepsilon_{\\text{tot}} - \\lambda_{\\min}(A) $$\nThis inequality involves the true, exact smallest eigenvalue $\\lambda_{\\min}(A)$, which is unknown. We only have access to its floating-point estimate, which we denote $\\widehat{\\lambda}_{\\min}(A)$. The problem provides a model for the uncertainty in this estimate based on the backward stability of standard symmetric eigensolvers. The computed eigenvalues are the exact eigenvalues of a nearby matrix $A + \\Delta A$, where the perturbation $\\Delta A$ is bounded in norm by $\\lVert \\Delta A \\rVert_2 \\le \\varepsilon_{\\text{back}}$. Here, $\\varepsilon_{\\text{back}} = c \\, n \\, u \\, \\lVert A \\rVert_2$.\nSo, $\\widehat{\\lambda}_{\\min}(A) = \\lambda_{\\min}(A + \\Delta A)$. By Weyl's inequality:\n$$ \\lvert \\lambda_{\\min}(A) - \\lambda_{\\min}(A + \\Delta A) \\rvert \\le \\lVert \\Delta A \\rVert_2 \\le \\varepsilon_{\\text{back}} $$\n$$ \\lvert \\lambda_{\\min}(A) - \\widehat{\\lambda}_{\\min}(A) \\rvert \\le \\varepsilon_{\\text{back}} $$\nThis gives us a guaranteed lower bound for the true smallest eigenvalue in terms of the computed one:\n$$ \\lambda_{\\min}(A) \\ge \\widehat{\\lambda}_{\\min}(A) - \\varepsilon_{\\text{back}} $$\nTo satisfy the condition $\\delta > \\varepsilon_{\\text{tot}} - \\lambda_{\\min}(A)$ robustly, we must satisfy it for the worst-case (smallest) possible value of $\\lambda_{\\min}(A)$. Substituting its lower bound:\n$$ \\delta > \\varepsilon_{\\text{tot}} - (\\widehat{\\lambda}_{\\min}(A) - \\varepsilon_{\\text{back}}) $$\nThe total uncertainty $\\varepsilon_{\\text{tot}}$ is defined as the sum of the backward error and a condition-sensitivity margin: $\\varepsilon_{\\text{tot}} = \\varepsilon_{\\text{back}} + \\varepsilon_{\\text{cond}}$. Substituting this into the inequality:\n$$ \\delta > (\\varepsilon_{\\text{back}} + \\varepsilon_{\\text{cond}}) - \\widehat{\\lambda}_{\\min}(A) + \\varepsilon_{\\text{back}} $$\n$$ \\delta > 2\\varepsilon_{\\text{back}} + \\varepsilon_{\\text{cond}} - \\widehat{\\lambda}_{\\min}(A) $$\nWe seek the smallest non-negative $\\delta$ satisfying this strict inequality. This leads to the formula:\n$$ \\delta = \\max \\left(0, 2\\varepsilon_{\\text{back}} + \\varepsilon_{\\text{cond}} - \\widehat{\\lambda}_{\\min}(A) \\right) $$\n\nThe complete algorithm is as follows:\n1.  For a given symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, compute its eigenvalues $\\{\\widehat{\\lambda}_i\\}$ using a standard numerical eigensolver.\n2.  From the computed eigenvalues, determine the estimated smallest eigenvalue $\\widehat{\\lambda}_{\\min}(A) = \\min_i \\widehat{\\lambda}_i$ and the estimated spectral norm $\\widehat{\\lVert A \\rVert_2} = \\max_i \\lvert \\widehat{\\lambda}_i \\rvert$.\n3.  Calculate the normwise backward error bound: $\\varepsilon_{\\text{back}} = c \\, n \\, u \\, \\widehat{\\lVert A \\rVert_2}$, with $c=10$, $n$ being the matrix dimension, and $u=2^{-53}$ the unit roundoff.\n4.  Test for numerical singularity. The matrix is considered numerically singular if $\\min_i \\lvert \\widehat{\\lambda}_i \\rvert \\le \\tau_{\\text{sing}}$, where the threshold is $\\tau_{\\text{sing}} = 100 \\, n \\, u \\, \\widehat{\\lVert A \\rVert_2}$.\n5.  Calculate the condition-sensitivity margin $\\varepsilon_{\\text{cond}}$.\n    - If the matrix is numerically singular, set $\\varepsilon_{\\text{cond}} = 0$.\n    - Otherwise, compute the estimated spectral condition number $\\widehat{\\kappa}_2(A) = \\dfrac{\\max_i \\lvert \\widehat{\\lambda}_i \\rvert}{\\min_i \\lvert \\widehat{\\lambda}_i \\rvert} = \\dfrac{\\widehat{\\lVert A \\rVert_2}}{\\min_i \\lvert \\widehat{\\lambda}_i \\rvert}$. Then, $\\varepsilon_{\\text{cond}} = c' \\, n \\, u \\, \\widehat{\\kappa}_2(A) \\, \\widehat{\\lVert A \\rVert_2}$, with $c'=2$.\n6.  Finally, compute the required jitter $\\delta$ using the derived formula: $\\delta = \\max(0, 2\\varepsilon_{\\text{back}} + \\varepsilon_{\\text{cond}} - \\widehat{\\lambda}_{\\min}(A))$.\nThis procedure is applied to each test matrix.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to calculate the diagonal jitter for each test case.\n    \"\"\"\n    \n    # Define constants as specified in the problem statement.\n    u = 2**-53  # Unit roundoff for IEEE binary64\n    c = 10      # Constant for backward error\n    c_prime = 2 # Constant for condition-sensitivity margin\n    k_tau = 100   # Constant for singularity threshold\n\n    test_cases = [\n        # Case 1: nearly indefinite, tiny negative eigenvalue\n        np.array([\n            [1.0, 0.99],\n            [0.99, 0.98]\n        ]),\n        # Case 2: positive semidefinite with zero eigenvalue\n        np.array([\n            [1.0, 1.0],\n            [1.0, 1.0]\n        ]),\n        # Case 3: ill-conditioned positive definite, tiny smallest eigenvalue\n        np.array([\n            [1e-12, 0.0, 0.0],\n            [0.0, 1.0, 0.999],\n            [0.0, 0.999, 2.0]\n        ]),\n        # Case 4: well-conditioned positive definite\n        np.array([\n            [5.0, 0.0, 0.0],\n            [0.0, 5.0, 0.0],\n            [0.0, 0.0, 5.0]\n        ])\n    ]\n\n    def calculate_delta(A: np.ndarray) - float:\n        \"\"\"\n        Calculates the smallest non-negative diagonal jitter delta for a given matrix A.\n        \"\"\"\n        n = A.shape[0]\n\n        # Step 1  2: Compute eigenvalues and derived quantities.\n        # Use eigh for symmetric matrices. It returns eigenvalues in ascending order.\n        eigvals = np.linalg.eigh(A)[0]\n        \n        lambda_min_hat = eigvals[0]\n        norm_A_2_hat = np.max(np.abs(eigvals))\n        min_abs_lambda = np.min(np.abs(eigvals))\n\n        # Step 3: Calculate backward error bound.\n        eps_back = c * n * u * norm_A_2_hat\n        \n        # Step 4: Test for numerical singularity.\n        tau_sing = k_tau * n * u * norm_A_2_hat\n        is_singular = min_abs_lambda = tau_sing\n        \n        # Step 5: Calculate condition-sensitivity margin.\n        if is_singular:\n            eps_cond = 0.0\n        else:\n            # Note: For non-singular symmetric A, kappa_2(A) calculation is based on eigenvalues.\n            # Using norm_A_2_hat / min_abs_lambda directly avoids recomputing norms.\n            kappa_2_hat = norm_A_2_hat / min_abs_lambda\n            eps_cond = c_prime * n * u * kappa_2_hat * norm_A_2_hat\n\n        # Step 6: Combine bounds to produce the final delta.\n        delta_raw = 2 * eps_back + eps_cond - lambda_min_hat\n        delta = max(0.0, delta_raw)\n        \n        return delta\n\n    results = []\n    for A in test_cases:\n        delta_k = calculate_delta(A)\n        results.append(delta_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}