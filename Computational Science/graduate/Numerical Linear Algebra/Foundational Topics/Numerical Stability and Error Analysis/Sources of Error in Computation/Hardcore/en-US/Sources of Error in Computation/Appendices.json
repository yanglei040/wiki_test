{
    "hands_on_practices": [
        {
            "introduction": "A deep understanding of computational error begins with the number system itself. This exercise moves beyond abstract definitions to have you derive the concrete operational boundaries of the widely used IEEE 754 standard from first principles. By calculating the exact values for the smallest and largest representable numbers, the unit roundoff, and the thresholds for overflow and underflow, you will build a quantitative map of the floating-point world ().",
            "id": "3579628",
            "problem": "Consider arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) in base $2$ with the default rounding mode of round to nearest, ties to even. Focus on the two standard formats:\n- binary32 (commonly called single precision), which uses $p=24$ fraction bits of precision for normal numbers, an exponent field of $8$ bits with a bias of $127$, and supports subnormal numbers;\n- binary64 (commonly called double precision), which uses $p=53$ fraction bits of precision for normal numbers, an exponent field of $11$ bits with a bias of $1023$, and supports subnormal numbers.\n\nWork from first principles of the format: the representation of normal numbers with an implicit leading $1$ in the significand and an unbiased exponent range determined by the exponent field bounds and bias; the representation of subnormal numbers with no implicit leading $1$ and the smallest fixed exponent equal to the minimum normal exponent; and the fact that the default rounding is to nearest with ties to even. Adopt the widely used unit roundoff model for normalized results, namely that a correctly rounded floating-point operation satisfies $\\mathrm{fl}(x\\circ y)=(x\\circ y)(1+\\delta)$ with $|\\delta|\\leq u$ for a unit roundoff $u$ depending only on the format, provided the exact result is finite and normal.\n\nYour tasks are:\n1. Determine, for each of binary32 and binary64, the smallest positive subnormal number, the smallest positive normal number, and the largest finite representable number. Express each exactly as a product or sum of powers of $2$.\n2. Determine the unit roundoff $u$ for each of binary32 and binary64 under round to nearest, ties to even.\n3. For floating-point multiplication in each format, determine two magnitude thresholds $T_{\\mathrm{under}}$ and $T_{\\mathrm{over}}$ with the following properties:\n   - If $|x|,|y|\\geq T_{\\mathrm{under}}$ and $x,y$ are representable in the format, then the exact product $xy$ is at least the smallest positive normal number in magnitude.\n   - If $|x|,|y|\\leq T_{\\mathrm{over}}$ and $x,y$ are representable in the format, then the exact product $xy$ is at most the largest finite representable number in magnitude.\n   Use these thresholds to delineate a region $[T_{\\mathrm{under}},T_{\\mathrm{over}}]$ of input magnitudes within which the floating-point product is guaranteed to be a finite normal number and hence the standard rounding error model with $|\\delta|\\leq u$ applies to multiplication. Express $T_{\\mathrm{under}}$ and $T_{\\mathrm{over}}$ exactly in terms of powers of $2$ and radicals if needed.\n\nReport your final results as a single row matrix with entries in the following order:\n- binary32 smallest positive subnormal, binary32 smallest positive normal, binary32 largest finite, binary32 unit roundoff, binary32 $T_{\\mathrm{under}}$, binary32 $T_{\\mathrm{over}}$, binary64 smallest positive subnormal, binary64 smallest positive normal, binary64 largest finite, binary64 unit roundoff, binary64 $T_{\\mathrm{under}}$, binary64 $T_{\\mathrm{over}}$.\n\nNo units are required. Do not approximate; give exact expressions. The final answer must be a single row matrix as specified.",
            "solution": "The problem statement is a well-posed exercise in numerical analysis, concerning the properties of the IEEE 754 standard for floating-point arithmetic. It is scientifically grounded, self-contained, and objective. All terms and parameters are standard and clearly defined. The problem is valid.\n\nThe solution is structured in three parts, addressing the three tasks specified in the problem for both the binary32 and binary64 formats.\n\nA floating-point number $v$ in the IEEE 754 standard is represented as $v = (-1)^s \\times m \\times 2^E$, where $s$ is the sign bit, $m$ is the significand, and $E$ is the unbiased exponent. The exponent is determined by a stored exponent field of $k$ bits, denoted $e$, and a bias, via $E = e - \\text{bias}$.\n\nFor **binary32** (single precision):\nThe format uses a total of $32$ bits, with $k=8$ bits for the exponent and a total precision of $p=24$ bits for the significand ($1$ implicit bit for normal numbers, plus $23$ explicit fraction bits). The bias is $127$.\nThe stored exponent $e$ ranges from $0$ to $2^8 - 1 = 255$.\nNormal numbers correspond to $1 \\le e \\le 254$. The significand has an implicit leading $1$, so $m = 1.f_{1}f_{2}...f_{23}$, which means $1 \\le m  2$. The unbiased exponent is $E = e - 127$, ranging from $E_{\\min} = 1-127 = -126$ to $E_{\\max} = 254-127 = 127$.\nSubnormal numbers correspond to $e=0$. The significand has an implicit leading $0$, so $m = 0.f_{1}f_{2}...f_{23}$, which means $0  m  1$. The exponent is fixed at the minimum normal exponent, $E_{\\min} = -126$.\n\nFor **binary64** (double precision):\nThe format uses a total of $64$ bits, with $k=11$ bits for the exponent and a total precision of $p=53$ bits for the significand ($1$ implicit bit plus $52$ explicit fraction bits). The bias is $1023$.\nThe stored exponent $e$ ranges from $0$ to $2^{11} - 1 = 2047$.\nNormal numbers correspond to $1 \\le e \\le 2046$. The significand is $1 \\le m  2$. The unbiased exponent is $E = e - 1023$, ranging from $E_{\\min} = 1-1023 = -1022$ to $E_{\\max} = 2046-1023 = 1023$.\nSubnormal numbers correspond to $e=0$. The significand is $0  m  1$. The exponent is fixed at $E_{\\min} = -1022$.\n\n**1. Characteristic Numbers**\n\n**binary32:**\n- **Smallest positive subnormal number ($SPS_{32}$):** This occurs with the minimum possible non-zero significand for a subnormal number and the fixed subnormal exponent. The significand is $m=0.0...01_2 = 2^{-23}$. The exponent is $E_{\\min} = -126$.\n$$SPS_{32} = 2^{-23} \\times 2^{-126} = 2^{-149}$$\n- **Smallest positive normal number ($SPN_{32}$):** This occurs with the minimum normal exponent and the minimum normal significand. The exponent is $E_{\\min} = -126$. The significand is $m=1.0...0_2 = 1$.\n$$SPN_{32} = 1 \\times 2^{-126} = 2^{-126}$$\n- **Largest finite representable number ($LFN_{32}$):** This occurs with the maximum normal exponent and the maximum possible significand. The exponent is $E_{\\max}=127$. The significand is $m=1.1...1_2$ (with $23$ ones after the binary point), which is $m = \\sum_{i=0}^{23} 2^{-i} = 2 - 2^{-23}$.\n$$LFN_{32} = (2 - 2^{-23}) \\times 2^{127} = 2^{128} - 2^{104}$$\n\n**binary64:**\n- **Smallest positive subnormal number ($SPS_{64}$):** The significand is $m=0.0...01_2 = 2^{-52}$ (with $52$ fraction bits). The exponent is $E_{\\min} = -1022$.\n$$SPS_{64} = 2^{-52} \\times 2^{-1022} = 2^{-1074}$$\n- **Smallest positive normal number ($SPN_{64}$):** The exponent is $E_{\\min} = -1022$. The significand is $m=1$.\n$$SPN_{64} = 1 \\times 2^{-1022} = 2^{-1022}$$\n- **Largest finite representable number ($LFN_{64}$):** The exponent is $E_{\\max}=1023$. The significand is $m=1.1...1_2$ (with $52$ ones after the point), which is $m = 2 - 2^{-52}$.\n$$LFN_{64} = (2 - 2^{-52}) \\times 2^{1023} = 2^{1024} - 2^{971}$$\n\n**2. Unit Roundoff**\n\nThe unit roundoff, $u$, for a rounding mode of round to nearest, is half the machine epsilon, $\\epsilon$. The machine epsilon is the distance between $1$ and the next largest representable floating-point number. For a system with precision $p$, the number $1$ is represented with significand $1$ and exponent $0$. The next larger number has significand $1+2^{-(p-1)}$. Thus, $\\epsilon = 2^{-(p-1)}$.\nThe unit roundoff is $u = \\frac{1}{2}\\epsilon = \\frac{1}{2} \\times 2^{-(p-1)} = 2^{-p}$.\n\n- **binary32:** With precision $p=24$, the unit roundoff is $u_{32} = 2^{-24}$.\n- **binary64:** With precision $p=53$, the unit roundoff is $u_{64} = 2^{-53}$.\n\n**3. Multiplication Thresholds**\n\nWe need to determine thresholds $T_{\\mathrm{under}}$ and $T_{\\mathrm{over}}$ for floating-point multiplication.\n\n- **Underflow Threshold $T_{\\mathrm{under}}$:** The problem defines $T_{\\mathrm{under}}$ such that if $|x| \\geq T_{\\mathrm{under}}$ and $|y| \\geq T_{\\mathrm{under}}$, then the exact product $|xy| \\geq SPN$. Since we have $|x||y| \\geq T_{\\mathrm{under}}^2$, we must enforce $T_{\\mathrm{under}}^2 \\geq SPN$. The tightest such threshold is achieved when equality holds:\n$$T_{\\mathrm{under}}^2 = SPN \\implies T_{\\mathrm{under}} = \\sqrt{SPN}$$\n- **For binary32:** $T_{\\mathrm{under},32} = \\sqrt{SPN_{32}} = \\sqrt{2^{-126}} = 2^{-63}$.\n- **For binary64:** $T_{\\mathrm{under},64} = \\sqrt{SPN_{64}} = \\sqrt{2^{-1022}} = 2^{-511}$.\n\n- **Overflow Threshold $T_{\\mathrm{over}}$:** The problem defines $T_{\\mathrm{over}}$ such that if $|x| \\leq T_{\\mathrm{over}}$ and $|y| \\leq T_{\\mathrm{over}}$, then the exact product $|xy| \\leq LFN$. Since we have $|x||y| \\leq T_{\\mathrm{over}}^2$, we must enforce $T_{\\mathrm{over}}^2 \\leq LFN$. The largest such threshold is achieved when equality holds:\n$$T_{\\mathrm{over}}^2 = LFN \\implies T_{\\mathrm{over}} = \\sqrt{LFN}$$\n- **For binary32:** $T_{\\mathrm{over},32} = \\sqrt{LFN_{32}} = \\sqrt{(2-2^{-23}) \\times 2^{127}} = \\sqrt{2^{128} \\times (1-2^{-24})} = 2^{64}\\sqrt{1-2^{-24}}$.\n- **For binary64:** $T_{\\mathrm{over},64} = \\sqrt{LFN_{64}} = \\sqrt{(2-2^{-52}) \\times 2^{1023}} = \\sqrt{2^{1024} \\times (1-2^{-53})} = 2^{512}\\sqrt{1-2^{-53}}$.\n\nThese thresholds define a range $[T_{\\mathrm{under}}, T_{\\mathrm{over}}]$ for input magnitudes. If $|x|$ and $|y|$ are both within this range, their product $|xy|$ will fall in the range $[SPN, LFN]$, ensuring the exact result is a finite normal number and the standard rounding error model $\\mathrm{fl}(xy)=xy(1+\\delta)$ with $|\\delta| \\leq u$ is applicable.\n\n**Summary of Results:**\nThe twelve requested values are assembled in a single row matrix in the specified order.\n1. binary32 $SPS$: $2^{-149}$\n2. binary32 $SPN$: $2^{-126}$\n3. binary32 $LFN$: $2^{128}-2^{104}$\n4. binary32 $u$: $2^{-24}$\n5. binary32 $T_{\\mathrm{under}}$: $2^{-63}$\n6. binary32 $T_{\\mathrm{over}}$: $2^{64}\\sqrt{1-2^{-24}}$\n7. binary64 $SPS$: $2^{-1074}$\n8. binary64 $SPN$: $2^{-1022}$\n9. binary64 $LFN$: $2^{1024}-2^{971}$\n10. binary64 $u$: $2^{-53}$\n11. binary64 $T_{\\mathrm{under}}$: $2^{-511}$\n12. binary64 $T_{\\mathrm{over}}$: $2^{512}\\sqrt{1-2^{-53}}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2^{-149}  2^{-126}  2^{128}-2^{104}  2^{-24}  2^{-63}  2^{64}\\sqrt{1-2^{-24}}  2^{-1074}  2^{-1022}  2^{1024}-2^{971}  2^{-53}  2^{-511}  2^{512}\\sqrt{1-2^{-53}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Individual rounding errors may seem negligible, but their cumulative effect in an algorithm can be significant. This practice explores this phenomenon by analyzing one of the most fundamental numerical algorithms: sequential summation. You will derive sharp, worst-case error bounds for this process under two different rounding modes, revealing how the choice of rounding rule can impact the accuracy of a computation ().",
            "id": "3579598",
            "problem": "Consider a normalized radix-$\\beta$ floating-point system with precision $p \\geq 2$, no subnormal numbers, and no overflow or underflow in the computations that follow. Let $n \\geq 2$ and let $a_1,\\dots,a_n$ be strictly positive real numbers with exact total sum $S = \\sum_{i=1}^{n} a_i$. Consider the left-to-right summation algorithm that computes a floating-point approximation $\\widehat{S}$ of $S$ by the recurrence\n$$\n\\widehat{s}_1 = a_1,\\quad \\widehat{s}_k = \\operatorname{fl}(\\widehat{s}_{k-1} + a_k),\\quad k=2,\\dots,n,\n$$\nand outputs $\\widehat{S} = \\widehat{s}_n$. Assume that each floating-point operation satisfies the standard rounding model\n$$\n\\operatorname{fl}(x) = x(1+\\delta),\n$$\nwith the constraint on $\\delta$ depending on the rounding mode:\n- Round-to-nearest (ties to even): $|\\delta| \\leq u$, where $u = \\tfrac{1}{2}\\beta^{1-p}$.\n- Round-toward-zero: for $x0$, $\\delta \\in [-u_{0},0]$, where $u_{0} = \\beta^{1-p}$.\n\nUsing only the above model and the fact that $a_k0$ for all $k$, derive, for each rounding mode, a sharp supremum upper bound on the absolute relative forward error\n$$\n\\left|\\frac{\\widehat{S}-S}{S}\\right|\n$$\ntaken over all strictly positive input sequences $(a_1,\\dots,a_n)$ with fixed total $S$ and fixed order. Then, compute the ratio $R(n,\\beta,p)$ of the round-toward-zero supremal bound to the round-to-nearest supremal bound, and express your final answer as a single closed-form analytic expression in terms of $n$, $\\beta$, and $p$. No numerical approximation is required for the final expression.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n- A normalized radix-$\\beta$ floating-point system with precision $p \\geq 2$.\n- No subnormal numbers, overflow, or underflow.\n- A sequence of $n \\geq 2$ strictly positive real numbers $a_1, \\dots, a_n$.\n- The exact sum $S = \\sum_{i=1}^{n} a_i$.\n- A left-to-right summation algorithm defined by the recurrence:\n  $$ \\widehat{s}_1 = a_1 $$\n  $$ \\widehat{s}_k = \\operatorname{fl}(\\widehat{s}_{k-1} + a_k), \\quad k=2,\\dots,n $$\n- The computed sum is $\\widehat{S} = \\widehat{s}_n$.\n- The standard rounding model is $\\operatorname{fl}(x) = x(1+\\delta)$.\n- The bounds on $\\delta$ are:\n  1.  **Round-to-nearest (ties to even):** $|\\delta| \\leq u$, where $u = \\frac{1}{2}\\beta^{1-p}$.\n  2.  **Round-toward-zero:** For $x0$, $\\delta \\in [-u_0, 0]$, where $u_0 = \\beta^{1-p}$.\n- The task is to derive a sharp supremum upper bound on the absolute relative forward error $|\\frac{\\widehat{S}-S}{S}|$ for each rounding mode, taken over all strictly positive input sequences $(a_1, \\dots, a_n)$ with a fixed total sum $S$ and fixed order.\n- The final goal is to compute the ratio $R(n, \\beta, p)$ of the round-toward-zero supremal bound to the round-to-nearest supremal bound.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in the error analysis of floating-point algorithms, a core topic in numerical linear algebra and numerical analysis.\n\n- **Scientifically Grounded:** The problem is based on the standard axioms of floating-point arithmetic. The summation algorithm and the error models are fundamental concepts in numerical analysis. The setup is scientifically and mathematically sound.\n- **Well-Posed:** The problem asks for a sharp supremum (least upper bound) of a well-defined error metric over a specified set of inputs. This is a well-defined mathematical optimization problem that admits a unique solution.\n- **Objective:** The problem is stated in precise, unambiguous mathematical language.\n- **Completeness and Consistency:** All necessary definitions, parameters ($n, \\beta, p$), and constraints ($a_k0$, no overflow/underflow) are provided. The model is self-contained and internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n## SOLUTION\n\n### General Error Analysis\nLet $S_k = \\sum_{i=1}^k a_i$ be the exact partial sum, and let $e_k = \\widehat{s}_k - S_k$ be the absolute error in the $k$-th computed partial sum.\nAccording to the problem statement, $\\widehat{s}_1 = a_1$. Since $S_1 = a_1$, the initial error is $e_1 = 0$.\n\nFor $k \\ge 2$, the recurrence for the computed sum is $\\widehat{s}_k = \\operatorname{fl}(\\widehat{s}_{k-1} + a_k)$. Using the standard rounding model, we have:\n$$ \\widehat{s}_k = (\\widehat{s}_{k-1} + a_k)(1+\\delta_k) $$\nwhere $\\delta_k$ is the relative error of the $k$-th floating-point addition.\nWe can express the error $e_k$ in terms of the previous error $e_{k-1}$:\n$$ S_k + e_k = ((S_{k-1} + e_{k-1}) + a_k)(1+\\delta_k) = (S_k + e_{k-1})(1+\\delta_k) = S_k(1+\\delta_k) + e_{k-1}(1+\\delta_k) $$\n$$ e_k = S_k\\delta_k + e_{k-1}(1+\\delta_k) $$\nThis is a recurrence for the error, with $e_1 = 0$. We can unroll it to find the total error $E = e_n$:\n$e_2 = S_2\\delta_2 + e_1(1+\\delta_2) = S_2\\delta_2$.\n$e_3 = S_3\\delta_3 + e_2(1+\\delta_3) = S_3\\delta_3 + S_2\\delta_2(1+\\delta_3)$.\nIn general, the total absolute error $E = e_n$ is:\n$$ E = \\sum_{k=2}^n S_k\\delta_k \\prod_{j=k+1}^n (1+\\delta_j) $$\nwhere the empty product for $k=n$ is defined as $1$. Since all $a_k  0$, all partial sums $S_k$ are strictly positive. We seek the supremum of a bound on $|\\frac{E}{S}|$.\n\n### Supremum Bound for Round-to-Nearest\nIn this mode, $|\\delta_k| \\le u = \\frac{1}{2}\\beta^{1-p}$ for each $k \\in \\{2, \\ldots, n\\}$.\nTo find the supremum of $|E|$, we should choose the errors $\\delta_k$ to maximize the magnitude of each term in the sum, which occurs when all $\\delta_k$ have the same sign and maximal magnitude. We must compare the case where all $\\delta_k=u$ with the case where all $\\delta_k=-u$.\n\n1.  Let $\\delta_k = u$ for all $k$. The error $E_+$ is:\n    $$ E_+ = \\sum_{k=2}^n S_k u \\prod_{j=k+1}^n (1+u) = u \\sum_{k=2}^n S_k (1+u)^{n-k} $$\n    Since $a_k  0$ and $u0$, we have $E_+  0$. We express $E_+$ as a linear combination of the $a_i$'s. Since $S_k = \\sum_{i=1}^k a_i$:\n    $$ E_+ = u \\sum_{k=2}^n \\left(\\sum_{i=1}^k a_i\\right) (1+u)^{n-k} = u \\sum_{i=1}^n a_i \\sum_{k=\\max(2,i)}^n (1+u)^{n-k} $$\n    The coefficient of $a_i$ is $C_i = u \\sum_{k=\\max(2,i)}^n (1+u)^{n-k}$. This is a geometric series sum.\n    For $i=1$: $C_1 = u \\sum_{k=2}^n (1+u)^{n-k} = u \\left( (1+u)^{n-2} + \\dots + 1 \\right) = u \\frac{(1+u)^{n-1}-1}{(1+u)-1} = (1+u)^{n-1}-1$.\n    For $i \\ge 2$: $C_i = u \\sum_{k=i}^n (1+u)^{n-k} = u \\left( (1+u)^{n-i} + \\dots + 1 \\right) = u \\frac{(1+u)^{n-i+1}-1}{(1+u)-1} = (1+u)^{n-i+1}-1$.\n    Observe that $C_1 = (1+u)^{n-1}-1$ and $C_2 = (1+u)^{n-2+1}-1 = (1+u)^{n-1}-1$, so $C_1=C_2$. For $i2$, $C_i$ decreases as $i$ increases. The ordering is $C_1 = C_2  C_3  \\dots  C_n  0$.\n\n2.  Let $\\delta_k = -u$ for all $k$. The error $E_-$ is negative, and its magnitude is:\n    $$ |E_-| = u \\sum_{k=2}^n S_k (1-u)^{n-k} $$\n    By a similar derivation, $|E_-| = \\sum_{i=1}^n a_i C'_i$, where $C'_1=C'_2 = 1-(1-u)^{n-1}$ and $C'_i=1-(1-u)^{n-i+1}$ for $i \\ge 2$.\n    For $u0$ and $n \\ge 2$, it can be shown that $(1+u)^{n-1}-1  1-(1-u)^{n-1}$. Thus, $|E_+|  |E_-|$ for any choice of positive $a_i$.\n\nTo find the supremum bound, we must maximize $E_+ = \\sum_{i=1}^n C_i a_i$ subject to $\\sum a_i = S$ and $a_k0$. Since the coefficients are largest for $i=1$ and $i=2$, the supremum is achieved in the limit where all the mass of the sum is concentrated on $a_1$ or $a_2$. For example, let $a_1 = S - (n-1)\\epsilon$ and $a_k = \\epsilon$ for $k \\in \\{2,\\dots,n\\}$. As $\\epsilon \\to 0^+$, we have $a_1 \\to S$ and $a_k \\to 0$ for $k1$. The error $E_+$ approaches $S \\cdot C_1$.\nThe supremum of the relative error is therefore:\n$$ B_{RN} = \\sup \\left|\\frac{\\widehat{S}-S}{S}\\right| = C_1 = (1+u)^{n-1}-1 $$\n\n### Supremum Bound for Round-toward-Zero\nIn this mode, for positive operands, $\\delta_k \\in [-u_0, 0]$, where $u_0 = \\beta^{1-p}$.\nSince $a_k  0$, all intermediate sums $\\widehat{s}_{k-1}$ and $\\widehat{s}_{k-1}+a_k$ are positive.\nThe error is $E = \\sum_{k=2}^n S_k\\delta_k \\prod_{j=k+1}^n (1+\\delta_j)$.\nAs $S_k0$, $\\delta_k \\le 0$, and $(1+\\delta_j) \\in [1-u_0, 1]$, every term in the sum is non-positive, so $E \\le 0$. We want to find the supremum of $|E| = -E$.\n$$ -E = \\sum_{k=2}^n S_k(-\\delta_k) \\prod_{j=k+1}^n (1+\\delta_j) $$\nTo maximize $-E$, we need to choose the sequence of $\\delta_k \\in [-u_0, 0]$ optimally. Let's analyze the contribution of each $\\delta_k$. From the error recurrence $e_k = e_{k-1}(1+\\delta_k) + S_k\\delta_k$, we want to maximize $|e_n|=-e_n$.\n$$ -e_k = -e_{k-1}(1+\\delta_k)-S_k\\delta_k = -e_{k-1} - (e_{k-1}+S_k)\\delta_k = -e_{k-1} - \\widehat{s}_{k-1}\\delta_k $$\nSince $-e_{k-1} = |e_{k-1}|$, we want to maximize this quantity at each step. The term involving $\\delta_k$ is $-\\widehat{s}_{k-1}\\delta_k$. As $\\widehat{s}_{k-1}  0$, its coefficient $-\\widehat{s}_{k-1}$ is negative. To maximize a product $C \\cdot x$ with $C0$ and $x$ in an interval, we must choose $x$ to be the minimum value in the interval. Thus, for each $k$, we must choose $\\delta_k = -u_0$. This choice maximizes $|e_k|$ at every step.\nTherefore, the maximum magnitude of the error is achieved when $\\delta_k = -u_0$ for all $k \\in \\{2, \\ldots, n\\}$.\nIn this case, the magnitude of the error is:\n$$ |E| = \\sum_{k=2}^n S_k u_0 \\prod_{j=k+1}^n (1-u_0) = u_0 \\sum_{k=2}^n S_k (1-u_0)^{n-k} $$\nThis expression is analogous to $E_+$ from the round-to-nearest case, with $u$ replaced by $u_0$ and $(1+u)$ by $(1-u_0)$, and an overall sign flip. The analysis of its maximization over $a_k$ is identical. We express $|E|$ as $|E| = \\sum_{i=1}^n D_i a_i$. The coefficients are:\n$D_i = 1-(1-u_0)^{n-i+1}$ for $i \\ge 2$, and $D_1=D_2 = 1-(1-u_0)^{n-1}$.\nThe ordering is $D_1=D_2  D_3  \\dots  D_n  0$. The supremum is achieved by letting $a_1 \\to S$ (or $a_2 \\to S$), which gives $|E|/S \\to D_1$.\nThe supremum of the relative error is:\n$$ B_{RTZ} = \\sup \\left|\\frac{\\widehat{S}-S}{S}\\right| = D_1 = 1-(1-u_0)^{n-1} $$\n\n### Ratio of Supremal Bounds\nThe problem asks for a ratio $R(n, \\beta, p) = \\frac{B_{RTZ}}{B_{RN}}$.\n$$ R(n, \\beta, p) = \\frac{1-(1-u_0)^{n-1}}{(1+u)^{n-1}-1} $$\nNow, we substitute the definitions $u = \\frac{1}{2}\\beta^{1-p}$ and $u_0 = \\beta^{1-p}$.\n$$ R(n, \\beta, p) = \\frac{1-(1-\\beta^{1-p})^{n-1}}{(1+\\frac{1}{2}\\beta^{1-p})^{n-1}-1} $$\nThis is the final closed-form analytic expression for the ratio.",
            "answer": "$$ \\boxed{\\frac{1-(1-\\beta^{1-p})^{n-1}}{(1+\\frac{1}{2}\\beta^{1-p})^{n-1}-1}} $$"
        },
        {
            "introduction": "While rounding error causes a gradual loss of precision, exceeding the representable range of floating-point numbers leads to catastrophic failure. This hands-on coding exercise provides a striking demonstration of this principle by showing that the fundamental mathematical identity $\\sqrt{x \\cdot x} = \\lvert x \\rvert$ does not hold universally in finite-precision arithmetic. By testing values near the overflow and underflow thresholds, you will empirically verify the predictable failure modes caused by these computational boundaries ().",
            "id": "3276013",
            "problem": "Design and implement a program that empirically demonstrates that, in finite-precision arithmetic compliant with the Institute of Electrical and Electronics Engineers (IEEE) $754$ binary$64$ (double precision), the identity $\\sqrt{x \\cdot x} = \\lvert x \\rvert$ does not hold for all representable $x$. Your program must use only double-precision floating-point operations supplied by the runtime and must not emulate higher precision.\n\nUse the following fundamental base to guide your reasoning and implementation: floating-point arithmetic represents real numbers approximately, using a fixed precision and a bounded exponent range; basic operations are rounded to the nearest representable value (ties to even), with overflow mapping to $\\pm \\infty$ and underflow potentially producing subnormal numbers or zero. These are standard, well-tested properties of IEEE $754$ binary$64$ arithmetic.\n\nTask specification:\n- For each test value $x$, compute $y = \\sqrt{x \\cdot x}$ and $z = \\lvert x \\rvert$ using the platform’s double-precision arithmetic. For each case, record a Boolean indicating whether $y$ and $z$ are bitwise equal at the floating-point level using the language’s exact equality operator for doubles.\n- Construct the test suite so that it covers typical magnitudes, overflow, and underflow. Define the following machine parameters for binary$64$:\n  - $M$: the largest finite positive number representable (maximum finite).\n  - $m$: the smallest positive normal number representable (minimum positive normal).\n  - $\\varepsilon$: the distance between $1$ and the next larger representable number (unit roundoff near $1$).\n  - $s = m \\cdot \\varepsilon$: the smallest positive subnormal number representable.\n  - $T_{\\text{overflow}} = \\sqrt{M}$.\n  - $T_{\\text{zero}} = \\sqrt{s}$.\n  - $\\operatorname{nextafter}(a, b)$: the representable number adjacent to $a$ in the direction of $b$.\n- Using only double-precision arithmetic and the above parameters, build the following test suite of values $x$:\n  1. $x_1 = 1.234567890123456$.\n  2. $x_2 = -98765.4321$.\n  3. $x_3 = 0$.\n  4. $x_4 = 2^{600}$.\n  5. $x_5 = -2^{600}$.\n  6. $x_6 = 2^{-600}$.\n  7. $x_7 = -2^{-600}$.\n  8. $x_8 = \\operatorname{nextafter}(T_{\\text{overflow}}, 0)$.\n  9. $x_9 = \\operatorname{nextafter}(T_{\\text{overflow}}, +\\infty)$.\n  10. $x_{10} = \\operatorname{nextafter}(T_{\\text{zero}}, 0)$.\n  11. $x_{11} = \\operatorname{nextafter}(T_{\\text{zero}}, +\\infty)$.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $x_1$ through $x_{11}$. Each list element must be a Boolean indicating whether $\\sqrt{x \\cdot x}$ equals $\\lvert x \\rvert$ under exact floating-point equality. For example, an output of the form $[b_1,b_2,\\dots,b_{11}]$ where each $b_i$ is either true or false.\n\nNotes and requirements:\n- All computations must be carried out in double precision (IEEE $754$ binary$64$).\n- No physical units are involved.\n- Angles are not involved.\n- The final output must be exactly one line in the specified list format.",
            "solution": "The problem statement is valid. It presents a well-posed and scientifically grounded investigation into the limitations of finite-precision arithmetic, a fundamental topic in numerical analysis and scientific computing. The problem is self-contained, its terms are rigorously defined, and the required computations are feasible within the specified IEEE $754$ binary$64$ environment.\n\nThe mathematical identity $\\sqrt{x \\cdot x} = \\lvert x \\rvert$ is an axiom for real numbers, $\\forall x \\in \\mathbb{R}$. However, computer arithmetic does not operate on the field of real numbers, $\\mathbb{R}$, but on a finite subset of rational numbers, representable as floating-point values. The IEEE $754$ binary$64$ standard for double-precision floating-point numbers imposes strict limits on both the range and precision of these values. These limitations are the source of discrepancies between ideal mathematical identities and their computational evaluation. The identity fails precisely when the intermediate computation of $x \\cdot x$ results in overflow or underflow.\n\nA number in the IEEE $754$ binary$64$ format is represented using a sign bit, an $11$-bit exponent, and a $52$-bit significand (fraction). This structure defines the key properties of the system:\n- The maximum finite positive number, $M \\approx 1.797 \\times 10^{308}$. Any result larger than $M$ overflows to positive infinity ($+\\infty$).\n- The smallest positive normal number, $m \\approx 2.225 \\times 10^{-308}$, corresponding to the minimum normal exponent of $-1022$.\n- The smallest positive subnormal number, $s \\approx 4.94 \\times 10^{-324}$. Any result with a magnitude smaller than $s$ that cannot be represented as a subnormal number will underflow and be flushed to $0$. The problem correctly defines $s$ as $s = m \\cdot \\varepsilon$, where $\\varepsilon$ is the machine epsilon, because $m=2^{-1022}$ and $\\varepsilon=2^{-52}$, yielding $s=2^{-1074}$.\n\nThe test suite is designed to probe these limits. We will analyze each case to predict whether the computational evaluation of `sqrt(x*x) == abs(x)` will yield true or false.\n\n1.  **Cases $x_1, x_2, x_3$ (Well-behaved values):**\n    - For $x_1 = 1.234567890123456$ and $x_2 = -98765.4321$, the values of $x$ and $x \\cdot x$ are well within the range of normal floating-point numbers. While the product $x \\cdot x$ may require more than $53$ bits of precision and thus incur a rounding error, modern floating-point units, particularly the `sqrt` function, are specified to be highly accurate (correctly rounded). For most well-behaved inputs, the composition of these operations is expected to be numerically stable and recover the original magnitude. The identity will hold.\n    - For $x_3 = 0$, the identity is trivially true: $\\sqrt{0.0 \\cdot 0.0} = \\sqrt{0.0} = 0.0$, and $\\lvert 0.0 \\rvert = 0.0$.\n\n2.  **Cases $x_4, x_5, x_9$ (Overflow):**\n    - The threshold for overflow in the multiplication is $x$ such that $\\lvert x \\rvert  \\sqrt{M}$. The value $T_{\\text{overflow}} = \\sqrt{M} \\approx 1.34 \\times 10^{154}$.\n    - For $x_4 = 2^{600}$ and $x_5 = -2^{600}$, the magnitude is $\\lvert x \\rvert = 2^{600}$. The intermediate product is $(2^{600})^2 = 2^{1200}$. The maximum exponent for a binary$64$ number is $1023$. Since $1200  1023$, the product $x \\cdot x$ will overflow to $+\\infty$. The subsequent operation $\\sqrt{+\\infty}$ returns $+\\infty$. Since $x$ is a finite number, $+\\infty \\ne \\lvert x \\rvert$. The identity will fail.\n    - For $x_9 = \\operatorname{nextafter}(T_{\\text{overflow}}, +\\infty)$, the value of $x_9$ is by definition the smallest representable number strictly greater than $T_{\\text{overflow}}$. Therefore, $x_9^2  M$, causing $x_9 \\cdot x_9$ to overflow to $+\\infty$. As before, the result $\\sqrt{+\\infty} = +\\infty$ is not equal to the finite value $\\lvert x_9 \\rvert$. The identity will fail.\n\n3.  **Cases $x_6, x_7, x_{10}$ (Underflow):**\n    - The threshold for underflow in the multiplication is related to the smallest positive subnormal number, $s = 2^{-1074}$. If $x \\cdot x$ produces a result less than the midpoint between $0$ and $s$ (i.e., $ s/2$), it will be flushed to $0$. This occurs when $\\lvert x \\rvert  \\sqrt{s}$. We define $T_{\\text{zero}} = \\sqrt{s} = \\sqrt{2^{-1074}} = 2^{-537}$.\n    - For $x_6 = 2^{-600}$ and $x_7 = -2^{-600}$, the magnitude is $\\lvert x \\rvert = 2^{-600}$. The intermediate product is $(2^{-600})^2 = 2^{-1200}$. The smallest representable (subnormal) positive number has an effective exponent of $-1074$. Since $-1200  -1074$, the result of $x \\cdot x$ is too small to be represented and underflows to $0.0$. Then $\\sqrt{0.0} = 0.0$, which is not equal to the original non-zero magnitude $2^{-600}$. The identity will fail.\n    - For $x_{10} = \\operatorname{nextafter}(T_{\\text{zero}}, 0)$, the value of $x_{10}$ is strictly less than $T_{\\text{zero}}$. Therefore, $x_{10}^2  T_{\\text{zero}}^2 = s$. This product $x_{10} \\cdot x_{10}$ falls into the range where it underflows to $0.0$. The result $\\sqrt{0.0} = 0.0$ is not equal to the non-zero value $\\lvert x_{10} \\rvert$. The identity will fail.\n\n4.  **Cases $x_8, x_{11}$ (Boundary-adjacent values):**\n    - For $x_8 = \\operatorname{nextafter}(T_{\\text{overflow}}, 0)$, the value is the largest representable number strictly less than $T_{\\text{overflow}}$. Thus, $x_8^2  M$, so the product $x_8 \\cdot x_8$ will not overflow and will result in a finite number close to $M$. The subsequent correctly rounded square root operation is expected to precisely reverse the squaring, so $\\sqrt{x_8 \\cdot x_8}$ will equal $\\lvert x_8 \\rvert$. The identity will hold.\n    - For $x_{11} = \\operatorname{nextafter}(T_{\\text{zero}}, +\\infty)$, the value is strictly greater than $T_{\\text{zero}} = \\sqrt{s}$. The product $x_{11}^2  s$. This result is large enough to be represented as a non-zero (subnormal) number. The square root of this subnormal number is well-defined and is expected to be computed with sufficient accuracy to recover the original value $\\lvert x_{11} \\rvert$. The identity will hold.\n\nThe empirical demonstration will thus show that the identity fails for cases $x_4, x_5, x_6, x_7, x_9, x_{10}$, and holds for the remaining cases. This confirms that the failures are predictable consequences of overflow and underflow in finite-precision arithmetic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the failure of the identity sqrt(x*x) = abs(x)\n    in IEEE 754 double-precision arithmetic due to overflow and underflow.\n    \"\"\"\n    \n    # Define machine parameters for binary64 (double precision) using numpy.finfo.\n    finfo = np.finfo(np.float64)\n    M = finfo.max          # Largest finite positive number\n    m = finfo.tiny         # Smallest positive normal number\n    eps = finfo.eps        # Machine epsilon\n    \n    # Per the problem statement, s is the smallest positive subnormal number.\n    # np.finfo.smallest_subnormal provides this directly.\n    # The problem's formula s = m * eps is also correct for binary64.\n    # Let's verify: m = 2**-1022, eps = 2**-52, so m*eps = 2**-1074.\n    # np.finfo.smallest_subnormal is also 2**-1074.\n    s = finfo.smallest_subnormal\n\n    # Define thresholds based on the machine parameters.\n    # T_overflow is the threshold for x above which x*x overflows.\n    T_overflow = np.sqrt(M)\n    \n    # T_zero is the threshold for x below which x*x may underflow to zero.\n    T_zero = np.sqrt(s)\n\n    # Build the test suite of values for x.\n    test_suite = [\n        1.234567890123456,                          # x1: Normal number\n        -98765.4321,                                # x2: Normal negative number\n        0.0,                                        # x3: Zero\n        2.0**600,                                   # x4: Large number causing overflow in x*x\n        -2.0**600,                                  # x5: Large negative number causing overflow in x*x\n        2.0**-600,                                  # x6: Small number causing underflow in x*x\n        -2.0**-600,                                 # x7: Small negative number causing underflow in x*x\n        np.nextafter(T_overflow, 0),                # x8: Value just below overflow threshold\n        np.nextafter(T_overflow, np.inf),           # x9: Value just above overflow threshold\n        np.nextafter(T_zero, 0),                    # x10: Value just below underflow-to-zero threshold\n        np.nextafter(T_zero, np.inf)                # x11: Value just above underflow-to-zero threshold\n    ]\n\n    results = []\n    for x in test_suite:\n        # Perform the computations in double precision.\n        # y = sqrt(x * x)\n        y = np.sqrt(x * x)\n        \n        # z = |x|\n        z = np.abs(x)\n        \n        # Record a Boolean indicating whether y and z are bitwise equal.\n        # The '==' operator on numpy floats performs this exact comparison.\n        # The result string must be lowercase 'true' or 'false'.\n        is_equal = (y == z)\n        results.append(str(is_equal).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}