{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration of computational error, we must first quantify the fundamental resolution of our number system. This practice  guides you through empirically estimating the unit roundoff, $u$, which represents the smallest distinguishable relative step away from the number $1$. More than just a simple calculation, this exercise forces us to confront how clever compilers can subvert naive attempts at measurement, teaching us to write code that robustly probes the true behavior of the underlying hardware.",
            "id": "3579641",
            "problem": "You are to write a complete and runnable program that empirically estimates the unit roundoff $u$ for floating-point arithmetic by iterative halving of a step size $\\varepsilon$ using rigorous stopping criteria that avoid compiler optimization pitfalls. Work in the mathematical setting of a normalized binary floating-point system with base $b = 2$, precision $p$, and rounding to nearest with ties to even. The unit roundoff $u$ is the maximal relative error incurred when rounding an exact real to the nearest floating-point number; equivalently, $u$ is one half of the distance, measured in absolute value, from $1$ to the next representable floating-point number strictly greater than $1$. Use this as the foundational starting point and do not assume any specific \"shortcut\" formula.\n\nDesign an experiment that is universally applicable and that makes no reference to any physical units. Your implementation must be in terms of binary arithmetic on the following two concrete formats: the $64$-bit format (often called binary$64$) and the $32$-bit format (often called binary$32$). The experiment must be constructed from first principles:\n\n- Begin from the definition that adding a sufficiently small positive $\\varepsilon$ to $1$ in the target format will round back to $1$, and that there exists a smallest positive $\\varepsilon$ for which $1 + \\varepsilon$ rounds to a value strictly larger than $1$.\n- Iteratively halve a positive $\\varepsilon$ starting from $\\varepsilon = 1$ until you cross this threshold.\n\nYou must implement three estimators:\n\n1. A robust estimator that strictly avoids optimization pitfalls and unintended type promotions:\n   - Maintain a typed variable $\\varepsilon$ and a typed constant $1$ in the target format. At each step, consider the candidate $\\varepsilon/2$ and evaluate the guard expression $((1 + \\varepsilon/2) - 1)$ strictly in the target format. If this guard equals $0$ in the target format, stop; otherwise set $\\varepsilon \\leftarrow \\varepsilon/2$ and continue. At termination, return the robust estimate $u_{\\mathrm{robust}} = \\varepsilon/2$. The algebraic form $((1 + \\varepsilon/2) - 1)$ is required to prevent a compiler from simplifying the expression $1 + \\varepsilon/2$ when aggressive optimizations are enabled and to ensure a rounding step is actually executed in the target format.\n   - To avoid mixed-precision promotion, ensure every literal used in these computations (such as $0$, $1$, and $2$) is represented in the same target format as $\\varepsilon$.\n\n2. A naive estimator that uses the control logic $1 + \\varepsilon \\neq 1$ in the target format:\n   - Initialize $\\varepsilon = 1$ in the target format and repeatedly set $\\varepsilon \\leftarrow \\varepsilon/2$ while $1 + \\varepsilon \\neq 1$ in the target format. On exit, return the naive estimate $u_{\\mathrm{naive}} = \\varepsilon$.\n\n3. An intentionally unsafe estimator that demonstrates a failure mode via mixed-type promotion:\n   - Let $1$ be an untyped double-precision scalar (for example, a host-language default floating literal), while $\\varepsilon$ is maintained in $32$-bit precision. Run the same naive loop $1 + \\varepsilon \\neq 1$, allowing the addition to be evaluated at higher precision than the storage of $\\varepsilon$; return the resulting $u_{\\mathrm{unsafe}}$ to expose the effect of evaluation at the wrong precision.\n\nTo validate results without revealing shortcut formulas, construct a reference value using the definition of a unit in the last place at $1$:\n- In the target format, compute the next representable number above $1$, call it $\\operatorname{next}(1)$, and set the reference distance $\\varepsilon_{\\mathrm{ref}} = \\operatorname{next}(1) - 1$. Then set the reference unit roundoff $u_{\\mathrm{ref}} = \\varepsilon_{\\mathrm{ref}}/2$.\n\nYour program must instantiate the following test suite and produce a single line of output aggregating all results:\n\n- Test case A (binary$64$): compute $u_{64}^{\\mathrm{robust}}$, $u_{64}^{\\mathrm{naive}}$, the absolute deviation $\\lvert u_{64}^{\\mathrm{robust}} - u_{64}^{\\mathrm{ref}} \\rvert$, and a boolean $b_{64}$ that is true precisely when both $u_{64}^{\\mathrm{robust}}$ and $u_{64}^{\\mathrm{naive}}$ are close to $u_{64}^{\\mathrm{ref}}$ within a small relative tolerance that is scientifically justified for the format.\n- Test case B (binary$32$): compute $u_{32}^{\\mathrm{robust}}$, $u_{32}^{\\mathrm{naive}}$, the absolute deviation $\\lvert u_{32}^{\\mathrm{robust}} - u_{32}^{\\mathrm{ref}} \\rvert$, and a boolean $b_{32}$ with the analogous meaning as above.\n- Test case C (intentional failure): compute $u_{\\mathrm{unsafe}}$ using the mixed-precision construction in the $32$-bit loop, then compute booleans $n_{64}$ and $n_{32}$ indicating whether $u_{\\mathrm{unsafe}}$ is close to the $64$-bit reference and to the $32$-bit reference, respectively, using format-appropriate relative tolerances; this test should highlight that $n_{64}$ is true while $n_{32}$ is false.\n\nDesign for coverage:\n- The robust estimator exercises the half-ulp boundary at $1$ and enforces rounding in the specified format, covering the subtle tie-to-even boundary where $\\varepsilon = \\operatorname{ulp}(1)/2$ produces no change.\n- The naive estimator covers the common textbook pattern and should agree in an interpreter that does not optimize away expressions but may fail under aggressive optimization in compiled environments.\n- The unsafe estimator provides an edge-case failure mode via mixed-type arithmetic leading to evaluation at unintended precision.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets with the following $11$ entries in order:\n  - $u_{64}^{\\mathrm{robust}}$, $u_{64}^{\\mathrm{naive}}$, $\\lvert u_{64}^{\\mathrm{robust}} - u_{64}^{\\mathrm{ref}} \\rvert$, $b_{64}$, $u_{32}^{\\mathrm{robust}}$, $u_{32}^{\\mathrm{naive}}$, $\\lvert u_{32}^{\\mathrm{robust}} - u_{32}^{\\mathrm{ref}} \\rvert$, $b_{32}$, $u_{\\mathrm{unsafe}}$, $n_{64}$, $n_{32}$.\n- The numeric entries must be represented as standard floating-point literals, and the boolean entries must be represented as the language-native boolean literals. For example, an acceptable output shape is $[x_1,x_2,x_3,\\mathrm{True},x_5,x_6,x_7,\\mathrm{True},x_9,\\mathrm{True},\\mathrm{False}]$ with the specific numeric values determined by your implementation and the arithmetic of the execution environment.\n\nYour solution must adhere to the following constraints to avoid optimization pitfalls:\n- Ensure that all arithmetic in the robust and naive estimators is performed strictly in the target format by using typed scalars for $0$, $1$, and $2$, and by avoiding any mixed-precision operations.\n- Use the guard form $((1 + \\varepsilon/2) - 1)$ in the robust estimator to make the rounding step explicit and non-eliminable by algebraic simplification.\n- For the unsafe test, intentionally violate the previous constraint to provoke a failure.\n\nNo user input should be read, and no external data should be used. The program must be deterministic. The final line of program output must be exactly the described list, with no additional text before or after it.",
            "solution": "The user has provided a valid, well-posed problem statement that is scientifically grounded in the principles of numerical analysis and computer arithmetic. The task is to write a program that empirically estimates the unit roundoff $u$ for `binary64` and `binary32` floating-point formats using several distinct algorithms, including ones designed to highlight common pitfalls.\n\nThe solution will be developed by first establishing the theoretical basis for the unit roundoff, then designing algorithms for its estimation as specified, and finally constructing a test suite to validate the results and demonstrate specific computational phenomena.\n\n**1. Theoretical Foundation: Unit Roundoff and Machine Epsilon**\n\nIn a floating-point number system with base $b$, precision $p$, and a specified rounding rule, not all real numbers can be represented exactly. The error introduced by approximating a real number with a floating-point number is called roundoff error. The *unit roundoff*, denoted $u$, is a fundamental measure of the worst-case relative error of this approximation.\n\nFor a normalized binary floating-point system ($b=2$) using the \"round-to-nearest, ties-to-even\" rule (as specified by the IEEE 754 standard), the unit roundoff is directly related to the *machine epsilon*, $\\varepsilon_m$. The machine epsilon is defined as the distance between $1$ and the next larger representable floating-point number. Mathematically, $\\varepsilon_m = \\operatorname{next}(1) - 1$. For a system with precision $p$, $\\varepsilon_m = 2^{1-p}$.\n\nThe unit roundoff $u$ is half the machine epsilon:\n$$u = \\frac{1}{2} \\varepsilon_m = \\frac{1}{2} (\\operatorname{next}(1) - 1) = 2^{-p}$$\nThis value $u$ is the critical threshold for addition to $1$. For any positive real number $\\delta  u$, the sum $1+\\delta$ will round back down to $1$. For $\\delta  u$, the sum $1+\\delta$ will round to a value greater than $1$. At the boundary case $\\delta = u$, the \"ties-to-even\" rule is invoked. Since the number $1$ can be written with a significand of $1.00\\dots0$, which has a least significant bit of $0$ (even), the sum $1+u$ is rounded down to $1$. Therefore, the smallest positive machine number $\\varepsilon$ for which $1+\\varepsilon  1$ is $\\varepsilon_m$. The largest machine number $\\varepsilon'$ for which $1+\\varepsilon' = 1$ is $u$. Our estimation algorithms are designed to find this boundary.\n\n**2. Reference Value Calculation ($u_{\\mathrm{ref}}$)**\n\nTo validate our empirical estimates, a reference value $u_{\\mathrm{ref}}$ is calculated directly from the definition. The `numpy.nextafter` function provides a reliable way to find the next representable floating-point number. For a given data type (e.g., `binary64` or `binary32`), we compute:\n1.  One in the target format: $o = \\mathrm{dtype}(1)$.\n2.  The next representable number after $o$ towards a larger value (e.g., $2$): $n = \\operatorname{nextafter}(o, \\mathrm{dtype}(2))$.\n3.  The reference machine epsilon: $\\varepsilon_{\\mathrm{ref}} = n - o$.\n4.  The reference unit roundoff: $u_{\\mathrm{ref}} = \\varepsilon_{\\mathrm{ref}} / \\mathrm{dtype}(2)$.\n\nThis provides a ground truth against which our algorithmic estimators can be compared.\n\n**3. Algorithm Implementation**\n\nThree distinct estimators are implemented, each demonstrating a different aspect of computational practice. All arithmetic is constrained to the specified precision by explicitly casting all numeric literals ($0, 1, 2$) to the target data type.\n\n**3.1. Robust Estimator ($u_{\\mathrm{robust}}$)**\n\nThis estimator is designed to be immune to compiler optimizations that might subvert the intended floating-point comparison.\n- Initialize $\\varepsilon$ to $1$ in the target data type, `dtype`.\n- The core of the algorithm is a `while` loop that iteratively halves $\\varepsilon$. The loop's continuation condition is `((dtype(1) + (eps / dtype(2))) - dtype(1)) != dtype(0)`.\n- This specific form `((1 + ε/2) - 1)` is crucial. An aggressive compiler might simplify `1 + ε  1` by performing the comparison in a higher-precision processor register. However, the required subtraction ` - 1` forces the result of `1 + ε/2` to be rounded and stored (at least conceptually) before the subtraction can proceed, thus ensuring the check is performed at the precision of `dtype`.\n- The loop terminates when `eps` is equal to the machine epsilon, $\\varepsilon_m$. At this point, the candidate `eps/2` is equal to the unit roundoff $u$. The expression `(1 + u) - 1` correctly evaluates to $0$ due to rounding, and the loop condition becomes false.\n- The value of `eps` upon loop exit is therefore $\\varepsilon_m$. The function correctly returns $\\varepsilon/2$, which is $u$.\n\n**3.2. Naive Estimator ($u_{\\mathrm{naive}}$)**\n\nThis estimator uses a more straightforward but potentially fragile logic, common in introductory texts.\n- Initialize $\\varepsilon$ to $1$ in the target `dtype`.\n- The loop condition is `(dtype(1) + eps) != dtype(1)`.\n- The loop successively halves `eps` as long as `1 + eps` is distinguishable from `1`.\n- The loop terminates when `eps` becomes the first value for which `1 + eps` rounds back to `1`. In a system with round-to-nearest-ties-to-even, this value is precisely the unit roundoff, $u$.\n- The function returns the final value of `eps`, which is $u$.\n- In an interpreted environment like the specified Python/NumPy setup, where expressions are typically evaluated as written without aggressive reordering, this estimator is expected to yield the correct result.\n\n**3.3. Unsafe Estimator ($u_{\\mathrm{unsafe}}$)**\n\nThis estimator is designed to fail intentionally by demonstrating the peril of mixed-precision arithmetic.\n- The variable $\\varepsilon$ is maintained as a `binary32` (`numpy.float32`) number.\n- However, the numeric literal $1$ is a standard Python float, which is `binary64` (`numpy.float64`).\n- The loop condition is `(1.0 + eps) != 1.0`.\n- Due to type promotion rules, the `binary32` value `eps` is promoted to `binary64` before the addition. The entire comparison is therefore performed using `binary64` arithmetic.\n- Consequently, the loop will not terminate when `eps` reaches the `binary32` unit roundoff, but will continue until `eps` is small enough to be lost when added to $1.0$ in `binary64` precision.\n- The final value of `eps` will be a `binary32` representation of the `binary64` unit roundoff, $u_{64}$. This demonstrates how an apparently simple loop can produce a result for the wrong precision if types are not handled carefully.\n\n**4. Verification and Output**\n\nThe program computes the specified values for the three test cases (A, B, and C). The boolean flags ($b_{64}, b_{32}, n_{64}, n_{32}$) are determined by direct equality comparison (`==`). This is the most scientifically rigorous choice for \"closeness\" in this context, as all calculated reference and estimated values are exact powers of two, and any deviation would indicate a failure of the algorithm, not a minor rounding discrepancy. The absolute deviation $|\\dots|$ is calculated for completeness as requested. The final results are aggregated into a single formatted list for output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiments and print the results.\n    \"\"\"\n\n    def get_reference_u(dtype):\n        \"\"\"\n        Calculates the reference unit roundoff u for a given dtype\n        based on the definition u = (nextafter(1) - 1) / 2.\n        \"\"\"\n        one = dtype(1)\n        two = dtype(2)\n        # Find the next representable floating point number after 1.\n        next_one = np.nextafter(one, two)\n        # The distance is the machine epsilon for the number 1.\n        eps_ref = next_one - one\n        # Unit roundoff is half the machine epsilon.\n        u_ref = eps_ref / two\n        return u_ref\n\n    def robust_estimator(dtype):\n        \"\"\"\n        Estimates unit roundoff using a method robust against compiler optimization.\n        Loop terminates when eps = machine_epsilon. Returns eps/2.\n        \"\"\"\n        one = dtype(1)\n        two = dtype(2)\n        zero = dtype(0)\n        eps = dtype(1)\n\n        # The loop condition ((1 + eps/2) - 1) != 0 forces the rounding\n        # of (1 + eps/2) before the outer subtraction, preventing optimizations\n        # that might use higher-precision registers.\n        while ((one + (eps / two)) - one) != zero:\n            eps = eps / two\n        \n        # When the loop terminates, eps is the machine epsilon.\n        # The problem asks for the unit roundoff, which is eps / 2.\n        u_robust = eps / two\n        return u_robust\n\n    def naive_estimator(dtype):\n        \"\"\"\n        Estimates unit roundoff using the common textbook algorithm.\n        This can be unreliable under aggressive compiler optimization.\n        \"\"\"\n        one = dtype(1)\n        two = dtype(2)\n        eps = dtype(1)\n\n        # Loop until 1 + eps is indistinguishable from 1.\n        # The loop breaks when eps is the first value for which this occurs,\n        # which is the unit roundoff u.\n        while (one + eps) != one:\n            eps = eps / two\n            \n        u_naive = eps\n        return u_naive\n\n    def unsafe_estimator():\n        \"\"\"\n        Demonstrates failure mode from mixed-precision arithmetic.\n        'eps' is float32, but '1.0' is a float64 literal. The addition\n        is promoted to float64, yielding the float64 unit roundoff.\n        \"\"\"\n        # Python's default float is float64.\n        one_64 = 1.0 \n        eps = np.float32(1)\n        two_32 = np.float32(2)\n\n        # The comparison (one_64 + eps) promotes eps to float64.\n        # The loop will thus run until eps is at the float64 precision limit.\n        while (one_64 + eps) != one_64:\n            eps = eps / two_32\n\n        u_unsafe = eps\n        return u_unsafe\n\n    # Define the floating-point types to be tested.\n    dtype64 = np.float64\n    dtype32 = np.float32\n\n    # --- Test Case A: binary64 ---\n    u64_ref = get_reference_u(dtype64)\n    u64_robust = robust_estimator(dtype64)\n    u64_naive = naive_estimator(dtype64)\n    # The robust estimator is expected to be exact, so deviation should be 0.\n    dev64 = np.abs(u64_robust - u64_ref)\n    # Both estimators should return the exact reference value.\n    b64 = (u64_robust == u64_ref) and (u64_naive == u64_ref)\n\n    # --- Test Case B: binary32 ---\n    u32_ref = get_reference_u(dtype32)\n    u32_robust = robust_estimator(dtype32)\n    u32_naive = naive_estimator(dtype32)\n    # The robust estimator is expected to be exact, so deviation should be 0.\n    dev32 = np.abs(u32_robust - u32_ref)\n    # Both estimators should return the exact reference value.\n    b32 = (u32_robust == u32_ref) and (u32_naive == u32_ref)\n\n    # --- Test Case C: Intentional Failure ---\n    u_unsafe = unsafe_estimator()\n    # Check if the unsafe result (a float32) matches the float64 reference.\n    # The comparison should be u_unsafe == float32(u64_ref) for correctness.\n    n64 = (u_unsafe == dtype32(u64_ref))\n    # Check if the unsafe result matches the float32 reference (it shouldn't).\n    n32 = (u_unsafe == u32_ref)\n\n    # Aggregate results into a list for printing.\n    results = [\n        u64_robust,\n        u64_naive,\n        dev64,\n        b64,\n        u32_robust,\n        u32_naive,\n        dev32,\n        b32,\n        u_unsafe,\n        n64,\n        n32\n    ]\n\n    # Format the final output string as specified.\n    # str() correctly formats floats and booleans (as True/False).\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While unit roundoff governs errors in precision, the finite range of floating-point numbers introduces a different class of catastrophic error: overflow and underflow. This exercise  demonstrates this principle in a striking way by showing how the seemingly infallible identity $\\sqrt{x \\cdot x} = \\lvert x \\rvert$ breaks down. By testing values near the limits of the representable range, you will see firsthand how an intermediate calculation can overflow to infinity or underflow to zero, leading to a mathematically incorrect final result.",
            "id": "3276013",
            "problem": "Design and implement a program that empirically demonstrates that, in finite-precision arithmetic compliant with the Institute of Electrical and Electronics Engineers (IEEE) $754$ binary$64$ (double precision), the identity $\\sqrt{x \\cdot x} = \\lvert x \\rvert$ does not hold for all representable $x$. Your program must use only double-precision floating-point operations supplied by the runtime and must not emulate higher precision.\n\nUse the following fundamental base to guide your reasoning and implementation: floating-point arithmetic represents real numbers approximately, using a fixed precision and a bounded exponent range; basic operations are rounded to the nearest representable value (ties to even), with overflow mapping to $\\pm \\infty$ and underflow potentially producing subnormal numbers or zero. These are standard, well-tested properties of IEEE $754$ binary$64$ arithmetic.\n\nTask specification:\n- For each test value $x$, compute $y = \\sqrt{x \\cdot x}$ and $z = \\lvert x \\rvert$ using the platform’s double-precision arithmetic. For each case, record a Boolean indicating whether $y$ and $z$ are bitwise equal at the floating-point level using the language’s exact equality operator for doubles.\n- Construct the test suite so that it covers typical magnitudes, overflow, and underflow. Define the following machine parameters for binary$64$:\n  - $M$: the largest finite positive number representable (maximum finite).\n  - $m$: the smallest positive normal number representable (minimum positive normal).\n  - $\\varepsilon$: the distance between $1$ and the next larger representable number (unit roundoff near $1$).\n  - $s = m \\cdot \\varepsilon$: the smallest positive subnormal number representable.\n  - $T_{\\text{overflow}} = \\sqrt{M}$.\n  - $T_{\\text{zero}} = \\sqrt{s}$.\n  - $\\operatorname{nextafter}(a, b)$: the representable number adjacent to $a$ in the direction of $b$.\n- Using only double-precision arithmetic and the above parameters, build the following test suite of values $x$:\n  1. $x_1 = 1.234567890123456$.\n  2. $x_2 = -98765.4321$.\n  3. $x_3 = 0$.\n  4. $x_4 = 2^{600}$.\n  5. $x_5 = -2^{600}$.\n  6. $x_6 = 2^{-600}$.\n  7. $x_7 = -2^{-600}$.\n  8. $x_8 = \\operatorname{nextafter}(T_{\\text{overflow}}, 0)$.\n  9. $x_9 = \\operatorname{nextafter}(T_{\\text{overflow}}, +\\infty)$.\n  10. $x_{10} = \\operatorname{nextafter}(T_{\\text{zero}}, 0)$.\n  11. $x_{11} = \\operatorname{nextafter}(T_{\\text{zero}}, +\\infty)$.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $x_1$ through $x_{11}$. Each list element must be a Boolean indicating whether $\\sqrt{x \\cdot x}$ equals $\\lvert x \\rvert$ under exact floating-point equality. For example, an output of the form $[b_1,b_2,\\dots,b_{11}]$ where each $b_i$ is either true or false.\n\nNotes and requirements:\n- All computations must be carried out in double precision (IEEE $754$ binary$64$).\n- No physical units are involved.\n- Angles are not involved.\n- The final output must be exactly one line in the specified list format.",
            "solution": "The problem statement is valid. It presents a well-posed and scientifically grounded investigation into the limitations of finite-precision arithmetic, a fundamental topic in numerical analysis and scientific computing. The problem is self-contained, its terms are rigorously defined, and the required computations are feasible within the specified IEEE $754$ binary$64$ environment.\n\nThe mathematical identity $\\sqrt{x \\cdot x} = \\lvert x \\rvert$ is an axiom for real numbers, $\\forall x \\in \\mathbb{R}$. However, computer arithmetic does not operate on the field of real numbers, $\\mathbb{R}$, but on a finite subset of rational numbers, representable as floating-point values. The IEEE $754$ binary$64$ standard for double-precision floating-point numbers imposes strict limits on both the range and precision of these values. These limitations are the source of discrepancies between ideal mathematical identities and their computational evaluation. The identity fails precisely when the intermediate computation of $x \\cdot x$ results in overflow or underflow.\n\nA number in the IEEE $754$ binary$64$ format is represented using a sign bit, an $11$-bit exponent, and a $52$-bit significand (fraction). This structure defines the key properties of the system:\n- The maximum finite positive number, $M \\approx 1.797 \\times 10^{308}$. Any result larger than $M$ overflows to positive infinity ($+\\infty$).\n- The smallest positive normal number, $m \\approx 2.225 \\times 10^{-308}$, corresponding to the minimum normal exponent of $-1022$.\n- The smallest positive subnormal number, $s \\approx 4.94 \\times 10^{-324}$. Any result with a magnitude smaller than $s$ that cannot be represented as a subnormal number will underflow and be flushed to $0$. The problem correctly defines $s$ as $s = m \\cdot \\varepsilon$, where $\\varepsilon$ is the machine epsilon, because $m=2^{-1022}$ and $\\varepsilon=2^{-52}$, yielding $s=2^{-1074}$.\n\nThe test suite is designed to probe these limits. We will analyze each case to predict whether the computational evaluation of `` `sqrt(x*x) == abs(x)` `` will yield true or false.\n\n1.  **Cases $x_1, x_2, x_3$ (Well-behaved values):**\n    - For $x_1 = 1.234567890123456$ and $x_2 = -98765.4321$, the values of $x$ and $x \\cdot x$ are well within the range of normal floating-point numbers. While the product $x \\cdot x$ may require more than $53$ bits of precision and thus incur a rounding error, modern floating-point units, particularly the `sqrt` function, are specified to be highly accurate (correctly rounded). For most well-behaved inputs, the composition of these operations is expected to be numerically stable and recover the original magnitude. The identity will hold.\n    - For $x_3 = 0$, the identity is trivially true: $\\sqrt{0.0 \\cdot 0.0} = \\sqrt{0.0} = 0.0$, and $\\lvert 0.0 \\rvert = 0.0$.\n\n2.  **Cases $x_4, x_5, x_9$ (Overflow):**\n    - The threshold for overflow in the multiplication is $x$ such that $\\lvert x \\rvert > \\sqrt{M}$. The value $T_{\\text{overflow}} = \\sqrt{M} \\approx 1.34 \\times 10^{154}$.\n    - For $x_4 = 2^{600}$ and $x_5 = -2^{600}$, the magnitude is $\\lvert x \\rvert = 2^{600}$. The intermediate product is $(2^{600})^2 = 2^{1200}$. The maximum exponent for a binary$64$ number is $1023$. Since $1200 > 1023$, the product $x \\cdot x$ will overflow to $+\\infty$. The subsequent operation $\\sqrt{+\\infty}$ returns $+\\infty$. Since $x$ is a finite number, $+\\infty \\ne \\lvert x \\rvert$. The identity will fail.\n    - For $x_9 = \\operatorname{nextafter}(T_{\\text{overflow}}, +\\infty)$, the value of $x_9$ is by definition the smallest representable number strictly greater than $T_{\\text{overflow}}$. Therefore, $x_9^2 > M$, causing $x_9 \\cdot x_9$ to overflow to $+\\infty$. As before, the result $\\sqrt{+\\infty} = +\\infty$ is not equal to the finite value $\\lvert x_9 \\rvert$. The identity will fail.\n\n3.  **Cases $x_6, x_7, x_{10}$ (Underflow):**\n    - The threshold for underflow in the multiplication is related to the smallest positive subnormal number, $s = 2^{-1074}$. If $x \\cdot x$ produces a result less than the midpoint between $0$ and $s$ (i.e., $ s/2$), it will be flushed to $0$. This occurs when $\\lvert x \\rvert  \\sqrt{s}$. We define $T_{\\text{zero}} = \\sqrt{s} = \\sqrt{2^{-1074}} = 2^{-537}$.\n    - For $x_6 = 2^{-600}$ and $x_7 = -2^{-600}$, the magnitude is $\\lvert x \\rvert = 2^{-600}$. The intermediate product is $(2^{-600})^2 = 2^{-1200}$. The smallest representable (subnormal) positive number has an effective exponent of $-1074$. Since $-1200  -1074$, the result of $x \\cdot x$ is too small to be represented and underflows to $0.0$. Then $\\sqrt{0.0} = 0.0$, which is not equal to the original non-zero magnitude $2^{-600}$. The identity will fail.\n    - For $x_{10} = \\operatorname{nextafter}(T_{\\text{zero}}, 0)$, the value of $x_{10}$ is strictly less than $T_{\\text{zero}}$. Therefore, $x_{10}^2  T_{\\text{zero}}^2 = s$. This product $x_{10} \\cdot x_{10}$ falls into the range where it underflows to $0.0$. The result $\\sqrt{0.0} = 0.0$ is not equal to the non-zero value $\\lvert x_{10} \\rvert$. The identity will fail.\n\n4.  **Cases $x_8, x_{11}$ (Boundary-adjacent values):**\n    - For $x_8 = \\operatorname{nextafter}(T_{\\text{overflow}}, 0)$, the value is the largest representable number strictly less than $T_{\\text{overflow}}$. Thus, $x_8^2  M$, so the product $x_8 \\cdot x_8$ will not overflow and will result in a finite number close to $M$. The subsequent correctly rounded square root operation is expected to precisely reverse the squaring, so $\\sqrt{x_8 \\cdot x_8}$ will equal $\\lvert x_8 \\rvert$. The identity will hold.\n    - For $x_{11} = \\operatorname{nextafter}(T_{\\text{zero}}, +\\infty)$, the value is strictly greater than $T_{\\text{zero}} = \\sqrt{s}$. The product $x_{11}^2 > s$. This result is large enough to be represented as a non-zero (subnormal) number. The square root of this subnormal number is well-defined and is expected to be computed with sufficient accuracy to recover the original value $\\lvert x_{11} \\rvert$. The identity will hold.\n\nThe empirical demonstration will thus show that the identity fails for cases $x_4, x_5, x_6, x_7, x_9, x_{10}$, and holds for the remaining cases. This confirms that the failures are predictable consequences of overflow and underflow in finite-precision arithmetic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the failure of the identity sqrt(x*x) = abs(x)\n    in IEEE 754 double-precision arithmetic due to overflow and underflow.\n    \"\"\"\n    \n    # Define machine parameters for binary64 (double precision) using numpy.finfo.\n    finfo = np.finfo(np.float64)\n    M = finfo.max          # Largest finite positive number\n    m = finfo.tiny         # Smallest positive normal number\n    eps = finfo.eps        # Machine epsilon\n    \n    # Per the problem statement, s is the smallest positive subnormal number.\n    # np.finfo.smallest_subnormal provides this directly.\n    # The problem's formula s = m * eps is also correct for binary64.\n    # Let's verify: m = 2**-1022, eps = 2**-52, so m*eps = 2**-1074.\n    # np.finfo.smallest_subnormal is also 2**-1074.\n    s = finfo.smallest_subnormal\n\n    # Define thresholds based on the machine parameters.\n    # T_overflow is the threshold for x above which x*x overflows.\n    T_overflow = np.sqrt(M)\n    \n    # T_zero is the threshold for x below which x*x may underflow to zero.\n    T_zero = np.sqrt(s)\n\n    # Build the test suite of values for x.\n    test_suite = [\n        1.234567890123456,                          # x1: Normal number\n        -98765.4321,                                # x2: Normal negative number\n        0.0,                                        # x3: Zero\n        2.0**600,                                   # x4: Large number causing overflow in x*x\n        -2.0**600,                                  # x5: Large negative number causing overflow in x*x\n        2.0**-600,                                  # x6: Small number causing underflow in x*x\n        -2.0**-600,                                 # x7: Small negative number causing underflow in x*x\n        np.nextafter(T_overflow, 0),                # x8: Value just below overflow threshold\n        np.nextafter(T_overflow, np.inf),           # x9: Value just above overflow threshold\n        np.nextafter(T_zero, 0),                    # x10: Value just below underflow-to-zero threshold\n        np.nextafter(T_zero, np.inf)                # x11: Value just above underflow-to-zero threshold\n    ]\n\n    results = []\n    for x in test_suite:\n        # Perform the computations in double precision.\n        # y = sqrt(x * x)\n        y = np.sqrt(x * x)\n        \n        # z = |x|\n        z = np.abs(x)\n        \n        # Record a Boolean indicating whether y and z are bitwise equal.\n        # The '==' operator on numpy floats performs this exact comparison.\n        is_equal = (y == z)\n        results.append(str(is_equal))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The final source of error we will investigate is not a property of the floating-point standard itself, but of how it is sometimes implemented—or ignored—for the sake of performance. Compilers can apply \"optimizations\" like reordering arithmetic operations, which, while valid for real numbers, violate the non-associative nature of floating-point arithmetic. This practice  challenges you to simulate these \"aggressive\" semantics and contrast them with strict IEEE 754 behavior, revealing how such changes can lead to dramatically different results in cases of catastrophic cancellation, signed zeros, and NaN handling.",
            "id": "3276009",
            "problem": "You are to write a complete program that demonstrates, on well-chosen numerical examples, how floating-point results can change when an implementation makes aggressive floating-point assumptions similar to those enabled by common compiler flags such as `-ffast-math`. The topic is sources of error in computation in numerical methods and scientific computing. The goal is to make the differences systematic, reproducible, and attributable to first principles of finite precision.\n\nStart from the following fundamental base:\n- The Institute of Electrical and Electronics Engineers (IEEE) 754 standard defines floating-point formats and semantics. In binary64, each operation is performed as if with infinite precision and then rounded to the nearest representable number, which we denote by the rounding operator $\\,\\mathrm{fl}(\\cdot)\\,$. For a basic operation $\\,\\circ\\in\\{+,-,\\times,\\div\\}\\,$, the computed result satisfies $\\,\\mathrm{fl}(x\\circ y)=(x\\circ y)(1+\\delta)\\,$ with $\\,|\\delta|\\le u\\,$, where $\\,u\\,$ is the unit roundoff. These operations are not associative in floating-point arithmetic, so $\\,\\mathrm{fl}((x+y)+z)\\,$ need not equal $\\,\\mathrm{fl}(x+(y+z))$.\n- IEEE 754 also specifies special values: Not a Number (NaN), $+\\infty$, $-\\infty$, signed zeros $\\,{+}0\\,$ and $\\,{-}0\\,$, and subnormal numbers. NaNs propagate through most operations. Division by $\\,{-}0\\,$ yields a signed infinity. Subnormal numbers fill the underflow gap but may be flushed to zero on some systems for speed.\n\nYou must implement two evaluation semantics for a fixed suite of expressions:\n- Strict semantics: emulate IEEE 754 binary64 behavior with the specified operation order, NaN propagation, signed zeros, and subnormals preserved.\n- Aggressive semantics: emulate a set of unsafe assumptions often permitted under aggressive optimization, namely:\n  - Reassociation of addition where convenient, including grouping positive and negative terms in a reduction.\n  - Treat every $\\,{-}0\\,$ as $\\,{+}0\\,$ before each operation (ignore the sign of zero).\n  - Flush subnormal inputs and results to $\\,0\\,$ at every step, that is, map any $\\,x\\,$ with $0|x|\\tau$ to $\\,0\\,$, where $\\,\\tau\\,$ is the minimal positive normal number in binary64.\n  - Treat any NaN input as $\\,0\\,$ before each operation (no NaN propagation).\n  - Apply the algebraic identity $\\,x/x=1\\,$ whenever the two operands are bitwise equal and finite, regardless of exceptional cases.\n\nYour program must evaluate the following test suite. Each test defines both the data and the expression to compute under the two semantics. Angles are not involved. There are no physical units.\n\n- Test $1$ (non-associativity in addition): Use $\\,x=10^{16}\\,$, $\\,y=-10^{16}\\,$, $\\,z=1\\,$. Under strict semantics, evaluate $\\,((x+y)+z)\\,$. Under aggressive semantics, evaluate $\\,x+(y+z)\\,$ with the stepwise aggressive rules above.\n- Test $2$ (signed zero in division): Use $\\,a=1.0\\,$, $\\,b=-0.0\\,$. Evaluate $\\,a/b\\,$ under both semantics.\n- Test $3$ (NaN folding via $\\,x/x=1\\,$): Use $\\,a=0.0\\,$. Evaluate $\\,\\big(a/a\\big)-1\\,$ under both semantics.\n- Test $4$ (subnormal flushing in a sum): Let $\\,s=\\mathrm{nextafter}(0,1)\\,$ be the minimal positive subnormal in binary64 and $\\,N=1000\\,$. Under strict semantics, compute the left-to-right sum $\\,\\sum_{k=1}^{N} s\\,$. Under aggressive semantics, apply stepwise flushing and signed-zero rules at each addition.\n- Test $5$ (reduction reordering): Let $\\,N=200000\\,$. Consider the alternating harmonic partial sum $\\,S_{N}=\\sum_{k=1}^{N}(-1)^{k+1}\\frac{1}{k}\\,$. Under strict semantics, sum left-to-right in index order. Under aggressive semantics, first sum all positive terms, then sum all negative terms, and finally add the two partial sums, with stepwise aggressive rules applied.\n\nFor each test $i\\in\\{1,2,3,4,5\\}$, define a boolean outcome $\\,B_i\\,$ that is true if and only if the two semantics behave differently in a numerically observable sense, namely:\n- One result is NaN and the other is not; or\n- Both are infinite with different signs; or\n- Both are finite but not exactly equal in value; or\n- Both compare equal numerically but one is $\\,{+}0\\,$ and the other is $\\,{-}0$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[B_1,B_2,B_3,B_4,B_5]$.\n\nConstraints and guidance:\n- Use binary64 arithmetic. In Python, you may rely on the default double-precision floats and operations provided by numerical libraries that follow IEEE 754.\n- Ensure that the aggressive semantics are implemented by explicit transformations in your code rather than relying on interpreter settings.\n- The final output must be reproducible without any external input.\n\nYour program must not read any input and must not print anything besides the final single-line output.",
            "solution": "The problem requires an implementation of two distinct floating-point evaluation semantics, \"strict\" and \"aggressive,\" to demonstrate how unsafe compiler optimizations can alter numerical results. The validity of the problem is established as follows:\n\n- **Scientific or Factual Soundness**: The problem is fundamentally sound. It is based on the well-established IEEE 754 standard for floating-point arithmetic and explores common, albeit unsafe, optimizations related to that standard. The concepts—non-associativity, signed zeros, NaN propagation, subnormal numbers, and catastrophic cancellation—are core topics in numerical analysis.\n- **Well-Posed**: The problem is well-posed. Each of the five tests is defined with specific data, expressions, and evaluation rules for both semantics. The criteria for comparing the outcomes are precise, ensuring a unique boolean result ($B_i$) for each test.\n- **Completeness and Consistency**: The problem statement is self-contained. It provides all necessary definitions for strict and aggressive semantics, all data for the test cases, and the exact required output format. There are no contradictory constraints.\n\nThe problem is therefore deemed valid. A step-by-step analysis and solution for each test case follows. We use binary64 floating-point arithmetic, for which the unit roundoff is $u = 2^{-53}$. The minimal positive normal number is $\\tau = 2^{-1022}$. $\\mathrm{fl}(\\cdot)$ denotes the rounding operation to the nearest representable number.\n\n### Test 1: Non-associativity in Addition\n- **Data**: $x = 10^{16}$, $y = -10^{16}$, $z = 1$.\n- **Strict Semantics**: The expression is evaluated as specified: $\\mathrm{fl}(\\mathrm{fl}(x+y)+z)$.\nThe inner operation is $\\mathrm{fl}(x+y) = \\mathrm{fl}(10^{16} - 10^{16}) = 0$.\nThe outer operation is then $\\mathrm{fl}(0+z) = \\mathrm{fl}(0+1) = 1$.\nThe strict result is $1.0$.\n- **Aggressive Semantics**: The expression is reordered to $\\mathrm{fl}(x + \\mathrm{fl}(y+z))$.\nThe inner operation is $\\mathrm{fl}(y+z) = \\mathrm{fl}(-10^{16} + 1)$. In binary64, the magnitude of the difference in exponents between $y$ and $z$ is too large for the addition to affect $y$. Since $|z/y| = 10^{-16}  u \\approx 1.11 \\times 10^{-16}$, the addition of $1$ is lost due to absorption (swamping). Thus, $\\mathrm{fl}(-10^{16} + 1) = -10^{16}$.\nThe outer operation is then $\\mathrm{fl}(x - 10^{16}) = \\mathrm{fl}(10^{16} - 10^{16}) = 0$.\nThe aggressive result is $0.0$.\n- **Outcome**: The results $1.0$ and $0.0$ are finite and not exactly equal. Therefore, $B_1$ is true.\n\n### Test 2: Signed Zero in Division\n- **Data**: $a = 1.0$, $b = -0.0$.\n- **Strict Semantics**: The expression is $\\mathrm{fl}(a/b)$. According to IEEE 754, division of a positive number by a negative zero yields negative infinity.\n$\\mathrm{fl}(1.0 / -0.0) = -\\infty$.\n- **Aggressive Semantics**: The rule \"Treat every $-0$ as $+0$ before each operation\" is applied.\nThe operand $b = -0.0$ is transformed to $+0.0$ before the division.\nThe expression becomes $\\mathrm{fl}(1.0 / +0.0)$, which yields positive infinity: $+\\infty$.\n- **Outcome**: The results are $-\\infty$ and $+\\infty$. These are both infinite but have different signs. Therefore, $B_2$ is true.\n\n### Test 3: NaN Folding via $x/x=1$\n- **Data**: $a = 0.0$.\n- **Strict Semantics**: The expression is $\\mathrm{fl}(\\mathrm{fl}(a/a)-1)$.\nThe division $\\mathrm{fl}(0.0/0.0)$ is an invalid operation under IEEE 754 and correctly produces a Not a Number (NaN).\nThe subsequent operation, $\\mathrm{fl}(\\mathrm{NaN} - 1)$, propagates the NaN. The strict result is NaN.\n- **Aggressive Semantics**: The rule \"Apply the algebraic identity $x/x=1$ whenever the two operands are bitwise equal and finite\" is applied.\nIn the sub-expression $a/a$, the operand $a=0.0$ is finite and bitwise equal to itself. The sub-expression evaluates to $1.0$ instead of NaN.\nThe full expression then becomes $\\mathrm{fl}(1.0 - 1.0) = 0.0$.\n- **Outcome**: One result is NaN and the other is not. Therefore, $B_3$ is true.\n\n### Test 4: Subnormal Flushing in a Sum\n- **Data**: $s$ is the minimal positive subnormal number in binary64 ($s = 2^{-1074}$), $N=1000$.\n- **Strict Semantics**: The expression is the left-to-right sum $\\sum_{k=1}^{N} s$. The sum is computed as $N \\times s = 1000 \\times 2^{-1074}$. This result is a non-zero value that is itself a subnormal number, as $1000 \\times 2^{-1074}  \\tau = 2^{-1022}$.\n- **Aggressive Semantics**: The rule \"Flush subnormal inputs and results to $0$\" is applied at each step.\nIn each addition of the sum, the term $s$ is a subnormal input and is flushed to $0.0$.\nThe sum becomes $\\sum_{k=1}^{N} 0.0$, which is exactly $0.0$.\n- **Outcome**: The results are a non-zero subnormal number and $0.0$. These are finite and not exactly equal. Therefore, $B_4$ is true.\n\n### Test 5: Reduction Reordering\n- **Data**: The alternating harmonic partial sum $S_{N}=\\sum_{k=1}^{N}(-1)^{k+1}\\frac{1}{k}$ for $N=200000$.\n- **Strict Semantics**: The sum is computed in left-to-right order: $1 - 1/2 + 1/3 - 1/4 + \\dots$. For an alternating series where terms decrease in magnitude, this order of summation is numerically stable. The result will be a close approximation of the series' limit, $\\ln(2) \\approx 0.69314718$.\n- **Aggressive Semantics**: The sum is reordered by grouping positive and negative terms: $$S_N = \\left(\\sum_{k \\text{ odd}} \\frac{1}{k}\\right) + \\left(\\sum_{k \\text{ even}} \\frac{-1}{k}\\right)$$\nLet $P = \\sum_{j=1}^{N/2} \\frac{1}{2j-1}$ and $M = \\sum_{j=1}^{N/2} \\frac{-1}{2j}$.\nFor large $N$, both $P$ and $M$ become large in magnitude. $P \\approx \\frac{1}{2}\\ln(N)$ and $M \\approx -\\frac{1}{2}\\ln(N)$. Computing $S_N = \\mathrm{fl}(P+M)$ involves adding two large numbers of opposite sign that are nearly equal in magnitude. This is a classic case of catastrophic cancellation, where most of the significant digits are lost. The precision of the aggressive result will be far lower than that of the strict result.\n- **Outcome**: The two finite results are a stable, accurate approximation and an inaccurate one suffering from catastrophic cancellation. They will not be exactly equal. Therefore, $B_5$ is true.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Demonstrates differences between strict and aggressive floating-point semantics.\n    \"\"\"\n\n    # --- Comparison Helper ---\n    def are_different(res_s, res_a):\n        \"\"\"\n        Determines if two results are observably different based on problem criteria.\n        \"\"\"\n        # Case 1: One is NaN and the other is not.\n        if np.isnan(res_s) != np.isnan(res_a):\n            return True\n        # Case 2: Both are infinite with different signs.\n        if np.isinf(res_s) and np.isinf(res_a) and res_s != res_a:\n            return True\n        # Case 3: Both are finite but not equal.\n        if np.isfinite(res_s) and np.isfinite(res_a) and res_s != res_a:\n            return True\n        # Case 4: Both are zero but with different signs.\n        if res_s == 0.0 and res_a == 0.0 and np.signbit(res_s) != np.signbit(res_a):\n            return True\n        return False\n\n    # --- Aggressive Semantics Helper ---\n    TAU = np.finfo(np.float64).tiny # Minimal positive normal number\n\n    def aggressive_transform(x):\n        \"\"\"\n        Applies a set of aggressive transformations to a single float value.\n        \"\"\"\n        val = np.float64(x)\n        # Rule: Treat any NaN input as 0\n        if np.isnan(val):\n            return np.float64(0.0)\n        # Rule: Treat every -0 as +0\n        if val == 0.0 and np.signbit(val):\n            val = np.float64(0.0)\n        # Rule: Flush subnormal inputs and results to 0\n        if 0  np.abs(val)  TAU:\n            return np.float64(0.0)\n        return val\n\n    results = []\n\n    # --- Test 1: Non-associativity in addition ---\n    x, y, z = np.float64(1e16), np.float64(-1e16), np.float64(1.0)\n    # Strict\n    res1_s = (x + y) + z\n    # Aggressive\n    inner_sum = aggressive_transform(y) + aggressive_transform(z)\n    res1_a = aggressive_transform(x) + aggressive_transform(inner_sum)\n    results.append(are_different(res1_s, res1_a))\n\n    # --- Test 2: Signed zero in division ---\n    a, b = np.float64(1.0), np.copysign(np.float64(0.0), -1.0) # -0.0\n    # Strict\n    res2_s = a / b\n    # Aggressive\n    res2_a = aggressive_transform(a) / aggressive_transform(b)\n    results.append(are_different(res2_s, res2_a))\n\n    # --- Test 3: NaN folding via x/x=1 ---\n    a = np.float64(0.0)\n    # Strict\n    res3_s = (a / a) - 1.0\n    # Aggressive: special rule x/x=1 for finite x\n    term1_a = np.float64(1.0) # a/a - 1.0\n    res3_a = aggressive_transform(term1_a) - aggressive_transform(1.0)\n    results.append(are_different(res3_s, res3_a))\n    \n    # --- Test 4: Subnormal flushing in a sum ---\n    s = np.nextafter(np.float64(0), np.float64(1)) # Minimal positive subnormal\n    N = 1000\n    # Strict\n    total_s = np.float64(0.0)\n    for _ in range(N):\n        total_s += s\n    res4_s = total_s\n    # Aggressive\n    total_a = aggressive_transform(0.0)\n    term_a = aggressive_transform(s) # flushes s to 0.0\n    for _ in range(N):\n        total_a += term_a\n        total_a = aggressive_transform(total_a)\n    res4_a = total_a\n    results.append(are_different(res4_s, res4_a))\n\n    # --- Test 5: Reduction reordering ---\n    N = 200000\n    # Strict: left-to-right summation\n    total_s = np.float64(0.0)\n    for k in range(1, N + 1):\n        total_s += ((-1)**(k + 1)) / k\n    res5_s = total_s\n    # Aggressive: sum positives, then negatives\n    pos_sum = np.float64(0.0)\n    for k in range(1, N + 1, 2):\n        pos_sum += 1.0 / k\n    \n    neg_sum = np.float64(0.0)\n    for k in range(2, N + 1, 2):\n        neg_sum += -1.0 / k\n    \n    # Per the problem description, stepwise rules are applied. For this test,\n    # reassociation is the key rule. The other aggressive transforms (NaN, subnormal)\n    # do not trigger on these terms.\n    res5_a = pos_sum + neg_sum\n    results.append(are_different(res5_s, res5_a))\n\n    # Final print statement in the exact required format.\n    # Note: str() on a Python boolean produces \"True\" or \"False\" with capitalization.\n    results_str = [str(b) for b in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}