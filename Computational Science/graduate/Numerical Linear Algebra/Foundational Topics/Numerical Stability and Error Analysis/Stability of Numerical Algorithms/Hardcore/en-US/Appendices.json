{
    "hands_on_practices": [
        {
            "introduction": "In numerical analysis, the ultimate goal is to minimize the forward error, the difference between the computed and exact solutions. However, we can typically only measure the residual, which quantifies how well the computed solution satisfies the original equation. This exercise provides a fundamental hands-on calculation to connect these two crucial quantities, demonstrating how the matrix's condition number acts as an amplifier between the relative residual and the relative forward error. ",
            "id": "3581515",
            "problem": "Consider the linear system $A x = b$ with \n$$A = \\mathrm{diag}(1, 10, 100), \\quad b = \\begin{pmatrix} 6 \\\\ 8 \\\\ 0 \\end{pmatrix}.$$\nA numerical algorithm produces an approximate solution $\\hat{x}$ whose residual $r$ is measured as \n$$r = b - A \\hat{x} = \\begin{pmatrix} 1 \\times 10^{-6} \\\\ -2 \\times 10^{-6} \\\\ 2 \\times 10^{-6} \\end{pmatrix}.$$\nUsing the Euclidean vector norm $\\|\\cdot\\|_{2}$ and the associated induced matrix norm, and taking the matrix condition number in the spectral norm $\\kappa_{2}(A)$ as known for this $A$, derive from first principles an a priori upper bound (dependent only on $\\kappa_{2}(A)$, $\\|r\\|_{2}$, and $\\|b\\|_{2}$) for the relative forward error $\\dfrac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}}$, where $x$ is the exact solution to $A x = b$. Then evaluate this bound numerically for the given data. Round your final numerical bound to four significant figures.",
            "solution": "The user wants to find an a priori upper bound for the relative forward error of an approximate solution to a linear system and then evaluate this bound numerically.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens:**\n- Linear system: $A x = b$\n- Matrix $A$: $A = \\mathrm{diag}(1, 10, 100)$\n- Vector $b$: $b = \\begin{pmatrix} 6 \\\\ 8 \\\\ 0 \\end{pmatrix}$\n- Exact solution: $x$\n- Approximate solution: $\\hat{x}$\n- Residual vector $r$: $r = b - A \\hat{x} = \\begin{pmatrix} 1 \\times 10^{-6} \\\\ -2 \\times 10^{-6} \\\\ 2 \\times 10^{-6} \\end{pmatrix}$\n- Norms: Euclidean vector norm $\\|\\cdot\\|_{2}$ and the associated induced matrix norm (spectral norm).\n- Known constant: Condition number $\\kappa_{2}(A)$.\n- Goal: Derive an upper bound for the relative forward error $\\dfrac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}}$ in terms of $\\kappa_{2}(A)$, $\\|r\\|_{2}$, and $\\|b\\|_{2}$, and then compute its numerical value.\n\n**1.2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, dealing with the relationship between the residual and the forward error. This is a fundamental concept in the analysis of numerical algorithms for solving linear systems. The principles involved are well-established.\n- **Well-Posed:** The matrix $A$ is diagonal with non-zero diagonal entries, so it is invertible, guaranteeing a unique solution $x$ exists. The problem asks for a standard upper bound, which is a well-defined derivation. All necessary data is provided.\n- **Objective:** The problem is stated using precise mathematical language and definitions, free from ambiguity or subjective claims.\n- **Other criteria:** The problem is self-contained, consistent, and scientifically feasible. It does not violate any of the specified invalidity criteria.\n\n**1.3. Verdict and Action:**\nThe problem is valid. A complete solution will be provided.\n\n### Step 2: Derivation of the Upper Bound\n\nThe relative forward error is defined as $\\dfrac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}}$. We need to find an upper bound for this quantity.\n\nLet $e = x - \\hat{x}$ be the forward error vector.\nThe residual is given as $r = b - A \\hat{x}$.\nThe exact solution $x$ satisfies the equation $A x = b$.\nSubstituting $b = A x$ into the expression for the residual, we get:\n$$r = A x - A \\hat{x} = A(x - \\hat{x}) = A e$$\nSince the matrix $A$ is invertible, we can express the error $e$ in terms of the residual $r$:\n$$e = A^{-1} r$$\nTaking the Euclidean norm of both sides and applying the property of induced matrix norms, $\\|Mv\\|_{2} \\le \\|M\\|_{2}\\|v\\|_{2}$, we obtain a bound on the absolute forward error:\n$$\\|e\\|_{2} = \\|x - \\hat{x}\\|_{2} = \\|A^{-1} r\\|_{2} \\le \\|A^{-1}\\|_{2} \\|r\\|_{2}$$\nThis gives an upper bound on the norm of the error vector. To find a bound for the relative error, we need to find a lower bound for $\\|x\\|_{2}$.\nFrom the original equation $A x = b$, we take the norm of both sides:\n$$\\|b\\|_{2} = \\|A x\\|_{2}$$\nUsing the same property of induced matrix norms, we have:\n$$\\|b\\|_{2} \\le \\|A\\|_{2} \\|x\\|_{2}$$\nSince $A$ is invertible and $b \\neq 0$, the solution $x$ is non-zero, so $\\|x\\|_{2}  0$. We can rearrange the inequality to obtain a lower bound on $\\|x\\|_{2}$:\n$$\\|x\\|_{2} \\ge \\frac{\\|b\\|_{2}}{\\|A\\|_{2}}$$\nTaking the reciprocal, we get an upper bound for $\\dfrac{1}{\\|x\\|_{2}}$:\n$$\\frac{1}{\\|x\\|_{2}} \\le \\frac{\\|A\\|_{2}}{\\|b\\|_{2}}$$\nNow, we can combine the bounds for $\\|x - \\hat{x}\\|_{2}$ and $\\dfrac{1}{\\|x\\|_{2}}$ to bound the relative forward error:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le (\\|A^{-1}\\|_{2} \\|r\\|_{2}) \\left( \\frac{\\|A\\|_{2}}{\\|b\\|_{2}} \\right)$$\nRearranging the terms, we get:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le \\|A\\|_{2} \\|A^{-1}\\|_{2} \\frac{\\|r\\|_{2}}{\\|b\\|_{2}}$$\nBy definition, the condition number of $A$ in the spectral norm is $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. Substituting this into the inequality gives the desired a priori upper bound:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le \\kappa_{2}(A) \\frac{\\|r\\|_{2}}{\\|b\\|_{2}}$$\n\n### Step 3: Numerical Evaluation of the Bound\n\nTo evaluate this bound, we must compute the values of $\\kappa_{2}(A)$, $\\|b\\|_{2}$, and $\\|r\\|_{2}$.\n\n**1. Calculation of $\\kappa_{2}(A)$:**\nThe matrix $A$ is a diagonal matrix: $A = \\mathrm{diag}(1, 10, 100)$. For a symmetric matrix (which includes diagonal matrices), the spectral norm $\\|A\\|_{2}$ is equal to its spectral radius, which is the maximum absolute value of its eigenvalues. The eigenvalues of a diagonal matrix are its diagonal entries.\nThe eigenvalues of $A$ are $\\lambda_1 = 1$, $\\lambda_2 = 10$, and $\\lambda_3 = 100$.\nTherefore, the spectral norm of $A$ is:\n$$\\|A\\|_{2} = \\max\\{|1|, |10|, |100|\\} = 100$$\nThe inverse of $A$ is $A^{-1} = \\mathrm{diag}(1^{-1}, 10^{-1}, 100^{-1}) = \\mathrm{diag}(1, 0.1, 0.01)$.\nThe eigenvalues of $A^{-1}$ are $1$, $0.1$, and $0.01$.\nThe spectral norm of $A^{-1}$ is:\n$$\\|A^{-1}\\|_{2} = \\max\\{|1|, |0.1|, |0.01|\\} = 1$$\nThe condition number $\\kappa_{2}(A)$ is the product of these norms:\n$$\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = 100 \\times 1 = 100$$\nAlternatively, for a symmetric positive definite matrix, $\\kappa_{2}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{100}{1} = 100$.\n\n**2. Calculation of $\\|b\\|_{2}$:**\nThe vector $b$ is given as $b = \\begin{pmatrix} 6 \\\\ 8 \\\\ 0 \\end{pmatrix}$. Its Euclidean norm is:\n$$\\|b\\|_{2} = \\sqrt{6^2 + 8^2 + 0^2} = \\sqrt{36 + 64} = \\sqrt{100} = 10$$\n\n**3. Calculation of $\\|r\\|_{2}$:**\nThe residual vector $r$ is given as $r = \\begin{pmatrix} 1 \\times 10^{-6} \\\\ -2 \\times 10^{-6} \\\\ 2 \\times 10^{-6} \\end{pmatrix}$. Its Euclidean norm is:\n$$\\|r\\|_{2} = \\sqrt{(1 \\times 10^{-6})^2 + (-2 \\times 10^{-6})^2 + (2 \\times 10^{-6})^2}$$\n$$\\|r\\|_{2} = \\sqrt{1 \\times 10^{-12} + 4 \\times 10^{-12} + 4 \\times 10^{-12}} = \\sqrt{9 \\times 10^{-12}} = 3 \\times 10^{-6}$$\n\n**4. Final Calculation of the Bound:**\nSubstituting the computed values into the derived inequality:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le \\kappa_{2}(A) \\frac{\\|r\\|_{2}}{\\|b\\|_{2}} = 100 \\times \\frac{3 \\times 10^{-6}}{10}$$\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le 10 \\times (3 \\times 10^{-6}) = 3 \\times 10^{-5}$$\nThe problem requires the numerical bound to be rounded to four significant figures.\n$$3 \\times 10^{-5} = 3.000 \\times 10^{-5}$$\nThus, the upper bound for the relative forward error is $3.000 \\times 10^{-5}$.",
            "answer": "$$\n\\boxed{3.000 \\times 10^{-5}}\n$$"
        },
        {
            "introduction": "Gaussian elimination with partial pivoting (GEPP) is a cornerstone of numerical linear algebra, but its stability is not unconditional. Its backward stability is governed by the 'growth factor,' a measure of how large intermediate values become during the elimination process. This comprehensive practice guides you through both theoretical derivation and practical implementation to explore the relationship between growth factor, backward error, and the ultimate forward error, revealing the important distinction between worst-case theoretical bounds and typical performance. ",
            "id": "3581484",
            "problem": "Consider Gaussian elimination with partial pivoting (GEPP) applied to a nonsingular square matrix of size $n \\times n$. Let $\\rho$ denote the growth factor as the ratio between the maximum absolute entry generated during the elimination process and the maximum absolute entry of the original matrix. The following tasks focus on establishing fundamental relationships among backward stability, forward error, condition number, and growth factor, and on constructing explicit matrices that produce large growth factors yet exhibit backward stability in double precision arithmetic.\n\nTask 1 (Principle derivation). Starting from the definitions of backward error and forward error and the standard floating-point model $\\mathrm{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$ for any basic arithmetic operation $\\circ$ and unit roundoff $u$, derive a normwise backward error bound for Gaussian elimination with partial pivoting (GEPP). Specifically, show that the computed solution $\\widehat{x}$ satisfies $(A + \\Delta A)\\widehat{x} = b$ where the perturbation $\\Delta A$ obeys a bound of the form\n$$\n\\frac{\\|\\Delta A\\|}{\\|A\\|} \\le c\\, n\\, \\rho\\, u + \\mathcal{O}(u^2),\n$$\nwhere $c$ is a moderate constant independent of $A$, $n$ is the dimension, and $\\rho$ is the growth factor produced by the elimination. Then use the fact that, for an invertible $A$, the forward error can be related to the backward error via the condition number $\\kappa$ in a submultiplicative matrix norm to obtain a bound of the form\n$$\n\\frac{\\|\\widehat{x} - x\\|}{\\|x\\|} \\le \\frac{\\kappa(A) \\frac{\\|\\Delta A\\|}{\\|A\\|}}{1 - \\kappa(A) \\frac{\\|\\Delta A\\|}{\\|A\\|}},\n$$\nwhere $x$ is the exact solution to $A x = b$ and $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$.\n\nTask 2 (Worst-case construction). Construct, for a given $n$, a matrix $W_n$ with entries\n$$\n(W_n)_{ij} = \n\\begin{cases}\n1,  j \\ge i, \\\\\n-1,  j  i,\n\\end{cases}\n$$\nand a right-hand side $b = \\mathbf{1}$ (the vector of all ones). This matrix is known to cause near worst-case growth factor under GEPP. Implement GEPP to compute an approximate solution $\\widehat{x}$ in double precision and track the growth factor $\\rho$ as the maximum absolute entry observed during elimination divided by the maximum absolute entry in the original matrix. Also compute the residual $r = b - A\\widehat{x}$, the normwise relative backward error\n$$\n\\beta = \\frac{\\|r\\|}{\\|A\\| \\|\\widehat{x}\\| + \\|b\\|},\n$$\nand estimate the forward error bound by substituting the bound $\\|\\Delta A\\| / \\|A\\| \\le c n \\rho u$ into the inequality from Task 1. Use the infinity norm for all norms.\n\nTask 3 (Quantification of influence). Based on Task 1 and Task 2, quantify how the growth factor $\\rho$ influences the forward error bound through the term $c n \\rho u$ and the condition number $\\kappa(A)$. Report, for each test case, the observed growth factor $\\rho$, the theoretical upper bound $2^{n-1}$, the ratio $\\rho / 2^{n-1}$, the actual relative backward error $\\beta$, the bound proxy $c n \\rho u$, and the resulting forward error bound obtained from Task 1. Take $c = 3$ and $u$ equal to the double-precision unit roundoff.\n\nImplementation requirements. Your program must implement GEPP explicitly to:\n- Perform row swaps according to partial pivoting.\n- Accumulate and report the observed growth factor $\\rho$.\n- Solve the linear system via the computed LU factors.\n- Compute the residual $r$, the infinity norms $\\|A\\|_{\\infty}$, $\\|A^{-1}\\|_{\\infty}$, and the condition number $\\kappa_{\\infty}(A)$.\n- Compute the quantities requested above.\n\nTest suite. Run the program on the following matrix sizes:\n- $n = 2$ (boundary case),\n- $n = 5$ (small case),\n- $n = 10$ (moderate case),\n- $n = 15$ (larger case),\n- $n = 20$ (stress case).\n\nFor each $n$, use the $W_n$ construction and $b = \\mathbf{1}$. All reported values should be computed in double precision floating-point arithmetic. No physical units or angles are involved.\n\nFinal output format. Your program should produce a single line of output containing a comma-separated list of results for the test suite, enclosed in square brackets. Each test-case result must be a list of six floating-point numbers in the order\n$$\n[\\rho,\\;2^{n-1},\\;\\rho/2^{n-1},\\;\\beta,\\;cn\\rho u,\\;\\text{forward\\_bound}],\n$$\nso the overall output is a list of lists, for example\n$$\n\\big[ [\\cdots], [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\big].\n$$",
            "solution": "This problem requires a theoretical derivation of error bounds for Gaussian elimination with partial pivoting (GEPP) and a numerical implementation to study the behavior of a specific class of matrices. The problem is well-posed and scientifically grounded in the principles of numerical linear algebra. A minor note is that the characterization of the matrix $W_n$ as causing \"near worst-case growth factor\" is inaccurate; for the specified structure, the growth factor $\\rho$ remains small (specifically, $\\rho=2$ for $n1$). However, this does not invalidate the computational task, which is to construct this matrix, observe its actual growth factor, and analyze the resulting errors, thereby providing a valuable comparison between a specific case and the theoretical worst-case bounds.\n\n### Task 1: Principle Derivation\n\nWe first derive the normwise backward and forward error bounds for the solution of a linear system $Ax=b$ obtained via GEPP.\n\n**Normwise Backward Error Bound**\n\nThe process of solving $Ax=b$ using GEPP in floating-point arithmetic can be viewed as a sequence of steps, each introducing roundoff error. The main steps are the LU factorization of a permuted matrix $PA$ and the solution of two triangular systems.\n\n1.  **LU Factorization:** The GEPP algorithm computes triangular factors $\\widehat{L}$ and $\\widehat{U}$ and a permutation matrix $P$ such that they are the exact factors of a perturbed matrix:\n    $$ \\widehat{L}\\widehat{U} = PA + E $$\n    where $E$ is the error matrix resulting from roundoff in the elimination steps. A cornerstone result in the backward error analysis of Gaussian elimination, established by Wilkinson, provides a bound on the magnitude of the entries of $E$. In the standard model of floating-point arithmetic where $fl(a \\circ b) = (a \\circ b)(1+\\delta)$ for $|\\delta| \\le u$ (unit roundoff), the error matrix $E$ can be bounded. A common form of this bound is $|E| \\le \\gamma_n |\\widehat{L}||\\widehat{U}|$, where $|M|$ denotes the matrix of absolute values of the entries of $M$, and $\\gamma_n = \\frac{nu}{1-nu}$.\n\n2.  **Triangular Solves:** The solution $\\widehat{x}$ is obtained by solving $\\widehat{L}y=Pb$ (forward substitution) and $\\widehat{U}\\widehat{x}=y$ (backward substitution). These steps also introduce roundoff errors. The computed solution $\\widehat{x}$ can be shown to be the exact solution to a perturbed triangular system $(\\widehat{U}+\\Delta\\widehat{U})\\widehat{x} = y$.\n\n3.  **Combined Effect:** When all sources of error are combined, it can be proven that the computed solution $\\widehat{x}$ is the exact solution of a perturbed original system:\n    $$ (A+\\Delta A)\\widehat{x} = b $$\n    The norm of the perturbation matrix $\\Delta A$ is bounded. This bound depends on the properties of the computed factors. Partial pivoting ensures that all entries of the computed lower triangular factor $\\widehat{L}$ satisfy $|\\widehat{l}_{ij}| \\le 1$. The magnitude of the entries of the upper triangular factor $\\widehat{U}$ is related to the **growth factor** $\\rho$, defined as:\n    $$ \\rho = \\frac{\\max_{i,j,k} |a_{ij}^{(k)}|}{\\max_{i,j} |a_{ij}^{(1)}|} $$\n    where $a_{ij}^{(k)}$ are the entries of the matrix after $k-1$ steps of elimination. Specifically, $\\max_{i,j}|\\widehat{u}_{ij}| \\le \\rho \\max_{i,j}|a_{ij}|$.\n\n4.  **The Bound:** A full derivation consolidates the errors from factorization and substitution to bound $\\|\\Delta A\\|$. A classic result gives a bound like $\\|\\Delta A\\|_{\\infty} \\le f(n) \\rho \\|A\\|_{\\infty} u + \\mathcal{O}(u^2)$, where $f(n)$ is a low-degree polynomial in $n$ (e.g., $f(n) \\approx 3n^2$). The problem statement posits a simplified bound:\n    $$ \\frac{\\|\\Delta A\\|}{\\|A\\|} \\le c\\, n\\, \\rho\\, u + \\mathcal{O}(u^2) $$\n    where $c$ is a moderate constant. This form captures the linear dependence on the growth factor $\\rho$ and unit roundoff $u$, and a simplified linear dependence on the dimension $n$. We accept this form as given for the problem. This inequality states that GEPP is normwise backward stable as long as the growth factor $\\rho$ is not excessively large.\n\n**Normwise Forward Error Bound**\n\nGiven the backward error relationship, we can derive a bound on the forward error $\\frac{\\|\\widehat{x}-x\\|}{\\|x\\|}$.\n\n1.  Let $x$ be the exact solution to $Ax=b$ and $\\widehat{x}$ be the computed solution satisfying $(A+\\Delta A)\\widehat{x}=b$. We assume $A$ is invertible.\n2.  From $(A+\\Delta A)\\widehat{x}=b$, we have $A\\widehat{x} + \\Delta A \\widehat{x} = b$. Substituting $b=Ax$, we get $A\\widehat{x} + \\Delta A \\widehat{x} = Ax$.\n3.  Rearranging gives $A(\\widehat{x}-x) = -\\Delta A \\widehat{x}$. Pre-multiplying by $A^{-1}$ yields the error equation:\n    $$ \\widehat{x}-x = -A^{-1} \\Delta A \\widehat{x} $$\n4.  To express the error relative to $\\|x\\|$, we substitute $\\widehat{x} = x + (\\widehat{x}-x)$ on the right-hand side:\n    $$ \\widehat{x}-x = -A^{-1} \\Delta A (x + (\\widehat{x}-x)) $$\n5.  We then solve for the error vector $\\widehat{x}-x$:\n    $$ (\\widehat{x}-x) + A^{-1} \\Delta A (\\widehat{x}-x) = -A^{-1} \\Delta A x $$\n    $$ (I + A^{-1} \\Delta A)(\\widehat{x}-x) = -A^{-1} \\Delta A x $$\n6.  If the perturbation is small enough such that $\\|A^{-1}\\Delta A\\|  1$, the matrix $(I + A^{-1}\\Delta A)$ is invertible. We can write:\n    $$ \\widehat{x}-x = -(I + A^{-1} \\Delta A)^{-1} A^{-1} \\Delta A x $$\n7.  Taking a submultiplicative matrix norm on both sides:\n    $$ \\|\\widehat{x}-x\\| \\le \\|(I + A^{-1} \\Delta A)^{-1}\\| \\|A^{-1}\\| \\|\\Delta A\\| \\|x\\| $$\n8.  Using the property $\\|(I+B)^{-1}\\| \\le \\frac{1}{1-\\|B\\|}$ for $\\|B\\|1$ (which follows from the Neumann series), we get:\n    $$ \\|\\widehat{x}-x\\| \\le \\frac{1}{1 - \\|A^{-1}\\Delta A\\|} \\|A^{-1}\\| \\|\\Delta A\\| \\|x\\| $$\n9.  Dividing by $\\|x\\|$ (assuming $x\\neq 0$):\n    $$ \\frac{\\|\\widehat{x}-x\\|}{\\|x\\|} \\le \\frac{\\|A^{-1}\\| \\|\\Delta A\\|}{1 - \\|A^{-1}\\| \\|\\Delta A\\|} $$\n10. The term $\\|A^{-1}\\| \\|\\Delta A\\|$ can be expressed using the condition number $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$ and the relative backward error $\\frac{\\|\\Delta A\\|}{\\|A\\|}$:\n    $$ \\|A^{-1}\\| \\|\\Delta A\\| = (\\|A\\|\\|A^{-1}\\|) \\left(\\frac{\\|\\Delta A\\|}{\\|A\\|}\\right) = \\kappa(A) \\frac{\\|\\Delta A\\|}{\\|A\\|} $$\n11. Substituting this into the inequality yields the final forward error bound:\n    $$ \\frac{\\|\\widehat{x} - x\\|}{\\|x\\|} \\le \\frac{\\kappa(A) \\frac{\\|\\Delta A\\|}{\\|A\\|}}{1 - \\kappa(A) \\frac{\\|\\Delta A\\|}{\\|A\\|}} $$\nThis crucial result shows that the relative forward error is controlled by the product of the condition number and the relative backward error. A small backward error (guaranteed by GEPP if $\\rho$ is small) does not guarantee a small forward error if the matrix is ill-conditioned (i.e., $\\kappa(A)$ is large).\n\n### Tasks 2 and 3: Implementation and Quantification\n\nThe following Python code implements GEPP to solve the system $W_n x = \\mathbf{1}$ for the specified matrix $W_n$. It calculates the growth factor $\\rho$, backward error $\\beta$, and the theoretical forward error bound for the given test cases.",
            "answer": "```python\nimport numpy as np\n\ndef gepp(A, b):\n    \"\"\"\n    Solves the linear system Ax=b using Gaussian elimination with partial pivoting.\n    \n    Args:\n        A (np.ndarray): The n x n coefficient matrix.\n        b (np.ndarray): The n x 1 right-hand side vector.\n        \n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The solution vector x.\n            - float: The observed growth factor rho.\n    \"\"\"\n    n = A.shape[0]\n    # Work on copies to preserve original matrices\n    A_copy = A.astype(np.float64)\n    b_copy = b.astype(np.float64)\n\n    max_abs_A = np.max(np.abs(A_copy))\n    if max_abs_A == 0:\n        # A is a zero matrix, handle as a special case\n        rho = 1.0 # Or undefined. In this problem, A is nonsingular.\n    else:\n        # Keep track of the maximum absolute value seen during elimination\n        max_abs_val_during_elimination = max_abs_A\n\n    # Elimination phase\n    for k in range(n - 1):\n        # Find pivot row (the one with the largest element in the current column)\n        pivot_row_idx = k + np.argmax(np.abs(A_copy[k:, k]))\n        \n        # Swap rows in A and b if necessary\n        if pivot_row_idx != k:\n            A_copy[[k, pivot_row_idx]] = A_copy[[pivot_row_idx, k]]\n            b_copy[[k, pivot_row_idx]] = b_copy[[pivot_row_idx, k]]\n            \n        # Check for singularity\n        if A_copy[k, k] == 0:\n            raise np.linalg.LinAlgError(\"Matrix is singular or near-singular.\")\n\n        # Update rows below the pivot\n        for i in range(k + 1, n):\n            multiplier = A_copy[i, k] / A_copy[k, k]\n            # Store multiplier in the lower-triangular part of A\n            A_copy[i, k] = multiplier\n            # Update the rest of the row\n            A_copy[i, k + 1:] -= multiplier * A_copy[k, k + 1:]\n        \n        # Update the maximum absolute value for growth factor calculation.\n        # This is a key step for tracking rho.\n        current_max = np.max(np.abs(A_copy))\n        if current_max  max_abs_val_during_elimination:\n            max_abs_val_during_elimination = current_max\n\n    if max_abs_A  0:\n        rho = max_abs_val_during_elimination / max_abs_A\n    \n    # At this point, A_copy contains U in its upper triangle and L (excluding\n    # the main diagonal of 1s) in its strict lower triangle.\n\n    # Forward substitution to solve Ly = b\n    y = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        y[i] = b_copy[i] - np.dot(A_copy[i, :i], y[:i])\n        \n    # Backward substitution to solve Ux = y\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (y[i] - np.dot(A_copy[i, i + 1:], x[i + 1:])) / A_copy[i, i]\n        \n    return x, rho\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the required quantities.\n    \"\"\"\n    test_cases = [2, 5, 10, 15, 20]\n    all_results_str = []\n    \n    # Unit roundoff for double precision (IEEE 754)\n    u = np.finfo(np.float64).eps / 2.0\n    c = 3.0\n    \n    for n in test_cases:\n        # Task 2: Construct the matrix W_n and vector b\n        W_n = np.ones((n, n), dtype=np.float64)\n        # Set j  i entries to -1\n        rows, cols = np.tril_indices(n, k=-1)\n        W_n[rows, cols] = -1.0\n        \n        b = np.ones(n, dtype=np.float64)\n        \n        # Perform GEPP and get solution and growth factor\n        x_hat, rho = gepp(W_n, b)\n        \n        # Task 3: Quantify influence\n        \n        # 1. Observed growth factor rho (already computed)\n        \n        # 2. Theoretical upper bound for rho\n        rho_bound = 2.0**(n - 1)\n        \n        # 3. Ratio rho / 2^(n-1)\n        rho_ratio = rho / rho_bound\n        \n        # 4. Actual relative backward error beta\n        r = b - W_n @ x_hat\n        norm_A_inf = np.linalg.norm(W_n, ord=np.inf)\n        norm_x_hat_inf = np.linalg.norm(x_hat, ord=np.inf)\n        norm_b_inf = np.linalg.norm(b, ord=np.inf)\n        norm_r_inf = np.linalg.norm(r, ord=np.inf)\n        \n        denominator_beta = norm_A_inf * norm_x_hat_inf + norm_b_inf\n        beta = norm_r_inf / denominator_beta if denominator_beta  0 else 0.0\n\n        # 5. Bound proxy c*n*rho*u\n        bound_proxy = c * n * rho * u\n        \n        # 6. Resulting forward error bound\n        # First, compute condition number kappa_inf(A)\n        # Note: In a real application, one would use a condition number estimator\n        # instead of computing the inverse explicitly. For this problem, it's fine.\n        norm_A_inv_inf = np.linalg.norm(np.linalg.inv(W_n), ord=np.inf)\n        kappa_inf = norm_A_inf * norm_A_inv_inf\n        \n        # Use the bound from Task 1\n        rel_back_err_term = kappa_inf * bound_proxy\n        \n        if rel_back_err_term = 1.0:\n            forward_bound = np.inf\n        else:\n            forward_bound = rel_back_err_term / (1.0 - rel_back_err_term)\n            \n        case_results = [\n            rho, \n            rho_bound, \n            rho_ratio, \n            beta, \n            bound_proxy, \n            forward_bound\n        ]\n        all_results_str.append(str(case_results))\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For large-scale eigenvalue problems, iterative methods like the symmetric Lanczos process are indispensable. In exact arithmetic, the algorithm generates a perfectly orthonormal basis for the Krylov subspace, but this property is quickly lost in the presence of floating-point roundoff, leading to inaccurate results. This implementation-focused practice challenges you to combat this instability by implementing and evaluating a selective reorthogonalization strategy, offering a practical lesson in the trade-offs between computational cost and numerical stability. ",
            "id": "3581514",
            "problem": "Design and implement a program that investigates the stability of the symmetric Lanczos process under finite precision arithmetic by applying a selective reorthogonalization strategy to maintain orthogonality among Krylov basis vectors and quantifying its effect on the accuracy of Ritz values. Your program must implement three variants of the Lanczos iteration for real symmetric matrices: no reorthogonalization, selective reorthogonalization, and full reorthogonalization. The empirical comparison must adhere to the following specifications.\n\nFundamental base and assumptions:\n- Use the standard floating-point model of rounding for binary64 arithmetic: for any basic operation, $\\mathrm{fl}(x \\circ y) = (x \\circ y) (1 + \\delta)$ with $|\\delta| \\leq \\epsilon_{\\mathrm{mach}}$, where $\\epsilon_{\\mathrm{mach}}$ denotes machine epsilon for double precision.\n- The Krylov subspace of order $k$ for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ is $\\mathcal{K}_{k}(A, v_{1}) = \\mathrm{span}\\{v_{1}, A v_{1}, \\dots, A^{k-1} v_{1} \\}$. The Lanczos process attempts to generate an orthonormal basis $\\{v_1, \\dots, v_k\\}$ for $\\mathcal{K}_{k}(A, v_{1})$, producing a real symmetric tridiagonal matrix $T_{k} \\in \\mathbb{R}^{k \\times k}$ whose eigenvalues are the Ritz values with respect to $(A, \\mathcal{K}_{k})$.\n- In exact arithmetic, the Lanczos vectors are orthonormal and a $3$-term recurrence suffices. In finite precision arithmetic, loss of orthogonality may occur; reorthogonalization can mitigate this instability.\n\nSelective reorthogonalization directive:\n- Implement a selective reorthogonalization criterion based on measured loss of orthogonality. Let $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ with $c = 10$. At Lanczos step $j$, after forming the candidate vector $w$, compute inner products $h_{i} = v_{i}^{\\top} w$ for $i = 1, \\dots, j$. If $|h_{i}| > \\eta$ for any $i$, perform modified Gram–Schmidt reorthogonalization against those $v_{i}$ for which the inequality holds, and repeat this check at most $2$ passes to reduce $|h_{i}|$ back near $\\mathcal{O}(\\epsilon_{\\mathrm{mach}})$.\n- For the full reorthogonalization variant, at each step $j$ perform modified Gram–Schmidt reorthogonalization against all previously computed Lanczos vectors $\\{v_1, \\dots, v_j\\}$, performing at most $2$ passes.\n- For the no reorthogonalization variant, perform only the $3$-term recurrence without any Gram–Schmidt steps.\n\nAccuracy metric:\n- For a given symmetric matrix $A$, a target subspace dimension $k$, and a target count $r \\leq k$, compute the $r$ largest Ritz values from $T_{k}$ (in algebraic order) and compare them against the $r$ largest true eigenvalues of $A$ (in algebraic order). Define the error as the maximum absolute deviation after sorting both sets in descending order:\n$$\nE = \\max_{1 \\leq i \\leq r} \\left| \\lambda^{(A)}_{(i)} - \\theta^{(T)}_{(i)} \\right|\n$$\nwhere $\\lambda^{(A)}_{(i)}$ are the $r$ largest eigenvalues of $A$ and $\\theta^{(T)}_{(i)}$ are the $r$ largest Ritz values.\n- Define the improvement factor due to selective reorthogonalization as\n$$\n\\mathcal{I} = \\frac{E_{\\mathrm{none}}}{E_{\\mathrm{sel}}}\n$$\nwhere $E_{\\mathrm{none}}$ is the error under no reorthogonalization and $E_{\\mathrm{sel}}$ is the error under selective reorthogonalization. If $E_{\\mathrm{sel}} = 0$, report $\\mathcal{I} = 10^{16}$.\n\nImplementation requirements:\n- Implement the symmetric Lanczos process that returns the tridiagonal $T_{k}$ for each of the three variants described above.\n- Use a fixed, reproducible random starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ for each test case.\n- Compute true eigenvalues of $A$ using a numerically stable symmetric eigensolver.\n\nTest suite:\n- Your program must run the following three test cases and report the improvement factor $\\mathcal{I}$ for each case as defined above. In all cases, use double precision arithmetic, no physical units are involved, and no angles are used.\n    1. Happy path with well-separated spectrum:\n        - $n = 80$, $k = 30$, $r = 5$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda = \\mathrm{diag}(\\ell_{1}, \\dots, \\ell_{n})$ with $\\ell_{i}$ linearly spaced from $1$ to $100$, and $Q$ is the orthogonal factor from the $\\mathrm{QR}$ factorization of a random Gaussian matrix with seed $0$.\n        - Use a random Gaussian starting vector with seed $1$, normalized to unit $2$-norm.\n    2. Challenging cluster near the top of the spectrum:\n        - $n = 120$, $k = 60$, $r = 8$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda$ has $10$ eigenvalues near $10$ given by $10 + \\delta_{i}$ with $\\delta_{i}$ i.i.d. uniformly sampled in $[-10^{-8}, 10^{-8}]$, and the remaining $110$ eigenvalues linearly spaced in $[0.1, 9.9]$. Use seed $2$ for generating $Q$ and for the uniform samples.\n        - Use a random Gaussian starting vector with seed $3$, normalized to unit $2$-norm.\n    3. Boundary case with long run and structured operator:\n        - $n = 90$, $k = 90$, $r = 10$.\n        - Construct $A$ as the tridiagonal Toeplitz matrix with zeros on the diagonal and ones on the first sub- and super-diagonals, i.e., $A_{i,i} = 0$ and $A_{i,i+1} = A_{i+1,i} = 1$ for $i = 1, \\dots, n-1$.\n        - Use a random Gaussian starting vector with seed $4$, normalized to unit $2$-norm.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly $3$ floating-point numbers corresponding to the improvement factors $\\mathcal{I}$ for the test cases $1$, $2$, and $3$ in this order. For example, the format must be exactly like $[x_1,x_2,x_3]$ with no extra spaces, where $x_i$ are decimal strings.\n\nScoring and acceptance criteria:\n- The program must be self-contained and runnable without user input.\n- The selective reorthogonalization must use the threshold $\\eta = 10 \\sqrt{\\epsilon_{\\mathrm{mach}}}$ and at most $2$ Gram–Schmidt passes per step.\n- Each $\\mathcal{I}$ must be a finite nonnegative float, using the rule $\\mathcal{I} = 10^{16}$ if $E_{\\mathrm{sel}} = 0$.",
            "solution": "The user requests an implementation and analysis of the symmetric Lanczos process, focusing on numerical stability. Specifically, the task is to compare the accuracy of Ritz values computed with no reorthogonalization against those computed using a selective reorthogonalization strategy. The comparison is quantified by an improvement factor, $\\mathcal{I}$. The problem is well-defined, scientifically sound, and provides all necessary parameters, including matrix constructions, algorithm specifications, and evaluation metrics, making it a valid and verifiable numerical experiment.\n\nI will structure the solution by first implementing the core Lanczos iteration, which can operate in two modes: `none` (no reorthogonalization) and `selective`. Then, I will create helper functions to construct the specific matrices and starting vectors for each of the three test cases. A main function will orchestrate the execution for each test case, compute the required errors and improvement factors, and format the output as specified.\n\n### The Symmetric Lanczos Algorithm\n\nThe Lanczos process generates an orthonormal basis $\\{v_1, v_2, \\dots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_k(A, v_1)$ and a symmetric tridiagonal matrix $T_k$. The recurrence relation is:\n$$ \\beta_j v_{j+1} = A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1} $$\nwhere $\\alpha_j = v_j^\\top A v_j$ and $\\beta_j = \\|A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1}\\|_2$. In finite precision arithmetic, the orthogonality of the vectors $\\{v_j\\}$ is quickly lost, leading to inaccuracies in the Ritz values (eigenvalues of $T_k$).\n\n### Reorthogonalization Strategies\n\nTo counteract this instability, reorthogonalization is employed.\n1.  **No Reorthogonalization (`none`)**: The basic recurrence is used without any correction. This is cheap but numerically unstable for long iterations.\n2.  **Selective Reorthogonalization (`selective`)**: Orthogonality is selectively enforced. At each step $j$, the new vector $w_j = A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1}$ (which should be orthogonal to $v_1, \\dots, v_j$) is checked. If its projection onto any previous vector $v_i$ is larger than a threshold $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ (with $c=10$), we reorthogonalize $w_j$ against those specific vectors. The problem specifies using Modified Gram-Schmidt (MGS) for this correction, repeated for at most two passes to ensure orthogonality is restored. This provides a balance between the cost of full reorthogonalization and the instability of no reorthogonalization.\n\n### Implementation Details\n\nA function `lanczos_iteration(A, v1, k, mode)` will be implemented. It will take the matrix $A$, starting vector $v_1$, number of iterations $k$, and a `mode` string (`'none'` or `'selective'`).\n\n-   The loop will iterate $j$ from $0$ to $k-1$.\n-   Inside the loop, for each new vector, the three-term recurrence will be applied.\n-   If `mode` is `'selective'`, the reorthogonalization logic will be triggered. This involves computing inner products $h_i = v_i^\\top w$ and, if $|h_i|  \\eta$, applying MGS steps: $w \\leftarrow w - (v_i^\\top w)v_i$. This check-and-correct procedure is done in up to two passes.\n-   The function returns the $k \\times k$ tridiagonal matrix $T_k$.\n\n### Evaluation\n\nFor each test case:\n1.  The specified $n \\times n$ matrix $A$ and starting vector $v_1$ are constructed.\n2.  The true $r$ largest eigenvalues of $A$, denoted $\\lambda^{(A)}_{(i)}$, are computed using a stable eigensolver (`numpy.linalg.eigh`).\n3.  The Lanczos iteration is run for both `mode='none'` and `mode='selective'` to obtain $T_{k, \\text{none}}$ and $T_{k, \\text{sel}}$.\n4.  The $r$ largest eigenvalues (Ritz values) of each $T_k$ matrix, denoted $\\theta^{(T)}_{(i)}$, are computed using an efficient tridiagonal eigensolver (`scipy.linalg.eigh_tridiagonal`).\n5.  The errors $E_{\\mathrm{none}}$ and $E_{\\mathrm{sel}}$ are calculated as the maximum absolute difference between the corresponding sorted lists of true eigenvalues and Ritz values:\n    $$ E = \\max_{1 \\leq i \\leq r} \\left| \\lambda^{(A)}_{(i)} - \\theta^{(T)}_{(i)} \\right| $$\n6.  The improvement factor $\\mathcal{I} = E_{\\mathrm{none}} / E_{\\mathrm{sel}}$ is computed. The special case where $E_{\\mathrm{sel}}=0$ is handled by setting $\\mathcal{I}=10^{16}$.\n\nThis process is repeated for the three defined test cases, and the resulting improvement factors are collected and printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef lanczos_iteration(A, v1, k, mode='none'):\n    \"\"\"\n    Performs the symmetric Lanczos iteration for a matrix A and starting vector v1.\n\n    Args:\n        A (np.ndarray): The symmetric matrix.\n        v1 (np.ndarray): The starting vector of unit norm.\n        k (int): The number of Lanczos steps (dimension of Krylov subspace).\n        mode (str): Reorthogonalization mode: 'none' or 'selective'.\n\n    Returns:\n        np.ndarray: The symmetric tridiagonal matrix T_k of size k x k.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k + 1))\n    alphas = np.zeros(k)\n    betas = np.zeros(k)\n\n    eps_mach = np.finfo(float).eps\n    eta = 10.0 * np.sqrt(eps_mach)\n    breakdown_tol = 1e-14\n\n    V[:, 0] = v1\n\n    for j in range(k):\n        v_curr = V[:, j]\n        w = A @ v_curr\n\n        if j  0:\n            w -= betas[j - 1] * V[:, j - 1]\n\n        alphas[j] = v_curr.T @ w\n        w -= alphas[j] * v_curr\n\n        if mode == 'selective':\n            # Perform selective reorthogonalization with at most 2 MGS passes\n            for _ in range(2):\n                # Projections of w onto the basis V\n                h = V[:, :j + 1].T @ w\n                indices_to_reortho = np.where(np.abs(h)  eta)[0]\n                \n                if len(indices_to_reortho) == 0:\n                    break  # Orthogonality is sufficient\n                \n                # Apply MGS steps for the vectors that lost orthogonality\n                for i in indices_to_reortho:\n                    w -= (V[:, i].T @ w) * V[:, i]\n\n        betas[j] = np.linalg.norm(w)\n\n        if betas[j]  breakdown_tol:\n            # Breakdown: Krylov subspace is invariant or exhausted.\n            k_actual = j + 1\n            T_k = np.zeros((k, k))\n            sub_T_alphas = alphas[:k_actual]\n            sub_T_betas = betas[:k_actual - 1]\n            T_k_sub = np.diag(sub_T_alphas) + np.diag(sub_T_betas, 1) + np.diag(sub_T_betas, -1)\n            T_k[:k_actual, :k_actual] = T_k_sub\n            return T_k\n            \n        V[:, j + 1] = w / betas[j]\n\n    T_k = np.diag(alphas) + np.diag(betas[:k - 1], 1) + np.diag(betas[:k - 1], -1)\n    return T_k\n\ndef run_test_case(case_id):\n    \"\"\"\n    Sets up and runs a single test case, returning the improvement factor.\n    \"\"\"\n    if case_id == 1:\n        n, k, r = 80, 30, 5\n        rng_Q = np.random.default_rng(0)\n        G = rng_Q.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        lambda_vals = np.linspace(1, 100, n)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(1)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 2:\n        n, k, r = 120, 60, 8\n        rng = np.random.default_rng(2)\n        cluster_vals = 10.0 + rng.uniform(-1e-8, 1e-8, size=10)\n        other_vals = np.linspace(0.1, 9.9, n - 10)\n        lambda_vals = np.concatenate((other_vals, cluster_vals))\n        G = rng.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(3)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 3:\n        n, k, r = 90, 90, 10\n        A = np.diag(np.ones(n - 1), 1) + np.diag(np.ones(n - 1), -1)\n        rng_v1 = np.random.default_rng(4)\n        v1 = rng_v1.standard_normal(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n        \n    v1 /= np.linalg.norm(v1)\n\n    # Compute true eigenvalues of A\n    true_eigvals = np.linalg.eigh(A)[0]\n    largest_true_eigvals = np.flip(true_eigvals[-r:]) # descending order\n\n    # Run Lanczos with no reorthogonalization\n    T_none = lanczos_iteration(A, v1, k, mode='none')\n    ritz_vals_none = eigh_tridiagonal(np.diag(T_none), np.diag(T_none, 1))[0]\n    largest_ritz_none = np.flip(ritz_vals_none[-r:])\n\n    # Run Lanczos with selective reorthogonalization\n    T_sel = lanczos_iteration(A, v1, k, mode='selective')\n    ritz_vals_sel = eigh_tridiagonal(np.diag(T_sel), np.diag(T_sel, 1))[0]\n    largest_ritz_sel = np.flip(ritz_vals_sel[-r:])\n\n    # Compute errors\n    E_none = np.max(np.abs(largest_true_eigvals - largest_ritz_none))\n    E_sel = np.max(np.abs(largest_true_eigvals - largest_ritz_sel))\n    \n    # Compute improvement factor\n    if E_sel == 0.0:\n        if E_none == 0.0:\n            return 1.0\n        return 1e16\n    \n    return E_none / E_sel\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # The problem statement defines the test cases.\n    # We simply iterate through them.\n    test_cases = [1, 2, 3]\n\n    results = []\n    for case_id in test_cases:\n        improvement_factor = run_test_case(case_id)\n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}