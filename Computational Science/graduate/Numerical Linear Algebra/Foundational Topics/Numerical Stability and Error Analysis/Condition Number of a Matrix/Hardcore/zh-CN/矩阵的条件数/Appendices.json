{
    "hands_on_practices": [
        {
            "introduction": "矩阵的条件数是衡量其求逆或求解线性方程组时数值敏感性的核心指标。本练习旨在通过一个简单的 $2 \\times 2$ 对称矩阵，将条件数的定义与奇异值直接联系起来，从而为理解条件数的几何意义——即矩阵对空间的最大拉伸与最小拉伸之比——提供一个具体的计算实例 。",
            "id": "1049315",
            "problem": "考虑对称的 $2 \\times 2$ 矩阵 $A = \\begin{bmatrix} 2  1 \\\\ 1  2 \\end{bmatrix}$。关于谱范数（2-范数）的矩阵求逆条件数定义为 $\\kappa(A) = \\|A\\|_2 \\cdot \\|A^{-1}\\|_2$，其中 $\\| \\cdot \\|_2$ 表示谱范数。这可以等价地用 $A$ 的奇异值 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 来表示。请通过确定 $A$ 的奇异值来计算 $\\kappa(A)$。",
            "solution": "1. 对称正定矩阵的谱范数等于其最大特征值，其逆矩阵的谱范数等于其最小特征值的倒数。\n2. 计算 \n$$A=\\begin{bmatrix} 2  1 \\\\ 1  2 \\end{bmatrix}$$ \n的特征值，通过求解 \n$$\\det(A-\\lambda I)=(2-\\lambda)^2-1=\\lambda^2-4\\lambda+3=0\\,. $$  \n3. 解方程 $\\lambda^2-4\\lambda+3=0$ 得到 \n$$\\lambda=\\frac{4\\pm\\sqrt{16-12}}{2}=\\{3,1\\}\\,. $$  \n4. 因此，奇异值为 $\\sigma_{\\max}=3$ 和 $\\sigma_{\\min}=1$，从而得出 \n$$\\kappa(A)=\\frac{\\sigma_{\\max}}{\\sigma_{\\min}}=\\frac{3}{1}=3\\,. $$",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "在掌握了条件数的基本计算后，我们进一步探究其行为特性，特别是矩阵结构如何影响其数值稳定性。本练习通过分析一个看似微小扰动的单位矩阵 $A(\\epsilon) = I + \\epsilon E_{12}$，揭示了非正规矩阵的一个重要特征：即使矩阵在元素上“接近”一个良态矩阵，其条件数也可能显著增大。通过推导条件数 $\\kappa_2(A(\\epsilon))$ 与扰动大小 $\\epsilon$ 的解析关系，您将深化对矩阵敏感性来源的理解 。",
            "id": "2210773",
            "problem": "考虑一个用于模拟二维物理过程的线性方程组 $A \\mathbf{x} = \\mathbf{b}$。理想情况下，该系统的行为由 $2 \\times 2$ 单位矩阵 $I_2$ 描述。然而，由于一个微小的制造缺陷，引入了一个弱交叉耦合项。现在，系统矩阵由 $A(\\epsilon) = I_2 + \\epsilon E_{12}$ 给出，其中 $\\epsilon$ 是一个表示缺陷大小的非负实数，而 $E_{12}$ 是在第一行第二列的元素为1，其余元素为零的矩阵。\n\n解 $\\mathbf{x}$ 对向量 $\\mathbf{b}$ 中扰动的敏感性由矩阵 $A(\\epsilon)$ 的条件数来表征。关于矩阵2-范数的条件数定义为 $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$。\n\n求出这个扰动矩阵的2-范数条件数 $\\kappa_2(A(\\epsilon))$。将你的答案表示为关于 $\\epsilon$ 的闭式解析表达式。",
            "solution": "我们已知 $A(\\epsilon) = I_{2} + \\epsilon E_{12} = \\begin{pmatrix} 1  \\epsilon \\\\ 0  1 \\end{pmatrix}$，其中 $\\epsilon \\geq 0$。2-范数条件数定义为 $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$。利用矩阵2-范数的奇异值表征，我们有 $\\|A\\|_{2} = \\sigma_{\\max}(A)$ 以及 $\\|A^{-1}\\|_{2} = \\sigma_{\\max}(A^{-1}) = 1/\\sigma_{\\min}(A)$，因此\n$$\n\\kappa_{2}(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\sqrt{\\frac{\\lambda_{\\max}(A^{T}A)}{\\lambda_{\\min}(A^{T}A)}},\n$$\n其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是 $A^{T}A$ 的最大和最小特征值。\n\n计算 $A^{T}A$：\n$$\nA^{T} = \\begin{pmatrix} 1  0 \\\\ \\epsilon  1 \\end{pmatrix}, \\quad\nA^{T}A = \\begin{pmatrix} 1  \\epsilon \\\\ \\epsilon  1 + \\epsilon^{2} \\end{pmatrix}.\n$$\n$A^{T}A$ 的特征值满足\n$$\n\\det\\!\\left(A^{T}A - \\lambda I\\right) = \\det\\!\\begin{pmatrix} 1 - \\lambda  \\epsilon \\\\ \\epsilon  1 + \\epsilon^{2} - \\lambda \\end{pmatrix} = (1 - \\lambda)(1 + \\epsilon^{2} - \\lambda) - \\epsilon^{2} = 0.\n$$\n展开得\n$$\n(1 - \\lambda)(1 + \\epsilon^{2} - \\lambda) - \\epsilon^{2} = 1 - (2 + \\epsilon^{2})\\lambda + \\lambda^{2} = 0,\n$$\n所以特征值为\n$$\n\\lambda_{\\pm} = \\frac{(2 + \\epsilon^{2}) \\pm \\sqrt{(2 + \\epsilon^{2})^{2} - 4}}{2} = \\frac{(2 + \\epsilon^{2}) \\pm \\epsilon \\sqrt{\\epsilon^{2} + 4}}{2}.\n$$\n因此，\n$$\n\\kappa_{2}(A(\\epsilon)) = \\sqrt{\\frac{\\lambda_{+}}{\\lambda_{-}}}\n= \\sqrt{\\frac{(2 + \\epsilon^{2}) + \\epsilon \\sqrt{\\epsilon^{2} + 4}}{(2 + \\epsilon^{2}) - \\epsilon \\sqrt{\\epsilon^{2} + 4}}}.\n$$\n为了简化，注意到\n$$\n(2 + \\epsilon^{2}) \\pm \\epsilon \\sqrt{\\epsilon^{2} + 4} = \\frac{\\left(\\sqrt{\\epsilon^{2} + 4} \\pm \\epsilon\\right)^{2}}{2},\n$$\n所以\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\sqrt{\\epsilon^{2} + 4} + \\epsilon}{\\sqrt{\\epsilon^{2} + 4} - \\epsilon} = \\frac{\\left(\\sqrt{\\epsilon^{2} + 4} + \\epsilon\\right)^{2}}{4} = 1 + \\frac{\\epsilon^{2}}{2} + \\frac{\\epsilon}{2}\\sqrt{\\epsilon^{2} + 4}.\n$$\n对于 $\\epsilon \\geq 0$，这些等价的闭式表达式中的任何一个都是有效的。",
            "answer": "$$\\boxed{1 + \\frac{\\epsilon^{2}}{2} + \\frac{\\epsilon}{2}\\sqrt{\\epsilon^{2} + 4}}$$"
        },
        {
            "introduction": "理论探讨最终要服务于解决实际问题，尤其是在处理源于物理测量的病态 (ill-posed) 逆问题时。这类问题的特征是其关联矩阵的奇异值快速衰减，导致条件数巨大，直接求解会极度放大测量噪声。本计算实践  引导您通过编程实现截断奇异值分解 (TSVD) 这一正则化方法，您将学习如何通过最小化预期误差来确定最佳截断点，从而在抑制噪声（方差）和保留信号（偏差）之间做出关键权衡。",
            "id": "3540130",
            "problem": "考虑一个离散线性逆问题，其中未知向量为 $x \\in \\mathbb{R}^n$，数据为 $b \\in \\mathbb{R}^n$，由 $b = A_h x + \\eta$ 建模。这里，$A_h \\in \\mathbb{R}^{n \\times n}$ 是一个紧前向算子的离散化，而 $\\eta \\in \\mathbb{R}^n$ 是加性测量噪声。假设 $A_h$ 具有奇异值分解（SVD）形式 $A_h = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma = \\operatorname{diag}(s_1, s_2, \\dots, s_n)$，其奇异值 $s_i$ 满足指数衰减模型 $s_i = e^{-\\alpha (i-1)}$，其中衰减参数 $\\alpha  0$ 为固定值，索引 $i \\in \\{1,2,\\dots,n\\}$。此外，将矩阵 $M$ 的谱条件数 $\\kappa_2(M)$ 定义为 $\\kappa_2(M) = \\lVert M \\rVert_2 \\lVert M^{-1} \\rVert_2$，其中 $\\lVert \\cdot \\rVert_2$ 表示由欧几里得范数诱导的算子范数。对于所述模型下的矩阵 $A_h$，其谱条件数仅取决于极端奇异值。\n\n为了在存在噪声的情况下稳定反演过程，考虑使用截断奇异值分解（TSVD）。对于选定的截断索引 $r \\in \\{0,1,\\dots,n\\}$，该方法仅保留前 $r$ 个奇异分量：TSVD 重构解 $x_r$ 是通过应用与截断对角阵 $\\Sigma_r = \\operatorname{diag}(s_1, s_2, \\dots, s_r)$ 相关联的伪逆得到的，并丢弃所有索引 $i  r$ 的分量。在此框架下，假设真实解 $x$ 在右奇异向量基下的系数呈指数衰减，即 $c_i = e^{-\\beta (i-1)}$，其中平滑度参数 $\\beta  0$ 为固定值，而 $c_i = v_i^\\top x$，$v_i$ 是 $V$ 的第 $i$ 列。测量噪声 $\\eta$ 被建模为一个随机向量，其分量独立、均值为零且具有公共方差 $\\sigma^2$，即 $\\mathbb{E}[\\eta] = 0$ 和 $\\operatorname{Cov}(\\eta) = \\sigma^2 I$，其中 $I$ 是单位矩阵。在这些假设下，选择 TSVD 截断索引 $r$ 必须在截断算子的条件数与反演过程中的噪声放大之间取得平衡。\n\n您的任务是编写一个完整的程序，对每个指定的测试用例，仅从给定的定义和建模假设出发，执行以下计算：\n\n- 计算完整矩阵 $A_h$ 的谱条件数 $\\kappa_2(A_h)$。\n- 确定 TSVD 截断索引 $r^\\star \\in \\{0,1,\\dots,n\\}$，该索引最小化在右奇异向量基下的期望平方重构误差，同时考虑前述关于奇异值、真实系数的指数模型以及关于 $\\eta$ 的白噪声模型。分析必须从 SVD 表示、奇异向量的正交性以及欧几里得范数和期望值的性质开始，不得使用任何未经论证的快捷公式。如果多个截断值达到相同的最小值，则选择最小的那个 $r^\\star$。\n- 计算在保留子空间上受限的截断算子的谱条件数，定义为 $\\kappa_2^{(r^\\star)} = s_1 / s_{r^\\star}$（如果 $r^\\star \\geq 1$）。对于边界情况 $r^\\star = 0$，按照惯例定义 $\\kappa_2^{(0)} = 0$，因为零算子不执行反演。\n- 将所选截断索引 $r^\\star$ 处的期望平方重构误差分解为两个非负贡献项：由保留分量上的噪声放大引起的方差项，以及由于截断尾部分量而产生的偏差项。分别报告所选 $r^\\star$ 的这两个贡献项的数值。\n\n程序必须为以下测试套件实现上述功能，参数以有序元组 $(n,\\alpha,\\beta,\\sigma)$ 的形式给出：\n\n- 测试用例 1：$(n,\\alpha,\\beta,\\sigma) = (50, 0.08, 0.04, 10^{-3})$。\n- 测试用例 2：$(n,\\alpha,\\beta,\\sigma) = (50, 0.08, 0.04, 0)$。\n- 测试用例 3：$(n,\\alpha,\\beta,\\sigma) = (80, 0.20, 0.05, 5 \\cdot 10^{-3})$。\n- 测试用例 4：$(n,\\alpha,\\beta,\\sigma) = (1, 0.10, 0.10, 10^{-2})$。\n- 测试用例 5：$(n,\\alpha,\\beta,\\sigma) = (30, 0.15, 0.20, 10^{-4})$。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素本身是包含五个值的列表，顺序为 $[\\kappa_2(A_h), r^\\star, \\kappa_2^{(r^\\star)}, \\text{variance}, \\text{bias}]$。类型必须如下：$\\kappa_2(A_h)$ 为浮点数，$r^\\star$ 为整数，$\\kappa_2^{(r^\\star)}$ 为浮点数，方差和偏差贡献项均为浮点数。例如，输出必须具有形式 $[[v_{11},v_{12},v_{13},v_{14},v_{15}],[v_{21},v_{22},v_{23},v_{24},v_{25}],\\dots]$，不含任何额外的解释性文本。",
            "solution": "用户提供的问题是数值线性代数中一个明确定义的任务，具体涉及使用截断奇异值分解（TSVD）方法对离散线性逆问题进行正则化。问题的核心是通过最小化期望均方误差来找到一个最优的正则化参数——即截断索引 $r$。解决方案需要推导该误差的解析表达式，将其分解为偏差和方差分量，然后实现一个数值程序，为一组给定的测试用例找到最优截断索引及相关量。所有模型和假设，包括奇异值和解系数的指数衰减，在此类问题的理论分析中都是标准的。\n\n所需各量的系统推导过程如下。\n\n首先，我们处理完整矩阵 $A_h$ 的谱条件数。可逆矩阵 $M$ 的谱条件数定义为 $\\kappa_2(M) = \\lVert M \\rVert_2 \\lVert M^{-1} \\rVert_2$。对于具有奇异值分解（SVD）$A_h = U\\Sigma V^\\top$ 的矩阵 $A_h$，其算子范数 $\\lVert A_h \\rVert_2$ 是其最大的奇异值 $s_1$，而 $\\lVert A_h^{-1} \\rVert_2$ 是其最小奇异值的倒数 $1/s_n$。奇异值由模型 $s_i = e^{-\\alpha(i-1)}$ 给出，其中 $i \\in \\{1, \\dots, n\\}$。因此，$s_1 = e^{-\\alpha(1-1)} = e^0 = 1$，$s_n = e^{-\\alpha(n-1)}$。所以条件数为：\n$$\n\\kappa_2(A_h) = \\frac{s_1}{s_n} = \\frac{1}{e^{-\\alpha(n-1)}} = e^{\\alpha(n-1)}\n$$\n\n其次，我们构建期望平方重构误差的公式。问题表述为 $b = A_h x + \\eta$。使用截断索引 $r$ 的 TSVD 重构解为 $x_r = A_h^\\dagger_r b$，其中 $A_h^\\dagger_r$ 是与截断奇异值矩阵 $\\Sigma_r = \\operatorname{diag}(s_1, \\dots, s_r, 0, \\dots, 0)$ 对应的伪逆。在 SVD 基下，$A_h^\\dagger_r = V \\Sigma_r^\\dagger U^\\top$，其中 $\\Sigma_r^\\dagger = \\operatorname{diag}(1/s_1, \\dots, 1/s_r, 0, \\dots, 0)$。将数据模型代入重构公式，得到：\n$$\nx_r = V \\Sigma_r^\\dagger U^\\top (U \\Sigma V^\\top x + \\eta) = V \\Sigma_r^\\dagger \\Sigma V^\\top x + V \\Sigma_r^\\dagger U^\\top \\eta\n$$\n我们在右奇异向量 $V$ 的基下分析误差。令 $\\hat{x} = V^\\top x$ 和 $\\hat{x}_r = V^\\top x_r$ 分别为真实解和重构解的系数向量。将 $x_r$ 的方程左乘 $V^\\top$ 得到：\n$$\n\\hat{x}_r = \\Sigma_r^\\dagger \\Sigma \\hat{x} + \\Sigma_r^\\dagger \\hat{\\eta}\n$$\n其中 $\\hat{\\eta} = U^\\top \\eta$。误差向量为 $\\hat{x}_r - \\hat{x}$。其分量为：\n$$\n(\\hat{x}_r - \\hat{x})_i = \\begin{cases} \\frac{1}{s_i} \\hat{\\eta}_i  \\text{for } 1 \\le i \\le r \\\\ -\\hat{x}_i  \\text{for } r+1 \\le i \\le n \\end{cases}\n$$\n误差的欧几里得范数平方为 $\\lVert \\hat{x}_r - \\hat{x} \\rVert_2^2 = \\sum_{i=1}^r \\frac{1}{s_i^2}\\hat{\\eta}_i^2 + \\sum_{i=r+1}^n \\hat{x}_i^2$。为求期望误差，我们对此量取期望。噪声模型 $\\mathbb{E}[\\eta]=0$ 和 $\\operatorname{Cov}(\\eta)=\\sigma^2 I$，结合 $U$ 的正交性，意味着变换后的噪声向量 $\\hat{\\eta}$ 的分量也独立，且均值为零，方差为 $\\sigma^2$。因此，$\\mathbb{E}[\\hat{\\eta}_i^2] = \\sigma^2$。系数 $\\hat{x}_i$ 是确定性的。对于截断索引 $r$，总期望误差 $E_r$ 为：\n$$\nE_r = \\mathbb{E}[\\lVert \\hat{x}_r - \\hat{x} \\rVert_2^2] = \\sum_{i=1}^r \\frac{\\mathbb{E}[\\hat{\\eta}_i^2]}{s_i^2} + \\sum_{i=r+1}^n \\hat{x}_i^2 = \\sum_{i=1}^r \\frac{\\sigma^2}{s_i^2} + \\sum_{i=r+1}^n \\hat{x}_i^2\n$$\n该表达式分解为两项：由噪声放大引起的方差项 $\\text{Var}(r) = \\sum_{i=1}^r \\frac{\\sigma^2}{s_i^2}$，以及由截断引起的偏差项 $\\text{Bias}(r) = \\sum_{i=r+1}^n \\hat{x}_i^2$。使用给定的模型 $s_i = e^{-\\alpha(i-1)}$ 和 $\\hat{x}_i = c_i = e^{-\\beta(i-1)}$，它们变为：\n$$\n\\text{Var}(r) = \\sigma^2 \\sum_{i=1}^r e^{2\\alpha(i-1)}, \\quad \\text{Bias}(r) = \\sum_{i=r+1}^n e^{-2\\beta(i-1)}\n$$\n对于边界情况，若 $r=0$，方差为 $0$。若 $r=n$，偏差为 $0$。\n\n第三，为找到最优截断索引 $r^\\star$，我们必须找到使 $E_r = \\text{Var}(r) + \\text{Bias}(r)$ 最小化的整数 $r \\in \\{0, 1, \\dots, n\\}$。这可以通过为每个可能的 $r$ 值计算 $E_r$ 并选择产生最小值的那个来实现。如果出现平局，则选择最小的那个 $r$。\n\n第四，截断算子的谱条件数 $\\kappa_2^{(r^\\star)}$ 定义为 $\\kappa_2^{(r^\\star)} = s_1/s_{r^\\star}$（当 $r^\\star \\ge 1$ 时），并按惯例定义 $\\kappa_2^{(0)}=0$。使用奇异值模型，这变为：\n$$\n\\kappa_2^{(r^\\star)} = \\frac{e^{-\\alpha(1-1)}}{e^{-\\alpha(r^\\star-1)}} = e^{\\alpha(r^\\star-1)} \\quad \\text{for } r^\\star \\ge 1\n$$\n\n最后，在最优截断索引 $r^\\star$ 处的方差和偏差贡献可以直接从上面推导出的各自的公式计算得出。实现将对每个测试用例系统地执行这些计算。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It iterates through each case, computes the required values, and\n    formats the output as specified.\n    \"\"\"\n    test_cases = [\n        # (n, alpha, beta, sigma)\n        (50, 0.08, 0.04, 1e-3),\n        (50, 0.08, 0.04, 0.0),\n        (80, 0.20, 0.05, 5e-3),\n        (1, 0.10, 0.10, 1e-2),\n        (30, 0.15, 0.20, 1e-4),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, alpha, beta, sigma = case\n        result = _solve_case(n, alpha, beta, sigma)\n        results.append(result)\n\n    # Format the final output string to match the problem specification\n    # precisely: [[v1,v2,...],[v1,v2,...],...] with no spaces.\n    output_parts = []\n    for res in results:\n        res_str = \"[\" + \",\".join(map(str, res)) + \"]\"\n        output_parts.append(res_str)\n    \n    final_output = \"[\" + \",\".join(output_parts) + \"]\"\n    print(final_output)\n\ndef _solve_case(n, alpha, beta, sigma):\n    \"\"\"\n    Computes the required quantities for a single test case.\n\n    Args:\n        n (int): Dimension of the problem.\n        alpha (float): Decay parameter for singular values.\n        beta (float): Smoothness parameter for the true solution.\n        sigma (float): Standard deviation of the noise.\n\n    Returns:\n        list: A list containing [kappa_full, r_star, kappa_trunc, \n                                 final_variance, final_bias].\n    \"\"\"\n    # 1. Compute the spectral condition number of the full matrix A_h\n    # kappa_2(A_h) = exp(alpha * (n-1))\n    kappa_full = np.exp(alpha * (n - 1))\n\n    # 2. Find the optimal TSVD cutoff r_star that minimizes expected error\n    # Pre-compute terms used in the error calculation for efficiency\n    indices = np.arange(n)  # Represents (i-1) for i from 1 to n\n\n    # Variance terms: sigma^2 * exp(2*alpha*(i-1))\n    var_terms = (sigma ** 2) * np.exp(2 * alpha * indices)\n    # Cumulative sum of variance terms. var_sum_cumulative[r] = sum_{i=1 to r}\n    var_sum_cumulative = np.concatenate(([0], np.cumsum(var_terms)))\n\n    # Bias terms: exp(-2*beta*(i-1))\n    bias_terms = np.exp(-2 * beta * indices)\n    # Total bias if all components are truncated (r=0)\n    total_bias = np.sum(bias_terms)\n    # Cumulative sum of bias terms. cum_bias_sum[r] = sum_{i=1 to r}\n    cum_bias_sum = np.concatenate(([0], np.cumsum(bias_terms)))\n\n    min_error = -1.0\n    r_star = -1\n\n    # Iterate through all possible cutoff values r from 0 to n\n    for r in range(n + 1):\n        # Variance for cutoff r is sum of first r terms\n        variance_r = var_sum_cumulative[r]\n        # Bias for cutoff r is sum of remaining terms (from r+1 to n)\n        bias_r = total_bias - cum_bias_sum[r]\n\n        error_r = variance_r + bias_r\n\n        # Update if a new minimum is found. The strict inequality '' ensures\n        # that the smallest r is chosen in case of a tie.\n        if r_star == -1 or error_r  min_error:\n            min_error = error_r\n            r_star = r\n\n    # 3. Compute the spectral condition number of the truncated operator\n    if r_star == 0:\n        kappa_trunc = 0.0\n    else:\n        # kappa_2^(r*) = s_1 / s_{r*} = exp(alpha * (r_star - 1))\n        kappa_trunc = np.exp(alpha * (r_star - 1))\n\n    # 4. Compute the final variance and bias contributions at r_star\n    final_variance = var_sum_cumulative[r_star]\n    final_bias = total_bias - cum_bias_sum[r_star]\n\n    return [kappa_full, r_star, kappa_trunc, final_variance, final_bias]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}