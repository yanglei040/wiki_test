{
    "hands_on_practices": [
        {
            "introduction": "Mastery of numerical concepts begins with a firm grasp of the fundamental definitions. This exercise moves beyond simple formula application, requiring you to compute condition numbers directly from the definitions of the induced $1$-norm and $\\infty$-norm. By working through the mechanics for a simple $2 \\times 2$ matrix, you will not only solidify your understanding of these specific norms but also uncover and prove a surprising general identity connecting $\\kappa_1(A)$ and $\\kappa_\\infty(A)$ for any matrix of this size .",
            "id": "3567330",
            "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the matrix\n$$\nA \\;=\\; \\begin{pmatrix} 7  -3 \\\\ 2  5 \\end{pmatrix}.\n$$\nStarting from the definitions of induced matrix norms and the condition number, compute the induced $1$-norm $\\|A\\|_{1}$, the induced infinity-norm $\\|A\\|_{\\infty}$, and the corresponding condition numbers $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$. Use only the definition that for a vector norm $\\|\\cdot\\|_{p}$, the induced matrix norm is $\\|A\\|_{p} \\,=\\, \\displaystyle \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{p}}{\\|x\\|_{p}}$, and the definition of the condition number $\\kappa_{p}(A) \\,=\\, \\|A\\|_{p}\\,\\|A^{-1}\\|_{p}$. After carrying out these computations for the given matrix, analyze and explain any observed differences between $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$ by deriving, from first principles, a general identity for arbitrary invertible $2 \\times 2$ matrices that accounts for your observation.\n\nAs your final reported value, provide the exact value of the ratio $\\kappa_{1}(A) / \\kappa_{\\infty}(A)$ in simplest form. No rounding is required.",
            "solution": "The problem will be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n- The matrix $A \\in \\mathbb{R}^{2 \\times 2}$ is given as $A = \\begin{pmatrix} 7  -3 \\\\ 2  5 \\end{pmatrix}$.\n- The definition of the induced matrix norm is $\\|A\\|_{p} = \\displaystyle \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{p}}{\\|x\\|_{p}}$.\n- The definition of the condition number is $\\kappa_{p}(A) = \\|A\\|_{p}\\,\\|A^{-1}\\|_{p}$.\n- The task is to compute $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, $\\kappa_{1}(A)$, and $\\kappa_{\\infty}(A)$ starting from these definitions.\n- The task includes analyzing the observed relationship between $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$ by deriving a general identity for arbitrary invertible $2 \\times 2$ matrices.\n- The final reported value is the exact value of the ratio $\\kappa_{1}(A) / \\kappa_{\\infty}(A)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on fundamental and standard definitions in numerical linear algebra, specifically induced matrix norms and condition numbers. The concepts are mathematically rigorous.\n- **Well-Posed**: The problem is well-defined. The matrix $A$ is specified, and the quantities to be computed are based on standard, non-ambiguous definitions. The matrix $A$ is invertible as its determinant is non-zero, so $A^{-1}$ and the condition numbers exist. A unique solution is expected.\n- **Objective**: The language is precise and mathematical, free of any subjectivity or opinion.\n- **Self-Contained and Consistent**: The problem provides all necessary information (the matrix and the base definitions) to proceed with the solution. There are no contradictions.\n- **Relevance**: The problem is directly related to the topic of problem conditioning and condition numbers in numerical linear algebra.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution\nThe problem requires the computation of matrix norms and condition numbers for a given matrix $A$, and an explanation for the observed results based on a general derivation.\n\nFirst, we must establish the computational formulas for the induced $1$-norm and $\\infty$-norm from their supremum definition, as stipulated. Let $M$ be a general $m \\times n$ matrix with entries $m_{ij}$.\n\nFor the $1$-norm, $\\|M\\|_1$:\nThe $1$-norm of a vector $v \\in \\mathbb{R}^k$ is $\\|v\\|_1 = \\sum_{i=1}^k |v_i|$. For $x \\in \\mathbb{R}^n$, we have:\n$$ \\|Mx\\|_1 = \\sum_{i=1}^m \\left| \\sum_{j=1}^n m_{ij} x_j \\right| \\le \\sum_{i=1}^m \\sum_{j=1}^n |m_{ij}| |x_j| $$\nRearranging the summation order:\n$$ \\|Mx\\|_1 \\le \\sum_{j=1}^n |x_j| \\left( \\sum_{i=1}^m |m_{ij}| \\right) \\le \\left( \\max_{1 \\le k \\le n} \\sum_{i=1}^m |m_{ik}| \\right) \\sum_{j=1}^n |x_j| = \\left( \\max_{k} \\sum_{i} |m_{ik}| \\right) \\|x\\|_1 $$\nThis shows that $\\|M\\|_1 \\le \\max_{j} \\sum_{i} |m_{ij}|$. To show equality, we must find a vector $x$ for which the bound is attained. Let $k$ be the column index for which the maximum absolute column sum is achieved. Let $x$ be the standard basis vector $e_k$, where $(e_k)_j = \\delta_{jk}$. Then $\\|x\\|_1 = 1$. For this $x$, $Mx$ is the $k$-th column of $M$.\n$$ \\|Me_k\\|_1 = \\sum_{i=1}^m |m_{ik}| = \\max_{j} \\sum_{i} |m_{ij}| $$\nThus, the supremum is the maximum absolute column sum: $\\|M\\|_1 = \\max_{j} \\sum_{i} |m_{ij}|$.\n\nFor the $\\infty$-norm, $\\|M\\|_\\infty$:\nThe $\\infty$-norm of a vector $v \\in \\mathbb{R}^k$ is $\\|v\\|_\\infty = \\max_{1 \\le i \\le k} |v_i|$. For $x \\in \\mathbb{R}^n$ with $\\|x\\|_\\infty \\le 1$:\n$$ \\|Mx\\|_\\infty = \\max_{1 \\le i \\le m} \\left| \\sum_{j=1}^n m_{ij} x_j \\right| \\le \\max_{1 \\le i \\le m} \\sum_{j=1}^n |m_{ij}| |x_j| \\le \\max_{1 \\le i \\le m} \\sum_{j=1}^n |m_{ij}| \\|x\\|_\\infty $$\nThis shows that $\\|M\\|_\\infty \\le \\left( \\max_{i} \\sum_{j} |m_{ij}| \\right) \\|x\\|_\\infty$. To show equality, let $k$ be the row index where the maximum absolute row sum occurs. Define a vector $x$ with components $x_j = \\text{sgn}(m_{kj})$. Then $\\|x\\|_\\infty=1$ (if the $k$-th row is not all zeros, which would be a trivial case). The $k$-th component of $Mx$ is:\n$$ (Mx)_k = \\sum_{j=1}^n m_{kj} x_j = \\sum_{j=1}^n m_{kj} \\text{sgn}(m_{kj}) = \\sum_{j=1}^n |m_{kj}| = \\max_{i} \\sum_{j} |m_{ij}| $$\nSince $\\|Mx\\|_\\infty \\ge |(Mx)_k|$, we have shown the bound is attained. Thus, the supremum is the maximum absolute row sum: $\\|M\\|_\\infty = \\max_{i} \\sum_{j} |m_{ij}|$.\n\nNow, we apply these formulas to the given matrix $A = \\begin{pmatrix} 7  -3 \\\\ 2  5 \\end{pmatrix}$.\n\n**Computation of Norms of $A$**\nThe absolute column sums for $A$ are:\n- Column 1: $|7| + |2| = 7 + 2 = 9$.\n- Column 2: $|-3| + |5| = 3 + 5 = 8$.\nSo, $\\|A\\|_1 = \\max(9, 8) = 9$.\n\nThe absolute row sums for $A$ are:\n- Row 1: $|7| + |-3| = 7 + 3 = 10$.\n- Row 2: $|2| + |5| = 2 + 5 = 7$.\nSo, $\\|A\\|_\\infty = \\max(10, 7) = 10$.\n\n**Computation of Inverse and its Norms**\nTo find the condition numbers, we need $A^{-1}$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nThe determinant of $A$ is $\\det(A) = (7)(5) - (-3)(2) = 35 + 6 = 41$. Since $\\det(A) \\neq 0$, $A$ is invertible.\n$$ A^{-1} = \\frac{1}{41} \\begin{pmatrix} 5  3 \\\\ -2  7 \\end{pmatrix} = \\begin{pmatrix} 5/41  3/41 \\\\ -2/41  7/41 \\end{pmatrix} $$\nNow we compute the norms of $A^{-1}$:\n- Column 1 sum: $|5/41| + |-2/41| = (5+2)/41 = 7/41$.\n- Column 2 sum: $|3/41| + |7/41| = (3+7)/41 = 10/41$.\nSo, $\\|A^{-1}\\|_1 = \\max(7/41, 10/41) = 10/41$.\n\n- Row 1 sum: $|5/41| + |3/41| = (5+3)/41 = 8/41$.\n- Row 2 sum: $|-2/41| + |7/41| = (2+7)/41 = 9/41$.\nSo, $\\|A^{-1}\\|_\\infty = \\max(8/41, 9/41) = 9/41$.\n\n**Computation of Condition Numbers**\nUsing the definition $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$:\n$$ \\kappa_{1}(A) = \\|A\\|_{1} \\|A^{-1}\\|_{1} = 9 \\cdot \\frac{10}{41} = \\frac{90}{41} $$\n$$ \\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = 10 \\cdot \\frac{9}{41} = \\frac{90}{41} $$\nWe observe that $\\kappa_{1}(A) = \\kappa_{\\infty}(A)$.\n\n**Analysis of the Observation**\nThe problem requests a derivation of a general identity for $2 \\times 2$ matrices to explain this observation.\nFirst, we establish a general identity relating the $1$-norm and $\\infty$-norm for any square matrix $M$ and its transpose $M^T$.\n$\\|M\\|_1 = \\max_j \\sum_i |m_{ij}|$.\n$\\|M^T\\|_\\infty = \\max_i \\sum_j |(M^T)_{ij}| = \\max_i \\sum_j |m_{ji}|$. By relabeling indices, this is clearly the same as the maximum absolute column sum of $M$. So, $\\|M\\|_1 = \\|M^T\\|_\\infty$.\nSimilarly, $\\|M\\|_\\infty = \\max_i \\sum_j |m_{ij}| = \\|M^T\\|_1$.\n\nUsing this, we can relate $\\kappa_1(A)$ to $\\kappa_\\infty(A^T)$:\n$\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1$.\nUsing the identities above and the property $(A^{-1})^T=(A^T)^{-1}$:\n$\\kappa_1(A) = \\|A^T\\|_\\infty \\|(A^{-1})^T\\|_\\infty = \\|A^T\\|_\\infty \\|(A^T)^{-1}\\|_\\infty = \\kappa_\\infty(A^T)$.\nThis identity, $\\kappa_1(A) = \\kappa_\\infty(A^T)$, holds for any invertible square matrix $A$.\n\nThe observation $\\kappa_1(A) = \\kappa_\\infty(A)$ for our specific matrix would be explained if, for this matrix $A$, $\\kappa_\\infty(A) = \\kappa_\\infty(A^T)$. We will now show this holds for *any* invertible $2 \\times 2$ matrix.\nLet $B = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ be an arbitrary invertible $2 \\times 2$ matrix, so $\\det(B) = ad-bc \\neq 0$.\n$\\|B\\|_\\infty = \\max(|a|+|b|, |c|+|d|)$.\n$B^{-1} = \\frac{1}{\\det(B)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\n$\\|B^{-1}\\|_\\infty = \\frac{1}{|\\det(B)|} \\max(|d|+|-b|, |-c|+|a|) = \\frac{1}{|\\det(B)|} \\max(|b|+|d|, |a|+|c|)$.\nSo, $\\kappa_\\infty(B) = \\|B\\|_\\infty \\|B^{-1}\\|_\\infty = \\frac{\\max(|a|+|b|, |c|+|d|) \\cdot \\max(|a|+|c|, |b|+|d|)}{|\\det(B)|}$.\n\nNow consider the transpose $B^T = \\begin{pmatrix} a  c \\\\ b  d \\end{pmatrix}$.\n$\\|B^T\\|_\\infty = \\max(|a|+|c|, |b|+|d|)$.\n$(B^T)^{-1} = (B^{-1})^T = \\frac{1}{\\det(B^T)} \\begin{pmatrix} d  -c \\\\ -b  a \\end{pmatrix}$. Note $\\det(B^T) = \\det(B)$.\n$\\|(B^T)^{-1}\\|_\\infty = \\frac{1}{|\\det(B)|} \\max(|d|+|-c|, |-b|+|a|) = \\frac{1}{|\\det(B)|} \\max(|c|+|d|, |a|+|b|)$.\nSo, $\\kappa_\\infty(B^T) = \\|B^T\\|_\\infty \\|(B^T)^{-1}\\|_\\infty = \\frac{\\max(|a|+|c|, |b|+|d|) \\cdot \\max(|a|+|b|, |c|+|d|)}{|\\det(B)|}$.\nBy the commutativity of multiplication, it is evident that $\\kappa_\\infty(B) = \\kappa_\\infty(B^T)$.\n\nSince this identity $\\kappa_\\infty(B) = \\kappa_\\infty(B^T)$ holds for any invertible $2 \\times 2$ matrix $B$, and we have the general identity $\\kappa_1(A) = \\kappa_\\infty(A^T)$, it follows that for any invertible $2 \\times 2$ matrix $A$:\n$$ \\kappa_1(A) = \\kappa_\\infty(A^T) = \\kappa_\\infty(A) $$\nThis general identity for all $2 \\times 2$ matrices explains why we observed $\\kappa_1(A) = \\kappa_\\infty(A)$ for the specific matrix given.\n\n**Final Calculation**\nThe problem asks for the ratio $\\kappa_{1}(A) / \\kappa_{\\infty}(A)$. Based on our specific calculations and the general identity we just proved:\n$$ \\frac{\\kappa_{1}(A)}{\\kappa_{\\infty}(A)} = \\frac{90/41}{90/41} = 1 $$",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The most insightful lessons in conditioning often come from exploring the boundary between well-posed and ill-posed problems. This practice centers on a matrix family $A_{\\varepsilon}$ that approaches a singular (and thus ill-posed) state as a parameter $\\varepsilon$ goes to zero . You will demonstrate that while the problem of solving $A_{\\varepsilon}x=b$ is theoretically well-posed for any $\\varepsilon  0$, its numerical stability catastrophically degrades. By constructing a specific input perturbation that achieves the maximum possible error amplification, you will gain a concrete understanding of how a large condition number manifests as extreme sensitivity in practice.",
            "id": "3567280",
            "problem": "Consider the linear system solving problem $f_{\\varepsilon}:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ defined by $f_{\\varepsilon}(b)=x$ where $A_{\\varepsilon}x=b$ with\n$$\nA_{\\varepsilon}=\\begin{pmatrix}1  1 \\\\ 1  1+\\varepsilon\\end{pmatrix},\n$$\nand parameter $0\\varepsilon\\leq 1$. Treat the problem as one of computing the solution $x$ from the data $b$ for a fixed $A_{\\varepsilon}$. Your tasks are:\n- Argue from first principles that, for each fixed $0\\varepsilon\\leq 1$, this problem is well-posed.\n- Using only core definitions from linear algebra and the operator $2$-norm, compute the condition number in the $2$-norm of the linear system solving map $b\\mapsto x$ associated with $A_{\\varepsilon}$, and give it as a closed-form analytic expression in terms of $\\varepsilon$.\n- Exhibit a specific choice of $b\\in\\mathbb{R}^{2}$ and a perturbation $\\delta b\\in\\mathbb{R}^{2}$ (both possibly depending on $\\varepsilon$) such that the relative change amplification\n$$\n\\frac{\\|f_{\\varepsilon}(b+\\delta b)-f_{\\varepsilon}(b)\\|_{2}/\\|f_{\\varepsilon}(b)\\|_{2}}{\\|\\delta b\\|_{2}/\\|b\\|_{2}}\n$$\nis attained exactly by your construction, and explain why this demonstrates that the problem is ill-conditioned for small $\\varepsilon$ but remains well-posed.\n\nProvide your final answer as the exact analytic expression for the $2$-norm condition number $\\kappa_{2}(A_{\\varepsilon})$ as a function of $\\varepsilon$. No numerical rounding is required or permitted. The final answer must be a single closed-form expression.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The problem is a linear system solving map $f_{\\varepsilon}:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ defined by $f_{\\varepsilon}(b)=x$, where $A_{\\varepsilon}x=b$.\n- The matrix is $A_{\\varepsilon}=\\begin{pmatrix}1  1 \\\\ 1  1+\\varepsilon\\end{pmatrix}$.\n- The parameter range is $0\\varepsilon\\leq 1$.\n- The task is to compute the solution $x$ from the data $b$.\n- The required tasks are:\n    1. Argue from first principles that the problem is well-posed for each fixed $0\\varepsilon\\leq 1$.\n    2. Compute the condition number $\\kappa_{2}(A_{\\varepsilon})$ in the operator $2$-norm as a closed-form expression in $\\varepsilon$.\n    3. Construct a specific $b\\in\\mathbb{R}^{2}$ and $\\delta b\\in\\mathbb{R}^{2}$ that achieve the maximal relative change amplification.\n    4. Explain how this demonstrates ill-conditioning for small $\\varepsilon$ despite the problem being well-posed.\n- The final answer must be the analytic expression for $\\kappa_{2}(A_{\\varepsilon})$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, based on the well-established concepts of linear systems, matrix norms, and condition numbers. It is scientifically and mathematically sound.\n- **Well-Posed:** The problem statement asks to prove its well-posedness. A preliminary check confirms this is possible. The determinant of the matrix $A_{\\varepsilon}$ is $\\det(A_{\\varepsilon}) = 1(1+\\varepsilon) - 1(1) = \\varepsilon$. Since the given range is $0  \\varepsilon \\leq 1$, the determinant is never zero, which implies the matrix is always invertible in this range. This is the basis for a well-posed problem (existence and uniqueness of the solution). The problem structure is sound.\n- **Objective:** The problem is stated in precise, objective mathematical language, free of any subjectivity.\n- **Incomplete or Contradictory Setup:** The problem is self-contained. All necessary information (the matrix, the parameter range, the norm) is provided. There are no contradictions.\n- **All other checklist items are satisfied.** The problem is a valid, well-structured mathematical problem.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A full solution will be provided.\n\n### Solution\n\nThe problem concerns the mapping $f_{\\varepsilon}(b) = x$ where $A_{\\varepsilon}x=b$. For a fixed $\\varepsilon \\in (0, 1]$, this can be written as $x = A_{\\varepsilon}^{-1}b$.\n\n**1. Well-Posedness**\n\nA problem is considered well-posed in the sense of Hadamard if a solution exists, is unique, and depends continuously on the input data. We analyze the problem $f_{\\varepsilon}(b)=x$ for any fixed $\\varepsilon$ such that $0  \\varepsilon \\leq 1$.\n\n- **Existence and Uniqueness:** A unique solution $x \\in \\mathbb{R}^{2}$ exists for any given $b \\in \\mathbb{R}^{2}$ if and only if the matrix $A_{\\varepsilon}$ is invertible. A square matrix is invertible if and only if its determinant is non-zero. The determinant of $A_{\\varepsilon}$ is:\n$$\n\\det(A_{\\varepsilon}) = \\det\\begin{pmatrix}1  1 \\\\ 1  1+\\varepsilon\\end{pmatrix} = (1)(1+\\varepsilon) - (1)(1) = 1 + \\varepsilon - 1 = \\varepsilon\n$$\nThe problem specifies that $0  \\varepsilon \\leq 1$. In this range, $\\det(A_{\\varepsilon}) = \\varepsilon \\neq 0$. Therefore, $A_{\\varepsilon}$ is invertible, which guarantees the existence and uniqueness of the solution $x = A_{\\varepsilon}^{-1}b$ for any $b \\in \\mathbb{R}^{2}$.\n\n- **Continuous Dependence:** The solution is given by the map $x = f_{\\varepsilon}(b) = A_{\\varepsilon}^{-1}b$. For a fixed $\\varepsilon$, the matrix $A_{\\varepsilon}^{-1}$ is a constant matrix. The mapping $b \\mapsto A_{\\varepsilon}^{-1}b$ is a linear transformation on the finite-dimensional vector space $\\mathbb{R}^{2}$. All linear transformations on finite-dimensional spaces are continuous. Thus, the solution $x$ depends continuously on the data $b$.\n\nSince all three conditions of existence, uniqueness, and continuous dependence are satisfied for any fixed $\\varepsilon \\in (0, 1]$, the problem is well-posed.\n\n**2. Condition Number Calculation**\n\nThe condition number of an invertible matrix $A$ in the operator $2$-norm is defined as $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$. The operator $2$-norm of a matrix is its largest singular value, $\\|A\\|_{2} = \\sigma_{\\max}(A)$. The singular values are the square roots of the eigenvalues of the matrix $A^T A$.\n\nThe given matrix $A_{\\varepsilon}$ is symmetric: $A_{\\varepsilon}^T = A_{\\varepsilon}$. For a symmetric matrix, the singular values are the absolute values of its eigenvalues. The eigenvalues of $A_{\\varepsilon}$ are the roots $\\lambda$ of the characteristic equation $\\det(A_{\\varepsilon} - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix}1-\\lambda  1 \\\\ 1  1+\\varepsilon-\\lambda\\end{pmatrix} = (1-\\lambda)(1+\\varepsilon-\\lambda) - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\varepsilon)\\lambda + (1+\\varepsilon) - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\varepsilon)\\lambda + \\varepsilon = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{(2+\\varepsilon) \\pm \\sqrt{(2+\\varepsilon)^2 - 4\\varepsilon}}{2} = \\frac{2+\\varepsilon \\pm \\sqrt{4+4\\varepsilon+\\varepsilon^2 - 4\\varepsilon}}{2} = \\frac{2+\\varepsilon \\pm \\sqrt{4+\\varepsilon^2}}{2}\n$$\nSince $\\varepsilon  0$, the trace $2+\\varepsilon  0$ and the determinant $\\varepsilon  0$, so both eigenvalues are positive. The eigenvalues are:\n$$\n\\lambda_{\\max} = \\frac{2+\\varepsilon + \\sqrt{4+\\varepsilon^2}}{2} \\quad \\text{and} \\quad \\lambda_{\\min} = \\frac{2+\\varepsilon - \\sqrt{4+\\varepsilon^2}}{2}\n$$\nThe singular values of $A_{\\varepsilon}$ are $\\sigma_{\\max} = \\lambda_{\\max}$ and $\\sigma_{\\min} = \\lambda_{\\min}$. Thus, $\\|A_{\\varepsilon}\\|_{2} = \\lambda_{\\max}$.\n\nThe singular values of the inverse matrix $A_{\\varepsilon}^{-1}$ are the reciprocals of the singular values of $A_{\\varepsilon}$. Therefore, the largest singular value of $A_{\\varepsilon}^{-1}$ is $1/\\sigma_{\\min}(A_{\\varepsilon}) = 1/\\lambda_{\\min}$.\n$$\n\\|A_{\\varepsilon}^{-1}\\|_{2} = \\frac{1}{\\lambda_{\\min}} = \\frac{2}{2+\\varepsilon - \\sqrt{4+\\varepsilon^2}}\n$$\nThe condition number is the ratio of the largest to the smallest singular value:\n$$\n\\kappa_{2}(A_{\\varepsilon}) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\frac{2+\\varepsilon + \\sqrt{4+\\varepsilon^2}}{2}}{\\frac{2+\\varepsilon - \\sqrt{4+\\varepsilon^2}}{2}} = \\frac{2+\\varepsilon + \\sqrt{4+\\varepsilon^2}}{2+\\varepsilon - \\sqrt{4+\\varepsilon^2}}\n$$\nTo obtain a more detailed expression, we can rationalize the denominator:\n$$\n\\kappa_{2}(A_{\\varepsilon}) = \\frac{(2+\\varepsilon + \\sqrt{4+\\varepsilon^2})^2}{(2+\\varepsilon - \\sqrt{4+\\varepsilon^2})(2+\\varepsilon + \\sqrt{4+\\varepsilon^2})} = \\frac{(2+\\varepsilon)^2 + 2(2+\\varepsilon)\\sqrt{4+\\varepsilon^2} + (4+\\varepsilon^2)}{(2+\\varepsilon)^2 - (4+\\varepsilon^2)}\n$$\nThe denominator is $(4+4\\varepsilon+\\varepsilon^2) - (4+\\varepsilon^2) = 4\\varepsilon$. The numerator is $4+4\\varepsilon+\\varepsilon^2 + 4+\\varepsilon^2 + 2(2+\\varepsilon)\\sqrt{4+\\varepsilon^2} = 8+4\\varepsilon+2\\varepsilon^2 + 2(2+\\varepsilon)\\sqrt{4+\\varepsilon^2}$.\n$$\n\\kappa_{2}(A_{\\varepsilon}) = \\frac{8+4\\varepsilon+2\\varepsilon^2 + 2(2+\\varepsilon)\\sqrt{4+\\varepsilon^2}}{4\\varepsilon} = \\frac{4+2\\varepsilon+\\varepsilon^2 + (2+\\varepsilon)\\sqrt{4+\\varepsilon^2}}{2\\varepsilon}\n$$\n\n**3. Construction of Worst-Case Perturbation**\n\nThe relative change amplification ratio is given by $\\frac{\\|\\delta x\\|_{2}/\\|x\\|_{2}}{\\|\\delta b\\|_{2}/\\|b\\|_{2}}$, where $x=A_{\\varepsilon}^{-1}b$ and $\\delta x=A_{\\varepsilon}^{-1}\\delta b$. The maximum value of this ratio is $\\kappa_{2}(A_{\\varepsilon})$. This maximum is attained when the input data $b$ and the perturbation $\\delta b$ are chosen along specific eigenvector directions.\n\nThe ratio can be written as $\\frac{\\|A_{\\varepsilon}^{-1}\\delta b\\|_{2}}{\\|\\delta b\\|_{2}} \\cdot \\frac{\\|b\\|_{2}}{\\|A_{\\varepsilon}^{-1}b\\|_{2}}$.\n- To maximize the first term, $\\frac{\\|A_{\\varepsilon}^{-1}\\delta b\\|_{2}}{\\|\\delta b\\|_{2}}$, we must choose $\\delta b$ to be an eigenvector of $A_{\\varepsilon}$ corresponding to its minimum eigenvalue $\\lambda_{\\min}$ (which corresponds to the maximum eigenvalue $1/\\lambda_{\\min}$ of $A_{\\varepsilon}^{-1}$). Let this eigenvector be $v_{\\min}$. Then this term becomes $\\|A_{\\varepsilon}^{-1}\\|_{2} = 1/\\lambda_{\\min}$.\n- To maximize the second term, $\\frac{\\|b\\|_{2}}{\\|A_{\\varepsilon}^{-1}b\\|_{2}}$, we must choose $b$ to be an eigenvector of $A_{\\varepsilon}$ corresponding to its maximum eigenvalue $\\lambda_{\\max}$ (which corresponds to the minimum eigenvalue $1/\\lambda_{\\max}$ of $A_{\\varepsilon}^{-1}$). Let this eigenvector be $v_{\\max}$. Then this term becomes $\\frac{\\|v_{\\max}\\|_{2}}{\\|A_{\\varepsilon}^{-1}v_{\\max}\\|_{2}} = \\frac{\\|v_{\\max}\\|_{2}}{\\|(1/\\lambda_{\\max})v_{\\max}\\|_{2}} = \\lambda_{\\max} = \\|A_{\\varepsilon}\\|_{2}$.\n\nLet's find these eigenvectors. For an eigenvalue $\\lambda$, an eigenvector $\\begin{pmatrix} z \\\\ y \\end{pmatrix}$ satisfies $(1-\\lambda)z + y = 0$, so we can choose an eigenvector of the form $\\begin{pmatrix} 1 \\\\ \\lambda-1 \\end{pmatrix}$.\n- Let $b = v_{\\max} = \\begin{pmatrix} 1 \\\\ \\lambda_{\\max}-1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{\\varepsilon + \\sqrt{4+\\varepsilon^2}}{2} \\end{pmatrix}$.\n- Let $\\delta b = v_{\\min} = \\begin{pmatrix} 1 \\\\ \\lambda_{\\min}-1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{\\varepsilon - \\sqrt{4+\\varepsilon^2}}{2} \\end{pmatrix}$.\n\nWith this choice:\n$x = A_{\\varepsilon}^{-1}b = A_{\\varepsilon}^{-1}v_{\\max} = \\frac{1}{\\lambda_{\\max}}v_{\\max}$.\n$\\delta x = A_{\\varepsilon}^{-1}\\delta b = A_{\\varepsilon}^{-1}v_{\\min} = \\frac{1}{\\lambda_{\\min}}v_{\\min}$.\n\nThe amplification ratio is:\n$$\n\\frac{\\|\\delta x\\|_{2}/\\|x\\|_{2}}{\\|\\delta b\\|_{2}/\\|b\\|_{2}} = \\frac{\\|\\frac{1}{\\lambda_{\\min}}v_{\\min}\\|_{2} / \\|\\frac{1}{\\lambda_{\\max}}v_{\\max}\\|_{2}}{\\|v_{\\min}\\|_{2} / \\|v_{\\max}\\|_{2}} = \\frac{(1/\\lambda_{\\min}) \\|v_{\\min}\\|_{2}}{(1/\\lambda_{\\max}) \\|v_{\\max}\\|_{2}} \\cdot \\frac{\\|v_{\\max}\\|_{2}}{\\|v_{\\min}\\|_{2}} = \\frac{1/\\lambda_{\\min}}{1/\\lambda_{\\max}} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa_{2}(A_{\\varepsilon})\n$$\nThis construction demonstrates how to choose inputs to achieve the maximum possible error amplification, which is exactly the condition number.\n\n**4. Ill-Conditioning vs. Well-Posedness**\n\nThe problem is well-posed for any fixed $\\varepsilon \\in (0, 1]$, as established in part $1$. However, the conditioning of the problem depends on the magnitude of $\\kappa_{2}(A_{\\varepsilon})$. Let's examine the behavior as $\\varepsilon \\to 0^{+}$.\n$$\n\\lim_{\\varepsilon\\to 0^{+}} \\kappa_{2}(A_{\\varepsilon}) = \\lim_{\\varepsilon\\to 0^{+}} \\frac{4+2\\varepsilon+\\varepsilon^2 + (2+\\varepsilon)\\sqrt{4+\\varepsilon^2}}{2\\varepsilon}\n$$\nAs $\\varepsilon \\to 0^{+}$, the numerator approaches $4+0+0+(2)\\sqrt{4} = 8$. The denominator approaches $0$.\nMore precisely, using the Taylor expansion $\\sqrt{4+\\varepsilon^2} = 2 + O(\\varepsilon^2)$, the numerator is $8+4\\varepsilon+O(\\varepsilon^2)$.\nThus, $\\kappa_{2}(A_{\\varepsilon}) \\approx \\frac{8+4\\varepsilon}{2\\varepsilon} = \\frac{4}{\\varepsilon} + 2$.\nAs $\\varepsilon \\to 0^{+}$, $\\kappa_{2}(A_{\\varepsilon}) \\to \\infty$.\n\nA very large condition number indicates that the problem is **ill-conditioned**. This means that even though a unique, continuous solution exists theoretically, small relative errors in the input data $b$ can be amplified into extremely large relative errors in the computed solution $x$. As $\\varepsilon$ becomes very small, the matrix $A_{\\varepsilon}$ approaches the singular matrix $\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$. The problem becomes ill-posed at the limit $\\varepsilon=0$. Ill-conditioning for $\\varepsilon  0$ reflects this proximity to a singular (ill-posed) problem.\n\nThe distinction is crucial:\n- **Well-posedness** is a qualitative, theoretical property. For any fixed $\\varepsilon  0$, no matter how small, the problem has a unique, stable solution, so it is well-posed.\n- **Conditioning** is a quantitative, practical property. As $\\varepsilon \\to 0^{+}$, the condition number grows without bound, making the problem numerically unstable and ill-conditioned. In finite-precision arithmetic, round-off errors in $b$ would be magnified by the factor $\\kappa_{2}(A_{\\varepsilon})$, potentially rendering the computed solution meaningless.\n\nThis example clearly demonstrates that a problem can be rigorously well-posed in a mathematical sense but simultaneously be so ill-conditioned as to be practically unsolvable with accuracy.",
            "answer": "$$\n\\boxed{\\frac{4+2\\varepsilon+\\varepsilon^2 + (2+\\varepsilon)\\sqrt{4+\\varepsilon^2}}{2\\varepsilon}}\n$$"
        },
        {
            "introduction": "The concept of conditioning extends far beyond square linear systems, playing a crucial role in the stability of countless numerical algorithms. This hands-on programming task applies the condition number as a diagnostic tool for a ubiquitous problem: polynomial least-squares data fitting . You will empirically discover why the intuitive choice of a monomial basis ($\\{1, x, x^2, \\dots\\}$) is notoriously ill-conditioned, leading to impractical designs even for moderate polynomial degrees. By contrasting this with the remarkable stability of the Chebyshev polynomial basis, you will see firsthand how a mathematically informed choice of problem representation is essential for numerical success.",
            "id": "3567261",
            "problem": "You are asked to design and implement a program that compares the numerical conditioning of polynomial least-squares fitting on the interval $[-1,1]$ when using the monomial basis versus the Chebyshev basis of the first kind. The comparison must be framed via the spectral (two-norm) condition number of the design matrix and a principled criterion for declaring loss of practical solvability.\n\nFundamental base and definitions to use:\n- In finite-dimensional linear algebra, the spectral norm (two-norm) of a matrix $A$ is the largest singular value, denoted $\\|A\\|_{2} = \\sigma_{\\max}(A)$. For a full column-rank matrix $A$, the two-norm condition number is defined as $\\kappa_{2}(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$, where $\\sigma_{\\min}(A)$ is the smallest nonzero singular value.\n- For a linear least-squares problem solved by a backward stable algorithm in floating-point arithmetic with machine epsilon $u$, a first-order forward error model predicts that the relative solution error is, up to modest constants, amplified by the condition number. This motivates a criterion of impracticality: if $\\kappa_{2}(A) \\cdot u$ exceeds a user-specified tolerance $\\tau$, then the design is considered numerically impractical for resolving relative errors at the tolerance level $\\tau$.\n- Adopt the model of IEEE-$754$ double precision arithmetic, and take the machine epsilon to be $u = \\epsilon_{\\mathrm{mach}}$, where in Python this is $u = \\mathrm{numpy.finfo}(\\mathrm{float}).\\mathrm{eps}$.\n\nSetup:\n- Let $m \\in \\mathbb{N}$ be the number of sample points, and let $n \\in \\mathbb{N}_{0}$ be the polynomial degree. Consider the equispaced grid $x_{i} = -1 + 2 \\cdot \\frac{i}{m-1}$ for $i = 0,1,\\dots,m-1$ on $[-1,1]$.\n- Define two design matrices for degree $n$:\n  1. The monomial design matrix $A_{\\mathrm{mono}} \\in \\mathbb{R}^{m \\times (n+1)}$ with entries $[A_{\\mathrm{mono}}]_{i,j} = x_{i}^{j}$ for $j = 0,1,\\dots,n$.\n  2. The Chebyshev design matrix $A_{\\mathrm{cheb}} \\in \\mathbb{R}^{m \\times (n+1)}$ with entries $[A_{\\mathrm{cheb}}]_{i,j} = T_{j}(x_{i})$ for $j = 0,1,\\dots,n$, where $T_{j}$ is the Chebyshev polynomial of the first kind defined by $T_{0}(x) = 1$, $T_{1}(x) = x$, and the three-term recurrence $T_{j+1}(x) = 2 x T_{j}(x) - T_{j-1}(x)$ for $j \\ge 1$.\n- For a given tolerance $\\tau  0$, define the impracticality threshold degree for a basis as the minimal degree $n_{\\star}$ such that $\\kappa_{2}(A_{n_{\\star}}) \\cdot u  \\tau$, where $A_{n}$ denotes the corresponding degree-$n$ design matrix. If no such $n \\le n_{\\max}$ exists, report $-1$. Here, set $n_{\\max} = \\min\\{m-1, 80\\}$ to ensure computational tractability.\n\nTask:\n- For each test case $(m,\\tau)$, compute:\n  1. The minimal degree $n_{\\star}^{\\mathrm{mono}}$ such that $\\kappa_{2}(A_{\\mathrm{mono}}^{(n_{\\star}^{\\mathrm{mono}})}) \\cdot u  \\tau$, or $-1$ if no $n \\le n_{\\max}$ satisfies the inequality.\n  2. The minimal degree $n_{\\star}^{\\mathrm{cheb}}$ such that $\\kappa_{2}(A_{\\mathrm{cheb}}^{(n_{\\star}^{\\mathrm{cheb}})}) \\cdot u  \\tau$, or $-1$ if no $n \\le n_{\\max}$ satisfies the inequality.\n- The two-norm condition number $\\kappa_{2}(A)$ must be computed via the singular value decomposition as the ratio $\\sigma_{\\max}(A)/\\sigma_{\\min}(A)$.\n\nTest suite:\n- Use the following three test cases, each specified as $(m,\\tau)$:\n  1. $(21, 10^{-12})$,\n  2. $(50, 10^{-2})$,\n  3. $(200, 10^{-8})$.\nFor each test case, the grid points are equispaced on $[-1,1]$ as defined above, and $n_{\\max} = \\min\\{m-1, 80\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line should contain a list of three lists. Each inner list corresponds to one test case (in the order given above) and contains exactly two integers $[n_{\\star}^{\\mathrm{mono}}, n_{\\star}^{\\mathrm{cheb}}]$. For example, an output line could look like $[[12, -1],[9, -1],[15, -1]]$ depending on the computed results. No extra text or whitespace is permitted beyond the brackets and commas.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically concerning the conditioning of linear least-squares problems. The problem is well-posed, with all necessary definitions, constants, and constraints clearly specified. Its language is objective and unambiguous. The task is to implement a well-defined numerical experiment to compare the conditioning of two different polynomial bases.\n\nThe solution proceeds by implementing a numerical search for the minimal polynomial degree at which the numerical conditioning of the least-squares problem becomes impractical, based on a given criterion. This is done for two distinct polynomial bases: the monomial basis and the Chebyshev basis.\n\nThe core of the problem lies in the relationship between the condition number of a design matrix $A$, the machine precision $u$, and the propagation of errors in solving the linear least-squares problem $\\min_{c} \\| Ac - y \\|_{2}$. A high condition number $\\kappa_{2}(A)$ amplifies representation and round-off errors. The problem provides a specific criterion for when this amplification becomes unacceptable: the problem is deemed numerically impractical if $\\kappa_{2}(A) \\cdot u  \\tau$, where $\\tau$ is a specified tolerance for relative error.\n\nThe overall algorithm is as follows:\n1.  Initialize the machine epsilon $u$ for IEEE-$754$ double-precision floating-point arithmetic.\n2.  For each test case, specified by a pair $(m, \\tau)$ representing the number of sample points and the impracticality tolerance, perform the following steps.\n3.  Determine the maximum polynomial degree to test, $n_{\\max} = \\min\\{m-1, 80\\}$. The constraint $n \\le m-1$ ensures that the design matrix $A \\in \\mathbb{R}^{m \\times (n+1)}$ has full column rank, which is necessary for a unique least-squares solution and a non-singular Gram matrix $A^T A$.\n4.  Generate the $m$ equispaced sample points $x_i$ on the interval $[-1, 1]$ using the formula $x_i = -1 + 2 \\frac{i}{m-1}$ for $i=0, 1, \\dots, m-1$.\n5.  Separately for the monomial and Chebyshev bases, find the minimal degree $n_{\\star} \\in \\{0, 1, \\dots, n_{\\max}\\}$ that satisfies the impracticality criterion. This is achieved by iterating $n$ from $0$ to $n_{\\max}$.\n    a. For each degree $n$, construct the corresponding $m \\times (n+1)$ design matrix, either $A_{\\mathrm{mono}}$ or $A_{\\mathrm{cheb}}$.\n    b. Compute the spectral (two-norm) condition number $\\kappa_{2}(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$. This is done by first computing the singular values of $A$ via Singular Value Decomposition (SVD). The singular values are returned in descending order, so $\\sigma_{\\max}$ is the first and $\\sigma_{\\min}$ is the last.\n    c. Check if the condition $\\kappa_{2}(A) \\cdot u  \\tau$ is met.\n    d. The first value of $n$ for which this inequality holds is the desired minimal degree $n_{\\star}$. If the loop completes without the condition being met, the result is reported as $-1$.\n\nThe construction of the design matrices is as follows:\n\nFor the monomial basis, the matrix $A_{\\mathrm{mono}}$ has entries $[A_{\\mathrm{mono}}]_{i,j} = x_i^j$ for $j=0, 1, \\dots, n$. The columns of this matrix are $\\{1, x, x^2, \\dots, x^n\\}$ evaluated at the grid points. This basis is known to be poorly conditioned, as for larger $j$, the functions $x^j$ become nearly linearly dependent on the interval $[-1, 1]$.\n\nFor the Chebyshev basis, the matrix $A_{\\mathrm{cheb}}$ has entries $[A_{\\mathrm{cheb}}]_{i,j} = T_j(x_i)$ for $j=0, 1, \\dots, n$. The functions $T_j(x)$ are the Chebyshev polynomials of the first kind. They are defined by the three-term recurrence relation:\n$$\nT_0(x) = 1 \\\\\nT_1(x) = x \\\\\nT_{j+1}(x) = 2x T_j(x) - T_{j-1}(x) \\quad \\text{for } j \\ge 1\n$$\nThis recurrence relation is used to generate the columns of $A_{\\mathrm{cheb}}$ iteratively. These polynomials form an orthogonal system with respect to a continuous weight function on $[-1, 1]$ and exhibit excellent numerical properties on discrete grids, leading to a much better-conditioned design matrix compared to the monomial basis.\n\nThe final output is a list of pairs $[n_{\\star}^{\\mathrm{mono}}, n_{\\star}^{\\mathrm{cheb}}]$ for each test case $(m, \\tau)$. This structure facilitates a direct comparison of the numerical stability of the two bases under the specified conditions.",
            "answer": "```python\nimport numpy as np\n\ndef find_n_star(m, tau, basis_type):\n    \"\"\"\n    Finds the minimal polynomial degree n_star for which the design matrix\n    becomes numerically impractical.\n\n    Args:\n        m (int): The number of sample points.\n        tau (float): The user-specified tolerance.\n        basis_type (str): The type of basis, either 'mono' or 'cheb'.\n\n    Returns:\n        int: The minimal degree n_star, or -1 if not found.\n    \"\"\"\n    u = np.finfo(float).eps\n    n_max = min(m - 1, 80)\n    x = np.linspace(-1, 1, m)\n    \n    n_star = -1\n\n    for n in range(n_max + 1):\n        # We need n+1 columns for degree n\n        num_cols = n + 1\n        \n        if basis_type == 'mono':\n            # A_ij = x_i^(j-1) is the definition but Python is 0-indexed j=0 to n\n            # A_ij = x_i^j\n            # The np.vander function with increasing=True creates columns x^0, x^1, ..., x^n\n            A = np.vander(x, N=num_cols, increasing=True)\n        elif basis_type == 'cheb':\n            # A_ij = T_j(x_i)\n            # The chebvander function creates columns T_0(x), T_1(x), ..., T_n(x)\n            A = np.polynomial.chebyshev.chebvander(x, n)\n        else:\n            raise ValueError(\"Unknown basis type\")\n\n        # The rank of the m x (n+1) matrix must be n+1.\n        # For n = m-1, this is guaranteed for distinct points.\n        if A.shape[0]  A.shape[1]:\n            # This case should not be reached due to n = m-1\n            continue\n\n        # Compute singular values\n        s = np.linalg.svd(A, compute_uv=False)\n        \n        # Condition number is sigma_max / sigma_min\n        # svd returns sorted singular values\n        if s[-1]  1e-16: # Avoid division by a near-zero singular value\n            kappa = np.inf\n        else:\n            kappa = s[0] / s[-1]\n        \n        if kappa * u  tau:\n            n_star = n\n            break\n            \n    return n_star\n\ndef solve():\n    \"\"\"\n    Main solver function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (21, 10**-12),\n        (50, 10**-2),\n        (200, 10**-8),\n    ]\n\n    all_results = []\n    \n    for m, tau in test_cases:\n        n_star_mono = find_n_star(m, tau, 'mono')\n        n_star_cheb = find_n_star(m, tau, 'cheb')\n        all_results.append([n_star_mono, n_star_cheb])\n    \n    # Format the final output string\n    # e.g., [[12, -1],[9, -1],[15, -1]]\n    output_str = \"[\" + \",\".join(f\"[{r[0]},{r[1]}]\" for r in all_results) + \"]\"\n    print(output_str)\n\nsolve()\n\n```"
        }
    ]
}