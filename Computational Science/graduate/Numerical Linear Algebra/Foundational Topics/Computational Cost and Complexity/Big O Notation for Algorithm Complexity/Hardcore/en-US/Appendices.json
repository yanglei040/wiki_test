{
    "hands_on_practices": [
        {
            "introduction": "Understanding algorithm complexity begins with the fundamental building blocks. This first practice focuses on dense matrix-vector multiplication, a core operation in countless numerical algorithms. By meticulously counting the individual floating-point operations (FLOPs) and applying the formal definition of Big O notation, you will derive its well-known $O(n^2)$ complexity from first principles, solidifying your grasp of the direct connection between an algorithm's structure and its computational cost .",
            "id": "3534517",
            "problem": "Consider dense matrix-vector multiplication in the sense of the Basic Linear Algebra Subprograms (BLAS), specifically the operation $y \\leftarrow A x$ with $A \\in \\mathbb{R}^{n \\times n}$ and $x, y \\in \\mathbb{R}^{n}$. Work within the standard floating-point operation (flop) cost model used in numerical linear algebra: each scalar floating-point addition counts as $1$ flop and each scalar floating-point multiplication counts as $1$ flop; ignore data movement, branching, and loop overhead. Assume no fused multiply-add is used.\n\nStarting from fundamental definitions of flop counting and the formal definition of Big O (Landau) notation, derive the exact flop count $F(n)$ for a straightforward, row-oriented implementation of the dense matrix-vector multiplication that computes each component $y_{i}$ as the dot product of the $i$-th row of $A$ with $x$ using a single accumulator. Then, using the formal definition of Big O notation, demonstrate that $F(n)$ is in the set $O(n^{2})$.\n\nReport the exact flop count as a single closed-form expression in $n$. Do not provide an inequality, equation, or asymptotic notation in the final answer. No rounding is required.",
            "solution": "The problem is validated as self-contained, consistent, and scientifically sound. It is a standard problem in introductory numerical linear algebra and computational complexity analysis.\n\nThe task is to determine the exact floating-point operation (flop) count for a dense matrix-vector multiplication $y \\leftarrow A x$, where $A \\in \\mathbb{R}^{n \\times n}$ and $x, y \\in \\mathbb{R}^{n}$, and then to formally prove that this count is in $O(n^2)$.\n\nFirst, we will derive the exact flop count, denoted by $F(n)$. The operation is defined component-wise for each element $y_i$ of the vector $y$, where $i$ ranges from $1$ to $n$. The formula for the $i$-th component is:\n$$\ny_i = \\sum_{j=1}^{n} A_{ij} x_j\n$$\nThis can be expanded as:\n$$\ny_i = A_{i1}x_1 + A_{i2}x_2 + \\dots + A_{in}x_n\n$$\nWe analyze the cost of computing a single component $y_i$ based on the specified flop model, where one scalar addition and one scalar multiplication each count as $1$ flop.\n\nFor a single $y_i$, the computation involves the following operations:\n$1.$ A sequence of $n$ scalar multiplications: $A_{i1}x_1, A_{i2}x_2, \\dots, A_{in}x_n$. This contributes $n$ multiplication FLOPs.\n$2.$ The summation of the $n$ products resulting from the multiplications. Adding $n$ terms together requires $n-1$ scalar additions. For example, $(A_{i1}x_1 + A_{i2}x_2)$ is one addition, adding $A_{i3}x_3$ to this sum is a second addition, and so on, until the $n$-th term is added, which constitutes the $(n-1)$-th addition. The use of a single accumulator for the dot product corresponds to this sequential summation. This contributes $n-1$ addition FLOPs.\n\nThe total number of FLOPs to compute a single component $y_i$ is the sum of the multiplication and addition FLOPs:\n$$\n\\text{Flops per component} = n + (n-1) = 2n-1\n$$\nSince the vector $y$ has $n$ components ($y_1, y_2, \\dots, y_n$), and the computation of each component is independent and has the same cost, the total flop count $F(n)$ for the entire matrix-vector multiplication is the number of components multiplied by the FLOPs per component:\n$$\nF(n) = n \\times (2n-1)\n$$\nExpanding this expression gives the final closed-form for the exact flop count:\n$$\nF(n) = 2n^2 - n\n$$\nNext, we must use the formal definition of Big O notation to show that $F(n)$ is in the set $O(n^2)$. A function $f(n)$ is in $O(g(n))$ if there exist a positive constant $C$ and a natural number $n_0$ such that for all $n \\geq n_0$, the inequality $|f(n)| \\leq C |g(n)|$ holds.\n\nIn this case, our function is $f(n) = F(n) = 2n^2 - n$, and we want to show it belongs to $O(n^2)$, so $g(n) = n^2$.\nWe need to find constants $C  0$ and $n_0 \\in \\mathbb{N}$ such that for all $n \\geq n_0$:\n$$\n|2n^2 - n| \\leq C |n^2|\n$$\nSince $n$ represents the dimension of the matrix, $n$ must be a positive integer, so $n \\geq 1$.\nFor $n \\geq 1$, the term $2n^2 - n = n(2n - 1)$ is non-negative, so $|2n^2 - n| = 2n^2 - n$. Also, for $n \\geq 1$, $|n^2| = n^2$.\nThe inequality becomes:\n$$\n2n^2 - n \\leq C n^2\n$$\nTo find a suitable value for $C$, we can manipulate the inequality. Since $n^2  0$ for $n \\geq 1$, we can divide both sides by $n^2$:\n$$\n2 - \\frac{1}{n} \\leq C\n$$\nWe need to find a constant $C$ that satisfies this for all $n$ from some $n_0$ onwards. Let us choose $n_0 = 1$. For all $n \\geq 1$, we know that $0  \\frac{1}{n} \\leq 1$.\nThis implies $2 - \\frac{1}{n}  2$.\nTherefore, we can choose any constant $C$ such that $C \\geq 2$. A simple and valid choice is $C = 2$.\n\nLet's verify our choice. We choose $C = 2$ and $n_0 = 1$. We must show that for all $n \\geq 1$, the inequality $2n^2 - n \\leq 2n^2$ holds.\nSubtracting $2n^2$ from both sides gives:\n$$\n-n \\leq 0\n$$\nThis inequality is true for all positive integers $n$, and thus certainly true for all $n \\geq 1$.\nSince we have found a constant $C = 2$ and an integer $n_0 = 1$ that satisfy the definition, we have formally demonstrated that $F(n) = 2n^2 - n$ is in $O(n^2)$.\n\nThe final answer requested is the exact flop count as a single closed-form expression in $n$. This is the function $F(n)$ we derived.",
            "answer": "$$\n\\boxed{2n^{2} - n}\n$$"
        },
        {
            "introduction": "We now advance from basic operations to the workhorses of numerical linear algebra: matrix factorizations. This exercise asks you to analyze the Cholesky factorization, a key algorithm for solving systems of linear equations with symmetric positive definite matrices. Deriving its $O(n^3)$ complexity requires a more sophisticated analysis involving the summation of costs over nested loops, providing essential practice for evaluating the efficiency of more complex, iterative algorithms .",
            "id": "3534512",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a dense symmetric positive definite (SPD) matrix. Consider the unblocked left-looking Cholesky factorization algorithm that computes a lower-triangular matrix $L$ with positive diagonal entries such that $A = L L^{\\top}$, using the following column-oriented procedure:\n- For $k = 1, \\dots, n$:\n  - Compute the diagonal element\n    $$L_{k k} = \\sqrt{A_{k k} - \\sum_{s=1}^{k-1} L_{k s}^{2}}.$$\n  - For each $i = k+1, \\dots, n$, compute\n    $$L_{i k} = \\frac{A_{i k} - \\sum_{s=1}^{k-1} L_{i s} L_{k s}}{L_{k k}}.$$\nAdopt a scalar operation cost model in which each of the following scalar arithmetic operations has unit cost: addition, subtraction, multiplication, division, and square root. Let $T(n)$ denote the total number of such scalar operations performed by the algorithm on an $n \\times n$ input.\n\nStarting from first principles—namely, the definitions of the Cholesky factorization, the above algorithmic steps, and the formal asymptotic definition of Big O notation $O(\\cdot)$—derive a closed-form expression for $T(n)$ and use it to conclude the asymptotic complexity class of the algorithm. Your derivation must explicitly account for the counts of scalar operations in each inner computation and sum them exactly across all loops.\n\nExpress your final answer as a single simplified analytic expression for $T(n)$ in terms of $n$ (no units). Do not provide inequalities or equations in the final answer.",
            "solution": "The problem asks for a derivation of the exact number of scalar arithmetic operations, denoted by $T(n)$, for an unblocked left-looking Cholesky factorization of an $n \\times n$ dense symmetric positive definite (SPD) matrix $A$. We are also asked to determine the asymptotic complexity of the algorithm. The allowed scalar operations, each with a unit cost, are addition, subtraction, multiplication, division, and square root.\n\nThe algorithm proceeds by computing the columns of the lower-triangular matrix $L$ sequentially. The total cost $T(n)$ is the sum of the costs of each step of the outer loop, which iterates from $k=1$ to $n$. Let $C_k$ be the cost of the operations performed during the $k$-th iteration of this loop. Then, $T(n) = \\sum_{k=1}^{n} C_k$.\n\nIn the $k$-th iteration, two main computations are performed:\n1. The calculation of the diagonal element $L_{kk}$.\n2. The calculation of the off-diagonal elements in the $k$-th column, $L_{ik}$, for $i = k+1, \\dots, n$.\n\nLet us analyze the cost of these computations for a general iteration $k$.\n\n**Cost of computing $L_{kk}$**\nThe formula is $L_{k k} = \\sqrt{A_{k k} - \\sum_{s=1}^{k-1} L_{k s}^{2}}$.\nFor $k=1$, the summation $\\sum_{s=1}^{0}$ is empty and evaluates to $0$. The expression simplifies to $L_{11} = \\sqrt{A_{11}}$. This requires $1$ square root operation.\nFor $k  1$, the computation involves several steps:\n- The sum $\\sum_{s=1}^{k-1} L_{k s}^{2}$ requires computing $k-1$ squares ($L_{ks}^2$), which costs $k-1$ multiplications.\n- These $k-1$ terms are summed, which requires $k-2$ additions.\n- Then, there is $1$ subtraction ($A_{kk} - \\dots$).\n- Finally, there is $1$ square root operation.\nThe total cost for computing $L_{kk}$ for $k  1$ is $(k-1) + (k-2) + 1 + 1 = 2k-1$ operations.\nLet's denote the cost for $L_{kk}$ as $C_{kk}$.\n$C_{kk} = \\begin{cases} 1  k=1 \\\\ 2k-1  k  1 \\end{cases}$.\n\n**Cost of computing $L_{ik}$ for $i=k+1, \\dots, n$**\nFor a fixed $k$, a loop runs for $i$ from $k+1$ to $n$. This loop has $n-k$ iterations. Inside this loop, the element $L_{ik}$ is computed using the formula:\n$L_{i k} = \\frac{A_{i k} - \\sum_{s=1}^{k-1} L_{i s} L_{k s}}{L_{k k}}$.\nLet $c_{ik}$ be the cost of computing a single element $L_{ik}$.\nFor $k=1$, the sum is empty. The formula becomes $L_{i1} = \\frac{A_{i1}}{L_{11}}$. This requires $1$ division.\nFor $k  1$, the computation involves:\n- The sum $\\sum_{s=1}^{k-1} L_{i s} L_{k s}$ requires $k-1$ multiplications and $k-2$ additions.\n- Then, there is $1$ subtraction ($A_{ik} - \\dots$).\n- Finally, there is $1$ division by $L_{kk}$.\nThe total cost for computing one element $L_{ik}$ for $k  1$ is $(k-1) + (k-2) + 1 + 1 = 2k-1$ operations.\nSo, the cost $c_{ik}$ is:\n$c_{ik} = \\begin{cases} 1  k=1 \\\\ 2k-1  k  1 \\end{cases}$.\n\nThe total cost for the inner loop over $i$ at a fixed step $k$ is $(n-k) \\times c_{ik}$.\n\n**Total cost for iteration $k$, $C_k$**\nWe can now find the total cost $C_k$ for each iteration $k$ of the outer loop.\nFor $k=1$:\n$C_1 = C_{11} + \\sum_{i=2}^{n} c_{i1} = 1 + (n-1) \\times 1 = n$.\nFor $k  1$:\n$C_k = C_{kk} + \\sum_{i=k+1}^{n} c_{ik} = (2k-1) + (n-k) \\times (2k-1)$.\nFactoring out $(2k-1)$, we get $C_k = (1 + n-k)(2k-1) = (n-k+1)(2k-1)$.\n\nWe can check if this general formula for $k1$ also holds for $k=1$:\nFor $k=1$, the formula gives $C_1 = (n-1+1)(2(1)-1) = n \\times 1 = n$.\nThe formula is consistent. Therefore, we can express the cost for any iteration $k \\in \\{1, \\dots, n\\}$ as $C_k = (n-k+1)(2k-1)$.\n\n**Total Operation Count $T(n)$**\nThe total number of operations is the sum of costs for all iterations:\n$$T(n) = \\sum_{k=1}^{n} C_k = \\sum_{k=1}^{n} (n-k+1)(2k-1).$$\nTo simplify this summation, we perform a change of index. Let $j = n-k+1$. As $k$ runs from $1$ to $n$, the new index $j$ runs from $n$ down to $1$. The transformation for $k$ is $k = n-j+1$.\nSubstituting this into the sum:\n$$T(n) = \\sum_{j=1}^{n} j \\cdot \\left( 2(n-j+1) - 1 \\right).$$\nThe term inside the sum is $j(2n - 2j + 2 - 1) = j(2n - 2j + 1) = 2nj - 2j^2 + j$.\nThe summation becomes:\n$$T(n) = \\sum_{j=1}^{n} (2nj - 2j^2 + j) = \\sum_{j=1}^{n} (-2j^2 + (2n+1)j).$$\nUsing the properties of linearity of summation:\n$$T(n) = -2 \\sum_{j=1}^{n} j^2 + (2n+1) \\sum_{j=1}^{n} j.$$\nWe use the standard formulas for the sum of the first $n$ integers and the sum of the first $n$ squares:\n$\\sum_{j=1}^{n} j = \\frac{n(n+1)}{2}$\n$\\sum_{j=1}^{n} j^2 = \\frac{n(n+1)(2n+1)}{6}$\nSubstituting these formulas into the expression for $T(n)$:\n$$T(n) = -2 \\left( \\frac{n(n+1)(2n+1)}{6} \\right) + (2n+1) \\left( \\frac{n(n+1)}{2} \\right).$$\n$$T(n) = -\\frac{n(n+1)(2n+1)}{3} + \\frac{n(n+1)(2n+1)}{2}.$$\nFactoring out the common term $n(n+1)(2n+1)$:\n$$T(n) = n(n+1)(2n+1) \\left( \\frac{1}{2} - \\frac{1}{3} \\right) = n(n+1)(2n+1) \\left( \\frac{3-2}{6} \\right) = \\frac{n(n+1)(2n+1)}{6}.$$\nThis is the closed-form expression for the total number of scalar operations. We can expand this to see the leading term:\n$$T(n) = \\frac{1}{6} (n(2n^2 + 3n + 1)) = \\frac{1}{6} (2n^3 + 3n^2 + n) = \\frac{1}{3}n^3 + \\frac{1}{2}n^2 + \\frac{1}{6}n.$$\n\n**Asymptotic Complexity**\nThe asymptotic complexity is determined by the highest-order term in the polynomial expression for $T(n)$. The dominant term is $\\frac{1}{3}n^3$.\nBy the formal definition of Big O notation, for a function $f(n)$ to be $O(g(n))$, there must exist a constant $C0$ and an integer $n_0$ such that $|f(n)| \\le C|g(n)|$ for all $n \\ge n_0$.\nIn our case, $T(n) = \\frac{1}{3}n^3 + \\frac{1}{2}n^2 + \\frac{1}{6}n$. For $n \\ge 1$:\n$$T(n) = \\frac{1}{3}n^3 + \\frac{1}{2}n^2 + \\frac{1}{6}n \\le \\frac{1}{3}n^3 + \\frac{1}{2}n^3 + \\frac{1}{6}n^3 = \\left(\\frac{1}{3} + \\frac{1}{2} + \\frac{1}{6}\\right)n^3 = \\frac{2+3+1}{6}n^3 = n^3.$$\nThus, we can choose $C=1$ and $n_0=1$, which satisfies the definition. The asymptotic complexity of the algorithm is $O(n^3)$.\nThe problem asks for the simplified analytic expression for $T(n)$.",
            "answer": "$$\\boxed{\\frac{n(n+1)(2n+1)}{6}}$$"
        },
        {
            "introduction": "Not all fast algorithms are iterative; some of the most powerful breakthroughs come from recursive, divide-and-conquer strategies. This final practice explores Strassen's revolutionary algorithm for matrix multiplication, which demonstrates that this fundamental task is not inherently an $O(n^3)$ problem. You will set up and analyze the recurrence relation that governs its complexity, revealing the surprising non-integer exponent in its famous $O(n^{\\log_{2} 7})$ performance and mastering a crucial tool for analyzing recursive algorithms .",
            "id": "3534539",
            "problem": "Consider multiplying two dense square matrices of dimension $n \\times n$ over the real numbers using Strassen’s method of block recursion. Assume the Random Access Machine (RAM) model in which each scalar addition and each scalar multiplication has unit cost. At one level of the recursion, Strassen’s construction partitions each input matrix into $2 \\times 2$ blocks of dimension $(n/2) \\times (n/2)$, performs exactly $7$ recursive multiplications of $(n/2) \\times (n/2)$ submatrices, and combines these results using a fixed finite number of additions and subtractions of $(n/2) \\times (n/2)$ matrices. Let $T(n)$ denote the total number of scalar arithmetic operations (additions and multiplications) required to multiply two $n \\times n$ matrices by Strassen’s algorithm, and suppose $n$ is a power of $2$ so that no floors or ceilings arise. Use the facts that adding or subtracting two $(m \\times m)$ matrices costs $\\Theta(m^{2})$ scalar operations and that the number of block additions and subtractions performed at each recursion level is a fixed constant independent of $n$.\n\nStarting from these principles, derive and analyze the divide-and-conquer recurrence satisfied by $T(n)$, explicitly outline the recursion tree it induces, and determine the dominant scaling of $T(n)$ as $n \\to \\infty$. Provide your final answer as the closed-form analytic expression representing the leading-order asymptotic arithmetic complexity of Strassen’s algorithm in $n$, ignoring constant factors and lower-order terms. Your final answer must be a single expression in $n$ (no inequalities or Big O symbols inside the final box).",
            "solution": "The problem asks for the asymptotic complexity of Strassen's matrix multiplication algorithm for two dense square matrices of dimension $n \\times n$. We are given that $n$ is a power of $2$, say $n = 2^k$ for some integer $k \\ge 0$. The analysis is based on a divide-and-conquer recurrence relation.\n\nLet $T(n)$ be the total number of scalar arithmetic operations (additions, subtractions, and multiplications) required to multiply two $n \\times n$ matrices using this algorithm. The method involves partitioning each $n \\times n$ matrix into four $(n/2) \\times (n/2)$ submatrices. It then recursively computes $7$ matrix products of size $(n/2) \\times (n/2)$. The cost of these recursive calls is $7 \\cdot T(n/2)$.\n\nAfter the recursive multiplications, the results are combined using a fixed number of matrix additions and subtractions. Let this constant number be $c_{ops}$. The problem states this number is finite and independent of $n$. The matrices being added or subtracted are of size $(n/2) \\times (n/2)$. Adding or subtracting two $m \\times m$ matrices requires $m^2$ scalar operations. Thus, one addition/subtraction of $(n/2) \\times (n/2)$ matrices costs $(n/2)^2 = n^2/4$ scalar operations. The total cost for this combination step at one level of recursion is therefore $c_{ops} \\cdot (n^2/4)$. This cost is proportional to $n^2$, so we can write it as $C n^2$ for some positive constant $C$.\n\nCombining the costs of the recursive calls and the combination step, we arrive at the recurrence relation for $T(n)$:\n$$T(n) = 7 T(n/2) + C n^2$$\nThe base case for the recursion is the multiplication of two $1 \\times 1$ matrices, which consists of a single scalar multiplication. Hence, $T(1) = 1$.\n\nTo determine the dominant scaling of $T(n)$, we can analyze the recursion tree induced by this recurrence.\nThe tree has a depth of $k = \\log_2(n)$, since at each level $j$ (from $j=0$ at the root to $j=k$ at the leaves), the problem size is reduced from $n/2^j$ to $n/2^{j+1}$. The recursion stops when the size is $1$, i.e., $n/2^k = 1$.\n\nLet's sum the costs at each level of the tree:\nLevel $0$ (root): The work done is the combination cost for the initial problem of size $n$, which is $C n^2$.\nLevel $1$: There are $7$ subproblems, each of size $n/2$. The total work done at this level is $7 \\times C(n/2)^2 = 7 C (n^2/4) = C n^2 (7/4)^1$.\nLevel $2$: There are $7^2$ subproblems, each of size $n/4$. The total work done at this level is $7^2 \\times C(n/4)^2 = 49 C (n^2/16) = C n^2 (7/4)^2$.\nLevel $j$: In general, at level $j$, there are $7^j$ subproblems, each of size $n/2^j$. The total combination work done at this level is $7^j \\times C(n/2^j)^2 = 7^j C (n^2/4^j) = C n^2 (7/4)^j$.\n\nThe total work done at all internal nodes of the recursion tree (i.e., the total cost of all additions and subtractions) is the sum of the work at levels $j=0$ to $j=k-1$:\n$$W_{internal} = \\sum_{j=0}^{k-1} C n^2 \\left(\\frac{7}{4}\\right)^j$$\nThis is a geometric series with ratio $r = 7/4$. The sum is:\n$$W_{internal} = C n^2 \\frac{(7/4)^k - 1}{(7/4) - 1} = C n^2 \\frac{(7/4)^{\\log_2(n)} - 1}{3/4} = \\frac{4C}{3} n^2 \\left(\\left(\\frac{7}{4}\\right)^{\\log_2(n)} - 1\\right)$$\nWe can simplify the term $(7/4)^{\\log_2(n)}$:\n$$\\left(\\frac{7}{4}\\right)^{\\log_2(n)} = \\frac{7^{\\log_2(n)}}{4^{\\log_2(n)}} = \\frac{n^{\\log_2(7)}}{(2^2)^{\\log_2(n)}} = \\frac{n^{\\log_2(7)}}{2^{2\\log_2(n)}} = \\frac{n^{\\log_2(7)}}{2^{\\log_2(n^2)}} = \\frac{n^{\\log_2(7)}}{n^2}$$\nSubstituting this back into the expression for $W_{internal}$:\n$$W_{internal} = \\frac{4C}{3} n^2 \\left(\\frac{n^{\\log_2(7)}}{n^2} - 1\\right) = \\frac{4C}{3} n^{\\log_2(7)} - \\frac{4C}{3} n^2$$\n\nThe total cost $T(n)$ is the sum of the internal work and the work done at the leaves.\nThe leaves correspond to the base cases of the recursion. The number of leaves at depth $k = \\log_2(n)$ is $7^k = 7^{\\log_2(n)} = n^{\\log_2(7)}$. Each leaf represents a $1 \\times 1$ matrix multiplication, which has a cost of $T(1)=1$.\nThe total work at the leaves is:\n$$W_{leaves} = 7^k \\cdot T(1) = n^{\\log_2(7)} \\cdot 1 = n^{\\log_2(7)}$$\n\nThe total cost is $T(n) = W_{internal} + W_{leaves}$:\n$$T(n) = \\left(\\frac{4C}{3} n^{\\log_2(7)} - \\frac{4C}{3} n^2\\right) + n^{\\log_2(7)} = \\left(1 + \\frac{4C}{3}\\right) n^{\\log_2(7)} - \\frac{4C}{3} n^2$$\nTo determine the dominant scaling as $n \\to \\infty$, we compare the exponents of $n$. The exponents are $\\log_2(7)$ and $2$.\nWe know that $2^2 = 4$ and $2^3 = 8$. Since $4  7  8$, we have $\\log_2(4)  \\log_2(7)  \\log_2(8)$, which implies $2  \\log_2(7)  3$. Numerically, $\\log_2(7) \\approx 2.807$.\nBecause $\\log_2(7)  2$, the term $n^{\\log_2(7)}$ grows faster than $n^2$. Therefore, the term $\\left(1 + \\frac{4C}{3}\\right) n^{\\log_2(7)}$ is the dominant term in the expression for $T(n)$.\n\nThe leading-order asymptotic arithmetic complexity is the part of the expression that grows fastest with $n$. Ignoring the constant factor $\\left(1 + \\frac{4C}{3}\\right)$ and the lower-order term $-\\frac{4C}{3} n^2$, the complexity is determined by the scaling of $n^{\\log_2(7)}$.\n\nThis result can also be confirmed using the Master Theorem for recurrences of the form $T(n) = a T(n/b) + f(n)$. In our case, $a=7$, $b=2$, and $f(n) = C n^2$. We compare $f(n)$ with $n^{\\log_b(a)} = n^{\\log_2(7)}$. Since $f(n) = C n^2$ and $2  \\log_2(7)$, we have $f(n) = O(n^{\\log_2(7) - \\epsilon})$ for some $\\epsilon  0$ (e.g., $\\epsilon = \\log_2(7) - 2$). This corresponds to Case 1 of the Master Theorem, which gives the solution $T(n) = \\Theta(n^{\\log_b(a)}) = \\Theta(n^{\\log_2(7)})$.\n\nThe question asks for the closed-form analytic expression for the leading-order asymptotic complexity, which is the functional form $n^{\\log_2(7)}$.",
            "answer": "$$\\boxed{n^{\\log_{2}(7)}}$$"
        }
    ]
}