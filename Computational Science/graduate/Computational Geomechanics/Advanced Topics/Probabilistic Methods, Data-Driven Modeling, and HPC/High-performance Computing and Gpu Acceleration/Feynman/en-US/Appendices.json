{
    "hands_on_practices": [
        {
            "introduction": "Any GPU-accelerated simulation begins with the fundamental task of distributing the computational workload, such as processing elements in a finite element mesh, across thousands of parallel threads. This requires a robust mapping from a problem-specific index (e.g., an element ID) to a unique thread's global index, which must cover all tasks and include a safety guard for superfluous threads. Mastering this fundamental indexing pattern is the first and most critical step in parallel programming on GPUs, forming the basis for virtually all kernel implementations. ",
            "id": "3529536",
            "problem": "Consider the parallel element-wise assembly step in a two-dimensional finite element method for computational geomechanics, executed on a Graphics Processing Unit (GPU) under the Single Instruction, Multiple Threads (SIMT) model. A mesh contains $N_e$ triangular elements that must each be processed exactly once by a distinct GPU thread. Threads are launched in a one-dimensional grid of blocks: each block contains $B$ threads, there are $G$ blocks in the grid, and each thread within a block and each block within the grid is indexed starting at $0$. The launch must be sufficient to cover all elements, but it is permitted to launch more threads than elements, provided superfluous threads safely perform no work. Your task is to determine the minimal grid size and an indexing function that assigns at most one element to each thread, covers all $N_e$ elements exactly once, and guarantees safety for any extra threads. Your program must, for each test case, compute the number of blocks $G$ and the total launched threads $T$, and verify with a logical argument that the indexing maps each element index to exactly one thread and that any extra threads perform no work.\n\nUse only first principles acceptable in high-performance computing and GPU acceleration for computational geomechanics: the SIMT execution model, the uniqueness/existence guaranteed by the integer division algorithm, and the definition of a guard condition based on an inequality. Do not rely on any pre-specified shortcut formula; derive any required expression from these bases. Indices are integers without physical units.\n\nDefine the following requirements for your design:\n- You must choose the smallest integer grid size $G$ compatible with complete coverage, given the provided $N_e$ and block size $B$.\n- You must state an explicit mapping from block index $b$ and thread index $t$ to a global index $g$ that a thread would use to decide whether to process an element.\n- You must include a safety guard condition expressed as an inequality that ensures threads corresponding to out-of-range global indices perform no work.\n- You must justify, using the integer division algorithm, that every element index $e \\in \\{0,1,\\dots,N_e-1\\}$ is mapped by exactly one valid thread under your mapping and guard.\n\nFor each test case, your program must output a result structured as a list of three values:\n- $G$: the minimal number of blocks (an integer),\n- $T$: the total number of launched threads, equal to $G \\cdot B$ (an integer),\n- a boolean flag indicating whether the mapping and guard together imply that each of the $N_e$ elements is processed exactly once and that any thread whose global index would exceed the valid range performs no work.\n\nTest suite (use these exact values):\n- Case 1 (typical large, non-divisible): $N_e = 10^6$, $B = 256$.\n- Case 2 (exact divisibility): $N_e = 1024$, $B = 256$.\n- Case 3 (small mesh fewer than one block): $N_e = 10$, $B = 256$.\n- Case 4 (single-thread blocks): $N_e = 17$, $B = 1$.\n- Case 5 (very large, non-divisible): $N_e = 10^7$, $B = 1024$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a three-item list in order. For example, the output format must be of the form\n\"[[G1,T1,ok1],[G2,T2,ok2],...]\" on a single line with no additional characters or whitespace. All outputs are integers and booleans without physical units.",
            "solution": "The problem requires the design and verification of a mapping scheme to assign $N_e$ computational tasks (element assemblies) to a grid of threads on a Graphics Processing Unit (GPU). The design must be derived from first principles of parallel computing.\n\nLet $N_e$ be the total number of triangular elements in the mesh, indexed from $0$ to $N_e-1$. Let $B$ be the number of threads per block, a fixed hardware/launch parameter. The threads within a block are indexed by $t \\in \\{0, 1, \\dots, B-1\\}$. The blocks are arranged in a one-dimensional grid of size $G$, indexed by $b \\in \\{0, 1, \\dots, G-1\\}$. The total number of launched threads is $T = G \\cdot B$.\n\nThe primary goal is to map each element index $e \\in \\{0, 1, \\dots, N_e-1\\}$ to exactly one thread, ensuring all elements are processed. This requires defining a global thread index, determining the minimum required grid size $G$, and establishing a safety condition for threads that are not assigned to any element.\n\nFirst, we define a mapping from a thread's local identifiers $(b, t)$ to a unique global index $g$. A standard and effective mapping is a linear, row-major ordering:\n$$g(b, t) = b \\cdot B + t$$\nThis function maps each unique pair $(b, t)$ to a unique integer $g$. The uniqueness is guaranteed by the Division Algorithm. If we assume $g(b_1, t_1) = g(b_2, t_2)$, then $b_1 B + t_1 = b_2 B + t_2$. Since $0 \\le t_1 < B$ and $0 \\le t_2 < B$, the uniqueness of the quotient and remainder upon division by $B$ implies that we must have $b_1 = b_2$ and $t_1 = t_2$. Thus, the mapping from the set of thread identifiers $\\{(b,t)\\}$ to the set of global indices $\\{0, 1, \\dots, T-1\\}$ is a bijection.\n\nNext, we determine the minimal number of blocks, $G$, required to process all $N_e$ elements. The most direct assignment strategy is to map element $e$ to the thread with global index $g=e$. To ensure every element can be assigned a thread, the total number of launched threads $T$ must be at least $N_e$.\n$$T \\ge N_e$$\nSubstituting $T = G \\cdot B$, we have:\n$$G \\cdot B \\ge N_e$$\nSince $G$ must be an integer, we seek the smallest integer $G$ that satisfies this inequality. Dividing by the positive integer $B$, we get $G \\ge N_e / B$. The smallest integer satisfying this is the ceiling of the division, $G = \\lceil N_e / B \\rceil$.\n\nAs the problem requires a derivation from the integer division algorithm, we formalize this. Let $N_e = qB + r$, where $q$ and $r$ are integers and $0 \\le r < B$.\n- If the division is exact ($r=0$), then $N_e = qB$. The $N_e$ elements can be perfectly distributed among $q$ blocks. Thus, the minimal number of blocks is $G=q = N_e/B$.\n- If the division is not exact ($r>0$), then $q$ full blocks can process $qB$ elements. The remaining $r$ elements, where $1 \\le r \\le B-1$, require one additional block to be processed. The total number of blocks is therefore $G = q+1$.\n\nThese two cases can be unified into a single expression using integer arithmetic, which is how it would be implemented in code. The ceiling operation $\\lceil x/y \\rceil$ for positive integers $x, y$ can be computed as $(x + y - 1) // y$, where `//` denotes integer division (i.e., discarding the remainder).\nLet's verify this formula: $G = (N_e + B - 1) // B$.\nLet $N_e = qB + r$ with $0 \\le r < B$.\n$$ G = \\lfloor \\frac{N_e + B - 1}{B} \\rfloor = \\lfloor \\frac{qB + r + B - 1}{B} \\rfloor = \\lfloor q + 1 + \\frac{r-1}{B} \\rfloor = q + 1 + \\lfloor \\frac{r-1}{B} \\rfloor $$\n- If $r=0$, then $\\lfloor \\frac{-1}{B} \\rfloor = -1$ (since $B \\ge 1$), so $G = q + 1 - 1 = q$. This is correct.\n- If $r>0$, then $1 \\le r \\le B-1$, which implies $0 \\le r-1 \\le B-2$. Therefore, $0 \\le \\frac{r-1}{B} < 1$, and $\\lfloor \\frac{r-1}{B} \\rfloor = 0$. This gives $G = q + 1 + 0 = q+1$. This is also correct.\nThus, the minimal grid size is correctly given by $G = (N_e + B - 1) // B$. The total number of launched threads is $T = G \\cdot B$.\n\nWith $T \\ge N_e$, some threads may have global indices $g$ that are greater than or equal to $N_e$. These threads are superfluous and must not perform any work. This is achieved through a safety guard condition. Each thread computes its global index $g = bB+t$ and proceeds with its task only if its index corresponds to a valid element index. The condition is:\n$$g < N_e$$\nA thread with global index $g$ is assigned to process element $e=g$. If $g \\ge N_e$, the condition is false, and the thread safely terminates its execution path for this task.\n\nFinally, we must justify that this scheme processes each element $e \\in \\{0, 1, \\dots, N_e-1\\}$ exactly once.\n1.  **Existence and Uniqueness**: For any element index $e$, the integer division algorithm states that there exist unique integers $q$ and $r$ such that $e = q \\cdot B + r$ and $0 \\le r < B$. We can uniquely identify a thread by setting its block index $b=q$ and its thread-in-block index $t=r$. Therefore, for each element $e$, there is one and only one pair $(b,t)$ that can generate it via the global index formula $g = bB+t=e$.\n\n2.  **Completeness**: We must show that the thread $(b,t)$ identified for element $e$ is actually launched. The blocks that are launched have indices from $0$ to $G-1$. The block index for element $e$ is $b = e // B = \\lfloor e/B \\rfloor$. Since $e < N_e$, it follows that $e/B < N_e/B$. As $b$ is an integer, we have $b = \\lfloor e/B \\rfloor \\le e/B < N_e/B \\le \\lceil N_e/B \\rceil = G$. Because $b$ is an integer and $G$ is an integer, $b < G$ strictly holds. This confirms that the block containing the thread responsible for element $e$ is always within the launched grid of $G$ blocks.\n\n3.  **Correctness and Safety**: The thread $(b,t)$ responsible for element $e$ computes its global index $g=e$. The safety check is $g < N_e$. Since we started with an element $e$ such that $e < N_e$, this condition is always true for threads assigned to valid elements. Conversely, any thread whose computed global index $g$ satisfies $g \\ge N_e$ will fail the check and perform no work.\n\nThis completes the derivation and proof. The scheme guarantees that each of the $N_e$ elements is processed by exactly one thread, and all superfluous threads remain idle. The boolean flag required in the output will therefore be `True` for all test cases, as the logic is sound by construction.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates GPU grid parameters for a set of test cases based on\n    first principles of thread-to-task mapping.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (Ne, B), where Ne is the number of elements\n    # and B is the block size (threads per block).\n    test_cases = [\n        (10**6, 256),\n        (1024, 256),\n        (10, 256),\n        (17, 1),\n        (10**7, 1024),\n    ]\n\n    results = []\n    for Ne, B in test_cases:\n        # 1. Calculate the minimal number of blocks (G).\n        # The number of blocks G must be large enough such that G * B >= Ne.\n        # The smallest integer G satisfying this is ceil(Ne / B).\n        # In integer arithmetic, this is equivalent to (Ne + B - 1) // B.\n        # This formula is derived from the properties of integer division\n        # and correctly handles both divisible and non-divisible cases.\n        G = (Ne + B - 1) // B\n\n        # 2. Calculate the total number of launched threads (T).\n        # This is the product of the number of blocks and the threads per block.\n        T = G * B\n\n        # 3. Verify the correctness of the mapping.\n        # The mapping g = b*B + t and the guard condition g < Ne ensure\n        # that for every element index e in [0, Ne-1], exactly one thread\n        # (with g = e) will process it. This is guaranteed by the\n        # uniqueness property of the integer division algorithm. Any thread with\n        # g >= Ne will not perform work due to the guard. Thus, the logic\n        # is always correct by construction.\n        is_correct = True\n\n        results.append([G, T, is_correct])\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists,\n    # with no whitespace between elements.\n    # e.g., \"[[G1,T1,True],[G2,T2,True]]\"\n    result_strings = [f\"[{g},{t},{str(ok)}]\" for g, t, ok in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a kernel is correctly executing on the GPU, the next challenge is to maximize its performance by making full use of the hardware's parallel processing capabilities. This exercise explores the concept of \"occupancy,\" a key metric determined by how a kernel's resource consumption, such as registers per thread and shared memory per block, fits within the architectural limits of a Streaming Multiprocessor (SM). By calculating occupancy, you learn to identify the specific resources that limit parallelism, which is the essential first step toward optimizing your code to hide memory latency and improve throughput. ",
            "id": "3529483",
            "problem": "A three-dimensional poroelastic finite element assembly kernel for a large-scale basin model is ported to a Graphics Processing Unit (GPU) to accelerate the computation of element-level stiffness and coupling matrices. Each thread updates one Gauss-point state and accumulates contributions in shared memory before a warp-synchronous reduction. Due to the constitutive update and tensor algebra, the kernel uses $R_{t}=64$ registers per thread. A per-block scratchpad holds shape function gradients, local matrices, and accumulators, requiring $S_{b}=48\\ \\mathrm{kB}$ of shared memory per block. The GPU has per-Streaming Multiprocessor (SM) resource limits given by $R_{\\max}=64\\times 1024$ registers per SM, $S_{\\max}=100\\ \\mathrm{kB}$ shared memory per SM, and $T_{\\max}=2048$ threads per SM. Assume threads are scheduled in warps of size $W=32$ and the kernel is launched with $B_{t}=256$ threads per block. Assume the architectural cap on concurrent blocks per SM is sufficiently large to be non-limiting for this kernel, and that resource allocations have no granularity overhead beyond the stated quantities. Use $1\\ \\mathrm{kB}=1024\\ \\mathrm{B}$.\n\nUsing only the fundamental resource constraints that (i) the total registers used by all resident blocks on an SM cannot exceed $R_{\\max}$, (ii) the total shared memory used by all resident blocks on an SM cannot exceed $S_{\\max}$, and (iii) the total resident threads on an SM cannot exceed $T_{\\max}$, compute the maximum number of resident blocks per SM and the resulting occupancy. Define occupancy as the ratio of active warps to the maximum possible warps per SM, and express the occupancy as a decimal fraction in exact form. Provide your final answer as two values in the order: maximum resident blocks per SM, occupancy. No rounding is required, and no units should be included in the final answer.",
            "solution": "The objective is to compute the maximum number of concurrent blocks per SM, denoted as $N_{B, \\text{max}}$, and the corresponding theoretical occupancy, $\\mathcal{O}$. The maximum number of blocks is constrained by the limited resources of an SM, namely registers, shared memory, and the thread scheduling capacity. We must calculate the maximum number of blocks supported by each resource individually and then determine the overall maximum as the minimum of these values.\n\nThe givens from the problem are:\n- Registers per thread: $R_{t} = 64$\n- Shared memory per block: $S_{b} = 48\\ \\mathrm{kB}$\n- Threads per block: $B_{t} = 256$\n- Maximum registers per SM: $R_{\\max} = 64 \\times 1024 = 65536$\n- Maximum shared memory per SM: $S_{\\max} = 100\\ \\mathrm{kB}$\n- Maximum threads per SM: $T_{\\max} = 2048$\n- Warp size: $W = 32$\n\nLet $N_B$ be the number of resident blocks on a single SM.\n\nFirst, we determine the limit on $N_B$ imposed by the total number of available registers. The total registers required by one block is the product of threads per block and registers per thread:\n$$R_{\\text{block}} = B_{t} \\times R_{t} = 256 \\times 64 = 16384\\ \\text{registers}$$\nThe total number of registers required by $N_B$ blocks is $N_B \\times R_{\\text{block}}$. This must not exceed the SM's register capacity, $R_{\\max}$.\n$$N_B \\times R_{\\text{block}} \\le R_{\\max}$$\nThe maximum number of blocks supported by the register file, $N_{B, \\text{reg}}$, is therefore:\n$$N_{B, \\text{reg}} = \\left\\lfloor \\frac{R_{\\max}}{R_{\\text{block}}} \\right\\rfloor = \\left\\lfloor \\frac{65536}{16384} \\right\\rfloor = \\lfloor 4 \\rfloor = 4$$\n\nSecond, we determine the limit imposed by the shared memory capacity. Each block requires $S_b = 48\\ \\mathrm{kB}$. The total shared memory required by $N_B$ blocks is $N_B \\times S_b$. This cannot exceed the SM's shared memory capacity, $S_{\\max}$.\n$$N_B \\times S_b \\le S_{\\max}$$\nThe maximum number of blocks supported by shared memory, $N_{B, \\text{smem}}$, is:\n$$N_{B, \\text{smem}} = \\left\\lfloor \\frac{S_{\\max}}{S_b} \\right\\rfloor = \\left\\lfloor \\frac{100\\ \\mathrm{kB}}{48\\ \\mathrm{kB}} \\right\\rfloor = \\lfloor 2.0833... \\rfloor = 2$$\n\nThird, we determine the limit imposed by the maximum number of resident threads. Each block consists of $B_t = 256$ threads. The total number of threads for $N_B$ blocks is $N_B \\times B_t$. This cannot exceed the SM's thread capacity, $T_{\\max}$.\n$$N_B \\times B_t \\le T_{\\max}$$\nThe maximum number of blocks supported by the thread scheduler, $N_{B, \\text{threads}}$, is:\n$$N_{B, \\text{threads}} = \\left\\lfloor \\frac{T_{\\max}}{B_t} \\right\\rfloor = \\left\\lfloor \\frac{2048}{256} \\right\\rfloor = \\lfloor 8 \\rfloor = 8$$\n\nThe overall maximum number of resident blocks per SM, $N_{B, \\text{max}}$, is the minimum of these three limits, as all constraints must be satisfied simultaneously. The problem states that the architectural cap on blocks per SM is non-limiting.\n$$N_{B, \\text{max}} = \\min(N_{B, \\text{reg}}, N_{B, \\text{smem}}, N_{B, \\text{threads}}) = \\min(4, 2, 8) = 2$$\nThus, a maximum of $2$ blocks can be resident on the SM, with the limiting resource being shared memory.\n\nNext, we compute the occupancy, $\\mathcal{O}$. Occupancy is defined as the ratio of active warps to the maximum possible warps per SM.\nThe number of active threads for $N_{B, \\text{max}}$ blocks is:\n$$T_{\\text{active}} = N_{B, \\text{max}} \\times B_t = 2 \\times 256 = 512\\ \\text{threads}$$\nThe number of active warps is the number of active threads divided by the warp size, $W$:\n$$W_{\\text{active}} = \\frac{T_{\\text{active}}}{W} = \\frac{512}{32} = 16\\ \\text{warps}$$\nThe maximum number of warps an SM can support is the maximum number of threads divided by the warp size:\n$$W_{\\max} = \\frac{T_{\\max}}{W} = \\frac{2048}{32} = 64\\ \\text{warps}$$\nThe occupancy is the ratio of these two quantities:\n$$\\mathcal{O} = \\frac{W_{\\text{active}}}{W_{\\max}} = \\frac{16}{64} = \\frac{1}{4}$$\nAlternatively, occupancy can be directly calculated as the ratio of active threads to maximum threads:\n$$\\mathcal{O} = \\frac{T_{\\text{active}}}{T_{\\max}} = \\frac{N_{B, \\text{max}} \\times B_t}{T_{\\max}} = \\frac{2 \\times 256}{2048} = \\frac{512}{2048} = \\frac{1}{4}$$\nAs a decimal fraction, this is $0.25$.\n\nThe final results are a maximum of $2$ resident blocks per SM and an occupancy of $0.25$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2 & 0.25 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world geomechanics simulations often involve datasets too large to fit entirely in GPU memory, requiring a continuous flow of data from the host CPU. This practice guides you through building a performance model to contrast naive data transfers using pageable memory with an advanced pipelining technique that overlaps computation with data movement using page-locked (pinned) memory. Developing this model provides crucial insight into how to structure a complete application to hide the significant latency of the PCIe bus, a vital skill for achieving high performance in complex simulation workflows. ",
            "id": "3529491",
            "problem": "Consider the global stiffness matrix $\\mathbf{K}$ assembly in a large-scale three-dimensional Finite Element Method (FEM) geomechanics simulation, where element-wise contributions are accumulated on a Graphics Processing Unit (GPU) to accelerate the assembly. The host (Central Processing Unit) prepares per-chunk element data of size $S$ (in gibibytes, GiB) that are transferred to the device over a Peripheral Component Interconnect Express (PCIe) interconnect of bandwidth $B$ (in gibibytes per second, GiB/s). Transfers may use either page-locked (pinned) or pageable host memory. The device supports overlapping of one host-to-device transfer with one kernel execution when using pinned memory. Assume that page-locked host memory enables Direct Memory Access (DMA), whereas pageable host memory forces a staging copy over the host memory subsystem of bandwidth $H$ (in GiB/s). Let each transfer incur a fixed latency $L_{\\mathrm{pin}}$ for pinned memory and $L_{\\mathrm{pag}}$ for pageable memory (both in seconds). Let the GPU kernel compute time per chunk be $C$ (in seconds), and there are $N$ chunks in the pipeline.\n\nYou must derive a performance model from first principles and implement it as a complete, runnable program that predicts the end-to-end execution time for the following scenarios:\n- Pinned memory with double-buffered overlap between transfer and compute on the GPU.\n- Pageable memory with no overlap between transfer and compute due to the lack of true asynchronous DMA transfers.\n\nBase your derivation only on the following core definitions and facts:\n- The definition of bandwidth as data moved per unit time.\n- The notion that a transfer with fixed latency and a sustained throughput behaves like a latency plus an amortized transfer time given by the data volume divided by the sustained bandwidth.\n- The scheduling rules of a two-stage pipeline where stage durations are the transfer time and the compute time, respectively, and overlapping reduces the steady-state stage duration to the maximum of the two.\n- The requirement that pageable transfers entail an explicit staging copy across the host memory subsystem in addition to the PCIe transfer, while pinned transfers do not.\n- Overlap between transfer and compute is possible only when using pinned memory.\n\nFrom these bases, derive the expressions needed to compute:\n1. The per-chunk transfer time with pinned host memory, expressed in seconds.\n2. The per-chunk transfer time with pageable host memory, expressed in seconds.\n3. The total time to process $N$ chunks with pinned memory using double-buffered overlap, expressed in seconds, assuming a single transfer overlaps with a single kernel execution at any time.\n4. The total time to process $N$ chunks with pageable memory without overlap, expressed in seconds.\n5. The steady-state overlap fraction (as a decimal, not a percentage) achievable with pinned memory in a two-stage pipeline, defined as the fraction of non-overlapped time removed by overlap in each steady-state stage.\n6. A boolean indicator of whether the overlapped steady-state is compute-bound, defined by whether the kernel compute time per chunk exceeds the transfer time per chunk for the pinned-memory case.\n\nYour program must compute the following outputs for each test case:\n- Total time with pinned memory and overlap (in seconds).\n- Total time with pageable memory and no overlap (in seconds).\n- The speedup factor (dimensionless) defined as the ratio of pageable time to pinned-overlap time.\n- The steady-state overlap fraction (decimal between $0$ and $1$).\n- The compute-bound indicator (boolean), which is true if compute dominates over transfer in the overlapped steady-state and false otherwise.\n\nAll times must be expressed in seconds. Angles are not used. Do not use percentage signs; overlap fraction must be a decimal. Round all floating-point outputs to six decimal places.\n\nTest Suite:\nUse the following five test cases. Each case is a tuple $(B, H, S, C, N, L_{\\mathrm{pin}}, L_{\\mathrm{pag}})$ with units $(\\mathrm{GiB/s}, \\mathrm{GiB/s}, \\mathrm{GiB}, \\mathrm{s}, \\text{integer}, \\mathrm{s}, \\mathrm{s})$.\n\n- Case A (happy path): $(12.0, 20.0, 4.0, 0.5, 8, 5\\times 10^{-6}, 3\\times 10^{-5})$.\n- Case B (compute-bound): $(24.0, 50.0, 2.0, 2.0, 4, 5\\times 10^{-6}, 3\\times 10^{-5})$.\n- Case C (transfer-bound): $(12.0, 20.0, 8.0, 0.1, 6, 5\\times 10^{-6}, 3\\times 10^{-5})$.\n- Case D (latency-visible small transfers): $(24.0, 40.0, 0.01, 0.002, 100, 5\\times 10^{-6}, 3\\times 10^{-5})$.\n- Case E (balanced boundary): $(12.0, 40.0, 3.6, 0.3, 5, 5\\times 10^{-6}, 3\\times 10^{-5})$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated Python-style list of lists. Each inner list must be ordered as:\n[total_time_pinned_overlap_seconds, total_time_pageable_seconds, speedup, overlap_fraction, compute_bound_boolean]\nFor example: [[...case A results...],[...case B results...],...].",
            "solution": "The core of the problem is to model the total time required to process $N$ chunks of data, each of size $S$, under two different memory management and scheduling schemes on a GPU-accelerated system. We will derive the necessary equations from the provided definitions.\n\n**1. Derivation of Per-Chunk Transfer Times**\n\nThe total time for a data transfer is modeled as the sum of a fixed latency and a variable time dependent on data size and bandwidth.\n\n**1.1. Pinned Memory Transfer Time ($T_{\\mathrm{pin}}$)**\nFor page-locked (pinned) host memory, the data transfer occurs directly from the host RAM to the GPU device memory via the PCIe bus. This is a Direct Memory Access (DMA) operation.\nThe givens are:\n- Data size per chunk: $S$ (in GiB)\n- PCIe bandwidth: $B$ (in GiB/s)\n- Pinned transfer latency: $L_{\\mathrm{pin}}$ (in s)\n\nThe time to move the data volume $S$ at a bandwidth $B$ is $S/B$. The total transfer time for one chunk, $T_{\\mathrm{pin}}$, is the sum of this data movement time and the fixed latency.\n$$ T_{\\mathrm{pin}} = L_{\\mathrm{pin}} + \\frac{S}{B} $$\n\n**1.2. Pageable Memory Transfer Time ($T_{\\mathrm{pag}}$)**\nFor pageable host memory, a direct DMA transfer is not possible. The data must first be copied from the pageable memory region to a pinned staging buffer in the host's RAM. This staging buffer is then used for the DMA transfer to the GPU. This process involves two sequential data movements.\nThe givens are:\n- Host memory subsystem bandwidth for the staging copy: $H$ (in GiB/s)\n- PCIe bandwidth for the transfer to GPU: $B$ (in GiB/s)\n- Pageable transfer latency (for the entire two-step process): $L_{\\mathrm{pag}}$ (in s)\n\nThe time for the host staging copy is $S/H$. The time for the subsequent PCIe transfer is $S/B$. The total transfer time for one chunk, $T_{\\mathrm{pag}}$, is the sum of the times for these two serial operations and the overall latency.\n$$ T_{\\mathrm{pag}} = L_{\\mathrm{pag}} + \\frac{S}{H} + \\frac{S}{B} $$\n\n**2. Derivation of Total Execution Times**\n\nWe now model the end-to-end time to process all $N$ chunks for both scenarios.\n\n**2.1. Pageable Memory (No Overlap)**\nIn this scenario, there is no overlap between data transfer and computation. For each of the $N$ chunks, the transfer must complete before computation can begin. The operations are strictly sequential.\nThe time to process a single chunk is the sum of its transfer time, $T_{\\mathrm{pag}}$, and its computation time, $C$.\n$$ T_{\\mathrm{chunk,pag}} = T_{\\mathrm{pag}} + C $$\nSince this process is repeated for $N$ chunks, the total time, $T_{\\mathrm{total,pag}}$, is simply $N$ times the time per chunk.\n$$ T_{\\mathrm{total,pag}} = N \\times (T_{\\mathrm{pag}} + C) $$\n\n**2.2. Pinned Memory (With Overlap)**\nThis scenario uses a double-buffered, two-stage pipeline to overlap data transfer and computation. The two stages are:\n- Stage 1: Transfer, with duration $T_{\\mathrm{pin}}$.\n- Stage 2: Compute, with duration $C$.\n\nThe total time to execute $N$ tasks in a two-stage pipeline is determined by the \"pipeline fill\" time, the steady-state execution, and the \"pipeline drain\" time. A simpler, equivalent model computes the time for the first chunk to complete and adds the time for the remaining $N-1$ chunks to emerge from the pipeline.\n\nThe first chunk is transferred (time $T_{\\mathrm{pin}}$) and then computed (time $C$). These initial operations are sequential, taking $T_{\\mathrm{pin}} + C$. While the first chunk's computation is in progress, the second chunk's transfer can occur in parallel. The rate at which subsequent chunks are completed is determined by the longest stage in the pipeline, which is $\\max(T_{\\mathrm{pin}}, C)$. After the first chunk is fully processed, one new chunk is completed every $\\max(T_{\\mathrm{pin}}, C)$ seconds for the remaining $N-1$ chunks.\n\nThe total time, $T_{\\mathrm{total,pin}}$, can be expressed as:\n$$ T_{\\mathrm{total,pin}} = (T_{\\mathrm{pin}} + C) + (N-1) \\times \\max(T_{\\mathrm{pin}}, C) $$\nThis can be rewritten by noting that $T_{\\mathrm{pin}} + C = \\min(T_{\\mathrm{pin}}, C) + \\max(T_{\\mathrm{pin}}, C)$.\n$$ T_{\\mathrm{total,pin}} = \\min(T_{\\mathrm{pin}}, C) + \\max(T_{\\mathrm{pin}}, C) + (N-1) \\times \\max(T_{\\mathrm{pin}}, C) $$\n$$ T_{\\mathrm{total,pin}} = \\min(T_{\\mathrm{pin}}, C) + N \\times \\max(T_{\\mathrm{pin}}, C) $$\nThis expression correctly models the total time: one of the processes (the shorter one) runs for one cycle, while the other (the longer one, which determines the pipeline's clock rate) effectively runs for $N$ cycles.\n\n**3. Derivation of Performance Metrics**\n\n**3.1. Speedup Factor**\nThe speedup is defined as the ratio of the execution time of the slower (pageable, no overlap) method to the faster (pinned, overlap) method.\n$$ \\text{Speedup} = \\frac{T_{\\mathrm{total,pag}}}{T_{\\mathrm{total,pin}}} $$\n\n**3.2. Steady-State Overlap Fraction ($f_{\\mathrm{overlap}}$)**\nThis is defined as the fraction of non-overlapped time removed by overlap in each steady-state stage.\n- The time for one cycle without overlap would be the sum of a transfer and a computation: $T_{\\mathrm{pin}} + C$.\n- The time saved by overlapping these two stages is the duration of the shorter stage, which runs in parallel with the longer one: $\\min(T_{\\mathrm{pin}}, C)$.\n- The fraction is the ratio of saved time to the non-overlapped time.\n$$ f_{\\mathrm{overlap}} = \\frac{\\min(T_{\\mathrm{pin}}, C)}{T_{\\mathrm{pin}} + C} $$\nThis value ranges from $0$ (if one stage has zero duration) to $0.5$ (if both stages have equal duration).\n\n**3.3. Compute-Bound Indicator ($I_{\\mathrm{compute-bound}}$)**\nThis boolean indicates if the pipeline's steady state is limited by computation. This occurs when the compute time per chunk, $C$, is greater than the transfer time per chunk, $T_{\\mathrm{pin}}$. The pipeline is \"compute-bound\".\n$$ I_{\\mathrm{compute-bound}} = (C > T_{\\mathrm{pin}}) $$\n\n**Summary of Formulas to Implement:**\n1.  Per-chunk pinned transfer time: $T_{\\mathrm{pin}} = L_{\\mathrm{pin}} + S/B$\n2.  Per-chunk pageable transfer time: $T_{\\mathrm{pag}} = L_{\\mathrm{pag}} + S/H + S/B$\n3.  Total pinned/overlapped time: $T_{\\mathrm{total,pin}} = N \\times \\max(T_{\\mathrm{pin}}, C) + \\min(T_{\\mathrm{pin}}, C)$\n4.  Total pageable/non-overlapped time: $T_{\\mathrm{total,pag}} = N \\times (T_{\\mathrm{pag}} + C)$\n5.  Speedup: $\\text{Speedup} = T_{\\mathrm{total,pag}} / T_{\\mathrm{total,pin}}$\n6.  Overlap fraction: $f_{\\mathrm{overlap}} = \\min(T_{\\mathrm{pin}}, C) / (T_{\\mathrm{pin}} + C)$\n7.  Compute-bound indicator: $I_{\\mathrm{compute-bound}} = (C > T_{\\mathrm{pin}})$\nThese formulas will be implemented to solve the problem for the given test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes performance metrics for FEM matrix assembly on a GPU\n    based on a derived performance model.\n    \"\"\"\n    \n    # Test cases as tuples: (B, H, S, C, N, L_pin, L_pag)\n    # B: PCIe bandwidth (GiB/s)\n    # H: Host memory bandwidth (GiB/s)\n    # S: Chunk size (GiB)\n    # C: Kernel compute time per chunk (s)\n    # N: Number of chunks\n    # L_pin: Pinned memory transfer latency (s)\n    # L_pag: Pageable memory transfer latency (s)\n    test_cases = [\n        (12.0, 20.0, 4.0, 0.5, 8, 5e-6, 3e-5),       # Case A\n        (24.0, 50.0, 2.0, 2.0, 4, 5e-6, 3e-5),       # Case B\n        (12.0, 20.0, 8.0, 0.1, 6, 5e-6, 3e-5),       # Case C\n        (24.0, 40.0, 0.01, 0.002, 100, 5e-6, 3e-5),   # Case D\n        (12.0, 40.0, 3.6, 0.3, 5, 5e-6, 3e-5),       # Case E\n    ]\n\n    case_results_str = []\n    for case in test_cases:\n        B, H, S, C, N, L_pin, L_pag = case\n        \n        # 1. Per-chunk transfer time with pinned host memory\n        T_pin = L_pin + S / B\n        \n        # 2. Per-chunk transfer time with pageable host memory\n        T_pag = L_pag + S / H + S / B\n        \n        # 3. Total time with pinned memory and double-buffered overlap\n        # Formula: N * max(T_pin, C) + min(T_pin, C)\n        total_time_pinned_overlap = N * max(T_pin, C) + min(T_pin, C)\n\n        # 4. Total time with pageable memory and no overlap\n        # Formula: N * (T_pag + C)\n        total_time_pageable = N * (T_pag + C)\n        \n        # 5. Speedup factor\n        speedup = total_time_pageable / total_time_pinned_overlap\n        \n        # 6. Steady-state overlap fraction\n        # Formula: min(T_pin, C) / (T_pin + C)\n        # Avoid division by zero, although T_pin and C are positive times.\n        if (T_pin + C) > 0:\n            overlap_fraction = min(T_pin, C) / (T_pin + C)\n        else:\n            overlap_fraction = 0.0\n\n        # 7. Compute-bound indicator\n        # True if compute time dominates transfer time in the overlapped case\n        compute_bound = C > T_pin\n        \n        # Format the inner list as a string with specified precision\n        inner_list_str = (\n            f\"[{total_time_pinned_overlap:.6f},\"\n            f\"{total_time_pageable:.6f},\"\n            f\"{speedup:.6f},\"\n            f\"{overlap_fraction:.6f},\"\n            f\"{compute_bound}]\"\n        )\n        case_results_str.append(inner_list_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(case_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}