{
    "hands_on_practices": [
        {
            "introduction": "Proper Orthogonal Decomposition can initially appear abstract, defined by a variational principle of maximizing projected energy. This first exercise demystifies the concept by grounding it in a direct, hands-on calculation with a small, manageable dataset. By starting from first principles with a $2 \\times 3$ snapshot matrix, you will derive and solve the core eigenproblem that yields the POD modes, reinforcing the fundamental link between POD and Singular Value Decomposition (SVD) .",
            "id": "3553475",
            "problem": "In a finite element simulation of a plane-strain geomechanical boundary value problem, suppose the displacement field has been reduced to two dominant degrees of freedom via static condensation, and three quasi-static load steps have been computed. Let the snapshot matrix be the $2 \\times 3$ matrix\n$$\nX=\\begin{bmatrix}1 & 1 & 0\\\\\n0 & 1 & 1\\end{bmatrix},\n$$\nwhere column $j$ stores the discrete displacement vector at load step $j$. Adopt the standard Euclidean inner product so that the weighting matrix is $W=I$, where $I$ denotes the $2 \\times 2$ identity matrix.\n\nUsing the variational definition of Proper Orthogonal Decomposition (POD), namely that the first mode $u_{1}$ solves the constrained maximization of the mean-squared projection of the snapshots onto $u_{1}$ subject to the normalization constraint with respect to $W$, derive from first principles the correlation eigenproblem that characterizes the POD modes for $W=I$. Then, for the given $X$:\n- Compute explicitly the first two POD modes $u_{1}$ and $u_{2}$, ensuring exact normalization in the $W$-inner product and clearly specifying a consistent sign convention for each mode.\n- Compute the associated singular values needed to form the optimal rank-one POD reconstruction $\\widehat{X}_{1}$ and the right singular vectors corresponding to $u_{1}$ and $u_{2}$.\n- Define the optimal rank-one POD reconstruction as $\\widehat{X}_{1}=\\sigma_{1}\\,u_{1}\\,v_{1}^{\\top}$ and the residual $R=X-\\widehat{X}_{1}$. Using only foundational definitions and results, compute the Frobenius norm $\\|R\\|_{F}$.\n\nProvide as your final answer the single real number equal to $\\|R\\|_{F}$. If you choose to approximate any intermediate quantities, round your final answer to four significant figures. Do not include units in your final answer.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The context is a finite element simulation of a plane-strain geomechanical boundary value problem.\n- The snapshot matrix is $X=\\begin{bmatrix}1 & 1 & 0\\\\ 0 & 1 & 1\\end{bmatrix}$, a $2 \\times 3$ matrix. The columns represent discrete displacement vectors for $3$ quasi-static load steps.\n- The weighting matrix is $W=I$, the $2 \\times 2$ identity matrix. The inner product is the standard Euclidean inner product.\n- The first POD mode $u_{1}$ is defined as the solution to the constrained maximization of the mean-squared projection of the snapshots onto $u_{1}$, subject to normalization with respect to $W$.\n- Required tasks:\n    1. Derive the correlation eigenproblem for $W=I$ from first principles.\n    2. Compute the first two POD modes $u_{1}$ and $u_{2}$.\n    3. Compute the associated singular values and right singular vectors.\n    4. Define the optimal rank-one POD reconstruction as $\\widehat{X}_{1}=\\sigma_{1}\\,u_{1}\\,v_{1}^{\\top}$.\n    5. Compute the Frobenius norm of the residual $R=X-\\widehat{X}_{1}$, denoted $\\|R\\|_{F}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses Proper Orthogonal Decomposition (POD), which is a standard and widely used model reduction technique in computational mechanics, including geomechanics. The formulation via variational principles and its connection to Singular Value Decomposition (SVD) are fundamental concepts in linear algebra and data analysis. The problem is based on established mathematical principles.\n- **Well-Posed:** The problem provides all necessary information (snapshot matrix $X$, weighting matrix $W$) and asks for specific, computable quantities. The matrix dimensions are consistent. The existence and uniqueness of POD modes (up to sign) and singular values are guaranteed for any non-zero matrix $X$.\n- **Objective:** The problem is stated in precise mathematical language, free from ambiguity or subjective claims. The tasks are clearly defined.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid as it is scientifically grounded, well-posed, objective, and self-contained. The solution will be derived as follows.\n\n### Solution\n\n**Part 1: Derivation of the Correlation Eigenproblem**\n\nThe variational problem for the first POD mode $u_1$ is to maximize the mean-squared projection of the snapshots onto a vector $u$, subject to a normalization constraint. Let the number of snapshots be $N_s = 3$. The snapshots are the columns of the matrix $X$, denoted $x_j \\in \\mathbb{R}^2$ for $j=1, 2, 3$.\n\nThe projection of a snapshot $x_j$ onto a vector $u$ is given by the inner product $\\langle x_j, u \\rangle_W = u^T W x_j$. Since $W=I$, this is the standard dot product, $u^T x_j$.\nThe mean-squared projection is the objective function to be maximized:\n$$J(u) = \\frac{1}{N_s} \\sum_{j=1}^{N_s} (\\langle x_j, u \\rangle_W)^2 = \\frac{1}{3} \\sum_{j=1}^{3} (u^T x_j)^2$$\nThe normalization constraint is $\\langle u, u \\rangle_W = 1$, which simplifies to $u^T W u = u^T I u = u^T u = 1$.\n\nWe can rewrite the objective function using matrix notation:\n$$J(u) = \\frac{1}{3} \\sum_{j=1}^{3} (u^T x_j)(x_j^T u) = u^T \\left( \\frac{1}{3} \\sum_{j=1}^{3} x_j x_j^T \\right) u$$\nThe matrix $X$ is given by $X = \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}$. The sum can be written as the matrix product $XX^T = \\sum_{j=1}^{3} x_j x_j^T$.\nSo, the objective function is $J(u) = u^T \\left(\\frac{1}{3} XX^T\\right) u$.\n\nWe use the method of Lagrange multipliers to solve the constrained optimization problem:\nMaximise $J(u)$ subject to $g(u) = u^T u - 1 = 0$.\nThe Lagrangian is $\\mathcal{L}(u, \\lambda) = J(u) - \\lambda g(u) = u^T \\left(\\frac{1}{3} XX^T\\right) u - \\lambda(u^T u - 1)$.\nTo find the stationary points, we take the gradient with respect to $u$ and set it to zero:\n$$\\nabla_u \\mathcal{L} = 2 \\left(\\frac{1}{3} XX^T\\right) u - 2\\lambda u = 0$$\nThis simplifies to the eigenproblem:\n$$\\left(\\frac{1}{3} XX^T\\right) u = \\lambda u$$\nThis is the correlation eigenproblem. The matrix $C_X = \\frac{1}{3} XX^T$ is known as the spatial correlation matrix. The POD modes $u_i$ are the eigenvectors of $C_X$, and the corresponding eigenvalues $\\lambda_i$ represent the mean energy captured by each mode.\n\n**Part 2: Computation of POD Modes and Singular Values**\n\nFirst, compute the matrix $K = XX^T$:\n$$K = XX^T = \\begin{bmatrix}1 & 1 & 0\\\\ 0 & 1 & 1\\end{bmatrix} \\begin{bmatrix}1 & 0\\\\ 1 & 1\\\\ 0 & 1\\end{bmatrix} = \\begin{bmatrix} (1)(1)+(1)(1)+(0)(0) & (1)(0)+(1)(1)+(0)(1) \\\\ (0)(1)+(1)(1)+(1)(0) & (0)(0)+(1)(1)+(1)(1) \\end{bmatrix} = \\begin{bmatrix}2 & 1\\\\ 1 & 2\\end{bmatrix}$$\nThe POD modes are the eigenvectors of $C_X = \\frac{1}{3}K$, which are the same as the eigenvectors of $K$. Let $\\theta$ be the eigenvalues of $K$. The characteristic equation is $\\det(K - \\theta I) = 0$.\n$$\\det\\begin{pmatrix} 2-\\theta & 1 \\\\ 1 & 2-\\theta \\end{pmatrix} = (2-\\theta)^2 - 1 = 0 \\implies \\theta^2 - 4\\theta + 3 = 0$$\nFactoring gives $(\\theta-3)(\\theta-1)=0$. The eigenvalues of $K$ are $\\theta_1 = 3$ and $\\theta_2 = 1$, ordered in descending magnitude.\n\nFor $\\theta_1 = 3$:\n$$(K - 3I)u_1 = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies -a+b=0 \\implies a=b$$\nThe unnormalized eigenvector is $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Normalizing it: $u_1 = \\frac{1}{\\sqrt{1^2+1^2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. We adopt the sign convention that the first non-zero element is positive.\n\nFor $\\theta_2 = 1$:\n$$(K - 1I)u_2 = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies a+b=0 \\implies a=-b$$\nThe unnormalized eigenvector is $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. Normalizing it: $u_2 = \\frac{1}{\\sqrt{1^2+(-1)^2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe POD modes are $u_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $u_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe POD modes $u_i$ are the left singular vectors of $X$. The singular values $\\sigma_i$ are related to the eigenvalues $\\theta_i$ of $XX^T$ by $\\sigma_i = \\sqrt{\\theta_i}$.\n$$\\sigma_1 = \\sqrt{\\theta_1} = \\sqrt{3}$$\n$$\\sigma_2 = \\sqrt{\\theta_2} = \\sqrt{1} = 1$$\n\n**Part 3: Computation of Right Singular Vectors**\n\nThe right singular vectors $v_i$ are columns of the matrix $V$ in the SVD $X=U\\Sigma V^T$. They can be computed using the relation $v_i = \\frac{1}{\\sigma_i} X^T u_i$.\nFor $v_1$:\n$$v_1 = \\frac{1}{\\sigma_1}X^T u_1 = \\frac{1}{\\sqrt{3}} \\begin{bmatrix}1 & 0\\\\ 1 & 1\\\\ 0 & 1\\end{bmatrix} \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) = \\frac{1}{\\sqrt{6}} \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}$$\nFor $v_2$:\n$$v_2 = \\frac{1}{\\sigma_2}X^T u_2 = \\frac{1}{1} \\begin{bmatrix}1 & 0\\\\ 1 & 1\\\\ 0 & 1\\end{bmatrix} \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\right) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}$$\n\n**Part 4: Computation of the Frobenius Norm of the Residual**\n\nThe Singular Value Decomposition of $X$ is given by $X = \\sum_{i=1}^r \\sigma_i u_i v_i^T$, where $r = \\text{rank}(X)=2$.\n$$X = \\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T$$\nThe optimal rank-one POD reconstruction is $\\widehat{X}_1 = \\sigma_1 u_1 v_1^T$.\nThe residual matrix is $R = X - \\widehat{X}_1$.\n$$R = (\\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T) - \\sigma_1 u_1 v_1^T = \\sigma_2 u_2 v_2^T$$\nThe Frobenius norm of a matrix $M$ is $\\|M\\|_F = \\sqrt{\\text{Tr}(M^T M)}$. A foundational result for the Frobenius norm of an outer product $ab^T$ is $\\|ab^T\\|_F = \\|a\\|_2 \\|b\\|_2$, where $\\| \\cdot \\|_2$ is the standard Euclidean vector norm.\n\nApplying this result to the residual matrix $R$:\n$$\\|R\\|_F = \\|\\sigma_2 u_2 v_2^T\\|_F = \\|\\sigma_2 u_2\\|_2 \\|v_2\\|_2$$\nSince $\\sigma_2$ is a scalar, $\\|\\sigma_2 u_2\\|_2 = |\\sigma_2| \\|u_2\\|_2$.\nThe vectors $u_2$ and $v_2$ are singular vectors, which are orthonormal by construction. Therefore, $\\|u_2\\|_2 = 1$ and $\\|v_2\\|_2 = 1$.\n$$\\|R\\|_F = \\sigma_2 \\cdot 1 \\cdot 1 = \\sigma_2$$\nWe found that the second singular value is $\\sigma_2=1$.\nTherefore, the Frobenius norm of the residual is:\n$$\\|R\\|_F = 1$$\n\nAs a verification, we can compute $R$ explicitly:\n$$R = \\sigma_2 u_2 v_2^T = 1 \\cdot \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\right) \\cdot \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 0 & -1 \\end{pmatrix}\\right) = \\frac{1}{2}\\begin{pmatrix} 1 & 0 & -1\\\\ -1 & 0 & 1\\end{pmatrix}$$\nThe squared Frobenius norm is the sum of the squares of its elements:\n$$\\|R\\|_F^2 = \\left(\\frac{1}{2}\\right)^2 + 0^2 + \\left(-\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + 0^2 + \\left(\\frac{1}{2}\\right)^2$$\n$$\\|R\\|_F^2 = \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{4} = 4 \\cdot \\frac{1}{4} = 1$$\nThus, $\\|R\\|_F = \\sqrt{1} = 1$. This confirms the result. The final answer is an exact integer, so no rounding is required.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Once we have computed a set of POD modes, the next logical step is to use them to build a computationally efficient reduced-order model (ROM). This practice demonstrates how to perform this critical step using Galerkin projection, a familiar concept from finite element analysis. By projecting a full-order stiffness equation onto a subspace spanned by a single POD mode, you will see how a complex system of equations can be reduced to a simple scalar problem, illustrating the power and mechanism of projection-based model reduction .",
            "id": "3553438",
            "problem": "A linear, quasi-static geomechanical model discretized by the finite element method leads to the algebraic system $K u = f$, where $K \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite stiffness matrix and $f \\in \\mathbb{R}^{n}$ is the load vector. A reduced-order model is constructed by Proper Orthogonal Decomposition (POD), using a single trial basis vector $V \\in \\mathbb{R}^{n \\times 1}$, normalized with respect to a weighted inner product induced by a symmetric positive definite matrix $W \\in \\mathbb{R}^{n \\times n}$. The approximate displacement is parameterized as $u \\approx \\bar{u} + V a$, where $\\bar{u} \\in \\mathbb{R}^{n}$ is a baseline field and $a \\in \\mathbb{R}$ is the reduced coordinate. The reduced equation is obtained by enforcing the residual $r(u) = K u - f$ to be orthogonal to the test space with respect to the $W$-weighted inner product.\n\nConsider the toy model with $n=2$, \n$$\nK=\\begin{bmatrix}2 & -1\\\\ -1 & 2\\end{bmatrix}, \n\\qquad \nf=\\begin{bmatrix}1\\\\ 0\\end{bmatrix},\n\\qquad \n\\bar{u}=\\begin{bmatrix}0\\\\ 0\\end{bmatrix},\n\\qquad\nV=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\ 1\\end{bmatrix},\n\\qquad\nW=I,\n$$\nwhere $I$ is the identity matrix. Using the residual orthogonality condition with respect to the $W$-weighted inner product, derive the reduced scalar equilibrium equation, compute the reduced operators, and solve for the reduced coefficient $a$.\n\nState only the value of $a$ as your final answer. No units are required. Do not round the answer; provide the exact value.",
            "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed problem in computational mechanics. All provided data is self-contained and consistent.\n\nThe problem asks for the solution of a reduced-order model obtained via Proper Orthogonal Decomposition (POD) and Galerkin projection. The approximate displacement field $u \\in \\mathbb{R}^{n}$ is parameterized as a linear combination of a baseline field $\\bar{u} \\in \\mathbb{R}^{n}$ and a single trial basis vector $V \\in \\mathbb{R}^{n \\times 1}$:\n$$u \\approx \\bar{u} + V a$$\nwhere $a \\in \\mathbb{R}$ is the reduced coordinate to be determined.\n\nThe full-order model is the linear algebraic system $K u = f$. The residual for a given approximate displacement $u$ is defined as:\n$$r(u) = K u - f$$\nSubstituting the parameterized approximation for $u$, the residual becomes a function of $a$:\n$$r(a) = K(\\bar{u} + V a) - f = K \\bar{u} + (K V) a - f$$\n\nThe reduced-order model is derived by enforcing the Galerkin condition, which states that the residual must be orthogonal to the test space. In this problem, the test space is spanned by the same basis vector $V$, and the orthogonality is defined with respect to the $W$-weighted inner product. The inner product of two vectors $x, y \\in \\mathbb{R}^{n}$ is $(x, y)_W = x^T W y$. The orthogonality condition is therefore:\n$$(V, r(a))_W = 0$$\nwhich in matrix notation is:\n$$V^T W r(a) = 0$$\nSubstituting the expression for the residual $r(a)$:\n$$V^T W (K \\bar{u} + (K V) a - f) = 0$$\nBy distributing $V^T W$, we obtain the scalar linear equation for $a$:\n$$V^T W K \\bar{u} + (V^T W K V) a - V^T W f = 0$$\nRearranging this equation to solve for $a$ gives the general reduced equation:\n$$(V^T W K V) a = V^T W f - V^T W K \\bar{u}$$\nThis equation is of the form $K_r a = f_r$, where $K_r$ is the reduced stiffness and $f_r$ is the reduced force term.\n\nFor the specific toy model provided, the parameters are:\n$n=2$\n$$\nK=\\begin{bmatrix}2 & -1\\\\ -1 & 2\\end{bmatrix}, \n\\quad \nf=\\begin{bmatrix}1\\\\ 0\\end{bmatrix},\n\\quad \n\\bar{u}=\\begin{bmatrix}0\\\\ 0\\end{bmatrix},\n\\quad\nV=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\ 1\\end{bmatrix},\n\\quad\nW=I=\\begin{bmatrix}1 & 0\\\\ 0 & 1\\end{bmatrix}\n$$\nSince $\\bar{u}$ is the zero vector, the term $V^T W K \\bar{u}$ is zero, and the reduced equation simplifies to:\n$$(V^T W K V) a = V^T W f$$\n\nFirst, we compute the reduced stiffness operator, $K_r = V^T W K V$. Since $W=I$, this is $K_r = V^T K V$:\n$$K_r = \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 1\\end{bmatrix}\\right) \\begin{bmatrix}2 & -1\\\\ -1 & 2\\end{bmatrix} \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\ 1\\end{bmatrix}\\right)$$\n$$K_r = \\frac{1}{2} \\begin{bmatrix}1 & 1\\end{bmatrix} \\left(\\begin{bmatrix}2 & -1\\\\ -1 & 2\\end{bmatrix} \\begin{bmatrix}1\\\\ 1\\end{bmatrix}\\right)$$\n$$K_r = \\frac{1}{2} \\begin{bmatrix}1 & 1\\end{bmatrix} \\begin{bmatrix}2(1) - 1(1) \\\\ -1(1) + 2(1)\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}1 & 1\\end{bmatrix} \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$$\n$$K_r = \\frac{1}{2} (1 \\cdot 1 + 1 \\cdot 1) = \\frac{1}{2}(2) = 1$$\n\nNext, we compute the reduced force operator, $f_r = V^T W f$. Since $W=I$, this is $f_r = V^T f$:\n$$f_r = \\left(\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 1\\end{bmatrix}\\right) \\begin{bmatrix}1\\\\ 0\\end{bmatrix}$$\n$$f_r = \\frac{1}{\\sqrt{2}} (1 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{\\sqrt{2}}$$\n\nThe reduced scalar equilibrium equation $K_r a = f_r$ is thus:\n$$1 \\cdot a = \\frac{1}{\\sqrt{2}}$$\n\nSolving for the reduced coordinate $a$, we find:\n$$a = \\frac{1}{\\sqrt{2}}$$",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "A crucial aspect of building any reduced-order model is deciding how many basis vectors, or modes, to retain. This choice represents a fundamental trade-off between the model's accuracy and its computational cost. This exercise explores the direct relationship between the singular value spectrum of the data and the resulting truncation error. You will use a given spectrum to determine the minimum number of POD modes required to satisfy a prescribed error tolerance, a practical skill essential for constructing reliable and efficient ROMs in engineering applications .",
            "id": "3553439",
            "problem": "Consider a finite element simulation of quasi-static consolidation in a saturated, anisotropic clay layer. The state variable is the displacement field sampled at $m$ discrete load steps, forming a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$. To reflect the energetic inner product used in computational geomechanics, let $W \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite matrix inducing the weighted inner product $\\langle u, v \\rangle_{W} = u^{\\top} W v$. Define the $W$-weighted snapshot matrix $X_{W} = W^{1/2} X$. The Proper Orthogonal Decomposition (POD) in the $W$-inner product is obtained by the singular value decomposition of $X_{W}$.\n\nStarting from first principles, use the orthogonality of singular vectors and the optimality of rank-$r$ projections in the Frobenius norm to derive the expression of the time-averaged truncation error of the best rank-$r$ $W$-orthogonal projection in terms of the singular values $\\{\\sigma_{j}\\}$ of $X_{W}$. Then, using the provided data, compute the bound and decide the smallest $r$ that achieves a prescribed relative tolerance.\n\nThe $W$-weighted snapshot matrix $X_{W}$ has the singular values\n$$\n\\{\\sigma_{j}\\}_{j=1}^{12} = \\{6.0,\\; 3.5,\\; 2.1,\\; 1.4,\\; 1.0,\\; 0.72,\\; 0.52,\\; 0.36,\\; 0.25,\\; 0.18,\\; 0.12,\\; 0.08\\}.\n$$\nAssume $m = 40$ snapshots and $\\varepsilon = 1.0 \\times 10^{-2}$. Let the truncation error bound be\n$$\n\\eta_{r} = \\sqrt{\\frac{1}{m} \\sum_{j=r+1}^{\\min(n,m)} \\sigma_{j}^{2}},\n$$\nand the relative tolerance criterion be\n$$\n\\frac{\\eta_{r}}{\\|X_{W}\\|_{F}} \\le \\varepsilon,\n$$\nwhere $\\|X_{W}\\|_{F}$ denotes the Frobenius norm.\n\nReport the smallest integer $r$ that satisfies the tolerance, together with the corresponding truncation error bound $\\eta_{r}$ and the achieved relative bound $\\eta_{r}/\\|X_{W}\\|_{F}$. Round the latter two quantities to four significant figures. Express the final dimensionless ratio as a pure number without a percentage sign.",
            "solution": "The user has provided a problem from computational geomechanics concerning Proper Orthogonal Decomposition (POD) and asks for a derivation of the truncation error followed by a numerical calculation to determine the required model order.\n\n**Problem Validation**\n\nFirst, the problem statement is validated.\n\n*   **Givens Extracted**:\n    1.  The context is a quasi-static consolidation simulation with a snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$.\n    2.  An energetic inner product is defined as $\\langle u, v \\rangle_{W} = u^{\\top} W v$ with $W$ being a symmetric positive definite matrix.\n    3.  The $W$-weighted snapshot matrix is $X_{W} = W^{1/2} X$.\n    4.  POD in the $W$-inner product is obtained via the Singular Value Decomposition (SVD) of $X_{W}$.\n    5.  Singular values of $X_W$: $\\{\\sigma_{j}\\}_{j=1}^{12} = \\{6.0,\\; 3.5,\\; 2.1,\\; 1.4,\\; 1.0,\\; 0.72,\\; 0.52,\\; 0.36,\\; 0.25,\\; 0.18,\\; 0.12,\\; 0.08\\}$. The rank of $X_W$ is $12$.\n    6.  Number of snapshots: $m = 40$.\n    7.  Prescribed relative tolerance: $\\varepsilon = 1.0 \\times 10^{-2}$.\n    8.  Truncation error bound definition: $\\eta_{r} = \\sqrt{\\frac{1}{m} \\sum_{j=r+1}^{\\min(n,m)} \\sigma_{j}^{2}}$.\n    9.  Relative tolerance criterion: $\\frac{\\eta_{r}}{\\|X_{W}\\|_{F}} \\le \\varepsilon$.\n*   **Validation Check**:\n    - **Scientific Grounding**: The problem is well-grounded in the theory of model order reduction using POD, specifically applied to finite element analysis. The use of a weighted norm (energy norm) is a standard and rigorous practice in mechanics. All concepts are scientifically sound.\n    - **Well-Posedness**: The problem is clearly stated. It asks for a standard derivation followed by a calculation for which all necessary data and formulas are provided. A unique solution exists.\n    - **Objectivity**: The language is precise and mathematical, with no subjective elements.\n    - **Completeness**: The problem is self-contained. While the formula for the Frobenius norm in terms of singular values is not given, it is fundamental knowledge expected for this topic.\n    - **Realism**: The context and data (decaying singular values) are realistic for an engineering application.\n\n*   **Verdict**: The problem is **valid**.\n\n**Part 1: Derivation of the Truncation Error**\n\nThe goal is to find the error incurred when approximating the full set of snapshots with a truncated POD basis of rank $r$. The snapshots are the columns of the matrix $X$, denoted by $\\{x_k\\}_{k=1}^m$. The error is measured in the energy norm induced by the matrix $W$.\n\nThe POD basis vectors, $\\{\\phi_j\\}_{j=1}^{\\text{rank}(X)}$, are a set of vectors that are optimal for representing the snapshot set. In the $W$-inner product, these basis vectors are $W$-orthogonal: $\\langle\\phi_i, \\phi_j\\rangle_W = \\phi_i^\\top W \\phi_j = \\delta_{ij}$ (for a normalized basis). The projection of a snapshot $x_k$ onto the subspace spanned by the first $r$ basis vectors is given by:\n$$\nx_{k,r} = \\sum_{j=1}^{r} \\langle x_k, \\phi_j \\rangle_W \\phi_j\n$$\nThe total projection error, squared and summed over all snapshots, is\n$$\nE_r = \\sum_{k=1}^{m} \\|x_k - x_{k,r}\\|_W^2 = \\sum_{k=1}^{m} \\langle x_k - x_{k,r}, x_k - x_{k,r} \\rangle_W\n$$\nUsing the definition of the $W$-inner product, this can be rewritten using the standard Euclidean $2$-norm:\n$$\nE_r = \\sum_{k=1}^{m} (x_k - x_{k,r})^\\top W (x_k - x_{k,r}) = \\sum_{k=1}^{m} \\| W^{1/2}(x_k - x_{k,r}) \\|_2^2\n$$\nThe \"method of snapshots\" finds the POD basis by performing an SVD on the weighted snapshot matrix $X_W = W^{1/2} X$. Let the SVD of $X_W$ be:\n$$\nX_W = U \\Sigma V^\\top\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m \\times m}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is a diagonal matrix of singular values $\\sigma_j \\ge 0$. The columns of $U$, $\\{u_j\\}$, are the left singular vectors and are known as the POD modes in the weighted space. The original POD basis vectors are $\\phi_j = W^{-1/2} u_j$.\n\nThe term $W^{1/2}x_k$ is the $k$-th column of $X_W$. The projection $x_{k,r}$ transforms to:\n$$\nW^{1/2} x_{k,r} = W^{1/2} \\sum_{j=1}^{r} (x_k^\\top W \\phi_j) \\phi_j = \\sum_{j=1}^{r} ((W^{1/2}x_k)^\\top (W^{1/2}\\phi_j)) (W^{1/2}\\phi_j) = \\sum_{j=1}^{r} (y_k^\\top u_j) u_j\n$$\nwhere $y_k = W^{1/2}x_k$ is the $k$-th column of $X_W$. This is the Euclidean projection of $y_k$ onto the subspace spanned by $\\{u_j\\}_{j=1}^r$. This projection can be written as $U_r U_r^\\top y_k$, where $U_r$ is the matrix containing the first $r$ columns of $U$.\n\nThe total error $E_r$ is the squared Frobenius norm of the difference between the matrix $X_W$ and its projection onto the subspace spanned by the first $r$ left singular vectors:\n$$\nE_r = \\sum_{k=1}^{m} \\|y_k - U_r U_r^\\top y_k\\|_2^2 = \\|X_W - U_r U_r^\\top X_W\\|_F^2\n$$\nBy the Eckart-Young-Mirsky theorem, the best rank-$r$ approximation of $X_W$ in the Frobenius norm is the truncated SVD, $X_{W,r} = U_r \\Sigma_r V_r^\\top$, where $\\Sigma_r$ contains the first $r$ singular values. It can be shown that $U_r U_r^\\top X_W = X_{W,r}$. Therefore, the error is:\n$$\nE_r = \\|X_W - X_{W,r}\\|_F^2\n$$\nUsing the SVD of $X_W$, the error matrix is:\n$$\nX_W - X_{W,r} = U \\Sigma V^\\top - U_r \\Sigma_r V_r^\\top = \\sum_{j=1}^{\\text{rank}(X_W)} \\sigma_j u_j v_j^\\top - \\sum_{j=1}^{r} \\sigma_j u_j v_j^\\top = \\sum_{j=r+1}^{\\text{rank}(X_W)} \\sigma_j u_j v_j^\\top\n$$\nThe Frobenius norm is invariant under orthogonal transformations, so:\n$$\nE_r = \\|X_W - X_{W,r}\\|_F^2 = \\left\\| U^\\top (X_W - X_{W,r}) V \\right\\|_F^2 = \\left\\| \\Sigma - \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix} \\right\\|_F^2 = \\sum_{j=r+1}^{\\text{rank}(X_W)} \\sigma_j^2\n$$\nThe problem defines the time-averaged truncation error bound $\\eta_r$ as the root-mean-square error over the $m$ snapshots:\n$$\n\\eta_r^2 = \\frac{1}{m} E_r = \\frac{1}{m} \\sum_{j=r+1}^{\\min(n,m)} \\sigma_j^2\n$$\nTaking the square root gives the desired expression:\n$$\n\\eta_r = \\sqrt{\\frac{1}{m} \\sum_{j=r+1}^{\\min(n,m)} \\sigma_j^2}\n$$\nThis completes the derivation.\n\n**Part 2: Numerical Calculation**\n\nWe need to find the smallest integer $r$ such that the relative tolerance criterion is met:\n$$\n\\frac{\\eta_{r}}{\\|X_{W}\\|_{F}} \\le \\varepsilon\n$$\nGiven data: $m=40$, $\\varepsilon = 1.0 \\times 10^{-2}=0.01$, and $\\{\\sigma_{j}\\}_{j=1}^{12} = \\{6.0,\\; 3.5,\\; 2.1,\\; 1.4,\\; 1.0,\\; 0.72,\\; 0.52,\\; 0.36,\\; 0.25,\\; 0.18,\\; 0.12,\\; 0.08\\}$. The rank of $X_W$ is $12$.\n\nFirst, we calculate the squared Frobenius norm of $X_W$ using its singular values:\n$$\n\\|X_{W}\\|_{F}^2 = \\sum_{j=1}^{12} \\sigma_{j}^{2}\n$$\nThe squared singular values are:\n$\\sigma_1^2 = 6.0^2 = 36$\n$\\sigma_2^2 = 3.5^2 = 12.25$\n$\\sigma_3^2 = 2.1^2 = 4.41$\n$\\sigma_4^2 = 1.4^2 = 1.96$\n$\\sigma_5^2 = 1.0^2 = 1.0$\n$\\sigma_6^2 = 0.72^2 = 0.5184$\n$\\sigma_7^2 = 0.52^2 = 0.2704$\n$\\sigma_8^2 = 0.36^2 = 0.1296$\n$\\sigma_9^2 = 0.25^2 = 0.0625$\n$\\sigma_{10}^2 = 0.18^2 = 0.0324$\n$\\sigma_{11}^2 = 0.12^2 = 0.0144$\n$\\sigma_{12}^2 = 0.08^2 = 0.0064$\n\nSumming these values gives:\n$$\n\\|X_{W}\\|_{F}^2 = 36 + 12.25 + 4.41 + 1.96 + 1.0 + 0.5184 + 0.2704 + 0.1296 + 0.0625 + 0.0324 + 0.0144 + 0.0064 = 56.6541\n$$\nSo, $\\|X_{W}\\|_{F} = \\sqrt{56.6541} \\approx 7.5268915$.\n\nThe tolerance criterion can be rewritten as:\n$$\n\\eta_r^2 \\le \\varepsilon^2 \\|X_{W}\\|_F^2\n$$\n$$\n\\frac{1}{m} \\sum_{j=r+1}^{12} \\sigma_j^2 \\le \\varepsilon^2 \\sum_{j=1}^{12} \\sigma_j^2\n$$\nPlugging in the values:\n$$\n\\frac{1}{40} \\sum_{j=r+1}^{12} \\sigma_j^2 \\le (0.01)^2 \\times 56.6541\n$$\n$$\n\\sum_{j=r+1}^{12} \\sigma_j^2 \\le 40 \\times 0.0001 \\times 56.6541 = 0.2266164\n$$\nWe need to find the smallest integer $r$ that satisfies this condition. We calculate the sum of the tail-end squared singular values, $S_r = \\sum_{j=r+1}^{12} \\sigma_j^2$.\nLet's test values of $r$ starting from a high value:\nFor $r=8$: $S_8 = \\sum_{j=9}^{12} \\sigma_j^2 = \\sigma_9^2+\\sigma_{10}^2+\\sigma_{11}^2+\\sigma_{12}^2 = 0.0625 + 0.0324 + 0.0144 + 0.0064 = 0.1157$.\nSince $0.1157 \\le 0.2266164$, the criterion is satisfied for $r=8$.\n\nLet's test $r=7$ to ensure $r=8$ is the smallest integer:\nFor $r=7$: $S_7 = \\sum_{j=8}^{12} \\sigma_j^2 = \\sigma_8^2 + S_8 = 0.1296 + 0.1157 = 0.2453$.\nSince $0.2453 > 0.2266164$, the criterion is not satisfied for $r=7$.\n\nTherefore, the smallest integer $r$ that satisfies the tolerance is $r=8$.\n\nFinally, we compute the truncation error bound $\\eta_8$ and the achieved relative bound $\\eta_8/\\|X_{W}\\|_F$.\n$$\n\\eta_8 = \\sqrt{\\frac{1}{m} \\sum_{j=9}^{12} \\sigma_j^2} = \\sqrt{\\frac{0.1157}{40}} = \\sqrt{0.0028925} \\approx 0.05378196...\n$$\nRounding to four significant figures, $\\eta_8 = 0.05378$.\n\nThe achieved relative bound is:\n$$\n\\frac{\\eta_8}{\\|X_{W}\\|_F} = \\frac{0.05378196...}{7.5268915...} \\approx 0.0071454...\n$$\nRounding to four significant figures, the relative bound is $0.007145$. This value is indeed less than or equal to the prescribed tolerance $\\varepsilon=0.01$.\n\nThe requested quantities are:\n- Smallest integer $r$: $8$\n- Truncation error bound $\\eta_r$: $0.05378$\n- Achieved relative bound $\\eta_r/\\|X_W\\|_F$: $0.007145$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n8 & 0.05378 & 0.007145\n\\end{pmatrix}\n}\n$$"
        }
    ]
}