{
    "hands_on_practices": [
        {
            "introduction": "A frequent challenge in geotechnical engineering is to quantify the uncertainty of an estimated parameter, such as the mean settlement of a foundation, from a finite set of measurements. While classical statistics provides formulas for confidence intervals, these often rely on the assumption of a normal distribution, which is rarely the case for geotechnical data that is often skewed. This practice introduces the nonparametric bootstrap, a powerful computational method for assessing uncertainty without making strong assumptions about the underlying data distribution. By implementing and testing the bootstrap method on synthetic data, you will gain a practical understanding of its robustness and its ability to provide more reliable confidence intervals for skewed variables .",
            "id": "3544691",
            "problem": "A geotechnical engineer is studying the vertical settlement of a shallow foundation. Let the random settlement be denoted by $S$ in millimeters, with $S \\ge 0$. The engineer collects an independent and identically distributed sample $\\{S_1,\\dots,S_n\\}$ of measured settlements and seeks to quantify the uncertainty in the mean settlement estimator using the nonparametric bootstrap and to assess its robustness under skewed distributions.\n\nUse the following foundational base: the definition of expectation and variance of a random variable; the definition of the sample mean $\\bar{S}_n = \\frac{1}{n}\\sum_{i=1}^n S_i$ as an estimator of the population mean; the law of large numbers (justifying that the bootstrap empirical distribution approximates the data-generating distribution); and the principle of resampling with replacement to approximate the sampling distribution of an estimator. Do not use any closed-form confidence interval expressions for the estimator. Instead, quantify uncertainty using bootstrap resampling and construct a bias-corrected and accelerated confidence interval. Ensure all computations are performed in millimeters and state answers in millimeters, rounded to three decimal places.\n\nFor each test case described below, perform the following steps:\n1. Simulate a synthetic dataset $\\{S_i\\}_{i=1}^n$ using the specified seed and the specified skewed distribution for $S$.\n2. Compute the point estimate $\\hat{\\mu} = \\bar{S}_n$.\n3. Perform $B$ bootstrap resamples (sampling with replacement from $\\{S_i\\}_{i=1}^n$) to obtain bootstrap replicates of the mean $\\{\\hat{\\mu}^{\\ast b}\\}_{b=1}^B$. From these replicates, compute:\n   - The bootstrap standard error $\\widehat{\\mathrm{se}}_{\\text{boot}}$ as the sample standard deviation of $\\{\\hat{\\mu}^{\\ast b}\\}_{b=1}^B$.\n   - A two-sided bias-corrected and accelerated interval at nominal level $0.95$ for $\\mu$ built from the bootstrap distribution of $\\hat{\\mu}$. Do not use any parametric or asymptotic Gaussian interval for this construction.\n4. For evaluation of robustness, compute the population mean $\\mu$ and the population standard deviation $\\sigma$ of $S$ for the specified distributional model and parameters. Compute the true standard error of the sample mean $\\mathrm{se}_{\\text{true}} = \\sigma / \\sqrt{n}$. Record:\n   - A coverage indicator $c$ defined as $c = 1$ if the computed interval contains $\\mu$ and $c = 0$ otherwise.\n   - The ratio $r = \\widehat{\\mathrm{se}}_{\\text{boot}} / \\mathrm{se}_{\\text{true}}$.\n5. For each test case, produce the result as a list $[\\hat{\\mu}, \\widehat{\\mathrm{se}}_{\\text{boot}}, L, U, c, r]$ where $[L,U]$ is the computed $0.95$ bias-corrected and accelerated interval for $\\mu$. All quantities except $c$ must be floats in millimeters; $c$ must be an integer $0$ or $1$. Round all floats to three decimal places.\n\nDistributions and population formulas to be used for step $4$:\n- If $S$ is lognormally distributed with parameters $(\\mu_{\\log}, \\sigma_{\\log})$, then $\\mu = \\exp(\\mu_{\\log} + \\tfrac{1}{2}\\sigma_{\\log}^2)$ and $\\sigma^2 = \\left(\\exp(\\sigma_{\\log}^2) - 1\\right)\\exp(2\\mu_{\\log} + \\sigma_{\\log}^2)$.\n- If $S$ is gamma distributed with shape $k$ and scale $\\theta$, then $\\mu = k\\theta$ and $\\sigma^2 = k\\theta^2$.\n\nTest suite:\n- Case A (moderate skew, larger $n$): Lognormal with $(\\mu_{\\log}, \\sigma_{\\log}) = (2.4, 0.5)$, $n = 60$, $B = 5000$, seed $= 20240501$.\n- Case B (high skew, small $n$): Lognormal with $(\\mu_{\\log}, \\sigma_{\\log}) = (2.0, 1.0)$, $n = 25$, $B = 5000$, seed $= 20240502$.\n- Case C (heavy-tailed gamma): Gamma with $(k, \\theta) = (0.5, 30.0)$, $n = 40$, $B = 5000$, seed $= 20240503$.\n- Case D (very high skew, boundary $n$): Lognormal with $(\\mu_{\\log}, \\sigma_{\\log}) = (2.3, 1.2)$, $n = 12$, $B = 5000$, seed $= 20240504$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list $[\\hat{\\mu}, \\widehat{\\mathrm{se}}_{\\text{boot}}, L, U, c, r]$ in millimeters for a test case, rounded to three decimal places, with $c$ as an integer. For example, the printed line should look like\n$[\\,[\\hat{\\mu}_A,\\widehat{\\mathrm{se}}_{\\text{boot},A},L_A,U_A,c_A,r_A],[\\hat{\\mu}_B,\\widehat{\\mathrm{se}}_{\\text{boot},B},L_B,U_B,c_B,r_B],\\dots\\,]$.",
            "solution": "The problem posed is to analyze the performance of the nonparametric bootstrap method for quantifying the uncertainty in the estimated mean of a skewed, non-negative random variable representing geotechnical settlement. This involves generating synthetic data, computing a point estimate, and then constructing a confidence interval using the bias-corrected and accelerated (BCa) bootstrap method. The results are then compared against the known true population parameters to evaluate the bootstrap method's accuracy and robustness, particularly for small sample sizes and highly skewed distributions. The problem is well-defined, scientifically sound, and computationally feasible. We proceed with the solution.\n\n### Scientific Principles and Algorithmic Design\n\nThe core of the problem lies in the application of resampling-based statistical inference. The foundational principles are the Law of Large Numbers, which provides the theoretical underpinning for the bootstrap, and the Central Limit Theorem, whose assumptions are often violated by skewed data, motivating the need for more robust methods like the BCa interval.\n\n**1. The Bootstrap Principle**\nLet $F$ be the true, unknown cumulative distribution function (CDF) of the settlement $S$. We have an independent and identically distributed sample $\\{S_1, S_2, \\dots, S_n\\}$ drawn from $F$. The sample mean $\\hat{\\mu} = \\bar{S}_n = \\frac{1}{n}\\sum_{i=1}^n S_i$ is our estimator for the true population mean $\\mu = E_F[S]$. To assess the uncertainty in $\\hat{\\mu}$, we need its sampling distribution. Since $F$ is unknown, we cannot derive this distribution directly.\n\nThe bootstrap principle circumvents this by substituting the empirical distribution function (EDF), $\\hat{F}_n$, for $F$. $\\hat{F}_n$ is the discrete uniform distribution that places a probability mass of $1/n$ on each observed data point $S_i$. The validity of this substitution is justified by the Glivenko-Cantelli theorem, which states that $\\hat{F}_n$ converges to $F$ as $n \\to \\infty$. We then approximate the sampling distribution of $\\hat{\\mu}$ by repeatedly drawing samples from $\\hat{F}_n$ and computing the statistic for each. This process is known as resampling with replacement.\n\n**2. Bootstrap Algorithm and Standard Error**\nThe algorithm to generate the bootstrap distribution of the mean is as follows:\n- For $b = 1, \\dots, B$, where $B$ is a large number of bootstrap replications:\n  1. Generate a bootstrap sample $\\{S_1^{*b}, \\dots, S_n^{*b}\\}$ by drawing $n$ times with replacement from the original sample $\\{S_1, \\dots, S_n\\}$.\n  2. Compute the mean of the bootstrap sample: $\\hat{\\mu}^{*b} = \\frac{1}{n} \\sum_{i=1}^n S_i^{*b}$.\n- The collection $\\{\\hat{\\mu}^{*1}, \\dots, \\hat{\\mu}^{*B}\\}$ forms the empirical bootstrap distribution, which serves as an approximation to the true sampling distribution of $\\hat{\\mu}$.\n- The bootstrap standard error, $\\widehat{\\mathrm{se}}_{\\text{boot}}$, is the sample standard deviation of these bootstrap replicates, providing an estimate of the true standard error, $\\mathrm{se}_{\\text{true}} = \\sigma/\\sqrt{n}$:\n$$ \\widehat{\\mathrm{se}}_{\\text{boot}} = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B \\left(\\hat{\\mu}^{*b} - \\frac{1}{B}\\sum_{j=1}^B \\hat{\\mu}^{*j}\\right)^2} $$\n\n**3. The Bias-Corrected and Accelerated (BCa) Confidence Interval**\nFor skewed or biased sampling distributions, a simple percentile interval constructed from the sorted bootstrap replicates $\\{\\hat{\\mu}^*\\}$ may exhibit poor coverage probability. The BCa interval improves upon the percentile method by adjusting the quantile endpoints to account for two factors: bias and acceleration (skewness).\n\nA $1-\\alpha$ confidence interval is given by $[\\hat{\\mu}^*_{(\\alpha_1)}, \\hat{\\mu}^*_{(\\alpha_2)}]$. Here, $\\hat{\\mu}^*_{(p)}$ denotes the $p$-th quantile of the bootstrap replicates. The BCa method computes the adjusted quantiles $\\alpha_1$ and $\\alpha_2$ as:\n$$ \\alpha_1 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z^{(\\alpha/2)}}{1 - \\hat{a}(\\hat{z}_0 + z^{(\\alpha/2)})} \\right) $$\n$$ \\alpha_2 = \\Phi\\left( \\hat{z}_0 + \\frac{\\hat{z}_0 + z^{(1-\\alpha/2)}}{1 - \\hat{a}(\\hat{z}_0 + z^{(1-\\alpha/2)})} \\right) $$\nwhere:\n- $\\Phi(\\cdot)$ is the standard normal cumulative distribution function (CDF).\n- $z^{(p)}$ is the $p$-th quantile of the standard normal distribution (i.e., $\\Phi^{-1}(p)$). For a $95\\%$ interval, $\\alpha=0.05$, $z^{(0.025)} \\approx -1.96$, and $z^{(0.975)} \\approx 1.96$.\n- $\\hat{z}_0$ is the bias-correction factor.\n- $\\hat{a}$ is the acceleration factor.\n\n**3.1. Bias-Correction Factor ($\\hat{z}_0$)**\nThe bias-correction factor measures the median bias of the bootstrap distribution. It is calculated as the standard normal quantile corresponding to the proportion of bootstrap replicates less than the original sample estimate $\\hat{\\mu}$:\n$$ \\hat{z}_0 = \\Phi^{-1}\\left( \\frac{\\#\\{\\hat{\\mu}^{*b} < \\hat{\\mu}\\}}{B} \\right) $$\nIf the bootstrap distribution is median-unbiased (i.e., $\\text{median}(\\{\\hat{\\mu}^{*b}\\}) = \\hat{\\mu}$), then this proportion is $0.5$ and $\\hat{z}_0 = \\Phi^{-1}(0.5) = 0$.\n\n**3.2. Acceleration Factor ($\\hat{a}$)**\nThe acceleration factor, $\\hat{a}$, corrects for the skewness of the sampling distribution, or equivalently, how the standard error of $\\hat{\\mu}$ changes as a function of the true parameter $\\mu$. It is estimated using the jackknife method. The jackknife estimate of the statistic is computed by systematically leaving out one observation at a time.\n1. For $i = 1, \\dots, n$, let $\\hat{\\mu}_{(-i)}$ be the sample mean of the data with the $i$-th point $S_i$ removed.\n2. Let $\\bar{\\mu}_{(\\cdot)} = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\mu}_{(-i)}$ be the mean of these jackknife estimates.\n3. The acceleration factor is then estimated as:\n$$ \\hat{a} = \\frac{\\sum_{i=1}^n (\\bar{\\mu}_{(\\cdot)} - \\hat{\\mu}_{(-i)})^3}{6 \\left[ \\sum_{i=1}^n (\\bar{\\mu}_{(\\cdot)} - \\hat{\\mu}_{(-i)})^2 \\right]^{3/2}} $$\nIf the sum of squares in the denominator is zero (which is highly unlikely for continuous data), $\\hat{a}$ is taken to be $0$.\n\n**4. Algorithmic Implementation Summary**\nFor each test case specified:\n1.  **Data Generation:** Set the random seed. Generate a sample $\\{S_i\\}_{i=1}^n$ of size $n$ from the specified distribution (Lognormal or Gamma) with its given parameters.\n2.  **Point Estimate:** Compute the sample mean $\\hat{\\mu} = \\bar{S}_n$.\n3.  **Bootstrap Resampling:**\n    - Generate $B$ bootstrap samples by resampling with replacement from the original data.\n    - For each bootstrap sample, compute its mean, yielding $B$ bootstrap replicates $\\{\\hat{\\mu}^{*b}\\}$.\n4.  **Uncertainty Quantification:**\n    - Compute the bootstrap standard error $\\widehat{\\mathrm{se}}_{\\text{boot}}$ as the sample standard deviation of $\\{\\hat{\\mu}^{*b}\\}$.\n    - Sort the bootstrap replicates.\n    - Compute the bias-correction factor $\\hat{z}_0$ using the proportion of bootstrap means less than $\\hat{\\mu}$.\n    - Compute the acceleration factor $\\hat{a}$ using the jackknife procedure on the original sample.\n    - Calculate the adjusted quantile levels $\\alpha_1$ and $\\alpha_2$ using the BCa formulas for a $95\\%$ confidence level ($\\alpha=0.05$).\n    - Determine the lower and upper bounds of the confidence interval, $[L, U]$, by finding the corresponding quantiles from the sorted bootstrap replicates.\n5.  **Evaluation:**\n    - Calculate the true population mean $\\mu$ and standard deviation $\\sigma$ using the provided analytical formulas for the given distribution.\n    - Calculate the true standard error of the mean: $\\mathrm{se}_{\\text{true}} = \\sigma / \\sqrt{n}$.\n    - Determine the coverage indicator $c$: $c=1$ if $\\mu \\in [L, U]$, and $c=0$ otherwise.\n    - Compute the performance ratio $r = \\widehat{\\mathrm{se}}_{\\text{boot}} / \\mathrm{se}_{\\text{true}}$.\n6.  **Output:** Store and format the final list $[\\hat{\\mu}, \\widehat{\\mathrm{se}}_{\\text{boot}}, L, U, c, r]$, rounding all floating-point numbers to three decimal places.\n\nThis comprehensive procedure allows for a rigorous evaluation of the BCa bootstrap method's performance under conditions of distributional skewness and varying sample sizes, which are characteristic challenges in geotechnical engineering and many other scientific fields.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, lognorm, gamma\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing bootstrap analysis for each test case.\n    \"\"\"\n    \n    # Test cases as specified in the problem statement.\n    # format: (distribution_name, {params}, n, B, seed)\n    test_cases = [\n        ('lognormal', {'mu_log': 2.4, 'sigma_log': 0.5}, 60, 5000, 20240501),\n        ('lognormal', {'mu_log': 2.0, 'sigma_log': 1.0}, 25, 5000, 20240502),\n        ('gamma', {'k': 0.5, 'theta': 30.0}, 40, 5000, 20240503),\n        ('lognormal', {'mu_log': 2.3, 'sigma_log': 1.2}, 12, 5000, 20240504),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        dist_name, params, n, B, seed = case\n        \n        # 1. Simulate synthetic dataset\n        rng = np.random.default_rng(seed)\n        if dist_name == 'lognormal':\n            mu_log, sigma_log = params['mu_log'], params['sigma_log']\n            # scipy.stats.lognorm takes s=sigma_log, scale=exp(mu_log)\n            # which is equivalent to np.random.lognormal(mean=mu_log, sigma=sigma_log)\n            data = rng.lognormal(mean=mu_log, sigma=sigma_log, size=n)\n            \n            # 4. Compute true population parameters (Lognormal)\n            mu_true = np.exp(mu_log + 0.5 * sigma_log**2)\n            sigma_true = np.sqrt((np.exp(sigma_log**2) - 1) * np.exp(2 * mu_log + sigma_log**2))\n\n        elif dist_name == 'gamma':\n            k, theta = params['k'], params['theta']\n            # scipy.stats.gamma takes a=k, scale=theta\n            data = rng.gamma(shape=k, scale=theta, size=n)\n\n            # 4. Compute true population parameters (Gamma)\n            mu_true = k * theta\n            sigma_true = np.sqrt(k * theta**2)\n\n        # 2. Compute point estimate\n        mu_hat = np.mean(data)\n\n        # 3. Perform B bootstrap resamples\n        bootstrap_means = np.zeros(B)\n        for b in range(B):\n            bootstrap_sample = rng.choice(data, size=n, replace=True)\n            bootstrap_means[b] = np.mean(bootstrap_sample)\n\n        # Compute bootstrap standard error\n        se_boot = np.std(bootstrap_means, ddof=1)\n\n        # Compute BCa confidence interval\n        # Bias-correction factor z0\n        prop_less = np.sum(bootstrap_means < mu_hat) / B\n        z0_hat = norm.ppf(prop_less)\n\n        # Acceleration factor a_hat (using jackknife)\n        jackknife_means = np.zeros(n)\n        for i in range(n):\n            jackknife_sample = np.delete(data, i)\n            jackknife_means[i] = np.mean(jackknife_sample)\n        \n        jack_mean_of_means = np.mean(jackknife_means)\n        devs = jack_mean_of_means - jackknife_means\n        num_a = np.sum(devs**3)\n        den_a = 6 * (np.sum(devs**2))**1.5\n        a_hat = 0.0 if den_a == 0.0 else num_a / den_a\n        \n        # BCa interval endpoints\n        alpha = 0.05\n        z_alpha1 = norm.ppf(alpha / 2)\n        z_alpha2 = norm.ppf(1 - alpha / 2)\n        \n        num1 = z0_hat + z_alpha1\n        adj_alpha1_arg = z0_hat + num1 / (1 - a_hat * num1)\n        \n        num2 = z0_hat + z_alpha2\n        adj_alpha2_arg = z0_hat + num2 / (1 - a_hat * num2)\n\n        adj_alpha1 = norm.cdf(adj_alpha1_arg)\n        adj_alpha2 = norm.cdf(adj_alpha2_arg)\n\n        # Find interval limits L and U from percentiles\n        bca_interval = np.percentile(bootstrap_means, [100 * adj_alpha1, 100 * adj_alpha2])\n        L, U = bca_interval[0], bca_interval[1]\n\n        # 4. Compute true standard error and metrics\n        se_true = sigma_true / np.sqrt(n)\n        \n        # 5. Record coverage indicator and ratio\n        c = 1 if (L <= mu_true <= U) else 0\n        r = se_boot / se_true if se_true > 0 else 0\n\n        # Append results for this case, rounding floats\n        current_result = [\n            round(mu_hat, 3), \n            round(se_boot, 3), \n            round(L, 3), \n            round(U, 3), \n            c, \n            round(r, 3)\n        ]\n        all_results.append(current_result)\n\n    # Final print statement in the exact required format without extra spaces\n    result_strings = []\n    for res in all_results:\n        # Format each float to exactly 3 decimal places\n        formatted_nums = [f\"{x:.3f}\" if i != 4 else str(int(x)) for i, x in enumerate(res)]\n        result_strings.append(f\"[{','.join(formatted_nums)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Geotechnical systems are fundamentally governed by the spatial variability of the ground. Accurately modeling this variability is crucial for a reliable performance assessment. This exercise guides you through a complete Monte Carlo simulation workflow that begins with generating realistic soil profiles. You will learn to combine a Markov chain, which captures geological sequencing trends, with site-specific Cone Penetration Test (CPT) data using Bayesian inference to produce conditioned stratigraphic simulations . The practice culminates in using these simulated ground models to estimate the probability of excessive deflection in an excavation support wall, directly connecting advanced geostatistical modeling to a practical design assessment.",
            "id": "3544647",
            "problem": "You are tasked with constructing a complete, runnable program that performs a Monte Carlo analysis of excavation wall deflection exceedance probability by generating random stratigraphic sequences conditioned on Cone Penetration Test (CPT) measurements using a Markov chain model. The scientific foundations must be explicit: the stratigraphic sequence is modeled as a discrete-time Markov chain consistent with the Kolmogorov property, the conditioning on CPT measurements uses Bayes’ theorem with a likelihood model, and the lateral soil reaction is idealized as a Winkler foundation obeying Hooke’s law. The lateral wall deflection is approximated by a kinematic upper-bound method grounded in equilibrium and the principle of virtual work for a uniform displacement shape.\n\nThe following elements define the modeling assumptions and data.\n\n1. States and Markov chain. Consider three soil states $S=\\{\\mathrm{SC},\\mathrm{S},\\mathrm{STC}\\}$ where $\\mathrm{SC}$ denotes soft clay, $\\mathrm{S}$ denotes sand, and $\\mathrm{STC}$ denotes stiff clay. A stratigraphy along depth indexed by $i=1,\\dots,n$ is represented by the sequence $s_1,\\dots,s_n$ with $s_i \\in S$. Let the Markov chain transition matrix be $T\\in\\mathbb{R}^{3\\times 3}$ with $T_{ab}=\\mathbb{P}(s_i=b\\mid s_{i-1}=a)$ for $a,b\\in S$, and let the initial distribution be $\\pi\\in\\mathbb{R}^3$ with $\\pi_a=\\mathbb{P}(s_1=a)$. The Markov chain assumes the Kolmogorov property $\\mathbb{P}(s_i\\mid s_{i-1},\\dots,s_1)=\\mathbb{P}(s_i\\mid s_{i-1})$.\n\n2. CPT observations and likelihood model. For each depth index $i$, you are given an observation vector $o_i=[q_{c,i},R_{f,i}]$ where $q_{c,i}$ is the CPT tip resistance expressed in megapascals (MPa) and $R_{f,i}$ is the friction ratio expressed as a decimal (e.g., $0.02$ corresponds to $2\\%$). The conditional likelihood of observing $o_i$ given the state $s_i$ is modeled as a bivariate Gaussian distribution with diagonal covariance,\n$$\n\\mathcal{L}(o_i\\mid s_i=a)=\\prod_{j=1}^{2}\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{a,j}}\\exp\\!\\left(-\\frac{(o_{i,j}-\\mu_{a,j})^2}{2\\sigma_{a,j}^2}\\right),\n$$\nwhere $\\mu_{a}=[\\mu_{a,1},\\mu_{a,2}]$ and $\\sigma_{a}=[\\sigma_{a,1},\\sigma_{a,2}]$ are the mean and standard deviation for state $a$ respectively, with $o_{i,1}=q_{c,i}$ and $o_{i,2}=R_{f,i}$.\n\n3. Conditional Markov chain sampling. For $i=1$, the posterior over $s_1$ given $o_1$ satisfies, by Bayes’ theorem,\n$$\n\\mathbb{P}(s_1=a\\mid o_1)\\propto \\pi_a\\,\\mathcal{L}(o_1\\mid s_1=a).\n$$\nFor $i\\geq 2$, the one-step posterior conditioned on the previous sampled state uses\n$$\n\\mathbb{P}(s_i=b\\mid s_{i-1}=a,o_i)\\propto T_{ab}\\,\\mathcal{L}(o_i\\mid s_i=b),\n$$\nand sampling $s_i$ is performed from this categorical distribution for each depth.\n\n4. Winkler foundation and equivalent stiffness. The lateral soil reaction is idealized as a Winkler foundation with subgrade reaction coefficient $k(z)$ (units $\\mathrm{N}/\\mathrm{m}^3$). Using a uniform lateral displacement shape $\\delta$ along the wall of height $H$, equilibrium of distributed forces requires\n$$\np_e H = \\delta \\int_{0}^{H}k(z)\\,\\mathrm{d}z,\n$$\nwhere $p_e$ is the uniform excavation-induced lateral pressure on the wall (units $\\mathrm{Pa}$). Solving for the lateral deflection gives\n$$\n\\delta = \\frac{p_e H}{\\int_{0}^{H}k(z)\\,\\mathrm{d}z}.\n$$\nDiscretizing the wall into $n$ layers of thickness $\\Delta z = H/n$ where $k(z)$ is piecewise constant with values $k_i$ over the $i$-th interval, the integral becomes $\\int_0^H k(z)\\,\\mathrm{d}z \\approx \\sum_{i=1}^{n}k_i\\,\\Delta z$.\n\n5. Subgrade reaction from CPT via constrained modulus. For a layer at depth index $i$ with state $s_i=a$, the subgrade reaction coefficient is linked to the constrained modulus $M_i$ as\n$$\nk_i = \\beta_{a}\\,M_i,\\quad M_i=c_{M,a}\\,q_{c,i},\n$$\nwhere $q_{c,i}$ is converted from $\\mathrm{MPa}$ to $\\mathrm{Pa}$ in $M_i$, $c_{M,a}$ is a dimensionless correlation factor dependent on the soil state, and $\\beta_a$ has units $\\mathrm{m}^{-1}$ representing a geometric scaling to obtain $\\mathrm{N}/\\mathrm{m}^3$. The constants $c_{M,a}$ and $\\beta_a$ are provided below.\n\n6. Monte Carlo exceedance probability. For each Monte Carlo replication, sample a stratigraphy $s_1,\\dots,s_n$ conditioned on the CPT measurements, compute $\\delta$ using the discretized formula, and record whether $\\delta>\\delta_{\\mathrm{th}}$ for a specified threshold $\\delta_{\\mathrm{th}}$ in meters. The exceedance probability is the fraction of replications where $\\delta>\\delta_{\\mathrm{th}}$.\n\nImplement the above algorithm using the following fixed parameters for states and likelihoods (the same across all test cases):\n- State means $\\mu_a$ and standard deviations $\\sigma_a$ for $a\\in S$:\n  - Soft clay ($\\mathrm{SC}$): $\\mu_{\\mathrm{SC}}=[1.5,0.04]$, $\\sigma_{\\mathrm{SC}}=[0.5,0.01]$.\n  - Sand ($\\mathrm{S}$): $\\mu_{\\mathrm{S}}=[12.0,0.01]$, $\\sigma_{\\mathrm{S}}=[3.0,0.004]$.\n  - Stiff clay ($\\mathrm{STC}$): $\\mu_{\\mathrm{STC}}=[6.0,0.03]$, $\\sigma_{\\mathrm{STC}}=[1.5,0.008]$.\n- Subgrade reaction correlation constants: $c_{M,\\mathrm{SC}}=8$, $c_{M,\\mathrm{S}}=5$, $c_{M,\\mathrm{STC}}=10$, and $\\beta_{\\mathrm{SC}}=0.2\\,\\mathrm{m}^{-1}$, $\\beta_{\\mathrm{S}}=1.2\\,\\mathrm{m}^{-1}$, $\\beta_{\\mathrm{STC}}=0.6\\,\\mathrm{m}^{-1}$.\n- Angle units do not apply; all outputs requiring physical units must be expressed in meters for deflection and Pascals ($\\mathrm{Pa}$) for pressure in computations.\n\nTest suite. Your program must compute the exceedance probability for the following three test cases. In all cases the CPT data have $n=20$ depth points; the wall height $H$ and pressure $p_e$ are specified, and $\\Delta z = H/n$ must be used. All CPT $q_{c,i}$ values are in $\\mathrm{MPa}$ and all $R_{f,i}$ values are decimals. All probabilities must be returned as decimal fractions.\n\n- Test case $\\mathrm{A}$ (mixed stratigraphy, moderate transitions):\n  - Wall height: $H=10$ meters.\n  - Lateral pressure: $p_e=50{,}000$ Pascals (i.e., $50$ kilopascals).\n  - Initial distribution: $\\pi=[0.4,0.4,0.2]$ for $[\\mathrm{SC},\\mathrm{S},\\mathrm{STC}]$.\n  - Transition matrix:\n    $$\n    T=\n    \\begin{bmatrix}\n    0.6 & 0.25 & 0.15\\\\\n    0.15 & 0.7 & 0.15\\\\\n    0.2 & 0.2 & 0.6\n    \\end{bmatrix}.\n    $$\n  - CPT arrays:\n    $$\n    q_c=[2.0,1.2,1.4,5.0,8.0,12.5,15.0,13.5,11.0,9.0,6.0,5.5,4.5,3.5,2.5,2.0,1.8,1.6,1.4,1.2],\n    $$\n    $$\n    R_f=[0.04,0.05,0.045,0.035,0.02,0.012,0.009,0.011,0.013,0.015,0.025,0.028,0.03,0.032,0.035,0.04,0.042,0.045,0.05,0.055].\n    $$\n  - Monte Carlo replications: $N_{\\mathrm{MC}}=5000$.\n  - Deflection threshold: $\\delta_{\\mathrm{th}}=0.002$ meters.\n\n- Test case $\\mathrm{B}$ (sand-dominated CPT, highly persistent transitions):\n  - Wall height: $H=12$ meters.\n  - Lateral pressure: $p_e=40{,}000$ Pascals.\n  - Initial distribution: $\\pi=[0.2,0.7,0.1]$.\n  - Transition matrix:\n    $$\n    T=\n    \\begin{bmatrix}\n    0.9 & 0.05 & 0.05\\\\\n    0.05 & 0.9 & 0.05\\\\\n    0.05 & 0.05 & 0.9\n    \\end{bmatrix}.\n    $$\n  - CPT arrays:\n    $$\n    q_c=[9.0,10.5,12.0,14.0,16.0,18.0,20.0,18.0,16.0,14.0,12.0,10.0,9.0,8.0,7.0,7.5,8.5,9.5,10.5,11.5],\n    $$\n    $$\n    R_f=[0.012,0.011,0.010,0.009,0.009,0.008,0.008,0.009,0.009,0.010,0.011,0.012,0.013,0.014,0.015,0.014,0.013,0.012,0.011,0.010].\n    $$\n  - Monte Carlo replications: $N_{\\mathrm{MC}}=3000$.\n  - Deflection threshold: $\\delta_{\\mathrm{th}}=0.0015$ meters.\n\n- Test case $\\mathrm{C}$ (clay-dominated CPT, moderate transitions, small sample):\n  - Wall height: $H=10$ meters.\n  - Lateral pressure: $p_e=80{,}000$ Pascals.\n  - Initial distribution: $\\pi=[0.6,0.2,0.2]$.\n  - Transition matrix:\n    $$\n    T=\n    \\begin{bmatrix}\n    0.7 & 0.15 & 0.15\\\\\n    0.3 & 0.4 & 0.3\\\\\n    0.25 & 0.2 & 0.55\n    \\end{bmatrix}.\n    $$\n  - CPT arrays:\n    $$\n    q_c=[1.0,1.2,1.1,1.3,1.5,1.6,1.7,1.8,1.9,2.0,2.0,1.9,1.8,1.7,1.6,1.5,1.4,1.3,1.2,1.1],\n    $$\n    $$\n    R_f=[0.055,0.05,0.052,0.048,0.045,0.043,0.042,0.040,0.038,0.036,0.037,0.039,0.041,0.043,0.045,0.047,0.05,0.052,0.054,0.056].\n    $$\n  - Monte Carlo replications: $N_{\\mathrm{MC}}=200$.\n  - Deflection threshold: $\\delta_{\\mathrm{th}}=0.005$ meters.\n\nProgram requirements:\n- Use a fixed pseudorandom seed to ensure reproducibility.\n- Implement sampling using the posterior categorical distributions defined above for $s_1$ and $s_i$ for $i\\geq 2$.\n- Convert $q_c$ from $\\mathrm{MPa}$ to $\\mathrm{Pa}$ before computing $M_i$; use $\\Delta z=H/n$ for each test case.\n- For each test case, report the exceedance probability $\\mathbb{P}(\\delta>\\delta_{\\mathrm{th}})$ as a decimal rounded to $5$ significant digits after the decimal point.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_A,r_B,r_C]$) where $r_A,r_B,r_C$ are the exceedance probabilities for test cases $\\mathrm{A}$, $\\mathrm{B}$, and $\\mathrm{C}$, respectively.\n\nAll numerical answers must be expressed in SI units where applicable, with deflections in meters and pressures in Pascals. The final printed line must contain exactly the bracketed list with decimal numbers rounded to $5$ digits after the decimal point.",
            "solution": "The problem requires the construction of a Monte Carlo simulation to estimate the probability that the lateral deflection of an excavation support wall exceeds a given threshold. This is a problem in geotechnical reliability analysis, integrating concepts from stochastic processes, Bayesian inference, and soil mechanics. The solution is structured by first defining the theoretical models for each component, then outlining the computational algorithm for the simulation.\n\n### Theoretical Framework\n\nThe solution rests on three interconnected models: a stochastic model for the ground stratigraphy, a geotechnical model for the wall-soil interaction, and a probabilistic model for inferring soil types from measurements.\n\n**1. Stochastic Stratigraphy: Discrete-Time Markov Chain**\n\nThe soil profile is idealized as a sequence of discrete layers, each belonging to one of three states, $S=\\{\\mathrm{SC}, \\mathrm{S}, \\mathrm{STC}\\}$, representing soft clay, sand, and stiff clay, respectively. The sequence of soil states with depth, $s_1, s_2, \\dots, s_n$, is modeled as a first-order, discrete-time Markov chain. This model is characterized by two components:\n- An initial probability distribution $\\pi \\in \\mathbb{R}^3$, where $\\pi_a = \\mathbb{P}(s_1=a)$ gives the probability of the first layer being of soil type $a \\in S$.\n- A transition matrix $T \\in \\mathbb{R}^{3 \\times 3}$, where the entry $T_{ab} = \\mathbb{P}(s_i=b \\mid s_{i-1}=a)$ is the probability of transitioning from state $a$ to state $b$ in one depth step.\nThe model adheres to the Markov (Kolmogorov) property, meaning the state of any layer $s_i$ depends only on the state of the immediately preceding layer $s_{i-1}$, i.e., $\\mathbb{P}(s_i \\mid s_{i-1}, \\dots, s_1) = \\mathbb{P}(s_i \\mid s_{i-1})$.\n\n**2. Bayesian Inference from CPT Data**\n\nThe uncertainty in the stratigraphy is reduced by conditioning the Markov model on in-situ Cone Penetration Test (CPT) measurements. At each depth indexed by $i$, an observation vector $o_i = [q_{c,i}, R_{f,i}]$ is available, consisting of the tip resistance $q_{c,i}$ and friction ratio $R_{f,i}$. The link between an observation $o_i$ and a soil state $s_i$ is given by a likelihood model, $\\mathcal{L}(o_i \\mid s_i=a)$. This likelihood is modeled as a bivariate Gaussian distribution with a diagonal covariance matrix:\n$$\n\\mathcal{L}(o_i \\mid s_i=a) = \\mathcal{N}(o_{i,1} \\mid \\mu_{a,1}, \\sigma_{a,1}^2) \\cdot \\mathcal{N}(o_{i,2} \\mid \\mu_{a,2}, \\sigma_{a,2}^2) = \\prod_{j=1}^{2}\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{a,j}}\\exp\\!\\left(-\\frac{(o_{i,j}-\\mu_{a,j})^2}{2\\sigma_{a,j}^2}\\right)\n$$\nwhere $[\\mu_{a,1}, \\mu_{a,2}]$ and $[\\sigma_{a,1}, \\sigma_{a,2}]$ are the state-dependent means and standard deviations for the CPT measurements.\n\nBayes' theorem is used to update the probability of the soil state at each depth. This defines the sampling distribution for the conditional simulation:\n- For the first layer ($i=1$), the posterior probability of state $s_1=a$ is proportional to the product of the prior probability and the likelihood:\n$$\n\\mathbb{P}(s_1=a \\mid o_1) \\propto \\pi_a \\mathcal{L}(o_1 \\mid s_1=a)\n$$\n- For subsequent layers ($i \\ge 2$), the posterior probability of state $s_i=b$, given the previously sampled state $s_{i-1}=a$ and the current observation $o_i$, is:\n$$\n\\mathbb{P}(s_i=b \\mid s_{i-1}=a, o_i) \\propto T_{ab} \\mathcal{L}(o_i \\mid s_i=b)\n$$\nIn each step, the proportionality is resolved by ensuring the probabilities sum to unity over all possible states.\n\n**3. Geotechnical Model: Winkler Foundation and Wall Deflection**\n\nThe mechanical behavior of the soil resisting the wall movement is simplified using the Winkler foundation model. The soil is represented as a series of independent linear elastic springs, with a subgrade reaction coefficient $k(z)$ in units of force per unit area per unit displacement ($\\mathrm{N/m^3}$). For a uniform lateral displacement $\\delta$ of the wall, the principle of virtual work (or simple force equilibrium) dictates that the work done by the external pressure must equal the energy stored in the soil springs. For a uniform excavation-induced pressure $p_e$ over a wall of height $H$, this equilibrium is expressed as:\n$$\np_e H = \\delta \\int_{0}^{H} k(z) \\, \\mathrm{d}z\n$$\nSolving for the deflection $\\delta$ yields:\n$$\n\\delta = \\frac{p_e H}{\\int_{0}^{H} k(z) \\, \\mathrm{d}z}\n$$\nThe integral is discretized over the $n$ soil layers, each of thickness $\\Delta z = H/n$. Assuming the subgrade reaction coefficient $k_i$ is constant within each layer $i$, the integral is approximated by a sum:\n$$\n\\int_{0}^{H} k(z) \\, \\mathrm{d}z \\approx \\sum_{i=1}^{n} k_i \\Delta z\n$$\nThe subgrade reaction coefficient $k_i$ for a given layer is determined from its soil type $s_i=a$ and CPT tip resistance $q_{c,i}$ through a standard empirical correlation. First, the constrained modulus $M_i$ is estimated as $M_i = c_{M,a} q_{c,i}$, where $c_{M,a}$ is a dimensionless factor. Then, $k_i$ is related to $M_i$ by $k_i = \\beta_a M_i$, where $\\beta_a$ is a scaling factor with units of $\\mathrm{m}^{-1}$. It is critical to ensure compatible units; thus, $q_{c,i}$ in megapascals ($\\mathrm{MPa}$) must be converted to Pascals ($\\mathrm{Pa}$, or $\\mathrm{N/m^2}$) by multiplying by $10^6$.\n\n### Monte Carlo Simulation Algorithm\n\nThe exceedance probability $\\mathbb{P}(\\delta > \\delta_{\\mathrm{th}})$ is estimated using a Monte Carlo simulation. The process involves generating a large number of random, but conditional, stratigraphic profiles and calculating the resulting deflection for each.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  **Initialization**: Set the number of replications $N_{\\mathrm{MC}}$, exceedance counter to zero, and define all model parameters ($H, p_e, \\pi, T, \\delta_{\\mathrm{th}}$, etc.). Pre-compute the likelihood matrix $\\mathcal{L}_{ia} = \\mathcal{L}(o_i \\mid s_i=a)$ for all layers $i=1,\\dots,n$ and all states $a \\in S$.\n2.  **Replication Loop**: Repeat for $k=1, \\dots, N_{\\mathrm{MC}}$:\n    a.  **Generate a Stratigraphic Profile**:\n        i.  For layer $i=1$: Compute the posterior probabilities $\\mathbb{P}(s_1=a \\mid o_1)$ and sample a state $s_1$ from this categorical distribution.\n        ii. For layers $i=2, \\dots, n$: Given the sampled state $s_{i-1}$, compute the posterior probabilities $\\mathbb{P}(s_i=b \\mid s_{i-1}, o_i)$ and sample a state $s_i$.\n    b.  **Calculate Wall Deflection**: Using the generated stratigraphy $(s_1, \\dots, s_n)$ and the given $q_c$ array, compute the subgrade reaction coefficient $k_i$ for each layer. Then, calculate the total wall deflection $\\delta$ using the discretized formula:\n        $$\n        \\delta = \\frac{p_e H}{\\sum_{i=1}^{n} k_i \\Delta z} = \\frac{p_e n}{\\sum_{i=1}^{n} k_i}\n        $$\n    c.  **Check for Exceedance**: Compare the computed deflection $\\delta$ with the threshold $\\delta_{\\mathrm{th}}$. If $\\delta > \\delta_{\\mathrm{th}}$, increment the exceedance counter.\n3.  **Estimate Probability**: After all replications are complete, the exceedance probability is estimated as the ratio of the number of exceedances to the total number of replications:\n    $$\n    \\mathbb{P}(\\delta > \\delta_{\\mathrm{th}}) \\approx \\frac{\\text{exceedance count}}{N_{\\mathrm{MC}}}\n    $$\nThis procedure is applied to each test case with its specific parameters to obtain the required probabilities. A fixed random seed ensures the results are reproducible.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run Monte Carlo simulations for all test cases and print the results.\n    \"\"\"\n    # Set a fixed seed for reproducibility of the Monte Carlo simulation.\n    np.random.seed(42)\n\n    # --- Fixed Parameters (common to all test cases) ---\n    # Map states to integer indices for array access\n    state_map = {'SC': 0, 'S': 1, 'STC': 2}\n    states = ['SC', 'S', 'STC']\n    n_states = len(states)\n\n    # Likelihood model parameters (mean and std. dev. for [qc, Rf])\n    # qc is in MPa, Rf is a decimal\n    means = np.array([\n        [1.5, 0.04],  # SC\n        [12.0, 0.01], # S\n        [6.0, 0.03]   # STC\n    ])\n    stds = np.array([\n        [0.5, 0.01],  # SC\n        [3.0, 0.004], # S\n        [1.5, 0.008]  # STC\n    ])\n    \n    # Subgrade reaction correlation constants\n    c_M = np.array([8.0, 5.0, 10.0])   # Dimensionless, for SC, S, STC\n    beta = np.array([0.2, 1.2, 0.6])  # Units of 1/m, for SC, S, STC\n\n    def calculate_likelihoods(qc, Rf, n_layers):\n        \"\"\"\n        Calculates the likelihood of observations for each layer and each state.\n        L_ia = P(o_i | s_i = a)\n        \"\"\"\n        likelihoods = np.zeros((n_layers, n_states))\n        observations = np.stack([qc, Rf], axis=1)\n        for i in range(n_layers):\n            for j in range(n_states):\n                # Likelihood is the product of two independent Gaussian PDFs\n                lik_qc = norm.pdf(observations[i, 0], loc=means[j, 0], scale=stds[j, 0])\n                lik_Rf = norm.pdf(observations[i, 1], loc=means[j, 1], scale=stds[j, 1])\n                likelihoods[i, j] = lik_qc * lik_Rf\n        return likelihoods\n\n    def sample_stratigraphy(pi, T, likelihoods, n_layers):\n        \"\"\"\n        Samples one realization of the stratigraphic profile conditioned on observations.\n        \"\"\"\n        stratigraphy = np.zeros(n_layers, dtype=int)\n        \n        # Layer 1 (i=0)\n        unnormalized_posterior_s1 = pi * likelihoods[0, :]\n        posterior_s1 = unnormalized_posterior_s1 / np.sum(unnormalized_posterior_s1)\n        stratigraphy[0] = np.random.choice(n_states, p=posterior_s1)\n\n        # Layers 2 to n (i=1 to n-1)\n        for i in range(1, n_layers):\n            prev_state = stratigraphy[i-1]\n            prior = T[prev_state, :]\n            unnormalized_posterior_si = prior * likelihoods[i, :]\n            # Handle case where all probabilities are zero to avoid NaN\n            if np.sum(unnormalized_posterior_si) == 0:\n                 # This can happen if an observation is extremely unlikely under all states.\n                 # A uniform distribution is a reasonable fallback.\n                 posterior_si = np.full(n_states, 1.0 / n_states)\n            else:\n                 posterior_si = unnormalized_posterior_si / np.sum(unnormalized_posterior_si)\n            \n            stratigraphy[i] = np.random.choice(n_states, p=posterior_si)\n        \n        return stratigraphy\n\n    def calculate_deflection(stratigraphy, qc, H, pe, n_layers):\n        \"\"\"\n        Calculates wall deflection for a given stratigraphy.\n        \"\"\"\n        # Convert qc from MPa to Pa\n        qc_pa = qc * 1e6\n        \n        # Calculate subgrade reaction coefficients k_i for each layer\n        # k_i = beta[s_i] * c_M[s_i] * qc_pa[i]\n        k_values = beta[stratigraphy] * c_M[stratigraphy] * qc_pa\n        \n        # Calculate deflection using the discretized formula: delta = (pe * n) / sum(k_i)\n        sum_k = np.sum(k_values)\n        if sum_k == 0:\n            return np.inf  # Avoid division by zero\n        \n        delta = (pe * n_layers) / sum_k\n        return delta\n\n\n    def run_monte_carlo(params):\n        \"\"\"\n        Runs the full Monte Carlo simulation for a single test case.\n        \"\"\"\n        H = params['H']\n        pe = params['pe']\n        pi = np.array(params['pi'])\n        T = np.array(params['T'])\n        qc = np.array(params['qc'])\n        Rf = np.array(params['Rf'])\n        N_MC = params['N_MC']\n        delta_th = params['delta_th']\n        n_layers = 20\n\n        # Pre-compute likelihoods as they are constant for all replications\n        likelihoods = calculate_likelihoods(qc, Rf, n_layers)\n\n        exceedance_count = 0\n        for _ in range(N_MC):\n            # 1. Sample a stratigraphic profile\n            stratigraphy = sample_stratigraphy(pi, T, likelihoods, n_layers)\n            \n            # 2. Calculate wall deflection\n            delta = calculate_deflection(stratigraphy, qc, H, pe, n_layers)\n            \n            # 3. Check for exceedance\n            if delta > delta_th:\n                exceedance_count += 1\n        \n        # 4. Estimate probability\n        probability = exceedance_count / N_MC\n        return probability\n\n    test_cases = [\n        {\n            'name': 'A',\n            'H': 10, 'pe': 50000,\n            'pi': [0.4, 0.4, 0.2],\n            'T': [[0.6, 0.25, 0.15], [0.15, 0.7, 0.15], [0.2, 0.2, 0.6]],\n            'qc': [2.0, 1.2, 1.4, 5.0, 8.0, 12.5, 15.0, 13.5, 11.0, 9.0, 6.0, 5.5, 4.5, 3.5, 2.5, 2.0, 1.8, 1.6, 1.4, 1.2],\n            'Rf': [0.04, 0.05, 0.045, 0.035, 0.02, 0.012, 0.009, 0.011, 0.013, 0.015, 0.025, 0.028, 0.03, 0.032, 0.035, 0.04, 0.042, 0.045, 0.05, 0.055],\n            'N_MC': 5000, 'delta_th': 0.002\n        },\n        {\n            'name': 'B',\n            'H': 12, 'pe': 40000,\n            'pi': [0.2, 0.7, 0.1],\n            'T': [[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]],\n            'qc': [9.0, 10.5, 12.0, 14.0, 16.0, 18.0, 20.0, 18.0, 16.0, 14.0, 12.0, 10.0, 9.0, 8.0, 7.0, 7.5, 8.5, 9.5, 10.5, 11.5],\n            'Rf': [0.012, 0.011, 0.010, 0.009, 0.009, 0.008, 0.008, 0.009, 0.009, 0.010, 0.011, 0.012, 0.013, 0.014, 0.015, 0.014, 0.013, 0.012, 0.011, 0.010],\n            'N_MC': 3000, 'delta_th': 0.0015\n        },\n        {\n            'name': 'C',\n            'H': 10, 'pe': 80000,\n            'pi': [0.6, 0.2, 0.2],\n            'T': [[0.7, 0.15, 0.15], [0.3, 0.4, 0.3], [0.25, 0.2, 0.55]],\n            'qc': [1.0, 1.2, 1.1, 1.3, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.0, 1.9, 1.8, 1.7, 1.6, 1.5, 1.4, 1.3, 1.2, 1.1],\n            'Rf': [0.055, 0.05, 0.052, 0.048, 0.045, 0.043, 0.042, 0.040, 0.038, 0.036, 0.037, 0.039, 0.041, 0.043, 0.045, 0.047, 0.05, 0.052, 0.054, 0.056],\n            'N_MC': 200, 'delta_th': 0.005\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        prob = run_monte_carlo(case)\n        results.append(f\"{prob:.5f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond continuously varying properties, some of the most significant geotechnical risks arise from discrete, spatially clustered anomalies like weak zones, boulders, or solution cavities. This advanced practice introduces a sophisticated tool for modeling such features: the Neyman-Scott cluster point process. You will implement a simulation to generate three-dimensional patterns of weak lenses and assess their impact on tunnel stability . Furthermore, this exercise introduces the powerful score-function (or likelihood-ratio) method, an efficient technique to calculate the sensitivity of the collapse probability with respect to the underlying model parameters, providing deep insight into which factors drive risk without the need for additional, costly simulations.",
            "id": "3544703",
            "problem": "Consider a three-dimensional domain modeled as a cube with side length $A$ in dimensionless units, equipped with periodic boundary conditions (a three-dimensional torus). Weak material lenses are modeled as spherical inclusions generated by a Neyman–Scott cluster point process (also called a Poisson cluster process) in three dimensions. A straight circular tunnel of radius $r_{\\mathrm{t}}$ and length $L$ is centered in the domain and aligned with the $z$-axis.\n\nA Neyman–Scott process is constructed as follows. Parent points are generated by a homogeneous Poisson point process with intensity $\\lambda_{\\mathrm{p}}$ (measured in expected number of parents per unit volume). Conditional on the parent points, each parent $i$ produces a Poisson-distributed number of offspring $K_i$ with mean $\\mu_{\\mathrm{c}}$, independently across parents. Each offspring is displaced from its parent location by an independent and identically distributed three-dimensional Gaussian offset with zero mean and standard deviation $\\sigma$ in each coordinate, and then wrapped into the cubic domain using periodic boundaries. Assume all random elements are mutually independent given parameters.\n\nEach offspring point represents the center of a spherical weak lens with fixed radius $r_{\\ell}$. Define a simple stability index $S$ as $S = 1 - \\beta M$, where $M$ is the number of weak lenses whose spheres intersect the tunnel solid and $\\beta$ is a dimensionless weakening factor. Assume the tunnel occupies the set of points with radial distance $\\sqrt{x^2 + y^2} \\le r_{\\mathrm{t}}$ and axial coordinate $|z| \\le L/2$; a spherical lens with center $(x,y,z)$ and radius $r_{\\ell}$ intersects the tunnel if and only if both conditions hold: $\\sqrt{x^2 + y^2} \\le r_{\\mathrm{t}} + r_{\\ell}$ and $|z| \\le L/2 + r_{\\ell}$. Declare a collapse event if and only if $S \\le 0$.\n\nYour task is to design and implement a Monte Carlo simulation to estimate the following for specified parameter sets:\n- The collapse probability $P_{\\mathrm{col}}(\\lambda_{\\mathrm{p}}, \\mu_{\\mathrm{c}})$.\n- The sensitivity of the collapse probability with respect to the parent intensity, $\\partial P_{\\mathrm{col}} / \\partial \\lambda_{\\mathrm{p}}$.\n- The sensitivity of the collapse probability with respect to the mean number of offspring per parent, $\\partial P_{\\mathrm{col}} / \\partial \\mu_{\\mathrm{c}}$.\n\nYou must use a likelihood-ratio (score-function) method derived from the probability law of the Neyman–Scott process. Treat the Gaussian dispersion and spatial wrapping as not depending on $\\lambda_{\\mathrm{p}}$ or $\\mu_{\\mathrm{c}}$. Use periodic boundary conditions to avoid edge effects. All quantities are dimensionless. Use the same pseudo-random seed across all test cases so that results are reproducible.\n\nFoundational base available to you:\n- Homogeneous Poisson point process on volume $V$ has the number of points $N$ distributed as Poisson with mean $\\lambda V$, and the log-likelihood contribution for $N$ is $N \\log(\\lambda V) - \\lambda V - \\log(N!)$.\n- Conditional on parents, independent Poisson offspring counts per parent have log-likelihood contributions $\\sum_{i=1}^{N_{\\mathrm{p}}} \\left( K_i \\log \\mu_{\\mathrm{c}} - \\mu_{\\mathrm{c}} - \\log (K_i!) \\right)$.\n- For likelihood-ratio sensitivity, for a parameter $\\theta$, $\\partial \\mathbb{E}[H]/\\partial \\theta = \\mathbb{E}\\left[H \\cdot \\partial \\log f(X;\\theta)/\\partial \\theta \\right]$, where $H$ is any integrable functional and $f$ is the sampling density.\n\nMonte Carlo requirements:\n- Implement a simulation with a fixed number of independent replications $N_{\\mathrm{mc}}$ per test case. In each replication, generate one realization of the Neyman–Scott process in the cubic domain, compute the collapse indicator (equal to $1$ for collapse and $0$ otherwise), and compute the score-function weights with respect to $\\lambda_{\\mathrm{p}}$ and $\\mu_{\\mathrm{c}}$ derived from the Poisson distributions for parents and offspring counts. Average appropriately to estimate the probability and its two sensitivities.\n- Use a single pseudo-random seed $s$ to initialize the generator. To diversify across test cases while maintaining reproducibility, you may add the test-case index to $s$.\n- Use periodic boundary wrapping for offspring locations: for each coordinate $u \\in \\{x,y,z\\}$, the wrapped coordinate is $u_{\\mathrm{wrapped}} = \\left(\\left(u + A/2\\right) \\bmod A\\right) - A/2$.\n\nTest suite:\nUse $N_{\\mathrm{mc}} = 5000$ and common seed $s = 12345$. For all cases, the domain is the cube of side $A = 10$, so the volume is $V = A^3$. The tunnel parameters are centered at the origin with axis along the $z$-axis.\n\nProvide four parameter sets to test different regimes:\n- Case $1$ (moderate clustering): $\\lambda_{\\mathrm{p}} = 0.02$, $\\mu_{\\mathrm{c}} = 10$, $\\sigma = 0.3$, $r_{\\ell} = 0.2$, $r_{\\mathrm{t}} = 0.5$, $L = 8$, $\\beta = 0.5$.\n- Case $2$ (sparse clusters): $\\lambda_{\\mathrm{p}} = 0.001$, $\\mu_{\\mathrm{c}} = 2$, $\\sigma = 0.4$, $r_{\\ell} = 0.2$, $r_{\\mathrm{t}} = 0.5$, $L = 8$, $\\beta = 0.5$.\n- Case $3$ (dense clusters, tight clustering): $\\lambda_{\\mathrm{p}} = 0.02$, $\\mu_{\\mathrm{c}} = 30$, $\\sigma = 0.1$, $r_{\\ell} = 0.2$, $r_{\\mathrm{t}} = 0.5$, $L = 8$, $\\beta = 0.5$.\n- Case $4$ (existence-dominated failure): $\\lambda_{\\mathrm{p}} = 0.015$, $\\mu_{\\mathrm{c}} = 5$, $\\sigma = 0.25$, $r_{\\ell} = 0.25$, $r_{\\mathrm{t}} = 0.5$, $L = 8$, $\\beta = 1.0$.\n\nOutput specification:\n- For each test case, compute three real numbers: the Monte Carlo estimate of the collapse probability, the Monte Carlo estimate of $\\partial P_{\\mathrm{col}}/\\partial \\lambda_{\\mathrm{p}}$, and the Monte Carlo estimate of $\\partial P_{\\mathrm{col}}/\\partial \\mu_{\\mathrm{c}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list containing three floats in the order described above, enclosed in square brackets. For example, your output format must be exactly like: \"[[p1,dpl1,dpm1],[p2,dpl2,dpm2],[p3,dpl3,dpm3],[p4,dpl4,dpm4]]\" where each symbol denotes a float as defined.\n\nAll answers are dimensionless real numbers, and you must use the parameters exactly as given in the test suite. No other input should be read.",
            "solution": "The user has provided a well-defined problem in computational geomechanics, requiring the estimation of a tunnel collapse probability and its sensitivities using a Monte Carlo simulation. The problem is scientifically grounded, self-contained, and algorithmically specified. It is therefore deemed valid.\n\n### I. Principle-Based Solution Design\n\nThe core task is to estimate the expectation of a function of a random process and the sensitivities of this expectation with respect to two parameters of the process. The random process is a Neyman-Scott cluster process, which models the spatial distribution of weak material lenses. The function is an indicator for a tunnel collapse event. The parameters are the intensity of parent points, $\\lambda_{\\mathrm{p}}$, and the mean number of offspring per parent, $\\mu_{\\mathrm{c}}$.\n\nLet $X$ represent a single realization of the Neyman-Scott process, which includes the number and locations of all generated lenses. Let $H(X)$ be the collapse indicator function, where $H(X)=1$ if collapse occurs and $H(X)=0$ otherwise. The collapse probability is $P_{\\mathrm{col}} = \\mathbb{E}[H(X)]$. The problem asks for this probability and its sensitivities, $\\partial P_{\\mathrm{col}}/\\partial \\lambda_{\\mathrm{p}}$ and $\\partial P_{\\mathrm{col}}/\\partial \\mu_{\\mathrm{c}}$.\n\nThe Likelihood-Ratio Method (also known as the Score-Function Method) is the prescribed technique. This method leverages the identity that for a parameter $\\theta$ and a family of probability distributions $f(x; \\theta)$, the derivative of an expectation can be expressed as another expectation:\n$$\n\\frac{\\partial}{\\partial \\theta} \\mathbb{E}[H(X)] = \\frac{\\partial}{\\partial \\theta} \\int H(x) f(x; \\theta) dx = \\int H(x) \\frac{\\partial f(x; \\theta)}{\\partial \\theta} dx\n$$\nAssuming regularity conditions that allow differentiation under the integral sign, we can write:\n$$\n\\int H(x) \\left( \\frac{1}{f(x; \\theta)} \\frac{\\partial f(x; \\theta)}{\\partial \\theta} \\right) f(x; \\theta) dx = \\int H(x) \\frac{\\partial \\log f(x; \\theta)}{\\partial \\theta} f(x; \\theta) dx = \\mathbb{E}\\left[H(X) \\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta}\\right]\n$$\nThe term $W_{\\theta}(X) = \\frac{\\partial \\log f(X; \\theta)}{\\partial \\theta}$ is called the score function or the likelihood-ratio weight.\n\n### II. Derivation of the Score Functions\n\nThe random elements of the process that depend on the parameters $\\lambda_{\\mathrm{p}}$ and $\\mu_{\\mathrm{c}}$ are the counts of parents and offspring.\n1.  The number of parent points, $N_{\\mathrm{p}}$, in the domain of volume $V=A^3$ follows a Poisson distribution with mean $\\lambda_{\\mathrm{p}} V$. The probability mass function is $P(N_{\\mathrm{p}}=n) = \\frac{(\\lambda_{\\mathrm{p}}V)^n e^{-\\lambda_{\\mathrm{p}}V}}{n!}$.\n2.  Given $N_{\\mathrm{p}}$, the number of offspring for each parent, $K_i$, follows a Poisson distribution with mean $\\mu_{\\mathrm{c}}$, independently for each parent $i=1, \\dots, N_{\\mathrm{p}}$. The probability mass function is $P(K_i=k) = \\frac{\\mu_{\\mathrm{c}}^k e^{-\\mu_{\\mathrm{c}}}}{k!}$.\n\nThe joint log-likelihood of the counts $(N_{\\mathrm{p}}, K_1, \\dots, K_{N_{\\mathrm{p}}})$ is the sum of the log-likelihoods of the individual processes:\n$$\n\\mathcal{L} = \\log P(N_{\\mathrm{p}}) + \\sum_{i=1}^{N_{\\mathrm{p}}} \\log P(K_i)\n$$\nUsing the standard form of the Poisson log-likelihood, $\\log P(k|\\lambda) = k \\log \\lambda - \\lambda - \\log(k!)$, we have:\n$$\n\\mathcal{L}(\\lambda_{\\mathrm{p}}, \\mu_{\\mathrm{c}}) = (N_{\\mathrm{p}} \\log(\\lambda_{\\mathrm{p}} V) - \\lambda_{\\mathrm{p}} V - \\log(N_{\\mathrm{p}}!)) + \\sum_{i=1}^{N_{\\mathrm{p}}} (K_i \\log \\mu_{\\mathrm{c}} - \\mu_{\\mathrm{c}} - \\log(K_i!))\n$$\nThe problem specifies that other random elements (locations) are not dependent on $\\lambda_{\\mathrm{p}}$ or $\\mu_{\\mathrm{c}}$, so their likelihood terms do not contribute to the derivatives with respect to these parameters.\n\nThe score function for $\\lambda_{\\mathrm{p}}$ is the partial derivative of $\\mathcal{L}$ with respect to $\\lambda_{\\mathrm{p}}$:\n$$\nW_{\\lambda_{\\mathrm{p}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\lambda_{\\mathrm{p}}} = \\frac{\\partial}{\\partial \\lambda_{\\mathrm{p}}} (N_{\\mathrm{p}} \\log \\lambda_{\\mathrm{p}} + N_{\\mathrm{p}} \\log V - \\lambda_{\\mathrm{p}} V) = \\frac{N_{\\mathrm{p}}}{\\lambda_{\\mathrm{p}}} - V\n$$\nThe score function for $\\mu_{\\mathrm{c}}$ is the partial derivative of $\\mathcal{L}$ with respect to $\\mu_{\\mathrm{c}}$:\n$$\nW_{\\mu_{\\mathrm{c}}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mu_{\\mathrm{c}}} = \\frac{\\partial}{\\partial \\mu_{\\mathrm{c}}} \\left( \\log \\mu_{\\mathrm{c}} \\sum_{i=1}^{N_{\\mathrm{p}}} K_i - N_{\\mathrm{p}} \\mu_{\\mathrm{c}} \\right) = \\frac{1}{\\mu_{\\mathrm{c}}} \\sum_{i=1}^{N_{\\mathrm{p}}} K_i - N_{\\mathrm{p}}\n$$\nLet $M_{\\mathrm{total}} = \\sum_{i=1}^{N_{\\mathrm{p}}} K_i$ be the total number of offspring (lenses) generated. The score function simplifies to $W_{\\mu_{\\mathrm{c}}} = \\frac{M_{\\mathrm{total}}}{\\mu_{\\mathrm{c}}} - N_{\\mathrm{p}}$.\n\n### III. Monte Carlo Estimators\n\nUsing a Monte Carlo simulation with $N_{\\mathrm{mc}}$ independent replications, we generate samples $(X_j)_{j=1}^{N_{\\mathrm{mc}}}$. For each replication $j$, we compute the collapse indicator $H_j = H(X_j)$ and the score functions $W_{\\lambda_{\\mathrm{p}}, j}$ and $W_{\\mu_{\\mathrm{c}}, j}$. The estimators for the desired quantities are the sample means:\n- Collapse probability: $\\hat{P}_{\\mathrm{col}} = \\frac{1}{N_{\\mathrm{mc}}} \\sum_{j=1}^{N_{\\mathrm{mc}}} H_j$\n- Sensitivity to $\\lambda_{\\mathrm{p}}$: $\\frac{\\widehat{\\partial P_{\\mathrm{col}}}}{\\partial \\lambda_{\\mathrm{p}}} = \\frac{1}{N_{\\mathrm{mc}}} \\sum_{j=1}^{N_{\\mathrm{mc}}} H_j \\cdot W_{\\lambda_{\\mathrm{p}}, j}$\n- Sensitivity to $\\mu_{\\mathrm{c}}$: $\\frac{\\widehat{\\partial P_{\\mathrm{col}}}}{\\partial \\mu_{\\mathrm{c}}} = \\frac{1}{N_{\\mathrm{mc}}} \\sum_{j=1}^{N_{\\mathrm{mc}}} H_j \\cdot W_{\\mu_{\\mathrm{c}}, j}$\n\n### IV. Algorithm for a Single Replication\n\nFor a given set of parameters $(\\lambda_{\\mathrm{p}}, \\mu_{\\mathrm{c}}, \\sigma, r_{\\ell}, r_{\\mathrm{t}}, L, \\beta)$ and domain side $A$, a single Monte Carlo replication proceeds as follows:\n\n1.  **Generate Parents**: Draw the number of parent points, $N_{\\mathrm{p}}$, from a Poisson distribution with mean $\\lambda_{\\mathrm{p}} V$, where $V = A^3$. If $N_{\\mathrm{p}} > 0$, generate their locations uniformly in the cube $[-A/2, A/2]^3$.\n\n2.  **Generate Offspring**: If $N_{\\mathrm{p}} > 0$, draw the number of offspring for each parent, $(K_i)_{i=1}^{N_{\\mathrm{p}}}$, from a Poisson distribution with mean $\\mu_{\\mathrm{c}}$. Calculate the total number of offspring, $M_{\\mathrm{total}} = \\sum_{i=1}^{N_{\\mathrmp}} K_i$.\n\n3.  **Position Offspring**: If $M_{\\mathrm{total}} > 0$:\n    a. For each of the $M_{\\mathrm{total}}$ offspring, identify its parent's location.\n    b. Generate a 3D displacement vector by drawing three independent samples from a normal distribution with mean $0$ and standard deviation $\\sigma$.\n    c. Add the displacement to the parent's location to get the unwrapped offspring location $(x,y,z)$.\n    d. Apply periodic boundary conditions to each coordinate $u \\in \\{x,y,z\\}$ using the formula $u_{\\mathrm{wrapped}} = ((u + A/2) \\pmod A) - A/2$ to bring the location inside the domain $[-A/2, A/2]^3$.\n\n4.  **Count Intersections**: Count the number of offspring (lenses), $M$, whose centers $(x_k, y_k, z_k)$ satisfy the intersection condition with the tunnel: $\\sqrt{x_k^2 + y_k^2} \\le r_{\\mathrm{t}} + r_{\\ell}$ and $|z_k| \\le L/2 + r_{\\ell}$.\n\n5.  **Check for Collapse**: A collapse occurs if the stability index $S = 1 - \\beta M \\le 0$, which is equivalent to $M \\ge 1/\\beta$. Set the collapse indicator $H=1$ if a collapse occurs, and $H=0$ otherwise.\n\n6.  **Calculate Scores**: Compute the score function values for this replication:\n    - $W_{\\lambda_{\\mathrm{p}}} = N_{\\mathrm{p}}/\\lambda_{\\mathrm{p}} - V$\n    - $W_{\\mu_{\\mathrm{c}}} = M_{\\mathrm{total}}/\\mu_{\\mathrm{c}} - N_{\\mathrm{p}}$\n\n7.  **Store Results**: The values for this replication are $H$, $H \\cdot W_{\\lambda_{\\mathrm{p}}}$, and $H \\cdot W_{\\mu_{\\mathrm{c}}}$. These are then added to running totals, which are averaged after all $N_{\\mathrm{mc}}$ replications are complete. Note that if $H=0$, the contributions to the sensitivity estimators are zero, simplifying the calculation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Monte Carlo simulation to estimate tunnel collapse probability\n    and its sensitivities using the likelihood-ratio method.\n    \"\"\"\n\n    def run_simulation(case_params, n_mc, seed):\n        \"\"\"\n        Runs the Monte Carlo simulation for a single test case.\n        \"\"\"\n        # Unpack parameters\n        lambda_p, mu_c, sigma, r_l, r_t, L, beta = case_params\n        A = 10.0\n        V = A**3\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Accumulators for estimates\n        sum_H = 0.0\n        sum_H_W_lp = 0.0\n        sum_H_W_mc = 0.0\n        \n        # Collapse condition threshold\n        M_collapse_threshold = 1.0 / beta\n\n        # Intersection geometry thresholds\n        radial_threshold_sq = (r_t + r_l)**2\n        axial_threshold = L / 2.0 + r_l\n\n        for _ in range(n_mc):\n            # Step 1: Generate Parents\n            Np = rng.poisson(lambda_p * V)\n            \n            if Np == 0:\n                # No parents means no offspring, no collapse.\n                # H is 0. W_lp is -V, W_mc is 0.\n                # H*W is 0. No need to update sums.\n                continue\n            \n            # locations are uniform in [-A/2, A/2]^3\n            parent_locs = rng.uniform(-A/2, A/2, size=(Np, 3))\n\n            # Step 2: Generate Offspring counts\n            # K_i for each parent\n            K_counts = rng.poisson(mu_c, size=Np)\n            M_total = np.sum(K_counts)\n\n            if M_total == 0:\n                # No offspring, no collapse.\n                # H is 0. W_lp_j is Np/lambda_p - V, W_mc_j is 0 - Np.\n                # H*W is 0. No need to update sums.\n                continue\n\n            # Step 3: Position Offspring\n            # Create an index to map each offspring to its parent\n            parent_indices = np.repeat(np.arange(Np), K_counts)\n            \n            # Get parent locations for all offspring\n            offspring_parent_locs = parent_locs[parent_indices]\n            \n            # Generate all displacements at once\n            displacements = rng.normal(0, sigma, size=(M_total, 3))\n            \n            # Calculate unwrapped offspring locations\n            offspring_locs_unwrapped = offspring_parent_locs + displacements\n            \n            # Apply periodic boundary wrapping\n            offspring_locs = ((offspring_locs_unwrapped + A/2) % A) - A/2\n\n            # Step 4: Count Intersections\n            x, y, z = offspring_locs[:, 0], offspring_locs[:, 1], offspring_locs[:, 2]\n            \n            radial_dist_sq = x**2 + y**2\n            axial_dist_abs = np.abs(z)\n\n            # Boolean mask for intersecting lenses\n            intersect_mask = (radial_dist_sq <= radial_threshold_sq) & (axial_dist_abs <= axial_threshold)\n            \n            M = np.sum(intersect_mask)\n\n            # Step 5: Check for Collapse\n            H = 1.0 if M >= M_collapse_threshold else 0.0\n\n            # Step 6 & 7: Calculate Scores and Update Accumulators\n            if H > 0: # Only need to calculate scores if collapse occurs\n                sum_H += H # which is just 1.0\n\n                W_lp = Np / lambda_p - V\n                W_mc = M_total / mu_c - Np\n\n                sum_H_W_lp += W_lp # H is 1\n                sum_H_W_mc += W_mc # H is 1\n        \n        # Calculate final estimates by averaging\n        p_col = sum_H / n_mc\n        dp_dlambda_p = sum_H_W_lp / n_mc\n        dp_dmu_c = sum_H_W_mc / n_mc\n\n        return [p_col, dp_dlambda_p, dp_dmu_c]\n\n    # Global simulation parameters\n    N_mc = 5000\n    common_seed = 12345\n    \n    # Test cases as defined in the problem\n    test_cases = [\n        # (lambda_p, mu_c, sigma, r_l, r_t, L, beta)\n        (0.02, 10, 0.3, 0.2, 0.5, 8, 0.5),    # Case 1\n        (0.001, 2, 0.4, 0.2, 0.5, 8, 0.5),   # Case 2\n        (0.02, 30, 0.1, 0.2, 0.5, 8, 0.5),    # Case 3\n        (0.015, 5, 0.25, 0.25, 0.5, 8, 1.0) # Case 4\n    ]\n\n    all_results = []\n    for i, case in enumerate(test_cases):\n        # Diversify seed per test case for independent-like sample paths\n        case_seed = common_seed + i\n        result = run_simulation(case, N_mc, case_seed)\n        all_results.append(result)\n    \n    # Format the output string exactly as specified\n    output_str = \"[\" + \",\".join([f\"[{p},{dpl},{dpm}]\" for p, dpl, dpm in all_results]) + \"]\"\n\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}