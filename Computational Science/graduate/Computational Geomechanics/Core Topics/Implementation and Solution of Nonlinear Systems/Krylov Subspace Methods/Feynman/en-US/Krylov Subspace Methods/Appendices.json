{
    "hands_on_practices": [
        {
            "introduction": "The ultimate goal of preconditioning is to transform a difficult linear system into one that is trivial to solve, and this exercise provides a glimpse into this ideal scenario. By analyzing a block-structured system from poroelasticity with a theoretically perfect block triangular preconditioner, you will uncover how the preconditioned operator's spectrum can be dramatically simplified . This practice reinforces the fundamental connection between the spectral properties of an operator, its minimal polynomial, and the guaranteed convergence of GMRES in a small, fixed number of steps.",
            "id": "3537468",
            "problem": "In a quasi-static linear poroelasticity (Biot) model for a saturated porous medium, a stabilized mixed finite element discretization leads to a symmetric indefinite block system of the form\n$$\n\\mathbf{A} \\begin{pmatrix} \\mathbf{u} \\\\ \\mathbf{p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{g} \\end{pmatrix}, \\quad \\mathbf{A} := \\begin{pmatrix} \\mathbf{K} & \\mathbf{G}^{\\top} \\\\ \\mathbf{G} & -\\mathbf{C} \\end{pmatrix},\n$$\nwhere $\\mathbf{u} \\in \\mathbb{R}^{n}$ are displacement unknowns and $\\mathbf{p} \\in \\mathbb{R}^{m}$ are pore pressure unknowns. The block $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ is the discrete divergence operator with full row rank, and $\\mathbf{C} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive semidefinite representing compressibility and storage terms. Consider the exact Schur complement defined by\n$$\n\\mathbf{S} := -\\mathbf{C} - \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top},\n$$\nand assume that $\\mathbf{C} + \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$ is invertible so that $\\mathbf{S}$ is invertible. Define the ideal lower block triangular preconditioner\n$$\n\\mathbf{M}_{L} := \\begin{pmatrix} \\mathbf{K} & 0 \\\\ \\mathbf{G} & -\\mathbf{S} \\end{pmatrix}.\n$$\nSuppose the Generalized Minimal Residual (GMRES) method is applied to the left-preconditioned system $\\mathbf{M}_{L}^{-1} \\mathbf{A} \\, \\mathbf{z} = \\mathbf{M}_{L}^{-1} \\mathbf{b}$, where $\\mathbf{z} \\in \\mathbb{R}^{n+m}$ and $\\mathbf{b} \\in \\mathbb{R}^{n+m}$ is the given right-hand side.\n\nStarting from the block definitions above, compute the spectrum of the preconditioned operator $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ and deduce the minimal polynomial. Use these to explain the effect on GMRES convergence when the ideal preconditioner is employed. Finally, report the number of distinct eigenvalues of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ as a single number. No rounding is required. Express your final answer as a pure number without units.",
            "solution": "The problem requires an analysis of the preconditioned linear system $\\mathbf{M}_{L}^{-1} \\mathbf{A} \\mathbf{z} = \\mathbf{M}_{L}^{-1} \\mathbf{b}$ arising from a finite element discretization of a poroelasticity model. The goal is to determine the spectrum and minimal polynomial of the preconditioned operator $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ and to relate these properties to the convergence of the GMRES method.\n\nThe given matrices are:\n$$\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{K} & \\mathbf{G}^{\\top} \\\\ \\mathbf{G} & -\\mathbf{C} \\end{pmatrix}, \\quad \\mathbf{M}_{L} = \\begin{pmatrix} \\mathbf{K} & 0 \\\\ \\mathbf{G} & -\\mathbf{S} \\end{pmatrix}\n$$\nwhere $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ has full row rank, $\\mathbf{C} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive semidefinite (SPSD), and $\\mathbf{S}$ is the exact Schur complement, defined as $\\mathbf{S} := -\\mathbf{C} - \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$. The problem assumes $\\mathbf{S}$ is invertible. This is a consistent assumption because $\\mathbf{K}$ being SPD implies $\\mathbf{K}^{-1}$ is SPD. Since $\\mathbf{G}$ has full row rank, the matrix $\\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$ is SPD. The matrix $\\mathbf{C}$ is SPSD. The sum of an SPD matrix ($\\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$) and an SPSD matrix ($\\mathbf{C}$) is SPD. Therefore, $\\mathbf{C} + \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$ is SPD and, in particular, invertible. This implies $\\mathbf{S} = -(\\mathbf{C} + \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top})$ is symmetric negative definite and thus invertible.\n\nFirst, we compute the inverse of the preconditioner $\\mathbf{M}_L$. Since $\\mathbf{M}_L$ is a block lower triangular matrix, its inverse $\\mathbf{M}_{L}^{-1}$ is also block lower triangular. Let\n$$\n\\mathbf{M}_{L}^{-1} = \\begin{pmatrix} \\mathbf{X} & \\mathbf{Y} \\\\ \\mathbf{Z} & \\mathbf{W} \\end{pmatrix}\n$$\nThe condition $\\mathbf{M}_L \\mathbf{M}_L^{-1} = I$ gives the system of block equations:\n$$\n\\begin{pmatrix} \\mathbf{K} & 0 \\\\ \\mathbf{G} & -\\mathbf{S} \\end{pmatrix} \\begin{pmatrix} \\mathbf{X} & \\mathbf{Y} \\\\ \\mathbf{Z} & \\mathbf{W} \\end{pmatrix} = \\begin{pmatrix} I_n & 0 \\\\ 0 & I_m \\end{pmatrix}\n$$\nThis leads to:\n1. $\\mathbf{K}\\mathbf{X} = I_n \\implies \\mathbf{X} = \\mathbf{K}^{-1}$ (since $\\mathbf{K}$ is invertible).\n2. $\\mathbf{K}\\mathbf{Y} = 0 \\implies \\mathbf{Y} = 0$ (since $\\mathbf{K}$ is invertible).\n3. $\\mathbf{G}\\mathbf{X} - \\mathbf{S}\\mathbf{Z} = 0 \\implies \\mathbf{S}\\mathbf{Z} = \\mathbf{G}\\mathbf{X} = \\mathbf{G}\\mathbf{K}^{-1} \\implies \\mathbf{Z} = \\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1}$ (since $\\mathbf{S}$ is invertible).\n4. $\\mathbf{G}\\mathbf{Y} - \\mathbf{S}\\mathbf{W} = I_m \\implies -\\mathbf{S}\\mathbf{W} = I_m \\implies \\mathbf{W} = -\\mathbf{S}^{-1}$.\n\nSo, the inverse of the preconditioner is:\n$$\n\\mathbf{M}_{L}^{-1} = \\begin{pmatrix} \\mathbf{K}^{-1} & 0 \\\\ \\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1} & -\\mathbf{S}^{-1} \\end{pmatrix}\n$$\nNext, we compute the preconditioned operator $\\mathbf{M}_{L}^{-1} \\mathbf{A}$:\n$$\n\\mathbf{M}_{L}^{-1} \\mathbf{A} = \\begin{pmatrix} \\mathbf{K}^{-1} & 0 \\\\ \\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1} & -\\mathbf{S}^{-1} \\end{pmatrix} \\begin{pmatrix} \\mathbf{K} & \\mathbf{G}^{\\top} \\\\ \\mathbf{G} & -\\mathbf{C} \\end{pmatrix}\n$$\nWe compute the product block by block:\nThe $(1,1)$ block is $\\mathbf{K}^{-1}\\mathbf{K} + 0 \\cdot \\mathbf{G} = I_n$.\nThe $(1,2)$ block is $\\mathbf{K}^{-1}\\mathbf{G}^{\\top} + 0 \\cdot (-\\mathbf{C}) = \\mathbf{K}^{-1}\\mathbf{G}^{\\top}$.\nThe $(2,1)$ block is $(\\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1})\\mathbf{K} + (-\\mathbf{S}^{-1})\\mathbf{G} = \\mathbf{S}^{-1}\\mathbf{G} - \\mathbf{S}^{-1}\\mathbf{G} = 0$.\nThe $(2,2)$ block is $(\\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1})\\mathbf{G}^{\\top} + (-\\mathbf{S}^{-1})(-\\mathbf{C}) = \\mathbf{S}^{-1}(\\mathbf{G}\\mathbf{K}^{-1}\\mathbf{G}^{\\top} + \\mathbf{C})$.\nUsing the definition of the Schur complement $\\mathbf{S} = -\\mathbf{C} - \\mathbf{G}\\mathbf{K}^{-1}\\mathbf{G}^{\\top}$, we have $\\mathbf{G}\\mathbf{K}^{-1}\\mathbf{G}^{\\top} + \\mathbf{C} = -\\mathbf{S}$.\nSubstituting this into the expression for the $(2,2)$ block gives $\\mathbf{S}^{-1}(-\\mathbf{S}) = -I_m$.\n\nThus, the preconditioned operator is a block upper triangular matrix:\n$$\n\\mathbf{M}_{L}^{-1} \\mathbf{A} = \\begin{pmatrix} I_n & \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0 & -I_m \\end{pmatrix}\n$$\nThe spectrum (set of eigenvalues) of a block triangular matrix is the union of the spectra of its diagonal blocks. The diagonal blocks are the identity matrix $I_n \\in \\mathbb{R}^{n \\times n}$ and the negative identity matrix $-I_m \\in \\mathbb{R}^{m \\times m}$.\nThe eigenvalues of $I_n$ are all $1$ (with algebraic multiplicity $n$).\nThe eigenvalues of $-I_m$ are all $-1$ (with algebraic multiplicity $m$).\nTherefore, the spectrum of $\\mathbf{M}_L^{-1} \\mathbf{A}$ is $\\sigma(\\mathbf{M}_L^{-1} \\mathbf{A}) = \\{1, -1\\}$.\n\nNow, we determine the minimal polynomial of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$. The minimal polynomial must have roots at all distinct eigenvalues, so its roots must be $1$ and $-1$. The simplest such polynomial is $q(\\lambda) = (\\lambda - 1)(\\lambda + 1) = \\lambda^2 - 1$. We check if this polynomial annihilates the matrix $\\mathbf{P} = \\mathbf{M}_{L}^{-1} \\mathbf{A}$.\n$$\n\\mathbf{P}^2 = \\begin{pmatrix} I_n & \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0 & -I_m \\end{pmatrix} \\begin{pmatrix} I_n & \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0 & -I_m \\end{pmatrix} = \\begin{pmatrix} I_n \\cdot I_n + \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\cdot 0 & I_n \\cdot \\mathbf{K}^{-1}\\mathbf{G}^{\\top} + \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\cdot (-I_m) \\\\ 0 \\cdot I_n + (-I_m) \\cdot 0 & 0 \\cdot \\mathbf{K}^{-1}\\mathbf{G}^{\\top} + (-I_m) \\cdot (-I_m) \\end{pmatrix}\n$$\n$$\n\\mathbf{P}^2 = \\begin{pmatrix} I_n & \\mathbf{K}^{-1}\\mathbf{G}^{\\top} - \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0 & I_m \\end{pmatrix} = \\begin{pmatrix} I_n & 0 \\\\ 0 & I_m \\end{pmatrix} = I_{n+m}\n$$\nSo, $\\mathbf{P}^2 - I = 0$. The polynomial $q(\\lambda) = \\lambda^2 - 1$ annihilates $\\mathbf{P}$. The minimal polynomial must divide $q(\\lambda)$. The divisors are $(\\lambda - 1)$ and $(\\lambda+1)$.\nThe polynomial $(\\lambda - 1)$ cannot be the minimal polynomial unless $\\mathbf{P} - I = 0$, which is not true in general as $\\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\neq 0$ and $-I_m \\neq I_m$.\nThe polynomial $(\\lambda + 1)$ cannot be the minimal polynomial unless $\\mathbf{P} + I = 0$, which is also not true.\nTherefore, the minimal polynomial of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ is $q(\\lambda) = \\lambda^2 - 1$.\n\nThe convergence of the GMRES method is governed by the properties of the operator, particularly its spectrum and its minimal polynomial. A key property of GMRES is that it is guaranteed to find the exact solution (in exact arithmetic) in a number of iterations that is at most the degree of the minimal polynomial of the system matrix.\nSince the degree of the minimal polynomial of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ is $2$, the GMRES method applied to the preconditioned system will converge to the exact solution in at most $2$ iterations. This demonstrates that the ideal preconditioner $\\mathbf{M}_L$ results in an extremely efficient solution process.\n\nFinally, the problem asks for the number of distinct eigenvalues of $\\mathbf{M}_L^{-1} \\mathbf{A}$. The spectrum is $\\{1, -1\\}$. These are two distinct values.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "While ideal preconditioners offer a theoretical benchmark, practical applications demand a careful balancing of computational resources. This practice problem simulates a common scenario in large-scale geomechanics: configuring the GMRES solver to run on a machine with finite memory . You will perform a detailed analysis of memory consumption and convergence estimates to determine an optimal restart parameter, highlighting the critical trade-off between the memory required to store Krylov basis vectors and the computational cost of solver restarts.",
            "id": "3537417",
            "problem": "A three-dimensional poroelastic finite element model of a deep rock mass leads to a large sparse linear system $A x = b$ that is solved with the Generalized Minimal Residual (GMRES) method using right preconditioning by an Incomplete LU (ILU) factorization. The number of degrees of freedom (DOF) is $n = 8.0 \\times 10^{6}$. The assembled stiffness matrix $A$ is stored in Compressed Sparse Row (CSR) format. The average number of nonzeros per row is $\\bar{z} = 60$, and both the floating-point values and column indices are stored in $64$-bit fields, each requiring $8$ bytes. The CSR row pointer array uses $8$ bytes per entry. The ILU(0) preconditioner has a nonzero count equal to $\\alpha \\, \\mathrm{nnz}(A)$ with $\\alpha = 1.2$, stored similarly to $A$ (values and indices in $64$-bit fields).\n\nAssume a compute node with total memory $M_{\\mathrm{node}} = 64\\,\\mathrm{GB}$, of which $M_{\\mathrm{OS}} = 5\\,\\mathrm{GB}$ must be reserved for the operating system and services. Additional finite element fields and assembly buffers occupy $M_{\\mathrm{FE}} = 8\\,\\mathrm{GB}$. Memory is measured in gigabytes with $1\\,\\mathrm{GB} = 10^{9}$ bytes. For GMRES workspace, at most a fraction $\\beta = 0.60$ of the remaining memory after subtracting $M_{\\mathrm{OS}}$, the CSR matrix storage, the ILU storage, and $M_{\\mathrm{FE}}$ may be used. The GMRES implementation stores the Arnoldi basis vectors explicitly; each vector of length $n$ uses $8n$ bytes. In addition to the Arnoldi basis, there is a fixed overhead of $n_{\\mathrm{ovh}} = 6$ vectors (for the current solution $x$, the right-hand side $b$, the residual $r$, the preconditioned image $z$, and two temporaries), and a dense upper Hessenberg matrix $H$ of size $(m+1) \\times m$ in double precision (for restarted GMRES with restart parameter $m$) or $(k+1) \\times k$ (for unrestarted GMRES with $k$ iterations), which uses $8(m+1)m$ or $8(k+1)k$ bytes, respectively.\n\nFrom prior runs on similar geomechanics models, the preconditioned operator yields an effective per-iteration residual reduction factor $\\rho_{\\mathrm{eff}} = 0.90$ in GMRES. The targeted relative residual tolerance is $\\varepsilon = 10^{-8}$.\n\nTasks:\n- Starting from the definitions above and fundamental memory accounting, derive formulas for the memory required to store $A$ in CSR and the ILU(0) factors.\n- Using these, compute the maximum GMRES workspace budget $M_{\\mathrm{kw}}$ in $\\mathrm{GB}$ available under the stated policy.\n- Estimate the GMRES workspace memory, in $\\mathrm{GB}$, needed to reach the tolerance $\\varepsilon$ without restart (choose the minimal integer $k$ iterations consistent with the given $\\rho_{\\mathrm{eff}}$).\n- For restarted GMRES with restart parameter $m$, derive the expected number of cycles $c(m)$ required to reach $\\varepsilon$ using the given $\\rho_{\\mathrm{eff}}$, and impose the balance criterion $c(m) \\leq 2$ while ensuring the GMRES workspace memory does not exceed $M_{\\mathrm{kw}}$.\n- Determine the smallest integer restart parameter $m$ satisfying both constraints.\n\nExpress your final answer as the chosen integer restart parameter $m$ (no units).",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All necessary data are provided, and the context is a standard application of numerical linear algebra in computational geomechanics. We proceed with the solution.\n\nThe problem requires a step-by-step analysis of memory constraints and performance estimates for the GMRES algorithm. We will address each task in sequence.\n\nFirst, we define all symbolic constants and given numerical values:\n- Number of degrees of freedom (DOF): $n = 8.0 \\times 10^{6}$\n- Average nonzeros per row in matrix $A$: $\\bar{z} = 60$\n- Storage for floating-point values and indices: $8$ bytes each\n- ILU(0) fill-in factor: $\\alpha = 1.2$\n- Total node memory: $M_{\\mathrm{node}} = 64\\,\\mathrm{GB} = 64 \\times 10^{9}$ bytes\n- OS reserved memory: $M_{\\mathrm{OS}} = 5\\,\\mathrm{GB} = 5 \\times 10^{9}$ bytes\n- FE data memory: $M_{\\mathrm{FE}} = 8\\,\\mathrm{GB} = 8 \\times 10^{9}$ bytes\n- GMRES workspace memory fraction: $\\beta = 0.60$\n- Bytes per vector entry (double precision): $8$ bytes\n- GMRES fixed overhead vectors: $n_{\\mathrm{ovh}} = 6$\n- Effective per-iteration residual reduction factor: $\\rho_{\\mathrm{eff}} = 0.90$\n- Target relative residual tolerance: $\\varepsilon = 10^{-8}$\n\nTask 1: Derive formulas for the memory required to store $A$ in CSR and the ILU(0) factors.\n\nThe total number of nonzero entries in matrix $A$, denoted $\\mathrm{nnz}(A)$, is the product of the number of rows $n$ and the average number of nonzeros per row $\\bar{z}$:\n$$\n\\mathrm{nnz}(A) = n \\times \\bar{z} = (8.0 \\times 10^{6}) \\times 60 = 4.8 \\times 10^{8}\n$$\nThe Compressed Sparse Row (CSR) format for an $n \\times n$ matrix requires storing three arrays: one for the nonzero floating-point values, one for their column indices, and one for the row pointers.\n- The `values` array stores $\\mathrm{nnz}(A)$ numbers, each requiring $8$ bytes.\n- The `indices` array stores $\\mathrm{nnz}(A)$ integers, each requiring $8$ bytes.\n- The `row_pointers` array has $n+1$ entries, each requiring $8$ bytes.\n\nThe formula for the total memory occupied by matrix $A$, $M_A$, is:\n$$\nM_A = \\mathrm{nnz}(A) \\times (8 + 8) + (n+1) \\times 8 = 16 \\cdot \\mathrm{nnz}(A) + 8(n+1)\n$$\nSubstituting the values:\n$$\nM_A = 16 \\times (4.8 \\times 10^{8}) + 8 \\times (8.0 \\times 10^{6} + 1) = 7.68 \\times 10^{9} + 6.4 \\times 10^{7} + 8 = 7.744 \\times 10^{9} + 8\\,\\mathrm{bytes}\n$$\nIn gigabytes ($1\\,\\mathrm{GB} = 10^9$ bytes), $M_A \\approx 7.744\\,\\mathrm{GB}$.\n\nFor the ILU(0) preconditioner, the number of nonzeros is $\\mathrm{nnz}(\\mathrm{ILU}) = \\alpha \\cdot \\mathrm{nnz}(A)$:\n$$\n\\mathrm{nnz}(\\mathrm{ILU}) = 1.2 \\times (4.8 \\times 10^{8}) = 5.76 \\times 10^{8}\n$$\nAssuming the ILU factors are stored in a similar CSR-like structure, the memory formula for the preconditioner, $M_{\\mathrm{ILU}}$, is:\n$$\nM_{\\mathrm{ILU}} = 16 \\cdot \\mathrm{nnz}(\\mathrm{ILU}) + 8(n+1)\n$$\nSubstituting the values:\n$$\nM_{\\mathrm{ILU}} = 16 \\times (5.76 \\times 10^{8}) + 8 \\times (8.0 \\times 10^{6} + 1) = 9.216 \\times 10^{9} + 6.4 \\times 10^{7} + 8 = 9.28 \\times 10^{9} + 8\\,\\mathrm{bytes}\n$$\nIn gigabytes, $M_{\\mathrm{ILU}} \\approx 9.28\\,\\mathrm{GB}$.\n\nTask 2: Compute the maximum GMRES workspace budget $M_{\\mathrm{kw}}$.\n\nThe available memory for computation is the total node memory minus memory for the OS, FE data, matrix $A$, and the ILU preconditioner.\n$$\nM_{\\mathrm{rem}} = M_{\\mathrm{node}} - M_{\\mathrm{OS}} - M_{\\mathrm{FE}} - M_A - M_{\\mathrm{ILU}}\n$$\nUsing the values in GB:\n$$\nM_{\\mathrm{rem}} \\approx 64 - 5 - 8 - 7.744 - 9.28 = 33.976\\,\\mathrm{GB}\n$$\nThe GMRES workspace budget $M_{\\mathrm{kw}}$ is a fraction $\\beta$ of this remaining memory:\n$$\nM_{\\mathrm{kw}} = \\beta \\cdot M_{\\mathrm{rem}} = 0.60 \\times 33.976\\,\\mathrm{GB} = 20.3856\\,\\mathrm{GB}\n$$\n\nTask 3: Estimate the GMRES workspace memory needed to reach the tolerance $\\varepsilon$ without restart.\n\nThe number of iterations $k$ required for the relative residual to drop to $\\varepsilon$ is estimated by the inequality:\n$$\n\\rho_{\\mathrm{eff}}^{k} \\le \\varepsilon\n$$\nTaking the natural logarithm of both sides:\n$$\nk \\ln(\\rho_{\\mathrm{eff}}) \\le \\ln(\\varepsilon) \\implies k \\ge \\frac{\\ln(\\varepsilon)}{\\ln(\\rho_{\\mathrm{eff}})}\n$$\nsince $\\ln(\\rho_{\\mathrm{eff}}) < 0$.\n$$\nk \\ge \\frac{\\ln(10^{-8})}{\\ln(0.90)} = \\frac{-8 \\ln(10)}{\\ln(0.90)} \\approx \\frac{-8 \\times 2.30259}{-0.10536} \\approx 174.83\n$$\nThe minimal integer number of iterations is $k = 175$.\n\nThe workspace memory for unrestarted GMRES, $M_{\\mathrm{GMRES}}(k)$, consists of storage for the basis vectors and the Hessenberg matrix.\n- Basis vectors: $k$ Arnoldi vectors + $n_{\\mathrm{ovh}}$ overhead vectors. Each vector has length $n$ and uses $8$ bytes per entry. Memory is $(k + n_{\\mathrm{ovh}}) \\times 8n$.\n- Hessenberg matrix $H$: A dense $(k+1) \\times k$ matrix, using $8(k+1)k$ bytes.\nThe formula for workspace memory is $M_{\\mathrm{GMRES}}(k) = (k + n_{\\mathrm{ovh}}) \\cdot 8n + 8(k+1)k$.\nSubstituting $k=175$ and $n_{\\mathrm{ovh}}=6$:\n$$\n\\text{Vector memory} = (175 + 6) \\times 8 \\times (8.0 \\times 10^6) = 181 \\times 64 \\times 10^6 = 11.584 \\times 10^9\\,\\text{bytes} = 11.584\\,\\mathrm{GB}\n$$\n$$\n\\text{Hessenberg memory} = 8 \\times (175+1) \\times 175 = 8 \\times 176 \\times 175 = 246400\\,\\text{bytes} \\approx 0.00025\\,\\mathrm{GB}\n$$\nThe total workspace memory for unrestarted GMRES is:\n$$\nM_{\\mathrm{GMRES}}(175) \\approx 11.584 + 0.00025 = 11.58425\\,\\mathrm{GB}\n$$\nThis is the estimated memory needed to reach the tolerance without restart.\n\nTask 4 & 5: Determine the smallest integer restart parameter $m$.\n\nFor restarted GMRES, we must find the smallest integer $m$ that satisfies two constraints: the workspace memory must not exceed $M_{\\mathrm{kw}}$, and the number of outer cycles must not exceed $2$.\n\nConstraint 1: Memory\nThe workspace for restarted GMRES with restart parameter $m$ is given by:\n$$\nM_{\\mathrm{GMRES}}(m) = (m + n_{\\mathrm{ovh}}) \\cdot 8n + 8(m+1)m\n$$\nThis must be less than or equal to the available budget $M_{\\mathrm{kw}}$:\n$$\nM_{\\mathrm{GMRES}}(m) \\le M_{\\mathrm{kw}}\n$$\nSubstituting values (in bytes):\n$$\n(m + 6) \\times 8 \\times (8.0 \\times 10^6) + 8(m+1)m \\le 20.3856 \\times 10^9\n$$\n$$\n(m + 6) \\times 6.4 \\times 10^7 + 8(m+1)m \\le 20.3856 \\times 10^9\n$$\nThe Hessenberg matrix memory $8(m+1)m$ is negligible compared to the vector storage. As a very accurate approximation, we can bound the vector storage component:\n$$\n(m + 6) \\times 6.4 \\times 10^7 \\lesssim 20.3856 \\times 10^9\n$$\n$$\nm + 6 \\lesssim \\frac{20.3856 \\times 10^9}{6.4 \\times 10^7} = \\frac{2038.56}{6.4} = 318.525\n$$\n$$\nm \\lesssim 312.525\n$$\nSince $m$ must be an integer, the memory constraint imposes $m \\le 312$.\n\nConstraint 2: Number of Cycles\nThe formula for the number of cycles $c(m)$ to reach the total number of iterations $k=175$ is:\n$$\nc(m) = \\left\\lceil \\frac{k}{m} \\right\\rceil = \\left\\lceil \\frac{175}{m} \\right\\rceil\n$$\nThe problem imposes the constraint $c(m) \\le 2$, which means $1 < c(m) \\le 2$ since $c(m)=1$ would imply $m \\ge 175$.\nThe condition $\\lceil 175/m \\rceil \\le 2$ implies that $1 < 175/m \\le 2$.\n- From $175/m \\le 2$, we get $m \\ge 175/2 = 87.5$.\n- From $1 < 175/m$, we get $m < 175$.\nCombining these, we need an integer $m$ such that $87.5 \\le m < 175$, which means $m \\in \\{88, 89, \\dots, 174\\}$. Thus, the smallest possible value for $m$ under this constraint is $m=88$.\n\nCombining both constraints:\n- From memory: $m \\le 312$.\n- From cycle count: $88 \\le m \\le 174$.\n\nThe smallest integer $m$ that satisfies both sets of conditions is the minimum value in the intersection of the two allowed ranges.\nThe intersection is $[88, 174]$. The smallest integer in this range is $88$.\n\nTherefore, the smallest integer restart parameter $m$ is $88$.",
            "answer": "$$\n\\boxed{88}\n$$"
        },
        {
            "introduction": "Modern computational geomechanics leverages not only sophisticated algorithms but also advanced hardware-aware strategies. This hands-on coding practice challenges you to implement a solver that embodies this paradigm: a mixed-precision, Flexible GMRES (FGMRES) method . You will discover why the 'flexibility' of FGMRES is essential when using preconditioners that change from one iteration to the next—as is the case in mixed-precision schemes—and assess how solver performance is impacted by the physical properties of the underlying model.",
            "id": "3537429",
            "problem": "Consider the quasi-static, small-strain, scalar surrogate of soil mechanics on the unit square domain, where the discrete balance of linear momentum reduces to the linear system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ with $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite. The matrix $\\mathbf{A}$ is the stiffness-like matrix arising from a variable-coefficient diffusion operator $-\\nabla \\cdot (k(\\mathbf{x}) \\nabla u)$ with zero Dirichlet boundary conditions on the entire boundary. The coefficient $k(\\mathbf{x})$ represents an effective stiffness surrogate and is piecewise-constant in horizontal layers to model a soil with stiffness contrast between layers.\n\nImplement a Flexible Generalized Minimal Residual (FGMRES) algorithm as an outer solver in double precision to approximately solve $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$, with a right preconditioner $\\mathbf{M}^{-1}$ applied in single precision. The preconditioner must be a diagonal (Jacobi) preconditioner computed and applied in single precision arithmetic, while all outer FGMRES vector operations and residual minimizations must be performed in double precision arithmetic. This models a mixed-precision strategy in which preconditioning is offloaded to a graphics processing unit (GPU) in single precision, while the Krylov subspace operations are performed in double precision on the central processing unit (CPU). The FGMRES implementation must be from first principles using the Arnoldi process with Givens rotations and must support restarts.\n\nYou must construct $\\mathbf{A}$ by finite differences on a uniform grid with $N_x$ nodes in the $x$-direction and $N_y$ nodes in the $y$-direction, using the standard five-point stencil for a variable coefficient operator. Let the unknowns be defined on interior nodes only, so the number of unknowns is $(N_x-2)(N_y-2)$. Let the grid spacings be $h_x = 1/(N_x-1)$ and $h_y = 1/(N_y-1)$ with $h_x = h_y$. Use harmonic averaging for the interface coefficient to ensure symmetry and coercivity: if $k_i$ and $k_j$ are the coefficients in adjacent cells across a face, the face coefficient is $k_f = 2 k_i k_j / (k_i + k_j)$. The right-hand side must be $\\mathbf{b}$ corresponding to a uniform body load $f = 1$, scaled consistently with the finite difference discretization.\n\nDefine a layered soil with a horizontal interface at $y = 0.5$. Let the stiffness surrogate take values $k_{\\text{soft}} = 1$ in the upper half ($y < 0.5$) and $k_{\\text{stiff}} = \\rho$ in the lower half ($y \\ge 0.5$), where $\\rho$ is the stiffness contrast ratio. The contrast ratio $\\rho$ controls the condition number of $\\mathbf{A}$ and thus the convergence behavior of Krylov methods.\n\nYou must assess how the stiffness contrast influences the achievable residual reduction under a fixed computational budget by reporting, for each test case, the ratio $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{r}_0\\|_2$ after the algorithm has expended a total number of matrix-vector products equal to a given maximum iteration budget, using FGMRES with specified restart length. Here, $\\mathbf{r}_k = \\mathbf{b} - \\mathbf{A} \\mathbf{x}_k$ is the residual at iteration $k$, and $\\mathbf{x}_0 = \\mathbf{0}$ is the initial guess.\n\nFundamental bases to use:\n- The finite difference discretization of $-\\nabla \\cdot (k(\\mathbf{x}) \\nabla u) = f$ on a uniform grid with zero Dirichlet boundary conditions and harmonic averaging of coefficients.\n- The definition of Krylov subspaces and the Arnoldi process for building an orthonormal basis and the associated upper Hessenberg matrix that yields a least-squares problem minimized by Givens rotations.\n- The preconditioning concept $\\mathbf{M} \\approx \\mathbf{A}$ and right-preconditioned FGMRES, with the requirement that preconditioning is applied in single precision and the outer FGMRES in double precision.\n\nYou must not use any external input. Implement everything self-contained and return a single line of output containing the results of the test suite as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case $1$: $(N_x, N_y, \\rho, m, K_{\\max}) = (34, 34, 1, 30, 60)$.\n- Case $2$: $(N_x, N_y, \\rho, m, K_{\\max}) = (34, 34, 10^3, 30, 60)$.\n- Case $3$: $(N_x, N_y, \\rho, m, K_{\\max}) = (34, 34, 10^6, 30, 60)$.\n- Case $4$ (edge case testing a short restart under extreme contrast): $(N_x, N_y, \\rho, m, K_{\\max}) = (34, 34, 10^6, 10, 20)$.\n\nAlgorithmic requirements:\n- Build $\\mathbf{A}$ as a sparse matrix using harmonic averaging across faces, with unknowns on interior nodes only. Use $\\mathbf{b}$ corresponding to $f = 1$ applied uniformly on interior nodes with proper scaling by cell area.\n- Implement right-preconditioned FGMRES with restart $m$ and a maximum total budget of $K_{\\max}$ matrix-vector products. Use $\\mathbf{x}_0 = \\mathbf{0}$.\n- Preconditioner $\\mathbf{M}^{-1}$ must be the diagonal inverse of $\\mathbf{A}$ computed and applied in single precision. That is, if $\\mathbf{D} = \\operatorname{diag}(\\mathbf{A})$, then apply $\\mathbf{z} = \\mathbf{D}^{-1} \\mathbf{r}$ using single precision arithmetic for both $\\mathbf{D}$ and $\\mathbf{r}$, casting inputs to single precision and outputs back to double precision.\n- Record the final residual reduction $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{r}_0\\|_2$ after the algorithm completes the allowed budget.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").\n- Each result must be a float written in scientific notation with exactly six digits after the decimal point (e.g., \"1.234567e-08\").\n- No units are involved; report pure dimensionless ratios.\n\nYour program will be judged on correctness, robustness, and adherence to the mixed-precision constraint and the specified output format. No user input is allowed, and no randomization should be used so that results are deterministic.",
            "solution": "The problem requires the implementation of a flexible, mixed-precision, preconditioned Generalized Minimal Residual (FGMRES) method to solve a system of linear equations $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$. This system arises from the finite difference discretization of a variable-coefficient diffusion equation, which serves as a scalar surrogate for quasi-static soil mechanics. The objective is to evaluate the solver's performance under varying stiffness contrasts in a layered soil model.\n\n### 1. Mathematical Model and Discretization\n\nThe physical problem is modeled by the partial differential equation (PDE) on the unit square domain $\\Omega = (0,1) \\times (0,1)$:\n$$ -\\nabla \\cdot (k(\\mathbf{x}) \\nabla u(\\mathbf{x})) = f(\\mathbf{x}) $$\nwith zero Dirichlet boundary conditions, $u(\\mathbf{x}) = 0$ on $\\partial\\Omega$. The coefficient $k(\\mathbf{x})$ represents a surrogate for soil stiffness and is piecewise-constant. For a horizontal interface at $y=0.5$, $k(\\mathbf{x})$ is $k_{\\text{soft}} = 1$ for $y < 0.5$ and $k_{\\text{stiff}} = \\rho$ for $y \\ge 0.5$, where $\\rho$ is the stiffness contrast. The source term $f(\\mathbf{x}) = 1$ corresponds to a uniform body load.\n\nWe discretize this PDE using a five-point finite difference stencil on a uniform grid with $N_x \\times N_y$ nodes and grid spacing $h = h_x = h_y = 1/(N_x-1)$. The unknowns $u_{i,j}$ are located at the interior nodes $(x_i, y_j)$ for $i,j \\in \\{1, \\dots, N_x-2\\}$. Integrating the PDE over a control volume of area $h^2$ around each node $(i,j)$ yields the discrete linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$.\n\nThe matrix entry $\\mathbf{A}_{pq}$ corresponding to the interaction between nodes $p$ and $q$ is derived from the flux term $-k_f \\nabla u \\cdot \\mathbf{n}$, where $k_f$ is the coefficient on the face between the cells. To ensure the resulting matrix $\\mathbf{A}$ is symmetric and positive definite (SPD), we use harmonic averaging for the coefficients on cell faces. For a face between cells with stiffnesses $k_1$ and $k_2$, the face stiffness is $k_f = 2 k_1 k_2 / (k_1 + k_2)$.\n\nThe stencil for an interior node $p$ corresponding to grid point $(i,j)$ is:\n$$ \\mathbf{A}_{p,p} u_{i,j} + \\mathbf{A}_{p,p_W} u_{i-1,j} + \\mathbf{A}_{p,p_E} u_{i+1,j} + \\mathbf{A}_{p,p_S} u_{i,j-1} + \\mathbf{A}_{p,p_N} u_{i,j+1} = \\mathbf{b}_p $$\nwhere the coefficients are given by the face stiffnesses:\n- Diagonal: $\\mathbf{A}_{p,p} = k_{i-1/2,j} + k_{i+1/2,j} + k_{i,j-1/2} + k_{i,j+1/2}$\n- Off-diagonals: $\\mathbf{A}_{p,p_W} = -k_{i-1/2,j}$, $\\mathbf{A}_{p,p_E} = -k_{i+1/2,j}$, etc.\nThe right-hand side is $\\mathbf{b}_p = f_{i,j} h^2 = h^2$, since $f=1$. The unknowns are arranged in a vector $\\mathbf{x}$ of size $n = (N_x-2)(N_y-2)$ using a row-major mapping from 2D grid indices to a 1D vector index.\n\n### 2. Flexible GMRES (FGMRES) with Mixed-Precision Preconditioning\n\nThe large, sparse linear system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ is solved iteratively. We use a restarted FGMRES method, which is a Krylov subspace method suitable for preconditioners that may vary at each step or are nonlinear. This \"flexibility\" is essential for our mixed-precision strategy.\n\n**Algorithm Outline**: FGMRES(m) iteratively finds an approximate solution in a Krylov subspace of dimension $m$. After $m$ iterations, the solver is restarted, using the current solution as the initial guess for the next cycle.\n\n1.  **Initialization**: Start with an initial guess $\\mathbf{x}_0 = \\mathbf{0}$. The initial residual is $\\mathbf{r}_0 = \\mathbf{b} - \\mathbf{A} \\mathbf{x}_0 = \\mathbf{b}$.\n2.  **Restart Cycle**: A cycle consists of $m$ steps of the Arnoldi process.\n    -   At the beginning of a cycle, the current residual $\\mathbf{r}$ is normalized to form the first basis vector of a new Krylov subspace: $\\mathbf{v}_1 = \\mathbf{r} / \\|\\mathbf{r}\\|_2$.\n    -   **Arnoldi Process**: For $j=1, \\dots, m$:\n        a.  **Preconditioning**: Apply the preconditioner to the current basis vector: $\\mathbf{z}_j = \\mathbf{M}^{-1}\\mathbf{v}_j$. This is the flexible step; standard GMRES would build a Krylov space for the fixed operator $\\mathbf{A}\\mathbf{M}^{-1}$.\n        b.  **Matrix-Vector Product**: Compute $\\mathbf{w} = \\mathbf{A} \\mathbf{z}_j$. This is the most computationally expensive operation.\n        c.  **Orthogonalization**: Orthonormalize $\\mathbf{w}$ against the existing basis vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_j\\}$ using the Modified Gram-Schmidt procedure. This yields the new basis vector $\\mathbf{v}_{j+1}$ and the $j$-th column of an $(m+1) \\times m$ upper Hessenberg matrix $\\bar{\\mathbf{H}}_m$. The Arnoldi process generates an orthonormal basis $\\mathbf{V}_{m+1} = [\\mathbf{v}_1, \\dots, \\mathbf{v}_{m+1}]$ and a matrix of preconditioned vectors $\\mathbf{Z}_m = [\\mathbf{z}_1, \\dots, \\mathbf{z}_m]$ such that $\\mathbf{A} \\mathbf{Z}_m = \\mathbf{V}_{m+1} \\bar{\\mathbf{H}}_m$.\n3.  **Least-Squares Problem**: The solution increment within the cycle is found by solving a small least-squares problem: Find $\\mathbf{y}_m \\in \\mathbb{R}^m$ that minimizes $\\| \\|\\mathbf{r}\\|_2 \\mathbf{e}_1 - \\bar{\\mathbf{H}}_m \\mathbf{y}_m \\|_2$. This problem is efficiently solved on-the-fly using Givens rotations, which transform $\\bar{\\mathbf{H}}_m$ into an upper-triangular matrix $\\mathbf{R}_m$ and the right-hand side $\\|\\mathbf{r}\\|_2 \\mathbf{e}_1$ into a new vector $\\mathbf{g}$.\n4.  **Solution Update**: Solve the triangular system $\\mathbf{R}_m \\mathbf{y}_m = \\mathbf{g}_{1:m}$ for $\\mathbf{y}_m$ via back-substitution. The solution is then updated: $\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{Z}_m \\mathbf{y}_m$.\n5.  **Termination**: The process continues for multiple restart cycles until a maximum budget of matrix-vector products, $K_{\\max}$, is expended.\n\n### 3. Mixed-Precision Jacobi Preconditioner\n\nPreconditioning is used to accelerate convergence by transforming the system into one that is easier to solve. We use a simple right preconditioner $\\mathbf{M}^{-1}$, where $\\mathbf{M}$ is the diagonal of $\\mathbf{A}$ (a Jacobi preconditioner). The preconditioned system is $\\mathbf{A} \\mathbf{M}^{-1} (\\mathbf{M} \\mathbf{x}) = \\mathbf{b}$.\n\nThe key requirement is mixed precision:\n-   The outer FGMRES loop (vector operations, inner products, norms) is performed in **double precision** (`numpy.float64`).\n-   The preconditioner application, $\\mathbf{z}=\\mathbf{M}^{-1}\\mathbf{r}$, is performed in **single precision** (`numpy.float32`). This simulates offloading this computationally intensive but often less accuracy-sensitive task to hardware like a GPU that excels at single-precision arithmetic.\n\nThe preconditioning step $\\mathbf{z} = \\mathbf{M}^{-1}\\mathbf{r}$ is implemented as follows:\n1.  The diagonal of $\\mathbf{A}$, stored as $\\mathbf{D} = \\operatorname{diag}(\\mathbf{A})$, is cast to single precision.\n2.  The input residual vector $\\mathbf{r}$ (double precision) is cast to single precision.\n3.  The element-wise division $z_i = r_i / D_i$ is performed in single precision.\n4.  The result vector $\\mathbf{z}$ is cast back to double precision before being used in the FGMRES algorithm.\n\nThis precision casting makes the preconditioner a nonlinear operator, necessitating the use of a flexible method like FGMRES.\n\n### 4. Final Assessment\n\nAfter the FGMRES algorithm expends the total budget of $K_{\\max}$ matrix-vector products, a final solution vector $\\mathbf{x}_{K_{\\max}}$ is obtained. The performance is assessed by computing the true final residual $\\mathbf{r}_{K_{\\max}} = \\mathbf{b} - \\mathbf{A} \\mathbf{x}_{K_{\\max}}$ and reporting the relative residual norm reduction: $\\|\\mathbf{r}_{K_{\\max}}\\|_2 / \\|\\mathbf{r}_0\\|_2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csr_matrix\nfrom scipy.linalg import solve_triangular\nimport math\n\ndef build_stiffness_matrix(Nx, Ny, rho):\n    \"\"\"\n    Builds the finite difference stiffness matrix A for the variable-coefficient\n    diffusion problem and the corresponding right-hand side vector b.\n\n    Args:\n        Nx (int): Number of grid points in the x-direction.\n        Ny (int): Number of grid points in the y-direction.\n        rho (float): Stiffness contrast ratio.\n\n    Returns:\n        tuple: A CSR sparse matrix (A) and a NumPy array (b).\n    \"\"\"\n    if Nx != Ny:\n        raise ValueError(\"Grid must be square (Nx=Ny).\")\n    h = 1.0 / (Nx - 1)\n    n_int_x = Nx - 2\n    n_int_y = Ny - 2\n    n_unknowns = n_int_x * n_int_y\n\n    y_coords = np.linspace(0, 1, Ny, dtype=np.float64)\n    k_vals_at_nodes = np.ones(Ny, dtype=np.float64)\n    k_vals_at_nodes[y_coords >= 0.5] = rho\n    \n    k_grid = np.ones((Nx, Ny), dtype=np.float64) * k_vals_at_nodes[np.newaxis, :].T\n\n    A = lil_matrix((n_unknowns, n_unknowns), dtype=np.float64)\n    b = np.full(n_unknowns, h * h, dtype=np.float64)\n\n    def harmonic_mean(k1, k2):\n        if k1 + k2 == 0: return 0.0\n        return 2.0 * k1 * k2 / (k1 + k2)\n\n    for j_node in range(1, Ny - 1):\n        for i_node in range(1, Nx - 1):\n            p = (j_node - 1) * n_int_x + (i_node - 1)\n\n            k_center = k_grid[i_node, j_node]\n            k_W = harmonic_mean(k_center, k_grid[i_node - 1, j_node])\n            k_E = harmonic_mean(k_center, k_grid[i_node + 1, j_node])\n            k_S = harmonic_mean(k_center, k_grid[i_node, j_node - 1])\n            k_N = harmonic_mean(k_center, k_grid[i_node, j_node + 1])\n\n            A[p, p] = k_W + k_E + k_S + k_N\n            \n            if i_node > 1: A[p, p - 1] = -k_W\n            if i_node < n_int_x: A[p, p + 1] = -k_E\n            if j_node > 1: A[p, p - n_int_x] = -k_S\n            if j_node < n_int_y: A[p, p + n_int_x] = -k_N\n                \n    return A.tocsr(), b\n\ndef givens_rotation(a, b):\n    \"\"\"Computes components of a Givens rotation matrix.\"\"\"\n    if b == 0.0:\n        return 1.0, 0.0\n    h = math.hypot(a, b)\n    return a / h, b / h\n\ndef fgmres(A, b, x0, m, K_max, preconditioner):\n    \"\"\"\n    Implements a right-preconditioned Flexible GMRES (FGMRES) method.\n\n    Args:\n        A (scipy.sparse.csr_matrix): The system matrix.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess.\n        m (int): The restart length.\n        K_max (int): Maximum number of matrix-vector products.\n        preconditioner (callable): The preconditioning function.\n\n    Returns:\n        tuple: The final solution vector (x) and the initial residual norm (r0_norm).\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy().astype(np.float64)\n    \n    # K_max is budget for Arnoldi matvecs, initial residual is separate\n    if np.any(x0):\n        r = b - A @ x\n    else:\n        r = b.copy()\n\n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return x, r0_norm\n\n    total_mv = 0\n    while total_mv < K_max:\n        m_cycle = min(m, K_max - total_mv)\n        if m_cycle <= 0: break\n\n        beta = np.linalg.norm(r)\n        if beta / r0_norm < 1e-12: break # converged\n\n        V = [r / beta]\n        Z = []\n        H = np.zeros((m_cycle + 1, m_cycle), dtype=np.float64)\n        \n        breakdown_j = -1\n        # Arnoldi process\n        for j in range(m_cycle):\n            z_j = preconditioner(V[j])\n            Z.append(z_j)\n            w = A @ z_j\n            total_mv += 1\n\n            w_ortho = w.copy()\n            for i in range(j + 1):\n                H[i, j] = np.dot(V[i], w_ortho)\n                w_ortho -= H[i, j] * V[i]\n            \n            H[j + 1, j] = np.linalg.norm(w_ortho)\n            if H[j + 1, j] < 1e-15:\n                breakdown_j = j\n                break\n            \n            V.append(w_ortho / H[j + 1, j])\n        \n        if breakdown_j != -1:\n            m_cycle = breakdown_j + 1\n            H = H[:m_cycle + 1, :m_cycle]\n\n        # Solve Least Squares via Givens rotations\n        g = np.zeros(m_cycle + 1, dtype=np.float64)\n        g[0] = beta\n        H_qr = H.copy()\n\n        for j in range(m_cycle):\n            c, s = givens_rotation(H_qr[j, j], H_qr[j + 1, j])\n            rot_mat = np.array([[c, s], [-s, c]])\n            H_qr[j:j+2, j:] = rot_mat @ H_qr[j:j+2, j:]\n            g[j:j+2] = rot_mat @ g[j:j+2]\n            \n        y = solve_triangular(H_qr[:m_cycle, :m_cycle], g[:m_cycle], check_finite=False)\n        \n        # Update solution vector x\n        for i in range(m_cycle):\n            x += y[i] * Z[i]\n\n        # Efficiently update residual for the next restart cycle\n        res_in_V_basis = beta * np.eye(m_cycle + 1, 1).flatten() - H @ y\n        r = np.zeros_like(b, dtype=np.float64)\n        for i in range(m_cycle + 1):\n            r += res_in_V_basis[i] * V[i]\n        \n        if breakdown_j != -1:\n            break\n            \n    return x, r0_norm\n\ndef solve():\n    test_cases = [\n        (34, 34, 1.0, 30, 60),\n        (34, 34, 1e3, 30, 60),\n        (34, 34, 1e6, 30, 60),\n        (34, 34, 1e6, 10, 20),\n    ]\n\n    results = []\n    for Nx, Ny, rho, m, K_max in test_cases:\n        A, b = build_stiffness_matrix(Nx, Ny, rho)\n        n_unknowns = (Nx-2) * (Ny-2)\n        x0 = np.zeros(n_unknowns, dtype=np.float64)\n        \n        # Define the mixed-precision preconditioner\n        D_single = A.diagonal().astype(np.float32)\n        def jacobi_preconditioner_sp(res_dp):\n            res_sp = res_dp.astype(np.float32)\n            z_sp = res_sp / D_single\n            return z_sp.astype(np.float64)\n        \n        # Run FGMRES\n        x_final, r0_norm = fgmres(A, b, x0, m, K_max, jacobi_preconditioner_sp)\n        \n        # Calculate final residual ratio for reporting\n        r_final = b - A @ x_final\n        final_ratio = np.linalg.norm(r_final) / r0_norm if r0_norm > 0 else 0.0\n        results.append(f\"{final_ratio:.6e}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}