{
    "hands_on_practices": [
        {
            "introduction": "在计算系统生物学中，一项核心技能是利用模型拟合的输出（如对数似然）来比较不同模型的优劣。本练习将您置于一个经典的单细胞转录组学场景中，您需要为基因表达计数选择最合适的统计分布。通过亲手计算 AIC、BIC 和 Akaike 权重，您将学会如何量化支持每个候选模型的证据，从而做出有根据的模型选择 。",
            "id": "3326796",
            "problem": "一项单细胞转录组学实验测量了一个基因在 $n$ 个细胞中的分子计数。您的任务是为此基因从三个候选计数模型中进行选择，每个模型都通过最大似然法拟合为广义线性模型 (Generalized Linear Model, GLM)：泊松 GLM (Poisson GLM)、负二项 GLM (Negative Binomial GLM) 和零膨胀负二项 GLM (Zero-Inflated Negative Binomial GLM)。设样本量为 $n = 300$。对于这些候选模型，最大化对数似然和自由参数数量如下（所有拟合都针对相同的数据，并且所有模型都使用对数链接函数 (log-link) 和基因特异性截距；负二项 GLM 包含一个离散参数，零膨胀负二项 GLM 进一步包含一个零膨胀参数）：\n- 泊松 GLM：最大化对数似然 $\\ell_{\\mathrm{P}} = -1185.7$，参数数量 $k_{\\mathrm{P}} = 1$。\n- 负二项 GLM：最大化对数似然 $\\ell_{\\mathrm{NB}} = -805.3$，参数数量 $k_{\\mathrm{NB}} = 2$。\n- 零膨胀负二项 GLM：最大化对数似然 $\\ell_{\\mathrm{ZINB}} = -799.9$，参数数量 $k_{\\mathrm{ZINB}} = 3$。\n\n从未知真实数据生成密度与候选模型之间的 Kullback–Leibler 散度的定义出发，并利用最大似然估计量的大样本行为以及对边际似然的 Laplace 近似，按以下步骤进行：\n1. 推导适用于在大小为 $n$ 的同一数据集上通过最大似然法拟合的模型的赤池信息准则 (Akaike Information Criterion, AIC) 和贝叶斯信息准则 (Bayesian Information Criterion, BIC) 的比较形式，并从第一性原理证明每个惩罚项的合理性。\n2. 使用您推导的表达式，计算每个模型的 AIC 和 BIC 值，然后通过减去每个准则在所有模型中的最小值来计算 AIC 差异 $\\Delta \\mathrm{AIC}_{i}$ 和 BIC 差异 $\\Delta \\mathrm{BIC}_{i}$。\n3. 使用 $\\Delta \\mathrm{AIC}_{i}$ 值，计算每个模型的赤池权重 (Akaike weights)，以量化它们的相对支持度。\n4. 最后，报告具有最强支持度的模型的赤池权重。将您最终报告的值四舍五入到四位有效数字。不要包含任何单位。",
            "solution": "问题陈述具有科学依据，内容自洽且表述清晰，可以进行严谨的求解。它涉及为计数数据选择统计模型，这是计算系统生物学中的一项标准任务，使用了公认的信息准则。所有必要的数据均已提供。\n\n**1. AIC 和 BIC 的推导**\n\n模型选择的目标是选择一个能够最好地逼近真实、未知的数据生成过程的模型。这通常被表述为选择一个在逼近真实分布时信息损失最小的模型，该信息损失由 Kullback–Leibler (KL) 散度来衡量。\n\n设数据的真实未知密度为 $f(x)$。设一个候选模型由密度 $g(x|\\theta)$ 表示，其中 $\\theta$ 是一个包含 $k$ 个参数的向量。从 $g$ 到 $f$ 的 KL 散度为：\n$$D_{\\mathrm{KL}}(f || g) = \\int f(x) \\ln\\left(\\frac{f(x)}{g(x|\\theta)}\\right) dx = \\mathbb{E}_{f}[\\ln(f(x))] - \\mathbb{E}_{f}[\\ln(g(x|\\theta))]$$\n由于 $\\mathbb{E}_{f}[\\ln(f(x))]$ 对所有候选模型都是常数，因此最小化 $D_{\\mathrm{KL}}(f || g)$ 等价于最大化模型的期望对数似然 $\\mathbb{E}_{f}[\\ln(g(x|\\theta))]$。参数向量 $\\theta$ 是未知的，通常通过最大化对数似然函数从数据中估计，从而得到最大似然估计 (MLE)，$\\hat{\\theta}$。我们希望估计的量是拟合模型 $g(x|\\hat{\\theta})$ 对从 $f(x)$ 中抽取的新数据的预测准确性。\n\n**赤池信息准则 (AIC)**\nAIC 是作为相对期望 KL 散度的渐近无偏估计量推导出来的。基于训练数据 $\\{x_i\\}_{i=1}^n$ 的最大化对数似然 $\\ell(\\hat{\\theta}) = \\sum_{i=1}^{n} \\ln(g(x_i|\\hat{\\theta}))$，是新数据上期望对数似然的一个有偏估计量。Akaike 证明，对于大样本，期望偏差约等于自由参数的数量 $k$。\n$$\\mathbb{E}[\\ell(\\hat{\\theta}) - \\mathbb{E}_{y}[\\ln g(y|\\hat{\\theta})]] \\approx k$$\n其中，外层期望是针对从 $f$ 中抽取的样本量为 $n$ 的数据集，内层期望是针对从 $f$ 中抽取的一个新数据点 $y$。因此，目标量 $\\mathbb{E}_{y}[\\ln g(y|\\hat{\\theta})]$ 的一个近似无偏估计量是 $\\ell(\\hat{\\theta}) - k$。\n由于信息准则通常被定义为需要最小化的量，Akaike 将此乘以 $-2$ 来定义他的准则：\n$$\\mathrm{AIC} = -2 (\\ell(\\hat{\\theta}) - k) = -2\\ell(\\hat{\\theta}) + 2k$$\n惩罚项 $2k$ 修正了样本内最大化对数似然的乐观偏差。\n\n**贝叶斯信息准则 (BIC)**\nBIC 是从贝叶斯角度推导出来的，旨在选择具有最高后验概率的模型。给定一组模型 $\\{M_j\\}$，我们希望找到使 $P(M_j | \\text{data})$ 最大化的那个模型。根据贝叶斯定理 (Bayes' theorem)，$P(M_j | \\text{data}) \\propto P(\\text{data} | M_j) P(M_j)$，其中 $P(\\text{data} | M_j)$ 是边际似然（或模型证据），$P(M_j)$ 是模型的先验概率。假设先验相等，选择最佳模型等价于最大化边际似然：\n$$P(\\text{data} | M_j) = \\int P(\\text{data} | \\theta_j, M_j) P(\\theta_j | M_j) d\\theta_j$$\n其中 $\\theta_j$ 是模型 $M_j$ 的参数向量，$P(\\text{data} | \\theta_j, M_j)$ 是似然 $L(\\theta_j)$，$P(\\theta_j | M_j)$ 是参数先验。这个积分通常是难以计算的。对于大样本量 $n$，我们可以使用 Laplace 近似来估计它。被积函数的对数 $\\ln(L(\\theta_j)P(\\theta_j))$ 在最大后验 (MAP) 估计 $\\tilde{\\theta}_j$ 周围进行泰勒展开。对于大的 $n$，似然函数主导先验，所以 $\\tilde{\\theta}_j \\approx \\hat{\\theta}_j$ (MLE)，并且我们可以近似 $\\ln(L(\\theta_j)P(\\theta_j)) \\approx \\ln(L(\\theta_j)) = \\ell(\\theta_j)$。\n对数边际似然的近似变为：\n$$\\ln P(\\text{data} | M_j) \\approx \\ell(\\hat{\\theta}_j) - \\frac{k_j}{2} \\ln(n)$$\n其中 $k_j$ 是模型 $M_j$ 中的参数数量。这个近似忽略了不随 $n$ 增长的项。与 AIC 类似，BIC 通常被定义为需要最小化的量，并乘以 $-2$：\n$$\\mathrm{BIC} = -2 \\ln P(\\text{data} | M_j) \\approx -2\\ell(\\hat{\\theta}_j) + k_j \\ln(n)$$\n对于任何 $n > e^2 \\approx 7.4$，惩罚项 $k_j \\ln(n)$ 对模型复杂度的惩罚比 AIC 更强。\n\n**2. AIC、BIC 及其差异的计算**\n\n给定数据如下：\n- 样本量：$n = 300$。\n- 泊松 (P)：$\\ell_{\\mathrm{P}} = -1185.7$, $k_{\\mathrm{P}} = 1$。\n- 负二项 (NB)：$\\ell_{\\mathrm{NB}} = -805.3$, $k_{\\mathrm{NB}} = 2$。\n- 零膨胀负二项 (ZINB)：$\\ell_{\\mathrm{ZINB}} = -799.9$, $k_{\\mathrm{ZINB}} = 3$。\n\n我们首先使用 $\\mathrm{AIC} = -2\\ell + 2k$ 计算 AIC 值：\n- $\\mathrm{AIC}_{\\mathrm{P}} = -2(-1185.7) + 2(1) = 2371.4 + 2 = 2373.4$\n- $\\mathrm{AIC}_{\\mathrm{NB}} = -2(-805.3) + 2(2) = 1610.6 + 4 = 1614.6$\n- $\\mathrm{AIC}_{\\mathrm{ZINB}} = -2(-799.9) + 2(3) = 1599.8 + 6 = 1605.8$\n\n最小 AIC 值为 $\\mathrm{AIC}_{\\min} = 1605.8$，对应 ZINB 模型。AIC 差异为 $\\Delta \\mathrm{AIC}_{i} = \\mathrm{AIC}_{i} - \\mathrm{AIC}_{\\min}$：\n- $\\Delta \\mathrm{AIC}_{\\mathrm{P}} = 2373.4 - 1605.8 = 767.6$\n- $\\Delta \\mathrm{AIC}_{\\mathrm{NB}} = 1614.6 - 1605.8 = 8.8$\n- $\\Delta \\mathrm{AIC}_{\\mathrm{ZINB}} = 1605.8 - 1605.8 = 0$\n\n接下来，我们使用 $\\mathrm{BIC} = -2\\ell + k \\ln(n)$ 计算 BIC 值。对于 $n=300$，$\\ln(300) \\approx 5.70378$。\n- $\\mathrm{BIC}_{\\mathrm{P}} = -2(-1185.7) + (1)\\ln(300) \\approx 2371.4 + 5.70378 = 2377.10378$\n- $\\mathrm{BIC}_{\\mathrm{NB}} = -2(-805.3) + (2)\\ln(300) \\approx 1610.6 + 2(5.70378) = 1610.6 + 11.40756 = 1622.00756$\n- $\\mathrm{BIC}_{\\mathrm{ZINB}} = -2(-799.9) + (3)\\ln(300) \\approx 1599.8 + 3(5.70378) = 1599.8 + 17.11134 = 1616.91134$\n\n最小 BIC 值为 $\\mathrm{BIC}_{\\min} = 1616.91134$，也对应 ZINB 模型。BIC 差异为 $\\Delta \\mathrm{BIC}_{i} = \\mathrm{BIC}_{i} - \\mathrm{BIC}_{\\min}$：\n- $\\Delta \\mathrm{BIC}_{\\mathrm{P}} \\approx 2377.10378 - 1616.91134 = 760.19244$\n- $\\Delta \\mathrm{BIC}_{\\mathrm{NB}} \\approx 1622.00756 - 1616.91134 = 5.09622$\n- $\\Delta \\mathrm{BIC}_{\\mathrm{ZINB}} \\approx 1616.91134 - 1616.91134 = 0$\n\n**3. 赤池权重的计算**\n\n模型 $i$ 的赤池权重 $w_i$ 量化了在给定数据和候选模型集的情况下，该模型是最佳近似模型的相对可能性。它由 $\\Delta \\mathrm{AIC}_i$ 值计算得出：\n$$w_i = \\frac{\\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_i)}{\\sum_{j=1}^{3} \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_j)}$$\n分母中的和为：\n$$S = \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_{\\mathrm{P}}) + \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_{\\mathrm{NB}}) + \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_{\\mathrm{ZINB}})$$\n$$S = \\exp(-\\frac{1}{2}(767.6)) + \\exp(-\\frac{1}{2}(8.8)) + \\exp(-\\frac{1}{2}(0))$$\n$$S = \\exp(-383.8) + \\exp(-4.4) + \\exp(0)$$\n项 $\\exp(-383.8)$ 小到可以忽略不计（实际上为零）。\n$$S \\approx 0 + 0.012277 + 1 = 1.012277$$\n现在我们计算各个权重：\n- $w_{\\mathrm{P}} = \\frac{\\exp(-383.8)}{S} \\approx 0$\n- $w_{\\mathrm{NB}} = \\frac{\\exp(-4.4)}{S} \\approx \\frac{0.012277}{1.012277} \\approx 0.012128$\n- $w_{\\mathrm{ZINB}} = \\frac{\\exp(0)}{S} = \\frac{1}{S} \\approx \\frac{1}{1.012277} \\approx 0.987872$\n\n**4. 最终答案**\n\n支持度最强的模型是 ZINB 模型，因为它具有最低的 AIC（和 BIC）值，因此具有最高的赤池权重。ZINB 模型的赤池权重约为 $0.987872$。将此值四舍五入到四位有效数字得到 $0.9879$。这表明，在给定数据和三个候选模型的情况下，ZINB 模型是这些选择中最佳近似模型的概率为 $98.79\\%$。",
            "answer": "$$\n\\boxed{0.9879}\n$$"
        },
        {
            "introduction": "仅仅选择一个“最佳”模型可能存在风险，因为它忽略了模型选择过程本身的不确定性。本练习将介绍模型平均，这是一种强大的技术，它利用 Akaike 权重来结合来自多个模型的预测。您将通过一个 MAPK 信号通路的例子，学习如何通过考虑多个竞争假设的相对支持度，来构建一个更稳健的预测，从而减少对单一模型的依赖 。",
            "id": "3326746",
            "problem": "一个丝裂原活化蛋白激酶 (MAPK) 级联模型被用来预测在表皮生长因子刺激后 $t = 10$ 分钟时磷酸化细胞外信号调节激酶 (ERK) 的比例。考虑了三个机理候选模型：$M_1$（质量作用，无反馈）、$M_2$（质量作用，有负反馈）和 $M_3$（支架介导的隔离）。每个模型都先前在不同的训练数据集上进行了校准；得到的赤池信息准则 (AIC) 分数相对于最佳模型产生了 $\\Delta \\mathrm{AIC}$ 值的差异：$M_1$ 的 $\\Delta_1 = 0$，$M_2$ 的 $\\Delta_2 = 2$，$M_3$ 的 $\\Delta_3 = 4$。对于当前条件（刺激物浓度固定且在各模型间匹配），在 $t = 10$ 分钟时磷酸化 ERK 比例的点预测值为：\n- $M_1$: $0.70$（任意单位），\n- $M_2$: $0.55$（任意单位），\n- $M_3$: $0.60$（任意单位）。\n\n使用基于赤池信息准则的有原则的信息论方法，计算这三个模型的赤池权重，然后通过将每个模型的预测值乘以其对应的权重来形成在 $t = 10$ 分钟时磷酸化 ERK 比例的模型平均预测。仅报告最终的模型平均磷酸化水平，以纯数字形式表示，四舍五入到四位有效数字。概念上的单位表示为任意单位 (a.u.)，但不要在最终的数值答案中包含单位。",
            "solution": "该问题提法明确，具有科学依据，并提供了计算唯一解所需的所有信息。这是使用赤池信息准则 (AIC) 进行模型平均的直接应用。\n\n三个候选模型表示为 $M_1$、$M_2$ 和 $M_3$。问题提供了相对于集合中最佳模型 ($\\mathrm{AIC}_{\\min}$) 的 AIC 分数差异，表示为 $\\Delta_i = \\mathrm{AIC}_i - \\mathrm{AIC}_{\\min}$。给定的值为：\n- 对于模型 $M_1$：$\\Delta_1 = 0$\n- 对于模型 $M_2$：$\\Delta_2 = 2$\n- 对于模型 $M_3$：$\\Delta_3 = 4$\n\n在时间 $t = 10$ 分钟时磷酸化 ERK 比例的点预测值为：\n- 对于模型 $M_1$：$y_1 = 0.70$\n- 对于模型 $M_2$：$y_2 = 0.55$\n- 对于模型 $M_3$：$y_3 = 0.60$\n\n第一步是计算每个模型 $i$ 的赤池权重 $w_i$。赤池权重表示在给定数据的情况下，模型 $i$ 是候选模型集合中最佳模型的概率。模型 $i$ 的赤池权重公式为：\n$$w_i = \\frac{\\exp(-\\frac{1}{2} \\Delta_i)}{\\sum_{j=1}^{R} \\exp(-\\frac{1}{2} \\Delta_j)}$$\n其中 $R$ 是候选模型的总数。在本例中，$R = 3$。\n\n我们首先为每个模型计算项 $\\exp(-\\frac{1}{2} \\Delta_i)$。该项与模型的相对似然度成正比。\n- 对于 $M_1$：$\\exp(-\\frac{1}{2} \\Delta_1) = \\exp(-\\frac{1}{2} \\cdot 0) = \\exp(0) = 1$。\n- 对于 $M_2$：$\\exp(-\\frac{1}{2} \\Delta_2) = \\exp(-\\frac{1}{2} \\cdot 2) = \\exp(-1)$。\n- 对于 $M_3$：$\\exp(-\\frac{1}{2} \\Delta_3) = \\exp(-\\frac{1}{2} \\cdot 4) = \\exp(-2)$。\n\n接下来，我们计算分母中的和，这是一个归一化因子，确保权重之和为 $1$：\n$$S = \\sum_{j=1}^{3} \\exp(-\\frac{1}{2} \\Delta_j) = \\exp(-\\frac{1}{2} \\Delta_1) + \\exp(-\\frac{1}{2} \\Delta_2) + \\exp(-\\frac{1}{2} \\Delta_3)$$\n$$S = 1 + \\exp(-1) + \\exp(-2)$$\n\n现在我们可以计算各个赤池权重：\n- $w_1 = \\frac{\\exp(-\\frac{1}{2} \\Delta_1)}{S} = \\frac{1}{1 + \\exp(-1) + \\exp(-2)}$\n- $w_2 = \\frac{\\exp(-\\frac{1}{2} \\Delta_2)}{S} = \\frac{\\exp(-1)}{1 + \\exp(-1) + \\exp(-2)}$\n- $w_3 = \\frac{\\exp(-\\frac{1}{2} \\Delta_3)}{S} = \\frac{\\exp(-2)}{1 + \\exp(-1) + \\exp(-2)}$\n\n最后一步是计算磷酸化 ERK 比例的模型平均预测值 $\\hat{y}_{\\text{avg}}$。这是每个模型预测值的加权平均，权重为赤池权重：\n$$\\hat{y}_{\\text{avg}} = \\sum_{i=1}^{3} w_i y_i = w_1 y_1 + w_2 y_2 + w_3 y_3$$\n\n代入权重的表达式和给定的预测值：\n$$\\hat{y}_{\\text{avg}} = \\left(\\frac{1}{S}\\right) y_1 + \\left(\\frac{\\exp(-1)}{S}\\right) y_2 + \\left(\\frac{\\exp(-2)}{S}\\right) y_3$$\n$$\\hat{y}_{\\text{avg}} = \\frac{y_1 + y_2 \\exp(-1) + y_3 \\exp(-2)}{1 + \\exp(-1) + \\exp(-2)}$$\n\n我们现在代入预测的数值 $y_1=0.70$，$y_2=0.55$ 和 $y_3=0.60$：\n$$\\hat{y}_{\\text{avg}} = \\frac{0.70 \\cdot 1 + 0.55 \\cdot \\exp(-1) + 0.60 \\cdot \\exp(-2)}{1 + \\exp(-1) + \\exp(-2)}$$\n\n为了获得最终的数值答案，我们使用指数函数的近似值：\n$\\exp(-1) \\approx 0.367879$\n$\\exp(-2) \\approx 0.135335$\n\n分母为：\n$S \\approx 1 + 0.367879 + 0.135335 = 1.503214$\n\n分子为：\n$N \\approx 0.70 + 0.55 \\cdot (0.367879) + 0.60 \\cdot (0.135335)$\n$N \\approx 0.70 + 0.20233345 + 0.081201$\n$N \\approx 0.98353445$\n\n模型平均预测值为：\n$$\\hat{y}_{\\text{avg}} = \\frac{N}{S} \\approx \\frac{0.98353445}{1.503214} \\approx 0.654289$$\n\n问题要求答案四舍五入到四位有效数字。第五位有效数字是 $8$，所以我们将第四位数字向上取整。\n$$\\hat{y}_{\\text{avg}} \\approx 0.6543$$\n这是在 $t=10$ 分钟时磷酸化 ERK 比例的模型平均预测值，以任意单位表示。",
            "answer": "$$\\boxed{0.6543}$$"
        },
        {
            "introduction": "系统生物学中的一个关键挑战是：如何用有限的数据（小样本量，$n$）来拟合复杂的模型（多参数，$p$）。本练习旨在探讨在这种挑战性情况下，贝叶斯信息准则（BIC）和小样本校正的 Akaike 信息准则（$AIC_c$）的不同行为。通过编程实现并比较这两个准则，您将深入理解它们不同的惩罚项如何导致不同的模型选择，这对于从稀疏数据中得出有效的科学结论至关重要 。",
            "id": "3326821",
            "problem": "考虑在计算系统生物学中，将两个候选非线性常微分方程（ODE）模型拟合到稀疏的单细胞时间序列数据。设观测数据为来自单个细胞的 $\\{(t_i, y_i)\\}_{i=1}^n$，其中 $t_i$ 是时间点，$y_i$ 是测得的分子丰度。每个模型 $M_j$ 带有参数矢量 $\\theta_j \\in \\mathbb{R}^{p_j}$，它会产生一个 ODE 系统的数值解 $x_{\\theta_j}(t)$ 以及在测量时间的相应预测值 $x_{\\theta_j}(t_i)$。假设残差是独立同分布的高斯分布，均值为零，方差未知，即 $y_i = x_{\\theta_j}(t_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。定义模型 $M_j$ 的残差平方和为 $\\mathrm{SSE}_j = \\sum_{i=1}^n (y_i - x_{\\theta_j}(t_i))^2$。\n\n从高斯噪声模型下的最大似然原理和信息论模型选择出发，推导贝叶斯信息准则（BIC）和小样本校正的赤池信息准则（$\\mathrm{AIC}_c$）的简化表达式（在竞争模型之间，可忽略相同的加法常数），这些表达式应为 $n$、$p_j$ 和 $\\mathrm{SSE}_j$ 的函数。使用这些表达式实现一个程序，对每个测试用例确定：\n- BIC 在 $M_1$ 和 $M_2$ 之间选择了哪个模型，\n- $\\mathrm{AIC}_c$ 在 $M_1$ 和 $M_2$ 之间选择了哪个模型（如果任一候选模型的 $n \\le p_j + 1$，则声明 $\\mathrm{AIC}_c$ 未定义），\n- $\\mathrm{AIC}_c$ 是否因小样本惩罚而逆转了 BIC 的选择，这被定义为一个布尔值：如果 BIC 和 $\\mathrm{AIC}_c$ 选择了不同的模型且 $\\mathrm{AIC}_c$ 有定义，则为 $true$，否则为 $false$。\n\n使用的基本原理：\n- 针对具有未知方差的独立高斯残差的最大似然估计。\n- 基于预期 Kullback–Leibler 散度的赤池信息准则（AIC）定义，以及基于边际似然的 Laplace 近似的贝叶斯信息准则（BIC）定义。\n- 导致 $\\mathrm{AIC}_c$ 的小样本校正。\n\n您的程序应实现推导出的公式，并将其应用于以下参数值的测试套件。每个测试用例是一个五元组 $(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2)$：\n\n- 案例 1：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,60,\\,5,\\,9,\\,120.0,\\,95.0\\,)$。\n- 案例 2：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,12,\\,3,\\,9,\\,120.0,\\,32.0\\,)$。\n- 案例 3：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,20,\\,7,\\,8,\\,110.0,\\,90.0\\,)$。\n- 案例 4：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,100,\\,6,\\,9,\\,200.0,\\,150.0\\,)$。\n- 案例 5：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,10,\\,3,\\,9,\\,80.0,\\,60.0\\,)$。\n- 案例 6：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,15,\\,5,\\,10,\\,100.0,\\,85.0\\,)$。\n\n覆盖性设计：\n- 测试套件包括一个 $n$ 适度超过 $p_j$ 的一般情况、 $p_j$ 接近 $n$ 的边界情况，以及一个因 $n \\le p_j + 1$ 而导致 $\\mathrm{AIC}_c$ 未定义的情况。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含六个案例的逆转布尔值，格式为逗号分隔的列表并用方括号括起，例如 $\\texttt{[true,false,...]}$。在输出中使用 Python 布尔字面量 $\\texttt{True}$ 和 $\\texttt{False}$。\n\n不应读取任何外部输入；所有计算必须使用提供的测试套件值进行。",
            "solution": "该问题要求为贝叶斯信息准则（BIC）和小样本校正的赤池信息准则（$\\mathrm{AIC}_c$）推导简化表达式，以便在两个拟合于时间序列数据的非线性常微分方程（ODE）模型之间进行选择。推导从独立同分布高斯噪声假设下的最大似然原理开始。\n\n设观测数据为 $\\{ (t_i, y_i) \\}_{i=1}^n$。对于一个给定模型 $M_j$，其参数矢量 $\\theta_j$ 的维度为 $p_j$，在时间 $t_i$ 的预测值为 $x_{\\theta_j}(t_i)$。假设残差服从正态分布，$\\varepsilon_i = y_i - x_{\\theta_j}(t_i) \\sim \\mathcal{N}(0, \\sigma^2)$，其方差 $\\sigma^2$ 未知。\n\n首先，我们构建似然函数。单个观测值 $y_i$ 的概率密度为：\n$$\nP(y_i | \\theta_j, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_{\\theta_j}(t_i))^2}{2\\sigma^2}\\right)\n$$\n假设独立性，整个数据集 $\\mathbf{y} = \\{y_1, \\dots, y_n\\}$ 的似然函数是各个概率的乘积：\n$$\nL(\\theta_j, \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n P(y_i | \\theta_j, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_{\\theta_j}(t_i))^2\\right)\n$$\n使用残差平方和的定义 $\\mathrm{SSE}_j = \\sum_{i=1}^n (y_i - x_{\\theta_j}(t_i))^2$，似然函数变为：\n$$\nL(\\theta_j, \\sigma^2 | \\mathbf{y}) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{\\mathrm{SSE}_j}{2\\sigma^2}\\right)\n$$\n对数似然函数 $\\ln L$ 更便于最大化：\n$$\n\\ln L(\\theta_j, \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{\\mathrm{SSE}_j}{2\\sigma^2}\n$$\n为了找到最大似然估计（MLEs），我们对此函数相对于参数进行最大化。问题提供了 $\\mathrm{SSE}_j$，它是在模型参数的 MLE $\\hat{\\theta}_j$ 处计算的。我们还必须找到方差的 MLE $\\hat{\\sigma}^2$。对 $\\sigma^2$ 求偏导数并令其为零，可得：\n$$\n\\frac{\\partial (\\ln L)}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{\\mathrm{SSE}_j}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}_j^2 = \\frac{\\mathrm{SSE}_j}{n}\n$$\n将 MLEs $\\hat{\\theta}_j$（它给出了 $\\mathrm{SSE}_j$）和 $\\hat{\\sigma}_j^2$ 代回到对数似然函数中，得到最大化对数似然 $\\mathcal{L}_j$：\n$$\n\\mathcal{L}_j = \\ln L(\\hat{\\theta}_j, \\hat{\\sigma}_j^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) - \\frac{\\mathrm{SSE}_j}{2(\\mathrm{SSE}_j/n)} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) + 1 \\right)\n$$\n这个最大化对数似然 $\\mathcal{L}_j$ 是两种信息准则的基本量。模型 $M_j$ 的估计参数总数为 $k_j = p_j + 1$，其中包括 $\\theta_j$ 中的 $p_j$ 个参数和方差 $\\sigma^2$。\n\n贝叶斯信息准则（BIC）定义为：\n$$\n\\mathrm{BIC}_j = -2\\mathcal{L}_j + k_j \\ln(n)\n$$\n代入 $\\mathcal{L}_j$ 和 $k_j$ 的表达式：\n$$\n\\mathrm{BIC}_j = n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) + 1 \\right) + (p_j+1)\\ln(n)\n$$\n$$\n\\mathrm{BIC}_j = n\\ln(2\\pi) + n\\ln(\\mathrm{SSE}_j) - n\\ln(n) + n + (p_j+1)\\ln(n)\n$$\n在比较同一数据集（固定的 $n$）上的模型 $M_1$ 和 $M_2$ 时，可以舍去在各模型间保持不变的项。这些项是 $n\\ln(2\\pi)$、$-n\\ln(n)$ 和 $n$。这样就留下了一个与真实 BIC 成正比的简化表达式，足以用于模型选择：\n$$\n\\mathrm{BIC}_j \\propto n\\ln(\\mathrm{SSE}_j) + (p_j+1)\\ln(n)\n$$\nBIC 值较低的模型更优。\n\n小样本校正的赤池信息准则（$\\mathrm{AIC}_c$）定义为：\n$$\n\\mathrm{AIC}_{c,j} = \\mathrm{AIC}_j + \\frac{2k_j(k_j+1)}{n-k_j-1}\n$$\n其中 $\\mathrm{AIC}_j = -2\\mathcal{L}_j + 2k_j$。代入 $\\mathcal{L}_j$ 和 $k_j$：\n$$\n\\mathrm{AIC}_{c,j} = n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) + 1 \\right) + 2(p_j+1) + \\frac{2(p_j+1)(p_j+2)}{n-(p_j+1)-1}\n$$\n同样，我们通过舍去在各模型间保持不变的项（$n\\ln(2\\pi)$、$-n\\ln(n)$ 和 $n$）来简化比较：\n$$\n\\mathrm{AIC}_{c,j} \\propto n\\ln(\\mathrm{SSE}_j) + 2(p_j+1) + \\frac{2(p_j+1)(p_j+2)}{n-p_j-2}\n$$\n$\\mathrm{AIC}_c$ 值较低的模型更优。根据规定，如果任一待考量模型的 $n \\le p_j+1$，则 $\\mathrm{AIC}_c$ 被视为未定义。如果 BIC 和 $\\mathrm{AIC}_c$ 选择了不同的模型，并且 $\\mathrm{AIC}_c$ 在比较中有定义，则发生逆转。\n\n为实现该解法，将对每个测试用例 $(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2)$ 使用以下公式：\n1. 使用 $\\mathrm{BIC}_j \\propto n\\ln(\\mathrm{SSE}_j) + (p_j+1)\\ln(n)$ 计算 $\\mathrm{BIC}_1$ 和 $\\mathrm{BIC}_2$，并确定具有最小值的模型。\n2. 检查 $\\mathrm{AIC}_c$ 是否对该比较有定义，即是否 $n > p_1+1$ 且 $n > p_2+1$。\n3. 如果有定义，则使用 $\\mathrm{AIC}_{c,j} \\propto n\\ln(\\mathrm{SSE}_j) + 2(p_j+1) + \\frac{2(p_j+1)(p_j+2)}{n-p_j-2}$ 计算 $\\mathrm{AIC}_{c,1}$ 和 $\\mathrm{AIC}_{c,2}$，并确定具有最小值的模型。\n4. 比较选择结果。如果 $\\mathrm{AIC}_c$ 有定义且选择不同，则发生了逆转。否则，没有发生逆转。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates model selection outcomes for a series of test cases using BIC and AICc,\n    and determines if AICc reverses the selection made by BIC.\n    \"\"\"\n    # Test cases as 5-tuples: (n, p1, p2, SSE1, SSE2)\n    # n: number of data points\n    # p1, p2: number of parameters for model 1 and model 2\n    # SSE1, SSE2: residual sum of squares for model 1 and model 2\n    test_cases = [\n        (60, 5, 9, 120.0, 95.0),\n        (12, 3, 9, 120.0, 32.0),\n        (20, 7, 8, 110.0, 90.0),\n        (100, 6, 9, 200.0, 150.0),\n        (10, 3, 9, 80.0, 60.0),\n        (15, 5, 10, 100.0, 85.0),\n    ]\n\n    reversal_results = []\n\n    for case in test_cases:\n        n, p1, p2, sse1, sse2 = case\n\n        # The total number of estimated parameters for model j, k_j, is p_j + 1\n        # (p_j model parameters + 1 variance parameter sigma^2).\n        k1 = p1 + 1\n        k2 = p2 + 1\n\n        # --- BIC Selection ---\n        # The simplified BIC formula for model comparison is:\n        # BIC_j = n * ln(SSE_j) + k_j * ln(n)\n        bic1 = n * np.log(sse1) + k1 * np.log(n)\n        bic2 = n * np.log(sse2) + k2 * np.log(n)\n        bic_selection = 1 if bic1  bic2 else 2\n\n        # --- AICc Selection ---\n        # The problem states AICc is undefined if n = p_j + 1 for either model.\n        # This also implies n  k_j for both models.\n        # The denominator of the correction term is n - k_j - 1.\n        # The condition n  p_j + 1 means n = p_j + 2, or n = k_j + 1.\n        # This ensures the denominator n - k_j - 1 = 0.\n        # The standard definition requires n - k_j - 1  0, which is n  p_j + 2.\n        # Case 5 has n=10, p2=9, so n = p2+1 and AICc is undefined as per problem.\n        aicc_is_defined = (n  p1 + 1) and (n  p2 + 1)\n\n        aicc_selection = None\n        if aicc_is_defined:\n            # The simplified AICc formula for model comparison is:\n            # AICc_j = n * ln(SSE_j) + 2*k_j + (2*k_j*(k_j+1)) / (n-k_j-1)\n            \n            # Check for non-positive denominator to avoid division by zero or negative.\n            # This is a safeguard, though the problem condition n  p_j + 1\n            # handles the provided test cases.\n            if (n - k1 - 1) = 0 or (n - k2 - 1) = 0:\n                 # This branch is not expected to be hit with problem's test data\n                 # and the n = p_j + 1 rule, but is good practice.\n                 aicc_is_defined = False\n            else:\n                aicc1_penalty = (2 * k1 * (k1 + 1)) / (n - k1 - 1)\n                aicc1 = n * np.log(sse1) + 2 * k1 + aicc1_penalty\n\n                aicc2_penalty = (2 * k2 * (k2 + 1)) / (n - k2 - 1)\n                aicc2 = n * np.log(sse2) + 2 * k2 + aicc2_penalty\n                \n                aicc_selection = 1 if aicc1  aicc2 else 2\n\n        # --- Reversal Check ---\n        # Reversal is true if AICc is defined and selects a different model than BIC.\n        is_reversal = False\n        if aicc_is_defined:\n            if bic_selection != aicc_selection:\n                is_reversal = True\n        \n        reversal_results.append(is_reversal)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, reversal_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}