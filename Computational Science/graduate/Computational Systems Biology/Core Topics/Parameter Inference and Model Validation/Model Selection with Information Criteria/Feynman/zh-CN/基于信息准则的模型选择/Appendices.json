{
    "hands_on_practices": [
        {
            "introduction": "本练习是信息标准应用的基础。我们将从模型拟合的原始输出——最大化对数似然和参数数量——出发，执行模型选择的核心计算。通过一个常见的单细胞转录组学情景 ，本练习将巩固您对如何计算赤池信息准则（Akaike Information Criterion, $AIC$）和贝叶斯信息准则（Bayesian Information Criterion, $BIC$）、使用 $\\Delta$ 值比较模型，以及利用赤池权重（Akaike weights）来量化各模型证据强度的理解。",
            "id": "3326796",
            "problem": "一项单细胞转录组学实验测量了单个基因在 $n$ 个细胞中的分子计数。您的任务是为此基因从三个候选计数模型中进行选择，每个模型都通过最大似然法拟合为广义线性模型 (GLM)：泊松 GLM、负二项 GLM 和零膨胀负二项 GLM。设样本量为 $n = 300$。对于这些候选模型，最大化对数似然和自由参数数量如下（所有拟合都基于相同的数据，且所有模型都使用带有基因特异性截距的对数连接函数；负二项 GLM 包含一个离散参数，而零膨胀负二项 GLM 还额外包含一个零膨胀参数）：\n- 泊松 GLM：最大化对数似然 $\\ell_{\\mathrm{P}} = -1185.7$，参数数量 $k_{\\mathrm{P}} = 1$。\n- 负二项 GLM：最大化对数似然 $\\ell_{\\mathrm{NB}} = -805.3$，参数数量 $k_{\\mathrm{NB}} = 2$。\n- 零膨胀负二项 GLM：最大化对数似然 $\\ell_{\\mathrm{ZINB}} = -799.9$，参数数量 $k_{\\mathrm{ZINB}} = 3$。\n\n从未知真实数据生成密度与候选模型之间的 Kullback–Leibler 散度的定义出发，并利用最大似然估计量的大样本行为以及对边际似然的拉普拉斯近似，按以下步骤进行：\n1. 推导适用于在同一大小为 $n$ 的数据集上通过最大似然法拟合的模型的赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的比较形式，并从第一性原理证明每个惩罚项的合理性。\n2. 使用您推导出的表达式，计算每个模型的 AIC 和 BIC 值，然后通过减去每个准则在所有模型中的最小值来计算 AIC 差值 $\\Delta \\mathrm{AIC}_{i}$ 和 BIC 差值 $\\Delta \\mathrm{BIC}_{i}$。\n3. 使用 $\\Delta \\mathrm{AIC}_{i}$ 值，计算每个模型的 Akaike 权重，以量化它们的相对支持度。\n4. 最后，报告支持度最强模型的 Akaike 权重。将您最终报告的数值四舍五入到四位有效数字。不要包含任何单位。",
            "solution": "该问题陈述具有科学依据、内容自洽且提法明确，可以进行严谨的求解。它涉及使用既定的信息准则为计数数据选择统计模型，这是计算系统生物学中的一项标准任务。所有必要的数据均已提供。\n\n**1. AIC 和 BIC 的推导**\n\n模型选择的目标是选择一个能最好地近似真实、未知的数据生成过程的模型。这通常被表述为选择一个在近似真实分布时信息损失最小的模型，该信息损失由 Kullback–Leibler (KL) 散度来衡量。\n\n设数据的真实未知密度为 $f(x)$。设一个候选模型由密度 $g(x|\\theta)$ 表示，其中 $\\theta$ 是一个包含 $k$ 个参数的向量。从 $g$ 到 $f$ 的 KL 散度为：\n$$D_{\\mathrm{KL}}(f || g) = \\int f(x) \\ln\\left(\\frac{f(x)}{g(x|\\theta)}\\right) dx = \\mathbb{E}_{f}[\\ln(f(x))] - \\mathbb{E}_{f}[\\ln(g(x|\\theta))]$$\n由于 $\\mathbb{E}_{f}[\\ln(f(x))]$ 对于所有候选模型都是一个常数，因此最小化 $D_{\\mathrm{KL}}(f || g)$ 等价于最大化模型的期望对数似然 $\\mathbb{E}_{f}[\\ln(g(x|\\theta))]$。参数向量 $\\theta$ 是未知的，通常通过最大化对数似然函数从数据中估计，从而得到最大似然估计 (MLE) $\\hat{\\theta}$。我们希望估计的量是拟合模型 $g(x|\\hat{\\theta})$ 对从 $f(x)$ 中抽取的新数据的预测准确性。\n\n**赤池信息准则 (AIC)**\nAIC 是作为相对期望 KL 散度的渐进无偏估计量推导出来的。基于训练数据 $\\{x_i\\}_{i=1}^n$ 的最大化对数似然 $\\ell(\\hat{\\theta}) = \\sum_{i=1}^{n} \\ln(g(x_i|\\hat{\\theta}))$ 是对新数据上期望对数似然的有偏估计。Akaike 证明，对于大样本，期望偏差约等于自由参数的数量 $k$。\n$$\\mathbb{E}[\\ell(\\hat{\\theta}) - \\mathbb{E}_{y}[\\ln g(y|\\hat{\\theta})]] \\approx k$$\n其中外层期望是针对从 $f$ 抽取的 $n$ 个大小的数据集，而内层期望是针对从 $f$ 抽取的一个新数据点 $y$。因此，目标量 $\\mathbb{E}_{y}[\\ln g(y|\\hat{\\theta})]$ 的一个近似无偏估计量是 $\\ell(\\hat{\\theta}) - k$。\n由于信息准则通常被定义为需要最小化的量，Akaike 将其乘以 $-2$ 来定义他的准则：\n$$\\mathrm{AIC} = -2 (\\ell(\\hat{\\theta}) - k) = -2\\ell(\\hat{\\theta}) + 2k$$\n惩罚项 $2k$ 修正了样本内最大化对数似然的乐观偏差。\n\n**贝叶斯信息准则 (BIC)**\nBIC 是从贝叶斯角度推导出来的，旨在选择具有最高后验概率的模型。给定一组模型 $\\{M_j\\}$，我们希望找到最大化 $P(M_j | \\text{data})$ 的模型。根据贝叶斯定理，$P(M_j | \\text{data}) \\propto P(\\text{data} | M_j) P(M_j)$，其中 $P(\\text{data} | M_j)$ 是边际似然（或模型证据），$P(M_j)$ 是模型的先验概率。假设先验相等，选择最佳模型等价于最大化边际似然：\n$$P(\\text{data} | M_j) = \\int P(\\text{data} | \\theta_j, M_j) P(\\theta_j | M_j) d\\theta_j$$\n其中 $\\theta_j$ 是模型 $M_j$ 的参数向量，$P(\\text{data} | \\theta_j, M_j)$ 是似然 $L(\\theta_j)$，而 $P(\\theta_j | M_j)$ 是参数先验。这个积分通常是难以处理的。对于大样本量 $n$，我们可以使用拉普拉斯近似来对其进行估计。被积函数的对数 $\\ln(L(\\theta_j)P(\\theta_j))$ 在最大后验 (MAP) 估计 $\\tilde{\\theta}_j$ 附近进行泰勒展开。对于大的 $n$，似然函数主导先验，因此 $\\tilde{\\theta}_j \\approx \\hat{\\theta}_j$（最大似然估计 MLE），并且我们可以近似 $\\ln(L(\\theta_j)P(\\theta_j)) \\approx \\ln(L(\\theta_j)) = \\ell(\\theta_j)$。\n对数边际似然的近似变为：\n$$\\ln P(\\text{data} | M_j) \\approx \\ell(\\hat{\\theta}_j) - \\frac{k_j}{2} \\ln(n)$$\n其中 $k_j$ 是模型 $M_j$ 中的参数数量。这个近似舍弃了不随 $n$ 增加的项。与 AIC 类似，BIC 通常被定义为需要最小化的量，并按 $-2$ 进行缩放：\n$$\\mathrm{BIC} = -2 \\ln P(\\text{data} | M_j) \\approx -2\\ell(\\hat{\\theta}_j) + k_j \\ln(n)$$\n对于任何 $n > e^2 \\approx 7.4$ 的情况，惩罚项 $k_j \\ln(n)$ 对模型复杂度的惩罚比 AIC 更强。\n\n**2. AIC、BIC 及其差值的计算**\n\n给定数据如下：\n- 样本量：$n = 300$。\n- 泊松 (P)：$\\ell_{\\mathrm{P}} = -1185.7$, $k_{\\mathrm{P}} = 1$。\n- 负二项 (NB)：$\\ell_{\\mathrm{NB}} = -805.3$, $k_{\\mathrm{NB}} = 2$。\n- 零膨胀负二项 (ZINB)：$\\ell_{\\mathrm{ZINB}} = -799.9$, $k_{\\mathrm{ZINB}} = 3$。\n\n我们首先使用 $\\mathrm{AIC} = -2\\ell + 2k$ 计算 AIC 值：\n- $\\mathrm{AIC}_{\\mathrm{P}} = -2(-1185.7) + 2(1) = 2371.4 + 2 = 2373.4$\n- $\\mathrm{AIC}_{\\mathrm{NB}} = -2(-805.3) + 2(2) = 1610.6 + 4 = 1614.6$\n- $\\mathrm{AIC}_{\\mathrm{ZINB}} = -2(-799.9) + 2(3) = 1599.8 + 6 = 1605.8$\n\n最小 AIC 值为 $\\mathrm{AIC}_{\\min} = 1605.8$，对应 ZINB 模型。AIC 差值为 $\\Delta \\mathrm{AIC}_{i} = \\mathrm{AIC}_{i} - \\mathrm{AIC}_{\\min}$：\n- $\\Delta \\mathrm{AIC}_{\\mathrm{P}} = 2373.4 - 1605.8 = 767.6$\n- $\\Delta \\mathrm{AIC}_{\\mathrm{NB}} = 1614.6 - 1605.8 = 8.8$\n- $\\Delta \\mathrm{AIC}_{\\mathrm{ZINB}} = 1605.8 - 1605.8 = 0$\n\n接下来，我们使用 $\\mathrm{BIC} = -2\\ell + k \\ln(n)$ 计算 BIC 值。对于 $n=300$，$\\ln(300) \\approx 5.70378$。\n- $\\mathrm{BIC}_{\\mathrm{P}} = -2(-1185.7) + (1)\\ln(300) \\approx 2371.4 + 5.70378 = 2377.10378$\n- $\\mathrm{BIC}_{\\mathrm{NB}} = -2(-805.3) + (2)\\ln(300) \\approx 1610.6 + 2(5.70378) = 1610.6 + 11.40756 = 1622.00756$\n- $\\mathrm{BIC}_{\\mathrm{ZINB}} = -2(-799.9) + (3)\\ln(300) \\approx 1599.8 + 3(5.70378) = 1599.8 + 17.11134 = 1616.91134$\n\n最小 BIC 值为 $\\mathrm{BIC}_{\\min} = 1616.91134$，也对应 ZINB 模型。BIC 差值为 $\\Delta \\mathrm{BIC}_{i} = \\mathrm{BIC}_{i} - \\mathrm{BIC}_{\\min}$：\n- $\\Delta \\mathrm{BIC}_{\\mathrm{P}} \\approx 2377.10378 - 1616.91134 = 760.19244$\n- $\\Delta \\mathrm{BIC}_{\\mathrm{NB}} \\approx 1622.00756 - 1616.91134 = 5.09622$\n- $\\Delta \\mathrm{BIC}_{\\mathrm{ZINB}} \\approx 1616.91134 - 1616.91134 = 0$\n\n**3. Akaike 权重的计算**\n\n模型 $i$ 的 Akaike 权重 $w_i$ 量化了在给定数据和候选模型集的情况下，该模型是最佳近似模型的相对可能性。它根据 $\\Delta \\mathrm{AIC}_i$ 值计算得出：\n$$w_i = \\frac{\\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_i)}{\\sum_{j=1}^{3} \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_j)}$$\n分母中的和为：\n$$S = \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_{\\mathrm{P}}) + \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_{\\mathrm{NB}}) + \\exp(-\\frac{1}{2}\\Delta \\mathrm{AIC}_{\\mathrm{ZINB}})$$\n$$S = \\exp(-\\frac{1}{2}(767.6)) + \\exp(-\\frac{1}{2}(8.8)) + \\exp(-\\frac{1}{2}(0))$$\n$$S = \\exp(-383.8) + \\exp(-4.4) + \\exp(0)$$\n项 $\\exp(-383.8)$ 小到可以忽略不计（实际上为零）。\n$$S \\approx 0 + 0.012277 + 1 = 1.012277$$\n现在我们计算各个权重：\n- $w_{\\mathrm{P}} = \\frac{\\exp(-383.8)}{S} \\approx 0$\n- $w_{\\mathrm{NB}} = \\frac{\\exp(-4.4)}{S} \\approx \\frac{0.012277}{1.012277} \\approx 0.012128$\n- $w_{\\mathrm{ZINB}} = \\frac{\\exp(0)}{S} = \\frac{1}{S} \\approx \\frac{1}{1.012277} \\approx 0.987872$\n\n**4. 最终答案**\n\n支持度最强的模型是 ZINB 模型，因为它具有最低的 AIC（和 BIC）值，因此具有最高的 Akaike 权重。ZINB 模型的 Akaike 权重约为 $0.987872$。将此值四舍五入到四位有效数字得到 $0.9879$。这表明，在给定数据和三个候选模型的情况下，ZINB 模型是这些选择中最佳近似模型的概率为 $98.79\\%$。",
            "answer": "$$\n\\boxed{0.9879}\n$$"
        },
        {
            "introduction": "在基础计算之上，本练习处理系统生物学中的一个关键挑战：当数据稀疏时（即数据点数量 $n$ 不远大于模型参数数量 $p$）的模型选择。我们将探讨标准 $AIC$ 在这种情况下为何可能产生误导，并引入其小样本校正形式 $AIC_c$。通过一个编码练习 ，您将研究 $AIC_c$ 和 $BIC$ 的不同行为，发现它们导致模型选择冲突的场景，并对其各自的优势建立直观认识。",
            "id": "3326821",
            "problem": "考虑在计算系统生物学中，将两个候选的非线性常微分方程（ODE）模型拟合到稀疏的单细胞时间序列数据。设观测数据为来自单个细胞的 $\\{(t_i, y_i)\\}_{i=1}^n$，其中 $t_i$ 是时间点，$y_i$ 是测得的分子丰度。每个模型 $M_j$ 的参数向量为 $\\theta_j \\in \\mathbb{R}^{p_j}$，它会产生一个 ODE 系统的数值解 $x_{\\theta_j}(t)$以及在测量时间的相应预测值 $x_{\\theta_j}(t_i)$。假设残差是独立同分布的高斯分布，均值为零，方差未知，即 $y_i = x_{\\theta_j}(t_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。为模型 $M_j$ 定义残差平方和 $\\mathrm{SSE}_j = \\sum_{i=1}^n (y_i - x_{\\theta_j}(t_i))^2$。\n\n从高斯噪声模型下的最大似然原理和信息论模型选择出发，推导贝叶斯信息准则（BIC）和小样本校正的赤池信息准则（$\\mathrm{AIC}_c$）的简化表达式（忽略在竞争模型中相同的加性常数），使其成为 $n$、$p_j$ 和 $\\mathrm{SSE}_j$ 的函数。使用这些表达式实现一个程序，对于每个测试用例，确定：\n- BIC 在 $M_1$ 和 $M_2$ 之间选择哪个模型，\n- $\\mathrm{AIC}_c$ 在 $M_1$ 和 $M_2$ 之间选择哪个模型（如果任一候选模型的 $n \\le p_j + 1$，则声明 $\\mathrm{AIC}_c$ 未定义），\n- $\\mathrm{AIC}_c$ 是否因小样本惩罚而逆转了 BIC 的选择，这被定义为一个布尔值：如果 BIC 和 $\\mathrm{AIC}_c$ 选择不同的模型且 $\\mathrm{AIC}_c$ 有定义，则为 $true$，否则为 $false$。\n\n使用的基本原理：\n- 对于未知方差的独立高斯残差的最大似然估计。\n- 基于预期 Kullback–Leibler 散度的赤池信息准则（AIC）定义，以及基于边际似然的拉普拉斯近似的贝叶斯信息准则（BIC）定义。\n- 导致 $\\mathrm{AIC}_c$ 的小样本校正。\n\n你的程序应实现推导出的公式，并将其应用于以下参数值的测试套件。每个测试用例是一个 5 元组 $(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2)$：\n\n- 案例 1：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,60,\\,5,\\,9,\\,120.0,\\,95.0\\,)$。\n- 案例 2：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,12,\\,3,\\,9,\\,120.0,\\,32.0\\,)$。\n- 案例 3：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,20,\\,7,\\,8,\\,110.0,\\,90.0\\,)$。\n- 案例 4：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,100,\\,6,\\,9,\\,200.0,\\,150.0\\,)$。\n- 案例 5：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,10,\\,3,\\,9,\\,80.0,\\,60.0\\,)$。\n- 案例 6：$(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2) = (\\,15,\\,5,\\,10,\\,100.0,\\,85.0\\,)$。\n\n覆盖性设计：\n- 该测试套件包括一个 $n$ 适度超过 $p_j$ 的一般情况， $p_j$ 接近 $n$ 的边界情况，以及一个由于 $n \\le p_j + 1$ 而导致 $\\mathrm{AIC}_c$ 未定义的情况。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含六个案例的逆转布尔值，形式为用方括号括起来的逗号分隔列表，例如 $\\texttt{[true,false,...]}$。在输出中使用 Python 布尔字面量 $\\texttt{True}$ 和 $\\texttt{False}$。\n\n不应读取任何外部输入；所有计算必须使用提供的测试套件值进行。",
            "solution": "该问题要求推导贝叶斯信息准则（BIC）和小样本校正的赤池信息准则（$\\mathrm{AIC}_c$）的简化表达式，以便在两个拟合于时间序列数据的非线性常微分方程（ODE）模型之间进行选择。推导从独立同分布高斯噪声假设下的最大似然原理开始。\n\n设观测数据为 $\\{ (t_i, y_i) \\}_{i=1}^n$。对于一个给定模型 $M_j$，其参数向量 $\\theta_j$ 的维度为 $p_j$，在时间 $t_i$ 的预测值为 $x_{\\theta_j}(t_i)$。假设残差服从正态分布，$\\varepsilon_i = y_i - x_{\\theta_j}(t_i) \\sim \\mathcal{N}(0, \\sigma^2)$，其方差 $\\sigma^2$ 未知。\n\n首先，我们构建似然函数。单个观测值 $y_i$ 的概率密度为：\n$$\nP(y_i | \\theta_j, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_{\\theta_j}(t_i))^2}{2\\sigma^2}\\right)\n$$\n假设独立性，整个数据集 $\\mathbf{y} = \\{y_1, \\dots, y_n\\}$ 的似然是各个概率的乘积：\n$$\nL(\\theta_j, \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n P(y_i | \\theta_j, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_{\\theta_j}(t_i))^2\\right)\n$$\n使用残差平方和的定义，$\\mathrm{SSE}_j = \\sum_{i=1}^n (y_i - x_{\\theta_j}(t_i))^2$，似然函数变为：\n$$\nL(\\theta_j, \\sigma^2 | \\mathbf{y}) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{\\mathrm{SSE}_j}{2\\sigma^2}\\right)\n$$\n对数似然函数 $\\ln L$ 更便于最大化：\n$$\n\\ln L(\\theta_j, \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{\\mathrm{SSE}_j}{2\\sigma^2}\n$$\n为了找到最大似然估计（MLEs），我们对此函数关于参数进行最大化。问题中提供的 $\\mathrm{SSE}_j$ 是在模型参数的 MLE $\\hat{\\theta}_j$ 处计算的。我们还必须找到方差的 MLE $\\hat{\\sigma}^2$。对 $\\sigma^2$ 求偏导数并令其为零，可得：\n$$\n\\frac{\\partial (\\ln L)}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{\\mathrm{SSE}_j}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}_j^2 = \\frac{\\mathrm{SSE}_j}{n}\n$$\n将 MLEs $\\hat{\\theta}_j$（它给出了 $\\mathrm{SSE}_j$）和 $\\hat{\\sigma}_j^2$ 代回对数似然函数，得到最大化对数似然 $\\mathcal{L}_j$：\n$$\n\\mathcal{L}_j = \\ln L(\\hat{\\theta}_j, \\hat{\\sigma}_j^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) - \\frac{\\mathrm{SSE}_j}{2(\\mathrm{SSE}_j/n)} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) + 1 \\right)\n$$\n这个最大化对数似然 $\\mathcal{L}_j$ 是两种信息准则的基础量。模型 $M_j$ 估计的参数总数为 $k_j = p_j + 1$，包括 $\\theta_j$ 中的 $p_j$ 个参数和方差 $\\sigma^2$。\n\n贝叶斯信息准则（BIC）定义为：\n$$\n\\mathrm{BIC}_j = -2\\mathcal{L}_j + k_j \\ln(n)\n$$\n代入 $\\mathcal{L}_j$ 和 $k_j$ 的表达式：\n$$\n\\mathrm{BIC}_j = n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) + 1 \\right) + (p_j+1)\\ln(n)\n$$\n$$\n\\mathrm{BIC}_j = n\\ln(2\\pi) + n\\ln(\\mathrm{SSE}_j) - n\\ln(n) + n + (p_j+1)\\ln(n)\n$$\n在同一数据集上（固定的 $n$）比较模型 $M_1$ 和 $M_2$ 时，可以省略在模型间恒定的项。这些项是 $n\\ln(2\\pi)$、$-n\\ln(n)$ 和 $n$。这样就留下一个与真实 BIC 成比例的简化表达式，足以用于模型选择：\n$$\n\\mathrm{BIC}_j \\propto n\\ln(\\mathrm{SSE}_j) + (p_j+1)\\ln(n)\n$$\nBIC 值较低的模型更优。\n\n小样本校正的赤池信息准则（$\\mathrm{AIC}_c$）定义为：\n$$\n\\mathrm{AIC}_{c,j} = \\mathrm{AIC}_j + \\frac{2k_j(k_j+1)}{n-k_j-1}\n$$\n其中 $\\mathrm{AIC}_j = -2\\mathcal{L}_j + 2k_j$。代入 $\\mathcal{L}_j$ 和 $k_j$：\n$$\n\\mathrm{AIC}_{c,j} = n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\mathrm{SSE}_j}{n}\\right) + 1 \\right) + 2(p_j+1) + \\frac{2(p_j+1)(p_j+2)}{n-(p_j+1)-1}\n$$\n同样，我们通过省略模型间恒定的项（$n\\ln(2\\pi)$、$-n\\ln(n)$ 和 $n$）来简化比较：\n$$\n\\mathrm{AIC}_{c,j} \\propto n\\ln(\\mathrm{SSE}_j) + 2(p_j+1) + \\frac{2(p_j+1)(p_j+2)}{n-p_j-2}\n$$\n$\\mathrm{AIC}_c$ 值较低的模型更优。根据题目要求，如果任一待考量模型的 $n \\le p_j+1$，则 $\\mathrm{AIC}_c$ 被视为未定义。如果 BIC 和 $\\mathrm{AIC}_c$ 选择了不同的模型，并且 $\\mathrm{AIC}_c$ 在比较中有定义，则发生逆转。\n\n为实现解决方案，将对每个测试用例 $(n, p_1, p_2, \\mathrm{SSE}_1, \\mathrm{SSE}_2)$ 使用以下公式：\n1. 使用 $\\mathrm{BIC}_j \\propto n\\ln(\\mathrm{SSE}_j) + (p_j+1)\\ln(n)$ 计算 $\\mathrm{BIC}_1$ 和 $\\mathrm{BIC}_2$，并确定值最小的模型。\n2. 检查 $\\mathrm{AIC}_c$ 在比较中是否有定义，即是否 $n > p_1+1$ 且 $n > p_2+1$。\n3. 如果有定义，则使用 $\\mathrm{AIC}_{c,j} \\propto n\\ln(\\mathrm{SSE}_j) + 2(p_j+1) + \\frac{2(p_j+1)(p_j+2)}{n-p_j-2}$ 计算 $\\mathrm{AIC}_{c,1}$ 和 $\\mathrm{AIC}_{c,2}$，并确定值最小的模型。\n4. 比较选择结果。如果 $\\mathrm{AIC}_c$ 有定义且选择结果不同，则发生了逆转。否则，没有发生逆转。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates model selection outcomes for a series of test cases using BIC and AICc,\n    and determines if AICc reverses the selection made by BIC.\n    \"\"\"\n    # Test cases as 5-tuples: (n, p1, p2, SSE1, SSE2)\n    # n: number of data points\n    # p1, p2: number of parameters for model 1 and model 2\n    # SSE1, SSE2: residual sum of squares for model 1 and model 2\n    test_cases = [\n        (60, 5, 9, 120.0, 95.0),\n        (12, 3, 9, 120.0, 32.0),\n        (20, 7, 8, 110.0, 90.0),\n        (100, 6, 9, 200.0, 150.0),\n        (10, 3, 9, 80.0, 60.0),\n        (15, 5, 10, 100.0, 85.0),\n    ]\n\n    reversal_results = []\n\n    for case in test_cases:\n        n, p1, p2, sse1, sse2 = case\n\n        # The total number of estimated parameters for model j, k_j, is p_j + 1\n        # (p_j model parameters + 1 variance parameter sigma^2).\n        k1 = p1 + 1\n        k2 = p2 + 1\n\n        # --- BIC Selection ---\n        # The simplified BIC formula for model comparison is:\n        # BIC_j = n * ln(SSE_j) + k_j * ln(n)\n        bic1 = n * np.log(sse1) + k1 * np.log(n)\n        bic2 = n * np.log(sse2) + k2 * np.log(n)\n        bic_selection = 1 if bic1  bic2 else 2\n\n        # --- AICc Selection ---\n        # The problem states AICc is undefined if n = p_j + 1 for either model.\n        # This also implies n > k_j for both models.\n        # The denominator of the correction term is n - k_j - 1.\n        # The condition n > p_j + 1 means n >= p_j + 2, or n >= k_j + 1.\n        # This ensures the denominator n - k_j - 1 >= 0.\n        # The standard definition requires n - k_j - 1 > 0, which is n > p_j + 2.\n        # Case 5 has n=10, p2=9, so n = p2+1 and AICc is undefined as per problem.\n        aicc_is_defined = (n > p1 + 1) and (n > p2 + 1)\n\n        aicc_selection = None\n        if aicc_is_defined:\n            # The simplified AICc formula for model comparison is:\n            # AICc_j = n * ln(SSE_j) + 2*k_j + (2*k_j*(k_j+1)) / (n-k_j-1)\n            \n            # Check for non-positive denominator to avoid division by zero or negative.\n            # This is a safeguard, though the problem condition n > p_j + 1\n            # handles the provided test cases.\n            if (n - k1 - 1) = 0 or (n - k2 - 1) = 0:\n                 # This branch is not expected to be hit with problem's test data\n                 # and the n > p_j + 1 rule, but is good practice.\n                 aicc_is_defined = False\n            else:\n                aicc1_penalty = (2 * k1 * (k1 + 1)) / (n - k1 - 1)\n                aicc1 = n * np.log(sse1) + 2 * k1 + aicc1_penalty\n\n                aicc2_penalty = (2 * k2 * (k2 + 1)) / (n - k2 - 1)\n                aicc2 = n * np.log(sse2) + 2 * k2 + aicc2_penalty\n                \n                aicc_selection = 1 if aicc1  aicc2 else 2\n\n        # --- Reversal Check ---\n        # Reversal is true if AICc is defined and selects a different model than BIC.\n        is_reversal = False\n        if aicc_is_defined:\n            if bic_selection != aicc_selection:\n                is_reversal = True\n        \n        reversal_results.append(is_reversal)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, reversal_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "我们最后的练习将探讨另一个至关重要的现实复杂问题：非独立的数据点，这是时间序列测量中的一个普遍特征。标准信息准则建立在误差独立同分布的假设之上，而自相关噪声常常违反这一假设。这个基于编码的练习  将指导您模拟此类数据、拟合模型，然后通过分析残差来诊断假设违规。您将学习如何计算一个“有效样本量”（$n_{eff}$）来校正 $BIC$ 的惩罚项，从而揭示考虑自相关性如何能够改变模型选择的结果。",
            "id": "3326773",
            "problem": "您的任务是实现一个完整的、可运行的程序，该程序使用信息准则对受自相关测量噪声影响的信号时间序列数据执行模型选择。程序必须从指定的生成性常微分方程（ODE）模型模拟数据，在独立高斯误差的假设下拟合两个竞争的ODE模型，从残差自相关中估计有效样本量，并使用有效样本量重新计算贝叶斯信息准则（BIC）以评估模型选择的敏感性。所有步骤都必须从适用于计算系统生物学的基本原理推导得出。\n\n您必须使用的基本基础包括以下经过充分检验的定义和事实：\n- 一阶自回归（AR(1)）过程定义：一个误差序列 $e_t$ 满足 $e_t = \\phi e_{t-1} + \\xi_t$，其中 $|\\phi|  1$，且 $\\xi_t$ 是均值为零的独立同分布高斯噪声。\n- 独立同分布测量噪声的高斯似然。\n- 残差自相关函数的定义及其在量化统计依赖性中的作用。\n- 积分自相关时间的概念及其与有效样本量的关系。\n\n信号状态变量为 $x(t)$。您将实现并比较两个模型：\n- 模型 $\\mathcal{M}_1$：$ \\dfrac{dx}{dt} = -k x + s u(t) $，参数为 $k > 0$ 和 $s > 0$。\n- 模型 $\\mathcal{M}_2$：$ \\dfrac{dx}{dt} = -k x + s u(t) - f x u(t) $，参数为 $k > 0$、$s > 0$ 和 $f > 0$。\n\n输入 $u(t)$ 是一个振幅为 $U$ 的阶跃函数，定义为对所有 $t \\ge 0$，$u(t) = U$。初始条件是 $x(0) = 0$。\n\n数据生成：\n- 基准真相模型是 $\\mathcal{M}_2$，参数为 $k = 0.7$，$s = 1.4$，$f = 0.5$，以及 $U = 1.0$。\n- 测量噪声以AR(1)过程 $e_t = \\phi e_{t-1} + \\xi_t$ 的形式添加，其中 $\\xi_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2)$ 的选择使得 $e_t$ 的平稳标准差为 $\\sigma = 0.2$。具体来说，对于目标平稳方差 $\\sigma^2$，使用 $\\sigma_{\\epsilon}^2 = \\sigma^2 (1 - \\phi^2)$。\n- 观测数据为 $y_t = x(t_t) + e_t$，其中 $t_t$ 是离散采样时间。\n\n模型拟合：\n- 对于每个模型 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$，通过在独立同分布误差的假设下最大化高斯似然来估计参数，这等效于在参数非负性约束下最小化 $y_t$ 与模型在 $t_t$ 处的预测值之间的残差平方和。\n- 拟合后，计算残差 $r_t = y_t - \\hat{x}(t_t)$。\n\n自相关和有效样本量：\n- 根据 $r_t$ 估计滞后为 $k = 1, 2, \\ldots$ 的样本自相关函数。\n- 使用估计的自相关，计算一个积分自相关时间和一个有效样本量 $n_{eff}$，该样本量反映了由于自相关导致的独立信息的减少。确保 $1 \\le n_{eff} \\le n$，其中 $n$ 是时间点的数量。\n\n信息准则：\n- 对于每个模型，使用独立高斯假设下的最大似然和实际样本量 $n$ 计算贝叶斯信息准则（BIC）。\n- 对于每个模型，将 $n$ 替换为 $n_{eff}$ 重新计算BIC的复杂度惩罚项，以评估敏感性。\n\n选择：\n- 对于每个测试用例，确定哪个模型被朴素BIC（使用 $n$）选中，以及哪个模型被校正后的BIC（使用 $n_{eff}$）选中。提供当使用 $n_{eff}$ 时选择是否发生变化。\n\n您的程序必须实现所述的流程，并按以下指定格式精确生成一行输出。不允许用户输入；程序必须是自包含的。\n\n测试套件：\n- 使用以下参数值集。每个案例定义了AR(1)系数 $\\phi$、采样间隔 $\\Delta t$ 和总时长 $T_{end}$，其中 $U = 1.0$ 且基准真相参数为 $k = 0.7$、$s = 1.4$、$f = 0.5$ 和 $\\sigma = 0.2$。\n    - 案例1：$\\phi = 0.0$，$\\Delta t = 0.1$，$T_{end} = 10.0$。\n    - 案例2：$\\phi = 0.4$，$\\Delta t = 0.1$，$T_{end} = 10.0$。\n    - 案例3：$\\phi = 0.8$，$\\Delta t = 0.1$，$T_{end} = 10.0$。\n    - 案例4：$\\phi = 0.95$，$\\Delta t = 0.1$，$T_{end} = 10.0$。\n    - 案例5（边界条件）：$\\phi = 0.9$，$\\Delta t = 0.2$，$T_{end} = 4.0$。\n- 对于每个案例，样本数量为 $n = \\lfloor T_{end}/\\Delta t \\rfloor + 1$，时间从 $t = 0$ 到 $t = T_{end}$（含）。\n\n最终输出规范：\n- 对于每个测试用例，输出一个列表 $[m_{naive}, m_{eff}, changed]$，其中 $m_{naive} \\in \\{1, 2\\}$ 是朴素BIC选择的模型索引，$m_{eff} \\in \\{1, 2\\}$ 是校正后BIC选择的模型索引，$changed$ 是一个布尔值，指示使用 $n_{eff}$ 时选择是否改变。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，并用方括号括起来。例如，输出应类似于 $[[1,2,True],[1,1,False],\\ldots]$，每个测试用例对应一个元素，并按上述顺序排列。\n\n角度单位不适用。不需要物理单位。所有数值答案必须按指定格式生成。最终打印的行必须严格符合指定格式，不得包含额外文本。",
            "solution": "问题陈述已经过严格验证，并被认为是有效的。它具有科学依据，提法明确，客观，并提供了足够的信息来构建一个唯一且有意义的解决方案。该问题要求实现一个计算流程，以评估贝叶斯信息准则（BIC）在模型选择中对时间序列数据残差自相关性的敏感性。这是计算系统生物学中一个标准且相关的任务。\n\n解决方案遵循一系列有原则的步骤进行：\n1.  推导指定常微分方程（ODE）模型的解析解。\n2.  模拟带有自相关测量噪声的时间序列数据。\n3.  通过最小化残差平方和（等效于最大化高斯似然）来估计两个竞争模型的参数。\n4.  估计残差自相关并计算有效样本量 $n_{eff}$。\n5.  使用 $n_{eff}$ 计算标准BIC和校正后的BIC。\n6.  比较一组测试用例的模型选择结果。\n\n### 1. ODE模型的解析解\n\n这两个竞争模型描述了信号状态变量 $x(t)$ 的时间演化。输入 $u(t)$ 是一个恒定振幅为 $U$ 的阶跃函数，即对于 $t \\ge 0$，$u(t) = U$。初始条件为 $x(0) = 0$。\n\n模型 $\\mathcal{M}_1$ 由线性一阶ODE给出：\n$$\n\\frac{dx}{dt} = -k x + s U\n$$\n参数为 $k > 0$ 和 $s > 0$。在 $x(0) = 0$ 条件下的解为：\n$$\nx_1(t; k, s) = \\frac{sU}{k} (1 - e^{-kt})\n$$\n\n模型 $\\mathcal{M}_2$ 引入了一个额外的反馈项：\n$$\n\\frac{dx}{dt} = -k x + s U - f x U\n$$\n参数为 $k > 0$，$s > 0$ 和 $f > 0$。这可以重排成相同的线性一阶形式：\n$$\n\\frac{dx}{dt} = -(k + fU) x + s U\n$$\n令 $k' = k + fU$，其解（类似于 $\\mathcal{M}_1$ 的解）为：\n$$\nx_2(t; k, s, f) = \\frac{sU}{k+fU} (1 - e^{-(k+fU)t})\n$$\n使用这些解析解进行模型拟合比重复进行ODEs的数值积分在计算上更高效，在数值上更稳定。\n\n### 2. 数据生成\n\n生成合成数据以模拟真实的实验情景。\n基准真相动力学遵循模型 $\\mathcal{M}_2$，参数为 $k_{true} = 0.7$，$s_{true} = 1.4$，$f_{true} = 0.5$，输入振幅 $U=1.0$。因此，真实的、无噪声的状态轨迹是 $x_{true}(t) = x_2(t; k_{true}, s_{true}, f_{true})$。\n\n测量噪声 $e_t$ 被建模为一阶平稳自回归过程（AR(1)）：\n$$\ne_t = \\phi e_{t-1} + \\xi_t\n$$\n其中 $\\xi_t$ 是独立同分布（i.i.d.）的高斯随机变量，均值为 $0$，方差为 $\\sigma_{\\epsilon}^2$。AR(1)过程的平稳方差是 $\\sigma^2 = \\frac{\\sigma_{\\epsilon}^2}{1 - \\phi^2}$。为达到目标平稳标准差 $\\sigma$，新息项 $\\xi_t$ 的方差被设置为 $\\sigma_{\\epsilon}^2 = \\sigma^2 (1 - \\phi^2)$。在模拟中，初始噪声项 $e_0$ 从平稳分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取，后续项则递归生成。\n\n在离散时间点 $t_i$ 的最终观测数据是 $y_i = x_{true}(t_i) + e_i$。使用固定的随机种子以确保噪声生成过程的可复现性。\n\n### 3. 模型拟合与参数估计\n\n对于每个模型 $\\mathcal{M}_j$，通过将其预测值 $x_j(t; \\theta_j)$ 拟合到观测数据 $y_i$ 来估计参数 $\\theta_j$。问题指定在（不正确的）独立同分布误差假设下最大化高斯似然。这等效于最小化残差平方和（SSR）：\n$$\n\\text{SSR}(\\theta_j) = \\sum_{i=1}^{n} (y_i - x_j(t_i; \\theta_j))^2\n$$\n其中 $n$ 是数据点的数量。此最小化过程使用数值优化方法，特别是L-BFGS-B算法，并受限于所有参数的非负性约束（例如 $k>0$, $s>0$, $f>0$）。此步骤对每个模型的结果是最佳拟合参数集 $\\hat{\\theta}_j$ 和相应的最小SSR，记为 $\\text{SSR}_{min, j}$。\n\n### 4. 有效样本量 ($n_{eff}$)\n\n残差中存在的自相关性违反了标准统计准则所依赖的独立同分布假设。有效样本量 $n_{eff}$ 量化了自相关数据等效于多少个独立样本。它是根据积分自相关时间（IAT） $\\tau$ 计算的。\n\n首先，为每个拟合模型计算残差：$r_{j,i} = y_i - x_j(t_i; \\hat{\\theta}_j)$。\n接下来，估计这些残差的样本自相关函数（ACF），$\\hat{\\rho}_j(k)$，对于滞后 $k$。\n然后计算IAT, $\\tau_j$。一个稳健的方法是在正相关项上对ACF求和：\n$$\n\\tau_j = 1 + 2 \\sum_{k=1}^{M_j} \\hat{\\rho}_j(k)\n$$\n其中求和窗口 $M_j$ 是使得所有前面的ACF部分和均为正的最大滞后。为简单和稳健起见，我们将采用一个常见的启发式方法：在第一个 $\\hat{\\rho}_j(k)$ 为非正的滞后 $k$ 处截断求和。如果 $\\hat{\\rho}_j(1) \\le 0$，则和为空，$\\tau_j=1$。\n\n最后，为每个模型拟合计算有效样本量：\n$$\nn_{eff, j} = \\frac{n}{\\tau_j}\n$$\n$n_{eff, j}$ 的值被约束在区间 $[1, n]$ 内，以确保其物理意义。每个模型都计算一个单独的 $n_{eff, j}$，因为残差的结构取决于拟合的质量和结构。\n\n### 5. 贝叶斯信息准则（BIC）\n\nBIC通过对模型复杂度的惩罚来进行模型选择。BIC的公式为：\n$$\n\\text{BIC} = K \\ln(N) - 2 \\ln(\\hat{L})\n$$\n其中 $K$ 是估计参数的数量，$N$ 是样本量，$\\hat{L}$ 是最大化的似然值。使用高斯噪声模型的剖面似然，其中残差方差 $\\sigma_r^2$ 也被估计为 $\\hat{\\sigma}_r^2 = \\text{SSR}_{min}/n$，BIC可以表示为（相差一个加性常数）：\n$$\n\\text{BIC} \\approx K \\ln(N) + n \\ln\\left(\\frac{\\text{SSR}_{min}}{n}\\right)\n$$\n参数数量 $K$ 必须包括所有估计量。对于模型 $\\mathcal{M}_1$，参数是 $\\{k, s\\}$ 加上残差方差 $\\sigma_r^2$，所以 $K_1 = 3$。对于模型 $\\mathcal{M}_2$，它们是 $\\{k, s, f\\}$ 加上 $\\sigma_r^2$，所以 $K_2 = 4$。\n\n为每个模型计算两个版本的BIC：\n1.  **朴素BIC ($\\text{BIC}_{naive}$)**：使用实际数据点数，$N=n$。\n2.  **校正后BIC ($\\text{BIC}_{eff}$)**：使用有效样本量，$N=n_{eff,j}$。\n\n### 6. 模型选择与比较\n\n对于每个测试用例，选择BIC值较低的模型。\n-   “朴素”选择基于比较 $\\text{BIC}_{naive,1}$ 和 $\\text{BIC}_{naive,2}$。\n-   “校正后”选择基于比较 $\\text{BIC}_{eff,1}$ 和 $\\text{BIC}_{eff,2}$。\n\n每个案例的最终输出报告了两种方法的所选模型索引（1或2），以及一个布尔标志，指示对有效样本量的校正是否改变了选择结果。此过程系统地评估了忽略残差自相关对模型选择的影响。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Performs model selection for signaling time series data with autocorrelated noise.\n\n    This program simulates data from a ground-truth ODE model with AR(1) noise,\n    fits two competing models, estimates the effective sample size from residuals,\n    and computes both naive and corrected Bayesian Information Criteria (BIC) to\n    assess the sensitivity of model selection to residual autocorrelation.\n    \"\"\"\n\n    # Set random seed for reproducibility of noise generation\n    np.random.seed(42)\n\n    # --- Helper Functions ---\n\n    def model_1_analytical(t, k, s, U=1.0):\n        \"\"\"Analytical solution for Model M1.\"\"\"\n        if k = 0:  # Avoid division by zero or non-physical behavior\n            return np.full_like(t, np.inf)\n        return (s * U / k) * (1.0 - np.exp(-k * t))\n\n    def model_2_analytical(t, k, s, f, U=1.0):\n        \"\"\"Analytical solution for Model M2.\"\"\"\n        k_prime = k + f * U\n        if k_prime = 0: # Avoid division by zero or non-physical behavior\n            return np.full_like(t, np.inf)\n        return (s * U / k_prime) * (1.0 - np.exp(-k_prime * t))\n\n    def generate_ar1_noise(n, phi, sigma):\n        \"\"\"Generates an AR(1) noise series.\"\"\"\n        if abs(phi)  1.0:\n            sigma_epsilon_sq = sigma**2 * (1.0 - phi**2)\n            sigma_epsilon = np.sqrt(sigma_epsilon_sq)\n        else: # Handle non-stationary case gracefully\n            sigma_epsilon = 0.0\n\n        xi = np.random.normal(0, sigma_epsilon, n)\n        e = np.zeros(n)\n        if n > 0:\n            # Initialize with stationary distribution\n            e[0] = np.random.normal(0, sigma)\n            for t in range(1, n):\n                e[t] = phi * e[t-1] + xi[t]\n        return e\n\n    def calculate_acf(series):\n        \"\"\"Calculates the autocorrelation function of a time series.\"\"\"\n        n = len(series)\n        if n == 0:\n            return np.array([])\n        mean_subtracted = series - np.mean(series)\n        autocov = np.correlate(mean_subtracted, mean_subtracted, mode='full')\n        # Normalize by lag-0 autocovariance\n        acf = autocov[n-1:] / autocov[n-1]\n        return acf\n\n    def calculate_neff(residuals):\n        \"\"\"Calculates the effective sample size from residuals.\"\"\"\n        n = len(residuals)\n        acf = calculate_acf(residuals)\n        \n        # Heuristic for IAT: sum positive ACF terms until the first non-positive one\n        tau_sum = 0.0\n        for lag in range(1, len(acf)):\n            if acf[lag] > 0:\n                tau_sum += acf[lag]\n            else:\n                break\n        \n        iat = 1.0 + 2.0 * tau_sum\n        \n        # tau must be at least 1, which implies neff = n\n        if iat  1.0:\n            iat = 1.0\n\n        neff = n / iat\n        # Clamp n_eff to be within [1, n]\n        neff = np.clip(neff, 1, n)\n        return neff\n\n    def calculate_bic(K, n_points, ssr, n_eff):\n        \"\"\"Calculates BIC using n_eff for the penalty term.\"\"\"\n        if ssr = 0 or n_points == 0 or n_eff = 0:\n            return np.inf\n        \n        # BIC = K*ln(N) + n*ln(SSR/n)\n        # We use n_eff for the sample size N in the penalty term\n        bic_val = K * np.log(n_eff) + n_points * np.log(ssr / n_points)\n        return bic_val\n\n    # --- Problem Constants ---\n    k_true, s_true, f_true = 0.7, 1.4, 0.5\n    U, sigma = 1.0, 0.2\n\n    test_cases = [\n        # (phi, dt, T_end)\n        (0.0, 0.1, 10.0),\n        (0.4, 0.1, 10.0),\n        (0.8, 0.1, 10.0),\n        (0.95, 0.1, 10.0),\n        (0.9, 0.2, 4.0),\n    ]\n\n    results = []\n\n    for phi, dt, T_end in test_cases:\n        # --- 1. Data Generation ---\n        t_eval = np.arange(0, T_end + dt/2, dt) # Use dt/2 to include T_end safely\n        n_points = len(t_eval)\n        \n        x_true = model_2_analytical(t_eval, k_true, s_true, f_true, U)\n        noise = generate_ar1_noise(n_points, phi, sigma)\n        y_obs = x_true + noise\n\n        # --- 2. Model Fitting ---\n\n        # Fit Model 1\n        obj_fun_m1 = lambda p: np.sum((y_obs - model_1_analytical(t_eval, p[0], p[1], U))**2)\n        initial_guess_m1 = [1.0, 1.0]\n        bounds_m1 = [(1e-9, None), (1e-9, None)]\n        res_m1 = minimize(obj_fun_m1, initial_guess_m1, method='L-BFGS-B', bounds=bounds_m1)\n        k1_fit, s1_fit = res_m1.x\n        ssr1 = res_m1.fun\n\n        # Fit Model 2\n        obj_fun_m2 = lambda p: np.sum((y_obs - model_2_analytical(t_eval, p[0], p[1], p[2], U))**2)\n        initial_guess_m2 = [1.0, 1.0, 1.0]\n        bounds_m2 = [(1e-9, None), (1e-9, None), (1e-9, None)]\n        res_m2 = minimize(obj_fun_m2, initial_guess_m2, method='L-BFGS-B', bounds=bounds_m2)\n        k2_fit, s2_fit, f2_fit = res_m2.x\n        ssr2 = res_m2.fun\n        \n        # --- 3. Residuals and n_eff ---\n        \n        # For Model 1\n        y_fit_m1 = model_1_analytical(t_eval, k1_fit, s1_fit, U)\n        residuals1 = y_obs - y_fit_m1\n        neff1 = calculate_neff(residuals1)\n\n        # For Model 2\n        y_fit_m2 = model_2_analytical(t_eval, k2_fit, s2_fit, f2_fit, U)\n        residuals2 = y_obs - y_fit_m2\n        neff2 = calculate_neff(residuals2)\n\n        # --- 4. BIC Calculation and Model Selection ---\n        \n        # Number of parameters (including residual variance)\n        K1, K2 = 3, 4\n\n        # Naive BIC (using n_points)\n        bic_naive1 = calculate_bic(K1, n_points, ssr1, n_points)\n        bic_naive2 = calculate_bic(K2, n_points, ssr2, n_points)\n        m_naive = 1 if bic_naive1  bic_naive2 else 2\n\n        # Corrected BIC (using n_eff)\n        bic_eff1 = calculate_bic(K1, n_points, ssr1, neff1)\n        bic_eff2 = calculate_bic(K2, n_points, ssr2, neff2)\n        m_eff = 1 if bic_eff1  bic_eff2 else 2\n        \n        changed = (m_naive != m_eff)\n\n        results.append(f\"[{m_naive},{m_eff},{'True' if changed else 'False'}]\")\n\n    # Final print statement\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}