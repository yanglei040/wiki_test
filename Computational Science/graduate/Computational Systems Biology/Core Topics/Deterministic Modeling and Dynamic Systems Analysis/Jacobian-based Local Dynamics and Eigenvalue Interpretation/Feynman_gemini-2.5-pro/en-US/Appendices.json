{
    "hands_on_practices": [
        {
            "introduction": "Understanding the local dynamics around a steady state is fundamental, but a crucial next step is to determine how that steady state shifts in response to parameter perturbations. This practice focuses on steady-state sensitivity analysis, a powerful technique for quantifying how changes in parameters, like a drug's concentration or a protein's synthesis rate, affect the system's equilibrium. By solving a linear system derived directly from the Jacobian, you will calculate the sensitivity vector and interpret its components in the context of the network's structure, providing a direct link between local interactions and global system response .",
            "id": "3321821",
            "problem": "Consider a two-state biochemical reaction module linearized around a stable steady state $x^{\\ast}$ under a scalar parameter $p$. Let the local dynamics be represented by the Jacobian matrix $J$ and the direct parameter input vector $\\partial f / \\partial p$ evaluated at $(x^{\\ast}, p^{\\ast})$, where\n$$\nJ=\\begin{pmatrix}-2  1\\\\ 0  -1\\end{pmatrix},\\qquad \\frac{\\partial f}{\\partial p}=\\begin{pmatrix}1\\\\ 0\\end{pmatrix}.\n$$\nAssume $f(x^{\\ast},p^{\\ast})=0$ and that the steady-state sensitivity vector $S_{x}:=\\frac{d x^{\\ast}}{d p}$ exists and is finite. Using only standard linearization and sensitivity definitions at steady state, compute the steady-state sensitivity $S_{x}$ by solving the appropriate linear system that relates $J$, $S_{x}$, and $\\partial f/\\partial p$. Then, briefly interpret the sign and relative magnitude of the sensitivity of each state in terms of the local coupling structure implied by $J$ and the stability indicated by its eigenvalues.\n\nReport your final sensitivity vector as a row vector with exact rational entries. No rounding is required, and no units are needed.",
            "solution": "The dynamics of the system near a steady state $x^{\\ast}$ are described by the vector of ordinary differential equations $\\frac{dx}{dt} = f(x, p)$. By definition, at steady state, the time derivatives are zero, so we have the condition $f(x^{\\ast}(p), p) = 0$, where we write $x^{\\ast}(p)$ to emphasize that the location of the steady state depends on the parameter $p$.\n\nThe steady-state sensitivity vector, $S_{x}$, is defined as the derivative of the steady-state solution $x^{\\ast}$ with respect to the parameter $p$, i.e., $S_{x} := \\frac{d x^{\\ast}}{d p}$. To find the relationship between $S_x$, the Jacobian $J$, and the direct parameter perturbation $\\frac{\\partial f}{\\partial p}$, we differentiate the steady-state equation $f(x^{\\ast}(p), p) = 0$ with respect to $p$ using the multivariable chain rule:\n$$\n\\frac{d}{dp} [f(x^{\\ast}(p), p)] = \\frac{\\partial f}{\\partial x} \\frac{d x^{\\ast}}{d p} + \\frac{\\partial f}{\\partial p} \\frac{d p}{d p} = 0\n$$\nBy definition, the Jacobian matrix is $J = \\frac{\\partial f}{\\partial x}$ evaluated at the steady state $(x^{\\ast}, p^{\\ast})$. The term $\\frac{d x^{\\ast}}{d p}$ is the sensitivity vector $S_x$. Since $\\frac{dp}{dp}=1$, the equation becomes:\n$$\nJ S_{x} + \\frac{\\partial f}{\\partial p} = 0\n$$\nThis can be rearranged into the linear system we must solve for $S_x$:\n$$\nJ S_{x} = - \\frac{\\partial f}{\\partial p}\n$$\nThe problem provides the specific matrices for the system at a particular steady state $(x^{\\ast}, p^{\\ast})$:\n$$\nJ=\\begin{pmatrix}-2  1\\\\ 0  -1\\end{pmatrix},\\qquad \\frac{\\partial f}{\\partial p}=\\begin{pmatrix}1\\\\ 0\\end{pmatrix}\n$$\nLet the sensitivity vector be $S_{x} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$. We substitute the given values into the linear system:\n$$\n\\begin{pmatrix}-2  1\\\\ 0  -1\\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = - \\begin{pmatrix}1\\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1\\\\ 0\\end{pmatrix}\n$$\nThis matrix equation corresponds to a system of two linear equations:\n1.  $-2s_1 + s_2 = -1$\n2.  $0s_1 - s_2 = 0$\n\nFrom the second equation, we immediately find that $-s_2 = 0$, which implies $s_2 = 0$.\nSubstituting $s_2 = 0$ into the first equation gives:\n$$\n-2s_1 + 0 = -1\n$$\n$$\n-2s_1 = -1\n$$\n$$\ns_1 = \\frac{1}{2}\n$$\nThus, the steady-state sensitivity vector is $S_{x} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$. As requested, in row vector form, this is $(\\frac{1}{2}, 0)$.\n\nFor the interpretation, we analyze the structure of the system and the result.\nFirst, we check stability. The Jacobian matrix $J$ is upper triangular, so its eigenvalues are its diagonal entries: $\\lambda_1 = -2$ and $\\lambda_2 = -1$. Since both eigenvalues are real and negative, the steady state is stable, as stated in the problem.\n\nThe components of the Jacobian $J$ describe the local interactions:\n- $J_{11} = -2$ and $J_{22} = -1$ indicate that both state variables $x_1$ and $x_2$ are self-inhibiting, a stabilizing feature.\n- $J_{12} = 1$ indicates that $x_2$ activates $x_1$.\n- $J_{21} = 0$ indicates that $x_1$ does not directly affect $x_2$. This establishes a cascade-like or feed-forward interaction where influence flows from $x_2$ to $x_1$, but not in the reverse direction.\n\nThe direct parameter input vector $\\frac{\\partial f}{\\partial p} = \\begin{pmatrix}1\\\\ 0\\end{pmatrix}$ shows that the parameter $p$ directly perturbs only the dynamics of $x_1$ (specifically, it increases its rate of change), and has no direct effect on $x_2$.\n\nThe sensitivity results $S_{x} = (\\frac{dx_1^{\\ast}}{dp}, \\frac{dx_2^{\\ast}}{dp}) = (\\frac{1}{2}, 0)$ can now be interpreted:\n- The sensitivity of the second state is zero ($s_2 = 0$). This is because the parameter $p$ does not directly affect $x_2$ (since $\\frac{\\partial f_2}{\\partial p} = 0$), and there is no indirect pathway for the perturbation to reach $x_2$ from $x_1$ (since $J_{21} = 0$). Thus, the steady-state value of $x_2$ is completely insensitive to changes in $p$.\n- The sensitivity of the first state is positive and finite ($s_1 = \\frac{1}{2}$). The positive sign indicates that an increase in $p$ leads to an increase in the steady-state value of $x_1$. This is expected, as $p$ directly promotes $x_1$. The magnitude, $\\frac{1}{2}$, reflects the balance at the new steady state. The direct positive perturbation from $p$ must be balanced by the system's response. Because $s_2=0$, the change in $x_1$ is only balanced by its own strong self-inhibition ($J_{11}=-2$). The resulting sensitivity can be seen from the first equation, $s_1 = \\frac{-1 - s_2}{-2} = \\frac{-1 - 0}{-2} = \\frac{1}{2}$. The strong self-damping attenuates the effect of the parameter perturbation.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A common misconception is that a stable steady state, guaranteed by eigenvalues with negative real parts, implies that all perturbations decay monotonically. This exercise confronts that notion by exploring the dynamics of non-normal systems, where the eigenvectors of the Jacobian are not orthogonal. You will explicitly compute the system's evolution to demonstrate how strong, asymmetric coupling in a hypothetical biochemical network can cause significant transient amplification of perturbations, even as the system ultimately returns to its stable equilibrium . This phenomenon is critical for understanding noise amplification and threshold responses in biological circuits.",
            "id": "3321850",
            "problem": "A two-species biochemical reaction network is linearized near a stable steady state to obtain the local dynamics of the deviation vector $x(t) \\in \\mathbb{R}^{2}$ governed by the linear ordinary differential equation $\\frac{d x}{d t} = J x$. The $2 \\times 2$ Jacobian matrix $J$ is\n$$\nJ \\;=\\; \\begin{pmatrix} -1  100 \\\\ 0  -1 \\end{pmatrix}.\n$$\nIn computational systems biology, the local flow map $x(t) = \\exp(J t) x(0)$ is determined by the matrix exponential $\\exp(J t)$, and the transient amplification of perturbations is quantified by the induced matrix $2$-norm $\\|\\exp(J t)\\|_{2}$, the largest singular value of $\\exp(J t)$.\n\nStarting only from the facts that (i) the solution of the linear system $\\frac{d x}{d t} = J x$ with initial condition $x(0)$ is $x(t) = \\exp(J t) x(0)$, and (ii) the spectral norm $\\|A\\|_{2}$ equals the largest singular value of $A$, perform the following:\n\n1. Compute $\\exp(J t)$ explicitly for all real $t \\ge 0$.\n2. Using first principles about singular values, derive an exact analytic expression for $\\|\\exp(J t)\\|_{2}$ as a function of $t$, and use it to demonstrate that there is transient growth, namely that there exists an interval of moderate $t  0$ for which $\\|\\exp(J t)\\|_{2}  1$ despite all eigenvalues of $J$ having negative real parts.\n3. Interpret mechanistically why the large off-diagonal entry $100$ leads to non-normal transient amplification in this two-species system.\n\nFinally, determine the exact closed-form expression (in elementary functions) of the supremum over $t \\ge 0$ of $\\|\\exp(J t)\\|_{2}$ for this specific matrix $J$, and report that single expression as your final answer. The amplification factor is dimensionless, so no unit is required. Do not round; provide the exact symbolic result.",
            "solution": "The solution addresses the four parts of the problem sequentially.\n\n**1. Computation of the Matrix Exponential $\\exp(J t)$**\n\nThe Jacobian matrix is $J = \\begin{pmatrix} -1  100 \\\\ 0  -1 \\end{pmatrix}$. We can decompose this into the sum of a diagonal matrix $D$ and a nilpotent matrix $N$.\nLet $D = -I = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix}$ and $N = \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix}$.\nThen $J = D + N$. Since the identity matrix $I$ commutes with any matrix, $D$ and $N$ commute ($DN = ND$). This allows us to use the property $\\exp(J t) = \\exp((D+N)t) = \\exp(Dt)\\exp(Nt)$.\n\nThe exponential of the diagonal part is straightforward:\n$$\\exp(Dt) = \\exp(-It) = e^{-t} I = \\begin{pmatrix} e^{-t}  0 \\\\ 0  e^{-t} \\end{pmatrix}$$\n\nFor the nilpotent part $N$, we compute its powers: $N^2 = \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$.\nSince $N^2=0$, all higher powers are also zero. The Taylor series for $\\exp(Nt)$ truncates:\n$$\\exp(Nt) = I + Nt + \\frac{(Nt)^2}{2!} + \\dots = I + Nt = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + t\\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  100t \\\\ 0  1 \\end{pmatrix}$$\n\nCombining these, we get the matrix exponential:\n$$\\exp(Jt) = \\exp(Dt)\\exp(Nt) = \\begin{pmatrix} e^{-t}  0 \\\\ 0  e^{-t} \\end{pmatrix} \\begin{pmatrix} 1  100t \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} e^{-t}  100t e^{-t} \\\\ 0  e^{-t} \\end{pmatrix}$$\n\n**2. Derivation of $\\|\\exp(J t)\\|_{2}$ and Demonstration of Transient Growth**\n\nThe eigenvalues of $J$ are its diagonal entries, $\\lambda_1 = \\lambda_2 = -1$. Both have negative real parts, so the system is asymptotically stable ($\\lim_{t\\to\\infty} x(t) = 0$).\n\nTo demonstrate transient growth, we can examine the initial rate of change of the norm. The initial growth rate of $\\|\\exp(Jt)\\|_2$ at $t=0$ is given by the largest eigenvalue of the symmetric part of $J$, which is $J_S = \\frac{1}{2}(J+J^T)$.\n$$J_S = \\frac{1}{2}\\left(\\begin{pmatrix} -1  100 \\\\ 0  -1 \\end{pmatrix} + \\begin{pmatrix} -1  0 \\\\ 100  -1 \\end{pmatrix}\\right) = \\begin{pmatrix} -1  50 \\\\ 50  -1 \\end{pmatrix}$$\nThe eigenvalues of $J_S$ are given by $(-1-\\lambda)^2 - 50^2 = 0$, so $\\lambda+1 = \\pm 50$. This yields $\\lambda_{S,1} = 49$ and $\\lambda_{S,2} = -51$. Since the largest eigenvalue is positive (49), the norm $\\|\\exp(Jt)\\|_2$ will initially increase from its value of $1$ at $t=0$. This demonstrates transient growth despite the system's asymptotic stability. The explicit expression for the norm over time is derived in part 4.\n\n**3. Mechanistic Interpretation**\n\nThe system of differential equations is:\n$$\\frac{dx_1}{dt} = -x_1 + 100x_2$$\n$$\\frac{dx_2}{dt} = -x_2$$\nBoth species decay, but species 2 strongly activates species 1. Consider an initial perturbation only in species 2, e.g., $x(0) = [0, \\epsilon]^T$. The amount of species 2 decays simply as $x_2(t) = \\epsilon e^{-t}$. However, this decaying $x_2$ continuously produces $x_1$. The solution for $x_1$ is $x_1(t) = 100 \\epsilon t e^{-t}$. The term $100t$ causes an initial rapid linear growth in $x_1$, which can make the overall state vector $\\|x(t)\\|$ much larger than the initial perturbation of size $\\epsilon$. Eventually, the exponential decay term $e^{-t}$ dominates, and the entire system returns to zero. This \"shear\" mechanism, where one decaying variable drives another, is the source of the non-normal transient amplification.\n\n**4. Supremum of $\\|\\exp(J t)\\|_{2}$**\n\nTo find the maximum amplification, we need to find the supremum of $F(t) = \\|\\exp(Jt)\\|_2$. An analytical expression for the norm is $F(t) = \\exp(\\arcsinh(50t) - t)$. To find the maximum, we set the derivative of the exponent to zero:\n$$\\frac{d}{dt}(\\arcsinh(50t) - t) = \\frac{50}{\\sqrt{(50t)^2+1}} - 1 = 0$$\nSolving for $t$ gives $50 = \\sqrt{2500t^2+1}$, which leads to $2500 = 2500t^2+1$, so $t^2 = 2499/2500$.\nThe time of maximum amplification is $t_{\\max} = \\frac{\\sqrt{2499}}{50} = \\frac{7\\sqrt{51}}{50}$.\n\nTo find the maximum value, we substitute $t_{\\max}$ back into $F(t)$. At this time, $50t_{\\max} = \\sqrt{2499} = 7\\sqrt{51}$.\nThe exponent is:\n$$\\arcsinh(7\\sqrt{51}) - \\frac{7\\sqrt{51}}{50} = \\ln(7\\sqrt{51} + \\sqrt{(7\\sqrt{51})^2+1}) - \\frac{7\\sqrt{51}}{50}$$\n$$= \\ln(7\\sqrt{51} + \\sqrt{2499+1}) - \\frac{7\\sqrt{51}}{50} = \\ln(50+7\\sqrt{51}) - \\frac{7\\sqrt{51}}{50}$$\nThe supremum is the exponential of this value:\n$$\\sup_{t \\ge 0} \\|\\exp(J t)\\|_{2} = \\exp\\left(\\ln(50+7\\sqrt{51}) - \\frac{7\\sqrt{51}}{50}\\right) = (50+7\\sqrt{51})\\exp\\left(-\\frac{7\\sqrt{51}}{50}\\right)$$",
            "answer": "$$\n\\boxed{(50+7\\sqrt{51})\\exp\\left(-\\frac{7\\sqrt{51}}{50}\\right)}\n$$"
        },
        {
            "introduction": "Many biological systems exhibit rhythmic behavior, from circadian clocks to metabolic oscillations, which arise from the loss of stability of a steady state. This comprehensive practice uses the famous repressilator model, a cornerstone of synthetic biology, to demonstrate how to predict the onset of sustained oscillations through a Hopf bifurcation. You will derive the Jacobian for this model, analyze its eigenvalues as a function of key biological parameters, and implement a computational workflow to identify the parameter regimes where the steady state becomes unstable and gives way to a limit cycle . This exercise integrates analytical theory with practical computation to explore one of the most important dynamic behaviors in systems biology.",
            "id": "3321845",
            "problem": "Consider the dimensionless three-gene repressilator model, a canonical synthetic gene circuit consisting of three transcriptional repressors arranged in a ring. Let $x(t)$, $y(t)$, and $z(t)$ denote the concentrations of the three gene products. The dynamics are modeled by the system of ordinary differential equations\n$$\n\\frac{dx}{dt} = -x + \\frac{\\alpha}{1 + \\left(\\frac{z}{K}\\right)^n}, \\quad\n\\frac{dy}{dt} = -y + \\frac{\\alpha}{1 + \\left(\\frac{x}{K}\\right)^n}, \\quad\n\\frac{dz}{dt} = -z + \\frac{\\alpha}{1 + \\left(\\frac{y}{K}\\right)^n},\n$$\nwhere $n \\ge 1$ is the Hill coefficient (dimensionless), $\\alpha  0$ is the maximal production rate (dimensionless), and $K  0$ is the repression threshold (dimensionless). Degradation rates are scaled to $1$ for all species.\n\nTasks:\n1. Starting from the definition of local linearization via the Jacobian matrix, consider the symmetric steady state $x^* = y^* = z^* = s$ defined implicitly by the fixed point condition $s = \\frac{\\alpha}{1 + \\left(\\frac{s}{K}\\right)^n}$. Derive the Jacobian matrix $J$ at this symmetric steady state in terms of $s$, $n$, $\\alpha$, and $K$, by computing the partial derivatives of the right-hand side with respect to the state variables and evaluating them at $x=y=z=s$.\n2. Using first principles for interpreting eigenvalues of the Jacobian matrix, explain how the sign of the real parts of eigenvalues determines local stability. Identify the conditions under which a complex conjugate pair of eigenvalues crosses the imaginary axis, indicating a Hopf bifurcation and the onset of sustained oscillations.\n3. Write a complete, runnable program that, for a given set of parameters $(n,\\alpha,K)$, performs the following steps:\n   - Numerically computes the symmetric steady state by solving for $s$ (or equivalently for $r = \\left(\\frac{s}{K}\\right)^n$) so that the fixed point condition is satisfied.\n   - Constructs the Jacobian matrix $J$ at $x=y=z=s$ using the computed derivatives.\n   - Computes the eigenvalues of $J$ and determines whether a complex conjugate pair has positive real part (strictly greater than zero), which indicates that the system is in an oscillatory regime associated with a Hopf bifurcation.\n   - Outputs, for each test case, an integer $1$ if the oscillatory regime is predicted (complex conjugate pair with positive real part) and $0$ otherwise (including the boundary case where the real part equals zero).\nAll variables and parameters are dimensionless; no physical units are required. Angles are not involved.\n\nTest suite:\nUse the following parameter sets $(n,\\alpha,K)$ to evaluate your program. These cover a general case, boundary conditions, and edge cases:\n- $(2, 100.0, 1.0)$\n- $(3, 4.0, 1.0)$\n- $(4, 2.0, 1.0)$\n- $(4, 4.0, 1.0)$\n- $(5, 1.0, 0.1)$\n- $(3, 1.0, 1.0)$\n\nFinal output format:\nYour program should produce a single line containing the results as a comma-separated list enclosed in square brackets. For example, for six test cases, the output must be of the form\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]\n$$\nwhere each $\\text{result}_i$ is the integer $0$ or $1$ as specified above.",
            "solution": "The solution provides the analytical derivations for the first two tasks of the problem. The third task is computational and is addressed by the code in the answer block.\n\n#### Task 1: Derivation of the Jacobian Matrix\n\nThe system of ODEs is given by $\\frac{d\\mathbf{x}}{dt} = \\mathbf{F}(\\mathbf{x})$, where $\\mathbf{x} = [x, y, z]^T$ and:\n$$\nf_x(x,y,z) = -x + \\frac{\\alpha}{1 + (z/K)^n} \\\\\nf_y(x,y,z) = -y + \\frac{\\alpha}{1 + (x/K)^n} \\\\\nf_z(x,y,z) = -z + \\frac{\\alpha}{1 + (y/K)^n}\n$$\nThe Jacobian matrix $J$ has elements $J_{ij} = \\frac{\\partial f_i}{\\partial x_j}$. We compute the partial derivatives:\n- The diagonal elements are $\\frac{\\partial f_x}{\\partial x} = -1$, $\\frac{\\partial f_y}{\\partial y} = -1$, and $\\frac{\\partial f_z}{\\partial z} = -1$.\n- The off-diagonal elements capture the repressive interactions. For example, $\\frac{\\partial f_x}{\\partial z}$ is:\n$$\n\\frac{\\partial f_x}{\\partial z} = \\frac{\\partial}{\\partial z} \\left( \\alpha \\left[ 1 + \\left(\\frac{z}{K}\\right)^n \\right]^{-1} \\right) = -\\alpha \\left( 1 + \\left(\\frac{z}{K}\\right)^n \\right)^{-2} \\cdot n \\left(\\frac{z}{K}\\right)^{n-1} \\cdot \\frac{1}{K} = -\\frac{\\alpha n z^{n-1}}{K^n \\left( 1 + (z/K)^n \\right)^2}\n$$\nThe other derivatives are either zero (e.g., $\\frac{\\partial f_x}{\\partial y} = 0$) or follow from the cyclic symmetry of the system.\n\nAt the symmetric steady state $x^* = y^* = z^* = s$, all diagonal entries are $-1$. The non-zero off-diagonal terms are all equal. Let's denote this common value by $\\beta$:\n$$\n\\beta = J_{13} = J_{21} = J_{32} = -\\frac{\\alpha n s^{n-1}}{K^n \\left( 1 + (s/K)^n \\right)^2}\n$$\nWe can simplify $\\beta$ using the steady-state condition $s = \\frac{\\alpha}{1 + (s/K)^n}$, which implies $\\alpha = s(1+(s/K)^n)$. Substituting this:\n$$\n\\beta = -\\frac{s(1+(s/K)^n) \\cdot n s^{n-1}}{K^n (1 + (s/K)^n)^2} = -\\frac{n s^n}{K^n (1 + (s/K)^n)} = -\\frac{n (s/K)^n}{1 + (s/K)^n}\n$$\nThe Jacobian matrix at the steady state, $J^*$, is therefore:\n$$\nJ^* = \\begin{pmatrix} -1  0  \\beta \\\\ \\beta  -1  0 \\\\ 0  \\beta  -1 \\end{pmatrix}\n$$\n\n#### Task 2: Eigenvalue Interpretation and Hopf Bifurcation\n\nLocal stability is determined by the eigenvalues of $J^*$. We find them by solving the characteristic equation $\\det(J^* - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} -1-\\lambda  0  \\beta \\\\ \\beta  -1-\\lambda  0 \\\\ 0  \\beta  -1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(-1-\\lambda)((-1-\\lambda)^2 - 0) + \\beta(\\beta^2 - 0) = 0\n$$\n$$\n(-1-\\lambda)^3 + \\beta^3 = 0 \\implies (\\lambda+1)^3 = \\beta^3\n$$\nThe solutions for $\\lambda+1$ are the three cubic roots of $\\beta^3$, which are $\\beta$, $\\beta e^{i2\\pi/3}$, and $\\beta e^{i4\\pi/3}$. This gives the three eigenvalues:\n- $\\lambda_1 = \\beta - 1$\n- $\\lambda_2 = -1 + \\beta e^{i2\\pi/3} = -1 + \\beta(-\\frac{1}{2} + i\\frac{\\sqrt{3}}{2}) = -1 - \\frac{\\beta}{2} + i\\frac{|\\beta|\\sqrt{3}}{2}$ (since $\\beta  0$)\n- $\\lambda_3 = -1 + \\beta e^{i4\\pi/3} = -1 + \\beta(-\\frac{1}{2} - i\\frac{\\sqrt{3}}{2}) = -1 - \\frac{\\beta}{2} - i\\frac{|\\beta|\\sqrt{3}}{2}$\n\nSince all parameters are positive, $\\beta$ is always negative. Therefore, the first eigenvalue $\\lambda_1 = \\beta - 1$ is always real and negative, corresponding to a stable mode.\n\nThe other two eigenvalues, $\\lambda_2$ and $\\lambda_3$, form a complex conjugate pair. Their real part is $\\text{Re}(\\lambda_{2,3}) = -1 - \\frac{\\beta}{2}$. A Hopf bifurcation, leading to the onset of oscillations, occurs when this real part crosses from negative to positive. The system becomes unstable when:\n$$\n\\text{Re}(\\lambda_{2,3}) > 0 \\implies -1 - \\frac{\\beta}{2} > 0 \\implies -\\frac{\\beta}{2} > 1 \\implies \\beta  -2\n$$\nSubstituting the expression for $\\beta$ and letting $r = (s/K)^n$:\n$$\n-\\frac{n r}{1 + r}  -2 \\implies \\frac{n r}{1 + r} > 2\n$$\n$$\nnr > 2(1 + r) \\implies (n-2)r > 2\n$$\nThis is the analytical condition for the symmetric steady state to become unstable and give rise to oscillations. This condition can only be met if $n > 2$, meaning a sufficiently high cooperativity (steepness of the repression curve) is required for the repressilator to oscillate.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the stability of the repressilator symmetric steady state for a suite of test cases.\n    \"\"\"\n    # Test cases defined as tuples of (n, alpha, K)\n    test_cases = [\n        # (n, alpha, K)\n        (2.0, 100.0, 1.0),\n        (3.0, 4.0, 1.0),\n        (4.0, 2.0, 1.0),\n        (4.0, 4.0, 1.0),\n        (5.0, 1.0, 0.1),\n        (3.0, 1.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, alpha, K = case\n\n        # The condition for oscillatory instability is (n-2)r  2, where r = (s/K)^n.\n        # This is only possible if n  2. If n = 2, the system cannot oscillate.\n        if n = 2:\n            results.append(0)\n            continue\n\n        # Define the function whose root we need to find to determine the steady state.\n        # The equation for the steady state can be rewritten in terms of r = (s/K)^n as:\n        # K * r^(1/n) - alpha / (1 + r) = 0\n        def fixed_point_equation(r):\n            # To avoid potential domain errors for r near 0 with n  1\n            if r = 0:\n                return -alpha\n            return K * (r**(1/n)) - alpha / (1 + r)\n\n        # We need to find a bracketing interval [a, b] for the root r  0\n        # such that f(a) and f(b) have opposite signs.\n        # For a very small r (e.g., 1e-9), f(r) approaches -alpha, which is negative.\n        # So we need to find an upper bound b such that f(b)  0.\n        # The steady state s must be less than alpha. Thus, r = (s/K)^n  (alpha/K)^n.\n        # Let's check the value at r_upper = (alpha/K)^n.\n        # f(r_upper) = K * ((alpha/K)^n)^(1/n) - alpha / (1 + (alpha/K)^n)\n        #            = K * (alpha/K) - alpha / (1 + (alpha/K)^n)\n        #            = alpha * (1 - 1 / (1 + (alpha/K)^n))\n        #            = alpha * ((alpha/K)^n / (1 + (alpha/K)^n))  0.\n        # So, a root is guaranteed to exist in the interval (0, (alpha/K)^n).\n        bracket_a = 1e-9\n        bracket_b = (alpha / K)**n\n        \n        # It's possible for the bracket bounds to be extremely close for some parameters,\n        # so we'll add a small epsilon to the upper bound to ensure a valid interval.\n        if bracket_b = bracket_a:\n            bracket_b = bracket_a + 1.0\n\n        # Use a robust root-finding algorithm (Brent's method)\n        sol = root_scalar(fixed_point_equation, bracket=[bracket_a, bracket_b], method='brentq')\n        r_steady_state = sol.root\n\n        # Check the condition for oscillatory instability: (n - 2) * r  2.\n        # The problem asks for the real part to be strictly greater than zero,\n        # so we use a strict inequality.\n        if (n - 2) * r_steady_state  2:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}