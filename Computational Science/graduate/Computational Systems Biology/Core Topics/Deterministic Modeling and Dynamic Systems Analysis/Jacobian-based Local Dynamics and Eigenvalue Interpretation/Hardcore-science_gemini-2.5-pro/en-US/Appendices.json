{
    "hands_on_practices": [
        {
            "introduction": "While direct computation of eigenvalues provides definitive information about local stability, it can be analytically or computationally challenging for large systems. This exercise introduces a powerful estimation tool, the Gershgorin circle theorem, which allows us to define regions in the complex plane guaranteed to contain the eigenvalues. By applying this theorem, you will learn to rapidly assess the stability of a steady state by ensuring all its eigenvalues are confined to the left-half plane, a practical skill for analyzing complex biological networks .",
            "id": "3321860",
            "problem": "A deterministic, continuous-time, three-species regulatory network is modeled by a system of ordinary differential equations (ODEs), $\\frac{d\\mathbf{x}}{dt}=\\mathbf{f}(\\mathbf{x})$, where $\\mathbf{x}\\in\\mathbb{R}^{3}$ represents the species concentrations. At a steady state $\\mathbf{x}^{\\ast}$, the local dynamics are governed by the Jacobian matrix $J=\\left.\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}\\right|_{\\mathbf{x}^{\\ast}}$. For the network under consideration, the linearization yields\n$$\nJ=\\begin{pmatrix}\n-5  1  0\\\\\n2  -4  1\\\\\n0  3  -6\n\\end{pmatrix}.\n$$\nUsing only fundamental definitions of local linearization and the criterion that asymptotic local stability requires all eigenvalues of $J$ to have strictly negative real parts, and employing the Gershgorin circle theorem from linear algebra to bound the eigenvalues of $J$, determine the smallest real number $\\gamma$ such that the real part of every eigenvalue of $J$ is guaranteed to be less than or equal to $\\gamma$. Express your final answer as an exact real number. No rounding is required.",
            "solution": "The user wants to solve the problem by following these steps.\n1. The user will validate the problem statement.\n2. The user will solve the problem.\n3. The user will provide the solution in the specified format.\n\nThe user will follow the instructions to the letter.\n\n**1. Problem Validation**\n\n*   **Step 1: Extract Givens**\n    *   System of ODEs: $\\frac{d\\mathbf{x}}{dt}=\\mathbf{f}(\\mathbf{x})$, with $\\mathbf{x}\\in\\mathbb{R}^{3}$.\n    *   Jacobian matrix at steady state $\\mathbf{x}^{\\ast}$:\n        $$\n        J=\\begin{pmatrix}\n        -5  1  0\\\\\n        2  -4  1\\\\\n        0  3  -6\n        \\end{pmatrix}.\n        $$\n    *   Mandated method: Gershgorin circle theorem.\n    *   Objective: Determine the smallest real number $\\gamma$ such that for any eigenvalue $\\lambda$ of $J$, it is guaranteed that $\\mathrm{Re}(\\lambda) \\le \\gamma$.\n    *   Answer format: Exact real number.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   **Scientific or Factual Unsoundness**: The problem is scientifically and mathematically sound. It involves the standard stability analysis of a dynamical system using linearization and eigenvalue analysis, which are core concepts in physics, engineering, and systems biology. The specified methodology, the Gershgorin circle theorem, is a standard result in linear algebra.\n    *   **Non-Formalizable or Irrelevant**: The problem is a formal mathematical exercise directly relevant to the topic of *Jacobian-based local dynamics and eigenvalue interpretation*.\n    *   **Incomplete or Contradictory Setup**: The problem is self-contained. It provides a specific matrix and a specific theorem to use, and the objective is clearly defined.\n    *   **Unrealistic or Infeasible**: The Jacobian matrix contains simple integer values, which are entirely plausible for a biological model after parameterization and linearization. The calculations are feasible.\n    *   **Ill-Posed or Poorly Structured**: The problem is well-posed. The Gershgorin circle theorem provides a concrete method to find a guaranteed upper bound for the real parts of the eigenvalues, and the question asks for the best such bound achievable with this theorem. This leads to a unique answer.\n    *   **Pseudo-Profound, Trivial, or Tautological**: The problem is a standard application of a non-trivial theorem and requires correct calculation and logical inference. It is neither trivial nor artificially complex.\n    *   **Outside Scientific Verifiability**: The result is mathematically verifiable.\n\n*   **Step 3: Verdict and Action**\n    *   The problem is **valid**. The solution will be derived.\n\n**2. Solution Derivation**\n\nThe problem requires finding an upper bound on the real parts of the eigenvalues of the Jacobian matrix $J$ using the Gershgorin circle theorem. The theorem states that every eigenvalue of an $n \\times n$ matrix $A$ lies within at least one of the Gershgorin disks $D(A_{ii}, R_i)$ in the complex plane, where $A_{ii}$ is the $i$-th diagonal element and $R_i = \\sum_{j \\ne i} |A_{ij}|$ is the sum of the absolute values of the non-diagonal elements in the $i$-th row. An eigenvalue $\\lambda$ must satisfy $|\\lambda - A_{ii}| \\le R_i$ for some $i$.\n\nThe given Jacobian matrix is\n$$\nJ=\\begin{pmatrix}\n-5  1  0\\\\\n2  -4  1\\\\\n0  3  -6\n\\end{pmatrix}.\n$$\n\nFirst, we apply the Gershgorin circle theorem to the rows of $J$.\nThe centers of the disks are the diagonal entries: $J_{11} = -5$, $J_{22} = -4$, and $J_{33} = -6$.\nThe radii of the disks are calculated as follows:\n- For the first row ($i=1$): The radius is $R_1 = |1| + |0| = 1$. The disk is $D_1 = \\{z \\in \\mathbb{C} : |z - (-5)| \\le 1\\}$.\n- For the second row ($i=2$): The radius is $R_2 = |2| + |1| = 3$. The disk is $D_2 = \\{z \\in \\mathbb{C} : |z - (-4)| \\le 3\\}$.\n- For the third row ($i=3$): The radius is $R_3 = |0| + |3| = 3$. The disk is $D_3 = \\{z \\in \\mathbb{C} : |z - (-6)| \\le 3\\}$.\n\nThe theorem guarantees that all eigenvalues of $J$ are located in the union of these three disks, $G_R = D_1 \\cup D_2 \\cup D_3$. We are interested in the maximum possible real part of any eigenvalue. For a disk $D(c, R)$, the maximum real part of any point $z$ within it is $\\mathrm{Re}(c) + R$.\n- Maximum real part for $D_1$: $\\mathrm{Re}(-5) + 1 = -5 + 1 = -4$.\n- Maximum real part for $D_2$: $\\mathrm{Re}(-4) + 3 = -4 + 3 = -1$.\n- Maximum real part for $D_3$: $\\mathrm{Re}(-6) + 3 = -6 + 3 = -3$.\n\nThe maximum real part for any point in the union $G_R$ is the maximum of these values:\n$$\n\\sup_{z \\in G_R} \\mathrm{Re}(z) = \\max(-4, -1, -3) = -1.\n$$\nThus, from the row-based analysis, we can guarantee that for any eigenvalue $\\lambda$ of $J$, $\\mathrm{Re}(\\lambda) \\le -1$.\n\nThe eigenvalues of a matrix are the same as the eigenvalues of its transpose, $J^T$. Therefore, the Gershgorin circle theorem can also be applied to $J^T$. This is equivalent to using the column sums of the original matrix $J$ to define the radii.\n$$\nJ^T=\\begin{pmatrix}\n-5  2  0\\\\\n1  -4  3\\\\\n0  1  -6\n\\end{pmatrix}.\n$$\nThe centers of the disks are the same diagonal entries: $J_{11} = -5$, $J_{22} = -4$, and $J_{33} = -6$.\nThe radii based on the columns of $J$ (rows of $J^T$) are:\n- For the first column ($j=1$): The radius is $R'_1 = |2| + |0| = 2$. The disk is $D'_1 = \\{z \\in \\mathbb{C} : |z - (-5)| \\le 2\\}$.\n- For the second column ($j=2$): The radius is $R'_2 = |1| + |3| = 4$. The disk is $D'_2 = \\{z \\in \\mathbb{C} : |z - (-4)| \\le 4\\}$.\n- For the third column ($j=3$): The radius is $R'_3 = |0| + |1| = 1$. The disk is $D'_3 = \\{z \\in \\mathbb{C} : |z - (-6)| \\le 1\\}$.\n\nThe set of all eigenvalues is also contained in the union of these column-based disks, $G_C = D'_1 \\cup D'_2 \\cup D'_3$. We find the maximum real part for any point in this region.\n- Maximum real part for $D'_1$: $\\mathrm{Re}(-5) + 2 = -5 + 2 = -3$.\n- Maximum real part for $D'_2$: $\\mathrm{Re}(-4) + 4 = -4 + 4 = 0$.\n- Maximum real part for $D'_3$: $\\mathrm{Re}(-6) + 1 = -6 + 1 = -5$.\n\nThe maximum real part for any point in the union $G_C$ is:\n$$\n\\sup_{z \\in G_C} \\mathrm{Re}(z) = \\max(-3, 0, -5) = 0.\n$$\nThus, from the column-based analysis, we can guarantee that for any eigenvalue $\\lambda$ of $J$, $\\mathrm{Re}(\\lambda) \\le 0$.\n\nThe set of eigenvalues, $\\Lambda(J)$, must be contained in both the region $G_R$ and the region $G_C$. Therefore, $\\Lambda(J) \\subseteq G_R \\cap G_C$. This implies that for any eigenvalue $\\lambda$, its real part must satisfy both bounds simultaneously: $\\mathrm{Re}(\\lambda) \\le -1$ and $\\mathrm{Re}(\\lambda) \\le 0$. The stronger of these two conditions is $\\mathrm{Re}(\\lambda) \\le -1$.\nThe smallest real number $\\gamma$ such that the real part of every eigenvalue is guaranteed by the theorem to be less than or equal to $\\gamma$ is the tightest bound we can derive. This is given by the minimum of the two upper bounds we found:\n$$\n\\gamma = \\min\\left(\\sup_{z \\in G_R} \\mathrm{Re}(z), \\sup_{z \\in G_C} \\mathrm{Re}(z)\\right) = \\min(-1, 0) = -1.\n$$\nTherefore, the Gershgorin circle theorem guarantees that the real part of every eigenvalue of $J$ is less than or equal to $-1$.",
            "answer": "$$\n\\boxed{-1}\n$$"
        },
        {
            "introduction": "The eigenvalues of the Jacobian determine a system's long-term fate, but they do not tell the whole story about its transient behavior. This practice delves into the crucial concept of non-normal dynamics, where strong coupling in a network can cause significant short-term amplification of perturbations even when the system is asymptotically stable. By explicitly calculating the matrix exponential and analyzing its norm, you will uncover the mechanism behind this transient growth and appreciate why the journey to a steady state can be as biologically important as the destination itself .",
            "id": "3321850",
            "problem": "A two-species biochemical reaction network is linearized near a stable steady state to obtain the local dynamics of the deviation vector $x(t) \\in \\mathbb{R}^{2}$ governed by the linear ordinary differential equation $\\frac{d x}{d t} = J x$. The $2 \\times 2$ Jacobian matrix $J$ is\n$$\nJ \\;=\\; \\begin{pmatrix} -1  100 \\\\ 0  -1 \\end{pmatrix}.\n$$\nIn computational systems biology, the local flow map $x(t) = \\exp(J t) x(0)$ is determined by the matrix exponential $\\exp(J t)$, and the transient amplification of perturbations is quantified by the induced matrix $2$-norm $\\|\\exp(J t)\\|_{2}$, the largest singular value of $\\exp(J t)$.\n\nStarting only from the facts that (i) the solution of the linear system $\\frac{d x}{d t} = J x$ with initial condition $x(0)$ is $x(t) = \\exp(J t) x(0)$, and (ii) the spectral norm $\\|A\\|_{2}$ equals the largest singular value of $A$, perform the following:\n\n1. Compute $\\exp(J t)$ explicitly for all real $t \\ge 0$.\n2. Using first principles about singular values, derive an exact analytic expression for $\\|\\exp(J t)\\|_{2}$ as a function of $t$, and use it to demonstrate that there is transient growth, namely that there exists an interval of moderate $t  0$ for which $\\|\\exp(J t)\\|_{2}  1$ despite all eigenvalues of $J$ having negative real parts.\n3. Interpret mechanistically why the large off-diagonal entry $100$ leads to non-normal transient amplification in this two-species system.\n\nFinally, determine the exact closed-form expression (in elementary functions) of the supremum over $t \\ge 0$ of $\\|\\exp(J t)\\|_{2}$ for this specific matrix $J$, and report that single expression as your final answer. The amplification factor is dimensionless, so no unit is required. Do not round; provide the exact symbolic result.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- The system dynamics are described by the linear ordinary differential equation $\\frac{d x}{d t} = J x$ for a deviation vector $x(t) \\in \\mathbb{R}^{2}$.\n- The Jacobian matrix is given as $J = \\begin{pmatrix} -1  100 \\\\ 0  -1 \\end{pmatrix}$.\n- The solution to the system is the local flow map $x(t) = \\exp(J t) x(0)$.\n- The transient amplification is quantified by the induced matrix $2$-norm $\\|\\exp(J t)\\|_{2}$.\n- The definition of the spectral norm is provided: $\\|A\\|_{2}$ is the largest singular value of $A$.\n- The time variable is non-negative, $t \\ge 0$.\n- The tasks are: (1) Compute $\\exp(J t)$ explicitly. (2) Derive an analytic expression for $\\|\\exp(J t)\\|_{2}$ and demonstrate transient growth. (3) Provide a mechanistic interpretation for the transient amplification. (4) Determine the exact closed-form expression for $\\sup_{t \\ge 0} \\|\\exp(J t)\\|_{2}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on fundamental principles of linear systems theory, matrix analysis, and their application to linearized biochemical reaction networks. The concept of non-normal matrices leading to transient growth despite asymptotic stability is a well-established and important topic in computational systems biology and dynamical systems. The given Jacobian matrix represents a simple, physically plausible interaction motif (e.g., a species that decays but is strongly produced by another decaying species).\n- **Well-Posed:** All required information is provided. The matrix $J$ is fully specified, and the tasks are standard mathematical computations that lead to a unique, well-defined solution.\n- **Objective:** The problem is stated using precise mathematical language, free from subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is mathematically sound, scientifically relevant, and well-posed. A complete solution will be provided.\n\n### Solution\n\nThe solution proceeds by addressing the four required components of the problem.\n\n**1. Computation of the Matrix Exponential $\\exp(J t)$**\n\nThe Jacobian matrix is $J = \\begin{pmatrix} -1  100 \\\\ 0  -1 \\end{pmatrix}$. We can decompose $J$ into the sum of a diagonal matrix $D$ and a nilpotent matrix $N$.\nLet $D = -1 \\cdot I = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix}$ and $N = \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix}$.\nThen $J = D + N$. We must check if $D$ and $N$ commute:\n$$DN = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  -100 \\\\ 0  0 \\end{pmatrix}$$\n$$ND = \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix} = \\begin{pmatrix} 0  -100 \\\\ 0  0 \\end{pmatrix}$$\nSince $DN = ND$, we can use the property $\\exp(J t) = \\exp((D+N)t) = \\exp(Dt)\\exp(Nt)$.\n\nThe exponential of the diagonal part is:\n$$\\exp(Dt) = \\exp(-It) = \\exp(-t) I = \\begin{pmatrix} \\exp(-t)  0 \\\\ 0  \\exp(-t) \\end{pmatrix}$$\n\nFor the nilpotent part, we compute its powers:\n$$N^2 = \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$$\nSince $N^2$ is the zero matrix, all higher powers $N^k$ for $k \\ge 2$ are also zero. The Taylor series for $\\exp(Nt)$ thus truncates:\n$$\\exp(Nt) = I + Nt + \\frac{(Nt)^2}{2!} + \\dots = I + Nt = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + t\\begin{pmatrix} 0  100 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  100t \\\\ 0  1 \\end{pmatrix}$$\n\nCombining these results, we get the explicit form of $\\exp(Jt)$:\n$$\\exp(Jt) = \\exp(Dt)\\exp(Nt) = \\begin{pmatrix} \\exp(-t)  0 \\\\ 0  \\exp(-t) \\end{pmatrix} \\begin{pmatrix} 1  100t \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\exp(-t)  100t \\exp(-t) \\\\ 0  \\exp(-t) \\end{pmatrix}$$\n\n**2. Derivation of $\\|\\exp(J t)\\|_{2}$ and Demonstration of Transient Growth**\n\nThe norm $\\|\\exp(J t)\\|_{2}$ is the largest singular value of $\\exp(J t)$, which is given by $\\sigma_{\\max} = \\sqrt{\\lambda_{\\max}(A^T A)}$, where $A = \\exp(J t)$.\nFirst, we compute the product $A^T A$:\n$$A = \\exp(-t)\\begin{pmatrix} 1  100t \\\\ 0  1 \\end{pmatrix}$$\n$$A^T A = \\left( \\exp(-t)\\begin{pmatrix} 1  0 \\\\ 100t  1 \\end{pmatrix} \\right) \\left( \\exp(-t)\\begin{pmatrix} 1  100t \\\\ 0  1 \\end{pmatrix} \\right) = \\exp(-2t)\\begin{pmatrix} 1  100t \\\\ 100t  1+(100t)^2 \\end{pmatrix}$$\nTo find the eigenvalues of $A^T A$, we find the eigenvalues of the matrix $M = \\begin{pmatrix} 1  100t \\\\ 100t  1+(100t)^2 \\end{pmatrix}$. The characteristic equation is $\\det(M-\\lambda I) = 0$:\n$$(1-\\lambda)(1+(100t)^2-\\lambda) - (100t)^2 = 0$$\n$$\\lambda^2 - (2+(100t)^2)\\lambda + (1+(100t)^2) - (100t)^2 = 0$$\n$$\\lambda^2 - (2+(100t)^2)\\lambda + 1 = 0$$\nThe largest eigenvalue of $M$ is given by the quadratic formula:\n$$\\lambda_{\\max}(M) = \\frac{2+(100t)^2 + \\sqrt{(2+(100t)^2)^2 - 4}}{2} = \\frac{2+(100t)^2 + \\sqrt{(100t)^4 + 4(100t)^2}}{2}$$\n$$\\lambda_{\\max}(M) = \\frac{2+(100t)^2 + 100t\\sqrt{(100t)^2+4}}{2}$$\nThe largest eigenvalue of $A^T A$ is $\\lambda_{\\max}(A^T A) = \\exp(-2t)\\lambda_{\\max}(M)$. The norm is its square root:\n$$\\|\\exp(J t)\\|_{2} = \\sqrt{\\exp(-2t)\\lambda_{\\max}(M)} = \\exp(-t)\\sqrt{\\frac{2+(100t)^2 + 100t\\sqrt{(100t)^2+4}}{2}}$$\nTo demonstrate transient growth, we show that the norm grows from its initial value of $1$. At $t=0$, $\\|\\exp(J \\cdot 0)\\|_{2} = \\|I\\|_{2} = 1$. The initial growth rate of the norm $\\|\\exp(Jt)\\|_2$ at $t=0$ is given by the largest eigenvalue of the symmetric part of $J$.\nThe symmetric part of $J$ is $J_S = \\frac{1}{2}(J+J^T) = \\frac{1}{2}\\begin{pmatrix} -2  100 \\\\ 100  -2 \\end{pmatrix} = \\begin{pmatrix} -1  50 \\\\ 50  -1 \\end{pmatrix}$.\nThe eigenvalues of $J_S$ are given by $(-1-\\lambda)^2 - 50^2 = 0$, so $\\lambda+1 = \\pm 50$, which gives $\\lambda_1=49$ and $\\lambda_2=-51$. The largest eigenvalue of $J_S$, which is the initial growth rate of the norm, is $49$. Since this is positive, $\\|\\exp(Jt)\\|_2$ initially increases from $1$, demonstrating transient growth.\nThe eigenvalues of $J$ are $\\lambda_1 = \\lambda_2 = -1$. Since all eigenvalues have negative real parts, the system is asymptotically stable, i.e., $\\lim_{t\\to\\infty} \\|\\exp(Jt)\\|_2 = 0$. The initial growth followed by eventual decay is a defining characteristic of stable non-normal systems. The non-normality is confirmed by $JJ^T \\neq J^TJ$.\n\n**3. Mechanistic Interpretation**\n\nThe system of differential equations is:\n$$\\frac{dx_1}{dt} = -x_1 + 100x_2$$\n$$\\frac{dx_2}{dt} = -x_2$$\nSpecies $1$ ($x_1$) and species $2$ ($x_2$) both undergo first-order degradation with rate constant $1$. However, species $2$ acts as a strong activator for species $1$, with a large coupling constant of $100$.\nThe solution for $x_2(t)$ is a simple exponential decay: $x_2(t) = x_2(0)\\exp(-t)$.\nSubstituting this into the equation for $x_1$ gives: $\\frac{dx_1}{dt} + x_1 = 100 x_2(0) \\exp(-t)$.\nThe solution is $x_1(t) = (x_1(0) + 100 x_2(0) t)\\exp(-t)$.\nConsider an initial perturbation solely in species $2$: $x(0) = \\begin{pmatrix} 0 \\\\ \\epsilon \\end{pmatrix}$. The state at time $t$ is $x(t) = \\begin{pmatrix} 100 \\epsilon t \\exp(-t) \\\\ \\epsilon \\exp(-t) \\end{pmatrix}$.\nThe norm of this state vector, $\\|x(t)\\|_2 = \\epsilon \\exp(-t) \\sqrt{(100t)^2+1}$, can become much larger than the initial norm $\\|x(0)\\|_2 = \\epsilon$. The term $100t\\exp(-t)$ shows that $x_1$'s concentration is initially amplified linearly in time with a large factor of $100$. This \"pumping\" of $x_1$ by $x_2$ leads to a large transient excursion of the system state before the exponential decay term $\\exp(-t)$ dominates at large $t$ and brings the system back to the steady state at the origin. This mechanism, where one state variable is amplified by another that is simultaneously decaying, is characteristic of amplification in non-normal systems with a \"shear-like\" flow.\n\n**4. Supremum of $\\|\\exp(J t)\\|_{2}$**\n\nTo find the maximum value of $\\|\\exp(J t)\\|_{2}$, we simplify its expression. Let $u = 100t$. We use the hyperbolic substitution $u = 2\\sinh(\\theta)$, so $t = \\frac{1}{50}\\sinh(\\theta)$ and $\\theta = \\arcsinh(50t)$.\nThe expression for $\\lambda_{\\max}(M)$ simplifies greatly under this substitution:\n$$100t\\sqrt{(100t)^2+4} = 2\\sinh(\\theta)\\sqrt{4\\sinh^2(\\theta)+4} = 2\\sinh(\\theta)(2\\cosh(\\theta)) = 2\\sinh(2\\theta)$$\n$$(100t)^2 = 4\\sinh^2(\\theta) = 2(\\cosh(2\\theta)-1)$$\n$$\\lambda_{\\max}(M) = \\frac{2 + 2(\\cosh(2\\theta)-1) + 2\\sinh(2\\theta)}{2} = \\cosh(2\\theta) + \\sinh(2\\theta) = \\exp(2\\theta)$$\nNow, substituting this back into the norm expression:\n$$\\|\\exp(J t)\\|_{2} = \\exp(-t)\\sqrt{\\exp(2\\theta)} = \\exp(-t)\\exp(\\theta) = \\exp(\\theta-t)$$\nSubstituting the definitions of $\\theta$ and $t$:\n$$F(t) = \\|\\exp(Jt)\\|_2 = \\exp(\\arcsinh(50t) - t)$$\nTo find the maximum, we can maximize the exponent $G(t) = \\arcsinh(50t) - t$. We find the critical point by setting the derivative to zero:\n$$G'(t) = \\frac{d}{dt}(\\arcsinh(50t)) - 1 = \\frac{50}{\\sqrt{(50t)^2+1}} - 1$$\nSetting $G'(t)=0$ gives:\n$$\\frac{50}{\\sqrt{(50t)^2+1}} = 1 \\implies 50 = \\sqrt{2500t^2+1}$$\n$$2500 = 2500t^2+1 \\implies 2500t^2 = 2499 \\implies t^2 = \\frac{2499}{2500}$$\nThe time of maximum amplification is $t_{\\max} = \\sqrt{\\frac{2499}{2500}} = \\frac{\\sqrt{2499}}{50}$. We can simplify $\\sqrt{2499} = \\sqrt{3 \\cdot 833} = \\sqrt{3 \\cdot 7 \\cdot 119} = \\sqrt{3 \\cdot 7 \\cdot 7 \\cdot 17} = 7\\sqrt{51}$.\nSo, $t_{\\max} = \\frac{7\\sqrt{51}}{50}$.\nThe maximum value is obtained by evaluating $F(t_{\\max})$:\n$$\\sup_{t \\ge 0} \\|\\exp(J t)\\|_{2} = F(t_{\\max}) = \\exp(\\arcsinh(50 t_{\\max}) - t_{\\max})$$\nAt $t_{\\max}$, we have $50t_{\\max} = 7\\sqrt{51}$. So we need to evaluate $\\arcsinh(7\\sqrt{51})$.\nUsing the logarithmic identity $\\arcsinh(z) = \\ln(z+\\sqrt{z^2+1})$:\n$$\\arcsinh(7\\sqrt{51}) = \\ln(7\\sqrt{51} + \\sqrt{(7\\sqrt{51})^2+1}) = \\ln(7\\sqrt{51} + \\sqrt{49 \\cdot 51+1}) = \\ln(7\\sqrt{51} + \\sqrt{2499+1}) = \\ln(7\\sqrt{51} + \\sqrt{2500}) = \\ln(50+7\\sqrt{51})$$\nThe exponent is therefore $\\ln(50+7\\sqrt{51}) - \\frac{7\\sqrt{51}}{50}$.\nThe supremum is:\n$$\\exp\\left(\\ln(50+7\\sqrt{51}) - \\frac{7\\sqrt{51}}{50}\\right) = (50+7\\sqrt{51})\\exp\\left(-\\frac{7\\sqrt{51}}{50}\\right)$$\nThis is the final exact expression for the maximum amplification factor.",
            "answer": "$$\n\\boxed{(50+7\\sqrt{51})\\exp\\left(-\\frac{7\\sqrt{51}}{50}\\right)}\n$$"
        },
        {
            "introduction": "A profound application of Jacobian analysis is not just to classify a system's stability, but to predict how its fundamental behavior changes as its underlying parameters are varied. This capstone exercise focuses on the repressilator, a landmark synthetic gene circuit, to demonstrate how local stability analysis can predict the emergence of complex behavior. You will derive the conditions for a Hopf bifurcation, where a stable steady state gives way to sustained oscillations as eigenvalues cross the imaginary axis, a foundational concept in understanding biological rhythms and designing synthetic oscillators .",
            "id": "3321845",
            "problem": "Consider the dimensionless three-gene repressilator model, a canonical synthetic gene circuit consisting of three transcriptional repressors arranged in a ring. Let $x(t)$, $y(t)$, and $z(t)$ denote the concentrations of the three gene products. The dynamics are modeled by the system of ordinary differential equations\n$$\n\\frac{dx}{dt} = -x + \\frac{\\alpha}{1 + \\left(\\frac{z}{K}\\right)^n}, \\quad\n\\frac{dy}{dt} = -y + \\frac{\\alpha}{1 + \\left(\\frac{x}{K}\\right)^n}, \\quad\n\\frac{dz}{dt} = -z + \\frac{\\alpha}{1 + \\left(\\frac{y}{K}\\right)^n},\n$$\nwhere $n \\ge 1$ is the Hill coefficient (dimensionless), $\\alpha  0$ is the maximal production rate (dimensionless), and $K  0$ is the repression threshold (dimensionless). Degradation rates are scaled to $1$ for all species.\n\nTasks:\n1. Starting from the definition of local linearization via the Jacobian matrix, consider the symmetric steady state $x^* = y^* = z^* = s$ defined implicitly by the fixed point condition $s = \\frac{\\alpha}{1 + \\left(\\frac{s}{K}\\right)^n}$. Derive the Jacobian matrix $J$ at this symmetric steady state in terms of $s$, $n$, $\\alpha$, and $K$, by computing the partial derivatives of the right-hand side with respect to the state variables and evaluating them at $x=y=z=s$.\n2. Using first principles for interpreting eigenvalues of the Jacobian matrix, explain how the sign of the real parts of eigenvalues determines local stability. Identify the conditions under which a complex conjugate pair of eigenvalues crosses the imaginary axis, indicating a Hopf bifurcation and the onset of sustained oscillations.\n3. Write a complete, runnable program that, for a given set of parameters $(n,\\alpha,K)$, performs the following steps:\n   - Numerically computes the symmetric steady state by solving for $s$ (or equivalently for $r = \\left(\\frac{s}{K}\\right)^n$) so that the fixed point condition is satisfied.\n   - Constructs the Jacobian matrix $J$ at $x=y=z=s$ using the computed derivatives.\n   - Computes the eigenvalues of $J$ and determines whether a complex conjugate pair has positive real part (strictly greater than zero), which indicates that the system is in an oscillatory regime associated with a Hopf bifurcation.\n   - Outputs, for each test case, an integer $1$ if the oscillatory regime is predicted (complex conjugate pair with positive real part) and $0$ otherwise (including the boundary case where the real part equals zero).\nAll variables and parameters are dimensionless; no physical units are required. Angles are not involved.\n\nTest suite:\nUse the following parameter sets $(n,\\alpha,K)$ to evaluate your program. These cover a general case, boundary conditions, and edge cases:\n- $(2, 100.0, 1.0)$\n- $(3, 4.0, 1.0)$\n- $(4, 2.0, 1.0)$\n- $(4, 4.0, 1.0)$\n- $(5, 1.0, 0.1)$\n- $(3, 1.0, 1.0)$\n\nFinal output format:\nYour program should produce a single line containing the results as a comma-separated list enclosed in square brackets. For example, for six test cases, the output must be of the form\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]\n$$\nwhere each $\\text{result}_i$ is the integer $0$ or $1$ as specified above.",
            "solution": "The problem is subjected to validation before proceeding with a solution.\n\n### Step 1: Extract Givens\n\nThe provided information consists of:\n- **System of dimensionless ordinary differential equations (ODEs):**\n$$\n\\frac{dx}{dt} = -x + \\frac{\\alpha}{1 + \\left(\\frac{z}{K}\\right)^n} \\\\\n\\frac{dy}{dt} = -y + \\frac{\\alpha}{1 + \\left(\\frac{x}{K}\\right)^n} \\\\\n\\frac{dz}{dt} = -z + \\frac{\\alpha}{1 + \\left(\\frac{y}{K}\\right)^n}\n$$\n- **State variables:** $x(t)$, $y(t)$, $z(t)$ representing concentrations.\n- **Parameters:**\n  - $n \\ge 1$: Hill coefficient.\n  - $\\alpha  0$: maximal production rate.\n  - $K  0$: repression threshold.\n- **Symmetric steady state:** A fixed point where $x^* = y^* = z^* = s$.\n- **Fixed point condition:** The steady state value $s$ is implicitly defined by the equation $s = \\frac{\\alpha}{1 + \\left(\\frac{s}{K}\\right)^n}$.\n- **Task 1:** Derive the Jacobian matrix $J$ at the symmetric steady state $s$ in terms of $s$, $n$, $\\alpha$, and $K$.\n- **Task 2:** Explain the relationship between the eigenvalues of the Jacobian and local stability, and identify the conditions for a Hopf bifurcation.\n- **Task 3:** Write a program to numerically compute the steady state, construct the Jacobian, compute its eigenvalues, and determine if the system is in an oscillatory regime by checking if a complex conjugate pair of eigenvalues has a strictly positive real part. The output should be $1$ for an oscillatory state and $0$ otherwise.\n- **Test suite:** A list of six parameter sets $(n, \\alpha, K)$: $(2, 100.0, 1.0)$, $(3, 4.0, 1.0)$, $(4, 2.0, 1.0)$, $(4, 4.0, 1.0)$, $(5, 1.0, 0.1)$, $(3, 1.0, 1.0)$.\n- **Final output format:** A single line with a comma-separated list of results, e.g., $[\\text{result}_1,\\dots,\\text{result}_6]$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientific Groundedness:** The problem is based on the repressilator model, a foundational and widely studied system in synthetic and systems biology. The equations use standard Hill kinetics to model transcriptional repression. The analysis of local stability via the Jacobian matrix is a canonical method in dynamical systems theory. The model and methods are scientifically sound.\n- **Well-Posedness:** The problem is well-posed. It requests specific, sequential tasks: analytical derivation, theoretical explanation, and numerical implementation. The equation for the steady state has a unique positive solution, and the subsequent analysis is deterministic. The tasks lead to a unique and meaningful outcome for each set of parameters.\n- **Objectivity:** The problem is stated using precise mathematical and scientific terminology, free from subjective or ambiguous language.\n- **Completeness and Consistency:** All necessary information (equations, definitions, parameters, tasks, and test cases) is provided. There are no missing data or internal contradictions.\n- **Feasibility:** The tasks are computationally and analytically feasible. The parameters are dimensionless, precluding physical inconsistencies.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid, as it is scientifically grounded, well-posed, objective, complete, and feasible. A complete solution will be provided.\n\n### Solution\n\nThe analysis of the repressilator model's local dynamics around its symmetric steady state involves linearization of the system, which is achieved through the Jacobian matrix. The eigenvalues of this matrix determine the stability of the steady state.\n\n#### Task 1: Derivation of the Jacobian Matrix\n\nThe system of ODEs can be written in vector form $\\frac{d\\mathbf{x}}{dt} = \\mathbf{F}(\\mathbf{x})$, where $\\mathbf{x} = [x, y, z]^T$ and $\\mathbf{F}(\\mathbf{x}) = [f_x(x,y,z), f_y(x,y,z), f_z(x,y,z)]^T$ with:\n$$\nf_x(x,y,z) = -x + \\frac{\\alpha}{1 + (z/K)^n} \\\\\nf_y(x,y,z) = -y + \\frac{\\alpha}{1 + (x/K)^n} \\\\\nf_z(x,y,z) = -z + \\frac{\\alpha}{1 + (y/K)^n}\n$$\nThe Jacobian matrix $J$ is a matrix of all first-order partial derivatives of the vector function $\\mathbf{F}$:\n$$\nJ = \\begin{pmatrix}\n\\frac{\\partial f_x}{\\partial x}  \\frac{\\partial f_x}{\\partial y}  \\frac{\\partial f_x}{\\partial z} \\\\\n\\frac{\\partial f_y}{\\partial x}  \\frac{\\partial f_y}{\\partial y}  \\frac{\\partial f_y}{\\partial z} \\\\\n\\frac{\\partial f_z}{\\partial x}  \\frac{\\partial f_z}{\\partial y}  \\frac{\\partial f_z}{\\partial z}\n\\end{pmatrix}\n$$\nWe compute the individual partial derivatives:\n- The diagonal elements are straightforward: $\\frac{\\partial f_x}{\\partial x} = -1$, $\\frac{\\partial f_y}{\\partial y} = -1$, $\\frac{\\partial f_z}{\\partial z} = -1$.\n- The off-diagonal elements capture the repressive interactions. For example, $\\frac{\\partial f_x}{\\partial z}$ is:\n$$\n\\frac{\\partial f_x}{\\partial z} = \\frac{\\partial}{\\partial z} \\left( \\alpha \\left[ 1 + \\left(\\frac{z}{K}\\right)^n \\right]^{-1} \\right) = -\\alpha \\left( 1 + \\left(\\frac{z}{K}\\right)^n \\right)^{-2} \\cdot n \\left(\\frac{z}{K}\\right)^{n-1} \\cdot \\frac{1}{K} = -\\frac{\\alpha n z^{n-1}}{K^n \\left( 1 + (z/K)^n \\right)^2}\n$$\nThe other derivatives are either zero (e.g., $\\frac{\\partial f_x}{\\partial y} = 0$) or follow from the cyclic symmetry of the system:\n$$\n\\frac{\\partial f_y}{\\partial x} = -\\frac{\\alpha n x^{n-1}}{K^n \\left( 1 + (x/K)^n \\right)^2}, \\quad \\frac{\\partial f_z}{\\partial y} = -\\frac{\\alpha n y^{n-1}}{K^n \\left( 1 + (y/K)^n \\right)^2}\n$$\nNow, we evaluate this matrix at the symmetric steady state $x = y = z = s$. The elements become:\n$$\nJ_{11} = J_{22} = J_{33} = -1 \\\\\nJ_{12} = J_{23} = J_{31} = 0\n$$\nThe remaining elements are all equal due to symmetry. Let's denote this common value by $\\beta$:\n$$\n\\beta = J_{13} = J_{21} = J_{32} = -\\frac{\\alpha n s^{n-1}}{K^n \\left( 1 + (s/K)^n \\right)^2}\n$$\nThe Jacobian matrix at the steady state, $J^*$, is:\n$$\nJ^* = \\begin{pmatrix} -1  0  \\beta \\\\ \\beta  -1  0 \\\\ 0  \\beta  -1 \\end{pmatrix}\n$$\nWe can simplify the expression for $\\beta$ by using the steady-state condition $s = \\frac{\\alpha}{1 + (s/K)^n}$. This gives $\\alpha = s(1+(s/K)^n)$. Substituting this into the expression for $\\beta$:\n$$\n\\beta = -\\frac{s(1+(s/K)^n) \\cdot n s^{n-1}}{K^n (1 + (s/K)^n)^2} = -\\frac{n s^n}{K^n (1 + (s/K)^n)} = -\\frac{n (s/K)^n}{1 + (s/K)^n}\n$$\nThis is the final expression for the off-diagonal terms of the Jacobian in terms of $s$, $n$, and $K$.\n\n#### Task 2: Eigenvalue Interpretation and Hopf Bifurcation\n\nAccording to the Hartman-Grobman theorem, the behavior of a nonlinear dynamical system near a hyperbolic fixed point is qualitatively the same as the behavior of its linearization at that point. The linearization is given by the system $\\frac{d(\\delta\\mathbf{x})}{dt} = J^* \\delta\\mathbf{x}$, where $\\delta\\mathbf{x}$ is a small perturbation from the fixed point. The solutions of this linear system are governed by the eigenvalues $\\lambda_i$ of the Jacobian matrix $J^*$.\n\n- If the real parts of all eigenvalues are negative, $\\text{Re}(\\lambda_i)  0$ for all $i$, any small perturbation will decay exponentially, and the fixed point is locally asymptotically stable.\n- If the real part of at least one eigenvalue is positive, $\\text{Re}(\\lambda_j)  0$ for some $j$, perturbations along the corresponding eigenvector direction will grow, rendering the fixed point unstable.\n- If at least one eigenvalue has a zero real part, while all others have negative real parts, the fixed point is non-hyperbolic. This is a critical point where the system's stability can change, a phenomenon known as a bifurcation.\n\nA Hopf bifurcation occurs when a pair of complex conjugate eigenvalues crosses the imaginary axis as a system parameter is varied. Specifically, if a pair of eigenvalues is $\\lambda = \\mu \\pm i\\omega$ with $\\omega \\neq 0$, the bifurcation occurs when the real part $\\mu$ passes through zero. If $\\mu$ changes from negative to positive, the stable fixed point loses its stability and typically gives rise to a stable limit cycle, which corresponds to sustained oscillations in the system.\n\nTo find the condition for a Hopf bifurcation in the repressilator model, we compute the eigenvalues of $J^*$. The characteristic equation is $\\det(J^* - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} -1-\\lambda  0  \\beta \\\\ \\beta  -1-\\lambda  0 \\\\ 0  \\beta  -1-\\lambda \\end{pmatrix} = (-1-\\lambda)\\left((-1-\\lambda)^2-0\\right) - 0 + \\beta(\\beta^2-0) = 0\n$$\n$$\n(-1-\\lambda)^3 + \\beta^3 = 0 \\implies (\\lambda+1)^3 = \\beta^3\n$$\nThe three solutions for $\\lambda+1$ are the cubic roots of $\\beta^3$, which are $\\beta e^{i0}$, $\\beta e^{i2\\pi/3}$, and $\\beta e^{i4\\pi/3}$. Recalling that $e^{i\\theta} = \\cos\\theta + i\\sin\\theta$, we find the three eigenvalues:\n- $\\lambda_1+1 = \\beta \\implies \\lambda_1 = \\beta - 1$\n- $\\lambda_2+1 = \\beta(-\\frac{1}{2} + i\\frac{\\sqrt{3}}{2}) \\implies \\lambda_2 = -1 - \\frac{\\beta}{2} + i\\frac{\\beta\\sqrt{3}}{2}$\n- $\\lambda_3+1 = \\beta(-\\frac{1}{2} - i\\frac{\\sqrt{3}}{2}) \\implies \\lambda_3 = -1 - \\frac{\\beta}{2} - i\\frac{\\beta\\sqrt{3}}{2}$\n\nSince $s, K  0$ and $n \\ge 1$, the term $(s/K)^n$ is positive. Thus, $\\beta = -n\\frac{(s/K)^n}{1+(s/K)^n}$ is always negative.\n- The real part of the first eigenvalue, $\\text{Re}(\\lambda_1) = \\beta - 1$, is therefore always negative. This eigenvalue is always stabilizing.\n- The other two eigenvalues, $\\lambda_2$ and $\\lambda_3$, form a complex conjugate pair. Their real part is $\\text{Re}(\\lambda_{2,3}) = -1 - \\frac{\\beta}{2}$.\n\nThe system becomes unstable and enters an oscillatory regime when this real part becomes positive:\n$$\n-1 - \\frac{\\beta}{2}  0 \\implies -\\frac{\\beta}{2}  1 \\implies \\beta  -2\n$$\nSubstituting the expression for $\\beta$:\n$$\n-\\frac{n (s/K)^n}{1 + (s/K)^n}  -2 \\implies \\frac{n (s/K)^n}{1 + (s/K)^n}  2\n$$\n$$\nn(s/K)^n  2(1 + (s/K)^n) \\implies (n-2)(s/K)^n  2\n$$\nThis is the condition for oscillatory instability. Note that this requires $n-2  0$, so oscillations are only possible for a Hill coefficient $n  2$.\n\n#### Task 3: Computational Implementation\n\nThe program must implement the following logic for each parameter set $(n, \\alpha, K)$:\n1.  Numerically solve for the symmetric steady state. It is convenient to solve for the dimensionless ratio $r = (s/K)^n$. Substituting $s = K r^{1/n}$ into the steady-state equation yields $K r^{1/n} = \\frac{\\alpha}{1+r}$. This is equivalent to finding the root of the function $g(r) = K r^{1/n} - \\frac{\\alpha}{1+r} = 0$ for $r  0$. This equation has a unique positive root.\n2.  Once the value of $r$ is found, evaluate the instability condition derived in Task 2: $(n-2)r  2$.\n3.  If the condition is met, the system is in an oscillatory regime, and the output is $1$. Otherwise, including the boundary case $(n-2)r = 2$ where the real part of the eigenvalues is zero, the output is $0$.\nThis procedure will be coded in Python using `scipy.optimize.root_scalar` for numerical root-finding.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the stability of the repressilator symmetric steady state for a suite of test cases.\n    \"\"\"\n    # Test cases defined as tuples of (n, alpha, K)\n    test_cases = [\n        # (n, alpha, K)\n        (2.0, 100.0, 1.0),\n        (3.0, 4.0, 1.0),\n        (4.0, 2.0, 1.0),\n        (4.0, 4.0, 1.0),\n        (5.0, 1.0, 0.1),\n        (3.0, 1.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, alpha, K = case\n\n        # The condition for oscillatory instability is (n-2)r  2, where r = (s/K)^n.\n        # This is only possible if n  2. If n = 2, the system cannot oscillate.\n        if n = 2:\n            results.append(0)\n            continue\n\n        # Define the function whose root we need to find to determine the steady state.\n        # The equation for the steady state can be rewritten in terms of r = (s/K)^n as:\n        # K * r^(1/n) - alpha / (1 + r) = 0\n        def fixed_point_equation(r):\n            # To avoid potential domain errors for r near 0 with n  1\n            if r = 0:\n                return -alpha\n            return K * (r**(1/n)) - alpha / (1 + r)\n\n        # We need to find a bracketing interval [a, b] for the root r  0\n        # such that f(a) and f(b) have opposite signs.\n        # For a very small r (e.g., 1e-9), f(r) approaches -alpha, which is negative.\n        # So we need to find an upper bound b such that f(b)  0.\n        # The steady state s must be less than alpha. Thus, r = (s/K)^n  (alpha/K)^n.\n        # Let's check the value at r_upper = (alpha/K)^n.\n        # f(r_upper) = K * ((alpha/K)^n)^(1/n) - alpha / (1 + (alpha/K)^n)\n        #            = K * (alpha/K) - alpha / (1 + (alpha/K)^n)\n        #            = alpha * (1 - 1 / (1 + (alpha/K)^n))\n        #            = alpha * ((alpha/K)^n / (1 + (alpha/K)^n))  0.\n        # So, a root is guaranteed to exist in the interval (0, (alpha/K)^n).\n        bracket_a = 1e-9\n        bracket_b = (alpha / K)**n\n        \n        # It's possible for the bracket bounds to be extremely close for some parameters,\n        # so we'll add a small epsilon to the upper bound to ensure a valid interval.\n        if bracket_b = bracket_a:\n            bracket_b = bracket_a + 1.0\n\n        # Use a robust root-finding algorithm (Brent's method)\n        sol = root_scalar(fixed_point_equation, bracket=[bracket_a, bracket_b], method='brentq')\n        r_steady_state = sol.root\n\n        # Check the condition for oscillatory instability: (n - 2) * r  2.\n        # The problem asks for the real part to be strictly greater than zero,\n        # so we use a strict inequality.\n        if (n - 2) * r_steady_state  2:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}