{
    "hands_on_practices": [
        {
            "introduction": "Understanding the long-term behavior of a biological network often begins with identifying its steady states and assessing their stability. This exercise provides foundational practice in this core task, starting from a realistic enzymatic network. You will apply conservation laws—a common feature in biochemical systems—to simplify the model before performing a linear stability analysis to determine the conditions under which the system settles to a stable equilibrium .",
            "id": "3351259",
            "problem": "Consider the following well-mixed enzymatic interconversion network modeled by mass-action kinetics. A substrate $S$ binds to an enzyme $E$ to form a complex $C$, which catalyzes the conversion of $S$ to a product $P$ and releases $E$. The product $P$ is converted back to $S$ through a separate process. The reaction scheme is:\n$$\nS + E \\xrightleftharpoons[k_{2}]{k_{1}} C, \\quad C \\xrightarrow{k_{3}} P + E, \\quad P \\xrightarrow{k_{4}} S,\n$$\nwhere $k_{1}$, $k_{2}$, $k_{3}$, and $k_{4}$ are positive rate constants. Let the state variables be the concentrations $S(t)$, $P(t)$, $E(t)$, and $C(t)$, which evolve according to ordinary differential equations (ODEs) derived from mass-action kinetics. Two independent conservation laws hold in this closed system:\n$$\nS_{T} = S + P + C, \\quad E_{T} = E + C,\n$$\nwhere $S_{T}$ and $E_{T}$ are positive constants representing the total substrate moieties and total enzyme moieties, respectively. Starting from the mass-action ODEs and these two conservation laws, eliminate two variables to obtain a reduced-order two-dimensional ODE system in the coordinates $(S, C)$. Define a steady state $(S^{*}, C^{*})$ as the point in reduced coordinates where both reduced ODE right-hand sides vanish. In computational systems biology, assessing local stability of a steady state is performed by linearizing the reduced dynamics and examining the Jacobian matrix at $(S^{*}, C^{*})$. \n\nTasks:\n- Derive the reduced-order ODE system in the variables $(S, C)$ using the conservation laws to eliminate $P$ and $E$.\n- From first principles, construct the Jacobian matrix of the reduced system at a generic steady state $(S^{*}, C^{*})$.\n- State how local stability should be assessed in the reduced coordinates, using fundamental criteria appropriate for two-dimensional systems.\n- Provide a single closed-form analytic expression for the two eigenvalues of the Jacobian matrix in terms of $k_{1}$, $k_{2}$, $k_{3}$, $k_{4}$, $E_{T}$, and the steady-state coordinates $(S^{*}, C^{*})$. Your final answer must be the pair of eigenvalues written as a compact analytic expression.\n\nNo numeric approximation or rounding is required, and no units should be reported in the final answer.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained, presenting a standard problem in computational systems biology. It is free from any scientific or mathematical flaws. Therefore, I will proceed with the solution.\n\nThe first step is to write the system of ordinary differential equations (ODEs) that describes the time evolution of the concentrations of the species $S$, $P$, $E$, and $C$, based on the law of mass action for the given reaction network:\n$$\nS + E \\xrightleftharpoons[k_{2}]{k_{1}} C, \\quad C \\xrightarrow{k_{3}} P + E, \\quad P \\xrightarrow{k_{4}} S\n$$\nThe corresponding ODEs are:\n$$\n\\frac{dS}{dt} = -k_{1} S E + k_{2} C + k_{4} P\n$$\n$$\n\\frac{dP}{dt} = k_{3} C - k_{4} P\n$$\n$$\n\\frac{dE}{dt} = -k_{1} S E + k_{2} C + k_{3} C = -k_{1} S E + (k_{2} + k_{3}) C\n$$\n$$\n\\frac{dC}{dt} = k_{1} S E - k_{2} C - k_{3} C = k_{1} S E - (k_{2} + k_{3}) C\n$$\nThe problem specifies two conservation laws: total substrate $S_{T} = S + P + C$ and total enzyme $E_{T} = E + C$. These allow for the reduction of the four-dimensional system to a two-dimensional one. We eliminate variables $P$ and $E$ in favor of $S$ and $C$:\n$$\nE(t) = E_{T} - C(t)\n$$\n$$\nP(t) = S_{T} - S(t) - C(t)\n$$\nSubstituting these expressions into the ODEs for $S$ and $C$ yields the reduced-order system.\n\nFor $\\frac{dS}{dt}$:\n$$\n\\frac{dS}{dt} = -k_{1} S (E_{T} - C) + k_{2} C + k_{4} (S_{T} - S - C)\n$$\n$$\n\\frac{dS}{dt} = -k_{1} E_{T} S + k_{1} S C + k_{2} C + k_{4} S_{T} - k_{4} S - k_{4} C\n$$\nLet us define the right-hand side as $f(S, C)$:\n$$\nf(S, C) = -k_{1} E_{T} S + k_{1} S C + (k_{2} - k_{4}) C - k_{4} S + k_{4} S_{T}\n$$\nFor $\\frac{dC}{dt}$:\n$$\n\\frac{dC}{dt} = k_{1} S (E_{T} - C) - (k_{2} + k_{3}) C\n$$\n$$\n\\frac{dC}{dt} = k_{1} E_{T} S - k_{1} S C - (k_{2} + k_{3}) C\n$$\nLet us define the right-hand side as $g(S, C)$:\n$$\ng(S, C) = k_{1} E_{T} S - k_{1} S C - (k_{2} + k_{3}) C\n$$\nThe reduced-order ODE system is thus:\n$$\n\\frac{dS}{dt} = f(S, C)\n$$\n$$\n\\frac{dC}{dt} = g(S, C)\n$$\nA steady state $(S^{*}, C^{*})$ is a point where the rates of change are zero, i.e., $f(S^{*}, C^{*}) = 0$ and $g(S^{*}, C^{*}) = 0$.\n\nTo analyze the local stability of a steady state, we linearize the system around $(S^{*}, C^{*})$. This involves computing the Jacobian matrix $J$ of the vector field $(f, g)$ and evaluating it at the steady state. The Jacobian matrix is defined as:\n$$\nJ = \\begin{pmatrix} \\frac{\\partial f}{\\partial S}  \\frac{\\partial f}{\\partial C} \\\\ \\frac{\\partial g}{\\partial S}  \\frac{\\partial g}{\\partial C} \\end{pmatrix}\n$$\nWe compute the four partial derivatives from the expressions for $f(S, C)$ and $g(S, C)$:\n$$\n\\frac{\\partial f}{\\partial S} = -k_{1} E_{T} + k_{1} C - k_{4}\n$$\n$$\n\\frac{\\partial f}{\\partial C} = k_{1} S + k_{2} - k_{4}\n$$\n$$\n\\frac{\\partial g}{\\partial S} = k_{1} E_{T} - k_{1} C\n$$\n$$\n\\frac{\\partial g}{\\partial C} = -k_{1} S - (k_{2} + k_{3})\n$$\nEvaluating these derivatives at the steady state $(S^{*}, C^{*})$ gives the Jacobian matrix at the fixed point:\n$$\nJ(S^{*}, C^{*}) = \\begin{pmatrix} -k_{1} E_{T} + k_{1} C^{*} - k_{4}  k_{1} S^{*} + k_{2} - k_{4} \\\\ k_{1} E_{T} - k_{1} C^{*}  -k_{1} S^{*} - k_{2} - k_{3} \\end{pmatrix}\n$$\nLocal stability of the steady state is determined by the eigenvalues of this matrix. For a two-dimensional system, the steady state is locally asymptotically stable if and only if both eigenvalues have negative real parts. This is equivalent to the Routh-Hurwitz criteria for a $2 \\times 2$ matrix:\n$$\n\\mathrm{Tr}(J)  0 \\quad \\text{and} \\quad \\mathrm{Det}(J)  0\n$$\nwhere $\\mathrm{Tr}(J)$ is the trace and $\\mathrm{Det}(J)$ is the determinant of the Jacobian matrix.\n\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\mathrm{det}(J - \\lambda I) = 0$, which is given by $\\lambda^2 - \\mathrm{Tr}(J)\\lambda + \\mathrm{Det}(J) = 0$. The solutions are found using the quadratic formula:\n$$\n\\lambda_{1,2} = \\frac{\\mathrm{Tr}(J) \\pm \\sqrt{\\mathrm{Tr}(J)^2 - 4\\mathrm{Det}(J)}}{2}\n$$\nWe now compute the trace and determinant of $J(S^{*}, C^{*})$ using the derived expressions for its elements. Let us use the notation $E^{*} = E_{T} - C^{*}$ for the steady-state enzyme concentration. The Jacobian elements can be written as: $J_{11} = -k_{1}E^{*} - k_4$, $J_{12} = k_1 S^* + k_2 - k_4$, $J_{21} = k_1 E^*$, $J_{22} = -k_1 S^* - k_2 - k_3$.\n\nThe trace is:\n$$\n\\mathrm{Tr}(J) = J_{11} + J_{22} = (-k_{1} (E_{T} - C^{*}) - k_{4}) + (-k_{1} S^{*} - k_{2} - k_{3})\n$$\n$$\n\\mathrm{Tr}(J) = -k_{1} S^{*} - k_{1}(E_{T} - C^{*}) - k_{2} - k_{3} - k_{4}\n$$\nThe determinant is:\n$$\n\\mathrm{Det}(J) = J_{11} J_{22} - J_{12} J_{21}\n$$\n$$\n\\mathrm{Det}(J) = (-k_{1}(E_{T}-C^{*}) - k_{4})(-k_{1} S^{*} - k_{2} - k_{3}) - (k_{1} S^{*} + k_{2} - k_{4})(k_{1}(E_{T}-C^{*}))\n$$\nExpanding this expression:\n$$\n\\mathrm{Det}(J) = (k_{1}(E_{T}-C^{*}) + k_{4})(k_{1} S^{*} + k_{2} + k_{3}) - (k_{1} S^{*} + k_{2} - k_{4})(k_{1}(E_{T}-C^{*}))\n$$\n$$\n\\mathrm{Det}(J) = [k_{1}^{2}S^{*}(E_{T}-C^{*}) + k_{1}(E_{T}-C^{*})(k_{2}+k_{3}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})] - [k_{1}^{2}S^{*}(E_{T}-C^{*}) + k_{1}k_{2}(E_{T}-C^{*}) - k_{1}k_{4}(E_{T}-C^{*})]\n$$\nAfter cancellation and rearrangement of terms:\n$$\n\\mathrm{Det}(J) = k_{1}(E_{T}-C^{*})k_{3} + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3}) + k_{1}k_{4}(E_{T}-C^{*})\n$$\n$$\n\\mathrm{Det}(J) = k_{1}(E_{T}-C^{*})(k_{3}+k_{4}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})\n$$\nLet's define $T = \\mathrm{Tr}(J)$ and $D = \\mathrm{Det}(J)$. The two eigenvalues are:\n$$\n\\lambda_{1} = \\frac{T + \\sqrt{T^2 - 4D}}{2} \\quad \\text{and} \\quad \\lambda_{2} = \\frac{T - \\sqrt{T^2 - 4D}}{2}\n$$\nSubstituting the full expressions for $T$ and $D$:\n$$\nT = -k_{1} S^{*} - k_{1}(E_{T} - C^{*}) - k_{2} - k_{3} - k_{4}\n$$\n$$\nD = k_{1}(E_{T}-C^{*})(k_{3}+k_{4}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})\n$$\nThese expressions provide the complete analytical form for the eigenvalues of the Jacobian matrix at the steady state $(S^{*}, C^{*})$.",
            "answer": "$$\n\\boxed{\n\\lambda_{1,2} = \\frac{T \\pm \\sqrt{T^2 - 4D}}{2}\n\\text{ where }\nT = -k_{1} S^{*} - k_{1}(E_{T} - C^{*}) - k_{2} - k_{3} - k_{4}\n\\text{ and }\nD = k_{1}(E_{T}-C^{*})(k_{3}+k_{4}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})\n}\n$$"
        },
        {
            "introduction": "While the eigenvalues of a system's Jacobian tell us about its ultimate fate, they can hide crucial details about the immediate, transient response to perturbations. This computational practice delves into the fascinating world of non-normal dynamics, where stable systems can exhibit surprisingly large, temporary amplification of signals or noise. By implementing and comparing several key metrics, you will gain a deeper intuition for how network architecture can shape transient behavior and amplify stochastic fluctuations in gene expression .",
            "id": "3351305",
            "problem": "Consider a gene regulatory network linearized about a steady state, yielding a linear time-invariant dynamical system for small deviations $x(t) \\in \\mathbb{R}^{n}$ of the form $dx/dt = J x(t)$, where $J \\in \\mathbb{R}^{n \\times n}$ is the Jacobian matrix evaluated at the steady state. In the presence of extrinsic or intrinsic fluctuations, a standard linear stochastic model augments this with an additive white-noise input $dx/dt = J x(t) + \\eta(t)$, where $\\eta(t)$ is zero-mean Gaussian white noise with covariance matrix $Q = \\sigma^{2} I$ for a given noise strength $\\sigma^{2} \\in \\mathbb{R}_{0}$ and identity matrix $I$. The steady state is said to be linearly asymptotically stable if all eigenvalues of $J$ have strictly negative real parts. Despite asymptotic stability, non-normality of $J$ (that is, $J$ does not commute with its transpose) can cause transient amplification of perturbations before decay. We define the following quantities grounded in core definitions:\n\n- The spectral abscissa is $\\alpha(J) = \\max \\{\\Re(\\lambda) : \\lambda \\text{ is an eigenvalue of } J\\}$.\n- The numerical abscissa is $\\omega(J) = \\sup_{\\|x\\|_{2} = 1} \\Re(x^{\\top} J x)$, which equals the instantaneous logarithmic growth rate of the Euclidean norm at $t = 0$ along the direction $x$ (with $\\|\\cdot\\|_{2}$ denoting the Euclidean norm).\n- The transient amplification factor at time $t \\ge 0$ is $G(t) = \\|e^{J t}\\|_{2}$, the induced matrix $2$-norm of the matrix exponential.\n- For the stochastic system with $Q = \\sigma^{2} I$, if the steady state is asymptotically stable, the stationary covariance $P$ satisfies the continuous-time Lyapunov equation $J P + P J^{\\top} + Q = 0$. To quantify potential burstiness in the expression of the first gene attributable to coupling and non-normality, define a baseline decoupled system with $J_{0} = \\mathrm{diag}(J)$, and let $P_{0}$ solve $J_{0} P_{0} + P_{0} J_{0}^{\\top} + Q = 0$. Define the burstiness index for the first gene as $B = P_{11} / (P_{0})_{11}$, comparing the steady-state variance with and without coupling.\n\nTask: Write a program that, for each test case given below, computes:\n- A boolean indicating linear asymptotic stability, namely whether $\\alpha(J)  0$.\n- The numerical abscissa $\\omega(J)$ defined above.\n- The maximum transient amplification $G_{\\max} = \\max_{t \\in [0, T_{\\max}]} G(t)$ using a uniform time grid on $[0, T_{\\max}]$ with $N$ points, where $T_{\\max} = 10$ and $N = 2001$, and the earliest time $t_{\\mathrm{peak}} \\in [0, T_{\\max}]$ at which this maximum is attained on the grid.\n- The burstiness index $B$ as defined above, provided the steady-state covariance exists (that is, provided the system is asymptotically stable).\n\nFoundational starting points you must use:\n- Linearization of a smooth nonlinear system near a fixed point yields $dx/dt = J x$.\n- The solution of $dx/dt = J x$ is $x(t) = e^{J t} x(0)$.\n- For a stable linear stochastic system with additive white noise covariance $Q$, the stationary covariance $P$ solves $J P + P J^{\\top} + Q = 0$.\n\nAvoid shortcuts that skip essential reasoning; base your algorithmic decisions on these standard facts and definitions.\n\nTest suite:\n- Case $1$: $J = \\begin{bmatrix} -1  0 \\\\ 0  -2 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n- Case $2$: $J = \\begin{bmatrix} -1  10 \\\\ 0  -2 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n- Case $3$: $J = \\begin{bmatrix} -1  2 \\\\ -2  -1 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n- Case $4$: $J = \\begin{bmatrix} -0.1  1.5 \\\\ 0  -0.2 \\end{bmatrix}$, $\\sigma^{2} = 0.5$.\n- Case $5$: $J = \\begin{bmatrix} -1  15  0 \\\\ 0  -2  15 \\\\ 0  0  -3 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n\nOutput specification:\n- For each case, output a list with five entries: $[\\text{stable}, \\omega, G_{\\max}, t_{\\mathrm{peak}}, B]$, where $\\text{stable}$ is a boolean, and $\\omega$, $G_{\\max}$, $t_{\\mathrm{peak}}$, and $B$ are real numbers rounded to six decimal places. If the system is not asymptotically stable, set $B$ to the special value computed from the Lyapunov equation only if it exists; otherwise, define $B$ to be $0.0$.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, for example $[[\\text{stable}_{1}, \\omega_{1}, G_{\\max,1}, t_{\\mathrm{peak},1}, B_{1}], \\ldots, [\\text{stable}_{5}, \\omega_{5}, G_{\\max,5}, t_{\\mathrm{peak},5}, B_{5}]]$.\n\nAll numeric quantities in your output must be pure numbers (booleans or real numbers), with no units, and the booleans must be the literal values True or False. Angles, if any appear, must be in radians. No percentage signs are allowed; use decimal fractions where needed.",
            "solution": "The problem requires a comprehensive analysis of several linear time-invariant systems, which are presented as linearized models of gene regulatory networks. The analysis involves assessing stability, quantifying transient behavior, and characterizing steady-state fluctuations in the presence of noise. The solution will be developed by systematically applying fundamental principles of linear systems theory and matrix analysis for each required metric.\n\n**1. Linear Asymptotic Stability**\n\nThe stability of a steady state in a dynamical system is defined by its response to small perturbations. For the given linear system, $\\frac{dx}{dt} = J x(t)$, the time evolution of a perturbation $x(t)$ from the steady state $x=0$ is given by $x(t) = e^{Jt} x(0)$. The long-term behavior as $t \\to \\infty$ is determined by the eigenvalues of the Jacobian matrix $J$. The solution can be expressed as a linear combination of modes of the form $p(t)e^{\\lambda t}$, where $\\lambda$ are the eigenvalues of $J$ and $p(t)$ is a polynomial in $t$ if the geometric multiplicity of $\\lambda$ is less than its algebraic multiplicity.\n\nFor the state $x(t)$ to decay to zero for any initial condition $x(0)$, all such modes must decay. This is guaranteed if and only if all eigenvalues $\\lambda$ of $J$ have strictly negative real parts, i.e., $\\Re(\\lambda)  0$. The spectral abscissa, defined as $\\alpha(J) = \\max \\{\\Re(\\lambda) : \\lambda \\text{ is an eigenvalue of } J\\}$, provides a concise criterion for stability. The system is linearly asymptotically stable if and only if its spectral abscissa is negative, $\\alpha(J)  0$.\n\nThe computational procedure is thus:\n1.  Compute the set of all eigenvalues of the matrix $J$.\n2.  Calculate the real part of each eigenvalue.\n3.  The largest of these real parts is the spectral abscissa, $\\alpha(J)$.\n4.  A boolean flag for stability is set to `True` if $\\alpha(J)  0$ and `False` otherwise.\n\n**2. Numerical Abscissa**\n\nWhile the spectral abscissa governs the asymptotic decay rate, the initial, instantaneous behavior of a perturbation's magnitude is determined by a different quantity. The instantaneous logarithmic growth rate of the Euclidean norm $\\|x(t)\\|_2$ at $t=0$ for a given initial direction $x(0)$ with $\\|x(0)\\|_2 = 1$ is given by:\n$$\n\\frac{d}{dt}\\ln(\\|x(t)\\|_2)\\Big|_{t=0} = \\Re(x(0)^{\\top} J x(0))\n$$\nSince $x(0)$ is a real vector, this simplifies. We first note that $x^{\\top} J x$ is a scalar, so it equals its transpose, $x^{\\top} J^{\\top} x$. Thus, we can write $x^{\\top} J x = \\frac{1}{2}(x^{\\top} J x + x^{\\top} J^{\\top} x) = x^{\\top} \\left( \\frac{J + J^{\\top}}{2} \\right) x$. The numerical abscissa, $\\omega(J)$, is the supremum of this growth rate over all possible unit-norm directions $x(0)$:\n$$\n\\omega(J) = \\sup_{\\|x\\|_{2} = 1} x^{\\top} \\left( \\frac{J + J^{\\top}}{2} \\right) x\n$$\nBy the Rayleigh-Ritz theorem, this supremum is equal to the largest eigenvalue of the symmetric matrix $H = \\frac{J + J^{\\top}}{2}$. A positive numerical abscissa, $\\omega(J)  0$, indicates that there exist initial perturbations whose magnitude will transiently increase, even if the system is asymptotically stable ($\\alpha(J)  0$).\n\nThe computational procedure is:\n1.  Construct the symmetric part of $J$: $H = \\frac{1}{2}(J + J^{\\top})$.\n2.  Compute the eigenvalues of $H$. As $H$ is symmetric, its eigenvalues are real.\n3.  The numerical abscissa $\\omega(J)$ is the maximum eigenvalue of $H$.\n\n**3. Maximum Transient Amplification**\n\nThe phenomenon of transient growth in a stable, non-normal system is quantified by the transient amplification factor, $G(t) = \\|e^{Jt}\\|_2$, where $\\|\\cdot\\|_2$ is the induced 2-norm (or spectral norm). This factor represents the maximum amplification that an initial condition of unit norm can experience by time $t$. For a normal matrix $J$ (i.e., one that commutes with its transpose, $JJ^{\\top} = J^{\\top}J$), it holds that $\\|e^{Jt}\\|_2 = e^{\\alpha(J)t}$. In this case, if the system is stable ($\\alpha(J)0$), $G(t)$ is a monotonically decreasing function, and its maximum is $G(0)=1$. However, for non-normal matrices, $G(t)$ can rise above $1$ before eventually decaying to $0$.\n\nTo find the maximum amplification $G_{\\max}$ and the earliest time $t_{\\mathrm{peak}}$ at which it occurs, we must evaluate $G(t)$ over the specified interval $[0, T_{\\max}]$. Since an analytical expression for the maximum is generally unavailable, we perform a numerical search on a discrete grid.\n\nThe computational procedure is:\n1.  Define a discrete time grid of $N=2001$ points, $t_i$, uniformly spaced over the interval $[0, T_{\\max}=10]$.\n2.  For each time point $t_i$:\n    a.  Compute the matrix exponential, $E_i = e^{Jt_i}$.\n    b.  Compute the induced $2$-norm of $E_i$, $G(t_i) = \\|E_i\\|_2$, which is equivalent to its largest singular value.\n3.  Find the maximum value among all computed $G(t_i)$ to obtain $G_{\\max}$.\n4.  Identify the index of the first occurrence of this maximum value and find the corresponding time $t_{\\mathrm{peak}}$ from the grid.\n\n**4. Burstiness Index**\n\nFor the stochastic system $\\frac{dx}{dt} = J x(t) + \\eta(t)$ with noise covariance $Q = \\sigma^2 I$, the steady-state covariance matrix $P = \\mathbb{E}[x(t) x(t)^{\\top}]$ exists if and only if the system is asymptotically stable. Under this condition, $P$ is the unique, positive semi-definite solution to the continuous-time Lyapunov equation:\n$$\nJ P + P J^{\\top} + Q = 0\n$$\nThe diagonal element $P_{11}$ is the stationary variance of the first component of the state vector, representing the expression level of the first gene.\n\nTo quantify the contribution of network coupling to this variance, we compare it with a baseline scenario where the system is decoupled. The decoupled dynamics are governed by the matrix $J_0 = \\mathrm{diag}(J)$, which retains only the self-regulation terms of the network. The stationary covariance of this decoupled system, $P_0$, is the solution to its corresponding Lyapunov equation:\n$$\nJ_0 P_0 + P_0 J_0^{\\top} + Q = 0\n$$\nThe burstiness index $B$ is defined as the ratio of the variance of the first gene in the coupled system to that in the decoupled system:\n$$\nB = \\frac{P_{11}}{(P_0)_{11}}\n$$\nA value of $B1$ indicates that the network coupling amplifies the fluctuations of the first gene. If the system is not asymptotically stable, a stationary covariance does not physically exist, and the burstiness index is defined to be $0.0$.\n\nThe computational procedure is:\n1.  Verify that the system is stable using the criterion from step 1. If not, set $B=0.0$.\n2.  If stable, construct the noise covariance matrix $Q = \\sigma^2 I$, where $I$ is the $n \\times n$ identity matrix.\n3.  Solve the Lyapunov equation $J P + P J^{\\top} = -Q$ for the matrix $P$ using a numerical solver.\n4.  Construct the decoupled Jacobian $J_0 = \\mathrm{diag}(J)$.\n5.  Solve the second Lyapunov equation $J_0 P_0 + P_0 J_0^{\\top} = -Q$ for the matrix $P_0$.\n6.  Calculate the burstiness index $B = P_{11} / (P_0)_{11}$.\n\nThis comprehensive approach provides a multi-faceted characterization of the given linear systems, adhering to the fundamental principles outlined in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem for all specified test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[-1.0, 0.0], [0.0, -2.0]]), 1.0),\n        (np.array([[-1.0, 10.0], [0.0, -2.0]]), 1.0),\n        (np.array([[-1.0, 2.0], [-2.0, -1.0]]), 1.0),\n        (np.array([[-0.1, 1.5], [0.0, -0.2]]), 0.5),\n        (np.array([[-1.0, 15.0, 0.0], [0.0, -2.0, 15.0], [0.0, 0.0, -3.0]]), 1.0)\n    ]\n\n    all_results = []\n    \n    for J, sigma_sq in test_cases:\n        result = compute_metrics(J, sigma_sq)\n        all_results.append(result)\n\n    # Format the final output string as specified\n    formatted_strings = []\n    for res in all_results:\n        stable = res[0]\n        # round all numeric values to 6 decimal places\n        omega = f\"{res[1]:.6f}\"\n        g_max = f\"{res[2]:.6f}\"\n        t_peak = f\"{res[3]:.6f}\"\n        b = f\"{res[4]:.6f}\"\n        formatted_strings.append(f\"[{stable},{omega},{g_max},{t_peak},{b}]\")\n    \n    final_output = f\"[{','.join(formatted_strings)}]\"\n    print(final_output)\n\ndef compute_metrics(J, sigma_sq):\n    \"\"\"\n    Computes all required metrics for a single test case.\n\n    Args:\n        J (np.ndarray): The Jacobian matrix.\n        sigma_sq (float): The noise strength.\n\n    Returns:\n        list: A list containing [stable, omega, G_max, t_peak, B].\n    \"\"\"\n    n = J.shape[0]\n\n    # 1. Linear Asymptotic Stability (Spectral Abscissa)\n    eigenvalues = linalg.eigvals(J)\n    spectral_abscissa = np.max(np.real(eigenvalues))\n    is_stable = spectral_abscissa  0\n\n    # 2. Numerical Abscissa\n    H = (J + J.T) / 2.0\n    # For a symmetric/Hermitian matrix, eigvals are real\n    # eigvalsh is specialized and more efficient for this\n    omega = np.max(linalg.eigvalsh(H))\n\n    # 3. Maximum Transient Amplification\n    T_max = 10.0\n    N = 2001\n    t_grid = np.linspace(0, T_max, N)\n    \n    G_values = []\n    for t in t_grid:\n        if t == 0:\n            # ||e^0||_2 = ||I||_2 = 1.0\n            norm_val = 1.0\n        else:\n            exp_Jt = linalg.expm(J * t)\n            norm_val = linalg.norm(exp_Jt, ord=2)\n        G_values.append(norm_val)\n\n    G_max = np.max(G_values)\n    # Get the index of the first occurrence of the maximum value\n    t_peak_idx = np.argmax(G_values)\n    t_peak = t_grid[t_peak_idx]\n\n    # 4. Burstiness Index\n    B = 0.0\n    if is_stable:\n        Q = sigma_sq * np.eye(n)\n        \n        # Full system covariance\n        # solve JP + PJ^T = -Q\n        P = linalg.solve_lyapunov(J, -Q)\n        \n        # Decoupled system covariance\n        J0 = np.diag(np.diag(J))\n        # solve J0*P0 + P0*J0^T = -Q\n        P0 = linalg.solve_lyapunov(J0, -Q)\n\n        P11 = P[0, 0]\n        P0_11 = P0[0, 0]\n        \n        # Avoid division by zero, though not expected in this problem for stable systems\n        if P0_11 != 0:\n            B = P11 / P0_11\n\n    return [is_stable, omega, G_max, t_peak, B]\n\nsolve()\n```"
        },
        {
            "introduction": "Many biological processes do not settle to a constant steady state but instead oscillate in response to periodic environmental cues, such as the circadian clock. This exercise extends stability analysis from fixed points to these dynamic periodic orbits using the powerful framework of Floquet theory. You will learn to determine if a gene regulatory network can \"entrain\" to an external rhythm, a critical question for understanding how organisms synchronize with their environment .",
            "id": "3351265",
            "problem": "Consider a single-species gene product with positive autoregulation subjected to an externally imposed seasonal modulation of its synthesis rate. The state variable is the concentration $x(t)$ measured in arbitrary units (A.U.), and time $t$ is measured in hours. The synthesis rate is modulated as a periodic function $ \\theta(t) = \\theta_0 + \\epsilon \\cos(\\omega t) $, where $ \\theta_0 $ and $ \\epsilon $ have units of A.U./hour, and the angular frequency $ \\omega $ has units of radians per hour. The degradation is first order with rate constant $ \\delta $ (units: $ \\mathrm{hour}^{-1} $). Autoregulation follows a Hill-type activation with exponent $ n $ and half-saturation concentration $ K $ (units: A.U.). A basal synthesis fraction $ b $ (dimensionless) ensures nonzero production even at vanishing $ x $. The dynamical law is specified by the ordinary differential equation\n$$\n\\frac{dx}{dt} = \\theta(t) \\left[ b + \\frac{\\left(\\frac{x}{K}\\right)^n}{1 + \\left(\\frac{x}{K}\\right)^n} \\right] - \\delta x.\n$$\nWithin the framework of linear stability analysis for periodically driven systems, the objective is to characterize entrainment versus instability around a period-$ T $ response $ x^*(t) $ to the forcing with $ T = \\frac{2\\pi}{\\omega} $ (in hours). A period-$ T $ solution $ x^*(t) $ is defined by the stroboscopic fixed-point condition $ x^*(0) = x_0^* $ such that $ x(T; x_0^*) = x_0^* $, where $ x(T; x_0) $ denotes the state reached after integrating the system from initial value $ x(0) = x_0 $ to time $ T $ under the given forcing.\n\nStarting from the model and fundamental definitions of Floquet theory, you must:\n- Numerically determine $ x_0^* $ by solving the stroboscopic fixed-point condition $ x(T; x_0^*) = x_0^* $. Use a mathematically principled approach such as root finding on the scalar function $ g(x_0) = x(T; x_0) - x_0 $ over a physically reasonable bracket.\n- Linearize the system around the periodic solution $ x^*(t) $ to obtain the variational equation $ \\frac{d \\varphi}{dt} = a(t) \\varphi $, where $ a(t) $ is the instantaneous Jacobian $ \\frac{\\partial f}{\\partial x} $ evaluated along $ x^*(t) $ and $ f(x,t) = \\theta(t)\\left[ b + \\frac{\\left(\\frac{x}{K}\\right)^n}{1 + \\left(\\frac{x}{K}\\right)^n} \\right] - \\delta x $. Integrate the variational equation over one forcing period $ T $ with the initial condition $ \\varphi(0) = 1 $ to obtain the Floquet multiplier $ \\mu = \\varphi(T) $.\n- Predict entrainment versus instability regimes using the criterion $ |\\mu|  1 $ for linear stability (entrainment) and $ |\\mu|  1 $ for linear instability (no entrainment), expressing the boolean outcome accordingly.\n\nAngle units: use radians for $ \\omega $, and hours for time. The Floquet multiplier $ \\mu $ is dimensionless. Computed $ \\mu $ values must be reported rounded to six decimal places.\n\nTest Suite:\nUse the following parameter sets, expressed as ordered tuples $ (\\theta_0, \\epsilon, \\omega, \\delta, K, n, b) $. For each case, $ \\omega $ corresponds to a 24-hour cycle, so $ \\omega = \\frac{2\\pi}{24} $ radians/hour and $ T = 24 $ hours.\n1. $(2.5, 1.0, \\frac{2\\pi}{24}, 1.2, 1.0, 4, 0.05)$\n2. $(10.0, 8.0, \\frac{2\\pi}{24}, 0.15, 1.0, 6, 0.05)$\n3. $(3.5, 3.0, \\frac{2\\pi}{24}, 0.5, 1.0, 5, 0.05)$\n\nRequired Final Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list containing two entries: the Floquet multiplier (a float rounded to six decimal places) and the stability classification (a boolean), in that order. For example: \n$[[\\mu_1, True], [\\mu_2, False], [\\mu_3, True]]$\n\nNo external input is permitted; all parameters must be hard-coded. The program must be self-contained and runnable as is.",
            "solution": "The problem requires a stability analysis of a periodic solution to a non-autonomous ordinary differential equation (ODE) representing a single-gene autoregulatory circuit with periodic forcing. The stability of such a period-$T$ solution, $x^*(t)$, is determined using Floquet theory. The core idea is to linearize the system around this periodic orbit and analyze the fate of small perturbations over one period of the forcing.\n\nThe governing ODE is:\n$$\n\\frac{dx}{dt} = f(x, t) = \\theta(t) \\left[ b + \\frac{\\left(\\frac{x}{K}\\right)^n}{1 + \\left(\\frac{x}{K}\\right)^n} \\right] - \\delta x\n$$\nwhere the synthesis rate is periodically modulated by $\\theta(t) = \\theta_0 + \\epsilon \\cos(\\omega t)$. The period of the external forcing is $T = \\frac{2\\pi}{\\omega}$.\n\nThe analysis proceeds in three sequential steps:\n\nStep 1: Determine the periodic solution $x^*(t)$.\nA periodic solution $x^*(t)$ must satisfy the condition $x^*(t+T) = x^*(t)$ for all $t$. This is equivalent to finding a \"stroboscopic fixed point\" $x_0^* = x^*(0)$ such that if we integrate the ODE starting from $x(0) = x_0^*$, the trajectory returns to the same value after one period, i.e., $x(T; x_0^*) = x_0^*$. Here, $x(T; x_0)$ denotes the numerical solution of the ODE at time $T$ with initial condition $x_0$. To find $x_0^*$, we define a function $g(x_0) = x(T; x_0) - x_0$. The desired initial value $x_0^*$ is a root of the equation $g(x_0) = 0$. This is a root-finding problem for a scalar function. We will employ a numerical root-finding algorithm, specifically the Brent-Dekker method, which requires a bracket $[a, b]$ where $g(a)$ and $g(b)$ have opposite signs. A physically meaningful search interval for the concentration $x$ (which must be non-negative) is established, for instance, from a very small positive value up to an estimated upper bound based on maximal production and minimal degradation rates. For a given $x_0$, the value of $g(x_0)$ is computed by numerically integrating the ODE from $t=0$ to $t=T$ using an adaptive solver such as `scipy.integrate.solve_ivp`.\n\nStep 2: Linearize the system and compute the Floquet multiplier $\\mu$.\nOnce the periodic orbit $x^*(t)$ is identified by its initial condition $x_0^*$, we analyze its stability. We consider a small perturbation $\\varphi(t)$ to the periodic solution, $x(t) = x^*(t) + \\varphi(t)$. Substituting this into the ODE and performing a Taylor expansion to first order gives the variational equation:\n$$\n\\frac{d\\varphi}{dt} \\approx \\frac{\\partial f(x,t)}{\\partial x}\\bigg|_{x=x^*(t)} \\varphi(t)\n$$\nLet $a(t) = \\frac{\\partial f(x,t)}{\\partial x}\\big|_{x=x^*(t)}$. The variational equation is $\\frac{d\\varphi}{dt} = a(t) \\varphi(t)$.\nThe Jacobian term $a(t)$ is calculated as:\n$$\na(t) = \\frac{\\partial}{\\partial x} \\left( \\theta(t) \\left[ b + \\frac{x^n}{K^n + x^n} \\right] - \\delta x \\right) \\Bigg|_{x=x^*(t)}\n$$\n$$\na(t) = \\theta(t) \\frac{n K^n (x^*(t))^{n-1}}{(K^n + (x^*(t))^n)^2} - \\delta\n$$\nThe solution to the linear variational equation is $\\varphi(t) = \\varphi(0) \\exp\\left(\\int_0^t a(\\tau) d\\tau\\right)$. The Floquet multiplier $\\mu$ gives the factor by which the perturbation is scaled after one period $T$. It is defined as $\\mu = \\varphi(T) / \\varphi(0)$. By setting the initial perturbation to unity, $\\varphi(0) = 1$, the multiplier is simply $\\mu = \\varphi(T)$.\nTo compute $\\mu$, we construct a coupled system of two ODEs:\n$$\n\\begin{cases}\n\\frac{dx}{dt} = f(x, t) \\\\\n\\frac{d\\varphi}{dt} = a(t) \\varphi = \\left( \\theta(t) \\frac{n K^n x^{n-1}}{(K^n + x^n)^2} - \\delta \\right) \\varphi\n\\end{cases}\n$$\nWe integrate this system over one period, from $t=0$ to $t=T$, with the initial conditions $x(0) = x_0^*$ and $\\varphi(0) = 1$. The value of the second component of the solution vector at $t=T$ is the Floquet multiplier $\\mu$.\n\nStep 3: Classify stability.\nThe stability of the periodic orbit $x^*(t)$ is determined by the magnitude of the Floquet multiplier $\\mu$:\n- If $|\\mu|  1$, perturbations decay over time, and the orbit is linearly stable. The system is said to be \"entrained\" to the periodic forcing.\n- If $|\\mu|  1$, perturbations grow, and the orbit is unstable. The system fails to entrain to the forcing period and may exhibit more complex dynamics like quasi-periodicity or chaos.\n- The case $|\\mu| = 1$ is a borderline (neutrally stable) case and often indicates a bifurcation point.\n\nThe algorithm to be implemented will perform these three steps for each parameter set provided in the test suite. The final output for each case will consist of the calculated Floquet multiplier $\\mu$, rounded to six decimal places, and a boolean value indicating stability ($True$ for $|\\mu|  1$, $False$ otherwise).",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It iterates through each parameter set, calculates the Floquet multiplier\n    and stability, and prints the results in the required format.\n    \"\"\"\n    \n    # Test cases as tuples: (theta0, epsilon, omega, delta, K, n, b)\n    test_cases = [\n        (2.5, 1.0, 2 * np.pi / 24, 1.2, 1.0, 4, 0.05),\n        (10.0, 8.0, 2 * np.pi / 24, 0.15, 1.0, 6, 0.05),\n        (3.5, 3.0, 2 * np.pi / 24, 0.5, 1.0, 5, 0.05)\n    ]\n\n    results = []\n    for params in test_cases:\n        mu, is_stable = analyze_system(params)\n        results.append([round(mu, 6), is_stable])\n\n    # Format the output as a string representation of a list of lists.\n    # e.g., [[-0.00018, True],[1.75881, False],[1.309038, False]]\n    # The default str() for a list provides the correct spacing and format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef analyze_system(params):\n    \"\"\"\n    Analyzes a single parameter set to find the Floquet multiplier and stability.\n    \n    Args:\n        params (tuple): A tuple of a single test case's parameters.\n    \n    Returns:\n        tuple: A tuple containing the Floquet multiplier (float) and stability (bool).\n    \"\"\"\n    theta0, eps, omega, delta, K, n, b = params\n    T = 2 * np.pi / omega\n\n    def ode_system(t, y, p):\n        \"\"\" The coupled ODE system for [x, phi]. \"\"\"\n        x, phi = y\n        theta0_p, eps_p, omega_p, delta_p, K_p, n_p, b_p = p\n        \n        theta_t = theta0_p + eps_p * np.cos(omega_p * t)\n        \n        # Non-linear term (Hill function)\n        x_over_K_n = (x / K_p)**n_p\n        hill_term = x_over_K_n / (1 + x_over_K_n)\n        \n        # dx/dt\n        dxdt = theta_t * (b_p + hill_term) - delta_p * x\n        \n        # Jacobian component a(t)\n        # Derivative of the Hill term w.r.t x\n        deriv_hill = (n_p * K_p**n_p * x**(n_p - 1)) / (K_p**n_p + x**n_p)**2\n        a_t = theta_t * deriv_hill - delta_p\n        \n        # dphi/dt\n        dphidt = a_t * phi\n        \n        return [dxdt, dphidt]\n\n    def stroboscopic_map_error(x0, p):\n        \"\"\"\n        Calculates g(x0) = x(T; x0) - x0. The root of this function is the\n        stroboscopic fixed point.\n        \"\"\"\n        # We only need to integrate the x component to find the fixed point.\n        # Initial condition y0 = [x, phi], but phi component is not used here.\n        y0 = [x0, 1.0] \n        sol = solve_ivp(ode_system, [0, T], y0, args=(p,), dense_output=True, rtol=1e-9, atol=1e-12)\n        x_T = sol.sol(T)[0]\n        return x_T - x0\n\n    # Step 1: Find the stroboscopic fixed point x0_star\n    # A robust search bracket for concentration, which must be positive.\n    # The upper bound is estimated based on maximum synthesis rate.\n    search_bracket = [1e-9, 2 * (theta0 + eps) / delta + 10]\n    try:\n        x0_star = brentq(stroboscopic_map_error, a=search_bracket[0], b=search_bracket[1], args=(params,), xtol=1e-12)\n    except ValueError:\n        # This might happen if g(a) and g(b) have the same sign.\n        # For this problem, the bracket should be safe. If not, this indicates an issue.\n        print(f\"Root finding failed for params: {params}\")\n        return np.nan, False\n\n    # Step 2: Calculate the Floquet multiplier mu\n    # Integrate the full coupled system with x(0) = x0_star and phi(0) = 1\n    y0_star = [x0_star, 1.0]\n    sol_full = solve_ivp(ode_system, [0, T], y0_star, args=(params,), rtol=1e-9, atol=1e-12)\n    phi_T = sol_full.y[1, -1]\n    mu = phi_T\n\n    # Step 3: Determine stability\n    is_stable = np.abs(mu)  1.0\n\n    return mu, is_stable\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}