{
    "hands_on_practices": [
        {
            "introduction": "Before any statistical comparison, raw read counts from RNA-sequencing must be normalized to account for technical artifacts like variable sequencing depth and RNA composition. This practice  provides hands-on experience with several foundational normalization methods. By implementing and comparing these strategies, you will gain a critical appreciation for how this essential preprocessing step can influence the final effect-size estimates and the overall conclusions of a differential expression study.",
            "id": "3301660",
            "problem": "You are given count matrices representing ribonucleic acid sequencing (RNA-seq) gene expression for a set of genes across biological replicates from two conditions, along with a requirement to study the sensitivity of gene-level effect-size estimates to the choice of normalization. The task involves three widely used normalization schemes: Trimmed Mean of M-values (TMM), Upper-Quartile, and the median-of-ratios size factors used in Differential Expression Sequencing (DESeq). The computational goal is to estimate, for each gene, an effect-size under each normalization, to compute pairwise differences in effect-size between methods, and to identify genes that are most sensitive to normalization choice.\n\nFoundational base and assumptions: Sequencing counts are nonnegative integers arising from molecular sampling with sample-specific multiplicative biases. Let there be $G$ genes indexed by $g \\in \\{0,\\dots,G-1\\}$ and $S$ samples indexed by $i \\in \\{0,\\dots,S-1\\}$. Let $x_{g i}$ denote the observed count of gene $g$ in sample $i$. Let $L_i = \\sum_{g=0}^{G-1} x_{g i}$ denote the library size of sample $i$. Each sample belongs to a condition $c_i \\in \\{\\text{A}, \\text{B}\\}$. For normalization, define sample-specific size factors $s_i$ such that normalized counts are $y_{g i} = x_{g i} / s_i$. Effect-size between conditions will be assessed by a log-ratio of group-level summary statistics of $y_{g i}$.\n\nNormalization methods to implement:\n- Trimmed Mean of M-values (TMM): Trimmed Mean of M-values (TMM) estimates relative scaling offsets by removing extreme genes and computing a mean of log-ratios of relative expression. Choose a reference sample $r$ as the sample whose library size $L_r$ is closest to $\\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$. For each non-reference sample $i \\neq r$, define per-gene quantities $p_{g i} = x_{g i} / L_i$ and $p_{g r} = x_{g r} / L_r$, then $M_{g i} = \\log_2\\!\\left(\\frac{p_{g i}}{p_{g r}}\\right)$ and $A_{g i} = \\frac{1}{2}\\log_2\\!\\left(p_{g i} p_{g r}\\right)$ for genes with $x_{g i} > 0$ and $x_{g r} > 0$. Exclude genes by trimming the lower and upper $p_M$ quantiles of $M_{g i}$ and the lower and upper $p_A$ quantiles of $A_{g i}$, then compute the trimmed mean $\\overline{M}_i$ over the retained genes. Set $s_r = 1$ and $s_i = 2^{\\overline{M}_i}$ for $i \\neq r$. If no genes remain after trimming for a sample $i$, set $s_i = 1$.\n- Upper-Quartile size factor: For each sample $i$, compute the upper quartile $U_i$ as the $75$th percentile of $\\{x_{g i} : x_{g i} > 0\\}$. Define $s_i = U_i$. If a sample has no positive counts, set $s_i = 1$.\n- DESeq median-of-ratios size factor: For each gene $g$ with strictly positive counts across all samples (i.e., $x_{g i} > 0$ for all $i$), compute the geometric mean $G_g = \\exp\\!\\left(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i}\\right)$. For each sample $i$, form the ratios $r_{g i} = x_{g i}/G_g$ over all such genes $g$ and define $s_i$ as $\\operatorname{median}\\{r_{g i}\\}_g$. If no gene has strictly positive counts across all samples, fall back to library-size scaling $s_i = L_i / \\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$.\n\nEffect-size definition: For a normalization method $m$ yielding size factors $s^{(m)}_i$, define normalized counts $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$. For a fixed pseudocount $c > 0$, define the group mean within condition $\\text{A}$ as $\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$ and within condition $\\text{B}$ as $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$, where $I_{\\text{A}}$ and $I_{\\text{B}}$ are the index sets of samples in conditions $\\text{A}$ and $\\text{B}$ respectively. The effect-size is $ES^{(m)}_g = \\log_2\\!\\left(\\frac{\\mu^{(m)}_{g,\\text{A}} + c}{\\mu^{(m)}_{g,\\text{B}} + c}\\right)$.\n\nSensitivity analysis: For a pair of methods $(m_1,m_2)$, define $\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$. Define the sensitivity score $S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$. For a threshold $\\tau > 0$, a gene is deemed sensitive if $S_g \\ge \\tau$.\n\nYour program must implement the three normalization methods, compute $ES^{(m)}_g$ for each gene and method, compute $\\Delta ES_g$ for all method pairs, compute $S_g$, and return the zero-based indices of genes classified as sensitive for each provided test case. If any intermediate quantity requires trimming or set exclusion, adhere to the definitions above. If any normalization method encounters a degenerate scenario (e.g., no valid genes or percentiles undefined), apply the explicit fallbacks described.\n\nTest suite (three cases; each case specifies an integer count matrix of shape $G \\times S$, group membership, pseudocount $c$, trimming parameters $p_M$ and $p_A$, and sensitivity threshold $\\tau$). For all cases, use zero-based gene indices and zero-based sample indices.\n\n- Case $1$ (happy path, varied library sizes, mixed differential expression):\n    - Counts $x_{g i}$ for $G = 8$ genes and $S = 6$ samples (rows are genes $g = 0,\\dots,7$; columns are samples $i = 0,\\dots,5$):\n      - Gene $0$: $[50,55,52,49,53,50]$\n      - Gene $1$: $[100,95,105,280,300,320]$\n      - Gene $2$: $[200,210,190,80,75,85]$\n      - Gene $3$: $[0,5,0,30,35,40]$\n      - Gene $4$: $[400,420,410,390,380,400]$\n      - Gene $5$: $[10,12,11,9,8,10]$\n      - Gene $6$: $[8000,8200,7800,7000,7200,7100]$\n      - Gene $7$: $[2,1,0,3,4,0]$\n    - Condition index sets: $I_{\\text{A}} = \\{0,1,2\\}$, $I_{\\text{B}} = \\{3,4,5\\}$.\n    - Pseudocount: $c = 0.5$.\n    - TMM trimming fractions: $p_M = 0.3$, $p_A = 0.05$.\n    - Sensitivity threshold: $\\tau = 0.3$.\n\n- Case $2$ (boundary case, many zeros, extreme library-size imbalance, DESeq fallback may be triggered if needed):\n    - Counts $x_{g i}$ for $G = 6$ genes and $S = 6$ samples:\n      - Gene $0$: $[0,0,0,0,0,0]$\n      - Gene $1$: $[5,0,2,0,5,2]$\n      - Gene $2$: $[0,0,50,0,0,150]$\n      - Gene $3$: $[20,2000,25,10,900,20]$\n      - Gene $4$: $[30,1500,20,20,800,30]$\n      - Gene $5$: $[10,1000,15,5,700,10]$\n    - Condition index sets: $I_{\\text{A}} = \\{0,1,2\\}$, $I_{\\text{B}} = \\{3,4,5\\}$.\n    - Pseudocount: $c = 0.5$.\n    - TMM trimming fractions: $p_M = 0.3$, $p_A = 0.05$.\n    - Sensitivity threshold: $\\tau = 0.5$.\n\n- Case $3$ (edge case, perfectly balanced counts; all methods should agree resulting in zero sensitivity):\n    - Counts $x_{g i}$ for $G = 5$ genes and $S = 6$ samples:\n      - Gene $0$: $[100,100,100,100,100,100]$\n      - Gene $1$: $[100,100,100,100,100,100]$\n      - Gene $2$: $[100,100,100,100,100,100]$\n      - Gene $3$: $[100,100,100,100,100,100]$\n      - Gene $4$: $[100,100,100,100,100,100]$\n    - Condition index sets: $I_{\\text{A}} = \\{0,1,2\\}$, $I_{\\text{B}} = \\{3,4,5\\}$.\n    - Pseudocount: $c = 0.5$.\n    - TMM trimming fractions: $p_M = 0.3$, $p_A = 0.05$.\n    - Sensitivity threshold: $\\tau = 0.01$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a bracketed, comma-separated list of zero-based gene indices with no spaces. For example, a valid output might look like $[[0,2,5],[1],[0,3]]$; if a case has no sensitive genes, output an empty list for that case as $[]$.",
            "solution": "The problem is deemed valid as it is scientifically grounded in established principles of computational systems biology, is well-posed with precise definitions and fallbacks, and is internally consistent. The task is to implement three distinct RNA-seq normalization methods—Trimmed Mean of M-values (TMM), Upper-Quartile (UQ), and the median-of-ratios scheme from DESeq—and to quantify the sensitivity of gene-level effect size estimates to the choice of method.\n\nLet the given count matrix be $X$, with elements $x_{g i}$ for gene $g \\in \\{0, \\dots, G-1\\}$ and sample $i \\in \\{0, \\dots, S-1\\}$. The library size for sample $i$ is $L_i = \\sum_{g=0}^{G-1} x_{g i}$. Each sample $i$ belongs to one of two conditions, denoted $\\text{A}$ and $\\text{B}$, with corresponding sample index sets $I_{\\text{A}}$ and $I_{\\text{B}}$. The core of the analysis involves computing sample-specific size factors $s_i$ for each of the three normalization methods, which are then used to obtain normalized counts $y_{g i} = x_{g i} / s_i$.\n\nThe first normalization method is the Trimmed Mean of M-values (TMM). This method estimates size factors relative to a chosen reference sample. The reference sample, indexed by $r$, is selected as the one whose library size $L_r$ is minimally distant from the median of all library sizes, $\\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$. By definition, $s_r = 1$. For every other sample $i$, we compute per-gene log-ratios of expression proportions, $M_{g i} = \\log_2(\\frac{p_{g i}}{p_{g r}})$, and average expression intensities, $A_{g i} = \\frac{1}{2}\\log_2(p_{g i} p_{g r})$, where $p_{g i} = x_{g i}/L_i$ and $p_{g r} = x_{g r}/L_r$. These calculations are restricted to genes with positive counts in both sample $i$ and the reference $r$. A set of robust genes is identified by trimming the values of $M_{gi}$ and $A_{gi}$. Specifically, genes are excluded if their $M_{gi}$ values fall in the lower or upper $p_M$ fraction of all $M_{gi}$ values for that sample, or if their $A_{gi}$ values fall in the lower or upper $p_A$ fraction of all $A_{gi}$ values. The size factor $s_i$ is then calculated as $s_i = 2^{\\overline{M}_i}$, where $\\overline{M}_i$ is the mean of the $M_{gi}$ values for the retained (non-trimmed) genes. If no genes remain after trimming, we use the fallback $s_i = 1$.\n\nThe second method is Upper-Quartile (UQ) normalization. This is a simpler approach where the size factor $s_i$ for each sample $i$ is set to the $75$th percentile of its positive counts, i.e., $s_i = U_i = \\text{percentile}(\\{x_{g i} : x_{g i} > 0\\}, 75)$. This method is robust to a small number of very highly expressed genes. If a sample contains no positive counts, its size factor is set to a default value of $s_i = 1$.\n\nThe third method is the median-of-ratios approach, characteristic of the DESeq package. This method first establishes a pseudo-reference sample by calculating the geometric mean of counts for each gene $g$ across all samples: $G_g = \\exp(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i})$. This calculation is restricted to genes that have strictly positive counts in every sample. For each sample $i$, a size factor $s_i$ is then determined as the median of the ratios $r_{g i} = x_{g i} / G_g$, taken over the set of genes used to compute the geometric means. A crucial fallback is defined: if no single gene has positive counts across all samples, the size factors are computed by library size normalization, $s_i = L_i / \\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$.\n\nOnce the size factors $s^{(m)}_i$ are computed for each method $m \\in \\{\\text{TMM}, \\text{UQ}, \\text{DESeq}\\}$, we can determine the effect size for each gene. Normalized counts are $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$. The arithmetic means of these normalized counts are computed for each gene $g$ within each condition: $\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$ and $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$. A pseudocount $c > 0$ is added to handle zeros and stabilize log-ratios for low-count genes. The effect size for gene $g$ under method $m$ is the log-base-2 fold change: $ES^{(m)}_g = \\log_2((\\mu^{(m)}_{g,\\text{A}} + c) / (\\mu^{(m)}_{g,\\text{B}} + c))$.\n\nFinally, to assess sensitivity to the normalization method, we compute the pairwise differences in effect size for each gene, $\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$. The overall sensitivity for gene $g$ is defined as the maximum absolute difference across the three pairs of methods: $S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$. A gene is classified as sensitive if its sensitivity score $S_g$ meets or exceeds a given threshold $\\tau$. The final output consists of the indices of these sensitive genes for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases for normalization sensitivity analysis.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"counts\": np.array([\n                [50, 55, 52, 49, 53, 50],\n                [100, 95, 105, 280, 300, 320],\n                [200, 210, 190, 80, 75, 85],\n                [0, 5, 0, 30, 35, 40],\n                [400, 420, 410, 390, 380, 400],\n                [10, 12, 11, 9, 8, 10],\n                [8000, 8200, 7800, 7000, 7200, 7100],\n                [2, 1, 0, 3, 4, 0]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.3\n        },\n        # Case 2 (boundary case, many zeros)\n        {\n            \"counts\": np.array([\n                [0, 0, 0, 0, 0, 0],\n                [5, 0, 2, 0, 5, 2],\n                [0, 0, 50, 0, 0, 150],\n                [20, 2000, 25, 10, 900, 20],\n                [30, 1500, 20, 20, 800, 30],\n                [10, 1000, 15, 5, 700, 10]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.5\n        },\n        # Case 3 (edge case, balanced counts)\n        {\n            \"counts\": np.array([\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.01\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        counts = params[\"counts\"]\n        cond_a_indices = params[\"cond_a_indices\"]\n        cond_b_indices = params[\"cond_b_indices\"]\n        pseudocount = params[\"pseudocount\"]\n        p_m = params[\"p_m\"]\n        p_a = params[\"p_a\"]\n        tau = params[\"tau\"]\n\n        # Run normalizations\n        s_tmm = normalize_tmm(counts, p_m, p_a)\n        s_uq = normalize_uq(counts)\n        s_deseq = normalize_deseq(counts)\n\n        # Calculate effect sizes\n        es_tmm = calculate_es(counts, s_tmm, cond_a_indices, cond_b_indices, pseudocount)\n        es_uq = calculate_es(counts, s_uq, cond_a_indices, cond_b_indices, pseudocount)\n        es_deseq = calculate_es(counts, s_deseq, cond_a_indices, cond_b_indices, pseudocount)\n\n        # Sensitivity analysis\n        delta_es_tmm_uq = np.abs(es_tmm - es_uq)\n        delta_es_tmm_deseq = np.abs(es_tmm - es_deseq)\n        delta_es_uq_deseq = np.abs(es_uq - es_deseq)\n        \n        sensitivity_scores = np.maximum.reduce([delta_es_tmm_uq, delta_es_tmm_deseq, delta_es_uq_deseq])\n        \n        sensitive_genes = np.where(sensitivity_scores >= tau)[0].tolist()\n        all_results.append(sensitive_genes)\n\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef normalize_tmm(counts, p_m, p_a):\n    num_genes, num_samples = counts.shape\n    lib_sizes = np.sum(counts, axis=0)\n\n    # Handle all zero lib sizes to avoid division by zero\n    lib_sizes[lib_sizes == 0] = 1\n\n    median_lib_size = np.median(lib_sizes)\n    dists = np.abs(lib_sizes - median_lib_size)\n    ref_idx = np.argmin(dists)\n\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        if i == ref_idx:\n            continue\n\n        x_i = counts[:, i]\n        x_r = counts[:, ref_idx]\n        \n        # Filter for genes with positive counts in both samples\n        valid_genes_mask = (x_i > 0) & (x_r > 0)\n        \n        if not np.any(valid_genes_mask):\n            size_factors[i] = 1.0\n            continue\n            \n        x_i_filt = x_i[valid_genes_mask]\n        x_r_filt = x_r[valid_genes_mask]\n        \n        lib_i = lib_sizes[i]\n        lib_r = lib_sizes[ref_idx]\n\n        p_i = x_i_filt / lib_i\n        p_r = x_r_filt / lib_r\n\n        m_values = np.log2(p_i / p_r)\n        a_values = 0.5 * np.log2(p_i * p_r)\n        \n        # Trim M and A values\n        m_low = np.percentile(m_values, 100 * p_m, interpolation='linear')\n        m_high = np.percentile(m_values, 100 * (1 - p_m), interpolation='linear')\n\n        a_low = np.percentile(a_values, 100 * p_a, interpolation='linear')\n        a_high = np.percentile(a_values, 100 * (1 - p_a), interpolation='linear')\n        \n        retained_mask = (m_values >= m_low) & (m_values <= m_high) & \\\n                        (a_values >= a_low) & (a_values <= a_high)\n        \n        retained_m_values = m_values[retained_mask]\n\n        if len(retained_m_values) == 0:\n            size_factors[i] = 1.0\n        else:\n            trimmed_mean_m = np.mean(retained_m_values)\n            size_factors[i] = 2**trimmed_mean_m\n            \n    return size_factors\n\ndef normalize_uq(counts):\n    num_samples = counts.shape[1]\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        sample_counts = counts[:, i]\n        positive_counts = sample_counts[sample_counts > 0]\n        \n        if len(positive_counts) == 0:\n            size_factors[i] = 1.0\n        else:\n            uq = np.percentile(positive_counts, 75, interpolation='linear')\n            size_factors[i] = uq if uq > 0 else 1.0\n\n    return size_factors\n\ndef normalize_deseq(counts):\n    num_genes, num_samples = counts.shape\n    \n    # Find genes with strictly positive counts across all samples\n    positive_across_all_mask = np.all(counts > 0, axis=1)\n\n    if not np.any(positive_across_all_mask):\n        # Fallback: library size normalization\n        lib_sizes = np.sum(counts, axis=0)\n        # Avoid division by zero for all-zero library or median of zero\n        lib_sizes[lib_sizes == 0] = 1\n        median_lib = np.median(lib_sizes)\n        if median_lib == 0:\n            median_lib = 1\n        return lib_sizes / median_lib\n\n    # Main path\n    counts_for_geo_mean = counts[positive_across_all_mask, :]\n    \n    with np.errstate(divide='ignore'):\n        log_counts = np.log(counts_for_geo_mean)\n    \n    geo_means = np.exp(np.mean(log_counts, axis=1))\n\n    ratios = counts_for_geo_mean / geo_means[:, np.newaxis]\n    size_factors = np.median(ratios, axis=0)\n    \n    return size_factors\n\ndef calculate_es(counts, size_factors, cond_a_indices, cond_b_indices, pseudocount):\n    # Normalize counts\n    norm_counts = counts / size_factors[np.newaxis, :]\n    \n    # Group means\n    mean_a = np.mean(norm_counts[:, cond_a_indices], axis=1)\n    mean_b = np.mean(norm_counts[:, cond_b_indices], axis=1)\n    \n    # Effect size\n    ratio = (mean_a + pseudocount) / (mean_b + pseudocount)\n    effect_sizes = np.log2(ratio)\n    \n    return effect_sizes\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "With normalized data in hand, the core task is to estimate the magnitude and significance of expression changes. The Negative Binomial (NB) Generalized Linear Model (GLM) is the statistical workhorse for this purpose, adeptly handling the count-based nature and overdispersion characteristic of RNA-seq data. In this exercise , you will move beyond simply using a software package and instead derive the parameter estimation algorithm from first principles, providing a deep, mechanistic understanding of how differential expression tools function.",
            "id": "3301611",
            "problem": "You are given a count-based gene expression scenario in computational systems biology for differential gene expression analysis and effect-size assessment. The fundamental base assumptions are: (i) the Central Dogma of Molecular Biology implies that gene expression levels can be quantified via read counts; (ii) count data are well-modeled by a Negative Binomial distribution when overdispersion beyond the Poisson model is present; (iii) Generalized Linear Models (GLM, Generalized Linear Model) provide a principled framework to relate covariates to the mean of a distribution through a link function; and (iv) Maximum Likelihood Estimation (MLE, Maximum Likelihood Estimation) provides parameter estimates by maximizing the likelihood of the observed data under a specified probabilistic model.\n\nFor each gene indexed by $g$, for samples indexed by $i = 1, \\dots, n$, the raw counts $y_{g i}$ are modeled as Negative Binomial random variables with mean parameter $\\mu_{g i}$ and known gene-specific dispersion parameter $\\phi_g$. The variance function is $V(y_{g i}) = \\mu_{g i} + \\phi_g \\mu_{g i}^2$. The GLM with log link specifies\n$$\n\\log \\mu_{g i} = \\alpha_g + x_i^\\top \\beta_g,\n$$\nwhere $\\alpha_g$ is an intercept capturing the baseline expression for gene $g$, $x_i$ is a known covariate vector for sample $i$, and $\\beta_g$ is the effect-size parameter vector (the differential expression effects) to be estimated. The dispersion $\\phi_g$ is known and fixed for each gene. Your task is to estimate $\\beta_g$ by maximum likelihood under the specified model assumptions.\n\nStarting from the fundamental definitions above, derive an algorithm that computes the maximum likelihood estimator of $\\beta_g$ (and the nuisance intercept $\\alpha_g$) based solely on the Negative Binomial log-likelihood and the GLM structure with a log link. The algorithm should be based on first principles: use the Maximum Likelihood Estimation framework, derive the necessary score equations, and design an iterative method that converges to the Maximum Likelihood Estimator. From the resulting observed information at the Maximum Likelihood Estimator, compute the Wald standard error for each component of $\\beta_g$. Interpret $\\beta_g$ as a log effect-size on the mean scale.\n\nImplement the derived algorithm as a complete, runnable program that, for each provided test case gene, outputs the estimated effect-size and its estimated standard error for the first covariate component of $\\beta_g$ (excluding the intercept). There are no physical units in this problem. All answers should be provided as floating-point numbers.\n\nUse the following test suite with $n = 6$ samples and a single covariate $x_i$ indicating treatment ($x_i = 1$) versus control ($x_i = 0$):\n\n- Covariates $x_i$ for $i = 1, \\dots, 6$: $\\{0, 0, 0, 1, 1, 1\\}$.\n\n- Gene $g = 1$: counts $y_{1 i}$ are $\\{20, 24, 22, 40, 45, 43\\}$ and dispersion $\\phi_1 = 0.1$.\n\n- Gene $g = 2$: counts $y_{2 i}$ are $\\{0, 1, 0, 3, 2, 1\\}$ and dispersion $\\phi_2 = 1.0$.\n\n- Gene $g = 3$: counts $y_{3 i}$ are $\\{100, 110, 90, 95, 105, 85\\}$ and dispersion $\\phi_3 = 0.5$.\n\n- Gene $g = 4$: counts $y_{4 i}$ are $\\{5, 6, 4, 5, 6, 4\\}$ and dispersion $\\phi_4 = 10^{-6}$.\n\nFor each gene, compute the Maximum Likelihood Estimator of the single-element $\\beta_g$ and its Wald standard error. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each gene, in order $g = 1, 2, 3, 4$, include a two-element list $[\\widehat{\\beta}_g, \\mathrm{SE}(\\widehat{\\beta}_g)]$ with both values rounded to six decimal places. For example, the overall output format must be\n$$\n[\\,[\\widehat{\\beta}_1,\\mathrm{SE}(\\widehat{\\beta}_1)],\\,[\\widehat{\\beta}_2,\\mathrm{SE}(\\widehat{\\beta}_2)],\\,[\\widehat{\\beta}_3,\\mathrm{SE}(\\widehat{\\beta}_3)],\\,[\\widehat{\\beta}_4,\\mathrm{SE}(\\widehat{\\beta}_4)]\\,],\n$$\nprinted as a single line with no additional text.",
            "solution": "The user-provided problem is a well-posed and scientifically grounded task in computational systems biology, specifically concerning the estimation of parameters in a Generalized Linear Model (GLM) for count data. The problem is valid as it is self-contained, objective, and based on established statistical principles for differential gene expression analysis. We can thus proceed with a complete solution.\n\nThe objective is to derive and implement an algorithm to find the Maximum Likelihood Estimator (MLE) for the effect-size parameters $\\beta_g$ in a Negative Binomial (NB) GLM. For each gene $g$, the observed counts $y_{gi}$ for samples $i=1, \\dots, n$ are modeled as:\n$$\ny_{gi} \\sim \\text{NB}(\\mu_{gi}, \\phi_g)\n$$\nThe mean is $\\mu_{gi}$ and the dispersion $\\phi_g$ is a known, fixed parameter for each gene. The variance is given by $V(y_{gi}) = \\mu_{gi} + \\phi_g \\mu_{gi}^2$. The mean $\\mu_{gi}$ is related to sample-specific covariates $x_i$ through a log link function:\n$$\n\\log(\\mu_{gi}) = \\eta_{gi} = \\alpha_g + x_i^\\top \\beta_g\n$$\nHere, $\\eta_{gi}$ is the linear predictor, $\\alpha_g$ is the intercept parameter, and $\\beta_g$ is the vector of effect-size parameters. Let $\\theta_g = (\\alpha_g, \\beta_g^\\top)^\\top$ be the full vector of parameters to be estimated. The design matrix $X$ for a gene has rows $(1, x_i^\\top)$, such that $\\eta_g = X \\theta_g$. For simplicity, we will drop the gene index $g$ in the following derivation.\n\n**1. Log-Likelihood Function**\n\nThe probability mass function of a Negative Binomial random variable $Y$ with mean $\\mu$ and dispersion $\\phi$ (where size parameter $r = 1/\\phi$) can be parameterized as:\n$$\nP(Y=y | \\mu, \\phi) = \\frac{\\Gamma(y + 1/\\phi)}{\\Gamma(y+1)\\Gamma(1/\\phi)} \\left( \\frac{\\mu}{1/\\phi + \\mu} \\right)^y \\left( \\frac{1/\\phi}{1/\\phi + \\mu} \\right)^{1/\\phi}\n$$\nThe log-likelihood for a single observation $y_i$ is:\n$$\n\\ell(\\theta | y_i) = \\log\\Gamma(y_i + 1/\\phi) - \\log\\Gamma(y_i+1) - \\log\\Gamma(1/\\phi) + y_i \\log(\\mu_i) + \\frac{1}{\\phi} \\log(1/\\phi) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\nIgnoring terms that do not depend on the parameters $\\theta$ (and thus $\\mu_i$), the relevant part of the log-likelihood is:\n$$\n\\ell(\\theta | y_i) \\propto y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\nThe total log-likelihood for $n$ independent observations is the sum over all samples:\n$$\nL(\\theta) = \\sum_{i=1}^n \\ell(\\theta | y_i) = \\sum_{i=1}^n \\left[ y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi) \\right]\n$$\n\n**2. Score Equations and Fisher Information**\n\nTo find the MLE, we must solve the score equations $U(\\theta) = \\nabla_\\theta L(\\theta) = 0$. We use the chain rule: $\\frac{\\partial L}{\\partial \\theta_j} = \\sum_i \\frac{\\partial \\ell_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\theta_j}$.\n\nThe components are:\n- $\\frac{\\partial \\eta_i}{\\partial \\theta_j} = X_{ij}$, where $X_{ij}$ is the element of the design matrix for sample $i$ and parameter $j$.\n- With the log link $\\eta_i = \\log(\\mu_i)$, we have $\\mu_i = e^{\\eta_i}$, so $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = e^{\\eta_i} = \\mu_i$.\n- $\\frac{\\partial \\ell_i}{\\partial \\mu_i} = \\frac{y_i}{\\mu_i} - \\frac{y_i + 1/\\phi}{\\mu_i + 1/\\phi} = \\frac{y_i(\\mu_i + 1/\\phi) - \\mu_i(y_i + 1/\\phi)}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i/\\phi - \\mu_i/\\phi}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)}$. Note that the variance is $V(\\mu_i) = \\mu_i(1+\\phi\\mu_i)$. The parameterization is consistent with the provided variance function $V(\\mu_i) = \\mu_i + \\phi\\mu_i^2$.\n\nCombining the parts for the score contributions:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\theta_j} = \\left( \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)} \\right) (\\mu_i) (X_{ij}) = \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}\n$$\nThe score vector $U(\\theta)$ has components $U_j(\\theta) = \\sum_{i=1}^n \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}$. In matrix notation, this is $U(\\theta) = X^\\top W_s (y - \\mu)$, where $W_s$ is a diagonal matrix with elements $(W_s)_{ii} = (1+\\phi\\mu_i)^{-1}$. These equations are non-linear in $\\theta$, requiring an iterative solution.\n\n**3. Fisher Scoring and Iteratively Reweighted Least Squares (IRLS)**\n\nThe standard method for solving GLM score equations is Fisher Scoring, an iterative procedure based on the Newton-Raphson algorithm. The update rule is:\n$$\n\\theta^{(t+1)} = \\theta^{(t)} + I(\\theta^{(t)})^{-1} U(\\theta^{(t)})\n$$\nwhere $I(\\theta)$ is the Fisher Information matrix, given by $I(\\theta) = -E[H(\\theta)]$, with $H(\\theta)$ being the Hessian matrix ($\\nabla_\\theta^2 L(\\theta)$). The $(j,k)$-th element of the Fisher Information matrix for a GLM is:\n$$I_{jk}(\\theta) = \\sum_{i=1}^n w_i X_{ij} X_{ik} \\quad \\text{where} \\quad w_i = \\frac{1}{V(\\mu_i)}\\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2$$\nFor our NB-GLM with a log link:\n- $V(\\mu_i) = \\mu_i + \\phi \\mu_i^2 = \\mu_i(1 + \\phi\\mu_i)$\n- $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\mu_i$\nThe weights are therefore:\n$$\nw_i = \\frac{1}{\\mu_i(1 + \\phi\\mu_i)} (\\mu_i)^2 = \\frac{\\mu_i}{1 + \\phi\\mu_i}\n$$\nThe Fisher Information matrix is $I(\\theta) = X^\\top W X$, where $W$ is the diagonal matrix of weights $w_i$.\n\nThe Fisher Scoring update can be rearranged into an Iteratively Reweighted Least Squares (IRLS) problem. Define a \"working response\" vector $z$:\n$$\nz_i = \\eta_i + (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\mu_i} = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}\n$$\nThe update for $\\theta$ is then obtained by solving the weighted least squares problem:\n$$\n\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}\n$$\nwhere quantities on the right-hand side are evaluated at the current estimate $\\theta^{(t)}$.\n\nThe IRLS algorithm proceeds as follows:\n1.  Initialize parameter vector $\\theta^{(0)}$. A reasonable starting point is $\\beta^{(0)}=0$ and $\\alpha^{(0)}=\\log(\\bar{y})$.\n2.  Iterate for $t=0, 1, 2, \\dots$ until convergence:\n    a. Calculate the linear predictor: $\\eta^{(t)} = X \\theta^{(t)}$.\n    b. Calculate the mean vector: $\\mu^{(t)} = \\exp(\\eta^{(t)})$.\n    c. Calculate the diagonal weight matrix $W^{(t)}$ with weights $w_i^{(t)} = \\frac{\\mu_i^{(t)}}{1 + \\phi\\mu_i^{(t)}}$.\n    d. Calculate the working response vector $z^{(t)}$ with components $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$.\n    e. Update the parameters by solving the linear system: $\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}$.\n3.  The iteration stops when the change in $\\theta$, e.g., $||\\theta^{(t+1)} - \\theta^{(t)}||^2$, is below a small tolerance.\n\n**4. Standard Errors**\n\nUpon convergence to the MLE $\\hat{\\theta}$, the asymptotic covariance matrix of the estimator is given by the inverse of the Fisher Information matrix evaluated at the MLE:\n$$\n\\text{Cov}(\\hat{\\theta}) = I(\\hat{\\theta})^{-1} = (X^\\top \\hat{W} X)^{-1}\n$$\nwhere $\\hat{W}$ is the weight matrix calculated using the final estimated means $\\hat{\\mu}$. The diagonal elements of this covariance matrix are the estimated variances of the parameter estimators. The Wald standard error for the $j$-th parameter estimate $\\hat{\\theta}_j$ is the square root of the $j$-th diagonal element:\n$$\n\\text{SE}(\\hat{\\theta}_j) = \\sqrt{(\\text{Cov}(\\hat{\\theta}))_{jj}}\n$$\nFor this problem, $\\theta = (\\alpha, \\beta)^\\top$, so we are interested in $\\hat{\\beta}$ and $\\text{SE}(\\hat{\\beta})$, which correspond to the second element of $\\hat{\\theta}$ and the square root of the $(2,2)$ element of the covariance matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_nb_glm(y_counts, covariates, phi, max_iter=100, tol=1e-8):\n    \"\"\"\n    Fits a Negative Binomial Generalized Linear Model using IRLS.\n\n    Args:\n        y_counts (np.ndarray): Vector of observed counts.\n        covariates (np.ndarray): Vector of covariates for the non-intercept term.\n        phi (float): Known dispersion parameter.\n        max_iter (int): Maximum number of iterations for IRLS.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        tuple: A tuple containing:\n            - beta_hat (float): The estimated coefficient for the covariate.\n            - se_beta (float): The standard error of the estimated coefficient.\n    \"\"\"\n    # Design matrix X should include an intercept column\n    n_samples = len(y_counts)\n    X = np.ones((n_samples, 2))\n    X[:, 1] = covariates\n\n    # Initial parameter estimates\n    # A safe starting point for the intercept if all y_counts are 0\n    mean_y = np.mean(y_counts)\n    if mean_y == 0:\n        alpha = -10.0  # Heuristic for log(small_number)\n    else:\n        alpha = np.log(mean_y)\n    beta = 0.0\n    theta = np.array([alpha, beta])\n\n    for i in range(max_iter):\n        # 1. Linear predictor and mean\n        eta = X @ theta\n        mu = np.exp(eta)\n\n        # 2. Weights for Fisher Information matrix\n        # w_i = mu_i / (1 + phi * mu_i)\n        weights = mu / (1.0 + phi * mu)\n        W = np.diag(weights)\n\n        # 3. Working response\n        # z_i = eta_i + (y_i - mu_i) / mu_i\n        # Handle mu_i close to zero to avoid division issues.\n        # If mu is very small, the weight is also very small, so the contribution\n        # to the weighted least squares is minimal. We can add a small epsilon.\n        mu_safe = np.maximum(mu, 1e-10)\n        z = eta + (y_counts - mu) / mu_safe\n\n        # 4. Update parameters via weighted least squares\n        # theta_new = (X^T W X)^-1 X^T W z\n        # Solving the linear system is more stable than inverting.\n        XT_W = X.T @ W\n        XT_W_X = XT_W @ X\n        XT_W_z = XT_W @ z\n        \n        try:\n            # Check if matrix is invertible\n            if np.linalg.det(XT_W_X) == 0:\n                # This can happen with perfect separation or other data issues\n                # Return NaN to indicate failure to converge\n                return np.nan, np.nan\n            theta_new = np.linalg.solve(XT_W_X, XT_W_z)\n        except np.linalg.LinAlgError:\n            # Fails to converge, likely due to singular matrix\n            return np.nan, np.nan\n\n        # 5. Check for convergence\n        diff = np.sum((theta_new - theta) ** 2)\n        theta = theta_new\n        if diff < tol:\n            break\n\n    # After convergence, calculate standard errors\n    # Cov(theta_hat) = (X^T W X)^-1\n    final_eta = X @ theta\n    final_mu = np.exp(final_eta)\n    final_weights = final_mu / (1.0 + phi * final_mu)\n    final_W = np.diag(final_weights)\n    \n    try:\n        cov_matrix = np.linalg.inv(X.T @ final_W @ X)\n        se_beta = np.sqrt(cov_matrix[1, 1])\n    except np.linalg.LinAlgError:\n        se_beta = np.nan\n\n    beta_hat = theta[1]\n    \n    return beta_hat, se_beta\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Structure: (counts, dispersion)\n    test_cases = [\n        (np.array([20, 24, 22, 40, 45, 43]), 0.1),\n        (np.array([0, 1, 0, 3, 2, 1]), 1.0),\n        (np.array([100, 110, 90, 95, 105, 85]), 0.5),\n        (np.array([5, 6, 4, 5, 6, 4]), 1e-6),\n    ]\n\n    # Covariate vector is the same for all cases.\n    covariates = np.array([0, 0, 0, 1, 1, 1])\n\n    results = []\n    for y_counts, phi in test_cases:\n        beta_hat, se_beta = fit_nb_glm(y_counts, covariates, phi)\n        results.append([beta_hat, se_beta])\n\n    # Format the output string as required.\n    # [[beta1,SE1],[beta2,SE2],...]\n    results_str_list = []\n    for beta, se in results:\n        # Round to six decimal places for output\n        beta_str = f\"{beta:.6f}\"\n        se_str = f\"{se:.6f}\"\n        results_str_list.append(f\"[{beta_str},{se_str}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A key measure of scientific validity is the ability to reproduce findings across different experiments, a significant challenge in genomics. Systematic biases, even after standard normalization, can distort the scale of measured effect sizes and impede cross-study comparisons. This final practice  introduces a powerful technique using external spike-in controls to diagnose and correct for such lab-specific biases, offering a practical method for calibrating log-fold changes and enhancing the reproducibility of your results.",
            "id": "3301688",
            "problem": "You are given replicate count measurements for messenger RNA (mRNA) genes and external spike-in standards in two independent laboratories. In each lab, a control condition and a treatment condition were assayed. Assume the following foundational model and definitions suitable for differential gene expression analysis and effect-size assessment:\n\n- The Log Fold Change (LFC) is defined as $LFC = \\log_{2}\\left(\\frac{\\bar{T} + \\epsilon}{\\bar{C} + \\epsilon}\\right)$, where $\\bar{T}$ is the arithmetic mean of treatment replicates, $\\bar{C}$ is the arithmetic mean of control replicates, and $\\epsilon$ is a small positive pseudocount to ensure numerical stability when zero counts occur.\n- Spike-in standards are synthetic molecules added at known ratios between treatment and control. Their true log fold changes, denoted $LFC^{true}$, are known a priori.\n- A lab-specific multiplicative scaling bias is modeled as $LFC^{obs} \\approx \\kappa \\cdot LFC^{true}$ for spike-ins, where $LFC^{obs}$ is the observed spike-in log fold change and $\\kappa$ is an unknown, lab-specific scaling factor.\n- The calibration rule is $LFC^{cal} = \\frac{LFC^{obs}}{\\hat{\\kappa}}$, where $\\hat{\\kappa}$ is an estimate of $\\kappa$ derived from spike-ins.\n\nStarting from this base, implement the following steps for each test case:\n\n1. For each lab:\n   a. Compute $LFC^{obs}$ for every spike-in using the given replicate counts and the specified $\\epsilon$.\n   b. Estimate $\\hat{\\kappa}$ by least squares under a regression-through-the-origin model that minimizes $\\sum_{i}\\left(LFC^{obs}_{i} - \\hat{\\kappa} \\cdot LFC^{true}_{i}\\right)^{2}$. This yields\n   $$\\hat{\\kappa} = \\dfrac{\\sum_{i} LFC^{true}_{i} \\cdot LFC^{obs}_{i}}{\\sum_{i} \\left(LFC^{true}_{i}\\right)^{2}}.$$\n   c. Compute $LFC^{obs}$ for each gene.\n   d. Calibrate gene log fold changes via $LFC^{cal} = \\frac{LFC^{obs}}{\\hat{\\kappa}}$.\n\n2. Within each lab, rank genes by absolute magnitude of $LFC^{obs}$ and of $LFC^{cal}$. Define the ranking function as sorting by descending absolute value, with ties broken lexicographically by gene name in ascending order. Determine whether any within-lab gene ranking changes after calibration. The change indicator should be a boolean.\n\n3. Quantify cross-lab reproducibility both before and after calibration using the Pearson correlation coefficient of signed gene log fold changes. Let $x$ be the vector of gene $LFC^{obs}$ in Lab A, and $y$ be the corresponding vector in Lab B for the same genes and ordering. Compute\n   $$r_{\\text{pre}} = \\dfrac{\\operatorname{Cov}(x, y)}{\\sigma_{x}\\sigma_{y}}$$\n   and\n   $$r_{\\text{post}} = \\dfrac{\\operatorname{Cov}\\left(x^{cal}, y^{cal}\\right)}{\\sigma_{x^{cal}}\\sigma_{y^{cal}}},$$\n   where $\\operatorname{Cov}$ denotes covariance and $\\sigma$ denotes standard deviation. Report the improvement as $\\Delta r = r_{\\text{post}} - r_{\\text{pre}}$.\n\n4. Your program should implement the above computations for the test suite provided below. The pseudocount must be $\\epsilon = 0.5$. There are no physical units to report and no angles involved. All fractional outputs must be in decimal form.\n\nTest Suite (three cases):\n\n- Case 1 (happy path: sizeable bias differences across labs, no zero counts):\n  - Labs: Lab1 and Lab2\n  - Genes and replicates:\n    - Control replicates (shared across labs):\n      - $G1$: $[190, 210]$, $G2$: $[175, 185]$, $G3$: $[145, 155]$, $G4$: $[215, 225]$, $G5$: $[95, 105]$, $G6$: $[295, 305]$, $G7$: $[75, 85]$, $G8$: $[390, 410]$\n    - Lab1 treatment replicates:\n      - $G1$: $[850, 865]$, $G2$: $[470, 480]$, $G3$: $[90, 95]$, $G4$: $[215, 225]$, $G5$: $[30, 32]$, $G6$: $[580, 595]$, $G7$: $[11, 12]$, $G8$: $[530, 540]$\n    - Lab2 treatment replicates:\n      - $G1$: $[455, 465]$, $G2$: $[310, 315]$, $G3$: $[112, 115]$, $G4$: $[215, 225]$, $G5$: $[50, 53]$, $G6$: $[440, 445]$, $G7$: $[26, 27]$, $G8$: $[470, 475]$\n  - Spike-ins:\n    - Control replicates (shared across labs): $S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - Lab1 treatment replicates: $S1$: $[38, 38]$, $S2$: $[98, 102]$, $S3$: $[260, 268]$\n    - Lab2 treatment replicates: $S1$: $[57, 58]$, $S2$: $[98, 102]$, $S3$: $[173, 175]$\n    - True spike-in log fold changes: $LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n- Case 2 (boundary condition: mild bias differences, modest effects):\n  - Labs: LabA and LabB\n  - Genes and replicates:\n    - Control replicates (shared across labs):\n      - $G1$: $[145, 155]$, $G2$: $[190, 210]$, $G3$: $[175, 185]$, $G4$: $[95, 105]$, $G5$: $[215, 225]$\n    - LabA treatment replicates:\n      - $G1$: $[214, 218]$, $G2$: $[137, 140]$, $G3$: $[178, 182]$, $G4$: $[205, 208]$, $G5$: $[105, 108]$\n    - LabB treatment replicates:\n      - $G1$: $[207, 210]$, $G2$: $[142, 145]$, $G3$: $[177, 183]$, $G4$: $[192, 195]$, $G5$: $[113, 115]$\n  - Spike-ins:\n    - Control replicates (shared across labs): $S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - LabA treatment replicates: $S1$: $[48, 49]$, $S2$: $[98, 102]$, $S3$: $[206, 209]$\n    - LabB treatment replicates: $S1$: $[51, 53]$, $S2$: $[98, 102]$, $S3$: $[192, 194]$\n    - True spike-in log fold changes: $LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n- Case 3 (edge case: zero counts in one replicate, larger bias differences):\n  - Labs: LabA and LabB\n  - Genes and replicates:\n    - LabA control replicates:\n      - $G1$: $[115, 125]$, $G2$: $[58, 62]$, $G3$: $[195, 205]$, $G4$: $[78, 82]$, $G5$: $[148, 152]$\n    - LabA treatment replicates:\n      - $G1$: $[292, 298]$, $G2$: $[8, 10]$, $G3$: $[260, 265]$, $G4$: $[42, 44]$, $G5$: $[148, 152]$\n    - LabB control replicates:\n      - $G1$: $[115, 125]$, $G2$: $[0, 60]$, $G3$: $[195, 205]$, $G4$: $[78, 82]$, $G5$: $[148, 152]$\n    - LabB treatment replicates:\n      - $G1$: $[193, 197]$, $G2$: $[11, 12]$, $G3$: $[230, 234]$, $G4$: $[56, 58]$, $G5$: $[148, 152]$\n  - Spike-ins:\n    - Control replicates (shared across labs): $S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - LabA treatment replicates: $S1$: $[40, 41]$, $S2$: $[98, 102]$, $S3$: $[244, 248]$\n    - LabB treatment replicates: $S1$: $[61, 62]$, $S2$: $[98, 102]$, $S3$: $[161, 163]$\n    - True spike-in log fold changes: $LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\nYour program must output a single line containing the list of results for the three cases, in order. For each case, return a two-element list $[\\Delta r, \\text{changed}]$, where $\\Delta r$ is a float (the improvement in Pearson correlation, $r_{\\text{post}} - r_{\\text{pre}}$), and $\\text{changed}$ is a boolean indicating whether any within-lab absolute-$LFC$ gene ranking changed due to calibration, aggregated across both labs in the case. The final output format must be a single line string of the form:\n\"[[delta_r_case1,changed_case1],[delta_r_case2,changed_case2],[delta_r_case3,changed_case3]]\".",
            "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It is scientifically grounded in the principles of computational systems biology, specifically concerning the normalization of differential gene expression data. The problem is well-posed, providing a complete and consistent set of definitions, data, and computational steps. All terms are formally defined, and the objectives are quantifiable.\n\nThe task is to implement a computational workflow for analyzing and correcting inter-laboratory bias in mRNA count data using external spike-in standards. This involves several steps: calculating log fold changes (LFCs), estimating a lab-specific bias scaling factor ($\\kappa$), calibrating the observed gene LFCs, and evaluating the impact of this calibration on gene rankings and cross-laboratory reproducibility.\n\nThe foundational constants and formulae are as follows:\n- The pseudocount for handling zero counts is $\\epsilon = 0.5$.\n- The observed Log Fold Change ($LFC^{obs}$) is defined for each gene or spike-in as:\n$$LFC = \\log_{2}\\left(\\frac{\\bar{T} + \\epsilon}{\\bar{C} + \\epsilon}\\right)$$\nwhere $\\bar{T}$ and $\\bar{C}$ are the arithmetic means of the treatment and control replicate counts, respectively.\n\nThe core of the analysis proceeds in three stages as specified.\n\n**1. Within-Laboratory Data Processing and Calibration**\n\nFor each laboratory dataset provided in a test case, the following procedure is applied:\n\na. **Spike-in LFC Calculation**: The $LFC^{obs}$ is calculated for each spike-in standard using its control and treatment replicate counts and the formula above.\n\nb. **Bias Factor Estimation ($\\hat{\\kappa}$)**: The problem posits a linear relationship between the observed spike-in LFCs ($LFC^{obs}_i$) and their known true LFCs ($LFC^{true}_i$), modeled as $LFC^{obs}_i \\approx \\kappa \\cdot LFC^{true}_i$. The lab-specific scaling factor $\\kappa$ is estimated via a regression-through-the-origin least squares method. The objective is to find $\\hat{\\kappa}$ that minimizes the sum of squared residuals, $\\sum_{i}\\left(LFC^{obs}_{i} - \\hat{\\kappa} \\cdot LFC^{true}_{i}\\right)^{2}$. The solution to this minimization problem is given by:\n$$\\hat{\\kappa} = \\dfrac{\\sum_{i} LFC^{true}_{i} \\cdot LFC^{obs}_{i}}{\\sum_{i} \\left(LFC^{true}_{i}\\right)^{2}}$$\nThis estimator, $\\hat{\\kappa}$, represents the systematic multiplicative bias of the laboratory's measurement process on the log-fold-change scale. The denominator is non-zero as the provided $LFC^{true}$ values are $[-1, 0, 1]$, making the sum of squares $2$.\n\nc. **Gene LFC Calculation**: The $LFC^{obs}$ is calculated for each endogenous gene using its respective replicate counts.\n\nd. **Gene LFC Calibration**: The observed gene LFCs are calibrated to correct for the estimated bias. The calibrated LFC ($LFC^{cal}$) is computed by dividing the observed LFC by the estimated scaling factor:\n$$LFC^{cal} = \\frac{LFC^{obs}}{\\hat{\\kappa}}$$\nThis procedure rescales the observed effects to what they would presumably be in an unbiased measurement system where $\\kappa=1$.\n\n**2. Within-Laboratory Rank Stability Analysis**\n\nTo assess the impact of calibration on the interpretation of gene expression changes within a single lab, the relative ordering of genes is compared before and after calibration. Genes are ranked based on the absolute magnitude of their LFC values, as this metric is often used to prioritize genes for further study. The specific ranking rule is:\n- Primary sort key: Descending order of the absolute LFC value ($|LFC|$).\n- Secondary sort key (tie-breaker): Ascending lexicographical order of the gene name.\n\nTwo ranked lists of gene names are generated for each lab: one using $|LFC^{obs}|$ and another using $|LFC^{cal}|$. These two lists are compared. If they are not identical, it indicates that the calibration procedure has altered the relative importance of at least one gene, and a boolean flag for change is set to true for that lab. For each test case, the final change indicator is the logical OR of the flags from the two labs.\n\n**3. Cross-Laboratory Reproducibility Assessment**\n\nA key goal of normalization is to improve the consistency of results between independent experiments. This is quantified by measuring the cross-lab agreement of gene LFCs before and after calibration. The Pearson correlation coefficient is used for this purpose.\n\nLet $x$ and $y$ be the vectors of $LFC^{obs}$ for all genes from Lab A and Lab B, respectively, maintaining a consistent gene order. The pre-calibration reproducibility, $r_{\\text{pre}}$, is:\n$$r_{\\text{pre}} = \\dfrac{\\operatorname{Cov}(x, y)}{\\sigma_{x}\\sigma_{y}}$$\nwhere $\\operatorname{Cov}(x, y)$ is the covariance of $x$ and $y$, and $\\sigma_{x}$ and $\\sigma_{y}$ are their standard deviations.\n\nSimilarly, let $x^{cal}$ and $y^{cal}$ be the vectors of $LFC^{cal}$ values. The post-calibration reproducibility, $r_{\\text{post}}$, is:\n$$r_{\\text{post}} = \\dfrac{\\operatorname{Cov}\\left(x^{cal}, y^{cal}\\right)}{\\sigma_{x^{cal}}\\sigma_{y^{cal}}}$$\n\nThe improvement in reproducibility is reported as the difference $\\Delta r = r_{\\text{post}} - r_{\\text{pre}}$. A positive $\\Delta r$ signifies that the spike-in calibration has made the gene LFC measurements more concordant across the two laboratories.\n\nThe described methodology will be systematically applied to each of the three test cases to generate the required output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis for all test cases.\n    \"\"\"\n    epsilon = 0.5\n\n    # Define the test cases from the problem statement.\n    gene_control_c1 = np.array([[190, 210], [175, 185], [145, 155], [215, 225], [95, 105], [295, 305], [75, 85], [390, 410]], dtype=float)\n    spike_control_c1 = np.array([[98, 102], [98, 102], [98, 102]], dtype=float)\n\n    gene_control_c2 = np.array([[145, 155], [190, 210], [175, 185], [95, 105], [215, 225]], dtype=float)\n    spike_control_c2 = np.array([[98, 102], [98, 102], [98, 102]], dtype=float)\n\n    test_cases = [\n        { # Case 1\n            \"gene_names\": [f\"G{i}\" for i in range(1, 9)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"Lab1\": {\n                    \"gene_c\": gene_control_c1,\n                    \"gene_t\": np.array([[850, 865], [470, 480], [90, 95], [215, 225], [30, 32], [580, 595], [11, 12], [530, 540]], dtype=float),\n                    \"spike_c\": spike_control_c1,\n                    \"spike_t\": np.array([[38, 38], [98, 102], [260, 268]], dtype=float)\n                },\n                \"Lab2\": {\n                    \"gene_c\": gene_control_c1,\n                    \"gene_t\": np.array([[455, 465], [310, 315], [112, 115], [215, 225], [50, 53], [440, 445], [26, 27], [470, 475]], dtype=float),\n                    \"spike_c\": spike_control_c1,\n                    \"spike_t\": np.array([[57, 58], [98, 102], [173, 175]], dtype=float)\n                }\n            }\n        },\n        { # Case 2\n            \"gene_names\": [f\"G{i}\" for i in range(1, 6)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"LabA\": {\n                    \"gene_c\": gene_control_c2,\n                    \"gene_t\": np.array([[214, 218], [137, 140], [178, 182], [205, 208], [105, 108]], dtype=float),\n                    \"spike_c\": spike_control_c2,\n                    \"spike_t\": np.array([[48, 49], [98, 102], [206, 209]], dtype=float)\n                },\n                \"LabB\": {\n                    \"gene_c\": gene_control_c2,\n                    \"gene_t\": np.array([[207, 210], [142, 145], [177, 183], [192, 195], [113, 115]], dtype=float),\n                    \"spike_c\": spike_control_c2,\n                    \"spike_t\": np.array([[51, 53], [98, 102], [192, 194]], dtype=float)\n                }\n            }\n        },\n        { # Case 3\n            \"gene_names\": [f\"G{i}\" for i in range(1, 6)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"LabA\": {\n                    \"gene_c\": np.array([[115, 125], [58, 62], [195, 205], [78, 82], [148, 152]], dtype=float),\n                    \"gene_t\": np.array([[292, 298], [8, 10], [260, 265], [42, 44], [148, 152]], dtype=float),\n                    \"spike_c\": np.array([[98, 102], [98, 102], [98, 102]], dtype=float),\n                    \"spike_t\": np.array([[40, 41], [98, 102], [244, 248]], dtype=float)\n                },\n                \"LabB\": {\n                    \"gene_c\": np.array([[115, 125], [0, 60], [195, 205], [78, 82], [148, 152]], dtype=float),\n                    \"gene_t\": np.array([[193, 197], [11, 12], [230, 234], [56, 58], [148, 152]], dtype=float),\n                    \"spike_c\": np.array([[98, 102], [98, 102], [98, 102]], dtype=float),\n                    \"spike_t\": np.array([[61, 62], [98, 102], [161, 163]], dtype=float)\n                }\n            }\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        case_result = _process_case(case, epsilon)\n        results.append(case_result)\n\n    # The required output format is a \"stringified\" list of lists with no spaces.\n    print(str(results).replace(' ', ''))\n\ndef _calculate_lfc(treatment_reps, control_reps, epsilon):\n    \"\"\"Computes log2 fold change with a pseudocount.\"\"\"\n    mean_t = np.mean(treatment_reps, axis=1)\n    mean_c = np.mean(control_reps, axis=1)\n    # Add epsilon to numerator and denominator to prevent log(0) and division by zero.\n    return np.log2((mean_t + epsilon) / (mean_c + epsilon))\n\ndef _process_lab(gene_names, lab_data, spike_lfc_true, epsilon):\n    \"\"\"Processes data for a single lab to get LFCs and ranking change.\"\"\"\n    # Step 1a: Compute observed LFC for spike-ins\n    spike_lfc_obs = _calculate_lfc(lab_data[\"spike_t\"], lab_data[\"spike_c\"], epsilon)\n    \n    # Step 1b: Estimate kappa\n    kappa_num = np.sum(spike_lfc_true * spike_lfc_obs)\n    kappa_den = np.sum(spike_lfc_true**2)\n    kappa_hat = kappa_num / kappa_den\n    \n    # Step 1c: Compute observed LFC for genes\n    gene_lfc_obs = _calculate_lfc(lab_data[\"gene_t\"], lab_data[\"gene_c\"], epsilon)\n    \n    # Step 1d: Calibrate gene LFCs\n    gene_lfc_cal = gene_lfc_obs / kappa_hat\n    \n    # Step 2: Determine if gene ranking changes\n    # Rank by descending absolute value, tie-break by ascending gene name\n    lfc_obs_with_names = list(zip(gene_names, gene_lfc_obs))\n    lfc_cal_with_names = list(zip(gene_names, gene_lfc_cal))\n    \n    # Create the ranking key: a tuple of (-abs(LFC), gene_name)\n    rank_obs = sorted(lfc_obs_with_names, key=lambda x: (-abs(x[1]), x[0]))\n    rank_cal = sorted(lfc_cal_with_names, key=lambda x: (-abs(x[1]), x[0]))\n    \n    # Extract just the gene names to check for ranking changes\n    ranked_names_obs = [item[0] for item in rank_obs]\n    ranked_names_cal = [item[0] for item in rank_cal]\n    \n    ranking_changed = (ranked_names_obs != ranked_names_cal)\n    \n    return gene_lfc_obs, gene_lfc_cal, ranking_changed\n\ndef _process_case(case_data, epsilon):\n    \"\"\"Processes a full test case with two labs.\"\"\"\n    lab_names = list(case_data[\"labs\"].keys())\n    lab_a_name, lab_b_name = lab_names[0], lab_names[1]\n\n    gene_names = case_data[\"gene_names\"]\n    spike_lfc_true = case_data[\"spike_lfc_true\"]\n\n    data_a = case_data[\"labs\"][lab_a_name]\n    lfc_obs_a, lfc_cal_a, changed_a = _process_lab(gene_names, data_a, spike_lfc_true, epsilon)\n    \n    data_b = case_data[\"labs\"][lab_b_name]\n    lfc_obs_b, lfc_cal_b, changed_b = _process_lab(gene_names, data_b, spike_lfc_true, epsilon)\n    \n    # Step 2 (final): Aggregate ranking change flag\n    case_changed = changed_a or changed_b\n    \n    # Step 3: Quantify cross-lab reproducibility improvement\n    # Using np.corrcoef which returns a 2x2 matrix. The value at [0,1] is the Pearson r.\n    r_pre = np.corrcoef(lfc_obs_a, lfc_obs_b)[0, 1]\n    r_post = np.corrcoef(lfc_cal_a, lfc_cal_b)[0, 1]\n    \n    delta_r = r_post - r_pre\n    \n    return [delta_r, case_changed]\n\nsolve()\n\n```"
        }
    ]
}