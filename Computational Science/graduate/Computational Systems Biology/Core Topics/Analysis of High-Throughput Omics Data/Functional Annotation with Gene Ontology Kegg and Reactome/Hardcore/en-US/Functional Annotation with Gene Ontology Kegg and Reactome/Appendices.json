{
    "hands_on_practices": [
        {
            "introduction": "Performing enrichment analysis across multiple functional categories from databases like Gene Ontology, KEGG, and Reactome inevitably involves testing thousands of hypotheses. This practice will guide you through the critical process of adjusting raw $p$-values to control for the high risk of false positives that arises from multiple comparisons . By applying both the classic Bonferroni correction and the modern Benjamini-Hochberg procedure, you will develop a quantitative understanding of the trade-off between statistical power and error control.",
            "id": "3312231",
            "problem": "A researcher performs gene set enrichment analysis for a differentially expressed gene list using three standardized annotation resources: Gene Ontology Biological Process (GO BP), Kyoto Encyclopedia of Genes and Genomes (KEGG), and Reactome Pathway Knowledgebase (Reactome). For a combined panel of $m=12$ candidate categories across these resources, each tested by a one-sided hypergeometric enrichment test under standard assumptions, the resulting raw $p$-values (already corrected for the combinatorial sample space of each test but not for multiple hypotheses across categories) are:\n$0.0008$, $0.0015$, $0.0042$, $0.0070$, $0.0110$, $0.0190$, $0.0280$, $0.0370$, $0.0490$, $0.1200$, $0.3300$, $0.6200$.\n\nAssume that exactly $m_0=9$ of the $m=12$ tested category-level hypotheses are true nulls and that the null $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$.\n\nUsing only first-principles definitions of family-wise error control and false discovery rate control:\n- Compute the Bonferroni-adjusted $p$-values for all $m=12$ tests and determine the number of rejected hypotheses at family-wise significance threshold $\\alpha_{\\text{FWER}}=0.05$.\n- Compute the Benjamini–Hochberg (BH) adjusted $p$-values for all $m=12$ tests and determine the number of rejected hypotheses at target false discovery rate $q=0.10$.\n\nThen, under the independence and uniformity assumptions specified:\n- Derive the expected number of false rejections for the Bonferroni procedure at $\\alpha_{\\text{FWER}}=0.05$.\n- Derive the bound on the expected proportion of false discoveries (false discovery rate) attained by the Benjamini–Hochberg procedure at $q=0.10$.\n\nFinally, define the dimensionless comparison metric\n$$\\rho \\equiv \\frac{E[V_{\\text{Bonf}}]/m}{\\left(\\frac{m_0}{m}\\right) q},$$\nwhere $E[V_{\\text{Bonf}}]$ is the expected number of false rejections under Bonferroni. Compute $\\rho$ exactly and report the single value of $\\rho$ as an exact analytic fraction. Do not include any units, and do not round.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. All necessary data, definitions, and assumptions for a unique and meaningful solution are provided. The problem is therefore deemed **valid**.\n\nThe problem requires the application of multiple hypothesis testing corrections (Bonferroni and Benjamini-Hochberg) to a set of $p$-values, followed by the derivation and calculation of related statistical quantities. Let the total number of tests be $m=12$, and let the provided raw $p$-values, already sorted in non-decreasing order, be denoted by $p_{(i)}$ for $i=1, 2, \\dots, 12$. The family-wise significance threshold is $\\alpha_{\\text{FWER}}=0.05$, and the target false discovery rate is $q=0.10$. The number of true null hypotheses is given as $m_0=9$.\n\n**1. Bonferroni Correction**\n\nThe Bonferroni correction controls the family-wise error rate (FWER) by adjusting each raw $p$-value, $p_i$. The Bonferroni-adjusted $p$-value, $p_{i, \\text{adj}}^{\\text{Bonf}}$, is defined as:\n$$p_{i, \\text{adj}}^{\\text{Bonf}} = \\min(m \\cdot p_i, 1)$$\nA hypothesis is rejected if its adjusted $p$-value is less than or equal to the FWER threshold $\\alpha_{\\text{FWER}}$. This is equivalent to rejecting any hypothesis for which the raw $p$-value $p_i \\le \\alpha_{\\text{FWER}} / m$.\n\nGiven $m=12$ and $\\alpha_{\\text{FWER}}=0.05$, the rejection threshold for raw $p$-values is $0.05/12 \\approx 0.004167$.\nThe provided raw $p$-values are:\n$p_{(1)} = 0.0008$, $p_{(2)} = 0.0015$, $p_{(3)} = 0.0042$, $p_{(4)} = 0.0070$, $p_{(5)} = 0.0110$, $p_{(6)} = 0.0190$, $p_{(7)} = 0.0280$, $p_{(8)} = 0.0370$, $p_{(9)} = 0.0490$, $p_{(10)} = 0.1200$, $p_{(11)} = 0.3300$, $p_{(12)} = 0.6200$.\n\nThe Bonferroni-adjusted $p$-values are calculated as $p_{(i), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\cdot p_{(i)}, 1)$:\n$p_{(1), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0008 = 0.0096$\n$p_{(2), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0015 = 0.0180$\n$p_{(3), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0042 = 0.0504$\n$p_{(4), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0070 = 0.0840$\n$p_{(5), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0110 = 0.1320$\n$p_{(6), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0190 = 0.2280$\n$p_{(7), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0280 = 0.3360$\n$p_{(8), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0370 = 0.4440$\n$p_{(9), \\text{adj}}^{\\text{Bonf}} = 12 \\times 0.0490 = 0.5880$\n$p_{(10), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\times 0.1200, 1) = \\min(1.44, 1) = 1.0$\n$p_{(11), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\times 0.3300, 1) = \\min(3.96, 1) = 1.0$\n$p_{(12), \\text{adj}}^{\\text{Bonf}} = \\min(12 \\times 0.6200, 1) = \\min(7.44, 1) = 1.0$\n\nTo find the number of rejected hypotheses, we compare each adjusted $p$-value to $\\alpha_{\\text{FWER}} = 0.05$:\n$p_{(1), \\text{adj}}^{\\text{Bonf}} = 0.0096 \\le 0.05$ (Reject)\n$p_{(2), \\text{adj}}^{\\text{Bonf}} = 0.0180 \\le 0.05$ (Reject)\n$p_{(3), \\text{adj}}^{\\text{Bonf}} = 0.0504 > 0.05$ (Do not reject)\nAll subsequent adjusted $p$-values are also greater than $0.05$.\nThus, the Bonferroni procedure rejects $2$ hypotheses.\n\n**2. Benjamini-Hochberg (BH) Correction**\n\nThe Benjamini-Hochberg procedure controls the False Discovery Rate (FDR). The BH-adjusted $p$-value, $p_{(i), \\text{adj}}^{\\text{BH}}$, for the $i$-th sorted raw $p$-value $p_{(i)}$ is computed using a step-down procedure that enforces monotonicity. The adjusted value for the largest raw $p$-value is the raw $p$-value itself: $p_{(m), \\text{adj}}^{\\text{BH}}=p_{(m)}$. For all other $p$-values, the adjustment is made recursively:\n$$p_{(i), \\text{adj}}^{\\text{BH}} = \\min\\left(p_{(i+1), \\text{adj}}^{\\text{BH}}, \\frac{m \\cdot p_{(i)}}{i}\\right) \\quad \\text{for } i=m-1, \\dots, 1$$\nThis is equivalent to $p_{(i), \\text{adj}}^{\\text{BH}} = \\min_{j \\ge i}\\left(\\frac{m \\cdot p_{(j)}}{j}\\right)$.\n\nLet's compute the BH-adjusted $p$-values:\n$p_{(12), \\text{adj}}^{\\text{BH}} = p_{(12)} = 0.6200$\n$p_{(11), \\text{adj}}^{\\text{BH}} = \\min(p_{(12), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.3300}{11}) = \\min(0.6200, 0.3600) = 0.3600$\n$p_{(10), \\text{adj}}^{\\text{BH}} = \\min(p_{(11), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.1200}{10}) = \\min(0.3600, 0.1440) = 0.1440$\n$p_{(9), \\text{adj}}^{\\text{BH}} = \\min(p_{(10), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0490}{9}) = \\min(0.1440, 0.0653\\overline{3}) = 0.0653\\overline{3}$\n$p_{(8), \\text{adj}}^{\\text{BH}} = \\min(p_{(9), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0370}{8}) = \\min(0.0653\\overline{3}, 0.0555) = 0.0555$\n$p_{(7), \\text{adj}}^{\\text{BH}} = \\min(p_{(8), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0280}{7}) = \\min(0.0555, 0.0480) = 0.0480$\n$p_{(6), \\text{adj}}^{\\text{BH}} = \\min(p_{(7), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0190}{6}) = \\min(0.0480, 0.0380) = 0.0380$\n$p_{(5), \\text{adj}}^{\\text{BH}} = \\min(p_{(6), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0110}{5}) = \\min(0.0380, 0.0264) = 0.0264$\n$p_{(4), \\text{adj}}^{\\text{BH}} = \\min(p_{(5), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0070}{4}) = \\min(0.0264, 0.0210) = 0.0210$\n$p_{(3), \\text{adj}}^{\\text{BH}} = \\min(p_{(4), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0042}{3}) = \\min(0.0210, 0.0168) = 0.0168$\n$p_{(2), \\text{adj}}^{\\text{BH}} = \\min(p_{(3), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0015}{2}) = \\min(0.0168, 0.0090) = 0.0090$\n$p_{(1), \\text{adj}}^{\\text{BH}} = \\min(p_{(2), \\text{adj}}^{\\text{BH}}, \\frac{12 \\cdot 0.0008}{1}) = \\min(0.0090, 0.0096) = 0.0090$\n\nA hypothesis is rejected if its BH-adjusted $p$-value is less than or equal to the target FDR $q=0.10$.\n$p_{(1), \\text{adj}}^{\\text{BH}}$ through $p_{(9), \\text{adj}}^{\\text{BH}}$ are all less than or equal to $0.10$. $p_{(10), \\text{adj}}^{\\text{BH}} = 0.1440 > 0.10$.\nThus, the Benjamini-Hochberg procedure rejects $9$ hypotheses.\n\n**3. Expected Number of False Rejections (Bonferroni)**\n\nLet $V_{\\text{Bonf}}$ be the number of false rejections (Type I errors) under the Bonferroni procedure. These are true null hypotheses that are incorrectly rejected. Let $H_0$ be the set of $m_0$ true null hypotheses.\n$$V_{\\text{Bonf}} = \\sum_{i \\in H_0} \\mathbb{I}\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. By the linearity of expectation, the expected number of false rejections is:\n$$E[V_{\\text{Bonf}}] = E\\left[\\sum_{i \\in H_0} \\mathbb{I}\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)\\right] = \\sum_{i \\in H_0} E\\left[\\mathbb{I}\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)\\right] = \\sum_{i \\in H_0} P\\left(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}\\right)$$\nUnder the assumption that $p$-values for true null hypotheses are independently and identically distributed as $\\mathrm{Uniform}(0,1)$, the probability of rejecting a single true null is $P(p_i \\le \\frac{\\alpha_{\\text{FWER}}}{m}) = \\frac{\\alpha_{\\text{FWER}}}{m}$.\nSince there are $m_0$ such hypotheses, the expected number of false rejections is:\n$$E[V_{\\text{Bonf}}] = m_0 \\cdot \\frac{\\alpha_{\\text{FWER}}}{m}$$\nSubstituting the given values: $m_0=9$, $m=12$, and $\\alpha_{\\text{FWER}}=0.05$:\n$$E[V_{\\text{Bonf}}] = 9 \\times \\frac{0.05}{12} = \\frac{9 \\times 0.05}{12} = \\frac{0.45}{12} = 0.0375 = \\frac{3}{80}$$\n\n**4. Bound on False Discovery Rate (Benjamini-Hochberg)**\n\nThe False Discovery Rate (FDR) is defined as the expected proportion of false discoveries among all discoveries, $E[V/R]$, where $V$ is the number of false rejections and $R$ is the total number of rejections ($V/R$ is defined as $0$ if $R=0$). The Benjamini-Hochberg procedure, when applied at a level $q$, guarantees that for independent tests (as assumed here), the FDR is controlled:\n$$\\text{FDR} = E\\left[\\frac{V}{R}\\right] \\le \\frac{m_0}{m}q$$\nThe problem asks for the bound attained by the procedure. This bound is the right-hand side of the inequality.\nSubstituting the given values: $m_0=9$, $m=12$, and $q=0.10$:\n$$\\text{Bound} = \\frac{9}{12} \\times 0.10 = \\frac{3}{4} \\times 0.10 = 0.75 \\times 0.10 = 0.075 = \\frac{3}{40}$$\n\n**5. Calculation of the Comparison Metric $\\rho$**\n\nThe dimensionless comparison metric $\\rho$ is defined as:\n$$\\rho \\equiv \\frac{E[V_{\\text{Bonf}}]/m}{\\left(\\frac{m_0}{m}\\right) q}$$\nWe substitute the derived expression for $E[V_{\\text{Bonf}}]$ from part 3:\n$$E[V_{\\text{Bonf}}] = m_0 \\frac{\\alpha_{\\text{FWER}}}{m}$$\nThe expression for $\\rho$ becomes:\n$$\\rho = \\frac{\\left(m_0 \\frac{\\alpha_{\\text{FWER}}}{m}\\right) / m}{\\left(\\frac{m_0}{m}\\right) q} = \\frac{\\frac{m_0 \\alpha_{\\text{FWER}}}{m^2}}{\\frac{m_0 q}{m}}$$\nSimplifying the expression by canceling terms:\n$$\\rho = \\frac{m_0 \\alpha_{\\text{FWER}}}{m^2} \\cdot \\frac{m}{m_0 q} = \\frac{\\alpha_{\\text{FWER}}}{m \\cdot q}$$\nNow, we compute the exact value of $\\rho$ using the given constants $\\alpha_{\\text{FWER}}=0.05$, $m=12$, and $q=0.10$:\n$$\\rho = \\frac{0.05}{12 \\times 0.10} = \\frac{0.05}{1.2} = \\frac{5/100}{12/10} = \\frac{5}{100} \\times \\frac{10}{12} = \\frac{50}{1200} = \\frac{1}{24}$$",
            "answer": "$$\\boxed{\\frac{1}{24}}$$"
        },
        {
            "introduction": "Gene Ontology's hierarchical structure, a Directed Acyclic Graph (DAG), means that enrichment signals can propagate from specific child terms to their broader ancestors, often leading to redundant findings. This exercise provides a hands-on demonstration of this issue and a principled way to solve it, using a simplified parent-child conditional testing approach . By working through this small-scale example, you will see how to distinguish true enrichment from inherited statistical artifacts, a key skill for accurate functional interpretation.",
            "id": "3312252",
            "problem": "A laboratory is annotating a small gene set to a toy Directed Acyclic Graph (DAG) of the Gene Ontology (GO). The goal is to compare naive Over-Representation Analysis (ORA) with a parent–child conditional approach on this DAG and quantify the difference in the number of significant terms called by the two methods.\n\nUse the following scientifically plausible setup:\n- The annotated gene universe is $12$ genes, $G = \\{g1, g2, \\dots, g12\\}$.\n- The GO DAG consists of terms $\\{T1, T2, T3, T4, T5, T6, T7\\}$ with parent–child relationships:\n  - $T1$ (root).\n  - $T2$ is a child of $T1$.\n  - $T3$ is a child of $T1$.\n  - $T4$ is a child of $T2$.\n  - $T5$ is a child of $T2$.\n  - $T6$ is a child of both $T2$ and $T3$ (multi-parent term).\n  - $T7$ is a child of $T1$.\n- Genes are directly annotated to the following leaf terms (and annotations propagate upward to all ancestors in the DAG):\n  - $T4$: $\\{g1, g2, g3\\}$.\n  - $T5$: $\\{g4, g5\\}$.\n  - $T6$: $\\{g6, g7, g8, g9\\}$.\n  - $T7$: $\\{g10, g11, g12\\}$.\n  Consequently, by propagation:\n  - $T2$ contains $\\{g1, g2, g3, g4, g5, g6, g7, g8, g9\\}$.\n  - $T3$ contains $\\{g6, g7, g8, g9\\}$.\n  - $T1$ contains all $12$ genes.\n- The study list (the query set) is $L = \\{g1, g6, g7, g8, g9\\}$.\n\nTasks to perform:\n1. Naive Over-Representation Analysis (ORA):\n   - For each term $T \\in \\{T3, T4, T5, T6\\}$, compute the one-sided enrichment $p$-value using the standard hypergeometric model with the population defined as the annotated universe ($N = |T1|$), term size $K = |T|$ (with propagated annotations), list size $n = |L|$, and observed overlap $k = |L \\cap T|$. Use the upper-tail convention “at least as many or more” matches.\n2. Parent–child approach (union variant):\n   - For each term $T \\in \\{T3, T4, T5, T6\\}$ with parents $\\mathrm{Pa}(T)$, define the conditioning set $S_{\\mathrm{parent}}(T)$ as the union of the genes annotated to all parents of $T$. Compute the one-sided enrichment $p$-value using the hypergeometric model but restricted to the population $N' = |S_{\\mathrm{parent}}(T)|$, with $K' = |T|$, $n' = |L \\cap S_{\\mathrm{parent}}(T)|$, and $k' = |L \\cap T|$.\n3. Multiple testing:\n   - Apply a Bonferroni correction across the $m = 4$ tested terms $\\{T3, T4, T5, T6\\}$, using a family-wise error rate $\\alpha = 0.05$. A term is called significant if its unadjusted $p$-value is at most $\\alpha/m$.\n4. Report the number of significant terms under naive ORA and under the parent–child approach. Then compute the difference defined as “naive ORA count minus parent–child count.”\n\nExpress the final answer as a single integer. No rounding is required. No units are required.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the field of computational systems biology, well-posed with all necessary data and definitions, and internally consistent. The task is to perform and compare two standard types of Gene Ontology (GO) enrichment analysis.\n\nThe analysis proceeds by first performing a naive Over-Representation Analysis (ORA), followed by a parent–child conditional analysis. The significance of GO terms is determined using the one-sided (upper-tail) hypergeometric test. The probability of observing at least $k$ successes in $n$ draws without replacement, from a population of size $N$ containing $K$ successes, is given by:\n$$p = P(X \\ge k) = \\sum_{i=k}^{\\min(n,K)} \\frac{\\binom{K}{i}\\binom{N-K}{n-i}}{\\binom{N}{n}}$$\nWe are asked to test $m=4$ terms: $\\{T_3, T_4, T_5, T_6\\}$. With a family-wise error rate of $\\alpha=0.05$, the Bonferroni-corrected significance threshold for each unadjusted $p$-value is $\\frac{\\alpha}{m} = \\frac{0.05}{4} = 0.0125$. A term is called significant if its $p$-value is less than or equal to this threshold.\n\nThe provided gene sets are:\n- Gene universe $G = \\{g_1, g_2, \\dots, g_{12}\\}$.\n- Study list $L = \\{g_1, g_6, g_7, g_8, g_9\\}$.\n- GO term annotations (propagated):\n  - $T_1 = \\{g_1, \\dots, g_{12}\\}$, so $|T_1| = 12$.\n  - $T_2 = \\{g_1, \\dots, g_9\\}$, so $|T_2| = 9$.\n  - $T_3 = \\{g_6, g_7, g_8, g_9\\}$, so $|T_3| = 4$.\n  - $T_4 = \\{g_1, g_2, g_3\\}$, so $|T_4| = 3$.\n  - $T_5 = \\{g_4, g_5\\}$, so $|T_5| = 2$.\n  - $T_6 = \\{g_6, g_7, g_8, g_9\\}$, so $|T_6| = 4$.\n\n### 1. Naive Over-Representation Analysis (ORA)\n\nFor naive ORA, the population is the entire gene universe $T_1$.\n- Population size: $N = |T_1| = 12$.\n- List size (number of draws): $n = |L| = 5$.\n\nWe analyze each term:\n\n**Term $T_3$:**\n- Term size (successes in population): $K = |T_3| = 4$.\n- Observed overlap: $k = |L \\cap T_3| = |\\{g_6, g_7, g_8, g_9\\}| = 4$.\n- The parameters are $N=12, K=4, n=5, k=4$.\n- The $p$-value is $P(X \\ge 4) = P(X=4) = \\frac{\\binom{4}{4}\\binom{12-4}{5-4}}{\\binom{12}{5}} = \\frac{\\binom{4}{4}\\binom{8}{1}}{\\binom{12}{5}}$.\n- $\\binom{12}{5} = \\frac{12 \\cdot 11 \\cdot 10 \\cdot 9 \\cdot 8}{5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 792$.\n- $p_{T_3} = \\frac{1 \\cdot 8}{792} = \\frac{1}{99} \\approx 0.0101$.\n- Since $0.0101 \\le 0.0125$, term $T_3$ is **significant**.\n\n**Term $T_4$:**\n- Term size: $K = |T_4| = 3$.\n- Observed overlap: $k = |L \\cap T_4| = |\\{g_1\\}| = 1$.\n- The parameters are $N=12, K=3, n=5, k=1$.\n- The $p$-value is $P(X \\ge 1) = 1 - P(X=0) = 1 - \\frac{\\binom{3}{0}\\binom{12-3}{5-0}}{\\binom{12}{5}} = 1 - \\frac{\\binom{3}{0}\\binom{9}{5}}{792}$.\n- $\\binom{9}{5} = \\binom{9}{4} = \\frac{9 \\cdot 8 \\cdot 7 \\cdot 6}{4 \\cdot 3 \\cdot 2 \\cdot 1} = 126$.\n- $p_{T_4} = 1 - \\frac{1 \\cdot 126}{792} = \\frac{792 - 126}{792} = \\frac{666}{792} = \\frac{37}{44} \\approx 0.8409$.\n- Since $0.8409 > 0.0125$, term $T_4$ is **not significant**.\n\n**Term $T_5$:**\n- Term size: $K = |T_5| = 2$.\n- Observed overlap: $k = |L \\cap T_5| = |\\emptyset| = 0$.\n- The parameters are $N=12, K=2, n=5, k=0$.\n- We are testing for over-representation (upper-tail). An observation of $k=0$ cannot be over-represented. The $p$-value is $P(X \\ge 0) = 1$.\n- Since $1 > 0.0125$, term $T_5$ is **not significant**.\n\n**Term $T_6$:**\n- Term size: $K = |T_6| = 4$.\n- Observed overlap: $k = |L \\cap T_6| = |\\{g_6, g_7, g_8, g_9\\}| = 4$.\n- The parameters are $N=12, K=4, n=5, k=4$. This is identical to the calculation for $T_3$.\n- $p_{T_6} = \\frac{1}{99} \\approx 0.0101$.\n- Since $0.0101 \\le 0.0125$, term $T_6$ is **significant**.\n\nThe number of significant terms under naive ORA is $2$ (terms $T_3$ and $T_6$).\n\n### 2. Parent–child Conditional Analysis\n\nHere, the population for a term $T$ is restricted to the genes in the union of its parents, $S_{\\mathrm{parent}}(T) = \\bigcup_{P \\in \\mathrm{Pa}(T)} P$.\n\n**Term $T_3$:**\n- Parent: $\\mathrm{Pa}(T_3)=\\{T_1\\}$.\n- Conditioning set: $S_{\\mathrm{parent}}(T_3) = T_1$.\n- This reduces to the naive ORA case. $N'=12, K'=4, n'=5, k'=4$.\n- $p'_{T_3} = \\frac{1}{99} \\approx 0.0101$.\n- Since $0.0101 \\le 0.0125$, term $T_3$ is **significant**.\n\n**Term $T_4$:**\n- Parent: $\\mathrm{Pa}(T_4)=\\{T_2\\}$.\n- Conditioning set: $S_{\\mathrm{parent}}(T_4) = T_2$.\n- New population size: $N' = |T_2| = 9$.\n- New list: $L \\cap T_2 = \\{g_1, g_6, g_7, g_8, g_9\\} \\cap \\{g_1, \\dots, g_9\\} = L$.\n- New list size: $n' = |L| = 5$.\n- Term size: $K' = |T_4| = 3$.\n- Observed overlap: $k' = |L \\cap T_4| = 1$.\n- The parameters are $N'=9, K'=3, n'=5, k'=1$.\n- $p'_{T_4} = P(X \\ge 1) = 1 - P(X=0) = 1 - \\frac{\\binom{3}{0}\\binom{9-3}{5-0}}{\\binom{9}{5}} = 1 - \\frac{\\binom{3}{0}\\binom{6}{5}}{\\binom{9}{5}}$.\n- $\\binom{9}{5} = 126$. $\\binom{6}{5} = 6$.\n- $p'_{T_4} = 1 - \\frac{1 \\cdot 6}{126} = 1 - \\frac{1}{21} = \\frac{20}{21} \\approx 0.9524$.\n- Since $0.9524 > 0.0125$, term $T_4$ is **not significant**.\n\n**Term $T_5$:**\n- Parent: $\\mathrm{Pa}(T_5)=\\{T_2\\}$.\n- The conditioning set is $T_2$, so the parameters are $N'=9, n'=5$.\n- Term size: $K' = |T_5| = 2$.\n- Observed overlap: $k' = |L \\cap T_5| = 0$.\n- $p'_{T_5} = P(X \\ge 0) = 1$.\n- Since $1 > 0.0125$, term $T_5$ is **not significant**.\n\n**Term $T_6$:**\n- Parents: $\\mathrm{Pa}(T_6)=\\{T_2, T_3\\}$.\n- Conditioning set: $S_{\\mathrm{parent}}(T_6) = T_2 \\cup T_3$. Since $T_3$ is a subset of $T_2$ ($T_3 \\subset T_2$), their union is $T_2$.\n- New population size: $N' = |T_2| = 9$.\n- New list size: $n' = |L \\cap T_2| = 5$.\n- Term size: $K' = |T_6| = 4$.\n- Observed overlap: $k' = |L \\cap T_6| = 4$.\n- The parameters are $N'=9, K'=4, n'=5, k'=4$.\n- $p'_{T_6} = P(X \\ge 4) = P(X=4) = \\frac{\\binom{4}{4}\\binom{9-4}{5-4}}{\\binom{9}{5}} = \\frac{\\binom{4}{4}\\binom{5}{1}}{\\binom{9}{5}}$.\n- $p'_{T_6} = \\frac{1 \\cdot 5}{126} = \\frac{5}{126} \\approx 0.0397$.\n- Since $0.0397 > 0.0125$, term $T_6$ is **not significant**.\n\nThe number of significant terms under the parent–child approach is $1$ (term $T_3$ only).\n\n### 3. Comparison and Final Answer\n\n- Number of significant terms (naive ORA): $2$.\n- Number of significant terms (parent–child): $1$.\n- The difference is defined as “naive ORA count minus parent–child count”.\n- Difference = $2 - 1 = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "A successful enrichment analysis often produces a long list of significant GO terms, many of which are semantically related and represent overlapping biological concepts. This advanced practice introduces a computational workflow to manage this redundancy by calculating semantic similarity, clustering related terms, and selecting a concise set of representatives . Completing this exercise will equip you with the skills to transform a raw result list into a structured, interpretable summary of the underlying biological themes.",
            "id": "3312268",
            "problem": "You are given a simplified Directed Acyclic Graph (DAG) that models a subset of the Gene Ontology (GO), intended for computational functional annotation. For completeness and context, the Kyoto Encyclopedia of Genes and Genomes (KEGG) and Reactome are other pathway resources, but this problem focuses on the Gene Ontology DAG semantics. The graph encodes the Biological Process branch with integer-labeled terms from $1$ to $12$, where each term may have zero or more parents. The DAG obeys the True Path Rule, meaning annotations to child terms also imply annotations to all ancestor terms. The problem requires you to construct a semantic similarity matrix for a set of GO terms under the Resnik definition, normalize it to a distance, and then propose a clustering with representative selection that reduces redundancy while preserving coverage of biological processes, measured as coverage of leaf processes. All quantities and steps must be derived from first principles given below and the provided data; no external databases are permitted.\n\nFoundational definitions to be used:\n- The Gene Ontology DAG is specified by parent relationships. Each term $t$ has a set of parents $\\mathrm{Pa}(t)$, and ancestors $\\mathrm{Anc}(t)$ defined recursively as $t$ itself plus ancestors of its parents. The DAG is acyclic and has a unique root at $t=1$.\n- The True Path Rule defines the count $c(t)$ for a term $t$ as the total number of annotations to all leaf descendants of $t$ (including $t$ if it is a leaf). The total annotation count is $C=\\sum_{\\ell \\in L}c(\\ell)$, where $L$ is the set of leaf terms.\n- The probability of a term is $p(t)=\\frac{c(t)}{C}$, and its Information Content is $IC(t)=-\\ln p(t)$ using the natural logarithm.\n- For two terms $a$ and $b$, the Most Informative Common Ancestor (MICA) is the ancestor in $\\mathrm{Anc}(a)\\cap \\mathrm{Anc}(b)$ with maximal $IC$; the Resnik semantic similarity is $s_{\\mathrm{Resnik}}(a,b)=IC(\\mathrm{MICA}(a,b))$.\n- To bound similarity to $[0,1]$ for clustering, use the normalized similarity $s(a,b)=\\frac{s_{\\mathrm{Resnik}}(a,b)}{\\max(IC(a),IC(b))}$, with the convention that $s(a,b)=0$ if $\\max(IC(a),IC(b))=0$.\n- Define a distance $d(a,b)=1-s(a,b)$.\n- Form a threshold graph on a given set $S$ of terms by connecting $a,b\\in S$ with an undirected edge if $d(a,b)\\le \\theta$, where $\\theta$ is a supplied threshold. Clusters are the connected components of this threshold graph.\n\nCoverage of biological processes:\n- Let $L$ be the leaves of the DAG. The leaf coverage set for any set of terms $X$ is $L(X)=\\bigcup_{t\\in X}L(t)$ where $L(t)$ is the set of leaves reachable from $t$ along child edges.\n- The coverage fraction of a representative set $R$ relative to the original set $S$ is $\\gamma(R,S)=\\frac{|L(R)|}{|L(S)|}$, with the convention $\\gamma(R,S)=1$ if $|L(S)|=0$.\n- Representative selection must reduce redundancy while preserving coverage: choose at most one representative per cluster initially to maximize coverage, and then greedily add additional terms across any clusters if needed until $\\gamma(R,S)\\ge \\alpha$, for a supplied coverage threshold $\\alpha$ expressed as a decimal. Within a cluster, choose the initial representative that maximizes $|L(\\{t\\})|$ intersected with $L(S)$; break ties by larger $IC$, then by smaller term index. For subsequent greedy additions, always choose the remaining term with largest marginal increase in $|L(R)|$; break ties by larger $IC$, then smaller index. The final representative list for each test case must be sorted in increasing term index.\n\nThe DAG and base leaf annotation counts are specified as follows:\n- Parent relationships (each pair $(u,v)$ means $u$ is the parent of $v$):\n  $(1,2)$, $(1,3)$, $(2,4)$, $(2,5)$, $(3,6)$, $(3,7)$, $(3,12)$, $(4,8)$, $(5,9)$, $(6,10)$, $(7,11)$.\n- Leaves are $8$, $9$, $10$, $11$, $12$.\n- Base leaf annotation counts are:\n  $c(8)=50$, $c(9)=70$, $c(10)=120$, $c(11)=90$, $c(12)=80$.\n- The total count is $C=50+70+120+90+80$.\n\nYour program must:\n1. Compute $c(t)$ for all $t\\in\\{1,\\ldots,12\\}$ by summing leaf counts over descendants.\n2. Compute $p(t)$ and $IC(t)$ for all terms.\n3. For each test case set $S$, build the normalized Resnik similarity matrix $[s(a,b)]_{a,b\\in S}$ and the corresponding distance matrix $[d(a,b)]_{a,b\\in S}$.\n4. Cluster $S$ into connected components of the threshold graph using the supplied $\\theta$.\n5. Select representatives as defined above to achieve coverage fraction at least $\\alpha$.\n6. Produce the final output as a single line containing the results for all test cases as a comma-separated list enclosed in square brackets. Each result must be a list of integers representing the selected representatives for that test case, sorted in increasing order. No spaces are allowed anywhere in this output.\n\nTest suite:\n- Test case $1$: $S=[3,4,5,6,7]$, $\\theta=0.8$, $\\alpha=0.8$.\n- Test case $2$: $S=[4,6]$, $\\theta=0.5$, $\\alpha=1.0$.\n- Test case $3$: $S=[3,6,10]$, $\\theta=0.8$, $\\alpha=0.67$.\n- Test case $4$: $S=[4,4,5,5]$, $\\theta=0.42$, $\\alpha=1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the sorted list of representative term indices for the corresponding test case. For example, a valid output format is $[[a_1,a_2],[b_1],[c_1,c_2,c_3],[d_1]]$. The output must contain no spaces.",
            "solution": "The problem requires the implementation of a multi-step bioinformatic algorithm to cluster Gene Ontology (GO) terms and select representative terms based on semantic similarity and process coverage. The problem is well-posed, scientifically grounded in the principles of computational functional genomics, and algorithmically specified. We will proceed with a systematic, principle-based solution.\n\nThe overall approach involves four main stages:\n1.  **System-wide Pre-computation**: We first process the given Directed Acyclic Graph (DAG) structure and annotation counts to compute properties for all terms, which are independent of the specific test cases.\n2.  **Pairwise Similarity and Distance Calculation**: For each test case, we compute a distance matrix for the given set of terms $S$.\n3.  **Clustering**: We use the distance matrix and a threshold $\\theta$ to group the terms in $S$ into clusters.\n4.  **Representative Selection**: We apply the specified two-phase algorithm to select a subset of representative terms $R \\subseteq S$ that satisfies a given coverage threshold $\\alpha$.\n\n**1. System-wide Pre-computation**\n\nFirst, we represent the GO hierarchy as a graph. The parent relationships define a graph with terms $\\{1, \\dots, 12\\}$ as nodes. For each term $t$, we can determine its set of parents $\\mathrm{Pa}(t)$, children $\\mathrm{Ch}(t)$, ancestors $\\mathrm{Anc}(t)$, and leaf descendants $L(t)$. The set of ancestors $\\mathrm{Anc}(t)$ is defined as $\\{t\\} \\cup \\bigcup_{p \\in \\mathrm{Pa}(t)} \\mathrm{Anc}(p)$. The set of leaf descendants $L(t)$ is found by traversing child-edges from $t$ until leaf nodes are reached. The set of all leaf terms in the graph is $L = \\{8, 9, 10, 11, 12\\}$.\n\nThe base annotation counts $c(\\ell)$ are given for each leaf term $\\ell \\in L$: $c(8)=50$, $c(9)=70$, $c(10)=120$, $c(11)=90$, $c(12)=80$. The total annotation count is $C = \\sum_{\\ell \\in L} c(\\ell) = 50+70+120+90+80 = 410$.\n\nAccording to the True Path Rule, the count for any term $t$, $c(t)$, is the sum of the base counts of all its leaf descendants:\n$$c(t) = \\sum_{\\ell \\in L(t)} c(\\ell)$$\nUsing this rule, we compute the counts for all non-leaf terms:\n- $c(7) = c(11) = 90$\n- $c(6) = c(10) = 120$\n- $c(5) = c(9) = 70$\n- $c(4) = c(8) = 50$\n- $c(3) = c(6) + c(7) + c(12) = 120 + 90 + 80 = 290$\n- $c(2) = c(4) + c(5) = 50 + 70 = 120$\n- $c(1) = c(2) + c(3) = 120 + 290 = 410$\n\nThe probability of a term $t$ is $p(t) = c(t)/C$. The Information Content (IC), a measure of specificity, is then calculated using the natural logarithm:\n$$IC(t) = -\\ln p(t) = -\\ln\\left(\\frac{c(t)}{C}\\right)$$\nFor example, $IC(1) = -\\ln(410/410) = 0$, and $IC(4) = -\\ln(50/410) \\approx 2.1041$. These IC values are pre-computed for all $12$ terms.\n\n**2. Pairwise Semantic Similarity and Distance Calculation**\n\nFor each pair of terms $(a, b)$ in a given set $S$, we calculate their semantic similarity. The Resnik similarity is defined by the Information Content of their Most Informative Common Ancestor (MICA). The MICA is the term $t_{\\mathrm{MICA}}$ in the set of common ancestors $\\mathrm{Anc}(a) \\cap \\mathrm{Anc}(b)$ that has the highest IC value.\n$$t_{\\mathrm{MICA}}(a,b) = \\arg\\max_{t \\in \\mathrm{Anc}(a)\\cap \\mathrm{Anc}(b)} IC(t)$$\nThe provided DAG is a tree, where each node other than the root has a single parent. In a tree, the MICA is uniquely identified as the Lowest Common Ancestor (LCA), since IC values strictly decrease as one ascends the tree from the LCA towards the root. The Resnik similarity is:\n$$s_{\\mathrm{Resnik}}(a,b) = IC(t_{\\mathrm{MICA}}(a,b))$$\nTo facilitate comparison and clustering, this similarity is normalized:\n$$s(a,b) = \\frac{s_{\\mathrm{Resnik}}(a,b)}{\\max(IC(a), IC(b))}$$\nwith the convention $s(a,b)=0$ if the denominator is zero. Finally, a distance metric is defined as:\n$$d(a,b) = 1 - s(a,b)$$\nThese distances are computed for all pairs in the set $S$ for a given test case, forming a symmetric distance matrix.\n\n**3. Clustering**\n\nUsing the computed distance matrix for a set $S$, we construct a threshold graph. An undirected edge is created between any two distinct terms $a,b \\in S$ if their distance $d(a,b)$ is less than or equal to a given threshold $\\theta$.\nThe clusters are then defined as the connected components of this threshold graph. We can find these components using standard graph traversal algorithms like Breadth-First Search (BFS) or Depth-First Search (DFS).\n\n**4. Representative Selection**\n\nThe goal is to select a representative subset $R \\subseteq S$ that reduces redundancy while maintaining biological process coverage. The process is as follows:\n\nFirst, calculate the total leaf coverage of the input set $S$, $L(S) = \\bigcup_{t \\in S} L(t)$.\n\nThe selection proceeds in two phases:\n-   **Initial Selection**: For each cluster found in the previous step, exactly one representative term is chosen. The selected representative $t^*$ from a cluster $C_i$ is the one that maximizes the size of its leaf set intersection with the total leaf set of S: $t^* = \\arg\\max_{t \\in C_i} |L(t) \\cap L(S)|$. Ties are resolved by selecting the term with the larger $IC$, and further ties by the smaller term index. This yields an initial representative set $R$.\n-   **Greedy Addition**: We calculate the coverage fraction $\\gamma(R,S) = |L(R)| / |L(S)|$, where $L(R) = \\bigcup_{t \\in R} L(t)$. If $\\gamma(R,S)$ is less than the required coverage threshold $\\alpha$, we iteratively add more terms to $R$. In each step, we select a term $t' \\in S \\setminus R$ that provides the largest marginal gain in leaf coverage, i.e., maximizes $|L(t') \\setminus L(R)|$. Ties are again broken by larger $IC$, and then by smaller term index. This continues until $\\gamma(R,S) \\ge \\alpha$ or no more terms can be added to increase coverage.\n\nThe final set of representatives $R$ for each test case is then sorted in increasing order of term index to produce the final result.",
            "answer": "$$\\boxed{[[3,6,7],[4,6],[3,10],[4,5]]}$$"
        }
    ]
}