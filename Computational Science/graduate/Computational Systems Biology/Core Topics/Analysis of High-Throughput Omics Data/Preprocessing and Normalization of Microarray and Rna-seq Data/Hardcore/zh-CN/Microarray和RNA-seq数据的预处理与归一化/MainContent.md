## 引言
高通量测序技术，特别是[微阵列](@entry_id:270888)和[RNA测序](@entry_id:178187)（[RNA-seq](@entry_id:140811)），彻底改变了我们研究基因表达和调控的能力。然而，这些技术在生成海量数据的同时，也不可避免地引入了源于实验过程的系统性技术偏差，如[批次效应](@entry_id:265859)、文库制备差异和测量误差。这些非生物学因素如果得不到妥善处理，会掩盖真实的生物学信号，甚至导致错误的科学结论。因此，严谨的[数据预处理](@entry_id:197920)与标准化，便成为连接原始测量数据与可靠生物学洞见的不可或缺的桥梁。

本文旨在系统性地解决这一关键问题，为研究人员提供一个从原理到实践的全面指南。我们将超越简单的操作步骤，深入探讨这些[标准化](@entry_id:637219)方法背后的统计学根基，阐明为何某些方法适用于特定数据类型，以及它们的核心假设和局限性。

通过本文，您将首先在“原理与机制”一章中，学习[微阵列](@entry_id:270888)和[RNA-seq](@entry_id:140811)数据生成过程的[统计模型](@entry_id:165873)、各类偏差的识别方法以及核心的[标准化](@entry_id:637219)算法。随后，在“应用与跨学科联系”一章中，我们将展示这些原理如何应用于构建分析流程、处理复杂变异、整合[多源](@entry_id:170321)数据以及应对[单细胞分析](@entry_id:274805)等前沿挑战。最后，通过“动手实践”部分，您将有机会亲手操作，巩固对关键[标准化](@entry_id:637219)技术的理解。让我们从深入理解数据背后的统计机制开始，为精准的生物学发现奠定坚实的基础。

## 原理与机制

本章将深入探讨高通量基因表达[数据预处理](@entry_id:197920)与[标准化](@entry_id:637219)的核心科学原理和统计机制。与介绍性章节不同，我们将直接进入技术细节，剖析从原始测量值到可用于下游生物学分析的[标准化](@entry_id:637219)表达矩阵过程中所涉及的关键步骤。我们将系统地阐述如何对测量误差进行建模，如何识别和校正系统性偏差，以及如何确保样本间的可比性，同时也会探讨实验设计中的关键考量。

### 测量过程的建模：从信号到数据

任何严谨的数据分析都始于对数据生成过程的深刻理解。无论是[微阵列](@entry_id:270888)的荧[光强度](@entry_id:177094)还是[RNA测序](@entry_id:178187)（[RNA-seq](@entry_id:140811)）的读数计数，原始数据都是真实生物信号与一系列技术伪影复杂相互作用的产物。

#### [微阵列](@entry_id:270888)强度模型

在[微阵列](@entry_id:270888)实验中，探针的观测强度$X$并非直接等同于其靶标基因的真实表达丰度$\mu$。一个被广泛接受的测量模型将观测强度分解为多个组成部分，清晰地反映了杂交和扫描过程中的物理现实 。该模型可以表示为：

$X = \alpha \mu \epsilon + B$

其中：
- $\mu$ 是我们真正关心的、未知的真实表达丰度。
- $\alpha$ 是一个芯片特异性的[乘性](@entry_id:187940)因子，代表了全局性的技术尺度差异，例如扫描仪增益或总体染料标记效率。
- $\epsilon$ 是一个乘性[测量误差](@entry_id:270998)项，通常假设其对数服从[正态分布](@entry_id:154414)，即$\log \epsilon \sim \mathcal{N}(0, \sigma^2)$。这种对数正态误差能够很好地描述信号依赖的、比例性的波动。
- $B$ 是一个加性背景项，代表了非特异性杂交、扫描仪[暗电流](@entry_id:154449)和荧光背景噪声的综合效应。通常，它被建模为一个正态分布的[随机变量](@entry_id:195330)，$B \sim \mathcal{N}(b, \tau^2)$。

另一个用于[背景校正](@entry_id:200834)的[生成模型](@entry_id:177561)，例如在RMA (Robust Multi-array Average) 算法中所见，将观测强度$X$建模为真实信号$S$和背景噪声$B$的直接加和：$X = S + B$ 。在这个模型中，背景$B$被假定为[正态分布](@entry_id:154414)，$B \sim \mathcal{N}(\mu, \sigma^2)$，这一假设的理论基础是中心极限定理（CLT），即背景噪声是众多微小、独立的随机因素累加的结果。而真实信号$S$则被建模为指数分布，$S \sim \mathrm{Exp}(\lambda)$。指数分布确保了信号的非负性，并且其重右尾特性（即大多数信号值较低，少数信号值非常高）与基因表达强度的[经验分布](@entry_id:274074)非常吻合 [@problem_id:3339362, E]。通过这种建模，我们可以从观测值$X$中推断出真实信号$S$的[期望值](@entry_id:153208)$\mathbb{E}[S|X=x]$，从而实现对背景噪声的校正。该推断过程涉及对[正态分布](@entry_id:154414)在非负区间上的截断，其后验[期望值](@entry_id:153208)的精确表达式为 $\mathbb{E}[S \mid X=x]=x-\mu-\lambda\sigma^2+\sigma\,\frac{\phi\left(\frac{x-\mu-\lambda\sigma^2}{\sigma}\right)}{\Phi\left(\frac{x-\mu-\lambda\sigma^2}{\sigma}\right)}$，其中$\phi(\cdot)$和$\Phi(\cdot)$分别是标准正态分布的[概率密度](@entry_id:175496)和累积分布函数 [@problem_id:3339362, A, B]。

#### 对数转换的基本原理

[微阵列](@entry_id:270888)分析中的一个标准步骤是对强度值进行对数转换。这一操作有着深刻的统计学动机。在上述$X = \alpha\mu\epsilon + B$模型中，如果背景项$B$可以忽略不计（即在高强度区域），则[模型简化](@entry_id:171175)为$X \approx \alpha\mu\epsilon$。进行对数转换后，我们得到：

$Y = \log X \approx \log \alpha + \log \mu + \log \epsilon$

这个简单的转换实现了两个重要目标：
1.  **误差结构的加性化**：原本的乘性误差$\epsilon$在对数尺度上变为了加性误差$\log\epsilon$。
2.  **[方差](@entry_id:200758)稳定化**：原始强度$X$的[方差](@entry_id:200758)$\mathrm{Var}(X)$与其均值（信号强度）的平方成正比，这是一种典型的[异方差性](@entry_id:136378)（heteroscedasticity）。经过对数转换后，只要背景可忽略，转换后数据$Y$的[方差](@entry_id:200758)$\mathrm{Var}(Y) = \mathrm{Var}(\log\epsilon) = \sigma^2$，成为一个与信号强度$\mu$无关的常数。这种[同方差性](@entry_id:634679)（homoscedasticity）是许多标准统计检验（如[t检验](@entry_id:272234)、ANOVA）的基本假设 [@problem_id:3339380, A]。
3.  **尺度因子的线性化**：芯片间的[乘性](@entry_id:187940)尺度因子$\alpha$在对数尺度上转变为一个加性偏移量$\log\alpha$。这使得芯片间的[标准化](@entry_id:637219)可以通过简单的中心化操作（例如，减去每张芯片上所有探针强度的中位数）来实现，而不会扭曲芯片内部的[倍数变化](@entry_id:272598)关系 [@problem_id:3339380, F]。

然而，对数转换的[方差](@entry_id:200758)稳定化效果并非完美。在低强度区域，加性背景噪声$B$（尤其是其[方差](@entry_id:200758)$\tau^2$）变得不可忽略。利用一阶泰勒展开（即[Delta方法](@entry_id:276272)）可以证明，此时对数转换后强度的[方差近似](@entry_id:268585)为 $\mathrm{Var}(Y) \approx \mathrm{Var}(X) / \{\mathrm{E}[X]\}^2$。当信号$\mu$趋近于零时，$\mathrm{E}[X]$减小，导致$\mathrm{Var}(Y)$显著增大。这解释了为什么在对数尺度的[微阵列](@entry_id:270888)数据中，低强度探针的[方差](@entry_id:200758)往往会“膨胀”，重新引入了[异方差性](@entry_id:136378) [@problem_id:3339380, C]。

#### RNA-seq计数数据的统计模型

与[微阵列](@entry_id:270888)的连续强度值不同，[RNA-seq](@entry_id:140811)实验产生的是离散的读数计数值。对计数[数据建模](@entry_id:141456)的最简单方法是[泊松分布](@entry_id:147769)，其核心特征是均值等于[方差](@entry_id:200758)，即$\mathrm{Var}(Y) = \mathbb{E}[Y]$。然而，在真实的生物学重复样本中，我们几乎总是观察到**[过度离散](@entry_id:263748)（overdispersion）**的现象，即计数的[方差](@entry_id:200758)远大于其均值。

[过度离散](@entry_id:263748)的根本原因在于潜在表达率的异质性。我们可以构建一个层级模型来理解这一点：假设在给定一个样本中，某个基因的真实表达率（或采样率）为$\Lambda$，则该基因的读数计数$Y$服从[泊松分布](@entry_id:147769)，$Y \mid \Lambda \sim \mathrm{Poisson}(\Lambda)$。然而，这个潜在表达率$\Lambda$在不同的生物学重复样本之间并非恒定，而是会因为生物学变异（例如，不同个体间[基因调控](@entry_id:143507)的细微差异）和技术变异（例如，文库制备效率的波动）而变化。因此，$\Lambda$本身是一个[随机变量](@entry_id:195330)。

根据**[全方差定律](@entry_id:184705)（law of total variance）**，我们可以推导出$Y$的边际[方差](@entry_id:200758) ：

$\mathrm{Var}(Y) = \mathbb{E}[\mathrm{Var}(Y \mid \Lambda)] + \mathrm{Var}(\mathbb{E}[Y \mid \Lambda]) = \mathbb{E}[\Lambda] + \mathrm{Var}(\Lambda)$

由于$\mathbb{E}[Y] = \mathbb{E}[\mathbb{E}[Y \mid \Lambda]] = \mathbb{E}[\Lambda]$，上式可以改写为：

$\mathrm{Var}(Y) = \mathbb{E}[Y] + \mathrm{Var}(\Lambda)$

这个公式清晰地揭示了[过度离散](@entry_id:263748)的来源：计数的总[方差](@entry_id:200758)等于其均值（泊松采样过程的内在[方差](@entry_id:200758)）加上潜在表达率的[方差](@entry_id:200758)。只要潜在表达率在样本间存在任何波动（即$\mathrm{Var}(\Lambda)>0$），[过度离散](@entry_id:263748)现象就必然发生 [@problem_id:3339388, A, D]。

**负二项分布**是描述[过度离散](@entry_id:263748)计数数据的标准模型。它可以通过一个特定的层级模型推导出来：假设潜在表达率$\Lambda$服从伽马[分布](@entry_id:182848)，即$\Lambda \sim \mathrm{Gamma}(r, r/\mu)$，而计数$Y$在给定$\Lambda$时服从泊松分布。这种伽马-泊松[混合分布](@entry_id:276506)的结果就是[负二项分布](@entry_id:262151)。其均值为$\mathbb{E}[Y]=\mu$，[方差](@entry_id:200758)为$\mathrm{Var}(Y)=\mu+\mu^2/r$。这里的参数$r$（或其倒数）被称为[离散度](@entry_id:168823)参数，它直接与$\mathrm{Var}(\Lambda)$相关。当$r \to \infty$时，$\mathrm{Var}(\Lambda) \to 0$，[负二项分布](@entry_id:262151)退化为泊松分布 [@problem_id:3339388, E]。

### 系统性偏差的识别与校正

高通量实验不可避免地会引入各种非生物学来源的系统性偏差。[预处理](@entry_id:141204)的核心任务之一就是识别并移除这些偏差。

#### 芯片[内部标准化](@entry_id:181400)（[微阵列](@entry_id:270888)）

对于双色c[DNA微阵列](@entry_id:274679)，一个关键的诊断工具是**MA-图**。该图将每个探针的[对数倍数变化](@entry_id:272578) $M = \log_{2}(R/G)$（$R$和$G$分别是红绿两种染料的荧[光强度](@entry_id:177094)）与平均对数强度 $A = \frac{1}{2}\log_{2}(RG)$ 进行可视化。在理想情况下（即两个样本间无[差异表达](@entry_id:748396)，且无染料偏差），所有数据点应围绕$M=0$水平线对称[分布](@entry_id:182848)。

然而，在实践中，由于染料标记效率、扫描仪响应等因素对强度存在依赖性，MA-图经常呈现出弯曲的趋势 [@problem_id:3339373, A]。例如，如果红色通道的检测器在高强度区域比绿色通道更容易饱和，那么对于高表达的基因（高A值），观测到的比率$R/G$会系统性地下降，导致MA-图的右侧向下弯曲 [@problem_id:3339373, E]。

校正这种强度依赖性偏差的经典方法是**局部加权散点平滑（LOWESS或LOESS）**。该方法在MA-图上拟合一条平滑曲线$\hat{h}(A)$来估计$M$对$A$的系统性依赖关系，然后从每个探针的$M$值中减去该趋势：$M_{\text{norm}} = M - \hat{h}(A)$。这个过程有效地将MA-图“拉直”，使数据点的中心趋势回到$M=0$，同时保留了$A$值，从而不扭曲强度尺度。需要注意的是，简单的全局缩放（即用常数分别乘以所有$R$值和$G$值）只能校正恒定的全局染料偏差，无法移除与强度相关的曲线形偏差 [@problem_id:3339373, B, C]。

#### 基因特异性偏差校正（[RNA-seq](@entry_id:140811)）

[RNA-seq](@entry_id:140811)数据也存在其特有的、与基因本身属性相关的偏差。

- **位置偏差（5'-3'偏差）**：理想情况下，测序读数应[均匀分布](@entry_id:194597)在转录本的全长上。然而，许多文库制备方法会引入位置偏差。最常见的**3'端偏差**，其主要机制包括：
    1.  **Oligo-dT[引物](@entry_id:192496)起始**：在富集[poly(A)尾](@entry_id:274750)mRNA的文库制备中，逆转录从3'端开始。逆转录酶的有限[持续合成能力](@entry_id:274928)（processivity）可能导致其在到达5'端之前脱落，从而使得文库中富含源自3'端的cDNA片段 [@problem_id:3339365, A]。
    2.  **RNA降解**：样本中预先存在的RNA降解会产生大量包含3'端[poly(A)尾](@entry_id:274750)的截短转录本片段，这些片段在poly(A)筛选后被保留，导致3'端覆盖度偏高 [@problem_id:3339365, C]。
    3.  **实验设计**：某些技术，如许多单细胞3'端计数（3'-tagging）方案，其设计初衷就是特异性地捕获和测序转录本的3'末端，以实现对转录本的计数而非全长测序 [@problem_id:3339365, E]。

  评估这种偏差需要定义不受转录本长度和[测序深度](@entry_id:178191)影响的指标。例如，可以计算归一化覆盖度[曲线的斜率](@entry_id:178976)、5'端和3'端区域覆盖度的积分差异，或覆盖度与归一化位置之间的[Spearman秩相关系数](@entry_id:177168)。这些指标在均匀覆盖下应接近于零，并随偏差程度单调变化，从而为质量控制提供量化依据 。

- **序列内容偏差（[GC含量](@entry_id:275315)偏差）**：基因或转录本的[GC含量](@entry_id:275315)会影响[RNA-seq](@entry_id:140811)流程的多个步骤。[GC含量](@entry_id:275315)较高的双链DNA片段更稳定，解链温度更高，这会影响PCR扩增的效率。此外，极端的[GC含量](@entry_id:275315)也可能影响测序过程中的碱基识别质量和后续的读数比对准确性。这些效应共同导致了一种基因特异性的、通常呈[非线性](@entry_id:637147)的样本依赖性偏差 。

  为了校正这种复杂的偏差，需要使用更高级的[标准化](@entry_id:637219)方法。**条件分位数标准化（Conditional Quantile Normalization, CQN）**是一种专门为此设计的策略。CQN的核心思想是，在一个样本内部，基因的表达量不应该与其[GC含量](@entry_id:275315)或长度等技术性协变量系统相关。CQN通过灵活的回归模型（如样条回归）来估计并移除在对数尺度上观测到的计数值对[GC含量](@entry_id:275315)和基因长度的系统性依赖。具体来说，它首先为每个样本拟合这种依赖关系，然后计算残差，最后对这些消除了协变量效应的残差进行分位数[标准化](@entry_id:637219)，从而在保留真实生物学差异的同时，校正了基因特异性的技术偏差 [@problem_id:3339408, A]。

### 实现样本间可比性：样本间标准化

在校正了样本内部的偏差后，下一步是确保不同样本（例如，来自不同个体或处理条件的重复）之间的表达量具有可比性。

#### 文库组成偏差与TMM[标准化](@entry_id:637219)

一个常见的误区是认为只需将每个样本的读数计数除以其总读数（即文库大小），就可以实现样本间的[标准化](@entry_id:637219)。这种方法忽略了**文库组成偏差**。想象一下，如果在一个样本中，少数几个基因的表达量极高，它们将“吃掉”测序总读数的很大一部分。这会导致其余所有基因的读数计数被系统性地压低，即使它们的真实表达水平与其他样本相比并无变化。

**M值的修剪均值（Trimmed Mean of M-values, TMM）**[标准化](@entry_id:637219)方法就是为了解决这一问题而设计的 。TMM假设大多[数基](@entry_id:634389)因在样本间并非[差异表达](@entry_id:748396)。它通过计算一个稳健的缩放因子来校正文库组成效应。其步骤如下：
1.  选择一个参考样本。
2.  对于每个待测样本，计算每个基因相对于参考样本的[对数倍数变化](@entry_id:272578)（$M_g = \log_2(\frac{y_{gi}/N_i}{y_{gr}/N_r})$）和平均对数丰度（$A_g$）。
3.  为了获得稳健的估计，TMM会“修剪”掉一部分基因：一部分是$M_g$值极端（很可能是真正[差异表达](@entry_id:748396)的基因）的基因，另一部分是$A_g$值极端（通常是低丰度、高变异）的基因。
4.  对剩余基因的$M_g$值进行加权平均，其中权重与计数的[方差](@entry_id:200758)成反比（即高丰度、低[方差](@entry_id:200758)的基因获得更高权重），得到一个对整体对数倍数偏移量的[稳健估计](@entry_id:261282)$\hat{m}$。
5.  最终的缩放因子为 $f_i = 2^{\hat{m}}$。这个因子随后被用于调整样本的有效文库大小，而不是直接替换总读数。这样，下游的[差异表达分析](@entry_id:266370)就能更准确地反映真实的生物学变化 [@problem_id:3339445, A]。

#### [分位数](@entry_id:178417)标准化及其关键假设

**分位数[标准化](@entry_id:637219)（Quantile Normalization）**是一种更为激进的标准化方法。它的目标是使所有样本的表达值具有完全相同的统计分布。该过程通过将每个样本中表达值排序，计算所有样本在相同[分位数](@entry_id:178417)上的平均值，然后用这个平均值替换每个样本原始的分位数上的值来实现。

这种方法的强大之处在于它能有效移除各种复杂的、[非线性](@entry_id:637147)的技术偏差。然而，它的力量也来源于一个非常强的核心假设：**绝大多[数基](@entry_id:634389)因在所比较的样本组之间没有[差异表达](@entry_id:748396)**，并且观测到的[分布](@entry_id:182848)差异主要是由技术因素造成的 。

在某些生物学场景中，这个假设可能不成立。例如，当一个处理（如药物治疗或[转录因子](@entry_id:137860)敲除）引起了全局性的转录重编程，导致大量基因的表达发生系统性上调或下调时，[分位数](@entry_id:178417)标准化可能会错误地将这种真实的生物学信号当作技术偏差而移除。

因此，在应用[分位数](@entry_id:178417)[标准化](@entry_id:637219)之前，批判性地评估其假设的合理性至关重要。我们可以设计一些检验来检测全局性的表达[分布偏移](@entry_id:638064)：
- **[非参数检验](@entry_id:176711)**：可以在[标准化](@entry_id:637219)之前，将每个实验组内的所有基因表达值汇集起来，构建组水平的[经验累积分布函数](@entry_id:167083)（ECDF）。然后，使用双样本Kolmogorov-Smirnov (KS)检验来比较这两组的[分布](@entry_id:182848)。为了获得可靠的[p值](@entry_id:136498)，应通过[置换](@entry_id:136432)样本标签来构建KS统计量的[零分布](@entry_id:195412)，这种[置换检验](@entry_id:175392)能够很好地处理基因间的复杂相关性 [@problem_id:3339463, A]。
- **利用对照基因**：如果在实验中加入了已知浓度的**外部RNA对照联盟（ERCC）**spike-in，它们可以作为完美的“地标”。由于spike-in的添加量在所有样本中是相同的，其测量值的[分布](@entry_id:182848)在不同实验组间不应有系统性差异。通过检验spike-in表达[分布](@entry_id:182848)是否存在显著的组间偏移，可以判断是否存在全局性的技术偏差，从而评估对内源基因应用分位数[标准化](@entry_id:637219)的风险 [@problem_id:3339463, D]。

### 高级视角与实验设计

#### RNA-seq数据的组分性质

深入思考RNA-seq数据的本质，我们会发现它具有**组分数据（compositional data）**的特性。每个样本的读数计数向量$\boldsymbol{x}=(x_{1}, \dots, x_{D})$受到总读数$N=\sum_{i=1}^{D} x_{i}$的约束。这个总数$N$（文库大小）是一个技术参数，而非生物学量的直接反映。这意味着我们能从数据中可靠推断的，仅仅是各个基因的**[相对丰度](@entry_id:754219)**，而非它们的绝对丰度。一个基因计数的增加必然导致其他基因[相对丰度](@entry_id:754219)的下降，这体现了组分数据的“闭合”特性 。

处理组分数据的经典方法论（由John Aitchison开创）建议使用对数比率转换来“打开”这个闭合空间，将数据投影到标准的[欧几里得空间](@entry_id:138052)中，以便应用标准的统计方法。**中心对数比率（Centered Log-Ratio, CLR）**变换是其中一种关键的变换，其定义为：

$T_i(\boldsymbol{x}) = \ln(x_i) - \frac{1}{D}\sum_{j=1}^{D} \ln(x_j)$

该变换通过从每个基因的对数值中减去该样本所有基因对数值的均值（即对数[几何均值](@entry_id:275527)）来实现。CLR变换具有两个重要性质：(1) **尺度不变性**：当所有计数乘以一个常数（如文库大小变化）时，CLR变换后的值保持不变；(2) **对称性**：它同等地对待所有基因。CLR变换将数据映射到一个满足$\sum_{i=1}^{D} T_i(\boldsymbol{x}) = 0$约束的[子空间](@entry_id:150286)中，为更高级的[统计建模](@entry_id:272466)提供了坚实基础 。

#### 混杂与可识别性

在所有高通量实验中，最危险的陷阱之一是**混杂（confounding）**。当一个我们关心的生物学变量与一个技术变量完全或高度相关时，混杂就发生了。一个经典的例子是，所有A组的样本都在第一批次处理，而所有B组的样本都在第二批次处理。在这种**完全混杂**的设计中，生物学效应（A组 vs. B组）与批次效应（批次1 vs. 批次2）变得无法区分 。

从统计模型的角度看，这导致了参数的**不可识别性（non-identifiability）**。在一个旨在同时估计生物学效应$\beta_g$和[批次效应](@entry_id:265859)$\gamma_g$的线性模型（或[广义线性模型](@entry_id:171019)）中，完全混杂使得模型的[设计矩阵](@entry_id:165826)列[线性相关](@entry_id:185830)（即矩阵不是满秩的）。这意味着有无穷多组$(\beta_g, \gamma_g)$的组合能够同样好地拟[合数](@entry_id:263553)据，我们永远无法将观测到的差异唯一地归因于生物学还是技术因素 [@problem_id:3339386, A]。

重要的是要认识到，不可识别性是一个实验设计层面的根本缺陷，它无法通过增加样本量或[测序深度](@entry_id:178191)来解决 [@problem_id:3339386, D]。同样，标准的[标准化](@entry_id:637219)方法，如TMM或ERCC spike-in normalization，也无法解决这个问题，因为它们不改变[设计矩阵](@entry_id:165826)的结构 [@problem_id:3339386, B, G]。

解决混杂问题的最佳策略在于**实验设计**：
- **平衡与[随机化](@entry_id:198186)**：最有效的策略是确保每个批次中都包含所有生物学条件的样本，即采用平衡设计。通过随机分配样本到不同批次，可以打破生物学条件和批次之间的关联 [@problem_id:3339386, A]。
- **桥接样本**：如果在不同批次中包含相同的样本（例如，将同一个生物样本的等分试样分配到多个批次中），可以创建“桥梁”，直接估计批次间的差异，从而解开混杂 [@problem_id:3339386, E]。

如果实验设计已经存在缺陷，一些计算方法可以尝试“拯救”数据，但它们都依赖于很强的假设。例如，**利用阴性对照基因去除不必要变异（RUVg）**的方法，假设存在一组已知不受生物学条件影响的基因（阴性对照基因）。通过这些基因，RUVg可以估计出潜在的技术变异（如批次效应）的结构，并将其从所有基因的表达值中移除。然而，这种方法的成功完全取决于能否找到真正可靠的阴性对照基因 [@problem_id:3339386, C]。