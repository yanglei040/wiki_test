{
    "hands_on_practices": [
        {
            "introduction": "In modern computational systems biology, we often work with datasets where the number of features (e.g., genes, $p$) vastly exceeds the number of samples (e.g., cells, $n$). This exercise explores a fundamental consequence of this $p \\gg n$ regime on Principal Component Analysis. By examining the algebraic structure of the sample covariance matrix, you will discover the mathematical reason why the number of meaningful principal components is limited by the number of samples, not the number of genes, providing a crucial piece of intuition for interpreting high-dimensional PCA results.",
            "id": "3302515",
            "problem": "You are analyzing a transcriptomics dataset in computational systems biology in which gene expression levels for $p$ genes are measured across $n$ biological samples, with $p \\gg n$. Let $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ denote the sample-by-gene data matrix whose $(i,j)$ entry is the expression level of gene $j$ in sample $i$. Let $\\mathbf{X}_{c}$ denote the column-centered matrix obtained by subtracting, for each gene $j$, its sample mean from $\\mathbf{X}$ so that each column of $\\mathbf{X}_{c}$ has mean $0$ across samples. Consider the feature-feature sample covariance matrix\n$$\n\\mathbf{S} \\;=\\; \\frac{1}{n-1}\\,\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c} \\;\\in\\; \\mathbb{R}^{p \\times p},\n$$\nthe object that is diagonalized in Principal Component Analysis (PCA), where Principal Component Analysis (PCA) is the orthogonal decomposition of feature variance along principal directions defined by the eigenvectors of $\\mathbf{S}$. Using only fundamental linear algebraic definitions and properties of centering and covariance, determine the maximum possible number of nonzero eigenvalues of $\\mathbf{S}$ in the regime $p \\gg n$. Justify the limitation by explicitly explaining the linear dependence structure among the columns of $\\mathbf{X}_{c}$ induced by centering across $n$ samples. Provide your final answer as a single closed-form expression in terms of $n$ and $p$ specialized to the regime $p \\gg n$. No rounding is required, and no units are involved.",
            "solution": "The objective is to determine the maximum possible number of nonzero eigenvalues of the feature-feature sample covariance matrix $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}$.\n\nA fundamental theorem in linear algebra states that the number of nonzero eigenvalues of any square matrix is equal to its rank. Therefore, the problem is equivalent to determining the maximum possible rank of the matrix $\\mathbf{S}$.\n\nThe matrix $\\mathbf{S}$ is defined as $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}$. The scalar factor $\\frac{1}{n-1}$ is nonzero (assuming $n > 1$, which is necessary to compute a covariance) and does not affect the rank of the matrix. Thus, we have:\n$$\n\\text{rank}(\\mathbf{S}) = \\text{rank}\\left(\\frac{1}{n-1}\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}\\right) = \\text{rank}(\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c})\n$$\nAnother fundamental property of linear algebra states that for any matrix $\\mathbf{A}$, the rank of $\\mathbf{A}^{\\top}\\mathbf{A}$ is equal to the rank of $\\mathbf{A}$. That is, $\\text{rank}(\\mathbf{A}^{\\top}\\mathbf{A}) = \\text{rank}(\\mathbf{A})$. Applying this property to the matrix $\\mathbf{X}_{c} \\in \\mathbb{R}^{n \\times p}$, we get:\n$$\n\\text{rank}(\\mathbf{S}) = \\text{rank}(\\mathbf{X}_{c}^{\\top}\\mathbf{X}_{c}) = \\text{rank}(\\mathbf{X}_{c})\n$$\nThe problem is now reduced to finding the maximum possible rank of the column-centered data matrix $\\mathbf{X}_{c}$.\n\nThe matrix $\\mathbf{X}_{c}$ has dimensions $n \\times p$. The rank of any matrix cannot exceed the minimum of its number of rows and columns. Therefore, $\\text{rank}(\\mathbf{X}_{c}) \\le \\min(n, p)$. Given the specified regime $p \\gg n$, this implies $\\min(n, p) = n$, so we have an initial upper bound:\n$$\n\\text{rank}(\\mathbf{X}_{c}) \\le n\n$$\nHowever, we must account for the constraint imposed by the column-centering operation. The problem states that $\\mathbf{X}_{c}$ is obtained by subtracting the sample mean from each column of the original data matrix $\\mathbf{X}$. Let $\\mathbf{x}_{c,j}$ denote the $j$-th column of $\\mathbf{X}_{c}$, which is a vector in $\\mathbb{R}^n$. The centering condition means that the sum of the elements in each column vector is zero:\n$$\n\\sum_{i=1}^{n} (\\mathbf{X}_{c})_{ij} = 0 \\quad \\text{for all } j \\in \\{1, 2, \\dots, p\\}\n$$\nThis can be expressed using vector notation. Let $\\mathbf{1}_{n} \\in \\mathbb{R}^n$ be the column vector consisting of $n$ ones. The centering condition is equivalent to the statement that each column vector $\\mathbf{x}_{c,j}$ is orthogonal to $\\mathbf{1}_{n}$:\n$$\n\\mathbf{1}_{n}^{\\top}\\mathbf{x}_{c,j} = 0 \\quad \\text{for all } j \\in \\{1, 2, \\dots, p\\}\n$$\nThis constraint has a crucial geometric interpretation: all $p$ column vectors of the matrix $\\mathbf{X}_{c}$ must lie within the subspace of $\\mathbb{R}^n$ that is orthogonal to the vector $\\mathbf{1}_{n}$. This subspace, often denoted as $\\mathbf{1}_{n}^{\\perp}$, has a dimension of $n-1$.\n\nThe rank of a matrix is defined as the dimension of its column space. The column space of $\\mathbf{X}_{c}$ is the vector space spanned by its $p$ column vectors, $\\text{span}\\{\\mathbf{x}_{c,1}, \\mathbf{x}_{c,2}, \\dots, \\mathbf{x}_{c,p}\\}$. Since every one of these vectors is confined to an $(n-1)$-dimensional subspace, the dimension of their span cannot exceed $n-1$. Accordingly, the rank of $\\mathbf{X}_{c}$ is bounded from above by $n-1$:\n$$\n\\text{rank}(\\mathbf{X}_{c}) \\le n-1\n$$\nThis upper bound is achievable if the original data in $\\mathbf{X}$ is sufficiently general. For instance, if we take $n-1$ columns of $\\mathbf{X}$ to be linearly independent and not lying entirely in the span of $\\mathbf{1}_n$, their projection onto the subspace $\\mathbf{1}_{n}^{\\perp}$ will also be linearly independent. Therefore, the *maximum possible* rank of $\\mathbf{X}_{c}$ is exactly $n-1$.\n\nThe regime $p \\gg n$ is critical. Since $p > n-1$, we have a set of $p$ vectors that reside in an $(n-1)$-dimensional space. This guarantees that the set of column vectors of $\\mathbf{X}_{c}$ must be linearly dependent, and the rank of the matrix is limited by the dimension of this subspace ($n-1$) rather than the number of features ($p$).\n\nIn summary:\n1. The number of nonzero eigenvalues of $\\mathbf{S}$ is equal to $\\text{rank}(\\mathbf{S})$.\n2. $\\text{rank}(\\mathbf{S}) = \\text{rank}(\\mathbf{X}_{c})$.\n3. The centering operation constrains the columns of $\\mathbf{X}_{c}$ to an $(n-1)$-dimensional subspace of $\\mathbb{R}^n$.\n4. Therefore, the maximum possible rank of $\\mathbf{X}_{c}$ is $n-1$.\n\nThe maximum possible number of nonzero eigenvalues of $\\mathbf{S}$ in the regime $p \\gg n$ is $n-1$.",
            "answer": "$$\n\\boxed{n-1}\n$$"
        },
        {
            "introduction": "Standard PCA implicitly assumes that variance is equally important across all feature directions, but this is often not the case in biological measurements where noise can be non-uniform. This practice introduces Generalized PCA, a powerful extension that allows us to define variance with respect to a custom Mahalanobis metric, effectively \"whitening\" the data to account for known noise structures or correlations. By deriving the corresponding generalized eigenvalue problem from first principles, you will learn how to adapt PCA to more accurately reflect the underlying geometry of your data .",
            "id": "3302534",
            "problem": "In a study of a three-gene signaling module assayed by single-cell RNA sequencing (scRNA-seq), suppose the sample covariance of log-expression across cells for the genes is modeled by a symmetric positive definite matrix $S \\in \\mathbb{R}^{3 \\times 3}$, and that known, gene-specific measurement noise levels define a positive definite Mahalanobis metric $M \\in \\mathbb{R}^{3 \\times 3}$. The first generalized principal component under the Mahalanobis metric is defined as the direction $v \\in \\mathbb{R}^{3}$ that maximizes the sample variance $v^{\\top} S v$ subject to the normalization constraint $v^{\\top} M v = 1$.\n\nStarting only from the definitions of variance, covariance, and the Mahalanobis norm, derive the constrained maximization principle for generalized principal component analysis (PCA) under a Mahalanobis metric, and then reduce it to a standard eigenvalue problem via an appropriate whitening transformation. Conclude by computing the exact maximum generalized variance (i.e., the largest generalized principal component variance) for the specific matrices\n$$\nS \\;=\\; \\begin{pmatrix}\n3.2 & 1.2 & 0.0 \\\\\n1.2 & 2.5 & 0.0 \\\\\n0.0 & 0.0 & 1.6\n\\end{pmatrix},\n\\qquad\nM \\;=\\; \\begin{pmatrix}\n1.0 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 0.0 \\\\\n0.0 & 0.0 & 2.0\n\\end{pmatrix}.\n$$\nProvide your final answer as a single closed-form analytic expression for the maximum generalized variance. Do not include units, and do not round.",
            "solution": "The problem is to find a vector $v \\in \\mathbb{R}^3$ that maximizes the generalized variance, given by the quadratic form $f(v) = v^{\\top} S v$, subject to the normalization constraint $g(v) = v^{\\top} M v - 1 = 0$.\n\n**Part 1: Derivation of the Constrained Maximization Principle**\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. We define the Lagrangian function $\\mathcal{L}(v, \\lambda)$ as:\n$$\n\\mathcal{L}(v, \\lambda) = f(v) - \\lambda g(v) = v^{\\top} S v - \\lambda(v^{\\top} M v - 1)\n$$\nwhere $\\lambda$ is a Lagrange multiplier. To find the extrema, we must set the gradient of $\\mathcal{L}$ with respect to $v$ to zero. The gradients of the quadratic forms, given that $S$ and $M$ are symmetric matrices, are $\\nabla_v(v^{\\top} S v) = 2Sv$ and $\\nabla_v(v^{\\top} M v) = 2Mv$.\n\nSetting the gradient of the Lagrangian to zero:\n$$\n\\nabla_v \\mathcal{L}(v, \\lambda) = 2Sv - \\lambda (2Mv) = 0\n$$\n$$\nSv - \\lambda Mv = 0\n$$\n$$\nSv = \\lambda Mv\n$$\nThis is a generalized eigenvalue problem. The vectors $v$ that satisfy this equation are the generalized eigenvectors, and the corresponding scalars $\\lambda$ are the generalized eigenvalues of the matrix pair $(S, M)$.\n\nTo see what the value of the objective function is at these stationary points, we left-multiply the equation $Sv = \\lambda Mv$ by $v^{\\top}$:\n$$\nv^{\\top} S v = v^{\\top} (\\lambda Mv) = \\lambda (v^{\\top} M v)\n$$\nApplying the constraint $v^{\\top} M v = 1$, we find:\n$$\nv^{\\top} S v = \\lambda\n$$\nThis shows that the variance along a generalized eigenvector $v$ is equal to the corresponding generalized eigenvalue $\\lambda$. Therefore, to maximize the variance $v^{\\top} S v$, we must choose the largest generalized eigenvalue, $\\lambda_{\\text{max}}$. The corresponding generalized eigenvector $v_{\\text{max}}$ is the first generalized principal component.\n\n**Part 2: Reduction to a Standard Eigenvalue Problem**\n\nThe generalized eigenvalue problem $Sv = \\lambda Mv$ can be converted into a standard eigenvalue problem. Since $M$ is a symmetric positive definite matrix, it has a unique symmetric positive definite square root $M^{1/2}$, and it also admits a Cholesky decomposition $M = LL^{\\top}$, where $L$ is a lower triangular matrix. Using the Cholesky decomposition is a common method for this \"whitening\" transformation.\n\nSubstitute $M = LL^{\\top}$ into the generalized eigenvalue equation:\n$$\nSv = \\lambda LL^{\\top}v\n$$\nSince $L$ is invertible (as $M$ is positive definite), we can pre-multiply by $L^{-1}$:\n$$\nL^{-1}Sv = \\lambda L^{\\top}v\n$$\nNow, we define a new \"whitened\" vector $u$ as $u = L^{\\top}v$. This implies $v = (L^{\\top})^{-1}u = L^{-\\top}u$. Substituting this expression for $v$ into the equation gives:\n$$\nL^{-1}S(L^{-\\top}u) = \\lambda u\n$$\n$$\n(L^{-1}SL^{-\\top})u = \\lambda u\n$$\nLet's define a new matrix $S' = L^{-1}SL^{-\\top}$. The equation becomes $S'u = \\lambda u$, which is a standard eigenvalue problem for the matrix $S'$. The matrix $S'$ is symmetric because $(L^{-1}SL^{-\\top})^{\\top} = (L^{-\\top})^{\\top}S^{\\top}(L^{-1})^{\\top} = L^{-1}SL^{-\\top} = S'$, as $S$ is symmetric.\n\nThe eigenvalues $\\lambda$ of $S'$ are identical to the generalized eigenvalues of the pair $(S, M)$. The original constrained optimization problem is transformed into maximizing $u^{\\top}S'u$ subject to $u^{\\top}u = 1$, whose maximum value is, by the Rayleigh-Ritz theorem, the largest eigenvalue of $S'$.\n\n**Part 3: Computation of Maximum Generalized Variance**\n\nWe need to find the largest generalized eigenvalue $\\lambda$ by solving the characteristic equation $\\det(S - \\lambda M) = 0$.\nGiven:\n$$\nS \\;=\\; \\begin{pmatrix}\n3.2 & 1.2 & 0.0 \\\\\n1.2 & 2.5 & 0.0 \\\\\n0.0 & 0.0 & 1.6\n\\end{pmatrix},\n\\qquad\nM \\;=\\; \\begin{pmatrix}\n1.0 & 0.0 & 0.0 \\\\\n0.0 & 0.5 & 0.0 \\\\\n0.0 & 0.0 & 2.0\n\\end{pmatrix}\n$$\nFirst, we compute $S - \\lambda M$:\n$$\nS - \\lambda M = \\begin{pmatrix}\n3.2 - 1.0\\lambda & 1.2 & 0.0 \\\\\n1.2 & 2.5 - 0.5\\lambda & 0.0 \\\\\n0.0 & 0.0 & 1.6 - 2.0\\lambda\n\\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(S - \\lambda M) = (1.6 - 2.0\\lambda) \\left| \\begin{matrix} 3.2 - \\lambda & 1.2 \\\\ 1.2 & 2.5 - 0.5\\lambda \\end{matrix} \\right|\n$$\nThe determinant is zero if either of the factors is zero.\n\nCase 1: The linear factor is zero.\n$$\n1.6 - 2.0\\lambda = 0 \\implies 2\\lambda = 1.6 \\implies \\lambda_1 = 0.8\n$$\n\nCase 2: The determinant of the $2 \\times 2$ submatrix is zero.\n$$\n(3.2 - \\lambda)(2.5 - 0.5\\lambda) - (1.2)^2 = 0\n$$\n$$\n(8.0 - 1.6\\lambda - 2.5\\lambda + 0.5\\lambda^2) - 1.44 = 0\n$$\n$$\n0.5\\lambda^2 - 4.1\\lambda + 6.56 = 0\n$$\nMultiplying by $2$ to clear the fraction:\n$$\n\\lambda^2 - 8.2\\lambda + 13.12 = 0\n$$\nWe use the quadratic formula to find the other two eigenvalues, $\\lambda_2$ and $\\lambda_3$:\n$$\n\\lambda = \\frac{-(-8.2) \\pm \\sqrt{(-8.2)^2 - 4(1)(13.12)}}{2(1)} = \\frac{8.2 \\pm \\sqrt{67.24 - 52.48}}{2} = \\frac{8.2 \\pm \\sqrt{14.76}}{2}\n$$\nTo find the exact form, we simplify the term under the square root: $\\sqrt{14.76} = \\sqrt{1476/100} = \\sqrt{36 \\times 41}/10 = 6\\sqrt{41}/10 = 3\\sqrt{41}/5$.\n\nSubstituting this back into the quadratic formula solution:\n$$\n\\lambda = \\frac{8.2 \\pm 3\\sqrt{41}/5}{2} = 4.1 \\pm \\frac{3\\sqrt{41}}{10}\n$$\nLet's write $4.1$ as a fraction, $41/10$:\n$$\n\\lambda = \\frac{41}{10} \\pm \\frac{3\\sqrt{41}}{10} = \\frac{41 \\pm 3\\sqrt{41}}{10}\n$$\nSo, the other two eigenvalues are:\n$$\n\\lambda_2 = \\frac{41 + 3\\sqrt{41}}{10} \\quad \\text{and} \\quad \\lambda_3 = \\frac{41 - 3\\sqrt{41}}{10}\n$$\nThe three generalized eigenvalues are $\\lambda_1 = 0.8$, $\\lambda_2 = \\frac{41 + 3\\sqrt{41}}{10}$, and $\\lambda_3 = \\frac{41 - 3\\sqrt{41}}{10}$.\n\nWe need to find the largest of these three values.\nSince $\\sqrt{41} > \\sqrt{36} = 6$, we have $3\\sqrt{41} > 18$.\n$$\n\\lambda_2 = \\frac{41 + 3\\sqrt{41}}{10} > \\frac{41 + 18}{10} = 5.9\n$$\n$$\n\\lambda_3 = \\frac{41 - 3\\sqrt{41}}{10}  \\frac{41 - 18}{10} = 2.3\n$$\nClearly, $\\lambda_2$ is the largest value. The maximum generalized variance is the largest generalized eigenvalue, $\\lambda_{\\text{max}} = \\lambda_2$.\n$$\n\\lambda_{\\text{max}} = \\frac{41 + 3\\sqrt{41}}{10}\n$$\nThis is the final closed-form analytic expression.",
            "answer": "$$\\boxed{\\frac{41 + 3\\sqrt{41}}{10}}$$"
        },
        {
            "introduction": "The utility of PCA can be compromised by its sensitivity to outliers, a common issue in single-cell datasets where a few anomalous cells can dominate the analysis and skew biological interpretations. This advanced practice introduces a formal framework from robust statistics, the Influence Function, to dissect and quantify this sensitivity. You will derive the analytical form of the influence of a single data point on PCA loadings and use this insight to construct a \"worst-case\" adversarial cell, comparing the theoretical prediction with the exact numerical outcome to build a deep understanding of model robustness .",
            "id": "3302561",
            "problem": "Consider a centered single-cell gene expression matrix representing $n$ cells across $p$ genes, modeled as realizations of a random vector $x \\in \\mathbb{R}^p$ with distribution $F$ satisfying zero mean, that is $E[x] = 0$. The sample covariance functional is defined by $S(F) = E[x x^\\top]$, and Principal Component Analysis (PCA) estimates loadings as the eigenvectors of the covariance. Let $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_p$ denote the distinct eigenvalues of a symmetric positive semi-definite matrix $S \\in \\mathbb{R}^{p \\times p}$, with corresponding orthonormal eigenvectors $v_1, v_2, \\ldots, v_p \\in \\mathbb{R}^p$. The leading PCA loading is $v_1$.\n\nRobustness to outlier cells may be quantified via the Influence Function (IF) from robust statistics for a functional $T(F)$, defined by the GÃ¢teaux derivative at $F$ under infinitesimal contamination at a point $a \\in \\mathbb{R}^p$, namely $F_\\varepsilon = (1 - \\varepsilon) F + \\varepsilon \\Delta_a$, as\n$$\n\\mathrm{IF}_T(a; F) = \\left. \\frac{d}{d\\varepsilon} T(F_\\varepsilon) \\right|_{\\varepsilon = 0}.\n$$\nIn this setting, we consider $T(F) = v_1(F)$, the leading PCA loading viewed as a functional of the covariance functional. Starting from the core definitions of the covariance, eigen-decomposition, and first principles of differentiating the eigenvalue equation, derive the first-order variation of $v_1$ under a symmetric perturbation to $S$ induced by an infinitesimal contamination at $a$, expressed entirely in terms of the spectral quantities of $S$ and the point $a$. Use this to obtain the corresponding Influence Function for the leading PCA loading.\n\nNext, define an adversarial cell $a^\\star \\in \\mathbb{R}^p$ subject to an energy constraint $\\|a^\\star\\|_2 \\le R$ that maximizes, to first order in $\\varepsilon$, the angular deviation of $v_1$ caused by contamination at $a^\\star$. Explicitly derive $a^\\star$ and the resulting first-order angular deviation in radians as a function of $R$, $\\varepsilon$, and the eigenvalues of $S$.\n\nYour program must implement the following algorithmic steps for each test case:\n- Center the provided cell-by-gene matrix $X \\in \\mathbb{R}^{n \\times p}$ by subtracting its column-wise mean to enforce $E[x] = 0$ in the empirical distribution.\n- Compute the sample covariance $S = \\frac{1}{n} X^\\top X$.\n- Compute the eigenvalues and eigenvectors of $S$, ordered so that $\\lambda_1$ is the largest eigenvalue, with corresponding orthonormal eigenvectors $v_1, \\ldots, v_p$.\n- Construct the adversarial cell $a^\\star$ that maximizes the first-order angular deviation of $v_1$ under the constraint $\\|a^\\star\\|_2 \\le R$, and compute the first-order predicted angular deviation $\\theta_{\\mathrm{IF}}$ in radians for contamination mass $\\varepsilon$.\n- Compute the exact leading eigenvector $w_1$ of the contaminated covariance $S_\\varepsilon = (1 - \\varepsilon) S + \\varepsilon a^\\star (a^\\star)^\\top$ and the exact angular deviation $\\theta_{\\mathrm{exact}} = \\arccos\\left( |v_1^\\top w_1| \\right)$ in radians.\n- Return the triple of floats $\\left[\\theta_{\\mathrm{exact}}, \\theta_{\\mathrm{IF}}, |\\theta_{\\mathrm{exact}} - \\theta_{\\mathrm{IF}}| \\right]$ in that order for each test case.\n\nAngle measures must be expressed in radians. The final program output must be a single line containing a list of the three test case results as comma-separated lists enclosed in square brackets, with no surrounding explanatory text, exactly in the format\n$$\n\\texttt{[[}\\theta_{\\mathrm{exact}}^{(1)}, \\theta_{\\mathrm{IF}}^{(1)}, |\\theta_{\\mathrm{exact}}^{(1)} - \\theta_{\\mathrm{IF}}^{(1)}| \\texttt{],[}\\theta_{\\mathrm{exact}}^{(2)}, \\theta_{\\mathrm{IF}}^{(2)}, |\\theta_{\\mathrm{exact}}^{(2)} - \\theta_{\\mathrm{IF}}^{(2)}| \\texttt{],[}\\theta_{\\mathrm{exact}}^{(3)}, \\theta_{\\mathrm{IF}}^{(3)}, |\\theta_{\\mathrm{exact}}^{(3)} - \\theta_{\\mathrm{IF}}^{(3)}| \\texttt{]]}.\n$$\n\nUse the following test suite, with each case specified by $(X, R, \\varepsilon)$:\n\n- Case A (well-separated leading eigenvalue):\n$$\nX^{(A)} =\n\\begin{bmatrix}\n2.0  1.0  0.2  -0.1 \\\\\n-1.5  0.5  -0.2  0.0 \\\\\n3.0  -0.5  0.1  0.2 \\\\\n-2.2  -0.6  0.0  0.1 \\\\\n1.8  0.9  -0.1  -0.2 \\\\\n-0.7  -1.1  0.3  0.0 \\\\\n2.4  0.2  0.0  -0.1 \\\\\n-1.9  0.8  -0.3  0.0\n\\end{bmatrix}, \\quad R^{(A)} = 2.0, \\quad \\varepsilon^{(A)} = 0.01.\n$$\n\n- Case B (nearly equal top two eigenvalues):\n$$\nX^{(B)} =\n\\begin{bmatrix}\n1.5  1.4  0.2  0.0 \\\\\n-1.2  -1.1  -0.1  0.1 \\\\\n1.8  -1.7  0.0  -0.2 \\\\\n-1.6  1.5  0.1  0.2 \\\\\n1.3  1.2  -0.2  0.0 \\\\\n-1.4  -1.5  0.2  0.0 \\\\\n1.7  -1.6  0.0  0.1 \\\\\n-1.3  1.4  -0.2  -0.1\n\\end{bmatrix}, \\quad R^{(B)} = 2.0, \\quad \\varepsilon^{(B)} = 0.05.\n$$\n\n- Case C (near-degenerate leading eigenvalues, stronger contamination):\n$$\nX^{(C)} =\n\\begin{bmatrix}\n1.00  1.02  0.05  0.00 \\\\\n-1.00  -0.98  -0.04  0.00 \\\\\n1.00  -0.98  0.00  -0.03 \\\\\n-1.00  1.02  -0.01  0.02 \\\\\n0.95  0.93  0.02  0.01 \\\\\n-0.95  -0.93  -0.02  -0.01 \\\\\n0.98  -0.96  0.03  -0.02 \\\\\n-0.98  0.96  -0.03  0.02\n\\end{bmatrix}, \\quad R^{(C)} = 1.5, \\quad \\varepsilon^{(C)} = 0.20.\n$$\n\nScientific realism and consistency constraints:\n- Treat $X$ as centered log-transformed expression approximations; centering will be performed explicitly by subtracting column means.\n- Ensure that all computations adhere strictly to the mathematical statements above without empirical shortcuts.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $\\left[\\text{Case A}, \\text{Case B}, \\text{Case C}\\right]$, where each case is itself a list of three floats $\\left[\\theta_{\\mathrm{exact}}, \\theta_{\\mathrm{IF}}, |\\theta_{\\mathrm{exact}} - \\theta_{\\mathrm{IF}}|\\right]$.",
            "solution": "The core of this problem is to derive the influence function ($\\mathrm{IF}$) for the leading principal component (PC) loading vector, use it to find an optimal adversarial data point, and compare the first-order approximation of the resulting perturbation to the exact numerical result.\n\n### Part 1: Derivation of the Influence Function for the Leading PC Loading\n\nLet $S \\in \\mathbb{R}^{p \\times p}$ be a symmetric positive semi-definite covariance matrix with distinct leading eigenvalue $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_p \\ge 0$ and corresponding orthonormal eigenvectors $v_1, v_2, \\ldots, v_p$. The defining eigendecomposition for the $k$-th eigenpair is:\n$$\nS v_k = \\lambda_k v_k\n$$\n\nThe data distribution $F$ is contaminated at a point $a \\in \\mathbb{R}^p$, yielding a mixture distribution $F_\\varepsilon = (1 - \\varepsilon) F + \\varepsilon \\Delta_a$. The covariance matrix for this contaminated distribution, denoted $S_\\varepsilon$, is:\n$$\nS_\\varepsilon = E_{x \\sim F_\\varepsilon}[x x^\\top] = (1 - \\varepsilon) E_{x \\sim F}[x x^\\top] + \\varepsilon E_{x \\sim \\Delta_a}[x x^\\top] = (1 - \\varepsilon) S + \\varepsilon a a^\\top\n$$\nWe can express this as a perturbation of $S$:\n$$\nS_\\varepsilon = S + \\varepsilon(a a^\\top - S)\n$$\nThe perturbation to $S$ is $P = a a^\\top - S$. The influence function is the derivative of the leading eigenvector functional $v_1(F)$ with respect to $\\varepsilon$ at $\\varepsilon=0$. Let $v_1(\\varepsilon)$ and $\\lambda_1(\\varepsilon)$ be the perturbed leading eigenvector and eigenvalue of $S_\\varepsilon$. The eigenvalue equation is $S_\\varepsilon v_1(\\varepsilon) = \\lambda_1(\\varepsilon) v_1(\\varepsilon)$. Differentiating with respect to $\\varepsilon$ and evaluating at $\\varepsilon=0$ yields:\n$$\n\\left. \\frac{d S_\\varepsilon}{d \\varepsilon} \\right|_{\\varepsilon=0} v_1(0) + S_\\varepsilon(0) \\left. \\frac{d v_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0} = \\left. \\frac{d \\lambda_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0} v_1(0) + \\lambda_1(0) \\left. \\frac{d v_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0}\n$$\nLet $\\dot{v}_1 = \\left. \\frac{d v_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0} = \\mathrm{IF}_{v_1}(a; F)$, $\\dot{\\lambda}_1 = \\left. \\frac{d \\lambda_1(\\varepsilon)}{d \\varepsilon} \\right|_{\\varepsilon=0}$, and $\\dot{S} = \\left. \\frac{d S_\\varepsilon}{d \\varepsilon} \\right|_{\\varepsilon=0} = a a^\\top - S$. The equation becomes:\n$$\n\\dot{S} v_1 + S \\dot{v}_1 = \\dot{\\lambda}_1 v_1 + \\lambda_1 \\dot{v}_1\n$$\nUsing $S v_1 = \\lambda_1 v_1$ and rearranging gives:\n$$\n(S - \\lambda_1 I) \\dot{v}_1 = (\\dot{\\lambda}_1 v_1 - \\dot{S} v_1)\n$$\nThe derivative $\\dot{v}_1$ can be expressed in the basis of eigenvectors $\\{v_k\\}_{k=1}^p$ as $\\dot{v}_1 = \\sum_{k=1}^p c_k v_k$. Since $v_1(\\varepsilon)$ must remain a unit vector, $\\|v_1(\\varepsilon)\\|_2^2 = 1$, its derivative is orthogonal to $v_1(\\varepsilon)$. At $\\varepsilon=0$, this implies $\\dot{v}_1^\\top v_1 = 0$, so $c_1=0$. Thus, $\\dot{v}_1 = \\sum_{k=2}^p c_k v_k$.\n\nTo find the coefficients $c_j$ for $j \\ge 2$, we left-multiply the rearranged equation by $v_j^\\top$:\n$$\nv_j^\\top (S - \\lambda_1 I) \\dot{v}_1 = v_j^\\top (\\dot{\\lambda}_1 v_1 - \\dot{S} v_1)\n$$\nThe left-hand side (LHS) becomes:\n$$\nv_j^\\top (S - \\lambda_1 I) \\sum_{k=2}^p c_k v_k = v_j^\\top \\sum_{k=2}^p c_k (\\lambda_k - \\lambda_1) v_k = c_j (\\lambda_j - \\lambda_1)\n$$\nThe right-hand side (RHS) becomes:\n$$\n\\dot{\\lambda}_1 (v_j^\\top v_1) - v_j^\\top \\dot{S} v_1 = -v_j^\\top \\dot{S} v_1 = -v_j^\\top (a a^\\top - S) v_1\n$$\nSince $v_j^\\top S v_1 = v_j^\\top (\\lambda_1 v_1) = \\lambda_1 (v_j^\\top v_1) = 0$ for $j \\ge 2$, the RHS simplifies to:\n$$\n-v_j^\\top (a a^\\top) v_1 = -(v_j^\\top a)(a^\\top v_1)\n$$\nEquating LHS and RHS, and solving for $c_j$ (assuming $\\lambda_1 \\neq \\lambda_j$):\n$$\nc_j = \\frac{-(v_j^\\top a)(a^\\top v_1)}{\\lambda_j - \\lambda_1} = \\frac{(a^\\top v_1)(a^\\top v_j)}{\\lambda_1 - \\lambda_j}\n$$\nSubstituting back, we obtain the influence function for the leading PC loading:\n$$\n\\mathrm{IF}_{v_1}(a; F) = \\dot{v}_1 = \\sum_{k=2}^p \\frac{(a^\\top v_1)(a^\\top v_k)}{\\lambda_1 - \\lambda_k} v_k\n$$\n\n### Part 2: Derivation of the Adversarial Cell\n\nThe adversarial cell $a^\\star$ is defined as the point $a$ with energy constraint $\\|a\\|_2 \\le R$ that maximizes the first-order angular deviation of $v_1$. For a small perturbation, the new vector is $v_1(\\varepsilon) \\approx v_1 + \\varepsilon \\dot{v}_1$. Since $\\dot{v}_1$ is orthogonal to $v_1$, the angle of deviation $\\theta$ for small $\\varepsilon$ is approximately $\\tan(\\theta) \\approx \\|\\varepsilon \\dot{v}_1\\| / \\|v_1\\| = \\varepsilon \\|\\dot{v}_1\\|$. Thus, we seek to maximize $\\|\\dot{v}_1\\| = \\|\\mathrm{IF}_{v_1}(a; F)\\|$.\n\nUsing the orthogonality of the eigenvectors $\\{v_k\\}$, the squared norm of the influence function is:\n$$\n\\|\\dot{v}_1\\|^2 = \\left\\| \\sum_{k=2}^p c_k v_k \\right\\|^2 = \\sum_{k=2}^p c_k^2 = (a^\\top v_1)^2 \\sum_{k=2}^p \\frac{(a^\\top v_k)^2}{(\\lambda_1 - \\lambda_k)^2}\n$$\nLet $\\alpha_k = a^\\top v_k$ be the projection of $a$ onto $v_k$. Then $a = \\sum_{k=1}^p \\alpha_k v_k$, and the constraint $\\|a\\|_2^2 \\le R^2$ becomes $\\sum_{k=1}^p \\alpha_k^2 \\le R^2$. We want to maximize:\n$$\nJ(\\alpha_1, \\ldots, \\alpha_p) = \\alpha_1^2 \\sum_{k=2}^p \\frac{\\alpha_k^2}{(\\lambda_1 - \\lambda_k)^2}\n$$\nThe maximization occurs on the boundary, so $\\sum_{k=1}^p \\alpha_k^2 = R^2$. The term $1/(\\lambda_1 - \\lambda_k)^2$ is largest for the eigenvalue $\\lambda_k$ closest to $\\lambda_1$, which is $\\lambda_2$. To maximize the sum, we should place all the \"energy\" of the non-$v_1$ components of $a$ onto the $v_2$ direction. This means we set $\\alpha_k = 0$ for all $k  2$. The problem reduces to maximizing $\\alpha_1^2 \\frac{\\alpha_2^2}{(\\lambda_1 - \\lambda_2)^2}$ subject to $\\alpha_1^2 + \\alpha_2^2 = R^2$. This is equivalent to maximizing the product $\\alpha_1^2 \\alpha_2^2$ subject to their sum being constant. The maximum is achieved when $\\alpha_1^2 = \\alpha_2^2 = R^2/2$.\n\nThus, $|\\alpha_1| = R/\\sqrt{2}$ and $|\\alpha_2| = R/\\sqrt{2}$. The optimal adversarial cell $a^\\star$ is a linear combination of $v_1$ and $v_2$. We can choose the signs arbitrarily; for simplicity, we set:\n$$\na^\\star = \\frac{R}{\\sqrt{2}} v_1 + \\frac{R}{\\sqrt{2}} v_2\n$$\n\n### Part 3: First-Order Angular Deviation\n\nThe first-order angular deviation is $\\theta_{\\mathrm{IF}} = \\varepsilon \\|\\mathrm{IF}_{v_1}(a^\\star; F)\\|$. We calculate this norm:\n$$\n\\mathrm{IF}_{v_1}(a^\\star; F) = \\sum_{k=2}^p \\frac{((a^\\star)^\\top v_1)((a^\\star)^\\top v_k)}{\\lambda_1 - \\lambda_k} v_k\n$$\nWith our choice of $a^\\star$, we have $(a^\\star)^\\top v_1 = R/\\sqrt{2}$, $(a^\\star)^\\top v_2 = R/\\sqrt{2}$, and $(a^\\star)^\\top v_k = 0$ for $k  2$. The sum has only one non-zero term (at $k=2$):\n$$\n\\mathrm{IF}_{v_1}(a^\\star; F) = \\frac{(R/\\sqrt{2})(R/\\sqrt{2})}{\\lambda_1 - \\lambda_2} v_2 = \\frac{R^2}{2(\\lambda_1 - \\lambda_2)} v_2\n$$\nThe norm is:\n$$\n\\|\\mathrm{IF}_{v_1}(a^\\star; F)\\| = \\left\\| \\frac{R^2}{2(\\lambda_1 - \\lambda_2)} v_2 \\right\\| = \\frac{R^2}{2(\\lambda_1 - \\lambda_2)}\n$$\nTherefore, the maximum first-order angular deviation is:\n$$\n\\theta_{\\mathrm{IF}} = \\varepsilon \\frac{R^2}{2(\\lambda_1 - \\lambda_2)}\n$$\n\nThe provided code implements these derived formulas to compute $\\theta_{\\mathrm{IF}}$ and compares it to the exact angle $\\theta_{\\mathrm{exact}}$ obtained by numerically computing the eigenvector of the contaminated covariance matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the final result.\n    \"\"\"\n\n    def calculate_deviations(X, R, epsilon):\n        \"\"\"\n        Performs the analysis for a single test case.\n\n        Args:\n            X (np.ndarray): The n x p cell-by-gene matrix.\n            R (float): The energy constraint for the adversarial cell.\n            epsilon (float): The contamination mass.\n\n        Returns:\n            list: A list of three floats: [theta_exact, theta_IF, error].\n        \"\"\"\n        # Step 1: Center the provided cell-by-gene matrix X.\n        n, p = X.shape\n        X_centered = X - np.mean(X, axis=0)\n\n        # Step 2: Compute the sample covariance S.\n        # S = (1/n) * X_centered.T @ X_centered\n        S = np.cov(X_centered, rowvar=False, bias=True)\n\n        # Step 3: Compute the eigenvalues and eigenvectors of S, ordered descending.\n        # np.linalg.eigh returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(S)\n        \n        # Sort in descending order\n        desc_indices = np.argsort(eigenvalues)[::-1]\n        lambdas = eigenvalues[desc_indices]\n        vectors = eigenvectors[:, desc_indices]\n\n        # Extract leading spectral quantities\n        lambda1 = lambdas[0]\n        lambda2 = lambdas[1]\n        v1 = vectors[:, 0]\n        v2 = vectors[:, 1]\n        \n        # Guard against division by zero if eigenvalues are identical, though problem implies lambda1  lambda2.\n        if np.isclose(lambda1, lambda2):\n            # In this degenerate case, the IF is theoretically infinite.\n            # We can return infinity or handle as an error. For this problem,\n            # this indicates a breakdown of the first-order approximation's premise.\n            # We will let the division proceed, resulting in np.inf as per standard float behavior.\n            pass\n\n        # Step 4: Construct the adversarial cell a_star and compute theta_IF.\n        a_star = (R / np.sqrt(2)) * (v1 + v2)\n        \n        # First-order predicted angular deviation from derived formula\n        theta_IF = epsilon * R**2 / (2 * (lambda1 - lambda2))\n\n        # Step 5: Compute the exact leading eigenvector w1 of the contaminated covariance S_epsilon.\n        S_epsilon = (1 - epsilon) * S + epsilon * np.outer(a_star, a_star)\n        \n        # Eigendecomposition of the contaminated matrix\n        eigvals_eps, eigvecs_eps = np.linalg.eigh(S_epsilon)\n        \n        # Sort in descending order\n        desc_indices_eps = np.argsort(eigvals_eps)[::-1]\n        w1 = eigvecs_eps[:, desc_indices_eps[0]]\n\n        # Compute the exact angular deviation theta_exact.\n        # The absolute value handles the arbitrary sign of eigenvectors.\n        # Clipping handles potential floating point inaccuracies where the dot product might be slightly  1.\n        dot_product = np.clip(np.abs(np.dot(v1, w1)), -1.0, 1.0)\n        theta_exact = np.arccos(dot_product)\n        \n        # Step 6: Return the triple of floats.\n        error = np.abs(theta_exact - theta_IF)\n        \n        return [theta_exact, theta_IF, error]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [2.0, 1.0, 0.2, -0.1],\n                [-1.5, 0.5, -0.2, 0.0],\n                [3.0, -0.5, 0.1, 0.2],\n                [-2.2, -0.6, 0.0, 0.1],\n                [1.8, 0.9, -0.1, -0.2],\n                [-0.7, -1.1, 0.3, 0.0],\n                [2.4, 0.2, 0.0, -0.1],\n                [-1.9, 0.8, -0.3, 0.0]\n            ]), 2.0, 0.01  # Case A: R=2.0, epsilon=0.01\n        ),\n        (\n            np.array([\n                [1.5, 1.4, 0.2, 0.0],\n                [-1.2, -1.1, -0.1, 0.1],\n                [1.8, -1.7, 0.0, -0.2],\n                [-1.6, 1.5, 0.1, 0.2],\n                [1.3, 1.2, -0.2, 0.0],\n                [-1.4, -1.5, 0.2, 0.0],\n                [1.7, -1.6, 0.0, 0.1],\n                [-1.3, 1.4, -0.2, -0.1]\n            ]), 2.0, 0.05  # Case B: R=2.0, epsilon=0.05\n        ),\n        (\n            np.array([\n                [1.00, 1.02, 0.05, 0.00],\n                [-1.00, -0.98, -0.04, 0.00],\n                [1.00, -0.98, 0.00, -0.03],\n                [-1.00, 1.02, -0.01, 0.02],\n                [0.95, 0.93, 0.02, 0.01],\n                [-0.95, -0.93, -0.02, -0.01],\n                [0.98, -0.96, 0.03, -0.02],\n                [-0.98, 0.96, -0.03, 0.02]\n            ]), 1.5, 0.20 # Case C: R=1.5, epsilon=0.20\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        X, R, epsilon = case\n        result_tuple = calculate_deviations(X, R, epsilon)\n        results.append(result_tuple)\n\n    # Format the final output string exactly as specified.\n    # Create strings for each sublist without spaces, then join them.\n    formatted_results = []\n    for res in results:\n        # Using a list comprehension and join to avoid spaces from default list str()\n        s = f\"[{','.join(map(str, res))}]\"\n        formatted_results.append(s)\n    \n    final_string = f\"[{','.join(formatted_results)}]\"\n    print(final_string)\n\nsolve()\n```"
        }
    ]
}