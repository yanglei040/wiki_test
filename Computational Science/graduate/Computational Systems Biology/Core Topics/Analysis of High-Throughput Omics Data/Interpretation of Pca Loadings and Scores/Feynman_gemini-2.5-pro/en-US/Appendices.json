{
    "hands_on_practices": [
        {
            "introduction": "A principal component analysis (PCA) model is more than a static snapshot of a single dataset; it defines a reusable transformation. A common task in systems biology is to project new samples—such as a new patient's data or a new experimental condition—into the latent space defined by a previously trained PCA model. This practice guides you through the fundamental derivation for this projection, underscoring why applying the exact same preprocessing steps, especially mean-centering with the *original* training data's mean, is non-negotiable for scientifically valid comparisons. ",
            "id": "3321042",
            "problem": "A transcriptomics study measures messenger RNA (mRNA) abundances for $m$ genes across $n$ training samples and performs Principal Component Analysis (PCA) on the preprocessed training data. The preprocessing pipeline consists of a fixed transformation applied to all training samples (log transformation to stabilize variance), followed by mean-centering across the training cohort. No variance scaling is applied beyond mean-centering. The PCA model is trained by computing the sample covariance matrix of the mean-centered data and performing eigen-decomposition to obtain an orthonormal loading matrix $P \\in \\mathbb{R}^{m \\times k}$ whose columns are eigenvectors corresponding to the top $k$ eigenvalues. The score coordinates of a training sample are defined by orthogonal projection of its mean-centered vector onto the loading subspace.\n\nFor a new biological replicate with raw measurements represented by $x_{\\mathrm{new}} \\in \\mathbb{R}^{m}$, projection into the trained principal component subspace must be done consistently with the training pipeline. Starting from core definitions of orthogonal projection onto an orthonormal basis and the construction of PCA from the eigen-decomposition of the covariance of the preprocessed training data, derive the expression for the score coordinates of $x_{\\mathrm{new}}$ in terms of the training mean vector $\\hat{\\mu}$ and the loading matrix $P$. Explain why applying identical preprocessing to $x_{\\mathrm{new}}$ as was applied to the training data (including the fixed transformation and centering by $\\hat{\\mu}$) is necessary for scientific consistency and interpretability in a computational systems biology setting.\n\nThen, apply your derivation to the following scientifically plausible metabolite panel with $m=3$ variables and $k=2$ components. The trained loading matrix (columns orthonormal) and the training mean vector computed after preprocessing are\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\\n0 & \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\n\\qquad\n\\hat{\\mu} \\;=\\; \\begin{pmatrix} 5 \\\\ 7 \\\\ 4 \\end{pmatrix}.\n$$\nA new sample, after applying the identical fixed transformation used in training, has measurements\n$$\nx_{\\mathrm{new}} \\;=\\; \\begin{pmatrix} 6 \\\\ 8.5 \\\\ 3 \\end{pmatrix}.\n$$\nCompute the first principal component score for this new sample based on your derived projection expression. Round your numerical answer to four significant figures. Express your final answer without units.",
            "solution": "The problem as stated is valid. It presents a standard, well-posed scenario in computational systems biology involving the application of Principal Component Analysis (PCA). The premises are scientifically sound, the definitions are clear, and all required information for a solution is provided.\n\nThe primary task is to derive the expression for the score coordinates of a new sample, $x_{\\mathrm{new}}$, projected into a PCA space that was constructed from a training dataset. This requires a systematic application of linear algebra and an understanding of the PCA methodology.\n\nLet the training data be represented by a matrix where each of the $n$ columns is a sample vector of $m$ measurements (e.g., gene abundances). The preprocessing pipeline on the training data involves a fixed transformation function, $f$, applied element-wise, followed by mean-centering. Let $x'_{i}$ be the $i$-th training sample after the application of $f$. The training mean vector, $\\hat{\\mu}$, is computed from these transformed samples:\n$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x'_{i} $$\nThe final preprocessed training vectors, $\\tilde{x}_i$, are the mean-centered vectors:\n$$ \\tilde{x}_i = x'_{i} - \\hat{\\mu} $$\nPCA is performed by finding the eigenvectors of the sample covariance matrix of the set of vectors $\\{\\tilde{x}_i\\}_{i=1}^n$. The loading matrix, $P \\in \\mathbb{R}^{m \\times k}$, is constructed by taking the $k$ orthonormal eigenvectors corresponding to the $k$ largest eigenvalues as its columns. These columns, $\\{p_1, p_2, \\dots, p_k\\}$, form an orthonormal basis for the principal subspace, which captures the maximal variance of the training data.\n\nThe score coordinates of any vector in the preprocessed space are found by projecting it onto this orthonormal basis. For a preprocessed training sample $\\tilde{x}_i$, its score vector $s_i \\in \\mathbb{R}^k$ is obtained by computing the dot product with each basis vector. The $j$-th score, $s_{ij}$, is $p_j^T \\tilde{x}_i$. In matrix form, this is:\n$$ s_i = P^T \\tilde{x}_i $$\nFor a new sample with raw measurements $x_{\\mathrm{raw, new}}$, its projection into the PCA space is meaningful only if it undergoes the exact same preprocessing transformation as the training data. This is a fundamental requirement for scientific consistency and interpretability.\n$1$. The fixed transformation $f$ is applied: $x_{\\mathrm{new}} = f(x_{\\mathrm{raw, new}})$. The problem provides this post-transformation vector directly.\n$2$. The vector is centered using the *training data's mean vector*, $\\hat{\\mu}$. This step is critical because the principal components are defined relative to the center of the training data cloud. To compare the new sample to the training samples, it must be placed within the same coordinate system, which necessitates using $\\hat{\\mu}$ as the origin.\nThus, the correctly preprocessed new vector is:\n$$ \\tilde{x}_{\\mathrm{new}} = x_{\\mathrm{new}} - \\hat{\\mu} $$\nThe score coordinates for this new sample, $s_{\\mathrm{new}}$, are then found by the same projection operation:\n$$ s_{\\mathrm{new}} = P^T \\tilde{x}_{\\mathrm{new}} $$\nSubstituting the expression for $\\tilde{x}_{\\mathrm{new}}$, we obtain the general formula for projecting a new (transformed) sample $x_{\\mathrm{new}}$ into the trained PCA space:\n$$ s_{\\mathrm{new}} = P^T (x_{\\mathrm{new}} - \\hat{\\mu}) $$\nThis derivation underscores why consistent preprocessing is not merely a procedural step but a conceptual necessity. The PCA model, defined by $P$ and $\\hat{\\mu}$, constitutes a fixed mapping from the original data space to the principal component space. Scientific conclusions drawn from the scores rely on the integrity of this mapping being preserved for all samples, both training and new.\n\nNow, we apply this derived expression to the specific numerical problem. The given values are:\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\\\\n0 & \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\n\\qquad\n\\hat{\\mu} \\;=\\; \\begin{pmatrix} 5 \\\\ 7 \\\\ 4 \\end{pmatrix},\n\\qquad\nx_{\\mathrm{new}} \\;=\\; \\begin{pmatrix} 6 \\\\ 8.5 \\\\ 3 \\end{pmatrix}.\n$$\nFirst, we compute the centered vector $\\tilde{x}_{\\mathrm{new}}$:\n$$\n\\tilde{x}_{\\mathrm{new}} = x_{\\mathrm{new}} - \\hat{\\mu} = \\begin{pmatrix} 6 \\\\ 8.5 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 5 \\\\ 7 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ -1 \\end{pmatrix}.\n$$\nThe problem asks for the first principal component score, which we denote as $s_{\\mathrm{new}, 1}$. This is the projection of $\\tilde{x}_{\\mathrm{new}}$ onto the first principal component vector, $p_1$, which is the first column of $P$.\n$$\np_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}.\n$$\nThe score is calculated as the dot product $p_1^T \\tilde{x}_{\\mathrm{new}}$:\n$$\ns_{\\mathrm{new}, 1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1.5 \\\\ -1 \\end{pmatrix} = (1)\\left(\\frac{1}{\\sqrt{2}}\\right) + (1.5)\\left(\\frac{1}{\\sqrt{2}}\\right) + (-1)(0).\n$$\n$$\ns_{\\mathrm{new}, 1} = \\frac{1 + 1.5}{\\sqrt{2}} = \\frac{2.5}{\\sqrt{2}}.\n$$\nEvaluating this expression numerically:\n$$\ns_{\\mathrm{new}, 1} = \\frac{2.5}{\\sqrt{2}} \\approx 1.76776695...\n$$\nRounding the result to four significant figures, as requested, gives $1.768$.",
            "answer": "$$\\boxed{1.768}$$"
        },
        {
            "introduction": "In modern computational biology, PCA is frequently used as a dimensionality reduction step before applying a supervised learning algorithm. When evaluating such a pipeline using cross-validation, a pernicious error known as data leakage can easily arise, leading to overly optimistic performance estimates and potentially spurious biological conclusions. This exercise contrasts a correct and an incorrect cross-validation procedure to illustrate how and why all data-dependent steps, including PCA, must be integrated within the validation loop to ensure the integrity of your results. ",
            "id": "3321102",
            "problem": "A research team is analyzing a single-cell ribonucleic acid sequencing (RNA-seq) dataset to discover pathway-level variation associated with an immune stimulation. The dataset consists of a gene expression matrix with $n = 2000$ cells and $p = 10000$ genes, and a binary condition label indicating stimulated versus control. They plan to use Principal Component Analysis (PCA) for dimensionality reduction followed by a classifier evaluated by $K$-fold cross-validation with $K = 5$. The preprocessing pipeline includes per-gene centering and scaling.\n\nTwo pipelines are considered:\n\n- Pipeline $\\mathcal{T}$: Within each fold, PCA is refit using only the training fold data to estimate loadings and centering/scaling parameters. Held-out cells are transformed by applying the training fold centering/scaling and projecting onto the training fold loadings to obtain principal component scores. The classifier is trained on training-fold scores and evaluated on held-out scores.\n\n- Pipeline $\\mathcal{L}$: PCA is fit once using all $n$ cells to estimate loadings and centering/scaling parameters. Within each fold, both training and held-out cells are transformed using these global parameters and loadings. The classifier is trained and evaluated as in Pipeline $\\mathcal{T}$.\n\nEmpirically, Pipeline $\\mathcal{L}$ yields a slightly higher cross-validated Area Under the Receiver Operating Characteristic (AUROC) than Pipeline $\\mathcal{T}$ when using $m = 20$ principal components, and the research team also observes that the first few principal components align strongly with the condition label.\n\nUsing core definitions of PCA as variance-maximizing orthonormal directions of the training data covariance, and the principle that cross-validation estimates require independence between training-derived transformations and held-out evaluations, answer the following multiple-choice question:\n\nWhich statements are correct regarding why PCA must be refit only on training folds and held-out scores computed by projection, and how leakage introduces interpretational bias?\n\nA. In $K$-fold cross-validation, fitting PCA on all $n$ samples causes data leakage because the eigenvectors depend on held-out data, which makes downstream performance estimates (e.g., AUROC) optimistically biased.\n\nB. The valid way to obtain held-out principal component scores is to center and scale held-out samples using training-fold statistics and then project onto training-fold loading vectors; this preserves independence required for unbiased risk estimation.\n\nC. Because PCA is unsupervised and does not use labels, fitting PCA on the full dataset within cross-validation is acceptable and cannot introduce leakage.\n\nD. Leakage in PCA can bias biological interpretation: genes with large absolute loadings may be overemphasized as biologically important for condition separation, since the loadings were influenced by held-out data structure, conflating exploratory and confirmatory analyses.\n\nE. Refitting PCA on each fold’s training data makes component directions incomparable across folds, rendering pathway-level interpretation formally invalid even if fold-consistent patterns are present.\n\nSelect all that apply.",
            "solution": "The problem statement presents a scenario comparing two data analysis pipelines, $\\mathcal{T}$ and $\\mathcal{L}$, for a single-cell RNA-seq dataset within a $K$-fold cross-validation framework. The core of the problem lies in the correct application of Principal Component Analysis (PCA) as a preprocessing step for a subsequent classification task, and the implications of an incorrect application (data leakage) on both performance evaluation and biological interpretation.\n\nFirst, let us formalize the process. The dataset is a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ with $n=2000$ cells and $p=10000$ genes. For each cell, there is a binary label. The goal of $K$-fold cross-validation is to partition the data into $K$ folds and, for each fold $k \\in \\{1, ..., K\\}$, treat the $k$-th fold as a held-out test set and the remaining $K-1$ folds as the training set. This process is repeated $K$ times, and the performance metric (e.g., AUROC) is averaged across the folds. The fundamental principle is that the test set for any fold must be completely independent of the entire model-fitting procedure, which includes any data-dependent preprocessing steps.\n\nPipeline $\\mathcal{L}$ violates this principle. It fits PCA on the entire dataset $\\mathbf{X}$ *before* the cross-validation loop. This means the centering parameters (mean $\\boldsymbol{\\mu}$), scaling parameters (standard deviation $\\boldsymbol{\\sigma}$), and the PCA loading vectors (the eigenvectors of the covariance matrix of the full dataset, which we can denote as $\\mathbf{W}$) are all determined using information from all $n$ cells. In each cross-validation fold, the held-out data was part of the dataset used to compute $\\boldsymbol{\\mu}$, $\\boldsymbol{\\sigma}$, and $\\mathbf{W}$. This constitutes data leakage.\n\nPipeline $\\mathcal{T}$ represents the correct procedure. Within each fold $k$, the centering parameters $\\boldsymbol{\\mu}^{(k)}$, scaling parameters $\\boldsymbol{\\sigma}^{(k)}$, and loading vectors $\\mathbf{W}^{(k)}$ are estimated using *only* the training data for that fold. The held-out data is then transformed using these pre-determined, training-fold-specific parameters. This mimics a real-world application where a model pipeline is trained on available data and then applied to new, unseen data, thereby providing an unbiased estimate of the model's generalization performance.\n\nThe empirical observation that Pipeline $\\mathcal{L}$ yields a higher AUROC than Pipeline $\\mathcal{T}$ is a classic symptom of data leakage. The model in Pipeline $\\mathcal{L}$ has an unfair advantage because the feature space (the principal components) it uses was constructed with partial knowledge of the test set's data structure.\n\nWith these principles established, we evaluate each statement.\n\n**A. In $K$-fold cross-validation, fitting PCA on all $n$ samples causes data leakage because the eigenvectors depend on held-out data, which makes downstream performance estimates (e.g., AUROC) optimistically biased.**\n\nThis statement is correct. PCA finds the eigenvectors (loadings) of the data's covariance matrix. In Pipeline $\\mathcal{L}$, the covariance matrix is computed from all $n$ samples. Therefore, for any given fold, the data in the held-out set has influenced the calculation of these eigenvectors. These eigenvectors define the coordinate system into which all data, including the held-out data, is projected. Because the transformation applied to the held-out data is dependent on that very same data, the independence required for unbiased performance estimation is broken. This leakage of information from the test set into the model training process typically leads to an overly optimistic performance estimate. The observed higher AUROC for Pipeline $\\mathcal{L}$ is a direct consequence of this optimistic bias.\n**Verdict: Correct.**\n\n**B. The valid way to obtain held-out principal component scores is to center and scale held-out samples using training-fold statistics and then project onto training-fold loading vectors; this preserves independence required for unbiased risk estimation.**\n\nThis statement accurately describes the correct procedure, embodied by Pipeline $\\mathcal{T}$. To obtain an unbiased estimate of the generalization error (or risk), the entire model-fitting process must be blind to the test set. This \"process\" includes all data-dependent transformations. Therefore, the statistics for centering and scaling (mean and standard deviation) and the PCA loading vectors must be computed solely from the training data of a given fold. The held-out data is then transformed using these fixed, training-derived parameters. This procedure preserves the crucial independence between the model development ecosystem and the data used for evaluation, which is the cornerstone of valid cross-validation.\n**Verdict: Correct.**\n\n**C. Because PCA is unsupervised and does not use labels, fitting PCA on the full dataset within cross-validation is acceptable and cannot introduce leakage.**\n\nThis statement is incorrect and articulates a common but critical fallacy. The fact that PCA is \"unsupervised\" (i.e., it does not use the class labels $\\mathbf{y}$) is irrelevant to the issue of data leakage in this context. Leakage occurs because information about the *features* of the test set (its distribution and covariance structure) is used to define the feature transformation (the principal components). Any data-driven transformation, supervised or not, must be included within the cross-validation loop and learned only from the training portion of the data to ensure an unbiased performance estimate. The principal components are parameters of the feature engineering step, and like all model parameters, they must be learned without access to the test set.\n**Verdict: Incorrect.**\n\n**D. Leakage in PCA can bias biological interpretation: genes with large absolute loadings may be overemphasized as biologically important for condition separation, since the loadings were influenced by held-out data structure, conflating exploratory and confirmatory analyses.**\n\nThis statement is correct. The consequences of data leakage extend beyond inflated performance metrics to the scientific conclusions drawn from the model. In bioinformatics, the PCA loadings are often interpreted to identify which original variables (genes) drive the variance captured by a principal component. The problem states that the first few PCs align with the condition label. In the leaky Pipeline $\\mathcal{L}$, these PCs are defined to maximize variance across the *entire* dataset. They are therefore \"unnaturally\" aligned to separate the conditions, having been constructed with foreknowledge of the test data's structure. When a researcher interprets the gene loadings for such a \"leaky\" PC, they may erroneously attribute strong biological importance to genes that appear to separate the conditions. This conclusion is biased because the component itself was tainted by information from the test set. It mixes the exploratory phase (finding patterns in training data) with the confirmatory phase (validating patterns on test data), potentially leading to non-reproducible scientific claims.\n**Verdict: Correct.**\n\n**E. Refitting PCA on each fold’s training data makes component directions incomparable across folds, rendering pathway-level interpretation formally invalid even if fold-consistent patterns are present.**\n\nThis statement is incorrect. While it is true that refitting PCA in each of the $K$ folds will produce $K$ distinct sets of loading vectors (i.e., the PC1 from fold 1 is not strictly identical to the PC1 from fold 2), this does not invalidate interpretation. The primary purpose of cross-validation (Pipeline $\\mathcal{T}$) is to obtain a robust estimate of the *performance* of the entire modeling strategy. The interpretation of the model's biological meaning is typically performed on a final model trained on the *entire dataset*, after the modeling strategy has been validated. Furthermore, observing a consistent pattern across the different folds (e.g., if PC1 in each fold consistently separates the conditions and is driven by a similar set of genes) would actually *strengthen* the confidence in the biological interpretation, as it suggests the finding is robust to the specific sampling of the training data. The claim that this procedure renders interpretation \"formally invalid\" is an overstatement and misrepresents best practices.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "The loading vectors derived from PCA are powerful, but they are still statistical estimates based on a finite sample. A crucial question for any rigorous analysis is: how stable are these loadings? If we were to collect a slightly different dataset, would the major contributing genes to a principal component change dramatically? This hands-on coding exercise introduces the non-parametric bootstrap, a powerful computational method to estimate confidence intervals for PCA loadings, allowing you to quantify the uncertainty and robustness of your interpretations. ",
            "id": "3321032",
            "problem": "You are given a computational systems biology scenario in which a gene expression data matrix is analyzed by Principal Component Analysis (PCA). Let the data be represented by a real-valued matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n$ samples (e.g., biological conditions) and $p$ variables (e.g., genes). The matrix is column-centered before PCA, so each gene has zero mean across samples. Principal Component Analysis (PCA) is defined from first principles as follows: compute the sample covariance matrix $C = \\frac{1}{n-1} X^\\top X$, then obtain its spectral decomposition or, equivalently, compute a Singular Value Decomposition (SVD) of the centered data matrix $X$ as $X = U \\Sigma V^\\top$ with orthonormal matrices $U$ and $V$, and diagonal matrix of singular values $\\Sigma$. The PCA loadings for the variables are the columns of $V$, and the PCA scores for the samples are given by $T = U \\Sigma$. Because PCA axes are only defined up to a sign, the loadings $v_k$ for principal component $k$ and $-v_k$ define the same axis; thus any interpretation must be sign-invariant.\n\nYou will implement a nonparametric bootstrap procedure to estimate confidence intervals for the loadings of the first principal component of variables (genes) by resampling samples. The bootstrap procedure must adhere to the following scientifically grounded steps:\n\n1. Begin from the column-centered data matrix $X$ and compute a reference loading vector $v_{\\text{ref}} \\in \\mathbb{R}^{p}$ for the first principal component from the SVD $X = U \\Sigma V^\\top$; this is the first column of $V$.\n2. For each bootstrap replicate $b \\in \\{1,2,\\dots,B\\}$:\n   - Resample $n$ rows from $X$ with replacement to form a bootstrap sample $X^{(b)}$, then re-center $X^{(b)}$ columnwise to obtain zero-mean genes in the bootstrap sample.\n   - Compute the first loading vector $v^{(b)}$ from an SVD of $X^{(b)}$.\n   - Resolve sign indeterminacy by aligning $v^{(b)}$ with $v_{\\text{ref}}$ using the rule: if $v_{\\text{ref}}^\\top v^{(b)} < 0$, replace $v^{(b)}$ by $-v^{(b)}$; otherwise leave $v^{(b)}$ unchanged.\n3. After $B$ aligned replicates, for each gene index $j \\in \\{1,2,\\dots,p\\}$, compute the empirical two-sided $(1-\\alpha)$ confidence interval for the first-component loading $v_j$ as $\\left[q_{j,\\alpha/2}, q_{j,1-\\alpha/2}\\right]$, where $q_{j,\\gamma}$ is the sample quantile at probability $\\gamma$ of the bootstrap distribution $\\{v^{(b)}_j\\}_{b=1}^B$.\n4. Quantify loading stability with two metrics:\n   - The fraction of genes whose confidence interval excludes $0$, that is the fraction of indices $j$ such that $q_{j,\\alpha/2} > 0$ or $q_{j,1-\\alpha/2} < 0$.\n   - The median confidence interval width across genes, defined as the median of $\\{q_{j,1-\\alpha/2} - q_{j,\\alpha/2}\\}_{j=1}^p$.\n5. Declare intervals “wide” if the median confidence interval width exceeds a fixed threshold $w^\\star$. Use $w^\\star = 0.15$.\n\nYou must generate synthetic data that reflects a plausible gene-module structure in computational systems biology, using a rank-$2$ latent-factor model with orthonormal true loading directions. Specifically, for given $n$, $p$, score standard deviations $\\tau_1$, $\\tau_2$ for the first two latent sample factors, and noise standard deviation $\\sigma$, construct two orthonormal loading vectors $v_1, v_2 \\in \\mathbb{R}^p$ such that $v_1$ has equal nonzero entries on the first $12$ genes and zeros elsewhere, and $v_2$ has equal nonzero entries on the next $12$ genes and zeros elsewhere. Normalize $v_1$ and $v_2$ to unit norm so that they are orthonormal. Then generate samples for $i \\in \\{1,\\dots,n\\}$ as\n$$\nX_{i,\\cdot} = s_{1,i} v_1^\\top + s_{2,i} v_2^\\top + \\varepsilon_{i,\\cdot},\n$$\nwhere $s_{1,i} \\sim \\mathcal{N}(0,\\tau_1^2)$, $s_{2,i} \\sim \\mathcal{N}(0,\\tau_2^2)$, and $\\varepsilon_{i,\\cdot} \\sim \\mathcal{N}(0,\\sigma^2 I_p)$ independently across $i$, with $I_p$ the $p \\times p$ identity matrix. Ensure the matrix is column-centered prior to PCA. This construction models a gene expression dataset in which two disjoint gene modules drive the first two principal components.\n\nDesign and implement the program to:\n\n- Generate $X$ for each test case as described.\n- Compute the reference first loading vector.\n- Perform the bootstrap with sign alignment.\n- Compute the two stability metrics and the “wide intervals” boolean flag.\n\nSet the confidence level to $(1-\\alpha) = 0.95$, i.e., $\\alpha = 0.05$, and use $B$ bootstrap replicates for each test case. Fix the random generator seed to $0$ for reproducibility.\n\nInterpretation guideline to encode in the solution reasoning: wide intervals indicate instability of loadings and can arise from high noise ($\\sigma$ large), small sample size ($n$ small), or near-degenerate leading eigenvalues (e.g., $\\tau_1 \\approx \\tau_2$), which cause component mixing across bootstrap samples even after sign alignment. In such cases, individual gene contributions to the component are not reliably determined.\n\nTest Suite:\nUse the following test suite of parameter tuples $(n, p, \\sigma, B, \\tau_1, \\tau_2)$:\n\n- Case $1$ (happy path): $(80, 40, 0.30, 250, 2.00, 1.00)$\n- Case $2$ (high noise boundary): $(40, 40, 0.70, 250, 1.50, 1.00)$\n- Case $3$ (near-degenerate factors, edge case): $(20, 40, 0.50, 250, 1.50, 1.50)$\n\nAnswer Specification:\nFor each test case, produce a list $[f, m, w]$ where $f$ is the fraction of genes whose interval excludes $0$ (expressed as a decimal rounded to $3$ decimal places), $m$ is the median confidence interval width (rounded to $3$ decimal places), and $w$ is a boolean indicating whether the intervals are “wide” under the threshold $w^\\star = 0.15$. Aggregate the results of all test cases into a single line of output containing a list of these per-case lists, printed exactly as a Python list literal. For example, the final output format must be of the form\n$$\n[[f_1, m_1, w_1],[f_2, m_2, w_2],[f_3, m_3, w_3]]\n$$\nwith no extra text. No physical units are involved. Angles do not appear. Percentages must be expressed as decimals, not with a percent sign.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is itself a list in the specified format. All floating-point values must be rounded to $3$ decimal places.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of statistical learning and computational biology, well-posed with a clear algorithmic specification, and objective in its formulation. The task is to implement a bootstrap procedure to evaluate the stability of Principal Component Analysis (PCA) loadings, a crucial step for the valid biological interpretation of PCA results.\n\nThe solution proceeds in four main stages:\n1.  Generation of synthetic gene expression data based on a latent factor model.\n2.  Computation of a reference PCA loading vector from the full dataset.\n3.  Execution of a non-parametric bootstrap procedure to generate a distribution of loading vectors.\n4.  Calculation of confidence intervals and stability metrics from the bootstrap distribution.\n\n**1. Synthetic Data Generation**\nThe foundation of this analysis is a synthetic dataset $X \\in \\mathbb{R}^{n \\times p}$ that models gene expression with a known underlying structure. The data for $n$ samples and $p$ genes is generated from a rank-$2$ latent factor model:\n$$\nX_{i,\\cdot} = s_{1,i} v_1^\\top + s_{2,i} v_2^\\top + \\varepsilon_{i,\\cdot}\n$$\nHere, $X_{i,\\cdot}$ is the row vector of expression values for sample $i$. The vectors $v_1, v_2 \\in \\mathbb{R}^p$ represent the true-state \"loading\" vectors, modeling two modules of co-regulated genes. They are constructed to be orthonormal. Specifically, $v_1$ has non-zero entries only for the first $12$ genes, and $v_2$ has non-zero entries for the next $12$ genes (genes $13$ through $24$). To ensure unit norm, $\\|v_k\\|_2=1$, the non-zero entries are set to $1/\\sqrt{12}$.\nThe scores $s_{1,i}$ and $s_{2,i}$ are drawn independently from Normal distributions, $s_{1,i} \\sim \\mathcal{N}(0, \\tau_1^2)$ and $s_{2,i} \\sim \\mathcal{N}(0, \\tau_2^2)$, representing the activity of the two latent biological processes across the $n$ samples. The matrix $\\varepsilon$ of the same dimensions as $X$ represents biological and technical noise, with each element drawn from $\\mathcal{N}(0, \\sigma^2)$. The relative magnitudes of $\\tau_1, \\tau_2,$ and $\\sigma$ determine the signal-to-noise ratio and the separability of the principal components.\nFinally, the generated matrix $X$ is column-centered, such that each gene (column) has a mean of $0$ across all samples. This is a prerequisite for PCA performed via SVD on the data matrix or via spectral decomposition of the covariance matrix.\n\n**2. Reference PCA and Loading Vector**\nPCA is performed on the centered data matrix, which we also denote as $X$. The Singular Value Decomposition (SVD) of $X$ is computed:\n$$\nX = U \\Sigma V^\\top\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{p \\times p}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times p}$ is a rectangular diagonal matrix of singular values. The columns of $V$ are the principal loading vectors. The first column of $V$, denoted $v_{\\text{ref}}$, is the loading vector for the first principal component (PC1). This vector's elements, $\\{v_{\\text{ref}, j}\\}_{j=1}^p$, quantify the contribution of each gene to PC1.\n\n**3. Bootstrap Procedure for Confidence Intervals**\nTo assess the stability of the estimated loadings in $v_{\\text{ref}}$, we employ a non-parametric bootstrap. This resampling technique simulates the process of collecting new datasets to see how much the estimated loadings would vary.\nThe procedure is executed for $B$ replicates:\n1.  A bootstrap sample $X^{(b)}$ is created by sampling $n$ rows from the centered data matrix $X$ with replacement.\n2.  Each bootstrap sample $X^{(b)}$ is re-centered to have column means of zero. This is crucial because resampling alters the column means.\n3.  SVD is performed on the re-centered bootstrap sample $X^{(b)}$ to obtain its first loading vector, $v^{(b)}$.\n4.  A critical step is to resolve the inherent sign ambiguity of eigenvectors. The direction of $v^{(b)}$ is aligned with the reference vector $v_{\\text{ref}}$. If the dot product $v_{\\text{ref}}^\\top v^{(b)}$ is negative, $v^{(b)}$ is flipped by multiplying by $-1$. This ensures all bootstrap loading vectors populate a unimodal distribution, provided there is no component swapping.\nThis process yields a set of $B$ aligned loading vectors $\\{v^{(b)}\\}_{b=1}^B$.\n\n**4. Stability Metrics Calculation**\nFrom the bootstrap distribution of loadings, we derive quantitative stability metrics. For each gene $j$, the distribution of its loading values across a large number of bootstrap replicates $\\{v_j^{(b)}\\}_{b=1}^B$ is used to construct a two-sided $(1-\\alpha)$ confidence interval (CI). With $\\alpha=0.05$, we use the percentile method, defining the CI as $[q_{j,0.025}, q_{j,0.975}]$, where $q_{j,\\gamma}$ is the sample quantile at probability $\\gamma$.\n\nTwo metrics are computed from these CIs:\n1.  **Fraction of CIs excluding zero:** This is the proportion of genes for which the $95\\%$ CI does not contain $0$. This identifies genes with a statistically stable, non-zero contribution to PC1. A high fraction suggests a robust and interpretable PC.\n2.  **Median CI width:** The width of the CI for gene $j$ is $q_{j,0.975} - q_{j,0.025}$. The median of these widths across all $p$ genes provides a single summary statistic for the overall precision of the loading estimates.\nA small median width indicates high stability. The problem defines intervals as \"wide\" if this median width exceeds a threshold $w^\\star = 0.15$.\n\nWide intervals, indicating loading instability, are symptomatic of several conditions. High noise (large $\\sigma$) or a small sample size ($n$) can obscure the underlying signal. Furthermore, if the leading singular values are close in magnitude (e.g., $\\tau_1 \\approx \\tau_2$), the corresponding principal components become poorly separated. In bootstrap replicates, the first and second PCs can \"swap,\" meaning the calculated $v^{(b)}$ corresponds to the original data's second PC. This leads to very wide, possibly bimodal, loading distributions that the simple sign-alignment cannot rectify, correctly reflecting the instability of the component's identity.\n\nThe implementation will systematically apply this entire pipeline to each test case, using a fixed random seed for reproducibility.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the bootstrap stability analysis for PCA loadings.\n    \"\"\"\n    # Initialize a random number generator with a fixed seed for reproducibility.\n    # The problem asks to fix the random generator seed to 0.\n    rng = np.random.default_rng(0)\n\n    # Define the test cases from the problem statement.\n    # Format: (n, p, sigma, B, tau1, tau2)\n    test_cases = [\n        (80, 40, 0.30, 250, 2.00, 1.00),  # Case 1: happy path\n        (40, 40, 0.70, 250, 1.50, 1.00),  # Case 2: high noise boundary\n        (20, 40, 0.50, 250, 1.50, 1.50),  # Case 3: near-degenerate factors\n    ]\n\n    # Global parameters\n    alpha = 0.05\n    w_star = 0.15\n    num_genes_in_module = 12\n\n    results = []\n    for n, p, sigma, B, tau1, tau2 in test_cases:\n        # 1. Generate synthetic data\n        # Construct true orthonormal loading vectors v1 and v2\n        v1 = np.zeros(p)\n        v1[:num_genes_in_module] = 1.0 / np.sqrt(num_genes_in_module)\n        \n        v2 = np.zeros(p)\n        v2[num_genes_in_module:2 * num_genes_in_module] = 1.0 / np.sqrt(num_genes_in_module)\n\n        # Generate scores from Normal distributions\n        s1 = rng.normal(0, tau1, n)\n        s2 = rng.normal(0, tau2, n)\n        S = np.c_[s1, s2]  # Shape (n, 2)\n\n        V_true = np.vstack([v1, v2])  # Shape (2, p)\n\n        # Generate noise\n        E = rng.normal(0, sigma, size=(n, p))\n\n        # Construct the data matrix X as per the latent factor model\n        X = S @ V_true + E\n\n        # Column-center the data matrix, a prerequisite for PCA\n        X_centered = X - X.mean(axis=0)\n\n        # 2. Compute reference loading vector from the full dataset\n        # SVD: X = U * Sigma * V^T. np.linalg.svd returns U, s, Vt\n        _, _, Vt_ref = np.linalg.svd(X_centered, full_matrices=False)\n        v_ref = Vt_ref[0, :]  # First loading vector is the first row of V^T\n\n        # 3. Perform bootstrap procedure\n        bootstrap_loadings = np.zeros((B, p))\n        for b in range(B):\n            # Resample n rows with replacement from the centered data\n            indices = rng.choice(n, size=n, replace=True)\n            X_b = X_centered[indices, :]\n\n            # Re-center the bootstrap sample\n            X_b_centered = X_b - X_b.mean(axis=0)\n\n            # Compute SVD for the bootstrap sample\n            try:\n                _, _, Vt_b = np.linalg.svd(X_b_centered, full_matrices=False)\n                v_b = Vt_b[0, :]\n            except np.linalg.LinAlgError:\n                # In rare cases, a bootstrap sample can be rank-deficient\n                # in a way that SVD fails. We can skip such a replicate.\n                # A robust implementation might replace it, but for B=250, skipping is fine.\n                v_b = np.full(p, np.nan)\n\n\n            # Resolve sign indeterminacy by aligning with the reference vector\n            if np.dot(v_ref, v_b) < 0:\n                v_b = -v_b\n\n            bootstrap_loadings[b, :] = v_b\n        \n        # Remove any NaN rows if SVD failed\n        bootstrap_loadings = bootstrap_loadings[~np.isnan(bootstrap_loadings).any(axis=1)]\n\n\n        # 4. Compute stability metrics\n        # Calculate the lower and upper quantiles for the confidence intervals\n        q_low, q_high = np.quantile(bootstrap_loadings, [alpha / 2, 1 - alpha / 2], axis=0)\n\n        # Metric 1: Fraction of genes whose CI excludes 0\n        ci_excludes_zero = (q_low > 0) | (q_high < 0)\n        frac_exclude_zero = np.mean(ci_excludes_zero)\n\n        # Metric 2: Median confidence interval width\n        widths = q_high - q_low\n        median_width = np.median(widths)\n\n        # Metric 3: \"Wide intervals\" flag\n        is_wide = median_width > w_star\n\n        # Append formatted results to the list\n        results.append([\n            round(frac_exclude_zero, 3),\n            round(median_width, 3),\n            is_wide\n        ])\n    \n    # Final print statement in the exact required format.\n    # The string representation of a list in Python is its literal form.\n    print(results)\n\n\nsolve()\n```",
            "answer": "`[[0.3, 0.082, False], [0.3, 0.177, True], [0.0, 0.729, True]]`"
        }
    ]
}