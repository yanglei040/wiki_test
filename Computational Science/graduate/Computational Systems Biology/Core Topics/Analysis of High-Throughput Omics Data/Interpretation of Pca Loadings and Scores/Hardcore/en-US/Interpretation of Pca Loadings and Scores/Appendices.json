{
    "hands_on_practices": [
        {
            "introduction": "A primary application of Principal Component Analysis (PCA) in systems biology is to create a low-dimensional map from a reference dataset, into which new samples can be placed. This exercise solidifies the fundamental mechanics of projecting a new data point into a pre-trained PCA space. Critically, it highlights why applying the exact same preprocessing steps used on the training data—especially centering with the original training mean—is essential for a scientifically consistent comparison .",
            "id": "3321042",
            "problem": "A transcriptomics study measures messenger RNA (mRNA) abundances for $m$ genes across $n$ training samples and performs Principal Component Analysis (PCA) on the preprocessed training data. The preprocessing pipeline consists of a fixed transformation applied to all training samples (log transformation to stabilize variance), followed by mean-centering across the training cohort. No variance scaling is applied beyond mean-centering. The PCA model is trained by computing the sample covariance matrix of the mean-centered data and performing eigen-decomposition to obtain an orthonormal loading matrix $P \\in \\mathbb{R}^{m \\times k}$ whose columns are eigenvectors corresponding to the top $k$ eigenvalues. The score coordinates of a training sample are defined by orthogonal projection of its mean-centered vector onto the loading subspace.\n\nFor a new biological replicate with raw measurements represented by $x_{\\mathrm{new}} \\in \\mathbb{R}^{m}$, projection into the trained principal component subspace must be done consistently with the training pipeline. Starting from core definitions of orthogonal projection onto an orthonormal basis and the construction of PCA from the eigen-decomposition of the covariance of the preprocessed training data, derive the expression for the score coordinates of $x_{\\mathrm{new}}$ in terms of the training mean vector $\\hat{\\mu}$ and the loading matrix $P$. Explain why applying identical preprocessing to $x_{\\mathrm{new}}$ as was applied to the training data (including the fixed transformation and centering by $\\hat{\\mu}$) is necessary for scientific consistency and interpretability in a computational systems biology setting.\n\nThen, apply your derivation to the following scientifically plausible metabolite panel with $m=3$ variables and $k=2$ components. The trained loading matrix (columns orthonormal) and the training mean vector computed after preprocessing are\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{6}} \\\\\n0  \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\n\\qquad\n\\hat{\\mu} \\;=\\; \\begin{pmatrix} 5 \\\\ 7 \\\\ 4 \\end{pmatrix}.\n$$\nA new sample, after applying the identical fixed transformation used in training, has measurements\n$$\nx_{\\mathrm{new}} \\;=\\; \\begin{pmatrix} 6 \\\\ 8.5 \\\\ 3 \\end{pmatrix}.\n$$\nCompute the first principal component score for this new sample based on your derived projection expression. Round your numerical answer to four significant figures. Express your final answer without units.",
            "solution": "The problem as stated is valid. It presents a standard, well-posed scenario in computational systems biology involving the application of Principal Component Analysis (PCA). The premises are scientifically sound, the definitions are clear, and all required information for a solution is provided.\n\nThe primary task is to derive the expression for the score coordinates of a new sample, $x_{\\mathrm{new}}$, projected into a PCA space that was constructed from a training dataset. This requires a systematic application of linear algebra and an understanding of the PCA methodology.\n\nLet the training data be represented by a matrix where each of the $n$ columns is a sample vector of $m$ measurements (e.g., gene abundances). The preprocessing pipeline on the training data involves a fixed transformation function, $f$, applied element-wise, followed by mean-centering. Let $x'_{i}$ be the $i$-th training sample after the application of $f$. The training mean vector, $\\hat{\\mu}$, is computed from these transformed samples:\n$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x'_{i} $$\nThe final preprocessed training vectors, $\\tilde{x}_i$, are the mean-centered vectors:\n$$ \\tilde{x}_i = x'_{i} - \\hat{\\mu} $$\nPCA is performed by finding the eigenvectors of the sample covariance matrix of the set of vectors $\\{\\tilde{x}_i\\}_{i=1}^n$. The loading matrix, $P \\in \\mathbb{R}^{m \\times k}$, is constructed by taking the $k$ orthonormal eigenvectors corresponding to the $k$ largest eigenvalues as its columns. These columns, $\\{p_1, p_2, \\dots, p_k\\}$, form an orthonormal basis for the principal subspace, which captures the maximal variance of the training data.\n\nThe score coordinates of any vector in the preprocessed space are found by projecting it onto this orthonormal basis. For a preprocessed training sample $\\tilde{x}_i$, its score vector $s_i \\in \\mathbb{R}^k$ is obtained by computing the dot product with each basis vector. The $j$-th score, $s_{ij}$, is $p_j^T \\tilde{x}_i$. In matrix form, this is:\n$$ s_i = P^T \\tilde{x}_i $$\nFor a new sample with raw measurements $x_{\\mathrm{raw, new}}$, its projection into the PCA space is meaningful only if it undergoes the exact same preprocessing transformation as the training data. This is a fundamental requirement for scientific consistency and interpretability.\n$1$. The fixed transformation $f$ is applied: $x_{\\mathrm{new}} = f(x_{\\mathrm{raw, new}})$. The problem provides this post-transformation vector directly.\n$2$. The vector is centered using the *training data's mean vector*, $\\hat{\\mu}$. This step is critical because the principal components are defined relative to the center of the training data cloud. To compare the new sample to the training samples, it must be placed within the same coordinate system, which necessitates using $\\hat{\\mu}$ as the origin.\nThus, the correctly preprocessed new vector is:\n$$ \\tilde{x}_{\\mathrm{new}} = x_{\\mathrm{new}} - \\hat{\\mu} $$\nThe score coordinates for this new sample, $s_{\\mathrm{new}}$, are then found by the same projection operation:\n$$ s_{\\mathrm{new}} = P^T \\tilde{x}_{\\mathrm{new}} $$\nSubstituting the expression for $\\tilde{x}_{\\mathrm{new}}$, we obtain the general formula for projecting a new (transformed) sample $x_{\\mathrm{new}}$ into the trained PCA space:\n$$ s_{\\mathrm{new}} = P^T (x_{\\mathrm{new}} - \\hat{\\mu}) $$\nThis derivation underscores why consistent preprocessing is not merely a procedural step but a conceptual necessity. The PCA model, defined by $P$ and $\\hat{\\mu}$, constitutes a fixed mapping from the original data space to the principal component space. Scientific conclusions drawn from the scores rely on the integrity of this mapping being preserved for all samples, both training and new.\n\nNow, we apply this derived expression to the specific numerical problem. The given values are:\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{6}} \\\\\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{6}} \\\\\n0  \\frac{2}{\\sqrt{6}}\n\\end{pmatrix},\n\\qquad\n\\hat{\\mu} \\;=\\; \\begin{pmatrix} 5 \\\\ 7 \\\\ 4 \\end{pmatrix},\n\\qquad\nx_{\\mathrm{new}} \\;=\\; \\begin{pmatrix} 6 \\\\ 8.5 \\\\ 3 \\end{pmatrix}.\n$$\nFirst, we compute the centered vector $\\tilde{x}_{\\mathrm{new}}$:\n$$\n\\tilde{x}_{\\mathrm{new}} = x_{\\mathrm{new}} - \\hat{\\mu} = \\begin{pmatrix} 6 \\\\ 8.5 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 5 \\\\ 7 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ -1 \\end{pmatrix}.\n$$\nThe problem asks for the first principal component score, which we denote as $s_{\\mathrm{new}, 1}$. This is the projection of $\\tilde{x}_{\\mathrm{new}}$ onto the first principal component vector, $p_1$, which is the first column of $P$.\n$$\np_1 = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}.\n$$\nThe score is calculated as the dot product $p_1^T \\tilde{x}_{\\mathrm{new}}$:\n$$\ns_{\\mathrm{new}, 1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1.5 \\\\ -1 \\end{pmatrix} = (1)\\left(\\frac{1}{\\sqrt{2}}\\right) + (1.5)\\left(\\frac{1}{\\sqrt{2}}\\right) + (-1)(0).\n$$\n$$\ns_{\\mathrm{new}, 1} = \\frac{1 + 1.5}{\\sqrt{2}} = \\frac{2.5}{\\sqrt{2}}.\n$$\nEvaluating this expression numerically:\n$$\ns_{\\mathrm{new}, 1} = \\frac{2.5}{\\sqrt{2}} \\approx 1.76776695...\n$$\nRounding the result to four significant figures, as requested, gives $1.768$.",
            "answer": "$$\\boxed{1.768}$$"
        },
        {
            "introduction": "After performing PCA, a key challenge is deciding how many components are biologically or technically meaningful. This practice moves beyond simple heuristics like scree plots to a more sophisticated, science-driven approach to component selection. By analyzing a realistic transcriptomics scenario, you will learn to integrate the statistical measure of variance explained with the biological interpretation of loadings and scores to identify signals of interest, technical artifacts, and even rare but important subpopulations .",
            "id": "3321098",
            "problem": "A transcriptomics study measures messenger RNA abundance for $p$ genes across $n$ samples, yielding a data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ whose columns (genes) have been log-transformed, centered, and scaled to unit variance across samples. Principal Component Analysis (PCA) is performed on the sample covariance matrix $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\top}\\mathbf{X}$. The eigenvalues of $\\mathbf{S}$ in descending order are $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{p} \\ge 0$, with corresponding orthonormal eigenvectors giving the loadings, and the principal component scores are the projections of samples onto these eigenvectors. For this dataset, the observed proportions of variance explained by the first six principal components are $0.38$, $0.22$, $0.09$, $0.04$, $0.02$, and $0.01$, respectively. The remainder of the variance is distributed across higher-order components. Loadings and scores diagnostics reveal the following:\n- Principal Component $1$ scores correlate strongly with per-sample Unique Molecular Identifier (UMI) counts ($r \\approx 0.93$), and loadings are enriched for ribosomal and mitochondrial gene sets, suggesting a technical or compositional effect.\n- Principal Component $2$ scores correlate with interferon treatment dose ($r \\approx 0.81$), and loadings are enriched for interferon-stimulated genes, suggesting a treatment gradient.\n- Principal Component $3$ scores align with batch labels across two sequencing runs ($r \\approx 0.88$) and negatively with RNA Integrity Number (RIN) ($r \\approx -0.62$), and loadings include stress-response and mitochondrial genes, suggesting a batch/quality artifact.\n- Principal Component $4$ has large positive loadings concentrated on a compact set of T-cell marker genes; only approximately $5\\%$ of samples have high positive scores on Principal Component $4$, and these scores correlate with independent flow-cytometry estimates of a rare T-cell subset ($r \\approx 0.77$), suggesting a biologically meaningful but low-prevalence subpopulation.\n\nUsing only the standard definitions of PCA (eigendecomposition of $\\mathbf{S}$ and the interpretation of eigenvalues as variances of principal components), derive a general expression for the cumulative variance explained as a function of the number of components $k$, and then, in light of both the cumulative variance profile and the domain knowledge encoded in the loadings and scores above, decide how many components you would interpret in this study and why.\n\nSelect the single option that both derives the cumulative variance explained function correctly from first principles and gives a justified choice of $k$ for interpretation in this scenario.\n\nA. $C(k) = \\dfrac{\\sum_{i=1}^{k} \\sigma_{i}}{\\sum_{j=1}^{\\min(n,p)} \\sigma_{j}}$, where $\\sigma_{i}$ are singular values of the centered $\\mathbf{X}$; choose $k=2$ because of a clear elbow after Principal Component $2$, and additional components are small and therefore ignorable even if they show specific gene set enrichment.\n\nB. $C(k) = \\dfrac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{j=1}^{p} \\lambda_{j}}$, where $\\lambda_{i}$ are the eigenvalues of $\\mathbf{S}$; choose to examine up to $k=4$ because $C(4) = 0.38+0.22+0.09+0.04 = 0.73$ exceeds a reasonable information-capture target for exploratory interpretation, Principal Component $2$ captures the treatment gradient, Principal Component $1$ and Principal Component $3$ are technical/confounding effects to be controlled rather than biologically interpreted, and Principal Component $4$—despite low variance—aligns with a rare but biologically validated T-cell subset.\n\nC. $C(k) = \\dfrac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{j=1}^{p} \\lambda_{j}}$, where $\\lambda_{i}$ are the eigenvalues of $\\mathbf{S}$; choose $k$ as the smallest number so that $C(k) \\ge 0.80$, which necessitates including many more than $6$ components if needed, because variance explained should be the sole criterion even if later components have noisy loadings without clear biological associations.\n\nD. $C(k) = \\sum_{g=1}^{p} \\ell_{gk}^{2}$, where $\\ell_{gk}$ are the loadings for gene $g$ on component $k$; choose $k=3$ because the sum of squared loadings on the first three components is large and Principal Component $4$ accounts for little variance and should be discarded regardless of its gene-level interpretation.",
            "solution": "The user has provided a problem concerning the interpretation of a Principal Component Analysis (PCA, Principal Component Analysis) performed on a transcriptomics dataset. The task is to validate the problem statement, derive an expression for the cumulative variance explained, and justify the number of components, $k$, to retain for interpretation.\n\n### Step 1: Extract Givens\n\n- Data matrix: $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, representing $p$ genes and $n$ samples.\n- Data preprocessing: Columns (genes) are log-transformed, centered, and scaled to unit variance.\n- PCA procedure: Eigendecomposition is performed on the sample covariance matrix $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\top}\\mathbf{X}$.\n- Eigenvalues of $\\mathbf{S}$: $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{p} \\ge 0$.\n- Loadings: Orthonormal eigenvectors of $\\mathbf{S}$.\n- Scores: Projections of samples onto the eigenvectors.\n- Observed proportions of variance explained for Principal Components (PCs) $1$ through $6$: $0.38$, $0.22$, $0.09$, $0.04$, $0.02$, and $0.01$.\n- PC1 interpretation: Correlates with per-sample Unique Molecular Identifier (UMI, Unique Molecular Identifier) counts ($r \\approx 0.93$); loadings enriched for ribosomal and mitochondrial genes; suggested technical/compositional effect.\n- PC2 interpretation: Correlates with interferon treatment dose ($r \\approx 0.81$); loadings enriched for interferon-stimulated genes; suggested treatment gradient.\n- PC3 interpretation: Correlates with batch labels ($r \\approx 0.88$) and negatively with RNA Integrity Number (RIN, RNA Integrity Number) ($r \\approx -0.62$); loadings include stress-response and mitochondrial genes; suggested batch/quality artifact.\n- PC4 interpretation: Large positive loadings on T-cell marker genes; $\\approx 5\\%$ of samples have high positive scores, which correlate with flow-cytometry estimates of a rare T-cell subset ($r \\approx 0.77$); suggested biologically meaningful subpopulation.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is scientifically sound, well-posed, and objective.\n1.  **Scientific Grounding**: The setup describes a standard workflow for PCA in transcriptomics. The data matrix dimensions, preprocessing steps (log-transformation, centering, scaling), definition of the covariance matrix $\\mathbf{S}$, and the relationship between its eigendecomposition and the PCA results (eigenvalues as component variances, eigenvectors as loadings) are all correct and fundamental to the method. The use of a covariance matrix on data that has been scaled to unit variance is equivalent to using a correlation matrix, which is a standard and valid approach. The biological context, including references to UMI, RIN, batch effects, interferon stimulation, and cell subset identification, are all highly realistic and common topics in modern genomics.\n2.  **Well-Posedness**: The problem asks for a derivation and a justification, both of which can be addressed using the provided information and standard statistical theory. A unique, meaningful conclusion can be reached.\n3.  **Objectivity**: The problem is stated using precise, quantitative language (e.g., correlation coefficients, variance proportions) and avoids subjective claims. The interpretations of the PCs are qualified with the word \"suggesting\", which is appropriate for exploratory data analysis.\n\nThe problem statement exhibits none of the invalidating flaws. It is internally consistent, scientifically grounded, and provides all necessary information to proceed with a solution.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. I will proceed to derive the solution and evaluate the options.\n\n### Solution Derivation\n\nThe first task is to derive a general expression for the cumulative proportion of variance explained, $C(k)$, by the first $k$ principal components.\n\nThe total variance of the dataset is the sum of the variances of the individual variables (genes). The problem states that the gene columns of the data matrix $\\mathbf{X}$ have been centered and scaled to have unit variance. Let the $j$-th column of $\\mathbf{X}$ be $\\mathbf{x}_j$. Then, $\\text{Var}(\\mathbf{x}_j) = 1$ for $j=1, \\ldots, p$.\nThe total variance in the dataset is therefore:\n$$\n\\text{Total Variance} = \\sum_{j=1}^{p} \\text{Var}(\\mathbf{x}_j) = \\sum_{j=1}^{p} 1 = p\n$$\nThe sample covariance matrix is given as $\\mathbf{S} = \\frac{1}{n-1}\\mathbf{X}^{\\top}\\mathbf{X}$. The diagonal elements of this matrix, $S_{jj}$, represent the sample variances of the genes. Since the data were scaled to unit variance, $S_{jj} = 1$ for all $j=1, \\ldots, p$.\nThe trace of a square matrix is the sum of its diagonal elements. Thus, the trace of the covariance matrix is equal to the total variance:\n$$\n\\text{Tr}(\\mathbf{S}) = \\sum_{j=1}^{p} S_{jj} = \\sum_{j=1}^{p} 1 = p\n$$\nA fundamental property of linear algebra is that the trace of a matrix is also equal to the sum of its eigenvalues. Let the eigenvalues of $\\mathbf{S}$ be $\\lambda_1, \\lambda_2, \\ldots, \\lambda_p$.\n$$\n\\text{Tr}(\\mathbf{S}) = \\sum_{j=1}^{p} \\lambda_j\n$$\nTherefore, the total variance in the dataset is captured by the sum of the eigenvalues of the covariance matrix: $\\sum_{j=1}^{p} \\lambda_j = p$.\n\nIn PCA, the variance captured by the $i$-th principal component is equal to the $i$-th eigenvalue, $\\lambda_i$. The cumulative variance explained by the first $k$ components is the sum of their individual variances: $\\sum_{i=1}^{k} \\lambda_i$.\nThe cumulative *proportion* of variance explained, $C(k)$, is the ratio of this cumulative variance to the total variance.\n$$\nC(k) = \\frac{\\text{Variance explained by first } k \\text{ PCs}}{\\text{Total Variance}} = \\frac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{j=1}^{p} \\lambda_{j}}\n$$\nThis expression is the correct general formula.\n\nThe second task is to decide how many components to interpret. This requires consideration of both the variance explained and the biological/technical meaning of each component.\n- **PC1 (38% variance)**: Identified as a major technical artifact related to sequencing depth (UMI counts). This is a crucial finding, as it represents a strong confounding effect that must be accounted for in any downstream analysis. It must be interpreted, but as a confounder, not a biological signal of interest.\n- **PC2 (22% variance)**: Captures the biological effect of interest (interferon treatment gradient). This component is central to the study's objective.\n- **PC3 (9% variance)**: Identified as another technical artifact related to batch and sample quality (RIN). Similar to PC1, this is an important confounder to identify.\n- **PC4 (4% variance)**: Although explaining little variance, this component isolates a rare but biologically meaningful cell subpopulation, independently validated by flow cytometry. In exploratory studies, discovering such subtle, low-prevalence biological signals is a primary goal. Discarding this component based on its low variance would be a significant scientific error, as it would mean overlooking a key finding.\n\nA naive approach, such as using a scree plot elbow (which might appear after PC2, where variance drops from $0.22$ to $0.09$) or a fixed variance cutoff, is inappropriate here. A sophisticated analysis requires interpreting all components that reveal significant structure, whether it is the biological signal of interest, a confounding technical variable, or a rare but validated biological phenomenon. Therefore, one must examine *at least* the first four components to gain a comprehensive understanding of the data structure. PC1 and PC3 reveal critical confounders, PC2 captures the main experimental signal, and PC4 uncovers an important secondary biological finding.\n\n### Option-by-Option Analysis\n\n**A. $C(k) = \\dfrac{\\sum_{i=1}^{k} \\sigma_{i}}{\\sum_{j=1}^{\\min(n,p)} \\sigma_{j}}$, where $\\sigma_{i}$ are singular values of the centered $\\mathbf{X}$; choose $k=2$ because of a clear elbow after Principal Component $2$, and additional components are small and therefore ignorable even if they show specific gene set enrichment.**\n\n- **Formula**: The formula is incorrect. The variance of a PC is proportional to the *square* of the corresponding singular value ($\\lambda_i \\propto \\sigma_i^2$), not the singular value itself. The correct formula should involve $\\sigma_i^2$.\n- **Justification**: The justification is flawed. It advocates for a naive \"elbow\" method and explicitly suggests ignoring interpretable components if their variance is small. This contradicts the principles of exploratory data analysis, where finding subtle but meaningful signals (like in PC4) is a primary goal.\n- **Verdict**: **Incorrect**.\n\n**B. $C(k) = \\dfrac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{j=1}^{p} \\lambda_{j}}$, where $\\lambda_{i}$ are the eigenvalues of $\\mathbf{S}$; choose to examine up to $k=4$ because $C(4) = 0.38+0.22+0.09+0.04 = 0.73$ exceeds a reasonable information-capture target for exploratory interpretation, Principal Component $2$ captures the treatment gradient, Principal Component $1$ and Principal Component $3$ are technical/confounding effects to be controlled rather than biologically interpreted, and Principal Component $4$—despite low variance—aligns with a rare but biologically validated T-cell subset.**\n\n- **Formula**: This formula, $C(k) = \\frac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{j=1}^{p} \\lambda_{j}}$, is correct as derived above.\n- **Justification**: The justification is excellent. It correctly calculates the cumulative variance $C(4) = 0.73$. Most importantly, it demonstrates a sophisticated understanding of PCA interpretation by balancing variance-explained with the scientific meaning of each component. It correctly identifies the roles of PC1/PC3 as confounders, PC2 as the primary signal, and PC4 as a crucial, albeit low-variance, biological finding. This reasoning is scientifically sound and methodologically rigorous.\n- **Verdict**: **Correct**.\n\n**C. $C(k) = \\dfrac{\\sum_{i=1}^{k} \\lambda_{i}}{\\sum_{j=1}^{p} \\lambda_{j}}$, where $\\lambda_{i}$ are the eigenvalues of $\\mathbf{S}$; choose $k$ as the smallest number so that $C(k) \\ge 0.80$, which necessitates including many more than $6$ components if needed, because variance explained should be the sole criterion even if later components have noisy loadings without clear biological associations.**\n\n- **Formula**: The formula is correct.\n- **Justification**: The justification is scientifically poor. It relies on an arbitrary variance cutoff ($0.80$) and explicitly states that component interpretation should be ignored. This is a mechanistic and unthinking application of PCA that risks missing important findings (like PC4) and including uninterpretable noise. The cumulative variance for the first $6$ PCs is $0.38+0.22+0.09+0.04+0.02+0.01 = 0.76$, so this rule would force the inclusion of components for which no interpretation is available, which is not a sound strategy.\n- **Verdict**: **Incorrect**.\n\n**D. $C(k) = \\sum_{g=1}^{p} \\ell_{gk}^{2}$, where $\\ell_{gk}$ are the loadings for gene $g$ on component $k$; choose $k=3$ because the sum of squared loadings on the first three components is large and Principal Component $4$ accounts for little variance and should be discarded regardless of its gene-level interpretation.**\n\n- **Formula**: The formula is incorrect. The loading vector for component $k$ is the $k$-th eigenvector of $\\mathbf{S}$. These eigenvectors are defined to be orthonormal, meaning they have a Euclidean norm of $1$. Thus, $\\sum_{g=1}^{p} \\ell_{gk}^{2} = 1$ for any component $k$. This expression is not the cumulative proportion of variance.\n- **Justification**: The reasoning is based on the incorrect premise that \"the sum of squared loadings on the first three components is large\". Furthermore, the recommendation to discard PC4 \"regardless of its gene-level interpretation\" is a repetition of the same fundamental mistake seen in options A and C, which prioritizes a simple statistical heuristic over scientific insight.\n- **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "When PCA is used as a preprocessing step for predictive modeling, methodological rigor is paramount to avoid drawing invalid conclusions. This exercise tackles the critical and often misunderstood issue of \"data leakage\" within a cross-validation framework. You will explore why fitting PCA must be treated as part of the model training process, performed independently on each training fold, to ensure that performance estimates are unbiased and biological interpretations are not built on circular logic .",
            "id": "3321102",
            "problem": "A research team is analyzing a single-cell ribonucleic acid sequencing (RNA-seq) dataset to discover pathway-level variation associated with an immune stimulation. The dataset consists of a gene expression matrix with $n = 2000$ cells and $p = 10000$ genes, and a binary condition label indicating stimulated versus control. They plan to use Principal Component Analysis (PCA) for dimensionality reduction followed by a classifier evaluated by $K$-fold cross-validation with $K = 5$. The preprocessing pipeline includes per-gene centering and scaling.\n\nTwo pipelines are considered:\n\n- Pipeline $\\mathcal{T}$: Within each fold, PCA is refit using only the training fold data to estimate loadings and centering/scaling parameters. Held-out cells are transformed by applying the training fold centering/scaling and projecting onto the training fold loadings to obtain principal component scores. The classifier is trained on training-fold scores and evaluated on held-out scores.\n\n- Pipeline $\\mathcal{L}$: PCA is fit once using all $n$ cells to estimate loadings and centering/scaling parameters. Within each fold, both training and held-out cells are transformed using these global parameters and loadings. The classifier is trained and evaluated as in Pipeline $\\mathcal{T}$.\n\nEmpirically, Pipeline $\\mathcal{L}$ yields a slightly higher cross-validated Area Under the Receiver Operating Characteristic (AUROC) than Pipeline $\\mathcal{T}$ when using $m = 20$ principal components, and the research team also observes that the first few principal components align strongly with the condition label.\n\nUsing core definitions of PCA as variance-maximizing orthonormal directions of the training data covariance, and the principle that cross-validation estimates require independence between training-derived transformations and held-out evaluations, answer the following multiple-choice question:\n\nWhich statements are correct regarding why PCA must be refit only on training folds and held-out scores computed by projection, and how leakage introduces interpretational bias?\n\nA. In $K$-fold cross-validation, fitting PCA on all $n$ samples causes data leakage because the eigenvectors depend on held-out data, which makes downstream performance estimates (e.g., AUROC) optimistically biased.\n\nB. The valid way to obtain held-out principal component scores is to center and scale held-out samples using training-fold statistics and then project onto training-fold loading vectors; this preserves independence required for unbiased risk estimation.\n\nC. Because PCA is unsupervised and does not use labels, fitting PCA on the full dataset within cross-validation is acceptable and cannot introduce leakage.\n\nD. Leakage in PCA can bias biological interpretation: genes with large absolute loadings may be overemphasized as biologically important for condition separation, since the loadings were influenced by held-out data structure, conflating exploratory and confirmatory analyses.\n\nE. Refitting PCA on each fold’s training data makes component directions incomparable across folds, rendering pathway-level interpretation formally invalid even if fold-consistent patterns are present.\n\nSelect all that apply.",
            "solution": "The problem statement presents a scenario comparing two data analysis pipelines, $\\mathcal{T}$ and $\\mathcal{L}$, for a single-cell RNA-seq dataset within a $K$-fold cross-validation framework. The core of the problem lies in the correct application of Principal Component Analysis (PCA) as a preprocessing step for a subsequent classification task, and the implications of an incorrect application (data leakage) on both performance evaluation and biological interpretation.\n\nFirst, let us formalize the process. The dataset is a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ with $n=2000$ cells and $p=10000$ genes. For each cell, there is a binary label. The goal of $K$-fold cross-validation is to partition the data into $K$ folds and, for each fold $k \\in \\{1, ..., K\\}$, treat the $k$-th fold as a held-out test set and the remaining $K-1$ folds as the training set. This process is repeated $K$ times, and the performance metric (e.g., AUROC) is averaged across the folds. The fundamental principle is that the test set for any fold must be completely independent of the entire model-fitting procedure, which includes any data-dependent preprocessing steps.\n\nPipeline $\\mathcal{L}$ violates this principle. It fits PCA on the entire dataset $\\mathbf{X}$ *before* the cross-validation loop. This means the centering parameters (mean $\\boldsymbol{\\mu}$), scaling parameters (standard deviation $\\boldsymbol{\\sigma}$), and the PCA loading vectors (the eigenvectors of the covariance matrix of the full dataset, which we can denote as $\\mathbf{W}$) are all determined using information from all $n$ cells. In each cross-validation fold, the held-out data was part of the dataset used to compute $\\boldsymbol{\\mu}$, $\\boldsymbol{\\sigma}$, and $\\mathbf{W}$. This constitutes data leakage.\n\nPipeline $\\mathcal{T}$ represents the correct procedure. Within each fold $k$, the centering parameters $\\boldsymbol{\\mu}^{(k)}$, scaling parameters $\\boldsymbol{\\sigma}^{(k)}$, and loading vectors $\\mathbf{W}^{(k)}$ are estimated using *only* the training data for that fold. The held-out data is then transformed using these pre-determined, training-fold-specific parameters. This mimics a real-world application where a model pipeline is trained on available data and then applied to new, unseen data, thereby providing an unbiased estimate of the model's generalization performance.\n\nThe empirical observation that Pipeline $\\mathcal{L}$ yields a higher AUROC than Pipeline $\\mathcal{T}$ is a classic symptom of data leakage. The model in Pipeline $\\mathcal{L}$ has an unfair advantage because the feature space (the principal components) it uses was constructed with partial knowledge of the test set's data structure.\n\nWith these principles established, we evaluate each statement.\n\n**A. In $K$-fold cross-validation, fitting PCA on all $n$ samples causes data leakage because the eigenvectors depend on held-out data, which makes downstream performance estimates (e.g., AUROC) optimistically biased.**\n\nThis statement is correct. PCA finds the eigenvectors (loadings) of the data's covariance matrix. In Pipeline $\\mathcal{L}$, the covariance matrix is computed from all $n$ samples. Therefore, for any given fold, the data in the held-out set has influenced the calculation of these eigenvectors. These eigenvectors define the coordinate system into which all data, including the held-out data, is projected. Because the transformation applied to the held-out data is dependent on that very same data, the independence required for unbiased performance estimation is broken. This leakage of information from the test set into the model training process typically leads to an overly optimistic performance estimate. The observed higher AUROC for Pipeline $\\mathcal{L}$ is a direct consequence of this optimistic bias.\n**Verdict: Correct.**\n\n**B. The valid way to obtain held-out principal component scores is to center and scale held-out samples using training-fold statistics and then project onto training-fold loading vectors; this preserves independence required for unbiased risk estimation.**\n\nThis statement accurately describes the correct procedure, embodied by Pipeline $\\mathcal{T}$. To obtain an unbiased estimate of the generalization error (or risk), the entire model-fitting process must be blind to the test set. This \"process\" includes all data-dependent transformations. Therefore, the statistics for centering and scaling (mean and standard deviation) and the PCA loading vectors must be computed solely from the training data of a given fold. The held-out data is then transformed using these fixed, training-derived parameters. This procedure preserves the crucial independence between the model development ecosystem and the data used for evaluation, which is the cornerstone of valid cross-validation.\n**Verdict: Correct.**\n\n**C. Because PCA is unsupervised and does not use labels, fitting PCA on the full dataset within cross-validation is acceptable and cannot introduce leakage.**\n\nThis statement is incorrect and articulates a common but critical fallacy. The fact that PCA is \"unsupervised\" (i.e., it does not use the class labels $\\mathbf{y}$) is irrelevant to the issue of data leakage in this context. Leakage occurs because information about the *features* of the test set (its distribution and covariance structure) is used to define the feature transformation (the principal components). Any data-driven transformation, supervised or not, must be included within the cross-validation loop and learned only from the training portion of the data to ensure an unbiased performance estimate. The principal components are parameters of the feature engineering step, and like all model parameters, they must be learned without access to the test set.\n**Verdict: Incorrect.**\n\n**D. Leakage in PCA can bias biological interpretation: genes with large absolute loadings may be overemphasized as biologically important for condition separation, since the loadings were influenced by held-out data structure, conflating exploratory and confirmatory analyses.**\n\nThis statement is correct. The consequences of data leakage extend beyond inflated performance metrics to the scientific conclusions drawn from the model. In bioinformatics, the PCA loadings are often interpreted to identify which original variables (genes) drive the variance captured by a principal component. The problem states that the first few PCs align with the condition label. In the leaky Pipeline $\\mathcal{L}$, these PCs are defined to maximize variance across the *entire* dataset. They are therefore \"unnaturally\" aligned to separate the conditions, having been constructed with foreknowledge of the test data's structure. When a researcher interprets the gene loadings for such a \"leaky\" PC, they may erroneously attribute strong biological importance to a gene that appears to separate the conditions. This conclusion is biased because the component itself was tainted by information from the test set. It mixes the exploratory phase (finding patterns in training data) with the confirmatory phase (validating patterns on test data), potentially leading to non-reproducible scientific claims.\n**Verdict: Correct.**\n\n**E. Refitting PCA on each fold’s training data makes component directions incomparable across folds, rendering pathway-level interpretation formally invalid even if fold-consistent patterns are present.**\n\nThis statement is incorrect. While it is true that refitting PCA in each of the $K$ folds will produce $K$ distinct sets of loading vectors (i.e., the PC1 from fold 1 is not strictly identical to the PC1 from fold 2), this does not invalidate interpretation. The primary purpose of cross-validation (Pipeline $\\mathcal{T}$) is to obtain a robust estimate of the *performance* of the entire modeling strategy. The interpretation of the model's biological meaning is typically performed on a final model trained on the *entire dataset*, after the modeling strategy has been validated. Furthermore, observing a consistent pattern across the different folds (e.g., if PC1 in each fold consistently separates the conditions and is driven by a similar set of genes) would actually *strengthen* the confidence in the biological interpretation, as it suggests the finding is robust to the specific sampling of the training data. The claim that this procedure renders interpretation \"formally invalid\" is an overstatement and misrepresents best practices.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        }
    ]
}