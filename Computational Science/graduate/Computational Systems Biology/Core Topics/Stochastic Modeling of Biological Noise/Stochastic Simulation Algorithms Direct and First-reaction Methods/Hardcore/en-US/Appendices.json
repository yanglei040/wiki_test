{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp how stochastic simulation algorithms operate, it's essential to work through the core mechanics of a single simulation step. This exercise guides you through a concrete calculation using the first-reaction method, one of the foundational variants of Gillespie's algorithm. By deriving and applying the formula to generate candidate event times from given propensities and random numbers, you will develop a hands-on understanding of how the interplay between reaction rates and chance determines the system's next move. ",
            "id": "3351938",
            "problem": "Consider a well-mixed reaction system at a fixed discrete state $x$ with $M=3$ reaction channels. At this state, the (time-homogeneous) propensities are $a_1(x)=2$, $a_2(x)=5$, and $a_3(x)=3$, in units of $\\mathrm{s}^{-1}$. In the first-reaction variant of Gillespie’s Stochastic Simulation Algorithm (SSA), one draws independent uniform random variables $r_i \\sim \\mathrm{Uniform}(0,1)$, one per channel, and maps them into candidate firing times for each channel. Assume the independent draws are $r_1=0.2$, $r_2=0.7$, and $r_3=0.9$.\n\nStarting from the definition of a propensity $a_i(x)$ as the instantaneous hazard rate for reaction $i$ under the Chemical Master Equation (CME), and using only fundamental properties of exponential waiting times and inverse transform sampling, derive the mapping from each $r_i$ to its corresponding candidate waiting time. Then determine explicitly the first-reaction method outcome $(\\tau,J)$, where $\\tau$ is the time to the next reaction and $J$ is the index of the firing reaction.\n\nProvide $\\tau$ as an exact expression in terms of natural logarithms and $J$ as an integer. Express the time in seconds. If you choose to report a numerical approximation for $\\tau$, round it to six significant figures.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All necessary information is provided, and the requested task is a standard application of the theory behind stochastic simulation algorithms in computational systems biology.\n\nThe solution proceeds in two parts. First, we derive the general formula for mapping a uniform random number to a candidate reaction time. Second, we apply this formula to the specific data provided to determine the outcome of the first-reaction method.\n\n**Part 1: Derivation of the Candidate Waiting Time Formula**\n\nThe fundamental premise of the Stochastic Simulation Algorithm (SSA) is that, for a well-mixed system in a given state $x$, each reaction channel $i$ behaves as an independent Poisson process. The parameter of this process is the propensity, $a_i(x)$. The propensity $a_i(x)$ is defined such that $a_i(x)dt$ is the probability that reaction $i$ will occur in the infinitesimal time interval $[t, t+dt)$, given the system is in state $x$ at time $t$. This makes $a_i(x)$ the instantaneous hazard rate for reaction $i$.\n\nLet $\\tau_i$ be the random variable representing the waiting time until the next occurrence of reaction $i$, assuming no other reactions take place. Let $P_i(t')$ be the probability that reaction $i$ has *not* occurred by time $t'$. The rate of decrease of this survival probability is proportional to the probability itself, with the constant of proportionality being the hazard rate $a_i(x)$. This gives the differential equation:\n$$\n\\frac{dP_i(t')}{dt'} = -a_i(x) P_i(t')\n$$\nThis is a first-order linear ordinary differential equation. With the initial condition $P_i(0)=1$ (i.e., at time $t'=0$, the reaction has certainly not yet occurred), the solution is:\n$$\nP_i(t') = \\exp(-a_i(x) t')\n$$\nThis is the survival function for the random variable $\\tau_i$. The cumulative distribution function (CDF), $F_i(t')$, which gives the probability that reaction $i$ *has* occurred by time $t'$, is given by:\n$$\nF_i(t') = 1 - P_i(t') = 1 - \\exp(-a_i(x) t')\n$$\nThis is the CDF of an exponential distribution with rate parameter $a_i(x)$.\n\nTo generate a sample variate $\\tau_i$ from this distribution, we use the method of inverse transform sampling. We draw a random number $r_i$ from a uniform distribution on the interval $(0,1)$, i.e., $r_i \\sim \\mathrm{Uniform}(0,1)$, and set it equal to the CDF evaluated at the desired time $\\tau_i$:\n$$\nr_i = F_i(\\tau_i) = 1 - \\exp(-a_i(x) \\tau_i)\n$$\nWe now solve this equation for $\\tau_i$:\n$$\n\\exp(-a_i(x) \\tau_i) = 1 - r_i\n$$\nTaking the natural logarithm of both sides:\n$$\n-a_i(x) \\tau_i = \\ln(1 - r_i)\n$$\n$$\n\\tau_i = -\\frac{1}{a_i(x)} \\ln(1 - r_i)\n$$\nSince $r_i$ is a uniform random variable on $(0,1)$, the quantity $(1 - r_i)$ is also a uniform random variable on $(0,1)$. Therefore, for simplicity and computational efficiency, we can replace $(1 - r_i)$ with another uniform random variate, which we can also call $r_i$ without loss of generality. This leads to the standard formula:\n$$\n\\tau_i = -\\frac{1}{a_i(x)} \\ln(r_i) = \\frac{1}{a_i(x)} \\ln\\left(\\frac{1}{r_i}\\right)\n$$\nThis is the required mapping from a uniform random number $r_i$ to the candidate waiting time $\\tau_i$ for reaction channel $i$.\n\n**Part 2: Determining the First-Reaction Outcome $(\\tau, J)$**\n\nThe first-reaction method involves calculating a candidate time $\\tau_i$ for each of the $M$ reaction channels and then selecting the reaction with the smallest candidate time as the one that occurs next. The time to the next reaction, $\\tau$, is the minimum of these candidate times, and the index of the reaction, $J$, is the index corresponding to this minimum time.\n$$\n\\tau = \\min_{i=1,..,M} \\{\\tau_i\\}\n$$\n$$\nJ = \\arg\\min_{i=1,..,M} \\{\\tau_i\\}\n$$\nWe are given the following values:\nNumber of reaction channels: $M=3$.\nPropensities: $a_1(x) = 2 \\, \\mathrm{s}^{-1}$, $a_2(x) = 5 \\, \\mathrm{s}^{-1}$, and $a_3(x) = 3 \\, \\mathrm{s}^{-1}$.\nRandom numbers: $r_1=0.2$, $r_2=0.7$, and $r_3=0.9$.\n\nUsing the derived formula $\\tau_i = \\frac{1}{a_i(x)} \\ln\\left(\\frac{1}{r_i}\\right)$, we calculate the candidate time for each channel:\nFor channel $1$:\n$$\n\\tau_1 = \\frac{1}{a_1(x)} \\ln\\left(\\frac{1}{r_1}\\right) = \\frac{1}{2} \\ln\\left(\\frac{1}{0.2}\\right) = \\frac{1}{2} \\ln(5)\n$$\nFor channel $2$:\n$$\n\\tau_2 = \\frac{1}{a_2(x)} \\ln\\left(\\frac{1}{r_2}\\right) = \\frac{1}{5} \\ln\\left(\\frac{1}{0.7}\\right) = \\frac{1}{5} \\ln\\left(\\frac{10}{7}\\right)\n$$\nFor channel $3$:\n$$\n\\tau_3 = \\frac{1}{a_3(x)} \\ln\\left(\\frac{1}{r_3}\\right) = \\frac{1}{3} \\ln\\left(\\frac{1}{0.9}\\right) = \\frac{1}{3} \\ln\\left(\\frac{10}{9}\\right)\n$$\nNow we must find the minimum of $\\{\\tau_1, \\tau_2, \\tau_3\\}$. Since $\\ln(z)$ is a monotonically increasing function, comparing $\\tau_i = \\frac{1}{k_i} \\ln(z_i)$ is equivalent to comparing $\\ln(z_i^{1/k_i})$. Therefore, we need to compare the arguments:\n$z_1^{1/a_1} = 5^{1/2} = \\sqrt{5}$\n$z_2^{1/a_2} = \\left(\\frac{10}{7}\\right)^{1/5}$\n$z_3^{1/a_3} = \\left(\\frac{10}{9}\\right)^{1/3}$\n\nLet's evaluate the arguments:\n$\\sqrt{5} \\approx 2.236$\n$\\left(\\frac{10}{7}\\right)^{1/5} \\approx (1.42857)^{0.2} \\approx 1.0738$\n$\\left(\\frac{10}{9}\\right)^{1/3} \\approx (1.11111)^{0.333...} \\approx 1.0358$\n\nComparing these values, we find that $\\left(\\frac{10}{9}\\right)^{1/3}$ is the smallest.\n$$\n\\left(\\frac{10}{9}\\right)^{1/3}  \\left(\\frac{10}{7}\\right)^{1/5}  5^{1/2}\n$$\nTherefore, by the monotonic property of the natural logarithm:\n$$\n\\ln\\left(\\left(\\frac{10}{9}\\right)^{1/3}\\right)  \\ln\\left(\\left(\\frac{10}{7}\\right)^{1/5}\\right)  \\ln\\left(5^{1/2}\\right)\n$$\nWhich simplifies to:\n$$\n\\frac{1}{3} \\ln\\left(\\frac{10}{9}\\right)  \\frac{1}{5} \\ln\\left(\\frac{10}{7}\\right)  \\frac{1}{2} \\ln(5)\n$$\nThis shows that $\\tau_3  \\tau_2  \\tau_1$.\n\nThe time to the next reaction, $\\tau$, is the minimum of these times, and the index of the firing reaction, $J$, is the index of that minimum time.\n$$\n\\tau = \\min\\{\\tau_1, \\tau_2, \\tau_3\\} = \\tau_3 = \\frac{1}{3} \\ln\\left(\\frac{10}{9}\\right) \\text{ seconds}\n$$\n$$\nJ = \\arg\\min\\{\\tau_1, \\tau_2, \\tau_3\\} = 3\n$$\nThe problem requests the exact expression for $\\tau$. The numerical value of $\\tau$, rounded to six significant figures, is approximately $0.0351202 \\, \\mathrm{s}$. The required output is the exact analytical expression and the integer index.\n\nThe outcome of the first-reaction method is $(\\tau, J) = \\left(\\frac{1}{3} \\ln\\left(\\frac{10}{9}\\right), 3\\right)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}\\ln\\left(\\frac{10}{9}\\right)  3 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond simulating a single event, a key consideration in practice is the computational efficiency of the chosen algorithm, especially for systems with many reaction channels. This practice challenges you to move from concrete calculation to analytical comparison, contrasting Gillespie's Direct Method with the more advanced Next Reaction Method.  By deriving the expected resource consumption and analyzing how computational costs scale with system size ($M$), you will uncover the fundamental trade-offs that inform which algorithm is best suited for a given modeling problem.",
            "id": "3351967",
            "problem": "Consider a well-mixed chemical reaction network modeled as a continuous-time jump process with $M$ reaction channels $\\{R_{i}\\}_{i=1}^{M}$ and state-dependent propensity functions $\\{a_{i}(\\boldsymbol{x})\\}_{i=1}^{M}$. Let $a_{0}(\\boldsymbol{x}) = \\sum_{i=1}^{M} a_{i}(\\boldsymbol{x})$ denote the total propensity. Assume throughout that the system evolves on a state space for which all propensities remain strictly positive, i.e., $a_{i}(\\boldsymbol{x}(t))  0$ for all $i$ and all times $t$ along the trajectory, so that no reaction channel is ever disabled. The evolution is simulated using a Stochastic Simulation Algorithm (SSA), which, at each event, advances time by a random increment and selects one channel to fire.\n\nStarting from the following fundamental bases:\n- The random time-change representation: each reaction channel $R_{i}$ can be represented as a unit-rate Poisson process time-changed by the integrated hazard $\\int_{0}^{t} a_{i}(\\boldsymbol{x}(s)) \\,\\mathrm{d}s$.\n- The inverse transform sampling principle: an exponential waiting time with rate $a$ is obtained from a single uniform $U(0,1)$ variate $r$ via $\\tau = -\\frac{1}{a} \\ln(r)$, and sampling a categorical choice among $M$ outcomes with probabilities $\\{p_{i}\\}_{i=1}^{M}$ can be performed using a single $U(0,1)$ variate and cumulative thresholds.\n\nFor two SSA variants:\n1. The Direct Method, which samples the next reaction time and then selects the reaction index according to the instantaneous probabilities $a_{i}(\\boldsymbol{x})/a_{0}(\\boldsymbol{x})$ at the current state.\n2. The Next Reaction Method (Gibson–Bruck), which maintains, for each channel $i$, a scheduled firing time derived from an exponentially distributed internal time and updates scheduled times deterministically upon propensity changes, drawing a fresh random variate only when a channel fires.\n\nDerive, from first principles, the expected number of independent $U(0,1)$ random variates consumed per executed reaction event by the Direct Method and by the Next Reaction Method under the stated assumptions. Then, based on your derivation, discuss the asymptotic impact on computational performance as $M$ becomes large, focusing specifically on the role of random number generation in per-event cost, and contrasting it with other algorithmic costs intrinsic to each method. Report your final answer as a row matrix containing the expected counts for the Direct Method and the Next Reaction Method, in that order. No rounding is required, and no units are to be included in the final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the established theory of stochastic chemical kinetics, well-posed with a clear objective, and formulated with objective, unambiguous language. It presents a standard, non-trivial comparison of two cornerstone algorithms in computational systems biology.\n\nThis problem requires the derivation of the expected number of random variates consumed per reaction event for two different Stochastic Simulation Algorithms (SSAs): the Direct Method (DM) and the Next Reaction Method (NRM). The analysis is based on the principles of inverse transform sampling for generating random variables from specified distributions.\n\nLet the state of the system at time $t$ be described by the vector of species populations $\\boldsymbol{x}(t)$. The system has $M$ reaction channels, with the propensity of reaction $R_i$ given by $a_i(\\boldsymbol{x})$. The total propensity is $a_0(\\boldsymbol{x}) = \\sum_{i=1}^{M} a_i(\\boldsymbol{x})$. We are given that $a_i(\\boldsymbol{x}(t))  0$ for all $i$ and $t$.\n\n**1. Direct Method (DM)**\n\nThe Direct Method, developed by Gillespie, simulates the trajectory of the system by repeatedly answering two questions at each step: (1) when will the next reaction occur, and (2) which reaction will it be?\n\n*   **Step 1: Sampling the time to the next event.**\n    The time, $\\tau$, until the next reaction event (of any type) is a random variable drawn from an exponential distribution with rate parameter equal to the total propensity, $a_0(\\boldsymbol{x})$. Using the inverse transform sampling principle provided, we can generate a value for $\\tau$ from a uniformly distributed random variate $r_1 \\sim U(0,1)$.\n    $$ \\tau = -\\frac{1}{a_0(\\boldsymbol{x})} \\ln(r_1) $$\n    This step consumes exactly **one** independent random variate, $r_1$.\n\n*   **Step 2: Sampling the index of the next reaction.**\n    Given that a reaction occurs at time $t+\\tau$, the probability that it is reaction $R_i$ is given by the ratio of its propensity to the total propensity, $p_i = a_i(\\boldsymbol{x})/a_0(\\boldsymbol{x})$. Selecting the reaction index $\\mu \\in \\{1, \\dots, M\\}$ is therefore a draw from a categorical distribution with probabilities $\\{p_i\\}_{i=1}^{M}$. This can be accomplished using a second independent uniform random variate $r_2 \\sim U(0,1)$. We search for the index $\\mu$ that satisfies the condition:\n    $$ \\sum_{i=1}^{\\mu-1} a_i(\\boldsymbol{x})  r_2 \\cdot a_0(\\boldsymbol{x}) \\le \\sum_{i=1}^{\\mu} a_i(\\boldsymbol{x}) $$\n    This step consumes exactly **one** independent random variate, $r_2$.\n\nFollowing these two sampling steps, the algorithm updates the time to $t+\\tau$ and the state vector $\\boldsymbol{x}$ according to the stoichiometry of reaction $R_\\mu$. These updates are deterministic.\n\nSince each reaction event in the Direct Method requires executing both Step 1 and Step 2, the total number of independent $U(0,1)$ random variates consumed per event is constant.\nExpected number for DM = $1$ (for time) + $1$ (for reaction index) = $2$.\n\n**2. Next Reaction Method (NRM)**\n\nThe Next Reaction Method (also known as the Gibson-Bruck algorithm) reformulates the simulation to avoid redundant calculations. It maintains a scheduled absolute firing time, $T_i$, for each of the $M$ reaction channels. These times are stored in a data structure, typically a priority queue, that allows for efficient retrieval of the minimum time.\n\nLet the system be at time $t$ with state $\\boldsymbol{x}$, and the set of scheduled firing times be $\\{T_i\\}_{i=1}^{M}$. A single event proceeds as follows:\n\n*   **Step 1: Identify the next event.**\n    The next reaction to occur, $R_\\mu$, is the one with the smallest scheduled firing time. The algorithm finds the index $\\mu$ such that $T_\\mu = \\min_{j \\in \\{1, \\dots, M\\}} \\{T_j\\}$. The time of the event is $t_{event} = T_\\mu$. This step is a deterministic search and consumes **zero** random variates.\n\n*   **Step 2: Update state and time.**\n    The simulation time is advanced to $t_{event}$, and the state vector $\\boldsymbol{x}$ is updated to $\\boldsymbol{x}'$ according to the stoichiometry of reaction $R_\\mu$. This is a deterministic update.\n\n*   **Step 3: Update scheduled firing times.**\n    This step is the core of the NRM's efficiency.\n    *   **For the channel that just fired ($R_\\mu$):** A new firing time must be scheduled. This requires generating a new random waiting time, $\\Delta T_\\mu$, from an exponential distribution with the new rate $a_\\mu(\\boldsymbol{x}')$. Using inverse transform sampling with a new random variate $r' \\sim U(0,1)$:\n        $$ \\Delta T_\\mu = -\\frac{1}{a_\\mu(\\boldsymbol{x}')} \\ln(r') $$\n        The new absolute scheduled time for channel $\\mu$ is $T_\\mu^{\\text{new}} = t_{event} + \\Delta T_\\mu$. This sub-step consumes exactly **one** random variate.\n    *   **For all other channels ($R_i$, where $i \\neq \\mu$):** The propensities $a_i$ might change due to the change in state from $\\boldsymbol{x}$ to $\\boldsymbol{x}'$. The NRM leverages the random time-change representation to update their scheduled times deterministically, without new random numbers. The key insight is that the underlying \"randomness\" (the unrealized portion of the previously drawn random number) is preserved. The remaining time to fire, $(T_i - t_{event})$, which was implicitly scaled by the old propensity $a_i(\\boldsymbol{x})$, is rescaled by the new propensity $a_i(\\boldsymbol{x}')$. The new scheduled time $T_i^{\\text{new}}$ is calculated as:\n        $$ T_i^{\\text{new}} = t_{event} + \\frac{a_i(\\boldsymbol{x})}{a_i(\\boldsymbol{x}')} (T_i - t_{event}) $$\n        This update is entirely deterministic and consumes **zero** random variates. The assumption $a_i  0$ ensures the denominator is non-zero.\n\nTherefore, for each executed reaction event, the NRM draws exactly one new random number, which is used to schedule the next occurrence of the channel that just fired.\nExpected number for NRM = $1$.\n\n**3. Asymptotic Impact on Computational Performance**\n\nThe analysis above shows that the Direct Method consumes $2$ random variates per event, while the Next Reaction Method consumes $1$. Based solely on the cost of random number generation (RNG), the NRM is twice as efficient. However, for large $M$, the role of RNG cost becomes negligible compared to other algorithmic costs.\n\n*   **Role of Random Number Generation:** The cost of generating random numbers is $O(1)$ per call. For both DM and NRM, the total per-event cost dedicated to RNG is therefore $O(1)$ with respect to $M$. While NRM has a better constant factor ($1$ vs $2$), this difference does not affect the asymptotic scaling of the algorithms. In modern computer architectures, high-quality pseudorandom number generators are extremely fast, making this constant-factor difference a minor component of the total execution time.\n\n*   **Other Intrinsic Algorithmic Costs:** The dominant factor in performance for large $M$ is the scaling of the non-RNG components of the algorithms.\n    *   **Direct Method:** At each step, DM must compute all $M$ propensities to find their sum $a_0$, an $O(M)$ operation. Subsequently, it performs a linear search to select the reaction channel, which is also an $O(M)$ operation. Thus, the total complexity per event for DM is $O(M)$.\n    *   **Next Reaction Method:** The NRM's performance depends heavily on the data structure used for the priority queue and the dependency structure of the reaction network.\n        *   Finding the minimum time is $O(1)$ with a binary heap.\n        *   Updating the times is the main cost. For the fired reaction, updating the heap is $O(\\log M)$. For the other reactions whose propensities change (let there be $N_{dep}$ such reactions), each update also costs $O(\\log M)$. The total cost is thus $O((1+N_{dep})\\log M)$.\n        *   In **sparse networks**, where each reaction affects only a small, constant number of other propensities, $N_{dep}$ is small and the per-event cost is $O(\\log M)$. This logarithmic scaling is the NRM's signature advantage, making it vastly superior to DM for large, sparse systems.\n        *   In **dense networks**, where a single reaction can affect nearly all other propensities, $N_{dep}$ approaches $M$, and the per-event cost becomes $O(M \\log M)$. In this scenario, the NRM is asymptotically slower than the $O(M)$ Direct Method.\n\n*   **Conclusion on Performance:** The asymptotic performance is dictated not by the $O(1)$ cost of random number generation but by the algorithmic complexity of updating the system state after an event. The NRM's true advantage lies in its $O(\\log M)$ scaling for sparse networks, which stems from its ability to avoid re-evaluating every channel at every step. The saving of one random number per event is a secondary benefit that is asymptotically insignificant.\n\nThe derived expected numbers of random variates consumed per event are $2$ for the Direct Method and $1$ for the Next Reaction Method.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A central task in computational systems biology is to connect theoretical models with experimental data. This practice provides a hands-on coding experience in solving the inverse problem: estimating unknown model parameters from an observed reaction trajectory.  You will implement the powerful principle of maximum likelihood estimation (MLE) and then use your estimated parameters to construct the generator matrix ($Q$) of the Chemical Master Equation, forging a direct link between a single stochastic realization and the ensemble-level mathematical description of the system.",
            "id": "3351918",
            "problem": "Consider a well-mixed reaction network modeled as a Continuous-Time Markov Chain (CTMC), where the Stochastic Simulation Algorithm (SSA), both in the Direct method and the First-Reaction method variants, is based on state-dependent reaction propensities. Let the set of reactions be indexed by $j \\in \\{1,\\dots,R\\}$. At a state $x$, the propensity of reaction $j$ is $a_j(x)$. The total propensity is $a_0(x) = \\sum_{j=1}^{R} a_j(x)$. In the Direct method, the waiting time to the next event from state $x$ is exponentially distributed with rate $a_0(x)$, and the next reaction is selected with probability $a_j(x)/a_0(x)$. In the First-Reaction method, independent exponential clocks with rates $a_j(x)$ are started for each reaction; the minimum clock determines the next event. Both methods are equivalent and generate sample paths from the same CTMC.\n\nYou will estimate reaction parameters by Maximum Likelihood from observed SSA event sequences and construct the generator of the Chemical Master Equation (CME). Assume a parameterization $a_j(x) = \\theta_j h_j(x)$, where $h_j(x)$ is a known nonnegative function and $\\theta_j$ is an unknown nonnegative parameter to be estimated. An observed trajectory consists of an initial state $x(0)$, a sequence of event times $0  t_1  \\cdots  t_N \\le T$ in seconds, associated reaction indices $j_1,\\dots,j_N$, and the state updates $x(t_n^+) = x(t_n^-) + \\nu_{j_n}$, where $\\nu_j$ is the stoichiometric change vector for reaction $j$. Between events, the state is constant. The likelihood is constructed using the fundamental Markov jump process properties: the waiting time is exponential with rate $a_0(x)$, and conditional on a jump occurring, reaction $j$ is selected with probability $a_j(x)/a_0(x)$. Use this to derive and implement Maximum Likelihood estimation of $\\theta_j$ for each reaction $j$ from the observed trajectory. Then, using your estimates, construct a truncated generator matrix $Q$ over a specified finite state set $\\mathcal{X}$, with off-diagonal entries $Q_{x,y} = a_j(x)$ if $y = x + \\nu_j \\in \\mathcal{X}$ for some $j$, and diagonal entries $Q_{x,x} = -\\sum_{y \\ne x} Q_{x,y}$. Relate your $\\hat{\\theta}_j$ estimates to the generator entries, and verify row-sum zero of $Q$.\n\nAll times must be treated in seconds. Your implementation must compute the log-likelihood at the Maximum Likelihood estimate for each test case. If a reaction has zero total exposure (i.e., $\\int_0^T h_j(x(t)) \\, dt = 0$) and zero events, define its Maximum Likelihood estimate as $0$ by convention. If a reaction has zero exposure but a positive number of events, the data are inconsistent; you may assume this does not occur in the provided test suite.\n\nTest Suite specification:\nProvide code that solves the following three test cases (the units for time are seconds, and the single species count is denoted by $x$):\n- Test case $1$: Two reactions with $R = 2$.\n  - Reaction $1$: $X \\rightarrow 2X$, stoichiometry $\\nu_1 = +1$, propensity $a_1(x) = \\theta_1 h_1(x)$ with $h_1(x) = x$.\n  - Reaction $2$: $X \\rightarrow \\varnothing$, stoichiometry $\\nu_2 = -1$, propensity $a_2(x) = \\theta_2 h_2(x)$ with $h_2(x) = x$.\n  - Initial state $x(0) = 2$.\n  - Event times and reaction indices: $(t_1,j_1) = (0.5,1)$, $(t_2,j_2) = (1.4,2)$, $(t_3,j_3) = (2.0,1)$, $(t_4,j_4) = (3.2,2)$, $(t_5,j_5) = (4.0,1)$.\n  - End time $T = 5.0$.\n  - Truncation set for generator: $\\mathcal{X} = \\{0,1,2,3,4,5\\}$.\n- Test case $2$: Two reactions with $R = 2$.\n  - Reaction $1$: $X \\rightarrow \\varnothing$, $\\nu_1 = -1$, $a_1(x) = \\theta_1 h_1(x)$ with $h_1(x) = x$.\n  - Reaction $2$: $\\varnothing \\rightarrow X$, $\\nu_2 = +1$, $a_2(x) = \\theta_2 h_2(x)$ with $h_2(x) = 1$.\n  - Initial state $x(0) = 1$.\n  - Event times and reaction indices: $(t_1,j_1) = (1.0,2)$, $(t_2,j_2) = (1.7,2)$, $(t_3,j_3) = (2.2,1)$, $(t_4,j_4) = (2.9,1)$.\n  - End time $T = 4.0$.\n  - Truncation set for generator: $\\mathcal{X} = \\{0,1,2,3,4,5\\}$.\n- Test case $3$: Two reactions with $R = 2$.\n  - Reaction $1$: $2X \\rightarrow \\varnothing$, $\\nu_1 = -2$, $a_1(x) = \\theta_1 h_1(x)$ with $h_1(x) = x(x-1)/2$.\n  - Reaction $2$: $\\varnothing \\rightarrow X$, $\\nu_2 = +1$, $a_2(x) = \\theta_2 h_2(x)$ with $h_2(x) = 1$.\n  - Initial state $x(0) = 0$.\n  - Event times and reaction indices: $(t_1,j_1) = (0.8,2)$, $(t_2,j_2) = (1.6,2)$, $(t_3,j_3) = (2.4,2)$.\n  - End time $T = 3.0$.\n  - Truncation set for generator: $\\mathcal{X} = \\{0,1,2,3\\}$.\n\nImplementation requirements:\n- For each test case, compute the Maximum Likelihood estimates $\\hat{\\theta}_j$ for all $j$, the log-likelihood at $\\hat{\\theta}$, and construct the truncated generator matrix $Q$ over the specified $\\mathcal{X}$. Verify that each row of $Q$ sums to zero up to numerical tolerance.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list formatted as $[\\hat{\\theta}_1,\\hat{\\theta}_2,\\dots,\\text{row\\_sums\\_zero},\\text{logL}]$. The boolean $\\text{row\\_sums\\_zero}$ must be either $\\text{True}$ or $\\text{False}$. Express all floating-point numbers rounded to $6$ decimal places. No spaces are allowed anywhere inside the output line. For example, the final output should look like $[[\\cdots],[\\cdots],[\\cdots]]$.",
            "solution": "The problem requires the derivation and implementation of Maximum Likelihood Estimation (MLE) for the parameters of a stochastic reaction network, modeled as a Continuous-Time Markov Chain (CTMC). Following parameter estimation from an observed trajectory, a truncated generator matrix for the associated Chemical Master Equation (CME) must be constructed.\n\nThe process begins with the formulation of the likelihood function for a single observed trajectory. A trajectory is defined by an initial state $x(0)$ at time $t=0$, and a sequence of $N$ reaction events occurring at times $0  t_1  t_2  \\dots  t_N \\le T$, where $T$ is the total observation time. Let the reaction occurring at time $t_n$ be $j_n$. The state of the system, $x(t)$, is a piecewise constant function. We denote the state just before the $n$-th event as $x_{n-1} = x(t_n^-)$, with the convention that $t_0=0$ and $x_0 = x(0)$.\n\nThe likelihood of observing a specific trajectory is derived from the fundamental properties of a Markov jump process. The probability density of waiting a time $\\tau$ in state $x$ and then having reaction $j$ occur is $a_j(x) e^{-a_0(x)\\tau}$, where $a_j(x)$ is the propensity of reaction $j$ and $a_0(x) = \\sum_{k=1}^{R} a_k(x)$ is the total propensity.\n\nThe likelihood of the entire observed path is the product of the probabilities of each inter-event interval and the probability of surviving from the last event time $t_N$ until $T$.\nThe likelihood function $L(\\theta)$ is given by:\n$$ L(\\theta) = \\left( \\prod_{n=1}^{N} a_{j_n}(x_{n-1}) e^{-a_0(x_{n-1})(t_n - t_{n-1})} \\right) \\cdot e^{-a_0(x_N)(T-t_N)} $$\nwhere $x_N$ is the state after the $N$-th event. This can be rewritten more compactly as:\n$$ L(\\theta) = \\left( \\prod_{n=1}^{N} a_{j_n}(x(t_n^-)) \\right) \\exp\\left( - \\int_0^T a_0(x(t)) dt \\right) $$\n\nFor maximization, it is more convenient to work with the log-likelihood, $\\mathcal{L}(\\theta) = \\ln L(\\theta)$:\n$$ \\mathcal{L}(\\theta) = \\sum_{n=1}^{N} \\ln a_{j_n}(x(t_n^-)) - \\int_0^T a_0(x(t)) dt $$\n\nThe problem specifies the propensity form $a_j(x) = \\theta_j h_j(x)$, where $\\theta_j$ are the unknown non-negative parameters and $h_j(x)$ are known non-negative functions. The total propensity is $a_0(x) = \\sum_{k=1}^{R} \\theta_k h_k(x)$. Substituting this into the log-likelihood equation yields:\n$$ \\mathcal{L}(\\theta) = \\sum_{n=1}^{N} \\ln(\\theta_{j_n} h_{j_n}(x(t_n^-))) - \\int_0^T \\sum_{k=1}^{R} \\theta_k h_k(x(t)) dt $$\nLet's group the terms by reaction index $j$. Let $N_j$ be the total number of times reaction $j$ occurs in the trajectory, i.e., $N_j = \\sum_{n=1}^N \\mathbb{I}(j_n = j)$, where $\\mathbb{I}$ is the indicator function. Let $E_j$ be the total integrated \"exposure\" for reaction $j$, defined as $E_j = \\int_0^T h_j(x(t)) dt$. The log-likelihood can then be expressed as:\n$$ \\mathcal{L}(\\theta) = \\sum_{j=1}^{R} \\left( N_j \\ln \\theta_j - \\theta_j E_j \\right) + \\sum_{n=1}^{N} \\ln h_{j_n}(x(t_n^-)) $$\nThe log-likelihood is a sum of terms, each depending on only one $\\theta_j$. We can find the MLE for each $\\theta_j$ by taking the partial derivative with respect to $\\theta_j$ and setting it to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_j} = \\frac{N_j}{\\theta_j} - E_j = 0 $$\nSolving for $\\theta_j$ gives the Maximum Likelihood Estimate $\\hat{\\theta}_j$:\n$$ \\hat{\\theta}_j = \\frac{N_j}{E_j} $$\nThis result is intuitive: the estimated rate constant is the number of observed events divided by the total exposure over the observation period. If $E_j = 0$ and $N_j = 0$, the problem specifies using the convention $\\hat{\\theta}_j=0$. This is consistent with the likelihood function being independent of $\\theta_j$ in this case. If $E_j=0$ but $N_j0$, the likelihood is unbounded, indicating inconsistent data, which is assumed not to occur.\n\nTo calculate the log-likelihood at the MLE, $\\mathcal{L}(\\hat{\\theta})$, we substitute $\\hat{\\theta}_j$ back into the log-likelihood formula. A convenient form for computation is:\n$$ \\mathcal{L}(\\hat{\\theta}) = \\sum_{n=1}^{N} \\ln(\\hat{a}_{j_n}(x(t_n^-))) - \\int_0^T \\hat{a}_0(x(t)) dt $$\nwhere $\\hat{a}_j(x) = \\hat{\\theta}_j h_j(x)$. The integral term can be simplified:\n$$ \\int_0^T \\hat{a}_0(x(t)) dt = \\int_0^T \\sum_{j=1}^{R} \\hat{\\theta}_j h_j(x(t)) dt = \\sum_{j=1}^{R} \\hat{\\theta}_j \\int_0^T h_j(x(t)) dt = \\sum_{j=1}^{R} \\hat{\\theta}_j E_j = \\sum_{j=1}^{R} \\frac{N_j}{E_j} E_j = \\sum_{j=1}^{R} N_j $$\nThe total number of events is $N = \\sum_{j=1}^{R} N_j$. So the integral term equals $N$. The final expression for the log-likelihood at the MLE is:\n$$ \\mathcal{L}(\\hat{\\theta}) = \\left( \\sum_{n=1}^{N} \\ln(\\hat{\\theta}_{j_n} h_{j_n}(x(t_n^-))) \\right) - N $$\n\nThe second part of the problem is to construct the truncated generator matrix $Q$ of the CME over a given finite state set $\\mathcal{X}$. The generator matrix describes the rates of transition between states. An off-diagonal element $Q_{x,y}$ gives the rate of transitioning from state $x$ to state $y \\ne x$. A transition from $x$ to $y$ occurs if there is a reaction $j$ such that $y = x + \\nu_j$, where $\\nu_j$ is the stoichiometry vector. The rate of this transition is the propensity $a_j(x)$. Using our estimated parameters, this is $\\hat{a}_j(x) = \\hat{\\theta}_j h_j(x)$.\nThus, for $x, y \\in \\mathcal{X}$ and $y \\ne x$:\n$$ Q_{x,y} = \\sum_{j: x+\\nu_j = y} \\hat{a}_j(x) $$\nThe diagonal elements $Q_{x,x}$ are defined such that each row of the matrix sums to zero. This reflects the fact that the diagonal entry represents the negative of the total rate of leaving state $x$:\n$$ Q_{x,x} = - \\sum_{y \\in \\mathcal{X}, y \\ne x} Q_{x,y} $$\n\nThe implementation will proceed as follows for each test case:\n$1$. Parse the given reaction network structure, initial state $x(0)$, event sequence, and observation time $T$.\n$2$. Reconstruct the piecewise constant state trajectory $x(t)$ for $t \\in [0, T]$.\n$3$. From the event sequence, count the number of occurrences $N_j$ for each reaction $j$.\n$4$. By integrating over the state trajectory $x(t)$, compute the exposure integrals $E_j = \\int_0^T h_j(x(t)) dt$.\n$5$. Calculate the MLE for each parameter $\\hat{\\theta}_j = N_j / E_j$, applying the specified convention for zero counts and exposure.\n$6$. Compute the log-likelihood $\\mathcal{L}(\\hat{\\theta})$ using the derived formula.\n$7$. Construct the generator matrix $Q$ on the truncated state space $\\mathcal{X}$. Iterate through each state $x \\in \\mathcal{X}$ to populate the corresponding row of $Q$. For each reaction $j$, calculate the target state $y=x+\\nu_j$. If $y \\in \\mathcal{X}$, add the rate $\\hat{a}_j(x) = \\hat{\\theta}_j h_j(x)$ to the off-diagonal entry $Q_{x,y}$.\n$8$. Calculate the diagonal elements $Q_{x,x}$ to ensure row sums are zero.\n$9$. Verify that all row sums of the constructed matrix $Q$ are numerically close to zero.\n$10$. Collate the estimated parameters $\\hat{\\theta}_j$, the row-sum verification result, and the log-likelihood value into the required output format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases and print the results.\n    \"\"\"\n\n    # Define test cases as per the problem description.\n    test_cases = [\n        {\n            \"R\": 2,\n            \"nu\": {1: 1, 2: -1},\n            \"h_funcs\": {1: lambda x: x, 2: lambda x: x},\n            \"x0\": 2,\n            \"events\": [(0.5, 1), (1.4, 2), (2.0, 1), (3.2, 2), (4.0, 1)],\n            \"T\": 5.0,\n            \"X_set\": list(range(6)),\n        },\n        {\n            \"R\": 2,\n            \"nu\": {1: -1, 2: 1},\n            \"h_funcs\": {1: lambda x: x, 2: lambda x: 1.0},\n            \"x0\": 1,\n            \"events\": [(1.0, 2), (1.7, 2), (2.2, 1), (2.9, 1)],\n            \"T\": 4.0,\n            \"X_set\": list(range(6)),\n        },\n        {\n            \"R\": 2,\n            \"nu\": {1: -2, 2: 1},\n            \"h_funcs\": {1: lambda x: x * (x - 1) / 2.0, 2: lambda x: 1.0},\n            \"x0\": 0,\n            \"events\": [(0.8, 2), (1.6, 2), (2.4, 2)],\n            \"T\": 3.0,\n            \"X_set\": list(range(4)),\n        },\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        res = solve_case(case)\n        \n        # Format the output for one case\n        thetas_str = ','.join([f\"{th:.6f}\" for th in res['thetas']])\n        row_sums_zero_str = str(res['row_sums_zero'])\n        logL_str = f\"{res['logL']:.6f}\"\n        \n        case_str = f\"[{thetas_str},{row_sums_zero_str},{logL_str}]\"\n        results_str_list.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str_list)}]\")\n\n\ndef solve_case(case_data):\n    \"\"\"\n    Solves a single test case.\n    \"\"\"\n    R = case_data[\"R\"]\n    nu = case_data[\"nu\"]\n    h_funcs = case_data[\"h_funcs\"]\n    x0 = case_data[\"x0\"]\n    events = case_data[\"events\"]\n    T = case_data[\"T\"]\n    X_set = case_data[\"X_set\"]\n\n    # 1. Reconstruct state trajectory and gather pre-event states\n    # Trajectory is a list of (time, state) tuples\n    trajectory = [(0.0, x0)]\n    pre_event_states = []\n    current_state = x0\n    for t_event, j_event in events:\n        pre_event_states.append(current_state)\n        current_state += nu[j_event]\n        trajectory.append((t_event, current_state))\n\n    # 2. Calculate Nj (event counts) and Ej (exposures)\n    N_counts = {j: 0 for j in range(1, R + 1)}\n    for _, j_event in events:\n        N_counts[j_event] += 1\n\n    E_integrals = {j: 0.0 for j in range(1, R + 1)}\n    last_t, last_x = trajectory[0]\n    \n    # Process intervals between events\n    for t_event, current_x in trajectory[1:]:\n        dt = t_event - last_t\n        for j in range(1, R + 1):\n            E_integrals[j] += h_funcs[j](last_x) * dt\n        last_t = t_event\n        last_x = current_x\n\n    # Process final interval from last event to T\n    dt_final = T - last_t\n    for j in range(1, R + 1):\n        E_integrals[j] += h_funcs[j](last_x) * dt_final\n\n    # 3. Compute MLEs\n    thetas = []\n    for j in range(1, R + 1):\n        Nj = N_counts[j]\n        Ej = E_integrals[j]\n        if Ej  0:\n            theta_j = Nj / Ej\n        else: # Handles Ej=0, Nj=0 case by convention\n            theta_j = 0.0\n        thetas.append(theta_j)\n    \n    # 4. Compute log-likelihood at MLE\n    logL = 0.0\n    if events:\n        for i, (t_event, j_event) in enumerate(events):\n            x_pre = pre_event_states[i]\n            h_val = h_funcs[j_event](x_pre)\n            theta_val = thetas[j_event - 1]\n            if theta_val  0 and h_val  0:\n                logL += np.log(theta_val * h_val)\n            # if theta_val or h_val is 0, propensity is 0. An event\n            # with 0 propensity is impossible in continuous time, implying\n            # likelihood of -inf. But test data is consistent.\n            # Special case for theta_val == 0 is where Nj = 0 for that j,\n            # so such an event wouldn't be in the list anyway.\n\n        total_N = sum(N_counts.values())\n        logL -= total_N\n    else: # No events\n        total_a0_integral = sum(thetas[j - 1] * E_integrals[j] for j in range(1, R + 1))\n        logL = -total_a0_integral\n\n    # 5. Construct generator matrix Q\n    n_states = len(X_set)\n    state_to_idx = {s: i for i, s in enumerate(X_set)}\n    Q = np.zeros((n_states, n_states))\n\n    for i, x in enumerate(X_set):\n        for j in range(1, R + 1):\n            propensity = thetas[j - 1] * h_funcs[j](x)\n            if propensity  0:\n                y = x + nu[j]\n                if y in state_to_idx:\n                    k = state_to_idx[y]\n                    Q[i, k] += propensity\n    \n    for i in range(n_states):\n        Q[i, i] = -np.sum(Q[i, :])\n\n    # 6. Verify row sums\n    row_sums = np.sum(Q, axis=1)\n    all_rows_sum_to_zero = np.all(np.isclose(row_sums, 0))\n\n    return {\n        \"thetas\": thetas,\n        \"row_sums_zero\": all_rows_sum_to_zero,\n        \"logL\": logL,\n    }\n\nsolve()\n```"
        }
    ]
}