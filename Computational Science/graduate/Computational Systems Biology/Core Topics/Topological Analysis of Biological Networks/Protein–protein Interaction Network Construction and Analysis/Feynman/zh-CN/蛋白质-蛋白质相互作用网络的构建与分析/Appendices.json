{
    "hands_on_practices": [
        {
            "introduction": "蛋白质相互作用（PPI）网络的构建是系统生物学分析的基石，但原始实验数据往往充满噪音且来源各异。本练习将指导您如何通过一个严谨的贝叶斯框架，整合来自不同实验方法和数据库的异构证据，从而构建一个可靠的加权PPI网络。通过这个实践，您将学会如何量化每个相互作用的置信度，这对于后续所有网络分析的准确性至关重要。",
            "id": "3341723",
            "problem": "给定关于候选蛋白质-蛋白质相互作用的多个异构证据来源。您的任务是设计并实现一个程序，通过协调证据类型和置信度来统一不同数据库中相互冲突的相互作用，然后计算具有不确定性区间的共识边。您需要在一个基于贝叶斯定理和伯努利观测共轭先验的、有数学原理的框架内进行工作，并以小数形式表示所有概率。\n\n需要使用的基本和核心定义：\n- 贝叶斯定理：对于一个假设 $\\mathcal{H}$ 和数据 $\\mathcal{D}$，后验概率满足 $P(\\mathcal{H}\\mid \\mathcal{D}) \\propto P(\\mathcal{D}\\mid \\mathcal{H}) P(\\mathcal{H})$。\n- 一个参数为 $p$ 的伯努利随机变量 $Y$ 的似然函数为 $P(Y=y\\mid p) = p^y (1-p)^{1-y}$，其中 $y\\in\\{0,1\\}$。\n- Beta 分布 $\\mathrm{Beta}(\\alpha,\\beta)$ 是伯努利似然的共轭先验。\n\n场景与协调设计：\n- 考虑蛋白质对之间的候选边，每条边都由一组数据库进行评估。每个数据库提供一条证据，包含：\n  1. 证据类型 $t$，来自集合 $\\{ \\text{酵母双杂交 (Y2H)}, \\text{亲和纯化-质谱联用 (AP-MS)}, \\text{共表达 (Coexp)}, \\text{文献整理 (Curated)} \\}$。\n  2. 一个连续的置信度分数 $r \\in [0,1]$。\n  3. 对相互作用的结论 $v \\in \\{ \\text{支持}, \\text{反驳} \\}$。\n  4. 数据库标识 $d$，来自集合 $\\{ \\text{BioGRID}, \\text{IntAct}, \\text{STRING}, \\text{Literature} \\}$。\n- 通过以下方式将异构证据协调统一为贝叶斯更新：\n  1. 通过一个校准的几率转换和有界影响，将每个置信度 $r$ 映射到一个非负的伪计数权重 $w$，该权重随 $r$ 单调增加：\n     $$ w = \\min\\left( W_{\\max},\\ \\lambda_{t}\\ \\eta_{d}\\ \\frac{r}{1-r} \\right), $$\n     其中 $\\lambda_{t}$ 是证据类型可靠性乘数，$\\eta_{d}$ 是数据库级别的校准因子。使用 $W_{\\max} = 10$，$\\lambda_{\\text{Y2H}}=1$，$\\lambda_{\\text{AP-MS}}=0.8$，$\\lambda_{\\text{Coexp}}=0.5$，$\\lambda_{\\text{Curated}}=1.2$，以及 $\\eta_{\\text{BioGRID}}=1$，$\\eta_{\\text{IntAct}}=1$，$\\eta_{\\text{STRING}}=0.9$，$\\eta_{\\text{Literature}}=1.1$。\n  2. 将支持性证据视为分数阶的伯努利“成功”，将反驳性证据视为分数阶的“失败”，并在边存在概率 $p$ 的对称、无信息先验下将它们结合起来。使用参数为 $\\alpha_0=\\beta_0=1$ 的 Beta 先验。\n\n需为每条边计算的目标量：\n- 在协调该边的所有证据后，边存在概率 $p$ 的后验分布。\n- $p$ 的后验均值。\n- $p$ 的质量为 $0.95$ 的中心可信区间（即，$0.025$ 和 $0.975$ 的后验分位数）。\n- 对该边的共识决策，定义为一个布尔值，当且仅当 $0.95$ 可信区间的下限至少为 $0.5$ 时为真。\n\n您的程序必须从第一性原理出发实现上述协调和贝叶斯更新过程，不得使用本问题陈述中提供的快捷公式。所有概率必须以小数表示，而非百分比。本问题不涉及角度。没有物理单位。\n\n测试套件：\n实现您的程序以评估以下 $5$ 条边，每条边由一系列形式为 $(d, t, v, r)$ 的证据项描述：\n\n- 边 $A$–$B$ (普遍的“理想路径”情况，主要为支持性的高置信度证据和一个弱反驳证据)：\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{支持}, r=0.95)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{支持}, r=0.85)$\n  - $(\\text{Literature}, \\text{Curated}, \\text{支持}, r=0.90)$\n  - $(\\text{STRING}, \\text{Coexp}, \\text{反驳}, r=0.20)$\n- 边 $C$–$D$ (冲突证据，支持与反驳证据大致平衡)：\n  - $(\\text{STRING}, \\text{Coexp}, \\text{支持}, r=0.60)$\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{反驳}, r=0.70)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{支持}, r=0.70)$\n  - $(\\text{Literature}, \\text{Curated}, \\text{反驳}, r=0.65)$\n- 边 $E$–$F$ (边界情况，完全是强有力的反驳证据)：\n  - $(\\text{STRING}, \\text{Coexp}, \\text{反驳}, r=0.95)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{反驳}, r=0.90)$\n  - $(\\text{Literature}, \\text{Curated}, \\text{反驳}, r=0.80)$\n- 边 $G$–$H$ (测试权重上限和接近零置信度的边缘情况)：\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{支持}, r=0.99)$\n  - $(\\text{STRING}, \\text{Coexp}, \\text{支持}, r=0.51)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{反驳}, r=0.05)$\n- 边 $I$–$J$ (具有不同可靠性乘数的中等强度混合证据)：\n  - $(\\text{Literature}, \\text{Curated}, \\text{支持}, r=0.70)$\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{反驳}, r=0.60)$\n\n输出规范：\n- 对每条边，输出一个列表，包含 $p$ 的后验均值、$p$ 的 $0.95$ 可信区间的下限和上限，以及如上定义的共识决策布尔值。将所有浮点数四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。其中每条边的结果本身是一个形式为 $[p_{\\text{mean}}, p_{\\text{lower}}, p_{\\text{upper}}, \\text{decision}]$ 的列表。例如，最终输出应类似于 $[[0.123456,0.012345,0.234567,\\text{True}],[\\dots],\\dots]$。",
            "solution": "该问题要求设计一个贝叶斯框架，以综合蛋白质-蛋白质相互作用的异构证据，并计算带有不确定性的共识决策。该问题陈述清晰，科学上基于标准统计原理，并为获得唯一解提供了所有必要的数据和定义。因此，我们可以进行形式化的推导和实现。\n\n问题的核心是确定一条边存在的后验概率分布，该概率由参数 $p \\in [0, 1]$ 表示，其中 $p=1$ 表示存在真实相互作用，$p=0$ 表示不存在相互作用。我们将 $p$ 建模为一个随机变量，并使用贝叶斯定理在我们整合证据时更新对 $p$ 的信念。\n\n**1. 贝叶斯模型构建**\n\n先验、后验、似然和证据之间的关系由贝叶斯定理给出：\n$$ P(p \\mid \\mathcal{D}) \\propto P(\\mathcal{D} \\mid p) P(p) $$\n这里，$P(p)$ 是相互作用概率 $p$ 的先验分布，代表我们在考虑任何特定证据 $\\mathcal{D}$ 之前的信念。$P(\\mathcal{D} \\mid p)$ 是似然，它量化了对于给定的 $p$ 值，观测到证据 $\\mathcal{D}$ 的可能性。$P(p \\mid \\mathcal{D})$ 是后验分布，代表我们在观测到 $\\mathcal{D}$ 后对 $p$ 的更新信念。\n\n**2. 先验分布**\n\n我们被要求对 $p$ 使用一个对称的、无信息的先验。Beta 分布 $\\mathrm{Beta}(\\alpha, \\beta)$ 是伯努利似然的共轭先验，这简化了贝叶斯更新过程。对于 Beta 先验，一个无信息的选择是参数为 $\\alpha_0=1$ 和 $\\beta_0=1$ 的情况。这对应于区间 $[0, 1]$ 上的均匀分布，意味着所有 $p$ 的值在先验上都被认为是等可能的。先验概率密度函数 (PDF) 是：\n$$ P(p) = \\frac{p^{\\alpha_0-1}(1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)} = \\frac{p^{1-1}(1-p)^{1-1}}{B(1, 1)} = 1 \\quad \\text{对于 } p \\in [0, 1] $$\n其中 $B(\\alpha, \\beta)$ 是 Beta 函数。\n\n**3. 证据协调与似然函数**\n\n每条边的证据来自多个来源，每个来源都有类型 $t$、置信度分数 $r$、结论 $v$ 和数据库来源 $d$。为了结合这些不同的信息片段，我们首先将它们协调成一个通用的“伪计数”权重 $w$。给定的转换为：\n$$ w = \\min\\left( W_{\\max},\\ \\lambda_{t}\\ \\eta_{d}\\ \\frac{r}{1-r} \\right) $$\n其中 $W_{\\max}=10$，$\\lambda_t$ 和 $\\eta_d$ 是指定的可靠性和校准乘数。项 $r/(1-r)$ 代表证据正确的几率。这个权重 $w$ 量化了单条证据的强度。\n\n我们将支持性结论解释为具有 $w$ 个“成功”的分数阶伯努利试验，将反驳性结论解释为具有 $w$ 个“失败”的分数阶试验。对于一个包含 $N$ 个证据项的集合 $\\mathcal{D} = \\{(v_i, w_i)\\}_{i=1}^N$，组合的似然函数是通过将每条证据视为独立的观测来构建的。对于权重为 $w_i$、结论为 $v_i$ 的单次观测，如果 $v_i = \\text{支持}$，其似然贡献与 $p^{w_i}$ 成正比；如果 $v_i = \\text{反驳}$，则与 $(1-p)^{w_i}$ 成正比。总似然是这些单独贡献的乘积：\n$$ P(\\mathcal{D} \\mid p) \\propto \\prod_{i=1}^N \\begin{cases} p^{w_i}  \\text{如果 } v_i = \\text{支持} \\\\ (1-p)^{w_i}  \\text{如果 } v_i = \\text{反驳} \\end{cases} $$\n设 $W_{\\text{支持}}$ 是所有支持性证据的权重之和，$W_{\\text{反驳}}$ 是所有反驳性证据的权重之和。似然函数简化为：\n$$ P(\\mathcal{D} \\mid p) \\propto p^{W_{\\text{支持}}} (1-p)^{W_{\\text{反驳}}} $$\n\n**4. 后验分布**\n\n应用贝叶斯定理，我们结合先验和似然：\n$$ P(p \\mid \\mathcal{D}) \\propto P(\\mathcal{D} \\mid p) P(p) \\propto \\left( p^{W_{\\text{支持}}} (1-p)^{W_{\\text{反驳}}} \\right) \\left( p^{\\alpha_0-1} (1-p)^{\\beta_0-1} \\right) $$\n$$ P(p \\mid \\mathcal{D}) \\propto p^{\\alpha_0 + W_{\\text{支持}} - 1} (1-p)^{\\beta_0 + W_{\\text{反驳}} - 1} $$\n这个表达式是 Beta 分布的核。因此，后验分布也是一个 Beta 分布：\n$$ p \\mid \\mathcal{D} \\sim \\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}}) $$\n其后验参数为：\n$$ \\alpha_{\\text{post}} = \\alpha_0 + W_{\\text{支持}} = 1 + W_{\\text{支持}} $$\n$$ \\beta_{\\text{post}} = \\beta_0 + W_{\\text{反驳}} = 1 + W_{\\text{反驳}} $$\n\n**5. 目标量的计算**\n\n一旦我们有了后验分布 $\\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})$，我们就可以计算所需的量：\n\n- **后验均值：** $\\mathrm{Beta}(\\alpha, \\beta)$ 随机变量的期望值提供了 $p$ 的点估计。\n  $$ p_{\\text{mean}} = E[p \\mid \\mathcal{D}] = \\frac{\\alpha_{\\text{post}}}{\\alpha_{\\text{post}} + \\beta_{\\text{post}}} $$\n\n- **可信区间：** $95\\%$ 的中心可信区间给出了 $p$ 的一个合理值范围。它由后验分布的 $0.025$ 和 $0.975$ 分位数定义。这些值是通过 Beta 累积分布函数 (CDF) 的逆函数（也称为百分点函数, PPF）找到的。\n  $$ p_{\\text{lower}} = \\text{PPF}_{\\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})}(0.025) $$\n  $$ p_{\\text{upper}} = \\text{PPF}_{\\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})}(0.975) $$\n\n- **共识决策：** 决策规则是可信区间的直接应用。如果存在强有力的信心表明其概率高于 $0.5$，则接受该相互作用。\n  $$ \\text{decision} = (p_{\\text{lower}} \\ge 0.5) $$\n\n这种结构化的、基于原理的方法能够对证据进行稳健且可解释的综合，最终为每个候选相互作用得出一个清晰、可量化的结论。我们现在将实现此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta as beta_dist\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the protein-protein interaction evidence harmonization problem.\n    \"\"\"\n    # Define constants and parameters provided in the problem statement.\n    LAMBDA_T = {\n        'Y2H': 1.0,\n        'AP-MS': 0.8,\n        'Coexp': 0.5,\n        'Curated': 1.2\n    }\n    ETA_D = {\n        'BioGRID': 1.0,\n        'IntAct': 1.0,\n        'STRING': 0.9,\n        'Literature': 1.1\n    }\n    W_MAX = 10.0\n    ALPHA_0 = 1.0\n    BETA_0 = 1.0\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        # Edge A–B\n        [\n            ('BioGRID', 'Y2H', 'support', 0.95),\n            ('IntAct', 'AP-MS', 'support', 0.85),\n            ('Literature', 'Curated', 'support', 0.90),\n            ('STRING', 'Coexp', 'refute', 0.20)\n        ],\n        # Edge C–D\n        [\n            ('STRING', 'Coexp', 'support', 0.60),\n            ('BioGRID', 'Y2H', 'refute', 0.70),\n            ('IntAct', 'AP-MS', 'support', 0.70),\n            ('Literature', 'Curated', 'refute', 0.65)\n        ],\n        # Edge E–F\n        [\n            ('STRING', 'Coexp', 'refute', 0.95),\n            ('IntAct', 'AP-MS', 'refute', 0.90),\n            ('Literature', 'Curated', 'refute', 0.80)\n        ],\n        # Edge G–H\n        [\n            ('BioGRID', 'Y2H', 'support', 0.99),\n            ('STRING', 'Coexp', 'support', 0.51),\n            ('IntAct', 'AP-MS', 'refute', 0.05)\n        ],\n        # Edge I–J\n        [\n            ('Literature', 'Curated', 'support', 0.70),\n            ('BioGRID', 'Y2H', 'refute', 0.60)\n        ]\n    ]\n\n    results = []\n    for edge_evidence in test_cases:\n        w_support = 0.0\n        w_refute = 0.0\n\n        for d, t, v, r in edge_evidence:\n            lambda_val = LAMBDA_T[t]\n            eta_val = ETA_D[d]\n\n            # The term r/(1-r) represents the odds.\n            # It is undefined for r=1. Handle this by treating it as infinite odds,\n            # which will always result in the maximum weight W_MAX.\n            if r == 1.0:\n                odds = float('inf')\n            elif r == 0.0:\n                odds = 0.0\n            else:\n                odds = r / (1.0 - r)\n\n            raw_weight = lambda_val * eta_val * odds\n            weight = min(W_MAX, raw_weight)\n\n            if v == 'support':\n                w_support += weight\n            elif v == 'refute':\n                w_refute += weight\n\n        # Calculate posterior Beta distribution parameters\n        alpha_post = ALPHA_0 + w_support\n        beta_post = BETA_0 + w_refute\n\n        # Calculate target quantities from the posterior distribution\n        \n        # 1. Posterior mean\n        if (alpha_post + beta_post) == 0:\n            # Should not happen with alpha_0, beta_0 > 0\n            p_mean = 0.5\n        else:\n            p_mean = alpha_post / (alpha_post + beta_post)\n\n        # 2. 95% central credible interval\n        p_lower = beta_dist.ppf(0.025, alpha_post, beta_post)\n        p_upper = beta_dist.ppf(0.975, alpha_post, beta_post)\n        \n        # 3. Consensus decision\n        decision = p_lower >= 0.5\n\n        # Format results for the current edge, rounding to 6 decimal places.\n        # np.round is used for safe rounding, e.g., for numbers like 2.5.\n        # Python's round() rounds to nearest even for .5 cases.\n        current_result = [\n            np.round(p_mean, 6),\n            np.round(p_lower, 6),\n            np.round(p_upper, 6),\n            decision\n        ]\n        results.append(current_result)\n\n    # Format the final output string as specified\n    # The str() function on a list and a boolean produces the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在构建了可靠的PPI网络之后，下一个核心任务是识别其中由功能上相关的蛋白质组成的“功能模块”或“蛋白质复合物”。本练习将带您从第一性原理出发，推导并实现马尔可夫聚类（MCL）算法，这是一种在生物网络中识别社群结构的强大方法。通过亲手实现基于随机游走的扩展（expansion）和膨胀（inflation）过程，您将深刻理解网络流如何揭示其内在的模块化结构。",
            "id": "3341694",
            "problem": "给定一个蛋白质-蛋白质相互作用（PPI）网络，它是一个无向加权图，由一个方形对称邻接矩阵 $A \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ 表示，其中 $n$ 是蛋白质（节点）的数量，$A_{ij}$ 表示蛋白质 $i$ 和蛋白质 $j$ 之间的非负相互作用权重。从图上定义的离散时间马尔可夫链的基本概念出发，根据第一性原理推导一个聚类过程，该过程基于重复的多步随机游走扩散和概率重加权操作。在推导中，仅使用马尔可夫链转移矩阵的核心定义和基本矩阵运算；不要引用任何专门的聚类公式。明确说明你推导出的随机游走矩阵上的扩张（expansion）和膨胀（inflation）算子，并解释膨胀参数如何控制最终聚类的粒度。\n\n将你推导的过程实现为一个完整的、可运行的程序，该程序不接受外部输入，并对以下测试套件进行操作。对于每个测试用例，你必须：\n\n- 通过添加自环并进行归一化，构建随机游走矩阵 $W$，使其成为一个列随机矩阵。\n- 通过对 $W$ 进行重复矩阵乘法，应用指数为整数 $e$ 的扩张算子。\n- 应用参数为 $r > 1$ 的膨胀算子，即进行逐元素幂运算，然后进行列归一化，以维持一个有效的转移矩阵。\n- 迭代扩张和膨胀过程，直到在指定的容差 $\\epsilon$ 下收敛，或达到指定的最大迭代次数 $T$；在膨胀后引入一个剪枝阈值 $\\tau$，将极小值置为零以保证数值稳定性。\n- 收敛后，通过将收敛矩阵的非零模式解释为无向图，并通过使用 $\\tau$ 进行阈值化和对称化后计算连通分量，将每个节点分配到一个簇中。\n- 对于每个测试用例，返回一个列表，其第一个元素是整数簇数 $k$，其余元素是按索引升序排列的节点 $0$ 到 $n-1$ 的整数簇标签。\n\n测试套件参数：\n\n1. 一个具有两个密集模块和弱模块间连接的图。邻接矩阵 $A_1 \\in \\mathbb{R}^{6 \\times 6}$ 为：\n$$\nA_1 =\n\\begin{bmatrix}\n0  1  1  0  0  0 \\\\\n1  0  1  0  0  0 \\\\\n1  1  0  0.05  0  0 \\\\\n0  0  0.05  0  1  1 \\\\\n0  0  0  1  0  1 \\\\\n0  0  0  1  1  0\n\\end{bmatrix}\n$$\n对 $A_1$ 运行三个案例：$(e=\\;2, r=\\;1.2, \\epsilon=\\;10^{-6}, \\tau=\\;10^{-4}, T=\\;50)$，$(e=\\;2, r=\\;2.0, \\epsilon=\\;10^{-6}, \\tau=\\;10^{-4}, T=\\;50)$，以及 $(e=\\;2, r=\\;4.0, \\epsilon=\\;10^{-6}, \\tau=\\;10^{-4}, T=\\;50)$。\n\n2. 一个捕获中心-辐射结构的星形图。邻接矩阵 $A_2 \\in \\mathbb{R}^{6 \\times 6}$ 为：\n$$\nA_2 =\n\\begin{bmatrix}\n0  1  1  1  1  1 \\\\\n1  0  0  0  0  0 \\\\\n1  0  0  0  0  0 \\\\\n1  0  0  0  0  0 \\\\\n1  0  0  0  0  0 \\\\\n1  0  0  0  0  0\n\\end{bmatrix}\n$$\n对 $A_2$ 运行一个案例：$(e=\\;2, r=\\;2.0, \\epsilon=\\;10^{-6}, \\tau=\\;10^{-4}, T=\\;50)$。\n\n3. 一个模拟近乎同质相互作用环境的完全图。邻接矩阵 $A_3 \\in \\mathbb{R}^{5 \\times 5}$ 为：\n$$\nA_3 =\n\\begin{bmatrix}\n0  1  1  1  1 \\\\\n1  0  1  1  1 \\\\\n1  1  0  1  1 \\\\\n1  1  1  0  1 \\\\\n1  1  1  1  0\n\\end{bmatrix}\n$$\n对 $A_3$ 运行一个案例：$(e=\\;2, r=\\;2.0, \\epsilon=\\;10^{-6}, \\tau=\\;10^{-4}, T=\\;50)$。\n\n重要实现要求：\n\n- 自环：在构建随机游走矩阵之前，加上一个缩放系数为1的单位矩阵 $I$（即 $A \\leftarrow A + I$），以确保非周期性和非零的列和。\n- 列随机归一化：对于一个矩阵 $M$，在构建和维护随机游走矩阵时，确保对每一列 $j$ 都有 $M_{ij} \\ge 0$ 和 $\\sum_{i} M_{ij} = 1$。\n- 收敛性：使用矩阵差的 $\\ell_1$ 范数来检查收敛性，即当 $\\sum_{i,j} \\lvert W^{(t+1)}_{ij} - W^{(t)}_{ij} \\rvert  \\epsilon$ 或当 $t$ 达到 $T$ 时停止。\n- 剪枝：在膨胀之后，将严格小于 $\\tau$ 的值设为 $0$，然后重新对列进行归一化。\n\n最终输出规范：\n\n- 对于每个测试用例，输出一个列表，其第一个元素是整数簇数 $k$，后面跟着 $n$ 个整数，给出每个节点 $0$ 到 $n-1$ 的簇标签。\n- 你的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由逗号分隔的列表，并用方括号括起来，例如 $\\texttt{[[k,\\ell_0,\\dots,\\ell_{n-1}],\\dots]}$。不应打印任何额外的文本。",
            "solution": "该问题要求推导并实现一个基于模拟随机游走的图聚类算法。该过程在文献中被称为马尔可夫聚类（Markov Clustering, MCL）算法。在此，我们按要求从第一性原理推导其核心组成部分。\n\n### 从第一性原理推导\n\n我们的出发点是一个蛋白质-蛋白质相互作用（PPI）网络，表示为一个无向加权图。该图的结构被编码在一个对称邻接矩阵 $A \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ 中，其中 $n$ 是蛋白质（节点）的数量，$A_{ij} \\ge 0$ 是蛋白质 $i$ 和 $j$ 之间的相互作用权重。其基本思想是，在此图上的随机游走更有可能停留在密集连接的区域（一个假定的簇）内，而不是通过稀疏的连接穿越到另一个区域。该算法旨在通过交替使用两个算子来放大这种趋势：扩张（expansion）和膨胀（inflation）。\n\n**1. 构建马尔可夫链**\n\n首先，我们将图上的随机游走形式化为离散时间马尔可夫链。链的状态是图的节点（蛋白质）。我们需要定义状态之间的转移概率。\n\n一个在节点 $j$ 的游走者可以移动到相邻节点 $i$，其概率与边权重 $A_{ij}$ 成正比。为了构建一个有效的随机过程，我们必须确保图是非周期的，并且每个节点至少有一条出路（包括到自身），以防止游走被困住。这可以通过为每个节点添加权重为1的自环来实现。修改后的邻接矩阵变为 $A' = A + I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n从节点 $j$ 转移到节点 $i$ 的概率，记为 $W_{ij}$，是边 $(j, i)$ 的权重除以从节点 $j$ 出发的所有边的权重之和。这个和是 $A'$ 第 $j$ 列的元素之和。\n$$\nW_{ij} = \\frac{A'_{ij}}{\\sum_{k=1}^{n} A'_{kj}}\n$$\n由此产生的矩阵 $W$ 是我们马尔可夫链的转移矩阵。通过这种构造，它是一个列随机矩阵，意味着它的每个元素 $W_{ij}$ 都是非负的，并且每一列的和都为1（对所有 $j$ 都有 $\\sum_{i=1}^{n} W_{ij} = 1$）。$W$ 的第 $j$ 列中的向量表示，在从节点 $j$ 开始的情况下，游走者经过一步后的位置的概率分布。\n\n**2. 扩张算子**\n\n扩张步骤模拟了随机游走在图上经过多步的扩散过程。矩阵 $W^2 = W \\times W$ 给出了一个2步游走的概率。具体来说，元素 $(W^2)_{ij}$ 是一个随机游走者从节点 $j$ 开始，在恰好两步内到达节点 $i$ 的概率。这是通过对所有可能的中间节点 $k$ 求和来计算的：\n$$\n(W^2)_{ij} = \\sum_{k=1}^{n} W_{ik} W_{kj}\n$$\n将此推广，对于整数指数 $e  1$，矩阵 $W^e$ 包含了 $e$ 步游走的概率。因此，扩张算子被定义为取一个随机矩阵 $M$ 的 $e$ 次幂：\n$$\n\\text{Expand}(M) = M^e\n$$\n这个操作允许概率“流”探索起始节点的邻域。在具有自然簇的图中，这种流将主要在这些簇内传播，从而相对于簇间连接，加强了簇内连接。\n\n**3. 膨胀算子**\n\n在扩张使流扩散之后，我们需要一个机制来收缩它，从而突出强流路径和弱流路径之间的区别。膨胀算子通过修改每一列中的概率分布，偏好高概率的转移并惩罚低概率的转移来达到这个目的。\n\n这是通过逐元素求幂然后重新归一化来实现的。对于给定的随机矩阵 $M$ 和膨胀参数 $r  1$，我们首先通过将 $M$ 的每个元素提升到 $r$ 次幂来计算一个矩阵 $M'$：$M'_{ij} = (M_{ij})^r$。大于1的 $r$ 值确保了较高的概率比较低的概率被放大的程度更大（例如，如果 $p_1  p_2$，那么 $p_1^r / p_2^r  p_1 / p_2$）。\n\n这个操作之后，$M'$ 的列和不再是1。为了恢复随机性，我们必须对每一列重新进行归一化。因此，膨胀算子 $\\Gamma_r$ 定义为：\n$$\n(\\Gamma_r(M))_{ij} = \\frac{(M_{ij})^r}{\\sum_{k=1}^{n} (M_{kj})^r}\n$$\n膨胀参数 $r$ 直接控制聚类的粒度。较高的 $r$ 值会施加更强的“赢家通吃”动态，更积极地增强概率之间的对比。这会导致更硬的簇边界，并倾向于将图分割成更多、更小（更细粒度）的簇。相反，接近1的 $r$ 值效果更温和，有利于发现更大、更粗粒度的簇。\n\n**4. 迭代过程**\n\nMCL 算法迭代这两个算子：\n1. 从初始转移矩阵 $W = W^{(0)}$ 开始。\n2. 对于每次迭代 $t=0, 1, 2, \\dots$：\n   a. **扩张：** $W^{(t, \\text{exp})} = \\text{Expand}(W^{(t)}) = (W^{(t)})^e$。\n   b. **膨胀：** $W^{(t+1)} = \\Gamma_r(W^{(t, \\text{exp})})$。\n这个过程一直持续到矩阵 $W^{(t)}$ 收敛，即连续迭代之间的变化小于给定的容差 $\\epsilon$。收敛后，该矩阵表示图的流的一个划分。它变成一个近乎幂等的矩阵，其非零项构成的图由几个不连通的分量组成，这些分量对应于识别出的簇。在每个分量内，其节点对应的列变得相同，代表了限制在该簇内的随机游走的平稳分布。最终的簇是通过从收敛矩阵中识别这些不相交的分量来提取的。",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\nfrom scipy.sparse import csr_matrix\n\ndef solve():\n    \"\"\"\n    Main function to run the MCL algorithm on the specified test suite.\n    \"\"\"\n\n    # --- Test Suite Definition ---\n\n    A1 = np.array([\n        [0, 1, 1, 0, 0, 0],\n        [1, 0, 1, 0, 0, 0],\n        [1, 1, 0, 0.05, 0, 0],\n        [0, 0, 0.05, 0, 1, 1],\n        [0, 0, 0, 1, 0, 1],\n        [0, 0, 0, 1, 1, 0]\n    ])\n\n    A2 = np.array([\n        [0, 1, 1, 1, 1, 1],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0]\n    ])\n\n    A3 = np.array([\n        [0, 1, 1, 1, 1],\n        [1, 0, 1, 1, 1],\n        [1, 1, 0, 1, 1],\n        [1, 1, 1, 0, 1],\n        [1, 1, 1, 1, 0]\n    ])\n\n    test_cases = [\n        {'A': A1, 'e': 2, 'r': 1.2, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A1, 'e': 2, 'r': 2.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A1, 'e': 2, 'r': 4.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A2, 'e': 2, 'r': 2.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A3, 'e': 2, 'r': 2.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n    ]\n\n    def run_mcl(A, e, r, epsilon, tau, T):\n        \"\"\"\n        Implements the Markov Clustering (MCL) algorithm.\n        \"\"\"\n        n = A.shape[0]\n\n        # 1. Add self-loops and normalize to get the initial transition matrix W\n        A_self_looped = A + np.identity(n)\n        col_sums = A_self_looped.sum(axis=0)\n        \n        # Handle columns that might sum to zero in a general case\n        # (though adding identity prevents this for non-empty graphs)\n        col_sums[col_sums == 0] = 1.0\n        W = A_self_looped / col_sums\n        \n        for _ in range(T):\n            W_prev = W\n\n            # 2. Expansion step: Simulate random walks\n            W = np.linalg.matrix_power(W, e)\n\n            # 3. Inflation step: Reweight probabilities\n            W = np.power(W, r)\n            \n            # Pruning step for numerical stability and sparsity\n            W[W  tau] = 0.0\n\n            # 4. Normalization step to restore column-stochastic property\n            col_sums = W.sum(axis=0)\n            \n            # Avoid division by zero for columns that become all-zero after pruning\n            non_zero_cols_mask = col_sums != 0\n            # Use np.divide for safe division\n            W = np.divide(W, col_sums, out=np.zeros_like(W), where=non_zero_cols_mask)\n\n            # 5. Check for convergence using the L1 norm of the difference\n            if np.sum(np.abs(W - W_prev))  epsilon:\n                break\n        \n        # 6. Cluster assignment from the converged matrix\n        # Create an undirected graph based on the non-zero pattern of W\n        adj_matrix_thresholded = W > tau\n        adj_matrix_symmetric = adj_matrix_thresholded | adj_matrix_thresholded.T\n\n        # Find connected components in the resulting graph\n        graph_sparse = csr_matrix(adj_matrix_symmetric)\n        n_components, labels = connected_components(csgraph=graph_sparse, directed=False, connection='weak')\n        \n        return [n_components] + labels.tolist()\n    \n    results = []\n    for case in test_cases:\n        result = run_mcl(case['A'], case['e'], case['r'], case['epsilon'], case['tau'], case['T'])\n        results.append(result)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的图聚类算法能有效识别密集的相互作用区域，但真实的蛋白质复合物组装还受到化学计量比和蛋白丰度的严格限制。本高级练习将引导您超越纯粹的拓扑分析，将复合物的推断问题构建为一个混合整数线性规划模型。通过施加化学计量、蛋白供给量和连接性等生物物理约束，您将学习如何构建一个更具机理性的模型，以预测细胞内可行的复合物组装状态。",
            "id": "3341707",
            "problem": "你的任务是，在一个蛋白质-蛋白质相互作用网络上，将一个考虑化学计量的复合物推断问题，构建并求解为一个混合整数网络流问题。其基础必须源于质量守恒和化学计量约束，并且必须强制要求组装复合物的整数性，以排除不可能的组装状态。\n\n从以下基本原则出发：(i) 质量守恒，要求组装所消耗的亚基等于化学计量所需的亚基；(ii) 网络流守恒，要求分配的亚基数量不超过供应量；(iii) 复合物计数的整数性，要求组装的复合物数量为整数值。利用这些原则推导出一个适合计算的混合整数公式。\n\n将蛋白质-蛋白质相互作用 (PPI) 网络定义为一个简单的无向图 $G = (V,E)$，其中 $V$是一组亚基类型，$E$ 是表示观察到的物理相互作用的边集。令 $C$ 为一个有限的复合物集合。对于每个复合物 $c \\in C$，定义一个化学计量向量 $\\mathbf{s}^{(c)} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$，其分量 $s_{u,c}$ 是每个组装好的复合物 $c$ 所需的亚基 $u \\in V$ 的拷贝数。令 $\\mathbf{n} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$ 为每种亚基的可用拷贝数。为每个 $c \\in C$ 定义决策变量 $x_c \\in \\mathbb{Z}_{\\ge 0}$，表示复合物 $c$ 的组装拷贝数；并定义整数流 $f_{u,c} \\in \\mathbb{Z}_{\\ge 0}$，表示在一个从亚基到复合物的概念性二分流中，分配给复合物 $c$ 的亚基 $u$ 的拷贝数。约束必须包括：\n- 对于每个亚基 $u \\in V$：$$\\sum_{c \\in C} f_{u,c} \\le n_u$$\n- 对于每个复合物 $c \\in C$ 和亚基 $u \\in V$：$$f_{u,c} = s_{u,c} \\, x_c$$\n- 对于每个复合物 $c \\in C$，只有当集合 $\\{u \\in V \\mid s_{u,c}  0\\}$ 上的诱导子图在 $G$ 中是连通的时，才允许组装；否则，$x_c = 0$。\n\n你的目标是计算整数向量 $\\mathbf{x} = (x_c)_{c \\in C}$，在满足上述约束的条件下，使得组装复合物的总数 $$\\sum_{c \\in C} x_c$$ 最大化。如果存在多个最优整数解，则通过在固定的复合物排序下选择字典序最小的 $\\mathbf{x}$ 来打破平局。\n\n实现一个程序，给定以下参数集测试套件，为每个案例输出最优的整数复合物计数 $\\mathbf{x}$。没有输入；请将测试套件硬编码。\n\n测试套件：\n- 案例 1：\n  - 亚基 $V = \\{A,B,C\\}$\n  - 边 $E = \\{\\{A,B\\}, \\{B,C\\}, \\{A,C\\}\\}$ (一个三角形)\n  - 复合物 $C = \\{X,Y\\}$ 及其化学计量：\n    - $X$: $s_{A,X} = 1$, $s_{B,X} = 1$, $s_{C,X} = 0$\n    - $Y$: $s_{A,Y} = 0$, $s_{B,Y} = 2$, $s_{C,Y} = 1$\n  - 供应量：$n_A = 3$, $n_B = 4$, $n_C = 2$\n  - 复合物排序：$[X,Y]$\n- 案例 2：\n  - 亚基 $V = \\{A,B,D\\}$\n  - 边 $E = \\{\\{A,B\\}\\}$ ($A$ 和 $D$ 之间没有边)\n  - 复合物 $C = \\{X,Z\\}$ 及其化学计量：\n    - $X$: $s_{A,X} = 1$, $s_{B,X} = 1$, $s_{D,X} = 0$\n    - $Z$: $s_{A,Z} = 1$, $s_{B,Z} = 0$, $s_{D,Z} = 1$\n  - 供应量：$n_A = 2$, $n_B = 2$, $n_D = 1$\n  - 复合物排序：$[X,Z]$\n- 案例 3：\n  - 亚基 $V = \\{A,B,C\\}$\n  - 边 $E = \\{\\{A,B\\}, \\{B,C\\}, \\{A,C\\}\\}$ (一个三角形)\n  - 复合物 $C = \\{T,W\\}$ 及其化学计量：\n    - $T$: $s_{A,T} = 1$, $s_{B,T} = 1$, $s_{C,T} = 1$\n    - $W$: $s_{A,W} = 2$, $s_{B,W} = 0$, $s_{C,W} = 1$\n  - 供应量：$n_A = 2$, $n_B = 1$, $n_C = 1$\n  - 复合物排序：$[T,W]$\n\n你的程序必须验证每个复合物的连通性可行性，根据化学计量和供应量计算 $x_c$ 的上界，并通过剪枝搜索整数解以满足每个 $u \\in V$ 的 $\\sum_{c \\in C} s_{u,c} x_c \\le n_u$ 约束。最终输出应为单行，包含一个以逗号分隔的列表的列表形式的结果，其中每个内部列表给出该案例在指定复合物排序下的最优整数计数。例如，输出格式应为 $[[x_{X},x_{Y}], [x_{X},x_{Z}], [x_{T},x_{W}]]$，精确地作为单个 Python 列表字面量打印在一行上。此问题不涉及物理单位或角度；所有输出均为无单位的整数。",
            "solution": "用户提供了一个定义明确的混合整数优化问题，用于从蛋白质-蛋白质相互作用 (PPI) 网络推断蛋白质复合物的组装。我将首先验证问题陈述，然后提供完整的解决方案。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n- **网络：** 一个简单无向图 $G = (V,E)$，其中 $V$ 是一组亚基类型，$E$ 是一组物理相互作用。\n- **复合物：** 一个有限的潜在复合物集合 $C$。\n- **化学计量：** 对于每个复合物 $c \\in C$，一个向量 $\\mathbf{s}^{(c)} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$，其中 $s_{u,c}$ 是一个复合物 $c$ 拷贝中亚基 $u \\in V$ 的拷贝数。\n- **供应量：** 一个向量 $\\mathbf{n} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$，其中 $n_u$ 是亚基 $u$ 的可用拷贝数。\n- **决策变量：**\n    - $x_c \\in \\mathbb{Z}_{\\ge 0}$：复合物 $c$ 的组装拷贝数。\n    - $f_{u,c} \\in \\mathbb{Z}_{\\ge 0}$：分配给复合物 $c$ 的亚基 $u$ 的拷贝数。\n- **约束：**\n    1.  供应约束：对于每个亚基 $u \\in V$，$\\sum_{c \\in C} f_{u,c} \\le n_u$。\n    2.  化学计量约束：对于每个复合物 $c \\in C$ 和亚基 $u \\in V$，$f_{u,c} = s_{u,c} \\, x_c$。\n    3.  连通性约束：对于每个复合物 $c \\in C$，如果由其组成亚基 $\\{u \\in V \\mid s_{u,c}  0\\}$ 诱导的 $G$ 的子图不连通，则 $x_c = 0$。\n- **目标函数：** 最大化组装复合物的总数，$\\sum_{c \\in C} x_c$。\n- **平局打破规则：** 在具有相同最大和的多个解中，根据指定的复合物排序，选择其向量 $\\mathbf{x} = (x_c)_{c \\in C}$ 字典序最小的解。\n- **测试套件：** 提供了问题的三个具体实例，包含所有必要参数（亚基、边、复合物、化学计量、供应量和排序）。\n\n**步骤2：使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题是计算系统生物学中用于蛋白质复合物预测的标准公式。它基于质量守恒（化学计量和供应约束）和生物物理可行性（连通性约束）的基本原则。这在科学上是合理的。\n- **良态性 (Well-Posed)：** 该问题是一个有界整数线性规划 (ILP) 问题。决策变量 $x_c$ 是非负整数。约束定义了一个有界可行域（对于连续松弛是一个凸多胞体）。在此区域内的整数点上最大化一个线性目标函数，保证了最优解的存在。指定的平局打破规则确保了解的唯一性。\n- **客观性：** 该问题使用精确的数学符号定义，并寻求一个可量化的、客观的结果。它没有主观或模糊的语言。\n- **完整性和一致性：** 提供了测试用例所需的所有数据。这些约束可以合并为一组单一、一致的不等式。通过将化学计量约束代入供应约束，我们得到对于每个亚基 $u$ 的 $\\sum_{c \\in C} s_{u,c} x_c \\le n_u$，这通过消除流变量 $f_{u,c}$ 简化了问题。不存在矛盾。\n\n**步骤3：结论与行动**\n\n该问题是有效的，因为它具有科学依据、良态、客观且定义完整。我将继续构建并实现一个解决方案。\n\n### 解法推导与算法设计\n\n问题在于求解一个小规模的整数线性规划 (ILP)。鉴于测试用例的规模较小，回溯递归搜索是一种合适且有效的方法。\n\n**1. 模型简化**\n流变量 $f_{u,c}$ 可以从模型中代换掉。约束 $f_{u,c} = s_{u,c} x_c$ 允许我们直接用主决策变量 $x_c$ 来重写供应约束 $\\sum_{c \\in C} f_{u,c} \\le n_u$：\n$$ \\sum_{c \\in C} s_{u,c} x_c \\le n_u \\quad \\forall u \\in V $$\n因此，优化问题变为：\n- **最大化：** $Z = \\sum_{c \\in C} x_c$\n- **约束条件：**\n    1. 对于每个亚基 $u \\in V$，$ \\sum_{c \\in C} s_{u,c} x_c \\le n_u$。\n    2. 如果复合物 $c$ 不满足连通性可行性，则 $x_c = 0$。\n    3. 对于每个复合物 $c \\in C$，$x_c \\in \\mathbb{Z}_{\\ge 0}$。\n\n**2. 连通性可行性检查**\n对于每个复合物 $c$，我们必须首先验证其结构可行性。这包括识别所需亚基的集合 $V_c = \\{u \\in V \\mid s_{u,c}  0\\}$，并检查由 $V_c$ 诱导的 $G$ 的子图是否连通。这个检查可以使用图遍历算法（如广度优先搜索 (BFS) 或深度优先搜索 (DFS)）高效地执行。\n算法如下：\n1. 如果集合 $V_c$ 为空或只包含一个亚基，则认为它是连通的。\n2. 否则，从 $V_c$ 中任选一个起始亚基 $u_{start}$。\n3. 从 $u_{start}$ 开始执行遍历（例如 BFS），只访问同样在 $V_c$ 中的邻居。\n4. 统计访问到的唯一亚基的数量。如果此数量等于 $V_c$ 的大小，则子图是连通的。否则，不连通。\n如果一个复合物被发现是不可行的，其计数 $x_c$ 在问题的剩余部分中被固定为 $0$。\n\n**3. 回溯搜索算法**\n递归回溯搜索将探索整数向量 $\\mathbf{x}$ 的解空间。搜索过程是根据指定的复合物排序，一次为一个复合物的计数做出决策。\n\n我们的递归函数 `search(k, x_path, current_supplies)` 的状态可以由以下内容定义：\n- `k`: 在指定排序中，我们正在确定其计数的复合物的索引。\n- `x_path`: 已为前 $k$ 个复合物确定的计数列表。\n- `current_supplies`: 在形成 `x_path` 中的复合物后，剩余亚基供应量的向量。\n\n算法流程如下：\n1.  **基本情况：** 如果所有复合物的计数都已确定（$k$ 等于复合物总数），我们就找到了一个完整且可行的解向量。我们将其和 $\\sum x_c$ 与迄今为止找到的最佳和进行比较。\n    - 如果当前和更大，我们更新找到的最佳解。\n    - 如果当前和相等，我们应用平局打破规则：如果当前解向量在字典序上小于找到的最佳解，我们更新它。\n\n2.  **递归步骤：** 对于排序中的第 $k$ 个复合物：\n    a. 检查它是否满足连通性可行性。如果不满足，其计数必须为 $0$。我们将当前复合物计数设为 $0$，继续递归到下一个复合物。\n    b. 如果可行，根据 `current_supplies` 计算该复合物的最大可能整数计数 $x_{k,max}$。这由限制性最强的亚基决定：\n       $$ x_{k,max} = \\min_{u \\in V \\text{ s.t. } s_{u,k}  0} \\left\\lfloor \\frac{n_{u, \\text{current}}}{s_{u,k}} \\right\\rfloor $$\n    c. 从 $x_{k,max}$ 向下迭代到 $0$，遍历该复合物所有可能的整数计数。向下迭代是一种贪心启发式方法，有助于更快地找到高总和的解。对于每个潜在的计数 `val`：\n       i.  通过减去消耗的亚基来更新供应量：$\\mathbf{n}_{\\text{new}} = \\mathbf{n}_{\\text{current}} - \\text{val} \\cdot \\mathbf{s}^{(k)}$。\n       ii. 对下一个复合物进行递归调用：`search(k+1, x_path + [val], n_new)`。\n\n此搜索系统地探索了复合物计数的所有有效组合，保证能找到符合目标和平局打破规则的最优解。",
            "answer": "```python\nimport numpy as np\n\nclass StoichiometrySolver:\n    \"\"\"\n    Solves the stoichiometry-aware complex inference problem for a single case\n    using a backtracking recursive search.\n    \"\"\"\n    def __init__(self, V, E, C, S, n, complex_order):\n        # Map subunit and complex names to integer indices for easier processing\n        self.subunit_names = list(V)\n        self.subunit_map = {name: i for i, name in enumerate(self.subunit_names)}\n        self.num_subunits = len(V)\n        \n        self.complex_names_in_order = list(complex_order)\n        self.complex_map = {name: i for i, name in enumerate(C)}\n        self.num_complexes = len(C)\n        \n        # The search will follow the specified complex ordering\n        self.search_order_indices = [self.complex_map[name] for name in self.complex_names_in_order]\n\n        # Store initial supplies as a NumPy array\n        self.initial_supplies = np.zeros(self.num_subunits, dtype=int)\n        for u_name, count in n.items():\n            if u_name in self.subunit_map:\n                self.initial_supplies[self.subunit_map[u_name]] = count\n\n        # Build adjacency list for the PPI graph\n        self.adj = {i: set() for i in range(self.num_subunits)}\n        for u_name, v_name in E:\n            u_idx, v_idx = self.subunit_map[u_name], self.subunit_map[v_name]\n            self.adj[u_idx].add(v_idx)\n            self.adj[v_idx].add(u_idx)\n\n        # Build stoichiometry matrix S[c_idx, u_idx]\n        self.stoichiometry = np.zeros((self.num_complexes, self.num_subunits), dtype=int)\n        for c_name, s_dict in S.items():\n            c_idx = self.complex_map[c_name]\n            for u_name, count in s_dict.items():\n                if u_name in self.subunit_map:\n                    u_idx = self.subunit_map[u_name]\n                    self.stoichiometry[c_idx, u_idx] = count\n        \n        # Placeholders for solver state\n        self.is_feasible = []\n        self.best_x = []\n        self.best_sum = -1\n\n    def _is_connected(self, subunit_indices):\n        \"\"\"Checks if the subgraph induced by a set of subunits is connected using BFS.\"\"\"\n        nodes = set(subunit_indices)\n        if not nodes or len(nodes) == 1:\n            return True\n\n        q = [next(iter(nodes))]\n        visited = {q[0]}\n        head = 0\n        while head  len(q):\n            u = q[head]\n            head += 1\n            for v in self.adj.get(u, set()):\n                if v in nodes and v not in visited:\n                    visited.add(v)\n                    q.append(v)\n        \n        return len(visited) == len(nodes)\n\n    def _check_all_complexes_feasibility(self):\n        \"\"\"Pre-computes connectivity feasibility for all complexes.\"\"\"\n        self.is_feasible = [False] * self.num_complexes\n        for c_idx in range(self.num_complexes):\n            required_subunit_indices = np.where(self.stoichiometry[c_idx] > 0)[0]\n            if self._is_connected(required_subunit_indices):\n                self.is_feasible[c_idx] = True\n\n    def _search(self, k, path, supplies):\n        \"\"\"\n        Recursive backtracking search.\n        k: index in the search order.\n        path: list of counts for complexes decided so far.\n        supplies: np.array of remaining subunit supplies.\n        \"\"\"\n        if k == len(self.search_order_indices):\n            # Base case: a complete, feasible assignment is found\n            current_sum = sum(path)\n            if current_sum > self.best_sum:\n                self.best_sum = current_sum\n                self.best_x = list(path)\n            elif current_sum == self.best_sum:\n                # Tie-breaking: choose lexicographically smallest vector\n                if path  self.best_x:\n                    self.best_x = list(path)\n            return\n\n        c_idx = self.search_order_indices[k]\n\n        if not self.is_feasible[c_idx]:\n            self._search(k + 1, path + [0], supplies)\n            return\n\n        s_vec = self.stoichiometry[c_idx]\n        \n        # Calculate max possible count for this complex given current supplies\n        max_count = float('inf')\n        if np.any(s_vec > 0):\n            # Mask to avoid division by zero\n            valid_mask = s_vec > 0\n            # Floor division for all required subunits\n            counts_per_subunit = supplies[valid_mask] // s_vec[valid_mask]\n            max_count = np.min(counts_per_subunit)\n        else:\n            # A complex with no subunits - shouldn't happen in a meaningful problem.\n            # No resources are consumed, but it contributes to the sum.\n            # The problem context implies s_vec is not all-zero for any c.\n            # Setting max_count to 0 is a safe assumption for this problem.\n            max_count = 0\n        \n        # Iterate from max possible count down to 0\n        for count in range(int(max_count), -1, -1):\n            new_supplies = supplies - s_vec * count\n            self._search(k + 1, path + [count], new_supplies)\n\n    def run(self):\n        \"\"\"Main method to run the solver for the configured case.\"\"\"\n        self._check_all_complexes_feasibility()\n        self.best_x = [0] * self.num_complexes\n        self.best_sum = 0\n        \n        self._search(0, [], self.initial_supplies)\n        \n        # The result self.best_x is already in the specified complex_order\n        return self.best_x\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver for each.\n    \"\"\"\n    test_cases = [\n        {\n            'V': {'A', 'B', 'C'},\n            'E': {('A', 'B'), ('B', 'C'), ('A', 'C')},\n            'C': {'X', 'Y'},\n            'S': {\n                'X': {'A': 1, 'B': 1, 'C': 0},\n                'Y': {'A': 0, 'B': 2, 'C': 1}\n            },\n            'n': {'A': 3, 'B': 4, 'C': 2},\n            'complex_order': ['X', 'Y']\n        },\n        {\n            'V': {'A', 'B', 'D'},\n            'E': {('A', 'B')},\n            'C': {'X', 'Z'},\n            'S': {\n                'X': {'A': 1, 'B': 1, 'D': 0},\n                'Z': {'A': 1, 'B': 0, 'D': 1}\n            },\n            'n': {'A': 2, 'B': 2, 'D': 1},\n            'complex_order': ['X', 'Z']\n        },\n        {\n            'V': {'A', 'B', 'C'},\n            'E': {('A', 'B'), ('B', 'C'), ('A', 'C')},\n            'C': {'T', 'W'},\n            'S': {\n                'T': {'A': 1, 'B': 1, 'C': 1},\n                'W': {'A': 2, 'B': 0, 'C': 1}\n            },\n            'n': {'A': 2, 'B': 1, 'C': 1},\n            'complex_order': ['T', 'W']\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        solver = StoichiometrySolver(\n            V=case['V'], \n            E=case['E'], \n            C=case['C'], \n            S=case['S'], \n            n=case['n'], \n            complex_order=case['complex_order']\n        )\n        result = solver.run()\n        all_results.append(result)\n\n    # Print the final result in the exact required format: [[a,b],[c,d],...]\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}