{
    "hands_on_practices": [
        {
            "introduction": "Before searching for motifs in complex biological networks, it is crucial to understand their fundamental combinatorial properties, as the symmetries of a motif's structure dictate how many distinct labeled variants can exist. This exercise  challenges you to apply principles from group theory, specifically Burnside's Lemma, to rigorously count the number of non-isomorphic patterns for a classic network motif. This provides deep insight into the structural diversity and classification of these recurring patterns.",
            "id": "3329444",
            "problem": "Consider the canonical transcriptional regulatory network motif known as the bi-fan: a directed 4-node motif on nodes $\\{A,B,C,D\\}$ with directed edges $A \\to C$, $A \\to D$, $B \\to C$, and $B \\to D$. This motif arises when two transcription factors jointly regulate two target genes. In the context of motif detection and counting algorithms in computational systems biology, an automorphism of a directed graph is a permutation of its nodes that preserves adjacency as well as edge direction, and isomorphism classes of labeled motifs correspond to orbits of the group action of the automorphism group on labelings.\n\nAssume each node is independently assigned one of $r$ biochemical labels (for example, functional categories), with no constraints on reuse of labels across nodes. Two labelings are considered the same pattern if one can be transformed into the other by an automorphism of the directed motif. Starting from the definitions of directed graph automorphisms and the Cauchy–Frobenius (Burnside’s) lemma, and without invoking any pre-stated shortcut formulas, do the following:\n\n1. Determine the size of the automorphism group of the directed bi-fan motif.\n2. Using Burnside’s lemma, derive a closed-form analytic expression, as a function of $r$, for the number of non-isomorphic labeled patterns of the bi-fan motif under node labelings drawn from $r$ labels with unlimited reuse.\n\nExpress your final result as a pair consisting of the automorphism group size and the count of non-isomorphic labeled patterns. No rounding is required. The final answer must be a single pair written as a row matrix, with the first entry being the group size and the second entry being the analytic expression in $r$.",
            "solution": "The user wants me to solve a problem that involves counting the number of non-equivalent ways to label a graph motif.\n\n### Step 1: Problem Validation\n\nFirst, I will validate the problem statement.\n\n#### Extraction of Givens\n- **Graph Motif**: Bi-fan, a directed $4$-node motif.\n- **Nodes**: $V = \\{A,B,C,D\\}$.\n- **Edges**: $E = \\{(A, C), (A, D), (B, C), (B, D)\\}$.\n- **Labels**: A set of $r$ biochemical labels. Each node is assigned one label. Label reuse is unlimited.\n- **Automorphism Definition**: A permutation of nodes that preserves adjacency and edge direction.\n- **Equivalence of Labelings**: Two labelings are considered the same pattern if one can be transformed into the other by an automorphism of the motif.\n- **Method**: Use the Cauchy–Frobenius (Burnside’s) lemma.\n- **Task 1**: Determine the size of the automorphism group of the directed bi-fan motif, $|\\text{Aut}(G)|$.\n- **Task 2**: Derive a closed-form analytic expression for the number of non-isomorphic labeled patterns as a function of $r$.\n- **Output**: A pair: (size of automorphism group, count of patterns).\n\n#### Validation Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is well-grounded in graph theory and its application to computational biology. The bi-fan is a standard network motif. Graph automorphisms, group actions, and Burnside's lemma are the correct mathematical tools for this type of combinatorial counting problem. The premises are factually and scientifically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable within the language of graph theory and group theory. It is directly relevant to the specified topic of motif detection and counting algorithms.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. The graph structure is explicitly defined, the labeling rules are clear, and the equivalence condition is specified. There are no missing definitions or contradictions.\n4.  **Unrealistic or Infeasible**: The problem is a theoretical exercise in combinatorics and is entirely feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The requested quantities (group size and number of orbits) are uniquely determined by the given information. The structure is logical and clear.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a correct application of non-trivial concepts from abstract algebra (group theory) to a combinatorial problem. It is a standard but substantive exercise.\n7.  **Outside Scientific Verifiability**: The derivation is a mathematical proof that is fully verifiable.\n\n**Verdict**: The problem is valid. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe problem is divided into two parts: finding the size of the automorphism group of the bi-fan motif and then using this to count the number of non-isomorphic labelings.\n\n#### Part 1: The Automorphism Group of the Bi-Fan Motif\n\nLet the directed graph be $G = (V, E)$, where the set of vertices is $V = \\{A, B, C, D\\}$ and the set of directed edges is $E = \\{(A, C), (A, D), (B, C), (B, D)\\}$.\n\nAn automorphism of $G$ is a permutation $\\sigma$ of the vertex set $V$ such that for any two vertices $u, v \\in V$, an edge $(u, v) \\in E$ if and only if $(\\sigma(u), \\sigma(v)) \\in E$.\n\nWe first analyze the structure of the graph by examining the in-degrees and out-degrees of the nodes:\n- Node $A$: in-degree $d_{in}(A) = 0$, out-degree $d_{out}(A) = 2$.\n- Node $B$: in-degree $d_{in}(B) = 0$, out-degree $d_{out}(B) = 2$.\n- Node $C$: in-degree $d_{in}(C) = 2$, out-degree $d_{out}(C) = 0$.\n- Node $D$: in-degree $d_{in}(D) = 2$, out-degree $d_{out}(D) = 0$.\n\nAn automorphism must preserve the in-degree and out-degree of each vertex. Therefore, an automorphism $\\sigma$ must map the set of source nodes $\\{A, B\\}$ to itself and the set of sink nodes $\\{C, D\\}$ to itself.\n$$ \\sigma(\\{A, B\\}) = \\{A, B\\} \\quad \\text{and} \\quad \\sigma(\\{C, D\\}) = \\{C, D\\} $$\nThis partitions the set of possible permutations into those that permute nodes within these two sets. Let's enumerate the permutations that satisfy this condition and check if they are automorphisms.\n\n1.  **The identity permutation, $g_1$**: $\\sigma(v)=v$ for all $v \\in V$. In cycle notation, this is $(A)(B)(C)(D)$. This is trivially an automorphism.\n\n2.  **Swap $A$ and $B$, $g_2$**: $\\sigma(A)=B$, $\\sigma(B)=A$, $\\sigma(C)=C$, $\\sigma(D)=D$. In cycle notation, $(A \\ B)(C)(D)$.\n    The original edges are $\\{(A, C), (A, D), (B, C), (B, D)\\}$.\n    Applying the permutation gives the image set of edges:\n    $\\{(\\sigma(A), \\sigma(C)), (\\sigma(A), \\sigma(D)), (\\sigma(B), \\sigma(C)), (\\sigma(B), \\sigma(D))\\}$\n    $= \\{(B, C), (B, D), (A, C), (A, D)\\}$.\n    This is the same as the original set of edges $E$. Thus, $g_2$ is an automorphism.\n\n3.  **Swap $C$ and $D$, $g_3$**: $\\sigma(A)=A$, $\\sigma(B)=B$, $\\sigma(C)=D$, $\\sigma(D)=C$. In cycle notation, $(A)(B)(C \\ D)$.\n    Applying the permutation gives the image set of edges:\n    $\\{(\\sigma(A), \\sigma(C)), (\\sigma(A), \\sigma(D)), (\\sigma(B), \\sigma(C)), (\\sigma(B), \\sigma(D))\\}$\n    $= \\{(A, D), (A, C), (B, D), (B, C)\\}$.\n    This is the same as the original set of edges $E$. Thus, $g_3$ is an automorphism.\n\n4.  **Swap $A$ and $B$ and swap $C$ and $D$, $g_4$**: $\\sigma(A)=B$, $\\sigma(B)=A$, $\\sigma(C)=D$, $\\sigma(D)=C$. In cycle notation, $(A \\ B)(C \\ D)$.\n    Applying the permutation gives the image set of edges:\n    $\\{(\\sigma(A), \\sigma(C)), (\\sigma(A), \\sigma(D)), (\\sigma(B), \\sigma(C)), (\\sigma(B), \\sigma(D))\\}$\n    $= \\{(B, D), (B, C), (A, D), (A, C)\\}$.\n    This is the same as the original set of edges $E$. Thus, $g_4$ is an automorphism.\n\nThese are the only four permutations that preserve the partitioning of vertices based on their degree profiles. So, the automorphism group is $\\text{Aut}(G) = \\{g_1, g_2, g_3, g_4\\}$.\nThe size of the automorphism group is $|\\text{Aut}(G)| = 4$.\n\n#### Part 2: Number of Non-isomorphic Labeled Patterns\n\nWe use the Cauchy–Frobenius lemma (Burnside's lemma) to count the number of distinct labeled patterns. The number of patterns (orbits) $N$ is the average number of fixed points over the group:\n$$ N = \\frac{1}{|\\text{Aut}(G)|} \\sum_{g \\in \\text{Aut}(G)} |X^g| $$\nHere, $X$ is the set of all possible labelings of the $4$ nodes with $r$ labels. The total number of such labelings is $|X| = r^4$. $X^g$ is the set of labelings that are left unchanged (fixed) by the automorphism $g$.\n\nA labeling is fixed by a permutation $g$ if and only if all nodes within any given cycle of $g$ have the same label. If $g$ has $c(g)$ disjoint cycles, the number of labelings fixed by $g$ is $|X^g| = r^{c(g)}$, since we can choose one of the $r$ labels independently for each cycle.\n\nWe need to find the number of cycles, $c(g)$, for each of the four automorphisms.\n\n1.  **For $g_1 = (A)(B)(C)(D)$**:\n    The permutation consists of $4$ cycles of length $1$.\n    Number of cycles, $c(g_1) = 4$.\n    Number of fixed labelings, $|X^{g_1}| = r^{c(g_1)} = r^4$.\n\n2.  **For $g_2 = (A \\ B)(C)(D)$**:\n    The permutation consists of one cycle of length $2$ and two cycles of length $1$.\n    Number of cycles, $c(g_2) = 3$.\n    Number of fixed labelings, $|X^{g_2}| = r^{c(g_2)} = r^3$. (Nodes $A$ and $B$ must have the same label).\n\n3.  **For $g_3 = (A)(B)(C \\ D)$**:\n    The permutation consists of one cycle of length $2$ and two cycles of length $1$.\n    Number of cycles, $c(g_3) = 3$.\n    Number of fixed labelings, $|X^{g_3}| = r^{c(g_3)} = r^3$. (Nodes $C$ and $D$ must have the same label).\n\n4.  **For $g_4 = (A \\ B)(C \\ D)$**:\n    The permutation consists of two cycles of length $2$.\n    Number of cycles, $c(g_4) = 2$.\n    Number of fixed labelings, $|X^{g_4}| = r^{c(g_4)} = r^2$. (Nodes $A$ and $B$ must have the same label, and nodes $C$ and $D$ must have the same label).\n\nNow, we can apply Burnside's lemma:\n$$ N(r) = \\frac{1}{4} \\left( |X^{g_1}| + |X^{g_2}| + |X^{g_3}| + |X^{g_4}| \\right) $$\n$$ N(r) = \\frac{1}{4} (r^4 + r^3 + r^3 + r^2) $$\n$$ N(r) = \\frac{1}{4} (r^4 + 2r^3 + r^2) $$\nThis expression can be simplified by factoring the polynomial in $r$:\n$$ N(r) = \\frac{r^2(r^2 + 2r + 1)}{4} = \\frac{r^2(r+1)^2}{4} = \\left( \\frac{r(r+1)}{2} \\right)^2 $$\nThis is the closed-form analytic expression for the number of non-isomorphic labeled patterns.\n\nThe final result is a pair consisting of the automorphism group size, which is $4$, and the derived expression for the number of patterns, $N(r)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4 & \\frac{r^2(r+1)^2}{4}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While combinatorial analysis defines motif structures, probabilistic models are needed to capture the sequence variability observed in biological data. This practice  focuses on the Bayesian approach to learning Position Weight Matrices (PWMs), a cornerstone of sequence motif representation. You will derive the posterior distribution and the Maximum A Posteriori (MAP) estimator, gaining a practical understanding of how prior knowledge, expressed as pseudocounts, regularizes motif models learned from finite data.",
            "id": "3329501",
            "problem": "Consider a de novo motif discovery setting in deoxyribonucleic acid (DNA) where a motif of length $L$ is summarized by a Position Weight Matrix (PWM), also known as a position-specific multinomial distribution, with one multinomial parameter vector per column. For each column $j \\in \\{1,\\dots,L\\}$, let $\\boldsymbol{\\theta}_{j} = (\\theta_{j,A}, \\theta_{j,C}, \\theta_{j,G}, \\theta_{j,T})$ denote the base probabilities satisfying $\\theta_{j,b} \\ge 0$ for each base $b \\in \\{A,C,G,T\\}$ and $\\sum_{b} \\theta_{j,b} = 1$. Suppose we observe $M$ independent motif instances aligned to this PWM, and for each column $j$ we summarize the data by counts $\\boldsymbol{n}_{j} = (n_{j,A}, n_{j,C}, n_{j,G}, n_{j,T})$ with $n_{j,b} \\in \\{0,1,2,\\dots\\}$ and $\\sum_{b} n_{j,b} = N_{j}$, where $N_{j} = M$. Assume conditional independence of columns given the PWM, and a conjugate prior over each column $j$ given by a Dirichlet distribution with hyperparameters $\\boldsymbol{\\alpha}_{j} = (\\alpha_{j,A}, \\alpha_{j,C}, \\alpha_{j,G}, \\alpha_{j,T})$ where $\\alpha_{j,b} > 0$, and the columns are a priori independent.\n\nUsing only Bayes’ theorem, the multinomial likelihood for each column, and the definition of the Dirichlet distribution, perform the following tasks:\n\n- Derive the posterior distribution $p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$ for an arbitrary column $j$ in closed form and state how it factorizes across columns. Your derivation must start from the likelihood-prior product and arrive at a properly normalized posterior family.\n\n- Derive the Maximum A Posteriori (MAP) estimator $\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}}$ for each base $b$ in column $j$ by maximizing the posterior density subject to the simplex constraint. Explicitly state the conditions under which the interior solution (i.e., strictly positive coordinates) holds.\n\n- Interpret the Dirichlet hyperparameters in terms of pseudocounts and prior strength for both the posterior mean and the MAP estimator. Provide the expressions that make this interpretation precise.\n\n- Now consider a specific column $j = 2$ with observed counts $\\boldsymbol{n}_{2} = (n_{2,A}, n_{2,C}, n_{2,G}, n_{2,T}) = (9, 5, 3, 3)$ and a symmetric Dirichlet prior with $\\boldsymbol{\\alpha}_{2} = (2, 2, 2, 2)$. Under the interior-solution conditions, compute the MAP estimate $\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}}$ for base $A$ at column $j=2$. Round your answer to $4$ significant figures. The final answer should be a single real number with no units.",
            "solution": "The problem asks for a Bayesian analysis of a Position Weight Matrix (PWM) for DNA motifs. We are given a multinomial likelihood for the base counts in each column of an alignment of motif instances, and a conjugate Dirichlet prior for the probability parameters of the PWM. The problem is well-posed, scientifically sound, and contains all necessary information for a complete solution.\n\n### Part 1: Posterior Distribution Derivation\n\nWe are asked to derive the posterior distribution $p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$ for the parameters $\\boldsymbol{\\theta}_{j}$ of an arbitrary column $j$, given the observed base counts $\\boldsymbol{n}_{j}$. We apply Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) \\propto p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j}) p(\\boldsymbol{\\theta}_{j})$$\n\nThe likelihood of observing the counts $\\boldsymbol{n}_{j} = (n_{j,A}, n_{j,C}, n_{j,G}, n_{j,T})$ in a column with a total of $N_{j} = \\sum_{b} n_{j,b}$ observations, given the probability vector $\\boldsymbol{\\theta}_{j}$, follows a multinomial distribution:\n$$p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j}) = \\frac{N_{j}!}{\\prod_{b} n_{j,b}!} \\prod_{b \\in \\{A,C,G,T\\}} \\theta_{j,b}^{n_{j,b}}$$\nwhere $b$ iterates over the four DNA bases.\n\nThe prior distribution for the parameters $\\boldsymbol{\\theta}_{j}$ is a Dirichlet distribution with hyperparameter vector $\\boldsymbol{\\alpha}_{j} = (\\alpha_{j,A}, \\alpha_{j,C}, \\alpha_{j,G}, \\alpha_{j,T})$:\n$$p(\\boldsymbol{\\theta}_{j}) = \\frac{\\Gamma(\\sum_{b} \\alpha_{j,b})}{\\prod_{b} \\Gamma(\\alpha_{j,b})} \\prod_{b \\in \\{A,C,G,T\\}} \\theta_{j,b}^{\\alpha_{j,b}-1}$$\n\nNow, we compute the posterior by multiplying the likelihood and the prior. We can ignore any terms that do not depend on $\\boldsymbol{\\theta}_{j}$, as they are absorbed into the normalization constant of the posterior distribution.\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) \\propto \\left( \\prod_{b} \\theta_{j,b}^{n_{j,b}} \\right) \\left( \\prod_{b} \\theta_{j,b}^{\\alpha_{j,b}-1} \\right)$$\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) \\propto \\prod_{b} \\theta_{j,b}^{n_{j,b} + \\alpha_{j,b} - 1}$$\nThis expression is the kernel of a Dirichlet distribution. By inspection, we can identify the parameters of the posterior distribution. The posterior is a Dirichlet distribution with updated hyperparameters $\\boldsymbol{\\alpha}_{j}' = \\boldsymbol{\\alpha}_{j} + \\boldsymbol{n}_{j}$. The full, normalized posterior distribution is:\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) = \\mathrm{Dirichlet}(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{\\alpha}_{j} + \\boldsymbol{n}_{j}) = \\frac{\\Gamma\\left(\\sum_{b} (\\alpha_{j,b} + n_{j,b})\\right)}{\\prod_{b} \\Gamma(\\alpha_{j,b} + n_{j,b})} \\prod_{b} \\theta_{j,b}^{\\alpha_{j,b} + n_{j,b} - 1}$$\nThis demonstrates the conjugacy of the Dirichlet prior with the multinomial likelihood.\n\nThe problem states that columns are conditionally independent given the PWM and are a priori independent. This implies that the joint likelihood and joint prior factorize over the columns $j=1, \\dots, L$:\n$$p(\\{\\boldsymbol{n}_{j}\\}_{j=1}^L \\mid \\{\\boldsymbol{\\theta}_{j}\\}_{j=1}^L) = \\prod_{j=1}^{L} p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j})$$\n$$p(\\{\\boldsymbol{\\theta}_{j}\\}_{j=1}^L) = \\prod_{j=1}^{L} p(\\boldsymbol{\\theta}_{j})$$\nConsequently, the joint posterior distribution for all parameters also factorizes across the columns:\n$$p(\\{\\boldsymbol{\\theta}_{j}\\}_{j=1}^L \\mid \\{\\boldsymbol{n}_{j}\\}_{j=1}^L) \\propto \\prod_{j=1}^{L} p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j}) p(\\boldsymbol{\\theta}_{j}) = \\prod_{j=1}^{L} p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$$\nThus, the posterior inference can be performed independently for each column of the PWM.\n\n### Part 2: Maximum A Posteriori (MAP) Estimator Derivation\n\nThe MAP estimator, $\\widehat{\\boldsymbol{\\theta}}_{j}^{\\mathrm{MAP}}$, is the value of $\\boldsymbol{\\theta}_{j}$ that maximizes the posterior probability density, subject to the simplex constraint $\\sum_{b} \\theta_{j,b} = 1$. Maximizing $p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$ is equivalent to maximizing its logarithm, $\\ln p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$.\n$$\\ln p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) = \\ln(C) + \\sum_{b} (\\alpha_{j,b} + n_{j,b} - 1) \\ln \\theta_{j,b}$$\nwhere $C$ is the normalization constant. We formulate the Lagrangian to solve this constrained optimization problem:\n$$\\mathcal{L}(\\boldsymbol{\\theta}_{j}, \\lambda) = \\sum_{b} (\\alpha_{j,b} + n_{j,b} - 1) \\ln \\theta_{j,b} - \\lambda \\left(\\sum_{b} \\theta_{j,b} - 1\\right)$$\nTaking the partial derivative with respect to each $\\theta_{j,b}$ and setting it to zero yields:\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{j,b}} = \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\theta_{j,b}} - \\lambda = 0 \\implies \\theta_{j,b} = \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\lambda}$$\nTo find the Lagrange multiplier $\\lambda$, we enforce the constraint $\\sum_{b} \\theta_{j,b} = 1$:\n$$\\sum_{b} \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\lambda} = 1 \\implies \\lambda = \\sum_{b} (\\alpha_{j,b} + n_{j,b} - 1)$$\nLet $\\alpha_{j,0} = \\sum_{b} \\alpha_{j,b}$ and $N_{j} = \\sum_{b} n_{j,b}$. The number of bases is $4$. Then:\n$$\\lambda = \\left(\\sum_{b} \\alpha_{j,b}\\right) + \\left(\\sum_{b} n_{j,b}\\right) - \\sum_{b} 1 = \\alpha_{j,0} + N_{j} - 4$$\nSubstituting $\\lambda$ back, we obtain the MAP estimator for each component:\n$$\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}} = \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\alpha_{j,0} + N_{j} - 4}$$\nAn interior solution (i.e., $\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}} > 0$ for all $b$) holds if the numerator is positive for all $b$, assuming the denominator is positive (which is typical). The condition is $\\alpha_{j,b} + n_{j,b} - 1 > 0$ for all $b \\in \\{A,C,G,T\\}$. If we require this to hold for any possible count data (including $n_{j,b}=0$), the condition simplifies to $\\alpha_{j,b} > 1$ for all $b$.\n\n### Part 3: Interpretation of Hyperparameters\n\nThe hyperparameters $\\boldsymbol{\\alpha}_{j}$ are often interpreted as *pseudocounts*, representing prior information as if we had already observed a certain number of bases.\n\nFor the **posterior mean**, which is the expected value of $\\boldsymbol{\\theta}_j$ under the posterior distribution, the formula is:\n$$\\widehat{\\theta}_{j,b}^{\\text{Mean}} = E[\\theta_{j,b} \\mid \\boldsymbol{n}_j] = \\frac{\\alpha_{j,b} + n_{j,b}}{\\sum_{k} (\\alpha_{j,k} + n_{j,k})} = \\frac{\\alpha_{j,b} + n_{j,b}}{\\alpha_{j,0} + N_{j}}$$\nIn this expression, each hyperparameter $\\alpha_{j,b}$ is directly added to the corresponding observed count $n_{j,b}$. Thus, $\\alpha_{j,b}$ literally acts as a pseudocount for base $b$. The total number of pseudocounts, $\\alpha_{j,0} = \\sum_b \\alpha_{j,b}$, determines the overall strength of the prior, as it is added to the total number of real observations $N_j$.\n\nFor the **MAP estimator**, the formula is:\n$$\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}} = \\frac{(\\alpha_{j,b} - 1) + n_{j,b}}{(\\alpha_{j,0} - 4) + N_{j}}$$\nHere, the effective pseudocount for base $b$ is $\\alpha_{j,b}-1$. The total prior strength influencing the MAP estimate is $\\alpha_{j,0}-4$. This interpretation highlights why the condition $\\alpha_{j,b} > 1$ is significant for ensuring positive pseudocounts, preventing the mode of the distribution from being driven to the boundaries of the simplex (i.e., $\\theta_{j,b}=0$) in the absence of observed counts for base $b$.\n\n### Part 4: Specific Calculation\n\nWe are given the following for column $j=2$:\n- Observed counts: $\\boldsymbol{n}_{2} = (9, 5, 3, 3)$.\n- Prior hyperparameters: $\\boldsymbol{\\alpha}_{2} = (2, 2, 2, 2)$.\n\nFirst, we calculate the total observed counts and total prior strength:\n- $N_{2} = \\sum_{b} n_{2,b} = 9 + 5 + 3 + 3 = 20$.\n- $\\alpha_{2,0} = \\sum_{b} \\alpha_{2,b} = 2 + 2 + 2 + 2 = 8$.\n\nWe need to compute the MAP estimate for base $A$, $\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}}$. The problem states to assume the interior-solution conditions hold, which we can verify: $\\alpha_{2,b} + n_{2,b} - 1 > 0$ for all $b$. For $b=A$, we have $2+9-1 = 10 > 0$, and similarly for the other bases.\n\nUsing the MAP estimator formula derived in Part 2 for $j=2$ and $b=A$:\n$$\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}} = \\frac{\\alpha_{2,A} + n_{2,A} - 1}{\\alpha_{2,0} + N_{2} - 4}$$\nSubstituting the given values:\n$$\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}} = \\frac{2 + 9 - 1}{8 + 20 - 4} = \\frac{10}{24} = \\frac{5}{12}$$\nConverting this fraction to a decimal gives approximately $0.416666...$. Rounding to $4$ significant figures, we get $0.4167$.",
            "answer": "$$\\boxed{0.4167}$$"
        },
        {
            "introduction": "A learned motif model is most powerful when used to scan new sequences for potential binding sites, a task that demands statistical rigor. This problem  moves from modeling to application, asking you to enhance the classic log-odds scoring method by incorporating position-specific prior information, such as from epigenomic data. You will derive how to adjust the decision threshold to maintain a constant false positive rate, a critical skill for developing sensitive and specific motif finders.",
            "id": "3329522",
            "problem": "You are scanning a genomic sequence for a transcription factor binding motif using a Position Probability Matrix (PPM) and a single-nucleotide background model. Let the motif length be $L$ and the position probability matrix be $P_{i}(a)$, where $i \\in \\{1,\\dots,L\\}$ indexes motif positions and $a \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ denotes nucleotides. The background distribution is $B(a)$ with independent positions. For each genomic position $j$, consider the window $X_{j} = (x_{j}, x_{j+1}, \\dots, x_{j+L-1})$ of observed nucleotides.\n\nThe classical likelihood-ratio log-odds score for window $X_{j}$ is defined by comparing the alternative hypothesis $H_{1}$ (the window is generated by the motif model) to the null hypothesis $H_{0}$ (the window is generated by the background model), under the assumption of independent positions given the model. Under a fixed decision threshold $t_{\\alpha}$, the false positive rate (Type I error) per position is calibrated to a target level $\\alpha$ by requiring that the probability under $H_{0}$ that the score exceeds $t_{\\alpha}$ equals $\\alpha$.\n\nNow suppose you incorporate position-specific prior probabilities of motif occurrence across the genome, derived from epigenomic covariates, such that the prior probability of $H_{1}$ at genomic position $j$ is $\\pi_{j} \\in (0,1)$. Using Bayes’ theorem for posterior odds under $H_{1}$ versus $H_{0}$ for window $X_{j}$, the posterior log-odds score becomes the sum of the classical log-likelihood ratio and a position-specific additive term determined by $\\pi_{j}$.\n\nAssume the decision rule is to call a motif at position $j$ when the posterior log-odds score at $j$ exceeds a threshold $\\tilde{t}_{j}$. Derive, in closed form, the value of $\\tilde{t}_{j}$ in terms of $t_{\\alpha}$ and $\\pi_{j}$ that guarantees the false positive rate per position remains equal to $\\alpha$ under $H_{0}$ for every $j$.\n\nYour final answer must be a single closed-form analytic expression for $\\tilde{t}_{j}$ in terms of $t_{\\alpha}$ and $\\pi_{j}$. No numerical evaluation is required and no units are involved. Do not provide inequalities or equations; provide only the expression itself.",
            "solution": "The problem requires the derivation of a new decision threshold, $\\tilde{t}_j$, for a posterior log-odds score, such that the false positive rate per position remains at a constant level $\\alpha$. The derivation proceeds by formalizing the scores and the statistical conditions.\n\nFirst, let us define the classical log-likelihood ratio score for a genomic window $X_j = (x_{j}, x_{j+1}, \\dots, x_{j+L-1})$. The alternative hypothesis, $H_1$, is that the window is generated by the motif model described by the position probability matrix (PPM) $P_i(a)$. The null hypothesis, $H_0$, is that the window is generated by the background model $B(a)$. Assuming independence of nucleotide positions within the window given the model, the likelihoods are:\n$P(X_j | H_1) = \\prod_{k=1}^{L} P_k(x_{j+k-1})$\n$P(X_j | H_0) = \\prod_{k=1}^{L} B(x_{j+k-1})$\n\nThe likelihood ratio is $LR(X_j) = \\frac{P(X_j | H_1)}{P(X_j | H_0)}$. The classical log-likelihood ratio score, which we denote as $S(X_j)$, is the natural logarithm of this ratio:\n$$S(X_j) = \\ln\\left( \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\right) = \\sum_{k=1}^{L} \\ln\\left( \\frac{P_k(x_{j+k-1})}{B(x_{j+k-1})} \\right)$$\n\nA motif is called at position $j$ if this score exceeds a threshold $t_{\\alpha}$. The false positive rate, $\\alpha$, is the probability of this event occurring when the null hypothesis $H_0$ is true. This is stated as:\n$$P(S(X_j) > t_{\\alpha} | H_0) = \\alpha$$\nThis equation defines the relationship between the threshold $t_{\\alpha}$ and the desired false positive rate $\\alpha$.\n\nNext, we incorporate the position-specific prior probability, $\\pi_j$, that a motif occurs at position $j$. The prior probability for $H_1$ is $P(H_1) = \\pi_j$, and for $H_0$ is $P(H_0) = 1 - \\pi_j$. Using Bayes' theorem, the posterior odds of $H_1$ versus $H_0$ given the observation $X_j$ are:\n$$\\frac{P(H_1 | X_j)}{P(H_0 | X_j)} = \\frac{P(X_j | H_1) P(H_1)}{P(X_j | H_0) P(H_0)} = \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\cdot \\frac{\\pi_j}{1 - \\pi_j}$$\nThe posterior log-odds score, which we denote as $\\tilde{S}(X_j)$, is the natural logarithm of the posterior odds:\n$$\\tilde{S}(X_j) = \\ln\\left( \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\cdot \\frac{\\pi_j}{1 - \\pi_j} \\right) = \\ln\\left( \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\right) + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\nSubstituting the definition of $S(X_j)$, we find the relationship between the posterior and classical scores:\n$$\\tilde{S}(X_j) = S(X_j) + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\nThe term $\\ln( \\frac{\\pi_j}{1 - \\pi_j} )$ is the log-prior-odds for position $j$.\n\nThe new decision rule is to call a motif at position $j$ if the posterior log-odds score $\\tilde{S}(X_j)$ exceeds a new, position-specific threshold $\\tilde{t}_j$. The core requirement of the problem is to choose $\\tilde{t}_j$ such that the false positive rate remains $\\alpha$ for every position $j$. This condition is expressed as:\n$$P(\\tilde{S}(X_j) > \\tilde{t}_j | H_0) = \\alpha$$\n\nTo find $\\tilde{t}_j$, we substitute the expression for $\\tilde{S}(X_j)$ into this condition:\n$$P\\left( S(X_j) + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right) > \\tilde{t}_j \\;\\middle|\\; H_0 \\right) = \\alpha$$\nThe term $\\ln( \\frac{\\pi_j}{1 - \\pi_j} )$ is a constant for a given position $j$. We can rearrange the inequality inside the probability statement to isolate the random variable $S(X_j)$:\n$$P\\left( S(X_j) > \\tilde{t}_j - \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right) \\;\\middle|\\; H_0 \\right) = \\alpha$$\n\nWe now have two statements that must hold true simultaneously:\n$1.$ $P(S(X_j) > t_{\\alpha} | H_0) = \\alpha$\n$2.$ $P\\left( S(X_j) > \\tilde{t}_j - \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right) \\;\\middle|\\; H_0 \\right) = \\alpha$\n\nBoth expressions describe the probability that the same random variable, $S(X_j)$ (whose distribution is determined by $H_0$), exceeds a certain threshold. For these probabilities to be equal to the same value $\\alpha$, the thresholds must be identical. Therefore, we can equate the arguments of the score function:\n$$t_{\\alpha} = \\tilde{t}_j - \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\n\nFinally, we solve for the desired threshold $\\tilde{t}_j$ by rearranging the equation:\n$$\\tilde{t}_j = t_{\\alpha} + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\nThis is the closed-form expression for the new threshold $\\tilde{t}_j$ that guarantees the false positive rate per position remains equal to $\\alpha$ under the null hypothesis $H_0$. It shows that the new threshold is the classical threshold adjusted by the log-prior-odds at that position.",
            "answer": "$$\\boxed{t_{\\alpha} + \\ln\\left(\\frac{\\pi_j}{1 - \\pi_j}\\right)}$$"
        }
    ]
}