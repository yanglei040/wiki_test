{
    "hands_on_practices": [
        {
            "introduction": "A fundamental task in motif discovery is to build a robust statistical model from a small set of aligned binding site examples. This exercise guides you through the Bayesian approach to learning a Position Weight Matrix (PWM), a cornerstone model for sequence motifs. By applying Bayes' theorem with a conjugate Dirichlet prior, you will derive how to formally update your belief about a motif's nucleotide probabilities, a process that is crucial for regularizing models and avoiding overfitting when data is scarce. ",
            "id": "3329501",
            "problem": "Consider a de novo motif discovery setting in deoxyribonucleic acid (DNA) where a motif of length $L$ is summarized by a Position Weight Matrix (PWM), also known as a position-specific multinomial distribution, with one multinomial parameter vector per column. For each column $j \\in \\{1,\\dots,L\\}$, let $\\boldsymbol{\\theta}_{j} = (\\theta_{j,A}, \\theta_{j,C}, \\theta_{j,G}, \\theta_{j,T})$ denote the base probabilities satisfying $\\theta_{j,b} \\ge 0$ for each base $b \\in \\{A,C,G,T\\}$ and $\\sum_{b} \\theta_{j,b} = 1$. Suppose we observe $M$ independent motif instances aligned to this PWM, and for each column $j$ we summarize the data by counts $\\boldsymbol{n}_{j} = (n_{j,A}, n_{j,C}, n_{j,G}, n_{j,T})$ with $n_{j,b} \\in \\{0,1,2,\\dots\\}$ and $\\sum_{b} n_{j,b} = N_{j}$, where $N_{j} = M$. Assume conditional independence of columns given the PWM, and a conjugate prior over each column $j$ given by a Dirichlet distribution with hyperparameters $\\boldsymbol{\\alpha}_{j} = (\\alpha_{j,A}, \\alpha_{j,C}, \\alpha_{j,G}, \\alpha_{j,T})$ where $\\alpha_{j,b} > 0$, and the columns are a priori independent.\n\nUsing only Bayesâ€™ theorem, the multinomial likelihood for each column, and the definition of the Dirichlet distribution, perform the following tasks:\n\n- Derive the posterior distribution $p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$ for an arbitrary column $j$ in closed form and state how it factorizes across columns. Your derivation must start from the likelihood-prior product and arrive at a properly normalized posterior family.\n\n- Derive the Maximum A Posteriori (MAP) estimator $\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}}$ for each base $b$ in column $j$ by maximizing the posterior density subject to the simplex constraint. Explicitly state the conditions under which the interior solution (i.e., strictly positive coordinates) holds.\n\n- Interpret the Dirichlet hyperparameters in terms of pseudocounts and prior strength for both the posterior mean and the MAP estimator. Provide the expressions that make this interpretation precise.\n\n- Now consider a specific column $j = 2$ with observed counts $\\boldsymbol{n}_{2} = (n_{2,A}, n_{2,C}, n_{2,G}, n_{2,T}) = (9, 5, 3, 3)$ and a symmetric Dirichlet prior with $\\boldsymbol{\\alpha}_{2} = (2, 2, 2, 2)$. Under the interior-solution conditions, compute the MAP estimate $\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}}$ for base $A$ at column $j=2$. Round your answer to $4$ significant figures. The final answer should be a single real number with no units.",
            "solution": "The problem asks for a Bayesian analysis of a Position Weight Matrix (PWM) for DNA motifs. We are given a multinomial likelihood for the base counts in each column of an alignment of motif instances, and a conjugate Dirichlet prior for the probability parameters of the PWM. The problem is well-posed, scientifically sound, and contains all necessary information for a complete solution.\n\n### Part 1: Posterior Distribution Derivation\n\nWe are asked to derive the posterior distribution $p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$ for the parameters $\\boldsymbol{\\theta}_{j}$ of an arbitrary column $j$, given the observed base counts $\\boldsymbol{n}_{j}$. We apply Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) \\propto p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j}) p(\\boldsymbol{\\theta}_{j})$$\n\nThe likelihood of observing the counts $\\boldsymbol{n}_{j} = (n_{j,A}, n_{j,C}, n_{j,G}, n_{j,T})$ in a column with a total of $N_{j} = \\sum_{b} n_{j,b}$ observations, given the probability vector $\\boldsymbol{\\theta}_{j}$, follows a multinomial distribution:\n$$p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j}) = \\frac{N_{j}!}{\\prod_{b} n_{j,b}!} \\prod_{b \\in \\{A,C,G,T\\}} \\theta_{j,b}^{n_{j,b}}$$\nwhere $b$ iterates over the four DNA bases.\n\nThe prior distribution for the parameters $\\boldsymbol{\\theta}_{j}$ is a Dirichlet distribution with hyperparameter vector $\\boldsymbol{\\alpha}_{j} = (\\alpha_{j,A}, \\alpha_{j,C}, \\alpha_{j,G}, \\alpha_{j,T})$:\n$$p(\\boldsymbol{\\theta}_{j}) = \\frac{\\Gamma(\\sum_{b} \\alpha_{j,b})}{\\prod_{b} \\Gamma(\\alpha_{j,b})} \\prod_{b \\in \\{A,C,G,T\\}} \\theta_{j,b}^{\\alpha_{j,b}-1}$$\n\nNow, we compute the posterior by multiplying the likelihood and the prior. We can ignore any terms that do not depend on $\\boldsymbol{\\theta}_{j}$, as they are absorbed into the normalization constant of the posterior distribution.\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) \\propto \\left( \\prod_{b} \\theta_{j,b}^{n_{j,b}} \\right) \\left( \\prod_{b} \\theta_{j,b}^{\\alpha_{j,b}-1} \\right)$$\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) \\propto \\prod_{b} \\theta_{j,b}^{n_{j,b} + \\alpha_{j,b} - 1}$$\nThis expression is the kernel of a Dirichlet distribution. By inspection, we can identify the parameters of the posterior distribution. The posterior is a Dirichlet distribution with updated hyperparameters $\\boldsymbol{\\alpha}_{j}' = \\boldsymbol{\\alpha}_{j} + \\boldsymbol{n}_{j}$. The full, normalized posterior distribution is:\n$$p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) = \\mathrm{Dirichlet}(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{\\alpha}_{j} + \\boldsymbol{n}_{j}) = \\frac{\\Gamma\\left(\\sum_{b} (\\alpha_{j,b} + n_{j,b})\\right)}{\\prod_{b} \\Gamma(\\alpha_{j,b} + n_{j,b})} \\prod_{b} \\theta_{j,b}^{\\alpha_{j,b} + n_{j,b} - 1}$$\nThis demonstrates the conjugacy of the Dirichlet prior with the multinomial likelihood.\n\nThe problem states that columns are conditionally independent given the PWM and are a priori independent. This implies that the joint likelihood and joint prior factorize over the columns $j=1, \\dots, L$:\n$$p(\\{\\boldsymbol{n}_{j}\\}_{j=1}^L \\mid \\{\\boldsymbol{\\theta}_{j}\\}_{j=1}^L) = \\prod_{j=1}^{L} p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j})$$\n$$p(\\{\\boldsymbol{\\theta}_{j}\\}_{j=1}^L) = \\prod_{j=1}^{L} p(\\boldsymbol{\\theta}_{j})$$\nConsequently, the joint posterior distribution for all parameters also factorizes across the columns:\n$$p(\\{\\boldsymbol{\\theta}_{j}\\}_{j=1}^L \\mid \\{\\boldsymbol{n}_{j}\\}_{j=1}^L) \\propto \\prod_{j=1}^{L} p(\\boldsymbol{n}_{j} \\mid \\boldsymbol{\\theta}_{j}) p(\\boldsymbol{\\theta}_{j}) = \\prod_{j=1}^{L} p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$$\nThus, the posterior inference can be performed independently for each column of the PWM.\n\n### Part 2: Maximum A Posteriori (MAP) Estimator Derivation\n\nThe MAP estimator, $\\widehat{\\boldsymbol{\\theta}}_{j}^{\\mathrm{MAP}}$, is the value of $\\boldsymbol{\\theta}_{j}$ that maximizes the posterior probability density, subject to the simplex constraint $\\sum_{b} \\theta_{j,b} = 1$. Maximizing $p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$ is equivalent to maximizing its logarithm, $\\ln p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j})$.\n$$\\ln p(\\boldsymbol{\\theta}_{j} \\mid \\boldsymbol{n}_{j}) = \\ln(C) + \\sum_{b} (\\alpha_{j,b} + n_{j,b} - 1) \\ln \\theta_{j,b}$$\nwhere $C$ is the normalization constant. We formulate the Lagrangian to solve this constrained optimization problem:\n$$\\mathcal{L}(\\boldsymbol{\\theta}_{j}, \\lambda) = \\sum_{b} (\\alpha_{j,b} + n_{j,b} - 1) \\ln \\theta_{j,b} - \\lambda \\left(\\sum_{b} \\theta_{j,b} - 1\\right)$$\nTaking the partial derivative with respect to each $\\theta_{j,b}$ and setting it to zero yields:\n$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{j,b}} = \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\theta_{j,b}} - \\lambda = 0 \\implies \\theta_{j,b} = \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\lambda}$$\nTo find the Lagrange multiplier $\\lambda$, we enforce the constraint $\\sum_{b} \\theta_{j,b} = 1$:\n$$\\sum_{b} \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\lambda} = 1 \\implies \\lambda = \\sum_{b} (\\alpha_{j,b} + n_{j,b} - 1)$$\nLet $\\alpha_{j,0} = \\sum_{b} \\alpha_{j,b}$ and $N_{j} = \\sum_{b} n_{j,b}$. The number of bases is $4$. Then:\n$$\\lambda = \\left(\\sum_{b} \\alpha_{j,b}\\right) + \\left(\\sum_{b} n_{j,b}\\right) - \\sum_{b} 1 = \\alpha_{j,0} + N_{j} - 4$$\nSubstituting $\\lambda$ back, we obtain the MAP estimator for each component:\n$$\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}} = \\frac{\\alpha_{j,b} + n_{j,b} - 1}{\\alpha_{j,0} + N_{j} - 4}$$\nAn interior solution (i.e., $\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}} > 0$ for all $b$) holds if the numerator is positive for all $b$, assuming the denominator is positive (which is typical). The condition is $\\alpha_{j,b} + n_{j,b} - 1 > 0$ for all $b \\in \\{A,C,G,T\\}$. If we require this to hold for any possible count data (including $n_{j,b}=0$), the condition simplifies to $\\alpha_{j,b} > 1$ for all $b$.\n\n### Part 3: Interpretation of Hyperparameters\n\nThe hyperparameters $\\boldsymbol{\\alpha}_{j}$ are often interpreted as *pseudocounts*, representing prior information as if we had already observed a certain number of bases.\n\nFor the **posterior mean**, which is the expected value of $\\boldsymbol{\\theta}_j$ under the posterior distribution, the formula is:\n$$\\widehat{\\theta}_{j,b}^{\\text{Mean}} = E[\\theta_{j,b} \\mid \\boldsymbol{n}_j] = \\frac{\\alpha_{j,b} + n_{j,b}}{\\sum_{k} (\\alpha_{j,k} + n_{j,k})} = \\frac{\\alpha_{j,b} + n_{j,b}}{\\alpha_{j,0} + N_{j}}$$\nIn this expression, each hyperparameter $\\alpha_{j,b}$ is directly added to the corresponding observed count $n_{j,b}$. Thus, $\\alpha_{j,b}$ literally acts as a pseudocount for base $b$. The total number of pseudocounts, $\\alpha_{j,0} = \\sum_b \\alpha_{j,b}$, determines the overall strength of the prior, as it is added to the total number of real observations $N_j$.\n\nFor the **MAP estimator**, the formula is:\n$$\\widehat{\\theta}_{j,b}^{\\mathrm{MAP}} = \\frac{(\\alpha_{j,b} - 1) + n_{j,b}}{(\\alpha_{j,0} - 4) + N_{j}}$$\nHere, the effective pseudocount for base $b$ is $\\alpha_{j,b}-1$. The total prior strength influencing the MAP estimate is $\\alpha_{j,0}-4$. This interpretation highlights why the condition $\\alpha_{j,b} > 1$ is significant for ensuring positive pseudocounts, preventing the mode of the distribution from being driven to the boundaries of the simplex (i.e., $\\theta_{j,b}=0$) in the absence of observed counts for base $b$.\n\n### Part 4: Specific Calculation\n\nWe are given the following for column $j=2$:\n- Observed counts: $\\boldsymbol{n}_{2} = (9, 5, 3, 3)$.\n- Prior hyperparameters: $\\boldsymbol{\\alpha}_{2} = (2, 2, 2, 2)$.\n\nFirst, we calculate the total observed counts and total prior strength:\n- $N_{2} = \\sum_{b} n_{2,b} = 9 + 5 + 3 + 3 = 20$.\n- $\\alpha_{2,0} = \\sum_{b} \\alpha_{2,b} = 2 + 2 + 2 + 2 = 8$.\n\nWe need to compute the MAP estimate for base $A$, $\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}}$. The problem states to assume the interior-solution conditions hold, which we can verify: $\\alpha_{2,b} + n_{2,b} - 1 > 0$ for all $b$. For $b=A$, we have $2+9-1 = 10 > 0$, and similarly for the other bases.\n\nUsing the MAP estimator formula derived in Part 2 for $j=2$ and $b=A$:\n$$\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}} = \\frac{\\alpha_{2,A} + n_{2,A} - 1}{\\alpha_{2,0} + N_{2} - 4}$$\nSubstituting the given values:\n$$\\widehat{\\theta}_{2,A}^{\\mathrm{MAP}} = \\frac{2 + 9 - 1}{8 + 20 - 4} = \\frac{10}{24} = \\frac{5}{12}$$\nConverting this fraction to a decimal gives approximately $0.416666...$. Rounding to $4$ significant figures, we get $0.4167$.",
            "answer": "$$\\boxed{0.4167}$$"
        },
        {
            "introduction": "Once a motif model is learned, it is used to scan genomes for new potential binding sites. While classical methods rely on a log-likelihood ratio score, modern approaches can achieve greater accuracy by incorporating external evidence, such as chromatin accessibility, as a position-specific prior probability. This practice challenges you to integrate such prior information into a formal Bayesian decision framework, deriving how the detection threshold must be adjusted to maintain a desired statistical rigor, specifically a constant false positive rate. ",
            "id": "3329522",
            "problem": "You are scanning a genomic sequence for a transcription factor binding motif using a Position Probability Matrix (PPM) and a single-nucleotide background model. Let the motif length be $L$ and the position probability matrix be $P_{i}(a)$, where $i \\in \\{1,\\dots,L\\}$ indexes motif positions and $a \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ denotes nucleotides. The background distribution is $B(a)$ with independent positions. For each genomic position $j$, consider the window $X_{j} = (x_{j}, x_{j+1}, \\dots, x_{j+L-1})$ of observed nucleotides.\n\nThe classical likelihood-ratio log-odds score for window $X_{j}$ is defined by comparing the alternative hypothesis $H_{1}$ (the window is generated by the motif model) to the null hypothesis $H_{0}$ (the window is generated by the background model), under the assumption of independent positions given the model. Under a fixed decision threshold $t_{\\alpha}$, the false positive rate (Type I error) per position is calibrated to a target level $\\alpha$ by requiring that the probability under $H_{0}$ that the score exceeds $t_{\\alpha}$ equals $\\alpha$.\n\nNow suppose you incorporate position-specific prior probabilities of motif occurrence across the genome, derived from epigenomic covariates, such that the prior probability of $H_{1}$ at genomic position $j$ is $\\pi_{j} \\in (0,1)$. Using Bayesâ€™ theorem for posterior odds under $H_{1}$ versus $H_{0}$ for window $X_{j}$, the posterior log-odds score becomes the sum of the classical log-likelihood ratio and a position-specific additive term determined by $\\pi_{j}$.\n\nAssume the decision rule is to call a motif at position $j$ when the posterior log-odds score at $j$ exceeds a threshold $\\tilde{t}_{j}$. Derive, in closed form, the value of $\\tilde{t}_{j}$ in terms of $t_{\\alpha}$ and $\\pi_{j}$ that guarantees the false positive rate per position remains equal to $\\alpha$ under $H_{0}$ for every $j$.\n\nYour final answer must be a single closed-form analytic expression for $\\tilde{t}_{j}$ in terms of $t_{\\alpha}$ and $\\pi_{j}$. No numerical evaluation is required and no units are involved. Do not provide inequalities or equations; provide only the expression itself.",
            "solution": "The problem requires the derivation of a new decision threshold, $\\tilde{t}_j$, for a posterior log-odds score, such that the false positive rate per position remains at a constant level $\\alpha$. The derivation proceeds by formalizing the scores and the statistical conditions.\n\nFirst, let us define the classical log-likelihood ratio score for a genomic window $X_j = (x_{j}, x_{j+1}, \\dots, x_{j+L-1})$. The alternative hypothesis, $H_1$, is that the window is generated by the motif model described by the position probability matrix (PPM) $P_i(a)$. The null hypothesis, $H_0$, is that the window is generated by the background model $B(a)$. Assuming independence of nucleotide positions within the window given the model, the likelihoods are:\n$P(X_j | H_1) = \\prod_{k=1}^{L} P_k(x_{j+k-1})$\n$P(X_j | H_0) = \\prod_{k=1}^{L} B(x_{j+k-1})$\n\nThe likelihood ratio is $LR(X_j) = \\frac{P(X_j | H_1)}{P(X_j | H_0)}$. The classical log-likelihood ratio score, which we denote as $S(X_j)$, is the natural logarithm of this ratio:\n$$S(X_j) = \\ln\\left( \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\right) = \\sum_{k=1}^{L} \\ln\\left( \\frac{P_k(x_{j+k-1})}{B(x_{j+k-1})} \\right)$$\n\nA motif is called at position $j$ if this score exceeds a threshold $t_{\\alpha}$. The false positive rate, $\\alpha$, is the probability of this event occurring when the null hypothesis $H_0$ is true. This is stated as:\n$$P(S(X_j) > t_{\\alpha} | H_0) = \\alpha$$\nThis equation defines the relationship between the threshold $t_{\\alpha}$ and the desired false positive rate $\\alpha$.\n\nNext, we incorporate the position-specific prior probability, $\\pi_j$, that a motif occurs at position $j$. The prior probability for $H_1$ is $P(H_1) = \\pi_j$, and for $H_0$ is $P(H_0) = 1 - \\pi_j$. Using Bayes' theorem, the posterior odds of $H_1$ versus $H_0$ given the observation $X_j$ are:\n$$\\frac{P(H_1 | X_j)}{P(H_0 | X_j)} = \\frac{P(X_j | H_1) P(H_1)}{P(X_j | H_0) P(H_0)} = \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\cdot \\frac{\\pi_j}{1 - \\pi_j}$$\nThe posterior log-odds score, which we denote as $\\tilde{S}(X_j)$, is the natural logarithm of the posterior odds:\n$$\\tilde{S}(X_j) = \\ln\\left( \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\cdot \\frac{\\pi_j}{1 - \\pi_j} \\right) = \\ln\\left( \\frac{P(X_j | H_1)}{P(X_j | H_0)} \\right) + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\nSubstituting the definition of $S(X_j)$, we find the relationship between the posterior and classical scores:\n$$\\tilde{S}(X_j) = S(X_j) + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\nThe term $\\ln( \\frac{\\pi_j}{1 - \\pi_j} )$ is the log-prior-odds for position $j$.\n\nThe new decision rule is to call a motif at position $j$ if the posterior log-odds score $\\tilde{S}(X_j)$ exceeds a new, position-specific threshold $\\tilde{t}_j$. The core requirement of the problem is to choose $\\tilde{t}_j$ such that the false positive rate remains $\\alpha$ for every position $j$. This condition is expressed as:\n$$P(\\tilde{S}(X_j) > \\tilde{t}_j | H_0) = \\alpha$$\n\nTo find $\\tilde{t}_j$, we substitute the expression for $\\tilde{S}(X_j)$ into this condition:\n$$P\\left( S(X_j) + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right) > \\tilde{t}_j \\;\\middle|\\; H_0 \\right) = \\alpha$$\nThe term $\\ln( \\frac{\\pi_j}{1 - \\pi_j} )$ is a constant for a given position $j$. We can rearrange the inequality inside the probability statement to isolate the random variable $S(X_j)$:\n$$P\\left( S(X_j) > \\tilde{t}_j - \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right) \\;\\middle|\\; H_0 \\right) = \\alpha$$\n\nWe now have two statements that must hold true simultaneously:\n$1.$ $P(S(X_j) > t_{\\alpha} | H_0) = \\alpha$\n$2.$ $P\\left( S(X_j) > \\tilde{t}_j - \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right) \\;\\middle|\\; H_0 \\right) = \\alpha$\n\nBoth expressions describe the probability that the same random variable, $S(X_j)$ (whose distribution is determined by $H_0$), exceeds a certain threshold. For these probabilities to be equal to the same value $\\alpha$, the thresholds must be identical. Therefore, we can equate the arguments of the score function:\n$$t_{\\alpha} = \\tilde{t}_j - \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\n\nFinally, we solve for the desired threshold $\\tilde{t}_j$ by rearranging the equation:\n$$\\tilde{t}_j = t_{\\alpha} + \\ln\\left( \\frac{\\pi_j}{1 - \\pi_j} \\right)$$\nThis is the closed-form expression for the new threshold $\\tilde{t}_j$ that guarantees the false positive rate per position remains equal to $\\alpha$ under the null hypothesis $H_0$. It shows that the new threshold is the classical threshold adjusted by the log-prior-odds at that position.",
            "answer": "$$\\boxed{t_{\\alpha} + \\ln\\left(\\frac{\\pi_j}{1 - \\pi_j}\\right)}$$"
        },
        {
            "introduction": "Moving from linear sequences to complex networks, the concept of a motif generalizes to recurring patterns of interconnection, such as feed-forward loops in gene regulation. Brute-force searching for these network motifs is computationally expensive, necessitating more elegant algorithmic solutions. This hands-on problem introduces a powerful method using tensor algebra to systematically count induced subgraphs, providing a bridge between abstract mathematical formalisms and high-performance scientific computing. ",
            "id": "3329491",
            "problem": "You are given a directed simple graph with no self-loops represented by a binary adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$, where $A_{ij} = 1$ if and only if there is a directed edge from node $i$ to node $j$, and $A_{ii} = 0$ for all $i$. Consider counting directed $3$-node motif classes through tensor-based constructions and contractions. Let the third-order adjacency tensor be defined by $T_{ijk} = A_{ij} A_{jk} A_{ki}$. The goal is to design a tensor-based method to count, class-wise, induced occurrences of the following $3$-node directed motifs:\n- Directed $3$-cycle ($C_3$): a triple of distinct nodes $\\{i,j,k\\}$ with edges $i \\to j$, $j \\to k$, $k \\to i$, and with no other directed edges among these three nodes.\n- Feed-forward loop (FFL): a triple of distinct nodes $\\{i,j,k\\}$ with edges $i \\to j$, $j \\to k$, $i \\to k$, and with no other directed edges among these three nodes.\n- Directed $2$-edge path ($P_3$): a triple of distinct nodes $\\{i,j,k\\}$ with edges $i \\to j$, $j \\to k$, and with no other directed edges among these three nodes.\n\nYour derivation and construction must start from the following fundamental base:\n- The definition of the adjacency matrix $A$ and boolean edge indicators.\n- The definition of the third-order adjacency tensor $T_{ijk}$ as above.\n- The definition and use of the Kronecker delta $\\delta_{ab}$ for index-distinctness enforcement via products such as $(1 - \\delta_{ij})(1 - \\delta_{jk})(1 - \\delta_{ik})$.\n- The principle that a tensor contraction is a summation over repeated indices, and that tensor entries can be composed multiplicatively from edge indicators and their complements to enforce edge-presence or edge-absence constraints.\n\nYour task:\n- Construct indicator tensors for each motif class that enforce both required edges and required non-edges on triples of distinct indices $(i,j,k)$.\n- Use tensor contractions (summing over $i$, $j$, $k$) to obtain raw counts.\n- Adjust raw counts to class-wise counts up to isomorphism. For the directed $3$-cycle motif $C_3$, observe and justify that each unique $3$-cycle contributes $3$ ordered realizations to the raw count due to cyclic rotations of $(i,j,k)$; thus divide the raw $C_3$ count by $3$. For the feed-forward loop (FFL) and the directed $2$-edge path $P_3$, argue and use that each induced instance contributes exactly $1$ ordered realization under the above indicator design; no division is needed.\n- Implement the above as a complete program that constructs the required tensors and performs the contractions efficiently without explicit $O(n^3)$ loops where possible.\n\nScalability discussion requirement:\n- In your solution, discuss how this tensor-based approach generalizes conceptually to $k = 4$ with fourth-order tensors (for example, for counting a directed $4$-cycle), including the asymptotic memory and time complexity in terms of $n$ for naive tensor materialization versus staged contractions, and identify which operations dominate. Your discussion should remain within a principle-based analysis referencing tensor contraction and adjacency composition, not providing implementation code for $k = 4$.\n\nTest suite:\nYour program must compute class-wise counts for the following directed graphs, each defined by an adjacency matrix $A^{(t)}$, where the entry in row $p$ and column $q$ is $A^{(t)}_{pq}$. All unspecified entries are $0$. All diagonal entries are $0$.\n\n- Test case $1$ ($n = 4$): one directed $3$-cycle on nodes $\\{0,1,2\\}$.\n  - Nonzero entries: $A^{(1)}_{01} = 1$, $A^{(1)}_{12} = 1$, $A^{(1)}_{20} = 1$.\n\n- Test case $2$ ($n = 4$): one feed-forward loop on nodes $\\{0,1,2\\}$ with $0 \\to 1$, $1 \\to 2$, $0 \\to 2$.\n  - Nonzero entries: $A^{(2)}_{01} = 1$, $A^{(2)}_{12} = 1$, $A^{(2)}_{02} = 1$.\n\n- Test case $3$ ($n = 5$): one directed $3$-cycle on nodes $\\{0,1,2\\}$ and one feed-forward loop on nodes $\\{2,3,4\\}$ with $2 \\to 3$, $3 \\to 4$, $2 \\to 4$.\n  - Nonzero entries: $A^{(3)}_{01} = 1$, $A^{(3)}_{12} = 1$, $A^{(3)}_{20} = 1$, $A^{(3)}_{23} = 1$, $A^{(3)}_{34} = 1$, $A^{(3)}_{24} = 1$.\n\n- Test case $4$ ($n = 4$): a bidirected triangle on nodes $\\{0,1,2\\}$ where both directions exist on every edge among $\\{0,1,2\\}$.\n  - Nonzero entries: $A^{(4)}_{01} = 1$, $A^{(4)}_{10} = 1$, $A^{(4)}_{02} = 1$, $A^{(4)}_{20} = 1$, $A^{(4)}_{12} = 1$, $A^{(4)}_{21} = 1$.\n\n- Test case $5$ ($n = 3$): empty graph (no edges).\n\nRequired outputs:\n- For each test case $t \\in \\{1,2,3,4,5\\}$, compute a list $[c_3^{(t)}, f^{(t)}, p^{(t)}]$ where $c_3^{(t)}$ is the class-wise count of induced directed $3$-cycles $C_3$, $f^{(t)}$ is the class-wise count of induced feed-forward loops FFL, and $p^{(t)}$ is the class-wise count of induced directed $2$-edge paths $P_3$.\n- Your program should produce a single line of output containing the results as a comma-separated list of these lists, enclosed in square brackets, in the order of the test cases, for example $[[c_3^{(1)},f^{(1)},p^{(1)}],[c_3^{(2)},f^{(2)},p^{(2)}],\\dots]$.\n\nNo physical or angular units are involved. All counts must be reported as nonnegative integers. Your program must be self-contained, must not read any input, and must use exactly the test suite above embedded in code. The implementation must rely on tensor-based indicator construction and contraction principles as described.",
            "solution": "The problem requires the design of a tensor-based method to count the number of induced occurrences for three specific $3$-node directed motif classes: the directed $3$-cycle ($C_3$), the feed-forward loop (FFL), and the directed $2$-edge path ($P_3$). The counting must be performed on a given directed simple graph represented by an adjacency matrix $A$.\n\nThe fundamental principle is to construct a third-order indicator tensor for each motif class. An indicator tensor, say $I(i, j, k)$, is defined such that its value is $1$ if the ordered triple of distinct nodes $(i, j, k)$ forms an instance of the motif with the specified roles, and $0$ otherwise. An entry $I(i, j, k)$ is constructed as a product of terms corresponding to all possible $\\binom{3}{2} \\times 2 = 6$ directed edges among the nodes $\\{i, j, k\\}$. An edge from node $u$ to node $v$ is required to be present if its corresponding factor in the product is $A_{uv}$. It is required to be absent if its factor is $(1 - A_{uv})$. The total number of ordered triples forming the motif, the raw count, is obtained by a full contraction of this tensor, which is the sum of all its elements: $\\sum_{i,j,k} I(i, j, k)$. Since the adjacency matrix is defined for a simple graph with no self-loops ($A_{ii}=0$), any term in the sum that involves non-distinct indices (e.g., $i=j$) will be zero due to a factor like $A_{ii}$ or $A_{jj}$, thereby automatically enforcing the distinct-node constraint for these specific motif structures.\n\nLet $A \\in \\{0, 1\\}^{n \\times n}$ be the adjacency matrix where $A_{ij}=1$ indicates a directed edge from node $i$ to node $j$. The indicator for a non-edge is $\\bar{A}_{ij} = 1 - A_{ij}$.\n\n**1. Directed 3-Cycle ($C_3$)**\nAn induced $C_3$ on a set of three distinct nodes $\\{i, j, k\\}$ consists of a cycle of edges, say $i \\to j \\to k \\to i$, and no other edges between these nodes. For an ordered triple $(i,j,k)$, the required edges are $i \\to j$, $j \\to k$, and $k \\to i$. The required non-edges are the reverse edges: $j \\to i$, $k \\to j$, and $i \\to k$.\n\nThe indicator tensor for an ordered triple $(i,j,k)$ forming a $C_3$ is:\n$$I_{C_3}(i,j,k) = A_{ij} A_{jk} A_{ki} (1-A_{ji}) (1-A_{kj}) (1-A_{ik})$$\nThe raw count of such ordered triples is the full tensor contraction:\n$$N_{C_3, \\text{raw}} = \\sum_{i,j,k=0}^{n-1} I_{C_3}(i,j,k)$$\nA single induced $C_3$ subgraph on the node set $\\{u, v, w\\}$ consists of the edges $\\{u \\to v, v \\to w, w \\to u\\}$. This single subgraph will be counted three times by the raw count, corresponding to the cyclically equivalent ordered triples $(u,v,w)$, $(v,w,u)$, and $(w,u,v)$. Each of these will satisfy the indicator tensor condition. Therefore, to obtain the class-wise count of unique $C_3$ subgraphs, we must divide the raw count by $3$.\n$$c_3 = \\frac{N_{C_3, \\text{raw}}}{3}$$\n\n**2. Feed-Forward Loop (FFL)**\nAn induced FFL on a set of three distinct nodes $\\{i, j, k\\}$ consists of a source node routing to a sink node both directly and through an intermediate node. Let's assign roles: $i$ is the source, $j$ is the intermediate, and $k$ is the sink. The required edges are $i \\to j$, $j \\to k$, and the feed-forward edge $i \\to k$. To be an induced subgraph, no other edges must exist between them. The required non-edges are $j \\to i$, $k \\to j$, and $k \\to i$.\n\nThe indicator tensor for an ordered triple $(i,j,k)$ forming an FFL with this role assignment is:\n$$I_{FFL}(i,j,k) = A_{ij} A_{jk} A_{ik} (1-A_{ji}) (1-A_{kj}) (1-A_{ki})$$\nThe raw count is the sum over all elements:\n$$N_{FFL, \\text{raw}} = \\sum_{i,j,k=0}^{n-1} I_{FFL}(i,j,k)$$\nIn any induced FFL subgraph, the roles of source, intermediate, and sink are unique, determined by the nodes' in-degrees and out-degrees within the subgraph (source: in-degree $0$, out-degree $2$; intermediate: in-degree $1$, out-degree $1$; sink: in-degree $2$, out-degree $0$). Consequently, only one of the $3! = 6$ possible ordered permutations of the three nodes will match the specific structure encoded in $I_{FFL}(i,j,k)$. Thus, each FFL subgraph is counted exactly once. No division is necessary.\n$$f = N_{FFL, \\text{raw}}$$\n\n**3. Directed 2-Edge Path ($P_3$)**\nAn induced $P_3$ on a set of three distinct nodes $\\{i, j, k\\}$ is a simple path of length two, say $i \\to j \\to k$, with no other edges connecting the three nodes. The roles are start ($i$), middle ($j$), and end ($k$). The required edges are $i \\to j$ and $j \\to k$. The required non-edges are $j \\to i$, $k \\to j$, and the two edges that would \"short-circuit\" the path, $i \\to k$ and $k \\to i$.\n\nThe indicator tensor for an ordered triple $(i,j,k)$ forming a $P_3$ is:\n$$I_{P_3}(i,j,k) = A_{ij} A_{jk} (1-A_{ji}) (1-A_{kj}) (1-A_{ik}) (1-A_{ki})$$\nThe raw count is obtained by contraction:\n$$N_{P_3, \\text{raw}} = \\sum_{i,j,k=0}^{n-1} I_{P_3}(i,j,k)$$\nSimilar to the FFL, the roles of start, middle, and end in an induced $P_3$ are unique (start: in-degree $0$, out-degree $1$; middle: in-degree $1$, out-degree $1$; end: in-degree $1$, out-degree $0$). Therefore, each unique induced $P_3$ subgraph corresponds to exactly one ordered triple $(i,j,k)$ that satisfies the indicator condition. The raw count is the final count.\n$$p = N_{P_3, \\text{raw}}$$\n\n**Scalability Discussion for $k=4$ Motifs**\nThe described tensor-based approach generalizes to motifs of any size $k$. For $k=4$, we would consider a fourth-order indicator tensor $I(i_1, i_2, i_3, i_4)$. The entries of this tensor are products of $A_{ab}$ or $(1-A_{ab})$ for all $4 \\times 3 = 12$ possible directed edges among the four nodes.\n\nA **naive materialization** of this tensor involves creating an $n \\times n \\times n \\times n$ array in memory. The memory complexity is $O(n^4)$, and the time complexity to compute the full contraction (sum) is also $O(n^4)$. This becomes computationally prohibitive for even moderately sized graphs.\n\nA more advanced technique, **staged contractions**, avoids building the full tensor. The total sum, $\\sum_{i_1, \\dots, i_k} I(i_1, \\dots, i_k)$, can be computed by evaluating the sum iteratively, one index at a time. This process can often be expressed as a series of matrix operations. For example, counting non-induced directed $4$-cycles ($i \\to j \\to k \\to l \\to i$) corresponds to the sum $\\sum_{i,j,k,l} A_{ij}A_{jk}A_{kl}A_{li}$. This sum is precisely the trace of the fourth power of the adjacency matrix, $\\text{Tr}(A^4)$. This can be computed with three matrix multiplications, taking $O(n^\\omega)$ time, where $\\omega \\approx 2.37$ is the matrix multiplication exponent, and requiring only $O(n^2)$ memory. Counting *induced* motifs involves an inclusion-exclusion summation of such matrix product traces, which is more complex but retains the core advantage: the complexity is dominated by matrix multiplication, e.g., $O(n^\\omega)$, and not by the naive $O(n^k)$. Staged contractions thus provide a path to scalability by reformulating the high-order tensor contraction into a sequence of lower-order (typically matrix) operations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the motif counting problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: n=4, one directed 3-cycle {0,1,2}\n        (4, [(0, 1), (1, 2), (2, 0)]),\n        \n        # Test case 2: n=4, one FFL on {0,1,2}\n        (4, [(0, 1), (1, 2), (0, 2)]),\n        \n        # Test case 3: n=5, C3 on {0,1,2} and FFL on {2,3,4}\n        (5, [(0, 1), (1, 2), (2, 0), (2, 3), (3, 4), (2, 4)]),\n        \n        # Test case 4: n=4, bidirected triangle on {0,1,2}\n        (4, [(0, 1), (1, 0), (0, 2), (2, 0), (1, 2), (2, 1)]),\n        \n        # Test case 5: n=3, empty graph\n        (3, [])\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        A = np.zeros((n, n), dtype=int)\n        for i, j in edges:\n            A[i, j] = 1\n        \n        counts = count_motifs(A)\n        results.append(counts)\n\n    # Format the final output string\n    # e.g., [[1,0,0],[0,1,0],[1,1,2],[0,0,0],[0,0,0]]\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    print(output_str)\n\ndef count_motifs(A: np.ndarray) -> list[int]:\n    \"\"\"\n    Counts induced 3-node motifs using tensor-based indicator construction.\n    \n    Args:\n        A: The n x n adjacency matrix of the graph.\n        \n    Returns:\n        A list [c3_count, ffl_count, p3_count].\n    \"\"\"\n    n = A.shape[0]\n    \n    # Broadcast A to 3rd-order tensors representing A_{xy} for all pairs (x,y)\n    # This materializes the n x n x n tensors for direct computation.\n    # We use Einstein summation notation strings for clarity, but this is equivalent\n    # to using `None` for broadcasting, e.g., A_ij = A[:, None, :]\n    A_ij = np.einsum('ij,k->ijk', A, np.ones(n))\n    A_jk = np.einsum('i,jk->ijk', np.ones(n), A)\n    A_ki = np.einsum('ik,j->ijk', A.T, np.ones(n)) # A_ki[i,j,k] = A[k,i]\n    \n    # Transposed versions for reverse edges\n    A_ji = np.einsum('ji,k->ijk', A, np.ones(n)) # A_ji[i,j,k] = A[j,i]\n    A_kj = np.einsum('i,kj->ijk', np.ones(n), A) # A_kj[i,j,k] = A[k,j]\n    A_ik = np.einsum('ik,j->ijk', A, np.ones(n))   # A_ik[i,j,k] = A[i,k]\n\n    # Indicator tensor for C3 (Directed 3-cycle)\n    # Edges: i->j, j->k, k->i\n    # Non-edges: j->i, k->j, i->k\n    I_C3 = A_ij * A_jk * A_ki * (1 - A_ji) * (1 - A_kj) * (1 - A_ik)\n    raw_c3_count = np.sum(I_C3)\n    # Each C3 is counted 3 times due to cyclic symmetry\n    c3_count = int(raw_c3_count // 3)\n\n    # Indicator tensor for FFL (Feed-forward loop)\n    # Edges: i->j, j->k, i->k\n    # Non-edges: j->i, k->j, k->i\n    I_FFL = A_ij * A_jk * A_ik * (1 - A_ji) * (1 - A_kj) * (1 - A_ki)\n    raw_ffl_count = np.sum(I_FFL)\n    # Each FFL has unique roles (source, intermediate, sink), counted once\n    ffl_count = int(raw_ffl_count)\n\n    # Indicator tensor for P3 (Directed 2-edge path)\n    # Edges: i->j, j->k\n    # Non-edges: j->i, k->j, i->k, k->i\n    I_P3 = A_ij * A_jk * (1 - A_ji) * (1 - A_kj) * (1 - A_ik) * (1 - A_ki)\n    raw_p3_count = np.sum(I_P3)\n    # Each P3 has unique roles (start, middle, end), counted once\n    p3_count = int(raw_p3_count)\n\n    return [c3_count, ffl_count, p3_count]\n\nsolve()\n```"
        }
    ]
}