{
    "hands_on_practices": [
        {
            "introduction": "The journey from a biological sample to digital sequence data is fraught with potential biases. This first practice delves into one of the most critical steps: library preparation via Polymerase Chain Reaction (PCR). Here, we will build a model from first principles to explore how local GC content can systematically affect amplification efficiency, leading to the well-known phenomenon of GC coverage bias . By combining thermodynamic models of primer binding with kinetic models of polymerase processivity, you will gain a quantitative understanding of how molecular-level interactions manifest as large-scale patterns in sequencing data.",
            "id": "3310818",
            "problem": "Consider a simplified, first-principles model of Polymerase Chain Reaction (PCR) amplification during sequencing library preparation that captures the impact of local guanine-cytosine (GC) content on primer hybridization kinetics and deoxyribonucleic acid (DNA) polymerase processivity. The goal is to derive, implement, and evaluate a predictive function that maps local GC fraction to an expected coverage bias after a fixed number of PCR cycles. Use only the following foundational bases.\n\n- Mass action equilibrium for primer-template binding with a single-site binding model: the equilibrium dissociation constant is $K_d = \\exp\\!\\left(\\Delta G/(R T)\\right)$, where $\\Delta G$ is the standard Gibbs free energy change, $R$ is the universal gas constant, and $T$ is absolute temperature in Kelvin. For primer concentration $C_p$ (molar), the equilibrium fraction of targets with primer bound is $f_{\\mathrm{bind}} = \\dfrac{C_p}{C_p + K_d}$.\n- A coarse-grained, sequence-dependent free energy model at fixed ionic strength: for a primer of length $L_p$, with local GC fraction $g \\in [0,1]$, the binding free energy is approximated by $\\Delta G(g) = L_p\\big[g\\,\\delta g_{\\mathrm{GC}} + (1-g)\\,\\delta g_{\\mathrm{AT}}\\big] + \\delta g_{\\mathrm{init}}$, where $\\delta g_{\\mathrm{GC}}$ and $\\delta g_{\\mathrm{AT}}$ are per-base contributions (kilocalories per mole), and $\\delta g_{\\mathrm{init}}$ is an initiation term (kilocalories per mole).\n- A per-base continuation model of processivity: let the per-base drop-off probability be $p_{\\mathrm{drop}}(g) = p_0\\big[1 + \\beta\\,(g - 0.5)\\big]$, clipped to the interval $[0,1)$. The per-base continuation probability is $q(g) = 1 - p_{\\mathrm{drop}}(g)$. The probability that a polymerase completes extension of an amplicon of length $L_{\\mathrm{ext}}$ bases is $p_{\\mathrm{ext}}(g) = \\big(q(g)\\big)^{L_{\\mathrm{ext}}}$.\n- Expected per-cycle fold amplification factor: one cycle yields an expected fold change $F(g) = 1 + f_{\\mathrm{bind}}(g)\\,p_{\\mathrm{ext}}(g)$, bounded in $[1,2]$. After $C$ cycles, the expected copy number multiplies by $\\big(F(g)\\big)^{C}$.\n\nDefine the GC-dependent coverage bias relative to a reference GC fraction $g_{\\mathrm{ref}}$ as the ratio\n$$\nB(g) = \\left(\\frac{F(g)}{F(g_{\\mathrm{ref}})}\\right)^{C}.\n$$\nYour tasks are:\n- Starting only from the above bases, derive the closed-form expression for $B(g)$ in terms of the parameters $\\{L_p,L_{\\mathrm{ext}},\\delta g_{\\mathrm{GC}},\\delta g_{\\mathrm{AT}},\\delta g_{\\mathrm{init}},T,C_p,p_0,\\beta,C,g_{\\mathrm{ref}},R\\}$.\n- Implement a program that computes $B(g)$ for several specified values of $g$ and parameter sets, following the numerical and unit conventions below.\n\nNumerical and unit conventions:\n- Use energy units in kilocalories per mole. Use temperature in Kelvin. Use concentration in molar. If primer concentration is specified in nanomolar, convert using $1\\,\\mathrm{nM} = 10^{-9}\\,\\mathrm{M}$.\n- Use $R = 1.987\\times 10^{-3}\\,\\mathrm{kcal}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$.\n- All outputs are dimensionless ratios. Round each reported bias to $6$ decimal places.\n\nTest suite to implement and evaluate:\nFor each test case, compute and return the list $\\big[B(g_1),B(g_2),\\ldots\\big]$ for the provided GC fractions, using the given parameters.\n\n- Test Case $1$ (happy path):\n  - GC fractions: $[0.2, 0.5, 0.8]$.\n  - $L_p = 20$, $L_{\\mathrm{ext}} = 300$, $\\delta g_{\\mathrm{GC}} = -2.5$ kilocalories per mole, $\\delta g_{\\mathrm{AT}} = -1.0$ kilocalories per mole, $\\delta g_{\\mathrm{init}} = +1.0$ kilocalories per mole.\n  - $T = 333$ Kelvin, $C_p = 250$ nanomolar, $p_0 = 0.005$, $\\beta = 0.8$, $C = 15$, $g_{\\mathrm{ref}} = 0.5$.\n\n- Test Case $2$ (low primer concentration and high temperature):\n  - GC fractions: $[0.2, 0.5, 0.8]$.\n  - $L_p = 20$, $L_{\\mathrm{ext}} = 300$, $\\delta g_{\\mathrm{GC}} = -2.5$, $\\delta g_{\\mathrm{AT}} = -1.0$, $\\delta g_{\\mathrm{init}} = +1.0$.\n  - $T = 343$, $C_p = 1$ nanomolar, $p_0 = 0.005$, $\\beta = 0.8$, $C = 15$, $g_{\\mathrm{ref}} = 0.5$.\n\n- Test Case $3$ (strong GC-dependent drop-off, including extreme GC bounds):\n  - GC fractions: $[0.0, 0.5, 1.0]$.\n  - $L_p = 20$, $L_{\\mathrm{ext}} = 300$, $\\delta g_{\\mathrm{GC}} = -2.5$, $\\delta g_{\\mathrm{AT}} = -1.0$, $\\delta g_{\\mathrm{init}} = +1.0$.\n  - $T = 333$, $C_p = 250$ nanomolar, $p_0 = 0.005$, $\\beta = 2.0$, $C = 15$, $g_{\\mathrm{ref}} = 0.5$.\n\n- Test Case $4$ (long amplicon with elevated drop-off and more cycles):\n  - GC fractions: $[0.2, 0.5, 0.8]$.\n  - $L_p = 20$, $L_{\\mathrm{ext}} = 1000$, $\\delta g_{\\mathrm{GC}} = -2.5$, $\\delta g_{\\mathrm{AT}} = -1.0$, $\\delta g_{\\mathrm{init}} = +1.0$.\n  - $T = 333$, $C_p = 250$ nanomolar, $p_0 = 0.010$, $\\beta = 0.8$, $C = 20$, $g_{\\mathrm{ref}} = 0.5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself the comma-separated list of rounded biases for the specified GC fractions, also enclosed in square brackets. For example, the output should look like $[[b_{1,1},b_{1,2},b_{1,3}],[b_{2,1},b_{2,2},b_{2,3}],\\ldots]$ with each $b_{i,j}$ rounded to $6$ decimal places.",
            "solution": "The problem requires the derivation and implementation of a function, $B(g)$, that quantifies the expected coverage bias in a Polymerase Chain Reaction (PCR) as a function of local guanine-cytosine (GC) fraction, $g$. The derivation must be performed from first principles as defined in the problem statement.\n\nThe analysis proceeds in a bottom-up fashion, starting from the most fundamental components of the model and systematically composing them to arrive at the final expression for the bias, $B(g)$.\n\n**1. Gibbs Free Energy of Primer Binding, $\\Delta G(g)$**\n\nThe initial step in PCR amplification is the hybridization of a primer to its target template strand. The stability of this binding is governed by the standard Gibbs free energy change, $\\Delta G$. The problem provides a coarse-grained linear model for $\\Delta G$ that depends on the local GC fraction, $g$, of the primer binding site.\n\nFor a primer of length $L_p$ and a local GC fraction $g \\in [0,1]$, the free energy is given by:\n$$\n\\Delta G(g) = L_p\\big[g\\,\\delta g_{\\mathrm{GC}} + (1-g)\\,\\delta g_{\\mathrm{AT}}\\big] + \\delta g_{\\mathrm{init}}\n$$\nHere, $\\delta g_{\\mathrm{GC}}$ and $\\delta g_{\\mathrm{AT}}$ are the average free energy contributions per GC and adenine-thymine (AT) base pair, respectively, and $\\delta g_{\\mathrm{init}}$ is a constant representing the energetic cost of initiating hybridization. The units are kilocalories per mole ($\\mathrm{kcal} \\cdot \\mathrm{mol}^{-1}$).\n\n**2. Equilibrium Dissociation Constant, $K_d(g)$**\n\nThe Gibbs free energy is related to the equilibrium dissociation constant, $K_d$, via the fundamental thermodynamic relation:\n$$\nK_d(g) = \\exp\\left(\\frac{\\Delta G(g)}{R T}\\right)\n$$\nwhere $R$ is the universal gas constant ($1.987 \\times 10^{-3} \\, \\mathrm{kcal} \\cdot \\mathrm{mol}^{-1} \\cdot \\mathrm{K}^{-1}$) and $T$ is the absolute temperature in Kelvin (K). A more negative $\\Delta G$ results in a smaller $K_d$, signifying stronger binding.\n\n**3. Fraction of Bound Primers, $f_{\\mathrm{bind}}(g)$**\n\nUsing the principles of mass action kinetics for a single-site binding model, the fraction of target DNA molecules that are bound by a primer at equilibrium, $f_{\\mathrm{bind}}$, is a function of the primer concentration, $C_p$, and the dissociation constant, $K_d$.\n$$\nf_{\\mathrm{bind}}(g) = \\frac{C_p}{C_p + K_d(g)}\n$$\nSubstituting the expression for $K_d(g)$ yields the GC-dependent fraction of bound primers:\n$$\nf_{\\mathrm{bind}}(g) = \\frac{C_p}{C_p + \\exp\\left(\\frac{\\Delta G(g)}{R T}\\right)}\n$$\nThis term represents the efficiency of the primer annealing step.\n\n**4. Polymerase Extension Probability, $p_{\\mathrm{ext}}(g)$**\n\nOnce a primer is bound, a DNA polymerase must extend it to create a full-length copy of the amplicon. The model assumes a GC-dependent, per-base drop-off probability, $p_{\\mathrm{drop}}(g)$. This phenomenological model posits a linear relationship:\n$$\np_{\\mathrm{drop, raw}}(g) = p_0\\big[1 + \\beta\\,(g - 0.5)\\big]\n$$\nwhere $p_0$ is the baseline drop-off probability at a neutral GC content of $g=0.5$, and $\\beta$ is a coefficient modulating the GC-dependence. Since probability must be in the interval $[0,1]$, this raw value is clipped:\n$$\np_{\\mathrm{drop}}(g) = \\mathrm{clip}\\big(p_{\\mathrm{drop, raw}}(g), [0, 1)\\big)\n$$\nThe probability of the polymerase *not* dropping off at a given base (the continuation probability) is $q(g) = 1 - p_{\\mathrm{drop}}(g)$. For the polymerase to successfully synthesize an entire amplicon of length $L_{\\mathrm{ext}}$, it must not drop off at any of the $L_{\\mathrm{ext}}$ bases. Assuming a memoryless Bernoulli process, this probability is:\n$$\np_{\\mathrm{ext}}(g) = \\big(q(g)\\big)^{L_{\\mathrm{ext}}} = \\left(1 - p_{\\mathrm{drop}}(g)\\right)^{L_{\\mathrm{ext}}}\n$$\nThis term represents the efficiency of the polymerase extension step.\n\n**5. Per-Cycle Amplification Factor, $F(g)$**\n\nOne cycle of PCR ideally doubles the amount of DNA. However, inefficiencies in binding and extension reduce the yield. The expected fold change, $F(g)$, in one cycle is modeled as:\n$$\nF(g) = 1 + f_{\\mathrm{bind}}(g) \\cdot p_{\\mathrm{ext}}(g)\n$$\nThe term '$1$' accounts for the original template molecules, and the product $f_{\\mathrm{bind}}(g) \\cdot p_{\\mathrm{ext}}(g)$ represents the fraction of templates that are successfully converted into new copies. This factor is bounded in $[1, 2]$.\n\n**6. Cumulative Coverage Bias, $B(g)$**\n\nAfter $C$ cycles of PCR, the initial number of molecules with GC fraction $g$ is amplified by a factor of $(F(g))^C$. The coverage bias, $B(g)$, is defined as the amplification of a region with GC fraction $g$ relative to that of a reference region with GC fraction $g_{\\mathrm{ref}}$.\n$$\nB(g) = \\frac{(F(g))^C}{(F(g_{\\mathrm{ref}}))^C} = \\left(\\frac{F(g)}{F(g_{\\mathrm{ref}})}\\right)^C\n$$\n\n**7. Final Closed-Form Expression**\n\nBy substituting the derived components, we obtain the complete expression for the coverage bias $B(g)$:\n$$\nB(g) = \\left( \\frac{1 + f_{\\mathrm{bind}}(g) \\cdot p_{\\mathrm{ext}}(g)}{1 + f_{\\mathrm{bind}}(g_{\\mathrm{ref}}) \\cdot p_{\\mathrm{ext}}(g_{\\mathrm{ref}})} \\right)^C\n$$\nwhere:\n- $f_{\\mathrm{bind}}(x) = \\dfrac{C_p}{C_p + \\exp\\left(\\dfrac{L_p[x\\,\\delta g_{\\mathrm{GC}} + (1-x)\\,\\delta g_{\\mathrm{AT}}] + \\delta g_{\\mathrm{init}}}{R T}\\right)}$\n- $p_{\\mathrm{ext}}(x) = \\left(1 - \\mathrm{clip}\\left(p_0[1 + \\beta(x - 0.5)], [0, 1)\\right)\\right)^{L_{\\mathrm{ext}}}$\n\nThis final expression incorporates all specified parameters $\\{L_p, L_{\\mathrm{ext}}, \\delta g_{\\mathrm{GC}}, \\delta g_{\\mathrm{AT}}, \\delta g_{\\mathrm{init}}, T, C_p, p_0, \\beta, C, g_{\\mathrm{ref}}, R\\}$ and can be implemented computationally.\n\nA noteworthy observation arises from the parameter values provided in the test cases. The values for $\\delta g_{\\mathrm{GC}}$ and $\\delta g_{\\mathrm{AT}}$ are strongly negative, leading to highly favorable (very negative) $\\Delta G$ values. This results in an extremely small dissociation constant, $K_d$, such that $K_d \\ll C_p$ across a wide range of conditions. Consequently, the primer binding fraction, $f_{\\mathrm{bind}}(g)$, is effectively saturated at $1.0$ for several test cases, including Case 2, which is described as \"low primer concentration and high temperature\". In this regime, the coverage bias is dominated entirely by the differences in polymerase processivity, $p_{\\mathrm{ext}}(g)$, rather than a combination of binding and processivity effects. While the problem is well-defined and solvable as stated, this parameterization serves to isolate one of the two modeled sources of bias.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCR bias problem for all test cases.\n    \"\"\"\n\n    # Universal gas constant in kcal mol^-1 K^-1\n    R = 1.987e-3\n\n    test_cases = [\n        {\n            \"name\": \"Test Case 1 (happy path)\",\n            \"params\": {\n                \"L_p\": 20, \"L_ext\": 300, \"delta_g_GC\": -2.5, \"delta_g_AT\": -1.0, \n                \"delta_g_init\": 1.0, \"T\": 333, \"C_p_nM\": 250, \"p_0\": 0.005, \n                \"beta\": 0.8, \"C\": 15, \"g_ref\": 0.5\n            },\n            \"gc_fractions\": [0.2, 0.5, 0.8]\n        },\n        {\n            \"name\": \"Test Case 2 (low primer concentration and high temperature)\",\n            \"params\": {\n                \"L_p\": 20, \"L_ext\": 300, \"delta_g_GC\": -2.5, \"delta_g_AT\": -1.0, \n                \"delta_g_init\": 1.0, \"T\": 343, \"C_p_nM\": 1, \"p_0\": 0.005, \n                \"beta\": 0.8, \"C\": 15, \"g_ref\": 0.5\n            },\n            \"gc_fractions\": [0.2, 0.5, 0.8]\n        },\n        {\n            \"name\": \"Test Case 3 (strong GC-dependent drop-off, including extreme GC bounds)\",\n            \"params\": {\n                \"L_p\": 20, \"L_ext\": 300, \"delta_g_GC\": -2.5, \"delta_g_AT\": -1.0, \n                \"delta_g_init\": 1.0, \"T\": 333, \"C_p_nM\": 250, \"p_0\": 0.005, \n                \"beta\": 2.0, \"C\": 15, \"g_ref\": 0.5\n            },\n            \"gc_fractions\": [0.0, 0.5, 1.0]\n        },\n        {\n            \"name\": \"Test Case 4 (long amplicon with elevated drop-off and more cycles)\",\n            \"params\": {\n                \"L_p\": 20, \"L_ext\": 1000, \"delta_g_GC\": -2.5, \"delta_g_AT\": -1.0, \n                \"delta_g_init\": 1.0, \"T\": 333, \"C_p_nM\": 250, \"p_0\": 0.010, \n                \"beta\": 0.8, \"C\": 20, \"g_ref\": 0.5\n            },\n            \"gc_fractions\": [0.2, 0.5, 0.8]\n        }\n    ]\n\n    all_results = []\n\n    def calculate_F(g, L_p, L_ext, delta_g_GC, delta_g_AT, delta_g_init, T, C_p_M, p_0, beta):\n        \"\"\"Calculates the per-cycle amplification factor F(g).\"\"\"\n        \n        # 1. Gibbs Free Energy of Primer Binding, delta_G(g)\n        delta_G = L_p * (g * delta_g_GC + (1 - g) * delta_g_AT) + delta_g_init\n        \n        # 2. Equilibrium Dissociation Constant, K_d(g)\n        K_d = np.exp(delta_G / (R * T))\n        \n        # 3. Fraction of Bound Primers, f_bind(g)\n        f_bind = C_p_M / (C_p_M + K_d)\n        \n        # 4. Polymerase Extension Probability, p_ext(g)\n        # The problem states clipping to [0,1). In floating point arithmetic,\n        # an upper bound of 1.0 is acceptable for np.clip.\n        p_drop_raw = p_0 * (1 + beta * (g - 0.5))\n        p_drop = np.clip(p_drop_raw, 0.0, 1.0)\n        q = 1.0 - p_drop\n        p_ext = q**L_ext\n        \n        # 5. Per-Cycle Amplification Factor, F(g)\n        F_g = 1.0 + f_bind * p_ext\n        \n        return F_g\n\n    for case in test_cases:\n        params = case[\"params\"]\n        gc_fractions = case[\"gc_fractions\"]\n        \n        # Convert primer concentration from nM to M\n        C_p_M = params[\"C_p_nM\"] * 1e-9\n        \n        # Calculate F for the reference GC fraction\n        F_g_ref = calculate_F(\n            g=params[\"g_ref\"],\n            L_p=params[\"L_p\"],\n            L_ext=params[\"L_ext\"],\n            delta_g_GC=params[\"delta_g_GC\"],\n            delta_g_AT=params[\"delta_g_AT\"],\n            delta_g_init=params[\"delta_g_init\"],\n            T=params[\"T\"],\n            C_p_M=C_p_M,\n            p_0=params[\"p_0\"],\n            beta=params[\"beta\"]\n        )\n        \n        case_results = []\n        for g in gc_fractions:\n            # Calculate F for the current GC fraction\n            F_g = calculate_F(\n                g=g,\n                L_p=params[\"L_p\"],\n                L_ext=params[\"L_ext\"],\n                delta_g_GC=params[\"delta_g_GC\"],\n                delta_g_AT=params[\"delta_g_AT\"],\n                delta_g_init=params[\"delta_g_init\"],\n                T=params[\"T\"],\n                C_p_M=C_p_M,\n                p_0=params[\"p_0\"],\n                beta=params[\"beta\"]\n            )\n            \n            # 6. Cumulative Coverage Bias, B(g)\n            if F_g_ref == 0:\n                # Avoid division by zero, though unlikely with F(g) >= 1\n                bias = np.inf if F_g > 0 else 1.0\n            else:\n                bias = (F_g / F_g_ref)**params[\"C\"]\n            \n            case_results.append(bias)\n\n        all_results.append(case_results)\n\n    # Format the final output string exactly as required\n    formatted_results = []\n    for res_list in all_results:\n        # Round each result to 6 decimal places and format as a string\n        formatted_list = [f\"{r:.6f}\" for r in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a DNA library is loaded onto a sequencer, the machine generates raw, analog signals—typically light intensities. This exercise takes you inside the \"black box\" of the base-calling process, where these noisy signals are converted into the digital A, C, G, and T reads that form the foundation of all downstream analysis . You will implement a probabilistic model that uses maximum likelihood estimation to make the base call and Bayesian inference to compute the corresponding Phred quality score. This practice is essential for understanding the nature of sequencing errors and the importance of quality scores in genomic analysis.",
            "id": "3310871",
            "problem": "You are given a probabilistic model for sequencing-by-synthesis signals in a four-channel Illumina system, where each cycle emits an intensity vector in the channel space corresponding to the nucleotides Adenine (A), Cytosine (C), Guanine (G), and Thymine (T). At cycle index $i$, the observed intensity vector is denoted by $\\mathbf{y}_i \\in \\mathbb{R}^4$, the per-cycle scalar scale factor is $s_i \\in \\mathbb{R}_{\\ge 0}$, the background baseline vector is $\\boldsymbol{\\beta} \\in \\mathbb{R}^4$, and the channel-wise noise variances are collected into the diagonal covariance $\\operatorname{diag}(\\boldsymbol{\\sigma}^2)$ with $\\boldsymbol{\\sigma}^2 \\in \\mathbb{R}^4_{>0}$. The calibration (cross-talk) matrix $\\mathbf{C} \\in \\mathbb{R}^{4 \\times 4}$ maps a one-hot base vector to expected normalized channel means, such that the expected mean intensity for base $b \\in \\{A,C,G,T\\}$ is the column $\\boldsymbol{\\mu}_b = \\mathbf{C}_{:,b}$, and the generative signal model is given by the sum of scaled mean and baseline plus additive noise:\n$$\n\\mathbf{y}_i = s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}_i, \\quad \\boldsymbol{\\varepsilon}_i \\sim \\mathcal{N}\\!\\left(\\mathbf{0}, \\operatorname{diag}(\\boldsymbol{\\sigma}^2)\\right).\n$$\nAssume channel noises are independent and Gaussian, and that cycles are independent conditioned on the base at each cycle. The set of prior probabilities over bases is $\\mathbf{p} = [p(A),p(C),p(G),p(T)]$ with $p(A)+p(C)+p(G)+p(T)=1$ and $p(b)>0$ for each base $b$.\n\nYour tasks are to compute, for each cycle $i$:\n1. The maximum likelihood base call, defined as the base $\\hat{b}_i$ that maximizes the likelihood $p(\\mathbf{y}_i \\mid b)$ under the stated Gaussian model. In case of ties within a tolerance threshold $\\tau$, select the base with the smallest index in lexicographic order $A \\rightarrow 0$, $C \\rightarrow 1$, $G \\rightarrow 2$, $T \\rightarrow 3$.\n2. The Phred quality score $Q_i$ for the chosen base call, defined by the standard transformation from the posterior error probability,\n$$\nQ_i = -10 \\log_{10}\\!\\left(P_{\\text{err},i}\\right),\n$$\nwhere $P_{\\text{err},i} = 1 - P(\\hat{b}_i \\mid \\mathbf{y}_i)$, and $P(b \\mid \\mathbf{y}_i)$ is computed using Bayes' theorem with the provided prior $\\mathbf{p}$ and the Gaussian likelihood $p(\\mathbf{y}_i \\mid b)$ implied by the generative model above. Use numerically stable computations for probabilities, and when $P_{\\text{err},i}$ is numerically zero, treat it as $10^{-300}$ to avoid undefined logarithms. Express all quality scores as floats rounded to two decimal places.\n\nBase indexing is as follows: $A \\rightarrow 0$, $C \\rightarrow 1$, $G \\rightarrow 2$, $T \\rightarrow 3$. There are no physical units for intensities in this problem.\n\nImplement a complete program that, given the following test suite of parameter sets, produces the specified outputs. For each test case, output a pair of lists: the first list contains the base calls per cycle as integers in $\\{0,1,2,3\\}$, and the second list contains the corresponding $Q$-scores as floats rounded to two decimal places. Aggregate the results of all provided test cases into a single line as a comma-separated list enclosed in square brackets, where each element of the top-level list corresponds to one test case and is itself a two-element list as described.\n\nUse the tie tolerance $\\tau = 10^{-12}$ for all test cases.\n\nTest Suite:\n- Test case $1$ (happy path, low noise):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(1)} = \\begin{bmatrix}\n    1.0 & 0.1 & 0.1 & 0.1 \\\\\n    0.1 & 1.0 & 0.1 & 0.1 \\\\\n    0.1 & 0.1 & 1.0 & 0.1 \\\\\n    0.1 & 0.1 & 0.1 & 1.0\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(1)} = [0.05, 0.05, 0.05, 0.05]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(1)} = [0.0, 0.0, 0.0, 0.0]$.\n  - Scale per cycle: $\\mathbf{s}^{(1)} = [1.0, 1.0, 1.0, 1.0, 1.0]$.\n  - Priors: $\\mathbf{p}^{(1)} = [0.25, 0.25, 0.25, 0.25]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(1)} = \\begin{bmatrix}\n    1.20 & 0.10 & 0.10 & 0.10 \\\\\n    0.10 & 1.10 & 0.10 & 0.10 \\\\\n    0.20 & 0.10 & 1.30 & 0.10 \\\\\n    0.10 & 0.10 & 0.10 & 1.25 \\\\\n    0.30 & 0.20 & 0.90 & 0.20\n    \\end{bmatrix}.\n    $$\n- Test case $2$ (high noise, ambiguous signals):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(2)} = \\begin{bmatrix}\n    1.0 & 0.1 & 0.1 & 0.1 \\\\\n    0.1 & 1.0 & 0.1 & 0.1 \\\\\n    0.1 & 0.1 & 1.0 & 0.1 \\\\\n    0.1 & 0.1 & 0.1 & 1.0\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(2)} = [0.5, 0.5, 0.5, 0.5]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(2)} = [0.0, 0.0, 0.0, 0.0]$.\n  - Scale per cycle: $\\mathbf{s}^{(2)} = [1.0, 1.0, 1.0]$.\n  - Priors: $\\mathbf{p}^{(2)} = [0.25, 0.25, 0.25, 0.25]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(2)} = \\begin{bmatrix}\n    0.50 & 0.50 & 0.50 & 0.50 \\\\\n    0.60 & 0.50 & 0.50 & 0.40 \\\\\n    0.50 & 0.40 & 0.60 & 0.50\n    \\end{bmatrix}.\n    $$\n- Test case $3$ (non-uniform priors, heterogeneous calibration and variances):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(3)} = \\begin{bmatrix}\n    0.9 & 0.2 & 0.2 & 0.2 \\\\\n    0.2 & 0.9 & 0.3 & 0.2 \\\\\n    0.2 & 0.3 & 0.9 & 0.2 \\\\\n    0.2 & 0.2 & 0.2 & 0.9\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(3)} = [0.1, 0.2, 0.1, 0.2]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(3)} = [0.05, 0.05, 0.05, 0.05]$.\n  - Scale per cycle: $\\mathbf{s}^{(3)} = [1.0, 0.8, 1.2]$.\n  - Priors: $\\mathbf{p}^{(3)} = [0.10, 0.40, 0.40, 0.10]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(3)} = \\begin{bmatrix}\n    0.95 & 0.13 & 0.10 & 0.10 \\\\\n    0.20 & 0.75 & 0.30 & 0.20 \\\\\n    0.20 & 0.25 & 1.10 & 0.20\n    \\end{bmatrix}.\n    $$\n- Test case $4$ (boundary case: zero intensities, low noise):\n  - Calibration matrix:\n    $$\n    \\mathbf{C}^{(4)} = \\begin{bmatrix}\n    1.0 & 0.1 & 0.1 & 0.1 \\\\\n    0.1 & 1.0 & 0.1 & 0.1 \\\\\n    0.1 & 0.1 & 1.0 & 0.1 \\\\\n    0.1 & 0.1 & 0.1 & 1.0\n    \\end{bmatrix}.\n    $$\n  - Variances: $\\boldsymbol{\\sigma}^{2\\,(4)} = [0.05, 0.05, 0.05, 0.05]$.\n  - Baseline: $\\boldsymbol{\\beta}^{(4)} = [0.0, 0.0, 0.0, 0.0]$.\n  - Scale per cycle: $\\mathbf{s}^{(4)} = [1.0, 1.0]$.\n  - Priors: $\\mathbf{p}^{(4)} = [0.25, 0.25, 0.25, 0.25]$.\n  - Observations:\n    $$\n    \\mathbf{Y}^{(4)} = \\begin{bmatrix}\n    0.00 & 0.00 & 0.00 & 0.00 \\\\\n    0.00 & 0.00 & 0.00 & 0.00\n    \\end{bmatrix}.\n    $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list: the first list contains the integer base calls per cycle using the mapping $A \\rightarrow 0$, $C \\rightarrow 1$, $G \\rightarrow 2$, $T \\rightarrow 3$, and the second list contains the corresponding Phred quality scores per cycle rounded to two decimal places. For example, the output format must be\n$$\n[\\,[\\,[\\text{calls}^{(1)}],\\,[\\text{qscores}^{(1)}]\\,],\\,\\ldots,\\,[\\,[\\text{calls}^{(4)}],\\,[\\text{qscores}^{(4)}]\\,]\\,].\n$$",
            "solution": "The problem requires the implementation of a statistical base-calling algorithm for a simplified four-channel sequencing-by-synthesis system. We are asked to determine the most likely DNA base for each sequencing cycle and to quantify the confidence in that call using a Phred quality score. This will be accomplished by applying principles of probability theory, specifically maximum likelihood estimation and Bayesian inference, to the provided generative signal model.\n\nThe process for each cycle $i$ involves two main computations:\n1.  **Maximum Likelihood (ML) Base Calling**: To find the base $\\hat{b}_i$ that is most likely to have generated the observed intensity vector $\\mathbf{y}_i$.\n2.  **Phred Quality Score ($Q_i$) Calculation**: To compute a quality score for the call $\\hat{b}_i$ based on its posterior error probability.\n\nWe will analyze each step based on fundamental principles.\n\n**Principle 1: The Gaussian Signal Model and Likelihood**\n\nThe generative model for the observed intensity vector $\\mathbf{y}_i \\in \\mathbb{R}^4$ at cycle $i$ for a given base $b \\in \\{A,C,G,T\\}$ is:\n$$\n\\mathbf{y}_i = s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}_i, \\quad \\boldsymbol{\\varepsilon}_i \\sim \\mathcal{N}\\!\\left(\\mathbf{0}, \\operatorname{diag}(\\boldsymbol{\\sigma}^2)\\right)\n$$\nwhere $\\boldsymbol{\\mu}_b$ is the column of the calibration matrix $\\mathbf{C}$ corresponding to base $b$. This equation states that the observed signal is the sum of a scaled ideal signal $s_i \\boldsymbol{\\mu}_b$, a constant background baseline $\\boldsymbol{\\beta}$, and additive Gaussian noise $\\boldsymbol{\\varepsilon}_i$.\n\nThe model implies that the conditional probability of observing $\\mathbf{y}_i$ given base $b$ follows a multivariate normal distribution:\n$$\n\\mathbf{y}_i \\mid b \\sim \\mathcal{N}\\!\\left(s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta}, \\operatorname{diag}(\\boldsymbol{\\sigma}^2)\\right)\n$$\nThe probability density function (PDF), which serves as the likelihood $p(\\mathbf{y}_i \\mid b)$, is:\n$$\np(\\mathbf{y}_i \\mid b) = \\frac{1}{\\sqrt{(2\\pi)^4 \\det(\\operatorname{diag}(\\boldsymbol{\\sigma}^2))}} \\exp\\left(-\\frac{1}{2} (\\mathbf{y}_i - (s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta}))^T (\\operatorname{diag}(\\boldsymbol{\\sigma}^2))^{-1} (\\mathbf{y}_i - (s_i \\boldsymbol{\\mu}_b + \\boldsymbol{\\beta}))\\right)\n$$\nSince the noise covariance matrix $\\boldsymbol{\\Sigma} = \\operatorname{diag}(\\boldsymbol{\\sigma}^2)$ is diagonal, the channels are independent. The exponent's argument, known as the squared Mahalanobis distance, simplifies to a weighted sum of squared errors (WSSE):\n$$\n\\chi^2(\\mathbf{y}_i, b) = \\sum_{j=0}^{3} \\frac{(y_{i,j} - (s_i C_{j,b} + \\beta_j))^2}{\\sigma_j^2}\n$$\nwhere the base index $b$ maps to the corresponding column of $\\mathbf{C}$, and $j \\in \\{0,1,2,3\\}$ indexes the four channels.\n\n**Principle 2: Maximum Likelihood Base Calling**\n\nThe ML base call $\\hat{b}_i$ is the base that maximizes the likelihood function:\n$$\n\\hat{b}_i = \\arg\\max_{b \\in \\{A,C,G,T\\}} p(\\mathbf{y}_i \\mid b)\n$$\nMaximizing the likelihood $p(\\mathbf{y}_i \\mid b)$ is equivalent to maximizing its natural logarithm, $\\ln p(\\mathbf{y}_i \\mid b)$, which is computationally more stable.\n$$\n\\ln p(\\mathbf{y}_i \\mid b) = -\\frac{1}{2} \\sum_{j=0}^{3} \\ln(2\\pi \\sigma_j^2) - \\frac{1}{2} \\sum_{j=0}^{3} \\frac{(y_{i,j} - (s_i C_{j,b} + \\beta_j))^2}{\\sigma_j^2}\n$$\nSince the first term is constant with respect to the base $b$, maximizing the log-likelihood is equivalent to minimizing the WSSE term, $\\chi^2(\\mathbf{y}_i, b)$.\n\nThe algorithm for ML base calling is as follows:\n1.  For each cycle $i$ and for each of the four bases $b$, calculate the log-likelihood $\\ln p(\\mathbf{y}_i \\mid b)$.\n2.  Convert the log-likelihoods to linear-scale likelihoods: $L_b = \\exp(\\ln p(\\mathbf{y}_i \\mid b))$.\n3.  Find the maximum likelihood, $L_{max} = \\max_{b} L_b$.\n4.  Identify the set of candidate bases $S_{cand} = \\{ b \\mid L_{max} - L_b  \\tau \\}$, where $\\tau=10^{-12}$ is the given tolerance.\n5.  The final base call $\\hat{b}_i$ is the base in $S_{cand}$ with the smallest lexicographical index ($A \\rightarrow 0, C \\rightarrow 1, G \\rightarrow 2, T \\rightarrow 3$).\n\n**Principle 3: Bayesian Inference and Phred Quality Score**\n\nThe Phred quality score $Q_i$ is derived from the posterior probability of error, $P_{\\text{err},i}$. To calculate this, we first need the posterior probability of each base $b$ given the observation $\\mathbf{y}_i$, denoted $P(b \\mid \\mathbf{y}_i)$. Using Bayes' theorem:\n$$\nP(b \\mid \\mathbf{y}_i) = \\frac{p(\\mathbf{y}_i \\mid b) p(b)}{p(\\mathbf{y}_i)} = \\frac{p(\\mathbf{y}_i \\mid b) p(b)}{\\sum_{b'} p(\\mathbf{y}_i \\mid b') p(b')}\n$$\nwhere $p(b)$ are the given prior probabilities for each base, and the denominator is the marginal likelihood (evidence), a normalization constant ensuring the posterior probabilities sum to $1$.\n\nThe posterior error probability for the call $\\hat{b}_i$ is the probability that the true base was any other base:\n$$\nP_{\\text{err},i} = P(\\text{base} \\neq \\hat{b}_i \\mid \\mathbf{y}_i) = 1 - P(\\hat{b}_i \\mid \\mathbf{y}_i) = \\sum_{b \\neq \\hat{b}_i} P(b \\mid \\mathbf{y}_i)\n$$\nThe Phred score is then defined as:\n$$\nQ_i = -10 \\log_{10}(P_{\\text{err},i})\n$$\n\n**Principle 4: Numerically Stable Computation**\n\nDirect computation of likelihoods and posterior probabilities can lead to numerical underflow, as these values can be extremely small. It is standard practice to perform calculations in log-space. Let $\\lambda_b = \\ln(p(\\mathbf{y}_i \\mid b) p(b)) = \\ln p(\\mathbf{y}_i \\mid b) + \\ln p(b)$ be the log-joint probability. The log of the posterior is:\n$$\n\\ln P(b \\mid \\mathbf{y}_i) = \\lambda_b - \\ln\\left(\\sum_{b'} \\exp(\\lambda_{b'})\\right)\n$$\nThe summation term is computed stably using the log-sum-exp trick. Let $\\lambda_{max} = \\max_{b'} \\lambda_{b'}$. Then:\n$$\n\\ln\\left(\\sum_{b'} \\exp(\\lambda_{b'})\\right) = \\lambda_{max} + \\ln\\left(\\sum_{b'} \\exp(\\lambda_{b'} - \\lambda_{max})\\right)\n$$\nThis avoids overflow in the exponentiation and underflow in the sum. Once the log-posteriors $\\ln P(b \\mid \\mathbf{y}_i)$ are computed, we can find $P(\\hat{b}_i \\mid \\mathbf{y}_i) = \\exp(\\ln P(\\hat{b}_i \\mid \\mathbf{y}_i))$ and subsequently $P_{\\text{err},i}$. The problem specifies that if $P_{\\text{err},i}$ is numerically zero, it should be treated as $10^{-300}$ to prevent an undefined logarithm when calculating $Q_i$.\n\nBy combining these principles, we can construct an algorithm to process the provided test suite and generate the required base calls and quality scores.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the DNA sequencing base calling problem for all test cases.\n    \"\"\"\n    \n    # Base mapping and problem constants\n    base_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    bases = ['A', 'C', 'G', 'T']\n    num_bases = 4\n    tau = 1e-12\n    p_err_floor = 1e-300\n\n    # Test Suite\n    test_cases = [\n        # Test case 1\n        {\n            \"C\": np.array([\n                [1.0, 0.1, 0.1, 0.1],\n                [0.1, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 1.0]\n            ]),\n            \"sigma_sq\": np.array([0.05, 0.05, 0.05, 0.05]),\n            \"beta\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"p\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"Y\": np.array([\n                [1.20, 0.10, 0.10, 0.10],\n                [0.10, 1.10, 0.10, 0.10],\n                [0.20, 0.10, 1.30, 0.10],\n                [0.10, 0.10, 0.10, 1.25],\n                [0.30, 0.20, 0.90, 0.20]\n            ])\n        },\n        # Test case 2\n        {\n            \"C\": np.array([\n                [1.0, 0.1, 0.1, 0.1],\n                [0.1, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 1.0]\n            ]),\n            \"sigma_sq\": np.array([0.5, 0.5, 0.5, 0.5]),\n            \"beta\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([1.0, 1.0, 1.0]),\n            \"p\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"Y\": np.array([\n                [0.50, 0.50, 0.50, 0.50],\n                [0.60, 0.50, 0.50, 0.40],\n                [0.50, 0.40, 0.60, 0.50]\n            ])\n        },\n        # Test case 3\n        {\n            \"C\": np.array([\n                [0.9, 0.2, 0.2, 0.2],\n                [0.2, 0.9, 0.3, 0.2],\n                [0.2, 0.3, 0.9, 0.2],\n                [0.2, 0.2, 0.2, 0.9]\n            ]),\n            \"sigma_sq\": np.array([0.1, 0.2, 0.1, 0.2]),\n            \"beta\": np.array([0.05, 0.05, 0.05, 0.05]),\n            \"s\": np.array([1.0, 0.8, 1.2]),\n            \"p\": np.array([0.10, 0.40, 0.40, 0.10]),\n            \"Y\": np.array([\n                [0.95, 0.13, 0.10, 0.10],\n                [0.20, 0.75, 0.30, 0.20],\n                [0.20, 0.25, 1.10, 0.20]\n            ])\n        },\n        # Test case 4\n        {\n            \"C\": np.array([\n                [1.0, 0.1, 0.1, 0.1],\n                [0.1, 1.0, 0.1, 0.1],\n                [0.1, 0.1, 1.0, 0.1],\n                [0.1, 0.1, 0.1, 1.0]\n            ]),\n            \"sigma_sq\": np.array([0.05, 0.05, 0.05, 0.05]),\n            \"beta\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"s\": np.array([1.0, 1.0]),\n            \"p\": np.array([0.25, 0.25, 0.25, 0.25]),\n            \"Y\": np.array([\n                [0.00, 0.00, 0.00, 0.00],\n                [0.00, 0.00, 0.00, 0.00]\n            ])\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        C, sigma_sq, beta, s_vec, p, Y = case[\"C\"], case[\"sigma_sq\"], case[\"beta\"], case[\"s\"], case[\"p\"], case[\"Y\"]\n        \n        calls_per_case = []\n        qscores_per_case = []\n        \n        num_cycles = Y.shape[0]\n        \n        log_priors = np.log(p)\n        log_likelihood_const = -0.5 * np.sum(np.log(2 * np.pi * sigma_sq))\n\n        for i in range(num_cycles):\n            y_i = Y[i, :]\n            s_i = s_vec[i]\n            \n            log_likelihoods = np.zeros(num_bases)\n            \n            for b_idx in range(num_bases):\n                mu_b = C[:, b_idx]\n                expected_mean = s_i * mu_b + beta\n                \n                # Calculate WSSE (chi-squared)\n                wsse = np.sum(((y_i - expected_mean)**2) / sigma_sq)\n                \n                # Log likelihood\n                log_likelihoods[b_idx] = log_likelihood_const - 0.5 * wsse\n            \n            # --- Maximum Likelihood Base Call ---\n            likelihoods = np.exp(log_likelihoods)\n            max_likelihood = np.max(likelihoods)\n            tied_indices = np.where(max_likelihood - likelihoods  tau)[0]\n            ml_base_call = np.min(tied_indices)\n            calls_per_case.append(ml_base_call)\n            \n            # --- Phred Quality Score Calculation ---\n            # Log joint probabilities\n            log_joint = log_likelihoods + log_priors\n            \n            # Log-sum-exp for normalization\n            log_joint_max = np.max(log_joint)\n            log_marginal = log_joint_max + np.log(np.sum(np.exp(log_joint - log_joint_max)))\n            \n            # Log posteriors\n            log_posteriors = log_joint - log_marginal\n            posteriors = np.exp(log_posteriors)\n            \n            p_correct = posteriors[ml_base_call]\n            p_err = 1.0 - p_correct\n            \n            # Handle numerical zero\n            if p_err = p_err_floor:\n                p_err = p_err_floor\n            \n            q_score = -10 * np.log10(p_err)\n            qscores_per_case.append(round(q_score, 2))\n            \n        all_results.append([calls_per_case, qscores_per_case])\n\n    # Format the final output string\n    result_strings = []\n    for calls, qscores in all_results:\n        calls_str = f\"[{','.join(map(str, calls))}]\"\n        qscores_str = f\"[{','.join([f'{q:.2f}' for q in qscores])}]\"\n        result_strings.append(f\"[{calls_str},{qscores_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "With high-quality sequencing reads in hand, a primary task is to determine their origin within a reference genome. This is the read mapping problem. This final practice guides you through the implementation of a modern, efficient solution using the seed-and-extend paradigm powered by the Ferragina-Manzini (FM) index . You will build and use this remarkable data structure, derived from the Burrows-Wheeler Transform, to perform ultra-fast exact seed matching, providing a hands-on look at the engine that drives widely used alignment tools like BWA and Bowtie.",
            "id": "3310867",
            "problem": "You are given a reference deoxyribonucleic acid (DNA) string and are asked to implement a principled seed-and-extend candidate mapping enumerator using the Ferragina–Manzini (FM) index. The task is to compute, for each provided read, all candidate start positions in the reference at which the read can align with at most $k$ mismatches, subject to a seed filtration defined by an exact-seed match constraint.\n\nFoundational base and definitions to use:\n- DNA is modeled as a string over the alphabet $\\Sigma = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$.\n- The Central Dogma of Molecular Biology establishes that DNA sequences encode genetic information; at the computational level, read mapping is formulated as searching substrings within a reference genome.\n- The FM index is derived from the Burrows–Wheeler Transform (BWT) and allows efficient exact substring search via backward search. Construct the FM index on the string $S\\$$ where $S$ is the reference and $\\$$ is a sentinel character lexicographically smaller than any character in $\\Sigma$.\n- Let $n = |S\\$|$ denote the length of the text after appending the sentinel. The FM index comprises:\n  1. The BWT string $B$, defined by $B[i] = S\\$\\big[(\\mathrm{SA}[i] - 1) \\bmod n\\big]$, where $\\mathrm{SA}$ is the suffix array of $S\\$$.\n  2. The array $C(c)$ giving the total number of characters in $S\\$$ that are lexicographically smaller than $c$.\n  3. The occurrence function $\\mathrm{Occ}(c, i)$ returning the number of occurrences of character $c$ in the prefix $B[0:i)$, with $0 \\le i \\le n$.\n- Backward search is performed using the last-first (LF) mapping. For a pattern $P$ and an interval $[l, r)$ of suffix array ranks, updating with character $c$ yields a new interval\n  $$l' = C(c) + \\mathrm{Occ}(c, l), \\quad r' = C(c) + \\mathrm{Occ}(c, r),$$\n  iterated from the last character of $P$ to the first. If at any step $l' \\ge r'$, the pattern does not occur. Otherwise, the exact occurrences of $P$ are given by the suffix array positions $\\mathrm{SA}[l:r)$.\n\nSeed-and-extend filtration:\n- Given a read $R$ of length $L$, a seed length $m$, and an integer $f \\ge 1$, form disjoint seeds by taking substrings $R[o_j : o_j + m]$ at offsets $o_j = jm$ for all integers $j$ such that $o_j + m \\le L$. Let $t = \\left\\lfloor \\frac{L}{m} \\right\\rfloor$ denote the number of such seeds.\n- A candidate start position $x$ in $S$ passes the filtration if at least $f$ of these seeds have exact matches in $S$ consistent with alignment at $x$. Formally, for a seed at offset $o$, any exact match position $p$ of that seed in $S$ implies a candidate alignment start $x = p - o$. Count how many seeds support a given $x$, and retain $x$ only if the count is at least $f$.\n- After filtration, perform extension: compute the number of mismatches between $R$ and $S[x : x + L]$ and retain $x$ if this number is at most $k$. Valid starts must satisfy $0 \\le x \\le |S| - L$.\n\nPrincipled guarantee to use for parameter selection:\n- For allowing up to $k$ mismatches, a classical pigeonhole principle argument ensures that if the read is partitioned into $k+1$ disjoint segments, at least one segment must be mismatch-free. This motivates taking $m \\approx \\left\\lfloor \\frac{L}{k + 1} \\right\\rfloor$ and a filtration requirement $f = 1$ for completeness. Stronger filtration criteria with $f > 1$ are valid but may exclude true mappings when mismatches exist.\n\nAlgorithmic requirements:\n- Construct the FM index for the provided reference $S$.\n- For each read and parameter set $(L, k, m, f)$, compute the set of candidate start positions in $S$ that satisfy the filtration and mismatch constraint, using only exact FM-index seed search plus extension by direct comparison.\n- All intervals and indexing must be self-consistent. Use zero-based indexing for positions in $S$. The sentinel index and out-of-range candidates must be discarded.\n\nTest suite:\n- Use the reference $S = \\text{\"ACGTACGTACGTACGTACGTACGT\"}$, with $|S| = 24$.\n- For each test case, the program should produce the sorted list of valid candidate starts in ascending order.\n- Test cases:\n  1. Case A (happy path): $R_1 = \\text{\"ACGTACGTACG\"}$, $L = 11$, $k = 1$, $m = 5$, $f = 1$.\n  2. Case B (boundary $k=0$): $R_2 = \\text{\"GTACGT\"}$, $L = 6$, $k = 0$, $m = 3$, $f = 2$.\n  3. Case C (boundary near ends): $R_3 = \\text{\"ACGTACG\"}$, $L = 7$, $k = 2$, $m = 3$, $f = 1$.\n  4. Case D (strict filtration edge): $R_4 = \\text{\"AAGTACGTA\"}$, $L = 9$, $k = 1$, $m = 4$, $f = 2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\text{[result1,result2,result3,result4]}$), where each $\\text{result}$ is itself a list of integers denoting the candidate start positions for the corresponding test case.\n- The lists must be sorted in ascending order, and the overall output must be exactly one line with no additional text.",
            "solution": "The task is to implement a seed-and-extend read mapping algorithm that enumerates all candidate start positions for a given DNA read within a reference sequence. The solution must adhere to specified algorithmic principles, employing the Ferragina–Manzini (FM) index for efficient seed searching. The process involves three main stages: FM-index construction, seed-based filtration, and extension-based verification.\n\n### 1. FM-Index Construction\n\nThe FM-index is a compressed full-text index that allows for efficient counting and locating of arbitrary patterns in a text. Its construction is a prerequisite for the mapping algorithm. We begin with the reference DNA string $S$.\n\n_Step 1: Text Preparation_\nFirst, a sentinel character, $\\$$, which is lexicographically smaller than any character in the DNA alphabet $\\Sigma = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$, is appended to the reference string $S$. This creates the text $T = S\\$$ of length $n = |S| + 1$. The sentinel ensures that every suffix of $T$ is unique and that the suffix array corresponds to a permutation of indices from $0$ to $n-1$.\n\n_Step 2: Suffix Array (SA)_\nThe suffix array, $\\mathrm{SA}$, of $T$ is an array of integers of length $n$. $\\mathrm{SA}[i]$ stores the starting position of the $i$-th lexicographically smallest suffix of $T$. For a small text like the one provided, the SA can be constructed by generating all suffixes, pairing them with their starting indices, and sorting these pairs.\n\n_Step 3: Burrows–Wheeler Transform (BWT)_\nThe BWT generates a permutation of the characters of $T$, denoted as the string $B$. It is defined by the relation $B[i] = T[(\\mathrm{SA}[i] - 1) \\bmod n]$ for $i \\in [0, n-1]$. Conceptually, $B$ is the last column of a matrix whose rows are all cyclic shifts of $T$ sorted lexicographically. This string $B$ has the crucial property of tending to group identical characters together, making it highly compressible and suitable for efficient queries.\n\n_Step 4: C-Table and Occurrence Function_\nTo enable efficient searching, two auxiliary data structures are built:\n1.  **C-Table**: The array $C(c)$ stores the total count of characters in $T$ that are lexicographically smaller than character $c$. This table allows us to instantly find the starting-rank block in the suffix array for all suffixes beginning with a specific character $c$.\n2.  **Occurrence Function ($\\mathrm{Occ}$)**: The function $\\mathrm{Occ}(c, i)$ returns the number of times character $c$ appears in the prefix of the BWT string, $B[0:i)$. For computational efficiency, this is pre-calculated and stored in a $2$D-array, where one dimension represents the characters of the alphabet and the other represents the positions in $B$.\n\n### 2. Exact Seed Search via Backward Search\n\nWith the FM-index constructed, we can perform highly efficient exact string matching using an algorithm known as backward search. This algorithm finds the suffix array interval $[l, r)$ corresponding to all suffixes that start with a given pattern $P$.\n\nThe search proceeds by iterating through the characters of the pattern $P$ from right to left. Starting with the full SA interval $[l, r) = [0, n)$, each character $c$ from $P$ updates the interval according to the last-first (LF) mapping property:\n$$l_{\\text{new}} = C(c) + \\mathrm{Occ}(c, l_{\\text{old}})$$\n$$r_{\\text{new}} = C(c) + \\mathrm{Occ}(c, r_{\\text{old}})$$\nIf at any point $l_{\\text{new}} \\ge r_{\\text{new}}$, the pattern $P$ does not exist in the text $T$. If the loop completes, the final interval $[l, r)$ identifies the range of suffixes, $\\mathrm{SA}[l \\dots r-1]$, that are prefixed by $P$. The values $\\mathrm{SA}[i]$ for $i \\in [l, r)$ are the starting positions of the exact matches of $P$ in $T$.\n\n### 3. Seed-and-Extend Filtration and Verification\n\nThis strategy uses short, exact matches (seeds) to rapidly identify a small set of promising candidate alignment locations, which are then verified in a more costly extension step.\n\n_Step 1: Seeding_\nGiven a read $R$ of length $L$, a seed length $m$, and a filtration threshold $f$, the read is partitioned into $t = \\lfloor L/m \\rfloor$ disjoint seeds. The $j$-th seed (using $0$-based indexing) is the substring $R[o_j : o_j + m]$, where the offset is $o_j = jm$ for $j \\in [0, t-1]$.\n\n_Step 2: Filtration_\nFor each of the $t$ seeds, we use the backward search algorithm on the FM-index to find all its exact match positions $\\{p_0, p_1, \\ldots\\}$ in the reference string $S$. Each match position $p$ for a seed at offset $o_j$ in the read implies a candidate start position for the entire read, calculated as $x = p - o_j$.\nA counter is maintained for each potential start position $x$. We iterate through all seeds and all their matches, incrementing the counter for the corresponding $x$ each time it is implied. After processing all seeds, we apply the filtration criterion: a candidate start position $x$ is retained only if its counter is at least $f$. This means at least $f$ distinct seeds from the read support an alignment starting at $x$.\n\n_Step 3: Extension and Verification_\nThe final stage verifies the filtered candidates. For each candidate start position $x$ that passes filtration, we perform two checks:\n1.  **Boundary Check**: The alignment must lie entirely within the reference $S$. This requires $0 \\le x \\le |S| - L$. Candidates outside this range are discarded.\n2.  **Mismatch Count**: The number of mismatches between the read $R$ and the corresponding reference substring $S[x : x + L]$ is computed. If this count is less than or equal to the permissible maximum, $k$, the position $x$ is accepted as a valid mapping location.\n\nThe final output for each read is the sorted list of all such validated start positions. This principled combination of compressed indexing and heuristic filtration allows for an efficient yet sensitive read mapping process.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import Counter\n\nclass FMIndex:\n    \"\"\"\n    An implementation of the Ferragina–Manzini (FM) index.\n    \"\"\"\n    def __init__(self, text, alphabet):\n        \"\"\"\n        Constructs the FM index for a given text.\n        Args:\n            text (str): The input string, ending with a sentinel '$'.\n            alphabet (list): A list of characters in the alphabet, sorted lexicographically.\n        \"\"\"\n        self.text = text\n        self.alphabet = alphabet\n        self.char_map = {c: i for i, c in enumerate(self.alphabet)}\n        self.n = len(text)\n\n        # 1. Suffix Array (SA) construction\n        suffixes = sorted([(self.text[i:], i) for i in range(self.n)])\n        self.sa = np.array([s[1] for s in suffixes], dtype=np.int32)\n\n        # 2. Burrows–Wheeler Transform (BWT) string B\n        bwt_chars = []\n        for i in range(self.n):\n            bwt_chars.append(self.text[(self.sa[i] - 1) % self.n])\n        self.bwt = \"\".join(bwt_chars)\n\n        # 3. C-Table (counts of chars lexicographically smaller than c)\n        self.c_table = {}\n        counts = Counter(self.text)\n        total = 0\n        for char in self.alphabet:\n            self.c_table[char] = total\n            total += counts.get(char, 0)\n        \n        # 4. Occurrence (Occ) table\n        self.occ = np.zeros((len(self.alphabet), self.n + 1), dtype=np.int32)\n        for i in range(self.n):\n            self.occ[:, i + 1] = self.occ[:, i]\n            if self.bwt[i] in self.char_map:\n                char_idx = self.char_map[self.bwt[i]]\n                self.occ[char_idx, i + 1] += 1\n\n    def _get_occ(self, char, index):\n        \"\"\" Helper to query the Occ table. \"\"\"\n        return self.occ[self.char_map[char], index]\n\n    def search(self, pattern):\n        \"\"\"\n        Performs backward search to find exact matches of a pattern.\n        Args:\n            pattern (str): The pattern to search for.\n        Returns:\n            list: A sorted list of starting positions of the pattern in the original text.\n        \"\"\"\n        if not pattern:\n            return []\n        \n        l, r = 0, self.n\n        for char in reversed(pattern):\n            if char not in self.char_map:\n                return []\n            \n            l = self.c_table[char] + self._get_occ(char, l)\n            r = self.c_table[char] + self._get_occ(char, r)\n            \n            if l = r:\n                return []\n        \n        return sorted([self.sa[i] for i in range(l, r)])\n\ndef find_candidates(fm_index, S, R, k, m, f):\n    \"\"\"\n    Implements the seed-and-extend mapping algorithm.\n    Args:\n        fm_index (FMIndex): The pre-computed FM index of the reference.\n        S (str): The reference DNA string.\n        R (str): The read DNA string.\n        k (int): Maximum allowed mismatches.\n        m (int): Seed length.\n        f (int): Minimum number of seeds required to support a candidate.\n    Returns:\n        list: A sorted list of valid start positions.\n    \"\"\"\n    L = len(R)\n    s_len = len(S)\n    \n    candidate_supports = Counter()\n    num_seeds = L // m\n    \n    for j in range(num_seeds):\n        offset = j * m\n        seed = R[offset : offset + m]\n        \n        match_positions = fm_index.search(seed)\n        \n        for p in match_positions:\n            candidate_start = p - offset\n            candidate_supports[candidate_start] += 1\n            \n    filtered_starts = []\n    for start_pos, count in candidate_supports.items():\n        if count = f:\n            filtered_starts.append(start_pos)\n            \n    valid_starts = []\n    for x in sorted(filtered_starts):\n        if 0 = x = s_len - L:\n            mismatches = 0\n            ref_substring = S[x : x + L]\n            for i in range(L):\n                if R[i] != ref_substring[i]:\n                    mismatches += 1\n            if mismatches = k:\n                valid_starts.append(x)\n                \n    return valid_starts\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    S = \"ACGTACGTACGTACGTACGTACGT\"\n    alphabet = ['$', 'A', 'C', 'G', 'T']\n    \n    text_with_sentinel = S + '$'\n    fm_index = FMIndex(text_with_sentinel, alphabet)\n\n    test_cases = [\n        # (R, L, k, m, f) - L is provided but len(R) is used as it's equivalent\n        (\"ACGTACGTACG\", 11, 1, 5, 1), # Case A\n        (\"GTACGT\", 6, 0, 3, 2),       # Case B\n        (\"ACGTACG\", 7, 2, 3, 1),       # Case C\n        (\"AAGTACGTA\", 9, 1, 4, 2)     # Case D\n    ]\n\n    results = []\n    for R, L, k, m, f in test_cases:\n        result = find_candidates(fm_index, S, R, k, m, f)\n        results.append(result)\n\n    # Format the output string to be exactly as required\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}