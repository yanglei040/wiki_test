{
    "hands_on_practices": [
        {
            "introduction": "When we perform many hypothesis tests simultaneously, the chance of making at least one false discovery inflates dramatically. Controlling the Family-Wise Error Rate (FWER) is the most stringent approach to managing this risk. This exercise will guide you through the mechanics of three classical FWER control procedures—Bonferroni, Holm, and Hochberg—allowing you to compare their relative power and understand their step-by-step application on a common set of $p$-values. ",
            "id": "3351050",
            "problem": "In a computational systems biology study, a researcher evaluates differential pathway activity across two conditions for a curated set of five disjoint pathways, each represented by an aggregate score derived from independent gene sets. For each pathway, a hypothesis test is performed comparing the activity scores across conditions using a well-calibrated test that yields valid $p$-values. The pathway-level tests are assumed to satisfy the conditions under which stepwise Family-Wise Error Rate (FWER) control procedures are valid, such as independence or positive dependence of the test statistics.\n\nThe researcher aims to control the Family-Wise Error Rate (FWER) at level $\\alpha=0.05$ using three classical procedures: the Bonferroni correction, the Holm step-down procedure, and the Hochberg step-up procedure. The ordered $p$-values across the five pathways are $p_{(1)}=0.002$, $p_{(2)}=0.01$, $p_{(3)}=0.03$, $p_{(4)}=0.07$, and $p_{(5)}=0.2$, with $m=5$ total hypotheses.\n\nStarting from the core definitions of FWER and valid multiple testing procedures, determine how many null hypotheses are rejected by each method. Report your final answer as a row of three numbers in the order (Bonferroni, Holm, Hochberg). No rounding is required.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- Total number of hypotheses (pathways), $m = 5$.\n- Desired Family-Wise Error Rate (FWER) control level, $\\alpha = 0.05$.\n- The five hypothesis tests are for disjoint pathways, and their test statistics are assumed to be independent or exhibit positive dependence, which validates the use of the Holm and Hochberg procedures.\n- The ordered $p$-values from the hypothesis tests are:\n  - $p_{(1)} = 0.002$\n  - $p_{(2)} = 0.01$\n  - $p_{(3)} = 0.03$\n  - $p_{(4)} = 0.07$\n  - $p_{(5)} = 0.2$\n- The three multiple testing procedures to be evaluated are: Bonferroni correction, Holm step-down procedure, and Hochberg step-up procedure.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in statistical inference, specifically multiple hypothesis testing, a critical topic in computational systems biology and many other scientific fields. The Bonferroni, Holm, and Hochberg procedures are standard, well-established methods for controlling the FWER. The assumptions provided (independence or positive dependence) are standard conditions under which these procedures are proven to be valid.\n- **Well-Posed:** The problem is clearly defined with all necessary numerical values ($m$, $\\alpha$, and the set of $p$-values) and a specific objective (determine the number of rejections for each method). A unique and meaningful solution can be derived by applying the definitions of the three procedures.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased language. There are no subjective elements or opinions.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a direct and well-formulated application of fundamental principles in multiple hypothesis testing. The solution will proceed.\n\nThe objective is to determine the number of null hypotheses rejected by each of three multiple testing procedures designed to control the Family-Wise Error Rate (FWER) at a level $\\alpha = 0.05$. The FWER is the probability of making at least one Type I error (falsely rejecting a true null hypothesis) among a family of hypothesis tests. We are given $m=5$ hypotheses with ordered $p$-values $p_{(1)} \\le p_{(2)} \\le p_{(3)} \\le p_{(4)} \\le p_{(5)}$.\n\n**1. Bonferroni Correction**\n\nThe Bonferroni correction is a single-step method that rejects any null hypothesis $H_i$ if its corresponding $p$-value $p_i$ satisfies the inequality:\n$$p_i \\le \\frac{\\alpha}{m}$$\nIn this case, the adjusted significance threshold is $\\frac{0.05}{5} = 0.01$. We compare each of the $p$-values to this threshold.\n- $p_{(1)} = 0.002 \\le 0.01$. The null hypothesis $H_{(1)}$ is rejected.\n- $p_{(2)} = 0.01 \\le 0.01$. The null hypothesis $H_{(2)}$ is rejected.\n- $p_{(3)} = 0.03 > 0.01$. The null hypothesis $H_{(3)}$ is not rejected.\n- $p_{(4)} = 0.07 > 0.01$. The null hypothesis $H_{(4)}$ is not rejected.\n- $p_{(5)} = 0.2 > 0.01$. The null hypothesis $H_{(5)}$ is not rejected.\n\nThe Bonferroni procedure rejects two hypotheses.\n\n**2. Holm Step-Down Procedure**\n\nThe Holm procedure is a sequential step-down method that is uniformly more powerful than the Bonferroni correction. The procedure tests the ordered $p$-values against sequentially less conservative thresholds. For $j = 1, 2, \\dots, m$, we compare $p_{(j)}$ to $\\frac{\\alpha}{m - j + 1}$. The procedure starts with the smallest $p$-value and stops at the first instance where the inequality is not met.\n\n- **Step 1 ($j=1$):** Compare $p_{(1)}$ with $\\frac{\\alpha}{m - 1 + 1} = \\frac{0.05}{5} = 0.01$.\n  $p_{(1)} = 0.002 \\le 0.01$. The condition is met. We reject $H_{(1)}$ and proceed to the next step.\n\n- **Step 2 ($j=2$):** Compare $p_{(2)}$ with $\\frac{\\alpha}{m - 2 + 1} = \\frac{0.05}{4} = 0.0125$.\n  $p_{(2)} = 0.01 \\le 0.0125$. The condition is met. We reject $H_{(2)}$ and proceed to the next step.\n\n- **Step 3 ($j=3$):** Compare $p_{(3)}$ with $\\frac{\\alpha}{m - 3 + 1} = \\frac{0.05}{3} \\approx 0.0167$.\n  $p_{(3)} = 0.03 > 0.0167$. The condition is not met. The procedure stops.\n\nAccording to the Holm procedure, we reject hypotheses $H_{(1)}$ and $H_{(2)}$ and fail to reject $H_{(3)}$, $H_{(4)}$, and $H_{(5)}$. Thus, the Holm procedure rejects two hypotheses.\n\n**3. Hochberg Step-Up Procedure**\n\nThe Hochberg procedure is a sequential step-up method that is uniformly more powerful than the Holm procedure (given the stated assumptions of independence or positive dependence). It proceeds from the largest $p$-value to the smallest. The goal is to find the largest index $k \\in \\{1, 2, \\dots, m\\}$ such that:\n$$p_{(k)} \\le \\frac{\\alpha}{m - k + 1}$$\nIf such a $k$ is found, all hypotheses $H_{(1)}, \\dots, H_{(k)}$ are rejected.\n\n- **Step 1 (test for $k=5$):** Compare $p_{(5)}$ with $\\frac{\\alpha}{m - 5 + 1} = \\frac{0.05}{1} = 0.05$.\n  $p_{(5)} = 0.2 > 0.05$. The condition is not met.\n\n- **Step 2 (test for $k=4$):** Compare $p_{(4)}$ with $\\frac{\\alpha}{m - 4 + 1} = \\frac{0.05}{2} = 0.025$.\n  $p_{(4)} = 0.07 > 0.025$. The condition is not met.\n\n- **Step 3 (test for $k=3$):** Compare $p_{(3)}$ with $\\frac{\\alpha}{m - 3 + 1} = \\frac{0.05}{3} \\approx 0.0167$.\n  $p_{(3)} = 0.03 > 0.0167$. The condition is not met.\n\n- **Step 4 (test for $k=2$):** Compare $p_{(2)}$ with $\\frac{\\alpha}{m - 2 + 1} = \\frac{0.05}{4} = 0.0125$.\n  $p_{(2)} = 0.01 \\le 0.0125$. The condition is met.\n\nThe largest index $k$ that satisfies the inequality is $k=2$. Therefore, the Hochberg procedure rejects all hypotheses from $H_{(1)}$ to $H_{(k=2)}$, which includes $H_{(1)}$ and $H_{(2)}$. The Hochberg procedure rejects two hypotheses.\n\nIn summary:\n- Bonferroni rejection count: $2$\n- Holm rejection count: $2$\n- Hochberg rejection count: $2$\nThe final answer is the row of these three counts.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 2 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Many statistical tests rely on assumptions about the underlying data distribution, which may not hold in practice. Permutation tests offer a powerful, non-parametric alternative by generating an exact null distribution directly from the data under the principle of exchangeability. This coding practice challenges you to implement a permutation test from first principles, building a deep, computational intuition for what a $p$-value represents in a non-parametric context. ",
            "id": "3351012",
            "problem": "You are given a single molecular phenotype measured in $n = n_1 + n_2$ biological samples, with $n_1$ designated as “case” and $n_2$ designated as “control.” Under the null hypothesis of no association, labels are exchangeable, and an exact permutation test enumerates all $\\binom{n}{n_1}$ distinct labelings obtained by reassigning the $n$ observed measurements to the two groups while preserving group sizes. The two-sample $t$-statistic with the equal-variance pooled estimate is used as the test statistic. For any fixed labeling, if $\\bar{x}_1$ and $\\bar{x}_2$ denote the sample means in the two groups, $s_1^2$ and $s_2^2$ the unbiased sample variances, and $s_p$ the pooled standard deviation, then the statistic is defined for $n_1 \\ge 2$ and $n_2 \\ge 2$ by\n$$\nt \\;=\\; \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\,\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\,,\n\\quad \\text{with} \\quad\ns_p \\;=\\; \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\,.\n$$\nIf $s_p = 0$ and $\\bar{x}_1 \\neq \\bar{x}_2$, define $|t| = +\\infty$. If $s_p = 0$ and $\\bar{x}_1 = \\bar{x}_2$, define $t = 0$. The exact two-sided permutation $p$-value is the proportion of permutations for which $|t| \\ge |t_{\\mathrm{obs}}|$, where $t_{\\mathrm{obs}}$ is the observed statistic under the original labeling.\n\nYour task is to write a complete, runnable program that, for each of several specified test cases, computes this exact two-sided permutation $p$-value by enumerating all $\\binom{n}{n_1}$ labelings and counting the fraction with $|t| \\ge |t_{\\mathrm{obs}}|$. This is a direct consequence of exchangeability under the null hypothesis and requires no parametric approximation.\n\nAll answers must be produced as dimensionless decimal numbers. Comparisons of real numbers must use a tiny absolute tolerance of $\\varepsilon = 10^{-12}$ to avoid floating-point artifacts when deciding whether $|t| \\ge |t_{\\mathrm{obs}}|$.\n\nUse the following test suite. In all cases, $n_1 = 4$ and $n_2 = 5$ so that $\\binom{n}{n_1} = \\binom{9}{4} = 126$.\n\n- Test case $1$ (happy path, scientifically realistic, exactly constructed so that the observed statistic equals the stated threshold): Use the $n = 9$ observed measurements\n  $$\n  \\bigl[\n  0.3809919006126299,\\;\n  1.1555885698541132,\\;\n  1.9301852390955966,\\;\n  2.7047819083370800,\\;\n  -1.2649110640673518,\\;\n  -0.6324555320336759,\\;\n  0.0000000000000000,\\;\n  0.6324555320336759,\\;\n  1.2649110640673518\n  \\bigr]\n  $$\n  where the originally observed “cases” are the first $4$ entries and the “controls” are the last $5$ entries. The observed statistic for this labeling is $t_{\\mathrm{obs}} = 2.3$. Compute the exact two-sided permutation $p$-value as the proportion of the $\\binom{9}{4}$ labelings with $|t| \\ge 2.3$.\n\n- Test case $2$ (degenerate boundary, pooled variance identically zero): Use the $n = 9$ measurements\n  $$\n  \\bigl[ 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0 \\bigr],\n  $$\n  with $t_{\\mathrm{obs}} = 0.0$. By definition, if $s_p = 0$ and the group means are equal, set $t = 0$ for that labeling. Compute the exact two-sided permutation $p$-value as the proportion of the $\\binom{9}{4}$ labelings with $|t| \\ge 0.0$.\n\n- Test case $3$ (stringent threshold edge case): Reuse the measurements from Test case $1$ and set $t_{\\mathrm{obs}} = 10.0$. Compute the proportion of the $\\binom{9}{4}$ labelings with $|t| \\ge 10.0$.\n\nYour program must:\n- Implement the two-sample pooled $t$-statistic as defined above for each labeling.\n- Enumerate all $\\binom{n}{n_1}$ case labelings and count the number with $|t| \\ge |t_{\\mathrm{obs}}| - \\varepsilon$, using $\\varepsilon = 10^{-12}$.\n- Return for each test case the exact two-sided permutation $p$-value as a decimal number equal to the count divided by $\\binom{n}{n_1}$.\n\nFinal output format:\n- Your program should produce a single line containing the results for all test cases as a comma-separated list enclosed in square brackets, for example, $\\bigl[\\text{result}_1,\\text{result}_2,\\text{result}_3\\bigr]$.\n- Each result must be formatted as a decimal with exactly $6$ digits after the decimal point.\n\nNo external input is required. All constants and data are provided above. The program must be self-contained and runnable in any modern environment supporting the specified libraries and versions. Angles are not involved. All outputs are unitless. The output must be reproducible to the stated precision.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in the principles of non-parametric statistical inference, is well-posed with a clear and complete set of definitions and data, and is expressed in objective, formal language. We may therefore proceed with a complete solution.\n\nThe problem requires the computation of an exact two-sided permutation $p$-value for a two-sample $t$-statistic. This method is fundamental to statistical hypothesis testing in computational biology and other fields, particularly when the assumptions of parametric tests (e.g., normality of data) are not met or are undesirable to make.\n\nThe core principle underpinning the permutation test is exchangeability under the null hypothesis ($H_0$). The null hypothesis posits no association between the molecular phenotype and the case/control status. If $H_0$ is true, the assignment of \"case\" or \"control\" labels to the observed measurements is arbitrary. Any permutation of these labels is equally likely. The permutation test operationalizes this principle by generating a null distribution for a chosen test statistic. This is achieved by computing the statistic for all possible relabelings of the data, while preserving the original group sizes, $n_1$ and $n_2$.\n\nThe total number of distinct ways to assign $n_1$ \"case\" labels to a set of $n = n_1 + n_2$ measurements is given by the binomial coefficient $\\binom{n}{n_1}$. For each of these labelings, we compute the test statistic, $t$. The collection of these $t$-values forms the exact null distribution.\n\nThe test statistic prescribed is the two-sample $t$-statistic assuming equal variances:\n$$\nt \\;=\\; \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\,\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n$$\nwhere $\\bar{x}_1$ and $\\bar{x}_2$ are the sample means for the two groups in a given permutation, and $s_p$ is the pooled standard deviation:\n$$\ns_p \\;=\\; \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\n$$\nHere, $s_1^2$ and $s_2^2$ are the unbiased sample variances. The conditions $n_1 \\ge 2$ and $n_2 \\ge 2$ ensure that the sample variances and the denominator in the $s_p$ formula are well-defined.\n\nThe problem correctly specifies the handling of the degenerate case where the pooled standard deviation $s_p = 0$. This occurs if and only if the variance within each group is zero (i.e., all measurements within group $1$ are identical, and all measurements within group $2$ are identical).\n- If $s_p = 0$ and the group means are different ($\\bar{x}_1 \\neq \\bar{x}_2$), the effect size is maximal relative to the null variation, so $|t|$ is defined as $+\\infty$.\n- If $s_p = 0$ and the group means are identical ($\\bar{x}_1 = \\bar{x}_2$), there is no difference between the groups, so $t$ is defined as $0$.\n\nThe two-sided permutation $p$-value is the proportion of permutations in the null distribution for which the absolute value of the test statistic, $|t|$, is greater than or equal to the absolute value of the statistic observed with the original data labeling, $|t_{\\mathrm{obs}}|$.\n$$\np\\text{-value} \\;=\\; \\frac{\\text{Number of permutations with } |t| \\ge |t_{\\mathrm{obs}}|}{\\text{Total number of permutations}}\n$$\nTo manage floating-point arithmetic artifacts, the comparison is implemented as $|t| \\ge |t_{\\mathrm{obs}}| - \\varepsilon$ for a small tolerance $\\varepsilon = 10^{-12}$.\n\nThe computational strategy is as follows:\n1.  For each test case, take the vector of $n$ measurements and the group sizes $n_1$ and $n_2$.\n2.  Generate all unique combinations of indices of size $n_1$ from the set of all indices $\\{0, 1, \\dots, n-1\\}$. Each combination represents a possible assignment of measurements to the \"case\" group. The remaining $n_2$ indices will form the \"control\" group. For the given parameters $n_1=4$ and $n=9$, there are $\\binom{9}{4}=126$ such combinations.\n3.  For each of these $126$ permutations:\n    a. Partition the data into a case group ($x_1$) and a control group ($x_2$).\n    b. Compute the means $\\bar{x}_1, \\bar{x}_2$ and unbiased sample variances $s_1^2, s_2^2$.\n    c. Calculate the pooled standard deviation $s_p$, handling the case $s_p=0$ with a numerical tolerance.\n    d. Calculate the $t$-statistic, $t_{perm}$, applying the special definitions when $s_p = 0$.\n    e. Compare $|t_{perm}|$ with $|t_{\\mathrm{obs}}|$ using the specified tolerance. If $|t_{perm}| \\ge |t_{\\mathrm{obs}}| - \\varepsilon$, increment a counter.\n4.  The final $p$-value is the total count from step 3e divided by the total number of permutations ($126$).\n\nThis procedure is applied to each of the three test cases specified.\n- **Test Case 1:** A standard, scientifically realistic scenario where the observed statistic $t_{\\mathrm{obs}} = 2.3$ is non-trivial.\n- **Test Case 2:** A degenerate case where all measurements are zero. This tests the implementation's handling of zero variance. For any permutation, all group values will be $0$. Thus, $\\bar{x}_1=\\bar{x}_2=0$ and $s_p=0$, yielding $t=0$ for all permutations. Since $t_{\\mathrm{obs}}=0$, the condition $|t| \\ge |t_{\\mathrm{obs}}|$ is always met, and the $p$-value is expected to be $1.0$.\n- **Test Case 3:** An edge case reusing the data from Test Case 1 but with a very stringent threshold of $t_{\\mathrm{obs}}=10.0$. We expect to find few, if any, permutations that yield a statistic this extreme. The $p$-value is expected to be very small, possibly $0$.\n\nThe implementation will use `numpy` for efficient array operations and `itertools.combinations` from the standard library to enumerate the labelings.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef calculate_t_statistic(x1: np.ndarray, x2: np.ndarray) -> float:\n    \"\"\"\n    Computes the two-sample pooled t-statistic.\n    \n    Handles the special case where the pooled standard deviation is zero.\n    \"\"\"\n    n1, n2 = len(x1), len(x2)\n    \n    # Per problem statement, n1 >= 2 and n2 >= 2, so sample variance is well-defined.\n    mean1 = np.mean(x1)\n    mean2 = np.mean(x2)\n    \n    var1 = np.var(x1, ddof=1)\n    var2 = np.var(x2, ddof=1)\n\n    # Check for the condition leading to sp = 0\n    # s_p = 0 iff var1 = 0 and var2 = 0\n    # Using a small tolerance for floating point comparison\n    is_var1_zero = np.isclose(var1, 0.0)\n    is_var2_zero = np.isclose(var2, 0.0)\n\n    if is_var1_zero and is_var2_zero:\n        if np.isclose(mean1, mean2):\n            return 0.0\n        else:\n            return np.inf\n\n    # Calculate pooled standard deviation\n    sp_sq_numerator = (n1 - 1) * var1 + (n2 - 1) * var2\n    sp_sq_denominator = n1 + n2 - 2\n    \n    # Denominator is guaranteed to be > 0 since n1, n2 >= 2.\n    sp_sq = sp_sq_numerator / sp_sq_denominator\n    \n    # sp can become numerically zero even if variances aren't exactly zero\n    if np.isclose(sp_sq, 0.0):\n        if np.isclose(mean1, mean2):\n            return 0.0\n        else:\n            return np.inf\n            \n    sp = np.sqrt(sp_sq)\n    \n    # Calculate t-statistic\n    t_numerator = mean1 - mean2\n    t_denominator = sp * np.sqrt(1/n1 + 1/n2)\n    \n    if np.isclose(t_denominator, 0.0): # Should not happen due to sp check, but for safety\n        return np.inf if not np.isclose(t_numerator, 0.0) else 0.0\n        \n    return t_numerator / t_denominator\n\ndef compute_permutation_p_value(data: np.ndarray, n1: int, n2: int, t_obs: float, epsilon: float) -> float:\n    \"\"\"\n    Computes the exact two-sided permutation p-value for a t-statistic.\n    \n    Enumerates all possible labelings and counts the fraction where\n    the absolute permuted t-statistic is >= the absolute observed t-statistic.\n    \"\"\"\n    n = n1 + n2\n    indices = np.arange(n)\n    \n    count_exceeding = 0\n    total_permutations = 0\n    \n    abs_t_obs = abs(t_obs)\n    \n    for case_indices in itertools.combinations(indices, n1):\n        total_permutations += 1\n        \n        # Create a boolean mask for efficient indexing\n        mask = np.zeros(n, dtype=bool)\n        mask[list(case_indices)] = True\n        \n        x1 = data[mask]\n        x2 = data[~mask]\n        \n        t_perm = calculate_t_statistic(x1, x2)\n        \n        if abs(t_perm) >= abs_t_obs - epsilon:\n            count_exceeding += 1\n            \n    if total_permutations == 0:\n        return 0.0\n        \n    return count_exceeding / total_permutations\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # Define common parameters and the comparison tolerance\n    n1, n2 = 4, 5\n    epsilon = 1e-12\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        {\n            \"data\": np.array([\n                0.3809919006126299, 1.1555885698541132, 1.9301852390955966,\n                2.7047819083370800, -1.2649110640673518, -0.6324555320336759,\n                0.0000000000000000, 0.6324555320336759, 1.2649110640673518\n            ]),\n            \"t_obs\": 2.3\n        },\n        {\n            \"data\": np.array([0.0] * 9),\n            \"t_obs\": 0.0\n        },\n        {\n            \"data\": np.array([ # Reusing data from test case 1\n                0.3809919006126299, 1.1555885698541132, 1.9301852390955966,\n                2.7047819083370800, -1.2649110640673518, -0.6324555320336759,\n                0.0000000000000000, 0.6324555320336759, 1.2649110640673518\n            ]),\n            \"t_obs\": 10.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_value = compute_permutation_p_value(\n            data=case[\"data\"],\n            n1=n1,\n            n2=n2,\n            t_obs=case[\"t_obs\"],\n            epsilon=epsilon\n        )\n        results.append(p_value)\n\n    # Format the final output string as specified\n    formatted_results = ','.join([f\"{r:.6f}\" for r in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A crucial task in computational systems biology is designing experiments that are sufficiently powered to detect true biological signals. This advanced exercise places you in the role of an experimental designer, tasked with determining the minimum sample size ($n$) required to achieve a desired statistical power in a large-scale study. You will integrate concepts of effect size ($\\mu$), multiple testing correction via the False Discovery Rate (FDR), and power into a single computational framework, providing an essential skill for planning high-throughput research. ",
            "id": "3351015",
            "problem": "You are studying the statistical power of large-scale, independent, two-sided tests for mean shifts in gene expression under a standard normal model with known variance, a setting common in computational systems biology. A fraction $\\pi_1$ of genes exhibit a non-null effect, while the remaining fraction $\\pi_0 = 1 - \\pi_1$ follow the null. For every test, you sample $n$ independent replicates and compute a two-sided $z$-test for a population mean under known variance, then correct for multiple testing using the Benjamini–Hochberg (BH) procedure to control the False Discovery Rate (FDR).\n\nFundamental base:\n- Under the null hypothesis, the standardized test statistic $Z$ satisfies $Z \\sim \\mathcal{N}(0,1)$, where $\\mathcal{N}$ denotes the normal distribution with mean $0$ and variance $1$.\n- Under the non-null hypothesis with effect size $\\mu$ and known standard deviation $\\sigma$, the sample mean $\\bar{X}$ satisfies $\\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma^2/n)$ by the Central Limit Theorem for independent, identically distributed normal data. Hence the standardized statistic satisfies $Z \\sim \\mathcal{N}(\\delta, 1)$, where $\\delta = \\sqrt{n}\\mu/\\sigma$.\n- The two-sided $p$-value is $p = 2\\left(1 - \\Phi(|Z|)\\right)$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nMultiple-testing model:\n- Let $m$ be the number of tests, and assume independence across tests. The BH procedure at FDR level $q$ (take $q = \\alpha$) rejects all tests with $p$-values less than or equal to a data-dependent threshold $\\tau$.\n- For large $m$, the empirical distribution of $p$-values converges to the mixture distribution $F(t) = \\pi_0 t + \\pi_1 F_1(t)$, where $F_1(t) = \\mathbb{P}(p \\le t \\mid H_1)$ is the distribution of $p$-values under the non-null. Under independence and as $m \\to \\infty$, the BH threshold $\\tau$ asymptotically satisfies the fixed-point equation $\\tau = q F(\\tau)$.\n\nPower:\n- The average power across the non-null tests equals $\\mathbb{P}(p \\le \\tau \\mid H_1) = F_1(\\tau)$.\n- For a two-sided test, define $z_{t/2} = \\Phi^{-1}(1 - t/2)$, where $\\Phi^{-1}$ is the inverse cumulative distribution function. Since $p \\le t$ is equivalent to $|Z| \\ge z_{t/2}$, we have $F_1(t) = \\mathbb{P}(|Z| \\ge z_{t/2} \\mid Z \\sim \\mathcal{N}(\\delta,1)) = \\left(1 - \\Phi\\left(z_{t/2} - \\delta\\right)\\right) + \\Phi\\left(-z_{t/2} - \\delta\\right)$.\n\nTask:\n- Write a complete, runnable program that, for a given set of parameters $(m, \\pi_1, \\mu, \\sigma, \\alpha)$, computes the minimal integer $n$ such that the average power $F_1(\\tau)$ is at least $0.8$. Use the asymptotic BH threshold defined by the fixed-point equation $\\tau = q F(\\tau)$ with $q = \\alpha$ and $F(t) = \\pi_0 t + \\pi_1 F_1(t)$.\n- Assume independence of tests and the two-sided $z$-test with known $\\sigma$ for every test.\n- If no such $n$ exists up to $n \\le 100{,}000$, report $-1$ for that test case.\n\nImportant notes:\n- There are no physical units in this problem.\n- Percentages must be treated as decimals; for example, FDR level $q = \\alpha = 0.05$ means $q = 0.05$.\n\nTest suite:\nCompute the minimal $n$ for the following parameter sets. Each parameter set is a tuple $(m, \\pi_1, \\mu, \\sigma, \\alpha)$.\n1. $(10{,}000, 0.1, 0.5, 1, 0.05)$\n2. $(10{,}000, 0.05, 0.4, 1, 0.05)$\n3. $(10{,}000, 0.2, 0.5, 1, 0.01)$\n4. $(10{,}000, 0.1, 1.0, 1, 0.05)$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the same order as the test suite. For example, the printed output must look exactly like $[n_1,n_2,n_3,n_4]$ with no spaces.",
            "solution": "The problem is scientifically sound, well-posed, and objective. It is firmly based on established principles of statistical hypothesis testing, including the properties of the normal distribution, the definition of $p$-values, and the asymptotic theory of the Benjamini–Hochberg procedure for controlling the False Discovery Rate (FDR). All necessary parameters ($m$, $\\pi_1$, $\\mu$, $\\sigma$, $\\alpha$) are provided for each test case, making the problem self-contained. The task requires finding a minimal integer $n$ satisfying a well-defined power constraint, which constitutes a formalizable and solvable computational problem.\n\nThe objective is to find the minimal integer sample size $n$ for which the average power across non-null tests, denoted as $F_1(\\tau)$, is at least $0.8$. The solution strategy involves a search for this optimal $n$. A key observation is that the average power, $F_1(\\tau)$, is a monotonically increasing function of the sample size $n$. A larger $n$ leads to a larger standardized effect size $\\delta$, which increases the power of each individual test. This, in turn, increases both the multiple testing threshold $\\tau$ and the final average power $F_1(\\tau)$. Given this monotonicity, a binary search algorithm is an efficient method to find the minimal $n$ within the specified search space of $[1, 100{,}000]$.\n\nFor any given sample size $n$, the computation of the average power is a sub-problem that must be solved. This computation proceeds in three main steps:\n\n1.  **Calculation of the Standardized Effect Size**: The foundation of the power calculation is the distribution of the test statistic $Z$ under the alternative hypothesis, which is given as $Z \\sim \\mathcal{N}(\\delta, 1)$. The non-centrality parameter $\\delta$ encapsulates the combined effect of the sample size $n$, the true mean effect size $\\mu$, and the known standard deviation $\\sigma$. It is calculated as:\n    $$ \\delta = \\frac{\\sqrt{n}\\mu}{\\sigma} $$\n\n2.  **Determination of the Multiple Testing Threshold $\\tau$**: The problem specifies using the asymptotic Benjamini–Hochberg (BH) threshold $\\tau$. This threshold is the non-trivial solution to the fixed-point equation $\\tau = q F(\\tau)$, where $q$ is the target FDR level (here, $q=\\alpha$) and $F(t)$ is the cumulative distribution function (CDF) of the $p$-values from all tests. The $p$-value distribution is a mixture of the uniform distribution from null tests and the distribution from non-null tests:\n    $$ F(t) = \\pi_0 t + \\pi_1 F_1(t) $$\n    where $\\pi_0 = 1 - \\pi_1$. The term $F_1(t)$ is the CDF of $p$-values under the alternative hypothesis, $F_1(t) = \\mathbb{P}(p \\le t \\mid H_1)$. For a two-sided $z$-test, a $p$-value $p \\le t$ is equivalent to the test statistic $|Z|$ being greater than or equal to a critical value $z_{t/2} = \\Phi^{-1}(1 - t/2)$, where $\\Phi$ is the standard normal CDF. Therefore, $F_1(t)$ is the probability $\\mathbb{P}(|Z| \\ge z_{t/2} \\mid Z \\sim \\mathcal{N}(\\delta,1))$, which has the analytical form:\n    $$ F_1(t) = \\left(1 - \\Phi\\left(z_{t/2} - \\delta\\right)\\right) + \\Phi\\left(-z_{t/2} - \\delta\\right) $$\n    The fixed-point equation $\\tau = \\alpha \\left( (1-\\pi_1)\\tau + \\pi_1 F_1(\\tau) \\right)$ is solved numerically. A robust method is fixed-point iteration. Starting with an initial estimate $\\tau_0 = \\alpha$, we iterate the equation $\\tau_{k+1} = \\alpha F(\\tau_k)$ until the value of $\\tau_k$ converges.\n\n3.  **Computation of Average Power**: Once the converged threshold $\\tau$ has been determined, the average power for the non-null tests is, by definition, $F_1(\\tau)$. This value is computed using the formula for $F_1(t)$ from the previous step, evaluated at the calculated threshold $\\tau$.\n\nThe complete algorithm integrates these components. A binary search is performed for $n$ in the range $[1, 100{,}000]$. For each candidate value of $n$ in the search:\n- The corresponding power is computed using the three-step process described above.\n- If the computed power is greater than or equal to the target of $0.8$, the current $n$ is a potential solution, and we search for a smaller $n$ in the lower half of the current range.\n- If the power is less than $0.8$, the candidate $n$ is too small, and we must search in the upper half of the range.\nThe search terminates when the minimal integer $n$ satisfying the power requirement is found. If the power at $n=100{,}000$ is still below $0.8$, no solution exists within the constraints, and the result is reported as $-1$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the minimal integer sample size n for a series of multiple testing scenarios\n    to achieve a target average power of 0.8.\n    \"\"\"\n    test_cases = [\n        # (m, pi1, mu, sigma, alpha)\n        (10000, 0.1, 0.5, 1, 0.05),\n        (10000, 0.05, 0.4, 1, 0.05),\n        (10000, 0.2, 0.5, 1, 0.01),\n        (10000, 0.1, 1.0, 1, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        # The number of tests m is provided but not used in the asymptotic formulation,\n        # which is consistent with the problem statement (m -> infinity).\n        m, pi1, mu, sigma, alpha = case\n        \n        def get_power_for_n(n):\n            \"\"\"\n            For a given n, calculates the average power F_1(tau).\n            This involves solving the fixed-point equation for tau.\n            \"\"\"\n            if n == 0:\n                return 0.0\n            \n            delta = np.sqrt(n) * mu / sigma\n            pi0 = 1.0 - pi1\n\n            def F1_t(t):\n                \"\"\"Calculates the CDF of p-values under the alternative hypothesis.\"\"\"\n                if t <= 0: return 0.0\n                if t >= 1: return 1.0\n\n                # Use errstate to prevent warnings for t near 0, where ppf(1) -> inf.\n                with np.errstate(divide='ignore', invalid='ignore'):\n                    z_half_t = norm.ppf(1.0 - t / 2.0)\n                \n                # If t is so small that z_half_t is infinite, power is effectively 0.\n                if np.isinf(z_half_t):\n                    return 0.0\n\n                power = (1.0 - norm.cdf(z_half_t - delta)) + norm.cdf(-z_half_t - delta)\n                return power\n\n            def F_t(t):\n                \"\"\"Calculates the mixture CDF of p-values.\"\"\"\n                t_clipped = np.clip(t, 0.0, 1.0)\n                return pi0 * t_clipped + pi1 * F1_t(t_clipped)\n\n            # Solve for tau using fixed-point iteration.\n            # Start with tau = alpha, which is an upper bound on the solution.\n            tau = float(alpha)\n            for _ in range(100):  # 100 iterations are sufficient for convergence.\n                tau_next = alpha * F_t(tau)\n                if abs(tau_next - tau) < 1e-12:\n                    tau = tau_next\n                    break\n                tau = tau_next\n            \n            # The average power is F1(tau).\n            return F1_t(tau)\n\n        # Binary search for the minimal n in the range [1, 100000].\n        low = 1\n        high = 100000\n        min_n = -1\n\n        while low <= high:\n            mid_n = low + (high - low) // 2\n            if mid_n == 0:\n                low = 1\n                continue\n            \n            power = get_power_for_n(mid_n)\n            \n            if power >= 0.8:\n                # This n is a potential solution. Try to find a smaller one.\n                min_n = mid_n\n                high = mid_n - 1\n            else:\n                # This n is too small. Increase it.\n                low = mid_n + 1\n\n        results.append(min_n)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}