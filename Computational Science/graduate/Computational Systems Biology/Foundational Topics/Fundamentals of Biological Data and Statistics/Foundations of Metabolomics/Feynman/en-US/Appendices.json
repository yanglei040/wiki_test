{
    "hands_on_practices": [
        {
            "introduction": "In metabolomics, mass spectrometry signals often represent a linear superposition of contributions from different metabolites, their adducts, and charge states. Before attempting to deconvolve these signals, we must first ask if a unique and stable solution for the underlying metabolite concentrations is even possible. This practice introduces the crucial concept of identifiability, which can be rigorously assessed using the tools of linear algebra to analyze the properties of the mixing matrix $A$ in the model $y = Ax + \\epsilon$ . Mastering this analysis is a critical first step for building any reliable quantitative model from instrumental data.",
            "id": "3311156",
            "problem": "Consider a mass spectrometry measurement model in metabolomics in which multiple adducts and charge states lead to linear mixing of metabolite contributions. The measurement model is additive and given by $y = A x + \\epsilon$, where $y \\in \\mathbb{R}^{m}$ is the observed signal vector across $m$ extracted ion features, $x \\in \\mathbb{R}^{n}$ is the nonnegative metabolite concentration vector for $n$ metabolites, $A \\in \\mathbb{R}^{m \\times n}$ encodes the linearized adduct and charge-state mapping from metabolite concentrations to expected feature intensities, and $\\epsilon \\in \\mathbb{R}^{m}$ is additive measurement noise. In this setting, an identifiability test should determine whether the mapping from $x$ to $y$ is uniquely invertible with respect to $x$ under the given $A$ and $\\epsilon$, and also assess whether the inversion is numerically stable. Starting from the foundational base of linear systems and least squares, define and implement identifiability as follows: theoretical uniqueness of $x$ occurs when the linear map has a trivial kernel, and least squares uniqueness is equivalently characterized by the positive definiteness of the normal operator. For numerical stability, quantify sensitivity through the condition number of $A$, defined via singular values. Your program must implement the following outputs for each case: a boolean for theoretical identifiability, a boolean for practical numerical identifiability under a specified condition-number threshold, the condition number of $A$, and the Euclidean norm of the estimation error for the least squares solution that minimizes $\\lVert y - A x \\rVert_2^2$.\n\nUse the following test suite of parameter values, where each matrix and vector is explicitly specified. In all cases, construct $y$ by $y = A x_{\\text{true}} + \\epsilon$. The goal is to determine whether $x$ is unique from $y$ under the given $A$ and $\\epsilon$ and to quantify stability and estimation error.\n\nCase $\\mathbf{1}$ (happy path, full column rank, moderate conditioning): Let $m = 5$, $n = 3$, \n$$\nA^{(1)} = \\begin{bmatrix}\n1.0  0.8  0.6 \\\\\n0.5  0.4  0.3 \\\\\n0.1  0.05  0.0 \\\\\n0.3  0.2  0.1 \\\\\n0.0  0.1  0.4\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(1)} = \\begin{bmatrix} 10.0 \\\\ 5.0 \\\\ 2.0 \\end{bmatrix},\\quad\n\\epsilon^{(1)} = \\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.0 \\\\ 0.005 \\\\ -0.01 \\end{bmatrix}.\n$$\n\nCase $\\mathbf{2}$ (rank-deficient due to adduct ambiguity): Let $m = 5$, $n = 4$, with two identical columns representing indistinguishable adduct contributions,\n$$\nA^{(2)} = \\begin{bmatrix}\n1.0  0.7  0.3  0.7 \\\\\n0.2  0.1  0.05  0.1 \\\\\n0.0  0.2  0.1  0.2 \\\\\n0.3  0.1  0.2  0.1 \\\\\n0.4  0.6  0.5  0.6\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(2)} = \\begin{bmatrix} 3.0 \\\\ 1.5 \\\\ 2.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\epsilon^{(2)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.01 \\\\ -0.01 \\end{bmatrix}.\n$$\n\nCase $\\mathbf{3}$ (ill-conditioned, nearly collinear adduct mappings): Let $m = 6$, $n = 3$, define the columns of $A^{(3)}$ as \n$$\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 0.2 \\\\ 0.1 \\\\ 0.05 \\\\ 0.02 \\end{bmatrix},\\quad\nc_2 = \\begin{bmatrix} 1.0 \\cdot (1 + 10^{-10}) \\\\ 0.5 \\cdot (1 + 10^{-10}) \\\\ 0.2 \\cdot (1 + 10^{-10}) \\\\ 0.1 \\cdot (1 + 10^{-10}) \\\\ 0.05 \\cdot (1 + 10^{-10}) \\\\ 0.02 \\cdot (1 + 10^{-10}) + 10^{-12} \\end{bmatrix},\\quad\nc_3 = \\begin{bmatrix} 0.01 \\\\ 0.005 \\\\ 0.002 \\\\ 0.001 \\\\ 0.0005 \\\\ 0.0002 \\end{bmatrix},\n$$\nand $A^{(3)} = [\\,c_1\\; c_2\\; c_3\\,]$,\n$$\nx_{\\text{true}}^{(3)} = \\begin{bmatrix} 10.0 \\\\ 10.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\epsilon^{(3)} = \\begin{bmatrix} 10^{-6} \\\\ -10^{-6} \\\\ 2\\cdot 10^{-6} \\\\ -2\\cdot 10^{-6} \\\\ 10^{-6} \\\\ -10^{-6} \\end{bmatrix}.\n$$\n\nCase $\\mathbf{4}$ (underdetermined system: more metabolites than features): Let $m = 3$, $n = 5$,\n$$\nA^{(4)} = \\begin{bmatrix}\n1.0  0.2  0.0  0.5  0.1 \\\\\n0.0  0.1  1.0  0.0  0.2 \\\\\n0.5  0.3  0.2  0.1  0.0\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(4)} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 0.5 \\\\ 0.8 \\\\ 0.3 \\end{bmatrix},\\quad\n\\epsilon^{(4)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nCase $\\mathbf{5}$ (square system, heteroscedastic noise): Let $m = 4$, $n = 4$,\n$$\nA^{(5)} = \\begin{bmatrix}\n1.0  0.0  0.5  0.2 \\\\\n0.1  1.0  0.3  0.0 \\\\\n0.0  0.2  1.0  0.1 \\\\\n0.3  0.0  0.0  1.0\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(5)} = \\begin{bmatrix} 2.0 \\\\ 1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix},\\quad\n\\epsilon^{(5)} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.1 \\\\ -0.15 \\end{bmatrix}.\n$$\n\nYour program must proceed from the following first-principles criteria: theoretical identifiability of $x$ is determined by whether the linear map $A$ has full column rank, which implies the kernel of $A$ is $\\{0\\}$ and the normal operator $A^{\\top}A$ is positive definite; least squares uniqueness is equivalent to this full column rank condition. Practical numerical identifiability is determined by bounding the condition number, defined by $\\kappa(A) = s_{\\max}/s_{\\min}$, where $s_{\\max}$ and $s_{\\min}$ are the largest and smallest singular values of $A$. Compute rank using a singular-value threshold of $\\tau = 10^{-12} s_{\\max}$, and declare practical numerical identifiability only if theoretical identifiability holds and $\\kappa(A)  10^{8}$. For each case, compute a least-squares estimate $\\hat{x}$ as the minimizer of $\\lVert y - A x \\rVert_2^2$ and report the estimation error norm $\\lVert \\hat{x} - x_{\\text{true}} \\rVert_2$.\n\nFinal output format: your program should produce a single line of output containing a list of lists, one per case, where each inner list is $[b_{\\text{theory}}, b_{\\text{numeric}}, \\kappa, e]$ with $b_{\\text{theory}}$ and $b_{\\text{numeric}}$ booleans indicating theoretical and practical identifiability respectively, $\\kappa$ the condition number of $A$, and $e$ the Euclidean norm of the estimation error. Express the floats $\\kappa$ and $e$ rounded to $6$ decimal places. The program must output this as a single line in the exact format, for example $[[\\text{True},\\text{True},1.234567,0.000001],[\\dots]]$, with no additional text. No physical units are involved. Angles are not used. Percentages are not used. The list must be fully determined by the specified inputs with no randomness.",
            "solution": "The problem requires an analysis of identifiability for a linear model $y = A x + \\epsilon$, which is a common representation for mass spectrometry data in metabolomics. Here, $y \\in \\mathbb{R}^{m}$ represents the measured intensities of $m$ ion features, $x \\in \\mathbb{R}^{n}$ is the vector of concentrations for $n$ metabolites, and $A \\in \\mathbb{R}^{m \\times n}$ is a matrix that linearly maps metabolite concentrations to their expected contributions to the ion features, accounting for adducts and charge states. The term $\\epsilon \\in \\mathbb{R}^{m}$ represents additive measurement noise. The task is to assess theoretical and practical identifiability of the metabolite concentrations $x$ from the measurements $y$, given the mapping $A$.\n\n**1. Theoretical Identifiability**\n\nTheoretical identifiability concerns the uniqueness of the solution $x$ in the idealized, noiseless case where $\\epsilon=0$, i.e., for the system $y = Ax$. A unique solution for $x$ exists for any given $y$ in the range of $A$ if and only if the linear map represented by $A$ is one-to-one (injective). This is equivalent to the condition that the null space, or kernel, of the matrix $A$ is trivial, containing only the zero vector: $\\ker(A) = \\{0\\}$.\n\nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$, the kernel is trivial if and only if its columns are linearly independent. This implies that the rank of the matrix must be equal to the number of columns, $n$. Formally, the condition for theoretical identifiability is:\n$$\n\\text{rank}(A) = n\n$$\nThis condition is also known as $A$ having full column rank. If this holds, the Gram matrix $A^{\\top}A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite, which guarantees that the unique least-squares solution exists.\n\nA necessary condition for full column rank is that the number of rows (measurements) must be greater than or equal to the number of columns (unknowns), i.e., $m \\geq n$. If $m  n$, the system is underdetermined, $\\text{rank}(A) \\leq m  n$, and a unique solution for $x$ is impossible; there exists an infinite family of solutions.\n\n**2. Practical and Numerical Identifiability**\n\nEven when a system is theoretically identifiable (i.e., $A$ has full column rank), the solution may be practically non-identifiable due to numerical instability. If the columns of $A$ are nearly linearly dependent, the system is said to be ill-conditioned. In such cases, small perturbations in the measurement vector $y$, caused by noise $\\epsilon$, can lead to very large changes in the estimated solution $\\hat{x}$.\n\nThis sensitivity is quantified by the condition number of the matrix $A$, denoted $\\kappa(A)$. Using the singular values of $A$, where $s_{\\max}$ is the largest and $s_{\\min}$ is the smallest non-zero singular value, the condition number is defined as:\n$$\n\\kappa(A) = \\frac{s_{\\max}}{s_{\\min}}\n$$\nA well-conditioned matrix has a small condition number (close to $1$), while an ill-conditioned matrix has a very large condition number. The problem specifies a threshold of $10^8$: the system is considered practically identifiable only if it is theoretically identifiable and $\\kappa(A)  10^8$.\n\n**3. Computational Algorithm**\n\nFor each test case specified by a set of parameters $(A, x_{\\text{true}}, \\epsilon)$, the following procedure is implemented:\n1.  The measurement vector $y$ is constructed as $y = A x_{\\text{true}} + \\epsilon$.\n2.  The Singular Value Decomposition (SVD) of $A$ is computed. The SVD yields the singular values $\\{s_i\\}$, which are used for all subsequent analyses.\n3.  **Theoretical identifiability ($b_{\\text{theory}}$)** is determined. The rank of $A$ is computed by counting the number of singular values $s_i$ that satisfy $s_i  \\tau$, where the threshold $\\tau$ is defined as $\\tau = 10^{-12} s_{\\max}$. If the computed rank equals $n$, $b_{\\text{theory}}$ is set to `True`; otherwise, it is `False`.\n4.  **Practical identifiability ($b_{\\text{numeric}}$)** is determined. The condition number $\\kappa(A)$ is computed from the singular values. If $b_{\\text{theory}}$ is `True` and $\\kappa(A)  10^8$, then $b_{\\text{numeric}}$ is set to `True`; otherwise, it is `False`.\n5.  The **least-squares estimate ($\\hat{x}$)** is calculated. This is the vector $\\hat{x}$ that minimizes the squared Euclidean norm of the residual, $\\|y - A x\\|_2^2$. This is robustly computed using standard numerical linear algebra routines, such as those provided by `numpy.linalg.lstsq`, which correctly handle full-rank, rank-deficient, and underdetermined systems. For rank-deficient and underdetermined cases, this function returns the unique minimum-norm solution.\n6.  The **estimation error ($e$)** is calculated as the Euclidean norm of the difference between the estimated and true concentration vectors: $e = \\|\\hat{x} - x_{\\text{true}}\\|_2$.\n\nThis procedure systematically evaluates each case against the rigorous criteria for identifiability and numerical stability, providing a quantitative assessment of the solvability of the underlying inverse problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format the results for all test cases.\n    \"\"\"\n\n    # Case 1: Happy path, full column rank, moderate conditioning\n    A1 = np.array([\n        [1.0, 0.8, 0.6],\n        [0.5, 0.4, 0.3],\n        [0.1, 0.05, 0.0],\n        [0.3, 0.2, 0.1],\n        [0.0, 0.1, 0.4]\n    ])\n    x_true1 = np.array([10.0, 5.0, 2.0])\n    epsilon1 = np.array([0.01, -0.02, 0.0, 0.005, -0.01])\n    case1 = (A1, x_true1, epsilon1)\n\n    # Case 2: Rank-deficient due to adduct ambiguity\n    A2 = np.array([\n        [1.0, 0.7, 0.3, 0.7],\n        [0.2, 0.1, 0.05, 0.1],\n        [0.0, 0.2, 0.1, 0.2],\n        [0.3, 0.1, 0.2, 0.1],\n        [0.4, 0.6, 0.5, 0.6]\n    ])\n    x_true2 = np.array([3.0, 1.5, 2.0, 0.5])\n    epsilon2 = np.array([0.0, 0.0, 0.0, 0.01, -0.01])\n    case2 = (A2, x_true2, epsilon2)\n\n    # Case 3: Ill-conditioned, nearly collinear adduct mappings\n    c1 = np.array([1.0, 0.5, 0.2, 0.1, 0.05, 0.02])\n    c2 = np.array([\n        1.0 * (1.0 + 1e-10), \n        0.5 * (1.0 + 1e-10), \n        0.2 * (1.0 + 1e-10), \n        0.1 * (1.0 + 1e-10), \n        0.05 * (1.0 + 1e-10), \n        0.02 * (1.0 + 1e-10) + 1e-12\n    ])\n    c3 = np.array([0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002])\n    A3 = np.vstack([c1, c2, c3]).T\n    x_true3 = np.array([10.0, 10.0, 0.5])\n    epsilon3 = np.array([1e-6, -1e-6, 2e-6, -2e-6, 1e-6, -1e-6])\n    case3 = (A3, x_true3, epsilon3)\n\n    # Case 4: Underdetermined system\n    A4 = np.array([\n        [1.0, 0.2, 0.0, 0.5, 0.1],\n        [0.0, 0.1, 1.0, 0.0, 0.2],\n        [0.5, 0.3, 0.2, 0.1, 0.0]\n    ])\n    x_true4 = np.array([1.0, 2.0, 0.5, 0.8, 0.3])\n    epsilon4 = np.array([0.0, 0.0, 0.0])\n    case4 = (A4, x_true4, epsilon4)\n\n    # Case 5: Square system\n    A5 = np.array([\n        [1.0, 0.0, 0.5, 0.2],\n        [0.1, 1.0, 0.3, 0.0],\n        [0.0, 0.2, 1.0, 0.1],\n        [0.3, 0.0, 0.0, 1.0]\n    ])\n    x_true5 = np.array([2.0, 1.0, 0.5, 3.0])\n    epsilon5 = np.array([0.05, -0.02, 0.1, -0.15])\n    case5 = (A5, x_true5, epsilon5)\n\n    test_cases = [case1, case2, case3, case4, case5]\n    results = []\n\n    for case in test_cases:\n        A, x_true, epsilon = case\n        \n        # Unpack dimensions\n        m, n = A.shape\n        \n        # Construct observed signal vector y\n        y = A @ x_true + epsilon\n        \n        # --- Identifiability Analysis ---\n        \n        # Compute singular values for rank and condition number\n        s = np.linalg.svd(A, compute_uv=False)\n        \n        # Theoretical identifiability based on rank\n        b_theory = False\n        if s.size  0: # Handle empty matrix case\n            s_max = s[0]\n            # Rank is computed using the problem-specified threshold\n            rank_threshold = 1e-12 * s_max\n            rank = np.sum(s  rank_threshold)\n            if rank == n:\n                b_theory = True\n        \n        # Compute condition number\n        # np.linalg.cond returns a large number for singular/ill-conditioned matrices\n        kappa = np.linalg.cond(A)\n        \n        # Practical numerical identifiability\n        b_numeric = b_theory and (kappa  1e8)\n        \n        # --- Least Squares Estimation ---\n        \n        # Compute the least-squares estimate for x\n        x_hat = np.linalg.lstsq(A, y, rcond=None)[0]\n        \n        # Compute the Euclidean norm of the estimation error\n        e = np.linalg.norm(x_hat - x_true)\n        \n        # Append the list of results for this case\n        results.append([b_theory, b_numeric, round(float(kappa), 6), round(float(e), 6)])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central task in stable isotope tracing experiments is to correct measured mass isotopomer distributions for the confounding effect of naturally abundant heavy isotopes. This practice models this systematic distortion as a linear transformation and casts the correction as a well-posed, constrained inverse problem. You will build the correction matrix from the first principles of probability theory and use nonnegative least-squares to recover the true biological labeling pattern, a foundational skill for any quantitative flux analysis .",
            "id": "3311163",
            "problem": "Consider a metabolite measured by high-resolution mass spectrometry whose mass isotopomer distribution (MID) is represented by a probability vector $x_{\\text{true}} \\in \\mathbb{R}^{K+1}$, where $x_{\\text{true}}[j]$ denotes the probability of having exactly $j$ tracer-derived heavy-isotope substitutions, for $j \\in \\{0,1,\\dots,K\\}$. Each non-tracer atom in the molecule independently undergoes a natural-abundance isotopic substitution that contributes a nonnegative integer mass shift. Model each atom’s mass shift as an independent draw from a discrete distribution $\\pi$ over $\\{0,1,\\dots,m\\}$ with probabilities $\\pi[0],\\pi[1],\\dots,\\pi[m]$ that satisfy $\\sum_{r=0}^{m} \\pi[r] = 1$, and let there be $N$ such atoms. Define the total natural-abundance mass shift $S$ as the sum of these $N$ independent draws. The observed MID $y_{\\text{obs}} \\in \\mathbb{R}^{K+1}$ is related to $x_{\\text{true}}$ by the linear system\n$$\ny_{\\text{obs}} = P(\\pi)\\, x_{\\text{true}},\n$$\nwhere $P(\\pi) \\in \\mathbb{R}^{(K+1)\\times(K+1)}$ is lower-triangular and encodes the distribution of $S$ via discrete convolution across atoms. Your task is to construct $P(\\pi)$ from first principles without using any closed-form shortcut formulas, correct $y_{\\text{obs}}$ to estimate $x_{\\text{true}}$ when $\\pi$ is uncertain, and quantify sensitivity to uncertainty in $\\pi$.\n\nFoundational starting point: Use only the independence of atoms, the axioms of probability, and the fact that the distribution of a sum of independent integer-valued random variables is given by repeated discrete convolution of their individual probability mass functions. Treat $x_{\\text{true}}$ and $y_{\\text{obs}}$ as probability vectors that sum to $1$.\n\nDefine the following computational tasks:\n\n1. Given $N$, $\\pi$, and $K$, construct the vector $h \\in \\mathbb{R}^{K+1}$ where $h[r]$ is the probability that $S=r$ for $r \\in \\{0,1,\\dots,K\\}$, using $N$-fold convolution of the atomic mass-shift distribution $\\pi$ (truncate to $\\{0,1,\\dots,K\\}$ if necessary). Then construct $P(\\pi)$ by setting\n$$\nP(\\pi)[k,j] = \n\\begin{cases}\nh[k-j],  \\text{if } k \\ge j, \\\\\n0,  \\text{if } k  j,\n\\end{cases}\n$$\nfor $k,j \\in \\{0,1,\\dots,K\\}$.\n\n2. Given $y_{\\text{obs}}$ and an estimated $\\pi_{\\text{est}}$, compute an estimate $x_{\\text{hat}}$ by solving a nonnegative least-squares problem\n$$\n\\min_{x \\in \\mathbb{R}^{K+1}} \\|P(\\pi_{\\text{est}})\\,x - y_{\\text{obs}}\\|_2 \\quad \\text{subject to } x \\ge 0,\n$$\nfollowed by renormalization of $x$ so that $\\sum_{j=0}^{K} x[j] = 1$.\n\n3. Sensitivity to $\\pi$: Let $\\pi_{\\text{true}}$ be the true atomic distribution used to form $y_{\\text{obs}}$, and $\\pi_{\\text{est}}$ the model used for correction. Define the estimation error $e = \\|x_{\\text{hat}} - x_{\\text{true}}\\|_2$. Estimate a local upper bound on the perturbation of $x_{\\text{hat}}$ with respect to uncertainty in $\\pi$ as follows. Approximate the Jacobian $J = \\partial x_{\\text{hat}} / \\partial \\pi$ numerically by central differences along a basis of perturbations that preserve the probability simplex constraint $\\sum_{r=0}^{m} \\pi[r] = 1$ by transferring a small amount $\\delta$ between a chosen component and a compensating component. Use the induced operator norm bound\n$$\n\\| \\Delta x_{\\text{hat}} \\|_2 \\le \\|\\Delta \\pi\\|_{1} \\cdot \\max_{i} \\| J_{:,i} \\|_2,\n$$\nto compute an upper bound for the actual discrepancy with $\\|\\Delta \\pi\\|_{1} = \\|\\pi_{\\text{est}} - \\pi_{\\text{true}}\\|_{1}$. Report this bound along with $e$.\n\n4. Also report the spectral condition number $\\kappa_2(P(\\pi_{\\text{est}}))$ to assess numerical conditioning.\n\nPhysical units are not involved in this problem.\n\nUse the following test suite of parameter values, which includes a general case, a near-degenerate boundary case, and a multi-shift edge case. For each case, first synthesize $y_{\\text{obs}} = P(\\pi_{\\text{true}})\\,x_{\\text{true}}$ deterministically (no noise), then compute $x_{\\text{hat}}$ using $\\pi_{\\text{est}}$, and finally compute $e$, the sensitivity bound, and $\\kappa_2(P(\\pi_{\\text{est}}))$.\n\n- Test Case $1$ (single-shift atomic distribution, moderate $N$):\n  - $N = 12$\n  - $m = 1$, with $\\pi_{\\text{true}} = [0.989,\\,0.011]$ and $\\pi_{\\text{est}} = [0.988,\\,0.012]$\n  - $K = 6$\n  - $x_{\\text{true}} = [0.25,\\,0.20,\\,0.18,\\,0.15,\\,0.12,\\,0.07,\\,0.03]$\n\n- Test Case $2$ (near-degenerate heavy probability, larger $N$):\n  - $N = 30$\n  - $m = 1$, with $\\pi_{\\text{true}} = [0.9995,\\,0.0005]$ and $\\pi_{\\text{est}} = [0.9985,\\,0.0015]$\n  - $K = 10$\n  - $x_{\\text{true}} = [0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.0]$\n\n- Test Case $3$ (multi-shift per atom, includes $+2$ mass shifts):\n  - $N = 6$\n  - $m = 2$, with $\\pi_{\\text{true}} = [0.95,\\,0.03,\\,0.02]$ and $\\pi_{\\text{est}} = [0.952,\\,0.031,\\,0.017]$\n  - $K = 8$\n  - $x_{\\text{true}} = [0.15,\\,0.14,\\,0.13,\\,0.12,\\,0.11,\\,0.10,\\,0.09,\\,0.08,\\,0.08]$\n\nProgram requirements:\n\n- Construct $P(\\pi)$ via explicit $N$-fold discrete convolution of $\\pi$ (no closed-form binomial or multinomial shortcut formulas).\n- Use nonnegative least squares to compute $x_{\\text{hat}}$ and then renormalize to sum to $1$.\n- Compute the sensitivity bound using a central-difference Jacobian that perturbs one component of $\\pi$ and compensates with the zeroth component (for the zeroth component, compensate with the first) to preserve the simplex constraint; use a sufficiently small $\\delta$ that maintains nonnegativity.\n- For each test case, output a list containing three real numbers in the following order: the error $e$, the bound, and $\\kappa_2(P(\\pi_{\\text{est}}))$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is itself a bracketed, comma-separated triple for a test case, with no spaces. For example, the output should look like\n  - \"[[e1,b1,k1],[e2,b2,k2],[e3,b3,k3]]\"\nwhere each symbol is replaced by the computed floating-point value for that test case.",
            "solution": "The problem requires the construction and application of a numerical workflow for natural abundance correction in mass spectrometry-based metabolomics, including a sensitivity analysis of the correction to uncertainties in the underlying atomic isotopic distribution. The solution will be developed from first principles as specified.\n\nThe core of the problem lies in the relationship between the true mass isotopomer distribution (MID) of a metabolite, $x_{\\text{true}}$, and the observed MID, $y_{\\text{obs}}$. This relationship is modeled as a linear system $y_{\\text{obs}} = P(\\pi) x_{\\text{true}}$. The matrix $P(\\pi)$ accounts for the confounding effect of natural abundance isotopes in the non-tracer part of the molecule.\n\nThe total mass shift, $S$, from natural abundance is the sum of $N$ independent, identically distributed random variables, each representing the mass shift from a single atom. The probability mass function (PMF) of each atomic shift is given by $\\pi = [\\pi[0], \\pi[1], \\dots, \\pi[m]]$. The axioms of probability dictate that the PMF of a sum of independent random variables is the discrete convolution of their individual PMFs.\n\nLet $h^{(n)}$ be the PMF of the total mass shift from $n$ atoms. We have $h^{(1)} = \\pi$. The PMF for $n$ atoms is found by convolving the PMF for $n-1$ atoms with $\\pi$:\n$$\nh^{(n)}[k] = (h^{(n-1)} * \\pi)[k] = \\sum_{j=0}^{k} h^{(n-1)}[j] \\pi[k-j]\n$$\nTo find the PMF for the total shift $S$ from $N$ atoms, denoted $h$, we perform this convolution iteratively $N-1$ times. The resulting vector $h$ must then be truncated to the relevant length of $K+1$.\n\n**Step 1: Construction of the Correction Matrix $P(\\pi)$**\n\nGiven $N$, $\\pi$, and $K$, we first compute the vector $h$ representing the PMF of the total natural abundance mass shift $S$, truncated to size $K+1$.\n1.  Initialize a vector `h_current` with the atomic PMF $\\pi$.\n2.  Repeat $N-1$ times: update `h_current` by convolving it with $\\pi$. This yields the PMF of the sum of shifts for $N$ atoms.\n3.  The final vector $h$ is obtained by taking the first $K+1$ elements of the result from the previous step.\n4.  The matrix $P(\\pi)$ of size $(K+1) \\times (K+1)$ is then constructed. Its elements are given by $P(\\pi)[k,j] = h[k-j]$ for $k \\ge j$ and $0$ otherwise. This structure reflects that a true mass isotopomer $j$ can only contribute to observed mass isotopomers $k \\ge j$, with the mass difference $k-j$ arising from natural abundance. This results in a lower-triangular Toeplitz matrix.\n\n**Step 2: Estimation of the True MID $x_{\\text{true}}$**\n\nGiven an observed MID, $y_{\\text{obs}}$, and an estimated atomic PMF, $\\pi_{\\text{est}}$, we seek to find an estimate $x_{\\text{hat}}$ for the true MID, $x_{\\text{true}}$. The problem is posed as inverting the linear system, but with the constraints that the elements of $x$ must be non-negative. This leads to the nonnegative least-squares (NNLS) problem:\n$$\n\\min_{x \\in \\mathbb{R}^{K+1}} \\|P(\\pi_{\\text{est}})\\,x - y_{\\text{obs}}\\|_2 \\quad \\text{subject to } x_j \\ge 0 \\text{ for all } j.\n$$\nThis is a standard convex optimization problem that can be solved using established algorithms. After solving for the non-negative vector, say $x_{\\text{nnls}}$, it is essential to renormalize it to ensure it represents a valid probability distribution, i.e., its elements sum to $1$.\n$$\nx_{\\text{hat}}[j] = \\frac{x_{\\text{nnls}}[j]}{\\sum_{i=0}^{K} x_{\\text{nnls}}[i]}\n$$\nA small epsilon, $\\epsilon  0$, should be added to the denominator for numerical stability, although in this problem context the sum is not expected to be zero.\n\n**Step 3: Sensitivity Analysis**\n\nThis step quantifies how sensitive the estimated MID, $x_{\\text{hat}}$, is to uncertainties in the estimated atomic PMF, $\\pi_{\\text{est}}$. We are asked to compute an upper bound on the error amplification. The bound is defined as $\\|\\Delta \\pi\\|_{1} \\cdot \\max_{i} \\| J_{:,i} \\|_2$, where $J = \\partial x_{\\text{hat}} / \\partial \\pi$ is the Jacobian matrix and $\\Delta \\pi = \\pi_{\\text{est}} - \\pi_{\\text{true}}$.\n\nThe Jacobian $J$ is approximated numerically using central differences. A crucial detail is that any perturbation to $\\pi$ must preserve the constraint that its elements sum to $1$. The prescribed scheme is to perturb a component $\\pi[i]$ and apply a compensatory perturbation to another component.\nFor each $i \\in \\{0, 1, \\dots, m\\}$:\n1.  Define a perturbation of size $\\delta \\ll 1$.\n2.  Create two perturbed vectors, $\\pi^{+}$ and $\\pi^{-}$:\n    -   If $i  0$: $\\pi^{+}[i] = \\pi_{\\text{est}}[i] + \\delta$, $\\pi^{+}[0] = \\pi_{\\text{est}}[0] - \\delta$. And $\\pi^{-}[i] = \\pi_{\\text{est}}[i] - \\delta$, $\\pi^{-}[0] = \\pi_{\\text{est}}[0] + \\delta$.\n    -   If $i = 0$: $\\pi^{+}[0] = \\pi_{\\text{est}}[0] + \\delta$, $\\pi^{+}[1] = \\pi_{\\text{est}}[1] - \\delta$. And $\\pi^{-}[0] = \\pi_{\\text{est}}[0] - \\delta$, $\\pi^{-}[1] = \\pi_{\\text{est}}[1] + \\delta$.\n3.  For each perturbed vector, re-compute the entire correction pipeline: construct $P(\\pi^{\\pm})$, solve the NNLS problem to get $x_{\\text{nnls}}(\\pi^{\\pm})$, and renormalize to get $x_{\\text{hat}}(\\pi^{\\pm})$.\n4.  The $i$-th column of the Jacobian is then approximated as:\n    $$\n    J_{:,i} \\approx \\frac{x_{\\text{hat}}(\\pi^{+}) - x_{\\text{hat}}(\\pi^{-})}{2\\delta}\n    $$\nAfter computing all $m+1$ columns of $J$, we find the maximum of their Euclidean norms: $\\max_{i} \\| J_{:,i} \\|_2$. The sensitivity bound is then calculated by multiplying this maximum norm by the $L_1$ norm of the error in $\\pi$, $\\|\\pi_{\\text{est}} - \\pi_{\\text{true}}\\|_1$.\n\nFinally, the actual estimation error, $e = \\|x_{\\text{hat}} - x_{\\text{true}}\\|_2$, is computed for comparison with the bound.\n\n**Step 4: Assessment of Numerical Conditioning**\n\nThe numerical stability of solving the linear system is related to the condition number of the matrix $P(\\pi_{\\text{est}})$. The spectral condition number, $\\kappa_2(P(\\pi_{\\text{est}}))$, is the ratio of the largest to the smallest singular value of the matrix. A large condition number indicates that the matrix is close to being singular, and small errors in $y_{\\text{obs}}$ or $P(\\pi_{\\text{est}})$ could be greatly amplified in the solution $x_{\\text{hat}}$. This is calculated using standard linear algebra routines.\n\nThese four steps provide a complete framework to perform the natural abundance correction, assess its numerical conditioning, and quantify its sensitivity to model parameters. The implementation will follow this logic for each of the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"N\": 12, \"m\": 1, \"K\": 6,\n            \"pi_true\": np.array([0.989, 0.011]),\n            \"pi_est\": np.array([0.988, 0.012]),\n            \"x_true\": np.array([0.25, 0.20, 0.18, 0.15, 0.12, 0.07, 0.03])\n        },\n        {\n            \"N\": 30, \"m\": 1, \"K\": 10,\n            \"pi_true\": np.array([0.9995, 0.0005]),\n            \"pi_est\": np.array([0.9985, 0.0015]),\n            \"x_true\": np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0])\n        },\n        {\n            \"N\": 6, \"m\": 2, \"K\": 8,\n            \"pi_true\": np.array([0.95, 0.03, 0.02]),\n            \"pi_est\": np.array([0.952, 0.031, 0.017]),\n            \"x_true\": np.array([0.15, 0.14, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08, 0.08])\n        }\n    ]\n\n    all_results = []\n\n    def construct_P(N, pi, K):\n        \"\"\"Constructs the correction matrix P from N, pi, and K.\"\"\"\n        # 1. N-fold convolution of pi\n        h_conv = pi.copy()\n        for _ in range(N - 1):\n            h_conv = np.convolve(h_conv, pi)\n        \n        # 2. Truncate to get h vector\n        h = np.zeros(K + 1)\n        len_h = min(len(h_conv), K + 1)\n        h[:len_h] = h_conv[:len_h]\n\n        # 3. Construct P matrix\n        P = np.zeros((K + 1, K + 1))\n        for j in range(K + 1):\n            for k in range(j, K + 1):\n                P[k, j] = h[k - j]\n        return P\n\n    def full_correction_pipeline(pi, y_obs, N, K):\n        \"\"\"Runs the full correction process for a given pi and y_obs.\"\"\"\n        P_est = construct_P(N, pi, K)\n        x_nnls, _ = nnls(P_est, y_obs)\n        # Renormalize\n        sum_x = np.sum(x_nnls)\n        if sum_x  1e-9:\n             x_hat = x_nnls / sum_x\n        else: # Handle zero-sum case\n             x_hat = np.full(K + 1, 1.0 / (K + 1))\n        return x_hat\n\n    for case in test_cases:\n        N, m, K = case[\"N\"], case[\"m\"], case[\"K\"]\n        pi_true, pi_est, x_true_raw = case[\"pi_true\"], case[\"pi_est\"], case[\"x_true\"]\n        \n        # Ensure x_true is a probability vector\n        x_true = x_true_raw / np.sum(x_true_raw)\n\n        # Synthesize observed data\n        P_true = construct_P(N, pi_true, K)\n        y_obs = P_true @ x_true\n        \n        # Task 2: Compute x_hat\n        x_hat = full_correction_pipeline(pi_est, y_obs, N, K)\n        \n        # Task 3 (part 1): Estimation error\n        error_e = np.linalg.norm(x_hat - x_true, 2)\n        \n        # Task 4: Condition number\n        P_est_for_cond = construct_P(N, pi_est, K)\n        cond_num = np.linalg.cond(P_est_for_cond, 2)\n\n        # Task 3 (part 2): Sensitivity analysis\n        delta = 1e-7\n        jacobian_cols_norms = []\n        for i in range(m + 1):\n            pi_plus = pi_est.copy()\n            pi_minus = pi_est.copy()\n            \n            # Perturb pi while preserving the sum-to-1 simplex constraint\n            if i  0:\n                comp_idx = 0\n            else: # i == 0\n                comp_idx = 1\n\n            pi_plus[i] += delta\n            pi_plus[comp_idx] -= delta\n            pi_minus[i] -= delta\n            pi_minus[comp_idx] += delta\n            \n            # Check for non-negativity after perturbation\n            if np.any(pi_plus  0) or np.any(pi_minus  0):\n                # This should not happen with a small delta on non-zero probabilities\n                # If it does, a smaller delta or different handling might be needed.\n                # For this problem, we assume delta is small enough.\n                pass\n\n            x_hat_plus = full_correction_pipeline(pi_plus, y_obs, N, K)\n            x_hat_minus = full_correction_pipeline(pi_minus, y_obs, N, K)\n            \n            jac_col = (x_hat_plus - x_hat_minus) / (2 * delta)\n            jacobian_cols_norms.append(np.linalg.norm(jac_col, 2))\n\n        max_jac_norm = np.max(jacobian_cols_norms)\n        delta_pi_norm_1 = np.linalg.norm(pi_est - pi_true, 1)\n        sensitivity_bound = delta_pi_norm_1 * max_jac_norm\n        \n        all_results.append([error_e, sensitivity_bound, cond_num])\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{e},{b},{k}]\" for e, b, k in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "More complex phenomena like in-source fragmentation can lead to situations where the deconvolution problem is ill-posed, with many potential source molecules contributing to a few observed ion signals. We can find meaningful solutions by incorporating prior knowledge, such as the assumption that only a sparse subset of sources is active at any given moment. This advanced practice guides you through formulating the problem in a Bayesian framework, where Maximum A Posteriori (MAP) estimation leads to the powerful non-negative LASSO (Least Absolute Shrinkage and Selection Operator) model, demonstrating how regularization enables discovery in complex datasets .",
            "id": "3311144",
            "problem": "You are modeling in-source fragmentation assignment in liquid chromatography–mass spectrometry using a linear superposition model grounded in standard measurement theory. Consider a collection of candidate precursor sources whose ionization and in-source fragmentation yield a mixture of fragment and adduct intensities observed at the detector. Adopt the following well-tested base and core definitions:\n\n- Linearity of signal formation at the detector under low space-charge and no saturation: the expected measured intensity vector is a linear combination of source contributions through a fixed fragmentation operator. Write the model as $ \\mathbf{y} = \\mathbf{F}\\,\\mathbf{s} + \\boldsymbol{\\epsilon} $, where $ \\mathbf{y} \\in \\mathbb{R}^m $ is the observed vector of fragment/adduct intensities across $ m $ features, $ \\mathbf{s} \\in \\mathbb{R}_{\\ge 0}^n $ is the vector of non-negative precursor source contributions across $ n $ candidates, $ \\mathbf{F} \\in \\mathbb{R}^{m \\times n} $ is a fixed fragmentation operator encoding yields and co-elution effects, and $ \\boldsymbol{\\epsilon} $ is additive noise.\n\n- Independent Gaussian measurement noise: assume $ \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_m) $ with unknown but fixed $ \\sigma^2  0 $.\n\n- Sparsity of active sources: most candidate precursors do not meaningfully contribute at a given retention-time slice. Encode this with an independent exponential prior on each non-negative source $ s_i $, $ p(s_i) \\propto \\exp(-\\alpha s_i) $ for $ s_i \\ge 0 $, with $ \\alpha  0 $ a regularization hyperparameter.\n\nTask. Starting only from the above base, derive the Maximum A Posteriori (MAP) estimator for $ \\mathbf{s} $ and show it is a convex optimization problem with a one-dimensional hyperparameter $ \\lambda  0 $ that trades off data fidelity against sparsity in a manner consistent with the Least Absolute Shrinkage and Selection Operator (LASSO) (spelled out on first appearance) penalty. Then implement a solver that computes the non-negative sparse source vector $ \\widehat{\\mathbf{s}} $ for each provided test instance.\n\nScientific realism constraints. Maintain $ s_i \\ge 0 $ for all $ i $ to reflect non-negative ion counts. Use a numerically stable algorithm guaranteed to converge to the global minimum of the convex objective under the given modeling assumptions.\n\nTest suite. For each test case below, $ \\mathbf{F} $ is specified as a real-valued matrix and $ \\mathbf{y} $ as a real-valued vector, both in arbitrary consistent intensity units, and $ \\lambda $ is a positive real scalar. Your program must compute the estimator $ \\widehat{\\mathbf{s}} $ for each case and report the results. There are $ 5 $ cases designed to cover a diverse set of conditions: identity operator with shrinkage, underdetermined mixing with an overlapping fragment channel, collinear columns, a high-shrinkage boundary that yields the zero solution, and a case with a zero column in $ \\mathbf{F} $.\n\n- Case $ 1 $ (identity operator; simple shrinkage): $ \\mathbf{F} = \\mathbf{I}_3 $, $ \\mathbf{y} = [\\,1.0,\\,0.1,\\,3.0\\,]^\\top $, $ \\lambda = 0.2 $.\n\n- Case $ 2 $ (underdetermined with a shared fragment channel): \n  $ \\mathbf{F} = \\begin{bmatrix} 1.0  0.0  1.0 \\\\ 0.0  1.0  1.0 \\end{bmatrix} $, \n  $ \\mathbf{y} = [\\,1.0,\\,1.0\\,]^\\top $, \n  $ \\lambda = 0.1 $.\n\n- Case $ 3 $ (collinear columns; ambiguity under sparsity): \n  $ \\mathbf{F} = \\begin{bmatrix} 1.0  1.0  0.0 \\\\ 1.0  1.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix} $, \n  $ \\mathbf{y} = [\\,1.0,\\,1.0,\\,0.0\\,]^\\top $, \n  $ \\lambda = 0.05 $.\n\n- Case $ 4 $ (boundary with strong regularization; zero solution): \n  $ \\mathbf{F} = \\mathbf{I}_2 $, \n  $ \\mathbf{y} = [\\,0.05,\\,0.02\\,]^\\top $, \n  $ \\lambda = 0.1 $.\n\n- Case $ 5 $ (degenerate operator with a zero column): \n  $ \\mathbf{F} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\end{bmatrix} $, \n  $ \\mathbf{y} = [\\,0.5,\\,0.7\\,]^\\top $, \n  $ \\lambda = 0.1 $.\n\nComputational requirements. Implement a convergent algorithm appropriate for this convex problem class, for example a coordinate-descent scheme with exact one-dimensional minimizers per coordinate and non-negativity enforced, or an equivalent proximal-gradient approach. The program must not rely on any external input and must compute the estimators for the five cases as specified.\n\nAnswer format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this outer list corresponds to one test case in order and must itself be a list representing $ \\widehat{\\mathbf{s}} $ for that case. Every numeric entry must be rounded to exactly $ 6 $ decimal places. For example, an output with three cases and two-dimensional solutions would look like $[\\,[0.123456,0.000000],[\\dots],[\\dots]\\,]$.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in established models of analytical chemistry signal processing, mathematically well-posed as a convex optimization problem, and provides a complete and consistent set of definitions and data for its resolution.\n\nHerein, we derive the Maximum A Posteriori (MAP) estimator for the source contribution vector $ \\mathbf{s} $, demonstrate that it corresponds to a convex optimization problem of the non-negative Least Absolute Shrinkage and Selection Operator (LASSO) type, and outline the coordinate descent algorithm used for its numerical solution.\n\n### 1. Bayesian Formulation and MAP Estimation\n\nThe goal is to find the most probable source vector $ \\mathbf{s} $ given the observed data $ \\mathbf{y} $. Using Bayes' theorem, the posterior probability of $ \\mathbf{s} $ is proportional to the product of the likelihood of the data given the sources and the prior probability of the sources:\n$$\np(\\mathbf{s} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\mathbf{s}) p(\\mathbf{s})\n$$\nThe MAP estimate, $ \\widehat{\\mathbf{s}}_{\\text{MAP}} $, is the value of $ \\mathbf{s} $ that maximizes this posterior probability, subject to the non-negativity constraint $ \\mathbf{s} \\in \\mathbb{R}_{\\ge 0}^n $. This is equivalent to minimizing the negative logarithm of the posterior:\n$$\n\\widehat{\\mathbf{s}}_{\\text{MAP}} = \\arg\\max_{\\mathbf{s} \\ge 0} p(\\mathbf{s} | \\mathbf{y}) = \\arg\\min_{\\mathbf{s} \\ge 0} [-\\log p(\\mathbf{y} | \\mathbf{s}) - \\log p(\\mathbf{s})]\n$$\n\n### 2. Likelihood and Prior Definitions\n\n**Likelihood Function**: The linear model is $ \\mathbf{y} = \\mathbf{F}\\mathbf{s} + \\boldsymbol{\\epsilon} $, where the noise is independent and identically distributed Gaussian, $ \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_m) $. Therefore, the likelihood of observing $ \\mathbf{y} $ for a given $ \\mathbf{s} $ is described by a multivariate Gaussian distribution:\n$$\np(\\mathbf{y} | \\mathbf{s}) = \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2\\right)\n$$\nThe negative log-likelihood is then:\n$$\n-\\log p(\\mathbf{y} | \\mathbf{s}) = \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + \\frac{m}{2} \\log(2\\pi\\sigma^2)\n$$\n\n**Prior Distribution**: The sources $ s_i $ are assumed to follow an independent exponential prior for $ s_i \\ge 0 $, given as $ p(s_i) \\propto \\exp(-\\alpha s_i) $ with $ \\alpha  0 $. The joint prior for the vector $ \\mathbf{s} $ is the product of the individual priors:\n$$\np(\\mathbf{s}) = \\prod_{i=1}^n p(s_i) \\propto \\prod_{i=1}^n \\exp(-\\alpha s_i) = \\exp\\left(-\\alpha \\sum_{i=1}^n s_i\\right)\n$$\nSince each $ s_i \\ge 0 $, we have $ s_i = |s_i| $, and thus $ \\sum_{i=1}^n s_i = \\|\\mathbf{s}\\|_1 $. The prior can be written as $ p(\\mathbf{s}) \\propto \\exp(-\\alpha \\|\\mathbf{s}\\|_1) $. The negative log-prior is:\n$$\n-\\log p(\\mathbf{s}) = \\alpha \\|\\mathbf{s}\\|_1 + C\n$$\nwhere $ C $ is a normalization constant.\n\n### 3. The Non-Negative LASSO Formulation\n\nCombining the negative log-likelihood and negative log-prior, and dropping constant terms that do not depend on $ \\mathbf{s} $, the MAP optimization problem becomes:\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{s} \\ge 0} \\left\\{ \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + \\alpha \\|\\mathbf{s}\\|_1 \\right\\}\n$$\nThe objective function can be multiplied by a positive constant, $ 2\\sigma^2 $, without changing the location of the minimum. This yields:\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{s} \\ge 0} \\left\\{ \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + 2\\alpha\\sigma^2 \\|\\mathbf{s}\\|_1 \\right\\}\n$$\nBy defining the regularization hyperparameter $ \\lambda = 2\\alpha\\sigma^2 $, where $ \\lambda  0 $, we arrive at the final objective function. To match the common form used in machine learning literature, we add a factor of $ 1/2 $ to the data fitting term, which is equivalent to scaling the entire objective by $ 1/2 $:\n$$\n\\mathcal{L}(\\mathbf{s}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + \\lambda \\|\\mathbf{s}\\|_1\n$$\nThe estimator is the solution to the minimization problem:\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{s} \\ge 0} \\mathcal{L}(\\mathbf{s})\n$$\nThis is the formulation for the non-negative LASSO. The term $ \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 $ measures data fidelity (least squares error), while the term $ \\lambda \\|\\mathbf{s}\\|_1 $ is a penalty that encourages sparsity (many components of $ \\mathbf{s} $ to be zero). The hyperparameter $ \\lambda $ controls the trade-off between these two objectives.\n\n### 4. Convexity of the Problem\n\nThe optimization problem is convex. This is established by examining the components of the objective function and the constraint set:\n1.  The data fidelity term $ f(\\mathbf{s}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 $ is a quadratic function of $ \\mathbf{s} $. Its Hessian matrix is $ \\mathbf{F}^\\top\\mathbf{F} $, which is always positive semi-definite. Therefore, $ f(\\mathbf{s}) $ is a convex function.\n2.  The penalty term $ g(\\mathbf{s}) = \\lambda \\|\\mathbf{s}\\|_1 $ is a scaled L1-norm, which is a known convex function.\n3.  The objective function $ \\mathcal{L}(\\mathbf{s}) = f(\\mathbf{s}) + g(\\mathbf{s}) $ is the sum of two convex functions, which is itself convex.\n4.  The constraint set $ \\mathbf{s} \\ge 0 $ defines the non-negative orthant, which is a convex set.\n\nMinimizing a convex function over a convex set is a convex optimization problem. A key property is that any local minimum is also a global minimum, which guarantees that algorithms designed to find local minima will find the optimal solution.\n\n### 5. Algorithmic Solution via Coordinate Descent\n\nCoordinate descent is an effective algorithm for this problem. It iteratively minimizes the objective function with respect to a single coordinate $ s_k $ at a time, holding all other coordinates fixed.\n\nLet's derive the update rule for the $k$-th coordinate, $ s_k $. The objective function, viewed as a function of only $ s_k $, can be written as:\n$$\n\\mathcal{L}(s_k) = \\frac{1}{2} \\left\\| \\left(\\mathbf{y} - \\sum_{j \\neq k} s_j \\mathbf{f}_j\\right) - s_k \\mathbf{f}_k \\right\\|_2^2 + \\lambda s_k + \\lambda \\sum_{j \\neq k} s_j\n$$\nwhere $ \\mathbf{f}_j $ is the $j$-th column of $ \\mathbf{F} $. Let $ \\mathbf{r}_k = \\mathbf{y} - \\sum_{j \\neq k} s_j \\mathbf{f}_j $ be the residual when the $k$-th source is excluded. The terms not involving $ s_k $ are constant. We minimize:\n$$\n\\mathcal{L}(s_k) = \\frac{1}{2} \\|\\mathbf{r}_k - s_k \\mathbf{f}_k\\|_2^2 + \\lambda s_k + \\text{const} = \\frac{1}{2} (s_k^2 \\|\\mathbf{f}_k\\|_2^2 - 2s_k \\mathbf{f}_k^\\top \\mathbf{r}_k) + \\lambda s_k + \\text{const}\n$$\nThe derivative with respect to $ s_k $ (for $ s_k  0 $) is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_k} = s_k \\|\\mathbf{f}_k\\|_2^2 - \\mathbf{f}_k^\\top \\mathbf{r}_k + \\lambda\n$$\nSetting the derivative to zero gives the unconstrained minimizer $ s_k^* = \\frac{\\mathbf{f}_k^\\top \\mathbf{r}_k - \\lambda}{\\|\\mathbf{f}_k\\|_2^2} $. Incorporating the non-negativity constraint $ s_k \\ge 0 $, the solution to the one-dimensional subproblem is:\n$$\ns_k^{\\text{new}} = \\max\\left(0, \\frac{\\mathbf{f}_k^\\top \\mathbf{r}_k - \\lambda}{\\|\\mathbf{f}_k\\|_2^2}\\right)\n$$\nFor efficient computation, we pre-calculate $ \\mathbf{C} = \\mathbf{F}^\\top\\mathbf{F} $ and $ \\mathbf{c} = \\mathbf{F}^\\top\\mathbf{y} $. The term $ \\mathbf{f}_k^\\top \\mathbf{r}_k $ can be written as:\n$$\n\\mathbf{f}_k^\\top \\mathbf{r}_k = \\mathbf{f}_k^\\top \\left(\\mathbf{y} - \\sum_{j \\neq k} s_j \\mathbf{f}_j\\right) = (\\mathbf{F}^\\top\\mathbf{y})_k - \\sum_{j \\neq k} (\\mathbf{F}^\\top\\mathbf{F})_{kj} s_j = c_k - \\sum_{j \\neq k} C_{kj} s_j\n$$\nThe update rule for the $k$-th coordinate becomes:\n$$\ns_k \\leftarrow \\max\\left(0, \\frac{c_k - \\sum_{j \\neq k} C_{kj} s_j - \\lambda}{C_{kk}}\\right)\n$$\nIf $ C_{kk} = \\|\\mathbf{f}_k\\|_2^2 = 0 $, the $k$-th column of $ \\mathbf{F} $ is zero. In this case, $ s_k $ only appears in the penalty term $ \\lambda s_k $. To minimize this term for $ s_k \\ge 0 $, we must set $ s_k = 0 $. The algorithm iterates through all coordinates repeatedly until the solution vector $ \\mathbf{s} $ converges.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef coordinate_descent_solver(F, y, lam, max_iter=100000, tol=1e-9):\n    \"\"\"\n    Solves the non-negative LASSO problem using coordinate descent.\n\n    The problem is to minimize:\n    0.5 * ||y - Fs||^2_2 + lambda * ||s||_1\n    subject to s = 0.\n\n    Args:\n        F (np.ndarray): The operator matrix (m x n).\n        y (np.ndarray): The observation vector (m,).\n        lam (float): The regularization hyperparameter.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        np.ndarray: The estimated source vector s (n,).\n    \"\"\"\n    m, n = F.shape\n    \n    # Pre-compute matrices for efficiency\n    C = F.T @ F\n    c = F.T @ y\n    \n    # Initialize the source vector\n    s = np.zeros(n)\n    \n    # Pre-compute diagonal of C\n    C_diag = np.diag(C)\n\n    for i in range(max_iter):\n        s_old = s.copy()\n        \n        for k in range(n):\n            # The term sum_{j != k} C_kj * s_j can be calculated as\n            # (C[k,:] @ s) - C[k, k] * s[k]\n            off_diagonal_sum = C[k, :] @ s - C[k, k] * s[k]\n            \n            numerator = c[k] - off_diagonal_sum - lam\n            \n            if C_diag[k]  1e-12: # Check for non-zero column in F\n                s[k] = np.maximum(0, numerator / C_diag[k])\n            else:\n                # If the k-th column of F is all zeros, C_diag[k] is 0.\n                # The objective is minimized with respect to s_k when s_k = 0\n                # due to the penalty term lambda * s_k.\n                s[k] = 0.0\n\n        # Check for convergence\n        if np.linalg.norm(s - s_old, ord=np.inf)  tol:\n            break\n            \n    return s\n\ndef solve():\n    \"\"\"\n    Defines and solves the test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: identity operator; simple shrinkage\n        {\n            \"F\": np.eye(3),\n            \"y\": np.array([1.0, 0.1, 3.0]),\n            \"lambda\": 0.2\n        },\n        # Case 2: underdetermined with a shared fragment channel\n        {\n            \"F\": np.array([[1.0, 0.0, 1.0], [0.0, 1.0, 1.0]]),\n            \"y\": np.array([1.0, 1.0]),\n            \"lambda\": 0.1\n        },\n        # Case 3: collinear columns; ambiguity under sparsity\n        {\n            \"F\": np.array([[1.0, 1.0, 0.0], [1.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"y\": np.array([1.0, 1.0, 0.0]),\n            \"lambda\": 0.05\n        },\n        # Case 4: boundary with strong regularization; zero solution\n        {\n            \"F\": np.eye(2),\n            \"y\": np.array([0.05, 0.02]),\n            \"lambda\": 0.1\n        },\n        # Case 5: degenerate operator with a zero column\n        {\n            \"F\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"y\": np.array([0.5, 0.7]),\n            \"lambda\": 0.1\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        s_hat = coordinate_descent_solver(case[\"F\"], case[\"y\"], case[\"lambda\"])\n        all_results.append(s_hat)\n\n    # Format the final output string exactly as required.\n    result_strings = []\n    for s_vector in all_results:\n        # Format each number to 6 decimal places\n        formatted_nums = [f\"{num:.6f}\" for num in s_vector]\n        # Create the list string, e.g., \"[0.123456,0.000000]\"\n        result_strings.append(f\"[{','.join(formatted_nums)}]\")\n\n    # Join all case results into the final output format, e.g., \"[[...],[...]]\"\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}