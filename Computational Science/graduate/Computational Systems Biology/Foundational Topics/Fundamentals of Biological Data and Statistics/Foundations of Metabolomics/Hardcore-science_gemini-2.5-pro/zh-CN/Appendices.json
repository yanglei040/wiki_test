{
    "hands_on_practices": [
        {
            "introduction": "在基于稳定同位素的代谢组学中，一个核心挑战是区分示踪剂引入的重同位素和天然存在的重同位素。这种天然丰度效应会混淆观测到的质量同位素体分布（MID），必须进行校正才能准确估计真实的标记模式。本练习将此校正问题构建为一个线性系统 ，您将从第一性原理出发，通过离散卷积构建校正矩阵，并应用非负最小二乘法来反演出真实的MID。",
            "id": "3311163",
            "problem": "考虑一个通过高分辨率质谱法测量的代谢物，其质量同位素体分布 (MID) 由一个概率向量 $x_{\\text{true}} \\in \\mathbb{R}^{K+1}$ 表示，其中 $x_{\\text{true}}[j]$ 表示发生恰好 $j$ 次示踪剂衍生的重同位素取代的概率，对于 $j \\in \\{0,1,\\dots,K\\}$。分子中的每个非示踪原子都独立地发生天然丰度同位素取代，这会产生一个非负整数的质量漂移。将每个原子的质量漂移建模为从 $\\{0,1,\\dots,m\\}$ 上的一个离散分布 $\\pi$ 中进行的独立抽样，其概率为 $\\pi[0],\\pi[1],\\dots,\\pi[m]$，满足 $\\sum_{r=0}^{m} \\pi[r] = 1$，并假设有 $N$ 个这样的原子。将总天然丰度质量漂移 $S$ 定义为这 $N$ 次独立抽样的总和。观测到的 MID $y_{\\text{obs}} \\in \\mathbb{R}^{K+1}$ 通过以下线性系统与 $x_{\\text{true}}$ 相关：\n$$\ny_{\\text{obs}} = P(\\pi)\\, x_{\\text{true}},\n$$\n其中 $P(\\pi) \\in \\mathbb{R}^{(K+1)\\times(K+1)}$ 是一个下三角矩阵，它通过原子间的离散卷积编码了 $S$ 的分布。您的任务是：从第一性原理出发构建 $P(\\pi)$，不使用任何闭式简化公式；在 $\\pi$ 不确定的情况下，校正 $y_{\\text{obs}}$ 以估计 $x_{\\text{true}}$；并量化对 $\\pi$ 不确定性的敏感度。\n\n基本出发点：仅使用原子的独立性、概率公理以及“独立整值随机变量之和的分布由其各自概率质量函数的重复离散卷积给出”这一事实。将 $x_{\\text{true}}$ 和 $y_{\\text{obs}}$ 视为元素之和为 $1$ 的概率向量。\n\n定义以下计算任务：\n\n1. 给定 $N$、$\\pi$ 和 $K$，使用原子质量漂移分布 $\\pi$ 的 $N$ 重卷积（如有必要，截断至 $\\{0,1,\\dots,K\\}$）构建向量 $h \\in \\mathbb{R}^{K+1}$，其中 $h[r]$ 是 $S=r$ 的概率，对于 $r \\in \\{0,1,\\dots,K\\}$。然后通过设置以下公式来构建 $P(\\pi)$：\n$$\nP(\\pi)[k,j] = \n\\begin{cases}\nh[k-j],  \\text{if } k \\ge j, \\\\\n0,  \\text{if } k  j,\n\\end{cases}\n$$\n对于 $k,j \\in \\{0,1,\\dots,K\\}$。\n\n2. 给定 $y_{\\text{obs}}$ 和一个估计的 $\\pi_{\\text{est}}$，通过求解一个非负最小二乘问题来计算估计值 $x_{\\text{hat}}$：\n$$\n\\min_{x \\in \\mathbb{R}^{K+1}} \\|P(\\pi_{\\text{est}})\\,x - y_{\\text{obs}}\\|_2 \\quad \\text{subject to } x \\ge 0,\n$$\n随后对 $x$ 进行重新归一化，使其满足 $\\sum_{j=0}^{K} x[j] = 1$。\n\n3. 对 $\\pi$ 的敏感度：设 $\\pi_{\\text{true}}$ 是用于生成 $y_{\\text{obs}}$ 的真实原子分布，$\\pi_{\\text{est}}$ 是用于校正的模型。定义估计误差 $e = \\|x_{\\text{hat}} - x_{\\text{true}}\\|_2$。按如下方式估计 $x_{\\text{hat}}$ 相对于 $\\pi$ 不确定性的扰动的局部上界。通过在一个保持概率单纯形约束 $\\sum_{r=0}^{m} \\pi[r] = 1$ 的扰动基上使用中心差分，数值近似雅可比矩阵 $J = \\partial x_{\\text{hat}} / \\partial \\pi$。这是通过在一个选定的分量和一个补偿分量之间转移一个很小的量 $\\delta$ 来实现的。使用诱导算子范数界\n$$\n\\| \\Delta x_{\\text{hat}} \\|_2 \\le \\|\\Delta \\pi\\|_{1} \\cdot \\max_{i} \\| J_{:,i} \\|_2,\n$$\n来计算实际差异的上界，其中 $\\|\\Delta \\pi\\|_{1} = \\|\\pi_{\\text{est}} - \\pi_{\\text{true}}\\|_{1}$。报告此界以及 $e$。\n\n4. 同时报告谱条件数 $\\kappa_2(P(\\pi_{\\text{est}}))$ 以评估数值条件。\n\n本问题不涉及物理单位。\n\n使用以下参数值测试套件，其中包括一个一般情况、一个近简并边界情况和一个多重漂移边缘情况。对于每种情况，首先确定性地（无噪声）合成 $y_{\\text{obs}} = P(\\pi_{\\text{true}})\\,x_{\\text{true}}$，然后使用 $\\pi_{\\text{est}}$ 计算 $x_{\\text{hat}}$，最后计算 $e$、敏感度界和 $\\kappa_2(P(\\pi_{\\text{est}}))$。\n\n- 测试用例1（单漂移原子分布，中等 $N$）：\n  - $N = 12$\n  - $m = 1$，$\\pi_{\\text{true}} = [0.989,\\,0.011]$ 且 $\\pi_{\\text{est}} = [0.988,\\,0.012]$\n  - $K = 6$\n  - $x_{\\text{true}} = [0.25,\\,0.20,\\,0.18,\\,0.15,\\,0.12,\\,0.07,\\,0.03]$\n\n- 测试用例2（近简并重概率，较大 $N$）：\n  - $N = 30$\n  - $m = 1$，$\\pi_{\\text{true}} = [0.9995,\\,0.0005]$ 且 $\\pi_{\\text{est}} = [0.9985,\\,0.0015]$\n  - $K = 10$\n  - $x_{\\text{true}} = [0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.0]$\n\n- 测试用例3（每个原子多重漂移，包括 $+2$ 質量漂移）：\n  - $N = 6$\n  - $m = 2$，$\\pi_{\\text{true}} = [0.95,\\,0.03,\\,0.02]$ 且 $\\pi_{\\text{est}} = [0.952,\\,0.031,\\,0.017]$\n  - $K = 8$\n  - $x_{\\text{true}} = [0.15,\\,0.14,\\,0.13,\\,0.12,\\,0.11,\\,0.10,\\,0.09,\\,0.08,\\,0.08]$\n\n程序要求：\n\n- 通过 $\\pi$ 的显式 $N$ 重离散卷积构建 $P(\\pi)$（不使用闭式二项式或多项式简化公式）。\n- 使用非负最小二乘法计算 $x_{\\text{hat}}$，然后重新归一化使其和为 $1$。\n- 使用中心差分雅可比矩阵计算敏感度界，该方法扰动 $\\pi$ 的一个分量并用第零个分量进行补偿（对于第零个分量，则用第一个分量进行补偿）以保持单纯形约束；使用一个足够小的 $\\delta$ 以保持非负性。\n- 对于每个测试用例，按以下顺序输出一个包含三个实数的列表：误差 $e$、界和 $\\kappa_2(P(\\pi_{\\text{est}}))$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个项目本身是用于一个测试用例的、由方括号括起来的、逗号分隔的三元组，不含空格。例如，输出应如下所示：\n  - \"[[e1,b1,k1],[e2,b2,k2],[e3,b3,k3]]\"\n其中每个符号都被该测试用例计算出的浮点值替换。",
            "solution": "该问题要求构建并应用一个用于质谱代谢组学中天然丰度校正的数值工作流，包括对校正过程受底层原子同位素分布不确定性影响的敏感度分析。解决方案将按照规定从第一性原理出发进行开发。\n\n问题的核心在于代谢物的真实质量同位素体分布 (MID) $x_{\\text{true}}$ 与观测到的 MID $y_{\\text{obs}}$ 之间的关系。该关系被建模为一个线性系统 $y_{\\text{obs}} = P(\\pi) x_{\\text{true}}$。矩阵 $P(\\pi)$ 解释了分子非示踪部分中天然丰度同位素的混杂效应。\n\n由天然丰度产生的总质量漂移 $S$ 是 $N$ 个独立同分布随机变量的总和，每个随机变量代表单个原子的质量漂移。每个原子漂移的概率质量函数 (PMF) 由 $\\pi = [\\pi[0], \\pi[1], \\dots, \\pi[m]]$ 给出。根据概率公理，独立随机变量之和的 PMF 是它们各自 PMF 的离散卷积。\n\n设 $h^{(n)}$ 是 $n$ 个原子总质量漂移的 PMF。我们有 $h^{(1)} = \\pi$。$n$ 个原子的 PMF 可通过将 $n-1$ 个原子的 PMF 与 $\\pi$ 进行卷积得到：\n$$\nh^{(n)}[k] = (h^{(n-1)} * \\pi)[k] = \\sum_{j=0}^{k} h^{(n-1)}[j] \\pi[k-j]\n$$\n为了找到 $N$ 个原子总漂移 $S$ 的 PMF（记为 $h$），我们迭代执行此卷积 $N-1$ 次。然后必须将得到的向量 $h$ 截断为相关长度 $K+1$。\n\n**步骤 1：构建校正矩阵 $P(\\pi)$**\n\n给定 $N$、$\\pi$ 和 $K$，我们首先计算代表总天然丰度质量漂移 $S$ 的 PMF 的向量 $h$，并将其截断为大小 $K+1$。\n1.  用原子 PMF $\\pi$ 初始化一个向量 `h_current`。\n2.  重复 $N-1$ 次：通过将 `h_current` 与 $\\pi$ 卷积来更新它。这将得到 $N$ 个原子漂移总和的 PMF。\n3.  最终向量 $h$ 是通过取上一步结果的前 $K+1$ 个元素得到的。\n4.  然后构建大小为 $(K+1) \\times (K+1)$ 的矩阵 $P(\\pi)$。其元素由 $P(\\pi)[k,j] = h[k-j]$（对于 $k \\ge j$）和 $0$（对于其他情况）给出。这种结构反映了真实的质量同位素体 $j$ 只能对观测到的质量同位素体 $k \\ge j$ 有贡献，其质量差 $k-j$ 源于天然丰度。这将产生一个下三角托普利茨矩阵。\n\n**步骤 2：估计真实 MID $x_{\\text{true}}$**\n\n给定一个观测到的 MID $y_{\\text{obs}}$ 和一个估计的原子 PMF $\\pi_{\\text{est}}$，我们旨在为真实 MID $x_{\\text{true}}$ 找到一个估计值 $x_{\\text{hat}}$。该问题被描述为对线性系统求逆，但附加了 $x$ 的元素必须为非负的约束。这导致了非负最小二乘 (NNLS) 问题：\n$$\n\\min_{x \\in \\mathbb{R}^{K+1}} \\|P(\\pi_{\\text{est}})\\,x - y_{\\text{obs}}\\|_2 \\quad \\text{subject to } x_j \\ge 0 \\text{ for all } j.\n$$\n这是一个标准的凸优化问题，可以使用已有算法求解。在求出非负向量（例如 $x_{\\text{nnls}}$）后，必须对其进行重新归一化，以确保它代表一个有效的概率分布，即其元素之和为 $1$。\n$$\nx_{\\text{hat}}[j] = \\frac{x_{\\text{nnls}}[j]}{\\sum_{i=0}^{K} x_{\\text{nnls}}[i]}\n$$\n为了数值稳定性，应在分母上加上一个小的 epsilon 值 $\\epsilon > 0$，尽管在此问题背景下，总和预计不会为零。\n\n**步骤 3：敏感度分析**\n\n这一步量化了估计的 MID $x_{\\text{hat}}$ 对估计的原子 PMF $\\pi_{\\text{est}}$ 中不确定性的敏感程度。我们被要求计算误差放大的上界。该界定义为 $\\|\\Delta \\pi\\|_{1} \\cdot \\max_{i} \\| J_{:,i} \\|_2$，其中 $J = \\partial x_{\\text{hat}} / \\partial \\pi$ 是雅可比矩阵，$\\Delta \\pi = \\pi_{\\text{est}} - \\pi_{\\text{true}}$。\n\n雅可比矩阵 $J$ 使用中心差分进行数值近似。一个关键细节是，对 $\\pi$ 的任何扰动都必须保持其元素之和为 $1$ 的约束。规定的方案是扰动一个分量 $\\pi[i]$ 并对另一个分量施加补偿性扰动。\n对于每个 $i \\in \\{0, 1, \\dots, m\\}$：\n1.  定义一个大小为 $\\delta \\ll 1$ 的扰动。\n2.  创建两个扰动向量 $\\pi^{+}$ 和 $\\pi^{-}$：\n    -   如果 $i > 0$：$\\pi^{+}[i] = \\pi_{\\text{est}}[i] + \\delta$, $\\pi^{+}[0] = \\pi_{\\text{est}}[0] - \\delta$。以及 $\\pi^{-}[i] = \\pi_{\\textest}[i] - \\delta$, $\\pi^{-}[0] = \\pi_{\\text{est}}[0] + \\delta$。\n    -   如果 $i = 0$：$\\pi^{+}[0] = \\pi_{\\text{est}}[0] + \\delta$, $\\pi^{+}[1] = \\pi_{\\text{est}}[1] - \\delta$。以及 $\\pi^{-}[0] = \\pi_{\\text{est}}[0] - \\delta$, $\\pi^{-}[1] = \\pi_{\\text{est}}[1] + \\delta$。\n3.  对于每个扰动向量，重新计算整个校正流程：构建 $P(\\pi^{\\pm})$，求解 NNLS 问题得到 $x_{\\text{nnls}}(\\pi^{\\pm})$，并重新归一化得到 $x_{\\text{hat}}(\\pi^{\\pm})$。\n4.  然后，雅可比矩阵的第 $i$ 列近似为：\n    $$\n    J_{:,i} \\approx \\frac{x_{\\text{hat}}(\\pi^{+}) - x_{\\text{hat}}(\\pi^{-})}{2\\delta}\n    $$\n在计算完 $J$ 的所有 $m+1$ 列后，我们找到它们欧几里得范数的最大值：$\\max_{i} \\| J_{:,i} \\|_2$。然后将此最大范数乘以 $\\pi$ 中误差的 $L_1$ 范数 $\\|\\pi_{\\text{est}} - \\pi_{\\text{true}}\\|_1$ 来计算敏感度界。\n\n最后，计算实际的估计误差 $e = \\|x_{\\text{hat}} - x_{\\text{true}}\\|_2$，以便与界进行比较。\n\n**步骤 4：数值条件评估**\n\n求解线性系统的数值稳定性与矩阵 $P(\\pi_{\\text{est}})$ 的条件数有关。谱条件数 $\\kappa_2(P(\\pi_{\\text{est}}))$ 是矩阵最大奇异值与最小奇异值的比值。一个大的条件数表明矩阵接近奇异，$y_{\\text{obs}}$ 或 $P(\\pi_{\\text{est}})$ 中的小误差可能会在解 $x_{\\text{hat}}$ 中被极大放大。这可以使用标准线性代数例程来计算。\n\n这四个步骤提供了一个完整的框架，用于执行天然丰度校正、评估其数值条件并量化其对模型参数的敏感度。对于每个给定的测试用例，实现都将遵循这一逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"N\": 12, \"m\": 1, \"K\": 6,\n            \"pi_true\": np.array([0.989, 0.011]),\n            \"pi_est\": np.array([0.988, 0.012]),\n            \"x_true\": np.array([0.25, 0.20, 0.18, 0.15, 0.12, 0.07, 0.03])\n        },\n        {\n            \"N\": 30, \"m\": 1, \"K\": 10,\n            \"pi_true\": np.array([0.9995, 0.0005]),\n            \"pi_est\": np.array([0.9985, 0.0015]),\n            \"x_true\": np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0])\n        },\n        {\n            \"N\": 6, \"m\": 2, \"K\": 8,\n            \"pi_true\": np.array([0.95, 0.03, 0.02]),\n            \"pi_est\": np.array([0.952, 0.031, 0.017]),\n            \"x_true\": np.array([0.15, 0.14, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08, 0.08])\n        }\n    ]\n\n    all_results = []\n\n    def construct_P(N, pi, K):\n        \"\"\"Constructs the correction matrix P from N, pi, and K.\"\"\"\n        # 1. N-fold convolution of pi\n        h_conv = pi.copy()\n        for _ in range(N - 1):\n            h_conv = np.convolve(h_conv, pi)\n        \n        # 2. Truncate to get h vector\n        h = np.zeros(K + 1)\n        len_h = min(len(h_conv), K + 1)\n        h[:len_h] = h_conv[:len_h]\n\n        # 3. Construct P matrix\n        P = np.zeros((K + 1, K + 1))\n        for j in range(K + 1):\n            for k in range(j, K + 1):\n                P[k, j] = h[k - j]\n        return P\n\n    def full_correction_pipeline(pi, y_obs, N, K):\n        \"\"\"Runs the full correction process for a given pi and y_obs.\"\"\"\n        P_est = construct_P(N, pi, K)\n        x_nnls, _ = nnls(P_est, y_obs)\n        # Renormalize\n        sum_x = np.sum(x_nnls)\n        if sum_x > 1e-9:\n             x_hat = x_nnls / sum_x\n        else: # Handle zero-sum case\n             x_hat = np.full(K + 1, 1.0 / (K + 1))\n        return x_hat\n\n    for case in test_cases:\n        N, m, K = case[\"N\"], case[\"m\"], case[\"K\"]\n        pi_true, pi_est, x_true_raw = case[\"pi_true\"], case[\"pi_est\"], case[\"x_true\"]\n        \n        # Ensure x_true is a probability vector\n        x_true = x_true_raw / np.sum(x_true_raw)\n\n        # Synthesize observed data\n        P_true = construct_P(N, pi_true, K)\n        y_obs = P_true @ x_true\n        \n        # Task 2: Compute x_hat\n        x_hat = full_correction_pipeline(pi_est, y_obs, N, K)\n        \n        # Task 3 (part 1): Estimation error\n        error_e = np.linalg.norm(x_hat - x_true, 2)\n        \n        # Task 4: Condition number\n        P_est_for_cond = construct_P(N, pi_est, K)\n        cond_num = np.linalg.cond(P_est_for_cond, 2)\n\n        # Task 3 (part 2): Sensitivity analysis\n        delta = 1e-7\n        jacobian_cols_norms = []\n        for i in range(m + 1):\n            pi_plus = pi_est.copy()\n            pi_minus = pi_est.copy()\n            \n            # Perturb pi while preserving the sum-to-1 simplex constraint\n            if i > 0:\n                comp_idx = 0\n            else: # i == 0\n                comp_idx = 1\n\n            pi_plus[i] += delta\n            pi_plus[comp_idx] -= delta\n            pi_minus[i] -= delta\n            pi_minus[comp_idx] += delta\n            \n            # Check for non-negativity after perturbation\n            if np.any(pi_plus  0) or np.any(pi_minus  0):\n                # This should not happen with a small delta on non-zero probabilities\n                # If it does, a smaller delta or different handling might be needed.\n                # For this problem, we assume delta is small enough.\n                pass\n\n            x_hat_plus = full_correction_pipeline(pi_plus, y_obs, N, K)\n            x_hat_minus = full_correction_pipeline(pi_minus, y_obs, N, K)\n            \n            jac_col = (x_hat_plus - x_hat_minus) / (2 * delta)\n            jacobian_cols_norms.append(np.linalg.norm(jac_col, 2))\n\n        max_jac_norm = np.max(jacobian_cols_norms)\n        delta_pi_norm_1 = np.linalg.norm(pi_est - pi_true, 1)\n        sensitivity_bound = delta_pi_norm_1 * max_jac_norm\n        \n        all_results.append([error_e, sensitivity_bound, cond_num])\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{e},{b},{k}]\" for e, b, k in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "与天然丰度校正中结构良好的问题不同，许多代谢组学反演问题可能不具备唯一解。例如，当多种代谢物形成相同的加合物或带电状态时，它们的信号会发生线性混合。在本练习中 ，我们将探讨可识别性（identifiability）这一关键概念，您将学习如何使用矩阵的秩和条件数等线性代数工具来判断是否可以从观测信号中唯一地确定代谢物浓度。",
            "id": "3311156",
            "problem": "考虑一个代谢组学中的质谱测量模型，其中多种加合物和电荷态导致代谢物贡献的线性混合。该测量模型是加性的，由 $y = A x + \\epsilon$ 给出，其中 $y \\in \\mathbb{R}^{m}$ 是 $m$ 个提取离子特征的观测信号向量，$x \\in \\mathbb{R}^{n}$ 是 $n$ 种代谢物的非负浓度向量，$A \\in \\mathbb{R}^{m \\times n}$ 编码了从代谢物浓度到预期特征强度的线性化加合物和电荷态映射，$\\epsilon \\in \\mathbb{R}^{m}$ 是加性测量噪声。在此背景下，可识别性检验应确定在给定的 $A$ 和 $\\epsilon$ 条件下，从 $x$ 到 $y$ 的映射相对于 $x$ 是否唯一可逆，并评估该反演过程是否数值稳定。从线性系统和最小二乘法的基础出发，如下定义并实现可识别性：当线性映射具有零核空间时，$x$ 存在理论唯一性，而最小二乘唯一性等价地由正规算子的正定性来表征。对于数值稳定性，通过由奇异值定义的 $A$ 的条件数来量化其敏感性。您的程序必须为每种情况实现以下输出：一个表示理论可识别性的布尔值，一个在指定条件数阈值下表示实践数值可识别性的布尔值，$A$ 的条件数，以及最小化 $\\lVert y - A x \\rVert_2^2$ 的最小二乘解的估计误差的欧几里得范数。\n\n使用以下参数值测试套件，其中每个矩阵和向量都被明确指定。在所有情况下，通过 $y = A x_{\\text{true}} + \\epsilon$ 构建 $y$。目标是确定在给定的 $A$ 和 $\\epsilon$ 下，$x$ 从 $y$ 中是否唯一，并量化稳定性和估计误差。\n\n情况 $\\mathbf{1}$ (理想情况，满列秩，中等条件数): 设 $m = 5$, $n = 3$, \n$$\nA^{(1)} = \\begin{bmatrix}\n1.0  0.8  0.6 \\\\\n0.5  0.4  0.3 \\\\\n0.1  0.05  0.0 \\\\\n0.3  0.2  0.1 \\\\\n0.0  0.1  0.4\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(1)} = \\begin{bmatrix} 10.0 \\\\ 5.0 \\\\ 2.0 \\end{bmatrix},\\quad\n\\epsilon^{(1)} = \\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.0 \\\\ 0.005 \\\\ -0.01 \\end{bmatrix}.\n$$\n\n情况 $\\mathbf{2}$ (因加合物模糊性导致的秩亏): 设 $m = 5$, $n = 4$, 其中两个相同的列代表无法区分的加合物贡献，\n$$\nA^{(2)} = \\begin{bmatrix}\n1.0  0.7  0.3  0.7 \\\\\n0.2  0.1  0.05  0.1 \\\\\n0.0  0.2  0.1  0.2 \\\\\n0.3  0.1  0.2  0.1 \\\\\n0.4  0.6  0.5  0.6\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(2)} = \\begin{bmatrix} 3.0 \\\\ 1.5 \\\\ 2.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\epsilon^{(2)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.01 \\\\ -0.01 \\end{bmatrix}.\n$$\n\n情况 $\\mathbf{3}$ (病态，加合物映射近似共线): 设 $m = 6$, $n = 3$, 定义 $A^{(3)}$ 的列为\n$$\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 0.2 \\\\ 0.1 \\\\ 0.05 \\\\ 0.02 \\end{bmatrix},\\quad\nc_2 = \\begin{bmatrix} 1.0 \\cdot (1 + 10^{-10}) \\\\ 0.5 \\cdot (1 + 10^{-10}) \\\\ 0.2 \\cdot (1 + 10^{-10}) \\\\ 0.1 \\cdot (1 + 10^{-10}) \\\\ 0.05 \\cdot (1 + 10^{-10}) \\\\ 0.02 \\cdot (1 + 10^{-10}) + 10^{-12} \\end{bmatrix},\\quad\nc_3 = \\begin{bmatrix} 0.01 \\\\ 0.005 \\\\ 0.002 \\\\ 0.001 \\\\ 0.0005 \\\\ 0.0002 \\end{bmatrix},\n$$\n且 $A^{(3)} = [\\,c_1\\; c_2\\; c_3\\,]$,\n$$\nx_{\\text{true}}^{(3)} = \\begin{bmatrix} 10.0 \\\\ 10.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\epsilon^{(3)} = \\begin{bmatrix} 10^{-6} \\\\ -10^{-6} \\\\ 2\\cdot 10^{-6} \\\\ -2\\cdot 10^{-6} \\\\ 10^{-6} \\\\ -10^{-6} \\end{bmatrix}.\n$$\n\n情况 $\\mathbf{4}$ (欠定系统：代谢物多于特征): 设 $m = 3$, $n = 5$,\n$$\nA^{(4)} = \\begin{bmatrix}\n1.0  0.2  0.0  0.5  0.1 \\\\\n0.0  0.1  1.0  0.0  0.2 \\\\\n0.5  0.3  0.2  0.1  0.0\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(4)} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 0.5 \\\\ 0.8 \\\\ 0.3 \\end{bmatrix},\\quad\n\\epsilon^{(4)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n情况 $\\mathbf{5}$ (方阵系统，异方差噪声): 设 $m = 4$, $n = 4$,\n$$\nA^{(5)} = \\begin{bmatrix}\n1.0  0.0  0.5  0.2 \\\\\n0.1  1.0  0.3  0.0 \\\\\n0.0  0.2  1.0  0.1 \\\\\n0.3  0.0  0.0  1.0\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(5)} = \\begin{bmatrix} 2.0 \\\\ 1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix},\\quad\n\\epsilon^{(5)} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.1 \\\\ -0.15 \\end{bmatrix}.\n$$\n\n您的程序必须遵循以下第一性原理准则：$x$ 的理论可识别性取决于线性映射 $A$ 是否为满列秩，这意味着 $A$ 的核为 $\\{0\\}$ 且正规算子 $A^{\\top}A$ 是正定的；最小二乘唯一性等价于此满列秩条件。实践数值可识别性通过限制条件数来确定，定义为 $\\kappa(A) = s_{\\max}/s_{\\min}$，其中 $s_{\\max}$ 和 $s_{\\min}$ 分别是 $A$ 的最大和最小奇异值。使用 $\\tau = 10^{-12} s_{\\max}$ 的奇异值阈值计算秩，并且仅当理论上可识别且 $\\kappa(A)  10^{8}$ 时，才声明为实践数值可识别。对于每种情况，计算最小化 $\\lVert y - A x \\rVert_2^2$ 的最小二乘估计 $\\hat{x}$，并报告估计误差范数 $\\lVert \\hat{x} - x_{\\text{true}} \\rVert_2$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个列表的列表，每个子列表对应一种情况，格式为 $[b_{\\text{theory}}, b_{\\text{numeric}}, \\kappa, e]$，其中 $b_{\\text{theory}}$ 和 $b_{\\text{numeric}}$ 分别是表示理论和实践可识别性的布尔值，$\\kappa$ 是 $A$ 的条件数，$e$ 是估计误差的欧几里得范数。将浮点数 $\\kappa$ 和 $e$ 四舍五入至 $6$ 位小数。程序必须以精确的格式单行输出，例如 $[[\\text{True},\\text{True},1.234567,0.000001],[\\dots]]$，无任何附加文本。不涉及物理单位。不使用角度。不使用百分比。该列表必须由指定的输入完全确定，不含随机性。",
            "solution": "该问题要求对线性模型 $y = A x + \\epsilon$ 进行可识别性分析，这是代谢组学中质谱数据的常见表示方法。在这里，$y \\in \\mathbb{R}^{m}$ 代表 $m$ 个离子特征的测量强度，$x \\in \\mathbb{R}^{n}$ 是 $n$ 种代谢物的浓度向量，$A \\in \\mathbb{R}^{m \\times n}$ 是一个将代谢物浓度线性映射到其对离子特征预期贡献的矩阵，其中考虑了加合物和电荷态。$\\epsilon \\in \\mathbb{R}^{m}$ 项代表加性测量噪声。任务是评估在给定映射 $A$ 的情况下，从测量值 $y$ 中识别代谢物浓度 $x$ 的理论和实践可识别性。\n\n**1. 理论可识别性**\n\n理论可识别性关注在理想化的无噪声情况（即 $\\epsilon=0$）下，系统 $y = Ax$ 的解 $x$ 的唯一性。对于 $A$ 的值域中的任何给定 $y$，当且仅当由 $A$ 表示的线性映射是一对一（单射）时，存在唯一的解 $x$。这等价于矩阵 $A$ 的零空间（或核）是平凡的，即只包含零向量：$\\ker(A) = \\{0\\}$。\n\n对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其核为平凡的当且仅当其列是线性无关的。这意味着矩阵的秩必须等于其列数 $n$。形式上，理论可识别性的条件是：\n$$\n\\text{rank}(A) = n\n$$\n这个条件也称为 $A$ 具有满列秩。如果此条件成立，则格拉姆矩阵 $A^{\\top}A \\in \\mathbb{R}^{n \\times n}$ 是对称正定的，这保证了唯一的最小二乘解存在。\n\n满列秩的一个必要条件是行数（测量值数量）必须大于或等于列数（未知数数量），即 $m \\geq n$。如果 $m  n$，则该系统是欠定的，$\\text{rank}(A) \\leq m  n$，不可能有唯一的解 $x$；存在无穷多个解。\n\n**2. 实践与数值可识别性**\n\n即使一个系统在理论上是可识别的（即 $A$ 具有满列秩），由于数值不稳定性，其解在实践中也可能无法识别。如果 $A$ 的列近似线性相关，则称该系统是病态的。在这种情况下，由噪声 $\\epsilon$ 引起的测量向量 $y$ 的微小扰动可能导致估计解 $\\hat{x}$ 发生巨大变化。\n\n这种敏感性通过矩阵 $A$ 的条件数来量化，记为 $\\kappa(A)$。利用 $A$ 的奇异值，其中 $s_{\\max}$ 是最大奇异值，$s_{\\min}$ 是最小的非零奇异值，条件数定义为：\n$$\n\\kappa(A) = \\frac{s_{\\max}}{s_{\\min}}\n$$\n一个良态矩阵的条件数很小（接近 1），而一个病态矩阵的条件数非常大。问题指定了一个阈值 $10^8$：仅当系统理论上可识别且 $\\kappa(A)  10^8$ 时，才认为该系统在实践上是可识别的。\n\n**3. 计算算法**\n\n对于由一组参数 $(A, x_{\\text{true}}, \\epsilon)$ 指定的每个测试用例，执行以下步骤：\n1.  构造测量向量 $y$，即 $y = A x_{\\text{true}} + \\epsilon$。\n2.  计算 $A$ 的奇异值分解 (SVD)。SVD 得到奇异值 $\\{s_i\\}$，用于所有后续分析。\n3.  **理论可识别性 ($b_{\\text{theory}}$)** 的确定。通过计算满足 $s_i > \\tau$ 的奇异值 $s_i$ 的数量来计算 $A$ 的秩，其中阈值 $\\tau$ 定义为 $\\tau = 10^{-12} s_{\\max}$。如果计算出的秩等于 $n$，则 $b_{\\text{theory}}$ 设置为 `True`；否则设置为 `False`。\n4.  **实践可识别性 ($b_{\\text{numeric}}$)** 的确定。根据奇异值计算条件数 $\\kappa(A)$。如果 $b_{\\text{theory}}$ 为 `True` 且 $\\kappa(A)  10^8$，则 $b_{\\text{numeric}}$ 设置为 `True`；否则设置为 `False`。\n5.  计算**最小二乘估计 ($\\hat{x}$)**。这是使残差的欧几里得范数平方 $\\|y - A x\\|_2^2$ 最小化的向量 $\\hat{x}$。该计算可使用标准的数值线性代数程序（例如 `numpy.linalg.lstsq` 提供的程序）稳健地完成，这些程序能正确处理满秩、秩亏和欠定系统。对于秩亏和欠定情况，此函数返回唯一的最小范数解。\n6.  计算**估计误差 ($e$)**，即估计浓度向量与真实浓度向量之差的欧几里得范数：$e = \\|\\hat{x} - x_{\\text{true}}\\|_2$。\n\n此过程根据严格的可识别性和数值稳定性准则系统地评估每种情况，从而对基础逆问题的可解性进行定量评估。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format the results for all test cases.\n    \"\"\"\n\n    # Case 1: Happy path, full column rank, moderate conditioning\n    A1 = np.array([\n        [1.0, 0.8, 0.6],\n        [0.5, 0.4, 0.3],\n        [0.1, 0.05, 0.0],\n        [0.3, 0.2, 0.1],\n        [0.0, 0.1, 0.4]\n    ])\n    x_true1 = np.array([10.0, 5.0, 2.0])\n    epsilon1 = np.array([0.01, -0.02, 0.0, 0.005, -0.01])\n    case1 = (A1, x_true1, epsilon1)\n\n    # Case 2: Rank-deficient due to adduct ambiguity\n    A2 = np.array([\n        [1.0, 0.7, 0.3, 0.7],\n        [0.2, 0.1, 0.05, 0.1],\n        [0.0, 0.2, 0.1, 0.2],\n        [0.3, 0.1, 0.2, 0.1],\n        [0.4, 0.6, 0.5, 0.6]\n    ])\n    x_true2 = np.array([3.0, 1.5, 2.0, 0.5])\n    epsilon2 = np.array([0.0, 0.0, 0.0, 0.01, -0.01])\n    case2 = (A2, x_true2, epsilon2)\n\n    # Case 3: Ill-conditioned, nearly collinear adduct mappings\n    c1 = np.array([1.0, 0.5, 0.2, 0.1, 0.05, 0.02])\n    c2 = np.array([\n        1.0 * (1.0 + 1e-10), \n        0.5 * (1.0 + 1e-10), \n        0.2 * (1.0 + 1e-10), \n        0.1 * (1.0 + 1e-10), \n        0.05 * (1.0 + 1e-10), \n        0.02 * (1.0 + 1e-10) + 1e-12\n    ])\n    c3 = np.array([0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002])\n    A3 = np.vstack([c1, c2, c3]).T\n    x_true3 = np.array([10.0, 10.0, 0.5])\n    epsilon3 = np.array([1e-6, -1e-6, 2e-6, -2e-6, 1e-6, -1e-6])\n    case3 = (A3, x_true3, epsilon3)\n\n    # Case 4: Underdetermined system\n    A4 = np.array([\n        [1.0, 0.2, 0.0, 0.5, 0.1],\n        [0.0, 0.1, 1.0, 0.0, 0.2],\n        [0.5, 0.3, 0.2, 0.1, 0.0]\n    ])\n    x_true4 = np.array([1.0, 2.0, 0.5, 0.8, 0.3])\n    epsilon4 = np.array([0.0, 0.0, 0.0])\n    case4 = (A4, x_true4, epsilon4)\n\n    # Case 5: Square system\n    A5 = np.array([\n        [1.0, 0.0, 0.5, 0.2],\n        [0.1, 1.0, 0.3, 0.0],\n        [0.0, 0.2, 1.0, 0.1],\n        [0.3, 0.0, 0.0, 1.0]\n    ])\n    x_true5 = np.array([2.0, 1.0, 0.5, 3.0])\n    epsilon5 = np.array([0.05, -0.02, 0.1, -0.15])\n    case5 = (A5, x_true5, epsilon5)\n\n    test_cases = [case1, case2, case3, case4, case5]\n    results = []\n\n    for case in test_cases:\n        A, x_true, epsilon = case\n        \n        # Unpack dimensions\n        m, n = A.shape\n        \n        # Construct observed signal vector y\n        y = A @ x_true + epsilon\n        \n        # --- Identifiability Analysis ---\n        \n        # Compute singular values for rank and condition number\n        s = np.linalg.svd(A, compute_uv=False)\n        \n        # Theoretical identifiability based on rank\n        b_theory = False\n        s_max = 0.0\n        if s.size > 0: # Handle empty matrix case\n            s_max = s[0]\n            # Rank is computed using the problem-specified threshold\n            rank_threshold = 1e-12 * s_max\n            rank = np.sum(s > rank_threshold)\n            if rank == n:\n                b_theory = True\n        \n        # Compute condition number\n        # np.linalg.cond returns a large number for singular/ill-conditioned matrices\n        kappa = np.linalg.cond(A) if s_max > 0 else np.inf\n        \n        # Practical numerical identifiability\n        b_numeric = b_theory and (kappa  1e8)\n        \n        # --- Least Squares Estimation ---\n        \n        # Compute the least-squares estimate for x\n        x_hat = np.linalg.lstsq(A, y, rcond=None)[0]\n        \n        # Compute the Euclidean norm of the estimation error\n        e = np.linalg.norm(x_hat - x_true)\n        \n        # Append the list of results for this case\n        results.append([b_theory, b_numeric, round(float(kappa), 6), round(float(e), 6)])\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "当观测特征的数量少于潜在的代谢物来源时，或者当不同来源产生非常相似的信号模式时，系统就会变得欠定或病态，此时经典的反演方法会失效。然而，我们可以利用一个合理的物理假设：在任何给定时刻，只有少数前体源是活跃的（即稀疏性）。本练习  指导您将此稀疏性先验知识整合到贝叶斯最大后验估计（MAP）框架中，从而推导出非负LASSO（Least Absolute Shrinkage and Selection Operator）模型，这是一种用于求解稀疏解的强大优化方法。",
            "id": "3311144",
            "problem": "您正在使用基于标准测量理论的线性叠加模型，对液相色谱-质谱联用技术中的源内碎裂分配进行建模。考虑一组候选前体源，其电离和源内碎裂在检测器上产生可观测到的碎片和加合物强度的混合物。采用以下经过充分检验的基本和核心定义：\n\n- 在低空间电荷且无饱和情况下，检测器信号形成的线性度：期望测量强度向量是源贡献通过一个固定碎裂算子的线性组合。模型写作 $ \\mathbf{y} = \\mathbf{F}\\,\\mathbf{s} + \\boldsymbol{\\epsilon} $，其中 $ \\mathbf{y} \\in \\mathbb{R}^m $ 是在 $ m $ 个特征上观测到的碎片/加合物强度向量，$ \\mathbf{s} \\in \\mathbb{R}_{\\ge 0}^n $ 是在 $ n $ 个候选前体中的非负源贡献向量，$ \\mathbf{F} \\in \\mathbb{R}^{m \\times n} $ 是一个固定的碎裂算子，编码了产额和共洗脱效应，而 $ \\boldsymbol{\\epsilon} $ 是加性噪声。\n\n- 独立高斯测量噪声：假设 $ \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_m) $，其中 $ \\sigma^2 > 0 $ 未知但固定。\n\n- 活性源的稀疏性：在给定的保留时间切片上，大多数候选前体没有显著贡献。通过对每个非负源 $ s_i $ 使用独立的指数先验来编码此特性，$ p(s_i) \\propto \\exp(-\\alpha s_i) $，对于 $ s_i \\ge 0 $，其中 $ \\alpha > 0 $ 是一个正则化超参数。\n\n任务。仅从上述基础出发，推导 $ \\mathbf{s} $ 的最大后验 (MAP) 估计量，并证明它是一个凸优化问题，其一维超参数 $ \\lambda > 0 $ 以一种与最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO)（首次出现时写全称）惩罚一致的方式，在数据保真度与稀疏性之间进行权衡。然后实现一个求解器，为每个提供的测试实例计算非负稀疏源向量 $ \\widehat{\\mathbf{s}} $。\n\n科学真实性约束。保持所有 $ i $ 的 $ s_i \\ge 0 $，以反映非负离子计数。使用一个在给定建模假设下保证收敛到凸目标全局最小值的数值稳定算法。\n\n测试套件。对于下方的每个测试案例，$ \\mathbf{F} $ 被指定为一个实值矩阵，$ \\mathbf{y} $ 被指定为一个实值向量，两者都使用任意一致的强度单位，$ \\lambda $ 是一个正实标量。您的程序必须计算每个案例的估计量 $ \\widehat{\\mathbf{s}} $ 并报告结果。设计了 $ 5 $ 个案例以覆盖多种不同情况：带收缩的单位算子、具有重叠碎片通道的欠定混合、共线列、产生零解的高收缩边界，以及 $ \\mathbf{F} $ 中含一个零列的案例。\n\n- 案例 $ 1 $（单位算子；简单收缩）：$ \\mathbf{F} = \\mathbf{I}_3 $，$ \\mathbf{y} = [\\,1.0,\\,0.1,\\,3.0\\,]^\\top $，$ \\lambda = 0.2 $。\n\n- 案例 $ 2 $（欠定，有共享碎片通道）：\n  $ \\mathbf{F} = \\begin{bmatrix} 1.0  0.0  1.0 \\\\ 0.0  1.0  1.0 \\end{bmatrix} $，\n  $ \\mathbf{y} = [\\,1.0,\\,1.0\\,]^\\top $，\n  $ \\lambda = 0.1 $。\n\n- 案例 $ 3 $（共线列；稀疏性下的模糊性）：\n  $ \\mathbf{F} = \\begin{bmatrix} 1.0  1.0  0.0 \\\\ 1.0  1.0  0.0 \\\\ 0.0  0.0  1.0 \\end{bmatrix} $，\n  $ \\mathbf{y} = [\\,1.0,\\,1.0,\\,0.0\\,]^\\top $，\n  $ \\lambda = 0.05 $。\n\n- 案例 $ 4 $（强正则化边界；零解）：\n  $ \\mathbf{F} = \\mathbf{I}_2 $，\n  $ \\mathbf{y} = [\\,0.05,\\,0.02\\,]^\\top $，\n  $ \\lambda = 0.1 $。\n\n- 案例 $ 5 $（含零列的退化算子）：\n  $ \\mathbf{F} = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  1.0  0.0 \\end{bmatrix} $，\n  $ \\mathbf{y} = [\\,0.5,\\,0.7\\,]^\\top $，\n  $ \\lambda = 0.1 $。\n\n计算要求。实现一个适用于此类凸问题收敛的算法，例如，采用坐标下降方案，每个坐标具有精确的一维最小化子并强制非负性，或等效的近端梯度方法。程序不得依赖任何外部输入，并且必须按规定计算五个案例的估计量。\n\n答案格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。此外层列表的每个元素按顺序对应一个测试案例，并且本身必须是表示该案例的 $ \\widehat{\\mathbf{s}} $ 的列表。每个数值条目必须四舍五入至恰好 $ 6 $ 位小数。例如，具有三个案例和二维解的输出可能看起来像 $[\\,[0.123456,0.000000],[\\dots],[\\dots]\\,]$。",
            "solution": "经评估，用户提供的问题是有效的。它在科学上基于分析化学信号处理的既定模型，在数学上被恰当地表述为一个凸优化问题，并为其解决提供了一套完整且一致的定义和数据。\n\n在此，我们推导源贡献向量 $ \\mathbf{s} $ 的最大后验 (MAP) 估计量，证明其对应于非负最小绝对收缩和选择算子 (LASSO) 类型的凸优化问题，并概述用于其数值解的坐标下降算法。\n\n### 1. 贝叶斯公式与最大后验估计\n\n目标是找到在给定观测数据 $ \\mathbf{y} $ 的情况下最可能的源向量 $ \\mathbf{s} $。使用贝叶斯定理，$ \\mathbf{s} $ 的后验概率与数据给定源的似然和源的先验概率的乘积成正比：\n$$\np(\\mathbf{s} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\mathbf{s}) p(\\mathbf{s})\n$$\nMAP 估计量 $ \\widehat{\\mathbf{s}}_{\\text{MAP}} $ 是最大化此后验概率的 $ \\mathbf{s} $ 值，受限于非负约束 $ \\mathbf{s} \\in \\mathbb{R}_{\\ge 0}^n $。这等价于最小化后验概率的负对数：\n$$\n\\widehat{\\mathbf{s}}_{\\text{MAP}} = \\arg\\max_{\\mathbf{s} \\ge 0} p(\\mathbf{s} | \\mathbf{y}) = \\arg\\min_{\\mathbf{s} \\ge 0} [-\\log p(\\mathbf{y} | \\mathbf{s}) - \\log p(\\mathbf{s})]\n$$\n\n### 2. 似然和先验定义\n\n**似然函数**：线性模型为 $ \\mathbf{y} = \\mathbf{F}\\mathbf{s} + \\boldsymbol{\\epsilon} $，其中噪声是独立同分布的高斯噪声，$ \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_m) $。因此，对于给定的 $ \\mathbf{s} $，观测到 $ \\mathbf{y} $ 的似然由一个多元高斯分布描述：\n$$\np(\\mathbf{y} | \\mathbf{s}) = \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2\\right)\n$$\n负对数似然则为：\n$$\n-\\log p(\\mathbf{y} | \\mathbf{s}) = \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + \\frac{m}{2} \\log(2\\pi\\sigma^2)\n$$\n\n**先验分布**：对于 $ s_i \\ge 0 $，源 $ s_i $ 被假定遵循独立的指数先验，给定为 $ p(s_i) \\propto \\exp(-\\alpha s_i) $，其中 $ \\alpha > 0 $。向量 $ \\mathbf{s} $ 的联合先验是各个先验的乘积：\n$$\np(\\mathbf{s}) = \\prod_{i=1}^n p(s_i) \\propto \\prod_{i=1}^n \\exp(-\\alpha s_i) = \\exp\\left(-\\alpha \\sum_{i=1}^n s_i\\right)\n$$\n由于每个 $ s_i \\ge 0 $，我们有 $ s_i = |s_i| $，因此 $ \\sum_{i=1}^n s_i = \\|\\mathbf{s}\\|_1 $。先验可以写成 $ p(\\mathbf{s}) \\propto \\exp(-\\alpha \\|\\mathbf{s}\\|_1) $。负对数先验是：\n$$\n-\\log p(\\mathbf{s}) = \\alpha \\|\\mathbf{s}\\|_1 + C\n$$\n其中 $ C $ 是一个归一化常数。\n\n### 3. 非负 LASSO 公式\n\n结合负对数似然和负对数先验，并去掉不依赖于 $ \\mathbf{s} $ 的常数项，MAP 优化问题变为：\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{s} \\ge 0} \\left\\{ \\frac{1}{2\\sigma^2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + \\alpha \\|\\mathbf{s}\\|_1 \\right\\}\n$$\n目标函数可以乘以一个正常数 $ 2\\sigma^2 $ 而不改变最小值的位置。这得到：\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{s} \\ge 0} \\left\\{ \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + 2\\alpha\\sigma^2 \\|\\mathbf{s}\\|_1 \\right\\}\n$$\n通过定义正则化超参数 $ \\lambda = 2\\alpha\\sigma^2 $，其中 $ \\lambda > 0 $，我们得到最终的目标函数。为了与机器学习文献中使用的常见形式匹配，我们给数据拟合项添加一个因子 $ 1/2 $，这等效于将整个目标函数缩放 $ 1/2 $：\n$$\n\\mathcal{L}(\\mathbf{s}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 + \\lambda \\|\\mathbf{s}\\|_1\n$$\n估计量是以下最小化问题的解：\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{s} \\ge 0} \\mathcal{L}(\\mathbf{s})\n$$\n这就是非负 LASSO 的公式。项 $ \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 $ 衡量数据保真度（最小二乘误差），而项 $ \\lambda \\|\\mathbf{s}\\|_1 $ 是一个鼓励稀疏性（$ \\mathbf{s} $ 的许多分量为零）的惩罚项。超参数 $ \\lambda $ 控制着这两个目标之间的权衡。\n\n### 4. 问题的凸性\n\n该优化问题是凸的。这可以通过检查目标函数的组成部分和约束集来确定：\n1.  数据保真度项 $ f(\\mathbf{s}) = \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{F}\\mathbf{s}\\|_2^2 $ 是 $ \\mathbf{s} $ 的二次函数。其海森矩阵是 $ \\mathbf{F}^\\top\\mathbf{F} $，它总是半正定的。因此，$ f(\\mathbf{s}) $ 是一个凸函数。\n2.  惩罚项 $ g(\\mathbf{s}) = \\lambda \\|\\mathbf{s}\\|_1 $ 是一个缩放的 L1 范数，它是一个已知的凸函数。\n3.  目标函数 $ \\mathcal{L}(\\mathbf{s}) = f(\\mathbf{s}) + g(\\mathbf{s}) $ 是两个凸函数的和，其本身也是凸的。\n4.  约束集 $ \\mathbf{s} \\ge 0 $ 定义了非负象限，这是一个凸集。\n\n在凸集上最小化一个凸函数是一个凸优化问题。一个关键性质是任何局部最小值也是全局最小值，这保证了旨在寻找局部最小值的算法将找到最优解。\n\n### 5. 通过坐标下降的算法求解\n\n坐标下降是解决此问题的有效算法。它一次只针对一个坐标 $ s_k $ 来迭代最小化目标函数，同时保持所有其他坐标固定。\n\n让我们推导第 $k$ 个坐标 $ s_k $ 的更新规则。目标函数，仅视为 $ s_k $ 的函数，可以写成：\n$$\n\\mathcal{L}(s_k) = \\frac{1}{2} \\left\\| \\left(\\mathbf{y} - \\sum_{j \\neq k} s_j \\mathbf{f}_j\\right) - s_k \\mathbf{f}_k \\right\\|_2^2 + \\lambda s_k + \\lambda \\sum_{j \\neq k} s_j\n$$\n其中 $ \\mathbf{f}_j $ 是 $ \\mathbf{F} $ 的第 $j$ 列。设 $ \\mathbf{r}_k = \\mathbf{y} - \\sum_{j \\neq k} s_j \\mathbf{f}_j $ 为排除第 $k$ 个源时的残差。不涉及 $ s_k $ 的项是常数。我们最小化：\n$$\n\\mathcal{L}(s_k) = \\frac{1}{2} (s_k^2 \\|\\mathbf{f}_k\\|_2^2 - 2s_k \\mathbf{f}_k^\\top \\mathbf{r}_k) + \\lambda s_k + \\text{const}\n$$\n关于 $ s_k $ 的导数（对于 $ s_k > 0 $）是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_k} = s_k \\|\\mathbf{f}_k\\|_2^2 - \\mathbf{f}_k^\\top \\mathbf{r}_k + \\lambda\n$$\n令导数为零，得到无约束最小化子 $ s_k^* = \\frac{\\mathbf{f}_k^\\top \\mathbf{r}_k - \\lambda}{\\|\\mathbf{f}_k\\|_2^2} $。结合非负约束 $ s_k \\ge 0 $，一维子问题的解是：\n$$\ns_k^{\\text{new}} = \\max\\left(0, \\frac{\\mathbf{f}_k^\\top \\mathbf{r}_k - \\lambda}{\\|\\mathbf{f}_k\\|_2^2}\\right)\n$$\n为了高效计算，我们预先计算 $ \\mathbf{C} = \\mathbf{F}^\\top\\mathbf{F} $ 和 $ \\mathbf{c} = \\mathbf{F}^\\top\\mathbf{y} $。项 $ \\mathbf{f}_k^\\top \\mathbf{r}_k $ 可以写成：\n$$\n\\mathbf{f}_k^\\top \\mathbf{r}_k = \\mathbf{f}_k^\\top \\left(\\mathbf{y} - \\sum_{j \\neq k} s_j \\mathbf{f}_j\\right) = (\\mathbf{F}^\\top\\mathbf{y})_k - \\sum_{j \\neq k} (\\mathbf{F}^\\top\\mathbf{F})_{kj} s_j = c_k - \\sum_{j \\neq k} C_{kj} s_j\n$$\n第 $k$ 个坐标的更新规则变为：\n$$\ns_k \\leftarrow \\max\\left(0, \\frac{c_k - \\sum_{j \\neq k} C_{kj} s_j - \\lambda}{C_{kk}}\\right)\n$$\n如果 $ C_{kk} = \\|\\mathbf{f}_k\\|_2^2 = 0 $，则 $ \\mathbf{F} $ 的第 $k$ 列为零。在这种情况下，$ s_k $ 只出现在惩罚项 $ \\lambda s_k $ 中。为了对 $ s_k \\ge 0 $ 最小化此项，我们必须设置 $ s_k = 0 $。该算法重复遍历所有坐标，直到解向量 $ \\mathbf{s} $ 收敛。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef coordinate_descent_solver(F, y, lam, max_iter=100000, tol=1e-9):\n    \"\"\"\n    Solves the non-negative LASSO problem using coordinate descent.\n\n    The problem is to minimize:\n    0.5 * ||y - Fs||^2_2 + lambda * ||s||_1\n    subject to s >= 0.\n\n    Args:\n        F (np.ndarray): The operator matrix (m x n).\n        y (np.ndarray): The observation vector (m,).\n        lam (float): The regularization hyperparameter.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        np.ndarray: The estimated source vector s (n,).\n    \"\"\"\n    m, n = F.shape\n    \n    # Pre-compute matrices for efficiency\n    C = F.T @ F\n    c = F.T @ y\n    \n    # Initialize the source vector\n    s = np.zeros(n)\n    \n    # Pre-compute diagonal of C\n    C_diag = np.diag(C)\n\n    for i in range(max_iter):\n        s_old = s.copy()\n        \n        for k in range(n):\n            # The term sum_{j != k} C_kj * s_j can be calculated as\n            # (C[k,:] @ s) - C[k, k] * s[k]\n            off_diagonal_sum = C[k, :] @ s - C[k, k] * s[k]\n            \n            numerator = c[k] - off_diagonal_sum - lam\n            \n            if C_diag[k] > 1e-12: # Check for non-zero column in F\n                s[k] = np.maximum(0, numerator / C_diag[k])\n            else:\n                # If the k-th column of F is all zeros, C_diag[k] is 0.\n                # The objective is minimized with respect to s_k when s_k = 0\n                # due to the penalty term lambda * s_k.\n                s[k] = 0.0\n\n        # Check for convergence\n        if np.linalg.norm(s - s_old, ord=np.inf)  tol:\n            break\n            \n    return s\n\ndef solve():\n    \"\"\"\n    Defines and solves the test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: identity operator; simple shrinkage\n        {\n            \"F\": np.eye(3),\n            \"y\": np.array([1.0, 0.1, 3.0]),\n            \"lambda\": 0.2\n        },\n        # Case 2: underdetermined with a shared fragment channel\n        {\n            \"F\": np.array([[1.0, 0.0, 1.0], [0.0, 1.0, 1.0]]),\n            \"y\": np.array([1.0, 1.0]),\n            \"lambda\": 0.1\n        },\n        # Case 3: colinear columns; ambiguity under sparsity\n        {\n            \"F\": np.array([[1.0, 1.0, 0.0], [1.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"y\": np.array([1.0, 1.0, 0.0]),\n            \"lambda\": 0.05\n        },\n        # Case 4: boundary with strong regularization; zero solution\n        {\n            \"F\": np.eye(2),\n            \"y\": np.array([0.05, 0.02]),\n            \"lambda\": 0.1\n        },\n        # Case 5: degenerate operator with a zero column\n        {\n            \"F\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"y\": np.array([0.5, 0.7]),\n            \"lambda\": 0.1\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        s_hat = coordinate_descent_solver(case[\"F\"], case[\"y\"], case[\"lambda\"])\n        all_results.append(s_hat)\n\n    # Format the final output string exactly as required.\n    result_strings = []\n    for s_vector in all_results:\n        # Format each number to 6 decimal places\n        formatted_nums = [f\"{num:.6f}\" for num in s_vector]\n        # Create the list string, e.g., \"[0.123456,0.000000]\"\n        result_strings.append(f\"[{','.join(formatted_nums)}]\")\n\n    # Join all case results into the final output format, e.g., \"[[...],[...]]\"\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}