{
    "hands_on_practices": [
        {
            "introduction": "Hidden Markov Models (HMMs) are a cornerstone of computational gene annotation, providing a probabilistic framework to identify genomic features. While the Viterbi algorithm offers an exact method for decoding the most likely sequence of hidden states (e.g., exons, introns), its quadratic complexity in the number of states, $O(T S^2)$, can be prohibitive for complex models. This practice  challenges you to explore a common practical solution: beam search, a heuristic that trades guaranteed optimality for significant speed gains. You will derive the complexity of both algorithms from first principles and, more importantly, quantify the relationship between the beam width parameter $b$ and the probability of maintaining the correct path, providing a formal basis for making principled speed-accuracy trade-offs.",
            "id": "3288378",
            "problem": "Consider a Hidden Markov Model (HMM) for gene annotation with $S$ hidden states and a genomic observation sequence of length $T$. An HMM is defined by an initial distribution over states, a transition probability matrix, and emission probability distributions. The Viterbi decoding computes the most probable hidden state sequence given the observations by dynamic programming over all state transitions. By the foundational definition of big-$O$ notation, algorithmic cost is measured in terms of the number of elementary arithmetic or comparison operations as functions of input sizes. In a fully connected HMM, at each time step $t \\in \\{1,\\dots,T\\}$ the Viterbi recurrence for each destination state considers all $S$ possible predecessor states, leading to an asymptotic cost proportional to $O(T S^2)$. Beam search is a heuristic that prunes the state space at each time step by retaining only the top $b$ states ranked by partial path scores; transitions are evaluated only from this beam to all $S$ destination states, resulting in a cost proportional to $O(T b S)$, which reduces to $O(T S)$ when $b$ is bounded independently of $S$.\n\nTo preserve decoding accuracy, the pruned beam must contain the predecessor state on the optimal path at each step. Formalize accuracy using a probabilistic rank model grounded in order-statistics: let $R_t$ be the random rank (among $S$ states, with rank $1$ being best) of the predecessor state that leads to the globally optimal Viterbi path at time $t$. Assume the ranks $R_t$ are independent and identically distributed (i.i.d.) across $t$ and satisfy a geometric tail bound parameterized by $p \\in (0,1)$, namely $\\mathbb{P}(R_t > r) = (1-p)^r$ for integers $r \\ge 0$, truncated at $S$. This tail bound is consistent with widely used score gap models in HMM decoding where the probability that a given competitor outperforms the optimal predecessor decays exponentially with rank. Under this model, the probability that the optimal predecessor is within the top $b$ at a single step is $\\mathbb{P}(R_t \\le b) = 1 - (1-p)^b$. Assuming independence across $T$ steps, the probability that the optimal path survives pruning over all $T$ steps is $\\left(1 - (1-p)^b\\right)^T$.\n\nYour tasks are:\n- Derive from first principles and definitions the asymptotic costs $O(T S^2)$ for the Viterbi algorithm and $O(T b S)$ for beam search, starting from the dynamic programming recurrence and the definition of big-$O$ notation.\n- Using the probabilistic rank model above, derive the minimal integer beam width $b(T,S,p,\\varepsilon)$ that guarantees $\\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon$ for a specified accuracy tolerance $\\varepsilon \\in (0,1)$ (expressed as a decimal, not a percentage), subject to the constraint $b \\le S$. Explicitly state how to choose $b$ when the unconstrained minimal integer exceeds $S$.\n- Implement a program that, for the following test suite of parameter values $(T,S,p,\\varepsilon)$, computes $b$ for each case and returns for each case a pair consisting of the chosen $b$ and a boolean indicating whether both accuracy is guaranteed (i.e., $\\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon$) and asymptotic cost is reduced relative to $O(T S^2)$ (i.e., $b  S$ so that $O(T b S) = o(T S^2)$):\n    1. $T = 1000$, $S = 50$, $p = 0.3$, $\\varepsilon = 0.01$.\n    2. $T = 1$, $S = 1000$, $p = 0.3$, $\\varepsilon = 0.01$.\n    3. $T = 10000$, $S = 10000$, $p = 0.1$, $\\varepsilon = 0.1$.\n    4. $T = 500$, $S = 200$, $p = 0.25$, $\\varepsilon = 0.000001$.\n    5. $T = 1000$, $S = 500$, $p = 0.01$, $\\varepsilon = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list of the form $[b,\\text{boolean}]$. For example, a valid output format is $[[b_1,\\text{boolean}_1],[b_2,\\text{boolean}_2],\\dots]$. No physical units apply to this problem. Angles are not used. All probabilities such as $\\varepsilon$ must be provided and interpreted as decimals.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in established principles of algorithm analysis and probabilistic modeling, is well-posed, objective, and contains all necessary information for a unique solution.\n\n### Part 1: Derivation of Asymptotic Costs\n\n#### Viterbi Algorithm Complexity: $O(T S^2)$\n\nThe Viterbi algorithm finds the most probable sequence of hidden states given a sequence of observations. It is a dynamic programming algorithm. Let $S$ be the number of hidden states and $T$ be the length of the observation sequence. Let $\\delta_t(j)$ be the probability of the most likely path of length $t$ ending in state $j$. The core of the algorithm is the recurrence relation:\n$$ \\delta_t(j) = \\left( \\max_{i=1,\\dots,S} \\left[ \\delta_{t-1}(i) \\cdot a_{ij} \\right] \\right) \\cdot e_j(o_t) \\quad \\text{for } j=1,\\dots,S \\text{ and } t=1,\\dots,T $$\nwhere $a_{ij}$ is the transition probability from state $i$ to state $j$, and $e_j(o_t)$ is the emission probability of observation $o_t$ from state $j$.\n\nTo analyze the computational cost, we count the number of elementary arithmetic operations. For numerical stability, computations are typically performed in log-space, where multiplications become additions. The recurrence becomes:\n$$ \\log \\delta_t(j) = \\left( \\max_{i=1,\\dots,S} \\left[ \\log \\delta_{t-1}(i) + \\log a_{ij} \\right] \\right) + \\log e_j(o_t) $$\n\nLet's analyze the cost for a single time step $t$:\n1.  To compute the value for a single destination state $j$, we must evaluate the expression inside the $\\max$ operator for each possible predecessor state $i \\in \\{1, \\dots, S\\}$.\n2.  For each $i$, this involves one addition: $\\log \\delta_{t-1}(i) + \\log a_{ij}$. This is performed $S$ times for a fixed $j$.\n3.  Then, we must find the maximum of these $S$ values. This requires $S-1$ comparisons.\n4.  Finally, one more addition is performed to add the log-emission probability.\n5.  Thus, for each destination state $j$, the number of operations is proportional to $S$ (for the additions) plus $S-1$ (for the comparisons), which is $O(S)$.\n\nSince this computation must be performed for all $S$ destination states ($j=1, \\dots, S$), the total cost for a single time step $t$ is $S \\times O(S) = O(S^2)$.\n\nThis process is repeated for each time step from $t=1$ to $t=T$. By the definition of big-O notation, which describes the limiting behavior as the input sizes ($T, S$) grow, the total asymptotic cost is the cost per time step multiplied by the number of time steps.\n$$ \\text{Total Cost} = T \\times O(S^2) = O(T S^2) $$\n\n#### Beam Search Algorithm Complexity: $O(T b S)$\n\nBeam search is a heuristic that reduces complexity by pruning the search space. At each time step, it only keeps a \"beam\" of the $b$ most promising states, where $b$ is the beam width.\n\nLet $\\mathcal{B}_{t-1}$ be the set of $b$ states with the highest scores $\\delta_{t-1}(i)$ at time $t-1$. The recurrence relation is modified to consider transitions only from states within this beam:\n$$ \\log \\delta_t(j) = \\left( \\max_{i \\in \\mathcal{B}_{t-1}} \\left[ \\log \\delta_{t-1}(i) + \\log a_{ij} \\right] \\right) + \\log e_j(o_t) $$\n\nLet's analyze the cost for a single time step $t$:\n1.  To compute the score for a single destination state $j$, we now iterate only over the $b$ states in the beam $\\mathcal{B}_{t-1}$.\n2.  This involves $b$ additions and finding the maximum of $b$ values (which takes $b-1$ comparisons). The cost per destination state $j$ is therefore $O(b)$.\n3.  This calculation is performed for all $S$ possible destination states, so the cost of computing all candidate scores for time $t$ is $S \\times O(b) = O(bS)$.\n4.  After computing the $S$ new scores, we must identify the top $b$ states to form the next beam, $\\mathcal{B}_t$. This can be done efficiently in $O(S)$ time using a linear-time selection algorithm (e.g., Quickselect) to find the $b$-th largest score and then filtering.\n5.  The total cost for one time step is the sum of these two steps: $O(bS) + O(S)$. Since $b \\ge 1$, this simplifies to $O(bS)$.\n\nThis process is repeated for all $T$ time steps. The total asymptotic cost is:\n$$ \\text{Total Cost} = T \\times O(bS) = O(T b S) $$\nWhen $b  S$, $O(T b S)$ is an asymptotic improvement over $O(T S^2)$. Formally, if $b  S$, then $T b S = o(T S^2)$.\n\n### Part 2: Derivation of Minimal Beam Width $b$\n\nWe are given the requirement that the probability of the optimal path surviving pruning over all $T$ steps must be at least $1-\\varepsilon$:\n$$ \\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon $$\nwhere $p \\in (0,1)$ is the parameter of the rank model, $b$ is the beam width, and $\\varepsilon \\in (0,1)$ is the accuracy tolerance. We need to find the minimal integer $b$ that satisfies this inequality, subject to $b \\le S$.\n\n1.  Take the $T$-th root of both sides. Since both sides are positive, the inequality direction is preserved.\n    $$ 1 - (1-p)^b \\ge (1 - \\varepsilon)^{1/T} $$\n2.  Rearrange the terms to isolate the term with $b$:\n    $$ (1-p)^b \\le 1 - (1 - \\varepsilon)^{1/T} $$\n3.  Take the logarithm of both sides. Since $p \\in (0,1)$, we have $0  1-p  1$, which means $\\log(1-p)$ is negative. Therefore, when we apply the logarithm and later divide by $\\log(1-p)$, we must reverse the inequality sign.\n    $$ b \\log(1-p) \\le \\log\\left(1 - (1 - \\varepsilon)^{1/T}\\right) $$\n4.  Divide by $\\log(1-p)$ and reverse the inequality:\n    $$ b \\ge \\frac{\\log\\left(1 - (1 - \\varepsilon)^{1/T}\\right)}{\\log(1-p)} $$\n    The base of the logarithm can be any value greater than $1$; the natural logarithm is standard.\n\nSince $b$ must be an integer, the minimal unconstrained integer beam width, let's call it $b_{req}$, is the smallest integer satisfying this condition, which is given by the ceiling function:\n$$ b_{req} = \\left\\lceil \\frac{\\ln\\left(1 - (1 - \\varepsilon)^{1/T}\\right)}{\\ln(1-p)} \\right\\rceil $$\n\nThe problem imposes the physical constraint that the beam width cannot exceed the total number of states, i.e., $b \\le S$. Therefore, the chosen beam width, $b_{chosen}$, must be:\n$$ b_{chosen} = \\min(S, b_{req}) $$\n\nIf the calculated required beam width $b_{req}$ exceeds the number of states $S$, we are forced to choose $b_{chosen} = S$. In this scenario, the chosen beam width $b_{chosen}$ is smaller than the required width $b_{req}$, so the accuracy guarantee $\\left(1 - (1-p)^{b_{chosen}}\\right)^T \\ge 1 - \\varepsilon$ will not be met. Furthermore, since $b_{chosen} = S$, there is no asymptotic cost reduction compared to the full Viterbi algorithm ($O(TBS) = O(TS^2)$).\n\n### Part 3: Implementation Logic\n\nFor each test case $(T, S, p, \\varepsilon)$:\n1.  Calculate the required beam width $b_{req}$ using the derived formula. For numerical stability with small $\\varepsilon$, the expression $(1-\\varepsilon)^{1/T}$ can be computed as $\\exp(\\frac{1}{T}\\ln(1-\\varepsilon))$. The terms $1 - \\exp(\\dots)$ and $\\ln(1-p)$ are best computed using `expm1` and `log1p` functions respectively to maintain precision for arguments close to $0$.\n    $$ b_{req} = \\left\\lceil \\frac{\\ln(-\\text{expm1}(\\frac{1}{T}\\ln(1-\\varepsilon)))}{\\ln(1-p)} \\right\\rceil = \\left\\lceil \\frac{\\ln(-\\text{expm1}(\\frac{\\text{log1p}(-\\varepsilon)}{T}))}{\\text{log1p}(-p)} \\right\\rceil$$\n2.  Determine the chosen beam width $b_{chosen} = \\min(S, b_{req})$.\n3.  Evaluate the two conditions for the boolean flag:\n    a.  **Accuracy Guarantee**: The accuracy is guaranteed if and only if the chosen beam width is sufficient, i.e., $b_{chosen} \\ge b_{req}$. This is equivalent to checking if $S \\ge b_{req}$.\n    b.  **Asymptotic Cost Reduction**: The cost is reduced if $b_{chosen}  S$.\n4.  The final boolean value is `True` if both conditions are met, and `False` otherwise. This simplifies to the single condition $b_{req}  S$. If $b_{req}  S$, we choose $b_{chosen} = b_{req}$, satisfying both $b_{chosen}  S$ and $b_{chosen} \\ge b_{req}$. If $b_{req} \\ge S$, it is impossible to satisfy both conditions simultaneously.\n5.  Return the pair $[b_{chosen}, \\text{boolean}]$.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the beam search parameter problem for a suite of test cases.\n    For each case, it computes the minimal beam width 'b' that guarantees\n    a certain accuracy and determines if this choice also reduces\n    asymptotic complexity relative to the Viterbi algorithm.\n    \"\"\"\n\n    # Test suite of parameters (T, S, p, epsilon)\n    test_cases = [\n        (1000, 50, 0.3, 0.01),\n        (1, 1000, 0.3, 0.01),\n        (10000, 10000, 0.1, 0.1),\n        (500, 200, 0.25, 0.000001),\n        (1000, 500, 0.01, 0.01),\n    ]\n\n    results = []\n    for T, S, p, epsilon in test_cases:\n        # Using numerically stable functions for calculations involving\n        # numbers close to 0 or 1.\n        # log(1-x) = log1p(-x)\n        # exp(x)-1 = expm1(x)\n        #\n        # Derivation:\n        # (1 - (1-p)^b)^T = 1 - epsilon\n        # 1 - (1-p)^b = (1 - epsilon)^(1/T)\n        # (1-p)^b = 1 - (1 - epsilon)^(1/T)\n        # b * log(1-p) = log(1 - (1-epsilon)^(1/T))\n        # b = log(1 - (1-epsilon)^(1/T)) / log(1-p)  (log(1-p) is negative)\n        \n        # (1-epsilon)^(1/T) = exp(log((1-epsilon)^(1/T))) = exp( (1/T) * log(1-epsilon) )\n        # Using log1p for log(1-epsilon) and expm1 for exp(x)-1:\n        # log(1 - exp( (1/T) * log1p(-epsilon) ))\n        # let term_in_log = -expm1((1/T) * log1p(-epsilon))\n        try:\n            log_numerator_arg = -math.expm1(math.log1p(-epsilon) / T)\n            \n            # Handle potential domain error if log_numerator_arg is not positive.\n            # This can happen if T is extremely large and epsilon is close to 1,\n            # but is unlikely with the given test cases.\n            if log_numerator_arg = 0:\n                # This case implies an effectively infinite required beam width,\n                # which is unrealistic but we handle it.\n                b_required_float = float('inf')\n            else:\n                numerator = math.log(log_numerator_arg)\n                denominator = math.log1p(-p)\n                b_required_float = numerator / denominator\n\n        except (ValueError, OverflowError):\n            b_required_float = float('inf')\n\n        # The required beam width must be an integer, so we take the ceiling.\n        if math.isinf(b_required_float) or math.isnan(b_required_float):\n            b_required = S + 1 # Effectively infinite, force to be  S\n        else:\n            b_required = int(math.ceil(b_required_float))\n\n        # The chosen beam width is constrained by the number of states S.\n        b_chosen = min(S, b_required)\n\n        # The accuracy is guaranteed if the chosen b is at least the required b.\n        # This is equivalent to S being large enough (S = b_required).\n        is_accuracy_guaranteed = (b_chosen = b_required)\n\n        # The asymptotic cost is reduced if b  S.\n        is_cost_reduced = (b_chosen  S)\n\n        # The final boolean flag is True only if BOTH conditions are met.\n        success_flag = is_accuracy_guaranteed and is_cost_reduced\n\n        # Append the pair [chosen_b, boolean_flag] to results.\n        results.append([b_chosen, success_flag])\n    \n    # Format the output as a string representing a list of lists.\n    # str() on a Python list [val, bool] produces '[val, True]' or '[val, False]',\n    # which matches the required output format style.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The revolutionary power of CRISPR-Cas9 gene editing hinges on its precision, making the prediction of off-target effects a critical computational challenge. Exhaustively searching a genome for potential off-targets, especially allowing for mismatches, is computationally demanding. A widely used and efficient strategy is 'seed-and-extend', where exact matches to short seed sequences trigger a more detailed analysis. This exercise  delves into the theoretical performance of this approach, asking you to analyze an off-target scanning pipeline using the Aho–Corasick automaton. By deriving the expected number of seed hits under a probabilistic genome model, you will gain insight into how seed design directly influences the algorithm's overall runtime and efficiency.",
            "id": "3288372",
            "problem": "A research team is building an exact multi-pattern index using the Aho–Corasick automaton to accelerate off-target scanning for a collection of clustered regularly interspaced short palindromic repeats (CRISPR) guide ribonucleic acid (RNA) sequences of fixed length. The genome is modeled as an independent and identically distributed sequence over the deoxyribonucleic acid (DNA) alphabet of size $a$, with $a=4$ for DNA. They wish to report all genomic loci that could be off-targets for any of $r$ guide sequences of length $L$ under a Hamming mismatch budget of at most $d$ across the $L$-nucleotide guide.\n\nTo avoid false negatives while using exact matching in the Aho–Corasick automaton, they adopt a seed-and-extend strategy: for each guide, they choose a collection of disjoint, contiguous seeds whose exact matches in the genome trigger verification of the full guide with the mismatch budget. They restrict attention to the following class of seed designs:\n- exactly $d+1$ disjoint, contiguous seeds per guide,\n- equal seed lengths,\n- the seeds partition the $L$ positions of the guide.\n\nAssume $L$ is divisible by $d+1$ so that the common seed length is $s=L/(d+1)$. The team will construct an Aho–Corasick automaton on the multiset of all seeds from all $r$ guides and scan a genome of length $n$. Let $z$ denote the total number of seed occurrences output by the automaton during the scan; the automaton ignores verification costs and outputs one occurrence per exact seed match.\n\nStarting from first principles and standard definitions (finite automata processing with amortized $O(1)$ per character and the pigeonhole principle for mismatch distribution), derive the expected value of $z$ under the independent and identically distributed genome model, and use it to express the expected asymptotically dominant term of the scanning time as a function of $n$, $r$, $L$, $d$, and $a$, ignoring preprocessing and verification costs but counting every output as constant time. Your answer must be a single closed-form expression in these symbols that captures the leading-order expected running time. Do not provide an inequality, do not provide a big-$O$ or big-$\\Theta$ bound, and do not include any units in the final answer.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and contains all necessary information. The derivation proceeds as follows.\n\n1.  **Model for Expected Scanning Time**: The Aho–Corasick automaton processes a text of length $n$ and reports $z$ total occurrences in time proportional to $n+z$. The expected scanning time $E[T]$ is therefore proportional to $E[n+z]$. Since $n$ is a fixed parameter for the random genome model, by linearity of expectation, $E[n+z] = n + E[z]$. The question asks for the closed-form expression of the asymptotically dominant part of this expected time.\n\n2.  **Derivation of Expected Matches $E[z]$**:\n    *   **Seed Parameters**: The strategy uses $r$ guides. For each guide, there are $d+1$ disjoint seeds of equal length. The total number of seed patterns input to the automaton is $N_{seeds} = r(d+1)$. The length of each seed is $s = L/(d+1)$.\n    *   **Probability of a Match**: The genome is modeled as an i.i.d. sequence over an alphabet of size $a$. The probability that a specific seed of length $s$ matches at a specific position in the genome is $p_{match} = a^{-s}$.\n    *   **Linearity of Expectation**: Let $I_{i,j}$ be an indicator variable that is 1 if seed $i$ matches at genomic position $j$, and 0 otherwise. The total number of matches is $z = \\sum_{i=1}^{N_{seeds}} \\sum_{j=1}^{n-s+1} I_{i,j}$. By linearity of expectation, $E[z] = \\sum_{i=1}^{N_{seeds}} \\sum_{j=1}^{n-s+1} E[I_{i,j}]$.\n    *   **Calculating $E[z]$**: Since $E[I_{i,j}] = p_{match} = a^{-s}$ for all seeds and positions, we have:\n        $$ E[z] = N_{seeds} \\cdot (n-s+1) \\cdot a^{-s} = r(d+1)(n-s+1)a^{-s} $$\n\n3.  **Asymptotically Dominant Term**: We substitute $E[z]$ back into the expression for the expected time. We are interested in the part that grows linearly with the genome length $n$.\n    $$ E[T] \\propto n + E[z] = n + r(d+1)(n-s+1)a^{-s} $$\n    Expanding the terms:\n    $$ E[T] \\propto n + n \\cdot r(d+1)a^{-s} - (s-1)r(d+1)a^{-s} $$\n    The terms that scale with $n$ constitute the asymptotically dominant part of the expected time. Factoring out $n$ from these terms gives:\n    $$ \\text{Dominant Part} \\propto n \\left(1 + r(d+1)a^{-s}\\right) $$\n\n4.  **Final Expression**: Substituting back $s=L/(d+1)$ and assuming a proportionality constant of 1 as implied by the request for a specific formula, we get the final expression:\n    $$ n \\left(1 + r (d+1) a^{-\\frac{L}{d+1}}\\right) $$\nThis expression represents the dominant component of the expected number of operations for the scanning phase.",
            "answer": "$$\n\\boxed{n \\left(1 + r (d+1) a^{-\\frac{L}{d+1}}\\right)}\n$$"
        },
        {
            "introduction": "Many fundamental problems in systems biology, such as probabilistic inference on signaling or metabolic networks, are computationally intractable in the worst case, often belonging to the NP-hard complexity class. However, real-world biological networks are not random; they possess distinct structural properties, such as sparsity and modularity, which we can exploit algorithmically. This practice  introduces the powerful concept of parameterized complexity through the lens of treewidth, a measure of a graph's 'tree-likeness'. You will derive the running time of an exact inference algorithm that is exponential only in the treewidth, not the overall network size, and then implement a heuristic to estimate this critical parameter for various network motifs, revealing how structure dictates computational feasibility.",
            "id": "3288367",
            "problem": "You are given undirected interaction graphs that abstract sparse biochemical reaction or signaling pathways. Consider exact probabilistic inference on these networks modeled as a factor graph where each molecular species state is a random variable with finite domain size $k$, and interactions induce factors over the corresponding variable scopes. Exact inference by variable elimination processes variables one by one, replacing factors that include an eliminated variable by their marginalization. The computational cost at each elimination step is dominated by multiplying all factors that include the eliminated variable and then summing out that variable.\n\nFundamental base and definitions:\n- A factor graph induces an undirected graph $G = (V,E)$ on variables $V$, where an edge in $E$ connects two variables that appear together in some factor scope.\n- For an elimination ordering $\\pi$ on $V$, the induced graph is formed by, at each elimination of a vertex $v$, adding fill-in edges to make the current neighborhood of $v$ a clique before removing $v$. The induced width of $\\pi$ is the maximum over all eliminated vertices of the current degree of the vertex at its elimination. The treewidth $t$ of $G$ is the minimum induced width over all orderings.\n- If each variable has domain size $k$, then when eliminating a variable whose current neighborhood forms a clique of size $w$, the dominant arithmetic cost of that step is proportional to $k^{w+1}$.\n\nTask overview:\n1. From first principles, derive an asymptotic running-time bound for exact inference by variable elimination on a factor graph with $n$ variables and treewidth $t$, where the bound is of the form $O(f(t)\\,n)$ and the function $f$ is exponential in $t$. Do not assume any specialized formulas; derive the expression using only the above definitions and the fact that the scope size at each elimination step is bounded by the induced clique size.\n2. Implement a program that:\n   - Computes an upper bound $\\hat{t}$ on the treewidth $t$ using the min-fill heuristic on an undirected graph $G$. The heuristic repeatedly eliminates a vertex whose neighbors require the minimum number of fill-in edges to become a clique (ties broken by smaller current degree, then by smaller vertex index), adding the necessary fill-in edges and recording the maximum neighborhood size encountered; the final maximum neighborhood size is $\\hat{t}$.\n   - For binary variables with $k=2$, computes the per-instance asymptotic upper bound on the number of arithmetic operations for variable elimination as $n \\cdot 2^{\\hat{t}+1}$.\n3. Use the following test suite of undirected graphs that model curated pathway modules. Vertices are indexed from $0$ to $n-1$. Each test item is specified by the pair $(n,\\text{edges})$, where $\\text{edges}$ is a set of unordered pairs:\n   - Boundary case (isolated variable): $(n,\\text{edges}) = \\left(1,\\ \\varnothing\\right)$.\n   - Pathway-like linear chain with $n=10$: $(n,\\text{edges}) = \\left(10,\\ \\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9)\\}\\right)$.\n   - Star module with $n=10$ and center at $0$: $(n,\\text{edges}) = \\left(10,\\ \\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,9)\\}\\right)$.\n   - Cycle motif with $n=8$: $(n,\\text{edges}) = \\left(8,\\ \\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,0)\\}\\right)$.\n   - Two-row grid module ($2\\times 4$) with $n=8$: $(n,\\text{edges}) = \\left(8,\\ \\{(0,1),(1,2),(2,3),(4,5),(5,6),(6,7),(0,4),(1,5),(2,6),(3,7)\\}\\right)$.\n   - Clique core with tail (dense hub within a module): $(n,\\text{edges}) = \\left(7,\\ \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(3,4),(4,5),(5,6)\\}\\right)$.\n4. Assume binary variables with $k=2$ for all test cases. Your program should process the test suite in the order listed above and output, for each test case, the pair $[\\hat{t}, n \\cdot 2^{\\hat{t}+1}]$.\n5. Final output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list. For example, an output for three cases would look like $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$ with $a_i$ and $b_i$ being integers.\n\nThere are no physical units involved in this task. All angles, if any appear, should be understood in radians, but none are used here. All requested numerical outputs are integers.",
            "solution": "The user has provided a valid problem statement. It is scientifically grounded in the principles of probabilistic graphical models and computational complexity, well-posed with clear definitions and objectives, and free of any of the invalidating flaws outlined in the instructions. I will now proceed with a complete solution.\n\n### Part 1: Derivation of the Asymptotic Running-Time Bound\n\nThe task is to derive an asymptotic running-time bound for exact inference by variable elimination on a factor graph with $n$ variables and treewidth $t$. The bound must be of the form $O(f(t)\\,n)$, where $f$ is an exponential function of $t$, and derived from the given first principles.\n\n1.  **Total Computational Cost**: The variable elimination algorithm processes each of the $n$ variables in the graph $G=(V, E)$ according to a specific elimination ordering, $\\pi = (v_1, v_2, \\dots, v_n)$. The total computational cost is the sum of the costs of eliminating each variable.\n    $$ \\text{Total Cost}(\\pi) = \\sum_{i=1}^{n} \\text{Cost}(\\text{eliminate } v_i) $$\n\n2.  **Cost of a Single Elimination Step**: The problem states that the cost of eliminating a single variable $v_i$ is proportional to $k^{w_i+1}$, where $k$ is the domain size of each variable and $w_i$ is the size of the clique formed by the neighborhood of $v_i$ at the moment of its elimination. This size $w_i$ is precisely the degree of vertex $v_i$ in the graph as it exists just before the $i$-th step. Let $d_i(v_i)$ be this degree. The cost of eliminating $v_i$ is thus $C \\cdot k^{d_i(v_i)+1}$ for some constant of proportionality $C$.\n\n3.  **Upper Bounding the Cost with Induced Width**: The induced width of the elimination ordering $\\pi$, denoted $w^*(\\pi)$, is defined as the maximum degree of any vertex at the time of its elimination over the entire process.\n    $$ w^*(\\pi) = \\max_{i \\in \\{1, \\dots, n\\}} d_i(v_i) $$\n    Therefore, for any variable $v_i$ in the ordering $\\pi$, its degree at elimination is bounded by the induced width: $d_i(v_i) \\leq w^*(\\pi)$. We can use this to establish an upper bound on the total cost for a given ordering $\\pi$:\n    $$ \\text{Total Cost}(\\pi) = \\sum_{i=1}^{n} C \\cdot k^{d_i(v_i)+1} \\leq \\sum_{i=1}^{n} C \\cdot k^{w^*(\\pi)+1} $$\n    Since the term $C \\cdot k^{w^*(\\pi)+1}$ is constant with respect to the summation over $n$ variables, we have:\n    $$ \\text{Total Cost}(\\pi) \\leq n \\cdot C \\cdot k^{w^*(\\pi)+1} $$\n\n4.  **Introducing Treewidth**: The variable elimination algorithm's efficiency depends critically on finding a good elimination ordering. An optimal ordering is one that minimizes the induced width. The treewidth, $t$, of the graph $G$ is defined as the minimum possible induced width over all possible elimination orderings.\n    $$ t = \\min_{\\pi} w^*(\\pi) $$\n    If we use an optimal elimination ordering, $\\pi_{\\text{opt}}$, then its induced width is equal to the treewidth of the graph, i.e., $w^*(\\pi_{\\text{opt}}) = t$.\n\n5.  **Final Asymptotic Bound**: By substituting the treewidth $t$ into our cost inequality for an optimal ordering, we obtain the upper bound on the complexity of the best-case variable elimination:\n    $$ \\text{Total Cost}(\\pi_{\\text{opt}}) \\leq n \\cdot C \\cdot k^{t+1} $$\n    In terms of asymptotic complexity, we abstract away the constant factor $C$. The running time is therefore bounded by:\n    $$ O(n \\cdot k^{t+1}) $$\n    This expression matches the required form $O(f(t) \\cdot n)$, where $f(t) = k^{t+1}$ is an exponential function of the treewidth $t$. This completes the derivation.\n\n### Part 2: Algorithmic Implementation\n\nThe second part of the task involves implementing an algorithm to compute an upper bound on the treewidth, $\\hat{t}$, using the min-fill heuristic, and then calculating the total cost using this bound.\n\n**Min-Fill Heuristic Logic**\nThe algorithm proceeds iteratively, eliminating one vertex at each step. In each step, we must select which vertex to eliminate from the set of remaining vertices. The selection follows a strict set of rules:\n1.  **Primary Criterion (Min-Fill)**: Choose the vertex that requires the minimum number of \"fill-in\" edges to make its current neighborhood a clique. The number of fill-in edges for a vertex with $d$ neighbors is $\\frac{d(d-1)}{2} - m$, where $m$ is the number of edges already present between pairs of its neighbors.\n2.  **First Tie-Breaker (Min-Degree)**: If multiple vertices have the same minimum fill-in count, choose from this subset the one with the smallest current degree.\n3.  **Second Tie-Breaker (Min-Index)**: If a tie still persists, choose the vertex with the smallest integer index.\n\n**Process:**\n1.  Initialize the graph from the given vertex count $n$ and edge set. An adjacency list representation (e.g., a dictionary of sets in Python) is suitable.\n2.  Maintain a set of remaining vertices, initially containing all vertices from $0$ to $n-1$.\n3.  Initialize a variable `max_degree_encountered` to $0$.\n4.  Loop $n$ times to eliminate each vertex:\n    a.  Among all remaining vertices, find the best vertex to eliminate by applying the min-fill heuristic with its tie-breaking rules. A clean way to implement this is to iterate through the candidates, calculating a tuple `(fill_count, degree, index)` for each, and finding the lexicographically smallest tuple.\n    b.  Once the vertex $v$ to be eliminated is chosen, record its current degree, $d_v$. Update `max_degree_encountered = max(max_degree_encountered, d_v)`.\n    c.  Modify the graph: add all necessary fill-in edges between the neighbors of $v$ to make them a clique.\n    d.  Remove $v$ from the graph structure: remove it from the set of remaining vertices and from the adjacency lists of its former neighbors.\n5.  After the loop terminates, `max_degree_encountered` holds the value of $\\hat{t}$, the induced width of the ordering found by the heuristic. This serves as an upper bound for the true treewidth $t$.\n\n**Cost Calculation:**\nFor each test case with $n$ variables, binary domain size ($k=2$), and a computed treewidth upper bound $\\hat{t}$, the total number of arithmetic operations is calculated as $n \\cdot 2^{\\hat{t}+1}$.\nThe final program implements this logic for each test case and formats the output as a list of `[t_hat, cost]` pairs.",
            "answer": "```python\nimport sys\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It computes an upper bound on treewidth using the min-fill heuristic\n    and then calculates the corresponding complexity bound for variable elimination.\n    \"\"\"\n\n    # The problem specifies that numpy and scipy are allowed, but they are not\n    # necessary for this specific implementation. Using only standard libraries.\n\n    test_cases = [\n        (1, set()),\n        (10, {(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)}),\n        (10, {(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9)}),\n        (8, {(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 0)}),\n        (8, {(0, 1), (1, 2), (2, 3), (4, 5), (5, 6), (6, 7), (0, 4), (1, 5), (2, 6), (3, 7)}),\n        (7, {(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (5, 6)}),\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        t_hat = compute_treewidth_upper_bound(n, edges)\n        # Using k=2 as specified for binary variables\n        # Cost is n * 2^(t_hat + 1). Use bit shift for efficiency and to handle large numbers.\n        cost = n * (1  (t_hat + 1))\n        results.append([t_hat, cost])\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef compute_treewidth_upper_bound(n, edges_set):\n    \"\"\"\n    Computes an upper bound on treewidth (the induced width) for a given graph\n    using the min-fill heuristic.\n\n    Args:\n        n (int): The number of vertices in the graph.\n        edges_set (set): A set of tuples representing the undirected edges.\n\n    Returns:\n        int: The computed treewidth upper bound (t_hat).\n    \"\"\"\n    if n = 1:\n        return 0\n\n    # Adjacency list representation using a dictionary of sets\n    adj = {i: set() for i in range(n)}\n    for u, v in edges_set:\n        adj[u].add(v)\n        adj[v].add(u)\n\n    remaining_nodes = set(range(n))\n    max_width_encountered = 0\n\n    # Eliminate one node in each iteration\n    for _ in range(n):\n        # Store (fill_count, degree, index) to find the best node to eliminate.\n        # Python's tuple comparison handles the tie-breaking logic automatically.\n        best_node_info = (float('inf'), float('inf'), -1)\n\n        # Iterate through remaining nodes in sorted order to ensure deterministic tie-breaking.\n        for node in sorted(list(remaining_nodes)):\n            neighbors = adj[node]\n            degree = len(neighbors)\n            \n            fill_count = 0\n            if degree  1:\n                neighbor_list = list(neighbors)\n                for i in range(degree):\n                    for j in range(i + 1, degree):\n                        u, v = neighbor_list[i], neighbor_list[j]\n                        if v not in adj[u]:\n                            fill_count += 1\n            \n            current_node_info = (fill_count, degree, node)\n            if current_node_info  best_node_info:\n                best_node_info = current_node_info\n\n        _, degree_at_elimination, node_to_eliminate = best_node_info\n        \n        max_width_encountered = max(max_width_encountered, degree_at_elimination)\n        \n        # Add fill-in edges (triangulate the neighbors)\n        neighbors_to_connect = list(adj[node_to_eliminate])\n        for i in range(len(neighbors_to_connect)):\n            for j in range(i + 1, len(neighbors_to_connect)):\n                u, v = neighbors_to_connect[i], neighbors_to_connect[j]\n                adj[u].add(v)\n                adj[v].add(u)\n        \n        # Remove the node from the graph\n        for neighbor in neighbors_to_connect:\n            adj[neighbor].remove(node_to_eliminate)\n        del adj[node_to_eliminate]\n        remaining_nodes.remove(node_to_eliminate)\n        \n    return max_width_encountered\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}