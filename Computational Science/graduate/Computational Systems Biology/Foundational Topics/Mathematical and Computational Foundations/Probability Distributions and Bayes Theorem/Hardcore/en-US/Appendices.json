{
    "hands_on_practices": [
        {
            "introduction": "Bayesian inference provides a full posterior distribution for unknown parameters, but often we need to summarize this distribution with a single point estimate. The choice of this summary statistic is not arbitrary; it is an optimal decision that depends on how we define the \"cost\" or \"loss\" of making an error. This first practice establishes the fundamental connection between a chosen loss function and the resulting Bayes estimator, demonstrating from first principles why the posterior mean, median, and mode are optimal under quadratic, absolute, and zero-one loss, respectively . Working through this exercise illuminates the core concepts of Bayesian decision theory using the canonical Beta-Bernoulli model.",
            "id": "3340172",
            "problem": "In a single-cell signaling experiment in computational systems biology, each cell is classified as transcriptionally active or inactive for a specific pathway, yielding a binary observation $x_{i} \\in \\{0,1\\}$ for cell $i$. Assume $x_{1},\\dots,x_{n}$ are conditionally independent and identically distributed given an unknown activation probability $\\theta \\in (0,1)$, with sampling model $x_{i} \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$. A prior belief about $\\theta$ is modeled by a Beta distribution with density proportional to $\\theta^{\\alpha_{0}-1} (1-\\theta)^{\\beta_{0}-1}$ for hyperparameters $\\alpha_{0} > 0$ and $\\beta_{0} > 0$.\n\nUsing only the definitions of the Bernoulli likelihood, the Beta prior, Bayes’ theorem for the posterior, and the definition of a Bayes estimator as the action $a$ that minimizes posterior expected loss $\\mathbb{E}[L(\\theta,a)\\mid x_{1:n}]$, do the following:\n\n1. Derive from first principles the Bayes estimator $\\delta^{\\star}(x_{1:n})$ for $\\theta$ under each of the following loss functions:\n   - Quadratic loss $L_{2}(\\theta,a) = (\\theta - a)^{2}$.\n   - Absolute loss $L_{1}(\\theta,a) = |\\theta - a|$.\n   - Zero–one loss $L_{01}(\\theta,a) = \\mathbf{1}\\{|\\theta - a| > 0\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Your derivation must justify any limiting or approximation arguments needed to make the zero–one loss well-defined for a continuous parameter.\n\n2. Now specialize to the case of a uniform prior with $\\alpha_{0} = 1$ and $\\beta_{0} = 1$, and experimental data consisting of $n = 3$ cells with $s = 0$ active cells observed. Use Bayes’ theorem to find the posterior parameters $(\\alpha,\\beta)$, and then compute the values of the three Bayes estimators you derived in part $1$ for this posterior.\n\nProvide your final answer as a single row matrix with entries in the order: quadratic-loss estimator, absolute-loss estimator, zero–one-loss estimator. Do not round; provide exact values.",
            "solution": "The problem asks for the derivation of three Bayes estimators for an unknown parameter $\\theta$ and their subsequent calculation for a specific data set and prior. The process consists of two parts: a general derivation from first principles and a specific application.\n\nFirst, we establish the Bayesian framework. The data $x_1, \\dots, x_n$ are conditionally independent and identically distributed draws from a Bernoulli distribution with parameter $\\theta$, i.e., $x_i \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$. The likelihood function for the entire dataset $x_{1:n} = (x_1, \\dots, x_n)$ is given by the product of individual probabilities:\n$$P(x_{1:n} \\mid \\theta) = \\prod_{i=1}^{n} P(x_i \\mid \\theta) = \\prod_{i=1}^{n} \\theta^{x_i} (1-\\theta)^{1-x_i} = \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i}$$\nLet $s = \\sum_{i=1}^{n} x_i$ be the total number of successes (active cells). The likelihood is then $L(\\theta; s, n) = \\theta^s (1-\\theta)^{n-s}$.\n\nThe prior belief about $\\theta$ is modeled by a Beta distribution, $\\theta \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)$, with probability density function (PDF):\n$$p(\\theta) = \\frac{\\theta^{\\alpha_0-1}(1-\\theta)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}$$\nwhere $B(\\alpha_0, \\beta_0)$ is the Beta function, which serves as a normalization constant.\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ given the data $x_{1:n}$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta \\mid x_{1:n}) \\propto P(x_{1:n} \\mid \\theta) p(\\theta) \\propto \\left( \\theta^s (1-\\theta)^{n-s} \\right) \\left( \\theta^{\\alpha_0-1}(1-\\theta)^{\\beta_0-1} \\right)$$\n$$p(\\theta \\mid x_{1:n}) \\propto \\theta^{s+\\alpha_0-1} (1-\\theta)^{n-s+\\beta_0-1}$$\nThis is the kernel of a Beta distribution, so the posterior distribution is also a Beta distribution:\n$$\\theta \\mid x_{1:n} \\sim \\mathrm{Beta}(\\alpha_0+s, \\beta_0+n-s)$$\nWe denote the posterior parameters as $\\alpha = \\alpha_0+s$ and $\\beta = \\beta_0+n-s$.\n\nA Bayes estimator $\\delta^{\\star}(x_{1:n})$ of $\\theta$ is an action $a$ that minimizes the posterior expected loss, $\\mathbb{E}[L(\\theta,a)\\mid x_{1:n}]$.\n$$\\delta^{\\star}(x_{1:n}) = \\arg\\min_{a} \\mathbb{E}[L(\\theta,a)\\mid x_{1:n}] = \\arg\\min_{a} \\int_{0}^{1} L(\\theta,a) p(\\theta \\mid x_{1:n}) d\\theta$$\n\n**Part 1: Derivation of Bayes Estimators**\n\nWe derive the estimator for each of the three specified loss functions.\n\n1.  **Quadratic Loss: $L_{2}(\\theta, a) = (\\theta - a)^{2}$**\n    The posterior expected loss is $R(a) = \\mathbb{E}[(\\theta-a)^2 \\mid x_{1:n}] = \\int_0^1 (\\theta-a)^2 p(\\theta \\mid x_{1:n}) d\\theta$.\n    To find the value of $a$ that minimizes $R(a)$, we take the derivative with respect to $a$ and set it to zero.\n    $$\\frac{d R(a)}{da} = \\frac{d}{da} \\int_0^1 (\\theta^2 - 2a\\theta + a^2) p(\\theta \\mid x_{1:n}) d\\theta$$\n    Using Leibniz's rule for differentiating under the integral sign:\n    $$\\frac{d R(a)}{da} = \\int_0^1 \\frac{\\partial}{\\partial a} (\\theta-a)^2 p(\\theta \\mid x_{1:n}) d\\theta = \\int_0^1 -2(\\theta-a) p(\\theta \\mid x_{1:n}) d\\theta$$\n    Setting the derivative to zero:\n    $$-2 \\int_0^1 (\\theta-a) p(\\theta \\mid x_{1:n}) d\\theta = 0$$\n    $$\\int_0^1 \\theta p(\\theta \\mid x_{1:n}) d\\theta - \\int_0^1 a p(\\theta \\mid x_{1:n}) d\\theta = 0$$\n    $$a \\int_0^1 p(\\theta \\mid x_{1:n}) d\\theta = \\int_0^1 \\theta p(\\theta \\mid x_{1:n}) d\\theta$$\n    Since $p(\\theta \\mid x_{1:n})$ is a PDF, $\\int_0^1 p(\\theta \\mid x_{1:n}) d\\theta = 1$. This leaves:\n    $$a = \\int_0^1 \\theta p(\\theta \\mid x_{1:n}) d\\theta = \\mathbb{E}[\\theta \\mid x_{1:n}]$$\n    The second derivative is $\\frac{d^2 R(a)}{da^2} = \\int_0^1 2 p(\\theta \\mid x_{1:n}) d\\theta = 2 > 0$, confirming this is a minimum.\n    Thus, the Bayes estimator under quadratic loss is the posterior mean.\n    $\\delta_{L_2}^{\\star}(x_{1:n}) = \\mathbb{E}[\\theta \\mid x_{1:n}]$.\n\n2.  **Absolute Loss: $L_{1}(\\theta, a) = |\\theta - a|$**\n    The posterior expected loss is $R(a) = \\mathbb{E}[|\\theta-a| \\mid x_{1:n}] = \\int_0^1 |\\theta-a| p(\\theta \\mid x_{1:n}) d\\theta$.\n    We can split the integral at $a$:\n    $$R(a) = \\int_0^a (a-\\theta) p(\\theta \\mid x_{1:n}) d\\theta + \\int_a^1 (\\theta-a) p(\\theta \\mid x_{1:n}) d\\theta$$\n    We differentiate with respect to $a$ using Leibniz's rule:\n    $$\\frac{d R(a)}{da} = \\left( (a-a)p(a|x_{1:n}) + \\int_0^a 1 \\cdot p(\\theta \\mid x_{1:n}) d\\theta \\right) - \\left( (a-a)p(a \\mid x_{1:n}) - \\int_a^1 1 \\cdot p(\\theta \\mid x_{1:n}) d\\theta \\right)$$\n    $$\\frac{d R(a)}{da} = \\int_0^a p(\\theta \\mid x_{1:n}) d\\theta - \\int_a^1 p(\\theta \\mid x_{1:n}) d\\theta = P(\\theta \\le a \\mid x_{1:n}) - P(\\theta > a \\mid x_{1:n})$$\n    Setting the derivative to zero yields:\n    $$P(\\theta \\le a \\mid x_{1:n}) = P(\\theta > a \\mid x_{1:n})$$\n    This equality holds if the probability mass is split equally on both sides of $a$. This is the definition of the median of the posterior distribution. Let $m$ be the posterior median. Then $P(\\theta \\le m \\mid x_{1:n}) = 1/2$.\n    Thus, the Bayes estimator under absolute loss is the posterior median.\n    $\\delta_{L_1}^{\\star}(x_{1:n}) = \\text{median}(\\theta \\mid x_{1:n})$.\n\n3.  **Zero–One Loss: $L_{01}(\\theta, a) = \\mathbf{1}\\{|\\theta - a| > 0\\}$**\n    For a continuous parameter $\\theta$, the loss function as stated leads to difficulties because $P(\\theta=a \\mid x_{1:n})=0$ for any $a$. The expected loss would be $\\mathbb{E}[\\mathbf{1}\\{\\theta \\neq a\\}\\mid x_{1:n}] = P(\\theta \\neq a \\mid x_{1:n}) = 1$ for all $a$, which is not useful.\n    As instructed, we must use a limiting argument. Consider a modified loss function $L_{\\epsilon}(\\theta, a) = \\mathbf{1}\\{|\\theta - a| > \\epsilon\\}$ for some small $\\epsilon > 0$. We want to minimize the posterior expected loss:\n    $$R(a) = \\mathbb{E}[L_{\\epsilon}(\\theta, a) \\mid x_{1:n}] = \\int_0^1 \\mathbf{1}\\{|\\theta - a| > \\epsilon\\} p(\\theta \\mid x_{1:n}) d\\theta = P(|\\theta - a| > \\epsilon \\mid x_{1:n})$$\n    Minimizing $P(|\\theta - a| > \\epsilon \\mid x_{1:n})$ is equivalent to maximizing its complement, $P(|\\theta - a| \\le \\epsilon \\mid x_{1:n})$.\n    $$P(|\\theta - a| \\le \\epsilon \\mid x_{1:n}) = P(a-\\epsilon \\le \\theta \\le a+\\epsilon \\mid x_{1:n}) = \\int_{a-\\epsilon}^{a+\\epsilon} p(\\theta \\mid x_{1:n}) d\\theta$$\n    For a small $\\epsilon$, and assuming $p(\\theta \\mid x_{1:n})$ is continuous at $a$, the integral can be approximated by the area of a rectangle with height $p(a \\mid x_{1:n})$ and width $2\\epsilon$:\n    $$\\int_{a-\\epsilon}^{a+\\epsilon} p(\\theta \\mid x_{1:n}) d\\theta \\approx p(a \\mid x_{1:n}) \\cdot (2\\epsilon)$$\n    To maximize this approximate probability, we must choose $a$ to be the value that maximizes the posterior density function $p(\\theta \\mid x_{1:n})$. This value is, by definition, the mode of the posterior distribution. This argument holds in the limit as $\\epsilon \\to 0$.\n    Thus, the Bayes estimator under zero-one loss for a continuous parameter is the posterior mode.\n    $\\delta_{L_{01}}^{\\star}(x_{1:n}) = \\arg\\max_{\\theta} p(\\theta \\mid x_{1:n})$.\n\n**Part 2: Application to Specific Case**\n\nWe are given a uniform prior, which corresponds to $\\alpha_0 = 1$ and $\\beta_0 = 1$. The data consists of $n=3$ cells with $s=0$ active cells.\n\nUsing the formula for the posterior parameters derived earlier:\n- $\\alpha = s + \\alpha_0 = 0 + 1 = 1$\n- $\\beta = n - s + \\beta_0 = 3 - 0 + 1 = 4$\n\nThe posterior distribution is $\\theta \\mid x_{1:n} \\sim \\mathrm{Beta}(1, 4)$.\nThe posterior PDF is $p(\\theta \\mid x_{1:n}) = \\frac{\\theta^{1-1}(1-\\theta)^{4-1}}{B(1,4)} = \\frac{(1-\\theta)^3}{B(1,4)}$.\nThe Beta function value is $B(1,4) = \\frac{\\Gamma(1)\\Gamma(4)}{\\Gamma(1+4)} = \\frac{0! \\cdot 3!}{4!} = \\frac{1 \\cdot 6}{24} = \\frac{1}{4}$.\nSo, $p(\\theta \\mid x_{1:n}) = 4(1-\\theta)^3$ for $\\theta \\in (0,1)$.\n\nWe now compute the three estimators for this $\\mathrm{Beta}(1, 4)$ posterior.\n\n1.  **Quadratic Loss Estimator (Posterior Mean):**\n    For a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution, the mean is $\\frac{\\alpha}{\\alpha+\\beta}$.\n    $$\\delta_{L_2}^{\\star} = \\frac{1}{1+4} = \\frac{1}{5}$$\n\n2.  **Absolute Loss Estimator (Posterior Median):**\n    We need to find the value $m$ such that $\\int_0^m p(\\theta \\mid x_{1:n}) d\\theta = \\frac{1}{2}$.\n    $$\\int_0^m 4(1-\\theta)^3 d\\theta = 4 \\left[ -\\frac{(1-\\theta)^4}{4} \\right]_0^m = - \\left[ (1-\\theta)^4 \\right]_0^m$$\n    $$= -((1-m)^4 - (1-0)^4) = 1 - (1-m)^4$$\n    Setting this equal to $\\frac{1}{2}$:\n    $$1 - (1-m)^4 = \\frac{1}{2}$$\n    $$(1-m)^4 = \\frac{1}{2}$$\n    Since $m \\in(0,1)$, $1-m$ must be positive. Taking the positive fourth root:\n    $$1-m = \\left(\\frac{1}{2}\\right)^{1/4}$$\n    $$\\delta_{L_1}^{\\star} = m = 1 - \\left(\\frac{1}{2}\\right)^{1/4}$$\n\n3.  **Zero–One Loss Estimator (Posterior Mode):**\n    We need to find the value of $\\theta$ that maximizes $p(\\theta \\mid x_{1:n}) = 4(1-\\theta)^3$ for $\\theta \\in (0,1)$.\n    The derivative of the density with respect to $\\theta$ is $\\frac{d}{d\\theta} 4(1-\\theta)^3 = -12(1-\\theta)^2$.\n    For $\\theta \\in (0,1)$, this derivative is always negative, which means the function $p(\\theta \\mid x_{1:n})$ is strictly decreasing on its support. The maximum value is therefore achieved at the left boundary of the interval, i.e., at $\\theta = 0$.\n    (Note: The general formula for the mode of a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution is $\\frac{\\alpha-1}{\\alpha+\\beta-2}$ for $\\alpha, \\beta > 1$. For the case $\\alpha=1, \\beta>1$, the mode is at $0$.)\n    $$\\delta_{L_{01}}^{\\star} = 0$$\n\nThe three estimators for the given case are $\\frac{1}{5}$, $1 - (\\frac{1}{2})^{1/4}$, and $0$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{5} & 1 - \\left(\\frac{1}{2}\\right)^{\\frac{1}{4}} & 0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "While conjugate priors offer mathematical convenience, as seen in the Beta-Bernoulli model, their functional form can be restrictive. Real-world modeling in computational biology often requires more flexible, non-conjugate priors that better reflect scientific knowledge but necessitate computational solutions. This practice delves into this crucial trade-off by comparing a conjugate Gamma prior with a non-conjugate Lognormal prior for a Poisson-distributed transcription rate . By implementing numerical integration and analyzing sensitivity, you will gain hands-on experience with the practical consequences of prior choice, particularly how the tail behavior of a prior can significantly influence posterior inferences in the common small-sample regimes.",
            "id": "3340205",
            "problem": "In computational systems biology, messenger RNA transcription counts from a homogeneous cell population during a fixed time window can be modeled as independent and identically distributed under a Poisson process. Let the per-cell transcription rate be denoted by $\\lambda$ in units of transcripts per minute, and let observed counts be modeled as $y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ for $i \\in \\{1,\\dots,N\\}$, where $N$ is the sample size and $y_i \\in \\{0,1,2,\\dots\\}$. The task is to compare how conjugate versus non-conjugate priors for $\\lambda$ influence the posterior in small-sample regimes, focusing on sensitivity to prior tail behavior.\n\nBase the analysis on Bayes' theorem, which states that the posterior density is proportional to the product of the likelihood and the prior, i.e., $p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda)\\,p(\\lambda)$, where $\\mathbf{y} = (y_1,\\dots,y_N)$ and $p(\\mathbf{y} \\mid \\lambda)$ is the product of Poisson likelihoods. Consider two priors:\n- Conjugate prior: $\\lambda \\sim \\mathrm{Gamma}(a,b)$, parameterized by shape $a>0$ and rate $b>0$.\n- Non-conjugate prior: $\\log \\lambda \\sim \\mathcal{N}(\\mu,\\sigma^2)$, that is, $\\lambda$ follows a lognormal distribution with parameters $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$.\n\nFrom first principles, derive the posterior for the conjugate prior and a computable expression for the posterior expectation under the non-conjugate prior. Then, define and compute the following sensitivity metrics for small-sample regimes:\n- For the lognormal prior, the absolute difference in posterior means when the log-scale standard deviation changes between a \"light-tail\" choice $\\sigma_{\\mathrm{light}}$ and a \"heavy-tail\" choice $\\sigma_{\\mathrm{heavy}}$, while holding the prior mean fixed at $m_0$ by adjusting $\\mu$ accordingly via $\\mu = \\log(m_0) - \\sigma^2/2$. Denote this as $\\Delta_{\\mathrm{logN}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{light}}]\\right|$.\n- For the gamma prior, the absolute difference in posterior means when the shape parameter changes between a \"light-tail\" choice $a_{\\mathrm{light}}$ and a \"heavy-tail\" choice $a_{\\mathrm{heavy}}$, while holding the prior mean fixed at $m_0$ by setting $b = a/m_0$. Denote this as $\\Delta_{\\mathrm{Gamma}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{light}}]\\right|$.\n- The ratio $R = \\Delta_{\\mathrm{logN}} / \\Delta_{\\mathrm{Gamma}}$ as a dimensionless measure of relative sensitivity.\n\nAll posterior means and differences for $\\lambda$ must be expressed in transcripts per minute. The ratio $R$ is dimensionless. Angles do not appear in this problem.\n\nImplement a program that, given the test suite below, computes $\\Delta_{\\mathrm{logN}}$, $\\Delta_{\\mathrm{Gamma}}$, and $R$ for each test case as floating-point numbers. Use numerically stable integration for the non-conjugate posterior expectation derived from Bayes' theorem. The final output must be a single line containing a list of lists, where each inner list corresponds to one test case and has the form $[\\Delta_{\\mathrm{logN}}, \\Delta_{\\mathrm{Gamma}}, R]$.\n\nTest suite (each case specifies $\\mathbf{y}$, $m_0$, $\\sigma_{\\mathrm{light}}$, $\\sigma_{\\mathrm{heavy}}$, $a_{\\mathrm{light}}$, $a_{\\mathrm{heavy}}$):\n1. Case $1$: $N=2$, $\\mathbf{y}=[0,1]$, $m_0=1.0$, $\\sigma_{\\mathrm{light}}=0.25$, $\\sigma_{\\mathrm{heavy}}=1.0$, $a_{\\mathrm{light}}=50.0$, $a_{\\mathrm{heavy}}=1.0$.\n2. Case $2$: $N=5$, $\\mathbf{y}=[0,0,1,0,2]$, $m_0=0.5$, $\\sigma_{\\mathrm{light}}=0.2$, $\\sigma_{\\mathrm{heavy}}=1.2$, $a_{\\mathrm{light}}=40.0$, $a_{\\mathrm{heavy}}=0.5$.\n3. Case $3$: $N=1$, $\\mathbf{y}=[5]$, $m_0=4.0$, $\\sigma_{\\mathrm{light}}=0.3$, $\\sigma_{\\mathrm{heavy}}=0.9$, $a_{\\mathrm{light}}=20.0$, $a_{\\mathrm{heavy}}=1.0$.\n4. Case $4$: $N=3$, $\\mathbf{y}=[0,0,0]$, $m_0=0.2$, $\\sigma_{\\mathrm{light}}=0.3$, $\\sigma_{\\mathrm{heavy}}=1.5$, $a_{\\mathrm{light}}=50.0$, $a_{\\mathrm{heavy}}=0.3$.\n5. Case $5$: $N=2$, $\\mathbf{y}=[50,60]$, $m_0=40.0$, $\\sigma_{\\mathrm{light}}=0.5$, $\\sigma_{\\mathrm{heavy}}=1.0$, $a_{\\mathrm{light}}=30.0$, $a_{\\mathrm{heavy}}=1.0$.\n\nFor each case, let $S=\\sum_{i=1}^{N} y_i$ denote the total count. Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, enclosed in square brackets, for example, `[[x_1,y_1,z_1],[x_2,y_2,z_2],...]`, where $x_i$, $y_i$, and $z_i$ are the three floats for case $i$ in the units specified above.",
            "solution": "The problem requires a Bayesian analysis of a Poisson model for mRNA transcript counts. We must compare the sensitivity of the posterior mean of the transcription rate, $\\lambda$, to the choice of prior distribution. Specifically, we compare a conjugate Gamma prior to a non-conjugate Lognormal prior, focusing on how the tail behavior of each prior influences the posterior in small-sample regimes.\n\nThe analysis proceeds in two main steps. First, we derive the analytical or computable forms of the posterior means for $\\lambda$ under both prior scenarios. Second, we implement these calculations numerically to compute the specified sensitivity metrics for a given suite of test cases.\n\nLet the observed data be $\\mathbf{y} = (y_1, \\dots, y_N)$, where each $y_i \\in \\{0, 1, 2, \\dots\\}$ is an independent draw from a Poisson distribution with rate $\\lambda$.\n$$y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$$\nThe likelihood of the entire dataset $\\mathbf{y}$ is the product of the individual Poisson probability mass functions:\n$$p(\\mathbf{y} \\mid \\lambda) = \\prod_{i=1}^N \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!} = \\frac{\\lambda^{\\sum y_i} e^{-N\\lambda}}{\\prod y_i!}$$\nFor Bayesian inference, we are interested in the likelihood as a function of the parameter $\\lambda$. We can ignore terms that do not depend on $\\lambda$. Let $S = \\sum_{i=1}^N y_i$ be the sum of the counts. The likelihood is then proportional to:\n$$p(\\mathbf{y} \\mid \\lambda) \\propto \\lambda^S e^{-N\\lambda}$$\nAccording to Bayes' theorem, the posterior distribution of $\\lambda$ is $p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda) p(\\lambda)$, where $p(\\lambda)$ is the prior distribution for $\\lambda$.\n\n### Conjugate Prior Analysis: Gamma-Poisson Model\n\nThe conjugate prior for the Poisson likelihood is the Gamma distribution. Let the prior for $\\lambda$ be:\n$$\\lambda \\sim \\mathrm{Gamma}(a, b)$$\nwhere $a > 0$ is the shape parameter and $b > 0$ is the rate parameter. The probability density function (PDF) is $p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} e^{-b\\lambda}$, which is proportional to $\\lambda^{a-1} e^{-b\\lambda}$.\n\nThe posterior distribution is found by multiplying the likelihood and the prior:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto (\\lambda^S e^{-N\\lambda}) (\\lambda^{a-1} e^{-b\\lambda}) = \\lambda^{a+S-1} e^{-(b+N)\\lambda}$$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution:\n$$\\lambda \\mid \\mathbf{y} \\sim \\mathrm{Gamma}(a', b')$$\nwith posterior shape $a' = a+S$ and posterior rate $b' = b+N$.\n\nThe mean of a $\\mathrm{Gamma}(a', b')$ distribution is $\\frac{a'}{b'}$. Therefore, the posterior mean of $\\lambda$ is:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{a+S}{b+N}$$\nThe problem states that the prior mean is held fixed at a value $m_0$. The mean of the prior $\\mathrm{Gamma}(a, b)$ distribution is $\\mathbb{E}[\\lambda] = a/b$. So, we have the constraint $m_0 = a/b$, which implies $b = a/m_0$. Substituting this into the posterior mean formula gives:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{a+S}{(a/m_0) + N}$$\nThis is the expression used to calculate the posterior mean for the Gamma prior.\n\n### Non-Conjugate Prior Analysis: Lognormal-Poisson Model\n\nThe non-conjugate prior for $\\lambda$ is specified as a Lognormal distribution. This means $\\log \\lambda$ follows a Normal distribution:\n$$\\log \\lambda \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\nThe PDF for $\\lambda$ is $p(\\lambda) = \\frac{1}{\\lambda \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right)$, for $\\lambda > 0$.\n\nThe posterior distribution is again proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto (\\lambda^S e^{-N\\lambda}) \\left( \\frac{1}{\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) \\right)$$\n$$p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{S-1} e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right)$$\nThis posterior is not a standard, named distribution. The posterior mean must be computed from its definition:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{\\int_0^\\infty \\lambda \\cdot p(\\lambda \\mid \\mathbf{y}) d\\lambda}{\\int_0^\\infty p(\\lambda \\mid \\mathbf{y}) d\\lambda}$$\nSubstituting the unnormalized posterior density, we get:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{\\int_0^\\infty \\lambda^S e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) d\\lambda}{\\int_0^\\infty \\lambda^{S-1} e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) d\\lambda}$$\nThese integrals do not have a closed-form solution and must be computed numerically. To improve numerical stability, we perform a change of variables. Let $x = \\log \\lambda$, which implies $\\lambda = e^x$ and $d\\lambda = e^x dx$. The integration domain changes from $(0, \\infty)$ to $(-\\infty, \\infty)$. Let's denote the numerator integral as $I_{\\text{num}}$ and the denominator integral as $I_{\\text{den}}$.\n$$I_{\\text{num}} = \\int_{-\\infty}^{\\infty} (e^x)^S e^{-Ne^x} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) e^x dx = \\int_{-\\infty}^{\\infty} \\exp\\left(x(S+1) - Ne^x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\n$$I_{\\text{den}} = \\int_{-\\infty}^{\\infty} (e^x)^{S-1} e^{-Ne^x} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) e^x dx = \\int_{-\\infty}^{\\infty} \\exp\\left(xS - Ne^x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nThe posterior mean is then $\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = I_{\\text{num}} / I_{\\text{den}}$. These integrals are well-behaved and can be reliably computed using numerical quadrature methods.\n\nThe problem constrains the prior mean to be $m_0$. The mean of a $\\mathrm{Lognormal}(\\mu, \\sigma^2)$ distribution is $\\mathbb{E}[\\lambda] = e^{\\mu + \\sigma^2/2}$. Setting this to $m_0$ and solving for $\\mu$ yields:\n$$\\mu = \\log(m_0) - \\frac{\\sigma^2}{2}$$\nThis expression for $\\mu$ is substituted into the integrands.\n\n### Computation of Sensitivity Metrics\n\nFor each test case, we compute the following quantities:\n$1$. $\\Delta_{\\mathrm{logN}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{light}}]\\right|$:\n    - We calculate two posterior means using the Lognormal prior.\n    - For the \"light-tail\" case, use $\\sigma = \\sigma_{\\mathrm{light}}$ and $\\mu_{\\mathrm{light}} = \\log(m_0) - \\sigma_{\\mathrm{light}}^2/2$.\n    - For the \"heavy-tail\" case, use $\\sigma = \\sigma_{\\mathrm{heavy}}$ and $\\mu_{\\mathrm{heavy}} = \\log(m_0) - \\sigma_{\\mathrm{heavy}}^2/2$.\n    - The posterior means are found by numerical integration, and $\\Delta_{\\mathrm{logN}}$ is the absolute difference.\n\n$2$. $\\Delta_{\\mathrm{Gamma}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{light}}]\\right|$:\n    - We calculate two posterior means using the Gamma prior. The variance of a Gamma prior with fixed mean $m_0$ is $m_0^2/a$. Thus, a smaller $a$ gives a heavier tail.\n    - For the \"light-tail\" case, use $a = a_{\\mathrm{light}}$ and $b_{\\mathrm{light}} = a_{\\mathrm{light}}/m_0$. The posterior mean is $\\frac{a_{\\mathrm{light}}+S}{b_{\\mathrm{light}}+N}$.\n    - For the \"heavy-tail\" case, use $a = a_{\\mathrm{heavy}}$ and $b_{\\mathrm{heavy}} = a_{\\mathrm{heavy}}/m_0$. The posterior mean is $\\frac{a_{\\mathrm{heavy}}+S}{b_{\\mathrm{heavy}}+N}$.\n    - $\\Delta_{\\mathrm{Gamma}}$ is the absolute difference between these two analytical values.\n\n$3$. $R = \\Delta_{\\mathrm{logN}} / \\Delta_{\\mathrm{Gamma}}$:\n    - This is the ratio of the two sensitivity metrics, providing a dimensionless comparison.\n\nThe implementation will consist of a main loop iterating through the test cases. Within the loop, helper functions will calculate the posterior means for both prior families under their respective light- and heavy-tail parameterizations. The final metrics are then computed and stored.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef calculate_gamma_posterior_mean(S, N, a, b):\n    \"\"\"\n    Calculates the posterior mean for a Gamma-Poisson model.\n    \n    Args:\n        S (int): Sum of observed counts.\n        N (int): Number of observations.\n        a (float): Shape parameter of the Gamma prior.\n        b (float): Rate parameter of the Gamma prior.\n        \n    Returns:\n        float: The posterior mean of lambda.\n    \"\"\"\n    posterior_shape = a + S\n    posterior_rate = b + N\n    return posterior_shape / posterior_rate\n\ndef calculate_lognormal_posterior_mean(S, N, mu, sigma):\n    \"\"\"\n    Calculates the posterior mean for a Lognormal-Poisson model via numerical integration.\n    \n    Args:\n        S (int): Sum of observed counts.\n        N (int): Number of observations.\n        mu (float): Location parameter of the Lognormal prior.\n        sigma (float): Scale parameter of the Lognormal prior.\n        \n    Returns:\n        float: The posterior mean of lambda.\n    \"\"\"\n    # Numerically unstable to integrate exp(log_integrand) directly.\n    # We define the log of the integrand and integrate its exponential.\n    # The change of variables is x = log(lambda).\n    \n    # log_integrand for the numerator of the posterior mean expectation\n    def log_integrand_num(x):\n        k = S + 1\n        return k * x - N * np.exp(x) - ((x - mu)**2) / (2 * sigma**2)\n\n    # log_integrand for the denominator (normalization constant)\n    def log_integrand_den(x):\n        k = S\n        return k * x - N * np.exp(x) - ((x - mu)**2) / (2 * sigma**2)\n\n    # To avoid overflow/underflow, find the peak of the log integrand and subtract it\n    # This integration is well-behaved, so direct integration is feasible,\n    # but this is a more robust approach if needed. However, quad is robust enough\n    # for these test cases.\n    integrand_num = lambda x: np.exp(log_integrand_num(x))\n    integrand_den = lambda x: np.exp(log_integrand_den(x))\n    \n    # quad returns (integral, error)\n    integral_num, _ = quad(integrand_num, -np.inf, np.inf)\n    integral_den, _ = quad(integrand_den, -np.inf, np.inf)\n    \n    if integral_den == 0:\n        # This case is unlikely with proper priors but is a safeguard.\n        return np.nan\n        \n    return integral_num / integral_den\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Case 1: (y, m0, sigma_light, sigma_heavy, a_light, a_heavy)\n        (np.array([0, 1]), 1.0, 0.25, 1.0, 50.0, 1.0),\n        # Case 2\n        (np.array([0, 0, 1, 0, 2]), 0.5, 0.2, 1.2, 40.0, 0.5),\n        # Case 3\n        (np.array([5]), 4.0, 0.3, 0.9, 20.0, 1.0),\n        # Case 4\n        (np.array([0, 0, 0]), 0.2, 0.3, 1.5, 50.0, 0.3),\n        # Case 5\n        (np.array([50, 60]), 40.0, 0.5, 1.0, 30.0, 1.0),\n    ]\n\n    results = []\n    for y, m0, sigma_light, sigma_heavy, a_light, a_heavy in test_cases:\n        S = np.sum(y)\n        N = len(y)\n\n        # Lognormal Prior Calculation\n        mu_light = np.log(m0) - (sigma_light**2) / 2\n        mu_heavy = np.log(m0) - (sigma_heavy**2) / 2\n        \n        mean_logN_light = calculate_lognormal_posterior_mean(S, N, mu_light, sigma_light)\n        mean_logN_heavy = calculate_lognormal_posterior_mean(S, N, mu_heavy, sigma_heavy)\n        \n        delta_logN = abs(mean_logN_heavy - mean_logN_light)\n\n        # Gamma Prior Calculation\n        b_light = a_light / m0\n        b_heavy = a_heavy / m0\n        \n        mean_gamma_light = calculate_gamma_posterior_mean(S, N, a_light, b_light)\n        mean_gamma_heavy = calculate_gamma_posterior_mean(S, N, a_heavy, b_heavy)\n        \n        delta_gamma = abs(mean_gamma_heavy - mean_gamma_light)\n\n        # Ratio Calculation\n        # Avoid division by zero, though unlikely in this problem context.\n        if delta_gamma == 0:\n            ratio = np.inf if delta_logN != 0 else 0.0\n        else:\n            ratio = delta_logN / delta_gamma\n            \n        results.append([delta_logN, delta_gamma, ratio])\n\n    # Format output to a list of lists string with no spaces.\n    # Ex: [[1.0,2.0,3.0],[4.0,5.0,6.0]]\n    formatted_results = []\n    for res in results:\n        formatted_sublist = \"[\" + \",\".join(map(str, res)) + \"]\"\n        formatted_results.append(formatted_sublist)\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Modern computational biology problems are rarely described by simple, flat models; they often involve multiple levels of uncertainty that are best captured by a hierarchical structure. This advanced practice guides you through a full, realistic Bayesian workflow for analyzing copy number variation (CNV) from sequencing data . You will construct and analyze a hierarchical Dirichlet-Multinomial model, perform inference on a discrete latent variable (the copy number state), and, critically, learn to evaluate your model's performance by computing the posterior predictive distribution and assessing its calibration using the Probability Integral Transform (PIT), an essential technique for model validation.",
            "id": "3340191",
            "problem": "Consider a computational systems biology model of copy number variation (CNV) where noisy sequencing coverage across $K$ genomic bins is summarized by a count vector $y = (y_1,\\dots,y_K)$ with total $n = \\sum_{i=1}^K y_i$. Assume the following probabilistic generative structure grounded in standard distributional definitions and Bayes' theorem:\n\n- For a discrete copy number state $c \\in \\mathcal{C}$ with prior probability $p(c)$, the latent bin-specific coverage fractions $p = (p_1,\\dots,p_K)$ follow a Dirichlet distribution with concentration parameters linked to the copy number state:\n  $$\n  p \\mid c \\sim \\mathrm{Dirichlet}\\big(\\alpha(c)\\big), \\quad \\alpha(c) = \\kappa \\, c \\, \\omega,\n  $$\n  where $\\kappa > 0$ is a concentration scaling constant and $\\omega = (\\omega_1,\\dots,\\omega_K)$ is a fixed nonnegative weight vector satisfying $\\sum_{i=1}^K \\omega_i = 1$.\n\n- Conditional on $p$, the bin counts $y$ follow a multinomial distribution:\n  $$\n  y \\mid p \\sim \\mathrm{Multinomial}(n, p).\n  $$\n\nThis model induces a Dirichlet-Multinomial marginal for $y$ given $c$ via integration over $p$, and admits conjugate updating for the posterior over $p$ given $y$ and $c$. Using Bayes' theorem, the posterior over the copy number $c$ given observed counts $y$ is proportional to the product of the prior $p(c)$ and the Dirichlet-Multinomial marginal likelihood $p(y \\mid c)$.\n\nYour tasks are:\n\n1. Derive from first principles the posterior probabilities $p(c \\mid y)$ for $c \\in \\mathcal{C}$ under the above hierarchical model, starting from Bayes' theorem and the core distributional definitions, without assuming shortcut formulas beyond these foundations.\n\n2. For a given new count vector $y_{\\text{new}} = (y_{\\text{new},1},\\dots,y_{\\text{new},K})$ with total $n_{\\text{new}} = \\sum_{i=1}^K y_{\\text{new},i}$, compute the posterior predictive probability $p(y_{\\text{new}} \\mid y)$ under the hierarchical mixture that integrates uncertainty in both $c$ and $p$.\n\n3. Evaluate predictive calibration using the Probability Integral Transform (PIT). For discrete counts, use the randomized PIT for the marginal count in each bin $k \\in \\{1,\\dots,K\\}$, where the marginal posterior predictive distribution is the Beta-Binomial induced by integrating out $p_k$ and the complement over the Dirichlet posterior. Aggregate the $K$ PIT values within each test case and compute the one-sample Kolmogorov-Smirnov statistic against the $\\mathrm{Uniform}(0,1)$ distribution to summarize calibration.\n\nImplement the computations in a single program that performs the following for each test case:\n\n- Compute the posterior $p(c \\mid y)$ over $\\mathcal{C}$ and report the posterior mode $\\hat{c}$.\n- Compute the posterior predictive probability $p(y_{\\text{new}} \\mid y)$ and report its natural logarithm as a float.\n- Compute the randomized PIT values for each bin and report the Kolmogorov-Smirnov statistic as a float.\n\nUse the following test suite with $K = 4$ and $\\omega = (0.4, 0.3, 0.2, 0.1)$:\n\n- Test case 1 (general case):\n  - $\\mathcal{C} = \\{1, 2, 3\\}$, prior $p(c) = (1/3, 1/3, 1/3)$, $\\kappa = 8$.\n  - Observed $y = (38, 31, 21, 10)$ with $n = 100$.\n  - New $y_{\\text{new}} = (33, 26, 15, 6)$ with $n_{\\text{new}} = 80$.\n\n- Test case 2 (boundary case with no observed counts):\n  - $\\mathcal{C} = \\{1, 2, 3\\}$, prior $p(c) = (0.2, 0.5, 0.3)$, $\\kappa = 6$.\n  - Observed $y = (0, 0, 0, 0)$ with $n = 0$.\n  - New $y_{\\text{new}} = (22, 15, 9, 4)$ with $n_{\\text{new}} = 50$.\n\n- Test case 3 (high concentration and large counts):\n  - $\\mathcal{C} = \\{1, 3\\}$, prior $p(c) = (0.5, 0.5)$, $\\kappa = 12$.\n  - Observed $y = (240, 180, 120, 60)$ with $n = 600$.\n  - New $y_{\\text{new}} = (120, 90, 60, 30)$ with $n_{\\text{new}} = 300$.\n\nCalibration via PIT must use randomized values computed as $u_k = F_k(y_{\\text{new},k}^- ) + r_k \\cdot \\big(F_k(y_{\\text{new},k}) - F_k(y_{\\text{new},k}^-)\\big)$ for each bin $k$, where $F_k(\\cdot)$ is the mixture marginal cumulative distribution function for the Beta-Binomial, $F_k(t^-)$ denotes the cumulative probability just below $t$, and $r_k$ are independent draws from $\\mathrm{Uniform}(0,1)$. To ensure deterministic output, fix the random seed and report the Kolmogorov-Smirnov statistic for each test case separately.\n\nFinal Output Requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n`[c_hat_1, log p(y_new,1 | y_1), KS_1, c_hat_2, log p(y_new,2 | y_2), KS_2, c_hat_3, log p(y_new,3 | y_3), KS_3]`,\nwhere the subscript indexes the test case. All reported values must be integers or floats. No physical units or angles are involved, and decimals must be expressed without the percentage sign.",
            "solution": "The user wants a solution for a hierarchical Bayesian modeling problem in computational systems biology. I will first derive the necessary mathematical formulas from first principles and then provide an implementation based on these derivations.\n\n### Method and Derivations\n\nThe problem requires the derivation and computation of three quantities: the posterior probability of copy number states, the posterior predictive probability of a new observation, and a calibration metric based on the Probability Integral Transform (PIT).\n\n#### 1. Posterior Probability $p(c \\mid y)$\n\nThe posterior probability of a copy number state $c$ given the observed counts $y$ is given by Bayes' theorem:\n$$\np(c \\mid y) = \\frac{p(y \\mid c) p(c)}{p(y)} = \\frac{p(y \\mid c) p(c)}{\\sum_{c' \\in \\mathcal{C}} p(y \\mid c') p(c')}\n$$\nThe core of this calculation is the marginal likelihood $p(y \\mid c)$, which is obtained by integrating out the latent probabilities $p$:\n$$\np(y \\mid c) = \\int p(y \\mid p) p(p \\mid c) \\, d p\n$$\nThe model specifies a Multinomial likelihood and a Dirichlet prior:\n-   Likelihood: $p(y \\mid p) = \\frac{n!}{\\prod_{k=1}^K y_k!} \\prod_{k=1}^K p_k^{y_k}$, where $n = \\sum_k y_k$.\n-   Prior: $p(p \\mid c) = \\frac{\\Gamma(\\sum_{k=1}^K \\alpha_k(c))}{\\prod_{k=1}^K \\Gamma(\\alpha_k(c))} \\prod_{k=1}^K p_k^{\\alpha_k(c) - 1}$, where $\\alpha(c) = \\kappa c \\omega$.\n\nCombining these, the integrand becomes:\n$$\np(y \\mid p) p(p \\mid c) = \\frac{n!}{\\prod_k y_k!} \\frac{\\Gamma(\\sum_k \\alpha_k(c))}{\\prod_k \\Gamma(\\alpha_k(c))} \\prod_{k=1}^K p_k^{y_k + \\alpha_k(c) - 1}\n$$\nThe integral over $p$ can be solved by recognizing that the term $\\prod_{k=1}^K p_k^{y_k + \\alpha_k(c) - 1}$ is the kernel of a Dirichlet distribution with parameters $y + \\alpha(c)$. The integral of a Dirichlet kernel is the reciprocal of its normalization constant:\n$$\n\\int \\prod_{k=1}^K p_k^{y_k + \\alpha_k(c) - 1} \\, d p = \\frac{\\prod_k \\Gamma(y_k + \\alpha_k(c))}{\\Gamma\\left(\\sum_k (y_k + \\alpha_k(c))\\right)} = \\frac{\\prod_k \\Gamma(y_k + \\alpha_k(c))}{\\Gamma(n + \\sum_k \\alpha_k(c))}\n$$\nSubstituting this back gives the marginal likelihood, which is the probability mass function (PMF) of the Dirichlet-Multinomial distribution:\n$$\np(y \\mid c) = \\frac{n!}{\\prod_k y_k!} \\frac{\\Gamma(\\alpha_0(c))}{\\prod_k \\Gamma(\\alpha_k(c))} \\frac{\\prod_k \\Gamma(y_k + \\alpha_k(c))}{\\Gamma(n + \\alpha_0(c))}\n$$\nwhere $\\alpha_0(c) = \\sum_k \\alpha_k(c) = \\kappa c \\sum_k \\omega_k = \\kappa c$.\n\nIn practice, computations are performed in log-space to prevent numerical underflow. Let $L(c) = p(y \\mid c)p(c)$. The log-posterior is then:\n$$\n\\log p(c \\mid y) = \\log L(c) - \\log\\left(\\sum_{c' \\in \\mathcal{C}} L(c')\\right)\n$$\nThe posterior mode is $\\hat{c} = \\arg\\max_{c \\in \\mathcal{C}} p(c \\mid y)$.\n\n#### 2. Posterior Predictive Probability $p(y_{\\text{new}} \\mid y)$\n\nThe posterior predictive probability for a new count vector $y_{\\text{new}}$ given the original data $y$ is obtained by marginalizing over all uncertain quantities, namely the copy number state $c$ and the latent proportions $p$. Using the law of total probability, we can write this as a mixture over the posterior for $c$:\n$$\np(y_{\\text{new}} \\mid y) = \\sum_{c \\in \\mathcal{C}} p(y_{\\text{new}}, c \\mid y) = \\sum_{c \\in \\mathcal{C}} p(y_{\\text{new}} \\mid y, c) p(c \\mid y)\n$$\nThe term $p(c \\mid y)$ is the posterior probability derived in the previous section. The term $p(y_{\\text{new}} \\mid y, c)$ is the predictive probability for a fixed state $c$, which requires integrating over the posterior distribution of $p$ given $y$ and $c$:\n$$\np(y_{\\text{new}} \\mid y, c) = \\int p(y_{\\text{new}} \\mid p) p(p \\mid y, c) \\, dp\n$$\nDue to the conjugacy of the Dirichlet prior and the Multinomial likelihood, the posterior for $p$ is also a Dirichlet distribution:\n$$\np(p \\mid y, c) \\propto p(y \\mid p) p(p \\mid c) \\propto \\left(\\prod_k p_k^{y_k}\\right) \\left(\\prod_k p_k^{\\alpha_k(c) - 1}\\right) = \\prod_k p_k^{y_k + \\alpha_k(c) - 1}\n$$\nThus, $p \\mid y, c \\sim \\mathrm{Dirichlet}(\\alpha(c) + y)$.\n\nThe calculation of $p(y_{\\text{new}} \\mid y, c)$ is analogous to the derivation of the marginal likelihood, but using the posterior $\\mathrm{Dirichlet}(\\alpha(c) + y)$ as the new prior. The result is another Dirichlet-Multinomial PMF:\n$$\np(y_{\\text{new}} \\mid y, c) \\text{ is the PMF of } \\mathrm{DirichletMultinomial}(n_{\\text{new}}, \\alpha(c)+y)\n$$\nExplicitly, letting $\\alpha'(c) = \\alpha(c)+y$ and $\\alpha'_0(c) = \\sum_k \\alpha'_k(c) = n+\\alpha_0(c)$:\n$$\np(y_{\\text{new}} \\mid y, c) = \\frac{n_{\\text{new}}!}{\\prod_k y_{\\text{new},k}!} \\frac{\\Gamma(\\alpha'_0(c))}{\\prod_k \\Gamma(\\alpha'_k(c))} \\frac{\\prod_k \\Gamma(y_{\\text{new},k} + \\alpha'_k(c))}{\\Gamma(n_{\\text{new}} + \\alpha'_0(c))}\n$$\nThe final posterior predictive probability is the weighted average of these probabilities, with weights given by the posterior $p(c \\mid y)$. In log-space, this requires the log-sum-exp operation for numerical stability:\n$$\n\\log p(y_{\\text{new}} \\mid y) = \\text{logsumexp}_{c \\in \\mathcal{C}} \\left( \\log p(y_{\\text{new}} \\mid y, c) + \\log p(c \\mid y) \\right)\n$$\n\n#### 3. Predictive Calibration via Randomized PIT\n\nThe Probability Integral Transform (PIT) is a method to assess the calibration of a probabilistic forecast. For a continuous random variable $X$ with CDF $F_X$, the random variable $U = F_X(X)$ is uniformly distributed on $[0,1]$. For a discrete variable $Y$ with CDF $F_Y$, this property does not hold directly. A common solution is the randomized PIT:\n$$\nU = F_Y(Y-1) + R \\cdot (F_Y(Y) - F_Y(Y-1)) = P(Y'  Y) + R \\cdot P(Y' = Y)\n$$\nwhere $R$ is an independent random draw from $\\mathrm{Uniform}(0,1)$, and $Y'$ is a random variable with the same distribution as the forecast for $Y$. If the forecast is well-calibrated, the values of $U$ will be uniformly distributed on $[0,1]$.\n\nIn this problem, we need the marginal posterior predictive distribution for the count in each bin $k$, $p(y_{\\text{new},k} \\mid y)$. This is a mixture of Beta-Binomial distributions:\n$$\np(y_{\\text{new},k} \\mid y) = \\sum_{c \\in \\mathcal{C}} p(y_{\\text{new},k} \\mid y, c) p(c \\mid y)\n$$\nThe component $p(y_{\\text{new},k} \\mid y, c)$ is derived by integrating out the single proportion $p_k$ from its posterior distribution. The marginal posterior for $p_k$ is a Beta distribution:\n$$\np_k \\mid y, c \\sim \\mathrm{Beta}\\left(y_k + \\alpha_k(c), \\sum_{j \\neq k} (y_j + \\alpha_j(c))\\right)\n$$\nSince the likelihood for $y_{\\text{new},k}$ is $\\mathrm{Binomial}(n_{\\text{new}}, p_k)$, the predictive distribution for a fixed $c$ is a Beta-Binomial distribution:\n$$\ny_{\\text{new},k} \\mid y, c \\sim \\mathrm{BetaBinomial}\\left(n_{\\text{new}}, a_k, b_k\\right)\n$$\nwhere $a_k = y_k + \\alpha_k(c)$ and $b_k = (n + \\alpha_0(c)) - a_k$.\n\nLet $F_{k,c}(t)$ be the CDF of this Beta-Binomial distribution. The mixture CDF for bin $k$ is $F_k(t) = \\sum_{c \\in \\mathcal{C}} p(c \\mid y) F_{k,c}(t)$. The randomized PIT value for each bin $k$ is:\n$$\nu_k = F_k(y_{\\text{new},k}-1) + r_k \\cdot \\left( F_k(y_{\\text{new},k}) - F_k(y_{\\text{new},k}-1) \\right)\n$$\nwhere $r_k \\sim \\mathrm{Uniform}(0,1)$ are independent random draws.\n\nFinally, the set of $K$ PIT values $\\{u_1, \\dots, u_K\\}$ is tested for uniformity using the one-sample Kolmogorov-Smirnov (KS) test against $\\mathrm{Uniform}(0,1)$. The KS statistic, which measures the maximum deviation between the empirical CDF of the PIT values and the uniform CDF, provides a single summary measure of calibration.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln, logsumexp\nfrom scipy.stats import betabinom, kstest\n\ndef solve():\n    \"\"\"\n    Solves the computational systems biology problem for all test cases.\n    \"\"\"\n    \n    # Global parameters\n    K = 4\n    omega = np.array([0.4, 0.3, 0.2, 0.1])\n    \n    # Define test cases\n    test_cases = [\n        {\n            \"C\": np.array([1, 2, 3]),\n            \"p_c\": np.array([1/3, 1/3, 1/3]),\n            \"kappa\": 8.0,\n            \"y\": np.array([38, 31, 21, 10]),\n            \"y_new\": np.array([33, 26, 15, 6]),\n        },\n        {\n            \"C\": np.array([1, 2, 3]),\n            \"p_c\": np.array([0.2, 0.5, 0.3]),\n            \"kappa\": 6.0,\n            \"y\": np.array([0, 0, 0, 0]),\n            \"y_new\": np.array([22, 15, 9, 4]),\n        },\n        {\n            \"C\": np.array([1, 3]),\n            \"p_c\": np.array([0.5, 0.5]),\n            \"kappa\": 12.0,\n            \"y\": np.array([240, 180, 120, 60]),\n            \"y_new\": np.array([120, 90, 60, 30]),\n        },\n    ]\n\n    # Initialize a single random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n    for case in test_cases:\n        c_hat, log_p_new, ks_stat = process_case(case, K, omega, rng)\n        results.extend([c_hat, log_p_new, ks_stat])\n\n    print(f\"[{','.join(f'{v:.6f}' if isinstance(v, float) else str(v) for v in results)}]\")\n\ndef log_dirichlet_multinomial_pmf(y, alpha):\n    \"\"\"\n    Computes the log PMF of the Dirichlet-Multinomial distribution.\n    log P(y|alpha) = log(n!) - sum(log(y_k!)) + log B(y+alpha) - log B(alpha)\n                   = lgamma(n+1) - sum(lgamma(y_k+1)) +\n                     (sum(lgamma(y+alpha)) - lgamma(n+sum(alpha))) -\n                     (sum(lgamma(alpha)) - lgamma(sum(alpha)))\n    \"\"\"\n    n = np.sum(y)\n    log_multinomial_coeff = gammaln(n + 1) - np.sum(gammaln(y + 1))\n    \n    alpha_sum = np.sum(alpha)\n    \n    log_B_ratio = (np.sum(gammaln(y + alpha)) - gammaln(n + alpha_sum)) - \\\n                  (np.sum(gammaln(alpha)) - gammaln(alpha_sum))\n\n    return log_multinomial_coeff + log_B_ratio\n\ndef process_case(case, K, omega, rng):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    C_set = case[\"C\"]\n    p_c = case[\"p_c\"]\n    kappa = case[\"kappa\"]\n    y = case[\"y\"]\n    y_new = case[\"y_new\"]\n    n = np.sum(y)\n    n_new = np.sum(y_new)\n    \n    num_c = len(C_set)\n    log_p_c = np.log(p_c)\n\n    # --- 1. Compute posterior p(c|y) ---\n    log_posterior_unnormalized = np.zeros(num_c)\n    alphas = []\n    \n    for i, c in enumerate(C_set):\n        alpha_c = kappa * c * omega\n        alphas.append(alpha_c)\n        alpha_c_sum = np.sum(alpha_c) # kappa * c\n\n        # Log marginal likelihood p(y|c) without constant multinomial coefficient term\n        log_marginal_lik_part = (gammaln(alpha_c_sum) - np.sum(gammaln(alpha_c)) +\n                                 np.sum(gammaln(y + alpha_c)) - gammaln(n + alpha_c_sum))\n        \n        log_posterior_unnormalized[i] = log_marginal_lik_part + log_p_c[i]\n\n    log_norm_const = logsumexp(log_posterior_unnormalized)\n    log_p_c_given_y = log_posterior_unnormalized - log_norm_const\n    p_c_given_y = np.exp(log_p_c_given_y)\n\n    c_hat = C_set[np.argmax(p_c_given_y)]\n\n    # --- 2. Compute posterior predictive p(y_new|y) ---\n    log_pred_terms = np.zeros(num_c)\n    for i, c in enumerate(C_set):\n        alpha_posterior = alphas[i] + y\n        log_p_ynew_given_yc = log_dirichlet_multinomial_pmf(y_new, alpha_posterior)\n        log_pred_terms[i] = log_p_ynew_given_yc + log_p_c_given_y[i]\n        \n    log_p_ynew_given_y = logsumexp(log_pred_terms)\n\n    # --- 3. Compute PIT values and KS statistic ---\n    pit_values = np.zeros(K)\n    for k in range(K):\n        y_new_k = y_new[k]\n        \n        # Calculate mixture CDF and PMF\n        F_less_k = 0.0 # P(Y_k  y_new_k)\n        F_equal_k = 0.0 # P(Y_k = y_new_k)\n        \n        for i, c in enumerate(C_set):\n            alpha_c = alphas[i]\n            alpha_c_sum = np.sum(alpha_c)\n            \n            # Beta-Binomial parameters\n            a = y[k] + alpha_c[k]\n            b = (n + alpha_c_sum) - (y[k] + alpha_c[k])\n            \n            pmf_val = betabinom.pmf(y_new_k, n_new, a, b)\n            cdf_less_val = betabinom.cdf(y_new_k - 1, n_new, a, b)\n            \n            F_less_k += p_c_given_y[i] * cdf_less_val\n            F_equal_k += p_c_given_y[i] * pmf_val\n            \n        r_k = rng.uniform()\n        pit_values[k] = F_less_k + r_k * F_equal_k\n\n    ks_statistic, _ = kstest(pit_values, 'uniform')\n\n    return c_hat, log_p_ynew_given_y, ks_statistic\n\nsolve()\n```"
        }
    ]
}