{
    "hands_on_practices": [
        {
            "introduction": "A firm grasp of probabilistic independence is essential for building sound statistical models. This exercise uses a simplified, hypothetical gene regulatory network to construct a memorable counterexample illustrating the crucial distinction between pairwise and mutual independence. By working from first principles, you will solidify your understanding of how joint and marginal probabilities define these different levels of independence, a concept that is critical for correctly specifying dependencies in complex biological systems. ",
            "id": "3340206",
            "problem": "In a simplified gene regulatory network, two upstream transcription factors, denoted by the binary random variables $X$ and $Y$, regulate the activation of a downstream gene $Z$. Each of $X$ and $Y$ represents an activation indicator: $X=1$ if transcription factor $T_1$ is active in a randomly sampled cell and $X=0$ otherwise; similarly for $Y$ and transcription factor $T_2$. Assume that $X$ and $Y$ are independent and identically distributed with $P(X=1)=P(Y=1)=\\frac{1}{2}$. The downstream gene $Z$ is activated by an exclusive OR (XOR) logic at the promoter: $Z=1$ if and only if exactly one of $X$ or $Y$ equals $1$, which can be implemented biologically when $T_1$ and $T_2$ individually recruit an activator complex but together form an inert heterodimer that prevents activation; otherwise $Z=0$.\n\nUsing only the core definitions of probability, independence, and Bayes’ theorem (without using any pre-derived shortcuts), do the following:\n\n1. Derive the joint probability mass function of $(X,Y,Z)$ implied by the biological statement above.\n2. From first principles, verify that each pair among $(X,Y)$, $(X,Z)$, and $(Y,Z)$ is independent.\n3. Using the formal definition of mutual independence for three variables, demonstrate that $(X,Y,Z)$ are not mutually independent by explicitly comparing $P(X=1,Y=1,Z=1)$ to the product of the corresponding marginals.\n4. Apply Bayes’ theorem to compute the posterior probability $P(X=1 \\mid Z=1)$.\n\nProvide all intermediate steps and justifications grounded in the fundamental definitions. For the final reported quantity, give the exact value of $P(X=1 \\mid Z=1)$ with no rounding. The final answer must be a single real number with no units.",
            "solution": "The problem will be addressed by completing the four specified tasks in sequence. All derivations will be performed from first principles as requested.\n\nThe given random variables are $X$, $Y$, and $Z$, taking values in $\\{0, 1\\}$. We are given that $X$ and $Y$ are independent and identically distributed (i.i.d.) with $P(X=1) = \\frac{1}{2}$ and $P(Y=1) = \\frac{1}{2}$. This implies $P(X=0) = 1 - P(X=1) = \\frac{1}{2}$ and $P(Y=0) = 1 - P(Y=1) = \\frac{1}{2}$. The downstream gene $Z$ is defined by the exclusive OR (XOR) logical relationship $Z = X \\oplus Y$.\n\n**1. Derivation of the joint probability mass function (PMF) of $(X,Y,Z)$**\n\nThe joint PMF of $(X,Y,Z)$ is given by $P(X=x, Y=y, Z=z)$ for $x, y, z \\in \\{0, 1\\}$. Using the definition of conditional probability, we can write:\n$$P(X=x, Y=y, Z=z) = P(Z=z \\mid X=x, Y=y) P(X=x, Y=y)$$\nSince $X$ and $Y$ are independent, their joint PMF is the product of their marginals:\n$$P(X=x, Y=y) = P(X=x) P(Y=y) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$$\nfor any combination of $x, y \\in \\{0, 1\\}$.\n\nThe variable $Z$ is a deterministic function of $X$ and $Y$, specifically $Z = X \\oplus Y$. This means the conditional probability $P(Z=z \\mid X=x, Y=y)$ is $1$ if $z = x \\oplus y$ and $0$ otherwise. The XOR function is defined as:\n$0 \\oplus 0 = 0$\n$0 \\oplus 1 = 1$\n$1 \\oplus 0 = 1$\n$1 \\oplus 1 = 0$\n\nWe can now compute the value of the joint PMF for all $8$ possible outcomes $(x,y,z)$:\n- $P(X=0, Y=0, Z=0) = P(Z=0 \\mid X=0, Y=0) P(X=0, Y=0) = 1 \\times \\frac{1}{4} = \\frac{1}{4}$\n- $P(X=0, Y=0, Z=1) = P(Z=1 \\mid X=0, Y=0) P(X=0, Y=0) = 0 \\times \\frac{1}{4} = 0$\n- $P(X=0, Y=1, Z=0) = P(Z=0 \\mid X=0, Y=1) P(X=0, Y=1) = 0 \\times \\frac{1}{4} = 0$\n- $P(X=0, Y=1, Z=1) = P(Z=1 \\mid X=0, Y=1) P(X=0, Y=1) = 1 \\times \\frac{1}{4} = \\frac{1}{4}$\n- $P(X=1, Y=0, Z=0) = P(Z=0 \\mid X=1, Y=0) P(X=1, Y=0) = 0 \\times \\frac{1}{4} = 0$\n- $P(X=1, Y=0, Z=1) = P(Z=1 \\mid X=1, Y=0) P(X=1, Y=0) = 1 \\times \\frac{1}{4} = \\frac{1}{4}$\n- $P(X=1, Y=1, Z=0) = P(Z=0 \\mid X=1, Y=1) P(X=1, Y=1) = 1 \\times \\frac{1}{4} = \\frac{1}{4}$\n- $P(X=1, Y=1, Z=1) = P(Z=1 \\mid X=1, Y=1) P(X=1, Y=1) = 0 \\times \\frac{1}{4} = 0$\n\nIn summary, the joint PMF is $P(X=x, Y=y, Z=z) = \\frac{1}{4}$ if $z = x \\oplus y$, and $0$ otherwise.\n\n**2. Verification of pairwise independence**\n\nTo verify pairwise independence, we first need the marginal PMFs of $X$, $Y$, and $Z$.\nThe marginals for $X$ and $Y$ are given: $P(X=x) = \\frac{1}{2}$ and $P(Y=y) = \\frac{1}{2}$ for $x, y \\in \\{0, 1\\}$.\nThe marginal PMF for $Z$ is calculated by summing the joint PMF $P(X,Y,Z)$ over all possible values of $X$ and $Y$:\n$$P(Z=1) = \\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y, Z=1)$$\n$$P(Z=1) = P(0,0,1) + P(0,1,1) + P(1,0,1) + P(1,1,1) = 0 + \\frac{1}{4} + \\frac{1}{4} + 0 = \\frac{2}{4} = \\frac{1}{2}$$\n$$P(Z=0) = 1 - P(Z=1) = 1 - \\frac{1}{2} = \\frac{1}{2}$$\nSo, $Z$ also follows a Bernoulli distribution with parameter $\\frac{1}{2}$.\n\n- **Independence of $(X, Y)$**: This is given in the problem statement. For completeness, we verify $P(X=x, Y=y) = P(X=x)P(Y=y)$. We have $P(X=x, Y=y) = \\frac{1}{4}$ and $P(X=x)P(Y=y) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. The condition holds.\n\n- **Independence of $(X, Z)$**: We must show that $P(X=x, Z=z) = P(X=x)P(Z=z)$ for all $x, z \\in \\{0, 1\\}$. The product of the marginals is $P(X=x)P(Z=z) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$. We compute the joint PMF of $(X, Z)$ by marginalizing out $Y$:\n$$P(X=x, Z=z) = \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y, Z=z)$$\n- $P(X=0, Z=0) = P(0,0,0) + P(0,1,0) = \\frac{1}{4} + 0 = \\frac{1}{4}$. This equals $P(X=0)P(Z=0) = \\frac{1}{4}$.\n- $P(X=0, Z=1) = P(0,0,1) + P(0,1,1) = 0 + \\frac{1}{4} = \\frac{1}{4}$. This equals $P(X=0)P(Z=1) = \\frac{1}{4}$.\n- $P(X=1, Z=0) = P(1,0,0) + P(1,1,0) = 0 + \\frac{1}{4} = \\frac{1}{4}$. This equals $P(X=1)P(Z=0) = \\frac{1}{4}$.\n- $P(X=1, Z=1) = P(1,0,1) + P(1,1,1) = \\frac{1}{4} + 0 = \\frac{1}{4}$. This equals $P(X=1)P(Z=1) = \\frac{1}{4}$.\nSince the condition holds for all pairs $(x, z)$, $X$ and $Z$ are independent.\n\n- **Independence of $(Y, Z)$**: By symmetry, the argument is identical to that for $(X, Z)$. We must show $P(Y=y, Z=z) = P(Y=y)P(Z=z) = \\frac{1}{4}$. We compute the joint PMF of $(Y,Z)$ by marginalizing out $X$:\n$$P(Y=y, Z=z) = \\sum_{x \\in \\{0,1\\}} P(X=x, Y=y, Z=z)$$\n- $P(Y=0, Z=0) = P(0,0,0) + P(1,0,0) = \\frac{1}{4} + 0 = \\frac{1}{4}$. This equals $P(Y=0)P(Z=0) = \\frac{1}{4}$.\n- $P(Y=0, Z=1) = P(0,0,1) + P(1,0,1) = 0 + \\frac{1}{4} = \\frac{1}{4}$. This equals $P(Y=0)P(Z=1) = \\frac{1}{4}$.\n- $P(Y=1, Z=0) = P(0,1,0) + P(1,1,0) = 0 + \\frac{1}{4} = \\frac{1}{4}$. This equals $P(Y=1)P(Z=0) = \\frac{1}{4}$.\n- $P(Y=1, Z=1) = P(0,1,1) + P(1,1,1) = \\frac{1}{4} + 0 = \\frac{1}{4}$. This equals $P(Y=1)P(Z=1) = \\frac{1}{4}$.\nSince the condition holds for all pairs $(y, z)$, $Y$ and $Z$ are independent.\n\nThus, all three pairs $(X,Y)$, $(X,Z)$, and $(Y,Z)$ are independent.\n\n**3. Demonstration of lack of mutual independence**\n\nThree random variables $(X,Y,Z)$ are mutually independent if and only if $P(X=x, Y=y, Z=z) = P(X=x)P(Y=y)P(Z=z)$ for all possible values $(x,y,z)$. We need only find one counterexample to show they are not mutually independent.\nLet's examine the case $(X=1, Y=1, Z=1)$.\nFrom the joint PMF derived in part 1:\n$$P(X=1, Y=1, Z=1) = 0$$\nThis is because if $X=1$ and $Y=1$, then by definition $Z = 1 \\oplus 1 = 0$, so the event $\\{Z=1\\}$ is impossible.\nNow, let's compute the product of the marginal probabilities:\n$$P(X=1)P(Y=1)P(Z=1) = \\frac{1}{2} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{8}$$\nComparing the two results:\n$$0 \\neq \\frac{1}{8}$$\n$$P(X=1, Y=1, Z=1) \\neq P(X=1)P(Y=1)P(Z=1)$$\nSince the condition for mutual independence is violated, the variables $(X,Y,Z)$ are not mutually independent, despite being pairwise independent.\n\n**4. Application of Bayes' theorem to compute $P(X=1 \\mid Z=1)$**\n\nBayes' theorem states that for two events $A$ and $B$, the conditional probability of $A$ given $B$ is:\n$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$$\nWe apply this theorem to the events $A = \\{X=1\\}$ and $B = \\{Z=1\\}$:\n$$P(X=1 \\mid Z=1) = \\frac{P(Z=1 \\mid X=1) P(X=1)}{P(Z=1)}$$\nWe will compute each term on the right-hand side from first principles.\n- $P(X=1)$: This is given as $\\frac{1}{2}$.\n- $P(Z=1)$: This was calculated in part 2 as $\\frac{1}{2}$.\n- $P(Z=1 \\mid X=1)$: This is the probability that $Z=1$ given that $X=1$. Since $Z = X \\oplus Y$, this is the probability that $1 \\oplus Y = 1$. This equation holds if and only if $Y=0$. Therefore:\n$$P(Z=1 \\mid X=1) = P(1 \\oplus Y = 1) = P(Y=0)$$\nSince $X$ and $Y$ are independent, the knowledge of $X=1$ does not alter the probability distribution of $Y$. We are given $P(Y=1)=\\frac{1}{2}$, so $P(Y=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\nThus, $P(Z=1 \\mid X=1) = \\frac{1}{2}$.\n\nSubstituting these values into Bayes' theorem:\n$$P(X=1 \\mid Z=1) = \\frac{(\\frac{1}{2}) \\times (\\frac{1}{2})}{\\frac{1}{2}} = \\frac{\\frac{1}{4}}{\\frac{1}{2}} = \\frac{1}{2}$$\nAlternatively, because we proved in part 2 that $X$ and $Z$ are independent, it follows directly from the definition of independence that $P(X=1 \\mid Z=1) = P(X=1) = \\frac{1}{2}$. Our calculation using Bayes' theorem confirms this result.\nThe posterior probability $P(X=1 \\mid Z=1)$ is $\\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "In Bayesian analysis, the posterior distribution represents our complete knowledge about a parameter after observing data. However, for reporting or decision-making, we often need a single point estimate. This exercise demonstrates how Bayesian decision theory provides a formal framework for this task, showing that common estimators like the posterior mean, median, and mode are not arbitrary choices but are in fact optimal estimates that arise from minimizing specific loss functions. ",
            "id": "3340172",
            "problem": "In a single-cell signaling experiment in computational systems biology, each cell is classified as transcriptionally active or inactive for a specific pathway, yielding a binary observation $x_{i} \\in \\{0,1\\}$ for cell $i$. Assume $x_{1},\\dots,x_{n}$ are conditionally independent and identically distributed given an unknown activation probability $\\theta \\in (0,1)$, with sampling model $x_{i} \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$. A prior belief about $\\theta$ is modeled by a Beta distribution with density proportional to $\\theta^{\\alpha_{0}-1} (1-\\theta)^{\\beta_{0}-1}$ for hyperparameters $\\alpha_{0} > 0$ and $\\beta_{0} > 0$.\n\nUsing only the definitions of the Bernoulli likelihood, the Beta prior, Bayes’ theorem for the posterior, and the definition of a Bayes estimator as the action $a$ that minimizes posterior expected loss $\\mathbb{E}[L(\\theta,a)\\,|\\,x_{1:n}]$, do the following:\n\n1. Derive from first principles the Bayes estimator $\\delta^{\\star}(x_{1:n})$ for $\\theta$ under each of the following loss functions:\n   - Quadratic loss $L_{2}(\\theta,a) = (\\theta - a)^{2}$.\n   - Absolute loss $L_{1}(\\theta,a) = |\\theta - a|$.\n   - Zero–one loss $L_{01}(\\theta,a) = \\mathbf{1}\\{|\\theta - a| > 0\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Your derivation must justify any limiting or approximation arguments needed to make the zero–one loss well-defined for a continuous parameter.\n\n2. Now specialize to the case of a uniform prior with $\\alpha_{0} = 1$ and $\\beta_{0} = 1$, and experimental data consisting of $n = 3$ cells with $s = 0$ active cells observed. Use Bayes’ theorem to find the posterior parameters $(\\alpha,\\beta)$, and then compute the values of the three Bayes estimators you derived in part $1$ for this posterior.\n\nProvide your final answer as a single row matrix with entries in the order: quadratic-loss estimator, absolute-loss estimator, zero–one-loss estimator. Do not round; provide exact values.",
            "solution": "The problem asks for the derivation of three Bayes estimators for an unknown parameter $\\theta$ and their subsequent calculation for a specific data set and prior. The process consists of two parts: a general derivation from first principles and a specific application.\n\nFirst, we establish the Bayesian framework. The data $x_{1}, \\dots, x_{n}$ are conditionally independent and identically distributed draws from a Bernoulli distribution with parameter $\\theta$, i.e., $x_i|\\theta \\sim \\mathrm{Bernoulli}(\\theta)$. The likelihood function for the entire dataset $x_{1:n} = (x_1, \\dots, x_n)$ is given by the product of individual probabilities:\n$$P(x_{1:n} | \\theta) = \\prod_{i=1}^{n} P(x_i | \\theta) = \\prod_{i=1}^{n} \\theta^{x_i} (1-\\theta)^{1-x_i} = \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i}$$\nLet $s = \\sum_{i=1}^{n} x_i$ be the total number of successes (active cells). The likelihood is then $L(\\theta; s, n) = \\theta^s (1-\\theta)^{n-s}$.\n\nThe prior belief about $\\theta$ is modeled by a Beta distribution, $\\theta \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)$, with probability density function (PDF):\n$$p(\\theta) = \\frac{\\theta^{\\alpha_0-1}(1-\\theta)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}$$\nwhere $B(\\alpha_0, \\beta_0)$ is the Beta function, which serves as a normalization constant.\n\nAccording to Bayes' theorem, the posterior distribution of $\\theta$ given the data $x_{1:n}$ is proportional to the product of the likelihood and the prior:\n$$p(\\theta|x_{1:n}) \\propto P(x_{1:n}|\\theta) p(\\theta) \\propto \\left( \\theta^s (1-\\theta)^{n-s} \\right) \\left( \\theta^{\\alpha_0-1}(1-\\theta)^{\\beta_0-1} \\right)$$\n$$p(\\theta|x_{1:n}) \\propto \\theta^{s+\\alpha_0-1} (1-\\theta)^{n-s+\\beta_0-1}$$\nThis is the kernel of a Beta distribution, so the posterior distribution is also a Beta distribution:\n$\\theta | x_{1:n} \\sim \\mathrm{Beta}(\\alpha_0+s, \\beta_0+n-s)$\nWe denote the posterior parameters as $\\alpha = \\alpha_0+s$ and $\\beta = \\beta_0+n-s$.\n\nA Bayes estimator $\\delta^{\\star}(x_{1:n})$ of $\\theta$ is an action $a$ that minimizes the posterior expected loss, $\\mathbb{E}[L(\\theta,a)\\,|\\,x_{1:n}]$.\n$$\\delta^{\\star}(x_{1:n}) = \\arg\\min_{a} \\mathbb{E}[L(\\theta,a)\\,|\\,x_{1:n}] = \\arg\\min_{a} \\int_{0}^{1} L(\\theta,a) p(\\theta|x_{1:n}) d\\theta$$\n\n**Part 1: Derivation of Bayes Estimators**\n\nWe derive the estimator for each of the three specified loss functions.\n\n1.  **Quadratic Loss: $L_{2}(\\theta, a) = (\\theta - a)^{2}$**\n    The posterior expected loss is $R(a) = \\mathbb{E}[(\\theta-a)^2 | x_{1:n}] = \\int_0^1 (\\theta-a)^2 p(\\theta|x_{1:n}) d\\theta$.\n    To find the value of $a$ that minimizes $R(a)$, we take the derivative with respect to $a$ and set it to zero.\n    $$\\frac{d R(a)}{da} = \\frac{d}{da} \\int_0^1 (\\theta^2 - 2a\\theta + a^2) p(\\theta|x_{1:n}) d\\theta$$\n    Using Leibniz's rule for differentiating under the integral sign:\n    $$\\frac{d R(a)}{da} = \\int_0^1 \\frac{\\partial}{\\partial a} (\\theta-a)^2 p(\\theta|x_{1:n}) d\\theta = \\int_0^1 -2(\\theta-a) p(\\theta|x_{1:n}) d\\theta$$\n    Setting the derivative to zero:\n    $$-2 \\int_0^1 (\\theta-a) p(\\theta|x_{1:n}) d\\theta = 0$$\n    $$\\int_0^1 \\theta p(\\theta|x_{1:n}) d\\theta - \\int_0^1 a p(\\theta|x_{1:n}) d\\theta = 0$$\n    $$a \\int_0^1 p(\\theta|x_{1:n}) d\\theta = \\int_0^1 \\theta p(\\theta|x_{1:n}) d\\theta$$\n    Since $p(\\theta|x_{1:n})$ is a PDF, $\\int_0^1 p(\\theta|x_{1:n}) d\\theta = 1$. This leaves:\n    $$a = \\int_0^1 \\theta p(\\theta|x_{1:n}) d\\theta = \\mathbb{E}[\\theta | x_{1:n}]$$\n    The second derivative is $\\frac{d^2 R(a)}{da^2} = \\int_0^1 2 p(\\theta|x_{1:n}) d\\theta = 2 > 0$, confirming this is a minimum.\n    Thus, the Bayes estimator under quadratic loss is the posterior mean.\n    $\\delta_{L_2}^{\\star}(x_{1:n}) = \\mathbb{E}[\\theta | x_{1:n}]$.\n\n2.  **Absolute Loss: $L_{1}(\\theta, a) = |\\theta - a|$**\n    The posterior expected loss is $R(a) = \\mathbb{E}[|\\theta-a| | x_{1:n}] = \\int_0^1 |\\theta-a| p(\\theta|x_{1:n}) d\\theta$.\n    We can split the integral at $a$:\n    $$R(a) = \\int_0^a (a-\\theta) p(\\theta|x_{1:n}) d\\theta + \\int_a^1 (\\theta-a) p(\\theta|x_{1:n}) d\\theta$$\n    We differentiate with respect to $a$ using Leibniz's rule:\n    $$\\frac{d R(a)}{da} = \\left( (a-a)p(a|x_{1:n}) + \\int_0^a 1 \\cdot p(\\theta|x_{1:n}) d\\theta \\right) - \\left( (a-a)p(a|x_{1:n}) - \\int_a^1 1 \\cdot p(\\theta|x_{1:n}) d\\theta \\right)$$\n    $$\\frac{d R(a)}{da} = \\int_0^a p(\\theta|x_{1:n}) d\\theta - \\int_a^1 p(\\theta|x_{1:n}) d\\theta = P(\\theta \\le a | x_{1:n}) - P(\\theta > a | x_{1:n})$$\n    Setting the derivative to zero yields:\n    $$P(\\theta \\le a | x_{1:n}) = P(\\theta > a | x_{1:n})$$\n    This equality holds if the probability mass is split equally on both sides of $a$. This is the definition of the median of the posterior distribution. Let $m$ be the posterior median. Then $P(\\theta \\le m | x_{1:n}) = 1/2$.\n    Thus, the Bayes estimator under absolute loss is the posterior median.\n    $\\delta_{L_1}^{\\star}(x_{1:n}) = \\text{median}(\\theta | x_{1:n})$.\n\n3.  **Zero–One Loss: $L_{01}(\\theta, a) = \\mathbf{1}\\{|\\theta - a| > 0\\}$**\n    For a continuous parameter $\\theta$, the loss function as stated leads to difficulties because $P(\\theta=a|x_{1:n})=0$ for any $a$. The expected loss would be $\\mathbb{E}[\\mathbf{1}\\{\\theta \\neq a\\}|x_{1:n}] = P(\\theta \\neq a|x_{1:n}) = 1$ for all $a$, which is not useful.\n    As instructed, we must use a limiting argument. Consider a modified loss function $L_{\\epsilon}(\\theta, a) = \\mathbf{1}\\{|\\theta - a| > \\epsilon\\}$ for some small $\\epsilon > 0$. We want to minimize the posterior expected loss:\n    $$R(a) = \\mathbb{E}[L_{\\epsilon}(\\theta, a) | x_{1:n}] = \\int_0^1 \\mathbf{1}\\{|\\theta - a| > \\epsilon\\} p(\\theta|x_{1:n}) d\\theta = P(|\\theta - a| > \\epsilon | x_{1:n})$$\n    Minimizing $P(|\\theta - a| > \\epsilon | x_{1:n})$ is equivalent to maximizing its complement, $P(|\\theta - a| \\le \\epsilon | x_{1:n})$.\n    $$P(|\\theta - a| \\le \\epsilon | x_{1:n}) = P(a-\\epsilon \\le \\theta \\le a+\\epsilon | x_{1:n}) = \\int_{a-\\epsilon}^{a+\\epsilon} p(\\theta|x_{1:n}) d\\theta$$\n    For a small $\\epsilon$, and assuming $p(\\theta|x_{1:n})$ is continuous at $a$, the integral can be approximated by the area of a rectangle with height $p(a|x_{1:n})$ and width $2\\epsilon$:\n    $$\\int_{a-\\epsilon}^{a+\\epsilon} p(\\theta|x_{1:n}) d\\theta \\approx p(a|x_{1:n}) \\cdot (2\\epsilon)$$\n    To maximize this approximate probability, we must choose $a$ to be the value that maximizes the posterior density function $p(\\theta|x_{1:n})$. This value is, by definition, the mode of the posterior distribution. This argument holds in the limit as $\\epsilon \\to 0$.\n    Thus, the Bayes estimator under zero-one loss for a continuous parameter is the posterior mode.\n    $\\delta_{L_{01}}^{\\star}(x_{1:n}) = \\arg\\max_{\\theta} p(\\theta|x_{1:n})$.\n\n**Part 2: Application to Specific Case**\n\nWe are given a uniform prior, which corresponds to $\\alpha_0 = 1$ and $\\beta_0 = 1$. The data consists of $n=3$ cells with $s=0$ active cells.\n\nUsing the formula for the posterior parameters derived earlier:\n- $\\alpha = s + \\alpha_0 = 0 + 1 = 1$\n- $\\beta = n - s + \\beta_0 = 3 - 0 + 1 = 4$\n\nThe posterior distribution is $\\theta | x_{1:n} \\sim \\mathrm{Beta}(1, 4)$.\nThe posterior PDF is $$p(\\theta|x_{1:n}) = \\frac{\\theta^{1-1}(1-\\theta)^{4-1}}{B(1,4)} = \\frac{(1-\\theta)^3}{B(1,4)}$$\nThe Beta function value is $$B(1,4) = \\frac{\\Gamma(1)\\Gamma(4)}{\\Gamma(1+4)} = \\frac{0! \\cdot 3!}{4!} = \\frac{1 \\cdot 6}{24} = \\frac{1}{4}$$\nSo, $p(\\theta|x_{1:n}) = 4(1-\\theta)^3$ for $\\theta \\in (0,1)$.\n\nWe now compute the three estimators for this $\\mathrm{Beta}(1, 4)$ posterior.\n\n1.  **Quadratic Loss Estimator (Posterior Mean):**\n    For a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution, the mean is $\\frac{\\alpha}{\\alpha+\\beta}$.\n    $$\\delta_{L_2}^{\\star} = \\frac{1}{1+4} = \\frac{1}{5}$$\n\n2.  **Absolute Loss Estimator (Posterior Median):**\n    We need to find the value $m$ such that $\\int_0^m p(\\theta|x_{1:n}) d\\theta = \\frac{1}{2}$.\n    $$\\int_0^m 4(1-\\theta)^3 d\\theta = 4 \\left[ -\\frac{(1-\\theta)^4}{4} \\right]_0^m = - \\left[ (1-\\theta)^4 \\right]_0^m$$\n    $$= -((1-m)^4 - (1-0)^4) = 1 - (1-m)^4$$\n    Setting this equal to $\\frac{1}{2}$:\n    $$1 - (1-m)^4 = \\frac{1}{2}$$\n    $$(1-m)^4 = \\frac{1}{2}$$\n    Since $m \\in(0,1)$, $1-m$ must be positive. Taking the positive fourth root:\n    $$1-m = \\left(\\frac{1}{2}\\right)^{1/4}$$\n    $$\\delta_{L_1}^{\\star} = m = 1 - \\left(\\frac{1}{2}\\right)^{1/4}$$\n\n3.  **Zero–One Loss Estimator (Posterior Mode):**\n    We need to find the value of $\\theta$ that maximizes $p(\\theta|x_{1:n}) = 4(1-\\theta)^3$ for $\\theta \\in (0,1)$.\n    The derivative of the density with respect to $\\theta$ is $\\frac{d}{d\\theta} 4(1-\\theta)^3 = -12(1-\\theta)^2$.\n    For $\\theta \\in (0,1)$, this derivative is always negative, which means the function $p(\\theta|x_{1:n})$ is strictly decreasing on its support. The maximum value is therefore achieved at the left boundary of the interval, i.e., at $\\theta = 0$.\n    (Note: The general formula for the mode of a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution is $\\frac{\\alpha-1}{\\alpha+\\beta-2}$ for $\\alpha, \\beta > 1$. For the case $\\alpha=1, \\beta>1$, the mode is at $0$.)\n    $$\\delta_{L_{01}}^{\\star} = 0$$\n\nThe three estimators for the given case are $\\frac{1}{5}$, $1 - (\\frac{1}{2})^{1/4}$, and $0$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{5} & 1 - \\left(\\frac{1}{2}\\right)^{\\frac{1}{4}} & 0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "The choice of a prior distribution is a defining feature of Bayesian modeling, with significant practical implications. This hands-on coding exercise explores the trade-offs between analytically convenient conjugate priors and more flexible non-conjugate priors. By implementing a Bayesian analysis of transcription count data, you will see how non-conjugacy necessitates numerical methods like quadrature and gain a practical understanding of how prior tail behavior can influence posterior conclusions, a vital consideration in data-limited biological research. ",
            "id": "3340205",
            "problem": "In computational systems biology, messenger RNA transcription counts from a homogeneous cell population during a fixed time window can be modeled as independent and identically distributed under a Poisson process. Let the per-cell transcription rate be denoted by $\\lambda$ in units of transcripts per minute, and let observed counts be modeled as $y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ for $i \\in \\{1,\\dots,N\\}$, where $N$ is the sample size and $y_i \\in \\{0,1,2,\\dots\\}$. The task is to compare how conjugate versus non-conjugate priors for $\\lambda$ influence the posterior in small-sample regimes, focusing on sensitivity to prior tail behavior.\n\nBase the analysis on Bayes' theorem, which states that the posterior density is proportional to the product of the likelihood and the prior, i.e., $p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda)\\,p(\\lambda)$, where $\\mathbf{y} = (y_1,\\dots,y_N)$ and $p(\\mathbf{y} \\mid \\lambda)$ is the product of Poisson likelihoods. Consider two priors:\n- Conjugate prior: $\\lambda \\sim \\mathrm{Gamma}(a,b)$, parameterized by shape $a>0$ and rate $b>0$.\n- Non-conjugate prior: $\\log \\lambda \\sim \\mathcal{N}(\\mu,\\sigma^2)$, that is, $\\lambda$ follows a lognormal distribution with parameters $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$.\n\nFrom first principles, derive the posterior for the conjugate prior and a computable expression for the posterior expectation under the non-conjugate prior. Then, define and compute the following sensitivity metrics for small-sample regimes:\n- For the lognormal prior, the absolute difference in posterior means when the log-scale standard deviation changes between a \"light-tail\" choice $\\sigma_{\\mathrm{light}}$ and a \"heavy-tail\" choice $\\sigma_{\\mathrm{heavy}}$, while holding the prior mean fixed at $m_0$ by adjusting $\\mu$ accordingly via $\\mu = \\log(m_0) - \\sigma^2/2$. Denote this as $\\Delta_{\\mathrm{logN}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{light}}]\\right|$.\n- For the gamma prior, the absolute difference in posterior means when the shape parameter changes between a \"light-tail\" choice $a_{\\mathrm{light}}$ and a \"heavy-tail\" choice $a_{\\mathrm{heavy}}$, while holding the prior mean fixed at $m_0$ by setting $b = a/m_0$. Denote this as $\\Delta_{\\mathrm{Gamma}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{light}}]\\right|$.\n- The ratio $R = \\Delta_{\\mathrm{logN}} / \\Delta_{\\mathrm{Gamma}}$ as a dimensionless measure of relative sensitivity.\n\nAll posterior means and differences for $\\lambda$ must be expressed in transcripts per minute. The ratio $R$ is dimensionless. Angles do not appear in this problem.\n\nImplement a program that, given the test suite below, computes $\\Delta_{\\mathrm{logN}}$, $\\Delta_{\\mathrm{Gamma}}$, and $R$ for each test case as floating-point numbers. Use numerically stable integration for the non-conjugate posterior expectation derived from Bayes' theorem. The final output must be a single line containing a list of lists, where each inner list corresponds to one test case and has the form $[\\Delta_{\\mathrm{logN}}, \\Delta_{\\mathrm{Gamma}}, R]$.\n\nTest suite (each case specifies $\\mathbf{y}$, $m_0$, $\\sigma_{\\mathrm{light}}$, $\\sigma_{\\mathrm{heavy}}$, $a_{\\mathrm{light}}$, $a_{\\mathrm{heavy}}$):\n1. Case $1$: $N=2$, $\\mathbf{y}=[0,1]$, $m_0=1.0$, $\\sigma_{\\mathrm{light}}=0.25$, $\\sigma_{\\mathrm{heavy}}=1.0$, $a_{\\mathrm{light}}=50.0$, $a_{\\mathrm{heavy}}=1.0$.\n2. Case $2$: $N=5$, $\\mathbf{y}=[0,0,1,0,2]$, $m_0=0.5$, $\\sigma_{\\mathrm{light}}=0.2$, $\\sigma_{\\mathrm{heavy}}=1.2$, $a_{\\mathrm{light}}=40.0$, $a_{\\mathrm{heavy}}=0.5$.\n3. Case $3$: $N=1$, $\\mathbf{y}=[5]$, $m_0=4.0$, $\\sigma_{\\mathrm{light}}=0.3$, $\\sigma_{\\mathrm{heavy}}=0.9$, $a_{\\mathrm{light}}=20.0$, $a_{\\mathrm{heavy}}=1.0$.\n4. Case $4$: $N=3$, $\\mathbf{y}=[0,0,0]$, $m_0=0.2$, $\\sigma_{\\mathrm{light}}=0.3$, $\\sigma_{\\mathrm{heavy}}=1.5$, $a_{\\mathrm{light}}=50.0$, $a_{\\mathrm{heavy}}=0.3$.\n5. Case $5$: $N=2$, $\\mathbf{y}=[50,60]$, $m_0=40.0$, $\\sigma_{\\mathrm{light}}=0.5$, $\\sigma_{\\mathrm{heavy}}=1.0$, $a_{\\mathrm{light}}=30.0$, $a_{\\mathrm{heavy}}=1.0$.\n\nFor each case, let $S=\\sum_{i=1}^{N} y_i$ denote the total count. Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, enclosed in square brackets, for example, $[[x_1,y_1,z_1],[x_2,y_2,z_2],\\dots]$, where $x_i$, $y_i$, and $z_i$ are the three floats for case $i$ in the units specified above.",
            "solution": "The problem requires a Bayesian analysis of a Poisson model for mRNA transcript counts. We must compare the sensitivity of the posterior mean of the transcription rate, $\\lambda$, to the choice of prior distribution. Specifically, we compare a conjugate Gamma prior to a non-conjugate Lognormal prior, focusing on how the tail behavior of each prior influences the posterior in small-sample regimes.\n\nThe analysis proceeds in two main steps. First, we derive the analytical or computable forms of the posterior means for $\\lambda$ under both prior scenarios. Second, we implement these calculations numerically to compute the specified sensitivity metrics for a given suite of test cases.\n\nLet the observed data be $\\mathbf{y} = (y_1, \\dots, y_N)$, where each $y_i \\in \\{0, 1, 2, \\dots\\}$ is an independent draw from a Poisson distribution with rate $\\lambda$.\n$$y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$$\nThe likelihood of the entire dataset $\\mathbf{y}$ is the product of the individual Poisson probability mass functions:\n$$p(\\mathbf{y} \\mid \\lambda) = \\prod_{i=1}^N \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!} = \\frac{\\lambda^{\\sum y_i} e^{-N\\lambda}}{\\prod y_i!}$$\nFor Bayesian inference, we are interested in the likelihood as a function of the parameter $\\lambda$. We can ignore terms that do not depend on $\\lambda$. Let $S = \\sum_{i=1}^N y_i$ be the sum of the counts. The likelihood is then proportional to:\n$$p(\\mathbf{y} \\mid \\lambda) \\propto \\lambda^S e^{-N\\lambda}$$\nAccording to Bayes' theorem, the posterior distribution of $\\lambda$ is $p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda) p(\\lambda)$, where $p(\\lambda)$ is the prior distribution for $\\lambda$.\n\n### Conjugate Prior Analysis: Gamma-Poisson Model\n\nThe conjugate prior for the Poisson likelihood is the Gamma distribution. Let the prior for $\\lambda$ be:\n$$\\lambda \\sim \\mathrm{Gamma}(a, b)$$\nwhere $a > 0$ is the shape parameter and $b > 0$ is the rate parameter. The probability density function (PDF) is $p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} e^{-b\\lambda}$, which is proportional to $\\lambda^{a-1} e^{-b\\lambda}$.\n\nThe posterior distribution is found by multiplying the likelihood and the prior:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto (\\lambda^S e^{-N\\lambda}) (\\lambda^{a-1} e^{-b\\lambda}) = \\lambda^{a+S-1} e^{-(b+N)\\lambda}$$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution:\n$$\\lambda \\mid \\mathbf{y} \\sim \\mathrm{Gamma}(a', b')$$\nwith posterior shape $a' = a+S$ and posterior rate $b' = b+N$.\n\nThe mean of a $\\mathrm{Gamma}(a', b')$ distribution is $\\frac{a'}{b'}$. Therefore, the posterior mean of $\\lambda$ is:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{a+S}{b+N}$$\nThe problem states that the prior mean is held fixed at a value $m_0$. The mean of the prior $\\mathrm{Gamma}(a, b)$ distribution is $\\mathbb{E}[\\lambda] = a/b$. So, we have the constraint $m_0 = a/b$, which implies $b = a/m_0$. Substituting this into the posterior mean formula gives:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{a+S}{(a/m_0) + N}$$\nThis is the expression used to calculate the posterior mean for the Gamma prior.\n\n### Non-Conjugate Prior Analysis: Lognormal-Poisson Model\n\nThe non-conjugate prior for $\\lambda$ is specified as a Lognormal distribution. This means $\\log \\lambda$ follows a Normal distribution:\n$$\\log \\lambda \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\nThe PDF for $\\lambda$ is $$p(\\lambda) = \\frac{1}{\\lambda \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right)$$, for $\\lambda > 0$.\n\nThe posterior distribution is again proportional to the product of the likelihood and the prior:\n$$p(\\lambda \\mid \\mathbf{y}) \\propto (\\lambda^S e^{-N\\lambda}) \\left( \\frac{1}{\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) \\right)$$\n$$p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{S-1} e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right)$$\nThis posterior is not a standard, named distribution. The posterior mean must be computed from its definition:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{\\int_0^\\infty \\lambda \\cdot p(\\lambda \\mid \\mathbf{y}) d\\lambda}{\\int_0^\\infty p(\\lambda \\mid \\mathbf{y}) d\\lambda}$$\nSubstituting the unnormalized posterior density, we get:\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{\\int_0^\\infty \\lambda^S e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) d\\lambda}{\\int_0^\\infty \\lambda^{S-1} e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) d\\lambda}$$\nThese integrals do not have a closed-form solution and must be computed numerically. To improve numerical stability, we perform a change of variables. Let $x = \\log \\lambda$, which implies $\\lambda = e^x$ and $d\\lambda = e^x dx$. The integration domain changes from $(0, \\infty)$ to $(-\\infty, \\infty)$. Let's denote the numerator integral as $I_{\\text{num}}$ and the denominator integral as $I_{\\text{den}}$.\n$$I_{\\text{num}} = \\int_{-\\infty}^{\\infty} (e^x)^S e^{-Ne^x} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) e^x dx = \\int_{-\\infty}^{\\infty} \\exp\\left(x(S+1) - Ne^x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\n$$I_{\\text{den}} = \\int_{-\\infty}^{\\infty} (e^x)^{S-1} e^{-Ne^x} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) e^x dx = \\int_{-\\infty}^{\\infty} \\exp\\left(xS - Ne^x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\nThe posterior mean is then $\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = I_{\\text{num}} / I_{\\text{den}}$. These integrals are well-behaved and can be reliably computed using numerical quadrature methods.\n\nThe problem constrains the prior mean to be $m_0$. The mean of a $\\mathrm{Lognormal}(\\mu, \\sigma^2)$ distribution is $\\mathbb{E}[\\lambda] = e^{\\mu + \\sigma^2/2}$. Setting this to $m_0$ and solving for $\\mu$ yields:\n$$\\mu = \\log(m_0) - \\frac{\\sigma^2}{2}$$\nThis expression for $\\mu$ is substituted into the integrands.\n\n### Computation of Sensitivity Metrics\n\nFor each test case, we compute the following quantities:\n1.  $\\Delta_{\\mathrm{logN}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{light}}]\\right|$:\n    - We calculate two posterior means using the Lognormal prior.\n    - For the \"light-tail\" case, use $\\sigma = \\sigma_{\\mathrm{light}}$ and $\\mu_{\\mathrm{light}} = \\log(m_0) - \\sigma_{\\mathrm{light}}^2/2$.\n    - For the \"heavy-tail\" case, use $\\sigma = \\sigma_{\\mathrm{heavy}}$ and $\\mu_{\\mathrm{heavy}} = \\log(m_0) - \\sigma_{\\mathrm{heavy}}^2/2$.\n    - The posterior means are found by numerical integration, and $\\Delta_{\\mathrm{logN}}$ is the absolute difference.\n\n2.  $\\Delta_{\\mathrm{Gamma}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{light}}]\\right|$:\n    - We calculate two posterior means using the Gamma prior. The variance of a Gamma prior with fixed mean $m_0$ is $m_0^2/a$. Thus, a smaller $a$ gives a heavier tail.\n    - For the \"light-tail\" case, use $a = a_{\\mathrm{light}}$ and $b_{\\mathrm{light}} = a_{\\mathrm{light}}/m_0$. The posterior mean is $\\frac{a_{\\mathrm{light}}+S}{b_{\\mathrm{light}}+N}$.\n    - For the \"heavy-tail\" case, use $a = a_{\\mathrm{heavy}}$ and $b_{\\mathrm{heavy}} = a_{\\mathrm{heavy}}/m_0$. The posterior mean is $\\frac{a_{\\mathrm{heavy}}+S}{b_{\\mathrm{heavy}}+N}$.\n    - $\\Delta_{\\mathrm{Gamma}}$ is the absolute difference between these two analytical values.\n\n3.  $R = \\Delta_{\\mathrm{logN}} / \\Delta_{\\mathrm{Gamma}}$:\n    - This is the ratio of the two sensitivity metrics, providing a dimensionless comparison.\n\nThe implementation will consist of a main loop iterating through the test cases. Within the loop, helper functions will calculate the posterior means for both prior families under their respective light- and heavy-tail parameterizations. The final metrics are then computed and stored.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef calculate_gamma_posterior_mean(S, N, a, b):\n    \"\"\"\n    Calculates the posterior mean for a Gamma-Poisson model.\n    \n    Args:\n        S (int): Sum of observed counts.\n        N (int): Number of observations.\n        a (float): Shape parameter of the Gamma prior.\n        b (float): Rate parameter of the Gamma prior.\n        \n    Returns:\n        float: The posterior mean of lambda.\n    \"\"\"\n    posterior_shape = a + S\n    posterior_rate = b + N\n    return posterior_shape / posterior_rate\n\ndef calculate_lognormal_posterior_mean(S, N, mu, sigma):\n    \"\"\"\n    Calculates the posterior mean for a Lognormal-Poisson model via numerical integration.\n    \n    Args:\n        S (int): Sum of observed counts.\n        N (int): Number of observations.\n        mu (float): Location parameter of the Lognormal prior.\n        sigma (float): Scale parameter of the Lognormal prior.\n        \n    Returns:\n        float: The posterior mean of lambda.\n    \"\"\"\n    # Numerically unstable to integrate exp(log_integrand) directly.\n    # We define the log of the integrand and integrate its exponential.\n    # The change of variables is x = log(lambda).\n    \n    # log_integrand for the numerator of the posterior mean expectation\n    def log_integrand_num(x):\n        k = S + 1\n        return k * x - N * np.exp(x) - ((x - mu)**2) / (2 * sigma**2)\n\n    # log_integrand for the denominator (normalization constant)\n    def log_integrand_den(x):\n        k = S\n        return k * x - N * np.exp(x) - ((x - mu)**2) / (2 * sigma**2)\n\n    # To avoid overflow/underflow, find the peak of the log integrand and subtract it\n    # This integration is well-behaved, so direct integration is feasible,\n    # but this is a more robust approach if needed. However, quad is robust enough\n    # for these test cases.\n    integrand_num = lambda x: np.exp(log_integrand_num(x))\n    integrand_den = lambda x: np.exp(log_integrand_den(x))\n    \n    # quad returns (integral, error)\n    integral_num, _ = quad(integrand_num, -np.inf, np.inf)\n    integral_den, _ = quad(integrand_den, -np.inf, np.inf)\n    \n    if integral_den == 0:\n        # This case is unlikely with proper priors but is a safeguard.\n        return np.nan\n        \n    return integral_num / integral_den\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Case 1: (y, m0, sigma_light, sigma_heavy, a_light, a_heavy)\n        (np.array([0, 1]), 1.0, 0.25, 1.0, 50.0, 1.0),\n        # Case 2\n        (np.array([0, 0, 1, 0, 2]), 0.5, 0.2, 1.2, 40.0, 0.5),\n        # Case 3\n        (np.array([5]), 4.0, 0.3, 0.9, 20.0, 1.0),\n        # Case 4\n        (np.array([0, 0, 0]), 0.2, 0.3, 1.5, 50.0, 0.3),\n        # Case 5\n        (np.array([50, 60]), 40.0, 0.5, 1.0, 30.0, 1.0),\n    ]\n\n    results = []\n    for y, m0, sigma_light, sigma_heavy, a_light, a_heavy in test_cases:\n        S = np.sum(y)\n        N = len(y)\n\n        # Lognormal Prior Calculation\n        mu_light = np.log(m0) - (sigma_light**2) / 2\n        mu_heavy = np.log(m0) - (sigma_heavy**2) / 2\n        \n        mean_logN_light = calculate_lognormal_posterior_mean(S, N, mu_light, sigma_light)\n        mean_logN_heavy = calculate_lognormal_posterior_mean(S, N, mu_heavy, sigma_heavy)\n        \n        delta_logN = abs(mean_logN_heavy - mean_logN_light)\n\n        # Gamma Prior Calculation\n        b_light = a_light / m0\n        b_heavy = a_heavy / m0\n        \n        mean_gamma_light = calculate_gamma_posterior_mean(S, N, a_light, b_light)\n        mean_gamma_heavy = calculate_gamma_posterior_mean(S, N, a_heavy, b_heavy)\n        \n        delta_gamma = abs(mean_gamma_heavy - mean_gamma_light)\n\n        # Ratio Calculation\n        # Avoid division by zero, though unlikely in this problem context.\n        if delta_gamma == 0:\n            ratio = np.inf if delta_logN != 0 else 0.0\n        else:\n            ratio = delta_logN / delta_gamma\n            \n        results.append([delta_logN, delta_gamma, ratio])\n\n    # Format output to a list of lists string with no spaces.\n    # Ex: [[1.0,2.0,3.0],[4.0,5.0,6.0]]\n    formatted_results = []\n    for res in results:\n        formatted_sublist = \"[\" + \",\".join(map(str, res)) + \"]\"\n        formatted_results.append(formatted_sublist)\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n\n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}