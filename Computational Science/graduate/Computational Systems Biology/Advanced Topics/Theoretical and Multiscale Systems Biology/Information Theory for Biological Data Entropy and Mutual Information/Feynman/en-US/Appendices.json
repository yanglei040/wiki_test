{
    "hands_on_practices": [
        {
            "introduction": "Many biological processes, from signal transduction to gene activation, can be simplified as binary switches that are either 'on' or 'off'. However, these signals are rarely transmitted with perfect fidelity due to inherent molecular stochasticity. This exercise models such a system as a Binary Symmetric Channel, a foundational concept in information theory, to quantify the impact of noise on information flow . By deriving the mutual information from first principles, you will gain a concrete understanding of how this metric captures the reliability of a simple biological switch.",
            "id": "3320002",
            "problem": "A binary signal transduction pathway in a cell transmits an upstream receptor state $X \\in \\{0,1\\}$ downstream to a transcriptional reporter $Y \\in \\{0,1\\}$. The receptor state $X$ represents absence ($0$) or presence ($1$) of a ligand, and in the considered environment the ligand is equally likely to be absent or present, so $P(X=0) = P(X=1) = \\frac{1}{2}$. The molecular readout is corrupted by independent flip noise $N \\sim \\mathrm{Bernoulli}(\\epsilon)$ representing stochastic mis-phosphorylation events, so the reporter follows $Y = X \\oplus N$, where $\\oplus$ denotes binary exclusive-or. Assume that $X$ and $N$ are independent.\n\nStarting from the foundational definitions of Shannon entropy and mutual information, and without invoking any pre-derived shortcuts, derive the mutual information $I(X;Y)$ between the receptor state $X$ and the reporter $Y$ for this binary symmetric relationship as a closed-form analytic expression in terms of $\\epsilon$. Express the final value in bits. The final answer must be a single analytic expression in $\\epsilon$.",
            "solution": "The problem statement is scientifically grounded, well-posed, and complete. It describes a classic Binary Symmetric Channel, a fundamental model in information theory that is aptly applied to the biological context of a noisy signal transduction pathway. All necessary parameters and relationships are defined, allowing for a unique and meaningful derivation from foundational principles. Therefore, a solution will be provided.\n\nThe objective is to derive the mutual information $I(X;Y)$ between the receptor state $X$ and the reporter state $Y$. The problem specifies that the derivation must start from the foundational definitions of Shannon entropy and mutual information, and the final result must be expressed in bits.\n\nThe mutual information $I(X;Y)$ can be defined in several equivalent ways. We will use the definition:\n$$I(X;Y) = H(Y) - H(Y|X)$$\nwhere $H(Y)$ is the entropy of the reporter state $Y$, and $H(Y|X)$ is the conditional entropy of $Y$ given the receptor state $X$. The base of the logarithm for entropy calculations will be $2$, as the result is required in bits.\n\nFirst, we calculate the conditional entropy $H(Y|X)$. By definition:\n$$H(Y|X) = -\\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y) \\log_2(P(Y=y|X=x))$$\nThis can be rewritten as:\n$$H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) \\left( -\\sum_{y \\in \\{0,1\\}} P(Y=y|X=x) \\log_2(P(Y=y|X=x)) \\right)$$\n$$H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) H(Y|X=x)$$\n\nWe need to determine the conditional probabilities $P(Y|X)$. The problem states $Y = X \\oplus N$, where $N \\sim \\mathrm{Bernoulli}(\\epsilon)$. This means $P(N=1) = \\epsilon$ and $P(N=0) = 1-\\epsilon$. The variables $X$ and $N$ are independent. The symbol $\\oplus$ denotes addition modulo $2$ (XOR).\n\nCase 1: $X=0$.\nThe output is $Y = 0 \\oplus N = N$.\n- $P(Y=0|X=0) = P(N=0) = 1-\\epsilon$\n- $P(Y=1|X=0) = P(N=1) = \\epsilon$\n\nCase 2: $X=1$.\nThe output is $Y = 1 \\oplus N$.\n- $Y=0$ if $N=1$. Thus, $P(Y=0|X=1) = P(N=1) = \\epsilon$.\n- $Y=1$ if $N=0$. Thus, $P(Y=1|X=1) = P(N=0) = 1-\\epsilon$.\n\nNow we can compute the conditional entropies $H(Y|X=x)$:\nFor $X=0$:\n$$H(Y|X=0) = -\\left( P(Y=0|X=0)\\log_2(P(Y=0|X=0)) + P(Y=1|X=0)\\log_2(P(Y=1|X=0)) \\right)$$\n$$H(Y|X=0) = -\\left( (1-\\epsilon)\\log_2(1-\\epsilon) + \\epsilon\\log_2(\\epsilon) \\right)$$\n\nFor $X=1$:\n$$H(Y|X=1) = -\\left( P(Y=0|X=1)\\log_2(P(Y=0|X=1)) + P(Y=1|X=1)\\log_2(P(Y=1|X=1)) \\right)$$\n$$H(Y|X=1) = -\\left( \\epsilon\\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon) \\right)$$\n\nBoth conditional entropies are identical. Let's denote this quantity as the binary entropy function, $H_b(\\epsilon)$:\n$$H_b(\\epsilon) = -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon))$$\nNote that by convention, $\\lim_{p \\to 0^+} p \\log_b(p) = 0$.\n\nNow, we compute the total conditional entropy $H(Y|X)$ using the given prior probabilities $P(X=0) = P(X=1) = \\frac{1}{2}$:\n$$H(Y|X) = P(X=0)H(Y|X=0) + P(X=1)H(Y|X=1)$$\n$$H(Y|X) = \\frac{1}{2} H_b(\\epsilon) + \\frac{1}{2} H_b(\\epsilon) = H_b(\\epsilon)$$\n$$H(Y|X) = -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon))$$\n\nNext, we calculate the entropy of the output, $H(Y)$. The definition is:\n$$H(Y) = -\\sum_{y \\in \\{0,1\\}} P(Y=y) \\log_2(P(Y=y))$$\nWe must first find the marginal probability distribution of $Y$. We use the law of total probability:\n$$P(Y=y) = \\sum_{x \\in \\{0,1\\}} P(Y=y|X=x) P(X=x)$$\n\nFor $Y=0$:\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1)$$\n$$P(Y=0) = (1-\\epsilon) \\cdot \\frac{1}{2} + \\epsilon \\cdot \\frac{1}{2} = \\frac{1-\\epsilon+\\epsilon}{2} = \\frac{1}{2}$$\n\nFor $Y=1$:\n$$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1)$$\n$$P(Y=1) = \\epsilon \\cdot \\frac{1}{2} + (1-\\epsilon) \\cdot \\frac{1}{2} = \\frac{\\epsilon+1-\\epsilon}{2} = \\frac{1}{2}$$\n\nThe output distribution is uniform, $P(Y=0) = P(Y=1) = \\frac{1}{2}$. Now we compute $H(Y)$:\n$$H(Y) = -\\left( P(Y=0)\\log_2(P(Y=0)) + P(Y=1)\\log_2(P(Y=1)) \\right)$$\n$$H(Y) = -\\left( \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) \\right)$$\n$$H(Y) = -\\log_2\\left(\\frac{1}{2}\\right) = -(-\\log_2(2)) = \\log_2(2) = 1 \\text{ bit}$$\n\nFinally, we substitute our results for $H(Y)$ and $H(Y|X)$ into the definition of mutual information:\n$$I(X;Y) = H(Y) - H(Y|X)$$\n$$I(X;Y) = 1 - \\left( -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)) \\right)$$\n$$I(X;Y) = 1 + \\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)$$\n\nThis expression represents the mutual information between the receptor state and the reporter state in bits, as a function of the noise probability $\\epsilon$.",
            "answer": "$$ \\boxed{1 + \\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)} $$"
        },
        {
            "introduction": "While binary models are useful, much of biological data is continuous, such as the fluorescence intensity from a reporter gene or protein concentrations. This practice transitions from the discrete to the continuous domain, modeling a realistic biochemical assay where the true biological signal is obscured by measurement noise . Deriving the mutual information for this Gaussian channel will reveal a fundamental and elegant relationship between the amount of information transferred and the system's Signal-to-Noise Ratio ($SNR$), providing a powerful way to assess the quality of experimental data.",
            "id": "3320034",
            "problem": "A single-cell biochemical reporter assay tracks the activity of a transcription factor by measuring fluorescence intensity after a log transformation and standardization across replicates. Let the underlying standardized biological signal across cells be modeled as a continuous random variable $X$ with $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$, representing cell-to-cell variability in true reporter activity. Let the measurement noise be modeled as an independent continuous random variable $N$ with $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$, capturing instrument noise and background. The observed measurement is $Y = X + N$.\n\nUsing only fundamental definitions of differential entropy and mutual information from information theory, derive a closed-form expression for the mutual information $I(X;Y)$ between the biological signal $X$ and the observation $Y$, expressed in natural units (nats). Then evaluate the mutual information for the parameter values $\\sigma_{X}^{2} = 2.0$ and $\\sigma_{N}^{2} = 0.5$, and provide the final numerical value in nats.\n\nAdditionally, interpret the Signal-to-Noise Ratio (SNR) as the ratio $\\sigma_{X}^{2}/\\sigma_{N}^{2}$ in the context of this assay, explaining how it controls the information transfer from $X$ to $Y$.\n\nExpress your final numerical answer for $I(X;Y)$ in nats and round to $4$ significant figures.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard model from signal processing and information theory applied to a realistic biological context. All necessary information is provided for a unique solution to be derived from fundamental principles.\n\nThe mutual information $I(X;Y)$ between two continuous random variables $X$ and $Y$ is defined in terms of their differential entropies. In natural units (nats), the definition is:\n$$I(X;Y) = h(Y) - h(Y|X)$$\nwhere $h(Y)$ is the differential entropy of the random variable $Y$, and $h(Y|X)$ is the conditional differential entropy of $Y$ given $X$. We must derive expressions for both terms.\n\nFirst, we determine the distribution of the observed measurement $Y$. The problem states that $Y = X + N$, where $X$ and $N$ are independent random variables.\nThe signal $X$ is distributed as a Gaussian, $X \\sim \\mathcal{N}(0, \\sigma_{X}^{2})$.\nThe noise $N$ is also distributed as a Gaussian, $N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$.\nThe sum of two independent Gaussian random variables is also a Gaussian random variable. The mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances.\nThe mean of $Y$ is:\n$$E[Y] = E[X + N] = E[X] + E[N] = 0 + 0 = 0$$\nThe variance of $Y$ is:\n$$\\text{Var}(Y) = \\text{Var}(X + N) = \\text{Var}(X) + \\text{Var}(N) = \\sigma_{X}^{2} + \\sigma_{N}^{2}$$\nTherefore, the distribution of the observation $Y$ is $Y \\sim \\mathcal{N}(0, \\sigma_{X}^{2} + \\sigma_{N}^{2})$.\n\nThe differential entropy $h(Z)$ for a continuous random variable $Z$ with a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is given by the formula:\n$$h(Z) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)$$\nApplying this formula to $Y$, we find its differential entropy:\n$$h(Y) = \\frac{1}{2} \\ln(2\\pi e (\\sigma_{X}^{2} + \\sigma_{N}^{2}))$$\n\nNext, we calculate the conditional differential entropy $h(Y|X)$. This is the expected value of the entropy of $Y$ conditioned on a specific value of $X$.\n$$h(Y|X) = \\int p(x) h(Y|X=x) \\,dx$$\nWhen $X$ is known to have a specific value $x$, the observation $Y$ is given by $Y = x + N$. Since $x$ is a constant in this context, the only source of randomness in $Y$ is the noise $N$. The distribution of $Y$ given $X=x$ is therefore a Gaussian distribution with mean $E[x+N] = x+E[N] = x$ and variance $\\text{Var}(x+N) = \\text{Var}(N) = \\sigma_{N}^{2}$. Thus, $Y|X=x \\sim \\mathcal{N}(x, \\sigma_{N}^{2})$.\nThe differential entropy of this conditional distribution is:\n$$h(Y|X=x) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})$$\nCrucially, this expression for the conditional entropy does not depend on the specific value $x$. Therefore, its expectation over all possible values of $x$ is simply the expression itself:\n$$h(Y|X) = E_X[h(Y|X=x)] = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})$$\n\nNow we can compute the mutual information $I(X;Y)$ by substituting the expressions for $h(Y)$ and $h(Y|X)$ into the fundamental definition:\n$$I(X;Y) = h(Y) - h(Y|X) = \\frac{1}{2} \\ln(2\\pi e (\\sigma_{X}^{2} + \\sigma_{N}^{2})) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})$$\nUsing the logarithmic identity $\\ln(a) - \\ln(b) = \\ln(a/b)$, we simplify the expression:\n$$I(X;Y) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{2\\pi e (\\sigma_{X}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right) \\right]$$\n$$I(X;Y) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)$$\n$$I(X;Y) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N}^{2}}\\right)$$\nThis is the closed-form expression for the mutual information between the signal $X$ and the observation $Y$ in nats.\n\nThe problem defines the Signal-to-Noise Ratio (SNR) as the ratio of signal variance to noise variance: $\\text{SNR} = \\sigma_{X}^{2}/\\sigma_{N}^{2}$. Substituting this into our derived expression yields:\n$$I(X;Y) = \\frac{1}{2} \\ln(1 + \\text{SNR})$$\nThis equation provides a direct interpretation of how the SNR controls information transfer. The mutual information $I(X;Y)$ quantifies the reduction in uncertainty about the biological signal $X$ gained by making the measurement $Y$.\nIf $\\text{SNR} \\to 0$ (i.e., $\\sigma_{N}^{2} \\gg \\sigma_{X}^{2}$), then $I(X;Y) \\to \\frac{1}{2} \\ln(1) = 0$. This signifies that the noise overwhelms the signal, and the measurement provides virtually no information about the true biological state.\nIf $\\text{SNR} \\to \\infty$ (i.e., $\\sigma_{N}^{2} \\to 0$), then $I(X;Y) \\to \\infty$. This represents a noiseless channel where the measurement $Y$ perfectly reflects the signal $X$, allowing for an arbitrarily large amount of information to be conveyed. Thus, the SNR is the critical parameter that governs the capacity of the measurement system to resolve the underlying biological variability.\n\nFinally, we evaluate the mutual information for the given parameter values: $\\sigma_{X}^{2} = 2.0$ and $\\sigma_{N}^{2} = 0.5$.\nFirst, calculate the SNR:\n$$\\text{SNR} = \\frac{\\sigma_{X}^{2}}{\\sigma_{N}^{2}} = \\frac{2.0}{0.5} = 4.0$$\nNow, substitute this value into the expression for mutual information:\n$$I(X;Y) = \\frac{1}{2} \\ln(1 + 4.0) = \\frac{1}{2} \\ln(5)$$\nUsing the natural logarithm:\n$$I(X;Y) \\approx \\frac{1}{2} \\times 1.6094379... \\approx 0.80471895...$$\nRounding to $4$ significant figures, the mutual information is $0.8047$ nats.",
            "answer": "$$\\boxed{0.8047}$$"
        },
        {
            "introduction": "The theoretical definitions of mutual information are essential, but how do we compute it from a finite set of experimental measurements, especially when we don't know the underlying probability distributions? This advanced practice introduces a powerful and widely used non-parametric method, the Kraskov-Stögbauer-Grassberger (KSG) estimator, which uses nearest-neighbor distances to estimate mutual information directly from data . Working through this problem provides a vital computational skill for inferring relationships in 'omics' data and highlights the practical advantages of rank-based methods in modern systems biology.",
            "id": "3320085",
            "problem": "Consider two real-valued random variables $X$ and $Y$ representing normalized gene expression levels of two genes measured across $N$ biological samples. Mutual information $I(X;Y)$ for continuous variables is defined from first principles via differential entropies as $I(X;Y) = H(X) + H(Y) - H(X,Y)$, where $H(Z) = -\\int f_Z(z) \\ln f_Z(z) \\, dz$ and $f_Z$ denotes the probability density function of $Z$. A widely used nonparametric approach in computational systems biology for estimating $I(X;Y)$ from finite samples is based on $k$-nearest neighbor counts and distances.\n\nYour tasks are:\n- Starting from the definitions above and the principle of local density estimation with $k$-nearest neighbors, present the Kraskov–Stögbauer–Grassberger (KSG) estimator for mutual information $I(X;Y)$ and derive its core formula that replaces explicit local radius terms with shared-neighborhood marginal counts. Explicitly justify how the estimator arises from coupling the joint and marginal neighborhoods under the same joint $k$-ball radius.\n- Using the derived estimator, compute the mutual information estimate for the following dataset of $N = 6$ paired observations:\n$(x_1,y_1) = (0.05, 0.05)$, $(x_2,y_2) = (0.31, 0.31)$, $(x_3,y_3) = (0.90, 0.90)$, $(x_4,y_4) = (1.57, 1.57)$, $(x_5,y_5) = (2.33, 2.33)$, $(x_6,y_6) = (3.80, 3.80)$.\nAdopt the following conventions for the estimator: use the maximum norm in the joint space, that is the $\\ell_{\\infty}$ norm with distance $d_{\\infty}((x_i,y_i),(x_j,y_j)) = \\max\\{|x_j - x_i|, |y_j - y_i|\\}$; let $k = 1$; define for each index $i$ the joint radius $\\epsilon_i$ as the distance to the $k$th nearest neighbor in the joint space; define the marginal counts $n_x(i)$ and $n_y(i)$ as the number of samples $j \\neq i$ such that $|x_j - x_i|  \\epsilon_i$ and $|y_j - y_i|  \\epsilon_i$, respectively (strict inequality); and use the natural logarithm so the result is expressed in nats.\n- Briefly discuss, in conceptual terms, why the estimator is robust to strictly monotonic transformations commonly applied in omics data preprocessing (for example, $y \\mapsto g(y)$ with $g$ strictly increasing), and note any caveats that might arise in practice.\n\nRound your final numerical estimate to $4$ significant figures. Express the mutual information in nats.",
            "solution": "The problem requires a three-part response: the derivation of the Kraskov–Stögbauer–Grassberger (KSG) estimator for mutual information, its application to a specific dataset, and a conceptual discussion of its robustness to monotonic transformations.\n\n### Part 1: Derivation of the KSG Estimator\n\nThe mutual information $I(X;Y)$ between two continuous random variables $X$ and $Y$ is defined in terms of their differential entropies as:\n$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\nwhere $H(Z) = -\\int f_Z(z) \\ln f_Z(z) \\, dz$ is the differential entropy of a random variable $Z$ with probability density function (PDF) $f_Z(z)$.\n\nA direct estimation of this quantity from a finite sample set $\\{(x_i, y_i)\\}_{i=1}^N$ is challenging due to the difficulty of estimating the PDFs $f_X$, $f_Y$, and $f_{XY}$. The KSG method provides an elegant solution by estimating the entropies using $k$-nearest neighbor (k-NN) distances in a way that causes systematic biases to cancel.\n\nThe Kozachenko-Leonenko (KL) estimator for the differential entropy of a $d$-dimensional random variable $Z$ from $N$ samples is given by:\n$$ \\hat{H}(Z) = \\psi(N) - \\psi(k) + \\ln(c_d) + \\frac{d}{N} \\sum_{i=1}^N \\ln(\\rho_i(k)) $$\nwhere $\\psi(\\cdot)$ is the digamma function, $k$ is the number of neighbors considered, $\\rho_i(k)$ is the distance from sample $z_i$ to its $k$-th nearest neighbor, and $c_d$ is the volume of the $d$-dimensional unit ball for the chosen norm.\n\nA naive application of the KL estimator to each term in the MI definition would involve finding k-NN distances in three different spaces (the $1$-D space of $X$, the $1$-D space of $Y$, and the $2$-D joint space of $(X,Y)$), leading to three different sets of distances. This introduces biases that do not cancel.\n\nThe core insight of the KSG estimator is to couple the estimation process by using a single distance scale for each point, determined in the joint space. Specifically, for each point $z_i=(x_i, y_i)$:\n1. We determine the distance $\\epsilon_i$ to its $k$-th nearest neighbor in the joint $(X,Y)$ space using a chosen norm (here, the maximum norm, $\\ell_{\\infty}$). This defines a local region (a square of side $2\\epsilon_i$) around $(x_i, y_i)$ which contains exactly $k$ other points.\n2. Instead of finding k-NN in the marginal spaces, we project this joint-space region onto the marginal axes. This defines intervals of length $2\\epsilon_i$ around $x_i$ and $y_i$.\n3. We then count the number of points whose marginal coordinates fall within these intervals. Let $n_x(i)$ be the number of points $j \\neq i$ such that $|x_j - x_i|  \\epsilon_i$, and $n_y(i)$ be the number of points $j \\neq i$ such that $|y_j - y_i|  \\epsilon_i$.\n\nThe entropy estimators are adapted for this scheme. The joint entropy estimator remains the standard KL form, as we fix $k$ neighbors in this space:\n$$ \\hat{H}(X,Y) = \\psi(N) - \\psi(k) + \\ln(c_2) + \\frac{2}{N} \\sum_{i=1}^N \\ln(\\epsilon_i) $$\nwhere $d=2$.\n\nFor the marginal entropies, the number of neighbors is no longer fixed at $k$ but varies for each point $i$ as $n_x(i)$ and $n_y(i)$. The entropy estimator for a variable number of neighbors becomes an average over the digamma of the neighbor counts. The total number of points in the marginal ball of radius $\\epsilon_i$ around $x_i$ is $n_x(i)+1$ (including point $i$ itself). The estimators for the marginal entropies are:\n$$ \\hat{H}(X) \\approx \\psi(N) - \\frac{1}{N}\\sum_{i=1}^N \\psi(n_x(i)+1) + \\ln(c_1) + \\frac{1}{N}\\sum_{i=1}^N \\ln(\\epsilon_i) $$\n$$ \\hat{H}(Y) \\approx \\psi(N) - \\frac{1}{N}\\sum_{i=1}^N \\psi(n_y(i)+1) + \\ln(c_1) + \\frac{1}{N}\\sum_{i=1}^N \\ln(\\epsilon_i) $$\nHere, the radius $\\epsilon_i$ is the same one determined in the joint space.\n\nCombining these into the MI formula $\\hat{I}(X;Y) = \\hat{H}(X) + \\hat{H}(Y) - \\hat{H}(X,Y)$:\n$$ \\hat{I}(X;Y) = \\left( \\psi(N) - \\langle\\psi(n_x+1)\\rangle \\!+\\! \\ln(c_1) \\!+\\! \\langle\\ln\\epsilon\\rangle \\right) \\!+\\! \\left( \\psi(N) - \\langle\\psi(n_y+1)\\rangle \\!+\\! \\ln(c_1) \\!+\\! \\langle\\ln\\epsilon\\rangle \\right) \\!-\\! \\left( \\psi(N) - \\psi(k) \\!+\\! \\ln(c_2) \\!+\\! 2\\langle\\ln\\epsilon\\rangle \\right) $$\nwhere $\\langle \\cdot \\rangle$ denotes the average over all $N$ points.\n\nThe terms involving the average logarithm of the radius, $\\langle\\ln\\epsilon\\rangle$, cancel out: $\\langle\\ln\\epsilon\\rangle + \\langle\\ln\\epsilon\\rangle - 2\\langle\\ln\\epsilon\\rangle = 0$.\nThe terms involving the unit ball volumes also cancel. For the maximum norm, the volume of a $d$-dimensional unit ball is $c_d = 2^d$. Thus, $c_1=2$ and $c_2=4$, so $\\ln(c_2) = \\ln(4) = 2\\ln(2) = 2\\ln(c_1)$. The volume terms cancel as well: $\\ln(c_1) + \\ln(c_1) - \\ln(c_2) = 0$.\n\nThe final expression for the KSG mutual information estimator is remarkably simple, as it depends only on the neighbor counts and is free of explicit metric-dependent terms:\n$$ \\hat{I}(X;Y) = \\psi(N) + \\psi(k) - \\frac{1}{N} \\sum_{i=1}^N \\left[ \\psi(n_x(i)+1) + \\psi(n_y(i)+1) \\right] $$\nThis formula replaces explicit estimations of local radii with local neighbor counts, which is the core of the KSG method's robustness.\n\n### Part 2: Calculation for the Given Dataset\n\nWe are given $N=6$ data points, and we must use $k=1$ and the maximum ($\\ell_{\\infty}$) norm.\nThe data points are: $P_1(0.05, 0.05)$, $P_2(0.31, 0.31)$, $P_3(0.90, 0.90)$, $P_4(1.57, 1.57)$, $P_5(2.33, 2.33)$, $P_6(3.80, 3.80)$.\nThe data fall on the line $y=x$. Consequently, the maximum norm distance simplifies to $d_{\\infty}(P_i, P_j) = \\max\\{|x_j - x_i|, |y_j - y_i|\\} = |x_j - x_i|$.\n\nFirst, we find the distance to the $k=1$ nearest neighbor, $\\epsilon_i$, for each point $i$.\n- For $P_1(0.05, 0.05)$: The nearest point is $P_2$. $\\epsilon_1 = |0.31 - 0.05| = 0.26$.\n- For $P_2(0.31, 0.31)$: The nearest point is $P_1$. $\\epsilon_2 = |0.05 - 0.31| = 0.26$.\n- For $P_3(0.90, 0.90)$: The nearest point is $P_2$. $\\epsilon_3 = |0.31 - 0.90| = 0.59$.\n- For $P_4(1.57, 1.57)$: The nearest point is $P_3$. $\\epsilon_4 = |0.90 - 1.57| = 0.67$.\n- For $P_5(2.33, 2.33)$: The nearest point is $P_4$. $\\epsilon_5 = |1.57 - 2.33| = 0.76$.\n- For $P_6(3.80, 3.80)$: The nearest point is $P_5$. $\\epsilon_6 = |2.33 - 3.80| = 1.47$.\n\nNext, we calculate the marginal counts $n_x(i)$ and $n_y(i)$. Since $y_j=x_j$ for all $j$, it is clear that $n_x(i) = n_y(i)$ for all $i$. We need to find the number of points $j \\neq i$ such that $|x_j - x_i|  \\epsilon_i$.\n- For $i=1$: $\\epsilon_1 = 0.26$. The closest point $P_2$ is at distance $|x_2 - x_1| = 0.26$. Since the inequality is strict ($)$, there are no points satisfying the condition. Thus, $n_x(1)=0$.\n- For $i=2$: $\\epsilon_2 = 0.26$. The closest point $P_1$ is at distance $|x_1 - x_2| = 0.26$. Again, due to strict inequality, $n_x(2)=0$.\n- For all other points $i=3, 4, 5, 6$, the $k=1$ nearest neighbor is the adjacent point in the sorted list, and its distance is exactly $\\epsilon_i$. There are no other points strictly closer than the nearest neighbor. Therefore, for all $i \\in \\{1, 2, 3, 4, 5, 6\\}$, we have $n_x(i)=0$ and $n_y(i)=0$.\n\nNow we substitute these values into the estimator formula:\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - \\frac{1}{6} \\sum_{i=1}^6 \\left[ \\psi(0+1) + \\psi(0+1) \\right] $$\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - \\frac{1}{6} \\sum_{i=1}^6 \\left[ 2\\psi(1) \\right] $$\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - \\frac{1}{6} (6 \\times 2\\psi(1)) $$\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - 2\\psi(1) = \\psi(6) - \\psi(1) $$\n\nThe digamma function is related to the Euler-Mascheroni constant $\\gamma$ and the harmonic numbers $H_n$. Specifically, $\\psi(1) = -\\gamma$ and for integers $m > 1$, $\\psi(m) = H_{m-1} - \\gamma$.\n$H_5 = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} = \\frac{60+30+20+15+12}{60} = \\frac{137}{60}$.\nSo, $\\psi(6) = H_5 - \\gamma = \\frac{137}{60} - \\gamma$.\n\nSubstituting these into our expression for $\\hat{I}(X;Y)$:\n$$ \\hat{I}(X;Y) = \\left(\\frac{137}{60} - \\gamma\\right) - (-\\gamma) = \\frac{137}{60} $$\nAs a decimal, $\\frac{137}{60} \\approx 2.28333...$. Rounding to 4 significant figures, the mutual information estimate is $2.283$ nats.\n\n### Part 3: Discussion on Robustness to Monotonic Transformations\n\nThe mutual information $I(X;Y)$ is, by definition, invariant under separate, strictly monotonic (and thus invertible) transformations of the variables, i.e., $I(g(X); h(Y)) = I(X;Y)$ for strictly monotonic functions $g$ and $h$. This property is crucial for analyzing biological data, where measurements are often subject to nonlinear but monotonic transformations during experimental procedures or preprocessing (e.g., logarithmic scaling, quantile normalization).\n\nThe KSG estimator is considered robust to such transformations due to its rank-based, metric-free formulation. The derivation above shows that the final formula for $\\hat{I}(X;Y)$ does not depend on the absolute distances $\\epsilon_i$ or the specific metric choice (as the volume terms $c_d$ cancel). It depends only on the integer neighbor counts $k$, $N$, $n_x(i)$, and $n_y(i)$. These counts are determined by the local rank-ordering of points. A monotonic transformation warps the space but preserves the one-dimensional ordering of data along each axis. For large sample sizes where the data is dense, this warping has a smooth effect on local neighborhoods, and the rank structure of neighbors tends to be preserved, leading to a stable MI estimate. This is a significant advantage over methods based on binning (histograms), which are highly sensitive to how data is scaled and transformed relative to bin boundaries.\n\nHowever, there are important caveats to this robustness, particularly for finite samples:\n1.  **Non-invariance in Practice**: The KSG estimator is generally *not* strictly invariant under such transformations for finite $N$. A nonlinear transformation $x' = g(x)$ can change the relative distances in the joint space. A point that was the $k$-th nearest neighbor in the $(X,Y)$ space may no longer be the $k$-th nearest neighbor in the $(X',Y)$ space. This changes the identity of the neighbor defining the radius $\\epsilon_i$, which in turn can alter the counts $n_x(i)$ and $n_y(i)$, leading to a different MI estimate.\n2.  **Dependence on Norm**: The degree of invariance also depends on the norm used. For example, under the maximum norm, the estimator is invariant to independent affine transformations $x \\to ax+b$, $y \\to cy+d$ only if the scaling factors are equal, i.e., $|a|=|c|$. Different scaling factors on each axis will alter the geometry of the nearest-neighbor search.\n3.  **Finite-Sample Effects**: The robustness is an asymptotic property that holds better for large $N$. For small and sparse datasets, like the one in this problem, changes in the identity of the few nearest neighbors can have a more pronounced effect on the final estimate.\n\nIn summary, the KSG estimator's robustness stems from its formulation that cancels out metric-dependent terms, relying instead on local neighbor counts. While not strictly invariant for finite samples, this design makes it far more stable against monotonic data transformations than many alternative estimators, a highly desirable feature in computational biology.",
            "answer": "$$\\boxed{2.283}$$"
        }
    ]
}