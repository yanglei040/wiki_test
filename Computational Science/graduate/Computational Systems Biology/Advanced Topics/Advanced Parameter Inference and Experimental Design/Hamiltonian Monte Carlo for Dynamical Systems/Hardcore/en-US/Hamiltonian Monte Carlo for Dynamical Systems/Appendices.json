{
    "hands_on_practices": [
        {
            "introduction": "To build a deep understanding of Hamiltonian Monte Carlo, it is invaluable to first analyze a case where its dynamics can be solved exactly. This exercise examines HMC applied to a Gaussian posterior distribution, which is not only analytically tractable but also serves as a fundamental benchmark for sampler performance. By deriving the exact flow, you will uncover its elegant connection to a system of harmonic oscillators, revealing why HMC with a well-chosen metric can be so efficient .",
            "id": "3318349",
            "problem": "Consider a parameter inference task in computational systems biology where the posterior distribution over a $d$-dimensional parameter vector $\\theta$ is Gaussian, $\\pi(\\theta) = \\mathcal{N}(0, \\Sigma)$, with a symmetric positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. To design an efficient sampler, we adopt Hamiltonian Monte Carlo (HMC) on the canonical phase space $(q,p) \\in \\mathbb{R}^{d} \\times \\mathbb{R}^{d}$ with Hamiltonian $H(q,p) = U(q) + K(p)$, where the potential energy $U(q)$ is defined from the negative log-posterior, $U(q) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q$, and the kinetic energy is chosen as $K(p) = \\frac{1}{2} p^{\\top} \\Sigma p$. This choice models the parameter space geometry and the momentum geometry with the same metric $\\Sigma$, which is plausible for dynamical systems whose linearized fluctuations share the same covariance structure.\n\nStarting only from the fundamental definitions of Hamiltonian dynamics, namely Hamilton’s equations $\\dot{q} = \\partial H/\\partial p$ and $\\dot{p} = - \\partial H/\\partial q$, derive the exact, continuous-time Hamiltonian flow induced by $H(q,p)$ and show that it is a set of decoupled harmonic oscillators with unit angular frequency. Then, provide the exact time-$t$ flow map $\\Phi_{t}$ that sends the initial state $(q(0), p(0))$ to $(q(t), p(t))$, expressed as a single closed-form $2d \\times 2d$ block matrix in terms of $t$ and $\\Sigma$.\n\nAnswer specification:\n- Express the final flow map $\\Phi_{t}$ using trigonometric functions of $t$ with $t$ interpreted in radians.\n- Your final answer must be a single closed-form analytic expression.\n- No rounding is required; provide the exact expression.",
            "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Domain**: Computational systems biology, Hamiltonian Monte Carlo (HMC).\n- **Phase Space**: Canonical phase space $(q,p) \\in \\mathbb{R}^{d} \\times \\mathbb{R}^{d}$.\n- **Parameter Vector**: $\\theta \\in \\mathbb{R}^d$, identified with the position coordinate $q$.\n- **Posterior Distribution**: A Gaussian distribution, $\\pi(\\theta) = \\mathcal{N}(0, \\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive-definite covariance matrix.\n- **Potential Energy**: $U(q) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q$. This is derived from the negative log-posterior, since $-\\ln(\\pi(q)) = -\\ln(\\text{const} \\cdot \\exp(-\\frac{1}{2} q^{\\top} \\Sigma^{-1} q)) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q + \\text{const}$. The constant does not affect the dynamics.\n- **Kinetic Energy**: $K(p) = \\frac{1}{2} p^{\\top} \\Sigma p$.\n- **Hamiltonian**: $H(q,p) = U(q) + K(p) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q + \\frac{1}{2} p^{\\top} \\Sigma p$.\n- **Equations of Motion**: Hamilton’s equations, $\\dot{q} = \\frac{\\partial H}{\\partial p}$ and $\\dot{p} = - \\frac{\\partial H}{\\partial q}$.\n- **Task**:\n    1.  Derive the exact, continuous-time Hamiltonian flow.\n    2.  Show that the flow represents a set of decoupled harmonic oscillators with unit angular frequency.\n    3.  Provide the exact time-$t$ flow map $\\Phi_{t}$ that maps $(q(0), p(0))$ to $(q(t), p(t))$ as a single $2d \\times 2d$ block matrix.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is firmly grounded in Hamiltonian mechanics and its application in statistics (specifically, Hamiltonian Monte Carlo). The formulation of the Hamiltonian with a position-dependent metric (Riemannian Manifold HMC) is a well-established advanced technique. The choice of kinetic energy $K(p) = \\frac{1}{2}p^{\\top} M^{-1} p$ with a mass matrix $M = \\Sigma^{-1}$ is a standard choice for a Gaussian target distribution with covariance $\\Sigma$, to adapt to the geometry of the parameter space. The problem statement sets $M^{-1} = \\Sigma$, which is precisely this case. The mathematics are standard vector calculus and ordinary differential equations. The problem is scientifically sound.\n- **Well-Posedness**: The problem asks for the solution to a system of linear, time-invariant ordinary differential equations with given initial conditions. Such a problem has a unique and stable solution. The problem is well-posed.\n- **Objectivity**: The problem is stated in precise, objective mathematical language.\n- **Completeness and Consistency**: All necessary components (Hamiltonian, equations of motion) are provided. There are no contradictions.\n- **Feasibility and Realism**: The scenario is a theoretically important and practical special case in HMC research. It is fully realistic and feasible to solve.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe solution proceeds by first deriving Hamilton's equations for the given Hamiltonian, then demonstrating that the resulting system of linear ordinary differential equations corresponds to a set of decoupled harmonic oscillators, and finally solving these equations to find the flow map.\n\nThe Hamiltonian is given by $H(q, p) = U(q) + K(p) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q + \\frac{1}{2} p^{\\top} \\Sigma p$.\n\nFirst, we compute the gradients of $H(q,p)$ with respect to $q$ and $p$. For a symmetric matrix $A$, the gradient of the quadratic form $x^{\\top} A x$ with respect to $x$ is $2 A x$. Thus, the partial derivatives are:\n$$\n\\frac{\\partial H}{\\partial p} = \\frac{\\partial}{\\partial p} \\left( \\frac{1}{2} p^{\\top} \\Sigma p \\right) = \\frac{1}{2} (2 \\Sigma p) = \\Sigma p\n$$\n$$\n\\frac{\\partial H}{\\partial q} = \\frac{\\partial}{\\partial q} \\left( \\frac{1}{2} q^{\\top} \\Sigma^{-1} q \\right) = \\frac{1}{2} (2 \\Sigma^{-1} q) = \\Sigma^{-1} q\n$$\nHere, we have used the fact that since $\\Sigma$ is symmetric, its inverse $\\Sigma^{-1}$ is also symmetric.\n\nSubstituting these gradients into Hamilton's equations, $\\dot{q} = \\frac{\\partial H}{\\partial p}$ and $\\dot{p} = - \\frac{\\partial H}{\\partial q}$, we obtain the system of equations governing the dynamics:\n$$\n\\dot{q} = \\Sigma p\n$$\n$$\n\\dot{p} = -\\Sigma^{-1} q\n$$\nThis is a system of $2d$ coupled first-order linear ordinary differential equations.\n\nTo show that this system describes a set of decoupled harmonic oscillators, we perform a change of coordinates. Since $\\Sigma$ is a real, symmetric, positive-definite matrix, it admits a spectral decomposition $\\Sigma = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix ($V^{\\top}V = VV^{\\top} = I$, where $I$ is the $d \\times d$ identity matrix) whose columns are the eigenvectors of $\\Sigma$, and $\\Lambda$ is a diagonal matrix containing the corresponding positive eigenvalues $\\lambda_i$. The inverse is then $\\Sigma^{-1} = V \\Lambda^{-1} V^{\\top}$.\n\nLet us define a new set of coordinates $(\\tilde{q}, \\tilde{p})$ via the transformation:\n$$\n\\tilde{q} = V^{\\top} q \\quad \\implies \\quad q = V \\tilde{q}\n$$\n$$\n\\tilde{p} = V^{\\top} p \\quad \\implies \\quad p = V \\tilde{p}\n$$\nThe time derivatives are related by $\\dot{q} = V \\dot{\\tilde{q}}$ and $\\dot{p} = V \\dot{\\tilde{p}}$. Substituting these into the equations of motion:\nFor the $\\dot{q}$ equation:\n$$\nV \\dot{\\tilde{q}} = \\Sigma (V \\tilde{p}) = (V \\Lambda V^{\\top}) V \\tilde{p} = V \\Lambda \\tilde{p}\n$$\nMultiplying by $V^{\\top}$ from the left yields:\n$$\n\\dot{\\tilde{q}} = \\Lambda \\tilde{p}\n$$\nFor the $\\dot{p}$ equation:\n$$\nV \\dot{\\tilde{p}} = -\\Sigma^{-1} (V \\tilde{q}) = -(V \\Lambda^{-1} V^{\\top}) V \\tilde{q} = -V \\Lambda^{-1} \\tilde{q}\n$$\nMultiplying by $V^{\\top}$ from the left yields:\n$$\n\\dot{\\tilde{p}} = -\\Lambda^{-1} \\tilde{q}\n$$\nBecause $\\Lambda$ is a diagonal matrix, $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$, the system decouples into $d$ independent pairs of equations for each component $i \\in \\{1, \\dots, d\\}$:\n$$\n\\dot{\\tilde{q}}_i = \\lambda_i \\tilde{p}_i\n$$\n$$\n\\dot{\\tilde{p}}_i = -\\frac{1}{\\lambda_i} \\tilde{q}_i\n$$\nTo see that these are harmonic oscillators, we can differentiate the first equation with respect to time and substitute the second:\n$$\n\\ddot{\\tilde{q}}_i = \\lambda_i \\dot{\\tilde{p}}_i = \\lambda_i \\left(-\\frac{1}{\\lambda_i} \\tilde{q}_i\\right) = -\\tilde{q}_i\n$$\nThis gives the equation for a simple harmonic oscillator, $\\ddot{\\tilde{q}}_i + \\tilde{q}_i = 0$. The standard form is $\\ddot{x} + \\omega^2 x = 0$, so the angular frequency is $\\omega_i = 1$ for all $i=1, \\dots, d$. This confirms that the dynamics are those of a set of $d$ decoupled harmonic oscillators, all with unit angular frequency.\n\nNow, we solve for the time-$t$ flow map. The solution to $\\ddot{\\tilde{q}}_i + \\tilde{q}_i = 0$ is of the form $\\tilde{q}_i(t) = A \\cos(t) + B \\sin(t)$. Applying the initial conditions:\n$\\tilde{q}_i(0) = A$.\n$\\dot{\\tilde{q}}_i(0) = B$. From the first-order equations, $\\dot{\\tilde{q}}_i(0) = \\lambda_i \\tilde{p}_i(0)$.\nSo, the solution for $\\tilde{q}_i(t)$ is:\n$$\n\\tilde{q}_i(t) = \\tilde{q}_i(0) \\cos(t) + \\lambda_i \\tilde{p}_i(0) \\sin(t)\n$$\nFor $\\tilde{p}_i(t)$, we have $\\tilde{p}_i(t) = \\frac{1}{\\lambda_i} \\dot{\\tilde{q}}_i(t) = \\frac{1}{\\lambda_i} \\left( -\\tilde{q}_i(0) \\sin(t) + \\lambda_i \\tilde{p}_i(0) \\cos(t) \\right)$:\n$$\n\\tilde{p}_i(t) = -\\frac{1}{\\lambda_i} \\tilde{q}_i(0) \\sin(t) + \\tilde{p}_i(0) \\cos(t)\n$$\nWe can write this in vector form for the transformed variables:\n$$\n\\tilde{q}(t) = \\cos(t) \\tilde{q}(0) + \\sin(t) \\Lambda \\tilde{p}(0)\n$$\n$$\n\\tilde{p}(t) = -\\sin(t) \\Lambda^{-1} \\tilde{q}(0) + \\cos(t) \\tilde{p}(0)\n$$\nFinally, we transform back to the original coordinates $(q,p)$:\n$q(t) = V\\tilde{q}(t) = V(\\cos(t) \\tilde{q}(0) + \\sin(t) \\Lambda \\tilde{p}(0))$\nUsing $\\tilde{q}(0) = V^{\\top}q(0)$ and $\\tilde{p}(0) = V^{\\top}p(0)$:\n$q(t) = V(\\cos(t) V^{\\top}q(0) + \\sin(t) \\Lambda V^{\\top}p(0)) = \\cos(t) (VV^{\\top})q(0) + \\sin(t) (V \\Lambda V^{\\top})p(0)$\nSince $VV^{\\top}=I$ and $\\Sigma = V \\Lambda V^{\\top}$, we get:\n$$\nq(t) = \\cos(t) q(0) + \\sin(t) \\Sigma p(0)\n$$\nSimilarly for $p(t)$:\n$p(t) = V\\tilde{p}(t) = V(-\\sin(t) \\Lambda^{-1} \\tilde{q}(0) + \\cos(t) \\tilde{p}(0))$\n$p(t) = V(-\\sin(t) \\Lambda^{-1} V^{\\top}q(0) + \\cos(t) V^{\\top}p(0)) = -\\sin(t) (V \\Lambda^{-1} V^{\\top})q(0) + \\cos(t) (VV^{\\top})p(0)$\nSince $\\Sigma^{-1} = V \\Lambda^{-1} V^{\\top}$, we get:\n$$\np(t) = -\\sin(t) \\Sigma^{-1} q(0) + \\cos(t) p(0)\n$$\nThis defines the time-$t$ flow map $\\Phi_t$ such that $\\begin{pmatrix} q(t) \\\\ p(t) \\end{pmatrix} = \\Phi_t \\begin{pmatrix} q(0) \\\\ p(0) \\end{pmatrix}$. Writing this in a $2d \\times 2d$ block matrix form:\n$$\n\\begin{pmatrix} q(t) \\\\ p(t) \\end{pmatrix} = \\begin{pmatrix} I \\cos(t) & \\Sigma \\sin(t) \\\\ -\\Sigma^{-1} \\sin(t) & I \\cos(t) \\end{pmatrix} \\begin{pmatrix} q(0) \\\\ p(0) \\end{pmatrix}\n$$\nThe flow map $\\Phi_t$ is therefore the $2d \\times 2d$ matrix on the right-hand side.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\cos(t) I & \\sin(t) \\Sigma \\\\\n-\\sin(t) \\Sigma^{-1} & \\cos(t) I\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While general MCMC diagnostics are essential, HMC's unique mechanics, based on Hamiltonian dynamics, call for specialized tools to assess sampler performance. This practice focuses on the Energy Bayesian Fraction of Missing Information (E-BFMI), a diagnostic that quantifies the sampler's ability to explore different energy levels of the posterior distribution. By deriving and applying the E-BFMI estimator, you will learn to diagnose the efficiency of momentum resampling, a critical component for avoiding slow exploration in complex dynamical system models .",
            "id": "3318373",
            "problem": "In a parameter inference study of a phosphorylation–dephosphorylation cascade modeled as a deterministic dynamical system with ordinary differential equations, you employ Hamiltonian Monte Carlo (HMC) to draw samples from the posterior over kinetic parameters. Denote the position by $q$ and momentum by $p$, the Hamiltonian by $H(q,p) = U(q) + K(p)$ with potential energy $U(q)$ equal to the negative log posterior density up to an additive constant, and kinetic energy $K(p)$ induced by a Gaussian momentum distribution. At each HMC iteration, a fresh momentum $p$ is drawn from a Gaussian distribution with mass matrix $M$, and a symplectic integrator approximately conserves the Hamiltonian along a trajectory; an accept–reject step corrects for numerical error. The stationary marginal distribution of the Hamiltonian values $E = H(q,p)$ along the chain quantifies the typical energy scales, while the resampled momentum $p$ drives vertical moves in energy space.\n\nThe energy Bayesian fraction of missing information (E-BFMI) is a dimensionless diagnostic that quantifies how effectively the resampled momentum injects kinetic energy to explore the marginal energy distribution. Starting from the definitions above and the interpretation of E-BFMI as the ratio of the expected squared vertical change in energy induced by momentum resampling to the variance of the marginal stationary energy distribution, derive a consistent estimator for E-BFMI from a single sequence of observed Hamiltonian energies $\\{E_{n}\\}_{n=1}^{N}$ recorded at successive iterations of a stationary HMC chain. Then, using your derived estimator, compute E-BFMI for the following sequence of $N=10$ consecutive post–warmup Hamiltonian energies (in arbitrary units) obtained from fitting the cascade model:\n$E_{1}=100.0$, $E_{2}=100.2$, $E_{3}=100.4$, $E_{4}=100.6$, $E_{5}=100.8$, $E_{6}=101.0$, $E_{7}=101.2$, $E_{8}=101.4$, $E_{9}=101.6$, $E_{10}=101.8$.\n\nReport the numerical value of E-BFMI. Round your answer to three significant figures. In your reasoning, explain briefly why low values of E-BFMI correspond to poor momentum energy exploration in HMC for this dynamical systems context. The final reported answer must be the computed E-BFMI value only.",
            "solution": "The problem is validated as scientifically sound, well-posed, and self-contained. It presents a clear task: to derive a consistent estimator for the Energy Bayesian Fraction of Missing Information (E-BFMI) from its provided definition and then compute its value for a given sequence of Hamiltonian energies. All concepts are standard in the field of computational statistics and its application to systems biology.\n\nThe problem defines E-BFMI as the ratio of the expected squared vertical change in energy induced by momentum resampling to the variance of the marginal stationary energy distribution. Let $\\{E_n\\}_{n=1}^{N}$ be a sequence of Hamiltonian energy values from a stationary HMC chain. The Hamiltonian is $H(q,p) = U(q) + K(p)$, and its value at iteration $n$ is $E_n$.\n\nFirst, we formalize the denominator: the variance of the marginal stationary energy distribution, $\\text{Var}(E)$. A consistent estimator for this variance, based on the observed sample $\\{E_n\\}$, is the sample variance:\n$$ \\widehat{\\text{Var}}(E) = \\frac{1}{N-1} \\sum_{n=1}^{N} (E_n - \\Bar{E})^2 $$\nwhere $\\Bar{E} = \\frac{1}{N} \\sum_{n=1}^{N} E_n$ is the sample mean of the energies.\n\nNext, we formalize the numerator: the expected squared vertical change in energy induced by momentum resampling. In an HMC iteration, the primary stochastic perturbation that allows the sampler to move between energy levels is the resampling of the momentum variables. The net change in energy from one accepted state to the next, $\\Delta E_n = E_n - E_{n-1}$, captures the result of this process. The expected squared change is thus $\\mathbb{E}[(\\Delta E)^2]$. A consistent estimator for this quantity is the sample mean of the squared successive differences:\n$$ \\widehat{\\mathbb{E}}[(\\Delta E)^2] = \\frac{1}{N-1} \\sum_{n=2}^{N} (E_n - E_{n-1})^2 $$\nNote that the sum runs from $n=2$ to $N$ as there are $N-1$ such differences.\n\nThe estimator for E-BFMI, denoted $\\widehat{\\text{E-BFMI}}$, is the ratio of the estimator for the numerator to the estimator for the denominator:\n$$ \\widehat{\\text{E-BFMI}} = \\frac{\\frac{1}{N-1} \\sum_{n=2}^{N} (E_n - E_{n-1})^2}{\\frac{1}{N-1} \\sum_{n=1}^{N} (E_n - \\Bar{E})^2} $$\nThe factor of $\\frac{1}{N-1}$ cancels, yielding the final form of the estimator:\n$$ \\widehat{\\text{E-BFMI}} = \\frac{\\sum_{n=2}^{N} (E_n - E_{n-1})^2}{\\sum_{n=1}^{N} (E_n - \\Bar{E})^2} $$\n\nNow, we apply this estimator to the provided data sequence of $N=10$ energies:\n$E = \\{100.0, 100.2, 100.4, 100.6, 100.8, 101.0, 101.2, 101.4, 101.6, 101.8\\}$.\n\nFirst, we compute the numerator, the sum of squared successive differences. The sequence is an arithmetic progression with a constant difference of $d=0.2$. Therefore, $E_n - E_{n-1} = 0.2$ for all $n \\in \\{2, \\dots, 10\\}$.\n$$ \\text{Numerator} = \\sum_{n=2}^{10} (E_n - E_{n-1})^2 = \\sum_{n=2}^{10} (0.2)^2 $$\nThere are $N-1 = 9$ terms in this sum.\n$$ \\text{Numerator} = 9 \\times (0.2)^2 = 9 \\times 0.04 = 0.36 $$\n\nNext, we compute the denominator, the sum of squared deviations from the mean. First, we find the sample mean, $\\Bar{E}$. For an arithmetic progression, the mean is the average of the first and last terms:\n$$ \\Bar{E} = \\frac{E_1 + E_{10}}{2} = \\frac{100.0 + 101.8}{2} = \\frac{201.8}{2} = 100.9 $$\nNow, we compute the deviations $(E_n - \\Bar{E})$:\n$\\{100.0-100.9, 100.2-100.9, \\dots, 101.8-100.9\\} = \\{-0.9, -0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7, 0.9\\}$\nWe then compute the sum of the squares of these deviations:\n$$ \\text{Denominator} = \\sum_{n=1}^{10} (E_n - \\Bar{E})^2 = (-0.9)^2 + (-0.7)^2 + (-0.5)^2 + (-0.3)^2 + (-0.1)^2 + (0.1)^2 + (0.3)^2 + (0.5)^2 + (0.7)^2 + (0.9)^2 $$\n$$ \\text{Denominator} = 0.81 + 0.49 + 0.25 + 0.09 + 0.01 + 0.01 + 0.09 + 0.25 + 0.49 + 0.81 $$\n$$ \\text{Denominator} = 2 \\times (0.81 + 0.49 + 0.25 + 0.09 + 0.01) = 2 \\times 1.65 = 3.3 $$\n\nFinally, we compute the E-BFMI value:\n$$ \\widehat{\\text{E-BFMI}} = \\frac{\\text{Numerator}}{\\text{Denominator}} = \\frac{0.36}{3.3} = \\frac{36}{330} = \\frac{6}{55} \\approx 0.109090... $$\nRounding to three significant figures, the result is $0.109$.\n\nA low value of E-BFMI, such as the one computed, indicates poor exploration of the energy landscape. The numerator, $\\sum (E_n - E_{n-1})^2$, quantifies the squared \"jumps\" in energy between successive HMC iterations, which are driven by the kinetic energy injected by momentum resampling. The denominator, $\\sum (E_n - \\Bar{E})^2$, quantifies the total variance of the energy across the posterior, representing the size of the energy landscape that needs to be explored. A small E-BFMI value implies that the inter-iteration energy jumps are small compared to the overall energy variance. This means the sampler is taking very small steps in energy space, leading to a slow, highly correlated exploration of the posterior distribution of the kinetic parameters. For the dynamical systems model, this indicates that the HMC sampler is inefficient and may struggle to escape local modes of the posterior, requiring a very large number of iterations to produce reliable inferences. This is often caused by a poorly tuned mass matrix $M$ that is misaligned with the geometry of the potential energy surface $U(q)$.",
            "answer": "$$ \\boxed{0.109} $$"
        },
        {
            "introduction": "Real-world biological systems, such as those with distinct cell subpopulations, often require more complex formulations like mixture-of-dynamics models. This hands-on coding exercise tasks you with building an HMC sampler for such a model and confronting a classic statistical challenge known as label switching, which arises from the model's inherent symmetries. Through implementing the sampler and its diagnostics, you will gain practical experience in identifying and understanding how model symmetries manifest in HMC trajectories and affect posterior inference .",
            "id": "3318302",
            "problem": "Consider a mixture-of-dynamics model for single-cell subpopulations, where each subpopulation index $k \\in \\{1,\\dots,K\\}$ has parameters $\\theta^{(k)} = (a_k, x_{0,k})$ that govern a scalar linear ordinary differential equation $dx/dt = -a_k x$ with solution $x_k(t) = x_{0,k} \\exp(-a_k t)$. We observe scalar measurements $y_i$ at times $t_i$, $i \\in \\{1,\\dots,N\\}$, drawn independently from a finite mixture with mixing weights $\\pi_k$ that satisfy $\\sum_{k=1}^K \\pi_k = 1$ and $\\pi_k \\ge 0$ for all $k$, and observation noise with known standard deviation $\\sigma$. Formally, the data model is\n$$\np(y_i \\mid \\{\\theta^{(k)}\\}_{k=1}^K, \\{\\pi_k\\}_{k=1}^K) \\;=\\; \\sum_{k=1}^K \\pi_k \\, \\mathcal{N}\\big(y_i \\,\\big|\\, x_{0,k}\\exp(-a_k t_i), \\, \\sigma^2 \\big),\n$$\nwith independence across $i$.\n\nTo apply Hamiltonian Monte Carlo (HMC), we will work in an unconstrained parameterization. Let $\\alpha_k = \\log a_k$ (so $a_k = \\exp(\\alpha_k)$) and $z \\in \\mathbb{R}^K$ be the logits of the mixing weights with softmax mapping\n$$\n\\pi_k(z) = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}.\n$$\nWe place independent Gaussian priors on the unconstrained parameters:\n$$\n\\alpha_k \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha^2), \\quad x_{0,k} \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2), \\quad z_k \\sim \\mathcal{N}(0, \\tau^2),\n$$\nindependently over $k \\in \\{1,\\dots,K\\}$. This choice preserves label symmetry because the priors are exchangeable across $k$.\n\nDefine the posterior density (up to a normalization constant) in the unconstrained variables $(\\{\\alpha_k\\}, \\{x_{0,k}\\}, z)$ as the product of the likelihood and the priors. Denote the log posterior by $\\log p(\\alpha, x_0, z \\mid \\{(t_i,y_i)\\}_{i=1}^N)$, where $\\alpha = (\\alpha_1,\\dots,\\alpha_K)$ and $x_0 = (x_{0,1},\\dots,x_{0,K})$. The Hamiltonian for HMC with identity mass matrix is\n$$\nH(q, r) \\;=\\; -\\log p(q) \\;+\\; \\frac{1}{2}\\, r^\\top r,\n$$\nwhere $q$ stacks the unconstrained parameters and $r$ is the momentum of matching dimension with elements independently distributed as standard normal at each HMC iteration.\n\nYou will address the label switching symmetry induced by the permutation group on component labels. For $K=2$, let $P$ be the label-swapping permutation that maps\n$$\n(\\alpha_1,\\alpha_2, x_{0,1},x_{0,2}, z_1, z_2) \\mapsto (\\alpha_2,\\alpha_1, x_{0,2},x_{0,1}, z_2, z_1),\n$$\nand applies the same swap to the grouped momentum components. Because both the likelihood and the priors are symmetric to relabeling, the posterior, its gradient, and thus the Hamiltonian vector field are invariant under $P$.\n\nStarting only from (i) Bayes’ rule, (ii) the definition of the softmax function and Gaussian log-density, (iii) the solution of the linear ordinary differential equation $x_k(t) = x_{0,k} \\exp(-a_k t)$ with $a_k = \\exp(\\alpha_k)$, and (iv) the definition of Hamiltonian Monte Carlo (HMC) leapfrog integration, derive expressions needed to compute the log posterior and its gradient with respect to the unconstrained variables. Use these expressions to construct a program that evaluates symmetry-induced diagnostics for HMC.\n\nUse the following fixed and scientifically plausible settings:\n- Number of mixture components: $K = 2$.\n- Number of observations: $N = 5$.\n- Observation times: $t = [\\,0.0,\\, 0.5,\\, 1.0,\\, 1.5,\\, 2.0\\,]$.\n- Observed data: $y = [\\,1.95,\\, 1.50,\\, -0.40,\\, 0.90,\\, 0.70\\,]$.\n- Observation noise standard deviation (known): $\\sigma = 0.1$.\n- Prior hyperparameters: $\\mu_\\alpha = 0.0$, $\\sigma_\\alpha = 1.0$, $\\mu_x = 0.0$, $\\sigma_x = 2.0$, $\\tau = 1.0$.\n\nDefine the base unconstrained parameter vector\n$$\nq_{\\mathrm{base}} = \\big(\\alpha_1, \\alpha_2, x_{0,1}, x_{0,2}, z_1, z_2\\big) \\\\\n= \\big(\\,-0.5108256238,\\; -0.1053605157,\\; 1.8,\\; -1.1,\\; 0.3,\\; -0.3 \\,\\big).\n$$\nDefine the associated momentum vector\n$$\nr_{\\mathrm{base}} = \\big(\\,0.10,\\; -0.20,\\; 0.30,\\; -0.10,\\; 0.05,\\; -0.05\\,\\big).\n$$\nDefine the permutation $P$ that swaps component labels as described above. Define the Hamiltonian Monte Carlo (HMC) integrator using the leapfrog method with step size $\\varepsilon = 0.01$ and number of steps $L = 25$. The identity mass matrix must be used.\n\nYour program must:\n- Implement the log posterior and its gradient with respect to the unconstrained parameters $(\\alpha, x_0, z)$ for $K=2$ and the specified data and priors, using the mixture likelihood as defined.\n- Implement the Hamiltonian, the leapfrog integrator, and the permutation operator $P$ that acts on both parameters and momenta by swapping component labels.\n- Compute the following test quantities, which serve as a test suite:\n    1. $T_1$: The absolute difference $|\\log p(q_{\\mathrm{base}}) - \\log p(P q_{\\mathrm{base}})|$.\n    2. $T_2$: The absolute difference $|H(q_{\\mathrm{base}}, r_{\\mathrm{base}}) - H(P q_{\\mathrm{base}}, P r_{\\mathrm{base}})|$.\n    3. $T_3$: Initialize two HMC proposals deterministically: from $(q_{\\mathrm{base}}, r_{\\mathrm{base}})$ and from $(P q_{\\mathrm{base}}, P r_{\\mathrm{base}})$, each integrated forward by $L$ leapfrog steps with step size $\\varepsilon$. Compute the acceptance probabilities\n       $$\n       a_1 = \\min\\left(1, \\exp\\big(-H(q', r') + H(q_{\\mathrm{base}}, r_{\\mathrm{base}})\\big)\\right), \\quad\n       a_2 = \\min\\left(1, \\exp\\big(-H(P q', P r') + H(P q_{\\mathrm{base}}, P r_{\\mathrm{base}})\\big)\\right),\n       $$\n       where $(q',r')$ is the proposal from $(q_{\\mathrm{base}}, r_{\\mathrm{base}})$, and $P q', P r'$ denotes the permuted proposal of the permuted initialization. Report $T_3 = |a_1 - a_2|$.\n    4. $T_4$: The Euclidean norm of the difference between the gradient at $q_{\\mathrm{base}}$ and the inverse-permuted gradient at $P q_{\\mathrm{base}}$, i.e.,\n       $$\n       T_4 = \\left\\|\\nabla \\log p(q_{\\mathrm{base}}) - P^{-1}\\big(\\nabla \\log p(P q_{\\mathrm{base}})\\big) \\right\\|_2.\n       $$\n    5. $T_5$: A stability diagnostic in an edge case with near-degenerate mixing weights. Let $q_{\\mathrm{edge}} = \\big(\\alpha_1, \\alpha_2, x_{0,1}, x_{0,2}, z_1, z_2\\big)$ where $(\\alpha_1, \\alpha_2, x_{0,1}, x_{0,2})$ are the same as in $q_{\\mathrm{base}}$, and $(z_1, z_2) = (-20.0, 0.0)$. Compute the gradient $\\nabla \\log p(q_{\\mathrm{edge}})$ and return a boolean $T_5$ indicating whether all components are finite (no Not-a-Number or infinities).\n\nYour program should produce a single line of output containing these results as a comma-separated list enclosed in square brackets, in the order $[T_1, T_2, T_3, T_4, T_5]$. All floating-point quantities should be output in their default string representation, and the final boolean should be either True or False. No angles or physical units are involved in this problem; therefore, no unit conversion is required. The program must take no input and must run deterministically with the provided constants.",
            "solution": "The user-provided problem has been validated and is determined to be a well-posed, scientifically grounded, and computationally feasible task. It requires the derivation and implementation of the log-posterior probability density and its gradient for a mixture-of-dynamics model, which are then used within a Hamiltonian Monte Carlo (HMC) framework to verify properties related to label-swapping symmetry.\n\n### I. Mathematical Formulation\n\nThe unconstrained parameter vector is $q = (\\alpha_1, \\alpha_2, x_{0,1}, x_{0,2}, z_1, z_2) \\in \\mathbb{R}^6$. The posterior density is given by Bayes' rule: $p(q \\mid \\text{data}) \\propto p(\\text{data} \\mid q) p(q)$. We work with the unnormalized log-posterior, denoted $\\log p(q)$.\n\n$\\log p(q) = \\log \\mathcal{L}(q) + \\log p_{\\text{prior}}(q)$\n\n**1. Log-Prior Density**\n\nThe priors on the unconstrained parameters are independent Gaussians: $\\alpha_k \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha^2)$, $x_{0,k} \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$, and $z_k \\sim \\mathcal{N}(0, \\tau^2)$. The log-prior density, ignoring constant terms, is:\n$$\n\\log p_{\\text{prior}}(q) = -\\sum_{k=1}^2 \\left( \\frac{(\\alpha_k - \\mu_\\alpha)^2}{2\\sigma_\\alpha^2} + \\frac{(x_{0,k} - \\mu_x)^2}{2\\sigma_x^2} + \\frac{z_k^2}{2\\tau^2} \\right)\n$$\n\n**2. Log-Likelihood Function**\n\nThe likelihood for $N$ independent observations is a product over each observation's likelihood, so the log-likelihood is a sum:\n$$\n\\log \\mathcal{L}(q) = \\sum_{i=1}^N \\log p(y_i \\mid q)\n$$\nwhere the likelihood for a single observation $y_i$ is a mixture of two Gaussian densities:\n$$\np(y_i \\mid q) = \\sum_{k=1}^2 \\pi_k(z) \\, \\mathcal{N}(y_i \\mid \\mu_{ik}, \\sigma^2)\n$$\nThe mean of the $k$-th component for the $i$-th observation is given by the ODE solution:\n$$\n\\mu_{ik} = x_{0,k} \\exp(-a_k t_i) = x_{0,k} \\exp(-\\exp(\\alpha_k) t_i)\n$$\nThe mixing weights $\\pi_k$ are given by the softmax function:\n$$\n\\pi_k(z) = \\frac{\\exp(z_k)}{\\sum_{j=1}^2 \\exp(z_j)}\n$$\nThe log-likelihood, ignoring the constant from the Gaussian normalization, is thus:\n$$\n\\log \\mathcal{L}(q) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^2 \\pi_k(z) \\exp\\left(-\\frac{(y_i - \\mu_{ik})^2}{2\\sigma^2}\\right) \\right)\n$$\nFor numerical stability, this expression should be computed using the log-sum-exp trick.\n\n**3. Gradient of the Log-Posterior**\n\nThe gradient $\\nabla_q \\log p(q)$ is required for the HMC leapfrog integrator. We compute the partial derivative with respect to each parameter.\nA key quantity is the responsibility $w_{ik}$, the posterior probability that observation $i$ was generated by component $k$:\n$$\nw_{ik} = \\frac{\\pi_k \\mathcal{N}(y_i \\mid \\mu_{ik}, \\sigma^2)}{\\sum_{j=1}^2 \\pi_j \\mathcal{N}(y_i \\mid \\mu_{ij}, \\sigma^2)}\n$$\nThe gradient of the log-likelihood is $\\nabla_q \\log \\mathcal{L}(q) = \\sum_i \\sum_k w_{ik} \\nabla_q \\log(\\pi_k \\mathcal{N}(y_i \\mid \\mu_{ik}, \\sigma^2))$.\n\n- **Gradient with respect to $\\alpha_j$**:\n  $$\n  \\frac{\\partial \\log p(q)}{\\partial \\alpha_j} = \\left( \\sum_{i=1}^N w_{ij} \\frac{y_i - \\mu_{ij}}{\\sigma^2} \\frac{\\partial \\mu_{ij}}{\\partial \\alpha_j} \\right) - \\frac{\\alpha_j - \\mu_\\alpha}{\\sigma_\\alpha^2}\n  $$\n  With $\\frac{\\partial \\mu_{ij}}{\\partial \\alpha_j} = -t_i \\exp(\\alpha_j) \\mu_{ij}$.\n\n- **Gradient with respect to $x_{0,j}$**:\n  $$\n  \\frac{\\partial \\log p(q)}{\\partial x_{0,j}} = \\left( \\sum_{i=1}^N w_{ij} \\frac{y_i - \\mu_{ij}}{\\sigma^2} \\frac{\\partial \\mu_{ij}}{\\partial x_{0,j}} \\right) - \\frac{x_{0,j} - \\mu_x}{\\sigma_x^2}\n  $$\n  With $\\frac{\\partial \\mu_{ij}}{\\partial x_{0,j}} = \\exp(-\\exp(\\alpha_j) t_i)$.\n\n- **Gradient with respect to $z_j$**:\n  The gradient of the log-softmax is $\\frac{\\partial \\log \\pi_k}{\\partial z_j} = \\delta_{kj} - \\pi_j$.\n  $$\n  \\frac{\\partial \\log p(q)}{\\partial z_j} = \\left( \\sum_{i=1}^N \\sum_{k=1}^2 w_{ik} (\\delta_{kj} - \\pi_j) \\right) - \\frac{z_j}{\\tau^2} = \\left( \\sum_{i=1}^N (w_{ij} - \\pi_j) \\right) - \\frac{z_j}{\\tau^2}\n  $$\n\nThese expressions form the basis for the gradient computation.\n\n### II. Algorithmic Implementation\n\n**1. Hamiltonian and Leapfrog Integrator**\n\nThe Hamiltonian with an identity mass matrix is $H(q, r) = -\\log p(q) + \\frac{1}{2}r^\\top r$.\nThe leapfrog integrator advances the state $(q, r)$ over $L$ steps of size $\\varepsilon$:\n1. $r \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_q \\log p(q)$\n2. For $l=1,\\dots,L$:\n   a. $q \\leftarrow q + \\varepsilon r$\n   b. If $l<L$, $r \\leftarrow r + \\varepsilon \\nabla_q \\log p(q)$\n3. $r \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_q \\log p(q)$ \nThe final momentum update for step $L$ uses only a half step to maintain symmetry. A common implementation combines the updates:\n1. $r \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_q \\log p(q)$\n2. For $l = 1,\\dots,L-1$:\n   a. $q \\leftarrow q + \\varepsilon r$\n   b. $r \\leftarrow r + \\varepsilon \\nabla_q \\log p(q)$\n3. $q \\leftarrow q + \\varepsilon r$\n4. $r \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_q \\log p(q)$\n\n**2. Symmetry Verification**\n\nThe problem requires computing five test quantities ($T_1$ to $T_5$) to verify the implementation's handling of the label-swapping symmetry, represented by permutation operator $P$. Given that the model is symmetric under this permutation, we expect:\n- **$T_1, T_2$**: The log-posterior and Hamiltonian must be invariant: $\\log p(q) = \\log p(Pq)$ and $H(q, r) = H(Pq, Pr)$. The differences should be zero up to floating-point precision.\n- **$T_3$**: The leapfrog integrator should be equivariant, meaning that integrating from a permuted state is equivalent to permuting the result of integrating from the original state. This, combined with Hamiltonian invariance, implies the acceptance probabilities $a_1$ and $a_2$ should be identical.\n- **$T_4$**: The gradient must be equivariant: $\\nabla_q \\log p(Pq) = P(\\nabla_q \\log p(q))$. The norm of the difference should be near zero.\n- **$T_5$**: This tests the numerical stability of the gradient calculation in an edge case where one mixing weight is close to zero, which can cause underflow or division by zero if not handled with techniques like log-sum-exp. The gradient components should all be finite.\n\nThe program implements these functions and computes the requested test statistics.",
            "answer": "```python\nimport numpy as np\nfrom scipy import special\n\ndef solve():\n    \"\"\"\n    Computes a set of symmetry-based diagnostics for a Hamiltonian Monte Carlo\n    implementation on a mixture-of-dynamics model.\n    \"\"\"\n    # Fixed parameters for the single test case\n    K = 2\n    t = np.array([0.0, 0.5, 1.0, 1.5, 2.0])\n    y = np.array([1.95, 1.50, -0.40, 0.90, 0.70])\n    sigma = 0.1\n    mu_alpha, sigma_alpha = 0.0, 1.0\n    mu_x, sigma_x = 0.0, 2.0\n    tau = 1.0\n    q_base = np.array([-0.5108256238, -0.1053605157, 1.8, -1.1, 0.3, -0.3])\n    r_base = np.array([0.10, -0.20, 0.30, -0.10, 0.05, -0.05])\n    epsilon = 0.01\n    L = 25\n\n    # Numerically stable helper functions\n    def logsumexp(x, axis=None):\n        return special.logsumexp(x, axis=axis)\n    \n    def softmax(x):\n        # Subtract max for numerical stability\n        e_x = np.exp(x - np.max(x))\n        return e_x / np.sum(e_x)\n\n    def get_log_posterior(q):\n        \"\"\"Computes the log posterior density (up to a constant).\"\"\"\n        alpha, x0, z = q[0:K], q[K:2*K], q[2*K:3*K]\n        \n        # Log likelihood term\n        a = np.exp(alpha)\n        mu = x0 * np.exp(-a * t[:, np.newaxis])\n        pi = softmax(z)\n        residuals = y[:, np.newaxis] - mu\n        log_gauss_contrib = -(residuals**2) / (2 * sigma**2)\n        log_lik_terms = np.log(pi) + log_gauss_contrib\n        total_log_lik = np.sum(logsumexp(log_lik_terms, axis=1))\n\n        # Log prior term\n        log_prior_alpha = -np.sum((alpha - mu_alpha)**2) / (2 * sigma_alpha**2)\n        log_prior_x0 = -np.sum((x0 - mu_x)**2) / (2 * sigma_x**2)\n        log_prior_z = -np.sum(z**2) / (2 * tau**2)\n        total_log_prior = log_prior_alpha + log_prior_x0 + log_prior_z\n        \n        return total_log_lik + total_log_prior\n\n    def get_grad_log_posterior(q):\n        \"\"\"Computes the gradient of the log posterior density.\"\"\"\n        alpha, x0, z = q[0:K], q[K:2*K], q[2*K:3*K]\n\n        # Common quantities for likelihood gradient\n        a = np.exp(alpha)\n        mu = x0 * np.exp(-a * t[:, np.newaxis])\n        pi = softmax(z)\n        \n        # Responsibilities (w_ik)\n        residuals = y[:, np.newaxis] - mu\n        log_gauss_contrib = -(residuals**2) / (2 * sigma**2)\n        log_lik_terms = np.log(pi) + log_gauss_contrib\n        log_lik_per_obs = logsumexp(log_lik_terms, axis=1)\n        responsibilities = np.exp(log_lik_terms - log_lik_per_obs[:, np.newaxis])\n\n        # Gradient of log likelihood\n        grad_lik_alpha = np.sum(responsibilities * (residuals / sigma**2) * (-t[:, np.newaxis] * a * mu), axis=0)\n        grad_lik_x0 = np.sum(responsibilities * (residuals / sigma**2) * np.exp(-a * t[:, np.newaxis]), axis=0)\n        grad_lik_z = np.sum(responsibilities - pi, axis=0)\n\n        # Gradient of log prior\n        grad_prior_alpha = -(alpha - mu_alpha) / sigma_alpha**2\n        grad_prior_x0 = -(x0 - mu_x) / sigma_x**2\n        grad_prior_z = -z / tau**2\n        \n        return np.concatenate([grad_lik_alpha + grad_prior_alpha, grad_lik_x0 + grad_prior_x0, grad_lik_z + grad_prior_z])\n\n    def get_hamiltonian(q, r):\n        \"\"\"Computes the Hamiltonian.\"\"\"\n        potential_energy = -get_log_posterior(q)\n        kinetic_energy = 0.5 * np.dot(r, r)\n        return potential_energy + kinetic_energy\n\n    def leapfrog(q_init, r_init, eps, num_steps, grad_func):\n        \"\"\"Performs L leapfrog steps.\"\"\"\n        q, r = q_init.copy(), r_init.copy()\n        \n        # Half step for momentum at the beginning\n        r += 0.5 * eps * grad_func(q)\n        # Full steps for position and momentum\n        for _ in range(num_steps - 1):\n            q += eps * r\n            r += eps * grad_func(q)\n        # Last full step for position\n        q += eps * r\n        # Last half step for momentum\n        r += 0.5 * eps * grad_func(q)\n        \n        return q, r\n\n    def permute(v):\n        \"\"\"Applies the label-swapping permutation P.\"\"\"\n        return np.array([v[1], v[0], v[3], v[2], v[5], v[4]])\n\n    # T1: Absolute difference in log posterior due to permutation\n    T1 = abs(get_log_posterior(q_base) - get_log_posterior(permute(q_base)))\n    \n    # T2: Absolute difference in Hamiltonian due to permutation\n    T2 = abs(get_hamiltonian(q_base, r_base) - get_hamiltonian(permute(q_base), permute(r_base)))\n    \n    # T3: Absolute difference in HMC acceptance probabilities\n    q_prime, r_prime = leapfrog(q_base, r_base, epsilon, L, get_grad_log_posterior)\n    H_init1 = get_hamiltonian(q_base, r_base)\n    H_final1 = get_hamiltonian(q_prime, r_prime)\n    a1 = min(1.0, np.exp(-H_final1 + H_init1))\n    \n    H_init2 = get_hamiltonian(permute(q_base), permute(r_base))\n    H_final2 = get_hamiltonian(permute(q_prime), permute(r_prime))\n    a2 = min(1.0, np.exp(-H_final2 + H_init2))\n    T3 = abs(a1 - a2)\n    \n    # T4: Norm of gradient difference under permutation (equivariance check)\n    grad_base = get_grad_log_posterior(q_base)\n    grad_permuted_q = get_grad_log_posterior(permute(q_base))\n    T4 = np.linalg.norm(grad_base - permute(grad_permuted_q))\n\n    # T5: Stability diagnostic for gradient calculation\n    q_edge = q_base.copy()\n    q_edge[4:6] = [-20.0, 0.0]\n    grad_edge = get_grad_log_posterior(q_edge)\n    T5 = bool(np.all(np.isfinite(grad_edge)))\n    \n    results = [T1, T2, T3, T4, T5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}