{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation in Hamiltonian Monte Carlo, it is invaluable to analyze its behavior in an idealized setting. This exercise explores the exact Hamiltonian dynamics for a Gaussian posterior distribution, a common case in Bayesian inference, when the kinetic energy metric is thoughtfully chosen to match the posterior's covariance . By deriving the exact solution to Hamilton's equations, you will uncover a beautiful connection to the physics of harmonic oscillators and gain a fundamental understanding of how HMC can be adapted to the geometry of the target distribution.",
            "id": "3318349",
            "problem": "Consider a parameter inference task in computational systems biology where the posterior distribution over a $d$-dimensional parameter vector $\\theta$ is Gaussian, $\\pi(\\theta) = \\mathcal{N}(0, \\Sigma)$, with a symmetric positive-definite covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$. To design an efficient sampler, we adopt Hamiltonian Monte Carlo (HMC, Hamiltonian Monte Carlo) on the canonical phase space $(q,p) \\in \\mathbb{R}^{d} \\times \\mathbb{R}^{d}$ with Hamiltonian $H(q,p) = U(q) + K(p)$, where the potential energy $U(q)$ is defined from the negative log-posterior, $U(q) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q$, and the kinetic energy is chosen as $K(p) = \\frac{1}{2} p^{\\top} \\Sigma p$. This choice models the parameter space geometry and the momentum geometry with the same metric $\\Sigma$, which is plausible for dynamical systems whose linearized fluctuations share the same covariance structure.\n\nStarting only from the fundamental definitions of Hamiltonian dynamics, namely Hamilton’s equations $\\dot{q} = \\partial H/\\partial p$ and $\\dot{p} = - \\partial H/\\partial q$, derive the exact, continuous-time Hamiltonian flow induced by $H(q,p)$ and show that it is a set of decoupled harmonic oscillators with unit angular frequency. Then, provide the exact time-$t$ flow map $\\Phi_{t}$ that sends the initial state $(q(0), p(0))$ to $(q(t), p(t))$, expressed as a single closed-form $2d \\times 2d$ block matrix in terms of $t$ and $\\Sigma$.\n\nAnswer specification:\n- Express the final flow map $\\Phi_{t}$ using trigonometric functions of $t$ with $t$ interpreted in radians.\n- Your final answer must be a single closed-form analytic expression.\n- No rounding is required; provide the exact expression.",
            "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Domain**: Computational systems biology, Hamiltonian Monte Carlo (HMC).\n- **Phase Space**: Canonical phase space $(q,p) \\in \\mathbb{R}^{d} \\times \\mathbb{R}^{d}$.\n- **Parameter Vector**: $\\theta \\in \\mathbb{R}^d$, identified with the position coordinate $q$.\n- **Posterior Distribution**: A Gaussian distribution, $\\pi(\\theta) = \\mathcal{N}(0, \\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive-definite covariance matrix.\n- **Potential Energy**: $U(q) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q$. This is derived from the negative log-posterior, since $-\\ln(\\pi(q)) = -\\ln(\\text{const} \\cdot \\exp(-\\frac{1}{2} q^{\\top} \\Sigma^{-1} q)) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q + \\text{const}$. The constant does not affect the dynamics.\n- **Kinetic Energy**: $K(p) = \\frac{1}{2} p^{\\top} \\Sigma p$.\n- **Hamiltonian**: $H(q,p) = U(q) + K(p) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q + \\frac{1}{2} p^{\\top} \\Sigma p$.\n- **Equations of Motion**: Hamilton’s equations, $\\dot{q} = \\frac{\\partial H}{\\partial p}$ and $\\dot{p} = - \\frac{\\partial H}{\\partial q}$.\n- **Task**:\n    1.  Derive the exact, continuous-time Hamiltonian flow.\n    2.  Show that the flow represents a set of decoupled harmonic oscillators with unit angular frequency.\n    3.  Provide the exact time-$t$ flow map $\\Phi_{t}$ that maps $(q(0), p(0))$ to $(q(t), p(t))$ as a single $2d \\times 2d$ block matrix.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is firmly grounded in Hamiltonian mechanics and its application in statistics (specifically, Hamiltonian Monte Carlo). The formulation of the Hamiltonian with a position-dependent metric (Riemannian Manifold HMC) is a well-established advanced technique. The choice of kinetic energy $K(p) = \\frac{1}{2}p^{\\top} M^{-1} p$ with a mass matrix $M = \\Sigma^{-1}$ is a standard choice for a Gaussian target distribution with covariance $\\Sigma$, to adapt to the geometry of the parameter space. The problem statement sets $M^{-1} = \\Sigma$, which is precisely this case. The mathematics are standard vector calculus and ordinary differential equations. The problem is scientifically sound.\n- **Well-Posedness**: The problem asks for the solution to a system of linear, time-invariant ordinary differential equations with given initial conditions. Such a problem has a unique and stable solution. The problem is well-posed.\n- **Objectivity**: The problem is stated in precise, objective mathematical language.\n- **Completeness and Consistency**: All necessary components (Hamiltonian, equations of motion) are provided. There are no contradictions.\n- **Feasibility and Realism**: The scenario is a theoretically important and practical special case in HMC research. It is fully realistic and feasible to solve.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe solution proceeds by first deriving Hamilton's equations for the given Hamiltonian, then demonstrating that the resulting system of linear ordinary differential equations corresponds to a set of decoupled harmonic oscillators, and finally solving these equations to find the flow map.\n\nThe Hamiltonian is given by $H(q, p) = U(q) + K(p) = \\frac{1}{2} q^{\\top} \\Sigma^{-1} q + \\frac{1}{2} p^{\\top} \\Sigma p$.\n\nFirst, we compute the gradients of $H(q,p)$ with respect to $q$ and $p$. For a symmetric matrix $A$, the gradient of the quadratic form $x^{\\top} A x$ with respect to $x$ is $2 A x$. Thus, the partial derivatives are:\n$$\n\\frac{\\partial H}{\\partial p} = \\frac{\\partial}{\\partial p} \\left( \\frac{1}{2} p^{\\top} \\Sigma p \\right) = \\frac{1}{2} (2 \\Sigma p) = \\Sigma p\n$$\n$$\n\\frac{\\partial H}{\\partial q} = \\frac{\\partial}{\\partial q} \\left( \\frac{1}{2} q^{\\top} \\Sigma^{-1} q \\right) = \\frac{1}{2} (2 \\Sigma^{-1} q) = \\Sigma^{-1} q\n$$\nHere, we have used the fact that since $\\Sigma$ is symmetric, its inverse $\\Sigma^{-1}$ is also symmetric.\n\nSubstituting these gradients into Hamilton's equations, $\\dot{q} = \\frac{\\partial H}{\\partial p}$ and $\\dot{p} = - \\frac{\\partial H}{\\partial q}$, we obtain the system of equations governing the dynamics:\n$$\n\\dot{q} = \\Sigma p\n$$\n$$\n\\dot{p} = -\\Sigma^{-1} q\n$$\nThis is a system of $2d$ coupled first-order linear ordinary differential equations.\n\nTo show that this system describes a set of decoupled harmonic oscillators, we perform a change of coordinates. Since $\\Sigma$ is a real, symmetric, positive-definite matrix, it admits a spectral decomposition $\\Sigma = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix ($V^{\\top}V = VV^{\\top} = I$, where $I$ is the $d \\times d$ identity matrix) whose columns are the eigenvectors of $\\Sigma$, and $\\Lambda$ is a diagonal matrix containing the corresponding positive eigenvalues $\\lambda_i$. The inverse is then $\\Sigma^{-1} = V \\Lambda^{-1} V^{\\top}$.\n\nLet us define a new set of coordinates $(\\tilde{q}, \\tilde{p})$ via the transformation:\n$$\n\\tilde{q} = V^{\\top} q \\quad \\implies \\quad q = V \\tilde{q}\n$$\n$$\n\\tilde{p} = V^{\\top} p \\quad \\implies \\quad p = V \\tilde{p}\n$$\nThe time derivatives are related by $\\dot{q} = V \\dot{\\tilde{q}}$ and $\\dot{p} = V \\dot{\\tilde{p}}$. Substituting these into the equations of motion:\nFor the $\\dot{q}$ equation:\n$$\nV \\dot{\\tilde{q}} = \\Sigma (V \\tilde{p}) = (V \\Lambda V^{\\top}) V \\tilde{p} = V \\Lambda \\tilde{p}\n$$\nMultiplying by $V^{\\top}$ from the left yields:\n$$\n\\dot{\\tilde{q}} = \\Lambda \\tilde{p}\n$$\nFor the $\\dot{p}$ equation:\n$$\nV \\dot{\\tilde{p}} = -\\Sigma^{-1} (V \\tilde{q}) = -(V \\Lambda^{-1} V^{\\top}) V \\tilde{q} = -V \\Lambda^{-1} \\tilde{q}\n$$\nMultiplying by $V^{\\top}$ from the left yields:\n$$\n\\dot{\\tilde{p}} = -\\Lambda^{-1} \\tilde{q}\n$$\nBecause $\\Lambda$ is a diagonal matrix, $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$, the system decouples into $d$ independent pairs of equations for each component $i \\in \\{1, \\dots, d\\}$:\n$$\n\\dot{\\tilde{q}}_i = \\lambda_i \\tilde{p}_i\n$$\n$$\n\\dot{\\tilde{p}}_i = -\\frac{1}{\\lambda_i} \\tilde{q}_i\n$$\nTo see that these are harmonic oscillators, we can differentiate the first equation with respect to time and substitute the second:\n$$\n\\ddot{\\tilde{q}}_i = \\lambda_i \\dot{\\tilde{p}}_i = \\lambda_i \\left(-\\frac{1}{\\lambda_i} \\tilde{q}_i\\right) = -\\tilde{q}_i\n$$\nThis gives the equation for a simple harmonic oscillator, $\\ddot{\\tilde{q}}_i + \\tilde{q}_i = 0$. The standard form is $\\ddot{x} + \\omega^2 x = 0$, so the angular frequency is $\\omega_i = 1$ for all $i=1, \\dots, d$. This confirms that the dynamics are those of a set of $d$ decoupled harmonic oscillators, all with unit angular frequency.\n\nNow, we solve for the time-$t$ flow map. The solution to $\\ddot{\\tilde{q}}_i + \\tilde{q}_i = 0$ is of the form $\\tilde{q}_i(t) = A \\cos(t) + B \\sin(t)$. Applying the initial conditions:\n$\\tilde{q}_i(0) = A$.\n$\\dot{\\tilde{q}}_i(0) = B$. From the first-order equations, $\\dot{\\tilde{q}}_i(0) = \\lambda_i \\tilde{p}_i(0)$.\nSo, the solution for $\\tilde{q}_i(t)$ is:\n$$\n\\tilde{q}_i(t) = \\tilde{q}_i(0) \\cos(t) + \\lambda_i \\tilde{p}_i(0) \\sin(t)\n$$\nFor $\\tilde{p}_i(t)$, we have $\\tilde{p}_i(t) = \\frac{1}{\\lambda_i} \\dot{\\tilde{q}}_i(t) = \\frac{1}{\\lambda_i} \\left( -\\tilde{q}_i(0) \\sin(t) + \\lambda_i \\tilde{p}_i(0) \\cos(t) \\right)$:\n$$\n\\tilde{p}_i(t) = -\\frac{1}{\\lambda_i} \\tilde{q}_i(0) \\sin(t) + \\tilde{p}_i(0) \\cos(t)\n$$\nWe can write this in vector form for the transformed variables:\n$$\n\\tilde{q}(t) = \\cos(t) \\tilde{q}(0) + \\sin(t) \\Lambda \\tilde{p}(0)\n$$\n$$\n\\tilde{p}(t) = -\\sin(t) \\Lambda^{-1} \\tilde{q}(0) + \\cos(t) \\tilde{p}(0)\n$$\nFinally, we transform back to the original coordinates $(q,p)$:\n$q(t) = V\\tilde{q}(t) = V(\\cos(t) \\tilde{q}(0) + \\sin(t) \\Lambda \\tilde{p}(0))$\nUsing $\\tilde{q}(0) = V^{\\top}q(0)$ and $\\tilde{p}(0) = V^{\\top}p(0)$:\n$q(t) = V(\\cos(t) V^{\\top}q(0) + \\sin(t) \\Lambda V^{\\top}p(0)) = \\cos(t) (VV^{\\top})q(0) + \\sin(t) (V \\Lambda V^{\\top})p(0)$\nSince $VV^{\\top}=I$ and $\\Sigma = V \\Lambda V^{\\top}$, we get:\n$$\nq(t) = \\cos(t) q(0) + \\sin(t) \\Sigma p(0)\n$$\nSimilarly for $p(t)$:\n$p(t) = V\\tilde{p}(t) = V(-\\sin(t) \\Lambda^{-1} \\tilde{q}(0) + \\cos(t) \\tilde{p}(0))$\n$p(t) = V(-\\sin(t) \\Lambda^{-1} V^{\\top}q(0) + \\cos(t) V^{\\top}p(0)) = -\\sin(t) (V \\Lambda^{-1} V^{\\top})q(0) + \\cos(t) (VV^{\\top})p(0)$\nSince $\\Sigma^{-1} = V \\Lambda^{-1} V^{\\top}$, we get:\n$$\np(t) = -\\sin(t) \\Sigma^{-1} q(0) + \\cos(t) p(0)\n$$\nThis defines the time-$t$ flow map $\\Phi_t$ such that $\\begin{pmatrix} q(t) \\\\ p(t) \\end{pmatrix} = \\Phi_t \\begin{pmatrix} q(0) \\\\ p(0) \\end{pmatrix}$. Writing this in a $2d \\times 2d$ block matrix form:\n$$\n\\begin{pmatrix} q(t) \\\\ p(t) \\end{pmatrix} = \\begin{pmatrix} I \\cos(t) & \\Sigma \\sin(t) \\\\ -\\Sigma^{-1} \\sin(t) & I \\cos(t) \\end{pmatrix} \\begin{pmatrix} q(0) \\\\ p(0) \\end{pmatrix}\n$$\nThe flow map $\\Phi_t$ is therefore the $2d \\times 2d$ matrix on the right-hand side.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\cos(t) I & \\sin(t) \\Sigma \\\\\n-\\sin(t) \\Sigma^{-1} & \\cos(t) I\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the theory of HMC provides a powerful sampling engine, in practice we must always diagnose whether our finite-length simulations have adequately explored the posterior distribution. This hands-on practice guides you through the calculation of two cornerstone MCMC diagnostics: the Gelman-Rubin potential scale reduction factor, $\\widehat{R}$, and the effective sample size, $N_{eff}$ . Mastering these tools is essential for ensuring the reliability of parameter estimates obtained from complex dynamical systems models in biology.",
            "id": "3318340",
            "problem": "In a systems biology study, a transcriptional regulation model described by a nonlinear ordinary differential equation is fit to time-course data using Hamiltonian Monte Carlo (HMC). Let $\\boldsymbol{\\theta}$ denote the kinetic parameters, and define the derived quantity of interest $g(\\boldsymbol{\\theta})$ as the predicted steady-state protein abundance under a fixed inducer concentration determined by the model’s dynamical equilibrium. Four independent HMC chains are run and post-warmup draws are obtained for $g(\\boldsymbol{\\theta})$. For each chain $k \\in \\{1,2,3,4\\}$, denote the draws by $y_{k,1},\\dots,y_{k,n}$, where $n$ is the number of post-warmup iterations per chain.\n\nYou are given that $m=4$ and $n=2000$. The chain-wise summary statistics for $g(\\boldsymbol{\\theta})$ are:\n- Chain $1$: sample mean $42.5$, sample variance $9.1$.\n- Chain $2$: sample mean $41.5$, sample variance $8.9$.\n- Chain $3$: sample mean $42.2$, sample variance $9.2$.\n- Chain $4$: sample mean $41.8$, sample variance $9.0$.\n\nWithin each chain, the marginal dynamics of $y_{k,i}$ may be approximated by an autoregressive process of order $1$, with lag-one autocorrelation coefficient $\\phi_{k}$ for the transformed series $y_{k,i}$:\n- Chain $1$: $\\phi_{1} = 0.70$.\n- Chain $2$: $\\phi_{2} = 0.65$.\n- Chain $3$: $\\phi_{3} = 0.72$.\n- Chain $4$: $\\phi_{4} = 0.68$.\n\nUsing only these summaries:\n1. Compute the classic (non-split) Gelman–Rubin potential scale reduction factor $\\widehat{R}$ for $g(\\boldsymbol{\\theta})$ from first principles.\n2. Compute a multi-chain effective sample size for $g(\\boldsymbol{\\theta})$ by aggregating the within-chain autocorrelations via the simple average across chains at each lag, and using the integrated autocorrelation time under the autoregressive order $1$ approximation for the aggregated autocorrelation.\n3. Based on the commonly adopted criteria $\\widehat{R} < 1.01$ and effective sample size at least $1000$ for $g(\\boldsymbol{\\theta})$, decide whether sampling for this derived quantity has converged satisfactorily.\n\nProvide as your final answer the computed $\\widehat{R}$, rounded to four significant figures. No units are required for the final answer.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the established principles of Bayesian statistics and Markov chain Monte Carlo (MCMC) diagnostics, specifically Hamiltonian Monte Carlo (HMC) in a systems biology context. The problem is well-posed, providing all necessary data—number of chains, iterations per chain, chain-wise summary statistics (means and variances), and lag-one autocorrelation coefficients—to compute the requested quantities. The language is objective and the data are consistent and numerically plausible. The tasks are specific and formalizable, leading to a unique solution.\n\nThe problem requires a three-part analysis: (1) computation of the Gelman–Rubin statistic $\\widehat{R}$, (2) computation of the multi-chain effective sample size $N_{eff}$, and (3) a conclusion on sampling convergence based on these two metrics.\n\n### Step 1: Compute the Gelman–Rubin Statistic $\\widehat{R}$\n\nThe Gelman–Rubin potential scale reduction factor, $\\widehat{R}$, is calculated to diagnose convergence by comparing the within-chain and between-chain variances of the MCMC draws. The provided data are:\n- Number of chains, $m=4$.\n- Number of post-warmup draws per chain, $n=2000$.\n- Chain-wise means $(\\bar{y}_k)$: $\\bar{y}_1=42.5$, $\\bar{y}_2=41.5$, $\\bar{y}_3=42.2$, $\\bar{y}_4=41.8$.\n- Chain-wise variances $(s_k^2)$: $s_1^2=9.1$, $s_2^2=8.9$, $s_3^2=9.2$, $s_4^2=9.0$.\n\nFirst, we calculate the average of the within-chain variances, denoted by $W$.\n$$W = \\frac{1}{m} \\sum_{k=1}^{m} s_k^2$$\nSubstituting the given values:\n$$W = \\frac{1}{4}(9.1 + 8.9 + 9.2 + 9.0) = \\frac{36.2}{4} = 9.05$$\n\nNext, we calculate the between-chain variance, $B$, which is the variance of the chain means, scaled by $n$. First, we compute the grand mean, $\\bar{\\bar{y}}$, which is the mean of all draws from all chains.\n$$\\bar{\\bar{y}} = \\frac{1}{m} \\sum_{k=1}^{m} \\bar{y}_k$$\n$$\\bar{\\bar{y}} = \\frac{1}{4}(42.5 + 41.5 + 42.2 + 41.8) = \\frac{168}{4} = 42.0$$\nNow, we calculate $B$:\n$$B = \\frac{n}{m-1} \\sum_{k=1}^{m} (\\bar{y}_k - \\bar{\\bar{y}})^2$$\n$$B = \\frac{2000}{4-1} \\left( (42.5-42.0)^2 + (41.5-42.0)^2 + (42.2-42.0)^2 + (41.8-42.0)^2 \\right)$$\n$$B = \\frac{2000}{3} \\left( (0.5)^2 + (-0.5)^2 + (0.2)^2 + (-0.2)^2 \\right)$$\n$$B = \\frac{2000}{3} (0.25 + 0.25 + 0.04 + 0.04) = \\frac{2000}{3} (0.58) = \\frac{1160}{3}$$\n\nThe estimated marginal posterior variance of the estimand, $\\widehat{\\text{var}}^+$, is a weighted average of $W$ and $B$.\n$$\\widehat{\\text{var}}^+ = \\frac{n-1}{n} W + \\frac{B}{n}$$\n$$\\widehat{\\text{var}}^+ = \\frac{2000-1}{2000} (9.05) + \\frac{1}{2000} \\left( \\frac{1160}{3} \\right) = \\frac{1999}{2000} (9.05) + \\frac{1160}{6000}$$\n$$\\widehat{\\text{var}}^+ \\approx (0.9995)(9.05) + 0.19333... \\approx 9.045475 + 0.193333... \\approx 9.238808...$$\nTo compute exactly:\n$$\\widehat{\\text{var}}^+ = \\frac{1999}{2000} \\frac{905}{100} + \\frac{1160}{6000} = \\frac{1809095}{200000} + \\frac{116}{600} = \\frac{361819}{40000} + \\frac{116}{600} = \\frac{1085457 + 46400}{1200000} = \\frac{1131857}{120000}$$\nWait, there is an arithmetic error above. Let's re-calculate:\n$\\frac{1160}{6000} = \\frac{116}{600} = \\frac{29}{150}$.\n$\\widehat{\\text{var}}^+ = \\frac{1999}{2000}\\frac{181}{20} + \\frac{1160}{6000} = \\frac{361819}{40000} + \\frac{1160}{6000} = \\frac{3 \\times 361819 + 2 \\times 11600}{120000} = \\frac{1085457 + 23200}{120000} = \\frac{1108657}{120000}$.\nThis decimal value is $9.238808333...$ as before. The first attempt at fraction arithmetic was flawed.\n\nFinally, the potential scale reduction factor $\\widehat{R}$ is the square root of the ratio of the total estimated variance to the within-chain variance.\n$$\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+}{W}}$$\n$$\\widehat{R} = \\sqrt{\\frac{1108657/120000}{9.05}} = \\sqrt{\\frac{1108657/120000}{181/20}} = \\sqrt{\\frac{1108657}{120000} \\cdot \\frac{20}{181}} = \\sqrt{\\frac{1108657}{6000 \\cdot 181}} = \\sqrt{\\frac{1108657}{1086000}}$$\n$$\\widehat{R} \\approx \\sqrt{1.020862799...} \\approx 1.0103775...$$\n\n### Step 2: Compute the Multi-Chain Effective Sample Size $N_{eff}$\n\nThe effective sample size, $N_{eff}$, adjusts the total number of samples, $N=mn$, for the autocorrelation present in the MCMC draws. $N_{eff} = N / \\tau$, where $\\tau$ is the integrated autocorrelation time.\nThe problem requires aggregating the within-chain autocorrelations by averaging at each lag $t$. For an AR(1) process, the autocorrelation at lag $t$ for chain $k$ is $\\rho_t^{(k)} = \\phi_k^t$.\nThe aggregated autocorrelation at lag $t$ is $\\bar{\\rho}_t = \\frac{1}{m} \\sum_{k=1}^{m} \\rho_t^{(k)} = \\frac{1}{m} \\sum_{k=1}^{m} \\phi_k^t$.\nThe integrated autocorrelation time, $\\tau$, is then:\n$$\\tau = 1 + 2 \\sum_{t=1}^{\\infty} \\bar{\\rho}_t = 1 + 2 \\sum_{t=1}^{\\infty} \\left( \\frac{1}{m} \\sum_{k=1}^{m} \\phi_k^t \\right)$$\nBy swapping the order of summation:\n$$\\tau = 1 + \\frac{2}{m} \\sum_{k=1}^{m} \\left( \\sum_{t=1}^{\\infty} \\phi_k^t \\right)$$\nThe inner sum is a geometric series: $\\sum_{t=1}^{\\infty} \\phi_k^t = \\frac{\\phi_k}{1-\\phi_k}$. Thus:\n$$\\tau = 1 + \\frac{2}{m} \\sum_{k=1}^{m} \\frac{\\phi_k}{1-\\phi_k}$$\nThe given autocorrelation coefficients are $\\phi_1=0.70$, $\\phi_2=0.65$, $\\phi_3=0.72$, $\\phi_4=0.68$.\nLet's compute the terms $\\frac{\\phi_k}{1-\\phi_k}$:\n- $k=1$: $\\frac{0.70}{1-0.70} = \\frac{0.70}{0.30} = \\frac{7}{3}$\n- $k=2$: $\\frac{0.65}{1-0.65} = \\frac{0.65}{0.35} = \\frac{13}{7}$\n- $k=3$: $\\frac{0.72}{1-0.72} = \\frac{0.72}{0.28} = \\frac{18}{7}$\n- $k=4$: $\\frac{0.68}{1-0.68} = \\frac{0.68}{0.32} = \\frac{17}{8}$\nNow, we compute $\\tau$:\n$$\\tau = 1 + \\frac{2}{4} \\left( \\frac{7}{3} + \\frac{13}{7} + \\frac{18}{7} + \\frac{17}{8} \\right) = 1 + \\frac{1}{2} \\left( \\frac{7}{3} + \\frac{31}{7} + \\frac{17}{8} \\right)$$\nUsing a common denominator of $168$:\n$$\\tau = 1 + \\frac{1}{2} \\left( \\frac{7 \\cdot 56}{168} + \\frac{31 \\cdot 24}{168} + \\frac{17 \\cdot 21}{168} \\right) = 1 + \\frac{1}{2} \\left( \\frac{392 + 744 + 357}{168} \\right)$$\n$$\\tau = 1 + \\frac{1}{2} \\left( \\frac{1493}{168} \\right) = 1 + \\frac{1493}{336} = \\frac{336+1493}{336} = \\frac{1829}{336} \\approx 5.44345$$\nThe total number of samples is $N = m \\times n = 4 \\times 2000 = 8000$.\nThe effective sample size is:\n$$N_{eff} = \\frac{N}{\\tau} = \\frac{8000}{1829/336} = \\frac{8000 \\times 336}{1829} = \\frac{2688000}{1829} \\approx 1469.65$$\n\n### Step 3: Conclusion on Sampling Convergence\n\nThe convergence criteria are given as $\\widehat{R} < 1.01$ and $N_{eff} \\ge 1000$.\n- Our calculated $\\widehat{R}$ is $\\approx 1.0104$. This value does not satisfy the condition $\\widehat{R} < 1.01$.\n- Our calculated $N_{eff}$ is $\\approx 1470$. This value satisfies the condition $N_{eff} \\ge 1000$.\n\nSince one of the two criteria ($\\widehat{R} < 1.01$) is not met, the sampling for the derived quantity $g(\\boldsymbol{\\theta})$ has not converged satisfactorily according to the specified standards. The between-chain variance is still slightly too high relative to the within-chain variance, indicating that the chains have not fully explored the same posterior distribution.\n\nThe final answer requested is the value of $\\widehat{R}$ rounded to four significant figures.\n$\\widehat{R} = 1.0103775...$\nThe first four significant figures are $1$, $0$, $1$, and $0$. The fifth significant figure is $3$. Since $3 < 5$, we round down, which gives $1.010$.",
            "answer": "$$\\boxed{1.010}$$"
        },
        {
            "introduction": "Beyond general convergence checks, Hamiltonian Monte Carlo offers unique diagnostics that probe the health of the Hamiltonian dynamics themselves. This exercise introduces the energy Bayesian fraction of missing information (E-BFMI), a metric that quantifies the sampler's ability to explore different energy levels, which is crucial for efficient posterior exploration . By computing and interpreting E-BFMI, you will learn to detect subtle inefficiencies in the HMC sampler that might indicate a poorly chosen mass matrix or other tuning problems.",
            "id": "3318373",
            "problem": "In a parameter inference study of a phosphorylation–dephosphorylation cascade modeled as a deterministic dynamical system with ordinary differential equations, you employ Hamiltonian Monte Carlo (HMC) to draw samples from the posterior over kinetic parameters. Denote the position by $q$ and momentum by $p$, the Hamiltonian by $H(q,p) = U(q) + K(p)$ with potential energy $U(q)$ equal to the negative log posterior density up to an additive constant, and kinetic energy $K(p)$ induced by a Gaussian momentum distribution. At each HMC iteration, a fresh momentum $p$ is drawn from a Gaussian distribution with mass matrix $M$, and a symplectic integrator approximately conserves the Hamiltonian along a trajectory; an accept–reject step corrects for numerical error. The stationary marginal distribution of the Hamiltonian values $E = H(q,p)$ along the chain quantifies the typical energy scales, while the resampled momentum $p$ drives vertical moves in energy space.\n\nThe energy Bayesian fraction of missing information (E-BFMI) is a dimensionless diagnostic that quantifies how effectively the resampled momentum injects kinetic energy to explore the marginal energy distribution. Starting from the definitions above and the interpretation of E-BFMI as the ratio of the expected squared vertical change in energy induced by momentum resampling to the variance of the marginal stationary energy distribution, derive a consistent estimator for E-BFMI from a single sequence of observed Hamiltonian energies $\\{E_{n}\\}_{n=1}^{N}$ recorded at successive iterations of a stationary HMC chain. Then, using your derived estimator, compute E-BFMI for the following sequence of $N=10$ consecutive post–warmup Hamiltonian energies (in arbitrary units) obtained from fitting the cascade model:\n$E_{1}=100.0$, $E_{2}=100.2$, $E_{3}=100.4$, $E_{4}=100.6$, $E_{5}=100.8$, $E_{6}=101.0$, $E_{7}=101.2$, $E_{8}=101.4$, $E_{9}=101.6$, $E_{10}=101.8$.\n\nReport the numerical value of E-BFMI. Round your answer to three significant figures. In your reasoning, explain briefly why low values of E-BFMI correspond to poor momentum energy exploration in HMC for this dynamical systems context. The final reported answer must be the computed E-BFMI value only.",
            "solution": "The problem is validated as scientifically sound, well-posed, and self-contained. It presents a clear task: to derive a consistent estimator for the Energy Bayesian Fraction of Missing Information (E-BFMI) from its provided definition and then compute its value for a given sequence of Hamiltonian energies. All concepts are standard in the field of computational statistics and its application to systems biology.\n\nThe problem defines E-BFMI as the ratio of the expected squared vertical change in energy induced by momentum resampling to the variance of the marginal stationary energy distribution. Let $\\{E_n\\}_{n=1}^{N}$ be a sequence of Hamiltonian energy values from a stationary HMC chain. The Hamiltonian is $H(q,p) = U(q) + K(p)$, and its value at iteration $n$ is $E_n$.\n\nFirst, we formalize the denominator: the variance of the marginal stationary energy distribution, $\\text{Var}(E)$. A consistent estimator for this variance, based on the observed sample $\\{E_n\\}$, is the sample variance:\n$$ \\widehat{\\text{Var}}(E) = \\frac{1}{N-1} \\sum_{n=1}^{N} (E_n - \\Bar{E})^2 $$\nwhere $\\Bar{E} = \\frac{1}{N} \\sum_{n=1}^{N} E_n$ is the sample mean of the energies.\n\nNext, we formalize the numerator: the expected squared vertical change in energy induced by momentum resampling. In an HMC iteration, the primary stochastic perturbation that allows the sampler to move between energy levels is the resampling of the momentum variables. The net change in energy from one accepted state to the next, $\\Delta E_n = E_n - E_{n-1}$, captures the result of this process. The expected squared change is thus $\\mathbb{E}[(\\Delta E)^2]$. A consistent estimator for this quantity is the sample mean of the squared successive differences:\n$$ \\widehat{\\mathbb{E}}[(\\Delta E)^2] = \\frac{1}{N-1} \\sum_{n=2}^{N} (E_n - E_{n-1})^2 $$\nNote that the sum runs from $n=2$ to $N$ as there are $N-1$ such differences.\n\nThe estimator for E-BFMI, denoted $\\widehat{\\text{E-BFMI}}$, is the ratio of the estimator for the numerator to the estimator for the denominator:\n$$ \\widehat{\\text{E-BFMI}} = \\frac{\\frac{1}{N-1} \\sum_{n=2}^{N} (E_n - E_{n-1})^2}{\\frac{1}{N-1} \\sum_{n=1}^{N} (E_n - \\Bar{E})^2} $$\nThe factor of $\\frac{1}{N-1}$ cancels, yielding the final form of the estimator:\n$$ \\widehat{\\text{E-BFMI}} = \\frac{\\sum_{n=2}^{N} (E_n - E_{n-1})^2}{\\sum_{n=1}^{N} (E_n - \\Bar{E})^2} $$\n\nNow, we apply this estimator to the provided data sequence of $N=10$ energies:\n$E = \\{100.0, 100.2, 100.4, 100.6, 100.8, 101.0, 101.2, 101.4, 101.6, 101.8\\}$.\n\nFirst, we compute the numerator, the sum of squared successive differences. The sequence is an arithmetic progression with a constant difference of $d=0.2$. Therefore, $E_n - E_{n-1} = 0.2$ for all $n \\in \\{2, \\dots, 10\\}$.\n$$ \\text{Numerator} = \\sum_{n=2}^{10} (E_n - E_{n-1})^2 = \\sum_{n=2}^{10} (0.2)^2 $$\nThere are $N-1 = 9$ terms in this sum.\n$$ \\text{Numerator} = 9 \\times (0.2)^2 = 9 \\times 0.04 = 0.36 $$\n\nNext, we compute the denominator, the sum of squared deviations from the mean. First, we find the sample mean, $\\Bar{E}$. For an arithmetic progression, the mean is the average of the first and last terms:\n$$ \\Bar{E} = \\frac{E_1 + E_{10}}{2} = \\frac{100.0 + 101.8}{2} = \\frac{201.8}{2} = 100.9 $$\nNow, we compute the deviations $(E_n - \\Bar{E})$:\n$\\{100.0-100.9, 100.2-100.9, \\dots, 101.8-100.9\\} = \\{-0.9, -0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7, 0.9\\}$\nWe then compute the sum of the squares of these deviations:\n$$ \\text{Denominator} = \\sum_{n=1}^{10} (E_n - \\Bar{E})^2 = (-0.9)^2 + (-0.7)^2 + (-0.5)^2 + (-0.3)^2 + (-0.1)^2 + (0.1)^2 + (0.3)^2 + (0.5)^2 + (0.7)^2 + (0.9)^2 $$\n$$ \\text{Denominator} = 0.81 + 0.49 + 0.25 + 0.09 + 0.01 + 0.01 + 0.09 + 0.25 + 0.49 + 0.81 $$\n$$ \\text{Denominator} = 2 \\times (0.81 + 0.49 + 0.25 + 0.09 + 0.01) = 2 \\times 1.65 = 3.3 $$\n\nFinally, we compute the E-BFMI value:\n$$ \\widehat{\\text{E-BFMI}} = \\frac{\\text{Numerator}}{\\text{Denominator}} = \\frac{0.36}{3.3} = \\frac{36}{330} = \\frac{6}{55} \\approx 0.109090... $$\nRounding to three significant figures, the result is $0.109$.\n\nA low value of E-BFMI, such as the one computed, indicates poor exploration of the energy landscape. The numerator, $\\sum (E_n - E_{n-1})^2$, quantifies the squared \"jumps\" in energy between successive HMC iterations, which are driven by the kinetic energy injected by momentum resampling. The denominator, $\\sum (E_n - \\Bar{E})^2$, quantifies the total variance of the energy across the posterior, representing the size of the energy landscape that needs to be explored. A small E-BFMI value implies that the inter-iteration energy jumps are small compared to the overall energy variance. This means the sampler is taking very small steps in energy space, leading to a slow, highly correlated exploration of the posterior distribution of the kinetic parameters. For the dynamical systems model, this indicates that the HMC sampler is inefficient and may struggle to escape local modes of the posterior, requiring a very large number of iterations to produce reliable inferences. This is often caused by a poorly tuned mass matrix $M$ that is misaligned with the geometry of the potential energy surface $U(q)$.",
            "answer": "$$ \\boxed{0.109} $$"
        }
    ]
}