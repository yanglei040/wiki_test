{
    "hands_on_practices": [
        {
            "introduction": "在深入研究复杂的数值方法之前，至关重要的是首先掌握轮廓似然的数学基础。本练习提供了一个在建模中最常见情景下的实践推导：处理未知的测量噪声。通过解析地“剖出”噪声方差 $\\sigma^2$，我们将揭示感兴趣参数 $\\theta$ 的似然函数如何与我们所熟悉的概念——残差平方和——直接相关。",
            "id": "3340975",
            "problem": "考虑一个计算系统生物学中的单输出动态模型，该模型由一个常微分方程（ODE）描述，它通过一个平滑观测函数将参数向量 $\\theta \\in \\mathbb{R}^{p}$ 映射到一个预测的测量轨迹。设观测数据为 $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$，其中 $t_{i}$ 是已知的采样时间，观测模型为\n$$\ny_{i} = f(t_{i}; \\theta) + \\varepsilon_{i},\n$$\n其中测量误差 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$ 是独立同分布的，$\\sigma^{2} > 0$ 是一个未知的滋扰方差。假设 ODE 和观测映射使得对于所有的 $i$ 和参数域中所有的 $\\theta$，$f(t_{i}; \\theta)$ 都是良定义且有限的。\n\n从高斯似然的定义和观测的独立性出发，写出数据的对数似然 $\\ell(\\theta, \\sigma^{2})$。然后，对于一个固定的 $\\theta$，在定义域 $\\sigma^{2} > 0$ 上关于 $\\sigma^{2}$ 解析地最大化 $\\ell(\\theta, \\sigma^{2})$，并将最大化者代回 $\\ell(\\theta, \\sigma^{2})$ 中，以获得仅作为 $\\theta$ 函数的轮廓对数似然 $\\tilde{\\ell}(\\theta)$。以残差平方和\n$$\nS(\\theta) = \\sum_{i=1}^{n}\\left(y_{i} - f(t_{i}; \\theta)\\right)^{2}\n$$\n的形式，用闭式表达你的最终结果。\n提供 $\\tilde{\\ell}(\\theta)$ 的单一解析表达式作为最终答案。不要省略不依赖于 $\\theta$ 的加性常数。你的最终答案应该是一个没有单位且无需四舍五入的单一表达式。",
            "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤1：提取已知条件\n-   **模型：** 一个单输出动态模型，其中参数向量 $\\theta \\in \\mathbb{R}^{p}$ 映射到一个预测轨迹 $f(t; \\theta)$。\n-   **数据：** 在已知采样时间 $t_i$ 上的 $n$ 个观测值集合 $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$。\n-   **观测模型：** 数据与模型之间的关系由 $y_{i} = f(t_{i}; \\theta) + \\varepsilon_{i}$ 给出。\n-   **误差结构：** 测量误差 $\\varepsilon_{i}$ 是独立同分布（i.i.d.）的，服从均值为 $0$、未知方差为 $\\sigma^{2} > 0$ 的正态分布，记为 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$。\n-   **函数性质：** 观测映射 $f(t_{i}; \\theta)$ 对所有相关输入都是良定义且有限的。\n-   **定义的量：** 残差平方和定义为 $S(\\theta) = \\sum_{i=1}^{n}\\left(y_{i} - f(t_{i}; \\theta)\\right)^{2}$。\n-   **目标：** 通过先写出完整对数似然 $\\ell(\\theta, \\sigma^2)$，然后就 $\\sigma^2$ 对其进行解析最大化，来推导轮廓对数似然 $\\tilde{\\ell}(\\theta)$。最终表达式应以 $S(\\theta)$ 表示，并包含所有常数。\n\n### 步骤2：使用提取的已知条件进行验证\n-   **科学依据：** 该问题具有科学依据。它描述了带有加性高斯噪声的非线性回归模型中参数估计的标准统计框架。这是所有定量STEM领域（包括计算系统生物学）中一种基础且广泛使用的方法。\n-   **适定性：** 该问题是适定的。它提供了执行所要求的推导所需的所有必要信息和定义。目标清晰明确，能够导出一个唯一的解析解。\n-   **客观性：** 该问题使用精确、客观的数学语言陈述，没有任何主观或有偏见的措辞。\n\n### 步骤3：结论与行动\n该问题被认为是**有效的**，因为它科学合理、适定、客观，并符合有效科学问题的所有标准。现在将开始求解过程。\n\n---\n\n轮廓对数似然函数 $\\tilde{\\ell}(\\theta)$ 的推导过程如下。\n\n首先，我们构建似然函数 $L(\\theta, \\sigma^2)$。假设误差 $\\varepsilon_i$ 从一个独立同分布的正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取，这意味着每个观测值 $y_i$ 是一个来自均值为 $f(t_i; \\theta)$、方差为 $\\sigma^2$ 的正态分布的随机变量。单个观测值 $y_i$ 的概率密度函数 (PDF) 为：\n$$\np(y_i | \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - f(t_i; \\theta))^2}{2\\sigma^2} \\right)\n$$\n由于观测的独立性，整个数据集（包含 $n$ 个观测值）的似然是各个概率密度函数的乘积：\n$$\nL(\\theta, \\sigma^2) = \\prod_{i=1}^{n} p(y_i | \\theta, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - f(t_i; \\theta))^2}{2\\sigma^2} \\right)\n$$\n该表达式可以合并为：\n$$\nL(\\theta, \\sigma^2) = \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - f(t_i; \\theta))^2 \\right)\n$$\n使用所提供的残差平方和定义 $S(\\theta) = \\sum_{i=1}^{n} (y_i - f(t_i; \\theta))^2$，似然函数简化为：\n$$\nL(\\theta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{S(\\theta)}{2\\sigma^2} \\right)\n$$\n接下来，我们通过取 $L(\\theta, \\sigma^2)$ 的自然对数来确定对数似然函数 $\\ell(\\theta, \\sigma^2)$：\n$$\n\\ell(\\theta, \\sigma^2) = \\ln(L(\\theta, \\sigma^2)) = \\ln\\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{S(\\theta)}{2\\sigma^2} \\right) \\right)\n$$\n利用对数的性质 $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ 和 $\\ln(a^b) = b \\ln(a)$，我们得到：\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2}\n$$\n这可以展开为：\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2}\n$$\n为了找到轮廓对数似然 $\\tilde{\\ell}(\\theta)$，我们必须首先对于一个固定的参数向量 $\\theta$，就滋扰参数 $\\sigma^2$ 对 $\\ell(\\theta, \\sigma^2)$ 进行最大化。我们通过求 $\\ell(\\theta, \\sigma^2)$ 关于 $\\sigma^2$ 的偏导数并将其设为零来找到临界点：\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2} \\right) = -\\frac{n}{2\\sigma^2} + \\frac{S(\\theta)}{2(\\sigma^2)^2}\n$$\n将导数设为零以求得估计量 $\\hat{\\sigma}^2(\\theta)$：\n$$\n-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{S(\\theta)}{2(\\hat{\\sigma}^2)^2} = 0\n$$\n假设 $S(\\theta) > 0$ 且因此 $\\hat{\\sigma}^2 > 0$，我们可以乘以 $2(\\hat{\\sigma}^2)^2$：\n$$\n-n\\hat{\\sigma}^2 + S(\\theta) = 0\n$$\n求解 $\\hat{\\sigma}^2$ 得到以 $\\theta$为条件的方差的最大似然估计量：\n$$\n\\hat{\\sigma}^2(\\theta) = \\frac{S(\\theta)}{n}\n$$\n为确认这是一个最大值，我们检查二阶导数：\n$$\n\\frac{\\partial^2 \\ell}{(\\partial \\sigma^2)^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2\\sigma^2} + \\frac{S(\\theta)}{2(\\sigma^2)^2} \\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{S(\\theta)}{(\\sigma^2)^3}\n$$\n在 $\\sigma^2 = \\hat{\\sigma}^2(\\theta) = S(\\theta)/n$ 处求值：\n$$\n\\left. \\frac{\\partial^2 \\ell}{(\\partial \\sigma^2)^2} \\right|_{\\sigma^2=\\hat{\\sigma}^2} = \\frac{n}{2(S(\\theta)/n)^2} - \\frac{S(\\theta)}{(S(\\theta)/n)^3} = \\frac{n^3}{2S(\\theta)^2} - \\frac{n^3}{S(\\theta)^2} = -\\frac{n^3}{2S(\\theta)^2}\n$$\n对于一个非平凡的模型拟合，$S(\\theta)>0$，并且由于 $n>0$，二阶导数为负。这证实了 $\\hat{\\sigma}^2(\\theta) = S(\\theta)/n$ 是一个局部最大值。由于它是定义域 $\\sigma^2 > 0$ 中唯一的临界点，因此它也是全局最大值。\n\n最后，通过将 $\\hat{\\sigma}^2(\\theta)$ 代回对数似然函数 $\\ell(\\theta, \\sigma^2)$，得到轮廓对数似然 $\\tilde{\\ell}(\\theta)$：\n$$\n\\tilde{\\ell}(\\theta) = \\ell(\\theta, \\hat{\\sigma}^2(\\theta)) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S(\\theta)}{n}\\right) - \\frac{S(\\theta)}{2\\left(\\frac{S(\\theta)}{n}\\right)}\n$$\n最后一项简化为 $\\frac{n}{2}$。因此，我们有：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S(\\theta)}{n}\\right) - \\frac{n}{2}\n$$\n通过组合对数项和常数，该表达式可以进一步紧凑化：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\left[ \\ln(2\\pi) + \\ln(S(\\theta)) - \\ln(n) + 1 \\right]\n$$\n使用 $\\ln(e) = 1$ 并组合方括号内的项：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\left[ \\ln\\left(\\frac{2\\pi S(\\theta)}{n}\\right) + \\ln(e) \\right]\n$$\n这导出了轮廓对数似然的最终闭式表达式：\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\ln\\left(\\frac{2\\pi e S(\\theta)}{n}\\right)\n$$\n这就是所需的关于 $\\theta$ 的函数，并保留了所有常数。",
            "answer": "$$\n\\boxed{-\\frac{n}{2} \\ln\\left(\\frac{2\\pi e S(\\theta)}{n}\\right)}\n$$"
        },
        {
            "introduction": "本练习将理论付诸实践，通过数值计算轮廓似然来评估实验数据中的参数可辨识性。它解决了一个在系统生物学中普遍存在的挑战：单一实验可能无法提供足够的信息来唯一确定所有模型参数。通过这个编程实践，您将看到如何通过汇集来自多个精心设计的实验的数据来打破这种不可辨识性，从而获得定义明确的参数估计，这是稳健建模的基石。",
            "id": "3340990",
            "problem": "考虑一个单室信号模型，其中受体或下游效应器的活化组分 $x(t)$ 在恒定输入 $u(t)$ 和两个动力学过程下演变：速率为 $k_{\\mathrm{on}}$ 的激活过程和速率为 $k_{\\mathrm{off}}$ 的失活过程。状态方程为常微分方程 $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$，初始条件为 $x(0) = 0$。测量模型为 $y(t) = s\\,x(t) + \\epsilon(t)$，其中 $s$ 是一个正的观测尺度，$\\epsilon(t)$ 是均值为零、方差 $\\sigma^2$ 已知的加性高斯噪声。在恒定输入 $u(t) = u_0$ 和足够长的采集时间 $t \\to \\infty$ 下，系统达到稳态 $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$，因此长时间后的测量值满足 $y \\approx s\\,x_\\infty$。\n\n两个实验在作用于动力学参数的不同抑制剂下探测同一个模型，这些抑制剂的作用是乘性的：\n- 实验 $1$ 修改激活过程：$k_{\\mathrm{on}} \\mapsto \\alpha\\,k_{\\mathrm{on}}$，其中 $\\alpha \\in (0,1]$，并保持失活过程不变。\n- 实验 $2$ 修改失活过程：$k_{\\mathrm{off}} \\mapsto \\beta\\,k_{\\mathrm{off}}$，其中 $\\beta \\ge 1$，并保持激活过程不变。\n\n对于每个实验 $i \\in \\{1,2\\}$，定义有效稳态组分 $x_{\\infty,i} = \\dfrac{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0}{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0 + \\beta_i\\,k_{\\mathrm{off}}}$ 和相应的测量值 $y_i = s\\,x_{\\infty,i} + \\epsilon_i$。为本题目的，假设噪声实现 $\\epsilon_i$ 是已知的确定性偏移量，并且方差 $\\sigma^2$ 对于似然函数是已知的。\n\n令 $a = k_{\\mathrm{on}}\\,u_0$ 和 $b = k_{\\mathrm{off}}$。给定在 $\\{(\\alpha_i,\\beta_i)\\}$ 条件下的测量值 $\\{y_i\\}$，高斯负对数似然（在忽略一个加性常数的情况下）与归一化残差平方和 $\\sum_i \\left(\\dfrac{y_i - s\\,\\dfrac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}}{\\sigma}\\right)^2$ 成正比。参数值 $s = \\vartheta$ 的轮廓似然是通过在 $a > 0$ 和 $b > 0$ 的约束下，将 $s$ 固定在 $s = \\vartheta$ 并对讨厌参数 $(a,b)$ 最小化该平方和得到的。\n\n您的任务是实现一个完整的程序，该程序能够：\n- 为指定的实验设计和已知的真实参数生成合成的稳态数据 $y_i$。\n- 通过对每个固定的 $s$ 在 $(a,b)$ 上最小化归一化残差平方和，从而在 $s$ 值的网格上计算联合轮廓似然。\n- 使用似然比准则和卡方截断值来评估 $s$ 的结构可辨识性。具体来说，对于给定的设计，计算轮廓 $\\mathrm{PL}(s)$ 作为最小化的归一化残差平方和。令 $\\mathrm{PL}_{\\min}$ 为其在网格上的最小值。$s$ 的 $95\\%$ 置信集为 $\\{s \\mid \\mathrm{PL}(s) - \\mathrm{PL}_{\\min} \\le \\chi^2_{1,0.95}\\}$，其中 $\\chi^2_{1,0.95}$ 表示自由度为 $1$ 的卡方分布的 $95\\%$ 分位数。当且仅当此置信集在扫描网格内两侧有界时，声明 $s$ 是可辨识的。否则，声明 $s$ 是不可辨识的。\n\n从以下基本点开始：\n- 在 $u(t) = u_0$ 和 $x(0) = 0$ 条件下，状态方程 $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$ 通过设置 $\\dot{x}(t) = 0$ 得到稳态 $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$。\n- 抑制剂对 $k_{\\mathrm{on}}$ 和 $k_{\\mathrm{off}}$ 起乘性且独立的作用，得到 $x_{\\infty,i} = \\dfrac{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0}{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0 + \\beta_i\\,k_{\\mathrm{off}}}$。\n- 高斯负对数似然与残差平方和除以 $\\sigma^2$ 成正比，似然比检验采用 $\\chi^2$ 阈值。\n\n设计一个多实验组合，以展示合并数据如何打破单一条件拟合中存在的不可辨识性。使用以下真实参数和测量噪声尺度：\n- $u_0 = 1.0$, $k_{\\mathrm{on}} = 1.5$, $k_{\\mathrm{off}} = 0.8$, $s = 2.0$, $\\sigma = 0.02$。\n- 对于每个测量值 $y_i$，使用下面各案例中指定的确定性噪声偏移 $\\epsilon_i$。\n\n实现四个测试案例：\n- 案例 $1$（单一条件的不可辨识性）：一个实验，条件为 $(\\alpha_1,\\beta_1) = (0.4,1.0)$，$\\epsilon_1 = 0.005$。在轮廓似然中仅使用 $y_1$；计算 $s$ 的可辨识性布尔值。\n- 案例 $2$（联合合并解决不可辨识性）：两个实验联合合并，条件为 $(\\alpha_1,\\beta_1) = (0.4,1.0)$，$\\epsilon_1 = 0.005$ 以及 $(\\alpha_2,\\beta_2) = (1.0,3.0)$，$\\epsilon_2 = -0.005$。在合并数据上计算 $s$ 的可辨识性布尔值。\n- 案例 $3$（相同条件不能解决不可辨识性）：两个实验条件相同，$(\\alpha_1,\\beta_1) = (1.0,1.0)$，$\\epsilon_1 = 0.003$ 以及 $(\\alpha_2,\\beta_2) = (1.0,1.0)$，$\\epsilon_2 = -0.003$。在合并数据上计算 $s$ 的可辨识性布尔值。\n- 案例 $4$（强失活抑制剂解决不可辨识性）：两个实验联合合并，条件为 $(\\alpha_1,\\beta_1) = (0.4,1.0)$，$\\epsilon_1 = 0.005$ 以及 $(\\alpha_2,\\beta_2) = (1.0,10.0)$，$\\epsilon_2 = -0.003$。在合并数据上计算 $s$ 的可辨识性布尔值。\n\n对于每个案例，在网格 $s \\in [0.3,4.0]$ 上使用包含 $41$ 个点的均匀离散化来扫描 $s$。使用卡方截断值 $\\chi^2_{1,0.95} \\approx 3.841458821$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，并按上述案例排序。每个条目必须是表示 $s$ 可辨识性的布尔值，使得最终输出行的格式为“[b1,b2,b3,b4]”。在此公式中所有量均为无量纲，因此不需要单位。",
            "solution": "该问题要求实现一个轮廓似然分析，以评估一个单室信号模型中缩放因子 $s$ 的参数可辨识性。该分析将在四个不同的实验场景下进行，以展示合并来自不同实验的数据如何解决可辨识性问题。\n\n首先，我们建立问题的数学基础。系统的状态 $x(t)$ 由常微分方程 $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$ 支配，初始条件为 $x(0) = 0$。在恒定输入 $u(t) = u_0$ 下，系统达到稳态，可通过设置 $\\dot{x}(t) = 0$ 求得。这得到 $k_{\\mathrm{on}}\\,u_0\\,(1 - x_\\infty) = k_{\\mathrm{off}}\\,x_\\infty$，解出 $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$。\n\n问题引入了 $a = k_{\\mathrm{on}}\\,u_0$ 和 $b = k_{\\mathrm{off}}$ 的重新参数化，因此稳态为 $x_\\infty = \\dfrac{a}{a+b}$。实验涉及乘性地修改速率常数的抑制剂。对于由扰动因子 $(\\alpha_i, \\beta_i)$ 标识的实验 $i$，有效稳态由以下公式给出：\n$$x_{\\infty,i} = \\frac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}$$\n实验 $i$ 的测量模型为 $y_i = s\\,x_{\\infty,i} + \\epsilon_i$，其中 $s$ 是我们感兴趣的观测尺度参数，$\\epsilon_i$ 是测量偏移。\n\n任务的核心是计算 $s$ 的轮廓似然。假设高斯噪声具有已知方差 $\\sigma^2$，参数 $(s, a, b)$ 的负对数似然与归一化残差平方和成正比。对于特定值 $s = \\vartheta$ 的轮廓似然定义为在讨厌参数 $(a,b)$ 上最小化该目标函数的值：\n$$ \\mathrm{PL}(\\vartheta) = \\min_{a>0, b>0} \\sum_i \\left( \\frac{y_i - \\vartheta\\,\\dfrac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}}{\\sigma} \\right)^2 $$\n\n为了评估可辨识性，我们使用似然比检验。$s$ 的 $95\\%$ 置信集定义为 $\\{s \\mid \\mathrm{PL}(s) - \\mathrm{PL}_{\\min} \\le \\chi^2_{1,0.95}\\}$，其中 $\\mathrm{PL}_{\\min}$ 是轮廓似然在扫描的 $s$ 范围内的最小值，而 $\\chi^2_{1,0.95} \\approx 3.841458821$ 是自由度为 $1$ 的卡方分布的 $95\\%$ 分位数。如果此置信区间在预定义的网格 $[0.3, 4.0]$ 内两侧都有界，则声明参数 $s$ 是可辨识的。这意味着轮廓似然必须在网格的两端都高于阈值 $\\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$。\n\n在实现数值程序之前，进行结构可辨识性分析可以为预期结果提供洞见。\n对于单个实验（案例1），我们有一个方程 $y_1 = s \\frac{\\alpha_1 a}{\\alpha_1 a + \\beta_1 b} + \\epsilon_1$ 和三个未知参数 $(s,a,b)$。该系统是欠定的，参数不是唯一可辨识的。对于任何给定的 $s > y_1-\\epsilon_1$，都可以找到一个满足方程的比率 $b/a$，从而导致一条等优解的路径。因此，我们预期 $s$ 是不可辨识的。\n\n对于两个实验（案例2、3、4），我们有一个包含两个方程的方程组：\n$$ y_1 - \\epsilon_1 = s \\frac{\\alpha_1 a}{\\alpha_1 a + \\beta_1 b} $$\n$$ y_2 - \\epsilon_2 = s \\frac{\\alpha_2 a}{\\alpha_2 a + \\beta_2 b} $$\n我们可以重新整理每个方程来求解比率 $b/a$：$\\frac{b}{a} = \\frac{\\alpha_i}{\\beta_i} \\left(\\frac{s}{y_i-\\epsilon_i} - 1\\right)$。将两个实验中 $b/a$ 的表达式相等，可以得到一个仅关于 $s$ 的方程：\n$$ \\frac{\\alpha_1}{\\beta_1} \\left(\\frac{s}{y_1-\\epsilon_1} - 1\\right) = \\frac{\\alpha_2}{\\beta_2} \\left(\\frac{s}{y_2-\\epsilon_2} - 1\\right) $$\n这是一个关于 $s$ 的线性方程。当且仅当 $s$ 的系数不同时，$s$ 存在唯一解，这简化为条件 $\\frac{\\alpha_1}{\\beta_1} \\neq \\frac{\\alpha_2}{\\beta_2}$。\n- 在案例2中，$\\frac{\\alpha_1}{\\beta_1} = 0.4$ 且 $\\frac{\\alpha_2}{\\beta_2} = 1/3 \\approx 0.333$。比率不同，因此 $s$ 应该是可辨识的。\n- 在案例3中，$\\frac{\\alpha_1}{\\beta_1} = 1.0$ 且 $\\frac{\\alpha_2}{\\beta_2} = 1.0$。比率相同，意味着从结构的角度来看，这些实验是冗余的。关于 $s$ 的方程变得无关紧要（$0=0$），$s$ 仍然是不可辨识的。\n- 在案例4中，$\\frac{\\alpha_1}{\\beta_1} = 0.4$ 且 $\\frac{\\alpha_2}{\\beta_2} = 0.1$。比率不同，因此 $s$ 应该是可辨识的。\n\n实现将按以下步骤进行：\n$1$. 定义真实参数、常数和 $s$ 的网格。\n$2$. 对于四个测试案例中的每一个：\n    a. 使用提供的真实参数 $(k_{\\mathrm{on}}, k_{\\mathrm{off}}, s)$、实验条件 $(\\alpha_i, \\beta_i)$ 和噪声偏移 $\\epsilon_i$ 生成合成的测量数据 $\\{y_i\\}$。\n    b. 初始化一个数组来存储轮廓似然值 $\\mathrm{PL}(s)$。\n    c. 遍历 $s$ 网格中的每个值 $\\vartheta$。对于每个 $\\vartheta$，定义一个代表残差平方和的参数 $(a,b)$ 的目标函数。\n    d. 使用 `scipy.optimize.minimize` 的 `L-BFGS-B` 方法和边界 $a>0, b>0$ 来找到该目标函数的最小值。该最小化值即为 $\\mathrm{PL}(\\vartheta)$。\n    e. 在扫描完整个网格后，找到轮廓的全局最小值 $\\mathrm{PL}_{\\min}$。\n    f. 应用可辨识性准则：检查是否 $\\mathrm{PL}(s_{min\\_grid}) > \\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$ 且 $\\mathrm{PL}(s_{max\\_grid}) > \\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$。\n    g. 存储得到的布尔值。\n$3$. 以指定的格式 `[b1,b2,b3,b4]` 打印最终的布尔值列表。\n这个过程将数值上验证我们结构分析的结论。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes parameter identifiability for a systems biology model using profile likelihood.\n    \"\"\"\n    # True parameters for data generation\n    u0_true = 1.0\n    k_on_true = 1.5\n    k_off_true = 0.8\n    s_true = 2.0\n    sigma = 0.02\n\n    # Derived true parameters for optimization\n    a_true = k_on_true * u0_true\n    b_true = k_off_true\n\n    # Constants for the analysis\n    chi2_cutoff = 3.841458821\n    s_grid = np.linspace(0.3, 4.0, 41)\n\n    # Definition of the four test cases\n    test_cases = [\n        {\n            # Case 1: Single experiment, expected to be non-identifiable.\n            \"alphas\": np.array([0.4]),\n            \"betas\": np.array([1.0]),\n            \"epsilons\": np.array([0.005]),\n        },\n        {\n            # Case 2: Two distinct experiments, expected to be identifiable.\n            \"alphas\": np.array([0.4, 1.0]),\n            \"betas\": np.array([1.0, 3.0]),\n            \"epsilons\": np.array([0.005, -0.005]),\n        },\n        {\n            # Case 3: Two identical experiments, expected to be non-identifiable.\n            \"alphas\": np.array([1.0, 1.0]),\n            \"betas\": np.array([1.0, 1.0]),\n            \"epsilons\": np.array([0.003, -0.003]),\n        },\n        {\n            # Case 4: Two distinct experiments, expected to be identifiable.\n            \"alphas\": np.array([0.4, 1.0]),\n            \"betas\": np.array([1.0, 10.0]),\n            \"epsilons\": np.array([0.005, -0.003]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        alphas = case[\"alphas\"]\n        betas = case[\"betas\"]\n        epsilons = case[\"epsilons\"]\n\n        # 1. Generate synthetic data using true parameter values\n        x_inf_true = (alphas * a_true) / (alphas * a_true + betas * b_true)\n        y_data = s_true * x_inf_true + epsilons\n\n        # Define the objective function (sum of squared normalized residuals)\n        # to be minimized for each point in the profile likelihood.\n        def objective_function(params, s_val, alphas_exp, betas_exp, y_exp, sigma_val):\n            a, b = params\n            # The denominator can't be zero due to a>0, b>0 and alpha,beta>0\n            y_model = s_val * (alphas_exp * a) / (alphas_exp * a + betas_exp * b)\n            residuals = (y_exp - y_model) / sigma_val\n            return np.sum(residuals**2)\n\n        # 2. Compute the profile likelihood over the grid of s values\n        pl_values = []\n        for s_val in s_grid:\n            # The optimization finds the best-fit nuisance parameters (a, b) for a fixed s.\n            # Initial guess is set to the true values for stability.\n            # Bounds enforce the physical constraint that rates must be positive.\n            res = minimize(\n                objective_function,\n                x0=[a_true, b_true],\n                args=(s_val, alphas, betas, y_data, sigma),\n                method='L-BFGS-B',\n                bounds=[(1e-9, None), (1e-9, None)]\n            )\n            pl_values.append(res.fun)\n\n        pl_values = np.array(pl_values)\n\n        # 3. Assess identifiability based on the likelihood ratio test\n        pl_min = np.min(pl_values)\n        threshold = pl_min + chi2_cutoff\n\n        # s is identifiable if the 95% confidence interval is bounded on both sides\n        # within the scanned grid. This means the profile at the grid boundaries must\n        # be above the threshold.\n        is_identifiable = (pl_values[0] > threshold) and (pl_values[-1] > threshold)\n        results.append(is_identifiable)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在分析现有数据的基础上，本练习将我们的重点转移到前瞻性的实验设计上。我们应如何选择新的实验来最有效地解决参数的模糊性？本练习引入了费雪信息矩阵（Fisher Information Matrix, FIM）这一强大工具，用以量化一个拟议测量的信息含量。通过计算费雪信息矩阵，您将学会预测哪个新的可观测量能够最好地打破参数之间的混淆，从而确保您的实验投入能够获得最大的信息回报。",
            "id": "3340909",
            "problem": "考虑一个单物种动力学系统，其状态 $x(t)$ 由常微分方程 $\\frac{dx}{dt} = r\\,x(t)$ 和初始条件 $x(0) = x_0$ 控制。其解析解为 $x(t) = x_0 \\exp(r t)$。存在一个基线可观测量 $h_{\\mathrm{base}}(x,\\theta) = c\\,x(t)$，其中 $\\theta = (x_0,r,c)$ 表示未知参数。如果仅随时间测量 $h_{\\mathrm{base}}(x,\\theta)$，参数对 $(x_0,c)$ 会发生混淆（只有乘积 $c\\,x_0$ 与 $r$ 一起是结构可辨识的）。请你判断，哪一个额外可观测量 $h_i(x,\\theta)$ 在与 $h_{\\mathrm{base}}(x,\\theta)$ 同时测量时，能最大程度地降低混淆参数对 $(x_0,c)$ 的轮廓似然的平坦度。\n\n使用以下候选测量函数，每个函数都将与基线测量一起在相同时间点进行评估：\n- $h_1(x,\\theta) = x(t)$,\n- $h_2(x,\\theta) = \\dot{x}(t) = r\\,x(t)$,\n- $h_3(x,\\theta) = \\int_0^t x(s)\\,ds$,\n- $h_4(x,\\theta) = \\big(x(t)\\big)^2$.\n\n假设每个测量输出均服从独立的 高斯观测模型，每个标量观测具有相同的方差 $\\sigma^2$。并假设测量在指定的时间网格 $\\{t_k\\}_{k=1}^N$ 上进行，且无缺失数据。使用独立高斯噪声下的对数似然的标准定义以及轮廓似然的概念。轮廓似然的定义是：固定一个参数，并对剩余参数优化对数似然。为了将轮廓平坦度的降低与信息论论证联系起来，请使用费雪信息矩阵 (FIM)。FIM 定义为在标称参数处对数似然的期望曲率（负海森矩阵），并可根据高斯模型下输出相对于参数的灵敏度计算得出。使用一个基于 FIM 在 $(x_0,c)$ 块上的标量来量化关于 $(x_0,c)$ 的信息量，并选择在添加到 $h_{\\mathrm{base}}(x,\\theta)$ 后能为该块带来最大信息增益的候选可观测量 $h_i$。\n\n你的程序必须：\n- 实现上述的 $x(t)$ 和所有 $h_i(x,\\theta)$。\n- 对于给定的测试用例 $(\\theta, \\{t_k\\}, \\sigma)$，使用输出对 $(x_0,r,c)$ 的有限差分灵敏度，为组合测量集 $\\{h_{\\mathrm{base}}, h_i\\}$ 计算在 $\\theta$ 处的期望费雪信息矩阵。\n- 提取与参数对 $(x_0,c)$ 对应的 $2\\times 2$ 费雪信息块，并计算一个标量信息分数，该分数随信息块的信息量增加而增加。你必须使用该 $2\\times 2$ 块的行列式的自然对数，并在对角线上添加一个小的数值正则化项以避免奇异性。\n- 对于每个测试用例，返回候选可观测量 $h_i$ 的索引 $i \\in \\{1,2,3,4\\}$，该可观测量在与基线测量结合时能使 $(x_0,c)$ 块的信息分数最大化。\n\n使用以下测试套件：\n- 测试用例 A (一般情况): $\\theta = (x_0,r,c) = (2.0,\\,0.7,\\,0.6)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,1.5,\\,16)$, $\\sigma = 0.1$。\n- 测试用例 B (近常数动力学边界): $\\theta = (x_0,r,c) = (1.0,\\,0.05,\\,0.5)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,0.3,\\,7)$, $\\sigma = 0.2$。\n- 测试用例 C (高噪声边缘情况): $\\theta = (3.0,\\,1.2,\\,0.3)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,0.6,\\,13)$, $\\sigma = 0.5$。\n- 测试用例 D (振幅主导区域): $\\theta = (0.5,\\,0.9,\\,1.3)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,1.0,\\,11)$, $\\sigma = 0.05$。\n\n在所有情况下，将时间视为无量纲，并以无单位的索引表示最终答案。\n\n输出规格：\n- 你的程序应生成单行输出，其中包含四个测试用例所选候选者的索引，形式为方括号内以逗号分隔的列表（例如，$\\mathrm{[i_A,i_B,i_C,i_D]}$），其中每个条目是 $\\{1,2,3,4\\}$ 中的一个整数。\n\n你的解决方案必须是一个完整、可运行的程序，能够执行所述计算，无需任何用户输入或外部文件。",
            "solution": "用户希望在一个简单的指数增长模型中，确定四个候选可观测量 $h_1, h_2, h_3, h_4$ 中的哪一个，在添加到基线测量 $h_{\\mathrm{base}}$ 后，能提供最多的信息来区分参数 $x_0$ 和 $c$。\n\n### **1. 问题验证**\n根据指定标准对问题陈述进行验证。\n\n- **第 1 步：提取已知信息**\n    - **模型**: $dx/dt = r\\,x(t)$，其中 $x(0) = x_0$。解为：$x(t) = x_0 \\exp(r t)$。\n    - **参数**: $\\theta = (x_0, r, c)$。\n    - **可观测量**: \n        - 基线: $h_{\\mathrm{base}}(t) = c\\,x(t)$。\n        - 候选: $h_1(t) = x(t)$， $h_2(t) = r\\,x(t)$， $h_3(t) = \\int_0^t x(s)\\,ds = \\frac{x_0}{r}(\\exp(rt)-1)$， $h_4(t) = (x(t))^2$。\n    - **方法论**:\n        1. 假设每个标量测量都存在方差为 $\\sigma^2$ 的独立高斯噪声。\n        2. 对于每个组合测量 $\\{h_{\\mathrm{base}}, h_i\\}$，计算参数 $(x_0, r, c)$ 的 $3 \\times 3$ 费雪信息矩阵 (FIM) $F$。总 FIM 是每个可观测量 FIM 的总和：$F_{\\text{total}} = F_{\\text{base}} + F_{i}$。\n        3. 在时间点 $\\{t_k\\}$ 测量的单个可观测量 $y(t;\\theta)$ 的 FIM 由 $F_{jk} = \\frac{1}{\\sigma^2} \\sum_{k=1}^N \\frac{\\partial y(t_k)}{\\partial \\theta_j} \\frac{\\partial y(t_k)}{\\partial \\theta_k}$ 给出。\n        4. 灵敏度 $\\frac{\\partial y}{\\partial \\theta_j}$ 将使用有限差分法计算。\n        5. 提取 $F_{\\text{total}}$ 中对应于参数 $(x_0, c)$ 的 $2 \\times 2$ 子矩阵。\n        6. 将信息分数计算为 $S_i = \\log(\\det(F_{(x_0,c)} + \\epsilon I))$，其中 $\\epsilon$ 是一个小的正则化常数，而 $I$ 是单位矩阵。\n        7. 选择使 $S_i$ 最大化的索引 $i \\in \\{1,2,3,4\\}$。\n    - **测试用例**: 给出了 $\\theta$、$\\{t_k\\}$ 和 $\\sigma$ 的具体值。\n\n- **第 2 步：使用提取的已知信息进行验证**\n    - 问题**有科学依据**，使用了动力系统、参数估计和最优实验设计 (FIM) 的标准原理。\n    - 问题是**良构的**，目标明确，并为每个测试用例提供了精确的量化方法以得出唯一答案。\n    - 问题是**客观的**，没有歧义。\n    - 提供了所有必要信息，使问题**完整且一致**。其组成部分（模型、可观测量、分析方法）都是标准的，不违反任何科学或数学原理。\n\n- **第 3 步：结论与行动**\n    - 问题被判定为**有效**。将提供一个解决方案。\n\n### **2. 基于原理的解决方案设计**\n\n问题的核心是在添加新测量时，量化关于参数子集 $(x_0, c)$ 的信息增益。费雪信息矩阵 (FIM) 提供了必要的理论框架。\n\n**费雪信息矩阵 (FIM)**\n对于一个参数为 $\\theta$，测量输出被方差为 $\\sigma^2$ 的独立同分布高斯噪声所干扰的模型，其 FIM 是一个矩阵，其元素由以下公式给出：\n$$\nF_{jk} = \\frac{1}{\\sigma^2} \\sum_{m} \\sum_{l=1}^{N} \\left( \\frac{\\partial y_m(t_l; \\theta)}{\\partial \\theta_j} \\right) \\left( \\frac{\\partial y_m(t_l; \\theta)}{\\partial \\theta_k} \\right)\n$$\n其中 $y_m$ 是第 $m$ 个可观测量，$t_l$ 是测量时间，$\\theta_j, \\theta_k$ 是参数。总 FIM 是每个独立可观测量的 FIM 之和。\n\n**使用 $h_{\\mathrm{base}}$ 时 $(x_0, c)$ 的结构不可辨识性**\n基线可观测量为 $h_{\\mathrm{base}}(t) = c \\cdot x_0 e^{rt}$。参数 $c$ 和 $x_0$ 仅以乘积 $c\\,x_0$ 的形式出现。这导致了结构不可辨识性。在数学上，关于 $x_0$ 和 $c$ 的灵敏度向量是线性相关的：\n$$\n\\frac{\\partial h_{\\mathrm{base}}}{\\partial x_0} = c \\, e^{rt} \\quad \\text{and} \\quad \\frac{\\partial h_{\\mathrm{base}}}{\\partial c} = x_0 \\, e^{rt}\n$$\n因此，$\\frac{\\partial h_{\\mathrm{base}}}{\\partial c} = \\frac{x_0}{c} \\frac{\\partial h_{\\mathrm{base}}}{\\partial x_0}$。这种线性相关性使得 $(x_0, c)$ 对应的 FIM 块是奇异的，反映了无法区分这些参数。其行列式为零。\n\n**解决不可辨识性问题**\n如果第二个可观测量 $h_i$ 的灵敏度打破了这种线性相关性，就可以解决这个问题。所有候选可观测量 $h_1, \\dots, h_4$ 都是 $x(t)$ 和 $r$ 的函数，但不是 $c$ 的函数。因此，对于它们中的每一个：\n$$\n\\frac{\\partial h_i}{\\partial c} = 0\n$$\n组合系统 $\\{h_{\\mathrm{base}}, h_i\\}$ 的总 FIM 为 $F_{\\text{total}} = F_{\\text{base}} + F_{i}$。对应于 $(x_0, c)$ 的块现在将包含来自两个可观测量的贡献。因为 $\\frac{\\partial h_i}{\\partial c} = 0$ 而 $\\frac{\\partial h_i}{\\partial x_0} \\neq 0$，所以 $x_0$ 和 $c$ 的新总灵敏度向量不再是线性相关的，FIM 块也变为非奇异的。\n\n**信息度量**\nFIM 块的行列式 $\\det(F_{(x_0,c)})$ 是该参数子集信息含量的度量。更大的行列式对应于更小的联合置信区域，从而具有更好的可辨识性。我们需要找到能使该行列式（或其对数，这是一个单调变换）最大化的可观测量 $h_i$。\n\n**算法实现**\n该解决方案将作为一个 Python 程序实现，对每个测试用例遵循以下步骤：\n1.  定义参数集 $\\theta = (x_0, r, c)$、时间网格 $\\{t_k\\}$ 和噪声标准差 $\\sigma$。\n2.  为系统状态 $x(t)$ 和所有可观测量 $h_{\\mathrm{base}}, h_1, h_2, h_3, h_4$ 定义函数。\n3.  实现一个函数来计算任何给定可观测量的灵敏度矩阵 $S$，其中 $S_{lk} = \\frac{\\partial y(t_l)}{\\partial \\theta_k}$。根据要求，这将使用中心有限差分格式进行数值计算。\n4.  实现一个从灵敏度矩阵计算 FIM 的函数：$F = \\frac{1}{\\sigma^2} S^T S$。\n5.  对于每个候选 $h_i$ ($i \\in \\{1,2,3,4\\}$)：\n    a. 计算 $F_{\\text{base}}$ 和 $F_{i}$。\n    b. 计算总 FIM: $F_{\\text{total}} = F_{\\text{base}} + F_{i}$。\n    c. 提取 $(x_0, c)$ 的 $2 \\times 2$ 子矩阵，这些参数位于索引 $0$ 和 $2$ 处。\n    d. 通过添加一个小的对角项 $\\epsilon I$ 来对子矩阵进行正则化，以确保数值稳定性。\n    e. 将分数计算为正则化后子矩阵行列式的自然对数。\n6.  确定产生最大分数的索引 $i$ 并报告它。\n对所有四个测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef x_model(t, params):\n    \"\"\"\n    Computes the state x(t) = x0 * exp(r*t).\n    \n    Args:\n        t (np.ndarray): Time points.\n        params (tuple): (x0, r, c).\n        \n    Returns:\n        np.ndarray: State x at times t.\n    \"\"\"\n    x0, r, c = params\n    return x0 * np.exp(r * t)\n\ndef h_base(t, params):\n    \"\"\"Observable h_base(t) = c * x(t).\"\"\"\n    x0, r, c = params\n    return c * x_model(t, params)\n\ndef h1(t, params):\n    \"\"\"Observable h1(t) = x(t).\"\"\"\n    return x_model(t, params)\n    \ndef h2(t, params):\n    \"\"\"Observable h2(t) = r * x(t).\"\"\"\n    x0, r, c = params\n    return r * x_model(t, params)\n\ndef h3(t, params):\n    \"\"\"Observable h3(t) = integral of x(s) from 0 to t.\"\"\"\n    x0, r, c = params\n    # Handle the case r -> 0 to avoid division by zero.\n    if abs(r)  1e-9:\n        return x0 * t\n    return (x_model(t, (x0, r, c)) - x0) / r\n\ndef h4(t, params):\n    \"\"\"Observable h4(t) = x(t)^2.\"\"\"\n    return x_model(t, params)**2\n\ndef calculate_sensitivities(model_func, params, t_grid):\n    \"\"\"\n    Calculates sensitivities of a model output with respect to parameters\n    using central finite differences.\n    \"\"\"\n    n_params = len(params)\n    n_times = len(t_grid)\n    sensitivities = np.zeros((n_times, n_params))\n    \n    eps_rel = 1e-7  # Relative step size for finite differences\n\n    for i in range(n_params):\n        params_plus = list(params)\n        params_minus = list(params)\n        \n        # Robust step size selection\n        step = eps_rel * (abs(params[i]) + 1e-8)\n        \n        params_plus[i] += step\n        params_minus[i] -= step\n        \n        y_plus = model_func(t_grid, tuple(params_plus))\n        y_minus = model_func(t_grid, tuple(params_minus))\n        \n        sensitivities[:, i] = (y_plus - y_minus) / (2 * step)\n        \n    return sensitivities\n\ndef calculate_fim(sensitivities, sigma):\n    \"\"\"\n    Calculates the Fisher Information Matrix from a sensitivity matrix.\n    FIM = (1/sigma^2) * S^T * S\n    \"\"\"\n    return (sensitivities.T @ sensitivities) / (sigma**2)\n\ndef find_best_observable(params, t_grid, sigma):\n    \"\"\"\n    Finds the best observable h_i to add to h_base for identifying (x0, c).\n    \"\"\"\n    candidate_observables = [h1, h2, h3, h4]\n    scores = []\n    \n    # FIM for h_base is calculated once\n    sens_base = calculate_sensitivities(h_base, params, t_grid)\n    fim_base = calculate_fim(sens_base, sigma)\n    \n    for h_i in candidate_observables:\n        # Calculate FIM for the candidate observable\n        sens_i = calculate_sensitivities(h_i, params, t_grid)\n        fim_i = calculate_fim(sens_i, sigma)\n        \n        # Total FIM is the sum\n        fim_total = fim_base + fim_i\n        \n        # Extract the 2x2 block for parameters (x0, c) at indices 0 and 2\n        fim_block = fim_total[np.ix_([0, 2], [0, 2])]\n        \n        # Regularize and compute score (log-determinant)\n        reg = 1e-12\n        fim_block_reg = fim_block + reg * np.eye(2)\n        score = np.log(np.linalg.det(fim_block_reg))\n        scores.append(score)\n        \n    # Return the 1-based index of the best observable\n    best_index = np.argmax(scores) + 1\n    return best_index\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {'params': (2.0, 0.7, 0.6), 't_grid': np.linspace(0.0, 1.5, 16), 'sigma': 0.1},\n        # Test case B\n        {'params': (1.0, 0.05, 0.5), 't_grid': np.linspace(0.0, 0.3, 7), 'sigma': 0.2},\n        # Test case C\n        {'params': (3.0, 1.2, 0.3), 't_grid': np.linspace(0.0, 0.6, 13), 'sigma': 0.5},\n        # Test case D\n        {'params': (0.5, 0.9, 1.3), 't_grid': np.linspace(0.0, 1.0, 11), 'sigma': 0.05}\n    ]\n\n    results = []\n    for case in test_cases:\n        best_idx = find_best_observable(case['params'], case['t_grid'], case['sigma'])\n        results.append(best_idx)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}