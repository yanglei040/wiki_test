{
    "hands_on_practices": [
        {
            "introduction": "Before diving into numerical methods, it is essential to grasp the analytical foundation of profile likelihood. This exercise demonstrates the first step in many profiling analyses: analytically eliminating a simple nuisance parameter. By profiling out the noise variance $\\sigma^2$ from a standard Gaussian likelihood, you will derive the exact form of the profiled log-likelihood, revealing its direct relationship to the residual sum of squares, a cornerstone of statistical modeling .",
            "id": "3340975",
            "problem": "Consider a single-output dynamic model in computational systems biology described by an ordinary differential equation (ODE) that maps a parameter vector $\\theta \\in \\mathbb{R}^{p}$ to a predicted measurement trajectory via a smooth observation function. Let the observed data be $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$, where $t_{i}$ are known sampling times, and the observation model is\n$$\ny_{i} = f(t_{i}; \\theta) + \\varepsilon_{i},\n$$\nwith independent and identically distributed measurement errors $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$, where $\\sigma^{2} > 0$ is an unknown nuisance variance. Assume the ODE and observation map are such that $f(t_{i}; \\theta)$ is well-defined and finite for all $i$ and all $\\theta$ in the parameter domain.\n\nStarting from the definition of the Gaussian likelihood and the independence of the observations, write down the log-likelihood $\\ell(\\theta, \\sigma^{2})$ for the data. Then, for a fixed $\\theta$, analytically maximize $\\ell(\\theta, \\sigma^{2})$ with respect to $\\sigma^{2}$ over the domain $\\sigma^{2} > 0$, and substitute the maximizer back into $\\ell(\\theta, \\sigma^{2})$ to obtain the profiled log-likelihood $\\tilde{\\ell}(\\theta)$ as a function of $\\theta$ alone. Express your final result in closed form in terms of the residual sum of squares\n$$\nS(\\theta) = \\sum_{i=1}^{n}\\left(y_{i} - f(t_{i}; \\theta)\\right)^{2}.\n$$\nProvide the final answer as a single analytic expression for $\\tilde{\\ell}(\\theta)$. Do not omit additive constants that do not depend on $\\theta$. Your final answer should be a single expression with no units and requires no rounding.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   **Model:** A single-output dynamic model where a parameter vector $\\theta \\in \\mathbb{R}^{p}$ maps to a predicted trajectory $f(t; \\theta)$.\n-   **Data:** A set of $n$ observations $\\{(t_{i}, y_{i})\\}_{i=1}^{n}$ at known sampling times $t_i$.\n-   **Observation Model:** The relationship between data and the model is given by $y_{i} = f(t_{i}; \\theta) + \\varepsilon_{i}$.\n-   **Error Structure:** The measurement errors $\\varepsilon_{i}$ are independent and identically distributed (i.i.d.) from a normal distribution with mean $0$ and unknown variance $\\sigma^{2} > 0$, denoted as $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$.\n-   **Function Properties:** The observation map $f(t_{i}; \\theta)$ is well-defined and finite for all relevant inputs.\n-   **Defined Quantity:** The residual sum of squares is defined as $S(\\theta) = \\sum_{i=1}^{n}\\left(y_{i} - f(t_{i}; \\theta)\\right)^{2}$.\n-   **Objective:** Derive the profiled log-likelihood $\\tilde{\\ell}(\\theta)$ by first writing the full log-likelihood $\\ell(\\theta, \\sigma^2)$ and then analytically maximizing it with respect to $\\sigma^2$. The final expression should be in terms of $S(\\theta)$ and include all constants.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Groundedness:** The problem is scientifically grounded. It describes the standard statistical framework for parameter estimation in nonlinear regression models with additive Gaussian noise. This is a fundamental and widely used methodology in all quantitative STEM fields, including computational systems biology.\n-   **Well-Posedness:** The problem is well-posed. It provides all necessary information and definitions to perform the requested derivation. The objective is clear and unambiguous, leading to a unique analytical solution.\n-   **Objectivity:** The problem is stated using precise, objective mathematical language, free from any subjective or biased phrasing.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically sound, well-posed, objective, and conforms to all criteria for a valid scientific problem. The solution process will now proceed.\n\n---\n\nThe derivation of the profiled log-likelihood function $\\tilde{\\ell}(\\theta)$ proceeds as follows.\n\nFirst, we formulate the likelihood function $L(\\theta, \\sigma^2)$. The assumption that the errors $\\varepsilon_i$ are drawn from an i.i.d. normal distribution $\\mathcal{N}(0, \\sigma^2)$ implies that each observation $y_i$ is a random variable from a normal distribution with mean $f(t_i; \\theta)$ and variance $\\sigma^2$. The probability density function (PDF) for a single observation $y_i$ is:\n$$\np(y_i | \\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - f(t_i; \\theta))^2}{2\\sigma^2} \\right)\n$$\nDue to the independence of observations, the likelihood of the entire dataset, which consists of $n$ observations, is the product of the individual PDFs:\n$$\nL(\\theta, \\sigma^2) = \\prod_{i=1}^{n} p(y_i | \\theta, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - f(t_i; \\theta))^2}{2\\sigma^2} \\right)\n$$\nThis expression can be consolidated:\n$$\nL(\\theta, \\sigma^2) = \\left( \\frac{1}{2\\pi\\sigma^2} \\right)^{n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - f(t_i; \\theta))^2 \\right)\n$$\nUsing the provided definition of the residual sum of squares, $S(\\theta) = \\sum_{i=1}^{n} (y_i - f(t_i; \\theta))^2$, the likelihood function simplifies to:\n$$\nL(\\theta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{S(\\theta)}{2\\sigma^2} \\right)\n$$\nNext, we determine the log-likelihood function, $\\ell(\\theta, \\sigma^2)$, by taking the natural logarithm of $L(\\theta, \\sigma^2)$:\n$$\n\\ell(\\theta, \\sigma^2) = \\ln(L(\\theta, \\sigma^2)) = \\ln\\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{S(\\theta)}{2\\sigma^2} \\right) \\right)\n$$\nUsing the properties of logarithms, $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ and $\\ln(a^b) = b \\ln(a)$, we get:\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2}\n$$\nThis can be expanded to:\n$$\n\\ell(\\theta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2}\n$$\nTo find the profiled log-likelihood $\\tilde{\\ell}(\\theta)$, we must first maximize $\\ell(\\theta, \\sigma^2)$ with respect to the nuisance parameter $\\sigma^2$ for a fixed parameter vector $\\theta$. We find the critical point by taking the partial derivative of $\\ell(\\theta, \\sigma^2)$ with respect to $\\sigma^2$ and setting it to zero:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{S(\\theta)}{2\\sigma^2} \\right) = -\\frac{n}{2\\sigma^2} + \\frac{S(\\theta)}{2(\\sigma^2)^2}\n$$\nSetting the derivative to zero to find the estimator $\\hat{\\sigma}^2(\\theta)$:\n$$\n-\\frac{n}{2\\hat{\\sigma}^2} + \\frac{S(\\theta)}{2(\\hat{\\sigma}^2)^2} = 0\n$$\nAssuming $S(\\theta) > 0$ and thus $\\hat{\\sigma}^2 > 0$, we can multiply by $2(\\hat{\\sigma}^2)^2$:\n$$\n-n\\hat{\\sigma}^2 + S(\\theta) = 0\n$$\nSolving for $\\hat{\\sigma}^2$ yields the maximum likelihood estimator for the variance, conditional on $\\theta$:\n$$\n\\hat{\\sigma}^2(\\theta) = \\frac{S(\\theta)}{n}\n$$\nTo confirm this is a maximum, we check the second derivative:\n$$\n\\frac{\\partial^2 \\ell}{(\\partial \\sigma^2)^2} = \\frac{\\partial}{\\partial \\sigma^2} \\left( -\\frac{n}{2\\sigma^2} + \\frac{S(\\theta)}{2(\\sigma^2)^2} \\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{S(\\theta)}{(\\sigma^2)^3}\n$$\nEvaluating at $\\sigma^2 = \\hat{\\sigma}^2(\\theta) = S(\\theta)/n$:\n$$\n\\left. \\frac{\\partial^2 \\ell}{(\\partial \\sigma^2)^2} \\right|_{\\sigma^2=\\hat{\\sigma}^2} = \\frac{n}{2(S(\\theta)/n)^2} - \\frac{S(\\theta)}{(S(\\theta)/n)^3} = \\frac{n^3}{2S(\\theta)^2} - \\frac{n^3 S(\\theta)}{S(\\theta)^3} = \\frac{n^3}{2S(\\theta)^2} - \\frac{n^3}{S(\\theta)^2} = -\\frac{n^3}{2S(\\theta)^2}\n$$\nFor a non-trivial model fit, $S(\\theta)>0$, and since $n>0$, the second derivative is negative. This confirms that $\\hat{\\sigma}^2(\\theta) = S(\\theta)/n$ is a local maximum. Since it is the only critical point in the domain $\\sigma^2 > 0$, it is the global maximum.\n\nFinally, the profiled log-likelihood $\\tilde{\\ell}(\\theta)$ is obtained by substituting $\\hat{\\sigma}^2(\\theta)$ back into the log-likelihood function $\\ell(\\theta, \\sigma^2)$:\n$$\n\\tilde{\\ell}(\\theta) = \\ell(\\theta, \\hat{\\sigma}^2(\\theta)) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S(\\theta)}{n}\\right) - \\frac{S(\\theta)}{2\\left(\\frac{S(\\theta)}{n}\\right)}\n$$\nThe last term simplifies to $\\frac{n}{2}$. Thus, we have:\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{S(\\theta)}{n}\\right) - \\frac{n}{2}\n$$\nThis expression can be further compacted by combining the logarithmic terms and the constant:\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\left[ \\ln(2\\pi) + \\ln(S(\\theta)) - \\ln(n) + 1 \\right]\n$$\nUsing $\\ln(e) = 1$ and combining the terms inside the square brackets:\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\left[ \\ln\\left(\\frac{2\\pi S(\\theta)}{n}\\right) + \\ln(e) \\right]\n$$\nThis leads to the final closed-form expression for the profiled log-likelihood:\n$$\n\\tilde{\\ell}(\\theta) = -\\frac{n}{2} \\ln\\left(\\frac{2\\pi e S(\\theta)}{n}\\right)\n$$\nThis is the desired function of $\\theta$, with all constants retained.",
            "answer": "$$\n\\boxed{-\\frac{n}{2} \\ln\\left(\\frac{2\\pi e S(\\theta)}{n}\\right)}\n$$"
        },
        {
            "introduction": "A key challenge in systems biology is designing experiments that can resolve parameter ambiguities. This practice explores how to use the Fisher Information Matrix (FIM), which quantifies the amount of information an experiment provides about model parameters, to make rational design choices. You will computationally determine which of several potential new measurements is most effective at breaking the confounding between two parameters in a simple dynamical model, providing a hands-on introduction to optimal experimental design .",
            "id": "3340909",
            "problem": "Consider a single-species dynamical system with state $x(t)$ governed by the ordinary differential equation $\\frac{dx}{dt} = r\\,x(t)$ and initial condition $x(0) = x_0$. The analytic solution is $x(t) = x_0 \\exp(r t)$. A baseline observable is available, $h_{\\text{base}}(x,\\boldsymbol{\\theta}) = c\\,x(t)$, where $\\boldsymbol{\\theta} = (x_0,r,c)$ denotes the unknown parameters. With only $h_{\\text{base}}(x,\\boldsymbol{\\theta})$ measured over time, the parameter pair $(x_0,c)$ is confounded (only the product $c\\,x_0$ is structurally identifiable together with $r$). You are asked to decide which additional observable $h_i(x,\\boldsymbol{\\theta})$, when measured concurrently with $h_{\\text{base}}(x,\\boldsymbol{\\theta})$, most reduces the flatness of the profile likelihood for the confounded pair $(x_0,c)$.\n\nUse the following candidate measurement functions, each to be added alongside the baseline measurement and evaluated at the same times:\n- $h_1(x,\\boldsymbol{\\theta}) = x(t)$,\n- $h_2(x,\\boldsymbol{\\theta}) = \\dot{x}(t) = r\\,x(t)$,\n- $h_3(x,\\boldsymbol{\\theta}) = \\int_0^t x(s)\\,ds$,\n- $h_4(x,\\boldsymbol{\\theta}) = (x(t))^2$.\n\nAssume an independent Gaussian observation model for each measured output with identical variance $\\sigma^2$ per scalar observation, and assume measurements are taken at a specified time grid $\\{t_k\\}_{k=1}^N$ without missing data. Use the standard definition of the log-likelihood under independent Gaussian noise and the notion of profile likelihood, defined by fixing one parameter and optimizing the log-likelihood over the remaining parameters. To connect the reduction in profile flatness to information-theoretic arguments, use the Fisher Information Matrix (FIM), defined by the expected curvature (negative Hessian) of the log-likelihood at the nominal parameter and computed from output sensitivities with respect to parameters under the Gaussian model. Quantify the informativeness about $(x_0,c)$ using a scalar based on the FIM restricted to the $(x_0,c)$ block, and select the candidate $h_i$ that yields the largest information gain for that block when added to $h_{\\text{base}}(x,\\boldsymbol{\\theta})$.\n\nYour program must:\n- Implement $x(t)$ and all $h_i(x,\\boldsymbol{\\theta})$ listed above.\n- For a given test case $(\\boldsymbol{\\theta}, \\{t_k\\}, \\sigma)$, compute the expected Fisher Information Matrix at $\\boldsymbol{\\theta}$ for the combined measurement set $\\{h_{\\text{base}}, h_i\\}$, using finite-difference sensitivities of the outputs with respect to $(x_0,r,c)$.\n- Extract the $2\\times 2$ Fisher Information block corresponding to the parameter pair $(x_0,c)$ and compute a scalar information score that increases when the block becomes more informative. You must use the natural logarithm of the determinant of this $2\\times 2$ block with a small numerical regularization added to the diagonal to avoid singularities.\n- For each test case, return the index $i \\in \\{1,2,3,4\\}$ of the candidate observable $h_i$ that maximizes the information score for the $(x_0,c)$ block when combined with the baseline measurement.\n\nUse the following test suite:\n- Test case A (general case): $\\boldsymbol{\\theta} = (x_0,r,c) = (2.0,\\,0.7,\\,0.6)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,1.5,\\,16)$, $\\sigma = 0.1$.\n- Test case B (near-constant dynamics boundary): $\\boldsymbol{\\theta} = (x_0,r,c) = (1.0,\\,0.05,\\,0.5)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,0.3,\\,7)$, $\\sigma = 0.2$.\n- Test case C (higher noise edge case): $\\boldsymbol{\\theta} = (x_0,r,c) = (3.0,\\,1.2,\\,0.3)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,0.6,\\,13)$, $\\sigma = 0.5$.\n- Test case D (amplitude-dominant regime): $\\boldsymbol{\\theta} = (x_0,r,c) = (0.5,\\,0.9,\\,1.3)$, $\\{t_k\\} = \\mathrm{linspace}(0,\\,1.0,\\,11)$, $\\sigma = 0.05$.\n\nIn all cases, treat time as dimensionless and express the final answer as indices without units.\n\nOutput specification:\n- Your program should produce a single line of output containing the chosen candidate indices for the four test cases as a comma-separated list enclosed in square brackets (for example, `[i_A,i_B,i_C,i_D]`), where each entry is an integer in $\\{1,2,3,4\\}$.\n\nYour solution must be a complete, runnable program that performs the described computations without requiring any user input or external files.",
            "solution": "The user wants to identify which of four candidate observables, $h_1, h_2, h_3, h_4$, when added to a baseline measurement $h_{\\text{base}}$, provides the most information to distinguish the parameters $x_0$ and $c$ in a simple exponential growth model.\n\n### **1. Problem Validation**\nThe problem statement is validated according to the specified criteria.\n\n- **Step 1: Extract Givens**\n    - **Model**: $\\frac{dx}{dt} = r\\,x(t)$ with $x(0) = x_0$. Solution: $x(t) = x_0 \\exp(r t)$.\n    - **Parameters**: $\\boldsymbol{\\theta} = (x_0, r, c)$.\n    - **Observables**: \n        - Baseline: $h_{\\text{base}}(t) = c\\,x(t)$.\n        - Candidates: $h_1(t) = x(t)$, $h_2(t) = r\\,x(t)$, $h_3(t) = \\int_0^t x(s)\\,ds = \\frac{x_0}{r}(\\exp(rt)-1)$, $h_4(t) = (x(t))^2$.\n    - **Methodology**:\n        1.  Assume independent Gaussian noise with variance $\\sigma^2$ for each scalar measurement.\n        2.  For each combined measurement $\\{h_{\\text{base}}, h_i\\}$, compute the $3 \\times 3$ Fisher Information Matrix (FIM), $F$, for parameters $(x_0, r, c)$. The total FIM is the sum of the FIMs for each observable: $F_{\\text{total}} = F_{\\text{base}} + F_{i}$.\n        3.  The FIM for a single observable $y(t;\\boldsymbol{\\theta})$ measured at times $\\{t_l\\}$ is given by $F_{jk} = \\frac{1}{\\sigma^2} \\sum_{l=1}^N \\frac{\\partial y(t_l)}{\\partial \\theta_j} \\frac{\\partial y(t_l)}{\\partial \\theta_k}$.\n        4.  Sensitivities $\\frac{\\partial y}{\\partial \\theta_j}$ are to be computed using finite differences.\n        5.  Extract the $2 \\times 2$ submatrix of $F_{\\text{total}}$ corresponding to parameters $(x_0, c)$.\n        6.  Compute the information score as $S_i = \\log(\\det(F_{(x_0,c)} + \\epsilon I))$, where $\\epsilon$ is a small regularization constant and $I$ is the identity matrix.\n        7.  Select the index $i \\in \\{1,2,3,4\\}$ that maximizes $S_i$.\n    - **Test Cases**: Specific values for $\\boldsymbol{\\theta}$, $\\{t_k\\}$, and $\\sigma$ are provided.\n\n- **Step 2: Validate Using Extracted Givens**\n    - The problem is **scientifically grounded**, using standard principles of dynamical systems, parameter estimation, and optimal experimental design (FIM).\n    - It is **well-posed**, with a clear objective and a precise, quantitative method to reach a unique answer for each test case.\n    - The problem is **objective** and free from ambiguity.\n    - All necessary information is provided, making the problem **complete and consistent**. Its components (model, observables, analysis method) are standard and do not violate any scientific or mathematical principles.\n\n- **Step 3: Verdict and Action**\n    - The problem is deemed **valid**. A solution will be provided.\n\n### **2. Principle-Based Solution Design**\n\nThe core of the problem is to quantify the information gain about a parameter subset $(x_0, c)$ when a new measurement is added. The Fisher Information Matrix (FIM) provides the necessary theoretical framework.\n\n**Fisher Information Matrix (FIM)**\nFor a model with parameters $\\boldsymbol{\\theta}$ and measured outputs corrupted by independent, identically distributed Gaussian noise with variance $\\sigma^2$, the FIM is a matrix whose elements are given by:\n$$\nF_{jk} = \\frac{1}{\\sigma^2} \\sum_{m} \\sum_{l=1}^{N} \\left( \\frac{\\partial y_m(t_l; \\boldsymbol{\\theta})}{\\partial \\theta_j} \\right) \\left( \\frac{\\partial y_m(t_l; \\boldsymbol{\\theta})}{\\partial \\theta_k} \\right)\n$$\nwhere $y_m$ is the $m$-th observable, $t_l$ are the measurement times, and $\\theta_j, \\theta_k$ are the parameters. The FIM is the sum of the FIMs for each individual observable.\n\n**Structural Non-Identifiability of $(x_0, c)$ with $h_{\\mathrm{base}}$**\nThe baseline observable is $h_{\\text{base}}(t) = c \\cdot x_0 e^{rt}$. The parameters $c$ and $x_0$ only appear as a product, $c\\,x_0$. This leads to structural non-identifiability. Mathematically, the sensitivity vectors with respect to $x_0$ and $c$ are linearly dependent:\n$$\n\\frac{\\partial h_{\\text{base}}}{\\partial x_0} = c \\, e^{rt} \\quad \\text{and} \\quad \\frac{\\partial h_{\\text{base}}}{\\partial c} = x_0 \\, e^{rt}\n$$\nThus, $\\frac{\\partial h_{\\text{base}}}{\\partial c} = \\frac{x_0}{c} \\frac{\\partial h_{\\text{base}}}{\\partial x_0}$. This linear dependence makes the corresponding FIM block for $(x_0, c)$ singular, reflecting the inability to distinguish these parameters. Its determinant is zero.\n\n**Resolving Non-Identifiability**\nAdding a second observable $h_i$ can resolve this issue if its sensitivities break the linear dependency. All candidate observables $h_1, \\dots, h_4$ are functions of $x(t)$ and $r$, but not $c$. Therefore, for each of them:\n$$\n\\frac{\\partial h_i}{\\partial c} = 0\n$$\nThe total FIM for the combined system $\\{h_{\\text{base}}, h_i\\}$ is $F_{\\text{total}} = F_{\\text{base}} + F_{i}$. The block corresponding to $(x_0, c)$ will now have contributions from both observables. Because $\\frac{\\partial h_i}{\\partial c} = 0$ while $\\frac{\\partial h_i}{\\partial x_0} \\neq 0$, the new total sensitivity vectors for $x_0$ and $c$ are no longer linearly dependent, and the FIM block becomes non-singular.\n\n**Information Metric**\nThe determinant of a FIM block, $\\det(F_{(x_0,c)})$, is a measure of the information content for that parameter subset. A larger determinant corresponds to a smaller joint confidence region and thus better identifiability. We are asked to find the observable $h_i$ that maximizes this determinant (or its logarithm, which is a monotonic transformation).\n\n**Algorithmic Implementation**\nThe solution will be implemented as a Python program following these steps for each test case:\n1.  Define the parameter set $\\boldsymbol{\\theta} = (x_0, r, c)$, the time grid $\\{t_k\\}$, and the noise standard deviation $\\sigma$.\n2.  Define functions for the system state $x(t)$ and all observables $h_{\\text{base}}, h_1, h_2, h_3, h_4$.\n3.  Implement a function to compute the sensitivity matrix $S$ for any given observable, where $S_{lk} = \\frac{\\partial y(t_l)}{\\partial \\theta_k}$. This will be done numerically using a central finite-difference scheme as requested.\n4.  Implement a function to compute the FIM from a sensitivity matrix: $F = \\frac{1}{\\sigma^2} S^T S$.\n5.  For each candidate $h_i$ ($i \\in \\{1,2,3,4\\}$):\n    a. Calculate $F_{\\text{base}}$ and $F_{i}$.\n    b. Compute the total FIM: $F_{\\text{total}} = F_{\\text{base}} + F_{i}$.\n    c. Extract the $2 \\times 2$ submatrix for $(x_0, c)$, which are the parameters at indices $0$ and $2$.\n    d. Regularize the submatrix by adding a small diagonal term $\\epsilon I$ to ensure numerical stability.\n    e. Calculate the score as the natural logarithm of the determinant of the regularized submatrix.\n6.  Determine the index $i$ that yields the maximum score and report it.\nThis process is repeated for all four test cases.",
            "answer": "```python\nimport numpy as np\n\ndef x_model(t, params):\n    \"\"\"\n    Computes the state x(t) = x0 * exp(r*t).\n    \n    Args:\n        t (np.ndarray): Time points.\n        params (tuple): (x0, r, c).\n        \n    Returns:\n        np.ndarray: State x at times t.\n    \"\"\"\n    x0, r, c = params\n    return x0 * np.exp(r * t)\n\ndef h_base(t, params):\n    \"\"\"Observable h_base(t) = c * x(t).\"\"\"\n    x0, r, c = params\n    return c * x_model(t, params)\n\ndef h1(t, params):\n    \"\"\"Observable h1(t) = x(t).\"\"\"\n    return x_model(t, params)\n    \ndef h2(t, params):\n    \"\"\"Observable h2(t) = r * x(t).\"\"\"\n    x0, r, c = params\n    return r * x_model(t, params)\n\ndef h3(t, params):\n    \"\"\"Observable h3(t) = integral of x(s) from 0 to t.\"\"\"\n    x0, r, c = params\n    # Handle the case r -> 0 to avoid division by zero.\n    if abs(r)  1e-9:\n        return x0 * t\n    return (x_model(t, (x0, r, c)) - x0) / r\n\ndef h4(t, params):\n    \"\"\"Observable h4(t) = x(t)^2.\"\"\"\n    return x_model(t, params)**2\n\ndef calculate_sensitivities(model_func, params, t_grid):\n    \"\"\"\n    Calculates sensitivities of a model output with respect to parameters\n    using central finite differences.\n    \"\"\"\n    n_params = len(params)\n    n_times = len(t_grid)\n    sensitivities = np.zeros((n_times, n_params))\n    \n    eps_rel = 1e-7  # Relative step size for finite differences\n\n    for i in range(n_params):\n        params_plus = list(params)\n        params_minus = list(params)\n        \n        # Robust step size selection\n        step = eps_rel * (abs(params[i]) + 1e-8)\n        \n        params_plus[i] += step\n        params_minus[i] -= step\n        \n        y_plus = model_func(t_grid, tuple(params_plus))\n        y_minus = model_func(t_grid, tuple(params_minus))\n        \n        sensitivities[:, i] = (y_plus - y_minus) / (2 * step)\n        \n    return sensitivities\n\ndef calculate_fim(sensitivities, sigma):\n    \"\"\"\n    Calculates the Fisher Information Matrix from a sensitivity matrix.\n    FIM = (1/sigma^2) * S^T * S\n    \"\"\"\n    return (sensitivities.T @ sensitivities) / (sigma**2)\n\ndef find_best_observable(params, t_grid, sigma):\n    \"\"\"\n    Finds the best observable h_i to add to h_base for identifying (x0, c).\n    \"\"\"\n    candidate_observables = [h1, h2, h3, h4]\n    scores = []\n    \n    # FIM for h_base is calculated once\n    sens_base = calculate_sensitivities(h_base, params, t_grid)\n    fim_base = calculate_fim(sens_base, sigma)\n    \n    for h_i in candidate_observables:\n        # Calculate FIM for the candidate observable\n        sens_i = calculate_sensitivities(h_i, params, t_grid)\n        fim_i = calculate_fim(sens_i, sigma)\n        \n        # Total FIM is the sum\n        fim_total = fim_base + fim_i\n        \n        # Extract the 2x2 block for parameters (x0, c) at indices 0 and 2\n        fim_block = fim_total[np.ix_([0, 2], [0, 2])]\n        \n        # Regularize and compute score (log-determinant)\n        reg = 1e-12\n        fim_block_reg = fim_block + reg * np.eye(2)\n        score = np.log(np.linalg.det(fim_block_reg))\n        scores.append(score)\n        \n    # Return the 1-based index of the best observable\n    best_index = np.argmax(scores) + 1\n    return best_index\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {'params': (2.0, 0.7, 0.6), 't_grid': np.linspace(0.0, 1.5, 16), 'sigma': 0.1},\n        # Test case B\n        {'params': (1.0, 0.05, 0.5), 't_grid': np.linspace(0.0, 0.3, 7), 'sigma': 0.2},\n        # Test case C\n        {'params': (3.0, 1.2, 0.3), 't_grid': np.linspace(0.0, 0.6, 13), 'sigma': 0.5},\n        # Test case D\n        {'params': (0.5, 0.9, 1.3), 't_grid': np.linspace(0.0, 1.0, 11), 'sigma': 0.05}\n    ]\n\n    results = []\n    for case in test_cases:\n        best_idx = find_best_observable(case['params'], case['t_grid'], case['sigma'])\n        results.append(best_idx)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Profile likelihood is most powerful when used to diagnose and resolve non-identifiability in complex, realistic models. This exercise simulates a common scenario in cell signaling, where parameters are unidentifiable from a single experiment but can be constrained by pooling data from multiple, complementary perturbations. By implementing a full profile likelihood analysis for a multi-experiment design, you will see firsthand how strategically combining data is crucial for building robust and predictive biological models .",
            "id": "3340990",
            "problem": "Consider a single-compartment signaling model in which the activated fraction $x(t)$ of a receptor or downstream effector evolves under a constant input $u(t)$ and two kinetic processes: activation with rate $k_{\\mathrm{on}}$ and deactivation with rate $k_{\\mathrm{off}}$. The state equation is the ordinary differential equation $\\dot{x}(t) = k_{\\text{on}}\\,u(t)\\,(1 - x(t)) - k_{\\text{off}}\\,x(t)$, with initial condition $x(0) = 0$. The measurement model is $y(t) = s\\,x(t) + \\epsilon(t)$, where $s$ is a positive observation scale and $\\epsilon(t)$ is additive Gaussian noise with zero mean and known variance $\\sigma^2$. Under a constant input $u(t) = u_0$ and sufficiently long acquisition time $t \\to \\infty$, the system reaches a steady state $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$, hence the late-time measurement satisfies $y \\approx s\\,x_\\infty$.\n\nTwo experiments probe the same model under distinct inhibitors that act multiplicatively on the kinetic parameters:\n- Experiment $1$ modifies activation: $k_{\\mathrm{on}} \\mapsto \\alpha\\,k_{\\mathrm{on}}$ with $\\alpha \\in (0,1]$, and leaves deactivation unchanged.\n- Experiment $2$ modifies deactivation: $k_{\\mathrm{off}} \\mapsto \\beta\\,k_{\\mathrm{off}}$ with $\\beta \\ge 1$, and leaves activation unchanged.\n\nFor each experiment $i \\in \\{1,2\\}$, define the effective steady-state fraction $x_{\\infty,i} = \\dfrac{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0}{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0 + \\beta_i\\,k_{\\mathrm{off}}}$ and the corresponding measurement $y_i = s\\,x_{\\infty,i} + \\epsilon_i$. Assume the noise realizations $\\epsilon_i$ are known deterministic offsets for the purpose of this problem, and the variance $\\sigma^2$ is known for the likelihood.\n\nLet $a = k_{\\text{on}}\\,u_0$ and $b = k_{\\text{off}}$. Given measurements $\\{y_i\\}$ under $\\{(\\alpha_i,\\beta_i)\\}$, the Gaussian negative log-likelihood (up to an additive constant) is proportional to the sum of squared normalized residuals $\\sum_i \\left(\\dfrac{y_i - s\\,\\dfrac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}}{\\sigma}\\right)^2$. The profile likelihood for a parameter value $s = \\vartheta$ is obtained by minimizing this sum of squares over the nuisance parameters $(a,b)$ subject to $a > 0$ and $b > 0$, while holding $s$ fixed at $s = \\vartheta$.\n\nYour task is to implement a complete program that:\n- Generates synthetic steady-state data $y_i$ for specified experiment designs and known true parameters.\n- Computes the joint profile likelihood over a grid of $s$ values by minimizing the sum of squared normalized residuals over $(a,b)$ for each fixed $s$.\n- Uses the likelihood ratio criterion with a Chi-square cutoff to assess structural identifiability of $s$. Specifically, for a given design, compute the profile $\\mathrm{PL}(s)$ as the minimized sum of squared normalized residuals. Let $\\mathrm{PL}_{\\min}$ be its minimum over the grid. The $95\\%$ confidence set for $s$ is $\\{s \\mid \\mathrm{PL}(s) - \\mathrm{PL}_{\\min} \\le \\chi^2_{1,0.95}\\}$, where $\\chi^2_{1,0.95}$ denotes the $95\\%$ quantile of the Chi-square distribution with $1$ degree of freedom. Declare $s$ identifiable if and only if this confidence set is bounded on both sides within the scanned grid. Otherwise, declare $s$ non-identifiable.\n\nStart from the following fundamental base:\n- The state equation $\\dot{x}(t) = k_{\\mathrm{on}}\\,u(t)\\,(1 - x(t)) - k_{\\mathrm{off}}\\,x(t)$ under $u(t) = u_0$ and $x(0) = 0$ implies a steady state $x_\\infty = \\dfrac{k_{\\mathrm{on}}\\,u_0}{k_{\\mathrm{on}}\\,u_0 + k_{\\mathrm{off}}}$ by setting $\\dot{x}(t) = 0$.\n- The inhibitors act multiplicatively and independently on $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$, yielding $x_{\\infty,i} = \\dfrac{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0}{\\alpha_i\\,k_{\\mathrm{on}}\\,u_0 + \\beta_i\\,k_{\\mathrm{off}}}$.\n- The Gaussian negative log-likelihood is proportional to the residual sum of squares divided by $\\sigma^2$, and the likelihood ratio test employs $\\chi^2$ thresholds.\n\nDesign a multi-experiment suite that demonstrates how pooling data breaks non-identifiability present in single-condition fits. Use the following true parameters and measurement noise scale:\n- $u_0 = 1.0$, $k_{\\mathrm{on}} = 1.5$, $k_{\\mathrm{off}} = 0.8$, $s = 2.0$, $\\sigma = 0.02$.\n- For each measurement $y_i$, use deterministic noise offsets $\\epsilon_i$ as specified per case below.\n\nImplement four test cases:\n- Case $1$ (single-condition non-identifiability): one experiment with $(\\alpha_1,\\beta_1) = (0.4,1.0)$ and $\\epsilon_1 = 0.005$. Use only $y_1$ in the profile likelihood; compute the identifiability boolean for $s$.\n- Case $2$ (joint pooling resolves non-identifiability): two experiments pooled jointly, with $(\\alpha_1,\\beta_1) = (0.4,1.0)$, $\\epsilon_1 = 0.005$ and $(\\alpha_2,\\beta_2) = (1.0,3.0)$, $\\epsilon_2 = -0.005$. Compute the identifiability boolean for $s$ on the pooled data.\n- Case $3$ (identical conditions do not resolve non-identifiability): two experiments with identical conditions, $(\\alpha_1,\\beta_1) = (1.0,1.0)$, $\\epsilon_1 = 0.003$ and $(\\alpha_2,\\beta_2) = (1.0,1.0)$, $\\epsilon_2 = -0.003$. Compute the identifiability boolean for $s$ on the pooled data.\n- Case $4$ (strong deactivation inhibitor resolves non-identifiability): two experiments pooled jointly, with $(\\alpha_1,\\beta_1) = (0.4,1.0)$, $\\epsilon_1 = 0.005$ and $(\\alpha_2,\\beta_2) = (1.0,10.0)$, $\\epsilon_2 = -0.003$. Compute the identifiability boolean for $s$ on the pooled data.\n\nFor each case, scan $s$ on a grid $s \\in [0.3,4.0]$ using a uniform discretization with $41$ points. Use the Chi-square cutoff $\\chi^2_{1,0.95} \\approx 3.841458821$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above. Each entry must be a boolean indicating identifiability of $s$, such that the final output line has the form \"[b1,b2,b3,b4]\". No units are required because all quantities are dimensionless in this formulation.",
            "solution": "The problem requires the implementation of a profile likelihood analysis to assess the parameter identifiability of a scaling factor, $s$, in a single-compartment signaling model. The analysis will be performed on four distinct experimental scenarios to demonstrate how combining data from different experiments can resolve identifiability issues.\n\nFirst, we establish the mathematical foundation of the problem. The system's state, $x(t)$, is governed by the ordinary differential equation $\\dot{x}(t) = k_{\\text{on}}\\,u(t)\\,(1 - x(t)) - k_{\\text{off}}\\,x(t)$ with $x(0) = 0$. Under a constant input $u(t) = u_0$, the system reaches a steady state, found by setting $\\dot{x}(t) = 0$. This yields $k_{\\text{on}}\\,u_0\\,(1 - x_\\infty) = k_{\\text{off}}\\,x_\\infty$, which solves to $x_\\infty = \\dfrac{k_{\\text{on}}\\,u_0}{k_{\\text{on}}\\,u_0 + k_{\\text{off}}}$.\n\nThe problem introduces a reparameterization with $a = k_{\\text{on}}\\,u_0$ and $b = k_{\\text{off}}$, so the steady state is $x_\\infty = \\dfrac{a}{a+b}$. Experiments involve inhibitors that modify the rate constants multiplicatively. For an experiment $i$, identified by perturbation factors $(\\alpha_i, \\beta_i)$, the effective steady state is given by:\n$$x_{\\infty,i} = \\frac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}$$\nThe measurement model for experiment $i$ is $y_i = s\\,x_{\\infty,i} + \\epsilon_i$, where $s$ is the observation scale parameter of interest and $\\epsilon_i$ is a measurement offset.\n\nThe core of the task is to compute the profile likelihood of $s$. The negative log-likelihood for the parameters $(s, a, b)$, assuming Gaussian noise with known variance $\\sigma^2$, is proportional to the sum of squared normalized residuals. The profile likelihood for a specific value $s = \\vartheta$ is defined as the minimum of this objective function over the nuisance parameters $(a,b)$:\n$$ \\mathrm{PL}(\\vartheta) = \\min_{a0, b0} \\sum_i \\left( \\frac{y_i - \\vartheta\\,\\dfrac{\\alpha_i\\,a}{\\alpha_i\\,a + \\beta_i\\,b}}{\\sigma} \\right)^2 $$\n\nTo assess identifiability, we use the likelihood ratio test. The $95\\%$ confidence set for $s$ is defined as $\\{s \\mid \\mathrm{PL}(s) - \\mathrm{PL}_{\\min} \\le \\chi^2_{1,0.95}\\}$, where $\\mathrm{PL}_{\\min}$ is the minimum value of the profile likelihood over the scanned range of $s$, and $\\chi^2_{1,0.95} \\approx 3.841458821$ is the $95\\%$ quantile of the Chi-square distribution with $1$ degree of freedom. The parameter $s$ is declared identifiable if this confidence interval is bounded on both sides within the predefined grid $[0.3, 4.0]$. This means the profile likelihood must rise above the threshold $\\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$ at both ends of the grid.\n\nBefore implementing the numerical procedure, a structural identifiability analysis provides insight into the expected results.\nFor a single experiment (Case $1$), we have one equation, $y_1 = s \\frac{\\alpha_1 a}{\\alpha_1 a + \\beta_1 b} + \\epsilon_1$, and three unknown parameters $(s,a,b)$. This system is underdetermined, and the parameters are not uniquely identifiable. For any given $s > y_1-\\epsilon_1$, one can find a ratio $b/a$ that satisfies the equation, leading to a path of equally optimal solutions. Thus, we expect $s$ to be non-identifiable.\n\nFor two experiments (Cases $2$, $3$, $4$), we have a system of two equations:\n$$ y_1 - \\epsilon_1 = s \\frac{\\alpha_1 a}{\\alpha_1 a + \\beta_1 b} $$\n$$ y_2 - \\epsilon_2 = s \\frac{\\alpha_2 a}{\\alpha_2 a + \\beta_2 b} $$\nWe can rearrange each equation to solve for the ratio $b/a$: $\\frac{b}{a} = \\frac{\\alpha_i}{\\beta_i} \\left(\\frac{s}{y_i-\\epsilon_i} - 1\\right)$. Equating the expressions for $b/a$ from both experiments yields an equation solely in terms of $s$:\n$$ \\frac{\\alpha_1}{\\beta_1} \\left(\\frac{s}{y_1-\\epsilon_1} - 1\\right) = \\frac{\\alpha_2}{\\beta_2} \\left(\\frac{s}{y_2-\\epsilon_2} - 1\\right) $$\nThis is a linear equation in $s$. A unique solution for $s$ exists if and only if the coefficients of $s$ are different, which simplifies to the condition $\\frac{\\alpha_1}{\\beta_1} \\neq \\frac{\\alpha_2}{\\beta_2}$.\n- In Case $2$, $\\frac{\\alpha_1}{\\beta_1} = 0.4$ and $\\frac{\\alpha_2}{\\beta_2} = 1/3 \\approx 0.333$. The ratios are different, so $s$ should be identifiable.\n- In Case $3$, $\\frac{\\alpha_1}{\\beta_1} = 1.0$ and $\\frac{\\alpha_2}{\\beta_2} = 1.0$. The ratios are identical, implying the experiments are redundant from a structural perspective. The equation for $s$ becomes trivial ($0=0$), and $s$ remains non-identifiable.\n- In Case $4$, $\\frac{\\alpha_1}{\\beta_1} = 0.4$ and $\\frac{\\alpha_2}{\\beta_2} = 0.1$. The ratios are different, so $s$ should be identifiable.\n\nThe implementation will proceed as follows:\n$1$. Define the true parameters, constants, and the grid for $s$.\n$2$. For each of the four test cases:\n    a. Generate the synthetic measurement data $\\{y_i\\}$ using the provided true parameters $(k_{\\mathrm{on}}, k_{\\mathrm{off}}, s)$, experimental conditions $(\\alpha_i, \\beta_i)$, and noise offsets $\\epsilon_i$.\n    b. Initialize an array to store the profile likelihood values, $\\mathrm{PL}(s)$.\n    c. Iterate through each value $\\vartheta$ in the grid for $s$. For each $\\vartheta$, define an objective function of parameters $(a,b)$ representing the sum of squared residuals.\n    d. Use `scipy.optimize.minimize` with the `L-BFGS-B` method and bounds $a0$, $b0$ to find the minimum of this objective function. The minimized value is $\\mathrm{PL}(\\vartheta)$.\n    e. After scanning the entire grid, find the global minimum of the profile, $\\mathrm{PL}_{\\min}$.\n    f. Apply the identifiability criterion: check if $\\mathrm{PL}(s_{min\\_grid})  \\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$ and $\\mathrm{PL}(s_{max\\_grid})  \\mathrm{PL}_{\\min} + \\chi^2_{1,0.95}$.\n    g. Store the resulting boolean value.\n$3$. Print the final list of booleans in the specified format `[b1,b2,b3,b4]`.\nThis procedure will numerically validate the conclusions from our structural analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes parameter identifiability for a systems biology model using profile likelihood.\n    \"\"\"\n    # True parameters for data generation\n    u0_true = 1.0\n    k_on_true = 1.5\n    k_off_true = 0.8\n    s_true = 2.0\n    sigma = 0.02\n\n    # Derived true parameters for optimization\n    a_true = k_on_true * u0_true\n    b_true = k_off_true\n\n    # Constants for the analysis\n    chi2_cutoff = 3.841458821\n    s_grid = np.linspace(0.3, 4.0, 41)\n\n    # Definition of the four test cases\n    test_cases = [\n        {\n            # Case 1: Single experiment, expected to be non-identifiable.\n            \"alphas\": np.array([0.4]),\n            \"betas\": np.array([1.0]),\n            \"epsilons\": np.array([0.005]),\n        },\n        {\n            # Case 2: Two distinct experiments, expected to be identifiable.\n            \"alphas\": np.array([0.4, 1.0]),\n            \"betas\": np.array([1.0, 3.0]),\n            \"epsilons\": np.array([0.005, -0.005]),\n        },\n        {\n            # Case 3: Two identical experiments, expected to be non-identifiable.\n            \"alphas\": np.array([1.0, 1.0]),\n            \"betas\": np.array([1.0, 1.0]),\n            \"epsilons\": np.array([0.003, -0.003]),\n        },\n        {\n            # Case 4: Two distinct experiments, expected to be identifiable.\n            \"alphas\": np.array([0.4, 1.0]),\n            \"betas\": np.array([1.0, 10.0]),\n            \"epsilons\": np.array([0.005, -0.003]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        alphas = case[\"alphas\"]\n        betas = case[\"betas\"]\n        epsilons = case[\"epsilons\"]\n\n        # 1. Generate synthetic data using true parameter values\n        x_inf_true = (alphas * a_true) / (alphas * a_true + betas * b_true)\n        y_data = s_true * x_inf_true + epsilons\n\n        # Define the objective function (sum of squared normalized residuals)\n        # to be minimized for each point in the profile likelihood.\n        def objective_function(params, s_val, alphas_exp, betas_exp, y_exp, sigma_val):\n            a, b = params\n            # The denominator can't be zero due to a>0, b>0 and alpha,beta>0\n            y_model = s_val * (alphas_exp * a) / (alphas_exp * a + betas_exp * b)\n            residuals = (y_exp - y_model) / sigma_val\n            return np.sum(residuals**2)\n\n        # 2. Compute the profile likelihood over the grid of s values\n        pl_values = []\n        for s_val in s_grid:\n            # The optimization finds the best-fit nuisance parameters (a, b) for a fixed s.\n            # Initial guess is set to the true values for stability.\n            # Bounds enforce the physical constraint that rates must be positive.\n            res = minimize(\n                objective_function,\n                x0=[a_true, b_true],\n                args=(s_val, alphas, betas, y_data, sigma),\n                method='L-BFGS-B',\n                bounds=[(1e-9, None), (1e-9, None)]\n            )\n            pl_values.append(res.fun)\n\n        pl_values = np.array(pl_values)\n\n        # 3. Assess identifiability based on the likelihood ratio test\n        pl_min = np.min(pl_values)\n        threshold = pl_min + chi2_cutoff\n\n        # s is identifiable if the 95% confidence interval is bounded on both sides\n        # within the scanned grid. This means the profile at the grid boundaries must\n        # be above the threshold.\n        is_identifiable = (pl_values[0] > threshold) and (pl_values[-1] > threshold)\n        results.append(is_identifiable)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\".lower())\n\nsolve()\n```"
        }
    ]
}