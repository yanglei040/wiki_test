{
    "hands_on_practices": [
        {
            "introduction": "In Bayesian calibration, the gradient of the objective function combines information from the data with prior knowledge about the parameters. This first exercise focuses on the prior's contribution, which acts as a powerful regularization term to guide the optimization. By deriving the gradient for a common multivariate Gaussian prior, you will gain a concrete understanding of how prior uncertainty, encoded in the covariance matrix $\\Sigma$, creates an anisotropic pull on the parameters during gradient descent .",
            "id": "3287571",
            "problem": "Consider calibrating a parameter vector $\\theta \\in \\mathbb{R}^{d}$ for a biochemical reaction network whose state $x(t;\\theta) \\in \\mathbb{R}^{n}$ evolves according to an Ordinary Differential Equation (ODE)\n$$\n\\frac{d x}{d t} = f\\big(x(t;\\theta),\\theta\\big), \\quad x(0;\\theta) = x_{0},\n$$\nwith observations $y_{i} \\in \\mathbb{R}^{m}$ at times $t_{i}$ related to the state by a measurement operator $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$. A standard negative log-posterior objective for gradient-based calibration is\n$$\nJ(\\theta) = \\frac{1}{2}\\sum_{i=1}^{N} \\left\\| W^{1/2}\\big(h(x(t_{i};\\theta)) - y_{i}\\big) \\right\\|_{2}^{2} \\;+\\; \\phi_{\\text{prior}}(\\theta),\n$$\nwhere $W \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite and encodes observational weights, and $\\phi_{\\text{prior}}(\\theta)$ is the negative log-prior penalty. Assume a multivariate normal prior on $\\theta$ with mean $\\mu \\in \\mathbb{R}^{d}$ and covariance $\\Sigma \\in \\mathbb{R}^{d \\times d}$, where $\\Sigma$ is symmetric positive definite, so the prior density is\n$$\np(\\theta) = (2\\pi)^{-d/2}|\\Sigma|^{-1/2}\\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right).\n$$\nStarting from this fundamental definition of the multivariate normal density, construct $\\phi_{\\text{prior}}(\\theta)$ and derive its gradient $\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta)$ explicitly in terms of $\\theta$, $\\mu$, and $\\Sigma$. Then, explain how the anisotropy encoded by $\\Sigma$ modifies step sizes in gradient-based calibration when combined with the adjoint-computed gradient of the data misfit, with reference to the eigenvalues and eigenvectors of $\\Sigma$ and the implications for choosing a global step size $\\alpha$ in an iteration of the form $\\theta_{k+1} = \\theta_{k} - \\alpha \\nabla_{\\theta}J(\\theta_{k})$. Your final answer must be the exact closed-form analytic expression for $\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta)$, with no numerical approximation.",
            "solution": "The problem asks for three components: the construction of the negative log-prior penalty $\\phi_{\\text{prior}}(\\theta)$, the derivation of its gradient $\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta)$, and an explanation of the implications of this prior for gradient-based calibration.\n\nFirst, we construct the negative log-prior penalty, $\\phi_{\\text{prior}}(\\theta)$, from the given prior probability density function (PDF). The prior distribution on the parameter vector $\\theta \\in \\mathbb{R}^{d}$ is a multivariate normal with mean $\\mu \\in \\mathbb{R}^{d}$ and covariance $\\Sigma \\in \\mathbb{R}^{d \\times d}$. The PDF is given as:\n$$\np(\\theta) = (2\\pi)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}}\\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right)\n$$\nThe negative log-prior, $\\phi_{\\text{prior}}(\\theta)$, is defined as the negative natural logarithm of the prior PDF, often omitting terms that are constant with respect to $\\theta$ as they do not affect the gradient. Taking the natural logarithm of $p(\\theta)$ gives:\n$$\n\\ln(p(\\theta)) = \\ln\\left((2\\pi)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}}\\right) + \\ln\\left(\\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right)\\right)\n$$\n$$\n\\ln(p(\\theta)) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma|) - \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\n$$\nTherefore, the negative log-prior is:\n$$\n\\phi_{\\text{prior}}(\\theta) = -\\ln(p(\\theta)) = \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) + \\frac{d}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln(|\\Sigma|)\n$$\nFor optimization purposes, the constant terms are irrelevant, so the penalty is effectively the quadratic term.\n\nNext, we derive the gradient of $\\phi_{\\text{prior}}(\\theta)$ with respect to $\\theta$. We need to compute the gradient of the expression derived above. The constant terms have a zero gradient.\n$$\n\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta) = \\nabla_{\\theta}\\left( \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) \\right)\n$$\nWe use the standard matrix calculus identity for the gradient of a quadratic form. For a vector variable $z$ and a symmetric matrix $A$, the gradient is given by $\\nabla_{z}\\left( (z-c)^{\\top}A(z-c) \\right) = 2A(z-c)$. In our case, the variable is $\\theta$, the constant vector is $\\mu$, and the matrix is $\\Sigma^{-1}$. The covariance matrix $\\Sigma$ is given as symmetric positive definite, which implies that its inverse, $\\Sigma^{-1}$, is also symmetric. Applying this identity, we get:\n$$\n\\nabla_{\\theta}\\left( (\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) \\right) = 2\\Sigma^{-1}(\\theta - \\mu)\n$$\nSubstituting this result back into the expression for the gradient of $\\phi_{\\text{prior}}(\\theta)$:\n$$\n\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta) = \\frac{1}{2} \\left( 2\\Sigma^{-1}(\\theta - \\mu) \\right) = \\Sigma^{-1}(\\theta - \\mu)\n$$\nThis is the analytical expression for the gradient of the prior penalty term.\n\nFinally, we explain the implications of this prior gradient for gradient-based calibration. The total gradient of the objective function $J(\\theta)$ is the sum of the gradient of the data-misfit term and the gradient of the prior penalty:\n$$\n\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta}\\left( \\frac{1}{2}\\sum_{i=1}^{N} \\left\\| W^{\\frac{1}{2}}(h(x(t_{i};\\theta)) - y_{i}) \\right\\|_{2}^{2} \\right) + \\Sigma^{-1}(\\theta - \\mu)\n$$\nThe data-misfit gradient, typically computed via the adjoint method, drives the parameters towards values that fit the observations $y_i$. The prior gradient, $\\Sigma^{-1}(\\theta - \\mu)$, acts as a regularization term that pulls the parameter vector $\\theta$ towards the prior mean $\\mu$.\n\nThe key to understanding its effect is the matrix $\\Sigma^{-1}$, known as the precision matrix. The covariance matrix $\\Sigma$ is symmetric positive definite and can be diagonalized as $\\Sigma = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix whose columns are the eigenvectors $v_j$ of $\\Sigma$, and $\\Lambda$ is a diagonal matrix of the corresponding positive eigenvalues $\\lambda_j$. The eigenvectors $v_j$ define the principal axes of the prior probability distribution, and the eigenvalues $\\lambda_j$ represent the variance (a measure of uncertainty) of the prior along those axes.\n\nThe precision matrix is then $\\Sigma^{-1} = (V \\Lambda V^{\\top})^{-1} = V \\Lambda^{-1} V^{\\top}$. Its eigenvalues are $1/\\lambda_j$. The prior gradient contribution is $\\Sigma^{-1}(\\theta-\\mu) = V \\Lambda^{-1} V^{\\top}(\\theta-\\mu)$. This term reveals the anisotropic nature of the regularization:\n\\begin{enumerate}\n    \\item A small eigenvalue $\\lambda_j$ of $\\Sigma$ corresponds to a direction $v_j$ in which the prior knowledge is very certain (low variance). The corresponding eigenvalue of $\\Sigma^{-1}$ is large ($1/\\lambda_j$). Consequently, any deviation of $\\theta$ from the mean $\\mu$ along this direction $v_j$ results in a very large gradient component, strongly penalizing the deviation and pushing the parameter value back towards the prior mean along that axis.\n    \\item A large eigenvalue $\\lambda_k$ of $\\Sigma$ corresponds to a direction $v_k$ of high prior uncertainty (large variance). The corresponding eigenvalue of $\\Sigma^{-1}$ is small ($1/\\lambda_k$). Deviations along this direction are penalized weakly, allowing the data-misfit gradient to have a greater influence on the parameter update in this direction.\n\\end{enumerate}\nThis anisotropy has significant implications for a standard gradient descent update step of the form $\\theta_{k+1} = \\theta_{k} - \\alpha \\nabla_{\\theta}J(\\theta_{k})$, where $\\alpha$ is a single, global step size. The total gradient vector $\\nabla_{\\theta}J(\\theta_{k})$ will have components of vastly different magnitudes, especially if the eigenvalues of $\\Sigma$ span several orders of magnitude. The maximum permissible step size $\\alpha$ is limited by the need to maintain stability in the direction of the largest gradient component, which is typically the direction associated with the smallest prior variance (smallest $\\lambda_j$, largest $1/\\lambda_j$). Using such a small $\\alpha$ for all parameter directions causes extremely slow convergence in directions where the gradient is small (e.g., those corresponding to large prior variances). This problem, known as ill-conditioning, demonstrates that an un-preconditioned gradient descent with a scalar step size is inefficient for optimizing objective functions with anisotropic priors. The prior's structure inherently suggests that different step sizes should be taken along different parameter directions, motivating the use of preconditioning or second-order optimization methods.",
            "answer": "$$\\boxed{\\Sigma^{-1}(\\theta - \\mu)}$$"
        },
        {
            "introduction": "While analytical derivations provide the blueprint for the gradient, their implementation in code is a frequent source of subtle bugs. This practice introduces a fundamental verification tool known as the discrete adjoint identity test, a numerical 'litmus test' for your solver's correctness. By implementing this test for a simple biochemical network, you will learn a robust procedure to validate that your discrete adjoint code accurately reflects the linearized forward model before deploying it in a full optimization loop .",
            "id": "3287583",
            "problem": "Consider a three-species biochemical reaction network modeled by an ordinary differential equation (ODE) in continuous time. Let the state be $x(t) \\in \\mathbb{R}^3$ with components $x_1(t)$, $x_2(t)$, $x_3(t)$ denoting concentrations, and let the parameter vector be $\\theta \\in \\mathbb{R}^7$ with components $\\theta_1,\\theta_2,\\theta_3,\\theta_4,\\theta_5,\\theta_6,\\theta_7$, all nonnegative reaction rate constants. The continuous-time right-hand side is defined by mass-action kinetics:\n$$\nf(x,\\theta) = \\begin{bmatrix}\n\\theta_1 - \\theta_2 x_1 - \\theta_3 x_1 x_2 \\\\\n\\theta_4 x_1 - \\theta_5 x_2 \\\\\n\\theta_6 x_2 - \\theta_7 x_3\n\\end{bmatrix},\n$$\nand its Jacobian with respect to $x$ is\n$$\nJ(x,\\theta) = \\frac{\\partial f(x,\\theta)}{\\partial x} = \\begin{bmatrix}\n-\\theta_2 - \\theta_3 x_2 & -\\theta_3 x_1 & 0 \\\\\n\\theta_4 & -\\theta_5 & 0 \\\\\n0 & \\theta_6 & -\\theta_7\n\\end{bmatrix}.\n$$\nDiscretize the dynamics over a single step using the backward Euler method with time step $\\Delta t > 0$ (in seconds):\n$$\nx_{k+1} - x_k - \\Delta t \\, f(x_{k+1},\\theta) = 0,\n$$\nand consider arbitrary perturbations $\\delta x_{k+1} \\in \\mathbb{R}^3$ at the state $x_{k+1}$, as well as arbitrary adjoint vectors $\\lambda_k,\\lambda_{k+1} \\in \\mathbb{R}^3$. The linearized tangent mapping for backward Euler is given by\n$$\n\\delta x_k = \\left(I - \\Delta t \\, J(x_{k+1},\\theta)\\right) \\, \\delta x_{k+1}.\n$$\nDefine the discrete adjoint residual at step $k+1$ by\n$$\nr_{k+1} := \\frac{\\left(I - \\Delta t \\, J(x_{k+1},\\theta)^\\top\\right)\\lambda_k - \\lambda_{k+1}}{\\Delta t}.\n$$\nThe discrete adjoint identity to be tested states that, for arbitrary perturbations $\\delta x_{k+1}$ and adjoint vectors $\\lambda_k,\\lambda_{k+1}$, the following equality holds:\n$$\n\\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle,\n$$\nwhere $\\langle \\cdot,\\cdot \\rangle$ denotes the standard Euclidean inner product.\n\nYour task is to implement a discrete adjoint identity test for this backward Euler discretization, using the above $f(x,\\theta)$ and $J(x,\\theta)$, and verify the identity numerically for multiple cases with arbitrary perturbations and adjoint vectors. For each test case, you must:\n- Compute $J(x_{k+1},\\theta)$.\n- Compute $\\delta x_k = \\left(I - \\Delta t \\, J(x_{k+1},\\theta)\\right)\\delta x_{k+1}$.\n- Compute $r_{k+1} = \\frac{\\left(I - \\Delta t \\, J(x_{k+1},\\theta)^\\top\\right)\\lambda_k - \\lambda_{k+1}}{\\Delta t}$.\n- Compute the absolute error\n$$\n\\varepsilon = \\left| \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle - \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle \\right|.\n$$\n\nScientific realism requirements:\n- All reaction rate constants $\\theta_i$ must be nonnegative.\n- All concentrations $x_i$ must be nonnegative.\n- Time step $\\Delta t$ must be positive and expressed in seconds.\n\nTest suite:\nImplement the test for the following five cases. In each case, generate the adjoint vectors $\\lambda_k,\\lambda_{k+1}$ and the perturbation $\\delta x_{k+1}$ using a fixed seed for a normal distribution with zero mean and unit variance (to ensure deterministic, arbitrary perturbations). Use the three-dimensional standard normal distribution independently for each vector.\n\n- Case $1$ (happy path): $\\Delta t = 0.1$ seconds, $\\theta = [0.8, 0.5, 0.4, 0.3, 0.2, 1.0, 0.7]$, $x_{k+1} = [1.0, 0.8, 0.5]$, seed $101$.\n- Case $2$ (boundary: very small time step): $\\Delta t = 10^{-6}$ seconds, $\\theta = [0.2, 0.1, 0.05, 0.3, 0.25, 0.4, 0.35]$, $x_{k+1} = [0.1, 0.05, 0.02]$, seed $202$.\n- Case $3$ (stiff-ish dynamics: large time step and rates): $\\Delta t = 1.0$ seconds, $\\theta = [2.0, 1.5, 1.2, 0.8, 0.9, 1.1, 1.0]$, $x_{k+1} = [3.0, 2.0, 1.0]$, seed $303$.\n- Case $4$ (near-degenerate $I - \\Delta t J$: large interactions): $\\Delta t = 0.9$ seconds, $\\theta = [1.0, 1.0, 5.0, 2.5, 0.1, 0.6, 0.5]$, $x_{k+1} = [10.0, 10.0, 10.0]$, seed $404$.\n- Case $5$ (near-zero Jacobian: very small concentrations): $\\Delta t = 0.05$ seconds, $\\theta = [0.1, 0.1, 0.05, 0.2, 0.1, 0.15, 0.12]$, $x_{k+1} = [10^{-9}, 2 \\cdot 10^{-9}, 3 \\cdot 10^{-9}]$, seed $505$.\n\nFinal output specification:\nYour program should produce a single line of output containing the absolute errors for the five cases as a comma-separated list enclosed in square brackets (e.g., $[e_1,e_2,e_3,e_4,e_5]$). Each $e_i$ must be a real number (floating-point) representing the computed $\\varepsilon$ for the corresponding case. The errors are dimensionless real numbers. No other text should be printed.",
            "solution": "The problem requires the numerical verification of a discrete adjoint identity for a system of ordinary differential equations (ODEs) discretized using the backward Euler method. The identity is a fundamental property relating the forward propagation of perturbations (the tangent model) to the backward propagation of sensitivities (the adjoint model). Verifying this identity is a critical step in a \"gradient check\" to ensure a correct implementation of an adjoint solver. The expected outcome is that the absolute error $\\varepsilon$ will be zero, up to the limits of floating-point arithmetic precision.\n\nThe derivation of the identity forms the theoretical basis. The backward Euler discretization defines a residual equation for a single time step, $G(x_k, x_{k+1}, \\theta) = x_{k+1} - x_k - \\Delta t \\, f(x_{k+1}, \\theta) = 0$. The linearized tangent model describes how a small perturbation $\\delta x_{k+1}$ at time $k+1$ relates to the corresponding perturbation $\\delta x_k$ at time $k$ required to keep the residual zero. This is found by taking the total differential of $G$:\n$$ dG = \\frac{\\partial G}{\\partial x_k} \\delta x_k + \\frac{\\partial G}{\\partial x_{k+1}} \\delta x_{k+1} = 0 $$\nWith $\\frac{\\partial G}{\\partial x_k} = -I$ and $\\frac{\\partial G}{\\partial x_{k+1}} = I - \\Delta t \\frac{\\partial f}{\\partial x_{k+1}} = I - \\Delta t J(x_{k+1}, \\theta)$, we have:\n$$ -I \\delta x_k + (I - \\Delta t J(x_{k+1}, \\theta)) \\delta x_{k+1} = 0 $$\nThis rearranges to the provided linearized tangent mapping:\n$$ \\delta x_k = (I - \\Delta t J(x_{k+1}, \\theta)) \\delta x_{k+1} $$\nLet us denote the linear operator $A = I - \\Delta t J(x_{k+1}, \\theta)$. Thus, $\\delta x_k = A \\delta x_{k+1}$.\n\nThe discrete adjoint identity, $\\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle$, connects the forward map $A$ to its adjoint, $A^\\top$. We can prove this identity by substituting the definitions of $\\delta x_k$ and $r_{k+1}$.\n\nLet LHS be the left-hand side and RHS be the right-hand side of the identity.\n$$ \\text{LHS} = \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nSubstitute $\\delta x_k = A \\delta x_{k+1}$:\n$$ \\text{LHS} = \\langle \\lambda_k, A \\delta x_{k+1} \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nUsing the property of the adjoint operator for the Euclidean inner product, $\\langle u, Av \\rangle = \\langle A^\\top u, v \\rangle$:\n$$ \\text{LHS} = \\langle A^\\top \\lambda_k, \\delta x_{k+1} \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nBy linearity of the inner product:\n$$ \\text{LHS} = \\langle A^\\top \\lambda_k - \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nNow consider the RHS:\n$$ \\text{RHS} = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle $$\nSubstitute the definition of the adjoint residual, $r_{k+1} = \\frac{A^\\top \\lambda_k - \\lambda_{k+1}}{\\Delta t}$:\n$$ \\text{RHS} = \\Delta t \\, \\left\\langle \\frac{A^\\top \\lambda_k - \\lambda_{k+1}}{\\Delta t}, \\delta x_{k+1} \\right\\rangle $$\nAgain, by linearity of the inner product:\n$$ \\text{RHS} = \\langle \\Delta t \\left( \\frac{A^\\top \\lambda_k - \\lambda_{k+1}}{\\Delta t} \\right), \\delta x_{k+1} \\rangle = \\langle A^\\top \\lambda_k - \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nSince LHS = RHS, the identity is mathematically exact.\n\nThe algorithmic implementation will follow these theoretical steps for each test case:\n1.  Set the random seed for reproducibility.\n2.  Generate the arbitrary vectors $\\lambda_k, \\lambda_{k+1}, \\delta x_{k+1} \\in \\mathbb{R}^3$ from a standard normal distribution.\n3.  Construct the Jacobian matrix $J(x_{k+1}, \\theta)$ using the provided state $x_{k+1}$ and parameters $\\theta$.\n4.  Compute the matrix $A = I - \\Delta t J(x_{k+1}, \\theta)$.\n5.  Calculate the propagated perturbation $\\delta x_k = A \\delta x_{k+1}$.\n6.  Calculate the adjoint residual $r_{k+1} = \\frac{1}{\\Delta t} (A^\\top \\lambda_k - \\lambda_{k+1})$.\n7.  Compute the LHS, $L = \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle$, using the vector dot product.\n8.  Compute the RHS, $R = \\Delta t \\langle r_{k+1}, \\delta x_{k+1} \\rangle$, using the vector dot product.\n9.  Finally, calculate the absolute error $\\varepsilon = |L - R|$. This value is expected to be on the order of machine epsilon, confirming the correctness of the numerical implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a discrete adjoint identity test for a backward Euler discretization\n    of a 3-species biochemical reaction network.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dt\": 0.1,\n            \"theta\": np.array([0.8, 0.5, 0.4, 0.3, 0.2, 1.0, 0.7]),\n            \"x_kp1\": np.array([1.0, 0.8, 0.5]),\n            \"seed\": 101,\n        },\n        {\n            \"dt\": 1e-6,\n            \"theta\": np.array([0.2, 0.1, 0.05, 0.3, 0.25, 0.4, 0.35]),\n            \"x_kp1\": np.array([0.1, 0.05, 0.02]),\n            \"seed\": 202,\n        },\n        {\n            \"dt\": 1.0,\n            \"theta\": np.array([2.0, 1.5, 1.2, 0.8, 0.9, 1.1, 1.0]),\n            \"x_kp1\": np.array([3.0, 2.0, 1.0]),\n            \"seed\": 303,\n        },\n        {\n            \"dt\": 0.9,\n            \"theta\": np.array([1.0, 1.0, 5.0, 2.5, 0.1, 0.6, 0.5]),\n            \"x_kp1\": np.array([10.0, 10.0, 10.0]),\n            \"seed\": 404,\n        },\n        {\n            \"dt\": 0.05,\n            \"theta\": np.array([0.1, 0.1, 0.05, 0.2, 0.1, 0.15, 0.12]),\n            \"x_kp1\": np.array([1e-9, 2e-9, 3e-9]),\n            \"seed\": 505,\n        },\n    ]\n\n    def compute_jacobian(x, theta):\n        \"\"\"\n        Computes the Jacobian matrix J(x, theta).\n        x: state vector [x1, x2, x3]\n        theta: parameter vector [theta1, ..., theta7]\n        \"\"\"\n        x1, x2, x3 = x\n        th1, th2, th3, th4, th5, th6, th7 = theta\n        \n        J = np.zeros((3, 3))\n        \n        J[0, 0] = -th2 - th3 * x2\n        J[0, 1] = -th3 * x1\n        J[0, 2] = 0.0\n        \n        J[1, 0] = th4\n        J[1, 1] = -th5\n        J[1, 2] = 0.0\n        \n        J[2, 0] = 0.0\n        J[2, 1] = th6\n        J[2, 2] = -th7\n        \n        return J\n\n    results = []\n    for case in test_cases:\n        dt = case[\"dt\"]\n        theta = case[\"theta\"]\n        x_kp1 = case[\"x_kp1\"]\n        seed = case[\"seed\"]\n\n        # Set the seed for deterministic random vector generation\n        np.random.seed(seed)\n\n        # Generate arbitrary vectors from N(0, I_3)\n        lambda_k = np.random.randn(3)\n        lambda_kp1 = np.random.randn(3)\n        delta_x_kp1 = np.random.randn(3)\n\n        # Compute the Jacobian J(x_{k+1}, theta)\n        J_kp1 = compute_jacobian(x_kp1, theta)\n\n        # The linear operator from the linearized backward Euler step\n        # A = I - dt * J\n        I = np.identity(3)\n        A = I - dt * J_kp1\n\n        # Compute delta_x_k using the linearized tangent mapping\n        # delta_x_k = (I - dt*J) * delta_x_{k+1}\n        delta_x_k = A @ delta_x_kp1\n\n        # Compute the discrete adjoint residual r_{k+1}\n        # r_{k+1} = ((I - dt*J^T)*lambda_k - lambda_{k+1}) / dt\n        A_T = A.T # (I - dt*J)^T = I - dt*J^T\n        r_kp1 = (A_T @ lambda_k - lambda_kp1) / dt\n        \n        # Compute the Left-Hand Side (LHS) of the adjoint identity\n        # LHS = <lambda_k, delta_x_k> - <lambda_{k+1}, delta_x_{k+1}>\n        lhs = np.dot(lambda_k, delta_x_k) - np.dot(lambda_kp1, delta_x_kp1)\n\n        # Compute the Right-Hand Side (RHS) of the adjoint identity\n        # RHS = dt * <r_{k+1}, delta_x_{k+1}>\n        rhs = dt * np.dot(r_kp1, delta_x_kp1)\n\n        # Compute the absolute error epsilon\n        # epsilon = |LHS - RHS|\n        error = np.abs(lhs - rhs)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world biological systems rarely exist in isolation, often involving the coupling of multiple physical and chemical processes. This capstone practice integrates the concepts of previous sections to tackle a coupled mechanochemical model, a common scenario in tissue and cell modeling. You will derive the complete partitioned adjoint system from first principles using the Lagrangian method and implement the full forward-adjoint workflow to compute the gradient, demonstrating how to handle the complexity of multi-physics interactions in a calibration context .",
            "id": "3287555",
            "problem": "Consider a mechanochemical model on a one-dimensional spatial domain that couples a quasi-static elastic equilibrium equation with a reaction–diffusion equation. The governing continuous Partial Differential Equations (PDEs) are given by elasticity $\\nabla\\cdot(\\sigma(u,p))=0$ and reaction–diffusion $\\partial_t c = \\nabla\\cdot(D\\nabla c) + R(c,u,p)$, where $u$ is displacement, $c$ is concentration, and $p$ is a vector of parameters. You will derive and implement a partitioned discrete adjoint for a specific linear, time-discrete, and space-discrete model to compute the gradient of a calibration cost functional when observations depend on both $u$ and $c$.\n\nStart from the following fundamental base:\n- Conservation of linear momentum in quasi-static elasticity, leading to a linear elliptic equation for $u$ under small strains.\n- Fick’s law of diffusion and mass balance for $c$, leading to a parabolic equation for $c$.\n- Standard finite difference discretization of spatial derivatives on a uniform grid with homogeneous Dirichlet boundary conditions.\n- Backward Euler time discretization for linear diffusion.\n\nDiscretization setup:\n- Spatial domain is $[0,1]$ with $N$ interior grid points, where $N = 10$. Let $\\Delta x = 1/(N+1)$ and $x_i = i \\Delta x$ for $i = 1,\\dots,N$.\n- Time interval is $[0,T]$ with $N_t = 5$ equal time steps and $\\Delta t = 0.1$, so $t_n = n \\Delta t$ for $n=0,\\dots,N_t$ and $T = N_t \\Delta t = 0.5$.\n- The discrete Laplacian $L \\in \\mathbb{R}^{N \\times N}$ is defined by homogeneous Dirichlet boundary conditions with the standard second-order central difference stencil:\n$$\n(L v)_i = \\frac{v_{i-1} - 2 v_i + v_{i+1}}{\\Delta x^2}, \\quad v_0 = v_{N+1} = 0,\\quad i=1,\\dots,N.\n$$\n- The elasticity operator is defined as $K = - E L + k_u I$, with $E = 1.0$, $k_u = 1.0$, and $I$ the $N\\times N$ identity.\n- The reaction–diffusion implicit operator is defined as $M(p_2) = I - \\Delta t\\, D\\, L + \\Delta t\\, p_2 I$, with diffusion coefficient $D = 0.1$.\n- The coupling constants are $\\gamma = 1.0$ in elasticity and parameter $p_3$ in the reaction term.\n- The parameter vector is $p = (p_1, p_2, p_3)$, where $p_1$ scales the chemomechanical body force, $p_2$ is a linear decay rate for $c$, and $p_3$ is a linear source proportional to $u$ in the reaction term.\n- Initial condition for concentration is $c^0_i = \\sin(\\pi x_i)$, encoded as the vector $c^0$ with entries $c^0_i = \\sin(\\pi x_i)$ for $i=1,\\dots,N$.\n- Homogeneous Dirichlet boundary conditions for both $u$ and $c$ are imposed, i.e., $u(0)=u(1)=0$ and $c(0,t)=c(1,t)=0$, translated into the discrete system by the choice of $L$.\n\nPartitioned forward time-stepping scheme:\nFor each time step $n=0,1,\\dots,N_t-1$, given $c^n$, compute\n1. Elasticity (quasi-static, linear):\n$$\nu^{n+1} = K^{-1} (\\gamma\\, p_1\\, c^n).\n$$\n2. Reaction–diffusion (backward Euler for diffusion and decay, linear in $c^{n+1}$):\n$$\nM(p_2)\\, c^{n+1} = c^n + \\Delta t\\, p_3\\, u^{n+1}.\n$$\nCollect the sequences $\\{u^{n+1}\\}_{n=0}^{N_t-1}$ and $\\{c^n\\}_{n=0}^{N_t}$.\n\nObservations and cost functional:\n- Define the observation vectors $u_{\\mathrm{obs}}$ and $c_{\\mathrm{obs}}$ at final time $T$ by simulating the above forward model with parameter $p^\\star = (0.5, 0.7, 0.3)$ and with no observational noise:\n$$\nu_{\\mathrm{obs}} = u^T(p^\\star), \\quad c_{\\mathrm{obs}} = c^T(p^\\star).\n$$\n- Define the calibration cost functional for any $p$ as\n$$\nJ(p) = \\frac{1}{2} \\| u^T(p) - u_{\\mathrm{obs}} \\|_2^2 + \\frac{1}{2} \\| c^T(p) - c_{\\mathrm{obs}} \\|_2^2 + \\frac{\\lambda}{2} \\| p \\|_2^2,\n$$\nwith Tikhonov regularization parameter $\\lambda = 10^{-3}$ and the Euclidean norm $\\|\\cdot\\|_2$ on $\\mathbb{R}^N$ and $\\mathbb{R}^3$. All quantities are dimensionless; you should report numerical values as dimensionless floats.\n\nTask:\n1. Derive a partitioned discrete adjoint for the above time-stepping scheme to compute the gradient $\\nabla_p J(p)$ with respect to $p = (p_1,p_2,p_3)$, starting from the discrete forward maps and using the chain rule and the properties of linear systems. The derivation must begin from the definitions given above and must not assume any pre-derived adjoint formulas. Your derivation must clearly identify the backward recursions for adjoint variables associated with $c^{n}$ and $u^{n+1}$, and the final expressions for the components of $\\nabla_p J(p)$.\n2. Implement a complete, runnable program that:\n   - Constructs $L$, $K$, and $M(p_2)$ for the specified grid and parameters.\n   - Generates $u_{\\mathrm{obs}}$ and $c_{\\mathrm{obs}}$ using $p^\\star$.\n   - For each test parameter vector $p$, runs the forward model and then computes $\\nabla_p J(p)$ using your partitioned discrete adjoint.\n3. Test suite:\n   Use the following three parameter vectors as test cases:\n   - Case A (general): $p = (0.4, 0.6, 0.2)$.\n   - Case B (weak mechanochemical coupling): $p = (10^{-6}, 0.6, 0.2)$.\n   - Case C (strong decay): $p = (0.4, 2.0, 0.2)$.\n4. Final output format:\n   Your program should produce a single line of output containing the results as a comma-separated list of floats enclosed in square brackets, corresponding to the concatenation of the gradient vectors for the three cases, in the order A, then B, then C. For example, the program should print\n$$\n[\\partial_{p_1} J(p^{A}), \\partial_{p_2} J(p^{A}), \\partial_{p_3} J(p^{A}), \\partial_{p_1} J(p^{B}), \\partial_{p_2} J(p^{B}), \\partial_{p_3} J(p^{B}), \\partial_{p_1} J(p^{C}), \\partial_{p_2} J(p^{C}), \\partial_{p_3} J(p^{C})].\n$$\nProvide the numerical values as plain decimal floats (not scientific notation), with no units, commas delimiting entries, and the entire list enclosed in a single pair of square brackets, and nothing else on the line.",
            "solution": "### 1. System Equations and Cost Functional\n\nThe discrete forward model evolves the state variables, displacement $u^n \\in \\mathbb{R}^N$ and concentration $c^n \\in \\mathbb{R}^N$, over time steps $n=1, \\dots, N_t$. Given the concentration at the previous step, $c^{n-1}$, the states at step $n$ are computed via a partitioned scheme:\n\n1.  **Elasticity Equation:**\n    $$K u^n = \\gamma p_1 c^{n-1} \\quad (*)$$\n2.  **Reaction-Diffusion Equation:**\n    $$M(p_2) c^n = c^{n-1} + \\Delta t p_3 u^n \\quad (**)$$\n\nHere, $K = -E L + k_u I$ and $M(p_2) = I - \\Delta t\\, D\\, L + \\Delta t\\, p_2 I$ are symmetric, positive-definite matrices for the given parameters. The initial condition is $c^0_i = \\sin(\\pi x_i)$. The parameters to be calibrated are $p=(p_1, p_2, p_3)$.\n\nThe cost functional to be minimized is:\n$$J(p) = \\frac{1}{2} \\| u^{N_t}(p) - u_{\\mathrm{obs}} \\|_2^2 + \\frac{1}{2} \\| c^{N_t}(p) - c_{\\mathrm{obs}} \\|_2^2 + \\frac{\\lambda}{2} \\| p \\|_2^2$$\nwhere $u^{N_t}$ and $c^{N_t}$ are the states at the final time $T=N_t \\Delta t$, and $u_{\\mathrm{obs}}$, $c_{\\mathrm{obs}}$ are observations generated using a reference parameter $p^\\star$.\n\nOur objective is to compute the gradient $\\nabla_p J(p)$ efficiently using the discrete adjoint method.\n\n### 2. The Lagrangian Method\n\nWe introduce a Lagrangian function $\\mathcal{L}$ by augmenting the cost functional $J$ with the governing equations, which act as constraints. We introduce vector-valued Lagrange multipliers (adjoint variables) $\\psi^n \\in \\mathbb{R}^N$ for the elasticity equation and $\\phi^n \\in \\mathbb{R}^N$ for the reaction-diffusion equation for each step $n=1, \\dots, N_t$.\n\nThe Lagrangian is:\n$$\n\\mathcal{L}(p, \\{u^n\\}, \\{c^n\\}) = J + \\sum_{n=1}^{N_t} (\\psi^n)^T (K u^n - \\gamma p_1 c^{n-1}) + \\sum_{n=1}^{N_t} (\\phi^n)^T (M(p_2) c^n - c^{n-1} - \\Delta t p_3 u^n)\n$$\nThe state variables $u^n$ and $c^n$ are implicitly functions of $p$. The total derivative of the cost functional with respect to any parameter $p_k$ is given by $dJ/dp_k = \\partial\\mathcal{L}/\\partial p_k$, provided that the partial derivatives of $\\mathcal{L}$ with respect to all state variables are zero. This condition defines the adjoint equations.\n\n### 3. Derivation of the Adjoint Equations\n\nWe find the adjoint equations by setting the partial derivatives of $\\mathcal{L}$ with respect to each state variable $u^n$ (for $n=1, \\dots, N_t$) and $c^n$ (for $n=1, \\dots, N_t$) to zero.\n\n**Derivative with respect to $u^n$:**\nThe state $u^n$ appears in the $n$-th elasticity constraint term, the $n$-th reaction-diffusion constraint term, and, if $n=N_t$, in the cost functional $J$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u^n} = (\\psi^n)^T K - (\\phi^n)^T (\\Delta t p_3 I) + \\delta_{n,N_t} \\frac{\\partial J}{\\partial u^{N_t}} = 0\n$$\nTaking the transpose and noting that $K$ is symmetric ($K^T=K$):\n$$K \\psi^n - \\Delta t p_3 \\phi^n + \\delta_{n,N_t} \\nabla_{u^{N_t}} J = 0$$\nwhere $\\nabla_{u^{N_t}} J = u^{N_t} - u_{\\mathrm{obs}}$.\nThis gives two cases:\n- For $n = N_t$, we have the terminal condition for $\\psi^{N_t}$:\n  $$K \\psi^{N_t} = \\Delta t p_3 \\phi^{N_t} - (u^{N_t} - u_{\\mathrm{obs}})$$\n- For $n = 1, \\dots, N_t-1$:\n  $$K \\psi^n = \\Delta t p_3 \\phi^n$$\n\n**Derivative with respect to $c^n$:**\nThe state $c^n$ appears in the $(n+1)$-th elasticity constraint, the $(n+1)$-th reaction-diffusion constraint, the $n$-th reaction-diffusion constraint, and, if $n=N_t$, in the cost functional $J$.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c^n} = (\\phi^n)^T M(p_2) - (\\psi^{n+1})^T (\\gamma p_1 I) - (\\phi^{n+1})^T I + \\delta_{n,N_t} \\frac{\\partial J}{\\partial c^{N_t}} = 0\n$$\nwhere we define $\\psi^{N_t+1} = 0$ and $\\phi^{N_t+1} = 0$. Taking the transpose and noting that $M(p_2)$ is symmetric ($M(p_2)^T = M(p_2)$):\n$$M(p_2) \\phi^n - \\gamma p_1 \\psi^{n+1} - \\phi^{n+1} + \\delta_{n,N_t} \\nabla_{c^{N_t}} J = 0$$\nwhere $\\nabla_{c^{N_t}} J = c^{N_t} - c_{\\mathrm{obs}}$.\nThis also gives two cases:\n- For $n = N_t$, we have the terminal condition for $\\phi^{N_t}$:\n  $$M(p_2) \\phi^{N_t} = - \\nabla_{c^{N_t}} J = -(c^{N_t} - c_{\\mathrm{obs}})$$\n- For $n = 1, \\dots, N_t-1$:\n  $$M(p_2) \\phi^n = \\phi^{n+1} + \\gamma p_1 \\psi^{n+1}$$\n\n**Partitioned Backward Recursion for Adjoints:**\nThese equations define a system that must be solved backward in time, from $n=N_t$ down to $n=1$.\n\n1.  **Terminal Step ($n=N_t$):**\n    First, solve for $\\phi^{N_t}$:\n    $$M(p_2) \\phi^{N_t} = -(c^{N_t} - c_{\\mathrm{obs}})$$\n    Then, using this $\\phi^{N_t}$, solve for $\\psi^{N_t}$:\n    $$K \\psi^{N_t} = \\Delta t p_3 \\phi^{N_t} - (u^{N_t} - u_{\\mathrm{obs}})$$\n\n2.  **Backward Loop ($n = N_t-1, \\dots, 1$):**\n    For each step $n$, assuming $\\phi^{n+1}$ and $\\psi^{n+1}$ are known from the previous step (in backward time):\n    First, solve for $\\phi^n$:\n    $$M(p_2) \\phi^n = \\phi^{n+1} + \\gamma p_1 \\psi^{n+1}$$\n    Then, using this $\\phi^n$, solve for $\\psi^n$:\n    $$K \\psi^n = \\Delta t p_3 \\phi^n$$\n\n### 4. Derivation of the Parameter Gradient\n\nWith the adjoint variables computed, we can find the gradient of the cost functional by differentiating the Lagrangian $\\mathcal{L}$ with respect to each parameter $p_k$.\n\n**Gradient with respect to $p_1$:**\nThe parameter $p_1$ only appears in the elasticity constraint term.\n$$\n\\nabla_{p_1} J = \\frac{\\partial \\mathcal{L}}{\\partial p_1} = \\frac{\\partial}{\\partial p_1} \\left( \\frac{\\lambda}{2} p_1^2 \\right) + \\sum_{n=1}^{N_t} (\\psi^n)^T (-\\gamma c^{n-1}) = \\lambda p_1 - \\gamma \\sum_{n=1}^{N_t} (\\psi^n)^T c^{n-1}\n$$\n\n**Gradient with respect to $p_2$:**\nThe parameter $p_2$ only appears in the reaction-diffusion constraint term, through $M(p_2)$. Note that $\\partial M(p_2) / \\partial p_2 = \\Delta t I$.\n$$\n\\nabla_{p_2} J = \\frac{\\partial \\mathcal{L}}{\\partial p_2} = \\frac{\\partial}{\\partial p_2} \\left( \\frac{\\lambda}{2} p_2^2 \\right) + \\sum_{n=1}^{N_t} (\\phi^n)^T \\left( \\frac{\\partial M(p_2)}{\\partial p_2} c^n \\right) = \\lambda p_2 + \\sum_{n=1}^{N_t} (\\phi^n)^T (\\Delta t c^n) = \\lambda p_2 + \\Delta t \\sum_{n=1}^{N_t} (\\phi^n)^T c^n\n$$\n\n**Gradient with respect to $p_3$:**\nThe parameter $p_3$ only appears in the reaction-diffusion constraint term.\n$$\n\\nabla_{p_3} J = \\frac{\\partial \\mathcal{L}}{\\partial p_3} = \\frac{\\partial}{\\partial p_3} \\left( \\frac{\\lambda}{2} p_3^2 \\right) + \\sum_{n=1}^{N_t} (\\phi^n)^T (-\\Delta t u^n) = \\lambda p_3 - \\Delta t \\sum_{n=1}^{N_t} (\\phi^n)^T u^n\n$$\n\nThese three expressions provide the components of the gradient $\\nabla_p J(p)$. The overall algorithm consists of a forward pass to compute and store the states $\\{u^n, c^n\\}$, followed by a backward pass to compute the adjoints $\\{\\psi^n, \\phi^n\\}$, and finally a gradient assembly step using the stored states and adjoints.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef construct_laplacian(N, dx):\n    \"\"\"Constructs the 1D discrete Laplacian matrix L with homogeneous Dirichlet BCs.\"\"\"\n    L = np.zeros((N, N))\n    diag_val = -2.0 / dx**2\n    off_diag_val = 1.0 / dx**2\n    np.fill_diagonal(L, diag_val)\n    if N > 1:\n        np.fill_diagonal(L[1:], off_diag_val)\n        np.fill_diagonal(L[:, 1:], off_diag_val)\n    return L\n\ndef forward_model(p, N, Nt, dt, dx, L, K_lu, c0, gamma):\n    \"\"\"Runs the forward simulation for a given parameter vector p.\"\"\"\n    p1, p2, p3 = p\n    \n    # Constants for reaction-diffusion operator\n    D = 0.1\n    # Elasticity operator K is constant, provided as LU-factored K_lu\n    \n    # Reaction-diffusion operator M depends on p2\n    I = np.identity(N)\n    M = I - dt * D * L + dt * p2 * I\n    M_lu = lu_factor(M)\n\n    # Store state histories\n    c_hist = {0: c0}\n    u_hist = {}\n\n    c_n_minus_1 = c0\n    for n in range(1, Nt + 1):\n        # 1. Elasticity\n        rhs_u = gamma * p1 * c_n_minus_1\n        u_n = lu_solve(K_lu, rhs_u)\n        \n        # 2. Reaction-diffusion\n        rhs_c = c_n_minus_1 + dt * p3 * u_n\n        c_n = lu_solve(M_lu, rhs_c)\n        \n        # Store states and update for next step\n        u_hist[n] = u_n\n        c_hist[n] = c_n\n        c_n_minus_1 = c_n\n        \n    return u_hist, c_hist, M_lu\n\ndef adjoint_model(p, N, Nt, dt, K_lu, M_lu, u_hist, c_hist, u_obs, c_obs, gamma):\n    \"\"\"Computes the adjoint variables using the backward recursion.\"\"\"\n    p1, p2, p3 = p\n    \n    phi_hist = {}\n    psi_hist = {}\n    \n    # Terminal conditions for adjoints at n = Nt\n    c_T = c_hist[Nt]\n    u_T = u_hist[Nt]\n    \n    # Adjoint phi_T\n    rhs_phi_T = -(c_T - c_obs)\n    # M is symmetric, so M^T = M. Use M_lu for solve with trans=1.\n    phi_T = lu_solve(M_lu, rhs_phi_T, trans=1) \n    phi_hist[Nt] = phi_T\n    \n    # Adjoint psi_T\n    rhs_psi_T = dt * p3 * phi_T - (u_T - u_obs)\n    # K is symmetric, so K^T = K. Use K_lu for solve with trans=1.\n    psi_T = lu_solve(K_lu, rhs_psi_T, trans=1)\n    psi_hist[Nt] = psi_T\n\n    # Backward loop from n = Nt-1 down to 1\n    for n in range(Nt - 1, 0, -1):\n        # Adjoint phi_n\n        rhs_phi = phi_hist[n + 1] + gamma * p1 * psi_hist[n + 1]\n        phi_n = lu_solve(M_lu, rhs_phi, trans=1)\n        phi_hist[n] = phi_n\n        \n        # Adjoint psi_n\n        rhs_psi = dt * p3 * phi_n\n        psi_n = lu_solve(K_lu, rhs_psi, trans=1)\n        psi_hist[n] = psi_n\n        \n    return psi_hist, phi_hist\n\ndef compute_gradient(p, Nt, dt, lambda_reg, psi_hist, phi_hist, u_hist, c_hist, gamma):\n    \"\"\"Assembles the gradient from state and adjoint variables.\"\"\"\n    p1, p2, p3 = p\n    \n    # Initialize with regularization term gradient\n    grad_p = lambda_reg * np.array(p)\n    \n    grad_p1_sum = 0.0\n    grad_p2_sum = 0.0\n    grad_p3_sum = 0.0\n    \n    for n in range(1, Nt + 1):\n        grad_p1_sum += np.dot(psi_hist[n], c_hist[n - 1])\n        grad_p2_sum += np.dot(phi_hist[n], c_hist[n])\n        grad_p3_sum += np.dot(phi_hist[n], u_hist[n])\n        \n    grad_p[0] += -gamma * grad_p1_sum\n    grad_p[1] += dt * grad_p2_sum\n    grad_p[2] += -dt * grad_p3_sum\n    \n    return grad_p\n\ndef solve():\n    # --- Problem Setup ---\n    N = 10\n    Nt = 5\n    dt = 0.1\n    dx = 1.0 / (N + 1)\n    \n    # Material and model parameters\n    E = 1.0\n    k_u = 1.0\n    gamma = 1.0\n    lambda_reg = 1e-3\n    \n    # Construct constant matrices and initial conditions\n    L = construct_laplacian(N, dx)\n    K = -E * L + k_u * np.identity(N)\n    K_lu = lu_factor(K)\n    \n    x = np.linspace(dx, 1.0 - dx, N)\n    c0 = np.sin(np.pi * x)\n\n    # --- Generate Observations ---\n    p_star = (0.5, 0.7, 0.3)\n    u_hist_star, c_hist_star, _ = forward_model(p_star, N, Nt, dt, dx, L, K_lu, c0, gamma)\n    u_obs = u_hist_star[Nt]\n    c_obs = c_hist_star[Nt]\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.4, 0.6, 0.2),      # Case A\n        (1e-6, 0.6, 0.2),     # Case B\n        (0.4, 2.0, 0.2),      # Case C\n    ]\n    \n    results = []\n    # In the code, the forward scheme is implemented from n=1..Nt,\n    # so the time-stepping loop runs Nt times.\n    # The problem statement has n=0..Nt-1. This is an equivalent number of steps.\n    # We follow the implementation which matches the derived adjoint equations (sum n=1..Nt)\n    for p_test in test_cases:\n        p_test_tuple = p_test\n        \n        # 1. Forward pass to get state histories\n        u_hist, c_hist, M_lu_test = forward_model(p_test_tuple, N, Nt, dt, dx, L, K_lu, c0, gamma)\n        \n        # 2. Backward pass to get adjoint variables\n        psi_hist, phi_hist = adjoint_model(p_test_tuple, N, Nt, dt, K_lu, M_lu_test, u_hist, c_hist, u_obs, c_obs, gamma)\n        \n        # 3. Assemble gradient\n        grad = compute_gradient(p_test_tuple, Nt, dt, lambda_reg, psi_hist, phi_hist, u_hist, c_hist, gamma)\n        \n        results.extend(grad)\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}