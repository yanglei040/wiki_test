{
    "hands_on_practices": [
        {
            "introduction": "Gradient-based calibration minimizes an objective function, which often includes a prior term to incorporate existing knowledge and regularize the problem. This practice focuses on deriving the gradient's contribution from a multivariate Gaussian prior, a foundational component in Bayesian model calibration. Understanding this derivation is crucial for appreciating how the prior's covariance structure, $\\Sigma$, anisotropically scales the gradient and shapes the optimization landscape. ",
            "id": "3287571",
            "problem": "Consider calibrating a parameter vector $\\theta \\in \\mathbb{R}^{d}$ for a biochemical reaction network whose state $x(t;\\theta) \\in \\mathbb{R}^{n}$ evolves according to an Ordinary Differential Equation (ODE)\n$$\n\\frac{d x}{d t} = f\\big(x(t;\\theta),\\theta\\big), \\quad x(0;\\theta) = x_{0},\n$$\nwith observations $y_{i} \\in \\mathbb{R}^{m}$ at times $t_{i}$ related to the state by a measurement operator $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$. A standard negative log-posterior objective for gradient-based calibration is\n$$\nJ(\\theta) = \\frac{1}{2}\\sum_{i=1}^{N} \\left\\| W^{1/2}\\big(h(x(t_{i};\\theta)) - y_{i}\\big) \\right\\|_{2}^{2} \\;+\\; \\phi_{\\text{prior}}(\\theta),\n$$\nwhere $W \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite and encodes observational weights, and $\\phi_{\\text{prior}}(\\theta)$ is the negative log-prior penalty. Assume a multivariate normal prior on $\\theta$ with mean $\\mu \\in \\mathbb{R}^{d}$ and covariance $\\Sigma \\in \\mathbb{R}^{d \\times d}$, where $\\Sigma$ is symmetric positive definite, so the prior density is\n$$\np(\\theta) = (2\\pi)^{-d/2}|\\Sigma|^{-1/2}\\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right).\n$$\nStarting from this fundamental definition of the multivariate normal density, construct $\\phi_{\\text{prior}}(\\theta)$ and derive its gradient $\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta)$ explicitly in terms of $\\theta$, $\\mu$, and $\\Sigma$. Then, explain how the anisotropy encoded by $\\Sigma$ modifies step sizes in gradient-based calibration when combined with the adjoint-computed gradient of the data misfit, with reference to the eigenvalues and eigenvectors of $\\Sigma$ and the implications for choosing a global step size $\\alpha$ in an iteration of the form $\\theta_{k+1} = \\theta_{k} - \\alpha \\nabla_{\\theta}J(\\theta_{k})$. Your final answer must be the exact closed-form analytic expression for $\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta)$, with no numerical approximation.",
            "solution": "The problem asks for three components: the construction of the negative log-prior penalty $\\phi_{\\text{prior}}(\\theta)$, the derivation of its gradient $\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta)$, and an explanation of the implications of this prior for gradient-based calibration.\n\nFirst, we construct the negative log-prior penalty, $\\phi_{\\text{prior}}(\\theta)$, from the given prior probability density function (PDF). The prior distribution on the parameter vector $\\theta \\in \\mathbb{R}^{d}$ is a multivariate normal with mean $\\mu \\in \\mathbb{R}^{d}$ and covariance $\\Sigma \\in \\mathbb{R}^{d \\times d}$. The PDF is given as:\n$$\np(\\theta) = (2\\pi)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}}\\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right)\n$$\nThe negative log-prior, $\\phi_{\\text{prior}}(\\theta)$, is defined as the negative natural logarithm of the prior PDF, often omitting terms that are constant with respect to $\\theta$ as they do not affect the gradient. Taking the natural logarithm of $p(\\theta)$ gives:\n$$\n\\ln(p(\\theta)) = \\ln\\left((2\\pi)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}}\\right) + \\ln\\left(\\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right)\\right)\n$$\n$$\n\\ln(p(\\theta)) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(|\\Sigma|) - \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\n$$\nTherefore, the negative log-prior is:\n$$\n\\phi_{\\text{prior}}(\\theta) = -\\ln(p(\\theta)) = \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) + \\frac{d}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln(|\\Sigma|)\n$$\nFor optimization purposes, the constant terms are irrelevant, so the penalty is effectively the quadratic term.\n\nNext, we derive the gradient of $\\phi_{\\text{prior}}(\\theta)$ with respect to $\\theta$. We need to compute the gradient of the expression derived above. The constant terms have a zero gradient.\n$$\n\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta) = \\nabla_{\\theta}\\left( \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) \\right)\n$$\nWe use the standard matrix calculus identity for the gradient of a quadratic form. For a vector variable $z$ and a symmetric matrix $A$, the gradient is given by $\\nabla_{z}\\left( (z-c)^{\\top}A(z-c) \\right) = 2A(z-c)$. In our case, the variable is $\\theta$, the constant vector is $\\mu$, and the matrix is $\\Sigma^{-1}$. The covariance matrix $\\Sigma$ is given as symmetric positive definite, which implies that its inverse, $\\Sigma^{-1}$, is also symmetric. Applying this identity, we get:\n$$\n\\nabla_{\\theta}\\left( (\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) \\right) = 2\\Sigma^{-1}(\\theta - \\mu)\n$$\nSubstituting this result back into the expression for the gradient of $\\phi_{\\text{prior}}(\\theta)$:\n$$\n\\nabla_{\\theta}\\phi_{\\text{prior}}(\\theta) = \\frac{1}{2} \\left( 2\\Sigma^{-1}(\\theta - \\mu) \\right) = \\Sigma^{-1}(\\theta - \\mu)\n$$\nThis is the analytical expression for the gradient of the prior penalty term.\n\nFinally, we explain the implications of this prior gradient for gradient-based calibration. The total gradient of the objective function $J(\\theta)$ is the sum of the gradient of the data-misfit term and the gradient of the prior penalty:\n$$\n\\nabla_{\\theta}J(\\theta) = \\nabla_{\\theta}\\left( \\frac{1}{2}\\sum_{i=1}^{N} \\left\\| W^{\\frac{1}{2}}(h(x(t_{i};\\theta)) - y_{i}) \\right\\|_{2}^{2} \\right) + \\Sigma^{-1}(\\theta - \\mu)\n$$\nThe data-misfit gradient, typically computed via the adjoint method, drives the parameters towards values that fit the observations $y_i$. The prior gradient, $\\Sigma^{-1}(\\theta - \\mu)$, acts as a regularization term that pulls the parameter vector $\\theta$ towards the prior mean $\\mu$.\n\nThe key to understanding its effect is the matrix $\\Sigma^{-1}$, known as the precision matrix. The covariance matrix $\\Sigma$ is symmetric positive definite and can be diagonalized as $\\Sigma = V \\Lambda V^{\\top}$, where $V$ is an orthogonal matrix whose columns are the eigenvectors $v_j$ of $\\Sigma$, and $\\Lambda$ is a diagonal matrix of the corresponding positive eigenvalues $\\lambda_j$. The eigenvectors $v_j$ define the principal axes of the prior probability distribution, and the eigenvalues $\\lambda_j$ represent the variance (a measure of uncertainty) of the prior along those axes.\n\nThe precision matrix is then $\\Sigma^{-1} = (V \\Lambda V^{\\top})^{-1} = V \\Lambda^{-1} V^{\\top}$. Its eigenvalues are $1/\\lambda_j$. The prior gradient contribution is $\\Sigma^{-1}(\\theta-\\mu) = V \\Lambda^{-1} V^{\\top}(\\theta-\\mu)$. This term reveals the anisotropic nature of the regularization:\n\\begin{enumerate}\n    \\item A small eigenvalue $\\lambda_j$ of $\\Sigma$ corresponds to a direction $v_j$ in which the prior knowledge is very certain (low variance). The corresponding eigenvalue of $\\Sigma^{-1}$ is large ($1/\\lambda_j$). Consequently, any deviation of $\\theta$ from the mean $\\mu$ along this direction $v_j$ results in a very large gradient component, strongly penalizing the deviation and pushing the parameter value back towards the prior mean along that axis.\n    \\item A large eigenvalue $\\lambda_k$ of $\\Sigma$ corresponds to a direction $v_k$ of high prior uncertainty (large variance). The corresponding eigenvalue of $\\Sigma^{-1}$ is small ($1/\\lambda_k$). Deviations along this direction are penalized weakly, allowing the data-misfit gradient to have a greater influence on the parameter update in this direction.\n\\end{enumerate}\nThis anisotropy has significant implications for a standard gradient descent update step of the form $\\theta_{k+1} = \\theta_{k} - \\alpha \\nabla_{\\theta}J(\\theta_{k})$, where $\\alpha$ is a single, global step size. The total gradient vector $\\nabla_{\\theta}J(\\theta_{k})$ will have components of vastly different magnitudes, especially if the eigenvalues of $\\Sigma$ span several orders of magnitude. The maximum permissible step size $\\alpha$ is limited by the need to maintain stability in the direction of the largest gradient component, which is typically the direction associated with the smallest prior variance (smallest $\\lambda_j$, largest $1/\\lambda_j$). Using such a small $\\alpha$ for all parameter directions causes extremely slow convergence in directions where the gradient is small (e.g., those corresponding to large prior variances). This problem, known as ill-conditioning, demonstrates that an un-preconditioned gradient descent with a scalar step size is inefficient for optimizing objective functions with anisotropic priors. The prior's structure inherently suggests that different step sizes should be taken along different parameter directions, motivating the use of preconditioning or second-order optimization methods.",
            "answer": "$$\\boxed{\\Sigma^{-1}(\\theta - \\mu)}$$"
        },
        {
            "introduction": "The theoretical correctness of adjoint equations is only useful if their implementation is also correct. This exercise presents a fundamental verification test for any discrete adjoint solver, often called a discrete adjoint identity test. You will implement a numerical check based on a mathematical tautology that must hold for a correctly implemented pair of tangent linear and adjoint operators, providing a powerful method for debugging and validation. ",
            "id": "3287583",
            "problem": "Consider a three-species biochemical reaction network modeled by an ordinary differential equation (ODE) in continuous time. Let the state be $x(t) \\in \\mathbb{R}^3$ with components $x_1(t)$, $x_2(t)$, $x_3(t)$ denoting concentrations, and let the parameter vector be $\\theta \\in \\mathbb{R}^7$ with components $\\theta_1,\\theta_2,\\theta_3,\\theta_4,\\theta_5,\\theta_6,\\theta_7$, all nonnegative reaction rate constants. The continuous-time right-hand side is defined by mass-action kinetics:\n$$\nf(x,\\theta) = \\begin{bmatrix}\n\\theta_1 - \\theta_2 x_1 - \\theta_3 x_1 x_2 \\\\\n\\theta_4 x_1 - \\theta_5 x_2 \\\\\n\\theta_6 x_2 - \\theta_7 x_3\n\\end{bmatrix},\n$$\nand its Jacobian with respect to $x$ is\n$$\nJ(x,\\theta) = \\frac{\\partial f(x,\\theta)}{\\partial x} = \\begin{bmatrix}\n-\\theta_2 - \\theta_3 x_2  -\\theta_3 x_1  0 \\\\\n\\theta_4  -\\theta_5  0 \\\\\n0  \\theta_6  -\\theta_7\n\\end{bmatrix}.\n$$\nDiscretize the dynamics over a single step using the backward Euler method with time step $\\Delta t  0$ (in seconds):\n$$\nx_{k+1} - x_k - \\Delta t \\, f(x_{k+1},\\theta) = 0,\n$$\nand consider arbitrary perturbations $\\delta x_{k+1} \\in \\mathbb{R}^3$ at the state $x_{k+1}$, as well as arbitrary adjoint vectors $\\lambda_k,\\lambda_{k+1} \\in \\mathbb{R}^3$. The linearized tangent mapping for backward Euler is given by\n$$\n\\delta x_k = \\left(I - \\Delta t \\, J(x_{k+1},\\theta)\\right) \\, \\delta x_{k+1}.\n$$\nDefine the discrete adjoint residual at step $k+1$ by\n$$\nr_{k+1} := \\frac{\\left(I - \\Delta t \\, J(x_{k+1},\\theta)^\\top\\right)\\lambda_k - \\lambda_{k+1}}{\\Delta t}.\n$$\nThe discrete adjoint identity to be tested states that, for arbitrary perturbations $\\delta x_{k+1}$ and adjoint vectors $\\lambda_k,\\lambda_{k+1}$, the following equality holds:\n$$\n\\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle,\n$$\nwhere $\\langle \\cdot,\\cdot \\rangle$ denotes the standard Euclidean inner product.\n\nYour task is to implement a discrete adjoint identity test for this backward Euler discretization, using the above $f(x,\\theta)$ and $J(x,\\theta)$, and verify the identity numerically for multiple cases with arbitrary perturbations and adjoint vectors. For each test case, you must:\n- Compute $J(x_{k+1},\\theta)$.\n- Compute $\\delta x_k = \\left(I - \\Delta t \\, J(x_{k+1},\\theta)\\right)\\delta x_{k+1}$.\n- Compute $r_{k+1} = \\frac{\\left(I - \\Delta t \\, J(x_{k+1},\\theta)^\\top\\right)\\lambda_k - \\lambda_{k+1}}{\\Delta t}$.\n- Compute the absolute error\n$$\n\\varepsilon = \\left| \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle - \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle \\right|.\n$$\n\nScientific realism requirements:\n- All reaction rate constants $\\theta_i$ must be nonnegative.\n- All concentrations $x_i$ must be nonnegative.\n- Time step $\\Delta t$ must be positive and expressed in seconds.\n\nTest suite:\nImplement the test for the following five cases. In each case, generate the adjoint vectors $\\lambda_k,\\lambda_{k+1}$ and the perturbation $\\delta x_{k+1}$ using a fixed seed for a normal distribution with zero mean and unit variance (to ensure deterministic, arbitrary perturbations). Use the three-dimensional standard normal distribution independently for each vector.\n\n- Case $1$ (happy path): $\\Delta t = 0.1$ seconds, $\\theta = [0.8, 0.5, 0.4, 0.3, 0.2, 1.0, 0.7]$, $x_{k+1} = [1.0, 0.8, 0.5]$, seed $101$.\n- Case $2$ (boundary: very small time step): $\\Delta t = 10^{-6}$ seconds, $\\theta = [0.2, 0.1, 0.05, 0.3, 0.25, 0.4, 0.35]$, $x_{k+1} = [0.1, 0.05, 0.02]$, seed $202$.\n- Case $3$ (stiff-ish dynamics: large time step and rates): $\\Delta t = 1.0$ seconds, $\\theta = [2.0, 1.5, 1.2, 0.8, 0.9, 1.1, 1.0]$, $x_{k+1} = [3.0, 2.0, 1.0]$, seed $303$.\n- Case $4$ (near-degenerate $I - \\Delta t J$: large interactions): $\\Delta t = 0.9$ seconds, $\\theta = [1.0, 1.0, 5.0, 2.5, 0.1, 0.6, 0.5]$, $x_{k+1} = [10.0, 10.0, 10.0]$, seed $404$.\n- Case $5$ (near-zero Jacobian: very small concentrations): $\\Delta t = 0.05$ seconds, $\\theta = [0.1, 0.1, 0.05, 0.2, 0.1, 0.15, 0.12]$, $x_{k+1} = [10^{-9}, 2 \\cdot 10^{-9}, 3 \\cdot 10^{-9}]$, seed $505$.\n\nFinal output specification:\nYour program should produce a single line of output containing the absolute errors for the five cases as a comma-separated list enclosed in square brackets (e.g., $[e_1,e_2,e_3,e_4,e_5]$). Each $e_i$ must be a real number (floating-point) representing the computed $\\varepsilon$ for the corresponding case. The errors are dimensionless real numbers. No other text should be printed.",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n\n- **State vector:** $x(t) \\in \\mathbb{R}^3$, with components $x_1(t)$, $x_2(t)$, $x_3(t)$ representing concentrations.\n- **Parameter vector:** $\\theta \\in \\mathbb{R}^7$, with components $\\theta_1, \\dots, \\theta_7$ representing nonnegative reaction rate constants.\n- **Continuous-time dynamics function:** $f(x,\\theta) = \\begin{bmatrix} \\theta_1 - \\theta_2 x_1 - \\theta_3 x_1 x_2 \\\\ \\theta_4 x_1 - \\theta_5 x_2 \\\\ \\theta_6 x_2 - \\theta_7 x_3 \\end{bmatrix}$.\n- **Jacobian of the dynamics:** $J(x,\\theta) = \\frac{\\partial f(x,\\theta)}{\\partial x} = \\begin{bmatrix} -\\theta_2 - \\theta_3 x_2  -\\theta_3 x_1  0 \\\\ \\theta_4  -\\theta_5  0 \\\\ 0  \\theta_6  -\\theta_7 \\end{bmatrix}$.\n- **Discretization method:** Backward Euler with time step $\\Delta t  0$, defined by the implicit equation $x_{k+1} - x_k - \\Delta t \\, f(x_{k+1},\\theta) = 0$.\n- **Perturbations and Adjoint Vectors:** Arbitrary perturbation $\\delta x_{k+1} \\in \\mathbb{R}^3$ and arbitrary adjoint vectors $\\lambda_k, \\lambda_{k+1} \\in \\mathbb{R}^3$.\n- **Linearized tangent mapping:** $\\delta x_k = \\left(I - \\Delta t \\, J(x_{k+1},\\theta)\\right) \\, \\delta x_{k+1}$, where $I$ is the $3 \\times 3$ identity matrix.\n- **Discrete adjoint residual:** $r_{k+1} := \\frac{\\left(I - \\Delta t \\, J(x_{k+1},\\theta)^\\top\\right)\\lambda_k - \\lambda_{k+1}}{\\Delta t}$.\n- **Discrete adjoint identity:** $\\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle$.\n- **Inner product:** $\\langle \\cdot,\\cdot \\rangle$ is the standard Euclidean inner product.\n- **Absolute error metric:** $\\varepsilon = \\left| \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle - \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle \\right|$.\n- **Scientific realism constraints:** $\\theta_i \\ge 0$, $x_i \\ge 0$, $\\Delta t  0$.\n- **Test Cases:** Five specific numerical cases are defined with values for $\\Delta t$, $\\theta$, $x_{k+1}$, and a seed for random number generation. The random vectors $\\lambda_k, \\lambda_{k+1}, \\delta x_{k+1}$ are to be drawn from a $3$-dimensional standard normal distribution.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is based on standard principles of chemical kinetics (mass-action), numerical analysis of ordinary differential equations (backward Euler method), and sensitivity analysis (adjoint methods). These are foundational concepts in computational systems biology. The model and equations are consistent with established theory.\n- **Well-Posed:** The problem provides a clear, unambiguous task: to numerically verify a given mathematical identity for a set of fully specified test cases. All necessary equations, parameters, and initial values are provided. The existence of a unique numerical result ($\\varepsilon$) for each case is guaranteed.\n- **Objective:** The problem is stated entirely in objective, mathematical language. The test cases are deterministic due to the specified seeds for random number generation.\n- **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** The core of the problem is the discrete adjoint identity. This identity is a mathematical tautology derived from the definitions of the tangent map and the adjoint residual. Let $A = I - \\Delta t \\, J(x_{k+1},\\theta)$. The tangent map is $\\delta x_k = A \\, \\delta x_{k+1}$. The adjoint residual is $r_{k+1} = \\frac{1}{\\Delta t}(A^\\top \\lambda_k - \\lambda_{k+1})$. The identity to be tested is $\\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle$. By substituting the definitions and using the property of the adjoint operator (transpose for the Euclidean inner product, $\\langle u, Av \\rangle = \\langle A^\\top u, v \\rangle$), the left-hand side becomes $\\langle \\lambda_k, A \\delta x_{k+1} \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\langle A^\\top \\lambda_k, \\delta x_{k+1} \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\langle A^\\top \\lambda_k - \\lambda_{k+1}, \\delta x_{k+1} \\rangle$. The right-hand side is $\\Delta t \\, \\langle \\frac{1}{\\Delta t}(A^\\top \\lambda_k - \\lambda_{k+1}), \\delta x_{k+1} \\rangle = \\langle A^\\top \\lambda_k - \\lambda_{k+1}, \\delta x_{k+1} \\rangle$. The identity is mathematically correct. The problem is sound.\n    2.  **Non-Formalizable/Irrelevant:** The problem is explicitly formal and directly relevant to verifying implementations of adjoint solvers in computational science.\n    3.  **Incomplete/Contradictory:** The problem is self-contained and internally consistent.\n    4.  **Unrealistic/Infeasible:** The given parameters adhere to the scientific realism constraints ($\\theta_i \\ge 0$, $x_i \\ge 0$, $\\Delta t  0$).\n    5.  **Ill-Posed/Poorly Structured:** The problem is well-structured and leads to a unique, stable result for each test case.\n    6.  **Pseudo-Profound/Trivial:** While the identity is a mathematical tautology, the task of its numerical verification is a non-trivial and fundamental practice in software engineering for scientific computing to ensure that the discrete operators have been implemented correctly.\n    7.  **Outside Scientific Verifiability:** The claim is numerically verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe problem requires the numerical verification of a discrete adjoint identity for a system of ordinary differential equations (ODEs) discretized using the backward Euler method. The identity is a fundamental property relating the forward propagation of perturbations (the tangent model) to the backward propagation of sensitivities (the adjoint model). Verifying this identity is a critical step in a \"gradient check\" to ensure a correct implementation of an adjoint solver. The expected outcome is that the absolute error $\\varepsilon$ will be zero, up to the limits of floating-point arithmetic precision.\n\nThe derivation of the identity forms the theoretical basis. The backward Euler discretization defines a residual equation for a single time step, $G(x_k, x_{k+1}, \\theta) = x_{k+1} - x_k - \\Delta t \\, f(x_{k+1}, \\theta) = 0$. The linearized tangent model describes how a small perturbation $\\delta x_{k+1}$ at time $k+1$ relates to the corresponding perturbation $\\delta x_k$ at time $k$ required to keep the residual zero. This is found by taking the total differential of $G$:\n$$ dG = \\frac{\\partial G}{\\partial x_k} \\delta x_k + \\frac{\\partial G}{\\partial x_{k+1}} \\delta x_{k+1} = 0 $$\nWith $\\frac{\\partial G}{\\partial x_k} = -I$ and $\\frac{\\partial G}{\\partial x_{k+1}} = I - \\Delta t \\frac{\\partial f}{\\partial x_{k+1}} = I - \\Delta t J(x_{k+1}, \\theta)$, we have:\n$$ -I \\delta x_k + (I - \\Delta t J(x_{k+1}, \\theta)) \\delta x_{k+1} = 0 $$\nThis rearranges to the provided linearized tangent mapping:\n$$ \\delta x_k = (I - \\Delta t J(x_{k+1}, \\theta)) \\delta x_{k+1} $$\nLet us denote the linear operator $A = I - \\Delta t J(x_{k+1}, \\theta)$. Thus, $\\delta x_k = A \\delta x_{k+1}$.\n\nThe discrete adjoint identity, $\\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle$, connects the forward map $A$ to its adjoint, $A^\\top$. We can prove this identity by substituting the definitions of $\\delta x_k$ and $r_{k+1}$.\n\nLet LHS be the left-hand side and RHS be the right-hand side of the identity.\n$$ \\text{LHS} = \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nSubstitute $\\delta x_k = A \\delta x_{k+1}$:\n$$ \\text{LHS} = \\langle \\lambda_k, A \\delta x_{k+1} \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nUsing the property of the adjoint operator for the Euclidean inner product, $\\langle u, Av \\rangle = \\langle A^\\top u, v \\rangle$:\n$$ \\text{LHS} = \\langle A^\\top \\lambda_k, \\delta x_{k+1} \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nBy linearity of the inner product:\n$$ \\text{LHS} = \\langle A^\\top \\lambda_k - \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nNow consider the RHS:\n$$ \\text{RHS} = \\Delta t \\, \\langle r_{k+1}, \\delta x_{k+1} \\rangle $$\nSubstitute the definition of the adjoint residual, $r_{k+1} = \\frac{A^\\top \\lambda_k - \\lambda_{k+1}}{\\Delta t}$:\n$$ \\text{RHS} = \\Delta t \\, \\left\\langle \\frac{A^\\top \\lambda_k - \\lambda_{k+1}}{\\Delta t}, \\delta x_{k+1} \\right\\rangle $$\nAgain, by linearity of the inner product:\n$$ \\text{RHS} = \\langle \\Delta t \\left( \\frac{A^\\top \\lambda_k - \\lambda_{k+1}}{\\Delta t} \\right), \\delta x_{k+1} \\rangle = \\langle A^\\top \\lambda_k - \\lambda_{k+1}, \\delta x_{k+1} \\rangle $$\nSince LHS = RHS, the identity is mathematically exact.\n\nThe algorithmic implementation will follow these theoretical steps for each test case:\n1.  Set the random seed for reproducibility.\n2.  Generate the arbitrary vectors $\\lambda_k, \\lambda_{k+1}, \\delta x_{k+1} \\in \\mathbb{R}^3$ from a standard normal distribution.\n3.  Construct the Jacobian matrix $J(x_{k+1}, \\theta)$ using the provided state $x_{k+1}$ and parameters $\\theta$.\n4.  Compute the matrix $A = I - \\Delta t J(x_{k+1}, \\theta)$.\n5.  Calculate the propagated perturbation $\\delta x_k = A \\delta x_{k+1}$.\n6.  Calculate the adjoint residual $r_{k+1} = \\frac{1}{\\Delta t} (A^\\top \\lambda_k - \\lambda_{k+1})$.\n7.  Compute the LHS, $L = \\langle \\lambda_k, \\delta x_k \\rangle - \\langle \\lambda_{k+1}, \\delta x_{k+1} \\rangle$, using the vector dot product.\n8.  Compute the RHS, $R = \\Delta t \\langle r_{k+1}, \\delta x_{k+1} \\rangle$, using the vector dot product.\n9.  Finally, calculate the absolute error $\\varepsilon = |L - R|$. This value is expected to be on the order of machine epsilon, confirming the correctness of the numerical implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a discrete adjoint identity test for a backward Euler discretization\n    of a 3-species biochemical reaction network.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dt\": 0.1,\n            \"theta\": np.array([0.8, 0.5, 0.4, 0.3, 0.2, 1.0, 0.7]),\n            \"x_kp1\": np.array([1.0, 0.8, 0.5]),\n            \"seed\": 101,\n        },\n        {\n            \"dt\": 1e-6,\n            \"theta\": np.array([0.2, 0.1, 0.05, 0.3, 0.25, 0.4, 0.35]),\n            \"x_kp1\": np.array([0.1, 0.05, 0.02]),\n            \"seed\": 202,\n        },\n        {\n            \"dt\": 1.0,\n            \"theta\": np.array([2.0, 1.5, 1.2, 0.8, 0.9, 1.1, 1.0]),\n            \"x_kp1\": np.array([3.0, 2.0, 1.0]),\n            \"seed\": 303,\n        },\n        {\n            \"dt\": 0.9,\n            \"theta\": np.array([1.0, 1.0, 5.0, 2.5, 0.1, 0.6, 0.5]),\n            \"x_kp1\": np.array([10.0, 10.0, 10.0]),\n            \"seed\": 404,\n        },\n        {\n            \"dt\": 0.05,\n            \"theta\": np.array([0.1, 0.1, 0.05, 0.2, 0.1, 0.15, 0.12]),\n            \"x_kp1\": np.array([1e-9, 2e-9, 3e-9]),\n            \"seed\": 505,\n        },\n    ]\n\n    def compute_jacobian(x, theta):\n        \"\"\"\n        Computes the Jacobian matrix J(x, theta).\n        x: state vector [x1, x2, x3]\n        theta: parameter vector [theta1, ..., theta7]\n        \"\"\"\n        x1, x2, x3 = x\n        th1, th2, th3, th4, th5, th6, th7 = theta\n        \n        J = np.zeros((3, 3))\n        \n        J[0, 0] = -th2 - th3 * x2\n        J[0, 1] = -th3 * x1\n        J[0, 2] = 0.0\n        \n        J[1, 0] = th4\n        J[1, 1] = -th5\n        J[1, 2] = 0.0\n        \n        J[2, 0] = 0.0\n        J[2, 1] = th6\n        J[2, 2] = -th7\n        \n        return J\n\n    results = []\n    for case in test_cases:\n        dt = case[\"dt\"]\n        theta = case[\"theta\"]\n        x_kp1 = case[\"x_kp1\"]\n        seed = case[\"seed\"]\n\n        # Set the seed for deterministic random vector generation\n        np.random.seed(seed)\n\n        # Generate arbitrary vectors from N(0, I_3)\n        lambda_k = np.random.randn(3)\n        lambda_kp1 = np.random.randn(3)\n        delta_x_kp1 = np.random.randn(3)\n\n        # Compute the Jacobian J(x_{k+1}, theta)\n        J_kp1 = compute_jacobian(x_kp1, theta)\n\n        # The linear operator from the linearized backward Euler step\n        # A = I - dt * J\n        I = np.identity(3)\n        A = I - dt * J_kp1\n\n        # Compute delta_x_k using the linearized tangent mapping\n        # delta_x_k = (I - dt*J) * delta_x_{k+1}\n        delta_x_k = A @ delta_x_kp1\n\n        # Compute the discrete adjoint residual r_{k+1}\n        # r_{k+1} = ((I - dt*J^T)*lambda_k - lambda_{k+1}) / dt\n        A_T = A.T # (I - dt*J)^T = I - dt*J^T\n        r_kp1 = (A_T @ lambda_k - lambda_kp1) / dt\n        \n        # Compute the Left-Hand Side (LHS) of the adjoint identity\n        # LHS = lambda_k, delta_x_k - lambda_{k+1}, delta_x_{k+1}\n        lhs = np.dot(lambda_k, delta_x_k) - np.dot(lambda_kp1, delta_x_kp1)\n\n        # Compute the Right-Hand Side (RHS) of the adjoint identity\n        # RHS = dt * r_{k+1}, delta_x_{k+1}\n        rhs = dt * np.dot(r_kp1, delta_x_kp1)\n\n        # Compute the absolute error epsilon\n        # epsilon = |LHS - RHS|\n        error = np.abs(lhs - rhs)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Applying adjoint methods to systems with long time horizons presents a significant practical challenge: the memory required to store the forward state trajectory for the backward pass can be prohibitive. This exercise tackles this bottleneck by exploring an optimal checkpointing algorithm, which judiciously trades a limited memory budget for recomputation. By deriving the relationship between memory, recomputation, and the number of solvable time steps, you will gain insight into a powerful technique for making large-scale adjoint computations feasible. ",
            "id": "3287580",
            "problem": "You are calibrating a nonlinear biochemical network model governed by an Ordinary Differential Equation (ODE) system on a time interval $[0, T]$ with a fixed-step integrator of $N$ uniform steps. To perform gradient-based calibration via the adjoint method (reverse-mode Automatic Differentiation (AD)), you must evaluate the adjoint ODE backward in time, which requires access to the forward (primal) state at each time step. Because the state dimension is large, you cannot retain all $N$ states. Instead, you adopt checkpointing: you selectively store exact copies of forward states at some time steps to limit recomputation during the adjoint sweep.\n\nAssume the following idealized and scientifically realistic conditions for the integrator and memory system:\n- The cost per forward step and per adjoint step is uniform and does not depend on time.\n- Each stored checkpoint is an exact copy of the forward state at a time step. At any time, you may hold at most $M$ such checkpoints in memory, in addition to the current working state of the integrator.\n- Recomputations during the adjoint sweep proceed by restoring a checkpoint and reintegrating forward to reconstruct the needed forward states. Disk input/output costs are negligible compared to the cost of forward steps.\n\nDefine a recomputation episode as a contiguous block of forward steps executed during the backward (adjoint) sweep after restoring from a stored checkpoint. Let $r$ denote the allowed number of such recomputation episodes in an optimal schedule. An optimal checkpointing scheme minimizes recomputation by minimizing $r$ subject to the memory limit $M$.\n\nStarting from first principles about reverse-mode AD for ODEs and the necessity of reconstructing primal states for the adjoint, perform the following:\n1. Derive a recurrence for $F(r, M)$, the maximum number of time steps that can be supported by an optimal schedule using at most $M$ in-memory checkpoints and at most $r$ recomputation episodes. Prove that this recurrence admits a closed-form solution of the form $F(r, M) = \\binom{M + r}{M}$ by a constructive argument grounded in the combinatorics of how segments can be covered under nested checkpointing.\n2. Using your derived result, determine the minimal integer $r^{\\star}$ such that an optimal memory-constrained checkpointing scheme with $M = 10$ can support a long simulation of $N = 1000$ steps on $[0, T]$ while minimizing recomputation.\n\nExpress your final answer as the single integer $r^{\\star}$. No rounding is needed. No units are required.",
            "solution": "This problem addresses the optimal checkpointing strategy for reverse-mode automatic differentiation of ODEs, a fundamental challenge in computational science, particularly in calibrating large-scale models. The solution is partitioned into two components as requested: first, the derivation and proof of the formula for the maximum number of supportable time steps, $F(r, M)$; second, the application of this formula to find the minimum required number of recomputation episodes, $r^{\\star}$, for a given problem size.\n\n### Part 1: Derivation of the Recurrence and Proof of the Closed-Form Solution for $F(r, M)$\n\nLet $F(r, M)$ be the maximum number of time steps that can be supported by an optimal checkpointing schedule using at most $M$ in-memory checkpoints and at most $r$ recomputation episodes. The core of the optimal strategy, often known as the 'revolve' algorithm, is recursive. We derive the governing recurrence relation by constructing the process.\n\nConsider an arbitrary time interval of $N$ steps to be covered with a budget of $(r, M)$ resources, where $r$ is the number of allowed recomputation episodes and $M$ is the number of available checkpoint slots. A recomputation episode is defined as restoring a state from a checkpoint and reintegrating forward.\n\nThe recursive strategy proceeds as follows:\n1.  A single checkpoint is placed at an intermediate time step, say $k$, where $1 \\le k  N$. This action consumes one of the $M$ available checkpoint slots.\n2.  The forward integration continues from step $k$ to the end of the interval at step $N$. The remaining subproblem is to cover the time interval $[k, N]$, which consists of $N-k$ steps. For this task, we have $M-1$ remaining checkpoint slots and still retain the full budget of $r$ recomputation episodes. Thus, the maximum length of this second segment is limited by $F(r, M-1)$.\n3.  After the forward pass is complete and the adjoint variables for the interval $[k, N]$ have been computed (by recursively applying the strategy), we are at time step $k$. At this point, all checkpoints taken in $(k, N]$ are no longer needed and can be discarded.\n4.  To compute the adjoints for the first interval, $[0, k]$, we must restore the primal state from the checkpoint saved at step $k$. This action, by definition, initiates a \"recomputation episode\". Therefore, it consumes one of the $r$ available episodes.\n5.  With the state at $k$ restored, we now face the subproblem of covering the interval $[0, k]$, which has length $k$. For this task, we have a reduced budget of $r-1$ recomputation episodes. However, since we are processing a new, independent segment, all $M$ checkpoint slots are once again available. Thus, the maximum length of this first segment is limited by $F(r-1, M)$.\n\nTo maximize the total number of steps $N = k + (N-k)$, we should choose the lengths of the subproblems to be maximal. This gives the recurrence relation:\n$$F(r, M) = F(r-1, M) + F(r, M-1)$$\nThis recurrence is valid for $r \\ge 1$ and $M \\ge 1$. To solve it, we must establish the boundary conditions for $r=0$ or $M=0$.\n\n**Boundary Conditions:**\n-   **Case 1: $F(r, 0)$** (No checkpoint slots). If $M=0$, we cannot place any checkpoints. The recursive strategy, which hinges on placing a checkpoint to subdivide the interval, cannot be applied. The problem cannot be broken down into smaller, manageable pieces. The only interval that can be processed without subdivision is a trivial one of a single time step. For an interval of length $1$ (e.g., from $t_0$ to $t_1$), we compute $x_1$ from $x_0$. For the backward adjoint pass, we need the state $x_0$, which is the given initial condition for the interval. This requires no storage and no recomputation. Therefore, for any non-negative $r$, the maximum number of steps we can handle with zero checkpoints is $1$.\n    $$F(r, 0) = 1 \\quad \\text{for } r \\ge 0$$\n-   **Case 2: $F(0, M)$** (No recomputation episodes). If $r=0$, we are not allowed to perform any recomputation episodes, which are defined by restoring from a checkpoint. The recursive strategy requires such a step to process the first sub-interval $[0, k]$. If this step is forbidden, the strategy can only be applied if the segment requiring recomputation has length $0$, which implies no meaningful subdivision is possible. As in the previous case, the only problem that can be solved is a trivial one-step interval.\n    $$F(0, M) = 1 \\quad \\text{for } M \\ge 0$$\n\nNow, we prove by induction that the closed-form solution is $F(r, M) = \\binom{M+r}{M}$. Let $G(r, M) = \\binom{M+r}{M}$. We must show that $G(r, M)$ satisfies the same recurrence and boundary conditions.\n\n1.  **Recurrence Relation:** The fundamental identity for binomial coefficients is Pascal's rule: $\\binom{n}{k} = \\binom{n-1}{k-1} + \\binom{n-1}{k}$. Let $n = M+r$ and $k=M$.\n    $$G(r, M-1) + G(r-1, M) = \\binom{(M-1)+r}{M-1} + \\binom{M+(r-1)}{M} = \\binom{M+r-1}{M-1} + \\binom{M+r-1}{M}$$\n    By Pascal's rule, this sum is equal to $\\binom{M+r}{M}$, which is $G(r, M)$. Thus, the formula satisfies the recurrence.\n\n2.  **Boundary Conditions:**\n    -   For $M=0$: $G(r, 0) = \\binom{0+r}{0} = 1$. This matches the boundary condition for $F(r, 0)$.\n    -   For $r=0$: $G(0, M) = \\binom{M+0}{M} = 1$. This matches the boundary condition for $F(0, M)$.\n\nSince the function $G(r, M)$ satisfies the same linear recurrence relation and boundary conditions as $F(r, M)$, the two functions must be identical. This concludes the proof that $F(r, M) = \\binom{M+r}{M}$.\n\n### Part 2: Determination of the Minimal Number of Recomputation Episodes $r^{\\star}$\n\nWe are given a simulation with $N=1000$ uniform time steps and an available memory for at most $M=10$ checkpoints. We need to find the minimal integer $r^{\\star}$ such that an optimal checkpointing scheme can support this simulation. This requires finding the smallest integer $r$ that satisfies the inequality:\n$$F(r, M) \\ge N$$\nSubstituting the given values and the derived formula:\n$$\\binom{10+r}{10} \\ge 1000$$\n\nWe test integer values of $r$ starting from $r=1$:\n-   For $r=1$: $F(1, 10) = \\binom{10+1}{10} = \\binom{11}{10} = \\frac{11!}{10!1!} = 11$. This is less than $1000$.\n-   For $r=2$: $F(2, 10) = \\binom{10+2}{10} = \\binom{12}{10} = \\frac{12!}{10!2!} = \\frac{12 \\times 11}{2} = 66$. This is less than $1000$.\n-   For $r=3$: $F(3, 10) = \\binom{10+3}{10} = \\binom{13}{10} = \\frac{13!}{10!3!} = \\frac{13 \\times 12 \\times 11}{3 \\times 2 \\times 1} = 13 \\times 2 \\times 11 = 286$. This is less than $1000$.\n-   For $r=4$: $F(4, 10) = \\binom{10+4}{10} = \\binom{14}{10} = \\frac{14!}{10!4!} = \\frac{14 \\times 13 \\times 12 \\times 11}{4 \\times 3 \\times 2 \\times 1} = 7 \\times 13 \\times 11 = 1001$.\n\nThe calculation for $r=4$ yields $F(4, 10) = 1001$, which is greater than or equal to the required $N=1000$ steps. Since $F(3, 10)  1000$, the minimal integer value for the number of recomputation episodes is $r^{\\star}=4$.",
            "answer": "$$\\boxed{4}$$"
        }
    ]
}