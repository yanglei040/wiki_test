{
    "hands_on_practices": [
        {
            "introduction": "A crucial step in quantifying uncertainty for dynamical models is to move from a deterministic description to a probabilistic one. This is often achieved by reformulating an ordinary differential equation (ODE) as a stochastic differential equation (SDE), which explicitly models random fluctuations. In this foundational practice , you will use the Euler-Maruyama approximation to derive the discrete-time transition density, which forms the basis of the likelihood function for time-series data and is a cornerstone of parameter inference in this context.",
            "id": "3357568",
            "problem": "You are given a continuous-time deterministic ordinary differential equation (ODE) model of a biological state vector $x(t) \\in \\mathbb{R}^d$ governed by $x'(t) = f(x(t), \\theta)$, where $f$ is the drift function and $\\theta$ is a vector of parameters. To quantify dynamical uncertainty, consider the Itô stochastic differential equation (SDE) $dx(t) = f(x(t), \\theta)\\,dt + \\Sigma^{1/2}\\,dW_t$, where $W_t$ is a standard $d$-dimensional Wiener process and $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is a constant, symmetric, positive semidefinite diffusion matrix. Measurements are taken at known times $t_k$, with $t_{k+1} - t_k = \\Delta t_k > 0$, and state values $x_k = x(t_k)$ are treated as directly observed states without measurement noise. Your task is to construct the discrete-time transition density implied by the Euler–Maruyama scheme and use it to compute the conditional log-density of $x_{k+1}$ given $x_k$ for provided test cases.\n\nStart from the standard definitions of an Itô SDE and a Wiener process, along with the existence and uniqueness conditions under global Lipschitz continuity and linear growth for $f$. Under the Euler–Maruyama approximation with step size $\\Delta t_k$, the increment over $[t_k,t_{k+1}]$ is approximated by a Gaussian random variable. Derive the form of the conditional transition density $p(x_{k+1}\\,|\\,x_k,\\theta,\\Sigma,\\Delta t_k)$ implied by Euler–Maruyama and express it explicitly in terms of $x_k$, $f(x_k,\\theta)$, $\\Delta t_k$, and $\\Sigma$. Then implement a program to compute the natural logarithm of the transition density for each of the test cases below. Additionally, in your written reasoning, enumerate all stochastic assumptions required for the construction to be valid.\n\nThe program you write must implement the following two deterministic drift models, which are common abstractions in computational systems biology:\n\n- One-dimensional logistic growth model with state $x \\in \\mathbb{R}$:\n  - $f(x,\\theta) = r\\,x\\,(1 - x/K)$,\n  - parameters $\\theta = (r, K)$ with $r > 0$ and $K > 0$.\n\n- Two-dimensional gene regulation model for messenger ribonucleic acid (mRNA) $m$ and protein $p$ with state $x = (m,p) \\in \\mathbb{R}^2$:\n  - $f_m(x,\\theta) = \\dfrac{\\alpha}{1 + (p/K)^n} - \\delta_m\\, m$,\n  - $f_p(x,\\theta) = \\beta\\, m - \\delta_p\\, p$,\n  - parameters $\\theta = (\\alpha, K, n, \\delta_m, \\beta, \\delta_p)$ with $\\alpha > 0$, $K > 0$, $n \\ge 1$, $\\delta_m > 0$, $\\beta > 0$, and $\\delta_p > 0$.\n\nFor each test case, you are given:\n- the model type,\n- the parameter vector $\\theta$,\n- the diffusion matrix $\\Sigma$ (either a scalar for the one-dimensional model or a diagonal matrix for the two-dimensional model),\n- the step size $\\Delta t_k$,\n- the state $x_k$ at time $t_k$,\n- and the subsequent state $x_{k+1}$ at time $t_{k+1}$.\n\nCompute the natural logarithm of the Euler–Maruyama transition density $p(x_{k+1}\\,|\\,x_k,\\theta,\\Sigma,\\Delta t_k)$ for each test case and output the results. The natural logarithm is dimensionless and must be reported as a real number without units.\n\nTest suite:\n- Case $1$ (one-dimensional logistic growth, general case): $\\theta = (r, K) = ($0.5$, $100.0$)$; $\\Sigma = [$0.04$]$; $\\Delta t_k = $0.1$; $x_k = [$50.0$]$; $x_{k+1} = [$52.0$]$.\n- Case $2$ (one-dimensional logistic growth, very small step size boundary): $\\theta = (r, K) = ($0.5$, $100.0$)$; $\\Sigma = [$0.01$]$; $\\Delta t_k = $10^{-6}$; $x_k = [$50.0$]$; $x_{k+1} = [$50.00001$]$.\n- Case $3$ (two-dimensional gene regulation, moderate noise): $\\theta = (\\alpha, K, n, \\delta_m, \\beta, \\delta_p) = ($10.0$, $50.0$, $2.0$, $1.0$, $5.0$, $0.2$)$; $\\Sigma = \\mathrm{diag}([$1.0$, $4.0$])$; $\\Delta t_k = $0.5$; $x_k = [$5.0$, $40.0$]$; $x_{k+1} = [$5.8$, $49.0$]$.\n- Case $4$ (two-dimensional gene regulation, highly anisotropic noise): $\\theta = (\\alpha, K, n, \\delta_m, \\beta, \\delta_p) = ($10.0$, $50.0$, $2.0$, $1.0$, $5.0$, $0.2$)$; $\\Sigma = \\mathrm{diag}([$0.01$, $9.0$])$; $\\Delta t_k = $0.2$; $x_k = [$2.0$, $10.0$]$; $x_{k+1} = [$3.4$, $11.7$]$.\n\nRequirements for the program:\n- Implement the derivation to obtain the Euler–Maruyama Gaussian transition density and compute its natural logarithm for each case.\n- Use double-precision floating-point arithmetic.\n- Output a single line containing a Python-style list with the four log-density values, each rounded to exactly six digits after the decimal point.\n- The final output format must be exactly a single line of the form $[v_1,v_2,v_3,v_4]$ where each $v_i$ is a floating-point number in fixed-point format with six digits after the decimal point and no units.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[v_1,v_2,v_3,v_4]$). The outputs are required to be floats with exactly six digits after the decimal point, corresponding to the four test cases in the order listed above.",
            "solution": "The problem requires the derivation and implementation of the conditional log-density of a state transition under the Euler–Maruyama discretization of an Itô stochastic differential equation (SDE). The problem is scientifically grounded, well-posed, and all necessary data are provided. We proceed with the solution.\n\n### 1. Underlying Principles and Stochastic Assumptions\n\nThe foundation of this problem is the Itô SDE, a standard mathematical tool for modeling systems that evolve continuously in time while subject to random fluctuations. The SDE is given as:\n$$\ndx(t) = f(x(t), \\theta)\\,dt + \\Sigma^{1/2}\\,dW_t\n$$\nwhere $x(t) \\in \\mathbb{R}^d$ is the state vector, $f(x, \\theta)$ is the deterministic drift term, $\\Sigma$ is the constant diffusion matrix, and $W_t$ is a $d$-dimensional standard Wiener process. The construction of the transition density relies on several key principles and assumptions:\n\n1.  **Existence and Uniqueness**: For the SDE to have a unique, non-explosive solution, the drift function $f$ and diffusion coefficient (here, a constant $\\Sigma^{1/2}$) must satisfy certain regularity conditions. The problem statement alludes to the standard sufficient conditions: global Lipschitz continuity and linear growth bounds on $f$.\n2.  **Markov Property**: The process $x(t)$ is a Markov process. This means the future evolution of the system from state $x_k = x(t_k)$ depends only on $x_k$ and not on the prior history of the process $\\{x(t) | t < t_k\\}$. This is a fundamental property of systems described by SDEs of this form.\n3.  **Gaussian Wiener Process Increments**: The defining feature of a standard Wiener process $W_t$ is that its increments over non-overlapping time intervals are independent and normally distributed. Specifically, for any $t_{k+1} > t_k$, the increment $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$ is a Gaussian random vector with mean $\\mathbb{E}[\\Delta W_k] = 0$ and covariance matrix $\\mathrm{Cov}(\\Delta W_k) = (t_{k+1} - t_k)I_d$, where $I_d$ is the $d \\times d$ identity matrix. We denote the time step as $\\Delta t_k = t_{k+1} - t_k$. Thus, $\\Delta W_k \\sim \\mathcal{N}(0, \\Delta t_k I_d)$.\n4.  **Time Homogeneity**: The drift function $f(x, \\theta)$ and the diffusion matrix $\\Sigma$ are assumed to be time-invariant. This implies that the dynamics of the system are stationary.\n5.  **Invertibility of the Diffusion Matrix**: To define a probability density function on $\\mathbb{R}^d$, the covariance matrix of the resulting distribution must be non-singular. This requires the diffusion matrix $\\Sigma$ to be strictly positive definite, not merely positive semidefinite. The test cases provided use positive definite matrices.\n\n### 2. Derivation of the Euler–Maruyama Transition Density\n\nThe Euler–Maruyama method provides a first-order numerical approximation for the solution of an SDE. Integrating the SDE from $t_k$ to $t_{k+1}$ yields:\n$$\nx(t_{k+1}) = x(t_k) + \\int_{t_k}^{t_{k+1}} f(x(s), \\theta) \\,ds + \\int_{t_k}^{t_{k+1}} \\Sigma^{1/2} \\,dW_s\n$$\nThe core approximation of the Euler–Maruyama scheme is to assume that the drift and diffusion coefficients are constant over the small time interval $[t_k, t_{k+1}]$, taking their values at the start of the interval, $t_k$. This gives:\n$$\nx_{k+1} \\approx x_k + f(x_k, \\theta) \\int_{t_k}^{t_{k+1}} ds + \\Sigma^{1/2} \\int_{t_k}^{t_{k+1}} dW_s\n$$\nThis simplifies to the discrete update rule:\n$$\nx_{k+1} \\approx x_k + f(x_k, \\theta) \\Delta t_k + \\Sigma^{1/2} \\Delta W_k\n$$\nwhere $x_k = x(t_k)$ and $\\Delta W_k = W_{t_{k+1}} - W_{t_k}$.\n\nThis approximation models the next state $x_{k+1}$ as a random variable whose distribution is conditioned on the current state $x_k$. Since $x_k$, $f(x_k, \\theta)$, $\\Delta t_k$, and $\\Sigma$ are all deterministic given the state at $t_k$, the stochasticity of $x_{k+1}$ arises entirely from the Wiener increment $\\Delta W_k$. As established, $\\Delta W_k$ is a Gaussian random vector. A linear transformation of a Gaussian random vector is also Gaussian. Therefore, $x_{k+1}$ conditioned on $x_k$ is a Gaussian random vector.\n\nWe now determine its mean and covariance.\n\n**Mean:**\n$$\n\\mathbb{E}[x_{k+1} | x_k] = \\mathbb{E}[x_k + f(x_k, \\theta)\\Delta t_k + \\Sigma^{1/2}\\Delta W_k | x_k]\n$$\nUsing the linearity of expectation and that $\\mathbb{E}[\\Delta W_k] = 0$:\n$$\n\\mu = \\mathbb{E}[x_{k+1} | x_k] = x_k + f(x_k, \\theta)\\Delta t_k\n$$\n\n**Covariance Matrix:**\n$$\n\\mathrm{Cov}(x_{k+1} | x_k) = \\mathrm{Cov}(x_k + f(x_k, \\theta)\\Delta t_k + \\Sigma^{1/2}\\Delta W_k | x_k)\n$$\nSince $x_k$ and $f(x_k, \\theta)\\Delta t_k$ are constant with respect to the conditioning, they do not contribute to the covariance.\n$$\n\\mathbf{C} = \\mathrm{Cov}(x_{k+1} | x_k) = \\mathrm{Cov}(\\Sigma^{1/2}\\Delta W_k) = \\Sigma^{1/2} \\mathrm{Cov}(\\Delta W_k) (\\Sigma^{1/2})^T\n$$\nGiven $\\mathrm{Cov}(\\Delta W_k) = \\Delta t_k I_d$ and that $\\Sigma$ is symmetric (so $(\\Sigma^{1/2})^T = \\Sigma^{1/2}$):\n$$\n\\mathbf{C} = \\Sigma^{1/2} (\\Delta t_k I_d) \\Sigma^{1/2} = \\Delta t_k \\Sigma^{1/2}\\Sigma^{1/2} = \\Delta t_k \\Sigma\n$$\n\nThus, the conditional transition density $p(x_{k+1} | x_k, \\theta, \\Sigma, \\Delta t_k)$ is the probability density function (PDF) of a multivariate normal distribution:\n$$\nx_{k+1} | x_k \\sim \\mathcal{N}\\left(\\mu, \\mathbf{C}\\right) \\quad \\text{where} \\quad \\mu = x_k + f(x_k, \\theta)\\Delta t_k \\quad \\text{and} \\quad \\mathbf{C} = \\Delta t_k \\Sigma\n$$\n\n### 3. Log-Density Formula\n\nThe PDF of a $d$-dimensional normal distribution $\\mathcal{N}(\\mu, \\mathbf{C})$ is:\n$$\np(y) = \\frac{1}{\\sqrt{(2\\pi)^d \\det(\\mathbf{C})}} \\exp\\left(-\\frac{1}{2} (y-\\mu)^T \\mathbf{C}^{-1} (y-\\mu)\\right)\n$$\nThe natural logarithm of this density, or log-density, is:\n$$\n\\ln p(y) = -\\frac{1}{2} \\ln\\left((2\\pi)^d \\det(\\mathbf{C})\\right) - \\frac{1}{2} (y-\\mu)^T \\mathbf{C}^{-1} (y-\\mu)\n$$\nExpanding the logarithm term:\n$$\n\\ln p(y) = -\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(\\mathbf{C})) - \\frac{1}{2} (y-\\mu)^T \\mathbf{C}^{-1} (y-\\mu)\n$$\nSubstituting $y = x_{k+1}$, $\\mu = x_k + f(x_k, \\theta)\\Delta t_k$, and $\\mathbf{C} = \\Delta t_k \\Sigma$:\n$$\n\\ln(\\det(\\mathbf{C})) = \\ln(\\det(\\Delta t_k \\Sigma)) = \\ln((\\Delta t_k)^d \\det(\\Sigma)) = d\\ln(\\Delta t_k) + \\ln(\\det(\\Sigma))\n$$\n$$\n\\mathbf{C}^{-1} = (\\Delta t_k \\Sigma)^{-1} = \\frac{1}{\\Delta t_k} \\Sigma^{-1}\n$$\nSubstituting these back into the log-density expression gives the final formula to be implemented:\n$$\n\\ln p(x_{k+1}|\\dots) = -\\frac{d}{2} \\ln(2\\pi) -\\frac{d}{2}\\ln(\\Delta t_k) - \\frac{1}{2}\\ln(\\det(\\Sigma)) - \\frac{1}{2\\Delta t_k} (x_{k+1} - \\mu)^T \\Sigma^{-1} (x_{k+1} - \\mu)\n$$\nThis formula will be applied to each test case. For the one-dimensional model, $d=1$ and the matrices become scalars. For the two-dimensional model, $d=2$ and standard matrix operations are used. In the provided test cases, $\\Sigma$ is diagonal, which simplifies the determinant and inverse calculations, but a general implementation is more robust.\nSpecifically, for a diagonal matrix $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_d^2)$, we have $\\det(\\Sigma) = \\prod_i \\sigma_i^2$ and $\\Sigma^{-1} = \\mathrm{diag}(1/\\sigma_1^2, \\dots, 1/\\sigma_d^2)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the conditional log-density for four test cases\n    based on the Euler-Maruyama approximation of SDEs.\n    \"\"\"\n\n    def f_logistic(x, theta):\n        \"\"\"Drift function for the one-dimensional logistic growth model.\"\"\"\n        r, K = theta\n        return r * x[0] * (1 - x[0] / K)\n\n    def f_gene_reg(x, theta):\n        \"\"\"Drift function for the two-dimensional gene regulation model.\"\"\"\n        alpha, K, n, delta_m, beta, delta_p = theta\n        m, p = x\n        f_m = alpha / (1 + (p / K)**n) - delta_m * m\n        f_p = beta * m - delta_p * p\n        return np.array([f_m, f_p])\n\n    def compute_log_density(d, x_k, x_k1, delta_t, f_val, Sigma):\n        \"\"\"\n        Computes the natural logarithm of the Euler-Maruyama transition density.\n\n        Args:\n            d (int): Dimension of the state space.\n            x_k (np.ndarray): State at time t_k.\n            x_k1 (np.ndarray): State at time t_k+1.\n            delta_t (float): Time step size.\n            f_val (np.ndarray): Drift function evaluated at x_k.\n            Sigma (np.ndarray): Diffusion matrix.\n\n        Returns:\n            float: The conditional log-density ln(p(x_{k+1}|x_k)).\n        \"\"\"\n        # Calculate the mean of the transition density\n        mu = x_k + f_val * delta_t\n        \n        # Deviation of the observed state from the mean\n        deviation = x_k1 - mu\n\n        # Calculate the components of the log-density formula:\n        # ln p = -d/2*ln(2*pi*delta_t) - 1/2*ln(det(Sigma)) - 1/(2*delta_t) * dev^T * Sigma^-1 * dev\n        \n        # Term 1: Normalization constant part\n        log_norm_const = -0.5 * d * (np.log(2 * np.pi) + np.log(delta_t))\n\n        # Term 2: Log-determinant of Sigma part\n        # Use slogdet for numerical stability, which returns (sign, log(abs(det)))\n        # Since Sigma is positive definite, sign is 1.\n        sign, log_det_Sigma = np.linalg.slogdet(Sigma)\n        log_det_term = -0.5 * log_det_Sigma\n        if sign <= 0: # Should not happen for positive definite Sigma\n            return -np.inf\n\n        # Term 3: Mahalanobis distance part\n        Sigma_inv = np.linalg.inv(Sigma)\n        mahalanobis_sq = deviation.T @ Sigma_inv @ deviation\n        mahalanobis_term = -0.5 * mahalanobis_sq / delta_t\n\n        return log_norm_const + log_det_term + mahalanobis_term\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"model\": \"logistic\",\n            \"theta\": (0.5, 100.0),\n            \"Sigma\": np.array([[0.04]]),\n            \"delta_t\": 0.1,\n            \"x_k\": np.array([50.0]),\n            \"x_k1\": np.array([52.0]),\n            \"d\": 1,\n            \"drift_func\": f_logistic\n        },\n        {\n            \"model\": \"logistic\",\n            \"theta\": (0.5, 100.0),\n            \"Sigma\": np.array([[0.01]]),\n            \"delta_t\": 1e-6,\n            \"x_k\": np.array([50.0]),\n            \"x_k1\": np.array([50.00001]),\n            \"d\": 1,\n            \"drift_func\": f_logistic\n        },\n        {\n            \"model\": \"gene_reg\",\n            \"theta\": (10.0, 50.0, 2.0, 1.0, 5.0, 0.2),\n            \"Sigma\": np.diag(np.array([1.0, 4.0])),\n            \"delta_t\": 0.5,\n            \"x_k\": np.array([5.0, 40.0]),\n            \"x_k1\": np.array([5.8, 49.0]),\n            \"d\": 2,\n            \"drift_func\": f_gene_reg\n        },\n        {\n            \"model\": \"gene_reg\",\n            \"theta\": (10.0, 50.0, 2.0, 1.0, 5.0, 0.2),\n            \"Sigma\": np.diag(np.array([0.01, 9.0])),\n            \"delta_t\": 0.2,\n            \"x_k\": np.array([2.0, 10.0]),\n            \"x_k1\": np.array([3.4, 11.7]),\n            \"d\": 2,\n            \"drift_func\": f_gene_reg\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Evaluate the drift function f at the current state x_k\n        f_val = case[\"drift_func\"](case[\"x_k\"], case[\"theta\"])\n        \n        # Ensure f_val is a numpy array for consistent calculations\n        if not isinstance(f_val, np.ndarray):\n            f_val = np.array([f_val])\n            \n        # Compute the log-density for the case\n        log_p = compute_log_density(\n            case[\"d\"],\n            case[\"x_k\"],\n            case[\"x_k1\"],\n            case[\"delta_t\"],\n            f_val,\n            case[\"Sigma\"]\n        )\n        results.append(log_p)\n\n    # Format the results to exactly six decimal places\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a probabilistic model and its corresponding likelihood are defined, we can perform Bayesian inference to quantify the uncertainty in our model parameters given observed data. This exercise  guides you through a complete, grid-based Bayesian analysis for a dynamical model of enzyme allostery, a common motif in systems biology. By combining prior beliefs with the likelihood of synthetic data, you will compute the posterior distribution and its summary statistics, providing a tangible understanding of how parameter uncertainties are characterized.",
            "id": "3357655",
            "problem": "You are asked to construct a Bayesian uncertainty quantification program for a dynamical model of enzyme allostery with hidden conformations. The fundamental base of this task consists of mass-action kinetics for ligand binding, two-state Markov process dynamics for conformational switching, and Bayes' theorem for parameter inference under a Gaussian observation model. The hidden state is represented by a continuously varying probability of the active conformation, and the model couples this hidden state to product formation. All angles appearing in trigonometric functions must be treated in radians. All physical quantities must be handled with explicit units as specified, and all outputs must be reported in the appropriate units.\n\nModel specification:\n- Let $L$ denote a constant ligand concentration (units: $\\mu\\mathrm{M}$), and let $K_d$ denote the dissociation constant (units: $\\mu\\mathrm{M}$). Under a rapid equilibrium approximation, the fraction of ligand-bound enzyme is $p_L = \\dfrac{L}{K_d + L}$.\n- Let $z(t)$ denote a hidden two-state conformation where the active state is occupied with probability $x(t)$ and the inactive state with probability $1 - x(t)$. The continuous-time Markov process for $x(t)$ is modeled by the ordinary differential equation\n$$\n\\frac{dx}{dt} = k_{\\text{on}}\\, p_L \\, (1 - x) - k_{\\text{off}}\\, (1 - p_L) \\, x,\n$$\nwhere $k_{\\text{on}}$ and $k_{\\text{off}}$ are conformational switching rate constants (units: $\\mathrm{s}^{-1}$). The initial condition is $x(0) = 0$.\n- Define $V_{\\max}$ (units: $\\mu\\mathrm{M}/\\mathrm{s}$) as the effective maximum rate for product formation under saturating substrate, and assume the product concentration $P(t)$ obeys\n$$\n\\frac{dP}{dt} = V_{\\max} \\, x(t), \\quad P(0) = 0.\n$$\n- Observation model: At discrete times $t_i$, noisy observations $y_i$ of $P(t)$ are available. The measurement noise is Gaussian with known standard deviation $\\sigma$ (units: $\\mu\\mathrm{M}$), so $y_i \\sim \\mathcal{N}(P(t_i), \\sigma^2)$.\n\nAnalytical solution under constant $L$:\n- Let $\\alpha = k_{\\text{on}}\\, p_L$ and $\\beta = k_{\\text{off}}\\, (1 - p_L)$. Then\n$$\nx_{\\mathrm{eq}} = \\frac{\\alpha}{\\alpha + \\beta}, \\quad x(t) = x_{\\mathrm{eq}} + (x(0) - x_{\\mathrm{eq}})\\, e^{-(\\alpha + \\beta)\\, t}.\n$$\nWith $x(0) = 0$, the product concentration is\n$$\nP(t) = V_{\\max} \\left( x_{\\mathrm{eq}}\\, t + \\frac{(x(0) - x_{\\mathrm{eq}})\\, \\left(1 - e^{-(\\alpha + \\beta)\\, t}\\right)}{\\alpha + \\beta} \\right) = V_{\\max} \\left( x_{\\mathrm{eq}}\\, t - \\frac{x_{\\mathrm{eq}} \\left(1 - e^{-(\\alpha + \\beta)\\, t}\\right)}{\\alpha + \\beta} \\right).\n$$\n\nBayesian inference objective:\n- Infer the posterior distribution of the parameter vector $\\theta = (k_{\\text{on}}, k_{\\text{off}}, K_d)$ given the observations $\\{(t_i, y_i)\\}_{i=1}^n$, with a log-uniform prior over each parameter within specified bounds. For a parameter $u \\in [u_{\\min}, u_{\\max}]$, the prior density is $p(u) \\propto 1/u$ on the interval and zero outside.\n- By Bayes' theorem, the posterior is proportional to the product of the prior and the likelihood:\n$$\np(\\theta \\mid \\{y_i\\}) \\propto p(\\theta)\\, \\prod_{i=1}^{n} \\exp\\left( -\\frac{(y_i - P(t_i; \\theta))^2}{2 \\sigma^2} \\right),\n$$\nwhere $P(t_i; \\theta)$ is the model-predicted product concentration at time $t_i$ given $\\theta$.\n\nComputational task:\n- Use a discretized parameter grid that is logarithmically spaced for each of $k_{\\text{on}}$, $k_{\\text{off}}$, and $K_d$ over the bounds provided in the test suite.\n- Compute the unnormalized posterior weight at each grid point as the product of the log-uniform prior density and the Gaussian likelihood (you may omit multiplicative constants common to all grid points).\n- Normalize the weights to obtain a discrete posterior distribution over the grid, then compute the posterior mean and posterior variance for each parameter. Report the posterior standard deviations (the square roots of the posterior variances).\n\nMeasurement construction for test data:\n- For each test case, synthetic observations are generated deterministically as\n$$\ny_i = P(t_i; \\theta_{\\text{true}}) + \\delta(t_i),\n$$\nwhere\n$$\n\\delta(t) = \\sigma \\left( 0.3 \\sin(0.7\\, t) + 0.2 \\cos(1.3\\, t) \\right),\n$$\nwith $t$ in $\\mathrm{s}$ and trigonometric arguments in radians.\n\nTest suite and units:\n- Common settings: $x(0) = 0$, $P(0) = 0$, $\\sigma = 0.05$ $\\mu\\mathrm{M}$, $V_{\\max} = 1.2$ $\\mu\\mathrm{M}/\\mathrm{s}$, observation times $t_i$ uniformly spaced from $t = 0$ $\\mathrm{s}$ to $t = 20$ $\\mathrm{s}$ in steps of $0.5$ $\\mathrm{s}$.\n- Parameter grid bounds for all cases: $k_{\\text{on}} \\in [0.01, 2.0]$ $\\mathrm{s}^{-1}$, $k_{\\text{off}} \\in [0.01, 2.0]$ $\\mathrm{s}^{-1}$, $K_d \\in [1.0, 200.0]$ $\\mu\\mathrm{M}$. Use $21$ logarithmically spaced points per parameter.\n- Test cases (each specifies $(k_{\\text{on}}, k_{\\text{off}}, K_d, L)$):\n    1. Case A (general scenario): $(0.5, 0.2, 5.0, 10.0)$ with units $(\\mathrm{s}^{-1}, \\mathrm{s}^{-1}, \\mu\\mathrm{M}, \\mu\\mathrm{M})$.\n    2. Case B (slow activation boundary): $(0.05, 0.5, 20.0, 5.0)$ with units $(\\mathrm{s}^{-1}, \\mathrm{s}^{-1}, \\mu\\mathrm{M}, \\mu\\mathrm{M})$.\n    3. Case C (weak binding edge): $(1.0, 0.05, 100.0, 40.0)$ with units $(\\mathrm{s}^{-1}, \\mathrm{s}^{-1}, \\mu\\mathrm{M}, \\mu\\mathrm{M})$.\n\nRequired outputs:\n- For each test case, compute the posterior standard deviations of $k_{\\text{on}}$ (units: $\\mathrm{s}^{-1}$), $k_{\\text{off}}$ (units: $\\mathrm{s}^{-1}$), and $K_d$ (units: $\\mu\\mathrm{M}$) under the specified grid-based Bayesian scheme.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the list has three nested lists, one per test case, each containing three floats corresponding to the posterior standard deviations of $k_{\\text{on}}$ (in $\\mathrm{s}^{-1}$), $k_{\\text{off}}$ (in $\\mathrm{s}^{-1}$), and $K_d$ (in $\\mu\\mathrm{M}$), in that exact order. For example: $[[s_{1,\\text{on}}, s_{1,\\text{off}}, s_{1,K_d}], [s_{2,\\text{on}}, s_{2,\\text{off}}, s_{2,K_d}], [s_{3,\\text{on}}, s_{3,\\text{off}}, s_{3,K_d}]]$ where each $s$ is a float.",
            "solution": "The construction proceeds from first principles of mass-action kinetics and Bayesian inference. First, we define the rapid equilibrium for ligand binding. Under this approximation, the fraction of bound enzymes is\n$$\np_L = \\frac{L}{K_d + L},\n$$\nwhich follows from mass-action binding equilibrium where the bound fraction increases with ligand concentration and decreases with higher dissociation constant.\n\nNext, we model conformational dynamics as a two-state continuous-time Markov process. Let the active conformation be favored by ligand binding, with transition rate proportional to $k_{\\text{on}}$ and $p_L$, and let the reverse transition occur with rate proportional to $k_{\\text{off}}$ and $(1 - p_L)$. The probability $x(t)$ of being in the active conformation satisfies the linear ordinary differential equation\n$$\n\\frac{dx}{dt} = k_{\\text{on}}\\, p_L\\, (1 - x) - k_{\\text{off}}\\, (1 - p_L)\\, x,\n$$\nwhich is derived from balancing the influx into the active state and the efflux from it. Defining $\\alpha = k_{\\text{on}}\\, p_L$ and $\\beta = k_{\\text{off}}\\, (1 - p_L)$ yields\n$$\n\\frac{dx}{dt} = \\alpha - (\\alpha + \\beta)\\, x.\n$$\nThis is a first-order linear inhomogeneous ordinary differential equation with solution\n$$\nx(t) = x_{\\mathrm{eq}} + (x(0) - x_{\\mathrm{eq}}) e^{-(\\alpha + \\beta)\\, t}, \\quad x_{\\mathrm{eq}} = \\frac{\\alpha}{\\alpha + \\beta}.\n$$\nWith $x(0) = 0$, this simplifies to\n$$\nx(t) = x_{\\mathrm{eq}} \\left( 1 - e^{-(\\alpha + \\beta)\\, t} \\right).\n$$\n\nProduct formation is assumed proportional to the active state occupancy, under the assumption of saturating substrate giving an effective rate $V_{\\max}$:\n$$\n\\frac{dP}{dt} = V_{\\max}\\, x(t), \\quad P(0) = 0.\n$$\nIntegrating,\n$$\nP(t) = \\int_0^t V_{\\max}\\, x(\\tau)\\, d\\tau = V_{\\max} \\int_0^t \\left[ x_{\\mathrm{eq}} + (x(0) - x_{\\mathrm{eq}}) e^{-(\\alpha + \\beta)\\, \\tau} \\right] d\\tau.\n$$\nSubstituting $x(0) = 0$ and evaluating the integral,\n$$\nP(t) = V_{\\max} \\left( x_{\\mathrm{eq}}\\, t - \\frac{x_{\\mathrm{eq}} \\left(1 - e^{-(\\alpha + \\beta)\\, t}\\right)}{\\alpha + \\beta} \\right).\n$$\n\nFor parameter inference, we use Bayes' theorem. With observations $y_i$ at times $t_i$ and Gaussian noise of known standard deviation $\\sigma$, the likelihood under independent noise is\n$$\n\\mathcal{L}(\\theta) = \\prod_{i=1}^{n} \\exp\\left( -\\frac{(y_i - P(t_i; \\theta))^2}{2 \\sigma^2} \\right),\n$$\nwhere constants such as $(2\\pi \\sigma^2)^{-n/2}$ can be omitted in proportional computations. The prior is chosen log-uniform for each parameter on specified bounds, expressing ignorance over orders of magnitude:\n$$\np(k_{\\text{on}}) \\propto \\frac{1}{k_{\\text{on}}}, \\quad p(k_{\\text{off}}) \\propto \\frac{1}{k_{\\text{off}}}, \\quad p(K_d) \\propto \\frac{1}{K_d},\n$$\nwithin their bounds, and zero outside. The joint prior is $p(\\theta) \\propto \\frac{1}{k_{\\text{on}}\\, k_{\\text{off}}\\, K_d}$.\n\nThe posterior is thus\n$$\np(\\theta \\mid \\{y_i\\}) \\propto \\left( \\frac{1}{k_{\\text{on}}\\, k_{\\text{off}}\\, K_d} \\right) \\prod_{i=1}^{n} \\exp\\left( -\\frac{(y_i - P(t_i; \\theta))^2}{2 \\sigma^2} \\right).\n$$\nTo compute posterior summaries, we discretize $\\theta$ on a logarithmically spaced grid within the bounds. At each grid point, we compute the unnormalized posterior weight\n$$\nw(\\theta) = \\left( \\frac{1}{k_{\\text{on}}\\, k_{\\text{off}}\\, K_d} \\right) \\exp\\left( -\\sum_{i=1}^{n} \\frac{(y_i - P(t_i; \\theta))^2}{2 \\sigma^2} \\right).\n$$\nFor numerical stability, we work with log-weights,\n$$\n\\log w(\\theta) = -\\log k_{\\text{on}} - \\log k_{\\text{off}} - \\log K_d - \\sum_{i=1}^{n} \\frac{(y_i - P(t_i; \\theta))^2}{2 \\sigma^2}.\n$$\nWe subtract the maximum log-weight across the grid and exponentiate to obtain normalized weights,\n$$\n\\tilde{w}(\\theta) = \\frac{\\exp(\\log w(\\theta) - \\max_{\\theta'} \\log w(\\theta'))}{\\sum_{\\theta''} \\exp(\\log w(\\theta'') - \\max_{\\theta'} \\log w(\\theta'))}.\n$$\nPosterior expectations follow by discrete summation:\n$$\n\\mathbb{E}[k_{\\text{on}}] = \\sum_{\\theta} \\tilde{w}(\\theta)\\, k_{\\text{on}}, \\quad \\mathbb{E}[k_{\\text{on}}^2] = \\sum_{\\theta} \\tilde{w}(\\theta)\\, k_{\\text{on}}^2,\n$$\nand the posterior variance is\n$$\n\\mathrm{Var}[k_{\\text{on}}] = \\mathbb{E}[k_{\\text{on}}^2] - \\left(\\mathbb{E}[k_{\\text{on}}]\\right)^2,\n$$\nwith analogous formulas for $k_{\\text{off}}$ and $K_d$. Posterior standard deviations are the square roots of these variances.\n\nSynthetic data for each test case is generated deterministically. For the true parameter $\\theta_{\\text{true}}$, compute $P(t_i; \\theta_{\\text{true}})$ using the analytical formula with $x(0) = 0$. The observations are\n$$\ny_i = P(t_i; \\theta_{\\text{true}}) + \\delta(t_i), \\quad \\delta(t) = \\sigma \\left( 0.3 \\sin(0.7\\, t) + 0.2 \\cos(1.3\\, t) \\right).\n$$\nThis deterministic construction allows reproducible evaluation of the likelihood while still representing structured deviations consistent with the Gaussian scale parameter $\\sigma$.\n\nAlgorithmic steps:\n1. For each test case, set $L$, $V_{\\max}$, $\\sigma$, and build the time grid $\\{t_i\\}$ from $t = 0$ $\\mathrm{s}$ to $t = 20$ $\\mathrm{s}$ with step $0.5$ $\\mathrm{s}$.\n2. Compute $y_i$ using $\\theta_{\\text{true}}$ and the deterministic offset $\\delta(t_i)$.\n3. Construct logarithmically spaced grids for $k_{\\text{on}}$, $k_{\\text{off}}$, and $K_d$ over the bounds $[0.01, 2.0]$ $\\mathrm{s}^{-1}$, $[0.01, 2.0]$ $\\mathrm{s}^{-1}$, and $[1.0, 200.0]$ $\\mu\\mathrm{M}$, respectively, with $21$ points per parameter.\n4. For each grid point, compute $p_L$, $\\alpha$, $\\beta$, $x_{\\mathrm{eq}}$, and $P(t_i; \\theta)$; then compute $\\log w(\\theta)$.\n5. Normalize weights using the log-sum-exp trick and compute posterior expectations and variances for each parameter.\n6. Report the posterior standard deviations in the required single-line output format: $[[s_{1,\\text{on}}, s_{1,\\text{off}}, s_{1,K_d}], [s_{2,\\text{on}}, s_{2,\\text{off}}, s_{2,K_d}], [s_{3,\\text{on}}, s_{3,\\text{off}}, s_{3,K_d}]]$, where each $s$ is a float and the units are $(\\mathrm{s}^{-1}, \\mathrm{s}^{-1}, \\mu\\mathrm{M})$, respectively.\n\nThis approach integrates the foundational principles of mass-action kinetics and Bayesian inference with a computational algorithm grounded in discrete posterior approximation, ensuring scientifically realistic uncertainty quantification for the dynamical model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_times(t_start=0.0, t_end=20.0, dt=0.5):\n    # Generate time points from t_start to t_end inclusive.\n    n = int(round((t_end - t_start) / dt)) + 1\n    return np.linspace(t_start, t_end, n)\n\ndef x_eq_alpha_beta(kon, koff, Kd, L):\n    # Compute p_L, alpha, beta, and x_eq for constant L\n    pL = L / (Kd + L)\n    alpha = kon * pL\n    beta = koff * (1.0 - pL)\n    denom = alpha + beta\n    # Avoid division by zero; denom>0 for positive rates and bounded pL\n    xeq = alpha / denom if denom > 0 else 0.0\n    return pL, alpha, beta, xeq, denom\n\ndef P_t(t, Vmax, xeq, denom):\n    # Analytical P(t) for x(0)=0: P(t) = Vmax*( xeq*t - xeq*(1 - exp(-denom*t))/denom )\n    if denom == 0.0:\n        # If denom == 0, then x(t)=xeq constant; xequation reduces to Vmax*xeq*t\n        return Vmax * xeq * t\n    return Vmax * (xeq * t - xeq * (1.0 - np.exp(-denom * t)) / denom)\n\ndef synthetic_observations(theta_true, L, Vmax, sigma, times):\n    # Generate deterministic observations y_i = P(t_i; theta_true) + delta(t_i)\n    kon_true, koff_true, Kd_true = theta_true\n    pL, alpha, beta, xeq, denom = x_eq_alpha_beta(kon_true, koff_true, Kd_true, L)\n    P = P_t(times, Vmax, xeq, denom)\n    # Deterministic offset delta(t) with angles in radians\n    delta = sigma * (0.3 * np.sin(0.7 * times) + 0.2 * np.cos(1.3 * times))\n    y = P + delta\n    return y\n\ndef log_posterior_weights(y, times, Vmax, sigma, L, kon_grid, koff_grid, Kd_grid):\n    # Compute log-weights over the 3D grid: log w = log prior + log likelihood\n    # Prior: log-uniform => log p(theta) = -log(kon) - log(koff) - log(Kd) (up to constant)\n    # Likelihood: sum over i of -(y_i - P(t_i; theta))^2 / (2 sigma^2)\n    logw_list = []\n    theta_list = []\n    # Precompute constants\n    inv2sigma2 = 1.0 / (2.0 * sigma * sigma)\n    # Loop over grid points\n    for kon in kon_grid:\n        log_prior_kon = -np.log(kon)\n        for koff in koff_grid:\n            log_prior_koff = -np.log(koff)\n            for Kd in Kd_grid:\n                log_prior_Kd = -np.log(Kd)\n                # Model computation\n                pL, alpha, beta, xeq, denom = x_eq_alpha_beta(kon, koff, Kd, L)\n                P = P_t(times, Vmax, xeq, denom)\n                resid = y - P\n                ll = -np.sum((resid * resid) * inv2sigma2)\n                logw = log_prior_kon + log_prior_koff + log_prior_Kd + ll\n                logw_list.append(logw)\n                theta_list.append((kon, koff, Kd))\n    logw_arr = np.array(logw_list)\n    theta_arr = np.array(theta_list)  # shape (N, 3)\n    return logw_arr, theta_arr\n\ndef posterior_stats(logw_arr, theta_arr):\n    # Normalize weights using log-sum-exp trick and compute posterior means and stds\n    max_logw = np.max(logw_arr)\n    w = np.exp(logw_arr - max_logw)\n    w_sum = np.sum(w)\n    if w_sum == 0.0:\n        # Degenerate case: return NaNs\n        return np.array([np.nan, np.nan, np.nan]), np.array([np.nan, np.nan, np.nan])\n    w_norm = w / w_sum\n    # Compute means\n    means = np.sum(theta_arr * w_norm[:, None], axis=0)\n    # Compute second moments\n    second_moments = np.sum((theta_arr ** 2) * w_norm[:, None], axis=0)\n    variances = second_moments - means ** 2\n    variances = np.maximum(variances, 0.0)  # Numerical safety\n    stds = np.sqrt(variances)\n    return means, stds\n\ndef solve():\n    # Define common settings\n    sigma = 0.05  # in uM\n    Vmax = 1.2    # in uM/s\n    times = generate_times(0.0, 20.0, 0.5)  # seconds\n\n    # Define parameter grids (logarithmically spaced)\n    kon_grid = np.logspace(np.log10(0.01), np.log10(2.0), 21)    # s^-1\n    koff_grid = np.logspace(np.log10(0.01), np.log10(2.0), 21)   # s^-1\n    Kd_grid = np.logspace(np.log10(1.0), np.log10(200.0), 21)    # uM\n\n    # Test cases: (kon_true, koff_true, Kd_true, L)\n    test_cases = [\n        (0.5, 0.2, 5.0, 10.0),   # Case A\n        (0.05, 0.5, 20.0, 5.0),  # Case B\n        (1.0, 0.05, 100.0, 40.0) # Case C\n    ]\n\n    results = []\n    for kon_true, koff_true, Kd_true, L in test_cases:\n        theta_true = (kon_true, koff_true, Kd_true)\n        # Generate synthetic observations\n        y = synthetic_observations(theta_true, L, Vmax, sigma, times)\n        # Compute log posterior weights over grid\n        logw_arr, theta_arr = log_posterior_weights(y, times, Vmax, sigma, L, kon_grid, koff_grid, Kd_grid)\n        # Compute posterior statistics\n        means, stds = posterior_stats(logw_arr, theta_arr)\n        # Append posterior standard deviations in required units [kon sd (1/s), koff sd (1/s), Kd sd (uM)]\n        # Round for readability\n        results.append([float(f\"{stds[0]:.6f}\"), float(f\"{stds[1]:.6f}\"), float(f\"{stds[2]:.6f}\")])\n\n    # Final print statement in the exact required format.\n    # Single line with nested lists for the three test cases.\n    print(f\"[{','.join(['['+','.join(map(str, r))+']' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond characterizing existing uncertainty, a key application of Uncertainty Quantification (UQ) is to guide future research by designing more informative experiments. This advanced practice  delves into the field of Bayesian optimal experimental design (OED) for a nonlinear oscillator model. You will implement the D-optimality criterion, using local sensitivity analysis to identify the measurement times that are predicted to be most effective at reducing parameter uncertainty, demonstrating how UQ can close the loop between modeling and experimentation.",
            "id": "3357652",
            "problem": "You are given a nonlinear dynamical model representing a simplified circadian oscillator in computational systems biology. The state vector is two-dimensional, $x(t) = [x_1(t), x_2(t)]^\\top$, with dynamics governed by a Stuart–Landau oscillator:\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\nx_1 \\\\\nx_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(\\mu - \\kappa (x_1^2 + x_2^2)) x_1 - \\omega x_2 \\\\\n(\\mu - \\kappa (x_1^2 + x_2^2)) x_2 + \\omega x_1\n\\end{bmatrix},\n$$\nwhere $\\theta = [\\mu, \\kappa, \\omega]^\\top$ is the parameter vector, $\\mu$ and $\\kappa$ are real-valued amplitude/damping parameters, and $\\omega$ is the angular frequency. The initial condition is fixed and independent of $\\theta$: $x(0) = [1.0, 0.0]^\\top$. The measurement model is additive Gaussian noise on the first state component,\n$y(t) = x_1(t) + \\varepsilon(t)$, $\\varepsilon(t) \\sim \\mathcal{N}(0,\\sigma^2)$,\nwith independent noise across sampling times.\n\nUncertainty quantification proceeds under a Gaussian prior for $\\theta$, $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$, and a local (first-order) approximation to the likelihood around $\\theta_0$ using sensitivities $\\partial x_1(t;\\theta)/\\partial \\theta$ evaluated at $\\theta_0$. Under this approximation, the posterior uncertainty can be expressed using a curvature-based update from the sensitivities and the measurement noise variance. The $D$-optimal experimental design criterion is to choose sampling times that minimize the posterior entropy of $\\theta$. You must implement this criterion using the standard curvature-based approximation obtained from local linearization of the measurement function and the Gaussian prior, and compute sampling times on a discrete candidate grid subject to resource constraints.\n\nFundamental base:\n- Bayes' rule and the properties of multivariate normal distributions.\n- Sensitivity analysis for ordinary differential equations: for an autonomous system $\\dot{x} = f(x,\\theta)$, the parameter sensitivity matrix $S(t) = \\partial x(t;\\theta)/\\partial \\theta$ satisfies a first-order linear nonhomogeneous system $\\dot{S}(t) = (\\partial f/\\partial x)\\,S(t) + \\partial f/\\partial \\theta$ with $S(0)=0$ when the initial condition does not depend on $\\theta$.\n- For independent Gaussian measurement noise, local curvature aggregation across sampling times is governed by the derivatives of the measurement function with respect to parameters.\n\nYour program must:\n1. For each candidate sampling time $t$ on a provided grid, compute the sensitivity vector $\\partial x_1(t;\\theta_0)/\\partial \\theta$ via integration of the augmented sensitivity system.\n2. Use these sensitivities and the noise variance to construct the appropriate curvature contributions for each time.\n3. Under a discrete resource constraint that limits the number of samples to $k$, and with a minimum separation $\\Delta$ between any two selected sampling times, perform an exhaustive search over all valid subsets of the candidate times of size $k$ to find the subset that minimizes the posterior entropy of $\\theta$ under the local Gaussian approximation defined above.\n4. Express all selected sampling times in hours, rounded to three decimal places.\n\nTest suite:\nYou must solve the $D$-optimal selection problem for each of the following four cases. In each case, the angular frequency $\\omega$ is in radians per hour, and all times are in hours.\n- Case A (happy path):\n  - $\\theta_0 = [0.5, 0.5, 2\\pi/24]^\\top$, $\\Sigma_0 = \\mathrm{diag}([0.3^2, 0.3^2, 0.1^2])$, $\\sigma = 0.1$.\n  - Candidate times: $t \\in \\{0, 3, 6, \\dots, 48\\}$ (inclusive).\n  - Budget: $k = 5$ samples.\n  - Minimum separation: $\\Delta = 6.0$ hours.\n- Case B (boundary case: no samples allowed):\n  - $\\theta_0 = [0.5, 0.5, 2\\pi/24]^\\top$, $\\Sigma_0 = \\mathrm{diag}([0.3^2, 0.3^2, 0.1^2])$, $\\sigma = 0.1$.\n  - Candidate times: $t \\in \\{0, 3, 6, \\dots, 48\\}$ (inclusive).\n  - Budget: $k = 0$ samples.\n  - Minimum separation: $\\Delta = 6.0$ hours.\n- Case C (edge case: high noise and sparse sampling):\n  - $\\theta_0 = [0.8, 1.2, 2\\pi/20]^\\top$, $\\Sigma_0 = \\mathrm{diag}([0.5^2, 0.5^2, 0.2^2])$, $\\sigma = 1.5$.\n  - Candidate times: $t \\in \\{0, 2, 4, \\dots, 24\\}$ (inclusive).\n  - Budget: $k = 3$ samples.\n  - Minimum separation: $\\Delta = 12.0$ hours.\n- Case D (edge case: strongly informative prior):\n  - $\\theta_0 = [1.2, 0.7, 2\\pi/26]^\\top$, $\\Sigma_0 = \\mathrm{diag}([0.1^2, 0.1^2, 0.05^2])$, $\\sigma = 0.15$.\n  - Candidate times: $t \\in \\{0, 6, 12, \\dots, 36\\}$ (inclusive).\n  - Budget: $k = 4$ samples.\n  - Minimum separation: $\\Delta = 6.0$ hours.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be the list of selected sampling times (in hours, rounded to three decimals) for the corresponding case, in the order A, B, C, D. For example, a valid output format is:\n$$\n[[t_{A,1}, t_{A,2}, \\dots, t_{A,5}],\\,[\\,],\\,[t_{C,1}, t_{C,2}, t_{C,3}],\\,[t_{D,1}, t_{D,2}, t_{D,3}, t_{D,4}]\\,].\n$$\nAll numerical answers must be floats or lists of floats; no percentage signs are permitted. Angles are in radians; times must be expressed in hours.",
            "solution": "The user-provided problem is a well-posed and scientifically grounded exercise in Bayesian optimal experimental design, a core topic in uncertainty quantification for dynamical models within computational systems biology. The problem is valid as it provides a complete and consistent setup based on established mathematical and statistical principles. We will proceed with a full solution.\n\nThe objective is to select a set of $k$ sampling times $\\{t_1, t_2, \\dots, t_k\\}$ from a discrete candidate grid that is optimal in the sense of the $D$-optimality criterion. This criterion seeks to minimize the generalized variance of the parameter posterior distribution, which is equivalent to minimizing the determinant of the posterior covariance matrix.\n\n### 1. Bayesian Framework and Posterior Approximation\n\nThe foundation of the solution is Bayes' rule, which relates the posterior probability of the parameters $\\theta$ given data $y$ to the likelihood of the data and the prior probability of the parameters:\n$$\np(\\theta | y) \\propto p(y | \\theta) p(\\theta)\n$$\nThe problem specifies a Gaussian prior for the parameter vector $\\theta = [\\mu, \\kappa, \\omega]^\\top$:\n$$\n\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0) \\implies p(\\theta) \\propto \\exp\\left(-\\frac{1}{2}(\\theta - \\theta_0)^\\top \\Sigma_0^{-1} (\\theta - \\theta_0)\\right)\n$$\nThe measurement model at a set of discrete times $\\{t_1, \\dots, t_k\\}$ is given by $y(t_i) = x_1(t_i; \\theta) + \\varepsilon_i$, where the noise terms $\\varepsilon_i$ are independent and identically distributed as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. The likelihood for the full data vector $y = [y(t_1), \\dots, y(t_k)]^\\top$ is:\n$$\np(y|\\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^k (y(t_i) - x_1(t_i; \\theta))^2\\right)\n$$\nSince the model $x_1(t_i; \\theta)$ is a nonlinear function of $\\theta$, the resulting posterior distribution is non-Gaussian. The problem specifies using a local approximation around the prior mean $\\theta_0$. We linearize the model output with respect to the parameters:\n$$\nx_1(t; \\theta) \\approx x_1(t; \\theta_0) + \\left(\\frac{\\partial x_1(t; \\theta)}{\\partial \\theta}\\bigg|_{\\theta_0}\\right)^\\top (\\theta - \\theta_0)\n$$\nSubstituting this approximation into the log-posterior yields a quadratic form in $\\theta$, implying that the posterior is approximately Gaussian, $p(\\theta|y) \\approx \\mathcal{N}(\\theta_N, \\Sigma_N)$. The posterior precision matrix (inverse covariance) $\\Sigma_N^{-1}$ is given by the sum of the prior precision and the information gained from the measurements:\n$$\n\\Sigma_N^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} \\sum_{i=1}^k s(t_i) s(t_i)^\\top\n$$\nwhere $s(t) = \\frac{\\partial x_1(t; \\theta)}{\\partial \\theta}\\big|_{\\theta_0}$ is the $3 \\times 1$ vector of measurement sensitivities with respect to the parameters, evaluated at the nominal parameter values $\\theta_0$. Each term $\\frac{1}{\\sigma^2} s(t_i) s(t_i)^\\top$ represents the Fisher information contribution from a single measurement at time $t_i$.\n\n### 2. D-Optimality Criterion\n\nThe $D$-optimality criterion aims to minimize the volume of the posterior uncertainty ellipsoid, which is proportional to $\\sqrt{\\det(\\Sigma_N)}$. This is equivalent to minimizing $\\det(\\Sigma_N)$, or, more conveniently, maximizing its inverse $\\det(\\Sigma_N^{-1})$. The design problem is thus to select the set of $k$ sampling times that maximizes the determinant of the posterior precision matrix:\n$$\n\\underset{\\{t_1, \\dots, t_k\\}}{\\text{argmax}} \\quad \\det\\left( \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} \\sum_{i=1}^k s(t_i) s(t_i)^\\top \\right)\n$$\nThis selection is subject to the constraints on the total number of samples, $k$, and the minimum time separation, $\\Delta$.\n\n### 3. Sensitivity Analysis for the Dynamical System\n\nTo compute the sensitivity vectors $s(t)$, we must solve the sensitivity equations associated with the main ODE system. Let the state be $z(t) = [x_1(t), x_2(t)]^\\top$ and the dynamics be $\\dot{z} = f(z, \\theta)$. The parameter sensitivity matrix $S(t) = \\frac{\\partial z(t)}{\\partial \\theta}$ is a $2 \\times 3$ matrix that evolves according to the linear nonhomogeneous ODE:\n$$\n\\frac{d}{dt}S(t) = J_z(t) S(t) + J_\\theta(t)\n$$\nwhere $J_z = \\frac{\\partial f}{\\partial z}$ is the Jacobian of the dynamics with respect to the state, and $J_\\theta = \\frac{\\partial f}{\\partial \\theta}$ is the Jacobian with respect to the parameters. The initial condition is $S(0) = \\mathbf{0}$, as the initial state $z(0)$ is independent of $\\theta$.\n\nThe Jacobians are:\n$$\nJ_z = \\frac{\\partial f}{\\partial z} = \\begin{bmatrix}\n\\mu - \\kappa(3x_1^2 + x_2^2) & -2\\kappa x_1 x_2 - \\omega \\\\\n-2\\kappa x_1 x_2 + \\omega & \\mu - \\kappa(x_1^2 + 3x_2^2)\n\\end{bmatrix}\n$$\n$$\nJ_\\theta = \\frac{\\partial f}{\\partial \\theta} = \\begin{bmatrix}\nx_1 & -(x_1^2+x_2^2)x_1 & -x_2 \\\\\nx_2 & -(x_1^2+x_2^2)x_2 & x_1\n\\end{bmatrix}\n$$\nWe form an augmented ODE system with an $8$-dimensional state vector $Y(t) = [x_1, x_2, S_{11}, S_{12}, S_{13}, S_{21}, S_{22}, S_{23}]^\\top$. This system is integrated numerically from $t=0$ with initial condition $Y(0)=[1.0, 0.0, 0, 0, 0, 0, 0, 0]^\\top$. The desired measurement sensitivity vector is the first row of the sensitivity submatrix: $s(t) = [S_{11}(t), S_{12}(t), S_{13}(t)]^\\top$.\n\n### 4. Algorithmic Procedure\n\nThe solution is implemented via the following algorithm for each test case:\n1.  **Initialization**: The parameters $\\theta_0$, $\\Sigma_0$, $\\sigma$, the candidate times $T_{\\text{cand}}$, the budget $k$, and the separation $\\Delta$ are defined. The prior precision matrix $\\Sigma_0^{-1}$ is computed. For the special case $k=0$, the optimal set is empty.\n2.  **Sensitivity Computation**: The $8$-dimensional augmented ODE system is solved numerically using `scipy.integrate.solve_ivp`. The solution is evaluated at all time points in $T_{\\text{cand}}$ to obtain the state $z(t)$ and sensitivity matrix $S(t)$ at each point. The required sensitivity vectors $s(t) = [S_{11}(t), S_{12}(t), S_{13}(t)]^\\top$ are extracted.\n3.  **Search for Optimal Times**: An exhaustive search is performed over all valid combinations of sampling times.\n    a. Using `itertools.combinations`, all subsets of $T_{\\text{cand}}$ of size $k$ are generated.\n    b. Each subset is tested against the minimum separation constraint: for any two selected times $t_i, t_j$ with $i \\ne j$, it must hold that $|t_i - t_j| \\ge \\Delta$.\n    c. For each valid subset $\\{t_1, \\dots, t_k\\}$, the determinant of the posterior precision matrix, $\\det(\\Sigma_N^{-1})$, is calculated.\n    d. The subset of times that yields the maximum determinant value is selected as the $D$-optimal design.\n4.  **Output Formatting**: The selected times are rounded to three decimal places and formatted as required.\n\nThis procedure guarantees finding the globally optimal design within the specified discrete search space and under the given local linearization approximation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom itertools import combinations\n\ndef augmented_ode(t, y, mu, kappa, omega):\n    \"\"\"\n    Defines the augmented ODE system for the Stuart-Landau oscillator and its parameter sensitivities.\n    \n    The state vector `y` has 8 components:\n    y = [x1, x2, S11, S12, S13, S21, S22, S23]\n    where Sij = d(xi)/d(theta_j) and theta = [mu, kappa, omega].\n    The sensitivity matrix S is flattened in row-major order.\n    \"\"\"\n    x1, x2 = y[0], y[1]\n    S = y[2:].reshape((2, 3))\n\n    # Original system dynamics (f)\n    r_sq = x1**2 + x2**2\n    d_x1_dt = (mu - kappa * r_sq) * x1 - omega * x2\n    d_x2_dt = (mu - kappa * r_sq) * x2 + omega * x1\n    \n    # Jacobian of f with respect to state x (J_z)\n    Jz = np.zeros((2, 2))\n    Jz[0, 0] = mu - kappa * (3 * x1**2 + x2**2)\n    Jz[0, 1] = -2 * kappa * x1 * x2 - omega\n    Jz[1, 0] = -2 * kappa * x1 * x2 + omega\n    Jz[1, 1] = mu - kappa * (x1**2 + 3 * x2**2)\n    \n    # Jacobian of f with respect to parameters theta (J_theta)\n    J_theta = np.zeros((2, 3))\n    J_theta[0, 0] = x1          # df1/dmu\n    J_theta[1, 0] = x2          # df2/dmu\n    J_theta[0, 1] = -r_sq * x1  # df1/dkappa\n    J_theta[1, 1] = -r_sq * x2  # df2/dkappa\n    J_theta[0, 2] = -x2         # df1/domega\n    J_theta[1, 2] = x1          # df2/domega\n\n    # Sensitivity dynamics: dS/dt = J_z * S + J_theta\n    dS_dt = Jz @ S + J_theta\n    \n    # Combine derivatives into a single flat vector\n    derivatives = np.concatenate(([d_x1_dt, d_x2_dt], dS_dt.flatten()))\n    \n    return derivatives\n\ndef find_optimal_times(params):\n    \"\"\"\n    Finds the D-optimal sampling times for a given experimental setup.\n    \"\"\"\n    theta0, Sigma0, sigma, t_candidates, k, delta_t = params\n    \n    if k == 0:\n        return []\n\n    mu0, kappa0, omega0 = theta0\n    \n    # 1. Compute prior precision matrix\n    Sigma0_inv = np.diag(1 / np.diag(Sigma0))\n\n    # 2. Integrate ODE system to get sensitivities at all candidate times\n    y0 = np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]) # x(0)=[1,0], S(0)=0\n    t_span = [0, max(t_candidates)] if t_candidates else [0, 0]\n    \n    sol = solve_ivp(\n        fun=augmented_ode,\n        t_span=t_span,\n        y0=y0,\n        t_eval=t_candidates,\n        args=(mu0, kappa0, omega0),\n        method='LSODA', \n        rtol=1e-6, \n        atol=1e-8\n    )\n\n    if not sol.success:\n        raise RuntimeError(f\"ODE integration failed for case with theta0={theta0}\")\n\n    # 3. Pre-compute curvature contributions for each candidate time\n    curvature_contributions = {}\n    for i, t in enumerate(sol.t):\n        # Sensitivity of measurement y=x1 w.r.t. theta: s(t) = [S11, S12, S13]\n        s_t = sol.y[2:5, i]\n        C_t = (1 / sigma**2) * np.outer(s_t, s_t)\n        curvature_contributions[t] = C_t\n\n    # 4. Exhaustive search for the optimal combination of times\n    best_determinant = -1.0\n    best_times = []\n    \n    candidate_indices = list(range(len(t_candidates)))\n\n    for index_combo in combinations(candidate_indices, k):\n        time_combo = [t_candidates[i] for i in index_combo]\n        \n        # Check separation constraint. Assumes time_combo is sorted, which it is.\n        is_valid = all(time_combo[i] - time_combo[i-1] >= delta_t for i in range(1, len(time_combo)))\n        \n        if not is_valid:\n            continue\n            \n        # Calculate posterior precision for this combination\n        posterior_precision = Sigma0_inv.copy()\n        for t in time_combo:\n            posterior_precision += curvature_contributions[t]\n            \n        # Calculate determinant (our objective function)\n        det = np.linalg.det(posterior_precision)\n        \n        if det > best_determinant:\n            best_determinant = det\n            best_times = time_combo\n\n    return [round(t, 3) for t in best_times]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        (\n            [0.5, 0.5, 2 * np.pi / 24],\n            np.diag([0.3**2, 0.3**2, 0.1**2]),\n            0.1,\n            np.arange(0, 48 + 3, 3).tolist(),\n            5,\n            6.0\n        ),\n        # Case B\n        (\n            [0.5, 0.5, 2 * np.pi / 24],\n            np.diag([0.3**2, 0.3**2, 0.1**2]),\n            0.1,\n            np.arange(0, 48 + 3, 3).tolist(),\n            0,\n            6.0\n        ),\n        # Case C\n        (\n            [0.8, 1.2, 2 * np.pi / 20],\n            np.diag([0.5**2, 0.5**2, 0.2**2]),\n            1.5,\n            np.arange(0, 24 + 2, 2).tolist(),\n            3,\n            12.0\n        ),\n        # Case D\n        (\n            [1.2, 0.7, 2 * np.pi / 26],\n            np.diag([0.1**2, 0.1**2, 0.05**2]),\n            0.15,\n            np.arange(0, 36 + 6, 6).tolist(),\n            4,\n            6.0\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        optimal_times = find_optimal_times(case)\n        results.append(optimal_times)\n\n    # Final print statement in the exact required format.\n    # The default str() for a list includes spaces, which is acceptable.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}