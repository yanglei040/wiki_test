{
    "hands_on_practices": [
        {
            "introduction": "在构建一个有用的生物数字孪生之前，一个基本问题是：我们选择的测量手段能否让我们推断出系统内部所有我们关心的状态？这个概念被称为“可观测性”。本练习 () 将此理论应用于一个实际的设计问题，要求你为一个急性炎症模型设计一个最小化的传感器组合。通过运用非线性控制理论中的李导数工具，你将确定需要测量哪些生物量才能确保系统在结构上是可观测的，从而为数字孪生的构建奠定坚实的基础。",
            "id": "3301928",
            "problem": "考虑为急性炎症的连续时间生物数字孪生设计一个最小传感器组。该孪生模型的内部状态是三个可测量的生物量：病原体载量 $x_1$、炎症介质浓度 $x_2$ 和组织损伤 $x_3$。其内部动力学通过以下平滑的常微分方程（ODE）系统进行建模，该系统基于宿主-病原体-炎症动力学的标准相互作用规则：\n$$\n\\begin{aligned}\n\\dot{x}_1 = a x_1 \\left(1 - \\frac{x_1}{K}\\right) - k x_2 x_1, \\\\\n\\dot{x}_2 = b x_1 - g x_2 - e x_3 x_2, \\\\\n\\dot{x}_3 = d x_2 - z x_3,\n\\end{aligned}\n$$\n其中 $a, K, k, b, g, e, d, z$ 是正常数，表示具有生物学可解释性的速率和容量（例如，$a$ 是病原体生长速率，$K$ 是承载能力，$k$ 是由 $x_2$ 介导的清除率，$b$ 是病原体对介质的刺激，$g$ 是介质衰减，$e$ 是损伤介导的介质移除，$d$ 是由 $x_2$ 驱动的损伤累积，$z$ 是损伤修复）。假设在生理学相关的操作范围内，$x_1, x_2, x_3  0$。\n\n您可以从标量测量函数集合 $\\{h_1(x) = x_1, \\; h_2(x) = x_2, \\; h_3(x) = x_3\\}$ 中选择输出；多传感器组对应于这些标量输出的拼接。使用基于李导数的非线性局部可观测性秩条件（Hermann–Krener 框架），提出一个最小传感器组，以实现该三状态系统的结构可观测性，并通过从所选输出的逐次李导数构造非线性可观测性矩阵（直到满足秩检验所需的阶数），并进行符号化的秩或行列式检验来证明您选择的合理性。将“结构可观测性”视为一种属性，即在所述正值域内，对于几乎所有状态和参数值，局部可观测性秩条件都得到满足（也就是说，失效集是一个低维流形，且秩条件不会因模型结构而恒为不满足）。\n\n报告实现结构可观测性所需的最小标量输出通道数作为您的最终答案。请用一个无单位的整数表示您的最终答案。无需四舍五入。",
            "solution": "用户希望找到实现一个三状态生物系统结构可观测性所需的最小传感器数量。\n\n### 问题验证\n该问题提供了一个模拟宿主-病原体动力学的非线性三维常微分方程（ODE）系统、一组可能的标量测量以及一个明确的目标。所要求的方法是 Hermann-Krener 可观测性秩条件。该系统描述如下：\n$$\n\\begin{aligned}\n\\dot{x}_1 = f_1(x) = a x_1 \\left(1 - \\frac{x_1}{K}\\right) - k x_2 x_1 \\\\\n\\dot{x}_2 = f_2(x) = b x_1 - g x_2 - e x_3 x_2 \\\\\n\\dot{x}_3 = f_3(x) = d x_2 - z x_3\n\\end{aligned}\n$$\n其中 $x = \\begin{pmatrix} x_1  x_2  x_3 \\end{pmatrix}^T$ 是状态向量。参数 $a, K, k, b, g, e, d, z$ 均为正常数，且状态为正，$x_1  0$, $x_2  0$, $x_3  0$。可能的传感器输出为 $h_1(x) = x_1$, $h_2(x) = x_2$ 和 $h_3(x) = x_3$。\n\n该问题具有科学依据，因为该模型是数学生物学中的一种标准表示。问题是适定的（well-posed），提供了应用指定控制理论框架所需的所有信息。语言客观而精确。该问题有效。\n\n### 求解推导\n为了确定系统的可观测性，我们应用非线性可观测性秩条件。对于一个状态空间维度为 $n$、输出为 $y = h(x)$ 的系统 $\\dot{x} = f(x)$，如果可观测性矩阵 $\\mathcal{O}(x)$ 的秩为 $n$，则该系统是局部可观测的。对于我们这个 $n=3$ 的系统，可观测性矩阵是一个 $3 \\times 3$ 矩阵，由输出函数 $h(x)$ 的逐次李导数的梯度构成：\n$$\n\\mathcal{O}(x) = \\begin{pmatrix} \\nabla (L_f^0 h(x)) \\\\ \\nabla (L_f^1 h(x)) \\\\ \\nabla (L_f^2 h(x)) \\end{pmatrix}\n$$\n其中 $L_f^0 h(x) = h(x)$，李导数递归定义为 $L_f^i h(x) = L_f(L_f^{i-1} h(x)) = \\nabla (L_f^{i-1} h(x)) \\cdot f(x)$。结构可观测性要求对于某个 $h(x)$ 的选择，$\\det(\\mathcal{O}(x))$ 不恒为零。\n\n我们寻求最小的传感器组。最小可能的配置是单个传感器。我们将测试单个标量输出 $h_1(x)$、$h_2(x)$ 或 $h_3(x)$ 是否能实现可观测性。我们将分析输出为 $y = h_3(x) = x_3$ 的情况。\n\n**第 1 步：零阶李导数**\n零阶李导数就是输出函数本身：\n$$L_f^0 h_3(x) = h_3(x) = x_3$$\n该函数相对于状态向量 $x = \\begin{pmatrix} x_1  x_2  x_3 \\end{pmatrix}^T$ 的梯度为：\n$$\\nabla (L_f^0 h_3(x)) = \\begin{pmatrix} \\frac{\\partial x_3}{\\partial x_1}  \\frac{\\partial x_3}{\\partial x_2}  \\frac{\\partial x_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\end{pmatrix}$$\n这构成了可观测性矩阵 $\\mathcal{O}_{h_3}(x)$ 的第一行。\n\n**第 2 步：一阶李导数**\n一阶李导数为：\n$$L_f^1 h_3(x) = L_f(x_3) = \\nabla x_3 \\cdot f(x) = \\begin{pmatrix} 0  0  1 \\end{pmatrix} \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ f_3(x) \\end{pmatrix} = f_3(x)$$\n代入 $f_3(x)$ 的表达式：\n$$L_f^1 h_3(x) = d x_2 - z x_3$$\n其梯度为：\n$$\\nabla (L_f^1 h_3(x)) = \\begin{pmatrix} \\frac{\\partial f_3}{\\partial x_1}  \\frac{\\partial f_3}{\\partial x_2}  \\frac{\\partial f_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0  d  -z \\end{pmatrix}$$\n这构成了 $\\mathcal{O}_{h_3}(x)$ 的第二行。\n\n**第 3 步：二阶李导数**\n二阶李导数为：\n$$L_f^2 h_3(x) = L_f(L_f^1 h_3(x)) = L_f(f_3(x)) = \\nabla f_3(x) \\cdot f(x)$$\n使用上一步计算的梯度：\n$$L_f^2 h_3(x) = \\begin{pmatrix} 0  d  -z \\end{pmatrix} \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ f_3(x) \\end{pmatrix} = d f_2(x) - z f_3(x)$$\n代入 $f_2(x)$ 和 $f_3(x)$ 的表达式：\n$$L_f^2 h_3(x) = d(b x_1 - g x_2 - e x_3 x_2) - z(d x_2 - z x_3)$$\n$$L_f^2 h_3(x) = db x_1 - dg x_2 - de x_2 x_3 - zd x_2 + z^2 x_3$$\n$$L_f^2 h_3(x) = db x_1 - (dg + zd)x_2 - de x_2 x_3 + z^2 x_3$$\n该函数的梯度为：\n$$\\nabla(L_f^2 h_3(x)) = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}(L_f^2 h_3)  \\frac{\\partial}{\\partial x_2}(L_f^2 h_3)  \\frac{\\partial}{\\partial x_3}(L_f^2 h_3) \\end{pmatrix}$$\n偏导数分别为：\n$$\\frac{\\partial}{\\partial x_1}(L_f^2 h_3(x)) = db$$\n$$\\frac{\\partial}{\\partial x_2}(L_f^2 h_3(x)) = -(dg + zd) - de x_3 = -(dg + zd + de x_3)$$\n$$\\frac{\\partial}{\\partial x_3}(L_f^2 h_3(x)) = -de x_2 + z^2$$\n这构成了 $\\mathcal{O}_{h_3}(x)$ 的第三行。\n\n**第 4 步：构造可观测性矩阵并检验其秩**\n我们使用计算出的梯度来组装可观测性矩阵：\n$$\n\\mathcal{O}_{h_3}(x) = \\begin{pmatrix} 0  0  1 \\\\ 0  d  -z \\\\ db  -(dg + zd + de x_3)  z^2 - de x_2 \\end{pmatrix}\n$$\n为检查秩是否为 $3$，我们计算其行列式。我们沿第一行进行代数余子式展开：\n$$\n\\det(\\mathcal{O}_{h_3}(x)) = 0 \\cdot \\det(\\dots) - 0 \\cdot \\det(\\dots) + 1 \\cdot \\det\\begin{pmatrix} 0  d \\\\ db  -(dg + zd + de x_3) \\end{pmatrix}\n$$\n$$\n\\det(\\mathcal{O}_{h_3}(x)) = (0) \\cdot (-(dg + zd + de x_3)) - (d) \\cdot (db) = -d^2 b\n$$\n参数 $b$ 和 $d$ 均为正常数。因此，行列式 $\\det(\\mathcal{O}_{h_3}(x)) = -d^2 b$ 是一个非零常数。由于行列式处处非零，可观测性矩阵 $\\mathcal{O}_{h_3}(x)$ 对于所有状态变量 $x$ 和参数的值都具有满秩（$3$）。\n\n这一结果表明，仅测量组织损伤 $x_3$ 就足以使系统结构上（在本例中也是全局上）可观测。由于单个传感器即已足够，因此不可能使用更少的传感器。所以，所需的最小标量输出通道数为 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "一旦我们确定了系统是可观测的，下一步就是从实验数据中构建和校准模型。现代生物学的一个强大之处在于能够进行靶向扰动，例如CRISPR基因编辑技术。本练习 () 模拟了这一过程，要求你利用合成的Perturb-seq（CRISPR扰动结合单细胞测序）数据来校准一个信号网络的因果模型。这个实践不仅能让你掌握如何从干预性数据中推断因果关系，还能让你测试校准后的数字孪生对未见过的组合干预的预测能力。",
            "id": "3301917",
            "problem": "您的任务是校准一个细胞信号网络的简化数字孪生，此过程使用汇集的成簇规律间隔短回文重复序列（CRISPR）扰动与单细胞RNA测序（Perturb-seq）的干预手段，然后评估其对未见干预组合的外推能力。该数字孪生由一个信号网络的常微分方程的线性化稳态近似构建而成。该网络有 $n = 4$ 个基因活性节点，并由 $m = 2$ 个外源输入通道驱动。线性化稳态结构因果模型为\n$$\n\\mathbf{x} = W \\mathbf{x} + S \\mathbf{u},\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^{4}$ 是基因 $\\{1,2,3,4\\}$ 的稳态活性向量，$\\mathbf{u} \\in \\mathbb{R}^{2}$ 是输入刺激向量，$W \\in \\mathbb{R}^{4 \\times 4}$ 是有向相互作用矩阵（对角线元素为零），$S \\in \\mathbb{R}^{4 \\times 2}$ 将输入映射到基因节点。稳定性要求 $W$ 的谱半径小于 $1$，以确保在没有干预的情况下 $(I - W)$ 是可逆的。\n\n干预是完美的基因敲除，形式化为 Pearl 的 do 算子。对于一个敲除集合 $K \\subset \\{1,2,3,4\\}$，受干预的系统将 $K$ 中节点的结构方程替换为将这些变量钳制为零。对于未受干预的节点，其稳态满足\n$$\n\\mathbf{x}_{R} = W_{R,R}\\mathbf{x}_{R} + S_{R} \\mathbf{u},\n$$\n其中 $R = \\{1,2,3,4\\} \\setminus K$，$W_{R,R}$ 是 $W$ 限制在 $R$ 中行和列的子矩阵，$S_{R}$ 是 $S$ 限制在 $R$ 中行的子矩阵。求解可得\n$$\n\\mathbf{x}_{R} = (I_{|R|} - W_{R,R})^{-1} S_{R} \\mathbf{u},\n$$\n且 $\\mathbf{x}_{K} = \\mathbf{0}$。\n\n用于生成观测数据的合成孪生的真实参数为\n$$\nW^\\star =\n\\begin{bmatrix}\n0  0.15  0  0.10 \\\\\n0.05  0  0.20  0 \\\\\n0.10  0  0  0.05 \\\\\n0  0.10  0.10  0\n\\end{bmatrix}, \\quad\nS^\\star =\n\\begin{bmatrix}\n1.0  0.0 \\\\\n0.2  0.5 \\\\\n0.0  1.0 \\\\\n0.3  0.0\n\\end{bmatrix}.\n$$\n\n训练实验：对于\n$$\n\\mathcal{U}_{\\text{train}} = \\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} \\right\\}\n$$\n中的每个输入向量，收集以下干预集合的稳态观测值：\n$$\n\\mathcal{K}_{\\text{train}} = \\left\\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\right\\}.\n$$\n也就是说，对于每个 $\\mathbf{u} \\in \\mathcal{U}_{\\text{train}}$，在野生型（无敲除）和每种单基因敲除情况下测量 $\\mathbf{x}$。测得的训练输出由上述真实模型生成，然后被标准差为 $\\sigma = 10^{-3}$ 的独立零均值高斯测量噪声所干扰。为保证可复现性，请使用固定的随机种子 $0$。在校准过程中，构建回归特征时，无论这些通道中的测量噪声如何，在设计矩阵中都将被敲除的变量视为精确为零。\n\n校准目标：使用训练数据，通过最小化所有适用方程的总平方误差来估计 $\\widehat{W}$ 和 $\\widehat{S}$。对于每个节点 $i \\in \\{1,2,3,4\\}$，当节点 $i$ 在给定的训练实验中未被敲除时，强制执行以下线性关系\n$$\nx_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{4} W_{i j} x_j + \\sum_{\\ell=1}^{2} S_{i \\ell} u_\\ell,\n$$\n其中对于在该实验中被敲除的任何 $j$，均有 $x_j = 0$。不要使用节点 $i$ 被敲除的实验中的方程。对所有 $i$ 施加 $W_{i i} = 0$。\n\n测试套件：校准后，通过预测三个未见过的干预和输入的稳态来评估校准后的孪生。对于每种情况，通过使用 $\\widehat{W}$ 和 $\\widehat{S}$ 求解干预后的系统来计算预测的 $\\widehat{\\mathbf{x}}$，如上所述，并与使用 $W^\\star$ 和 $S^\\star$ 计算的相应无噪声真实值 $\\mathbf{x}^\\star$进行比较。\n\n- 案例 A（未见的双基因敲除）：$K = \\{1,3\\}$, $\\mathbf{u} = \\begin{bmatrix} 0.7 \\\\ 0.1 \\end{bmatrix}$。\n- 案例 B（未见的输入，无敲除）：$K = \\emptyset$, $\\mathbf{u} = \\begin{bmatrix} 0.2 \\\\ 0.9 \\end{bmatrix}$。\n- 案例 C（未见的三重组合）：$K = \\{2,4\\}$, $\\mathbf{u} = \\begin{bmatrix} 0.4 \\\\ 0.4 \\end{bmatrix}$。\n\n对于每种情况，报告均方根误差\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} \\left( \\widehat{x}_i - x_i^\\star \\right)^2 }.\n$$\n\n您的程序必须按规定实现校准和评估，仅使用提供的训练和测试配置以及固定的噪声种子 $0$。最终输出必须是单行，包含案例 A、B 和 C 的三个 RMSE 值，按此顺序排列，形式为逗号分隔的列表，并用方括号括起来，例如 $\\left[ r_A, r_B, r_C \\right]$，其中每个 $r_\\cdot$ 是一个浮点数。本问题不涉及物理单位。不涉及角度。不使用百分比。输出必须严格为指定格式的单行，不含任何附加文本。程序必须是完整、可运行的程序。",
            "solution": "该问题要求校准一个细胞信号网络的简化数字孪生，并评估其预测能力。该过程包括三个主要阶段：生成合成训练数据，使用这些数据校准模型参数，以及在未见的测试案例上评估校准后的模型。\n\n数字孪生由一个线性化稳态结构因果模型表示：\n$$\n\\mathbf{x} = W \\mathbf{x} + S \\mathbf{u}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^{4}$ 是基因活性向量，$\\mathbf{u} \\in \\mathbb{R}^{2}$ 是外部刺激向量，$W \\in \\mathbb{R}^{4 \\times 4}$ 是基因-基因相互作用矩阵，对于所有 $i \\in \\{1,2,3,4\\}$ 都有 $W_{ii} = 0$，$S \\in \\mathbb{R}^{4 \\times 2}$ 是将输入映射到基因的矩阵。为了保证稳定性，$W$ 的谱半径必须小于 $1$。\n\n干预通过 Pearl 的 `do` 算子被建模为完美的基因敲除。对于一组被敲除的基因 $K \\subset \\{1,2,3,4\\}$，这些基因的活性被钳制为零，即 $\\mathbf{x}_{K} = \\mathbf{0}$。剩余基因（由集合 $R = \\{1,2,3,4\\} \\setminus K$ 索引）的活性通过求解简化的系统来确定：\n$$\n\\mathbf{x}_{R} = W_{R,R}\\mathbf{x}_{R} + S_{R} \\mathbf{u}\n$$\n其中 $W_{R,R}$ 和 $S_{R}$ 是对应于 $R$ 中基因的行和列的子矩阵。这给出了解决方案：\n$$\n\\mathbf{x}_{R} = (I_{|R|} - W_{R,R})^{-1} S_{R} \\mathbf{u}\n$$\n其中 $I_{|R|}$ 是大小为 $|R|$ 的单位矩阵。\n\n**第一部分：训练数据生成**\n\n首先，我们生成一个用于训练的合成数据集。该数据集源自真实模型参数：\n$$\nW^\\star =\n\\begin{bmatrix}\n0  0.15  0  0.10 \\\\\n0.05  0  0.20  0 \\\\\n0.10  0  0  0.05 \\\\\n0  0.10  0.10  0\n\\end{bmatrix}, \\quad\nS^\\star =\n\\begin{bmatrix}\n1.0  0.0 \\\\\n0.2  0.5 \\\\\n0.0  1.0 \\\\\n0.3  0.0\n\\end{bmatrix}\n$$\n训练实验包括来自 $\\mathcal{U}_{\\text{train}}$ 的所有输入向量和来自 $\\mathcal{K}_{\\text{train}}$ 的所有干预集的组合：\n$$\n\\mathcal{U}_{\\text{train}} = \\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} \\right\\}, \\quad\n\\mathcal{K}_{\\text{train}} = \\left\\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\right\\}\n$$\n这产生了 $3 \\times 5 = 15$ 个独特的实验。对于每一对 $(\\mathbf{u}, K)$，我们使用带有 $W^\\star$ 和 $S^\\star$ 的干预求解方程来计算无噪声的稳态向量 $\\mathbf{x}^\\star$。然后，为了模拟测量误差，我们向 $\\mathbf{x}^\\star$ 的每个分量添加标准差为 $\\sigma = 10^{-3}$ 的独立高斯噪声 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$。得到的噪声向量 $\\mathbf{x}_{\\text{obs}} = \\mathbf{x}^\\star + \\boldsymbol{\\epsilon}$ 构成了我们训练集中的一个观测值。为了保证噪声生成的可复现性，使用固定的随机种子 $0$。\n\n**第二部分：模型校准**\n\n校准的目标是根据生成的训练数据来估计模型参数 $\\widehat{W}$ 和 $\\widehat{S}$。该估计是通过最小化所有适用结构方程上的总平方误差来进行的。由于方程是解耦的，我们可以为每个基因 $i \\in \\{1,2,3,4\\}$ 分别执行线性回归。\n\n对于每个基因 $i$，我们拟合线性模型：\n$$\nx_i = \\sum_{j=1, j \\neq i}^{4} W_{i j} x_j + \\sum_{\\ell=1}^{2} S_{i \\ell} u_\\ell\n$$\n对于基因 $i$ 需要估计的参数是 $W$ 的第 $i$ 行的 $3$ 个非对角线元素和 $S$ 的第 $i$ 行的 $2$ 个元素，形成一个参数向量 $\\boldsymbol{\\theta}_i \\in \\mathbb{R}^5$。\n\n此回归的数据来源于所有基因 $i$ *未*被敲除的训练实验。对于每个这样的实验 $(\\mathbf{u}^{(k)}, K^{(k)}, \\mathbf{x}_{\\text{obs}}^{(k)})$，其中 $i \\notin K^{(k)}$，我们形成一个数据点。回归目标是基因 $i$ 的观测活性，$y_k = (\\mathbf{x}_{\\text{obs}}^{(k)})_i$。特征向量 $\\boldsymbol{\\phi}_k$ 由其他基因的活性和输入构成。根据规定，在特征向量中，任何被敲除基因 $j \\in K^{(k)}$ 的活性都被视为精确为 $0$，无论由于噪声其测量值是否非零。因此，我们构建一个特定于特征的状态向量 $\\mathbf{x}_{\\text{feat}}^{(k)}$，其中如果 $j \\notin K^{(k)}$，则 $(\\mathbf{x}_{\\text{feat}}^{(k)})_j = (\\mathbf{x}_{\\text{obs}}^{(k)})_j$，否则为 $0$。然后，特征向量为 $\\boldsymbol{\\phi}_k = [(\\mathbf{x}_{\\text{feat}}^{(k)})_{j \\neq i}, \\mathbf{u}^{(k)}]$。\n\n对于每个基因 $i$，我们收集所有这样的目标-特征对，以形成一个矩阵方程 $Y_i = \\Phi_i \\boldsymbol{\\theta}_i + \\text{误差}$。参数向量 $\\widehat{\\boldsymbol{\\theta}}_i$ 使用普通最小二乘法进行估计：\n$$\n\\widehat{\\boldsymbol{\\theta}}_i = (\\Phi_i^T \\Phi_i)^{-1} \\Phi_i^T Y_i\n$$\n然后，通过将 $\\widehat{\\boldsymbol{\\theta}}_i$ 中的相应元素填充到估计矩阵 $\\widehat{W}$ 和 $\\widehat{S}$ 的第 $i$ 行来组装它们，并确保 $\\widehat{W}_{ii} = 0$。\n\n**第三部分：在未见测试案例上的评估**\n\n由 $(\\widehat{W}, \\widehat{S})$ 定义的校准数字孪生在三个未见的测试案例上进行评估：\n- 案例 A：双基因敲除 $K = \\{1,3\\}$，输入为 $\\mathbf{u} = [0.7, 0.1]^T$。\n- 案例 B：未见过的输入 $\\mathbf{u} = [0.2, 0.9]^T$，无敲除，$K = \\emptyset$。\n- 案例 C：双基因敲除 $K = \\{2,4\\}$，输入为 $\\mathbf{u} = [0.4, 0.4]^T$。\n\n对于每个测试案例，我们执行两次计算：\n1.  使用校准参数 $(\\widehat{W}, \\widehat{S})$ 求解干预后的系统，计算预测状态 $\\widehat{\\mathbf{x}}$。\n2.  使用真实参数 $(W^\\star, S^\\star)$ 求解相同的系统，计算真实状态 $\\mathbf{x}^\\star$，代表无噪声的现实情况。\n\n性能通过预测值与真实值之间的均方根误差（RMSE）进行量化：\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} \\left( \\widehat{x}_i - x_i^\\star \\right)^2 }\n$$\n最终输出包括为案例 A、B 和 C 计算的三个 RMSE 值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calibrates and evaluates a digital twin of a cell signaling network.\n    The process involves:\n    1. Generating noisy training data from a ground-truth model.\n    2. Calibrating the model parameters (W, S) using least-squares regression.\n    3. Evaluating the calibrated model's predictions on unseen test cases against the noise-free ground truth.\n    \"\"\"\n    \n    # --- Ground-truth parameters and constants ---\n    W_star = np.array([\n        [0.0, 0.15, 0.0, 0.10],\n        [0.05, 0.0, 0.20, 0.0],\n        [0.10, 0.0, 0.0, 0.05],\n        [0.0, 0.10, 0.10, 0.0]\n    ])\n    \n    S_star = np.array([\n        [1.0, 0.0],\n        [0.2, 0.5],\n        [0.0, 1.0],\n        [0.3, 0.0]\n    ])\n    \n    sigma = 1e-3\n    num_genes = 4\n    num_inputs = 2\n    \n    # --- Training and test configurations ---\n    u_train = [\n        np.array([1.0, 0.0]),\n        np.array([0.0, 1.0]),\n        np.array([0.5, 0.5])\n    ]\n    # Intervention sets (1-based), will be converted to 0-based sets.\n    k_train_tuples = [(), (1,), (2,), (3,), (4,)]\n    \n    test_cases = [\n        {'K': {1, 3}, 'u': np.array([0.7, 0.1])},  # Case A\n        {'K': set(),  'u': np.array([0.2, 0.9])},  # Case B\n        {'K': {2, 4}, 'u': np.array([0.4, 0.4])}   # Case C\n    ]\n\n    def solve_intervened_system(W, S, u, K_0based):\n        \"\"\"\n        Solves for the steady-state vector x for a given intervened system.\n        \"\"\"\n        all_nodes = set(range(num_genes))\n        R_0based = sorted(list(all_nodes - K_0based))\n        \n        if not R_0based:\n            return np.zeros(num_genes)\n        \n        R_indices = np.array(R_0based)\n        W_RR = W[np.ix_(R_indices, R_indices)]\n        S_R = S[R_indices, :]\n        \n        I_R = np.identity(len(R_0based))\n        \n        try:\n            # Solve (I_R - W_RR) * x_R = S_R * u for x_R\n            x_R = np.linalg.solve(I_R - W_RR, S_R @ u)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, though unlikely with stable W\n            return np.full(num_genes, np.nan)\n        \n        x = np.zeros(num_genes)\n        x[R_indices] = x_R\n        return x\n\n    # --- 1. Generate training data ---\n    rng = np.random.default_rng(0)\n    training_data = []\n    for u in u_train:\n        for k_tuple in k_train_tuples:\n            K_0based = {k - 1 for k in k_tuple}\n            \n            x_star = solve_intervened_system(W_star, S_star, u, K_0based)\n            noise = rng.normal(loc=0.0, scale=sigma, size=num_genes)\n            x_obs = x_star + noise\n            training_data.append({'u': u, 'K': K_0based, 'x_obs': x_obs})\n            \n    # --- 2. Calibrate model (estimate W_hat, S_hat) ---\n    W_hat = np.zeros((num_genes, num_genes))\n    S_hat = np.zeros((num_genes, num_inputs))\n    \n    for i in range(num_genes): # For each gene i (0 to 3)\n        Phi_list = []\n        Y_list = []\n        \n        for exp in training_data:\n            if i not in exp['K']: # Use experiments where gene i is not knocked out\n                # Target is the observed value of gene i\n                Y_list.append(exp['x_obs'][i])\n                \n                # Feature vector construction\n                x_feat = exp['x_obs'].copy()\n                for k_node in exp['K']:\n                    x_feat[k_node] = 0.0 # Enforce knockout to be exactly zero\n                \n                x_features = np.delete(x_feat, i)\n                u_features = exp['u']\n                phi = np.concatenate((x_features, u_features))\n                Phi_list.append(phi)\n                \n        Phi = np.array(Phi_list)\n        Y = np.array(Y_list)\n        \n        # Solve the least squares problem: Phi * theta = Y\n        theta, _, _, _ = np.linalg.lstsq(Phi, Y, rcond=None)\n        \n        # Unpack theta into W_hat and S_hat rows\n        w_row_hat_flat = theta[:num_genes - 1]\n        s_row_hat = theta[num_genes - 1:]\n        \n        W_hat[i, :] = np.insert(w_row_hat_flat, i, 0.0)\n        S_hat[i, :] = s_row_hat\n\n    # --- 3. Evaluate on test suite ---\n    rmse_results = []\n    for case in test_cases:\n        K_test_1based = case['K']\n        u_test = case['u']\n        \n        K_test_0based = {k - 1 for k in K_test_1based}\n        \n        # Prediction from calibrated model\n        x_hat = solve_intervened_system(W_hat, S_hat, u_test, K_test_0based)\n        \n        # Ground truth (noise-free)\n        x_star_test = solve_intervened_system(W_star, S_star, u_test, K_test_0based)\n        \n        # Compute RMSE\n        rmse = np.sqrt(np.mean((x_hat - x_star_test)**2))\n        rmse_results.append(rmse)\n        \n    # --- Final Output ---\n    print(f\"[{','.join(f'{r:.10f}' for r in rmse_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "数字孪生的最终价值在于它不仅仅是一个被动的模型，更是一个主动的科学发现工具。一个成熟的数字孪生应该能够指导未来的实验，使其信息收益最大化。本练习 () 深入探讨了贝叶斯实验设计，其目标是选择一个能最有效地区分不同生物学机制假说的实验条件。这体现了自适应数字孪生的核心功能——通过主动学习来不断完善自身对生物系统的理解。",
            "id": "3301904",
            "problem": "生物学中的数字孪生 (DTB) 是一个生物系统的计算表示，该系统通过数据持续更新。考虑一个 DTB，其任务是通过选择一个实验刺激浓度 $u$（单位：纳摩尔，nM）来区分两种竞争性的机理通路假说 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$，以在 $\\mathcal{M}_1$ 是数据生成模型的情况下，最大化支持 $\\mathcal{M}_1$ 的贝叶斯因子（Bayes factor (BF)）的期望对数。在模型 $\\mathcal{M}_i$ 下，可观测值 $y$（无量纲，例如，归一化报告基因强度）遵循一个具有已知方差的高斯噪声模型和一个参数线性的单参数机理映射。具体来说，设机理映射为 $f_i(u,\\theta_i) = \\theta_i \\, g(u)$，其中 $g(u)$ 是一个固定的、已知的灵敏度函数，表征通路在刺激 $u$ 下的输入输出映射，而 $\\theta_i$ 是一个特定于模型的标量参数。假设高斯先验 $\\theta_i \\sim \\mathcal{N}(\\mu_{0,i}, \\tau_i^2)$ 和条件高斯测量模型 $y \\mid \\theta_i, \\mathcal{M}_i \\sim \\mathcal{N}(f_i(u,\\theta_i), \\sigma^2)$，其中测量方差 $\\sigma^2$ 已知。DTB 必须为每个指定的测试用例计算实验刺激 $u^\\star$，该值最大化期望对数贝叶斯因子 $\\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$，其中 $\\mathrm{BF}_{1,2}(u) = \\dfrac{p(y \\mid \\mathcal{M}_1)}{p(y \\mid \\mathcal{M}_2)}$ 且 $p(y \\mid \\mathcal{M}_i)$ 表示 $\\mathcal{M}_i$ 下的模型证据（边际似然）。\n\n您的程序必须：\n- 从基本的贝叶斯原理出发，推导如何将 $\\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$ 计算为 $u$、$\\sigma^2$、$\\mu_{0,i}$、$\\tau_i^2$ 和 $g(u)$ 的函数，从高斯先验、高斯似然和边际化的定义开始。\n- 使用指定的网格分辨率 $\\Delta u$，在闭区间 $[0, u_{\\max}]$ 上进行搜索，以找到 $u^\\star = \\arg\\max_{u \\in \\{0, \\Delta u, 2 \\Delta u, \\ldots, u_{\\max}\\}} \\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$。如果多个 $u$ 值达到相同的最大值，则选择其中最小的 $u$。\n- 输出 $u^\\star$，单位为纳摩尔 (nM)，四舍五入到三位小数。\n\n使用以下测试套件，它改变了灵敏度函数 $g(u)$ 和概率参数，以探测一般情况、边界情况和边缘情况。对于每种情况，灵敏度函数 $g(u)$ 要么是线性的 $g(u) = u$，要么是 Michaelis–Menten 型饱和形式 $g(u) = \\dfrac{u}{K + u}$，其中半饱和常数 $K$ 的单位为 nM。以下所有量和常数均使用其规定单位，所有数值常数均为实值。\n\n- 测试用例 1（一般情况，线性灵敏度）：\n  - $\\sigma^2 = 1.0$, $\\mu_{0,1} = 1.0$, $\\tau_1^2 = 0.5$, $\\mu_{0,2} = 0.2$, $\\tau_2^2 = 0.5$。\n  - $g(u) = u$。\n  - $u_{\\max} = 10.0$ nM, $\\Delta u = 0.001$ nM。\n\n- 测试用例 2（边界情况，先验和均值无法区分）：\n  - $\\sigma^2 = 1.0$, $\\mu_{0,1} = 0.0$, $\\tau_1^2 = 0.5$, $\\mu_{0,2} = 0.0$, $\\tau_2^2 = 0.5$。\n  - $g(u) = u$。\n  - $u_{\\max} = 10.0$ nM, $\\Delta u = 0.001$ nM。\n\n- 测试用例 3（边缘情况，饱和灵敏度，先验信息量不同）：\n  - $\\sigma^2 = 0.5$, $\\mu_{0,1} = 2.0$, $\\tau_1^2 = 0.01$, $\\mu_{0,2} = 1.0$, $\\tau_2^2 = 1.0$。\n  - $g(u) = \\dfrac{u}{K + u}$，其中 $K = 1.0$ nM。\n  - $u_{\\max} = 100.0$ nM, $\\Delta u = 0.1$ nM。\n\n- 测试用例 4（仅通过方差区分，均值相等，先验方差不同）：\n  - $\\sigma^2 = 0.5$, $\\mu_{0,1} = 0.0$, $\\tau_1^2 = 0.01$, $\\mu_{0,2} = 0.0$, $\\tau_2^2 = 1.0$。\n  - $g(u) = u$。\n  - $u_{\\max} = 20.0$ nM, $\\Delta u = 0.01$ nM。\n\n您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果，按上述测试用例的顺序排列。每个条目必须是一个浮点数，给出相应情况下的 $u^\\star$（单位 nM，四舍五入到三位小数），例如 $\\left[\\text{case}_1,\\text{case}_2,\\text{case}_3,\\text{case}_4\\right]$，其中每个 $\\text{case}_i$ 是为相应情况计算出的最优刺激。",
            "solution": "此问题有效。它在科学上基于贝叶斯统计和最优实验设计，问题设定良好，有明确的目标和约束，并使用精确、客观的语言进行表述。\n\n任务是找到刺激 $u^\\star$，以最大化模型 $\\mathcal{M}_1$ 相对于 $\\mathcal{M}_2$ 的期望对数贝叶斯因子，其中期望是基于由模型 $\\mathcal{M}_1$ 生成的数据计算的。该目标函数等价于两个模型的预测分布之间的 Kullback-Leibler (KL) 散度。\n\n$u^\\star = \\arg\\max_{u} \\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\frac{p(y \\mid u, \\mathcal{M}_1)}{p(y \\mid u, \\mathcal{M}_2)}\\right] = \\arg\\max_{u} D_{KL}\\left(p(y \\mid u, \\mathcal{M}_1) \\Vert p(y \\mid u, \\mathcal{M}_2)\\right)$\n\n首先，我们必须为每个模型 $\\mathcal{M}_i$ 推导出边际似然（或称模型证据）$p(y \\mid u, \\mathcal{M}_i)$。该模型由参数 $\\theta_i$ 的高斯先验和观测值 $y$ 的高斯似然定义。\n\n对于模型 $\\mathcal{M}_i$：\n1.  **先验**：参数 $\\theta_i$ 具有高斯先验分布：\n    $$p(\\theta_i \\mid \\mathcal{M}_i) = \\mathcal{N}(\\theta_i; \\mu_{0,i}, \\tau_i^2)$$\n    其中 $\\mu_{0,i}$ 是先验均值，$\\tau_i^2$ 是先验方差。\n\n2.  **似然**：给定 $\\theta_i$ 和刺激 $u$，可观测值 $y$ 遵循高斯分布：\n    $$p(y \\mid \\theta_i, u, \\mathcal{M}_i) = \\mathcal{N}(y; f_i(u, \\theta_i), \\sigma^2)$$\n    其中均值是机理映射 $f_i(u, \\theta_i) = \\theta_i g(u)$，$\\sigma^2$ 是已知的测量方差。\n\n边际似然 $p(y \\mid u, \\mathcal{M}_i)$ 是通过对参数 $\\theta_i$ 进行积分得到的：\n$$p(y \\mid u, \\mathcal{M}_i) = \\int p(y \\mid \\theta_i, u, \\mathcal{M}_i) p(\\theta_i \\mid \\mathcal{M}_i) d\\theta_i$$\n这是一个标准的高斯卷积，其结果是 $y$ 的一个高斯分布。我们来确定它的均值和方差。\n\n边际预测分布的均值为：\n$$\\mu_{y,i} = \\mathbb{E}[y \\mid u, \\mathcal{M}_i] = \\mathbb{E}_{\\theta_i \\sim p(\\theta_i \\mid \\mathcal{M}_i)}[\\mathbb{E}[y \\mid \\theta_i, u, \\mathcal{M}_i]]$$\n$$\\mu_{y,i} = \\mathbb{E}_{\\theta_i}[\\theta_i g(u)] = g(u) \\mathbb{E}_{\\theta_i}[\\theta_i] = \\mu_{0,i} g(u)$$\n\n边际预测分布的方差可使用全方差公式求得：\n$$\\sigma_{y,i}^2 = \\text{Var}(y \\mid u, \\mathcal{M}_i) = \\mathbb{E}_{\\theta_i}[\\text{Var}(y \\mid \\theta_i, u, \\mathcal{M}_i)] + \\text{Var}_{\\theta_i}(\\mathbb{E}[y \\mid \\theta_i, u, \\mathcal{M}_i])$$\n$$\\sigma_{y,i}^2 = \\mathbb{E}_{\\theta_i}[\\sigma^2] + \\text{Var}_{\\theta_i}(\\theta_i g(u))$$\n$$\\sigma_{y,i}^2 = \\sigma^2 + g(u)^2 \\text{Var}_{\\theta_i}(\\theta_i) = \\sigma^2 + g(u)^2 \\tau_i^2$$\n\n因此，模型 $\\mathcal{M}_i$ 的边际似然是一个高斯分布：\n$$p(y \\mid u, \\mathcal{M}_i) = \\mathcal{N}(y; \\mu_{y,i}, \\sigma_{y,i}^2) = \\mathcal{N}(y; \\mu_{0,i} g(u), \\sigma^2 + \\tau_i^2 g(u)^2)$$\n\n现在我们可以写出目标函数，即两个高斯预测分布 $p(y \\mid u, \\mathcal{M}_1)$ 和 $p(y \\mid u, \\mathcal{M}_2)$ 之间的 KL 散度。令 $p_1 = \\mathcal{N}(\\mu_{y,1}, \\sigma_{y,1}^2)$ 和 $p_2 = \\mathcal{N}(\\mu_{y,2}, \\sigma_{y,2}^2)$。KL 散度由以下公式给出：\n$$D_{KL}(p_1 \\Vert p_2) = \\log\\frac{\\sigma_{y,2}}{\\sigma_{y,1}} + \\frac{\\sigma_{y,1}^2 + (\\mu_{y,1} - \\mu_{y,2})^2}{2\\sigma_{y,2}^2} - \\frac{1}{2}$$\n代入均值和方差的表达式：\n- $\\mu_{y,1} = \\mu_{0,1} g(u)$\n- $\\sigma_{y,1}^2 = \\sigma^2 + \\tau_1^2 g(u)^2$\n- $\\mu_{y,2} = \\mu_{0,2} g(u)$\n- $\\sigma_{y,2}^2 = \\sigma^2 + \\tau_2^2 g(u)^2$\n\n需要最大化的目标函数 $F(u)$ 变为：\n$$F(u) = \\frac{1}{2} \\log\\left(\\frac{\\sigma^2 + \\tau_2^2 g(u)^2}{\\sigma^2 + \\tau_1^2 g(u)^2}\\right) + \\frac{(\\sigma^2 + \\tau_1^2 g(u)^2) + (\\mu_{0,1} g(u) - \\mu_{0,2} g(u))^2}{2(\\sigma^2 + \\tau_2^2 g(u)^2)} - \\frac{1}{2}$$\n简化第二项：\n$$F(u) = \\frac{1}{2} \\log\\left(\\frac{\\sigma^2 + \\tau_2^2 g(u)^2}{\\sigma^2 + \\tau_1^2 g(u)^2}\\right) + \\frac{\\sigma^2 + (\\tau_1^2 + (\\mu_{0,1} - \\mu_{0,2})^2) g(u)^2}{2(\\sigma^2 + \\tau_2^2 g(u)^2)} - \\frac{1}{2}$$\n\n这就是期望对数贝叶斯因子的最终解析表达式。问题要求在指定的离散集合中找到使该函数 $F(u)$ 最大化的 $u$ 值。\n\n求解策略如下：\n1.  对于每个测试用例，定义参数 $\\sigma^2, \\mu_{0,1}, \\tau_1^2, \\mu_{0,2}, \\tau_2^2$ 和函数 $g(u)$。\n2.  创建一个从 $0$ 到 $u_{\\max}$，步长为 $\\Delta u$ 的离散刺激值网格 $u$。\n3.  对于网格中的每个 $u$ 值，使用推导出的公式计算目标函数 $F(u)$ 的值。\n4.  找到 $F(u)$ 在网格上的最大值。\n5.  最优刺激 $u^\\star$ 是对应于此最大值的 $u$ 值。如果多个 $u$ 值产生相同的最大值，则按问题陈述选择最小的 $u$。这通过找到第一个最大值的索引来自然处理。\n6.  每个用例的最终结果四舍五入到三位小数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal experimental stimulus u* for model discrimination\n    based on maximizing the expected log Bayes factor.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {'sigma2': 1.0, 'mu01': 1.0, 'tau1_2': 0.5, 'mu02': 0.2, 'tau2_2': 0.5,\n         'g_type': 'linear', 'K': None, 'u_max': 10.0, 'delta_u': 0.001},\n        # Test Case 2\n        {'sigma2': 1.0, 'mu01': 0.0, 'tau1_2': 0.5, 'mu02': 0.0, 'tau2_2': 0.5,\n         'g_type': 'linear', 'K': None, 'u_max': 10.0, 'delta_u': 0.001},\n        # Test Case 3\n        {'sigma2': 0.5, 'mu01': 2.0, 'tau1_2': 0.01, 'mu02': 1.0, 'tau2_2': 1.0,\n         'g_type': 'saturating', 'K': 1.0, 'u_max': 100.0, 'delta_u': 0.1},\n        # Test Case 4\n        {'sigma2': 0.5, 'mu01': 0.0, 'tau1_2': 0.01, 'mu02': 0.0, 'tau2_2': 1.0,\n         'g_type': 'linear', 'K': None, 'u_max': 20.0, 'delta_u': 0.01},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        sigma2 = case['sigma2']\n        mu01 = case['mu01']\n        tau1_2 = case['tau1_2']\n        mu02 = case['mu02']\n        tau2_2 = case['tau2_2']\n        u_max = case['u_max']\n        delta_u = case['delta_u']\n\n        if case['g_type'] == 'linear':\n            # Sensitivity function g(u) = u\n            g_func = lambda u: u\n        else:  # 'saturating'\n            # Sensitivity function g(u) = u / (K + u)\n            K = case['K']\n            g_func = lambda u: u / (K + u)\n\n        # Create the grid of stimulus values u.\n        # Use np.linspace for robust handling of floating-point steps.\n        num_points = int(np.round(u_max / delta_u)) + 1\n        u_grid = np.linspace(0.0, u_max, num_points)\n\n        # Evaluate g(u) over the grid.\n        g_vals = g_func(u_grid)\n        g_vals_sq = g_vals**2\n\n        # Calculate the components of the objective function (KL divergence)\n        # in a vectorized manner.\n\n        # Variances of the predictive distributions\n        sigma_y1_sq = sigma2 + tau1_2 * g_vals_sq\n        sigma_y2_sq = sigma2 + tau2_2 * g_vals_sq\n\n        # The objective function F(u) is the KL-divergence D_KL( p(y|M1) || p(y|M2) )\n        \n        # Term 1: 0.5 * log(sigma_y2^2 / sigma_y1^2)\n        log_term = 0.5 * np.log(sigma_y2_sq / sigma_y1_sq)\n\n        # Term 2: (sigma_y1^2 + (mu_y1-mu_y2)^2) / (2*sigma_y2^2)\n        mu_diff_sq = (mu01 - mu02)**2\n        frac_numerator = sigma2 + (tau1_2 + mu_diff_sq) * g_vals_sq\n        frac_term = frac_numerator / (2.0 * sigma_y2_sq)\n\n        # Full objective function\n        objective_values = log_term + frac_term - 0.5\n\n        # Find the index of the maximum value.\n        # np.argmax returns the index of the first occurrence of the maximum,\n        # which satisfies the tie-breaking rule (choose the smallest u).\n        best_idx = np.argmax(objective_values)\n        u_star = u_grid[best_idx]\n\n        results.append(f\"{u_star:.3f}\")\n\n    # Print results in the required format: [case1,case2,case3,case4]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}