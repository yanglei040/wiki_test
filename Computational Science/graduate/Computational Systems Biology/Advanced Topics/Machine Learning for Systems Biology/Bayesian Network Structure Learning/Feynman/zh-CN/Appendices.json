{
    "hands_on_practices": [
        {
            "introduction": "基于分数的学习方法通过搜索具有最优分数的网络结构来推断贝叶斯网络。这个过程的核心是为每个候选结构计算一个分数。本练习将深入探讨这一过程的基石：计算单个潜在调控连边的分数变化。我们将使用贝叶斯狄利克雷等价均匀（BDeu）分数，这是该领域的一个常用选择。通过这个实践，你将具体理解观测到的数据计数如何转化为支持或反对一条边的证据，这对于理解贪婪搜索等结构学习算法至关重要。",
            "id": "3289730",
            "problem": "在计算系统生物学中，贝叶斯网络结构学习常被用于从离散化的表达谱中推断基因调控相互作用。考虑一个候选的调控-靶标对，其中潜在子节点 $Y$ 代表一个靶基因，具有 $r_{Y} = 3$ 个离散表达状态 $\\{ \\text{low}, \\text{baseline}, \\text{high} \\}$；潜在父节点 $X$ 代表一个转录调控因子，具有 $r_{X} = 2$ 个离散活动状态 $\\{ 0, 1 \\}$。给定 $N = 30$ 个独立的匹配基因活动谱样本，其中 $Y$ 和 $X$ 已按上述方式离散化。\n\n当忽略 $X$ 时（即 $Y$ 没有父节点），观测到 $Y$ 的计数如下：\n- $n_{Y=\\text{low}} = 14$,\n- $n_{Y=\\text{baseline}} = 9$,\n- $n_{Y=\\text{high}} = 7$。\n\n当 $X$ 被视为 $Y$ 的父节点时，观测到的 $Y$ 在给定 $X$ 条件下的列联表如下：\n- 对于 $X = 0$：$(n_{Y=\\text{low} \\mid X=0}, n_{Y=\\text{baseline} \\mid X=0}, n_{Y=\\text{high} \\mid X=0}) = (10, 5, 3)$，\n- 对于 $X = 1$：$(n_{Y=\\text{low} \\mid X=1}, n_{Y=\\text{baseline} \\mid X=1}, n_{Y=\\text{high} \\mid X=1}) = (4, 4, 4)$。\n\n假设 $Y$ 的条件概率表服从一个贝叶斯狄利克雷等价均匀 (BDeu) 先验，该先验在不同状态和父节点配置上是均匀的，并由一个等效样本量 $\\alpha > 0$ 参数化。使用狄利克雷-多项式共轭模型的性质以及贝叶斯网络边际似然的模块化可分解性。\n\n将为节点 $Y$ 添加父节点 $X$ 引起的 BDeu 局部分数变化量 $\\Delta S_{\\text{BDeu}}(Y; X)$ 定义为：节点 $Y$ 在以 $X$ 为父节点时的数据的对数边际似然，与节点 $Y$ 在没有父节点时的数据的对数边际似然之差：\n$$\n\\Delta S_{\\text{BDeu}}(Y; X) \\equiv \\ln p(\\text{data at } Y \\mid \\text{$X$ is a parent of $Y$}, \\alpha) \\;-\\; \\ln p(\\text{data at } Y \\mid \\text{$Y$ has no parents}, \\alpha).\n$$\n\n仅根据上述提供的计数，使用自然对数和伽马函数，计算 $\\Delta S_{\\text{BDeu}}(Y; X)$ 作为关于 $\\alpha$ 的闭式解析表达式。将最终答案表示为关于 $\\alpha$ 的单个解析表达式。不要进行数值近似。",
            "solution": "该问题要求计算当从一个潜在父节点 $X$ 到一个靶节点 $Y$ 添加一条边时，BDeu 局部分数的变化量 $\\Delta S_{\\text{BDeu}}(Y; X)$。该量定义为包含该边的模型 ($M_1$) 和不包含该边的模型 ($M_0$)下，节点 $Y$ 处数据的对数边际似然之差：\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(\\text{data at } Y \\mid M_1, \\alpha) - \\ln p(\\text{data at } Y \\mid M_0, \\alpha) $$\n对于一个给定的父节点集 $Pa(Y)$，单个节点 $Y$ 处数据 $D_Y$ 的对数边际似然的通用公式，是从狄利克雷-多项式共轭模型推导出来的。对于一个有 $r_Y$ 个状态的节点 $Y$ 及其具有 $q_Y$ 种配置的父节点，其对数边际似然为：\n$$ \\ln p(D_Y \\mid Pa(Y), \\alpha) = \\sum_{k=1}^{q_Y} \\left[ \\ln \\Gamma(\\alpha_k) - \\ln \\Gamma(n_k + \\alpha_k) + \\sum_{j=1}^{r_Y} \\left( \\ln \\Gamma(n_{jk} + \\alpha_{jk}) - \\ln \\Gamma(\\alpha_{jk}) \\right) \\right] $$\n其中 $n_{jk}$ 是当 $Y$ 处于状态 $j$ 且其父节点处于配置 $k$ 时的观测计数，$n_k = \\sum_{j=1}^{r_Y} n_{jk}$ 是父节点配置 $k$ 的总计数，而 $\\alpha_{jk}$ 是狄利克雷超参数。对于等效样本量为 $\\alpha$ 的 BDeu 先验，这些超参数被统一设置为：$\\alpha_{jk} = \\alpha / (q_Y r_Y)$。对于一个给定的父节点配置，这些超参数的和为 $\\alpha_k = \\sum_{j=1}^{r_Y} \\alpha_{jk} = r_Y \\cdot \\frac{\\alpha}{q_Y r_Y} = \\frac{\\alpha}{q_Y}$。\n\n我们首先计算模型 $M_0$（其中 $Y$ 没有父节点）的对数边际似然。\n在这种情况下，$Pa(Y) = \\emptyset$，因此只有一种父节点配置，$q_Y = 1$。节点 $Y$ 有 $r_Y = 3$ 个状态。\nBDeu 超参数为 $\\alpha_j = \\alpha / (1 \\cdot 3) = \\alpha/3$，其中 $j \\in \\{1, 2, 3\\}$。\n总等效样本量为 $\\alpha_0 = \\sum_{j=1}^{3} \\alpha_j = 3 (\\alpha/3) = \\alpha$。\n观测到的计数为 $n_{Y=\\text{low}} = 14$，$n_{Y=\\text{baseline}} = 9$，和 $n_{Y=\\text{high}} = 7$。我们将其表示为 $n_1=14$, $n_2=9$, $n_3=7$。总样本量为 $n = 14+9+7 = 30$。\n$M_0$ 的对数边际似然为：\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\sum_{j=1}^{3} \\left( \\ln \\Gamma(n_j + \\frac{\\alpha}{3}) - \\ln \\Gamma(\\frac{\\alpha}{3}) \\right) $$\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\ln \\Gamma(14 + \\frac{\\alpha}{3}) + \\ln \\Gamma(9 + \\frac{\\alpha}{3}) + \\ln \\Gamma(7 + \\frac{\\alpha}{3}) - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) $$\n\n接下来，我们计算模型 $M_1$（其中 $X$ 是 $Y$ 的父节点）的对数边际似然。\n父节点 $X$ 有 $r_X = 2$ 个状态，因此有 $q_Y = 2$ 种父节点配置。节点 $Y$ 仍然有 $r_Y = 3$ 个状态。\nBDeu 超参数为 $\\alpha_{jk} = \\alpha / (2 \\cdot 3) = \\alpha/6$。\n对于每个父节点配置 $k$，等效样本量为 $\\alpha_k = \\alpha/q_Y = \\alpha/2$。\n总对数边际似然是每个父节点配置项的总和。\n\n对于父节点状态 $X=0$（我们称此配置为 $k=1$）：\n计数为 $n_{11}=10, n_{12}=5, n_{13}=3$。总计数为 $n_1 = 10+5+3 = 18$。\n对分数的贡献是：\n$$ \\ln p_1 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\n对于父节点状态 $X=1$（配置 $k=2$）：\n计数为 $n_{21}=4, n_{22}=4, n_{23}=4$。总计数为 $n_2 = 4+4+4 = 12$。\n对分数的贡献是：\n$$ \\ln p_2 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\n$M_1$ 的总对数边际似然为 $\\ln p(D_Y \\mid M_1, \\alpha) = \\ln p_1 + \\ln p_2$：\n$$ \\ln p(D_Y \\mid M_1, \\alpha) = 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\n最后，我们计算差值 $\\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(D_Y \\mid M_1, \\alpha) - \\ln p(D_Y \\mid M_0, \\alpha)$：\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\left[ 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18+\\frac{\\alpha}{2}) - \\ln \\Gamma(12+\\frac{\\alpha}{2}) + \\dots - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) \\right] - \\left[ \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30+\\alpha) + \\dots - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) \\right] $$\n重新整理这些项得到最终表达式：\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right) $$\n该表达式表示为从 $X$到 $Y$ 添加调控链接所带来的对数证据（log-evidence）的变化。",
            "answer": "$$\n\\boxed{\\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right)}\n$$"
        },
        {
            "introduction": "学习到一个网络结构后，我们需要一种方法来衡量其与已知的“金标准”或参考网络相比的准确性。本练习将介绍结构汉明距离（Structural Hamming Distance, SHD），这是完成此项任务的标准度量。通过亲手计算SHD，你将学会区分不同类型的结构错误：缺失的边、多余的边和错误方向的边，从而能够更精确地评估和比较不同学习算法的性能。",
            "id": "3289677",
            "problem": "在计算系统生物学中，使用贝叶斯网络结构学习进行基因调控网络推断时，通常会使用定义在等价类代表上的距离来评估学习到的结构与参考结构的差异。考虑代表 $3$ 个基因（$X_1$、$X_2$ 和 $X_3$）的基因表达的随机变量。设参考结构为 $\\{X_1,X_2,X_3\\}$ 上的完全部分有向无环图 (CPDAG) $\\mathcal{G}^{\\star}$，其边为 $X_1 \\to X_2$ 和 $X_3 \\to X_2$，且 $X_1$ 和 $X_3$ 之间没有邻接关系。设学习到的 CPDAG 为 $\\widehat{\\mathcal{G}}$，其边为 $X_2 \\to X_1$ 和 $X_3 \\to X_1$，且 $X_2$ 和 $X_3$ 之间没有邻接关系。\n\n从贝叶斯网络和马尔可夫等价类的核心定义出发，首先根据协调邻接集和端点标记（尾部和箭头）所需的最小单边编辑操作次数，给出两个 CPDAG 之间结构汉明距离 (SHD) 的形式化定义。然后，使用您的定义，计算 $\\mathcal{G}^{\\star}$ 和 $\\widehat{\\mathcal{G}}$ 之间的 SHD。将您的最终答案表示为一个整数，无需四舍五入。",
            "solution": "该问题要求给出两个完全部分有向无环图 (CPDAG) 之间结构汉明距离 (SHD) 的形式化定义，并随后针对给定的参考结构 $\\mathcal{G}^{\\star}$ 和学习到的结构 $\\widehat{\\mathcal{G}}$ 进行计算。\n\n首先，我们建立背景。贝叶斯网络是一种概率图模型，通过有向无环图 (DAG) 表示一组随机变量及其条件依赖关系。多个 DAG 可以表示同一组条件独立性；这样一组 DAG 构成一个马尔可夫等价类。CPDAG 是马尔可夫等价类的规范图表示。它由一组顶点和一组边组成，其中边可以是有向的（强制的）或无向的。CPDAG 中的有向边 $X \\to Y$ 表示该方向存在于等价类中的所有 DAG 中。无向边 $X-Y$ 表示等价类中既包含带有 $X \\to Y$ 的 DAG，也包含带有 $Y \\to X$ 的 DAG。\n\n结构汉明距离 (SHD) 是一种度量，用于量化两个图（通常是学习到的图和真实图或参考图）之间的差异。对于在同一组顶点 $V$ 上定义的两个 CPDAG $\\mathcal{G}_1$ 和 $\\mathcal{G}_2$，SHD 是将 $\\mathcal{G}_1$ 转换为 $\\mathcal{G}_2$ 所需的基本图编辑操作的总数。这些操作可分为邻接关系（图的骨架）的差异和边的方向的差异。\n\nSHD 的形式化定义如下：\n设 $\\mathcal{G}_1$ 和 $\\mathcal{G}_2$ 是两个 CPDAG。SHD 是协调它们之间差异的成本总和，分两步计算：\n1.  **邻接差异**：此部分计算存在于一个图的骨架中但不存在于另一个图的骨架中的边的数量。设 $A_1$ 和 $A_2$ 分别是 $\\mathcal{G}_1$ 和 $\\mathcal{G}_2$ 的邻接集（表示邻域关系的无向边）。邻接差异的数量是这些集合的对称差的大小，即 $|A_1 \\Delta A_2| = |(A_1 \\setminus A_2) \\cup (A_2 \\setminus A_1)|$。此对称差中的每个元素对应于一条缺失边（如果 $\\mathcal{G}_2$ 是 $\\mathcal{G}_1$ 的估计，则为假阴性）或一条多余边（假阳性）。每个这样的差异对 SHD 的贡献为 $1$。\n\n2.  **方向差异**：此部分仅考虑同时存在于两个骨架中的边（即 $A_1 \\cap A_2$ 中的邻接关系）。对于每个公共邻接，如果边标记（尾部和箭头）不匹配，则会产生方向差异。对于 CPDAG，公共邻接 $\\{u,v\\}$ 可能的不匹配情况有：\n    *   $\\mathcal{G}_1$ 有 $u-v$（无向），而 $\\mathcal{G}_2$ 有 $u \\to v$ 或 $v \\to u$（有向）。\n    *   $\\mathcal{G}_1$ 有 $u \\to v$，而 $\\mathcal{G}_2$ 有 $v \\to u$（反转）。\n    每个公共边的此类方向不匹配对 SHD 的贡献为 $1$。\n\n总 SHD 是这两个步骤计数的总和。\n\n现在我们应用此定义来计算给定的参考 CPDAG $\\mathcal{G}^{\\star}$ 和学习到的 CPDAG $\\widehat{\\mathcal{G}}$ 之间的 SHD。随机变量（基因）集合为 $V = \\{X_1, X_2, X_3\\}$。\n\n参考结构 $\\mathcal{G}^{\\star}$ 具有边 $X_1 \\to X_2$ 和 $X_3 \\to X_2$，且 $X_1$ 和 $X_3$ 之间没有邻接关系。这种配置 $X_1 \\to X_2 \\leftarrow X_3$ 是一个 v-结构。在 CPDAG 中，形成 v-结构的边总是有向的。因此，$\\mathcal{G}^{\\star}$ 是一个恰好包含这两条有向边的图。\n*   $\\mathcal{G}^{\\star}$ 的邻接集：$A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$。\n*   $\\mathcal{G}^{\\star}$ 的有向边集：$D^{\\star} = \\{X_1 \\to X_2, X_3 \\to X_2\\}$。\n\n学习到的结构 $\\widehat{\\mathcal{G}}$ 具有边 $X_2 \\to X_1$ 和 $X_3 \\to X_1$，且 $X_2$ 和 $X_3$ 之间没有邻接关系。这种配置 $X_2 \\to X_1 \\leftarrow X_3$ 也是一个 v-结构。同样，这些边在 CPDAG 中必须是有向的。\n*   $\\widehat{\\mathcal{G}}$ 的邻接集：$\\widehat{A} = \\{\\{X_2, X_1\\}, \\{X_3, X_1\\}\\}$。注意，$\\{X_2, X_1\\}$ 与 $\\{X_1, X_2\\}$ 相同。为保持一致性，我们写作：$\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$。\n*   $\\widehat{\\mathcal{G}}$ 的有向边集：$\\widehat{D} = \\{X_2 \\to X_1, X_3 \\to X_1\\}$。\n\n我们分两步进行计算。\n\n**步骤 1：计算邻接差异**\n我们比较邻接集 $A^{\\star}$ 和 $\\widehat{A}$。\n$A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$\n$\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$\n\n*   在 $A^{\\star}$ 中但不在 $\\widehat{A}$ 中的边：$A^{\\star} \\setminus \\widehat{A} = \\{\\{X_2, X_3\\}\\}$。这是一条存在于参考图中但缺失于学习图中的边（假阴性）。这对 SHD 的贡献为 $1$。\n*   在 $\\widehat{A}$ 中但不在 $A^{\\star}$ 中的边：$\\widehat{A} \\setminus A^{\\star} = \\{\\{X_1, X_3\\}\\}$。这是一条缺失于参考图中但存在于学习图中的边（假阳性）。这对 SHD 的贡献为 $1$。\n\n邻接差异的总数为 $1 + 1 = 2$。\n\n**步骤 2：计算方向差异**\n我们检查邻接集交集中的边的方向。\n$A_{common} = A^{\\star} \\cap \\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\} \\cap \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\} = \\{\\{X_1, X_2\\}\\}$。\n存在一个公共邻接：$\\{X_1, X_2\\}$。\n\n*   在 $\\mathcal{G}^{\\star}$ 中，与此邻接对应的边是有向的：$X_1 \\to X_2$。\n*   在 $\\widehat{\\mathcal{G}}$ 中，与此邻接对应的边也是有向的，但方向相反：$X_2 \\to X_1$。\n\n这构成了一次边反转。一次反转是一个单一的方向错误。这对 SHD 的贡献为 $1$。\n\n**步骤 3：计算总 SHD**\n总 SHD 是邻接差异和方向差异的总和。\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = (\\text{邻接差异}) + (\\text{方向差异})$\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = 2 + 1 = 3$。\n\n因此，参考 CPDAG $\\mathcal{G}^{\\star}$ 和学习到的 CPDAG $\\widehat{\\mathcal{G}}$ 之间的结构汉明距离为 $3$。",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "最后一个练习将从计算技巧转向批判性思维。现实世界中的生物学数据，例如来自单细胞实验的数据，常常会受到选择偏倚的影响，这可能会误导学习算法。这个思想实验探讨了一种被称为“对撞偏倚”（collider bias）的常见陷阱。通过分析这个场景，你将面临挑战，需要识别出正确的统计策略，以避免从数据中推断出虚假的因果关系。",
            "id": "3289712",
            "problem": "在计算系统生物学的单细胞转录组学中，假设一个实验室从一部分基因表达测量值中计算出一个标量细胞周期得分 $C$，然后进行设门（gating）：只有 $C$ 高于某个阈值的细胞才被保留用于分析。你的任务是学习一对受细胞周期调控的基因 $X$ 和 $Y$ 上的因果贝叶斯网络的结构，其中下游的细胞周期得分 $C$ 是根据它们的表达构建的。目标是恢复设门前的因果基因图 $G$（在 $\\{X, Y\\}$ 上）。\n\n假设在未经选择的群体中，数据生成过程如下。基因 $X$ 和 $Y$ 由不同的上游因子独立调控（为具体起见，设 $X \\sim p_X$ 且 $Y \\sim p_Y$，并且 $X \\perp Y$），得分 $C$ 是通过一个带噪声的可测函数根据它们的表达计算得出的，$C = g(X, Y, \\varepsilon_C)$，其中 $\\varepsilon_C \\perp (X, Y)$，并且 $g$ 使得条件密度 $p(C \\mid X, Y)$ 联合地依赖于 $X$ 和 $Y$。细胞选择指示变量 $S$ 定义为 $S = \\mathbf{1}\\{C > c_\\star\\}$，其中 $c_\\star \\in \\mathbb{R}$ 是一个固定的阈值，只有 $S=1$ 的记录被保留。在所测量的变量中，真实的设门前因果图是一个有向无环图（DAG），其箭头为 $X \\to C \\leftarrow Y$ 和 $C \\to S$，并且 $X$ 和 $Y$ 之间没有边。\n\n你计划使用标准的基于分数的方法（如贝叶斯信息准则，BIC）或基于约束的条件独立性检验，从设门后的数据集中学习 $\\{X, Y\\}$ 上的贝叶斯网络（BN）结构。你可以使用的基本事实是：BN 分解 $p(x, y, c) = p(x)\\,p(y)\\,p(c \\mid x, y)$；对撞结构（collider）的定义（$X \\to C \\leftarrow Y$）；以及 d-分离的推论，即对一个对撞结构或其后代进行条件化，会在原本独立的父节点之间引入依赖关系。\n\n问题：在所述假设下，以下哪个程序能够一致地恢复 $\\{X, Y\\}$ 上的设门前因果图 $G$（具体来说，就是正确地拒绝在 $X$ 和 $Y$ 之间添加伪边）？\n\nA. 仅使用 $\\{X, Y\\}$ 对设门后的数据拟合贝叶斯网络，忽略 $C$ 因为它参与了选择过程，并依赖于稀疏性惩罚的分数（如 BIC）来避免假边。\n\nB. 通过显式地包含 $C$ 和一个选择节点 $S$（带有 $C \\to S$）来用选择图增强模型，并使用加权似然对设门后的记录进行学习，其中每个样本 $i$ 通过 $1/p(S_i=1 \\mid C_i)$进行重加权，使得目标函数正比于 $\\sum_{i: S_i=1} \\left[\\log p(X_i, Y_i, C_i) - \\log p(S_i=1 \\mid C_i)\\right]$。然后将学习到的选择前结构投影到 $\\{X, Y\\}$ 上以获得 $G$。\n\nC. 将 $C$ 作为一个协变量，并在所有 $X$ 和 $Y$ 之间的独立性检验中以 $C$ 为条件，因为 $C$ 是它们表面关联的来源。\n\nD. 在设门后的集合内进行随机子抽样，以使 $C$ 的边际分布与设门前群体的边际分布相匹配，然后对子抽样数据中的 $\\{X, Y\\}$ 运行一个未经调整的贝叶斯网络学习器。\n\n选择唯一的最佳选项。",
            "solution": "用户希望我验证并解决一个关于单细胞转录组学背景下，选择偏差下的贝叶斯网络结构学习的问题。\n\n### 步骤1：提取已知条件\n-   **变量**：基因表达水平 $X$ 和 $Y$；标量细胞周期得分 $C$；选择指示变量 $S$。\n-   **设门前（未经选择）的数据生成过程**：\n    -   $X$ 和 $Y$ 相互独立：$X \\perp Y$。在 $\\{X, Y\\}$ 上的真实因果图，记为 $G$，在它们之间没有边。\n    -   得分 $C$ 是 $X$ 和 $Y$ 的一个带噪声的函数：$C = g(X, Y, \\varepsilon_C)$，其中 $\\varepsilon_C \\perp (X, Y)$。\n    -   条件密度 $p(C \\mid X, Y)$ 同时依赖于 $X$ 和 $Y$。\n    -   在测量的变量上的真实因果有向无环图（DAG）是 $X \\to C \\leftarrow Y$。这意味着 $C$ 是一个对撞结构。\n-   **选择机制**：\n    -   对 $C$ 进行设门。如果一个细胞的得分 $C$ 高于一个固定阈值 $c_\\star$，则该细胞被选中。\n    -   选择指示变量是 $S = \\mathbf{1}\\{C > c_\\star\\}$。\n    -   只有 $S=1$ 的数据被保留用于分析。\n    -   包含选择的因果 DAG 是 $X \\to C \\leftarrow Y$ 和 $C \\to S$。因此，$S$ 是对撞结构 $C$ 的一个后代。\n-   **任务**：\n    -   从设门后（选择后）的数据集中学习 $\\{X, Y\\}$ 上的贝叶斯网络（BN）结构。\n    -   目标是一致地恢复真实的设门前图 $G$（即，推断出 $X$ 和 $Y$ 之间没有边）。\n-   **允许使用的方法/事实**：\n    -   用于 BN 结构学习的标准基于分数（例如，BIC）或基于约束的方法。\n    -   BN 分解：$p(x, y, c) = p(x)\\,p(y)\\,p(c \\mid x, y)$。\n    -   d-分离规则，特别是关于对撞结构的规则。\n\n### 步骤2：使用提取的已知条件进行验证\n-   **科学依据**：该问题在统计学和计算生物学中都有充分的依据。该场景描述了对撞分层偏差（collider stratification bias），这是一个有据可查的统计现象，经常出现在生物学研究中（例如，病例对照研究、流式细胞术或单细胞分析中的设门）。因果模型和选择效应是因果推断中的标准概念。\n-   **适定性**：该问题是适定的。它提出了一个清晰的数据生成过程和一个特定的选择机制，并要求给出一个程序来恢复真实的潜在因果结构，这是因果发现中的一个标准任务。可以根据已建立的统计学原理从选项中确定正确答案。\n-   **客观性**：该问题使用精确、客观的数学和统计语言陈述。没有歧义或主观内容。\n\n该问题不违反任何无效性标准。其设置是自洽的、一致的且科学合理的。\n\n### 步骤3：结论和行动\n问题陈述是有效的。我将继续推导解决方案并评估各个选项。\n\n### 推导\n\n问题的核心在于理解设门程序的统计效应。真实的因果图是 $X \\to C \\leftarrow Y$。在这个图中，$X$ 和 $Y$ 之间的路径被对撞结构 $C$ 阻断。根据 d-分离规则，这意味着 $X$ 和 $Y$ 在设门前群体中是边际独立的，即 $X \\perp Y$。目标是从数据中恢复这种独立性（无边）。\n\n然而，分析仅在“被保留”的细胞上进行，这意味着我们以事件 $S=1$ 为条件。选择变量 $S$ 是对撞结构 $C$ 的一个直接后代。d-分离的一个基本规则是，对一个对撞结构或其任何后代进行条件化，会打开其父节点之间的路径。在这种情况下，以 $S=1$ 为条件打开了路径 $X \\to C \\leftarrow Y$，在被选择的子群体中引入了 $X$ 和 $Y$ 之间的统计依赖性。\n在数学上，虽然 $p(X, Y) = p(X)p(Y)$，但 $p(X, Y \\mid S=1) = p(X \\mid S=1)p(Y \\mid S=1)$ 并不成立。因此，$X \\not\\perp Y \\mid S=1$。\n\n任何天真地应用于设门后数据的标准结构学习算法（无论是基于约束还是基于分数）都会检测到 $X$ 和 $Y$ 之间这种被诱导出的依赖性。对于一个足够大的数据集，该算法将以高置信度得出结论，$X$ 和 $Y$ 之间存在一条边，从而无法恢复真实的设门前图 $G$。\n\n因此，一个一致的程序必须明确地考虑并纠正这种选择偏差。我们现在将根据这一要求评估每个选项。\n\n### 逐项分析\n\n**A. 仅使用 $\\{X, Y\\}$ 对设门后的数据拟合贝叶斯网络，忽略 $C$ 因为它参与了选择过程，并依赖于稀疏性惩罚的分数（如 BIC）来避免假边。**\n\n这个程序直接将结构学习算法应用于来自设门后数据集的变量 $\\{X, Y\\}$。如上所述，在这个数据集中，$X$ 和 $Y$ 是统计相关的（$X \\not\\perp Y \\mid S=1$）。像 BIC（贝叶斯信息准则）这样的稀疏性惩罚项会惩罚模型复杂度，可以在有限样本中防止因微弱的伪相关而添加边。然而，由对撞偏差引起的依赖性是一个系统性的、结构性的产物，而不是随机噪声。随着样本量的增加，这种伪依赖性的统计证据将变得任意强，最终会压倒任何固定的稀疏性惩罚。因此，该方法不是一致的，并将错误地推断出 $X$ 和 $Y$ 之间存在一条边。\n\n**结论：不正确。**\n\n**B. 通过显式地包含 $C$ 和一个选择节点 $S$（带有 $C \\to S$）来用选择图增强模型，并使用加权似然对设门后的记录进行学习，其中每个样本 $i$ 通过 $1/p(S_i=1 \\mid C_i)$进行重加权，使得目标函数正比于 $\\sum_{i: S_i=1} \\left[\\log p(X_i, Y_i, C_i) - \\log p(S_i=1 \\mid C_i)\\right]$。然后将学习到的选择前结构投影到 $\\{X, Y\\}$ 上以获得 $G$。**\n\n这个选项提出了一种纠正选择偏差的策略，即通过将选择机制明确地纳入模型中。这是正确的概念性方法。通过对全套变量 $\\{X, Y, C\\}$ 进行建模并考虑选择过程，有可能恢复选择前分布 $p(X, Y, C)$ 的参数和结构。如果 $p(X, Y, C)$ 的结构被正确地学习为 $p(X)p(Y)p(C \\mid X, Y)$，那么就能正确地识别出 $X$ 和 $Y$ 之间没有边。\n\n这个选项描述了正确的高层策略。它正确地指出必须增强模型并使用修正的目标函数。尽管所提供的数学细节存在微妙之处（例如，描述似乎将逆概率加权（IPW）与近期文献中一个不同的目标函数混为一谈，并且在确定性阈值 $S = \\mathbf{1}\\{C > c_\\star\\}$下应用这些方法存在技术困难），但总体程序是所有选项中唯一一个旨在解决选择偏差这一核心统计问题的方案。与其他根本上被误导的选项相比，该选项描述了有效解决方案的类别。它代表了在选择偏差下实现一致估计的单一最佳原则性方法。\n\n**结论：正确。**\n\n**C. 将 $C$ 作为一个协变量，并在所有 $X$ 和 $Y$ 之间的独立性检验中以 $C$ 为条件，因为 $C$ 是它们表面关联的来源。**\n\n这个程序建议检验在给定 $C$ 的条件下 $X$ 和 $Y$ 的条件独立性，即 $X \\perp Y \\mid C$。问题陈述中指出，真实的图是 $X \\to C \\leftarrow Y$。在这种结构中，$C$ 是一个对撞结构。以对撞结构为条件会诱导其父节点之间的依赖关系。这种现象通常被称为“解释消除”（explaining away）。例如，如果高分 $C$ 可能是由 $X$ 或 $Y$ 的高值引起的，那么当知道 $C$ 很高而 $X$ 很低时，就提供了 $Y$ 必定很高的证据。因此，$X \\not\\perp Y \\mid C$。使用此程序的基于约束的算法会发现一个依赖关系，并在 $X$ 和 $Y$ 之间添加一条边。这种方法主动制造了它试图避免的偏差。\n\n**结论：不正确。**\n\n**D. 在设门后的集合内进行随机子抽样，以使 $C$ 的边际分布与设门前群体的边际分布相匹配，然后对子抽样数据中的 $\\{X, Y\\}$ 运行一个未经调整的贝叶斯网络学习器。**\n\n这个程序有几个缺陷。首先，它无法执行。设门后的集合只包含 $C > c_\\star$ 的细胞。我们无法通过对所有值都为 $C > c_\\star$ 的数据进行子抽样来重建完整的设门前 $C$ 的边际分布（该分布包含 $C \\le c_\\star$ 的值）。其次，即使这可能实现，纠正 $C$ 的边际分布也无法修复选择偏差。该偏差是对*联合*分布 $p(X, Y, C)$ 的扭曲，它在条件联合分布 $p(X, Y \\mid S=1)$ 中诱导了伪依赖。匹配边际 $p(C)$ 不会恢复 $X$ 和 $Y$ 的独立性。该程序定义不明确，并且没有解决问题的统计根源。\n\n**结论：不正确。**",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}