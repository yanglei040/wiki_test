{
    "hands_on_practices": [
        {
            "introduction": "Learning the structure of a biological network from data is a problem of immense combinatorial complexity. Before diving into specific search strategies, it is crucial to understand the scale of the challenge. This exercise explores the worst-case complexity of the foundational Peter and Clark (PC) algorithm, a classic constraint-based method, to reveal how the number of required statistical tests grows with the number of genes, providing a formal motivation for the development of more scalable and heuristic-driven approaches .",
            "id": "3289722",
            "problem": "In the context of inferring gene regulatory networks as Bayesian networks in computational systems biology, consider applying the Peter and Clark (PC) algorithm for skeleton learning on a set of $p$ random variables representing gene expression levels. A Bayesian network is a directed acyclic graph whose structure encodes conditional independence relations, and skeleton learning in the PC algorithm proceeds by iteratively testing conditional independence statements of the form $X \\perp Y \\mid S$ for pairs of variables $X$ and $Y$ and conditioning sets $S$.\n\nAssume that due to sample size and biological feasibility constraints, the maximal conditioning set size is capped at $k$, meaning the algorithm will not consider sets $S$ with $|S| > k$. In the worst case, no edges are removed until all conditional independence tests up to conditioning set size $k$ have been exhausted, so the adjacency sets remain at their largest possible size throughout the skeleton learning phase.\n\nStarting from the core definitions of conditional independence in Bayesian networks and combinatorial enumeration of conditioning sets, derive an explicit upper bound, as a single closed-form expression in terms of $p$ and $k$, on the total number of conditional independence tests the PC algorithm may perform in the worst case during skeleton learning. Then, based on this bound, articulate the implications for scalability as $p$ grows with fixed $k$ in high-dimensional gene expression settings. The final answer must be provided as a single symbolic expression. No numerical rounding is required.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- The number of random variables (genes) is $p$.\n- The algorithm is the Peter and Clark (PC) algorithm for skeleton learning.\n- The task is to infer a gene regulatory network as a Bayesian network.\n- Conditional independence tests are of the form $X \\perp Y \\mid S$.\n- The maximal size of the conditioning set $S$ is $k$, i.e., $|S| \\le k$.\n- The worst-case assumption is that no edges are removed until all conditional independence tests up to conditioning set size $k$ have been exhausted.\n- The objective is to derive a single closed-form expression for the upper bound on the total number of conditional independence tests.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in the established theory of graphical models and computational statistics. The PC algorithm is a foundational, non-proprietary algorithm for learning the structure of Bayesian networks. Its application to inferring gene regulatory networks is a classic and highly relevant problem in computational systems biology. The analysis of its worst-case complexity is a standard and necessary part of understanding the algorithm's practical limitations.\n- **Well-Posed:** The problem is clearly specified. It provides all necessary variables ($p$, $k$) and a well-defined worst-case scenario. The objective—to derive a closed-form expression for the upper bound on the number of tests—is unambiguous and leads to a unique mathematical result based on combinatorial principles.\n- **Objective:** The problem is stated in precise, formal language, free of ambiguity, subjectivity, or opinion.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is scientifically grounded, well-posed, objective, and complete for the specified task. A solution will be derived.\n\n### Derivation of the Upper Bound\n\nThe objective is to find an upper bound on the total number of conditional independence (CI) tests performed by the PC algorithm. The algorithm iteratively tests for conditional independence between pairs of variables for conditioning sets of increasing size. The worst-case scenario, as described, assumes the underlying graph is complete (or dense enough) such that no edges are removed during the initial stages of the algorithm. This forces the algorithm to perform the maximum possible number of tests.\n\nThe total number of tests can be calculated by considering all possible combinations of pairs of variables and valid conditioning sets.\n\n1.  **Selection of Variable Pairs:**\n    The first step in any CI test $X \\perp Y \\mid S$ is to choose a pair of distinct variables, $\\{X, Y\\}$, from the set of $p$ variables. The number of ways to choose $2$ variables from a set of $p$ is given by the binomial coefficient $\\binom{p}{2}$.\n    $$ \\text{Number of pairs} = \\binom{p}{2} = \\frac{p(p-1)}{2} $$\n\n2.  **Selection of Conditioning Sets:**\n    For each chosen pair $\\{X, Y\\}$, the algorithm tests for independence conditioned on subsets $S$ of the remaining $p-2$ variables. The PC algorithm proceeds by increasing the size of the conditioning set, which we denote as $s = |S|$. The problem states that the maximum size of the conditioning set is capped at $k$. Therefore, for any given pair $\\{X, Y\\}$, the algorithm will test conditioning sets of all sizes from $s=0$ up to $s=k$.\n\n    - For a conditioning set of size $s=0$, the set $S$ is the empty set, $\\emptyset$. There is only $\\binom{p-2}{0} = 1$ such set.\n    - For a conditioning set of size $s=1$, we must choose $1$ variable from the remaining $p-2$ variables. The number of such sets is $\\binom{p-2}{1}$.\n    - For a conditioning set of size $s$, we must choose $s$ variables from the remaining $p-2$ variables. The number of such sets is $\\binom{p-2}{s}$.\n\n    In the worst-case scenario, for a given pair $\\{X, Y\\}$, the algorithm must perform a CI test for every possible conditioning set $S$ with size $|S| \\in \\{0, 1, \\dots, k\\}$, where the members of $S$ are chosen from the $p-2$ variables other than $X$ and $Y$. The total number of conditioning sets to test for a single pair is the sum of the number of possible sets for each size $s$ from $0$ to $k$:\n    $$ \\text{Number of conditioning sets per pair} = \\sum_{s=0}^{k} \\binom{p-2}{s} $$\n\n3.  **Total Number of Conditional Independence Tests:**\n    The total upper bound on the number of CI tests, which we denote as $N_{max}$, is the product of the number of variable pairs and the number of conditioning sets tested per pair.\n    $$ N_{max}(p, k) = (\\text{Number of pairs}) \\times (\\text{Number of conditioning sets per pair}) $$\n    Substituting the expressions derived above, we obtain the final closed-form expression for the upper bound:\n    $$ N_{max}(p, k) = \\binom{p}{2} \\sum_{s=0}^{k} \\binom{p-2}{s} $$\n\n### Implications for Scalability\n\nTo understand the implications for scalability, we analyze the asymptotic behavior of this expression as the number of genes, $p$, grows large, for a fixed maximum conditioning set size $k$.\n\n- The first term, $\\binom{p}{2} = \\frac{p(p-1)}{2}$, is of order $O(p^2)$.\n- The second term is a partial sum of binomial coefficients, $\\sum_{s=0}^{k} \\binom{p-2}{s}$. For a fixed $k$ and large $p$, the dominant term in this sum is the one with the highest power of $p$. The term $\\binom{p-2}{s}$ is a polynomial in $p$ of degree $s$. Therefore, the highest-degree term in the summation is $\\binom{p-2}{k}$.\n$$ \\binom{p-2}{k} = \\frac{(p-2)(p-3)\\cdots(p-2-k+1)}{k!} = O(p^k) $$\n- The entire summation is therefore of order $O(p^k)$.\n\nCombining these, the overall complexity of the number of tests is:\n$$ N_{max}(p, k) \\sim O(p^2) \\cdot O(p^k) = O(p^{k+2}) $$\n\nThis polynomial complexity has significant implications for high-dimensional applications like genomics. While being polynomial in $p$ is far more tractable than being exponential (which would be the case if $k$ were allowed to grow with $p$), the degree of the polynomial is $k+2$. In practice, gene regulatory networks can exhibit long-range dependencies, requiring $k$ to be at least $3$ or $4$ for meaningful discovery. This leads to a computational burden on the order of $O(p^5)$ or $O(p^6)$, which can be computationally prohibitive when $p$ represents tens of thousands of genes. This scalability issue is a primary reason why direct application of the PC algorithm to large-scale biological data is challenging and often requires modifications, heuristics, or parallelization.\nThe derived expression provides the formal basis for this crucial conclusion.",
            "answer": "$$ \\boxed{\\binom{p}{2} \\sum_{s=0}^{k} \\binom{p-2}{s}} $$"
        },
        {
            "introduction": "In contrast to constraint-based methods, score-based approaches frame structure learning as an optimization problem: finding the graph that best explains the data according to a scoring metric. The Bayesian Dirichlet equivalent uniform (BDeu) score is a popular choice that balances model fit with complexity. This practice problem provides a concrete, hands-on calculation of the change in the local BDeu score when a potential regulatory edge is added, illustrating the fundamental computation that guides local search algorithms in their quest for an optimal network structure .",
            "id": "3289730",
            "problem": "In computational systems biology, Bayesian network structure learning is often applied to infer gene regulatory interactions from discretized expression profiles. Consider a candidate regulator-target pair where the potential child node $Y$ represents a target gene with $r_{Y} = 3$ discrete expression states $\\{ \\text{low}, \\text{baseline}, \\text{high} \\}$, and the potential parent node $X$ represents a transcriptional regulator with $r_{X} = 2$ discrete activity states $\\{ 0, 1 \\}$. You are given $N = 30$ independent samples of matched gene activity profiles in which $Y$ and $X$ have been discretized as above.\n\nThe observed counts for $Y$ when $X$ is ignored (i.e., no parents for $Y$) are:\n- $n_{Y=\\text{low}} = 14$,\n- $n_{Y=\\text{baseline}} = 9$,\n- $n_{Y=\\text{high}} = 7$.\n\nThe observed contingency table of $Y$ given $X$ when $X$ is considered as a parent of $Y$ is:\n- For $X = 0$: $(n_{Y=\\text{low} \\mid X=0}, n_{Y=\\text{baseline} \\mid X=0}, n_{Y=\\text{high} \\mid X=0}) = (10, 5, 3)$,\n- For $X = 1$: $(n_{Y=\\text{low} \\mid X=1}, n_{Y=\\text{baseline} \\mid X=1}, n_{Y=\\text{high} \\mid X=1}) = (4, 4, 4)$.\n\nAssume a Bayesian Dirichlet equivalent uniform (BDeu) prior for the conditional probability tables of $Y$ that is uniform across states and parent configurations, parameterized by an equivalent sample size $\\alpha > 0$. Use the properties of the Dirichlet-multinomial conjugate model and the modular decomposability of the Bayesian network marginal likelihood.\n\nDefine the change in the BDeu local score for adding the parent $X$ to node $Y$ as the difference between the log marginal likelihood of the data at $Y$ with $X$ as a parent and the log marginal likelihood of the data at $Y$ with no parents:\n$$\n\\Delta S_{\\text{BDeu}}(Y; X) \\equiv \\ln p(\\text{data at } Y \\mid \\text{$X$ is a parent of $Y$}, \\alpha) \\;-\\; \\ln p(\\text{data at } Y \\mid \\text{$Y$ has no parents}, \\alpha).\n$$\n\nCompute $\\Delta S_{\\text{BDeu}}(Y; X)$ as a closed-form analytic expression in terms of $\\alpha$, using the natural logarithm and the gamma function, based solely on the counts provided above. Express the final answer as a single analytic expression in terms of $\\alpha$. Do not approximate numerically.",
            "solution": "The problem requires the calculation of the change in the BDeu local score, $\\Delta S_{\\text{BDeu}}(Y; X)$, when an edge from a potential parent node $X$ is added to a target node $Y$. This quantity is defined as the difference between the log marginal likelihood of the data at node $Y$ for a model with the edge ($M_1$) and a model without the edge ($M_0$):\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(\\text{data at } Y \\mid M_1, \\alpha) - \\ln p(\\text{data at } Y \\mid M_0, \\alpha) $$\nThe general formula for the log marginal likelihood of the data $D_Y$ at a single node $Y$ with a given parent set $Pa(Y)$ is derived from the Dirichlet-multinomial conjugate model. For a node $Y$ with $r_Y$ states and parents with $q_Y$ configurations, the log marginal likelihood is:\n$$ \\ln p(D_Y \\mid Pa(Y), \\alpha) = \\sum_{k=1}^{q_Y} \\left[ \\ln \\Gamma(\\alpha_k) - \\ln \\Gamma(n_k + \\alpha_k) + \\sum_{j=1}^{r_Y} \\left( \\ln \\Gamma(n_{jk} + \\alpha_{jk}) - \\ln \\Gamma(\\alpha_{jk}) \\right) \\right] $$\nwhere $n_{jk}$ is the count of observations for state $j$ of $Y$ and parent configuration $k$, $n_k = \\sum_{j=1}^{r_Y} n_{jk}$ is the total count for parent configuration $k$, and $\\alpha_{jk}$ are the Dirichlet hyperparameters. For a BDeu prior with equivalent sample size $\\alpha$, these are set uniformly: $\\alpha_{jk} = \\alpha / (q_Y r_Y)$. The sum of these hyperparameters for a given parent configuration is $\\alpha_k = \\sum_{j=1}^{r_Y} \\alpha_{jk} = r_Y \\cdot \\frac{\\alpha}{q_Y r_Y} = \\frac{\\alpha}{q_Y}$.\n\nWe first calculate the log marginal likelihood for the model $M_0$ where $Y$ has no parents.\nIn this case, $Pa(Y) = \\emptyset$, so there is only one parent configuration, $q_Y = 1$. The node $Y$ has $r_Y = 3$ states.\nThe BDeu hyperparameters are $\\alpha_j = \\alpha / (1 \\cdot 3) = \\alpha/3$ for $j \\in \\{1, 2, 3\\}$.\nThe total equivalent sample size is $\\alpha_0 = \\sum_{j=1}^{3} \\alpha_j = 3 (\\alpha/3) = \\alpha$.\nThe observed counts are $n_{Y=\\text{low}} = 14$, $n_{Y=\\text{baseline}} = 9$, and $n_{Y=\\text{high}} = 7$. We denote these as $n_1=14$, $n_2=9$, $n_3=7$. The total sample size is $n = 14+9+7 = 30$.\nThe log marginal likelihood for $M_0$ is:\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\sum_{j=1}^{3} \\left( \\ln \\Gamma(n_j + \\frac{\\alpha}{3}) - \\ln \\Gamma(\\frac{\\alpha}{3}) \\right) $$\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\ln \\Gamma(14 + \\frac{\\alpha}{3}) + \\ln \\Gamma(9 + \\frac{\\alpha}{3}) + \\ln \\Gamma(7 + \\frac{\\alpha}{3}) - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) $$\n\nNext, we calculate the log marginal likelihood for the model $M_1$ where $X$ is the parent of $Y$.\nThe parent node $X$ has $r_X = 2$ states, so there are $q_Y = 2$ parent configurations. The node $Y$ still has $r_Y = 3$ states.\nThe BDeu hyperparameters are $\\alpha_{jk} = \\alpha / (2 \\cdot 3) = \\alpha/6$.\nFor each parent configuration $k$, the equivalent sample size is $\\alpha_k = \\alpha/q_Y = \\alpha/2$.\nThe total log marginal likelihood is the sum of terms for each parent configuration.\n\nFor the parent state $X=0$ (let's call this configuration $k=1$):\nThe counts are $n_{11}=10$, $n_{12}=5$, $n_{13}=3$. The total count is $n_1 = 10+5+3 = 18$.\nThe contribution to the score is:\n$$ \\ln p_1 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\nFor the parent state $X=1$ (configuration $k=2$):\nThe counts are $n_{21}=4$, $n_{22}=4$, $n_{23}=4$. The total count is $n_2 = 4+4+4 = 12$.\nThe contribution to the score is:\n$$ \\ln p_2 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\nThe total log marginal likelihood for $M_1$ is $\\ln p(D_Y \\mid M_1, \\alpha) = \\ln p_1 + \\ln p_2$:\n$$ \\ln p(D_Y \\mid M_1, \\alpha) = 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\nFinally, we compute the difference $\\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(D_Y \\mid M_1, \\alpha) - \\ln p(D_Y \\mid M_0, \\alpha)$:\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\left[ 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18+\\frac{\\alpha}{2}) - \\ln \\Gamma(12+\\frac{\\alpha}{2}) + \\dots - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) \\right] - \\left[ \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30+\\alpha) + \\dots - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) \\right] $$\nRearranging the terms gives the final expression:\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right) $$\nThis expression represents the change in log-evidence for adding the regulatory link from $X$ to $Y$.",
            "answer": "$$\n\\boxed{\\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right)}\n$$"
        },
        {
            "introduction": "After a network structure has been learned, a critical step in the scientific process is to evaluate its accuracy against a known reference or \"gold standard.\" The Structural Hamming Distance (SHD) provides a principled way to quantify the difference between two networks by counting the minimum number of edge additions, deletions, and reversals needed to make them identical. This exercise will help you master the definition and computation of SHD, an essential skill for benchmarking and comparing the performance of different structure learning algorithms .",
            "id": "3289677",
            "problem": "In computational systems biology, gene regulatory network inference with Bayesian network structure learning often evaluates learned structures against a reference using distances defined on equivalence-class representatives. Consider random variables representing gene expression for 3 genes, $X_1$, $X_2$, and $X_3$. Let the reference structure be the Completed Partially Directed Acyclic Graph (CPDAG) $\\mathcal{G}^{\\star}$ over $\\{X_1,X_2,X_3\\}$ with edges $X_1 \\to X_2$ and $X_3 \\to X_2$, and no adjacency between $X_1$ and $X_3$. Let the learned CPDAG be $\\widehat{\\mathcal{G}}$ with edges $X_2 \\to X_1$ and $X_3 \\to X_1$, and no adjacency between $X_2$ and $X_3$. \n\nStarting from the core definitions of Bayesian networks and Markov equivalence classes, first provide a formal definition of Structural Hamming Distance (SHD) between two CPDAGs in terms of the minimal number of single-edge edit operations that reconcile both the adjacency sets and the endpoint marks (tails and arrowheads). Then, using your definition, compute the SHD between $\\mathcal{G}^{\\star}$ and $\\widehat{\\mathcal{G}}$. Express your final answer as an integer with no rounding required.",
            "solution": "The problem requires a formal definition of the Structural Hamming Distance (SHD) between two Completed Partially Directed Acyclic Graphs (CPDAGs) and its subsequent computation for a given reference structure $\\mathcal{G}^{\\star}$ and a learned structure $\\widehat{\\mathcal{G}}$.\n\nFirst, we establish the context. A Bayesian network is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). Multiple DAGs can represent the same set of conditional independencies; such a set of DAGs forms a Markov equivalence class. A CPDAG is a canonical graphical representation for a Markov equivalence class. It consists of a set of vertices and a set of edges, where edges can be either directed (compelled) or undirected. A directed edge $X \\to Y$ in a CPDAG signifies that this orientation is present in all DAGs within the equivalence class. An undirected edge $X-Y$ indicates that the equivalence class contains DAGs with $X \\to Y$ as well as DAGs with $Y \\to X$.\n\nThe Structural Hamming Distance (SHD) is a metric used to quantify the difference between two graphs, often a learned graph and a true or reference graph. For two CPDAGs, $\\mathcal{G}_1$ and $\\mathcal{G}_2$, defined on the same set of vertices $V$, the SHD is the total number of fundamental graph edit operations required to transform $\\mathcal{G}_1$ into $\\mathcal{G}_2$. These operations are categorized into differences in adjacencies (the skeleton of the graph) and differences in edge orientations.\n\nFormally, the SHD is defined as follows:\nLet $\\mathcal{G}_1$ and $\\mathcal{G}_2$ be two CPDAGs. The SHD is the sum of the costs of reconciling their differences, calculated in two steps:\n1.  **Adjacency Differences**: This component counts the number of edges that are present in one graph's skeleton but not the other's. Let $A_1$ and $A_2$ be the sets of adjacencies (undirected edges representing neighborhood relationships) for $\\mathcal{G}_1$ and $\\mathcal{G}_2$, respectively. The number of adjacency differences is the size of the symmetric difference of these sets, $|A_1 \\Delta A_2| = |(A_1 \\setminus A_2) \\cup (A_2 \\setminus A_1)|$. Each element in this symmetric difference corresponds to either a missing edge (a false negative if $\\mathcal{G}_2$ is the estimate of $\\mathcal{G}_1$) or an extra edge (a false positive). Each such difference contributes $1$ to the SHD.\n\n2.  **Orientation Differences**: This component considers only the edges that are present in both skeletons (i.e., for adjacencies in $A_1 \\cap A_2$). For each common adjacency, an orientation difference occurs if the edge marks (tails and arrowheads) do not match. For CPDAGs, the possible mismatches for a common adjacency $\\{u,v\\}$ are:\n    *   $\\mathcal{G}_1$ has $u-v$ (undirected) and $\\mathcal{G}_2$ has $u \\to v$ or $v \\to u$ (directed).\n    *   $\\mathcal{G}_1$ has $u \\to v$ and $\\mathcal{G}_2$ has $v \\to u$ (reversal).\n    Each such orientation mismatch for a common edge contributes $1$ to the SHD.\n\nThe total SHD is the sum of the counts from these two steps.\n\nWe now apply this definition to compute the SHD between the given reference CPDAG $\\mathcal{G}^{\\star}$ and the learned CPDAG $\\widehat{\\mathcal{G}}$. The set of random variables (genes) is $V = \\{X_1, X_2, X_3\\}$.\n\nThe reference structure $\\mathcal{G}^{\\star}$ has edges $X_1 \\to X_2$ and $X_3 \\to X_2$, with no adjacency between $X_1$ and $X_3$. This configuration, $X_1 \\to X_2 \\leftarrow X_3$, is a v-structure. In a CPDAG, edges forming a v-structure are always directed. Thus, $\\mathcal{G}^{\\star}$ is a graph with exactly these two directed edges.\n*   Adjacency set for $\\mathcal{G}^{\\star}$: $A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$.\n*   Set of directed edges for $\\mathcal{G}^{\\star}$: $D^{\\star} = \\{X_1 \\to X_2, X_3 \\to X_2\\}$.\n\nThe learned structure $\\widehat{\\mathcal{G}}$ has edges $X_2 \\to X_1$ and $X_3 \\to X_1$, with no adjacency between $X_2$ and $X_3$. This configuration, $X_2 \\to X_1 \\leftarrow X_3$, is also a v-structure. Again, these edges must be directed in the CPDAG.\n*   Adjacency set for $\\widehat{\\mathcal{G}}$: $\\widehat{A} = \\{\\{X_2, X_1\\}, \\{X_3, X_1\\}\\}$. Note that $\\{X_2, X_1\\}$ is the same as $\\{X_1, X_2\\}$. Let's write it consistently: $\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$.\n*   Set of directed edges for $\\widehat{\\mathcal{G}}$: $\\widehat{D} = \\{X_2 \\to X_1, X_3 \\to X_1\\}$.\n\nWe proceed with the calculation in two steps.\n\n**Step 1: Compute Adjacency Differences**\nWe compare the adjacency sets $A^{\\star}$ and $\\widehat{A}$.\n$A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$\n$\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$\n\n*   Edges in $A^{\\star}$ but not in $\\widehat{A}$: $A^{\\star} \\setminus \\widehat{A} = \\{\\{X_2, X_3\\}\\}$. This is one edge present in the reference graph but absent in the learned graph (a false negative). This contributes $1$ to the SHD.\n*   Edges in $\\widehat{A}$ but not in $A^{\\star}$: $\\widehat{A} \\setminus A^{\\star} = \\{\\{X_1, X_3\\}\\}$. This is one edge absent in the reference graph but present in the learned graph (a false positive). This contributes $1$ to the SHD.\n\nThe total number of adjacency differences is $1 + 1 = 2$.\n\n**Step 2: Compute Orientation Differences**\nWe examine the orientations of edges in the intersection of the adjacency sets.\n$A_{common} = A^{\\star} \\cap \\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\} \\cap \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\} = \\{\\{X_1, X_2\\}\\}$.\nThere is one common adjacency: $\\{X_1, X_2\\}$.\n\n*   In $\\mathcal{G}^{\\star}$, the edge corresponding to this adjacency is directed: $X_1 \\to X_2$.\n*   In $\\widehat{\\mathcal{G}}$, the edge corresponding to this adjacency is also directed, but in the opposite direction: $X_2 \\to X_1$.\n\nThis constitutes an edge reversal. A reversal is a single orientation error. This contributes $1$ to the SHD.\n\n**Step 3: Calculate the Total SHD**\nThe total SHD is the sum of the adjacency differences and the orientation differences.\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = (\\text{Adjacency Differences}) + (\\text{Orientation Differences})$\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = 2 + 1 = 3$.\n\nTherefore, the Structural Hamming Distance between the reference CPDAG $\\mathcal{G}^{\\star}$ and the learned CPDAG $\\widehat{\\mathcal{G}}$ is $3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        }
    ]
}