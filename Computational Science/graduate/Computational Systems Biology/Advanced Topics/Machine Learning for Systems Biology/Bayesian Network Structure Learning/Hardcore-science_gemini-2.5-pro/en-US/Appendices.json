{
    "hands_on_practices": [
        {
            "introduction": "Score-based methods are a major class of algorithms for learning Bayesian network structures. These methods work by defining a score that measures how well a network structure fits the observed data, and then searching for the structure that maximizes this score. This exercise provides concrete practice with the Bayesian Dirichlet equivalent uniform (BDeu) score, a widely used scoring metric. By calculating the change in the local score when adding a potential regulatory link, you will gain a hands-on understanding of how these algorithms quantify evidence to make decisions about network edges.",
            "id": "3289730",
            "problem": "In computational systems biology, Bayesian network structure learning is often applied to infer gene regulatory interactions from discretized expression profiles. Consider a candidate regulator-target pair where the potential child node $Y$ represents a target gene with $r_{Y} = 3$ discrete expression states $\\{ \\text{low}, \\text{baseline}, \\text{high} \\}$, and the potential parent node $X$ represents a transcriptional regulator with $r_{X} = 2$ discrete activity states $\\{ 0, 1 \\}$. You are given $N = 30$ independent samples of matched gene activity profiles in which $Y$ and $X$ have been discretized as above.\n\nThe observed counts for $Y$ when $X$ is ignored (i.e., no parents for $Y$) are:\n- $n_{Y=\\text{low}} = 14$,\n- $n_{Y=\\text{baseline}} = 9$,\n- $n_{Y=\\text{high}} = 7$.\n\nThe observed contingency table of $Y$ given $X$ when $X$ is considered as a parent of $Y$ is:\n- For $X = 0$: $(n_{Y=\\text{low} \\mid X=0}, n_{Y=\\text{baseline} \\mid X=0}, n_{Y=\\text{high} \\mid X=0}) = (10, 5, 3)$,\n- For $X = 1$: $(n_{Y=\\text{low} \\mid X=1}, n_{Y=\\text{baseline} \\mid X=1}, n_{Y=\\text{high} \\mid X=1}) = (4, 4, 4)$.\n\nAssume a Bayesian Dirichlet equivalent uniform (BDeu) prior for the conditional probability tables of $Y$ that is uniform across states and parent configurations, parameterized by an equivalent sample size $\\alpha > 0$. Use the properties of the Dirichlet-multinomial conjugate model and the modular decomposability of the Bayesian network marginal likelihood.\n\nDefine the change in the BDeu local score for adding the parent $X$ to node $Y$ as the difference between the log marginal likelihood of the data at $Y$ with $X$ as a parent and the log marginal likelihood of the data at $Y$ with no parents:\n$$\n\\Delta S_{\\text{BDeu}}(Y; X) \\equiv \\ln p(\\text{data at } Y \\mid \\text{$X$ is a parent of $Y$}, \\alpha) \\;-\\; \\ln p(\\text{data at } Y \\mid \\text{$Y$ has no parents}, \\alpha).\n$$\n\nCompute $\\Delta S_{\\text{BDeu}}(Y; X)$ as a closed-form analytic expression in terms of $\\alpha$, using the natural logarithm and the gamma function, based solely on the counts provided above. Express the final answer as a single analytic expression in terms of $\\alpha$. Do not approximate numerically.",
            "solution": "The problem requires the calculation of the change in the BDeu local score, $\\Delta S_{\\text{BDeu}}(Y; X)$, when an edge from a potential parent node $X$ is added to a target node $Y$. This quantity is defined as the difference between the log marginal likelihood of the data at node $Y$ for a model with the edge ($M_1$) and a model without the edge ($M_0$):\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(\\text{data at } Y \\mid M_1, \\alpha) - \\ln p(\\text{data at } Y \\mid M_0, \\alpha) $$\nThe general formula for the log marginal likelihood of the data $D_Y$ at a single node $Y$ with a given parent set $Pa(Y)$ is derived from the Dirichlet-multinomial conjugate model. For a node $Y$ with $r_Y$ states and parents with $q_Y$ configurations, the log marginal likelihood is:\n$$ \\ln p(D_Y \\mid Pa(Y), \\alpha) = \\sum_{k=1}^{q_Y} \\left[ \\ln \\Gamma(\\alpha_k) - \\ln \\Gamma(n_k + \\alpha_k) + \\sum_{j=1}^{r_Y} \\left( \\ln \\Gamma(n_{jk} + \\alpha_{jk}) - \\ln \\Gamma(\\alpha_{jk}) \\right) \\right] $$\nwhere $n_{jk}$ is the count of observations for state $j$ of $Y$ and parent configuration $k$, $n_k = \\sum_{j=1}^{r_Y} n_{jk}$ is the total count for parent configuration $k$, and $\\alpha_{jk}$ are the Dirichlet hyperparameters. For a BDeu prior with equivalent sample size $\\alpha$, these are set uniformly: $\\alpha_{jk} = \\alpha / (q_Y r_Y)$. The sum of these hyperparameters for a given parent configuration is $\\alpha_k = \\sum_{j=1}^{r_Y} \\alpha_{jk} = r_Y \\cdot \\frac{\\alpha}{q_Y r_Y} = \\frac{\\alpha}{q_Y}$.\n\nWe first calculate the log marginal likelihood for the model $M_0$ where $Y$ has no parents.\nIn this case, $Pa(Y) = \\emptyset$, so there is only one parent configuration, $q_Y = 1$. The node $Y$ has $r_Y = 3$ states.\nThe BDeu hyperparameters are $\\alpha_j = \\alpha / (1 \\cdot 3) = \\alpha/3$ for $j \\in \\{1, 2, 3\\}$.\nThe total equivalent sample size is $\\alpha_0 = \\sum_{j=1}^{3} \\alpha_j = 3 (\\alpha/3) = \\alpha$.\nThe observed counts are $n_{Y=\\text{low}} = 14$, $n_{Y=\\text{baseline}} = 9$, and $n_{Y=\\text{high}} = 7$. We denote these as $n_1=14$, $n_2=9$, $n_3=7$. The total sample size is $n = 14+9+7 = 30$.\nThe log marginal likelihood for $M_0$ is:\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\sum_{j=1}^{3} \\left( \\ln \\Gamma(n_j + \\frac{\\alpha}{3}) - \\ln \\Gamma(\\frac{\\alpha}{3}) \\right) $$\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\ln \\Gamma(14 + \\frac{\\alpha}{3}) + \\ln \\Gamma(9 + \\frac{\\alpha}{3}) + \\ln \\Gamma(7 + \\frac{\\alpha}{3}) - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) $$\n\nNext, we calculate the log marginal likelihood for the model $M_1$ where $X$ is the parent of $Y$.\nThe parent node $X$ has $r_X = 2$ states, so there are $q_Y = 2$ parent configurations. The node $Y$ still has $r_Y = 3$ states.\nThe BDeu hyperparameters are $\\alpha_{jk} = \\alpha / (2 \\cdot 3) = \\alpha/6$.\nFor each parent configuration $k$, the equivalent sample size is $\\alpha_k = \\alpha/q_Y = \\alpha/2$.\nThe total log marginal likelihood is the sum of terms for each parent configuration.\n\nFor the parent state $X=0$ (let's call this configuration $k=1$):\nThe counts are $n_{11}=10$, $n_{12}=5$, $n_{13}=3$. The total count is $n_1 = 10+5+3 = 18$.\nThe contribution to the score is:\n$$ \\ln p_1 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\nFor the parent state $X=1$ (configuration $k=2$):\nThe counts are $n_{21}=4$, $n_{22}=4$, $n_{23}=4$. The total count is $n_2 = 4+4+4 = 12$.\nThe contribution to the score is:\n$$ \\ln p_2 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\nThe total log marginal likelihood for $M_1$ is $\\ln p(D_Y \\mid M_1, \\alpha) = \\ln p_1 + \\ln p_2$:\n$$ \\ln p(D_Y \\mid M_1, \\alpha) = 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\nFinally, we compute the difference $\\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(D_Y \\mid M_1, \\alpha) - \\ln p(D_Y \\mid M_0, \\alpha)$:\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\left[ 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18+\\frac{\\alpha}{2}) - \\ln \\Gamma(12+\\frac{\\alpha}{2}) + \\dots - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) \\right] - \\left[ \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30+\\alpha) + \\dots - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) \\right] $$\nRearranging the terms gives the final expression:\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right) $$\nThis expression represents the change in log-evidence for adding the regulatory link from $X$ to $Y$.",
            "answer": "$$\n\\boxed{\\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right)}\n$$"
        },
        {
            "introduction": "Once a network structure has been learned, a critical step is to evaluate its accuracy, especially when a ground-truth network is available for comparison. The Structural Hamming Distance (SHD) is a standard and intuitive metric for quantifying the differences between two network structures, particularly their Markov equivalence class representatives (CPDAGs). This problem will challenge you to first formally define and then compute the SHD, giving you a clear grasp of how structural errors—such as missing edges, extra edges, and reversed arrows—are counted.",
            "id": "3289677",
            "problem": "In computational systems biology, gene regulatory network inference with Bayesian network structure learning often evaluates learned structures against a reference using distances defined on equivalence-class representatives. Consider random variables representing gene expression for $3$ genes, $X_1$, $X_2$, and $X_3$. Let the reference structure be the Completed Partially Directed Acyclic Graph (CPDAG) $\\mathcal{G}^{\\star}$ over $\\{X_1,X_2,X_3\\}$ with edges $X_1 \\to X_2$ and $X_3 \\to X_2$, and no adjacency between $X_1$ and $X_3$. Let the learned CPDAG be $\\widehat{\\mathcal{G}}$ with edges $X_2 \\to X_1$ and $X_3 \\to X_1$, and no adjacency between $X_2$ and $X_3$. \n\nStarting from the core definitions of Bayesian networks and Markov equivalence classes, first provide a formal definition of Structural Hamming Distance (SHD) between two CPDAGs in terms of the minimal number of single-edge edit operations that reconcile both the adjacency sets and the endpoint marks (tails and arrowheads). Then, using your definition, compute the SHD between $\\mathcal{G}^{\\star}$ and $\\widehat{\\mathcal{G}}$. Express your final answer as an integer with no rounding required.",
            "solution": "The problem requires a formal definition of the Structural Hamming Distance (SHD) between two Completed Partially Directed Acyclic Graphs (CPDAGs) and its subsequent computation for a given reference structure $\\mathcal{G}^{\\star}$ and a learned structure $\\widehat{\\mathcal{G}}$.\n\nFirst, we establish the context. A Bayesian network is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph (DAG). Multiple DAGs can represent the same set of conditional independencies; such a set of DAGs forms a Markov equivalence class. A CPDAG is a canonical graphical representation for a Markov equivalence class. It consists of a set of vertices and a set of edges, where edges can be either directed (compelled) or undirected. A directed edge $X \\to Y$ in a CPDAG signifies that this orientation is present in all DAGs within the equivalence class. An undirected edge $X-Y$ indicates that the equivalence class contains DAGs with $X \\to Y$ as well as DAGs with $Y \\to X$.\n\nThe Structural Hamming Distance (SHD) is a metric used to quantify the difference between two graphs, often a learned graph and a true or reference graph. For two CPDAGs, $\\mathcal{G}_1$ and $\\mathcal{G}_2$, defined on the same set of vertices $V$, the SHD is the total number of fundamental graph edit operations required to transform $\\mathcal{G}_1$ into $\\mathcal{G}_2$. These operations are categorized into differences in adjacencies (the skeleton of the graph) and differences in edge orientations.\n\nFormally, the SHD is defined as follows:\nLet $\\mathcal{G}_1$ and $\\mathcal{G}_2$ be two CPDAGs. The SHD is the sum of the costs of reconciling their differences, calculated in two steps:\n1.  **Adjacency Differences**: This component counts the number of edges that are present in one graph's skeleton but not the other's. Let $A_1$ and $A_2$ be the sets of adjacencies (undirected edges representing neighborhood relationships) for $\\mathcal{G}_1$ and $\\mathcal{G}_2$, respectively. The number of adjacency differences is the size of the symmetric difference of these sets, $|A_1 \\Delta A_2| = |(A_1 \\setminus A_2) \\cup (A_2 \\setminus A_1)|$. Each element in this symmetric difference corresponds to either a missing edge (a false negative if $\\mathcal{G}_2$ is the estimate of $\\mathcal{G}_1$) or an extra edge (a false positive). Each such difference contributes $1$ to the SHD.\n\n2.  **Orientation Differences**: This component considers only the edges that are present in both skeletons (i.e., for adjacencies in $A_1 \\cap A_2$). For each common adjacency, an orientation difference occurs if the edge marks (tails and arrowheads) do not match. For CPDAGs, the possible mismatches for a common adjacency $\\{u,v\\}$ are:\n    *   $\\mathcal{G}_1$ has $u-v$ (undirected) and $\\mathcal{G}_2$ has $u \\to v$ or $v \\to u$ (directed).\n    *   $\\mathcal{G}_1$ has $u \\to v$ and $\\mathcal{G}_2$ has $v \\to u$ (reversal).\n    Each such orientation mismatch for a common edge contributes $1$ to the SHD.\n\nThe total SHD is the sum of the counts from these two steps.\n\nWe now apply this definition to compute the SHD between the given reference CPDAG $\\mathcal{G}^{\\star}$ and the learned CPDAG $\\widehat{\\mathcal{G}}$. The set of random variables (genes) is $V = \\{X_1, X_2, X_3\\}$.\n\nThe reference structure $\\mathcal{G}^{\\star}$ has edges $X_1 \\to X_2$ and $X_3 \\to X_2$, with no adjacency between $X_1$ and $X_3$. This configuration, $X_1 \\to X_2 \\leftarrow X_3$, is a v-structure. In a CPDAG, edges forming a v-structure are always directed. Thus, $\\mathcal{G}^{\\star}$ is a graph with exactly these two directed edges.\n*   Adjacency set for $\\mathcal{G}^{\\star}$: $A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$.\n*   Set of directed edges for $\\mathcal{G}^{\\star}$: $D^{\\star} = \\{X_1 \\to X_2, X_3 \\to X_2\\}$.\n\nThe learned structure $\\widehat{\\mathcal{G}}$ has edges $X_2 \\to X_1$ and $X_3 \\to X_1$, with no adjacency between $X_2$ and $X_3$. This configuration, $X_2 \\to X_1 \\leftarrow X_3$, is also a v-structure. Again, these edges must be directed in the CPDAG.\n*   Adjacency set for $\\widehat{\\mathcal{G}}$: $\\widehat{A} = \\{\\{X_2, X_1\\}, \\{X_3, X_1\\}\\}$. Note that $\\{X_2, X_1\\}$ is the same as $\\{X_1, X_2\\}$. Let's write it consistently: $\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$.\n*   Set of directed edges for $\\widehat{\\mathcal{G}}$: $\\widehat{D} = \\{X_2 \\to X_1, X_3 \\to X_1\\}$.\n\nWe proceed with the calculation in two steps.\n\n**Step 1: Compute Adjacency Differences**\nWe compare the adjacency sets $A^{\\star}$ and $\\widehat{A}$.\n$A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$\n$\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$\n\n*   Edges in $A^{\\star}$ but not in $\\widehat{A}$: $A^{\\star} \\setminus \\widehat{A} = \\{\\{X_2, X_3\\}\\}$. This is one edge present in the reference graph but absent in the learned graph (a false negative). This contributes $1$ to the SHD.\n*   Edges in $\\widehat{A}$ but not in $A^{\\star}$: $\\widehat{A} \\setminus A^{\\star} = \\{\\{X_1, X_3\\}\\}$. This is one edge absent in the reference graph but present in the learned graph (a false positive). This contributes $1$ to the SHD.\n\nThe total number of adjacency differences is $1 + 1 = 2$.\n\n**Step 2: Compute Orientation Differences**\nWe examine the orientations of edges in the intersection of the adjacency sets.\n$A_{common} = A^{\\star} \\cap \\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\} \\cap \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\} = \\{\\{X_1, X_2\\}\\}$.\nThere is one common adjacency: $\\{X_1, X_2\\}$.\n\n*   In $\\mathcal{G}^{\\star}$, the edge corresponding to this adjacency is directed: $X_1 \\to X_2$.\n*   In $\\widehat{\\mathcal{G}}$, the edge corresponding to this adjacency is also directed, but in the opposite direction: $X_2 \\to X_1$.\n\nThis constitutes an edge reversal. A reversal is a single orientation error. This contributes $1$ to the SHD.\n\n**Step 3: Calculate the Total SHD**\nThe total SHD is the sum of the adjacency differences and the orientation differences.\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = (\\text{Adjacency Differences}) + (\\text{Orientation Differences})$\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = 2 + 1 = 3$.\n\nTherefore, the Structural Hamming Distance between the reference CPDAG $\\mathcal{G}^{\\star}$ and the learned CPDAG $\\widehat{\\mathcal{G}}$ is $3$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "Applying structure learning algorithms to real-world biological data is fraught with potential pitfalls, and a naive analysis can often lead to incorrect conclusions. This thought exercise delves into the critical issue of selection bias, a common problem in fields like single-cell transcriptomics where data is often filtered based on specific criteria. By analyzing how conditioning on a 'collider' variable can induce spurious dependencies, you will develop the crucial skill of identifying and reasoning about the causal implications of data-collection procedures.",
            "id": "3289712",
            "problem": "In single-cell transcriptomics for computational systems biology, suppose a laboratory computes a scalar cell cycle score $C$ from a subset of gene expression measurements, and then performs gating: only cells with $C$ above a threshold are kept for analysis. You are tasked with structure learning of a causal Bayesian network over a pair of cell cycle–regulated genes $X$ and $Y$, where the downstream cell cycle score $C$ is constructed from their expression. The aim is to recover the pre-gating causal gene graph $G$ over $\\{X, Y\\}$.\n\nAssume the following data generating process in the unselected population. The genes $X$ and $Y$ are regulated independently by distinct upstream factors (for concreteness, let $X \\sim p_X$ and $Y \\sim p_Y$ with $X \\perp Y$), and the score $C$ is computed from their expression via a measurable function with noise, $C = g(X, Y, \\varepsilon_C)$, where $\\varepsilon_C \\perp (X, Y)$ and $g$ is such that the conditional density $p(C \\mid X, Y)$ depends jointly on both $X$ and $Y$. The cell selection indicator $S$ is defined by $S = \\mathbf{1}\\{C > c_\\star\\}$ for a fixed threshold $c_\\star \\in \\mathbb{R}$, and only records with $S=1$ are retained. The true pre-gating causal diagram among the measured variables is the directed acyclic graph (DAG) with arrows $X \\to C \\leftarrow Y$ and $C \\to S$, and with no edge between $X$ and $Y$.\n\nYou plan to learn a Bayesian network (BN) structure over $\\{X, Y\\}$ from the gated dataset using standard score-based methods such as the Bayesian Information Criterion (BIC), or constraint-based tests of conditional independence. The foundational facts you may use are: the BN factorization $p(x, y, c) = p(x)\\,p(y)\\,p(c \\mid x, y)$; the definition of a collider ($X \\to C \\leftarrow Y$); and $d$-separation implications that conditioning on a collider or its descendant induces dependence between otherwise independent parents.\n\nQuestion: Which of the following procedures yields a consistent recovery of the pre-gating causal graph $G$ over $\\{X, Y\\}$ (specifically, correctly refusing to add a spurious edge between $X$ and $Y$), under the stated assumptions?\n\nA. Fit the BN on the gated data using only $\\{X, Y\\}$, omitting $C$ because it is involved in selection, and rely on sparsity-penalized scores such as BIC to avoid false edges.\n\nB. Augment the model with a selection diagram by explicitly including $C$ and a selection node $S$ with $C \\to S$, and learn using a weighted likelihood on the gated records, where each sample $i$ is reweighted by $1/p(S_i=1 \\mid C_i)$ so that the objective is proportional to $\\sum_{i: S_i=1} \\left[\\log p(X_i, Y_i, C_i) - \\log p(S_i=1 \\mid C_i)\\right]$. Then project the learned pre-selection structure onto $\\{X, Y\\}$ to obtain $G$.\n\nC. Include $C$ as a covariate and condition on $C$ in all independence tests between $X$ and $Y$, because $C$ is the source of their apparent association.\n\nD. Randomly subsample within the gated set to match the marginal distribution of $C$ to that of the pre-gating population and then run an unadjusted BN learner on $\\{X, Y\\}$ from the subsampled data.\n\nSelect the single best option.",
            "solution": "The problem describes a classic case of selection bias, also known as collider stratification bias. The true causal structure in the unselected population is $X \\to C \\leftarrow Y$, which means $X$ and $Y$ are marginally independent. The goal is to recover this independence (no edge between $X$ and $Y$) from data that has been selected based on a descendant of the collider $C$. Conditioning on a collider or its descendant opens the path between the collider's parents, inducing a spurious statistical association. Therefore, in the gated dataset (where we have effectively conditioned on $S=1$, a descendant of $C$), $X$ and $Y$ will appear to be dependent. A consistent structure learning algorithm must correct for this bias.\n\nLet's evaluate the options:\n\n**A. Fit the BN on the gated data using only $\\{X, Y\\}$...**\nThis is the naive approach. Since $X$ and $Y$ are dependent in the gated dataset, any standard structure learning algorithm (score-based or constraint-based) will incorrectly infer an edge between them, especially with a large sample size. A penalty like BIC cannot overcome this systematic bias. This procedure is not consistent.\n\n**B. Augment the model... and learn using a weighted likelihood...**\nThis option proposes to explicitly model the selection process and use a modified objective function to correct for the bias. This is the correct high-level strategy. Methods like Inverse Probability Weighting (IPW) are designed for exactly this purpose: to estimate properties of a full population from a biased sample by reweighting the observed samples. By accounting for why certain data points were selected (i.e., modeling $p(S=1 \\mid C)$), one can recover the unbiased relationships in the pre-selection population. Among the given choices, this is the only one that describes a principled and statistically valid approach to handle selection bias.\n\n**C. Include $C$ as a covariate and condition on $C$ in all independence tests...**\nThis approach is fundamentally incorrect. Conditioning on the collider $C$ itself is the canonical way to *induce* dependence between its independent parents ($X$ and $Y$) via the \"explaining away\" effect. Therefore, testing for $X \\perp Y \\mid C$ would lead to the false conclusion that $X$ and $Y$ are dependent and should have an edge.\n\n**D. Randomly subsample within the gated set to match the marginal distribution of $C$...**\nThis procedure is impossible and misguided. It's impossible because the gated set only contains cells where $C > c_\\star$, so one cannot possibly reconstruct the pre-gating distribution of $C$ (which includes values $C \\le c_\\star$) by subsampling. It's misguided because the bias arises from a distortion of the joint distribution $p(X, Y, C)$, not just the marginal distribution of $C$. Correcting the marginal of $C$ would not remove the spurious dependence between $X$ and $Y$.\n\nTherefore, option B is the only one that proposes a conceptually correct method for recovering the true causal structure in the presence of collider stratification bias.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}