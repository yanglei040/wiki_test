{
    "hands_on_practices": [
        {
            "introduction": "在我们运用复杂的神经网络来模拟细胞动态之前，掌握控制系统的基本物理定律至关重要。本练习将回归化学动力学的基础，展示如何从反应化学计量构建常微分方程（ODE）系统。您将学习如何运用线性代数来揭示守恒量——这是任何有效模型都必须遵守的关键物理约束，为构建基于物理知识的神经网络模型打下坚实基础。",
            "id": "3333114",
            "problem": "考虑一个单细胞生化网络中的最小复合物形成模块，该模块被建模为一个神经微分方程 (NODE)。存在三种分子：浓度为 $x_{1}(t)$ 的游离蛋白 $M_{1}$，浓度为 $x_{2}(t)$ 的游离配体 $M_{2}$，以及浓度为 $x_{3}(t)$ 的结合复合物 $C$。该系统在充分混合、等温和恒定体积的条件下发生两个反应，遵循质量作用动力学：\n\n反应 $1$：$M_{1} + M_{2} \\rightarrow C$，速率为 $v_{1}(x) = k_{1}\\,x_{1}(t)\\,x_{2}(t)$，\n\n反应 $2$：$C \\rightarrow M_{1} + M_{2}$，速率为 $v_{2}(x) = k_{2}\\,x_{3}(t)$，\n\n其中 $k_{1} > 0$ 和 $k_{2} > 0$ 是速率常数。NODE 漂移 $f_{\\theta}(x,t)$ 受化学计量质量平衡的约束，通过 $f_{\\theta}(x,t) = S\\,v(x)$ 表示，其中 $S$ 是化学计量矩阵，而 $v(x) = \\big(v_{1}(x), v_{2}(x)\\big)^{\\top}$。\n\n任务：\n1. 以分子数量守恒和质量作用动力学为基本依据，为该双反应体系构建化学计量矩阵 $S$，并以 $\\dot{x}(t) = S\\,v(x(t))$ 的形式推导 $x(t) = \\big(x_{1}(t), x_{2}(t), x_{3}(t)\\big)^{\\top}$ 的常微分方程。\n2. 通过构建 $S^{\\top}$ 零空间的基，识别所有线性守恒部分，并给出一组向量 $\\{c^{(j)}\\}$，使得每个 $c^{(j)\\top} S = 0$。对于每个基向量 $c^{(j)}$，从基本原理出发验证 $\\frac{d}{dt}\\big(c^{(j)\\top} x(t)\\big) = 0$，因此对于解的定义域中的所有 $t$，有 $c^{(j)\\top} x(t) = c^{(j)\\top} x(0)$。\n3. 最后，给定初始浓度 $x_{1}(0) = 3\\,\\mathrm{mM}$，$x_{2}(0) = 2\\,\\mathrm{mM}$ 和 $x_{3}(0) = 1\\,\\mathrm{mM}$，计算所有始终保持不变的独立守恒量 $c^{(j)\\top} x(t)$ 的数值。最终数值以 $\\mathrm{mM}$ 为单位表示。无需四舍五入；请提供精确值。\n\n你的最终答案必须是写成单个 $\\LaTeX$ 行矩阵的不变量值行向量。",
            "solution": "该问题要求对一个简单的生化反应系统进行三部分分析。我们将遵循化学动力学和线性代数的原理，依次完成每个任务。\n\n**任务1：构建化学计量矩阵并推导常微分方程组**\n\n该系统涉及 $3$ 种分子（$M_{1}$、$M_{2}$、$C$）和 $2$ 个反应。系统状态由浓度向量 $x(t) = \\big(x_{1}(t), x_{2}(t), x_{3}(t)\\big)^{\\top}$ 描述。化学计量矩阵（记为 $S$）将反应速率与物种浓度的变化率联系起来。$S$ 的维度是（物种数）$\\times$（反应数），在本例中为 $3 \\times 2$。$S$ 的每一列代表在一个反应中每种分子的净变化量。\n\n反应如下：\n反应 $1$：$M_{1} + M_{2} \\rightarrow C$\n反应 $2$：$C \\rightarrow M_{1} + M_{2}$\n\n对于反应1，一个 $M_{1}$ 分子和一个 $M_{2}$ 分子被消耗，生成一个 $C$ 分子。相应的化学计量向量（$S$ 的第一列）因此是：\n$$ S_{\\cdot 1} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix} $$\n\n对于反应2，一个 $C$ 分子解离，生成一个 $M_{1}$ 分子和一个 $M_{2}$ 分子。相应的化学计量向量（$S$ 的第二列）是：\n$$ S_{\\cdot 2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\n\n合并这些列，我们构建出完整的化学计量矩阵 $S$：\n$$ S = \\begin{pmatrix} -1  1 \\\\ -1  1 \\\\ 1  -1 \\end{pmatrix} $$\n\n速率向量 $v(x)$ 由 $v(x) = \\big(v_{1}(x), v_{2}(x)\\big)^{\\top}$ 给出，其中 $v_{1}(x) = k_{1}\\,x_{1}(t)\\,x_{2}(t)$ 且 $v_{2}(x) = k_{2}\\,x_{3}(t)$。常微分方程组 (ODEs) 由关系式 $\\dot{x}(t) = S\\,v(x(t))$ 给出。\n\n执行矩阵向量乘法：\n$$ \\dot{x}(t) = \\begin{pmatrix} \\dot{x}_{1}(t) \\\\ \\dot{x}_{2}(t) \\\\ \\dot{x}_{3}(t) \\end{pmatrix} = \\begin{pmatrix} -1  1 \\\\ -1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} v_{1}(x) \\\\ v_{2}(x) \\end{pmatrix} = \\begin{pmatrix} -v_{1}(x) + v_{2}(x) \\\\ -v_{1}(x) + v_{2}(x) \\\\ v_{1}(x) - v_{2}(x) \\end{pmatrix} $$\n\n代入给定的速率定律，我们得到显式常微分方程组：\n$$ \\dot{x}_{1}(t) = -k_{1}\\,x_{1}(t)\\,x_{2}(t) + k_{2}\\,x_{3}(t) $$\n$$ \\dot{x}_{2}(t) = -k_{1}\\,x_{1}(t)\\,x_{2}(t) + k_{2}\\,x_{3}(t) $$\n$$ \\dot{x}_{3}(t) = k_{1}\\,x_{1}(t)\\,x_{2}(t) - k_{2}\\,x_{3}(t) $$\n\n**任务2：识别线性守恒部分**\n\n线性守恒部分是物种浓度的线性组合，其值随时间保持不变。这样的量由一个向量 $c$ 定义，使得标量 $c^{\\top}x(t)$ 是一个常数。这要求其时间导数为零：\n$$ \\frac{d}{dt}\\big(c^{\\top}x(t)\\big) = c^{\\top}\\dot{x}(t) = 0 $$\n代入常微分方程组 $\\dot{x}(t) = S\\,v(x(t))$，我们得到：\n$$ c^{\\top}S\\,v(x(t)) = 0 $$\n为了使该式对任何有效的浓度值（因此对任何有效的速率向量 $v(x)$）都成立，向量 $c^{\\top}S$ 必须是零向量。这等价于 $S^{\\top}c = 0$。因此，定义守恒部分的向量 $c$ 构成了 $S$ 的左零空间，或者等价地，$S^{\\top}$ 的零空间。\n\n我们计算 $S^{\\top}$ 的零空间：\n$$ S^{\\top} = \\begin{pmatrix} -1  -1  1 \\\\ 1  1  -1 \\end{pmatrix} $$\n我们寻找向量 $c = (c_{1}, c_{2}, c_{3})^{\\top}$ 使得 $S^{\\top}c = 0$：\n$$ \\begin{pmatrix} -1  -1  1 \\\\ 1  1  -1 \\end{pmatrix} \\begin{pmatrix} c_{1} \\\\ c_{2} \\\\ c_{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这得出了唯一的独立方程 $-c_{1} - c_{2} + c_{3} = 0$，即 $c_{3} = c_{1} + c_{2}$。该方程组有两个自由变量。我们可以选择 $c_{1}$ 和 $c_{2}$ 来构建零空间的基。\n\n令 $c_{1} = 1$ 和 $c_{2} = 0$。则 $c_{3} = 1$。这给出了第一个基向量：\n$$ c^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n令 $c_{1} = 0$ 和 $c_{2} = 1$。则 $c_{3} = 1$。这给出了第二个基向量：\n$$ c^{(2)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\n向量集合为 $\\{c^{(1)}, c^{(2)}\\}$。零空间的维度为 $2$，表示存在两个独立的线性守恒部分。\n\n让我们从基本原理出发，验证这些向量确实导出了守恒量。\n对于 $c^{(1)\\top}x(t) = x_{1}(t) + x_{3}(t)$：\n$$ \\frac{d}{dt}\\big(x_{1}(t) + x_{3}(t)\\big) = \\dot{x}_{1}(t) + \\dot{x}_{3}(t) = \\big(-k_{1}x_{1}x_{2} + k_{2}x_{3}\\big) + \\big(k_{1}x_{1}x_{2} - k_{2}x_{3}\\big) = 0 $$\n因此，$x_{1}(t) + x_{3}(t)$ 是一个常数。这个量代表蛋白质 $M_{1}$ 的总浓度，包括游离形式和在复合物 $C$ 中结合的形式。\n\n对于 $c^{(2)\\top}x(t) = x_{2}(t) + x_{3}(t)$：\n$$ \\frac{d}{dt}\\big(x_{2}(t) + x_{3}(t)\\big) = \\dot{x}_{2}(t) + \\dot{x}_{3}(t) = \\big(-k_{1}x_{1}x_{2} + k_{2}x_{3}\\big) + \\big(k_{1}x_{1}x_{2} - k_{2}x_{3}\\big) = 0 $$\n因此，$x_{2}(t) + x_{3}(t)$ 也是一个常数。这个量代表配体 $M_{2}$ 的总浓度，包括游离形式和结合形式。\n\n守恒意味着对于所有 $t$，都有 $c^{(j)\\top}x(t) = c^{(j)\\top}x(0)$。\n\n**任务3：计算不变量值**\n\n这些守恒量的值由初始条件决定：$x_{1}(0) = 3\\,\\mathrm{mM}$，$x_{2}(0) = 2\\,\\mathrm{mM}$ 和 $x_{3}(0) = 1\\,\\mathrm{mM}$。\n\n对于第一个守恒量 $c^{(1)\\top}x(t)$：\n$$ c^{(1)\\top}x(t) = x_{1}(t) + x_{3}(t) = x_{1}(0) + x_{3}(0) = 3 + 1 = 4 $$\n此不变量的值为 $4\\,\\mathrm{mM}$。\n\n对于第二个守恒量 $c^{(2)\\top}x(t)$：\n$$ c^{(2)\\top}x(t) = x_{2}(t) + x_{3}(t) = x_{2}(0) + x_{3}(0) = 2 + 1 = 3 $$\n此不变量的值为 $3\\,\\mathrm{mM}$。\n\n独立守恒量的数值是 $4$ 和 $3$。最终答案应为包含这些值的行矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4  3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "神经ODE的强大之处在于其能够直接从数据中学习复杂的动态，但这需要通过ODE求解器计算梯度。本练习将指导您实现伴随敏度方法（adjoint sensitivity method），这是一种高效计算这些梯度的核心算法。通过亲手编写伴随法并将其与有限差分法进行比较，您将对神经ODE训练的可行性原理获得深刻的实践理解，并体会到数值精度带来的细微影响。",
            "id": "3333095",
            "problem": "考虑一个简单的双物种细胞系统，其中状态向量 $x(t) \\in \\mathbb{R}^2$ 代表信使核糖核酸 (mRNA) 和蛋白质的无量纲化浓度。该系统根据常微分方程 $dx/dt = f_{\\theta}(x,t)$ 演化，其中 $f_{\\theta}$ 由一个小型前馈神经网络参数化，旨在近似非线性反应动力学。时间单位为秒。该神经网络定义如下：隐藏层预激活为 $z = W_1 x + b_1$，隐藏层激活为 $h = \\tanh(z)$，输出为\n$$\nf_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x,\n$$\n其中 $W_1 \\in \\mathbb{R}^{3 \\times 2}$，$b_1 \\in \\mathbb{R}^{3}$，$W_2 \\in \\mathbb{R}^{2 \\times 3}$，$b_2 \\in \\mathbb{R}^{2}$，$k_d \\in \\mathbb{R}^{2}$，且 $\\odot$ 表示逐元素乘法。损失定义在最终时间 $T$ 的终端状态上：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T; \\theta) - x_{\\mathrm{target}} \\|_2^2.\n$$\n初始条件为 $x(0) = x_0$。目标是使用伴随法计算梯度 $d\\mathcal{L}/d\\theta$，并将其与梯度的有限差分近似进行比较，以评估正确性以及求解器离散化对偏差的影响。\n\n从微积分的链式法则和常微分方程 (ODE) 的伴随灵敏度定义出发，推导必要的关系，以实现一种与上述连续时间神经 ODE 模型一致的、基于伴随法的梯度计算 $d\\mathcal{L}/d\\theta$。您的程序必须：\n- 使用固定步长的显式四阶 Runge–Kutta 方法积分正向动力学。\n- 使用一阶显式方法沿时间反向进行伴随积分，其中雅可比矩阵 $ \\partial f_{\\theta} / \\partial x $ 沿保存的正向轨迹计算。\n- 通过对瞬时灵敏度 $a(t)^\\top \\, \\partial f_{\\theta} / \\partial \\theta$ 进行时间积分来累积参数梯度，其中 $a(t)$ 表示伴随状态，并通过对参数扰动重新积分正向动力学计算的中心有限差分来验证结果。\n\n您的实现必须完全自包含，不调用任何外部数据。正向模型和所有参数的数值规定如下：\n- 模型参数 $\\theta$：\n  - $W_1 = \\begin{bmatrix} 0.8  -0.5 \\\\ 0.3  0.9 \\\\ -0.7  0.2 \\end{bmatrix}$，\n  - $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，\n  - $W_2 = \\begin{bmatrix} 0.5  -0.3  0.1 \\\\ -0.4  0.6  -0.2 \\end{bmatrix}$，\n  - $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\end{bmatrix}$，\n  - $k_d = \\begin{bmatrix} 0.3 \\\\ 0.5 \\end{bmatrix}$。\n- 初始状态和目标：\n  - $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.2 \\end{bmatrix}$，\n  - $x_{\\mathrm{target}} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$。\n- 最终时间：$T = 2.0$ 秒。\n\n您必须使用参数缩放因子 $s$ 来探索神经组件中近线性的区域，方法是按 $\\{ W_1, b_1, W_2, b_2 \\} \\mapsto s \\cdot \\{ W_1, b_1, W_2, b_2 \\}$ 缩放权重和偏置，同时保持 $k_d$ 不变。\n\n定义以下测试用例套件，每个用例由一个元组 $(N, \\epsilon, \\tau, s)$ 指定，其中 $N$ 是正向积分的 Runge–Kutta 步数（固定步长 $\\Delta t = T/N$），$\\epsilon$ 是有限差分步长，$\\tau$ 是梯度一致性的绝对容差，$s$ 是缩放因子：\n- 情况 1：$(N=500, \\epsilon=10^{-6}, \\tau=2 \\times 10^{-2}, s=1.0)$，\n- 情况 2：$(N=50, \\epsilon=10^{-6}, \\tau=2 \\times 10^{-2}, s=1.0)$，\n- 情况 3：$(N=20, \\epsilon=10^{-6}, \\tau=5 \\times 10^{-2}, s=1.0)$，\n- 情况 4：$(N=60, \\epsilon=10^{-6}, \\tau=1 \\times 10^{-2}, s=0.05)$。\n\n对每种情况，计算：\n- 使用连续伴随法和伴随方程的反向显式积分计算的基于伴随法的梯度 $d\\mathcal{L}/d\\theta$。\n- $d\\mathcal{L}/d\\theta$ 的中心有限差分近似，其中对 $\\theta$ 的每个标量分量施加扰动 $\\pm \\epsilon$（在扰动前应用缩放 $s$）。\n- 两个梯度向量之间的最大绝对分量差。\n\n您的程序应生成单行输出，包含一个在方括号中的逗号分隔列表，其中每个条目是一个布尔值，指示该情况下的最大绝对差是否严格小于规定的容差 $\\tau$。例如，输出格式必须与 $[b_1,b_2,b_3,b_4]$ 完全一样，其中每个 $b_i$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$，没有空格。所有数值答案必须是无单位的标量，除了 $T$（单位为秒）。本问题中不使用角度。所有结果都是根据上述数据确定的，不需要任何用户输入。",
            "solution": "问题要求计算损失函数关于神经常微分方程 (ODE) 模型参数的梯度。该梯度将使用连续伴随灵敏度方法计算，并与有限差分近似进行验证。\n\n### 基于原理的设计：伴随灵敏度分析\n\n该系统由一个 ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ 描述，初始条件为 $x(0) = x_0$。目标是计算一个依赖于最终时间 $T$ 状态的损失函数 $\\mathcal{L}(\\theta) = g(x(T))$ 的梯度。根据链式法则，该梯度为：\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial g}{\\partial x(T)} \\frac{dx(T)}{d\\theta}\n$$\n$\\frac{dx(T)}{d\\theta}$ 项表示最终状态对参数变化的灵敏度。其直接计算需要为每个参数积分一个灵敏度方程，这在计算上可能非常昂贵。伴随法提供了一种更高效的替代方案，尤其是在参数数量众多的情况下。\n\n伴随法引入一个伴随状态向量 $a(t) \\in \\mathbb{R}^n$，它是以下终端值问题的解：\n$$\n\\frac{da}{dt} = - \\left( \\frac{\\partial f_{\\theta}}{\\partial x} \\right)^\\top a(t) \\quad \\text{其中} \\quad a(T) = \\left( \\frac{\\partial g}{\\partial x(T)} \\right)^\\top\n$$\n损失函数关于参数 $\\theta$ 的梯度则由以下积分给出：\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\int_0^T a(t)^\\top \\frac{\\partial f_{\\theta}(x(t), t)}{\\partial \\theta} dt\n$$\n此公式需要对伴随 ODE 进行一次时间反向积分，并重用通过对原始 ODE 进行时间正向积分获得的状态轨迹 $x(t)$。\n\n### 在特定神经 ODE 模型上的应用\n\n问题给出了动力学、参数和损失函数的具体形式。\n\n1.  **系统动力学**：状态为 $x(t) \\in \\mathbb{R}^2$。动力学函数为 $f_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x$，其中 $h = \\tanh(z)$ 且 $z = W_1 x + b_1$。参数为 $\\theta = \\{W_1, b_1, W_2, b_2, k_d\\}$。\n\n2.  **损失函数与伴随终端条件**：损失为 $\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T) - x_{\\mathrm{target}} \\|_2^2$。损失关于最终状态的梯度是 $\\frac{\\partial \\mathcal{L}}{\\partial x(T)} = (x(T) - x_{\\mathrm{target}})^\\top$。因此，伴随状态 $a(t) \\in \\mathbb{R}^2$ 的终端条件是：\n    $$\n    a(T) = x(T) - x_{\\mathrm{target}}\n    $$\n\n3.  **动力学的雅可比矩阵 ($\\partial f_{\\theta} / \\partial x$)**：为了定义伴随 ODE，我们首先需要 $f_{\\theta}$ 关于状态 $x$ 的雅可比矩阵。使用链式法则：\n    $$\n    \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial x} - \\frac{\\partial (k_d \\odot x)}{\\partial x}\n    $$\n    导数为 $\\frac{\\partial z}{\\partial x} = W_1$，$\\frac{\\partial(k_d \\odot x)}{\\partial x} = \\text{diag}(k_d)$，以及 $\\frac{\\partial h}{\\partial z} = \\text{diag}(1 - \\tanh^2(z)) = \\text{diag}(1 - h^2)$，其中平方是逐元素的。完整的雅可比矩阵是一个 $2 \\times 2$ 矩阵：\n    $$\n    J(x, \\theta) = \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\text{diag}(1 - h^2) W_1 - \\text{diag}(k_d)\n    $$\n    因此伴随 ODE 为 $\\frac{da}{dt} = -J(x(t), \\theta)^\\top a(t)$。\n\n4.  **参数梯度 ($\\partial f_{\\theta} / \\partial \\theta$)**：我们需要 $f_{\\theta}$ 关于每个参数块的偏导数。\n    -   **关于 $W_2$ 的梯度**：$\\frac{\\partial f_{\\theta}}{\\partial (W_2)_{ij}} = e_i h_j^\\top$，其中 $e_i$ 是第 $i$ 个标准基向量。矩阵 $W_2$ 梯度的被积函数是 $a h^\\top$。\n        $$ \\frac{d\\mathcal{L}}{dW_2} = \\int_0^T a(t) h(t)^\\top dt $$\n    -   **关于 $b_2$ 的梯度**：$\\frac{\\partial f_{\\theta}}{\\partial b_2} = I$，单位矩阵。向量 $b_2$ 梯度的被积函数是 $a$。\n        $$ \\frac{d\\mathcal{L}}{db_2} = \\int_0^T a(t) dt $$\n    -   **关于 $W_1$ 的梯度**：使用链式法则，$\\frac{\\partial f_{\\theta}}{\\partial W_1} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial W_1}$。矩阵 $W_1$ 梯度的被积函数是 $\\text{diag}(1-h^2) (W_2^\\top a) x^\\top$。\n        $$ \\frac{d\\mathcal{L}}{dW_1} = \\int_0^T \\text{diag}(1 - h(t)^2) (W_2^\\top a(t)) x(t)^\\top dt $$\n    -   **关于 $b_1$ 的梯度**：类似地，向量 $b_1$ 梯度的被积函数是 $\\text{diag}(1-h^2) (W_2^\\top a)$。\n        $$ \\frac{d\\mathcal{L}}{db_1} = \\int_0^T \\text{diag}(1 - h(t)^2) W_2^\\top a(t) dt $$\n    -   **关于 $k_d$ 的梯度**：$\\frac{\\partial f_{\\theta}}{\\partial k_d} = -\\text{diag}(x)$。向量 $k_d$ 梯度的被积函数是 $-(a \\odot x)$。\n        $$ \\frac{d\\mathcal{L}}{dk_d} = \\int_0^T -(a(t) \\odot x(t)) dt $$\n\n### 算法设计与数值实现\n\n该算法分三个主要阶段进行：一个正向传播以求解状态轨迹，一个反向传播以求解伴随轨迹并累积参数梯度，以及一个使用有限差分的验证步骤。\n\n1.  **正向传播**：状态 ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ 使用显式四阶 Runge–Kutta (RK4) 方法，以固定步长 $\\Delta t = T/N$ 从 $t=0$ 积分到 $t=T$。在时间点 $\\{t_0, t_1, \\dots, t_N\\}$ 的状态 $\\{x_0, x_1, \\dots, x_N\\}$ 被存储起来以用于反向传播。\n\n2.  **反向传播（伴随与梯度）**：同时求解伴随 ODE 和参数梯度积分。\n    -   伴随 ODE 从 $t=T$ 反向积分到 $t=0$。问题指定了“一阶显式方法”。这被实现为时间反向系统的显式欧拉法。给定时间 $t_i$ 的 $a_i$，前一个时间步 $t_{i-1}$ 的状态近似为：\n        $$ a_{i-1} = a_i - \\Delta t \\left( \\frac{da}{dt} \\right)\\bigg|_{t_i} = a_i - \\Delta t \\left( -J(x_i, \\theta)^\\top a_i \\right) = a_i + \\Delta t J(x_i, \\theta)^\\top a_i $$\n    -   参数梯度积分使用与反向欧拉积分一致的简单求积规则进行近似。使用右黎曼和，从 $i=N$ 到 $i=1$ 累积每个时间步 $t_i$ 的梯度贡献：\n        $$ \\frac{d\\mathcal{L}}{d\\theta} \\approx \\sum_{i=1}^N \\left( a_i^\\top \\frac{\\partial f_{\\theta}}{\\partial \\theta}\\bigg|_{x_i} \\right) \\Delta t $$\n    -   该过程从 $a_N = x_N - x_{\\text{target}}$ 开始，并向后迭代，在每一步更新伴随状态并累加到总梯度中。\n\n3.  **通过有限差分进行验证**：计算出的伴随梯度与中心有限差分近似进行比较。对于每个标量参数 $\\theta_j$，其梯度分量近似为：\n    $$\n    \\frac{d\\mathcal{L}}{d\\theta_j} \\approx \\frac{\\mathcal{L}(\\theta + \\epsilon e_j) - \\mathcal{L}(\\theta - \\epsilon e_j)}{2\\epsilon}\n    $$\n    其中 $e_j$ 是一个标准基向量，$\\epsilon$ 是一个小的扰动。这需要为每个参数重新积分正向 ODE 两次。然后将伴随梯度向量和有限差分梯度向量之间的最大绝对差与给定的容差 $\\tau$ 进行比较。此比较评估了在特定数值离散化下连续伴随方法的准确性。由于连续伴随方法的“先优化后离散”性质与有限差分检查对 RK4 求解器隐含定义的“先离散后优化”性质，预计会出现差异。随着积分步长 $\\Delta t$ 趋近于 0，这些差异预计会减小。具有不同 $N$ 和非线性缩放因子 $s$ 的测试用例旨在探索此行为。",
            "answer": "```python\nimport numpy as np\n\nclass ModelParams:\n    \"\"\"\n    A helper class to manage model parameters, including packing to a flat vector\n    and unpacking from it, and applying scaling.\n    \"\"\"\n    def __init__(self, W1, b1, W2, b2, kd):\n        self.W1 = np.array(W1, dtype=np.float64)\n        self.b1 = np.array(b1, dtype=np.float64)\n        self.W2 = np.array(W2, dtype=np.float64)\n        self.b2 = np.array(b2, dtype=np.float64)\n        self.kd = np.array(kd, dtype=np.float64)\n        \n        self.shapes = [self.W1.shape, self.b1.shape, self.W2.shape, self.b2.shape, self.kd.shape]\n        self.sizes = [p.size for p in [self.W1, self.b1, self.W2, self.b2, self.kd]]\n        self.total_size = sum(self.sizes)\n\n    def pack(self):\n        \"\"\"Packs all parameters into a single flat numpy array.\"\"\"\n        return np.concatenate([p.flatten() for p in [self.W1, self.b1, self.W2, self.b2, self.kd]])\n\n    @classmethod\n    def from_flat(cls, theta_flat):\n        \"\"\"Creates a ModelParams object from a flat numpy array.\"\"\"\n        theta_flat = np.array(theta_flat, dtype=np.float64)\n        # Fixed shapes from the problem description\n        shapes = [(3, 2), (3,), (2, 3), (2,), (2,)]\n        sizes = [np.prod(s) for s in shapes]\n        \n        ptr = 0\n        unpacked_params = []\n        for i, shape in enumerate(shapes):\n            size = sizes[i]\n            param_flat = theta_flat[ptr:ptr+size]\n            unpacked_params.append(param_flat.reshape(shape))\n            ptr += size\n        \n        return cls(*unpacked_params)\n\n    def scale(self, s):\n        \"\"\"Applies scaling factor s to network weights and biases.\"\"\"\n        return ModelParams(self.W1 * s, self.b1 * s, self.W2 * s, self.b2 * s, self.kd)\n\ndef ode_func(x, params: ModelParams):\n    \"\"\"The ODE function dx/dt = f(x, t, theta).\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    return params.W2 @ h + params.b2 - params.kd * x\n\ndef run_rk4(x0, T, N, params: ModelParams):\n    \"\"\"Integrates the ODE using a 4th-order Runge-Kutta method.\"\"\"\n    dt = T / N\n    x_traj = np.zeros((N + 1, x0.shape[0]), dtype=np.float64)\n    x_traj[0] = x0\n    x = x0.copy()\n    \n    for i in range(N):\n        k1 = ode_func(x, params)\n        k2 = ode_func(x + 0.5 * dt * k1, params)\n        k3 = ode_func(x + 0.5 * dt * k2, params)\n        k4 = ode_func(x + dt * k3, params)\n        x += (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n        x_traj[i+1] = x\n        \n    return x_traj\n\ndef compute_loss(x_final, xtarget):\n    \"\"\"Computes the loss function L.\"\"\"\n    return 0.5 * np.sum((x_final - xtarget)**2)\n\ndef compute_dfdx(x, params: ModelParams):\n    \"\"\"Computes the Jacobian of the ODE function w.r.t. x.\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    diag_1_minus_h2 = np.diag(1 - h**2)\n    return params.W2 @ diag_1_minus_h2 @ params.W1 - np.diag(params.kd)\n\ndef compute_adjoint_gradient(x0, T, N, params: ModelParams, xtarget):\n    \"\"\"Computes the gradient dL/dtheta using the adjoint method.\"\"\"\n    dt = T / N\n    x_traj = run_rk4(x0, T, N, params)\n    \n    grad_W1 = np.zeros_like(params.W1)\n    grad_b1 = np.zeros_like(params.b1)\n    grad_W2 = np.zeros_like(params.W2)\n    grad_b2 = np.zeros_like(params.b2)\n    grad_kd = np.zeros_like(params.kd)\n    \n    a = x_traj[N] - xtarget\n    \n    for i in range(N, 0, -1):\n        x = x_traj[i]\n        \n        z = params.W1 @ x + params.b1\n        h = np.tanh(z)\n        \n        # Accumulate gradient contributions from time t_i\n        grad_b2 += a * dt\n        grad_W2 += np.outer(a, h) * dt\n        grad_kd += -(a * x) * dt\n        \n        v = (1 - h**2) * (params.W2.T @ a)\n        grad_b1 += v * dt\n        grad_W1 += np.outer(v, x) * dt\n        \n        # Update adjoint state from t_i to t_{i-1}\n        jacobian = compute_dfdx(x, params)\n        a = a + dt * (jacobian.T @ a)\n        \n    packed_grads = np.concatenate([\n        grad_W1.flatten(), grad_b1.flatten(), grad_W2.flatten(),\n        grad_b2.flatten(), grad_kd.flatten()\n    ])\n    return packed_grads\n\ndef compute_fd_gradient(x0, T, N, base_params: ModelParams, xtarget, epsilon):\n    \"\"\"Computes the gradient dL/dtheta using central finite differences.\"\"\"\n    base_theta = base_params.pack()\n    grad = np.zeros_like(base_theta)\n    \n    def loss_func(theta_flat):\n        params_pert = ModelParams.from_flat(theta_flat)\n        x_traj = run_rk4(x0, T, N, params_pert)\n        return compute_loss(x_traj[-1], xtarget)\n\n    for i in range(len(base_theta)):\n        theta_plus = base_theta.copy()\n        theta_plus[i] += epsilon\n        \n        theta_minus = base_theta.copy()\n        theta_minus[i] -= epsilon\n        \n        loss_plus = loss_func(theta_plus)\n        loss_minus = loss_func(theta_minus)\n        \n        grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n        \n    return grad\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    W1_base = np.array([[0.8, -0.5], [0.3, 0.9], [-0.7, 0.2]])\n    b1_base = np.array([0.1, -0.2, 0.05])\n    W2_base = np.array([[0.5, -0.3, 0.1], [-0.4, 0.6, -0.2]])\n    b2_base = np.array([0.0, 0.05])\n    kd_base = np.array([0.3, 0.5])\n    base_params_obj = ModelParams(W1_base, b1_base, W2_base, b2_base, kd_base)\n\n    x0 = np.array([0.5, 0.2])\n    xtarget = np.array([0.1, -0.1])\n    T = 2.0\n\n    test_cases = [\n        (500, 1e-6, 2e-2, 1.0),\n        (50, 1e-6, 2e-2, 1.0),\n        (20, 1e-6, 5e-2, 1.0),\n        (60, 1e-6, 1e-2, 0.05),\n    ]\n\n    results = []\n    \n    for N, epsilon, tau, s in test_cases:\n        scaled_params = base_params_obj.scale(s)\n        \n        grad_adj = compute_adjoint_gradient(x0, T, N, scaled_params, xtarget)\n        grad_fd = compute_fd_gradient(x0, T, N, scaled_params, xtarget, epsilon)\n\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        \n        results.append(max_abs_diff < tau)\n        \n    bool_str_results = [str(b) for b in results]\n    print(f\"[{','.join(bool_str_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在许多现实场景中，我们对生物系统的了解是片面的。混合模型（hybrid models）将已知的物理机理与一个待学习的神经网络组件相结合，为这类问题提供了强有力的解决方案。本练习模拟了一个真实的研究工作流程：您将训练一个神经网络，使其从含噪声的数据中学习系统动态的“未知”部分。通过这个过程，您将学会如何评估所学到的动态是否遵循质量守恒和非负性等基本物理定律，以确保您的模型不仅准确，而且在物理上是合理的。",
            "id": "3333151",
            "problem": "给定一个封闭的双物种生化途径，其状态向量为 $x(t) = [A(t),B(t)]^\\top$，描述了物种 $A$ 和 $B$ 的浓度。部分已知的机理由质量作用定律控制，产生一个已知的向量场 $f_{\\text{known}}(x)$，其速率常数为 $k_1$ 和 $k_2$：\n$$\n\\frac{dA}{dt} = -k_1 A + k_2 B,\\quad \\frac{dB}{dt} = k_1 A - k_2 B,\n$$\n根据总浓度的守恒，该系统满足不变量\n$$\n\\frac{d}{dt}\\left(A+B\\right) = 0.\n$$\n实际上，该途径还包含额外的、未知的调控效应。我们将其建模为一个加性修正项 $g_\\theta(x)$，该修正项由一个带有参数 $\\theta$ 的神经函数参数化，从而产生一个混合模型\n$$\n\\frac{dx}{dt} = f_{\\text{known}}(x) + g_\\theta(x).\n$$\n合成训练数据由一个基准真相系统生成，该系统增加了一个从物种 $A$ 到物种 $B$ 的微小饱和通量，\n$$\n\\frac{dA}{dt} = -k_1 A + k_2 B - \\alpha \\tanh\\left(\\beta A\\right),\\quad \\frac{dB}{dt} = k_1 A - k_2 B + \\alpha \\tanh\\left(\\beta A\\right),\n$$\n确保守恒定律 $A(t)+B(t)=A(0)+B(0)$ 在真实动力学中精确成立。您的任务是训练混合神经常微分方程，使 $g_\\theta(x)$ 从数据中学习未知的修正项，然后通过检查模拟轨迹上的不变量和状态非负性，评估学习到的修正项是否违反了物理约束。\n\n使用以下基本依据和事实：\n- 质量作用定律，它产生在 $A$ 和 $B$ 中具有严格非负速率常数的双线性通量。\n- 封闭系统中的守恒性，要求在没有流入和流出的情况下，$A(t)+B(t)$ 是不变量。\n- 浓度的非负性，要求对所有 $t$，$A(t)\\ge 0$ 和 $B(t)\\ge 0$。\n- 常微分方程的定义，$\\frac{dx}{dt}=f(x)$，以及通过数值积分生成轨迹。\n\n训练方案：\n1. 通过对基准真相系统在指定的初始条件下进行有限时间范围内的数值积分，生成合成的状态轨迹 $x(t_i)$。向每个观测到的状态坐标添加具有零均值和指定标准差的独立同分布高斯测量噪声，以模拟真实测量。时间单位为秒，浓度单位为任意但一致的单位。\n2. 在均匀的时间网格上，使用有限差分法从带噪声的观测值中近似时间导数 $\\dot{x}(t_i)$（对内部点使用中心差分，对边界点使用一阶单边差分）。\n3. 计算残差 $r(t_i) = \\dot{x}(t_i) - f_{\\text{known}}(x(t_i))$，学习到的修正项必须能解释这些残差。\n4. 将修正项 $g_\\theta(x)$ 参数化为一个随机特征神经函数\n$$\ng_\\theta(x) = K^\\top \\phi\\left(Vx + b\\right),\\quad \\phi(u) = \\tanh(u),\n$$\n其中 $V\\in \\mathbb{R}^{m\\times 2}$ 和 $b\\in \\mathbb{R}^{m}$ 是使用可复现的伪随机数生成器种子生成的固定随机特征，$m$ 是隐藏单元的数量，$K\\in \\mathbb{R}^{m\\times 2}$ 是可训练的输出权重。通过岭回归拟合 $K$，以最小化带有 $\\ell_2$ 正则化的残差平方误差。\n5. 用学习到的 $g_\\theta$ 构建混合模型，并从相同的初始条件对其轨迹进行数值积分。\n\n约束评估：\n- 不变量违规：计算在整个模拟轨迹中，$A(t)+B(t)$ 与其初始值的最大绝对偏差。如果此偏差超过容差 $\\tau_{\\text{inv}}=0.05$ 浓度单位，则认为不变量被违反。\n- 非负性违规：计算在整个模拟轨迹中 $A(t)$ 或 $B(t)$ 达到的最小值。如果该最小值小于 $-\\tau_{\\text{neg}}$，则认为非负性被违反。\n\n参数和单位：\n- 使用 $k_1=1.0$ $\\text{s}^{-1}$ 和 $k_2=0.5$ $\\text{s}^{-1}$。\n- 对基准真相修正项使用 $\\alpha=0.2$ $\\text{s}^{-1}$ 和 $\\beta=1.0$。\n- 使用时间范围 $T=5.0$ $\\text{s}$ 和 $N=101$ 个均匀间隔的样本。\n- 使用 $m=8$ 个隐藏单元和固定随机种子 $42$ 来生成 $V$ 和 $b$（例如，从一个标准正态分布生成 $V$，从一个正态分布生成 $b$）。\n- 使用岭正则化强度 $\\lambda=10^{-3}$。\n- 使用不变量容差 $\\tau_{\\text{inv}}=0.05$ 浓度单位和非负性容差 $\\tau_{\\text{neg}}=10^{-3}$ 浓度单位。\n\n测试套件：\n评估三个测试用例，每个用例指定为 $(A_0,B_0,\\sigma)$，其中 $A_0$ 和 $B_0$ 是初始浓度，$\\sigma$ 是以相同浓度单位表示的测量噪声标准差：\n1. $(1.0, 0.0, 0.001)$\n2. $(0.0, 1.0, 0.001)$\n3. $(0.5, 0.5, 0.05)$\n\n对于每个测试用例，按规定训练 $g_\\theta$，模拟混合轨迹，并确定是否发生违规。违规被定义为不变量违规和非负性违规的逻辑析取。要求的输出是包含三个布尔值的列表的单行文本，每个值对应一个测试用例，不得包含任何额外文本。格式必须严格如下：\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3]\n$$\n其中每个 $\\text{result}_i$ 是 $\\text{True}$ 或 $\\text{False}$，表示第 $i$ 个测试用例是否违反了任何物理约束。",
            "solution": "该问题要求我们为一个双物种生化系统构建并训练一个混合常微分方程 (ODE) 模型。该模型包括一个基于质量作用定律的已知分量 $f_{\\text{known}}(x)$ 和一个由神经函数参数化的学习分量 $g_\\theta(x)$。目标是在从基准真相系统生成的合成噪声数据上训练 $g_\\theta(x)$，然后评估得到的混合模型 $\\frac{dx}{dt} = f_{\\text{known}}(x) + g_\\theta(x)$ 是否违反了基本物理约束，即总质量守恒和浓度非负性。\n\n系统的状态由向量 $x(t) = [A(t), B(t)]^\\top$ 给出，表示两个物种 $A$ 和 $B$ 的浓度。\n\n对所提供的三个测试用例中的每一个都执行求解过程，并遵循指定的训练和评估方案。\n\n**第一步：合成数据生成**\n\n首先，我们通过模拟一个基准真相系统来生成训练数据。这个真实系统的动力学由以下 ODE 给出：\n$$\n\\frac{dA}{dt} = -k_1 A + k_2 B - \\alpha \\tanh(\\beta A) \\\\\n\\frac{dB}{dt} = k_1 A - k_2 B + \\alpha \\tanh(\\beta A)\n$$\n参数为 $k_1 = 1.0 \\text{ s}^{-1}$，$k_2 = 0.5 \\text{ s}^{-1}$，$\\alpha = 0.2 \\text{ s}^{-1}$ 和 $\\beta = 1.0$。对于测试套件中指定的每个初始条件 $(A_0, B_0)$，该系统在 $T=5.0 \\text{ s}$ 的时间范围内使用 $N=101$ 个均匀间隔的时间点进行数值积分。我们使用 `SciPy` 中的 `solve_ivp` 函数，这是一个标准的自适应步长 ODE 求解器。为模拟实验测量，我们向真实状态轨迹 $x(t_i)$ 添加均值为 $0$、标准差为 $\\sigma$ 的独立同分布高斯噪声，以获得观测状态 $x_{\\text{obs}}(t_i)$。\n\n**第二步：导数近似**\n\n为了训练模型，我们需要从带噪声的状态观测值 $x_{\\text{obs}}(t_i)$ 中近似时间导数 $\\dot{x}(t_i)$。我们在均匀的时间网格上使用有限差分格式。时间步长为 $\\Delta t = T / (N-1)$。我们对内部时间点采用二阶中心差分，对边界点采用指定的一阶前向/后向差分，这可以方便地使用 `numpy.gradient` 实现。\n\n**第三步：残差计算和回归目标构建**\n\n混合模型的核心思想是，学习到的分量 $g_\\theta(x)$ 应该解释已知物理 $f_{\\text{known}}(x)$ 未能捕捉的动力学。动力学的先验已知部分是：\n$$\nf_{\\text{known}}(x) = \\begin{pmatrix} -k_1 A + k_2 B \\\\ k_1 A - k_2 B \\end{pmatrix}\n$$\n我们在每个观测点计算残差 $r(t_i)$，这些残差作为我们机器学习模型的目标：\n$$\nr(t_i) = \\dot{x}_{\\text{obs}}(t_i) - f_{\\text{known}}(x_{\\text{obs}}(t_i))\n$$\n在没有噪声的情况下，$r(t_i)$ 将等于真实的未知修正项 $g_{\\text{true}}(x(t_i)) = [-\\alpha \\tanh(\\beta A(t_i)), \\alpha \\tanh(\\beta A(t_i))]^\\top$。由于测量噪声和数值微分误差，$r(t_i)$ 是这个真实修正项的一个带噪声的估计。\n\n**第四步：训练神经修正函数**\n\n修正项 $g_\\theta(x)$ 被参数化为一个随机特征神经函数：\n$$\ng_\\theta(x) = K^\\top \\phi(Vx + b)\n$$\n其中 $\\phi(u) = \\tanh(u)$ 是激活函数。特征映射参数，即矩阵 $V \\in \\mathbb{R}^{m \\times 2}$ 和向量 $b \\in \\mathbb{R}^{m}$，使用固定的伪随机数生成器种子 $42$ 从标准正态分布中生成一次，隐藏单元数为 $m=8$。此后它们保持不变。可训练的参数是输出权重 $K \\in \\mathbb{R}^{m \\times 2}$。\n\n我们通过求解带 $\\ell_2$ 正则化（岭回归）的线性最小二乘问题来确定 $K$。目标是最小化在所有 $N$ 个训练点上的以下损失函数：\n$$\n\\mathcal{L}(K) = \\sum_{i=1}^{N} \\| g_\\theta(x_{\\text{obs}}(t_i)) - r(t_i) \\|_2^2 + \\lambda \\|K\\|_F^2\n$$\n其中 $\\| \\cdot \\|_F$ 是弗罗贝尼乌斯范数，$\\lambda = 10^{-3}$ 是正则化强度。令 $\\Phi \\in \\mathbb{R}^{m \\times N}$ 是其第 $i$ 列为 $\\phi(V x_{\\text{obs}}(t_i) + b)$ 的矩阵，令 $R \\in \\mathbb{R}^{2 \\times N}$ 是残差矩阵。问题变为最小化 $\\|\\Phi^\\top K - R^\\top\\|_F^2 + \\lambda \\|K\\|_F^2$。$K$ 的解析解由下式给出：\n$$\nK = (\\Phi \\Phi^\\top + \\lambda I_m)^{-1} (\\Phi R^\\top)\n$$\n其中 $I_m$ 是 $m \\times m$ 的单位矩阵。该系统通过数值方法求解 $K$。\n\n**第五步：混合模型模拟与约束评估**\n\n利用学习到的权重矩阵 $K$，完整的混合动力系统定义如下：\n$$\n\\frac{dx}{dt} = f_{\\text{known}}(x) + K^\\top \\phi(Vx + b)\n$$\n我们从初始条件 $(A_0, B_0)$ 出发，在时间范围 $T=5.0 \\text{ s}$ 内对该系统进行数值积分，以获得模拟轨迹 $x_{\\text{sim}}(t)$。\n\n最后，我们根据两个物理约束来评估此轨迹：\n1.  **不变量违规**：真实系统守恒总浓度，即 $A(t)+B(t) = A_0+B_0$。学习到的模型在结构上没有被约束来强制执行这一点。我们通过计算轨迹上总和的最大绝对偏差来检查违规：$\\max_t |(A_{\\text{sim}}(t)+B_{\\text{sim}}(t)) - (A_0+B_0)|$。如果此偏差超过容差 $\\tau_{\\text{inv}}=0.05$，则该约束被违反。\n2.  **非负性违规**：浓度必须为非负。我们检查是否有任何浓度变得小于负容差：$\\min_t(A_{\\text{sim}}(t), B_{\\text{sim}}(t)) < -\\tau_{\\text{neg}}$，其中 $\\tau_{\\text{neg}}=10^{-3}$。\n\n如果任一约束被违反，则测试用例结果为违规（`True`）。对所有三个测试用例重复此过程，并汇总布尔结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the hybrid neural ODE training and evaluation for all test cases.\n    \"\"\"\n    \n    # Define problem parameters\n    k1 = 1.0  # s^-1\n    k2 = 0.5  # s^-1\n    alpha = 0.2  # s^-1\n    beta = 1.0\n    \n    # Time horizon and sampling\n    T = 5.0  # s\n    N = 101\n    t_span = [0, T]\n    t_eval = np.linspace(0, T, N)\n    dt = t_eval[1] - t_eval[0]\n    \n    # Model and training parameters\n    m = 8  # Number of hidden units\n    ridge_lambda = 1e-3\n    nn_seed = 42\n    \n    # Constraint evaluation tolerances\n    tau_inv = 0.05\n    tau_neg = 1e-3\n\n    # Test suite\n    test_cases = [\n        (1.0, 0.0, 0.001),  # (A0, B0, sigma)\n        (0.0, 1.0, 0.001),\n        (0.5, 0.5, 0.05),\n    ]\n\n    # Initialize random feature network parameters (once for all tests)\n    rng_nn = np.random.default_rng(seed=nn_seed)\n    V = rng_nn.standard_normal(size=(m, 2))\n    b = rng_nn.standard_normal(size=(m, 1))\n\n    # Initialize a separate RNG for measurement noise for reproducibility\n    noise_seed = 123\n    rng_noise = np.random.default_rng(seed=noise_seed)\n\n    # --- RHS function definitions ---\n\n    def ground_truth_rhs(t, y):\n        A, B = y\n        dAdt = -k1 * A + k2 * B - alpha * np.tanh(beta * A)\n        dBdt = k1 * A - k2 * B + alpha * np.tanh(beta * A)\n        return [dAdt, dBdt]\n\n    def known_rhs(y):\n        A, B = y\n        dAdt = -k1 * A + k2 * B\n        dBdt = k1 * A - k2 * B\n        return np.array([dAdt, dBdt])\n    \n    def hybrid_rhs(t, y, V_mat, b_vec, K_mat, k1_val, k2_val):\n        y_col = y.reshape(-1, 1)\n        \n        # Known part\n        f_known = np.array([\n            -k1_val * y_col[0] + k2_val * y_col[1],\n            k1_val * y_col[0] - k2_val * y_col[1]\n        ]).reshape(-1, 1)\n        \n        # Learned correction\n        phi_val = np.tanh(V_mat @ y_col + b_vec)\n        g_theta = K_mat.T @ phi_val\n\n        dydt = f_known + g_theta\n        return dydt.flatten()\n\n    # --- Main loop over test cases ---\n    \n    results = []\n    \n    for A0, B0, sigma in test_cases:\n        # Step 1: Generate synthetic data\n        x0 = [A0, B0]\n        sol_true = solve_ivp(\n            ground_truth_rhs, t_span, x0, t_eval=t_eval, method='RK45'\n        )\n        x_true = sol_true.y\n        \n        # Add measurement noise\n        noise = rng_noise.normal(loc=0.0, scale=sigma, size=x_true.shape)\n        x_obs = x_true + noise\n        \n        # Step 2: Approximate derivatives\n        x_dot_obs = np.gradient(x_obs, dt, axis=1, edge_order=1)\n        \n        # Step 3: Compute residuals\n        f_known_obs = known_rhs(x_obs)\n        residuals = x_dot_obs - f_known_obs\n        \n        # Step 4: Train correction model (Ridge Regression)\n        # Phi shape (m, N)\n        Phi = np.tanh(V @ x_obs + b)\n        \n        # Solve (Phi Phi^T + lambda I) K = Phi R^T for K\n        # A_mat shape (m, m)\n        A_matrix = Phi @ Phi.T + ridge_lambda * np.identity(m)\n        # B_mat shape (m, 2)\n        B_matrix = Phi @ residuals.T\n        \n        # K shape (m, 2)\n        K = np.linalg.solve(A_matrix, B_matrix)\n        \n        # Step 5: Simulate hybrid model\n        sol_hybrid = solve_ivp(\n            hybrid_rhs, t_span, x0, t_eval=t_eval, method='RK45', \n            args=(V, b, K, k1, k2)\n        )\n        x_sim = sol_hybrid.y\n        \n        # Step 6: Evaluate constraints\n        # Invariant violation\n        initial_sum = A0 + B0\n        trajectory_sum = x_sim[0, :] + x_sim[1, :]\n        invariant_dev = np.max(np.abs(trajectory_sum - initial_sum))\n        invariant_violation = invariant_dev > tau_inv\n        \n        # Nonnegativity violation\n        min_conc = np.min(x_sim)\n        nonneg_violation = min_conc < -tau_neg\n        \n        # Combine results\n        violation = invariant_violation or nonneg_violation\n        results.append(violation)\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}