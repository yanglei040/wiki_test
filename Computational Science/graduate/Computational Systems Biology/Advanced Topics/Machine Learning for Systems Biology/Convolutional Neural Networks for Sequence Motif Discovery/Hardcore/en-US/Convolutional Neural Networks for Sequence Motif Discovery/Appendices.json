{
    "hands_on_practices": [
        {
            "introduction": "The fundamental operation of a convolutional neural network in sequence analysis is the convolution itself, where filters scan across the input. This exercise demystifies this core component by demonstrating its direct mathematical equivalence to the classic bioinformatics method of Position Weight Matrix (PWM) scanning . By implementing the log-odds scoring from first principles, you will gain a concrete understanding of how a CNN filter learns to function as a sophisticated motif detector.",
            "id": "3297925",
            "problem": "You are given the task of implementing a rigorous Position Weight Matrix (PWM) scan and demonstrating its equivalence to a single-channel, multi-filter operation of a Convolutional Neural Network (CNN) in the context of sequence motif discovery. The scientific base should begin from probability model definitions and log-likelihood ratios. A Position Weight Matrix (PWM) represents a motif of length $L$ by a set of position-specific multinomial distributions over Deoxyribonucleic Acid (DNA) nucleotides. A background model gives independent nucleotide probabilities. You must compute windowed log-odds scores across a DNA sequence and identify all start indices where the score strictly exceeds a given threshold.\n\nFundamental base:\n- Let the motif be represented by a PWM with entries $p_{i,b}$ where $i \\in \\{0,1,\\dots,L-1\\}$ indexes position and $b \\in \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ indexes nucleotides. Let the background distribution be $q_b$ with $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ and each $q_b > 0$.\n- Under the independent positions assumption, the motif model assigns to a length-$L$ subsequence $x_{t:t+L-1}$ the probability $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$, while the background model assigns $\\prod_{i=0}^{L-1} q_{x_{t+i}}$. The log-odds (log-likelihood ratio) score is defined as\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\n- Define the log-odds weight matrix $W$ by $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$ using the natural logarithm. Encode the DNA sequence into a one-hot matrix $X \\in \\{0,1\\}^{T \\times 4}$ where $T$ is the sequence length and channels correspond in order to $\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}$. For any unknown nucleotide symbol, such as $N$, encode as the zero vector $(0,0,0,0)$ so it contributes nothing to the log-odds sum. Then the score can be written as\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}} W_{i,b} \\, X_{t+i,b},\n$$\nwhich is exactly a valid-mode multi-channel convolution used in a Convolutional Neural Network (CNN) where the filter $W$ spans $L$ positions and $4$ channels.\n\nProgram requirements:\n- Implement a function that, given a DNA sequence, a PWM specified by its per-position nucleotide probabilities $p_{i,b}$, a background distribution $q_b$, and a real-valued threshold $\\tau$, computes $S(t)$ for all valid start positions $t \\in \\{0,1,\\dots,T-L\\}$ and returns the list of indices $t$ such that $S(t) > \\tau$.\n- Use the natural logarithm for all log-odds computations. Handle unknown nucleotides $N$ by contributing $0$ at those positions as specified above.\n- Follow strictly the \"exceeding\" criterion: include positions only if $S(t)$ is strictly greater than $\\tau$.\n- Operate only in the forward orientation; do not consider reverse complements.\n- All mathematical quantities must be treated exactly as defined above.\n\nTest suite:\nYou must hard-code and evaluate the following five test cases. For each case, report the list of start indices where the score strictly exceeds the specified threshold.\n\n- Case $1$ (happy path, uniform background, exact motif present repeatedly):\n    - Sequence: $\\texttt{ACGTACGTACGT}$.\n    - PWM length $L = 4$ with probabilities at each position (order $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$):\n        - Position $0$: $(0.7, 0.1, 0.1, 0.1)$.\n        - Position $1$: $(0.1, 0.7, 0.1, 0.1)$.\n        - Position $2$: $(0.1, 0.1, 0.7, 0.1)$.\n        - Position $3$: $(0.1, 0.1, 0.1, 0.7)$.\n    - Background $q = (0.25, 0.25, 0.25, 0.25)$.\n    - Threshold $\\tau = 3.5$.\n\n- Case $2$ (boundary where no window exceeds a high threshold):\n    - Sequence: $\\texttt{AAAAACCCCC}$.\n    - PWM and background identical to Case $1$.\n    - Threshold $\\tau = 4.0$.\n\n- Case $3$ (unknown symbols $N$ reduce contributions, but an exact match appears once):\n    - Sequence: $\\texttt{NNNACGTNNN}$.\n    - PWM and background identical to Case $1$.\n    - Threshold $\\tau = 3.5$.\n\n- Case $4$ (non-uniform background, different PWM length and composition):\n    - Sequence: $\\texttt{ACGACGTTAC}$.\n    - PWM length $L = 3$ with probabilities (order $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$):\n        - Position $0$: $(0.6, 0.2, 0.1, 0.1)$.\n        - Position $1$: $(0.1, 0.6, 0.2, 0.1)$.\n        - Position $2$: $(0.1, 0.2, 0.6, 0.1)$.\n    - Background $q = (0.1, 0.4, 0.4, 0.1)$.\n    - Threshold $\\tau = 2.4$.\n\n- Case $5$ (exact equality to threshold should not be included):\n    - Sequence: $\\texttt{ACGT}$.\n    - PWM and background identical to Case $1$.\n    - Threshold $\\tau = 4.118477668724633$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of integers of positions that strictly exceed the threshold, in the order of the test suite above. For example, an output might look like $[\\,[0,4,8],[],[3],[0,3],[]\\,]$.",
            "solution": "We begin from the independent positions model for motifs and the independent and identically distributed background. Let a length-$L$ motif be described by a Position Weight Matrix (PWM) with entries $p_{i,b}$, where $i \\in \\{0,\\dots,L-1\\}$ indexes motif positions and $b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ indexes nucleotides. Let the background model be parameterized by $q_b$ such that $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ with each $q_b > 0$. For a DNA sequence $x_0 x_1 \\dots x_{T-1}$, consider any window start index $t$ with $0 \\le t \\le T-L$. The motif model assigns probability $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$ to the subsequence $x_{t:t+L-1}$, while the background model assigns $\\prod_{i=0}^{L-1} q_{x_{t+i}}$. The canonical scanning score is the natural log-likelihood ratio\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\nDefining the log-odds weight matrix $W$ by $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$, we have\n$$\nS(t) = \\sum_{i=0}^{L-1} W_{i, x_{t+i}}.\n$$\nIntroduce one-hot encoding $X \\in \\{0,1\\}^{T \\times 4}$ with channel order $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$, i.e., $X_{j,b} = 1$ if $x_j=b$ and $0$ otherwise; for unknown symbols (e.g., $N$), set $X_{j,b}=0$ for all $b$. Then, for any start $t$, the score can be written as\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b} W_{i,b} \\, X_{t+i,b},\n$$\nwhich is identical to a valid-mode convolution across the sequence length dimension with a filter $W$ spanning $L$ positions and $4$ channels, as used in Convolutional Neural Networks (CNN) for sequence motif discovery. In CNN terms, $S(t)$ is the activation of a single filter applied to the one-hot-encoded input at location $t$.\n\nAlgorithmic design:\n- Compute $W$ via $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$ using the natural logarithm, guaranteeing finite values since each $p_{i,b}$ and $q_b$ are strictly positive in the provided test suite.\n- One-hot encode the sequence, mapping $\\mathrm{A} \\mapsto (1,0,0,0)$, $\\mathrm{C} \\mapsto (0,1,0,0)$, $\\mathrm{G} \\mapsto (0,0,1,0)$, $\\mathrm{T} \\mapsto (0,0,0,1)$, and $N \\mapsto (0,0,0,0)$.\n- For each valid start index $t$, compute $S(t)$ as the tensor contraction $S(t) = \\sum_{i,b} W_{i,b} X_{t+i,b}$, and collect those $t$ for which $S(t) > \\tau$.\n- Do this for each of the five test cases and aggregate the results in order.\n\nAnalytical expectations for the provided test suite:\n- Case $1$: The motif is highly specific for the pattern $\\texttt{ACGT}$ across positions. With uniform background $q_b = 0.25$, the per-position match log-odds weight is $\\log(0.7/0.25) = \\log(2.8) \\approx 1.029619417$ and the mismatch weight is $\\log(0.1/0.25) = \\log(0.4) \\approx -0.916290732$. A perfect match subsequence $\\texttt{ACGT}$ yields total score $4 \\times \\log(2.8) \\approx 4.118477669$, which strictly exceeds $\\tau = 3.5$. In the sequence $\\texttt{ACGTACGTACGT}$, perfect matches occur at positions $0$, $4$, and $8$. All other $4$-length windows are rotations that induce all mismatches, yielding negative scores, so the result is $[0,4,8]$.\n- Case $2$: With the same PWM and background, the sequence $\\texttt{AAAAACCCCC}$ contains no $\\texttt{ACGT}$ window. Even the best windows cannot exceed $\\tau = 4.0$. For example, $\\texttt{AAAA}$ yields $S \\approx 1.029619417 + 3 \\times (-0.916290732) \\approx -1.719252779$. Thus, no positions exceed the threshold and the result is $[]$.\n- Case $3$: Unknowns $N$ contribute zero as per the specified encoding and summation rule. The sequence $\\texttt{NNNACGTNNN}$ contains a perfect match $\\texttt{ACGT}$ starting at position $3$ with $S \\approx 4.118477669 > 3.5$, while all windows overlapping $N$ without the exact motif accumulate insufficient or negative scores. Therefore, the result is $[3]$.\n- Case $4$: Non-uniform background $q = (0.1, 0.4, 0.4, 0.1)$ and PWM length $L=3$ specific to $\\texttt{ACG}$. The perfect match score is $\\log(0.6/0.1) + \\log(0.6/0.4) + \\log(0.6/0.4) = \\log(6) + 2 \\log(1.5) \\approx 2.602689685$, which exceeds $\\tau = 2.4$. In the sequence $\\texttt{ACGACGTTAC}$, perfect matches occur at positions $0$ and $3$, and other windows are suboptimal with scores below the threshold. Hence the result is $[0,3]$.\n- Case $5$: The threshold is set as $\\tau = 4.118477668724633$, which is slightly larger than the exact perfect match score obtained from the PWM and uniform background. For the sequence $\\texttt{ACGT}$, the only window is the entire sequence, which achieves a score of approximately $4.118477668724632$ and does not strictly exceed the threshold due to the strict inequality. Therefore, the result is $[]$.\n\nImplementation notes:\n- The computation uses the natural logarithm consistently.\n- The program produces the single-line output aggregating all five cases in order as $[\\,[0,4,8],[],[3],[0,3],[]\\,]$.\n- The approach demonstrates the connection between statistical PWM scanning and the valid-mode multi-channel convolution central to Convolutional Neural Networks (CNN) used in computational systems biology for motif discovery.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nNUC_ORDER = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\ndef one_hot_encode(seq: str) -> np.ndarray:\n    \"\"\"\n    Encode DNA sequence into one-hot with channels in order A, C, G, T.\n    Unknowns (e.g., 'N') are encoded as all zeros.\n    \"\"\"\n    X = np.zeros((len(seq), 4), dtype=float)\n    for i, ch in enumerate(seq.upper()):\n        idx = NUC_ORDER.get(ch, None)\n        if idx is not None:\n            X[i, idx] = 1.0\n        # else: unknown contributes 0 vector\n    return X\n\ndef log_odds_weights(pwm_probs: np.ndarray, background: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute log-odds weight matrix W_{i,b} = ln(p_{i,b} / q_b).\n    pwm_probs: shape (L, 4)\n    background: shape (4,)\n    \"\"\"\n    # Ensure arrays are float for division and log\n    pwm = np.array(pwm_probs, dtype=float)\n    q = np.array(background, dtype=float)\n    # Avoid division by zero: test suite guarantees q_b > 0 and p_{i,b} > 0\n    W = np.log(pwm / q[None, :])\n    return W\n\ndef pwm_scan_positions(seq: str, pwm_probs: np.ndarray, background: np.ndarray, threshold: float) -> list:\n    \"\"\"\n    Perform PWM scanning: compute windowed log-odds scores and return positions exceeding threshold.\n    Uses natural logarithm and strict inequality.\n    \"\"\"\n    X = one_hot_encode(seq)\n    W = log_odds_weights(pwm_probs, background)\n    L = W.shape[0]\n    T = X.shape[0]\n    results = []\n    # Slide window and compute score S(t) = sum_{i,b} W[i,b] * X[t+i,b]\n    # Equivalent to valid-mode multi-channel convolution\n    for t in range(T - L + 1):\n        window = X[t:t+L, :]  # shape (L, 4)\n        score = float(np.sum(window * W))\n        if score > threshold:\n            results.append(t)\n    return results\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1\n    seq1 = \"ACGTACGTACGT\"\n    pwm1 = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # position 0\n        [0.1, 0.7, 0.1, 0.1],  # position 1\n        [0.1, 0.1, 0.7, 0.1],  # position 2\n        [0.1, 0.1, 0.1, 0.7],  # position 3\n    ], dtype=float)\n    bg_uniform = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)\n    tau1 = 3.5\n\n    # Case 2\n    seq2 = \"AAAAACCCCC\"\n    tau2 = 4.0\n\n    # Case 3\n    seq3 = \"NNNACGTNNN\"\n    tau3 = 3.5\n\n    # Case 4\n    seq4 = \"ACGACGTTAC\"\n    pwm4 = np.array([\n        [0.6, 0.2, 0.1, 0.1],  # position 0\n        [0.1, 0.6, 0.2, 0.1],  # position 1\n        [0.1, 0.2, 0.6, 0.1],  # position 2\n    ], dtype=float)\n    bg_nonuniform = np.array([0.1, 0.4, 0.4, 0.1], dtype=float)\n    tau4 = 2.4\n\n    # Case 5\n    seq5 = \"ACGT\"\n    tau5 = 4.118477668724633  # Slightly above the perfect match sum for PWM1 under uniform bg\n\n    test_cases = [\n        (seq1, pwm1, bg_uniform, tau1),\n        (seq2, pwm1, bg_uniform, tau2),\n        (seq3, pwm1, bg_uniform, tau3),\n        (seq4, pwm4, bg_nonuniform, tau4),\n        (seq5, pwm1, bg_uniform, tau5),\n    ]\n\n    results = []\n    for seq, pwm, bg, tau in test_cases:\n        positions = pwm_scan_positions(seq, pwm, bg, tau)\n        results.append(positions)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line: list of lists of integers\n    print(f\"[{','.join([str(lst) for lst in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Following the convolution step, a pooling layer is typically used to aggregate filter activations and achieve a degree of positional invariance. This practice explores max-pooling, not merely as a dimensionality reduction technique, but as a crucial operator for enhancing the detection of strong motif signals against a noisy background . You will combine a concrete numerical calculation with a probabilistic analysis to build an intuition for how pooling influences the network's sensitivity and robustness.",
            "id": "3297865",
            "problem": "A Convolutional Neural Network (CNN) is applied to a deoxyribonucleic acid (DNA) sequence to discover transcription factor binding motifs via one-dimensional convolution followed by max-pooling. Consider a single convolutional filter producing the one-dimensional activation map on a sequence of length $14$, given by the ordered values $a_{1:14} = \\{a_{1}, a_{2}, \\dots, a_{14}\\}$,\nwhere\n$a_{1} = 0.10$, $a_{2} = 0.05$, $a_{3} = 0.12$, $a_{4} = 0.03$, $a_{5} = 0.20$, $a_{6} = 0.11$, $a_{7} = 0.09$, $a_{8} = 0.08$, $a_{9} = 0.15$, $a_{10} = 0.10$, $a_{11} = 0.07$, $a_{12} = 0.13$, $a_{13} = 0.04$, $a_{14} = 0.06$.\n\nThe max-pooling layer uses a window size $W = 3$ and stride $S = 2$ with valid pooling (no padding and only windows that fit entirely inside the activation map). First, compute the pooled output vector by taking the maximum within each valid window that starts at indices $1, 1+S, 1+2S, \\dots$.\n\nNext, analyze detectability of a weak motif signal under the following probabilistic model at the pre-pooling stage. Within any given pooling window of size $W$, suppose exactly one position contains a weak motif signal and the remaining $W-1$ positions contain only noise. Model the pre-pooling activation at the motif position as a Gaussian random variable $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu_{w}, \\sigma^{2})$ with mean shift $\\mu_{w} > 0$ and variance $\\sigma^{2} > 0$, while the non-motif positions are independent and identically distributed Gaussian random variables $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^{2})$. Let the post-pooling output for the window be $Y = \\max\\{X_{1}, X_{2}, \\dots, X_{W}\\}$, where one of the $X_{i}$ is $X_{\\text{motif}}$ and the others are $X_{\\text{noise}}$, and assume independence among all $X_{i}$.\n\nDerive a closed-form analytic expression, in terms of $\\mu_{w}$, $\\sigma$, $W$, and a detection threshold $\\tau \\in \\mathbb{R}$, for the probability that max-pooling produces a detectable output exceeding the threshold, that is, $\\mathbb{P}(Y > \\tau)$, expressed using the Gaussian cumulative distribution function $\\Phi(\\cdot)$. Express your final answer as a single closed-form expression. No numerical approximation or rounding is required. The requested final answer should be only the analytic expression for $\\mathbb{P}(Y > \\tau)$; do not include the intermediate pooled vector in the final answer.",
            "solution": "The user-provided problem is assessed as valid because it is scientifically grounded in the principles of convolutional neural networks and probability theory, is well-posed with a clear and complete setup, and is expressed objectively. The problem consists of two distinct parts: a numerical calculation and an analytical derivation.\n\nPart 1: Computation of the Pooled Output Vector\n\nThe first part of the problem requires the computation of the output of a max-pooling layer applied to a given one-dimensional activation map.\n\nThe input is the activation map $a_{1:14} = \\{a_{1}, a_{2}, \\dots, a_{14}\\}$ of length $L=14$.\nThe max-pooling parameters are a window size of $W=3$ and a stride of $S=2$.\nThe pooling operation is 'valid' pooling, meaning no padding is used, and only windows that fit entirely within the input sequence are considered.\n\nThe starting index of the $k$-th window (using $1$-based indexing) is given by $1 + (k-1)S$. The window must fit within the sequence length $L$. The last element of the window is at index $1 + (k-1)S + W - 1$. This must be less than or equal to $L$.\n$1 + (k-1)S + W - 1 \\le L$\n$(k-1)S + W \\le L$\n$(k-1)2 + 3 \\le 14$\n$(k-1)2 \\le 11$\n$k-1 \\le 5.5$\n$k \\le 6.5$\nSince $k$ must be an integer, the maximum value for $k$ is $6$. Thus, there are $6$ valid pooling windows.\n\nLet $p_k$ be the output of the $k$-th pooling window. We calculate $p_k$ for $k=1, \\dots, 6$.\n\n1.  For $k=1$: The window starts at index $1$ and covers indices $\\{1, 2, 3\\}$. The input values are $\\{a_1, a_2, a_3\\} = \\{0.10, 0.05, 0.12\\}$.\n    $p_1 = \\max(0.10, 0.05, 0.12) = 0.12$.\n\n2.  For $k=2$: The window starts at index $1 + S = 3$ and covers indices $\\{3, 4, 5\\}$. The input values are $\\{a_3, a_4, a_5\\} = \\{0.12, 0.03, 0.20\\}$.\n    $p_2 = \\max(0.12, 0.03, 0.20) = 0.20$.\n\n3.  For $k=3$: The window starts at index $1 + 2S = 5$ and covers indices $\\{5, 6, 7\\}$. The input values are $\\{a_5, a_6, a_7\\} = \\{0.20, 0.11, 0.09\\}$.\n    $p_3 = \\max(0.20, 0.11, 0.09) = 0.20$.\n\n4.  For $k=4$: The window starts at index $1 + 3S = 7$ and covers indices $\\{7, 8, 9\\}$. The input values are $\\{a_7, a_8, a_9\\} = \\{0.09, 0.08, 0.15\\}$.\n    $p_4 = \\max(0.09, 0.08, 0.15) = 0.15$.\n\n5.  For $k=5$: The window starts at index $1 + 4S = 9$ and covers indices $\\{9, 10, 11\\}$. The input values are $\\{a_9, a_{10}, a_{11}\\} = \\{0.15, 0.10, 0.07\\}$.\n    $p_5 = \\max(0.15, 0.10, 0.07) = 0.15$.\n\n6.  For $k=6$: The window starts at index $1 + 5S = 11$ and covers indices $\\{11, 12, 13\\}$. The input values are $\\{a_{11}, a_{12}, a_{13}\\} = \\{0.07, 0.13, 0.04\\}$.\n    $p_6 = \\max(0.07, 0.13, 0.04) = 0.13$.\n\nThe pooled output vector is $\\{p_1, p_2, p_3, p_4, p_5, p_6\\} = \\{0.12, 0.20, 0.20, 0.15, 0.15, 0.13\\}$.\n\nPart 2: Analytical Derivation of the Detection Probability\n\nThe second part of the problem asks for a closed-form expression for the probability that the output of a max-pooling window, $Y$, exceeds a threshold $\\tau$.\n\nThe pooling window contains $W$ random variables, $\\{X_1, X_2, \\dots, X_W\\}$. One of these variables corresponds to a motif signal, $X_{\\text{motif}}$, and the remaining $W-1$ variables correspond to noise, $X_{\\text{noise}}$.\nThe distributions are given as:\n- Motif signal: $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu_{w}, \\sigma^{2})$\n- Noise signal: $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^{2})$\n\nThe activation values $X_i$ within a window are assumed to be independent. The output of the max-pooling operation is $Y = \\max\\{X_1, X_2, \\dots, X_W\\}$. We want to find the probability $\\mathbb{P}(Y > \\tau)$.\n\nIt is more convenient to first calculate the complementary probability, $\\mathbb{P}(Y \\le \\tau)$, which is the cumulative distribution function (CDF) of $Y$ evaluated at $\\tau$.\nThe event $Y \\le \\tau$ is equivalent to the event that all individual activations are less than or equal to $\\tau$.\n$$ \\mathbb{P}(Y \\le \\tau) = \\mathbb{P}(\\max\\{X_1, X_2, \\dots, X_W\\} \\le \\tau) = \\mathbb{P}(X_1 \\le \\tau, X_2 \\le \\tau, \\dots, X_W \\le \\tau) $$\nSince the random variables $X_i$ are independent, we can write the joint probability as the product of the marginal probabilities:\n$$ \\mathbb{P}(Y \\le \\tau) = \\prod_{i=1}^{W} \\mathbb{P}(X_i \\le \\tau) $$\nThe set of random variables $\\{X_1, \\dots, X_W\\}$ consists of one instance of $X_{\\text{motif}}$ and $W-1$ instances of $X_{\\text{noise}}$. Because the product is commutative, the specific index of the motif does not alter the final expression for the product. The product is composed of one term for the motif and $W-1$ identical terms for the noise variables.\n$$ \\mathbb{P}(Y \\le \\tau) = \\mathbb{P}(X_{\\text{motif}} \\le \\tau) \\cdot \\left[ \\mathbb{P}(X_{\\text{noise}} \\le \\tau) \\right]^{W-1} $$\nLet $\\Phi(\\cdot)$ be the CDF of the standard normal distribution $\\mathcal{N}(0, 1)$. For a general normal random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, its CDF is given by $\\mathbb{P}(X \\le x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$.\n\nUsing this formula, we can express the probabilities for our variables:\n1.  For the motif signal, $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu_{w}, \\sigma^{2})$:\n    $$ \\mathbb{P}(X_{\\text{motif}} \\le \\tau) = \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) $$\n2.  For the noise signal, $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^{2})$:\n    $$ \\mathbb{P}(X_{\\text{noise}} \\le \\tau) = \\Phi\\left(\\frac{\\tau - 0}{\\sigma}\\right) = \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) $$\nSubstituting these expressions back into the equation for $\\mathbb{P}(Y \\le \\tau)$:\n$$ \\mathbb{P}(Y \\le \\tau) = \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) \\left[ \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) \\right]^{W-1} $$\nThe desired probability is $\\mathbb{P}(Y > \\tau)$, which is obtained by the law of total probability:\n$$ \\mathbb{P}(Y > \\tau) = 1 - \\mathbb{P}(Y \\le \\tau) $$\nTherefore, the final analytical expression for the detection probability is:\n$$ \\mathbb{P}(Y > \\tau) = 1 - \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) \\left[ \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) \\right]^{W-1} $$\nThis expression is in terms of the given parameters $\\mu_{w}$, $\\sigma$, $W$, $\\tau$, and the standard normal CDF $\\Phi(\\cdot)$, as requested by the problem.",
            "answer": "$$\n\\boxed{1 - \\Phi\\left(\\frac{\\tau - \\mu_{w}}{\\sigma}\\right) \\left[ \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) \\right]^{W-1}}\n$$"
        },
        {
            "introduction": "After a model is trained, a critical task is to interpret what biological features the learned filters represent. This exercise guides you through the process of designing and implementing a principled metric to quantify how successfully a CNN filter has \"recovered\" a known ground-truth motif . By applying concepts from information theory, such as the symmetric Kullback-Leibler divergence $D_{\\mathrm{sym}}(p, q)$, you will develop a practical and robust tool for model validation and biological discovery.",
            "id": "3297894",
            "problem": "You are given a known Position Weight Matrix (PWM) and a Convolutional Neural Network (CNN) filter represented by real-valued weights. The goal is to design, implement, and evaluate a quantitative motif recovery metric that uses a symmetric Kullback–Leibler divergence (KLD) between the known PWM and a PWM derived from the filter. The metric must be grounded in first principles of probability distributions over deoxyribonucleic acid (DNA) nucleotides and information theory and must include a principled aggregation across motif positions and alignment handling. The alphabet of nucleotides is $\\{A, C, G, T\\}$ and the PWM rows are ordered in that fixed alphabet.\n\nFundamental base and definitions to use:\n- A PWM $P \\in [0,1]^{L \\times 4}$ for a motif of length $L$ is a row-wise probability distribution over the nucleotides, where for each position $j \\in \\{1,\\dots,L\\}$ the row $P_{j,\\cdot}$ satisfies $\\sum_{a \\in \\{A,C,G,T\\}} P_{j,a} = 1$ and $P_{j,a} \\ge 0$.\n- A CNN filter is given by a real-valued matrix $W \\in \\mathbb{R}^{L_f \\times 4}$ of length $L_f$; from this, derive a probability distribution at each position via the softmax transformation with temperature $\\tau > 0$. For each position $j$ and nucleotide $a$, define $$Q_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.$$\n- To avoid undefined logarithms when any probability is zero, apply Dirichlet-style pseudocount smoothing with parameter $\\alpha > 0$ to each PWM row independently: for any row vector $R_{j,\\cdot}$ that sums to $1$, define the smoothed row $$\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{\\sum_{b \\in \\{A,C,G,T\\}} \\left(R_{j,b} + \\alpha\\right)} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha}.$$\n- The Kullback–Leibler divergence between two discrete distributions $p$ and $q$ over the same support is $$D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right),$$ with all logarithms in the natural base (nats). The symmetric Kullback–Leibler divergence is $$D_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).$$\n- The information content at position $j$ for a PWM row $p = P_{j,\\cdot}$ is defined using natural logarithms as $$I_j = \\log(4) - H(p), \\quad\\text{where}\\quad H(p) = -\\sum_{a} p_a \\log p_a.$$ This yields $I_j \\in [0, \\log(4)]$.\n\nAggregation across positions and alignment:\n- If the lengths of the known PWM and the filter-derived PWM differ, align the shorter PWM within the longer by sliding it across all possible offsets and evaluating the recovery metric at each alignment. Let $P$ have length $L_P$ and $Q$ have length $L_Q$, and let $S = \\min(L_P, L_Q)$ denote the overlap length when aligning the shorter motif within the longer motif. For each offset $s$ permitted by the lengths, define the set of overlapped positions $\\{0,\\dots,S-1\\}$, and compute the position-wise symmetric KLD between the corresponding smoothed rows. Aggregate across positions using one of two schemes:\n    1. Uniform averaging: assign weights $w_j = \\frac{1}{S}$ for $j \\in \\{0,\\dots,S-1\\}$ and compute $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),$$ where $j'$ and $k'$ are the appropriate indices within $P$ and $Q$ under offset $s$ and the chosen alignment direction.\n    2. Information-content weighting: assign weights based on the known PWM rows in the overlap, $w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$, with the convention that if $\\sum I_{u'} = 0$ then use uniform weights. Compute $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right).$$\n- Define the motif recovery metric as the minimum aggregated symmetric KLD across all valid alignments: $$M = \\min_{s} D_{\\mathrm{agg}}(s).$$ Lower values indicate better recovery. Report the result in nats.\n\nImplementation requirements:\n- Implement the described metric exactly as stated, with the filter-to-PWM softmax, per-row pseudocount smoothing, symmetric KLD, two aggregation modes, and sliding alignment selecting the shorter motif to slide across the longer.\n- Use the fixed nucleotide order $\\{A, C, G, T\\}$ consistently.\n- All logarithms must be natural logarithms; report results in nats as decimal floats. Round final outputs to six decimal places in the final line.\n\nTest suite:\nImplement your program to compute $M$ for each of the following four test cases. No external input is used; the test cases are embedded in the program. In all cases, the alphabet order is $\\{A, C, G, T\\}$.\n\nTest case $1$ (happy path, length match, information-content weighting):\n- Known PWM $P^{(1)}$ of length $L_P = 6$:\n$$\n\\begin{bmatrix}\n0.10 & 0.40 & 0.40 & 0.10 \\\\\n0.05 & 0.05 & 0.85 & 0.05 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.70 & 0.10 & 0.10 & 0.10 \\\\\n0.10 & 0.10 & 0.10 & 0.70 \\\\\n0.40 & 0.10 & 0.40 & 0.10\n\\end{bmatrix}\n$$\n- Filter weights $W^{(1)}$ of length $L_Q = 6$, with per-row perturbations added to $\\log(P^{(1)}_{j,\\cdot})$:\nRow $1$: $\\log(P^{(1)}_{1,\\cdot}) + [0.00, -0.10, +0.10, 0.00]$; Row $2$: $\\log(P^{(1)}_{2,\\cdot}) + [-0.05, +0.20, -0.15, 0.00]$; Row $3$: $\\log(P^{(1)}_{3,\\cdot}) + [+0.30, -0.10, -0.10, -0.10]$; Row $4$: $\\log(P^{(1)}_{4,\\cdot}) + [-0.20, +0.05, +0.05, +0.10]$; Row $5$: $\\log(P^{(1)}_{5,\\cdot}) + [0.00, 0.00, 0.00, 0.00]$; Row $6$: $\\log(P^{(1)}_{6,\\cdot}) + [+0.05, -0.05, +0.05, -0.05]$.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-4}$, aggregation mode: information-content weighting.\n\nTest case $2$ (boundary, exact match, uniform averaging):\n- Known PWM $P^{(2)}$ of length $L_P = 4$:\n$$\n\\begin{bmatrix}\n0.30 & 0.20 & 0.40 & 0.10 \\\\\n0.10 & 0.10 & 0.70 & 0.10 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.20 & 0.50 & 0.20 & 0.10\n\\end{bmatrix}\n$$\n- Filter weights $W^{(2)}$ of length $L_Q = 4$: for each row $j$, set $W^{(2)}_{j,a} = \\log\\left(P^{(2)}_{j,a}\\right)$ so that the filter-derived PWM matches the known PWM exactly under softmax.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-9}$, aggregation mode: uniform averaging.\n\nTest case $3$ (length mismatch, submotif alignment, information-content weighting):\n- Known PWM $P^{(3)}$ of length $L_P = 8$:\n$$\n\\begin{bmatrix}\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.80 & 0.05 & 0.05 & 0.10 \\\\\n0.05 & 0.85 & 0.05 & 0.05 \\\\\n0.05 & 0.05 & 0.85 & 0.05 \\\\\n0.10 & 0.05 & 0.05 & 0.80 \\\\\n0.70 & 0.10 & 0.10 & 0.10 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.25 & 0.25 & 0.25 & 0.25\n\\end{bmatrix}\n$$\n- Filter weights $W^{(3)}$ of length $L_Q = 5$: for rows corresponding to $P^{(3)}$ positions $2$ through $6$, set $W^{(3)}_{j,a} = \\log\\left(P^{(3)}_{j+1,a}\\right) + \\delta_{j,a}$, where $\\delta_{j,a}$ are small perturbations, e.g., for each row $j$ use $[+0.01, -0.01, 0.00, 0.00]$.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-5}$, aggregation mode: information-content weighting.\n\nTest case $4$ (edge case with zeros, uniform averaging):\n- Known PWM $P^{(4)}$ of length $L_P = 4$ with deterministic rows:\n$$\n\\begin{bmatrix}\n1.00 & 0.00 & 0.00 & 0.00 \\\\\n0.00 & 1.00 & 0.00 & 0.00 \\\\\n0.00 & 0.00 & 1.00 & 0.00 \\\\\n0.00 & 0.00 & 0.00 & 1.00 \\\\\n\\end{bmatrix}\n$$\n- Filter weights $W^{(4)}$ of length $L_Q = 4$ favoring different nucleotides via large logits:\nRow $1$: $[-2.0, -2.0, -2.0, +2.0]$; Row $2$: $[+2.0, -2.0, -2.0, -2.0]$; Row $3$: $[-2.0, +2.0, -2.0, -2.0]$; Row $4$: $[-2.0, -2.0, +2.0, -2.0]$.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-3}$, aggregation mode: uniform averaging.\n\nFinal output format:\nYour program should produce a single line of output containing the four results $M^{(1)}, M^{(2)}, M^{(3)}, M^{(4)}$ as a comma-separated list enclosed in square brackets, rounded to six decimal places, for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$. The unit is nats (natural logarithm base). No other text should be printed.",
            "solution": "The task is to construct a quantitative motif recovery metric for Convolutional Neural Network (CNN) filter-derived motifs against a known Position Weight Matrix (PWM), rooted in the principles of probability distributions and information theory, and then implement it in code with a small test suite. The derivation proceeds from the following foundational elements.\n\nFirst principles and core definitions:\n1. A Position Weight Matrix (PWM) $P$ is formed by rows $P_{j,\\cdot}$ that are valid discrete probability distributions over nucleotides $\\{A,C,G,T\\}$, i.e., for each position $j$, one has $\\sum_{a} P_{j,a} = 1$ and $P_{j,a} \\ge 0$.\n2. A CNN filter $W$ is a real-valued matrix; to compare it to a PWM, it must be mapped into a valid distribution at each row. The softmax function is a canonical way to transform real-valued logits to probabilities. Given a temperature $\\tau > 0$, the derived PWM $Q$ is\n$$\nQ_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.\n$$\nThis respects the axioms of probability: $Q_{j,a} \\ge 0$ and $\\sum_{a} Q_{j,a} = 1$ for each $j$.\n\n3. To ensure numerical stability and avoid undefined logarithms when a probability component is zero, apply per-row Dirichlet-style pseudocount smoothing, parameterized by a scalar $\\alpha > 0$. For any row distribution $R_{j,\\cdot}$ with $\\sum_{a} R_{j,a} = 1$, we define\n$$\n\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha},\n$$\nwhich preserves normalization and enforces strict positivity $\\widehat{R}_{j,a} > 0$ for all components.\n\n4. The Kullback–Leibler divergence between distributions $p$ and $q$ on the same support, with natural logarithms, is\n$$\nD_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right).\n$$\nIts symmetric counterpart is defined as\n$$\nD_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).\n$$\nThis symmetric version is nonnegative, equals zero if and only if $p = q$, and penalizes discrepancies in both directions.\n\n5. Information content at a motif position quantifies deviation from the uniform distribution. Using natural logarithms, let\n$$\nH(p) = -\\sum_{a} p_a \\log p_a, \\quad I_j = \\log(4) - H(P_{j,\\cdot}).\n$$\nHere, $\\log(4)$ is the maximum entropy over four equiprobable nucleotides, so $I_j \\in [0, \\log(4)]$.\n\nAggregation across positions and alignment:\nA motif of length $L_P$ and a filter-derived PWM of length $L_Q$ may differ in length. For comparison, slide the shorter motif across the longer and compute an aggregated divergence at each valid alignment. Let the shorter motif have length $S$ and the longer have length $L_{\\mathrm{long}}$. For each offset $s \\in \\{0, 1, \\dots, L_{\\mathrm{long}} - S\\}$, define a mapping of rows between the shorter and longer PWMs by direct index correspondence within the overlapped region. For the overlapped rows, compute\n$$\nD_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),\n$$\nwhere $(j',k')$ denote the aligned rows in the known PWM and the filter-derived PWM, and the weights $w_j$ follow one of two schemes:\n- Uniform averaging: $w_j = \\frac{1}{S}$, reflecting equal importance of all positions.\n- Information-content weighting: $w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$, where $I_{j'}$ is computed from the known PWM rows in the overlap. If $\\sum I_{u'} = 0$ (e.g., all rows are uniform), then use $w_j = \\frac{1}{S}$.\n\nThe recovery metric is the minimum aggregated divergence across all valid offsets:\n$$\nM = \\min_s D_{\\mathrm{agg}}(s).\n$$\nA lower $M$ indicates that the filter-derived PWM more closely matches the known PWM at its best alignment, which is what we expect when the filter captures the motif.\n\nAlgorithmic design:\n1. Input preparation: construct the known PWM matrix $P$ and the filter weights matrix $W$, along with scalar parameters $\\tau$ (softmax temperature), $\\alpha$ (pseudocount), and the aggregation mode selection (uniform or information-content weighting).\n2. Softmax conversion: for each row $j$ in $W$, compute $Q_{j,\\cdot}$ using the softmax with temperature $\\tau$, implemented in a numerically stable manner by subtracting the row maximum before exponentiation to avoid overflow. This yields a proper PWM $Q$.\n3. Smoothing: for $P$ and $Q$, apply pseudocount smoothing independently to each row using $\\alpha$, obtaining $\\widehat{P}$ and $\\widehat{Q}$.\n4. Alignment handling: identify the shorter and longer among $\\widehat{P}$ and $\\widehat{Q}$. Slide the shorter across the longer for all valid offsets $s$ and, for each alignment, compute $D_{\\mathrm{agg}}(s)$ via:\n   - Position-wise symmetric KLD: $D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right)$ at each overlapped index.\n   - Weights $w_j$: either uniform $\\frac{1}{S}$ or based on information content $I_{j'}$ of the known PWM rows within the overlapped region. When computing $I_{j'}$, use the smoothed known PWM to ensure strictly positive entries (this prevents undefined logarithms and keeps the measure stable).\n5. Minimization: select $M = \\min_s D_{\\mathrm{agg}}(s)$ as the metric for the case.\n6. Output: produce a single line with the metrics for all test cases, rounded to six decimal places and expressed in nats.\n\nJustification of choices:\n- The softmax conversion from CNN filter logits to per-position distributions is standard in machine learning and ensures compatibility with PWM semantics.\n- Smoothing with $\\alpha$ is necessary for numeric stability because $D_{\\mathrm{KL}}$ requires strictly positive support. The choice of a small $\\alpha$ keeps the original distributions predominant while avoiding undefined logarithms.\n- Symmetric KLD penalizes mismatches symmetrically and is appropriate for comparing two distributions without privileging one direction of divergence.\n- Information-content weighting reflects biological and statistical relevance by emphasizing positions with higher specificity (lower entropy). Uniform averaging serves as a baseline when such weighting is not intended or informative.\n- Sliding alignment addresses the realistic scenario of filters capturing submotifs or being trained with different receptive field sizes than the motif length.\n\nTest suite coverage:\n- Test case $1$ exercises the general case with moderate perturbations and information-content weighting.\n- Test case $2$ checks the boundary of exact equality, expecting a result near zero.\n- Test case $3$ tests length mismatch and alignment sensitivity, with high-information submotifs.\n- Test case $4$ examines deterministic rows with zeros, confirming that smoothing leads to finite divergences and stable computation.\n\nThe implementation strictly adheres to natural logarithms; therefore, all reported values are in nats. The final output is a comma-separated list enclosed in square brackets, containing four decimal floats rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Compute row-wise softmax with temperature tau in a numerically stable way.\n    \"\"\"\n    # Subtract max per row for numerical stability.\n    z = logits / tau\n    z = z - np.max(z, axis=1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\ndef smooth_pwm(pwm: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Apply Dirichlet-style pseudocount smoothing per row:\n    (p + alpha) / (1 + 4*alpha), assuming each row sums to 1.\n    \"\"\"\n    # Each row normalization denominator is 1 + 4*alpha because sum(row) == 1.\n    denom = 1.0 + 4.0 * alpha\n    return (pwm + alpha) / denom\n\ndef symmetric_kl(p: np.ndarray, q: np.ndarray) -> float:\n    \"\"\"\n    Compute symmetric KL divergence between two distributions p and q\n    using natural logarithms. Assumes p,q > 0 and sum to 1.\n    \"\"\"\n    # Small epsilon guard (should not be necessary after smoothing,\n    # but we include minimal safeguard).\n    eps = 1e-300\n    p = np.clip(p, eps, 1.0)\n    q = np.clip(q, eps, 1.0)\n    d1 = np.sum(p * np.log(p / q))\n    d2 = np.sum(q * np.log(q / p))\n    return float(d1 + d2)\n\ndef information_content_rows(pwm: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute information content per row: I_j = log(4) - H(p_j),\n    where H(p) = -sum p log p with natural logarithms.\n    \"\"\"\n    # Ensure strictly positive entries to avoid log(0)\n    eps = 1e-300\n    p = np.clip(pwm, eps, 1.0)\n    entropy = -np.sum(p * np.log(p), axis=1)  # nats\n    I = np.log(4.0) - entropy\n    return I\n\ndef aggregate_divergence(P_hat: np.ndarray, Q_hat: np.ndarray,\n                         mode: str, P_for_weights: np.ndarray) -> float:\n    \"\"\"\n    Align shorter across longer and compute minimum aggregated symmetric KL divergence.\n    mode: 'uniform' or 'ic' (information-content weighting using P_for_weights).\n    P_for_weights: PWM to compute information content for weights (use smoothed known PWM).\n    \"\"\"\n    len_P = P_hat.shape[0]\n    len_Q = Q_hat.shape[0]\n    # Determine which to slide: slide the shorter across the longer\n    if len_P <= len_Q:\n        shorter = P_hat\n        longer = Q_hat\n        weights_base = P_for_weights\n        slide_P_over_Q = True\n    else:\n        shorter = Q_hat\n        longer = P_hat\n        # For weights, we should use known PWM rows; if we are sliding Q over P,\n        # weights_base should be P_for_weights (known PWM).\n        weights_base = P_for_weights\n        slide_P_over_Q = False\n\n    S = shorter.shape[0]\n    L = longer.shape[0]\n    best = np.inf\n    # Precompute info content of weights_base rows as needed\n    info_weights = information_content_rows(weights_base)\n    for s in range(L - S + 1):\n        # Build weights for this overlapped region\n        if mode == 'uniform':\n            w = np.ones(S, dtype=float) / float(S)\n        elif mode == 'ic':\n            # Use information content from the known PWM rows in the overlap\n            if slide_P_over_Q:\n                # Overlap indices in P: 0..S-1\n                I = info_weights[:S]\n            else:\n                # Sliding Q over P: overlap indices in P are s..s+S-1\n                I = info_weights[s:s+S]\n            total_I = np.sum(I)\n            if total_I <= 0.0:\n                w = np.ones(S, dtype=float) / float(S)\n            else:\n                w = I / total_I\n        else:\n            raise ValueError(\"Unknown aggregation mode\")\n\n        # Compute weighted symmetric KL over the overlap at offset s\n        total = 0.0\n        for j in range(S):\n            if slide_P_over_Q:\n                p_row = shorter[j]\n                q_row = longer[s + j]\n            else:\n                p_row = longer[s + j]\n                q_row = shorter[j]\n            d_sym = symmetric_kl(p_row, q_row)\n            total += w[j] * d_sym\n        if total < best:\n            best = total\n    return float(best)\n\ndef motif_recovery_metric(P_known: np.ndarray, W_filter: np.ndarray,\n                          tau: float, alpha: float, agg_mode: str) -> float:\n    \"\"\"\n    Compute the motif recovery metric M between known PWM P_known and filter weights W_filter.\n    Steps:\n    - Convert filter weights to PWM via softmax with temperature tau.\n    - Smooth both P and Q with pseudocount alpha.\n    - Align shorter across longer; compute aggregated symmetric KL at each offset.\n    - Return minimum aggregated divergence across offsets.\n    \"\"\"\n    Q_filter = softmax_rows(W_filter, tau)\n    # Smooth both PWMs\n    P_hat = smooth_pwm(P_known, alpha)\n    Q_hat = smooth_pwm(Q_filter, alpha)\n    # For information-content weighting, use smoothed known PWM\n    M = aggregate_divergence(P_hat, Q_hat, agg_mode, P_hat)\n    return M\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    P1 = np.array([\n        [0.10, 0.40, 0.40, 0.10],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.10, 0.10, 0.10, 0.70],\n        [0.40, 0.10, 0.40, 0.10],\n    ], dtype=float)\n    # Build W1 as log(P1) plus perturbations per row\n    logP1 = np.log(P1)\n    perturbations1 = np.array([\n        [0.00, -0.10, +0.10, 0.00],\n        [-0.05, +0.20, -0.15, 0.00],\n        [+0.30, -0.10, -0.10, -0.10],\n        [-0.20, +0.05, +0.05, +0.10],\n        [0.00, 0.00, 0.00, 0.00],\n        [+0.05, -0.05, +0.05, -0.05],\n    ], dtype=float)\n    W1 = logP1 + perturbations1\n    tau1 = 1.0\n    alpha1 = 1e-4\n    agg1 = 'ic'\n\n    # Test case 2\n    P2 = np.array([\n        [0.30, 0.20, 0.40, 0.10],\n        [0.10, 0.10, 0.70, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.20, 0.50, 0.20, 0.10],\n    ], dtype=float)\n    W2 = np.log(P2)  # exact match under softmax\n    tau2 = 1.0\n    alpha2 = 1e-9\n    agg2 = 'uniform'\n\n    # Test case 3\n    P3 = np.array([\n        [0.25, 0.25, 0.25, 0.25],\n        [0.80, 0.05, 0.05, 0.10],\n        [0.05, 0.85, 0.05, 0.05],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.10, 0.05, 0.05, 0.80],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.25, 0.25, 0.25, 0.25],\n    ], dtype=float)\n    # Filter corresponds to positions 2..6 of P3 with slight perturbations\n    logP3 = np.log(P3)\n    W3_core = logP3[1:6, :]  # rows 2..6\n    perturbations3 = np.array([\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n    ], dtype=float)\n    W3 = W3_core + perturbations3\n    tau3 = 1.0\n    alpha3 = 1e-5\n    agg3 = 'ic'\n\n    # Test case 4\n    P4 = np.array([\n        [1.00, 0.00, 0.00, 0.00],\n        [0.00, 1.00, 0.00, 0.00],\n        [0.00, 0.00, 1.00, 0.00],\n        [0.00, 0.00, 0.00, 1.00],\n    ], dtype=float)\n    W4 = np.array([\n        [-2.0, -2.0, -2.0, +2.0],  # favors T\n        [+2.0, -2.0, -2.0, -2.0],  # favors A\n        [-2.0, +2.0, -2.0, -2.0],  # favors C\n        [-2.0, -2.0, +2.0, -2.0],  # favors G\n    ], dtype=float)\n    tau4 = 1.0\n    alpha4 = 1e-3\n    agg4 = 'uniform'\n\n    test_cases = [\n        (P1, W1, tau1, alpha1, agg1),\n        (P2, W2, tau2, alpha2, agg2),\n        (P3, W3, tau3, alpha3, agg3),\n        (P4, W4, tau4, alpha4, agg4),\n    ]\n\n    results = []\n    for P_known, W_filter, tau, alpha, agg_mode in test_cases:\n        M = motif_recovery_metric(P_known, W_filter, tau, alpha, agg_mode)\n        # Round to 6 decimal places\n        results.append(f\"{M:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}