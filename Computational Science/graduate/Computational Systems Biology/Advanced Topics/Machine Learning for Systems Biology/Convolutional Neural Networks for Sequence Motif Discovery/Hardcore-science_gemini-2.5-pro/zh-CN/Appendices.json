{
    "hands_on_practices": [
        {
            "introduction": "要真正理解卷积神经网络（CNN）如何发现基序，我们必须首先将其与成熟的生物信息学方法联系起来。本练习将指导您从头开始实现基于对数几率分数的经典位置权重矩阵（PWM）扫描算法，并证明该过程在数学上等同于CNN滤波器在独热编码序列上执行的卷积操作。通过亲手构建，您将对滤波器权重如何对应于基序的概率表示有一个具体的认识 ()。",
            "id": "3297925",
            "problem": "您的任务是实现一个严格的位置权重矩阵（PWM）扫描，并证明在序列基序发现的背景下，它等同于卷积神经网络（CNN）的单通道、多滤波器操作。科学基础应从概率模型定义和对数似然比开始。位置权重矩阵（PWM）通过一组位置特异性的多项分布来表示一个长度为 $L$ 的基序，该分布基于脱氧核糖核酸（DNA）的核苷酸。背景模型给出独立的核苷酸概率。您必须计算DNA序列上窗口化的对数优势比分数，并找出所有分数严格超过给定阈值的起始索引。\n\n基本原理：\n- 设基序由一个PWM表示，其条目为 $p_{i,b}$，其中 $i \\in \\{0,1,\\dots,L-1\\}$ 是位置索引，$b \\in \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 是核苷酸索引。设背景分布为 $q_b$，满足 $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ 且每个 $q_b > 0$。\n- 在独立位置假设下，对于一个长度为 $L$ 的子序列 $x_{t:t+L-1}$，基序模型赋予其概率 $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$，而背景模型赋予其概率 $\\prod_{i=0}^{L-1} q_{x_{t+i}}$。对数优势比（对数似然比）分数定义为\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\n- 定义对数优势比权重矩阵 $W$，其元素为 $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$，使用自然对数。将DNA序列编码为一个独热矩阵 $X \\in \\{0,1\\}^{T \\times 4}$，其中 $T$ 是序列长度，通道按顺序对应 $\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}$。对于任何未知的核苷酸符号（如 $N$），编码为零向量 $(0,0,0,0)$，使其对对数优势比总和的贡献为零。那么分数可以写成\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}} W_{i,b} \\, X_{t+i,b},\n$$\n这正是在卷积神经网络（CNN）中使用的有效模式多通道卷积，其中滤波器 $W$ 跨越 $L$ 个位置和 $4$ 个通道。\n\n程序要求：\n- 实现一个函数，给定一个DNA序列、一个由其各位置核苷酸概率 $p_{i,b}$ 指定的PWM、一个背景分布 $q_b$ 和一个实值阈值 $\\tau$，计算所有有效起始位置 $t \\in \\{0,1,\\dots,T-L\\}$ 的 $S(t)$，并返回满足 $S(t) > \\tau$ 的索引 $t$ 的列表。\n- 所有对数优势比计算均使用自然对数。按照上述规定，通过在未知核苷酸 $N$ 的位置贡献 $0$ 来处理它们。\n- 严格遵循“超过”标准：仅当 $S(t)$ 严格大于 $\\tau$ 时才包括该位置。\n- 仅在正向链上操作；不考虑反向互补链。\n- 所有数学量必须严格按照上述定义处理。\n\n测试套件：\n您必须硬编码并评估以下五个测试用例。对于每个用例，报告分数严格超过指定阈值的起始索引列表。\n\n- 用例 $1$（理想情况，均匀背景，精确基序重复出现）：\n    - 序列：$\\texttt{ACGTACGTACGT}$。\n    - PWM 长度 $L = 4$，各位置的概率（顺序为 $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$）：\n        - 位置 $0$：$(0.7, 0.1, 0.1, 0.1)$。\n        - 位置 $1$：$(0.1, 0.7, 0.1, 0.1)$。\n        - 位置 $2$：$(0.1, 0.1, 0.7, 0.1)$。\n        - 位置 $3$：$(0.1, 0.1, 0.1, 0.7)$。\n    - 背景 $q = (0.25, 0.25, 0.25, 0.25)$。\n    - 阈值 $\\tau = 3.5$。\n\n- 用例 $2$（边界情况，没有窗口超过高阈值）：\n    - 序列：$\\texttt{AAAAACCCCC}$。\n    - PWM 和背景与用例 $1$ 相同。\n    - 阈值 $\\tau = 4.0$。\n\n- 用例 $3$（未知符号 $N$ 减少贡献，但精确匹配出现一次）：\n    - 序列：$\\texttt{NNNACGTNNN}$。\n    - PWM 和背景与用例 $1$ 相同。\n    - 阈值 $\\tau = 3.5$。\n\n- 用例 $4$（非均匀背景，不同的PWM长度和组成）：\n    - 序列：$\\texttt{ACGACGTTAC}$。\n    - PWM 长度 $L = 3$，概率（顺序为 $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$）：\n        - 位置 $0$：$(0.6, 0.2, 0.1, 0.1)$。\n        - 位置 $1$：$(0.1, 0.6, 0.2, 0.1)$。\n        - 位置 $2$：$(0.1, 0.2, 0.6, 0.1)$。\n    - 背景 $q = (0.1, 0.4, 0.4, 0.1)$。\n    - 阈值 $\\tau = 2.4$。\n\n- 用例 $5$（精确等于阈值的情况不应包括在内）：\n    - 序列：$\\texttt{ACGT}$。\n    - PWM 和背景与用例 $1$ 相同。\n    - 阈值 $\\tau = 4.118477668724633$。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且本身是一个由严格超过阈值的位置组成的整数列表，按上述测试套件的顺序排列。例如，一个输出可能看起来像 $[\\,[0,4,8],[],[3],[0,3],[]\\,]$。",
            "solution": "我们从基序的独立位置模型和独立同分布的背景模型开始。设一个长度为 $L$ 的基序由一个位置权重矩阵（PWM）描述，其条目为 $p_{i,b}$，其中 $i \\in \\{0,\\dots,L-1\\}$ 索引基序位置，$b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ 索引核苷酸。设背景模型由 $q_b$ 参数化，使得 $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ 且每个 $q_b > 0$。对于一个DNA序列 $x_0 x_1 \\dots x_{T-1}$，考虑任何窗口起始索引 $t$，其中 $0 \\le t \\le T-L$。基序模型赋予子序列 $x_{t:t+L-1}$ 的概率为 $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$，而背景模型赋予的概率为 $\\prod_{i=0}^{L-1} q_{x_{t+i}}$。标准的扫描分数是自然对数似然比\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\n定义对数优势比权重矩阵 $W$，其元素为 $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$，我们有\n$$\nS(t) = \\sum_{i=0}^{L-1} W_{i, x_{t+i}}.\n$$\n引入独热编码 $X \\in \\{0,1\\}^{T \\times 4}$，通道顺序为 $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$，即如果 $x_j=b$，则 $X_{j,b} = 1$，否则为 $0$；对于未知符号（例如 $N$），对所有 $b$ 设置 $X_{j,b}=0$。然后，对于任何起始位置 $t$，分数可以写成\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b} W_{i,b} \\, X_{t+i,b},\n$$\n这与在序列长度维度上使用一个跨越 $L$ 个位置和 $4$ 个通道的滤波器 $W$ 进行的有效模式卷积是相同的，正如在卷积神经网络（CNN）中用于序列基序发现那样。用CNN的术语来说，$S(t)$ 是在位置 $t$ 将单个滤波器应用于独热编码输入后的激活值。\n\n算法设计：\n- 通过 $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$ 使用自然对数计算 $W$，由于所提供的测试套件中每个 $p_{i,b}$ 和 $q_b$ 都是严格为正的，这保证了值的有限性。\n- 对序列进行独热编码，映射 $\\mathrm{A} \\mapsto (1,0,0,0)$, $\\mathrm{C} \\mapsto (0,1,0,0)$, $\\mathrm{G} \\mapsto (0,0,1,0)$, $\\mathrm{T} \\mapsto (0,0,0,1)$，以及 $N \\mapsto (0,0,0,0)$。\n- 对于每个有效的起始索引 $t$，计算 $S(t)$ 作为张量缩并 $S(t) = \\sum_{i,b} W_{i,b} X_{t+i,b}$，并收集那些满足 $S(t) > \\tau$ 的 $t$。\n- 对五个测试用例中的每一个执行此操作，并按顺序汇总结果。\n\n对所提供测试套件的分析预期：\n- 用例 1：该基序对模式 $\\texttt{ACGT}$ 在各个位置上具有高度特异性。在均匀背景 $q_b = 0.25$ 下，每个位置匹配的对数优势比权重为 $\\log(0.7/0.25) = \\log(2.8) \\approx 1.029619417$，错配的权重为 $\\log(0.1/0.25) = \\log(0.4) \\approx -0.916290732$。一个完美的匹配子序列 $\\texttt{ACGT}$ 产生的总分数为 $4 \\times \\log(2.8) \\approx 4.118477669$，严格超过 $\\tau = 3.5$。在序列 $\\texttt{ACGTACGTACGT}$ 中，完美匹配出现在位置 $0$、 $4$ 和 $8$。所有其他长度为 $4$ 的窗口都是导致全部错配的旋转，产生负分，所以结果是 $[0,4,8]$。\n- 用例 2：使用相同的PWM和背景，序列 $\\texttt{AAAAACCCCC}$ 不包含 $\\texttt{ACGT}$ 窗口。即使是最好的窗口也无法超过 $\\tau = 4.0$。例如，$\\texttt{AAAA}$ 产生 $S \\approx 1.029619417 + 3 \\times (-0.916290732) \\approx -1.719252779$。因此，没有位置超过阈值，结果是 $[]$。\n- 用例 3：根据指定的编码和求和规则，未知符号 $N$ 的贡献为零。序列 $\\texttt{NNNACGTNNN}$ 在位置 $3$ 处包含一个完美匹配 $\\texttt{ACGT}$，其 $S \\approx 4.118477669 > 3.5$，而所有与 $N$ 重叠但没有精确基序的窗口累积的分数不足或为负。因此，结果是 $[3]$。\n- 用例 4：非均匀背景 $q = (0.1, 0.4, 0.4, 0.1)$ 和对 $\\texttt{ACG}$ 特异的PWM长度 $L=3$。完美匹配分数为 $\\log(0.6/0.1) + \\log(0.6/0.4) + \\log(0.6/0.4) = \\log(6) + 2 \\log(1.5) \\approx 2.602689685$，超过了 $\\tau = 2.4$。在序列 $\\texttt{ACGACGTTAC}$ 中，完美匹配出现在位置 $0$ 和 $3$，其他窗口不是最优的，分数低于阈值。因此结果是 $[0,3]$。\n- 用例 5：阈值被设置为 $\\tau = 4.118477668724633$，这略大于从PWM和均匀背景中得到的精确完美匹配分数。对于序列 $\\texttt{ACGT}$，唯一的窗口是整个序列，其获得的分数约为 $4.118477668724632$，由于严格不等式，它没有严格超过阈值。因此，结果是 $[]$。\n\n实现说明：\n- 计算一致使用自然对数。\n- 程序生成单行输出，按顺序汇总所有五个用例的结果，格式为 $[\\,[0,4,8],[],[3],[0,3],[]\\,]$。\n- 该方法展示了统计PWM扫描与在计算系统生物学中用于基序发现的卷积神经网络（CNN）核心的有效模式多通道卷积之间的联系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nNUC_ORDER = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\ndef one_hot_encode(seq: str) -> np.ndarray:\n    \"\"\"\n    Encode DNA sequence into one-hot with channels in order A, C, G, T.\n    Unknowns (e.g., 'N') are encoded as all zeros.\n    \"\"\"\n    X = np.zeros((len(seq), 4), dtype=float)\n    for i, ch in enumerate(seq.upper()):\n        idx = NUC_ORDER.get(ch, None)\n        if idx is not None:\n            X[i, idx] = 1.0\n        # else: unknown contributes 0 vector\n    return X\n\ndef log_odds_weights(pwm_probs: np.ndarray, background: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute log-odds weight matrix W_{i,b} = ln(p_{i,b} / q_b).\n    pwm_probs: shape (L, 4)\n    background: shape (4,)\n    \"\"\"\n    # Ensure arrays are float for division and log\n    pwm = np.array(pwm_probs, dtype=float)\n    q = np.array(background, dtype=float)\n    # Avoid division by zero: test suite guarantees q_b > 0 and p_{i,b} > 0\n    W = np.log(pwm / q[None, :])\n    return W\n\ndef pwm_scan_positions(seq: str, pwm_probs: np.ndarray, background: np.ndarray, threshold: float) -> list:\n    \"\"\"\n    Perform PWM scanning: compute windowed log-odds scores and return positions exceeding threshold.\n    Uses natural logarithm and strict inequality.\n    \"\"\"\n    X = one_hot_encode(seq)\n    W = log_odds_weights(pwm_probs, background)\n    L = W.shape[0]\n    T = X.shape[0]\n    results = []\n    # Slide window and compute score S(t) = sum_{i,b} W[i,b] * X[t+i,b]\n    # Equivalent to valid-mode multi-channel convolution\n    for t in range(T - L + 1):\n        window = X[t:t+L, :]  # shape (L, 4)\n        score = float(np.sum(window * W))\n        if score > threshold:\n            results.append(t)\n    return results\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1\n    seq1 = \"ACGTACGTACGT\"\n    pwm1 = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # position 0\n        [0.1, 0.7, 0.1, 0.1],  # position 1\n        [0.1, 0.1, 0.7, 0.1],  # position 2\n        [0.1, 0.1, 0.1, 0.7],  # position 3\n    ], dtype=float)\n    bg_uniform = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)\n    tau1 = 3.5\n\n    # Case 2\n    seq2 = \"AAAAACCCCC\"\n    tau2 = 4.0\n\n    # Case 3\n    seq3 = \"NNNACGTNNN\"\n    tau3 = 3.5\n\n    # Case 4\n    seq4 = \"ACGACGTTAC\"\n    pwm4 = np.array([\n        [0.6, 0.2, 0.1, 0.1],  # position 0\n        [0.1, 0.6, 0.2, 0.1],  # position 1\n        [0.1, 0.2, 0.6, 0.1],  # position 2\n    ], dtype=float)\n    bg_nonuniform = np.array([0.1, 0.4, 0.4, 0.1], dtype=float)\n    tau4 = 2.4\n\n    # Case 5\n    seq5 = \"ACGT\"\n    tau5 = 4.118477668724633  # Slightly above the perfect match sum for PWM1 under uniform bg\n\n    test_cases = [\n        (seq1, pwm1, bg_uniform, tau1),\n        (seq2, pwm1, bg_uniform, tau2),\n        (seq3, pwm1, bg_uniform, tau3),\n        (seq4, pwm4, bg_nonuniform, tau4),\n        (seq5, pwm1, bg_uniform, tau5),\n    ]\n\n    results = []\n    for seq, pwm, bg, tau in test_cases:\n        positions = pwm_scan_positions(seq, pwm, bg, tau)\n        results.append(positions)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line: list of lists of integers\n    print(f\"[{','.join([str(lst) for lst in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "卷积操作会产生一个激活图谱，但我们该如何概括其中的信息呢？本练习将深入探讨池化层的关键作用，通过比较最大池化、平均池化和随机池化这三种不同的策略。您将超越简单的定义，进行更深入的统计分析，推导并实现公式，以观察每种方法在噪声中检测微弱或重复基序实例时的表现。这项练习突出了CNN架构中重要的设计权衡及其对生物信号检测能力的影响 ()。",
            "id": "3297848",
            "problem": "考虑一个一维卷积神经网络（CNN）处理DNA序列，使用单个卷积滤波器来检测特定的短序列基序。假设滤波器在池化窗口内各个位置的预激活值遵循以下生成模型：每个位置 $i \\in \\{1,\\dots,n\\}$ 产生一个标量预激活值 $X_i = s_i \\mu + \\varepsilon_i$，其中 $s_i \\in \\{0,1\\}$ 表示在位置 $i$ 处是否存在（$1$）或不存在（$0$）基序实例，每个基序实例的信号幅度为 $\\mu > 0$，噪声 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 在各个位置上独立，方差为 $\\sigma^2$。网络在池化前应用修正线性单元（ReLU）非线性变换，得到 $A_i = \\max(0,X_i)$。在一个大小为 $n$ 的池化窗口中，假设有 $m$ 个位置 $s_i = 1$（存在基序），以及 $n - m$ 个位置 $s_i = 0$（不存在基序）。如果池化输出超过一个固定的非负阈值 $\\tau \\ge 0$，则认为检测成功。\n\n定义三种池化选择：\n- 最大池化：$Y_{\\text{max}} = \\max_{1 \\le i \\le n} A_i$。\n- 平均池化：$Y_{\\text{avg}} = \\frac{1}{n} \\sum_{i=1}^n A_i$。\n- 随机池化：以概率 $p_i = \\frac{A_i}{\\sum_{j=1}^n A_j}$ 选择一个索引 $I$，并设 $Y_{\\text{stoch}} = A_I$。\n\n在上述独立性和高斯噪声假设下，从第一性原理出发，为每种池化方法推导检测概率 $\\mathbb{P}(Y_{\\text{pool}} \\ge \\tau)$ 的表达式。表达式应使用参数 $(n,m,\\mu,\\sigma^2,\\tau)$ 表示，且仅使用标准正态分布的标准函数。您的推导必须从分布的定义和独立性假设开始，并使用关于高斯随机变量和修正线性单元（ReLU）截断的成熟结论。对于随机池化，提供一个大 $n$ 渐近近似，该近似应由大数定律提供科学依据，并得出一个包含截断高斯随机变量期望的封闭形式表达式。\n\n您的程序必须实现推导出的公式，以计算每种池化方法的检测概率，并在一组测试用例上评估池化选择对检测微弱、重复的基序实例的影响。将所有概率表示为保留六位小数的浮点数。\n\n使用以下参数元组 $(n,m,\\mu,\\sigma^2,\\tau)$ 的测试套件：\n- 情况 $1$：$(n,m,\\mu,\\sigma^2,\\tau) = (10,3,0.5,0.25,0.6)$。\n- 情况 $2$：$(n,m,\\mu,\\sigma^2,\\tau) = (50,20,0.2,1.0,0.3)$。\n- 情况 $3$：$(n,m,\\mu,\\sigma^2,\\tau) = (8,0,0.5,0.25,0.1)$。\n- 情况 $4$：$(n,m,\\mu,\\sigma^2,\\tau) = (15,5,0.3,0.09,1.0)$。\n- 情况 $5$：$(n,m,\\mu,\\sigma^2,\\tau) = (10,2,0.8,0.01,0.7)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身是一个包含三个按顺序排列的舍入浮点数 $[p_{\\text{max}},p_{\\text{avg}},p_{\\text{stoch}}]$ 的列表。例如，输出的形式应为 $[[p_{1,\\text{max}},p_{1,\\text{avg}},p_{1,\\text{stoch}}],[p_{2,\\text{max}},p_{2,\\text{avg}},p_{2,\\text{stoch}}],\\dots]$。",
            "solution": "该问题定义良好且具有科学依据，为一个卷积神经网络的激活值定义了一个简化但连贯的生成模型。我们的任务是为三种不同的池化操作（最大池化、平均池化和随机池化）推导检测概率 $\\mathbb{P}(Y_{\\text{pool}} \\ge \\tau)$。推导将从第一性原理出发，基于所给定的分布和独立性假设进行。\n\n设预激活值为 $X_i = s_i \\mu + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立同分布（i.i.d.）的高斯噪声项。大小为 $n$ 的窗口包含 $m$ 个 $s_i=1$（基序）的位置和 $n-m$ 个 $s_i=0$（噪声）的位置。因此，我们有两类预激活值：\n1. $m$ 个 $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu, \\sigma^2)$ 实例。\n2. $n-m$ 个 $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^2)$ 实例。\n经过ReLU后的激活值为 $A_i = \\max(0, X_i)$。$A_i$是独立的随机变量，从两个不同的分布中抽取，分别对应于 $A_{\\text{motif}} = \\max(0, X_{\\text{motif}})$ 和 $A_{\\text{noise}} = \\max(0, X_{\\text{noise}})$。设 $\\Phi(\\cdot)$ 和 $\\phi(\\cdot)$ 分别表示标准正态分布 $\\mathcal{N}(0,1)$ 的累积分布函数（CDF）和概率密度函数（PDF）。检测阈值为 $\\tau \\ge 0$。\n\n每种池化方法的检测概率推导如下。\n\n对于最大池化，输出为 $Y_{\\text{max}} = \\max_{1 \\le i \\le n} A_i$。检测概率为 $\\mathbb{P}(Y_{\\text{max}} \\ge \\tau)$。计算其互补事件（即未检测到）的概率更方便，即 $\\mathbb{P}(Y_{\\text{max}}  \\tau) = \\mathbb{P}(A_1  \\tau, A_2  \\tau, \\dots, A_n  \\tau)$。由于 $A_i$ 的独立性，这个联合概率可以分解为各个概率的乘积：\n$$ \\mathbb{P}(Y_{\\text{max}}  \\tau) = [\\mathbb{P}(A_{\\text{motif}}  \\tau)]^m [\\mathbb{P}(A_{\\text{noise}}  \\tau)]^{n-m} $$\n对于任何激活值 $A_i = \\max(0, X_i)$ 和一个非负阈值 $\\tau \\ge 0$，条件 $A_i  \\tau$ 等价于 $X_i  \\tau$。现在我们可以通过标准化变量来计算每个群体的概率。对于 $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu, \\sigma^2)$，我们有 $\\mathbb{P}(A_{\\text{motif}}  \\tau) = \\mathbb{P}(X_{\\text{motif}}  \\tau) = \\Phi\\left(\\frac{\\tau - \\mu}{\\sigma}\\right)$。对于 $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^2)$，我们有 $\\mathbb{P}(A_{\\text{noise}}  \\tau) = \\mathbb{P}(X_{\\text{noise}}  \\tau) = \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)$。因此，检测概率为：\n$$ P_{\\text{max}} = \\mathbb{P}(Y_{\\text{max}} \\ge \\tau) = 1 - \\left[\\Phi\\left(\\frac{\\tau - \\mu}{\\sigma}\\right)\\right]^m \\left[\\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right]^{n-m} $$\n\n对于平均池化，输出为 $Y_{\\text{avg}} = \\frac{1}{n} \\sum_{i=1}^n A_i$。检测概率为 $\\mathbb{P}(Y_{\\text{avg}} \\ge \\tau)$，这等价于 $\\mathbb{P}(\\sum_{i=1}^n A_i \\ge n\\tau)$。和 $S_A = \\sum_{i=1}^n A_i$ 是一组独立但不同分布的修正高斯随机变量之和。$S_A$ 的精确分布是一个复杂的卷积。我们应用中心极限定理（CLT）来用正态分布近似 $S_A$ 的分布，即 $S_A \\approx \\mathcal{N}(\\mathbb{E}[S_A], \\text{Var}(S_A))$。首先，我们必须找到一个通用修正高斯变量 $A = \\max(0, X)$ 的均值和方差，其中 $X \\sim \\mathcal{N}(\\nu, \\sigma^2)$。均值为 $\\mathbb{E}[A] = \\int_0^\\infty x \\frac{1}{\\sigma}\\phi(\\frac{x-\\nu}{\\sigma})dx = \\sigma\\phi(\\nu/\\sigma) + \\nu\\Phi(\\nu/\\sigma)$。二阶矩为 $\\mathbb{E}[A^2] = \\int_0^\\infty x^2 \\frac{1}{\\sigma}\\phi(\\frac{x-\\nu}{\\sigma})dx = (\\sigma^2+\\nu^2)\\Phi(\\nu/\\sigma) + \\sigma\\nu\\phi(\\nu/\\sigma)$。方差为 $\\text{Var}(A) = \\mathbb{E}[A^2] - (\\mathbbE[A])^2$。将这些应用到存在基序（$\\nu=\\mu$）和不存在基序（$\\nu=0$）的群体上，我们可以计算出均值 $E_S = \\mathbb{E}[S_A] = m \\cdot \\mathbb{E}[A_{\\text{motif}}] + (n-m) \\cdot \\mathbb{E}[A_{\\text{noise}}]$ 和方差 $V_S = \\text{Var}(S_A) = m \\cdot \\text{Var}(A_{\\text{motif}}) + (n-m) \\cdot \\text{Var}(A_{\\text{noise}})$。使用CLT近似，检测概率为：\n$$ P_{\\text{avg}} = \\mathbb{P}(S_A \\ge n\\tau) = \\mathbb{P}\\left(\\frac{S_A - E_S}{\\sqrt{V_S}} \\ge \\frac{n\\tau - E_S}{\\sqrt{V_S}}\\right) = 1 - \\Phi\\left(\\frac{n\\tau - E_S}{\\sqrt{V_S}}\\right) $$\n\n对于随机池化，输出 $Y_{\\text{stoch}} = A_I$ 的检测概率为 $P_{\\text{stoch}} = \\mathbb{P}(Y_{\\text{stoch}} \\ge \\tau)$。根据全期望定律，这等于 $P_{\\text{stoch}} = \\mathbb{E}\\left[\\frac{\\sum_i A_i \\mathbb{I}(A_i \\ge \\tau)}{\\sum_j A_j}\\right]$。这是一个随机变量比率的期望。我们使用一个由大数定律证明其合理性的大 $n$ 近似：$\\mathbb{E}[W/S_A] \\approx \\mathbb{E}[W]/\\mathbb{E}[S_A]$，其中 $W = \\sum_i A_i \\mathbb{I}(A_i \\ge \\tau)$ 且 $S_A = \\sum_j A_j$。分母 $\\mathbb{E}[S_A]$与平均池化计算的$E_S$相同。分子是 $\\mathbb{E}[W] = m \\cdot \\mathbb{E}[A_{\\text{motif}} \\mathbb{I}(A_{\\text{motif}} \\ge \\tau)] + (n-m) \\cdot \\mathbb{E}[A_{\\text{noise}} \\mathbb{I}(A_{\\text{noise}} \\ge \\tau)]$。一个在 $\\tau \\ge 0$ 处进行下截断的修正高斯变量的期望是 $\\mathbb{E}[A \\cdot \\mathbb{I}(A \\ge \\tau)] = \\int_\\tau^\\infty x \\frac{1}{\\sigma}\\phi(\\frac{x-\\nu}{\\sigma})dx = \\sigma\\phi(\\frac{\\tau-\\nu}{\\sigma}) + \\nu(1 - \\Phi(\\frac{\\tau-\\ν}{\\sigma}))$。我们将其表示为 $E_{A,\\tau}(\\nu, \\sigma)$。近似的最终表达式为：\n$$ P_{\\text{stoch}} \\approx \\frac{m \\cdot E_{A,\\tau}(\\mu, \\sigma) + (n-m) \\cdot E_{A,\\tau}(0, \\sigma)}{m \\cdot \\mathbb{E}[A_{\\text{motif}}] + (n-m) \\cdot \\mathbb{E}[A_{\\text{noise}}]} $$",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_probabilities(n, m, mu, sigma_sq, tau):\n    \"\"\"\n    Calculates the detection probabilities for max, average, and stochastic pooling.\n    \n    Args:\n        n (int): Pooling window size.\n        m (int): Number of motif-present positions.\n        mu (float): Signal amplitude per motif instance.\n        sigma_sq (float): Noise variance.\n        tau (float): Detection threshold.\n        \n    Returns:\n        list: A list containing [p_max, p_avg, p_stoch].\n    \"\"\"\n    if sigma_sq == 0:\n        # Avoid division by zero and handle deterministic case, though not in test suite.\n        # This implementation assumes sigma_sq > 0 as per problem context.\n        sigma = 1e-9 \n    else:\n        sigma = np.sqrt(sigma_sq)\n\n    # Standard normal functions\n    phi = norm.pdf\n    Phi = norm.cdf\n\n    # --- Max Pooling ---\n    # P(Y_max >= tau) = 1 - P(Y_max < tau)\n    # P(Y_max < tau) = P(A_motif < tau)^m * P(A_noise < tau)^(n-m)\n    # P(A < tau) = P(max(0,X) < tau) = P(X < tau) for tau >= 0\n    p_non_detection_motif = Phi((tau - mu) / sigma) if m > 0 else 1.0\n    p_non_detection_noise = Phi(tau / sigma) if (n - m) > 0 else 1.0\n    \n    # Handle potential floating point errors with powers\n    p_max_nd = (p_non_detection_motif ** m) * (p_non_detection_noise ** (n - m))\n    p_max = 1.0 - p_max_nd\n\n    # --- Common calculations for Average and Stochastic Pooling ---\n\n    # Helper function for mean of rectified Gaussian N(nu, sigma^2)\n    def E_A(nu, sig):\n        z = nu / sig\n        return sig * phi(z) + nu * Phi(z)\n\n    # Helper function for variance of rectified Gaussian N(nu, sigma^2)\n    def Var_A(nu, sig):\n        z = nu / sig\n        mean_A = E_A(nu, sig)\n        E_A_sq = (sig**2 + nu**2) * Phi(z) + nu * sig * phi(z)\n        return E_A_sq - mean_A**2\n\n    # Moments for motif and noise populations\n    mean_A_motif = E_A(mu, sigma)\n    var_A_motif = Var_A(mu, sigma)\n    \n    mean_A_noise = E_A(0, sigma)\n    var_A_noise = Var_A(0, sigma)\n\n    # --- Average Pooling (CLT Approximation) ---\n    # S_A ~ N(E_S, V_S)\n    E_S = m * mean_A_motif + (n - m) * mean_A_noise\n    V_S = m * var_A_motif + (n - m) * var_A_noise\n\n    # Handle V_S=0 case\n    if V_S == 0:\n        # If variance is zero, distribution is a point mass at E_S\n        p_avg = 1.0 if E_S >= n * tau else 0.0\n    else:\n        # P(S_A >= n*tau)\n        z_avg = (n * tau - E_S) / np.sqrt(V_S)\n        p_avg = 1.0 - Phi(z_avg)\n\n    # --- Stochastic Pooling (Large-n Approximation) ---\n    # P_stoch ~= E[W] / E[S_A] where W = sum(A_i * I(A_i >= tau))\n    \n    # Helper for E[A * I(A >= tau)] for A=max(0,N(nu,sigma^2))\n    def E_A_trunc_tau(nu, sig, t):\n        z = (t - nu) / sig\n        return sig * phi(z) + nu * (1 - Phi(z))\n\n    E_W_motif = E_A_trunc_tau(mu, sigma, tau)\n    E_W_noise = E_A_trunc_tau(0, sigma, tau)\n    \n    E_W = m * E_W_motif + (n - m) * E_W_noise\n    \n    # Denominator E_S is the same as for average pooling\n    if E_S > 0:\n        p_stoch = E_W / E_S\n    else:\n        # This case happens if all activations are almost surely zero\n        p_stoch = 0.0\n\n    # Ensure probabilities are in [0,1] range\n    p_max = np.clip(p_max, 0, 1)\n    p_avg = np.clip(p_avg, 0, 1)\n    p_stoch = np.clip(p_stoch, 0, 1)\n\n    return [p_max, p_avg, p_stoch]\n\n\ndef solve():\n    \"\"\"\n    Executes the analysis for the test suite and prints the final results.\n    \"\"\"\n    test_cases = [\n        # (n, m, mu, sigma^2, tau)\n        (10, 3, 0.5, 0.25, 0.6),\n        (50, 20, 0.2, 1.0, 0.3),\n        (8, 0, 0.5, 0.25, 0.1),\n        (15, 5, 0.3, 0.09, 1.0),\n        (10, 2, 0.8, 0.01, 0.7),\n    ]\n\n    results_list = []\n    for case in test_cases:\n        n, m, mu, sigma_sq, tau = case\n        probabilities = calculate_probabilities(n, m, mu, sigma_sq, tau)\n        results_list.append(probabilities)\n\n    # Format the output string as per problem specification\n    rounded_results = [[round(p, 6) for p in res] for res in results_list]\n    results_str_list = [str(r).replace(' ', '') for r in rounded_results]\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "一个训练好的CNN在被我们有效解释之前，就像一个“黑箱”。这项综合性练习将通过让您开发一个严谨的基序恢复量化指标，来应对模型评估这一关键挑战。您将利用信息论的原理（如Kullback-Leibler散度），实现一个比较CNN滤波器权重与已知PWM的完整流程。通过解决长度不匹配和位置重要性不均等等实际问题，您将构建一个强大的工具，用于验证您的网络是否成功发现了具有生物学意义的特征 ()。",
            "id": "3297894",
            "problem": "给定一个已知的位置权重矩阵 (PWM) 和一个由实值权重表示的卷积神经网络 (CNN) 滤波器。目标是设计、实现并评估一个定量的基序（motif）恢复度量标准，该度量标准使用已知 PWM 与从该滤波器派生的 PWM 之间的对称 Kullback–Leibler 散度 (KLD)。该度量标准必须基于脱氧核糖核酸 (DNA) 核苷酸上的概率分布和信息论的第一性原理，并且必须包括一种有原则的跨基序位置的聚合方法和对齐处理机制。核苷酸字母表为 $\\{A, C, G, T\\}$，并且 PWM 的行按此固定字母表顺序排列。\n\n使用的基本原理和定义：\n- 对于一个长度为 $L$ 的基序，其 PWM $P \\in [0,1]^{L \\times 4}$ 是一个关于核苷酸的逐行概率分布，其中对于每个位置 $j \\in \\{1,\\dots,L\\}$，行 $P_{j,\\cdot}$ 满足 $\\sum_{a \\in \\{A,C,G,T\\}} P_{j,a} = 1$ 且 $P_{j,a} \\ge 0$。\n- 一个 CNN 滤波器由一个长度为 $L_f$ 的实值矩阵 $W \\in \\mathbb{R}^{L_f \\times 4}$ 给出；由此，通过带有温度参数 $\\tau > 0$ 的 softmax 变换，在每个位置上派生出一个概率分布。对于每个位置 $j$ 和核苷酸 $a$，定义 $$Q_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.$$\n- 为避免任何概率为零时出现未定义的对数，对每个 PWM 行独立应用参数为 $\\alpha > 0$ 的 Dirichlet 风格伪计数平滑：对于任何总和为 1 的行向量 $R_{j,\\cdot}$，定义平滑后的行 $$\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{\\sum_{b \\in \\{A,C,G,T\\}} \\left(R_{j,b} + \\alpha\\right)} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha}.$$\n- 两个在相同支撑集上的离散分布 $p$ 和 $q$ 之间的 Kullback–Leibler 散度为 $$D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right),$$ 所有对数均以自然底数（nats）为基。对称 Kullback–Leibler 散度为 $$D_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).$$\n- 对于 PWM 的第 $j$ 行 $p = P_{j,\\cdot}$，其位置 $j$ 上的信息含量使用自然对数定义为 $$I_j = \\log(4) - H(p), \\quad\\text{其中}\\quad H(p) = -\\sum_{a} p_a \\log p_a.$$ 这得到 $I_j \\in [0, \\log(4)]$。\n\n跨位置聚合与对齐：\n- 如果已知 PWM 和从滤波器派生的 PWM 的长度不同，则通过将较短的 PWM 在所有可能的偏移量上滑动，在较长的 PWM 内进行对齐，并在每个对齐位置评估恢复度量。设 $P$ 的长度为 $L_P$，$Q$ 的长度为 $L_Q$，并令 $S = \\min(L_P, L_Q)$ 表示将较短基序在较长基序内对齐时的重叠长度。对于由长度允许的每个偏移量 $s$，定义重叠位置的集合 $\\{0,\\dots,S-1\\}$，并计算相应平滑行之间的逐位置对称 KLD。使用以下两种方案之一跨位置进行聚合：\n    1. 均匀平均：对 $j \\in \\{0,\\dots,S-1\\}$ 分配权重 $w_j = \\frac{1}{S}$，并计算 $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),$$ 其中 $j'$ 和 $k'$ 是在偏移量 $s$ 和选定的对齐方向下，$P$ 和 $Q$ 中的适当索引。\n    2. 信息含量加权：根据重叠区域中已知 PWM 的行来分配权重，$w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$，并约定如果 $\\sum I_{u'} = 0$，则使用均匀权重。计算 $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right).$$\n- 将基序恢复度量定义为所有有效对齐中的最小聚合对称 KLD：$$M = \\min_{s} D_{\\mathrm{agg}}(s).$$ 值越低表示恢复效果越好。以 nats 为单位报告结果。\n\n实现要求：\n- 完全按照所述实现该度量，包括滤波器到 PWM 的 softmax 转换、每行伪计数平滑、对称 KLD、两种聚合模式以及选择较短基序在较长基序上滑动的对齐方式。\n- 始终使用固定的核苷酸顺序 $\\{A, C, G, T\\}$。\n- 所有对数必须是自然对数；以 nats 为单位报告结果，格式为十进制浮点数。在最后一行将最终输出四舍五入到六位小数。\n\n测试套件：\n实现您的程序，为以下四个测试用例计算 $M$。不使用外部输入；测试用例嵌入在程序中。在所有情况下，字母表顺序均为 $\\{A, C, G, T\\}$。\n\n测试用例 1（理想情况，长度匹配，信息含量加权）：\n- 长度为 $L_P = 6$ 的已知 PWM $P^{(1)}$：\n$$\n\\begin{bmatrix}\n0.10  0.40  0.40  0.10 \\\\\n0.05  0.05  0.85  0.05 \\\\\n0.25  0.25  0.25  0.25 \\\\\n0.70  0.10  0.10  0.10 \\\\\n0.10  0.10  0.10  0.70 \\\\\n0.40  0.10  0.40  0.10\n\\end{bmatrix}\n$$\n- 长度为 $L_Q = 6$ 的滤波器权重 $W^{(1)}$，通过在 $\\log(P^{(1)}_{j,\\cdot})$ 上添加逐行扰动得到：\n第 1 行：$\\log(P^{(1)}_{1,\\cdot}) + [0.00, -0.10, +0.10, 0.00]$；第 2 行：$\\log(P^{(1)}_{2,\\cdot}) + [-0.05, +0.20, -0.15, 0.00]$；第 3 行：$\\log(P^{(1)}_{3,\\cdot}) + [+0.30, -0.10, -0.10, -0.10]$；第 4 行：$\\log(P^{(1)}_{4,\\cdot}) + [-0.20, +0.05, +0.05, +0.10]$；第 5 行：$\\log(P^{(1)}_{5,\\cdot}) + [0.00, 0.00, 0.00, 0.00]$；第 6 行：$\\log(P^{(1)}_{6,\\cdot}) + [+0.05, -0.05, +0.05, -0.05]$。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-4}$，聚合模式：信息含量加权。\n\n测试用例 2（边界情况，精确匹配，均匀平均）：\n- 长度为 $L_P = 4$ 的已知 PWM $P^{(2)}$：\n$$\n\\begin{bmatrix}\n0.30  0.20  0.40  0.10 \\\\\n0.10  0.10  0.70  0.10 \\\\\n0.25  0.25  0.25  0.25 \\\\\n0.20  0.50  0.20  0.10\n\\end{bmatrix}\n$$\n- 长度为 $L_Q = 4$ 的滤波器权重 $W^{(2)}$：对于每一行 $j$，设置 $W^{(2)}_{j,a} = \\log\\left(P^{(2)}_{j,a}\\right)$，以便通过 softmax 变换，从滤波器派生的 PWM 与已知 PWM 完全匹配。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-9}$，聚合模式：均匀平均。\n\n测试用例 3（长度不匹配，子基序对齐，信息含量加权）：\n- 长度为 $L_P = 8$ 的已知 PWM $P^{(3)}$：\n$$\n\\begin{bmatrix}\n0.25  0.25  0.25  0.25 \\\\\n0.80  0.05  0.05  0.10 \\\\\n0.05  0.85  0.05  0.05 \\\\\n0.05  0.05  0.85  0.05 \\\\\n0.10  0.05  0.05  0.80 \\\\\n0.70  0.10  0.10  0.10 \\\\\n0.25  0.25  0.25  0.25 \\\\\n0.25  0.25  0.25  0.25\n\\end{bmatrix}\n$$\n- 长度为 $L_Q = 5$ 的滤波器权重 $W^{(3)}$：对于对应于 $P^{(3)}$ 位置 2 到 6 的行，设置 $W^{(3)}_{j,a} = \\log\\left(P^{(3)}_{j+1,a}\\right) + \\delta_{j,a}$，其中 $\\delta_{j,a}$ 是小的扰动，例如，对每行 $j$ 使用 $[+0.01, -0.01, 0.00, 0.00]$。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-5}$，聚合模式：信息含量加权。\n\n测试用例 4（含零的边缘情况，均匀平均）：\n- 长度为 $L_P = 4$ 且包含确定性行的已知 PWM $P^{(4)}$：\n$$\n\\begin{bmatrix}\n1.00  0.00  0.00  0.00 \\\\\n0.00  1.00  0.00  0.00 \\\\\n0.00  0.00  1.00  0.00 \\\\\n0.00  0.00  0.00  1.00 \\\\\n\\end{bmatrix}\n$$\n- 长度为 $L_Q = 4$ 的滤波器权重 $W^{(4)}$，通过较大的 logit 值偏好不同的核苷酸：\n第 1 行：$[-2.0, -2.0, -2.0, +2.0]$；第 2 行：$[+2.0, -2.0, -2.0, -2.0]$；第 3 行：$[-2.0, +2.0, -2.0, -2.0]$；第 4 行：$[-2.0, -2.0, +2.0, -2.0]$。\n- 温度 $\\tau = 1.0$，伪计数 $\\alpha = 10^{-3}$，聚合模式：均匀平均。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个结果 $M^{(1)}, M^{(2)}, M^{(3)}, M^{(4)}$，以逗号分隔并用方括号括起，四舍五入到六位小数，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$。单位是 nats（自然对数底）。不应打印任何其他文本。",
            "solution": "任务是为卷积神经网络 (CNN) 滤波器派生的基序构建一个定量的恢复度量标准，用于与已知的位置权重矩阵 (PWM) 进行比较，该度量标准根植于概率分布和信息论的原理，然后用代码实现并附带一个小型测试套件。推导过程从以下基本要素开始。\n\n第一性原理和核心定义：\n1. 一个位置权重矩阵 (PWM) $P$ 由多行 $P_{j,\\cdot}$ 构成，这些行是关于核苷酸 $\\{A,C,G,T\\}$ 的有效离散概率分布，即对于每个位置 $j$，有 $\\sum_{a} P_{j,a} = 1$ 且 $P_{j,a} \\ge 0$。\n2. 一个 CNN 滤波器 $W$ 是一个实值矩阵；为将其与 PWM 比较，必须将其每一行映射为一个有效的分布。softmax 函数是将实值 logit 转换为概率的规范方法。给定温度参数 $\\tau > 0$，派生出的 PWM $Q$ 为\n$$\nQ_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.\n$$\n这遵循了概率公理：对于每个 $j$，有 $Q_{j,a} \\ge 0$ 和 $\\sum_{a} Q_{j,a} = 1$。\n\n3. 为确保数值稳定性并避免当概率分量为零时出现未定义的对数，应用由标量 $\\alpha > 0$ 参数化的逐行 Dirichlet 风格伪计数平滑。对于任何行分布 $R_{j,\\cdot}$，其中 $\\sum_{a} R_{j,a} = 1$，我们定义\n$$\n\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha},\n$$\n这保持了归一化并强制所有分量严格为正 $\\widehat{R}_{j,a} > 0$。\n\n4. 在相同支撑集上，分布 $p$ 和 $q$ 之间的 Kullback–Leibler 散度（使用自然对数）为\n$$\nD_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right).\n$$\n其对称对应项定义为\n$$\nD_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).\n$$\n此对称版本非负，当且仅当 $p = q$ 时为零，并对双向的差异进行惩罚。\n\n5. 基序位置上的信息含量量化了与均匀分布的偏差。使用自然对数，令\n$$\nH(p) = -\\sum_{a} p_a \\log p_a, \\quad I_j = \\log(4) - H(P_{j,\\cdot}).\n$$\n这里，$\\log(4)$ 是四个等概率核苷酸上的最大熵，因此 $I_j \\in [0, \\log(4)]$。\n\n跨位置聚合与对齐：\n一个长度为 $L_P$ 的基序和一个长度为 $L_Q$ 的滤波器派生 PWM 的长度可能不同。为了进行比较，将较短的基序在较长的基序上滑动，并在每个有效对齐位置计算聚合散度。设较短基序的长度为 $S$，较长基序的长度为 $L_{\\mathrm{long}}$。对于每个偏移量 $s \\in \\{0, 1, \\dots, L_{\\mathrm{long}} - S\\}$，通过重叠区域内的直接索引对应关系，在较短和较长 PWM 的行之间定义一个映射。对于重叠的行，计算\n$$\nD_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),\n$$\n其中 $(j',k')$ 表示已知 PWM 和滤波器派生 PWM 中的对齐行，权重 $w_j$ 遵循以下两种方案之一：\n- 均匀平均：$w_j = \\frac{1}{S}$，反映所有位置同等重要。\n- 信息含量加权：$w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$，其中 $I_{j'}$ 是根据重叠区域中已知 PWM 的行计算的。如果 $\\sum I_{u'} = 0$（例如，所有行都是均匀分布），则使用 $w_j = \\frac{1}{S}$。\n\n恢复度量是所有有效偏移量中的最小聚合散度：\n$$\nM = \\min_s D_{\\mathrm{agg}}(s).\n$$\n一个较低的 $M$ 值表示在最佳对齐位置，滤波器派生的 PWM 与已知 PWM 更接近，这正是我们期望当滤波器捕获到基序时出现的情况。\n\n算法设计：\n1. 输入准备：构建已知 PWM 矩阵 $P$ 和滤波器权重矩阵 $W$，以及标量参数 $\\tau$（softmax 温度）、$\\alpha$（伪计数）和聚合模式选择（均匀或信息含量加权）。\n2. Softmax 转换：对于 $W$ 中的每一行 $j$，使用温度为 $\\tau$ 的 softmax 计算 $Q_{j,\\cdot}$，通过在指数化之前减去行最大值，以数值稳定的方式实现，以避免溢出。这将产生一个合法的 PWM $Q$。\n3. 平滑处理：对 $P$ 和 $Q$，使用 $\\alpha$ 独立地对每一行应用伪计数平滑，得到 $\\widehat{P}$ 和 $\\widehat{Q}$。\n4. 对齐处理：在 $\\widehat{P}$ 和 $\\widehat{Q}$ 中识别出较长和较短的一个。将较短的在较长的上滑动所有有效偏移量 $s$，对于每个对齐，通过以下方式计算 $D_{\\mathrm{agg}}(s)$：\n   - 逐位置对称 KLD：在每个重叠索引处计算 $D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right)$。\n   - 权重 $w_j$：可以是均匀的 $\\frac{1}{S}$，或基于重叠区域内已知 PWM 行的信息含量 $I_{j'}$。在计算 $I_{j'}$ 时，使用平滑后的已知 PWM 以确保严格为正的条目（这可以防止未定义的对数并保持度量的稳定性）。\n5. 最小化：选择 $M = \\min_s D_{\\mathrm{agg}}(s)$作为该情况的度量。\n6. 输出：生成一行包含所有测试用例度量的结果，四舍五入到六位小数，并以 nats 表示。\n\n选择的理由：\n- 从 CNN 滤波器 logit 到逐位置分布的 softmax 转换是机器学习中的标准做法，并确保了与 PWM 语义的兼容性。\n- 使用 $\\alpha$ 进行平滑对于数值稳定性是必要的，因为 $D_{\\mathrm{KL}}$ 要求严格为正的支撑集。选择一个小的 $\\alpha$ 可以保持原始分布占主导地位，同时避免未定义的对数。\n- 对称 KLD 对称地惩罚不匹配，适用于比较两个分布而不偏向于某个散度方向。\n- 信息含量加权通过强调具有更高特异性（更低熵）的位置来反映生物学和统计学上的相关性。当不打算或不需要这种加权时，均匀平均可作为基线。\n- 滑动对齐解决了滤波器捕获子基序或训练时使用的感受野大小与基序长度不同的现实情况。\n\n测试套件覆盖范围：\n- 测试用例 1 检验了一般情况，包含中等扰动和信息含量加权。\n- 测试用例 2 检查了精确相等的边界情况，期望结果接近于零。\n- 测试用例 3 测试了长度不匹配和对齐敏感性，包含高信息量子基序。\n- 测试用例 4 检验了含有零的确定性行，确认平滑处理能够得到有限的散度值和稳定的计算。\n\n该实现严格遵守使用自然对数；因此，所有报告的值都以 nats 为单位。最终输出是一个用方括号括起来的逗号分隔列表，包含四个四舍五入到六位小数的十进制浮点数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Compute row-wise softmax with temperature tau in a numerically stable way.\n    \"\"\"\n    # Subtract max per row for numerical stability.\n    z = logits / tau\n    z = z - np.max(z, axis=1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\ndef smooth_pwm(pwm: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Apply Dirichlet-style pseudocount smoothing per row:\n    (p + alpha) / (1 + 4*alpha), assuming each row sums to 1.\n    \"\"\"\n    # Each row normalization denominator is 1.0 + 4.0 * alpha because sum(row) == 1.\n    denom = 1.0 + 4.0 * alpha\n    return (pwm + alpha) / denom\n\ndef symmetric_kl(p: np.ndarray, q: np.ndarray) -> float:\n    \"\"\"\n    Compute symmetric KL divergence between two distributions p and q\n    using natural logarithms. Assumes p,q > 0 and sum to 1.\n    \"\"\"\n    # Small epsilon guard (should not be necessary after smoothing,\n    # but we include minimal safeguard).\n    eps = 1e-300\n    p = np.clip(p, eps, 1.0)\n    q = np.clip(q, eps, 1.0)\n    d1 = np.sum(p * np.log(p / q))\n    d2 = np.sum(q * np.log(q / p))\n    return float(d1 + d2)\n\ndef information_content_rows(pwm: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute information content per row: I_j = log(4) - H(p_j),\n    where H(p) = -sum p log p with natural logarithms.\n    \"\"\"\n    # Ensure strictly positive entries to avoid log(0)\n    eps = 1e-300\n    p = np.clip(pwm, eps, 1.0)\n    entropy = -np.sum(p * np.log(p), axis=1)  # nats\n    I = np.log(4.0) - entropy\n    return I\n\ndef aggregate_divergence(P_hat: np.ndarray, Q_hat: np.ndarray,\n                         mode: str, P_for_weights: np.ndarray) -> float:\n    \"\"\"\n    Align shorter across longer and compute minimum aggregated symmetric KL divergence.\n    mode: 'uniform' or 'ic' (information-content weighting using P_for_weights).\n    P_for_weights: PWM to compute information content for weights (use smoothed known PWM).\n    \"\"\"\n    len_P = P_hat.shape[0]\n    len_Q = Q_hat.shape[0]\n    # Determine which to slide: slide the shorter across the longer\n    if len_P <= len_Q:\n        shorter = P_hat\n        longer = Q_hat\n        weights_base = P_for_weights\n        slide_P_over_Q = True\n    else:\n        shorter = Q_hat\n        longer = P_hat\n        # For weights, we should use known PWM rows; if we are sliding Q over P,\n        # weights_base should be P_for_weights (known PWM).\n        weights_base = P_for_weights\n        slide_P_over_Q = False\n\n    S = shorter.shape[0]\n    L = longer.shape[0]\n    best = np.inf\n    # Precompute info content of weights_base rows as needed\n    info_weights = information_content_rows(weights_base)\n    for s in range(L - S + 1):\n        # Build weights for this overlapped region\n        if mode == 'uniform':\n            w = np.ones(S, dtype=float) / float(S)\n        elif mode == 'ic':\n            # Use information content from the known PWM rows in the overlap\n            if slide_P_over_Q:\n                # Overlap indices in P: 0..S-1\n                I = info_weights[:S]\n            else:\n                # Sliding Q over P: overlap indices in P are s..s+S-1\n                I = info_weights[s:s+S]\n            total_I = np.sum(I)\n            if total_I == 0.0:\n                w = np.ones(S, dtype=float) / float(S)\n            else:\n                w = I / total_I\n        else:\n            raise ValueError(\"Unknown aggregation mode\")\n\n        # Compute weighted symmetric KL over the overlap at offset s\n        total = 0.0\n        for j in range(S):\n            if slide_P_over_Q:\n                p_row = shorter[j]\n                q_row = longer[s + j]\n            else:\n                p_row = longer[s + j]\n                q_row = shorter[j]\n            d_sym = symmetric_kl(p_row, q_row)\n            total += w[j] * d_sym\n        if total < best:\n            best = total\n    return float(best)\n\ndef motif_recovery_metric(P_known: np.ndarray, W_filter: np.ndarray,\n                          tau: float, alpha: float, agg_mode: str) -> float:\n    \"\"\"\n    Compute the motif recovery metric M between known PWM P_known and filter weights W_filter.\n    Steps:\n    - Convert filter weights to PWM via softmax with temperature tau.\n    - Smooth both P and Q with pseudocount alpha.\n    - Align shorter across longer; compute aggregated symmetric KL at each offset.\n    - Return minimum aggregated divergence across offsets.\n    \"\"\"\n    Q_filter = softmax_rows(W_filter, tau)\n    # Smooth both PWMs\n    P_hat = smooth_pwm(P_known, alpha)\n    Q_hat = smooth_pwm(Q_filter, alpha)\n    # For information-content weighting, use smoothed known PWM\n    M = aggregate_divergence(P_hat, Q_hat, agg_mode, P_hat)\n    return M\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    P1 = np.array([\n        [0.10, 0.40, 0.40, 0.10],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.10, 0.10, 0.10, 0.70],\n        [0.40, 0.10, 0.40, 0.10],\n    ], dtype=float)\n    # Build W1 as log(P1) plus perturbations per row\n    logP1 = np.log(P1)\n    perturbations1 = np.array([\n        [0.00, -0.10, +0.10, 0.00],\n        [-0.05, +0.20, -0.15, 0.00],\n        [+0.30, -0.10, -0.10, -0.10],\n        [-0.20, +0.05, +0.05, +0.10],\n        [0.00, 0.00, 0.00, 0.00],\n        [+0.05, -0.05, +0.05, -0.05],\n    ], dtype=float)\n    W1 = logP1 + perturbations1\n    tau1 = 1.0\n    alpha1 = 1e-4\n    agg1 = 'ic'\n\n    # Test case 2\n    P2 = np.array([\n        [0.30, 0.20, 0.40, 0.10],\n        [0.10, 0.10, 0.70, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.20, 0.50, 0.20, 0.10],\n    ], dtype=float)\n    W2 = np.log(P2)  # exact match under softmax\n    tau2 = 1.0\n    alpha2 = 1e-9\n    agg2 = 'uniform'\n\n    # Test case 3\n    P3 = np.array([\n        [0.25, 0.25, 0.25, 0.25],\n        [0.80, 0.05, 0.05, 0.10],\n        [0.05, 0.85, 0.05, 0.05],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.10, 0.05, 0.05, 0.80],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.25, 0.25, 0.25, 0.25],\n    ], dtype=float)\n    # Filter corresponds to positions 2..6 of P3 with slight perturbations\n    logP3 = np.log(P3)\n    W3_core = logP3[1:6, :]  # rows 2..6\n    perturbations3 = np.array([\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n    ], dtype=float)\n    W3 = W3_core + perturbations3\n    tau3 = 1.0\n    alpha3 = 1e-5\n    agg3 = 'ic'\n\n    # Test case 4\n    P4 = np.array([\n        [1.00, 0.00, 0.00, 0.00],\n        [0.00, 1.00, 0.00, 0.00],\n        [0.00, 0.00, 1.00, 0.00],\n        [0.00, 0.00, 0.00, 1.00],\n    ], dtype=float)\n    W4 = np.array([\n        [-2.0, -2.0, -2.0, +2.0],  # favors T\n        [+2.0, -2.0, -2.0, -2.0],  # favors A\n        [-2.0, +2.0, -2.0, -2.0],  # favors C\n        [-2.0, -2.0, +2.0, -2.0],  # favors G\n    ], dtype=float)\n    tau4 = 1.0\n    alpha4 = 1e-3\n    agg4 = 'uniform'\n\n    test_cases = [\n        (P1, W1, tau1, alpha1, agg1),\n        (P2, W2, tau2, alpha2, agg2),\n        (P3, W3, tau3, alpha3, agg3),\n        (P4, W4, tau4, alpha4, agg4),\n    ]\n\n    results = []\n    for P_known, W_filter, tau, alpha, agg_mode in test_cases:\n        M = motif_recovery_metric(P_known, W_filter, tau, alpha, agg_mode)\n        # Round to 6 decimal places\n        results.append(f\"{M:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}