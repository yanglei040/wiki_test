{
    "hands_on_practices": [
        {
            "introduction": "To understand how Convolutional Neural Networks (CNNs) identify patterns in biological sequences, we first connect them to classical bioinformatics methods. This foundational exercise guides you through implementing a Position Weight Matrix (PWM) scan from first principles, a cornerstone of motif discovery. By framing the log-odds scoring as a convolution, you will build a direct, mathematical bridge between traditional motif scanning and the core operation of a CNN filter, revealing the latter as a powerful and generalizable evolution of the former .",
            "id": "3297925",
            "problem": "You are given the task of implementing a rigorous Position Weight Matrix (PWM) scan and demonstrating its equivalence to a single-channel, multi-filter operation of a Convolutional Neural Network (CNN) in the context of sequence motif discovery. The scientific base should begin from probability model definitions and log-likelihood ratios. A Position Weight Matrix (PWM) represents a motif of length $L$ by a set of position-specific multinomial distributions over Deoxyribonucleic Acid (DNA) nucleotides. A background model gives independent nucleotide probabilities. You must compute windowed log-odds scores across a DNA sequence and identify all start indices where the score strictly exceeds a given threshold.\n\nFundamental base:\n- Let the motif be represented by a PWM with entries $p_{i,b}$ where $i \\in \\{0,1,\\dots,L-1\\}$ indexes position and $b \\in \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ indexes nucleotides. Let the background distribution be $q_b$ with $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ and each $q_b > 0$.\n- Under the independent positions assumption, the motif model assigns to a length-$L$ subsequence $x_{t:t+L-1}$ the probability $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$, while the background model assigns $\\prod_{i=0}^{L-1} q_{x_{t+i}}$. The log-odds (log-likelihood ratio) score is defined as\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\n- Define the log-odds weight matrix $W$ by $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$ using the natural logarithm. Encode the DNA sequence into a one-hot matrix $X \\in \\{0,1\\}^{T \\times 4}$ where $T$ is the sequence length and channels correspond in order to $\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}$. For any unknown nucleotide symbol, such as $N$, encode as the zero vector $(0,0,0,0)$ so it contributes nothing to the log-odds sum. Then the score can be written as\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}} W_{i,b} \\, X_{t+i,b},\n$$\nwhich is exactly a valid-mode multi-channel convolution used in a Convolutional Neural Network (CNN) where the filter $W$ spans $L$ positions and $4$ channels.\n\nProgram requirements:\n- Implement a function that, given a DNA sequence, a PWM specified by its per-position nucleotide probabilities $p_{i,b}$, a background distribution $q_b$, and a real-valued threshold $\\tau$, computes $S(t)$ for all valid start positions $t \\in \\{0,1,\\dots,T-L\\}$ and returns the list of indices $t$ such that $S(t) > \\tau$.\n- Use the natural logarithm for all log-odds computations. Handle unknown nucleotides $N$ by contributing $0$ at those positions as specified above.\n- Follow strictly the \"exceeding\" criterion: include positions only if $S(t)$ is strictly greater than $\\tau$.\n- Operate only in the forward orientation; do not consider reverse complements.\n- All mathematical quantities must be treated exactly as defined above.\n\nTest suite:\nYou must hard-code and evaluate the following five test cases. For each case, report the list of start indices where the score strictly exceeds the specified threshold.\n\n- Case $1$ (happy path, uniform background, exact motif present repeatedly):\n    - Sequence: `ACGTACGTACGT`.\n    - PWM length $L = 4$ with probabilities at each position (order $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$):\n        - Position $0$: $(0.7, 0.1, 0.1, 0.1)$.\n        - Position $1$: $(0.1, 0.7, 0.1, 0.1)$.\n        - Position $2$: $(0.1, 0.1, 0.7, 0.1)$.\n        - Position $3$: $(0.1, 0.1, 0.1, 0.7)$.\n    - Background $q = (0.25, 0.25, 0.25, 0.25)$.\n    - Threshold $\\tau = 3.5$.\n\n- Case $2$ (boundary where no window exceeds a high threshold):\n    - Sequence: `AAAAACCCCC`.\n    - PWM and background identical to Case $1$.\n    - Threshold $\\tau = 4.0$.\n\n- Case $3$ (unknown symbols $N$ reduce contributions, but an exact match appears once):\n    - Sequence: `NNNACGTNNN`.\n    - PWM and background identical to Case $1$.\n    - Threshold $\\tau = 3.5$.\n\n- Case $4$ (non-uniform background, different PWM length and composition):\n    - Sequence: `ACGACGTTAC`.\n    - PWM length $L = 3$ with probabilities (order $\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}$):\n        - Position $0$: $(0.6, 0.2, 0.1, 0.1)$.\n        - Position $1$: $(0.1, 0.6, 0.2, 0.1)$.\n        - Position $2$: $(0.1, 0.2, 0.6, 0.1)$.\n    - Background $q = (0.1, 0.4, 0.4, 0.1)$.\n    - Threshold $\\tau = 2.4$.\n\n- Case $5$ (exact equality to threshold should not be included):\n    - Sequence: `ACGT`.\n    - PWM and background identical to Case $1$.\n    - Threshold $\\tau = 4.118477668724633$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of integers of positions that strictly exceed the threshold, in the order of the test suite above. For example, an output might look like `[[0, 4, 8], [], [3], [0, 3], []]`.",
            "solution": "We begin from the independent positions model for motifs and the independent and identically distributed background. Let a length-$L$ motif be described by a Position Weight Matrix (PWM) with entries $p_{i,b}$, where $i \\in \\{0,\\dots,L-1\\}$ indexes motif positions and $b \\in \\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$ indexes nucleotides. Let the background model be parameterized by $q_b$ such that $q_{\\mathrm{A}} + q_{\\mathrm{C}} + q_{\\mathrm{G}} + q_{\\mathrm{T}} = 1$ with each $q_b > 0$. For a DNA sequence $x_0 x_1 \\dots x_{T-1}$, consider any window start index $t$ with $0 \\le t \\le T-L$. The motif model assigns probability $\\prod_{i=0}^{L-1} p_{i, x_{t+i}}$ to the subsequence $x_{t:t+L-1}$, while the background model assigns $\\prod_{i=0}^{L-1} q_{x_{t+i}}$. The canonical scanning score is the natural log-likelihood ratio\n$$\nS(t) = \\log \\frac{\\prod_{i=0}^{L-1} p_{i, x_{t+i}}}{\\prod_{i=0}^{L-1} q_{x_{t+i}}}\n= \\sum_{i=0}^{L-1} \\log \\frac{p_{i, x_{t+i}}}{q_{x_{t+i}}}.\n$$\nDefining the log-odds weight matrix $W$ by $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$, we have\n$$\nS(t) = \\sum_{i=0}^{L-1} W_{i, x_{t+i}}.\n$$\nIntroduce one-hot encoding $X \\in \\{0,1\\}^{T \\times 4}$ with channel order $(\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T})$, i.e., $X_{j,b} = 1$ if $x_j=b$ and $0$ otherwise; for unknown symbols (e.g., $N$), set $X_{j,b}=0$ for all $b$. Then, for any start $t$, the score can be written as\n$$\nS(t) = \\sum_{i=0}^{L-1} \\sum_{b} W_{i,b} \\, X_{t+i,b},\n$$\nwhich is identical to a valid-mode convolution across the sequence length dimension with a filter $W$ spanning $L$ positions and $4$ channels, as used in Convolutional Neural Networks (CNN) for sequence motif discovery. In CNN terms, $S(t)$ is the activation of a single filter applied to the one-hot-encoded input at location $t$.\n\nAlgorithmic design:\n- Compute $W$ via $W_{i,b} = \\log\\left(\\frac{p_{i,b}}{q_b}\\right)$ using the natural logarithm, guaranteeing finite values since each $p_{i,b}$ and $q_b$ are strictly positive in the provided test suite.\n- One-hot encode the sequence, mapping $\\mathrm{A} \\mapsto (1,0,0,0)$, $\\mathrm{C} \\mapsto (0,1,0,0)$, $\\mathrm{G} \\mapsto (0,0,1,0)$, $\\mathrm{T} \\mapsto (0,0,0,1)$, and $N \\mapsto (0,0,0,0)$.\n- For each valid start index $t$, compute $S(t)$ as the tensor contraction $S(t) = \\sum_{i,b} W_{i,b} X_{t+i,b}$, and collect those $t$ for which $S(t) > \\tau$.\n- Do this for each of the five test cases and aggregate the results in order.\n\nAnalytical expectations for the provided test suite:\n- Case $1$: The motif is highly specific for the pattern `ACGT` across positions. With uniform background $q_b = 0.25$, the per-position match log-odds weight is $\\log(0.7/0.25) = \\log(2.8) \\approx 1.029619417$ and the mismatch weight is $\\log(0.1/0.25) = \\log(0.4) \\approx -0.916290732$. A perfect match subsequence `ACGT` yields total score $4 \\times \\log(2.8) \\approx 4.118477669$, which strictly exceeds $\\tau = 3.5$. In the sequence `ACGTACGTACGT`, perfect matches occur at positions $0$, $4$, and $8$. All other $4$-length windows are rotations that induce all mismatches, yielding negative scores, so the result is `[0,4,8]`.\n- Case $2$: With the same PWM and background, the sequence `AAAAACCCCC` contains no `ACGT` window. Even the best windows cannot exceed $\\tau = 4.0$. For example, `AAAA` yields $S \\approx 1.029619417 + 3 \\times (-0.916290732) \\approx -1.719252779$. Thus, no positions exceed the threshold and the result is `[]`.\n- Case $3$: Unknowns $N$ contribute zero as per the specified encoding and summation rule. The sequence `NNNACGTNNN` contains a perfect match `ACGT` starting at position $3$ with $S \\approx 4.118477669 > 3.5$, while all windows overlapping $N$ without the exact motif accumulate insufficient or negative scores. Therefore, the result is `[3]`.\n- Case $4$: Non-uniform background $q = (0.1, 0.4, 0.4, 0.1)$ and PWM length $L=3$ specific to `ACG`. The perfect match score is $\\log(0.6/0.1) + \\log(0.6/0.4) + \\log(0.6/0.4) = \\log(6) + 2 \\log(1.5) \\approx 2.602689685$, which exceeds $\\tau = 2.4$. In the sequence `ACGACGTTAC`, perfect matches occur at positions $0$ and $3$, and other windows are suboptimal with scores below the threshold. Hence the result is `[0,3]`.\n- Case $5$: The threshold is set as $\\tau = 4.118477668724633$, which is slightly larger than the exact perfect match score obtained from the PWM and uniform background. For the sequence `ACGT`, the only window is the entire sequence, which achieves a score of approximately $4.118477668724632$ and does not strictly exceed the threshold due to the strict inequality. Therefore, the result is `[]`.\n\nImplementation notes:\n- The computation uses the natural logarithm consistently.\n- The program produces the single-line output aggregating all five cases in order as `[[0, 4, 8], [], [3], [0, 3], []]`.\n- The approach demonstrates the connection between statistical PWM scanning and the valid-mode multi-channel convolution central to Convolutional Neural Networks (CNN) used in computational systems biology for motif discovery.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nNUC_ORDER = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\ndef one_hot_encode(seq: str) -> np.ndarray:\n    \"\"\"\n    Encode DNA sequence into one-hot with channels in order A, C, G, T.\n    Unknowns (e.g., 'N') are encoded as all zeros.\n    \"\"\"\n    X = np.zeros((len(seq), 4), dtype=float)\n    for i, ch in enumerate(seq.upper()):\n        idx = NUC_ORDER.get(ch, None)\n        if idx is not None:\n            X[i, idx] = 1.0\n        # else: unknown contributes 0 vector\n    return X\n\ndef log_odds_weights(pwm_probs: np.ndarray, background: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute log-odds weight matrix W_{i,b} = ln(p_{i,b} / q_b).\n    pwm_probs: shape (L, 4)\n    background: shape (4,)\n    \"\"\"\n    # Ensure arrays are float for division and log\n    pwm = np.array(pwm_probs, dtype=float)\n    q = np.array(background, dtype=float)\n    # Avoid division by zero: test suite guarantees q_b > 0 and p_{i,b} > 0\n    W = np.log(pwm / q[None, :])\n    return W\n\ndef pwm_scan_positions(seq: str, pwm_probs: np.ndarray, background: np.ndarray, threshold: float) -> list:\n    \"\"\"\n    Perform PWM scanning: compute windowed log-odds scores and return positions exceeding threshold.\n    Uses natural logarithm and strict inequality.\n    \"\"\"\n    X = one_hot_encode(seq)\n    W = log_odds_weights(pwm_probs, background)\n    L = W.shape[0]\n    T = X.shape[0]\n    results = []\n    # Slide window and compute score S(t) = sum_{i,b} W[i,b] * X[t+i,b]\n    # Equivalent to valid-mode multi-channel convolution\n    for t in range(T - L + 1):\n        window = X[t:t+L, :]  # shape (L, 4)\n        score = float(np.sum(window * W))\n        if score > threshold:\n            results.append(t)\n    return results\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case 1\n    seq1 = \"ACGTACGTACGT\"\n    pwm1 = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # position 0\n        [0.1, 0.7, 0.1, 0.1],  # position 1\n        [0.1, 0.1, 0.7, 0.1],  # position 2\n        [0.1, 0.1, 0.1, 0.7],  # position 3\n    ], dtype=float)\n    bg_uniform = np.array([0.25, 0.25, 0.25, 0.25], dtype=float)\n    tau1 = 3.5\n\n    # Case 2\n    seq2 = \"AAAAACCCCC\"\n    tau2 = 4.0\n\n    # Case 3\n    seq3 = \"NNNACGTNNN\"\n    tau3 = 3.5\n\n    # Case 4\n    seq4 = \"ACGACGTTAC\"\n    pwm4 = np.array([\n        [0.6, 0.2, 0.1, 0.1],  # position 0\n        [0.1, 0.6, 0.2, 0.1],  # position 1\n        [0.1, 0.2, 0.6, 0.1],  # position 2\n    ], dtype=float)\n    bg_nonuniform = np.array([0.1, 0.4, 0.4, 0.1], dtype=float)\n    tau4 = 2.4\n\n    # Case 5\n    seq5 = \"ACGT\"\n    tau5 = 4.118477668724633  # Slightly above the perfect match sum for PWM1 under uniform bg\n\n    test_cases = [\n        (seq1, pwm1, bg_uniform, tau1),\n        (seq2, pwm1, bg_uniform, tau2),\n        (seq3, pwm1, bg_uniform, tau3),\n        (seq4, pwm4, bg_nonuniform, tau4),\n        (seq5, pwm1, bg_uniform, tau5),\n    ]\n\n    results = []\n    for seq, pwm, bg, tau in test_cases:\n        positions = pwm_scan_positions(seq, pwm, bg, tau)\n        results.append(positions)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line: list of lists of integers\n    print(f\"[{','.join([str(lst) for lst in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After a convolutional layer identifies potential motif hits, a pooling layer is typically used to summarize the activations and provide invariance to the exact position of the motif. This practice moves beyond a simple definition of pooling to a deeper, quantitative comparison of how different strategies—max, average, and stochastic—affect detection performance. By modeling weak, repeated motif signals within a noisy background, you will analyze the specific strengths and weaknesses of each pooling method, a critical consideration when designing networks for complex biological scenarios .",
            "id": "3297848",
            "problem": "Consider a one-dimensional Convolutional Neural Network (CNN) processing a DNA sequence to detect a specific short sequence motif using a single convolutional filter. Assume the following generative model for the filter's pre-activation at positions inside a pooling window: each position $i \\in \\{1,\\dots,n\\}$ produces a scalar pre-activation $X_i = s_i \\mu + \\varepsilon_i$, where $s_i \\in \\{0,1\\}$ indicates the presence ($1$) or absence ($0$) of a motif instance at position $i$, the signal amplitude per motif instance is $\\mu > 0$, and the noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ is independent across positions with variance $\\sigma^2$. The network applies a Rectified Linear Unit (ReLU) nonlinearity to obtain $A_i = \\max(0,X_i)$ before pooling. In a pooling window of size $n$, suppose there are exactly $m$ positions with $s_i = 1$ (motif-present) and $n - m$ positions with $s_i = 0$ (motif-absent). A detection is deemed successful if the pooled output exceeds a fixed, nonnegative threshold $\\tau \\ge 0$.\n\nDefine three pooling choices:\n- Max pooling: $Y_{\\text{max}} = \\max_{1 \\le i \\le n} A_i$.\n- Average pooling: $Y_{\\text{avg}} = \\frac{1}{n} \\sum_{i=1}^n A_i$.\n- Stochastic pooling: choose an index $I$ with probability $p_i = \\frac{A_i}{\\sum_{j=1}^n A_j}$ and set $Y_{\\text{stoch}} = A_I$.\n\nUnder the independence and Gaussian noise assumptions described above, derive, from first principles, expressions for the detection probabilities $\\mathbb{P}(Y_{\\text{pool}} \\ge \\tau)$ for each pooling method, expressed in terms of the parameters $(n,m,\\mu,\\sigma^2,\\tau)$, using only standard functions of the standard normal distribution. Your derivation must start from the definition of the distributions and independence assumptions and use well-tested facts about Gaussian random variables and truncation by the Rectified Linear Unit (ReLU). For stochastic pooling, provide a large-$n$ asymptotic approximation that is scientifically justified by the Law of Large Numbers and yields a closed-form expression involving expectations of truncated Gaussian random variables.\n\nYour program must implement the derived formulas to compute the detection probability for each pooling method and evaluate the impact of pooling choice on detecting weak, repeated motif instances across a test suite. Express all probabilities as decimal floats rounded to six decimal places.\n\nUse the following test suite of parameter tuples $(n,m,\\mu,\\sigma^2,\\tau)$:\n- Case $1$: $(n,m,\\mu,\\sigma^2,\\tau) = (10,3,0.5,0.25,0.6)$.\n- Case $2$: $(n,m,\\mu,\\sigma^2,\\tau) = (50,20,0.2,1.0,0.3)$.\n- Case $3$: $(n,m,\\mu,\\sigma^2,\\tau) = (8,0,0.5,0.25,0.1)$.\n- Case $4$: $(n,m,\\mu,\\sigma^2,\\tau) = (15,5,0.3,0.09,1.0)$.\n- Case $5$: $(n,m,\\mu,\\sigma^2,\\tau) = (10,2,0.8,0.01,0.7)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three rounded floats $[p_{\\text{max}},p_{\\text{avg}},p_{\\text{stoch}}]$ in that order. For example, the output should be of the form `[[p_1,max,p_1,avg,p_1,stoch],[p_2,max,p_2,avg,p_2,stoch],...]`.",
            "solution": "The problem is well-posed and scientifically grounded, defining a simplified but coherent generative model for a Convolutional Neural Network's activations. We are tasked with deriving the detection probabilities $\\mathbb{P}(Y_{\\text{pool}} \\ge \\tau)$ for three different pooling operations: max, average, and stochastic. The derivation will proceed from first principles, based on the provided distributions and independence assumptions.\n\nLet the pre-activations be $X_i = s_i \\mu + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are i.i.d. Gaussian noise terms. The window of size $n$ contains $m$ positions where $s_i=1$ (motif) and $n-m$ positions where $s_i=0$ (noise). Thus, we have two populations of pre-activations:\n1. $m$ instances of $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n2. $n-m$ instances of $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^2)$.\nThe post-ReLU activations are given by $A_i = \\max(0, X_i)$. The $A_i$ are independent random variables, drawn from two distinct distributions corresponding to $A_{\\text{motif}} = \\max(0, X_{\\text{motif}})$ and $A_{\\text{noise}} = \\max(0, X_{\\text{noise}})$. Let $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ denote the CDF and PDF of the standard normal distribution $\\mathcal{N}(0,1)$, respectively. The detection threshold is $\\tau \\ge 0$.\n\nThe derivation for the detection probability for each pooling method is as follows.\n\nFor max pooling, the output is $Y_{\\text{max}} = \\max_{1 \\le i \\le n} A_i$. The detection probability is $\\mathbb{P}(Y_{\\text{max}} \\ge \\tau)$. It is more convenient to compute the probability of the complement event, non-detection, $\\mathbb{P}(Y_{\\text{max}} < \\tau) = \\mathbb{P}(A_1 < \\tau, A_2 < \\tau, \\dots, A_n < \\tau)$. Due to the independence of the $A_i$, this joint probability factors into a product of individual probabilities:\n$$ \\mathbb{P}(Y_{\\text{max}} < \\tau) = \\prod_{i=1}^n \\mathbb{P}(A_i < \\tau) = [\\mathbb{P}(A_{\\text{motif}} < \\tau)]^m [\\mathbb{P}(A_{\\text{noise}} < \\tau)]^{n-m} $$\nFor any activation $A_i = \\max(0, X_i)$ and a non-negative threshold $\\tau \\ge 0$, the condition $A_i < \\tau$ is equivalent to $X_i < \\tau$. We can now compute the probabilities for each population by standardizing the variables. For $X_{\\text{motif}} \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we have $\\mathbb{P}(A_{\\text{motif}} < \\tau) = \\mathbb{P}(X_{\\text{motif}} < \\tau) = \\Phi\\left(\\frac{\\tau - \\mu}{\\sigma}\\right)$. For $X_{\\text{noise}} \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\mathbb{P}(A_{\\text{noise}} < \\tau) = \\mathbb{P}(X_{\\text{noise}} < \\tau) = \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)$. The detection probability is therefore:\n$$ P_{\\text{max}} = \\mathbb{P}(Y_{\\text{max}} \\ge \\tau) = 1 - \\left[\\Phi\\left(\\frac{\\tau - \\mu}{\\sigma}\\right)\\right]^m \\left[\\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right]^{n-m} $$\n\nFor average pooling, the output is $Y_{\\text{avg}} = \\frac{1}{n} \\sum_{i=1}^n A_i$. The detection probability is $\\mathbb{P}(Y_{\\text{avg}} \\ge \\tau)$, which is equivalent to $\\mathbb{P}(\\sum_{i=1}^n A_i \\ge n\\tau)$. The sum $S_A = \\sum_{i=1}^n A_i$ is a sum of independent, but not identically distributed, rectified Gaussian random variables. We apply the Central Limit Theorem (CLT) to approximate the distribution of $S_A$ with a normal distribution, $S_A \\approx \\mathcal{N}(\\mathbb{E}[S_A], \\text{Var}(S_A))$. First, we must find the mean and variance of a generic rectified Gaussian variable $A = \\max(0, X)$, where $X \\sim \\mathcal{N}(\\nu, \\sigma^2)$. The mean is $\\mathbb{E}[A] = \\int_0^\\infty x \\frac{1}{\\sigma}\\phi(\\frac{x-\\nu}{\\sigma})dx = \\sigma\\phi(\\nu/\\sigma) + \\nu\\Phi(\\nu/\\sigma)$. The second moment is $\\mathbb{E}[A^2] = \\int_0^\\infty x^2 \\frac{1}{\\sigma}\\phi(\\frac{x-\\nu}{\\sigma})dx = (\\sigma^2+\\nu^2)\\Phi(\\nu/\\sigma) + \\sigma\\nu\\phi(\\nu/\\sigma)$. The variance is $\\text{Var}(A) = \\mathbb{E}[A^2] - (\\mathbb{E}[A])^2$. Applying these to the motif-present ($\\nu=\\mu$) and motif-absent ($\\nu=0$) populations, we can compute the mean $E_S = \\mathbb{E}[S_A] = m \\cdot \\mathbb{E}[A_{\\text{motif}}] + (n-m) \\cdot \\mathbb{E}[A_{\\text{noise}}]$ and variance $V_S = \\text{Var}(S_A) = m \\cdot \\text{Var}(A_{\\text{motif}}) + (n-m) \\cdot \\text{Var}(A_{\\text{noise}})$. With the CLT approximation, the detection probability is:\n$$ P_{\\text{avg}} = \\mathbb{P}(S_A \\ge n\\tau) = \\mathbb{P}\\left(\\frac{S_A - E_S}{\\sqrt{V_S}} \\ge \\frac{n\\tau - E_S}{\\sqrt{V_S}}\\right) = 1 - \\Phi\\left(\\frac{n\\tau - E_S}{\\sqrt{V_S}}\\right) $$\n\nFor stochastic pooling, the output $Y_{\\text{stoch}} = A_I$ has a detection probability $P_{\\text{stoch}} = \\mathbb{P}(Y_{\\text{stoch}} \\ge \\tau)$. By the law of total expectation, this is $P_{\\text{stoch}} = \\mathbb{E}\\left[\\frac{\\sum_i A_i \\mathbb{I}(A_i \\ge \\tau)}{\\sum_j A_j}\\right]$. This is an expectation of a ratio of random variables. We use a large-$n$ approximation justified by the Law of Large Numbers: $\\mathbb{E}[W/S_A] \\approx \\mathbb{E}[W]/\\mathbb{E}[S_A]$, where $W = \\sum_i A_i \\mathbb{I}(A_i \\ge \\tau)$ and $S_A = \\sum_j A_j$. The denominator $\\mathbb{E}[S_A]$ is the same $E_S$ calculated for average pooling. The numerator is $\\mathbb{E}[W] = m \\cdot \\mathbb{E}[A_{\\text{motif}} \\mathbb{I}(A_{\\text{motif}} \\ge \\tau)] + (n-m) \\cdot \\mathbb{E}[A_{\\text{noise}} \\mathbb{I}(A_{\\text{noise}} \\ge \\tau)]$. The expectation of a rectified Gaussian truncated from below at $\\tau \\ge 0$ is $\\mathbb{E}[A \\cdot \\mathbb{I}(A \\ge \\tau)] = \\int_\\tau^\\infty x \\frac{1}{\\sigma}\\phi(\\frac{x-\\nu}{\\sigma})dx = \\sigma\\phi(\\frac{\\tau-\\nu}{\\sigma}) + \\nu(1 - \\Phi(\\frac{\\tau-\\nu}{\\sigma}))$. We denote this as $E_{A,\\tau}(\\nu, \\sigma)$. The final expression for the approximation is:\n$$ P_{\\text{stoch}} \\approx \\frac{m \\cdot E_{A,\\tau}(\\mu, \\sigma) + (n-m) \\cdot E_{A,\\tau}(0, \\sigma)}{m \\cdot \\mathbb{E}[A_{\\text{motif}}] + (n-m) \\cdot \\mathbb{E}[A_{\\text{noise}}]} $$",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_probabilities(n, m, mu, sigma_sq, tau):\n    \"\"\"\n    Calculates the detection probabilities for max, average, and stochastic pooling.\n    \n    Args:\n        n (int): Pooling window size.\n        m (int): Number of motif-present positions.\n        mu (float): Signal amplitude per motif instance.\n        sigma_sq (float): Noise variance.\n        tau (float): Detection threshold.\n        \n    Returns:\n        list: A list containing [p_max, p_avg, p_stoch].\n    \"\"\"\n    if sigma_sq <= 0:\n        # Avoid division by zero and handle deterministic case, though not in test suite.\n        # This implementation assumes sigma_sq > 0 as per problem context.\n        sigma = 1e-9 \n    else:\n        sigma = np.sqrt(sigma_sq)\n\n    # Standard normal functions\n    phi = norm.pdf\n    Phi = norm.cdf\n\n    # --- Max Pooling ---\n    # P(Y_max >= tau) = 1 - P(Y_max < tau)\n    # P(Y_max < tau) = P(A_motif < tau)^m * P(A_noise < tau)^(n-m)\n    # P(A < tau) = P(max(0,X) < tau) = P(X < tau) for tau >= 0\n    p_non_detection_motif = Phi((tau - mu) / sigma) if m > 0 else 1.0\n    p_non_detection_noise = Phi(tau / sigma) if (n - m) > 0 else 1.0\n    \n    # Handle potential floating point errors with powers\n    p_max_nd = (p_non_detection_motif ** m) * (p_non_detection_noise ** (n - m))\n    p_max = 1.0 - p_max_nd\n\n    # --- Common calculations for Average and Stochastic Pooling ---\n\n    # Helper function for mean of rectified Gaussian N(nu, sigma^2)\n    def E_A(nu, sig):\n        z = nu / sig\n        return sig * phi(z) + nu * Phi(z)\n\n    # Helper function for variance of rectified Gaussian N(nu, sigma^2)\n    def Var_A(nu, sig):\n        z = nu / sig\n        mean_A = E_A(nu, sig)\n        E_A_sq = (sig**2 + nu**2) * Phi(z) + nu * sig * phi(z)\n        return E_A_sq - mean_A**2\n\n    # Moments for motif and noise populations\n    mean_A_motif = E_A(mu, sigma)\n    var_A_motif = Var_A(mu, sigma)\n    \n    mean_A_noise = E_A(0, sigma)\n    var_A_noise = Var_A(0, sigma)\n\n    # --- Average Pooling (CLT Approximation) ---\n    # S_A ~ N(E_S, V_S)\n    E_S = m * mean_A_motif + (n - m) * mean_A_noise\n    V_S = m * var_A_motif + (n - m) * var_A_noise\n\n    # Handle V_S=0 case\n    if V_S <= 0:\n        # If variance is zero, distribution is a point mass at E_S\n        p_avg = 1.0 if E_S >= n * tau else 0.0\n    else:\n        # P(S_A >= n*tau)\n        z_avg = (n * tau - E_S) / np.sqrt(V_S)\n        p_avg = 1.0 - Phi(z_avg)\n\n    # --- Stochastic Pooling (Large-n Approximation) ---\n    # P_stoch ~= E[W] / E[S_A] where W = sum(A_i * I(A_i >= tau))\n    \n    # Helper for E[A * I(A >= tau)] for A=max(0,N(nu,sigma^2))\n    def E_A_trunc_tau(nu, sig, t):\n        z = (t - nu) / sig\n        return sig * phi(z) + nu * (1 - Phi(z))\n\n    E_W_motif = E_A_trunc_tau(mu, sigma, tau)\n    E_W_noise = E_A_trunc_tau(0, sigma, tau)\n    \n    E_W = m * E_W_motif + (n - m) * E_W_noise\n    \n    # Denominator E_S is the same as for average pooling\n    if E_S > 0:\n        p_stoch = E_W / E_S\n    else:\n        # This case happens if all activations are almost surely zero\n        p_stoch = 0.0\n\n    # Ensure probabilities are in [0,1] range\n    p_max = np.clip(p_max, 0, 1)\n    p_avg = np.clip(p_avg, 0, 1)\n    p_stoch = np.clip(p_stoch, 0, 1)\n\n    return [p_max, p_avg, p_stoch]\n\n\ndef solve():\n    \"\"\"\n    Executes the analysis for the test suite and prints the final results.\n    \"\"\"\n    test_cases = [\n        # (n, m, mu, sigma^2, tau)\n        (10, 3, 0.5, 0.25, 0.6),\n        (50, 20, 0.2, 1.0, 0.3),\n        (8, 0, 0.5, 0.25, 0.1),\n        (15, 5, 0.3, 0.09, 1.0),\n        (10, 2, 0.8, 0.01, 0.7),\n    ]\n\n    results_list = []\n    for case in test_cases:\n        n, m, mu, sigma_sq, tau = case\n        probabilities = calculate_probabilities(n, m, mu, sigma_sq, tau)\n        results_list.append(probabilities)\n\n    # Format the output string as per problem specification\n    rounded_results = [[round(p, 6) for p in res] for res in results_list]\n    results_str_list = [str(r).replace(' ', '') for r in rounded_results]\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A trained deep learning model is often a \"black box,\" but its biological utility hinges on our ability to interpret what it has learned. This final practice addresses the crucial task of model interpretation by guiding you to design and implement a rigorous metric for evaluating if a CNN filter has successfully recovered a known biological motif. You will use the symmetric Kullback-Leibler divergence to compare a filter's learned weights to a ground-truth PWM, providing a quantitative method to validate the biological relevance of your network's features .",
            "id": "3297894",
            "problem": "You are given a known Position Weight Matrix (PWM) and a Convolutional Neural Network (CNN) filter represented by real-valued weights. The goal is to design, implement, and evaluate a quantitative motif recovery metric that uses a symmetric Kullback–Leibler divergence (KLD) between the known PWM and a PWM derived from the filter. The metric must be grounded in first principles of probability distributions over deoxyribonucleic acid (DNA) nucleotides and information theory and must include a principled aggregation across motif positions and alignment handling. The alphabet of nucleotides is $\\{A, C, G, T\\}$ and the PWM rows are ordered in that fixed alphabet.\n\nFundamental base and definitions to use:\n- A PWM $P \\in [0,1]^{L \\times 4}$ for a motif of length $L$ is a row-wise probability distribution over the nucleotides, where for each position $j \\in \\{1,\\dots,L\\}$ the row $P_{j,\\cdot}$ satisfies $\\sum_{a \\in \\{A,C,G,T\\}} P_{j,a} = 1$ and $P_{j,a} \\ge 0$.\n- A CNN filter is given by a real-valued matrix $W \\in \\mathbb{R}^{L_f \\times 4}$ of length $L_f$; from this, derive a probability distribution at each position via the softmax transformation with temperature $\\tau > 0$. For each position $j$ and nucleotide $a$, define $$Q_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.$$\n- To avoid undefined logarithms when any probability is zero, apply Dirichlet-style pseudocount smoothing with parameter $\\alpha > 0$ to each PWM row independently: for any row vector $R_{j,\\cdot}$ that sums to $1$, define the smoothed row $$\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{\\sum_{b \\in \\{A,C,G,T\\}} \\left(R_{j,b} + \\alpha\\right)} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha}.$$\n- The Kullback–Leibler divergence between two discrete distributions $p$ and $q$ over the same support is $$D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right),$$ with all logarithms in the natural base (nats). The symmetric Kullback–Leibler divergence is $$D_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).$$\n- The information content at position $j$ for a PWM row $p = P_{j,\\cdot}$ is defined using natural logarithms as $$I_j = \\log(4) - H(p), \\quad\\text{where}\\quad H(p) = -\\sum_{a} p_a \\log p_a.$$ This yields $I_j \\in [0, \\log(4)]$.\n\nAggregation across positions and alignment:\n- If the lengths of the known PWM and the filter-derived PWM differ, align the shorter PWM within the longer by sliding it across all possible offsets and evaluating the recovery metric at each alignment. Let $P$ have length $L_P$ and $Q$ have length $L_Q$, and let $S = \\min(L_P, L_Q)$ denote the overlap length when aligning the shorter motif within the longer motif. For each offset $s$ permitted by the lengths, define the set of overlapped positions $\\{0,\\dots,S-1\\}$, and compute the position-wise symmetric KLD between the corresponding smoothed rows. Aggregate across positions using one of two schemes:\n    1. Uniform averaging: assign weights $w_j = \\frac{1}{S}$ for $j \\in \\{0,\\dots,S-1\\}$ and compute $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),$$ where $j'$ and $k'$ are the appropriate indices within $P$ and $Q$ under offset $s$ and the chosen alignment direction.\n    2. Information-content weighting: assign weights based on the known PWM rows in the overlap, $w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$, with the convention that if $\\sum I_{u'} = 0$ then use uniform weights. Compute $$D_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right).$$\n- Define the motif recovery metric as the minimum aggregated symmetric KLD across all valid alignments: $$M = \\min_{s} D_{\\mathrm{agg}}(s).$$ Lower values indicate better recovery. Report the result in nats.\n\nImplementation requirements:\n- Implement the described metric exactly as stated, with the filter-to-PWM softmax, per-row pseudocount smoothing, symmetric KLD, two aggregation modes, and sliding alignment selecting the shorter motif to slide across the longer.\n- Use the fixed nucleotide order $\\{A, C, G, T\\}$ consistently.\n- All logarithms must be natural logarithms; report results in nats as decimal floats. Round final outputs to six decimal places in the final line.\n\nTest suite:\nImplement your program to compute $M$ for each of the following four test cases. No external input is used; the test cases are embedded in the program. In all cases, the alphabet order is $\\{A, C, G, T\\}$.\n\nTest case $1$ (happy path, length match, information-content weighting):\n- Known PWM $P^{(1)}$ of length $L_P = 6$:\n$$\n\\begin{bmatrix}\n0.10 & 0.40 & 0.40 & 0.10 \\\\\n0.05 & 0.05 & 0.85 & 0.05 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.70 & 0.10 & 0.10 & 0.10 \\\\\n0.10 & 0.10 & 0.10 & 0.70 \\\\\n0.40 & 0.10 & 0.40 & 0.10\n\\end{bmatrix}\n$$\n- Filter weights $W^{(1)}$ of length $L_Q = 6$, with per-row perturbations added to $\\log(P^{(1)}_{j,\\cdot})$:\nRow $1$: $\\log(P^{(1)}_{1,\\cdot}) + [0.00, -0.10, +0.10, 0.00]$; Row $2$: $\\log(P^{(1)}_{2,\\cdot}) + [-0.05, +0.20, -0.15, 0.00]$; Row $3$: $\\log(P^{(1)}_{3,\\cdot}) + [+0.30, -0.10, -0.10, -0.10]$; Row $4$: $\\log(P^{(1)}_{4,\\cdot}) + [-0.20, +0.05, +0.05, +0.10]$; Row $5$: $\\log(P^{(1)}_{5,\\cdot}) + [0.00, 0.00, 0.00, 0.00]$; Row $6$: $\\log(P^{(1)}_{6,\\cdot}) + [+0.05, -0.05, +0.05, -0.05]$.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-4}$, aggregation mode: information-content weighting.\n\nTest case $2$ (boundary, exact match, uniform averaging):\n- Known PWM $P^{(2)}$ of length $L_P = 4$:\n$$\n\\begin{bmatrix}\n0.30 & 0.20 & 0.40 & 0.10 \\\\\n0.10 & 0.10 & 0.70 & 0.10 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.20 & 0.50 & 0.20 & 0.10\n\\end{bmatrix}\n$$\n- Filter weights $W^{(2)}$ of length $L_Q = 4$: for each row $j$, set $W^{(2)}_{j,a} = \\log\\left(P^{(2)}_{j,a}\\right)$ so that the filter-derived PWM matches the known PWM exactly under softmax.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-9}$, aggregation mode: uniform averaging.\n\nTest case $3$ (length mismatch, submotif alignment, information-content weighting):\n- Known PWM $P^{(3)}$ of length $L_P = 8$:\n$$\n\\begin{bmatrix}\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.80 & 0.05 & 0.05 & 0.10 \\\\\n0.05 & 0.85 & 0.05 & 0.05 \\\\\n0.05 & 0.05 & 0.85 & 0.05 \\\\\n0.10 & 0.05 & 0.05 & 0.80 \\\\\n0.70 & 0.10 & 0.10 & 0.10 \\\\\n0.25 & 0.25 & 0.25 & 0.25 \\\\\n0.25 & 0.25 & 0.25 & 0.25\n\\end{bmatrix}\n$$\n- Filter weights $W^{(3)}$ of length $L_Q = 5$: for rows corresponding to $P^{(3)}$ positions $2$ through $6$, set $W^{(3)}_{j,a} = \\log\\left(P^{(3)}_{j+1,a}\\right) + \\delta_{j,a}$, where $\\delta_{j,a}$ are small perturbations, e.g., for each row $j$ use $[+0.01, -0.01, 0.00, 0.00]$.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-5}$, aggregation mode: information-content weighting.\n\nTest case $4$ (edge case with zeros, uniform averaging):\n- Known PWM $P^{(4)}$ of length $L_P = 4$ with deterministic rows:\n$$\n\\begin{bmatrix}\n1.00 & 0.00 & 0.00 & 0.00 \\\\\n0.00 & 1.00 & 0.00 & 0.00 \\\\\n0.00 & 0.00 & 1.00 & 0.00 \\\\\n0.00 & 0.00 & 0.00 & 1.00 \\\\\n\\end{bmatrix}\n$$\n- Filter weights $W^{(4)}$ of length $L_Q = 4$ favoring different nucleotides via large logits:\nRow $1$: $[-2.0, -2.0, -2.0, +2.0]$; Row $2$: $[+2.0, -2.0, -2.0, -2.0]$; Row $3$: $[-2.0, +2.0, -2.0, -2.0]$; Row $4$: $[-2.0, -2.0, +2.0, -2.0]$.\n- Temperature $\\tau = 1.0$, pseudocount $\\alpha = 10^{-3}$, aggregation mode: uniform averaging.\n\nFinal output format:\nYour program should produce a single line of output containing the four results $M^{(1)}, M^{(2)}, M^{(3)}, M^{(4)}$ as a comma-separated list enclosed in square brackets, rounded to six decimal places, for example, `[result_1,result_2,result_3,result_4]`. The unit is nats (natural logarithm base). No other text should be printed.",
            "solution": "The task is to construct a quantitative motif recovery metric for Convolutional Neural Network (CNN) filter-derived motifs against a known Position Weight Matrix (PWM), rooted in the principles of probability distributions and information theory, and then implement it in code with a small test suite. The derivation proceeds from the following foundational elements.\n\nFirst principles and core definitions:\n1. A Position Weight Matrix (PWM) $P$ is formed by rows $P_{j,\\cdot}$ that are valid discrete probability distributions over nucleotides $\\{A,C,G,T\\}$, i.e., for each position $j$, one has $\\sum_{a} P_{j,a} = 1$ and $P_{j,a} \\ge 0$.\n2. A CNN filter $W$ is a real-valued matrix; to compare it to a PWM, it must be mapped into a valid distribution at each row. The softmax function is a canonical way to transform real-valued logits to probabilities. Given a temperature $\\tau > 0$, the derived PWM $Q$ is\n$$\nQ_{j,a} = \\frac{\\exp\\left(W_{j,a} / \\tau\\right)}{\\sum_{b \\in \\{A,C,G,T\\}} \\exp\\left(W_{j,b} / \\tau\\right)}.\n$$\nThis respects the axioms of probability: $Q_{j,a} \\ge 0$ and $\\sum_{a} Q_{j,a} = 1$ for each $j$.\n\n3. To ensure numerical stability and avoid undefined logarithms when a probability component is zero, apply per-row Dirichlet-style pseudocount smoothing, parameterized by a scalar $\\alpha > 0$. For any row distribution $R_{j,\\cdot}$ with $\\sum_{a} R_{j,a} = 1$, we define\n$$\n\\widehat{R}_{j,a} = \\frac{R_{j,a} + \\alpha}{1 + 4\\alpha},\n$$\nwhich preserves normalization and enforces strict positivity $\\widehat{R}_{j,a} > 0$ for all components.\n\n4. The Kullback–Leibler divergence between distributions $p$ and $q$ on the same support, with natural logarithms, is\n$$\nD_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{a} p_a \\log\\left(\\frac{p_a}{q_a}\\right).\n$$\nIts symmetric counterpart is defined as\n$$\nD_{\\mathrm{sym}}(p, q) = D_{\\mathrm{KL}}(p \\,\\|\\, q) + D_{\\mathrm{KL}}(q \\,\\|\\, p).\n$$\nThis symmetric version is nonnegative, equals zero if and only if $p = q$, and penalizes discrepancies in both directions.\n\n5. Information content at a motif position quantifies deviation from the uniform distribution. Using natural logarithms, let\n$$\nH(p) = -\\sum_{a} p_a \\log p_a, \\quad I_j = \\log(4) - H(P_{j,\\cdot}).\n$$\nHere, $\\log(4)$ is the maximum entropy over four equiprobable nucleotides, so $I_j \\in [0, \\log(4)]$.\n\nAggregation across positions and alignment:\nA motif of length $L_P$ and a filter-derived PWM of length $L_Q$ may differ in length. For comparison, slide the shorter motif across the longer and compute an aggregated divergence at each valid alignment. Let the shorter motif have length $S$ and the longer have length $L_{\\mathrm{long}}$. For each offset $s \\in \\{0, 1, \\dots, L_{\\mathrm{long}} - S\\}$, define a mapping of rows between the shorter and longer PWMs by direct index correspondence within the overlapped region. For the overlapped rows, compute\n$$\nD_{\\mathrm{agg}}(s) = \\sum_{j=0}^{S-1} w_j \\, D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right),\n$$\nwhere $(j',k')$ denote the aligned rows in the known PWM and the filter-derived PWM, and the weights $w_j$ follow one of two schemes:\n- Uniform averaging: $w_j = \\frac{1}{S}$, reflecting equal importance of all positions.\n- Information-content weighting: $w_j = \\frac{I_{j'}}{\\sum_{u=0}^{S-1} I_{u'}}$, where $I_{j'}$ is computed from the known PWM rows in the overlap. If $\\sum I_{u'} = 0$ (e.g., all rows are uniform), then use $w_j = \\frac{1}{S}$.\n\nThe recovery metric is the minimum aggregated divergence across all valid offsets:\n$$\nM = \\min_s D_{\\mathrm{agg}}(s).\n$$\nA lower $M$ indicates that the filter-derived PWM more closely matches the known PWM at its best alignment, which is what we expect when the filter captures the motif.\n\nAlgorithmic design:\n1. Input preparation: construct the known PWM matrix $P$ and the filter weights matrix $W$, along with scalar parameters $\\tau$ (softmax temperature), $\\alpha$ (pseudocount), and the aggregation mode selection (uniform or information-content weighting).\n2. Softmax conversion: for each row $j$ in $W$, compute $Q_{j,\\cdot}$ using the softmax with temperature $\\tau$, implemented in a numerically stable manner by subtracting the row maximum before exponentiation to avoid overflow. This yields a proper PWM $Q$.\n3. Smoothing: for $P$ and $Q$, apply pseudocount smoothing independently to each row using $\\alpha$, obtaining $\\widehat{P}$ and $\\widehat{Q}$.\n4. Alignment handling: identify the shorter and longer among $\\widehat{P}$ and $\\widehat{Q}$. Slide the shorter across the longer for all valid offsets $s$ and, for each alignment, compute $D_{\\mathrm{agg}}(s)$ via:\n   - Position-wise symmetric KLD: $D_{\\mathrm{sym}}\\!\\left(\\widehat{P}_{j'}, \\widehat{Q}_{k'}\\right)$ at each overlapped index.\n   - Weights $w_j$: either uniform $\\frac{1}{S}$ or based on information content $I_{j'}$ of the known PWM rows within the overlapped region. When computing $I_{j'}$, use the smoothed known PWM to ensure strictly positive entries (this prevents undefined logarithms and keeps the measure stable).\n5. Minimization: select $M = \\min_s D_{\\mathrm{agg}}(s)$ as the metric for the case.\n6. Output: produce a single line with the metrics for all test cases, rounded to six decimal places and expressed in nats.\n\nJustification of choices:\n- The softmax conversion from CNN filter logits to per-position distributions is standard in machine learning and ensures compatibility with PWM semantics.\n- Smoothing with $\\alpha$ is necessary for numeric stability because $D_{\\mathrm{KL}}$ requires strictly positive support. The choice of a small $\\alpha$ keeps the original distributions predominant while avoiding undefined logarithms.\n- Symmetric KLD penalizes mismatches symmetrically and is appropriate for comparing two distributions without privileging one direction of divergence.\n- Information-content weighting reflects biological and statistical relevance by emphasizing positions with higher specificity (lower entropy). Uniform averaging serves as a baseline when such weighting is not intended or informative.\n- Sliding alignment addresses the realistic scenario of filters capturing submotifs or being trained with different receptive field sizes than the motif length.\n\nTest suite coverage:\n- Test case $1$ exercises the general case with moderate perturbations and information-content weighting.\n- Test case $2$ checks the boundary of exact equality, expecting a result near zero.\n- Test case $3$ tests length mismatch and alignment sensitivity, with high-information submotifs.\n- Test case $4$ examines deterministic rows with zeros, confirming that smoothing leads to finite divergences and stable computation.\n\nThe implementation strictly adheres to natural logarithms; therefore, all reported values are in nats. The final output is a comma-separated list enclosed in square brackets, containing four decimal floats rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray, tau: float) -> np.ndarray:\n    \"\"\"\n    Compute row-wise softmax with temperature tau in a numerically stable way.\n    \"\"\"\n    # Subtract max per row for numerical stability.\n    z = logits / tau\n    z = z - np.max(z, axis=1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\ndef smooth_pwm(pwm: np.ndarray, alpha: float) -> np.ndarray:\n    \"\"\"\n    Apply Dirichlet-style pseudocount smoothing per row:\n    (p + alpha) / (1 + 4*alpha), assuming each row sums to 1.\n    \"\"\"\n    # Each row normalization denominator is 1 + 4*alpha because sum(row) == 1.\n    denom = 1.0 + 4.0 * alpha\n    return (pwm + alpha) / denom\n\ndef symmetric_kl(p: np.ndarray, q: np.ndarray) -> float:\n    \"\"\"\n    Compute symmetric KL divergence between two distributions p and q\n    using natural logarithms. Assumes p,q > 0 and sum to 1.\n    \"\"\"\n    # Small epsilon guard (should not be necessary after smoothing,\n    # but we include minimal safeguard).\n    eps = 1e-300\n    p = np.clip(p, eps, 1.0)\n    q = np.clip(q, eps, 1.0)\n    d1 = np.sum(p * np.log(p / q))\n    d2 = np.sum(q * np.log(q / p))\n    return float(d1 + d2)\n\ndef information_content_rows(pwm: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute information content per row: I_j = log(4) - H(p_j),\n    where H(p) = -sum p log p with natural logarithms.\n    \"\"\"\n    # Ensure strictly positive entries to avoid log(0)\n    eps = 1e-300\n    p = np.clip(pwm, eps, 1.0)\n    entropy = -np.sum(p * np.log(p), axis=1)  # nats\n    I = np.log(4.0) - entropy\n    return I\n\ndef aggregate_divergence(P_hat: np.ndarray, Q_hat: np.ndarray,\n                         mode: str, P_for_weights: np.ndarray) -> float:\n    \"\"\"\n    Align shorter across longer and compute minimum aggregated symmetric KL divergence.\n    mode: 'uniform' or 'ic' (information-content weighting using P_for_weights).\n    P_for_weights: PWM to compute information content for weights (use smoothed known PWM).\n    \"\"\"\n    len_P = P_hat.shape[0]\n    len_Q = Q_hat.shape[0]\n    # Determine which to slide: slide the shorter across the longer\n    if len_P <= len_Q:\n        shorter = P_hat\n        longer = Q_hat\n        weights_base = P_for_weights\n        slide_P_over_Q = True\n    else:\n        shorter = Q_hat\n        longer = P_hat\n        # For weights, we should use known PWM rows; if we are sliding Q over P,\n        # weights_base should be P_for_weights (known PWM).\n        weights_base = P_for_weights\n        slide_P_over_Q = False\n\n    S = shorter.shape[0]\n    L = longer.shape[0]\n    best = np.inf\n    # Precompute info content of weights_base rows as needed\n    info_weights = information_content_rows(weights_base)\n    for s in range(L - S + 1):\n        # Build weights for this overlapped region\n        if mode == 'uniform':\n            w = np.ones(S, dtype=float) / float(S)\n        elif mode == 'ic':\n            # Use information content from the known PWM rows in the overlap\n            if slide_P_over_Q:\n                # Overlap indices in P: 0..S-1\n                I = info_weights[:S]\n            else:\n                # Sliding Q over P: overlap indices in P are s..s+S-1\n                I = info_weights[s:s+S]\n            total_I = np.sum(I)\n            if total_I <= 0.0:\n                w = np.ones(S, dtype=float) / float(S)\n            else:\n                w = I / total_I\n        else:\n            raise ValueError(\"Unknown aggregation mode\")\n\n        # Compute weighted symmetric KL over the overlap at offset s\n        total = 0.0\n        for j in range(S):\n            if slide_P_over_Q:\n                p_row = shorter[j]\n                q_row = longer[s + j]\n            else:\n                p_row = longer[s + j]\n                q_row = shorter[j]\n            d_sym = symmetric_kl(p_row, q_row)\n            total += w[j] * d_sym\n        if total < best:\n            best = total\n    return float(best)\n\ndef motif_recovery_metric(P_known: np.ndarray, W_filter: np.ndarray,\n                          tau: float, alpha: float, agg_mode: str) -> float:\n    \"\"\"\n    Compute the motif recovery metric M between known PWM P_known and filter weights W_filter.\n    Steps:\n    - Convert filter weights to PWM via softmax with temperature tau.\n    - Smooth both P and Q with pseudocount alpha.\n    - Align shorter across longer; compute aggregated symmetric KL at each offset.\n    - Return minimum aggregated divergence across offsets.\n    \"\"\"\n    Q_filter = softmax_rows(W_filter, tau)\n    # Smooth both PWMs\n    P_hat = smooth_pwm(P_known, alpha)\n    Q_hat = smooth_pwm(Q_filter, alpha)\n    # For information-content weighting, use smoothed known PWM\n    M = aggregate_divergence(P_hat, Q_hat, agg_mode, P_hat)\n    return M\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    P1 = np.array([\n        [0.10, 0.40, 0.40, 0.10],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.10, 0.10, 0.10, 0.70],\n        [0.40, 0.10, 0.40, 0.10],\n    ], dtype=float)\n    # Build W1 as log(P1) plus perturbations per row\n    logP1 = np.log(P1)\n    perturbations1 = np.array([\n        [0.00, -0.10, +0.10, 0.00],\n        [-0.05, +0.20, -0.15, 0.00],\n        [+0.30, -0.10, -0.10, -0.10],\n        [-0.20, +0.05, +0.05, +0.10],\n        [0.00, 0.00, 0.00, 0.00],\n        [+0.05, -0.05, +0.05, -0.05],\n    ], dtype=float)\n    W1 = logP1 + perturbations1\n    tau1 = 1.0\n    alpha1 = 1e-4\n    agg1 = 'ic'\n\n    # Test case 2\n    P2 = np.array([\n        [0.30, 0.20, 0.40, 0.10],\n        [0.10, 0.10, 0.70, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.20, 0.50, 0.20, 0.10],\n    ], dtype=float)\n    W2 = np.log(P2)  # exact match under softmax\n    tau2 = 1.0\n    alpha2 = 1e-9\n    agg2 = 'uniform'\n\n    # Test case 3\n    P3 = np.array([\n        [0.25, 0.25, 0.25, 0.25],\n        [0.80, 0.05, 0.05, 0.10],\n        [0.05, 0.85, 0.05, 0.05],\n        [0.05, 0.05, 0.85, 0.05],\n        [0.10, 0.05, 0.05, 0.80],\n        [0.70, 0.10, 0.10, 0.10],\n        [0.25, 0.25, 0.25, 0.25],\n        [0.25, 0.25, 0.25, 0.25],\n    ], dtype=float)\n    # Filter corresponds to positions 2..6 of P3 with slight perturbations\n    logP3 = np.log(P3)\n    W3_core = logP3[1:6, :]  # rows 2..6\n    perturbations3 = np.array([\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n        [+0.01, -0.01, 0.00, 0.00],\n    ], dtype=float)\n    W3 = W3_core + perturbations3\n    tau3 = 1.0\n    alpha3 = 1e-5\n    agg3 = 'ic'\n\n    # Test case 4\n    P4 = np.array([\n        [1.00, 0.00, 0.00, 0.00],\n        [0.00, 1.00, 0.00, 0.00],\n        [0.00, 0.00, 1.00, 0.00],\n        [0.00, 0.00, 0.00, 1.00],\n    ], dtype=float)\n    W4 = np.array([\n        [-2.0, -2.0, -2.0, +2.0],  # favors T\n        [+2.0, -2.0, -2.0, -2.0],  # favors A\n        [-2.0, +2.0, -2.0, -2.0],  # favors C\n        [-2.0, -2.0, +2.0, -2.0],  # favors G\n    ], dtype=float)\n    tau4 = 1.0\n    alpha4 = 1e-3\n    agg4 = 'uniform'\n\n    test_cases = [\n        (P1, W1, tau1, alpha1, agg1),\n        (P2, W2, tau2, alpha2, agg2),\n        (P3, W3, tau3, alpha3, agg3),\n        (P4, W4, tau4, alpha4, agg4),\n    ]\n\n    results = []\n    for P_known, W_filter, tau, alpha, agg_mode in test_cases:\n        M = motif_recovery_metric(P_known, W_filter, tau, alpha, agg_mode)\n        # Round to 6 decimal places\n        results.append(f\"{M:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}