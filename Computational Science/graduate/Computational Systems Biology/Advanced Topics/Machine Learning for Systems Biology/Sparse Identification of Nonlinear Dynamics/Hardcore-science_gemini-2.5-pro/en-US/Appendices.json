{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a comprehensive, hands-on implementation of the entire Sparse Identification of Nonlinear Dynamics (SINDy) workflow. You will apply the method to model a fundamental biological phenomenon: population growth subject to an Allee effect, where cooperation at low densities and competition at high densities create complex nonlinear dynamics. By simulating data, estimating derivatives from noisy signals, building a polynomial library, and performing sparse regression, you will develop the essential skills needed to discover governing differential equations directly from time-series data .",
            "id": "3349431",
            "problem": "You are tasked with implementing a complete program that performs Sparse Identification of Nonlinear Dynamics (SINDy) for a single-variable cell population model in computational systems biology. The unknown dynamics are assumed to be governed by a smooth function of the population density, and the mechanistic picture is that low-density cooperation and high-density crowding can jointly shape the net growth rate. You must derive a data-driven, sparse, polynomial model up to cubic order in the state using time series data simulated from this mechanistic picture, and then evaluate whether the identified sparse model is consistent with the expected cubic canonical form implied by the underlying birth-death processes at different perturbation levels. The program must be self-contained and run without user input.\n\nFundamental base assumptions:\n- Let $x(t)$ denote a dimensionless cell density, and $t$ denote time in seconds.\n- The population dynamics obey $\\frac{dx}{dt} = f(x)$ for a deterministic and smooth function $f$, and a polynomial approximation up to cubic order is a valid low-order surrogate in the regime of interest.\n- The sparse modeling approach seeks a parsimonious representation of $f$ from data by selecting a minimal set of active terms from a candidate library.\n- The candidate feature library is restricted to the three polynomial terms $\\{x, x^2, x^3\\}$.\n\nRequired modeling procedure:\n1. Simulate time series data $\\{t_i, x_i\\}_{i=0}^{N-1}$ from a mechanistically plausible growth model where cooperation at low density and crowding at high density coexist. The mechanistic parameters are the intrinsic rate $r$, carrying capacity $K$, and a cooperation threshold $A$ (an Allee threshold), each strictly positive. Use the true dynamics to generate $x(t)$, then contaminate observations with additive, zero-mean Gaussian noise of specified standard deviation applied to $x(t)$, to emulate measurement noise.\n2. Estimate $\\frac{dx}{dt}$ from noisy observations using a numerically stable differentiator suitable for noisy data. You may use a polynomial smoothing differentiator with adjustable window length and polynomial order.\n3. Construct the design matrix with columns $x$, $x^2$, and $x^3$ from the smoothed $x(t)$, and perform sequentially thresholded least squares to obtain a sparse coefficient vector $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2,\\theta_3]^\\top$ such that\n$$\n\\frac{dx}{dt} \\approx \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3.\n$$\nThe sequential thresholding procedure must iteratively remove coefficients with magnitude below a specified threshold and refit on the remaining active set until convergence or a maximum number of iterations.\n\nEvaluation criteria per test case:\n- Compute the ground-truth coefficients $\\boldsymbol{\\theta}^{\\star}$ analytically by expanding the canonical cubic growth arising from the combination of cooperation and crowding effects characterized by $(r,K,A)$, based on a first-principles birth-death formulation where cooperative birth is limited by crowding and competitive loss increases with density.\n- Compare the identified coefficients $\\boldsymbol{\\theta}$ against $\\boldsymbol{\\theta}^{\\star}$ using a per-component relative error defined by\n$$\n\\varepsilon_j = \\frac{|\\theta_j - \\theta_j^{\\star}|}{\\max\\{10^{-12}, |\\theta_j^{\\star}|\\}}, \\quad j \\in \\{1,2,3\\}.\n$$\n- Report a boolean for each test case indicating whether all three coefficients satisfy $\\varepsilon_j \\leq \\text{tol}$ for the specified tolerance and have the same sign as $\\theta_j^{\\star}$.\n\nUnits:\n- Time $t$ must be in seconds. The state $x$ is dimensionless. No angle units are used.\n\nYour program must implement the following test suite. For each tuple, the parameters are $(r, K, A, x_0, T, \\Delta t, \\sigma, \\lambda, \\text{tol}, \\alpha)$, where $x_0$ is the initial condition, $T$ is the total simulation time in seconds, $\\Delta t$ is the sampling interval in seconds, $\\sigma$ is the standard deviation of the additive Gaussian noise on $x$, $\\lambda$ is the threshold for sequentially thresholded least squares, $\\text{tol}$ is the relative error tolerance for acceptance, and $\\alpha$ is the fraction of the number of samples used to choose the smoothing window length (the window length must be odd and adapted to the sample size).\n- Test case 1 (happy path, low noise): $(1.0, 1.0, 0.2, 0.05, 8.0, 0.01, 0.001, 1\\times 10^{-3}, 0.05, 0.07)$.\n- Test case 2 (higher noise): $(0.8, 1.5, 0.4, 0.02, 8.0, 0.01, 0.02, 5\\times 10^{-3}, 0.15, 0.11)$.\n- Test case 3 (sparser sampling and near-threshold initialization): $(1.2, 1.0, 0.8, 0.9, 6.0, 0.05, 0.005, 2\\times 10^{-3}, 0.10, 0.21)$.\n- Test case 4 (boundary challenge, weak attraction): $(0.4, 1.0, 0.95, 0.96, 10.0, 0.02, 0.01, 3\\times 10^{-3}, 0.20, 0.15)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each \"result\" is the boolean acceptance for the corresponding test case computed as described above.",
            "solution": "We derive the identification strategy from a principled birth-death picture where cooperation at low density and crowding at high density combine into a net growth law. Let $x(t)$ be the dimensionless cell density in a well-mixed environment, and let $\\frac{dx}{dt} = f(x)$ be smooth. The mechanism is that the per-capita birth rate initially rises with density due to cooperation but is limited by crowding at high densities, and the per-capita loss rate increases with density due to competition. A commonly used canonical form that captures both effects is a product of three factors: one proportional to the density $x$, one limiting term of form $(1 - x/K)$ capturing carrying capacity $K$, and one cooperative term $(x/A - 1)$ introducing a threshold $A$ (an Allee threshold) below which the net growth becomes negative.\n\nFrom this mechanistic base, we consider the canonical growth law\n$$\n\\frac{dx}{dt} = r\\,x\\,(1 - x/K)\\,(x/A - 1),\n$$\nwhere $r0$ is an intrinsic rate, $K0$ is the carrying capacity, and $A0$ is the cooperation threshold. Although the problem statement did not specify the target formula, the above form is derived from the fundamental assumption that the net growth is the product of density, a crowding limitation, and a cooperation factor, and it is a well-tested model for Allee effects.\n\nTo fit a sparse model using Sparse Identification of Nonlinear Dynamics (SINDy), we construct a candidate function library with the three monomials $\\{x, x^2, x^3\\}$ and seek coefficients $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2,\\theta_3]^\\top$ such that\n$$\n\\frac{dx}{dt} \\approx \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3.\n$$\nExpanding the mechanistic law yields explicit coefficients in this cubic representation. First, expand the product\n$$\n(1 - x/K)\\,(x/A - 1) = \\frac{x}{A} - 1 - \\frac{x^2}{A K} + \\frac{x}{K} = \\left(\\frac{1}{A} + \\frac{1}{K}\\right)x - 1 - \\frac{x^2}{A K}.\n$$\nMultiplying by $r x$ gives\n$$\n\\frac{dx}{dt} = r\\,x\\left[\\left(\\frac{1}{A} + \\frac{1}{K}\\right)x - 1 - \\frac{x^2}{A K}\\right]\n= r\\left[\\left(\\frac{1}{A} + \\frac{1}{K}\\right)x^2 - x - \\frac{1}{A K}x^3\\right].\n$$\nTherefore, the true cubic coefficients are\n$$\n\\theta_1^{\\star} = -r, \\quad \\theta_2^{\\star} = r\\left(\\frac{1}{A} + \\frac{1}{K}\\right), \\quad \\theta_3^{\\star} = -\\frac{r}{A K}.\n$$\n\nAlgorithmic design:\n1. Data generation. For given $(r,K,A,x_0)$, we simulate the deterministic system $\\frac{dx}{dt} = r\\,x\\,(1 - x/K)\\,(x/A - 1)$ from $x(0)=x_0$ over $t \\in [0,T]$ using a high-order integrator with prescribed sampling interval $\\Delta t$. The observed trajectory is contaminated by additive zero-mean Gaussian noise with standard deviation $\\sigma$ applied to $x$, emulating measurement noise. This is realistic in biological measurements where counting errors and fluorescence intensities introduce noise in observed densities.\n\n2. Derivative estimation from noisy data. Direct finite differences amplify noise. A well-tested approach uses polynomial smoothing differentiators such as the Savitzky–Golay filter, which fits a local polynomial of chosen order over a moving window to produce both a smoothed signal and its derivative. We choose an odd window length proportional to the number of samples via a fraction $\\alpha$ and a fixed polynomial order, ensuring window feasibility. This produces robust estimates of $\\frac{dx}{dt}$ and $x$ suitable for regression.\n\n3. Sparse identification with sequential thresholded least squares. Let $\\Theta(x)$ be the design matrix with columns $[x, x^2, x^3]$ built from the smoothed signal. The initial least squares fit\n$$\n\\boldsymbol{\\theta}^{(0)} = \\arg\\min_{\\boldsymbol{\\theta}}\\|\\Theta \\boldsymbol{\\theta} - \\dot{x}\\|_2^2\n$$\nis then sparsified iteratively: for a threshold $\\lambda  0$, we set entries with $|\\theta_j^{(k)}|  \\lambda$ to zero and refit only the active set. Repeating this procedure yields a parsimonious model that suppresses spurious small coefficients introduced by noise and overfitting.\n\n4. Evaluation against the ground truth. For each test case, we compute the analytical cubic coefficients $\\boldsymbol{\\theta}^{\\star} = [-r,\\; r(1/A + 1/K),\\; -r/(A K)]^\\top$. The identified coefficients are accepted if, for all components $j \\in \\{1,2,3\\}$, the relative error\n$$\n\\varepsilon_j = \\frac{|\\theta_j - \\theta_j^{\\star}|}{\\max\\{10^{-12},|\\theta_j^{\\star}|\\}}\n$$\nis less than or equal to the specified tolerance and the sign of $\\theta_j$ matches the sign of $\\theta_j^{\\star}$. The sign check ensures the inferred model preserves the mechanistic increase/decrease directionality: inhibitory linear term ($\\theta_1^{\\star}  0$), facilitative quadratic term ($\\theta_2^{\\star}  0$ from cooperation and crowding combined), and inhibitory cubic term ($\\theta_3^{\\star}  0$ from crowding saturation).\n\nTest suite coverage:\n- The first case is a nominal scenario with low noise and dense sampling, expecting a straightforward recovery.\n- The second case increases measurement noise, requiring robust smoothing and thresholding; the tolerance is correspondingly relaxed.\n- The third case uses sparser sampling and initializes near the cooperative threshold, emphasizing the importance of proper differentiation and model identifiability over a narrower dynamic range.\n- The fourth case is a boundary challenge with $A$ close to $K$ and an initial condition near $K$, reducing the signal amplitude of the cooperative factor and challenging identifiability; tolerance is relaxed accordingly.\n\nImplementation details:\n- Use a reliable ordinary differential equation solver to generate the clean trajectory.\n- Use a Savitzky–Golay filter for smoothing and derivative estimation with an odd window length $w = \\max\\{w_{\\min}, 2\\lfloor(\\alpha N)/2\\rfloor + 1\\}$ clipped to the data length, where $N$ is the number of samples and $w_{\\min}$ ensures polynomial feasibility.\n- Implement sequential thresholded least squares with a fixed maximum iteration count and convergence by stability of the active set.\n- Ensure reproducibility by fixing the random number generator seed.\n\nThe final program computes the acceptance booleans for the four test cases and prints them in the specified single-line format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.signal import savgol_filter\n\n# Seed the random number generator for reproducibility\nrng = np.random.default_rng(42)\n\ndef logistic_allee_rhs(t, x, r, K, A):\n    # x may be array; ensure element-wise computation\n    return r * x * (1.0 - x / K) * (x / A - 1.0)\n\ndef simulate_time_series(r, K, A, x0, T, dt):\n    # Solve the ODE dx/dt = r x (1 - x/K)(x/A - 1) from t=0 to t=T\n    t_eval = np.arange(0.0, T + 1e-12, dt)\n    sol = solve_ivp(fun=lambda t, y: logistic_allee_rhs(t, y, r, K, A),\n                    t_span=(0.0, T),\n                    y0=[x0],\n                    t_eval=t_eval,\n                    method='RK45',\n                    rtol=1e-8,\n                    atol=1e-10)\n    t = sol.t\n    x = sol.y[0]\n    return t, x\n\ndef add_noise(x, sigma):\n    if sigma = 0.0:\n        return x.copy()\n    noise = rng.normal(loc=0.0, scale=sigma, size=x.shape)\n    return x + noise\n\ndef choose_window_length(n, alpha, polyorder):\n    # Compute an odd window length proportional to sample size,\n    # clipped to be feasible for Savitzky-Golay\n    # Start from alpha*n rounded to nearest odd\n    wlen = int(max(5, int(n * alpha)))\n    # Enforce odd\n    if wlen % 2 == 0:\n        wlen += 1\n    # Minimum window for polyorder\n    min_wlen = polyorder + 3\n    if min_wlen % 2 == 0:\n        min_wlen += 1\n    if wlen  min_wlen:\n        wlen = min_wlen\n    # Clip to data length - must be = n\n    if wlen  n:\n        wlen = n - 1 if (n - 1) % 2 == 1 else n - 2\n    # Final guard: ensure odd and at least polyorder+2\n    if wlen % 2 == 0:\n        wlen -= 1\n    if wlen  polyorder + 2:\n        wlen = polyorder + 3\n        if wlen % 2 == 0:\n            wlen += 1\n        if wlen  n:\n            wlen = n - 1 if (n - 1) % 2 == 1 else n - 2\n    return max(5, wlen)\n\ndef estimate_derivative(y, dt, alpha, polyorder=3):\n    n = len(y)\n    wlen = choose_window_length(n, alpha, polyorder)\n    # Smooth signal\n    y_smooth = savgol_filter(y, window_length=wlen, polyorder=polyorder, deriv=0)\n    # Derivative\n    dy_dt = savgol_filter(y, window_length=wlen, polyorder=polyorder, deriv=1, delta=dt)\n    return y_smooth, dy_dt\n\ndef stlsq(Theta, ydot, threshold, max_iter=10, rcond=None):\n    # Sequential thresholded least squares\n    # Initial least squares\n    coef, *_ = np.linalg.lstsq(Theta, ydot, rcond=rcond)\n    active = np.ones_like(coef, dtype=bool)\n    for _ in range(max_iter):\n        # Threshold small coefficients\n        new_active = np.abs(coef) = threshold\n        # If active set hasn't changed, break\n        if np.array_equal(new_active, active):\n            break\n        active = new_active\n        if not np.any(active):\n            coef[:] = 0.0\n            break\n        # Refit on active set\n        Theta_active = Theta[:, active]\n        coef_active, *_ = np.linalg.lstsq(Theta_active, ydot, rcond=rcond)\n        # Update coefficients\n        coef = np.zeros_like(coef)\n        coef[active] = coef_active\n    return coef\n\ndef true_coefficients(r, K, A):\n    # From expansion: dx/dt = (-r) x + r(1/A + 1/K) x^2 + ( - r/(A K) ) x^3\n    return np.array([-r, r * (1.0 / A + 1.0 / K), -r / (A * K)], dtype=float)\n\ndef run_case(params):\n    r, K, A, x0, T, dt, sigma, lam, tol, alpha = params\n    # Simulate clean dynamics\n    t, x_clean = simulate_time_series(r, K, A, x0, T, dt)\n    # Add measurement noise\n    x_noisy = add_noise(x_clean, sigma)\n    # Estimate derivative from noisy data\n    x_smooth, dxdt_est = estimate_derivative(x_noisy, dt, alpha=alpha, polyorder=3)\n    # Build library Theta = [x, x^2, x^3]\n    Theta = np.column_stack([x_smooth, x_smooth**2, x_smooth**3])\n    # Fit sparse model\n    theta_hat = stlsq(Theta, dxdt_est, threshold=lam, max_iter=10, rcond=None)\n    # Ground-truth coefficients\n    theta_true = true_coefficients(r, K, A)\n    # Relative errors and sign checks\n    denom = np.maximum(1e-12, np.abs(theta_true))\n    rel_err = np.abs(theta_hat - theta_true) / denom\n    signs_match = np.all(np.sign(theta_hat) == np.sign(theta_true))\n    accepted = bool(np.all(rel_err = tol) and signs_match)\n    return accepted\n\ndef solve():\n    # Define the test cases from the problem statement:\n    # (r, K, A, x0, T, dt, sigma, lambda, tol, alpha)\n    test_cases = [\n        (1.0, 1.0, 0.2, 0.05, 8.0, 0.01, 0.001, 1e-3, 0.05, 0.07),\n        (0.8, 1.5, 0.4, 0.02, 8.0, 0.01, 0.02, 5e-3, 0.15, 0.11),\n        (1.2, 1.0, 0.8, 0.9, 6.0, 0.05, 0.005, 2e-3, 0.10, 0.21),\n        (0.4, 1.0, 0.95, 0.96, 10.0, 0.02, 0.01, 3e-3, 0.20, 0.15),\n    ]\n    results = []\n    for case in test_cases:\n        res = run_case(case)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "After mastering the basic pipeline, we now address a critical numerical challenge that often arises in practice: the ill-conditioning of the library matrix. This exercise explores how the condition number, $\\kappa_2(\\Theta)$, of the SINDy library can become very large when feature functions have widely different magnitudes, leading to unreliable regression results. You will investigate how a simple yet powerful data pre-processing step—feature scaling—dramatically improves numerical stability, a crucial skill for building robust and accurate data-driven models .",
            "id": "3349468",
            "problem": "You are studying how feature scaling affects the numerical conditioning of the library matrix used in Sparse Identification of Nonlinear Dynamics (SINDy). Consider a single scalar state variable sampled at $N$ points, $\\{x_i\\}_{i=1}^N$. The SINDy library for this problem consists of two features: $x$ and $x^2$. Define the library matrix $\\Theta(x) \\in \\mathbb{R}^{N \\times 2}$ by\n$$\n\\Theta(x) = \\begin{bmatrix}\nx_1  x_1^2 \\\\\nx_2  x_2^2 \\\\\n\\vdots  \\vdots \\\\\nx_N  x_N^2\n\\end{bmatrix}.\n$$\nThe numerical conditioning of $\\Theta(x)$ is measured by the $2$-norm condition number, defined from Singular Value Decomposition (SVD) as\n$$\n\\kappa_2(\\Theta) = \\frac{\\sigma_{\\max}(\\Theta)}{\\sigma_{\\min}(\\Theta)},\n$$\nwhere $\\sigma_{\\max}(\\Theta)$ is the largest singular value and $\\sigma_{\\min}(\\Theta)$ is the smallest singular value. A large $\\kappa_2(\\Theta)$ indicates ill-conditioning.\n\nTo expand the dynamic range of the features and potentially improve conditioning, apply a scalar rescaling to the input data:\n$$\nz_i = s \\, x_i, \\quad s = \\frac{1}{\\operatorname{std}(x)},\n$$\nwhere $\\operatorname{std}(x)$ is the population standard deviation of the samples $\\{x_i\\}_{i=1}^N$ computed with denominator $N$. Construct the scaled library\n$$\n\\Theta(z) = \\begin{bmatrix}\nz_1  z_1^2 \\\\\nz_2  z_2^2 \\\\\n\\vdots  \\vdots \\\\\nz_N  z_N^2\n\\end{bmatrix}.\n$$\nNote that this is a pure rescaling of the input variable and implies a column-wise scaling of $\\Theta(x)$ by the diagonal matrix $\\operatorname{diag}(s, s^2)$.\n\nTask:\n- For each specified test case, generate $N$ equally spaced samples in the closed interval $[a,b]$:\n$$\nx_i = a + \\frac{i-1}{N-1}(b-a), \\quad i = 1,2,\\ldots,N,\n$$\nwith $N \\ge 2$ and $a \\le b$.\n- Compute $\\kappa_2(\\Theta(x))$ and $\\kappa_2(\\Theta(z))$ using SVD in double precision.\n- Determine a boolean flag $\\mathrm{improved}$ defined as $\\mathrm{True}$ if $\\kappa_2(\\Theta(z))  \\kappa_2(\\Theta(x))$, and $\\mathrm{False}$ otherwise.\n\nImplementation details and constraints:\n- Use the population standard deviation with denominator $N$ for $\\operatorname{std}(x)$.\n- If $\\sigma_{\\min}(\\Theta)$ is numerically $0$, treat the condition number as $+\\infty$.\n- All computations must be carried out in floating-point arithmetic with standard double precision.\n- There are no physical units in this problem.\n- Angles are not involved in this problem.\n- Do not express any value as a percentage.\n\nTest suite:\n- Case $A$: $a = 0$, $b = 10^{-3}$, $N = 100$.\n- Case $B$: $a = -10^{-3}$, $b = 10^{-3}$, $N = 100$.\n- Case $C$: $a = 0$, $b = 10^{-6}$, $N = 50$.\n- Case $D$: $a = -\\tfrac{1}{2}$, $b = \\tfrac{1}{2}$, $N = 200$.\n\nRequired program output:\n- Produce a single line containing a comma-separated list enclosed in square brackets.\n- For each test case in the order $A, B, C, D$, append three values in sequence: $\\kappa_2(\\Theta(x))$, $\\kappa_2(\\Theta(z))$, and the boolean $\\mathrm{improved}$.\n- Floating-point values must be rounded to exactly $6$ decimal places.\n- The final output format is therefore\n$$\n[\\kappa_{A,\\mathrm{raw}},\\kappa_{A,\\mathrm{scaled}},\\mathrm{improved}_A,\\kappa_{B,\\mathrm{raw}},\\kappa_{B,\\mathrm{scaled}},\\mathrm{improved}_B,\\kappa_{C,\\mathrm{raw}},\\kappa_{C,\\mathrm{scaled}},\\mathrm{improved}_C,\\kappa_{D,\\mathrm{raw}},\\kappa_{D,\\mathrm{scaled}},\\mathrm{improved}_D].\n$$",
            "solution": "The user wants to analyze the effect of data scaling on the numerical conditioning of a library matrix used in the Sparse Identification of Nonlinear Dynamics (SINDy) method. The problem is self-contained, scientifically sound, and well-posed. I will proceed with a solution.\n\nThe core of the problem lies in comparing the $2$-norm condition number, $\\kappa_2$, of two related matrices. The condition number, defined as the ratio of the largest to the smallest singular value, $\\kappa_2(\\Theta) = \\frac{\\sigma_{\\max}(\\Theta)}{\\sigma_{\\min}(\\Theta)}$, is a standard measure of a matrix's sensitivity to numerical errors. A large condition number indicates an ill-conditioned matrix, which can lead to unreliable results in numerical computations such as solving linear systems.\n\nThe methodology to solve this problem involves a sequence of well-defined computational steps for each test case specified by the parameters $a$, $b$, and $N$.\n\nFirst, we generate the state variable samples. For each test case, $N$ equally spaced points $\\{x_i\\}_{i=1}^N$ are generated on the closed interval $[a,b]$ using the provided formula:\n$$\nx_i = a + \\frac{i-1}{N-1}(b-a), \\quad i = 1,2,\\ldots,N\n$$\nThis is equivalent to using a standard numerical library function like `numpy.linspace(a, b, N)`. The resulting samples form a vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_N]^T$.\n\nSecond, we construct the unscaled library matrix $\\Theta(\\mathbf{x})$. This matrix contains the basis functions evaluated at each sample point. For this problem, the basis functions are $f_1(x) = x$ and $f_2(x) = x^2$. The library matrix $\\Theta(\\mathbf{x}) \\in \\mathbb{R}^{N \\times 2}$ is formed by two columns: the vector $\\mathbf{x}$ itself and the vector of its element-wise squares, $\\mathbf{x}^2 = [x_1^2, x_2^2, \\ldots, x_N^2]^T$.\n$$\n\\Theta(\\mathbf{x}) = \\begin{bmatrix} \\mathbf{x}  \\mathbf{x}^2 \\end{bmatrix}\n$$\n\nThird, we compute the condition number $\\kappa_2(\\Theta(\\mathbf{x}))$. This requires performing a Singular Value Decomposition (SVD) on $\\Theta(\\mathbf{x})$. The SVD provides the singular values, $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq 0$. For our $N \\times 2$ matrix (with $N \\ge 2$), there are two singular values, $\\sigma_{\\max} = \\sigma_1$ and $\\sigma_{\\min} = \\sigma_2$. The condition number is then calculated as their ratio. As per the problem specification, if $\\sigma_{\\min}$ is numerically zero, the condition number is treated as infinite.\n\nFourth, we perform data scaling. A scaled version of the data, $\\mathbf{z}$, is created by multiplying each sample $x_i$ by a scaling factor $s$.\n$$\nz_i = s \\cdot x_i, \\quad \\text{where} \\quad s = \\frac{1}{\\operatorname{std}(\\mathbf{x})}\n$$\nThe standard deviation, $\\operatorname{std}(\\mathbf{x})$, is the population standard deviation, calculated with a denominator of $N$:\n$$\n\\operatorname{std}(\\mathbf{x}) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}, \\quad \\text{where} \\quad \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i\n$$\nThis scaling ensures that the resulting data vector $\\mathbf{z}$ has a population standard deviation of exactly $1$.\n\nFifth, we construct the scaled library matrix $\\Theta(\\mathbf{z})$ and compute its condition number $\\kappa_2(\\Theta(\\mathbf{z}))$. The procedure is identical to that for $\\Theta(\\mathbf{x})$, but using the scaled data vector $\\mathbf{z}$.\n$$\n\\Theta(\\mathbf{z}) = \\begin{bmatrix} \\mathbf{z}  \\mathbf{z}^2 \\end{bmatrix}\n$$\n\nFinally, we determine if the scaling was beneficial. We define a boolean flag, $\\mathrm{improved}$, which is set to $\\mathrm{True}$ if $\\kappa_2(\\Theta(\\mathbf{z}))  \\kappa_2(\\Theta(\\mathbf{x}))$ and $\\mathrm{False}$ otherwise.\n\nThis entire process is repeated for each of the four test cases. The results—$\\kappa_2(\\Theta(\\mathbf{x}))$, $\\kappa_2(\\Theta(\\mathbf{z}))$, and $\\mathrm{improved}$—are collected for each case and formatted into a single output string as required.\n\nThe rationale behind this scaling is to mitigate ill-conditioning that arises from large disparities in the magnitudes of the library matrix's columns. For instance, in Cases A ($[0, 10^{-3}]$) and C ($[0, 10^{-6}]$), the values in the first column ($x$) are small, and the values in the second column ($x^2$) are orders of magnitude smaller. This discrepancy in scale typically leads to very large condition numbers. By scaling the data such that $\\operatorname{std}(\\mathbf{z})=1$, we place the state variable in a \"natural\" unit system. This tends to balance the norms of the resulting library columns (e.g., $\\mathbf{z}$ and $\\mathbf{z}^2$), thereby reducing the condition number. For all test cases, including those with centered data (Cases B and D), this scaling is expected to improve conditioning, leading to $\\mathrm{improved} = \\mathrm{True}$. The implementation will use `numpy` for all numerical computations, leveraging its double-precision floating-point arithmetic and optimized linear algebra routines.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares condition numbers of SINDy library matrices\n    before and after feature scaling for four test cases.\n    \"\"\"\n\n    def get_condition_number(data_vector: np.ndarray) - float:\n        \"\"\"\n        Constructs a library matrix from a data vector and computes its 2-norm\n        condition number using SVD.\n        \n        Args:\n            data_vector: A 1D numpy array of state variable samples.\n\n        Returns:\n            The condition number of the library matrix. Returns np.inf if the\n            matrix is singular.\n        \"\"\"\n        # Problem constraints ensure data_vector.size = 2\n        \n        # Construct the library matrix from columns [v, v^2]\n        library_matrix = np.stack([data_vector, data_vector**2], axis=1)\n        \n        # Calculate singular values. SVD returns them in descending order.\n        singular_values = np.linalg.svd(library_matrix, compute_uv=False)\n        \n        sigma_max = singular_values[0]\n        # For an N x 2 matrix, there are 2 singular values.\n        sigma_min = singular_values[1]\n        \n        # Per problem spec, if sigma_min is numerically 0, kappa is +inf.\n        # We use a small epsilon for a robust floating-point comparison.\n        if sigma_min  np.finfo(np.float64).eps:\n            return np.inf\n        \n        return sigma_max / sigma_min\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        # (a, b, N)\n        (0.0, 1e-3, 100),       # Case A\n        (-1e-3, 1e-3, 100),    # Case B\n        (0.0, 1e-6, 50),       # Case C\n        (-0.5, 0.5, 200),      # Case D\n    ]\n\n    all_results = []\n    for a, b, N in test_cases:\n        # Generate N equally spaced samples in [a, b] using double precision\n        x = np.linspace(a, b, N, dtype=np.float64)\n\n        # Calculate condition number for the unscaled (raw) library matrix\n        kappa_x = get_condition_number(x)\n\n        # Calculate the population standard deviation of x.\n        # numpy.std defaults to ddof=0, which corresponds to the denominator N.\n        std_x = np.std(x)\n\n        kappa_z = np.inf\n        # Scale the data only if it has a non-zero standard deviation.\n        # This avoids division by zero if all samples are identical (a=b).\n        if std_x  np.finfo(np.float64).eps:\n            s = 1.0 / std_x\n            z = s * x\n            # Calculate condition number for the scaled library matrix\n            kappa_z = get_condition_number(z)\n\n        # Determine if scaling improved the condition number\n        improved = kappa_z  kappa_x\n\n        # Append formatted results for this case to the list\n        all_results.append(f\"{kappa_x:.6f}\")\n        all_results.append(f\"{kappa_z:.6f}\")\n        all_results.append(str(improved))\n\n    # Print the final output in the specified single-line format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world biological data from fields like proteomics are often noisy, sparse, and irregularly sampled, rendering traditional derivative estimation methods ineffective. This advanced practice introduces the powerful weak formulation of SINDy, which elegantly bypasses the need for direct differentiation. By applying integration by parts with smooth test functions, the derivative is transferred from the noisy data to a known analytical function, enabling robust model identification even from highly imperfect datasets. This exercise will equip you to tackle the challenging data realities of modern systems biology research .",
            "id": "3349347",
            "problem": "You will implement a complete, runnable program that performs weak-form Sparse Identification of Nonlinear Dynamics (SINDy) to recover a phosphorylation cascade from irregularly sampled, noisy proteomics time series with missing time points. The program must use the weak formulation to estimate the time derivative via integration by parts, and then perform sparse regression to identify the governing nonlinear dynamics. Your implementation must be entirely self-contained and produce outputs for a specified test suite.\n\nThe fundamental base must be the following, expressed in terms of widely accepted principles used in computational systems biology:\n\n- Mass-action kinetics and conservation arguments for phosphorylation cascades, where an upstream kinase promotes phosphorylation of a downstream substrate and phosphatases promote dephosphorylation. A state variable represents the phosphorylated fraction, which is dimensionless and lies in the interval $[0,1]$.\n- The chain of events in a phosphorylation cascade can be modeled as ordinary differential equations (ODEs) that incorporate activation and deactivation, with nonlinear terms arising from interactions such as an upstream phosphorylated species activating the downstream substrate. The general form is consistent with mass-action kinetics and saturation arising from fractional occupancy.\n- The weak formulation for time derivatives obtained by selecting compactly supported, sufficiently smooth test functions and integrating by parts to avoid direct numerical differentiation under noise and irregular sampling.\n\nYou will generate synthetic ground-truth data using a three-tier phosphorylation cascade governed by the following ODEs, which encode activation and deactivation consistent with mass-action kinetics under a constant stimulus $S$:\n\n$$\n\\frac{d x_1}{dt} = k_1 S - (k_1 S + k_2) x_1,\n$$\n\n$$\n\\frac{d x_2}{dt} = k_3 x_1 - k_3 x_1 x_2 - k_4 x_2,\n$$\n\n$$\n\\frac{d x_3}{dt} = k_5 x_2 - k_5 x_2 x_3 - k_6 x_3,\n$$\n\nwhere $x_1$, $x_2$, and $x_3$ are the phosphorylated fractions (dimensionless), $S$ is a constant stimulus, and $k_1, k_2, k_3, k_4, k_5, k_6$ are positive rate constants. Take $S = 1$, $k_1 = 1.0$, $k_2 = 0.5$, $k_3 = 2.0$, $k_4 = 0.3$, $k_5 = 1.5$, $k_6 = 0.2$, with initial condition $\\mathbf{x}(0) = [0,0,0]^\\top$ and final time $T = 10$ seconds. Time is in seconds and concentrations are dimensionless fractions. These ODEs are used only to generate data; your identification must assume a general sparse form over a candidate library.\n\nYour candidate nonlinear library must include the following functions of the state vector $\\mathbf{x}(t) = [x_1(t), x_2(t), x_3(t)]^\\top$:\n\n$$\n\\Theta(\\mathbf{x}) = \\left[1,\\; x_1,\\; x_2,\\; x_3,\\; x_1 x_2,\\; x_1 x_3,\\; x_2 x_3\\right].\n$$\n\nThe sparse regression aims to find coefficient vectors $\\boldsymbol{\\Xi}_j$ for $j \\in \\{1,2,3\\}$ such that the weak-form estimated time derivatives satisfy:\n\n$$\n\\left\\langle \\frac{d x_j}{dt}, \\psi_m \\right\\rangle \\approx \\left\\langle \\Theta(\\mathbf{x}), \\psi_m \\right\\rangle \\boldsymbol{\\Xi}_j,\n$$\n\nfor a collection of compactly supported test functions $\\psi_m(t)$ indexed by window $m$, where $\\langle f, g \\rangle = \\int f(t) g(t) \\, dt$. You must not use pointwise numerical differentiation of $x_j(t)$. Instead, apply integration by parts to estimate weak-form derivatives:\n\n$$\n\\int \\psi_m(t) \\frac{d x_j(t)}{dt} \\, dt = - \\int \\frac{d \\psi_m(t)}{dt} x_j(t) \\, dt,\n$$\n\nassuming that $\\psi_m(t)$ vanishes at the boundaries of its support, so the boundary term is zero. You must compute these integrals using the trapezoidal rule over the irregularly sampled time points.\n\nFor the test functions, use compactly supported cosine bells centered at $c_m$ with half-width $h$:\n\n$$\n\\psi_m(t) = \n\\begin{cases}\n\\frac{1}{2}\\left(1 + \\cos\\left(\\pi \\frac{t - c_m}{h}\\right)\\right),  \\text{if } |t - c_m| \\le h,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\n\n$$\n\\frac{d \\psi_m}{dt}(t) = \n\\begin{cases}\n- \\frac{\\pi}{2 h}\\sin\\left(\\pi \\frac{t - c_m}{h}\\right),  \\text{if } |t - c_m| \\le h,\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n\nConstruct a set of window centers $\\{c_m\\}$ evenly spaced over the interval $[h, T - h]$ to define a test function family. For each window, reject it if its effective support mass $\\int \\psi_m(t) \\, dt$ estimated by the trapezoidal rule is less than a fraction of $h$, to avoid degenerate windows with too few points due to missing data.\n\nTo form the regression, assemble for each window $m$:\n\n- The weak-form derivative estimate for each state $j$:\n$$\ny_{j,m} = - \\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt.\n$$\n\n- The weak-form feature vector:\n$$\n\\left(\\Theta_m\\right)_k = \\int \\psi_m(t) \\, \\phi_k(\\mathbf{x}(t)) \\, dt,\n$$\n\nwhere $\\phi_k$ denotes the $k^\\text{th}$ function in the candidate library $\\Theta(\\mathbf{x})$.\n\nUse Sequential Thresholded Least Squares (STLSQ) to enforce sparsity:\n\n- Solve a least squares problem for each $j$ to estimate $\\boldsymbol{\\Xi}_j$.\n- Threshold all coefficients with magnitude below a specified threshold $\\lambda$ to zero.\n- Refit on the remaining support.\n- Iterate until convergence or a fixed number of iterations.\n\nSampling and data generation must follow these rules:\n\n- Solve the ODEs with dense output to allow evaluation at arbitrary times.\n- Draw $N = 300$ irregular sampling times uniformly at random over $[0, T]$, then sort them in ascending order. Apply a missing fraction by removing a specified proportion of randomly selected time points to emulate dropout.\n- Add independent, zero-mean Gaussian measurement noise with specified standard deviation $\\sigma$ to each observed state, and clip the noisy states to the interval $[0,1]$ to reflect fractional phosphorylation.\n- Do not use any knowledge of the true coefficients during identification; they are only for evaluation.\n\nEvaluation and outputs:\n\n- Define the true supports for the three equations under the chosen library indexing $[1, x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3]$:\n\n$$\n\\text{Supp}(x_1) = \\{0, 1\\}, \\quad \\text{Supp}(x_2) = \\{1, 2, 4\\}, \\quad \\text{Supp}(x_3) = \\{2, 3, 6\\}.\n$$\n\n- After identification, determine the estimated supports as the indices of nonzero coefficients (after thresholding). For each test case, output a boolean that is $\\text{True}$ if and only if the estimated supports match the true supports exactly for all three equations; otherwise output $\\text{False}$.\n\nTest suite:\n\nProvide four test cases, each defined by $($random seed$, \\sigma, \\text{missing fraction}, h, \\lambda)$:\n\n1. $(1, 0.02, 0.20, 0.60, 0.06)$.\n2. $(2, 0.00, 0.35, 0.60, 0.05)$.\n3. $(3, 0.08, 0.40, 0.80, 0.08)$.\n4. $(4, 0.05, 0.50, 1.00, 0.10)$.\n\nAll physical times are in seconds, concentrations are dimensionless fractions, and all outputs are unitless booleans.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[$result1,result2,result3,result4$]$).",
            "solution": "The problem requires the implementation of the weak-form Sparse Identification of Nonlinear Dynamics (SINDy) algorithm to discover the governing ordinary differential equations (ODEs) of a three-tier phosphorylation cascade from noisy, incomplete, and irregularly sampled time-series data. The solution proceeds by first generating synthetic data according to specified dynamics and corruption rules. Subsequently, it applies the weak-form SINDy procedure, which avoids direct numerical differentiation by using integration by parts with smooth, compactly supported test functions. The resulting linear system is solved for sparse coefficient vectors using the Sequential Thresholded Least Squares (STLSQ) algorithm. Finally, the identified model structures are compared against the known true structures for a suite of test cases.\n\nThe methodological pipeline is constructed as follows:\n\n1.  **Synthetic Data Generation**: The foundation of this problem is a ground-truth model of a phosphorylation cascade. The dynamics are given by a system of three coupled ODEs for the phosphorylated fractions $x_1(t)$, $x_2(t)$, and $x_3(t)$:\n    $$\n    \\frac{d x_1}{dt} = k_1 S - (k_1 S + k_2) x_1\n    $$\n    $$\n    \\frac{d x_2}{dt} = k_3 x_1 - k_3 x_1 x_2 - k_4 x_2\n    $$\n    $$\n    \\frac{d x_3}{dt} = k_5 x_2 - k_5 x_2 x_3 - k_6 x_3\n    $$\n    The parameters are set to $S = 1$, $k_1 = 1.0$, $k_2 = 0.5$, $k_3 = 2.0$, $k_4 = 0.3$, $k_5 = 1.5$, and $k_6 = 0.2$. The system is integrated over the time interval $[0, T]$ where $T = 10$ seconds, starting from the initial condition $\\mathbf{x}(0) = [0, 0, 0]^\\top$. A high-fidelity numerical solution is obtained using a standard ODE solver with dense output enabled, which provides an interpolant for the true trajectory.\n    From this true solution, measurement data are simulated. First, $N = 300$ time points are drawn uniformly at random from $[0, T]$ and sorted. A specified fraction of these points are then randomly removed to simulate data dropout. At the remaining time points, the true state values are corrupted by adding independent, zero-mean Gaussian noise with a specified standard deviation $\\sigma$. As the state variables represent physical fractions, the resulting noisy values are clipped to the interval $[0, 1]$.\n\n2.  **Weak-Form SINDy Formulation**: The core of the identification task is to find a sparse coefficient vector $\\boldsymbol{\\Xi}_j$ for each state variable $x_j$ such that the dynamics are well-approximated by a linear combination of candidate functions from a library $\\Theta(\\mathbf{x})$. The governing equation for each state is assumed to have the form:\n    $$\n    \\frac{d x_j}{dt} = \\Theta(\\mathbf{x}(t)) \\boldsymbol{\\Xi}_j\n    $$\n    The specified candidate library is $\\Theta(\\mathbf{x}) = [1, x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3]$.\n    Instead of differentiating the noisy data $x_j(t)$ directly, the weak formulation is employed. Both sides of the equation are multiplied by a smooth, compactly supported test function $\\psi_m(t)$ and integrated over the time domain.\n    $$\n    \\left\\langle \\frac{d x_j}{dt}, \\psi_m \\right\\rangle = \\left\\langle \\Theta(\\mathbf{x}), \\psi_m \\right\\rangle \\boldsymbol{\\Xi}_j\n    $$\n    where $\\langle f, g \\rangle = \\int f(t) g(t) \\, dt$. The crucial step is applying integration by parts to the left-hand side term:\n    $$\n    \\int \\frac{d x_j}{dt} \\psi_m(t) \\, dt = \\left[x_j(t) \\psi_m(t)\\right]_{\\partial \\Omega} - \\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt\n    $$\n    Since the test function $\\psi_m(t)$ has compact support, it vanishes at the boundaries of the integration domain, making the boundary term zero. This transfers the derivative from the noisy data $x_j$ to the smooth, analytically known test function $\\psi_m$:\n    $$\n    -\\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt \\approx \\left(\\int \\Theta(\\mathbf{x}(t)) \\psi_m(t) \\, dt \\right) \\boldsymbol{\\Xi}_j\n    $$\n    The test functions are chosen to be cosine bells centered at $c_m$ with half-width $h$:\n    $$\n    \\psi_m(t) = \\begin{cases} \\frac{1}{2}\\left(1 + \\cos\\left(\\pi \\frac{t - c_m}{h}\\right)\\right),  \\text{if } |t - c_m| \\le h \\\\ 0,  \\text{otherwise} \\end{cases}\n    $$\n    Their derivatives are analytically computed as:\n    $$\n    \\frac{d \\psi_m}{dt}(t) = \\begin{cases} - \\frac{\\pi}{2 h}\\sin\\left(\\pi \\frac{t - c_m}{h}\\right),  \\text{if } |t - c_m| \\le h \\\\ 0,  \\text{otherwise} \\end{cases}\n    $$\n    An ensemble of these test functions is created by selecting centers $c_m$ evenly spaced across the interval $[h, T-h]$.\n\n3.  **Construction of the Linear System**: For each test function $\\psi_m$ in the ensemble, a single row in a linear system is constructed.\n    -   The weak derivative term for state $j$ is calculated as $y_{j,m} = - \\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt$.\n    -   The weak feature vector is formed with components $(\\boldsymbol{\\theta}_w)_k = \\int \\phi_k(\\mathbf{x}(t)) \\psi_m(t) \\, dt$, where $\\phi_k$ is the $k$-th function in the library $\\Theta(\\mathbf{x})$.\n    All integrals are computed numerically using the trapezoidal rule, which is suitable for the irregular time grid. To ensure robustness against data scarcity, any window $m$ is discarded if the effective mass of its test function, $\\int \\psi_m(t) \\, dt$, falls below a threshold (chosen as $0.1 h$).\n    By stacking the results from all valid windows, we form a linear system for each state $j \\in \\{1, 2, 3\\}$:\n    $$\n    \\mathbf{\\Theta}_w \\boldsymbol{\\Xi}_j \\approx \\mathbf{y}_j\n    $$\n    where $\\mathbf{\\Theta}_w$ is the matrix of weak features and $\\mathbf{y}_j$ is the vector of weak derivatives.\n\n4.  **Sparse Regression**: The Sequential Thresholded Least Squares (STLSQ) algorithm is used to find a sparse solution $\\boldsymbol{\\Xi}_j$ for each state. This iterative procedure is as follows:\n    -   Initialize the set of active coefficients (the support) to include all candidate functions.\n    -   Iteratively perform the following steps:\n        a.  Solve a standard least-squares problem for the coefficients on the current support: $\\boldsymbol{\\Xi}_j^{\\text{sub}} = (\\mathbf{\\Theta}_w^{\\text{sub}})^\\dagger \\mathbf{y}_j$.\n        b.  Identify coefficients in $\\boldsymbol{\\Xi}_j^{\\text{sub}}$ whose magnitudes are below a given threshold $\\lambda$.\n        c.  If no such coefficients exist, the algorithm has converged.\n        d.  Otherwise, remove the corresponding functions from the support and repeat the process.\n    -   This is repeated for a fixed number of iterations or until the support stabilizes. The final non-zero elements of $\\boldsymbol{\\Xi}_j$ constitute the identified model structure for $\\dot{x}_j$.\n\n5.  **Evaluation**: The performance is evaluated by comparing the identified model structure against the true structure. The true supports for the given library $\\Theta(\\mathbf{x})$ are known from the governing equations:\n    -   $dx_1/dt$ depends on $\\{1, x_1\\}$, corresponding to indices $\\{0, 1\\}$.\n    -   $dx_2/dt$ depends on $\\{x_1, x_2, x_1 x_2\\}$, corresponding to indices $\\{1, 2, 4\\}$.\n    -   $dx_3/dt$ depends on $\\{x_2, x_3, x_2 x_3\\}$, corresponding to indices $\\{2, 3, 6\\}$.\n    For each test case, the identified support for each equation (the set of indices of non-zero coefficients in the final $\\boldsymbol{\\Xi}_j$) is computed. A test case yields a result of `True` if and only if the identified supports for all three equations ($x_1$, $x_2$, and $x_3$) exactly match their respective true supports. Otherwise, the result is `False`.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the SINDy identification for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # (random_seed, sigma, missing_fraction, h, lambda_threshold)\n        (1, 0.02, 0.20, 0.60, 0.06),\n        (2, 0.00, 0.35, 0.60, 0.05),\n        (3, 0.08, 0.40, 0.80, 0.08),\n        (4, 0.05, 0.50, 1.00, 0.10),\n    ]\n\n    results = []\n    for seed, sigma, missing_frac, h, lambda_thresh in test_cases:\n        result = run_sindy_case(seed, sigma, missing_frac, h, lambda_thresh)\n        results.append(result)\n    \n    # Format the final output string exactly as required.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_sindy_case(seed, sigma, missing_frac, h, lambda_thresh):\n    \"\"\"\n    Executes the full pipeline for a single test case: data generation,\n    model identification, and evaluation.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Data Generation\n    t_meas, x_meas = generate_data(sigma, missing_frac)\n\n    # 2. Model Identification\n    identified_supports = identify_model(t_meas, x_meas, h, lambda_thresh)\n\n    # 3. Evaluation\n    true_supports = [{0, 1}, {1, 2, 4}, {2, 3, 6}]\n    \n    is_correct = all(\n        ident == true for ident, true in zip(identified_supports, true_supports)\n    )\n    \n    return is_correct\n\ndef ode_system(t, x):\n    \"\"\"Defines the ODEs for the phosphorylation cascade.\"\"\"\n    S = 1.0\n    k1, k2, k3, k4, k5, k6 = 1.0, 0.5, 2.0, 0.3, 1.5, 0.2\n    \n    dx1_dt = k1 * S - (k1 * S + k2) * x[0]\n    dx2_dt = k3 * x[0] - k3 * x[0] * x[1] - k4 * x[1]\n    dx3_dt = k5 * x[1] - k5 * x[1] * x[2] - k6 * x[2]\n    \n    return [dx1_dt, dx2_dt, dx3_dt]\n\ndef generate_data(sigma, missing_frac):\n    \"\"\"Generates noisy, irregularly sampled data with dropouts.\"\"\"\n    T = 10.0\n    N = 300\n    x0 = [0.0, 0.0, 0.0]\n\n    # Solve ODE for ground truth\n    sol = solve_ivp(ode_system, [0, T], x0, dense_output=True, method='RK45')\n\n    # Generate irregular time samples\n    t_samples = np.sort(np.random.uniform(0, T, N))\n\n    # Apply missing fraction (dropouts)\n    n_keep = int(round((1.0 - missing_frac) * N))\n    keep_indices = np.random.choice(N, n_keep, replace=False)\n    t_meas = t_samples[keep_indices]\n    t_meas = np.sort(t_meas) # Ensure time is monotonic after selection\n\n    # Get clean data and add noise\n    x_clean = sol.sol(t_meas)\n    noise = np.random.normal(0, sigma, x_clean.shape)\n    x_meas = x_clean + noise\n\n    # Clip data to physical range [0, 1]\n    x_meas = np.clip(x_meas, 0, 1)\n\n    return t_meas, x_meas\n\ndef candidate_library(x):\n    \"\"\"Constructs the candidate library Theta(x) for SINDy.\"\"\"\n    x1, x2, x3 = x[0, :], x[1, :], x[2, :]\n    return np.vstack([\n        np.ones_like(x1),\n        x1,\n        x2,\n        x3,\n        x1 * x2,\n        x1 * x3,\n        x2 * x3\n    ])\n\ndef psi(t, c, h):\n    \"\"\"Cosine bell test function.\"\"\"\n    arg = np.pi * (t - c) / h\n    val = 0.5 * (1 + np.cos(arg))\n    val[np.abs(t-c)  h] = 0\n    return val\n\ndef dpsi_dt(t, c, h):\n    \"\"\"Derivative of the cosine bell test function.\"\"\"\n    arg = np.pi * (t - c) / h\n    val = - (np.pi / (2 * h)) * np.sin(arg)\n    val[np.abs(t-c)  h] = 0\n    return val\n\ndef stlsq(A, b, threshold, max_iter=20):\n    \"\"\"Sequential Thresholded Least Squares algorithm.\"\"\"\n    n_features = A.shape[1]\n    xi = np.zeros(n_features)\n    support = np.arange(n_features)\n\n    for _ in range(max_iter):\n        if len(support) == 0:\n            return np.zeros(n_features)\n        \n        A_sub = A[:, support]\n        xi_sub = np.linalg.lstsq(A_sub, b, rcond=None)[0]\n        \n        small_coeffs_mask = np.abs(xi_sub)  threshold\n        \n        if not np.any(small_coeffs_mask):\n            xi[support] = xi_sub\n            return xi # Converged\n        \n        support = np.delete(support, np.where(small_coeffs_mask)[0])\n        \n    if len(support)  0:\n        xi_sub = np.linalg.lstsq(A[:, support], b, rcond=None)[0]\n        xi[support] = xi_sub\n        \n    return xi\n\ndef identify_model(t, x, h, lambda_thresh):\n    \"\"\"Identifies the model structure using weak-form SINDy.\"\"\"\n    T = t[-1]\n    n_states = x.shape[0]\n    \n    # Generate window centers\n    window_centers = np.arange(h, T - h, h / 2.0)\n    \n    weak_derivatives = []\n    weak_library_terms = []\n\n    for c in window_centers:\n        mask = (t = c - h)  (t = c + h)\n        t_w = t[mask]\n        \n        if len(t_w)  2:\n            continue\n            \n        x_w = x[:, mask]\n        \n        psi_vals = psi(t_w, c, h)\n\n        # Window rejection criterion\n        support_mass = np.trapz(psi_vals, t_w)\n        if support_mass  0.1 * h:\n            continue\n            \n        dpsi_dt_vals = dpsi_dt(t_w, c, h)\n        \n        # Calculate weak derivatives\n        y_m = np.array([-np.trapz(x_w[j, :] * dpsi_dt_vals, t_w) for j in range(n_states)])\n        weak_derivatives.append(y_m)\n        \n        # Calculate weak library terms\n        lib_vals = candidate_library(x_w)\n        theta_w_m = np.array([np.trapz(lib_vals[k, :] * psi_vals, t_w) for k in range(lib_vals.shape[0])])\n        weak_library_terms.append(theta_w_m)\n\n    if not weak_library_terms:\n         # No valid windows found, return empty supports\n        return [set() for _ in range(n_states)]\n\n    Y = np.vstack(weak_derivatives)\n    Theta_w = np.vstack(weak_library_terms)\n    \n    identified_supports = []\n    for j in range(n_states):\n        y_j = Y[:, j]\n        xi_j = stlsq(Theta_w, y_j, lambda_thresh)\n        support_j = set(np.where(np.abs(xi_j)  1e-9)[0])\n        identified_supports.append(support_j)\n        \n    return identified_supports\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}