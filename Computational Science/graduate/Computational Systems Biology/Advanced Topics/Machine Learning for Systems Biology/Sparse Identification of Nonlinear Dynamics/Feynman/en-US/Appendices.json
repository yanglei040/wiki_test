{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a complete walkthrough of the standard SINDy algorithm applied to a classic problem in population dynamics. You will model a cell population exhibiting an Allee effect, where cooperation at low densities and competition at high densities create nonlinear growth patterns . By implementing the entire pipeline—from simulating noisy data to performing sparse regression—you will gain hands-on experience with the core components of SINDy and learn to validate your data-driven model against an analytical ground truth.",
            "id": "3349431",
            "problem": "You are tasked with implementing a complete program that performs Sparse Identification of Nonlinear Dynamics (SINDy) for a single-variable cell population model in computational systems biology. The unknown dynamics are assumed to be governed by a smooth function of the population density, and the mechanistic picture is that low-density cooperation and high-density crowding can jointly shape the net growth rate. You must derive a data-driven, sparse, polynomial model up to cubic order in the state using time series data simulated from this mechanistic picture, and then evaluate whether the identified sparse model is consistent with the expected cubic canonical form implied by the underlying birth-death processes at different perturbation levels. The program must be self-contained and run without user input.\n\nFundamental base assumptions:\n- Let $x(t)$ denote a dimensionless cell density, and $t$ denote time in seconds.\n- The population dynamics obey $\\frac{dx}{dt} = f(x)$ for a deterministic and smooth function $f$, and a polynomial approximation up to cubic order is a valid low-order surrogate in the regime of interest.\n- The sparse modeling approach seeks a parsimonious representation of $f$ from data by selecting a minimal set of active terms from a candidate library.\n- The candidate feature library is restricted to the three polynomial terms $\\{x, x^2, x^3\\}$.\n\nRequired modeling procedure:\n1. Simulate time series data $\\{t_i, x_i\\}_{i=0}^{N-1}$ from a mechanistically plausible growth model where cooperation at low density and crowding at high density coexist. The mechanistic parameters are the intrinsic rate $r$, carrying capacity $K$, and a cooperation threshold $A$ (an Allee threshold), each strictly positive. Use the true dynamics to generate $x(t)$, then contaminate observations with additive, zero-mean Gaussian noise of specified standard deviation applied to $x(t)$, to emulate measurement noise.\n2. Estimate $\\frac{dx}{dt}$ from noisy observations using a numerically stable differentiator suitable for noisy data. You may use a polynomial smoothing differentiator with adjustable window length and polynomial order.\n3. Construct the design matrix with columns $x$, $x^2$, and $x^3$ from the smoothed $x(t)$, and perform sequentially thresholded least squares to obtain a sparse coefficient vector $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2,\\theta_3]^\\top$ such that\n$$\n\\frac{dx}{dt} \\approx \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3.\n$$\nThe sequential thresholding procedure must iteratively remove coefficients with magnitude below a specified threshold and refit on the remaining active set until convergence or a maximum number of iterations.\n\nEvaluation criteria per test case:\n- Compute the ground-truth coefficients $\\boldsymbol{\\theta}^{\\star}$ analytically by expanding the canonical cubic growth arising from the combination of cooperation and crowding effects characterized by $(r,K,A)$, based on a first-principles birth-death formulation where cooperative birth is limited by crowding and competitive loss increases with density.\n- Compare the identified coefficients $\\boldsymbol{\\theta}$ against $\\boldsymbol{\\theta}^{\\star}$ using a per-component relative error defined by\n$$\n\\varepsilon_j = \\frac{|\\theta_j - \\theta_j^{\\star}|}{\\max\\{10^{-12}, |\\theta_j^{\\star}|\\}}, \\quad j \\in \\{1,2,3\\}.\n$$\n- Report a boolean for each test case indicating whether all three coefficients satisfy $\\varepsilon_j \\leq \\text{tol}$ for the specified tolerance and have the same sign as $\\theta_j^{\\star}$.\n\nUnits:\n- Time $t$ must be in seconds. The state $x$ is dimensionless. No angle units are used.\n\nYour program must implement the following test suite. For each tuple, the parameters are $(r, K, A, x_0, T, \\Delta t, \\sigma, \\lambda, \\text{tol}, \\alpha)$, where $x_0$ is the initial condition, $T$ is the total simulation time in seconds, $\\Delta t$ is the sampling interval in seconds, $\\sigma$ is the standard deviation of the additive Gaussian noise on $x$, $\\lambda$ is the threshold for sequentially thresholded least squares, $\\text{tol}$ is the relative error tolerance for acceptance, and $\\alpha$ is the fraction of the number of samples used to choose the smoothing window length (the window length must be odd and adapted to the sample size).\n- Test case 1 (happy path, low noise): $(1.0, 1.0, 0.2, 0.05, 8.0, 0.01, 0.001, 1\\times 10^{-3}, 0.05, 0.07)$.\n- Test case 2 (higher noise): $(0.8, 1.5, 0.4, 0.02, 8.0, 0.01, 0.02, 5\\times 10^{-3}, 0.15, 0.11)$.\n- Test case 3 (sparser sampling and near-threshold initialization): $(1.2, 1.0, 0.8, 0.9, 6.0, 0.05, 0.005, 2\\times 10^{-3}, 0.10, 0.21)$.\n- Test case 4 (boundary challenge, weak attraction): $(0.4, 1.0, 0.95, 0.96, 10.0, 0.02, 0.01, 3\\times 10^{-3}, 0.20, 0.15)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each \"result\" is the boolean acceptance for the corresponding test case computed as described above.",
            "solution": "We derive the identification strategy from a principled birth-death picture where cooperation at low density and crowding at high density combine into a net growth law. Let $x(t)$ be the dimensionless cell density in a well-mixed environment, and let $\\frac{dx}{dt} = f(x)$ be smooth. The mechanism is that the per-capita birth rate initially rises with density due to cooperation but is limited by crowding at high densities, and the per-capita loss rate increases with density due to competition. A commonly used canonical form that captures both effects is a product of three factors: one proportional to the density $x$, one limiting term of form $(1 - x/K)$ capturing carrying capacity $K$, and one cooperative term $(x/A - 1)$ introducing a threshold $A$ (an Allee threshold) below which the net growth becomes negative.\n\nFrom this mechanistic base, we consider the canonical growth law\n$$\n\\frac{dx}{dt} = r\\,x\\,(1 - x/K)\\,(x/A - 1),\n$$\nwhere $r>0$ is an intrinsic rate, $K>0$ is the carrying capacity, and $A>0$ is the cooperation threshold. Although the problem statement did not specify the target formula, the above form is derived from the fundamental assumption that the net growth is the product of density, a crowding limitation, and a cooperation factor, and it is a well-tested model for Allee effects.\n\nTo fit a sparse model using Sparse Identification of Nonlinear Dynamics (SINDy), we construct a candidate function library with the three monomials $\\{x, x^2, x^3\\}$ and seek coefficients $\\boldsymbol{\\theta} = [\\theta_1,\\theta_2,\\theta_3]^\\top$ such that\n$$\n\\frac{dx}{dt} \\approx \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3.\n$$\nExpanding the mechanistic law yields explicit coefficients in this cubic representation. First, expand the product\n$$\n(1 - x/K)\\,(x/A - 1) = \\frac{x}{A} - 1 - \\frac{x^2}{A K} + \\frac{x}{K} = \\left(\\frac{1}{A} + \\frac{1}{K}\\right)x - 1 - \\frac{x^2}{A K}.\n$$\nMultiplying by $r x$ gives\n$$\n\\frac{dx}{dt} = r\\,x\\left[\\left(\\frac{1}{A} + \\frac{1}{K}\\right)x - 1 - \\frac{x^2}{A K}\\right]\n= r\\left[\\left(\\frac{1}{A} + \\frac{1}{K}\\right)x^2 - x - \\frac{1}{A K}x^3\\right].\n$$\nTherefore, the true cubic coefficients are\n$$\n\\theta_1^{\\star} = -r, \\quad \\theta_2^{\\star} = r\\left(\\frac{1}{A} + \\frac{1}{K}\\right), \\quad \\theta_3^{\\star} = -\\frac{r}{A K}.\n$$\n\nAlgorithmic design:\n1. Data generation. For given $(r,K,A,x_0)$, we simulate the deterministic system $\\frac{dx}{dt} = r\\,x\\,(1 - x/K)\\,(x/A - 1)$ from $x(0)=x_0$ over $t \\in [0,T]$ using a high-order integrator with prescribed sampling interval $\\Delta t$. The observed trajectory is contaminated by additive zero-mean Gaussian noise with standard deviation $\\sigma$ applied to $x$, emulating measurement noise. This is realistic in biological measurements where counting errors and fluorescence intensities introduce noise in observed densities.\n\n2. Derivative estimation from noisy data. Direct finite differences amplify noise. A well-tested approach uses polynomial smoothing differentiators such as the Savitzky–Golay filter, which fits a local polynomial of chosen order over a moving window to produce both a smoothed signal and its derivative. We choose an odd window length proportional to the number of samples via a fraction $\\alpha$ and a fixed polynomial order, ensuring window feasibility. This produces robust estimates of $\\frac{dx}{dt}$ and $x$ suitable for regression.\n\n3. Sparse identification with sequential thresholded least squares. Let $\\Theta(x)$ be the design matrix with columns $[x, x^2, x^3]$ built from the smoothed signal. The initial least squares fit\n$$\n\\boldsymbol{\\theta}^{(0)} = \\arg\\min_{\\boldsymbol{\\theta}}\\|\\Theta \\boldsymbol{\\theta} - \\dot{x}\\|_2^2\n$$\nis then sparsified iteratively: for a threshold $\\lambda > 0$, we set entries with $|\\theta_j^{(k)}| < \\lambda$ to zero and refit only the active set. Repeating this procedure yields a parsimonious model that suppresses spurious small coefficients introduced by noise and overfitting.\n\n4. Evaluation against the ground truth. For each test case, we compute the analytical cubic coefficients $\\boldsymbol{\\theta}^{\\star} = [-r,\\; r(1/A + 1/K),\\; -r/(A K)]^\\top$. The identified coefficients are accepted if, for all components $j \\in \\{1,2,3\\}$, the relative error\n$$\n\\varepsilon_j = \\frac{|\\theta_j - \\theta_j^{\\star}|}{\\max\\{10^{-12},|\\theta_j^{\\star}|\\}}\n$$\nis less than or equal to the specified tolerance and the sign of $\\theta_j$ matches the sign of $\\theta_j^{\\star}$. The sign check ensures the inferred model preserves the mechanistic increase/decrease directionality: inhibitory linear term ($\\theta_1^{\\star} < 0$), facilitative quadratic term ($\\theta_2^{\\star} > 0$ from cooperation and crowding combined), and inhibitory cubic term ($\\theta_3^{\\star} < 0$ from crowding saturation).\n\nTest suite coverage:\n- The first case is a nominal scenario with low noise and dense sampling, expecting a straightforward recovery.\n- The second case increases measurement noise, requiring robust smoothing and thresholding; the tolerance is correspondingly relaxed.\n- The third case uses sparser sampling and initializes near the cooperative threshold, emphasizing the importance of proper differentiation and model identifiability over a narrower dynamic range.\n- The fourth case is a boundary challenge with $A$ close to $K$ and an initial condition near $K$, reducing the signal amplitude of the cooperative factor and challenging identifiability; tolerance is relaxed accordingly.\n\nImplementation details:\n- Use a reliable ordinary differential equation solver to generate the clean trajectory.\n- Use a Savitzky–Golay filter for smoothing and derivative estimation with an odd window length $w = \\max\\{w_{\\min}, 2\\lfloor(\\alpha N)/2\\rfloor + 1\\}$ clipped to the data length, where $N$ is the number of samples and $w_{\\min}$ ensures polynomial feasibility.\n- Implement sequential thresholded least squares with a fixed maximum iteration count and convergence by stability of the active set.\n- Ensure reproducibility by fixing the random number generator seed.\n\nThe final program computes the acceptance booleans for the four test cases and prints them in the specified single-line format.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.signal import savgol_filter\n\n# Seed the random number generator for reproducibility\nrng = np.random.default_rng(42)\n\ndef logistic_allee_rhs(t, x, r, K, A):\n    # x may be array; ensure element-wise computation\n    return r * x * (1.0 - x / K) * (x / A - 1.0)\n\ndef simulate_time_series(r, K, A, x0, T, dt):\n    # Solve the ODE dx/dt = r x (1 - x/K)(x/A - 1) from t=0 to t=T\n    t_eval = np.arange(0.0, T + 1e-12, dt)\n    sol = solve_ivp(fun=lambda t, y: logistic_allee_rhs(t, y, r, K, A),\n                    t_span=(0.0, T),\n                    y0=[x0],\n                    t_eval=t_eval,\n                    method='RK45',\n                    rtol=1e-8,\n                    atol=1e-10)\n    t = sol.t\n    x = sol.y[0]\n    return t, x\n\ndef add_noise(x, sigma):\n    if sigma <= 0.0:\n        return x.copy()\n    noise = rng.normal(loc=0.0, scale=sigma, size=x.shape)\n    return x + noise\n\ndef choose_window_length(n, alpha, polyorder):\n    # Compute an odd window length proportional to sample size,\n    # clipped to be feasible for Savitzky-Golay\n    # Start from alpha*n rounded to nearest odd\n    wlen = int(max(5, int(n * alpha)))\n    # Enforce odd\n    if wlen % 2 == 0:\n        wlen += 1\n    # Minimum window for polyorder\n    min_wlen = polyorder + 3\n    if min_wlen % 2 == 0:\n        min_wlen += 1\n    if wlen < min_wlen:\n        wlen = min_wlen\n    # Clip to data length - must be <= n\n    if wlen > n:\n        wlen = n - 1 if (n - 1) % 2 == 1 else n - 2\n    # Final guard: ensure odd and at least polyorder+2\n    if wlen % 2 == 0:\n        wlen -= 1\n    if wlen < polyorder + 2:\n        wlen = polyorder + 3\n        if wlen % 2 == 0:\n            wlen += 1\n        if wlen > n:\n            wlen = n - 1 if (n - 1) % 2 == 1 else n - 2\n    return max(5, wlen)\n\ndef estimate_derivative(y, dt, alpha, polyorder=3):\n    n = len(y)\n    wlen = choose_window_length(n, alpha, polyorder)\n    # Smooth signal\n    y_smooth = savgol_filter(y, window_length=wlen, polyorder=polyorder, deriv=0)\n    # Derivative\n    dy_dt = savgol_filter(y, window_length=wlen, polyorder=polyorder, deriv=1, delta=dt)\n    return y_smooth, dy_dt\n\ndef stlsq(Theta, ydot, threshold, max_iter=10, rcond=None):\n    # Sequential thresholded least squares\n    # Initial least squares\n    coef, *_ = np.linalg.lstsq(Theta, ydot, rcond=rcond)\n    active = np.ones_like(coef, dtype=bool)\n    for _ in range(max_iter):\n        # Threshold small coefficients\n        new_active = np.abs(coef) >= threshold\n        # If active set hasn't changed, break\n        if np.array_equal(new_active, active):\n            break\n        active = new_active\n        if not np.any(active):\n            coef[:] = 0.0\n            break\n        # Refit on active set\n        Theta_active = Theta[:, active]\n        coef_active, *_ = np.linalg.lstsq(Theta_active, ydot, rcond=rcond)\n        # Update coefficients\n        coef = np.zeros_like(coef)\n        coef[active] = coef_active\n    return coef\n\ndef true_coefficients(r, K, A):\n    # From expansion: dx/dt = (-r) x + r(1/A + 1/K) x^2 + ( - r/(A K) ) x^3\n    return np.array([-r, r * (1.0 / A + 1.0 / K), -r / (A * K)], dtype=float)\n\ndef run_case(params):\n    r, K, A, x0, T, dt, sigma, lam, tol, alpha = params\n    # Simulate clean dynamics\n    t, x_clean = simulate_time_series(r, K, A, x0, T, dt)\n    # Add measurement noise\n    x_noisy = add_noise(x_clean, sigma)\n    # Estimate derivative from noisy data\n    x_smooth, dxdt_est = estimate_derivative(x_noisy, dt, alpha=alpha, polyorder=3)\n    # Build library Theta = [x, x^2, x^3]\n    Theta = np.column_stack([x_smooth, x_smooth**2, x_smooth**3])\n    # Fit sparse model\n    theta_hat = stlsq(Theta, dxdt_est, threshold=lam, max_iter=10, rcond=None)\n    # Ground-truth coefficients\n    theta_true = true_coefficients(r, K, A)\n    # Relative errors and sign checks\n    denom = np.maximum(1e-12, np.abs(theta_true))\n    rel_err = np.abs(theta_hat - theta_true) / denom\n    signs_match = np.all(np.sign(theta_hat) == np.sign(theta_true))\n    accepted = bool(np.all(rel_err <= tol) and signs_match)\n    return accepted\n\ndef solve():\n    # Define the test cases from the problem statement:\n    # (r, K, A, x0, T, dt, sigma, lambda, tol, alpha)\n    test_cases = [\n        (1.0, 1.0, 0.2, 0.05, 8.0, 0.01, 0.001, 1e-3, 0.05, 0.07),\n        (0.8, 1.5, 0.4, 0.02, 8.0, 0.01, 0.02, 5e-3, 0.15, 0.11),\n        (1.2, 1.0, 0.8, 0.9, 6.0, 0.05, 0.005, 2e-3, 0.10, 0.21),\n        (0.4, 1.0, 0.95, 0.96, 10.0, 0.02, 0.01, 3e-3, 0.20, 0.15),\n    ]\n    results = []\n    for case in test_cases:\n        res = run_case(case)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having identified a model, a critical next question is whether it is unique. This exercise explores the concept of observational equivalence, where structurally different models can produce indistinguishable outputs under certain experimental conditions . You will analyze two distinct models and design numerical experiments that can break this equivalence, highlighting the crucial role of targeted data acquisition in disambiguating mechanistic hypotheses.",
            "id": "3349463",
            "problem": "Consider a single measured molecular species with concentration denoted by $y(t)$, influenced by an external inducer input $u(t)$, where $t$ is time in seconds. Assume the dynamics are governed by deterministic ordinary differential equations (ODEs) derived from net production minus degradation, a standard postulate in chemical kinetics and gene regulation: the rate of change of $y$ equals the sum of production terms minus degradation terms. Under the framework of Sparse Identification of Nonlinear Dynamics (SINDy), the candidate library of functions for $f(y,u)$ contains sparse combinations of low-order polynomial and interaction terms. Two models are non-isomorphic if the supports (the sets of active library terms with nonzero coefficients) differ.\n\nYou must construct and analyze two sparse models, each expressed as an ODE for $y(t)$ using the same candidate library of functions, but with different supports, and show that they are observationally equivalent under a specified experimental protocol. Then you must simulate proposed changes to the protocol that break the observational equivalence. Observational equivalence means that the measured trajectories $y_A(t)$ and $y_B(t)$ for models $A$ and $B$ are indistinguishable within a specified tolerance over the experimental window.\n\nUse the following mathematically specified setup:\n\n- Candidate library $\\Theta(y,u)$ contains the terms $\\{y, y^3, u, y\\,u\\}$.\n- Define Model $A$ with the sparse right-hand side\n$$\\frac{dy}{dt} = -\\delta\\,y + \\alpha\\,u + \\beta\\,y^3,$$\nand Model $B$ with the sparse right-hand side\n$$\\frac{dy}{dt} = -\\delta\\,y + \\alpha\\,u + \\gamma\\,y\\,u.$$\n- Parameters are fixed to $ \\delta = 0.5 $ (per second), $ \\alpha = 1.0 $ (arbitrary units per second), $ \\beta = 0.1 $ (per second per $(\\text{arbitrary units})^2$), and $ \\gamma = 0.2 $ (per second). The input $u(t)$ is dimensionless. Concentration $y$ is in arbitrary units (a.u.). Time is in seconds. For sinusoidal input, the angular frequency $ \\omega $ must be specified in radians per second.\n- Experimental protocol $\\mathcal{P}_0$ (baseline equivalence): $y(0)=0$ and $u(t)=0$ for all $t$ in $[0,T]$, where $T=10$ seconds.\n- Define the observational equivalence tolerance $ \\varepsilon = 10^{-12} $ (in a.u.).\n\nYour task is to write a complete, runnable program that numerically simulates both models under the following five protocols (test suite), computes the root-mean-square error (RMSE) between the trajectories $y_A(t)$ and $y_B(t)$ over the time window, and aggregates the results:\n\n1. Protocol $\\mathcal{P}_0$: $y(0)=0$, $u(t)=0$, $t \\in [0, T]$ with $T=10$ seconds. Output a boolean indicating whether the RMSE is less than or equal to $ \\varepsilon $ (i.e., observational equivalence holds).\n2. Protocol $\\mathcal{P}_1$: $y(0)=10^{-3}$, $u(t)=0$, $t \\in [0, T]$ with $T=10$ seconds. Output the RMSE as a float in a.u.\n3. Protocol $\\mathcal{P}_2$: $y(0)=0$, $u(t)=10^{-3}$ (constant), $t \\in [0, T]$ with $T=10$ seconds. Output the RMSE as a float in a.u.\n4. Protocol $\\mathcal{P}_3$: $y(0)=0.5$, $u(t)=0$, $t \\in [0, T]$ with $T=10$ seconds. Output the RMSE as a float in a.u.\n5. Protocol $\\mathcal{P}_4$: $y(0)=0.1$, $u(t)=\\sin(\\omega t)$ with $ \\omega = 1 $ radians per second, $t \\in [0, T]$ with $T=10$ seconds. Output the RMSE as a float in a.u.\n\nNumerical simulation requirements:\n- Solve each initial value problem on a uniform time grid of $N=1001$ points over $[0, T]$.\n- Use a high-accuracy ODE integrator appropriate for smooth right-hand sides.\n- Compute RMSE using\n$$\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^{N}\\left(y_A(t_k) - y_B(t_k)\\right)^2},$$\nexpressed in a.u.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the five protocols above, with the first element a boolean and the remaining four as floats (e.g., $[True, 0.00123, 0.0456, 0.78, 0.12]$). All numerical outputs must be in arbitrary units (a.u.) for $y$-based quantities and seconds for time. The sinusoidal input uses radians per second for $ \\omega $.",
            "solution": "The problem is valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent setup for a numerical experiment in computational systems biology. The task is to analyze the observational equivalence of two distinct nonlinear dynamical models under various experimental protocols, a core concept in system identification and model validation.\n\nThe solution proceeds by first analyzing the conditions for observational equivalence and then implementing a numerical simulation to quantify the divergence between the models under protocols designed to break this equivalence.\n\nThe system under consideration is a single molecular species with concentration $y(t)$ influenced by an external input $u(t)$. The dynamics are described by an ordinary differential equation (ODE) of the form $\\frac{dy}{dt} = f(y, u)$.\n\nThe two proposed models, Model A and Model B, are derived from a candidate library $\\Theta(y,u) = \\{y, y^3, u, y\\,u\\}$ within the Sparse Identification of Nonlinear Dynamics (SINDy) framework. Their governing equations are:\n- **Model A**:\n$$\n\\frac{dy_A}{dt} = -\\delta\\,y_A + \\alpha\\,u + \\beta\\,y_A^3\n$$\n- **Model B**:\n$$\n\\frac{dy_B}{dt} = -\\delta\\,y_B + \\alpha\\,u + \\gamma\\,y_B\\,u\n$$\n\nThe models share linear terms ($-\\delta y$ and $\\alpha u$) but differ in their nonlinear terms: Model A possesses a cubic self-interaction term ($\\beta y_A^3$), while Model B has a bilinear interaction term with the input ($\\gamma y_B u$). The parameters are given as $\\delta = 0.5\\,\\text{s}^{-1}$, $\\alpha = 1.0\\,\\text{a.u.}\\cdot\\text{s}^{-1}$, $\\beta = 0.1\\,\\text{s}^{-1}\\cdot\\text{a.u.}^{-2}$, and $\\gamma = 0.2\\,\\text{s}^{-1}$.\n\nThe core of the problem is to investigate five experimental protocols ($\\mathcal{P}_0$ through $\\mathcal{P}_4$) and determine whether the resulting trajectories, $y_A(t)$ and $y_B(t)$, are observationally equivalent.\n\n**Protocol $\\mathcal{P}_0$: Baseline Equivalence**\nThis protocol specifies the initial condition $y(0) = 0$ and a null input $u(t) = 0$ for all time $t \\in [0, T]$, where $T=10$ seconds.\nFor Model A, the ODE becomes:\n$$\n\\frac{dy_A}{dt} = -\\delta\\,y_A + \\alpha(0) + \\beta\\,y_A^3 = -0.5\\,y_A + 0.1\\,y_A^3\n$$\nWith the initial condition $y_A(0) = 0$, it is clear that $y_A(t) = 0$ is a fixed point. Since the right-hand side is locally Lipschitz, the solution to this initial value problem is unique. Therefore, $y_A(t) = 0$ for all $t \\geq 0$.\nFor Model B, the ODE becomes:\n$$\n\\frac{dy_B}{dt} = -\\delta\\,y_B + \\alpha(0) + \\gamma\\,y_B(0) = -0.5\\,y_B\n$$\nWith the initial condition $y_B(0) = 0$, the unique solution is similarly $y_B(t) = 0$ for all $t \\geq 0$.\nSince $y_A(t) = y_B(t) = 0$ for the entire time course, the root-mean-square error (RMSE) is:\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^{N}\\left(0 - 0\\right)^2} = 0\n$$\nThis value is less than the specified tolerance $\\varepsilon = 10^{-12}$, so the models are observationally equivalent under protocol $\\mathcal{P}_0$. This demonstrates a critical challenge in system identification: an insufficiently rich experimental protocol may fail to distinguish between structurally different models.\n\n**Protocols $\\mathcal{P}_1$ - $\\mathcal{P}_4$: Breaking Equivalence**\nThese protocols are designed to activate the distinct nonlinear terms and thus break the equivalence. A numerical approach is required to solve the nonlinear ODEs and quantify the difference between their trajectories.\n\nThe numerical simulation will be performed using a high-accuracy method. The `scipy.integrate.solve_ivp` function, which implements methods like the explicit Runge-Kutta method of order $5(4)$ (`RK45`), is well-suited for these smooth, non-stiff ODEs. We will solve each initial value problem over a uniform time grid of $N=1001$ points from $t=0$ to $t=10$ seconds.\n\n- **For Protocol $\\mathcal{P}_1$ ($y(0)=10^{-3}, u(t)=0$):** The input $u(t)$ is zero, so the ODEs simplify to $\\frac{dy_A}{dt} = -0.5\\,y_A + 0.1\\,y_A^3$ and $\\frac{dy_B}{dt} = -0.5\\,y_B$. The presence of the $\\beta y_A^3$ term in Model A, which is absent in Model B, will cause their trajectories to diverge from the shared non-zero initial condition.\n- **For Protocol $\\mathcal{P}_2$ ($y(0)=0, u(t)=10^{-3}$):** Both models are driven by the constant input $u(t) = 10^{-3}$. At $t=0$, both $\\frac{dy}{dt}$ are equal to $\\alpha u = 1.0 \\times 10^{-3}$. However, as $y(t)$ becomes non-zero, the nonlinear terms $\\beta y_A^3$ and $\\gamma y_B u$ will activate. Since $u$ is constant, the term in Model B ($\\gamma y_B u$) will grow linearly with $y_B$, while the term in Model A ($\\beta y_A^3$) will grow cubically with $y_A$. This difference in functional form will lead to diverging trajectories.\n- **For Protocol $\\mathcal{P}_3$ ($y(0)=0.5, u(t)=0$):** This is similar to $\\mathcal{P}_1$ but with a much larger initial condition. The contribution of the cubic term $\\beta y_A^3$ is highly sensitive to the magnitude of $y_A$. A larger $y_A(0)$ will result in a much larger initial effect from this term, leading to a significantly greater RMSE compared to $\\mathcal{P}_1$.\n- **For Protocol $\\mathcal{P}_4$ ($y(0)=0.1, u(t)=\\sin(\\omega t)$ with $\\omega=1$):** This protocol involves both a non-zero initial condition and a time-varying input. Both differing nonlinear terms, $\\beta y_A^3$ and $\\gamma y_B u$, will be active and will vary over time. The sinusoidal input provides a rich excitation signal that will persistently probe the different dynamic responses of the two models, revealing a substantial difference.\n\nThe subsequent program implements this strategy. It defines functions for the right-hand sides of both model ODEs. It then sequentially executes each of the five protocols, calling `solve_ivp` for each model, computing the RMSE between the resulting trajectories $y_A(t_k)$ and $y_B(t_k)$, and formatting the results as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Simulates two SINDy-identified models under five experimental protocols\n    to test for observational equivalence and computes the RMSE between their\n    trajectories.\n    \"\"\"\n\n    # Define model parameters\n    delta = 0.5  # Degradation rate (per second)\n    alpha = 1.0  # Input-driven production rate (a.u. per second)\n    beta = 0.1   # Cubic self-interaction rate (per second per a.u.^2)\n    gamma = 0.2  # Bilinear interaction rate (per second)\n\n    # General simulation settings\n    sim_time = 10.0  # Total simulation time in seconds\n    num_points = 1001  # Number of points in the time grid\n    equivalence_tolerance = 1e-12  # Tolerance for observational equivalence\n\n    # Define the right-hand side (RHS) of the ODE for Model A\n    def model_A_rhs(t, y, u_func):\n        \"\"\"RHS for dy/dt = -delta*y + alpha*u + beta*y^3.\"\"\"\n        y_val = y[0]\n        u_val = u_func(t)\n        return [-delta * y_val + alpha * u_val + beta * y_val**3]\n\n    # Define the RHS of the ODE for Model B\n    def model_B_rhs(t, y, u_func):\n        \"\"\"RHS for dy/dt = -delta*y + alpha*u + gamma*y*u.\"\"\"\n        y_val = y[0]\n        u_val = u_func(t)\n        return [-delta * y_val + alpha * u_val + gamma * y_val * u_val]\n\n    # Define the five experimental protocols (test suite)\n    protocols = [\n        {'id': 'P0', 'y0': [0.0], 'u_func': lambda t: 0.0},\n        {'id': 'P1', 'y0': [1e-3], 'u_func': lambda t: 0.0},\n        {'id': 'P2', 'y0': [0.0], 'u_func': lambda t: 1e-3},\n        {'id': 'P3', 'y0': [0.5], 'u_func': lambda t: 0.0},\n        {'id': 'P4', 'y0': [0.1], 'u_func': lambda t: np.sin(1.0 * t)},\n    ]\n\n    results = []\n    \n    # Common time span and evaluation points for all protocols\n    t_span = [0, sim_time]\n    t_eval = np.linspace(0, sim_time, num_points)\n\n    # High-accuracy solver options\n    solver_options = {'method': 'RK45', 'rtol': 1e-8, 'atol': 1e-8}\n\n    for i, p in enumerate(protocols):\n        y0 = p['y0']\n        u_func = p['u_func']\n\n        # Solve for Model A\n        sol_A = solve_ivp(\n            model_A_rhs, t_span, y0, args=(u_func,), t_eval=t_eval, **solver_options\n        )\n        # Flatten the output array for 1D analysis\n        y_A = sol_A.y.flatten()\n\n        # Solve for Model B\n        sol_B = solve_ivp(\n            model_B_rhs, t_span, y0, args=(u_func,), t_eval=t_eval, **solver_options\n        )\n        # Flatten the output array for 1D analysis\n        y_B = sol_B.y.flatten()\n\n        # Ensure solvers were successful and returned arrays of expected shape\n        if y_A.shape[0] != num_points or y_B.shape[0] != num_points:\n            raise RuntimeError(f\"ODE solver failed for protocol {p['id']}.\")\n\n        # Compute Root-Mean-Square Error (RMSE)\n        rmse = np.sqrt(np.mean((y_A - y_B)**2))\n\n        # For the first protocol, check for observational equivalence\n        if i == 0:\n            results.append(rmse <= equivalence_tolerance)\n        else:\n            results.append(rmse)\n\n    # Format the results into the required single-line string output\n    formatted_results = []\n    for r in results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Format floats to a reasonable precision for output consistency\n            formatted_results.append(f\"{r:.15g}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Real-world biological data are often noisy, sparse, and irregularly sampled, posing a major challenge to methods that require numerical differentiation. This advanced practice introduces the weak formulation of SINDy, a powerful technique that overcomes this limitation by recasting the derivative in an integral form . You will apply this method to identify the dynamics of a phosphorylation cascade from imperfect data, learning how to robustly discover governing equations without directly differentiating a noisy signal.",
            "id": "3349347",
            "problem": "You will implement a complete, runnable program that performs weak-form Sparse Identification of Nonlinear Dynamics (SINDy) to recover a phosphorylation cascade from irregularly sampled, noisy proteomics time series with missing time points. The program must use the weak formulation to estimate the time derivative via integration by parts, and then perform sparse regression to identify the governing nonlinear dynamics. Your implementation must be entirely self-contained and produce outputs for a specified test suite.\n\nThe fundamental base must be the following, expressed in terms of widely accepted principles used in computational systems biology:\n\n- Mass-action kinetics and conservation arguments for phosphorylation cascades, where an upstream kinase promotes phosphorylation of a downstream substrate and phosphatases promote dephosphorylation. A state variable represents the phosphorylated fraction, which is dimensionless and lies in the interval $[0,1]$.\n- The chain of events in a phosphorylation cascade can be modeled as ordinary differential equations (ODEs) that incorporate activation and deactivation, with nonlinear terms arising from interactions such as an upstream phosphorylated species activating the downstream substrate. The general form is consistent with mass-action kinetics and saturation arising from fractional occupancy.\n- The weak formulation for time derivatives obtained by selecting compactly supported, sufficiently smooth test functions and integrating by parts to avoid direct numerical differentiation under noise and irregular sampling.\n\nYou will generate synthetic ground-truth data using a three-tier phosphorylation cascade governed by the following ODEs, which encode activation and deactivation consistent with mass-action kinetics under a constant stimulus $S$:\n\n$$\n\\frac{d x_1}{dt} = k_1 S - (k_1 S + k_2) x_1\n$$,\n\n$$\n\\frac{d x_2}{dt} = k_3 x_1 - k_3 x_1 x_2 - k_4 x_2\n$$,\n\n$$\n\\frac{d x_3}{dt} = k_5 x_2 - k_5 x_2 x_3 - k_6 x_3\n$$,\n\nwhere $x_1$, $x_2$, and $x_3$ are the phosphorylated fractions (dimensionless), $S$ is a constant stimulus, and $k_1, k_2, k_3, k_4, k_5, k_6$ are positive rate constants. Take $S = 1$, $k_1 = 1.0$, $k_2 = 0.5$, $k_3 = 2.0$, $k_4 = 0.3$, $k_5 = 1.5$, $k_6 = 0.2$, with initial condition $\\mathbf{x}(0) = [0,0,0]^\\top$ and final time $T = 10$ seconds. Time is in seconds and concentrations are dimensionless fractions. These ODEs are used only to generate data; your identification must assume a general sparse form over a candidate library.\n\nYour candidate nonlinear library must include the following functions of the state vector $\\mathbf{x}(t) = [x_1(t), x_2(t), x_3(t)]^\\top$:\n\n$$\n\\Theta(\\mathbf{x}) = \\left[1,\\; x_1,\\; x_2,\\; x_3,\\; x_1 x_2,\\; x_1 x_3,\\; x_2 x_3\\right]\n$$.\n\nThe sparse regression aims to find coefficient vectors $\\boldsymbol{\\Xi}_j$ for $j \\in \\{1,2,3\\}$ such that the weak-form estimated time derivatives satisfy:\n\n$$\n\\left\\langle \\frac{d x_j}{dt}, \\psi_m \\right\\rangle \\approx \\left\\langle \\Theta(\\mathbf{x}), \\psi_m \\right\\rangle \\boldsymbol{\\Xi}_j\n$$,\n\nfor a collection of compactly supported test functions $\\psi_m(t)$ indexed by window $m$, where $\\langle f, g \\rangle = \\int f(t) g(t) \\, dt$. You must not use pointwise numerical differentiation of $x_j(t)$. Instead, apply integration by parts to estimate weak-form derivatives:\n\n$$\n\\int \\psi_m(t) \\frac{d x_j(t)}{dt} \\, dt = - \\int \\frac{d \\psi_m(t)}{dt} x_j(t) \\, dt\n$$,\n\nassuming that $\\psi_m(t)$ vanishes at the boundaries of its support, so the boundary term is zero. You must compute these integrals using the trapezoidal rule over the irregularly sampled time points.\n\nFor the test functions, use compactly supported cosine bells centered at $c_m$ with half-width $h$:\n\n$$\n\\psi_m(t) = \n\\begin{cases}\n\\frac{1}{2}\\left(1 + \\cos\\left(\\pi \\frac{t - c_m}{h}\\right)\\right), & \\text{if } |t - c_m| \\le h,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\n\n$$\n\\frac{d \\psi_m}{dt}(t) = \n\\begin{cases}\n- \\frac{\\pi}{2 h}\\sin\\left(\\pi \\frac{t - c_m}{h}\\right), & \\text{if } |t - c_m| \\le h,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\n\nConstruct a set of window centers $\\{c_m\\}$ evenly spaced over the interval $[h, T - h]$ to define a test function family. For each window, reject it if its effective support mass $\\int \\psi_m(t) \\, dt$ estimated by the trapezoidal rule is less than a fraction of $h$, to avoid degenerate windows with too few points due to missing data.\n\nTo form the regression, assemble for each window $m$:\n\n- The weak-form derivative estimate for each state $j$:\n$$\ny_{j,m} = - \\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt\n$$.\n\n- The weak-form feature vector:\n$$\n(\\Theta_m)_k = \\int \\psi_m(t) \\, \\phi_k(\\mathbf{x}(t)) \\, dt\n$$,\n\nwhere $\\phi_k$ denotes the $k^\\text{th}$ function in the candidate library $\\Theta(\\mathbf{x})$.\n\nUse Sequential Thresholded Least Squares (STLSQ) to enforce sparsity:\n\n- Solve a least squares problem for each $j$ to estimate $\\boldsymbol{\\Xi}_j$.\n- Threshold all coefficients with magnitude below a specified threshold $\\lambda$ to zero.\n- Refit on the remaining support.\n- Iterate until convergence or a fixed number of iterations.\n\nSampling and data generation must follow these rules:\n\n- Solve the ODEs with dense output to allow evaluation at arbitrary times.\n- Draw $N = 300$ irregular sampling times uniformly at random over $[0, T]$, then sort them in ascending order. Apply a missing fraction by removing a specified proportion of randomly selected time points to emulate dropout.\n- Add independent, zero-mean Gaussian measurement noise with specified standard deviation $\\sigma$ to each observed state, and clip the noisy states to the interval $[0,1]$ to reflect fractional phosphorylation.\n- Do not use any knowledge of the true coefficients during identification; they are only for evaluation.\n\nEvaluation and outputs:\n\n- Define the true supports for the three equations under the chosen library indexing $[1, x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3]$:\n\n$$\n\\text{Supp}(x_1) = \\{0, 1\\}, \\quad \\text{Supp}(x_2) = \\{1, 2, 4\\}, \\quad \\text{Supp}(x_3) = \\{2, 3, 6\\}\n$$.\n\n- After identification, determine the estimated supports as the indices of nonzero coefficients (after thresholding). For each test case, output a boolean that is $\\text{True}$ if and only if the estimated supports match the true supports exactly for all three equations; otherwise output $\\text{False}$.\n\nTest suite:\n\nProvide four test cases, each defined by $($random seed$, \\sigma, \\text{missing fraction}, h, \\lambda)$:\n\n1. (1, 0.02, 0.20, 0.60, 0.06).\n2. (2, 0.00, 0.35, 0.60, 0.05).\n3. (3, 0.08, 0.40, 0.80, 0.08).\n4. (4, 0.05, 0.50, 1.00, 0.10).\n\nAll physical times are in seconds, concentrations are dimensionless fractions, and all outputs are unitless booleans.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[$result1,result2,result3,result4$]$).",
            "solution": "The problem requires the implementation of the weak-form Sparse Identification of Nonlinear Dynamics (SINDy) algorithm to discover the governing ordinary differential equations (ODEs) of a three-tier phosphorylation cascade from noisy, incomplete, and irregularly sampled time-series data. The solution proceeds by first generating synthetic data according to specified dynamics and corruption rules. Subsequently, it applies the weak-form SINDy procedure, which avoids direct numerical differentiation by using integration by parts with smooth, compactly supported test functions. The resulting linear system is solved for sparse coefficient vectors using the Sequential Thresholded Least Squares (STLSQ) algorithm. Finally, the identified model structures are compared against the known true structures for a suite of test cases.\n\nThe methodological pipeline is constructed as follows:\n\n1.  **Synthetic Data Generation**: The foundation of this problem is a ground-truth model of a phosphorylation cascade. The dynamics are given by a system of three coupled ODEs for the phosphorylated fractions $x_1(t)$, $x_2(t)$, and $x_3(t)$:\n    $$\n    \\frac{d x_1}{dt} = k_1 S - (k_1 S + k_2) x_1\n    $$\n    $$\n    \\frac{d x_2}{dt} = k_3 x_1 - k_3 x_1 x_2 - k_4 x_2\n    $$\n    $$\n    \\frac{d x_3}{dt} = k_5 x_2 - k_5 x_2 x_3 - k_6 x_3\n    $$\n    The parameters are set to $S = 1$, $k_1 = 1.0$, $k_2 = 0.5$, $k_3 = 2.0$, $k_4 = 0.3$, $k_5 = 1.5$, and $k_6 = 0.2$. The system is integrated over the time interval $[0, T]$ where $T = 10$ seconds, starting from the initial condition $\\mathbf{x}(0) = [0, 0, 0]^\\top$. A high-fidelity numerical solution is obtained using a standard ODE solver with dense output enabled, which provides an interpolant for the true trajectory.\n    From this true solution, measurement data are simulated. First, $N = 300$ time points are drawn uniformly at random from $[0, T]$ and sorted. A specified fraction of these points are then randomly removed to simulate data dropout. At the remaining time points, the true state values are corrupted by adding independent, zero-mean Gaussian noise with a specified standard deviation $\\sigma$. As the state variables represent physical fractions, the resulting noisy values are clipped to the interval $[0, 1]$.\n\n2.  **Weak-Form SINDy Formulation**: The core of the identification task is to find a sparse coefficient vector $\\boldsymbol{\\Xi}_j$ for each state variable $x_j$ such that the dynamics are well-approximated by a linear combination of candidate functions from a library $\\Theta(\\mathbf{x})$. The governing equation for each state is assumed to have the form:\n    $$\n    \\frac{d x_j}{dt} = \\Theta(\\mathbf{x}(t)) \\boldsymbol{\\Xi}_j\n    $$\n    The specified candidate library is $\\Theta(\\mathbf{x}) = [1, x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3]$.\n    Instead of differentiating the noisy data $x_j(t)$ directly, the weak formulation is employed. Both sides of the equation are multiplied by a smooth, compactly supported test function $\\psi_m(t)$ and integrated over the time domain.\n    $$\n    \\left\\langle \\frac{d x_j}{dt}, \\psi_m \\right\\rangle = \\left\\langle \\Theta(\\mathbf{x}), \\psi_m \\right\\rangle \\boldsymbol{\\Xi}_j\n    $$\n    where $\\langle f, g \\rangle = \\int f(t) g(t) \\, dt$. The crucial step is applying integration by parts to the left-hand side term:\n    $$\n    \\int \\frac{d x_j}{dt} \\psi_m(t) \\, dt = \\left[x_j(t) \\psi_m(t)\\right]_{\\partial \\Omega} - \\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt\n    $$\n    Since the test function $\\psi_m(t)$ has compact support, it vanishes at the boundaries of the integration domain, making the boundary term zero. This transfers the derivative from the noisy data $x_j$ to the smooth, analytically known test function $\\psi_m$:\n    $$\n    -\\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt \\approx \\left(\\int \\Theta(\\mathbf{x}(t)) \\psi_m(t) \\, dt \\right) \\boldsymbol{\\Xi}_j\n    $$\n    The test functions are chosen to be cosine bells centered at $c_m$ with half-width $h$:\n    $$\n    \\psi_m(t) = \\begin{cases} \\frac{1}{2}\\left(1 + \\cos\\left(\\pi \\frac{t - c_m}{h}\\right)\\right), & \\text{if } |t - c_m| \\le h \\\\ 0, & \\text{otherwise} \\end{cases}\n    $$\n    Their derivatives are analytically computed as:\n    $$\n    \\frac{d \\psi_m}{dt}(t) = \\begin{cases} - \\frac{\\pi}{2 h}\\sin\\left(\\pi \\frac{t - c_m}{h}\\right), & \\text{if } |t - c_m| \\le h \\\\ 0, & \\text{otherwise} \\end{cases}\n    $$\n    An ensemble of these test functions is created by selecting centers $c_m$ evenly spaced across the interval $[h, T-h]$.\n\n3.  **Construction of the Linear System**: For each test function $\\psi_m$ in the ensemble, a single row in a linear system is constructed.\n    -   The weak derivative term for state $j$ is calculated as $y_{j,m} = - \\int x_j(t) \\frac{d \\psi_m}{dt}(t) \\, dt$.\n    -   The weak feature vector is formed with components $(\\boldsymbol{\\theta}_w)_k = \\int \\phi_k(\\mathbf{x}(t)) \\psi_m(t) \\, dt$, where $\\phi_k$ is the $k$-th function in the library $\\Theta(\\mathbf{x})$.\n    All integrals are computed numerically using the trapezoidal rule, which is suitable for the irregular time grid. To ensure robustness against data scarcity, any window $m$ is discarded if the effective mass of its test function, $\\int \\psi_m(t) \\, dt$, falls below a threshold (chosen as $0.1 h$).\n    By stacking the results from all valid windows, we form a linear system for each state $j \\in \\{1, 2, 3\\}$:\n    $$\n    \\mathbf{\\Theta}_w \\boldsymbol{\\Xi}_j \\approx \\mathbf{y}_j\n    $$\n    where $\\mathbf{\\Theta}_w$ is the matrix of weak features and $\\mathbf{y}_j$ is the vector of weak derivatives.\n\n4.  **Sparse Regression**: The Sequential Thresholded Least Squares (STLSQ) algorithm is used to find a sparse solution $\\boldsymbol{\\Xi}_j$ for each state. This iterative procedure is as follows:\n    -   Initialize the set of active coefficients (the support) to include all candidate functions.\n    -   Iteratively perform the following steps:\n        a.  Solve a standard least-squares problem for the coefficients on the current support: $\\boldsymbol{\\Xi}_j^{\\text{sub}} = (\\mathbf{\\Theta}_w^{\\text{sub}})^\\dagger \\mathbf{y}_j$.\n        b.  Identify coefficients in $\\boldsymbol{\\Xi}_j^{\\text{sub}}$ whose magnitudes are below a given threshold $\\lambda$.\n        c.  If no such coefficients exist, the algorithm has converged.\n        d.  Otherwise, remove the corresponding functions from the support and repeat the process.\n    -   This is repeated for a fixed number of iterations or until the support stabilizes. The final non-zero elements of $\\boldsymbol{\\Xi}_j$ constitute the identified model structure for $\\dot{x}_j$.\n\n5.  **Evaluation**: The performance is evaluated by comparing the identified model structure against the true structure. The true supports for the given library $\\Theta(\\mathbf{x})$ are known from the governing equations:\n    -   $dx_1/dt$ depends on $\\{1, x_1\\}$, corresponding to indices $\\{0, 1\\}$.\n    -   $dx_2/dt$ depends on $\\{x_1, x_2, x_1 x_2\\}$, corresponding to indices $\\{1, 2, 4\\}$.\n    -   $dx_3/dt$ depends on $\\{x_2, x_3, x_2 x_3\\}$, corresponding to indices $\\{2, 3, 6\\}$.\n    For each test case, the identified support for each equation (the set of indices of non-zero coefficients in the final $\\boldsymbol{\\Xi}_j$) is computed. A test case yields a result of `True` if and only if the identified supports for all three equations ($x_1$, $x_2$, and $x_3$) exactly match their respective true supports. Otherwise, the result is `False`.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the SINDy identification for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # (random_seed, sigma, missing_fraction, h, lambda_threshold)\n        (1, 0.02, 0.20, 0.60, 0.06),\n        (2, 0.00, 0.35, 0.60, 0.05),\n        (3, 0.08, 0.40, 0.80, 0.08),\n        (4, 0.05, 0.50, 1.00, 0.10),\n    ]\n\n    results = []\n    for seed, sigma, missing_frac, h, lambda_thresh in test_cases:\n        result = run_sindy_case(seed, sigma, missing_frac, h, lambda_thresh)\n        results.append(result)\n    \n    # Format the final output string exactly as required.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_sindy_case(seed, sigma, missing_frac, h, lambda_thresh):\n    \"\"\"\n    Executes the full pipeline for a single test case: data generation,\n    model identification, and evaluation.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Data Generation\n    t_meas, x_meas = generate_data(sigma, missing_frac)\n\n    # 2. Model Identification\n    identified_supports = identify_model(t_meas, x_meas, h, lambda_thresh)\n\n    # 3. Evaluation\n    true_supports = [{0, 1}, {1, 2, 4}, {2, 3, 6}]\n    \n    is_correct = all(\n        ident == true for ident, true in zip(identified_supports, true_supports)\n    )\n    \n    return is_correct\n\ndef ode_system(t, x):\n    \"\"\"Defines the ODEs for the phosphorylation cascade.\"\"\"\n    S = 1.0\n    k1, k2, k3, k4, k5, k6 = 1.0, 0.5, 2.0, 0.3, 1.5, 0.2\n    \n    dx1_dt = k1 * S - (k1 * S + k2) * x[0]\n    dx2_dt = k3 * x[0] - k3 * x[0] * x[1] - k4 * x[1]\n    dx3_dt = k5 * x[1] - k5 * x[1] * x[2] - k6 * x[2]\n    \n    return [dx1_dt, dx2_dt, dx3_dt]\n\ndef generate_data(sigma, missing_frac):\n    \"\"\"Generates noisy, irregularly sampled data with dropouts.\"\"\"\n    T = 10.0\n    N = 300\n    x0 = [0.0, 0.0, 0.0]\n\n    # Solve ODE for ground truth\n    sol = solve_ivp(ode_system, [0, T], x0, dense_output=True, method='RK45')\n\n    # Generate irregular time samples\n    t_samples = np.sort(np.random.uniform(0, T, N))\n\n    # Apply missing fraction (dropouts)\n    n_keep = int(round((1.0 - missing_frac) * N))\n    keep_indices = np.random.choice(N, n_keep, replace=False)\n    t_meas = t_samples[keep_indices]\n    t_meas = np.sort(t_meas) # Ensure time is monotonic after selection\n\n    # Get clean data and add noise\n    x_clean = sol.sol(t_meas)\n    noise = np.random.normal(0, sigma, x_clean.shape)\n    x_meas = x_clean + noise\n\n    # Clip data to physical range [0, 1]\n    x_meas = np.clip(x_meas, 0, 1)\n\n    return t_meas, x_meas\n\ndef candidate_library(x):\n    \"\"\"Constructs the candidate library Theta(x) for SINDy.\"\"\"\n    x1, x2, x3 = x[0, :], x[1, :], x[2, :]\n    return np.vstack([\n        np.ones_like(x1),\n        x1,\n        x2,\n        x3,\n        x1 * x2,\n        x1 * x3,\n        x2 * x3\n    ])\n\ndef psi(t, c, h):\n    \"\"\"Cosine bell test function.\"\"\"\n    arg = np.pi * (t - c) / h\n    val = 0.5 * (1 + np.cos(arg))\n    val[np.abs(t-c) > h] = 0\n    return val\n\ndef dpsi_dt(t, c, h):\n    \"\"\"Derivative of the cosine bell test function.\"\"\"\n    arg = np.pi * (t - c) / h\n    val = - (np.pi / (2 * h)) * np.sin(arg)\n    val[np.abs(t-c) > h] = 0\n    return val\n\ndef stlsq(A, b, threshold, max_iter=20):\n    \"\"\"Sequential Thresholded Least Squares algorithm.\"\"\"\n    n_features = A.shape[1]\n    xi = np.zeros(n_features)\n    support = np.arange(n_features)\n\n    for _ in range(max_iter):\n        if len(support) == 0:\n            return np.zeros(n_features)\n        \n        A_sub = A[:, support]\n        xi_sub = np.linalg.lstsq(A_sub, b, rcond=None)[0]\n        \n        small_coeffs_mask = np.abs(xi_sub) < threshold\n        \n        if not np.any(small_coeffs_mask):\n            xi[support] = xi_sub\n            return xi # Converged\n        \n        support = np.delete(support, np.where(small_coeffs_mask)[0])\n        \n    if len(support) > 0:\n        xi_sub = np.linalg.lstsq(A[:, support], b, rcond=None)[0]\n        xi[support] = xi_sub\n        \n    return xi\n\ndef identify_model(t, x, h, lambda_thresh):\n    \"\"\"Identifies the model structure using weak-form SINDy.\"\"\"\n    T = t[-1]\n    n_states = x.shape[0]\n    \n    # Generate window centers\n    window_centers = np.arange(h, T - h, h / 2.0)\n    \n    weak_derivatives = []\n    weak_library_terms = []\n\n    for c in window_centers:\n        mask = (t >= c - h) & (t <= c + h)\n        t_w = t[mask]\n        \n        if len(t_w) < 2:\n            continue\n            \n        x_w = x[:, mask]\n        \n        psi_vals = psi(t_w, c, h)\n\n        # Window rejection criterion\n        support_mass = np.trapz(psi_vals, t_w)\n        if support_mass < 0.1 * h:\n            continue\n            \n        dpsi_dt_vals = dpsi_dt(t_w, c, h)\n        \n        # Calculate weak derivatives\n        y_m = np.array([-np.trapz(x_w[j, :] * dpsi_dt_vals, t_w) for j in range(n_states)])\n        weak_derivatives.append(y_m)\n        \n        # Calculate weak library terms\n        lib_vals = candidate_library(x_w)\n        theta_w_m = np.array([np.trapz(lib_vals[k, :] * psi_vals, t_w) for k in range(lib_vals.shape[0])])\n        weak_library_terms.append(theta_w_m)\n\n    if not weak_library_terms:\n         # No valid windows found, return empty supports\n        return [set() for _ in range(n_states)]\n\n    Y = np.vstack(weak_derivatives)\n    Theta_w = np.vstack(weak_library_terms)\n    \n    identified_supports = []\n    for j in range(n_states):\n        y_j = Y[:, j]\n        xi_j = stlsq(Theta_w, y_j, lambda_thresh)\n        support_j = set(np.where(np.abs(xi_j) > 1e-9)[0])\n        identified_supports.append(support_j)\n        \n    return identified_supports\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}