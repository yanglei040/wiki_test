{
    "hands_on_practices": [
        {
            "introduction": "The power of Support Vector Machines stems from a simple yet elegant geometric principle: finding the hyperplane that creates the widest possible \"street\" between two classes of data. This first exercise takes you back to this core concept. By working with a small, symmetric dataset, you will manually derive the parameters of the maximum-margin classifier, gaining a concrete understanding of how the data's geometry directly determines the optimal solution .",
            "id": "3353372",
            "problem": "A computational systems biology group is training a linear Support Vector Machine (SVM) classifier to distinguish two cell fates based on two transcriptomic features: the log-transformed expression deviations of transcription factors $X_{1}$ and $X_{2}$ from a baseline. They collect $6$ labeled samples: the positively labeled class ($+1$) has feature vectors $(2,2)$, $(2,0)$, and $(0,2)$, and the negatively labeled class ($-1$) has feature vectors $(-2,-2)$, $(-2,0)$, and $(0,-2)$. Assume the data are strictly linearly separable and that the classifier is the hard-margin maximum-margin linear SVM in $\\mathbb{R}^{2}$.\n\nUsing only the core definition of the hard-margin linear SVM (minimize the squared Euclidean norm of the weight vector subject to correct classification with a unit functional margin) and standard geometric facts about Euclidean norms and margins, determine the unique maximum-margin separating hyperplane parameters $(w,b)$, where $w \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}$, and the geometric margin width $2/\\|w\\|$. Report your final result as a single row vector $(w_{1},w_{2},b,\\gamma)$, where $\\gamma=2/\\|w\\|$.\n\nProvide exact values with no rounding.",
            "solution": "The problem is assessed to be valid. It is a well-posed, self-contained, and scientifically grounded problem in the domain of machine learning, specifically concerning Support Vector Machines (SVMs).\n\n### Step 1: Extract Givens\n- **Task**: Determine the parameters $(w, b)$ of a hard-margin linear SVM and the geometric margin width $\\gamma$.\n- **Feature Space**: $\\mathbb{R}^{2}$ with features $X_1$ and $X_2$.\n- **Data Points**: A set of $N=6$ labeled samples $(x_i, y_i)$.\n- **Positive Class ($y_i = +1$)**:\n  - $x_1 = (2, 2)$\n  - $x_2 = (2, 0)$\n  - $x_3 = (0, 2)$\n- **Negative Class ($y_i = -1$)**:\n  - $x_4 = (-2, -2)$\n  - $x_5 = (-2, 0)$\n  - $x_6 = (0, -2)$\n- **Model**: Hard-margin maximum-margin linear SVM. The separating hyperplane is defined by $w \\cdot x + b = 0$, where $w = (w_1, w_2) \\in \\mathbb{R}^2$ and $b \\in \\mathbb{R}$.\n- **Output Format**: Report the result as a single row vector $(w_1, w_2, b, \\gamma)$, where $\\gamma = 2/\\|w\\|$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is valid.\n- **Scientifically Grounded**: The problem is a canonical example of applying a hard-margin SVM, a fundamental concept in machine learning and computational biology. All principles and definitions are standard.\n- **Well-Posed**: The data are specified, and by visual inspection, they are linearly separable. The optimization problem for a hard-margin SVM on a linearly separable dataset is a convex optimization problem, which guarantees a unique solution for the maximum-margin hyperplane.\n- **Objective**: The problem is stated using precise mathematical language with no ambiguity or subjective elements.\n- **Completeness**: All necessary data points and their labels are provided.\n\n### Step 3: Derivation of the Solution\nThe hard-margin linear SVM aims to find a separating hyperplane $w \\cdot x + b = 0$ that maximizes the geometric margin, which is equivalent to minimizing the squared Euclidean norm of the weight vector, $\\|w\\|^2$. The optimization problem is formulated as:\n$$\n\\begin{aligned}\n \\underset{w, b}{\\text{minimize}}\n  \\frac{1}{2} \\|w\\|^2 \\\\\n \\text{subject to}\n  y_i(w \\cdot x_i + b) \\ge 1, \\quad \\text{for } i = 1, \\dots, 6\n\\end{aligned}\n$$\nThe constraints ensure that all data points are classified correctly with a functional margin of at least $1$.\n\nLet $w = (w_1, w_2)$. The $6$ constraints are:\nFor $y_i = +1$:\n1. $1 \\cdot (w_1(2) + w_2(2) + b) \\ge 1 \\implies 2w_1 + 2w_2 + b \\ge 1$\n2. $1 \\cdot (w_1(2) + w_2(0) + b) \\ge 1 \\implies 2w_1 + b \\ge 1$\n3. $1 \\cdot (w_1(0) + w_2(2) + b) \\ge 1 \\implies 2w_2 + b \\ge 1$\n\nFor $y_i = -1$:\n4. $-1 \\cdot (w_1(-2) + w_2(-2) + b) \\ge 1 \\implies 2w_1 + 2w_2 - b \\ge 1$\n5. $-1 \\cdot (w_1(-2) + w_2(0) + b) \\ge 1 \\implies 2w_1 - b \\ge 1$\n6. $-1 \\cdot (w_1(0) + w_2(-2) + b) \\ge 1 \\implies 2w_2 - b \\ge 1$\n\nWe can exploit the symmetry of the data to simplify the problem. The set of data points is symmetric with respect to swapping the coordinates $X_1$ and $X_2$. This suggests the optimal weight vector $w$ should have equal components, i.e., $w_1 = w_2$.\nThe data set is also symmetric with respect to the origin: for each point $x_i$ in class $y_i$, the point $-x_i$ is in class $-y_i$. This suggests the separating hyperplane should pass through the origin, which implies the bias term $b=0$.\n\nLet's test this hypothesis by setting $w_1 = w_2 = w_c$ and $b=0$. The optimization problem becomes minimizing $\\|w\\|^2 = w_c^2 + w_c^2 = 2w_c^2$, subject to the simplified constraints.\nSubstituting $w_1 = w_2 = w_c$ and $b=0$ into the constraints:\n1. $2w_c + 2w_c \\ge 1 \\implies 4w_c \\ge 1$\n2. $2w_c \\ge 1$\n3. $2w_c \\ge 1$\n4. $2w_c + 2w_c \\ge 1 \\implies 4w_c \\ge 1$\n5. $2w_c \\ge 1$\n6. $2w_c \\ge 1$\n\nAll constraints simplify to the single condition $2w_c \\ge 1$, which is $w_c \\ge \\frac{1}{2}$. The constraint $4w_c \\ge 1$ (i.e., $w_c \\ge \\frac{1}{4}$) is redundant.\nWe must minimize the objective function $2w_c^2$ subject to $w_c \\ge \\frac{1}{2}$. Since the objective function is monotonically increasing for $w_c  0$, the minimum is achieved at the boundary of the feasible region, i.e., when $w_c$ is at its minimum possible value.\nThus, the optimal value is $w_c = \\frac{1}{2}$.\n\nThis gives the unique solution for the weight vector and bias:\n$w_1 = \\frac{1}{2}$\n$w_2 = \\frac{1}{2}$\n$b = 0$\n\nThe weight vector is $w = (\\frac{1}{2}, \\frac{1}{2})$. The separating hyperplane is $\\frac{1}{2}X_1 + \\frac{1}{2}X_2 = 0$, or $X_1 + X_2 = 0$.\n\nLet's verify this solution against the original constraints.\nWith $w=(\\frac{1}{2}, \\frac{1}{2})$ and $b=0$:\n1. For $x_1=(2,2)$: $y_1(w \\cdot x_1 + b) = 1(\\frac{1}{2}(2) + \\frac{1}{2}(2) + 0) = 1(1+1) = 2 \\ge 1$. (Correct)\n2. For $x_2=(2,0)$: $y_2(w \\cdot x_2 + b) = 1(\\frac{1}{2}(2) + \\frac{1}{2}(0) + 0) = 1(1) = 1 \\ge 1$. (Correct, lies on margin)\n3. For $x_3=(0,2)$: $y_3(w \\cdot x_3 + b) = 1(\\frac{1}{2}(0) + \\frac{1}{2}(2) + 0) = 1(1) = 1 \\ge 1$. (Correct, lies on margin)\n4. For $x_4=(-2,-2)$: $y_4(w \\cdot x_4 + b) = -1(\\frac{1}{2}(-2) + \\frac{1}{2}(-2) + 0) = -1(-1-1) = 2 \\ge 1$. (Correct)\n5. For $x_5=(-2,0)$: $y_5(w \\cdot x_5 + b) = -1(\\frac{1}{2}(-2) + \\frac{1}{2}(0) + 0) = -1(-1) = 1 \\ge 1$. (Correct, lies on margin)\n6. For $x_6=(0,-2)$: $y_6(w \\cdot x_6 + b) = -1(\\frac{1}{2}(0) + \\frac{1}{2}(-2) + 0) = -1(-1) = 1 \\ge 1$. (Correct, lies on margin)\n\nThe points for which the equality $y_i(w \\cdot x_i + b) = 1$ holds are the support vectors: $(2,0), (0,2), (-2,0), (0,-2)$. The solution is consistent.\n\nThe final step is to calculate the geometric margin width $\\gamma = \\frac{2}{\\|w\\|}$.\nFirst, calculate the Euclidean norm of $w$:\n$$ \\|w\\| = \\sqrt{w_1^2 + w_2^2} = \\sqrt{(\\frac{1}{2})^2 + (\\frac{1}{2})^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{2}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} $$\nNow, calculate the margin width $\\gamma$:\n$$ \\gamma = \\frac{2}{\\|w\\|} = \\frac{2}{\\frac{\\sqrt{2}}{2}} = \\frac{4}{\\sqrt{2}} = \\frac{4\\sqrt{2}}{2} = 2\\sqrt{2} $$\n\nThe required parameters are $w_1 = \\frac{1}{2}$, $w_2 = \\frac{1}{2}$, $b=0$, and $\\gamma = 2\\sqrt{2}$. The final result is the row vector $(w_1, w_2, b, \\gamma)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2}  0  2\\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building a powerful classifier is only half the battle; we must also honestly evaluate its performance on new, unseen data. This exercise tackles a subtle but critical issue in machine learning methodology: the optimistic bias introduced when hyperparameters are tuned and evaluated on the same data. By analyzing different evaluation strategies, you will understand why a procedure like nested cross-validation is essential for obtaining trustworthy generalization estimates, a cornerstone of reproducible computational biology .",
            "id": "3353406",
            "problem": "In a high-dimensional gene expression classification task in computational systems biology, a researcher uses a Support Vector Machine (SVM) with a radial basis function kernel to distinguish tumor from normal tissue based on messenger ribonucleic acid (mRNA) counts. The SVM has hyperparameters $\\lambda = (C, \\gamma)$, where $C$ controls the penalty on the hinge loss and $\\gamma$ controls the kernel width. Let the unknown data-generating distribution be $\\mathcal{D}$ over pairs $(\\mathbf{x}, y)$ with $\\mathbf{x} \\in \\mathbb{R}^p$ and $y \\in \\{-1, +1\\}$. The learning algorithm $A_{\\lambda}$ maps a sample $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ to a classifier $h_{\\lambda, S}$. The population risk of a classifier $h$ is $R_{\\mathcal{D}}(h) = \\mathbb{E}_{(\\mathbf{x}, y) \\sim \\mathcal{D}}[\\ell(h(\\mathbf{x}), y)]$ for a bounded loss $\\ell$, and the $k$-fold Cross-Validation (CV) estimator for a fixed $\\lambda$ on $S$ is $\\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$, defined as the average loss on held-out folds when training $A_{\\lambda}$ on the corresponding training splits. Define the hyperparameter selected by grid search with CV as $\\hat{\\lambda}(S) = \\arg\\min_{\\lambda \\in \\Lambda} \\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$, where $\\Lambda$ is a finite grid of candidate values for $(C,\\gamma)$.\n\nThe researcher wishes to estimate the expected generalization performance of the final tuned procedure $S \\mapsto h_{\\hat{\\lambda}(S), S}$. Consider two estimators of this expected performance: the naive reuse estimator $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$, and the nested estimator that for each outer fold $W_j$ performs an inner CV-based hyperparameter search on $S \\setminus W_j$ to obtain $\\hat{\\lambda}(S \\setminus W_j)$, trains $A_{\\hat{\\lambda}(S \\setminus W_j)}$ on $S \\setminus W_j$, and evaluates on $W_j$, averaging across outer folds to produce $\\widehat{R}_{\\mathrm{nest}}$.\n\nUsing only foundational definitions of population risk, empirical risk, independence between training and test splits in cross-validation, and properties of expectations over minima and maxima of random variables, which statement best explains why Nested Cross-Validation is required to obtain unbiased generalization estimates when selecting the SVM hyperparameters $(C, \\gamma)$ in this biological classification setting?\n\nA. The reuse of the same data $S$ to both select $\\hat{\\lambda}(S)$ and to estimate $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$ couples the estimator to the sample noise, inducing selection bias via the inequality $\\mathbb{E}[\\min_{\\lambda} Z_{\\lambda}] \\le \\min_{\\lambda} \\mathbb{E}[Z_{\\lambda}]$ with $Z_{\\lambda} = \\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$. An outer fold $W_j$ in Nested Cross-Validation is independent of the inner hyperparameter search on $S \\setminus W_j$, so averaging losses on $W_j$ yields an approximately unbiased estimate of $\\mathbb{E}[R_{\\mathcal{D}}(h_{\\hat{\\lambda}(S), S})]$.\n\nB. Using $k$-fold CV with a sufficiently large $k$ (for example, Leave-One-Out Cross-Validation with $k = n$) guarantees unbiasedness of $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$ even when hyperparameters are tuned on the same data, so nesting is unnecessary.\n\nC. Nested Cross-Validation is needed because the radial basis function kernel induces a non-convex optimization landscape for SVM training, and nesting ensures convergence to the global optimum, which in turn makes the performance estimate unbiased.\n\nD. Nested Cross-Validation is not about unbiasedness; its sole purpose is to reduce the variance of the performance estimate, while the bias of $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$ remains unaffected by whether hyperparameters are tuned on the same data.\n\nE. Nested Cross-Validation is only necessary in high-dimensional regimes with $p \\gg n$ and becomes unnecessary when $p \\ll n$, because the margin distribution can be estimated without bias from a single cross-validation loop in low-dimensional settings.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a detailed derivation of the correct answer and an evaluation of all provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Domain:** High-dimensional gene expression classification in computational systems biology.\n- **Model:** Support Vector Machine (SVM) with a radial basis function (RBF) kernel.\n- **Hyperparameters:** $\\lambda = (C, \\gamma)$, where $C$ is the hinge loss penalty and $\\gamma$ is the kernel width.\n- **Data Space:** The data-generating distribution is $\\mathcal{D}$ over pairs $(\\mathbf{x}, y)$, with $\\mathbf{x} \\in \\mathbb{R}^p$ and $y \\in \\{-1, +1\\}$.\n- **Learning Algorithm:** $A_{\\lambda}$ maps a sample $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ to a classifier $h_{\\lambda, S}$.\n- **Population Risk:** The a priori risk of a classifier $h$ is $R_{\\mathcal{D}}(h) = \\mathbb{E}_{(\\mathbf{x}, y) \\sim \\mathcal{D}}[\\ell(h(\\mathbf{x}), y)]$ for a bounded loss function $\\ell$.\n- **Cross-Validation (CV) Estimator:** For a fixed $\\lambda$, $\\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$ is the $k$-fold CV error estimate on sample $S$.\n- **Hyperparameter Selection Rule:** $\\hat{\\lambda}(S) = \\arg\\min_{\\lambda \\in \\Lambda} \\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$ over a finite grid $\\Lambda$.\n- **Quantity of Interest:** The expected generalization performance of the complete modeling procedure, i.e., the expected risk of the classifier chosen after hyperparameter tuning: $\\mathbb{E}_S[R_{\\mathcal{D}}(h_{\\hat{\\lambda}(S), S})]$.\n- **Estimator 1 (Naive Reuse):** $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$. This is the CV score of the selected hyperparameter on the same data $S$.\n- **Estimator 2 (Nested CV):** $\\widehat{R}_{\\mathrm{nest}}$. This involves an outer loop partitioning $S$ into folds $W_j$. For each fold, an inner CV search is performed on $S \\setminus W_j$ to select a hyperparameter $\\hat{\\lambda}(S \\setminus W_j)$, a model is trained on $S \\setminus W_j$ using this hyperparameter, and its performance is evaluated on the hold-out fold $W_j$. The final estimate is the average of these performances.\n- **Question:** Explain why Nested Cross-Validation is necessary for an unbiased generalization estimate when selecting hyperparameters, using foundational principles.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly located within the well-established fields of statistical learning theory and computational biology. All concepts—SVMs, RBF kernels, cross-validation, hyperparameter tuning, population risk, and selection bias—are standard and rigorously defined. The scenario of classifying gene expression data is a canonical application. The problem is scientifically sound.\n- **Well-Posed:** All terms are formally defined. The distinction between the naive estimator and the nested estimator is described correctly. The question asks for an explanation of a known statistical phenomenon (selection bias in model validation) in this specific context. A unique and correct conceptual answer exists.\n- **Objective:** The language is formal, precise, and free of any subjective or opinion-based claims.\n- **Completeness and Consistency:** The problem provides all necessary definitions to reason about the bias of the estimators. There are no contradictions.\n- **Realism:** The scenario is not only realistic but is a textbook example of best practices in machine learning model evaluation. High-dimensionality ($p \\gg n$) is a common challenge in genomics.\n- **Other Flaws:** The problem is not trivial, pseudo-profound, or tautological. It addresses a subtle but critical point in applied machine learning that is often a source of error. The \"selection bias\" issue is a core concept in model assessment.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is well-posed, scientifically sound, and addresses a key concept in machine learning methodology. I will proceed to derive the solution.\n\n### Derivation and Option Analysis\n\nThe core of the problem lies in understanding how the process of *selection* based on an empirical performance estimate introduces bias. We want to estimate the true, expected risk of our entire model-building procedure, which includes hyperparameter tuning. Let this be $R_{\\text{true}} = \\mathbb{E}_S[R_{\\mathcal{D}}(h_{\\hat{\\lambda}(S), S})]$.\n\nLet's analyze the naive reuse estimator: $\\widehat{R}_{\\text{naive}} = \\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$.\nBy definition of $\\hat{\\lambda}(S)$, this estimator is equivalent to:\n$$ \\widehat{R}_{\\text{naive}} = \\min_{\\lambda \\in \\Lambda} \\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S) $$\nFor each specific $\\lambda \\in \\Lambda$, the CV estimate $\\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$ is a random variable that depends on the sample $S$. Let's denote this random variable by $Z_{\\lambda}(S)$. It is known that for a fixed $\\lambda$, $Z_{\\lambda}(S)$ is an approximately unbiased estimator of the true risk of a model trained on a data set of size $n(1 - 1/k)$. That is, $\\mathbb{E}_S[Z_{\\lambda}(S)] \\approx R_{\\mathcal{D}, n(1-1/k)}(A_\\lambda)$.\n\nThe naive estimator is $\\min_{\\lambda \\in \\Lambda} Z_{\\lambda}(S)$. To assess its bias, we examine its expectation over the distribution of samples $S$:\n$$ \\mathbb{E}_S[\\widehat{R}_{\\text{naive}}] = \\mathbb{E}_S\\left[\\min_{\\lambda \\in \\Lambda} Z_{\\lambda}(S)\\right] $$\nA fundamental property of the expectation operator is that for any set of random variables $\\{X_i\\}$, $\\mathbb{E}[\\min_i X_i] \\le \\min_i \\mathbb{E}[X_i]$. This is a form of Jensen's inequality, as the minimum function is concave. Applying this gives:\n$$ \\mathbb{E}_S\\left[\\min_{\\lambda \\in \\Lambda} Z_{\\lambda}(S)\\right] \\le \\min_{\\lambda \\in \\Lambda} \\mathbb{E}_S[Z_{\\lambda}(S)] $$\nThe term on the right, $\\min_{\\lambda \\in \\Lambda} \\mathbb{E}_S[Z_{\\lambda}(S)]$, represents the risk of the truly optimal hyperparameter (the one that is best *on average* over all possible datasets). Our target quantity, $R_{\\text{true}} = \\mathbb{E}_S[R_{\\mathcal{D}}(h_{\\hat{\\lambda}(S), S})]$, is the expected risk of the hyperparameter chosen for a *specific* sample $S$, averaged over samples. The naive estimator is optimistically biased because the minimization is performed *inside* the expectation. We have chosen the $\\lambda$ that looks best on the particular noise realization in our sample $S$, and we are reporting that \"lucky\" low error. This is selection bias.\n\nNow, consider the nested cross-validation estimator, $\\widehat{R}_{\\mathrm{nest}}$. The procedure is designed to break the dependence that causes this bias. In the outer loop, a fold $W_j$ is held out as a pristine test set. The entire model selection pipeline, including the inner CV to find $\\hat{\\lambda}(S \\setminus W_j)$, is performed on the training set $S \\setminus W_j$. The resulting model, $h_{\\hat{\\lambda}(S \\setminus W_j), S \\setminus W_j}$, is then evaluated on $W_j$.\nBecause $W_j$ was not used in any way to select $\\hat{\\lambda}(S \\setminus W_j)$ or to train the final model for that fold, the performance on $W_j$ is an unbiased estimate of the true risk of the model produced by the procedure run on $S \\setminus W_j$. By averaging these unbiased estimates over all outer folds, $\\widehat{R}_{\\mathrm{nest}}$ provides an approximately unbiased estimate of the performance of the *entire procedure* (training on a sample of size $\\approx n(1-1/k_{\\text{outer}})$ and tuning with inner CV). This properly estimates the generalization performance of the complete workflow.\n\n### Option-by-Option Analysis\n\n**A. The reuse of the same data $S$ to both select $\\hat{\\lambda}(S)$ and to estimate $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$ couples the estimator to the sample noise, inducing selection bias via the inequality $\\mathbb{E}[\\min_{\\lambda} Z_{\\lambda}] \\le \\min_{\\lambda} \\mathbb{E}[Z_{\\lambda}]$ with $Z_{\\lambda} = \\widehat{R}_{\\mathrm{CV}}(A_{\\lambda}; S)$. An outer fold $W_j$ in Nested Cross-Validation is independent of the inner hyperparameter search on $S \\setminus W_j$, so averaging losses on $W_j$ yields an approximately unbiased estimate of $\\mathbb{E}[R_{\\mathcal{D}}(h_{\\hat{\\lambda}(S), S})]$.**\nThis statement is perfectly aligned with the derivation above. It correctly identifies the reuse of data as the source of coupling and selection bias. It provides the precise mathematical inequality that formalizes this bias. It then correctly explains that nested CV breaks this coupling by using an independent outer test fold for evaluation, leading to an approximately unbiased estimate of the overall procedure's performance.\n**Verdict: Correct.**\n\n**B. Using $k$-fold CV with a sufficiently large $k$ (for example, Leave-One-Out Cross-Validation with $k = n$) guarantees unbiasedness of $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$ even when hyperparameters are tuned on the same data, so nesting is unnecessary.**\nThis is incorrect. Increasing $k$ in standard CV reduces the bias of the performance estimate for a *fixed* hyperparameter $\\lambda$, as the training sets used in the CV folds become more similar in size to the full dataset. However, it does not remove the *selection bias* introduced by choosing the best $\\lambda$ based on the CV results and then reporting that same (minimal) CV score. The optimistic bias from the `min` operation remains regardless of the value of $k$.\n**Verdict: Incorrect.**\n\n**C. Nested Cross-Validation is needed because the radial basis function kernel induces a non-convex optimization landscape for SVM training, and nesting ensures convergence to the global optimum, which in turn makes the performance estimate unbiased.**\nThis statement contains multiple falsehoods. First, the standard formulation of the SVM dual problem is a convex quadratic programming problem, even with an RBF kernel; it has a unique global optimum. Second, nested cross-validation is a procedure for *performance estimation*, not for *optimization*. It does not change the optimization algorithm or its convergence properties. Its purpose is to estimate the generalization error of a given modeling pipeline, whatever that pipeline may be. The unbiasedness of the estimate comes from data partitioning, not from properties of the optimization landscape.\n**Verdict: Incorrect.**\n\n**D. Nested Cross-Validation is not about unbiasedness; its sole purpose is to reduce the variance of the performance estimate, while the bias of $\\widehat{R}_{\\mathrm{CV}}(A_{\\hat{\\lambda}(S)}; S)$ remains unaffected by whether hyperparameters are tuned on the same data.**\nThis is incorrect. The primary motivation for nested cross-validation is to obtain an approximately **unbiased** estimate of the generalization error by correcting the optimistic selection bias of the naive approach. Standard cross-validation already serves to reduce variance compared to a single train-test split. The claim that the bias is unaffected is the opposite of the truth; the bias is the very problem being solved.\n**Verdict: Incorrect.**\n\n**E. Nested Cross-Validation is only necessary in high-dimensional regimes with $p \\gg n$ and becomes unnecessary when $p \\ll n$, because the margin distribution can be estimated without bias from a single cross-validation loop in low-dimensional settings.**\nThis is incorrect. The selection bias is a fundamental statistical phenomenon that occurs whenever a selection is made based on empirical performance, and that performance is then reported as the final estimate. It is not limited to high-dimensional ($p \\gg n$) settings. While the magnitude of the bias may be larger when the risk of overfitting is higher (as in $p \\gg n$ scenarios), the bias exists even in low-dimensional ($p \\ll n$) problems. The claim about estimating the margin distribution without bias is unfounded and does not negate the core issue of selection bias.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Real-world biological data is rarely as clean as textbook examples, often containing systematic, non-biological variations known as batch effects. This final capstone exercise challenges you to build a complete and robust classification pipeline that handles such complexities. You will implement a batch correction method and integrate it correctly within a cross-validation loop to prevent data leakage, a critical skill for developing reliable biomarkers from multi-cohort omics studies .",
            "id": "3353416",
            "problem": "You are given a scenario in computational systems biology where multi-cohort omics measurements are used for binary biological classification using Support Vector Machines (SVM). Cohorts correspond to laboratory batches, each potentially imprinting a systematic location and scale deviation on measured features (batch effects). Your task is to formalize how batch effects arise, derive a location-scale batch correction procedure following a parametric Empirical Bayes (EB) perspective, and implement a rigorous training pipeline that performs batch correction strictly within training folds to prevent data leakage before SVM fitting. Then, evaluate this pipeline on synthetic multi-cohort omics data under multiple test conditions.\n\nFundamental base and generative model:\n- Omics features are measurements of molecular abundance influenced by biological condition and technical effects. Let $p$ denote the number of features and $n$ denote the number of samples. Each sample $i$ belongs to a batch $B_i \\in \\{1,\\dots, B\\}$ and has a binary class label $Y_i \\in \\{-1, +1\\}$.\n- A simplified and scientifically grounded measurement model for feature $g \\in \\{1,\\dots, p\\}$ in sample $i$ is\n$$\nX_{g,i} \\;=\\; \\mu_g \\;+\\; s_g Y_i \\;+\\; \\delta_{g, B_i} \\;+\\; \\gamma_{g, B_i} \\,\\varepsilon_{g,i},\n$$\nwhere $\\mu_g$ is a baseline abundance for feature $g$, $s_g$ is the biological signal loading for feature $g$, $\\delta_{g,b}$ is a batch-specific location (additive) effect for feature $g$ in batch $b$, $\\gamma_{g,b}$ is a batch-specific scale (multiplicative) effect, and $\\varepsilon_{g,i}$ is independent noise with zero mean and finite variance. This model captures how batch effects arise: $\\delta_{g,b}$ and $\\gamma_{g,b}$ systematically alter feature distributions across batches and can confound downstream classifiers if not corrected.\n\nBatch correction principle:\n- The parametric EB ComBat-style correction removes batch effects by standardizing features, estimating batch-specific location and scale parameters within the training data, and adjusting both training and test features using those training-derived parameters. For each feature $g$, define the global training mean and standard deviation:\n$$\n\\hat{\\mu}_g \\;=\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_{g,i}, \\qquad\n\\hat{\\sigma}_g \\;=\\; \\sqrt{ \\frac{1}{n_{\\text{train}}-1} \\sum_{i \\in \\text{train}} (X_{g,i} - \\hat{\\mu}_g)^2 }.\n$$\nStandardize training and test by $Z_{g,i} = (X_{g,i} - \\hat{\\mu}_g)/\\hat{\\sigma}_g$. For each batch $b$ present in the training set, estimate batch-wise standardized location and scale for feature $g$:\n$$\n\\hat{m}_{g,b} \\;=\\; \\frac{1}{n_{b,\\text{train}}} \\sum_{i \\in \\text{train}, B_i=b} Z_{g,i}, \\qquad\n\\hat{s}_{g,b} \\;=\\; \\sqrt{ \\frac{1}{n_{b,\\text{train}}-1} \\sum_{i \\in \\text{train}, B_i=b} (Z_{g,i} - \\hat{m}_{g,b})^2 }.\n$$\nAdjust training and test standardized values by $Z_{g,i}^{\\text{adj}} = (Z_{g,i} - \\hat{m}_{g,B_i}) / \\hat{s}_{g,B_i}$ for batches present in training; if a test batch is absent from training, do not adjust that batch (identity transform) or use a well-defined fallback that does not depend on test data. Finally, rescale back using the global training statistics: $X_{g,i}^{\\text{adj}} = Z_{g,i}^{\\text{adj}} \\hat{\\sigma}_g + \\hat{\\mu}_g$. This is a location-scale ComBat-style correction without EB shrinkage, performed strictly within training folds to prevent data leakage.\n\nClassifier and optimization objective:\n- Use a linear SVM classifier fit in the primal, minimizing the regularized empirical risk. Let $w \\in \\mathbb{R}^p$ and $b \\in \\mathbb{R}$. The squared hinge loss objective for training samples $(x_i, y_i)$ is\n$$\n\\min_{w,b} \\quad \\frac{\\lambda}{2}\\|w\\|_2^2 \\;+\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} \\left( \\max\\{0, 1 - y_i (w^\\top x_i + b)\\} \\right)^2,\n$$\nwhere $\\lambda  0$ is the regularization parameter. Optimize this objective by stochastic gradient descent (SGD) over mini-batches. For a training example $(x_i, y_i)$ with margin $m_i = y_i (w^\\top x_i + b)$, the gradient contributions are\n$$\n\\nabla_w \\;\\leftarrow\\; \\lambda w \\;-\\; \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i x_i (1 - m_i), \\qquad\n\\nabla_b \\;\\leftarrow\\; - \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i (1 - m_i).\n$$\nUpdate by $(w, b) \\leftarrow (w, b) - \\eta (\\nabla_w, \\nabla_b)$ with learning rate $\\eta  0$.\n\nData leakage consideration:\n- A preprocessing operator that depends on the entire dataset (including test samples) is a data-dependent transformation. If batch correction parameters are estimated on all samples before cross-validation, test-set information influences the training transformation, breaking the independence assumption between training and test sets. Formally, letting $\\mathcal{T}$ denote the training set and $\\mathcal{S}$ the test set, a data-dependent map $\\mathcal{P}$ that depends on $\\mathcal{T} \\cup \\mathcal{S}$ changes the distribution of transformed $\\mathcal{T}$ conditional on $\\mathcal{S}$, violating the standard generalization setup and inflating performance.\n\nTasks:\n- Implement the above ComBat-style batch correction strictly within training folds.\n- Fit a linear SVM using the squared hinge loss via stochastic gradient descent with the provided objective.\n- Use $K$-fold cross-validation with $K=5$ and compute the mean classification accuracy across folds, where accuracy is the fraction of correctly classified test samples expressed as a decimal (not a percentage).\n\nTest suite and required outputs:\n- All random draws must be controlled with a fixed pseudo-random seed to ensure determinism. Use seed $2025$ for data generation and any stochastic optimization.\n- Use $p=50$ features and binary labels $Y_i \\in \\{-1,+1\\}$ assigned approximately equally within each batch.\n- For all cases, set the following generative parameters: feature baseline $\\mu_g = 0$ for all $g$, number of signal features $p_{\\text{sig}} = 20$ (features $1$ through $20$), signal strength $s_g = s = 1.0$ for $g \\le p_{\\text{sig}}$ and $s_g = 0$ otherwise, noise $\\varepsilon_{g,i} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ with $\\sigma_\\varepsilon = 0.5$. For each batch $b$ and feature $g$, draw location offsets $\\delta_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\delta^2)$ with $\\sigma_\\delta = 1.0$ and scale effects $\\gamma_{g,b}$ from a log-normal distribution with log-scale standard deviation $\\sigma_\\gamma = 0.3$ and zero log-mean, that is, $\\log \\gamma_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)$.\n- Use SVM hyperparameters: regularization $\\lambda = 10^{-3}$, learning rate $\\eta = 0.05$, epochs $E = 20$, mini-batch size $m = 32$.\n- Construct folds by stratifying on the class labels whenever not otherwise specified, and keep folds disjoint. When an entire batch is absent from a training fold, for that batch in the corresponding test set do not adjust (identity transform) rather than using any test-derived quantities.\n\nProvide three test cases:\n1. Case $1$ (balanced multi-cohort): $B=3$ batches, with sizes $[40, 40, 40]$ samples per batch. Perform $5$-fold cross-validation, do ComBat-style correction within each training fold, fit the SVM, and report the mean test accuracy across folds as a decimal rounded to four digits.\n2. Case $2$ (edge case: batch absent from training): $B=3$ batches, with sizes $[50, 50, 10]$. Construct $5$-fold cross-validation so that fold $1$ has all $10$ samples from the small batch in its test set and none in its training set; for the other folds, use stratified splitting. Perform within-fold batch correction and report the mean test accuracy across folds as a decimal rounded to four digits.\n3. Case $3$ (data leakage sensitivity): Reuse Case $1$ data. Compute two pipelines: Pipeline $\\mathcal{P}_{\\text{leak}}$ where batch correction parameters are fit on the entire dataset once and applied prior to cross-validation, and Pipeline $\\mathcal{P}_{\\text{strict}}$ where parameters are fit strictly within training folds. Report the difference in mean test accuracy (Pipeline $\\mathcal{P}_{\\text{leak}}$ minus Pipeline $\\mathcal{P}_{\\text{strict}}$) across the same folds, as a decimal rounded to four digits.\n\nFinal output format:\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (for example, $[0.8123,0.7450,0.0360]$). The entries must be in the order of Case $1$, Case $2$, and Case $3$. All values must be decimals rounded to four digits. No other text should be printed.",
            "solution": "The problem requires the development and evaluation of a computational pipeline for binary classification of multi-cohort omics data. The core tasks involve generating synthetic data based on a specified generative model that includes biological signals and batch effects, implementing a principled batch correction procedure, training a linear Support Vector Machine (SVM) classifier, and evaluating the entire pipeline using cross-validation. A critical aspect of the problem is the correct handling of batch correction within the cross-validation framework to prevent data leakage from the test set into the training process. The solution is presented in three parts: formalization of the data model and algorithms, a description of the cross-validation and evaluation strategy, and the implementation details for the three specified test cases.\n\nFirst, we formalize the underlying scientific and mathematical principles. The generative model for the measurement $X_{g,i}$ of feature $g \\in \\{1, \\dots, p\\}$ in sample $i \\in \\{1, \\dots, n\\}$ is given by:\n$$\nX_{g,i} \\;=\\; \\mu_g \\;+\\; s_g Y_i \\;+\\; \\delta_{g, B_i} \\;+\\; \\gamma_{g, B_i} \\,\\varepsilon_{g,i}\n$$\nHere, $\\mu_g$ is the baseline abundance of feature $g$, $Y_i \\in \\{-1, +1\\}$ is the binary class label, and $s_g$ represents the feature's association with the class (biological signal). The terms $\\delta_{g, B_i}$ and $\\gamma_{g, B_i}$ are, respectively, the additive (location) and multiplicative (scale) batch effects for the batch $B_i$ to which sample $i$ belongs. The term $\\varepsilon_{g,i}$ represents independent, identically distributed noise drawn from a distribution with a mean of $0$ and finite variance. Per the problem specification, the parameters are instantiated as follows: $\\mu_g=0$, $s_g=1.0$ for the first $p_{\\text{sig}}=20$ features and $s_g=0$ otherwise, $\\delta_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\delta^2)$ with $\\sigma_\\delta=1.0$, $\\log \\gamma_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)$ with $\\sigma_\\gamma=0.3$, and $\\varepsilon_{g,i} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ with $\\sigma_\\varepsilon=0.5$.\n\nTo mitigate the confounding influence of batch effects, a parametric Empirical Bayes (EB) ComBat-style correction is applied. To prevent data leakage, all parameters for this correction must be estimated exclusively from the training data of each cross-validation fold. The procedure for each feature $g$ is as follows:\n1.  Compute the global mean $\\hat{\\mu}_g$ and standard deviation $\\hat{\\sigma}_g$ using only training samples.\n    $$\n    \\hat{\\mu}_g \\;=\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_{g,i}, \\qquad\n    \\hat{\\sigma}_g \\;=\\; \\sqrt{ \\frac{1}{n_{\\text{train}}-1} \\sum_{i \\in \\text{train}} (X_{g,i} - \\hat{\\mu}_g)^2 }\n    $$\n2.  Standardize both training and test data using these training-derived parameters: $Z_{g,i} = (X_{g,i} - \\hat{\\mu}_g) / \\hat{\\sigma}_g$. A small constant $\\epsilon$ is added to $\\hat{\\sigma}_g$ for numerical stability.\n3.  For each batch $b$ present in the training set, estimate its specific location and scale effects on the standardized data.\n    $$\n    \\hat{m}_{g,b} \\;=\\; \\frac{1}{n_{b,\\text{train}}} \\sum_{i \\in \\text{train}, B_i=b} Z_{g,i}, \\qquad\n    \\hat{s}_{g,b} \\;=\\; \\sqrt{ \\frac{1}{n_{b,\\text{train}}-1} \\sum_{i \\in \\text{train}, B_i=b} (Z_{g,i} - \\hat{m}_{g,b})^2 }\n    $$\n    A stability constant is also used for the denominator of $\\hat{s}_{g,b}$. If a batch contains only one sample in the training fold, its scale correction factor $\\hat{s}_{g,b}$ is set to $1$.\n4.  Adjust the standardized values. For a sample $i$ in batch $B_i$ that was present during training, the adjustment is $Z_{g,i}^{\\text{adj}} = (Z_{g,i} - \\hat{m}_{g,B_i}) / \\hat{s}_{g,B_i}$. If batch $B_i$ (e.g., from a test sample) was not in the training set, no adjustment is made (i.e., $Z_{g,i}^{\\text{adj}} = Z_{g,i}$).\n5.  Rescale the adjusted data back to the original feature space dimensions using the global training parameters: $X_{g,i}^{\\text{adj}} = Z_{g,i}^{\\text{adj}} \\hat{\\sigma}_g + \\hat{\\mu}_g$.\n\nThe corrected data $x_i^{\\text{adj}} \\in \\mathbb{R}^p$ and corresponding labels $y_i$ are then used to train a linear SVM classifier. The classifier seeks to find a weight vector $w \\in \\mathbb{R}^p$ and a bias term $b \\in \\mathbb{R}$ by minimizing the squared hinge loss with $L_2$ regularization:\n$$\n\\min_{w,b} \\quad \\frac{\\lambda}{2}\\|w\\|_2^2 \\;+\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} \\left( \\max\\{0, 1 - y_i (w^\\top x_i^{\\text{adj}} + b)\\} \\right)^2\n$$\nThis objective function is optimized using Stochastic Gradient Descent (SGD). For each mini-batch $\\mathcal{B}$ of training samples, the weights and bias are updated iteratively. With a learning rate $\\eta  0$ and regularization parameter $\\lambda  0$, the updates are performed using the gradients:\n$$\n\\nabla_w J = \\lambda w \\;-\\; \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i x_i^{\\text{adj}} (1 - m_i)\n$$\n$$\n\\nabla_b J = - \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i (1 - m_i)\n$$\nwhere $m_i = y_i (w^\\top x_i^{\\text{adj}} + b)$ is the margin for sample $i$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The parameters are updated as $(w,b) \\leftarrow (w,b) - \\eta (\\nabla_w J, \\nabla_b J)$.\n\nThe evaluation will be performed using $K=5$-fold cross-validation. The dataset is partitioned into $5$ disjoint folds. In each of $5$ iterations, one fold serves as the test set and the remaining $4$ as the training set. The mean classification accuracy across the $5$ test folds is the final performance metric.\n\nCase 1 involves a balanced design with $B=3$ batches of sizes $[40, 40, 40]$. A standard stratified $5$-fold split is used. The batch correction is performed strictly within each training fold.\n\nCase 2 examines an edge case with batch sizes $[50, 50, 10]$. The $5$ folds are constructed such that for the first fold, the entire small batch of $10$ samples is part of the test set, ensuring it is absent from the corresponding training set. This tests the specified fallback mechanism (identity transform for unknown batches). The remaining folds partition the rest of the data.\n\nCase 3 directly quantifies the impact of data leakage. Using the same data and folds as Case $1$, we compare two pipelines. The strict pipeline, $\\mathcal{P}_{\\text{strict}}$, performs correction within each fold, as described above. The leaky pipeline, $\\mathcal{P}_{\\text{leak}}$, estimates batch correction parameters once on the entire dataset before the cross-validation begins. The reported result is the difference in mean accuracy, $\\text{Acc}(\\mathcal{P}_{\\text{leak}}) - \\text{Acc}(\\mathcal{P}_{\\text{strict}})$, which is expected to be positive, artificially inflating performance due to the leakage of test set information into the training transformation. All computations adhere to the specified random seed of $2025$ for full reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the execution of the three test cases.\n    \"\"\"\n\n    def generate_data(batch_sizes, p, p_sig, s, sigma_eps, sigma_delta, sigma_gamma, seed):\n        \"\"\"Generates synthetic omics data based on the specified model.\"\"\"\n        rng = np.random.default_rng(seed)\n        n_samples = sum(batch_sizes)\n        n_batches = len(batch_sizes)\n\n        # Assign samples to batches and create class labels (Y)\n        batches = np.repeat(np.arange(1, n_batches + 1), batch_sizes)\n        y = np.ones(n_samples, dtype=int)\n        start_idx = 0\n        for size in batch_sizes:\n            # Assign labels approximately equally within each batch\n            n_pos = size // 2\n            y[start_idx : start_idx + n_pos] = 1\n            y[start_idx + n_pos : start_idx + size] = -1\n            rng.shuffle(y[start_idx : start_idx + size])\n            start_idx += size\n        \n        # Generate model parameters\n        mu = np.zeros(p)\n        s_vec = np.zeros(p)\n        s_vec[:p_sig] = s\n\n        # Batch effects\n        delta = rng.normal(0, sigma_delta, size=(p, n_batches))\n        log_gamma = rng.normal(0, sigma_gamma, size=(p, n_batches))\n        gamma = np.exp(log_gamma)\n\n        # Noise\n        epsilon = rng.normal(0, sigma_eps, size=(n_samples, p))\n\n        # Construct data matrix X\n        X = np.zeros((n_samples, p))\n        for i in range(n_samples):\n            batch_idx = batches[i] - 1\n            X[i, :] = mu + s_vec * y[i] + delta[:, batch_idx] + gamma[:, batch_idx] * epsilon[i, :]\n        \n        return X, y, batches\n\n    def combat_fit(X_train, batches_train):\n        \"\"\"Estimates ComBat parameters from training data.\"\"\"\n        train_batches_unique = np.unique(batches_train)\n        p = X_train.shape[1]\n\n        # Global stats\n        mu_g = np.mean(X_train, axis=0)\n        sigma_g = np.std(X_train, axis=0, ddof=1)\n        # Add a small epsilon for stability if std dev is zero\n        sigma_g[sigma_g == 0] = 1e-8\n\n        # Standardize data\n        Z = (X_train - mu_g) / sigma_g\n\n        # Batch-specific stats\n        m_gb = np.zeros((p, len(train_batches_unique)))\n        s_gb = np.ones((p, len(train_batches_unique)))\n        \n        batch_map = {batch_id: i for i, batch_id in enumerate(train_batches_unique)}\n\n        for batch_id, i in batch_map.items():\n            idx = (batches_train == batch_id)\n            n_b = np.sum(idx)\n            if n_b  0:\n                Z_b = Z[idx, :]\n                m_gb[:, i] = np.mean(Z_b, axis=0)\n                if n_b  1:\n                    s_gb[:, i] = np.std(Z_b, axis=0, ddof=1)\n        # Add a small epsilon for stability\n        s_gb[s_gb == 0] = 1e-8\n\n        return {'mu_g': mu_g, 'sigma_g': sigma_g, 'm_gb': m_gb, 's_gb': s_gb, 'batch_map': batch_map}\n    \n    def combat_transform(X, batches, params):\n        \"\"\"Applies ComBat correction using pre-fitted parameters.\"\"\"\n        mu_g, sigma_g, m_gb, s_gb, batch_map = params['mu_g'], params['sigma_g'], params['m_gb'], params['s_gb'], params['batch_map']\n        \n        X_adj = np.copy(X)\n        Z = (X - mu_g) / sigma_g\n\n        for b_id in np.unique(batches):\n            if b_id in batch_map:\n                idx = (batches == b_id)\n                batch_col_idx = batch_map[b_id]\n                \n                Z_adj_b = (Z[idx, :] - m_gb[:, batch_col_idx]) / s_gb[:, batch_col_idx]\n                X_adj[idx, :] = Z_adj_b * sigma_g + mu_g\n        \n        return X_adj\n\n    def svm_sgd(X_train, y_train, lambda_, eta, epochs, batch_size, seed):\n        \"\"\"Trains a linear SVM with squared hinge loss via SGD.\"\"\"\n        rng = np.random.default_rng(seed)\n        n, p = X_train.shape\n        w = np.zeros(p)\n        b = 0.0\n\n        indices = np.arange(n)\n        for _ in range(epochs):\n            rng.shuffle(indices)\n            for i in range(0, n, batch_size):\n                batch_indices = indices[i:i + batch_size]\n                X_batch, y_batch = X_train[batch_indices], y_train[batch_indices]\n                \n                margins = y_batch * (X_batch @ w + b)\n                \n                # Identify samples with margin  1\n                misclassified_mask = margins  1\n                \n                if np.any(misclassified_mask):\n                    y_mis = y_batch[misclassified_mask]\n                    X_mis = X_batch[misclassified_mask]\n                    one_minus_m = 1 - margins[misclassified_mask]\n\n                    grad_w = lambda_ * w - (2 / len(X_batch)) * np.sum( (y_mis * one_minus_m)[:, np.newaxis] * X_mis, axis=0)\n                    grad_b = -(2 / len(X_batch)) * np.sum(y_mis * one_minus_m)\n                else:\n                    grad_w = lambda_ * w\n                    grad_b = 0\n\n                w -= eta * grad_w\n                b -= eta * grad_b\n        return w, b\n\n    def predict(X, w, b):\n        \"\"\"Makes predictions using the trained SVM.\"\"\"\n        return np.sign(X @ w + b)\n\n    def accuracy(y_true, y_pred):\n        \"\"\"Calculates classification accuracy.\"\"\"\n        return np.mean(y_true == y_pred)\n\n    def create_stratified_folds(y, k, seed):\n        \"\"\"Creates stratified K-folds.\"\"\"\n        rng = np.random.default_rng(seed)\n        pos_indices = np.where(y == 1)[0]\n        neg_indices = np.where(y == -1)[0]\n        rng.shuffle(pos_indices)\n        rng.shuffle(neg_indices)\n        \n        pos_folds = np.array_split(pos_indices, k)\n        neg_folds = np.array_split(neg_indices, k)\n        \n        folds = []\n        for i in range(k):\n            test_indices = np.concatenate((pos_folds[i], neg_folds[i]))\n            folds.append(test_indices)\n            \n        return folds\n\n    def run_cv_pipeline(X, y, batches, folds, p, lambda_, eta, epochs, batch_size, cv_seed, pipeline_mode, full_data_params=None):\n        \"\"\"Runs the full CV pipeline.\"\"\"\n        accuracies = []\n        n_samples = len(y)\n        all_indices = np.arange(n_samples)\n\n        for test_indices in folds:\n            train_indices = np.setdiff1d(all_indices, test_indices)\n\n            X_train, y_train, batches_train = X[train_indices], y[train_indices], batches[train_indices]\n            X_test, y_test, batches_test = X[test_indices], y[test_indices], batches[test_indices]\n            \n            if pipeline_mode == 'strict':\n                params = combat_fit(X_train, batches_train)\n            elif pipeline_mode == 'leaky':\n                params = full_data_params\n            else:\n                raise ValueError(\"Invalid pipeline mode\")\n\n            X_train_adj = combat_transform(X_train, batches_train, params)\n            X_test_adj = combat_transform(X_test, batches_test, params)\n            \n            # Train SVM\n            w, b = svm_sgd(X_train_adj, y_train, lambda_, eta, epochs, batch_size, cv_seed)\n            \n            # Evaluate\n            y_pred = predict(X_test_adj, w, b)\n            acc = accuracy(y_test, y_pred)\n            accuracies.append(acc)\n            \n        return np.mean(accuracies)\n\n    # Common parameters\n    p = 50\n    p_sig = 20\n    s_val = 1.0\n    sigma_eps = 0.5\n    sigma_delta = 1.0\n    sigma_gamma = 0.3\n    data_seed = 2025\n    \n    lambda_ = 1e-3\n    eta = 0.05\n    epochs = 20\n    batch_size = 32\n    k_folds = 5\n\n    results = []\n\n    # --- Case 1 ---\n    batch_sizes_1 = [40, 40, 40]\n    X1, y1, batches1 = generate_data(batch_sizes_1, p, p_sig, s_val, sigma_eps, sigma_delta, sigma_gamma, data_seed)\n    folds1 = create_stratified_folds(y1, k_folds, data_seed)\n    acc1 = run_cv_pipeline(X1, y1, batches1, folds1, p, lambda_, eta, epochs, batch_size, data_seed, 'strict')\n    results.append(acc1)\n\n    # --- Case 2 ---\n    batch_sizes_2 = [50, 50, 10]\n    X2, y2, batches2 = generate_data(batch_sizes_2, p, p_sig, s_val, sigma_eps, sigma_delta, sigma_gamma, data_seed)\n    \n    # Custom fold generation for Case 2\n    rng_case2 = np.random.default_rng(data_seed)\n    b3_indices = np.where(batches2 == 3)[0]\n    b12_indices = np.where(batches2 != 3)[0]\n    y_b12 = y2[b12_indices]\n\n    b12_pos = b12_indices[y_b12 == 1]\n    b12_neg = b12_indices[y_b12 == -1]\n    rng_case2.shuffle(b12_pos)\n    rng_case2.shuffle(b12_neg)\n\n    # 12 extra samples for fold 1's test set\n    test1_extra_indices = np.concatenate((b12_pos[:6], b12_neg[:6]))\n    test_fold1 = np.concatenate((b3_indices, test1_extra_indices))\n    \n    remaining_b12_pos = b12_pos[6:]\n    remaining_b12_neg = b12_neg[6:]\n    remaining_b12_y = np.concatenate((np.ones(len(remaining_b12_pos)), -1*np.ones(len(remaining_b12_neg))))\n    \n    other_folds = create_stratified_folds(remaining_b12_y, k_folds - 1, data_seed)\n    \n    remaining_b12_indices = np.concatenate((remaining_b12_pos, remaining_b12_neg))\n    \n    folds2 = [test_fold1]\n    for fold in other_folds:\n        folds2.append(remaining_b12_indices[fold])\n\n    acc2 = run_cv_pipeline(X2, y2, batches2, folds2, p, lambda_, eta, epochs, batch_size, data_seed, 'strict')\n    results.append(acc2)\n\n    # --- Case 3 ---\n    # Re-use data and folds from Case 1\n    # Pipeline_strict is just acc1\n    acc_strict = acc1\n    \n    # Pipeline_leak\n    leaky_params = combat_fit(X1, batches1)\n    acc_leak = run_cv_pipeline(X1, y1, batches1, folds1, p, lambda_, eta, epochs, batch_size, data_seed, 'leaky', full_data_params=leaky_params)\n    \n    diff = acc_leak - acc_strict\n    results.append(diff)\n    \n    # Print final output\n    print(f\"[{','.join([f'{r:.4f}' for r in results])}]\")\n\nsolve()\n\n```"
        }
    ]
}