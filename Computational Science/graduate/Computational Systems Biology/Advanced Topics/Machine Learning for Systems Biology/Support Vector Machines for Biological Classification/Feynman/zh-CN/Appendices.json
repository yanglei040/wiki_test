{
    "hands_on_practices": [
        {
            "introduction": "支持向量机（SVM）的核心思想是找到一个能以最大“间隔”将不同类别的数据点分离开的决策边界。这个动手练习将带你回到最初的起点，通过一个简单但具有代表性的二维数据集，手动计算硬间隔SVM的参数。通过这个过程，你将直观地理解最大间隔超平面的几何意义，并掌握其基本优化原理。",
            "id": "3353372",
            "problem": "一个计算系统生物学小组正在训练一个线性支持向量机（SVM）分类器，以根据两个转录组学特征来区分两种细胞命运：转录因子 $X_{1}$ 和 $X_{2}$ 相对于基线的对数转换表达偏差。他们收集了 $6$ 个带标签的样本：正标签类别（$+1$）的特征向量为 $(2,2)$、$(2,0)$ 和 $(0,2)$，负标签类别（$-1$）的特征向量为 $(-2,-2)$、$(-2,0)$ 和 $(0,-2)$。假设数据是严格线性可分的，并且分类器是 $\\mathbb{R}^{2}$ 中的硬间隔最大间隔线性支持向量机。\n\n仅使用硬间隔线性SVM的核心定义（在满足单位函数间隔下正确分类的约束条件下，最小化权重向量的欧几里得范数的平方）和关于欧几里得范数和间隔的标准几何事实，确定唯一最大间隔分离超平面参数 $(w,b)$，其中 $w \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}$，以及几何间隔宽度 $2/\\|w\\|$。请将最终结果报告为单个行向量 $(w_{1},w_{2},b,\\gamma)$，其中 $\\gamma=2/\\|w\\|$。\n\n提供精确值，无需四舍五入。",
            "solution": "该问题被评估为有效。这是一个适定、自洽且具有科学依据的机器学习领域问题，具体涉及支持向量机（SVM）。\n\n### 第1步：提取已知条件\n- **任务**：确定硬间隔线性SVM的参数 $(w, b)$ 和几何间隔宽度 $\\gamma$。\n- **特征空间**：$\\mathbb{R}^{2}$，特征为 $X_1$ 和 $X_2$。\n- **数据点**：一组 $N=6$ 个带标签的样本 $(x_i, y_i)$。\n- **正类别 ($y_i = +1$)**：\n  - $x_1 = (2, 2)$\n  - $x_2 = (2, 0)$\n  - $x_3 = (0, 2)$\n- **负类别 ($y_i = -1$)**：\n  - $x_4 = (-2, -2)$\n  - $x_5 = (-2, 0)$\n  - $x_6 = (0, -2)$\n- **模型**：硬间隔最大间隔线性SVM。分离超平面由 $w \\cdot x + b = 0$ 定义，其中 $w = (w_1, w_2) \\in \\mathbb{R}^2$ 且 $b \\in \\mathbb{R}$。\n- **输出格式**：将结果报告为单个行向量 $(w_1, w_2, b, \\gamma)$，其中 $\\gamma = 2/\\|w\\|$。\n\n### 第2步：使用提取的已知条件进行验证\n问题是有效的。\n- **科学依据**：该问题是应用硬间隔SVM的一个典型例子，这是机器学习和计算生物学中的一个基本概念。所有原理和定义都是标准的。\n- **适定性**：数据已指定，通过目视检查，它们是线性可分的。在线性可分数据集上，硬间隔SVM的优化问题是一个凸优化问题，这保证了最大间隔超平面存在唯一解。\n- **目标**：问题以精确的数学语言陈述，没有歧义或主观因素。\n- **完整性**：所有必要的数据点及其标签都已提供。\n\n### 第3步：推导解答\n硬间隔线性SVM的目标是找到一个分离超平面 $w \\cdot x + b = 0$，该超平面最大化几何间隔，这等价于最小化权重向量的欧几里得范数的平方 $\\|w\\|^2$。优化问题被表述为：\n$$\n\\begin{aligned}\n \\underset{w, b}{\\text{minimize}}\n  \\frac{1}{2} \\|w\\|^2 \\\\\n \\text{subject to}\n  y_i(w \\cdot x_i + b) \\ge 1, \\quad \\text{for } i = 1, \\dots, 6\n\\end{aligned}\n$$\n约束条件确保所有数据点都被正确分类，并且函数间隔至少为 $1$。\n\n设 $w = (w_1, w_2)$。这 $6$ 个约束条件是：\n对于 $y_i = +1$：\n1. $1 \\cdot (w_1(2) + w_2(2) + b) \\ge 1 \\implies 2w_1 + 2w_2 + b \\ge 1$\n2. $1 \\cdot (w_1(2) + w_2(0) + b) \\ge 1 \\implies 2w_1 + b \\ge 1$\n3. $1 \\cdot (w_1(0) + w_2(2) + b) \\ge 1 \\implies 2w_2 + b \\ge 1$\n\n对于 $y_i = -1$：\n4. $-1 \\cdot (w_1(-2) + w_2(-2) + b) \\ge 1 \\implies 2w_1 + 2w_2 - b \\ge 1$\n5. $-1 \\cdot (w_1(-2) + w_2(0) + b) \\ge 1 \\implies 2w_1 - b \\ge 1$\n6. $-1 \\cdot (w_1(0) + w_2(-2) + b) \\ge 1 \\implies 2w_2 - b \\ge 1$\n\n我们可以利用数据的对称性来简化问题。数据集关于交换坐标 $X_1$ 和 $X_2$ 是对称的。这表明最优权重向量 $w$ 的分量应该相等，即 $w_1 = w_2$。\n数据集也关于原点对称：对于类别 $y_i$ 中的每个点 $x_i$，点 $-x_i$ 属于类别 $-y_i$。这表明分离超平面应该穿过原点，这意味着偏置项 $b=0$。\n\n让我们通过设置 $w_1 = w_2 = w_c$ 和 $b=0$ 来检验这个假设。优化问题变为最小化 $\\|w\\|^2 = w_c^2 + w_c^2 = 2w_c^2$，并服从简化的约束条件。\n将 $w_1 = w_2 = w_c$ 和 $b=0$ 代入约束条件：\n1. $2w_c + 2w_c \\ge 1 \\implies 4w_c \\ge 1$\n2. $2w_c \\ge 1$\n3. $2w_c \\ge 1$\n4. $2w_c + 2w_c \\ge 1 \\implies 4w_c \\ge 1$\n5. $2w_c \\ge 1$\n6. $2w_c \\ge 1$\n\n所有约束都简化为单一条件 $2w_c \\ge 1$，即 $w_c \\ge \\frac{1}{2}$。约束 $4w_c \\ge 1$（即 $w_c \\ge \\frac{1}{4}$）是多余的。\n我们必须在 $w_c \\ge \\frac{1}{2}$ 的约束下最小化目标函数 $2w_c^2$。由于当 $w_c > 0$ 时目标函数是单调递增的，所以最小值在可行域的边界处取得，即当 $w_c$ 取其可能的最小值时。\n因此，最优值为 $w_c = \\frac{1}{2}$。\n\n这给出了权重向量和偏置的唯一解：\n$w_1 = \\frac{1}{2}$\n$w_2 = \\frac{1}{2}$\n$b = 0$\n\n权重向量是 $w = (\\frac{1}{2}, \\frac{1}{2})$。分离超平面是 $\\frac{1}{2}X_1 + \\frac{1}{2}X_2 = 0$，或 $X_1 + X_2 = 0$。\n\n让我们根据原始约束条件验证此解。\n当 $w=(\\frac{1}{2}, \\frac{1}{2})$ 且 $b=0$ 时：\n1. 对于 $x_1=(2,2)$： $y_1(w \\cdot x_1 + b) = 1(\\frac{1}{2}(2) + \\frac{1}{2}(2) + 0) = 1(1+1) = 2 \\ge 1$。（正确）\n2. 对于 $x_2=(2,0)$： $y_2(w \\cdot x_2 + b) = 1(\\frac{1}{2}(2) + \\frac{1}{2}(0) + 0) = 1(1) = 1 \\ge 1$。（正确，位于间隔边界上）\n3. 对于 $x_3=(0,2)$： $y_3(w \\cdot x_3 + b) = 1(\\frac{1}{2}(0) + \\frac{1}{2}(2) + 0) = 1(1) = 1 \\ge 1$。（正确，位于间隔边界上）\n4. 对于 $x_4=(-2,-2)$： $y_4(w \\cdot x_4 + b) = -1(\\frac{1}{2}(-2) + \\frac{1}{2}(-2) + 0) = -1(-1-1) = 2 \\ge 1$。（正确）\n5. 对于 $x_5=(-2,0)$： $y_5(w \\cdot x_5 + b) = -1(\\frac{1}{2}(-2) + \\frac{1}{2}(0) + 0) = -1(-1) = 1 \\ge 1$。（正确，位于间隔边界上）\n6. 对于 $x_6=(0,-2)$： $y_6(w \\cdot x_6 + b) = -1(\\frac{1}{2}(0) + \\frac{1}{2}(-2) + 0) = -1(-1) = 1 \\ge 1$。（正确，位于间隔边界上）\n\n等式 $y_i(w \\cdot x_i + b) = 1$ 成立的点是支持向量：$(2,0), (0,2), (-2,0), (0,-2)$。该解是一致的。\n\n最后一步是计算几何间隔宽度 $\\gamma = \\frac{2}{\\|w\\|}$。\n首先，计算 $w$ 的欧几里得范数：\n$$ \\|w\\| = \\sqrt{w_1^2 + w_2^2} = \\sqrt{(\\frac{1}{2})^2 + (\\frac{1}{2})^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{2}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2} $$\n现在，计算间隔宽度 $\\gamma$：\n$$ \\gamma = \\frac{2}{\\|w\\|} = \\frac{2}{\\frac{\\sqrt{2}}{2}} = \\frac{4}{\\sqrt{2}} = \\frac{4\\sqrt{2}}{2} = 2\\sqrt{2} $$\n\n所需参数为 $w_1 = \\frac{1}{2}$，$w_2 = \\frac{1}{2}$，$b=0$ 和 $\\gamma = 2\\sqrt{2}$。最终结果是行向量 $(w_1, w_2, b, \\gamma)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  \\frac{1}{2}  0  2\\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理论上的硬间隔在面对含有噪声或重叠的真实生物数据时往往过于严苛。本练习将引导你从理想走向现实，为一组非线性可分的数据点实现一个软间隔SVM分类器。你将通过编程实践，深入探索正则化参数 $C$ 如何调控模型在最大化间隔和容忍错分样本之间的权衡，并亲手计算松弛变量 $\\xi_i$ 以量化间隔的违背程度。",
            "id": "3353409",
            "problem": "您的任务是构建一个完整、可运行的程序，通过训练支持向量机 (SVM)，确定一个双基因生物分类问题的最优线性分类器。背景是计算系统生物学，其中基因表达特征用于将样本分类为阳性表型和阴性表型。特征是双基因表达向量 $x = (G_1, G_2)$，其中 $G_1$ 和 $G_2$ 表示归一化的基因表达测量值，由于高通量分析中典型的中心化和缩放，这些值可能为正或负。\n\n您必须实现的推导基础是带有基于间隔的损失的正则化经验风险最小化框架、线性分类中的间隔定义以及通过松弛变量惩罚错误分类的思想。必须从这些原则出发推导出线性决策边界，而不能依赖问题陈述中给出的快捷公式。\n\n给定两类样本：阳性表型和阴性表型。所有测试用例的数据点都是固定的，并按以下顺序排列：\n- 标记为 $+1$ 的阳性表型样本：$x_1 = (2, 2)$，$x_2 = (1, 0)$，$x_3 = (2, 1)$。\n- 标记为 $-1$ 的阴性表型样本：$x_4 = (-2, -2)$，$x_5 = (-1, 0.5)$，$x_6 = (-2, -1)$。\n\n样本顺序严格为 $\\{x_1, x_2, x_3, x_4, x_5, x_6\\}$，在报告松弛变量时必须保留。分类规则必须是形式为 $f(x) = w_1 G_1 + w_2 G_2 + b$ 的线性决策函数，松弛变量定义为 $\\{\\xi_i\\}_{i=1}^6$ 以量化对间隔的违反。\n\n对于下面的每个测试用例，您的程序必须构建最优分离超平面参数 $(w_1, w_2, b)$ 和最优松弛变量 $\\xi_i$，这些参数是在具有指定正则化参数 $C$ 的线性软间隔支持向量机下得到的。底层的优化问题必须足够精确地求解，以揭示决策边界和松弛变量。所有数值输出必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 测试用例 $1$：$C = 1$。\n- 测试用例 $2$：$C = 0.25$。\n- 测试用例 $3$：$C = 5$。\n\n对于每个测试用例，按顺序计算并报告以下内容：\n- 首先是决策边界系数 $w_1$、$w_2$ 和 $b$，为实数。\n- 然后是对应于有序样本 $\\{x_1, x_2, x_3, x_4, x_5, x_6\\}$ 的六个松弛变量 $\\xi_1, \\xi_2, \\xi_3, \\xi_4, \\xi_5, \\xi_6$。\n\n所有报告的量都必须是四舍五入到 $6$ 位小数的实数。不涉及物理单位、角度单位或百分比。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表按所列顺序汇总了三个测试用例的输出。具体来说，最终输出应为一个包含 $27$ 个数字的扁平列表：\n$[w_1^{(1)}, w_2^{(1)}, b^{(1)}, \\xi_1^{(1)}, \\ldots, \\xi_6^{(1)}, w_1^{(2)}, w_2^{(2)}, b^{(2)}, \\xi_1^{(2)}, \\ldots, \\xi_6^{(2)}, w_1^{(3)}, w_2^{(3)}, b^{(3)}, \\xi_1^{(3)}, \\ldots, \\xi_6^{(3)}]$，\n其中上标 $(k)$ 表示测试用例 $k$。\n\n在整个过程中必须保持科学真实性和合理性，并且算法应从上述基本原理推导得出，不使用本问题陈述中提供的快捷公式。所有数学实体，包括所有数字，在您的推理中必须以 LaTeX 格式书写。",
            "solution": "我们从二元分类的正则化经验风险最小化原则出发，寻求一个线性决策函数 $f(x) = w_1 G_1 + w_2 G_2 + b$ 来分离两个类别。对于一个带标签的样本 $(x_i, y_i)$，其中 $y_i \\in \\{+1, -1\\}$，其间隔定义为 $y_i f(x_i) = y_i (w^\\top x_i + b)$，其中 $w = (w_1, w_2)$，$x_i = (G_{1,i}, G_{2,i})$。间隔的概念鼓励正确分类的样本获得大的正值，并惩罚那些未达到足够间隔的样本。\n\n为了引入对不可分数据的鲁棒性，我们引入松弛变量 $\\xi_i \\geq 0$ 以允许违反间隔。经典的线性软间隔支持向量机 (SVM) 是通过在最大化间隔和合页损失惩罚之间进行权衡而推导出来的。样本 $i$ 的合页损失为 $\\max(0, 1 - y_i (w^\\top x_i + b))$，总的正则化经验风险将此损失与对 $w$ 的二次正则化相结合，以控制模型容量。\n\n线性软间隔 SVM 的原始优化问题是：\n$$\n\\min_{w, b, \\xi} \\quad \\frac{1}{2} \\| w \\|_2^2 + C \\sum_{i=1}^n \\xi_i\n$$\n约束条件为：\n$$\ny_i (w^\\top x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad i = 1, \\ldots, n,\n$$\n其中 $C > 0$ 是正则化参数，控制间隔宽度和合页损失惩罚之间的权衡，$n$ 是训练样本的数量。在我们的设置中，$n = 6$，样本顺序固定为 $\\{x_1, x_2, x_3, x_4, x_5, x_6\\}$，标签为 $y_1 = y_2 = y_3 = +1$ 和 $y_4 = y_5 = y_6 = -1$。\n\n为了高效且精确地计算最优解，我们使用拉格朗日乘子法推导不等式约束的对偶问题。为 $y_i (w^\\top x_i + b) \\geq 1 - \\xi_i$ 引入乘子 $\\alpha_i \\geq 0$，为 $\\xi_i \\geq 0$ 引入乘子 $\\mu_i \\geq 0$。Karush-Kuhn-Tucker (KKT) 条件导出了对偶形式，其中变量 $\\alpha_i$ 被约束在 $0 \\leq \\alpha_i \\leq C$。通过平稳性条件消去 $w$ 和 $b$，对偶目标变为：\n$$\n\\max_{\\alpha \\in \\mathbb{R}^n} \\quad \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle\n$$\n服从等式约束：\n$$\n\\sum_{i=1}^n \\alpha_i y_i = 0\n$$\n和箱式约束：\n$$\n0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\ldots, n.\n$$\n\n定义格拉姆矩阵 (Gram matrix) $K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = \\langle x_i, x_j \\rangle$，以及 $Q_{ij} = y_i y_j K_{ij}$，对偶目标可以紧凑地写成：\n$$\n\\max_{\\alpha} \\quad \\mathbf{1}^\\top \\alpha - \\frac{1}{2} \\alpha^\\top Q \\alpha\n$$\n约束条件为：\n$$\ny^\\top \\alpha = 0, \\quad 0 \\leq \\alpha \\leq C \\mathbf{1}.\n$$\n我们可以等价地最小化：\n$$\n\\min_{\\alpha} \\quad \\frac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha\n$$\n在相同的约束下。\n\n一旦找到最优的 $\\alpha^\\star$，原始参数可通过以下方式恢复：\n$$\nw^\\star = \\sum_{i=1}^n \\alpha_i^\\star y_i x_i,\n$$\n偏置项 $b^\\star$ 从 KKT 条件计算得出。对于任何满足 $0  \\alpha_i^\\star  C$ 的样本（一个“自由”支持向量），KKT 互补松弛性意味着：\n$$\ny_i (w^{\\star\\top} x_i + b^\\star) = 1,\n$$\n所以对于这类样本：\n$$\nb^\\star = y_i - w^{\\star\\top} x_i.\n$$\n如果存在多个自由支持向量，对它们所蕴含的 $b^\\star$ 值取平均在数值上是鲁棒的。如果没有自由支持向量（所有 $\\alpha_i^\\star$ 都在边界 $0$ 或 $C$ 上），可以从所有样本的间隔约束中推导出一个一致的 $b^\\star$ 区间：\n$$\nb^\\star \\in \\bigcap_{i: y_i = +1} (-\\infty, 1 - w^{\\star\\top} x_i] \\ \\cap \\ \\bigcap_{i: y_i = -1} [-1 - w^{\\star\\top} x_i, \\infty).\n$$\n选择区间 $[b_{\\text{low}}, b_{\\text{high}}]$ 的中点，其中\n$$\nb_{\\text{low}} = \\max_{i: y_i = -1} \\left(-1 - w^{\\star\\top} x_i \\right), \\quad b_{\\text{high}} = \\min_{i: y_i = +1} \\left(1 - w^{\\star\\top} x_i \\right),\n$$\n即使在退化情况下也能确保数值稳定性。\n\n最后，松弛变量直接从合页损失定义计算：\n$$\n\\xi_i^\\star = \\max \\left( 0, 1 - y_i \\left( w^{\\star\\top} x_i + b^\\star \\right) \\right), \\quad i = 1, \\ldots, n.\n$$\n\n算法规划：\n- 为固定的样本顺序构建 $X \\in \\mathbb{R}^{6 \\times 2}$ 和 $y \\in \\mathbb{R}^6$。\n- 对于每个给定 $C$ 的测试用例：\n  - 构造格拉姆矩阵 $K$ 和矩阵 $Q$，其中 $Q_{ij} = y_i y_j K_{ij}$。\n  - 使用约束优化器，在约束 $y^\\top \\alpha = 0$ 和 $0 \\leq \\alpha \\leq C \\mathbf{1}$ 下，最小化对偶目标 $\\frac{1}{2} \\alpha^\\top Q \\alpha - \\mathbf{1}^\\top \\alpha$。优化器使用解析梯度 $\\nabla f(\\alpha) = Q \\alpha - \\mathbf{1}$ 和等式约束的雅可比矩阵 $\\nabla (y^\\top \\alpha) = y$。\n  - 通过 $w^\\star = \\sum_i \\alpha_i^\\star y_i x_i$ 从 $\\alpha^\\star$ 恢复 $w^\\star$。\n  - 如果有自由支持向量，则使用它们计算 $b^\\star$；否则使用所述的区间中点法。\n  - 按固定的样本顺序计算 $\\xi_i^\\star = \\max(0, 1 - y_i (w^{\\star\\top} x_i + b^\\star))$。\n- 将所有报告值四舍五入到 $6$ 位小数。\n- 将每个测试用例的输出聚合为 $(w_1, w_2, b, \\xi_1, \\xi_2, \\xi_3, \\xi_4, \\xi_5, \\xi_6)$，并将三个测试用例的结果连接起来。\n- 打印最终的单行方括号逗号分隔列表。\n\n此过程基于间隔、合页损失、正则化和凸优化对偶理论的定义，确保了科学合理性和数学一致性。它遵循问题陈述中描述的约束和预期输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef svm_linear_soft_margin(X, y, C):\n    \"\"\"\n    Train a linear soft-margin SVM using the dual problem with a linear kernel.\n    Returns w (2,), b (scalar), xi (n,), alpha (n,).\n    \"\"\"\n    n, d = X.shape\n    # Gram matrix K_ij = x_i . x_j\n    K = X @ X.T\n    # Q_ij = y_i y_j K_ij\n    Q = (y[:, None] * y[None, :]) * K\n\n    # Objective: 0.5 * alpha^T Q alpha - 1^T alpha\n    def objective(alpha):\n        return 0.5 * alpha.dot(Q).dot(alpha) - np.sum(alpha)\n\n    # Gradient: Q alpha - 1\n    def grad(alpha):\n        return Q.dot(alpha) - np.ones(n)\n\n    # Equality constraint: y^T alpha = 0\n    constraints = {\n        'type': 'eq',\n        'fun': lambda a: float(np.dot(y, a)),\n        'jac': lambda a: y.astype(float)\n    }\n\n    # Bounds: 0 = alpha_i = C\n    bounds = [(0.0, float(C))] * n\n\n    # Initial guess: small values satisfying equality constraint\n    alpha0 = np.zeros(n, dtype=float)\n    pos_idx = np.where(y > 0)[0]\n    neg_idx = np.where(y  0)[0]\n    if len(pos_idx) > 0 and len(neg_idx) > 0:\n        a0 = min(0.1, C / 10.0)\n        alpha0[pos_idx[0]] = a0\n        alpha0[neg_idx[0]] = a0\n\n    res = minimize(\n        objective, alpha0, jac=grad, bounds=bounds, constraints=constraints,\n        method='SLSQP', options={'maxiter': 1000, 'ftol': 1e-9, 'disp': False}\n    )\n    alpha = res.x\n\n    # Compute w = sum_i alpha_i y_i x_i\n    w = (alpha * y) @ X  # shape (2,)\n\n    # Compute b from KKT:\n    tol = 1e-6\n    free_sv = np.where((alpha > tol)  (alpha  C - tol))[0]\n    if free_sv.size > 0:\n        b_vals = y[free_sv] - X[free_sv].dot(w)\n        b = float(np.mean(b_vals))\n    else:\n        # Interval method over all samples\n        b_low = -np.inf\n        b_high = np.inf\n        for i in range(n):\n            wx = float(np.dot(w, X[i]))\n            if y[i] == 1:\n                b_high = min(b_high, 1.0 - wx)\n            else:\n                b_low = max(b_low, -1.0 - wx)\n        if not np.isfinite(b_low):\n            b_low = -1e6\n        if not np.isfinite(b_high):\n            b_high = 1e6\n        b = 0.5 * (b_low + b_high)\n\n    # Slack variables: xi_i = max(0, 1 - y_i (w^T x_i + b))\n    margins = y * (X.dot(w) + b)\n    xi = np.maximum(0.0, 1.0 - margins)\n\n    return w, b, xi, alpha\n\ndef solve():\n    # Fixed dataset in specified order:\n    # Positive: (2,2), (1,0), (2,1); Negative: (-2,-2), (-1,0.5), (-2,-1)\n    X = np.array([\n        [2.0, 2.0],\n        [1.0, 0.0],\n        [2.0, 1.0],\n        [-2.0, -2.0],\n        [-1.0, 0.5],\n        [-2.0, -1.0],\n    ], dtype=float)\n    y = np.array([1, 1, 1, -1, -1, -1], dtype=float)\n\n    # Test cases: C values\n    test_cases = [1.0, 0.25, 5.0]\n\n    results = []\n    for C in test_cases:\n        w, b, xi, _ = svm_linear_soft_margin(X, y, C)\n        # Order: w1, w2, b, xi1..xi6\n        case_values = [w[0], w[1], b] + list(xi)\n        # Round to 6 decimals for output stability\n        case_values = [float(f\"{v:.6f}\") for v in case_values]\n        results.extend(case_values)\n\n    # Final print statement in the exact required format.\n    print(\"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在计算系统生物学中，将机器学习模型应用于真实的组学数据，意味着必须处理系统性的技术变异，例如“批次效应”。这个综合性练习要求你设计并实现一个完整的分析流程，该流程不仅要矫正多队列数据中的批次效应，还必须在交叉验证的框架内严格执行，以防止“数据泄漏”。通过解决这个贴近科研实践的挑战，你将学会如何构建严谨、可信的生物信息学分类模型，并深刻理解避免数据预处理中偏差的重要性。",
            "id": "3353416",
            "problem": "给定一个计算系统生物学中的场景，其中多队列（multi-cohort）组学测量数据被用于支持向量机（SVM）进行二元生物学分类。队列对应于实验室批次，每个批次都可能在测量的特征上留下系统的位置和尺度偏差（即批次效应）。您的任务是形式化批次效应的产生方式，遵循参数化经验贝叶斯（EB）视角推导一个位置-尺度批次校正程序，并实施一个严格的训练流程，该流程严格在训练折（training folds）内执行批次校正，以防止在SVM拟合前发生数据泄露。然后，在多种测试条件下，对合成的多队列组学数据评估此流程。\n\n基本和生成模型：\n- 组学特征是受生物学条件和技术效应影响的分子丰度的测量值。设 $p$ 表示特征数量，$n$ 表示样本数量。每个样本 $i$ 属于一个批次 $B_i \\in \\{1,\\dots, B\\}$，并有一个二元类别标签 $Y_i \\in \\{-1, +1\\}$。\n- 对于样本 $i$ 中的特征 $g \\in \\{1,\\dots, p\\}$，一个简化且有科学依据的测量模型是\n$$\nX_{g,i} \\;=\\; \\mu_g \\;+\\; s_g Y_i \\;+\\; \\delta_{g, B_i} \\;+\\; \\gamma_{g, B_i} \\,\\varepsilon_{g,i},\n$$\n其中 $\\mu_g$ 是特征 $g$ 的基线丰度，$s_g$ 是特征 $g$ 的生物信号载荷，$\\delta_{g,b}$ 是批次 $b$ 中特征 $g$ 的特定于批次的位置（加性）效应，$\\gamma_{g,b}$ 是特定于批次的尺度（乘性）效应，$\\varepsilon_{g,i}$ 是均值为零、方差有限的独立噪声。该模型捕捉了批次效应的产生方式：$\\delta_{g,b}$ 和 $\\gamma_{g,b}$ 系统地改变了跨批次的特征分布，如果不进行校正，可能会混淆下游的分类器。\n\n批次校正原理：\n- 参数化EB ComBat风格的校正通过标准化特征、在训练数据中估计特定于批次的位置和尺度参数，并使用这些从训练中派生的参数来调整训练和测试特征，从而消除批次效应。对于每个特征 $g$，定义全局训练均值和标准差：\n$$\n\\hat{\\mu}_g \\;=\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_{g,i}, \\qquad\n\\hat{\\sigma}_g \\;=\\; \\sqrt{ \\frac{1}{n_{\\text{train}}-1} \\sum_{i \\in \\text{train}} (X_{g,i} - \\hat{\\mu}_g)^2 }.\n$$\n通过 $Z_{g,i} = (X_{g,i} - \\hat{\\mu}_g)/\\hat{\\sigma}_g$ 对训练和测试数据进行标准化。对于训练集中存在的每个批次 $b$，估计特征 $g$ 的批次标准化位置和尺度：\n$$\n\\hat{m}_{g,b} \\;=\\; \\frac{1}{n_{b,\\text{train}}} \\sum_{i \\in \\text{train}, B_i=b} Z_{g,i}, \\qquad\n\\hat{s}_{g,b} \\;=\\; \\sqrt{ \\frac{1}{n_{b,\\text{train}}-1} \\sum_{i \\in \\text{train}, B_i=b} (Z_{g,i} - \\hat{m}_{g,b})^2 }.\n$$\n对于训练中存在的批次，通过 $Z_{g,i}^{\\text{adj}} = (Z_{g,i} - \\hat{m}_{g,B_i}) / \\hat{s}_{g,B_i}$ 调整训练和测试的标准化值；如果一个测试批次在训练中不存在，则不对该批次进行调整（恒等变换），或使用一个不依赖于测试数据的明确定义的备用方案。最后，使用全局训练统计量重新缩放回来：$X_{g,i}^{\\text{adj}} = Z_{g,i}^{\\text{adj}} \\hat{\\sigma}_g + \\hat{\\mu}_g$。这是一种位置-尺度ComBat风格的校正，没有EB收缩，并且严格在训练折内执行以防止数据泄露。\n\n分类器和优化目标：\n- 使用在线性SVM分类器，在原始（primal）问题中进行拟合，最小化正则化经验风险。设 $w \\in \\mathbb{R}^p$ 和 $b \\in \\mathbb{R}$。对于训练样本 $(x_i, y_i)$，平方合页损失（squared hinge loss）目标函数为\n$$\n\\min_{w,b} \\quad \\frac{\\lambda}{2}\\|w\\|_2^2 \\;+\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} \\left( \\max\\{0, 1 - y_i (w^\\top x_i + b)\\} \\right)^2,\n$$\n其中 $\\lambda > 0$ 是正则化参数。通过在小批量（mini-batches）上进行随机梯度下降（SGD）来优化此目标。对于一个训练样本 $(x_i, y_i)$，其间隔（margin）为 $m_i = y_i (w^\\top x_i + b)$，梯度贡献为\n$$\n\\nabla_w \\;\\leftarrow\\; \\lambda w \\;-\\; \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i x_i (1 - m_i), \\qquad\n\\nabla_b \\;\\leftarrow\\; - \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i (1 - m_i).\n$$\n使用学习率 $\\eta > 0$ 进行更新：$(w, b) \\leftarrow (w, b) - \\eta (\\nabla_w, \\nabla_b)$。\n\n数据泄露考量：\n- 一个依赖于整个数据集（包括测试样本）的预处理算子是一种数据依赖的转换。如果在交叉验证之前对所有样本估计批次校正参数，测试集的信息会影响训练转换，从而打破了训练集和测试集之间的独立性假设。形式上，设 $\\mathcal{T}$ 为训练集，$\\mathcal{S}$ 为测试集，一个依赖于 $\\mathcal{T} \\cup \\mathcal{S}$ 的数据依赖映射 $\\mathcal{P}$ 会改变以 $\\mathcal{S}$ 为条件的转换后 $\\mathcal{T}$ 的分布，这违反了标准的泛化设置并会夸大性能。\n\n任务：\n- 严格在训练折内实现上述ComBat风格的批次校正。\n- 使用所提供的目标函数，通过随机梯度下降法拟合一个使用平方合页损失的线性SVM。\n- 使用 $K=5$ 的K折交叉验证，并计算各折的平均分类准确率，其中准确率是正确分类的测试样本的比例，以小数形式表示（非百分比）。\n\n测试套件和要求输出：\n- 所有随机抽样必须由固定的伪随机种子控制，以确保确定性。使用种子 $2025$ 进行数据生成和任何随机优化。\n- 使用 $p=50$ 个特征和二元标签 $Y_i \\in \\{-1,+1\\}$，在每个批次内大致均等分配。\n- 对于所有情况，设置以下生成参数：所有 $g$ 的特征基线 $\\mu_g = 0$，信号特征数量 $p_{\\text{sig}} = 20$（特征 $1$ 到 $20$），对于 $g \\le p_{\\text{sig}}$ 信号强度 $s_g = s = 1.0$，否则 $s_g = 0$，噪声 $\\varepsilon_{g,i} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ 且 $\\sigma_\\varepsilon = 0.5$。对于每个批次 $b$ 和特征 $g$，从 $\\mathcal{N}(0, \\sigma_\\delta^2)$（其中 $\\sigma_\\delta = 1.0$）中抽取位置偏移 $\\delta_{g,b}$，从对数正态分布中抽取尺度效应 $\\gamma_{g,b}$，其对数尺度标准差为 $\\sigma_\\gamma = 0.3$，对数均值为零，即 $\\log \\gamma_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)$。\n- 使用SVM超参数：正则化 $\\lambda = 10^{-3}$，学习率 $\\eta = 0.05$，周期数 $E = 20$，小批量大小 $m = 32$。\n- 除非另有规定，否则通过对类别标签进行分层来构建折，并保持折之间不相交。当整个批次在训练折中缺失时，对于相应测试集中的该批次，不进行调整（恒等变换），而不是使用任何从测试中派生的量。\n\n提供三个测试案例：\n1. 案例1（均衡多队列）：$B=3$ 个批次，每个批次大小为 $[40, 40, 40]$ 个样本。执行5折交叉验证，在每个训练折内进行ComBat风格的校正，拟合SVM，并报告各折的平均测试准确率，结果为四位小数。\n2. 案例2（边缘案例：批次在训练中缺失）：$B=3$ 个批次，大小为 $[50, 50, 10]$。构建5折交叉验证，使得第1折的测试集包含来自小批次的所有10个样本，而其训练集中没有任何样本；对于其他折，使用分层分割。执行折内批次校正，并报告各折的平均测试准确率，结果为四位小数。\n3. 案例3（数据泄露敏感性）：重用案例1的数据。计算两个流程：流程 $\\mathcal{P}_{\\text{leak}}$，其中批次校正参数在整个数据集上拟合一次，并在交叉验证之前应用；以及流程 $\\mathcal{P}_{\\text{strict}}$，其中参数严格在训练折内拟合。报告在相同折上平均测试准确率的差异（流程 $\\mathcal{P}_{\\text{leak}}$ 减去流程 $\\mathcal{P}_{\\text{strict}}$），结果为四位小数。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含三个结果，形式为方括号括起来的逗号分隔列表（例如，$[0.8123,0.7450,0.0360]$）。条目必须按案例1、案例2和案例3的顺序排列。所有值必须是四舍五入到四位小数的小数。不应打印任何其他文本。",
            "solution": "该问题要求开发和评估一个用于多队列组学数据二元分类的计算流程。核心任务包括：基于包含生物信号和批次效应的指定生成模型来生成合成数据，实施一个有原则的批次校正程序，训练一个线性支持向量机（SVM）分类器，并使用交叉验证来评估整个流程。问题的关键在于在交叉验证框架内正确处理批次校正，以防止测试集数据泄露到训练过程中。解决方案分为三部分：数据模型和算法的形式化，交叉验证和评估策略的描述，以及针对三个指定测试案例的实施细节。\n\n首先，我们形式化底层的科学和数学原理。对于样本 $i \\in \\{1, \\dots, n\\}$ 中的特征 $g \\in \\{1, \\dots, p\\}$，其测量值 $X_{g,i}$ 的生成模型由下式给出：\n$$\nX_{g,i} \\;=\\; \\mu_g \\;+\\; s_g Y_i \\;+\\; \\delta_{g, B_i} \\;+\\; \\gamma_{g, B_i} \\,\\varepsilon_{g,i}\n$$\n在这里，$\\mu_g$ 是特征 $g$ 的基线丰度，$Y_i \\in \\{-1, +1\\}$ 是二元类别标签，$s_g$ 代表该特征与类别的关联度（生物信号）。$\\delta_{g, B_i}$ 和 $\\gamma_{g, B_i}$ 分别是样本 $i$ 所属批次 $B_i$ 的加性（位置）和乘性（尺度）批次效应。$\\varepsilon_{g,i}$ 项代表从均值为0、方差有限的分布中抽取的独立同分布噪声。根据问题规范，参数实例化如下：$\\mu_g=0$，$s_g=1.0$（对于前 $p_{\\text{sig}}=20$ 个特征）否则 $s_g=0$，$\\delta_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\delta^2)$ 且 $\\sigma_\\delta=1.0$，$\\log \\gamma_{g,b} \\sim \\mathcal{N}(0, \\sigma_\\gamma^2)$ 且 $\\sigma_\\gamma=0.3$，以及 $\\varepsilon_{g,i} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ 且 $\\sigma_\\varepsilon=0.5$。\n\n为了减轻批次效应的混淆影响，应用了参数化经验贝叶斯（EB）ComBat风格的校正。为防止数据泄露，此校正的所有参数必须完全从每个交叉验证折的训练数据中估计。对每个特征 $g$ 的程序如下：\n1.  仅使用训练样本计算全局均值 $\\hat{\\mu}_g$ 和标准差 $\\hat{\\sigma}_g$。\n    $$\n    \\hat{\\mu}_g \\;=\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} X_{g,i}, \\qquad\n    \\hat{\\sigma}_g \\;=\\; \\sqrt{ \\frac{1}{n_{\\text{train}}-1} \\sum_{i \\in \\text{train}} (X_{g,i} - \\hat{\\mu}_g)^2 }\n    $$\n2.  使用这些从训练中派生的参数对训练和测试数据进行标准化：$Z_{g,i} = (X_{g,i} - \\hat{\\mu}_g) / \\hat{\\sigma}_g$。为保证数值稳定性，向 $\\hat{\\sigma}_g$ 中加入一个小的常数 $\\epsilon$。\n3.  对于训练集中存在的每个批次 $b$，估计其在标准化数据上的特定位置和尺度效应。请注意：问题陈述中 $\\hat{s}_{g,b}$ 的定义存在一个印刷错误，即递归的自我引用。将被实现的正确公式，是以估计的均值 $\\hat{m}_{g,b}$ 为中心来计算方差。\n    $$\n    \\hat{m}_{g,b} \\;=\\; \\frac{1}{n_{b,\\text{train}}} \\sum_{i \\in \\text{train}, B_i=b} Z_{g,i}, \\qquad\n    \\hat{s}_{g,b} \\;=\\; \\sqrt{ \\frac{1}{n_{b,\\text{train}}-1} \\sum_{i \\in \\text{train}, B_i=b} (Z_{g,i} - \\hat{m}_{g,b})^2 }\n    $$\n    同样为 $\\hat{s}_{g,b}$ 的分母使用一个稳定常数。如果一个批次在训练折中只包含一个样本，其尺度校正因子 $\\hat{s}_{g,b}$ 被设为 $1$。\n4.  调整标准化值。对于一个在训练期间出现过的批次 $B_i$ 中的样本 $i$，调整为 $Z_{g,i}^{\\text{adj}} = (Z_{g,i} - \\hat{m}_{g,B_i}) / \\hat{s}_{g,B_i}$。如果批次 $B_i$（例如，来自测试样本）不在训练集中，则不进行调整（即，$Z_{g,i}^{\\text{adj}} = Z_{g,i}$）。\n5.  使用全局训练参数将调整后的数据重新缩放回原始特征空间维度：$X_{g,i}^{\\text{adj}} = Z_{g,i}^{\\text{adj}} \\hat{\\sigma}_g + \\hat{\\mu}_g$。\n\n然后，使用校正后的数据 $x_i^{\\text{adj}} \\in \\mathbb{R}^p$ 和相应的标签 $y_i$ 来训练一个线性SVM分类器。该分类器旨在通过最小化带有 $L_2$ 正则化的平方合页损失来寻找权重向量 $w \\in \\mathbb{R}^p$ 和偏置项 $b \\in \\mathbb{R}$：\n$$\n\\min_{w,b} \\quad \\frac{\\lambda}{2}\\|w\\|_2^2 \\;+\\; \\frac{1}{n_{\\text{train}}} \\sum_{i \\in \\text{train}} \\left( \\max\\{0, 1 - y_i (w^\\top x_i^{\\text{adj}} + b)\\} \\right)^2\n$$\n此目标函数使用随机梯度下降（SGD）进行优化。对于每个小批量 $\\mathcal{B}$ 的训练样本，权重和偏置被迭代更新。给定学习率 $\\eta > 0$ 和正则化参数 $\\lambda > 0$，使用以下梯度进行更新：\n$$\n\\nabla_w J = \\lambda w \\;-\\; \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i x_i^{\\text{adj}} (1 - m_i)\n$$\n$$\n\\nabla_b J = - \\frac{2}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{1}\\{m_i  1\\} \\, y_i (1 - m_i)\n$$\n其中 $m_i = y_i (w^\\top x_i^{\\text{adj}} + b)$ 是样本 $i$ 的间隔，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。参数更新方式为 $(w,b) \\leftarrow (w,b) - \\eta (\\nabla_w J, \\nabla_b J)$。\n\n评估将使用 $K=5$ 折交叉验证进行。数据集被划分为5个不相交的折。在5次迭代中，每次迭代一个折作为测试集，其余4个作为训练集。最终的性能指标是5个测试折的平均分类准确率。\n\n案例1涉及一个均衡设计，有 $B=3$ 个批次，大小为 $[40, 40, 40]$。使用标准的分层5折分割。批次校正严格在每个训练折内进行。\n\n案例2研究一个边缘案例，批次大小为 $[50, 50, 10]$。构建5个折，使得对于第一个折，包含10个样本的小批次全部作为测试集的一部分，从而确保它在相应的训练集中缺失。这测试了指定的备用机制（对未知批次进行恒等变换）。其余的折划分剩余的数据。\n\n案例3直接量化数据泄露的影响。使用与案例1相同的数据和折，我们比较两个流程。严格流程 $\\mathcal{P}_{\\text{strict}}$ 在每个折内执行校正，如上所述。泄露流程 $\\mathcal{P}_{\\text{leak}}$ 在交叉验证开始前，在整个数据集上一次性估计批次校正参数。报告的结果是平均准确率的差异，$\\text{Acc}(\\mathcal{P}_{\\text{leak}}) - \\text{Acc}(\\mathcal{P}_{\\text{strict}})$，预计该值为正，反映了由于测试集信息泄露到训练变换中而导致的人为性能夸大。所有计算都遵循指定的随机种子 $2025$，以实现完全的可复现性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the execution of the three test cases.\n    \"\"\"\n\n    def generate_data(batch_sizes, p, p_sig, s, sigma_eps, sigma_delta, sigma_gamma, seed):\n        \"\"\"Generates synthetic omics data based on the specified model.\"\"\"\n        rng = np.random.default_rng(seed)\n        n_samples = sum(batch_sizes)\n        n_batches = len(batch_sizes)\n\n        # Assign samples to batches and create class labels (Y)\n        batches = np.repeat(np.arange(1, n_batches + 1), batch_sizes)\n        y = np.ones(n_samples, dtype=int)\n        start_idx = 0\n        for size in batch_sizes:\n            # Assign labels approximately equally within each batch\n            n_pos = size // 2\n            y[start_idx : start_idx + n_pos] = 1\n            y[start_idx + n_pos : start_idx + size] = -1\n            rng.shuffle(y[start_idx : start_idx + size])\n            start_idx += size\n        \n        # Generate model parameters\n        mu = np.zeros(p)\n        s_vec = np.zeros(p)\n        s_vec[:p_sig] = s\n\n        # Batch effects\n        delta = rng.normal(0, sigma_delta, size=(p, n_batches))\n        log_gamma = rng.normal(0, sigma_gamma, size=(p, n_batches))\n        gamma = np.exp(log_gamma)\n\n        # Noise\n        epsilon = rng.normal(0, sigma_eps, size=(n_samples, p))\n\n        # Construct data matrix X\n        X = np.zeros((n_samples, p))\n        for i in range(n_samples):\n            batch_idx = batches[i] - 1\n            X[i, :] = mu + s_vec * y[i] + delta[:, batch_idx] + gamma[:, batch_idx] * epsilon[i, :]\n        \n        return X, y, batches\n\n    def combat_fit(X_train, batches_train):\n        \"\"\"Estimates ComBat parameters from training data.\"\"\"\n        train_batches_unique = np.unique(batches_train)\n        p = X_train.shape[1]\n\n        # Global stats\n        mu_g = np.mean(X_train, axis=0)\n        sigma_g = np.std(X_train, axis=0, ddof=1)\n        # Add a small epsilon for stability if std dev is zero\n        sigma_g[sigma_g == 0] = 1e-8\n\n        # Standardize data\n        Z = (X_train - mu_g) / sigma_g\n\n        # Batch-specific stats\n        m_gb = np.zeros((p, len(train_batches_unique)))\n        s_gb = np.ones((p, len(train_batches_unique)))\n        \n        batch_map = {batch_id: i for i, batch_id in enumerate(train_batches_unique)}\n\n        for batch_id, i in batch_map.items():\n            idx = (batches_train == batch_id)\n            n_b = np.sum(idx)\n            if n_b > 0:\n                Z_b = Z[idx, :]\n                m_gb[:, i] = np.mean(Z_b, axis=0)\n                if n_b > 1:\n                    s_gb[:, i] = np.std(Z_b, axis=0, ddof=1)\n        # Add a small epsilon for stability\n        s_gb[s_gb == 0] = 1e-8\n\n        return {'mu_g': mu_g, 'sigma_g': sigma_g, 'm_gb': m_gb, 's_gb': s_gb, 'batch_map': batch_map}\n    \n    def combat_transform(X, batches, params):\n        \"\"\"Applies ComBat correction using pre-fitted parameters.\"\"\"\n        mu_g, sigma_g, m_gb, s_gb, batch_map = params['mu_g'], params['sigma_g'], params['m_gb'], params['s_gb'], params['batch_map']\n        \n        X_adj = np.copy(X)\n        Z = (X - mu_g) / sigma_g\n\n        for b_id in np.unique(batches):\n            if b_id in batch_map:\n                idx = (batches == b_id)\n                batch_col_idx = batch_map[b_id]\n                \n                Z_adj_b = (Z[idx, :] - m_gb[:, batch_col_idx]) / s_gb[:, batch_col_idx]\n                X_adj[idx, :] = Z_adj_b * sigma_g + mu_g\n        \n        return X_adj\n\n    def svm_sgd(X_train, y_train, lambda_, eta, epochs, batch_size, seed):\n        \"\"\"Trains a linear SVM with squared hinge loss via SGD.\"\"\"\n        rng = np.random.default_rng(seed)\n        n, p = X_train.shape\n        w = np.zeros(p)\n        b = 0.0\n\n        indices = np.arange(n)\n        for _ in range(epochs):\n            rng.shuffle(indices)\n            for i in range(0, n, batch_size):\n                batch_indices = indices[i:i + batch_size]\n                X_batch, y_batch = X_train[batch_indices], y_train[batch_indices]\n                \n                margins = y_batch * (X_batch @ w + b)\n                \n                # Identify samples with margin  1\n                misclassified_mask = margins  1\n                \n                if np.any(misclassified_mask):\n                    y_mis = y_batch[misclassified_mask]\n                    X_mis = X_batch[misclassified_mask]\n                    one_minus_m = 1 - margins[misclassified_mask]\n\n                    grad_w = lambda_ * w - (2 / len(X_batch)) * np.sum( (y_mis * one_minus_m)[:, np.newaxis] * X_mis, axis=0)\n                    grad_b = -(2 / len(X_batch)) * np.sum(y_mis * one_minus_m)\n                else:\n                    grad_w = lambda_ * w\n                    grad_b = 0\n\n                w -= eta * grad_w\n                b -= eta * grad_b\n        return w, b\n\n    def predict(X, w, b):\n        \"\"\"Makes predictions using the trained SVM.\"\"\"\n        return np.sign(X @ w + b)\n\n    def accuracy(y_true, y_pred):\n        \"\"\"Calculates classification accuracy.\"\"\"\n        return np.mean(y_true == y_pred)\n\n    def create_stratified_folds(y, k, seed):\n        \"\"\"Creates stratified K-folds.\"\"\"\n        rng = np.random.default_rng(seed)\n        pos_indices = np.where(y == 1)[0]\n        neg_indices = np.where(y == -1)[0]\n        rng.shuffle(pos_indices)\n        rng.shuffle(neg_indices)\n        \n        pos_folds = np.array_split(pos_indices, k)\n        neg_folds = np.array_split(neg_indices, k)\n        \n        folds = []\n        for i in range(k):\n            test_indices = np.concatenate((pos_folds[i], neg_folds[i]))\n            folds.append(test_indices)\n            \n        return folds\n\n    def run_cv_pipeline(X, y, batches, folds, p, lambda_, eta, epochs, batch_size, cv_seed, pipeline_mode, full_data_params=None):\n        \"\"\"Runs the full CV pipeline.\"\"\"\n        accuracies = []\n        n_samples = len(y)\n        all_indices = np.arange(n_samples)\n\n        for test_indices in folds:\n            train_indices = np.setdiff1d(all_indices, test_indices)\n\n            X_train, y_train, batches_train = X[train_indices], y[train_indices], batches[train_indices]\n            X_test, y_test, batches_test = X[test_indices], y[test_indices], batches[test_indices]\n            \n            if pipeline_mode == 'strict':\n                params = combat_fit(X_train, batches_train)\n            elif pipeline_mode == 'leaky':\n                params = full_data_params\n            else:\n                raise ValueError(\"Invalid pipeline mode\")\n\n            X_train_adj = combat_transform(X_train, batches_train, params)\n            X_test_adj = combat_transform(X_test, batches_test, params)\n            \n            # Train SVM\n            w, b = svm_sgd(X_train_adj, y_train, lambda_, eta, epochs, batch_size, cv_seed)\n            \n            # Evaluate\n            y_pred = predict(X_test_adj, w, b)\n            acc = accuracy(y_test, y_pred)\n            accuracies.append(acc)\n            \n        return np.mean(accuracies)\n\n    # Common parameters\n    p = 50\n    p_sig = 20\n    s_val = 1.0\n    sigma_eps = 0.5\n    sigma_delta = 1.0\n    sigma_gamma = 0.3\n    data_seed = 2025\n    \n    lambda_ = 1e-3\n    eta = 0.05\n    epochs = 20\n    batch_size = 32\n    k_folds = 5\n\n    results = []\n\n    # --- Case 1 ---\n    batch_sizes_1 = [40, 40, 40]\n    X1, y1, batches1 = generate_data(batch_sizes_1, p, p_sig, s_val, sigma_eps, sigma_delta, sigma_gamma, data_seed)\n    folds1 = create_stratified_folds(y1, k_folds, data_seed)\n    acc1 = run_cv_pipeline(X1, y1, batches1, folds1, p, lambda_, eta, epochs, batch_size, data_seed, 'strict')\n    results.append(acc1)\n\n    # --- Case 2 ---\n    batch_sizes_2 = [50, 50, 10]\n    X2, y2, batches2 = generate_data(batch_sizes_2, p, p_sig, s_val, sigma_eps, sigma_delta, sigma_gamma, data_seed)\n    \n    # Custom fold generation for Case 2\n    rng_case2 = np.random.default_rng(data_seed)\n    b3_indices = np.where(batches2 == 3)[0]\n    b12_indices = np.where(batches2 != 3)[0]\n    y_b12 = y2[b12_indices]\n\n    b12_pos = b12_indices[y_b12 == 1]\n    b12_neg = b12_indices[y_b12 == -1]\n    rng_case2.shuffle(b12_pos)\n    rng_case2.shuffle(b12_neg)\n\n    # 12 extra samples for fold 1's test set\n    test1_extra_indices = np.concatenate((b12_pos[:6], b12_neg[:6]))\n    test_fold1 = np.concatenate((b3_indices, test1_extra_indices))\n    \n    remaining_b12_pos = b12_pos[6:]\n    remaining_b12_neg = b12_neg[6:]\n    remaining_b12_y = np.concatenate((np.ones(len(remaining_b12_pos)), -1*np.ones(len(remaining_b12_neg))))\n    \n    other_folds = create_stratified_folds(remaining_b12_y, k_folds - 1, data_seed)\n    \n    remaining_b12_indices = np.concatenate((remaining_b12_pos, remaining_b12_neg))\n    \n    folds2 = [test_fold1]\n    for fold in other_folds:\n        folds2.append(remaining_b12_indices[fold])\n\n    acc2 = run_cv_pipeline(X2, y2, batches2, folds2, p, lambda_, eta, epochs, batch_size, data_seed, 'strict')\n    results.append(acc2)\n\n    # --- Case 3 ---\n    # Re-use data and folds from Case 1\n    # Pipeline_strict is just acc1\n    acc_strict = acc1\n    \n    # Pipeline_leak\n    leaky_params = combat_fit(X1, batches1)\n    acc_leak = run_cv_pipeline(X1, y1, batches1, folds1, p, lambda_, eta, epochs, batch_size, data_seed, 'leaky', full_data_params=leaky_params)\n    \n    diff = acc_leak - acc_strict\n    results.append(diff)\n    \n    # Print final output\n    print(f\"[{','.join([f'{r:.4f}' for r in results])}]\")\n\nsolve()\n\n```"
        }
    ]
}