{
    "hands_on_practices": [
        {
            "introduction": "A primary task in network inference is choosing the right mathematical tool to quantify the relationship between biological variables like gene expression levels. This exercise explores the fundamental difference between two of the most common measures: the Pearson correlation coefficient and mutual information. By constructing a hypothetical scenario where two variables have identical zero correlation but markedly different mutual information, you will gain a crucial insight: correlation only captures linear dependencies, whereas mutual information can detect any statistical association, including important nonlinear regulatory relationships that correlation would miss .",
            "id": "3331801",
            "problem": "In computational systems biology, gene regulatory network inference often starts from basic statistical dependencies between gene expression variables. The Pearson correlation coefficient between two random variables $X$ and $Y$ is defined as $\\rho_{XY} = \\dfrac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$, where $\\operatorname{Cov}(X,Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])]$. Mutual information (MI) is defined for continuous random variables by $I(X;Y) = \\int \\int p_{X,Y}(x,y) \\log \\left( \\dfrac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)} \\right) \\, dx \\, dy$, where $p_{X,Y}$ is the joint density and $p_X$, $p_Y$ are marginals. By definition, $I(X;Y) = 0$ if and only if $X$ and $Y$ are independent, and $I(X;Y) > 0$ when $Y$ probabilistically depends on $X$.\n\nYou are asked to construct a pair of variables that have identical Pearson correlation but different mutual information and to interpret what this means biologically in terms of nonlinear regulation. Consider the following options, each proposing a construction and an interpretation. Select the single option that correctly constructs such a pair and provides a scientifically sound interpretation within gene regulatory network inference.\n\nA. Let $X \\sim \\mathcal{N}(0,1)$ and $\\epsilon \\sim \\mathcal{N}(0,1)$ independent. Define $Y_1 = \\epsilon$ and $Y_2 = X + \\epsilon$. Claim: $\\rho_{X,Y_1} = \\rho_{X,Y_2} = 0$ and $I(X;Y_1) = I(X;Y_2)$ because correlation fully determines mutual information. Biological interpretation: correlation suffices to infer regulation in these cases.\n\nB. Let $X \\sim \\mathcal{N}(0,1)$ and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent, with $\\sigma^2 \\in (0,\\infty)$. Define $Y_1 = \\epsilon$ and $Y_2 = X^2 + \\epsilon$. Claim: $\\rho_{X,Y_1} = \\rho_{X,Y_2} = 0$ while $I(X;Y_1) = 0$ and $I(X;Y_2) > 0$. Biological interpretation: a nonlinear regulatory relationship (e.g., thresholding or cooperative activation) can produce zero correlation but positive mutual information, so relying solely on correlation may miss true regulatory interactions that mutual information can detect.\n\nC. Let $X \\sim \\mathcal{N}(0,1)$. Construct two pairs $(X,Y_1)$ and $(X,Y_2)$ with $\\rho_{X,Y_1} = \\rho_{X,Y_2} = 0.6$. Take $Y_1 = X + \\epsilon_1$ and $Y_2 = \\operatorname{sign}(X) + \\epsilon_2$, where $\\epsilon_1,\\epsilon_2$ are independent zero-mean noises chosen so that the correlations match. Claim: $I(X;Y_1) = I(X;Y_2)$ by the data processing inequality, so equal correlation implies equal mutual information. Biological interpretation: both interactions are linear in effect and equally strong.\n\nD. Let $X$ be a symmetric zero-mean continuous variable (for example, $X \\sim \\mathcal{N}(0,1)$), and let $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ be independent with $\\sigma^2 \\in (0,\\infty)$. Define $Y_1 = |X| + \\epsilon$ and $Y_2 = \\epsilon$. Claim: $\\rho_{X,Y_1} = \\rho_{X,Y_2} = 0$ and $I(X;Y_1) = I(X;Y_2) = 0$. Biological interpretation: when correlations are zero, there is no regulation to infer, so mutual information adds nothing.\n\nWhich option is correct?",
            "solution": "The core idea of this problem is to find a scenario where the Pearson correlation is zero, but a statistical dependency still exists, which can be captured by mutual information. Correlation measures *linear* dependency, so a purely non-linear relationship can result in zero correlation. Mutual information, in contrast, captures *any* kind of statistical dependency.\n\nWe need to find an option that constructs two pairs of variables, $(X, Y_1)$ and $(X, Y_2)$, such that $\\rho_{X,Y_1} = \\rho_{X,Y_2}$ (ideally zero) but $I(X;Y_1) \\neq I(X;Y_2)$, and provides a correct biological interpretation.\n\nLet's analyze each option:\n\n**Option A:** This option is incorrect. It claims $\\rho_{X,Y_1} = \\rho_{X,Y_2} = 0$. However, for the pair $(X, Y_2)$ where $Y_2 = X + \\epsilon$, the covariance is $\\operatorname{Cov}(X, X+\\epsilon) = \\operatorname{Var}(X) = 1$. Thus, the correlation $\\rho_{X,Y_2}$ is non-zero (specifically, $1/\\sqrt{2}$), contradicting the claim.\n\n**Option B:** This option correctly constructs the desired scenario.\n*   **Pair 1: $(X, Y_1)$** where $Y_1=\\epsilon$. Since $X$ and $Y_1$ are independent, $\\rho_{X,Y_1} = 0$ and $I(X;Y_1) = 0$.\n*   **Pair 2: $(X, Y_2)$** where $Y_2 = X^2 + \\epsilon$. $X \\sim \\mathcal{N}(0,1)$ is a symmetric distribution, so its odd moments are zero (e.g., $\\mathbb{E}[X]=0, \\mathbb{E}[X^3]=0$). The covariance is $\\operatorname{Cov}(X, X^2+\\epsilon) = \\mathbb{E}[X(X^2+\\epsilon)] - \\mathbb{E}[X]\\mathbb{E}[X^2+\\epsilon] = \\mathbb{E}[X^3] + \\mathbb{E}[X\\epsilon] = 0 + \\mathbb{E}[X]\\mathbb{E}[\\epsilon] = 0$. Thus, $\\rho_{X,Y_2} = 0$.\n*   However, $Y_2$ clearly depends on $X$. The conditional distribution of $Y_2$ given $X=x$ is a normal distribution centered at $x^2$. Since this depends on $x$, the variables are not independent, and therefore $I(X;Y_2) > 0$.\n*   We have successfully found a case where $\\rho_{X,Y_1} = \\rho_{X,Y_2} = 0$ but $I(X;Y_1) = 0$ while $I(X;Y_2) > 0$.\n*   The interpretation is also correct: a non-linear relationship, such as the one modeled by $Y=X^2+\\epsilon$ (which can represent a regulatory mechanism like homodimer binding), is missed by correlation but detected by mutual information. This highlights the importance of MI in network inference.\n\n**Option C:** This option is incorrect. Its central claim, \"equal correlation implies equal mutual information,\" is fundamentally false and is the very misconception the problem is designed to address. The relationship between correlation and MI is only straightforward for bivariate Gaussian distributions, which is not the case for the pair $(X, \\operatorname{sign}(X)+\\epsilon_2)$. The use of the data processing inequality is also incorrect in this context.\n\n**Option D:** This option is incorrect. It correctly calculates that the correlation for $(X, |X|+\\epsilon)$ is zero. However, it falsely claims that the mutual information $I(X;Y_1)$ is also zero. Since the value of $Y_1 = |X|+\\epsilon$ depends on $X$ (e.g., its conditional mean is $|x|$), the variables are not independent, and thus $I(X;Y_1) > 0$. The interpretation that zero correlation implies no regulation is the fallacy being exposed.\n\nTherefore, Option B is the only one that correctly constructs the example and provides a sound scientific interpretation.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Once a potential association between two genes is measured, we must determine if it is statistically significant or simply a result of random chance. This practice delves into the classical statistical theory behind testing the significance of a Pearson correlation coefficient, $\\rho$. You will derive the exact null distribution for the sample correlation coefficient, $r$, starting from the familiar principles of linear regression, and see how this leads directly to the formula for the p-value . This fundamental exercise demystifies the p-value calculation, moving you beyond black-box software functions to a robust understanding of the hypothesis test itself.",
            "id": "3331728",
            "problem": "In a gene coexpression network reconstruction task, the presence of an undirected edge between two genes is assessed by testing whether their population correlation is nonzero. Suppose we observe $n$ independent and identically distributed paired measurements $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ of two gene expression levels and assume bivariate normality with unknown means and variances. Under the null hypothesis of no linear association, $H_{0}:\\rho=0$, consider the sample Pearson correlation coefficient\n$$\nr \\;=\\; \\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}\\,\\sqrt{\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}}\\,,\n$$\nwhere $\\bar{X}$ and $\\bar{Y}$ denote the sample means.\n\nStarting only from the fundamental properties of the bivariate normal model and classical least-squares regression under $H_{0}$, derive the null sampling distribution of $r$ and obtain the two-sided edge $p$-value for testing $H_{0}:\\rho=0$ as an analytic expression in terms of $r$ and $n$. Express your final $p$-value in closed form using the regularized incomplete beta function $I_{x}(a,b)$, defined as $I_{x}(a,b)=\\frac{B_{x}(a,b)}{B(a,b)}$, where $B_{x}(a,b)$ is the incomplete beta function and $B(a,b)$ is the complete beta function.\n\nYour final answer must be a single closed-form analytic expression depending on $r$ and $n$, with no numerical evaluation required.",
            "solution": "The problem asks for the derivation of the null sampling distribution of the sample Pearson correlation coefficient $r$ and the corresponding two-sided $p$-value for testing the null hypothesis $H_{0}: \\rho=0$, where $\\rho$ is the population correlation. The data are $n$ independent and identically distributed pairs $\\{(X_{i},Y_{i})\\}_{i=1}^{n}$ from a bivariate normal distribution. The derivation is to be based on the principles of classical least-squares regression.\n\nFirst, we establish the connection between the sample correlation coefficient $r$ and simple linear regression. Consider the simple linear regression model of $Y$ on $X$:\n$$Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\epsilon_{i}$$\nwhere $\\epsilon_{i}$ are independent and identically distributed normal random variables with mean $0$ and variance $\\sigma^{2}$. The least-squares estimator for the slope, $\\hat{\\beta}_{1}$, is given by:\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}\n$$\nThe sample Pearson correlation coefficient $r$ is defined as:\n$$\nr = \\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}\\,\\sqrt{\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}}\n$$\nBy comparing these two expressions, we can relate $r$ and $\\hat{\\beta}_{1}$:\n$$\nr = \\hat{\\beta}_{1} \\frac{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}}{\\sqrt{\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}}\n$$\n\nUnder the assumption of a bivariate normal distribution for $(X,Y)$, the conditional distribution of $Y$ given $X=x$ is normal. The population regression line is $E[Y|X=x] = \\mu_{Y} + \\rho\\frac{\\sigma_{Y}}{\\sigma_{X}}(x - \\mu_{X})$. The slope of this line is $\\beta_{1} = \\rho\\frac{\\sigma_{Y}}{\\sigma_{X}}$. Therefore, the null hypothesis of zero population correlation, $H_{0}: \\rho=0$, is equivalent to the null hypothesis of zero slope in the linear regression model, $H_{0}: \\beta_{1}=0$.\n\nTo test $H_{0}: \\beta_{1}=0$, we use the t-statistic:\n$$\nT = \\frac{\\hat{\\beta}_{1} - 0}{\\text{SE}(\\hat{\\beta}_{1})}\n$$\nwhere $\\text{SE}(\\hat{\\beta}_{1})$ is the standard error of the slope estimator. The standard error is calculated as $\\text{SE}(\\hat{\\beta}_{1}) = \\frac{s_{e}}{\\sqrt{S_{xx}}}$, where $S_{xx} = \\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}$ and $s_{e}^{2}$ is the unbiased estimator of the error variance $\\sigma^{2}$:\n$$\ns_{e}^{2} = \\frac{\\text{SSE}}{n-2}\n$$\nHere, SSE is the sum of squared errors (or residuals), $\\text{SSE} = \\sum_{i=1}^{n}(Y_{i} - \\hat{Y}_{i})^{2}$, where $\\hat{Y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}X_{i}$ are the fitted values.\n\nThe total sum of squares, $\\text{SST} = \\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}$, can be partitioned into the sum of squares due to regression, SSR, and the sum of squares of errors, SSE, i.e., $\\text{SST} = \\text{SSR} + \\text{SSE}$. A fundamental identity in regression analysis is that the coefficient of determination, $r^{2}$, is the fraction of total variance explained by the regression:\n$$\nr^{2} = \\frac{\\text{SSR}}{\\text{SST}}\n$$\nFrom this, we can express SSE in terms of SST and $r^2$:\n$$\n\\text{SSE} = \\text{SST} - \\text{SSR} = \\text{SST} - r^{2}\\text{SST} = (1-r^{2})\\text{SST}\n$$\nNow, we can express the $t$-statistic in terms of $r$. First, rewrite the numerator of the $t$-statistic:\n$$\n\\hat{\\beta}_{1} = r \\frac{\\sqrt{\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}} = r \\frac{\\sqrt{\\text{SST}}}{\\sqrt{S_{xx}}}\n$$\nThe $t$-statistic is:\n$$\nT = \\frac{\\hat{\\beta}_{1}}{s_{e}/\\sqrt{S_{xx}}} = \\frac{\\hat{\\beta}_{1}\\sqrt{S_{xx}}}{s_{e}} = \\frac{\\left(r \\frac{\\sqrt{\\text{SST}}}{\\sqrt{S_{xx}}}\\right)\\sqrt{S_{xx}}}{\\sqrt{\\frac{\\text{SSE}}{n-2}}} = \\frac{r\\sqrt{\\text{SST}}}{\\sqrt{\\frac{(1-r^{2})\\text{SST}}{n-2}}}\n$$\nSimplifying this expression, we get:\n$$\nT = \\frac{r\\sqrt{\\text{SST}}}{\\sqrt{1-r^{2}}\\sqrt{\\frac{\\text{SST}}{n-2}}} = \\frac{r}{\\sqrt{1-r^{2}}} \\sqrt{n-2} = r\\sqrt{\\frac{n-2}{1-r^{2}}}\n$$\nUnder the null hypothesis $H_{0}: \\rho=0$, this statistic $T$ follows a Student's t-distribution with $\\nu = n-2$ degrees of freedom. This relationship characterizes the null sampling distribution of $r$ by defining the distribution of a pivotal quantity based on it.\n\nThe two-sided $p$-value is the probability of observing a value of the test statistic at least as extreme as the one observed, which we denote as $t_{obs} = r\\sqrt{\\frac{n-2}{1-r^{2}}}$.\n$$\np = P(|T_{\\nu}| \\ge |t_{obs}|)\n$$\nwhere $T_{\\nu}$ is a random variable following the t-distribution with $\\nu=n-2$ degrees of freedom. This is equivalent to:\n$$\np = P(T_{\\nu}^{2} \\ge t_{obs}^{2})\n$$\nThe square of a Student's t-distributed random variable with $\\nu$ degrees of freedom follows an F-distribution with $1$ and $\\nu$ degrees of freedom, i.e., $T_{\\nu}^{2} \\sim F_{1,\\nu}$. Thus, under $H_{0}$:\n$$\nT^{2} = r^{2}\\frac{n-2}{1-r^{2}} \\sim F_{1, n-2}\n$$\nThe $p$-value is the probability that an F-distributed random variable is greater than or equal to the observed value of $T^2$:\n$$\np = P\\left(F_{1, n-2} \\ge r^{2}\\frac{n-2}{1-r^{2}}\\right) = 1 - P\\left(F_{1, n-2} \\le r^{2}\\frac{n-2}{1-r^{2}}\\right)\n$$\nThe cumulative distribution function (CDF) of an F-distributed random variable $F_{d_1, d_2}$ is related to the regularized incomplete beta function $I_{x}(a,b)$ by the identity:\n$$\nP(F_{d_1, d_2} \\le x) = I_{\\frac{d_1 x}{d_1 x + d_2}}\\left(\\frac{d_1}{2}, \\frac{d_2}{2}\\right)\n$$\nIn our case, $d_1=1$, $d_2=n-2$, and the value of the random variable is $x = r^{2}\\frac{n-2}{1-r^{2}}$. The argument of the regularized incomplete beta function is:\n$$\n\\frac{d_1 x}{d_1 x + d_2} = \\frac{1 \\cdot \\left(r^{2}\\frac{n-2}{1-r^{2}}\\right)}{1 \\cdot \\left(r^{2}\\frac{n-2}{1-r^{2}}\\right) + (n-2)} = \\frac{r^{2}(n-2)}{r^{2}(n-2) + (1-r^{2})(n-2)} = \\frac{r^{2}(n-2)}{n-2} = r^{2}\n$$\nThe parameters of the beta function are $a = d_1/2 = 1/2$ and $b = d_2/2 = (n-2)/2$.\nSubstituting these into the expression for the $p$-value, we obtain:\n$$\np = 1 - I_{r^{2}}\\left(\\frac{1}{2}, \\frac{n-2}{2}\\right)\n$$\nUsing the symmetry property of the regularized incomplete beta function, $1 - I_{x}(a,b) = I_{1-x}(b,a)$, we can write the $p$-value in its final form:\n$$\np = I_{1-r^{2}}\\left(\\frac{n-2}{2}, \\frac{1}{2}\\right)\n$$\nThis is the analytic expression for the two-sided $p$-value for testing $H_{0}: \\rho=0$ in terms of the sample correlation $r$ and sample size $n$.",
            "answer": "$$\n\\boxed{I_{1-r^{2}}\\left(\\frac{n-2}{2}, \\frac{1}{2}\\right)}\n$$"
        },
        {
            "introduction": "In a complex biological system, a correlation between two genes does not guarantee a direct interaction; both could be regulated by a third, confounding factor. This practice introduces a more sophisticated tool, partial correlation, for disentangling direct from indirect associations. By working with a given covariance matrix for three hypothetical gene expression variables, you will learn to calculate the correlation between two genes while statistically accounting for the effect of a third . This is the foundational concept behind Gaussian Graphical Models (GGMs), providing a powerful framework for inferring network structures that better reflect direct conditional dependencies.",
            "id": "3331773",
            "problem": "A common approach to infer undirected geneâ€“gene interaction networks in Computational Systems Biology is to model joint gene expression as a multivariate normal and use conditional dependence to decide the presence of edges in a Gaussian Graphical Model (GGM). Consider three genes with centered expression variables $X_{1}$, $X_{2}$, and $X_{3}$ that are jointly multivariate normal with covariance matrix\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n2.5 & 1.2 & 0.8 \\\\\n1.2 & 3.0 & 1.5 \\\\\n0.8 & 1.5 & 2.0\n\\end{pmatrix}.\n$$\nUsing only fundamental definitions of covariance, correlation, conditional covariance, and the precision matrix (the inverse of the covariance matrix), compute the partial correlation between $X_{1}$ and $X_{2}$ given $X_{3}$. Then, interpret whether $X_{1}$ and $X_{2}$ are conditionally independent given $X_{3}$, and discuss the implication for the presence or absence of an edge between $X_{1}$ and $X_{2}$ in a Gaussian Graphical Model when conditioning on $X_{3}$.\n\nExpress your final numerical answer for the partial correlation as a decimal rounded to four significant figures. The answer is dimensionless and must be given as a single number without any additional text or symbols such as a percentage sign.",
            "solution": "The problem requires the calculation of the partial correlation between two variables, $X_1$ and $X_2$, in a multivariate normal distribution, conditional on a third variable, $X_3$. This value is then used to infer conditional independence and the structure of the corresponding Gaussian Graphical Model (GGM).\n\nThe variables $X_1$, $X_2$, and $X_3$ are jointly multivariate normal with mean vector $\\boldsymbol{\\mu} = \\mathbf{0}$ (as they are centered) and a given covariance matrix $\\Sigma$:\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{21} & \\sigma_{22} & \\sigma_{23} \\\\\n\\sigma_{31} & \\sigma_{32} & \\sigma_{33}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n2.5 & 1.2 & 0.8 \\\\\n1.2 & 3.0 & 1.5 \\\\\n0.8 & 1.5 & 2.0\n\\end{pmatrix}\n$$\nThe partial correlation between $X_1$ and $X_2$ given $X_3$, denoted $\\rho_{12 \\cdot 3}$, is the correlation between the residuals of $X_1$ and $X_2$ after regressing out $X_3$. For a multivariate normal distribution, this is equivalent to the correlation of the conditional distribution of $(X_1, X_2)$ given $X_3=x_3$. This conditional correlation does not depend on the value of $x_3$.\n\nWe can compute this partial correlation using two fundamental, equivalent methods as requested by the problem statement.\n\nMethod 1: Using the definition of Conditional Covariance\n\nThe partial correlation is the normalized conditional covariance:\n$$\n\\rho_{12 \\cdot 3} = \\frac{\\text{Cov}(X_1, X_2 | X_3)}{\\sqrt{\\text{Var}(X_1 | X_3) \\text{Var}(X_2 | X_3)}}\n$$\nThe terms in this expression are the elements of the conditional covariance matrix of $(X_1, X_2)$ given $X_3$. This matrix can be calculated using the Schur complement of the block $\\Sigma_{33}$ in $\\Sigma$. Let's partition the covariance matrix as:\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}\n$$\nwhere the block corresponding to the variables we are conditioning on, $X_3$, is $\\Sigma_{22} = \\sigma_{33} = 2.0$. The block for the variables of interest, $(X_1, X_2)$, is $\\Sigma_{11} = \\begin{pmatrix} 2.5 & 1.2 \\\\ 1.2 & 3.0 \\end{pmatrix}$. The off-diagonal blocks are $\\Sigma_{12} = \\begin{pmatrix} 0.8 \\\\ 1.5 \\end{pmatrix}$ and $\\Sigma_{21} = \\begin{pmatrix} 0.8 & 1.5 \\end{pmatrix}$.\n\nThe conditional covariance matrix of $(X_1, X_2)$ given $X_3$ is given by:\n$$\n\\Sigma_{(1,2)|3} = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\nSubstituting the values:\n$$\n\\Sigma_{(1,2)|3} = \\begin{pmatrix} 2.5 & 1.2 \\\\ 1.2 & 3.0 \\end{pmatrix} - \\begin{pmatrix} 0.8 \\\\ 1.5 \\end{pmatrix} (2.0)^{-1} \\begin{pmatrix} 0.8 & 1.5 \\end{pmatrix}\n$$\n$$\n\\Sigma_{(1,2)|3} = \\begin{pmatrix} 2.5 & 1.2 \\\\ 1.2 & 3.0 \\end{pmatrix} - \\frac{1}{2.0} \\begin{pmatrix} 0.8 \\times 0.8 & 0.8 \\times 1.5 \\\\ 1.5 \\times 0.8 & 1.5 \\times 1.5 \\end{pmatrix}\n$$\n$$\n\\Sigma_{(1,2)|3} = \\begin{pmatrix} 2.5 & 1.2 \\\\ 1.2 & 3.0 \\end{pmatrix} - 0.5 \\begin{pmatrix} 0.64 & 1.2 \\\\ 1.2 & 2.25 \\end{pmatrix}\n$$\n$$\n\\Sigma_{(1,2)|3} = \\begin{pmatrix} 2.5 - 0.32 & 1.2 - 0.6 \\\\ 1.2 - 0.6 & 3.0 - 1.125 \\end{pmatrix} = \\begin{pmatrix} 2.18 & 0.6 \\\\ 0.6 & 1.875 \\end{pmatrix}\n$$\nFrom this matrix, we identify:\n$\\text{Var}(X_1 | X_3) = 2.18$\n$\\text{Var}(X_2 | X_3) = 1.875$\n$\\text{Cov}(X_1, X_2 | X_3) = 0.6$\n\nNow, we compute the partial correlation:\n$$\n\\rho_{12 \\cdot 3} = \\frac{0.6}{\\sqrt{2.18 \\times 1.875}} = \\frac{0.6}{\\sqrt{4.0875}} \\approx \\frac{0.6}{2.021756} \\approx 0.296772\n$$\n\nMethod 2: Using the Precision Matrix\n\nThe precision matrix, $K$, is the inverse of the covariance matrix, $K = \\Sigma^{-1}$. For a GGM, conditional independence between two variables $X_i$ and $X_j$ given all other variables is equivalent to the corresponding element $K_{ij}$ of the precision matrix being zero. The partial correlation $\\rho_{ij \\cdot \\{rest\\}}$ is directly related to the elements of $K$:\n$$\n\\rho_{ij \\cdot \\{rest\\}} = -\\frac{K_{ij}}{\\sqrt{K_{ii}K_{jj}}}\n$$\nIn our three-variable case, $\\{rest\\}$ for the pair $(1,2)$ is just $\\{3\\}$, so the formula is $\\rho_{12 \\cdot 3} = -K_{12}/\\sqrt{K_{11}K_{22}}$.\n\nFirst, we compute $K = \\Sigma^{-1}$. The inverse is given by $K = \\frac{1}{\\det(\\Sigma)} \\text{adj}(\\Sigma)$.\nThe determinant of $\\Sigma$ is:\n$$\n\\det(\\Sigma) = 2.5(3.0 \\times 2.0 - 1.5 \\times 1.5) - 1.2(1.2 \\times 2.0 - 1.5 \\times 0.8) + 0.8(1.2 \\times 1.5 - 3.0 \\times 0.8)\n$$\n$$\n\\det(\\Sigma) = 2.5(6.0 - 2.25) - 1.2(2.4 - 1.2) + 0.8(1.8 - 2.4)\n$$\n$$\n\\det(\\Sigma) = 2.5(3.75) - 1.2(1.2) + 0.8(-0.6) = 9.375 - 1.44 - 0.48 = 7.455\n$$\nThe adjugate matrix, $\\text{adj}(\\Sigma)$, is the transpose of the cofactor matrix. Since $\\Sigma$ is symmetric, its cofactor matrix will also be symmetric, so $\\text{adj}(\\Sigma)$ is just the cofactor matrix itself.\n$$\nC_{11} = (3.0 \\times 2.0 - 1.5 \\times 1.5) = 3.75\n$$\n$$\nC_{22} = (2.5 \\times 2.0 - 0.8 \\times 0.8) = 4.36\n$$\n$$\nC_{12} = -(1.2 \\times 2.0 - 1.5 \\times 0.8) = -(2.4 - 1.2) = -1.2\n$$\nThe precision matrix is:\n$$\nK = \\frac{1}{7.455}\n\\begin{pmatrix}\n3.75 & -1.2 & \\dots \\\\\n-1.2 & 4.36 & \\dots \\\\\n\\dots & \\dots & \\dots\n\\end{pmatrix}\n$$\nwhere we only need the top-left $2 \\times 2$ block elements for our calculation.\nUsing the formula for partial correlation:\n$$\n\\rho_{12 \\cdot 3} = -\\frac{K_{12}}{\\sqrt{K_{11}K_{22}}} = -\\frac{-1.2/7.455}{\\sqrt{(3.75/7.455)(4.36/7.455)}} = -\\frac{-1.2}{\\sqrt{3.75 \\times 4.36}}\n$$\n$$\n\\rho_{12 \\cdot 3} = \\frac{1.2}{\\sqrt{16.35}} \\approx \\frac{1.2}{4.043513} \\approx 0.296772\n$$\nBoth methods yield the same result, confirming the calculation. Rounding to four significant figures, the partial correlation is $0.2968$.\n\nInterpretation:\n\n1.  Conditional Independence: For a multivariate normal distribution, two variables are conditionally independent given a set of other variables if and only if their partial correlation is zero. Here, we calculated $\\rho_{12 \\cdot 3} \\approx 0.2968$. Since this value is non-zero, $X_1$ and $X_2$ are **not** conditionally independent given $X_3$. This means that even after accounting for the linear effect of gene $X_3$ on both $X_1$ and $X_2$, there is still a residual correlation between them.\n\n2.  Implication for GGM: In the context of Gaussian Graphical Models, an edge between two nodes $i$ and $j$ exists if and only if the variables $X_i$ and $X_j$ are not conditionally independent given all other variables in the model. This is equivalent to the condition $K_{ij} \\neq 0$. As we have found, $\\rho_{12 \\cdot 3} \\neq 0$, which implies $K_{12} \\neq 0$. Therefore, in a GGM constructed from these three variables, there **is** an edge between the nodes representing gene $X_1$ and gene $X_2$. This edge represents a direct relationship (or interaction) that is not fully explained by the path through $X_3$.",
            "answer": "$$\n\\boxed{0.2968}\n$$"
        }
    ]
}