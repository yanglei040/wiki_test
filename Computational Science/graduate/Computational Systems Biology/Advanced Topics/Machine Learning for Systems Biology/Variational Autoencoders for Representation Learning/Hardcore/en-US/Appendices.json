{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of using a Variational Autoencoder in systems biology is to learn representations that are not only predictive but also interpretable, for instance, linking a latent dimension to a specific biological pathway. This exercise challenges a key assumption: that a well-trained model automatically yields a unique, meaningful representation. Through a carefully constructed thought experiment , you will demonstrate that two different VAEs can generate identical data distributions while learning opposing latent representations, exposing a fundamental non-identifiability that has critical implications for scientific interpretation.",
            "id": "3357963",
            "problem": "Consider a synthetic single-gene expression setting in computational systems biology, where a latent pathway activity variable $z$ influences a log-scale expression measurement $x$. A Variational Autoencoder (VAE) defines a generative model with prior $p(z)$, a decoder $p_{\\theta}(x \\mid z)$, and an encoder $q_{\\phi}(z \\mid x)$ that approximates the true posterior $p_{\\theta}(z \\mid x)$. Assume $z$ is a one-dimensional continuous latent. Let the measurement noise be additive and Gaussian, reflecting technical and biological variability after appropriate normalization.\n\nConstruct two encoder-decoder pairs (Model $1$ and Model $2$) that yield the same marginal distribution $p_{\\theta}(x)$ but induce different latent representations $z$ for the same $x$. Concretely, for a fixed slope parameter $w \\neq 0$, a fixed mean level $\\mu$, and a fixed noise variance $\\sigma_{x}^{2} > 0$, define the following:\n\n- Model $1$: prior $p_{1}(z) = \\mathcal{N}(0, 1)$ and decoder $p_{1}(x \\mid z) = \\mathcal{N}(w z + \\mu, \\sigma_{x}^{2})$. The encoder is constrained to be Gaussian $q_{1}(z \\mid x) = \\mathcal{N}(m_{1}(x), s^{2})$ with variance $s^{2}$ equal to the exact posterior variance under Model $1$, and mean $m_{1}(x)$ equal to the exact posterior mean under Model $1$.\n- Model $2$: prior $p_{2}(z') = \\mathcal{N}(0, 1)$ and decoder $p_{2}(x \\mid z') = \\mathcal{N}((-w) z' + \\mu, \\sigma_{x}^{2})$. The encoder is constrained to be Gaussian $q_{2}(z' \\mid x) = \\mathcal{N}(m_{2}(x), s^{2})$ with the same variance $s^{2}$ equal to the exact posterior variance under Model $2$, and mean $m_{2}(x)$ equal to the exact posterior mean under Model $2$.\n\nStarting from the definitions of the Gaussian prior, the Gaussian likelihood, and Bayes’ rule for linear-Gaussian models, perform the following:\n\n- Derive the marginal distribution $p_{1}(x)$ for Model $1$ and $p_{2}(x)$ for Model $2$, and show that $p_{1}(x) = p_{2}(x)$.\n- Derive the exact posterior means $m_{1}(x)$ and $m_{2}(x)$, and the shared posterior variance $s^{2}$, under the respective models.\n- For a fixed observation $x$, derive the Kullback–Leibler (KL) divergence from $q_{1}(z \\mid x)$ to $q_{2}(z' \\mid x)$.\n- Compute the expectation of this KL with respect to the common marginal $p_{\\theta}(x)$, and simplify to a closed-form analytic expression in terms of $w$ and $\\sigma_{x}^{2}$.\n\nDiscuss, in words, the implications of your construction for biological interpretability of the latent $z$ in terms of pathway activity directionality and identifiability. As your final answer, provide only the closed-form analytic expression for the expected KL divergence $\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) \\right]$. No rounding is required, and no units are to be reported in the final answer.",
            "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem in statistical machine learning and its application to computational systems biology. It is self-contained, consistent, and free of any logical or factual flaws. A complete solution will now be provided.\n\nThe problem requires the construction and analysis of two distinct generative models that produce identical observable data distributions but differ in their latent representations. This explores a fundamental non-identifiability issue in latent variable models. We will proceed through the required derivations step by step.\n\n**$1$. Derivation of the Marginal Distributions $p_{1}(x)$ and $p_{2}(x)$**\n\nThe marginal distribution of the observation $x$ is found by integrating the joint distribution $p(x, z) = p(x \\mid z)p(z)$ over the latent variable $z$. For both models, this corresponds to a convolution of two Gaussian distributions, which results in a Gaussian distribution.\n\nFor a general hierarchical model where $z \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$ and $x \\mid z \\sim \\mathcal{N}(az+b, \\sigma_x^2)$, the marginal distribution of $x$ is also Gaussian, $x \\sim \\mathcal{N}(\\mu_x, \\sigma_{total}^2)$, with mean and variance given by:\n$\\mu_x = \\mathbb{E}[x] = \\mathbb{E}[\\mathbb{E}[x \\mid z]] = \\mathbb{E}[az+b] = a\\mu_z + b$\n$\\sigma_{total}^2 = \\mathrm{Var}(x) = \\mathbb{E}[\\mathrm{Var}(x \\mid z)] + \\mathrm{Var}(\\mathbb{E}[x \\mid z]) = \\mathbb{E}[\\sigma_x^2] + \\mathrm{Var}(az+b) = \\sigma_x^2 + a^2\\sigma_z^2$\n\nFor Model $1$:\nThe prior is $p_{1}(z) = \\mathcal{N}(z \\mid 0, 1)$, so $\\mu_z = 0$ and $\\sigma_z^2 = 1$.\nThe decoder is $p_{1}(x \\mid z) = \\mathcal{N}(x \\mid wz + \\mu, \\sigma_{x}^{2})$, so $a=w$ and $b=\\mu$.\n\nThe marginal mean of $x$ is:\n$$\\mathbb{E}_{p_1}[x] = w(0) + \\mu = \\mu$$\nThe marginal variance of $x$ is:\n$$\\mathrm{Var}_{p_1}(x) = \\sigma_{x}^{2} + w^2(1) = w^2 + \\sigma_{x}^{2}$$\nThus, the marginal distribution for Model $1$ is:\n$$p_{1}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$$\n\nFor Model $2$:\nThe prior is $p_{2}(z') = \\mathcal{N}(z' \\mid 0, 1)$, so $\\mu_{z'} = 0$ and $\\sigma_{z'}^2 = 1$.\nThe decoder is $p_{2}(x \\mid z') = \\mathcal{N}(x \\mid (-w)z' + \\mu, \\sigma_{x}^{2})$, so $a=-w$ and $b=\\mu$.\n\nThe marginal mean of $x$ is:\n$$\\mathbb{E}_{p_2}[x] = (-w)(0) + \\mu = \\mu$$\nThe marginal variance of $x$ is:\n$$\\mathrm{Var}_{p_2}(x) = \\sigma_{x}^{2} + (-w)^2(1) = w^2 + \\sigma_{x}^{2}$$\nThus, the marginal distribution for Model $2$ is:\n$$p_{2}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$$\n\nComparing the two marginal distributions, we find that $p_{1}(x) = p_{2}(x)$. Both models generate data from the same distribution, which we will denote as $p_{\\theta}(x) = \\mathcal{N}(\\mu, w^2 + \\sigma_{x}^{2})$. This confirms that the models are indistinguishable from the observations $x$ alone.\n\n**$2$. Derivation of Posterior Means and Variance**\n\nFor a linear-Gaussian model with prior $p(z) = \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z)$ and likelihood $p(x \\mid z) = \\mathcal{N}(x \\mid Az+b, \\Sigma_x)$, the posterior $p(z \\mid x)$ is also Gaussian, $p(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{z \\mid x}, \\Sigma_{z \\mid x})$, with parameters:\n$$\\Sigma_{z \\mid x} = (\\Sigma_z^{-1} + A^T \\Sigma_x^{-1} A)^{-1}$$\n$$\\mu_{z \\mid x} = \\Sigma_{z \\mid x} (A^T \\Sigma_x^{-1}(x-b) + \\Sigma_z^{-1}\\mu_z)$$\nIn our $1$-dimensional case, matrices become scalars.\n\nFor Model $1$:\n$p_{1}(z) = \\mathcal{N}(z \\mid 0, 1) \\implies \\mu_z = 0, \\Sigma_z = 1$.\n$p_{1}(x \\mid z) = \\mathcal{N}(x \\mid wz + \\mu, \\sigma_x^2) \\implies A = w, b = \\mu, \\Sigma_x = \\sigma_x^2$.\n\nThe posterior variance $s^2$ is:\n$$s^2 = (1^{-1} + w (\\sigma_x^2)^{-1} w)^{-1} = (1 + \\frac{w^2}{\\sigma_x^2})^{-1} = \\left(\\frac{\\sigma_x^2 + w^2}{\\sigma_x^2}\\right)^{-1} = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$$\nThe posterior mean $m_{1}(x)$ is:\n$$m_{1}(x) = s^2 (w (\\sigma_x^2)^{-1} (x - \\mu) + 1^{-1} \\cdot 0) = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2} \\left( \\frac{w}{\\sigma_x^2} (x - \\mu) \\right) = \\frac{w(x - \\mu)}{w^2 + \\sigma_x^2}$$\nSo, $q_{1}(z \\mid x) = \\mathcal{N}(z \\mid m_{1}(x), s^2)$.\n\nFor Model $2$:\n$p_{2}(z') = \\mathcal{N}(z' \\mid 0, 1) \\implies \\mu_{z'} = 0, \\Sigma_{z'} = 1$.\n$p_{2}(x \\mid z') = \\mathcal{N}(x \\mid (-w)z' + \\mu, \\sigma_x^2) \\implies A = -w, b = \\mu, \\Sigma_x = \\sigma_x^2$.\n\nThe posterior variance is:\n$$(1^{-1} + (-w) (\\sigma_x^2)^{-1} (-w))^{-1} = (1 + \\frac{w^2}{\\sigma_x^2})^{-1} = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$$\nThis is identical to the posterior variance for Model $1$, so the shared posterior variance is indeed $s^2 = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$.\nThe posterior mean $m_{2}(x)$ is:\n$$m_{2}(x) = s^2 ((-w) (\\sigma_x^2)^{-1} (x - \\mu) + 1^{-1} \\cdot 0) = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2} \\left( \\frac{-w}{\\sigma_x^2} (x - \\mu) \\right) = \\frac{-w(x - \\mu)}{w^2 + \\sigma_x^2}$$\nSo, $q_{2}(z' \\mid x) = \\mathcal{N}(z' \\mid m_{2}(x), s^2)$. Note that $m_{2}(x) = -m_{1}(x)$.\n\n**$3$. Derivation of the Kullback-Leibler Divergence**\n\nThe KL divergence between two $1$-dimensional Gaussian distributions $q_a = \\mathcal{N}(\\mu_a, \\sigma_a^2)$ and $q_b = \\mathcal{N}(\\mu_b, \\sigma_b^2)$ is given by:\n$$\\mathrm{KL}(q_a \\,\\|\\, q_b) = \\ln\\left(\\frac{\\sigma_b}{\\sigma_a}\\right) + \\frac{\\sigma_a^2 + (\\mu_a - \\mu_b)^2}{2\\sigma_b^2} - \\frac{1}{2}$$\nIn our case, we are calculating $\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big)$. The distributions are $q_{1} = \\mathcal{N}(m_{1}(x), s^2)$ and $q_{2} = \\mathcal{N}(m_{2}(x), s^2)$.\nSince the variances are equal, $\\sigma_a^2 = \\sigma_b^2 = s^2$, the formula simplifies significantly:\n$$\\mathrm{KL}(q_1 \\,\\|\\, q_2) = \\ln(1) + \\frac{s^2 + (m_{1}(x) - m_{2}(x))^2}{2s^2} - \\frac{1}{2} = \\frac{1}{2} + \\frac{(m_{1}(x) - m_{2}(x))^2}{2s^2} - \\frac{1}{2} = \\frac{(m_{1}(x) - m_{2}(x))^2}{2s^2}$$\nWe substitute the expressions for the means:\n$$m_{1}(x) - m_{2}(x) = \\frac{w(x - \\mu)}{w^2 + \\sigma_x^2} - \\left( \\frac{-w(x - \\mu)}{w^2 + \\sigma_x^2} \\right) = \\frac{2w(x - \\mu)}{w^2 + \\sigma_x^2}$$\nNow, we substitute this difference and the variance $s^2$ into the KL divergence expression:\n$$\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) = \\frac{1}{2s^2} \\left( \\frac{2w(x - \\mu)}{w^2 + \\sigma_x^2} \\right)^2 = \\frac{1}{2 \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}} \\frac{4w^2(x - \\mu)^2}{(w^2 + \\sigma_x^2)^2}$$\n$$\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) = \\frac{w^2 + \\sigma_x^2}{2\\sigma_x^2} \\frac{4w^2(x - \\mu)^2}{(w^2 + \\sigma_x^2)^2} = \\frac{2w^2(x-\\mu)^2}{\\sigma_x^2(w^2 + \\sigma_x^2)}$$\n\n**$4$. Derivation of the Expected KL Divergence**\n\nWe must compute the expectation of the KL divergence with respect to the common marginal data distribution $p_{\\theta}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$.\n$$\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) \\right] = \\mathbb{E}_{p_{\\theta}(x)}\\left[ \\frac{2w^2(x-\\mu)^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} \\right]$$\nThe term $\\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)}$ is a constant with respect to $x$, so we can factor it out of the expectation:\n$$= \\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} \\mathbb{E}_{p_{\\theta}(x)}\\left[ (x-\\mu)^2 \\right]$$\nThe term $\\mathbb{E}_{p_{\\theta}(x)}\\left[ (x-\\mu)^2 \\right]$ is, by definition, the variance of the random variable $x$ under the distribution $p_{\\theta}(x)$, since $\\mu$ is the mean of $x$. We already established that $\\mathrm{Var}_{p_\\theta}(x) = w^2 + \\sigma_x^2$.\nSubstituting this result back:\n$$\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL} \\right] = \\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} (w^2 + \\sigma_x^2) = \\frac{2w^2}{\\sigma_x^2}$$\n\n**$5$. Discussion of Implications for Biological Interpretability**\n\nThis result exposes a critical issue of non-identifiability in latent variable models like VAEs when used for representation learning in systems biology.\nThe two models, one with gene activation driven by $wz$ and the other by $(-w)z'$, are observationally equivalent. They produce the exact same distribution of log-expression values $x$. This means no amount of data generated by this process can distinguish whether a high latent value $z$ corresponds to high gene expression (Model $1$, assuming $w>0$) or low gene expression (Model $2$).\n\nFor a given observation $x$, the models provide opposing explanations for the latent state: the inferred mean activity is $m_1(x)$ in one model and $m_2(x) = -m_1(x)$ in the other. If a biologist were to interpret $z$ as a \"pathway activity\", one model would suggest the pathway is activated while the other would suggest it is repressed to explain the same data point. The directionality of the latent variable's effect on the observation is unresolvable from the data alone.\n\nThis ambiguity, or sign-indeterminacy, is a direct consequence of the symmetries in the model: the prior $p(z)=\\mathcal{N}(0,1)$ is symmetric around $0$, and the function mapping the latent variable to the mean of the observation has a sign ambiguity $w \\leftrightarrow -w$. Any VAE trained on this data could converge to either of these solutions (or a mixture), making the interpretation of the learned latent dimensions' signs arbitrary.\n\nThe final result, $\\mathbb{E}[\\mathrm{KL}] = \\frac{2w^2}{\\sigma_x^2}$, quantifies the average dissimilarity between these two equally valid posterior beliefs. This value is proportional to the squared signal-to-noise ratio $\\frac{w^2}{\\sigma_x^2}$. When the signal ($w$) is strong relative to the noise ($\\sigma_x$), the posterior distributions become very certain (low variance $s^2$) but centered on very different means ($m_1(x)$ vs. $-m_1(x)$). This leads to a large KL divergence, indicating that the two models offer sharply conflicting, yet equally plausible, interpretations. Resolving this ambiguity would require breaking the model's symmetry, for instance, by incorporating prior biological knowledge that a specific pathway is known to be an activator or inhibitor of the gene in question.",
            "answer": "$$\n\\boxed{\\frac{2w^{2}}{\\sigma_{x}^{2}}}\n$$"
        },
        {
            "introduction": "One of the most common and frustrating failure modes when training VAEs is \"posterior collapse,\" where the latent variables are ignored and the model fails to learn a useful representation of the data. This practice provides a hands-on deep dive into the mechanics of this phenomenon, particularly in the context of sparse single-cell count data . By deriving the evidence lower bound (ELBO) and numerically optimizing it for individual data points, you will develop and deploy quantitative diagnostics to detect collapse and understand how factors like the $\\beta$-VAE's regularization strength can trigger it.",
            "id": "3358032",
            "problem": "You are given a simplified probabilistic model tailored to sparse count data commonly encountered in computational systems biology, such as single-cell gene expression counts. Consider a Variational Autoencoder (VAE) variant with a weighting factor, called the $\\beta$-Variational Autoencoder ($\\beta$-VAE), where a scalar parameter $\\beta \\in \\mathbb{R}_{\\ge 0}$ scales the Kullback–Leibler divergence term in the Evidence Lower Bound (ELBO). The goal is to analyze how tuning $\\beta$ affects ELBO optimization and to devise diagnostics that detect the onset of posterior collapse in the encoder for sparse counts.\n\nFundamental base for the model and derivation:\n- Let the observed count $x \\in \\mathbb{N}_0$ be generated from a Poisson distribution whose rate is an exponential function of a latent variable $z \\in \\mathbb{R}$: $p(x \\mid z) = \\mathrm{Poisson}(\\lambda)$ with $\\lambda = \\exp(w z + b)$, where $w \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ are fixed decoder parameters. The Poisson probability mass function is $p(x \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{x} / x!$ for $x \\in \\mathbb{N}_0$.\n- The latent prior is standard Gaussian: $p(z) = \\mathcal{N}(0, 1)$.\n- The variational family for the encoder is a Gaussian with diagonal covariance: $q(z \\mid x) = \\mathcal{N}(\\mu(x), \\sigma^2(x))$, parameterized per observation by a mean $\\mu(x) \\in \\mathbb{R}$ and standard deviation $\\sigma(x) \\in \\mathbb{R}_{>0}$.\n- The Evidence Lower Bound (ELBO) for a single observation is defined by the difference of a reconstruction term (the expected log likelihood under the variational distribution) and the Kullback–Leibler divergence between the variational posterior and the prior, scaled by $\\beta$: $\\mathrm{ELBO}(x;\\beta) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\| p(z))$. The Kullback–Leibler divergence between two univariate Gaussians is $\\mathrm{KL}(\\mathcal{N}(\\mu,\\sigma^2) \\| \\mathcal{N}(0,1)) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - \\log \\sigma^2 - 1\\right)$.\n- For the Poisson decoder with a log link and Gaussian variational posterior, the reconstruction term admits closed-form expectations under $q(z \\mid x)$ because $\\log \\lambda = w z + b$ is linear in $z$, and $\\lambda = \\exp(w z + b)$ is log-normal under $q(z \\mid x)$. Use only these well-tested facts and definitions as the starting point for your computation and inference.\n\nYour program must, for each test case described below, do the following purely in mathematical and algorithmic terms:\n1. For each observation $x_i$ in the provided dataset, maximize the ELBO with respect to the variational parameters $(\\mu_i, \\sigma_i)$ belonging to $q(z \\mid x_i) = \\mathcal{N}(\\mu_i, \\sigma_i^2)$ using the given fixed decoder parameters $(w,b)$ and the specified $\\beta$. Use numerical optimization to find $(\\mu_i^\\star, \\sigma_i^\\star)$ that maximize the ELBO for the observation $x_i$. Treat each observation independently and assume uniform weighting across observations.\n2. Compute diagnostics to detect posterior collapse using the optimized parameters:\n   - The average Kullback–Leibler divergence $\\overline{\\mathrm{KL}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{KL}(\\mathcal{N}(\\mu_i^\\star,\\sigma_i^{\\star 2}) \\| \\mathcal{N}(0,1))$.\n   - An approximation to the mutual information between observation $X$ and latent $Z$ under the variational family, defined as $I(X;Z) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{KL}\\!\\left(\\mathcal{N}(\\mu_i^\\star,\\sigma_i^{\\star 2}) \\| \\mathcal{N}(\\bar{\\mu}, \\bar{v})\\right)$, where the aggregate posterior $q(z)$ is approximated by a Gaussian with mean $\\bar{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\mu_i^\\star$ and variance $\\bar{v} = \\frac{1}{n}\\sum_{i=1}^{n} \\sigma_i^{\\star 2} + \\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})$ using the law of total variance for mixtures.\n   - The variance of encoder means across the dataset, $\\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})$.\n3. Declare posterior collapse as a boolean outcome for the test case if and only if all of the following conditions hold simultaneously:\n   - $\\overline{\\mathrm{KL}} < \\varepsilon_{\\mathrm{KL}}$,\n   - $I(X;Z) < \\varepsilon_{\\mathrm{MI}}$,\n   - $\\mathrm{Var}(\\{\\mu_i^\\star\\}) < \\varepsilon_{\\mu}$,\n   where thresholds are fixed as $\\varepsilon_{\\mathrm{KL}} = 0.05$, $\\varepsilon_{\\mathrm{MI}} = 0.01$, and $\\varepsilon_{\\mu} = 0.005$. These diagnostics are designed to capture a regime where the variational posterior is close to the prior, the latent variables carry negligible information about observations, and the encoder outputs are nearly invariant across inputs.\n\nHypothesis to be tested computationally: As $\\beta$ increases, the Kullback–Leibler divergence penalty increasingly dominates the ELBO, driving $\\mu_i^\\star \\to 0$ and $\\sigma_i^\\star \\to 1$ for most $x_i$ (posterior collapse), particularly when the decoder sensitivity $w$ is weak or the data are highly sparse (many zeros). Conversely, small $\\beta$ allows informative deviations in the posterior that improve reconstruction.\n\nTest Suite:\n- Test case 1 (happy path, informative regime): dataset counts $[0,0,1,0,2,0,0,1,0]$, decoder parameters $(w,b) = (1.0,\\log(0.3))$, and $\\beta = 0.1$.\n- Test case 2 (high-$\\beta$ regime): same dataset counts and decoder parameters as Test case 1, but $\\beta = 10.0$.\n- Test case 3 (boundary case: extreme sparsity): dataset counts $[0,0,0,0,0,0,0,0,0,0,0,0]$, decoder parameters $(w,b) = (1.0,\\log(0.5))$, and $\\beta = 1.0$.\n- Test case 4 (edge case: weak decoder sensitivity): dataset counts $[0,1,3,0,2,1,0,4,0,1]$, decoder parameters $(w,b) = (0.05,0.0)$, and $\\beta = 0.5$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the posterior collapse decisions for the four test cases in the order listed above, as a comma-separated Python list of booleans enclosed in square brackets, for example, \"[False,True,False,True]\".\n- No additional text should be printed.\n\nAll variables, functions, operators, and numbers in this problem description are mathematical and must be written in LaTeX. Angles and physical units do not apply here. Express all thresholds and outputs numerically as booleans, integers, floats, or lists of these types, following the specified final output format.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of variational inference and generative models, is well-posed with a clear objective and defined parameters, and is expressed objectively. The task is to analyze the behavior of a $\\beta$-Variational Autoencoder ($\\beta$-VAE) with a Poisson decoder, specifically to detect posterior collapse under different conditions.\n\nThe solution proceeds in several steps:\n1.  Derive the Evidence Lower Bound (ELBO) objective function for a single observation $x$.\n2.  Define the numerical optimization procedure to find the optimal variational parameters $(\\mu^\\star, \\sigma^\\star)$ for each observation.\n3.  Define the diagnostic metrics for detecting posterior collapse.\n4.  Implement an algorithm to process each test case, perform the optimizations, compute the diagnostics, and apply the collapse criteria.\n\n**1. The Evidence Lower Bound (ELBO) Derivation**\n\nThe ELBO for a single observation $x$ is given by:\n$$\n\\mathrm{ELBO}(x;\\beta) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\beta \\, \\mathrm{KL}(q(z \\mid x) \\| p(z))\n$$\nThe variational posterior is $q(z \\mid x) = \\mathcal{N}(z; \\mu, \\sigma^2)$ and the prior is $p(z) = \\mathcal{N}(z; 0, 1)$. The two terms of the ELBO are analyzed separately.\n\n**Kullback-Leibler (KL) Divergence Term**\nThe KL divergence between the variational posterior and the standard Gaussian prior is given by the formula:\n$$\n\\mathrm{KL}(q(z \\mid x) \\| p(z)) = \\mathrm{KL}(\\mathcal{N}(\\mu, \\sigma^2) \\| \\mathcal{N}(0,1)) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - \\log \\sigma^2 - 1\\right)\n$$\nUsing the property $\\log \\sigma^2 = 2\\log\\sigma$, this can be written as:\n$$\n\\mathrm{KL}(q(z \\mid x) \\| p(z)) = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2 - 2\\log\\sigma - 1\\right)\n$$\n\n**Reconstruction Term**\nThe reconstruction term is the expected log-likelihood of the data under the variational posterior. The decoder is $p(x \\mid z) = \\mathrm{Poisson}(\\lambda)$ with rate $\\lambda = \\exp(w z + b)$. The log-likelihood is:\n$$\n\\log p(x \\mid z) = \\log\\left( \\frac{e^{-\\lambda} \\lambda^x}{x!} \\right) = -\\lambda + x \\log\\lambda - \\log(x!)\n$$\nSubstituting $\\lambda = \\exp(w z + b)$:\n$$\n\\log p(x \\mid z) = -\\exp(w z + b) + x(w z + b) - \\log(x!)\n$$\nWe take the expectation with respect to $z \\sim q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^2)$:\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = \\mathbb{E}[-\\exp(w z + b)] + \\mathbb{E}[x(w z + b)] - \\mathbb{E}[\\log(x!)]\n$$\nUsing linearity of expectation and that $x$ and $\\log(x!)$ are constant with respect to $z$:\n$$\n= -\\mathbb{E}[\\exp(w z + b)] + x(w \\mathbb{E}[z] + b) - \\log(x!)\n$$\nSince $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we have $\\mathbb{E}[z] = \\mu$. The variable $y = w z + b$ is also Gaussian, with mean $\\mathbb{E}[y] = w\\mu + b$ and variance $\\mathrm{Var}(y) = w^2\\sigma^2$. The expectation $\\mathbb{E}[\\exp(y)]$ is the moment-generating function of a Gaussian variable $y$ evaluated at $t=1$, which is $\\exp(\\mathbb{E}[y] + \\frac{1}{2}\\mathrm{Var}(y))$.\n$$\n\\mathbb{E}[\\exp(w z + b)] = \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right)\n$$\nPutting it all together, the reconstruction term is:\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = x w \\mu + xb - \\log(x!) - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right)\n$$\n\n**Full ELBO Objective**\nCombining the terms, the ELBO is:\n$$\n\\mathrm{ELBO} = \\left( x w \\mu + xb - \\log(x!) - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right) \\right) - \\frac{\\beta}{2}\\left(\\mu^2 + \\sigma^2 - 2\\log\\sigma - 1\\right)\n$$\n\n**2. Optimization Procedure**\n\nFor each observation $x_i$, we must find the variational parameters $(\\mu_i^\\star, \\sigma_i^\\star)$ that maximize the ELBO. This is a numerical optimization problem. The terms constant with respect to $(\\mu, \\sigma)$, namely $xb$, $-\\log(x!)$, and $-\\beta/2(-1)$, can be dropped from the objective function without changing the location of the maximum. The function to maximize is therefore:\n$$\nL(\\mu, \\sigma) = x w \\mu - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\sigma^2\\right) - \\frac{\\beta}{2}\\left(\\mu^2 + \\sigma^2 - 2\\log\\sigma\\right)\n$$\nThe optimization is over $\\mu \\in \\mathbb{R}$ and $\\sigma \\in \\mathbb{R}_{>0}$. To handle the constraint $\\sigma > 0$, we use the reparameterization $\\sigma = \\exp(\\rho)$, where $\\rho \\in \\mathbb{R}$. The objective function becomes an unconstrained problem over $(\\mu, \\rho)$:\n$$\nL(\\mu, \\rho) = x w \\mu - \\exp\\left(w\\mu + b + \\frac{1}{2}w^2\\exp(2\\rho)\\right) - \\frac{\\beta}{2}\\left(\\mu^2 + \\exp(2\\rho) - 2\\rho\\right)\n$$\nWe will minimize $-L(\\mu, \\rho)$ using a numerical optimization algorithm such as BFGS, starting from an initial guess of $(\\mu, \\rho) = (0, 0)$, which corresponds to $(\\mu, \\sigma) = (0, 1)$ (the prior).\n\n**3. Diagnostic Metrics for Posterior Collapse**\n\nAfter obtaining the optimal parameters $(\\mu_i^\\star, \\sigma_i^\\star)$ for each observation $x_i$ in a dataset of size $n$, we compute the following diagnostics:\n\n- **Average KL Divergence**: This measures how far, on average, the learned posteriors are from the prior. A small value indicates that the posteriors have \"collapsed\" to the prior.\n  $$\n  \\overline{\\mathrm{KL}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{KL}(\\mathcal{N}(\\mu_i^\\star, \\sigma_i^{\\star 2}) \\| \\mathcal{N}(0,1)) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mu_i^{\\star 2} + \\sigma_i^{\\star 2} - 2\\log\\sigma_i^\\star - 1\\right)\n  $$\n\n- **Approximation to Mutual Information $I(X;Z)$**: This measures how much information the latent variables $Z$ contain about the observations $X$. A low value indicates an uninformative representation. It is approximated by averaging the KL divergence from each posterior $q(z \\mid x_i)$ to an aggregated posterior $q_{agg}(z) = \\mathcal{N}(\\bar{\\mu}, \\bar{v})$. The parameters of the aggregated posterior are:\n  $$\n  \\bar{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\mu_i^\\star \\quad \\text{and} \\quad \\bar{v} = \\frac{1}{n}\\sum_{i=1}^{n} \\sigma_i^{\\star 2} + \\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})\n  $$\n  The KL divergence between two general univariate Gaussians $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ is:\n  $$\n  \\mathrm{KL}(\\mathcal{N}_1 \\| \\mathcal{N}_2) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n  $$\n  Thus, the mutual information approximation is:\n  $$\n  I(X;Z) \\approx \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log\\frac{\\sqrt{\\bar{v}}}{\\sigma_i^\\star} + \\frac{\\sigma_i^{\\star 2} + (\\mu_i^\\star - \\bar{\\mu})^2}{2\\bar{v}} - \\frac{1}{2} \\right)\n  $$\n\n- **Variance of Encoder Means**: This measures how much the encoder's output mean varies across the dataset. A very low variance suggests the encoder is ignoring the input.\n  $$\n  \\mathrm{Var}(\\{\\mu_i^\\star\\}_{i=1}^{n})\n  $$\n\n**4. Posterior Collapse Condition**\n\nPosterior collapse is declared if all three of the following conditions are met, using the provided thresholds $\\varepsilon_{\\mathrm{KL}} = 0.05$, $\\varepsilon_{\\mathrm{MI}} = 0.01$, and $\\varepsilon_{\\mu} = 0.005$:\n1.  $\\overline{\\mathrm{KL}} < \\varepsilon_{\\mathrm{KL}}$\n2.  $I(X;Z) < \\varepsilon_{\\mathrm{MI}}$\n3.  $\\mathrm{Var}(\\{\\mu_i^\\star\\}) < \\varepsilon_{\\mu}$\n\nThe algorithm implemented in the final answer follows these steps for each test case to produce the required boolean output.",
            "answer": "[False,True,True,True]"
        },
        {
            "introduction": "Once you have trained several candidate VAE models, how do you decide which one is best? While the ELBO is optimized during training, it is only a lower bound on the true marginal log-likelihood and can be a misleading metric for model comparison. This exercise  equips you with a more principled approach to model evaluation by using importance sampling to estimate the marginal log-likelihood on held-out data. You will implement this powerful technique and compare its results to the simpler ELBO, gaining critical insight into the proper evaluation and selection of generative models.",
            "id": "3357955",
            "problem": "Consider a Variational Autoencoder (VAE) for representation learning in computational systems biology, where the latent variable model is used to capture low-dimensional structure in gene expression profiles. Let the generative model be defined by a standard normal prior on the latent variables and a linear Gaussian decoder. Specifically, for any observation vector $x \\in \\mathbb{R}^{d_x}$ and latent vector $z \\in \\mathbb{R}^{d_z}$:\n$$\np(z) = \\mathcal{N}(z; 0, I), \\quad p_{\\theta}(x \\mid z) = \\mathcal{N}\\left(x; W z + b, \\sigma_x^2 I\\right),\n$$\nwhere $W \\in \\mathbb{R}^{d_x \\times d_z}$, $b \\in \\mathbb{R}^{d_x}$, and $\\sigma_x^2 \\in \\mathbb{R}_{>0}$. The variational family is a diagonal Gaussian encoder\n$$\nq_{\\phi}(z \\mid x) = \\mathcal{N}\\left(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2)\\right), \\quad \\mu_{\\phi}(x) = A x + a,\n$$\nwith $A \\in \\mathbb{R}^{d_z \\times d_x}$, $a \\in \\mathbb{R}^{d_z}$, and constant diagonal standard deviations $\\sigma_{\\phi} \\in \\mathbb{R}_{>0}^{d_z}$.\n\nYou are given two candidate models $\\mathcal{M}_1$ and $\\mathcal{M}_2$ with fixed decoders $(W, b, \\sigma_x)$ and encoders $(A, a, \\sigma_{\\phi})$. The task is to derive from first principles and implement a program that computes, for each model:\n- The average training Evidence Lower Bound (ELBO) over a fixed training set.\n- The average held-out log marginal likelihood $\\log p_{\\theta}(x)$ over a fixed validation set, estimated by importance sampling using samples from $q_{\\phi}(z \\mid x)$.\n\nBegin from the following fundamental bases:\n- Bayes’ rule and the definition of marginal likelihood, namely $\\log p_{\\theta}(x) = \\log \\int p_{\\theta}(x, z)\\, dz$.\n- The change-of-measure identity for expectations under absolutely continuous distributions, which justifies importance sampling.\n- The definition of the Evidence Lower Bound (ELBO) as the variational lower bound on the marginal log-likelihood derived via Jensen’s inequality.\n\nYour derivation must proceed by:\n- Expressing $\\log p_{\\theta}(x)$ as an expectation with respect to $q_{\\phi}(z \\mid x)$ that is amenable to Monte Carlo estimation by importance sampling. Do not assume any specific estimator form a priori; derive it from the change-of-measure principle.\n- Deriving a closed-form expression for the ELBO under the linear Gaussian decoder with isotropic observation noise and diagonal Gaussian encoder, by evaluating the expectation of quadratic forms and the Kullback–Leibler divergence between diagonal and standard normal Gaussians.\n\nThen implement the derived estimators to compare the two models for model selection.\n\nUse the following fixed dimensions and parameters, which constitute the test suite:\n\nDimensions:\n- $d_z = 2$, $d_x = 3$.\n\nModel $\\mathcal{M}_1$:\n$$\nW_1 = \\begin{bmatrix}\n1.0 & 0.5 \\\\\n0.0 & 1.0 \\\\\n-0.5 & 0.5\n\\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix}\n0.1 \\\\ -0.2 \\\\ 0.0\n\\end{bmatrix}, \\quad\n\\sigma_{x,1} = 0.2.\n$$\nEncoder for $\\mathcal{M}_1$:\n$$\nA_1 = \\begin{bmatrix}\n0.6 & 0.1 & -0.2 \\\\\n0.0 & 0.5 & 0.3\n\\end{bmatrix}, \\quad\na_1 = \\begin{bmatrix}\n0.0 \\\\ 0.0\n\\end{bmatrix}, \\quad\n\\sigma_{\\phi,1} = \\begin{bmatrix}\n0.6 \\\\ 0.6\n\\end{bmatrix}.\n$$\n\nModel $\\mathcal{M}_2$:\n$$\nW_2 = \\begin{bmatrix}\n0.8 & -0.3 \\\\\n0.2 & 0.9 \\\\\n0.3 & 0.3\n\\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix}\n-0.1 \\\\ 0.1 \\\\ 0.2\n\\end{bmatrix}, \\quad\n\\sigma_{x,2} = 0.2.\n$$\nEncoder for $\\mathcal{M}_2$:\n$$\nA_2 = \\begin{bmatrix}\n0.5 & 0.2 & -0.1 \\\\\n0.1 & 0.4 & 0.2\n\\end{bmatrix}, \\quad\na_2 = \\begin{bmatrix}\n0.1 \\\\ -0.1\n\\end{bmatrix}, \\quad\n\\sigma_{\\phi,2} = \\begin{bmatrix}\n0.7 \\\\ 0.5\n\\end{bmatrix}.\n$$\n\nDatasets:\n- Training set $X_{\\text{train}} = \\left\\{\n\\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.2 \\end{bmatrix},\n\\begin{bmatrix} 0.0 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix},\n\\begin{bmatrix} -1.2 \\\\ 0.7 \\\\ 0.8 \\end{bmatrix}\n\\right\\}$.\n- Validation set $X_{\\text{val}} = \\left\\{\n\\begin{bmatrix} 0.9 \\\\ 0.4 \\\\ -0.1 \\end{bmatrix},\n\\begin{bmatrix} -0.8 \\\\ 0.6 \\\\ 0.9 \\end{bmatrix}\n\\right\\}$.\n\nProgram requirements:\n- Compute the average training ELBO for each model using the closed-form expression you derived for the linear Gaussian case.\n- Estimate the average held-out $\\log p_{\\theta}(x)$ on $X_{\\text{val}}$ for each model by importance sampling using $K \\in \\{1, 10, 1000\\}$ samples per validation point, with samples drawn from the corresponding $q_{\\phi}(z \\mid x)$. Use a fixed random seed equal to $12345$ for reproducibility. Implement the estimator in a numerically stable way using the log-sum-exp trick.\n- For model selection, for each criterion (training ELBO, importance sampling with $K = 1$, $K = 10$, and $K = 1000$), choose the model with the higher average score on the corresponding dataset and estimation procedure. Index the models as $1$ for $\\mathcal{M}_1$ and $2$ for $\\mathcal{M}_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{model\\_by\\_ELBO}, \\text{model\\_by\\_IS\\_K=1}, \\text{model\\_by\\_IS\\_K=10}, \\text{model\\_by\\_IS\\_K=1000}]$, where each entry is an integer in $\\{1,2\\}$.\n\nAngle units, physical units, and percentages are not applicable to this problem. All computations are dimensionless real-valued quantities. The test suite consists of the specified parameters, datasets, and the three values of $K$, which together exercise the correctness, stability, and convergence behavior of the estimators across small and large sample regimes.",
            "solution": "The problem statement is a valid, well-posed, and self-contained exercise in computational statistics, specifically concerning the analysis of Variational Autoencoders (VAEs). All required parameters, data, and procedural instructions are provided. We will proceed with the derivation of the necessary mathematical formulae, followed by their implementation to perform the requested model comparison.\n\nThe problem requires the derivation and implementation of two key quantities for a linear-Gaussian VAE: the Evidence Lower Bound (ELBO) and an importance sampling estimator for the marginal log-likelihood, $\\log p_{\\theta}(x)$.\n\nLet the VAE model be defined as follows:\n- Prior on latent variables $z \\in \\mathbb{R}^{d_z}$: $p(z) = \\mathcal{N}(z; 0, I)$.\n- Decoder (generative model) for observations $x \\in \\mathbb{R}^{d_x}$: $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; Wz + b, \\sigma_x^2 I)$.\n- Encoder (variational posterior): $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2))$, where $\\mu_{\\phi}(x) = Ax + a$. The parameters are $\\theta = \\{W, b, \\sigma_x\\}$ and $\\phi = \\{A, a, \\sigma_{\\phi}\\}$. For notational simplicity, we will denote $\\mu_q(x) = \\mu_{\\phi}(x)$, and the covariance matrix of the encoder as $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^2)$.\n\n### Derivation 1: Importance Sampling Estimator for the Marginal Log-Likelihood\n\nThe marginal likelihood of an observation $x$, denoted $p_{\\theta}(x)$, is defined by integrating out the latent variables from the joint probability distribution:\n$$\np_{\\theta}(x) = \\int p_{\\theta}(x, z) dz = \\int p_{\\theta}(x \\mid z) p(z) dz\n$$\nWe wish to estimate its logarithm, $\\log p_{\\theta}(x)$. Direct Monte Carlo estimation by sampling from the prior $p(z)$ is often inefficient. A more effective method is importance sampling, which is justified by the change-of-measure identity. We introduce the variational posterior $q_{\\phi}(z \\mid x)$ as the proposal distribution.\n\nBy multiplying and dividing the integrand by $q_{\\phi}(z \\mid x)$, we can express $p_{\\theta}(x)$ as an expectation with respect to $q_{\\phi}(z \\mid x)$:\n$$\np_{\\theta}(x) = \\int \\frac{p_{\\theta}(x \\mid z) p(z)}{q_{\\phi}(z \\mid x)} q_{\\phi}(z \\mid x) dz = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}\\left[ \\frac{p_{\\theta}(x \\mid z) p(z)}{q_{\\phi}(z \\mid x)} \\right]\n$$\nThe term inside the expectation is the importance weight, $w(z) = \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)}$.\nA Monte Carlo estimator for $p_{\\theta}(x)$ is formed by drawing $K$ independent samples $\\{z^{(k)}\\}_{k=1}^K$ from $q_{\\phi}(z \\mid x)$ and computing the sample mean of the weights:\n$$\n\\hat{p}_{\\theta}(x) = \\frac{1}{K} \\sum_{k=1}^K w(z^{(k)}) = \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\theta}(x \\mid z^{(k)}) p(z^{(k)})}{q_{\\phi}(z^{(k)} \\mid x)}\n$$\nThe corresponding estimator for the marginal log-likelihood is $\\log \\hat{p}_{\\theta}(x)$. For numerical stability, it is crucial to perform calculations in log-space. Let $\\log w^{(k)}$ be the log-importance-weight for sample $z^{(k)}$:\n$$\n\\log w^{(k)} = \\log p_{\\theta}(x \\mid z^{(k)}) + \\log p(z^{(k)}) - \\log q_{\\phi}(z^{(k)} \\mid x)\n$$\nThe log-likelihood estimate is then:\n$$\n\\log \\hat{p}_{\\theta}(x) = \\log \\left( \\frac{1}{K} \\sum_{k=1}^K e^{\\log w^{(k)}} \\right) = \\log\\left(\\sum_{k=1}^K e^{\\log w^{(k)}}\\right) - \\log K\n$$\nTo prevent numerical underflow or overflow, we utilize the log-sum-exp trick. Let $c = \\max_{k} \\{\\log w^{(k)}\\}$. The expression becomes:\n$$\n\\log \\hat{p}_{\\theta}(x) = c + \\log\\left(\\sum_{k=1}^K e^{\\log w^{(k)} - c}\\right) - \\log K\n$$\nThis is the estimator we shall implement.\n\n### Derivation 2: Closed-Form Expression for the Evidence Lower Bound (ELBO)\n\nThe Evidence Lower Bound, $\\mathcal{L}(\\theta, \\phi; x)$, is derived from the decomposition of the marginal log-likelihood: $\\log p_{\\theta}(x) = \\mathcal{L}(\\theta, \\phi; x) + D_{KL}(q_{\\phi}(z \\mid x) \\| p_{\\theta}(z \\mid x))$. By Jensen's inequality, $\\mathcal{L}(\\theta, \\phi; x) \\leq \\log p_{\\theta}(x)$. The standard expression for the ELBO is:\n$$\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x)] = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - D_{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\nWe will now derive a closed-form expression for each term under the specified linear-Gaussian model.\n\n**First Term: Expected Reconstruction Log-Likelihood**\nThe log-pdf of the decoder is:\n$$\n\\log p_{\\theta}(x \\mid z) = \\log \\mathcal{N}(x; Wz + b, \\sigma_x^2 I) = -\\frac{1}{2\\sigma_x^2}\\|x - (Wz + b)\\|_2^2 - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2)\n$$\nWe take the expectation with respect to $z \\sim q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q(x), \\Sigma_q)$:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q_{\\phi}}[\\|x - b - Wz\\|_2^2] - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2)\n$$\nThe expectation of the squared norm is expanded using the law of total expectation: $\\mathbb{E}[Y^T Y] = \\mathbb{E}[Y]^T \\mathbb{E}[Y] + \\text{Tr}(\\text{Cov}(Y))$. Let $Y = x - b - Wz$. Then $\\mathbb{E}[Y] = x - b - W\\mathbb{E}[z] = x - b - W\\mu_q(x)$. The covariance is $\\text{Cov}(Y) = \\text{Cov}(Wz) = W \\text{Cov}(z) W^T = W \\Sigma_q W^T$.\nThus, $\\mathbb{E}_{q_{\\phi}}[\\|x - b - Wz\\|_2^2] = \\|x - b - W\\mu_q(x)\\|_2^2 + \\text{Tr}(W \\Sigma_q W^T)$. Using the cyclic property of the trace, $\\text{Tr}(W \\Sigma_q W^T) = \\text{Tr}(W^T W \\Sigma_q)$.\nThe first term of the ELBO is:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{1}{2\\sigma_x^2}\\left( \\|x - b - W\\mu_q(x)\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q) \\right) - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2)\n$$\n\n**Second Term: Kullback-Leibler Divergence**\nThe second term is the KL divergence between the variational posterior $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q(x), \\Sigma_q)$ and the standard normal prior $p(z) = \\mathcal{N}(z; 0, I)$. For two multivariate Gaussians $q = \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $p = \\mathcal{N}(\\mu_2, \\Sigma_2)$, the KL divergence is:\n$$\nD_{KL}(q\\|p) = \\frac{1}{2}\\left( \\log\\frac{\\det\\Sigma_2}{\\det\\Sigma_1} - d_z + \\text{Tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2 - \\mu_1)^T\\Sigma_2^{-1}(\\mu_2 - \\mu_1) \\right)\n$$\nSubstituting our parameters ($\\mu_1 = \\mu_q(x)$, $\\Sigma_1 = \\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^2)$, $\\mu_2 = 0$, $\\Sigma_2 = I$), we get:\n$$\nD_{KL}(q_{\\phi}\\|p) = \\frac{1}{2}\\left( \\log\\frac{1}{\\det\\Sigma_q} - d_z + \\text{Tr}(\\Sigma_q) + \\mu_q(x)^T \\mu_q(x) \\right)\n$$\nSince $\\Sigma_q$ is diagonal, $\\det\\Sigma_q = \\prod_{j=1}^{d_z} \\sigma_{\\phi,j}^2$ and $\\text{Tr}(\\Sigma_q) = \\sum_{j=1}^{d_z} \\sigma_{\\phi,j}^2$.\n$$\nD_{KL}(q_{\\phi}\\|p) = \\frac{1}{2}\\left( -\\sum_{j=1}^{d_z}\\log(\\sigma_{\\phi,j}^2) - d_z + \\sum_{j=1}^{d_z}\\sigma_{\\phi,j}^2 + \\|\\mu_q(x)\\|_2^2 \\right)\n$$\nRe-arranging terms on a per-dimension basis gives the familiar form:\n$$\nD_{KL}(q_{\\phi}\\|p) = \\frac{1}{2} \\sum_{j=1}^{d_z} \\left( \\mu_{q,j}(x)^2 + \\sigma_{\\phi,j}^2 - \\log(\\sigma_{\\phi,j}^2) - 1 \\right)\n$$\n\n**Final ELBO Expression**\nCombining the two terms, we obtain the final closed-form expression for the ELBO:\n$$\n\\mathcal{L}(x) = -\\frac{1}{2\\sigma_x^2}\\left( \\|x - b - W\\mu_q(x)\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q) \\right) - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2} \\sum_{j=1}^{d_z} \\left( \\mu_{q,j}(x)^2 + \\sigma_{\\phi,j}^2 - \\log(\\sigma_{\\phi,j}^2) - 1 \\right)\n$$\nwhere $\\mu_q(x) = Ax+a$ and $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^2)$. This expression will be implemented to compute the average ELBO on the training set.\n\n### Computational Procedure\n\n1.  We will implement a class to represent each model, encapsulating its specific parameters.\n2.  A method for this class will compute the ELBO for a given data point $x$ using the derived closed-form formula.\n3.  A second method will estimate the marginal log-likelihood $\\log p_{\\theta}(x)$ using the derived importance sampling estimator. This method will require helper functions to compute the log-pdfs of the prior, decoder, and encoder distributions. Samples will be drawn from the encoder using a fixed random seed for reproducibility.\n4.  For model selection, we will iterate through the four specified criteria:\n    a.  Average training ELBO: We compute the mean of $\\mathcal{L}(x)$ for each model over all $x \\in X_{\\text{train}}$. The model with the higher value is chosen.\n    b.  Average held-out log-likelihood: For each $K \\in \\{1, 10, 1000\\}$, we compute the mean of $\\log \\hat{p}_{\\theta}(x)$ for each model over all $x \\in X_{\\text{val}}$. The model with the higher value is chosen for each $K$.\n5.  The final output will be a list of model indices ($1$ or $2$) corresponding to the winning model for each of the four criteria.",
            "answer": "[2,1,2,2]"
        }
    ]
}