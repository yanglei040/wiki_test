{
    "hands_on_practices": [
        {
            "introduction": "在分析高维生物数据（如基因表达谱）时，预测变量之间的共线性是一个常见挑战。本练习将通过奇异值分解（SVD）来揭示岭回归如何应对此问题，让您从根本上理解其收缩机制 。通过推导并计算一个具体案例，您将深入体会岭回归是如何沿着不同主成分方向差异化地调整系数，从而稳定模型估计的。",
            "id": "3345370",
            "problem": "在计算系统生物学的一项转录调控网络推断任务中，一个基因表达向量 $y \\in \\mathbb{R}^{n}$ 被建模为两个转录因子（TF）活性谱的线性组合，这两个活性谱收集在设计矩阵 $X \\in \\mathbb{R}^{n \\times 2}$ 中。由于转录因子谱存在近似共线性，因此使用岭回归通过最小化惩罚最小二乘目标函数来估计转录因子效应 $\\beta \\in \\mathbb{R}^{2}$：\n$$\n\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}.\n$$\n令 $X$ 的奇异值分解（SVD）为 $X = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times 2}$ 和 $V \\in \\mathbb{R}^{2 \\times 2}$ 是标准正交矩阵，且 $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}) \\in \\mathbb{R}^{2 \\times 2}$ 满足 $\\sigma_{1} \\ge \\sigma_{2} > 0$。从岭回归的定义和线性代数恒等式出发，推导岭估计量 $\\hat{\\beta}^{\\mathrm{ridge}}$ 关于 $U$、$\\Sigma$、$V$ 和 $y$ 的闭式表达式，并用它来分析收缩作用如何沿着转录因子共线性的方向发生。\n\n假设 $n = 100$，$\\lambda = 1$，且SVD各分量满足\n$$\n\\Sigma = \\operatorname{diag}(10,\\,1), \\quad V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}, \\quad U^{\\top}y = \\begin{pmatrix}10 \\\\ 2\\end{pmatrix}.\n$$\n计算第一个转录因子系数，即 $\\hat{\\beta}^{\\mathrm{ridge}}$ 的第一个分量，并用精确值表示答案（不要四舍五入）。",
            "solution": "所述问题具有科学依据，提法恰当且客观。它为统计学习理论（岭回归）中的一个标准问题，应用于相关领域（计算系统生物学），提供了一套完整且一致的已知条件。该问题是有效的。\n\n岭回归估计量 $\\hat{\\beta}^{\\mathrm{ridge}}$ 是使目标函数最小化的向量 $\\beta$：\n$$L(\\beta) = \\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}$$\n为求最小值，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度并令其为零。目标函数可以写成：\n$$L(\\beta) = \\frac{1}{n}(y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top}\\beta = \\frac{1}{n}(y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta) + \\lambda \\beta^{\\top}\\beta$$\n关于 $\\beta$ 的梯度是：\n$$\\nabla_{\\beta} L(\\beta) = \\frac{1}{n}(-2X^{\\top}y + 2X^{\\top}X\\beta) + 2\\lambda\\beta$$\n将梯度设为零以求最优的 $\\hat{\\beta}^{\\mathrm{ridge}}$：\n$$\\frac{1}{n}(2X^{\\top}X\\hat{\\beta}^{\\mathrm{ridge}} - 2X^{\\top}y) + 2\\lambda\\hat{\\beta}^{\\mathrm{ridge}} = 0$$\n乘以 $\\frac{n}{2}$：\n$$(X^{\\top}X)\\hat{\\beta}^{\\mathrm{ridge}} - X^{\\top}y + n\\lambda\\hat{\\beta}^{\\mathrm{ridge}} = 0$$\n重新整理各项以解出 $\\hat{\\beta}^{\\mathrm{ridge}}$：\n$$(X^{\\top}X + n\\lambda I)\\hat{\\beta}^{\\mathrm{ridge}} = X^{\\top}y$$\n这就得到了岭回归解的标准形式：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = (X^{\\top}X + n\\lambda I)^{-1} X^{\\top}y$$\n现在我们将奇异值分解（SVD）$X = U\\Sigma V^{\\top}$ 代入此表达式。首先，我们计算分量 $X^{\\top}X$ 和 $X^{\\top}y$：\n$$X^{\\top}X = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top}$$\n因为 $U$ 的列是标准正交的，所以 $U^{\\top}U = I_{2}$，即 $2 \\times 2$ 的单位矩阵。另外，$\\Sigma$ 是对角矩阵，所以 $\\Sigma^{\\top} = \\Sigma$。\n$$X^{\\top}X = V\\Sigma I_{2} \\Sigma V^{\\top} = V\\Sigma^{2}V^{\\top}$$\n对于 $X^{\\top}y$：\n$$X^{\\top}y = (U\\Sigma V^{\\top})^{\\top}y = V\\Sigma^{\\top}U^{\\top}y = V\\Sigma U^{\\top}y$$\n将这些代回到 $\\hat{\\beta}^{\\mathrm{ridge}}$ 的表达式中：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = (V\\Sigma^{2}V^{\\top} + n\\lambda I)^{-1} (V\\Sigma U^{\\top}y)$$\n因为 $V$ 是一个标准正交矩阵（$V^{-1} = V^{\\top}$），我们可以将单位矩阵 $I$ 重写为 $I = VV^{\\top}$。\n$$(V\\Sigma^{2}V^{\\top} + n\\lambda VV^{\\top})^{-1} = (V(\\Sigma^{2} + n\\lambda I)V^{\\top})^{-1}$$\n使用性质 $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$，我们得到：\n$$(V(\\Sigma^{2} + n\\lambda I)V^{\\top})^{-1} = (V^{\\top})^{-1}(\\Sigma^{2} + n\\lambda I)^{-1}V^{-1} = V(\\Sigma^{2} + n\\lambda I)^{-1}V^{\\top}$$\n因此，$\\hat{\\beta}^{\\mathrm{ridge}}$ 的表达式变为：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = V(\\Sigma^{2} + n\\lambda I)^{-1}V^{\\top} V\\Sigma U^{\\top}y$$\n因为 $V^{\\top}V = I_{2}$：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = V(\\Sigma^{2} + n\\lambda I)^{-1}\\Sigma U^{\\top}y$$\n这就是岭估计量关于 $U$、$\\Sigma$、$V$ 和 $y$ 的闭式表达式。\n\n为了分析收缩的作用方式，我们考虑普通最小二乘（OLS）估计量 $\\hat{\\beta}^{\\mathrm{OLS}}$ 的解，它对应于 $\\lambda=0$ 的情况（暂时忽略 $1/n$ 的缩放，因为 OLS 定义为最小化 $\\|y-X\\beta\\|_2^2$）。OLS 估计量为 $\\hat{\\beta}^{\\mathrm{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y = V\\Sigma^{-1}U^{\\top}y$。我们为参数定义一个旋转坐标系 $\\alpha = V^{\\top}\\beta$。那么 $\\hat{\\alpha}^{\\mathrm{OLS}} = V^{\\top}\\hat{\\beta}^{\\mathrm{OLS}} = \\Sigma^{-1}U^{\\top}y$。对于岭估计量，$\\hat{\\alpha}^{\\mathrm{ridge}} = V^{\\top}\\hat{\\beta}^{\\mathrm{ridge}} = (\\Sigma^{2} + n\\lambda I)^{-1}\\Sigma U^{\\top}y$。\n我们将其分量式地写出。$\\hat{\\alpha}^{\\mathrm{OLS}}$ 的第 $j$ 个分量是 $(\\hat{\\alpha}^{\\mathrm{OLS}})_{j} = \\frac{(U^{\\top}y)_j}{\\sigma_j}$。\n$\\hat{\\alpha}^{\\mathrm{ridge}}$ 的第 $j$ 个分量是 $(\\hat{\\alpha}^{\\mathrm{ridge}})_{j} = \\frac{\\sigma_j}{\\sigma_j^2 + n\\lambda}(U^{\\top}y)_j$。\n我们可以在这个旋转坐标系中用 OLS 解来表示岭回归的解：\n$$(\\hat{\\alpha}^{\\mathrm{ridge}})_{j} = \\frac{\\sigma_j^2}{\\sigma_j^2 + n\\lambda} \\left(\\frac{(U^{\\top}y)_j}{\\sigma_j}\\right) = \\frac{\\sigma_j^2}{\\sigma_j^2 + n\\lambda} (\\hat{\\alpha}^{\\mathrm{OLS}})_{j}$$\n收缩因子为 $s_j = \\frac{\\sigma_j^2}{\\sigma_j^2 + n\\lambda}$。由于 $\\sigma_1 \\ge \\sigma_2  0$，并且函数 $f(x) = \\frac{x}{x + c}$（对于 $c0$）在 $x0$ 时是增函数，我们有 $s_1 \\ge s_2$。这表明与较小奇异值 $\\sigma_2$ 对应的系数会更强烈地向零收缩。$V$ 的列所定义的方向是预测变量的主成分方向。一个小的奇异值 $\\sigma_j$ 对应于预测变量空间中方差较小的方向，这是一个（近似）共线性的方向。因此，岭回归沿着共线性较高的方向应用更大的收缩，从而稳定估计值。\n\n现在，我们使用给定的值计算第一个转录因子系数：\n$n = 100$，$\\lambda = 1$，$\\Sigma = \\operatorname{diag}(10, 1)$，$V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}$，以及 $U^{\\top}y = \\begin{pmatrix}10 \\\\ 2\\end{pmatrix}$。\n\n首先，计算项 $n\\lambda$：\n$$n\\lambda = 100 \\times 1 = 100$$\n接下来，计算 $\\Sigma^2$：\n$$\\Sigma^{2} = \\operatorname{diag}(10^{2}, 1^{2}) = \\begin{pmatrix}100  0 \\\\ 0  1\\end{pmatrix}$$\n现在，计算矩阵 $(\\Sigma^{2} + n\\lambda I)$：\n$$\\Sigma^{2} + n\\lambda I = \\begin{pmatrix}100  0 \\\\ 0  1\\end{pmatrix} + 100\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} = \\begin{pmatrix}200  0 \\\\ 0  101\\end{pmatrix}$$\n其逆矩阵是：\n$$(\\Sigma^{2} + n\\lambda I)^{-1} = \\begin{pmatrix}\\frac{1}{200}  0 \\\\ 0  \\frac{1}{101}\\end{pmatrix}$$\n接下来，我们计算向量 $\\Sigma U^{\\top}y$：\n$$\\Sigma U^{\\top}y = \\begin{pmatrix}10  0 \\\\ 0  1\\end{pmatrix} \\begin{pmatrix}10 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}10 \\times 10 \\\\ 1 \\times 2\\end{pmatrix} = \\begin{pmatrix}100 \\\\ 2\\end{pmatrix}$$\n现在，我们可以计算中间向量 $z = (\\Sigma^{2} + n\\lambda I)^{-1}\\Sigma U^{\\top}y$：\n$$z = \\begin{pmatrix}\\frac{1}{200}  0 \\\\ 0  \\frac{1}{101}\\end{pmatrix} \\begin{pmatrix}100 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}\\frac{100}{200} \\\\ \\frac{2}{101}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{2}{101}\\end{pmatrix}$$\n最后，我们计算 $\\hat{\\beta}^{\\mathrm{ridge}} = Vz$：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{2}{101}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\\frac{1}{2} + \\frac{2}{101} \\\\ \\frac{1}{2} - \\frac{2}{101}\\end{pmatrix}$$\n题目要求的是第一个转录因子系数，即该向量的第一个分量：\n$$(\\hat{\\beta}^{\\mathrm{ridge}})_1 = \\frac{1}{\\sqrt{2}}\\left(\\frac{1}{2} + \\frac{2}{101}\\right)$$\n我们对括号内的和式进行通分：\n$$\\frac{1}{2} + \\frac{2}{101} = \\frac{101}{202} + \\frac{4}{202} = \\frac{105}{202}$$\n所以，第一个系数是：\n$$(\\hat{\\beta}^{\\mathrm{ridge}})_1 = \\frac{1}{\\sqrt{2}}\\frac{105}{202} = \\frac{105}{202\\sqrt{2}}$$\n将分母有理化得到最终的精确表达式：\n$$(\\hat{\\beta}^{\\mathrm{ridge}})_1 = \\frac{105\\sqrt{2}}{202 \\times 2} = \\frac{105\\sqrt{2}}{404}$$\n数字 $105 = 3 \\times 5 \\times 7$ 和 $404 = 4 \\times 101$ 没有公因子，所以这个分数是最简形式。",
            "answer": "$$\\boxed{\\frac{105\\sqrt{2}}{404}}$$"
        },
        {
            "introduction": "理解了岭回归如何收缩系数后，下一个关键问题是这种收缩在统计学上意味着什么。本练习引导您推导岭回归估计量所引入的偏差（bias）的解析表达式，从而深入探究正则化的核心——偏差-方差权衡 。通过这个推导，您将清晰地看到我们是如何通过接受一定的模型偏差来换取估计量方差的显著降低，这对于在高维设置下构建稳健的模型至关重要。",
            "id": "3345309",
            "problem": "在一项关于转录调控的系统级研究中，考虑一个线性模型，该模型根据 $p$ 个转录因子活性特征来预测一个目标基因在 $n$ 个实验条件下的表达。令 $X \\in \\mathbb{R}^{n \\times p}$ 表示设计矩阵，其具有满列秩且其列已经过中心化和标准化处理，令 $y \\in \\mathbb{R}^{n}$ 表示响应向量。假设数据由线性模型 $y = X \\beta^{\\star} + \\varepsilon$ 生成，其中 $\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实系数向量，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 是均值为零、方差为 $\\sigma^{2}  0$ 的高斯噪声。考虑带有惩罚参数 $\\alpha  0$ 的岭回归估计量，其定义为以下表达式的最小化子\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\frac{\\alpha}{2} \\| \\beta \\|_{2}^{2} \\right\\}.\n$$\n令格拉姆矩阵 $X^{\\top} X$ 具有特征分解 $X^{\\top} X = Q \\Lambda Q^{\\top}$，其中 $Q \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，$\\Lambda = \\operatorname{diag}(\\lambda_{1}, \\ldots, \\lambda_{p})$ 且对所有 $j$ 都有 $\\lambda_{j}  0$。令 $\\hat{\\beta}_{\\text{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y$ 表示普通最小二乘（OLS）估计量。\n\n任务：\n1. 从 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 的定义出发，推导其正规方程，并求解 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 的闭式解。\n2. 利用 $X^{\\top} X$ 的特征分解，将 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 表示为 $\\hat{\\beta}_{\\text{OLS}}$ 在 $X^{\\top} X$ 特征基下的一个线性变换，并确定施加于 OLS 系数在每个特征向量 $q_{j}$ 方向上的收缩因子。\n3. 使用数据生成模型 $y = X \\beta^{\\star} + \\varepsilon$，计算期望偏差向量 $\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star}$，并用投影 $b_{j}^{\\star} = q_{j}^{\\top} \\beta^{\\star}$ 和特征值 $\\{\\lambda_{j}\\}$ 在特征基 $\\{q_{j}\\}_{j=1}^{p}$ 中表示它。\n4. 最后，计算该偏差向量的欧几里得范数的平方 $\\| \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} \\|_{2}^{2}$，得到一个以 $\\{ \\lambda_{j} \\}$、$\\alpha$ 和 $\\{ b_{j}^{\\star} \\}$ 表示的闭式解析表达式。将此最终表达式作为您的答案。\n\n您的最终答案必须是一个单一的闭式解析表达式。不包含单位。不提供不等式或等式。不要四舍五入；不需要数值近似。",
            "solution": "该问题已经过验证，被认为是有效的，因为它具有科学依据、是适定的、客观的且内部一致。我们可以着手提供一个合理的解决方案。\n\n该问题要求推导岭回归估计量的偏差平方。我们将按顺序完成四个指定的任务。\n\n**任务1：推导正规方程并求解 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 的闭式解**\n\n岭回归估计量 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 被定义为目标函数 $J(\\beta)$ 的最小化子：\n$$\nJ(\\beta) = \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\frac{\\alpha}{2} \\| \\beta \\|_{2}^{2}\n$$\n其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\alpha  0$ 是正则化参数。\n\n为找到最小化子，我们首先展开目标函数。欧几里得范数的平方可以写成向量内积的形式：\n$$\nJ(\\beta) = \\frac{1}{2} (y - X \\beta)^{\\top}(y - X \\beta) + \\frac{\\alpha}{2} \\beta^{\\top}\\beta\n$$\n展开第一项得到：\n$$\nJ(\\beta) = \\frac{1}{2} (y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta) + \\frac{\\alpha}{2} \\beta^{\\top}\\beta\n$$\n由于 $y^{\\top}X\\beta$ 是一个标量，它等于其转置 $\\beta^{\\top}X^{\\top}y$。因此，我们可以合并这两项：\n$$\nJ(\\beta) = \\frac{1}{2} y^{\\top}y - y^{\\top}X\\beta + \\frac{1}{2}\\beta^{\\top}X^{\\top}X\\beta + \\frac{\\alpha}{2} \\beta^{\\top}\\beta\n$$\n目标函数 $J(\\beta)$ 是凸函数，并且由于 $\\alpha  0$，它是严格凸的。因此，在梯度相对于 $\\beta$ 为零的地方存在一个唯一的最小值。我们计算梯度 $\\nabla_{\\beta} J(\\beta)$：\n$$\n\\nabla_{\\beta} J(\\beta) = \\nabla_{\\beta} \\left( \\frac{1}{2} y^{\\top}y - y^{\\top}X\\beta + \\frac{1}{2}\\beta^{\\top}X^{\\top}X\\beta + \\frac{\\alpha}{2} \\beta^{\\top}\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} J(\\beta) = 0 - X^{\\top}y + X^{\\top}X\\beta + \\alpha\\beta\n$$\n令梯度为零，得到岭回归的正规方程：\n$$\n(X^{\\top}X + \\alpha I) \\beta = X^{\\top}y\n$$\n其中 $I$ 是 $p \\times p$ 的单位矩阵。矩阵 $X^{\\top}X$ 是半正定的。由于 $\\alpha  0$，矩阵 $(X^{\\top}X + \\alpha I)$ 是正定的，因此是可逆的。求解 $\\beta$ 得到岭估计量的闭式解：\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}y\n$$\n\n**任务2：用 $\\hat{\\beta}_{\\text{OLS}}$ 表示 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$**\n\n普通最小二乘（OLS）估计量由 $\\hat{\\beta}_{\\text{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y$ 给出。根据这个定义，我们可以写出 $X^{\\top} y = (X^{\\top} X) \\hat{\\beta}_{\\text{OLS}}$。将此式代入 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 的表达式中：\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = (X^{\\top}X + \\alpha I)^{-1} (X^{\\top}X) \\hat{\\beta}_{\\text{OLS}}\n$$\n现在我们使用格拉姆矩阵的特征分解 $X^{\\top}X = Q \\Lambda Q^{\\top}$，其中 $Q$ 是一个正交矩阵（$Q^{\\top}Q = I$），$\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_p)$。\n$(X^{\\top}X + \\alpha I)$ 项可以重写为：\n$$\nX^{\\top}X + \\alpha I = Q \\Lambda Q^{\\top} + \\alpha Q I Q^{\\top} = Q(\\Lambda + \\alpha I)Q^{\\top}\n$$\n其逆矩阵为：\n$$\n(X^{\\top}X + \\alpha I)^{-1} = (Q(\\Lambda + \\alpha I)Q^{\\top})^{-1} = Q(\\Lambda + \\alpha I)^{-1}Q^{\\top}\n$$\n将这些代回 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 的表达式中：\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = \\left( Q(\\Lambda + \\alpha I)^{-1}Q^{\\top} \\right) \\left( Q \\Lambda Q^{\\top} \\right) \\hat{\\beta}_{\\text{OLS}}\n$$\n由于 $Q^{\\top}Q = I$，上式可简化为：\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = Q (\\Lambda + \\alpha I)^{-1} \\Lambda Q^{\\top} \\hat{\\beta}_{\\text{OLS}}\n$$\n矩阵 $(\\Lambda + \\alpha I)^{-1}$ 和 $\\Lambda$ 都是对角矩阵。它们的乘积是一个对角矩阵 $S = (\\Lambda + \\alpha I)^{-1} \\Lambda$，其对角元素为 $s_{jj} = \\frac{\\lambda_j}{\\lambda_j + \\alpha}$。所以，$\\hat{\\beta}_{\\text{ridge}}(\\alpha) = QSQ^{\\top}\\hat{\\beta}_{\\text{OLS}}$。\n为了观察在特征基 $\\{q_j\\}$ 中的效果，我们将估计量投影到这个基上。令 $\\hat{b}_{\\text{OLS}} = Q^{\\top}\\hat{\\beta}_{\\text{OLS}}$ 和 $\\hat{b}_{\\text{ridge}} = Q^{\\top}\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 为在特征基中的系数向量。根据上述关系，我们有：\n$$\nQ^{\\top}\\hat{\\beta}_{\\text{ridge}}(\\alpha) = S Q^{\\top}\\hat{\\beta}_{\\text{OLS}} \\implies \\hat{b}_{\\text{ridge}} = S \\hat{b}_{\\text{OLS}}\n$$\n以分量形式表示，对于每个特征向量 $q_j$，岭估计的投影是 OLS 估计投影的一个缩放版本：\n$$\nq_j^{\\top} \\hat{\\beta}_{\\text{ridge}}(\\alpha) = \\frac{\\lambda_j}{\\lambda_j + \\alpha} (q_j^{\\top} \\hat{\\beta}_{\\text{OLS}})\n$$\n$\\frac{\\lambda_j}{\\lambda_j + \\alpha}$ 项是施加于 OLS 解的第 $j$ 个主成分上的收缩因子。由于 $\\lambda_j  0$ 和 $\\alpha  0$，该因子总是在区间 $(0, 1)$ 内。\n\n**任务3：计算期望偏差向量**\n\n估计量 $\\hat{\\beta}$ 的偏差定义为 $\\mathbb{E}[\\hat{\\beta}] - \\beta^{\\star}$。首先，我们计算 $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ 的期望。使用数据生成模型 $y = X \\beta^{\\star} + \\varepsilon$：\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = (X^{\\top}X + \\alpha I)^{-1} X^{\\top} (X \\beta^{\\star} + \\varepsilon) = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star} + (X^{\\top}X + \\alpha I)^{-1} X^{\\top} \\varepsilon\n$$\n取期望，并注意到 $X$、$\\beta^{\\star}$ 和 $\\alpha$ 是非随机的，且 $\\mathbb{E}[\\varepsilon] = 0$：\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star} + (X^{\\top}X + \\alpha I)^{-1} X^{\\top} \\mathbb{E}[\\varepsilon]\n$$\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star}\n$$\n因此，偏差向量为：\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star} - \\beta^{\\star}\n$$\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = \\left[ (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X - I \\right] \\beta^{\\star}\n$$\n为了简化方括号中的矩阵表达式，令 $A = X^{\\top}X$。我们有：\n$$\n(A + \\alpha I)^{-1} A - I = (A + \\alpha I)^{-1} A - (A + \\alpha I)^{-1}(A + \\alpha I) = (A + \\alpha I)^{-1} [A - (A + \\alpha I)] = -\\alpha (A + \\alpha I)^{-1}\n$$\n所以偏差向量是：\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha (X^{\\top}X + \\alpha I)^{-1} \\beta^{\\star}\n$$\n使用特征分解 $(X^{\\top}X + \\alpha I)^{-1} = Q(\\Lambda + \\alpha I)^{-1}Q^{\\top}$：\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha Q(\\Lambda + \\alpha I)^{-1}Q^{\\top} \\beta^{\\star}\n$$\n为了在特征基 $\\{q_j\\}_{j=1}^p$ 中表示它，我们通过将其投影到每个 $q_j$ 上来找到偏差向量的分量：\n$$\nq_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = q_j^{\\top} \\left( -\\alpha Q(\\Lambda + \\alpha I)^{-1}Q^{\\top} \\beta^{\\star} \\right)\n$$\n由于 $q_j^{\\top}Q$ 是 $Q^{\\top}Q = I$ 的第 $j$ 行，所以它是标准基向量 $e_j^{\\top}$。\n$$\nq_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha e_j^{\\top} (\\Lambda + \\alpha I)^{-1} Q^{\\top} \\beta^{\\star}\n$$\n项 $Q^{\\top}\\beta^{\\star}$ 是一个向量，其分量为 $b_k^{\\star} = q_k^{\\top}\\beta^{\\star}$。项 $e_j^{\\top} (\\Lambda + \\alpha I)^{-1}$ 选取对角矩阵 $(\\Lambda + \\alpha I)^{-1}$ 的第 $j$ 个对角元素，即 $\\frac{1}{\\lambda_j+\\alpha}$，并将其乘以 $e_j^\\top$。所以表达式变为：\n$$\nq_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha \\frac{1}{\\lambda_j + \\alpha} (q_j^{\\top} \\beta^{\\star}) = -\\frac{\\alpha b_j^{\\star}}{\\lambda_j + \\alpha}\n$$\n因此，在特征基中表示的偏差向量为：\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = \\sum_{j=1}^{p} \\left( -\\frac{\\alpha b_j^{\\star}}{\\lambda_j + \\alpha} \\right) q_j\n$$\n\n**任务4：计算偏差向量的欧几里得范数的平方**\n\n最后的任务是计算 $\\| \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} \\|_{2}^{2} = \\| \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\|_{2}^{2}$。\n由于特征向量 $\\{q_j\\}_{j=1}^p$ 构成了 $\\mathbb{R}^p$ 的一个标准正交基，一个向量的欧几里得范数的平方是其在该基中各分量平方的和。使用任务3中推导出的分量：\n$$\n\\| \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\|_{2}^{2} = \\sum_{j=1}^{p} \\left( q_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\right)^2\n$$\n$$\n\\| \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\|_{2}^{2} = \\sum_{j=1}^{p} \\left( -\\frac{\\alpha b_j^{\\star}}{\\lambda_j + \\alpha} \\right)^2\n$$\n简化此表达式得到最终的闭式结果：\n$$\n\\| \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} \\|_{2}^{2} = \\sum_{j=1}^{p} \\frac{\\alpha^2 (b_{j}^{\\star})^2}{(\\lambda_{j} + \\alpha)^2}\n$$\n这个量就是岭回归估计量的偏差平方。",
            "answer": "$$\n\\boxed{\\sum_{j=1}^{p} \\frac{\\alpha^2 (b_{j}^{\\star})^2}{(\\lambda_{j} + \\alpha)^2}}\n$$"
        },
        {
            "introduction": "与岭回归不同，LASSO（最小绝对收缩和选择算子）通过其 $\\ell_{1}$ 范数惩罚项能够实现变量选择，从而产生稀疏解。本练习将带您深入 LASSO 的优化理论核心，通过推导其对偶问题来揭示稀疏性产生的条件 。掌握对偶理论不仅能帮助您理解 LASSO 解的结构，也是高级算法设计与分析的基础。",
            "id": "3345330",
            "problem": "考虑一个计算系统生物学背景，其中单个目标基因的表达被建模为两个白化转录因子活性特征的线性组合。设 $X \\in \\mathbb{R}^{n \\times p}$ 表示特征的设计矩阵，$y \\in \\mathbb{R}^{n}$ 表示测得的基因表达。在这种高维推断背景下，一种标准方法是使用最小绝对收缩和选择算子 (LASSO)，它解决以下优化问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中 $\\lambda  0$ 控制转录因子系数向量 $ \\beta $ 的稀疏性。此公式的对偶问题可以通过引入一个显式残差变量并应用凸分析中的 Fenchel-Legendre 共轭和拉格朗日对偶性来获得。从凸优化的基本原理（拉格朗日构造，通过对原始变量求下确界来形成对偶函数，以及从凸共轭中出现的约束）出发，推导 LASSO 的对偶问题，并证明它对对偶变量 $ u \\in \\mathbb{R}^{n} $ 施加了约束 $ \\|X^{\\top} u\\|_{\\infty} \\le \\lambda $。\n\n然后，具体到一个生物学上合理的白化设计，其中特征在样本空间中是标准正交的，即 $ X = I_{2} $（$ 2 \\times 2 $ 单位矩阵），并设 $ y = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} $ 和 $ \\lambda = 1 $。利用 Karush-Kuhn-Tucker 条件通过残差和次梯度关系将原始解和对偶解联系起来，计算原始最优化子 $ \\beta^{\\star} $、对偶最优化子 $ u^{\\star} $，并使用原始-对偶间隙来证明此实例的最优性。报告原始-对偶间隙 $\\Delta := f(\\beta^{\\star}) - g(u^{\\star})$ 的数值，其中 $ f $ 是原始目标函数，$ g $ 是对偶目标函数。无需四舍五入，也不涉及物理单位。最终报告的量必须是一个实数。",
            "solution": "用户希望我解决一个与 LASSO 优化相关的问题。\n\n该问题需要解决两个主要部分：首先，推导 LASSO 公式的对偶问题，并证明它导致对对偶变量的特定约束；其次，解决一个 LASSO 问题的具体实例，找到最优的原始解和对偶解，并计算原始-对偶间隙。\n\n### 第一部分：LASSO 对偶问题的推导\n\n原始 LASSO 问题表述为：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} f(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right)\n$$\n为了推导对偶问题，我们首先通过引入一个显式残差变量 $r \\in \\mathbb{R}^{n}$ 来重新表述原始问题。这使我们能够将二次项与包含 $X$ 的项分开。问题变成一个等价的约束优化问题：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}, r \\in \\mathbb{R}^{n}} \\frac{1}{2}\\|r\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad X\\beta + r = y\n$$\n我们为等式约束 $X\\beta + r - y = 0$ 引入一个拉格朗日乘子向量 $u \\in \\mathbb{R}^{n}$。拉格朗日函数 $L$ 是原始变量 $\\beta, r$ 和对偶变量 $u$ 的函数：\n$$\nL(\\beta, r, u) = \\frac{1}{2}\\|r\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} + u^{\\top}(X\\beta + r - y)\n$$\n拉格朗日对偶函数 $g(u)$ 定义为拉格朗日函数关于原始变量的下确界：\n$$\ng(u) = \\inf_{\\beta \\in \\mathbb{R}^{p}, r \\in \\mathbb{R}^{n}} L(\\beta, r, u)\n$$\n我们可以重新排列拉格朗日函数中的项，以对变量进行分组：\n$$\ng(u) = -u^{\\top}y + \\inf_{r \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2}\\|r\\|_{2}^{2} + u^{\\top}r \\right) + \\inf_{\\beta \\in \\mathbb{R}^{p}} \\left( \\lambda \\|\\beta\\|_{1} + u^{\\top}X\\beta \\right)\n$$\n这两个下确界可以分开计算。它们对应于函数 $\\frac{1}{2}\\|r\\|_{2}^{2}$ 和 $\\lambda\\|\\beta\\|_{1}$ 的凸共轭的负数。\n\n对于包含 $r$ 的项，函数 $h_1(r) = \\frac{1}{2}r^{\\top}r + u^{\\top}r$ 是一个凸二次函数。其最小值可以通过将其关于 $r$ 的梯度设为零来找到：\n$$\n\\nabla_r h_1(r) = r + u = 0 \\implies r = -u\n$$\n将此代回，最小值为：\n$$\n\\inf_{r} \\left( \\frac{1}{2}\\|r\\|_{2}^{2} + u^{\\top}r \\right) = \\frac{1}{2}\\|-u\\|_{2}^{2} + u^{\\top}(-u) = \\frac{1}{2}\\|u\\|_{2}^{2} - \\|u\\|_{2}^{2} = -\\frac{1}{2}\\|u\\|_{2}^{2}\n$$\n对于包含 $\\beta$ 的项，设 $z = X^{\\top}u$。表达式为 $\\inf_{\\beta} (\\lambda \\|\\beta\\|_{1} + z^{\\top}\\beta)$。这可以按分量进行分析：\n$$\n\\inf_{\\beta} \\left( \\lambda \\sum_{j=1}^{p} |\\beta_j| + \\sum_{j=1}^{p} z_j \\beta_j \\right) = \\sum_{j=1}^{p} \\inf_{\\beta_j} (\\lambda|\\beta_j| + z_j\\beta_j)\n$$\n对于每个分量 $j$，如果 $|z_j|  \\lambda$，那么可以选择与 $z_j$ 符号相反的 $\\beta_j$，并让其大小趋于无穷大，这使得表达式 $\\lambda|\\beta_j| + z_j\\beta_j$ 趋于 $-\\infty$。如果 $|z_j| \\le \\lambda$，那么 $\\lambda|\\beta_j| + z_j\\beta_j \\ge \\lambda|\\beta_j| - |z_j||\\beta_j| = (\\lambda-|z_j|)|\\beta_j| \\ge 0$。最小值是 $0$，在 $\\beta_j=0$ 时达到。因此，要使对所有 $\\beta$ 的下确界是有限的（并等于 $0$），我们需要对所有 $j$ 都有 $|(X^{\\top}u)_j| \\le \\lambda$。这正是约束 $\\|X^{\\top}u\\|_{\\infty} \\le \\lambda$。\n\n综合这些结果，对偶函数为：\n$$\ng(u) = \\begin{cases} -\\frac{1}{2}\\|u\\|_{2}^{2} - y^{\\top}u  \\text{if } \\|X^{\\top}u\\|_{\\infty} \\le \\lambda \\\\ -\\infty  \\text{otherwise} \\end{cases}\n$$\n对偶问题是最大化 $g(u)$。这等价于：\n$$\n\\max_{u \\in \\mathbb{R}^{n}} \\; \\left( -\\frac{1}{2}\\|u\\|_{2}^{2} - y^{\\top}u \\right) \\quad \\text{subject to} \\quad \\|X^{\\top}u\\|_{\\infty} \\le \\lambda\n$$\n这完成了推导，并表明对偶问题对对偶变量 $u$ 施加了所要求的约束。\n\n### 第二部分：具体实例的求解\n\n给定具体实例，$X = I_2$（$2 \\times 2$ 单位矩阵），这意味着 $n=p=2$。数据为 $y = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix}$ 和 $\\lambda = 1$。\n\n**原始解**\n当 $X=I_2$ 时，原始目标函数为：\n$$\nf(\\beta) = \\frac{1}{2} \\|y - \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\sum_{j=1}^{2}(y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{2}|\\beta_j|\n$$\n该问题是可分的，意味着我们可以通过独立地最小化每个分量的目标函数来找到最优的 $\\beta_j^{\\star}$：$\\min_{\\beta_j} \\frac{1}{2}(y_j - \\beta_j)^2 + \\lambda|\\beta_j|$。这个一维问题的解是软阈值算子：\n$$\n\\beta_j^{\\star} = S_{\\lambda}(y_j) = \\text{sign}(y_j)\\max(|y_j|-\\lambda, 0)\n$$\n对于 $j=1$：$y_1=3, \\lambda=1 \\implies \\beta_1^{\\star} = \\text{sign}(3)\\max(3-1, 0) = 1 \\cdot 2 = 2$。\n对于 $j=2$：$y_2=-0.5, \\lambda=1 \\implies \\beta_2^{\\star} = \\text{sign}(-0.5)\\max(0.5-1, 0) = -1 \\cdot 0 = 0$。\n最优原始解为 $\\beta^{\\star} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$。\n\n**对偶解**\n根据 Karush-Kuhn-Tucker (KKT) 条件，强对偶性成立，最优原始变量和对偶变量是相互关联的。具体来说，拉格朗日函数关于 $r$ 的平稳性条件是 $\\nabla_r L = r + u = 0$，这意味着 $r^{\\star} = -u^{\\star}$。原始可行性约束 $X\\beta+r=y$ 意味着 $r^{\\star} = y - X\\beta^{\\star}$。结合这两者，我们找到最优对偶变量：\n$$\nu^{\\star} = -r^{\\star} = -(y - X\\beta^{\\star}) = X\\beta^{\\star} - y\n$$\n代入此实例的具体值（$X=I_2$）：\n$$\nu^{\\star} = I_2 \\beta^{\\star} - y = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix}\n$$\n我们可以验证这个 $u^{\\star}$ 是对偶可行的：$\\|X^{\\top}u^{\\star}\\|_{\\infty} = \\|I_2^{\\top}u^{\\star}\\|_{\\infty} = \\|u^{\\star}\\|_{\\infty} = \\max(|-1|, |0.5|) = 1$。由于 $\\lambda=1$，约束 $\\|X^{\\top}u^{\\star}\\|_{\\infty} \\le \\lambda$ 得到满足（$1 \\le 1$）。\n\n**原始-对偶间隙计算**\n最后一步是计算原始-对偶间隙 $\\Delta = f(\\beta^{\\star}) - g(u^{\\star})$，它作为最优性的证明。\n\n在 $\\beta^{\\star}$ 处的原始目标函数值：\n残差为 $r^{\\star} = y - X\\beta^{\\star} = y - \\beta^{\\star} = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -0.5 \\end{pmatrix}$。\n解的 L1 范数为 $\\|\\beta^{\\star}\\|_{1} = |2| + |0| = 2$。\n$$\nf(\\beta^{\\star}) = \\frac{1}{2}\\|r^{\\star}\\|_{2}^{2} + \\lambda\\|\\beta^{\\star}\\|_{1} = \\frac{1}{2}(1^2 + (-0.5)^2) + 1 \\cdot (2) = \\frac{1}{2}(1.25) + 2 = 0.625 + 2 = 2.625\n$$\n\n在 $u^{\\star}$ 处的对偶目标函数值：\n$$\ng(u^{\\star}) = -\\frac{1}{2}\\|u^{\\star}\\|_{2}^{2} - y^{\\top}u^{\\star}\n$$\n对偶解的 L2 范数平方为 $\\|u^{\\star}\\|_{2}^{2} = (-1)^2 + (0.5)^2 = 1 + 0.25 = 1.25$。\n内积为 $y^{\\top}u^{\\star} = \\begin{pmatrix} 3  -0.5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix} = (3)(-1) + (-0.5)(0.5) = -3 - 0.25 = -3.25$。\n$$\ng(u^{\\star}) = -\\frac{1}{2}(1.25) - (-3.25) = -0.625 + 3.25 = 2.625\n$$\n\n最后，原始-对偶间隙为：\n$$\n\\Delta = f(\\beta^{\\star}) - g(u^{\\star}) = 2.625 - 2.625 = 0\n$$\n零间隙证实了所计算的原始解和对偶解是最优的。",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}