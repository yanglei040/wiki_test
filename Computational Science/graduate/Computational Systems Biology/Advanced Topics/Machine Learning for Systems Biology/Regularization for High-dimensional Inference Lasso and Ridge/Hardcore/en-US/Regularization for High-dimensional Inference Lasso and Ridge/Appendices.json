{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply ridge regression, we must first understand its inner workings. This exercise guides you through a foundational derivation that reveals how ridge regression modifies the standard least-squares solution. By working through the normal equations and using an eigendecomposition of the data's covariance structure, you will see precisely how ridge regression introduces bias by shrinking coefficient estimates along specific directions, providing a clear illustration of the bias-variance trade-off at the heart of regularization. ",
            "id": "3345309",
            "problem": "In a systems-level study of transcriptional regulation, consider a linear model that predicts the expression of a target gene across $n$ experimental conditions from $p$ transcription factor activity features. Let $X \\in \\mathbb{R}^{n \\times p}$ denote the design matrix with full column rank and centered, standardized columns, and let $y \\in \\mathbb{R}^{n}$ denote the response vector. Assume the data were generated by the linear model $y = X \\beta^{\\star} + \\varepsilon$, where $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true coefficient vector and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is mean-zero Gaussian noise with variance $\\sigma^{2}  0$. Consider the ridge regression estimator with penalty parameter $\\alpha  0$, defined as the minimizer\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\frac{\\alpha}{2} \\| \\beta \\|_{2}^{2} \\right\\}.\n$$\nLet the Gram matrix admit the eigendecomposition $X^{\\top} X = Q \\Lambda Q^{\\top}$, where $Q \\in \\mathbb{R}^{p \\times p}$ is orthogonal, and $\\Lambda = \\operatorname{diag}(\\lambda_{1}, \\ldots, \\lambda_{p})$ with $\\lambda_{j}  0$ for all $j$. Let $\\hat{\\beta}_{\\text{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y$ denote the Ordinary Least Squares (OLS) estimator.\n\nTasks:\n1. Starting from the definition of $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$, derive the normal equations and solve for $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ in closed form.\n2. Using the eigendecomposition of $X^{\\top} X$, express $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ as a linear transformation of $\\hat{\\beta}_{\\text{OLS}}$ in the eigenbasis of $X^{\\top} X$, and identify the shrinkage factor applied to the OLS coefficient along each eigenvector $q_{j}$.\n3. Using the data-generating model $y = X \\beta^{\\star} + \\varepsilon$, compute the expected bias vector $\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star}$ and express it in the eigenbasis $\\{q_{j}\\}_{j=1}^{p}$ in terms of the projections $b_{j}^{\\star} = q_{j}^{\\top} \\beta^{\\star}$ and the eigenvalues $\\{\\lambda_{j}\\}$.\n4. Finally, compute the squared Euclidean norm of this bias vector, $\\| \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} \\|_{2}^{2}$, as a closed-form analytic expression in terms of $\\{ \\lambda_{j} \\}$, $\\alpha$, and $\\{ b_{j}^{\\star} \\}$. Provide this final expression as your answer.\n\nYour final answer must be a single closed-form analytic expression. Do not include units. Do not provide an inequality or equation. Do not round; no numerical approximation is required.",
            "solution": "The problem is subjected to validation and is deemed valid as it is scientifically grounded, well-posed, objective, and internally consistent. We may proceed with a reasoned solution.\n\nThe problem asks for a derivation of the squared bias of the ridge regression estimator. We will proceed through the four specified tasks.\n\n**Task 1: Derivation of the Normal Equations and Closed-Form Solution for $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$**\n\nThe ridge regression estimator $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ is defined as the minimizer of the objective function $J(\\beta)$:\n$$\nJ(\\beta) = \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\frac{\\alpha}{2} \\| \\beta \\|_{2}^{2}\n$$\nwhere $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector, and $\\alpha  0$ is the regularization parameter.\n\nTo find the minimizer, we first expand the objective function. The squared Euclidean norms can be written as vector inner products:\n$$\nJ(\\beta) = \\frac{1}{2} (y - X \\beta)^{\\top}(y - X \\beta) + \\frac{\\alpha}{2} \\beta^{\\top}\\beta\n$$\nExpanding the first term gives:\n$$\nJ(\\beta) = \\frac{1}{2} (y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta) + \\frac{\\alpha}{2} \\beta^{\\top}\\beta\n$$\nSince the term $y^{\\top}X\\beta$ is a scalar, it is equal to its transpose $\\beta^{\\top}X^{\\top}y$. Thus, we can combine these terms:\n$$\nJ(\\beta) = \\frac{1}{2} y^{\\top}y - y^{\\top}X\\beta + \\frac{1}{2}\\beta^{\\top}X^{\\top}X\\beta + \\frac{\\alpha}{2} \\beta^{\\top}\\beta\n$$\nThe objective function $J(\\beta)$ is convex, and since $\\alpha  0$, it is strictly convex. Therefore, a unique minimum exists where the gradient with respect to $\\beta$ is zero. We compute the gradient $\\nabla_{\\beta} J(\\beta)$:\n$$\n\\nabla_{\\beta} J(\\beta) = \\nabla_{\\beta} \\left( \\frac{1}{2} y^{\\top}y - y^{\\top}X\\beta + \\frac{1}{2}\\beta^{\\top}X^{\\top}X\\beta + \\frac{\\alpha}{2} \\beta^{\\top}\\beta \\right)\n$$\n$$\n\\nabla_{\\beta} J(\\beta) = 0 - X^{\\top}y + X^{\\top}X\\beta + \\alpha\\beta\n$$\nSetting the gradient to zero yields the normal equations for ridge regression:\n$$\n(X^{\\top}X + \\alpha I) \\beta = X^{\\top}y\n$$\nwhere $I$ is the $p \\times p$ identity matrix. The matrix $X^{\\top}X$ is positive semi-definite. Since $\\alpha  0$, the matrix $(X^{\\top}X + \\alpha I)$ is positive definite and therefore invertible. Solving for $\\beta$ gives the closed-form solution for the ridge estimator:\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}y\n$$\n\n**Task 2: Expression of $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ in terms of $\\hat{\\beta}_{\\text{OLS}}$**\n\nThe Ordinary Least Squares (OLS) estimator is given by $\\hat{\\beta}_{\\text{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y$. From this definition, we can write $X^{\\top} y = (X^{\\top} X) \\hat{\\beta}_{\\text{OLS}}$. Substituting this into the expression for $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$:\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = (X^{\\top}X + \\alpha I)^{-1} (X^{\\top}X) \\hat{\\beta}_{\\text{OLS}}\n$$\nWe now use the eigendecomposition of the Gram matrix, $X^{\\top}X = Q \\Lambda Q^{\\top}$, where $Q$ is an orthogonal matrix ($Q^{\\top}Q = I$) and $\\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_p)$.\nThe term $(X^{\\top}X + \\alpha I)$ can be rewritten as:\n$$\nX^{\\top}X + \\alpha I = Q \\Lambda Q^{\\top} + \\alpha Q I Q^{\\top} = Q(\\Lambda + \\alpha I)Q^{\\top}\n$$\nIts inverse is:\n$$\n(X^{\\top}X + \\alpha I)^{-1} = (Q(\\Lambda + \\alpha I)Q^{\\top})^{-1} = Q(\\Lambda + \\alpha I)^{-1}Q^{\\top}\n$$\nSubstituting these back into the expression for $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$:\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = \\left( Q(\\Lambda + \\alpha I)^{-1}Q^{\\top} \\right) \\left( Q \\Lambda Q^{\\top} \\right) \\hat{\\beta}_{\\text{OLS}}\n$$\nSince $Q^{\\top}Q = I$, this simplifies to:\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = Q (\\Lambda + \\alpha I)^{-1} \\Lambda Q^{\\top} \\hat{\\beta}_{\\text{OLS}}\n$$\nThe matrices $(\\Lambda + \\alpha I)^{-1}$ and $\\Lambda$ are diagonal. Their product is a diagonal matrix $S = (\\Lambda + \\alpha I)^{-1} \\Lambda$ with diagonal entries $s_{jj} = \\frac{\\lambda_j}{\\lambda_j + \\alpha}$. So, $\\hat{\\beta}_{\\text{ridge}}(\\alpha) = QSQ^{\\top}\\hat{\\beta}_{\\text{OLS}}$.\nTo see the effect in the eigenbasis $\\{q_j\\}$, we project the estimators onto this basis. Let $\\hat{b}_{\\text{OLS}} = Q^{\\top}\\hat{\\beta}_{\\text{OLS}}$ and $\\hat{b}_{\\text{ridge}} = Q^{\\top}\\hat{\\beta}_{\\text{ridge}}(\\alpha)$ be the coefficient vectors in the eigenbasis. From the above relation, we have:\n$$\nQ^{\\top}\\hat{\\beta}_{\\text{ridge}}(\\alpha) = S Q^{\\top}\\hat{\\beta}_{\\text{OLS}} \\implies \\hat{b}_{\\text{ridge}} = S \\hat{b}_{\\text{OLS}}\n$$\nIn component form, for each eigenvector $q_j$, the projection of the ridge estimate is a scaled version of the projection of the OLS estimate:\n$$\nq_j^{\\top} \\hat{\\beta}_{\\text{ridge}}(\\alpha) = \\frac{\\lambda_j}{\\lambda_j + \\alpha} (q_j^{\\top} \\hat{\\beta}_{\\text{OLS}})\n$$\nThe term $\\frac{\\lambda_j}{\\lambda_j + \\alpha}$ is the shrinkage factor applied to the $j$-th principal component of the OLS solution. Since $\\lambda_j  0$ and $\\alpha  0$, this factor is always in the interval $(0, 1)$.\n\n**Task 3: Computation of the Expected Bias Vector**\n\nThe bias of an estimator $\\hat{\\beta}$ is defined as $\\mathbb{E}[\\hat{\\beta}] - \\beta^{\\star}$. First, we compute the expectation of $\\hat{\\beta}_{\\text{ridge}}(\\alpha)$. Using the data-generating model $y = X \\beta^{\\star} + \\varepsilon$:\n$$\n\\hat{\\beta}_{\\text{ridge}}(\\alpha) = (X^{\\top}X + \\alpha I)^{-1} X^{\\top} (X \\beta^{\\star} + \\varepsilon) = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star} + (X^{\\top}X + \\alpha I)^{-1} X^{\\top} \\varepsilon\n$$\nTaking the expectation, and noting that $X$, $\\beta^{\\star}$, and $\\alpha$ are non-random and $\\mathbb{E}[\\varepsilon] = 0$:\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star} + (X^{\\top}X + \\alpha I)^{-1} X^{\\top} \\mathbb{E}[\\varepsilon]\n$$\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star}\n$$\nThe bias vector is therefore:\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} = (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X \\beta^{\\star} - \\beta^{\\star}\n$$\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = \\left[ (X^{\\top}X + \\alpha I)^{-1} X^{\\top}X - I \\right] \\beta^{\\star}\n$$\nTo simplify the matrix expression in the brackets, let $A = X^{\\top}X$. We have:\n$$\n(A + \\alpha I)^{-1} A - I = (A + \\alpha I)^{-1} A - (A + \\alpha I)^{-1}(A + \\alpha I) = (A + \\alpha I)^{-1} [A - (A + \\alpha I)] = -\\alpha (A + \\alpha I)^{-1}\n$$\nSo the bias vector is:\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha (X^{\\top}X + \\alpha I)^{-1} \\beta^{\\star}\n$$\nUsing the eigendecomposition $(X^{\\top}X + \\alpha I)^{-1} = Q(\\Lambda + \\alpha I)^{-1}Q^{\\top}$:\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha Q(\\Lambda + \\alpha I)^{-1}Q^{\\top} \\beta^{\\star}\n$$\nTo express this in the eigenbasis $\\{q_j\\}_{j=1}^p$, we find the components of the bias vector by projecting it onto each $q_j$:\n$$\nq_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = q_j^{\\top} \\left( -\\alpha Q(\\Lambda + \\alpha I)^{-1}Q^{\\top} \\beta^{\\star} \\right)\n$$\nSince $q_j^{\\top}Q$ is the $j$-th row of $Q^{\\top}Q = I$, it is the standard basis vector $e_j^{\\top}$.\n$$\nq_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha e_j^{\\top} (\\Lambda + \\alpha I)^{-1} Q^{\\top} \\beta^{\\star}\n$$\nThe term $Q^{\\top}\\beta^{\\star}$ is a vector with components $b_k^{\\star} = q_k^{\\top}\\beta^{\\star}$. The term $e_j^{\\top} (\\Lambda + \\alpha I)^{-1}$ selects the $j$-th diagonal entry of the diagonal matrix $(\\Lambda + \\alpha I)^{-1}$, which is $\\frac{1}{\\lambda_j+\\alpha}$, and multiplies it by $e_j^\\top$. So the expression becomes:\n$$\nq_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = -\\alpha \\frac{1}{\\lambda_j + \\alpha} (q_j^{\\top} \\beta^{\\star}) = -\\frac{\\alpha b_j^{\\star}}{\\lambda_j + \\alpha}\n$$\nThus, the bias vector expressed in the eigenbasis is:\n$$\n\\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) = \\sum_{j=1}^{p} \\left( -\\frac{\\alpha b_j^{\\star}}{\\lambda_j + \\alpha} \\right) q_j\n$$\n\n**Task 4: Computation of the Squared Euclidean Norm of the Bias Vector**\n\nThe final task is to compute $\\| \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} \\|_{2}^{2} = \\| \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\|_{2}^{2}$.\nSince the eigenvectors $\\{q_j\\}_{j=1}^p$ form an orthonormal basis for $\\mathbb{R}^p$, the squared Euclidean norm of a vector is the sum of the squares of its components in that basis. Using the components derived in Task 3:\n$$\n\\| \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\|_{2}^{2} = \\sum_{j=1}^{p} \\left( q_j^{\\top} \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\right)^2\n$$\n$$\n\\| \\text{Bias}(\\hat{\\beta}_{\\text{ridge}}(\\alpha)) \\|_{2}^{2} = \\sum_{j=1}^{p} \\left( -\\frac{\\alpha b_j^{\\star}}{\\lambda_j + \\alpha} \\right)^2\n$$\nSimplifying this expression gives the final closed-form result:\n$$\n\\| \\mathbb{E}[\\hat{\\beta}_{\\text{ridge}}(\\alpha)] - \\beta^{\\star} \\|_{2}^{2} = \\sum_{j=1}^{p} \\frac{\\alpha^2 (b_{j}^{\\star})^2}{(\\lambda_{j} + \\alpha)^2}\n$$\nThis quantity is the squared bias of the ridge regression estimator.",
            "answer": "$$\n\\boxed{\\sum_{j=1}^{p} \\frac{\\alpha^2 (b_{j}^{\\star})^2}{(\\lambda_{j} + \\alpha)^2}}\n$$"
        },
        {
            "introduction": "The LASSO's ability to perform variable selection stems directly from the geometry of its $\\ell_{1}$ penalty. To fully grasp this, we move beyond simple mechanics and into the realm of convex optimization. This practice challenges you to derive the dual problem of the LASSO, a powerful theoretical tool that uncovers fundamental constraints on the solution and is key to developing efficient algorithms. By solving a simple case and verifying the primal-dual gap, you will gain a deeper appreciation for the mathematical structure that enables sparse inference. ",
            "id": "3345330",
            "problem": "Consider a computational systems biology setting where the expression of a single target gene is modeled as a linear combination of two whitened transcription factor activity features. Let $X \\in \\mathbb{R}^{n \\times p}$ denote the design matrix of features and $y \\in \\mathbb{R}^{n}$ denote the measured gene expression. A standard approach to high-dimensional inference in such settings uses the Least Absolute Shrinkage and Selection Operator (LASSO), which solves the optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwhere $\\lambda  0$ controls the sparsity of the transcription factor coefficient vector $ \\beta $. The dual problem of this formulation can be obtained by introducing an explicit residual variable and applying Fenchel-Legendre conjugacy and Lagrangian duality from convex analysis. Starting from the basic principles of convex optimization (Lagrangian construction, infimum over primal variables to form the dual function, and constraint emergence from convex conjugates), derive the dual problem for the LASSO and show that it imposes the constraint $ \\|X^{\\top} u\\|_{\\infty} \\le \\lambda $ on the dual variable $ u \\in \\mathbb{R}^{n} $.\n\nThen, specialize to a biologically plausible whitened design in which features are orthonormal in sample space, namely $ X = I_{2} $ (the $ 2 \\times 2 $ identity matrix), with $ y = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} $ and $ \\lambda = 1 $. Using the Karush-Kuhn-Tucker conditions to link the primal and dual solutions via the residual and subgradient relations, compute the primal optimizer $ \\beta^{\\star} $, the dual optimizer $ u^{\\star} $, and use the primal-dual gap to certify optimality for this instance. Report the numerical value of the primal-dual gap $\\Delta := f(\\beta^{\\star}) - g(u^{\\star})$, where $ f $ is the primal objective and $ g $ is the dual objective. No rounding is required, and no physical units are involved. The final reported quantity must be a single real number.",
            "solution": "The user wants me to solve a problem related to LASSO optimization.\n\nThe problem requires two main parts: first, to derive the dual problem of the LASSO formulation and show that it results in a specific constraint on the dual variable; second, to solve a specific instance of the LASSO problem, find the optimal primal and dual solutions, and compute the primal-dual gap.\n\n### Part 1: Derivation of the LASSO Dual Problem\n\nThe primal LASSO problem is stated as:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} f(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right)\n$$\nTo derive the dual problem, we first reformulate the primal problem by introducing an explicit residual variable $r \\in \\mathbb{R}^{n}$. This allows us to separate the quadratic term from the term involving $X$. The problem becomes an equivalent constrained optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}, r \\in \\mathbb{R}^{n}} \\frac{1}{2}\\|r\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad X\\beta + r = y\n$$\nWe introduce a Lagrange multiplier vector $u \\in \\mathbb{R}^{n}$ for the equality constraint $X\\beta + r - y = 0$. The Lagrangian $L$ is a function of the primal variables $\\beta, r$ and the dual variable $u$:\n$$\nL(\\beta, r, u) = \\frac{1}{2}\\|r\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} + u^{\\top}(X\\beta + r - y)\n$$\nThe Lagrange dual function $g(u)$ is defined as the infimum of the Lagrangian over the primal variables:\n$$\ng(u) = \\inf_{\\beta \\in \\mathbb{R}^{p}, r \\in \\mathbb{R}^{n}} L(\\beta, r, u)\n$$\nWe can rearrange the terms in the Lagrangian to group variables:\n$$\ng(u) = -u^{\\top}y + \\inf_{r \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2}\\|r\\|_{2}^{2} + u^{\\top}r \\right) + \\inf_{\\beta \\in \\mathbb{R}^{p}} \\left( \\lambda \\|\\beta\\|_{1} + u^{\\top}X\\beta \\right)\n$$\nThe two infima can be computed separately. These correspond to the negative of the convex conjugates of the functions $\\frac{1}{2}\\|r\\|_{2}^{2}$ and $\\lambda\\|\\beta\\|_{1}$.\n\nFor the term involving $r$, the function $h_1(r) = \\frac{1}{2}r^{\\top}r + u^{\\top}r$ is a convex quadratic. Its minimum is found by setting its gradient with respect to $r$ to zero:\n$$\n\\nabla_r h_1(r) = r + u = 0 \\implies r = -u\n$$\nSubstituting this back, the minimum value is:\n$$\n\\inf_{r} \\left( \\frac{1}{2}\\|r\\|_{2}^{2} + u^{\\top}r \\right) = \\frac{1}{2}\\|-u\\|_{2}^{2} + u^{\\top}(-u) = \\frac{1}{2}\\|u\\|_{2}^{2} - \\|u\\|_{2}^{2} = -\\frac{1}{2}\\|u\\|_{2}^{2}\n$$\nFor the term involving $\\beta$, let $z = X^{\\top}u$. The expression is $\\inf_{\\beta} (\\lambda \\|\\beta\\|_{1} + z^{\\top}\\beta)$. This can be analyzed component-wise:\n$$\n\\inf_{\\beta} \\left( \\lambda \\sum_{j=1}^{p} |\\beta_j| + \\sum_{j=1}^{p} z_j \\beta_j \\right) = \\sum_{j=1}^{p} \\inf_{\\beta_j} (\\lambda|\\beta_j| + z_j\\beta_j)\n$$\nFor each component $j$, if $|z_j|  \\lambda$, then one can choose $\\beta_j$ with a sign opposite to $z_j$ and let its magnitude go to infinity, which makes the expression $\\lambda|\\beta_j| + z_j\\beta_j$ tend to $-\\infty$. If $|z_j| \\le \\lambda$, then $\\lambda|\\beta_j| + z_j\\beta_j \\ge \\lambda|\\beta_j| - |z_j||\\beta_j| = (\\lambda-|z_j|)|\\beta_j| \\ge 0$. The minimum is $0$, achieved at $\\beta_j=0$. Thus, for the infimum over all $\\beta$ to be finite (and equal to $0$), we require $|(X^{\\top}u)_j| \\le \\lambda$ for all $j$. This is precisely the constraint $\\|X^{\\top}u\\|_{\\infty} \\le \\lambda$.\n\nCombining these results, the dual function is:\n$$\ng(u) = \\begin{cases} -\\frac{1}{2}\\|u\\|_{2}^{2} - y^{\\top}u  \\text{if } \\|X^{\\top}u\\|_{\\infty} \\le \\lambda \\\\ -\\infty  \\text{otherwise} \\end{cases}\n$$\nThe dual problem is to maximize $g(u)$. This is equivalent to:\n$$\n\\max_{u \\in \\mathbb{R}^{n}} \\; \\left( -\\frac{1}{2}\\|u\\|_{2}^{2} - y^{\\top}u \\right) \\quad \\text{subject to} \\quad \\|X^{\\top}u\\|_{\\infty} \\le \\lambda\n$$\nThis completes the derivation and shows that the dual problem imposes the required constraint on the dual variable $u$.\n\n### Part 2: Solution for the Specific Instance\n\nWe are given the specific instance with $X = I_2$ (the $2 \\times 2$ identity matrix), which implies $n=p=2$. The data are $y = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix}$ and $\\lambda = 1$.\n\n**Primal Solution**\nThe primal objective function with $X=I_2$ is:\n$$\nf(\\beta) = \\frac{1}{2} \\|y - \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\sum_{j=1}^{2}(y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{2}|\\beta_j|\n$$\nThe problem is separable, meaning we can find the optimal $\\beta_j^{\\star}$ by minimizing each component's objective function independently: $\\min_{\\beta_j} \\frac{1}{2}(y_j - \\beta_j)^2 + \\lambda|\\beta_j|$. The solution to this 1D problem is the soft-thresholding operator:\n$$\n\\beta_j^{\\star} = S_{\\lambda}(y_j) = \\text{sign}(y_j)\\max(|y_j|-\\lambda, 0)\n$$\nFor $j=1$: $y_1=3, \\lambda=1 \\implies \\beta_1^{\\star} = \\text{sign}(3)\\max(3-1, 0) = 1 \\cdot 2 = 2$.\nFor $j=2$: $y_2=-0.5, \\lambda=1 \\implies \\beta_2^{\\star} = \\text{sign}(-0.5)\\max(0.5-1, 0) = -1 \\cdot 0 = 0$.\nThe optimal primal solution is $\\beta^{\\star} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$.\n\n**Dual Solution**\nFrom the Karush-Kuhn-Tucker (KKT) conditions, strong duality holds and the optimal primal and dual variables are linked. Specifically, the stationarity condition for the Lagrangian with respect to $r$ is $\\nabla_r L = r + u = 0$, implying $r^{\\star} = -u^{\\star}$. The primal feasibility constraint $X\\beta+r=y$ means $r^{\\star} = y - X\\beta^{\\star}$. Combining these, we find the optimal dual variable:\n$$\nu^{\\star} = -r^{\\star} = -(y - X\\beta^{\\star}) = X\\beta^{\\star} - y\n$$\nSubstituting the specific values for this instance ($X=I_2$):\n$$\nu^{\\star} = I_2 \\beta^{\\star} - y = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix}\n$$\nWe can verify that this $u^{\\star}$ is dual feasible: $\\|X^{\\top}u^{\\star}\\|_{\\infty} = \\|I_2^{\\top}u^{\\star}\\|_{\\infty} = \\|u^{\\star}\\|_{\\infty} = \\max(|-1|, |0.5|) = 1$. Since $\\lambda=1$, the constraint $\\|X^{\\top}u^{\\star}\\|_{\\infty} \\le \\lambda$ is satisfied ($1 \\le 1$).\n\n**Primal-Dual Gap Calculation**\nThe final step is to compute the primal-dual gap, $\\Delta = f(\\beta^{\\star}) - g(u^{\\star})$, which serves as a certificate of optimality.\n\nValue of the primal objective at $\\beta^{\\star}$:\nThe residual is $r^{\\star} = y - X\\beta^{\\star} = y - \\beta^{\\star} = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -0.5 \\end{pmatrix}$.\nThe L1-norm of the solution is $\\|\\beta^{\\star}\\|_{1} = |2| + |0| = 2$.\n$$\nf(\\beta^{\\star}) = \\frac{1}{2}\\|r^{\\star}\\|_{2}^{2} + \\lambda\\|\\beta^{\\star}\\|_{1} = \\frac{1}{2}(1^2 + (-0.5)^2) + 1 \\cdot (2) = \\frac{1}{2}(1.25) + 2 = 0.625 + 2 = 2.625\n$$\n\nValue of the dual objective at $u^{\\star}$:\n$$\ng(u^{\\star}) = -\\frac{1}{2}\\|u^{\\star}\\|_{2}^{2} - y^{\\top}u^{\\star}\n$$\nThe squared L2-norm of the dual solution is $\\|u^{\\star}\\|_{2}^{2} = (-1)^2 + (0.5)^2 = 1 + 0.25 = 1.25$.\nThe inner product is $y^{\\top}u^{\\star} = \\begin{pmatrix} 3  -0.5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix} = (3)(-1) + (-0.5)(0.5) = -3 - 0.25 = -3.25$.\n$$\ng(u^{\\star}) = -\\frac{1}{2}(1.25) - (-3.25) = -0.625 + 3.25 = 2.625\n$$\n\nFinally, the primal-dual gap is:\n$$\n\\Delta = f(\\beta^{\\star}) - g(u^{\\star}) = 2.625 - 2.625 = 0\n$$\nA zero gap confirms that the computed primal and dual solutions are optimal.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "Theoretical understanding of regularization is essential, but its successful application hinges on a practical question: how do we choose the optimal amount of regularization? This hands-on coding exercise bridges theory and practice by tasking you with implementing two robust methods for selecting the ridge penalty parameter, $\\lambda$: K-fold cross-validation and generalized cross-validation (GCV). By applying these techniques to simulated high-dimensional data, you will not only learn a critical data analysis skill but also connect the chosen $\\lambda$ back to the data's underlying spectral properties, reinforcing the concepts from our theoretical explorations. ",
            "id": "3345318",
            "problem": "You are given a high-dimensional linear model motivated by gene expression analysis, where the design matrix has a controlled eigen-spectrum that emulates correlated expression programs. Let $n$ denote the number of samples, $p$ the number of features (genes), and let the response vector be $y \\in \\mathbb{R}^{n}$. Consider ridge regression, which estimates the coefficient vector $\\hat{\\beta}_{\\lambda}$ by minimizing the penalized empirical risk\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{2}^{2},\n$$\nfor a penalty parameter $\\lambda \\ge 0$. Let the Singular Value Decomposition (SVD) of $X$ be $X = U \\operatorname{diag}(d) V^{\\top}$ with $U \\in \\mathbb{R}^{n \\times r}$, $V \\in \\mathbb{R}^{p \\times r}$, and $r = \\min(n,p)$. Define the eigen-spectrum of $X^{\\top}X/n$ by $\\gamma_{i} = d_{i}^{2}/n$ for $i \\in \\{1,\\dots,r\\}$, and $\\gamma_{i} = 0$ for $i \\in \\{r+1,\\dots,p\\}$.\n\nYou will implement model selection for the ridge penalty using two approaches: Generalized Cross-Validation (GCV) and $K$-fold Cross-Validation (CV), and then relate the selected penalty to the eigen-spectrum $\\{\\gamma_{i}\\}$.\n\nDefinitions to be used as foundational starting points:\n- Generalized Cross-Validation (GCV): For a linear smoother with prediction $\\hat{y}_{\\lambda} = H_{\\lambda} y$ where $H_{\\lambda}$ is the hat matrix, the GCV score is\n$$\n\\mathrm{GCV}(\\lambda) = \\frac{n \\, \\lVert y - \\hat{y}_{\\lambda} \\rVert_{2}^{2}}{\\big(n - \\mathrm{tr}(H_{\\lambda})\\big)^{2}}.\n$$\n- For ridge regression with the above scaling, the hat matrix can be expressed in the SVD basis as $H_{\\lambda} = U \\operatorname{diag}\\!\\left(\\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}\\right) U^{\\top}$, implying an effective Degrees of Freedom (DoF) $\\mathrm{df}(\\lambda) = \\mathrm{tr}(H_{\\lambda}) = \\sum_{i=1}^{r} \\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}$.\n- $K$-fold Cross-Validation (CV): Partition the index set $\\{1,\\dots,n\\}$ into $K$ disjoint folds. For each $\\lambda$ and each fold, train ridge regression on the $K-1$ training folds and compute the mean squared prediction error on the held-out fold; then average across folds to obtain $\\mathrm{CV}_{K}(\\lambda)$.\n\nData-generation assumptions: The response is generated from $y = X\\beta^{\\star} + \\varepsilon$, with noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, and a sparse ground-truth vector $\\beta^{\\star} \\in \\mathbb{R}^{p}$ with $s$ nonzero entries, each of magnitude $b  0$.\n\nConstruction of the design matrix with a prescribed eigen-spectrum: For each case, construct $X$ with SVD $X = U \\operatorname{diag}(d) V^{\\top}$ such that $\\gamma_{i} = d_{i}^{2}/n$ matches the specified spectrum for $i \\in \\{1,\\dots,r\\}$, where $r=\\min(n,p)$. Use orthonormal matrices $U$ and $V$ derived from a fixed-seed QR factorization of Gaussian random matrices to ensure replicability. Scale the nonzero singular values by $d_{i} = \\sqrt{n \\gamma_{i}}$.\n\nAngle units are not applicable. No physical units are required.\n\nProgram requirements:\n- Implement the ridge predictor, $\\hat{y}_{\\lambda}$, and compute $\\mathrm{GCV}(\\lambda)$ using the linear-smoother formulation in the SVD basis to avoid forming the hat matrix explicitly.\n- Implement $K$-fold Cross-Validation to evaluate $\\mathrm{CV}_{K}(\\lambda)$ across a logarithmically spaced grid of $\\lambda$ values.\n- For each test case, report:\n  $1)$ the $\\lambda$ that minimizes $\\mathrm{GCV}(\\lambda)$,\n  $2)$ the $\\lambda$ that minimizes $\\mathrm{CV}_{K}(\\lambda)$,\n  $3)$ the effective Degrees of Freedom at the GCV-selected $\\lambda$, $\\mathrm{df}(\\lambda_{\\mathrm{GCV}})$,\n  $4)$ the fraction of nonzero spectral components deemed effectively active by the eigen-thresholding heuristic $\\gamma_{i}  \\lambda_{\\mathrm{GCV}}$, that is, $\\frac{1}{r}\\sum_{i=1}^{r} \\mathbf{1}\\{\\gamma_{i}  \\lambda_{\\mathrm{GCV}}\\}$,\n  $5)$ the difference in $K$-fold Cross-Validation mean squared error between evaluating at $\\lambda_{\\mathrm{GCV}}$ and at $\\lambda_{\\mathrm{CV}}$, that is, $\\mathrm{CV}_{K}(\\lambda_{\\mathrm{GCV}}) - \\mathrm{CV}_{K}(\\lambda_{\\mathrm{CV}})$.\n- Use a logarithmic grid of $L$ values for $\\lambda$ between $10^{-6}$ and $10^{2}$, with $L = 60$. In the event of ties when selecting a minimizing $\\lambda$, choose the smallest $\\lambda$ among minimizers.\n- Fold construction: For $K$-fold splitting, if $n$ is not divisible by $K$, allocate $\\lfloor n/K \\rfloor$ or $\\lceil n/K \\rceil$ samples per fold so that folds differ by at most $1$ sample; assign extra samples to the earliest folds in index order.\n- Randomness control: Use a fixed base seed $2025$ and add the test case index $c \\in \\{1,2,3\\}$ to this base for all pseudorandom elements in that case, ensuring replicability.\n\nTest suite:\n- Case $1$ (high-dimensional, power-law spectrum): $n=80$, $p=200$, spectrum $\\gamma_{i} \\propto i^{-\\alpha}$ with $\\alpha=1.5$ normalized to have mean equal to $1$, $K=5$, $s=10$, $b=1.0$, $\\sigma=0.5$.\n- Case $2$ (low-dimensional, flat spectrum, nearly noiseless): $n=120$, $p=60$, spectrum $\\gamma_{i} \\equiv 1$ for $i \\in \\{1,\\dots,r\\}$, $K=10$, $s=8$, $b=1.0$, $\\sigma=10^{-6}$.\n- Case $3$ (high-dimensional, steep geometric spectrum): $n=60$, $p=300$, spectrum $\\gamma_{i} \\propto \\rho^{i-1}$ with $\\rho=0.7$ normalized to have mean equal to $1$, $K=3$, $s=5$, $b=1.0$, $\\sigma=1.5$.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of three lists, each inner list containing the five requested values in the order specified above. The exact format must be:\n\"[[case1_value1,case1_value2,case1_value3,case1_value4,case1_value5],[case2_value1,case2_value2,case2_value3,case2_value4,case2_value5],[case3_value1,case3_value2,case3_value3,case3_value4,case3_value5]]\"\nwith no spaces anywhere in the line. All reported numerical values must be standard real numbers. No other text should be printed.",
            "solution": "The user has provided a problem in the domain of computational systems biology, focused on regularization for high-dimensional inference. The task is to implement and compare two methods for selecting the penalty parameter $\\lambda$ in ridge regression: Generalized Cross-Validation (GCV) and $K$-fold Cross-Validation (CV). The analysis is performed on simulated data where the design matrix $X$ is constructed to have a specific, controlled eigen-spectrum, emulating correlated explanatory variables common in biological datasets like gene expression profiles.\n\nThe linear model is given by $y = X\\beta^{\\star} + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta^{\\star} \\in \\mathbb{R}^{p}$ is the true sparse coefficient vector, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is a vector of i.i.d. Gaussian noise.\n\nRidge regression finds an estimate $\\hat{\\beta}_{\\lambda}$ by solving the optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{2}^{2}\n$$\nThe solution to this problem is given by $\\hat{\\beta}_{\\lambda} = (X^{\\top}X + n\\lambda I_p)^{-1}X^{\\top}y$. The corresponding prediction is $\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda} = H_{\\lambda}y$, where $H_{\\lambda} = X(X^{\\top}X + n\\lambda I_p)^{-1}X^{\\top}$ is the \"hat\" matrix.\n\nThe design matrix $X$ is constructed via its Singular Value Decomposition (SVD), $X = U \\operatorname{diag}(d) V^{\\top}$, where $r = \\min(n,p)$, $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{p \\times r}$ are matrices with orthonormal columns, and $d$ is a vector of $r$ singular values. The eigenvalues of the covariance-like matrix $X^{\\top}X/n$ are specified as $\\{\\gamma_i\\}_{i=1}^r$, which are related to the singular values by $\\gamma_i = d_i^2/n$. This allows for the construction of data with pre-defined correlational structures.\n\nThe procedure for each test case is as follows:\n\nFirst, we generate the data. This involves:\n1.  Setting a unique random seed for reproducibility.\n2.  Generating the orthonormal matrices $U$ and $V$ by performing QR decomposition on Gaussian random matrices of appropriate sizes.\n3.  Computing the eigenvalues $\\{\\gamma_i\\}_{i=1}^r$ according to the specified spectral decay profile (power-law, flat, or geometric) and normalizing them to have a mean of $1$.\n4.  Calculating the singular values $d_i = \\sqrt{n \\gamma_i}$.\n5.  Constructing the design matrix $X = U \\operatorname{diag}(d) V^{\\top}$.\n6.  Creating the true sparse coefficient vector $\\beta^{\\star}$ with $s$ nonzero entries of magnitude $b$.\n7.  Generating the noise vector $\\varepsilon$ and computing the response $y = X\\beta^{\\star} + \\varepsilon$.\n\nSecond, we perform model selection for $\\lambda$ using GCV. The GCV score is defined as:\n$$\n\\mathrm{GCV}(\\lambda) = \\frac{n \\, \\lVert y - \\hat{y}_{\\lambda} \\rVert_{2}^{2}}{\\big(n - \\mathrm{tr}(H_{\\lambda})\\big)^{2}}\n$$\nA key insight is that both the prediction $\\hat{y}_{\\lambda}$ and the trace of the hat matrix, $\\mathrm{tr}(H_{\\lambda})$, can be computed efficiently using the SVD of $X$. As stated in the problem, the hat matrix can be written as $H_{\\lambda} = U \\operatorname{diag}\\!\\left(\\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}\\right) U^{\\top}$. The effective degrees of freedom (DoF) is then $\\mathrm{df}(\\lambda) = \\mathrm{tr}(H_{\\lambda}) = \\sum_{i=1}^{r} \\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}$. The prediction $\\hat{y}_{\\lambda}$ is calculated as $\\hat{y}_{\\lambda} = U \\operatorname{diag}(\\frac{\\gamma_i}{\\gamma_i+\\lambda}) (U^{\\top}y)$. This avoids forming the large matrices $H_\\lambda$ or $X^\\top X$, making the computation efficient. We evaluate $\\mathrm{GCV}(\\lambda)$ over a logarithmic grid of $\\lambda$ values and select $\\lambda_{\\mathrm{GCV}}$ as the value that minimizes this score.\n\nThird, we perform model selection using $K$-fold Cross-Validation. The dataset is partitioned into $K$ disjoint folds. For each $\\lambda$ on the grid and for each fold $k \\in \\{1, \\dots, K\\}$, we train a ridge regression model on the remaining $K-1$ folds (the training set, $(X_{\\text{train}}, y_{\\text{train}})$) and calculate the prediction error on the held-out fold (the validation set, $(X_{\\text{val}}, y_{\\text{val}})$).\nThe ridge coefficient for the $k$-th fold, $\\hat{\\beta}_{\\lambda}^{(k)}$, is found by solving the normal equations on the training data:\n$$\n(X_{\\text{train}}^{\\top}X_{\\text{train}} + n_{\\text{train}}\\lambda I_p)\\hat{\\beta}_{\\lambda}^{(k)} = X_{\\text{train}}^{\\top}y_{\\text{train}}\n$$\nwhere $n_{\\text{train}}$ is the number of samples in the training set. This linear system is solved for $\\hat{\\beta}_{\\lambda}^{(k)}$. For cases where $p  n_{\\text{train}}$, it is more computationally efficient to solve an equivalent system of smaller size (the \"kernel trick\").\nThe mean squared error for the fold is calculated, and the final CV score, $\\mathrm{CV}_K(\\lambda)$, is the average of these errors over all $K$ folds. We select $\\lambda_{\\mathrm{CV}}$ as the value that minimizes $\\mathrm{CV}_K(\\lambda)$.\n\nFinally, for each test case, we compute and report five quantities:\n1.  $\\lambda_{\\mathrm{GCV}}$: The optimal penalty selected by GCV.\n2.  $\\lambda_{\\mathrm{CV}}$: The optimal penalty selected by $K$-fold CV.\n3.  $\\mathrm{df}(\\lambda_{\\mathrm{GCV}})$: The effective degrees of freedom at $\\lambda_{\\mathrm{GCV}}$, calculated as $\\sum_{i=1}^{r} \\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda_{\\mathrm{GCV}}}$. This measures the model complexity.\n4.  The fraction of effectively active spectral components, given by $\\frac{1}{r}\\sum_{i=1}^{r} \\mathbf{1}\\{\\gamma_{i}  \\lambda_{\\mathrm{GCV}}\\}$. This is an intuitive heuristic for how many principal components of the data are being used by the ridge model, where $\\lambda_{\\mathrm{GCV}}$ acts as a threshold on the eigenvalues.\n5.  $\\mathrm{CV}_{K}(\\lambda_{\\mathrm{GCV}}) - \\mathrm{CV}_{K}(\\lambda_{\\mathrm{CV}})$: The difference in $K$-fold CV error between using the GCV-selected lambda and the CV-selected lambda. This measures how well GCV approximates the more computationally intensive K-fold CV. A small positive value indicates GCV is a good proxy.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation across all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'case_idx': 1, 'n': 80, 'p': 200, 'spectrum_type': 'power', 'alpha': 1.5, 'K': 5, 's': 10, 'b': 1.0, 'sigma': 0.5},\n        {'case_idx': 2, 'n': 120, 'p': 60, 'spectrum_type': 'flat', 'K': 10, 's': 8, 'b': 1.0, 'sigma': 1e-6},\n        {'case_idx': 3, 'n': 60, 'p': 300, 'spectrum_type': 'geometric', 'rho': 0.7, 'K': 3, 's': 5, 'b': 1.0, 'sigma': 1.5},\n    ]\n\n    all_results = []\n    \n    L = 60\n    lambda_grid = np.logspace(-6, 2, L)\n\n    for case in test_cases:\n        # Unpack parameters\n        n, p = case['n'], case['p']\n        K, s, b, sigma = case['K'], case['s'], case['b'], case['sigma']\n        \n        # Set seed for reproducibility\n        seed = 2025 + case['case_idx']\n        rng = np.random.default_rng(seed)\n\n        # --- Data Generation ---\n        r = min(n, p)\n\n        # Generate orthonormal matrices U and V\n        U, _ = np.linalg.qr(rng.standard_normal((n, r)), mode='reduced')\n        V, _ = np.linalg.qr(rng.standard_normal((p, r)), mode='reduced')\n\n        # Generate eigen-spectrum gamma\n        if case['spectrum_type'] == 'power':\n            i = np.arange(1, r + 1)\n            unnorm_gamma = i**(-case['alpha'])\n            norm_const = r / np.sum(unnorm_gamma)\n            gamma = norm_const * unnorm_gamma\n        elif case['spectrum_type'] == 'flat':\n            gamma = np.ones(r)\n        elif case['spectrum_type'] == 'geometric':\n            i = np.arange(r)\n            unnorm_gamma = case['rho']**i\n            norm_const = r / np.sum(unnorm_gamma)\n            gamma = norm_const * unnorm_gamma\n        \n        d = np.sqrt(n * gamma)\n        \n        # Construct design matrix X\n        X = U @ np.diag(d) @ V.T\n\n        # Generate beta_star and response y\n        beta_star = np.zeros(p)\n        beta_star[:s] = b\n        \n        epsilon = rng.standard_normal(n) * sigma\n        y = X @ beta_star + epsilon\n\n        # --- GCV Calculation ---\n        gcv_scores = np.zeros(L)\n        y_u = U.T @ y\n        \n        for i, lam in enumerate(lambda_grid):\n            # Effective degrees of freedom\n            dof = np.sum(gamma / (gamma + lam))\n            \n            # Prediction y_hat using SVD\n            w_lam = (gamma / (gamma + lam)) * y_u\n            y_hat = U @ w_lam\n            \n            # Residual Sum of Squares (RSS)\n            rss = np.sum((y - y_hat)**2)\n\n            # GCV score\n            denominator = (n - dof)**2\n            if denominator > 1e-9: # Avoid division by zero\n                gcv_scores[i] = n * rss / denominator\n            else:\n                gcv_scores[i] = np.inf\n\n\n        lam_gcv_idx = np.argmin(gcv_scores)\n        lam_gcv = lambda_grid[lam_gcv_idx]\n\n        # --- K-fold CV Calculation ---\n        # Create folds sequentially\n        indices = np.arange(n)\n        fold_sizes = [n // K + 1] * (n % K) + [n // K] * (K - (n % K))\n        \n        current = 0\n        folds = []\n        for fold_size in fold_sizes:\n            start, stop = current, current + fold_size\n            folds.append(indices[start:stop])\n            current = stop\n\n        cv_scores = np.zeros(L)\n        \n        for i, lam in enumerate(lambda_grid):\n            total_se = 0.0\n            for k in range(K):\n                val_idx = folds[k]\n                train_idx = np.setdiff1d(indices, val_idx)\n                \n                X_train, y_train = X[train_idx, :], y[train_idx]\n                X_val, y_val = X[val_idx, :], y[val_idx]\n                \n                n_train = len(train_idx)\n                \n                # Solve for beta_hat for the current fold\n                if p > n_train:\n                    # Use kernel trick (Woodbury identity) for p > n\n                    A = X_train @ X_train.T + n_train * lam * np.identity(n_train)\n                    alpha = np.linalg.solve(A, y_train)\n                    beta_hat = X_train.T @ alpha\n                else:\n                    # Use standard formulation\n                    A = X_train.T @ X_train + n_train * lam * np.identity(p)\n                    b_vec = X_train.T @ y_train\n                    beta_hat = np.linalg.solve(A, b_vec)\n                \n                y_hat_val = X_val @ beta_hat\n                total_se += np.sum((y_val - y_hat_val)**2)\n            \n            cv_scores[i] = total_se / n\n\n        lam_cv_idx = np.argmin(cv_scores)\n        lam_cv = lambda_grid[lam_cv_idx]\n\n        # --- Calculate Final Results ---\n        # 1. lambda_gcv\n        res1 = lam_gcv\n        \n        # 2. lambda_cv\n        res2 = lam_cv\n        \n        # 3. df(lambda_gcv)\n        res3 = np.sum(gamma / (gamma + lam_gcv))\n        \n        # 4. Fraction of active components\n        res4 = np.mean(gamma > lam_gcv)\n        \n        # 5. CV error difference\n        cv_at_gcv = cv_scores[lam_gcv_idx]\n        cv_at_cv = cv_scores[lam_cv_idx] \n        res5 = cv_at_gcv - cv_at_cv\n        \n        all_results.append([res1, res2, res3, res4, res5])\n\n    # Format the final output string\n    case_strings = []\n    for res_list in all_results:\n        case_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        }
    ]
}