{
    "hands_on_practices": [
        {
            "introduction": "在我们应用或模拟 Barabási-Albert (BA) 模型之前，我们必须首先理解其基本的数学性质。这项基础练习将引导您推导该模型最著名的结果——稳态度分布 $P(k)$ 。通过使用主方程方法，您不仅将了解 BA 模型为何能生成无标度网络，还将练习研究动态系统所必需的分析技巧。",
            "id": "3316388",
            "problem": "考虑一个计算系统生物学中演化的无向分子相互作用网络的生成模型，其中新的分子种类（节点）按顺序进入系统，并根据 Barabási-Albert (BA) 优先连接机制与现有种类建立相互作用（边）。Barabási-Albert (BA) 模型的定义如下：在每个离散时间步，引入一个新节点，该节点恰好有 $m \\ge 1$ 条边；每条边以与现有节点的当前度成正比的概率连接到一个现有节点。令 $k$ 表示度，令 $P(k)$ 表示在大型网络极限下，度为 $k$ 的节点的稳态分数，限定于 $k \\ge m$。\n\n从上述基本的 BA 规则（优先连接与度成正比，以及每增加一条边总度增量守恒）出发，通过编写并求解各度等级中节点预期数量的大时间主方程，推导 $P(k)$ (对于 $k \\ge m$) 的正确归一化离散形式，并确定唯一的预因子以确保 $\\sum_{k=m}^{\\infty} P(k) = 1$。您的最终答案必须是一个关于 $k$ 和 $m$ 的单一闭式解析表达式，对所有整数 $k \\ge m$ 均有效。不需要数值近似，最终表达式中也不应包含单位。",
            "solution": "所述问题是有效的。它在科学上基于网络理论的既定原则，特别是 Barabási-Albert (BA) 模型。该问题是适定的、客观的，并包含了进行严格推导所需的所有必要信息。我们将着手使用主方程方法推导度分布 $P(k)$。\n\n令 $N_k(t)$ 为时间步 $t$ 时度为 $k$ 的节点的期望数量。网络中的总节点数为 $N(t)$。模型从一个包含 $m_0$ 个节点的初始核心开始，在每个时间步增加一个新节点。因此，$N(t) = m_0 + t$。对于大型网络（$t \\to \\infty$），我们可以近似为 $N(t) \\approx t$。\n\n每个新节点引入 $m$ 条边。在时间 $t$ 的总边数是 $E(t) = E_0 + mt$，其中 $E_0$ 是初始核心中的边数。无向网络中的度之和是边数的两倍，所以 $\\sum_{i} k_i(t) = 2E(t) = 2(E_0 + mt)$。在大时间极限下，这个和主要由 $t$ 的线性项决定，得到 $\\sum_i k_i(t) \\approx 2mt$。\n\nBA 模型采用优先连接，即一条新边连接到现有节点 $i$ 的概率 $\\Pi(k_i)$ 与其度 $k_i$ 成正比。这个概率由下式给出：\n$$\n\\Pi(k_i) = \\frac{k_i}{\\sum_{j} k_j(t)} \\approx \\frac{k_i}{2mt}\n$$\n\n我们现在将在对大 $t$ 有效的连续近似下，为 $N_k(t)$ 的变化率（表示为 $\\frac{dN_k}{dt}$）建立主方程。度为 $k$ 的节点数量因两个过程而改变：\n1.  一个度为 $k-1$ 的节点获得一条边，成为一个度为 $k$ 的节点。\n2.  一个度为 $k$ 的节点获得一条边，成为一个度为 $k+1$ 的节点。\n\n在每个时间步 $dt$，增加一个新节点，形成 $m$ 条边。单位时间内增加的新边数为 $m$。这 $m$ 条新边连接到度为 $k$ 的节点的速率是 $m$ 乘以连接到任何此类节点的总概率。这个概率是此类节点的数量 $N_k(t)$ 乘以连接到单个节点的概率 $\\frac{k}{2mt}$。\n因此，度为 $k-1$ 的节点转化为度为 $k$ 的速率是：\n$$\n\\text{$N_k$ 的增益项} = m \\cdot N_{k-1}(t) \\cdot \\frac{k-1}{2mt} = \\frac{(k-1) N_{k-1}(t)}{2t}\n$$\n度为 $k$ 的节点转化为度为 $k+1$ 的速率是：\n$$\n\\text{$N_k$ 的损失项} = m \\cdot N_k(t) \\cdot \\frac{k}{2mt} = \\frac{k N_k(t)}{2t}\n$$\n合并这些项得到 $k > m$ 的主方程：\n$$\n\\frac{dN_k}{dt} = \\frac{(k-1) N_{k-1}}{2t} - \\frac{k N_k}{2t}\n$$\n\n对于 $k=m$ 存在一个特殊情况。度为 $m$ 的节点只能通过引入新节点来创建，因为每个新节点都具有度 $m$。因此，在每个时间步，$N_m$ 增加 1。当度为 $m$ 的节点获得一条边时，它们就会消失。因此，$N_m$ 的主方程是：\n$$\n\\frac{dN_m}{dt} = 1 - \\frac{m N_m}{2t}\n$$\n请注意，我们忽略了新节点连接到自身或其 $m$ 条边中有两条连接到同一目标的微小概率，因为这些事件在大型网络极限中可以忽略不计。\n\n我们关心的是大时间极限下的稳态度分布 $P(k)$。在此极限下，度为 $k$ 的节点分数 $P(k) = \\frac{N_k(t)}{N(t)}$ 变为常数。使用近似 $N(t) \\approx t$，我们有 $N_k(t) = P(k) N(t) \\approx P(k) t$。对 $t$ 求导得到 $\\frac{dN_k}{dt} \\approx P(k)$。\n\n将这些关系代入主方程：\n对于 $k > m$：\n$$\nP(k) = \\frac{(k-1) P(k-1)t}{2t} - \\frac{k P(k)t}{2t}\n$$\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\n整理 $P(k)$ 的表达式：\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{k+2}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\n这是一个关于 $P(k)$ (对于 $k > m$) 的递推关系。\n\n对于 $k=m$：\n$$\nP(m) = 1 - \\frac{m P(m)t}{2t}\n$$\n$$\nP(m) = 1 - \\frac{m}{2} P(m)\n$$\n整理 $P(m)$ 的表达式：\n$$\nP(m) \\left(1 + \\frac{m}{2}\\right) = 1\n$$\n$$\nP(m) \\left(\\frac{m+2}{2}\\right) = 1\n$$\n$$\nP(m) = \\frac{2}{m+2}\n$$\n我们现在通过将 $P(k)$ 的递推关系从 $k$ 展开到 $m$ 来求解它：\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1) = \\frac{k-1}{k+2} \\frac{k-2}{k+1} P(k-2) = \\dots\n$$\n$$\nP(k) = \\left( \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} \\right) P(m)\n$$\n这个乘积可以计算为：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m)(m+1)\\dots(k-1)}{(m+3)(m+4)\\dots(k+2)} = \\frac{\\frac{(k-1)!}{(m-1)!}}{\\frac{(k+2)!}{(m+2)!}} = \\frac{(k-1)!}{(m-1)!} \\frac{(m+2)!}{(k+2)!}\n$$\n$$\n= \\frac{(m+2)(m+1)m}{k(k+1)(k+2)}\n$$\n将此代回 $P(k)$ 的表达式中：\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} P(m)\n$$\n现在，使用我们找到的 $P(m)$ 的值：\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\cdot \\frac{2}{m+2} = \\frac{2m(m+1)}{k(k+1)(k+2)}\n$$\n这就是对于 $k \\ge m$ 的度分布的闭式表达式。\n\n最后，我们必须验证此分布是正确归一化的，即 $\\sum_{k=m}^{\\infty} P(k) = 1$。该和为：\n$$\n\\sum_{k=m}^{\\infty} P(k) = \\sum_{k=m}^{\\infty} \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}\n$$\n和中的项可以使用部分分式分解：\n$$\n\\frac{1}{k(k+1)(k+2)} = \\frac{1}{2k} - \\frac{1}{k+1} + \\frac{1}{2(k+2)} = \\frac{1}{2}\\left[\\left(\\frac{1}{k} - \\frac{1}{k+1}\\right) - \\left(\\frac{1}{k+1} - \\frac{1}{k+2}\\right)\\right]\n$$\n这是一个伸缩级数。令 $S = \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}$。\n使用恒等式 $\\sum_{i=n}^{\\infty} \\frac{1}{i(i+1)(i+2)} = \\frac{1}{2n(n+1)}$，这可以通过伸缩求和法证明：\n$$\nS = \\frac{1}{2} \\sum_{k=m}^{\\infty} \\left[ \\frac{1}{k(k+1)} - \\frac{1}{(k+1)(k+2)} \\right] = \\frac{1}{2} \\left[ \\frac{1}{m(m+1)} \\right]\n$$\n将此结果代回归一化和中：\n$$\n\\sum_{k=m}^{\\infty} P(k) = 2m(m+1) \\cdot \\left( \\frac{1}{2m(m+1)} \\right) = 1\n$$\n该分布已正确归一化。预因子不是假设的，而是直接从主方程形式体系中推导出来的，满足了问题的要求。因此，$P(k)$ 的最终表达式得到了验证。对于大的 $k$，$P(k) \\sim k^{-3}$，这是 BA 模型众所周知的幂律特征。",
            "answer": "$$\\boxed{\\frac{2m(m+1)}{k(k+1)(k+2)}}$$"
        },
        {
            "introduction": "理论上的度分布是一个渐近结果，只对无限大的网络有效。这个计算练习让您能够在更 realistic 的有限时间和规模情景下探索模型的行为 。通过从头开始实现 BA 的生长过程，您将观察到网络的瞬态动力学，并定量地测量经验度分布如何收敛到理论上的 $k^{-3}$ 幂律，从而弥合抽象理论与具体模拟之间的鸿沟。",
            "id": "3316319",
            "problem": "你的任务是形式化、分析并实现一个基于仿真的方法，用于量化 Barabási-Albert (BA) 优先连接模型中的瞬态度分布，并测量其向指数为 $3$ 的幂律收敛的过程。建模背景是计算系统生物学，其中无标度拓扑被用作分子相互作用网络的生成模型。你的程序必须能够复现指定的测试套件，并输出一行汇总所要求的数值结果。\n\n生成过程必须按如下方式定义。\n\n- 从一个种子网络开始，该网络是一个包含 $m_0$ 个节点的完全图。也就是说，初始节点数为 $m_0$，每个节点都与所有其他节点相连，这意味着每个种子节点的初始度为 $m_0 - 1$，初始总度数为 $m_0(m_0 - 1)$。\n\n- 在每个离散时间步 $t \\in \\{1,2,\\dots\\}$，添加一个新节点，该节点恰好有 $m$ 条边，其中 $m \\leq m_0$。这 $m$ 条边中的每一条都连接到一个不同的现有节点，这些节点是根据其当前度成比例的概率进行无放回抽样选择的。在一个时间步内，这 $m$ 个目标节点是使用在添加任何新边之前冻结的度值进行无放回抽样的。在该时间步中选择所有 $m$ 个目标后，添加这 $m$ 条边，使每个目标节点的度增加 $1$，并将新节点的度设置为 $m$。根据构造，不允许自环和多重边。\n\n- 令 $N(t) = m_0 + t$ 表示 $t$ 步后的节点数。令 $k_i(t)$ 表示节点 $i$ 在时间 $t$ 的度。令 $P_t(k)$ 表示在时间 $t$ 时度至少为 $m$ 的节点上的经验概率质量函数 (PMF)，即 $P_t(k) = \\frac{1}{|\\{i : k_i(t) \\ge m\\}|}\\sum_{i=1}^{N(t)} \\mathbf{1}\\{k_i(t)=k,\\,k \\ge m\\}$。\n\n- 使用重复的独立实现来估计 $P_t(k)$ 作为平均 PMF。具体来说，对于给定的参数元组 $(m_0,m,t)$，使用上述模型模拟 $R$ 个独立的网络，每个网络都从一个大小为 $m_0$ 的完全图开始，生长恰好 $t$ 步。对于每次实现，计算仅限于 $k \\ge m$ 的度数直方图，将其归一化为 PMF，然后在 $R$ 次实现中对 PMF 进行平均。等效地，你可以汇集所有实现中 $k \\ge m$ 的度数计数，然后进行一次归一化。\n\n你的分析必须从适用于 BA 模型的第一性原理出发：\n\n- 总度数 $S(t)$ 满足 $S(t) = S(0) + 2mt$，其中 $S(0) = m_0(m_0 - 1)$。\n\n- 在时间 $t+1$ 时，一个度为 $k_i(t)$ 的节点接收到一个连接的概率是 $k_i(t)/S(t)$。\n\n- 使用连续率或主方程方法推导渐近度分布 $P_{\\infty}(k)$（对于 $k \\ge m$）及其幂律指数。不要假设 $P_{\\infty}(k)$ 的形式；从上述模型定义和给定的 $S(t)$ 守恒关系中推导它。\n\n为了量化 $P_t(k)$ 向渐近线的收敛，定义以下距离。\n\n- 令 $K_{\\max}(t)$ 表示在时间 $t$ 汇集 $R$ 次实现的结果后，在度至少为 $m$ 的节点中观察到的最大度。在有限支撑集 $\\{m, m+1, \\dots, K_{\\max}(t)\\}$ 上定义一个参考分布 $Q_t(k)$，该分布与指数为 $3$ 的渐近幂律成正比，即对于 $k \\in \\{m,\\dots,K_{\\max}(t)\\}$，$Q_t(k) \\propto k^{-3}$，否则 $Q_t(k) = 0$，并进行归一化，使得 $\\sum_{k=m}^{K_{\\max}(t)} Q_t(k) = 1$。经验分布 $P_t(k)$（在相同支撑集上限制和归一化）与 $Q_t(k)$ 之间的全变差 (TV) 距离定义为\n$$\n\\mathrm{TV}(P_t, Q_t) \\;=\\; \\frac{1}{2} \\sum_{k=m}^{K_{\\max}(t)} \\bigl| P_t(k) - Q_t(k) \\bigr| \\,.\n$$\n\n你的任务有两个方面：\n\n- 从模型的优先连接规则和总度数守恒出发，解析地推导渐近分布 $P_{\\infty}(k)$ 的幂律指数。你的推导必须清楚地证明所有使用的近似和极限的合理性。\n\n- 实现一个程序，对于下面指定的每个测试用例，模拟该模型并使用上述过程计算 TV 距离 $\\mathrm{TV}(P_t,Q_t)$。\n\n测试套件：\n\n- 情况 1：$(m_0, m, t, R, \\mathrm{seed}) = (3, 1, 1, 10000, 17)$。\n\n- 情况 2：$(m_0, m, t, R, \\mathrm{seed}) = (5, 2, 30, 3000, 42)$。\n\n- 情况 3：$(m_0, m, t, R, \\mathrm{seed}) = (2, 1, 10, 4000, 12345)$。\n\n- 情况 4：$(m_0, m, t, R, \\mathrm{seed}) = (8, 3, 50, 2000, 2024)$。\n\n程序要求：\n\n- 程序必须严格按照规定实现模拟，并使用给定的种子以保证可复现性，每个测试用例初始化一个伪随机数生成器。\n\n- 对于每个测试用例，计算一个等于 $\\mathrm{TV}(P_t,Q_t)$ 的实数。\n\n- 输出格式：你的程序应生成单行输出，其中包含一个逗号分隔的结果列表，并用方括号括起来（例如，“[0.123456,0.234567,0.345678,0.456789]”）。每个数字必须四舍五入到小数点后六位。\n\n- 不涉及物理单位。不出现角度。所有输出均为十进制形式的实数。\n\n你最终提交的必须是一个独立的、可运行的程序，且不应需要任何输入。单行输出必须按上述顺序汇总四个情况的 TV 距离。",
            "solution": "该问题要求对 Barabási-Albert (BA) 模型的渐近幂律指数进行解析推导，并通过数值模拟来量化瞬态度分布向此渐近形式的收敛过程。\n\n### 渐近度分布的解析推导\n\n我们按照规定推导 BA 模型的渐近度分布 $P_{\\infty}(k)$。我们使用连续率方程方法，该方法对给定度的节点数随时间的变化进行建模。\n\n令 $N_k(t)$ 为在时间 $t$ 时度为 $k$ 的节点数。总节点数为 $N(t) = m_0 + t$，总度数为 $S(t) = m_0(m_0 - 1) + 2mt$。对于较大的时间 $t$，我们可以做近似 $N(t) \\approx t$ 和 $S(t) \\approx 2mt$。\n\n当一个节点在每个时间步被选为新增的 $m$ 条边中的一条的目标时，其度增加一。一个度为 $k_i(t)$ 的特定节点 $i$ 被选为单条新边的目标的概率是 $\\Pi(i) = k_i(t)/S(t)$。由于在每个时间步添加 $m$ 条边，并且对于连续模型假设这些选择是独立的（这对于大型网络是一个有效的近似），一个度为 $k$ 的节点获得一条边的速率是 $m \\cdot (k/S(t))$。\n\n度为 $k$ 的节点数 $N_k$ 因两个过程而改变：\n1.  **增加**：一个度为 $k-1$ 的节点被选中，其度变为 $k$。此过程的总速率是这类节点的数量 $N_{k-1}(t)$ 乘以每个节点获得一条边的速率：$m \\frac{k-1}{S(t)} N_{k-1}(t)$。\n2.  **减少**：一个度为 $k$ 的节点被选中，其度变为 $k+1$。此过程的总速率是 $m \\frac{k}{S(t)} N_k(t)$。\n\n此外，在每个时间步，会引入一个度为 $m$ 的新节点。这对 $N_m$ 起到源项的作用，贡献速率为 $1$。\n\n因此，$N_k(t)$ 变化的速率方程为：\n$$\n\\frac{d N_k(t)}{dt} = m \\frac{k-1}{S(t)} N_{k-1}(t) - m \\frac{k}{S(t)} N_k(t) + \\delta_{k,m}\n$$\n其中 $\\delta_{k,m}$ 是克罗内克 delta 函数，当 $k=m$ 时等于 $1$，否则为 $0$。当 $k=m$ 时，关于 $k-1$ 的项为零，因为没有节点的度小于 $m$。\n\n我们寻求在 $t$ 很大时的稳态解，此时度分布 $P(k)$ 变得与时间无关。我们假设 $N_k(t) = N(t) P(k) = (m_0+t) P(k)$。对 $t$ 求导得到 $\\frac{d N_k(t)}{dt} = P(k)$。\n\n将此式以及大 $t$ 近似 $N(t) \\approx t$ 和 $S(t) \\approx 2mt$ 代入速率方程：\n$$\nP(k) \\approx m \\frac{k-1}{2mt} (t \\cdot P(k-1)) - m \\frac{k}{2mt} (t \\cdot P(k)) + \\delta_{k,m}\n$$\n$$\nP(k) \\approx \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k) + \\delta_{k,m}\n$$\n\n对于 $k > m$，源项为零：\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\n重新整理各项：\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{2+k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n这得到递推关系：\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\n我们可以通过迭代求解：\n$$\nP(k) = P(m) \\prod_{j=m+1}^{k} \\frac{j-1}{j+2}\n$$\n该乘积可以展开为：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{m}{m+3} \\cdot \\frac{m+1}{m+4} \\cdot \\frac{m+2}{m+5} \\cdots \\frac{k-1}{k+2}\n$$\n这可以用阶乘表示，或更一般地，用伽马函数 $\\Gamma(z+1) = z!$ 表示：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{\\prod_{j=m+1}^{k} (j-1)}{\\prod_{j=m+1}^{k} (j+2)} = \\frac{\\Gamma(k)/\\Gamma(m)}{\\Gamma(k+3)/\\Gamma(m+3)} = \\frac{\\Gamma(m+3)}{\\Gamma(m)} \\frac{\\Gamma(k)}{\\Gamma(k+3)}\n$$\n使用属性 $\\Gamma(z+1) = z\\Gamma(z)$，我们有：\n$\\Gamma(m+3) = (m+2)(m+1)m\\Gamma(m)$ 和 $\\Gamma(k+3) = (k+2)(k+1)k\\Gamma(k)$。\n因此，乘积简化为：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m+2)(m+1)m\\Gamma(m)}{\\Gamma(m)} \\frac{\\Gamma(k)}{(k+2)(k+1)k\\Gamma(k)} = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)}\n$$\n所以，渐近度分布的形式为：\n$$\nP(k) = P(m) \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\quad \\text{for } k \\ge m\n$$\n对于大的 $k$，项 $k(k+1)(k+2) \\sim k^3$。因此，分布遵循幂律：\n$$\nP(k) \\propto k^{-3}\n$$\n幂律指数是 $\\gamma=3$。此推导证实了问题陈述中给出的值。比例常数取决于 $m$ 但不取决于 $k$。\n\n### 模拟与测量算法\n\n该任务要求实现一个模拟来生成 BA 网络，并计算经验度分布与理论幂律分布之间的全变差 (TV) 距离。\n\n1.  **模拟设置**：对于每个测试用例 $(m_0, m, t, R, \\mathrm{seed})$，我们执行 $R$ 次独立模拟。每个用例都使用给定的 `seed` 初始化一个伪随机数生成器，以确保可复现性。\n\n2.  **网络生成**：每次模拟运行过程如下：\n    -   **初始化**：网络在时间 $t=0$ 时以 $m_0$ 个节点在一个完全图中开始。每个初始节点的度为 $m_0 - 1$。使用一个数组存储网络中所有节点的度。\n    -   **生长**：模拟运行 $t$ 个时间步。在每个时间步中：\n        a. 添加一个新节点。\n        b. 选择 $m$ 个不同的现有节点作为连接目标。选择是概率性的，选择任何节点的概率与其当前度成正比。这通过从当前节点集中无放回地抽样 $m$ 个节点索引来实现，使用它们的度除以总度数作为概率分布。在第 $s$ 步（从 0 开始索引）开始时，总度数为 $S(s) = m_0(m_0-1) + 2ms$。\n        c. 将所选的 $m$ 个目标节点的度各增加 $1$。\n        d. 将新节点添加到网络中，其度为 $m$。将其度附加到度数组中。\n\n3.  **度聚合**：在给定测试用例的 $R$ 次模拟之后，汇集所有实现产生的最终度列表。构建一个频率图（或字典）来存储在所有 $R \\times (m_0 + t)$ 个节点中观察到的每个度值 $k$ 的总计数。\n\n4.  **PMF 计算**：\n    -   令聚合的度计数为 $C(k)$。\n    -   在所有模拟中观察到的最大度为 $K_{\\max} = \\max\\{k \\mid C(k)>0\\}$。\n    -   分析仅限于度 $k \\ge m$。我们的分布的支撑集是整数集合 $\\{m, m+1, \\dots, K_{\\max}\\}$。\n    -   **经验 PMF $P_t(k)$**：度 $k \\ge m$ 的节点总数为 $N_{total} = \\sum_{j=m}^{K_{\\max}} C(j)$。则经验 PMF 为 $P_t(k) = C(k) / N_{total}$，其中 $k \\in \\{m, \\dots, K_{\\max}\\}$。\n    -   **参考 PMF $Q_t(k)$**：这是一个在相同有限支撑集上指数为 $3$ 的归一化幂律分布。对于每个 $k \\in \\{m, \\dots, K_{\\max}\\}$，我们设置一个未归一化的值 $q'(k) = k^{-3}$。归一化常数为 $C_Q = \\sum_{j=m}^{K_{\\max}} j^{-3}$。参考 PMF 为 $Q_t(k) = q'(k) / C_Q$。\n\n5.  **全变差距离**：两个分布之间的距离计算如下：\n    $$\n    \\mathrm{TV}(P_t, Q_t) = \\frac{1}{2} \\sum_{k=m}^{K_{\\max}} |P_t(k) - Q_t(k)|\n    $$\n    为每个测试用例计算此值，并四舍五入到六位小数。\n\n最终输出是针对指定测试用例的这些 TV 距离值的列表，格式化为方括号内的逗号分隔字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(m0, m, t, rng):\n    \"\"\"\n    Runs a single realization of the Barabási-Albert model.\n\n    Args:\n        m0 (int): Number of nodes in the initial complete graph.\n        m (int): Number of edges added with each new node.\n        t (int): Number of time steps to simulate.\n        rng (np.random.Generator): The random number generator to use.\n\n    Returns:\n        np.ndarray: An array of the degrees of all nodes at the end of the simulation.\n    \"\"\"\n    # Initial network: a complete graph on m0 nodes.\n    # Each node has degree m0 - 1.\n    degrees = np.full(m0, m0 - 1, dtype=int)\n    \n    # Growth process for t steps.\n    for step in range(t):\n        current_num_nodes = m0 + step\n        \n        # Total degree can be computed analytically to be slightly faster\n        # S(t) = S(0) + 2mt. At start of step `step` (0-indexed), total nodes added is `step`.\n        total_degree = m0 * (m0 - 1) + 2 * m * step\n        \n        # Attachment probabilities are proportional to degree.\n        if total_degree == 0:\n            # Handle the case of no edges, though not possible with m0 >= 2.\n            # If m0=1, total_degree is 0. But m = m0 implies m=1. This case is not in the test suite.\n            # If so, attachment is uniform.\n            probs = np.ones(current_num_nodes) / current_num_nodes\n        else:\n            probs = degrees / total_degree\n\n        # Select m distinct nodes to attach to, without replacement.\n        node_indices = np.arange(current_num_nodes)\n        targets = rng.choice(node_indices, size=m, replace=False, p=probs)\n        \n        # Update degrees of target nodes.\n        degrees[targets] += 1\n        \n        # Add the new node with degree m.\n        degrees = np.append(degrees, m)\n        \n    return degrees\n\ndef calculate_tv_distance(m0, m, t, R, seed):\n    \"\"\"\n    Calculates the Total Variation distance for a given set of parameters.\n\n    Args:\n        m0 (int): Initial number of nodes.\n        m (int): Edges per new node.\n        t (int): Number of time steps.\n        R (int): Number of independent realizations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The computed TV distance.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    total_degree_counts = {}\n\n    for _ in range(R):\n        final_degrees = run_simulation(m0, m, t, rng)\n        for k in final_degrees:\n            total_degree_counts[k] = total_degree_counts.get(k, 0) + 1\n\n    if not total_degree_counts:\n        return 0.0\n\n    k_max_obs = max(total_degree_counts.keys())\n\n    # The problem defines the distributions on the support k >= m.\n    if m > k_max_obs:\n        return 0.0\n\n    k_support = np.arange(m, k_max_obs + 1)\n\n    # Compute empirical PMF P_t(k)\n    counts_in_support = np.array([total_degree_counts.get(k, 0) for k in k_support])\n    total_nodes_in_support = np.sum(counts_in_support)\n    \n    if total_nodes_in_support == 0:\n        return 0.0\n    \n    p_t = counts_in_support / total_nodes_in_support\n    \n    # Compute reference PMF Q_t(k)\n    q_unnormalized = k_support.astype(np.float64)**-3\n    norm_const = np.sum(q_unnormalized)\n\n    if norm_const == 0:\n      # This case is unlikely unless k_support is pathological\n      q_t = np.zeros_like(p_t)\n    else:    \n      q_t = q_unnormalized / norm_const\n    \n    # Compute Total Variation distance\n    tv_dist = 0.5 * np.sum(np.abs(p_t - q_t))\n    \n    return tv_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (3, 1, 1, 10000, 17),\n        (5, 2, 30, 3000, 42),\n        (2, 1, 10, 4000, 12345),\n        (8, 3, 50, 2000, 2024),\n    ]\n\n    results = []\n    for case in test_cases:\n        m0, m, t, R, seed = case\n        tv_distance = calculate_tv_distance(m0, m, t, R, seed)\n        results.append(f\"{tv_distance:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "计算系统生物学中的一项关键任务是确定观察到的生物网络是否遵循像 BA 模型这样的特定模型。这需要将模型拟合到数据并估计其参数，例如幂律指数 $\\gamma$。这个实践问题解决了统计推断的关键议题 ，首先揭示了一种常见但有缺陷的图形估计方法中固有的偏差，然后引导您推导出统计上严谨的最大似然估计 (MLE)。",
            "id": "3316339",
            "problem": "在计算系统生物学中，蛋白质-蛋白质相互作用网络和转录调控网络已通过类似于 Barabási–Albert 过程的带有优先连接的增长机制进行建模，从而产生重尾度分布。假设对一个大型网络进行了采样，并且只保留度至少为一个已知下限截断值 $k_{\\min}$ 的节点，以减轻低度模型错误设定的影响。设保留的度是由具有尾指数 $\\gamma$ 的离散幂律模型生成的独立观测值 $k_1, k_2, \\dots, k_n$，因此对于整数 $k \\geq k_{\\min}$，模型赋予的概率为\n$$\nP(k \\mid \\gamma, k_{\\min}) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})},\n$$\n其中 $\\zeta(\\gamma, k_{\\min})$ 是确保在离散支持上归一化的 Hurwitz zeta 函数。分析师通常使用普通最小二乘法 (OLS) 对直方图化的度数计数，来拟合 $\\ln P(k)$ 与 $\\ln k$ 的斜率。\n\n从多项式或泊松采样模型下直方图计数的统计特性以及上述离san幂律似然的定义出发，从第一性原理上解释为什么对分箱的对数-对数数据使用 OLS 通常会在有限样本中产生对尾指数 $\\gamma$ 的有偏估计。然后，通过计算 Fisher 信息，推导该模型下 $\\gamma$ 的离散最大似然估计 (MLE) 及其大样本方差。你的推导必须从上面定义的似然函数以及期望和方差的标准性质出发，不得调用任何快捷公式。\n\n将你的最终答案表示为关于 $k_{\\min}$、$n$ 和观测度 $\\{k_i\\}_{i=1}^{n}$ 的封闭形式解析表达式，如果需要，可以使用特殊函数及其导数。不要提供数值近似。不需要单位。如果你引入一个缩写词（例如，OLS），你必须在第一次使用时定义它。",
            "solution": "这个问题要求两个不同的部分：首先，从原理上解释为什么对数分箱的度数计数进行普通最小二乘法 (OLS) 回归会得到幂律指数的有偏估计；其次，推导离散幂律分布下指数 $\\gamma$ 的最大似然估计 (MLE) 及其大样本方差。\n\n### 第一部分：在对数-对数分箱数据上使用OLS的偏差\n\n对于 $k \\geq k_{\\min}$，观测到度为 $k$ 的概率模型由下式给出\n$$ P(k \\mid \\gamma, k_{\\min}) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} $$\n对这个方程取自然对数，得到 $\\ln(P(k))$ 和 $\\ln(k)$ 之间的线性关系：\n$$ \\ln(P(k)) = -\\gamma \\ln(k) - \\ln(\\zeta(\\gamma, k_{\\min})) $$\n这个方程的形式是 $y = mx + c$，其中 $y = \\ln(P(k))$，$m = -\\gamma$，$x = \\ln(k)$，以及 $c = -\\ln(\\zeta(\\gamma, k_{\\min}))$。\n\n在对数分箱数据上应用 OLS 方法，首先需要从数据中构建一个经验概率分布。设 $n_k$ 为样本中度为 $k$ 的节点数，总样本大小为 $n = \\sum_{i=1}^n 1$。对于不同的 $k$ 值，计数集合 $\\{n_k\\}$ 服从多项式分布。度为 $k$ 的经验概率估计为 $\\hat{P}(k) = n_k / N_{obs}$，其中 $N_{obs}$ 是直方图中节点的总数（如果我们对原始观测值进行分箱，则为 $n$）。为简单起见，我们将计数 $n_k$ 视为独立的泊松随机变量，其均值为 $\\lambda_k = n \\cdot P(k)$，这是在大 $n$ 和小 $P(k)$ 情况下的一个常用近似。\n\nOLS 过程随后对点 $(\\ln(k), \\ln(\\hat{P}(k)))$ 拟合一条直线，将斜率的相反数作为 $\\gamma$ 的估计值。在有限样本中，这个过程通常是有偏的，至少有三个基本原因。\n\n1.  **对数变换带来的系统性偏差**：OLS 回归是在 $\\ln(\\hat{P}(k))$ 上执行的，而不是 $\\hat{P}(k)$。虽然经验频率 $\\hat{P}(k)$ 是真实概率 $P(k)$ 的无偏估计（即 $\\mathbb{E}[\\hat{P}(k)] = P(k)$），但对数是一个非线性的凹函数。根据琴生不等式，对于任何凹函数 $f$ 和随机变量 $X$，有 $\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])$。将此应用于 $f(x) = \\ln(x)$ 和 $X = \\hat{P}(k)$：\n    $$ \\mathbb{E}[\\ln(\\hat{P}(k))] \\leq \\ln(\\mathbb{E}[\\hat{P}(k)]) = \\ln(P(k)) $$\n    这个不等式表明，回归中因变量 $\\ln(\\hat{P}(k))$ 的期望值系统性地低于它本应估计的真值 $\\ln(P(k))$。这给回归拟合带来了一个系统性偏差，即使所有其他假设都得到满足，这个偏差也不会消失。\n\n2.  **违反同方差性**：OLS 的有效性和效率的一个关键假设是所有观测值的误差方差是恒定的（同方差性）。在对数-对数回归中，“误差”是 $\\ln(\\hat{P}(k))$ 与真实直线的偏差。让我们分析一下 $\\ln(\\hat{P}(k))$ 的方差。使用泊松模型处理计数 $n_k$，我们有 $\\mathbb{E}[n_k] = nP(k)$ 和 $\\text{Var}(n_k) = nP(k)$。经验概率是 $\\hat{P}(k) = n_k/n$，所以 $\\text{Var}(\\hat{P}(k)) = \\frac{1}{n^2}\\text{Var}(n_k) = \\frac{P(k)}{n}$。对 $\\ln(\\hat{P}(k))$ 在 $P(k)$ 周围使用一阶泰勒展开（delta 方法）：\n    $$ \\ln(\\hat{P}(k)) \\approx \\ln(P(k)) + \\frac{1}{P(k)}(\\hat{P}(k) - P(k)) $$\n    这个近似的方差是：\n    $$ \\text{Var}(\\ln(\\hat{P}(k))) \\approx \\frac{1}{P(k)^2} \\text{Var}(\\hat{P}(k)) = \\frac{1}{P(k)^2} \\frac{P(k)}{n} = \\frac{1}{n P(k)} $$\n    由于 $P(k)$ 是 $k$ 的函数，这个方差显然不是恒定的。对于幂律分布，$P(k)$ 随着 $k$ 的增加而减小，这意味着 $\\text{Var}(\\ln(\\hat{P}(k)))$ 对于较大的 $k$ 值会急剧增加。分布尾部的数据点基于非常少的计数，因此高度不确定，其误差方差要大得多。OLS 对所有点给予相同的权重，这意味着这些高方差、不可靠的尾部点会对估计的斜率产生不成比例的巨大影响，从而导致对 $\\gamma$ 的有偏和低效估计。\n\n3.  **零计数的处理**：对于有限样本 $n$，很可能对于某些度 $k$（特别是大的 $k$），计数 $n_k$ 将为零。在这种情况下，$\\hat{P}(k)=0$，而 $\\ln(\\hat{P}(k))$ 是未定义的。一种常见但统计上无原则的临时解决方案是简单地从回归中省略这些点。这种选择性地移除数据会引入系统性偏差，因为它以非随机的方式丢弃了信息，实际上是截断了经验分布的尾部。\n\n基于这些原因，对对数-对数分箱数据使用 OLS 不是估计幂律指数的可靠方法。应首选像最大似然估计这样有原则的方法。\n\n### 第二部分：最大似然估计 (MLE) 及其方差\n\nMLE 是从样本的似然函数推导出来的。给定来自离散幂律分布 $P(k \\mid \\gamma, k_{\\min})$ 的 $n$ 个独立同分布的观测值 $k_1, k_2, \\dots, k_n$，似然函数 $L(\\gamma)$ 是每个观测值概率的乘积：\n$$ L(\\gamma) = \\prod_{i=1}^{n} P(k_i \\mid \\gamma, k_{\\min}) = \\prod_{i=1}^{n} \\frac{k_i^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} = \\frac{ \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} }{ \\left[ \\zeta(\\gamma, k_{\\min}) \\right]^n } $$\n使用对数似然函数 $\\mathcal{L}(\\gamma) = \\ln L(\\gamma)$ 更为方便：\n$$ \\mathcal{L}(\\gamma) = \\ln \\left( \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} \\right) - \\ln \\left( \\left[ \\zeta(\\gamma, k_{\\min}) \\right]^n \\right) $$\n$$ \\mathcal{L}(\\gamma) = -\\gamma \\sum_{i=1}^{n} \\ln(k_i) - n \\ln(\\zeta(\\gamma, k_{\\min})) $$\n为了找到 MLE，记为 $\\hat{\\gamma}$，我们将 $\\mathcal{L}(\\gamma)$ 对 $\\gamma$ 求导，并令结果为零。Hurwitz zeta 函数 $\\zeta(\\gamma, k_{\\min}) = \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma}$ 关于 $\\gamma$ 的导数是：\n$$ \\zeta'(\\gamma, k_{\\min}) = \\frac{\\partial}{\\partial \\gamma} \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} = \\sum_{j=k_{\\min}}^{\\infty} \\frac{\\partial}{\\partial \\gamma} e^{-\\gamma \\ln(j)} = \\sum_{j=k_{\\min}}^{\\infty} (-\\ln(j)) e^{-\\gamma \\ln(j)} = -\\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} \\ln(j) $$\n对对数似然函数求导：\n$$ \\frac{d\\mathcal{L}}{d\\gamma} = -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{1}{\\zeta(\\gamma, k_{\\min})} \\frac{d}{d\\gamma}\\zeta(\\gamma, k_{\\min}) = -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} $$\n将此导数设为零，定义了 MLE $\\hat{\\gamma}$：\n$$ -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} = 0 $$\n整理后得到必须为 $\\hat{\\gamma}$ 求解的方程：\n$$ \\frac{1}{n} \\sum_{i=1}^{n} \\ln(k_i) = - \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} $$\n这是一个关于 $\\hat{\\gamma}$ 的超越方程，必须进行数值求解。这个方程本身就是估计量的正式定义。\n\n接下来，我们推导 $\\hat{\\gamma}$ 的大样本方差。方差由 Fisher 信息 $I(\\gamma)$ 的倒数给出。对于 $n$ 个独立同分布的观测值，$I(\\gamma) = - \\mathbb{E} \\left[ \\frac{d^2\\mathcal{L}}{d\\gamma^2} \\right]$。我们首先计算对数似然函数的二阶导数：\n$$ \\frac{d^2\\mathcal{L}}{d\\gamma^2} = \\frac{d}{d\\gamma} \\left( -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} \\right) = -n \\frac{d}{d\\gamma} \\left( \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} \\right) $$\n使用商法则：\n$$ \\frac{d^2\\mathcal{L}}{d\\gamma^2} = -n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})\\zeta(\\gamma, k_{\\min}) - (\\zeta'(\\gamma, k_{\\min}))^2}{(\\zeta(\\gamma, k_{\\min}))^2} \\right] $$\n其中 $\\zeta''(\\gamma, k_{\\min})$ 是 Hurwitz zeta 函数的二阶导数：\n$$ \\zeta''(\\gamma, k_{\\min}) = \\frac{\\partial}{\\partial \\gamma} \\left( -\\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} \\ln(j) \\right) = -\\sum_{j=k_{\\min}}^{\\infty} (-\\ln(j)) j^{-\\gamma} \\ln(j) = \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} (\\ln(j))^2 $$\n$\\frac{d^2\\mathcal{L}}{d\\gamma^2}$ 的表达式不依赖于数据 $\\{k_i\\}$，所以其期望就是表达式本身：$\\mathbb{E} \\left[ \\frac{d^2\\mathcalL}{d\\gamma^2} \\right] = \\frac{d^2\\mathcal{L}}{d\\gamma^2}$。\n因此，Fisher 信息是：\n$$ I(\\gamma) = - \\frac{d^2\\mathcal{L}}{d\\gamma^2} = n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})\\zeta(\\gamma, k_{\\min}) - (\\zeta'(\\gamma, k_{\\min}))^2}{(\\zeta(\\gamma, k_{\\min}))^2} \\right] = n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} - \\left(\\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})}\\right)^2 \\right] $$\n在大样本量 $n$ 的极限下，MLE $\\hat{\\gamma}$ 的方差接近 Cramér-Rao 下界，即 Fisher 信息的倒数。\n$$ \\text{Var}(\\hat{\\gamma}) \\approx [I(\\gamma)]^{-1} = \\frac{1}{n} \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} - \\left(\\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})}\\right)^2 \\right]^{-1} $$\n在实践中，$\\gamma$ 的真值是未知的，因此通过将 MLE $\\hat{\\gamma}$ 代入 $\\gamma$ 来估计此方差。",
            "answer": "$$\n\\boxed{\n\\pmatrix{\n\\frac{1}{n}\\sum_{i=1}^{n} \\ln(k_i) = - \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})}\n\n\\frac{1}{n} \\left[ \\frac{\\zeta''(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} - \\left( \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} \\right)^2 \\right]^{-1}\n}\n}\n$$"
        }
    ]
}