{
    "hands_on_practices": [
        {
            "introduction": "Biological function emerges from processes that are fundamentally stochastic. This practice delves into the origins of noise in gene expression, a cornerstone of cellular function, by formally distinguishing between intrinsic and extrinsic sources of variability. By analyzing a canonical model of bursty transcription using the Chemical Master Equation, you will derive the Fano factor, a crucial metric that reveals how the mechanics of production shape the statistics of protein or mRNA levels .",
            "id": "3305386",
            "problem": "Consider a single-gene expression module for messenger ribonucleic acid (mRNA) where synthesis occurs in stochastic bursts and degradation is first-order. Bursts arrive as a homogeneous Poisson process with rate $k$ (bursts per unit time). A burst adds a random number $M$ of mRNA molecules, where $M$ is independent and identically distributed across bursts and geometric with support on $\\{0,1,2,\\dots\\}$ and probability mass function $\\Pr(M=m)=p(1-p)^{m}$; the degradation of each mRNA molecule occurs independently at rate $\\gamma$ (per molecule). Define the Fano factor $F$ as $F=\\frac{\\operatorname{Var}(n)}{\\mathbb{E}[n]}$, where $n$ is the steady-state mRNA copy number. The geometric burst-size distribution has mean $\\mathbb{E}[M]=\\frac{1-p}{p}$ and variance $\\operatorname{Var}(M)=\\frac{1-p}{p^{2}}$, so the mean burst size $b$ satisfies $b=\\frac{1-p}{p}$.\n\nYour tasks are:\n- Precisely differentiate intrinsic noise and extrinsic noise in gene expression using the law of total variance and conditioning on environmental or parametric variables that fluctuate across the cell population.\n- Starting from the Chemical Master Equation (CME) and the generator formulation for expectations under jump processes, derive closed-form steady-state expressions for $\\mathbb{E}[n]$ and $\\operatorname{Var}(n)$ for this bursty birth–death model, and then compute the Fano factor $F$.\n\nExpress your final answer as a single closed-form analytic expression in terms of $b$ only. No numerical approximation is required, and no units are needed because $F$ is dimensionless.",
            "solution": "The problem presents two tasks: first, to differentiate between intrinsic and extrinsic noise in the context of gene expression using a formal statistical framework, and second, to derive the Fano factor for a specific stochastic model of mRNA expression.\n\nThe total noise, or variability, in the copy number $n$ of a molecular species across a population of cells can be quantified by its variance, $\\operatorname{Var}(n)$. This total variance can be decomposed into two distinct components, termed intrinsic and extrinsic noise, by applying the law of total variance. Let $\\mathbf{Z}$ be a vector of random variables representing the state of the cellular environment or context, which fluctuates from cell to cell. These variables may include concentrations of polymerases, ribosomes, metabolic enzymes, cell volume, or the cell cycle phase. The law of total variance states:\n$$\n\\operatorname{Var}(n) = \\mathbb{E}[\\operatorname{Var}(n | \\mathbf{Z})] + \\operatorname{Var}(\\mathbb{E}[n | \\mathbf{Z}])\n$$\nThe two terms on the right-hand side correspond to intrinsic and extrinsic noise, respectively.\n\n$1$. **Intrinsic Noise**: The term $\\mathbb{E}[\\operatorname{Var}(n | \\mathbf{Z})]$ quantifies intrinsic noise. The conditional variance, $\\operatorname{Var}(n | \\mathbf{Z}=\\mathbf{z})$, represents the variability in the copy number $n$ for a subpopulation of cells that share the exact same extrinsic state $\\mathbf{z}$. This variability arises from the inherently stochastic nature of the biochemical reactions of gene expression (e.g., transcription initiation, mRNA degradation), where the timing of individual reaction events is a random process even in a perfectly constant environment. The intrinsic noise is the expectation of this conditional variance, averaged over all possible extrinsic states $\\mathbf{Z}$ that exist in the population.\n\n$2$. **Extrinsic Noise**: The term $\\operatorname{Var}(\\mathbb{E}[n | \\mathbf{Z})]$ quantifies extrinsic noise. The conditional expectation, $\\mathbb{E}[n | \\mathbf{Z}=\\mathbf{z}]$, is the average copy number of $n$ for cells in the specific extrinsic state $\\mathbf{z}$. This average level can depend on the state $\\mathbf{z}$. The extrinsic noise is the variance of this conditional mean, capturing how the average expression level itself fluctuates as the extrinsic state $\\mathbf{Z}$ varies across the cell population. It is the contribution to total variance caused by cell-to-cell differences in the cellular machinery and environment.\n\nNext, we derive the Fano factor for the given model. The model consists of two processes governing the mRNA copy number $n$:\n- **Bursty Production**: New mRNA molecules are produced in bursts, arriving as a Poisson process with rate $k$. Each burst adds $M$ molecules, where $M$ is a random variable with a geometric distribution, $\\Pr(M=m) = p(1-p)^m$ for $m \\in \\{0, 1, 2, \\dots\\}$. The bursts are independent and identically distributed.\n- **First-Order Degradation**: Each mRNA molecule degrades independently with a rate constant $\\gamma$. The total degradation rate for $n$ molecules is $\\gamma n$.\n\nThe time evolution of the expectation of any function $f(n)$ of the state can be derived from the Chemical Master Equation (CME) using the generator formalism. The rate of change of $\\mathbb{E}[f(n)]$ is given by $\\frac{d\\mathbb{E}[f(n)]}{dt} = \\mathbb{E}[\\mathcal{L}f(n)]$, where $\\mathcal{L}$ is the generator of the process. For this model, the generator acting on $f(n)$ is:\n$$\n\\mathcal{L}f(n) = \\gamma n [f(n-1) - f(n)] + k \\sum_{m=0}^{\\infty} \\Pr(M=m)[f(n+m) - f(n)]\n$$\nTo find the steady-state moments, we set $\\frac{d\\mathbb{E}[f(n)]}{dt} = 0$.\n\nFirst, we derive the steady-state mean, $\\mathbb{E}[n]$, by setting $f(n)=n$.\n$$\n\\frac{d\\mathbb{E}[n]}{dt} = \\mathbb{E}[\\gamma n ((n-1) - n) + k \\sum_{m=0}^{\\infty} \\Pr(M=m)((n+m) - n)]\n$$\n$$\n\\frac{d\\mathbb{E}[n]}{dt} = \\mathbb{E}[-\\gamma n + k \\sum_{m=0}^{\\infty} m \\Pr(M=m)]\n$$\nThe summation term is the definition of the expectation of the burst size, $\\mathbb{E}[M]$.\n$$\n\\frac{d\\mathbb{E}[n]}{dt} = -\\gamma \\mathbb{E}[n] + k \\mathbb{E}[M]\n$$\nAt steady state, $\\frac{d\\mathbb{E}[n]}{dt}=0$, which yields the steady-state mean:\n$$\n\\mathbb{E}[n] = \\frac{k}{\\gamma} \\mathbb{E}[M]\n$$\n\nNext, we derive the steady-state second moment, $\\mathbb{E}[n^2]$, by setting $f(n)=n^2$.\n$$\n\\frac{d\\mathbb{E}[n^2]}{dt} = \\mathbb{E}[\\gamma n ((n-1)^2 - n^2) + k \\sum_{m=0}^{\\infty} \\Pr(M=m)((n+m)^2 - n^2)]\n$$\nWe evaluate the terms inside the square brackets:\n- Degradation: $\\gamma n(n^2-2n+1-n^2) = \\gamma n(-2n+1) = -2\\gamma n^2 + \\gamma n$.\n- Production: $k \\sum_{m=0}^{\\infty} \\Pr(M=m)(n^2+2nm+m^2-n^2) = k \\sum_{m=0}^{\\infty} \\Pr(M=m)(2nm+m^2) = k (2n \\sum_{m=0}^{\\infty} m \\Pr(M=m) + \\sum_{m=0}^{\\infty} m^2 \\Pr(M=m)) = k(2n\\mathbb{E}[M] + \\mathbb{E}[M^2])$.\nTaking the expectation gives the evolution of $\\mathbb{E}[n^2]$:\n$$\n\\frac{d\\mathbb{E}[n^2]}{dt} = \\mathbb{E}[-2\\gamma n^2 + \\gamma n + k(2n\\mathbb{E}[M] + \\mathbb{E}[M^2])]\n$$\n$$\n\\frac{d\\mathbb{E}[n^2]}{dt} = -2\\gamma \\mathbb{E}[n^2] + \\gamma \\mathbb{E}[n] + 2k\\mathbb{E}[M]\\mathbb{E}[n] + k\\mathbb{E}[M^2]\n$$\nAt steady state, $\\frac{d\\mathbb{E}[n^2]}{dt}=0$. We use the steady-state relation $k\\mathbb{E}[M] = \\gamma \\mathbb{E}[n]$ and substitute $\\mathbb{E}[n^2] = \\operatorname{Var}(n) + (\\mathbb{E}[n])^2$:\n$$\n0 = -2\\gamma (\\operatorname{Var}(n) + (\\mathbb{E}[n])^2) + \\gamma \\mathbb{E}[n] + 2(\\gamma\\mathbb{E}[n])\\mathbb{E}[n] + k\\mathbb{E}[M^2]\n$$\n$$\n0 = -2\\gamma\\operatorname{Var}(n) - 2\\gamma(\\mathbb{E}[n])^2 + \\gamma\\mathbb{E}[n] + 2\\gamma(\\mathbb{E}[n])^2 + k\\mathbb{E}[M^2]\n$$\nThe terms involving $(\\mathbb{E}[n])^2$ cancel, simplifying the equation to:\n$$\n2\\gamma\\operatorname{Var}(n) = \\gamma\\mathbb{E}[n] + k\\mathbb{E}[M^2]\n$$\nSolving for the variance $\\operatorname{Var}(n)$:\n$$\n\\operatorname{Var}(n) = \\frac{1}{2}\\mathbb{E}[n] + \\frac{k}{2\\gamma}\\mathbb{E}[M^2]\n$$\nThe Fano factor $F$ is defined as $F = \\frac{\\operatorname{Var}(n)}{\\mathbb{E}[n]}$. Substituting the expressions for mean and variance:\n$$\nF = \\frac{\\frac{1}{2}\\mathbb{E}[n] + \\frac{k}{2\\gamma}\\mathbb{E}[M^2]}{\\mathbb{E}[n]} = \\frac{1}{2} + \\frac{k\\mathbb{E}[M^2]}{2\\gamma\\mathbb{E}[n]}\n$$\nNow, substitute $\\mathbb{E}[n] = \\frac{k}{\\gamma}\\mathbb{E}[M]$ into the Fano factor expression:\n$$\nF = \\frac{1}{2} + \\frac{k\\mathbb{E}[M^2]}{2\\gamma(\\frac{k}{\\gamma}\\mathbb{E}[M])} = \\frac{1}{2} + \\frac{\\mathbb{E}[M^2]}{2\\mathbb{E}[M]}\n$$\nThis is a general result for any burst size distribution. The problem specifies a geometric distribution with mean burst size $b=\\mathbb{E}[M]=\\frac{1-p}{p}$ and variance $\\operatorname{Var}(M)=\\frac{1-p}{p^2}$. We must express $F$ in terms of $b$ only.\nFirst, we express $\\operatorname{Var}(M)$ in terms of $b$. From $b=\\frac{1-p}{p}=\\frac{1}{p}-1$, we have $\\frac{1}{p}=b+1$.\n$$\n\\operatorname{Var}(M) = \\frac{1-p}{p^2} = \\left(\\frac{1-p}{p}\\right)\\frac{1}{p} = b(b+1)\n$$\nNext, we find $\\mathbb{E}[M^2]$ using the relation $\\operatorname{Var}(M) = \\mathbb{E}[M^2] - (\\mathbb{E}[M])^2$:\n$$\n\\mathbb{E}[M^2] = \\operatorname{Var}(M) + (\\mathbb{E}[M])^2 = b(b+1) + b^2 = b^2+b+b^2 = 2b^2+b\n$$\nFinally, we substitute $\\mathbb{E}[M]=b$ and $\\mathbb{E}[M^2]=2b^2+b$ into the expression for $F$:\n$$\nF = \\frac{1}{2} + \\frac{2b^2+b}{2b}\n$$\nAssuming a non-trivial process where $b>0$, we can simplify:\n$$\nF = \\frac{1}{2} + \\frac{b(2b+1)}{2b} = \\frac{1}{2} + \\frac{2b+1}{2} = \\frac{1}{2} + b + \\frac{1}{2} = 1+b\n$$\nThis result shows that the Fano factor, a measure of noise relative to the mean, is composed of a Poisson component ($1$) and a term linearly proportional to the mean burst size ($b$).",
            "answer": "$$\n\\boxed{1+b}\n$$"
        },
        {
            "introduction": "The complexity of biological networks is often managed through a modular architecture, where groups of densely interconnected components perform specific functions. This practice introduces spectral clustering, a powerful method for uncovering this hidden modularity directly from network interaction data. By implementing the algorithm and analyzing the spectrum of the graph Laplacian, you will see how a mathematical feature known as the \"eigen-gap\" signals the presence of robust, cohesive modules within a larger system .",
            "id": "3305369",
            "problem": "You are given a family of symmetric, nonnegative weighted graphs that model regulatory networks. Each graph has $n$ nodes, an adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$ with $W_{ij} = W_{ji} \\ge 0$ and $W_{ii} = 0$, a degree vector $d \\in \\mathbb{R}^{n}$ defined by $d_i = \\sum_{j=1}^n W_{ij}$, and a diagonal degree matrix $D = \\mathrm{diag}(d_1,\\dots,d_n)$. Define the symmetric normalized graph Laplacian (GL) by $L_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$, where $I$ is the identity matrix and $D^{-1/2}$ is the diagonal matrix with entries $D^{-1/2}_{ii} = d_i^{-1/2}$ for $d_i > 0$ (if $d_i = 0$, interpret the corresponding row and column as isolated with zero entries). The spectral clustering approach chooses a number of clusters $k$ and partitions nodes using the first $k$ eigenvectors of $L_{\\mathrm{sym}}$. The eigen-gap between consecutive eigenvalues is defined by $\\Delta_i = \\lambda_{i+1} - \\lambda_i$ for an ordered spectrum $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$. The chosen $k$ is the index at which $\\Delta_k$ is maximal (with ties broken by picking the smallest such $k$), constrained to $1 \\le k \\le n-1$.\n\nStarting from these foundational definitions, implement the following for each test instance:\n- Compute the ordered eigenvalues $\\lambda_1,\\dots,\\lambda_n$ and corresponding orthonormal eigenvectors of $L_{\\mathrm{sym}}$, compute all gaps $\\Delta_i$ for $i \\in \\{1,\\dots,n-1\\}$, and select $k$ as described above. Report the eigen-gap magnitude $g^\\star = \\Delta_k$.\n- Form the matrix $U \\in \\mathbb{R}^{n \\times k}$ by stacking the first $k$ eigenvectors as columns. Row-normalize $U$ so that each row has Euclidean norm $1$. Then cluster the $n$ rows of $U$ into $k$ clusters by minimizing the within-cluster sum of squared Euclidean distances via $k$-means.\n- Compute the weighted modularity $Q$ of the clustering using the definition\n$$\nQ = \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\left( W_{ij} - \\frac{d_i d_j}{2m} \\right) \\mathbf{1}\\{c_i = c_j\\},\n$$\nwhere $m = \\tfrac{1}{2} \\sum_{i=1}^n d_i$ is the total edge weight, $c_i \\in \\{1,\\dots,k\\}$ is the cluster label for node $i$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Report $Q$ as a real number.\n- Define a robustness test for modular structure: zero out all inter-cluster edges to produce $\\widetilde{W}$ with $\\widetilde{W}_{ij} = W_{ij}$ if $c_i = c_j$ and $\\widetilde{W}_{ij} = 0$ otherwise. For each cluster $\\mathcal{C}$, consider the induced subgraph with adjacency $W^{(\\mathcal{C})}$ and its combinatorial Laplacian $L^{(\\mathcal{C})} = D^{(\\mathcal{C})} - W^{(\\mathcal{C})}$, where $D^{(\\mathcal{C})}_{uu} = \\sum_{v \\in \\mathcal{C}} W^{(\\mathcal{C})}_{uv}$. Compute the second-smallest eigenvalue $\\lambda_2^{(\\mathcal{C})}$ (the algebraic connectivity) of $L^{(\\mathcal{C})}$ for each cluster of size at least $2$. Define a small threshold $\\tau = 10^{-6}$ and return a boolean $\\mathrm{robust}$ that is true if $k \\ge 2$ and $\\lambda_2^{(\\mathcal{C})} > \\tau$ for all clusters of size at least $2$ (clusters of size $1$ are trivially connected and may be ignored for this test); otherwise return false.\n\nIn addition, justify algorithmically and mathematically why the magnitude of the eigen-gap $g^\\star$ signals modular structure that enhances functional robustness, starting only from the definitions of $W$, $D$, $L_{\\mathrm{sym}}$, and the concept of clustering by the first $k$ eigenvectors.\n\nTest Suite and Graph Construction Rules:\nEach test instance is defined by parameters $(n, \\{\\mathcal{C}_\\ell\\}_{\\ell=1}^r, w_{\\mathrm{intra}}, w_{\\mathrm{inter}})$, where $n$ is the number of nodes, $\\{\\mathcal{C}_\\ell\\}$ is a partition of $\\{1,\\dots,n\\}$ into $r$ nonempty clusters (used only to construct $W$; the algorithm must rediscover structure), $w_{\\mathrm{intra}} \\ge 0$ is the weight assigned to edges within the same construction cluster, and $w_{\\mathrm{inter}} \\ge 0$ is the weight assigned to edges between different construction clusters. The adjacency matrix is constructed by\n$$\nW_{ij} = \\begin{cases}\n0 & \\text{if } i=j, \\\\\nw_{\\mathrm{intra}} & \\text{if } i \\neq j \\text{ and } \\exists \\ell \\text{ with } i \\in \\mathcal{C}_\\ell, j \\in \\mathcal{C}_\\ell, \\\\\nw_{\\mathrm{inter}} & \\text{if } i \\neq j \\text{ and } \\exists \\ell \\neq \\ell' \\text{ with } i \\in \\mathcal{C}_\\ell, j \\in \\mathcal{C}_{\\ell'}.\n\\end{cases}\n$$\n\nProvide results for the following five test instances:\n- Test $1$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2,3\\}$, $\\mathcal{C}_2 = \\{4,5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.05$.\n- Test $2$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2,3\\}$, $\\mathcal{C}_2 = \\{4,5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.35$.\n- Test $3$: $n = 6$, construction cluster $\\mathcal{C}_1 = \\{1,2,3,4,5,6\\}$, $w_{\\mathrm{intra}} = 0.9$, $w_{\\mathrm{inter}} = 0.9$.\n- Test $4$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2\\}$, $\\mathcal{C}_2 = \\{3,4\\}$, $\\mathcal{C}_3 = \\{5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.02$.\n- Test $5$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2,3\\}$, $\\mathcal{C}_2 = \\{4,5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ [k_1, g^\\star_1, Q_1, \\mathrm{robust}_1], [k_2, g^\\star_2, Q_2, \\mathrm{robust}_2], \\dots ]$). Each $k_i$ must be an integer, each $g^\\star_i$ and $Q_i$ must be floats, and each $\\mathrm{robust}_i$ must be a boolean. No other output should be printed.",
            "solution": "The problem requires a multi-step analysis of graph structures using spectral methods to identify modularity and assess its robustness. The analysis is predicated on the properties of the symmetric normalized graph Laplacian, $L_{\\mathrm{sym}}$. We will first provide the mathematical and algorithmic justification for the approach, then detail the implementation steps.\n\n### Mathematical and Algorithmic Justification\n\nThe core of the problem lies in using the spectrum of the graph Laplacian to understand the structure of a network. The symmetric normalized Laplacian, $L_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$, is a central object in spectral graph theory. Its eigenvalues, $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n \\le 2$, and corresponding eigenvectors encode deep structural information about the graph represented by the adjacency matrix $W$.\n\n**1. Eigen-gap and Modular Structure:**\n\nThe link between the eigen-gap and modularity is best understood by starting with an ideal case. Consider a graph that is composed of $k$ disconnected components. In this case, the adjacency matrix $W$ can be arranged into a block-diagonal form (after reordering nodes). The Laplacian $L_{\\mathrm{sym}}$ will also be block-diagonal. The spectrum of a block-diagonal matrix is the union of the spectra of its blocks. For a connected graph, the smallest eigenvalue of its Laplacian is $\\lambda_1 = 0$, with a corresponding eigenvector whose entries are proportional to $d_i^{1/2}$. For a graph with $k$ connected components, the Laplacian has an eigenvalue of $0$ with multiplicity $k$. That is, $\\lambda_1 = \\lambda_2 = \\dots = \\lambda_k = 0$, and $\\lambda_{k+1} > 0$. The eigenspace corresponding to the zero eigenvalues is spanned by vectors that are piecewise constant over the connected components. Specifically, for each component $\\mathcal{C}$, a vector $v$ with entries $v_i = d_i^{1/2}$ if node $i \\in \\mathcal{C}$ and $v_i=0$ otherwise, is an eigenvector for $\\lambda=0$.\n\nThis leads to a large eigen-gap, $\\Delta_k = \\lambda_{k+1} - \\lambda_k = \\lambda_{k+1} > 0$. The first $k$ eigenvectors, which form the matrix $U$, can be chosen such that for any node $i$, its corresponding row in $U$ is non-zero in only one position. All nodes within the same component are mapped to the same point in the $k$-dimensional embedding space. Clustering these points is trivial and perfectly recovers the components.\n\nNow, consider a nearly-decomposable graph: a graph with $k$ dense clusters (modules) that are sparsely connected to each other. Such a graph is a small perturbation of the ideal case of $k$ disconnected components. The inter-cluster edges, weighted by $w_{\\mathrm{inter}}$, act as the perturbation. According to matrix perturbation theory, the spectrum of the perturbed Laplacian, $L_{\\mathrm{sym}}$, will be close to the spectrum of the unperturbed (block-diagonal) one. The first $k$ eigenvalues, $\\lambda_1, \\dots, \\lambda_k$, will be small (close to $0$), while $\\lambda_{k+1}$ will be significantly larger. This results in a large eigen-gap $g^\\star = \\Delta_k = \\lambda_{k+1} - \\lambda_k$. The corresponding eigenvectors $v_1, \\dots, v_k$ are no longer perfectly piecewise constant but are *approximately* piecewise constant on the modules.\n\nWhen we form the matrix $U \\in \\mathbb{R}^{n \\times k}$ with these eigenvectors, the rows of $U$ (representing the nodes) will form tight, well-separated bundles in $\\mathbb{R}^k$. Each bundle corresponds to a module in the original graph. These bundles are readily separable by clustering algorithms like $k$-means, thus revealing the underlying modular structure. The larger the gap $g^\\star$, the better the separation between modules and the more pronounced the modular structure.\n\n**2. Modularity and Functional Robustness:**\n\nFunctional robustness in a biological network often relates to the stability of functional modules against perturbations. The problem formalizes this by testing the internal connectivity of the identified modules. After finding a partition $\\{c_i\\}$, the robustness test involves conceptually severing all inter-cluster connections. A module is considered robust if the resulting isolated subgraph remains strongly connected.\n\nThe algebraic connectivity of a graph, given by the second-smallest eigenvalue $\\lambda_2$ of its combinatorial Laplacian, quantifies its connectivity. A value of $\\lambda_2 > 0$ implies the graph is connected, and a larger $\\lambda_2$ corresponds to a \"more robustly\" connected graph that is harder to disconnect by removing edges or nodes.\n\nThe procedure computes $\\lambda_2^{(\\mathcal{C})}$ for each identified cluster $\\mathcal{C}$ (of size at least 2). The condition $\\lambda_2^{(\\mathcal{C})} > \\tau$ for a small threshold $\\tau$ checks if each module is, at a minimum, connected. A large eigen-gap $g^\\star$ implies that the graph is well-approximated by disconnected components. Since a module is defined by dense internal connections ($w_{\\mathrm{intra}}$ is typically much larger than $w_{\\mathrm{inter}}$), each discovered cluster $\\mathcal{C}$ is expected to be a densely connected subgraph. Densely connected graphs have high algebraic connectivity. Therefore, a large eigen-gap $g^\\star$ signals a modular structure where each module is internally cohesive and thus robust to the removal of its connections to other modules. This provides a direct link between the spectral signature of modularity ($g^\\star$) and a quantifiable measure of structural robustness ($\\lambda_2^{(\\mathcal{C})}$).\n\n### Algorithmic Implementation Steps\n\nFor each test case defined by $(n, \\{\\mathcal{C}_\\ell\\}, w_{\\mathrm{intra}}, w_{\\mathrm{inter}})$:\n1.  **Construct Adjacency Matrix $W$**: An $n \\times n$ matrix is created where $W_{ij}$ is set to $w_{\\mathrm{intra}}$ if nodes $i$ and $j$ are in the same construction cluster, $w_{\\mathrm{inter}}$ if they are in different clusters, and $0$ if $i=j$.\n2.  **Compute Graph Laplacians**: The degree vector $d$ is computed by summing the rows of $W$. The diagonal degree matrix $D$ is formed. The symmetric normalized Laplacian is computed as $L_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$. For any node $i$ with $d_i=0$, the corresponding entry $D^{-1/2}_{ii}$ is treated as $0$.\n3.  **Eigendecomposition and Gap Analysis**: The eigenvalues $\\lambda_1, \\dots, \\lambda_n$ and orthonormal eigenvectors of $L_{\\mathrm{sym}}$ are computed. The eigenvalues are sorted in ascending order. The eigen-gaps $\\Delta_i = \\lambda_{i+1} - \\lambda_i$ for $i=1, \\dots, n-1$ are calculated. The number of clusters $k$ is determined as the index that maximizes this gap, i.e., $k = \\mathrm{arg\\,max}_{i \\in \\{1, \\dots, n-1\\}} \\Delta_i$. Ties are broken by choosing the smallest $k$. The maximal gap magnitude is $g^\\star = \\Delta_k$.\n4.  **Spectral Embedding and Clustering**: A matrix $U \\in \\mathbb{R}^{n \\times k}$ is formed using the first $k$ eigenvectors as its columns. Each row of $U$ is normalized to have a Euclidean norm of $1$. The $n$ row-vectors are then clustered into $k$ groups using a $k$-means algorithm. To ensure a stable and optimal clustering, the $k$-means algorithm is run multiple times with random initializations, and the partition that minimizes the total within-cluster sum of squared distances is selected.\n5.  **Modularity Calculation**: The weighted modularity $Q$ of the resulting partition $\\{c_i\\}$ is computed using the formula:\n    $$\n    Q = \\frac{1}{2m} \\sum_{i,j} \\left( W_{ij} - \\frac{d_i d_j}{2m} \\right) \\mathbf{1}\\{c_i = c_j\\}\n    $$\n    where $m=\\frac{1}{2}\\sum_i d_i$ is the total weight of all edges.\n6.  **Robustness Test**: The boolean value $\\mathrm{robust}$ is determined. It is set to `False` if $k < 2$. Otherwise, for each cluster $\\mathcal{C}$ with size at least $2$, its induced subgraph is extracted. The combinatorial Laplacian $L^{(\\mathcal{C})}$ of this subgraph is formed, and its second-smallest eigenvalue $\\lambda_2^{(\\mathcal{C})}$ (algebraic connectivity) is computed. If $\\lambda_2^{(\\mathcal{C})} \\le \\tau=10^{-6}$ for any such cluster, $\\mathrm{robust}$ is set to `False`. If all such clusters pass the test, $\\mathrm{robust}$ is `True`.\n7.  **Store and Report Results**: The tuple $(k, g^\\star, Q, \\mathrm{robust})$ is stored for each test case. The final output is a list of these tuples.",
            "answer": "```python\nimport numpy as np\nfrom scipy.cluster.vq import kmeans, vq\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases.\n    \"\"\"\n    test_cases = [\n        # Test 1\n        (6, [{0, 1, 2}, {3, 4, 5}], 1.0, 0.05),\n        # Test 2\n        (6, [{0, 1, 2}, {3, 4, 5}], 1.0, 0.35),\n        # Test 3\n        (6, [{0, 1, 2, 3, 4, 5}], 0.9, 0.9),\n        # Test 4\n        (6, [{0, 1}, {2, 3}, {4, 5}], 1.0, 0.02),\n        # Test 5\n        (6, [{0, 1, 2}, {3, 4, 5}], 1.0, 0.0),\n    ]\n\n    all_results = []\n    for n, clusters_def, w_intra, w_inter in test_cases:\n        # Step 1: Construct Adjacency Matrix W\n        W = np.zeros((n, n))\n        node_to_cluster_map = {node: i for i, cl in enumerate(clusters_def) for node in cl}\n        \n        for i in range(n):\n            for j in range(i + 1, n):\n                if node_to_cluster_map[i] == node_to_cluster_map[j]:\n                    W[i, j] = W[j, i] = w_intra\n                else:\n                    W[i, j] = W[j, i] = w_inter\n\n        # Step 2: Compute Graph Laplacian L_sym\n        d = np.sum(W, axis=1)\n        D = np.diag(d)\n        \n        D_inv_sqrt_vals = np.zeros(n)\n        d_positive_mask = d > 0\n        D_inv_sqrt_vals[d_positive_mask] = 1.0 / np.sqrt(d[d_positive_mask])\n        D_inv_sqrt = np.diag(D_inv_sqrt_vals)\n        \n        I = np.identity(n)\n        L_sym = I - D_inv_sqrt @ W @ D_inv_sqrt\n\n        # Step 3: Eigendecomposition and Gap Analysis\n        e_vals, e_vecs = np.linalg.eigh(L_sym)\n        \n        # Round eigenvalues to handle numerical precision issues\n        e_vals = np.round(e_vals, 10)\n        \n        gaps = e_vals[1:] - e_vals[:-1]\n        \n        # Find k, constrained to 1 <= k <= n-1. \n        # Gap indices are 0 to n-2, mapping to k=1 to n-1.\n        if n > 1:\n            k_idx = np.argmax(gaps[:n-1])\n            k = k_idx + 1\n            g_star = gaps[k_idx]\n        else: # single node graph\n            k = 1\n            g_star = 0.0\n\n        # Step 4: Spectral Embedding and Clustering\n        U = e_vecs[:, :k]\n        \n        # Row-normalize U\n        row_norms = np.linalg.norm(U, axis=1, keepdims=True)\n        U_norm = np.zeros_like(U)\n        non_zero_rows = row_norms.flatten() > 0\n        U_norm[non_zero_rows] = U[non_zero_rows] / row_norms[non_zero_rows]\n        \n        # k-means clustering with restarts for stability\n        best_distortion = np.inf\n        best_centroids = None\n        num_restarts = 20\n        # If there's only 1 cluster, results are trivial\n        if k == 1:\n            labels = np.zeros(n, dtype=int)\n        else:\n            for _ in range(num_restarts):\n                centroids, distortion = kmeans(U_norm, k, iter=10)\n                if distortion < best_distortion:\n                    best_distortion = distortion\n                    best_centroids = centroids\n            labels, _ = vq(U_norm, best_centroids)\n        \n        # Step 5: Modularity Calculation\n        m = 0.5 * np.sum(d)\n        if m == 0:\n            Q = 0.0\n        else:\n            Q = 0.0\n            for i in range(n):\n                for j in range(n):\n                    if labels[i] == labels[j]:\n                        Q += (W[i, j] - (d[i] * d[j]) / (2 * m))\n            Q /= (2 * m)\n\n        # Step 6: Robustness Test\n        tau = 1e-6\n        is_robust = False\n        if k >= 2:\n            is_robust = True\n            unique_labels = np.unique(labels)\n            for label in unique_labels:\n                cluster_nodes = np.where(labels == label)[0]\n                if len(cluster_nodes) >= 2:\n                    W_c = W[np.ix_(cluster_nodes, cluster_nodes)]\n                    d_c = np.sum(W_c, axis=1)\n                    D_c = np.diag(d_c)\n                    L_c = D_c - W_c\n                    \n                    if L_c.shape[0] > 1:\n                        c_e_vals = np.linalg.eigh(L_c)[0]\n                        lambda_2_c = c_e_vals[1]\n                        if lambda_2_c <= tau:\n                            is_robust = False\n                            break\n        \n        all_results.append([k, g_star, Q, is_robust])\n\n    # Final print statement\n    result_str = ','.join([f\"[{r[0]},{r[1]:.8f},{r[2]:.8f},{str(r[3]).lower()}]\" for r in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Robustness is not absolute; even stable biological systems can approach critical \"tipping points\" where their function collapses. This exercise explores the phenomenon of critical slowing down, which provides universal early-warning signals of an impending state transition. By analyzing a canonical stochastic model near a bifurcation, you will derive how statistical measures like variance and autocorrelation change, providing a framework for detecting and understanding the fragility of emergent biological states .",
            "id": "3305415",
            "problem": "Consider a single-gene regulatory module near a saddle-node bifurcation, where the linearized dynamics around a stable equilibrium are modeled by the stochastic differential equation (SDE) with additive white noise: $$\\mathrm{d}x(t) = -k(\\mu)\\, x(t)\\,\\mathrm{d}t + \\sigma\\, \\mathrm{d}W_t,$$ where $x(t)$ is the deviation from the equilibrium, $W_t$ is a standard Wiener process (Brownian motion), $\\sigma > 0$ is the noise amplitude, and $k(\\mu) = k_0\\left(\\mu_c - \\mu\\right)$ is the restoring rate that depends linearly on the distance to a critical parameter value $\\mu_c$, with $k_0 > 0$ and $\\mu < \\mu_c$ to ensure stability. This setting is a canonical model for critical slowing down and early-warning signals in computational systems biology, where emergence and robustness are studied via dynamical systems and stochastic processes.\n\nStarting from this SDE and the definition of stationarity for a linear system driven by white noise, derive, in terms of the parameters $k(\\mu)$, $\\sigma$, and a sampling interval $\\Delta t$, the stationary autocovariance function, the lag-$1$ autocorrelation at sampling interval $\\Delta t$ defined as the autocovariance at lag $\\Delta t$ divided by the zero-lag autocovariance, and the stationary variance. Show how these quantities scale as $\\mu \\to \\mu_c^{-}$ (equivalently, $k(\\mu) \\to 0^{+}$), and explain why their behavior constitutes early-warning signals of fragility and loss of robustness and modularity in the biological system.\n\nImplement a program that, using the derived expressions, computes for each of the following parameter sets the lag-$1$ autocorrelation and stationary variance. Use dimensionless units for time and state. Then, using only the first three cases, compute the scaling exponent by fitting a line (via least squares) to $\\log$-variance versus $\\log k(\\mu)$ and returning the slope. Also, using only the first three cases ordered by increasing $\\mu$, return a boolean that is $\\mathrm{True}$ if both the lag-$1$ autocorrelation and the stationary variance increase monotonically as $\\mu$ approaches $\\mu_c$ (i.e., as $k(\\mu)$ decreases), and $\\mathrm{False}$ otherwise.\n\nTest suite (each case is $(\\mu, \\mu_c, k_0, \\sigma, \\Delta t)$):\n- Case $1$: $(0.2, 1.0, 1.0, 0.5, 0.1)$\n- Case $2$: $(0.99, 1.0, 1.0, 0.5, 0.1)$\n- Case $3$: $(0.0, 1.0, 1.0, 0.5, 0.1)$\n- Case $4$: $(0.99, 1.0, 1.0, 0.0, 0.1)$\n- Case $5$: $(0.99, 1.0, 1.0, 0.5, 10.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order: for each of the five cases, first the lag-$1$ autocorrelation and then the stationary variance, followed by the estimated scaling exponent using only Cases $1$–$3$, and finally the early-warning boolean computed using only Cases $1$–$3$ ordered by increasing $\\mu$. For example, the format should be $$[\\rho_1^{(1)}, \\operatorname{Var}^{(1)}, \\rho_1^{(2)}, \\operatorname{Var}^{(2)}, \\rho_1^{(3)}, \\operatorname{Var}^{(3)}, \\rho_1^{(4)}, \\operatorname{Var}^{(4)}, \\rho_1^{(5)}, \\operatorname{Var}^{(5)}, s, \\mathrm{EWS}],$$ where each $\\rho_1^{(i)}$ and $\\operatorname{Var}^{(i)}$ is a floating-point number, $s$ is a floating-point number, and $\\mathrm{EWS}$ is a boolean. No other output is permitted.",
            "solution": "The core of the problem lies in analyzing the stochastic differential equation (SDE) for a single-gene regulatory module, which is a form of the Ornstein-Uhlenbeck process:\n$$ \\mathrm{d}x(t) = -k(\\mu)\\, x(t)\\,\\mathrm{d}t + \\sigma\\, \\mathrm{d}W_t $$\nHere, $x(t)$ is the state's deviation from equilibrium, $k(\\mu) = k_0(\\mu_c - \\mu)$ is the stability or restoring rate, with $k_0 > 0$ and $\\mu < \\mu_c$ ensuring $k(\\mu)>0$ and thus stability. $\\sigma$ is the magnitude of the additive white noise, represented by the differential of a Wiener process, $\\mathrm{d}W_t$.\n\nThe formal solution to this SDE for $x(t)$ given an initial condition $x(s)$ at time $s<t$ is:\n$$ x(t) = x(s)e^{-k(\\mu)(t-s)} + \\sigma \\int_s^t e^{-k(\\mu)(t-u)}\\,\\mathrm{d}W_u $$\nWe are interested in the stationary properties of the process, which corresponds to the limit where the system has been running for an infinitely long time ($s \\to -\\infty$). In this limit, the influence of any specific initial condition vanishes. Assuming the process starts from $t_0 \\to -\\infty$ with $x(t_0)=0$, the state at time $t$ is given by the stochastic integral:\n$$ x(t) = \\sigma \\int_{-\\infty}^t e^{-k(\\mu)(t-u)}\\,\\mathrm{d}W_u $$\nThe process has a mean of $\\mathbb{E}[x(t)] = 0$ because the expectation of the stochastic integral is zero.\n\nFirst, we derive the stationary variance, $\\operatorname{Var}(x)$. By definition, $\\operatorname{Var}(x) = \\mathbb{E}[x(t)^2] - (\\mathbb{E}[x(t)])^2$. Since the mean is zero, this simplifies to $\\operatorname{Var}(x) = \\mathbb{E}[x(t)^2]$.\n$$ \\operatorname{Var}(x) = \\mathbb{E}\\left[\\left(\\sigma \\int_{-\\infty}^t e^{-k(\\mu)(t-u)}\\,\\mathrm{d}W_u\\right)^2\\right] $$\nUsing the Itô isometry property, which states that $\\mathbb{E}\\left[\\left(\\int_a^b f(u)\\,\\mathrm{d}W_u\\right)^2\\right] = \\int_a^b \\mathbb{E}[f(u)^2]\\,\\mathrm{d}u$, and since our integrand is deterministic, we have:\n$$ \\operatorname{Var}(x) = \\sigma^2 \\int_{-\\infty}^t \\left(e^{-k(\\mu)(t-u)}\\right)^2\\,\\mathrm{d}u = \\sigma^2 \\int_{-\\infty}^t e^{-2k(\\mu)(t-u)}\\,\\mathrm{d}u $$\nTo solve the integral, we make a substitution $v = 2k(\\mu)(t-u)$, so $\\mathrm{d}v = -2k(\\mu)\\,\\mathrm{d}u$. The integration limits change from $u \\to -\\infty$ to $v \\to \\infty$ and from $u \\to t$ to $v \\to 0$:\n$$ \\operatorname{Var}(x) = \\sigma^2 \\int_{\\infty}^0 e^{-v} \\left(\\frac{-\\mathrm{d}v}{2k(\\mu)}\\right) = \\frac{\\sigma^2}{2k(\\mu)} \\int_0^\\infty e^{-v}\\,\\mathrm{d}v = \\frac{\\sigma^2}{2k(\\mu)} [-e^{-v}]_0^\\infty = \\frac{\\sigma^2}{2k(\\mu)}(0 - (-1)) $$\nThus, the stationary variance is:\n$$ \\operatorname{Var}(x) = \\frac{\\sigma^2}{2k(\\mu)} $$\n\nNext, we derive the stationary autocovariance function, $C(\\tau) = \\mathbb{E}[x(t)x(t+\\tau)]$ for a lag $\\tau > 0$. We express $x(t+\\tau)$ in terms of $x(t)$:\n$$ x(t+\\tau) = x(t)e^{-k(\\mu)\\tau} + \\sigma \\int_t^{t+\\tau} e^{-k(\\mu)(t+\\tau-u)}\\,\\mathrm{d}W_u $$\nSubstituting this into the expectation for $C(\\tau)$:\n$$ C(\\tau) = \\mathbb{E}\\left[x(t)\\left(x(t)e^{-k(\\mu)\\tau} + \\sigma \\int_t^{t+\\tau} e^{-k(\\mu)(t+\\tau-u)}\\,\\mathrm{d}W_u\\right)\\right] $$\n$$ C(\\tau) = \\mathbb{E}[x(t)^2]e^{-k(\\mu)\\tau} + \\mathbb{E}\\left[x(t) \\cdot \\sigma \\int_t^{t+\\tau} e^{-k(\\mu)(t+\\tau-u)}\\,\\mathrm{d}W_u\\right] $$\nThe second term is zero. This is because $x(t)$ is a function of the Wiener process $W_s$ for times $s \\le t$, while the integral term involves increments of the Wiener process $\\mathrm{d}W_u$ for times $u > t$. Since increments of a Wiener process are independent of the process's past, the two factors are independent. The expectation of their product is the product of their expectations, and the expectation of the stochastic integral is zero. Therefore:\n$$ C(\\tau) = \\mathbb{E}[x(t)^2]e^{-k(\\mu)\\tau} = \\operatorname{Var}(x)e^{-k(\\mu)\\tau} $$\nThe stationary autocovariance function for any lag $\\tau$ is thus:\n$$ C(\\tau) = \\frac{\\sigma^2}{2k(\\mu)} e^{-k(\\mu)|\\tau|} $$\n\nThe lag-$1$ autocorrelation at a sampling interval $\\Delta t$, denoted $\\rho_1$, is defined as the autocovariance at lag $\\tau=\\Delta t$ divided by the zero-lag autocovariance, which is the variance $C(0) = \\operatorname{Var}(x)$.\n$$ \\rho_1 = \\rho(\\Delta t) = \\frac{C(\\Delta t)}{C(0)} = \\frac{\\operatorname{Var}(x)e^{-k(\\mu)\\Delta t}}{\\operatorname{Var}(x)} = e^{-k(\\mu)\\Delta t} $$\n\nNow, we analyze the behavior of these quantities as the system approaches the critical point $\\mu \\to \\mu_c^{-}$, which implies $k(\\mu) \\to 0^{+}$.\n1.  **Stationary Variance**: $\\operatorname{Var}(x) = \\frac{\\sigma^2}{2k(\\mu)}$. As $k(\\mu) \\to 0^{+}$, the variance diverges to infinity: $\\operatorname{Var}(x) \\to \\infty$. The scaling is $\\operatorname{Var}(x) \\propto k(\\mu)^{-1}$.\n2.  **Lag-$1$ Autocorrelation**: $\\rho_1 = e^{-k(\\mu)\\Delta t}$. As $k(\\mu) \\to 0^{+}$, the exponent $-k(\\mu)\\Delta t \\to 0$, so $\\rho_1 \\to e^0 = 1$.\n\nThis behavior provides clear early-warning signals (EWS) for an impending critical transition (the saddle-node bifurcation at $\\mu = \\mu_c$).\n-   The slowing of the system's dynamics as it approaches the bifurcation is termed **critical slowing down**. The rate of return to equilibrium, $k(\\mu)$, vanishes at the critical point.\n-   An increase in variance signifies a loss of **robustness**. The \"potential well\" around the stable equilibrium becomes shallower, so random perturbations (noise) cause much larger fluctuations in the system's state. The system becomes more fragile and less able to buffer against stochasticity.\n-   An increase in autocorrelation toward $1$ indicates that the system's state becomes more persistent over time. Because of critical slowing down, perturbations decay very slowly. This means successive measurements are highly correlated, reflecting the system's increasing \"memory\" of its past states.\n-   This loss of a fast, restorative timescale also leads to a loss of **modularity**. A module is functionally useful partly because its internal dynamics are fast relative to its interactions with other modules. As a module's characteristic timescale, proportional to $1/k(\\mu)$, blows up near a bifurcation, this timescale separation is lost. The module's slow dynamics can now couple strongly with other system components, breaking its functional and dynamical independence.\n\nThe numerical implementation will calculate these quantities for the given parameters, compute the scaling exponent of variance versus $k(\\mu)$ (which is theoretically $-1$), and verify that variance and autocorrelation increase as the system approaches the critical point.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and calculating early-warning signals\n    for a single-gene regulatory module near a saddle-node bifurcation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (mu, mu_c, k0, sigma, delta_t)\n    test_cases = [\n        (0.2, 1.0, 1.0, 0.5, 0.1),  # Case 1\n        (0.99, 1.0, 1.0, 0.5, 0.1), # Case 2\n        (0.0, 1.0, 1.0, 0.5, 0.1),  # Case 3\n        (0.99, 1.0, 1.0, 0.0, 0.1), # Case 4\n        (0.99, 1.0, 1.0, 0.5, 10.0),# Case 5\n    ]\n\n    all_results = []\n    case_data = {}  # Dictionary to store results for post-processing\n\n    # Process all five cases\n    for i, case in enumerate(test_cases, 1):\n        mu, mu_c, k0, sigma, delta_t = case\n        \n        # Calculate the restoring rate k(mu)\n        k_val = k0 * (mu_c - mu)\n\n        # Calculate stationary variance: Var = sigma^2 / (2*k)\n        # This holds for k > 0. If sigma is 0, Var is 0.\n        if k_val > 0:\n            variance = (sigma**2) / (2 * k_val)\n        else:\n            # Handle the case k_val <= 0. Not expected by problem constraints (mu < mu_c)\n            # but included for robustness.\n            variance = float('inf') if sigma != 0 else 0\n\n        # Calculate lag-1 autocorrelation: rho_1 = exp(-k * delta_t)\n        lag1_autocorr = np.exp(-k_val * delta_t)\n\n        all_results.extend([lag1_autocorr, variance])\n        \n        # Store results for later use in scaling and monotonicity checks\n        case_data[i] = {\n            'mu': mu,\n            'k': k_val,\n            'var': variance,\n            'rho1': lag1_autocorr\n        }\n\n    # Part 2: Compute the scaling exponent using Cases 1, 2, and 3\n    # The theoretical relationship is Var = C * k^(-1), so log(Var) = log(C) - 1*log(k).\n    # The slope of log(Var) vs log(k) should be -1.\n    k_vals_fit = [case_data[1]['k'], case_data[2]['k'], case_data[3]['k']]\n    var_vals_fit = [case_data[1]['var'], case_data[2]['var'], case_data[3]['var']]\n\n    log_k = np.log(k_vals_fit)\n    log_var = np.log(var_vals_fit)\n\n    # Perform a linear fit (degree 1 polynomial) to find the slope\n    slope, _ = np.polyfit(log_k, log_var, 1)\n    scaling_exponent = slope\n    all_results.append(scaling_exponent)\n\n    # Part 3: Compute the early-warning signal (EWS) boolean\n    # Check if both variance and lag-1 autocorrelation increase monotonically\n    # as mu approaches mu_c for Cases 1, 2, 3.\n    # Order cases by increasing mu: Case 3 (mu=0.0), Case 1 (mu=0.2), Case 2 (mu=0.99)\n    ordered_indices = sorted([1, 2, 3], key=lambda i: case_data[i]['mu'])\n    \n    variances_ordered = [case_data[i]['var'] for i in ordered_indices]\n    rhos_ordered = [case_data[i]['rho1'] for i in ordered_indices]\n    \n    # Check for strict monotonicity\n    var_is_monotonic = all(variances_ordered[j] < variances_ordered[j+1] for j in range(len(variances_ordered) - 1))\n    rho_is_monotonic = all(rhos_ordered[j] < rhos_ordered[j+1] for j in range(len(rhos_ordered) - 1))\n\n    ews_boolean = var_is_monotonic and rho_is_monotonic\n    all_results.append(ews_boolean)\n\n    # Final print statement in the exact required format.\n    # The boolean `True` will be converted to the string 'True'.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}