{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in mastering approximation methods is to test them in a context where an exact solution is known. This exercise explores the fundamental birth-death process, for which the discrete Chemical Master Equation (CME) can be solved exactly. By deriving the Linear Noise Approximation (LNA) from the Fokker-Planck equation and comparing its prediction for the stationary variance to the exact CME result, you will directly verify the accuracy of the continuous framework for systems with linear dynamics .",
            "id": "3310020",
            "problem": "Consider a single-species birth-death process in a well-mixed compartment whose size is parameterized by the system size $\\Omega$. Molecules of species $X$ are synthesized from the void at a zero-order macroscopic rate $k_b \\Omega$ and are degraded at a first-order microscopic rate $k_d$ per molecule. Let $n(t)$ denote the copy number of $X$ at time $t$, and define the concentration $x(t) = n(t)/\\Omega$. Assume $k_b > 0$, $k_d > 0$, and that the system admits a unique, stable macroscopic fixed point.\n\nStarting from the Chemical Master Equation (CME) and the Kramers-Moyal expansion leading to the Fokker-Planck approximation, perform the van Kampen system-size expansion to the Linear Noise Approximation (LNA). Derive the effective drift and diffusion about the macroscopic fixed point for the concentration fluctuations, and deduce the stationary variance predicted by the LNA for the copy number $n$. Then, compute the exact stationary distribution from the CME and use it to obtain the exact steady-state variance of $n$. Show that the LNA variance equals the exact CME variance. Finally, quantify the leading non-Gaussian corrections by deriving the third and fourth cumulants of the centered and rescaled fluctuation variable $\\xi = \\sqrt{\\Omega}\\big(x - \\bar{x}\\big)$ at stationarity, where $\\bar{x}$ is the macroscopic fixed point, and determine their scaling with $\\Omega$. Express these cumulants in terms of $k_b$, $k_d$, and $\\Omega$.\n\nProvide as your final answer the closed-form symbolic expression for the stationary variance of the copy number $n$ as a function of $k_b$, $k_d$, and $\\Omega$. No numerical values are required.",
            "solution": "The problem asks for a detailed analysis of a stochastic birth-death process using both the exact Chemical Master Equation (CME) and the Fokker-Planck approximation via a system-size expansion. We will first validate the problem statement according to the specified criteria, then proceed with the derivations if the problem is found to be valid.\n\n### Problem Validation\n**Step 1: Extract Givens**\n-   **System:** A single-species ($X$) birth-death process.\n-   **System size parameter:** $\\Omega$.\n-   **Reactions and Rates:**\n    1.  Birth: $\\emptyset \\xrightarrow{k_b \\Omega} X$ (zero-order macroscopic synthesis).\n    2.  Death: $X \\xrightarrow{k_d} \\emptyset$ (first-order microscopic degradation).\n-   **Variables:** $n(t)$ is the copy number of $X$; $x(t) = n(t)/\\Omega$ is the concentration.\n-   **Constants:** $k_b > 0$, $k_d > 0$.\n-   **Assumption:** The system possesses a unique, stable macroscopic fixed point.\n-   **Tasks:**\n    1.  Begin with the CME, use Kramers-Moyal expansion to get the Fokker-Planck equation.\n    2.  Apply the van Kampen system-size expansion to derive the Linear Noise Approximation (LNA).\n    3.  Find the LNA's effective drift and diffusion for concentration fluctuations.\n    4.  Calculate the stationary variance of the copy number, $\\text{Var}(n)$, from the LNA.\n    5.  Solve the CME for the exact stationary distribution $P_s(n)$.\n    6.  Calculate the exact stationary variance of $n$ from $P_s(n)$.\n    7.  Demonstrate that the LNA variance equals the exact CME variance.\n    8.  Derive the stationary third and fourth cumulants of the rescaled fluctuation variable $\\xi = \\sqrt{\\Omega}(x - \\bar{x})$, where $\\bar{x}$ is the macroscopic fixed point.\n    9.  Determine the scaling of these cumulants with $\\Omega$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes the canonical linear birth-death process, a cornerstone model in stochastic chemical kinetics and systems biology. All concepts invoked—CME, Fokker-Planck equation, van Kampen expansion, LNA, cumulants—are standard and well-established theoretical tools in this field. The setup is scientifically and mathematically sound. **(Valid)**\n-   **Well-Posed:** The problem is clearly defined with all necessary parameters ($k_b, k_d, \\Omega$) and reactions. The assumption of a unique, stable fixed point is consistent with the linear dynamics. The tasks are specific and lead to a unique, meaningful set of results. **(Valid)**\n-   **Objective:** The problem is stated in precise, formal, and unbiased technical language, free from any subjectivity or ambiguity. **(Valid)**\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed to the solution.\n\n### I. Macroscopic Dynamics and Fixed Point\n\nThe reactions are:\n$$\n\\emptyset \\xrightarrow{k_b \\Omega} X \\quad , \\quad X \\xrightarrow{k_d} \\emptyset\n$$\nThe corresponding propensity functions for the number of molecules $n$ are:\n-   Birth rate: $w_+(n) = k_b \\Omega$\n-   Death rate: $w_-(n) = k_d n$\n\nThe deterministic rate equation for the concentration $x = n/\\Omega$ is obtained by neglecting fluctuations:\n$$\n\\frac{dx}{dt} = \\frac{1}{\\Omega} \\left( w_+(\\Omega x) - w_-(\\Omega x) \\right) = \\frac{1}{\\Omega} (k_b \\Omega - k_d \\Omega x) = k_b - k_d x\n$$\nThe system has a macroscopic fixed point $\\bar{x}$ where $\\frac{dx}{dt} = 0$:\n$$\nk_b - k_d \\bar{x} = 0 \\implies \\bar{x} = \\frac{k_b}{k_d}\n$$\nThe stability of this fixed point is determined by the Jacobian of the rate equation, $J = \\frac{d}{dx}(k_b - k_d x) = -k_d$. Since $k_d > 0$, the eigenvalue is negative, confirming the fixed point is unique and stable as assumed. The corresponding mean copy number at the fixed point is $\\bar{n} = \\Omega \\bar{x} = \\frac{k_b \\Omega}{k_d}$.\n\n### II. Linear Noise Approximation (LNA) and Variance\n\nWe perform the van Kampen system-size expansion by splitting the concentration $x(t)$ into its macroscopic part $\\bar{x}$ and a fluctuation term $\\xi(t)$ scaled by $\\Omega^{-1/2}$:\n$$\nx(t) = \\bar{x} + \\frac{1}{\\sqrt{\\Omega}} \\xi(t) \\quad \\implies \\quad n(t) = \\Omega\\bar{x} + \\sqrt{\\Omega}\\xi(t)\n$$\nThe Fokker-Planck equation corresponding to the CME is given by $\\frac{\\partial p(x,t)}{\\partial t} = -\\frac{\\partial}{\\partial x}[A(x)p(x,t)] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[B(x)p(x,t)]$, where the drift $A(x)$ and diffusion $B(x)$ coefficients are:\n$$\nA(x) = \\frac{w_+(\\Omega x) - w_-(\\Omega x)}{\\Omega} = k_b - k_d x\n$$\n$$\nB(x) = \\frac{w_+(\\Omega x) + w_-(\\Omega x)}{\\Omega^2} = \\frac{k_b \\Omega + k_d \\Omega x}{\\Omega^2} = \\frac{k_b + k_d x}{\\Omega}\n$$\nWe now expand $A(x)$ and $B(x)$ around the fixed point $\\bar{x}$ using $x = \\bar{x} + \\Omega^{-1/2}\\xi$:\n$$\nA(x) = k_b - k_d(\\bar{x} + \\Omega^{-1/2}\\xi) = (k_b - k_d \\bar{x}) - k_d \\Omega^{-1/2}\\xi = -k_d \\Omega^{-1/2}\\xi\n$$\nFor the LNA, we only need the leading term of $B(x)$, evaluated at the fixed point:\n$$\nB(\\bar{x}) = \\frac{k_b + k_d \\bar{x}}{\\Omega} = \\frac{k_b + k_d(k_b/k_d)}{\\Omega} = \\frac{2k_b}{\\Omega}\n$$\nThe dynamics of the fluctuation variable $\\xi$ are described by a Langevin equation derived from these expansions:\n$$\n\\frac{d(\\Omega^{-1/2}\\xi)}{dt} = A(\\bar{x} + \\Omega^{-1/2}\\xi) + \\sqrt{B(\\bar{x} + \\Omega^{-1/2}\\xi)} \\eta'(t)\n$$\nwhere $\\eta'(t)$ is Gaussian white noise with $\\langle \\eta'(t)\\eta'(t') \\rangle = \\delta(t-t')$. To leading order:\n$$\n\\Omega^{-1/2}\\frac{d\\xi}{dt} \\approx -k_d \\Omega^{-1/2}\\xi + \\sqrt{\\frac{2k_b}{\\Omega}} \\eta'(t)\n$$\n$$\n\\frac{d\\xi}{dt} = -k_d\\xi + \\sqrt{2k_b}\\eta'(t)\n$$\nThis is an Ornstein-Uhlenbeck process, $d\\xi/dt = a\\xi + b\\eta'(t)$, with a linear drift coefficient $a = -k_d$ and a constant diffusion (noise strength) $b = \\sqrt{2k_b}$. The effective drift for $\\xi$ is $-k_d\\xi$ and the effective diffusion coefficient (in the FPE for $\\xi$) is $b^2 = 2k_b$.\nThe stationary variance of such a process is given by $\\text{Var}(\\xi) = -\\frac{b^2}{2a}$:\n$$\n\\text{Var}(\\xi) = -\\frac{(\\sqrt{2k_b})^2}{2(-k_d)} = \\frac{2k_b}{2k_d} = \\frac{k_b}{k_d}\n$$\nThe variance of the copy number $n$ is related to the variance of $\\xi$ by:\n$$\n\\text{Var}(n) = \\text{Var}(\\Omega\\bar{x} + \\sqrt{\\Omega}\\xi) = \\text{Var}(\\sqrt{\\Omega}\\xi) = \\Omega\\text{Var}(\\xi)\n$$\nThus, the variance predicted by the LNA is:\n$$\n\\text{Var}_{\\text{LNA}}(n) = \\Omega\\left(\\frac{k_b}{k_d}\\right) = \\frac{k_b\\Omega}{k_d}\n$$\n\n### III. Exact CME Solution and Variance\n\nThe Chemical Master Equation for the probability $P(n,t)$ of having $n$ molecules at time $t$ is:\n$$\n\\frac{dP(n,t)}{dt} = w_+(n-1)P(n-1,t) - w_+(n)P(n,t) + w_-(n+1)P(n+1,t) - w_-(n)P(n,t)\n$$\nAt steady state, $\\frac{dP(n,t)}{dt} = 0$. This implies a detailed balance condition for the probability flow:\n$$\nw_+(n-1)P_s(n-1) = w_-(n)P_s(n)\n$$\nSubstituting the propensity functions:\n$$\n(k_b\\Omega) P_s(n-1) = (k_d n) P_s(n)\n$$\nThis gives a recurrence relation for the stationary distribution $P_s(n)$:\n$$\nP_s(n) = \\frac{k_b\\Omega}{k_d n} P_s(n-1)\n$$\nBy iterating this relation, we find:\n$$\nP_s(n) = \\frac{k_b\\Omega}{k_d n} \\cdot \\frac{k_b\\Omega}{k_d(n-1)} \\cdots \\frac{k_b\\Omega}{k_d \\cdot 1} P_s(0) = \\frac{1}{n!} \\left(\\frac{k_b\\Omega}{k_d}\\right)^n P_s(0)\n$$\nThe normalization condition $\\sum_{n=0}^\\infty P_s(n) = 1$ allows us to determine $P_s(0)$:\n$$\nP_s(0) \\sum_{n=0}^\\infty \\frac{1}{n!} \\left(\\frac{k_b\\Omega}{k_d}\\right)^n = 1\n$$\nRecognizing the sum as the Taylor series for the exponential function, we have:\n$$\nP_s(0) \\exp\\left(\\frac{k_b\\Omega}{k_d}\\right) = 1 \\implies P_s(0) = \\exp\\left(-\\frac{k_b\\Omega}{k_d}\\right)\n$$\nSubstituting this back, we obtain the exact stationary distribution:\n$$\nP_s(n) = \\frac{(\\frac{k_b\\Omega}{k_d})^n}{n!} \\exp\\left(-\\frac{k_b\\Omega}{k_d}\\right)\n$$\nThis is a Poisson distribution with parameter $\\lambda = \\frac{k_b\\Omega}{k_d}$. For a Poisson distribution, both the mean and the variance are equal to the parameter $\\lambda$.\nTherefore, the exact stationary variance of the copy number $n$ is:\n$$\n\\text{Var}_{\\text{CME}}(n) = \\lambda = \\frac{k_b\\Omega}{k_d}\n$$\n\n### IV. Comparison of LNA and Exact Variance\n\nComparing the results from Section II and Section III:\n$$\n\\text{Var}_{\\text{LNA}}(n) = \\frac{k_b\\Omega}{k_d} \\quad \\text{and} \\quad \\text{Var}_{\\text{CME}}(n) = \\frac{k_b\\Omega}{k_d}\n$$\nWe see that $\\text{Var}_{\\text{LNA}}(n) = \\text{Var}_{\\text{CME}}(n)$. For systems with linear propensities, the LNA gives the exact variance.\n\n### V. Higher-Order Cumulants\n\nTo find the non-Gaussian corrections, we analyze the Fokker-Planck equation beyond the LNA. We need to include the fluctuation-dependence of the diffusion coefficient. The FPE for $\\xi$ is derived by a change of variables from the FPE for $x$. This leads to a moment hierarchy for $\\xi$. The stationary moments $\\langle\\xi^m\\rangle_s$ satisfy:\n$$\nm k_d \\langle \\xi^m \\rangle_s = \\frac{m(m-1)}{2} \\langle \\xi^{m-2} B_\\xi(\\xi) \\rangle_s\n$$\nwhere $B_\\xi(\\xi)$ is the diffusion coefficient in the FPE for $\\xi$. Transforming from $B(x)$:\n$B_\\xi(\\xi) = \\Omega B(x(\\xi)) = \\Omega \\frac{k_b+k_d(\\bar{x}+\\xi/\\sqrt{\\Omega})}{\\Omega} = k_b+k_d\\bar{x} + \\frac{k_d\\xi}{\\sqrt{\\Omega}} = 2k_b + \\frac{k_d\\xi}{\\sqrt{\\Omega}}$.\nThe moment hierarchy is:\n$$\nm k_d \\langle \\xi^m \\rangle_s = \\frac{m(m-1)}{2} \\left\\langle \\xi^{m-2} \\left(2k_b + \\frac{k_d\\xi}{\\sqrt{\\Omega}}\\right) \\right\\rangle_s\n$$\n$$\nk_d \\langle \\xi^m \\rangle_s = (m-1)k_b \\langle \\xi^{m-2} \\rangle_s + \\frac{(m-1)k_d}{2\\sqrt{\\Omega}} \\langle \\xi^{m-1} \\rangle_s\n$$\nWe solve this hierarchy for the stationary moments:\n-   For $m=1$: $k_d \\langle \\xi \\rangle_s = 0$. Since $k_d>0$, $\\langle \\xi \\rangle_s = 0$. This means $\\xi$ is a centered variable, as expected.\n-   For $m=2$: $k_d \\langle \\xi^2 \\rangle_s = (1)k_b \\langle \\xi^0 \\rangle_s + 0 = k_b$. So, $\\langle \\xi^2 \\rangle_s = \\frac{k_b}{k_d}$.\n-   For $m=3$: $k_d \\langle \\xi^3 \\rangle_s = (2)k_b \\langle \\xi \\rangle_s + \\frac{(2)k_d}{2\\sqrt{\\Omega}} \\langle \\xi^2 \\rangle_s = 0 + \\frac{k_d}{\\sqrt{\\Omega}}\\left(\\frac{k_b}{k_d}\\right) = \\frac{k_b}{\\sqrt{\\Omega}}$. So, $\\langle \\xi^3 \\rangle_s = \\frac{k_b}{k_d\\sqrt{\\Omega}}$.\n-   For $m=4$: $k_d \\langle \\xi^4 \\rangle_s = (3)k_b \\langle \\xi^2 \\rangle_s + \\frac{(3)k_d}{2\\sqrt{\\Omega}} \\langle \\xi^3 \\rangle_s = 3k_b\\left(\\frac{k_b}{k_d}\\right) + \\frac{3k_d}{2\\sqrt{\\Omega}}\\left(\\frac{k_b}{k_d\\sqrt{\\Omega}}\\right) = \\frac{3k_b^2}{k_d} + \\frac{3k_b}{2\\Omega}$. So, $\\langle \\xi^4 \\rangle_s = \\frac{3k_b^2}{k_d^2} + \\frac{3k_b}{2k_d\\Omega}$.\n\nThe cumulants $\\kappa_m$ of a centered variable are related to its moments.\n-   **Third Cumulant ($\\kappa_3$):** $\\kappa_3(\\xi) = \\langle\\xi^3\\rangle_s$.\n    $$\n    \\kappa_3(\\xi) = \\frac{k_b}{k_d\\sqrt{\\Omega}}\n    $$\n    This is the leading non-Gaussian correction, scaling as $\\Omega^{-1/2}$.\n\n-   **Fourth Cumulant ($\\kappa_4$):** $\\kappa_4(\\xi) = \\langle\\xi^4\\rangle_s - 3(\\langle\\xi^2\\rangle_s)^2$.\n    $$\n    \\kappa_4(\\xi) = \\left(\\frac{3k_b^2}{k_d^2} + \\frac{3k_b}{2k_d\\Omega}\\right) - 3\\left(\\frac{k_b}{k_d}\\right)^2 = \\frac{3k_b}{2k_d\\Omega}\n    $$\n    This correction scales as $\\Omega^{-1}$. Both $\\kappa_3$ and $\\kappa_4$ vanish in the macroscopic limit $\\Omega \\to \\infty$, where the distribution becomes Gaussian.\n\nThe final answer requested is the stationary variance of the copy number $n$.\n$$\n\\text{Var}(n) = \\frac{k_b\\Omega}{k_d}\n$$",
            "answer": "$$\n\\boxed{\\frac{k_b \\Omega}{k_d}}\n$$"
        },
        {
            "introduction": "The Fokker-Planck equation is the continuous representation of an underlying stochastic differential equation (SDE) governed by the rules of Itô calculus. This practice delves into a critical and often counter-intuitive feature of these rules: the emergence of a \"spurious drift\" when applying a nonlinear transformation to a process with multiplicative noise. Understanding this noise-induced drift is essential for correctly formulating and interpreting models, especially when working with logarithmic concentrations .",
            "id": "3310023",
            "problem": "In a mesoscopic model of intracellular biochemistry, let $x(t) \\in (0,\\infty)$ denote a strictly positive molecular abundance or concentration evolving under an Itô stochastic differential equation due to stochastic synthesis and degradation. Assume that $x(t)$ satisfies an Itô stochastic differential equation of the form $dx = a(x)\\,dt + b(x)\\,dW_t$, where $a(x)$ and $b(x)$ are sufficiently smooth functions on $(0,\\infty)$, $b(x) > 0$ for $x>0$, and $W_t$ is a standard Wiener process. Consider the logarithmic change of variables $y = \\ln x$.\n\nTasks:\n- Starting from the definitions of Itô stochastic calculus and the Fokker–Planck equation, derive the stochastic differential equation for $y(t)$ and explicitly identify its drift and diffusion coefficients in terms of $a(x)$, $b(x)$, and $x$.\n- Derive the Fokker–Planck equation for the probability density $p_{y}(y,t)$ associated with $y(t)$. Your derivation should be consistent with probability conservation under the change of variables $y = \\ln x$ and the assumed smoothness of $a(x)$ and $b(x)$.\n- Clearly identify the noise-induced drift (also called the spurious drift) that arises in the drift of the $y$-dynamics due solely to the multiplicative noise in $x$-space.\n\nAnswer specification:\n- As your final reported answer, provide a single closed-form analytic expression for the spurious drift term in the $y$-stochastic differential equation, written in terms of $x$ and $b(x)$ only. Do not include units. No rounding is required.",
            "solution": "The user wants me to solve a problem related to stochastic differential equations (SDEs) and the Fokker-Planck equation in the context of biochemical noise.\n\nHere's my plan:\n1.  **Problem Validation:** I will first validate the problem statement to ensure it is scientifically sound, well-posed, and objective.\n2.  **Derivation of SDE for y(t):** I will use Itô's lemma to derive the SDE for the transformed variable $y(t) = \\ln(x(t))$.\n3.  **Derivation of Fokker-Planck Equation:** I will write down the Fokker-Planck equation corresponding to the derived SDE for $y(t)$.\n4.  **Identification of Spurious Drift:** I will analyze the drift term of the new SDE to identify the component that arises as a correction term from Itô's calculus, which is the noise-induced or spurious drift.\n5.  **Final Answer Formulation:** I will provide the identified spurious drift term as the final answer, formatted according to the specified rules.\n\n### Step 1: Extract Givens\n- The state variable is $x(t) \\in (0,\\infty)$.\n- The evolution of $x(t)$ is described by the Itô stochastic differential equation: $dx = a(x)\\,dt + b(x)\\,dW_{t}$.\n- $a(x)$ and $b(x)$ are sufficiently smooth functions on $(0,\\infty)$.\n- $b(x) > 0$ for $x > 0$.\n- $W_{t}$ is a standard Wiener process.\n- The change of variables is $y = \\ln x$.\n- Tasks:\n    1. Derive the SDE for $y(t)$ and identify its drift and diffusion coefficients.\n    2. Derive the Fokker–Planck equation for the probability density $p_{y}(y,t)$.\n    3. Identify the noise-induced (spurious) drift in the $y$-dynamics.\n- Final Answer: Provide the expression for the spurious drift in terms of $x$ and $b(x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is reviewed against the validation criteria:\n- **Scientifically Grounded:** The problem is firmly rooted in the established mathematical framework of stochastic calculus (Itô SDEs) and statistical mechanics (Fokker-Planck equation). These are standard tools for modeling stochastic processes in physics, chemistry, and biology, particularly for mesoscopic systems with intrinsic noise. The setup is scientifically sound.\n- **Well-Posed:** All necessary information is provided. The functions $a(x)$ and $b(x)$ are defined as sufficiently smooth, which ensures the applicability of Itô's lemma. The domain $x(t) \\in (0,\\infty)$ is consistent with the logarithmic transformation $y = \\ln x$. The tasks are specific and lead to a unique, well-defined mathematical expression.\n- **Objective:** The problem is stated in precise, formal mathematical language, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, ambiguity).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with a full, reasoned solution.\n\n### Derivation of the SDE for $y(t)$\nWe are given the stochastic process $x(t)$ satisfying the Itô SDE:\n$$\ndx(t) = a(x) \\, dt + b(x) \\, dW_t\n$$\nWe perform a change of variables to $y(t) = f(x(t))$, where the transformation function is $f(x) = \\ln x$. To find the SDE for $y(t)$, we must use Itô's lemma. For a general twice-differentiable function $f(x)$, Itô's lemma states:\n$$\ndf(x) = f'(x) \\, dx + \\frac{1}{2} f''(x) \\, (dx)^2\n$$\nFor our specific transformation, $f(x) = \\ln x$, we compute the first and second derivatives with respect to $x$:\n$$\nf'(x) = \\frac{d}{dx}(\\ln x) = \\frac{1}{x}\n$$\n$$\nf''(x) = \\frac{d^2}{dx^2}(\\ln x) = -\\frac{1}{x^2}\n$$\nNext, we compute the term $(dx)^2$ using the Itô multiplication rules: $(dt)^2 = 0$, $dt \\, dW_t = 0$, and $(dW_t)^2 = dt$.\n$$\n(dx)^2 = (a(x) \\, dt + b(x) \\, dW_t)^2 = a(x)^2 \\, (dt)^2 + 2 a(x) b(x) \\, dt \\, dW_t + b(x)^2 \\, (dW_t)^2\n$$\nApplying the Itô rules, only the last term is non-zero:\n$$\n(dx)^2 = b(x)^2 \\, dt\n$$\nNow we substitute $f'(x)$, $f''(x)$, $dx$, and $(dx)^2$ into the formula for Itô's lemma:\n$$\ndy = d(\\ln x) = \\left(\\frac{1}{x}\\right) (a(x) \\, dt + b(x) \\, dW_t) + \\frac{1}{2} \\left(-\\frac{1}{x^2}\\right) (b(x)^2 \\, dt)\n$$\nWe group the terms proportional to $dt$ and $dW_t$:\n$$\ndy = \\left( \\frac{a(x)}{x} - \\frac{b(x)^2}{2x^2} \\right) dt + \\frac{b(x)}{x} \\, dW_t\n$$\nThis is the stochastic differential equation for $y(t)$. From this form, we can explicitly identify its drift and diffusion coefficients.\nThe drift coefficient for the $y$-dynamics is:\n$$\nA_y(x) = \\frac{a(x)}{x} - \\frac{b(x)^2}{2x^2}\n$$\nThe diffusion coefficient for the $y$-dynamics is:\n$$\nB_y(x) = \\frac{b(x)}{x}\n$$\nAlthough these are written as functions of $x$, they can be expressed as functions of $y$ by substituting $x=e^y$.\n\n### Derivation of the Fokker-Planck Equation for $p_y(y,t)$\nThe Fokker-Planck equation describes the time evolution of the probability density function $p(z,t)$ of a stochastic process $z(t)$ governed by the general SDE $dz = A(z) \\, dt + B(z) \\, dW_t$. The general form of the equation is:\n$$\n\\frac{\\partial p(z,t)}{\\partial t} = - \\frac{\\partial}{\\partial z} [A(z) p(z,t)] + \\frac{1}{2} \\frac{\\partial^2}{\\partial z^2} [B(z)^2 p(z,t)]\n$$\nFor our variable $y(t)$, the drift is $A_y(y) = \\frac{a(e^y)}{e^y} - \\frac{b(e^y)^2}{2(e^y)^2}$ and the diffusion is $B_y(y) = \\frac{b(e^y)}{e^y}$. Substituting these into the general Fokker-Planck equation, we get the equation for the probability density $p_y(y,t)$:\n$$\n\\frac{\\partial p_y(y,t)}{\\partial t} = - \\frac{\\partial}{\\partial y} \\left[ \\left( \\frac{a(e^y)}{e^y} - \\frac{b(e^y)^2}{2(e^y)^2} \\right) p_y(y,t) \\right] + \\frac{1}{2} \\frac{\\partial^2}{\\partial y^2} \\left[ \\left( \\frac{b(e^y)}{e^y} \\right)^2 p_y(y,t) \\right]\n$$\nThis derivation is consistent with probability conservation, as it directly follows from the SDE derived using Itô's lemma.\n\n### Identification of the Noise-Induced (Spurious) Drift\nThe drift coefficient for the $y$-dynamics is $A_y(x) = \\frac{a(x)}{x} - \\frac{b(x)^2}{2x^2}$. We can analyze the origin of each term.\nThe first term, $\\frac{a(x)}{x}$, is the drift that would be obtained by applying the rules of ordinary differential calculus to the deterministic part of the dynamics. If the noise term $b(x)$ were zero, we would have $dx/dt = a(x)$, and by the chain rule, $dy/dt = (dy/dx)(dx/dt) = (1/x)a(x)$. Thus, $\\frac{a(x)}{x}$ is the naively transformed drift.\nThe second term, $-\\frac{b(x)^2}{2x^2}$, arises entirely from the $\\frac{1}{2} f''(x) (dx)^2$ term in Itô's lemma. This term, known as the Itô correction, is non-zero because the transformation $f(x) = \\ln x$ is nonlinear (i.e., $f''(x) \\neq 0$) and the noise is multiplicative (i.e., $b(x)$ depends on $x$, and $b(x) > 0$). This correction term added to the drift of the transformed variable is precisely the noise-induced drift, also called the spurious drift. It represents the effect of the coupling between the noise and the curvature of the transformation.\nTherefore, the noise-induced drift in the SDE for $y(t)$ is given by the expression:\n$$\n\\text{Spurious Drift} = - \\frac{b(x)^2}{2x^2}\n$$\nThis expression depends only on the state variable $x$ and the diffusion coefficient $b(x)$ of the original process, as requested.",
            "answer": "$$\n\\boxed{-\\frac{b(x)^{2}}{2x^{2}}}\n$$"
        },
        {
            "introduction": "The ultimate utility of theoretical models lies in their ability to explain and predict real-world phenomena from experimental data. This practice bridges the gap between the Fokker-Planck formalism and practical data analysis. You will use the derived stationary solution of a Fokker-Planck equation as a likelihood model to perform parameter inference, estimating key biochemical rates from simulated snapshot measurements of a gene product's concentration .",
            "id": "3310085",
            "problem": "You are given a one-dimensional stochastic model for a gene product concentration, formulated as an Itô Stochastic Differential Equation (SDE) with additive noise. The concentration is non-dimensionalized so that all parameters and variables are dimensionless. The dynamics are specified by the SDE\n$$\n\\mathrm{d}x_t = a(x_t)\\,\\mathrm{d}t + \\sqrt{2D}\\,\\mathrm{d}W_t,\n$$\nwhere $x_t$ is the non-dimensional concentration at time $t$, $a(x)$ is the drift, $D > 0$ is the constant noise intensity, and $W_t$ is a standard Wiener process. In this model, the drift is given by a linear birth-death form\n$$\na(x) = k_s - \\gamma x,\n$$\nwith $k_s > 0$ the non-dimensional production rate and $\\gamma > 0$ the non-dimensional linear degradation rate.\n\nFrom the Itô SDE, the corresponding Fokker-Planck Equation (FPE) for the probability density $p(x,t)$ is\n$$\n\\frac{\\partial}{\\partial t}p(x,t)= -\\frac{\\partial}{\\partial x}\\big(a(x)\\,p(x,t)\\big) + D\\,\\frac{\\partial^2}{\\partial x^2}p(x,t),\n$$\nwhich is used as the foundational starting point for the derivations below.\n\nTask A (Derive stationary density from the Fokker-Planck Equation): Starting only from the given SDE and Fokker-Planck Equation, derive the stationary probability density $p^\\star(x)$ under suitable boundary conditions in the sense of vanishing probability current at $\\lvert x\\rvert\\to\\infty$. Do not assume or quote the stationary solution; derive it from first principles by setting $\\partial_t p(x,t)=0$ and requiring physical normalizability. Express the final stationary density in closed form and identify its location and scale (mean and variance) in terms of $k_s$, $\\gamma$, and $D$.\n\nTask B (Parameter inference via Maximum Likelihood Estimation): Suppose $\\gamma$ is known and fixed, but $k_s$ and $D$ are unknown. You observe $N$ independent and identically distributed snapshot measurements $\\{x_i\\}_{i=1}^N$ drawn from the stationary density $p^\\star(x)$ derived in Task A. Using Maximum Likelihood Estimation (MLE), derive explicit estimators $\\widehat{k_s}$ and $\\widehat{D}$ in terms of the sample mean and sample variance computed from $\\{x_i\\}$. Define all steps clearly and base your derivation on the log-likelihood of the stationary density.\n\nTask C (Implement and evaluate on a test suite): Implement a program that\n- uses the stationary density $p^\\star(x)$ derived in Task A for data generation,\n- for each test case, generates $N$ synthetic snapshots with a fixed random seed, then computes $\\widehat{k_s}$ and $\\widehat{D}$ via the MLE derived in Task B,\n- outputs the estimates for each test case as floats, rounded to six decimal places.\n\nAll variables and parameters are dimensionless by construction, so no physical units are required in your output.\n\nTest Suite:\n- Case $1$: $\\gamma=1.0$, $k_s=10.0$, $D=2.0$, $N=500$, seed $=42$.\n- Case $2$: $\\gamma=1.5$, $k_s=3.0$, $D=0.1$, $N=1000$, seed $=123$.\n- Case $3$: $\\gamma=0.8$, $k_s=8.0$, $D=4.0$, $N=30$, seed $=7$.\n\nCoverage objectives:\n- Case $1$ is a general case with moderate variance and ample data,\n- Case $2$ probes a near-deterministic regime with small noise intensity,\n- Case $3$ stresses small-sample behavior with relatively large variance.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[\\widehat{k_s},\\widehat{D}]$ where each float is rounded to six decimal places. The three case results must appear in the order $1,2,3$. For example, your final output line should look like\n$$\n[[\\widehat{k_s}^{(1)},\\widehat{D}^{(1)}],[\\widehat{k_s}^{(2)},\\widehat{D}^{(2)}],[\\widehat{k_s}^{(3)},\\widehat{D}^{(3)}]]\n$$\nprinted as a single line with no additional text.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the established theory of stochastic processes, specifically the Ornstein-Uhlenbeck process, and its application to biochemical reaction kinetics via the Fokker-Planck formalism. The problem is well-posed, with all necessary information provided and no internal contradictions. It presents a standard, non-trivial exercise in theoretical derivation and computational implementation, relevant to the field of computational systems biology. We may therefore proceed with a full solution.\n\nThe problem is partitioned into three tasks: A) derivation of the stationary probability density, B) derivation of maximum likelihood estimators for model parameters, and C) implementation and evaluation of these estimators on synthetic data. We will address each in sequence.\n\n**Task A: Derivation of the Stationary Probability Density**\n\nThe starting point is the Fokker-Planck Equation (FPE) for the probability density $p(x,t)$ of the concentration $x$:\n$$\n\\frac{\\partial}{\\partial t}p(x,t) = -\\frac{\\partial}{\\partial x}J(x,t)\n$$\nwhere $J(x,t)$ is the probability current, given by:\n$$\nJ(x,t) = a(x)p(x,t) - D\\frac{\\partial}{\\partial x}p(x,t)\n$$\nThe drift term is specified as $a(x) = k_s - \\gamma x$.\n\nTo find the stationary probability density, denoted $p^\\star(x)$, we seek a solution at steady state, where the probability density no longer changes with time. This is achieved by setting $\\frac{\\partial p}{\\partial t} = 0$. This condition implies that the stationary probability current, $J^\\star(x)$, must be constant with respect to $x$:\n$$\n\\frac{\\partial J^\\star(x)}{\\partial x} = 0 \\implies J^\\star(x) = \\text{constant}\n$$\nThe problem specifies a physically meaningful boundary condition: the probability current must vanish at infinity, i.e., $\\lim_{|x|\\to\\infty} J^\\star(x) = 0$. This requires the constant to be zero, leading to the condition $J^\\star(x) = 0$ for all $x$.\n\nSetting the stationary current to zero yields a first-order ordinary differential equation for $p^\\star(x)$:\n$$\na(x)p^\\star(x) - D\\frac{\\mathrm{d}}{\\mathrm{d}x}p^\\star(x) = 0\n$$\nSubstituting the given drift term $a(x) = k_s - \\gamma x$:\n$$\n(k_s - \\gamma x)p^\\star(x) = D\\frac{\\mathrm{d}p^\\star(x)}{\\mathrm{d}x}\n$$\nThis is a separable differential equation. We can rearrange it as:\n$$\n\\frac{\\mathrm{d}p^\\star}{p^\\star} = \\frac{k_s - \\gamma x}{D} \\mathrm{d}x\n$$\nIntegrating both sides from some reference point to $x$:\n$$\n\\int \\frac{\\mathrm{d}p^\\star}{p^\\star} = \\int \\frac{k_s - \\gamma x}{D} \\mathrm{d}x\n$$\n$$\n\\ln p^\\star(x) = \\frac{1}{D} \\left( k_s x - \\frac{\\gamma x^2}{2} \\right) + C_0\n$$\nwhere $C_0$ is the constant of integration. Exponentiating both sides gives:\n$$\np^\\star(x) = C_1 \\exp\\left( \\frac{k_s x}{D} - \\frac{\\gamma x^2}{2D} \\right)\n$$\nwhere $C_1 = e^{C_0}$ is a normalization constant. To identify the form of this distribution, we complete the square in the exponent:\n$$\n-\\frac{\\gamma}{2D}\\left(x^2 - \\frac{2k_s}{\\gamma}x\\right) = -\\frac{\\gamma}{2D}\\left( \\left(x - \\frac{k_s}{\\gamma}\\right)^2 - \\left(\\frac{k_s}{\\gamma}\\right)^2 \\right) = -\\frac{\\left(x - k_s/\\gamma\\right)^2}{2D/\\gamma} + \\frac{k_s^2}{2D\\gamma}\n$$\nSubstituting this back, we get:\n$$\np^\\star(x) = C_1 \\exp\\left(\\frac{k_s^2}{2D\\gamma}\\right) \\exp\\left( -\\frac{\\left(x - k_s/\\gamma\\right)^2}{2(D/\\gamma)} \\right)\n$$\nLet's define a new normalization constant $C = C_1 \\exp\\left(\\frac{k_s^2}{2D\\gamma}\\right)$. The density is then:\n$$\np^\\star(x) = C \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n$$\nThis is immediately recognizable as the probability density function of a Gaussian (Normal) distribution, $N(\\mu, \\sigma^2)$, with location (mean) $\\mu$ and scale (variance) $\\sigma^2$ given by:\n$$\n\\mu = \\frac{k_s}{\\gamma}\n$$\n$$\n\\sigma^2 = \\frac{D}{\\gamma}\n$$\nThe normalization constant $C$ is determined by the condition $\\int_{-\\infty}^{\\infty} p^\\star(x) \\mathrm{d}x = 1$, which for a Gaussian distribution requires $C = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}$. Thus, the stationary density is:\n$$\np^\\star(x) = \\frac{1}{\\sqrt{2\\pi D/\\gamma}} \\exp\\left( -\\frac{(x - k_s/\\gamma)^2}{2(D/\\gamma)} \\right)\n$$\nThe mean of the stationary distribution is $\\mathbb{E}[x] = k_s/\\gamma$ and the variance is $\\text{Var}(x) = D/\\gamma$.\n\n**Task B: Parameter Inference via Maximum Likelihood Estimation (MLE)**\n\nWe are given $N$ independent and identically distributed (i.i.d.) measurements $\\{x_i\\}_{i=1}^N$ drawn from the stationary distribution $p^\\star(x) \\sim N(k_s/\\gamma, D/\\gamma)$. The parameter $\\gamma$ is known, while $k_s$ and $D$ are to be estimated.\n\nThe likelihood function $\\mathcal{L}(k_s, D)$ for the observed data is the product of the probabilities of each observation:\n$$\n\\mathcal{L}(k_s, D | \\{x_i\\}, \\gamma) = \\prod_{i=1}^N p^\\star(x_i | k_s, D, \\gamma) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi D/\\gamma}} \\exp\\left( -\\frac{\\gamma(x_i - k_s/\\gamma)^2}{2D} \\right)\n$$\nMaximizing the likelihood is equivalent to maximizing the log-likelihood function, $\\ell = \\ln \\mathcal{L}$.\n$$\n\\ell(k_s, D) = \\sum_{i=1}^N \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(D/\\gamma) - \\frac{\\gamma(x_i - k_s/\\gamma)^2}{2D} \\right)\n$$\n$$\n\\ell(k_s, D) = -\\frac{N}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln(D) + \\frac{N}{2}\\ln(\\gamma) - \\frac{\\gamma}{2D}\\sum_{i=1}^N(x_i - k_s/\\gamma)^2\n$$\nTo find the Maximum Likelihood Estimators (MLEs) $\\widehat{k_s}$ and $\\widehat{D}$, we compute the partial derivatives of $\\ell$ with respect to $k_s$ and $D$ and set them to zero.\n\nFor $k_s$:\n$$\n\\frac{\\partial \\ell}{\\partial k_s} = -\\frac{\\gamma}{2D} \\sum_{i=1}^N 2(x_i - k_s/\\gamma)\\left(-\\frac{1}{\\gamma}\\right) = \\frac{1}{D}\\sum_{i=1}^N (x_i - k_s/\\gamma) = 0\n$$\nSince $D>0$, we must have $\\sum_{i=1}^N (x_i - k_s/\\gamma)=0$. This leads to:\n$$\n\\sum_{i=1}^N x_i = N\\frac{k_s}{\\gamma} \\implies \\frac{k_s}{\\gamma} = \\frac{1}{N}\\sum_{i=1}^N x_i = \\bar{x}\n$$\nHere, $\\bar{x}$ is the sample mean. The estimator for $k_s$ is therefore:\n$$\n\\widehat{k_s} = \\gamma \\bar{x}\n$$\n\nFor $D$:\n$$\n\\frac{\\partial \\ell}{\\partial D} = -\\frac{N}{2D} + \\frac{\\gamma}{2D^2}\\sum_{i=1}^N(x_i - k_s/\\gamma)^2 = 0\n$$\nMultiplying by $2D^2$ gives:\n$$\n-ND + \\gamma\\sum_{i=1}^N(x_i - k_s/\\gamma)^2 = 0 \\implies D = \\frac{\\gamma}{N}\\sum_{i=1}^N(x_i - k_s/\\gamma)^2\n$$\nTo find the MLE $\\widehat{D}$, we substitute the MLE for $k_s/\\gamma$, which is $\\bar{x}$:\n$$\n\\widehat{D} = \\frac{\\gamma}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2\n$$\nThis expression is $\\gamma$ times the sample variance, $s^2 = \\frac{1}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2$. Thus, the estimator for $D$ is:\n$$\n\\widehat{D} = \\gamma s^2\n$$\n\n**Task C: Implementation and Evaluation**\n\nThe estimators derived in Task B, $\\widehat{k_s} = \\gamma \\bar{x}$ and $\\widehat{D} = \\gamma s^2$, form the basis of the computational task. The implementation strategy is as follows:\n1.  For each test case $(\\gamma, k_s, D, N, \\text{seed})$, we first determine the parameters of the underlying stationary Normal distribution: mean $\\mu = k_s/\\gamma$ and standard deviation $\\sigma = \\sqrt{D/\\gamma}$.\n2.  Using a pseudorandom number generator initialized with the specified seed for reproducibility, we draw $N$ samples from this Normal distribution, $N(\\mu, \\sigma^2)$.\n3.  From this synthetic dataset, we compute the sample mean $\\bar{x}$ and the sample variance $s^2$ (using $N$ in the denominator, as prescribed by the MLE derivation).\n4.  We then apply the derived estimator formulas to calculate $\\widehat{k_s}$ and $\\widehat{D}$.\n5.  Finally, the results for each case are rounded to six decimal places and formatted into the required output string. This procedure directly operationalizes the theoretical results from Tasks A and B.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating synthetic data and applying MLE estimators.\n    \n    For each test case, the function performs the following steps:\n    1. Sets up a random number generator with a specific seed for reproducibility.\n    2. Calculates the theoretical mean (mu) and standard deviation (sigma) of the\n       stationary Normal distribution from the given parameters (k_s, gamma, D).\n    3. Generates N synthetic data points by sampling from this Normal distribution.\n    4. Computes the sample mean and sample variance of the generated data.\n    5. Calculates the Maximum Likelihood Estimates (MLEs) for k_s and D using\n       the derived formulas:\n          k_s_hat = gamma * sample_mean\n          D_hat = gamma * sample_variance\n    6. Formats the estimated parameters to six decimal places.\n    7. Collects the results and prints them in the specified single-line format.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple contains: (gamma, k_s, D, N, seed)\n    test_cases = [\n        (1.0, 10.0, 2.0, 500, 42),\n        (1.5, 3.0, 0.1, 1000, 123),\n        (0.8, 8.0, 4.0, 30, 7),\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        gamma, k_s, D, N, seed = case\n        \n        # 1. Initialize the random number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n        \n        # 2. Define the parameters of the stationary Normal distribution.\n        # Mean mu = k_s / gamma\n        # Variance sigma^2 = D / gamma\n        mu = k_s / gamma\n        sigma = np.sqrt(D / gamma)\n        \n        # 3. Generate N synthetic data points (snapshots) from the stationary distribution.\n        samples = rng.normal(loc=mu, scale=sigma, size=N)\n        \n        # 4. Compute sample statistics from the generated data.\n        sample_mean = np.mean(samples)\n        # np.var() uses the N denominator by default, which is correct for the MLE.\n        sample_variance = np.var(samples)\n        \n        # 5. Compute the MLE estimates for k_s and D using the derived formulas.\n        k_s_hat = gamma * sample_mean\n        D_hat = gamma * sample_variance\n        \n        # 6. Format the result for the current case as a string \"[k_s_hat, D_hat]\".\n        # The :.6f format specifier rounds the float to 6 decimal places.\n        results_as_strings.append(f\"[{k_s_hat:.6f},{D_hat:.6f}]\")\n\n    # 7. Join the results for all test cases and print in the final required format.\n    # Example format: [[k_s1,D1],[k_s2,D2],[k_s3,D3]]\n    final_output_string = f\"[{','.join(results_as_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}