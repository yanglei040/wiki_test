{
    "hands_on_practices": [
        {
            "introduction": "To truly understand RNA velocity, we must first build the engine. This exercise guides you through implementing a complete, albeit simplified, velocity analysis pipeline from first principles. By processing raw counts through normalization, statistical modeling, and dimensionality reduction, you will gain a concrete understanding of how a discrete set of molecular counts is transformed into a continuous vector field representing cellular dynamics .",
            "id": "3346376",
            "problem": "You are given a small collection of single-cell spliced and unspliced messenger ribonucleic acid (mRNA) count matrices and asked to implement a complete, reproducible preprocessing and normalization pipeline appropriate for downstream ribonucleic acid (RNA) velocity analysis and vector field estimation. The pipeline must be built from first principles and must include the following steps: (i) cell-wise library-size normalization with a target library size, (ii) pseudocount addition to stabilize ratios and logarithms, (iii) logarithmic transformation, (iv) gene-wise standardization, (v) per-gene linear regression of unspliced on spliced to obtain residual-based velocity estimates, (vi) projection of gene-wise velocities into a two-dimensional embedding derived from principal component analysis (PCA) on the spliced data, and (vii) kernel-based smoothing of the projected velocities into a continuous vector field evaluated at a specified query point.\n\nFundamental base and definitions:\n- The Central Dogma of molecular biology specifies that Deoxyribonucleic Acid (DNA) is transcribed into RNA, which is spliced and processed to yield mature (spliced) transcripts; the observed single-cell counts of spliced and unspliced RNA per gene arise from this process and are subject to technical sampling variation.\n- We assume that each cell $i$ has observed nonnegative integer spliced counts $S_{i g}$ and unspliced counts $U_{i g}$ for each gene $g$. The cell-wise library size is defined as $L_i = \\sum_{g} \\left(S_{i g} + U_{i g}\\right)$.\n- The aim of library-size normalization is to rescale each cell so that total molecular content is on a comparable scale across cells, which is critical before fitting models of transcriptional dynamics across the population. The logarithmic transformation stabilizes variance, and gene-wise standardization ensures comparability of spliced and unspliced signals in linear modeling.\n- Residual-based RNA velocity uses a linear model for each gene relating unspliced to spliced signals, and estimates velocity as the deviation from the fitted steady-state relationship.\n\nRequired computations to implement:\n1. Cell-wise library-size normalization with pseudocount addition:\n   - For each cell $i$, compute $L_i = \\sum_{g} \\left(S_{i g} + U_{i g}\\right)$.\n   - Given a target library size $L_0$ and a small positive floor $\\varepsilon_L$ to avoid division by zero, define the size factor $s_i = \\max(L_i,\\varepsilon_L)/L_0$.\n   - Given a pseudocount $p  0$, define pseudocount-adjusted and normalized counts\n     $$\n     \\tilde{S}_{i g} = \\frac{S_{i g} + p}{s_i}, \\quad \\tilde{U}_{i g} = \\frac{U_{i g} + p}{s_i}.\n     $$\n2. Logarithmic transformation:\n   - Define log-transformed quantities\n     $$\n     \\hat{S}_{i g} = \\log\\!\\left(1 + \\tilde{S}_{i g}\\right), \\quad \\hat{U}_{i g} = \\log\\!\\left(1 + \\tilde{U}_{i g}\\right).\n     $$\n3. Gene-wise standardization across cells:\n   - For each gene $g$, compute the means across cells $m^S_g = \\frac{1}{n}\\sum_i \\hat{S}_{i g}$ and $m^U_g = \\frac{1}{n}\\sum_i \\hat{U}_{i g}$, and the population standard deviations\n     $$\n     \\sigma^S_g = \\sqrt{\\frac{1}{n}\\sum_i \\left(\\hat{S}_{i g} - m^S_g\\right)^2}, \\quad \\sigma^U_g = \\sqrt{\\frac{1}{n}\\sum_i \\left(\\hat{U}_{i g} - m^U_g\\right)^2}.\n     $$\n   - With a small numerical stabilizer $\\delta  0$, define standardized quantities\n     $$\n     \\bar{S}_{i g} = \\frac{\\hat{S}_{i g} - m^S_g}{\\max(\\sigma^S_g,\\delta)}, \\quad \\bar{U}_{i g} = \\frac{\\hat{U}_{i g} - m^U_g}{\\max(\\sigma^U_g,\\delta)}.\n     $$\n4. Linear regression per gene and residual-based velocities:\n   - For each gene $g$, fit a univariate linear model with intercept relating $\\bar{U}_{i g}$ to $\\bar{S}_{i g}$ across cells $i$:\n     $$\n     \\bar{U}_{i g} = \\alpha_g \\bar{S}_{i g} + \\beta_g + \\varepsilon_{i g}.\n     $$\n   - Let $x_i = \\bar{S}_{i g}$, $y_i = \\bar{U}_{i g}$, $\\mu_x = \\frac{1}{n}\\sum_i x_i$, $\\mu_y = \\frac{1}{n}\\sum_i y_i$, $\\mathrm{var}(x)=\\frac{1}{n}\\sum_i (x_i-\\mu_x)^2$, $\\mathrm{cov}(x,y)=\\frac{1}{n}\\sum_i (x_i-\\mu_x)(y_i-\\mu_y)$. If $\\mathrm{var}(x) \\ge \\delta$, set $\\alpha_g = \\mathrm{cov}(x,y)/\\mathrm{var}(x)$ and $\\beta_g = \\mu_y - \\alpha_g \\mu_x$; otherwise set $\\alpha_g = 0$ and $\\beta_g = \\mu_y$. Define the residual-based velocity per cell and gene as\n     $$\n     v_{i g} = y_i - \\left(\\alpha_g x_i + \\beta_g\\right).\n     $$\n5. Two-dimensional embedding by principal component analysis:\n   - Let $Z$ be the matrix with entries $Z_{i g} = \\bar{S}_{i g}$. Compute the singular value decomposition $Z = U \\Sigma V^\\top$. Take the first $2$ columns of $V$ to obtain the gene loadings matrix $W \\in \\mathbb{R}^{G \\times 2}$.\n   - Define the two-dimensional embedding of cells as\n     $$\n     X_i = \\sum_{g} Z_{i g} W_{g, :} \\in \\mathbb{R}^2,\n     $$\n     and the projected velocity vectors as\n     $$\n     \\mathbf{v}^{\\mathrm{emb}}_i = \\sum_{g} v_{i g} W_{g, :} \\in \\mathbb{R}^2.\n     $$\n6. Kernel smoothing of the vector field at a query point:\n   - Given a query point $\\mathbf{q} \\in \\mathbb{R}^2$, a bandwidth $h  0$, and a Gaussian kernel with weights\n     $$\n     w_i = \\exp\\!\\left(-\\frac{\\lVert X_i - \\mathbf{q}\\rVert^2}{2 h^2}\\right),\n     $$\n     define the smoothed vector field at $\\mathbf{q}$ as\n     $$\n     \\hat{\\mathbf{V}}(\\mathbf{q}) = \\frac{\\sum_i w_i \\, \\mathbf{v}^{\\mathrm{emb}}_i}{\\sum_i w_i}.\n     $$\n\nEdge cases to handle:\n- If $L_i = 0$ for a cell, enforce the floor with $\\varepsilon_L$.\n- If $\\sigma^S_g$ or $\\sigma^U_g$ is zero, use the stabilizer $\\delta$ in the denominator for standardization.\n- If $\\mathrm{var}(x)  \\delta$ in regression, set $\\alpha_g = 0$ and $\\beta_g = \\mu_y$ to avoid division by near-zero.\n\nTest suite:\nFor each test case, you are given spliced counts $S$, unspliced counts $U$, and parameters $(L_0, p, \\varepsilon_L, \\delta, h, \\mathbf{q})$. Implement all steps $1$–$6$ above and return the smoothed vector field $\\hat{\\mathbf{V}}(\\mathbf{q})$ as a list of two floating-point numbers rounded to six decimal places.\n\n- Test case $1$ (happy path, moderate counts):\n  - $S^{(1)} = \\begin{bmatrix} 10  5  0 \\\\ 3  8  1 \\\\ 0  2  9 \\\\ 6  4  3 \\end{bmatrix}$, $U^{(1)} = \\begin{bmatrix} 4  2  0 \\\\ 1  3  0 \\\\ 0  1  5 \\\\ 2  1  1 \\end{bmatrix}$.\n  - Parameters: $L_0 = 1000.0$, $p = 1.0$, $\\varepsilon_L = 1.0$, $\\delta = 10^{-8}$, $h = 1.0$, $\\mathbf{q} = (0.0, 0.0)$.\n- Test case $2$ (boundary condition with zero-count cell and sparse genes):\n  - $S^{(2)} = \\begin{bmatrix} 0  0  0 \\\\ 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{bmatrix}$, $U^{(2)} = \\begin{bmatrix} 0  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\\\ 1  0  0 \\end{bmatrix}$.\n  - Parameters: $L_0 = 1000.0$, $p = 0.5$, $\\varepsilon_L = 1.0$, $\\delta = 10^{-8}$, $h = 0.5$, $\\mathbf{q} = (0.0, 0.0)$.\n- Test case $3$ (edge case with highly uneven library sizes and zero-variance genes):\n  - $S^{(3)} = \\begin{bmatrix} 1000  50  7 \\\\ 50  50  7 \\\\ 3000  50  7 \\\\ 10  50  7 \\end{bmatrix}$, $U^{(3)} = \\begin{bmatrix} 200  10  3 \\\\ 10  10  3 \\\\ 600  10  3 \\\\ 2  10  3 \\end{bmatrix}$.\n  - Parameters: $L_0 = 10000.0$, $p = 1.0$, $\\varepsilon_L = 1.0$, $\\delta = 10^{-8}$, $h = 2.0$, $\\mathbf{q} = (0.0, 0.0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list corresponding to $\\hat{\\mathbf{V}}(\\mathbf{q})$ rounded to six decimal places. For example: $[\\,[v^{(1)}_x, v^{(1)}_y], [v^{(2)}_x, v^{(2)}_y], [v^{(3)}_x, v^{(3)}_y]\\,]$.",
            "solution": "The problem presents a well-defined and scientifically grounded task from the field of computational systems biology: to implement a simplified RNA velocity analysis pipeline. The problem is valid as it provides a complete, self-contained, and logically consistent set of instructions, definitions, and test data. It is free from scientific inaccuracies, ambiguities, or contradictions. The steps described are standard, albeit simplified, procedures in single-cell data analysis, encompassing normalization, transformation, statistical modeling, dimensionality reduction, and vector field estimation. All required mathematical formulas, parameters, and edge-case handling rules are explicitly stated, making the problem well-posed and enabling the computation of a unique, verifiable solution.\n\nThe solution proceeds by systematically implementing each of the six specified computational steps. The implementation is designed to be a direct translation of the provided mathematical formalism into executable code.\n\n1.  **Cell-wise Library-Size Normalization**: For each cell, the total RNA counts (spliced plus unspliced) are computed to determine its library size, $L_i$. A size factor, $s_i$, is then calculated by scaling this library size relative to a target library size, $L_0$, while enforcing a minimum floor, $\\varepsilon_L$, to prevent division by zero for cells with no counts. The raw spliced ($S_{ig}$) and unspliced ($U_{ig}$) counts for each gene $g$ in cell $i$ are first augmented by a pseudocount, $p$, and then divided by the cell's size factor, $s_i$. This yields normalized counts $\\tilde{S}_{ig}$ and $\\tilde{U}_{ig}$, which are adjusted for sequencing depth and cell size variations.\n\n2.  **Logarithmic Transformation**: The normalized counts, $\\tilde{S}_{ig}$ and $\\tilde{U}_{ig}$, are log-transformed using the function $\\log(1+x)$. This common technique, implemented as `np.log1p` for numerical stability, serves to stabilize the variance across the range of expression values and make the data distribution more symmetric, which is beneficial for downstream linear modeling. The resulting quantities are denoted $\\hat{S}_{ig}$ and $\\hat{U}_{ig}$.\n\n3.  **Gene-wise Standardization**: To make expression values comparable across different genes, which may have vastly different dynamic ranges, each gene's log-transformed counts are standardized. For each gene $g$, the mean ($m^S_g, m^U_g$) and population standard deviation ($\\sigma^S_g, \\sigma^U_g$) are computed across all cells. The standardized values, $\\bar{S}_{ig}$ and $\\bar{U}_{ig}$, are obtained by subtracting the mean and dividing by the standard deviation. A small stabilizer, $\\delta$, is used in the denominator to prevent division by zero for genes with no variance in their expression across the cell population.\n\n4.  **Linear Regression and Residual-based Velocities**: The core of RNA velocity estimation in this model lies in relating the unspliced and spliced mRNA abundance. For each gene $g$, a simple linear regression model, $\\bar{U}_{ig} = \\alpha_g \\bar{S}_{ig} + \\beta_g$, is fitted to predict the standardized unspliced abundance from the standardized spliced abundance. The slope $\\alpha_g$ and intercept $\\beta_g$ are computed using the ordinary least squares formulas. An edge case is handled for genes with near-zero variance in spliced counts, where the slope is set to zero. The RNA velocity, $v_{ig}$, for each cell $i$ and gene $g$ is defined as the residual from this regression—the difference between the observed standardized unspliced abundance, $\\bar{U}_{ig}$, and its predicted value from the model. This residual represents the deviation from the expected steady-state relationship between splicing and degradation, indicating whether a gene's expression is increasing or decreasing.\n\n5.  **PCA Embedding and Velocity Projection**: To visualize the developmental trajectories, cells are often embedded in a low-dimensional space. Here, Principal Component Analysis (PCA) is performed on the standardized spliced data matrix, $Z_{ig} = \\bar{S}_{ig}$. The matrix of gene loadings corresponding to the first two principal components, denoted $W$, is extracted from the singular value decomposition (SVD) of $Z$. The two-dimensional cell embedding, $X_i$, is computed by projecting the standardized spliced data onto these loadings ($X = Z W$). Similarly, the high-dimensional gene-wise velocity vectors, $v_{ig}$, are projected into the same two-dimensional space to yield embedded velocity vectors, $\\mathbf{v}^{\\mathrm{emb}}_i$.\n\n6.  **Kernel Smoothing**: The final step is to estimate a continuous vector field from the discrete velocity vectors of individual cells. Given a query point $\\mathbf{q}$ in the two-dimensional embedding, a Gaussian kernel is used to compute a weighted average of the embedded velocities, $\\mathbf{v}^{\\mathrm{emb}}_i$. Cells closer to the query point in the embedding space receive higher weights. This smoothing process provides a robust estimate, $\\hat{\\mathbf{V}}(\\mathbf{q})$, of the local velocity vector, representing the likely direction and magnitude of cellular state change at that position in the developmental landscape.\n\nEach of these steps is implemented using the `numpy` library for efficient array-based computation, carefully following the provided formulas and handling all specified edge conditions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_rna_velocity_pipeline(S, U, params):\n    \"\"\"\n    Implements the complete RNA velocity pipeline for a single dataset.\n    \n    Args:\n        S (np.ndarray): Spliced counts matrix (cells x genes).\n        U (np.ndarray): Unspliced counts matrix (cells x genes).\n        params (tuple): A tuple of parameters (L0, p, eps_L, delta, h, q).\n\n    Returns:\n        list: The smoothed 2D vector [vx, vy], rounded to 6 decimal places.\n    \"\"\"\n    L0, p, eps_L, delta, h, q_tuple = params\n    S = np.array(S, dtype=float)\n    U = np.array(U, dtype=float)\n    q = np.array(q_tuple, dtype=float)\n    \n    n_cells, n_genes = S.shape\n\n    # Step 1: Cell-wise library-size normalization\n    L = S.sum(axis=1) + U.sum(axis=1)\n    s = np.maximum(L, eps_L) / L0\n    S_tilde = (S + p) / s[:, np.newaxis]\n    U_tilde = (U + p) / s[:, np.newaxis]\n\n    # Step 2: Logarithmic transformation\n    S_hat = np.log1p(S_tilde)\n    U_hat = np.log1p(U_tilde)\n    \n    # Step 3: Gene-wise standardization\n    m_S = S_hat.mean(axis=0)\n    m_U = U_hat.mean(axis=0)\n    # np.std computes population standard deviation (ddof=0) by default\n    sigma_S = S_hat.std(axis=0)\n    sigma_U = U_hat.std(axis=0)\n    \n    denom_S = np.maximum(sigma_S, delta)\n    denom_U = np.maximum(sigma_U, delta)\n    \n    S_bar = (S_hat - m_S) / denom_S\n    U_bar = (U_hat - m_U) / denom_U\n\n    # Step 4: Linear regression per gene and residual-based velocities\n    v = np.zeros_like(S_bar)\n    for g in range(n_genes):\n        x = S_bar[:, g]\n        y = U_bar[:, g]\n        \n        mu_x = x.mean()\n        mu_y = y.mean()\n        \n        # np.var computes population variance (ddof=0) by default\n        var_x = np.var(x)\n        \n        if var_x = delta:\n            cov_xy = np.mean((x - mu_x) * (y - mu_y))\n            alpha_g = cov_xy / var_x\n            beta_g = mu_y - alpha_g * mu_x\n        else:\n            alpha_g = 0.0\n            beta_g = mu_y\n            \n        v[:, g] = y - (alpha_g * x + beta_g)\n\n    # Step 5: Two-dimensional embedding by PCA\n    Z = S_bar\n    # Using full_matrices=False is efficient and gives Vh of shape (min(n,G), G)\n    try:\n        _, _, Vh = np.linalg.svd(Z, full_matrices=False)\n    except np.linalg.LinAlgError:\n         return [0.0, 0.0] # Failsafe for non-finite data\n\n    # V is the transpose of Vh\n    V = Vh.T\n    \n    # Handle cases with fewer than 2 genes/principal components\n    if V.shape[1]  2:\n        W = np.zeros((n_genes, 2))\n        W[:, :V.shape[1]] = V[:, :V.shape[1]]\n    else:\n        # W are the first 2 columns of V (gene loadings)\n        W = V[:, :2]\n\n    # Project data and velocities onto the 2D embedding space\n    X_emb = Z @ W\n    v_emb = v @ W\n    \n    # Step 6: Kernel smoothing of the vector field at a query point\n    diff = X_emb - q\n    sq_dists = np.sum(diff**2, axis=1)\n    weights = np.exp(-sq_dists / (2 * h**2))\n    \n    sum_weights = np.sum(weights)\n    \n    if sum_weights  delta: # Check for effective zero\n        V_q_hat = np.array([0.0, 0.0])\n    else:\n        weighted_v_emb = weights[:, np.newaxis] * v_emb\n        V_q_hat = np.sum(weighted_v_emb, axis=0) / sum_weights\n        \n    return [round(c, 6) for c in V_q_hat]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the pipeline for each, and prints the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"S\": np.array([[10, 5, 0], [3, 8, 1], [0, 2, 9], [6, 4, 3]]),\n            \"U\": np.array([[4, 2, 0], [1, 3, 0], [0, 1, 5], [2, 1, 1]]),\n            \"params\": (1000.0, 1.0, 1.0, 1e-8, 1.0, (0.0, 0.0))\n        },\n        {\n            \"S\": np.array([[0, 0, 0], [2, 0, 0], [0, 1, 0], [0, 0, 3]]),\n            \"U\": np.array([[0, 0, 0], [0, 1, 0], [0, 0, 2], [1, 0, 0]]),\n            \"params\": (1000.0, 0.5, 1.0, 1e-8, 0.5, (0.0, 0.0))\n        },\n        {\n            \"S\": np.array([[1000, 50, 7], [50, 50, 7], [3000, 50, 7], [10, 50, 7]]),\n            \"U\": np.array([[200, 10, 3], [10, 10, 3], [600, 10, 3], [2, 10, 3]]),\n            \"params\": (10000.0, 1.0, 1.0, 1e-8, 2.0, (0.0, 0.0))\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _calculate_rna_velocity_pipeline(case[\"S\"], case[\"U\"], case[\"params\"])\n        results.append(result)\n\n    # The str() representation of a list of lists matches the required format structure.\n    # Ex: [[-0.0, 0.0], [1.0, 2.0]] - str - '[[-0.0, 0.0], [1.0, 2.0]]'\n    # The problem asks for a comma-separated list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A model is only as reliable as its assumptions, and neighbor-based smoothing is a critical assumption in vector field estimation. This practice moves from implementation to critical evaluation by constructing a controlled, adversarial scenario where two distinct cellular lineages are misleadingly close in the embedding space. By quantifying the resulting error, you will develop crucial intuition about how embedding geometry can introduce artifacts and learn to critically assess the outputs of standard pipelines .",
            "id": "3346448",
            "problem": "You are given the task of quantifying the bias introduced by the choice of a neighbor graph in estimating an RNA velocity vector field when two developmentally distinct lineages are embedded in a space where $k$-nearest neighbors (kNN) mixes them. The analysis must be grounded in first principles: RNA velocity is the time derivative of gene expression along a latent manifold, and vector field estimation from single-cell data commonly smooths local velocity estimates using a neighbor graph. You must mathematically construct an adversarial embedding and compute a quantitative bias metric as a function of the number of neighbors $k$ and the manifold curvature $\\kappa$.\n\nConstruct the following purely mathematical model:\n\n- There are two lineages (curves) embedded in two-dimensional Euclidean space. Let the shared curve shape be a parabola with curvature controlled near the origin. Define the first lineage as the parametric curve $\\alpha(t) = (x(t), y(t))$ with\n  $$\n  x(t) = t, \\quad y(t) = \\frac{1}{2}\\,\\kappa\\,t^2,\n  $$\n  and the second lineage as $\\beta(t) = (x(t), y(t) + \\delta)$ with the same $x(t)$ and an added vertical offset $\\delta$ so that the lineages remain distinct but close. The parameter $t$ is sampled uniformly.\n\n- The true velocity is the time derivative of the embedding along each curve. For the first lineage, define\n  $$\n  \\mathbf{v}_A(t) = \\frac{d\\alpha}{dt} = \\left(1,\\, \\kappa\\, t\\right).\n  $$\n  For the second lineage, define the opposite progression direction to create an adversarial configuration:\n  $$\n  \\mathbf{v}_B(t) = -\\mathbf{v}_A(t).\n  $$\n  For error computation, normalize the true velocities to unit vectors:\n  $$\n  \\widehat{\\mathbf{v}}_A(t) = \\frac{\\mathbf{v}_A(t)}{\\|\\mathbf{v}_A(t)\\|_2}, \\quad \\widehat{\\mathbf{v}}_B(t) = \\frac{\\mathbf{v}_B(t)}{\\|\\mathbf{v}_B(t)\\|_2}.\n  $$\n\n- The estimated velocity at a cell is obtained by averaging the true velocities of its $k$ Euclidean nearest neighbors in the embedded space (including neighbors from both lineages) and then comparing the direction to the ground truth at that cell. Let $\\mathcal{N}_k(i)$ denote the index set of the $k$ nearest neighbors of cell $i$ excluding itself, and let $\\mathbf{v}_j^{\\text{true}}$ denote the unit true velocity at neighbor $j$. Define the estimated velocity at $i$ as the unweighted mean\n  $$\n  \\mathbf{v}_i^{\\text{est}} = \\frac{1}{k}\\sum_{j \\in \\mathcal{N}_k(i)} \\mathbf{v}_j^{\\text{true}}.\n  $$\n  The per-cell directional error is defined using the cosine dissimilarity:\n  $$\n  e_i = 1 - \\frac{\\mathbf{v}_i^{\\text{est}} \\cdot \\mathbf{v}_i^{\\text{true}}}{\\|\\mathbf{v}_i^{\\text{est}}\\|_2 \\, \\|\\mathbf{v}_i^{\\text{true}}\\|_2}.\n  $$\n  If $\\|\\mathbf{v}_i^{\\text{est}}\\|_2 = 0$, define $e_i = 1$.\n\n- Quantify the average bias $E(k,\\kappa)$ in a local window near the closest approach of the two lineages (where mixing is most adversarial). Specifically, sample $N$ cells per lineage at uniformly spaced parameter values $t$ over the interval $[-T, T]$. Define $h = \\frac{2T}{N-1}$ and set the vertical offset to $\\delta = \\eta h$ with a constant $\\eta \\in (0,1)$ to ensure that cross-lineage points at the same parameter $t$ are closer than same-lineage neighbors. Compute the mean error over both lineages restricted to the window $|t| \\le \\tau$:\n  $$\n  E(k,\\kappa) = \\frac{1}{M}\\sum_{i: |t_i| \\le \\tau} e_i,\n  $$\n  where $M$ is the number of cells across both lineages whose parameter values satisfy $|t_i| \\le \\tau$.\n\nImplement the following algorithm:\n\n- Fix the constants $N$, $T$, $\\eta$, and $\\tau$ to the values $N = 401$, $T = 1$, $\\eta = 0.2$, and $\\tau = 0.2$.\n- For each test case $(k,\\kappa)$, construct both lineages with the specified $\\kappa$ and compute $E(k,\\kappa)$ as defined above. Use Euclidean distance in $\\mathbb{R}^2$ for $k$-nearest neighbors. Concatenate the two lineages into a single dataset before building neighbor sets so that neighbor selection can mix lineages.\n- Normalize all true velocities to unit length before neighbor averaging. The unnormalized average $\\mathbf{v}_i^{\\text{est}}$ may be non-unit; use it directly in the cosine dissimilarity formula.\n\nTest suite:\n\n- Use the following test cases to probe different regimes, including boundary and adversarial conditions:\n  - $(k,\\kappa) = (1, 0)$\n  - $(k,\\kappa) = (1, 1)$\n  - $(k,\\kappa) = (5, 0.5)$\n  - $(k,\\kappa) = (15, 0)$\n  - $(k,\\kappa) = (51, 2)$\n  - $(k,\\kappa) = (151, 1.5)$\n\nRequired output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example $[r_1,r_2,\\dots,r_6]$, where each $r_i$ is the computed value of $E(k,\\kappa)$ for the corresponding test case. Each $r_i$ must be a floating-point number.",
            "solution": "We formalize the problem of neighbor-induced bias in RNA velocity vector field estimation using a controlled adversarial construction. The foundational principles are as follows: RNA velocity is the time derivative of gene expression along a latent cellular trajectory; in an embedding, this trajectory is a curve, and the local velocity vector aligns with the tangent. A common estimator smooths cell-wise velocities across a $k$-nearest neighbors (kNN) graph built on Euclidean distances in the embedding. If the embedding geometry causes different lineages to be closer than within-lineage neighbors, the neighbor averaging introduces a systematic bias by mixing distinct directions.\n\nWe proceed step by step.\n\n$1.$ Construct two lineages in $\\mathbb{R}^2$ as curves with tunable curvature $\\kappa$:\n- The first lineage is $\\alpha(t) = (t, \\frac{1}{2}\\kappa t^2)$ for $t \\in [-T,T]$.\n- The second lineage is $\\beta(t) = (t, \\delta + \\frac{1}{2}\\kappa t^2)$, an offset copy displaced vertically by $\\delta$.\n\nThis choice allows direct control over curvature near $t=0$. For the parabola $y = a t^2$ with $a = \\frac{\\kappa}{2}$, the curvature at the origin equals $\\kappa$ by the standard curvature formula for plane curves $y(x)$, which at $x=0$ reduces to $|y''(0)|$ since $y'(0)=0$.\n\n$2.$ Define true velocities as time derivatives of the embedding, consistent with the interpretation of RNA velocity:\n- For lineage $A$, $\\mathbf{v}_A(t) = \\frac{d\\alpha}{dt} = (1,\\kappa t)$.\n- For lineage $B$, enforce an adversarial opposite developmental direction $\\mathbf{v}_B(t) = -\\mathbf{v}_A(t)$.\nNormalize to unit vectors:\n$$\n\\widehat{\\mathbf{v}}_A(t) = \\frac{(1,\\kappa t)}{\\sqrt{1+(\\kappa t)^2}}, \\quad \\widehat{\\mathbf{v}}_B(t) = -\\widehat{\\mathbf{v}}_A(t).\n$$\nThis guarantees that, for cells with the same parameter $t$ drawn from different lineages, the true velocities are antiparallel, maximizing potential bias upon mixing.\n\n$3.$ Sampling and adversarial proximity. Sample $N$ cells per lineage at parameters $t_i$ uniformly spaced over $[-T,T]$. The parameter spacing is $h = \\frac{2T}{N-1}$. Set the vertical offset to $\\delta = \\eta h$ for a constant $\\eta \\in (0,1)$. For small $|t|$, the Euclidean distance between cross-lineage points at the same $t$ is $\\delta$, while the distance to same-lineage neighbors at $t \\pm h$ is approximately $h\\sqrt{1+(\\kappa t)^2}$. By choosing $\\delta = \\eta h$ with $\\eta  1$, we ensure that near $t=0$ the cross-lineage neighbor at identical $t$ is closer than same-lineage neighbors, and thus the kNN graph preferentially mixes lineages in that region.\n\n$4.$ kNN-based estimator. Concatenate both lineages into a single dataset of $2N$ cells located at points $\\{\\mathbf{x}_i\\}_{i=1}^{2N}$. For each cell $i$, compute its set $\\mathcal{N}_k(i)$ of $k$ nearest neighbors in Euclidean distance, excluding itself. Estimate the local velocity by unweighted neighbor averaging:\n$$\n\\mathbf{v}_i^{\\text{est}} = \\frac{1}{k} \\sum_{j \\in \\mathcal{N}_k(i)} \\mathbf{v}_j^{\\text{true}}.\n$$\nWe let $\\mathbf{v}_j^{\\text{true}}$ be the unit true velocity of neighbor $j$ corresponding to its lineage.\n\n$5.$ Error metric. To quantify directional bias, compute the cosine dissimilarity at each cell:\n$$\ne_i = 1 - \\frac{\\mathbf{v}_i^{\\text{est}} \\cdot \\mathbf{v}_i^{\\text{true}}}{\\|\\mathbf{v}_i^{\\text{est}}\\|_2 \\, \\|\\mathbf{v}_i^{\\text{true}}\\|_2}.\n$$\nSince $\\|\\mathbf{v}_i^{\\text{true}}\\|_2 = 1$ by construction, this reduces to $1 - \\frac{\\mathbf{v}_i^{\\text{est}} \\cdot \\mathbf{v}_i^{\\text{true}}}{\\|\\mathbf{v}_i^{\\text{est}}\\|_2}$. If $\\|\\mathbf{v}_i^{\\text{est}}\\|_2 = 0$, define $e_i = 1$ (maximal dissimilarity of random orientation on average). The mean error in a window $|t|\\le \\tau$ across both lineages is\n$$\nE(k,\\kappa) = \\frac{1}{M} \\sum_{i: |t_i| \\le \\tau} e_i.\n$$\nWe focus on a localized window to measure bias where mixing is strongest and to limit curvature-induced global effects.\n\n$6.$ Parameter choices and test suite. Fix $N = 401$, $T = 1$, and choose $\\eta = 0.2$ so that $\\delta = 0.2\\,h$, where $h = \\frac{2}{400} = 0.005$. Thus $\\delta = 0.001$. Choose window half-width $\\tau = 0.2$. Evaluate $E(k,\\kappa)$ for the specified pairs $(k,\\kappa)$:\n- $(1,0)$: boundary case with straight, parallel lineages and maximally adversarial opposite directions. With $k=1$, the nearest neighbor of a cell near $t=0$ lies on the opposite lineage at distance $\\delta$, giving $\\mathbf{v}_i^{\\text{est}} \\approx -\\mathbf{v}_i^{\\text{true}}$ and $e_i \\approx 2$ locally.\n- $(1,1)$: curved case with $k=1$, still adversarial; curvature slightly changes neighbor geometry.\n- $(5,0.5)$: small neighbor smoothing with low curvature.\n- $(15,0)$: stronger smoothing on straight lineages.\n- $(51,2)$: high smoothing and high curvature, increasing within-lineage neighbor distances and thus cross-lineage mixing near $t=0$.\n- $(151,1.5)$: very strong smoothing, averaging over many neighbors, potentially diluting directionality and increasing the chance of cancellation.\n\n$7.$ Computational method. For each $(k,\\kappa)$:\n- Build coordinates for both lineages using the given $\\kappa$ and $\\delta$.\n- Compute pairwise Euclidean distances and obtain $\\mathcal{N}_k(i)$ by sorting distances.\n- Compute $\\mathbf{v}_i^{\\text{est}}$ and $e_i$ for all $i$.\n- Restrict to cells with $|t| \\le \\tau$ and compute the mean to obtain $E(k,\\kappa)$.\n\nThe final program implements this algorithm deterministically and outputs the list $[E(1,0), E(1,1), E(5,0.5), E(15,0), E(51,2), E(151,1.5)]$ as a single line of comma-separated floats in brackets.\n\nQualitative expectations:\n- For $k=1$, the adversarial design yields large errors near $t=0$, close to $2$ when the nearest neighbor is on the opposite lineage with opposite direction.\n- As $k$ increases, the estimator averages over mixed neighbors; depending on the proportion of opposite-lineage neighbors, $\\mathbf{v}_i^{\\text{est}}$ can be attenuated or even nearly cancel, increasing the cosine dissimilarity toward $1$ if the estimate becomes small in magnitude or misaligned.\n- Increasing $\\kappa$ can increase the within-lineage neighbor distances at fixed parameter spacing near the window edges (since points separate more in the $y$-direction), enhancing cross-lineage mixing and tending to increase $E(k,\\kappa)$ for fixed $k$ in the adversarial region.\n\nThese behaviors emerge from the interplay between the geometric embedding and the kNN graph, grounded in the definition of RNA velocity as the manifold tangent and the estimator as neighbor averaging in the embedding.",
            "answer": "```python\nimport numpy as np\n\ndef build_lineages_and_velocities(kappa, N=401, T=1.0, eta=0.2):\n    # Parameter grid\n    t = np.linspace(-T, T, N)\n    h = (2*T) / (N-1)\n    delta = eta * h\n\n    # Lineage A coordinates: (t, 0.5 * kappa * t^2)\n    xA = t.copy()\n    yA = 0.5 * kappa * t**2\n\n    # Lineage B coordinates: shifted vertically by delta\n    xB = t.copy()\n    yB = delta + 0.5 * kappa * t**2\n\n    # True velocities (unit vectors)\n    # v_A = (1, kappa * t) normalized\n    denom = np.sqrt(1.0 + (kappa * t)**2)\n    vAx = 1.0 / denom\n    vAy = (kappa * t) / denom\n\n    # v_B = -v_A\n    vBx = -vAx\n    vBy = -vAy\n\n    # Stack coordinates and velocities\n    X = np.vstack([np.column_stack([xA, yA]), np.column_stack([xB, yB])])  # shape (2N, 2)\n    V_true = np.vstack([np.column_stack([vAx, vAy]), np.column_stack([vBx, vBy])])  # shape (2N, 2)\n    # Parameter t replicated for both lineages\n    t_all = np.concatenate([t, t])\n\n    return X, V_true, t_all\n\ndef knn_indices(X, k):\n    # Compute pairwise distances\n    # X shape: (M, 2)\n    M = X.shape[0]\n    # Use (x^2 + y^2) - 2 X X^T + (x^2 + y^2)^T trick\n    sq = np.sum(X**2, axis=1, keepdims=True)  # (M,1)\n    d2 = sq + sq.T - 2.0 * (X @ X.T)\n    # Ensure self-distance is inf to exclude self\n    np.fill_diagonal(d2, np.inf)\n    # Get k nearest indices for each point\n    # argsort along axis 1, take first k\n    idx = np.argpartition(d2, kth=k-1, axis=1)[:, :k]\n    # For stable ordering by actual distance within the k subset\n    # Gather distances for these idx and sort within rows\n    row_indices = np.arange(M)[:, None]\n    d2_k = d2[row_indices, idx]\n    order_within = np.argsort(d2_k, axis=1)\n    idx_sorted = idx[row_indices, order_within]\n    return idx_sorted\n\ndef compute_mean_error(X, V_true, t_all, k, tau=0.2):\n    # Build kNN graph\n    k = int(k)\n    k = max(1, min(k, X.shape[0]-1))\n    nn_idx = knn_indices(X, k)\n    # Estimate velocities by averaging neighbors' true velocities\n    V_est = np.zeros_like(V_true)\n    for i in range(X.shape[0]):\n        nbrs = nn_idx[i]\n        v_mean = V_true[nbrs].mean(axis=0)\n        V_est[i] = v_mean\n\n    # Compute per-cell cosine dissimilarity\n    # Handle zero norm estimates\n    est_norm = np.linalg.norm(V_est, axis=1)\n    # True norm is 1 by construction, but compute robustly\n    true_norm = np.linalg.norm(V_true, axis=1)\n    # Avoid division by zero\n    eps = 1e-12\n    cos_sim = np.sum(V_est * V_true, axis=1) / (np.maximum(est_norm, eps) * np.maximum(true_norm, eps))\n    # Clip for numerical safety\n    cos_sim = np.clip(cos_sim, -1.0, 1.0)\n    e = 1.0 - cos_sim\n\n    # Restrict to window |t| = tau\n    mask = np.abs(t_all) = tau\n    if not np.any(mask):\n        return float(np.nan)\n    E = float(e[mask].mean())\n    return E\n\ndef solve():\n    # Constants per the problem statement\n    N = 401\n    T = 1.0\n    eta = 0.2\n    tau = 0.2\n\n    # Test cases: (k, kappa)\n    test_cases = [\n        (1, 0.0),\n        (1, 1.0),\n        (5, 0.5),\n        (15, 0.0),\n        (51, 2.0),\n        (151, 1.5),\n    ]\n\n    results = []\n    for k, kappa in test_cases:\n        X, V_true, t_all = build_lineages_and_velocities(kappa, N=N, T=T, eta=eta)\n        E = compute_mean_error(X, V_true, t_all, k=k, tau=tau)\n        # Round to 6 decimal places for stable output\n        results.append(f\"{E:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Standard RNA velocity provides a powerful but relative measure of cellular change, lacking an absolute time scale. This advanced exercise tackles this limitation by moving from the simple steady-state assumption to a full kinetic model. You will implement a hierarchical Bayesian framework to integrate multimodal data from metabolic labeling experiments, enabling the inference of physically meaningful, calibrated transcription and splicing rates, and pushing toward a more quantitative and biophysically grounded understanding of cellular dynamics .",
            "id": "3346357",
            "problem": "Consider a simplified kinetic model for Ribonucleic Acid (RNA) transcription and processing where the unspliced and spliced counts for gene $g$ are denoted by $u_g$ and $s_g$, respectively. The core dynamical system is captured by the following Ordinary Differential Equations (ODEs):\n$$\n\\frac{du_g}{dt} = \\alpha_g - \\beta_g u_g, \\quad \\frac{ds_g}{dt} = \\beta_g u_g - \\gamma_g s_g,\n$$\nwhere $\\alpha_g$ is the transcription initiation rate, $\\beta_g$ is the splicing rate, and $\\gamma_g$ is the degradation rate. The velocity of spliced RNA is defined as:\n$$\nv_g = \\frac{ds_g}{dt} = \\beta_g u_g - \\gamma_g s_g,\n$$\nand is expressed in molecules per minute when $\\gamma_g$ is in $\\text{min}^{-1}$ and $u_g, s_g$ are in molecules.\n\nTo calibrate the transcription rate scale, we incorporate metabolic labeling data from 4-thiouridine (4sU), which provides a noisy linear proxy for $\\alpha_g$. Specifically, we assume an observation model\n$$\nz_g = c \\, \\alpha_g + \\epsilon^{(z)}_g, \\quad \\epsilon^{(z)}_g \\sim \\mathcal{N}(0, \\sigma_z^2),\n$$\nwhere $z_g$ is the derived measurement from 4sU labeling, $c$ is a known calibration scale, and $\\sigma_z^2$ is the measurement noise variance. We also include a quasi-steady-state coupling between $\\alpha_g$ and $\\beta_g$ via the unspliced count $u_g$, treated as a noisy constraint:\n$$\nr_g = \\alpha_g - u_g \\beta_g + \\epsilon^{(r)}_g, \\quad \\epsilon^{(r)}_g \\sim \\mathcal{N}(0, \\sigma_r^2),\n$$\nwith a pseudo-observation $r_g = 0$ representing small departure from steady-state in the unspliced pool.\n\nWe place a hierarchical Bayesian prior over the per-gene parameters with shared hyperparameters:\n$$\n\\alpha_g \\mid \\mu_\\alpha \\sim \\mathcal{N}(\\mu_\\alpha, \\tau_\\alpha^2), \\quad \\beta_g \\mid \\mu_\\beta \\sim \\mathcal{N}(\\mu_\\beta, \\tau_\\beta^2),\n$$\n$$\n\\mu_\\alpha \\sim \\mathcal{N}(m_{0,\\alpha}, s_{0,\\alpha}^2), \\quad \\mu_\\beta \\sim \\mathcal{N}(m_{0,\\beta}, s_{0,\\beta}^2),\n$$\nwith $\\tau_\\alpha^2$ and $\\tau_\\beta^2$ taken as fixed and known.\n\nUnder this linear-Gaussian hierarchical model, the joint posterior over the variables\n$$\n\\mathbf{x} = \\left[\\alpha_1, \\ldots, \\alpha_n, \\beta_1, \\ldots, \\beta_n, \\mu_\\alpha, \\mu_\\beta\\right]^\\top\n$$\nis Gaussian. Let the posterior precision matrix be $\\mathbf{K}$ and the posterior information vector be $\\mathbf{h}$ so that the posterior mean $\\boldsymbol{m}$ satisfies\n$$\n\\mathbf{K} \\, \\boldsymbol{m} = \\mathbf{h},\n$$\nand the posterior covariance is $\\mathbf{\\Sigma} = \\mathbf{K}^{-1}$. The posterior distribution of $v_g$ is Gaussian because $v_g$ is an affine function of $\\beta_g$. Its mean and variance are\n$$\n\\mathbb{E}[v_g] = u_g \\, \\mathbb{E}[\\beta_g] - \\gamma_g s_g, \\quad \\mathrm{Var}(v_g) = u_g^2 \\, \\mathrm{Var}(\\beta_g).\n$$\n\nDefine an identifiability signal-to-noise ratio (SNR) criterion for the velocity:\n$$\n\\mathrm{SNR}_g = \\frac{|\\mathbb{E}[v_g]|}{\\sqrt{\\mathrm{Var}(v_g)}},\n$$\nand declare $v_g$ identifiable if $\\mathrm{SNR}_g \\geq T$, where $T$ is a specified threshold.\n\nYour task is to implement a program that:\n- Constructs the posterior for the hierarchical model using linear-Gaussian factors corresponding to the priors and the likelihoods described above.\n- Computes, for each gene in each test case, the posterior mean of $v_g$ in molecules per minute, the posterior standard deviation of $v_g$ in molecules per minute, and the identifiability boolean according to the SNR threshold $T$.\n\nUse the following test suite parameter sets:\n\n- Test Case $1$ (general case with informative 4sU):\n  - $n = 3$, genes indexed by $g = 1, 2, 3$.\n  - $u = [5.0, 2.0, 8.0]$ molecules, $s = [20.0, 10.0, 30.0]$ molecules.\n  - $z = [5.2, 1.8, 9.0]$ arbitrary units.\n  - $c = 1.0$, $\\sigma_z = 0.5$, $\\sigma_r = 0.3$.\n  - $\\gamma_g = 0.1$ $\\text{min}^{-1}$ for all $g$.\n  - $m_{0,\\alpha} = 2.0$, $s_{0,\\alpha} = 2.0$, $\\tau_\\alpha = 1.0$.\n  - $m_{0,\\beta} = 0.5$, $s_{0,\\beta} = 2.0$, $\\tau_\\beta = 1.0$.\n  - $T = 2.0$.\n\n- Test Case $2$ (weak 4sU calibration, high noise):\n  - $n = 3$, $u = [5.0, 2.0, 8.0]$, $s = [20.0, 10.0, 30.0]$.\n  - $z = [5.2, 1.8, 9.0]$.\n  - $c = 1.0$, $\\sigma_z = 10.0$, $\\sigma_r = 0.3$.\n  - $\\gamma_g = 0.1$ $\\text{min}^{-1}$ for all $g$.\n  - $m_{0,\\alpha} = 2.0$, $s_{0,\\alpha} = 2.0$, $\\tau_\\alpha = 1.0$.\n  - $m_{0,\\beta} = 0.5$, $s_{0,\\beta} = 2.0$, $\\tau_\\beta = 1.0$.\n  - $T = 2.0$.\n\n- Test Case $3$ (boundary case with $u = 0$ for one gene and no 4sU calibration):\n  - $n = 2$, $u = [0.0, 3.0]$, $s = [15.0, 12.0]$.\n  - $z = [0.0, 0.0]$ (non-informative due to zero calibration).\n  - $c = 0.0$, $\\sigma_z = 1.0$, $\\sigma_r = 0.2$.\n  - $\\gamma_g = 0.2$ $\\text{min}^{-1}$ for all $g$.\n  - $m_{0,\\alpha} = 2.0$, $s_{0,\\alpha} = 2.0$, $\\tau_\\alpha = 1.0$.\n  - $m_{0,\\beta} = 0.5$, $s_{0,\\beta} = 2.0$, $\\tau_\\beta = 1.0$.\n  - $T = 1.0$.\n\nYour program should produce a single line of output containing, for each test case, a list of three elements: \n- the list of posterior mean velocities $[\\mathbb{E}[v_1], \\ldots, \\mathbb{E}[v_n]]$ in molecules per minute (floats),\n- the list of posterior standard deviations $[\\sqrt{\\mathrm{Var}(v_1)}, \\ldots, \\sqrt{\\mathrm{Var}(v_n)}]$ in molecules per minute (floats),\n- the list of identifiability booleans $[\\mathrm{SNR}_1 \\ge T, \\ldots, \\mathrm{SNR}_n \\ge T]$.\n\nThe final output format must be a single line containing the results for all test cases as a comma-separated list enclosed in square brackets, for example:\n$$\n[\\text{case1\\_result}, \\text{case2\\_result}, \\text{case3\\_result}],\n$$\nwhere each $\\text{caseX\\_result}$ is itself a list with the structure described above. All velocities and standard deviations must be expressed in molecules per minute, with each float presented to four decimal places. Angles are not involved. No percentages are used; any ratio must be represented as a decimal. The program must be completely deterministic and self-contained.",
            "solution": "The problem requires the computation of posterior statistics for RNA velocity within a specified hierarchical Bayesian model. The model is a linear-Gaussian system, which implies that the joint posterior distribution over all unknown parameters is also Gaussian. This allows for an analytical solution by constructing and solving a linear system, bypassing the need for computationally intensive sampling methods like Markov Chain Monte Carlo (MCMC). The solution proceeds in three main stages: construction of the posterior distribution's canonical form, solving for the posterior parameters, and computing the desired velocity statistics.\n\n### 1. Bayesian Model and Posterior Formulation\n\nThe vector of unknown parameters is $\\mathbf{x} = [\\alpha_1, \\ldots, \\alpha_n, \\beta_1, \\ldots, \\beta_n, \\mu_\\alpha, \\mu_\\beta]^\\top$, with dimension $2n+2$. The posterior distribution $p(\\mathbf{x} | \\text{data})$ is proportional to the product of all prior and likelihood terms:\n$$\np(\\mathbf{x} | \\text{data}) \\propto p(\\mu_\\alpha) p(\\mu_\\beta) \\left( \\prod_{g=1}^n p(\\alpha_g | \\mu_\\alpha) p(\\beta_g | \\mu_\\beta) p(z_g | \\alpha_g) p(r_g=0 | \\alpha_g, \\beta_g) \\right).\n$$\nSince all factors are Gaussian, their product is also Gaussian. The log of the posterior density is a quadratic function of $\\mathbf{x}$:\n$$\n\\log p(\\mathbf{x} | \\text{data}) = -\\frac{1}{2}\\mathbf{x}^\\top \\mathbf{K} \\mathbf{x} + \\mathbf{h}^\\top \\mathbf{x} + \\text{const.},\n$$\nwhere $\\mathbf{K}$ is the posterior precision matrix and $\\mathbf{h}$ is the posterior information vector. A key property of this canonical form is that the total precision matrix $\\mathbf{K}$ is the sum of the precision matrices from each factor, and the total information vector $\\mathbf{h}$ is the sum of the information vectors from each factor. We will construct $\\mathbf{K}$ and $\\mathbf{h}$ by summing the contributions from each term in the model.\n\nLet the indices of the state vector $\\mathbf{x}$ be as follows: $\\alpha_g$ is at index $g-1$, $\\beta_g$ is at index $n+g-1$ (for $g \\in \\{1, \\ldots, n\\}$), $\\mu_\\alpha$ is at index $2n$, and $\\mu_\\beta$ is at index $2n+1$.\n\n### 2. Construction of the Precision Matrix ($\\mathbf{K}$) and Information Vector ($\\mathbf{h}$)\n\nWe analyze each factor's contribution to $\\mathbf{K}$ and $\\mathbf{h}$.\n\n- **Hyperpriors**: $p(\\mu_\\alpha) = \\mathcal{N}(\\mu_\\alpha | m_{0,\\alpha}, s_{0,\\alpha}^2)$ and $p(\\mu_\\beta) = \\mathcal{N}(\\mu_\\beta | m_{0,\\beta}, s_{0,\\beta}^2)$.\n  The log-priors are $-\\frac{1}{2s_{0,\\alpha}^2}(\\mu_\\alpha - m_{0,\\alpha})^2$ and $-\\frac{1}{2s_{0,\\beta}^2}(\\mu_\\beta - m_{0,\\beta})^2$. Expanding these quadratic forms yields:\n  - Contribution to $\\mathbf{K}[2n, 2n]$: $1/s_{0,\\alpha}^2$. Contribution to $\\mathbf{h}[2n]$: $m_{0,\\alpha}/s_{0,\\alpha}^2$.\n  - Contribution to $\\mathbf{K}[2n+1, 2n+1]$: $1/s_{0,\\beta}^2$. Contribution to $\\mathbf{h}[2n+1]$: $m_{0,\\beta}/s_{0,\\beta}^2$.\n\n- **Hierarchical Priors**: For each gene $g$, $p(\\alpha_g | \\mu_\\alpha) = \\mathcal{N}(\\alpha_g | \\mu_\\alpha, \\tau_\\alpha^2)$ and $p(\\beta_g | \\mu_\\beta) = \\mathcal{N}(\\beta_g | \\mu_\\beta, \\tau_\\beta^2)$.\n  The log-priors involve pairs of variables, e.g., $-\\frac{1}{2\\tau_\\alpha^2}(\\alpha_g - \\mu_\\alpha)^2 = -\\frac{1}{2\\tau_\\alpha^2}(\\alpha_g^2 - 2\\alpha_g\\mu_\\alpha + \\mu_\\alpha^2)$.\n  - For $p(\\alpha_g|\\mu_\\alpha)$, the precision matrix contribution on variables $(\\alpha_g, \\mu_\\alpha)$ is $\\frac{1}{\\tau_\\alpha^2}\\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$. This affects $\\mathbf{K}[g-1, g-1]$, $\\mathbf{K}[2n, 2n]$, and the off-diagonal terms $\\mathbf{K}[g-1, 2n]$ and $\\mathbf{K}[2n, g-1]$.\n  - Similarly for $p(\\beta_g|\\mu_\\beta)$, the precision matrix contribution on $(\\beta_g, \\mu_\\beta)$ is $\\frac{1}{\\tau_\\beta^2}\\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$.\n\n- **Likelihoods**: For each gene $g$, we have two observation terms.\n  - $p(z_g | \\alpha_g) = \\mathcal{N}(z_g | c\\alpha_g, \\sigma_z^2)$: The log-likelihood is $-\\frac{1}{2\\sigma_z^2}(z_g - c\\alpha_g)^2 = -\\frac{c^2}{2\\sigma_z^2}\\alpha_g^2 + \\frac{cz_g}{\\sigma_z^2}\\alpha_g + \\text{const}$.\n    - Contribution to $\\mathbf{K}[g-1, g-1]$: $c^2/\\sigma_z^2$. Contribution to $\\mathbf{h}[g-1]$: $cz_g/\\sigma_z^2$.\n  - $p(r_g=0 | \\alpha_g, \\beta_g) = \\mathcal{N}(0 | \\alpha_g - u_g\\beta_g, \\sigma_r^2)$: The log-likelihood is $-\\frac{1}{2\\sigma_r^2}(\\alpha_g - u_g\\beta_g)^2 = -\\frac{1}{2\\sigma_r^2}(\\alpha_g^2 - 2u_g\\alpha_g\\beta_g + u_g^2\\beta_g^2)$.\n    - The precision matrix contribution on variables $(\\alpha_g, \\beta_g)$ is $\\frac{1}{\\sigma_r^2}\\begin{pmatrix} 1  -u_g \\\\ -u_g  u_g^2 \\end{pmatrix}$.\n\nBy initializing $\\mathbf{K}$ and $\\mathbf{h}$ to zero and summing these contributions over all genes and all model factors, we obtain the complete posterior precision matrix and information vector.\n\n### 3. Solving for Posterior Moments and Velocity Statistics\n\nThe posterior mean vector $\\boldsymbol{m}$ and covariance matrix $\\mathbf{\\Sigma}$ are found by:\n$$\n\\boldsymbol{m} = \\mathbf{K}^{-1}\\mathbf{h} \\quad \\text{and} \\quad \\mathbf{\\Sigma} = \\mathbf{K}^{-1}.\n$$\nComputationally, it is more stable to solve the linear system $\\mathbf{K}\\boldsymbol{m} = \\mathbf{h}$ for $\\boldsymbol{m}$ and then compute $\\mathbf{\\Sigma}$ by inverting $\\mathbf{K}$.\n\nThe posterior moments of $\\beta_g$ for each gene are extracted directly from $\\boldsymbol{m}$ and $\\mathbf{\\Sigma}$:\n- Posterior mean: $\\mathbb{E}[\\beta_g] = \\boldsymbol{m}_{n+g-1}$\n- Posterior variance: $\\mathrm{Var}(\\beta_g) = \\mathbf{\\Sigma}_{n+g-1, n+g-1}$\n\nThe velocity $v_g = \\beta_g u_g - \\gamma_g s_g$ is an affine function of the Gaussian random variable $\\beta_g$. Thus, its posterior distribution is also Gaussian. Its mean and variance are:\n$$\n\\mathbb{E}[v_g] = u_g \\, \\mathbb{E}[\\beta_g] - \\gamma_g s_g\n$$\n$$\n\\mathrm{Var}(v_g) = \\mathrm{Var}(u_g \\beta_g - \\gamma_g s_g) = u_g^2 \\, \\mathrm{Var}(\\beta_g)\n$$\nThe posterior standard deviation is $\\sqrt{\\mathrm{Var}(v_g)}$.\n\nFinally, the signal-to-noise ratio is calculated:\n$$\n\\mathrm{SNR}_g = \\frac{|\\mathbb{E}[v_g]|}{\\sqrt{\\mathrm{Var}(v_g)}}.\n$$\nA special case arises when $u_g = 0$, leading to $\\mathrm{Var}(v_g) = 0$. If $\\mathbb{E}[v_g] \\neq 0$, the SNR is infinite. If both are zero, the SNR is zero. The velocity $v_g$ is deemed identifiable if $\\mathrm{SNR}_g \\geq T$.\n\nThe implementation will follow these steps for each test case, constructing and solving the system, and then computing the required statistics for each gene.",
            "answer": "```python\nimport numpy as np\n\ndef format_result(mean_v, std_v, iden):\n    \"\"\"\n    Formats the results for a single test case into the required string representation.\n    \"\"\"\n    mean_v_str = [f\"{v:.4f}\" for v in mean_v]\n    std_v_str = [f\"{v:.4f}\" for v in std_v]\n    # Use join to create a comma-separated list without spaces, e.g., \"[True,False]\"\n    iden_str = ','.join(map(str, iden))\n    \n    return f\"[[{','.join(mean_v_str)}],[{','.join(std_v_str)}],[{iden_str}]]\"\n\ndef compute_posterior_stats(params):\n    \"\"\"\n    Constructs the posterior and computes velocity statistics for a single test case.\n    \"\"\"\n    n, u, s, z, c, sigma_z, sigma_r, gamma_val, m0_alpha, s0_alpha, tau_alpha, m0_beta, s0_beta, tau_beta, T = params\n\n    gamma = [gamma_val] * n\n    dim = 2 * n + 2\n    K = np.zeros((dim, dim))\n    h = np.zeros(dim)\n\n    # Indices map\n    mu_alpha_idx = 2 * n\n    mu_beta_idx = 2 * n + 1\n\n    # Add hyperprior contributions\n    if s0_alpha  0:\n        prec_m0_alpha = 1 / s0_alpha**2\n        K[mu_alpha_idx, mu_alpha_idx] += prec_m0_alpha\n        h[mu_alpha_idx] += m0_alpha * prec_m0_alpha\n    if s0_beta  0:\n        prec_m0_beta = 1 / s0_beta**2\n        K[mu_beta_idx, mu_beta_idx] += prec_m0_beta\n        h[mu_beta_idx] += m0_beta * prec_m0_beta\n\n    # Pre-calculate factor precisions\n    prec_tau_alpha = 1 / tau_alpha**2 if tau_alpha  0 else 0\n    prec_tau_beta = 1 / tau_beta**2 if tau_beta  0 else 0\n    prec_z = 1 / sigma_z**2 if sigma_z  0 else 0\n    prec_r = 1 / sigma_r**2 if sigma_r  0 else 0\n\n    # Loop over genes to add prior and likelihood contributions\n    for g in range(n):\n        alpha_idx = g\n        beta_idx = n + g\n\n        # Hierarchical prior on alpha_g\n        K[alpha_idx, alpha_idx] += prec_tau_alpha\n        K[mu_alpha_idx, mu_alpha_idx] += prec_tau_alpha\n        K[alpha_idx, mu_alpha_idx] -= prec_tau_alpha\n        K[mu_alpha_idx, alpha_idx] -= prec_tau_alpha\n\n        # Hierarchical prior on beta_g\n        K[beta_idx, beta_idx] += prec_tau_beta\n        K[mu_beta_idx, mu_beta_idx] += prec_tau_beta\n        K[beta_idx, mu_beta_idx] -= prec_tau_beta\n        K[mu_beta_idx, beta_idx] -= prec_tau_beta\n\n        # Likelihood from z_g\n        K[alpha_idx, alpha_idx] += c**2 * prec_z\n        h[alpha_idx] += c * z[g] * prec_z\n        \n        # Likelihood from r_g\n        ug = u[g]\n        K[alpha_idx, alpha_idx] += prec_r\n        K[beta_idx, beta_idx] += ug**2 * prec_r\n        K[alpha_idx, beta_idx] -= ug * prec_r\n        K[beta_idx, alpha_idx] -= ug * prec_r\n\n    # Solve for posterior mean and calculate covariance\n    m = np.linalg.solve(K, h)\n    Sigma = np.linalg.inv(K)\n\n    # Calculate and collect velocity statistics for each gene\n    mean_velocities = []\n    std_dev_velocities = []\n    identifiability = []\n\n    for g in range(n):\n        beta_idx = n + g\n        mean_beta_g = m[beta_idx]\n        var_beta_g = Sigma[beta_idx, beta_idx]\n        \n        # Ensure variance is non-negative due to potential floating point errors\n        var_beta_g = max(0, var_beta_g)\n\n        mean_vg = u[g] * mean_beta_g - gamma[g] * s[g]\n        var_vg = u[g]**2 * var_beta_g\n        std_dev_vg = np.sqrt(var_vg)\n        \n        # SNR calculation, handling division by zero\n        if std_dev_vg == 0.0:\n            snr_g = np.inf if np.abs(mean_vg)  0 else 0.0\n        else:\n            snr_g = np.abs(mean_vg) / std_dev_vg\n\n        is_identifiable = snr_g = T\n        \n        mean_velocities.append(mean_vg)\n        std_dev_velocities.append(std_dev_vg)\n        identifiability.append(is_identifiable)\n\n    return format_result(mean_velocities, std_dev_velocities, identifiability)\n\ndef solve():\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Test Case 1\n        (3, [5.0, 2.0, 8.0], [20.0, 10.0, 30.0], [5.2, 1.8, 9.0], 1.0, 0.5, 0.3, 0.1, 2.0, 2.0, 1.0, 0.5, 2.0, 1.0, 2.0),\n        # Test Case 2\n        (3, [5.0, 2.0, 8.0], [20.0, 10.0, 30.0], [5.2, 1.8, 9.0], 1.0, 10.0, 0.3, 0.1, 2.0, 2.0, 1.0, 0.5, 2.0, 1.0, 2.0),\n        # Test Case 3\n        (2, [0.0, 3.0], [15.0, 12.0], [0.0, 0.0], 0.0, 1.0, 0.2, 0.2, 2.0, 2.0, 1.0, 0.5, 2.0, 1.0, 1.0)\n    ]\n    \n    results = []\n    for case in test_cases:\n        result_str = compute_posterior_stats(case)\n        results.append(result_str)\n\n    # Print the final output as a single-line string\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}