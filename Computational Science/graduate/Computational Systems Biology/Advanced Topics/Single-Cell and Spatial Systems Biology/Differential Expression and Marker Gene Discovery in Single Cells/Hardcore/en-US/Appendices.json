{
    "hands_on_practices": [
        {
            "introduction": "A foundational step in any differential expression analysis is to make gene counts comparable across cells that have been sequenced to different depths. This practice introduces the concept of library size normalization to correct for technical variability. By working through this exercise, you will learn to compute normalized expression values and the log-2 fold-change, which is the standard metric for quantifying expression differences in a way that is both statistically tractable and biologically intuitive.",
            "id": "3301307",
            "problem": "Consider single-cell ribonucleic acid sequencing (scRNA-seq) counts for a single gene measured in two cells, with observed counts $x_1=50$ and $x_2=100$, and corresponding library size normalization factors $s_1=1.0$ and $s_2=2.5$. In a standard generative framework, assume the count $x_j$ for cell $j$ is a realization from a distribution whose mean scales linearly with the cell’s library size via a multiplicative size factor, specifically $\\mathbb{E}[x_j]=s_j \\,\\mu$, where $\\mu$ denotes the underlying gene-specific expression level on the count scale that is comparable across cells after accounting for library size. Under this framework, the normalized count $\\tilde{x}_j$ is defined as the count per unit size factor for cell $j$, and the fold-change in expression between cell $2$ and cell $1$ is defined on the normalized scale. The log fold-change uses the logarithm base $2$ to quantify differences on a binary multiplicative scale that is standard in differential expression analyses for single cells.\n\nUsing these principles, compute the normalized counts $\\tilde{x}_1$ and $\\tilde{x}_2$ and then the base-$2$ logarithm of the fold-change, $\\log_{2}\\!\\left(\\frac{\\tilde{x}_2}{\\tilde{x}_1}\\right)$. Report only the final log fold-change value as your answer. Round your final answer to six significant figures.",
            "solution": "The problem provides the observed ribonucleic acid (RNA) counts and library size normalization factors for a single gene measured in two distinct cells. The given values are:\n- Observed count for cell $1$: $x_1 = 50$\n- Observed count for cell $2$: $x_2 = 100$\n- Library size factor for cell $1$: $s_1 = 1.0$\n- Library size factor for cell $2$: $s_2 = 2.5$\n\nThe normalized count, denoted as $\\tilde{x}_j$ for cell $j$, is defined as the \"count per unit size factor\". This translates to the mathematical operation of dividing the observed count $x_j$ by the corresponding cell-specific size factor $s_j$:\n$$ \\tilde{x}_j = \\frac{x_j}{s_j} $$\nThis normalization procedure is a standard and essential step in single-cell transcriptomics. It aims to adjust for technical variations in sequencing depth, thereby making gene expression levels comparable across cells that may have been sequenced to different extents. The problem specifies a generative model where the expected count is $\\mathbb{E}[x_j] = s_j \\mu$, with $\\mu$ representing the true underlying gene-specific expression level that is common to both cells. The expected value of the normalized count under this model is:\n$$ \\mathbb{E}[\\tilde{x}_j] = \\mathbb{E}\\left[\\frac{x_j}{s_j}\\right] = \\frac{1}{s_j} \\mathbb{E}[x_j] = \\frac{1}{s_j} (s_j \\mu) = \\mu $$\nThis result confirms that the expectation of the normalized count is independent of the cell's size factor, which validates its use for comparing expression levels.\n\nWe proceed to calculate the normalized counts for cell $1$ and cell $2$ using the provided data.\nFor cell $1$, the normalized count $\\tilde{x}_1$ is:\n$$ \\tilde{x}_1 = \\frac{x_1}{s_1} = \\frac{50}{1.0} = 50 $$\nFor cell $2$, the normalized count $\\tilde{x}_2$ is:\n$$ \\tilde{x}_2 = \\frac{x_2}{s_2} = \\frac{100}{2.5} = 40 $$\n\nThe fold-change (FC) in expression from cell $1$ to cell $2$ is calculated as the ratio of their normalized counts. It is crucial to use the normalized counts for this calculation, not the raw counts, to obtain a biologically meaningful comparison.\n$$ \\text{FC} = \\frac{\\tilde{x}_2}{\\tilde{x}_1} = \\frac{40}{50} = 0.8 $$\n\nThe final quantity to be computed is the log fold-change (LFC), which is the base-$2$ logarithm of the fold-change. The base $2$ is conventional in genomics as it provides an intuitive scale where a change of $+1$ or $-1$ corresponds to a doubling or halving of expression, respectively.\n$$ \\text{LFC} = \\log_{2}(\\text{FC}) = \\log_{2}\\left(\\frac{\\tilde{x}_2}{\\tilde{x}_1}\\right) = \\log_{2}(0.8) $$\nAn LFC value of $0$ indicates no change, a positive LFC indicates upregulation, and a negative LFC indicates downregulation. Since the normalized expression in cell $2$ ($40$) is lower than in cell $1$ ($50$), we expect a negative LFC.\n\nTo find the numerical value, we apply the change of base formula for logarithms, $\\log_{b}(a) = \\frac{\\ln(a)}{\\ln(b)}$:\n$$ \\log_{2}(0.8) = \\frac{\\ln(0.8)}{\\ln(2)} $$\nUsing a calculator for the numerical values of the natural logarithms:\n$$ \\log_{2}(0.8) \\approx \\frac{-0.2231435513}{0.6931471806} \\approx -0.3219280949 $$\nThe problem requires that the final answer is rounded to six significant figures. The computed value is $-0.3219280949\\ldots$. The first six significant digits are $3$, $2$, $1$, $9$, $2$, $8$. The seventh digit is $0$, which is less than $5$, so we do not round up the sixth significant digit.\nThe resulting log fold-change, rounded to six significant figures, is $-0.321928$.",
            "answer": "$$\n\\boxed{-0.321928}\n$$"
        },
        {
            "introduction": "After normalizing data, the next challenge is to determine if an observed change in expression is statistically significant or simply due to random noise. Single-cell count data exhibits a property called overdispersion, where the variance is much larger than the mean, making simple statistical models like the Poisson distribution inadequate. This practice delves into the Negative Binomial model and guides you through estimating the overdispersion parameter, a crucial step for accurately modeling the data and avoiding false positives in downstream tests.",
            "id": "3301330",
            "problem": "In single-cell ribonucleic acid sequencing (scRNA-seq) using Unique Molecular Identifier (UMI) counts, per-gene counts across cells are commonly modeled by a Gamma–Poisson mixture, which yields a Negative Binomial (NB) marginal distribution. In this formulation, the conditional count $X \\mid \\lambda$ is assumed to follow a Poisson distribution with rate $\\lambda$, written as $X \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$, and the rate $\\lambda$ varies across cells due to biological heterogeneity and technical effects. Suppose $\\lambda$ follows a Gamma distribution with mean $E[\\lambda] = \\mu$ and variance $\\mathrm{Var}(\\lambda) = \\alpha \\mu^{2}$, where $\\alpha \\geq 0$ is an overdispersion parameter that quantifies extra-Poisson variability.\n\nUsing only the laws of total expectation and total variance, derive the variance of $X$ in terms of $\\mu$ and $\\alpha$. Then, based on the method of moments, derive an estimator $\\hat{\\alpha}$ that equates the theoretical variance to the sample variance $\\hat{v}$ and enforces the non-negativity constraint $\\alpha \\geq 0$. For the gene $G$ observed across $n=6$ single cells with UMI counts $\\{2, 0, 3, 1, 4, 0\\}$, compute the sample mean $\\hat{\\mu}$ and the unbiased sample variance $\\hat{v}$, defined by\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i}, \\quad\n\\hat{v} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\hat{\\mu})^{2}\n$$\nand evaluate your derived $\\hat{\\alpha}$ using these estimates. Provide the final numerical value of $\\hat{\\alpha}$ in exact form (no rounding). Briefly interpret the result in relation to the Poisson limit and discuss its implication for marker gene discovery in differential expression analysis.",
            "solution": "The problem asks for several derivations and calculations based on a Gamma-Poisson mixture model for single-cell count data. Let $X$ be the random variable representing the UMI count for a gene in a single cell.\n\nFirst, we derive the variance of $X$ in terms of the mean expression $\\mu$ and the overdispersion parameter $\\alpha$. We are given the conditional distribution $X \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$ and the properties of the rate parameter $\\lambda$: $E[\\lambda] = \\mu$ and $\\mathrm{Var}(\\lambda) = \\alpha \\mu^{2}$.\n\nWe use the law of total variance, which states:\n$$\n\\mathrm{Var}(X) = E[\\mathrm{Var}(X \\mid \\lambda)] + \\mathrm{Var}(E[X \\mid \\lambda])\n$$\nFor a Poisson distribution with rate $\\lambda$, both the mean and the variance are equal to the rate parameter. Thus, we have:\n$$\nE[X \\mid \\lambda] = \\lambda\n$$\n$$\n\\mathrm{Var}(X \\mid \\lambda) = \\lambda\n$$\nSubstituting these into the law of total variance gives:\n$$\n\\mathrm{Var}(X) = E[\\lambda] + \\mathrm{Var}(\\lambda)\n$$\nUsing the given properties of $\\lambda$, $E[\\lambda] = \\mu$ and $\\mathrm{Var}(\\lambda) = \\alpha \\mu^{2}$, we obtain the expression for the variance of $X$:\n$$\n\\mathrm{Var}(X) = \\mu + \\alpha \\mu^{2}\n$$\nThe mean of $X$ can be found using the law of total expectation: $E[X] = E[E[X \\mid \\lambda]] = E[\\lambda] = \\mu$. Thus, the marginal distribution of $X$ has mean $\\mu$ and variance $\\mu + \\alpha \\mu^{2}$. This marginal distribution is the Negative Binomial distribution.\n\nNext, we derive an estimator for $\\alpha$ using the method of moments. This method involves equating the theoretical moments of the distribution to the sample moments calculated from the data. The first two moments are the mean and the variance.\nLet $\\hat{\\mu}$ be the sample mean and $\\hat{v}$ be the unbiased sample variance. We replace the population moments in our variance expression with these sample estimators:\n$$\n\\hat{v} = \\hat{\\mu} + \\hat{\\alpha} \\hat{\\mu}^{2}\n$$\nWe can now solve for the estimator $\\hat{\\alpha}$. If $\\hat{\\mu} > 0$, we have:\n$$\n\\hat{\\alpha} \\hat{\\mu}^{2} = \\hat{v} - \\hat{\\mu}\n$$\n$$\n\\hat{\\alpha} = \\frac{\\hat{v} - \\hat{\\mu}}{\\hat{\\mu}^{2}}\n$$\nThe problem states that $\\alpha \\geq 0$. To enforce this constraint, the estimator must be non-negative. This leads to the final form of the estimator:\n$$\n\\hat{\\alpha} = \\max\\left(0, \\frac{\\hat{v} - \\hat{\\mu}}{\\hat{\\mu}^{2}}\\right)\n$$\nThis form ensures that if the sample variance is less than the sample mean (a situation that can arise from sampling variability, particularly if the true distribution is Poisson), the estimated overdispersion is clipped to $0$.\n\nNow, we compute the numerical value of $\\hat{\\alpha}$ for the gene $G$ with observed UMI counts $\\{2, 0, 3, 1, 4, 0\\}$ across $n=6$ cells.\nFirst, we calculate the sample mean $\\hat{\\mu}$:\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} = \\frac{1}{6}(2 + 0 + 3 + 1 + 4 + 0) = \\frac{10}{6} = \\frac{5}{3}\n$$\nNext, we calculate the unbiased sample variance $\\hat{v}$:\n$$\n\\hat{v} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_{i} - \\hat{\\mu})^{2} = \\frac{1}{5} \\left[ \\left(2 - \\frac{5}{3}\\right)^{2} + \\left(0 - \\frac{5}{3}\\right)^{2} + \\left(3 - \\frac{5}{3}\\right)^{2} + \\left(1 - \\frac{5}{3}\\right)^{2} + \\left(4 - \\frac{5}{3}\\right)^{2} + \\left(0 - \\frac{5}{3}\\right)^{2} \\right]\n$$\n$$\n\\hat{v} = \\frac{1}{5} \\left[ \\left(\\frac{1}{3}\\right)^{2} + \\left(-\\frac{5}{3}\\right)^{2} + \\left(\\frac{4}{3}\\right)^{2} + \\left(-\\frac{2}{3}\\right)^{2} + \\left(\\frac{7}{3}\\right)^{2} + \\left(-\\frac{5}{3}\\right)^{2} \\right]\n$$\n$$\n\\hat{v} = \\frac{1}{5} \\left[ \\frac{1}{9} + \\frac{25}{9} + \\frac{16}{9} + \\frac{4}{9} + \\frac{49}{9} + \\frac{25}{9} \\right] = \\frac{1}{5} \\left( \\frac{1+25+16+4+49+25}{9} \\right) = \\frac{1}{5} \\left( \\frac{120}{9} \\right) = \\frac{24}{9} = \\frac{8}{3}\n$$\nHaving computed $\\hat{\\mu} = \\frac{5}{3}$ and $\\hat{v} = \\frac{8}{3}$, we can now evaluate $\\hat{\\alpha}$.\nThe numerator is $\\hat{v} - \\hat{\\mu} = \\frac{8}{3} - \\frac{5}{3} = \\frac{3}{3} = 1$.\nThe denominator is $\\hat{\\mu}^{2} = \\left(\\frac{5}{3}\\right)^{2} = \\frac{25}{9}$.\nThe ratio is $\\frac{\\hat{v} - \\hat{\\mu}}{\\hat{\\mu}^{2}} = \\frac{1}{25/9} = \\frac{9}{25}$.\nSince $\\frac{9}{25} > 0$, the estimator is:\n$$\n\\hat{\\alpha} = \\frac{9}{25}\n$$\n\nFinally, we briefly interpret this result. The overdispersion parameter $\\alpha$ quantifies the extent to which the variance of the data exceeds that expected from a Poisson distribution. For a Poisson process, $\\mathrm{Var}(X) = E[X]$, which corresponds to $\\alpha=0$. Our estimate, $\\hat{\\alpha} = \\frac{9}{25} = 0.36$, is positive, indicating that the observed variance ($\\hat{v} = \\frac{8}{3} \\approx 2.67$) is substantially larger than the mean ($\\hat{\\mu} = \\frac{5}{3} \\approx 1.67$). This phenomenon is known as overdispersion and is characteristic of scRNA-seq data.\n\nThe implication for marker gene discovery is critical. Statistical tests for differential expression that assume a Poisson distribution (i.e., assume $\\alpha=0$) would use an incorrectly small variance estimate. This leads to an inflation of statistical significance and a high false positive rate, where genes are incorrectly identified as markers. Models that account for overdispersion, such as those based on the Negative Binomial distribution, incorporate gene-specific estimates of $\\alpha$ to provide a more accurate assessment of variance. This allows for a more rigorous statistical test, improving the reliability of marker gene discovery by properly controlling for the high biological and technical variability inherent in single-cell data. The positive value of $\\hat{\\alpha}$ for gene $G$ demonstrates the necessity of using such an overdispersed model for any differential analysis involving this gene.",
            "answer": "$$\\boxed{\\frac{9}{25}}$$"
        },
        {
            "introduction": "When testing thousands of genes for differential expression simultaneously, the probability of obtaining false positives by chance becomes extremely high. To address this, we must correct for multiple comparisons, and the most common approach is to control the False Discovery Rate (FDR). This exercise provides a hands-on walkthrough of the Benjamini-Hochberg procedure, empowering you to convert raw $p$-values into adjusted $p$-values and generate a statistically robust list of significant marker genes from a large-scale experiment.",
            "id": "3301279",
            "problem": "In a single-cell ribonucleic acid sequencing (scRNA-seq) experiment, you perform differential expression testing for candidate marker genes of a target cell cluster using a well-calibrated gene-level test whose $p$-values under true null hypotheses are independently and identically distributed as $\\mathrm{Uniform}(0,1)$. The notion of False Discovery Rate (FDR) is defined as the expected proportion of false rejections among all rejections, namely $\\mathrm{FDR} = \\mathbb{E}\\!\\left[\\frac{V}{\\max(R,1)}\\right]$, where $V$ is the number of false rejections and $R$ is the total number of rejections. Under independence or Positive Regression Dependence on a Subset (PRDS), the Benjamini–Hochberg (BH) step-up procedure is known to control FDR at a target level $q$.\n\nYou have obtained $p$-values for five genes in the target cluster versus all other cells: $\\{0.001, 0.02, 0.03, 0.2, 0.6\\}$. Assume the independence or PRDS condition holds. Starting from the fundamental definition of $p$-values and the FDR control objective, and invoking only the standard BH step-up decision rule (without assuming any shortcut formulas for adjusted $p$-values), derive the BH-adjusted $p$-values in the original gene order and identify which genes are discoveries at target FDR level $q = 0.1$.\n\nProvide your final answer as a single row vector containing, in order: the five BH-adjusted $p$-values in the original gene order, followed by five binary indicators (with $1$ indicating a discovery and $0$ indicating a non-discovery) in the original gene order. No rounding is required.",
            "solution": "The problem requires the calculation of Benjamini–Hochberg (BH) adjusted $p$-values for a set of five gene-level test results and the identification of significant discoveries at a specified False Discovery Rate (FDR) level. The calculation must be derived from the fundamental principles of the BH procedure.\n\nLet $m$ be the total number of hypotheses tested, which is the number of genes. In this case, $m=5$.\nThe observed raw $p$-values are given as the set $\\{0.001, 0.02, 0.03, 0.2, 0.6\\}$. Let us denote these as $P_1, P_2, P_3, P_4, P_5$ in their original order:\n$P_1 = 0.001$\n$P_2 = 0.02$\n$P_3 = 0.03$\n$P_4 = 0.2$\n$P_5 = 0.6$\n\nThe target FDR level is $q = 0.1$.\n\nThe BH procedure first requires ordering the raw $p$-values in a non-decreasing sequence. Let $P_{(1)} \\le P_{(2)} \\le \\dots \\le P_{(m)}$ be the sorted $p$-values.\nIn this specific case, the original $p$-values are already sorted, so the sorted list is identical to the original list:\n$P_{(1)} = 0.001$\n$P_{(2)} = 0.02$\n$P_{(3)} = 0.03$\n$P_{(4)} = 0.2$\n$P_{(5)} = 0.6$\n\nThe BH step-up decision rule states that for a given FDR level $q$, one finds the largest integer $k \\in \\{1, \\dots, m\\}$ such that $P_{(k)} \\le \\frac{k}{m}q$. If such a $k$ exists, all hypotheses $H_{(i)}$ for $i=1, \\dots, k$ are rejected. Otherwise, no hypotheses are rejected.\n\nThe BH-adjusted $p$-value for a specific hypothesis $H_i$ with raw $p$-value $P_i$ is defined as the smallest FDR level $q$ at which the hypothesis $H_i$ would be rejected by the BH procedure. Let the rank of $P_i$ in the sorted list be $j$, so $P_i = P_{(j)}$. According to the BH rule, hypothesis $H_{(j)}$ is rejected at level $q$ if there exists some index $k' \\ge j$ such that $P_{(k')} \\le \\frac{k'}{m}q$. This is equivalent to requiring $q \\ge \\frac{m P_{(k')}}{k'}$ for at least one $k' \\ge j$. To find the smallest $q$ that satisfies this, we must find the minimum of these lower bounds over all possible $k'$. Therefore, the adjusted $p$-value for the hypothesis with raw $p$-value $P_{(j)}$, denoted $p_{\\text{adj},(j)}$, is:\n$$p_{\\text{adj},(j)} = \\min_{k=j, \\dots, m} \\left\\{ \\frac{m P_{(k)}}{k} \\right\\}$$\nAdditionally, adjusted $p$-values must be capped at $1$ and must be monotonically non-decreasing with the raw $p$-values. The formula above can result in a non-monotonic sequence if applied naively. For instance, it's possible that for $P_{(j)}  P_{(j+1)}$, the calculated $p_{\\text{adj},(j)}$ could be greater than $p_{\\text{adj},(j+1)}$. To enforce monotonicity, the adjusted $p$-value for the $j$-th hypothesis is taken as the minimum of all adjusted values for hypotheses with larger or equal raw $p$-values. This leads to the standard computational algorithm.\n\nLet's compute the adjusted $p$-values. A common and efficient way to enforce this monotonicity is to compute the values iteratively, starting from the largest raw $p$-value and moving downwards.\nLet $p_{\\text{adj},(i)}$ be the adjusted $p$-value corresponding to the raw $p$-value $P_{(i)}$.\n\nFor the largest $p$-value, $P_{(m)}$:\n$p_{\\text{adj},(m)} = \\min\\left(1, \\frac{m}{m} P_{(m)}\\right) = P_{(m)}$\nFor $i = m-1, \\dots, 1$:\n$p_{\\text{adj},(i)} = \\min\\left(p_{\\text{adj},(i+1)}, \\frac{m}{i} P_{(i)}\\right)$\nThis process ensures that $p_{\\text{adj},(1)} \\le p_{\\text{adj},(2)} \\le \\dots \\le p_{\\text{adj},(m)}$.\n\nLet's apply this procedure to our data, where $m=5$:\n1.  For $i=5$:\n    $p_{\\text{adj},(5)} = \\frac{5}{5} P_{(5)} = 1 \\times 0.6 = 0.6$.\n\n2.  For $i=4$:\n    $p_{\\text{adj},(4)} = \\min\\left(p_{\\text{adj},(5)}, \\frac{5}{4} P_{(4)}\\right) = \\min\\left(0.6, \\frac{5}{4} \\times 0.2\\right) = \\min(0.6, 0.25) = 0.25$.\n\n3.  For $i=3$:\n    $p_{\\text{adj},(3)} = \\min\\left(p_{\\text{adj},(4)}, \\frac{5}{3} P_{(3)}\\right) = \\min\\left(0.25, \\frac{5}{3} \\times 0.03\\right) = \\min(0.25, 0.05) = 0.05$.\n\n4.  For $i=2$:\n    $p_{\\text{adj},(2)} = \\min\\left(p_{\\text{adj},(3)}, \\frac{5}{2} P_{(2)}\\right) = \\min\\left(0.05, \\frac{5}{2} \\times 0.02\\right) = \\min(0.05, 0.05) = 0.05$.\n\n5.  For $i=1$:\n    $p_{\\text{adj},(1)} = \\min\\left(p_{\\text{adj},(2)}, \\frac{5}{1} P_{(1)}\\right) = \\min\\left(0.05, \\frac{5}{1} \\times 0.001\\right) = \\min(0.05, 0.005) = 0.005$.\n\nThe calculated adjusted $p$-values, corresponding to the sorted raw $p$-values, are $\\{0.005, 0.05, 0.05, 0.25, 0.6\\}$. Since the original $p$-values were already sorted, these are also the adjusted $p$-values in the original gene order.\n$p_{1, \\text{adj}} = 0.005$\n$p_{2, \\text{adj}} = 0.05$\n$p_{3, \\text{adj}} = 0.05$\n$p_{4, \\text{adj}} = 0.25$\n$p_{5, \\text{adj}} = 0.6$\n\nNext, we identify which genes are \"discoveries\" at the target FDR level $q = 0.1$. A gene is considered a discovery if its adjusted $p$-value is less than or equal to $q$.\nLet $D_i$ be the binary indicator for gene $i$ ($1$ for discovery, $0$ for non-discovery).\n$D_i = 1$ if $p_{i, \\text{adj}} \\le q$, and $D_i = 0$ if $p_{i, \\text{adj}}  q$.\n\n1.  Gene 1: $p_{1, \\text{adj}} = 0.005 \\le 0.1 \\implies D_1 = 1$.\n2.  Gene 2: $p_{2, \\text{adj}} = 0.05 \\le 0.1 \\implies D_2 = 1$.\n3.  Gene 3: $p_{3, \\text{adj}} = 0.05 \\le 0.1 \\implies D_3 = 1$.\n4.  Gene 4: $p_{4, \\text{adj}} = 0.25  0.1 \\implies D_4 = 0$.\n5.  Gene 5: $p_{5, \\text{adj}} = 0.6  0.1 \\implies D_5 = 0$.\n\nThe discoveries are the first three genes. The final answer requires a single row vector containing the five adjusted $p$-values followed by the five binary indicators, all in the original gene order.\nThe vector is composed of $\\{p_{1, \\text{adj}}, p_{2, \\text{adj}}, p_{3, \\text{adj}}, p_{4, \\text{adj}}, p_{5, \\text{adj}}, D_1, D_2, D_3, D_4, D_5\\}$.\nThis corresponds to the values $\\{0.005, 0.05, 0.05, 0.25, 0.6, 1, 1, 1, 0, 0\\}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.005  0.05  0.05  0.25  0.6  1  1  1  0  0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}