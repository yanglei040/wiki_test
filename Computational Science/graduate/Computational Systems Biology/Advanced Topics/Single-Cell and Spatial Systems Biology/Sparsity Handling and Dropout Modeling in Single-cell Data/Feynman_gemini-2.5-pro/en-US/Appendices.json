{
    "hands_on_practices": [
        {
            "introduction": "Before tackling the complexities of zero-inflation, it's essential to master the foundational building block of count data modeling. This exercise  walks you through a cornerstone of Bayesian statistics: the conjugate Gamma-Poisson model. By deriving the posterior distribution for a gene's expression rate, you will gain hands-on practice with how prior beliefs are updated by observed data, a core principle that underpins many sophisticated algorithms in genomics.",
            "id": "3349825",
            "problem": "In single-cell RNA sequencing (scRNA-seq), sparse observed counts per gene are often modeled using a conjugate Gamma-Poisson framework without explicit zero-inflation to reflect technical dropout, thereby attributing zeros to low underlying rates. Consider a single gene indexed by $g$ in a single cell indexed by $c$ with observed unique molecular identifier (UMI) count $X_{gc} \\in \\{0,1,2,\\ldots\\}$. Assume a known, strictly positive exposure or size factor $s_{c} > 0$ that captures differences in sequencing depth and efficiency. Let the latent expression rate parameter for gene $g$ in cell $c$ be $\\theta_{gc} > 0$.\n\nAssume the following model components:\n- Likelihood: $X_{gc} \\mid \\theta_{gc} \\sim \\mathrm{Poisson}(s_{c}\\,\\theta_{gc})$.\n- Prior: $\\theta_{gc} \\sim \\mathrm{Gamma}(a_{g}, b_{g})$ with shape $a_{g} > 0$ and rate $b_{g} > 0$, with density $p(\\theta_{gc}) = \\dfrac{b_{g}^{a_{g}}}{\\Gamma(a_{g})}\\,\\theta_{gc}^{a_{g}-1}\\exp(-b_{g}\\theta_{gc})$.\n\nStarting from Bayes’ rule and using the standard forms of the Poisson probability mass function and Gamma probability density function, derive the closed-form posterior distribution $p(\\theta_{gc}\\mid X_{gc})$ under this conjugate model. Then, compute and simplify the closed-form expressions for the posterior mean $E[\\theta_{gc}\\mid X_{gc}]$ and posterior variance $\\mathrm{Var}[\\theta_{gc}\\mid X_{gc}]$ in terms of $a_{g}$, $b_{g}$, $s_{c}$, and $X_{gc}$.\n\nProvide the final answer as analytic expressions. No rounding is required. Express multiple quantities as a single row matrix using the $\\mathrm{pmatrix}$ convention. No physical units are involved since all quantities are dimensionless rates and counts.",
            "solution": "The goal is to derive the posterior distribution $p(\\theta_{gc} \\mid X_{gc})$ and its moments (mean and variance) using the specified Gamma-Poisson conjugate model.\n\n### Derivation of the Posterior Distribution\n\nAccording to Bayes' rule, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto p(X_{gc} \\mid \\theta_{gc}) \\, p(\\theta_{gc})$$\n\nThe likelihood function is given by the Poisson probability mass function (PMF) with rate $\\lambda = s_{c}\\theta_{gc}$:\n$$p(X_{gc} \\mid \\theta_{gc}) = \\frac{(s_{c}\\theta_{gc})^{X_{gc}} \\exp(-s_{c}\\theta_{gc})}{X_{gc}!}$$\n\nThe prior distribution for $\\theta_{gc}$ is the Gamma PDF with shape $a_g$ and rate $b_g$:\n$$p(\\theta_{gc}) = \\frac{b_{g}^{a_{g}}}{\\Gamma(a_{g})} \\theta_{gc}^{a_{g}-1} \\exp(-b_{g}\\theta_{gc})$$\n\nSubstituting these into Bayes' rule and dropping all terms that are constant with respect to $\\theta_{gc}$:\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto \\left( (\\theta_{gc})^{X_{gc}} \\exp(-s_{c}\\theta_{gc}) \\right) \\left( \\theta_{gc}^{a_{g}-1} \\exp(-b_{g}\\theta_{gc}) \\right)$$\n\nCombining terms by adding the exponents:\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto \\theta_{gc}^{(a_{g} + X_{gc}) - 1} \\exp(-(b_{g} + s_{c})\\theta_{gc})$$\n\nThis expression is the kernel of a Gamma distribution. By comparing it to the standard form of a Gamma PDF, $p(y) \\propto y^{\\alpha-1}e^{-\\beta y}$, we can identify the posterior parameters:\n- Posterior shape: $\\alpha' = a_{g} + X_{gc}$\n- Posterior rate: $\\beta' = b_{g} + s_{c}$\n\nThus, the posterior distribution is a Gamma distribution:\n$$\\theta_{gc} \\mid X_{gc} \\sim \\mathrm{Gamma}(a_{g} + X_{gc}, b_{g} + s_{c})$$\n\n### Calculation of Posterior Mean and Variance\n\nFor a random variable $Y \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, the mean and variance are given by $E[Y] = \\alpha/\\beta$ and $\\mathrm{Var}[Y] = \\alpha/\\beta^2$. Applying these formulas to our posterior distribution with shape $\\alpha'$ and rate $\\beta'$ yields:\n\nThe posterior mean of $\\theta_{gc}$ is:\n$$E[\\theta_{gc} \\mid X_{gc}] = \\frac{\\alpha'}{\\beta'} = \\frac{a_{g} + X_{gc}}{b_{g} + s_{c}}$$\n\nThe posterior variance of $\\theta_{gc}$ is:\n$$\\mathrm{Var}[\\theta_{gc} \\mid X_{gc}] = \\frac{\\alpha'}{(\\beta')^2} = \\frac{a_{g} + X_{gc}}{(b_{g} + s_{c})^2}$$\n\nThese expressions represent the updated estimates for the mean and variance of the latent expression rate after observing the count data $X_{gc}$. The posterior mean is a weighted average of the prior mean $\\frac{a_g}{b_g}$ and the data-derived Maximum Likelihood Estimate $\\frac{X_{gc}}{s_c}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\dfrac{a_{g} + X_{gc}}{b_{g} + s_{c}} & \\dfrac{a_{g} + X_{gc}}{(b_{g} + s_{c})^{2}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Single-cell data is notoriously sparse, with an abundance of zeros that cannot be explained by simple count distributions alone. This practice  introduces the Zero-Inflated Negative Binomial (ZINB) model, a workhorse for distinguishing biological silence from technical dropouts. Deriving the updates for the Expectation-Maximization (EM) algorithm will equip you with the essential statistical machinery for fitting latent variable models, a critical skill for any computational biologist working with sparse data.",
            "id": "3349849",
            "problem": "Consider a single gene measured across $n$ cells in single-cell ribonucleic acid sequencing (scRNA-seq). The observed count in cell $i$ is $y_i \\in \\{0,1,2,\\dots\\}$, for $i=1,\\dots,n$. Due to technical dropout, the data exhibit excess zeros beyond what a Negative Binomial (NB) distribution would generate. A standard modeling approach is the Zero-Inflated Negative Binomial (ZINB) mixture model. Let $d_i \\in \\{0,1\\}$ be a latent binary indicator for dropout in cell $i$, with $d_i=1$ indicating a structural zero caused by dropout and $d_i=0$ indicating that the count arose from the NB process. Assume\n$$\nd_i \\sim \\mathrm{Bernoulli}(\\pi),\n\\quad\ny_i \\mid d_i\n\\sim\n\\begin{cases}\n0 & \\text{if } d_i = 1, \\\\\n\\mathrm{NB}(\\mu,\\alpha) & \\text{if } d_i = 0,\n\\end{cases}\n$$\nwhere $\\pi \\in (0,1)$ is the dropout probability, $\\mu>0$ is the NB mean, and $\\alpha>0$ is the NB dispersion (shape) parameter. Use the following well-tested forms and facts:\n- The Negative Binomial (NB) probability mass function parameterized by mean $\\mu$ and dispersion $\\alpha$ is\n$$\np_{\\mathrm{NB}}(y \\mid \\mu,\\alpha)\n=\n\\frac{\\Gamma(y+\\alpha)}{\\Gamma(\\alpha)\\,y!}\n\\left(\\frac{\\alpha}{\\alpha+\\mu}\\right)^{\\alpha}\n\\left(\\frac{\\mu}{\\alpha+\\mu}\\right)^{y}.\n$$\n- The digamma function $\\psi(x)$ is the derivative of $\\ln\\Gamma(x)$, and the trigamma function $\\psi'(x)$ is its derivative.\n\nStarting from the complete-data likelihood and the definition of the Expectation-Maximization (EM) algorithm (Expectation-Maximization (EM): iteratively maximize a lower bound on the observed-data log-likelihood by alternating between computing conditional expectations of latent variables and maximizing the expected complete-data log-likelihood), derive an EM algorithm for the ZINB model with latent dropout indicators. Under a mean-field approximation in which the variational posterior factorizes across cells as $q(d_1,\\dots,d_n) = \\prod_{i=1}^n q_i(d_i)$ with $q_i(d_i=1) \\in [0,1]$, perform the following:\n\n- Derive the E-step update giving $q_i(d_i=1)$ for each cell $i$, explicitly in terms of $\\pi$, $\\mu$, and $\\alpha$ and the observed $y_i$.\n- Derive the M-step updates for $\\pi$, $\\mu$, and $\\alpha$ that maximize the expected complete-data log-likelihood under the mean-field approximation. Provide a closed-form update for $\\pi$ and $\\mu$, and provide an explicit Newton step for $\\alpha$ in terms of $\\psi(\\cdot)$ and $\\psi'(\\cdot)$.\n- Clearly identify the sufficient statistics or weights that appear in the updates.\n\nReport only the closed-form M-step update for the NB mean parameter $\\mu$ as your final answer. No numerical evaluation is required, and no rounding is needed. Express the final answer as a single analytic expression.",
            "solution": "The goal is to derive the Expectation-Maximization (EM) algorithm for estimating the parameters $(\\pi, \\mu, \\alpha)$ of the specified Zero-Inflated Negative Binomial (ZINB) model. The algorithm iterates between an E-step (computing the expected values of latent variables) and an M-step (maximizing the expected complete-data log-likelihood).\n\n### Complete-Data Log-Likelihood\nThe complete data for the model are $\\{(y_i, d_i)\\}_{i=1}^n$, where $y_i$ is the observed count and $d_i$ is the latent dropout indicator. The joint probability of a single observation $(y_i, d_i)$ is:\n$$p(y_i, d_i \\mid \\pi, \\mu, \\alpha) = [\\pi \\cdot \\mathbf{1}_{y_i=0}]^{d_i} [(1-\\pi) \\cdot p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)]^{1-d_i}$$\nwhere $\\mathbf{1}_{y_i=0}$ is an indicator function. This formulation correctly implies that if $y_i > 0$, then $d_i$ must be 0.\n\nThe complete-data log-likelihood $\\mathcal{L}_c(\\theta; Y, D)$ is the sum of the logarithms of these probabilities:\n$$\\mathcal{L}_c(\\theta) = \\sum_{i=1}^n \\left\\{ d_i \\log\\pi + (1-d_i) [\\log(1-\\pi) + \\log p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)] \\right\\}$$\nNote that the $d_i \\log\\pi$ term only contributes if $y_i=0$.\n\n### E-Step: Computing Posterior Probabilities of Dropout\nIn the E-step, we compute the posterior probability of dropout for each cell, given the observed data $y_i$ and the current parameter estimates $\\theta^{(t)} = (\\pi^{(t)}, \\mu^{(t)}, \\alpha^{(t)})$. Let's denote this probability by $\\gamma_i$:\n$$\\gamma_i \\equiv p(d_i=1 \\mid y_i, \\theta^{(t)})$$\n\n- If $y_i > 0$, a dropout is impossible, so $d_i$ must be 0. Therefore, $\\gamma_i = 0$.\n- If $y_i = 0$, the zero could be a dropout ($d_i=1$) or a biological zero from the NB component ($d_i=0$). Using Bayes' theorem:\n$$\n\\gamma_i = p(d_i=1 \\mid y_i=0, \\theta^{(t)}) = \\frac{p(y_i=0 \\mid d_i=1, \\theta^{(t)}) p(d_i=1 \\mid \\theta^{(t)})}{p(y_i=0 \\mid d_i=1, \\theta^{(t)}) p(d_i=1 \\mid \\theta^{(t)}) + p(y_i=0 \\mid d_i=0, \\theta^{(t)}) p(d_i=0 \\mid \\theta^{(t)})}\n$$\nSubstituting the model definitions:\n$$\n\\gamma_i = \\frac{1 \\cdot \\pi^{(t)}}{1 \\cdot \\pi^{(t)} + p_{\\mathrm{NB}}(0 \\mid \\mu^{(t)}, \\alpha^{(t)}) \\cdot (1-\\pi^{(t)})} = \\frac{\\pi^{(t)}}{\\pi^{(t)} + (1-\\pi^{(t)}) \\left(\\frac{\\alpha^{(t)}}{\\alpha^{(t)}+\\mu^{(t)}}\\right)^{\\alpha^{(t)}}}\n$$\nThese probabilities $\\gamma_i$ are the responsibilities computed in the E-step.\n\n### M-Step: Maximizing the Expected Log-Likelihood\nIn the M-step, we update the parameters by maximizing the expected complete-data log-likelihood, $Q(\\theta \\mid \\theta^{(t)})$, where the expectation is taken with respect to the posterior distribution of the latent variables computed in the E-step.\n$$Q(\\theta \\mid \\theta^{(t)}) = E_{D \\mid Y, \\theta^{(t)}}[\\mathcal{L}_c(\\theta)]$$\n$$Q(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^n \\left[ \\gamma_i \\log \\pi + (1-\\gamma_i) \\log(1-\\pi) \\right] + \\sum_{i=1}^n (1-\\gamma_i) \\log p_{\\mathrm{NB}}(y_i|\\mu,\\alpha)$$\nWe maximize this function with respect to $\\pi$, $\\mu$, and $\\alpha$ separately.\n\n**M-step update for $\\pi$**:\nMaximizing the first term with respect to $\\pi$ by taking the derivative and setting it to zero yields the update:\n$$\\pi^{(t+1)} = \\frac{\\sum_{i=1}^n \\gamma_i}{n}$$\nThis is the average responsibility for the dropout component.\n\n**M-step update for $\\mu$**:\nTo update $\\mu$, we maximize the second term, which is a weighted NB log-likelihood.\n$$\\frac{\\partial}{\\partial \\mu} \\sum_{i=1}^n (1-\\gamma_i) \\log p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha^{(t)}) = \\sum_{i=1}^n (1-\\gamma_i) \\left( \\frac{y_i}{\\mu} - \\frac{y_i+\\alpha^{(t)}}{\\alpha^{(t)}+\\mu} \\right) = 0$$\nSolving for $\\mu$ gives the closed-form update:\n$$\\mu^{(t+1)} = \\frac{\\sum_{i=1}^n (1-\\gamma_i)y_i}{\\sum_{i=1}^n (1-\\gamma_i)}$$\nThis is the weighted average of the counts, where each count is weighted by the probability that it did not come from a dropout event.\n\n**M-step update for $\\alpha$**:\nThe update for the dispersion parameter $\\alpha$ does not have a closed-form solution. It requires numerically maximizing the weighted log-likelihood term with respect to $\\alpha$, typically using a Newton-Raphson step involving the digamma $\\psi(\\cdot)$ and trigamma $\\psi'(\\cdot)$ functions.",
            "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} (1-\\gamma_i) y_i}{\\sum_{i=1}^{n} (1-\\gamma_i)}}$$"
        },
        {
            "introduction": "After mastering the mechanics of advanced dropout models, it is crucial to understand *why* they are necessary. This exercise  presents a powerful thought experiment to illustrate a critical lesson: the nature of the missing data mechanism matters immensely. By analytically deriving the bias that arises from incorrectly assuming a simple dropout process (Missing At Random), you will gain a deeper appreciation for the dangers of naive imputation and the importance of mechanism-aware models for handling single-cell sparsity.",
            "id": "3349850",
            "problem": "Consider a single lowly expressed gene measured by Unique Molecular Identifier (UMI) counts across a large population of single cells. Assume the true per-cell counts are independent and identically distributed according to a Poisson distribution with mean $\\lambda$, so that $X \\sim \\mathrm{Poisson}(\\lambda)$ models the true count in a cell. The observed count $Y$ is affected by a dropout mechanism that depends on the unobserved $X$ (i.e., Missing Not At Random (MNAR)): if $X=0$ then $Y=0$; if $X=1$ then $Y=0$ with probability $d \\in (0,1)$ and $Y=1$ otherwise; if $X \\ge 2$ then $Y=X$ (no dropout). This mechanism is a proof-of-concept representation of stronger dropout for low counts.\n\nA practitioner incorrectly assumes a Missing At Random (MAR) mechanism with a constant dropout probability $r \\in (0,1)$ that is independent of $X$ and corrects the observed mean by inverse probability weighting, estimating the gene’s mean expression by\n$$\n\\mu_{\\mathrm{MAR}} \\equiv \\frac{\\mathbb{E}[Y]}{1-r}.\n$$\nSuppose the practitioner is somehow given the exact overall dropout probability $r$ for this gene, defined as the probability that a dropout event occurs in a randomly chosen cell under the true data-generating process. You may assume that $r$ is known and is the same $r$ appearing in the MAR correction above. Using only the fundamental properties of the Poisson distribution and the definitions of Missing At Random (MAR) and Missing Not At Random (MNAR), derive the bias of this MAR-based estimator relative to the true mean, defined as\n$$\n\\mathrm{Bias}(\\lambda,r) \\equiv \\mu_{\\mathrm{MAR}} - \\lambda,\n$$\nand express it in closed form as a function of $\\lambda$ and $r$, eliminating the nuisance parameter $d$. Your final answer must be a single closed-form analytic expression for $\\mathrm{Bias}(\\lambda,r)$ in terms of $\\lambda$ and $r$ only. No numerical evaluation is required, and no rounding is needed.",
            "solution": "The problem asks for the bias of a Missing At Random (MAR) based estimator for the mean expression, when the true data-generating process is Missing Not At Random (MNAR). The bias is defined as $\\mathrm{Bias}(\\lambda,r) \\equiv \\mu_{\\mathrm{MAR}} - \\lambda$, where $\\mu_{\\mathrm{MAR}} = \\frac{\\mathbb{E}[Y]}{1-r}$. Here, $\\lambda$ is the true mean of the unobserved count $X$, $Y$ is the observed count, and $r$ is the overall dropout probability. To find the bias, we must first derive an expression for the expected observed count, $\\mathbb{E}[Y]$, under the true MNAR data-generating process.\n\nThe true count $X$ is a random variable following a Poisson distribution with mean $\\lambda$, denoted as $X \\sim \\mathrm{Poisson}(\\lambda)$. Its probability mass function (PMF) is given by $P(X=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nThe relationship between the observed count $Y$ and the true count $X$ is given by the MNAR mechanism:\n1. If $X=0$, then $Y=0$.\n2. If $X=1$, then $Y=0$ with probability $d$ and $Y=1$ with probability $1-d$, where $d \\in (0,1)$.\n3. If $X \\ge 2$, then $Y=X$.\n\nWe can compute the expected value of $Y$ using the law of total expectation: $\\mathbb{E}[Y] = \\sum_{k=0}^{\\infty} \\mathbb{E}[Y | X=k] P(X=k)$.\nLet's determine the conditional expectations $\\mathbb{E}[Y | X=k]$ from the MNAR model:\n- For $k=0$: $\\mathbb{E}[Y | X=0] = 0$.\n- For $k=1$: $\\mathbb{E}[Y | X=1] = 0 \\cdot P(Y=0|X=1) + 1 \\cdot P(Y=1|X=1) = 0 \\cdot d + 1 \\cdot (1-d) = 1-d$.\n- For $k \\ge 2$: $\\mathbb{E}[Y | X=k] = k$.\n\nSubstituting these into the law of total expectation:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[Y | X=0]P(X=0) + \\mathbb{E}[Y | X=1]P(X=1) + \\sum_{k=2}^{\\infty} \\mathbb{E}[Y | X=k]P(X=k)\n$$\n$$\n\\mathbb{E}[Y] = (0)P(X=0) + (1-d)P(X=1) + \\sum_{k=2}^{\\infty} k P(X=k)\n$$\nThe expectation of the Poisson-distributed variable $X$ is $\\mathbb{E}[X] = \\lambda$, which can be written as:\n$$\n\\mathbb{E}[X] = \\sum_{k=0}^{\\infty} k P(X=k) = (0)P(X=0) + (1)P(X=1) + \\sum_{k=2}^{\\infty} k P(X=k) = P(X=1) + \\sum_{k=2}^{\\infty} k P(X=k)\n$$\nFrom this, we can express the sum $\\sum_{k=2}^{\\infty} k P(X=k)$ as $\\lambda - P(X=1)$. Substituting this into the expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = (1-d)P(X=1) + (\\lambda - P(X=1))\n$$\n$$\n\\mathbb{E}[Y] = P(X=1) - dP(X=1) + \\lambda - P(X=1)\n$$\n$$\n\\mathbb{E}[Y] = \\lambda - dP(X=1)\n$$\nThe problem requires the final bias to be a function of $\\lambda$ and $r$, so we must eliminate the nuisance parameter $d$. The parameter $r$ is defined as the overall dropout probability. A dropout event is when a true non-zero count is observed as zero. According to the MNAR model, this only occurs when $X=1$.\nTherefore, $r$ is the probability of the joint event $\\{Y=0 \\text{ and } X>0\\}$.\n$$\nr = P(Y=0, X>0)\n$$\nThe condition $X>0$ and $Y=0$ can only be met if $X=1$ and a dropout occurs. For $X \\ge 2$, $Y=X \\ge 2$, so $Y$ cannot be $0$. For $X=0$, $X$ is not greater than $0$. Thus, the event $\\{Y=0, X>0\\}$ is identical to the event $\\{Y=0, X=1\\}$.\n$$\nr = P(Y=0, X=1) = P(Y=0 | X=1) P(X=1)\n$$\nFrom the model definition, $P(Y=0 | X=1) = d$. So, we have the relationship:\n$$\nr = dP(X=1)\n$$\nNow we can substitute this directly into our expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = \\lambda - r\n$$\nWith this compact expression for $\\mathbb{E}[Y]$, we can find the MAR-based estimator's population value, $\\mu_{\\mathrm{MAR}}$:\n$$\n\\mu_{\\mathrm{MAR}} = \\frac{\\mathbb{E}[Y]}{1-r} = \\frac{\\lambda - r}{1-r}\n$$\nFinally, we can compute the bias, $\\mathrm{Bias}(\\lambda,r)$, by subtracting the true mean $\\lambda$:\n$$\n\\mathrm{Bias}(\\lambda,r) = \\mu_{\\mathrm{MAR}} - \\lambda = \\frac{\\lambda - r}{1-r} - \\lambda\n$$\nTo simplify, we find a common denominator:\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{\\lambda - r - \\lambda(1-r)}{1-r}\n$$\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{\\lambda - r - \\lambda + \\lambda r}{1-r}\n$$\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{\\lambda r - r}{1-r}\n$$\nFactoring out $r$ from the numerator gives the final closed-form expression for the bias:\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{r(\\lambda - 1)}{1-r}\n$$\nThis expression depends only on $\\lambda$ and $r$, as required.",
            "answer": "$$\n\\boxed{\\frac{r(\\lambda - 1)}{1-r}}\n$$"
        }
    ]
}