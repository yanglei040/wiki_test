{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering the Runge phenomenon is to understand its most effective cure: the use of non-uniformly spaced interpolation points. This practice guides you through the elegant derivation of Chebyshev-Lobatto nodes from a simple trigonometric mapping, revealing why their characteristic clustering at the interval endpoints is not an accident but a direct consequence of this mapping. By performing this derivation, you will gain a fundamental insight into how this specific node distribution stabilizes polynomial interpolation and conquers the oscillations that plague uniform grids .",
            "id": "3413813",
            "problem": "Consider polynomial interpolation within a spectral element basis employing Chebyshev polynomials for use in a Discontinuous Galerkin (DG) method on the interval $[-1,1]$. Angles must be treated in radians.\n\n1. Start from the defining relation of the Chebyshev polynomials of the first kind, $T_{k}(x) = \\cos(k \\arccos x)$, and the change of variables $x = \\cos \\xi$ with inverse $\\xi = \\arccos x$. Derive how a uniform grid in the angle variable $\\xi$, specifically $\\xi_{j} = j \\pi/n$ for $j = 0,1,\\dots,n$, transforms into the set of Chebyshev–Lobatto nodes in $x$, namely $x_{j} = \\cos(j \\pi/n)$. As part of your derivation, use the Jacobian $\\mathrm{d}\\xi/\\mathrm{d}x$ to explain the endpoint clustering of the Chebyshev–Lobatto nodes and why this clustering mitigates the Runge phenomenon for analytic $f$.\n\n2. Let $f(x) = \\frac{1}{1 + 25 x^{2}}$. For $n = 6$, construct the unique polynomial $p_{n}(x)$ of degree at most $n$ that interpolates $f$ at the Chebyshev–Lobatto nodes $x_{j} = \\cos(j \\pi/n)$, $j=0,1,\\dots,n$. Evaluate $p_{6}(x)$ at the point $x_{\\star} = \\cos(\\pi/6)$ and express your result as a single, exact analytic expression.\n\nYour final answer must be a single analytic expression. No rounding is required.",
            "solution": "The problem is divided into two parts. The first part requires a derivation and explanation regarding Chebyshev-Lobatto nodes. The second part requires the construction and evaluation of a specific interpolating polynomial.\n\n### Part 1: Derivation and Explanation of Chebyshev-Lobatto Nodes\n\nWe are given the change of variables $x = \\cos \\xi$ for $x \\in [-1, 1]$, which implies $\\xi \\in [0, \\pi]$. The inverse transformation is $\\xi = \\arccos x$.\n\nA uniform grid in the angle variable $\\xi$ is defined by the points $\\xi_{j} = \\frac{j \\pi}{n}$ for $j = 0, 1, \\dots, n$. Applying the transformation $x = \\cos \\xi$ to these points, we obtain the corresponding grid points in the $x$ domain:\n$$\nx_{j} = \\cos(\\xi_{j}) = \\cos\\left(\\frac{j \\pi}{n}\\right) \\quad \\text{for } j = 0, 1, \\dots, n.\n$$\nThese are, by definition, the Chebyshev-Lobatto nodes. This completes the derivation of the transformation from a uniform grid in $\\xi$ to the specified node set in $x$.\n\nTo explain the endpoint clustering, we examine the relationship between infinitesimal intervals $\\mathrm{d}x$ and $\\mathrm{d}\\xi$. The Jacobian of the inverse transformation $\\xi(x)$ is the derivative:\n$$\n\\frac{\\mathrm{d}\\xi}{\\mathrm{d}x} = \\frac{\\mathrm{d}}{\\mathrm{d}x}(\\arccos x) = -\\frac{1}{\\sqrt{1 - x^2}}.\n$$\nThe magnitude of this derivative, $\\left|\\frac{\\mathrm{d}\\xi}{\\mathrm{d}x}\\right| = \\frac{1}{\\sqrt{1 - x^2}}$, represents the local ratio of the length of an interval in $\\xi$ to the corresponding interval in $x$.\n\nA uniform grid in $\\xi$ implies a constant spacing $\\Delta\\xi = \\xi_{j+1} - \\xi_j = \\frac{\\pi}{n}$. The corresponding spacing in $x$, $\\Delta x_j = x_{j+1} - x_j$, is not constant. We can relate the spacings by $\\Delta\\xi \\approx \\left|\\frac{\\mathrm{d}\\xi}{\\mathrm{d}x}\\right| |\\Delta x|$. Rearranging for $|\\Delta x|$ gives:\n$$\n|\\Delta x| \\approx \\left|\\frac{\\mathrm{d}x}{\\mathrm{d}\\xi}\\right| \\Delta\\xi = |\\sin(\\xi)| \\Delta\\xi = \\sqrt{1-x^2} \\Delta\\xi.\n$$\nSince $\\Delta\\xi$ is constant, the spacing between nodes in the $x$ domain, $|\\Delta x|$, is proportional to $\\sqrt{1-x^2}$. As $x$ approaches the endpoints of the interval, $x \\to \\pm 1$, the term $\\sqrt{1-x^2}$ approaches $0$. Consequently, the spacing $|\\Delta x|$ becomes significantly smaller near the endpoints compared to the center of the interval (where $x=0$ and the spacing is maximal). This non-uniform spacing, denser at the boundaries, is known as endpoint clustering.\n\nThis clustering mitigates the Runge phenomenon. The error in polynomial interpolation of a function $f(x)$ by a polynomial $p_n(x)$ of degree at most $n$ is given by:\n$$\nf(x) - p_n(x) = \\frac{f^{(n+1)}(\\eta_x)}{(n+1)!} \\prod_{j=0}^{n} (x-x_j)\n$$\nfor some $\\eta_x \\in [-1, 1]$. The Runge phenomenon, characterized by large oscillations near the endpoints, occurs with uniformly spaced nodes because the nodal polynomial $\\omega_{n+1}(x) = \\prod_{j=0}^{n} (x-x_j)$ becomes very large in magnitude near the interval boundaries. The Chebyshev-Lobatto nodes counteract this in two ways. First, the node clustering places more constraints on the polynomial near the endpoints, taming oscillations where they tend to be largest. Second, the resulting nodal polynomial for Chebyshev-Lobatto nodes has a much smaller maximum magnitude on $[-1, 1]$ compared to that for uniform nodes. The maximum of $|\\omega_{n+1}(x)|$ grows only as $O(2^{-n})$, whereas for uniform nodes it grows exponentially. This property is closely related to the slow, logarithmic growth of the Lebesgue constant $\\Lambda_n = O(\\log n)$ for Chebyshev nodes, which guarantees convergence of the interpolant $p_n(x)$ to any analytic function $f(x)$ as $n \\to \\infty$.\n\n### Part 2: Polynomial Interpolation and Evaluation\n\nWe are asked to find the value of the interpolating polynomial $p_6(x)$ for the function $f(x) = \\frac{1}{1 + 25 x^2}$ at the point $x_{\\star} = \\cos(\\pi/6)$. The polynomial $p_6(x)$ is of degree at most $n=6$ and interpolates $f(x)$ at the $n+1=7$ Chebyshev-Lobatto nodes.\n\nThe interpolation nodes are given by $x_j = \\cos(j \\pi/n)$ for $n=6$ and $j = 0, 1, \\dots, 6$.\nLet's list the nodes:\n- $x_0 = \\cos(0\\pi/6) = \\cos(0) = 1$\n- $x_1 = \\cos(1\\pi/6) = \\cos(\\pi/6) = \\frac{\\sqrt{3}}{2}$\n- $x_2 = \\cos(2\\pi/6) = \\cos(\\pi/3) = \\frac{1}{2}$\n- $x_3 = \\cos(3\\pi/6) = \\cos(\\pi/2) = 0$\n- $x_4 = \\cos(4\\pi/6) = \\cos(2\\pi/3) = -\\frac{1}{2}$\n- $x_5 = \\cos(5\\pi/6) = -\\frac{\\sqrt{3}}{2}$\n- $x_6 = \\cos(6\\pi/6) = \\cos(\\pi) = -1$\n\nThe point at which we must evaluate the polynomial is $x_{\\star} = \\cos(\\pi/6)$. By direct comparison with the list of nodes, we observe that the evaluation point is one of the interpolation nodes:\n$$\nx_{\\star} = x_1 = \\frac{\\sqrt{3}}{2}.\n$$\nBy the definition of an interpolating polynomial, its value at any of the interpolation nodes must be equal to the value of the original function at that same node. That is, for a unique polynomial $p_n(x)$ of degree at most $n$ that interpolates a function $f(x)$ at $n+1$ distinct nodes $\\{x_j\\}_{j=0}^n$, it must satisfy the conditions:\n$$\np_n(x_j) = f(x_j) \\quad \\text{for all } j = 0, 1, \\dots, n.\n$$\nIn our case with $n=6$, we have $p_6(x_j) = f(x_j)$ for $j=0, \\dots, 6$. Since our evaluation point $x_\\star$ is the node $x_1$, the value of the polynomial is simply the value of the function at that node:\n$$\np_6(x_{\\star}) = p_6(x_1) = f(x_1).\n$$\nTherefore, the task reduces to evaluating the function $f(x)$ at $x = x_1 = \\frac{\\sqrt{3}}{2}$.\n$$\nf\\left(\\frac{\\sqrt{3}}{2}\\right) = \\frac{1}{1 + 25 \\left(\\frac{\\sqrt{3}}{2}\\right)^2} = \\frac{1}{1 + 25 \\left(\\frac{3}{4}\\right)} = \\frac{1}{1 + \\frac{75}{4}}.\n$$\nTo simplify this expression, we find a common denominator in the bottom fraction:\n$$\nf\\left(\\frac{\\sqrt{3}}{2}\\right) = \\frac{1}{\\frac{4}{4} + \\frac{75}{4}} = \\frac{1}{\\frac{4+75}{4}} = \\frac{1}{\\frac{79}{4}}.\n$$\nInverting the fraction gives the final result:\n$$\np_6(\\cos(\\pi/6)) = \\frac{4}{79}.\n$$\nThis is the exact analytic value. There is no need to explicitly construct the full expression for the polynomial $p_6(x)$ in the monomial basis or any other basis, as its value at the specified point is determined directly by the interpolation condition.",
            "answer": "$$\\boxed{\\frac{4}{79}}$$"
        },
        {
            "introduction": "Having theoretically explored the properties of Chebyshev nodes, we now turn to a computational investigation to quantify their effectiveness. This practice introduces the concept of an amplification factor, a practical metric for measuring how sensitive an interpolation scheme is to small perturbations in the data. By implementing and comparing this factor for equispaced points, Chebyshev points, and a piecewise 'discontinuous Galerkin' approach, you will generate concrete evidence of the stability gained and see firsthand why modern numerical methods favor these advanced strategies .",
            "id": "3413836",
            "problem": "You are asked to study the sensitivity amplification associated with polynomial reconstructions in spectral and discontinuous Galerkin methods, with a focus on the Runge phenomenon near the interval endpoints. Consider nodal interpolation operators built on different node sets over the interval $[-1,1]$. For a given set of nodes $\\{x_j\\}_{j=0}^{N}$ and their associated barycentric Lagrange basis functions $\\{\\ell_j(x)\\}_{j=0}^{N}$, the reconstruction at a point $x$ from nodal data $\\{y_j\\}$ is $u_p(x)=\\sum_{j=0}^{N} \\ell_j(x)\\,y_j$. Suppose the nodal data $\\{y_j\\}$ are corrupted by additive independent identically distributed perturbations with zero mean and variance $\\sigma^2$. The amplification factor at a point $x$ is defined as the ratio of the standard deviation of the reconstructed value $u_p(x)$ to the standard deviation $\\sigma$ of the nodal perturbations, which quantifies how noise is amplified by the interpolation operator at $x$.\n\nUse the following foundational base:\n- The nodal Lagrange basis satisfies $\\ell_j(x_k)=\\delta_{jk}$ and $\\sum_{j=0}^{N}\\ell_j(x)=1$ for all $x\\in[-1,1]$.\n- The barycentric interpolation formula represents $u_p(x)$ as a linear combination of data $\\{y_j\\}$, hence perturbations propagate linearly to the reconstruction.\n- For independent identically distributed inputs with variance $\\sigma^2$, the output variance under a linear operator with coefficients $\\{\\phi_j(x)\\}$ equals $\\sigma^2\\sum_{j}\\phi_j(x)^2$.\n\nYour tasks:\n1. For global interpolation on $[-1,1]$, construct three node sets:\n   - Equally spaced nodes: $x_j=-1+\\dfrac{2j}{N}$ for $j=0,1,\\dots,N$.\n   - Chebyshev–Lobatto nodes: $x_j=\\cos\\!\\left(\\dfrac{\\pi j}{N}\\right)$ for $j=0,1,\\dots,N$.\n   - A piecewise “discontinuous Galerkin style” reconstruction on $K$ uniform elements: partition $[-1,1]$ into $K$ subintervals and on each element use local Chebyshev–Lobatto nodes of degree $p$ (so $p+1$ nodes per element). The global reconstruction is defined locally on each element by the corresponding local nodal interpolant without any continuity constraints across elements.\n\n2. For each reconstruction, evaluate the amplification factor $A(x)$ on a uniform grid of $M$ points in $[-1,1]$ (including the endpoints), where $A(x)$ is defined as $A(x)=\\dfrac{\\sigma(u_p(x))}{\\sigma}$, and $\\sigma(u_p(x))$ is the standard deviation of the reconstructed value at $x$ under independent identically distributed nodal perturbations of variance $\\sigma^2$. Use the fact that, due to linearity and independence, $A(x)$ equals the Euclidean norm of the vector of interpolation shape functions $\\{\\ell_j(x)\\}$ at $x$.\n\n3. Quantify endpoint sensitivity spikes by computing the following scalar metrics from $A(x)$ evaluated on the grid:\n   - $A_{\\max}=\\max_{x\\in[-1,1]} A(x)$,\n   - $A_{\\text{bdry}}=\\max_{x:\\,|x|\\ge \\alpha} A(x)$ with $\\alpha=0.95$,\n   - $A_{\\text{ctr}}=\\max_{x:\\,|x|\\le \\beta} A(x)$ with $\\beta=0.5$,\n   - The ratio $R=\\dfrac{A_{\\text{bdry}}}{A_{\\text{ctr}}}$.\n\n4. Implement the computation using the barycentric Lagrange formulation. For Chebyshev–Lobatto nodes, use the known stable barycentric weights. For equally spaced nodes and for local elemental nodes, you may compute weights directly from the nodal products. Handle the case $x=x_j$ exactly by enforcing $\\ell_j(x)=1$ and $\\ell_{k\\neq j}(x)=0$.\n\n5. Provide the following test suite of parameter choices:\n   - Test case $1$: Global interpolation on equally spaced nodes with degree $N=20$ and evaluation grid size $M=1001$.\n   - Test case $2$: Global interpolation on Chebyshev–Lobatto nodes with degree $N=20$ and evaluation grid size $M=1001$.\n   - Test case $3$: Piecewise reconstruction with $K=10$ elements, local degree $p=3$ (local Chebyshev–Lobatto nodes), and evaluation grid size $M=1001$.\n   - Test case $4$: Global interpolation on equally spaced nodes with low degree $N=2$ and evaluation grid size $M=1001$.\n\n6. For each test case, output a list of four floating-point numbers `[A_max,A_bdry,A_ctr,R]`. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one list. For example, the output format should be of the form $[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],\\dots]$ with all entries as decimal numbers.\n\nNo physical units are involved in this task. Angles must be in radians. All returned values must be plain decimal numbers without percentage signs. Your program must be self-contained, require no input, and compute the requested quantities deterministically.",
            "solution": "We begin from fundamental definitions of nodal polynomial interpolation and linear propagation of independent identically distributed noise through linear mappings. Let $\\{x_j\\}_{j=0}^{N}\\subset[-1,1]$ be distinct nodes. The nodal Lagrange basis functions $\\{\\ell_j(x)\\}_{j=0}^{N}$ are defined by the conditions $\\ell_j(x_k)=\\delta_{jk}$ and they form a partition of unity $\\sum_{j=0}^{N}\\ell_j(x)=1$ for all $x\\in[-1,1]$. The nodal interpolant of degree at most $N$ is $u_p(x)=\\sum_{j=0}^{N}\\ell_j(x)\\,y_j$.\n\nThe barycentric interpolation formula realizes $u_p(x)$ in a numerically stable form. If $w_j$ are barycentric weights compatible with $\\{x_j\\}$, then for $x\\neq x_j$,\n$$\nu_p(x)=\\frac{\\sum_{j=0}^{N}\\dfrac{w_j}{x-x_j}\\,y_j}{\\sum_{j=0}^{N}\\dfrac{w_j}{x-x_j}},\n$$\nand at $x=x_j$, $u_p(x)=y_j$. Importantly, this shows that $u_p(x)$ is a linear functional of the data $\\{y_j\\}$:\n$$\nu_p(x) = \\sum_{j=0}^{N} \\phi_j(x)\\,y_j,\\quad \\text{where}\\quad \\phi_j(x) = \\begin{cases}\n\\dfrac{w_j/(x-x_j)}{\\sum_{k=0}^{N} w_k/(x-x_k)}, & x\\neq x_j,\\\\\n1, & x=x_j\\ \\text{and index equals}\\ j,\\\\\n0, & x=x_j\\ \\text{and index not equal to}\\ j.\n\\end{cases}\n$$\nThe coefficients $\\{\\phi_j(x)\\}$ are the values of the Lagrange basis functions $\\{\\ell_j(x)\\}$ at $x$.\n\nConsider additive independent identically distributed perturbations on the data $y_j = \\bar{y}_j + \\varepsilon_j$, with $\\mathbb{E}[\\varepsilon_j]=0$ and $\\operatorname{Var}(\\varepsilon_j)=\\sigma^2$, independent over $j$. The propagation through the linear mapping yields\n$$\nu_p(x) = \\sum_{j=0}^{N} \\phi_j(x)\\,\\bar{y}_j + \\sum_{j=0}^{N} \\phi_j(x)\\,\\varepsilon_j.\n$$\nTherefore, the variance of $u_p(x)$ due to the perturbations is\n$$\n\\operatorname{Var}(u_p(x)) = \\mathbb{E}\\Big[\\Big(\\sum_{j=0}^{N} \\phi_j(x)\\,\\varepsilon_j\\Big)^2\\Big] = \\sum_{j=0}^{N}\\phi_j(x)^2\\,\\operatorname{Var}(\\varepsilon_j) = \\sigma^2 \\sum_{j=0}^{N}\\phi_j(x)^2,\n$$\nwhere we used independence and identical variance. Consequently, the amplification factor, defined as the ratio of the output standard deviation to the input standard deviation, is\n$$\nA(x) = \\frac{\\sigma(u_p(x))}{\\sigma} = \\sqrt{\\sum_{j=0}^{N}\\phi_j(x)^2}.\n$$\nThus, $A(x)$ equals the Euclidean norm of the vector of shape functions $\\{\\ell_j(x)\\}$ evaluated at $x$.\n\nWe now describe the node constructions and corresponding barycentric weights:\n- For equally spaced nodes $x_j=-1+\\dfrac{2j}{N}$, generic barycentric weights can be taken as $w_j = \\left(\\prod_{k\\neq j}(x_j-x_k)\\right)^{-1}$; they are unique up to a nonzero scalar factor, which cancels in the barycentric formula and in the computation of $\\{\\phi_j(x)\\}$.\n- For Chebyshev–Lobatto nodes $x_j = \\cos\\!\\left(\\dfrac{\\pi j}{N}\\right)$, numerically stable weights are known in closed form, $w_j = (-1)^j c_j$ with $c_0=c_N=\\tfrac{1}{2}$ and $c_j=1$ for $j=1,\\dots,N-1$. Any nonzero constant scaling of these weights leaves $\\{\\phi_j(x)\\}$ invariant.\n- For piecewise “discontinuous Galerkin style” reconstruction, partition $[-1,1]$ into $K$ elements of equal length. On each element we place local Chebyshev–Lobatto nodes of degree $p$, i.e., $p+1$ nodes obtained by mapping $\\{\\cos(\\pi i/p)\\}_{i=0}^{p}$ from the reference interval $[-1,1]$ to the element. The reconstruction at a point $x$ is computed from the local element’s nodal data only, using the local barycentric formula. Because the mapping from the full set of nodal data to $u_p(x)$ is block-local, the corresponding shape vector $\\{\\phi_j(x)\\}$ has nonzero entries only for nodes on the element containing $x$.\n\nFor each test case, we evaluate $A(x)$ on a uniform grid of $M$ points in $[-1,1]$ (including endpoints). We then compute the scalars:\n- $A_{\\max}=\\max_{x\\in[-1,1]} A(x)$,\n- $A_{\\text{bdry}}=\\max_{|x|\\ge \\alpha} A(x)$ with $\\alpha=0.95$,\n- $A_{\\text{ctr}}=\\max_{|x|\\le \\beta} A(x)$ with $\\beta=0.5$,\n- $R=\\dfrac{A_{\\text{bdry}}}{A_{\\text{ctr}}}$.\n\nInterpretation with respect to the Runge phenomenon: The Runge phenomenon manifests as large oscillations and sensitivity near the endpoints for global interpolation on equally spaced nodes as the degree grows. The amplification factor $A(x)$ directly captures the sensitivity to perturbations; spikes of $A(x)$ near $x=\\pm 1$ indicate severe amplification and are closely tied to the growth of the Lebesgue function for equispaced nodes. Chebyshev–Lobatto nodes mitigate this growth, thus reducing endpoint spikes. An alternative robust approach is to use piecewise low-degree interpolation, as in discontinuous Galerkin methods; locality keeps $A(x)$ bounded uniformly across the interval, including near the endpoints.\n\nAlgorithmic design:\n1. Construct nodes and barycentric weights for each configuration.\n2. Build an evaluation grid of $M$ equally spaced points in $[-1,1]$.\n3. For each evaluation point $x$:\n   - If $x$ coincides with a node (within a tight tolerance), set the corresponding shape vector to the identity selector.\n   - Otherwise, compute the barycentric coefficients $\\{\\phi_j(x)\\}$, either globally or locally on the element for the piecewise case.\n   - Compute $A(x)=\\sqrt{\\sum_j \\phi_j(x)^2}$.\n4. From the discrete $A(x)$ values on the grid, compute $A_{\\max}$, $A_{\\text{bdry}}$, $A_{\\text{ctr}}$, and $R$.\n5. Repeat for the test suite:\n   - Test case $1$: Global equispaced nodes, $N=20$, $M=1001$.\n   - Test case $2$: Global Chebyshev–Lobatto nodes, $N=20$, $M=1001$.\n   - Test case $3$: Piecewise reconstruction with $K=10$, local degree $p=3$ (local Chebyshev–Lobatto nodes), $M=1001$.\n   - Test case $4$: Global equispaced nodes, low degree $N=2$, $M=1001$.\n6. Output a single line: a list of four-element lists `[A_max,A_bdry,A_ctr,R]` for each test case.\n\nBy comparing $R$ across test cases, one identifies the sensitivity spikes near the endpoints. We expect global equispaced nodes with $N=20$ to exhibit large $R$, Chebyshev–Lobatto nodes to reduce $R$ substantially, and the piecewise reconstruction to keep $R$ close to unity. The low-degree equispaced case should also have modest $R$. These observations motivate robust fitting strategies such as using Chebyshev–Lobatto nodes for global interpolation or adopting piecewise low-degree reconstructions typical of discontinuous Galerkin methods to mitigate the Runge phenomenon.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef equispaced_nodes(N: int) -> np.ndarray:\n    # N+1 nodes on [-1,1]\n    return np.linspace(-1.0, 1.0, N + 1)\n\ndef chebyshev_lobatto_nodes(N: int) -> np.ndarray:\n    # x_j = cos(pi*j/N), j=0..N\n    j = np.arange(N + 1)\n    return np.cos(np.pi * j / N)\n\ndef bary_weights_generic(nodes: np.ndarray) -> np.ndarray:\n    # Compute generic barycentric weights w_j = 1 / prod_{k!=j} (x_j - x_k)\n    # This is O(n^2) and acceptable for moderate N.\n    n = nodes.size\n    w = np.empty(n, dtype=float)\n    for j in range(n):\n        diffs = nodes[j] - np.delete(nodes, j)\n        prod = np.prod(diffs)\n        w[j] = 1.0 / prod\n    return w\n\ndef bary_weights_cheb_lobatto(N: int) -> np.ndarray:\n    # Stable weights for Chebyshev-Lobatto nodes: w_j = (-1)^j * c_j with c_0=c_N=1/2, else 1\n    j = np.arange(N + 1)\n    w = (-1.0) ** j\n    if N >= 1:\n        w[0] *= 0.5\n        w[-1] *= 0.5\n    return w\n\ndef barycentric_phi_matrix(x_eval: np.ndarray, nodes: np.ndarray, weights: np.ndarray, tol: float = 1e-14) -> np.ndarray:\n    # Compute the matrix Phi where Phi[i,j] = phi_j(x_eval[i])\n    # Handle x equal to nodes by one-hot rows.\n    x_eval = np.asarray(x_eval)\n    nodes = np.asarray(nodes)\n    weights = np.asarray(weights)\n    m = x_eval.size\n    n = nodes.size\n    Phi = np.zeros((m, n), dtype=float)\n\n    # For vectorized computation, we will process rows where x does not coincide with any node\n    # First, detect coincidences\n    # Use broadcasting to compute |x - nodes| and check for small values\n    diff = np.abs(x_eval[:, None] - nodes[None, :])\n    row_has_hit = np.any(diff <= tol, axis=1)\n    # For rows with a hit, set one-hot\n    hit_rows = np.where(row_has_hit)[0]\n    for i in hit_rows:\n        j_hit = np.where(diff[i, :] <= tol)[0]\n        # In rare cases of multiple matches due to tolerance, pick the closest\n        if j_hit.size > 1:\n            j_closest = j_hit[np.argmin(diff[i, j_hit])]\n            j_hit = np.array([j_closest], dtype=int)\n        Phi[i, j_hit[0]] = 1.0\n\n    # For rows without hit, compute barycentric coefficients\n    reg_rows = np.where(~row_has_hit)[0]\n    if reg_rows.size > 0:\n        xx = x_eval[reg_rows][:, None]  # shape (mr,1)\n        T = 1.0 / (xx - nodes[None, :])  # shape (mr,n)\n        WT = weights[None, :] * T\n        denom = np.sum(WT, axis=1)  # shape (mr,)\n        # Avoid division by zero; denom should not be zero for valid weights/nodes\n        Phi_reg = WT / denom[:, None]\n        Phi[reg_rows, :] = Phi_reg\n\n    return Phi\n\ndef amplification_from_phi(Phi: np.ndarray) -> np.ndarray:\n    # A(x) = sqrt(sum_j phi_j(x)^2)\n    return np.sqrt(np.sum(Phi * Phi, axis=1))\n\ndef piecewise_dg_phi_matrix(x_eval: np.ndarray, K: int, p: int) -> np.ndarray:\n    # Build piecewise Chebyshev-Lobatto local interpolation on K uniform elements, degree p per element.\n    # Global nodes are discontinuous across elements; total nodes = K*(p+1).\n    # The mapping is block-local: for x in element e, only the local (p+1) nodes contribute.\n    m = x_eval.size\n    # Element boundaries\n    edges = np.linspace(-1.0, 1.0, K + 1)\n    h = edges[1] - edges[0]\n    # Build global nodes (for completeness, but we need only Phi)\n    total_nodes = K * (p + 1)\n    Phi = np.zeros((m, total_nodes), dtype=float)\n\n    # Precompute local reference nodes and weights for Chebyshev-Lobatto of degree p\n    if p == 0:\n        r_ref = np.array([0.0])\n        w_ref = np.array([1.0])\n    else:\n        r_ref = np.cos(np.pi * np.arange(p + 1) / p)\n        # Stable weights for Chebyshev-Lobatto on reference\n        w_ref = bary_weights_cheb_lobatto(p)\n\n    # For each evaluation point, find its element and compute local phi, then place into global Phi\n    # Determine element indices: e in {0, ..., K-1}\n    # Map x to element by index e = floor((x - (-1))/h), clamp to [0, K-1]; special case for x=1 assign last element.\n    t = (x_eval - (-1.0)) / h\n    e_idx = np.floor(t).astype(int)\n    e_idx = np.clip(e_idx, 0, K - 1)\n    # Correct those exactly at the rightmost boundary to last element\n    rightmost = np.isclose(x_eval, 1.0, atol=1e-14)\n    e_idx[rightmost] = K - 1\n\n    # Process each element's evaluation points in batch for efficiency\n    for e in range(K):\n        mask = (e_idx == e)\n        if not np.any(mask):\n            continue\n        x_local = x_eval[mask]\n        # Element bounds\n        xl = edges[e]\n        xr = edges[e + 1]\n        # Affine mapping from reference r in [-1,1] to physical x: x = 0.5*(xr+xl) + 0.5*(xr-xl)*r\n        # Inverse mapping r = (2*(x - 0.5*(xr+xl))) / (xr - xl)\n        r = (2.0 * (x_local - 0.5 * (xr + xl))) / (xr - xl)\n\n        # Compute local phi on reference nodes\n        Phi_loc = barycentric_phi_matrix(r, r_ref, w_ref)\n\n        # Insert into global Phi at the block corresponding to element e\n        col_start = e * (p + 1)\n        col_end = col_start + (p + 1)\n        Phi[np.where(mask)[0][:, None], np.arange(col_start, col_end)[None, :]] = Phi_loc\n\n    return Phi\n\ndef compute_metrics_from_A(x_eval: np.ndarray, A: np.ndarray, alpha: float = 0.95, beta: float = 0.5):\n    # Compute A_max, A_bdry (|x|>=alpha), A_ctr (|x|<=beta), ratio\n    A_max = float(np.max(A))\n    bdry_mask = np.abs(x_eval) >= alpha\n    ctr_mask = np.abs(x_eval) <= beta\n    A_bdry = float(np.max(A[bdry_mask])) if np.any(bdry_mask) else float('nan')\n    A_ctr = float(np.max(A[ctr_mask])) if np.any(ctr_mask) else float('nan')\n    R = float(A_bdry / A_ctr) if (A_ctr != 0.0 and np.isfinite(A_bdry) and np.isfinite(A_ctr)) else float('nan')\n    return [A_max, A_bdry, A_ctr, R]\n\ndef run_test_cases():\n    results = []\n\n    # Common evaluation grid size\n    M = 1001\n    x_eval = np.linspace(-1.0, 1.0, M)\n\n    # Test case 1: Global equispaced nodes, N=20\n    N1 = 20\n    nodes1 = equispaced_nodes(N1)\n    w1 = bary_weights_generic(nodes1)\n    Phi1 = barycentric_phi_matrix(x_eval, nodes1, w1)\n    A1 = amplification_from_phi(Phi1)\n    metrics1 = compute_metrics_from_A(x_eval, A1, alpha=0.95, beta=0.5)\n    results.append([float(f\"{v:.12g}\") for v in metrics1])\n\n    # Test case 2: Global Chebyshev-Lobatto nodes, N=20\n    N2 = 20\n    nodes2 = chebyshev_lobatto_nodes(N2)\n    w2 = bary_weights_cheb_lobatto(N2)\n    Phi2 = barycentric_phi_matrix(x_eval, nodes2, w2)\n    A2 = amplification_from_phi(Phi2)\n    metrics2 = compute_metrics_from_A(x_eval, A2, alpha=0.95, beta=0.5)\n    results.append([float(f\"{v:.12g}\") for v in metrics2])\n\n    # Test case 3: Piecewise DG-style, K=10 elements, local degree p=3\n    K3 = 10\n    p3 = 3\n    Phi3 = piecewise_dg_phi_matrix(x_eval, K=K3, p=p3)\n    A3 = amplification_from_phi(Phi3)\n    metrics3 = compute_metrics_from_A(x_eval, A3, alpha=0.95, beta=0.5)\n    results.append([float(f\"{v:.12g}\") for v in metrics3])\n\n    # Test case 4: Global equispaced nodes, low degree N=2\n    N4 = 2\n    nodes4 = equispaced_nodes(N4)\n    w4 = bary_weights_generic(nodes4)\n    Phi4 = barycentric_phi_matrix(x_eval, nodes4, w4)\n    A4 = amplification_from_phi(Phi4)\n    metrics4 = compute_metrics_from_A(x_eval, A4, alpha=0.95, beta=0.5)\n    results.append([float(f\"{v:.12g}\") for v in metrics4])\n\n    return results\n\ndef solve():\n    results = run_test_cases()\n    # Final print statement in the exact required format.\n    # Print a single line: list of lists with 4 floats each.\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Mitigating the Runge phenomenon is not always straightforward, and seemingly intuitive solutions can have unintended consequences. This final practice explores one such strategy: directly constraining the polynomial's derivative at the endpoints to suppress overshoots. Through an exercise in constrained least-squares approximation, you will derive the solution and, more importantly, analyze its impact on the method's convergence rate, revealing a crucial lesson about the hidden costs of such 'fixes' and the potential loss of spectral accuracy .",
            "id": "3413852",
            "problem": "Consider the least-squares approximation on the interval $[-1,1]$ with respect to the standard Lebesgue measure. Let $f(x) = \\frac{1}{1+\\kappa^{2} x^{2}}$ for a fixed parameter $\\kappa > 0$. To mitigate endpoint overshoots associated with the Runge phenomenon in global polynomial approximations, we impose a slope-clamping constraint at the right endpoint to suppress oscillations near $x=1$.\n\nDefine the constrained least-squares problem: find the polynomial $p \\in \\mathbb{P}_{2}$ of degree $\\leq 2$ that minimizes\n$$\n\\int_{-1}^{1} \\bigl(f(x) - p(x)\\bigr)^{2} \\, dx\n$$\nsubject to the endpoint condition $p'(1)=0$. Here $\\mathbb{P}_{2}$ is spanned by the Legendre polynomials $\\{P_{0}(x),P_{1}(x),P_{2}(x)\\}$ with $P_{0}(x)=1$, $P_{1}(x)=x$, and $P_{2}(x)=\\frac{1}{2}(3x^{2}-1)$.\n\nStarting from the orthogonality of Legendre polynomials and the definition of the least-squares projection, derive the constrained minimizer in the form\n$$\np(x) \\;=\\; \\alpha_{0} P_{0}(x) \\;+\\; \\beta \\bigl(P_{2}(x) - 3 P_{1}(x)\\bigr),\n$$\nand obtain a closed-form analytic expression for the coefficient $\\beta$ as a function of $\\kappa$.\n\nThen, based on first principles of convex optimization and orthogonal projections, briefly analyze the feasibility of the constraint $p'(1)=0$ and explain whether imposing a fixed finite number of such linear endpoint constraints (independent of the polynomial degree) changes the asymptotic spectral accuracy in $N$ for analytic target functions $f$. Your reasoning must begin from standard properties of $L^{2}$-projections and orthogonal polynomial bases and must not assume any pre-derived formulas for constrained projections.\n\nProvide as your final answer the explicit analytic expression for $\\beta(\\kappa)$. No numerical evaluation is required, and no rounding is permitted. Express your final answer in its simplest exact form using elementary functions of $\\kappa$ only.",
            "solution": "### Derivation of the Constrained Minimizer\nLet the polynomial $p(x) \\in \\mathbb{P}_{2}$ be expressed in the basis of Legendre polynomials:\n$$\np(x) = c_{0} P_{0}(x) + c_{1} P_{1}(x) + c_{2} P_{2}(x)\n$$\nwhere $P_{0}(x)=1$, $P_{1}(x)=x$, and $P_{2}(x)=\\frac{1}{2}(3x^2-1)$. The coefficients $c_0, c_1, c_2$ are real constants to be determined.\n\nThe problem imposes a constraint on the derivative of $p(x)$ at the endpoint $x=1$:\n$$\np'(1) = 0\n$$\nThe derivative of $p(x)$ is:\n$$\np'(x) = c_{0} P_{0}'(x) + c_{1} P_{1}'(x) + c_{2} P_{2}'(x)\n$$\nUsing the given forms, we find the derivatives: $P_{0}'(x)=0$, $P_{1}'(x)=1$, and $P_{2}'(x) = \\frac{1}{2}(6x) = 3x$.\nEvaluating at $x=1$: $P_{0}'(1)=0$, $P_{1}'(1)=1$, $P_{2}'(1)=3$.\nThe constraint equation becomes:\n$$\np'(1) = c_{1}(1) + c_{2}(3) = c_{1} + 3c_{2} = 0\n$$\nThis gives a linear relationship between the coefficients: $c_{1} = -3c_{2}$.\n\nSubstituting this relation back into the expression for $p(x)$:\n$$\np(x) = c_{0} P_{0}(x) + (-3c_{2}) P_{1}(x) + c_{2} P_{2}(x) = c_{0} P_{0}(x) + c_{2} \\bigl(P_{2}(x) - 3P_{1}(x)\\bigr)\n$$\nThis expression matches the target form $p(x) = \\alpha_{0} P_{0}(x) + \\beta \\bigl(P_{2}(x) - 3 P_{1}(x)\\bigr)$ by identifying $\\alpha_0 = c_0$ and $\\beta=c_2$.\nThe problem is now to find the coefficients $c_0, c_2$ that minimize the functional\n$$\nJ(c_{0}, c_{2}) = \\int_{-1}^{1} \\Biggl(f(x) - \\Bigl(c_{0} P_{0}(x) + c_{2} \\bigl(P_{2}(x) - 3P_{1}(x)\\bigr)\\Bigr) \\Biggr)^{2} \\, dx\n$$\nThis is a standard least-squares problem, which is solved by projecting the function $f(x)$ onto the subspace of $\\mathbb{P}_2$ spanned by the basis functions $\\psi_{0}(x) = P_{0}(x)$ and $\\psi_{1}(x) = P_{2}(x) - 3P_{1}(x)$. The normal equations for the coefficients are:\n$$\n\\begin{pmatrix} \\langle \\psi_{0}, \\psi_{0} \\rangle & \\langle \\psi_{0}, \\psi_{1} \\rangle \\\\ \\langle \\psi_{1}, \\psi_{0} \\rangle & \\langle \\psi_{1}, \\psi_{1} \\rangle \\end{pmatrix} \\begin{pmatrix} c_{0} \\\\ c_{2} \\end{pmatrix} = \\begin{pmatrix} \\langle f, \\psi_{0} \\rangle \\\\ \\langle f, \\psi_{1} \\rangle \\end{pmatrix}\n$$\nwhere $\\langle g, h \\rangle = \\int_{-1}^{1} g(x)h(x) \\, dx$ is the standard $L^2$ inner product on $[-1,1]$.\n\nWe utilize the orthogonality property of Legendre polynomials: $\\langle P_j, P_k \\rangle = \\frac{2}{2k+1} \\delta_{jk}$.\nThe off-diagonal term of the Gram matrix is:\n$$\n\\langle \\psi_{0}, \\psi_{1} \\rangle = \\langle P_{0}, P_{2} - 3P_{1} \\rangle = \\langle P_{0}, P_{2} \\rangle - 3 \\langle P_{0}, P_{1} \\rangle = 0 - 3(0) = 0\n$$\nSince the basis $\\{\\psi_0, \\psi_1\\}$ is orthogonal, the Gram matrix is diagonal, and the equations for the coefficients decouple:\n$$\nc_{0} \\langle \\psi_{0}, \\psi_{0} \\rangle = \\langle f, \\psi_{0} \\rangle \\implies c_{0} = \\frac{\\langle f, P_{0} \\rangle}{\\langle P_{0}, P_{0} \\rangle}\n$$\n$$\nc_{2} \\langle \\psi_{1}, \\psi_{1} \\rangle = \\langle f, \\psi_{1} \\rangle \\implies c_{2} = \\beta = \\frac{\\langle f, P_{2} - 3P_{1} \\rangle}{\\langle P_{2} - 3P_{1}, P_{2} - 3P_{1} \\rangle}\n$$\nWe need to calculate the terms in the expression for $\\beta$.\n\nThe denominator is:\n$$\n\\langle P_{2} - 3P_{1}, P_{2} - 3P_{1} \\rangle = \\langle P_{2}, P_{2} \\rangle - 6 \\langle P_{2}, P_{1} \\rangle + 9 \\langle P_{1}, P_{1} \\rangle\n$$\nUsing orthogonality, $\\langle P_{2}, P_{1} \\rangle = 0$. Using the norm formula, $\\langle P_{k}, P_{k} \\rangle = \\frac{2}{2k+1}$:\n$$\n\\langle P_{2}, P_{2} \\rangle = \\frac{2}{2(2)+1} = \\frac{2}{5} \\quad \\text{and} \\quad \\langle P_{1}, P_{1} \\rangle = \\frac{2}{2(1)+1} = \\frac{2}{3}\n$$\nSo, the denominator is $\\frac{2}{5} + 9 \\left(\\frac{2}{3}\\right) = \\frac{2}{5} + 6 = \\frac{32}{5}$.\n\nThe numerator is:\n$$\n\\langle f, P_{2} - 3P_{1} \\rangle = \\langle f, P_{2} \\rangle - 3 \\langle f, P_{1} \\rangle\n$$\nThe function $f(x) = \\frac{1}{1+\\kappa^2 x^2}$ is an even function of $x$. The polynomial $P_{1}(x)=x$ is an odd function. Therefore, the product $f(x)P_{1}(x)$ is an odd function, and its integral over the symmetric interval $[-1, 1]$ is zero:\n$$\n\\langle f, P_{1} \\rangle = \\int_{-1}^{1} \\frac{x}{1+\\kappa^2 x^2} \\, dx = 0\n$$\nThe numerator thus simplifies to $\\langle f, P_{2} \\rangle$:\n$$\n\\langle f, P_{2} \\rangle = \\int_{-1}^{1} \\frac{1}{1+\\kappa^2 x^2} \\cdot \\frac{1}{2}(3x^2-1) \\, dx\n$$\nThe integrand is an even function, so we can write the integral as:\n$$\n\\langle f, P_{2} \\rangle = 2 \\int_{0}^{1} \\frac{3x^2-1}{2(1+\\kappa^2 x^2)} \\, dx = \\int_{0}^{1} \\frac{3x^2-1}{1+\\kappa^2 x^2} \\, dx\n$$\nWe decompose the integrand:\n$$\n\\frac{3x^2-1}{1+\\kappa^2 x^2} = \\frac{3}{\\kappa^2}\\frac{\\kappa^2 x^2}{1+\\kappa^2 x^2} - \\frac{1}{1+\\kappa^2 x^2} = \\frac{3}{\\kappa^2}\\frac{1+\\kappa^2 x^2-1}{1+\\kappa^2 x^2} - \\frac{1}{1+\\kappa^2 x^2}\n$$\n$$\n= \\frac{3}{\\kappa^2}\\left(1 - \\frac{1}{1+\\kappa^2 x^2}\\right) - \\frac{1}{1+\\kappa^2 x^2} = \\frac{3}{\\kappa^2} - \\left(\\frac{3}{\\kappa^2} + 1\\right)\\frac{1}{1+\\kappa^2 x^2} = \\frac{3}{\\kappa^2} - \\frac{3+\\kappa^2}{\\kappa^2}\\frac{1}{1+\\kappa^2 x^2}\n$$\nIntegrating from $0$ to $1$:\n$$\n\\int_{0}^{1} \\left( \\frac{3}{\\kappa^2} - \\frac{3+\\kappa^2}{\\kappa^2}\\frac{1}{1+\\kappa^2 x^2} \\right) dx = \\left[ \\frac{3x}{\\kappa^2} - \\frac{3+\\kappa^2}{\\kappa^2} \\frac{\\arctan(\\kappa x)}{\\kappa} \\right]_{0}^{1}\n$$\n$$\n= \\left( \\frac{3}{\\kappa^2} - \\frac{3+\\kappa^2}{\\kappa^3} \\arctan(\\kappa) \\right) - (0 - 0) = \\frac{3\\kappa - (3+\\kappa^2)\\arctan(\\kappa)}{\\kappa^3}\n$$\nFinally, we assemble the expression for $\\beta$:\n$$\n\\beta = \\frac{\\text{Numerator}}{\\text{Denominator}} = \\frac{\\frac{3\\kappa - (3+\\kappa^2)\\arctan(\\kappa)}{\\kappa^3}}{\\frac{32}{5}} = \\frac{5}{32\\kappa^3} \\bigl(3\\kappa - (3+\\kappa^2)\\arctan(\\kappa)\\bigr)\n$$\n\n### Analysis of Constraint Feasibility and Asymptotic Accuracy\n\n**Feasibility of the Constraint:**\nThe constraint $p'(1)=0$ is a linear equation on the coefficients of the polynomial $p(x)$. For any polynomial degree $N$, the set $V_N = \\{p \\in \\mathbb{P}_N \\mid p'(1)=0\\}$ is an affine subspace of $\\mathbb{P}_N$. This subspace is non-empty; for example, any constant polynomial $p(x)=c$ belongs to $V_N$ for $N \\ge 0$. The least-squares problem is the minimization of a strictly convex functional, $\\|f-p\\|_{L^2}^2$, over the non-empty, closed, convex set $V_N$. By standard principles of convex optimization, a unique minimizer is guaranteed to exist. Therefore, the constraint is always feasible.\n\n**Impact on Asymptotic Accuracy:**\nImposing a fixed, finite number of linear constraints, such as $p'(1)=0$, fundamentally changes the asymptotic convergence behavior of the approximation for an analytic function $f$. Spectral accuracy, characterized by an error that decays exponentially with the polynomial degree $N$ (i.e., $\\|f-p_N\\|_{L^2} \\sim \\exp(-cN)$), is lost.\n\nTo see this from first principles, let $p_N$ be the unconstrained $L^2$-projection of $f$ onto $\\mathbb{P}_N$ and $p_N^c$ be the constrained projection. The constrained solution can be found using a Lagrange multiplier $\\lambda$. The difference between the constrained and unconstrained approximations is a specific polynomial determined by the constraint. By the Pythagorean theorem for orthogonal projections, the error of the constrained approximation can be decomposed:\n$$\n\\|f - p_N^c\\|_{L^2}^2 = \\|f - p_N\\|_{L^2}^2 + \\|p_N - p_N^c\\|_{L^2}^2\n$$\nThe first term, $\\|f - p_N\\|_{L^2}^2$, is the error of the unconstrained spectral projection, which decays exponentially for analytic $f$. The second term, $\\|p_N - p_N^c\\|_{L^2}^2$, represents the penalty for enforcing the constraint. It can be shown that this term is equal to $(p_N'(1) - 0)^2 / D_N$, where $p_N'(1)$ is the derivative of the unconstrained approximation at $x=1$ (which converges to $f'(1)$ as $N\\to\\infty$ for an analytic $f$, assuming $f'(1) \\ne 0$) and $D_N$ is a term dependent on the basis.\nFor the Legendre basis, $D_N = \\sum_{j=1}^N \\frac{(P_j'(1))^2}{\\|P_j\\|^2}$.\nUsing the known properties $P_j'(1) = j(j+1)/2$ and $\\|P_j\\|^2 = 2/(2j+1)$, the sum $D_N$ can be shown to grow polynomially with $N$, specifically $D_N \\sim O(N^6)$.\n\nTherefore, the additional error term $\\|p_N - p_N^c\\|_{L^2}^2$ decays polynomially as $O(N^{-6})$. The total error is the sum of an exponentially decaying term and a polynomially decaying term. For large $N$, the polynomial term dominates. Consequently, the overall convergence rate is no longer exponential but algebraic. Imposing such a constraint replaces spectral accuracy with a much slower polynomial rate of convergence.",
            "answer": "$$\n\\boxed{\\frac{5}{32\\kappa^3} \\bigl(3\\kappa - (3+\\kappa^2)\\arctan(\\kappa)\\bigr)}\n$$"
        }
    ]
}