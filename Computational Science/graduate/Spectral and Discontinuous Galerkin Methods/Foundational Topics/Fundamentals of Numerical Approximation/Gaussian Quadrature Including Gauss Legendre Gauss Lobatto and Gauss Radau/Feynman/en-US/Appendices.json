{
    "hands_on_practices": [
        {
            "introduction": "The power of spectral and discontinuous Galerkin methods lies in their use of a reference element, where all basis functions and quadrature rules are defined once. This foundational exercise guides you through the essential mechanics of mapping these computations from the reference interval $[-1, 1]$ to any physical interval $[a, b]$. By deriving the transformation for quadrature nodes and weights under an affine map, you will gain a concrete understanding of how the change-of-variables formula and the Jacobian are fundamental to applying high-order methods to real-world domains .",
            "id": "3388861",
            "problem": "In the construction of high-order numerical fluxes and stiffness/mass matrices for Spectral and Discontinuous Galerkin (DG) methods, element-wise integrals on a physical interval element $E=[a,b]$ are evaluated by mapping to a reference interval $[-1,1]$. Consider an affine mapping $\\phi:[-1,1]\\to [a,b]$ given by $\\phi(\\xi)=\\alpha+\\beta\\,\\xi$ with constant Jacobian $J=\\phi'(\\xi)=\\beta>0$. A Gauss-type quadrature on the reference interval $[-1,1]$ with nodes $\\{\\xi_i\\}_{i=1}^{n}$ and weights $\\{w_i\\}_{i=1}^{n}$ approximates $\\int_{-1}^{1} g(\\xi)\\,d\\xi$ by $\\sum_{i=1}^{n} w_i\\,g(\\xi_i)$, and is exact for polynomials up to a degree that depends on the chosen rule (Gauss–Legendre, Gauss–Lobatto, or Gauss–Radau).\n\nStarting from the change-of-variables formula for integrals and the defining property of the reference quadrature, derive the quadrature rule on the physical element $E=[a,b]$ obtained by transporting the Gauss-type reference rule through $\\phi$. Your derivation must:\n- Express the mapped nodes and mapped weights in terms of $a$, $b$, $\\{\\xi_i\\}$, and $\\{w_i\\}$.\n- Justify that the degree of polynomial exactness is preserved under this affine mapping for Gauss–Legendre, Gauss–Lobatto, and Gauss–Radau rules.\n\nProvide your final result as a single closed-form analytic expression for the mapped quadrature approximation of $\\int_{a}^{b} f(x)\\,dx$ in terms of $a$, $b$, $\\{\\xi_i\\}$, and $\\{w_i\\}$, where $f$ is a sufficiently smooth function. Do not include any derivation or explanation in your final answer. The final answer must be a single analytic expression and must not include units.",
            "solution": "The problem is to derive the quadrature rule on a physical element $E=[a,b]$ by transporting a reference Gauss-type quadrature rule from the reference interval $[-1,1]$ via an affine mapping. We must also justify that the degree of polynomial exactness is preserved.\n\nFirst, we determine the parameters of the affine mapping $\\phi(\\xi) = \\alpha + \\beta\\xi$ that maps the reference interval $[-1,1]$ to the physical interval $[a,b]$. The endpoints must map to each other, so we require $\\phi(-1) = a$ and $\\phi(1) = b$. This gives a system of two linear equations for $\\alpha$ and $\\beta$:\n$$\n\\begin{cases}\n\\alpha - \\beta = a \\\\\n\\alpha + \\beta = b\n\\end{cases}\n$$\nAdding the two equations yields $2\\alpha = a+b$, so $\\alpha = \\frac{a+b}{2}$. Subtracting the first equation from the second yields $2\\beta = b-a$, so $\\beta = \\frac{b-a}{2}$. The condition $\\beta > 0$ implies $b>a$, which is consistent with the standard definition of an interval $[a,b]$.\nThe mapping is therefore:\n$$\nx = \\phi(\\xi) = \\frac{a+b}{2} + \\frac{b-a}{2}\\xi\n$$\nThe Jacobian of this transformation, $J$, is the derivative of $\\phi$ with respect to $\\xi$:\n$$\nJ = \\frac{d\\phi}{d\\xi} = \\beta = \\frac{b-a}{2}\n$$\nSince $a$ and $b$ are constants, the Jacobian $J$ is also a constant.\n\nNext, we apply the change-of-variables formula to the integral of a function $f(x)$ over the physical element $[a,b]$:\n$$\n\\int_{a}^{b} f(x)\\,dx = \\int_{\\phi^{-1}(a)}^{\\phi^{-1}(b)} f(\\phi(\\xi)) \\frac{d\\phi}{d\\xi}\\,d\\xi = \\int_{-1}^{1} f(\\phi(\\xi)) J\\,d\\xi\n$$\nSubstituting the expressions for $\\phi(\\xi)$ and $J$, we get:\n$$\n\\int_{a}^{b} f(x)\\,dx = \\int_{-1}^{1} f\\left( \\frac{a+b}{2} + \\frac{b-a}{2}\\xi \\right) \\left( \\frac{b-a}{2} \\right) d\\xi\n$$\nSince the Jacobian $J$ is a constant, we can move it outside the integral:\n$$\n\\int_{a}^{b} f(x)\\,dx = \\frac{b-a}{2} \\int_{-1}^{1} f\\left( \\frac{a+b}{2} + \\frac{b-a}{2}\\xi \\right) d\\xi\n$$\nNow, we approximate the integral on the reference interval $[-1,1]$ using the given Gauss-type quadrature rule. Let $g(\\xi) = f(\\phi(\\xi)) = f\\left( \\frac{a+b}{2} + \\frac{b-a}{2}\\xi \\right)$. The reference quadrature rule states:\n$$\n\\int_{-1}^{1} g(\\xi)\\,d\\xi \\approx \\sum_{i=1}^{n} w_i g(\\xi_i)\n$$\nwhere $\\{\\xi_i\\}_{i=1}^{n}$ are the reference nodes and $\\{w_i\\}_{i=1}^{n}$ are the reference weights.\nSubstituting this approximation into our transformed integral gives:\n$$\n\\int_{a}^{b} f(x)\\,dx \\approx \\frac{b-a}{2} \\sum_{i=1}^{n} w_i g(\\xi_i) = \\frac{b-a}{2} \\sum_{i=1}^{n} w_i f\\left( \\frac{a+b}{2} + \\frac{b-a}{2}\\xi_i \\right)\n$$\nThis expression is the quadrature rule on the physical element $[a,b]$. We can identify the mapped quadrature nodes, $x_i$, and the mapped quadrature weights, $W_i$. The quadrature rule on $[a,b]$ is of the form $\\sum_{i=1}^{n} W_i f(x_i)$. By comparing forms, we deduce:\nThe mapped nodes are $x_i = \\phi(\\xi_i) = \\frac{a+b}{2} + \\frac{b-a}{2}\\xi_i$.\nThe mapped weights are $W_i = J \\cdot w_i = \\frac{b-a}{2}w_i$.\n\nFinally, we must justify that the degree of polynomial exactness is preserved. Let the reference quadrature rule be exact for all polynomials in $\\xi$ up to degree $k$. The specific value of $k$ depends on the quadrature type (e.g., for $n$-point Gauss-Legendre, $k=2n-1$). This means for any polynomial $p(\\xi)$ of degree at most $k$, the quadrature is exact:\n$$\n\\int_{-1}^{1} p(\\xi)\\,d\\xi = \\sum_{i=1}^{n} w_i p(\\xi_i)\n$$\nNow, consider an arbitrary polynomial $P(x)$ of degree at most $k$ on the physical interval $[a,b]$. We want to show that the mapped quadrature rule is exact for $P(x)$.\nThe integral of $P(x)$ is:\n$$\n\\int_{a}^{b} P(x)\\,dx = \\frac{b-a}{2} \\int_{-1}^{1} P\\left(\\phi(\\xi)\\right) d\\xi\n$$\nThe quadrature approximation for this integral is:\n$$\n\\sum_{i=1}^{n} W_i P(x_i) = \\sum_{i=1}^{n} \\left(\\frac{b-a}{2}w_i\\right) P(\\phi(\\xi_i)) = \\frac{b-a}{2} \\sum_{i=1}^{n} w_i P(\\phi(\\xi_i))\n$$\nLet's analyze the composite function $p(\\xi) = P(\\phi(\\xi))$. Since $P(x)$ is a polynomial of degree at most $k$ and $\\phi(\\xi) = \\alpha + \\beta\\xi$ is a polynomial of degree $1$ in $\\xi$ (with $\\beta \\neq 0$), the composition $P(\\phi(\\xi))$ is also a polynomial in $\\xi$ of the same degree as $P(x)$, i.e., at most $k$.\nSince $p(\\xi) = P(\\phi(\\xi))$ is a polynomial of degree at most $k$, the reference quadrature rule is exact for it:\n$$\n\\int_{-1}^{1} P(\\phi(\\xi))\\,d\\xi = \\sum_{i=1}^{n} w_i P(\\phi(\\xi_i))\n$$\nMultiplying both sides by the constant Jacobian $J = \\frac{b-a}{2}$, we get:\n$$\n\\frac{b-a}{2} \\int_{-1}^{1} P(\\phi(\\xi))\\,d\\xi = \\frac{b-a}{2} \\sum_{i=1}^{n} w_i P(\\phi(\\xi_i))\n$$\nThe left side is the exact value of $\\int_{a}^{b} P(x)\\,dx$, and the right side is the value computed by the mapped quadrature rule. Since they are equal, the mapped rule is exact for any polynomial $P(x)$ of degree up to $k$. Thus, the degree of polynomial exactness is preserved under the affine mapping. This holds for Gauss–Legendre, Gauss–Lobatto, and Gauss–Radau rules, as the argument only relies on the properties of the affine map and the definition of quadrature exactness, not on the specific nodes and weights.",
            "answer": "$$\n\\boxed{\n\\frac{b-a}{2} \\sum_{i=1}^{n} w_i f\\left( \\frac{a+b}{2} + \\frac{b-a}{2}\\xi_i \\right)\n}\n$$"
        },
        {
            "introduction": "Once we can apply quadrature rules to any element, a critical question arises: which rule should we choose? This practice explores the profound consequences of selecting Gauss-Legendre versus Gauss-Lobatto quadrature, a central design choice in spectral element methods. You will investigate the trade-off between the computational efficiency gained from \"mass lumping\"—which yields a desirable diagonal mass matrix with Gauss-Lobatto nodes—and the potential loss of accuracy measured by numerical dispersion . This hands-on analysis illuminates the deep connection between quadrature, matrix structure, and the physical fidelity of wave propagation simulations.",
            "id": "3388916",
            "problem": "Consider the reference interval $[-1,1]$ and a polynomial space of degree $N \\ge 1$. Let $\\{x_i\\}_{i=0}^N$ be nodal points and $\\{w_i\\}_{i=0}^N$ be nonnegative weights defining a quadrature rule on $[-1,1]$. Define the Lagrange nodal basis $\\{\\ell_j(x)\\}_{j=0}^N$ by $\\ell_j(x_i)=\\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. The nodal Spectral Element Method (SEM) uses these basis functions to approximate functions and their derivatives. This problem focuses on two choices of quadrature and nodes: Gauss–Legendre (GL) and Gauss–Lobatto–Legendre (GLL), and on quantifying the trade-off between diagonal mass matrices and dispersion error in the context of spectral and Discontinuous Galerkin (DG) methods.\n\nYou must start from the following foundational facts and definitions:\n\n- The $(N+1)$-point Gauss–Legendre quadrature integrates exactly all polynomials up to degree $2N+1$ on $[-1,1]$. The nodes are the zeros of the Legendre polynomial $P_{N+1}(x)$, and the weights are strictly positive.\n- The $(N+1)$-point Gauss–Lobatto–Legendre quadrature includes the endpoints $\\pm 1$, and the interior nodes are the zeros of $P_N'(x)$. Its degree of exactness is $2N-1$. The weights are strictly positive and can be written in terms of $P_N(x)$ evaluated at the nodes.\n- For nodal Lagrange basis functions $\\ell_j(x)$ at the quadrature nodes $\\{x_i\\}$, the quadrature-based mass matrix $M_{\\text{quad}}$ is diagonal with entries $M_{ii}^{\\text{quad}} = w_i$. For Gauss–Legendre quadrature with $N+1$ nodes, the quadrature-based mass matrix is also the exact consistent mass matrix. For Gauss–Lobatto–Legendre quadrature with $N+1$ nodes, the quadrature-based mass matrix differs from the exact consistent mass matrix, an effect commonly termed as mass-lumping.\n\nYour tasks are:\n\n- Derive from first principles, using only the above facts and the definition of Lagrange interpolation, a numerically stable procedure to construct the differentiation matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ with entries $D_{ij} = \\ell_j'(x_i)$. You must do this without relying on any pre-derived closed-form differentiation formula in the problem statement, and you must justify stability in terms of conditioning.\n- For a complex-valued plane wave $u(x) = \\exp(\\mathrm{i}\\,\\kappa x)$ with real wavenumber $\\kappa$, define the mass-weighted least-squares residual $r(\\kappa) = \\| D\\,u - \\mathrm{i}\\,\\kappa\\,u \\|_{M}$, where $u \\in \\mathbb{C}^{N+1}$ is the vector of nodal samples $u_i = u(x_i)$ and $\\|v\\|_M^2 = v^* M v$ with $M$ the quadrature-based mass matrix and ${}^*$ denoting the conjugate transpose. Derive the expression for the discrete wavenumber $\\kappa_d$ that minimizes $r(\\kappa)$ over $\\kappa \\in \\mathbb{R}$, and express it in terms of $D$, $M$, and $u$ only.\n- For each scheme (GL and GLL), compute the relative dispersion error $E(N,\\kappa) = |\\kappa_d - \\kappa|/|\\kappa|$ for specified $(N,\\kappa)$.\n- Quantify the deviation between the diagonal mass-lumped matrix and the exact consistent mass matrix. Let $M_{\\text{true}}$ denote the exact consistent mass matrix with entries $M_{ij}^{\\text{true}} = \\int_{-1}^1 \\ell_i(x)\\,\\ell_j(x)\\,\\mathrm{d}x$. Using a sufficiently high-order Gauss–Legendre quadrature with $Q$ points, approximate $M_{\\text{true}}$ and compute the normalized off-diagonal Frobenius norm\n$$\n\\rho(N) = \\frac{\\| M_{\\text{true}} - \\operatorname{diag}(\\operatorname{diag}(M_{\\text{true}})) \\|_F}{\\| M_{\\text{true}} \\|_F}.\n$$\nReport $\\rho(N)$ for both GL and GLL nodes to contrast exact diagonalization (GL) and mass-lumping (GLL).\n\nImplementation and numerical details you must follow:\n\n- Construct GL nodes and weights with $N+1$ points.\n- Construct GLL nodes and weights with $N+1$ points. The interior nodes must be the zeros of $P_N'(x)$, and the weights must use the standard positive GLL formula in terms of $P_N(x)$ evaluated at the nodes. Endpoints are included at $x=\\pm 1$.\n- Build the Lagrange basis via barycentric data and assemble the differentiation matrix $D$ by differentiating the interpolant. Use the barycentric representation to control numerical conditioning.\n- Approximate $M_{\\text{true}}$ using Gauss–Legendre quadrature with $Q=200$ points, and evaluate $\\ell_j(x)$ robustly at quadrature abscissae via the barycentric formula. You must adhere to a purely mathematical definition of all entities; no physical units are involved.\n\nTest suite:\n\nCompute and aggregate results for the following inputs:\n\n- Dispersion error tests:\n  - Case $\\#1$: $(N,\\kappa) = (2, 1.0)$.\n  - Case $\\#2$: $(N,\\kappa) = (4, 2.5)$.\n  - Case $\\#3$: $(N,\\kappa) = (8, 7.0)$.\n  - Case $\\#4$: $(N,\\kappa) = (6, 10^{-6})$.\n  For each case, output two floating-point numbers: $E_{\\text{GL}}$ and $E_{\\text{GLL}}$.\n- Mass matrix diagonality test:\n  - Case $\\#5$: $N=6$. Output two floating-point numbers: $\\rho_{\\text{GL}}$ and $\\rho_{\\text{GLL}}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$\n[E_{\\text{GL}}^{(1)}, E_{\\text{GLL}}^{(1)}, E_{\\text{GL}}^{(2)}, E_{\\text{GLL}}^{(2)}, E_{\\text{GL}}^{(3)}, E_{\\text{GLL}}^{(3)}, E_{\\text{GL}}^{(4)}, E_{\\text{GLL}}^{(4)}, \\rho_{\\text{GL}}, \\rho_{\\text{GLL}}].\n$$\nAll floating-point outputs must be plain decimals. No other text should be printed. Angles and physical units do not apply in this problem.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of numerical analysis and spectral methods, well-posed, and expressed in objective mathematical language. It is self-contained and free of contradictions or ambiguities. The tasks require substantive, non-trivial derivations and computations based on established theory. We may, therefore, proceed with the solution.\n\nThe solution is presented in four parts: derivation of the differentiation matrix, derivation of the discrete wavenumber, procedure for computing dispersion error, and procedure for quantifying mass matrix diagonality.\n\n### 1. Construction of the Differentiation Matrix $D$\n\nLet $\\{x_i\\}_{i=0}^N$ be a set of $N+1$ distinct nodal points on the interval $[-1, 1]$. The Lagrange interpolating polynomial $p(x)$ of degree at most $N$ that passes through the points $(x_j, u_j)$ is given by $p(x) = \\sum_{j=0}^N u_j \\ell_j(x)$, where $\\{\\ell_j(x)\\}_{j=0}^N$ is the basis of Lagrange polynomials satisfying $\\ell_j(x_i) = \\delta_{ij}$.\n\nThe derivative of the interpolant is $p'(x) = \\sum_{j=0}^N u_j \\ell_j'(x)$. Evaluating this derivative at the nodal points $x_i$ gives the vector of derivative values, $p'(x_i) = \\sum_{j=0}^N u_j \\ell_j'(x_i)$. This can be expressed as a matrix-vector product $\\mathbf{u}' = D \\mathbf{u}$, where $\\mathbf{u} = [u_0, \\dots, u_N]^T$, $\\mathbf{u}' = [p'(x_0), \\dots, p'(x_N)]^T$, and $D$ is the $(N+1) \\times (N+1)$ differentiation matrix with entries $D_{ij} = \\ell_j'(x_i)$.\n\nA numerically stable procedure to compute $D_{ij}$ is derived from the barycentric form of the Lagrange polynomial, avoiding direct evaluation of $\\ell_j(x)$ and its derivative, which can be ill-conditioned.\n\nThe Lagrange basis polynomial $\\ell_j(x)$ is defined as:\n$$\n\\ell_j(x) = \\prod_{k=0, k \\neq j}^{N} \\frac{x-x_k}{x_j-x_k}\n$$\nFor the off-diagonal entries $D_{ij}$ with $i \\neq j$, we differentiate $\\ell_j(x)$ and evaluate at $x=x_i$:\n$$\n\\ell_j'(x_i) = \\left. \\frac{d}{dx} \\left( \\prod_{k=0, k \\neq j}^{N} \\frac{x-x_k}{x_j-x_k} \\right) \\right|_{x=x_i}\n$$\nUsing the product rule, the only non-zero term in the sum at $x=x_i$ is the one where the factor $(x-x_i)$ is differentiated. This yields:\n$$\n\\ell_j'(x_i) = \\frac{1}{\\prod_{k \\neq j}(x_j-x_k)} \\prod_{k \\neq j, k \\neq i} (x_i-x_k)\n$$\nWe introduce the barycentric weights $\\beta_j = 1 / \\prod_{k \\neq j} (x_j - x_k)$. The expression for $\\ell_j'(x_i)$ can be rewritten in terms of these weights:\n$$\nD_{ij} = \\ell_j'(x_i) = \\frac{\\beta_j}{\\beta_i} \\frac{1}{x_i - x_j} \\quad \\text{for } i \\neq j\n$$\nThis formula is known to be numerically stable for reasonable distributions of nodes.\n\nFor the diagonal entries $D_{ii}$, we use the property that the derivative of a constant function is zero. Let $u(x)=1$, so $u_j=1$ for all $j$. Its polynomial interpolant is exactly $p(x)=1$, and its derivative is $p'(x)=0$. At any node $x_i$, we must have:\n$$\np'(x_i) = \\sum_{j=0}^{N} D_{ij} u_j = \\sum_{j=0}^{N} D_{ij} (1) = 0\n$$\nThis implies that the sum of each row of the differentiation matrix is zero. We can, therefore, compute the diagonal entries as:\n$$\nD_{ii} = - \\sum_{j=0, j \\neq i}^{N} D_{ij}\n$$\nThis procedure avoids catastrophic cancellation that can occur when differentiating the formula for $\\ell_i(x)$ directly and provides a stable algorithm for constructing $D$.\n\n### 2. Derivation of the Discrete Wavenumber $\\kappa_d$\n\nWe are tasked to find the real wavenumber $\\kappa_d$ that minimizes the mass-weighted least-squares residual $r(\\kappa) = \\| D\\mathbf{u} - \\mathrm{i}\\kappa\\mathbf{u} \\|_M$ for a given nodal vector $\\mathbf{u} \\in \\mathbb{C}^{N+1}$ corresponding to the plane wave $u(x) = \\exp(\\mathrm{i}\\kappa x)$. The norm is defined as $\\|\\mathbf{v}\\|_M^2 = \\mathbf{v}^* M \\mathbf{v}$, where $M$ is the diagonal mass matrix with entries $M_{ii}=w_i > 0$ and $\\mathbf{v}^*$ is the conjugate transpose of $\\mathbf{v}$.\n\nMinimizing $r(\\kappa)$ is equivalent to minimizing its square, $r(\\kappa)^2$:\n$$\nr(\\kappa)^2 = \\| D\\mathbf{u} - \\mathrm{i}\\kappa\\mathbf{u} \\|_M^2 = (D\\mathbf{u} - \\mathrm{i}\\kappa\\mathbf{u})^* M (D\\mathbf{u} - \\mathrm{i}\\kappa\\mathbf{u})\n$$\nExpanding this expression, and using the fact that $\\kappa$ is real:\n$$\nr(\\kappa)^2 = ((D\\mathbf{u})^* + \\mathrm{i}\\kappa\\mathbf{u}^*) M (D\\mathbf{u} - \\mathrm{i}\\kappa\\mathbf{u})\n$$\n$$\nr(\\kappa)^2 = (D\\mathbf{u})^*M(D\\mathbf{u}) - \\mathrm{i}\\kappa(D\\mathbf{u})^*M\\mathbf{u} + \\mathrm{i}\\kappa\\mathbf{u}^*M(D\\mathbf{u}) + \\kappa^2\\mathbf{u}^*M\\mathbf{u}\n$$\nThis is a quadratic function of $\\kappa$ of the form $f(\\kappa) = a\\kappa^2 + b\\kappa + c$, where:\n$a = \\mathbf{u}^*M\\mathbf{u}$\n$b = \\mathrm{i}(\\mathbf{u}^*M(D\\mathbf{u}) - (D\\mathbf{u})^*M\\mathbf{u})$\n$c = (D\\mathbf{u})^*M(D\\mathbf{u})$\n\nLet $z = \\mathbf{u}^*M(D\\mathbf{u})$. Then $(D\\mathbf{u})^*M\\mathbf{u} = ((D\\mathbf{u})^*M\\mathbf{u})^* = \\mathbf{u}^*M^*(D\\mathbf{u})^{**} = \\mathbf{u}^*M(D\\mathbf{u}) = z^*$, since $M$ is real and diagonal, thus Hermitian ($M^*=M$).\nThe coefficient $b$ becomes $b = \\mathrm{i}(z - z^*) = \\mathrm{i}(2\\mathrm{i}\\operatorname{Im}(z)) = -2\\operatorname{Im}(z)$.\nThe quadratic is $r(\\kappa)^2 = (\\mathbf{u}^*M\\mathbf{u})\\kappa^2 - 2\\operatorname{Im}(\\mathbf{u}^*M(D\\mathbf{u}))\\kappa + (D\\mathbf{u})^*M(D\\mathbf{u})$.\nTo find the minimum, we take the derivative with respect to $\\kappa$ and set it to zero:\n$$\n\\frac{d}{d\\kappa} r(\\kappa)^2 = 2(\\mathbf{u}^*M\\mathbf{u})\\kappa - 2\\operatorname{Im}(\\mathbf{u}^*M(D\\mathbf{u})) = 0\n$$\nSince the mass matrix $M$ is positive definite, $\\mathbf{u}^*M\\mathbf{u} > 0$ for any non-zero $\\mathbf{u}$. Solving for $\\kappa$, we obtain the discrete wavenumber $\\kappa_d$:\n$$\n\\kappa_d = \\frac{\\operatorname{Im}(\\mathbf{u}^* M D \\mathbf{u})}{\\mathbf{u}^* M \\mathbf{u}}\n$$\nThis expression provides $\\kappa_d$ purely in terms of the differentiation matrix $D$, the mass matrix $M$, and the nodal vector $\\mathbf{u}$.\n\n### 3. Computation of Relative Dispersion Error $E(N,\\kappa)$\n\nFor each scheme (Gauss-Legendre, GL, and Gauss-Lobatto-Legendre, GLL) and each given pair $(N, \\kappa)$, the relative dispersion error $E = |\\kappa_d - \\kappa|/|\\kappa|$ is computed as follows:\n1.  For the specified degree $N$, determine the $N+1$ quadrature nodes $\\{x_i\\}_{i=0}^N$ and weights $\\{w_i\\}_{i=0}^N$ for the chosen scheme (GL or GLL).\n2.  Construct the $(N+1) \\times (N+1)$ differentiation matrix $D$ using the stable barycentric procedure described in Part 1.\n3.  Construct the diagonal mass matrix $M$ with entries $M_{ii} = w_i$.\n4.  Sample the plane wave $u(x) = \\exp(\\mathrm{i}\\kappa x)$ at the nodes to form the vector $\\mathbf{u}$ with entries $u_i = \\exp(\\mathrm{i}\\kappa x_i)$.\n5.  Compute the discrete wavenumber $\\kappa_d$ using the formula derived in Part 2.\n6.  Calculate the relative dispersion error $E(N,\\kappa) = |\\kappa_d - \\kappa| / |\\kappa|$.\n\n### 4. Computation of Mass Matrix Diagonality $\\rho(N)$\n\nThis task quantifies the deviation of the exact consistent mass matrix $M_{\\text{true}}$ from a purely diagonal form. The entries of $M_{\\text{true}}$ are $M_{ij}^{\\text{true}} = \\int_{-1}^1 \\ell_i(x)\\ell_j(x)\\,\\mathrm{d}x$.\n\nFor the Gauss-Legendre (GL) scheme with $N+1$ nodes, the quadrature rule is exact for polynomials of degree up to $2(N+1)-1=2N+1$. Since $\\ell_i(x)\\ell_j(x)$ is a polynomial of degree $2N$, and $2N \\le 2N+1$ for $N \\ge 0$, the integral is computed exactly by the quadrature rule itself:\n$$\nM_{ij}^{\\text{true}} = \\int_{-1}^1 \\ell_i(x)\\ell_j(x)\\,\\mathrm{d}x = \\sum_{k=0}^N w_k \\ell_i(x_k)\\ell_j(x_k) = \\sum_{k=0}^N w_k \\delta_{ik}\\delta_{jk} = w_i\\delta_{ij}\n$$\nThus, for the GL scheme, the true mass matrix $M_{\\text{true}}$ is identical to the diagonal quadrature-based mass matrix $M_{\\text{quad}}$. Consequently, $M_{\\text{true}} - \\operatorname{diag}(\\operatorname{diag}(M_{\\text{true}}))$ is a zero matrix, and the normalized off-diagonal Frobenius norm $\\rho_{\\text{GL}}(N)$ is theoretically zero. Any non-zero result would be due to finite-precision arithmetic.\n\nFor the Gauss-Lobatto-Legendre (GLL) scheme with $N+1$ nodes, the degree of exactness is $2N-1$. Since $2N > 2N-1$ for $N \\ge 1$, the GLL quadrature does not exactly integrate $\\ell_i(x)\\ell_j(x)$, and $M_{\\text{true}}$ is not diagonal. This effect is known as mass lumping.\n\nTo compute $\\rho(N)$ for a given $N$, we perform the following steps for both GL and GLL node sets:\n1.  Determine the $N+1$ nodes $\\{x_j\\}$ for the scheme.\n2.  Approximate the integral for $M_{ij}^{\\text{true}}$ using a high-order Gauss-Legendre quadrature with $Q=200$ points, denoted $\\{y_q, w'_q\\}_{q=0}^{Q-1}$.\n$$\nM_{ij}^{\\text{true}} \\approx \\sum_{q=0}^{Q-1} w'_q \\ell_i(y_q) \\ell_j(y_q)\n$$\n3.  The values $\\ell_j(y_q)$ are computed robustly using the second barycentric formula. Let $\\beta_j$ be the barycentric weights for the nodes $\\{x_j\\}$. Then:\n$$\n\\ell_j(y_q) = \\frac{ \\frac{\\beta_j}{y_q - x_j} }{ \\sum_{k=0}^N \\frac{\\beta_k}{y_q - x_k} }\n$$\nThis allows evaluation of the basis functions at the quadrature points $\\{y_q\\}$.\n4.  Assemble the matrix $M_{\\text{true}}$ using the high-order quadrature sum. In matrix notation, if $V$ is the $Q \\times (N+1)$ matrix with entries $V_{qj} = \\ell_j(y_q)$ and $W'$ is the diagonal $Q \\times Q$ matrix of weights $w'_q$, then $M_{\\text{true}} \\approx V^T W' V$.\n5.  Compute the normalized off-diagonal Frobenius norm:\n$$\n\\rho(N) = \\frac{\\| M_{\\text{true}} - \\operatorname{diag}(\\operatorname{diag}(M_{\\text{true}})) \\|_F}{\\| M_{\\text{true}} \\|_F}\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_legendre, legendre, eval_legendre\n\ndef solve():\n    \"\"\"\n    Main solver function that computes and prints the results for the given test cases.\n    \"\"\"\n\n    def get_gl_nodes_weights(n_points):\n        \"\"\"\n        Computes n-point Gauss-Legendre nodes and weights.\n        \"\"\"\n        nodes, weights = roots_legendre(n_points)\n        return nodes, weights\n\n    def get_gll_nodes_weights(n_points):\n        \"\"\"\n        Computes n-point Gauss-Lobatto-Legendre nodes and weights.\n        n_points = N + 1, where N is the polynomial degree.\n        \"\"\"\n        if n_points < 2:\n            raise ValueError(\"GLL quadrature requires at least 2 points.\")\n        \n        N = n_points - 1\n        \n        if N > 0:\n            p_n_prime_roots = legendre(N).deriv(1).roots\n        else:\n            p_n_prime_roots = np.array([])\n            \n        nodes = np.concatenate(([-1.0], np.real(p_n_prime_roots), [1.0]))\n        nodes.sort()\n        \n        weights = 2 / (N * (N + 1) * eval_legendre(N, nodes)**2)\n        \n        return nodes, weights\n\n    def barycentric_weights(nodes):\n        \"\"\"\n        Computes barycentric weights for a given set of nodes.\n        \"\"\"\n        n = len(nodes)\n        weights = np.ones(n, dtype=np.float64)\n        for j in range(n):\n            prod = 1.0\n            for k in range(n):\n                if k != j:\n                    prod *= (nodes[j] - nodes[k])\n            weights[j] = 1.0 / prod\n        return weights\n\n    def differentiation_matrix(nodes):\n        \"\"\"\n        Constructs the differentiation matrix using the barycentric formula.\n        \"\"\"\n        n = len(nodes)\n        b_weights = barycentric_weights(nodes)\n        D = np.zeros((n, n), dtype=np.float64)\n        \n        for i in range(n):\n            row_sum = 0.0\n            for j in range(n):\n                if i == j:\n                    continue\n                term = (b_weights[j] / b_weights[i]) / (nodes[i] - nodes[j])\n                D[i, j] = term\n                row_sum += term\n            D[i, i] = -row_sum\n        return D\n\n    def compute_dispersion_error(N, kappa, scheme):\n        \"\"\"\n        Computes the relative dispersion error E(N, kappa).\n        \"\"\"\n        n_points = N + 1\n        \n        if scheme == 'GL':\n            nodes, weights = get_gl_nodes_weights(n_points)\n        elif scheme == 'GLL':\n            nodes, weights = get_gll_nodes_weights(n_points)\n        else:\n            raise ValueError(\"Unknown scheme\")\n\n        D = differentiation_matrix(nodes)\n        M = np.diag(weights)\n        u = np.exp(1j * kappa * nodes)\n        \n        u_H = u.conj().T\n        \n        numerator = np.imag(u_H @ M @ D @ u)\n        denominator = u_H @ M @ u\n        \n        kappa_d = numerator / denominator.real\n        \n        if np.abs(kappa) == 0:\n            return np.abs(kappa_d)\n            \n        return np.abs(kappa_d - kappa) / np.abs(kappa)\n\n    def compute_mass_matrix_diagonality(N, scheme, Q=200):\n        \"\"\"\n        Computes the normalized off-diagonal Frobenius norm rho(N).\n        \"\"\"\n        n_points = N + 1\n        \n        if scheme == 'GL':\n            x_nodes, _ = get_gl_nodes_weights(n_points)\n        elif scheme == 'GLL':\n            x_nodes, _ = get_gll_nodes_weights(n_points)\n        else:\n            raise ValueError(\"Unknown scheme\")\n\n        b_weights = barycentric_weights(x_nodes)\n        y_q, w_q = get_gl_nodes_weights(Q)\n        \n        V_mat = np.zeros((Q, n_points), dtype=np.float64)\n        \n        # Denominator for second barycentric formula\n        bary_denom = np.zeros(Q, dtype=np.float64)\n        for k in range(n_points):\n            bary_denom += b_weights[k] / (y_q - x_nodes[k])\n\n        for j in range(n_points):\n            # Numerator for second barycentric formula\n            bary_num = b_weights[j] / (y_q - x_nodes[j])\n            V_mat[:, j] = bary_num / bary_denom\n            \n            # Handle cases where y_q is very close to a node x_j\n            # This is unlikely for roots of different Legendre polynomials\n            # but good practice for robustness.\n            exact_indices = np.where(np.isclose(y_q, x_nodes[j]))\n            if len(exact_indices[0]) > 0:\n                V_mat[exact_indices, :] = 0.0\n                V_mat[exact_indices, j] = 1.0\n\n        M_true = V_mat.T @ np.diag(w_q) @ V_mat\n        \n        # For GL, M_true is theoretically diagonal. rho should be near zero.\n        # This implementation computes it explicitly for both cases as a validation.\n\n        M_diag_only = np.diag(np.diag(M_true))\n        \n        num_norm = np.linalg.norm(M_true - M_diag_only, 'fro')\n        den_norm = np.linalg.norm(M_true, 'fro')\n        \n        if den_norm == 0:\n            return 0.0\n            \n        return num_norm / den_norm\n\n    # Test suite from the problem statement\n    dispersion_tests = [\n        (2, 1.0),\n        (4, 2.5),\n        (8, 7.0),\n        (6, 1e-6)\n    ]\n    mass_matrix_test_N = 6\n\n    results = []\n\n    # Run dispersion error tests\n    for N, kappa in dispersion_tests:\n        E_GL = compute_dispersion_error(N, kappa, 'GL')\n        E_GLL = compute_dispersion_error(N, kappa, 'GLL')\n        results.extend([E_GL, E_GLL])\n\n    # Run mass matrix diagonality test\n    rho_GL = compute_mass_matrix_diagonality(mass_matrix_test_N, 'GL')\n    rho_GLL = compute_mass_matrix_diagonality(mass_matrix_test_N, 'GLL')\n    results.extend([rho_GL, rho_GLL])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond simple, straight-sided elements, this advanced practice addresses a crucial challenge in modeling complex geometries: curved boundaries. When an element mapping is non-affine, its Jacobian determinant is no longer a constant, introducing a non-polynomial factor that can corrupt the accuracy of a fixed-order quadrature rule. This exercise introduces a powerful modern technique, a posteriori error estimation, to quantify this geometric error and intelligently adapt the quadrature order to preserve the spectral accuracy of the method . This is essential for robust and efficient simulation on realistic, curved domains.",
            "id": "3388902",
            "problem": "Consider a reference hexahedron given by the Cartesian product of three intervals, $[-1,1] \\times [-1,1] \\times [-1,1]$, with coordinates $(\\xi,\\eta,\\zeta)$. Let a smooth, curved trilinear-like mapping $\\Phi : [-1,1]^3 \\to \\mathbb{R}^3$ be defined componentwise by\n$$\nx(\\xi,\\eta,\\zeta) = \\xi + \\alpha \\sin(\\xi)\\cos(\\eta)\\sin(\\zeta),\\quad\ny(\\xi,\\eta,\\zeta) = \\eta + \\beta \\sin(\\eta)\\cos(\\zeta)\\sin(\\xi),\\quad\nz(\\xi,\\eta,\\zeta) = \\zeta + \\gamma \\sin(\\zeta)\\cos(\\xi)\\sin(\\eta),\n$$\nwhere all angles are expressed in radians and the parameters $\\alpha$, $\\beta$, and $\\gamma$ control the curvature. Denote by $J(\\xi,\\eta,\\zeta)$ the Jacobian determinant of the mapping $\\Phi$, i.e., $J(\\xi,\\eta,\\zeta) = \\det\\left(\\frac{\\partial(x,y,z)}{\\partial(\\xi,\\eta,\\zeta)}\\right)$.\n\nIn spectral and discontinuous Galerkin methods, volume and surface integrals are often computed using Gaussian quadrature rules. Let $\\Pi_q$ denote the orthogonal $L^2([-1,1]^3)$ projection onto the tensor-product polynomial space $\\mathcal{P}_q \\otimes \\mathcal{P}_q \\otimes \\mathcal{P}_q$, where $\\mathcal{P}_q$ is the space of polynomials of degree at most $q$ on $[-1,1]$. Let $\\{L_n(t)\\}_{n\\ge 0}$ be the sequence of Legendre polynomials on $[-1,1]$, with the standard orthogonality relation\n$$\n\\int_{-1}^1 L_m(t)L_n(t)\\,dt = \\frac{2}{2n+1}\\,\\delta_{mn}.\n$$\n\nA fundamental basis for a posteriori estimation of geometric quadrature error is the inequality derived from the Cauchy–Schwarz inequality:\n$$\n\\left|\\int_{[-1,1]^3} g(\\xi,\\eta,\\zeta)\\,\\big(J(\\xi,\\eta,\\zeta)-\\Pi_q J(\\xi,\\eta,\\zeta)\\big)\\,d\\xi\\,d\\eta\\,d\\zeta\\right|\n\\le \\|g\\|_{L^2([-1,1]^3)}\\;\\|J-\\Pi_q J\\|_{L^2([-1,1]^3)},\n$$\nwhere $g$ is any integrand, such as a product of trial/test polynomials and a coefficient. This suggests an a posteriori estimator based on the $L^2$ norm of the projection defect $\\|J-\\Pi_q J\\|$. For non-polynomial coefficients $c(x,y,z)$ appearing in discontinuous Galerkin formulations, a similar estimator $\\|c\\circ\\Phi - \\Pi_q(c\\circ\\Phi)\\|$ can be used to adapt the quadrature to maintain spectral convergence.\n\nYou must implement a program that:\n- Computes the Legendre triple-expansion coefficients of $J(\\xi,\\eta,\\zeta)$ and of $c(\\xi,\\eta,\\zeta) := c\\big(x(\\xi,\\eta,\\zeta),y(\\xi,\\eta,\\zeta),z(\\xi,\\eta,\\zeta)\\big)$, using high-order Gauss–Legendre quadrature in each coordinate on $[-1,1]$.\n- Uses the orthogonality of Legendre polynomials and the definition of $\\Pi_q$ to compute the posteriori estimators\n$$\nE_J(q) := \\|J - \\Pi_q J\\|_{L^2([-1,1]^3)},\\quad\nE_c(q) := \\|c\\circ\\Phi - \\Pi_q(c\\circ\\Phi)\\|_{L^2([-1,1]^3)}.\n$$\n- Finds the smallest integer $q$ such that both $E_J(q) \\le \\varepsilon$ and $E_c(q) \\le \\varepsilon$ for the specified tolerance $\\varepsilon$. Set $q_{\\text{req}}$ to that value (if no $q$ up to the search limit satisfies the tolerance, use the largest searched $q$).\n- Based on a given discontinuous Galerkin polynomial degree $p$, computes the minimal one-dimensional quadrature orders required to exactly integrate the product of a degree-$p$ polynomial with the projected quantity $\\Pi_q$ for three classical Gaussian rules:\n  1. Gauss–Legendre (exact for polynomials up to degree $2N-1$): choose $N_{\\mathrm{GL}}$ as the smallest integer satisfying $p+q \\le 2N_{\\mathrm{GL}}-1$.\n  2. Gauss–Lobatto (exact for polynomials up to degree $2N-3$): choose $N_{\\mathrm{GLL}}$ as the smallest integer satisfying $p+q \\le 2N_{\\mathrm{GLL}}-3$.\n  3. Gauss–Radau (exact for polynomials up to degree $2N-2$): choose $N_{\\mathrm{GR}}$ as the smallest integer satisfying $p+q \\le 2N_{\\mathrm{GR}}-2$.\n\nUse the following test suite of parameter sets, which exercises mild curvature, stronger curvature, and a boundary affine case:\n- Case 1 (mild curvature):\n  - $(\\alpha,\\beta,\\gamma) = (0.20,\\,0.15,\\,0.10)$.\n  - Coefficient $c(x,y,z) = \\exp\\big(0.7x - 0.3y + 0.5z\\big)$.\n  - Discontinuous Galerkin degree $p=4$.\n  - Tolerance $\\varepsilon = 10^{-6}$.\n- Case 2 (stronger curvature):\n  - $(\\alpha,\\beta,\\gamma) = (0.45,\\,0.35,\\,0.40)$.\n  - Coefficient $c(x,y,z) = \\cos(x) + y^2 + z^3$.\n  - Discontinuous Galerkin degree $p=6$.\n  - Tolerance $\\varepsilon = 10^{-6}$.\n- Case 3 (affine boundary case):\n  - $(\\alpha,\\beta,\\gamma) = (0,\\,0,\\,0)$.\n  - Coefficient $c(x,y,z) = 1$.\n  - Discontinuous Galerkin degree $p=3$.\n  - Tolerance $\\varepsilon = 10^{-12}$.\n\nImplementation details and fundamental base constraints:\n- Use the orthogonality of Legendre polynomials on $[-1,1]$ and the $L^2$ projection defined by Legendre tensor-product bases. If $a_{ijk}$ are the triple-expansion coefficients of a function $f(\\xi,\\eta,\\zeta)$ in the Legendre basis, then for $\\Pi_q f$ the error norm satisfies\n$$\n\\|f - \\Pi_q f\\|_{L^2([-1,1]^3)}^2 = \\sum_{\\substack{0\\le i,j,k\\\\ \\text{not all}\\le q}} |a_{ijk}|^2 \\left(\\frac{2}{2i+1}\\right)\\left(\\frac{2}{2j+1}\\right)\\left(\\frac{2}{2k+1}\\right).\n$$\n- To compute $a_{ijk}$, use separability of the tensor-product quadrature weights to perform three successive contractions of the weighted function values with the precomputed values of $L_i(\\xi)$, $L_j(\\eta)$, and $L_k(\\zeta)$. Do not assume any shortcut formula for $J$ or for the projections.\n- Use high-order Gauss–Legendre quadrature in each dimension for assembling the inner products, with angles in radians.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results for the three cases as a list of three lists, each inner list holding the three integers $[N_{\\mathrm{GL}},N_{\\mathrm{GLL}},N_{\\mathrm{GR}}]$ for that case, in the same order as the cases above. The format must be exactly a single Python list literal on one line, for example, \"[[n11,n12,n13],[n21,n22,n23],[n31,n32,n33]]\", with no extra spaces or text.\n\nYour implementation must be in Python, be fully self-contained, use angles in radians, and rely only on the standard library, NumPy, and SciPy as specified. No input should be read from the user or external files; all values must be hard-coded from the test suite above.",
            "solution": "The starting point is the $L^2$ orthogonality of Legendre polynomials on $[-1,1]$, namely that for all nonnegative integers $m$ and $n$,\n$$\n\\int_{-1}^1 L_m(t)\\,L_n(t)\\,dt = \\frac{2}{2n+1}\\,\\delta_{mn}.\n$$\nOn the reference hexahedron, a tensor-product basis is formed by $\\{L_i(\\xi)L_j(\\eta)L_k(\\zeta)\\}_{i,j,k\\ge 0}$. The $L^2([-1,1]^3)$ projection $\\Pi_q$ onto $\\mathcal{P}_q \\otimes \\mathcal{P}_q \\otimes \\mathcal{P}_q$ is defined by orthogonality: $f - \\Pi_q f$ is orthogonal to every basis function with an index triple $(i,j,k)$ satisfying $0\\le i,j,k\\le q$. If $a_{ijk}$ are the Legendre triple-expansion coefficients of $f$,\n$$\na_{ijk} = \\frac{\\int_{[-1,1]^3} f(\\xi,\\eta,\\zeta)\\,L_i(\\xi)L_j(\\eta)L_k(\\zeta)\\,d\\xi\\,d\\eta\\,d\\zeta}{\\left(\\frac{2}{2i+1}\\right)\\left(\\frac{2}{2j+1}\\right)\\left(\\frac{2}{2k+1}\\right)},\n$$\nthen the projection is $\\Pi_q f = \\sum_{0\\le i,j,k\\le q} a_{ijk} L_i(\\xi)L_j(\\eta)L_k(\\zeta)$. By orthogonality, the squared norm of the projection defect equals the sum of the squared coefficients weighted by the basis norms over indices not captured by the projection:\n$$\n\\|f-\\Pi_q f\\|_{L^2([-1,1]^3)}^2\n= \\sum_{\\substack{i,j,k\\ge 0\\\\ \\text{not all}\\le q}} |a_{ijk}|^2 \\,\\left(\\frac{2}{2i+1}\\right)\\left(\\frac{2}{2j+1}\\right)\\left(\\frac{2}{2k+1}\\right).\n$$\nThis identity provides a principled a posteriori estimator, since the projection coefficients can be computed numerically using high-order Gauss–Legendre quadrature in each dimension. For smooth $f$, such as the Jacobian determinant $J(\\xi,\\eta,\\zeta)$ and a composed coefficient $c(\\xi,\\eta,\\zeta)=c(x(\\xi,\\eta,\\zeta),y(\\xi,\\eta,\\zeta),z(\\xi,\\eta,\\zeta))$, spectral convergence (i.e., exponential decay in $q$) is expected due to analyticity, and decreasing $\\|f-\\Pi_q f\\|$ provides a robust criterion for quadrature adaptation.\n\nFor the quadrature rules considered:\n- Gauss–Legendre with $N$ points per dimension is exact for univariate polynomials up to degree $2N-1$.\n- Gauss–Lobatto with $N$ points per dimension (including endpoints) is exact up to degree $2N-3$.\n- Gauss–Radau with $N$ points per dimension (including one endpoint) is exact up to degree $2N-2$.\n\nIn a discontinuous Galerkin formulation with polynomial degree $p$, a typical volume integrand is a product of a degree-$p$ polynomial $g$ and a projected non-polynomial factor $\\Pi_q h$ (with $h$ being $J$ or $c\\circ\\Phi$). The product has degree at most $p+q$ in each coordinate. To guarantee exactness of the quadrature for such products in each dimension, one must choose the smallest $N$ satisfying:\n$$\np+q \\le 2N_{\\mathrm{GL}}-1,\\quad\np+q \\le 2N_{\\mathrm{GLL}}-3,\\quad\np+q \\le 2N_{\\mathrm{GR}}-2,\n$$\nrespectively. Concretely,\n$$\nN_{\\mathrm{GL}} = \\left\\lceil \\frac{p+q+1}{2} \\right\\rceil,\\quad\nN_{\\mathrm{GLL}} = \\left\\lceil \\frac{p+q+3}{2} \\right\\rceil,\\quad\nN_{\\mathrm{GR}} = \\left\\lceil \\frac{p+q+2}{2} \\right\\rceil.\n$$\n\nAlgorithmic design:\n1. Choose a sufficiently high Gauss–Legendre order $M$ per dimension to approximate the Legendre inner products accurately for the smooth functions under consideration. Evaluate nodes and weights on $[-1,1]$ in each coordinate.\n2. Precompute arrays of Legendre polynomial values $L_n$ at the Gauss–Legendre nodes in each dimension for $n=0,1,\\dots,q_{\\max}$ via the three-term recurrence\n$$\nL_0(t)=1,\\quad L_1(t)=t,\\quad L_{n+1}(t)=\\frac{(2n+1)tL_n(t)-nL_{n-1}(t)}{n+1}.\n$$\n3. For each test case, form the mapping $\\Phi$ and compute the Jacobian determinant $J(\\xi,\\eta,\\zeta)$ on the tensor grid of nodes. Compute $c(\\xi,\\eta,\\zeta)$ on that grid using the given coefficient function (composed with $\\Phi$).\n4. Assemble the inner products $\\int J L_i L_j L_k$ and $\\int (c\\circ\\Phi) L_i L_j L_k$ by multiplying the function values by the tensor-product quadrature weights and performing three successive contractions with the Legendre value arrays (first in $\\xi$, then $\\eta$, then $\\zeta$). Divide by the normalization factors $\\left(\\frac{2}{2i+1}\\right)\\left(\\frac{2}{2j+1}\\right)\\left(\\frac{2}{2k+1}\\right)$ to obtain the coefficients $a_{ijk}$.\n5. For each $q=0,1,\\dots,q_{\\max}$, compute the posteriori estimators $E_J(q)$ and $E_c(q)$ by summing the squared coefficients weighted by the basis norms over all index triples not inside the $q$-cube $\\{0,\\dots,q\\}^3$.\n6. Select $q_{\\text{req}}$ as the smallest $q$ such that both $E_J(q)\\le \\varepsilon$ and $E_c(q)\\le \\varepsilon$; if none satisfy the tolerance, set $q_{\\text{req}}=q_{\\max}$.\n7. Compute the minimal one-dimensional quadrature orders as\n$$\nN_{\\mathrm{GL}} = \\left\\lceil \\frac{p+q_{\\text{req}}+1}{2} \\right\\rceil,\\quad\nN_{\\mathrm{GLL}} = \\left\\lceil \\frac{p+q_{\\text{req}}+3}{2} \\right\\rceil,\\quad\nN_{\\mathrm{GR}} = \\left\\lceil \\frac{p+q_{\\text{req}}+2}{2} \\right\\rceil.\n$$\n\nThis approach is principle-based: it uses the core definitions of orthogonal projection and Gaussian quadrature exactness, along with the Cauchy–Schwarz inequality, to derive a computable a posteriori estimator for the geometric and coefficient-induced quadrature errors. By enforcing exactness for the polynomial part and controlling the projection defect of the non-polynomial factors, the method adapts the quadrature order to preserve spectral convergence.\n\nThe final program implements these steps for the specified test suite and outputs a single line with the three triplets $[N_{\\mathrm{GL}},N_{\\mathrm{GLL}},N_{\\mathrm{GR}}]$ for Cases 1–3.",
            "answer": "```python\nimport numpy as np\nfrom numpy.polynomial.legendre import leggauss\n\ndef legendre_values(x, q_max):\n    \"\"\"\n    Compute Legendre polynomial values L_n(x) for n=0..q_max using recurrence.\n    Returns an array of shape (len(x), q_max+1).\n    \"\"\"\n    npts = x.size\n    L = np.zeros((npts, q_max + 1), dtype=float)\n    L[:, 0] = 1.0\n    if q_max >= 1:\n        L[:, 1] = x\n        for n in range(1, q_max):\n            L[:, n + 1] = ((2 * n + 1) * x * L[:, n] - n * L[:, n - 1]) / (n + 1)\n    return L\n\ndef mapping_and_jacobian(xi, eta, zeta, alpha, beta, gamma):\n    \"\"\"\n    Compute the mapping (x,y,z) and the Jacobian determinant J for the given parameters.\n    xi, eta, zeta are 3D arrays of shape (nx, ny, nz).\n    \"\"\"\n    # Trigonometric factors\n    sin_xi = np.sin(xi); cos_xi = np.cos(xi)\n    sin_eta = np.sin(eta); cos_eta = np.cos(eta)\n    sin_zeta = np.sin(zeta); cos_zeta = np.cos(zeta)\n\n    # Mapping components\n    x = xi + alpha * sin_xi * cos_eta * sin_zeta\n    y = eta + beta * sin_eta * cos_zeta * sin_xi\n    z = zeta + gamma * sin_zeta * cos_xi * sin_eta\n\n    # Partial derivatives\n    # dx/dxi, dx/deta, dx/dzeta\n    dxdxi = 1.0 + alpha * cos_xi * cos_eta * sin_zeta\n    dxdeta = -alpha * sin_xi * sin_eta * sin_zeta\n    dxdzeta = alpha * sin_xi * cos_eta * cos_zeta\n\n    # dy/dxi, dy/deta, dy/dzeta\n    dydxi = beta * sin_eta * cos_zeta * cos_xi\n    dydeta = 1.0 + beta * cos_eta * cos_zeta * sin_xi\n    dydzeta = -beta * sin_eta * sin_zeta * sin_xi\n\n    # dz/dxi, dz/deta, dz/dzeta\n    dzdxi = -gamma * sin_zeta * sin_xi * sin_eta\n    dzdeta = gamma * sin_zeta * cos_xi * cos_eta\n    dzdzeta = 1.0 + gamma * cos_zeta * cos_xi * sin_eta\n\n    # Jacobian determinant for 3x3 matrix\n    # det = a11*(a22*a33 - a23*a32) - a12*(a21*a33 - a23*a31) + a13*(a21*a32 - a22*a31)\n    a11 = dxdxi; a12 = dxdeta; a13 = dxdzeta\n    a21 = dydxi; a22 = dydeta; a23 = dydzeta\n    a31 = dzdxi; a32 = dzdeta; a33 = dzdzeta\n\n    J = (\n        a11 * (a22 * a33 - a23 * a32)\n        - a12 * (a21 * a33 - a23 * a31)\n        + a13 * (a21 * a32 - a22 * a31)\n    )\n\n    return x, y, z, J\n\ndef coefficient_case(x, y, z, case_id):\n    \"\"\"\n    Compute the coefficient c(x,y,z) for the given case_id.\n    \"\"\"\n    if case_id == 1:\n        return np.exp(0.7 * x - 0.3 * y + 0.5 * z)\n    elif case_id == 2:\n        return np.cos(x) + y**2 + z**3\n    elif case_id == 3:\n        return np.ones_like(x)\n    else:\n        raise ValueError(\"Invalid case_id\")\n\ndef compute_legendre_coeffs_tensor(f_vals, Lx, Ly, Lz, wx, wy, wz):\n    \"\"\"\n    Compute triple Legendre coefficients a_{ijk} for f using separable Gauss-Legendre quadrature.\n    f_vals: array of shape (nx, ny, nz)\n    Lx: (nx, q+1), Ly: (ny, q+1), Lz: (nz, q+1)\n    wx, wy, wz: 1D arrays of weights of length nx, ny, nz\n    Returns coeffs of shape (q+1, q+1, q+1) containing integrals divided by normalization.\n    \"\"\"\n    nx, ny, nz = f_vals.shape\n    q_max = Lx.shape[1] - 1\n\n    # Apply tensor-product weights\n    weighted = f_vals * (wx[:, None, None]) * (wy[None, :, None]) * (wz[None, None, :])\n\n    # First contraction along xi: result shape (q+1, ny, nz)\n    # Lx.T @ weighted.reshape(nx, ny*nz)\n    tmp1 = Lx.T @ weighted.reshape(nx, ny * nz)  # (q+1, ny*nz)\n    tmp1 = tmp1.reshape(q_max + 1, ny, nz)\n\n    # Second contraction along eta: result shape (q+1, q+1, nz)\n    # tensordot Ly.T (q+1,ny) with tmp1 (q+1,ny,nz) -> (q+1,q+1,nz)\n    tmp2 = np.tensordot(Ly.T, tmp1, axes=[1, 1])  # shape (q+1, q+1, nz)\n\n    # Third contraction along zeta: result shape (q+1, q+1, q+1)\n    coeffs_integrals = np.tensordot(Lz.T, tmp2, axes=[1, 2])  # shape (q+1, q+1, q+1)\n\n    # Normalize by Legendre basis norms\n    norms_1d = 2.0 / (2.0 * np.arange(q_max + 1) + 1.0)  # shape (q+1,)\n    norm3D = norms_1d[:, None, None] * norms_1d[None, :, None] * norms_1d[None, None, :]\n    coeffs = coeffs_integrals / norm3D\n\n    return coeffs, norm3D\n\ndef estimator_error(coeffs, norm3D, q):\n    \"\"\"\n    Compute E(q) = ||f - Pi_q f||_L2 from coefficients using orthogonality:\n    sum over indices not all <= q of |a_{ijk}|^2 * norm_{ijk}, square root.\n    \"\"\"\n    q_max = coeffs.shape[0] - 1\n    # Mask for indices i<=q and j<=q and k<=q\n    idx = np.arange(q_max + 1)\n    mask_i = idx[:, None, None] <= q\n    mask_j = idx[None, :, None] <= q\n    mask_k = idx[None, None, :] <= q\n    mask_inside = mask_i & mask_j & mask_k  # shape (q_max+1, q_max+1, q_max+1)\n\n    mask_outside = ~mask_inside\n    err_sq = np.sum((coeffs[mask_outside] ** 2) * (norm3D[mask_outside]))\n    return np.sqrt(err_sq)\n\ndef minimal_q_for_tolerance(coeffs_J, coeffs_c, norm3D, eps, q_max):\n    \"\"\"\n    Find the smallest q in 0..q_max such that both E_J(q) <= eps and E_c(q) <= eps.\n    If none satisfies, return q_max.\n    \"\"\"\n    for q in range(q_max + 1):\n        EJ = estimator_error(coeffs_J, norm3D, q)\n        Ec = estimator_error(coeffs_c, norm3D, q)\n        if EJ <= eps and Ec <= eps:\n            return q\n    return q_max\n\ndef quadrature_orders(p, q):\n    \"\"\"\n    Compute minimal 1D quadrature orders for GL, GLL, and GR rules.\n    \"\"\"\n    N_GL = int(np.ceil((p + q + 1) / 2.0))\n    N_GLL = int(np.ceil((p + q + 3) / 2.0))\n    N_GR = int(np.ceil((p + q + 2) / 2.0))\n    # Ensure minimum of 1 point\n    return max(N_GL, 1), max(N_GLL, 1), max(N_GR, 1)\n\ndef solve():\n    # High-order Gauss-Legendre for inner products\n    M = 50  # number of points per dimension for assembling inner products\n    x_nodes, x_weights = leggauss(M)\n    y_nodes, y_weights = leggauss(M)\n    z_nodes, z_weights = leggauss(M)\n\n    # Max projection degree to search\n    q_max = 14\n\n    # Precompute Legendre values up to q_max\n    Lx = legendre_values(x_nodes, q_max)\n    Ly = legendre_values(y_nodes, q_max)\n    Lz = legendre_values(z_nodes, q_max)\n\n    # Create tensor grid\n    XI = x_nodes[:, None, None] * np.ones((1, M, M))\n    ETA = y_nodes[None, :, None] * np.ones((M, 1, M))\n    ZETA = z_nodes[None, None, :] * np.ones((M, M, 1))\n\n    results = []\n\n    test_cases = [\n        # (alpha, beta, gamma, case_id, p, eps)\n        (0.20, 0.15, 0.10, 1, 4, 1e-6),\n        (0.45, 0.35, 0.40, 2, 6, 1e-6),\n        (0.00, 0.00, 0.00, 3, 3, 1e-12),\n    ]\n\n    for alpha, beta, gamma, case_id, p, eps in test_cases:\n        # Mapping and Jacobian on grid\n        x, y, z, J = mapping_and_jacobian(XI, ETA, ZETA, alpha, beta, gamma)\n        # Coefficient on grid\n        c_vals = coefficient_case(x, y, z, case_id)\n\n        # Compute Legendre coefficients for J and c using separable contractions\n        coeffs_J, norm3D = compute_legendre_coeffs_tensor(J, Lx, Ly, Lz, x_weights, y_weights, z_weights)\n        coeffs_c, _ = compute_legendre_coeffs_tensor(c_vals, Lx, Ly, Lz, x_weights, y_weights, z_weights)\n\n        # Find minimal q satisfying both estimators\n        q_req = minimal_q_for_tolerance(coeffs_J, coeffs_c, norm3D, eps, q_max)\n\n        # Compute quadrature orders\n        N_GL, N_GLL, N_GR = quadrature_orders(p, q_req)\n        results.append([N_GL, N_GLL, N_GR])\n\n    # Print results in required single-line format\n    print(f\"{results}\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}