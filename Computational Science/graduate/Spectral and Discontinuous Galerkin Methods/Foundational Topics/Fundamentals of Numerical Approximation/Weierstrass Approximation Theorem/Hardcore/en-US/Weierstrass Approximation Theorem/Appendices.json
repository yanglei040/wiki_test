{
    "hands_on_practices": [
        {
            "introduction": "The Weierstrass approximation theorem guarantees that any continuous function on a closed interval can be uniformly approximated by a polynomial. This exercise explores the crucial nuance that the \"best\" such approximation is not a unique concept, as it depends entirely on how we measure error. By comparing the minimax polynomial approximant in the uniform norm ($L^{\\infty}$) with the orthogonal projection in a weighted $L^2$ space for the function $f(x)=|x|$, you will gain a deeper appreciation for how the choice of norm fundamentally alters the nature of the optimal approximation.  This practice provides a concrete foundation for understanding the different approximation strategies that underpin various spectral methods.",
            "id": "3428460",
            "problem": "Let $f(x)=|x|$ on $[-1,1]$. The Weierstrass approximation theorem guarantees that polynomials are dense in $C([-1,1])$ under the uniform norm, a foundational fact underpinning spectral methods and Discontinuous Galerkin (DG) methods that employ polynomial bases. Consider two distinct approximation strategies, each fundamental to spectral analysis with Chebyshev polynomials:\n\n1. The minimax (uniform norm) approximation in $\\mathbb{P}_{1}$, the space of polynomials of degree at most $1$, where the error is measured in the uniform norm $\\|g\\|_{\\infty}=\\sup_{x\\in[-1,1]}|g(x)|$.\n\n2. The Chebyshev weighted Lebesgue $L^{2}$ projection onto $\\mathbb{P}_{1}$ with weight $w(x)=(1-x^{2})^{-1/2}$, i.e., the orthogonal projection with respect to the inner product $\\langle g,h\\rangle_{w}=\\int_{-1}^{1}g(x)h(x)w(x)\\,dx$.\n\nTasks:\n- Compute the exact minimax uniform-norm error $E_{\\infty}=\\inf_{p\\in\\mathbb{P}_{1}}\\|f-p\\|_{\\infty}$ and identify the minimizer $p^{\\star}\\in\\mathbb{P}_{1}$.\n- Compute the exact Chebyshev weighted $L^{2}$ projection $P_{1}^{w}f\\in\\mathbb{P}_{1}$ and the exact weighted $L^{2}$ error $\\|f-P_{1}^{w}f\\|_{L^{2}_{w}}=\\big(\\int_{-1}^{1}(f-p)^{2}w\\,dx\\big)^{1/2}$.\n- Using the Chebyshev series of $f$, derive a rigorous upper bound on the uniform error of the truncated Chebyshev series of degree $N$ for $f$, expressed in terms of $N$ via the tail of its Chebyshev coefficients. State the resulting bound explicitly with constants in closed form.\n\nFinally, report the exact analytical expression for the ratio\n$$\nR=\\frac{E_{\\infty}}{\\|f-P_{1}^{w}f\\|_{L^{2}_{w}}}.\n$$\nNo rounding is required and no units should be used in the final ratio.",
            "solution": "The problem asks for an analysis of two different polynomial approximations of degree at most $1$ for the function $f(x)=|x|$ on the interval $[-1,1]$, and for a bound on the error of the truncated Chebyshev series.\n\n**Part 1: Minimax Approximation**\n\nWe seek the polynomial $p(x) \\in \\mathbb{P}_1$ that minimizes the uniform norm of the error, $\\|f-p\\|_{\\infty}$. Let $p(x) = ax+b$. We want to find $a, b \\in \\mathbb{R}$ that minimize $E(a,b) = \\sup_{x \\in [-1,1]} ||x| - (ax+b)|$. The resulting minimum error is $E_{\\infty}$.\n\nThe function $f(x)=|x|$ is an even function on the symmetric interval $[-1,1]$. It is a known result from approximation theory that the best uniform approximation polynomial $p^{\\star}(x)$ in $\\mathbb{P}_n$ to an even function on $[-1,1]$ is also an even polynomial. For $p(x)=ax+b \\in \\mathbb{P}_1$ to be even, we must have $p(-x)=p(x)$, which means $a(-x)+b = ax+b$. This implies $-ax=ax$ for all $x \\in [-1,1]$, which requires $a=0$. Thus, the best approximation must be a constant polynomial, $p^{\\star}(x)=b$.\n\nWe now need to find the constant $b$ that minimizes $\\sup_{x \\in [-1,1]} ||x|-b|$. Let the error function be $e(x) = |x|-b$. The extreme values of $|e(x)|$ on $[-1,1]$ will occur at the critical points of $e(x)$, which are the endpoints $x=\\pm 1$ and the point where the derivative of $|x|$ is undefined, $x=0$.\nThe error values at these points are:\n$e(-1) = |-1| - b = 1-b$\n$e(0) = |0| - b = -b$\n$e(1) = |1| - b = 1-b$\n\nAccording to the Chebyshev equioscillation theorem, for a polynomial of degree $n=0$ (a constant), there must be at least $n+2=2$ points where the error reaches its maximum magnitude with alternating signs. For $n=1$, our original search space, we'd need $3$ points. Using our three candidate points $x_0=-1$, $x_1=0$, $x_2=1$, we seek to satisfy the equioscillation condition. The error must satisfy $|e(x_0)|=|e(x_1)|=|e(x_2)|=E_{\\infty}$ and the signs must alternate, e.g., $e(x_0) = -e(x_1) = e(x_2)$.\nLet's set $e(-1) = -e(0)$ and $e(1) = -e(0)$.\nThe first condition gives $1-b = -(-b) = b$, which implies $2b=1$, so $b=1/2$.\nThe second condition is identical and also yields $b=1/2$.\nWith $b=1/2$, the polynomial is $p^{\\star}(x) = 1/2$.\nThe error at the equioscillation points is:\n$e(-1) = 1 - 1/2 = 1/2$\n$e(0) = -1/2$\n$e(1) = 1 - 1/2 = 1/2$\nThe maximum error magnitude is $E_{\\infty} = 1/2$. We must verify that for all other $x \\in [-1,1]$, the error magnitude does not exceed this value. For $x \\in [-1,1]$, we have $0 \\le |x| \\le 1$. The error function $e(x)=|x|-1/2$ thus ranges from $-1/2$ (at $x=0$) to $1/2$ (at $x=\\pm 1$). So, $|e(x)| = ||x|-1/2| \\le 1/2$ for all $x \\in [-1,1]$.\nThe conditions of the equioscillation theorem are satisfied for $n=0$ and also for $n=1$ (since $p^\\star \\in \\mathbb{P}_1$ and we have 3 points of equioscillation).\nThe minimax polynomial in $\\mathbb{P}_1$ is $p^{\\star}(x) = 1/2$.\nThe minimax uniform-norm error is $E_{\\infty} = 1/2$.\n\n**Part 2: Chebyshev Weighted $L^2$ Projection**\n\nWe seek the orthogonal projection $P_1^w f$ of $f(x)=|x|$ onto $\\mathbb{P}_1$ with respect to the inner product $\\langle g,h\\rangle_w = \\int_{-1}^1 g(x)h(x)(1-x^2)^{-1/2} dx$.\nThe space $\\mathbb{P}_1$ is spanned by the first two Chebyshev polynomials, $T_0(x)=1$ and $T_1(x)=x$, which form an orthogonal basis.\nThe projection is given by $P_1^w f(x) = c_0 T_0(x) + c_1 T_1(x)$, where the coefficients are\n$c_k = \\frac{\\langle f, T_k \\rangle_w}{\\langle T_k, T_k \\rangle_w}$.\nThe norms of the basis functions are well-known:\n$\\langle T_0, T_0 \\rangle_w = \\int_{-1}^1 1^2 \\frac{dx}{\\sqrt{1-x^2}} = [\\arcsin x]_{-1}^1 = \\pi$.\n$\\langle T_1, T_1 \\rangle_w = \\int_{-1}^1 x^2 \\frac{dx}{\\sqrt{1-x^2}} = \\pi/2$.\n\nFor $c_0$:\n$\\langle f, T_0 \\rangle_w = \\int_{-1}^1 |x| \\frac{dx}{\\sqrt{1-x^2}}$. The integrand is even, so this is $2 \\int_0^1 \\frac{x}{\\sqrt{1-x^2}} dx$.\nUsing substitution $u=1-x^2$, $du=-2xdx$: $2 \\int_1^0 \\frac{-du/2}{\\sqrt{u}} = \\int_0^1 u^{-1/2} du = [2\\sqrt{u}]_0^1 = 2$.\nSo, $c_0 = \\frac{2}{\\pi}$.\n\nFor $c_1$:\n$\\langle f, T_1 \\rangle_w = \\int_{-1}^1 |x|x \\frac{dx}{\\sqrt{1-x^2}}$. The integrand $g(x)=\\frac{x|x|}{\\sqrt{1-x^2}}$ is an odd function, since $g(-x) = \\frac{(-x)|-x|}{\\sqrt{1-(-x)^2}} = -\\frac{x|x|}{\\sqrt{1-x^2}} = -g(x)$. The integral of an odd function over a symmetric interval $[-1,1]$ is $0$.\nSo, $\\langle f, T_1 \\rangle_w = 0$, which gives $c_1 = 0$.\n\nThe projection is $P_1^w f(x) = \\frac{2}{\\pi} T_0(x) + 0 \\cdot T_1(x) = \\frac{2}{\\pi}$.\n\nNext, we compute the weighted $L^2$ error, $\\|f - P_1^w f\\|_{L^2_w}$.\nBy the Pythagorean theorem in Hilbert spaces, for an orthogonal projection:\n$\\|f - P_1^w f\\|_{L^2_w}^2 = \\|f\\|_{L^2_w}^2 - \\|P_1^w f\\|_{L^2_w}^2$.\nFirst term:\n$\\|f\\|_{L^2_w}^2 = \\int_{-1}^1 |x|^2 \\frac{dx}{\\sqrt{1-x^2}} = \\int_{-1}^1 \\frac{x^2}{\\sqrt{1-x^2}} dx$. This integral is equal to $\\langle T_1, T_1 \\rangle_w$, which we already know is $\\pi/2$.\nSecond term:\n$\\|P_1^w f\\|_{L^2_w}^2 = \\|\\frac{2}{\\pi} T_0\\|_{L^2_w}^2 = (\\frac{2}{\\pi})^2 \\langle T_0, T_0 \\rangle_w = \\frac{4}{\\pi^2} \\cdot \\pi = \\frac{4}{\\pi}$.\nSo, the error squared is $\\|f - P_1^w f\\|_{L^2_w}^2 = \\frac{\\pi}{2} - \\frac{4}{\\pi} = \\frac{\\pi^2 - 8}{2\\pi}$.\nThe weighted $L^2$ error is $\\|f - P_1^w f\\|_{L^2_w} = \\sqrt{\\frac{\\pi^2 - 8}{2\\pi}}$.\n\n**Part 3: Uniform Error Bound for Truncated Chebyshev Series**\n\nThe Chebyshev series of $f(x)=|x|$ is $f(x) = \\sum_{k=0}^\\infty a_k T_k(x)$ where $a_k = \\frac{\\langle f, T_k \\rangle_w}{\\langle T_k, T_k \\rangle_w}$.\nWe found $a_0 = c_0 = 2/\\pi$ and $a_1 = c_1 = 0$. Since $f(x)$ is even, $a_k=0$ for all odd $k$.\nFor even $k=2m$ with $m \\ge 1$:\n$a_{2m} = \\frac{\\langle f, T_{2m} \\rangle_w}{\\langle T_{2m}, T_{2m} \\rangle_w} = \\frac{1}{\\pi/2} \\int_{-1}^1 \\frac{|x|T_{2m}(x)}{\\sqrt{1-x^2}} dx = \\frac{4}{\\pi} \\int_0^1 \\frac{x T_{2m}(x)}{\\sqrt{1-x^2}} dx$.\nLet $x=\\cos\\theta$, $dx=-\\sin\\theta d\\theta$:\nThe integral becomes $\\int_{\\pi/2}^0 \\frac{\\cos\\theta \\cos(2m\\theta)}{ \\sin\\theta} (-\\sin\\theta d\\theta) = \\int_0^{\\pi/2} \\cos\\theta \\cos(2m\\theta) d\\theta$.\nUsing the identity $2\\cos A \\cos B = \\cos(A-B)+\\cos(A+B)$:\n$\\frac{1}{2} \\int_0^{\\pi/2} [\\cos((2m-1)\\theta) + \\cos((2m+1)\\theta)] d\\theta = \\frac{1}{2} \\left[ \\frac{\\sin((2m-1)\\theta)}{2m-1} + \\frac{\\sin((2m+1)\\theta)}{2m+1} \\right]_0^{\\pi/2}$\n$= \\frac{1}{2} \\left( \\frac{\\sin(m\\pi-\\pi/2)}{2m-1} + \\frac{\\sin(m\\pi+\\pi/2)}{2m+1} \\right) = \\frac{1}{2} \\left( \\frac{(-1)^{m+1}}{2m-1} + \\frac{(-1)^m}{2m+1} \\right) = \\frac{(-1)^{m+1}}{2} \\left( \\frac{1}{2m-1} - \\frac{1}{2m+1} \\right)$\n$= \\frac{(-1)^{m+1}}{2} \\frac{(2m+1)-(2m-1)}{4m^2-1} = \\frac{(-1)^{m+1}}{4m^2-1}$.\nSo, $\\langle f, T_{2m} \\rangle_w = 2 \\int_0^{\\pi/2} \\cos\\theta \\cos(2m\\theta) d\\theta = 2 \\frac{(-1)^{m+1}}{4m^2-1}$.\n$a_{2m} = \\frac{2(-1)^{m+1}/(4m^2-1)}{\\pi/2} = \\frac{4}{\\pi} \\frac{(-1)^{m+1}}{4m^2-1}$ for $m \\ge 1$.\n\nThe truncated Chebyshev series of degree $N$ is $S_N f = \\sum_{k=0}^N a_k T_k(x)$. The uniform error is bounded by the tail of the coefficients:\n$\\|f - S_N f\\|_{\\infty} = \\|\\sum_{k=N+1}^\\infty a_k T_k(x)\\|_{\\infty} \\le \\sum_{k=N+1}^\\infty |a_k|$, since $|T_k(x)|\\le 1$.\nThe sum is over even indices $2m > N$, i.e., $m > N/2$. The first integer is $m_{min} = \\lfloor N/2 \\rfloor + 1$.\nThe bound is $\\sum_{m=\\lfloor N/2 \\rfloor+1}^\\infty |a_{2m}| = \\sum_{m=\\lfloor N/2 \\rfloor+1}^\\infty \\frac{4}{\\pi} \\frac{1}{4m^2-1}$.\nThe sum can be computed exactly using the telescoping series property of $\\frac{1}{4m^2-1} = \\frac{1}{2}(\\frac{1}{2m-1} - \\frac{1}{2m+1})$:\n$\\sum_{m=M}^\\infty \\frac{1}{4m^2-1} = \\frac{1}{2(2M-1)}$.\nSubstituting $M = \\lfloor N/2 \\rfloor + 1$, the bound is $\\frac{4}{\\pi} \\cdot \\frac{1}{2(2(\\lfloor N/2 \\rfloor + 1) - 1)} = \\frac{2}{\\pi(2\\lfloor N/2 \\rfloor + 1)}$.\nThis can be written explicitly for even and odd $N$:\nIf $N$ is even, $N=2k$, the bound is $\\frac{2}{\\pi(2k+1)} = \\frac{2}{\\pi(N+1)}$.\nIf $N$ is odd, $N=2k-1$, the bound is $\\frac{2}{\\pi(2(k-1)+1)} = \\frac{2}{\\pi(2k-1)} = \\frac{2}{\\pi N}$.\nThe upper bound on the uniform error is $\\frac{2}{\\pi(2\\lfloor N/2 \\rfloor + 1)}$.\n\n**Part 4: Final Ratio**\n\nFinally, we compute the ratio $R = \\frac{E_{\\infty}}{\\|f-P_{1}^{w}f\\|_{L^{2}_{w}}}$.\nUsing the results from Part 1 and Part 2:\n$E_{\\infty} = 1/2$\n$\\|f-P_{1}^{w}f\\|_{L^{2}_{w}} = \\sqrt{\\frac{\\pi^2 - 8}{2\\pi}}$\nThe ratio is:\n$$R = \\frac{1/2}{\\sqrt{\\frac{\\pi^2 - 8}{2\\pi}}} = \\frac{1}{2} \\sqrt{\\frac{2\\pi}{\\pi^2-8}} = \\sqrt{\\frac{1}{4} \\frac{2\\pi}{\\pi^2-8}} = \\sqrt{\\frac{\\pi}{2(\\pi^2-8)}}$$",
            "answer": "$$ \\boxed{\\sqrt{\\frac{\\pi}{2(\\pi^{2}-8)}}} $$"
        },
        {
            "introduction": "The power of the Weierstrass theorem lies not just in the existence of an approximation, but in our ability to construct one with a desired accuracy. This practice translates this theoretical guarantee into the practical language of Discontinuous Galerkin (DG) methods. You will work with a piecewise polynomial space and determine the specific mesh size $h$ and local polynomial degree $p$ required to approximate the function $f(x)=|x|$ within a prescribed error tolerance $\\epsilon$.  This hands-on task directly connects the abstract principles of approximation to the core concepts of $h$- and $p$-refinement, which are central to the convergence and efficiency of modern numerical schemes.",
            "id": "3428493",
            "problem": "Let $f(x) = |x|$ on the compact interval $[-1,1]$. Consider the space of broken (piecewise) polynomials used in Discontinuous Galerkin (DG) methods,\n$$\nV_{h}^{p} := \\left\\{ v \\in L^{\\infty}([-1,1]) \\,\\Big|\\, v|_{K} \\in \\mathbb{P}_{p}(K) \\text{ for each mesh element } K \\text{ of a uniform partition of } [-1,1] \\text{ with element size } h \\right\\},\n$$\nwhere $\\mathbb{P}_{p}(K)$ denotes polynomials of degree at most $p$ restricted to $K$, and $h$ is the length of each element in the uniform partition. Using fundamental properties of continuity on compact sets and the idea underlying the Weierstrass approximation theorem, construct an explicit $v_{h}^{p} \\in V_{h}^{p}$ that achieves a prescribed uniform approximation error $\\epsilon > 0$,\n$$\n\\|f - v_{h}^{p}\\|_{L^{\\infty}([-1,1])} \\le \\epsilon,\n$$\nby appropriately selecting the mesh size $h$ and the local polynomial degree $p$. Determine the minimal integer degree $p$ for which such a construction is possible and the largest admissible uniform element size $h$ that still guarantees the error bound. Express your final answer as the pair $(p,h)$ in terms of $\\epsilon$. No rounding is required, and no physical units are involved. Provide the pair in the form of a single row matrix as the final answer.",
            "solution": "The goal is to find the minimal integer degree $p$ and the corresponding largest mesh size $h$ to ensure that the function $f(x)=|x|$ can be approximated by a function $v_h^p \\in V_h^p$ with an error no greater than $\\epsilon$ in the $L^{\\infty}$ norm.\n\nThe space $V_h^p$ consists of functions that are polynomials of degree at most $p$ on each element $K$ of a uniform partition of $[-1,1]$. The total length of the interval is $2$. A uniform partition into elements of size $h$ implies there are $N=2/h$ such elements. The global approximation error is the maximum of the local approximation errors over all elements:\n$$\n\\|f - v_{h}^{p}\\|_{L^{\\infty}([-1,1])} = \\max_{K} \\|f - v_{h}^{p}\\|_{L^{\\infty}(K)}.\n$$\nTo construct the optimal $v_h^p$, we should choose the polynomial on each element $K$ to be the best $L^{\\infty}$ approximation of $f(x)$ from $\\mathbb{P}_p(K)$. The error on an element $K$ is then given by $E_{p,K}(f) = \\inf_{q \\in \\mathbb{P}_p(K)} \\|f - q\\|_{L^{\\infty}(K)}$.\n\nThe key feature of $f(x)=|x|$ is its non-differentiability at $x=0$. Away from the origin, $f(x)$ is either $x$ or $-x$, both of which are polynomials of degree $1$.\n\nWe need to find the minimal integer degree $p$ for which a suitable construction is possible. We start by testing the lowest possible non-negative integer degree, $p=0$.\n\n**Case: $p=0$ (Piecewise Constant Approximation)**\n\nFor $p=0$, the space $V_h^0$ consists of piecewise constant functions. On each element $K$, we approximate $f(x)$ by a constant $c_K$. The best constant approximant $c_K$ in the $L^{\\infty}$ norm for a function $g$ on an interval $K$ is given by $c_K = \\frac{1}{2}(\\sup_{x \\in K} g(x) + \\inf_{x \\in K} g(x))$. The resulting minimal error on that element is $E_{0,K}(g) = \\frac{1}{2}(\\sup_{x \\in K} g(x) - \\inf_{x \\in K} g(x))$.\n\nLet's analyze the error for $f(x)=|x|$:\n1.  **Element $K$ does not contain the origin $x=0$**:\n    Let $K = [a, a+h]$ with $a>0$. On this element, $f(x)=x$. The function is monotonic.\n    The error is $E_{0,K}(f) = \\frac{1}{2}(f(a+h) - f(a)) = \\frac{1}{2}((a+h) - a) = \\frac{h}{2}$.\n    Similarly, for an element $K = [-b-h, -b]$ with $b>0$, $f(x)=-x$. The function is also monotonic on $K$.\n    The error is $E_{0,K}(f) = \\frac{1}{2}(f(-b) - f(-b-h)) = \\frac{1}{2}((b+h) - b) = \\frac{h}{2}$.\n    For any element not containing the origin, the approximation error is exactly $h/2$.\n\n2.  **Element $K$ contains the origin $x=0$**:\n    A uniform partition of $[-1,1]$ can place the origin either as a boundary point between two elements (if the number of elements $N$ is even) or in the interior of an element (if $N$ is odd). We must consider the worst-case scenario. a) If $K = [0, h]$ or $K=[-h, 0]$, the analysis is the same as in case 1, and the error is $h/2$. b) If $K=[-h/2, h/2]$, the function $f(x)=|x|$ is not monotonic. On this interval, $\\inf_{x \\in K} f(x) = f(0) = 0$ and $\\sup_{x \\in K} f(x) = f(h/2) = f(-h/2) = h/2$.\n    The error is $E_{0,K}(f) = \\frac{1}{2}(\\frac{h}{2} - 0) = \\frac{h}{4}$.\n\nThe maximum error over all possible elements is therefore $\\max(h/2, h/4) = h/2$.\nSo, for $p=0$, we can construct $v_h^0$ such that $\\|f - v_h^0\\|_{L^{\\infty}([-1,1])} = h/2$.\nTo satisfy the condition $\\|f - v_h^0\\|_{L^{\\infty}([-1,1])} \\le \\epsilon$, we must have:\n$$\n\\frac{h}{2} \\le \\epsilon \\implies h \\le 2\\epsilon.\n$$\nThis shows that a construction is possible for $p=0$, provided we choose $h \\le 2\\epsilon$. Since $p$ must be a non-negative integer, $p=0$ is the minimal possible degree.\n\nThe problem asks for the largest admissible uniform element size $h$ for this minimal degree. From the inequality $h \\le 2\\epsilon$, the largest value for $h$ is $2\\epsilon$.\n\nThis is consistent with the hint about continuity. The function $f(x)=|x|$ is uniformly continuous. In fact, it is Lipschitz continuous with a Lipschitz constant of $1$, i.e., $||x| - |y|| \\le |x-y|$. On any interval of length $h$, the range of the function is at most $h$. For a piecewise constant approximation, the error is bounded by the range of the function, and our more precise calculation gives an error of $h/2$.\n\nThus, the minimal integer degree is $p=0$, and the largest admissible element size is $h=2\\epsilon$. The required pair is $(p, h) = (0, 2\\epsilon)$.\n\nFor completeness, let's consider $p=1$. On any element $K$ not containing the origin, $f(x)$ is linear, so we can set $v_h^1|_K = f|_K$ for an error of $0$. On an element containing the origin, like $K_0=[-h/2, h/2]$, we need to find the best linear approximation to $|x|$. The best $L^\\infty$ approximation of an even function on a symmetric interval is even, so the best linear approximation is a constant. This reduces to the $p=0$ case on $K_0$, giving an error of $h/4$. Thus, for $p=1$, the total error is $h/4$. The condition $h/4 \\le \\epsilon$ implies $h \\le 4\\epsilon$. While this allows for a larger $h$, the problem asks for the result associated with the *minimal* degree $p$, which is $p=0$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0 & 2\\epsilon \\end{pmatrix} } $$"
        },
        {
            "introduction": "While higher-degree polynomials can provide more accurate approximations of a function, a hidden cost emerges when we need to compute its derivativesâ€”a necessary step for solving differential equations. This exercise uncovers this critical trade-off using a key tool from approximation theory: the inverse inequality. You will derive a sharp bound on the error in a numerical flux, showing how perturbations in a polynomial approximation can be amplified by a factor proportional to $N^2/h$ when differentiated.  This reveals a fundamental stability constraint in spectral and DG methods, demonstrating why simply increasing the polynomial degree $N$ is not always a panacea for achieving higher accuracy.",
            "id": "3428487",
            "problem": "Consider a single physical element $I=[x_{L},x_{R}]$ of length $h=x_{R}-x_{L}$ in a discontinuous Galerkin discretization of a scalar diffusion model with diffusion coefficient $\\kappa>0$. Let $\\xi\\in[-1,1]$ denote the reference coordinate, and let the affine mapping between the reference and physical coordinates be $x(\\xi)=x_{c}+\\frac{h}{2}\\,\\xi$, where $x_{c}=\\frac{x_{L}+x_{R}}{2}$ is the element center.\n\nLet $u\\in C([-1,1])$ be the exact solution expressed in the reference coordinate. By the Weierstrass approximation theorem, for each integer $N\\ge 1$ there exists a polynomial $p_{N}\\in\\mathbb{P}_{N}([-1,1])$ that approximates $u$ uniformly on $[-1,1]$. In a polynomial-based discontinuous Galerkin method, the numerical flux for diffusion at the right face (corresponding to $\\xi=1$) uses the trace of a polynomial derivative; in particular, define the face-consistent diffusive flux based on a polynomial $q_{N}\\in\\mathbb{P}_{N}([-1,1])$ by\n$$\nF[q_{N}]=-\\kappa\\,\\partial_{x}q_{N}\\big|_{x=x_{R}}.\n$$\nSuppose that due to modal aliasing, underintegration, or roundoff effects, the computed polynomial is $q_{N}=p_{N}+e_{N}$, where $e_{N}\\in\\mathbb{P}_{N}([-1,1])$ satisfies the uniform perturbation bound $\\|e_{N}\\|_{L^{\\infty}([-1,1])}\\le \\delta$ for some $\\delta>0$. The error in the diffusive numerical flux at the right face induced solely by $e_{N}$ is\n$$\n\\Delta F = F[p_{N}+e_{N}]-F[p_{N}]=-\\kappa\\,\\partial_{x}e_{N}\\big|_{x=x_{R}}.\n$$\n\nStarting only from well-tested facts, including the Weierstrass approximation theorem and classical inverse inequalities for polynomials on $[-1,1]$, and using the mapping relation between $\\partial_{x}$ and $\\partial_{\\xi}$ on $I$, determine the smallest constant $C(N,h,\\kappa)$ such that the face flux error satisfies\n$$\n|\\Delta F|\\le C(N,h,\\kappa)\\,\\delta\n$$\nfor all perturbations $e_{N}\\in\\mathbb{P}_{N}([-1,1])$ with $\\|e_{N}\\|_{L^{\\infty}([-1,1])}\\le\\delta$. Your final answer must be a single closed-form analytic expression for $C(N,h,\\kappa)$ in terms of $N$, $h$, and $\\kappa$.",
            "solution": "The problem requires finding the smallest constant $C(N,h,\\kappa)$ that bounds the error in the diffusive numerical flux, $|\\Delta F|$, given a perturbation $e_N$ to a polynomial approximation. The bound is of the form $|\\Delta F| \\le C(N,h,\\kappa)\\delta$, where $\\delta$ is an upper bound on the $L^{\\infty}$ norm of the perturbation polynomial $e_N$.\n\nFirst, we express the flux error $|\\Delta F|$ in terms of the given quantities. The definition of the flux error is:\n$$\n\\Delta F = -\\kappa\\,\\partial_{x}e_{N}\\big|_{x=x_{R}}\n$$\nThe magnitude of this error is therefore $|\\Delta F| = \\kappa |\\partial_{x}e_{N}\\big|_{x=x_{R}}|$, since the diffusion coefficient $\\kappa$ is positive.\n\nThe polynomial $e_N$ is given as an element of $\\mathbb{P}_{N}([-1,1])$, the space of polynomials of degree at most $N$ on the reference interval $[-1,1]$. This means $e_N$ is a function of the reference coordinate $\\xi$. In the expression for the flux, the derivative is with respect to the physical coordinate $x$. This implies that $e_N$ is treated as a composite function on the physical domain $I=[x_L, x_R]$, i.e., $e_N(x) = e_N(\\xi(x))$, where $\\xi(x)$ is the inverse mapping from physical to reference coordinates.\n\nWe are given the affine mapping from the reference coordinate $\\xi$ to the physical coordinate $x$:\n$$\nx(\\xi) = x_{c} + \\frac{h}{2}\\xi\n$$\nwhere $x_c = \\frac{x_L+x_R}{2}$ is the element center and $h=x_R-x_L$ is the element length.\n\nTo find the relationship between the derivatives $\\partial_x$ and $\\partial_\\xi$, we use the chain rule:\n$$\n\\frac{d}{dx} = \\frac{d\\xi}{dx} \\frac{d}{d\\xi}\n$$\nFrom the mapping $x(\\xi)$, we can find $\\frac{dx}{d\\xi} = \\frac{h}{2}$. Therefore, its inverse is $\\frac{d\\xi}{dx} = \\frac{2}{h}$. The derivative operator relationship is:\n$$\n\\partial_x = \\frac{2}{h}\\partial_\\xi\n$$\nApplying this to the polynomial $e_N$, we get $\\partial_x e_N(\\xi(x)) = \\frac{2}{h}\\frac{d e_N}{d\\xi}(\\xi)$.\n\nThe flux error is evaluated at the right face of the element, $x=x_R$. We need to find the corresponding coordinate $\\xi$ on the reference element. Using the inverse mapping $\\xi(x) = \\frac{2}{h}(x-x_c)$:\n$$\n\\xi(x_R) = \\frac{2}{h}\\left(x_R - \\frac{x_L+x_R}{2}\\right) = \\frac{2}{h}\\left(\\frac{2x_R-x_L-x_R}{2}\\right) = \\frac{2}{h}\\left(\\frac{x_R-x_L}{2}\\right) = \\frac{2}{h}\\left(\\frac{h}{2}\\right) = 1\n$$\nSo, the right face $x=x_R$ corresponds to $\\xi=1$.\n\nNow, we can write the flux error magnitude entirely in terms of quantities on the reference element:\n$$\n|\\Delta F| = \\kappa \\left|\\left(\\frac{2}{h}\\frac{d e_N}{d\\xi}\\right)\\Big|_{\\xi=1}\\right| = \\frac{2\\kappa}{h} \\left|\\frac{d e_N}{d\\xi}(1)\\right|\n$$\nThe problem states that the perturbation polynomial $e_N \\in \\mathbb{P}_N([-1,1])$ satisfies the uniform bound $\\|e_N\\|_{L^{\\infty}([-1,1])} \\le \\delta$. We need to find the smallest constant $C(N,h,\\kappa)$ such that:\n$$\n|\\Delta F| \\le C(N,h,\\kappa)\\delta\n$$\nSubstituting our expression for $|\\Delta F|$:\n$$\n\\frac{2\\kappa}{h} \\left|\\frac{d e_N}{d\\xi}(1)\\right| \\le C(N,h,\\kappa)\\delta\n$$\nThis inequality must hold for all $e_N \\in \\mathbb{P}_N([-1,1])$ with $\\|e_N\\|_{L^{\\infty}([-1,1])} \\le \\delta$. To find the smallest such constant $C(N,h,\\kappa)$, we must consider the worst-case scenario. This corresponds to finding the supremum of the left-hand side over all admissible polynomials $e_N$.\n$$\nC(N,h,\\kappa) = \\sup_{\\substack{e_N \\in \\mathbb{P}_N \\\\ 0 < \\|e_N\\|_\\infty \\le \\delta}} \\frac{1}{\\delta} \\left(\\frac{2\\kappa}{h} \\left|e_N'(1)\\right|\\right) = \\frac{2\\kappa}{h} \\sup_{\\substack{e_N \\in \\mathbb{P}_N \\\\ 0 < \\|e_N\\|_\\infty \\le \\delta}} \\frac{\\left|e_N'(1)\\right|}{\\delta}\n$$\nSince for any such $e_N$, the polynomial $\\tilde{e}_N = e_N / \\|e_N\\|_\\infty$ has unit norm, and $|e_N'(1)|/\\|e_N\\|_\\infty = |\\tilde{e}_N'(1)|$, the supremum is independent of $\\delta$. We can simplify by considering polynomials with unit norm:\n$$\nC(N,h,\\kappa) = \\frac{2\\kappa}{h} \\sup_{\\substack{e_N \\in \\mathbb{P}_N \\\\ \\|e_N\\|_\\infty = 1}} \\left|e_N'(1)\\right|\n$$\nThe problem is now reduced to finding the maximum possible value of the derivative of a polynomial of degree $N$ at an endpoint of the interval $[-1,1]$, given that its $L^\\infty$ norm on that interval is $1$. This is a classical result in approximation theory, which falls under the category of \"classical inverse inequalities\" mentioned in the problem statement.\n\nFor any polynomial $p \\in \\mathbb{P}_N([-1,1])$, the following sharp inequality (known as the Markov Brothers' inequality at an endpoint, or Bernstein's inequality at an endpoint) holds:\n$$\n|p'(\\xi)| \\le N^2 \\|p\\|_{L^\\infty([-1,1])} \\quad \\text{for } \\xi = \\pm 1\n$$\nThis bound is sharp, meaning there exists a polynomial for which equality is achieved. The extremal polynomial is the Chebyshev polynomial of the first kind, $T_N(\\xi) = \\cos(N\\arccos \\xi)$. For this polynomial, $\\|T_N\\|_{L^\\infty([-1,1])}=1$, and its derivative at the endpoints is $|T_N'(\\pm 1)| = N^2$.\n\nTherefore, the supremum we seek is:\n$$\n\\sup_{\\substack{e_N \\in \\mathbb{P}_N \\\\ \\|e_N\\|_\\infty = 1}} \\left|e_N'(1)\\right| = N^2\n$$\nSubstituting this value back into our expression for $C(N,h,\\kappa)$:\n$$\nC(N,h,\\kappa) = \\frac{2\\kappa}{h} \\cdot N^2 = \\frac{2\\kappa N^2}{h}\n$$\nThis is the smallest constant for which the inequality holds for all specified perturbations, as it is derived from a sharp inequality.",
            "answer": "$$\n\\boxed{\\frac{2\\kappa N^{2}}{h}}\n$$"
        }
    ]
}