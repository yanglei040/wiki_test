{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a direct application of the core algorithm. This exercise requires constructing a Newton interpolating polynomial from first principles for a given function and a set of nodes. By methodically calculating the divided differences and assembling the polynomial, you will gain a concrete understanding of the mechanics that underpin this powerful interpolation technique .",
            "id": "3283191",
            "problem": "Consider the polynomial interpolation problem: given distinct nodes $x_{0}, x_{1}, \\dots, x_{n}$ and function data $f(x_{i})$, find a polynomial $p$ of degree at most $n$ such that $p(x_{i}) = f(x_{i})$ for all $i$. It is known from the foundational properties of polynomials that if two polynomials of degree at most $n$ agree at $n+1$ distinct points, then they are identical. You are asked to use this framework to determine the interpolant of the function $f(x) = \\cos(\\pi x)$.\n\nStarting from the recursive definition of divided differences and without invoking any pre-stated closed-form formulas, construct the interpolating polynomial in the Newton form for the data from integer nodes $x = 0, 1, 2, 3$. Then, expand and simplify your result into the standard monomial basis $1, x, x^{2}, x^{3}$ with exact coefficients. Briefly justify, on the basis of the general interpolation uniqueness principle, why the polynomial you obtain is the unique interpolant of degree at most $3$ for these data.\n\nExpress your final polynomial explicitly as a single simplified algebraic expression in $x$. No rounding is required; present exact rational coefficients.",
            "solution": "The task is to find the unique interpolating polynomial $p(x)$ of degree at most $3$ for the function $f(x) = \\cos(\\pi x)$ at the distinct integer nodes $x_0 = 0$, $x_1 = 1$, $x_2 = 2$, and $x_3 = 3$. We will construct this polynomial in its Newton form and then simplify it to the standard monomial basis.\n\nFirst, we evaluate the function $f(x)$ at the given nodes:\n$f(x_0) = f(0) = \\cos(0 \\cdot \\pi) = \\cos(0) = 1$\n$f(x_1) = f(1) = \\cos(1 \\cdot \\pi) = \\cos(\\pi) = -1$\n$f(x_2) = f(2) = \\cos(2 \\cdot \\pi) = \\cos(2\\pi) = 1$\n$f(x_3) = f(3) = \\cos(3 \\cdot \\pi) = \\cos(3\\pi) = -1$\n\nThe Newton form of the interpolating polynomial $p(x)$ for $n+1 = 4$ points is given by:\n$$p(x) = f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) + f[x_0, x_1, x_2, x_3](x - x_0)(x - x_1)(x - x_2)$$\nwhere $f[\\dots]$ are the divided differences. We must construct these coefficients recursively, starting from the zeroth-order differences, which are the function values themselves.\n\nThe zeroth-order divided differences are:\n$f[x_0] = f(0) = 1$\n$f[x_1] = f(1) = -1$\n$f[x_2] = f(2) = 1$\n$f[x_3] = f(3) = -1$\n\nThe first-order divided differences are calculated as:\n$f[x_0, x_1] = \\frac{f[x_1] - f[x_0]}{x_1 - x_0} = \\frac{-1 - 1}{1 - 0} = -2$\n$f[x_1, x_2] = \\frac{f[x_2] - f[x_1]}{x_2 - x_1} = \\frac{1 - (-1)}{2 - 1} = \\frac{2}{1} = 2$\n$f[x_2, x_3] = \\frac{f[x_3] - f[x_2]}{x_3 - x_2} = \\frac{-1 - 1}{3 - 2} = \\frac{-2}{1} = -2$\n\nThe second-order divided differences are:\n$f[x_0, x_1, x_2] = \\frac{f[x_1, x_2] - f[x_0, x_1]}{x_2 - x_0} = \\frac{2 - (-2)}{2 - 0} = \\frac{4}{2} = 2$\n$f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{-2 - 2}{3 - 1} = \\frac{-4}{2} = -2$\n\nFinally, the third-order divided difference is:\n$f[x_0, x_1, x_2, x_3] = \\frac{f[x_1, x_2, x_3] - f[x_0, x_1, x_2]}{x_3 - x_0} = \\frac{-2 - 2}{3 - 0} = -\\frac{4}{3}$\n\nThe required coefficients for the Newton form are the top diagonal of the divided difference table: $f[x_0]=1$, $f[x_0, x_1]=-2$, $f[x_0, x_1, x_2]=2$, and $f[x_0, x_1, x_2, x_3]=-\\frac{4}{3}$.\n\nSubstituting these values and the nodes ($x_0=0, x_1=1, x_2=2$) into the Newton polynomial formula:\n$$p(x) = 1 + (-2)(x - 0) + 2(x - 0)(x - 1) + \\left(-\\frac{4}{3}\\right)(x - 0)(x - 1)(x - 2)$$\n$$p(x) = 1 - 2x + 2x(x - 1) - \\frac{4}{3}x(x - 1)(x - 2)$$\n\nNow, we expand and simplify this expression to obtain the polynomial in the standard monomial basis $\\{1, x, x^2, x^3\\}$.\nFirst, expand the products of binomials:\n$x(x - 1) = x^2 - x$\n$x(x - 1)(x - 2) = x(x^2 - 3x + 2) = x^3 - 3x^2 + 2x$\n\nSubstitute these back into the expression for $p(x)$:\n$p(x) = 1 - 2x + 2(x^2 - x) - \\frac{4}{3}(x^3 - 3x^2 + 2x)$\n$p(x) = 1 - 2x + 2x^2 - 2x - \\frac{4}{3}x^3 + \\frac{12}{3}x^2 - \\frac{8}{3}x$\n$p(x) = 1 - 2x + 2x^2 - 2x - \\frac{4}{3}x^3 + 4x^2 - \\frac{8}{3}x$\n\nFinally, collect terms by powers of $x$:\nThe $x^3$ term is: $-\\frac{4}{3}x^3$\nThe $x^2$ term is: $2x^2 + 4x^2 = 6x^2$\nThe $x$ term is: $-2x - 2x - \\frac{8}{3}x = -4x - \\frac{8}{3}x = \\left(-\\frac{12}{3} - \\frac{8}{3}\\right)x = -\\frac{20}{3}x$\nThe constant term is: $1$\n\nCombining these terms, the simplified polynomial is:\n$$p(x) = -\\frac{4}{3}x^3 + 6x^2 - \\frac{20}{3}x + 1$$\n\nTo justify the uniqueness of this polynomial, we invoke the fundamental theorem of polynomial interpolation, which was stated in the problem. The polynomial $p(x)$ we constructed has degree $3$, which is \"at most $3$\". By its construction, it satisfies the interpolation conditions $p(x_i) = f(x_i)$ at the $n+1=4$ distinct points $x_0=0, x_1=1, x_2=2, x_3=3$.\nSuppose there existed another polynomial, say $q(x)$, also of degree at most $3$, that satisfied the same conditions, i.e., $q(x_i) = f(x_i)$ for $i=0, 1, 2, 3$. Then, the two polynomials $p(x)$ and $q(x)$ would have the same values at $4$ distinct points. According to the principle that \"if two polynomials of degree at most $n$ agree at $n+1$ distinct points, then they are identical\", it follows that $p(x)$ and $q(x)$ must be the same polynomial. Therefore, the polynomial $p(x) = -\\frac{4}{3}x^3 + 6x^2 - \\frac{20}{3}x + 1$ is the unique interpolating polynomial of degree at most $3$ for the given data.",
            "answer": "$$\n\\boxed{-\\frac{4}{3}x^3 + 6x^2 - \\frac{20}{3}x + 1}\n$$"
        },
        {
            "introduction": "Moving from computation to conceptual insight, this problem explores the structural properties of the Newton polynomial. It prompts you to consider what happens when the data being interpolated already comes from a low-degree polynomial. This practice reveals the profound connection between divided differences and the intrinsic \"polynomial-ness\" of data, showing that higher-order Newton coefficients vanish, a key property rooted in the uniqueness of the interpolant .",
            "id": "2181790",
            "problem": "A set of five data points $(x_i, y_i)$ for $i=0, 1, 2, 3, 4$ are collected, where all $x_i$ values are distinct. It is known with certainty that these points lie on the graph of a quadratic polynomial $P(x) = ax^2 + bx + d$, where $a, b, d$ are real constants and $a \\neq 0$.\n\nA numerical analyst constructs the unique degree-4 polynomial that passes through all five points. The polynomial is written in Newton form as:\n$$N_4(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)(x-x_1) + c_3(x-x_0)(x-x_1)(x-x_2) + c_4(x-x_0)(x-x_1)(x-x_2)(x-x_3)$$\nDetermine the values of the coefficients $c_3$ and $c_4$.\n\nA. $c_3 = a$ and $c_4 = b$\n\nB. $c_3 = 0$ and $c_4 = a$\n\nC. $c_3 = a$ and $c_4 = 0$\n\nD. $c_3 = 0$ and $c_4 = 0$\n\nE. The values of $c_3$ and $c_4$ cannot be determined without knowing the specific values of $x_i$.",
            "solution": "Let the data come from the quadratic polynomial $P(x)=ax^{2}+bx+d$ with $a \\neq 0$. The Newton interpolation polynomial through nodes $x_{0},\\dots,x_{4}$ is written as\n$$\nN_{4}(x)=c_{0}+c_{1}(x-x_{0})+c_{2}(x-x_{0})(x-x_{1})+c_{3}(x-x_{0})(x-x_{1})(x-x_{2})+c_{4}(x-x_{0})(x-x_{1})(x-x_{2})(x-x_{3}).\n$$\nBy construction, the Newton coefficients are the divided differences:\n$$\nc_{0}=P[x_{0}],\\quad c_{1}=P[x_{0},x_{1}],\\quad c_{2}=P[x_{0},x_{1},x_{2}],\\quad c_{3}=P[x_{0},x_{1},x_{2},x_{3}],\\quad c_{4}=P[x_{0},x_{1},x_{2},x_{3},x_{4}].\n$$\nFor a $C^{k}$ function $f$, the $k$-th order divided difference satisfies\n$$\nf[x_{0},\\dots,x_{k}]=\\frac{f^{(k)}(\\xi)}{k!}\n$$\nfor some $\\xi$ in the convex hull of $\\{x_{0},\\dots,x_{k}\\}$. Applying this to $f=P$ with $\\deg P=2$ gives $P^{(3)}(x)=0$ for all $x$. Therefore, for any distinct nodes $x_{0},\\dots,x_{k}$,\n$$\nP[x_{0},x_{1},x_{2},x_{3}]=0,\\qquad P[x_{0},x_{1},x_{2},x_{3},x_{4}]=0.\n$$\nHence\n$$\nc_{3}=0,\\qquad c_{4}=0.\n$$\nEquivalently, since $N_{4}(x)$ interpolates $P$ at five distinct points and $P$ is quadratic, the difference $N_{4}(x)-P(x)$ is a polynomial of degree at most $4$ with five distinct roots, hence identically zero; thus $N_{4}(x)\\equiv P(x)$, forcing the cubic and quartic Newton coefficients to vanish.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "This final practice bridges theory and application, demonstrating how the Newton form serves as a basis for constructing advanced numerical operators. You will derive a spectral differentiation matrix directly from the recurrence relations of the Newton basis polynomials, a tool fundamental to spectral and Discontinuous Galerkin methods. This exercise not only highlights the utility of Newton interpolation beyond simple curve-fitting but also provides hands-on experience in building and analyzing the core components of modern numerical solvers .",
            "id": "3402293",
            "problem": "Let $n \\in \\mathbb{N}$ and let $X = \\{x_0, x_1, \\dots, x_n\\}$ be a set of $n+1$ distinct real nodes. Consider the Newton interpolation basis defined by $L_0(x) = 1$ and, for $k \\geq 1$, $L_k(x) = \\prod_{j=0}^{k-1} (x - x_j)$. The unique interpolating polynomial of degree at most $n$ is written in Newton form as $p(x) = \\sum_{k=0}^{n} a_k L_k(x)$, where the coefficients $a_k$ are determined by the interpolation conditions $p(x_i) = u_i$ for a given nodal data vector $u = (u_0, \\dots, u_n)^\\top$.\n\n1. Using only the Newton basis definition and the product rule for differentiation, derive a recurrence for $L_k'(x)$ in terms of $L_{k-1}(x)$ and $L_{k-1}'(x)$. Then use this recurrence to construct a matrix $L'(X)$ of size $(n+1) \\times (n+1)$ whose $(i,k)$ entry is $L_k'(x_i)$ and a lower-triangular Newton-Vandermonde matrix $V(X)$ whose $(i,k)$ entry is $L_k(x_i)$.\n\n2. Show that the vector of derivatives of $p$ at the nodes can be expressed as $p'(X) = L'(X)\\,a$, where $a$ is the Newton coefficient vector. Using the relation $u = V(X)\\,a$, eliminate $a$ and derive the spectral differentiation matrix in the nodal basis,\n$$\nD^{(N)}(X) = L'(X)\\,V(X)^{-1},\n$$\nwhich maps nodal values $u$ to nodal derivatives $p'(X)$, i.e., $p'(X) = D^{(N)}(X)\\,u$.\n\n3. Independently, define the barycentric weights $w_i$ by\n$$\nw_i = \\left( \\prod_{\\substack{j=0 \\\\ j \\neq i}}^{n} (x_i - x_j) \\right)^{-1},\n$$\nand derive the barycentric spectral differentiation matrix $D^{(B)}(X)$ with off-diagonal entries\n$$\nD^{(B)}_{ij} = \\frac{w_j}{w_i}\\,\\frac{1}{x_i - x_j}, \\quad i \\neq j,\n$$\nand diagonal entries\n$$\nD^{(B)}_{ii} = -\\sum_{\\substack{j=0 \\\\ j \\neq i}}^{n} D^{(B)}_{ij}.\n$$\nExplain why $D^{(B)}(X)$ represents the same linear operator as $D^{(N)}(X)$ for distinct nodes, hence $D^{(N)}(X) = D^{(B)}(X)$.\n\n4. In the context of Spectral and Discontinuous Galerkin (DG) methods, volume terms require repeated application of spectral differentiation matrices to nodal vectors. Adopt the following operation-count cost model for applying a preassembled dense $(n+1)\\times(n+1)$ differentiation matrix $D$ to a nodal vector $u$: count one multiplication for each floating-point multiplication or division and one addition for each floating-point addition or subtraction. Under this model, the application of $d = D\\,u$ incurs exactly $(n+1)^2$ multiplications and $(n+1)(n)$ additions.\n\nImplement a program that, for each test case below, constructs $D^{(N)}(X)$ using the recurrences for $L_k(x)$ and $L_k'(x)$ and triangular solves, constructs $D^{(B)}(X)$ using barycentric weights, and then computes the following quantities:\n- The maximum absolute entrywise difference $$\\Delta_D = \\max_{0 \\leq i,j \\leq n} \\left| D^{(N)}_{ij} - D^{(B)}_{ij} \\right|.$$\n- For the polynomial $q(x) = \\sum_{k=0}^{n} c_k x^k$ with $c_k = \\frac{(-1)^k}{k+1}$, the maximum absolute difference of the nodal derivatives computed by the two matrices, $$\\Delta_{d} = \\max_{0 \\leq i \\leq n} \\left| (D^{(N)} u)_i - (D^{(B)} u)_i \\right|,$$ where $u_i = q(x_i)$.\n- The ratio of application costs $$R = \\frac{C_{\\text{app}}(D^{(N)})}{C_{\\text{app}}(D^{(B)})}$$ under the above cost model.\n\nTest suite:\n- Case A (happy path): $n = 8$, Chebyshev–Gauss–Lobatto nodes $x_i = -\\cos\\left(\\frac{\\pi i}{n}\\right)$ on $[-1,1]$.\n- Case B (boundary condition): $n = 1$, equispaced nodes $x_0 = -1$, $x_1 = 1$.\n- Case C (edge case, clustered nodes): $n = 5$, equispaced nodes on $[-10^{-3}, 10^{-3}]$.\n\nFinal output format:\nYour program should produce a single line of output containing the nine results, ordered as $[\\Delta_D^{A}, \\Delta_{d}^{A}, R^{A}, \\Delta_D^{B}, \\Delta_{d}^{B}, R^{B}, \\Delta_D^{C}, \\Delta_{d}^{C}, R^{C}]$, as a comma-separated list enclosed in square brackets. All nine values must be represented as floating-point numbers. No physical units or angle units apply to this problem.",
            "solution": "The problem is structured in four parts. The first three are theoretical derivations, and the fourth is a computational implementation and comparison.\n\n### Part 1: Recurrence for $L_k'(x)$ and Matrix Construction\n\nThe Newton basis polynomials are defined by $L_0(x) = 1$ and $L_k(x) = \\prod_{j=0}^{k-1} (x - x_j)$ for $k \\geq 1$. We can express $L_k(x)$ in terms of $L_{k-1}(x)$ for $k \\geq 1$:\n$$\nL_k(x) = \\left(\\prod_{j=0}^{k-2} (x - x_j)\\right) (x - x_{k-1}) = L_{k-1}(x) (x - x_{k-1})\n$$\nTo find a recurrence for the derivative $L_k'(x)$, we apply the product rule for differentiation to this expression:\n$$\nL_k'(x) = \\frac{d}{dx} \\left[ L_{k-1}(x) (x - x_{k-1}) \\right] = L_{k-1}'(x) \\cdot (x - x_{k-1}) + L_{k-1}(x) \\cdot 1\n$$\nThis gives the desired recurrence relation for $k \\geq 1$:\n$$\nL_k'(x) = L_{k-1}(x) + (x - x_{k-1}) L_{k-1}'(x)\n$$\nThe base case is derived from $L_0(x) = 1$, which implies $L_0'(x) = 0$.\n\nUsing these definitions, we construct two $(n+1) \\times (n+1)$ matrices.\n1.  The Newton-Vandermonde matrix $V(X)$ has entries $(V(X))_{ik} = L_k(x_i)$. For $k > i$, the product defining $L_k(x_i) = \\prod_{j=0}^{k-1} (x_i - x_j)$ includes the term $(x_i - x_i) = 0$. Thus, $L_k(x_i) = 0$ for $k > i$, which means $V(X)$ is a lower-triangular matrix. Since the nodes $\\{x_j\\}$ are distinct, the diagonal entries $L_k(x_k) = \\prod_{j=0}^{k-1} (x_k - x_j)$ are all non-zero, and $V(X)$ is invertible.\n2.  The matrix of basis derivatives $L'(X)$ has entries $(L'(X))_{ik} = L_k'(x_i)$. Its columns can be computed iteratively using the recurrence. The first column is all zeros, since $L_0'(x_i)=0$. Each subsequent column $k$ can be computed from column $k-1$ of $L'(X)$ and column $k-1$ of $V(X)$.\n\n### Part 2: The Newton Spectral Differentiation Matrix $D^{(N)}(X)$\n\nThe interpolating polynomial is $p(x) = \\sum_{k=0}^{n} a_k L_k(x)$. Its derivative is $p'(x) = \\sum_{k=0}^{n} a_k L_k'(x)$. Evaluating this at the interpolation nodes $x_i$ for $i = 0, \\dots, n$ gives the system of equations:\n$$\np'(x_i) = \\sum_{k=0}^{n} L_k'(x_i) a_k\n$$\nIn matrix form, this is $p'(X) = L'(X) a$, where $p'(X)$ is the vector of nodal derivatives $[p'(x_0), \\dots, p'(x_n)]^\\top$.\n\nThe coefficients $a = [a_0, \\dots, a_n]^\\top$ are determined by the interpolation conditions $p(x_i) = u_i$, which in matrix form is $u = V(X) a$. Since $V(X)$ is invertible, we can write $a = V(X)^{-1} u$.\n\nSubstituting this expression for $a$ into the equation for $p'(X)$, we get:\n$$\np'(X) = L'(X) (V(X)^{-1} u) = (L'(X) V(X)^{-1}) u\n$$\nThis shows that the linear operator that maps the vector of nodal function values $u$ to the vector of nodal derivative values $p'(X)$ is the matrix $D^{(N)}(X) = L'(X) V(X)^{-1}$. This is the spectral differentiation matrix in the Newton basis.\n\n### Part 3: The Barycentric Spectral Differentiation Matrix $D^{(B)}(X)$\n\nThe interpolating polynomial $p(x)$ is unique. Its representation in the Lagrange basis is $p(x) = \\sum_{j=0}^{n} u_j \\ell_j(x)$, where $\\ell_j(x) = \\prod_{k=0, k \\neq j}^{n} \\frac{x-x_k}{x_j-x_k}$ are the Lagrange cardinal polynomials. The derivative is $p'(x) = \\sum_{j=0}^{n} u_j \\ell_j'(x)$.\n\nThe spectral differentiation matrix $D$ has entries $D_{ij} = \\ell_j'(x_i)$.\nFor the off-diagonal entries ($i \\neq j$), we compute:\n$$\nD_{ij} = \\ell_j'(x_i) = \\left. \\frac{d}{dx} \\left( \\frac{\\prod_{k \\neq j} (x-x_k)}{\\prod_{k \\neq j} (x_j-x_k)} \\right) \\right|_{x=x_i}\n$$\nThe numerator is a product of terms $(x-x_k)$. By the product rule, the derivative is a sum of terms where one factor is differentiated. When we evaluate at $x=x_i$, the only term that survives is the one where the factor $(x-x_i)$ was differentiated.\n$$\n\\ell_j'(x_i) = \\frac{\\prod_{k \\neq j, i} (x_i-x_k)}{\\prod_{k \\neq j} (x_j-x_k)} = \\frac{1}{x_i-x_j} \\frac{\\prod_{k \\neq i} (x_i-x_k)}{\\prod_{k \\neq j} (x_j-x_k)}\n$$\nUsing the definition of the barycentric weights $w_i = (\\prod_{k \\neq i} (x_i - x_k))^{-1}$, this becomes:\n$$\nD_{ij} = \\frac{1}{x_i-x_j} \\frac{w_j^{-1}}{w_i^{-1}} = \\frac{w_j}{w_i} \\frac{1}{x_i-x_j}, \\quad i \\neq j\n$$\nThis matches the off-diagonal entries of $D^{(B)}(X)$.\n\nFor the diagonal entries ($i=j$), we use the fact that the sum of the Lagrange polynomials is unity: $\\sum_{j=0}^{n} \\ell_j(x) = 1$. Differentiating with respect to $x$ yields $\\sum_{j=0}^{n} \\ell_j'(x) = 0$. Evaluating at a node $x_i$:\n$$\n\\sum_{j=0}^{n} \\ell_j'(x_i) = 0 \\implies \\ell_i'(x_i) + \\sum_{j \\neq i} \\ell_j'(x_i) = 0\n$$\nThus, the diagonal entry is the negative sum of the off-diagonal entries in the same row:\n$$\nD_{ii} = \\ell_i'(x_i) = - \\sum_{j \\neq i} \\ell_j'(x_i) = - \\sum_{j \\neq i} D_{ij}\n$$\nThis matches the definition of the diagonal entries of $D^{(B)}(X)$.\n\nThe equality $D^{(N)}(X) = D^{(B)}(X)$ stems from the uniqueness of the interpolating polynomial $p(x)$ for a given set of distinct nodes and data. The spectral differentiation matrix is the unique matrix representation of the linear operator that maps nodal values $u$ to the nodal values of the derivative of $p(x)$. Since both $D^{(N)}(X)$ and $D^{(B)}(X)$ are derived to be this unique operator (albeit from different polynomial bases), they must be identical.\n\n### Part 4: Computational Analysis\n\nThe implementation will construct both matrices $D^{(N)}(X)$ and $D^{(B)}(X)$ for each test case.\n-   $D^{(N)}(X)$ is computed by first building the matrices $V(X)$ and $L'(X)$ using their recurrence relations. Then, the matrix equation $D^{(N)} V = L'$, or equivalently $V^\\top (D^{(N)})^\\top = (L')^\\top$, is solved for $(D^{(N)})^\\top$ using a triangular solver, which is efficient and numerically stable.\n-   $D^{(B)}(X)$ is computed by first calculating the barycentric weights $w_i$ and then using the explicit formulas for its entries.\n-   $\\Delta_D$ and $\\Delta_d$ are maximum absolute differences, measuring the numerical discrepancy between the two theoretically identical matrices and their results when applied to a test vector. These differences are expected to be close to machine precision.\n-   The cost ratio $R$ compares the operational cost of applying a pre-assembled matrix to a vector. The problem defines a cost model for applying a dense $(n+1) \\times (n+1)$ matrix: $(n+1)^2$ multiplications and $(n+1)n$ additions. Since both $D^{(N)}(X)$ and $D^{(B)}(X)$ are dense matrices of the same size, their application costs under this model are identical.\n    $$\n    C_{\\text{app}}(D^{(N)}) = C_{\\text{app}}(D^{(B)}) = (n+1)^2 + n(n+1)\n    $$\n    Therefore, the ratio $R = \\frac{C_{\\text{app}}(D^{(N)})}{C_{\\text{app}}(D^{(B)})} = 1$ for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing and comparing Newton and Barycentric\n    spectral differentiation matrices for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case A (Chebyshev)\",\n            \"n\": 8,\n            \"nodes_func\": lambda n: -np.cos(np.pi * np.arange(n + 1) / n)\n        },\n        {\n            \"name\": \"Case B (Boundary)\",\n            \"n\": 1,\n            \"nodes_func\": lambda n: np.array([-1.0, 1.0])\n        },\n        {\n            \"name\": \"Case C (Clustered)\",\n            \"n\": 5,\n            \"nodes_func\": lambda n: np.linspace(-1e-3, 1e-3, n + 1)\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        x = case[\"nodes_func\"](n)\n        n_p1 = n + 1\n\n        # 1. Construct D^(N)(X)\n        # Build Newton-Vandermonde V and its derivative L_prime\n        V = np.zeros((n_p1, n_p1))\n        L_prime = np.zeros((n_p1, n_p1))\n\n        V[:, 0] = 1.0\n        # L_prime[:, 0] is already 0.0\n\n        for k in range(1, n_p1):\n            V[:, k] = V[:, k - 1] * (x - x[k - 1])\n            L_prime[:, k] = V[:, k - 1] + (x - x[k - 1]) * L_prime[:, k - 1]\n\n        # Solve V.T @ D_N.T = L_prime.T for D_N.T\n        # V.T is upper triangular, so we use lower=False\n        try:\n            D_N_T = solve_triangular(V.T, L_prime.T, lower=False)\n            D_N = D_N_T.T\n        except np.linalg.LinAlgError:\n            # For ill-conditioned cases, direct inversion might be needed\n            # as a fallback, though solve_triangular is preferred.\n            V_inv = np.linalg.inv(V)\n            D_N = L_prime @ V_inv\n            \n        # 2. Construct D^(B)(X)\n        D_B = np.zeros((n_p1, n_p1))\n        \n        # Calculate barycentric weights w_i\n        w = np.ones(n_p1)\n        for i in range(n_p1):\n            for j in range(n_p1):\n                if i != j:\n                    w[i] *= (x[i] - x[j])\n        w = 1.0 / w\n\n        # Fill D_B entries\n        for i in range(n_p1):\n            row_sum = 0.0\n            for j in range(n_p1):\n                if i != j:\n                    term = (w[j] / w[i]) / (x[i] - x[j])\n                    D_B[i, j] = term\n                    row_sum += term\n            D_B[i, i] = -row_sum\n\n        # 3. Compute required quantities\n        # Delta_D: Max absolute difference between D_N and D_B\n        delta_D = np.max(np.abs(D_N - D_B))\n\n        # Delta_d: Max absolute difference in application to a vector u\n        # u is nodal values of q(x) = sum_{k=0 to n} c_k * x^k\n        # where c_k = (-1)^k / (k+1)\n        c = [((-1)**k) / (k + 1) for k in range(n_p1)]\n        u = np.polynomial.polynomial.polyval(x, c)\n\n        d_N = D_N @ u\n        d_B = D_B @ u\n        delta_d = np.max(np.abs(d_N - d_B))\n\n        # R: Ratio of application costs\n        # The cost model is for a pre-assembled dense matrix-vector product.\n        # Since both D_N and D_B are dense (n+1)x(n+1) matrices, their\n        # application costs are identical.\n        R = 1.0\n\n        results.extend([delta_D, delta_d, R])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}