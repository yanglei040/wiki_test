{
    "hands_on_practices": [
        {
            "introduction": "This practice explores a fundamental property of the Newton form of the interpolating polynomial. By considering the case where data points lie on a polynomial of a lower degree than the interpolant, we gain insight into the relationship between divided differences and the smoothness of the underlying function. This exercise  demonstrates why higher-order Newton coefficients vanish, a principle directly connected to the error analysis of polynomial interpolation and a foundation for more advanced applications.",
            "id": "2181790",
            "problem": "A set of five data points $(x_i, y_i)$ for $i=0, 1, 2, 3, 4$ are collected, where all $x_i$ values are distinct. It is known with certainty that these points lie on the graph of a quadratic polynomial $P(x) = ax^2 + bx + d$, where $a, b, d$ are real constants and $a \\neq 0$.\n\nA numerical analyst constructs the unique degree-4 polynomial that passes through all five points. The polynomial is written in Newton form as:\n$$N_4(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)(x-x_1) + c_3(x-x_0)(x-x_1)(x-x_2) + c_4(x-x_0)(x-x_1)(x-x_2)(x-x_3)$$\nDetermine the values of the coefficients $c_3$ and $c_4$.\n\nA. $c_3 = a$ and $c_4 = b$\n\nB. $c_3 = 0$ and $c_4 = a$\n\nC. $c_3 = a$ and $c_4 = 0$\n\nD. $c_3 = 0$ and $c_4 = 0$\n\nE. The values of $c_3$ and $c_4$ cannot be determined without knowing the specific values of $x_i$.",
            "solution": "Let the data come from the quadratic polynomial $P(x)=ax^{2}+bx+d$ with $a\\neq 0$. The Newton interpolation polynomial through nodes $x_{0},\\dots,x_{4}$ is written as\n$$\nN_{4}(x)=c_{0}+c_{1}(x-x_{0})+c_{2}(x-x_{0})(x-x_{1})+c_{3}(x-x_{0})(x-x_{1})(x-x_{2})+c_{4}(x-x_{0})(x-x_{1})(x-x_{2})(x-x_{3}).\n$$\nBy construction, the Newton coefficients are the divided differences:\n$$\nc_{0}=P[x_{0}],\\quad c_{1}=P[x_{0},x_{1}],\\quad c_{2}=P[x_{0},x_{1},x_{2}],\\quad c_{3}=P[x_{0},x_{1},x_{2},x_{3}],\\quad c_{4}=P[x_{0},x_{1},x_{2},x_{3},x_{4}].\n$$\nFor a $C^{k}$ function $f$, the $k$-th order divided difference satisfies\n$$\nf[x_{0},\\dots,x_{k}]=\\frac{f^{(k)}(\\xi)}{k!}\n$$\nfor some $\\xi$ in the convex hull of $\\{x_{0},\\dots,x_{k}\\}$. Applying this to $f=P$ with $\\deg P=2$ gives $P^{(3)}(x)=0$ for all $x$. Therefore, for any distinct nodes $x_{0},\\dots,x_{k}$,\n$$\nP[x_{0},x_{1},x_{2},x_{3}]=0,\\qquad P[x_{0},x_{1},x_{2},x_{3},x_{4}]=0.\n$$\nHence\n$$\nc_{3}=0,\\qquad c_{4}=0.\n$$\nEquivalently, since $N_{4}(x)$ interpolates $P$ at five distinct points and $P$ is quadratic, the difference $N_{4}(x)-P(x)$ is a polynomial of degree at most $4$ with five distinct roots, hence identically zero; thus $N_{4}(x)\\equiv P(x)$, forcing the cubic and quartic Newton coefficients to vanish.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Building on the fundamentals of the Newton polynomial, this exercise bridges the gap between interpolation theory and its computational implementation in spectral methods. You will derive the spectral differentiation matrix, a cornerstone of high-order methods, directly from the recursive structure of the Newton basis polynomials and their derivatives. This hands-on practice  not only reinforces the theoretical underpinnings but also provides a practical method for constructing these critical operators, comparing it to the more common barycentric approach.",
            "id": "3402293",
            "problem": "Let $n \\in \\mathbb{N}$ and let $X = \\{x_0, x_1, \\dots, x_n\\}$ be a set of $n+1$ distinct real nodes. Consider the Newton interpolation basis defined by $L_0(x) = 1$ and, for $k \\geq 1$, $L_k(x) = \\prod_{j=0}^{k-1} (x - x_j)$. The unique interpolating polynomial of degree at most $n$ is written in Newton form as $p(x) = \\sum_{k=0}^{n} a_k L_k(x)$, where the coefficients $a_k$ are determined by the interpolation conditions $p(x_i) = u_i$ for a given nodal data vector $\\boldsymbol{u} = (u_0, \\dots, u_n)^\\top$.\n\n1. Using only the Newton basis definition and the product rule for differentiation, derive a recurrence for $L_k'(x)$ in terms of $L_{k-1}(x)$ and $L_{k-1}'(x)$. Then use this recurrence to construct a matrix $L'(X)$ of size $(n+1) \\times (n+1)$ whose $(i,k)$ entry is $L_k'(x_i)$ and a lower-triangular Newton-Vandermonde matrix $V(X)$ whose $(i,k)$ entry is $L_k(x_i)$.\n\n2. Show that the vector of derivatives of $p$ at the nodes can be expressed as $\\boldsymbol{p'}(X) = L'(X)\\,\\boldsymbol{a}$, where $\\boldsymbol{a}$ is the Newton coefficient vector. Using the relation $\\boldsymbol{u} = V(X)\\,\\boldsymbol{a}$, eliminate $\\boldsymbol{a}$ and derive the spectral differentiation matrix in the nodal basis,\n$$\nD^{(N)}(X) = L'(X)\\,V(X)^{-1},\n$$\nwhich maps nodal values $\\boldsymbol{u}$ to nodal derivatives $\\boldsymbol{p'}(X)$, i.e., $\\boldsymbol{p'}(X) = D^{(N)}(X)\\,\\boldsymbol{u}$.\n\n3. Independently, define the barycentric weights $w_i$ by\n$$\nw_i = \\left( \\prod_{\\substack{j=0 \\\\ j \\neq i}}^{n} (x_i - x_j) \\right)^{-1},\n$$\nand derive the barycentric spectral differentiation matrix $D^{(B)}(X)$ with off-diagonal entries\n$$\nD^{(B)}_{ij} = \\frac{w_j}{w_i}\\,\\frac{1}{x_i - x_j}, \\quad i \\neq j,\n$$\nand diagonal entries\n$$\nD^{(B)}_{ii} = -\\sum_{\\substack{j=0 \\\\ j \\neq i}}^{n} D^{(B)}_{ij}.\n$$\nExplain why $D^{(B)}(X)$ represents the same linear operator as $D^{(N)}(X)$ for distinct nodes, hence $D^{(N)}(X) = D^{(B)}(X)$.\n\n4. In the context of Spectral and Discontinuous Galerkin (DG) methods, volume terms require repeated application of spectral differentiation matrices to nodal vectors. Adopt the following operation-count cost model for applying a preassembled dense $(n+1)\\times(n+1)$ differentiation matrix $D$ to a nodal vector $\\boldsymbol{u}$: count one multiplication for each floating-point multiplication or division and one addition for each floating-point addition or subtraction. Under this model, the application of $\\boldsymbol{d} = D\\,\\boldsymbol{u}$ incurs exactly $(n+1)^2$ multiplications and $(n+1)(n)$ additions.\n\nImplement a program that, for each test case below, constructs $D^{(N)}(X)$ using the recurrences for $L_k(x)$ and $L_k'(x)$ and triangular solves, constructs $D^{(B)}(X)$ using barycentric weights, and then computes the following quantities:\n- The maximum absolute entrywise difference $\\Delta_D = \\max_{0 \\leq i,j \\leq n} \\left| D^{(N)}_{ij} - D^{(B)}_{ij} \\right|$.\n- For the polynomial $q(x) = \\sum_{k=0}^{n} c_k x^k$ with $c_k = \\frac{(-1)^k}{k+1}$, the maximum absolute difference of the nodal derivatives computed by the two matrices, $\\Delta_{d} = \\max_{0 \\leq i \\leq n} \\left| (D^{(N)} \\boldsymbol{u})_i - (D^{(B)} \\boldsymbol{u})_i \\right|$, where $u_i = q(x_i)$.\n- The ratio of application costs $R = \\frac{C_{\\text{app}}(D^{(N)})}{C_{\\text{app}}(D^{(B)})}$ under the above cost model.\n\nTest suite:\n- Case A (happy path): $n = 8$, Chebyshev–Gauss–Lobatto nodes $x_i = -\\cos\\left(\\frac{\\pi i}{n}\\right)$ on $[-1,1]$.\n- Case B (boundary condition): $n = 1$, equispaced nodes $x_0 = -1$, $x_1 = 1$.\n- Case C (edge case, clustered nodes): $n = 5$, equispaced nodes on $[-10^{-3}, 10^{-3}]$.\n\nFinal output format:\nYour program should produce a single line of output containing the nine results, ordered as $[\\Delta_D^A, \\Delta_d^A, R^A, \\Delta_D^B, \\Delta_d^B, R^B, \\Delta_D^C, \\Delta_d^C, R^C]$, as a comma-separated list enclosed in square brackets. All nine values must be represented as floating-point numbers. No physical units or angle units apply to this problem.",
            "solution": "The problem statement is a well-posed and scientifically sound exercise in numerical analysis, specifically concerning the theory and computation of spectral differentiation matrices. It is self-contained, logically consistent, and all terms are formally defined. The problem is valid and I will proceed with a full derivation and solution.\n\nThe problem is structured in four parts. The first three are theoretical derivations, and the fourth is a computational implementation and comparison.\n\n### Part 1: Recurrence for $L_k'(x)$ and Matrix Construction\n\nThe Newton basis polynomials are defined by $L_0(x) = 1$ and $L_k(x) = \\prod_{j=0}^{k-1} (x - x_j)$ for $k \\geq 1$. We can express $L_k(x)$ in terms of $L_{k-1}(x)$ for $k \\geq 1$:\n$$\nL_k(x) = \\left(\\prod_{j=0}^{k-2} (x - x_j)\\right) (x - x_{k-1}) = L_{k-1}(x) (x - x_{k-1})\n$$\nTo find a recurrence for the derivative $L_k'(x)$, we apply the product rule for differentiation to this expression:\n$$\nL_k'(x) = \\frac{d}{dx} \\left[ L_{k-1}(x) (x - x_{k-1}) \\right] = L_{k-1}'(x) \\cdot (x - x_{k-1}) + L_{k-1}(x) \\cdot 1\n$$\nThis gives the desired recurrence relation for $k \\geq 1$:\n$$\nL_k'(x) = L_{k-1}(x) + (x - x_{k-1}) L_{k-1}'(x)\n$$\nThe base case is derived from $L_0(x) = 1$, which implies $L_0'(x) = 0$.\n\nUsing these definitions, we construct two $(n+1) \\times (n+1)$ matrices.\n1.  The Newton-Vandermonde matrix $V(X)$ has entries $(V(X))_{ik} = L_k(x_i)$. For $k > i$, the product defining $L_k(x_i) = \\prod_{j=0}^{k-1} (x_i - x_j)$ includes the term $(x_i - x_i) = 0$. Thus, $L_k(x_i) = 0$ for $k > i$, which means $V(X)$ is a lower-triangular matrix. Since the nodes $\\{x_j\\}$ are distinct, the diagonal entries $L_k(x_k) = \\prod_{j=0}^{k-1} (x_k - x_j)$ are all non-zero, and $V(X)$ is invertible.\n2.  The matrix of basis derivatives $L'(X)$ has entries $(L'(X))_{ik} = L_k'(x_i)$. Its columns can be computed iteratively using the recurrence. The first column is all zeros, since $L_0'(x_i)=0$. Each subsequent column $k$ can be computed from column $k-1$ of $L'(X)$ and column $k-1$ of $V(X)$.\n\n### Part 2: The Newton Spectral Differentiation Matrix $D^{(N)}(X)$\n\nThe interpolating polynomial is $p(x) = \\sum_{k=0}^{n} a_k L_k(x)$. Its derivative is $p'(x) = \\sum_{k=0}^{n} a_k L_k'(x)$. Evaluating this expression at the interpolation nodes $x_i$ for $i = 0, \\dots, n$ gives the system of equations:\n$$\np'(x_i) = \\sum_{k=0}^{n} L_k'(x_i) a_k\n$$\nIn matrix form, this is $\\boldsymbol{p'}(X) = L'(X) \\boldsymbol{a}$, where $\\boldsymbol{p'}(X)$ is the vector of nodal derivatives $[p'(x_0), \\dots, p'(x_n)]^\\top$.\n\nThe coefficients $\\boldsymbol{a} = [a_0, \\dots, a_n]^\\top$ are determined by the interpolation conditions $p(x_i) = u_i$, which in matrix form is $\\boldsymbol{u} = V(X) \\boldsymbol{a}$. Since $V(X)$ is invertible, we can write $\\boldsymbol{a} = V(X)^{-1} \\boldsymbol{u}$.\n\nSubstituting this expression for $\\boldsymbol{a}$ into the equation for $\\boldsymbol{p'}(X)$, we get:\n$$\n\\boldsymbol{p'}(X) = L'(X) (V(X)^{-1} \\boldsymbol{u}) = (L'(X) V(X)^{-1}) \\boldsymbol{u}\n$$\nThis shows that the linear operator that maps the vector of nodal function values $\\boldsymbol{u}$ to the vector of nodal derivative values $\\boldsymbol{p'}(X)$ is the matrix $D^{(N)}(X) = L'(X) V(X)^{-1}$. This is the spectral differentiation matrix in the Newton basis.\n\n### Part 3: The Barycentric Spectral Differentiation Matrix $D^{(B)}(X)$\n\nThe interpolating polynomial $p(x)$ is unique. Its representation in the Lagrange basis is $p(x) = \\sum_{j=0}^{n} u_j \\ell_j(x)$, where $\\ell_j(x) = \\prod_{k=0, k \\neq j}^{n} \\frac{x-x_k}{x_j-x_k}$ are the Lagrange cardinal polynomials. The derivative is $p'(x) = \\sum_{j=0}^{n} u_j \\ell_j'(x)$.\n\nThe spectral differentiation matrix $D$ has entries $D_{ij} = \\ell_j'(x_i)$.\nFor the off-diagonal entries ($i \\neq j$), we compute:\n$$\nD_{ij} = \\ell_j'(x_i) = \\left. \\frac{d}{dx} \\left( \\frac{\\prod_{k \\neq j} (x-x_k)}{\\prod_{k \\neq j} (x_j-x_k)} \\right) \\right|_{x=x_i}\n$$\nThe numerator is a product of terms $(x-x_k)$. By the product rule, the derivative is a sum of terms where one factor is differentiated. When we evaluate at $x=x_i$, the only term that survives is the one where the factor $(x-x_i)$ was differentiated.\n$$\n\\ell_j'(x_i) = \\frac{\\prod_{k \\neq j, i} (x_i-x_k)}{\\prod_{k \\neq j} (x_j-x_k)} = \\frac{1}{x_i-x_j} \\frac{\\prod_{k \\neq i} (x_i-x_k)}{\\prod_{k \\neq j} (x_j-x_k)}\n$$\nUsing the definition of the barycentric weights $w_i = (\\prod_{k \\neq i} (x_i - x_k))^{-1}$, this becomes:\n$$\nD_{ij} = \\frac{1}{x_i-x_j} \\frac{w_j^{-1}}{w_i^{-1}} = \\frac{w_j}{w_i} \\frac{1}{x_i-x_j}, \\quad i \\neq j\n$$\nThis matches the off-diagonal entries of $D^{(B)}(X)$.\n\nFor the diagonal entries ($i=j$), we use the fact that the sum of the Lagrange polynomials is unity: $\\sum_{j=0}^{n} \\ell_j(x) = 1$. Differentiating with respect to $x$ yields $\\sum_{j=0}^{n} \\ell_j'(x) = 0$. Evaluating at a node $x_i$:\n$$\n\\sum_{j=0}^{n} \\ell_j'(x_i) = 0 \\implies \\ell_i'(x_i) + \\sum_{j \\neq i} \\ell_j'(x_i) = 0\n$$\nThus, the diagonal entry is the negative sum of the off-diagonal entries in the same row:\n$$\nD_{ii} = \\ell_i'(x_i) = - \\sum_{j \\neq i} \\ell_j'(x_i) = - \\sum_{j \\neq i} D_{ij}\n$$\nThis matches the definition of the diagonal entries of $D^{(B)}(X)$.\n\nThe equality $D^{(N)}(X) = D^{(B)}(X)$ stems from the uniqueness of the interpolating polynomial $p(x)$ for a given set of distinct nodes and data. The spectral differentiation matrix is the unique matrix representation of the linear operator that maps nodal values $\\boldsymbol{u}$ to the nodal values of the derivative of $p(x)$. Since both $D^{(N)}(X)$ and $D^{(B)}(X)$ are derived to be this unique operator (albeit from different polynomial bases), they must be identical.\n\n### Part 4: Computational Analysis\n\nThe implementation will construct both matrices $D^{(N)}(X)$ and $D^{(B)}(X)$ for each test case.\n-   $D^{(N)}(X)$ is computed by first building the matrices $V(X)$ and $L'(X)$ using their recurrence relations. Then, the matrix equation $D^{(N)} V = L'$, or equivalently $V^\\top (D^{(N)})^\\top = (L')^\\top$, is solved for $(D^{(N)})^\\top$ using a triangular solver, which is efficient and numerically stable.\n-   $D^{(B)}(X)$ is computed by first calculating the barycentric weights $w_i$ and then using the explicit formulas for its entries.\n-   $\\Delta_D$ and $\\Delta_d$ are maximum absolute differences, measuring the numerical discrepancy between the two theoretically identical matrices and their results when applied to a test vector. These differences are expected to be close to machine precision.\n-   The cost ratio $R$ compares the operational cost of applying a pre-assembled matrix to a vector. The problem defines a cost model for applying a dense $(n+1) \\times (n+1)$ matrix: $(n+1)^2$ multiplications and $(n+1)n$ additions. Since both $D^{(N)}(X)$ and $D^{(B)}(X)$ are dense matrices of the same size, their application costs under this model are identical.\n    $$\n    C_{\\text{app}}(D^{(N)}) = C_{\\text{app}}(D^{(B)}) = (n+1)^2 + n(n+1)\n    $$\n    Therefore, the ratio $R = \\frac{C_{\\text{app}}(D^{(N)})}{C_{\\text{app}}(D^{(B)})} = 1$ for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing and comparing Newton and Barycentric\n    spectral differentiation matrices for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case A (Chebyshev)\",\n            \"n\": 8,\n            \"nodes_func\": lambda n: -np.cos(np.pi * np.arange(n + 1) / n)\n        },\n        {\n            \"name\": \"Case B (Boundary)\",\n            \"n\": 1,\n            \"nodes_func\": lambda n: np.array([-1.0, 1.0])\n        },\n        {\n            \"name\": \"Case C (Clustered)\",\n            \"n\": 5,\n            \"nodes_func\": lambda n: np.linspace(-1e-3, 1e-3, n + 1)\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        x = case[\"nodes_func\"](n)\n        n_p1 = n + 1\n\n        # 1. Construct D^(N)(X)\n        # Build Newton-Vandermonde V and its derivative L_prime\n        V = np.zeros((n_p1, n_p1))\n        L_prime = np.zeros((n_p1, n_p1))\n\n        V[:, 0] = 1.0\n        # L_prime[:, 0] is already 0.0\n\n        for k in range(1, n_p1):\n            V[:, k] = V[:, k - 1] * (x - x[k - 1])\n            L_prime[:, k] = V[:, k - 1] + (x - x[k - 1]) * L_prime[:, k - 1]\n\n        # Solve V.T @ D_N.T = L_prime.T for D_N.T\n        # V.T is upper triangular, so we use lower=False\n        try:\n            D_N_T = solve_triangular(V.T, L_prime.T, lower=False)\n            D_N = D_N_T.T\n        except np.linalg.LinAlgError:\n            # For ill-conditioned cases, direct inversion might be needed\n            # as a fallback, though solve_triangular is preferred.\n            V_inv = np.linalg.inv(V)\n            D_N = L_prime @ V_inv\n            \n        # 2. Construct D^(B)(X)\n        D_B = np.zeros((n_p1, n_p1))\n        \n        # Calculate barycentric weights w_i\n        w = np.ones(n_p1)\n        for i in range(n_p1):\n            for j in range(n_p1):\n                if i != j:\n                    w[i] *= (x[i] - x[j])\n        w = 1.0 / w\n\n        # Fill D_B entries\n        for i in range(n_p1):\n            row_sum = 0.0\n            for j in range(n_p1):\n                if i != j:\n                    term = (w[j] / w[i]) / (x[i] - x[j])\n                    D_B[i, j] = term\n                    row_sum += term\n            D_B[i, i] = -row_sum\n\n        # 3. Compute required quantities\n        # Delta_D: Max absolute difference between D_N and D_B\n        delta_D = np.max(np.abs(D_N - D_B))\n\n        # Delta_d: Max absolute difference in application to a vector u\n        # u is nodal values of q(x) = sum_{k=0 to n} c_k * x^k\n        # where c_k = (-1)^k / (k+1)\n        c = [((-1)**k) / (k + 1) for k in range(n_p1)]\n        u = np.polynomial.polynomial.polyval(x, c)\n\n        d_N = D_N @ u\n        d_B = D_B @ u\n        delta_d = np.max(np.abs(d_N - d_B))\n\n        # R: Ratio of application costs\n        # The cost model is for a pre-assembled dense matrix-vector product.\n        # Since both D_N and D_B are dense (n+1)x(n+1) matrices, their\n        # application costs are identical.\n        R = 1.0\n\n        results.extend([delta_D, delta_d, R])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice moves into the heart of modern discontinuous Galerkin (DG) methods, tackling the challenge of representing solutions with sharp features like shocks. It critically examines why the interpolating property of the Newton polynomial is not merely a convenience but a necessity for preserving fundamental numerical properties like conservation. By comparing the standard interpolant with a non-interpolating least-squares approximation , you will explore how design choices in the element-local reconstruction directly impact the stability and accuracy of the scheme, particularly in the context of shock capturing and limiters.",
            "id": "3402305",
            "problem": "Consider a one-dimensional scalar conservation law $\\partial_t u + \\partial_x f(u) = 0$ on a single reference element $K=[-1,1]$ within a nodal discontinuous Galerkin (DG) method. Let the polynomial degree be $N$, and let the solution degrees of freedom be the nodal values $\\{u_i(t)\\}_{i=0}^N$ at the Legendre–Gauss–Lobatto (LGL) nodes $\\{x_i\\}_{i=0}^N \\subset [-1,1]$. Assume the element contains an underresolved internal shock located at $x_s \\in (-1,1)$, so that the true $u(x,t)$ is not well approximated by polynomials of degree $N$.\n\nTwo element-local reconstructions $\\hat u(x)$ of degree $N$ are considered for the volume term:\n- Newton interpolation $I_N u(x)$, the unique degree-$N$ polynomial that interpolates the nodal data $\\{(x_i,u_i)\\}_{i=0}^N$. It is constructed in the Newton divided-difference form with respect to the LGL nodes.\n- A discrete least-squares fit $P_N^{\\mathrm{LS}} u(x)$, the degree-$N$ polynomial minimizing $\\sum_{j=1}^M \\left(P_N^{\\mathrm{LS}} u(\\xi_j)-u(\\xi_j)\\right)^2$ over a set of oversampled points $\\{\\xi_j\\}_{j=1}^M \\subset [-1,1]$ with $M \\gg N+1$ (for definiteness, take $\\{\\xi_j\\}$ as Chebyshev points of the first kind), without any constraints enforcing agreement with the nodal degrees of freedom or the cell average.\n\nIn the strong-form collocation DG discretization on LGL nodes, let $M$ denote the diagonal mass matrix with entries $M_{ii}=w_i$ (LGL quadrature weights), and let $D$ be the nodal differentiation matrix satisfying the summation-by-parts (SBP) relation $M D + D^\\top M = B$, where $B=\\mathrm{diag}(-1,0,\\dots,0,1)$ acts at the endpoints. The semidiscrete volume term uses the nodal evaluation of the reconstructed flux $g_i = f(\\hat u(x_i))$ so that\n$$\nM \\,\\dot{\\boldsymbol{u}} = - S \\,\\boldsymbol{g} + \\boldsymbol{b}, \\quad S:=M D,\n$$\nwith $\\boldsymbol{u}=(u_0,\\dots,u_N)^\\top$, $\\boldsymbol{g}=(g_0,\\dots,g_N)^\\top$, and $\\boldsymbol{b}$ collecting the consistent numerical fluxes at $x=-1$ and $x=1$.\n\nAssume exact interface fluxes and standard strong-form collocation on LGL nodes with volume quadrature exact for polynomials up to degree $2N-1$. You may further assume basic regularity of $f$ and that $f(u)$ is evaluated pointwise at nodal points for the volume term.\n\nWhich of the following statements correctly identify regimes where Newton interpolation $I_N u$ is preferable to the least-squares fit $P_N^{\\mathrm{LS}} u$ for the element-local reconstruction used in the DG volume term? Select all that apply.\n\nA. When $f$ is linear, the SBP property implies exact semidiscrete conservation of the cell average if the reconstructed flux is based on the Newton interpolant $I_N u$ (so that the reconstruction matches the evolved nodal degrees of freedom at the endpoints). Replacing $I_N u$ by an unconstrained $P_N^{\\mathrm{LS}} u$ generally breaks this exact conservation at machine precision because the volume flux trace at endpoints no longer matches the face fluxes. Hence, in this regime interpolation is preferable.\n\nB. If the shock lies exactly at an LGL node, using the unconstrained least-squares fit $P_N^{\\mathrm{LS}} u$ with many equispaced samples guarantees a strictly smaller $L^\\infty$ error than $I_N u$ while preserving semidiscrete conservation in the DG scheme. Hence, in this regime least-squares is preferable to interpolation.\n\nC. Under overintegration of the volume term with quadrature exact for integrands up to total polynomial degree $3N$, the discrete volume integrals computed with $I_N u$ and $P_N^{\\mathrm{LS}} u$ are identical for any data $u$ producing underresolved shocks, so there is no advantage to interpolation in this regime.\n\nD. In element-local limiting for troubled cells that enforces preservation of the cell average and imposes a bounded total variation at the nodal degrees of freedom, using $I_N u$ preserves both the limiter-enforced cell average and nodal bounds in the reconstruction used by the volume term. An unconstrained refit by $P_N^{\\mathrm{LS}} u$ can violate the preserved cell average and the nodal bounds, destroying conservation and limiter guarantees. Hence, in this regime interpolation is preferable.",
            "solution": "The user wants a critical analysis of a problem statement concerning numerical methods for conservation laws, followed by a detailed solution if the problem is valid.\n\n### Step 1: Extract Givens\n- **PDE**: One-dimensional scalar conservation law, $\\partial_t u + \\partial_x f(u) = 0$.\n- **Domain**: A single reference element $K=[-1,1]$.\n- **Numerical Method**: Nodal discontinuous Galerkin (DG) of polynomial degree $N$.\n- **Degrees of Freedom (DoFs)**: Nodal values $\\{u_i(t)\\}_{i=0}^N$ at Legendre–Gauss–Lobatto (LGL) nodes $\\{x_i\\}_{i=0}^N$.\n- **Solution Type**: Contains an underresolved internal shock.\n- **Reconstruction 1 ($\\hat u = I_N u$)**: The unique degree-$N$ Newton interpolating polynomial passing through the nodal data points $\\{(x_i, u_i)\\}_{i=0}^N$. By definition, $I_N u(x_i) = u_i$.\n- **Reconstruction 2 ($\\hat u = P_N^{\\mathrm{LS}} u$)**: A degree-$N$ discrete least-squares polynomial fit that minimizes $\\sum_{j=1}^M (P_N^{\\mathrm{LS}} u(\\xi_j)-u(\\xi_j))^2$ over $M \\gg N+1$ Chebyshev points $\\{\\xi_j\\}$. This fit is \"unconstrained,\" meaning it does not necessarily interpolate the nodal data ($P_N^{\\mathrm{LS}} u(x_i) \\neq u_i$) or preserve the cell average.\n- **Discretization Details**: Strong-form collocation on LGL nodes. The semidiscrete equation is $M \\,\\dot{\\boldsymbol{u}} = - S \\,\\boldsymbol{g} + \\boldsymbol{b}$, where:\n    - $\\boldsymbol{u}=(u_0,\\dots,u_N)^\\top$ is the vector of nodal DoFs.\n    - $M$ is the diagonal LGL mass matrix with $M_{ii}=w_i > 0$.\n    - $D$ is the nodal differentiation matrix.\n    - $S = MD$.\n    - $\\boldsymbol{g}=(g_0,\\dots,g_N)^\\top$ is the nodal flux vector, with $g_i = f(\\hat u(x_i))$.\n    - $\\boldsymbol{b}$ contains numerical flux contributions at the element boundaries.\n- **Summation-By-Parts (SBP) Property**: The operators satisfy $M D + D^\\top M = B$, where $B=\\mathrm{diag}(-1,0,\\dots,0,1)$.\n- **Assumptions**: Exact interface fluxes are used in the sense that the numerical flux calculation is done exactly. The volume quadrature is exact for polynomials of degree up to $2N-1$ (this is a standard property of LGL quadrature).\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the outlined criteria.\n- **Scientific Grounding**: The problem is firmly located within the well-established field of numerical analysis for partial differential equations, specifically focusing on DG methods for hyperbolic conservation laws. All concepts—DG, LGL nodes, SBP property, strong form, Newton interpolation, least-squares approximation, and shock-capturing limiters—are standard and rigorously defined in the literature. The problem is scientifically sound.\n- **Well-Posedness**: The question asks for a qualitative comparison of two reconstruction methods within a DG framework under specific conditions. The provided information is sufficient to analyze the properties of the resulting numerical schemes, such as conservation and compatibility with limiters. The question is well-posed.\n- **Objectivity**: The problem is stated in precise, technical language, free from subjectivity or ambiguity. The distinction between the two reconstruction methods is clearly articulated.\n- **Completeness and Consistency**: The problem is self-contained. It defines the governing equation, the discretization, the operators and their properties (SBP), and the two distinct reconstruction approaches. There are no apparent contradictions in the problem setup.\n- **Realism**: The scenario is a realistic idealization used to study the behavior of numerical schemes. The challenges of representing shocks with high-order polynomials and the trade-offs between different reconstruction strategies are central topics in the development of DG methods.\n- **Other Flaws**: The problem is not trivial, ill-posed, or pseudo-profound. It addresses subtle but critical aspects of implementing stable and accurate DG schemes.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full analysis of the options is warranted.\n\n### Principle-Based Derivation\n\nThe core of the problem lies in the properties of the semi-discrete DG scheme, particularly its ability to conserve the cell-average of the solution, $\\bar{u} = \\frac{1}{2}\\int_{-1}^1 u_h(x) dx$. In the nodal DG setting with LGL nodes, the polynomial solution is $u_h(x) = \\sum_{i=0}^N u_i L_i(x)$, where $L_i(x)$ are the Lagrange basis polynomials. Using the LGL quadrature rule, which is exact for polynomials of degree up to $2N-1$, the cell average is computed exactly as $\\bar{u} = \\frac{1}{2} \\sum_{i=0}^N w_i u_i = \\frac{1}{2L} \\boldsymbol{1}^\\top M \\boldsymbol{u}$ (where the element length is $L=2$).\n\nThe time evolution of the total mass in the cell, $\\int_{-1}^1 u_h dx = \\boldsymbol{1}^\\top M \\boldsymbol{u}$, is given by taking the dot product of the semi-discrete equation with $\\boldsymbol{1}^\\top$:\n$$ \\frac{d}{dt}(\\boldsymbol{1}^\\top M \\boldsymbol{u}) = \\boldsymbol{1}^\\top M \\dot{\\boldsymbol{u}} = \\boldsymbol{1}^\\top(-S\\boldsymbol{g} + \\boldsymbol{b}) = -\\boldsymbol{1}^\\top S\\boldsymbol{g} + \\boldsymbol{1}^\\top\\boldsymbol{b} $$\nThe term $\\boldsymbol{1}^\\top S\\boldsymbol{g}$ represents the contribution from the volume integral. Let's analyze it. $S=MD$. The $j$-th component of the row vector $\\boldsymbol{1}^\\top S$ is $\\sum_{i=0}^N (\\boldsymbol{1}^\\top S)_{i} S_{ij} = \\sum_{i=0}^N S_{ij} = \\sum_{i=0}^N w_i D_{ij}$. For a differentiation matrix $D$ on LGL nodes, it is a known property that $\\sum_{i=0}^N w_i D_{ij} = \\int_{-1}^1 \\frac{d}{dx} L_j(x) dx = L_j(1) - L_j(-1)$. Since $L_j(x)$ is the Lagrange polynomial for node $x_j$, we have $L_j(x_k) = \\delta_{jk}$. With LGL nodes, $x_0=-1$ and $x_N=1$. Thus, $L_j(1) = \\delta_{jN}$ and $L_j(-1) = \\delta_{j0}$.\nThis gives $(\\boldsymbol{1}^\\top S)_j = \\delta_{jN} - \\delta_{j0}$.\nTherefore, the volume term contribution is:\n$$ \\boldsymbol{1}^\\top S \\boldsymbol{g} = \\sum_{j=0}^N (\\delta_{jN} - \\delta_{j0}) g_j = g_N - g_0 $$\nThis identity, which relies on the SBP-like properties of the nodal DG operators on LGL nodes, shows that the discrete volume integral for the cell average \"telescopes\" to the boundaries.\n\nThe total change in cell mass is thus:\n$$ \\frac{d}{dt}\\int_{-1}^1 u_h dx = -(g_N - g_0) + \\boldsymbol{1}^\\top\\boldsymbol{b} $$\nThe scheme is defined to be conservative if this quantity equals $g_0^* - g_N^*$, where $g_0^* = f^*(u_0^-, u_0)$ and $g_N^* = f^*(u_N, u_N^+)$ are the numerical fluxes at the element boundaries $x=-1$ and $x=1$, respectively. A standard strong-form DG implementation ensures this by defining the boundary term $\\boldsymbol{b}$ appropriately to enforce this condition. This cancellation crucially requires that the \"internal\" flux values, $g_0$ and $g_N$, are consistent with the arguments of the numerical flux.\n\nWe now evaluate each option based on this understanding.\n\n### Option-by-Option Analysis\n\n**A. When $f$ is linear, the SBP property implies exact semidiscrete conservation of the cell average if the reconstructed flux is based on the Newton interpolant $I_N u$ (so that the reconstruction matches the evolved nodal degrees of freedom at the endpoints). Replacing $I_N u$ by an unconstrained $P_N^{\\mathrm{LS}} u$ generally breaks this exact conservation at machine precision because the volume flux trace at endpoints no longer matches the face fluxes. Hence, in this regime interpolation is preferable.**\n\n- **Analysis with $I_N u$**: The reconstruction is $\\hat u(x) = I_N u(x)$. By definition, $I_N u(x_i) = u_i$. The nodal flux vector is $\\boldsymbol{g}=(f(u_0), f(u_1), \\dots, f(u_N))^\\top$. The volume integral contribution telescopes to $g_N - g_0 = f(u_N) - f(u_0)$. The arguments to the physical flux at the boundary, $u_N$ and $u_0$, are precisely the DoFs used to define the state for the numerical flux calculation, $f^*(u_N, \\cdot)$ and $f^*(\\cdot, u_0)$. This consistency allows for the boundary corrections to be formulated such that exact semidiscrete conservation of the cell average is achieved. The SBP property is essential for the discrete volume term to telescope to the boundaries. Linearity of $f$ is a sufficient but not necessary condition; conservation holds for nonlinear $f$ as well, but the statement is not incorrect in citing the linear case.\n- **Analysis with $P_N^{\\mathrm{LS}} u$**: The reconstruction is $\\hat u(x) = P_N^{\\mathrm{LS}} u(x)$. It is unconstrained, so in general, $P_N^{\\mathrm{LS}} u(x_i) \\neq u_i$. The nodal flux vector is $\\boldsymbol{g}^{\\mathrm{LS}}=(f(P_N^{\\mathrm{LS}} u(x_0)), \\dots, f(P_N^{\\mathrm{LS}} u(x_N)))^\\top$. The volume integral contribution becomes $g_N^{\\mathrm{LS}} - g_0^{\\mathrm{LS}} = f(P_N^{\\mathrm{LS}} u(1)) - f(P_N^{\\mathrm{LS}} u(-1))$. However, the numerical flux calculation at the boundary still depends on the evolved DoFs, $u_N$ and $u_0$. Since $u_N \\neq P_N^{\\mathrm{LS}} u(1)$ and $u_0 \\neq P_N^{\\mathrm{LS}} u(-1)$, the volume flux trace does not match the surface flux trace. The cancellation required for conservation is broken.\n- **Verdict**: The statement is correct. Conservation is a fundamental requirement for methods simulating conservation laws. The use of an interpolating polynomial ($I_N u$) is critical for achieving this in a strong-form nodal DG scheme. The unconstrained least-squares fit breaks this property. Therefore, interpolation is preferable. **Correct**.\n\n**B. If the shock lies exactly at an LGL node, using the unconstrained least-squares fit $P_N^{\\mathrm{LS}} u$ with many equispaced samples guarantees a strictly smaller $L^\\infty$ error than $I_N u$ while preserving semidiscrete conservation in the DG scheme. Hence, in this regime least-squares is preferable to interpolation.**\n\n- **Analysis**: This statement contains several flawed claims.\n    1. **Conservation**: As established in the analysis of A, the unconstrained fit $P_N^{\\mathrm{LS}} u$ does *not* preserve semidiscrete conservation. This part of the statement is false.\n    2. **$L^\\infty$ error**: A least-squares fit minimizes an $L^2$-type norm, not the $L^\\infty$ norm. Approximating a discontinuous function (a shock) with a global polynomial, whether by interpolation or least-squares, induces Gibbs oscillations. There is no guarantee that a least-squares fit will have a strictly smaller $L^\\infty$ error than an interpolant. In fact, both are expected to have $O(1)$ errors in the $L^\\infty$ norm near the discontinuity. The claim of a *guaranteed strictly smaller* error is far too strong.\n    3. **Sampling points**: The statement mentions \"equispaced samples,\" which are known to be poor choices for high-degree polynomial approximation due to the Runge phenomenon. This contradicts the problem's setup, which specifies Chebyshev points, a much better choice.\n- **Verdict**: The statement is incorrect on multiple grounds. It falsely claims conservation is preserved and makes an unsubstantiated, strong claim about error reduction. **Incorrect**.\n\n**C. Under overintegration of the volume term with quadrature exact for integrands up to total polynomial degree $3N$, the discrete volume integrals computed with $I_N u$ and $P_N^{\\mathrm{LS}} u$ are identical for any data $u$ producing underresolved shocks, so there is no advantage to interpolation in this regime.**\n\n- **Analysis**: The \"discrete volume integral\" contribution to the cell average update is the scalar value $\\boldsymbol{1}^\\top S \\boldsymbol{g}$. As derived earlier, $\\boldsymbol{1}^\\top S \\boldsymbol{g} = g_N - g_0$. This result is an exact algebraic identity for LGL-based SBP operators and does not depend on the degree of the quadrature rule, as long as the rule is the LGL one (which defines the mass matrix $M$). Overintegration (using a higher-degree rule) does not change this telescoping sum property.\n- The integral computed with $I_N u$ yields $f(u_N) - f(u_0)$.\n- The integral computed with $P_N^{\\mathrm{LS}} u$ yields $f(P_N^{\\mathrm{LS}} u(1)) - f(P_N^{\\mathrm{LS}} u(-1))$.\nSince $u_i \\neq P_N^{\\mathrm{LS}} u(x_i)$ in general, these two quantities are not identical. The claim that they are \"identical for any data $u$\" is false. The two reconstruction methods lead to different discrete volume integrals.\n- **Verdict**: The premise of the statement is false. Therefore, the statement is incorrect. **Incorrect**.\n\n**D. In element-local limiting for troubled cells that enforces preservation of the cell average and imposes a bounded total variation at the nodal degrees of freedom, using $I_N u$ preserves both the limiter-enforced cell average and nodal bounds in the reconstruction used by the volume term. An unconstrained refit by $P_N^{\\mathrm{LS}} u$ can violate the preserved cell average and the nodal bounds, destroying conservation and limiter guarantees. Hence, in this regime interpolation is preferable.**\n\n- **Analysis**: This option describes a shock-capturing/limiting procedure, which is standard for DG methods in the presence of shocks. A limiter modifies the nodal DoFs, $\\boldsymbol{u} \\to \\boldsymbol{u}^{lim}$, to satisfy certain properties, such as preserving the cell average and enforcing nodal bounds (e.g., monotonicity).\n- **Analysis with $I_N u$**: If the new state is represented by the interpolant of the limited DoFs, $\\hat u(x) = I_N u^{lim}(x)$, then:\n    1. **Cell Average**: The cell average of the reconstruction is $\\frac{1}{2}\\sum_i w_i (I_N u^{lim}(x_i)) = \\frac{1}{2}\\sum_i w_i u_i^{lim}$. Since the limiter is designed to ensure $\\sum_i w_i u_i^{lim} = \\sum_i w_i u_i$, the cell average is preserved by construction.\n    2. **Nodal Bounds**: The volume term calculation in the collocation scheme uses flux values $g_i = f(\\hat u(x_i))$. With this reconstruction, $\\hat u(x_i) = u_i^{lim}$. Thus, the state values used in the volume term are precisely the limited nodal values, which satisfy the bounds imposed by the limiter. The guarantees of the limiter are directly inherited.\n- **Analysis with $P_N^{\\mathrm{LS}} u$**: If, after obtaining the limited DoFs $\\boldsymbol{u}^{lim}$, one performs an unconstrained least-squares refit to get a new polynomial $P_N^{\\mathrm{LS}} u$, then:\n    1. **Cell Average**: An unconstrained least-squares fit does not, in general, preserve the integral (cell average) of the data it is fitting. So, $\\int_{-1}^1 P_N^{\\mathrm{LS}} u(x) dx \\neq \\int_{-1}^1 I_N u^{lim}(x) dx$. This violates cell average preservation, a key function of the limiter, and breaks global conservation.\n    2. **Nodal Bounds**: The values used in the volume term would be $f(P_N^{\\mathrm{LS}} u(x_i))$. Since $P_N^{\\mathrm{LS}} u(x_i) \\neq u_i^{lim}$, there is no guarantee that the new values $P_N^{\\mathrm{LS}} u(x_i)$ will satisfy the bounds enforced by the limiter on $u_i^{lim}$. This defeats the purpose of the limiter's bound-enforcing or monotonicity-preserving properties.\n- **Verdict**: The statement accurately describes why interpolation is essential when using limiters that act on the nodal DoFs. An unconstrained refitting step would undermine the very purpose of the limiter. Therefore, interpolation is preferable in this regime. **Correct**.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}