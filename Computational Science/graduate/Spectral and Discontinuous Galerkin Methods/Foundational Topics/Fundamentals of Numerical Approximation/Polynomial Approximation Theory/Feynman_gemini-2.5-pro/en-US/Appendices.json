{
    "hands_on_practices": [
        {
            "introduction": "At the heart of approximation theory lies the question of finding the \"best\" polynomial representation of a given function. This exercise grounds the abstract concept of the $L^2$ projection in a concrete calculation, demonstrating that it yields the unique best approximation in the least-squares sense. By working through the projection of a simple parabola onto a line, you will verify the fundamental orthogonality condition that underpins the power and elegance of spectral methods .",
            "id": "3408976",
            "problem": "Consider the one-dimensional reference element $[-1,1]$ used in high-order spectral and Discontinuous Galerkin methods for conservation laws. Let $\\{P_n(x)\\}_{n=0}^{\\infty}$ denote the Legendre polynomials on $[-1,1]$ with weight $w(x)=1$, characterized by $P_0(x)=1$, $P_1(x)=x$, and orthogonality $\\int_{-1}^{1} P_n(x) P_m(x)\\,dx = \\frac{2}{2n+1}\\,\\delta_{nm}$. Let the inner product and induced norm be $\\langle u,v\\rangle := \\int_{-1}^{1} u(x)v(x)\\,dx$ and $\\|u\\|_{L^2(-1,1)} := \\sqrt{\\langle u,u\\rangle}$, respectively. Define $f(x)=x^2$. \n\nUsing only the orthogonality of the Legendre basis and the variational characterization of the $L^2$ projection onto $\\mathbb{P}_1 := \\operatorname{span}\\{1,x\\}$ (namely, that the projection $p_1 \\in \\mathbb{P}_1$ satisfies $\\langle f-p_1, q\\rangle = 0$ for all $q \\in \\mathbb{P}_1$), perform the following:\n\n1) Compute the Legendre modal coefficients of $f$ in the basis $\\{P_n\\}$.\n\n2) Use these coefficients to construct the $L^2$ projection $p_1$ of $f$ onto $\\mathbb{P}_1$.\n\n3) Verify that $p_1$ is the unique best $L^2$ approximation of $f$ by both checking the orthogonality conditions and by appealing to the Pythagorean decomposition induced by orthogonality of the Legendre basis.\n\nReport your final answer as the exact analytic expression for $p_1(x)$. No rounding is required and no units are involved. Your final boxed answer must contain only the expression for $p_1(x)$.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It is a standard exercise in polynomial approximation theory. We will proceed with the solution by following the three specified steps.\n\nThe function to be approximated is $f(x)=x^2$ on the interval $[-1,1]$. The approximation space is $\\mathbb{P}_1$, the space of polynomials of degree at most $1$, which is spanned by the first two Legendre polynomials, $\\{P_0(x), P_1(x)\\}$. The Legendre polynomials $\\{P_n(x)\\}$ form an orthogonal basis for the space of square-integrable functions $L^2(-1,1)$ with respect to the inner product $\\langle u,v\\rangle = \\int_{-1}^{1} u(x)v(x)\\,dx$. The orthogonality property is given as $\\langle P_n, P_m \\rangle = \\frac{2}{2n+1}\\delta_{nm}$.\n\n**1) Computation of Legendre Modal Coefficients**\n\nAny function $f \\in L^2(-1,1)$ can be represented by its Legendre series expansion:\n$$f(x) = \\sum_{n=0}^{\\infty} \\hat{f}_n P_n(x)$$\nwhere the modal coefficients $\\hat{f}_n$ are found by projecting $f$ onto each basis function $P_n$:\n$$\\hat{f}_n = \\frac{\\langle f, P_n \\rangle}{\\langle P_n, P_n \\rangle} = \\frac{\\langle f, P_n \\rangle}{\\|P_n\\|^2}$$\nUsing the given normalization, $\\|P_n\\|^2 = \\frac{2}{2n+1}$, the formula for the coefficients becomes:\n$$\\hat{f}_n = \\frac{2n+1}{2} \\int_{-1}^{1} f(x) P_n(x) \\,dx$$\nIn this problem, $f(x)=x^2$. Since $f(x)$ is a polynomial of degree $2$, its Legendre series will be finite, terminating at $n=2$. We can find the coefficients by direct algebraic manipulation. We are given $P_0(x)=1$ and $P_1(x)=x$. The third Legendre polynomial is $P_2(x) = \\frac{1}{2}(3x^2-1)$. We can express $x^2$ in terms of $P_0(x)$ and $P_2(x)$:\n$$P_2(x) = \\frac{3}{2}x^2 - \\frac{1}{2}P_0(x) \\implies \\frac{3}{2}x^2 = P_2(x) + \\frac{1}{2}P_0(x)$$\n$$x^2 = \\frac{2}{3}P_2(x) + \\frac{1}{3}P_0(x)$$\nThus, the exact Legendre series for $f(x)=x^2$ is:\n$$f(x) = \\frac{1}{3}P_0(x) + 0 \\cdot P_1(x) + \\frac{2}{3}P_2(x)$$\nFrom this expansion, we can directly identify the modal coefficients:\n$\\hat{f}_0 = \\frac{1}{3}$\n$\\hat{f}_1 = 0$\n$\\hat{f}_2 = \\frac{2}{3}$\n$\\hat{f}_n = 0$ for all $n > 2$.\n\n**2) Construction of the $L^2$ Projection**\n\nThe $L^2$ projection of $f$ onto the subspace $\\mathbb{P}_k$ (polynomials of degree at most $k$) is obtained by truncating the Legendre series at $n=k$. In our case, we seek the projection $p_1(x)$ onto $\\mathbb{P}_1 = \\operatorname{span}\\{P_0, P_1\\}$. This is given by:\n$$p_1(x) = \\sum_{n=0}^{1} \\hat{f}_n P_n(x) = \\hat{f}_0 P_0(x) + \\hat{f}_1 P_1(x)$$\nSubstituting the coefficients we found:\n$$p_1(x) = \\left(\\frac{1}{3}\\right) P_0(x) + (0) P_1(x) = \\frac{1}{3}(1) + 0(x) = \\frac{1}{3}$$\nThe $L^2$ projection of $f(x)=x^2$ onto $\\mathbb{P}_1$ is the constant function $p_1(x) = \\frac{1}{3}$.\n\n**3) Verification of Best Approximation**\n\nWe must verify that $p_1(x) = \\frac{1}{3}$ is the unique best $L^2$ approximation of $f(x)=x^2$ in $\\mathbb{P}_1$.\n\n**Verification via Orthogonality Condition:**\nThe variational characterization states that $p_1$ is the $L^2$ projection if and only if the error $f-p_1$ is orthogonal to the approximation space $\\mathbb{P}_1$. That is, $\\langle f-p_1, q \\rangle = 0$ for all $q \\in \\mathbb{P}_1$. It suffices to check this for a basis of $\\mathbb{P}_1$, for instance, $\\{P_0(x), P_1(x)\\} = \\{1, x\\}$.\nThe error function is $e(x) = f(x) - p_1(x) = x^2 - \\frac{1}{3}$.\nFor $q(x) = P_0(x) = 1$:\n$$\\langle e, P_0 \\rangle = \\int_{-1}^{1} \\left(x^2 - \\frac{1}{3}\\right) (1) \\,dx = \\left[\\frac{x^3}{3} - \\frac{x}{3}\\right]_{-1}^{1} = \\left(\\frac{1}{3} - \\frac{1}{3}\\right) - \\left(-\\frac{1}{3} - (-\\frac{1}{3})\\right) = 0 - 0 = 0$$\nFor $q(x) = P_1(x) = x$:\n$$\\langle e, P_1 \\rangle = \\int_{-1}^{1} \\left(x^2 - \\frac{1}{3}\\right) (x) \\,dx = \\int_{-1}^{1} \\left(x^3 - \\frac{x}{3}\\right) \\,dx$$\nThe integrand is an odd function, and the integral is over a symmetric interval $[-1,1]$, so the result is $0$.\nSince the error is orthogonal to the basis vectors of $\\mathbb{P}_1$, it is orthogonal to the entire space, thus confirming $p_1(x) = \\frac{1}{3}$ is the correct $L^2$ projection.\n\n**Verification via Pythagorean Decomposition:**\nLet $q(x)$ be any arbitrary polynomial in $\\mathbb{P}_1$. We want to show that $\\|f-p_1\\|^2 \\le \\|f-q\\|^2$. We decompose the error $f-q$ as:\n$$f-q = (f-p_1) + (p_1-q)$$\nNow consider the squared $L^2$-norm:\n$$\\|f-q\\|^2 = \\langle (f-p_1) + (p_1-q), (f-p_1) + (p_1-q) \\rangle$$\n$$= \\langle f-p_1, f-p_1 \\rangle + 2\\langle f-p_1, p_1-q \\rangle + \\langle p_1-q, p_1-q \\rangle$$\n$$= \\|f-p_1\\|^2 + 2\\langle f-p_1, p_1-q \\rangle + \\|p_1-q\\|^2$$\nThe term $p_1-q$ is a difference of two polynomials in $\\mathbb{P}_1$, so it is also in $\\mathbb{P}_1$. The error term $f-p_1$ is $x^2 - \\frac{1}{3} = \\frac{2}{3}P_2(x)$. By the orthogonality of Legendre polynomials, $P_2$ is orthogonal to every polynomial in $\\mathbb{P}_1 = \\operatorname{span}\\{P_0, P_1\\}$. Therefore, the cross term is zero:\n$$\\langle f-p_1, p_1-q \\rangle = \\left\\langle \\frac{2}{3}P_2(x), p_1(x)-q(x) \\right\\rangle = 0$$\nThe squared norm simplifies to the Pythagorean theorem for orthogonal vectors:\n$$\\|f-q\\|^2 = \\|f-p_1\\|^2 + \\|p_1-q\\|^2$$\nSince $\\|p_1-q\\|^2 \\ge 0$, it is clear that $\\|f-q\\|^2 \\ge \\|f-p_1\\|^2$. The minimum approximation error is achieved when $\\|p_1-q\\|^2 = 0$, which implies $p_1-q=0$ (almost everywhere). This occurs uniquely when $q=p_1$. This confirms that $p_1(x)=\\frac{1}{3}$ is the unique best $L^2$ approximation to $f(x)=x^2$ from the space $\\mathbb{P}_1$.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "While modal coefficients provide a complete description of a polynomial, practical algorithms in spectral and discontinuous Galerkin methods operate on function values at specific grid points, or nodes. This practice bridges the modal and nodal representations by focusing on the Gauss-Lobatto-Legendre (GLL) points, which are essential for high-order numerical schemes due to their excellent approximation and integration properties. You will construct the key operator that maps between these representations and compute the pointwise interpolation error, a critical metric for assessing approximation quality .",
            "id": "3409031",
            "problem": "Consider the one-dimensional reference interval $[-1,1]$ and the polynomial interpolation framework used in spectral and Discontinuous Galerkin (DG) methods. Let $\\{P_{n}(x)\\}_{n=0}^{\\infty}$ denote the Legendre polynomials on $[-1,1]$, orthogonal with respect to the weight $1$, and normalized so that $P_{n}(1)=1$. For a given polynomial degree $N$, the Gauss–Lobatto–Legendre (GLL) nodes are defined as the union of the endpoints $\\{-1,1\\}$ and the $N-1$ distinct interior roots of $P_{N}'(x)$.\n\nWork with degree $N=3$. The modal-to-nodal interpolation matrix $V$ is defined by $V_{jk}=P_{k}(x_{j})$ for $j=1,\\dots,N+1$ and $k=0,\\dots,N$, where $\\{x_{j}\\}_{j=1}^{N+1}$ are the GLL nodes. This matrix maps modal Legendre coefficients to nodal values of a degree-$N$ polynomial on the GLL grid. Construct this matrix $V$ explicitly for $N=3$ on $[-1,1]$.\n\nThen, consider the function $f(x)=\\exp(x)$, and let $p_{3}(x)$ be its degree-$3$ Lagrange interpolant at the $N+1=4$ GLL nodes. Using the fundamental definition of Lagrange interpolation at distinct nodes, derive the interpolation weights $\\{\\ell_{j}(0)\\}_{j=1}^{4}$ at $x=0$ and compute the pointwise interpolation error at $x=0$, defined by $E(0)=f(0)-p_{3}(0)$. Express your final answer as a single closed-form analytical expression. No rounding is required, and no units are involved.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It is a standard exercise in polynomial approximation theory, specifically concerning Lagrange interpolation on Gauss-Lobatto-Legendre nodes. The problem is deemed valid.\n\nThe solution is constructed in three parts as requested: first, the determination of the Gauss-Lobatto-Legendre (GLL) nodes and the construction of the modal-to-nodal matrix $V$; second, the derivation of the Lagrange interpolation weights at $x=0$; and third, the computation of the pointwise interpolation error at $x=0$.\n\nFirst, we determine the GLL nodes for polynomial degree $N=3$ on the interval $[-1,1]$. These nodes consist of the endpoints $\\{-1,1\\}$ and the roots of the derivative of the Legendre polynomial $P_3(x)$. The Legendre polynomials, normalized such that $P_n(1)=1$, are:\n$P_0(x) = 1$\n$P_1(x) = x$\n$P_2(x) = \\frac{1}{2}(3x^2 - 1)$\n$P_3(x) = \\frac{1}{2}(5x^3 - 3x)$\n\nThe derivative of $P_3(x)$ is:\n$$P_3'(x) = \\frac{d}{dx} \\left( \\frac{1}{2}(5x^3 - 3x) \\right) = \\frac{1}{2}(15x^2 - 3)$$\nSetting $P_3'(x)=0$ to find the interior nodes:\n$$15x^2 - 3 = 0 \\implies x^2 = \\frac{3}{15} = \\frac{1}{5} \\implies x = \\pm \\frac{1}{\\sqrt{5}} = \\pm \\frac{\\sqrt{5}}{5}$$\nThe $N+1=4$ GLL nodes, ordered from smallest to largest, are:\n$$x_1 = -1, \\quad x_2 = -\\frac{\\sqrt{5}}{5}, \\quad x_3 = \\frac{\\sqrt{5}}{5}, \\quad x_4 = 1$$\n\nNext, we construct the modal-to-nodal interpolation matrix $V$, defined by $V_{jk} = P_k(x_j)$ for $j \\in \\{1, 2, 3, 4\\}$ and $k \\in \\{0, 1, 2, 3\\}$. We evaluate the first four Legendre polynomials at each GLL node.\nFor $x_1=-1$: $P_0(-1)=1$, $P_1(-1)=-1$, $P_2(-1)=1$, $P_3(-1)=-1$.\nFor $x_4=1$: $P_0(1)=1$, $P_1(1)=1$, $P_2(1)=1$, $P_3(1)=1$.\nFor $x_2=-\\frac{\\sqrt{5}}{5}$:\n$P_0(-\\frac{\\sqrt{5}}{5}) = 1$\n$P_1(-\\frac{\\sqrt{5}}{5}) = -\\frac{\\sqrt{5}}{5}$\n$P_2(-\\frac{\\sqrt{5}}{5}) = \\frac{1}{2}\\left(3\\left(-\\frac{\\sqrt{5}}{5}\\right)^2 - 1\\right) = \\frac{1}{2}\\left(\\frac{3}{5} - 1\\right) = -\\frac{1}{5}$\n$P_3(-\\frac{\\sqrt{5}}{5}) = \\frac{1}{2}\\left(5\\left(-\\frac{\\sqrt{5}}{5}\\right)^3 - 3\\left(-\\frac{\\sqrt{5}}{5}\\right)\\right) = \\frac{1}{2}\\left(-\\frac{1}{\\sqrt{5}} + \\frac{3}{\\sqrt{5}}\\right) = \\frac{1}{\\sqrt{5}} = \\frac{\\sqrt{5}}{5}$\nFor $x_3=\\frac{\\sqrt{5}}{5}$:\n$P_0(\\frac{\\sqrt{5}}{5}) = 1$\n$P_1(\\frac{\\sqrt{5}}{5}) = \\frac{\\sqrt{5}}{5}$\n$P_2(\\frac{\\sqrt{5}}{5}) = \\frac{1}{2}\\left(3\\left(\\frac{\\sqrt{5}}{5}\\right)^2 - 1\\right) = -\\frac{1}{5}$\n$P_3(\\frac{\\sqrt{5}}{5}) = \\frac{1}{2}\\left(5\\left(\\frac{\\sqrt{5}}{5}\\right)^3 - 3\\left(\\frac{\\sqrt{5}}{5}\\right)\\right) = \\frac{1}{2}\\left(\\frac{1}{\\sqrt{5}} - \\frac{3}{\\sqrt{5}}\\right) = -\\frac{1}{\\sqrt{5}} = -\\frac{\\sqrt{5}}{5}$\n\nThe resulting modal-to-nodal matrix $V$ is:\n$$V = \\begin{pmatrix} P_0(x_1) & P_1(x_1) & P_2(x_1) & P_3(x_1) \\\\ P_0(x_2) & P_1(x_2) & P_2(x_2) & P_3(x_2) \\\\ P_0(x_3) & P_1(x_3) & P_2(x_3) & P_3(x_3) \\\\ P_0(x_4) & P_1(x_4) & P_2(x_4) & P_3(x_4) \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 1 & -1 \\\\ 1 & -\\frac{\\sqrt{5}}{5} & -\\frac{1}{5} & \\frac{\\sqrt{5}}{5} \\\\ 1 & \\frac{\\sqrt{5}}{5} & -\\frac{1}{5} & -\\frac{\\sqrt{5}}{5} \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}$$\n\nFinally, we compute the pointwise interpolation error $E(0) = f(0) - p_3(0)$, where $p_3(x)$ is the degree-$3$ Lagrange interpolant of $f(x) = \\exp(x)$ at the GLL nodes. The value of the interpolant at $x=0$ is $p_3(0) = \\sum_{j=1}^{4} f(x_j) \\ell_j(0)$, where $\\ell_j(x)$ are the Lagrange basis polynomials. The weights $\\ell_j(0)$ are given by $\\ell_j(0) = \\prod_{k=1, k \\ne j}^{4} \\frac{0-x_k}{x_j-x_k}$.\n\nThe weights are calculated as follows:\n$\\ell_1(0) = \\frac{(-x_2)(-x_3)(-x_4)}{(x_1-x_2)(x_1-x_3)(x_1-x_4)} = \\frac{(\\frac{\\sqrt{5}}{5})(-\\frac{\\sqrt{5}}{5})(-1)}{(-1+\\frac{\\sqrt{5}}{5})(-1-\\frac{\\sqrt{5}}{5})(-1-1)} = \\frac{\\frac{1}{5}}{(1 - \\frac{1}{5})(-2)} = \\frac{\\frac{1}{5}}{(\\frac{4}{5})(-2)} = \\frac{1/5}{-8/5} = -\\frac{1}{8}$.\n\nDue to the symmetry of the nodes about $x=0$ ($x_1=-x_4, x_2=-x_3$), we can infer $\\ell_4(0) = \\ell_1(0) = -\\frac{1}{8}$. Let's confirm:\n$\\ell_4(0) = \\frac{(-x_1)(-x_2)(-x_3)}{(x_4-x_1)(x_4-x_2)(x_4-x_3)} = \\frac{(1)(\\frac{\\sqrt{5}}{5})(-\\frac{\\sqrt{5}}{5})}{(1-(-1))(1-(-\\frac{\\sqrt{5}}{5}))(1-\\frac{\\sqrt{5}}{5})} = \\frac{-1/5}{2(1-\\frac{1}{5})} = \\frac{-1/5}{2(4/5)} = \\frac{-1/5}{8/5} = -\\frac{1}{8}$.\n\n$\\ell_2(0) = \\frac{(-x_1)(-x_3)(-x_4)}{(x_2-x_1)(x_2-x_3)(x_2-x_4)} = \\frac{(1)(-\\frac{\\sqrt{5}}{5})(-1)}{(-\\frac{\\sqrt{5}}{5}-(-1))(-\\frac{\\sqrt{5}}{5}-\\frac{\\sqrt{5}}{5})(-\\frac{\\sqrt{5}}{5}-1)} = \\frac{\\frac{\\sqrt{5}}{5}}{(1-\\frac{\\sqrt{5}}{5})(-\\frac{2\\sqrt{5}}{5})(-1-\\frac{\\sqrt{5}}{5})} = \\frac{\\frac{\\sqrt{5}}{5}}{(1-\\frac{1}{5})(\\frac{2\\sqrt{5}}{5})} = \\frac{\\frac{\\sqrt{5}}{5}}{(\\frac{4}{5})(\\frac{2\\sqrt{5}}{5})} = \\frac{\\frac{\\sqrt{5}}{5}}{\\frac{8\\sqrt{5}}{25}} = \\frac{\\sqrt{5}}{5} \\frac{25}{8\\sqrt{5}} = \\frac{5}{8}$.\n\nAgain, due to symmetry, $\\ell_3(0) = \\ell_2(0) = \\frac{5}{8}$. The sum of the weights is $\\sum_{j=1}^{4} \\ell_j(0) = -\\frac{1}{8} + \\frac{5}{8} + \\frac{5}{8} - \\frac{1}{8} = \\frac{8}{8} = 1$, as expected.\n\nThe interpolated value at $x=0$ is:\n$$p_3(0) = f(x_1)\\ell_1(0) + f(x_2)\\ell_2(0) + f(x_3)\\ell_3(0) + f(x_4)\\ell_4(0)$$\n$$p_3(0) = \\exp(-1)\\left(-\\frac{1}{8}\\right) + \\exp\\left(-\\frac{\\sqrt{5}}{5}\\right)\\left(\\frac{5}{8}\\right) + \\exp\\left(\\frac{\\sqrt{5}}{5}\\right)\\left(\\frac{5}{8}\\right) + \\exp(1)\\left(-\\frac{1}{8}\\right)$$\nWe can group terms:\n$$p_3(0) = \\frac{5}{8} \\left( \\exp\\left(\\frac{\\sqrt{5}}{5}\\right) + \\exp\\left(-\\frac{\\sqrt{5}}{5}\\right) \\right) - \\frac{1}{8} \\left( \\exp(1) + \\exp(-1) \\right)$$\nUsing the identity $\\cosh(z) = \\frac{\\exp(z)+\\exp(-z)}{2}$:\n$$p_3(0) = \\frac{5}{8} \\left( 2\\cosh\\left(\\frac{\\sqrt{5}}{5}\\right) \\right) - \\frac{1}{8} \\left( 2\\cosh(1) \\right) = \\frac{5}{4}\\cosh\\left(\\frac{\\sqrt{5}}{5}\\right) - \\frac{1}{4}\\cosh(1)$$\n\nThe interpolation error at $x=0$ is $E(0) = f(0) - p_3(0)$. Since $f(0)=\\exp(0)=1$:\n$$E(0) = 1 - \\left( \\frac{5}{4}\\cosh\\left(\\frac{\\sqrt{5}}{5}\\right) - \\frac{1}{4}\\cosh(1) \\right) = 1 - \\frac{5}{4}\\cosh\\left(\\frac{\\sqrt{5}}{5}\\right) + \\frac{1}{4}\\cosh(1)$$",
            "answer": "$$\\boxed{1 + \\frac{1}{4}\\cosh(1) - \\frac{5}{4}\\cosh\\left(\\frac{\\sqrt{5}}{5}\\right)}$$"
        },
        {
            "introduction": "A significant challenge in using high-order polynomials is the Gibbs phenomenon, where spurious oscillations arise near discontinuities, degrading the quality of the approximation. This computational exercise introduces spectral filtering, a powerful and widely used technique to tame these oscillations and restore accuracy away from the jump. By implementing a filter and applying it to the Legendre series of a step function, you will gain practical insight into managing non-smooth features in solutions, a common scenario in the simulation of conservation laws .",
            "id": "3409037",
            "problem": "Consider the polynomial approximation of a piecewise constant function by Legendre polynomials within the context of spectral methods and Discontinuous Galerkin (DG) methods. Let $P_k(x)$ denote the $k$-th Legendre polynomial on the interval $[-1,1]$, normalized so that $P_k(1)=1$. The Legendre polynomials form an orthogonal basis of $L^2([-1,1])$ with respect to the unit weight. The $L^2$-orthogonal projection of a function $f(x)$ onto the space of polynomials of degree at most $N$ can be represented as a Legendre series truncated at degree $N$:\n$$\n\\Pi_N f(x) = \\sum_{k=0}^{N} a_k P_k(x),\n$$\nwhere the modal coefficients $a_k$ are given by\n$$\na_k = \\frac{2k+1}{2} \\int_{-1}^{1} f(x) P_k(x)\\,dx.\n$$\nIn spectral and Discontinuous Galerkin (DG) approximations, discontinuities in $f(x)$ yield oscillatory errors known as the Gibbs phenomenon. A common strategy to mitigate these oscillations is to apply a modal filter to the coefficients. An exponential filter of order $p$ and strength $\\alpha$ multiplies each modal coefficient $a_k$ by a damping factor\n$$\n\\sigma_k = \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^p\\right),\n$$\nproducing a filtered approximation\n$$\n\\Pi_N^{\\sigma} f(x) = \\sum_{k=0}^{N} \\sigma_k a_k P_k(x).\n$$\n\nYour task is to implement a program that:\n1. Computes the Legendre coefficients $a_k$ for the function $f(x) = \\mathrm{sign}(x)$, where $f(x) = -1$ for $x \\in [-1,0)$, $f(x) = 1$ for $x \\in (0,1]$, and $f(0)=0$ for definiteness.\n2. Constructs the truncated Legendre approximation $\\Pi_N f(x)$ and the exponentially filtered approximation $\\Pi_N^{\\sigma} f(x)$ for specified degrees $N$, filter orders $p$, and filter strengths $\\alpha$.\n3. Evaluates the pointwise absolute error $|f(x) - \\Pi_N^{\\sigma} f(x)|$ at specified evaluation points $x$ away from the discontinuity at $x=0$, and compares it to the unfiltered error $|f(x) - \\Pi_N f(x)|$ to assess the filter’s effect on the pointwise error.\n\nFundamental base to use:\n- Orthogonality and completeness of Legendre polynomials in $L^2([-1,1])$ with weight $1$.\n- Spectral projection in the Legendre basis via the inner product integral definition of $a_k$.\n- The definition and implementation of an exponential modal filter as described above.\n\nAlgorithmic requirements:\n- Compute $a_k$ using quadrature that is exact for polynomials: perform Gaussian quadrature separately on $[-1,0]$ and $[0,1]$ to evaluate $\\int_{-1}^{1} \\mathrm{sign}(x) P_k(x)\\,dx$ robustly, ensuring accuracy for each $k \\leq N$.\n- Evaluate $P_k(x)$ pointwise to build $\\Pi_N f(x)$ and $\\Pi_N^{\\sigma} f(x)$.\n- Use the exponential filter with specified $(\\alpha,p)$; for the unfiltered case use $p=0$ (so $\\sigma_k = 1$).\n\nTest suite:\nFor each case below, compute the requested metric, and aggregate all results into a single list for the final output.\n\n- Case $1$ (baseline, \"happy path\"): $N=16$, unfiltered ($p=0$, any $\\alpha$ is irrelevant), evaluation points $x \\in \\{-0.9,-0.5,-0.25,0.25,0.5,0.9\\}$. Output the mean absolute error $\\frac{1}{6}\\sum |f(x)-\\Pi_N f(x)|$ as a single float.\n\n- Case $2$ (filtered counterpart): $N=16$, filtered with $\\alpha=36$, $p=8$, same evaluation points. Output the mean absolute error $\\frac{1}{6}\\sum |f(x)-\\Pi_N^{\\sigma} f(x)|$ as a single float.\n\n- Case $3$ (improvement factor at moderate degree): $N=32$, compare unfiltered ($p=0$) versus filtered ($\\alpha=36$, $p=8$) on the same evaluation points. Output the improvement factor defined as the ratio of the unfiltered mean absolute error to the filtered mean absolute error (a single float).\n\n- Case $4$ (boundary condition near the discontinuity): $N=32$, evaluate at $x \\in \\{-10^{-3},10^{-3}\\}$ to probe behavior very near $x=0$. Output the improvement factor defined as the ratio of the unfiltered mean absolute error to the filtered mean absolute error (a single float). This tests the limitation of the filter in the immediate vicinity of the jump.\n\n- Case $5$ (higher degree behavior): $N=64$, filtered with $\\alpha=36$, $p=8$, evaluate at $x \\in \\{-0.9,-0.5,-0.25,0.25,0.5,0.9\\}$. Output the maximum absolute error $\\max |f(x)-\\Pi_N^{\\sigma} f(x)|$ over the set as a single float.\n\nAll evaluation points are dimensionless and require no physical units.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, for example, $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is a Python float. No other text should be printed.",
            "solution": "The problem requires the implementation and analysis of a polynomial approximation for the piecewise constant function $f(x) = \\mathrm{sign}(x)$ on the interval $[-1,1]$. The approximation is based on a truncated Legendre series, and the effect of an exponential modal filter on the approximation accuracy is investigated. The solution involves computing Legendre coefficients, constructing the standard and filtered polynomial approximations, and evaluating their pointwise errors against the true function.\n\nThe core of the methodology rests on the principles of spectral methods and Fourier-like series expansions. A function $f(x)$ in the Hilbert space $L^2([-1,1])$ can be represented by its series expansion in an orthogonal basis. For the basis of Legendre polynomials $\\{P_k(x)\\}_{k=0}^{\\infty}$, which are orthogonal with respect to the unit weight function on $[-1,1]$, this expansion is given by:\n$$\nf(x) = \\sum_{k=0}^{\\infty} a_k P_k(x)\n$$\nThe modal coefficients $a_k$ are determined by the orthogonal projection of $f(x)$ onto each basis function $P_k(x)$:\n$$\na_k = \\frac{\\langle f, P_k \\rangle}{\\langle P_k, P_k \\rangle} = \\frac{\\int_{-1}^{1} f(x) P_k(x) \\,dx}{\\int_{-1}^{1} P_k(x)^2 \\,dx}\n$$\nGiven the standard normalization where $\\int_{-1}^{1} P_k(x)^2 \\,dx = \\frac{2}{2k+1}$, the coefficient formula simplifies to the one provided in the problem statement:\n$$\na_k = \\frac{2k+1}{2} \\int_{-1}^{1} f(x) P_k(x)\\,dx\n$$\nIn practice, the infinite series is truncated at a finite degree $N$, yielding the polynomial approximation $\\Pi_N f(x) = \\sum_{k=0}^{N} a_k P_k(x)$.\n\nFor the specific function $f(x) = \\mathrm{sign}(x)$, which is an odd function, its representation in the Legendre basis will only contain odd-indexed Legendre polynomials, which are themselves odd functions. Thus, all coefficients $a_k$ for even indices $k$ must be zero. The non-zero coefficients for odd $k$ are calculated by evaluating the defining integral. The problem specifies a robust numerical approach for this integral due to the discontinuity of $f(x)$ at $x=0$. The integral is split into two parts:\n$$\n\\int_{-1}^{1} \\mathrm{sign}(x) P_k(x)\\,dx = \\int_{-1}^{0} (-1) P_k(x)\\,dx + \\int_{0}^{1} (1) P_k(x)\\,dx\n$$\nEach integral on the sub-intervals $[-1,0]$ and $[0,1]$ involves a smooth integrand (a polynomial). These integrals are computed in the implementation using high-order Gauss-Legendre quadrature, which is exact for polynomials up to a certain degree. The number of quadrature points $M$ is chosen such that the quadrature is exact for polynomials of degree up to $N$, i.e., $2M-1 \\ge N$.\n\nWhen approximating discontinuous functions like $\\mathrm{sign}(x)$, the truncated series $\\Pi_N f(x)$ exhibits spurious oscillations near the discontinuity, a phenomenon known as the Gibbs phenomenon. To mitigate these oscillations, a filter is applied to the modal coefficients. The problem specifies an exponential filter, where each coefficient $a_k$ is multiplied by a damping factor $\\sigma_k$:\n$$\n\\sigma_k = \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^p\\right)\n$$\nThe parameters $\\alpha > 0$ and $p \\ge 2$ control the strength and order of the filter, respectively. Higher-order modes (large $k$) are damped more heavily, smoothing the approximation. The filtered approximation is then:\n$$\n\\Pi_N^{\\sigma} f(x) = \\sum_{k=0}^{N} \\sigma_k a_k P_k(x)\n$$\nThe unfiltered case is specified to correspond to $p=0$, which, by problem directive, implies $\\sigma_k = 1$ for all $k$.\n\nThe final step is to evaluate the pointwise accuracy of both the unfiltered approximation $\\Pi_N f(x)$ and the filtered approximation $\\Pi_N^{\\sigma} f(x)$. For a given set of evaluation points $\\{x_j\\}$, the approximations are computed by summing the series. The series evaluation is performed efficiently using the Clenshaw algorithm, as implemented in `numpy.polynomial.legendre.legval`. The absolute error $|f(x_j) - \\Pi_N f(x_j)|$ and $|f(x_j) - \\Pi_N^{\\sigma} f(x_j)|$ are then calculated. The test cases require computing specific metrics based on these errors, such as the mean absolute error, maximum absolute error, and the ratio of unfiltered to filtered error, to quantify the filter's performance.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Determine the required polynomial degree $N$.\n2.  Compute the Legendre coefficients $a_k$ for $k=0, \\dots, N$ using split Gauss-Legendre quadrature. These coefficients are cached to avoid redundant calculations.\n3.  For the filtered and unfiltered approximations, construct the vector of series coefficients. For the unfiltered case, this is simply $\\{a_k\\}$. For the filtered case, this is $\\{ \\sigma_k a_k \\}$.\n4.  At the specified evaluation points $x_j$, compute the values of the polynomial approximations.\n5.  Calculate the pointwise absolute errors with respect to the true function $\\mathrm{sign}(x_j)$.\n6.  Compute and store the final metric (mean error, max error, or error ratio) as required by the specific test case.\nThe results from all test cases are aggregated into a single list for the final output.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_legendre, eval_legendre\nfrom numpy.polynomial.legendre import legval\n\ndef solve():\n    \"\"\"\n    Solves the polynomial approximation problem by computing Legendre coefficients,\n    applying a modal filter, evaluating errors, and reporting the specified metrics.\n    \"\"\"\n\n    # Cache for Legendre coefficients to avoid recomputation for the same degree N.\n    coeffs_cache = {}\n\n    def get_legendre_coeffs_sign_x(N: int) -> np.ndarray:\n        \"\"\"\n        Computes the Legendre series coefficients for f(x) = sign(x) up to degree N.\n        The coefficients are computed using Gauss-Legendre quadrature on sub-intervals\n        [-1,0] and [0,1] to handle the discontinuity at x=0.\n        \"\"\"\n        if N in coeffs_cache:\n            return coeffs_cache[N]\n\n        coeffs = np.zeros(N + 1)\n        \n        # An M-point Gauss rule is exact for polynomials of degree 2M-1.\n        # We integrate f(x)*P_k(x), where f is piecewise constant. So we integrate P_k(x) on\n        # subdomains, which is a polynomial of degree k. To be exact for all k = N, we need 2M-1 >= N.\n        num_points = int(np.ceil((N + 1) / 2.0))\n        nodes, weights = roots_legendre(num_points)\n\n        # Map nodes and weights for integral over [0, 1]\n        nodes_01 = 0.5 * (nodes + 1)\n        weights_01 = 0.5 * weights\n\n        # Map nodes and weights for integral over [-1, 0]\n        nodes_m10 = 0.5 * (nodes - 1)\n        weights_m10 = 0.5 * weights\n\n        for k in range(N + 1):\n            # The function sign(x)*P_k(x) is odd if k is even, so its integral is 0.\n            # Its integral is non-zero only if k is odd.\n            if k % 2 == 0:\n                coeffs[k] = 0.0\n                continue\n\n            # Integral of P_k(x) over [0, 1]\n            pk_vals_01 = eval_legendre(k, nodes_01)\n            integral_01 = np.sum(weights_01 * pk_vals_01)\n\n            # Integral of P_k(x) over [-1, 0]\n            pk_vals_m10 = eval_legendre(k, nodes_m10)\n            integral_m10 = np.sum(weights_m10 * pk_vals_m10)\n            \n            # Integral of sign(x)*P_k(x) over [-1, 1] is\n            # = integral[-1,0] (-1)*P_k(x) dx + integral[0,1] (1)*P_k(x) dx\n            integral_total = -integral_m10 + integral_01\n            \n            coeffs[k] = (2 * k + 1) / 2.0 * integral_total\n        \n        coeffs_cache[N] = coeffs\n        return coeffs\n\n    def calculate_approximation_error(N, p, alpha, x_eval, unfiltered):\n        \"\"\"\n        Calculates the pointwise absolute error of the (filtered) approximation.\n        \"\"\"\n        a_k = get_legendre_coeffs_sign_x(N)\n        \n        if unfiltered:\n            # Per problem statement, sigma_k = 1 for the unfiltered case.\n            coeffs_to_eval = a_k\n        else:\n            k_indices = np.arange(N + 1, dtype=float)\n            # Handle N=0 case to avoid division by zero.\n            if N  0:\n                eta = k_indices / N\n            else:\n                eta = np.zeros_like(k_indices)\n            sigma = np.exp(-alpha * eta**p)\n            coeffs_to_eval = a_k * sigma\n\n        y_approx = legval(x_eval, coeffs_to_eval)\n        y_true = np.sign(x_eval)\n        \n        return np.abs(y_true - y_approx)\n\n    results = []\n    \n    # Common evaluation points for Cases 1, 2, 3, 5\n    x_eval_common = np.array([-0.9, -0.5, -0.25, 0.25, 0.5, 0.9])\n\n    # Case 1: N=16, unfiltered, mean absolute error\n    N1 = 16\n    errors1 = calculate_approximation_error(N1, p=0, alpha=0, x_eval=x_eval_common, unfiltered=True)\n    result1 = np.mean(errors1)\n    results.append(result1)\n    \n    # Case 2: N=16, filtered, mean absolute error\n    N2, p2, alpha2 = 16, 8, 36\n    errors2 = calculate_approximation_error(N2, p=p2, alpha=alpha2, x_eval=x_eval_common, unfiltered=False)\n    result2 = np.mean(errors2)\n    results.append(result2)\n    \n    # Case 3: N=32, improvement factor\n    N3, p3, alpha3 = 32, 8, 36\n    unfiltered_errors3 = calculate_approximation_error(N3, p=0, alpha=0, x_eval=x_eval_common, unfiltered=True)\n    filtered_errors3 = calculate_approximation_error(N3, p=p3, alpha=alpha3, x_eval=x_eval_common, unfiltered=False)\n    mean_err_unfiltered3 = np.mean(unfiltered_errors3)\n    mean_err_filtered3 = np.mean(filtered_errors3)\n    result3 = mean_err_unfiltered3 / mean_err_filtered3\n    results.append(result3)\n\n    # Case 4: N=32, improvement factor near discontinuity\n    N4, p4, alpha4 = 32, 8, 36\n    x_eval4 = np.array([-1e-3, 1e-3])\n    unfiltered_errors4 = calculate_approximation_error(N4, p=0, alpha=0, x_eval=x_eval4, unfiltered=True)\n    filtered_errors4 = calculate_approximation_error(N4, p=p4, alpha=alpha4, x_eval=x_eval4, unfiltered=False)\n    mean_err_unfiltered4 = np.mean(unfiltered_errors4)\n    mean_err_filtered4 = np.mean(filtered_errors4)\n    result4 = mean_err_unfiltered4 / mean_err_filtered4\n    results.append(result4)\n    \n    # Case 5: N=64, filtered, max absolute error\n    N5, p5, alpha5 = 64, 8, 36\n    errors5 = calculate_approximation_error(N5, p=p5, alpha=alpha5, x_eval=x_eval_common, unfiltered=False)\n    result5 = np.max(errors5)\n    results.append(result5)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}