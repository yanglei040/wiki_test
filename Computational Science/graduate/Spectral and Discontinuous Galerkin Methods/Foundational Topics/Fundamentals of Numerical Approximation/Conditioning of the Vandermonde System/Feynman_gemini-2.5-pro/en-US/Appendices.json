{
    "hands_on_practices": [
        {
            "introduction": "The most intuitive choice for interpolation points is often an equispaced grid, yet this choice can lead to significant numerical issues. This practice provides a direct, computational experience of the severe instability that arises when combining these simple nodes with a standard monomial basis. By calculating the condition number of the resulting Vandermonde matrix, you will witness firsthand its exponential growth and understand why this approach is untenable for high-degree polynomials. ",
            "id": "3256284",
            "problem": "Consider the derivation of Newton–Cotes quadrature rules, where the weights are determined by enforcing exactness on a polynomial basis. Let there be $n$ equally spaced nodes on an interval, denoted by $x_0, x_1, \\dots, x_{n-1}$. To construct weights that integrate polynomials exactly up to degree $n-1$, one uses the linear system that matches moments of monomials. The coefficient matrix in this linear system is the Vandermonde matrix $V \\in \\mathbb{R}^{n \\times n}$ with entries $V_{i,j} = x_i^j$ for $i = 0,1,\\dots,n-1$ and $j = 0,1,\\dots,n-1$. The numerical sensitivity of solving such a linear system is governed by the condition number of $V$ in the matrix $2$-norm, defined by $\\kappa_2(V) = \\|V\\|_2 \\|V^{-1}\\|_2$, which can be computed as the ratio of the largest singular value to the smallest singular value of $V$.\n\nStarting from the fundamental fact that Newton–Cotes weights are obtained by solving a linear system whose coefficient matrix is a Vandermonde matrix constructed from equispaced nodes, and the definition of the matrix $2$-norm condition number via singular values, investigate how the conditioning of this Vandermonde matrix depends on the number of nodes $n$ and on the interval scaling. Specifically, compare two intervals:\n(1) the unit interval $[0,1]$ with nodes $x_i = \\frac{i}{n-1}$ for $n > 1$ and $x_0 = 0$ for $n = 1$, and\n(2) the symmetric interval $[-1,1]$ with nodes $x_i = -1 + \\frac{2 i}{n-1}$ for $n > 1$ and $x_0 = 0$ for $n = 1$.\nFor each $n$, construct the corresponding Vandermonde matrix $V$ using the monomial basis $1, x, x^2, \\dots, x^{n-1}$, compute $\\kappa_2(V)$ via singular value decomposition, and report the base-$10$ logarithm $\\log_{10}(\\kappa_2(V))$ for both intervals. Additionally, report the difference $\\log_{10}(\\kappa_2(V \\text{ on } [0,1])) - \\log_{10}(\\kappa_2(V \\text{ on } [-1,1]))$ to quantify the effect of interval scaling on conditioning.\n\nYour task is to write a complete, runnable program that performs these computations in double precision for the following test suite of node counts $n$: $n \\in \\{1,2,3,5,10,15,20\\}$. This test suite includes a boundary case $n=1$, small cases, and larger cases where conditioning becomes severe. For each $n$ in the test suite, your program must output a triple of floats $[\\log_{10}(\\kappa_2(V_{[0,1]})), \\log_{10}(\\kappa_2(V_{[-1,1]})), \\log_{10}(\\kappa_2(V_{[0,1]})) - \\log_{10}(\\kappa_2(V_{[-1,1]}))]$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one $n$ in the test suite and is itself a comma-separated triple enclosed in square brackets. For example, the output should look like $[[a,b,c],[d,e,f],\\dots]$ with no spaces.\n\nNo physical units or angles are involved. All answers must be floats as specified, and the numerical values should be computed using the matrix $2$-norm condition number via singular values as described.",
            "solution": "The problem requires an investigation into the numerical conditioning of Vandermonde matrices constructed from equispaced nodes on two distinct intervals: the unit interval $[0,1]$ and the symmetric interval $[-1,1]$. This analysis is fundamental to understanding the numerical stability of methods for deriving Newton-Cotes quadrature weights, which rely on solving a linear system involving such a matrix.\n\nThe derivation of weights for an $n$-point quadrature rule that is exact for all polynomials up to degree $n-1$ requires solving the linear system $Vw = \\mathbf{m}$. In this system, $V$ is an $n \\times n$ Vandermonde matrix, $w$ is the vector of unknown weights, and $\\mathbf{m}$ is the vector of moments of the monomial basis functions, with entries $m_j = \\int_a^b x^j dx$ for $j=0, 1, \\dots, n-1$. The Vandermonde matrix $V$ is constructed from the $n$ quadrature nodes $x_0, x_1, \\dots, x_{n-1}$, with entries defined as $V_{i,j} = x_i^j$ for $i,j \\in \\{0, \\dots, n-1\\}$.\n\nThe numerical stability of solving $Vw = \\mathbf{m}$ is dictated by the condition number of the matrix $V$. A large condition number signifies that small errors in the input data (e.g., the moment vector $\\mathbf{m}$) can lead to large errors in the computed solution (the weights $w$). The matrix $2$-norm condition number, denoted $\\kappa_2(V)$, is defined as $\\kappa_2(V) = \\|V\\|_2 \\|V^{-1}\\|_2$. It is calculated as the ratio of the largest singular value ($\\sigma_{\\max}$) to the smallest singular value ($\\sigma_{\\min}$) of the matrix:\n$$ \\kappa_2(V) = \\frac{\\sigma_{\\max}(V)}{\\sigma_{\\min}(V)} $$\nSingular values are computed via Singular Value Decomposition (SVD). For Vandermonde matrices constructed from equispaced points, $\\kappa_2(V)$ is known to grow exponentially with the number of nodes $n$, leading to extreme ill-conditioning. To manage the large magnitude of these numbers, we analyze their base-$10$ logarithm, $\\log_{10}(\\kappa_2(V))$.\n\nThe procedure to address the problem is as follows:\nFor each number of nodes $n$ in the test suite $\\{1, 2, 3, 5, 10, 15, 20\\}$:\n$1$. The special case $n=1$ is handled first. The problem specifies the node $x_0 = 0$ for both intervals. This results in a $1 \\times 1$ Vandermonde matrix $V = [x_0^0] = [1]$. The singular value is $1$, so $\\kappa_2(V) = 1$ and $\\log_{10}(\\kappa_2(V)) = 0$ for both intervals.\n\n$2$. For cases where $n > 1$:\n   a. **Interval $[0,1]$**: A set of $n$ equispaced nodes $\\{x_i\\}$ is generated using the formula $x_i = \\frac{i}{n-1}$ for $i=0, 1, \\dots, n-1$. The corresponding Vandermonde matrix $V_{[0,1]}$ is constructed.\n   b. **Interval $[-1,1]$**: A second set of $n$ equispaced nodes $\\{x_i\\}$ is generated using the formula $x_i = -1 + \\frac{2i}{n-1}$ for $i=0, 1, \\dots, n-1$. The corresponding Vandermonde matrix $V_{[-1,1]}$ is constructed.\n   c. For both matrices, the $2$-norm condition number is computed using SVD-based methods.\n   d. The base-$10$ logarithm of each condition number is calculated.\n   e. The difference, $\\log_{10}(\\kappa_2(V_{[0,1]})) - \\log_{10}(\\kappa_2(V_{[-1,1]}))$, is computed to quantify the impact of interval choice on conditioning.\n\nThe choice of interval has a profound effect on conditioning. On $[0,1]$, all nodes are non-negative. The basis functions $x^j$ and $x^{j+1}$ behave similarly over this interval, resulting in column vectors of the Vandermonde matrix that are nearly linearly dependent. This near-collinearity is the source of the severe ill-conditioning. In contrast, the interval $[-1,1]$ is symmetric about the origin. This symmetry induces a degree of orthogonality between the column vectors of the Vandermonde matrix (e.g., for odd and even powers $j$), which significantly mitigates the ill-conditioning. The following program implements this analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the log10 of the 2-norm condition number for Vandermonde matrices\n    on two different intervals for a given set of node counts.\n    \"\"\"\n    \n    # Test suite of node counts as specified in the problem statement.\n    test_cases_n = [1, 2, 3, 5, 10, 15, 20]\n    \n    results = []\n    \n    for n in test_cases_n:\n        # The problem statement defines V_{i,j} = x_i^j, which corresponds to\n        # increasing powers in the numpy.vander function.\n        # This is handled by setting the 'increasing' parameter to True.\n        \n        # We perform calculations in double precision as requested.\n        # numpy's default float is float64 (double precision).\n        dtype = np.float64\n        \n        # Handle the special case n=1 as per the problem description.\n        if n == 1:\n            # For n=1, x0=0 for both intervals. V=[[1]].\n            # The condition number of a 1x1 non-zero matrix is 1.\n            # log10(1) = 0.\n            log_kappa_01 = 0.0\n            log_kappa_m11 = 0.0\n        else:\n            # --- Interval [0, 1] ---\n            # Generate n equispaced nodes from 0 to 1.\n            nodes_01 = np.linspace(0.0, 1.0, n, dtype=dtype)\n            # Construct the Vandermonde matrix.\n            V_01 = np.vander(nodes_01, N=n, increasing=True)\n            # Compute the 2-norm condition number.\n            kappa_01 = np.linalg.cond(V_01, p=2)\n            # Compute the base-10 logarithm.\n            log_kappa_01 = np.log10(kappa_01)\n            \n            # --- Interval [-1, 1] ---\n            # Generate n equispaced nodes from -1 to 1.\n            nodes_m11 = np.linspace(-1.0, 1.0, n, dtype=dtype)\n            # Construct the Vandermonde matrix.\n            V_m11 = np.vander(nodes_m11, N=n, increasing=True)\n            # Compute the 2-norm condition number.\n            kappa_m11 = np.linalg.cond(V_m11, p=2)\n            # Compute the base-10 logarithm.\n            log_kappa_m11 = np.log10(kappa_m11)\n\n        # Calculate the difference in log-condition numbers.\n        diff = log_kappa_01 - log_kappa_m11\n        \n        # Store the triple of results for this n.\n        results.append([log_kappa_01, log_kappa_m11, diff])\n        \n    # Format the final output string exactly as required: [[a,b,c],[d,e,f],...].\n    # No spaces are permitted in the final output string.\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n    \n    # Print the single-line result to standard output.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "After observing the dramatic ill-conditioning of the Vandermonde matrix, we now seek a deeper, theoretical explanation for this behavior. This exercise guides you to connect the matrix condition number, $\\kappa_{2}(V)$, to the eigenvalues of its Gram matrix, $G = V^{\\top} V$, which directly quantifies the non-orthogonality of the basis vectors. This analysis reveals precisely how the near-linear dependence among columns of the Vandermonde matrix translates into numerical instability. ",
            "id": "3372894",
            "problem": "In high-order spectral and discontinuous Galerkin methods, nodal representations of polynomials of degree at most $n-1$ on distinct nodes $\\{x_i\\}_{i=1}^{n}$ lead to a Vandermonde matrix $V \\in \\mathbb{R}^{n \\times n}$ defined by columns that evaluate a chosen polynomial basis at the nodes. Consider such an invertible Vandermonde matrix $V$ with Gram matrix $G = V^{\\top} V$. The spectral condition number in the Euclidean norm is defined by $\\kappa_{2}(V) = \\|V\\|_{2} \\,\\|V^{-1}\\|_{2}$, where $\\|\\cdot\\|_{2}$ denotes the spectral norm induced by the Euclidean inner product.\n\nStarting from the definitions of singular values, spectral norm, and the Gram matrix, carry out the following:\n\n- Derive an expression for $\\kappa_{2}(V)$ in terms of the eigenvalues of $G$.\n- Suppose, in addition, that the columns of $V$ have been scaled to unit Euclidean norm and that the pairwise inner products between distinct columns are uniformly bounded in magnitude by a constant $\\varepsilon \\in [0,1)$, i.e., if $v_i$ denotes the $i$-th column then $v_i^{\\top} v_i = 1$ for all $i$ and $\\max_{i \\neq j} |v_i^{\\top} v_j| \\le \\varepsilon$. Use only standard spectral facts to analyze how this near-orthogonality assumption constrains the spectrum of $G$, and deduce a bound on $\\kappa_{2}(V)$ that depends only on $n$ and $\\varepsilon$.\n\nGive your final answer as the single, closed-form analytic expression for the bound on $\\kappa_{2}(V)$ you obtain under the near-orthogonality assumption. No numerical evaluation is required.",
            "solution": "We begin from core definitions in numerical linear algebra. For any real matrix $V \\in \\mathbb{R}^{n \\times n}$, the spectral norm is $\\|V\\|_{2} = \\sigma_{\\max}(V)$, the largest singular value of $V$, and if $V$ is invertible, then $\\|V^{-1}\\|_{2} = \\sigma_{\\max}(V^{-1}) = 1/\\sigma_{\\min}(V)$, where $\\sigma_{\\min}(V)$ is the smallest singular value of $V$. Therefore,\n$$\n\\kappa_{2}(V) \\;=\\; \\|V\\|_{2}\\,\\|V^{-1}\\|_{2} \\;=\\; \\frac{\\sigma_{\\max}(V)}{\\sigma_{\\min}(V)}.\n$$\nThe singular values of $V$ are nonnegative square roots of the eigenvalues of the positive definite Gram matrix $G = V^{\\top} V$. Concretely, if $\\lambda_{1}(G) \\ge \\lambda_{2}(G) \\ge \\cdots \\ge \\lambda_{n}(G) > 0$ are the eigenvalues of $G$, then\n$$\n\\sigma_{i}(V) \\;=\\; \\sqrt{\\lambda_{i}(G)} \\quad \\text{for each } i,\n$$\nhence\n$$\n\\sigma_{\\max}(V) \\;=\\; \\sqrt{\\lambda_{\\max}(G)}, \\qquad \\sigma_{\\min}(V) \\;=\\; \\sqrt{\\lambda_{\\min}(G)}.\n$$\nIt follows immediately that\n$$\n\\kappa_{2}(V) \\;=\\; \\frac{\\sqrt{\\lambda_{\\max}(G)}}{\\sqrt{\\lambda_{\\min}(G)}} \\;=\\; \\sqrt{\\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)}}.\n$$\n\nWe now incorporate the near-orthogonality assumption. Let $v_{1},\\dots,v_{n}$ be the columns of $V$, scaled so that $v_{i}^{\\top} v_{i} = 1$ for all $i$, with pairwise coherences bounded by $\\varepsilon \\in [0,1)$:\n$$\n\\max_{i \\neq j} |v_{i}^{\\top} v_{j}| \\;\\le\\; \\varepsilon.\n$$\nUnder these assumptions, the Gram matrix $G = V^{\\top} V$ has the structure of a Hermitian matrix with diagonal entries equal to $1$ and off-diagonal entries bounded in magnitude by $\\varepsilon$:\n$$\nG_{ii} \\;=\\; 1, \\qquad |G_{ij}| \\;\\le\\; \\varepsilon \\quad \\text{for } i \\neq j.\n$$\nWe use the Gershgorin circle theorem to constrain the eigenvalues of $G$. For each row $i$, the Gershgorin disc is centered at $G_{ii} = 1$ with radius\n$$\nR_{i} \\;=\\; \\sum_{j \\neq i} |G_{ij}| \\;\\le\\; (n-1)\\,\\varepsilon.\n$$\nTherefore, every eigenvalue $\\lambda(G)$ of $G$ lies in the union of these discs, which implies the crude but uniform bounds\n$$\n\\lambda_{\\min}(G) \\;\\ge\\; 1 - (n-1)\\varepsilon, \\qquad \\lambda_{\\max}(G) \\;\\le\\; 1 + (n-1)\\varepsilon.\n$$\nBecause $G$ is positive definite, we require $1 - (n-1)\\varepsilon > 0$ for the lower bound to be meaningful; this is ensured if $(n-1)\\varepsilon < 1$. Combining these bounds with the earlier identity for $\\kappa_{2}(V)$ yields\n$$\n\\kappa_{2}(V) \\;=\\; \\sqrt{\\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)}} \\;\\le\\; \\sqrt{\\frac{1 + (n-1)\\varepsilon}{1 - (n-1)\\varepsilon}}.\n$$\n\nThis expression quantifies the effect of near-orthogonality: as $\\varepsilon \\to 0$, the Gram matrix $G$ approaches the identity, so its eigenvalues cluster near $1$, and the condition number bound approaches $1$. Conversely, increasing $\\varepsilon$ inflates the spread between the largest and smallest eigenvalues of $G$, thereby worsening the condition number; the sensitivity grows with $n$ through the factor $(n-1)\\varepsilon$.\n\nThe requested final analytic expression is the bound on $\\kappa_{2}(V)$ derived above.",
            "answer": "$$\\boxed{\\sqrt{\\frac{1+(n-1)\\varepsilon}{\\,1-(n-1)\\varepsilon\\,}}}$$"
        },
        {
            "introduction": "Identifying non-orthogonality as the culprit of ill-conditioning naturally points to a powerful solution: employing an orthogonal basis. This exercise demonstrates the cornerstone of stable spectral methods by guiding you through the construction of an ideal preconditioner. You will see how a change of basis to orthonormal Legendre polynomials, combined with strategic Gauss–Legendre quadrature nodes, transforms the ill-conditioned monomial system into a perfectly conditioned one with $\\kappa_{2}=1$. ",
            "id": "3372834",
            "problem": "Consider a one-dimensional element on the interval $[-1,1]$ in a nodal spectral method or a Discontinuous Galerkin (DG) discretization. Let $N \\in \\mathbb{N}$ be fixed and let $\\{x_{i}\\}_{i=1}^{N}$ denote the $N$ Gauss–Legendre nodes, that is, the $N$ distinct roots of the Legendre polynomial of degree $N$. Let $\\{w_{i}\\}_{i=1}^{N}$ be the associated Gauss–Legendre quadrature weights. Define the monomial Vandermonde matrix $V \\in \\mathbb{R}^{N \\times N}$ by\n$$\nV_{i,k} = x_{i}^{k}, \\quad 1 \\leq i \\leq N, \\quad 0 \\leq k \\leq N-1.\n$$\nLet $\\{p_{j}\\}_{j=0}^{N-1}$ be the Legendre polynomials on $[-1,1]$ normalized to be orthonormal with respect to the standard $L^{2}([-1,1])$ inner product with unit weight, that is,\n$$\n\\int_{-1}^{1} p_{j}(x)\\,p_{k}(x)\\,\\mathrm{d}x = \\delta_{jk}, \\quad 0 \\leq j,k \\leq N-1.\n$$\nDefine the evaluation matrix $\\Phi \\in \\mathbb{R}^{N \\times N}$ by\n$$\n\\Phi_{i,j} = p_{j}(x_{i}), \\quad 1 \\leq i \\leq N, \\quad 0 \\leq j \\leq N-1.\n$$\nThere exists a change-of-basis matrix $C \\in \\mathbb{R}^{N \\times N}$ from the monomial basis to the orthonormal polynomial basis such that $\\Phi = V C$. Also consider the diagonal matrix of quadrature weights $W = \\mathrm{diag}(w_{1},\\dots,w_{N})$ and its positive definite square root $L = W^{1/2} = \\mathrm{diag}(\\sqrt{w_{1}},\\dots,\\sqrt{w_{N}})$.\n\nStarting only from the definitions above and the basic properties of orthonormal polynomials and Gauss–Legendre quadrature, derive a preconditioner based on projection to the orthonormal polynomial basis that transforms the monomial Vandermonde matrix into a near-orthogonal evaluation matrix. In particular, construct left and right preconditioners built from $L$ and $C$ such that the preconditioned matrix has orthonormal columns in the Euclidean inner product, and use this to rigorously analyze the effectiveness of this preconditioning on the $2$-norm condition number $\\kappa_{2}$.\n\nYour task is to compute the exact value of the $2$-norm condition number $\\kappa_{2}$ of the fully preconditioned matrix when the polynomial degree is $N-1$ and the nodes are the $N$ Gauss–Legendre points with the associated quadrature weights. Express your final answer as an exact number. No rounding is required.",
            "solution": "The objective is to derive a preconditioning strategy for the monomial Vandermonde matrix $V$ and to compute the $2$-norm condition number of the resulting preconditioned matrix. The preconditioning should be constructed using the provided matrices $L$ and $C$.\n\nLet the preconditioned matrix be denoted by $A$. A general two-sided preconditioning of a matrix $V$ takes the form $A = P_L V P_R$, where $P_L$ and $P_R$ are the left and right preconditioners, respectively. The problem specifies that these preconditioners are to be constructed from the change-of-basis matrix $C$ and the scaled quadrature weight matrix $L = W^{1/2}$.\n\nThe problem states that there exists a change-of-basis matrix $C \\in \\mathbb{R}^{N \\times N}$ from the monomial basis $\\{x^k\\}_{k=0}^{N-1}$ to the orthonormal polynomial basis $\\{p_j\\}_{j=0}^{N-1}$ such that $\\Phi = V C$. This relation connects the ill-conditioned monomial Vandermonde matrix $V$ to the better-conditioned evaluation matrix $\\Phi$ of the orthonormal polynomials. A natural choice for the right preconditioner is $P_R = C$, as this effectively changes the basis of the problem from monomials to the orthonormal Legendre polynomials.\n\nApplying this right preconditioner, we get an intermediate matrix $V C = \\Phi$. The columns of $\\Phi$ are the values of the orthonormal polynomials $\\{p_j\\}_{j=0}^{N-1}$ evaluated at the Gauss-Legendre nodes $\\{x_i\\}_{i=1}^{N}$. The problem requires that the final preconditioned matrix has columns that are orthonormal in the Euclidean inner product. Let's examine the columns of $\\Phi$. The inner product of the $j$-th and $k$-th columns of $\\Phi$ is given by:\n$$\n(\\Phi_{:,j})^T (\\Phi_{:,k}) = \\sum_{i=1}^{N} \\Phi_{ij} \\Phi_{ik} = \\sum_{i=1}^{N} p_j(x_i) p_k(x_i)\n$$\nThis expression is not generally equal to the Kronecker delta $\\delta_{jk}$. However, we can use the fundamental property of Gauss-Legendre quadrature. The quadrature rule, using $N$ nodes $\\{x_i\\}$ and weights $\\{w_i\\}$, is exact for any polynomial $f(x)$ of degree at most $2N-1$. That is,\n$$\n\\int_{-1}^{1} f(x)\\,\\mathrm{d}x = \\sum_{i=1}^{N} w_i f(x_i)\n$$\nThe polynomials $\\{p_j(x)\\}_{j=0}^{N-1}$ are given to be orthonormal on $[-1,1]$ with a unit weight function:\n$$\n\\int_{-1}^{1} p_j(x) p_k(x)\\,\\mathrm{d}x = \\delta_{jk}\n$$\nConsider the function $f(x) = p_j(x) p_k(x)$. The degree of this polynomial is $j+k$. Since $0 \\le j, k \\le N-1$, the maximum degree is $(N-1) + (N-1) = 2N-2$. As $2N-2 \\le 2N-1$, the Gauss-Legendre quadrature rule is exact for $p_j(x) p_k(x)$. Therefore, we can write:\n$$\n\\delta_{jk} = \\int_{-1}^{1} p_j(x) p_k(x)\\,\\mathrm{d}x = \\sum_{i=1}^{N} w_i p_j(x_i) p_k(x_i)\n$$\nLet us express this result in matrix form. The sum on the right side is the $(j,k)$-th entry of the matrix product $\\Phi^T W \\Phi$, where $W = \\mathrm{diag}(w_1, \\dots, w_N)$. Specifically:\n$$\n(\\Phi^T W \\Phi)_{jk} = \\sum_{l=1}^{N} (\\Phi^T)_{jl} (W \\Phi)_{lk} = \\sum_{l=1}^{N} \\Phi_{lj} \\sum_{m=1}^{N} W_{lm} \\Phi_{mk} = \\sum_{l=1}^{N} \\Phi_{lj} (w_l \\Phi_{lk}) = \\sum_{l=1}^{N} w_l p_j(x_l) p_k(x_l)\n$$\nComparing with the quadrature result, we find:\n$$\n\\Phi^T W \\Phi = I\n$$\nwhere $I$ is the $N \\times N$ identity matrix. This shows that the columns of $\\Phi$ are orthogonal with respect to the inner product weighted by the matrix $W$.\n\nThe problem demands that the columns of the preconditioned matrix $A$ be orthonormal in the standard Euclidean inner product, which means we require $A^T A = I$. Let the left preconditioner be $P_L$. The preconditioned matrix is $A = P_L V C = P_L \\Phi$. Let's compute $A^T A$:\n$$\nA^T A = (P_L \\Phi)^T (P_L \\Phi) = \\Phi^T P_L^T P_L \\Phi\n$$\nWe want this to be equal to the identity matrix $I$. Comparing with our derived property $\\Phi^T W \\Phi = I$, an obvious choice is to set $P_L^T P_L = W$.\nThe problem provides the matrix $L = W^{1/2} = \\mathrm{diag}(\\sqrt{w_1},\\dots,\\sqrt{w_N})$. As the quadrature weights $w_i$ are positive, $L$ is a real, symmetric, positive-definite matrix. Let's choose $P_L=L$. Then:\n$$\nP_L^T P_L = L^T L = L L = W^{1/2} W^{1/2} = W\n$$\nThis choice satisfies the condition. Thus, the fully preconditioned matrix is $A = L V C$.\n\nWe can now verify the orthonormality of the columns of $A$:\n$$\nA = L(VC) = L\\Phi\n$$\n$$\nA^T A = (L\\Phi)^T (L\\Phi) = \\Phi^T L^T L \\Phi = \\Phi^T W \\Phi = I\n$$\nSince $A$ is an $N \\times N$ square matrix and satisfies $A^T A = I$, $A$ is an orthogonal matrix.\n\nThe final task is to compute the $2$-norm condition number, $\\kappa_2(A)$, of this matrix $A$. The condition number is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. Alternatively, for a non-singular matrix, it can be expressed as the ratio of the largest to the smallest singular value: $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$.\n\nThe singular values of a matrix $A$ are the square roots of the eigenvalues of the matrix $A^T A$. In our case, we have established that $A^T A = I$. The eigenvalues of the identity matrix $I$ are all equal to $1$. Let $\\lambda_j$ be the eigenvalues of $A^T A$. Then $\\lambda_j = 1$ for all $j=1, \\dots, N$.\nThe singular values $\\sigma_j(A)$ are given by:\n$$\n\\sigma_j(A) = \\sqrt{\\lambda_j(A^T A)} = \\sqrt{1} = 1 \\quad \\text{for } j=1, \\dots, N\n$$\nSince all singular values of $A$ are equal to $1$, the largest singular value is $\\sigma_{\\max}(A) = 1$ and the smallest singular value is $\\sigma_{\\min}(A) = 1$.\nThe $2$-norm condition number of $A$ is therefore:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\frac{1}{1} = 1\n$$\nThis result demonstrates the profound effectiveness of the preconditioning. The original monomial Vandermonde matrix $V$ is notoriously ill-conditioned, with its condition number growing exponentially with $N$. The preconditioned matrix $A=LVC$ is an orthogonal matrix, possessing the optimal condition number of $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}