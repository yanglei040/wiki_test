{
    "hands_on_practices": [
        {
            "introduction": "To be effective, a spectral filter must damp high-frequency oscillations without corrupting the physically meaningful, low-frequency components of the solution. This often translates to designing a filter that preserves low-degree polynomials with high fidelity. This practice will guide you through the analytical derivation of a critical filter parameter, the filter order, based on a prescribed accuracy tolerance for the well-resolved parts of the numerical solution .",
            "id": "3418245",
            "problem": "Consider a one-dimensional modal spectral approximation on the standard element $[-1,1]$ using an orthonormal Legendre polynomial basis, truncated at modal index $N$. Let the expansion of a function $u(x)$ be $u(x) = \\sum_{k=0}^{N} a_{k} \\phi_{k}(x)$, where $\\{\\phi_{k}\\}_{k=0}^{N}$ are the orthonormal modes and $a_{k}$ are the corresponding modal coefficients. In a Discontinuous Galerkin (DG) approximation, stabilization by spectral filtering is performed by multiplying modal coefficients by a filter transfer function $\\sigma_{k}$.\n\nAssume an exponential filter defined by the modal transfer function\n$$\n\\sigma_{k} = \\exp\\!\\left(-\\alpha \\left(\\frac{k}{N}\\right)^{m}\\right),\n$$\nwith $m > 0$ the filter order and $\\alpha > 0$ the filter strength. The filter is required to approximately preserve all polynomials of degree at most $r$ (with $0  r  N$) in the following sense: for any polynomial $p(x)$ with modal coefficients $\\{a_{k}\\}_{k=0}^{r}$ and $a_{k}=0$ for $k>r$, the relative modal perturbation induced by filtering on all retained modes must satisfy\n$$\n\\max_{0 \\leq k \\leq r} \\left|1 - \\sigma_{k}\\right| \\leq \\varepsilon,\n$$\nfor a prescribed tolerance $0  \\varepsilon  1$. Assume $\\alpha > -\\ln(1 - \\varepsilon)$ so that the preservation constraint is nontrivial and requires $m > 0$.\n\nStarting from fundamental definitions of the modal filter, derive a closed-form analytical expression for the minimal order $m$ that guarantees the above preservation requirement. Your final answer should be a single analytic expression in terms of $r$, $N$, $\\alpha$, and $\\varepsilon$. No numerical evaluation or rounding is required. Express your final answer as a single expression and do not include inequalities in the final answer.",
            "solution": "The objective is to derive a closed-form analytical expression for the minimal filter order $m$ that satisfies the specified polynomial preservation requirement.\n\nThe problem states that for a polynomial of degree at most $r$, where $0  r  N$, the modal coefficients $a_k$ are zero for $k > r$. The preservation requirement for the filter is given by the inequality:\n$$\n\\max_{0 \\leq k \\leq r} |1 - \\sigma_{k}| \\leq \\varepsilon\n$$\nwhere $0  \\varepsilon  1$ is a prescribed tolerance, and $\\sigma_k$ is the filter's modal transfer function.\n\nThe filter is defined as an exponential filter:\n$$\n\\sigma_{k} = \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^{m}\\right)\n$$\nwith filter order $m > 0$ and filter strength $\\alpha > 0$. The indices $k$ are non-negative integers.\n\nFirst, let us analyze the term $|1 - \\sigma_k|$. For $k=0$, the exponent is $-\\alpha(0/N)^m = 0$, so $\\sigma_0 = \\exp(0) = 1$. This gives $|1 - \\sigma_0| = |1-1| = 0$. For any $k > 0$, since $\\alpha > 0$ and $m > 0$, the exponent $-\\alpha(k/N)^m$ is strictly negative. Consequently, $0  \\sigma_k  1$. This implies that $1 - \\sigma_k$ is always positive for $k > 0$. Therefore, for all $k \\ge 0$, the absolute value is redundant:\n$$\n|1 - \\sigma_k| = 1 - \\sigma_k\n$$\nThe preservation constraint simplifies to:\n$$\n\\max_{0 \\leq k \\leq r} (1 - \\sigma_{k}) \\leq \\varepsilon\n$$\nNext, we determine which value of $k$ in the range $0 \\le k \\le r$ maximizes the function $f(k) = 1 - \\sigma_k$. Let us analyze the monotonicity of $f(k)$ by treating $k$ as a continuous real variable for $k \\ge 0$. The function is:\n$$\nf(k) = 1 - \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^{m}\\right)\n$$\nWe compute the derivative of $f(k)$ with respect to $k$:\n$$\n\\frac{df}{dk} = -\\frac{d}{dk} \\left[ \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^{m}\\right) \\right]\n$$\nUsing the chain rule,\n$$\n\\frac{df}{dk} = - \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^{m}\\right) \\cdot \\left(-\\alpha \\frac{m k^{m-1}}{N^m}\\right)\n$$\n$$\n\\frac{df}{dk} = \\left(\\frac{\\alpha m}{N^m}\\right) k^{m-1} \\exp\\left(-\\alpha \\left(\\frac{k}{N}\\right)^{m}\\right)\n$$\nGiven that $\\alpha > 0$, $m > 0$, and $N > 0$, the pre-factor $(\\alpha m / N^m)$ is positive. The exponential term is always positive. For $k \\ge 0$, the term $k^{m-1}$ is non-negative. It is zero only at $k=0$ if $m>1$, and positive for $k>0$ if $m \\le 1$. In all cases for $k \\ge 0$, $\\frac{df}{dk} \\ge 0$. This shows that $f(k)$ is a monotonically non-decreasing function of $k$.\n\nFor a discrete set of integer values $\\{0, 1, 2, ..., r\\}$, a monotonically non-decreasing function attains its maximum at the largest value in the set. Therefore, the maximum of $f(k)$ occurs at $k=r$.\n$$\n\\max_{0 \\leq k \\leq r} (1 - \\sigma_{k}) = 1 - \\sigma_{r}\n$$\nThe preservation constraint reduces to a single inequality involving the mode $k=r$:\n$$\n1 - \\sigma_{r} \\leq \\varepsilon\n$$\nSubstituting the expression for $\\sigma_r$:\n$$\n1 - \\exp\\left(-\\alpha \\left(\\frac{r}{N}\\right)^{m}\\right) \\leq \\varepsilon\n$$\nWe now solve this inequality for the filter order $m$. Rearranging the terms, we get:\n$$\n1 - \\varepsilon \\leq \\exp\\left(-\\alpha \\left(\\frac{r}{N}\\right)^{m}\\right)\n$$\nSince $0  \\varepsilon  1$, the term $1 - \\varepsilon$ is between $0$ and $1$. We can take the natural logarithm of both sides. As the natural logarithm function is monotonically increasing, the direction of the inequality is preserved:\n$$\n\\ln(1 - \\varepsilon) \\leq -\\alpha \\left(\\frac{r}{N}\\right)^{m}\n$$\nMultiplying by $-1$ reverses the inequality:\n$$\n-\\ln(1 - \\varepsilon) \\geq \\alpha \\left(\\frac{r}{N}\\right)^{m}\n$$\nSince $\\alpha > 0$, we can divide by $\\alpha$ without changing the inequality direction:\n$$\n\\frac{-\\ln(1 - \\varepsilon)}{\\alpha} \\geq \\left(\\frac{r}{N}\\right)^{m}\n$$\nTo isolate $m$, we take the natural logarithm of both sides again. The problem states that $\\alpha > -\\ln(1 - \\varepsilon)$, which implies that the left side, $\\frac{-\\ln(1 - \\varepsilon)}{\\alpha}$, is a positive number less than $1$. The base of the power on the right side, $r/N$, is also between $0$ and $1$ since $0  r  N$. Taking the natural logarithm of a number between $0$ and $1$ yields a negative result.\n$$\n\\ln\\left(\\frac{-\\ln(1 - \\varepsilon)}{\\alpha}\\right) \\geq \\ln\\left(\\left(\\frac{r}{N}\\right)^{m}\\right)\n$$\nUsing the logarithm property $\\ln(x^y) = y \\ln(x)$:\n$$\n\\ln\\left(\\frac{-\\ln(1 - \\varepsilon)}{\\alpha}\\right) \\geq m \\ln\\left(\\frac{r}{N}\\right)\n$$\nTo solve for $m$, we must divide by $\\ln(r/N)$. Since $0  r  N$, we have $0  r/N  1$, which implies that $\\ln(r/N)$ is a negative number. Dividing by a negative number reverses the inequality sign:\n$$\n\\frac{\\ln\\left(\\frac{-\\ln(1 - \\varepsilon)}{\\alpha}\\right)}{\\ln\\left(\\frac{r}{N}\\right)} \\leq m\n$$\nThis inequality provides the condition that $m$ must satisfy. To find the minimal order $m$ that guarantees the preservation requirement, we select the smallest possible value for $m$, which is the lower bound of this inequality.\n$$\nm_{\\text{min}} = \\frac{\\ln\\left(\\frac{-\\ln(1 - \\varepsilon)}{\\alpha}\\right)}{\\ln\\left(\\frac{r}{N}\\right)}\n$$\nThis expression provides the required closed-form analytical expression for the minimal filter order $m$.",
            "answer": "$$\\boxed{\\frac{\\ln\\left(\\frac{-\\ln(1 - \\varepsilon)}{\\alpha}\\right)}{\\ln\\left(\\frac{r}{N}\\right)}}$$"
        },
        {
            "introduction": "While filtering is essential for stability, it is not a \"free lunch\" and can degrade the formal accuracy of a numerical method. A particularly important casualty is often the property of superconvergence, where the solution exhibits higher-than-expected accuracy at specific points. This exercise provides a concrete, hands-on calculation of the error introduced by filtering at such a point, offering a clear view of the trade-off between stability and high-order accuracy .",
            "id": "3418287",
            "problem": "Consider the scalar linear advection equation $u_{t} + a u_{x} = 0$ on the interval $[-1,1]$ with constant advection speed $a>0$. Let a single-element spectral collocation discretization be used with Legendre–Gauss–Lobatto nodes of polynomial degree $N$, so that the numerical solution is represented exactly (in the absence of filtering) by a Legendre modal expansion $u_{N}(x,t) = \\sum_{k=0}^{N} \\hat{u}_{k}(t) P_{k}(x)$, where $P_{k}(x)$ are Legendre polynomials satisfying $P_{k}(1)=1$ for all $k$. In this collocation setting, the point $x=1$ is a flux point: for polynomial data of degree at most $N$, the nodal and modal representations agree and the conservative flux evaluated at $x=1$ is exact in the unfiltered scheme.\n\nTo stabilize oscillations, an exponential modal filter is applied at each time step to the modal coefficients via $\\hat{u}_{k} \\mapsto \\sigma_{k} \\hat{u}_{k}$, where the transfer function is \n$$\n\\sigma_{k} = \\exp\\!\\big(-\\alpha \\left(\\tfrac{k}{N}\\right)^{p}\\big),\n$$ \nwith filter strength $\\alpha>0$ and filter order $p \\in \\mathbb{N}$. Assume that at a fixed time $t^{\\star}$ the exact solution $u(x,t^{\\star})$ equals the single Legendre mode $P_{N}(x)$, so that the unfiltered spectral collocation representation recovers $u(x,t^{\\star})$ exactly with $\\hat{u}_{N}(t^{\\star})=1$ and $\\hat{u}_{k}(t^{\\star})=0$ for $k\\neq N$.\n\nStarting from the core definitions of Legendre polynomial modal expansions and the action of diagonal modal filtering, derive the pointwise error at the flux point $x=1$ introduced by the filter, defined by \n$$\n\\varepsilon(\\alpha,p,N) = u_{N}^{\\mathrm{filtered}}(1,t^{\\star}) - u(1,t^{\\star}),\n$$ \nand provide a closed-form analytic expression for $\\varepsilon(\\alpha,p,N)$ in terms of $\\alpha$, $p$, and $N$. Then, using first principles of modal-to-point evaluation at $x=1$, characterize the necessary and sufficient condition on the diagonal modal transfer function $k \\mapsto \\sigma_{k}$ under which the flux-point superconvergence (exactness of the flux-point value for all polynomials of degree at most $N$) survives filtering.\n\nExpress the final answer as the single analytic expression for $\\varepsilon(\\alpha,p,N)$. No rounding is required and no units are involved.",
            "solution": "The analysis proceeds in two parts as requested by the problem statement.\n\n**Part 1: Derivation of the Pointwise Error $\\varepsilon(\\alpha,p,N)$**\n\nThe error at the flux point $x=1$ is defined as $\\varepsilon(\\alpha,p,N) = u_{N}^{\\mathrm{filtered}}(1,t^{\\star}) - u(1,t^{\\star})$. We will compute each term separately.\n\nFirst, we determine the value of the exact solution at the point of interest. The problem states that at time $t^{\\star}$, the exact solution is $u(x,t^{\\star}) = P_{N}(x)$. We evaluate this at $x=1$:\n$$\nu(1,t^{\\star}) = P_{N}(1)\n$$\nThe problem specifies the property that Legendre polynomials satisfy $P_{k}(1) = 1$ for all $k$. Therefore, for $k=N$, we have:\n$$\nu(1,t^{\\star}) = 1\n$$\n\nNext, we determine the value of the filtered numerical solution, $u_{N}^{\\mathrm{filtered}}(1,t^{\\star})$. The unfiltered solution, $u_{N}(x,t^{\\star})$, is given to have modal coefficients $\\hat{u}_{N}(t^{\\star})=1$ and $\\hat{u}_{k}(t^{\\star})=0$ for all $k \\neq N$. The filtering process acts on these modal coefficients. Let the filtered coefficients be denoted $\\hat{u}_{k}^{\\mathrm{filtered}}$. The transformation is given by:\n$$\n\\hat{u}_{k}^{\\mathrm{filtered}} = \\sigma_{k} \\hat{u}_{k}(t^{\\star})\n$$\nFor $k \\neq N$, we have $\\hat{u}_{k}(t^{\\star})=0$, so $\\hat{u}_{k}^{\\mathrm{filtered}} = \\sigma_{k} \\cdot 0 = 0$.\nFor $k = N$, we have $\\hat{u}_{N}(t^{\\star})=1$, so $\\hat{u}_{N}^{\\mathrm{filtered}} = \\sigma_{N} \\cdot 1 = \\sigma_{N}$.\n\nThe filter transfer function is $\\sigma_{k} = \\exp(-\\alpha (\\frac{k}{N})^{p})$. For the specific mode $k=N$, this becomes:\n$$\n\\sigma_{N} = \\exp\\big(-\\alpha \\left(\\tfrac{N}{N}\\right)^{p}\\big) = \\exp(-\\alpha \\cdot 1^{p}) = \\exp(-\\alpha)\n$$\nThus, the only non-zero filtered modal coefficient is $\\hat{u}_{N}^{\\mathrm{filtered}} = \\exp(-\\alpha)$.\n\nThe filtered numerical solution is reconstructed from these filtered coefficients:\n$$\nu_{N}^{\\mathrm{filtered}}(x,t^{\\star}) = \\sum_{k=0}^{N} \\hat{u}_{k}^{\\mathrm{filtered}} P_{k}(x)\n$$\nSince only the coefficient for $k=N$ is non-zero, this sum collapses to a single term:\n$$\nu_{N}^{\\mathrm{filtered}}(x,t^{\\star}) = \\hat{u}_{N}^{\\mathrm{filtered}} P_{N}(x) = \\exp(-\\alpha) P_{N}(x)\n$$\nNow, we evaluate this filtered solution at the flux point $x=1$:\n$$\nu_{N}^{\\mathrm{filtered}}(1,t^{\\star}) = \\exp(-\\alpha) P_{N}(1)\n$$\nUsing the property $P_{N}(1)=1$, we find:\n$$\nu_{N}^{\\mathrm{filtered}}(1,t^{\\star}) = \\exp(-\\alpha)\n$$\n\nFinally, we compute the error $\\varepsilon(\\alpha,p,N)$ by substituting the values we found:\n$$\n\\varepsilon(\\alpha,p,N) = u_{N}^{\\mathrm{filtered}}(1,t^{\\star}) - u(1,t^{\\star}) = \\exp(-\\alpha) - 1\n$$\nThis is the closed-form analytic expression for the error. Notably, under the specific condition that the initial error is concentrated in the highest mode $k=N$, the error introduced by the filter is independent of the polynomial degree $N$ and the filter order $p$.\n\n**Part 2: Condition for Preserving Flux-Point Superconvergence**\n\nFlux-point superconvergence implies that the numerical solution is exact at the point $x=1$ for any solution that is a polynomial of degree at most $N$. Let us consider an arbitrary polynomial solution $u(x)$ in this space, represented by its Legendre modal expansion:\n$$\nu(x) = \\sum_{k=0}^{N} \\hat{u}_{k} P_{k}(x)\n$$\nThe exact value of this solution at $x=1$ is:\n$$\nu(1) = \\sum_{k=0}^{N} \\hat{u}_{k} P_{k}(1) = \\sum_{k=0}^{N} \\hat{u}_{k} \\cdot 1 = \\sum_{k=0}^{N} \\hat{u}_{k}\n$$\nThe filtered numerical representation of this solution, $u_{N}^{\\mathrm{filtered}}(x)$, has modal coefficients $\\hat{u}_{k}^{\\mathrm{filtered}} = \\sigma_{k} \\hat{u}_{k}$. Its value at $x=1$ is:\n$$\nu_{N}^{\\mathrm{filtered}}(1) = \\sum_{k=0}^{N} \\hat{u}_{k}^{\\mathrm{filtered}} P_{k}(1) = \\sum_{k=0}^{N} (\\sigma_{k} \\hat{u}_{k}) \\cdot 1 = \\sum_{k=0}^{N} \\sigma_{k} \\hat{u}_{k}\n$$\nFor superconvergence to be preserved by the filter, the filtered value must be equal to the exact value for any choice of coefficients $\\{\\hat{u}_{k}\\}_{k=0}^{N}$. This gives the condition:\n$$\n\\sum_{k=0}^{N} \\sigma_{k} \\hat{u}_{k} = \\sum_{k=0}^{N} \\hat{u}_{k}\n$$\nRearranging the terms, we get:\n$$\n\\sum_{k=0}^{N} \\sigma_{k} \\hat{u}_{k} - \\sum_{k=0}^{N} \\hat{u}_{k} = 0\n$$\n$$\n\\sum_{k=0}^{N} (\\sigma_{k} - 1) \\hat{u}_{k} = 0\n$$\nThis equation must hold for any arbitrary vector of coefficients $(\\hat{u}_{0}, \\hat{u}_{1}, \\dots, \\hat{u}_{N})$. In the vector space of coefficients, this is equivalent to the dot product of the vector $(\\sigma_{0}-1, \\dots, \\sigma_{N}-1)$ with an arbitrary vector $(\\hat{u}_{0}, \\dots, \\hat{u}_{N})$ being zero. This is only possible if the first vector is the zero vector. Therefore, each component must be zero:\n$$\n\\sigma_{k} - 1 = 0 \\quad \\text{for all } k \\in \\{0, 1, \\dots, N\\}\n$$\nThis yields the necessary and sufficient condition:\n$$\n\\sigma_{k} = 1 \\quad \\text{for all } k \\in \\{0, 1, \\dots, N\\}\n$$\nIn words, to preserve the exactness of the solution at the flux point for all polynomials up to degree $N$, the filter's transfer function must be unity for all corresponding modes. This means the filter must act as the identity operator on the solution space, effectively applying no filtering at all. Any non-trivial filter (i.e., where $\\sigma_{k}  1$ for some $k$) will destroy the superconvergence property.",
            "answer": "$$\n\\boxed{\\exp(-\\alpha) - 1}\n$$"
        },
        {
            "introduction": "Having designed a filter and understood its costs, the final practical question is how to use it most effectively within a time-integration loop. Applying a filter too often can harm accuracy and add computational overhead, while applying it too seldom may not provide sufficient stability. This exercise challenges you to build and apply a quantitative decision model that balances stability, accuracy loss, and cost to determine the optimal filtering frequency for a given problem .",
            "id": "3418290",
            "problem": "You are asked to design and implement a quantitative decision rule for when to apply a spectral filter in Strong Stability Preserving Runge–Kutta (SSPRK) time integration, specifically comparing application at every stage versus once per full time step, in the context of spectral or Discontinuous Galerkin (DG) spatial discretizations of linear problems. Your goal is to define, compute, and compare a stability-accuracy-cost objective for two choices of filter application frequency, and then choose the frequency that minimizes the objective.\n\nBase model. Consider the linear test equation $y'(t) = \\lambda y(t)$ where $\\lambda \\in \\mathbb{C}$ models a semi-discrete eigenvalue from the spatial discretization of a linear hyperbolic or advection-dominated problem. Let $\\Delta t$ be a fixed time step and define $z = \\lambda \\Delta t \\in \\mathbb{C}$. An explicit $s$-stage Strong Stability Preserving Runge–Kutta (SSPRK) scheme advances $y^n$ to $y^{n+1}$ with a stability polynomial $R_s(z) \\in \\mathbb{C}$ such that for the linear test equation $y^{n+1} = R_s(z)\\, y^n$. For the canonical explicit SSPRK methods with order equal to stages for $s \\in \\{1,2,3\\}$, the stability polynomials are\n- For $s=1$ (Forward Euler): $R_1(z) = 1 + z$.\n- For $s=2$ (Second-order SSPRK): $R_2(z) = 1 + z + \\tfrac{1}{2} z^2$.\n- For $s=3$ (Third-order SSPRK): $R_3(z) = 1 + z + \\tfrac{1}{2} z^2 + \\tfrac{1}{6} z^3$.\n\nSpectral filter. Assume an exponential spectral filter acting on modal coefficients indexed by a normalized modal coordinate $\\theta \\in [0,1]$ with transfer function\n$$\n\\phi(\\theta; \\alpha, p) = \\exp\\!\\big(- \\alpha\\, \\theta^p\\big),\n$$\nwhere $\\alpha  0$ controls filter strength and $p \\in \\mathbb{N}$ (typically even) controls filter sharpness. Applying the filter $m$ times multiplies a mode of index $\\theta$ by $\\phi(\\theta)^m$.\n\nDecision space. You will compare two policies within one time step:\n- Apply the filter once per full step: $m = 1$.\n- Apply the filter after every stage: $m = s$.\n\nStability metric. To quantify stabilization for the high-frequency content, use the filtered one-step amplification at the most oscillatory mode $\\theta_h = 1$:\n$$\nG(m; z, s, \\alpha, p) = \\big|\\phi(1;\\alpha,p)^m \\, R_s(z)\\big|.\n$$\nDefine the stability penalty as the positive part of instability beyond unit magnitude:\n$$\n\\mathcal{S}(m) = \\max\\big(0,\\, G(m; z, s, \\alpha, p) - 1\\big).\n$$\n\nAccuracy-loss metric. To quantify accuracy loss due to over-filtering on a smooth solution with a decaying modal energy spectrum $E(\\theta) = \\theta^{-q}$ for $\\theta \\in (0,1]$ with $q  0$, and assuming only modes above a cutoff $\\theta_c \\in (0,1)$ are intentionally damped, define the per-step dissipation as\n$$\n\\mathcal{D}(m) = \\int_{\\theta_c}^{1} \\Big(1 - \\exp\\!\\big(- m \\alpha\\, \\theta^p\\big)\\Big)\\, \\theta^{-q}\\, d\\theta.\n$$\nThis integral must be evaluated numerically.\n\nComputational cost model. Let the cost be proportional to the number of filter applications:\n$$\n\\mathcal{C}(m) = w_c \\, m,\n$$\nwith a given weight $w_c  0$.\n\nObjective. Combine the three components with nonnegative weights $w_s$, $w_a$, and $w_c$:\n$$\nJ(m) = w_s\\, \\mathcal{S}(m) + w_a\\, \\mathcal{D}(m) + \\mathcal{C}(m).\n$$\nChoose the frequency $m \\in \\{1, s\\}$ that minimizes $J(m)$. In case of an exact tie, choose $m = 1$.\n\nImplementation requirements.\n- Implement $R_s(z)$ for $s \\in \\{1,2,3\\}$ as defined above.\n- Compute $\\mathcal{S}(m)$ exactly from the given formulas.\n- Compute $\\mathcal{D}(m)$ via numerical quadrature with sufficient accuracy using Gaussian quadrature on $[\\theta_c,1]$.\n- Use $\\theta_h = 1$ for the stability metric.\n- The program must return, for each test case, the integer decision: $0$ if $m=1$ is optimal (filter once per step) and $1$ if $m=s$ is optimal (filter every stage).\n\nTest suite. Use the following test cases. Each test case specifies the tuple $(s, \\Re z, \\Im z, \\alpha, p, q, \\theta_c, w_s, w_a, w_c)$:\n- Case A (moderate advection-like oscillation, mild filter, moderate stability weight): $(s, \\Re z, \\Im z, \\alpha, p, q, \\theta_c, w_s, w_a, w_c) = (\\,3,\\, 0,\\, 2,\\, 0.2,\\, 8,\\, 4.0,\\, 0.7,\\, 5.0,\\, 1.0,\\, 0.2\\,)$.\n- Case B (amplifying scalar test, strong filter, high stability weight): $(\\,3,\\, 1.5,\\, 0,\\, 1.0,\\, 8,\\, 4.0,\\, 0.7,\\, 10.0,\\, 0.5,\\, 0.1\\,)$.\n- Case C (Forward Euler edge case, inherently stable step): $(\\,1,\\, -1.0,\\, 0,\\, 0.5,\\, 8,\\, 4.0,\\, 0.7,\\, 5.0,\\, 1.0,\\, 0.2\\,)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must align with cases A, B, C in order. Each entry must be an integer: $0$ for choosing filter once per step and $1$ for choosing filter every stage. For example, a valid output with three results is of the form \"[0,1,0]\". No extra text should be printed.",
            "solution": "The problem requires the formulation and implementation of a quantitative decision rule to determine the optimal frequency for applying a spectral filter during the time integration of a linear ordinary differential equation using a Strong Stability Preserving Runge–Kutta (SSPRK) method. The decision is between applying the filter once per full time step ($m=1$) or once per stage ($m=s$). The optimal choice is the one that minimizes a composite objective function $J(m)$ that balances stability, accuracy, and computational cost.\n\nThe analysis is based on the linear test equation $y'(t) = \\lambda y(t)$, where the complex number $z = \\lambda \\Delta t$ represents the product of a semi-discrete eigenvalue $\\lambda$ and the time step $\\Delta t$. An $s$-stage explicit SSPRK method evolves the solution via a stability polynomial $R_s(z)$, such that $y^{n+1} = R_s(z) y^n$. The polynomials for $s \\in \\{1,2,3\\}$ are given as:\n$$\nR_1(z) = 1 + z\n$$\n$$\nR_2(z) = 1 + z + \\frac{1}{2} z^2\n$$\n$$\nR_3(z) = 1 + z + \\frac{1}{2} z^2 + \\frac{1}{6} z^3\n$$\n\nA spectral filter is introduced to damp high-frequency modes, which are often the source of numerical instability in spectral or Discontinuous Galerkin methods. The filter's action on a mode with normalized index $\\theta \\in [0,1]$ is described by the transfer function $\\phi(\\theta; \\alpha, p) = \\exp(-\\alpha \\theta^p)$, where $\\alpha > 0$ is the filter strength and $p \\in \\mathbb{N}$ is the filter sharpness. Applying the filter $m$ times reduces the amplitude of a mode by a factor of $\\phi(\\theta)^m$.\n\nThe decision framework is built upon minimizing the objective function $J(m)$:\n$$\nJ(m) = w_s\\, \\mathcal{S}(m) + w_a\\, \\mathcal{D}(m) + \\mathcal{C}(m)\n$$\nwhere $w_s$, $w_a$, and $w_c$ are non-negative weights for stability, accuracy (dissipation), and cost, respectively. We evaluate $J(m)$ for $m=1$ and $m=s$ and select the value of $m$ that yields the minimum $J$.\n\nThe three components of the objective function are defined as follows:\n\n1.  **Stability Penalty $\\mathcal{S}(m)$**: This term quantifies the degree of instability. It is based on the amplification factor of the highest-frequency mode, taken at $\\theta_h=1$. The combined amplification of the time-stepper and $m$ filter applications is $G(m; z, s, \\alpha, p) = |\\phi(1;\\alpha,p)^m R_s(z)|$. Since $\\phi(1; \\alpha, p) = \\exp(-\\alpha)$, this simplifies to $G(m) = \\exp(-m\\alpha)|R_s(z)|$. The penalty is the amount by which this amplification factor exceeds unity:\n    $$\n    \\mathcal{S}(m) = \\max\\big(0,\\, G(m) - 1\\big) = \\max\\big(0,\\, e^{-m\\alpha}|R_s(z)| - 1\\big)\n    $$\n\n2.  **Accuracy-Loss Metric $\\mathcal{D}(m)$**: This term quantifies the undesired dissipation of energy from modes that are considered part of the solution's well-resolved content. Assuming a modal energy spectrum $E(\\theta) = \\theta^{-q}$ for $q>0$, the accuracy loss is modeled as the total dissipation for modes above a certain cutoff $\\theta_c \\in (0,1)$. The dissipation for a single mode $\\theta$ is $1 - \\phi(\\theta)^m$. The total accuracy loss is the integral of this dissipation, weighted by the energy spectrum:\n    $$\n    \\mathcal{D}(m) = \\int_{\\theta_c}^{1} \\Big(1 - \\phi(\\theta)^m\\Big) E(\\theta)\\, d\\theta = \\int_{\\theta_c}^{1} \\Big(1 - e^{-m \\alpha \\theta^p}\\Big)\\, \\theta^{-q}\\, d\\theta\n    $$\n    This integral does not have a general closed-form solution and must be evaluated numerically, for which we employ Gaussian quadrature.\n\n3.  **Computational Cost $\\mathcal{C}(m)$**: This term models the computational overhead of applying the filter. It is assumed to be directly proportional to the number of filter applications, $m$, within a single time step:\n    $$\n    \\mathcal{C}(m) = w_c \\, m\n    $$\n\nThe decision algorithm for a given set of parameters $(s, z, \\alpha, p, q, \\theta_c, w_s, w_a, w_c)$ is as follows:\n1.  Calculate $J(1) = w_s \\mathcal{S}(1) + w_a \\mathcal{D}(1) + \\mathcal{C}(1)$.\n2.  Calculate $J(s) = w_s \\mathcal{S}(s) + w_a \\mathcal{D}(s) + \\mathcal{C}(s)$. Note that if $s=1$, this step is identical to the first, and $J(1)=J(s)$.\n3.  Compare the two values. If $J(1) \\le J(s)$, the optimal strategy is to filter once per step ($m=1$), which is encoded as the output $0$. Otherwise, if $J(1) > J(s)$, the optimal strategy is to filter every stage ($m=s$), encoded as $1$. The tie-breaking rule (choosing $m=1$ in case of a tie) is handled by the non-strict inequality.\n\nThis procedure provides a rigorous, quantitative basis for making a key implementation choice in stabilized numerical schemes, balancing the competing demands of stability, accuracy, and efficiency.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the decision problem for spectral filter application frequency\n    for a given set of test cases.\n    \"\"\"\n\n    # test_cases: (s, Re_z, Im_z, alpha, p, q, theta_c, w_s, w_a, w_c)\n    test_cases = [\n        # Case A: Moderate advection-like oscillation, mild filter\n        (3, 0.0, 2.0, 0.2, 8, 4.0, 0.7, 5.0, 1.0, 0.2),\n        # Case B: Amplifying scalar test, strong filter\n        (3, 1.5, 0.0, 1.0, 8, 4.0, 0.7, 10.0, 0.5, 0.1),\n        # Case C: Forward Euler edge case, inherently stable step\n        (1, -1.0, 0.0, 0.5, 8, 4.0, 0.7, 5.0, 1.0, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        s, re_z, im_z, alpha, p, q, theta_c, w_s, w_a, w_c = case\n        z = complex(re_z, im_z)\n\n        # Calculate objective J for m=1 and m=s\n        j_1 = calculate_objective(1, s, z, alpha, p, q, theta_c, w_s, w_a, w_c)\n        j_s = calculate_objective(s, s, z, alpha, p, q, theta_c, w_s, w_a, w_c)\n\n        # Decision rule: choose m that minimizes J. Tie goes to m=1.\n        if j_1 = j_s:\n            results.append(0)  # m=1 is optimal\n        else:\n            results.append(1)  # m=s is optimal\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef calculate_objective(m, s, z, alpha, p, q, theta_c, w_s, w_a, w_c):\n    \"\"\"\n    Calculates the objective function J(m) for a given set of parameters.\n    \n    Args:\n        m (int): Number of filter applications.\n        s (int): Number of stages in the SSPRK method.\n        z (complex): Scaled eigenvalue z = lambda * dt.\n        alpha (float): Filter strength parameter.\n        p (int): Filter sharpness parameter.\n        q (float): Modal energy spectrum decay rate.\n        theta_c (float): Cutoff mode for dissipation calculation.\n        w_s (float): Weight for the stability penalty.\n        w_a (float): Weight for the accuracy-loss metric.\n        w_c (float): Weight for the computational cost.\n\n    Returns:\n        float: The value of the objective function J(m).\n    \"\"\"\n\n    # 1. Calculate Stability Polynomial R_s(z)\n    if s == 1:\n        # R_1(z) = 1 + z\n        r_s_val = 1.0 + z\n    elif s == 2:\n        # R_2(z) = 1 + z + 0.5*z^2\n        r_s_val = 1.0 + z + 0.5 * z**2\n    elif s == 3:\n        # R_3(z) = 1 + z + 0.5*z^2 + (1/6)*z^3\n        r_s_val = 1.0 + z + 0.5 * z**2 + (1.0/6.0) * z**3\n    else:\n        raise ValueError(\"s must be 1, 2, or 3.\")\n\n    # 2. Calculate Stability Penalty S(m)\n    abs_r_s = np.abs(r_s_val)\n    # phi(1) = exp(-alpha)\n    phi_1 = np.exp(-alpha)\n    # G(m) = |phi(1)^m * R_s(z)|\n    g_m = (phi_1**m) * abs_r_s\n    s_m = max(0.0, g_m - 1.0)\n    \n    # 3. Calculate Accuracy-Loss Metric D(m)\n    # Integrand: (1 - exp(-m*alpha*theta^p)) * theta^-q\n    integrand = lambda theta: (1.0 - np.exp(-m * alpha * theta**p)) * theta**(-q)\n    # Use scipy.integrate.quad for numerical integration\n    d_m, _ = quad(integrand, theta_c, 1.0)\n\n    # 4. Calculate Computational Cost C(m)\n    c_m = w_c * m\n\n    # 5. Calculate final objective J(m)\n    j_m = w_s * s_m + w_a * d_m + w_c * m\n    \n    return j_m\n\nsolve()\n```"
        }
    ]
}