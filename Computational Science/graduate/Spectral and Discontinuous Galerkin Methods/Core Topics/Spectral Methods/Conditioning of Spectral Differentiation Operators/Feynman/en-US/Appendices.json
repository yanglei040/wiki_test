{
    "hands_on_practices": [
        {
            "introduction": "The performance of iterative solvers for linear systems is deeply connected to the condition number of the system matrix. For spectral differentiation operators, this condition number is known to grow rapidly with the polynomial degree $N$, posing a significant computational challenge. This exercise guides you through a fundamental analysis to uncover the root cause of this ill-conditioning by examining the operator's behavior in a weighted norm that mirrors the continuous setting . By analyzing the action of the Chebyshev differentiation matrix on the \"smoothest\" and \"roughest\" possible functions within a mean-zero space, you will derive the asymptotic scaling of its extremal singular values and, consequently, the famous $\\mathcal{O}(N^4)$ scaling of the squared condition number.",
            "id": "3372568",
            "problem": "Consider the Chebyshev–Gauss–Lobatto grid defined by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j \\in \\{0,1,\\dots,N\\}$, and let $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ denote the spectral differentiation matrix obtained by differentiating the degree–$N$ Lagrange interpolant built on $\\{x_j\\}$. Let $\\omega_j$ denote any set of quadrature weights that yields a spectrally accurate discrete approximation to the weighted $L^2$ inner product associated with the Chebyshev weight $w(x) = \\frac{1}{\\sqrt{1-x^2}}$, that is, for sufficiently smooth $f$, $\\sum_{j=0}^{N} \\omega_j f(x_j) \\approx \\int_{-1}^{1} \\frac{f(x)}{\\sqrt{1-x^2}}\\,dx$ with errors decaying faster than any power of $N$. Define the discrete weighted $\\ell^2$ norm by $\\|v\\|_{2,w} = \\left(\\sum_{j=0}^{N} \\omega_j |v_j|^2\\right)^{1/2}$ on vectors $v \\in \\mathbb{R}^{N+1}$, and the mean-zero subspace\n$$\n\\mathcal{M} = \\left\\{ v \\in \\mathbb{R}^{N+1} : \\sum_{j=0}^{N} \\omega_j v_j = 0 \\right\\}.\n$$\nConsider the restriction $D|_{\\mathcal{M}}$ and its Singular Value Decomposition (SVD), namely, the decomposition into singular values and singular vectors with respect to the inner product induced by $\\| \\cdot \\|_{2,w}$. Let $\\sigma_{\\min}(N)$ and $\\sigma_{\\max}(N)$ denote, respectively, the smallest and largest singular values of $D|_{\\mathcal{M}}$ in the weighted $\\ell^2$ norm. Using only fundamental facts about Chebyshev polynomials, the node distribution $\\{x_j\\}$, and norm equivalence between spectrally accurate quadrature and the corresponding continuous weighted $L^2$ norm, derive the asymptotic scaling of $\\sigma_{\\min}(N)$ and $\\sigma_{\\max}(N)$ as $N \\to \\infty$, and explain the dominant modal content (in terms of Chebyshev polynomials) of the right singular vectors associated with these two extremal singular values.\n\nYour final reported quantity must be the exponent $\\alpha$ in the leading-order asymptotic scaling of the condition number\n$$\n\\kappa(N) = \\frac{\\sigma_{\\max}(N)}{\\sigma_{\\min}(N)} \\sim C N^{\\alpha}\n$$\nfor some constant $C>0$ independent of $N$. Provide the single value of $\\alpha$ as your final answer. No rounding is required.",
            "solution": "The user wants to determine the asymptotic scaling of the condition number for the Chebyshev spectral differentiation matrix restricted to a mean-zero subspace.\n\nThe problem is defined on the Chebyshev-Gauss-Lobatto (CGL) grid $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j \\in \\{0, 1, \\dots, N\\}$. Let $D$ be the associated $(N+1) \\times (N+1)$ spectral differentiation matrix. The analysis is performed with respect to a discrete weighted inner product $\\langle u, v \\rangle_{2,w} = \\sum_{j=0}^{N} \\omega_j u_j v_j$, which induces the norm $\\|v\\|_{2,w} = \\left(\\sum_{j=0}^{N} \\omega_j |v_j|^2\\right)^{1/2}$. The weights $\\omega_j$ are such that this inner product is a spectrally accurate approximation of the continuous weighted inner product $\\langle f, g \\rangle_{L^2_w} = \\int_{-1}^{1} \\frac{f(x)g(x)}{\\sqrt{1-x^2}}\\,dx$. The problem relies on the norm equivalence between $\\|v\\|_{2,w}$ and $\\|p\\|_{L^2_w}$, where $v_j=p(x_j)$ for a polynomial $p(x)$ of degree at most $N$.\n\nThe operator is restricted to the mean-zero subspace $\\mathcal{M} = \\left\\{ v \\in \\mathbb{R}^{N+1} : \\sum_{j=0}^{N} \\omega_j v_j = 0 \\right\\}$. This discrete condition is equivalent to the continuous condition $\\int_{-1}^{1} \\frac{p(x)}{\\sqrt{1-x^2}}\\,dx=0$ for the corresponding polynomial $p(x)$. In the basis of Chebyshev polynomials $T_k(x)$, this condition implies that the coefficient of $T_0(x)=1$ is zero. Therefore, $\\mathcal{M}$ corresponds to the space of polynomials $\\text{span}\\{T_1(x), T_2(x), \\dots, T_N(x)\\}$.\n\nThe singular values $\\sigma$ of the restricted operator $D|_{\\mathcal{M}}$ are given by the square roots of the eigenvalues of $(D|_{\\mathcal{M}})^* (D|_{\\mathcal{M}})$, where the adjoint is taken with respect to the weighted inner product. Equivalently, they are characterized by the Rayleigh quotient:\n$$\n\\sigma = \\sup_{v \\in \\mathcal{M}, v \\neq 0} \\frac{\\|Dv\\|_{2,w}}{\\|v\\|_{2,w}}\n$$\nBy the norm equivalence principle stated in the problem, the scaling of the singular values of the discrete operator $D$ can be determined by analyzing the norm of the continuous differentiation operator $\\mathcal{D} = \\frac{d}{dx}$ acting on the corresponding polynomial space, with norms taken in $L^2_w$. We seek the scaling of $\\sigma_{\\min}(N)$ and $\\sigma_{\\max}(N)$, which correspond to the infimum and supremum of the ratio $\\frac{\\|\\mathcal{D}p\\|_{L^2_w}}{\\|p\\|_{L^2_w}}$ over all $p \\in \\text{span}\\{T_1, \\dots, T_N\\}$.\n\n### **Analysis of the Minimum Singular Value, $\\sigma_{\\min}(N)$**\n\nThe minimum singular value corresponds to the mode that is least amplified by differentiation. This corresponds to the \"smoothest\" or lowest-frequency function in the space $\\mathcal{M}$. The basis for the polynomial space corresponding to $\\mathcal{M}$ is $\\{T_k(x)\\}_{k=1}^N$. The lowest frequency mode is $T_1(x)=x$.\n\nLet's compute the Rayleigh quotient for the polynomial $p(x) = T_1(x)$:\nThe squared norm of $p(x)$ in $L^2_w$ is:\n$$\n\\|T_1\\|^2_{L^2_w} = \\int_{-1}^{1} \\frac{T_1(x)^2}{\\sqrt{1-x^2}}\\,dx = \\frac{\\pi}{2}\n$$\nThe derivative is $p'(x) = T_1'(x) = 1 = T_0(x)$. The squared norm of the derivative is:\n$$\n\\|T_1'\\|^2_{L^2_w} = \\int_{-1}^{1} \\frac{T_0(x)^2}{\\sqrt{1-x^2}}\\,dx = \\pi\n$$\nThe Rayleigh quotient for $T_1(x)$ is $\\frac{\\|T_1'\\|_{L^2_w}}{\\|T_1\\|_{L^2_w}} = \\sqrt{\\frac{\\pi}{\\pi/2}} = \\sqrt{2}$.\n\nThis provides an upper bound for the smallest singular value. Since the operator $\\mathcal{D}$ on the space of mean-zero functions is invertible, its singular values are bounded away from zero. The family of spaces $\\text{span}\\{T_1, \\dots, T_N\\}$ is nested. The smallest singular value $\\sigma_{\\min}(N)$ is a non-increasing function of $N$ and is bounded below by a positive constant. Therefore, it converges to a positive constant as $N \\to \\infty$.\n$$\n\\sigma_{\\min}(N) \\sim \\mathcal{O}(1)\n$$\nThe right singular vector associated with $\\sigma_{\\min}(N)$ is dominated by the lowest-frequency mode, $T_1(x)$.\n\n### **Analysis of the Maximum Singular Value, $\\sigma_{\\max}(N)$**\n\nThe maximum singular value is the operator norm $\\|D|_{\\mathcal{M}}\\|_{2,w}$. To find its scaling, we can construct a test vector $v \\in \\mathcal{M}$ that is greatly amplified by $D$. The differentiation operator is known to produce large derivatives for functions that vary rapidly. On the CGL grid, the node density is highest near the endpoints $x=\\pm 1$. This suggests that a function localized near an endpoint will have a large discrete derivative.\n\nLet's consider the Lagrange basis polynomial $p_0(x)=L_0(x)$ associated with the node $x_0 = 1$. The corresponding vector $v^{(0)}$ has components $v^{(0)}_j = \\delta_{j0}$. This vector is not in the mean-zero subspace $\\mathcal{M}$, since $\\sum_{j=0}^N \\omega_j v^{(0)}_j = \\omega_0 \\neq 0$. We project $v^{(0)}$ onto $\\mathcal{M}$ to get a valid test vector $\\tilde{v}$. The vector corresponding to the constant function $T_0(x)=1$ is $\\mathbf{1}=(1,1,\\dots,1)^T$. The projection is:\n$$\n\\tilde{v} = v^{(0)} - \\frac{\\langle v^{(0)}, \\mathbf{1} \\rangle_{2,w}}{\\|\\mathbf{1}\\|_{2,w}^2} \\mathbf{1}\n$$\nWe have $\\langle v^{(0)}, \\mathbf{1} \\rangle_{2,w} = \\sum_{j=0}^N \\omega_j \\delta_{j0} (1) = \\omega_0$. The sum of the quadrature weights is exact for the integral of $T_0(x)=1$: $\\|\\mathbf{1}\\|_{2,w}^2 = \\sum_{j=0}^N \\omega_j = \\int_{-1}^1 \\frac{1}{\\sqrt{1-x^2}}dx = \\pi$.\nSo, $\\tilde{v} = v^{(0)} - \\frac{\\omega_0}{\\pi} \\mathbf{1}$.\n\nNow we evaluate the norm of $\\tilde{v}$ and $D\\tilde{v}$.\nThe squared norm of $\\tilde{v}$ is:\n$$\n\\|\\tilde{v}\\|_{2,w}^2 = \\|v^{(0)}\\|_{2,w}^2 - \\frac{|\\langle v^{(0)}, \\mathbf{1} \\rangle_{2,w}|^2}{\\|\\mathbf{1}\\|_{2,w}^2} = \\omega_0 - \\frac{\\omega_0^2}{\\pi}\n$$\nFor the commonly used Clenshaw-Curtis quadrature weights, $\\omega_0 = \\frac{\\pi}{2N}$. Thus, $\\|\\tilde{v}\\|_{2,w}^2 = \\frac{\\pi}{2N} - \\mathcal{O}(N^{-2})$, which scales as $\\mathcal{O}(N^{-1})$.\n\nNext, we evaluate $D\\tilde{v}$. Since $D$ applied to a constant vector is zero, $D\\mathbf{1}=0$.\n$$\nD\\tilde{v} = D\\left(v^{(0)} - \\frac{\\omega_0}{\\pi} \\mathbf{1}\\right) = Dv^{(0)}\n$$\nThe vector $Dv^{(0)}$ is the $0$-th column of the differentiation matrix $D$. Its components are $(Dv^{(0)})_j = D_{j0}$. The squared norm is:\n$$\n\\|D\\tilde{v}\\|_{2,w}^2 = \\|Dv^{(0)}\\|_{2,w}^2 = \\sum_{j=0}^{N} \\omega_j (D_{j0})^2\n$$\nThe entries of the Chebyshev differentiation matrix are known. The dominant entry in the $0$-th column is the diagonal element:\n$$\nD_{00} = \\frac{2N^2+1}{6} \\sim \\mathcal{O}(N^2)\n$$\nThe off-diagonal entries $D_{j0}$ are smaller. The contribution from the $j=0$ term to the sum will dominate the scaling. Let's approximate the norm by this single term:\n$$\n\\|D\\tilde{v}\\|_{2,w}^2 \\approx \\omega_0 (D_{00})^2 \\approx \\left(\\frac{\\pi}{2N}\\right) \\left(\\frac{2N^2+1}{6}\\right)^2 \\approx \\frac{\\pi}{2N} \\frac{4N^4}{36} = \\frac{\\pi}{18} N^3\n$$\nThe squared norm scales as $\\mathcal{O}(N^3)$.\nThe Rayleigh quotient for this test vector $\\tilde{v}$ is:\n$$\n\\frac{\\|D\\tilde{v}\\|_{2,w}^2}{\\|\\tilde{v}\\|_{2,w}^2} \\approx \\frac{\\mathcal{O}(N^3)}{\\mathcal{O}(N^{-1})} = \\mathcal{O}(N^4)\n$$\nSince $\\sigma_{\\max}^2(N) \\ge \\frac{\\|D\\tilde{v}\\|_{2,w}^2}{\\|\\tilde{v}\\|_{2,w}^2}$, we have $\\sigma_{\\max}^2(N) \\sim \\mathcal{O}(N^4)$, which implies:\n$$\n\\sigma_{\\max}(N) \\sim \\mathcal{O}(N^2)\n$$\nThis is a well-known result for Chebyshev differentiation. The right singular vector associated with $\\sigma_{\\max}(N)$ is not a single Chebyshev polynomial but rather a function highly localized near one of the boundaries ($x=\\pm 1$), which is a broadband superposition of many $T_k(x)$ modes.\n\n### **Condition Number Scaling**\nThe condition number $\\kappa(N)$ is the ratio of the largest to the smallest singular value:\n$$\n\\kappa(N) = \\frac{\\sigma_{\\max}(N)}{\\sigma_{\\min}(N)} \\sim \\frac{\\mathcal{O}(N^2)}{\\mathcal{O}(1)} = \\mathcal{O}(N^2)\n$$\nThe problem asks for the exponent $\\alpha$ in the scaling law $\\kappa(N) \\sim C N^\\alpha$. Comparing with our result, we find $\\alpha=2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "While the differentiation operator itself is intrinsically ill-conditioned, our practical choices in implementing spectral methods can either exacerbate or mitigate numerical stability issues. A crucial choice is the basis used to represent polynomials. This exercise provides a concrete comparison between a naive monomial basis and a basis of orthonormal Legendre polynomials . By constructing the Vandermonde matrix, which maps basis coefficients to nodal values, for each case, you will directly compute and compare their condition numbers, revealing why orthogonal bases are essential for maintaining accuracy in numerical computations.",
            "id": "3372519",
            "problem": "Consider spectral collocation on the interval $[-1,1]$ with $3$ distinct interpolation nodes $x_{0}=-1$, $x_{1}=0$, $x_{2}=1$. Let $\\{\\psi_{0}(x),\\psi_{1}(x),\\psi_{2}(x)\\}$ be a modal basis and define the Vandermonde matrix $V \\in \\mathbb{R}^{3 \\times 3}$ and its derivative-evaluation counterpart $V_{x} \\in \\mathbb{R}^{3 \\times 3}$ by $(V)_{ij}=\\psi_{j}(x_{i})$ and $(V_{x})_{ij}=\\psi_{j}'(x_{i})$ for $i,j \\in \\{0,1,2\\}$. For any quadratic polynomial $u(x)$, define the nodal differentiation operator $D \\in \\mathbb{R}^{3 \\times 3}$ by the requirement $u'(x_{i})=\\sum_{j=0}^{2} D_{ij} u(x_{j})$ for $i \\in \\{0,1,2\\}$. Starting only from the uniqueness of polynomial interpolation and the definition of these evaluation matrices, derive $D$ in terms of $V$ and $V_{x}$, and determine whether column-wise rescaling of the modal basis leaves $D$ invariant.\n\nNow consider two specific modal bases:\n- The monomial basis $\\{\\psi_{0}(x)=1,\\ \\psi_{1}(x)=x,\\ \\psi_{2}(x)=x^{2}\\}$.\n- The $L^{2}([-1,1])$-orthonormal Legendre basis $\\{\\phi_{0},\\phi_{1},\\phi_{2}\\}$ given by $\\phi_{n}(x)=\\sqrt{\\frac{2n+1}{2}}P_{n}(x)$, where $P_{n}(x)$ is the degree-$n$ Legendre polynomial on $[-1,1]$.\n\nFor each basis, form the associated Vandermonde matrix at the nodes $x_{0},x_{1},x_{2}$ and compute its Euclidean $2$-norm condition number $\\kappa_{2}(V)=\\sigma_{\\max}(V)/\\sigma_{\\min}(V)$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values, respectively, as defined by the Singular Value Decomposition (SVD). Using only first principles of polynomial interpolation and linear algebra, obtain exact closed-form expressions for these condition numbers. Then compute the exact closed-form expression for the ratio\n$$\nR \\;=\\; \\frac{\\kappa_{2}\\big(V_{\\mathrm{mono}}\\big)}{\\kappa_{2}\\big(V_{\\mathrm{orth}}\\big)} \\, .\n$$\nYour final answer must be this single expression for $R$. Do not approximate; do not round. Express the final answer in a closed form.",
            "solution": "We proceed from fundamental interpolation and linear algebra principles. For any quadratic polynomial $u(x)$, there exists a unique coefficient vector $a \\in \\mathbb{R}^{3}$ in the chosen modal basis $\\{\\psi_{0},\\psi_{1},\\psi_{2}\\}$ such that\n$$\nu(x) \\;=\\; \\sum_{j=0}^{2} a_{j}\\,\\psi_{j}(x).\n$$\nEvaluating at the nodes gives the nodal value vector $u \\in \\mathbb{R}^{3}$ with entries $u_{i}=u(x_{i})$. By definition of the Vandermonde matrix, this reads\n$$\nu \\;=\\; V\\,a.\n$$\nDifferentiating the polynomial and evaluating at the nodes gives $u'(x_{i})=\\sum_{j=0}^{2} a_{j}\\,\\psi_{j}'(x_{i})$, hence the vector of nodal derivatives satisfies\n$$\nu' \\;=\\; V_{x}\\,a.\n$$\nSince $V$ is nonsingular for distinct nodes and a basis, we have $a=V^{-1}u$. Substituting into the derivative relation yields\n$$\nu' \\;=\\; V_{x}\\,V^{-1}\\,u.\n$$\nBy the definition of the nodal differentiation operator $D$, namely $u'=D\\,u$ for all quadratic polynomials $u$, we conclude\n$$\nD \\;=\\; V_{x}\\,V^{-1}.\n$$\nNow consider column-wise rescaling of the modal basis. Let $\\tilde{\\psi}_{j}(x)=s_{j}\\,\\psi_{j}(x)$ with nonzero scalars $s_{j}$, and define the diagonal matrix $S=\\mathrm{diag}(s_{0},s_{1},s_{2})$. Then the new matrices satisfy $\\tilde{V}=V\\,S$ and $\\tilde{V}_{x}=V_{x}\\,S$. The corresponding nodal differentiation operator is\n$$\n\\tilde{D} \\;=\\; \\tilde{V}_{x}\\,\\tilde{V}^{-1} \\;=\\; V_{x}\\,S\\,(S^{-1}V^{-1}) \\;=\\; V_{x}\\,V^{-1} \\;=\\; D.\n$$\nTherefore, column-wise scaling (including $L^{2}$-orthonormalization) leaves $D$ invariant. However, the conditioning of $V$ generally changes under such scaling.\n\nWe now specialize to the two given bases at the nodes $x_{0}=-1$, $x_{1}=0$, $x_{2}=1$.\n\nMonomial basis. With $\\psi_{0}(x)=1$, $\\psi_{1}(x)=x$, $\\psi_{2}(x)=x^{2}$, the Vandermonde matrix is\n$$\nV_{\\mathrm{mono}} \\;=\\;\n\\begin{pmatrix}\n\\psi_{0}(-1) & \\psi_{1}(-1) & \\psi_{2}(-1) \\\\\n\\psi_{0}(0) & \\psi_{1}(0) & \\psi_{2}(0) \\\\\n\\psi_{0}(1) & \\psi_{1}(1) & \\psi_{2}(1)\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 & -1 & 1 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\nThe singular values of $V_{\\mathrm{mono}}$ are the square roots of the eigenvalues of $V_{\\mathrm{mono}}^{\\top}V_{\\mathrm{mono}}$. Compute\n$$\nV_{\\mathrm{mono}}^{\\top}V_{\\mathrm{mono}} \\;=\\;\n\\begin{pmatrix}\n3 & 0 & 2 \\\\\n0 & 2 & 0 \\\\\n2 & 0 & 2\n\\end{pmatrix}.\n$$\nThis symmetric matrix has a decoupled middle coordinate with eigenvalue $\\lambda_{2}=2$, and a $2\\times 2$ block on indices $\\{1,3\\}$ equal to $\\begin{pmatrix}3 & 2 \\\\ 2 & 2\\end{pmatrix}$, whose eigenvalues solve\n$$\n\\lambda^{2} - 5\\lambda + 2 \\;=\\; 0,\n$$\nso $\\lambda_{\\pm}=\\frac{5 \\pm \\sqrt{17}}{2}$. Hence the eigenvalues of $V_{\\mathrm{mono}}^{\\top}V_{\\mathrm{mono}}$ are $\\left\\{\\frac{5+\\sqrt{17}}{2},\\,2,\\,\\frac{5-\\sqrt{17}}{2}\\right\\}$, and the Euclidean $2$-norm condition number is\n$$\n\\kappa_{2}\\big(V_{\\mathrm{mono}}\\big) \\;=\\; \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}}\n\\;=\\; \\sqrt{\\frac{\\frac{5+\\sqrt{17}}{2}}{\\frac{5-\\sqrt{17}}{2}}}\n\\;=\\; \\sqrt{\\frac{5+\\sqrt{17}}{5-\\sqrt{17}}}.\n$$\n\n$L^{2}([-1,1])$-orthonormal Legendre basis. With $\\phi_{n}(x)=\\sqrt{\\frac{2n+1}{2}}P_{n}(x)$, we evaluate at $x \\in \\{-1,0,1\\}$. Using $P_{0}(x)=1$, $P_{1}(x)=x$, $P_{2}(x)=\\frac{1}{2}(3x^{2}-1)$, we have\n$$\n\\phi_{0}(x)=\\sqrt{\\frac{1}{2}}\\cdot 1,\\quad\n\\phi_{1}(x)=\\sqrt{\\frac{3}{2}}\\cdot x,\\quad\n\\phi_{2}(x)=\\sqrt{\\frac{5}{2}}\\cdot \\frac{1}{2}(3x^{2}-1).\n$$\nThus\n$$\nV_{\\mathrm{orth}} \\;=\\;\n\\begin{pmatrix}\n\\phi_{0}(-1) & \\phi_{1}(-1) & \\phi_{2}(-1) \\\\\n\\phi_{0}(0) & \\phi_{1}(0) & \\phi_{2}(0) \\\\\n\\phi_{0}(1) & \\phi_{1}(1) & \\phi_{2}(1)\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\sqrt{\\frac{1}{2}} & -\\sqrt{\\frac{3}{2}} & \\sqrt{\\frac{5}{2}} \\\\\n\\sqrt{\\frac{1}{2}} & 0 & -\\sqrt{\\frac{5}{2}}\\cdot \\frac{1}{2} \\\\\n\\sqrt{\\frac{1}{2}} & \\sqrt{\\frac{3}{2}} & \\sqrt{\\frac{5}{2}}\n\\end{pmatrix}.\n$$\nCompute $V_{\\mathrm{orth}}^{\\top}V_{\\mathrm{orth}}$ by column inner products. Let the columns be $c_{1}$, $c_{2}$, $c_{3}$. Then\n$$\nc_{1}\\cdot c_{1} \\;=\\; \\frac{3}{2},\\quad\nc_{2}\\cdot c_{2} \\;=\\; 3,\\quad\nc_{3}\\cdot c_{3} \\;=\\; \\frac{45}{8},\\quad\nc_{1}\\cdot c_{2} \\;=\\; 0,\\quad\nc_{2}\\cdot c_{3} \\;=\\; 0,\\quad\nc_{1}\\cdot c_{3} \\;=\\; \\frac{3\\sqrt{5}}{4}.\n$$\nTherefore\n$$\nV_{\\mathrm{orth}}^{\\top}V_{\\mathrm{orth}} \\;=\\;\n\\begin{pmatrix}\n\\frac{3}{2} & 0 & \\frac{3\\sqrt{5}}{4} \\\\\n0 & 3 & 0 \\\\\n\\frac{3\\sqrt{5}}{4} & 0 & \\frac{45}{8}\n\\end{pmatrix}.\n$$\nAgain the middle coordinate decouples with eigenvalue $\\lambda_{2}=3$. The $2\\times 2$ block on indices $\\{1,3\\}$ is $\\begin{pmatrix}\\frac{3}{2} & \\frac{3\\sqrt{5}}{4} \\\\ \\frac{3\\sqrt{5}}{4} & \\frac{45}{8}\\end{pmatrix}$, with trace\n$$\nt \\;=\\; \\frac{3}{2}+\\frac{45}{8} \\;=\\; \\frac{57}{8},\n$$\nand determinant\n$$\n\\det \\;=\\; \\frac{3}{2}\\cdot \\frac{45}{8} - \\left(\\frac{3\\sqrt{5}}{4}\\right)^{2}\n\\;=\\; \\frac{135}{16} - \\frac{45}{16} \\;=\\; \\frac{45}{8}.\n$$\nHence the two eigenvalues of this block are\n$$\n\\lambda_{\\pm} \\;=\\; \\frac{t \\pm \\sqrt{t^{2}-4\\det}}{2}\n\\;=\\; \\frac{\\frac{57}{8} \\pm \\sqrt{\\frac{57^{2}}{64} - \\frac{4\\cdot 45}{8}}}{2}\n\\;=\\; \\frac{\\frac{57}{8} \\pm \\frac{\\sqrt{1809}}{8}}{2}\n\\;=\\; \\frac{57 \\pm \\sqrt{1809}}{16}.\n$$\nThus the eigenvalues of $V_{\\mathrm{orth}}^{\\top}V_{\\mathrm{orth}}$ are $\\left\\{\\frac{57+\\sqrt{1809}}{16},\\,3,\\,\\frac{57-\\sqrt{1809}}{16}\\right\\}$, and the Euclidean $2$-norm condition number is\n$$\n\\kappa_{2}\\big(V_{\\mathrm{orth}}\\big) \\;=\\; \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}}\n\\;=\\; \\sqrt{\\frac{\\frac{57+\\sqrt{1809}}{16}}{\\frac{57-\\sqrt{1809}}{16}}}\n\\;=\\; \\sqrt{\\frac{57+\\sqrt{1809}}{57-\\sqrt{1809}}}.\n$$\n\nFinally, the requested ratio is\n$$\nR \\;=\\; \\frac{\\kappa_{2}\\big(V_{\\mathrm{mono}}\\big)}{\\kappa_{2}\\big(V_{\\mathrm{orth}}\\big)}\n\\;=\\; \\sqrt{\\frac{\\frac{5+\\sqrt{17}}{5-\\sqrt{17}}}{\\frac{57+\\sqrt{1809}}{57-\\sqrt{1809}}}}\n\\;=\\; \\sqrt{\\frac{(5+\\sqrt{17})(57-\\sqrt{1809})}{(5-\\sqrt{17})(57+\\sqrt{1809})}}.\n$$\nThis computation demonstrates that $L^{2}$-orthonormalization rescales the columns of $V$ in a way that reduces $\\kappa_{2}(V)$ for these nodes, while leaving the exact nodal differentiation operator $D$ invariant, thereby mitigating the numerical deterioration that would otherwise arise from forming $D$ via operations involving ill-conditioned $V$.",
            "answer": "$$\\boxed{\\sqrt{\\frac{(5+\\sqrt{17})(57-\\sqrt{1809})}{(5-\\sqrt{17})(57+\\sqrt{1809})}}}$$"
        },
        {
            "introduction": "After understanding the sources of ill-conditioning, the natural next step is to find ways to cure it. This is the role of preconditioning, which transforms a poorly conditioned system into a well-conditioned one that is easier for iterative solvers to handle. This practice offers a crystal-clear illustration of this principle in the elegant context of Fourier spectral methods . You will construct a discrete integration operator and show that it acts as a perfect left preconditioner for the differentiation operator on the mean-zero subspace, dramatically reducing the condition number from $\\mathcal{O}(N)$ to an ideal value of $1$.",
            "id": "3372558",
            "problem": "Consider the $2\\pi$-periodic interval and an even number $N \\geq 4$ of equispaced nodes $x_{j} = -\\pi + \\frac{2\\pi j}{N}$ for $j = 0, 1, \\dots, N-1$. Let $\\mathbf{u} \\in \\mathbb{C}^{N}$ denote nodal values of a trigonometric interpolant. Define the Fourier spectral differentiation operator $D$ to be the linear map that takes $\\mathbf{u}$ to the nodal values of the derivative of its $2\\pi$-periodic trigonometric interpolant evaluated at $\\{x_{j}\\}_{j=0}^{N-1}$. Let $\\mathcal{Z} \\subset \\mathbb{C}^{N}$ denote the discrete mean-zero subspace,\n$$\n\\mathcal{Z} = \\left\\{\\mathbf{u} \\in \\mathbb{C}^{N} : \\sum_{j=0}^{N-1} u_{j} = 0 \\right\\}.\n$$\nAdopt the unitary Discrete Fourier Transform (DFT) matrix $F$ so that $F$ diagonalizes circulant operators. With the standard ordering of wavenumbers $\\{k\\}$ as $k = 0, 1, \\dots, \\frac{N}{2}, -\\frac{N}{2}+1, \\dots, -1$, the spectral symbol of the differentiation operator is the diagonal matrix $\\Lambda = \\mathrm{diag}(i k)$.\n\nConstruct a discrete integration operator $I$ by its Fourier-space action that maps nonzero wavenumber coefficients $\\hat{u}_{k}$ to $\\hat{v}_{k} = \\frac{1}{i k}\\hat{u}_{k}$ for $k \\neq 0$ and sets the zero-wavenumber coefficient to zero, i.e., use the diagonal matrix $\\Lambda^{\\dagger} = \\mathrm{diag}(\\mu_{k})$ with $\\mu_{0} = 0$ and $\\mu_{k} = \\frac{1}{i k}$ for $k \\neq 0$, and define $I = F^{*}\\Lambda^{\\dagger}F$. Let $P$ denote the orthogonal projection onto $\\mathcal{Z}$ induced by zeroing the $k=0$ mode in Fourier space, i.e., $P = F^{*}\\Pi F$ with $\\Pi = \\mathrm{diag}(\\pi_{k})$, $\\pi_{0} = 0$ and $\\pi_{k} = 1$ for $k \\neq 0$.\n\nStarting from these definitions and first principles of Fourier spectral collocation, show that $ID = P$. Then, using the $2$-norm condition number $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$ for a nonsingular operator $A$, consider the restriction of operators to the mean-zero subspace $\\mathcal{Z}$. Determine the exact closed-form expression for\n$$\nr(N) = \\frac{\\kappa_{2}\\!\\left(ID\\big|_{\\mathcal{Z}}\\right)}{\\kappa_{2}\\!\\left(D\\big|_{\\mathcal{Z}}\\right)}\n$$\nas a function of $N$. Express your final answer as a simplified analytic expression. Do not provide an inequality. No rounding is required.",
            "solution": "The problem asks for two main results. First, to show that the product of the discrete integration operator $I$ and the discrete differentiation operator $D$ is equal to the orthogonal projection operator $P$ onto the mean-zero subspace $\\mathcal{Z}$. Second, to determine the ratio $r(N)$ of the $2$-norm condition numbers of the operators $ID$ and $D$, both restricted to the subspace $\\mathcal{Z}$.\n\nWe begin with the first part: proving the identity $ID=P$.\nThe operators $D$, $I$, and $P$ are defined in terms of the unitary Discrete Fourier Transform (DFT) matrix $F$ and their respective symbols, which are diagonal matrices. The definitions are:\n$D = F^{*}\\Lambda F$, with $\\Lambda = \\mathrm{diag}(ik)$\n$I = F^{*}\\Lambda^{\\dagger}F$, with $\\Lambda^{\\dagger} = \\mathrm{diag}(\\mu_{k})$ where $\\mu_{0}=0$ and $\\mu_{k} = \\frac{1}{ik}$ for $k \\neq 0$.\n$P = F^{*}\\Pi F$, with $\\Pi = \\mathrm{diag}(\\pi_{k})$ where $\\pi_{0}=0$ and $\\pi_{k} = 1$ for $k \\neq 0$.\n\nLet us compute the product $ID$:\n$$\nID = (F^{*}\\Lambda^{\\dagger}F)(F^{*}\\Lambda F)\n$$\nSince the DFT matrix $F$ is unitary, we have $F F^{*} = \\mathbb{I}$, where $\\mathbb{I}$ is the $N \\times N$ identity matrix. The product simplifies to:\n$$\nID = F^{*}\\Lambda^{\\dagger}(F F^{*})\\Lambda F = F^{*}\\Lambda^{\\dagger}\\mathbb{I}\\Lambda F = F^{*}(\\Lambda^{\\dagger}\\Lambda)F\n$$\nNow we must compute the matrix product $\\Lambda^{\\dagger}\\Lambda$. Since both $\\Lambda^{\\dagger}$ and $\\Lambda$ are diagonal matrices, their product is also a diagonal matrix. The $k$-th diagonal entry of $\\Lambda^{\\dagger}\\Lambda$ is the product of the $k$-th diagonal entries of $\\Lambda^{\\dagger}$ and $\\Lambda$. Let us denote the diagonal entries of $\\Lambda^{\\dagger}$ as $\\mu_k$ and those of $\\Lambda$ as $\\lambda_k = ik$.\n\nFor the wavenumber $k=0$:\nThe corresponding diagonal entry is $\\mu_{0} \\cdot \\lambda_{0} = 0 \\cdot (i \\cdot 0) = 0$.\n\nFor any non-zero wavenumber $k \\neq 0$ from the set $\\{1, \\dots, \\frac{N}{2}, -\\frac{N}{2}+1, \\dots, -1\\}$:\nThe corresponding diagonal entry is $\\mu_{k} \\cdot \\lambda_{k} = \\left(\\frac{1}{ik}\\right) \\cdot (ik) = 1$.\n\nThus, the product matrix $\\Lambda^{\\dagger}\\Lambda$ is a diagonal matrix whose entry corresponding to $k=0$ is $0$ and all other diagonal entries are $1$. This is precisely the definition of the matrix $\\Pi = \\mathrm{diag}(\\pi_{k})$.\nTherefore, $\\Lambda^{\\dagger}\\Lambda = \\Pi$.\nSubstituting this back into the expression for $ID$, we get:\n$$\nID = F^{*}\\Pi F\n$$\nBy definition, $P = F^{*}\\Pi F$. We have thus shown that $ID = P$.\n\nNext, we proceed to the second part: the determination of $r(N) = \\frac{\\kappa_{2}\\!\\left(ID\\big|_{\\mathcal{Z}}\\right)}{\\kappa_{2}\\!\\left(D\\big|_{\\mathcal{Z}}\\right)}$.\nThis involves calculating the $2$-norm condition numbers of the operators $ID$ and $D$ when their domain is restricted to the mean-zero subspace $\\mathcal{Z}$. The space $\\mathcal{Z}$ consists of all vectors $\\mathbf{u} \\in \\mathbb{C}^{N}$ such that their $k=0$ Fourier coefficient is zero. The operator $P$ is the orthogonal projection onto $\\mathcal{Z}$.\n\nLet's first analyze the numerator, $\\kappa_{2}\\!\\left(ID\\big|_{\\mathcal{Z}}\\right)$.\nFrom the first part, we have $ID = P$. So we need to calculate $\\kappa_{2}\\!\\left(P\\big|_{\\mathcal{Z}}\\right)$. The operator $P\\big|_{\\mathcal{Z}}$ is the restriction of the projection $P$ to its own range, $\\mathcal{Z}$. By definition of a projection, for any vector $\\mathbf{u}$ in the range of $P$, we have $P\\mathbf{u} = \\mathbf{u}$. Thus, for any $\\mathbf{u} \\in \\mathcal{Z}$, $P\\mathbf{u} = \\mathbf{u}$. This means that $P\\big|_{\\mathcal{Z}}$ is the identity operator on the subspace $\\mathcal{Z}$, let's call it $\\mathbb{I}_{\\mathcal{Z}}$.\nThe $2$-norm condition number of any identity operator on a non-trivial vector space is $1$. Formally, $\\kappa_{2}(\\mathbb{I}_{\\mathcal{Z}}) = \\|\\mathbb{I}_{\\mathcal{Z}}\\|_{2} \\|(\\mathbb{I}_{\\mathcal{Z}})^{-1}\\|_{2}$. The inverse $(\\mathbb{I}_{\\mathcal{Z}})^{-1}$ is $\\mathbb{I}_{\\mathcal{Z}}$ itself. The $2$-norm of the identity operator is $\\|\\mathbb{I}_{\\mathcal{Z}}\\|_{2} = \\sup_{\\mathbf{u} \\in \\mathcal{Z}, \\|\\mathbf{u}\\|_{2}=1} \\|\\mathbb{I}_{\\mathcal{Z}}\\mathbf{u}\\|_{2} = \\sup_{\\mathbf{u} \\in \\mathcal{Z}, \\|\\mathbf{u}\\|_{2}=1} \\|\\mathbf{u}\\|_{2} = 1$.\nTherefore,\n$$\n\\kappa_{2}\\!\\left(ID\\big|_{\\mathcal{Z}}\\right) = \\kappa_{2}\\!\\left(P\\big|_{\\mathcal{Z}}\\right) = 1 \\cdot 1 = 1\n$$\n\nNow, let's analyze the denominator, $\\kappa_{2}\\!\\left(D\\big|_{\\mathcal{Z}}\\right)$.\nThe condition number is given by $\\kappa_{2}\\!\\left(D\\big|_{\\mathcal{Z}}\\right) = \\|D\\big|_{\\mathcal{Z}}\\|_{2} \\|(D\\big|_{\\mathcal{Z}})^{-1}\\|_{2}$.\nThe operator $D = F^{*}\\Lambda F$ is a normal matrix because $F$ is unitary and $\\Lambda$ is diagonal, hence $D^{*}D = DD^{*}$. An important property of a normal operator is that its $2$-norm is equal to its spectral radius (the maximum absolute value of its eigenvalues).\nThe eigenvalues of $D$ are the diagonal entries of $\\Lambda$, which are $\\{ik\\}$ for the given set of wavenumbers $k = 0, 1, \\dots, \\frac{N}{2}, -\\frac{N}{2}+1, \\dots, -1$. The corresponding eigenvectors are the columns of $F^{*}$, which are the complex exponentials sampled at the nodes $\\{x_j\\}$.\nThe subspace $\\mathcal{Z}$ is the space of vectors with a zero mean, which corresponds to having a zero $k=0$ Fourier coefficient. This means $\\mathcal{Z}$ is spanned by the eigenvectors of $D$ corresponding to all non-zero wavenumbers. Therefore, $\\mathcal{Z}$ is an invariant subspace of $D$. The operator $D\\big|_{\\mathcal{Z}}$ maps $\\mathcal{Z}$ to itself, and its eigenvalues are precisely the eigenvalues of $D$ for $k \\neq 0$, i.e., $\\{ik\\}$ for $k \\in \\{1, \\dots, \\frac{N}{2}, -\\frac{N}{2}+1, \\dots, -1\\}$.\nSince $D\\big|_{\\mathcal{Z}}$ is also a normal operator (as a restriction of a normal operator to an invariant subspace spanned by its eigenvectors), its $2$-norm is the maximum of the absolute values of its eigenvalues:\n$$\n\\|D\\big|_{\\mathcal{Z}}\\|_{2} = \\max_{k \\neq 0} |ik| = \\max_{k \\neq 0} |k|\n$$\nThe set of non-zero wavenumbers is $k \\in \\{1, 2, \\dots, \\frac{N}{2}\\} \\cup \\{-1, -2, \\dots, -\\frac{N}{2}+1\\}$. The absolute values are $|k| \\in \\{1, 2, \\dots, \\frac{N}{2}\\}$. The maximum value is achieved at $k=\\frac{N}{2}$.\n$$\n\\|D\\big|_{\\mathcal{Z}}\\|_{2} = \\frac{N}{2}\n$$\nNext, we find the norm of the inverse operator $(D\\big|_{\\mathcal{Z}})^{-1}$. Since all eigenvalues of $D\\big|_{\\mathcal{Z}}$ are non-zero, this operator is invertible. The eigenvalues of $(D\\big|_{\\mathcal{Z}})^{-1}$ are the reciprocals of the eigenvalues of $D\\big|_{\\mathcal{Z}}$, which are $\\{\\frac{1}{ik}\\}$ for $k \\neq 0$.\nThe $2$-norm of the inverse is the maximum absolute value of its eigenvalues:\n$$\n\\|(D\\big|_{\\mathcal{Z}})^{-1}\\|_{2} = \\max_{k \\neq 0} \\left|\\frac{1}{ik}\\right| = \\max_{k \\neq 0} \\frac{1}{|k|} = \\frac{1}{\\min_{k \\neq 0} |k|}\n$$\nThe minimum absolute value of the non-zero wavenumbers is $1$, which occurs for $k=1$ and $k=-1$.\n$$\n\\|(D\\big|_{\\mathcal{Z}})^{-1}\\|_{2} = \\frac{1}{1} = 1\n$$\nNow we can compute the condition number of $D\\big|_{\\mathcal{Z}}$:\n$$\n\\kappa_{2}\\!\\left(D\\big|_{\\mathcal{Z}}\\right) = \\|D\\big|_{\\mathcal{Z}}\\|_{2} \\|(D\\big|_{\\mathcal{Z}})^{-1}\\|_{2} = \\frac{N}{2} \\cdot 1 = \\frac{N}{2}\n$$\nFinally, we compute the ratio $r(N)$:\n$$\nr(N) = \\frac{\\kappa_{2}\\!\\left(ID\\big|_{\\mathcal{Z}}\\right)}{\\kappa_{2}\\!\\left(D\\big|_{\\mathcal{Z}}\\right)} = \\frac{1}{\\frac{N}{2}} = \\frac{2}{N}\n$$\nThis is the simplified analytic expression for the ratio as a function of $N \\geq 4$.",
            "answer": "$$\\boxed{\\frac{2}{N}}$$"
        }
    ]
}