{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on practice by constructing a spectral differentiation matrix from scratch for a simple, low-degree case. This exercise  will provide a concrete feel for the relationship between Lagrange interpolating polynomials and the entries of the differentiation operator $D$. By also forming a related symmetric stiffness-like matrix $S$, you will gain insight into how appropriate weighting transforms a non-normal operator into a symmetric one, a crucial step in formulating stable numerical methods for differential equations.",
            "id": "3372555",
            "problem": "Consider the Legendre–Gauss–Lobatto (LGL) collocation at polynomial degree $n=2$ on the interval $[-1,1]$ with nodes $x_0=-1$, $x_1=0$, $x_2=1$ and quadrature weights $w_0=w_2=\\frac{1}{3}$ and $w_1=\\frac{4}{3}$. Let $\\{\\ell_j(x)\\}_{j=0}^2$ be the Lagrange cardinal polynomials associated with these nodes, and let the spectral differentiation matrix $D\\in\\mathbb{R}^{3\\times 3}$ be defined by $D_{ij}=\\ell_j'(x_i)$, $0\\le i,j\\le 2$. Let the diagonal mass matrix be $W=\\mathrm{diag}(w_0,w_1,w_2)$, and define the diagonally weighted, symmetric stiffness-like matrix\n$$\nS \\;=\\; -\\,W^{-1/2}\\,D^{\\top}\\,W\\,D\\,W^{-1/2}\\,\\in\\mathbb{R}^{3\\times 3}.\n$$\nYour tasks are:\n- Using only the definitions above and first principles (e.g., properties of Lagrange polynomials and standard matrix norms), construct $D$ explicitly and form $S$.\n- Invoke the Gershgorin circle theorem to derive Gershgorin disks for $D$ and for $S$, and write their unions as intervals on the real line (noting that $S$ is symmetric).\n- Compute the exact eigenvalues of $D$ and of $S$ to assess the tightness of the Gershgorin bounds and to discuss qualitative conditioning insights afforded by the diagonal weight scaling that maps $D$ to $S$.\n\nFinally, let $\\rho$ denote the spectral radius (maximum absolute eigenvalue) of $S$, and let $\\rho_G$ denote the smallest uniform bound on $|\\lambda|$ implied by the Gershgorin intervals for $S$ (i.e., the maximum absolute value over the endpoints of the Gershgorin union). Provide the exact value of the ratio $\\rho_G/\\rho$ as your final answer. Express your final answer in exact form; no rounding is required.",
            "solution": "The problem asks for a detailed analysis of a spectral differentiation matrix $D$ and a related symmetric stiffness-like matrix $S$ for Legendre–Gauss–Lobatto (LGL) collocation at polynomial degree $n=2$. The tasks include constructing these matrices, applying the Gershgorin circle theorem, computing their exact eigenvalues, discussing conditioning, and finally calculating a specific ratio derived from the analysis of $S$.\n\nFirst, we construct the differentiation matrix $D$. The LGL nodes for $n=2$ are given as $x_0=-1$, $x_1=0$, and $x_2=1$. The corresponding Lagrange cardinal polynomials $\\{\\ell_j(x)\\}_{j=0}^2$ are:\n$$\n\\ell_0(x) = \\frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)} = \\frac{x(x-1)}{(-1)(-2)} = \\frac{1}{2}(x^2 - x)\n$$\n$$\n\\ell_1(x) = \\frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)} = \\frac{(x+1)(x-1)}{(1)(-1)} = 1 - x^2\n$$\n$$\n\\ell_2(x) = \\frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)} = \\frac{(x+1)x}{(2)(1)} = \\frac{1}{2}(x^2 + x)\n$$\nTheir derivatives are:\n$$\n\\ell_0'(x) = x - \\frac{1}{2}, \\quad \\ell_1'(x) = -2x, \\quad \\ell_2'(x) = x + \\frac{1}{2}\n$$\nThe entries of the differentiation matrix $D$ are $D_{ij} = \\ell_j'(x_i)$. Evaluating the derivatives at the nodes:\n$$\nD_{0j} = (\\ell_0'(-1), \\ell_1'(-1), \\ell_2'(-1)) = (-\\frac{3}{2}, 2, -\\frac{1}{2})\n$$\n$$\nD_{1j} = (\\ell_0'(0), \\ell_1'(0), \\ell_2'(0)) = (-\\frac{1}{2}, 0, \\frac{1}{2})\n$$\n$$\nD_{2j} = (\\ell_0'(1), \\ell_1'(1), \\ell_2'(1)) = (\\frac{1}{2}, -2, \\frac{3}{2})\n$$\nThus, the differentiation matrix is:\n$$\nD = \\begin{pmatrix} -3/2 & 2 & -1/2 \\\\ -1/2 & 0 & 1/2 \\\\ 1/2 & -2 & 3/2 \\end{pmatrix}\n$$\n\nNext, we construct the matrix $S$. The quadrature weights are given as $w_0=1/3$, $w_1=4/3$, $w_2=1/3$. The diagonal mass matrix is $W = \\mathrm{diag}(1/3, 4/3, 1/3)$, and its inverse square root is $W^{-1/2} = \\mathrm{diag}(\\sqrt{3}, \\sqrt{3}/2, \\sqrt{3})$.\nThe matrix $S$ is defined as $S = -W^{-1/2}\\,D^{\\top}\\,W\\,D\\,W^{-1/2}$. Let's first compute the stiffness matrix $K = D^\\top W D$:\n$$\nK = \\begin{pmatrix} -3/2 & -1/2 & 1/2 \\\\ 2 & 0 & -2 \\\\ -1/2 & 1/2 & 3/2 \\end{pmatrix} \\begin{pmatrix} 1/3 & 0 & 0 \\\\ 0 & 4/3 & 0 \\\\ 0 & 0 & 1/3 \\end{pmatrix} \\begin{pmatrix} -3/2 & 2 & -1/2 \\\\ -1/2 & 0 & 1/2 \\\\ 1/2 & -2 & 3/2 \\end{pmatrix}\n$$\n$$\nK = \\begin{pmatrix} -1/2 & -2/3 & 1/6 \\\\ 2/3 & 0 & -2/3 \\\\ -1/6 & 2/3 & 1/2 \\end{pmatrix} \\begin{pmatrix} -3/2 & 2 & -1/2 \\\\ -1/2 & 0 & 1/2 \\\\ 1/2 & -2 & 3/2 \\end{pmatrix} = \\begin{pmatrix} 7/6 & -4/3 & 1/6 \\\\ -4/3 & 8/3 & -4/3 \\\\ 1/6 & -4/3 & 7/6 \\end{pmatrix}\n$$\nNow we apply the scaling: $S = -W^{-1/2} K W^{-1/2}$.\n$$\nS = - \\begin{pmatrix} \\sqrt{3} & 0 & 0 \\\\ 0 & \\sqrt{3}/2 & 0 \\\\ 0 & 0 & \\sqrt{3} \\end{pmatrix} \\begin{pmatrix} 7/6 & -4/3 & 1/6 \\\\ -4/3 & 8/3 & -4/3 \\\\ 1/6 & -4/3 & 7/6 \\end{pmatrix} \\begin{pmatrix} \\sqrt{3} & 0 & 0 \\\\ 0 & \\sqrt{3}/2 & 0 \\\\ 0 & 0 & \\sqrt{3} \\end{pmatrix}\n$$\n$$\nS = - \\begin{pmatrix} 3(7/6) & (3/2)(-4/3) & 3(1/6) \\\\ (3/2)(-4/3) & (3/4)(8/3) & (3/2)(-4/3) \\\\ 3(1/6) & (3/2)(-4/3) & 3(7/6) \\end{pmatrix} = - \\begin{pmatrix} 7/2 & -2 & 1/2 \\\\ -2 & 2 & -2 \\\\ 1/2 & -2 & 7/2 \\end{pmatrix}\n$$\n$$\nS = \\begin{pmatrix} -7/2 & 2 & -1/2 \\\\ 2 & -2 & 2 \\\\ -1/2 & 2 & -7/2 \\end{pmatrix}\n$$\n\nNow, we apply the Gershgorin circle theorem. For a matrix $A$, the disks are $G_i = \\{z \\in \\mathbb{C} : |z - A_{ii}| \\leq \\sum_{j \\neq i} |A_{ij}|\\}$.\nFor $D$:\n$G_0$: center $z_0 = -3/2$, radius $R_0 = |2| + |-1/2| = 5/2$.\n$G_1$: center $z_1 = 0$, radius $R_1 = |-1/2| + |1/2| = 1$.\n$G_2$: center $z_2 = 3/2$, radius $R_2 = |1/2| + |-2| = 5/2$.\nThe problem asks for the union as intervals on the real line. This can be interpreted as the intersection of the union of the disks with the real axis.\n$G_0 \\cap \\mathbb{R} = [-3/2 - 5/2, -3/2 + 5/2] = [-4, 1]$.\n$G_1 \\cap \\mathbb{R} = [-1, 1]$.\n$G_2 \\cap \\mathbb{R} = [3/2 - 5/2, 3/2 + 5/2] = [-1, 4]$.\nThe union of these intervals is $[-4, 4]$.\n\nFor $S$, which is symmetric, its eigenvalues are real, so the Gershgorin disks are intervals on the real line.\n$G_0$: center $c_0 = -7/2$, radius $R_0 = |2| + |-1/2| = 5/2$. Interval is $[-7/2 - 5/2, -7/2 + 5/2] = [-6, -1]$.\n$G_1$: center $c_1 = -2$, radius $R_1 = |2| + |2| = 4$. Interval is $[-2 - 4, -2 + 4] = [-6, 2]$.\n$G_2$: center $c_2 = -7/2$, radius $R_2 = |-1/2| + |2| = 5/2$. Interval is $[-7/2 - 5/2, -7/2 + 5/2] = [-6, -1]$.\nThe union of the Gershgorin intervals for $S$ is $[-6, 2]$.\n\nNext, we compute the exact eigenvalues.\nFor $D$, the sum of each row is $0$, so $D\\mathbf{1}=\\mathbf{0}$, meaning $\\lambda=0$ is an eigenvalue. The characteristic polynomial is $\\det(D-\\lambda I)$.\n$\\mathrm{tr}(D) = -3/2 + 0 + 3/2 = 0$.\n$\\det(D) = (-3/2)(0 - (-1)) - 2(-3/4-1/4) + (-1/2)(1-0) = -3/2 - 2(-1) - 1/2 = -2+2=0$.\nThe sum of $2 \\times 2$ principal minors is $C_2 = (1) + (-9/4 - (-1/4)) + (1) = 1-2+1=0$.\nThe characteristic polynomial is $-\\lambda^3 + \\mathrm{tr}(D)\\lambda^2 - C_2\\lambda + \\det(D) = -\\lambda^3 = 0$.\nThus, all eigenvalues of $D$ are $0$. $D$ is a nilpotent matrix; specifically $D^3=0$. The Gershgorin bounds are correct (the union of disks contains $0$) but very loose.\n\nFor $S$, we know it is singular because $\\det(S) \\propto \\det(D^\\top W D) = \\det(D^\\top)\\det(W)\\det(D) = 0$. So $\\lambda_1=0$ is an eigenvalue.\nThe trace is $\\mathrm{tr}(S) = -7/2 - 2 - 7/2 = -9$. Since $\\lambda_1+\\lambda_2+\\lambda_3=-9$, we have $\\lambda_2+\\lambda_3=-9$.\nThe sum of $2 \\times 2$ principal minors is:\n$C_2 = \\det\\begin{pmatrix}-7/2 & 2 \\\\ 2 & -2\\end{pmatrix} + \\det\\begin{pmatrix}-7/2 & -1/2 \\\\ -1/2 & -7/2\\end{pmatrix} + \\det\\begin{pmatrix}-2 & 2 \\\\ 2 & -7/2\\end{pmatrix}$\n$C_2 = (7-4) + (49/4 - 1/4) + (7-4) = 3 + 12 + 3 = 18$.\nThe characteristic polynomial is $-\\lambda^3 - 9\\lambda^2 - 18\\lambda = -\\lambda(\\lambda^2+9\\lambda+18) = 0$.\nFactoring the quadratic gives $-\\lambda(\\lambda+3)(\\lambda+6)=0$.\nThe eigenvalues of $S$ are $\\{0, -3, -6\\}$. These lie within the Gershgorin union $[-6, 2]$. The eigenvalue $-6$ lies on the boundary of the union, indicating the bound is tight in that respect.\n\nQualitative conditioning insights: The matrix $D$ is a discretization of a first-derivative operator. For this specific low order $n=2$, it is nilpotent. Nilpotent matrices are pathologically ill-conditioned in many numerical contexts, and their spectral radius ($\\rho(D)=0$) can be a misleading indicator of their norm or amplification behavior.\nThe matrix $S$ is related to $D$ by $S = -\\tilde{D}^\\top\\tilde{D}$ where $\\tilde{D} = W^{1/2}DW^{-1/2}$ is a weighted version of $D$. $S$ represents the operator $-d^2/dx^2$, which is self-adjoint. The weight matrix $W$ is crucial; it introduces the correct inner product structure, resulting in a symmetric matrix $S$ with real, distinct (non-zero part) eigenvalues. While $D$ is non-normal and ill-behaved, $S$ is symmetric and well-conditioned on its range. The condition number on the range of $S$ is $|\\lambda_{max}|/|\\lambda_{min}| = |-6|/|-3|=2$, which is excellent. The diagonal scaling thus transforms a non-normal, singular operator into a symmetric, singular operator whose non-trivial action is well-conditioned.\n\nFinally, we compute the ratio $\\rho_G/\\rho$.\nThe spectral radius of $S$ is $\\rho = \\max\\{|0|, |-3|, |-6|\\} = 6$.\nThe Gershgorin union for $S$ is the interval $U = [-6, 2]$. The smallest uniform bound on $|\\lambda|$ implied by this union is $\\rho_G = \\sup_{z \\in U} |z| = \\max\\{|-6|, |2|\\} = 6$.\nThe ratio is therefore:\n$$\n\\frac{\\rho_G}{\\rho} = \\frac{6}{6} = 1\n$$",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Having built a small-scale differentiation matrix, we now explore the fundamental reason for its poor conditioning as the resolution increases. This theoretical practice  challenges you to derive the asymptotic scaling of the condition number for the Chebyshev differentiation matrix. By analyzing the operator's effect on the \"smoothest\" and \"roughest\" polynomials in the basis, you will uncover the origin of the characteristic $O(N^2)$ growth in the condition number, a cornerstone result in the analysis of spectral methods.",
            "id": "3372568",
            "problem": "Consider the Chebyshev–Gauss–Lobatto grid defined by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j \\in \\{0,1,\\dots,N\\}$, and let $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ denote the spectral differentiation matrix obtained by differentiating the degree–$N$ Lagrange interpolant built on $\\{x_j\\}$. Let $\\omega_j$ denote any set of quadrature weights that yields a spectrally accurate discrete approximation to the weighted $L^2$ inner product associated with the Chebyshev weight $w(x) = \\frac{1}{\\sqrt{1-x^2}}$, that is, for sufficiently smooth $f$, $\\sum_{j=0}^{N} \\omega_j f(x_j) \\approx \\int_{-1}^{1} \\frac{f(x)}{\\sqrt{1-x^2}}\\,dx$ with errors decaying faster than any power of $N$. Define the discrete weighted $\\ell^2$ norm by $\\|v\\|_{2,w} = \\left(\\sum_{j=0}^{N} \\omega_j |v_j|^2\\right)^{1/2}$ on vectors $v \\in \\mathbb{R}^{N+1}$, and the mean-zero subspace\n$$\n\\mathcal{M} = \\left\\{ v \\in \\mathbb{R}^{N+1} : \\sum_{j=0}^{N} \\omega_j v_j = 0 \\right\\}.\n$$\nConsider the restriction $D|_{\\mathcal{M}}$ and its Singular Value Decomposition (SVD), namely, the decomposition into singular values and singular vectors with respect to the inner product induced by $\\| \\cdot \\|_{2,w}$. Let $\\sigma_{\\min}(N)$ and $\\sigma_{\\max}(N)$ denote, respectively, the smallest and largest singular values of $D|_{\\mathcal{M}}$ in the weighted $\\ell^2$ norm. Using only fundamental facts about Chebyshev polynomials, the node distribution $\\{x_j\\}$, and norm equivalence between spectrally accurate quadrature and the corresponding continuous weighted $L^2$ norm, derive the asymptotic scaling of $\\sigma_{\\min}(N)$ and $\\sigma_{\\max}(N)$ as $N \\to \\infty$, and explain the dominant modal content (in terms of Chebyshev polynomials) of the right singular vectors associated with these two extremal singular values.\n\nYour final reported quantity must be the exponent $\\alpha$ in the leading-order asymptotic scaling of the condition number\n$$\n\\kappa(N) = \\frac{\\sigma_{\\max}(N)}{\\sigma_{\\min}(N)} \\sim C N^{\\alpha}\n$$\nfor some constant $C>0$ independent of $N$. Provide the single value of $\\alpha$ as your final answer. No rounding is required.",
            "solution": "The user wants to determine the asymptotic scaling of the condition number for the Chebyshev spectral differentiation matrix restricted to a mean-zero subspace.\n\nThe problem is defined on the Chebyshev-Gauss-Lobatto (CGL) grid $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j \\in \\{0, 1, \\dots, N\\}$. Let $D$ be the associated $(N+1) \\times (N+1)$ spectral differentiation matrix. The analysis is performed with respect to a discrete weighted inner product $\\langle u, v \\rangle_{2,w} = \\sum_{j=0}^{N} \\omega_j u_j v_j$, which induces the norm $\\|v\\|_{2,w} = \\left(\\sum_{j=0}^{N} \\omega_j |v_j|^2\\right)^{1/2}$. The weights $\\omega_j$ are such that this inner product is a spectrally accurate approximation of the continuous weighted inner product $\\langle f, g \\rangle_{L^2_w} = \\int_{-1}^{1} \\frac{f(x)g(x)}{\\sqrt{1-x^2}}\\,dx$. The problem relies on the norm equivalence between $\\|v\\|_{2,w}$ and $\\|p\\|_{L^2_w}$, where $v_j=p(x_j)$ for a polynomial $p(x)$ of degree at most $N$.\n\nThe operator is restricted to the mean-zero subspace $\\mathcal{M} = \\left\\{ v \\in \\mathbb{R}^{N+1} : \\sum_{j=0}^{N} \\omega_j v_j = 0 \\right\\}$. This discrete condition is equivalent to the continuous condition $\\int_{-1}^{1} \\frac{p(x)}{\\sqrt{1-x^2}}\\,dx=0$ for the corresponding polynomial $p(x)$. In the basis of Chebyshev polynomials $T_k(x)$, this condition implies that the coefficient of $T_0(x)=1$ is zero. Therefore, $\\mathcal{M}$ corresponds to the space of polynomials $\\text{span}\\{T_1(x), T_2(x), \\dots, T_N(x)\\}$.\n\nThe singular values $\\sigma$ of the restricted operator $D|_{\\mathcal{M}}$ are given by the square roots of the eigenvalues of $(D|_{\\mathcal{M}})^* (D|_{\\mathcal{M}})$, where the adjoint is taken with respect to the weighted inner product. Equivalently, they are characterized by the Rayleigh quotient:\n$$\n\\sigma = \\sup_{v \\in \\mathcal{M}, v \\neq 0} \\frac{\\|Dv\\|_{2,w}}{\\|v\\|_{2,w}}\n$$\nBy the norm equivalence principle stated in the problem, the scaling of the singular values of the discrete operator $D$ can be determined by analyzing the norm of the continuous differentiation operator $\\mathcal{D} = \\frac{d}{dx}$ acting on the corresponding polynomial space, with norms taken in $L^2_w$. We seek the scaling of $\\sigma_{\\min}(N)$ and $\\sigma_{\\max}(N)$, which correspond to the infimum and supremum of the ratio $\\frac{\\|\\mathcal{D}p\\|_{L^2_w}}{\\|p\\|_{L^2_w}}$ over all $p \\in \\text{span}\\{T_1, \\dots, T_N\\}$.\n\n### **Analysis of the Minimum Singular Value, $\\sigma_{\\min}(N)$**\n\nThe minimum singular value corresponds to the mode that is least amplified by differentiation. This corresponds to the \"smoothest\" or lowest-frequency function in the space $\\mathcal{M}$. The basis for the polynomial space corresponding to $\\mathcal{M}$ is $\\{T_k(x)\\}_{k=1}^N$. The lowest frequency mode is $T_1(x)=x$.\n\nLet's compute the Rayleigh quotient for the polynomial $p(x) = T_1(x)$:\nThe squared norm of $p(x)$ in $L^2_w$ is:\n$$\n\\|T_1\\|^2_{L^2_w} = \\int_{-1}^{1} \\frac{T_1(x)^2}{\\sqrt{1-x^2}}\\,dx = \\frac{\\pi}{2}\n$$\nThe derivative is $p'(x) = T_1'(x) = 1 = T_0(x)$. The squared norm of the derivative is:\n$$\n\\|T_1'\\|^2_{L^2_w} = \\int_{-1}^{1} \\frac{T_0(x)^2}{\\sqrt{1-x^2}}\\,dx = \\pi\n$$\nThe Rayleigh quotient for $T_1(x)$ is $\\frac{\\|T_1'\\|_{L^2_w}}{\\|T_1\\|_{L^2_w}} = \\sqrt{\\frac{\\pi}{\\pi/2}} = \\sqrt{2}$.\n\nThis provides an upper bound for the smallest singular value. Since the operator $\\mathcal{D}$ on the space of mean-zero functions is invertible, its singular values are bounded away from zero. The family of spaces $\\text{span}\\{T_1, \\dots, T_N\\}$ is nested. The smallest singular value $\\sigma_{\\min}(N)$ is a non-increasing function of $N$ and is bounded below by a positive constant. Therefore, it converges to a positive constant as $N \\to \\infty$.\n$$\n\\sigma_{\\min}(N) \\sim O(1)\n$$\nThe right singular vector associated with $\\sigma_{\\min}(N)$ is dominated by the lowest-frequency mode, $T_1(x)$.\n\n### **Analysis of the Maximum Singular Value, $\\sigma_{\\max}(N)$**\n\nThe maximum singular value is the operator norm $\\|D|_{\\mathcal{M}}\\|_{2,w}$. To find its scaling, we can construct a test vector $v \\in \\mathcal{M}$ that is greatly amplified by $D$. The differentiation operator is known to produce large derivatives for functions that vary rapidly. On the CGL grid, the node density is highest near the endpoints $x=\\pm 1$. This suggests that a function localized near an endpoint will have a large discrete derivative.\n\nLet's consider the Lagrange basis polynomial $p_0(x)=L_0(x)$ associated with the node $x_0 = 1$. The corresponding vector $v^{(0)}$ has components $v^{(0)}_j = \\delta_{j0}$. This vector is not in the mean-zero subspace $\\mathcal{M}$, since $\\sum_{j=0}^N \\omega_j v^{(0)}_j = \\omega_0 \\neq 0$. We project $v^{(0)}$ onto $\\mathcal{M}$ to get a valid test vector $\\tilde{v}$. The vector corresponding to the constant function $T_0(x)=1$ is $\\mathbf{1}=(1,1,\\dots,1)^T$. The projection is:\n$$\n\\tilde{v} = v^{(0)} - \\frac{\\langle v^{(0)}, \\mathbf{1} \\rangle_{2,w}}{\\|\\mathbf{1}\\|_{2,w}^2} \\mathbf{1}\n$$\nWe have $\\langle v^{(0)}, \\mathbf{1} \\rangle_{2,w} = \\sum_{j=0}^N \\omega_j \\delta_{j0} (1) = \\omega_0$. The sum of the quadrature weights is exact for the integral of $T_0(x)=1$: $\\|\\mathbf{1}\\|_{2,w}^2 = \\sum_{j=0}^N \\omega_j = \\int_{-1}^1 \\frac{1}{\\sqrt{1-x^2}}dx = \\pi$.\nSo, $\\tilde{v} = v^{(0)} - \\frac{\\omega_0}{\\pi} \\mathbf{1}$.\n\nNow we evaluate the norm of $\\tilde{v}$ and $D\\tilde{v}$.\nThe squared norm of $\\tilde{v}$ is:\n$$\n\\|\\tilde{v}\\|_{2,w}^2 = \\|v^{(0)}\\|_{2,w}^2 - \\frac{|\\langle v^{(0)}, \\mathbf{1} \\rangle_{2,w}|^2}{\\|\\mathbf{1}\\|_{2,w}^2} = \\omega_0 - \\frac{\\omega_0^2}{\\pi}\n$$\nFor the commonly used Clenshaw-Curtis quadrature weights, $\\omega_0 = \\frac{\\pi}{2N}$. Thus, $\\|\\tilde{v}\\|_{2,w}^2 = \\frac{\\pi}{2N} - O(N^{-2})$, which scales as $O(N^{-1})$.\n\nNext, we evaluate $D\\tilde{v}$. Since $D$ applied to a constant vector is zero, $D\\mathbf{1}=0$.\n$$\nD\\tilde{v} = D\\left(v^{(0)} - \\frac{\\omega_0}{\\pi} \\mathbf{1}\\right) = Dv^{(0)}\n$$\nThe vector $Dv^{(0)}$ is the $0$-th column of the differentiation matrix $D$. Its components are $(Dv^{(0)})_j = D_{j0}$. The squared norm is:\n$$\n\\|D\\tilde{v}\\|_{2,w}^2 = \\|Dv^{(0)}\\|_{2,w}^2 = \\sum_{j=0}^{N} \\omega_j (D_{j0})^2\n$$\nThe entries of the Chebyshev differentiation matrix are known. The dominant entry in the $0$-th column is the diagonal element:\n$$\nD_{00} = \\frac{2N^2+1}{6} \\sim O(N^2)\n$$\nThe off-diagonal entries $D_{j0}$ are smaller. The contribution from the $j=0$ term to the sum will dominate the scaling. Let's approximate the norm by this single term:\n$$\n\\|D\\tilde{v}\\|_{2,w}^2 \\approx \\omega_0 (D_{00})^2 \\approx \\left(\\frac{\\pi}{2N}\\right) \\left(\\frac{2N^2+1}{6}\\right)^2 \\approx \\frac{\\pi}{2N} \\frac{4N^4}{36} = \\frac{\\pi}{18} N^3\n$$\nThe squared norm scales as $O(N^3)$.\nThe Rayleigh quotient for this test vector $\\tilde{v}$ is:\n$$\n\\frac{\\|D\\tilde{v}\\|_{2,w}^2}{\\|\\tilde{v}\\|_{2,w}^2} \\approx \\frac{O(N^3)}{O(N^{-1})} = O(N^4)\n$$\nSince $\\sigma_{\\max}^2(N) \\ge \\frac{\\|D\\tilde{v}\\|_{2,w}^2}{\\|\\tilde{v}\\|_{2,w}^2}$, we have $\\sigma_{\\max}^2(N) \\sim O(N^4)$, which implies:\n$$\n\\sigma_{\\max}(N) \\sim O(N^2)\n$$\nThis is a well-known result for Chebyshev differentiation. The right singular vector associated with $\\sigma_{\\max}(N)$ is not a single Chebyshev polynomial but rather a function highly localized near one of the boundaries ($x=\\pm 1$), which is a broadband superposition of many $T_k(x)$ modes.\n\n### **Condition Number Scaling**\nThe condition number $\\kappa(N)$ is the ratio of the largest to the smallest singular value:\n$$\n\\kappa(N) = \\frac{\\sigma_{\\max}(N)}{\\sigma_{\\min}(N)} \\sim \\frac{O(N^2)}{O(1)} = O(N^2)\n$$\nThe problem asks for the exponent $\\alpha$ in the scaling law $\\kappa(N) \\sim C N^\\alpha$. Comparing with our result, we find $\\alpha=2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Our final practice moves from theory to computational application, addressing how the conditioning of spectral operators manifests in solving partial differential equations in higher dimensions. In this exercise , you will construct a 2D stiffness matrix for the Laplacian operator and numerically validate how its condition number is degraded by both polynomial degree and geometric anisotropy. You will then implement and test different preconditioners, providing a practical demonstration of how to mitigate ill-conditioning in realistic scientific computing problems.",
            "id": "3372537",
            "problem": "Consider the symmetric positive definite stiffness matrix arising from a tensor-product spectral element discretization of the scalar Laplacian on a single affine rectangular element with anisotropic stretch. Let the reference square be $[-1,1]^2$ with coordinates $(\\xi,\\eta)$, and map it to the physical element by $(x,y)=(a\\,\\xi,b\\,\\eta)$ with $a,b&gt;0$. Define the aspect ratio $r:=a/b\\ge 1$. The weak form of the Laplacian on the physical element yields the bilinear form\n$$\n\\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, \\mathrm{d}x\\mathrm{d}y\n=\n\\int_{[-1,1]^2}\n\\left(\\frac{b}{a} \\, \\partial_\\xi u \\, \\partial_\\xi v + \\frac{a}{b} \\, \\partial_\\eta u \\, \\partial_\\eta v \\right) \\, \\mathrm{d}\\xi \\mathrm{d}\\eta,\n$$\nso that in a Legendre–Gauss–Lobatto (LGL) tensor-product nodal discretization with homogeneous Dirichlet boundary conditions, the stiffness matrix has the form\n$$\nK(r,p) \\;=\\; \\frac{1}{r}\\,\\bigl(K_x \\otimes M_y\\bigr) \\;+\\; r \\,\\bigl(M_x \\otimes K_y\\bigr),\n$$\nwhere $p$ is the polynomial degree per coordinate direction, $K_x = D^\\top W D \\in \\mathbb{R}^{(p+1)\\times(p+1)}$ is the one-dimensional stiffness on the reference interval $[-1,1]$ built from the LGL first-derivative differentiation matrix $D$ and diagonal quadrature weight matrix $W$, and $M_x=W$ is the one-dimensional mass matrix (with an analogous definition for $K_y,M_y$). Homogeneous Dirichlet boundary conditions are imposed by eliminating the boundary nodes, i.e., restricting $K_x,M_x$ and $K_y,M_y$ to the interior indices $1,2,\\dots,p-1$, so that $K(r,p)\\in\\mathbb{R}^{(p-1)^2\\times(p-1)^2}$. Define the spectral condition number in the Euclidean (two-) norm as\n$$\n\\kappa_2(A) \\;:=\\; \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)},\n$$\nand for a symmetric positive definite preconditioner $P$, define the generalized preconditioned condition number by\n$$\n\\kappa_2\\bigl(P^{-1}K\\bigr) \\;:=\\; \\frac{\\lambda_{\\max}\\text{ of }(K,P)}{\\lambda_{\\min}\\text{ of }(K,P)},\n$$\ni.e., the ratio of the largest to the smallest generalized eigenvalue $\\lambda$ of $K \\mathbf{v} = \\lambda P \\mathbf{v}$ with $P$ symmetric positive definite.\n\nStarting from the weak form and the tensor-product construction above, implement a program that:\n\n1) Constructs the one-dimensional LGL nodes, quadrature weights, and the first-derivative differentiation matrix for a given $p\\in\\mathbb{N}$, forms the one-dimensional mass and stiffness matrices, applies homogeneous Dirichlet boundary conditions by restriction to interior nodes, and builds the two-dimensional matrix $K(r,p)$ for prescribed $r\\ge 1$ via the Kronecker sum structure shown above.\n\n2) Validates the scaling law $\\kappa_2\\bigl(K(r,p)\\bigr) \\sim r^2 p^4$ numerically by computing, for a set of $(p,r)$ pairs, the normalized quantity\n$$\n\\Theta(p,r) \\;:=\\; \\frac{\\kappa_2\\bigl(K(r,p)\\bigr)}{r^2\\, p^4}.\n$$\n\n3) Tests two preconditioners to assess reduction of $r$-dependence:\n   - An isotropic Jacobi (diagonal) preconditioner $P_J := \\mathrm{diag}\\bigl(K(r,p)\\bigr)$.\n   - An anisotropy-aware Kronecker-sum preconditioner\n     $$\n     P_K \\;:=\\; \\frac{1}{r}\\,\\bigl(K_x \\otimes I_y\\bigr) \\;+\\; r\\,\\bigl(I_x \\otimes K_y\\bigr),\n     $$\n     where $I_x$ and $I_y$ are identity matrices of sizes $(p-1)\\times(p-1)$ corresponding to the interior nodes in each coordinate direction.\n   For each $r$, compute the generalized condition numbers $\\kappa_2\\bigl(P_J^{-1}K(r,p)\\bigr)$ and $\\kappa_2\\bigl(P_K^{-1}K(r,p)\\bigr)$ at fixed $p$, and report the ratios with respect to the isotropic case $r=1$, namely\n   $$\n   \\rho_J(r) \\;:=\\; \\frac{\\kappa_2\\bigl(P_J^{-1}K(r,p)\\bigr)}{\\kappa_2\\bigl(P_J^{-1}K(1,p)\\bigr)}, \\qquad\n   \\rho_K(r) \\;:=\\; \\frac{\\kappa_2\\bigl(P_K^{-1}K(r,p)\\bigr)}{\\kappa_2\\bigl(P_K^{-1}K(1,p)\\bigr)}.\n   $$\n\nUse the following test suite:\n\n- Scaling-law tests for $(p,r)\\in\\{(8,1),(8,4),(8,16),(12,1),(12,4),(12,16)\\}$, i.e., $p\\in\\{8,12\\}$ and $r\\in\\{1,4,16\\}$.\n- Preconditioner tests at fixed $p=12$ for $r\\in\\{1,4,16\\}$, computing the three values $\\rho_J(r)$ and the three values $\\rho_K(r)$.\n\nYour program must compute:\n\n- The six normalized condition numbers $\\Theta(p,r)$ for the six $(p,r)$ pairs above as floating-point values.\n- The three values $\\rho_J(r)$ for $r\\in\\{1,4,16\\}$ at $p=12$ as floating-point values.\n- The three values $\\rho_K(r)$ for $r\\in\\{1,4,16\\}$ at $p=12$ as floating-point values.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_1,result\\_2,\\dots]$) in the following order:\n\n- First the six $\\Theta(p,r)$ values in the order $(8,1)$, $(8,4)$, $(8,16)$, $(12,1)$, $(12,4)$, $(12,16)$.\n- Then the three $\\rho_J(r)$ values for $r=1$, $r=4$, $r=16$ at $p=12$.\n- Finally the three $\\rho_K(r)$ values for $r=1$, $r=4$, $r=16$ at $p=12$.\n\nAll angles, if any arise, must be expressed in radians. There are no physical units in this problem, so no unit conversion is required. All numerical outputs must be floating-point numbers.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in the field of numerical analysis, specifically spectral methods for partial differential equations. It is well-posed, objective, and contains a complete and consistent set of definitions and constraints to arrive at a unique numerical solution. The tasks involve standard, albeit non-trivial, procedures in computational science, such as the construction of spectral differentiation operators and the analysis of matrix condition numbers.\n\nThe solution is implemented by following the sequence of tasks outlined in the problem statement. The core of the method involves constructing the necessary one-dimensional and two-dimensional matrices and then performing eigenvalue analyses to determine their condition numbers.\n\n**1. One-Dimensional Discretization on the Reference Interval**\n\nThe foundation of the method is the one-dimensional Legendre-Gauss-Lobatto (LGL) nodal basis on the reference interval $[-1, 1]$. For a given polynomial degree $p \\in \\mathbb{N}$, we define $p+1$ LGL nodes and corresponding quadrature weights.\n\nLet $L_p(\\xi)$ be the Legendre polynomial of degree $p$. The $p+1$ LGL nodes, denoted $\\xi_j$ for $j=0, 1, \\dots, p$, are the roots of the polynomial $(1-\\xi^2)L'_p(\\xi)$. Specifically, $\\xi_0 = -1$, $\\xi_p = 1$, and the interior nodes $\\{\\xi_j\\}_{j=1}^{p-1}$ are the roots of the derivative of the Legendre polynomial, $L'_p(\\xi)$.\n\nThe associated LGL quadrature weights, $w_j$, are given by the formula:\n$$\nw_j = \\frac{2}{p(p+1) [L_p(\\xi_j)]^2}, \\quad j = 0, 1, \\dots, p.\n$$\nThese weights define a diagonal mass matrix, $M_{p+1} = W = \\mathrm{diag}(w_0, w_1, \\dots, w_p) \\in \\mathbb{R}^{(p+1)\\times(p+1)}$.\n\nThe first-derivative differentiation matrix, $D \\in \\mathbb{R}^{(p+1)\\times(p+1)}$, which maps function values at the LGL nodes to derivative values at those same nodes, has entries:\n$$\nD_{ij} = \\begin{cases}\n\\frac{L_p(\\xi_i)}{L_p(\\xi_j)(\\xi_i - \\xi_j)} & \\text{if } i \\neq j \\\\\n-\\frac{p(p+1)}{4} & \\text{if } i=j=0 \\\\\n\\frac{p(p+1)}{4} & \\text{if } i=j=p \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe one-dimensional stiffness matrix for the operator $-\\frac{d^2}{d\\xi^2}$ is then constructed using the weak form, leading to $K_{p+1} = D^\\top W D$.\n\nHomogeneous Dirichlet boundary conditions are imposed by restricting the basis to functions that are zero at the endpoints $\\xi_0 = -1$ and $\\xi_p = 1$. In this nodal framework, this is equivalent to eliminating the first and last rows and columns of the full matrices. The resulting interior matrices, $K_x$ and $M_x$, are $(p-1) \\times (p-1)$ submatrices of $K_{p+1}$ and $M_{p+1}$ corresponding to the interior nodes $j=1, \\dots, p-1$. Due to symmetry, the same matrices $K_y=K_x$ and $M_y=M_x$ are used for the $\\eta$ direction.\n\n**2. Two-Dimensional Stiffness Matrix and Preconditioners**\n\nThe two-dimensional stiffness matrix $K(r,p)$ for the anisotropic Laplacian on the physical element is assembled using the Kronecker product ($\\otimes$), which naturally arises from the tensor-product basis functions. Given the weak form and the aspect ratio $r=a/b$, the matrix is:\n$$\nK(r,p) = \\frac{1}{r}(K_x \\otimes M_y) + r(M_x \\otimes K_y) \\in \\mathbb{R}^{(p-1)^2 \\times (p-1)^2}.\n$$\nThis matrix is symmetric and positive definite (SPD). Its condition number, $\\kappa_2(K(r,p))$, is computed as the ratio of its largest to its smallest eigenvalue, $\\lambda_{\\max}/\\lambda_{\\min}$. These eigenvalues are computed numerically. The normalized quantity $\\Theta(p,r)$ is then calculated to validate the scaling law:\n$$\n\\Theta(p,r) = \\frac{\\kappa_2(K(r,p))}{r^2 p^4}.\n$$\n\nTwo preconditioners are constructed and tested:\n- The **Jacobi preconditioner**, $P_J$, is the diagonal matrix formed by the diagonal entries of the stiffness matrix: $P_J = \\mathrm{diag}(K(r,p))$. It is simple to construct and apply but often ineffective for anisotropy.\n- The **Kronecker-sum preconditioner**, $P_K$, is designed to better capture the anisotropic structure of the operator while having a structure amenable to fast inversion solvers (though direct inversion is not required here). It is defined as:\n$$\nP_K = \\frac{1}{r}(K_x \\otimes I_y) + r(I_x \\otimes K_y),\n$$\nwhere $I_x$ and $I_y$ are identity matrices of size $(p-1) \\times (p-1)$.\n\n**3. Preconditioned System Analysis**\n\nFor an SPD preconditioner $P$, the condition number of the preconditioned system $P^{-1}K$ is given by the ratio of the largest to the smallest generalized eigenvalue $\\lambda$ of the problem $K\\mathbf{v} = \\lambda P\\mathbf{v}$. Both $P_J$ and $P_K$ are SPD, so this is well-defined.\n\nThe effectiveness of the preconditioners in handling anisotropy is assessed by computing the ratios $\\rho_J(r)$ and $\\rho_K(r)$ for a fixed polynomial degree $p=12$. These ratios normalize the condition number of the preconditioned system at a given aspect ratio $r$ by its value in the isotropic case ($r=1$):\n$$\n\\rho_J(r) = \\frac{\\kappa_2(P_J^{-1}K(r,p))}{\\kappa_2(P_J^{-1}K(1,p))}, \\qquad \\rho_K(r) = \\frac{\\kappa_2(P_K^{-1}K(r,p))}{\\kappa_2(P_K^{-1}K(1,p))}.\n$$\nA value of $\\rho(r)$ close to $1$ indicates that the preconditioner successfully mitigates the ill-conditioning introduced by the geometric anisotropy.\n\nThe implementation computes these quantities for the specified test suite of $(p,r)$ pairs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre\nfrom scipy.linalg import eigh\n\ndef get_lgl_matrices(p: int):\n    \"\"\"\n    Computes 1D Legendre-Gauss-Lobatto mass and stiffness matrices for a given\n    polynomial degree p, with homogeneous Dirichlet boundary conditions imposed.\n\n    Args:\n        p: The polynomial degree.\n\n    Returns:\n        A tuple (Kx_int, Mx_int) containing the (p-1)x(p-1) interior \n        stiffness and mass matrices.\n    \"\"\"\n    if p < 2:\n        raise ValueError(\"Polynomial degree p must be at least 2 for interior nodes.\")\n\n    # 1. Compute LGL nodes and weights for degree p\n    # The p+1 LGL nodes are the roots of (1-x^2) * L_p'(x), where L_p is the\n    # Legendre polynomial of degree p.\n    lp_poly = legendre(p)\n    lp_deriv_poly = lp_poly.deriv(1)\n    \n    # Interior nodes are the roots of L_p'(x)\n    interior_nodes = lp_deriv_poly.roots\n    # The full set of nodes includes the endpoints -1 and +1\n    nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n\n    # LGL quadrature weights\n    lp_vals_at_nodes = lp_poly(nodes)\n    weights = 2.0 / (p * (p + 1) * lp_vals_at_nodes**2)\n    \n    # 2. Compute the (p+1)x(p+1) differentiation matrix D\n    N = p\n    D = np.zeros((N + 1, N + 1))\n    # Off-diagonal entries\n    for i in range(N + 1):\n        for j in range(N + 1):\n            if i != j:\n                D[i, j] = lp_vals_at_nodes[i] / (lp_vals_at_nodes[j] * (nodes[i] - nodes[j]))\n    \n    # Diagonal entries\n    D[0, 0] = -N * (N + 1) / 4.0\n    D[N, N] = N * (N + 1) / 4.0\n    # Diagonal entries for interior nodes are 0 and are already set.\n\n    # 3. Form full (p+1)x(p+1) 1D mass and stiffness matrices\n    W = np.diag(weights)\n    M_full = W\n    K_full = D.T @ W @ D\n    \n    # 4. Apply homogeneous Dirichlet BCs by restricting to interior nodes.\n    # The interior nodes are indexed from 1 to p-1.\n    interior_slice = slice(1, p)\n    Mx_int = M_full[interior_slice, interior_slice]\n    Kx_int = K_full[interior_slice, interior_slice]\n    \n    return Kx_int, Mx_int\n\ndef solve():\n    # Define the test cases from the problem statement.\n    scaling_law_tests = [(8, 1), (8, 4), (8, 16), (12, 1), (12, 4), (12, 16)]\n    preconditioner_p = 12\n    preconditioner_r_values = [1, 4, 16]\n\n    results = []\n\n    # --- Part 1: Scaling-law tests ---\n    theta_results = []\n    for p, r in scaling_law_tests:\n        Kx, Mx = get_lgl_matrices(p)\n        Ky, My = Kx, Mx # Same discretization in each direction\n        \n        # Build the 2D stiffness matrix K(r, p)\n        K_rp = (1.0 / r) * np.kron(Kx, My) + r * np.kron(Mx, Ky)\n        \n        # Compute the spectral condition number kappa_2(K).\n        # K_rp is symmetric, so we use eigvalsh for stability and efficiency.\n        eigenvalues = np.linalg.eigvalsh(K_rp)\n        kappa_2_K = eigenvalues[-1] / eigenvalues[0]\n        \n        # Compute the normalized quantity Theta(p, r)\n        theta = kappa_2_K / (r**2 * p**4)\n        theta_results.append(theta)\n    \n    results.extend(theta_results)\n\n    # --- Part 2: Preconditioner tests ---\n    p = preconditioner_p\n    r_values = preconditioner_r_values\n\n    # Get 1D interior matrices for p=12\n    Kx, Mx = get_lgl_matrices(p)\n    Ky, My = Kx, Mx\n    \n    # Precompute identity matrices of size (p-1)x(p-1)\n    dim_int = p - 1\n    Ix = np.eye(dim_int)\n    Iy = np.eye(dim_int)\n\n    # Calculate baseline condition numbers for the isotropic case r=1\n    r_base = 1.0\n    K_base = (1.0 / r_base) * np.kron(Kx, My) + r_base * np.kron(Mx, Ky)\n    \n    P_J_base = np.diag(np.diag(K_base))\n    gen_eigvals_J_base = eigh(K_base, P_J_base, eigvals_only=True)\n    kappa_J_base = gen_eigvals_J_base[-1] / gen_eigvals_J_base[0]\n    \n    P_K_base = (1.0 / r_base) * np.kron(Kx, Iy) + r_base * np.kron(Ix, Ky)\n    gen_eigvals_K_base = eigh(K_base, P_K_base, eigvals_only=True)\n    kappa_K_base = gen_eigvals_K_base[-1] / gen_eigvals_K_base[0]\n\n    rho_J_results = []\n    rho_K_results = []\n    \n    for r in r_values:\n        K_rp = (1.0 / r) * np.kron(Kx, My) + r * np.kron(Mx, Ky)\n        \n        # Jacobi Preconditioner\n        P_J_rp = np.diag(np.diag(K_rp))\n        gen_eigvals_J_rp = eigh(K_rp, P_J_rp, eigvals_only=True)\n        kappa_J_rp = gen_eigvals_J_rp[-1] / gen_eigvals_J_rp[0]\n        rho_J = kappa_J_rp / kappa_J_base\n        rho_J_results.append(rho_J)\n        \n        # Kronecker-sum Preconditioner\n        P_K_rp = (1.0 / r) * np.kron(Kx, Iy) + r * np.kron(Ix, Ky)\n        # For r=1, P_K_rp == P_K_base, and K_rp == K_base, so rho_K will be 1 as expected.\n        gen_eigvals_K_rp = eigh(K_rp, P_K_rp, eigvals_only=True)\n        kappa_K_rp = gen_eigvals_K_rp[-1] / gen_eigvals_K_rp[0]\n        rho_K = kappa_K_rp / kappa_K_base\n        rho_K_results.append(rho_K)\n        \n    results.extend(rho_J_results)\n    results.extend(rho_K_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}