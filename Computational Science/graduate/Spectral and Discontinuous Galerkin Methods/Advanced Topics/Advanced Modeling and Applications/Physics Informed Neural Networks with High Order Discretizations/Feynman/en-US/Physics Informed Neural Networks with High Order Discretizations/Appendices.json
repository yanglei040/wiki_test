{
    "hands_on_practices": [
        {
            "introduction": "The foundation of training any physics-informed neural network lies in correctly defining a loss function and deriving its gradients with respect to the network's parameters. This first exercise  provides practice with this fundamental process in a setting that blends PINNs with high-order spectral methods. You will design a composite loss for the Burgers' equation that includes not only the PDE residual at Fourier collocation points but also enforces a physical conservation law, and then derive the analytical gradient essential for optimization.",
            "id": "3408367",
            "problem": "Consider the viscous Burgers’ equation on the periodic spatial domain $[0,1]$,\n$$u_{t}(x,t)+u(x,t)\\,u_{x}(x,t)=\\nu\\,u_{xx}(x,t),$$\nwith viscosity parameter $\\nu>0$ and periodic boundary conditions in $x$. Let $u_{\\theta}(x,t)$ be a smooth parametric ansatz provided by a Physics-Informed Neural Network (PINN), where $\\theta=\\{\\theta_{k}\\}_{k=1}^{p}$ denotes the network parameters and all mixed partial derivatives $\\partial^{\\alpha+\\beta}u_{\\theta}/\\partial x^{\\alpha}\\partial t^{\\beta}$ exist and are continuous for the orders needed below. To incorporate high-order spatial discretization appropriate for periodic domains, place $N_{s}$ equispaced Fourier collocation points\n$$x_{j}=\\frac{j}{N_{s}},\\quad j=0,1,\\dots,N_{s}-1,$$\nand $N_{t}$ time collocation points $\\{t_{i}\\}_{i=1}^{N_{t}}$ in a time interval of interest. Let $\\mathbf{D}\\in\\mathbb{R}^{N_{s}\\times N_{s}}$ and $\\mathbf{D}^{(2)}\\in\\mathbb{R}^{N_{s}\\times N_{s}}$ denote the first- and second-order Fourier spectral differentiation matrices on this grid, acting on spatial nodal values at fixed time. For the spatial integral, use the trapezoidal rule with uniform weights $w_{j}=\\frac{1}{N_{s}}$, which is spectrally accurate for periodic, sufficiently smooth functions.\n\nDefine the pointwise physics residual at each space-time collocation pair $(x_{j},t_{i})$ by\n$$R_{ij}(\\theta)=\\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j},$$\nwhere $\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\in\\mathbb{R}^{N_{s}}$ collects $\\{u_{\\theta}(x_{j},t_{i})\\}_{j=0}^{N_{s}-1}$ and the subscript $(\\cdot)_{j}$ selects the $j$-th component. The periodic mass $\\int_{0}^{1}u(x,t)\\,dx$ is invariant in time for smooth solutions with periodic boundary conditions; impose this as a soft constraint at the time collocation points by requiring $\\int_{0}^{1}u_{\\theta}(x,t_{i})\\,dx=M$ for a prescribed constant mass $M\\in\\mathbb{R}$.\n\nDesign the following composite loss,\n$$\\mathcal{L}(\\theta)=\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}+\\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2},$$\nwith positive weights $\\lambda_{r}>0$ and $\\lambda_{m}>0$. Starting only from the Burgers’ equation, periodic boundary conditions, the definitions above, and standard rules of calculus, derive the symbolic gradient of $\\mathcal{L}(\\theta)$ with respect to a generic network parameter $\\theta_{k}$, expressing your result in terms of $u_{\\theta}$, its parameter sensitivities $\\partial u_{\\theta}/\\partial\\theta_{k}$, and the spectral differentiation operators acting on spatial nodal values. You must explicitly handle the chain rule for the nonlinear advection term and justify commuting parameter differentiation with spatial and temporal differentiation wherever used.\n\nYour final answer must be a single closed-form analytic expression for $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{k}}$ in terms of the quantities defined above. No numerical approximation or rounding is required.",
            "solution": "The problem requires the derivation of the gradient of a composite loss function $\\mathcal{L}(\\theta)$ with respect to a generic neural network parameter $\\theta_k$. The loss function is designed for training a Physics-Informed Neural Network (PINN) to solve the viscous Burgers' equation on a periodic domain.\n\n**Problem Validation**\n\nFirst, a validation of the problem statement is performed.\n\n*   **Givens:**\n    *   **PDE:** Viscous Burgers' equation, $u_{t}(x,t)+u(x,t)\\,u_{x}(x,t)=\\nu\\,u_{xx}(x,t)$, on $x \\in [0,1]$.\n    *   **Boundary Conditions:** Periodic in $x$.\n    *   **Ansatz:** A smooth parametric function $u_{\\theta}(x,t)$, where $\\theta=\\{\\theta_{k}\\}_{k=1}^{p}$. All necessary mixed partial derivatives exist and are continuous.\n    *   **Discretization:** $N_{s}$ equispaced spatial points $x_{j}=\\frac{j}{N_{s}}$ for $j=0,1,\\dots,N_{s}-1$, and $N_{t}$ time points $\\{t_{i}\\}_{i=1}^{N_{t}}$.\n    *   **Spectral Operators:** First- and second-order Fourier spectral differentiation matrices, $\\mathbf{D}$ and $\\mathbf{D}^{(2)}$.\n    *   **Quadrature:** Trapezoidal rule with weights $w_{j}=\\frac{1}{N_{s}}$.\n    *   **Pointwise Residual:** $R_{ij}(\\theta)=\\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}$.\n    *   **Mass Conservation:** $\\int_{0}^{1}u_{\\theta}(x,t_{i})\\,dx=M$ for a constant $M$.\n    *   **Loss Function:** $\\mathcal{L}(\\theta)=\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}+\\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2}$, with $\\lambda_r > 0$, $\\lambda_m > 0$.\n\n*   **Validation:**\n    The problem is scientifically grounded, well-posed, and objective. It describes a standard and contemporary method in scientific machine learning. The components (Burgers' equation, spectral methods, PINN loss function) are all standard and correctly formulated. The assumptions, such as the smoothness of $u_{\\theta}$, are explicitly stated and sufficient for the required derivation.\n    There is a minor notational discrepancy: the spatial grid is indexed $j=0, \\dots, N_s-1$, whereas the sum over the residuals in the loss function is written with index $j=1, \\dots, N_s$. The sum for the mass conservation term is correctly indexed $j=0, \\dots, N_s-1$. It is a reasonable and necessary assumption that the sum over residuals is also intended to be over all $N_s$ grid points, and so we will proceed by taking the sum from $j=0$ to $N_s-1$. This does not affect the validity of the problem's core structure.\n\n*   **Verdict:** The problem is valid.\n\n**Derivation of the Gradient**\n\nThe loss function $\\mathcal{L}(\\theta)$ is a sum of two components: a residual loss $\\mathcal{L}_r(\\theta)$ and a mass conservation loss $\\mathcal{L}_m(\\theta)$.\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_r(\\theta) + \\mathcal{L}_m(\\theta)\n$$\nwhere\n$$\n\\mathcal{L}_r(\\theta) = \\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}\n$$\n$$\n\\mathcal{L}_m(\\theta) = \\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2}\n$$\nBy the linearity of differentiation, the gradient is the sum of the gradients of the components:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{k}} = \\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}} + \\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}}\n$$\nWe will compute each term separately.\n\n**1. Gradient of the Residual Loss $\\mathcal{L}_r(\\theta)$**\n\nUsing the chain rule, we differentiate $\\mathcal{L}_r(\\theta)$ with respect to a generic parameter $\\theta_k$:\n$$\n\\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}\\right) = \\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} 2 R_{ij}(\\theta) \\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}}\n$$\nTo proceed, we must compute the derivative of the residual, $\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}}$. The residual is given by:\n$$\nR_{ij}(\\theta) = \\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\n$$\nWe differentiate $R_{ij}(\\theta)$ term by term with respect to $\\theta_k$:\n$$\n\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\frac{\\partial u_{\\theta}}{\\partial t}\\right) + \\frac{\\partial}{\\partial \\theta_{k}}\\left(u_{\\theta}\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}\\right)_{j}\\right) - \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}\\right)_{j}\\right)\n$$\nwhere all functions are evaluated at $(x_j, t_i)$.\n\n*   **First term (time derivative):** The problem states that $u_{\\theta}$ is smooth and all its mixed partial derivatives exist and are continuous. This allows us to apply Clairaut's theorem to exchange the order of differentiation:\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\frac{\\partial u_{\\theta}}{\\partial t}\\right) = \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)\n    $$\n*   **Second term (nonlinear advection):** We apply the product rule for differentiation:\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right) = \\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_{j},t_{i})\\right)\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j} + u_{\\theta}(x_{j},t_{i})\\frac{\\partial}{\\partial \\theta_{k}}\\left(\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right)\n    $$\n    The differentiation matrices $\\mathbf{D}$ and $\\mathbf{D}^{(2)}$ are constant with respect to $\\theta$. The derivative $\\frac{\\partial}{\\partial\\theta_k}$ acts on the vector of nodal values $\\mathbf{u}_{\\theta}(\\cdot, t_i)$. The differentiation with respect to $\\theta_k$ and the matrix-vector multiplication (a linear operation) can be interchanged:\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right) = \\mathbf{D}\\,\\left(\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)\n    $$\n    Here, $\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}$ is the vector of parameter sensitivities, whose $j$-th component is $\\frac{\\partial u_{\\theta}(x_j, t_i)}{\\partial \\theta_k}$. So, the second term becomes:\n    $$\n    \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_{j},t_{i}) \\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j} + u_{\\theta}(x_{j},t_{i}) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)_{j}\n    $$\n*   **Third term (viscosity):** The viscosity $\\nu$ and matrix $\\mathbf{D}^{(2)}$ are constant. Using the same argument as for the advection term:\n    $$\n    -\\frac{\\partial}{\\partial \\theta_{k}}\\left(\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right) = -\\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)_{j}\n    $$\n\nCombining these results, the derivative of the residual is:\n$$\n\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) (\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i))_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j}\n$$\nSubstituting this into the expression for $\\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}}$ gives its complete form.\n\n**2. Gradient of the Mass Conservation Loss $\\mathcal{L}_m(\\theta)$**\n\nLet $C_i(\\theta) = \\sum_{j=0}^{N_s-1} w_j u_{\\theta}(x_j, t_i) - M$. The loss term is $\\mathcal{L}_m(\\theta) = \\lambda_m \\frac{1}{N_t} \\sum_{i=1}^{N_t} C_i(\\theta)^2$.\nApplying the chain rule:\n$$\n\\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}} = \\lambda_m \\frac{1}{N_t} \\sum_{i=1}^{N_t} 2 C_i(\\theta) \\frac{\\partial C_i(\\theta)}{\\partial \\theta_k}\n$$\nThe derivative of the inner term $C_i(\\theta)$ is:\n$$\n\\frac{\\partial C_i(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k}\\left(\\sum_{j=0}^{N_s-1} w_j u_{\\theta}(x_j, t_i) - M\\right) = \\sum_{j=0}^{N_s-1} w_j \\frac{\\partial u_{\\theta}(x_j, t_i)}{\\partial \\theta_k}\n$$\nsince the weights $w_j$ and the mass $M$ are constants.\nSubstituting this back, we get:\n$$\n\\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}} = \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)\n$$\n\n**3. Total Gradient**\n\nThe final expression for the gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}$ is the sum of the derivatives of the two loss components:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k} = \\frac{2\\lambda_{r}}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} R_{ij}(\\theta) \\left[ \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) (\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i))_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} \\right] \\\\\n+ \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)\n$$\nThis is the symbolic gradient requested, expressed in terms of the defined quantities.",
            "answer": "$$\n\\boxed{\\frac{2\\lambda_{r}}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} R_{ij}(\\theta) \\left[ \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) \\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i)\\right)_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} \\right] + \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)}\n$$"
        },
        {
            "introduction": "Once we understand how to formulate the training objective, a critical practical question arises: how should we represent the solution within our high-order framework? This computational exercise  delves into this by comparing two common parameterizations from spectral methods: a modal basis using Legendre polynomials and a nodal basis using Lagrange polynomials at Gauss-Lobatto-Legendre nodes. By analyzing the numerical properties of the mass and stiffness matrices for each, you will gain insight into how basis choice affects the stability and performance of the optimization process.",
            "id": "3408343",
            "problem": "You are asked to implement and compare two polynomial parameterizations commonly used in spectral and discontinuous Galerkin methods for Physics-Informed Neural Networks (PINN). One parameterization is the modal basis using Legendre polynomials, and the other is the nodal basis using Lagrange polynomials at Gauss–Lobatto–Legendre (GLL) nodes. The primary goals are to quantify the conditioning of the mass matrix and to quantify gradient propagation through the mass and stiffness matrices at high polynomial degrees.\n\nUse the following fundamental base:\n- The Physics-Informed Neural Network (PINN) loss for an operator $L$ applied to an unknown $u$ is defined by a residual $r = L u - f$, and an $L^2$ inner product-induced energy norm $||r||_{M}^2 = r^T M r$, where $M$ is the mass matrix associated with the chosen basis and inner product on the interval $[-1,1]$ with unit weight.\n- The Legendre polynomials $\\{P_n(x)\\}_{n=0}^{p}$ on $[-1,1]$ satisfy the orthogonality relation $\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm}$.\n- The Lagrange nodal basis functions $\\{ \\ell_i(x) \\}_{i=0}^{p}$ at Gauss–Lobatto–Legendre nodes $\\{x_i\\}_{i=0}^{p}$ satisfy $\\ell_i(x_j) = \\delta_{ij}$. The interior GLL nodes are the roots of the derivative of $P_p(x)$, and the endpoints are $-1$ and $1$.\n- The mass matrix $M$ and stiffness matrix $K$ are defined by $M_{ij} = \\int_{-1}^{1} \\phi_i(x) \\phi_j(x) \\, dx$ and $K_{ij} = \\int_{-1}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$, respectively, where $\\{\\phi_i\\}$ is the chosen basis.\n- The gradient of the PINN loss with respect to coefficients $c$ in a linear residual model $r = A c - b$ with $A = K$ and weighting by the mass matrix is $\\nabla L(c) = K^T M (K c - b)$. A key quantity governing gradient propagation is the operator norm $||K^T M||_2 = ||K M||_2$.\n- The condition number in the $2$-norm of a symmetric positive definite matrix $M$ is $\\kappa(M) = \\lambda_{\\max}(M)/\\lambda_{\\min}(M)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $M$.\n\nTask requirements:\n1. Construct the modal Legendre basis of degree $p$ on $[-1,1]$ with unit weight. In this basis, use the exact diagonal mass matrix $M_{\\text{modal}}$ with entries $M_{nn} = \\frac{2}{2n+1}$ for $n = 0,1,\\dots,p$, and the exact diagonal stiffness matrix $K_{\\text{modal}}$ with entries $K_{nn} = n(n+1) \\frac{2}{2n+1}$.\n2. Construct the nodal Lagrange basis at Gauss–Lobatto–Legendre nodes of degree $p$. Express each nodal basis function $\\ell_i(x)$ in the Legendre modal basis as $\\ell_i(x) = \\sum_{n=0}^{p} \\beta_{n i} P_n(x)$, where the coefficient column vector $\\beta_{\\cdot i}$ satisfies $\\sum_{n=0}^{p} \\beta_{n i} P_n(x_j) = \\delta_{i j}$ for all nodes $x_j$. Implement this by forming the square evaluation matrix $V \\in \\mathbb{R}^{(p+1)\\times(p+1)}$ with entries $V_{j n} = P_n(x_j)$ and solving $V \\beta = I_{p+1}$ for $\\beta \\in \\mathbb{R}^{(p+1)\\times(p+1)}$.\n3. Using the change of basis coefficients, assemble the exact nodal mass and stiffness matrices via $M_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta$ and $K_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta$.\n4. For both parameterizations, compute the condition number $\\kappa(M)$ of their mass matrices and the operator norm $||K M||_2$.\n5. Implement numerically stable evaluations of $\\{P_n(x)\\}_{n=0}^{p}$ using the three-term recurrence. Compute the interior GLL nodes as the roots of $P_p'(x)$ using a reliable polynomial root finder. All computations are on $[-1,1]$ with unit weight; there are no physical units; all quantities are dimensionless.\n\nTest suite:\n- Use polynomial degrees $p$ in the set $\\{1, 8, 24\\}$ to cover a basic case, a moderate case, and a high-order case.\n- For each $p$, compute and return a list of four floating-point values:\n  - $\\kappa(M_{\\text{modal}})$,\n  - $\\kappa(M_{\\text{nodal}})$,\n  - $||K_{\\text{modal}} M_{\\text{modal}}||_2$,\n  - $||K_{\\text{nodal}} M_{\\text{nodal}}||_2$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list enclosed in square brackets. For example, the output format must be like $[[a,b,c,d],[e,f,g,h],[i,j,k,l]]$.\n\nConstraints and notes:\n- All matrices must be assembled exactly using the described basis relations; no mass lumping or approximate quadrature is permitted for the nodal matrices.\n- Use real arithmetic in double precision. If any symmetric matrix has very small negative eigenvalues due to round-off, treat them as numerical artifacts; you should still compute the condition number using its eigenvalues.\n- Angles are not involved; no angle units are required.",
            "solution": "The user's request is to validate and solve a problem from the domain of numerical analysis for partial differential equations, specifically concerning spectral methods and their application in Physics-Informed Neural Networks (PINNs).\n\n### Step 1: Extract Givens\n\n-   **Problem Domain**: Physics-Informed Neural Networks (PINNs) loss functions, polynomial parameterizations from spectral/discontinuous Galerkin methods.\n-   **Mathematical Space**: Functions on the interval $[-1,1]$ with a unit weighting function for the $L^2$ inner product.\n-   **Modal Basis**: Legendre polynomials $\\{P_n(x)\\}_{n=0}^{p}$.\n    -   Orthogonality: $\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm}$.\n    -   Mass Matrix ($M_{\\text{modal}}$): Diagonal with entries $(M_{\\text{modal}})_{nn} = \\frac{2}{2n+1}$.\n    -   Stiffness Matrix ($K_{\\text{modal}}$): Diagonal with entries $(K_{\\text{modal}})_{nn} = n(n+1) \\frac{2}{2n+1}$.\n-   **Nodal Basis**: Lagrange polynomials $\\{\\ell_i(x)\\}_{i=0}^{p}$ defined on Gauss–Lobatto–Legendre (GLL) nodes $\\{x_i\\}_{i=0}^{p}$.\n    -   Nodal Property: $\\ell_i(x_j) = \\delta_{ij}$.\n    -   GLL Nodes: Endpoints $[-1, 1]$ and the roots of $P_p'(x)$, the first derivative of the Legendre polynomial of degree $p$.\n-   **Matrix Definitions**:\n    -   Mass Matrix: $M_{ij} = \\int_{-1}^{1} \\phi_i(x) \\phi_j(x) \\, dx$.\n    -   Stiffness Matrix: $K_{ij} = \\int_{-1}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$.\n-   **Change of Basis**: The Lagrange basis functions are expressed in the Legendre basis as $\\ell_i(x) = \\sum_{n=0}^{p} \\beta_{ni} P_n(x)$. The matrix of coefficients $\\beta$ is obtained by solving the system $V \\beta = I_{p+1}$, where $V$ is the evaluation matrix with entries $V_{jn} = P_n(x_j)$.\n-   **Nodal Matrix Assembly**:\n    -   $M_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta$.\n    -   $K_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta$.\n-   **Quantities to Compute**: For each basis (modal and nodal):\n    1.  Condition number of the mass matrix: $\\kappa(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$.\n    2.  Operator norm of the stiffness-mass product: $||KM||_2$.\n-   **Numerical Procedures**:\n    -   Use the three-term recurrence for stable evaluation of Legendre polynomials.\n    -   Use a reliable polynomial root finder for GLL nodes.\n    -   Use double-precision floating-point arithmetic.\n-   **Test Suite**: Polynomial degrees $p \\in \\{1, 8, 24\\}$.\n-   **Output Format**: A single line of text: `[[p1_res_1, ..., p1_res_4], [p2_res_1, ..., p2_res_4], [p3_res_1, ..., p3_res_4]]`.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is critically examined for validity.\n\n1.  **Scientific Grounding**: The concepts presented are standard in the field of spectral methods. The definitions of mass and stiffness matrices, Legendre polynomials, GLL nodes, and change of basis are all well-established. There is an apparent contradiction: the general definition of the stiffness matrix is given as $K_{ij} = \\int_{-1}^1 \\phi_i'(x) \\phi_j'(x) dx$, which for the Legendre basis ($\\phi_n=P_n$) results in a non-diagonal matrix. However, the problem then explicitly instructs to \"use the exact diagonal stiffness matrix $K_{\\text{modal}}$ with entries $K_{nn} = n(n+1) \\frac{2}{2n+1}$\". This prescribed matrix corresponds to the weak form of the Sturm-Liouville operator $Lu = -((1-x^2)u')'$, for which Legendre polynomials are eigenfunctions, not the standard Laplacian operator $-u''$. Given the explicit instruction, the general definition can be interpreted as contextual background, and the specific formula for $K_{\\text{modal}}$ as a direct, overriding command. With this interpretation, the problem is scientifically consistent. It instructs a comparison based on a well-defined spectral operator.\n2.  **Well-Posedness**: The problem is well-posed. It requests the computation of specific, uniquely defined quantities given a clear set of inputs and procedures. Finding polynomial roots, solving linear systems with a well-conditioned Vandermonde-like matrix (a property of GLL nodes), and performing matrix analysis are all standard, solvable numerical tasks.\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language, free from subjective or biased claims.\n\nThe problem is deemed **valid** under the interpretation that the specified diagonal formula for $K_{\\text{modal}}$ is a directive for the implementation.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. The solution will proceed by implementing the specified computations.\n\n### Solution\n\nThe solution involves a systematic computation for each polynomial degree $p$ from the test suite $\\{1, 8, 24\\}$. For each $p$, we will first construct the diagonal mass and stiffness matrices for the modal (Legendre) basis and compute the required metrics. Then, we will determine the GLL nodes, set up the change-of-basis machinery, construct the corresponding dense nodal mass and stiffness matrices, and compute their metrics.\n\n**1. Modal Basis Computations**\n\nFor a polynomial degree $p$, the matrices are of size $(p+1) \\times (p+1)$. Let $n = 0, 1, \\dots, p$.\nThe mass matrix $M_{\\text{modal}}$ and stiffness matrix $K_{\\text{modal}}$ are diagonal with entries:\n$$\n(M_{\\text{modal}})_{nn} = \\frac{2}{2n+1}\n$$\n$$\n(K_{\\text{modal}})_{nn} = n(n+1) \\frac{2}{2n+1}\n$$\nSince these matrices are diagonal, their eigenvalues are their diagonal entries. The condition number of $M_{\\text{modal}}$ is the ratio of its largest to smallest eigenvalue:\n$$\n\\kappa(M_{\\text{modal}}) = \\frac{\\max_n (M_{\\text{modal}})_{nn}}{\\min_n (M_{\\text{modal}})_{nn}} = \\frac{(M_{\\text{modal}})_{00}}{(M_{\\text{modal}})_{pp}} = \\frac{2}{2/(2p+1)} = 2p+1\n$$\nThe product $K_{\\text{modal}} M_{\\text{modal}}$ is also a diagonal matrix with entries $(K_{\\text{modal}})_{nn} (M_{\\text{modal}})_{nn}$. The $2$-norm of a diagonal matrix is the maximum absolute value of its diagonal entries.\n$$\n||K_{\\text{modal}} M_{\\text{modal}}||_2 = \\max_n \\left| n(n+1) \\left(\\frac{2}{2n+1}\\right)^2 \\right|\n$$\n\n**2. Nodal Basis Computations**\n\n**a. GLL Nodes:** For degree $p$, the $p+1$ GLL nodes $\\{x_j\\}_{j=0}^p$ consist of the endpoints $x_0 = -1$, $x_p = 1$, and the $p-1$ roots of the derivative of the Legendre polynomial $P_p'(x)$. These roots are found using a numerical polynomial root-finding algorithm.\n\n**b. Change of Basis:** We construct the Vandermonde-like matrix $V$ of size $(p+1) \\times (p+1)$ with entries $V_{jn} = P_n(x_j)$. This requires stable evaluation of Legendre polynomials up to degree $p$ at the GLL nodes, typically done via the three-term recurrence relation:\n$$\n(n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x), \\quad P_0(x)=1, \\quad P_1(x)=x\n$$\nThe matrix of change-of-basis coefficients, $\\beta$, which transforms nodal basis coefficients to modal coefficients, is the inverse of $V$, i.e., $\\beta = V^{-1}$. Numerically, it is found by solving the linear system $V \\beta = I$, where $I$ is the identity matrix.\n\n**c. Nodal Matrices:** The mass and stiffness matrices in the nodal basis are obtained by applying the change of basis transformation to their modal counterparts:\n$$\nM_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta\n$$\n$$\nK_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta\n$$\nThese resulting matrices, $M_{\\text{nodal}}$ and $K_{\\text{nodal}}$, are dense and symmetric.\n\n**d. Nodal Metrics:**\nThe condition number $\\kappa(M_{\\text{nodal}})$ is computed using its singular values, $\\sigma_i$, as $\\kappa_2(M_{\\text{nodal}}) = \\sigma_{\\max}/\\sigma_{\\min}$. Since $M_{\\text{nodal}}$ is symmetric positive definite, this is equivalent to the ratio of its maximum to minimum eigenvalues.\nThe product $K_{\\text{nodal}} M_{\\text{nodal}}$ is not generally symmetric. Its $2$-norm, $||K_{\\text{nodal}} M_{\\text{nodal}}||_2$, is its largest singular value, which is computed using a standard numerical linear algebra library function.\n\nThis procedure is systematically applied for each $p \\in \\{1, 8, 24\\}$, and the four resulting scalar metrics are collected for the final output. The behavior of these metrics as $p$ increases reveals important properties of the two bases; for instance, the condition number of the nodal mass matrix is known to be bounded by a small constant independent of $p$, unlike a naive equispaced nodal basis, while the condition number of the modal mass matrix grows with $p$.",
            "answer": "```python\nimport numpy as np\nfrom numpy.polynomial.legendre import legvander, Legendre\n\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n\ndef solve():\n    \"\"\"\n    Main function to run the complete analysis as specified in the problem.\n    It computes and compares conditioning and gradient propagation metrics\n    for modal (Legendre) and nodal (Lagrange-GLL) bases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases_p = [1, 8, 24]\n\n    def get_gll_nodes(p: int) -> np.ndarray:\n        \"\"\"\n        Computes the Gauss-Lobatto-Legendre (GLL) nodes for a given polynomial degree p.\n        The GLL nodes include the endpoints -1 and 1, and the roots of P_p'(x).\n        \"\"\"\n        if p == 0:\n            return np.array([0.0])\n        \n        if p == 1:\n            return np.array([-1.0, 1.0])\n\n        # For p > 1, interior nodes are the roots of the derivative of the Legendre polynomial P_p(x).\n        # We use numpy's polynomial tools to find these roots efficiently and robustly.\n        # Create a Legendre polynomial object for P_p(x).\n        # The coefficients are [0, 0, ..., 1] to represent P_p.\n        c = np.zeros(p + 1)\n        c[p] = 1.0\n        P_p = Legendre(c)\n        \n        # Get its derivative, P_p'(x).\n        P_p_prime = P_p.deriv()\n        \n        # Find the roots of P_p'(x).\n        interior_nodes = P_p_prime.roots()\n        \n        # The roots of P_p' are guaranteed to be real and in (-1, 1).\n        # We take the real part to discard negligible imaginary parts from numerical precision errors.\n        interior_nodes = np.real(interior_nodes)\n        \n        # Combine the sorted interior nodes with the endpoints to form the GLL node set.\n        nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n        return nodes\n\n    def compute_metrics(p: int) -> list[float]:\n        \"\"\"\n        Computes the four required metrics for a given polynomial degree p:\n        1. kappa(M_modal)\n        2. kappa(M_nodal)\n        3. ||K_modal M_modal||_2\n        4. ||K_nodal M_nodal||_2\n        \"\"\"\n        N = p + 1\n        n_vals = np.arange(N, dtype=float)\n\n        # 1. MODAL BASIS (Legendre polynomials)\n        # The mass matrix M_modal is diagonal due to the orthogonality of Legendre polynomials.\n        # M_{nn} = integral(P_n * P_n) dx = 2 / (2n + 1).\n        m_diag = 2.0 / (2.0 * n_vals + 1.0)\n        M_modal = np.diag(m_diag)\n        \n        # The problem defines a specific diagonal \"stiffness\" matrix K_modal.\n        # This corresponds to the Sturm-Liouville operator for Legendre polynomials.\n        # K_{nn} = n(n+1) * 2 / (2n + 1).\n        k_diag = n_vals * (n_vals + 1.0) * m_diag\n        K_modal = np.diag(k_diag)\n\n        # Compute quantities for the modal basis.\n        # Condition number of M_modal. For a diagonal matrix, it's max(diag)/min(diag).\n        cond_M_modal = np.max(m_diag) / np.min(m_diag)\n        \n        # 2-norm of K_modal * M_modal.\n        # The product is a diagonal matrix, so its 2-norm is the maximum absolute diagonal entry.\n        km_modal_diag = k_diag * m_diag\n        norm_KM_modal = np.max(np.abs(km_modal_diag))\n\n        # 2. NODAL BASIS (Lagrange polynomials at GLL nodes)\n        # Get the GLL nodes for degree p.\n        gll_nodes = get_gll_nodes(p)\n\n        # Build the Vandermonde-like matrix V, where V_ij = P_j(x_i).\n        # This matrix maps modal coefficients to nodal values.\n        V = legvander(gll_nodes, p)\n\n        # The change-of-basis matrix from nodal to modal coefficients is beta = V^-1.\n        # We solve the system V * beta = I, which is more numerically stable than direct inversion.\n        I = np.identity(N, dtype=float)\n        beta = np.linalg.solve(V, I)\n\n        # Transform the modal mass and stiffness matrices to the nodal basis.\n        # The transformation rule is M_nodal = beta^T * M_modal * beta.\n        M_nodal = beta.T @ M_modal @ beta\n        K_nodal = beta.T @ K_modal @ beta\n        \n        # Compute quantities for the nodal basis.\n        # Condition number of M_nodal. M_nodal is symmetric positive definite.\n        # np.linalg.cond(A, 2) computes the ratio of largest to smallest singular value.\n        cond_M_nodal = np.linalg.cond(M_nodal, 2)\n        \n        # 2-norm of K_nodal * M_nodal.\n        # The product matrix is not guaranteed to be symmetric.\n        # The 2-norm (or operator norm) is its largest singular value.\n        km_nodal_prod = K_nodal @ M_nodal\n        norm_KM_nodal = np.linalg.norm(km_nodal_prod, 2)\n        \n        return [cond_M_modal, cond_M_nodal, norm_KM_modal, norm_KM_nodal]\n\n    results = []\n    for p in test_cases_p:\n        # Main logic to calculate the result for one case goes here.\n        metrics = compute_metrics(p)\n        results.append(metrics)\n\n    # Final print statement in the exact required format.\n    # e.g., [[a,b,c,d],[e,f,g,h],[i,j,k,l]]\n    print(f\"[{','.join(['[' + ','.join(map(str, res)) + ']' for res in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "High-order methods are powerful, but they come with subtleties that can lead to training failure if not handled carefully, especially for nonlinear problems. This exercise  simulates a common numerical pitfall: aliasing instability caused by under-integration. You will intentionally use an insufficient quadrature rule to compute the loss for a nonlinear PDE and observe how this seemingly small shortcut can corrupt the training process and lead to a breakdown of the optimization, providing a tangible lesson on the importance of accurate numerical integration.",
            "id": "3408351",
            "problem": "Consider the one-dimensional nonlinear boundary value problem on the interval $[-1,1]$ defined by the strong form residual $R(x)$\n$$\nR(x) = -\\frac{d^2 u}{dx^2}(x) + \\alpha\\,u(x)^m - f(x),\n$$\nwith homogeneous Dirichlet boundary conditions $u(-1)=0$ and $u(1)=0$. Let the target source be the polynomial $f(x) = \\gamma\\,x^2$, where $\\gamma$ is a positive constant. The unknown $u(x)$ is represented by a polynomial ansatz of degree $p$,\n$$\nu(x) = \\sum_{k=0}^{p} a_k\\,x^k,\n$$\nwhere the coefficients $a_k$ are trainable parameters.\n\nThe PINN (Physics-Informed Neural Network) training objective (residual minimization with boundary penalties) is defined by the functional\n$$\n\\mathcal{J}(a) = \\int_{-1}^{1} R(x)^2\\,dx \\;+\\; \\beta \\left(u(-1)^2 + u(1)^2\\right),\n$$\nwhere $a = (a_0,\\dots,a_p)$ are the trainable coefficients, and $\\beta$ is a positive penalty parameter. Assume gradient-based minimization of $\\mathcal{J}(a)$, where the gradient is computed analytically from the chain rule using the polynomial representation of $u(x)$ and integrated by quadrature.\n\nLet $\\tilde{Q}$ denote a reduced Gauss–Legendre quadrature with $\\tilde{q}$ points on $[-1,1]$ used to approximate the residual integral in both the objective and its gradient during training, while $Q^\\star$ denotes an exact quadrature chosen to integrate the squared residual polynomial exactly. By the classical Gauss–Legendre precision property, a quadrature with $q$ points integrates any polynomial of degree up to $2q-1$ exactly. For the polynomial residual constructed above, the residual polynomial degree is\n$$\nD_R = \\max\\left(p-2,\\,m\\,p,\\,2\\right),\n$$\nand the squared residual integrand has degree\n$$\nD_{R^2} = 2 D_R.\n$$\nTherefore, the exact quadrature can be chosen as $q_\\text{full} = \\left\\lceil \\frac{D_{R^2}+1}{2}\\right\\rceil$ points.\n\nYour task is to write a complete program that:\n- Implements the polynomial ansatz, the residual $R(x)$, and the objective $\\mathcal{J}(a)$.\n- Computes the gradient $\\nabla \\mathcal{J}(a)$ under the reduced quadrature $\\tilde{Q}$ by analytic differentiation, where the derivative of the nonlinearity satisfies $\\frac{\\partial}{\\partial a_j}\\left(u(x)^m\\right) = m\\,u(x)^{m-1}\\,x^j$ and $\\frac{\\partial}{\\partial a_j}\\left(\\frac{d^2 u}{dx^2}(x)\\right) = j(j-1)\\,x^{j-2}$ for $j\\ge 2$, with zero contribution for $j\\in\\{0,1\\}$.\n- Trains the coefficients $a$ by gradient descent for a fixed number of iterations $N$, using the reduced quadrature $\\tilde{Q}$ for both the objective value and its gradient at each step, while evaluating the true objective $\\mathcal{J}(a)$ with $Q^\\star$ only for performance assessment (not for gradient computation).\n- Quantifies aliasing and training behavior through the following metrics for each test case:\n    1. The aliasing threshold ratio\n       $$\n       r_\\text{alias} = \\frac{2\\tilde{q}-1}{D_{R^2}},\n       $$\n       which compares the exactness threshold of $\\tilde{Q}$ to the degree of the squared residual integrand; values below $1$ indicate under-integration and potential aliasing.\n    2. The initial relative quadrature error\n       $$\n       E_\\text{init} = \\frac{\\left|\\mathcal{J}_{\\tilde{Q}}(a^{(0)}) - \\mathcal{J}_{Q^\\star}(a^{(0)})\\right|}{\\mathcal{J}_{Q^\\star}(a^{(0)})},\n       $$\n       with $a^{(0)}$ the initial coefficients.\n    3. The final relative quadrature error\n       $$\n       E_\\text{final} = \\frac{\\left|\\mathcal{J}_{\\tilde{Q}}(a^{(N)}) - \\mathcal{J}_{Q^\\star}(a^{(N)})\\right|}{\\mathcal{J}_{Q^\\star}(a^{(N)})},\n       $$\n       with $a^{(N)}$ the trained coefficients.\n    4. A training breakdown indicator $B$ defined as a boolean,\n       $$\n       B = \\text{true if } \\mathcal{J}_{Q^\\star}(a^{(N)}) \\ge \\left(1 - \\delta\\right)\\,\\mathcal{J}_{Q^\\star}(a^{(0)}), \\text{ otherwise false}\n       $$\n       where $\\delta$ is a prescribed target fractional improvement.\n\nThe training must use a fixed step size $\\eta$ and a fixed number of iterations $N$. The initial coefficients must be set to $a^{(0)} = (0,\\dots,0)$.\n\nYou must implement this for the following test suite of parameter values, which covers a variety of polynomial degrees, nonlinearity strengths, and quadrature under-integration levels:\n- Case $1$: $p=3$, $m=2$, $\\alpha=1.0$, $\\beta=10.0$, $\\gamma=1.0$, $\\tilde{q}=4$, $\\eta=10^{-3}$, $N=200$, $\\delta=0.5$.\n- Case $2$: $p=9$, $m=2$, $\\alpha=1.0$, $\\beta=10.0$, $\\gamma=1.0$, $\\tilde{q}=10$, $\\eta=8\\times 10^{-4}$, $N=250$, $\\delta=0.5$.\n- Case $3$: $p=9$, $m=3$, $\\alpha=1.0$, $\\beta=10.0$, $\\gamma=1.0$, $\\tilde{q}=12$, $\\eta=6\\times 10^{-4}$, $N=300$, $\\delta=0.5$.\n- Case $4$ (edge case): $p=5$, $m=3$, $\\alpha=1.0$, $\\beta=10.0$, $\\gamma=1.0$, $\\tilde{q}=2$, $\\eta=5\\times 10^{-4}$, $N=300$, $\\delta=0.5$.\n\nThe final output format of your program must be a single line that aggregates the results of all test cases into a comma-separated list enclosed in square brackets, where each test case result is a list $[r_\\text{alias},E_\\text{init},E_\\text{final},B]$. Each floating-point number must be printed in decimal form rounded to six decimal places, and the boolean must be printed in the programming language’s canonical boolean format. For example:\n$$\n[\\,[r_1,E_1,F_1,B_1],\\,[r_2,E_2,F_2,B_2],\\,[r_3,E_3,F_3,B_3],\\,[r_4,E_4,F_4,B_4]\\,].\n$$\n\nAll angles, if any were to appear, must be expressed in radians. No physical units are required for this problem. Your implementation must be self-contained and runnable without user input.",
            "solution": "The user has provided a well-defined computational problem to investigate the effects of quadrature aliasing in a Physics-Informed Neural Network (PINN) framework using a polynomial ansatz. The problem is scientifically grounded in numerical analysis and computational mathematics, complete, and internally consistent. It is therefore deemed valid. The following solution provides a step-by-step analytical and algorithmic breakdown.\n\n### 1. Problem Formulation\n\nThe problem considers a one-dimensional nonlinear boundary value problem (BVP) on the domain $x \\in [-1, 1]$:\n$$\n-\\frac{d^2 u}{dx^2}(x) + \\alpha\\,u(x)^m = f(x)\n$$\nwith homogeneous Dirichlet boundary conditions $u(-1) = 0$ and $u(1) = 0$. The source term is given by $f(x) = \\gamma x^2$. The strong form residual is:\n$$\nR(x) = -\\frac{d^2 u}{dx^2}(x) + \\alpha\\,u(x)^m - f(x)\n$$\nThe unknown function $u(x)$ is approximated by a polynomial of degree $p$:\n$$\nu(x; a) = \\sum_{k=0}^{p} a_k\\,x^k\n$$\nwhere $a = (a_0, \\dots, a_p)$ is the vector of trainable coefficients.\n\nThe training objective is to minimize the functional $\\mathcal{J}(a)$, which combines a least-squares residual with a boundary penalty term:\n$$\n\\mathcal{J}(a) = \\int_{-1}^{1} R(x; a)^2\\,dx \\;+\\; \\beta \\left(u(-1; a)^2 + u(1; a)^2\\right)\n$$\nHere, $\\beta > 0$ is a penalty parameter.\n\n### 2. Quadrature and Aliasing\n\nThe integral in the objective functional is computed numerically using Gauss-Legendre quadrature. A quadrature rule with $q$ points, $(x_i, w_i)_{i=1}^q$, approximates an integral as:\n$$\n\\int_{-1}^{1} g(x)\\,dx \\approx \\sum_{i=1}^{q} w_i\\,g(x_i)\n$$\nThis approximation is exact if $g(x)$ is a polynomial of degree at most $2q-1$.\n\nThe problem specifies two quadrature rules:\n1.  An **exact quadrature** $Q^\\star$ with $q_\\text{full}$ points, chosen to integrate the squared residual polynomial $R(x;a)^2$ exactly. The degree of $R(x;a)$ is $D_R = \\max(p-2, m p, 2)$, so the degree of $R(x;a)^2$ is $D_{R^2} = 2 D_R$. Thus, we need $2q_\\text{full}-1 \\ge D_{R^2}$, which gives $q_\\text{full} = \\lceil (D_{R^2}+1)/2 \\rceil$.\n2.  A **reduced quadrature** $\\tilde{Q}$ with $\\tilde{q}$ points, where $\\tilde{q} < q_\\text{full}$. Using $\\tilde{Q}$ to integrate $R(x;a)^2$ can lead to aliasing errors if $2\\tilde{q}-1 < D_{R^2}$. This inexact integration is used during the training process for both the objective function evaluation and its gradient.\n\n### 3. Gradient-Based Optimization\n\nThe coefficients $a$ are trained using gradient descent. The update rule at iteration $k$ is:\n$$\na^{(k+1)} = a^{(k)} - \\eta \\nabla \\mathcal{J}_{\\tilde{Q}}(a^{(k)})\n$$\nwhere $\\eta$ is the learning rate and $\\nabla \\mathcal{J}_{\\tilde{Q}}$ is the gradient of the objective functional computed using the reduced quadrature $\\tilde{Q}$.\n\nThe components of the gradient, $\\frac{\\partial \\mathcal{J}}{\\partial a_j}$, are derived using the chain rule:\n$$\n\\frac{\\partial \\mathcal{J}}{\\partial a_j} = \\frac{\\partial}{\\partial a_j} \\left( \\int_{-1}^{1} R(x; a)^2\\,dx \\right) + \\frac{\\partial}{\\partial a_j} \\left( \\beta \\left(u(-1; a)^2 + u(1; a)^2\\right) \\right)\n$$\nApproximating the integral with a quadrature rule with $q$ points $(x_i, w_i)$:\n$$\n\\frac{\\partial \\mathcal{J}}{\\partial a_j} \\approx \\sum_{i=1}^{q} w_i \\cdot 2 R(x_i; a) \\frac{\\partial R(x_i; a)}{\\partial a_j} + 2\\beta \\left( u(-1; a) \\frac{\\partial u(-1; a)}{\\partial a_j} + u(1; a) \\frac{\\partial u(1; a)}{\\partial a_j} \\right)\n$$\nThe required partial derivatives of the polynomial terms are:\n- $\\frac{\\partial u(x; a)}{\\partial a_j} = \\frac{\\partial}{\\partial a_j} \\left( \\sum_{k=0}^{p} a_k x^k \\right) = x^j$\n- $\\frac{\\partial}{\\partial a_j} \\left( \\frac{d^2u}{dx^2}(x; a) \\right) = \\frac{\\partial}{\\partial a_j} \\left( \\sum_{k=2}^{p} a_k k(k-1)x^{k-2} \\right) = j(j-1)x^{j-2}$ for $j \\ge 2$, and $0$ otherwise.\n- $\\frac{\\partial R(x; a)}{\\partial a_j} = -\\frac{\\partial}{\\partial a_j}\\left(\\frac{d^2u}{dx^2}\\right) + \\alpha m\\, u(x;a)^{m-1} \\frac{\\partial u(x;a)}{\\partial a_j} = -j(j-1)x^{j-2} + \\alpha m\\, u(x;a)^{m-1} x^j$\n\nThe training process uses the reduced quadrature (with $\\tilde{q}$ points) to compute this gradient.\n\n### 4. Algorithmic Implementation\n\nThe solution is implemented by representing all functions as polynomial objects, which simplifies operations like differentiation and exponentiation.\n\n1.  **Initialization**: For each test case, parameters $(p, m, \\alpha, \\beta, \\gamma, \\tilde{q}, \\eta, N, \\delta)$ are set. The polynomial coefficients are initialized to zero: $a^{(0)} = (0, \\dots, 0)_{p+1}$. The degrees $D_R$ and $D_{R^2}$ are computed, and the number of points for exact quadrature, $q_\\text{full}$, is determined.\n\n2.  **Function and Gradient Evaluation**:\n    - A function `compute_J(a, q, params)` is implemented. It takes coefficients `a`, number of quadrature points `q`, and problem parameters. It constructs the polynomial objects for `u`, `u''`, `u^m`, and `R`, evaluates $R(x)^2$ at the Gauss-Legendre points for the given `q`, performs the weighted sum, and adds the boundary penalty term.\n    - A function `compute_grad_J(a, q_tilde, params)` is implemented. It calculates the gradient vector $\\nabla \\mathcal{J}_{\\tilde{Q}}(a)$. For each component $j=0, \\dots, p$, it evaluates the analytical gradient expression at the $\\tilde{q}$ quadrature points and performs the summation.\n\n3.  **Training Loop**:\n    - The initial objective values $\\mathcal{J}_{\\tilde{Q}}(a^{(0)})$ and $\\mathcal{J}_{Q^\\star}(a^{(0)})$ are computed.\n    - A loop runs for $N$ iterations:\n        - The gradient $\\nabla \\mathcal{J}_{\\tilde{Q}}(a^{(k)})$ is computed.\n        - The coefficients are updated: $a^{(k+1)} = a^{(k)} - \\eta \\nabla \\mathcal{J}_{\\tilde{Q}}(a^{(k)})$.\n\n4.  **Metrics Calculation**:\n    - After $N$ iterations, we have the final coefficients $a^{(N)}$.\n    - The final objective values $\\mathcal{J}_{\\tilde{Q}}(a^{(N)})$ and $\\mathcal{J}_{Q^\\star}(a^{(N)})$ are computed.\n    - The four specified metrics are calculated:\n        - **Aliasing threshold ratio**: $r_\\text{alias} = (2\\tilde{q}-1)/D_{R^2}$.\n        - **Initial relative quadrature error**: $E_\\text{init} = |\\mathcal{J}_{\\tilde{Q}}(a^{(0)}) - \\mathcal{J}_{Q^\\star}(a^{(0)})| / \\mathcal{J}_{Q^\\star}(a^{(0)})$. With $a^{(0)}=(0,\\ldots,0)$, $R(x) = -\\gamma x^2$, with degree 2. So $R^2$ has degree 4. Any quadrature with $2q-1 \\ge 4 \\implies q \\ge 3$ is exact. Since all $\\tilde{q} \\ge 2$, some cases might have initial error, but for the given test cases where $\\tilde{q} \\ge 4$ (for nonzero $J_{init}$ calculation), $E_\\text{init}$ will be zero. For $\\tilde{q}=2$, $2\\tilde{q}-1=3 < 4$, so there will be an initial error.\n        - **Final relative quadrature error**: $E_\\text{final} = |\\mathcal{J}_{\\tilde{Q}}(a^{(N)}) - \\mathcal{J}_{Q^\\star}(a^{(N)})| / \\mathcal{J}_{Q^\\star}(a^{(N)})$.\n        - **Training breakdown indicator**: $B = (\\mathcal{J}_{Q^\\star}(a^{(N)}) \\ge (1 - \\delta)\\mathcal{J}_{Q^\\star}(a^{(0)}))$.\n\nThis structured approach allows for a direct simulation of the training dynamics under quadrature-aliasing and a quantitative assessment of its impact.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.polynomial import Polynomial\nfrom numpy.polynomial.legendre import leggauss\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (p, m, alpha, beta, gamma, q_tilde, eta, N, delta)\n        (3, 2, 1.0, 10.0, 1.0, 4, 1e-3, 200, 0.5),\n        (9, 2, 1.0, 10.0, 1.0, 10, 8e-4, 250, 0.5),\n        (9, 3, 1.0, 10.0, 1.0, 12, 6e-4, 300, 0.5),\n        (5, 3, 1.0, 10.0, 1.0, 2, 5e-4, 300, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        p, m, alpha, beta, gamma, q_tilde, eta, N, delta = case\n        \n        # Determine polynomial degrees and required quadrature for exactness\n        D_R = max(p - 2, m * p, 2)\n        D_R2 = 2 * D_R\n        q_full = int(np.ceil((D_R2 + 1) / 2))\n\n        # Initial coefficients\n        a_0 = np.zeros(p + 1)\n\n        # 1. Alias ratio\n        r_alias = (2 * q_tilde - 1) / D_R2\n\n        # 2. Initial relative quadrature error\n        J_tilde_init = compute_J(a_0, q_tilde, p, m, alpha, beta, gamma)\n        J_star_init = compute_J(a_0, q_full, p, m, alpha, beta, gamma)\n        if J_star_init == 0:\n            E_init = 0.0\n        else:\n            E_init = np.abs(J_tilde_init - J_star_init) / J_star_init\n        \n        # Training loop\n        a_k = np.copy(a_0)\n        for _ in range(N):\n            grad = compute_grad_J(a_k, q_tilde, p, m, alpha, beta, gamma)\n            a_k -= eta * grad\n        a_N = a_k\n\n        # 3. Final relative quadrature error\n        J_tilde_final = compute_J(a_N, q_tilde, p, m, alpha, beta, gamma)\n        J_star_final = compute_J(a_N, q_full, p, m, alpha, beta, gamma)\n        \n        if J_star_final == 0:\n            # This case occurs if the optimizer finds the exact zero-residual solution\n            # and the quadrature error is also zero.\n            E_final = 0.0\n        else:\n            E_final = np.abs(J_tilde_final - J_star_final) / J_star_final\n\n        # 4. Training breakdown indicator\n        B = J_star_final >= (1 - delta) * J_star_init\n        \n        results.append(f\"[{r_alias:.6f},{E_init:.6f},{E_final:.6f},{str(B).lower()}]\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef compute_J(a, q, p, m, alpha, beta, gamma):\n    \"\"\"\n    Computes the objective functional J(a) using a quadrature with q points.\n    \"\"\"\n    u = Polynomial(a)\n    u_xx = u.deriv(2)\n    f = Polynomial([0, 0, gamma])\n    \n    # Nonlinear term u^m can be computationally heavy, handle case by case.\n    if m == 2:\n        u_m = u * u\n    elif m == 3:\n        u_m = u * u * u\n    else: # general case\n        u_m = u**m\n\n    R = -u_xx + alpha * u_m - f\n    \n    x_q, w_q = leggauss(q)\n    \n    R_vals_sq = R(x_q)**2\n    res_integral = np.sum(w_q * R_vals_sq)\n    \n    boundary_term = beta * (u(-1)**2 + u(1)**2)\n    \n    return res_integral + boundary_term\n\ndef compute_grad_J(a, q_tilde, p, m, alpha, beta, gamma):\n    \"\"\"\n    Computes the gradient of J(a) using reduced quadrature with q_tilde points.\n    \"\"\"\n    u = Polynomial(a)\n    u_xx = u.deriv(2)\n    f = Polynomial([0, 0, gamma])\n\n    if m == 2:\n        u_m = u * u\n    elif m == 3:\n        u_m = u * u * u\n    else:\n        u_m = u**m\n\n    R = -u_xx + alpha * u_m - f\n    \n    x_q, w_q = leggauss(q_tilde)\n    grad = np.zeros(p + 1)\n    \n    # Pre-compute values at quadrature points for efficiency\n    R_vals = R(x_q)\n    u_vals = u(x_q)\n\n    # Handle cases for u^(m-1) to avoid 0^0. m is >= 2 in test cases.\n    if m > 1:\n        u_m1_vals = u_vals**(m - 1)\n    else: # m=1\n        u_m1_vals = np.ones_like(x_q)\n\n    for j in range(p + 1):\n        # dR/da_j = -d(u_xx)/da_j + alpha * m * u^(m-1) * du/da_j\n        \n        # du/da_j = x^j\n        du_daj_vals = x_q**j\n        \n        # d(u_xx)/da_j = j(j-1)x^(j-2)\n        if j >= 2:\n            du_xx_daj_vals = j * (j - 1) * (x_q**(j - 2))\n        else:\n            du_xx_daj_vals = np.zeros_like(x_q)\n            \n        dR_daj_vals = -du_xx_daj_vals + alpha * m * u_m1_vals * du_daj_vals\n        \n        # Integrand for the gradient component: 2 * R * dR/da_j\n        integrand_vals = 2 * R_vals * dR_daj_vals\n        integral_part = np.sum(w_q * integrand_vals)\n        \n        # Boundary term contribution\n        # d/da_j(u(-1)^2 + u(1)^2) = 2*u(-1)*(-1)^j + 2*u(1)*(1)^j\n        boundary_part = beta * (2 * u(-1) * ((-1)**j) + 2 * u(1) * (1**j))\n        \n        grad[j] = integral_part + boundary_part\n        \n    return grad\n\nsolve()\n```"
        }
    ]
}