{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络（PINN）的训练核心在于最小化一个精心构造的损失函数。本练习将指导您为一个包含偏微分方程（PDE）残差和物理守恒定律（质量守恒）的复合损失函数推导其梯度。掌握对网络参数的解析梯度计算是理解PINN训练机制和开发自定义模型的基石。",
            "id": "3408367",
            "problem": "考虑周期性空间域 $[0,1]$ 上的粘性 Burgers 方程，\n$$u_{t}(x,t)+u(x,t)\\,u_{x}(x,t)=\\nu\\,u_{xx}(x,t),$$\n其中粘度参数 $\\nu0$，且在 $x$ 方向上具有周期性边界条件。令 $u_{\\theta}(x,t)$ 是一个由物理信息神经网络 (PINN) 提供的光滑参数化拟设，其中 $\\theta=\\{\\theta_{k}\\}_{k=1}^{p}$ 表示网络参数，且对于下文所需的阶数，所有混合偏导数 $\\partial^{\\alpha+\\beta}u_{\\theta}/\\partial x^{\\alpha}\\partial t^{\\beta}$ 都存在且连续。为引入适用于周期性域的高阶空间离散化，设置 $N_{s}$ 个等距 Fourier 配置点\n$$x_{j}=\\frac{j}{N_{s}},\\quad j=0,1,\\dots,N_{s}-1,$$\n以及在感兴趣的时间区间内设置 $N_{t}$ 个时间配置点 $\\{t_{i}\\}_{i=1}^{N_{t}}$。令 $\\mathbf{D}\\in\\mathbb{R}^{N_{s}\\times N_{s}}$ 和 $\\mathbf{D}^{(2)}\\in\\mathbb{R}^{N_{s}\\times N_{s}}$ 表示此网格上的一阶和二阶 Fourier 谱微分矩阵，作用于固定时间下的空间节点值。对于空间积分，使用具有均匀权重 $w_{j}=\\frac{1}{N_{s}}$ 的梯形法则，该法则对于周期性的、足够光滑的函数是谱精确的。\n\n定义每个时空配置点对 $(x_{j},t_{i})$ 上的逐点物理残差为\n$$R_{ij}(\\theta)=\\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j},$$\n其中 $\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\in\\mathbb{R}^{N_{s}}$ 汇集了 $\\{u_{\\theta}(x_{j},t_{i})\\}_{j=0}^{N_{s}-1}$，下标 $(\\cdot)_{j}$ 选择第 $j$ 个分量。对于具有周期性边界条件的光滑解，周期质量 $\\int_{0}^{1}u(x,t)\\,dx$ 不随时间变化；通过要求在时间配置点上 $\\int_{0}^{1}u_{\\theta}(x,t_{i})\\,dx=M$（其中 $M\\in\\mathbb{R}$ 为预设的恒定质量）来将其作为软约束施加。\n\n设计以下复合损失函数，\n$$\\mathcal{L}(\\theta)=\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}+\\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2},$$\n其中权重 $\\lambda_{r}0$ 且 $\\lambda_{m}0$。仅从 Burgers 方程、周期性边界条件、上述定义以及标准微积分法则出发，推导 $\\mathcal{L}(\\theta)$ 相对于通用网络参数 $\\theta_{k}$ 的符号梯度，并用 $u_{\\theta}$、其参数敏感度 $\\partial u_{\\theta}/\\partial\\theta_{k}$ 以及作用于空间节点值的谱微分算子来表示结果。您必须显式地处理非线性平流项的链式法则，并证明在任何使用的地方参数微分与空间和时间微分的可交换性。\n\n您的最终答案必须是关于 $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{k}}$ 的单一闭式解析表达式，用上述定义的量表示。不需要数值近似或四舍五入。",
            "solution": "该问题要求推导复合损失函数 $\\mathcal{L}(\\theta)$ 相对于通用神经网络参数 $\\theta_k$ 的梯度。该损失函数是为训练一个物理信息神经网络 (PINN) 以求解周期域上的粘性 Burgers 方程而设计的。\n\n**问题验证**\n\n首先，对问题陈述进行验证。\n\n*   **已知条件：**\n    *   **偏微分方程(PDE)：** 粘性 Burgers 方程，$u_{t}(x,t)+u(x,t)\\,u_{x}(x,t)=\\nu\\,u_{xx}(x,t)$，在 $x \\in [0,1]$ 上。\n    *   **边界条件：** 在 $x$ 方向上是周期性的。\n    *   **拟设：** 一个光滑的参数化函数 $u_{\\theta}(x,t)$，其中 $\\theta=\\{\\theta_{k}\\}_{k=1}^{p}$。所有必要的混合偏导数都存在且连续。\n    *   **离散化：** $N_{s}$ 个等距空间点 $x_{j}=\\frac{j}{N_{s}}$，其中 $j=0,1,\\dots,N_{s}-1$，以及 $N_{t}$ 个时间点 $\\{t_{i}\\}_{i=1}^{N_{t}}$。\n    *   **谱算子：** 一阶和二阶 Fourier 谱微分矩阵 $\\mathbf{D}$ 和 $\\mathbf{D}^{(2)}$。\n    *   **求积：** 梯形法则，权重为 $w_{j}=\\frac{1}{N_{s}}$。\n    *   **逐点残差：** $R_{ij}(\\theta)=\\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}$。\n    *   **质量守恒：** $\\int_{0}^{1}u_{\\theta}(x,t_{i})\\,dx=M$，其中 $M$ 为常数。\n    *   **损失函数：** $\\mathcal{L}(\\theta)=\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}+\\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2}$，其中 $\\lambda_r  0$，$\\lambda_m  0$。\n\n*   **验证：**\n    该问题具有科学依据，是适定的、客观的。它描述了科学机器学习中一种标准的、现代的方法。其组成部分（Burgers 方程、谱方法、PINN 损失函数）都是标准的且表述正确。诸如 $u_{\\theta}$ 的光滑性等假设都已明确说明，并且足以进行所需的推导。\n    原问题陈述中残差项的求和索引为 $j=1, \\dots, N_s$，而配置点定义为 $j=0, \\dots, N_s-1$。这是一个小的记法不一致。为与配置点和质量守恒项的定义保持一致，我们假定残差的求和也应覆盖所有 $N_s$ 个网格点，即从 $j=0$ 到 $N_s-1$。该修正在问题描述中已更正，不影响问题核心结构的有效性。\n\n*   **结论：** 问题是有效的。\n\n**梯度推导**\n\n损失函数 $\\mathcal{L}(\\theta)$ 是两个分量的和：残差损失 $\\mathcal{L}_r(\\theta)$ 和质量守恒损失 $\\mathcal{L}_m(\\theta)$。\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_r(\\theta) + \\mathcal{L}_m(\\theta)\n$$\n其中\n$$\n\\mathcal{L}_r(\\theta) = \\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}\n$$\n$$\n\\mathcal{L}_m(\\theta) = \\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2}\n$$\n根据微分的线性性质，梯度是各分量梯度的和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{k}} = \\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}} + \\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}}\n$$\n我们将分别计算每一项。\n\n**1. 残差损失 $\\mathcal{L}_r(\\theta)$ 的梯度**\n\n使用链式法则，我们对 $\\mathcal{L}_r(\\theta)$ 关于通用参数 $\\theta_k$ 进行微分：\n$$\n\\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}\\right) = \\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} 2 R_{ij}(\\theta) \\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}}\n$$\n为了继续，我们必须计算残差的导数 $\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}}$。残差由以下公式给出：\n$$\nR_{ij}(\\theta) = \\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\n$$\n我们对 $R_{ij}(\\theta)$ 的每一项关于 $\\theta_k$ 进行微分：\n$$\n\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\frac{\\partial u_{\\theta}}{\\partial t}\\right) + \\frac{\\partial}{\\partial \\theta_{k}}\\left(u_{\\theta}\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}\\right)_{j}\\right) - \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}\\right)_{j}\\right)\n$$\n其中所有函数都在 $(x_j, t_i)$ 处求值。\n\n*   **第一项（时间导数）：** 问题陈述中指出 $u_{\\theta}$ 是光滑的，并且其所有混合偏导数都存在且连续。这使我们能够应用 Clairaut 定理来交换微分顺序：\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\frac{\\partial u_{\\theta}}{\\partial t}\\right) = \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)\n    $$\n*   **第二项（非线性平流）：** 我们应用乘法法则进行微分：\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right) = \\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_{j},t_{i})\\right)\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j} + u_{\\theta}(x_{j},t_{i})\\frac{\\partial}{\\partial \\theta_{k}}\\left(\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right)\n    $$\n    微分矩阵 $\\mathbf{D}$ 和 $\\mathbf{D}^{(2)}$ 相对于 $\\theta$ 是常数。导数 $\\frac{\\partial}{\\partial\\theta_k}$ 作用于节点值向量 $\\mathbf{u}_{\\theta}(\\cdot, t_i)$。关于 $\\theta_k$ 的微分和矩阵-向量乘法（一个线性运算）可以交换顺序：\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right) = \\mathbf{D}\\,\\left(\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)\n    $$\n    这里，$\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}$ 是参数敏感度向量，其第 $j$ 个分量是 $\\frac{\\partial u_{\\theta}(x_j, t_i)}{\\partial \\theta_k}$。因此，第二项变为：\n    $$\n    \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_{j},t_{i}) \\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j} + u_{\\theta}(x_{j},t_{i}) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)_{j}\n    $$\n*   **第三项（粘度）：** 粘度 $\\nu$ 和矩阵 $\\mathbf{D}^{(2)}$ 是常数。使用与平流项相同的论证：\n    $$\n    -\\frac{\\partial}{\\partial \\theta_{k}}\\left(\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right) = -\\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)_{j}\n    $$\n\n综合这些结果，残差的导数为：\n$$\n\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) (\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i))_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j}\n$$\n将此代入 $\\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}}$ 的表达式中，得到其完整形式。\n\n**2. 质量守恒损失 $\\mathcal{L}_m(\\theta)$ 的梯度**\n\n令 $C_i(\\theta) = \\sum_{j=0}^{N_s-1} w_j u_{\\theta}(x_j, t_i) - M$。损失项为 $\\mathcal{L}_m(\\theta) = \\lambda_m \\frac{1}{N_t} \\sum_{i=1}^{N_t} C_i(\\theta)^2$。\n应用链式法则：\n$$\n\\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}} = \\lambda_m \\frac{1}{N_t} \\sum_{i=1}^{N_t} 2 C_i(\\theta) \\frac{\\partial C_i(\\theta)}{\\partial \\theta_k}\n$$\n内部项 $C_i(\\theta)$ 的导数为：\n$$\n\\frac{\\partial C_i(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k}\\left(\\sum_{j=0}^{N_s-1} w_j u_{\\theta}(x_j, t_i) - M\\right) = \\sum_{j=0}^{N_s-1} w_j \\frac{\\partial u_{\\theta}(x_j, t_i)}{\\partial \\theta_k}\n$$\n因为权重 $w_j$ 和质量 $M$ 是常数。\n将其代回，我们得到：\n$$\n\\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}} = \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)\n$$\n\n**3. 总梯度**\n\n梯度 $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}$ 的最终表达式是两个损失分量导数的和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k} = \\frac{2\\lambda_{r}}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} R_{ij}(\\theta) \\left[ \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) (\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i))_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} \\right] \\\\\n+ \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)\n$$\n这就是所要求的符号梯度，用已定义的量表示。",
            "answer": "$$\n\\boxed{\\frac{2\\lambda_{r}}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} R_{ij}(\\theta) \\left[ \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) \\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i)\\right)_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} \\right] + \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)}\n$$"
        },
        {
            "introduction": "从“如何训练”转向“为何训练困难”是深入理解PINN的关键一步。本练习使用经典的热方程来揭示谱刚度（spectral stiffness）的概念，这是PINN训练面临的一大挑战。通过运用傅里叶分析，您将理解PDE算子自身的性质如何导致训练过程中的“谱偏差”（spectral bias），并学会如何利用这些分析来指导设计有效的预处理策略，从而加速和稳定训练过程。",
            "id": "3408301",
            "problem": "考虑一维热方程 $u_{t} = \\kappa u_{xx}$，定义在周期性域 $x \\in [0,L]$ 上，其中热扩散系数 $\\kappa  0$。设 $u(x,t)$ 具有傅里叶级数表示 $u(x,t) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}(t) \\exp(\\mathrm{i} \\xi_{k} x)$，其中 $\\xi_{k} = \\frac{2\\pi k}{L}$ 且 $\\mathrm{i}$ 表示虚数单位。使用时间步长 $\\Delta t  0$ 的 Backward Euler 时间离散化定义为 $(u^{n+1} - u^{n})/\\Delta t = \\kappa (u^{n+1})_{xx}$，其中 $u^{n}(x) \\approx u(x, t^{n})$ 且 $t^{n+1} = t^{n} + \\Delta t$。在空间上使用傅里叶谱方法，推导在此隐式格式下谱系数 $\\hat{u}_{k}^{n}$ 的离散演化方程，并推导出模态阻尼因子 $g_{k}$，使得 $\\hat{u}_{k}^{n+1} = g_{k} \\, \\hat{u}_{k}^{n}$。根据模态正交性和线性算子对角化的第一性原理，简要解释如何利用此阻尼因子对物理信息神经网络 (Physics-Informed Neural Network, PINN) 的训练损失进行预处理，以缓解采用高阶离散化时的谱刚度问题。\n\n作为最终答案，提供一个关于 $\\kappa$、$\\Delta t$、$k$ 和 $L$ 的 $g_{k}$ 的单一闭式解析表达式。最终答案不应包含单位，也不应提供任何额外评论。",
            "solution": "该问题陈述经核实具有科学依据、良定且客观，不含任何矛盾或歧义。我们可以着手求解。\n\n问题考虑的是定义在周期性域 $x \\in [0,L]$ 上的一维热方程，其热扩散系数为 $\\kappa  0$：\n$$u_{t} = \\kappa u_{xx}$$\n其中 $u(x,t)$ 是场变量（例如，温度），下标表示偏微分。\n\n时间离散化采用 Backward Euler 格式，时间步长为 $\\Delta t  0$。这种隐式格式将时间步 $n+1$ 的解与时间步 $n$ 的解关联起来，如下所示：\n$$\\frac{u^{n+1}(x) - u^{n}(x)}{\\Delta t} = \\kappa (u^{n+1})_{xx}$$\n其中 $u^{n}(x)$ 是解 $u(x, t^{n})$ 在时间 $t^{n} = n \\Delta t$ 的数值近似。重新整理该方程可以清楚地显示其隐式性质：\n$$u^{n+1}(x) - \\Delta t \\kappa (u^{n+1})_{xx} = u^{n}(x)$$\n\n对于空间离散化，采用傅里叶谱方法。每个时间步 $n$ 的解由其在周期性域上的傅里叶级数表示：\n$$u^{n}(x) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\n其中 $\\hat{u}_{k}^{n}$ 是在时间 $t^n$ 的复傅里叶系数，$\\mathrm{i}$ 是虚数单位，$\\xi_{k} = \\frac{2\\pi k}{L}$ 是对于任意整数 $k$ 的离散空间波数。\n\n对于具有常系数的线性偏微分方程，傅里叶谱方法的主要优点是微分算子在傅里叶空间中变为简单的乘法。$u^{n+1}(x)$ 的空间二阶导数表示为：\n$$(u^{n+1})_{xx} = \\frac{\\partial^2}{\\partial x^2} \\left( \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x) \\right) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n+1} (\\mathrm{i} \\xi_{k})^{2} \\exp(\\mathrm{i} \\xi_{k} x) = \\sum_{k \\in \\mathbb{Z}} (-\\xi_{k}^{2}) \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x)$$\n\n现在，我们将 $u^{n}(x)$、$u^{n+1}(x)$ 和 $(u^{n+1})_{xx}$ 的傅里叶级数表示代入 Backward Euler 方程中：\n$$\\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x) - \\Delta t \\kappa \\left( \\sum_{k \\in \\mathbb{Z}} (-\\xi_{k}^{2}) \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x) \\right) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\n根据线性性质，我们可以将基函数 $\\exp(\\mathrm{i} \\xi_{k} x)$ 提取出来：\n$$\\sum_{k \\in \\mathbb{Z}} \\left( \\hat{u}_{k}^{n+1} + \\Delta t \\kappa \\xi_{k}^{2} \\hat{u}_{k}^{n+1} \\right) \\exp(\\mathrm{i} \\xi_{k} x) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\n$$\\sum_{k \\in \\mathbb{Z}} \\left[ \\hat{u}_{k}^{n+1} (1 + \\Delta t \\kappa \\xi_{k}^{2}) \\right] \\exp(\\mathrm{i} \\xi_{k} x) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\n函数族 $\\{\\exp(\\mathrm{i} \\xi_{k} x)\\}_{k \\in \\mathbb{Z}}$ 在区间 $[0,L]$ 上是正交的。这种正交性允许我们对每个模态 $k$ 的系数分别建立等式，从而将偏微分方程转化为一组关于傅里叶系数的代数方程组：\n$$\\hat{u}_{k}^{n+1} (1 + \\Delta t \\kappa \\xi_{k}^{2}) = \\hat{u}_{k}^{n}$$\n这就是在指定隐式格式下谱系数的离散演化方程。为了找到模态阻尼因子 $g_k$，我们求解 $\\hat{u}_{k}^{n+1}$：\n$$\\hat{u}_{k}^{n+1} = \\frac{1}{1 + \\Delta t \\kappa \\xi_{k}^{2}} \\hat{u}_{k}^{n}$$\n通过与所要求的形式 $\\hat{u}_{k}^{n+1} = g_{k} \\, \\hat{u}_{k}^{n}$ 进行比较，我们确定阻尼因子 $g_k$ 为：\n$$g_{k} = \\frac{1}{1 + \\Delta t \\kappa \\xi_{k}^{2}}$$\n最后，代入波数的定义 $\\xi_{k} = \\frac{2\\pi k}{L}$，我们得到其显式形式：\n$$g_{k} = \\frac{1}{1 + \\Delta t \\kappa \\left(\\frac{2\\pi k}{L}\\right)^{2}} = \\frac{1}{1 + \\frac{4\\pi^2 k^2 \\kappa \\Delta t}{L^2}}$$\n\n关于问题的第二部分，即对 PINN 损失进行预处理：\n从第一性原理出发，线性算子 $\\mathcal{L}u = \\kappa u_{xx}$ 被傅里叶基函数 $\\exp(\\mathrm{i} \\xi_k x)$ 对角化。相应的特征值为 $\\lambda_k = -\\kappa \\xi_k^2$。每个模态的连续动力学由 $\\frac{d\\hat{u}_k}{dt} = \\lambda_k \\hat{u}_k$ 控制，这导致衰减率与 $\\xi_k^2$ 成正比。这种二次依赖性在低频模态（小 $|k|$，演化缓慢）和高频模态（大 $|k|$，衰减极快）之间造成了巨大的时间尺度分离。这种性质被称为刚度。\n\nPhysics-Informed Neural Network (PINN) 通过最小化基于偏微分方程残差 $R = u_t - \\kappa u_{xx}$ 的损失函数进行训练。由于偏微分方程算子的刚度，与残差的高频分量相关的训练梯度的大小可能与低频分量的梯度大小差异巨大。这种不平衡通常被称为谱偏差（spectral bias），它使得优化问题变得病态，导致收敛缓慢，并且难以学习解的高频特征。\n\n模态正交性原理允许将偏微分方程残差分解为其谱分量 $\\hat{R}_k$。阻尼因子 $g_k$ 量化了系统动力学中固有的对高频模态的强抑制作用。为了创造一个更平衡、条件良好的优化环境，可以对 PINN 损失进行预处理。这涉及到在傅里叶域中对损失函数进行重新加权。一个预处理后的损失可以采用 $L_{precond} = \\sum_k w_k |\\hat{R}_k|^2$ 的形式，其中权重 $w_k$ 的选择旨在抵消刚度。推导出的阻尼因子 $g_k$ 为这些权重的选择提供了信息。由于高频模态被强烈阻尼（即对于大的 $|k|$，有 $g_k \\ll 1$），一个合适的预处理器将对这些模态施加较大的权重。例如，一个与 $(1 - g_k)^{-1} \\approx (\\Delta t \\kappa \\xi_k^2)^{-1}$（对于小 $\\Delta t$）或简单地与 $\\xi_k^2$ 成比例的权重 $w_k$，会放大高频残差的贡献。这平衡了整个频谱上的梯度信号，迫使神经网络更均匀地学习所有模态分量，从而减轻了谱刚度的影响。",
            "answer": "$$\\boxed{\\frac{1}{1 + \\frac{4 \\pi^{2} k^{2} \\kappa \\Delta t}{L^{2}}}}$$"
        },
        {
            "introduction": "将高阶方法应用于PINN时，一个关键的实践问题是如何选择合适的基函数来参数化解。本编码练习旨在将理论选择付诸实践，通过比较模态基（勒让德多项式）与节点基（拉格朗日多项式），您将亲手计算并分析它们在数值稳定性（条件数）和梯度传播效率上的差异。这项实践揭示了不同实现方式之间的重要权衡，为构建高性能的高阶PINN模型提供了宝贵的经验。",
            "id": "3408343",
            "problem": "要求您实现并比较两种在物理信息神经网络 (PINN) 的谱方法和间断 Galerkin 方法中常用的多项式参数化。一种参数化是使用 Legendre 多项式的模态基，另一种是使用 Gauss–Lobatto–Legendre (GLL) 节点上的 Lagrange 多项式的节点基。主要目标是量化质量矩阵的条件数，并量化在高阶多项式下通过质量矩阵和刚度矩阵的梯度传播。\n\n使用以下基本要素：\n- 对于作用于未知量 $u$ 的算子 $L$，物理信息神经网络 (PINN) 的损失由残差 $r = L u - f$ 和一个 $L^2$ 内积诱导的能量范数 $\\|r\\|_{M}^2 = r^T M r$ 定义，其中 $M$ 是与在区间 $[-1,1]$ 上具有单位权重的所选基和内积相关联的质量矩阵。\n- 在 $[-1,1]$ 上的 Legendre 多项式 $\\{P_n(x)\\}_{n=0}^{p}$ 满足正交关系 $\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm}$。\n- 在 Gauss–Lobatto–Legendre 节点 $\\{x_i\\}_{i=0}^{p}$ 上的 Lagrange 节点基函数 $\\{ \\ell_i(x) \\}_{i=0}^{p}$ 满足 $\\ell_i(x_j) = \\delta_{ij}$。内部 GLL 节点是 $P_p(x)$ 导数的根，端点是 $-1$ 和 $1$。\n- 质量矩阵 $M$ 和刚度矩阵 $K$ 分别由 $M_{ij} = \\int_{-1}^{1} \\phi_i(x) \\phi_j(x) \\, dx$ 和 $K_{ij} = \\int_{-1}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$ 定义，其中 $\\{\\phi_i\\}$ 是所选的基。\n- 在线性残差模型 $r = A c - b$（其中 $A = K$）中，PINN 损失相对于系数 $c$ 的梯度，并由质量矩阵加权，为 $\\nabla L(c) = K^T M (K c - b)$。控制梯度传播的一个关键量是算子范数 $\\|K^T M\\|_2 = \\|K M\\|_2$。\n- 对称正定矩阵 $M$ 在 $2$-范数下的条件数是 $\\kappa(M) = \\lambda_{\\max}(M)/\\lambda_{\\min}(M)$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 是 $M$ 的最大和最小特征值。\n\n任务要求：\n1. 在区间 $[-1,1]$ 上以单位权重构建 $p$ 次的模态 Legendre 基。在此基中，使用精确的对角质量矩阵 $M_{\\text{modal}}$，其元素为 $M_{nn} = \\frac{2}{2n+1}$（对于 $n = 0,1,\\dots,p$），以及精确的对角刚度矩阵 $K_{\\text{modal}}$，其元素为 $K_{nn} = n(n+1) \\frac{2}{2n+1}$。\n2. 构建 $p$ 次的 Gauss–Lobatto–Legendre 节点上的节点 Lagrange 基。将每个节点基函数 $\\ell_i(x)$ 在 Legendre 模态基中表示为 $\\ell_i(x) = \\sum_{n=0}^{p} \\beta_{n i} P_n(x)$，其中系数列向量 $\\beta_{\\cdot i}$ 对于所有节点 $x_j$ 满足 $\\sum_{n=0}^{p} \\beta_{n i} P_n(x_j) = \\delta_{i j}$。通过构建元素为 $V_{j n} = P_n(x_j)$ 的方阵求值矩阵 $V \\in \\mathbb{R}^{(p+1)\\times(p+1)}$ 并求解 $V \\beta = I_{p+1}$ 得到 $\\beta \\in \\mathbb{R}^{(p+1)\\times(p+1)}$ 来实现。\n3. 使用基变换系数，通过 $M_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta$ 和 $K_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta$ 组装精确的节点质量矩阵和刚度矩阵。\n4. 对两种参数化，计算其质量矩阵的条件数 $\\kappa(M)$ 和算子范数 $\\|K M\\|_2$。\n5. 使用三项递推关系实现 $\\{P_n(x)\\}_{n=0}^{p}$ 的数值稳定计算。使用可靠的多项式求根器计算 $P_p'(x)$ 的根作为内部 GLL 节点。所有计算均在 $[-1,1]$ 上以单位权重进行；没有物理单位；所有量均为无量纲。\n\n测试套件：\n- 使用集合 $\\{1, 8, 24\\}$ 中的多项式次数 $p$，以覆盖基本情况、中等情况和高阶情况。\n- 对每个 $p$，计算并返回一个包含四个浮点值的列表：\n  - $\\kappa(M_{\\text{modal}})$，\n  - $\\kappa(M_{\\text{nodal}})$，\n  - $\\|K_{\\text{modal}} M_{\\text{modal}}\\|_2$，\n  - $\\|K_{\\text{nodal}} M_{\\text{nodal}}\\|_2$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身也是一个用方括号括起来的逗号分隔列表。例如，输出格式必须类似于 `[[a,b,c,d],[e,f,g,h],[i,j,k,l]]`。\n\n约束和说明：\n- 所有矩阵必须使用所述的基关系精确组装；对于节点矩阵，不允许使用质量集中或近似求积。\n- 使用双精度实数运算。如果任何对称矩阵由于舍入误差而出现非常小的负特征值，请将其视为数值伪影；您仍应使用其特征值计算条件数。\n- 不涉及角度；无需角度单位。",
            "solution": "用户的请求是验证并解决一个偏微分方程数值分析领域的问题，该问题具体涉及谱方法及其在物理信息神经网络 (PINN) 中的应用。\n\n### 第1步：提取给定信息\n\n- **问题领域**：物理信息神经网络 (PINN) 损失函数，来自谱方法/间断 Galerkin 方法的多项式参数化。\n- **数学空间**：在区间 $[-1,1]$ 上的函数，使用单位权重函数计算 $L^2$ 内积。\n- **模态基**：Legendre 多项式 $\\{P_n(x)\\}_{n=0}^{p}$。\n    - 正交性：$\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm}$。\n    - 质量矩阵 ($M_{\\text{modal}}$)：对角矩阵，元素为 $(M_{\\text{modal}})_{nn} = \\frac{2}{2n+1}$。\n    - 刚度矩阵 ($K_{\\text{modal}}$)：对角矩阵，元素为 $(K_{\\text{modal}})_{nn} = n(n+1) \\frac{2}{2n+1}$。\n- **节点基**：定义在 Gauss–Lobatto–Legendre (GLL) 节点 $\\{x_i\\}_{i=0}^{p}$ 上的 Lagrange 多项式 $\\{\\ell_i(x)\\}_{i=0}^{p}$。\n    - 节点性质：$\\ell_i(x_j) = \\delta_{ij}$。\n    - GLL 节点：端点 $[-1, 1]$ 以及 $p$ 次 Legendre 多项式的一阶导数 $P_p'(x)$ 的根。\n- **矩阵定义**：\n    - 质量矩阵：$M_{ij} = \\int_{-1}^{1} \\phi_i(x) \\phi_j(x) \\, dx$。\n    - 刚度矩阵：$K_{ij} = \\int_{-1}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$。\n- **基变换**：Lagrange 基函数在 Legendre 基中表示为 $\\ell_i(x) = \\sum_{n=0}^{p} \\beta_{ni} P_n(x)$。系数矩阵 $\\beta$ 通过求解系统 $V \\beta = I_{p+1}$ 获得，其中 $V$ 是求值矩阵，其元素为 $V_{jn} = P_n(x_j)$。\n- **节点矩阵组装**：\n    - $M_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta$。\n    - $K_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta$。\n- **待计算量**：对于每种基（模态基和节点基）：\n    1.  质量矩阵的条件数：$\\kappa(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$。\n    2.  刚度-质量乘积的算子范数：$\\|KM\\|_2$。\n- **数值程序**：\n    - 使用三项递推关系稳定地计算 Legendre 多项式。\n    - 使用可靠的多项式求根器求解 GLL 节点。\n    - 使用双精度浮点运算。\n- **测试套件**：多项式次数 $p \\in \\{1, 8, 24\\}$。\n- **输出格式**：单行文本：`[[p1_res_1, ..., p1_res_4], [p2_res_1, ..., p2_res_4], [p3_res_1, ..., p3_res_4]]`。\n\n### 第2步：使用提取的给定信息进行验证\n\n对问题陈述的有效性进行严格审查。\n\n1.  **科学依据**：所提出的概念在谱方法领域是标准的。质量和刚度矩阵、Legendre 多项式、GLL 节点和基变换的定义都是公认的。存在一个需要澄清的细节：刚度矩阵的通用定义是 $K_{ij} = \\int_{-1}^1 \\phi_i'(x) \\phi_j'(x) dx$，对于 Legendre 基 ($\\phi_n=P_n$)，这会产生一个非对角矩阵（对应于拉普拉斯算子 $-u''$）。然而，问题接着明确指示“使用精确的对角刚度矩阵 $K_{\\text{modal}}$，其元素为 $K_{nn} = n(n+1) \\frac{2}{2n+1}$”。这个矩阵并非直接由标准拉普拉斯算子的弱形式积分得到。它似乎是基于Legendre多项式作为特征函数所对应的Sturm-Liouville算子 $Lu = -((1-x^2)u')'$ 来构造的。该算子的特征值为 $\\lambda_n = n(n+1)$，而所给定的 $K_{nn}$ 正是特征值 $\\lambda_n$ 与相应质量矩阵元素 $M_{nn}$ 的乘积。这是一种在谱方法中有时会用到的构造方式。鉴于这一明确指示，通用定义可被视为背景，而 $K_{\\text{modal}}$ 的具体公式是必须遵循的直接指令。根据这种解释，该问题在科学上是一致的。\n2.  **适定性**：该问题是适定的。它要求在给定一组清晰的输入和程序的情况下，计算特定的、唯一定义的量。寻找多项式根、求解具有良条件范德蒙德类矩阵（GLL 节点的一个性质）的线性系统以及进行矩阵分析都是标准的、可解的数值任务。\n3.  **客观性**：问题以精确、客观的数学语言陈述，没有主观或带有偏见的断言。\n\n在“指定的 $K_{\\text{modal}}$ 对角公式是实现指令”这一解释下，该问题被认为是**有效的**。\n\n### 第3步：结论与行动\n\n该问题有效。解决方案将继续执行指定的计算。\n\n### 解决方案\n\n该解决方案涉及对测试套件 $\\{1, 8, 24\\}$ 中的每个多项式次数 $p$ 进行系统性计算。对于每个 $p$，我们将首先为模态 (Legendre) 基构建对角质量矩阵和刚度矩阵，并计算所需的度量。然后，我们将确定 GLL 节点，建立基变换机制，构建相应的稠密节点质量矩阵和刚度矩阵，并计算它们的度量。\n\n**1. 模态基计算**\n\n对于一个多项式次数 $p$，矩阵的大小为 $(p+1) \\times (p+1)$。令 $n = 0, 1, \\dots, p$。\n质量矩阵 $M_{\\text{modal}}$ 和刚度矩阵 $K_{\\text{modal}}$ 是对角矩阵，其元素为：\n$$\n(M_{\\text{modal}})_{nn} = \\frac{2}{2n+1}\n$$\n$$\n(K_{\\text{modal}})_{nn} = n(n+1) \\frac{2}{2n+1}\n$$\n由于这些矩阵是对角矩阵，它们的特征值就是其对角线元素。$M_{\\text{modal}}$ 的条件数是其最大与最小特征值之比：\n$$\n\\kappa(M_{\\text{modal}}) = \\frac{\\max_n (M_{\\text{modal}})_{nn}}{\\min_n (M_{\\text{modal}})_{nn}} = \\frac{(M_{\\text{modal}})_{00}}{(M_{\\text{modal}})_{pp}} = \\frac{2}{2/(2p+1)} = 2p+1\n$$\n乘积 $K_{\\text{modal}} M_{\\text{modal}}$ 也是一个对角矩阵，其元素为 $(K_{\\text{modal}})_{nn} (M_{\\text{modal}})_{nn}$。对角矩阵的 $2$-范数是其对角线元素绝对值的最大值。\n$$\n\\|K_{\\text{modal}} M_{\\text{modal}}\\|_2 = \\max_n \\left| n(n+1) \\left(\\frac{2}{2n+1}\\right)^2 \\right|\n$$\n\n**2. 节点基计算**\n\n**a. GLL 节点：** 对于次数 $p$，$p+1$ 个 GLL 节点 $\\{x_j\\}_{j=0}^p$ 包括端点 $x_0 = -1$，$x_p = 1$，以及 Legendre 多项式导数 $P_p'(x)$ 的 $p-1$ 个根。这些根使用数值多项式求根算法找到。\n\n**b. 基变换：** 我们构建大小为 $(p+1) \\times (p+1)$ 的范德蒙德类矩阵 $V$，其元素为 $V_{jn} = P_n(x_j)$。这需要稳定地计算在 GLL 节点上直到 $p$ 次的 Legendre 多项式的值，通常通过三项递推关系完成：\n$$\n(n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x), \\quad P_0(x)=1, \\quad P_1(x)=x\n$$\n从节点基系数到模态基系数的基变换矩阵 $\\beta$ 是 $V$ 的逆矩阵，即 $\\beta = V^{-1}$。在数值上，它通过求解线性系统 $V \\beta = I$ 来找到，其中 $I$ 是单位矩阵。\n\n**c. 节点矩阵：** 节点基中的质量矩阵和刚度矩阵是通过将基变换应用于其模态对应项得到的：\n$$\nM_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta\n$$\n$$\nK_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta\n$$\n这些得到的矩阵 $M_{\\text{nodal}}$ 和 $K_{\\text{nodal}}$ 是稠密的对称矩阵。\n\n**d. 节点度量：**\n$M_{\\text{nodal}}$ 的条件数 $\\kappa(M_{\\text{nodal}})$ 使用其奇异值 $\\sigma_i$ 计算为 $\\kappa_2(M_{\\text{nodal}}) = \\sigma_{\\max}/\\sigma_{\\min}$。由于 $M_{\\text{nodal}}$ 是对称正定的，这等价于其最大与最小特征值之比。\n乘积 $K_{\\text{nodal}} M_{\\text{nodal}}$通常不是对称的。其 $2$-范数 $\\|K_{\\text{nodal}} M_{\\text{nodal}}\\|_2$ 是其最大的奇异值，这可以通过标准的数值线性代数库函数计算。\n\n对每个 $p \\in \\{1, 8, 24\\}$ 系统地应用此过程，并收集四个标量度量以用于最终输出。这些度量随 $p$ 增加的行为揭示了两种基的重要属性；例如，众所周知，节点质量矩阵的条件数受一个与 $p$ 无关的小常数限制，这与朴素的等距节点基不同，而模态质量矩阵的条件数则随 $p$ 增长。",
            "answer": "```python\nimport numpy as np\nfrom numpy.polynomial.legendre import legvander, Legendre\n\ndef solve():\n    \"\"\"\n    Main function to run the complete analysis as specified in the problem.\n    It computes and compares conditioning and gradient propagation metrics\n    for modal (Legendre) and nodal (Lagrange-GLL) bases.\n    \"\"\"\n    test_cases_p = [1, 8, 24]\n\n    def get_gll_nodes(p: int) - np.ndarray:\n        \"\"\"\n        Computes the Gauss-Lobatto-Legendre (GLL) nodes for a given polynomial degree p.\n        The GLL nodes include the endpoints -1 and 1, and the roots of P_p'(x).\n        \"\"\"\n        if p == 0:\n            return np.array([0.0])\n        \n        if p == 1:\n            return np.array([-1.0, 1.0])\n\n        # For p  1, interior nodes are the roots of the derivative of the Legendre polynomial P_p(x).\n        c = np.zeros(p + 1)\n        c[p] = 1.0\n        P_p = Legendre(c)\n        \n        P_p_prime = P_p.deriv()\n        \n        interior_nodes = P_p_prime.roots()\n        \n        # The roots of P_p' are guaranteed to be real and in (-1, 1).\n        interior_nodes = np.real(interior_nodes)\n        \n        nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n        return nodes\n\n    def compute_metrics(p: int) - list[float]:\n        \"\"\"\n        Computes the four required metrics for a given polynomial degree p.\n        \"\"\"\n        N = p + 1\n        n_vals = np.arange(N, dtype=float)\n\n        # MODAL BASIS (Legendre polynomials)\n        m_diag = 2.0 / (2.0 * n_vals + 1.0)\n        M_modal = np.diag(m_diag)\n        \n        k_diag = n_vals * (n_vals + 1.0) * m_diag\n        K_modal = np.diag(k_diag)\n\n        cond_M_modal = np.max(m_diag) / np.min(m_diag)\n        \n        km_modal_diag = k_diag * m_diag\n        norm_KM_modal = np.max(np.abs(km_modal_diag))\n\n        # NODAL BASIS (Lagrange polynomials at GLL nodes)\n        gll_nodes = get_gll_nodes(p)\n\n        V = legvander(gll_nodes, p)\n\n        I = np.identity(N, dtype=float)\n        beta = np.linalg.solve(V, I)\n\n        M_nodal = beta.T @ M_modal @ beta\n        K_nodal = beta.T @ K_modal @ beta\n        \n        cond_M_nodal = np.linalg.cond(M_nodal, 2)\n        \n        km_nodal_prod = K_nodal @ M_nodal\n        norm_KM_nodal = np.linalg.norm(km_nodal_prod, 2)\n        \n        return [cond_M_modal, cond_M_nodal, norm_KM_modal, norm_KM_nodal]\n\n    results = []\n    for p in test_cases_p:\n        metrics = compute_metrics(p)\n        results.append(metrics)\n\n    print(f\"[{','.join(['[' + ','.join(map(str, res)) + ']' for res in results])}]\")\n\nsolve()\n```"
        }
    ]
}