{
    "hands_on_practices": [
        {
            "introduction": "The foundation of the intrusive stochastic Galerkin method lies in approximating the solution of a stochastic problem using a basis of functions that are tailored to the probability distribution of the random inputs. A crucial first step in this process is to ensure these basis functions are orthonormal with respect to the probability measure. This exercise  guides you through the fundamental calculation of normalizing a standard polynomial basis, demonstrating how orthonormality simplifies the inner products that form the core of the Galerkin projection.",
            "id": "3392649",
            "problem": "Consider intrusive stochastic Galerkin methods in which a scalar random input is modeled by a uniform random variable $\\xi \\sim \\mathcal{U}(-1,1)$ with probability density function (PDF) $\\rho(\\xi)=\\frac{1}{2}$ for $\\xi \\in [-1,1]$ and $\\rho(\\xi)=0$ otherwise. In the Galerkin projection, the inner product is the expectation $\\mathbb{E}[\\cdot]$ with respect to $\\rho$, so for any square-integrable functions $f$ and $g$ the inner product is defined by $\\langle f, g \\rangle_{\\rho} := \\mathbb{E}[f(\\xi) g(\\xi)] = \\int_{-1}^{1} f(\\xi) g(\\xi) \\rho(\\xi) \\, \\mathrm{d}\\xi$. The polynomial basis is chosen from the classical Legendre polynomials $\\{P_n(\\xi)\\}_{n=0}^{\\infty}$ owing to their orthogonality under the uniform weight on $[-1,1]$. In many intrusive stochastic Galerkin formulations, such as those based on Generalized Polynomial Chaos (gPC), the basis is required to be orthonormal under $\\langle \\cdot, \\cdot \\rangle_{\\rho}$ to obtain simplified algebraic structures.\n\nStarting from the fundamental definitions above, determine the normalization constants $c_n$ such that the scaled basis functions $\\Psi_n(\\xi) := c_n P_n(\\xi)$ form an orthonormal system in $L^{2}_{\\rho}([-1,1])$, that is, $\\mathbb{E}[\\Psi_m(\\xi)\\Psi_n(\\xi)] = \\delta_{mn}$ where $\\delta_{mn}$ denotes the Kronecker delta. Then, using the same definitions, verify the expected-value identity $\\mathbb{E}[P_m(\\xi) P_n(\\xi)]$ in closed form. Your final answer must be the closed-form expression for $c_n$ in terms of $n$; no numerical approximation is required, and no units are involved.",
            "solution": "The problem will first be validated to ensure its scientific and logical integrity.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- A scalar random input is modeled by a uniform random variable $\\xi \\sim \\mathcal{U}(-1,1)$.\n- The probability density function (PDF) is $\\rho(\\xi)=\\frac{1}{2}$ for $\\xi \\in [-1,1]$ and $\\rho(\\xi)=0$ otherwise.\n- The inner product for square-integrable functions $f$ and $g$ is defined as the expectation $\\langle f, g \\rangle_{\\rho} := \\mathbb{E}[f(\\xi) g(\\xi)] = \\int_{-1}^{1} f(\\xi) g(\\xi) \\rho(\\xi) \\, \\mathrm{d}\\xi$.\n- The polynomial basis is the set of classical Legendre polynomials $\\{P_n(\\xi)\\}_{n=0}^{\\infty}$.\n- The scaled basis functions are $\\Psi_n(\\xi) := c_n P_n(\\xi)$, where $c_n$ are the normalization constants.\n- The orthonormality condition is $\\mathbb{E}[\\Psi_m(\\xi)\\Psi_n(\\xi)] = \\delta_{mn}$, where $\\delta_{mn}$ is the Kronecker delta.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded**: The problem is firmly rooted in the mathematical theory of uncertainty quantification, specifically using Generalized Polynomial Chaos (gPC) expansions with intrusive Galerkin methods. The use of Legendre polynomials as an orthogonal basis for a uniform random variable is a standard and correct application of the Askey scheme. All definitions and properties are consistent with established literature in this field.\n- **Well-Posed**: The problem is well-posed. It asks for the determination of normalization constants $c_n$ based on a clear and complete set of definitions, leading to a unique solution (conventionally, $c_n > 0$).\n- **Objective**: The problem is stated using precise and objective mathematical language, free from any subjectivity or ambiguity.\n- **Completeness**: The problem provides all necessary information: the probability distribution, the definition of the inner product, the basis polynomials, and the orthonormality condition. It is self-contained.\n- **Consistency**: There are no internal contradictions. The definitions are standard and mutually compatible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard and well-defined mathematical exercise in the context of spectral methods for uncertainty quantification. A full solution will be provided.\n\n**Solution Derivation**\n\nThe objective is to find the normalization constants $c_n$ such that the basis functions $\\Psi_n(\\xi) = c_n P_n(\\xi)$ are orthonormal with respect to the inner product defined by the expectation operator $\\mathbb{E}[\\cdot]$. The orthonormality condition is given as:\n$$\n\\mathbb{E}[\\Psi_m(\\xi)\\Psi_n(\\xi)] = \\delta_{mn}\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta, which is $1$ if $m=n$ and $0$ if $m \\neq n$.\n\nSubstituting the definition of $\\Psi_n(\\xi)$ into the condition, we get:\n$$\n\\mathbb{E}[ (c_m P_m(\\xi)) (c_n P_n(\\xi)) ] = \\delta_{mn}\n$$\nSince $c_m$ and $c_n$ are deterministic constants, they can be factored out of the expectation:\n$$\nc_m c_n \\mathbb{E}[P_m(\\xi) P_n(\\xi)] = \\delta_{mn}\n$$\nNext, we evaluate the expectation term $\\mathbb{E}[P_m(\\xi) P_n(\\xi)]$ using its definition in terms of the probability density function $\\rho(\\xi)$. The random variable $\\xi$ is uniformly distributed on $[-1, 1]$, so its PDF is $\\rho(\\xi) = \\frac{1}{2}$ for $\\xi \\in [-1, 1]$.\n$$\n\\mathbb{E}[P_m(\\xi) P_n(\\xi)] = \\int_{-\\infty}^{\\infty} P_m(\\xi) P_n(\\xi) \\rho(\\xi) \\, \\mathrm{d}\\xi = \\int_{-1}^{1} P_m(\\xi) P_n(\\xi) \\left(\\frac{1}{2}\\right) \\, \\mathrm{d}\\xi\n$$\nFactoring out the constant $\\frac{1}{2}$:\n$$\n\\mathbb{E}[P_m(\\xi) P_n(\\xi)] = \\frac{1}{2} \\int_{-1}^{1} P_m(\\xi) P_n(\\xi) \\, \\mathrm{d}\\xi\n$$\nThis expression constitutes the verification of the expected-value identity requested in the problem. To evaluate it in closed form, we use the standard orthogonality property of the Legendre polynomials, which states:\n$$\n\\int_{-1}^{1} P_m(x) P_n(x) \\, \\mathrm{d}x = \\frac{2}{2n+1} \\delta_{mn}\n$$\nSubstituting this known result into our expression for the expectation gives the closed-form identity:\n$$\n\\mathbb{E}[P_m(\\xi) P_n(\\xi)] = \\frac{1}{2} \\left( \\frac{2}{2n+1} \\delta_{mn} \\right) = \\frac{1}{2n+1} \\delta_{mn}\n$$\nNow, we substitute this result back into the orthonormality condition for the scaled basis $\\Psi_n(\\xi)$:\n$$\nc_m c_n \\left( \\frac{1}{2n+1} \\delta_{mn} \\right) = \\delta_{mn}\n$$\nTo find the constants $c_n$, we consider the case where $m = n$. In this case, $\\delta_{nn} = 1$:\n$$\nc_n c_n \\left( \\frac{1}{2n+1} \\delta_{nn} \\right) = \\delta_{nn} \\implies c_n^2 \\left( \\frac{1}{2n+1} \\right) = 1\n$$\nSolving for $c_n^2$:\n$$\nc_n^2 = 2n+1\n$$\nBy convention, the normalization constant is taken to be the positive square root. Therefore:\n$$\nc_n = \\sqrt{2n+1}\n$$\nFor the case where $m \\neq n$, the orthogonality property ensures $\\delta_{mn} = 0$, leading to the identity $0 = 0$, which is consistent.\n\nThus, the orthonormal basis functions are $\\Psi_n(\\xi) = \\sqrt{2n+1} P_n(\\xi)$. The normalization constant $c_n$ depends only on the degree $n$ of the polynomial.",
            "answer": "$$\n\\boxed{\\sqrt{2n+1}}\n$$"
        },
        {
            "introduction": "Once a suitable orthonormal basis is established, the next step is to apply the Galerkin projection to the governing equation, which transforms the stochastic partial differential equation into a large, coupled system of deterministic equations. A key insight is that this resulting system is often not fully dense. This practice  invites you to discover the specific block-sparse structure of the Galerkin matrix for a common and important class of affine-parametric problems, revealing how the three-term recurrence relation of orthogonal polynomials dictates the coupling between stochastic modes.",
            "id": "3392692",
            "problem": "Consider an intrusive stochastic Galerkin (SG) discretization of a spatially discretized scalar elliptic model problem using a Discontinuous Galerkin (DG) method in space. The uncertain coefficient is modeled by two independent random variables $\\xi_{1}$ and $\\xi_{2}$, each uniformly distributed on $[-1,1]$, and is given by the affine parametrization $a(x,\\xi) = a_{0}(x) + a_{1}(x)\\,\\xi_{1} + a_{2}(x)\\,\\xi_{2}$, where $a_{0}(x)$, $a_{1}(x)$, and $a_{2}(x)$ are deterministic coefficient fields and $x$ denotes the spatial coordinate. The SG method uses a multivariate Legendre polynomial chaos basis that is orthonormal with respect to the product measure on $[-1,1]^{2}$, truncated to total polynomial degree $p=2$ in stochastic dimension $d=2$. Let the multi-index set be\n$$\n\\mathcal{I}_{2} \\equiv \\{ \\alpha \\in \\mathbb{N}_{0}^{2} : \\alpha_{1} + \\alpha_{2} \\leq 2 \\},\n$$\nand define multivariate basis functions $\\psi_{\\alpha}(\\xi) = L_{\\alpha_{1}}(\\xi_{1})\\,L_{\\alpha_{2}}(\\xi_{2})$, where $L_{n}$ denotes the orthonormal Legendre polynomial of degree $n$ on $[-1,1]$.\n\nThe global SG system matrix is block-partitioned according to the stochastic modes indexed by $\\alpha,\\beta \\in \\mathcal{I}_{2}$, yielding an $\\lvert \\mathcal{I}_{2} \\rvert \\times \\lvert \\mathcal{I}_{2} \\rvert$ block matrix. Each block $(\\alpha,\\beta)$ is a spatial DG matrix multiplied by a stochastic moment\n$$\n\\int_{[-1,1]^{2}} \\psi_{\\alpha}(\\xi)\\,\\psi_{\\beta}(\\xi)\\,q(\\xi)\\,\\mathrm{d}\\mu(\\xi),\n$$\nfor $q(\\xi) \\in \\{1,\\xi_{1},\\xi_{2}\\}$ arising from $a_{0}(x)$, $a_{1}(x)$, and $a_{2}(x)$, respectively, and $\\mathrm{d}\\mu(\\xi)$ is the uniform product measure. Using only foundational properties of orthonormal Legendre polynomials and their three-term recurrence, construct the sparsity pattern of the global SG block matrix for $d=2$ and $p=2$ with the given $a(x,\\xi)$, by determining exactly which $(\\alpha,\\beta)$ pairs yield nonzero blocks. Then, compute the total number of nonzero blocks in the $\\lvert \\mathcal{I}_{2} \\rvert \\times \\lvert \\mathcal{I}_{2} \\rvert$ block matrix.\n\nProvide your final answer as a single integer. No rounding is needed.",
            "solution": "We start from the foundational definitions of the intrusive stochastic Galerkin (SG) method with an orthonormal polynomial chaos basis. The multivariate basis functions are built as tensor products of one-dimensional orthonormal Legendre polynomials, and orthonormality implies\n$$\n\\int_{[-1,1]^{2}} \\psi_{\\alpha}(\\xi)\\,\\psi_{\\beta}(\\xi)\\,\\mathrm{d}\\mu(\\xi) = \\delta_{\\alpha\\beta},\n$$\nwhere $\\delta_{\\alpha\\beta}$ is the Kronecker delta, and $\\mathrm{d}\\mu(\\xi)$ denotes the uniform product measure.\n\nThe coefficient $a(x,\\xi) = a_{0}(x) + a_{1}(x)\\,\\xi_{1} + a_{2}(x)\\,\\xi_{2}$ induces three contributions to the global SG matrix. For the constant term $q(\\xi)=1$, orthonormality yields only diagonal stochastic coupling:\n$$\n\\int_{[-1,1]^{2}} \\psi_{\\alpha}(\\xi)\\,\\psi_{\\beta}(\\xi)\\,\\mathrm{d}\\mu(\\xi) = \\delta_{\\alpha\\beta}.\n$$\nThus, the $a_{0}(x)$ contribution produces nonzero blocks only on the diagonal $(\\alpha,\\alpha)$.\n\nFor the linear terms $q(\\xi)=\\xi_{1}$ and $q(\\xi)=\\xi_{2}$, nonzero couplings are determined by the three-term recurrence relation of Legendre polynomials. For orthonormal Legendre polynomials $\\{L_{n}\\}_{n\\geq 0}$ on $[-1,1]$, the standard recurrence implies\n$$\n\\xi\\,L_{n}(\\xi) = c_{n+1}\\,L_{n+1}(\\xi) + c_{n-1}\\,L_{n-1}(\\xi),\n$$\nwith nonzero coefficients $c_{n\\pm 1}$ for all $n\\geq 0$ where indices are valid. Consequently, the one-dimensional triple product\n$$\n\\int_{-1}^{1} L_{m}(\\xi)\\,\\xi\\,L_{n}(\\xi)\\,\\mathrm{d}\\xi\n$$\nis nonzero if and only if $m = n \\pm 1$, and is zero otherwise (including when $m=n$, by parity and orthogonality). In multiple dimensions, separability gives\n$$\n\\int_{[-1,1]^{2}} \\psi_{\\alpha}(\\xi)\\,\\psi_{\\beta}(\\xi)\\,\\xi_{i}\\,\\mathrm{d}\\mu(\\xi)\n= \\left( \\int_{-1}^{1} L_{\\alpha_{i}}(\\xi_{i})\\,\\xi_{i}\\,L_{\\beta_{i}}(\\xi_{i})\\,\\mathrm{d}\\xi_{i} \\right)\n\\prod_{j\\neq i} \\left( \\int_{-1}^{1} L_{\\alpha_{j}}(\\xi_{j})\\,L_{\\beta_{j}}(\\xi_{j})\\,\\mathrm{d}\\xi_{j} \\right).\n$$\nTherefore, this multivariate integral is nonzero if and only if:\n- $\\beta_{j} = \\alpha_{j}$ for all $j \\neq i$ (by orthonormality in the unaffected dimensions), and\n- $\\beta_{i} = \\alpha_{i} \\pm 1$ (by the one-dimensional recurrence selection rule).\n\nHence, the stochastic coupling induced by $q(\\xi)=\\xi_{i}$ connects multi-indices that differ by exactly one in component $i$ and are equal in the other component.\n\nWe now enumerate the truncated multi-index set for $d=2$ and total degree $p=2$:\n$$\n\\mathcal{I}_{2} = \\{ (0,0),\\, (1,0),\\, (0,1),\\, (2,0),\\, (1,1),\\, (0,2) \\}.\n$$\nThere are $\\lvert \\mathcal{I}_{2} \\rvert = \\binom{d+p}{p} = \\binom{4}{2} = 6$ stochastic modes.\n\nWe construct the sparsity pattern by listing all pairs $(\\alpha,\\beta)$ for which the block is nonzero.\n\n1. Constant term $q(\\xi)=1$ ($a_{0}(x)$ contribution):\n- Nonzero only for $\\beta=\\alpha$. This yields $6$ diagonal nonzero blocks: $(0,0)$, $(1,0)$, $(0,1)$, $(2,0)$, $(1,1)$, $(0,2)$ along the diagonal.\n\n2. Linear term $q(\\xi)=\\xi_{1}$ ($a_{1}(x)$ contribution):\n- Nonzero when $\\beta = \\alpha \\pm (1,0)$ and the pair remains inside $\\mathcal{I}_{2}$.\n- Valid unordered neighbor pairs in $\\mathcal{I}_{2}$ under $\\pm(1,0)$ are:\n$$\n(0,0)\\text{–}(1,0),\\quad (1,0)\\text{–}(2,0),\\quad (0,1)\\text{–}(1,1).\n$$\nThese are $3$ undirected edges, which correspond to $6$ off-diagonal block positions in the full block matrix because each undirected edge yields two oriented pairs $(\\alpha,\\beta)$ and $(\\beta,\\alpha)$.\n\n3. Linear term $q(\\xi)=\\xi_{2}$ ($a_{2}(x)$ contribution):\n- Nonzero when $\\beta = \\alpha \\pm (0,1)$ and the pair remains inside $\\mathcal{I}_{2}$.\n- Valid unordered neighbor pairs in $\\mathcal{I}_{2}$ under $\\pm(0,1)$ are:\n$$\n(0,0)\\text{–}(0,1),\\quad (0,1)\\text{–}(0,2),\\quad (1,0)\\text{–}(1,1).\n$$\nThese are $3$ undirected edges, again producing $6$ off-diagonal block positions.\n\nNo pair appears in both the $\\xi_{1}$ and $\\xi_{2}$ lists, since the selection rules require a change in exactly one coordinate by $\\pm 1$, and the two coordinates are distinct.\n\nFinally, the total number of nonzero blocks in the $\\lvert \\mathcal{I}_{2} \\rvert \\times \\lvert \\mathcal{I}_{2} \\rvert$ block matrix is the sum of the diagonal blocks from $a_{0}(x)$ and the off-diagonal blocks from the linear terms:\n$$\n6 \\text{ (diagonal)} + 6 \\text{ (off-diagonal from } \\xi_{1}\\text{)} + 6 \\text{ (off-diagonal from } \\xi_{2}\\text{)} = 18.\n$$\nThus, the global SG block matrix has $18$ nonzero blocks.",
            "answer": "$$\\boxed{18}$$"
        },
        {
            "introduction": "Understanding the sparse structure of the Galerkin matrix is essential, but leveraging this structure for efficient computation is the ultimate goal. For problems with separable operators and tensor-product bases, the system matrix exhibits a highly structured form known as a Kronecker sum. This final practice  challenges you to bridge the gap between mathematical theory and practical implementation by developing a \"matrix-free\" algorithm that exploits this Kronecker structure, dramatically reducing memory requirements and computational cost compared to assembling the full, explicit matrix.",
            "id": "3392640",
            "problem": "Consider an intrusive stochastic Galerkin (SG) discretization of a linear elliptic model problem with an affine random coefficient, discretized in a finite-dimensional physical trial space $V_h$ of dimension $n_h$ using a spectral or discontinuous Galerkin spatial discretization, and in a polynomial chaos expansion (PCE) basis of dimension $n_p$ for the stochastic space. Under the affine-in-parameters assumption, the global SG matrix $A_{\\mathrm{SG}}$ has the Kronecker-sum structure\n$$\nA_{\\mathrm{SG}} \\;=\\; \\sum_{r=0}^{m-1} G_r \\,\\otimes\\, K_r,\n$$\nwhere $K_r \\in \\mathbb{R}^{n_h \\times n_h}$ are deterministic spatial matrices derived from $V_h$ (for example, stiffness-like matrices from spectral or discontinuous Galerkin formulations) and $G_r \\in \\mathbb{R}^{n_p \\times n_p}$ are stochastic coefficient matrices formed from PCE inner products. The symbol $\\otimes$ denotes the Kronecker product. The unknown SG coefficient vector is obtained by stacking the physical degrees of freedom for each stochastic basis function into a single vector of length $n_h n_p$.\n\nStarting from the fundamental properties of Kronecker products and the vectorization operator (specifically, the identity\n$$\n\\mathrm{vec}(A X B^\\top) \\;=\\; (B \\otimes A)\\,\\mathrm{vec}(X),\n$$\nfor conforming matrices $A$, $X$, and $B$), derive and implement an algorithm that applies $A_{\\mathrm{SG}}$ to a vector without explicitly forming $A_{\\mathrm{SG}}$. Your implementation must exploit the Kronecker structure to avoid forming dense blocks and must be correct for arbitrary input parameters $n_h$, $n_p$, and $m$.\n\nFor the purposes of testing and reproducibility, in your program construct $K_r$ and $G_r$ deterministically as follows:\n- For each test case, let $n_h \\geq 1$, $n_p \\geq 1$, and $m \\geq 1$ be given integers.\n- Define $K_0 \\in \\mathbb{R}^{n_h \\times n_h}$ as the tridiagonal matrix with diagonal entries $2$ and sub- and super-diagonal entries $-1$. For $n_h=1$, this reduces to the scalar $2$.\n- For each $r \\in \\{1,\\dots,m-1\\}$, define $K_r \\in \\mathbb{R}^{n_h \\times n_h}$ as tridiagonal with diagonal entries $2 + \\frac{r}{\\max(1,n_h)}$ and sub- and super-diagonal entries $-\\frac{1}{r+1}$ (entries outside the tridiagonal are $0$).\n- Define $G_0 \\in \\mathbb{R}^{n_p \\times n_p}$ as the identity matrix of size $n_p$.\n- For each $r \\in \\{1,\\dots,m-1\\}$, define $G_r \\in \\mathbb{R}^{n_p \\times n_p}$ as $\\frac{1}{r+1}$ times the symmetric off-diagonal adjacency matrix with ones on the first sub- and super-diagonals and zeros elsewhere. Concretely, $(G_r)_{i,j} = \\frac{1}{r+1}$ if $|i-j|=1$ and $0$ otherwise. For $n_p=1$, $G_r$ is the $1\\times 1$ zero matrix for $r \\ge 1$.\n\nTo verify correctness, let $x \\in \\mathbb{R}^{n_h n_p}$ be the vectorization (in column-major order) of a matrix $U \\in \\mathbb{R}^{n_h \\times n_p}$ defined by $U_{i,j} = i + \\frac{j}{10}$ using one-based indices so that entries are strictly positive and reproducible. Compare the result of your matrix-free Kronecker apply algorithm with the explicit product $A_{\\mathrm{SG}} x$ where $A_{\\mathrm{SG}}$ is formed via summation of Kronecker products; report the Euclidean norm of the difference.\n\nEstimate the memory complexity of two approaches in terms of the count of scalars that must be stored:\n- Naive explicit assembly storing $A_{\\mathrm{SG}}$ densely: requires $(n_h n_p)^2$ scalars.\n- Kronecker-factor storage storing all $K_r$ and $G_r$ but never forming $A_{\\mathrm{SG}}$: requires $m(n_h^2 + n_p^2)$ scalars.\n\nYour program must:\n- Implement the matrix-free application of $A_{\\mathrm{SG}}$ using the Kronecker identity, taking care to consistently use column-major vectorization when reshaping between vectors and matrices.\n- Construct the explicit $A_{\\mathrm{SG}}$ via Kronecker products and verify the matrix-free implementation by computing the Euclidean norm of the difference between the two applied results.\n- Compute and report the two memory scalar counts described above.\n- Use the following test suite of parameter tuples $(n_h,n_p,m)$:\n    1. $(4,3,3)$ as a general case,\n    2. $(5,2,2)$ as a medium case,\n    3. $(3,1,2)$ as a boundary case in the stochastic dimension,\n    4. $(1,4,2)$ as a boundary case in the physical dimension.\n- For each test case, produce a three-element list $[e, s_{\\mathrm{naive}}, s_{\\mathrm{kron}}]$ where $e$ is the Euclidean norm (a float) of the difference between the explicit and matrix-free products, $s_{\\mathrm{naive}}$ is the integer scalar count for naive explicit storage, and $s_{\\mathrm{kron}}$ is the integer scalar count for Kronecker-factor storage.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is itself a three-element list as specified (for example, $[[e_1,s_{1,\\mathrm{naive}},s_{1,\\mathrm{kron}}],[e_2,s_{2,\\mathrm{naive}},s_{2,\\mathrm{kron}}],\\dots]$). No physical units or angle units are involved in this problem.\n\nProvide clear and complete code that adheres to the execution environment specified in the final answer section.",
            "solution": "The problem requires the derivation and implementation of a matrix-free algorithm to compute the matrix-vector product $y = A_{\\mathrm{SG}} x$, where the global stochastic Galerkin (SG) matrix $A_{\\mathrm{SG}}$ possesses a Kronecker-sum structure. The derivation relies on the properties of the Kronecker product and the vectorization operator.\n\nThe global SG matrix is given as a sum over $m$ terms:\n$$\nA_{\\mathrm{SG}} \\;=\\; \\sum_{r=0}^{m-1} G_r \\,\\otimes\\, K_r\n$$\nHere, $K_r \\in \\mathbb{R}^{n_h \\times n_h}$ are deterministic spatial matrices, $G_r \\in \\mathbb{R}^{n_p \\times n_p}$ are stochastic coefficient matrices, and $\\otimes$ denotes the Kronecker product. The resulting matrix $A_{\\mathrm{SG}}$ has dimensions $(n_h n_p) \\times (n_h n_p)$.\n\nThe matrix-vector product to be computed is $y = A_{\\mathrm{SG}} x$. Using the linearity of matrix-vector multiplication, we can distribute the product over the sum:\n$$\ny \\;=\\; \\left( \\sum_{r=0}^{m-1} G_r \\,\\otimes\\, K_r \\right) x \\;=\\; \\sum_{r=0}^{m-1} (G_r \\,\\otimes\\, K_r) x\n$$\nThis expression indicates that the final result $y$ is the sum of the results from applying each term $(G_r \\otimes K_r)$ to the vector $x$.\n\nThe core of the matrix-free approach is to avoid forming the large $(n_h n_p) \\times (n_h n_p)$ matrices $G_r \\otimes K_r$. To do this, we leverage the structure of the vector $x$. As stated, the SG solution vector $x$ is formed by stacking the physical degrees of freedom for each stochastic basis function. This is equivalent to the column-major vectorization of a matrix $U \\in \\mathbb{R}^{n_h \\times n_p}$, where the columns of $U$ correspond to the stochastic basis functions and the rows correspond to the physical degrees of freedom. We can write this relationship as $x = \\mathrm{vec}(U)$.\n\nThe problem provides the following identity for conforming matrices $A$, $X$, and $B$:\n$$\n\\mathrm{vec}(A X B^\\top) \\;=\\; (B \\otimes A)\\,\\mathrm{vec}(X)\n$$\nWe can apply this identity to each term in our sum, $(G_r \\otimes K_r) x = (G_r \\otimes K_r) \\mathrm{vec}(U)$. Let's match the pattern $(B \\otimes A)\\,\\mathrm{vec}(X)$:\n- Let $B = G_r$.\n- Let $A = K_r$.\n- Let $X = U$.\n\nSubstituting these into the identity gives:\n$$\n(G_r \\otimes K_r) \\mathrm{vec}(U) \\;=\\; \\mathrm{vec}(K_r U G_r^\\top)\n$$\nThis is the key insight: the action of the large Kronecker product matrix $(G_r \\otimes K_r)$ on the vector $\\mathrm{vec}(U)$ is equivalent to performing smaller matrix-matrix multiplications and then vectorizing the result.\n\nNow, we substitute this back into the expression for $y$:\n$$\ny \\;=\\; \\sum_{r=0}^{m-1} \\mathrm{vec}(K_r U G_r^\\top)\n$$\nSince the vectorization operator $\\mathrm{vec}(\\cdot)$ is linear, we can interchange it with the summation:\n$$\ny \\;=\\; \\mathrm{vec}\\left(\\sum_{r=0}^{m-1} K_r U G_r^\\top\\right)\n$$\nThis final equation provides a direct recipe for the matrix-free algorithm:\n\n1.  **Un-vectorize**: Given the input vector $x \\in \\mathbb{R}^{n_h n_p}$, reshape it into a matrix $U \\in \\mathbb{R}^{n_h \\times n_p}$ using a column-major ordering.\n2.  **Accumulate**: Initialize an accumulator matrix $Y_{\\mathrm{mat}} \\in \\mathbb{R}^{n_h \\times n_p}$ to all zeros. Then, for each $r$ from $0$ to $m-1$, compute the matrix product $K_r U G_r^\\top$ and add it to $Y_{\\mathrm{mat}}$.\n3.  **Vectorize**: Once the loop is complete, vectorize the final matrix $Y_{\\mathrm{mat}}$ using a column-major ordering to obtain the result vector $y \\in \\mathbb{R}^{n_h n_p}$.\n\nThis algorithm avoids the explicit construction and storage of the $(n_h n_p) \\times (n_h n_p)$ matrix $A_{\\mathrm{SG}}$. Instead, it only requires storing the smaller factor matrices $K_r$ and $G_r$, leading to significant memory savings, as quantified by the problem's memory complexity formulas. The computational cost is dominated by the $m$ iterations of matrix multiplications, which is typically much more efficient than a single large matrix-vector product with $A_{\\mathrm{SG}}$.\n\nThe implementation will follow this derived algorithm. For verification, the result will be compared against the direct computation using explicitly formed Kronecker products. The problem provides deterministic rules for constructing the matrices $K_r$ and $G_r$ and the input vector $x$, allowing for a reproducible test. The memory costs are calculated using the provided formulas: $s_{\\mathrm{naive}} = (n_h n_p)^2$ and $s_{\\mathrm{kron}} = m (n_h^2 + n_p^2)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_K_matrices(nh, m):\n    \"\"\"Constructs the list of spatial matrices K_r.\"\"\"\n    Ks = []\n    # K_0\n    if nh == 1:\n        K0 = np.array([[2.0]])\n    else:\n        K0 = np.zeros((nh, nh))\n        np.fill_diagonal(K0, 2.0)\n        np.fill_diagonal(K0[1:], -1.0)\n        np.fill_diagonal(K0[:, 1:], -1.0)\n    Ks.append(K0)\n\n    # K_r for r > 0\n    for r in range(1, m):\n        Kr = np.zeros((nh, nh))\n        diag_val = 2.0 + r / float(max(1, nh))\n        if nh == 1:\n            Kr[0, 0] = diag_val\n        else:\n            off_diag_val = -1.0 / (r + 1.0)\n            np.fill_diagonal(Kr, diag_val)\n            np.fill_diagonal(Kr[1:], off_diag_val)\n            np.fill_diagonal(Kr[:, 1:], off_diag_val)\n        Ks.append(Kr)\n    return Ks\n\ndef build_G_matrices(np_, m):\n    \"\"\"Constructs the list of stochastic matrices G_r.\"\"\"\n    Gs = []\n    # G_0\n    G0 = np.identity(np_)\n    Gs.append(G0)\n\n    # G_r for r > 0\n    for r in range(1, m):\n        Gr = np.zeros((np_, np_))\n        if np_ > 1:\n            factor = 1.0 / (r + 1.0)\n            np.fill_diagonal(Gr[1:], factor)\n            np.fill_diagonal(Gr[:, 1:], factor)\n        Gs.append(Gr)\n    return Gs\n\ndef solve_case(nh, np_, m):\n    \"\"\"\n    Solves the problem for a single test case (nh, np, m).\n    \n    Returns a list containing [error, s_naive, s_kron].\n    \"\"\"\n    # 1. Construct matrices K_r and G_r\n    Ks = build_K_matrices(nh, m)\n    Gs = build_G_matrices(np_, m)\n\n    # 2. Construct input matrix U and vector x\n    # U_{i,j} = i + j/10 (1-based indices)\n    # Using 0-based numpy arrays: U[i,j] = (i+1) + (j+1)/10\n    i_indices = np.arange(1, nh + 1).reshape(nh, 1)\n    j_indices = np.arange(1, np_ + 1).reshape(1, np_)\n    U = i_indices + j_indices / 10.0\n    \n    # Vectorize U in column-major order ('F' for Fortran-style)\n    x = U.flatten(order='F')\n\n    # 3. Matrix-free application: y_matfree = vec(sum(K_r @ U @ G_r.T))\n    Y_mat = np.zeros((nh, np_))\n    for r in range(m):\n        Kr = Ks[r]\n        Gr_T = Gs[r].T\n        Y_mat += Kr @ U @ Gr_T\n    y_matfree = Y_mat.flatten(order='F')\n\n    # 4. Explicit application for verification: y_explicit = (sum(G_r kron K_r)) @ x\n    A_sg = np.zeros((nh * np_, nh * np_))\n    for r in range(m):\n        A_sg += np.kron(Gs[r], Ks[r])\n    y_explicit = A_sg @ x\n\n    # 5. Compute Euclidean norm of the difference\n    error = np.linalg.norm(y_explicit - y_matfree)\n\n    # 6. Compute memory scalar counts\n    s_naive = (nh * np_)**2\n    s_kron = m * (nh**2 + np_**2)\n\n    return [error, s_naive, s_kron]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 3, 3),  # General case\n        (5, 2, 2),  # Medium case\n        (3, 1, 2),  # Boundary case in stochastic dimension\n        (1, 4, 2),  # Boundary case in physical dimension\n    ]\n\n    results = []\n    for case in test_cases:\n        nh, np_, m = case\n        result = solve_case(nh, np_, m)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is desired.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}