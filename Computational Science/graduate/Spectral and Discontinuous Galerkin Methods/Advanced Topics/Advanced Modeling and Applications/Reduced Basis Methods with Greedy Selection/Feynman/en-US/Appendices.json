{
    "hands_on_practices": [
        {
            "introduction": "The remarkable efficiency of Reduced Basis Methods stems from a critical computational strategy known as the offline/online decomposition. For problems where the governing operators exhibit an affine dependence on the parameters, we can perform all computationally intensive tasks involving the high-dimensional discretization in a one-time \"offline\" stage. This practice guides you through the derivation of this decomposition for a Discontinuous Galerkin (DG) discretization , revealing how the small reduced system matrix can be assembled almost instantaneously in the \"online\" stage for any new parameter value.",
            "id": "3411778",
            "problem": "Consider the linear, coercive, parametric diffusion model posed on a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$ with $d \\in \\{1,2,3\\}$,\n$$\n- \\nabla \\cdot \\big(k(x;\\mu) \\nabla u(x;\\mu)\\big) = f(x) \\quad \\text{in } \\Omega,\n$$\nwith homogeneous Dirichlet boundary conditions on $\\partial \\Omega$. Assume a shape-regular mesh $\\mathcal{T}_{h}$ of $\\Omega$ and a symmetric interior penalty discontinuous Galerkin (DG) discretization using polynomials of degree $p$ elementwise, with the standard definitions of jumps and averages on interior faces. The coefficient $k(x;\\mu)$ admits an affine parameter decomposition\n$$\nk(x;\\mu) = \\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu)\\, k_{q}(x),\n$$\nwhere the scalar coefficient functions $\\theta_{q}^{a}(\\mu)$ depend only on the parameter $\\mu$, and the spatial coefficient snapshots $k_{q}(x)$ are parameter-independent.\n\nLet $V_{h}$ denote the DG finite element space, and let $a(\\cdot,\\cdot;\\mu)$ be the bilinear form associated with the symmetric interior penalty DG method (with penalty parameter $\\sigma>0$ chosen sufficiently large for coercivity), constructed from the differential operator $-\\nabla \\cdot (k(x;\\mu) \\nabla \\cdot)$. Assume that a reduced basis $V_{N} = \\operatorname{span}\\{\\zeta_{1},\\dots,\\zeta_{N}\\} \\subset V_{h}$ has been produced offline by a greedy algorithm over a training set in parameter space, with the basis vectors $\\zeta_{i} \\in V_{h}$ represented in the DG space.\n\nStarting only from the defining properties of the symmetric interior penalty DG bilinear form $a(\\cdot,\\cdot;\\mu)$ and the affine decomposition of $k(x;\\mu)$, derive explicit offline expressions for parameter-independent reduced matrices $A_{q}^{N} \\in \\mathbb{R}^{N \\times N}$ whose entries depend solely on the reduced basis functions and the spatial coefficient components $k_{q}(x)$, and show how to perform the online assembly of the reduced stiffness matrix $A^{N}(\\mu) \\in \\mathbb{R}^{N \\times N}$ in terms of these offline matrices and the scalars $\\theta_{q}^{a}(\\mu)$, without accessing the high-dimensional DG space online. For concreteness, use the standard symmetric interior penalty DG bilinear form with interior face contributions and Nitsche-type boundary terms for homogeneous Dirichlet data:\n$$\na(u,v;\\mu) \n= \\sum_{K \\in \\mathcal{T}_{h}} \\int_{K} k(x;\\mu)\\, \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x\n- \\sum_{F \\in \\mathcal{F}_{i}} \\int_{F} \\big\\{ k(x;\\mu)\\nabla u \\cdot n_{F} \\big\\} \\,[v] \\,\\mathrm{d}s\n- \\sum_{F \\in \\mathcal{F}_{i}} \\int_{F} \\big\\{ k(x;\\mu)\\nabla v \\cdot n_{F} \\big\\} \\,[u] \\,\\mathrm{d}s\n+ \\sum_{F \\in \\mathcal{F}_{i}} \\int_{F} \\frac{\\sigma\\, k(x;\\mu)}{h_{F}}\\, [u]\\, [v] \\,\\mathrm{d}s\n$$\n$$\n- \\sum_{F \\in \\mathcal{F}_{b}} \\int_{F} k(x;\\mu)\\, \\nabla u \\cdot n \\, v \\,\\mathrm{d}s\n- \\sum_{F \\in \\mathcal{F}_{b}} \\int_{F} k(x;\\mu)\\, \\nabla v \\cdot n \\, u \\,\\mathrm{d}s\n+ \\sum_{F \\in \\mathcal{F}_{b}} \\int_{F} \\frac{\\sigma\\, k(x;\\mu)}{h_{F}}\\, u\\, v \\,\\mathrm{d}s,\n$$\nwhere $\\mathcal{F}_{i}$ denotes the set of interior faces, $\\mathcal{F}_{b}$ the set of boundary faces, $n_{F}$ is a fixed unit normal on face $F$, $n$ the outward unit normal on $\\partial \\Omega$, $h_{F}$ the face-dependent mesh size, $[\\,\\cdot\\,]$ denotes the jump across a face, and $\\{\\,\\cdot\\,\\}$ denotes the average across a face.\n\nYour final answer must be a single closed-form analytic expression for the online reduced stiffness matrix $A^{N}(\\mu)$ in terms of the offline matrices and the parameter-dependent scalars. No numerical evaluation is required. Express the final matrix expression symbolically; no rounding is needed and no physical units are involved.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the field of reduced basis methods for partial differential equations. All necessary information is provided, and the goal is clearly defined.\n\nThe objective is to derive the offline/online decomposition for the reduced stiffness matrix associated with a symmetric interior penalty discontinuous Galerkin (SIPG) discretization of a parametric diffusion problem.\n\nThe high-fidelity problem seeks a solution $u(x;\\mu) \\in V_h$ for a given parameter $\\mu$. The associated weak formulation, based on the provided SIPG bilinear form $a(\\cdot,\\cdot;\\mu)$, is: find $u \\in V_h$ such that\n$$\na(u,v;\\mu) = L(v) \\quad \\forall v \\in V_h,\n$$\nwhere $L(v) = \\int_{\\Omega} f(x)v(x)\\,\\mathrm{d}x$.\n\nThe reduced basis method constructs an approximation $u_N(x;\\mu)$ in a low-dimensional space $V_N = \\operatorname{span}\\{\\zeta_{1},\\dots,\\zeta_{N}\\} \\subset V_h$. The reduced solution is expressed as a linear combination of the pre-computed basis functions $\\zeta_j$:\n$$\nu_N(x;\\mu) = \\sum_{j=1}^{N} u_j^N(\\mu) \\zeta_j(x),\n$$\nwhere $u_j^N(\\mu)$ are the unknown scalar coefficients.\n\nApplying the Galerkin projection principle, we require the residual of the weak form to be orthogonal to the reduced space $V_N$. This is equivalent to enforcing the weak form for all basis functions $v_N = \\zeta_i(x)$ for $i=1,\\dots,N$:\n$$\na(u_N, \\zeta_i; \\mu) = L(\\zeta_i) \\quad \\text{for } i=1,\\dots,N.\n$$\nSubstituting the expansion for $u_N(x;\\mu)$ and using the linearity of $a(\\cdot,\\cdot;\\mu)$ in its first argument, we obtain:\n$$\n\\sum_{j=1}^{N} u_j^N(\\mu) \\, a(\\zeta_j, \\zeta_i; \\mu) = L(\\zeta_i) \\quad \\text{for } i=1,\\dots,N.\n$$\nThis is a dense linear system of equations of size $N \\times N$, which can be written in matrix form as:\n$$\nA^N(\\mu) \\mathbf{u}^N(\\mu) = F^N,\n$$\nwhere $\\mathbf{u}^N(\\mu) = [u_1^N(\\mu), \\dots, u_N^N(\\mu)]^T$ is the vector of unknown coefficients, $F^N$ is the reduced load vector with entries $(F^N)_i = L(\\zeta_i)$, and $A^N(\\mu)$ is the reduced stiffness matrix. The entries of the reduced stiffness matrix are given by:\n$$\n(A^N(\\mu))_{ij} = a(\\zeta_j, \\zeta_i; \\mu).\n$$\nThe core of the offline/online decomposition lies in exploiting the specific structure of the bilinear form $a(\\cdot,\\cdot;\\mu)$ and the diffusion coefficient $k(x;\\mu)$. The SIPG bilinear form $a(\\cdot,\\cdot;\\mu)$ is given as:\n$$\na(u,v;\\mu) \n= \\sum_{K \\in \\mathcal{T}_{h}} \\int_{K} k(x;\\mu)\\, \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x\n- \\sum_{F \\in \\mathcal{F}_{i}} \\int_{F} \\big\\{ k(x;\\mu)\\nabla u \\cdot n_{F} \\big\\} \\,[v] \\,\\mathrm{d}s\n- \\sum_{F \\in \\mathcal{F}_{i}} \\int_{F} \\big\\{ k(x;\\mu)\\nabla v \\cdot n_{F} \\big\\} \\,[u] \\,\\mathrm{d}s\n+ \\sum_{F \\in \\mathcal{F}_{i}} \\int_{F} \\frac{\\sigma\\, k(x;\\mu)}{h_{F}}\\, [u]\\, [v] \\,\\mathrm{d}s\n$$\n$$\n- \\sum_{F \\in \\mathcal{F}_{b}} \\int_{F} k(x;\\mu)\\, \\nabla u \\cdot n \\, v \\,\\mathrm{d}s\n- \\sum_{F \\in \\mathcal{F}_{b}} \\int_{F} k(x;\\mu)\\, \\nabla v \\cdot n \\, u \\,\\mathrm{d}s\n+ \\sum_{F \\in \\mathcal{F}_{b}} \\int_{F} \\frac{\\sigma\\, k(x;\\mu)}{h_{F}}\\, u\\, v \\,\\mathrm{d}s.\n$$\nBy inspection, every term in this form is linear with respect to the diffusion coefficient function $k(x;\\mu)$. This means that if $k(x;\\mu)$ can be expressed as a linear combination of functions, the bilinear form will follow the same linear combination. Let us denote the bilinear form's dependence on the coefficient as $a[k](u,v)$. Then, for any scalar constants $c_1, c_2$ and functions $k_1, k_2$, we have $a[c_1 k_1 + c_2 k_2](u,v) = c_1 a[k_1](u,v) + c_2 a[k_2](u,v)$.\n\nThe problem states that the coefficient $k(x;\\mu)$ admits an affine parameter decomposition:\n$$\nk(x;\\mu) = \\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu)\\, k_{q}(x).\n$$\nHere, the functions $\\theta_{q}^{a}(\\mu)$ depend only on the parameter $\\mu$, and the functions $k_{q}(x)$ depend only on the spatial variable $x$.\n\nWe can now substitute this decomposition into the expression for the entries of the reduced stiffness matrix. Using the linearity of the bilinear form with respect to the coefficient function, we get:\n$$\n(A^N(\\mu))_{ij} = a(\\zeta_j, \\zeta_i; \\mu) = a[k(\\cdot;\\mu)](\\zeta_j, \\zeta_i) = a\\left[\\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu) k_{q}(\\cdot)\\right](\\zeta_j, \\zeta_i)\n$$\n$$\n(A^N(\\mu))_{ij} = \\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu) \\, a[k_{q}(\\cdot)](\\zeta_j, \\zeta_i).\n$$\nThis expression separates the parameter-dependent parts from the parameter-independent parts. The terms $a[k_{q}(\\cdot)](\\zeta_j, \\zeta_i)$ depend only on the fixed spatial functions $k_q(x)$ and the fixed reduced basis functions $\\zeta_j(x)$ and $\\zeta_i(x)$. They do not depend on the parameter $\\mu$.\n\nThis enables the offline/online computational strategy.\n\n**Offline Stage:** We pre-compute and store the $Q_a$ parameter-independent matrices $A_q^N \\in \\mathbb{R}^{N \\times N}$ for $q=1, \\dots, Q_a$. The entries of these matrices are defined as:\n$$\n(A_q^N)_{ij} = a[k_q(\\cdot)](\\zeta_j, \\zeta_i).\n$$\nExplicitly, this is the SIPG bilinear form evaluated with the coefficient function $k_q(x)$ for the pair of basis functions $(\\zeta_j, \\zeta_i)$. This is a computationally intensive step, as it involves integrals over the high-dimensional finite element mesh for all pairs of basis functions and for each coefficient component $k_q(x)$. However, it is performed only once.\n\n**Online Stage:** For any new parameter value $\\mu$, the reduced stiffness matrix $A^N(\\mu)$ can be assembled very rapidly. First, we evaluate the $Q_a$ scalar functions $\\theta_q^a(\\mu)$. Then, we construct $A^N(\\mu)$ as a linear combination of the pre-computed offline matrices:\n$$\n(A^N(\\mu))_{ij} = \\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu) (A_q^N)_{ij}.\n$$\nThis is a statement about the entries of the matrices. In matrix notation, this becomes a simple and fast matrix summation:\n$$\nA^N(\\mu) = \\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu) A_q^N.\n$$\nThis online assembly requires only $Q_a-1$ additions of $N \\times N$ matrices and $Q_a$ scalar-matrix multiplications. Since $N$ is small (typically $N \\ll \\dim(V_h)$), this step is extremely efficient and does not require any access to the high-dimensional DG space $V_h$, thus achieving the desired computational speed-up.\n\nThe final expression demonstrates how the reduced stiffness matrix for any parameter value $\\mu$ is constructed online from the pre-computed, parameter-independent matrices $A_q^N$ and the parameter-dependent scalar functions $\\theta_q^a(\\mu)$.",
            "answer": "$$\n\\boxed{A^{N}(\\mu) = \\sum_{q=1}^{Q_{a}} \\theta_{q}^{a}(\\mu)\\, A_{q}^{N}}\n$$"
        },
        {
            "introduction": "The greedy algorithm constructs a reduced basis by iteratively selecting snapshots that best capture the solution manifold. To ensure the resulting basis is numerically stable and well-conditioned, each new snapshot must be made orthogonal to the existing basis vectors. This process requires defining an appropriate inner product—often related to the natural energy norm of the problem—and implementing an orthonormalization procedure like Gram-Schmidt. This hands-on coding exercise  solidifies this concept by having you derive and implement the matrix representation of a Discontinuous Galerkin (DG) energy inner product and use it to perform this essential orthonormalization step.",
            "id": "3411758",
            "problem": "Consider a one-dimensional Symmetric Interior Penalty Galerkin (SIPG) discretization of a scalar model problem on the interval $[0,1]$ with a uniform mesh of $N_e$ elements of size $h = 1/N_e$. On each element, use piecewise linear Discontinuous Galerkin (DG) basis functions with two local degrees of freedom per element, corresponding to the values at the left and right endpoints of the element. Let the Discontinuous Galerkin (DG) energy inner product be defined by the bilinear form\n$$\n\\langle u, v \\rangle_E \\;=\\; \\sum_{K} \\int_{K} u'(x)\\,v'(x)\\,dx \\;+\\; \\sum_{F} \\frac{\\alpha}{h} \\,[u]_F\\,[v]_F,\n$$\nwhere $u'(x)$ denotes the derivative of $u$ on each element $K$, $\\alpha > 0$ is the interior penalty parameter, and $[u]_F$ is the jump of $u$ across an interior face $F$. On boundary faces, interpret the jump as the trace of the function, thereby adding a penalty term $\\frac{\\alpha}{h}\\,u^2$ on each boundary face.\n\n1. Derive the element contribution to the matrix representation of the inner product for the derivative term by mapping the reference element $[-1,1]$ to a physical element $K$ and using the identity $u'(x) = \\frac{2}{h}\\, \\frac{du}{d\\xi}$, where $\\xi \\in [-1,1]$ is the reference coordinate. Show that for piecewise linear basis functions on the reference element, the element matrix contribution for $\\int_{K} u'(x)\\,v'(x)\\,dx$ has the form\n$$\n\\frac{1}{h}\\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}.\n$$\n\n2. Express the contribution of each interior face to the matrix representation of the penalty term. For a face between element $e$ and element $e+1$, let $u_{e,R}$ denote the right-endpoint degree of freedom of element $e$ and $u_{e+1,L}$ denote the left-endpoint degree of freedom of element $e+1$. Use the definition of the jump $[u]_F = u_{e,R} - u_{e+1,L}$ to show the symmetric $2 \\times 2$ matrix stencil acting on the pair $\\left(u_{e,R}, u_{e+1,L}\\right)$ is\n$$\n\\frac{\\alpha}{h}\\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}.\n$$\nFor the boundary faces, add the scalar penalty $\\frac{\\alpha}{h}$ to the diagonal entry corresponding to the boundary degree of freedom.\n\n3. Assemble the full symmetric positive definite matrix $G \\in \\mathbb{R}^{(2N_e)\\times(2N_e)}$ that represents the DG energy inner product $\\langle u, v \\rangle_E = u^\\top G v$ for the described discretization. Assume that continuous functions $f(x)$ are mapped to DG snapshots $s \\in \\mathbb{R}^{2N_e}$ by sampling at the endpoints of each element: the left degree of freedom of element $e$ stores $f(e/N_e)$ and the right degree of freedom stores $f((e+1)/N_e)$.\n\n4. In a Reduced Basis (RB) method with greedy selection constrained by orthogonality, the current reduced space is represented by a matrix $V \\in \\mathbb{R}^{(2N_e)\\times r}$ whose columns are already orthonormal with respect to $\\langle \\cdot, \\cdot \\rangle_E$, i.e., $V^\\top G V = I_r$. Given a new snapshot $s \\in \\mathbb{R}^{2N_e}$, orthonormalize $s$ against the current basis $V$ with respect to $\\langle \\cdot, \\cdot \\rangle_E$ by:\n- computing the projection coefficients $c = V^\\top G s$,\n- forming the residual $r = s - V c$,\n- computing the DG-energy norm $\\|r\\|_E = \\sqrt{r^\\top G r}$,\n- deciding acceptance based on a tolerance $\\tau > 0$: if $\\|r\\|_E \\le \\tau$, the snapshot is rejected; otherwise, define $v_{\\text{new}} = r/\\|r\\|_E$ and verify $V^\\top G v_{\\text{new}} \\approx 0$.\n\nImplement a program that performs steps $1$–$4$ and outputs, for each test case, the triplet $[\\|r\\|_E, \\max\\left(|V^\\top G v_{\\text{new}}|\\right), \\text{accepted\\_flag}]$, where $\\text{accepted\\_flag}$ is the integer $1$ if accepted and $0$ if rejected. If the snapshot is rejected, set the second entry $\\max\\left(|V^\\top G v_{\\text{new}}|\\right)$ to $0.0$.\n\nUse the following test suite (each test case specifies $(N_e, \\alpha, \\tau, \\text{basis-snapshots}, \\text{new-snapshot})$):\n- Test Case A (general case): $N_e = 4$, $\\alpha = 10$, $\\tau = 10^{-10}$, basis snapshots from $f_1(x) = \\sin(\\pi x)$ and $f_2(x) = \\cos(2\\pi x)$, new snapshot from $f_3(x) = e^{-x}$.\n- Test Case B (boundary-size case, single element): $N_e = 1$, $\\alpha = 10$, $\\tau = 10^{-10}$, empty basis $V$ (no columns), new snapshot given explicitly as the vector $s = [1.0, -0.5]$.\n- Test Case C (near-collinearity edge case): $N_e = 4$, $\\alpha = 50$, $\\tau = 10^{-8}$, basis snapshots from $f_1(x) = \\sin(\\pi x)$ and $f_2(x) = \\cos(2\\pi x)$, new snapshot $s = v_1 + \\varepsilon w$ where $v_1$ is the first column of the already orthonormalized basis $V$, $w \\in \\mathbb{R}^{2N_e}$ is the deterministic vector with entries alternating $1,-1,1,-1,\\dots$, and $\\varepsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist $[\\|r\\|_E,\\max(|V^\\top G v_{\\text{new}}|),\\text{accepted\\_flag}]$ to the outer list. For example, the output format must be like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$ with numeric values and integers only.",
            "solution": "The problem statement is well-posed, scientifically grounded, and provides all necessary information to derive and implement a solution. It consists of four parts: derivation of element and face matrices for a Symmetric Interior Penalty Galerkin (SIPG) discretization, assembly of the global matrix representing the DG energy inner product, and implementation of a single step of a greedy algorithm for reduced basis generation. We will address each part systematically.\n\n### Part 1: Derivation of the Element Derivative Matrix\nWe are asked to derive the element matrix contribution from the term $\\int_{K} u'(x)\\,v'(x)\\,dx$. We consider a reference element $\\xi \\in [-1, 1]$ with linear basis functions $\\hat{\\phi}_L(\\xi) = \\frac{1-\\xi}{2}$ and $\\hat{\\phi}_R(\\xi) = \\frac{1+\\xi}{2}$. Their derivatives are $\\frac{d\\hat{\\phi}_L}{d\\xi} = -1/2$ and $\\frac{d\\hat{\\phi}_R}{d\\xi} = 1/2$.\n\nA physical element $K$ of length $h$ is mapped from the reference element. The problem provides the relation between the derivatives in the physical and reference coordinates: $u'(x) = \\frac{2}{h} \\frac{du}{d\\xi}$. The differential length element also transforms as $dx = \\frac{h}{2} d\\xi$.\n\nThe entries of the $2 \\times 2$ element matrix $A^K$ are given by $A^K_{ij} = \\int_{K} \\phi_i'(x) \\phi_j'(x) dx$, where $\\phi_i$ and $\\phi_j$ are the physical basis functions corresponding to the left ($L$) and right ($R$) degrees of freedom. By changing the integration variable to $\\xi$, we get:\n$$\nA^K_{ij} = \\int_{-1}^{1} \\left(\\frac{2}{h} \\frac{d\\hat{\\phi}_i}{d\\xi}\\right) \\left(\\frac{2}{h} \\frac{d\\hat{\\phi}_j}{d\\xi}\\right) \\left(\\frac{h}{2} d\\xi\\right) = \\frac{2}{h} \\int_{-1}^{1} \\frac{d\\hat{\\phi}_i}{d\\xi} \\frac{d\\hat{\\phi}_j}{d\\xi} d\\xi.\n$$\nWe compute the four entries of the matrix:\n$A^K_{LL} = \\frac{2}{h} \\int_{-1}^{1} \\left(-\\frac{1}{2}\\right) \\left(-\\frac{1}{2}\\right) d\\xi = \\frac{2}{h} \\cdot \\frac{1}{4} \\int_{-1}^{1} d\\xi = \\frac{2}{h} \\cdot \\frac{1}{4} \\cdot 2 = \\frac{1}{h}$.\n$A^K_{RR} = \\frac{2}{h} \\int_{-1}^{1} \\left(\\frac{1}{2}\\right) \\left(\\frac{1}{2}\\right) d\\xi = \\frac{2}{h} \\cdot \\frac{1}{4} \\cdot 2 = \\frac{1}{h}$.\n$A^K_{LR} = A^K_{RL} = \\frac{2}{h} \\int_{-1}^{1} \\left(-\\frac{1}{2}\\right) \\left(\\frac{1}{2}\\right) d\\xi = \\frac{2}{h} \\cdot \\left(-\\frac{1}{4}\\right) \\cdot 2 = -\\frac{1}{h}$.\n\nCombining these entries, the element matrix for the derivative term is:\n$$\nA^K = \\frac{1}{h}\\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix},\n$$\nwhich confirms the form given in the problem.\n\n### Part 2: Contribution of the Penalty Term\nThe penalty term in the DG energy inner product is $\\sum_{F} \\frac{\\alpha}{h} [u]_F [v]_F$.\nFor an interior face $F$ located between element $e$ and element $e+1$, the jump is defined as $[u]_F = u_{e,R} - u_{e+1,L}$, where $u_{e,R}$ is the value at the right endpoint of element $e$ and $u_{e+1,L}$ is the value at the left endpoint of element $e+1$. The contribution to the bilinear form is $\\frac{\\alpha}{h} (u_{e,R} - u_{e+1,L})(v_{e,R} - v_{e+1,L})$.\nThis corresponds to a quadratic form on the vector of degrees of freedom $\\mathbf{u} = \\begin{pmatrix} u_{e,R} & u_{e+1,L} \\end{pmatrix}^\\top$. The contribution to the energy is $\\frac{\\alpha}{h} ([u]_F)^2 = \\frac{\\alpha}{h} (u_{e,R} - u_{e+1,L})^2 = \\frac{\\alpha}{h} (u_{e,R}^2 - 2u_{e,R}u_{e+1,L} + u_{e+1,L}^2)$. This quadratic form can be expressed in matrix form as $\\mathbf{u}^\\top M_F \\mathbf{u}$. The symmetric matrix $M_F$ that yields this form is:\n$$\nM_F = \\frac{\\alpha}{h}\\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}.\n$$\nThis is the $2 \\times 2$ stencil that acts on the pair of degrees of freedom $(u_{e,R}, u_{e+1,L})$.\n\nFor boundary faces, the jump is interpreted as the trace of the function. At the left boundary ($x=0$), the face involves the degree of freedom $u_{0,L}$. The penalty term is $\\frac{\\alpha}{h} u_{0,L}^2$. This adds a scalar value $\\frac{\\alpha}{h}$ to the diagonal entry of the global matrix corresponding to the $u_{0,L}$ degree of freedom. Similarly, at the right boundary ($x=1$), the penalty term is $\\frac{\\alpha}{h} u_{N_e-1,R}^2$, which adds $\\frac{\\alpha}{h}$ to the diagonal entry for the $u_{N_e-1,R}$ degree of freedom.\n\n### Part 3: Assembly of the Global Matrix $G$\nThe global matrix $G \\in \\mathbb{R}^{(2N_e) \\times (2N_e)}$ is assembled by summing the contributions from all elements and faces. The degrees of freedom are ordered element by element: $(u_{0,L}, u_{0,R}, u_{1,L}, u_{1,R}, \\dots, u_{N_e-1,L}, u_{N_e-1,R})$. The global index for degree of freedom $u_{e,L}$ is $2e$, and for $u_{e,R}$ it is $2e+1$.\n\nThe assembly process is as follows:\n1. Initialize $G$ as a zero matrix of size $(2N_e) \\times (2N_e)$.\n2. For each element $e = 0, \\dots, N_e-1$:\n   Add the element derivative matrix $A^K = \\frac{1}{h} \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}$ to the submatrix of $G$ corresponding to indices $(2e, 2e+1)$.\n3. For each interior face $f$ between element $e$ and $e+1$ (where $e = 0, \\dots, N_e-2$):\n   Add the face penalty matrix $M_F = \\frac{\\alpha}{h} \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}$ to the submatrix of $G$ corresponding to indices $(2e+1, 2e+2)$. These indices correspond to the DoFs $u_{e,R}$ and $u_{e+1,L}$.\n4. Add boundary penalties:\n   Add $\\frac{\\alpha}{h}$ to $G[0,0]$ for the left boundary.\n   Add $\\frac{\\alpha}{h}$ to $G[2N_e-1, 2N_e-1]$ for the right boundary.\n\nThe resulting matrix $G$ is symmetric and, for $\\alpha > 0$, positive definite, thus defining a valid inner product $\\langle u, v \\rangle_E = u^\\top G v$.\n\nSnapshots $s \\in \\mathbb{R}^{2N_e}$ for a continuous function $f(x)$ are generated by sampling $f(x)$ at the physical locations of the degrees of freedom. For element $e$, its left and right nodes are at $x_e = eh$ and $x_{e+1} = (e+1)h$. Thus, the snapshot vector $s$ has components $s_{2e} = f(eh)$ and $s_{2e+1} = f((e+1)h)$ for $e = 0, \\dots, N_e-1$.\n\n### Part 4: Reduced Basis Orthogonalization\nThe procedure described is a modified Gram-Schmidt orthonormalization with respect to the energy inner product defined by $G$. Given an existing orthonormal basis $V \\in \\mathbb{R}^{(2N_e) \\times r}$ (satisfying $V^\\top G V = I_r$) and a new snapshot $s \\in \\mathbb{R}^{2N_e}$, we project $s$ onto the space spanned by the columns of $V$ and find the residual.\n1. The projection of $s$ onto the basis $V$ is $s_{\\text{proj}} = Vc$, where $c$ are the projection coefficients. These are computed by ensuring the residual $r=s-s_{\\text{proj}}$ is $G$-orthogonal to the basis: $V^\\top G (s-Vc)=0$. This gives $V^\\top G s - (V^\\top G V)c = 0$. Since $V^\\top G V = I_r$, we have $c = V^\\top G s$.\n2. The residual is $r = s - Vc = s - V(V^\\top G s)$.\n3. The DG-energy norm of the residual is $\\|r\\|_E = \\sqrt{\\langle r, r \\rangle_E} = \\sqrt{r^\\top G r}$.\n4. The snapshot is considered linearly independent enough to be added to the basis if its orthogonal component is sufficiently large, i.e., $\\|r\\|_E > \\tau$. If this condition holds, the snapshot is `accepted` (flag=$1$) and the new normalized basis vector is $v_{\\text{new}} = r / \\|r\\|_E$. Otherwise, it is `rejected` (flag=$0$).\n5. Post-acceptance, we verify the orthogonality of the new vector against the old basis by computing $\\max(|V^\\top G v_{\\text{new}}|)$. Due to finite precision arithmetic, this will be close to, but not exactly, zero. If the snapshot is rejected, this value is set to $0.0$.\n\nFor test cases with predefined \"basis snapshots\", we first construct the basis $V$ by applying this greedy procedure sequentially to each basis snapshot, starting with an empty basis. Then, we apply the procedure one more time to the \"new snapshot\" against the fully constructed basis $V$ to obtain the required output triplet.",
            "answer": "```python\nimport numpy as np\n\ndef assemble_G(Ne, alpha):\n    \"\"\"\n    Assembles the global matrix G for the DG energy inner product.\n    \"\"\"\n    h = 1.0 / Ne\n    h_inv = 1.0 / h\n    alpha_h = alpha / h\n    dim = 2 * Ne\n    G = np.zeros((dim, dim))\n\n    # Part 1: Element derivative contributions\n    element_mat = h_inv * np.array([[1, -1], [-1, 1]])\n    for e in range(Ne):\n        G[2*e:2*e+2, 2*e:2*e+2] += element_mat\n\n    # Part 2: Face penalty contributions\n    face_mat = alpha_h * np.array([[1, -1], [-1, 1]])\n    # Interior faces\n    for e in range(Ne - 1):\n        G[2*e+1:2*e+3, 2*e+1:2*e+3] += face_mat\n    \n    # Boundary faces\n    G[0, 0] += alpha_h  # Left boundary at x=0\n    G[dim-1, dim-1] += alpha_h  # Right boundary at x=1\n    \n    return G\n\ndef generate_snapshot(func, Ne):\n    \"\"\"\n    Generates a DG snapshot vector from a continuous function.\n    \"\"\"\n    h = 1.0 / Ne\n    dim = 2 * Ne\n    s = np.zeros(dim)\n    for e in range(Ne):\n        x_left = e * h\n        x_right = (e + 1) * h\n        s[2*e] = func(x_left)\n        s[2*e+1] = func(x_right)\n    return s\n\ndef orthonormalization_step(s, V, G, tau):\n    \"\"\"\n    Performs one step of G-orthonormalization of snapshot s against basis V.\n    V is a numpy matrix with basis vectors as columns.\n    Returns: (residual_norm, max_ortho_error, accepted_flag, new_vector)\n    \"\"\"\n    r = s\n    if V.shape[1] > 0:\n        Gs = G @ s\n        c = V.T @ Gs\n        r = s - V @ c\n\n    Gr = G @ r\n    # Use max(0, ...) to handle potential small negative values from floating point errors\n    norm_r_sq = r.T @ Gr\n    norm_r = np.sqrt(max(0, norm_r_sq))\n\n    if norm_r <= tau:\n        accepted_flag = 0\n        max_ortho_err = 0.0\n        v_new = None\n    else:\n        accepted_flag = 1\n        v_new = r / norm_r\n        if V.shape[1] > 0:\n            Gv_new = G @ v_new\n            d = V.T @ Gv_new\n            max_ortho_err = np.max(np.abs(d))\n        else:\n            max_ortho_err = 0.0\n\n    return norm_r, max_ortho_err, accepted_flag, v_new\n\ndef solve():\n    \"\"\"\n    Main solver function to process all test cases.\n    \"\"\"\n    # Define test cases: (Ne, alpha, tau, basis_funcs, new_snapshot_spec)\n    test_cases = [\n        # Test Case A\n        (4, 10, 1e-10, \n         [lambda x: np.sin(np.pi * x), lambda x: np.cos(2 * np.pi * x)], \n         {'type': 'func', 'def': lambda x: np.exp(-x)}),\n        # Test Case B\n        (1, 10, 1e-10, \n         [], \n         {'type': 'vector', 'def': np.array([1.0, -0.5])}),\n        # Test Case C\n        (4, 50, 1e-8, \n         [lambda x: np.sin(np.pi * x), lambda x: np.cos(2 * np.pi * x)], \n         {'type': 'special', 'eps': 1e-12})\n    ]\n\n    all_results = []\n\n    for Ne, alpha, tau, basis_funcs, new_snapshot_spec in test_cases:\n        \n        G = assemble_G(Ne, alpha)\n        dim = 2 * Ne\n\n        # Build orthonormal basis V\n        V_list = []\n        V_matrix = np.empty((dim, 0))\n        for func in basis_funcs:\n            s_basis = generate_snapshot(func, Ne)\n            _, _, accepted, v_new = orthonormalization_step(s_basis, V_matrix, G, tau)\n            if accepted:\n                V_list.append(v_new)\n                V_matrix = np.array(V_list).T\n        \n        # Prepare the new snapshot for processing\n        if new_snapshot_spec['type'] == 'func':\n            s_new = generate_snapshot(new_snapshot_spec['def'], Ne)\n        elif new_snapshot_spec['type'] == 'vector':\n            s_new = new_snapshot_spec['def']\n        elif new_snapshot_spec['type'] == 'special':\n            if not V_list:\n                # This case requires a non-empty basis\n                # Fallback to a zero vector to avoid crashing, though problem implies v1 exists\n                s_new = np.zeros(dim)\n            else:\n                v1 = V_list[0]\n                eps = new_snapshot_spec['eps']\n                w = np.array([(-1)**i for i in range(dim)])\n                s_new = v1 + eps * w\n\n        # Process the new snapshot against the final basis V\n        norm_r, max_ortho_err, accepted_flag, _ = orthonormalization_step(s_new, V_matrix, G, tau)\n        \n        all_results.append([norm_r, max_ortho_err, accepted_flag])\n\n    # Format output\n    output_str = \"[\" + \",\".join([f\"[{r},{e},{f}]\" for r, e, f in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While a standard greedy algorithm operates on a fixed, pre-defined training set, a more powerful and efficient approach is to adaptively refine this set during the offline stage. This allows the algorithm to intelligently focus on regions of the parameter domain where the error is largest, leading to a more compact and effective reduced basis. In this capstone exercise , you will implement a complete adaptive greedy algorithm for a spectral problem, integrating the high-fidelity model, a sharp error estimator, and the logic that drives the adaptive enrichment of the training set, providing a comprehensive view of the method in practice.",
            "id": "3411750",
            "problem": "Consider the parametric, coercive, symmetric boundary value problem on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions: find $u(\\cdot;\\mu)$ such that\n$$\n-\\mu\\,u''(x;\\mu) + \\gamma\\,u(x;\\mu) = f(x), \\quad x\\in (0,1), \\quad u(0;\\mu)=u(1;\\mu)=0,\n$$\nwhere the parameter $\\mu$ varies in a compact parameter domain $\\mathcal{P}=[\\mu_{\\min},\\mu_{\\max}]$, the reaction coefficient $\\gamma>0$ is fixed and independent of $\\mu$, and the source is $f(x)=1$. A spectral Galerkin full-order discretization in the orthonormal sine basis $\\{\\varphi_k(x)\\}_{k=1}^{\\mathcal{N}_h}$ with $\\varphi_k(x)=\\sqrt{2}\\sin(k\\pi x)$ yields a diagonal stiffness-plus-mass structure with diagonal entries $\\mu\\,(k\\pi)^2+\\gamma$ for $k\\in\\{1,\\dots,\\mathcal{N}_h\\}$, and right-hand side coefficients $b_k=\\sqrt{2}\\int_0^1 \\sin(k\\pi x)\\,dx=\\sqrt{2}(1-(-1)^k)/(k\\pi)$. Therefore, the full-order coefficient vector $u^{\\mathrm{truth}}(\\mu)\\in\\mathbb{R}^{\\mathcal{N}_h}$ satisfies a symmetric positive definite system for each $\\mu\\in\\mathcal{P}$.\n\nDefine a reduced basis space $V_N=\\mathrm{span}\\{u^{\\mathrm{truth}}(\\mu_n)\\}_{n=1}^N$ constructed by a greedy algorithm over a finite training set $\\Xi\\subset\\mathcal{P}$. At each greedy step, the parameter $\\mu^\\star\\in\\Xi$ that maximizes the current reduced-basis a posteriori error estimator is selected, and the corresponding full-order snapshot is appended to the basis after orthonormalization with respect to a parameter-independent inner product induced by the spectral stiffness matrix. The error estimator is the dual norm of the residual based on the full-order operator, which, in this symmetric coercive setting, is equivalent to the energy-norm error. Assume the residual-based estimator is computed exactly in the full-order space by diagonal inversion of the spectral operator.\n\nYour tasks are:\n\n1) Starting from the fundamental facts that (i) the spectral Galerkin method yields a diagonal representation of the operator in the sine basis with diagonals $\\mu\\,(k\\pi)^2+\\gamma$, (ii) the Galerkin orthogonality holds for symmetric coercive problems, and (iii) the dual norm of the residual provides a sharp a posteriori estimator of the energy-norm error for symmetric coercive problems, derive a principled greedy selection rule over a training set $\\Xi$ and a rigorous residual-based error estimator map $\\mu\\mapsto \\Delta_N(\\mu)$ suitable for this problem. Your derivation must begin from these fundamentals and must not assume any unproven shortcuts.\n\n2) Design and implement an adaptive refinement strategy for the training set $\\Xi$ driven by the current estimator error map $\\mu\\mapsto \\Delta_N(\\mu)$. The refinement must:\n- Partition $\\mathcal{P}$ into subintervals induced by the sorted points of $\\Xi$.\n- For each subinterval, estimate its local maximal error using $\\Delta_N(\\mu)$ evaluated over a dense, uniform probe grid $\\mathcal{G}\\subset\\mathcal{P}$.\n- Add new parameter points at subinterval midpoints according to a quantitative criterion: select up to a prescribed number $\\mathcal{K}$ of subintervals whose local maximal error exceeds a fraction $\\theta\\in(0,1)$ of the current global maximal error, with ties broken by larger local maxima first. If a subinterval contains no probe grid point, evaluate the estimator at its midpoint to decide refinement.\n- Repeat refinement for a prescribed number of rounds $\\mathcal{R}$, rebuilding the reduced basis at each round by the greedy procedure until either the basis size reaches a prescribed cap $N_{\\max}$ or the maximal training-set estimator is below a tolerance $\\varepsilon_{\\mathrm{tol}}$.\n\n3) Implement the above as a complete program that uses the spectral Galerkin full-order model with $\\mathcal{N}_h$ sine modes, constructs reduced bases by greedy selection with orthonormalization in the stiffness-induced inner product, evaluates the residual-based estimator exactly by diagonal inversion, and adaptively refines $\\Xi$ according to the criterion described.\n\nYour program must execute the following test suite and produce the requested final outputs.\n\nTest Suite:\n- Case A (happy path, broad variation): $\\mathcal{N}_h=80$, $\\gamma=1$, $\\mathcal{P}=[\\mu_{\\min},\\mu_{\\max}]=[0.1,1.0]$, initial uniform training set size $M_0=5$ (including endpoints), probe grid size $|\\mathcal{G}|=400$, greedy cap $N_{\\max}=6$, tolerance $\\varepsilon_{\\mathrm{tol}}=10^{-6}$, refinement rounds $\\mathcal{R}=2$, per-round additions cap $\\mathcal{K}=2$, threshold fraction $\\theta=0.6$.\n- Case B (narrower domain): $\\mathcal{N}_h=80$, $\\gamma=1$, $\\mathcal{P}=[0.5,1.0]$, $M_0=4$, $|\\mathcal{G}|=300$, $N_{\\max}=5$, $\\varepsilon_{\\mathrm{tol}}=10^{-8}$, $\\mathcal{R}=2$, $\\mathcal{K}=1$, $\\theta=0.7$.\n- Case C (boundary emphasis near small diffusivity): $\\mathcal{N}_h=80$, $\\gamma=1$, $\\mathcal{P}=[0.05,0.2]$, $M_0=6$, $|\\mathcal{G}|=500$, $N_{\\max}=8$, $\\varepsilon_{\\mathrm{tol}}=10^{-7}$, $\\mathcal{R}=3$, $\\mathcal{K}=2$, $\\theta=0.5$.\n\nAnswer Specification:\n- For each case, after completing all refinement rounds, rebuild the reduced basis by the greedy algorithm for the final training set and compute the final worst-case estimator $\\max_{\\mu\\in\\mathcal{G}}\\Delta_N(\\mu)$ over the probe grid $\\mathcal{G}$ with the final basis. Return this maximal value as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_A,r_B,r_C]$, where $r_A$, $r_B$, and $r_C$ are the three floating-point results for Cases A, B, and C.\n\nAngle units are not applicable. No physical units are required. All numerical quantities must be reported in default floating-point units. Ensure scientific realism by using the exact diagonal inverses implied by the spectral Galerkin discretization for evaluating the residual-based estimator.",
            "solution": "We begin from core principles for symmetric, coercive, parameterized partial differential equations discretized by spectral or discontinuous Galerkin methods. The operator associated with the boundary value problem is, for each parameter $\\mu\\in\\mathcal{P}$, the bilinear form\n$$\na_\\mu(u,v) = \\mu \\int_0^1 u'(x)\\,v'(x)\\,dx + \\gamma \\int_0^1 u(x)\\,v(x)\\,dx,\n$$\nwhich is symmetric and coercive for any $\\mu>0$ and $\\gamma>0$. In the spectral Galerkin method with the sine basis $\\{\\varphi_k(x)\\}_{k=1}^{\\mathcal{N}_h}$ where $\\varphi_k(x)=\\sqrt{2}\\sin(k\\pi x)$, the stiffness part and the mass part are diagonal because $\\{\\varphi_k\\}$ are eigenfunctions of the Dirichlet Laplacian and orthonormal in $L^2(0,1)$. Specifically,\n$$\n\\int_0^1 \\varphi_k'(x)\\,\\varphi_j'(x)\\,dx = (k\\pi)^2\\,\\delta_{kj},\\qquad \\int_0^1 \\varphi_k(x)\\,\\varphi_j(x)\\,dx = \\delta_{kj}.\n$$\nTherefore, the spectral Galerkin full-order discrete operator is diagonal with entries $d_k(\\mu)=\\mu\\,(k\\pi)^2 + \\gamma$ for $k\\in\\{1,\\dots,\\mathcal{N}_h\\}$. The right-hand side coefficients for $f(x)=1$ are\n$$\nb_k = \\int_0^1 f(x)\\,\\varphi_k(x)\\,dx = \\sqrt{2}\\int_0^1 \\sin(k\\pi x)\\,dx = \\frac{\\sqrt{2}\\,\\bigl(1-(-1)^k\\bigr)}{k\\pi}.\n$$\nThe full-order discrete solution coefficients are thus explicitly given by\n$$\nu^{\\mathrm{truth}}_k(\\mu) = \\frac{b_k}{\\mu\\,(k\\pi)^2 + \\gamma}.\n$$\n\nFor a reduced basis of dimension $N$, spanned by selected full-order snapshots $V_N=\\mathrm{span}\\{u^{\\mathrm{truth}}(\\mu_n)\\}_{n=1}^N$, the Galerkin reduced system is\n$$\nA_N(\\mu)\\,\\alpha(\\mu) = B_N, \\qquad A_N(\\mu) = V^\\top A(\\mu)\\,V,\\quad B_N=V^\\top b,\n$$\nwhere $V\\in\\mathbb{R}^{\\mathcal{N}_h\\times N}$ has as columns a basis of $V_N$, $A(\\mu)\\in\\mathbb{R}^{\\mathcal{N}_h\\times \\mathcal{N}_h}$ is the full-order diagonal operator with entries $d_k(\\mu)$, and $b\\in\\mathbb{R}^{\\mathcal{N}_h}$ is the full-order right-hand side vector. The reduced solution is $u_N(\\mu)=V\\,\\alpha(\\mu)$.\n\nThe residual for a reduced solution is the linear functional\n$$\nr_\\mu(v) = f(v) - a_\\mu(u_N(\\mu),v) = \\langle b - A(\\mu)u_N(\\mu),\\,v\\rangle,\n$$\nwhich in coefficient vector form is $r(\\mu) = b - A(\\mu)\\,u_N(\\mu)$. For symmetric coercive problems, the dual norm of the residual with respect to the energy norm generated by $a_\\mu(\\cdot,\\cdot)$ is equal to the energy-norm error. Concretely, since $A(\\mu)$ is symmetric positive definite and $u^{\\mathrm{truth}}(\\mu)$ satisfies $A(\\mu)\\,u^{\\mathrm{truth}}(\\mu)=b$, one has the error $e(\\mu)=u^{\\mathrm{truth}}(\\mu)-u_N(\\mu)$ satisfying $A(\\mu)e(\\mu)=r(\\mu)$. Then\n$$\n\\|e(\\mu)\\|_{a_\\mu}^2 = a_\\mu(e(\\mu),e(\\mu)) = e(\\mu)^\\top A(\\mu)\\,e(\\mu) = e(\\mu)^\\top r(\\mu) = r(\\mu)^\\top A(\\mu)^{-1} r(\\mu),\n$$\nwhich shows the exact identity\n$$\n\\Delta_N(\\mu) := \\|r(\\mu)\\|_{A(\\mu)^{-1}} = \\sqrt{r(\\mu)^\\top A(\\mu)^{-1} r(\\mu)} = \\|e(\\mu)\\|_{a_\\mu}.\n$$\nBecause the full-order operator $A(\\mu)$ is diagonal in this spectral setting, the computation of $A(\\mu)^{-1}$ is exact, robust, and inexpensive: it amounts to entrywise reciprocal of the diagonal entries $d_k(\\mu)$.\n\nThe greedy selection rule over a finite training set $\\Xi\\subset\\mathcal{P}$ follows the standard paradigm grounded in these principles. Given a current reduced basis $V_N$, the next parameter is chosen by\n$$\n\\mu_{N+1} \\in \\operatorname*{arg\\,max}_{\\mu\\in\\Xi} \\Delta_N(\\mu),\n$$\nand the corresponding snapshot $u^{\\mathrm{truth}}(\\mu_{N+1})$ is orthonormalized against $V_N$ to obtain $V_{N+1}$. The orthonormalization is performed with respect to a parameter-independent inner product that reflects the stiffness part of the bilinear form; here we use the inner product $\\langle u,v\\rangle_K = u^\\top K\\,v$ where $K$ is the diagonal matrix with entries $(k\\pi)^2$, since $A(\\mu)=\\mu K + \\gamma I$ and $K$ encodes the parameter-independent differential operator. This choice preserves numerical stability and aligns with the coercivity structure.\n\nAdaptive refinement of the training set $\\Xi$ is driven by the current map $\\mu\\mapsto \\Delta_N(\\mu)$, evaluated over a dense probe grid $\\mathcal{G}$. Let the sorted training set be $\\Xi=\\{\\mu^{(0)},\\mu^{(1)},\\dots,\\mu^{(m)}\\}$ with $\\mu^{(0)}=\\mu_{\\min}$ and $\\mu^{(m)}=\\mu_{\\max}$. Define subintervals $I_j=[\\mu^{(j)},\\mu^{(j+1)}]$ for $j\\in\\{0,\\dots,m-1\\}$. For each $I_j$, compute a local indicator\n$$\nE_j = \\max_{\\mu\\in \\mathcal{G}\\cap I_j}\\Delta_N(\\mu),\n$$\nwith the convention that if $\\mathcal{G}\\cap I_j=\\emptyset$, we evaluate $\\Delta_N$ at the midpoint $\\frac{1}{2}(\\mu^{(j)}+\\mu^{(j+1)})$ to define $E_j$. Let $E_{\\max}=\\max_j E_j$. Given a threshold fraction $\\theta\\in(0,1)$ and a per-round cap $\\mathcal{K}\\in\\mathbb{N}$, we select up to $\\mathcal{K}$ indices $j$ with $E_j\\ge \\theta E_{\\max}$, breaking ties by larger $E_j$ first, and refine by inserting the midpoints of the selected intervals into $\\Xi$. This procedure is repeated for a prescribed number $\\mathcal{R}$ of rounds. At each round, the reduced basis is rebuilt by the greedy algorithm using the current $\\Xi$ until either the maximal training-set estimator is below a prescribed tolerance $\\varepsilon_{\\mathrm{tol}}$ or the reduced basis size reaches a cap $N_{\\max}$.\n\nAlgorithmic design:\n\n- Full-order model assembly: construct vectors of diagonal entries $d_k(\\mu)=\\mu\\,(k\\pi)^2+\\gamma$ for $k\\in\\{1,\\dots,\\mathcal{N}_h\\}$ and the right-hand side $b_k=\\sqrt{2}(1-(-1)^k)/(k\\pi)$. Also form the vector of stiffness eigenvalues $\\kappa_k=(k\\pi)^2$.\n\n- Inner product and orthonormalization: use modified Gram–Schmidt with respect to $\\langle \\cdot,\\cdot\\rangle_K$, i.e., $\\langle u,v\\rangle_K = \\sum_{k=1}^{\\mathcal{N}_h} \\kappa_k\\,u_k\\,v_k$. Normalize by the induced norm $\\|v\\|_K=\\sqrt{\\langle v,v\\rangle_K}$.\n\n- Reduced Galerkin solve: for a given $\\mu$, assemble $A_N(\\mu) = \\mu I + \\gamma G$ where $G=V^\\top V$ because $V^\\top K V=I$ by $K$-orthonormality, compute $B_N=V^\\top b$, solve $A_N(\\mu)\\alpha(\\mu)=B_N$, and set $u_N(\\mu)=V\\,\\alpha(\\mu)$.\n\n- Residual-based estimator: form the residual $r(\\mu)=b - (\\mu K + \\gamma I) u_N(\\mu)$ and compute\n$$\n\\Delta_N(\\mu) = \\left(\\sum_{k=1}^{\\mathcal{N}_h} \\frac{r_k(\\mu)^2}{\\mu\\,(k\\pi)^2+\\gamma}\\right)^{1/2},\n$$\nwhere diagonal inversion is exact due to the spectral representation.\n\n- Greedy selection: iterate on $\\Xi$ to select $\\mu^\\star$ maximizing $\\Delta_N(\\mu)$, append the snapshot $u^{\\mathrm{truth}}(\\mu^\\star)$ after $K$-orthonormalization, and stop if either $N=N_{\\max}$ or $\\max_{\\mu\\in\\Xi}\\Delta_N(\\mu)\\le \\varepsilon_{\\mathrm{tol}}$.\n\n- Adaptive refinement: evaluate $\\Delta_N(\\mu)$ on $\\mathcal{G}$, compute subinterval indicators $E_j$, select up to $\\mathcal{K}$ subintervals with $E_j\\ge \\theta E_{\\max}$ sorted by $E_j$ descending, insert midpoints, and proceed to the next round.\n\nThe test suite specifies three cases with different parameter domains and refinement settings. For each case, after completing all refinement rounds, we rebuild the reduced basis using the final training set and compute the final worst-case estimator $\\max_{\\mu\\in\\mathcal{G}}\\Delta_N(\\mu)$ over the probe grid. The program outputs these three floating-point values in a single-line list $[r_A,r_B,r_C]$.\n\nThis design is grounded in the symmetry and coercivity of $a_\\mu(\\cdot,\\cdot)$, the Galerkin orthogonality, and the exact computability of the dual residual norm in the spectral Galerkin discretization. The adaptive refinement criterion targets subintervals contributing most to the current maximal error, ensuring efficient reduction of the supremal error by enriching $\\Xi$ where the estimator landscape is steep or elevated. The use of the stiffness-induced inner product for orthonormalization stabilizes the greedy basis construction across $\\mu$ and aligns with the dominant differential operator, which is particularly appropriate in spectral and discontinuous Galerkin contexts where operator-induced norms are natural.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sine_spectral_operators(Nh, gamma):\n    \"\"\"\n    Build spectral operators for the 1D problem:\n    -mu * u'' + gamma * u = f, u(0)=u(1)=0 on (0,1),\n    using sine basis phi_k = sqrt(2) sin(k*pi*x), k=1..Nh.\n    Returns:\n        kappa: array of stiffness eigenvalues (k*pi)^2\n        b: right-hand side coefficients\n        gamma: reaction coefficient (as given)\n    \"\"\"\n    k = np.arange(1, Nh + 1, dtype=float)\n    kappa = (np.pi * k) ** 2\n    # b_k = sqrt(2) * (1 - (-1)^k) / (k*pi)\n    # compute (-1)^k = cos(pi*k) with care; but integer parity is fine:\n    parity = (-1.0) ** k  # numpy handles float power for integer-like\n    b = np.sqrt(2.0) * (1.0 - parity) / (k * np.pi)\n    return kappa, b, gamma\n\ndef truth_snapshot(mu, kappa, b, gamma):\n    \"\"\"\n    Compute full-order (truth) coefficients for a given mu:\n    u_k(mu) = b_k / (mu * kappa_k + gamma).\n    \"\"\"\n    denom = mu * kappa + gamma\n    return b / denom\n\ndef inner_K(u, v, kappa):\n    \"\"\"Compute K-inner product: <u, v>_K = sum_k kappa_k * u_k * v_k.\"\"\"\n    return float(np.dot(kappa * u, v))\n\ndef norm_K(u, kappa):\n    return np.sqrt(max(inner_K(u, u, kappa), 0.0))\n\ndef K_orthonormalize(V, v, kappa, tol=1e-12):\n    \"\"\"\n    Modified Gram-Schmidt orthonormalization of vector v against columns of V\n    with respect to K-inner product. Returns normalized vector or None if too small.\n    \"\"\"\n    w = v.copy()\n    if V is not None and V.shape[1] > 0:\n        for j in range(V.shape[1]):\n            q = V[:, j]\n            proj = inner_K(q, w, kappa)\n            w -= proj * q\n    nrm = norm_K(w, kappa)\n    if nrm < tol:\n        return None\n    return w / nrm\n\ndef build_reduced_basis_greedy(Xi, Nmax, eps_tol, kappa, b, gamma):\n    \"\"\"\n    Build reduced basis using greedy selection over training set Xi.\n    Orthonormalize snapshots w.r.t. K-inner product.\n    Stop when either N reaches Nmax or max estimator on Xi is below eps_tol.\n    Returns V (Nh x N), and the final max estimator over Xi.\n    \"\"\"\n    Nh = kappa.size\n    V = np.zeros((Nh, 0), dtype=float)\n    # Precompute useful quantities\n    # We'll iteratively expand V and recompute reduced quantities as needed.\n    # Define function to compute reduced solution and estimator for given mu.\n    def reduced_solution_and_estimator(mu, V):\n        if V.shape[1] == 0:\n            uN = np.zeros(Nh, dtype=float)\n        else:\n            # Build reduced matrices: A_N(mu) = mu * (V^T K V) + gamma * (V^T I V)\n            # Since V is K-orthonormal, V^T K V = I.\n            G = V.T @ V  # L2 Gram matrix\n            Bred = V.T @ b\n            AN = mu * np.eye(V.shape[1]) + gamma * G\n            alpha = np.linalg.solve(AN, Bred)\n            uN = V @ alpha\n        # Residual r = b - (mu*K + gamma*I) * uN\n        AuN = mu * (kappa * uN) + gamma * uN\n        r = b - AuN\n        # Estimator: sqrt( r^T A(mu)^{-1} r ) with A diagonal diag(mu*kappa + gamma)\n        inv_diag = 1.0 / (mu * kappa + gamma)\n        est2 = float(np.dot(r * inv_diag, r))\n        est = np.sqrt(max(est2, 0.0))\n        return uN, est\n\n    # Greedy loop\n    max_est = np.inf\n    for _ in range(Nmax if Nmax is not None else 10**9):\n        # Evaluate estimator on training set\n        est_vals = []\n        for mu in Xi:\n            _, est = reduced_solution_and_estimator(mu, V)\n            est_vals.append(est)\n        est_vals = np.array(est_vals)\n        max_idx = int(np.argmax(est_vals))\n        max_est = float(est_vals[max_idx])\n        if max_est <= eps_tol:\n            break\n        # Select new parameter and add its snapshot\n        mu_star = Xi[max_idx]\n        snap = truth_snapshot(mu_star, kappa, b, gamma)\n        new_vec = K_orthonormalize(V, snap, kappa)\n        if new_vec is None:\n            # If collinear, perturb by adding another point with second largest estimator\n            # or break if no expansion possible.\n            # Try to find another candidate\n            est_order = np.argsort(-est_vals)\n            added = False\n            for idx in est_order:\n                mu_try = Xi[idx]\n                snap_try = truth_snapshot(mu_try, kappa, b, gamma)\n                new_vec_try = K_orthonormalize(V, snap_try, kappa)\n                if new_vec_try is not None:\n                    V = np.column_stack([V, new_vec_try])\n                    added = True\n                    break\n            if not added:\n                break\n        else:\n            V = np.column_stack([V, new_vec])\n        if V.shape[1] >= (Nmax if Nmax is not None else V.shape[1]):\n            break\n    return V, max_est\n\ndef estimator_on_grid(mu_grid, V, kappa, b, gamma):\n    \"\"\"Compute estimator values on a grid of mu for a fixed reduced basis V.\"\"\"\n    Nh = kappa.size\n    est = np.zeros_like(mu_grid, dtype=float)\n    # Precompute reduced matrices\n    if V.shape[1] == 0:\n        # Zero reduced space: residual is b\n        for i, mu in enumerate(mu_grid):\n            inv_diag = 1.0 / (mu * kappa + gamma)\n            est2 = float(np.dot(b * inv_diag, b))\n            est[i] = np.sqrt(max(est2, 0.0))\n        return est\n    G = V.T @ V\n    Bred = V.T @ b\n    I_N = np.eye(V.shape[1])\n    for i, mu in enumerate(mu_grid):\n        AN = mu * I_N + gamma * G\n        alpha = np.linalg.solve(AN, Bred)\n        uN = V @ alpha\n        AuN = mu * (kappa * uN) + gamma * uN\n        r = b - AuN\n        inv_diag = 1.0 / (mu * kappa + gamma)\n        est2 = float(np.dot(r * inv_diag, r))\n        est[i] = np.sqrt(max(est2, 0.0))\n    return est\n\ndef adaptive_refine_Xi(Xi, R, K_add, theta, Nmax, eps_tol, mu_min, mu_max, Gsize, kappa, b, gamma):\n    \"\"\"\n    Adaptive refinement of Xi driven by estimator map. For R rounds:\n    - Build reduced basis with greedy selection on current Xi.\n    - Evaluate estimator on dense uniform grid of size Gsize.\n    - Compute interval-wise local maxima and add up to K_add midpoints where local max >= theta * global max.\n    Returns the refined Xi and the final reduced basis V built on the last round.\n    \"\"\"\n    Xi = np.array(sorted(set(Xi)), dtype=float)\n    for _ in range(R):\n        # Build reduced basis on current Xi\n        V, _ = build_reduced_basis_greedy(Xi, Nmax, eps_tol, kappa, b, gamma)\n        # Probe grid\n        mu_grid = np.linspace(mu_min, mu_max, Gsize)\n        est_grid = estimator_on_grid(mu_grid, V, kappa, b, gamma)\n        # Compute interval indicators\n        Xi_sorted = np.array(sorted(Xi))\n        intervals = list(zip(Xi_sorted[:-1], Xi_sorted[1:]))\n        local_E = []\n        interval_midpoints = []\n        for (left, right) in intervals:\n            # Points in (left, right]; include right to avoid empty last bin\n            mask = (mu_grid > left) & (mu_grid <= right)\n            if np.any(mask):\n                Ej = float(np.max(est_grid[mask]))\n            else:\n                mid = 0.5 * (left + right)\n                mid_est = float(estimator_on_grid(np.array([mid]), V, kappa, b, gamma)[0])\n                Ej = mid_est\n            local_E.append(Ej)\n            interval_midpoints.append(0.5 * (left + right))\n        local_E = np.array(local_E, dtype=float)\n        if local_E.size == 0:\n            break\n        Emax = float(np.max(local_E))\n        # Select intervals with local_E >= theta * Emax\n        candidates = [(idx, local_E[idx], interval_midpoints[idx]) for idx in range(len(local_E)) if local_E[idx] >= theta * Emax]\n        # Sort by descending local_E\n        candidates.sort(key=lambda t: -t[1])\n        # Add up to K_add midpoints\n        additions = []\n        for idx, Ej, mid in candidates[:K_add]:\n            additions.append(mid)\n        if len(additions) == 0:\n            # If no interval meets threshold, add the midpoint of the global maximum interval\n            max_idx = int(np.argmax(local_E))\n            additions = [interval_midpoints[max_idx]]\n        # Update Xi\n        Xi = np.array(sorted(set(list(Xi) + additions)), dtype=float)\n    # Final basis built on last Xi\n    V, _ = build_reduced_basis_greedy(Xi, Nmax, eps_tol, kappa, b, gamma)\n    return Xi, V\n\ndef run_case(Nh, gamma, mu_min, mu_max, M0, Gsize, Nmax, eps_tol, R, K_add, theta):\n    # Build operators\n    kappa, b, gamma = sine_spectral_operators(Nh, gamma)\n    # Initial training set Xi: uniform grid including endpoints, size M0\n    Xi = np.linspace(mu_min, mu_max, M0)\n    # Adaptive refinement\n    Xi_refined, V_final = adaptive_refine_Xi(Xi, R, K_add, theta, Nmax, eps_tol, mu_min, mu_max, Gsize, kappa, b, gamma)\n    # Rebuild final reduced basis with greedy on refined Xi (already done in adaptive_refine_Xi, but ensure it's consistent)\n    V_final, _ = build_reduced_basis_greedy(Xi_refined, Nmax, eps_tol, kappa, b, gamma)\n    # Evaluate final worst-case estimator on probe grid\n    mu_grid = np.linspace(mu_min, mu_max, Gsize)\n    est_grid = estimator_on_grid(mu_grid, V_final, kappa, b, gamma)\n    return float(np.max(est_grid))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Nh=80, gamma=1, P=[0.1,1.0], M0=5, G=400, Nmax=6, tol=1e-6, R=2, K_add=2, theta=0.6\n        (80, 1.0, 0.1, 1.0, 5, 400, 6, 1e-6, 2, 2, 0.6),\n        # Case B: Nh=80, gamma=1, P=[0.5,1.0], M0=4, G=300, Nmax=5, tol=1e-8, R=2, K_add=1, theta=0.7\n        (80, 1.0, 0.5, 1.0, 4, 300, 5, 1e-8, 2, 1, 0.7),\n        # Case C: Nh=80, gamma=1, P=[0.05,0.2], M0=6, G=500, Nmax=8, tol=1e-7, R=3, K_add=2, theta=0.5\n        (80, 1.0, 0.05, 0.2, 6, 500, 8, 1e-7, 3, 2, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        Nh, gamma, mu_min, mu_max, M0, Gsize, Nmax, eps_tol, R, K_add, theta = case\n        res = run_case(Nh, gamma, mu_min, mu_max, M0, Gsize, Nmax, eps_tol, R, K_add, theta)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}