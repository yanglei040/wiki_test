{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we first need to understand why standard Algebraic Multigrid (AMG) methods often fall short when applied to high-order discretizations. This foundational exercise introduces the concept of operator complexity, a critical metric for evaluating the efficiency of a multigrid hierarchy. By working through this idealized model, you will derive how the choice of polynomial degree $p$ and the coarsening strategy directly impact the computational cost of the coarse-grid operators, revealing a fundamental challenge in designing AMG for high-order systems. ",
            "id": "3363047",
            "problem": "Consider a scalar, second-order, uniformly elliptic diffusion operator posed on a bounded domain in spatial dimension $d \\in \\{2,3\\}$ and discretized by the interior penalty Discontinuous Galerkin method using a tensor-product spectral basis of polynomial degree $p \\geq 1$ on a quasiuniform mesh with $N_{e}$ elements. Let the number of local degrees of freedom per element be the dimension of the tensor-product polynomial space, namely $n_{p} = (p+1)^{d}$. The fine-grid stiffness matrix $A_{0}$ arising from this discretization is block-sparse: assume each element couples to itself and, on average, to $\\alpha$ neighboring elements (so that there are $(\\alpha+1)$ dense $n_{p} \\times n_{p}$ blocks per element in the sparsity pattern). In an Algebraic Multigrid (AMG) method (Algebraic Multigrid (AMG)) using aggregation-based coarsening, consider forming coarse aggregates by grouping $k \\geq 1$ contiguous elements per aggregate, and define the coarse space on each aggregate to be piecewise constants (one coarse unknown per aggregate). Assume $N_{e}$ is divisible by $k$ and that the aggregate-level adjacency mirrors the fine-level average in the sense that the aggregate graph has the same average neighbor count $\\alpha$. Using only these structural assumptions and definitions, carry out the following:\n\n- Derive the number of coarse unknowns $N_{c}$ produced by aggregating $k$ elements per aggregate.\n- Starting from the definition of operator complexity $C_{\\mathrm{op}} = \\dfrac{\\mathrm{nnz}(A_{0}) + \\mathrm{nnz}(A_{1})}{\\mathrm{nnz}(A_{0})}$, estimate $\\mathrm{nnz}(A_{0})$ and $\\mathrm{nnz}(A_{1})$ in terms of $N_{e}$, $p$, $k$, $d$, and $\\alpha$, and simplify $C_{\\mathrm{op}}$ to a closed-form expression that makes explicit how the choice of $k$ and $p$ affects the operator complexity.\n\nExpress your final answer as a row matrix $\\left(N_{c},\\, C_{\\mathrm{op}}\\right)$, written as a single analytical expression. No numerical evaluation is required.",
            "solution": "The problem statement has been validated and is determined to be a well-posed, scientifically grounded problem in the field of numerical analysis. It provides a clear and consistent set of definitions and assumptions sufficient for the requested derivations.\n\nThe task is to derive expressions for the number of coarse unknowns, $N_c$, and the operator complexity, $C_{\\mathrm{op}}$, based on the provided model of an aggregation-based Algebraic Multigrid (AMG) method for a Discontinuous Galerkin (DG) discretization.\n\nFirst, we derive the number of coarse unknowns, $N_c$. The problem specifies that the fine grid has $N_e$ elements. Coarsening is performed by creating aggregates, each containing $k \\geq 1$ elements. The coarse space is defined as piecewise constant on these aggregates, meaning there is one coarse unknown for each aggregate. The problem assumes that $N_e$ is divisible by $k$, ensuring that the elements can be partitioned perfectly into aggregates. Therefore, the total number of aggregates, which is equal to the number of coarse unknowns $N_c$, is given by:\n$$\nN_c = \\frac{N_e}{k}\n$$\n\nNext, we estimate the number of non-zero entries in the fine-grid stiffness matrix, $\\mathrm{nnz}(A_0)$. The discretization uses a spectral basis of polynomial degree $p$ on a $d$-dimensional domain, leading to $n_p = (p+1)^d$ local degrees of freedom (DOFs) per element. The stiffness matrix $A_0$ is block-sparse. According to the problem statement, \"each element couples to itself and, on average, to $\\alpha$ neighboring elements.\" This implies that in the block-matrix structure of $A_0$, each of the $N_e$ block-rows contains, on average, $(\\alpha+1)$ non-zero blocks: one block for the element's self-coupling (the diagonal block) and $\\alpha$ blocks for couplings to its neighbors. The problem further states that these are \"dense $n_p \\times n_p$ blocks\". A dense $n_p \\times n_p$ block contains $n_p^2$ non-zero entries. The total number of non-zero entries in $A_0$ can thus be estimated by multiplying the number of elements (which corresponds to the number of block-rows) by the average number of non-zero blocks per block-row and the number of non-zeros per block.\n$$\n\\mathrm{nnz}(A_0) \\approx N_e (\\alpha+1) n_p^2\n$$\nSubstituting the given definition $n_p = (p+1)^d$, we get:\n$$\n\\mathrm{nnz}(A_0) \\approx N_e (\\alpha+1) \\left((p+1)^d\\right)^2 = N_e (\\alpha+1) (p+1)^{2d}\n$$\n\nThen, we estimate the number of non-zero entries in the coarse-grid operator matrix, $\\mathrm{nnz}(A_1)$. The coarse operator $A_1$ has dimensions $N_c \\times N_c$. The problem provides a key simplifying assumption regarding its structure: \"the aggregate-level adjacency mirrors the fine-level average in the sense that the aggregate graph has the same average neighbor count $\\alpha$.\" This assumption implies that the sparsity pattern of $A_1$ is analogous to that of a low-order discretization on the graph of aggregates. Each row of $A_1$ will have a non-zero diagonal entry (representing the self-interaction of an aggregate) and, on average, $\\alpha$ non-zero off-diagonal entries (representing interactions with neighboring aggregates). Thus, each row of $A_1$ has an average of $(\\alpha+1)$ non-zero entries. The total number of non-zeros in $A_1$ is the product of the number of rows ($N_c$) and the average number of non-zeros per row.\n$$\n\\mathrm{nnz}(A_1) \\approx N_c (\\alpha+1)\n$$\nSubstituting the expression for $N_c$:\n$$\n\\mathrm{nnz}(A_1) \\approx \\frac{N_e}{k} (\\alpha+1)\n$$\n\nFinally, we derive the operator complexity, $C_{\\mathrm{op}}$. The definition provided is:\n$$\nC_{\\mathrm{op}} = \\frac{\\mathrm{nnz}(A_0) + \\mathrm{nnz}(A_1)}{\\mathrm{nnz}(A_0)}\n$$\nWe can rewrite this expression as:\n$$\nC_{\\mathrm{op}} = 1 + \\frac{\\mathrm{nnz}(A_1)}{\\mathrm{nnz}(A_0)}\n$$\nSubstituting the derived estimates for $\\mathrm{nnz}(A_0)$ and $\\mathrm{nnz}(A_1)$:\n$$\nC_{\\mathrm{op}} \\approx 1 + \\frac{\\frac{N_e}{k} (\\alpha+1)}{N_e (\\alpha+1) (p+1)^{2d}}\n$$\nThe terms $N_e$ and $(\\alpha+1)$ cancel out, leading to a simplified expression that depends only on the aggregation size $k$, the polynomial degree $p$, and the spatial dimension $d$.\n$$\nC_{\\mathrm{op}} = 1 + \\frac{\\frac{1}{k}}{(p+1)^{2d}} = 1 + \\frac{1}{k(p+1)^{2d}}\n$$\nThis result makes explicit the relationship between the algorithmic choice of aggregation size ($k$) and the discretization choice of polynomial degree ($p$) in determining the multigrid operator complexity.\n\nThe two final expressions are $N_c = \\frac{N_e}{k}$ and $C_{\\mathrm{op}} = 1 + \\frac{1}{k(p+1)^{2d}}$. They are presented as a row matrix as requested.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{N_e}{k} & 1 + \\frac{1}{k(p+1)^{2d}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having established the need for specialized strategies, we now turn to a core design principle: exploiting the inherent structure of the problem. Many high-order spectral element methods are built upon tensor-product bases, which endows the resulting linear system with a rich structure that generic AMG approaches would ignore. This computational practice guides you through a comparison of two prolongation operators—one that respects the tensor-product structure and one that is purely algebraic and \"naive\"—allowing you to quantify the dramatic improvement in convergence that comes from a structure-aware design. ",
            "id": "3362994",
            "problem": "Consider the Poisson problem on the reference square with homogeneous Dirichlet boundary conditions, stated in its weak form as: find a scalar field $u$ such that for all smooth test functions $v$ with $v=0$ on the boundary,\n$$\n\\int_{-1}^{1}\\int_{-1}^{1} \\nabla u \\cdot \\nabla v \\, dx\\,dy = \\int_{-1}^{1}\\int_{-1}^{1} f \\, v \\, dx\\,dy,\n$$\nwhere $f$ is a given source. In a spectral element discretization on a single element, the approximation space is the tensor-product space of Lagrange polynomials on Gauss-Lobatto-Legendre (GLL) nodes of polynomial degree $p$, and the bilinear form is discretized with GLL quadrature. In this setting, the stiffness operator $A$ with homogeneous Dirichlet boundary conditions restricted to interior nodes can be expressed by starting from the one-dimensional differentiation matrix and quadrature weights and using Kronecker products.\n\nYour task is to implement and compare two algebraic multigrid (AMG) prolongation strategies for a two-grid method that coarsens the polynomial degree from a fine degree $p_{\\mathrm{f}}$ to a coarse degree $p_{\\mathrm{c}} = \\max(2, \\lfloor p_{\\mathrm{f}}/2 \\rfloor)$. The two prolongations differ in whether they preserve the tensor-product coupling induced by the elemental degree-of-freedom ordering:\n\n1. Tensor-preserving prolongation: Construct a one-dimensional interpolation matrix $P_1$ mapping interior coarse GLL nodes to interior fine GLL nodes by evaluating coarse Lagrange basis polynomials (defined on the full set of coarse GLL nodes including endpoints) at fine interior node locations, and then define the two-dimensional prolongation as $P_{\\mathrm{tensor}} = P_1 \\otimes P_1$.\n\n2. Naive pointwise prolongation: Independently of tensor structure, for each fine interior node $(x_i,y_j)$ select the single nearest coarse interior node $(\\hat{x}_\\alpha,\\hat{y}_\\beta)$ in the Euclidean metric and inject with weight $1$ into that coarse degree of freedom (all other weights $0$), producing a sparse interpolation matrix $P_{\\mathrm{naive}}$ with exactly one nonzero per row.\n\nUse a two-grid method with one pre-smoothing and one post-smoothing step, each given by weighted Jacobi. Let $A$ be the fine-grid operator, let $D$ denote the diagonal of $A$, and define the weighted Jacobi smoother as $S_\\omega = I - \\omega D^{-1} A$. Choose the weight $\\omega$ by minimizing the spectral radius of the Jacobi iteration operator based on the symmetric similarity $D^{-1/2} A D^{-1/2}$ as\n$$\n\\omega^\\star = \\frac{2}{\\lambda_{\\min}(D^{-1/2} A D^{-1/2}) + \\lambda_{\\max}(D^{-1/2} A D^{-1/2})}.\n$$\nLet the restriction be the transpose of the prolongation, $R = P^\\top$, and let the coarse operator be the Galerkin product $A_{\\mathrm{c}} = R A P$. The coarse-grid correction operator is $I - P A_{\\mathrm{c}}^{-1} R A$. The two-grid error-propagation operator is then\n$$\nE = S_\\omega \\left(I - P A_{\\mathrm{c}}^{-1} R A\\right) S_\\omega.\n$$\nThe two-grid convergence factor is the spectral radius $\\rho(E)$.\n\nFundamental base you must use:\n- The spectral element stiffness matrix on the reference interval based on GLL quadrature and Lagrange basis satisfies the one-dimensional forms $M = \\mathrm{diag}(w)$ and $S = D^\\top \\mathrm{diag}(w) D$, where $w$ are the GLL quadrature weights and $D$ is the differentiation matrix with entries $D_{ij} = \\ell_j'(x_i)$ for Lagrange basis $\\ell_j$ at GLL nodes $x_j$.\n- In two dimensions with homogeneous Dirichlet boundary conditions enforced by eliminating boundary nodes, the operator is the Kronecker sum $A = S \\otimes M + M \\otimes S$.\n- The Gauss-Lobatto-Legendre nodes of degree $p$ are the endpoints $\\{-1,1\\}$ and the roots of the derivative of the Legendre polynomial of degree $p$, and the GLL quadrature weights are $w_i = \\frac{2}{p(p+1)} \\frac{1}{[P_p(x_i)]^2}$.\n- The barycentric Lagrange formula yields stable evaluation of Lagrange polynomials and their derivatives on arbitrary nodes, and the differentiation matrix entries are $D_{ij} = \\frac{\\alpha_j}{\\alpha_i} \\frac{1}{x_i - x_j}$ for $i \\neq j$ and $D_{ii} = -\\sum_{m \\neq i} D_{im}$, where $\\alpha_j$ are barycentric weights $\\alpha_j = \\prod_{m \\neq j} (x_j - x_m)^{-1}$.\n\nImplement the following steps in a single program:\n- For each polynomial degree $p_{\\mathrm{f}}$ in a specified test suite, build the one-dimensional GLL nodes and weights for degree $p_{\\mathrm{f}}$ and degree $p_{\\mathrm{c}}$, assemble the fine-grid two-dimensional stiffness matrix $A$ restricted to interior nodes, and construct both $P_{\\mathrm{tensor}}$ and $P_{\\mathrm{naive}}$ as defined above.\n- For each prolongation, build the two-grid error-propagation matrix $E$ with one pre- and one post-weighted Jacobi smoothing step using the optimally chosen $\\omega^\\star$ defined above, and compute $\\rho(E)$.\n- Quantify the benefit of preserving tensor-product coupling by reporting, for each $p_{\\mathrm{f}}$, the ratio $r = \\rho(E_{\\mathrm{naive}})/\\rho(E_{\\mathrm{tensor}})$.\n\nTest suite:\n- Use the fine polynomial degrees $p_{\\mathrm{f}} \\in \\{4,6,8,12\\}$. For each, set $p_{\\mathrm{c}} = \\max(2, \\lfloor p_{\\mathrm{f}}/2 \\rfloor)$ as described.\n\nAnswer specification:\n- For each $p_{\\mathrm{f}}$ in the test suite, compute the ratio $r$ as a floating-point number rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the ratios in the order of the test suite as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$.\n- No physical units are involved.",
            "solution": "The problem requires the implementation and comparison of two algebraic multigrid (AMG) prolongation strategies for a spectral element discretization of the Poisson problem. The comparison is based on the convergence factor of a two-grid method. The analysis will be performed for a set of fine-grid polynomial degrees $p_{\\mathrm{f}}$.\n\nFirst, we establish the mathematical and computational framework as specified.\n\nThe problem considers the Poisson equation on the reference square $\\Omega = [-1,1]^2$ with homogeneous Dirichlet boundary conditions. The weak form is: find $u \\in H_0^1(\\Omega)$ such that\n$$\n\\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, d\\Omega = \\int_{\\Omega} f v \\, d\\Omega \\quad \\forall v \\in H_0^1(\\Omega)\n$$\nThis is discretized using a single spectral element with basis functions constructed from Lagrange polynomials on Gauss-Lobatto-Legendre (GLL) nodes. For a polynomial degree $p$, there are $p+1$ GLL nodes $x_i$ on $[-1,1]$, which are the roots of $(1-x^2)P_p'(x)$, where $P_p(x)$ is the Legendre polynomial of degree $p$. The corresponding GLL quadrature weights $w_i$ are given by\n$$\nw_i = \\frac{2}{p(p+1) [P_p(x_i)]^2}\n$$\nThe one-dimensional differentiation matrix $D$ has entries $D_{ij} = \\ell_j'(x_i)$, where $\\ell_j(x)$ is the Lagrange polynomial for node $x_j$. The problem provides a formula based on barycentric weights $\\alpha_j = (\\prod_{k \\neq j} (x_j - x_k))^{-1}$:\n$$\nD_{ij} = \\frac{\\alpha_j}{\\alpha_i (x_i - x_j)} \\quad \\text{for } i \\neq j\n$$\nFor the derivative of a constant function to be zero, the row sums of the differentiation matrix must be zero, i.e., $\\sum_j D_{ij} = 0$. This implies the diagonal entries must be $D_{ii} = -\\sum_{j \\neq i} D_{ij}$.\n\nUsing GLL quadrature, the one-dimensional mass and stiffness matrices are $M_{\\mathrm{1D}} = \\mathrm{diag}(w)$ and $S_{\\mathrm{1D}} = D^\\top M_{\\mathrm{1D}} D$, respectively, where $w$ is the vector of GLL weights. These are matrices of size $(p+1) \\times (p+1)$.\n\nTo enforce homogeneous Dirichlet boundary conditions, we restrict the problem to the basis functions associated with interior nodes. This corresponds to taking the submatrices for interior nodes, indexed from $1$ to $p-1$. Let $M$ and $S$ be these interior matrices of size $(p-1) \\times (p-1)$. The two-dimensional stiffness operator $A$ for the interior degrees of freedom is then given by the Kronecker sum:\n$$\nA = S \\otimes M + M \\otimes S\n$$\nThis fine-grid operator $A$ is constructed for the fine polynomial degree $p_{\\mathrm{f}}$.\n\nThe two-grid method requires a prolongation operator $P$ that maps from a coarse grid to the fine grid. Here, the coarse grid corresponds to a lower polynomial degree $p_{\\mathrm{c}} = \\max(2, \\lfloor p_{\\mathrm{f}}/2 \\rfloor)$. The prolongation maps from the space of $(p_c-1)^2$ coarse interior degrees of freedom to the space of $(p_f-1)^2$ fine interior degrees of freedom.\n\nTwo prolongation strategies are compared:\n\n1.  **Tensor-preserving prolongation ($P_{\\mathrm{tensor}}$)**: This operator is built from a one-dimensional interpolant $P_1$ and preserves the tensor-product structure of the grid. $P_1$ is a matrix of size $(p_{\\mathrm{f}}-1) \\times (p_{\\mathrm{c}}-1)$ whose entries are given by evaluating the coarse-grid Lagrange basis functions (associated with interior coarse nodes) at the fine-grid interior node locations. Specifically, $(P_1)_{ij} = \\ell_j^{(c)}(x_i^{(f)})$, where $\\ell_j^{(c)}$ is the $j$-th interior Lagrange basis polynomial on the coarse GLL nodes and $x_i^{(f)}$ is the $i$-th interior fine GLL node. The two-dimensional prolongator is then $P_{\\mathrm{tensor}} = P_1 \\otimes P_1$.\n\n2.  **Naive pointwise prolongation ($P_{\\mathrm{naive}}$)**: This is a purely algebraic approach that ignores the grid's tensor structure. For each fine interior node $(x_i, y_j)$, we find the single nearest coarse interior node $(\\hat{x}_\\alpha, \\hat{y}_\\beta)$ in the Euclidean metric. The corresponding row in the prolongation matrix $P_{\\mathrm{naive}}$ contains a single non-zero entry of $1$ in the column corresponding to the coarse degree of freedom $(\\alpha, \\beta)$.\n\nThe two-grid method has one pre-smoothing and one post-smoothing step using a weighted Jacobi smoother $S_\\omega = I - \\omega D_A^{-1} A$, where $D_A$ is the diagonal of $A$. The optimal weight $\\omega^\\star$ is chosen to minimize the spectral radius of the Jacobi iteration operator:\n$$\n\\omega^\\star = \\frac{2}{\\lambda_{\\min}(D_A^{-1/2} A D_A^{-1/2}) + \\lambda_{\\max}(D_A^{-1/2} A D_A^{-1/2})}\n$$\nThe restriction operator is the transpose of the prolongation, $R = P^\\top$, and the coarse-grid operator is formed by the Galerkin product $A_{\\mathrm{c}} = R A P$. The two-grid error-propagation operator is given by:\n$$\nE = S_{\\omega^\\star} \\left(I - P A_{\\mathrm{c}}^{-1} R A\\right) S_{\\omega^\\star}\n$$\nThe effectiveness of the method is measured by the convergence factor, which is the spectral radius of this operator, $\\rho(E)$.\n\nThe core task is to compute, for each $p_{\\mathrm{f}}$ in the test suite $\\{4, 6, 8, 12\\}$, the ratio $r = \\rho(E_{\\mathrm{naive}})/\\rho(E_{\\mathrm{tensor}})$, where $E_{\\mathrm{naive}}$ and $E_{\\mathrm{tensor}}$ are the error-propagation operators constructed using $P_{\\mathrm{naive}}$ and $P_{\\mathrm{tensor}}$, respectively. This ratio quantifies the performance gain from using a prolongation operator that preserves the underlying structure of the discretization.\n\nThe computational procedure for each $p_{\\mathrm{f}}$ is as follows:\n1.  Determine $p_{\\mathrm{c}} = \\max(2, \\lfloor p_{\\mathrm{f}}/2 \\rfloor)$.\n2.  Compute GLL nodes and weights for both degrees $p_{\\mathrm{f}}$ and $p_{\\mathrm{c}}$.\n3.  For degree $p_{\\mathrm{f}}$, assemble the 1D interior matrices $M$ and $S$, then form the 2D fine-grid stiffness matrix $A$.\n4.  Construct the two prolongation matrices, $P_{\\mathrm{tensor}}$ and $P_{\\mathrm{naive}}$.\n5.  Calculate the optimal Jacobi weight $\\omega^\\star$ from the eigenvalues of the scaled matrix $D_A^{-1/2} A D_A^{-1/2}$.\n6.  For each prolongation matrix $P$, assemble the corresponding error-propagation matrix $E$ and compute its spectral radius $\\rho(E)$.\n7.  Compute the ratio $r$ of the spectral radii.\n\nThis procedure is systematically executed for all specified values of $p_{\\mathrm{f}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_jacobi, legendre\n\ndef get_gll_nodes_weights(p):\n    \"\"\"\n    Computes Gauss-Lobatto-Legendre (GLL) nodes and weights for a given polynomial degree p.\n    \"\"\"\n    if p == 0:\n        return np.array([0.0]), np.array([2.0])\n    if p == 1:\n        return np.array([-1.0, 1.0]), np.array([1.0, 1.0])\n    \n    # Interior nodes are roots of P_p'(x) which are roots of Jacobi poly P_{p-1}^{(1,1)}(x)\n    interior_nodes, _ = roots_jacobi(p - 1, 1, 1)\n    nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n    \n    # Weights using the formula w_i = 2 / (p(p+1) * [P_p(x_i)]^2)\n    poly = legendre(p)\n    weights = 2.0 / (p * (p + 1) * (poly(nodes))**2)\n    \n    return nodes, weights\n\ndef get_diff_matrix(nodes):\n    \"\"\"\n    Computes the 1D differentiation matrix for a given set of nodes.\n    \"\"\"\n    n = len(nodes)\n    D = np.zeros((n, n))\n    \n    # Using the formula D_ij = (alpha_j/alpha_i) / (x_i-x_j)\n    # This is equivalent to c_i / (c_j * (x_i-x_j)) where c_k = prod_{m!=k}(x_k-x_m).\n    # This avoids calculating alpha_k which can be very small or large.\n    c = np.ones(n)\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                c[i] *= (nodes[i] - nodes[j])\n\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                D[i, j] = (c[i] / c[j]) / (nodes[i] - nodes[j])\n    \n    # Diagonal entries from row-sum-to-zero property (D * 1 = 0)\n    for i in range(n):\n        D[i, i] = -np.sum(D[i, :])\n        \n    return D\n\ndef barycentric_lagrange_eval(eval_points, basis_nodes):\n    \"\"\"\n    Evaluates Lagrange basis polynomials defined on `basis_nodes` at `eval_points`.\n    Returns matrix L where L[i, j] = l_j(eval_points[i]).\n    \"\"\"\n    n_eval = len(eval_points)\n    n_basis = len(basis_nodes)\n    L = np.zeros((n_eval, n_basis))\n\n    # Barycentric weights for basis_nodes, alpha_j = 1 / product(x_j - x_k)\n    bary_weights = np.ones(n_basis)\n    for j in range(n_basis):\n        for k in range(n_basis):\n            if j != k:\n                bary_weights[j] /= (basis_nodes[j] - basis_nodes[k])\n\n    for i, x in enumerate(eval_points):\n        # Check if x is one of the basis nodes\n        match_indices = np.where(np.isclose(x, basis_nodes))\n        if len(match_indices[0]) > 0:\n            L[i, match_indices[0][0]] = 1.0\n            continue\n        \n        # Barycentric formula: l_j(x) = (w_j / (x-x_j)) / sum_k(w_k / (x-x_k))\n        terms = bary_weights / (x - basis_nodes)\n        L[i, :] = terms / np.sum(terms)\n        \n    return L\n\ndef compute_convergence_ratio(p_f):\n    \"\"\"\n    Computes the ratio of convergence factors for naive vs. tensor-product prolongation.\n    \"\"\"\n    p_c = max(2, p_f // 2)\n\n    # 1. GLL data for fine and coarse grids\n    x_f, w_f = get_gll_nodes_weights(p_f)\n    x_c, _ = get_gll_nodes_weights(p_c)\n    \n    x_f_int, x_c_int = x_f[1:-1], x_c[1:-1]\n    n_f_int_1d, n_c_int_1d = len(x_f_int), len(x_c_int)\n\n    # 2. 1D fine-grid matrices (interior)\n    D_f_full = get_diff_matrix(x_f)\n    M_f_full = np.diag(w_f)\n    S_f_full = D_f_full.T @ M_f_full @ D_f_full\n    \n    M_f_int = M_f_full[1:-1, 1:-1]\n    S_f_int = S_f_full[1:-1, 1:-1]\n\n    # 3. 2D fine-grid stiffness matrix A\n    A = np.kron(S_f_int, M_f_int) + np.kron(M_f_int, S_f_int)\n    n_A = A.shape[0]\n\n    # 4a. Tensor-preserving prolongation P_tensor\n    # P1 maps from coarse interior basis to fine interior nodes\n    L_eval = barycentric_lagrange_eval(x_f_int, x_c)\n    P1 = L_eval[:, 1:-1]  # Select columns for interior coarse basis functions\n    P_tensor = np.kron(P1, P1)\n\n    # 4b. Naive pointwise prolongation P_naive\n    P_naive = np.zeros((n_f_int_1d**2, n_c_int_1d**2))\n    \n    fine_nodes_2d = np.array(np.meshgrid(x_f_int, x_f_int)).T.reshape(-1, 2)\n    coarse_nodes_2d = np.array(np.meshgrid(x_c_int, x_c_int)).T.reshape(-1, 2)\n    \n    for i, f_node in enumerate(fine_nodes_2d):\n        dists_sq = np.sum((coarse_nodes_2d - f_node)**2, axis=1)\n        j_min = np.argmin(dists_sq)\n        P_naive[i, j_min] = 1.0\n\n    # 5. Two-grid analysis\n    # Optimal Jacobi weight (same for both P)\n    A_diag = np.diag(A)\n    # The matrix A is SPD, so its diagonal is positive.\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(A_diag))\n    A_scaled = D_inv_sqrt @ A @ D_inv_sqrt\n    eigvals_scaled = np.linalg.eigh(A_scaled)[0]\n    omega_opt = 2.0 / (np.min(eigvals_scaled) + np.max(eigvals_scaled))\n    \n    # Smoother matrix\n    S_omega = np.eye(n_A) - omega_opt * np.diag(1.0 / A_diag) @ A\n    \n    rhos = {}\n    for name, P in [(\"tensor\", P_tensor), (\"naive\", P_naive)]:\n        R = P.T\n        A_c = R @ A @ P\n        \n        # Check for singularity of coarse operator\n        if np.linalg.cond(A_c) > 1/np.finfo(A_c.dtype).eps:\n            # For P_naive, A_c can be singular if some coarse nodes are not targeted\n            # by any fine node, making columns of P zero. Use pseudoinverse.\n            A_c_inv = np.linalg.pinv(A_c)\n        else:\n            A_c_inv = np.linalg.inv(A_c)\n\n        # Two-grid error propagation operator\n        CGC = np.eye(n_A) - P @ A_c_inv @ R @ A\n        E = S_omega @ CGC @ S_omega\n        \n        # Spectral radius\n        rho = np.max(np.abs(np.linalg.eigvals(E)))\n        rhos[name] = rho\n        \n    return rhos[\"naive\"] / rhos[\"tensor\"]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [4, 6, 8, 12]\n\n    results = []\n    for p_f in test_cases:\n        ratio = compute_convergence_ratio(p_f)\n        results.append(round(ratio, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Even with structure-aware methods, it is crucial to have tools to diagnose and remedy poor performance, especially when dealing with very high polynomial degrees. This final practice provides a powerful diagnostic framework based on spectral analysis. You will implement a numerical experiment to identify which specific low-energy eigenmodes of the system are poorly approximated by a given coarse space—a common failure mode in AMG for high-order systems. More importantly, you will see how to fix this deficiency by \"enriching\" the coarse space with functions that explicitly capture these problematic modes. ",
            "id": "3363022",
            "problem": "You are asked to design and implement a diagnostic for algebraic multigrid applied to very high polynomial degree spectral and discontinuous Galerkin discretizations, focusing on the failure mode where the coarse space does not capture certain spectral components. The goal is to identify which generalized eigenmodes of the element-local operator are poorly represented by a low-degree coarse space and to demonstrate that enriching the coarse space with a small number of interior bubble modes reduces this deficiency.\n\nConsider the one-dimensional Poisson operator on the reference element $\\left[-1,1\\right]$ with homogeneous Dirichlet boundary conditions at $x=-1$ and $x=1$. Use a Spectral Element Method with Legendre–Gauss–Lobatto (LGL) nodes of polynomial degree $p$, with $p\\geq 12$. Let $N=p$ denote the number of subintervals in the polynomial representation, resulting in $N+1$ LGL nodes $\\{x_i\\}_{i=0}^{N}$ and corresponding quadrature weights $\\{w_i\\}_{i=0}^{N}$. Use the standard Lagrange cardinal basis at these nodes. Construct the mass matrix $M$ and stiffness matrix $K$ by LGL quadrature:\n- $M = \\operatorname{diag}(w_0,\\dots,w_N)$,\n- $K = D^\\top W D$, where $W=\\operatorname{diag}(w_0,\\dots,w_N)$ and $D$ is the differentiation matrix $D_{ij} = \\phi_j'(x_i)$ with $\\{\\phi_j\\}$ the Lagrange basis polynomials.\n\nImpose homogeneous Dirichlet boundary conditions by restricting to the interior degrees of freedom corresponding to nodes $i=1,\\dots,N-1$, producing reduced matrices $M_{\\mathrm{int}}$ and $K_{\\mathrm{int}}$ of size $(N-1)\\times(N-1)$. Consider the generalized eigenproblem\n$$\nK_{\\mathrm{int}} v = \\lambda M_{\\mathrm{int}} v,\n$$\nand let $\\{(\\lambda_k,v_k)\\}_{k=1}^{N-1}$ denote its eigenpairs ordered by non-decreasing eigenvalue, with the eigenvectors $M_{\\mathrm{int}}$-orthonormal.\n\nDefine a low-order coarse space by $p$-coarsening to polynomial degree $r$ with $1<r<p$. Let the coarse grid use LGL nodes of degree $r$ on the same reference element with interior nodes indexed by $j=1,\\dots,r-1$. Define the prolongation matrix $P\\in\\mathbb{R}^{(N-1)\\times(r-1)}$ by evaluating each coarse interior Lagrange basis function $\\ell_j$ at the fine interior nodes $x_i$, i.e., $P_{ij}=\\ell_j(x_i)$ for $i=1,\\dots,N-1$ and $j=1,\\dots,r-1$. For any fine-space vector $u\\in\\mathbb{R}^{N-1}$, define the $M_{\\mathrm{int}}$-orthogonal projection onto the coarse space $\\mathcal{V}_c=\\operatorname{range}(P)$ by\n$$\n\\Pi_c u = P\\left(P^\\top M_{\\mathrm{int}} P\\right)^{-1} P^\\top M_{\\mathrm{int}} u.\n$$\nFor a given $m$ with $1\\le m\\le N-1$, define the coarse-space miss set among the lowest modes as\n$$\n\\mathcal{M}_{\\mathrm{before}} = \\left\\{ k\\in\\{1,\\dots,m\\} \\ \\middle|\\ \\left\\| v_k - \\Pi_c v_k \\right\\|_{M_{\\mathrm{int}}} > \\varepsilon \\right\\},\n$$\nwhere $\\|x\\|_{M_{\\mathrm{int}}} = \\sqrt{x^\\top M_{\\mathrm{int}} x}$ and $\\varepsilon>0$ is a tolerance. The cardinality $|\\mathcal{M}_{\\mathrm{before}}|$ quantifies how many of the $m$ lowest modes are not adequately represented by the coarse space.\n\nTo model enrichment by interior bubble modes, augment the coarse space with $b$ interior bubble functions. Construct these bubble functions by selecting $b$ vectors from the set of residual components\n$$\nq_k = v_k - \\Pi_c v_k, \\quad k=1,\\dots,N-1,\n$$\nordered by decreasing $M_{\\mathrm{int}}$-norm $\\|q_k\\|_{M_{\\mathrm{int}}}$, and $M_{\\mathrm{int}}$-orthonormalizing them while maintaining $M_{\\mathrm{int}}$-orthogonality to $\\mathcal{V}_c$. Let $B\\in\\mathbb{R}^{(N-1)\\times b}$ collect the $b$ enriched bubble vectors and define the enriched prolongation $P_{\\mathrm{enr}}=[P\\ \\ B]$. Define the enriched projection $\\Pi_{\\mathrm{enr}} u = P_{\\mathrm{enr}}\\left(P_{\\mathrm{enr}}^\\top M_{\\mathrm{int}} P_{\\mathrm{enr}}\\right)^{-1} P_{\\mathrm{enr}}^\\top M_{\\mathrm{int}} u$ and the enriched miss set\n$$\n\\mathcal{M}_{\\mathrm{after}} = \\left\\{ k\\in\\{1,\\dots,m\\} \\ \\middle|\\ \\left\\| v_k - \\Pi_{\\mathrm{enr}} v_k \\right\\|_{M_{\\mathrm{int}}} > \\varepsilon \\right\\}.\n$$\n\nYour program must:\n- Construct $M_{\\mathrm{int}}$ and $K_{\\mathrm{int}}$ for the specified $p$.\n- Compute the first $m$ generalized eigenpairs $(\\lambda_k,v_k)$ ordered by ascending $\\lambda_k$, with $v_k$ $M_{\\mathrm{int}}$-orthonormal.\n- Build the prolongation matrix $P$ for degree $r$.\n- Compute the miss counts $|\\mathcal{M}_{\\mathrm{before}}|$ and $|\\mathcal{M}_{\\mathrm{after}}|$ for the specified tolerance $\\varepsilon$, using the enrichment strategy with $b$ interior bubble vectors as defined above.\n\nInput is fixed by the test suite embedded in the program. The outputs must be computed numerically by your code without user input.\n\nUse the following test suite of parameter tuples $(p,r,m,b,\\varepsilon)$:\n- Test A (happy path): $(p,r,m,b,\\varepsilon) = (12,2,6,2,0.25)$.\n- Test B (near-resolution coarse space): $(p,r,m,b,\\varepsilon) = (12,10,6,0,0.25)$.\n- Test C (harder case with higher degree): $(p,r,m,b,\\varepsilon) = (16,3,8,3,0.25)$.\n- Test D (aggressive enrichment): $(p,r,m,b,\\varepsilon) = (12,2,6,6,0.25)$.\n\nFor each test case, your program must output a list of two integers $[|\\mathcal{M}_{\\mathrm{before}}|,|\\mathcal{M}_{\\mathrm{after}}|]$. The final output must be a single line containing all test results as a comma-separated list enclosed in square brackets, where each test result is itself a two-integer list, for example, $[[a_1,a_2],[b_1,b_2],[c_1,c_2],[d_1,d_2]]$ with no spaces.\n\nAll mathematical quantities in this problem are dimensionless, so no physical units are required. The angle unit is not applicable. All numerical answers are to be represented as integers as specified above.",
            "solution": "### Problem Validation\n\n#### Step 1: Extract Givens\n\nThe problem provides the following data and definitions:\n- **Operator and Domain**: One-dimensional Poisson operator on the reference element $\\left[-1,1\\right]$ with homogeneous Dirichlet boundary conditions at $x=-1$ and $x=1$.\n- **Discretization**: Spectral Element Method with Legendre–Gauss–Lobatto (LGL) nodes of polynomial degree $p$, where $p \\ge 12$. The number of subintervals is $N=p$, with $N+1$ LGL nodes $\\{x_i\\}_{i=0}^{N}$ and corresponding quadrature weights $\\{w_i\\}_{i=0}^{N}$. The basis consists of Lagrange cardinal basis functions $\\{\\phi_j\\}$ at these nodes.\n- **Matrices**:\n    - Mass matrix: $M = \\operatorname{diag}(w_0,\\dots,w_N)$.\n    - Stiffness matrix: $K = D^\\top W D$, where $W=\\operatorname{diag}(w_0,\\dots,w_N)$ and $D$ is the differentiation matrix with entries $D_{ij} = \\phi_j'(x_i)$.\n- **Boundary Conditions**: Homogeneous Dirichlet, imposed by restricting to interior degrees of freedom (nodes $i=1,\\dots,N-1$), yielding matrices $M_{\\mathrm{int}}$ and $K_{\\mathrm{int}}$ of size $(N-1)\\times(N-1)$.\n- **Generalized Eigenproblem**: $K_{\\mathrm{int}} v = \\lambda M_{\\mathrm{int}} v$, with eigenpairs $\\{(\\lambda_k,v_k)\\}_{k=1}^{N-1}$ ordered by non-decreasing eigenvalue $\\lambda_k$, and eigenvectors $v_k$ being $M_{\\mathrm{int}}$-orthonormal.\n- **Coarse Space**: Defined by $p$-coarsening to a lower polynomial degree $r$ where $1<r<p$. The coarse grid uses LGL nodes of degree $r$.\n- **Prolongation**: Matrix $P\\in\\mathbb{R}^{(N-1)\\times(r-1)}$ defined by $P_{ij}=\\ell_j(x_i)$, where $\\{\\ell_j\\}_{j=1}^{r-1}$ are the coarse interior Lagrange basis functions and $\\{x_i\\}_{i=1}^{N-1}$ are the fine interior nodes.\n- **Projection**: The $M_{\\mathrm{int}}$-orthogonal projection onto the coarse space $\\mathcal{V}_c=\\operatorname{range}(P)$ is $\\Pi_c u = P\\left(P^\\top M_{\\mathrm{int}} P\\right)^{-1} P^\\top M_{\\mathrm{int}} u$.\n- **\"Before\" Miss Set**: For a given integer $m$ ($1\\le m\\le N-1$) and tolerance $\\varepsilon>0$, $\\mathcal{M}_{\\mathrm{before}} = \\left\\{ k\\in\\{1,\\dots,m\\} \\ \\middle|\\ \\left\\| v_k - \\Pi_c v_k \\right\\|_{M_{\\mathrm{int}}} > \\varepsilon \\right\\}$, where $\\|x\\|_{M_{\\mathrm{int}}} = \\sqrt{x^\\top M_{\\mathrm{int}} x}$.\n- **Enrichment**: The coarse space is augmented with $b$ interior bubble functions. These are constructed by selecting $b$ vectors from the set of residual components $\\{q_k = v_k - \\Pi_c v_k\\}_{k=1}^{N-1}$ that have the largest $M_{\\mathrm{int}}$-norms. These selected vectors are then $M_{\\mathrm{int}}$-orthonormalized while maintaining orthogonality to $\\mathcal{V}_c$ to form the columns of a matrix $B\\in\\mathbb{R}^{(N-1)\\times b}$.\n- **Enriched Projection**: Enriched prolongation is $P_{\\mathrm{enr}}=[P\\ \\ B]$. The enriched projection is $\\Pi_{\\mathrm{enr}} u = P_{\\mathrm{enr}}\\left(P_{\\mathrm{enr}}^\\top M_{\\mathrm{int}} P_{\\mathrm{enr}}\\right)^{-1} P_{\\mathrm{enr}}^\\top M_{\\mathrm{int}} u$.\n- **\"After\" Miss Set**: $\\mathcal{M}_{\\mathrm{after}} = \\left\\{ k\\in\\{1,\\dots,m\\} \\ \\middle|\\ \\left\\| v_k - \\Pi_{\\mathrm{enr}} v_k \\right\\|_{M_{\\mathrm{int}}} > \\varepsilon \\right\\}$.\n- **Task**: Compute the cardinalities $|\\mathcal{M}_{\\mathrm{before}}|$ and $|\\mathcal{M}_{\\mathrm{after}}|$.\n- **Test Suite**: $(p,r,m,b,\\varepsilon)$ tuples: $(12,2,6,2,0.25)$, $(12,10,6,0,0.25)$, $(16,3,8,3,0.25)$, $(12,2,6,6,0.25)$.\n\n#### Step 2: Validate Using Extracted Givens\n\nThe problem statement is subjected to validation against the established criteria:\n1.  **Scientifically Grounded**: The problem is set in the well-established field of numerical analysis for partial differential equations, specifically concerning spectral element methods and algebraic multigrid. The 1D Poisson equation is a canonical model problem. All constructs—LGL nodes, Lagrange basis, mass and stiffness matrices, generalized eigenproblems, p-coarsening, projection operators, and space enrichment—are standard and rigorously defined concepts in this domain. The problem is free of scientific flaws.\n2.  **Well-Posed**: The problem requests the computation of the cardinalities of two sets. The definitions of these sets are deterministic, relying on a sequence of standard numerical linear algebra operations (matrix assembly, eigenvalue decomposition, matrix multiplication, inversion, and norm computations). Given the specified input parameters, the process leads to a unique numerical outcome, contingent only on floating-point precision, for which the tolerance $\\varepsilon > 0$ provides robustness. The problem is well-posed.\n3.  **Objective**: The problem is expressed in precise, unambiguous mathematical language. All terms are formally defined. There is no subjective or opinion-based content.\n4.  **Complete and Consistent**: All necessary information, including parameters for multiple test cases, is provided. The definitions are self-consistent. For example, the dimensions of all matrices and vectors are compatible for the specified operations. The constraints on the parameters ($p \\ge 12$, $1 < r < p$) are satisfied by the test cases.\n5.  **No Other Flaws**: The problem is not trivial, metaphorical, or tautological. It represents a valid and meaningful numerical experiment to diagnose a common failure mode in algebraic multigrid methods for high-order discretizations.\n\n#### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution\n\nThe proposed diagnostic procedure is a computational analysis of the spectral approximation properties of a coarse space used in algebraic multigrid (AMG) for high-order spectral element discretizations. The core idea is to identify which low-frequency eigenmodes of the discrete operator are poorly represented by a standard low-order coarse space and to quantify the improvement gained by enriching that space with carefully chosen \"bubble\" functions. The procedure is broken down into the following steps.\n\n**1. System Matrix Construction**\nThe analysis begins with the discretization of the 1D Poisson operator on the reference element $\\Omega = [-1, 1]$ with homogeneous Dirichlet boundary conditions. A spectral element method with polynomial degree $p=N$ is used.\n\n- **Nodes and Weights**: The Legendre-Gauss-Lobatto (LGL) nodes $\\{x_i\\}_{i=0}^N$ are the roots of $(1-x^2)L_N'(x)$, where $L_N(x)$ is the Legendre polynomial of degree $N$. The associated quadrature weights $\\{w_i\\}_{i=0}^N$ are given by $w_i = \\frac{2}{N(N+1) [L_N(x_i)]^2}$. This choice of nodes and weights allows for the evaluation of integrals via LGL quadrature, which is exact for polynomials of degree up to $2N-1$.\n\n- **Mass and Stiffness Matrices**: Using the Lagrange basis functions $\\{\\phi_j\\}_{j=0}^N$ defined by $\\phi_j(x_i) = \\delta_{ij}$, the mass and stiffness matrices are defined with respect to the $L^2$ inner product $\\langle u,v \\rangle = \\int_{-1}^1 u(x)v(x)dx$. Using LGL quadrature, the mass matrix entries become $M_{ij} = \\int_{-1}^1 \\phi_i(x)\\phi_j(x)dx \\approx \\sum_{k=0}^N w_k \\phi_i(x_k)\\phi_j(x_k) = w_i \\delta_{ij}$. Thus, the mass matrix is diagonal: $M = W = \\operatorname{diag}(w_0, \\dots, w_N)$. The stiffness matrix entries are $K_{ij} = \\int_{-1}^1 \\phi_i'(x)\\phi_j'(x)dx \\approx \\sum_{k=0}^N w_k \\phi_i'(x_k)\\phi_j'(x_k)$. In matrix form, this is $K = D^\\top W D$, where $D$ is the spectral differentiation matrix with entries $D_{ki} = \\phi_i'(x_k)$.\n\n- **Boundary Conditions**: Homogeneous Dirichlet boundary conditions ($u(-1)=u(1)=0$) are imposed by eliminating the degrees of freedom corresponding to the boundary nodes $x_0=-1$ and $x_N=1$. This is achieved by restricting the system to the interior nodes $i=1, \\dots, N-1$. The resulting matrices, $K_{\\mathrm{int}}$ and $M_{\\mathrm{int}}$, are the $(N-1) \\times (N-1)$ submatrices corresponding to these interior indices.\n\n**2. Generalized Eigenvalue Problem**\nThe eigenmodes of the discrete operator are the solutions to the generalized eigenvalue problem $K_{\\mathrm{int}} v = \\lambda M_{\\mathrm{int}} v$. The eigenvalues $\\lambda_k$ approximate the eigenvalues of the continuous Laplacian operator, and the eigenvectors $v_k$ represent the corresponding discrete eigenfunctions. The eigenvectors are normalized to be $M_{\\mathrm{int}}$-orthonormal, i.e., $v_k^\\top M_{\\mathrm{int}} v_j = \\delta_{kj}$, which is the discrete analogue of the $L^2$-orthogonality of the continuous eigenfunctions. We are interested in the first $m$ modes, which correspond to the lowest frequencies.\n\n**3. Coarse Space and Projection**\nAn essential component of multigrid is the coarse space. Here, we use $p$-coarsening, where the coarse space consists of polynomials of a lower degree $r < p$.\n- **Prolongation**: The coarse space is spanned by the interior Lagrange basis functions $\\{\\ell_j\\}_{j=1}^{r-1}$ of degree $r$. The prolongation operator $P$ maps from the coarse representation to the fine representation. Its columns are simply the coarse basis functions evaluated at the fine interior nodes: $P_{ij} = \\ell_j(x_i)$.\n- **Projection**: The quality of the coarse-space approximation is measured by projection. The $M_{\\mathrm{int}}$-orthogonal projection of a fine-grid vector $u$ onto the coarse space $\\mathcal{V}_c = \\operatorname{range}(P)$ is given by $\\Pi_c u = P(P^\\top M_{\\mathrm{int}} P)^{-1} P^\\top M_{\\mathrm{int}} u$. The error in approximation is $u - \\Pi_c u$, and its magnitude is measured in the $M_{\\mathrm{int}}$-norm, $\\|u - \\Pi_c u\\|_{M_{\\mathrm{int}}}$, which corresponds to the $L^2$ norm in the continuous setting.\n\n**4. Coarse-Space Miss Sets**\n- **Initial Miss Set**: We quantify the deficiency of the coarse space by identifying which of the first $m$ eigenmodes are poorly represented. The set $\\mathcal{M}_{\\mathrm{before}}$ contains the indices $k \\in \\{1, \\dots, m\\}$ for which the projection error $\\|v_k - \\Pi_c v_k\\|_{M_{\\mathrm{int}}}$ exceeds a given tolerance $\\varepsilon$. The cardinality $|\\mathcal{M}_{\\mathrm{before}}|$ is our first diagnostic metric.\n\n- **Enrichment**: To improve the coarse space, we enrich it with functions that are specifically designed to capture what the original coarse space misses. The vectors $q_k = v_k - \\Pi_c v_k$ are the components of the eigenvectors that are orthogonal to the coarse space. The modes with the largest $\\|q_k\\|_{M_{\\mathrm{int}}}$ are the ones most poorly represented. We select the $b$ vectors $q_k$ with the largest norms. These vectors are, by construction, $M_{\\mathrm{int}}$-orthogonal to $\\mathcal{V}_c$. We then perform an $M_{\\mathrm{int}}$-Gram-Schmidt procedure on them to obtain an $M_{\\mathrm{int}}$-orthonormal set $\\{b_i\\}_{i=1}^b$, which form the columns of the matrix $B$.\n\n- **Enriched Miss Set**: The enriched prolongation operator is $P_{\\mathrm{enr}} = [P \\ B]$, spanning the enriched space $\\mathcal{V}_{\\mathrm{enr}} = \\mathcal{V}_c \\oplus \\operatorname{range}(B)$. The new projection is $\\Pi_{\\mathrm{enr}}$. The analysis is repeated to find the \"after\" miss set, $\\mathcal{M}_{\\mathrm{after}}$, by checking the error $\\|v_k - \\Pi_{\\mathrm{enr}} v_k\\|_{M_{\\mathrm{int}}}$ against $\\varepsilon$. The cardinality $|\\mathcal{M}_{\\mathrm{after}}|$ is our second diagnostic metric, which is expected to be smaller than $|\\mathcal{M}_{\\mathrm{before}}|$.\n\nThis process provides a quantitative measure of how a targeted enrichment strategy can resolve deficiencies in a standard coarse space for high-order methods.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom numpy.polynomial.legendre import Legendre\n\ndef lgl_points_weights(p):\n    \"\"\"\n    Computes Legendre-Gauss-Lobatto (LGL) nodes and weights for a given polynomial degree p.\n    \"\"\"\n    if p == 0:\n        return np.array([0.0]), np.array([2.0])\n    if p == 1:\n        return np.array([-1.0, 1.0]), np.array([1.0, 1.0])\n\n    # Interior nodes are the roots of the derivative of the Legendre polynomial of degree p.\n    leg_p = Legendre.basis(p)\n    leg_p_deriv = leg_p.deriv(1)\n    # Roots can be complex due to precision, take real part.\n    int_nodes = np.sort(leg_p_deriv.roots().real)\n    nodes = np.concatenate(([-1.0], int_nodes, [1.0]))\n\n    # LGL weights formula\n    leg_p_vals_at_nodes = leg_p(nodes)\n    weights = 2 / (p * (p + 1) * leg_p_vals_at_nodes**2)\n    \n    return nodes, weights\n\ndef lagrange_deriv_matrix(p, nodes):\n    \"\"\"\n    Computes the 1D spectral differentiation matrix for LGL nodes.\n    \"\"\"\n    N = p\n    D = np.zeros((N + 1, N + 1))\n    leg_p = Legendre.basis(N)\n    p_vals = leg_p(nodes)\n\n    # Off-diagonal entries\n    for i in range(N + 1):\n        for j in range(N + 1):\n            if i != j:\n                D[i, j] = p_vals[i] / (p_vals[j] * (nodes[i] - nodes[j]))\n    \n    # Diagonal entries\n    D[0, 0] = -N * (N + 1) / 4.0\n    D[N, N] = N * (N + 1) / 4.0\n    # Interior diagonal entries are 0, which is the default.\n    \n    return D\n\ndef evaluate_lagrange_basis(nodes_eval, nodes_basis, j):\n    \"\"\"\n    Evaluates the j-th Lagrange basis polynomial defined on nodes_basis at points nodes_eval.\n    l_j(x) = product_{k!=j} (x - x_k) / (x_j - x_k)\n    \"\"\"\n    xj = nodes_basis[j]\n    other_nodes = np.delete(nodes_basis, j)\n    # Using np.newaxis for broadcasting to evaluate for all nodes_eval at once\n    numerator = np.prod(nodes_eval[:, np.newaxis] - other_nodes, axis=1)\n    denominator = np.prod(xj - other_nodes)\n    return numerator / denominator\n\ndef run_diagnostic(p, r, m, b, epsilon):\n    \"\"\"\n    Runs the full diagnostic procedure for a given set of parameters.\n    \"\"\"\n    # 1. Construct matrices for fine grid (degree p)\n    N = p\n    fine_nodes, fine_weights = lgl_points_weights(N)\n    D_fine = lagrange_deriv_matrix(N, fine_nodes)\n    \n    M = np.diag(fine_weights)\n    K = D_fine.T @ M @ D_fine\n    \n    # Impose Dirichlet BCs by restricting to interior nodes\n    M_int = np.diag(fine_weights[1:N])\n    K_int = K[1:N, 1:N]\n    \n    # 2. Solve generalized eigenproblem K_int v = lambda M_int v\n    # eigh returns M_int-orthonormal eigenvectors, sorted by eigenvalue.\n    eigenvalues, eigenvectors = eigh(K_int, M_int)\n    \n    # 3. Define coarse space and prolongation matrix P\n    fine_interior_nodes = fine_nodes[1:N]\n    num_fine_interior = N - 1\n    \n    coarse_nodes, _ = lgl_points_weights(r)\n    num_coarse_interior = r - 1\n\n    if num_coarse_interior > 0:\n        P = np.zeros((num_fine_interior, num_coarse_interior))\n        for j_coarse in range(num_coarse_interior):\n            # Coarse interior basis function indices are 1 to r-1\n            P[:, j_coarse] = evaluate_lagrange_basis(fine_interior_nodes, coarse_nodes, j_coarse + 1)\n    else:\n        P = np.zeros((num_fine_interior, 0))\n\n    # 4. Compute miss count before enrichment\n    if P.shape[1] > 0:\n        coarse_mass = P.T @ M_int @ P\n        coarse_mass_inv = np.linalg.inv(coarse_mass)\n    \n    miss_count_before = 0\n    for k in range(m):\n        vk = eigenvectors[:, k]\n        if P.shape[1] > 0:\n            pi_c_vk = P @ (coarse_mass_inv @ (P.T @ (M_int @ vk)))\n        else: # Empty coarse space\n            pi_c_vk = np.zeros_like(vk)\n        \n        residual = vk - pi_c_vk\n        norm_sq = residual.T @ M_int @ residual\n        norm = np.sqrt(max(0, norm_sq)) # Clamp to avoid sqrt of small negative floats\n        if norm > epsilon:\n            miss_count_before += 1\n\n    # 5. Enrichment and miss count after\n    if b == 0:\n        return [miss_count_before, miss_count_before]\n\n    # Select b enrichment vectors from residuals of all modes\n    all_residuals = []\n    all_norms_sq = []\n    for k in range(num_fine_interior):\n        vk = eigenvectors[:, k]\n        if P.shape[1] > 0:\n            pi_c_vk = P @ (coarse_mass_inv @ (P.T @ (M_int @ vk)))\n        else:\n            pi_c_vk = np.zeros_like(vk)\n        qk = vk - pi_c_vk\n        norm_sq = qk.T @ M_int @ qk\n        all_residuals.append(qk)\n        all_norms_sq.append(max(0, norm_sq))\n    \n    indices_to_enrich = np.argsort(all_norms_sq)[-b:]\n    enrichment_vectors = [all_residuals[i] for i in indices_to_enrich]\n    \n    # M_int-Gram-Schmidt orthonormalization\n    B = np.zeros((num_fine_interior, b))\n    for i in range(b):\n        u_i = enrichment_vectors[i].copy()\n        for j in range(i):\n            bj = B[:, j]\n            # Orthogonal projection: u_i -= (u_i^T M b_j) b_j\n            proj = (u_i.T @ M_int @ bj) * bj\n            u_i -= proj\n        \n        norm_ui_sq = u_i.T @ M_int @ u_i\n        if norm_ui_sq > 1e-20:\n            B[:, i] = u_i / np.sqrt(norm_ui_sq)\n\n    # Enriched prolongation and projection\n    P_enr = np.hstack([P, B])\n    \n    M_enr_coarse = P_enr.T @ M_int @ P_enr\n    M_enr_coarse_inv = np.linalg.inv(M_enr_coarse)\n\n    miss_count_after = 0\n    for k in range(m):\n        vk = eigenvectors[:, k]\n        pi_enr_vk = P_enr @ (M_enr_coarse_inv @ (P_enr.T @ (M_int @ vk)))\n        \n        residual = vk - pi_enr_vk\n        norm_sq = residual.T @ M_int @ residual\n        norm = np.sqrt(max(0, norm_sq))\n        if norm > epsilon:\n            miss_count_after += 1\n            \n    return [miss_count_before, miss_count_after]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (12, 2, 6, 2, 0.25),   # Test A\n        (12, 10, 6, 0, 0.25),  # Test B\n        (16, 3, 8, 3, 0.25),   # Test C\n        (12, 2, 6, 6, 0.25),   # Test D\n    ]\n\n    results = []\n    for params in test_cases:\n        p, r, m, b, epsilon = params\n        result = run_diagnostic(p, r, m, b, epsilon)\n        results.append(result)\n\n    # Format output as specified: [[a1,a2],[b1,b2],...] with no spaces\n    result_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}