## Applications and Interdisciplinary Connections

Having established the foundational principles and discrete formulations of Discontinuous Galerkin (DG), Weak Galerkin (WG), and Virtual Element Methods (VEM), we now turn our attention to their application in diverse and challenging contexts. This chapter bridges the gap between theory and practice by demonstrating how the core mechanisms of these methods are utilized to address complex problems in scientific computing. The focus is not to re-derive the methods, but to illuminate their utility, comparative advantages, and interdisciplinary connections. We will explore how these methods perform in wave propagation problems, how they are adapted for nonlinear systems, what their relative computational costs are, and how they can be coupled in sophisticated hybrid [numerical schemes](@entry_id:752822). Through these applications, the deep theoretical connections and distinct practical philosophies of DG, WG, and VEM will be brought into sharp relief.

### Numerical Analysis of Wave Propagation Phenomena

A critical application domain for advanced numerical methods is the simulation of wave phenomena, which are central to fields ranging from acoustics and electromagnetism to [seismology](@entry_id:203510). The primary challenge in such simulations is not merely stability, but accuracy—specifically, the ability of the numerical scheme to propagate waves with the correct speed and without excessive [artificial damping](@entry_id:272360) over long distances. The properties of [numerical dispersion](@entry_id:145368) (wave speed error) and dissipation (amplitude decay) are therefore of paramount importance.

#### Dispersion and Dissipation in Advection Problems

The [linear advection equation](@entry_id:146245), $u_t + a u_x = 0$, serves as the archetypal model for [hyperbolic systems](@entry_id:260647) and [wave propagation](@entry_id:144063). A fundamental measure of a scheme's accuracy for this equation is its ability to preserve the [group velocity](@entry_id:147686), which for the exact solution is the constant advection speed $a$. Numerical methods invariably introduce a group velocity that depends on the [wavenumber](@entry_id:172452), leading to dispersive errors where different frequency components of a [wave packet](@entry_id:144436) travel at different speeds, distorting the solution.

We can explore this by comparing a standard, lowest-order ($p=0$) upwind DG method with a related WG formulation. For the upwind DG method on a uniform mesh of size $h$, a von Neumann analysis reveals that the numerical group velocity is $v_{g,\mathrm{DG}} = a \cos(\theta)$, where $\theta = \xi h$ is the dimensionless [wavenumber](@entry_id:172452). This shows the classic dispersive nature of the scheme: long waves ($\theta \to 0$) travel at nearly the correct speed, while short waves ($\theta \to \pi$) are significantly slowed, with the shortest resolvable wave having zero group velocity.

A comparable lowest-order WG method can be constructed using piecewise constant functions in element interiors and on the skeleton, with a [stabilization term](@entry_id:755314) that penalizes the jump between them. Under a simplified symbol-level model where the effect of this stabilization, of strength $\tau \ge 0$, is to introduce an attenuation factor of $(1+\tau)^{-1}$ on the upwind inter-element flux, the resulting WG [group velocity](@entry_id:147686) is found to be $v_{g,\mathrm{WG}} = \frac{a \cos(\theta)}{1+\tau}$. This result is illuminating: the [stabilization parameter](@entry_id:755311) $\tau$ directly modulates the wave propagation speed. Increasing $\tau$ enhances stability but simultaneously increases the dispersive error by further slowing down all wave components. The direct connection to DG becomes evident when we seek to match the [group velocity](@entry_id:147686) of the two methods. Equating $v_{g,\mathrm{DG}}$ and $v_{g,\mathrm{WG}}$ yields $\frac{a \cos(\theta)}{1+\tau} = a \cos(\theta)$, which implies $\tau=0$ for any non-zero wave mode. This demonstrates that, at the level of this analysis, the WG method with its stabilization removed is dynamically equivalent to the upwind DG method, highlighting that stabilization in WG is a tunable feature that separates it from a pure DG formulation .

#### Mitigating Pollution Error in High-Frequency Wave Problems

While [linear advection](@entry_id:636928) reveals fundamental dispersive properties, the time-harmonic Helmholtz equation, $-u'' - k^2 u = 0$, presents a more severe challenge known as [numerical pollution](@entry_id:752816). In the high-frequency regime (when the wavenumber $k$ is large), standard discretizations require a rapidly increasing number of degrees of freedom per wavelength to maintain a given accuracy. This "pollution error" manifests as a severe [phase lag](@entry_id:172443), rendering simulations of long-distance [wave propagation](@entry_id:144063) computationally intractable.

A powerful aspect of modern DG, WG, and VEM formulations is that they can be designed to mitigate this error. When formulated through hybridization, where the global system is expressed purely in terms of trace unknowns on the mesh skeleton, these seemingly disparate methods can be analyzed within a unified framework based on their discrete Dirichlet-to-Neumann (DtN) maps. For the one-dimensional Helmholtz problem, analysis shows that the leading-order [dispersion error](@entry_id:748555) for this class of methods takes a universal form. Assuming a suitable symmetric [stabilization term](@entry_id:755314) is included in the formulation, controlled by a dimensionless parameter $\sigma$, the error in the numerical [wavenumber](@entry_id:172452) $\kappa_h$ can be expressed as $\kappa_h - k \approx C(1-\sigma) k (kh)^2$ for small mesh sizes $h$.

This structure reveals a remarkable opportunity. The constant $C$ and the term $(kh)^2$ represent the intrinsic pollution error of the underlying consistent part of the [discretization](@entry_id:145012). The term $(1-\sigma)$ shows that the [stabilization parameter](@entry_id:755311) $\sigma$ directly counteracts this error. By choosing $\sigma=1$, the leading-order error term can be completely eliminated. This leads to a so-called "pollution-free" or "dispersion-optimized" scheme. The condition $\sigma=1$ translates into a specific choice for the physical [stabilization parameter](@entry_id:755311) $\tau$. For a standard high-order formulation where the stabilization is scaled by $p^2/k$ (with $p$ being the polynomial degree), this optimal choice is found to be $\tau^{\star} = p^2/k$. This application demonstrates a profound connection: despite their different construction philosophies, DG, WG, and VEM can be tuned through a common stabilization strategy to achieve identical, dramatically improved accuracy for a notoriously difficult class of problems .

### Advanced Implementation Strategies and Practical Considerations

Beyond theoretical analysis, the choice between DG, WG, and VEM often hinges on practical implementation details, particularly when confronting nonlinearities or considering computational efficiency.

#### Handling Nonlinearities and Aliasing

When solving [nonlinear partial differential equations](@entry_id:168847), such as those involving a Burgers-type nonlinearity $f(u)=u^2$ or a more general polynomial nonlinearity $f(u)=u^s$, a central task is the computation of [volume integrals](@entry_id:183482) of the form $\int_K f(u_h) \varphi_h \, dx$, where $u_h, \varphi_h \in \mathbb{P}_p(K)$ are polynomial approximations of degree $p$. A naive or inexact evaluation of this integral can lead to aliasing errors, where unresolved high-frequency components of the integrand erroneously contribute to the computed low-frequency modes, often leading to instability.

The strategies to avoid aliasing differ fundamentally between DG and VEM/WG, reflecting their core philosophies. In a DG context, integrals are typically approximated by numerical quadrature. To evaluate the integral exactly and thus eliminate aliasing, the [quadrature rule](@entry_id:175061) must be exact for the entire integrand. Since $u_h^s$ has a degree up to $sp$ and $\varphi_h$ has a degree up to $p$, the product $u_h^s \varphi_h$ is a polynomial of degree up to $(s+1)p$. Therefore, a DG implementation must employ a [quadrature rule](@entry_id:175061) with a minimal [degree of exactness](@entry_id:175703) $q_{\min} = (s+1)p$. For a quadratic Burgers-type nonlinearity ($s=2$), this requires a rule exact for polynomials of degree $3p$. This practice, known as over-integration, ensures stability and accuracy at the cost of additional quadrature points and computational work.

In contrast, VEM and WG are "quadrature-free" in the sense that they rely on the exact evaluation of integrals of polynomials using pre-computed geometric moments. To implement a nonlinear term, one can use a projector-based approach. The integral $\int_K u_h^s \varphi_h \, dx$ can be computed exactly if the method has access to all polynomial moments of degree up to the degree of the integrand. This imposes an identical requirement on the method's capabilities: the minimal highest degree of virtual moments required, $R_{\min}$, must be equal to the maximal degree of the integrand, so $R_{\min} = (s+1)p$. For the Burgers case ($s=2$), this means the method must be constructed with exact moments up to degree $3p$. This reveals a crucial parallel: while the mechanisms differ (quadrature points vs. exact moments), the underlying mathematical necessity to resolve polynomials of degree $(s+1)p$ is the same for both classes of methods when treating nonlinearities exactly .

#### Computational Cost and Algorithmic Choice

A decisive factor in selecting a numerical method is its computational cost. Comparing the Floating Point Operation (FLOP) counts for assembling local element matrices in DG and VEM reveals significant structural differences. While precise costs are architecture-dependent, we can construct illustrative models based on the dominant algorithmic steps.

The cost of a DG assembly is primarily driven by loops over quadrature points. For a 2D element, the cost scales with the number of interior quadrature points (proportional to $(p+1)^2$) and the number of face quadrature points (proportional to $n_F (p+1)$, where $n_F$ is the number of faces). At each point, basis functions are evaluated, leading to a total cost that scales strongly with polynomial degree $p$ and linearly with the number of faces $n_F$.

The VEM assembly cost has a different character. It avoids quadrature but requires the construction of a local $L^2$-projector for each element. This involves solving a dense, local linear system whose size $N_p$ is the dimension of the [polynomial space](@entry_id:269905), which is $\binom{p+2}{2} \approx p^2/2$. The dominant cost is the Cholesky factorization of the associated Gram matrix, which requires $\mathcal{O}(N_p^3) \approx \mathcal{O}(p^6)$ FLOPs. This cost is very high for large $p$ but, crucially, has a much weaker dependence on the number of element faces $n_F$.

By setting up and comparing formal cost models, $F_{\mathrm{DG}}(p,n_F)$ and $F_{\mathrm{VEM}}(p,n_F)$, we can analyze their trade-offs. Solving for the number of faces $n_F$ at which the costs are equal, $F_{\mathrm{VEM}} = F_{\mathrm{DG}}$, yields a crossover threshold, $n_F^{\star}(p)$. This threshold represents the number of faces for a given $p$ at which the VEM projector cost becomes equal to the DG quadrature cost. For elements with fewer faces than $n_F^{\star}$, DG may be computationally cheaper, while for elements with more faces (complex polygons), the VEM approach could be more efficient, as its dominant cost is largely independent of $n_F$. This analysis underscores that there is no universally "cheaper" method; the optimal choice is a nuanced decision based on the interplay between polynomial degree and mesh geometry, with VEM showing a particular advantage for discretizations involving highly complex polygonal elements .

### Interdisciplinary Coupling: Hybrid Spatial and Temporal Discretizations

The distinct strengths of DG, WG, and VEM make them ideal candidates for use in coupled multi-[physics simulations](@entry_id:144318), where different numerical methods are applied to different physical operators within the same problem. A prime example arises in the solution of [advection-diffusion equations](@entry_id:746317), which combine hyperbolic (advective) and parabolic (diffusive) characteristics.

Consider the linear advection-[diffusion equation](@entry_id:145865), $u_t + a u_x = \nu u_{xx}$. The advection term is non-dissipative and its discretization often benefits from the [upwinding](@entry_id:756372) and [local conservation](@entry_id:751393) properties of DG. The diffusion term is stiff, meaning that an [explicit time-stepping](@entry_id:168157) scheme would require an extremely small time step, $\Delta t = \mathcal{O}(h^2)$, for stability. This stiffness makes it a natural candidate for an [implicit time-stepping](@entry_id:172036) method. WG and VEM are excellently suited for elliptic and parabolic problems like diffusion.

This motivates a hybrid [spatial discretization](@entry_id:172158) coupled with an Implicit-Explicit (IMEX) time-stepping scheme. One can treat the DG advection term explicitly (e.g., with Forward Euler) and the WG/VEM diffusion term implicitly (e.g., with Backward Euler). This approach leverages the best of all worlds: the favorable properties of DG for advection and the [unconditional stability](@entry_id:145631) of an [implicit method](@entry_id:138537) for the stiff diffusion, allowing for a much larger time step than a fully explicit scheme.

A [linear stability analysis](@entry_id:154985) of such an IMEX scheme confirms its utility. The [amplification factor](@entry_id:144315) for a single Fourier mode depends on the eigenvalues of both the explicit advection operator, $\lambda_c$, and the implicit [diffusion operator](@entry_id:136699), $\lambda_d$. The stability condition is $|1+\Delta t \lambda_c| \le |1-\Delta t \lambda_d|$. Since the [diffusion operator](@entry_id:136699) is dissipative, its eigenvalues $\lambda_d$ are non-positive real numbers, which means the implicit term $|1-\Delta t \lambda_d|$ is always greater than or equal to 1. This term enhances the [stability region](@entry_id:178537) of the explicit part. However, to find a time-step limit that is robust for any amount of diffusion ($\nu \ge 0$), we must consider the worst case, which is the pure advection limit ($\nu \to 0$). In this limit, the stability constraint reduces to that of the explicit Forward Euler method applied to the DG advection operator alone: $|1+\Delta t \lambda_c| \le 1$. The analysis then proceeds by finding the [spectral radius](@entry_id:138984) of the DG operator. For an upwind DG scheme of degree $p$, this leads to a CFL-type condition of the form $\Delta t_{\max} = \frac{h}{(2p+1)|a|}$. This application powerfully demonstrates how different methods from the DG/WG/VEM family can be used as building blocks in a sophisticated, coupled simulation framework, combining their individual strengths to efficiently and stably solve complex, multi-scale problems .

### Conclusion

The applications explored in this chapter illustrate that Discontinuous Galerkin, Weak Galerkin, and Virtual Element methods form a deeply interconnected family of advanced numerical techniques. Their relationships are not merely historical or superficial; they are revealed through rigorous comparative analysis of their performance in critical areas like wave propagation, where stabilization parameters can be tuned to achieve identical, high-fidelity results. Their distinct philosophies—quadrature-based approximation versus projector-based exact evaluation—lead to different practical strategies for handling challenges like nonlinearities and different computational cost profiles, making the choice between them a sophisticated function of polynomial degree, problem physics, and [mesh topology](@entry_id:167986). Finally, their complementary strengths enable their use as components in powerful hybrid schemes for tackling multi-physics problems. The modern computational scientist, therefore, does not view these methods in isolation but as a versatile and powerful toolkit, where an understanding of their applications and interdisciplinary connections is essential for designing the next generation of scientific simulations.