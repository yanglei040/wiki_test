## Introduction
Solving nonlinear [systems of conservation laws](@entry_id:755768), which govern phenomena from [gas dynamics](@entry_id:147692) to shallow water flows, poses a significant computational challenge. Solutions can form sharp discontinuities (shocks), and standard numerical methods may become unstable or converge to non-physical results. Discrete entropy analysis offers a powerful and systematic framework to overcome this problem by constructing [high-order schemes](@entry_id:750306) that are provably stable and adhere to fundamental physical principles like the Second Law of Thermodynamics. This article addresses the knowledge gap between the continuous mathematical theory of entropy and its practical implementation in modern numerical methods. It provides a comprehensive guide to designing and analyzing robust schemes for complex simulations.

The following chapters will guide you through this topic. In "Principles and Mechanisms," you will learn the core concepts, from the continuous [entropy condition](@entry_id:166346) to the discrete building blocks—like Summation-By-Parts operators and [entropy-conservative fluxes](@entry_id:749013)—that mimic this condition numerically. "Applications and Interdisciplinary Connections" will demonstrate the framework's power by exploring its use in [computational fluid dynamics](@entry_id:142614), [geophysical modeling](@entry_id:749869), and its connections to advanced areas like uncertainty quantification. Finally, "Hands-On Practices" will highlight key practical considerations for implementing these sophisticated methods. We begin by exploring the foundational principles and mechanisms that underpin this entire framework.

## Principles and Mechanisms

The development of numerical methods for nonlinear [systems of conservation laws](@entry_id:755768) is fundamentally concerned not only with accuracy but also with stability. For [hyperbolic systems](@entry_id:260647), solutions can develop discontinuities (shocks) even from smooth initial data, posing a profound challenge for numerical schemes. A naive discretization may produce [spurious oscillations](@entry_id:152404) or, worse, converge to a non-physical solution. The mathematical theory of entropy provides a rigorous framework for ensuring that a numerical solution is both stable and physically relevant. This chapter elucidates the core principles of entropy analysis and the mechanisms by which these principles are translated into the design of modern, high-order [numerical schemes](@entry_id:752822).

### The Continuous Entropy Condition

A system of conservation laws is given by
$$ \partial_t \boldsymbol{u} + \sum_{i=1}^d \partial_{x_i} \boldsymbol{f}_i(\boldsymbol{u}) = 0 $$
where $\boldsymbol{u}(x,t)$ is a vector of conserved quantities and $\boldsymbol{f}_i$ are the flux functions. A central issue is that the [weak form](@entry_id:137295) of this equation, which admits discontinuous solutions, does not guarantee a unique solution. To restore uniqueness and select the physically correct solution, an additional constraint is required: the [entropy condition](@entry_id:166346).

This is formalized through the concept of an **entropy pair**, $(U, \boldsymbol{F})$, where $U(\boldsymbol{u})$ is a scalar, convex function of the state $\boldsymbol{u}$, called the **entropy function**, and $\boldsymbol{F} = (F_1, \dots, F_d)$ is the associated **entropy flux vector**. For smooth solutions, this pair must satisfy its own conservation law, derived from the original system:
$$ \partial_t U(\boldsymbol{u}) + \sum_{i=1}^d \partial_{x_i} F_i(\boldsymbol{u}) = 0 $$
Multiplying the original PDE by $(\nabla_{\boldsymbol{u}} U)^T$ and applying the chain rule, we see that this [entropy conservation](@entry_id:749018) law holds if the entropy flux satisfies the [compatibility condition](@entry_id:171102)
$$ \nabla_{\boldsymbol{u}} F_i(\boldsymbol{u}) = (\nabla_{\boldsymbol{u}} U(\boldsymbol{u}))^T \nabla_{\boldsymbol{u}} \boldsymbol{f}_i(\boldsymbol{u}) $$
for each spatial direction $i$. For [weak solutions](@entry_id:161732) containing shocks, this equality is relaxed to the **[entropy inequality](@entry_id:184404)**:
$$ \partial_t U(\boldsymbol{u}) + \sum_{i=1}^d \partial_{x_i} F_i(\boldsymbol{u}) \le 0 $$
This inequality signifies that the total entropy in a closed system can only decrease (or stay constant), which corresponds to the dissipation inherent in physical shock phenomena.

The requirement of convexity for $U(\boldsymbol{u})$ is paramount; it is this property that allows the [entropy condition](@entry_id:166346) to select a unique, stable solution. This distinguishes a true entropy from a generic energy functional. For instance, while the quadratic functional $E(\boldsymbol{u}) = \frac{1}{2}\|\boldsymbol{u}\|^2$ is convex and often used for stability analysis of linear problems, it only qualifies as an entropy for a nonlinear problem if a compatible flux can be found .

A canonical example is the one-dimensional inviscid Burgers' equation, $u_t + \partial_x (\frac{1}{2}u^2) = 0$. Let us choose the convex entropy function $U(u) = \frac{1}{2}u^2$. The physical flux is $f(u) = \frac{1}{2}u^2$. The compatibility condition for the entropy flux $F(u)$ becomes:
$$ F'(u) = U'(u) f'(u) = (u)(u) = u^2 $$
Integrating with respect to $u$ and setting the integration constant to zero gives the corresponding entropy flux $F(u) = \frac{1}{3}u^3$. Thus, $(\frac{1}{2}u^2, \frac{1}{3}u^3)$ is a valid entropy pair for Burgers' equation .

### Entropy Variables and Symmetrization

For systems of equations, the analysis is most elegantly performed using a change of variables. The **entropy variables**, also known as [dual variables](@entry_id:151022), are defined as the gradient of the entropy function with respect to the conservative [state vector](@entry_id:154607):
$$ \boldsymbol{v}(\boldsymbol{u}) = \nabla_{\boldsymbol{u}} U(\boldsymbol{u}) $$
Since $U(\boldsymbol{u})$ is strictly convex, the mapping from the conservative variables $\boldsymbol{u}$ to the entropy variables $\boldsymbol{v}$ is invertible. This transformation has the remarkable property of symmetrizing the quasi-[linear form](@entry_id:751308) of the conservation law, a feature that is foundational to both continuous and discrete stability analyses.

A primary example is the system of 1D compressible Euler equations for an ideal gas. The conservative state is $\boldsymbol{q} = (\rho, \rho u, \rho E)^T$. The physically derived mathematical entropy is $\eta(\boldsymbol{q}) = -\frac{\rho s}{\gamma - 1}$, where $s = \ln(p) - \gamma \ln(\rho)$ is the specific [thermodynamic entropy](@entry_id:155885) (up to constants). The entropy variables $\boldsymbol{v} = \partial \eta / \partial \boldsymbol{q}$ can be derived through careful application of the [chain rule](@entry_id:147422). In terms of the more intuitive primitive variables $(\rho, u, p)$, the entropy variables are :
$$ \boldsymbol{v} = \begin{pmatrix} \frac{\gamma - \ln(p) + \gamma\ln(\rho)}{\gamma-1} - \frac{\rho u^2}{2p} \\ \frac{\rho u}{p} \\ -\frac{\rho}{p} \end{pmatrix} $$
This explicit form reveals a critical dependency. The entropy function $\eta$ involves $\ln(\rho)$ and $\ln(p)$, and its derivatives (the entropy variables) involve terms like $1/p$. Consequently, the entire entropy framework is only valid in the physical state space where the density $\rho$ and pressure $p$ are strictly positive. If a numerical solution were to produce non-positive values at any point, the entropy variables would become undefined. This would invalidate not only the theoretical analysis but also the practical implementation of many [entropy-stable schemes](@entry_id:749017), which often rely on explicit formulas for $\boldsymbol{v}$ .

### The Architecture of an Entropy-Stable Discretization

The principal goal of [discrete entropy analysis](@entry_id:748504) is to construct a [semi-discretization](@entry_id:163562), $\frac{d\boldsymbol{u}_h}{dt} = \mathcal{L}(\boldsymbol{u}_h)$, that satisfies a discrete analogue of the [entropy inequality](@entry_id:184404). For a nodal method, this typically takes the form:
$$ \frac{d}{dt} \sum_i w_i U(\boldsymbol{u}_i) \le \text{Boundary Terms} $$
where $\boldsymbol{u}_i$ are the solution values at quadrature nodes and $w_i$ are the corresponding weights. Modern [entropy-stable schemes](@entry_id:749017), particularly high-order Discontinuous Galerkin (DG) and [spectral collocation methods](@entry_id:755162), achieve this by meticulously designing the discrete operators to mimic the continuous analysis. This involves three key components .

#### Summation-By-Parts Operators and Quadrature

A cornerstone of provably stable nodal DG schemes is the use of [quadrature rules](@entry_id:753909) whose nodes and weights support a **Summation-By-Parts (SBP)** property. For a discrete [differentiation matrix](@entry_id:149870) $D$ and a [diagonal mass matrix](@entry_id:173002) $M$ formed from the [quadrature weights](@entry_id:753910), the SBP property states that the matrix $Q = MD$ satisfies $Q + Q^T = B$, where $B$ is a boundary matrix that isolates values at the element boundaries. This algebraic identity is a discrete analogue of integration by parts, and it is the essential tool for manipulating discrete derivative terms in the stability proof.

A crucial insight of modern SBP-based analysis is that it bypasses the need for highly accurate quadrature of the nonlinear entropy function $U(\boldsymbol{u}_h)$. Older stability analyses sometimes required the [quadrature rule](@entry_id:175061) to be exact for polynomials of very high degree to control aliasing errors from the nonlinear terms. The SBP approach, however, relies on algebraic cancellation. The stability proof is achieved through the structure of the SBP operator and the specific construction of the [numerical fluxes](@entry_id:752791), not by exactly integrating the nonlinear entropy. The minimal requirement is that the quadrature supports the SBP property for the chosen polynomial approximation space .

#### Entropy Conservation in Volume Integrals

A naive discretization of the flux divergence term, $\nabla \cdot \boldsymbol{f}(\boldsymbol{u}_h)$, within each element can introduce spurious numerical errors that look like [entropy production](@entry_id:141771) or destruction, leading to instability. To prevent this, the [volume integrals](@entry_id:183482) are discretized using a special **flux differencing** or **split form**. This involves a **two-point entropy-conservative flux**, denoted $\boldsymbol{f}^{ec}(\boldsymbol{u}_L, \boldsymbol{u}_R)$, which is designed to satisfy a discrete version of the chain rule.

The defining property of this flux, often called Tadmor's identity, links it to the jump in the entropy variables $\boldsymbol{v}$ and the jump in an **entropy potential** $\psi(\boldsymbol{u}) = \boldsymbol{v}(\boldsymbol{u})^T \boldsymbol{f}(\boldsymbol{u}) - F(\boldsymbol{u})$. The condition for [entropy conservation](@entry_id:749018) at an interface (or between two nodes) is  :
$$ (\boldsymbol{v}_R - \boldsymbol{v}_L)^T \boldsymbol{f}^{ec}(\boldsymbol{u}_L, \boldsymbol{u}_R) = \psi(\boldsymbol{u}_R) - \psi(\boldsymbol{u}_L) $$
This identity ensures that the contribution of the flux term to the entropy change rate perfectly cancels the contribution from the [volume integrals](@entry_id:183482) when summed over a periodic domain, leading to a discrete conservation of entropy. When combined with the SBP property, this structure allows the volume contributions within each element to be expressed as terms on the element boundary, creating a [telescoping sum](@entry_id:262349) across the mesh. This reduces the entire entropy budget to contributions from element interfaces . As noted previously, explicit formulas for $\boldsymbol{f}^{ec}$ for systems like the Euler equations often depend on logarithmic means of quantities like density and pressure, again underscoring the necessity of positivity .

#### Controlled Dissipation at Interfaces

For problems involving shocks, mere [entropy conservation](@entry_id:749018) is insufficient; physical entropy dissipation must be modeled. In a DG framework, this dissipation is introduced at the interfaces between elements through the **numerical flux**, $\boldsymbol{f}^*(\boldsymbol{u}^-, \boldsymbol{u}^+)$. An **entropy-stable flux** is constructed by augmenting an entropy-conservative flux with a carefully crafted dissipation term. A general form is:
$$ \boldsymbol{f}^{es}(\boldsymbol{u}^-, \boldsymbol{u}^+) = \boldsymbol{f}^{ec}(\boldsymbol{u}^-, \boldsymbol{u}^+) - \frac{1}{2} \mathbf{D}(\boldsymbol{u}^-, \boldsymbol{u}^+) (\boldsymbol{v}^+ - \boldsymbol{v}^-) $$
Here, the dissipation matrix $\mathbf{D}$ must be symmetric and [positive semi-definite](@entry_id:262808). The added term's contribution to the interface entropy balance is $- \frac{1}{2} (\boldsymbol{v}^+ - \boldsymbol{v}^-)^T \mathbf{D} (\boldsymbol{v}^+ - \boldsymbol{v}^-) \le 0$. This guarantees that the interface flux removes entropy from the system, providing the necessary stabilization to capture shocks correctly . The combination of an entropy-conservative volume discretization and an entropy-stable interface flux produces a provably entropy-stable [semi-discretization](@entry_id:163562). The resulting total change in entropy is determined by the sum of these dissipative interface contributions and any physical fluxes at the domain's outer boundaries . Consistency requires that when the states on both sides of an interface are equal ($\boldsymbol{u}^- = \boldsymbol{u}^+$), the numerical entropy flux must equal the physical entropy flux, ensuring that the scheme converges to the correct continuous equation under [mesh refinement](@entry_id:168565) .

### Extensions of the Framework

The principles of [discrete entropy analysis](@entry_id:748504) can be extended to more complex scenarios, such as systems with non-zero source terms or the full [discretization](@entry_id:145012) in time.

#### Entropy-Compatible Source Terms

Many physical systems involve source terms, such as the [shallow water equations](@entry_id:175291) with a non-flat bottom bathymetry $b(x)$:
$$ \partial_t \boldsymbol{q} + \partial_x \boldsymbol{f}(\boldsymbol{q}) = \boldsymbol{s}(\boldsymbol{q},x), \qquad \boldsymbol{s}(\boldsymbol{q},x) = \begin{pmatrix} 0 \\ - g h \,\partial_x b(x) \end{pmatrix} $$
A key challenge for such systems is to design a scheme that is **well-balanced**, meaning it can preserve physically relevant steady states (like a lake at rest where velocity is zero and the water surface $h+b$ is flat) exactly at the discrete level. From an entropy perspective, this means the [numerical discretization](@entry_id:752782) of the source term must be **entropy-compatible**. Its contribution to the discrete entropy evolution must precisely cancel the entropy generated by the flux divergence term for these steady states. This is achieved by discretizing the [source term](@entry_id:269111) not in isolation, but in a manner that is consistent with the SBP operator and the two-point structure of the entropy-conservative flux used for the homogeneous part. This careful construction ensures that the discrete [integration by parts](@entry_id:136350) balances the source and flux terms, just as in the continuous case, leading to zero net [entropy production](@entry_id:141771) for steady states and maintaining stability for dynamic solutions .

#### Entropy-Stable Time Integration

A provably entropy-stable [semi-discretization](@entry_id:163562) $\frac{d\boldsymbol{u}_h}{dt} = \mathcal{L}(\boldsymbol{u}_h)$ is only half the solution. The [time integration](@entry_id:170891) method must also preserve this stability property. Not all [time-stepping schemes](@entry_id:755998) do. A common approach is to use **Strong-Stability-Preserving (SSP) Runge-Kutta** methods.

The key is that the spatial operator $\mathcal{L}$ is designed to satisfy a **forward Euler [entropy condition](@entry_id:166346)**: for a sufficiently small time step $\tau \le \Delta t_{\mathrm{FE}}$, a single forward Euler step is entropy non-increasing, i.e., $H(\boldsymbol{u}_h + \tau \mathcal{L}(\boldsymbol{u}_h)) \le H(\boldsymbol{u}_h)$, where $H$ is the total discrete entropy. SSP methods are advantageous because they can be expressed as a convex combination of forward Euler-like steps. For an SSP method with coefficient $\mathcal{C}_{\mathrm{SSP}}$, each internal stage is a convex combination of previous stages and forward Euler updates with an effective step size $\Delta t / \mathcal{C}_{\mathrm{SSP}}$.

Because the discrete entropy functional $H(\boldsymbol{u}_h)$ is a [convex function](@entry_id:143191) of the discrete solution, one can apply Jensen's inequality to the convex combination form of the SSP method. This allows one to prove by induction that if each internal forward Euler step is entropy non-increasing, then the entire multi-stage update is also entropy non-increasing. This holds provided the time step $\Delta t$ satisfies the condition $\Delta t \le \mathcal{C}_{\mathrm{SSP}} \Delta t_{\mathrm{FE}}$. This elegant result connects the stability of the spatial operator to the fully-discrete scheme, ensuring that the desired [entropy stability](@entry_id:749023) is maintained through the time-stepping process .