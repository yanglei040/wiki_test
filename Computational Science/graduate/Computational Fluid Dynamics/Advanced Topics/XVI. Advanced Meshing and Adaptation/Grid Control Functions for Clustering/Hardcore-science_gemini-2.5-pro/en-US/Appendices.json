{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is foundational to understanding adaptive grid generation. You will implement the one-dimensional discrete equidistribution principle from first principles, translating the continuous mathematical concept into a working numerical algorithm. This practice is essential for building an intuition for how monitor functions control node clustering and provides a robust framework for testing different clustering strategies .",
            "id": "3325995",
            "problem": "Consider the one-dimensional mesh generation problem on a closed interval where the goal is to place nodes to achieve controlled clustering in regions of interest according to the equidistribution principle. The fundamental base is the following. For a bounded interval $[a,b]$ and a strictly positive monitor function $m(x)$, the equidistribution principle prescribes a mesh mapping $x(s)$ from a computational coordinate $s \\in [0,1]$ to the physical coordinate $x \\in [a,b]$ such that the measure induced by $m(x)$ is evenly distributed among the mesh cells. In continuous form, this requirement can be expressed as a statement that the density of cells is controlled by $m(x)$, and that equal portions of the total monitor measure are assigned to each cell. In discrete form, this principle states that the discrete monitor-weighted measure per cell should be equal across all cells.\n\nYou must implement a discrete equidistribution algorithm that constructs node locations $\\{x_i\\}_{i=0}^N$ on $[a,b]$ for a given strictly positive monitor function $m(x)$ so that a discrete approximation of the monitor-weighted measures per cell is approximately equal across all cells. The algorithm must rely on first principles: approximate the cumulative monitor measure over $[a,b]$ by a nonnegative, increasing function constructed from $m(x)$, then invert this cumulative function at uniformly distributed levels to obtain node positions $\\{x_i\\}_{i=0}^N$. The reasoning must start from the core definition of equidistribution and avoid shortcuts.\n\nThe discrete equidistribution must be implemented as follows:\n- Construct a fine reference grid $\\{x^{\\mathrm{ref}}_j\\}_{j=0}^{J}$ on $[a,b]$ with $x^{\\mathrm{ref}}_0=a$ and $x^{\\mathrm{ref}}_{J}=b$, and approximate the cumulative monitor measure $C(x)$ using a consistent numerical quadrature of $m(x)$ on the reference grid. The approximation must yield a nondecreasing array $\\{C_j\\}_{j=0}^{J}$ with $C_0=0$ and $C_J \\approx \\int_a^b m(\\xi)\\,d\\xi$.\n- Compute the target levels $\\{T_i\\}_{i=0}^{N}$ with $T_i = \\frac{i}{N} C_J$.\n- Obtain the node positions $\\{x_i\\}_{i=0}^N$ by inverting the discrete cumulative mapping, i.e., by finding $x_i$ such that $C(x_i) \\approx T_i$, using a stable monotone interpolation from $\\{(C_j, x^{\\mathrm{ref}}_j)\\}$.\n\nAfter constructing $\\{x_i\\}$, evaluate the quality of equidistribution and clustering with two quantitative metrics:\n- The mesh size ratio $R = \\frac{\\max_i \\Delta x_i}{\\min_i \\Delta x_i}$ where $\\Delta x_i = x_{i+1}-x_i$ for $i=0,\\dots,N-1$.\n- The discrete equidistribution residual defined by $E_\\infty = \\max_{i} \\left| \\frac{w_i - W_{\\mathrm{eq}}}{W_{\\mathrm{eq}}} \\right|$, where $w_i$ is the trapezoidal approximation of the monitor-weighted measure over cell $[x_i,x_{i+1}]$, i.e., $w_i \\approx \\frac{1}{2}\\left(m(x_i)+m(x_{i+1})\\right)\\Delta x_i$, and $W_{\\mathrm{eq}} = \\frac{1}{N} \\int_a^b m(\\xi)\\,d\\xi$.\n\nUse the following test suite of monitor functions and parameters to construct $4$ meshes and report the metrics above:\n- Test case $1$ (Gaussian clustering on a unit interval): $[a,b]=[0,1]$, $N=40$, $m(x) = 1 + A \\exp\\left(-\\frac{(x-x_0)^2}{2\\sigma^2}\\right)$ with $A=9$, $x_0=0.5$, $\\sigma=0.08$.\n- Test case $2$ (piecewise-constant bump): $[a,b]=[0,1]$, $N=40$, $m(x) = 1 + A$ if $x \\in [x_1,x_2]$ and $m(x)=1$ otherwise, where $A=4$, $x_1=0.45$, $x_2=0.55$.\n- Test case $3$ (boundary-layer-type clustering near the left boundary): $[a,b]=[0,1]$, $N=60$, $m(x) = 1 + \\frac{A}{(x+\\varepsilon)^p}$ with $A=0.2$, $\\varepsilon=10^{-3}$, $p=0.5$.\n- Test case $4$ (oscillatory monitor with guaranteed positivity): $[a,b]=[0,1]$, $N=50$, $m(x) = \\varepsilon + \\sin^2(2\\pi x)$ with $\\varepsilon=0.1$.\n\nAll quantities in this problem are dimensionless, so no physical units are required.\n\nYour program must compute, for each test case, the pair $[R, E_\\infty]$ defined above. The final program output must be a single line containing the results for the $4$ test cases aggregated in a single comma-separated list of lists, with no spaces, in the format $[[R_1,E_{\\infty,1}],[R_2,E_{\\infty,2}],[R_3,E_{\\infty,3}],[R_4,E_{\\infty,4}]]$, where $R_k$ and $E_{\\infty,k}$ are the values for test case $k$.\n\nDesign for coverage:\n- The Gaussian case tests smooth clustering.\n- The piecewise-constant bump tests discontinuity handling.\n- The boundary-layer case tests strong clustering near a domain endpoint with an integrable singularity.\n- The oscillatory monitor tests positivity maintenance in regions where the base function may be near zero.\n\nYour solution should be derived from the equidistribution principle and the core definitions above, must be self-contained, and must produce the exact required output format.",
            "solution": "The problem of generating a one-dimensional grid that clusters nodes according to a specified density is a cornerstone of adaptive numerical methods. The equidistribution principle provides a rigorous mathematical foundation for this task. The solution presented herein follows the specified discrete algorithm, which is a direct numerical implementation of this principle.\n\nThe equidistribution principle states that for a given strictly positive monitor function $m(x)$ on a physical domain $[a,b]$, the optimal node distribution $\\{x_i\\}_{i=0}^N$ is one where the integral of the monitor function over each mesh cell $[x_i, x_{i+1}]$ is constant. Mathematically, this is expressed as:\n$$ \\int_{x_i}^{x_{i+1}} m(\\xi) \\,d\\xi = \\text{constant} \\quad \\text{for } i = 0, 1, \\dots, N-1 $$\nThe constant value must be the total \"monitor mass\" divided by the number of cells, $N$. The total mass is the integral of the monitor function over the entire domain, $\\int_a^b m(\\xi) \\,d\\xi$. Thus, for each cell, we have:\n$$ \\int_{x_i}^{x_{i+1}} m(\\xi) \\,d\\xi = \\frac{1}{N} \\int_a^b m(\\xi) \\,d\\xi $$\nTo find the position of a specific node $x_i$, we can sum the contributions from all cells up to that node:\n$$ \\sum_{k=0}^{i-1} \\int_{x_k}^{x_{k+1}} m(\\xi) \\,d\\xi = \\sum_{k=0}^{i-1} \\frac{1}{N} \\int_a^b m(\\xi) \\,d\\xi $$\n$$ \\int_a^{x_i} m(\\xi) \\,d\\xi = \\frac{i}{N} \\int_a^b m(\\xi) \\,d\\xi $$\nThis equation is central to the method. We define the cumulative monitor function $C(x)$ as the integral of $m(\\xi)$ from the start of the domain to a point $x$:\n$$ C(x) = \\int_a^x m(\\xi) \\,d\\xi $$\nWith this definition, the equidistribution condition for node $x_i$ becomes:\n$$ C(x_i) = \\frac{i}{N} C(b) $$\nSince $m(x)$ is strictly positive, $C(x)$ is a strictly increasing function and therefore possesses a unique inverse, $C^{-1}$. The location of each node $x_i$ can then be found by inverting the cumulative function:\n$$ x_i = C^{-1}\\left(\\frac{i}{N} C(b)\\right) $$\nThis equation demonstrates that the nodes $\\{x_i\\}$ in the physical space $[a,b]$ are the images of a set of uniformly spaced points in the \"monitor-weighted\" or computational space $[0, C(b)]$.\n\nThe specified algorithm is a discrete realization of this inversion process. It consists of the following steps:\n\n1.  **Approximation of the Cumulative Monitor Function**: The continuous function $C(x)$ is not known analytically in general. We approximate it numerically. First, a fine, uniform reference grid $\\{x^{\\mathrm{ref}}_j\\}_{j=0}^{J}$ is created on the interval $[a,b]$, with $x^{\\mathrm{ref}}_j = a + j(b-a)/J$. The monitor function $m(x)$ is evaluated at these points to obtain $\\{m_j = m(x^{\\mathrm{ref}}_j)\\}$. The cumulative integral $C(x)$ is then approximated using a numerical quadrature rule. We use the trapezoidal rule, which yields a discrete cumulative function array $\\{C_j\\}_{j=0}^{J}$, where $C_j \\approx C(x^{\\mathrm{ref}}_j)$. This is computed as $C_0=0$ and for $j > 0$:\n    $$ C_j = \\sum_{k=1}^{j} \\frac{1}{2} (m_{k-1} + m_k) (x^{\\mathrm{ref}}_k - x^{\\mathrm{ref}}_{k-1}) $$\n    The final value, $C_J$, serves as the numerical approximation of the total integral $\\int_a^b m(\\xi)\\,d\\xi$. This discrete function $\\{C_j\\}$ is guaranteed to be nondecreasing since $m(x) > 0$.\n\n2.  **Computation of Target Levels**: Following the principle $C(x_i) = \\frac{i}{N} C(b)$, we define a set of $N+1$ uniformly spaced target levels $\\{T_i\\}_{i=0}^N$ in the range of the approximate cumulative function $[0, C_J]$:\n    $$ T_i = \\frac{i}{N} C_J \\quad \\text{for } i = 0, 1, \\dots, N $$\n\n3.  **Inversion via Interpolation**: The final step is to find the physical node positions $\\{x_i\\}$ that correspond to these target levels. This is an inversion of the discrete mapping we constructed, which is defined by the pairs $\\{(C_j, x^{\\mathrm{ref}}_j)\\}$. We seek $x_i$ such that $C(x_i) \\approx T_i$. This is achieved using stable monotone interpolation. Linear interpolation is a suitable choice. For each target level $T_i$, we find its position within the array $\\{C_j\\}$ and linearly interpolate the corresponding values in $\\{x^{\\mathrm{ref}}_j\\}$ to find an estimate for $x_i$. This procedure effectively approximates the inverse function $C^{-1}$.\n\nAfter generating the mesh $\\{x_i\\}_{i=0}^N$, its quality is evaluated using two metrics:\n\n-   **Mesh Size Ratio ($R$)**: This metric quantifies the degree of mesh stretching or clustering. It is defined as the ratio of the largest cell size to the smallest cell size:\n    $$ R = \\frac{\\max_{i} \\Delta x_i}{\\min_{i} \\Delta x_i}, \\quad \\text{where } \\Delta x_i = x_{i+1} - x_i $$\n    A value of $R=1$ corresponds to a uniform mesh, while large values of $R$ indicate strong clustering.\n\n-   **Discrete Equidistribution Residual ($E_\\infty$)**: This metric measures how well the generated mesh satisfies the discrete version of the equidistribution principle. It is the maximum relative deviation of the monitor-weighted measure in any cell from the ideal, uniform measure:\n    $$ E_\\infty = \\max_{i} \\left| \\frac{w_i - W_{\\mathrm{eq}}}{W_{\\mathrm{eq}}} \\right| $$\n    Here, $w_i$ is the trapezoidal approximation of the integral of $m(x)$ over the cell $[x_i, x_{i+1}]$:\n    $$ w_i = \\frac{1}{2} (m(x_i) + m(x_{i+1})) \\Delta x_i $$\n    And $W_{\\mathrm{eq}}$ is the ideal, constant measure per cell, defined using the total numerically-computed monitor integral $C_J$:\n    $$ W_{\\mathrm{eq}} = \\frac{C_J}{N} $$\n    A small value of $E_\\infty$ indicates that the algorithm has successfully produced a mesh that is very close to being perfectly equidistributed.\n\nThe provided Python code implements this algorithm and computes these metrics for the four specified test cases, each designed to test the algorithm's robustness against different types of monitor functions (smooth, discontinuous, singular, and oscillatory).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates 1D meshes based on the equidistribution principle for a suite of test cases\n    and evaluates them using specified quality metrics.\n    \"\"\"\n\n    def generate_and_evaluate(a, b, N, m_func, J_ref=20000):\n        \"\"\"\n        Core function to generate a mesh and compute metrics for a single test case.\n\n        Args:\n            a (float): Start of the interval.\n            b (float): End of the interval.\n            N (int): Number of mesh cells.\n            m_func (callable): The monitor function m(x).\n            J_ref (int): Number of intervals in the fine reference grid.\n\n        Returns:\n            tuple: A tuple containing the mesh size ratio (R) and the\n                   equidistribution residual (E_inf).\n        \"\"\"\n        # Step 1: Approximate the cumulative monitor function\n        # Create a fine reference grid\n        x_ref = np.linspace(a, b, J_ref + 1)\n        # Evaluate the monitor function on the reference grid\n        m_ref = m_func(x_ref)\n\n        # Compute the discrete cumulative integral using the trapezoidal rule\n        dx_ref = (b - a) / J_ref\n        integrals = 0.5 * (m_ref[:-1] + m_ref[1:]) * dx_ref\n        C_ref = np.concatenate(([0.0], np.cumsum(integrals)))\n        C_total = C_ref[-1]\n\n        # Step 2: Compute target equidistribution levels\n        T_i = np.linspace(0.0, C_total, N + 1)\n\n        # Step 3: Invert the mapping via linear interpolation to find node positions\n        x_nodes = np.interp(T_i, C_ref, x_ref)\n\n        # Ensure boundary nodes are exact\n        x_nodes[0] = a\n        x_nodes[-1] = b\n\n        # --- Metric Calculation ---\n\n        # Calculate cell sizes\n        delta_x = x_nodes[1:] - x_nodes[:-1]\n\n        # Metric 1: Mesh size ratio R\n        min_delta_x = np.min(delta_x)\n        if min_delta_x <= 0:\n            # This should not occur with a strictly positive monitor function\n            R = np.inf\n        else:\n            R = np.max(delta_x) / min_delta_x\n\n        # Metric 2: Discrete equidistribution residual E_inf\n        m_at_nodes = m_func(x_nodes)\n        # Calculate the monitor-weighted measure for each cell\n        w_i = 0.5 * (m_at_nodes[:-1] + m_at_nodes[1:]) * delta_x\n        # Calculate the ideal, constant measure per cell\n        W_eq = C_total / N\n        \n        # Calculate the maximum relative deviation\n        if W_eq <= 0:\n            E_inf = np.inf\n        else:\n            E_inf = np.max(np.abs(w_i / W_eq - 1.0))\n\n        return R, E_inf\n\n    # Define the test suite of monitor functions and parameters\n    test_cases = [\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 40,\n            \"m_func\": lambda x: 1.0 + 9.0 * np.exp(-((x - 0.5)**2) / (2 * 0.08**2))\n        },\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 40,\n            \"m_func\": lambda x: 1.0 + 4.0 * np.logical_and(x >= 0.45, x <= 0.55)\n        },\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 60,\n            \"m_func\": lambda x: 1.0 + 0.2 / ((x + 1e-3)**0.5)\n        },\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 50,\n            \"m_func\": lambda x: 0.1 + np.sin(2 * np.pi * x)**2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        R, E_inf = generate_and_evaluate(case[\"a\"], case[\"b\"], case[\"N\"], case[\"m_func\"])\n        results.append(f\"[{R},{E_inf}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from one-dimensional isotropic clustering, this next practice explores the power of anisotropic adaptation in two dimensions. You will construct a metric tensor designed to align mesh elements with a sharp material interface, a common requirement in multiphase flow simulations. This exercise directly connects the geometric concept of a metric tensor to a critical numerical objective: minimizing artificial diffusion to preserve the sharpness of advected features .",
            "id": "3325976",
            "problem": "Consider a two-dimensional interface-advection problem in Computational Fluid Dynamics (CFD). A sharp material interface is embedded in a uniform flow with velocity magnitude $U$ and direction angle $\\phi$ (in radians). The interface at a point has curvature $\\kappa$ (with units $\\text{m}^{-1}$) and a tangent direction angle $\\theta$ (in radians). The local mesh is controlled by a symmetric positive-definite metric tensor $M$ designed to enforce an element aspect ratio $AR$ aligned with the principal curvature directions: the tangent and the normal of the interface. The metric is defined by $M = \\mathbf{R}^\\top \\,\\text{diag}(m_\\parallel,m_\\perp)\\,\\mathbf{R}$, where $\\mathbf{R}$ is the orthonormal rotation mapping the global basis into the tangent-normal basis, $m_\\parallel$ controls size along the tangent, and $m_\\perp$ controls size along the normal. The ratio constraint is $m_\\perp / m_\\parallel = \\chi |\\kappa|$, where $\\chi$ is a user-specified clustering sensitivity with units $\\text{m}$.\n\nAssume the mesh-control convention $m_\\parallel = 1/h_0^2$ with $h_0$ the target isotropic spacing (in $\\text{m}$). The directional spacings implied by $M$ are $h_\\parallel = h_0$ and $h_\\perp = h_0 / \\sqrt{m_\\perp / m_\\parallel} = h_0 / \\sqrt{\\chi |\\kappa|}$. Enforce a bounded aspect ratio by defining $AR = h_\\parallel / h_\\perp = \\sqrt{\\chi |\\kappa|}$ and applying $AR \\leftarrow \\min(AR_{\\max},\\max(AR,1))$ to satisfy $AR \\ge 1$ and $AR \\le AR_{\\max}$ for a given $AR_{\\max}$ (dimensionless).\n\nThe transport of a passive scalar by linear advection is discretized using a first-order upwind finite volume method with uniform time step $\\Delta t$ chosen such that the Courant number $\\nu = U \\Delta t / h_0$ is constant across the mesh. The modified equation analysis for first-order upwind in one spatial dimension yields an artificial diffusion term with effective diffusion coefficient $D_{\\text{num}} = \\tfrac{1}{2} |a| \\Delta x (1 - \\nu)$, where $a$ is the advection speed and $\\Delta x$ is the mesh spacing. In two dimensions, take the diffusion along a direction $\\mathbf{\\hat{d}}$ to be $D_{\\text{num}}(\\mathbf{\\hat{d}}) = \\tfrac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{d}}| h_{\\mathbf{\\hat{d}}} (1 - \\nu)$, where $\\mathbf{u}$ is the velocity vector and $h_{\\mathbf{\\hat{d}}}$ is the spacing along $\\mathbf{\\hat{d}}$. Focusing on the normal direction $\\mathbf{\\hat{n}}$ to the interface, the isotropic mesh uses $h_\\perp^{\\text{iso}} = h_0$, while the anisotropic mesh uses $h_\\perp^{\\text{aniso}} = h_0 / AR$. The numerical diffusion reduction factor is defined as the ratio $R = D_{\\text{num}}^{\\text{iso}}(\\mathbf{\\hat{n}}) / D_{\\text{num}}^{\\text{aniso}}(\\mathbf{\\hat{n}})$. If $|\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| = 0$ or $\\nu = 1$, define $R = 0$ because there is no numerical diffusion to reduce.\n\nTasks:\n- Starting from the modified equation for first-order upwind advection and the definition of the metric tensor $M$ with the given constraint $m_\\perp / m_\\parallel = \\chi |\\kappa|$, derive the expression of $R$ in terms of $AR$, $\\nu$, and the normal component of $\\mathbf{u}$.\n- Implement a program that:\n  1. Constructs the rotation matrix $\\mathbf{R}$ from $\\theta$, with columns given by the tangent unit vector $\\mathbf{\\hat{t}} = [\\cos\\theta, \\sin\\theta]^\\top$ and the normal unit vector $\\mathbf{\\hat{n}} = [-\\sin\\theta, \\cos\\theta]^\\top$.\n  2. Builds $M$ from $\\mathbf{R}$, $m_\\parallel = 1/h_0^2$, and $m_\\perp = m_\\parallel \\, AR^2$ after applying the aspect ratio bounds.\n  3. Computes $R$ using the normal component $u_n = \\mathbf{u}\\cdot\\mathbf{\\hat{n}}$, the Courant number $\\nu$, and the directional spacings.\n- Use the following test suite of parameter sets, each ordered as $(U, h_0, \\nu, \\kappa, \\chi, \\theta, \\phi, AR_{\\max})$ and expressed in the units indicated: $U$ in $\\text{m/s}$, $h_0$ in $\\text{m}$, $\\nu$ dimensionless, $\\kappa$ in $\\text{m}^{-1}$, $\\chi$ in $\\text{m}$, $\\theta$ and $\\phi$ in radians, $AR_{\\max}$ dimensionless.\n  1. $(U = 1.0, h_0 = 0.01, \\nu = 0.5, \\kappa = 5.0, \\chi = 0.5, \\theta = 0.5235987756, \\phi = 1.0471975512, AR_{\\max} = 6.0)$\n  2. $(U = 1.0, h_0 = 0.01, \\nu = 0.5, \\kappa = 5.0, \\chi = 0.5, \\theta = 0.5235987756, \\phi = 0.5235987756, AR_{\\max} = 6.0)$\n  3. $(U = 2.0, h_0 = 0.02, \\nu = 0.7, \\kappa = 100.0, \\chi = 0.5, \\theta = 1.2, \\phi = 0.7, AR_{\\max} = 5.0)$\n  4. $(U = 1.5, h_0 = 0.015, \\nu = 0.6, \\kappa = 0.0, \\chi = 0.5, \\theta = 0.7853981634, \\phi = 0.0, AR_{\\max} = 10.0)$\n  5. $(U = 1.0, h_0 = 0.01, \\nu = 1.0, \\kappa = 10.0, \\chi = 0.3, \\theta = 0.2, \\phi = 1.0, AR_{\\max} = 8.0)$\n\nYour program should produce a single line of output containing the numerical diffusion reduction factors $R$ for the five test cases as a comma-separated list of decimal floats with six digits after the decimal point, enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$). Angles must be interpreted in radians, and all physical quantities must be handled with their specified units. The outputs $R$ are dimensionless.",
            "solution": "The problem requires the derivation and computation of a numerical diffusion reduction factor, $R$, for an interface-advection problem in computational fluid dynamics. The solution proceeds in two stages: first, an analytical derivation of the expression for $R$, and second, the implementation of a computational procedure to evaluate $R$ for a given set of test parameters.\n\n**Derivation of the Reduction Factor $R$**\n\nThe numerical diffusion reduction factor $R$ is defined as the ratio of the numerical diffusion coefficient in the interface-normal direction for an isotropic reference mesh to that of the metric-aligned anisotropic mesh.\n$$\nR = \\frac{D_{\\text{num}}^{\\text{iso}}(\\mathbf{\\hat{n}})}{D_{\\text{num}}^{\\text{aniso}}(\\mathbf{\\hat{n}})}\n$$\nThe problem specifies the formula for the numerical diffusion coefficient along an arbitrary direction $\\mathbf{\\hat{d}}$ as:\n$$\nD_{\\text{num}}(\\mathbf{\\hat{d}}) = \\frac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{d}}| h_{\\mathbf{\\hat{d}}} (1 - \\nu)\n$$\nwhere $\\mathbf{u}$ is the fluid velocity vector, $h_{\\mathbf{\\hat{d}}}$ is the local mesh spacing in direction $\\mathbf{\\hat{d}}$, and $\\nu$ is the reference Courant number.\n\nFor the isotropic case, the mesh spacing is uniform, $h_{\\mathbf{\\hat{d}}} = h_0$ for all directions. The spacing normal to the interface is therefore $h_\\perp^{\\text{iso}} = h_0$. The corresponding numerical diffusion coefficient is:\n$$\nD_{\\text{num}}^{\\text{iso}}(\\mathbf{\\hat{n}}) = \\frac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| h_0 (1 - \\nu)\n$$\nFor the anisotropic case, the mesh is refined in the normal direction. The spacing is given by $h_\\perp^{\\text{aniso}} = h_0 / AR$, where $AR$ is the bounded aspect ratio. The numerical diffusion coefficient in this case is:\n$$\nD_{\\text{num}}^{\\text{aniso}}(\\mathbf{\\hat{n}}) = \\frac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| \\left(\\frac{h_0}{AR}\\right) (1 - \\nu)\n$$\nSubstituting these expressions into the definition of $R$ yields:\n$$\nR = \\frac{\\frac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| h_0 (1 - \\nu)}{\\frac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| \\frac{h_0}{AR} (1 - \\nu)}\n$$\nThe term $\\frac{1}{2} |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| (1 - \\nu)$ represents the magnitude of numerical diffusion effects. If this term is non-zero, it can be cancelled from the numerator and denominator, simplifying the expression significantly:\n$$\nR = \\frac{h_0}{h_0 / AR} = AR\n$$\nThe problem explicitly handles the case where this common factor is zero by stating: \"If $|\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| = 0$ or $\\nu = 1$, define $R = 0$\". This is physically sound, as these conditions imply that the numerical diffusion for the first-order upwind scheme is zero, hence there is no diffusion to reduce.\n\nTherefore, the complete expression for $R$ is:\n$$\nR =\n\\begin{cases}\nAR & \\text{if } |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| \\neq 0 \\text{ and } \\nu \\neq 1 \\\\\n0 & \\text{if } |\\mathbf{u}\\cdot \\mathbf{\\hat{n}}| = 0 \\text{ or } \\nu = 1\n\\end{cases}\n$$\nwhere $AR$ is the bounded aspect ratio.\n\n**Computational Algorithm**\n\nThe algorithm to compute $R$ for each test case $(U, h_0, \\nu, \\kappa, \\chi, \\theta, \\phi, AR_{\\max})$ is as follows:\n\n1.  Calculate the raw aspect ratio, $AR_{\\text{raw}}$, from the interface curvature $\\kappa$ and the clustering sensitivity $\\chi$:\n    $$AR_{\\text{raw}} = \\sqrt{\\chi |\\kappa|}$$\n2.  Apply the specified bounds $AR \\ge 1$ and $AR \\le AR_{\\max}$ to find the effective aspect ratio, $AR$:\n    $$AR = \\min(AR_{\\max}, \\max(AR_{\\text{raw}}, 1.0))$$\n3.  Evaluate the conditions for the special case $R=0$.\n    a. Check if the Courant number $\\nu = 1.0$.\n    b. Compute the normal component of the velocity, $u_n = \\mathbf{u}\\cdot \\mathbf{\\hat{n}}$. The velocity vector is $\\mathbf{u} = [U \\cos\\phi, U \\sin\\phi]^\\top$ and the interface normal vector is $\\mathbf{\\hat{n}} = [-\\sin\\theta, \\cos\\theta]^\\top$. Their dot product is:\n    $$u_n = U(-\\cos\\phi \\sin\\theta + \\sin\\phi \\cos\\theta) = U \\sin(\\phi - \\theta)$$\n    Check if $|u_n|$ is numerically zero.\n4.  If either $\\nu = 1.0$ or $|u_n| \\approx 0$, the reduction factor is $R = 0.0$.\n5.  Otherwise, the reduction factor is equal to the bounded aspect ratio: $R = AR$.\n\nThis procedure is implemented for each of the five test cases provided. While the problem describes the construction of the full metric tensor $M$, the derived expression for $R$ demonstrates that only the resulting aspect ratio $AR$ and the flow kinematics relative to the interface normal are required for the final computation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CFD numerical diffusion reduction factor problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (U, h0, nu, kappa, chi, theta, phi, AR_max)\n    test_cases = [\n        (1.0, 0.01, 0.5, 5.0, 0.5, 0.5235987756, 1.0471975512, 6.0),\n        (1.0, 0.01, 0.5, 5.0, 0.5, 0.5235987756, 0.5235987756, 6.0),\n        (2.0, 0.02, 0.7, 100.0, 0.5, 1.2, 0.7, 5.0),\n        (1.5, 0.015, 0.6, 0.0, 0.5, 0.7853981634, 0.0, 10.0),\n        (1.0, 0.01, 1.0, 10.0, 0.3, 0.2, 1.0, 8.0)\n    ]\n\n    results = []\n    # A small tolerance for floating point comparisons to zero.\n    TOLERANCE = 1e-9\n\n    for case in test_cases:\n        U, h0, nu, kappa, chi, theta, phi, AR_max = case\n\n        # Step 1: Calculate the raw aspect ratio.\n        # AR_raw = sqrt(chi * |kappa|)\n        ar_raw = np.sqrt(chi * np.abs(kappa))\n\n        # Step 2: Apply the bounds to obtain the effective aspect ratio AR.\n        # AR = min(AR_max, max(AR_raw, 1.0))\n        ar_bounded = np.min([AR_max, np.max([ar_raw, 1.0])])\n\n        # Step 3: Evaluate the conditions for setting R=0.\n        \n        # Condition a: Check if Courant number nu is 1.\n        is_nu_one = np.isclose(nu, 1.0, atol=TOLERANCE)\n\n        # Condition b: Check if the normal component of velocity is zero.\n        # u_n = U * sin(phi - theta)\n        u_n = U * np.sin(phi - theta)\n        is_un_zero = np.isclose(u_n, 0.0, atol=TOLERANCE)\n\n        # Steps 4 and 5: Calculate the reduction factor R.\n        # If nu=1 or |u_n|=0, then R=0. Otherwise, R=AR.\n        if is_nu_one or is_un_zero:\n            R = 0.0\n        else:\n            R = ar_bounded\n        \n        results.append(R)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The final practice elevates the discussion from implementing adaptation to intelligently designing it. Here, the focus shifts to deriving the monitor function itself from the physics of the problem, specifically by analyzing the behavior of an incompressible flow solver. This exercise challenges you to devise a grid control strategy based on the pressure Poisson equation's residual, demonstrating how targeted clustering can be used not just to improve accuracy but also to accelerate the convergence of the entire simulation .",
            "id": "3325924",
            "problem": "An incompressible projection method for the two-dimensional lid-driven cavity at Reynolds number $Re=10^4$ advances velocity by solving a pressure Poisson partial differential equation (PDE). With constant density $\\rho$, let the pressure Poisson residual be defined pointwise as $R_p(\\mathbf{x})=\\Delta p(\\mathbf{x}) - \\nabla\\cdot\\left(\\rho\\,\\mathbf{u}(\\mathbf{x})\\cdot\\nabla \\mathbf{u}(\\mathbf{x})\\right)$, evaluated using the current discrete solution and discrete operators. Consider a grid adaptation framework in Computational Fluid Dynamics (CFD) in which a positive grid control (monitor) function $M(\\mathbf{x})$ drives node clustering via an elliptic equidistribution principle so that, along computational coordinates, the integral of $M$ is approximately uniform. You are tasked to construct $M$ from $R_p$ to target regions that most affect the accuracy and convergence of the pressure solve within the projection method and to assess whether clustering towards large $|R_p|$ accelerates the overall convergence to a divergence-free state (at fixed number of control volumes) for the lid-driven cavity flow at $Re=10^4$.\n\nBase your reasoning on the governing incompressible Navier–Stokes equations, the projection method structure, properties of elliptic solves for the pressure, and the equidistribution rationale that clustering should follow a surrogate for local truncation error or residual intensity. You may appeal to smoothing and normalization of $R_p$ if justified to maintain robustness and well-posed equidistribution. Assume that the mesh generator enforces $M(\\mathbf{x}) \\ge c > 0$ and that excessive anisotropy can deteriorate elliptic solver conditioning.\n\nWhich of the following constructions of $M(\\mathbf{x})$ and associated claim about convergence is most consistent with these principles and would be expected to accelerate convergence of the projection method on the lid-driven cavity at $Re=10^4$ when compared against a uniform grid with the same number of control volumes?\n\nA. Define $M(\\mathbf{x}) = 1 + \\alpha\\,\\widetilde{R}(\\mathbf{x})$ where $\\widetilde{R}(\\mathbf{x}) = S_\\ell\\!\\left(\\dfrac{|R_p(\\mathbf{x})|}{\\langle |R_p|\\rangle_\\Omega + \\varepsilon}\\right)$, with $S_\\ell$ a local smoothing operator of width $\\ell$ comparable to the local cell size, $\\langle\\cdot\\rangle_\\Omega$ a domain average, $\\varepsilon>0$ a small bound for positivity, and $\\alpha>0$ a user-set clustering intensity. Equidistribution of $M$ concentrates nodes near large $|R_p|$ regions (shear layers and corners), reducing local truncation error of the pressure Poisson PDE and thereby decreasing the number of projection iterations required to reach a divergence-free tolerance; net effect: faster overall convergence at fixed degrees of freedom.\n\nB. Define $M(\\mathbf{x}) = \\left(1 + \\alpha\\,\\dfrac{|R_p(\\mathbf{x})|}{\\max_\\Omega |R_p| + \\varepsilon}\\right)^{-1}$ so that cells are coarsened where $|R_p|$ is large and refined where $|R_p|$ is small. This inverts the residual weighting to lessen stiffness near strong features; net effect: faster pressure solver convergence because the operator becomes more diagonally dominant.\n\nC. Define $M(\\mathbf{x}) = 1 + \\alpha\\,|\\nabla\\cdot \\mathbf{u}(\\mathbf{x})|$ so that nodes follow violations of incompressibility directly; since the projection method enforces $\\nabla\\cdot \\mathbf{u}\\approx 0$ in a few steps, the adaptation ceases to move, avoiding unnecessary clustering; net effect: faster convergence because the mesh remains near-uniform and well-conditioned.\n\nD. Define $M(\\mathbf{x}) = 1 + \\alpha\\,\\dfrac{|\\nabla R_p(\\mathbf{x})|}{\\langle |\\nabla R_p|\\rangle_\\Omega}$ without any smoothing, allowing very sharp clustering at residual fronts; such aggressive adaptation aligns cells with residual gradients, which is where discretization errors originate; net effect: faster convergence because sharp clustering always improves elliptic solver rates on the pressure Poisson PDE.\n\nSelect the option that most accurately reflects the construction of $M$ from $R_p$ and the expected impact on convergence, justified by the stated principles. Here $\\alpha>0$, $\\ell>0$, and $\\varepsilon>0$ are user-chosen constants, and all other symbols have their standard meaning in incompressible flow numerics. Assume Partial Differential Equation (PDE) discretizations are at least second order in smooth regions and that the projection method employs a standard multigrid-preconditioned Krylov solver whose convergence is sensitive to both truncation error distribution and operator conditioning.",
            "solution": "Begin with the incompressible Navier–Stokes equations for velocity $\\mathbf{u}$ and pressure $p$ with constant density $\\rho$ and kinematic viscosity $\\nu$,\n$$\n\\frac{\\partial \\mathbf{u}}{\\partial t} + \\mathbf{u}\\cdot \\nabla \\mathbf{u} = -\\frac{1}{\\rho}\\nabla p + \\nu \\Delta \\mathbf{u}, \\quad \\nabla \\cdot \\mathbf{u} = 0.\n$$\nIn a projection method, an intermediate velocity $\\mathbf{u}^\\star$ is computed without pressure, followed by a pressure correction obtained from a pressure Poisson PDE so that the updated velocity $\\mathbf{u}^{n+1}$ satisfies $\\nabla\\cdot \\mathbf{u}^{n+1}=0$. For constant $\\rho$, the pressure solve has the form\n$$\n\\Delta p = \\mathcal{F}(\\mathbf{u}),\n$$\nwith a right-hand side $\\mathcal{F}(\\mathbf{u})$ derived from $\\nabla\\cdot \\mathbf{u}^\\star$ for time-accurate projection, or, for steady-state formulations, from $-\\rho\\,\\nabla\\cdot(\\mathbf{u}\\cdot\\nabla \\mathbf{u})$ once viscous terms are accounted for. The pointwise residual\n$$\nR_p(\\mathbf{x}) = \\Delta p(\\mathbf{x}) - \\nabla\\cdot\\left(\\rho\\,\\mathbf{u}(\\mathbf{x})\\cdot\\nabla \\mathbf{u}(\\mathbf{x})\\right)\n$$\nis thus a natural measure of imbalance in the steady (or quasi-steady) pressure Poisson PDE. In practice, $R_p$ is computed with discrete operators, so its magnitude reflects both local truncation error and under-resolution of sharp features such as shear layers and corner singularities in the lid-driven cavity at $Re=10^4$.\n\nA grid control (monitor) function $M(\\mathbf{x})$ in an elliptic equidistribution framework is designed so that, when integrated along computational coordinates, its cumulative integral is uniform. The heuristic from equidistribution is to choose $M$ proportional to a scalar field that correlates with local error density; then, clustering where $M$ is large reduces local discretization error, which, for elliptic PDEs, improves the uniformity of the residual and often accelerates solver convergence by alleviating localized stiffness. However, two countervailing effects must be managed: positivity and smoothness of $M$ to avoid mesh tangling, and normalization to avoid pathological concentration driven by outliers. Smoothing over a length $\\ell$ comparable to the local mesh scale is standard to suppress high-frequency oscillations in $M$ that would otherwise induce near-singular meshes. Normalization by a domain average or maximum keeps the clustering intensity parameter $\\alpha$ interpretable and bounded.\n\nWe now analyze each option against these principles.\n\nOption A constructs\n$$\nM(\\mathbf{x}) = 1 + \\alpha\\,\\widetilde{R}(\\mathbf{x}), \\quad \\widetilde{R}(\\mathbf{x}) = S_\\ell\\!\\left(\\frac{|R_p(\\mathbf{x})|}{\\langle |R_p|\\rangle_\\Omega + \\varepsilon}\\right),\n$$\nwith $S_\\ell$ a smoothing operator (e.g., convolution with a compact kernel of width $\\ell$ or the solution of a screened Poisson filter), and normalization by the domain average. This satisfies positivity, boundedness, and smoothness. Because $|R_p|$ is large in thin shear layers beneath the lid and near corners where the pressure curvature must counterbalance convective acceleration, equidistribution of $M$ clusters nodes where the pressure Poisson PDE forcing is strongest. For a second-order discretization, the local truncation error of the elliptic solve scales like $O(h^2 \\, |D^4 p|)$ in smooth regions, and in regions with rapid variations of $p$ or the right-hand side, smaller $h$ reduces both discretization error and the amplitude of algebraic residuals observed during iterative solves. In the projection method, the pressure solve is the dominant elliptic step controlling how rapidly $\\nabla\\cdot \\mathbf{u}$ decays to the tolerance; reducing localized errors where the Poisson residual is strongest diminishes the number of pressure iterations needed to reach a given divergence tolerance and can reduce the number of outer time steps or sub-iterations to steady state. Provided $\\alpha$ and $\\ell$ are chosen to avoid extreme anisotropy, the operator conditioning is not unduly degraded. Thus, clustering toward large $|R_p|$ with a smoothed, normalized monitor is expected to accelerate overall convergence at fixed degrees of freedom. Option A is consistent with the principles and expected effect; it is Correct.\n\nOption B sets\n$$\nM(\\mathbf{x}) = \\left(1 + \\alpha\\,\\frac{|R_p(\\mathbf{x})|}{\\max_\\Omega |R_p| + \\varepsilon}\\right)^{-1},\n$$\nwhich explicitly coarsens where $|R_p|$ is large and refines where $|R_p|$ is small. This contradicts the equidistribution rationale: it deprioritizes regions where the elliptic PDE forcing and pressure curvature are greatest, leaving large truncation errors and residuals localized. While one might argue that coarsening near strong features reduces the stiffness of the discrete Laplacian (by avoiding small cells), the net effect is to increase the discretization error in precisely those regions that control the global divergence-free constraint. In projection methods, under-resolving the pressure field near strong shear layers leads to slower decay of $\\nabla\\cdot \\mathbf{u}$ and larger pressure corrections per iteration. Therefore, the claim of faster convergence is unsupported. Option B is Incorrect.\n\nOption C proposes\n$$\nM(\\mathbf{x}) = 1 + \\alpha\\,|\\nabla\\cdot \\mathbf{u}(\\mathbf{x})|.\n$$\nWhile $\\nabla\\cdot \\mathbf{u}$ measures violation of incompressibility, in a properly implemented projection method this scalar is rapidly reduced after the first pressure correction at each time step, often to near machine precision relative to the discrete operator. Consequently, $|\\nabla\\cdot \\mathbf{u}|$ is a poor steady indicator of where pressure resolution is needed; it quickly becomes nearly zero almost everywhere, and the monitor reverts to approximately constant, defeating the purpose of targeted clustering. Moreover, focusing grid motion on $\\nabla\\cdot \\mathbf{u}$ ignores the elliptic balance encoded in $R_p$ between pressure curvature and convective acceleration that persists even when $\\nabla\\cdot \\mathbf{u}=0$. This design does not, in general, accelerate convergence relative to a uniform grid; it either does nothing (monitor nearly uniform) or chases transients rather than structural pressure features. Option C is Incorrect.\n\nOption D uses\n$$\nM(\\mathbf{x}) = 1 + \\alpha\\,\\frac{|\\nabla R_p(\\mathbf{x})|}{\\langle |\\nabla R_p|\\rangle_\\Omega},\n$$\nwithout any smoothing. Sensitivity to $|\\nabla R_p|$ detects fronts in the residual rather than its magnitude. In principle, residual gradients can highlight internal layers, but without smoothing or bounding, this monitor is highly oscillatory and can induce excessive, rapidly varying clustering. This tends to produce near-singular meshes, damages the conditioning of the discrete Laplacian (especially in stretched, skewed cells), and can degrade multigrid effectiveness by introducing strong coefficient anisotropy misaligned with grid lines. The blanket claim that such aggressive clustering “always improves” elliptic solver rates is incorrect; in many elliptic solvers, overly sharp mesh grading increases the condition number and slows Krylov convergence unless carefully preconditioned and aligned. Absent smoothing and normalization safeguards, the construction is not robust and is not reliably accelerating. Option D is Incorrect.\n\nTherefore, the option that both constructs a scientifically sound monitor from $R_p$ and correctly predicts accelerated convergence for the projection method on the lid-driven cavity at $Re=10^4$ is Option A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}