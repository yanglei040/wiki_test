{
    "hands_on_practices": [
        {
            "introduction": "The first step in any credible simulation is *verification*—ensuring the code correctly solves the chosen mathematical model. A cornerstone of this process is quantifying the discretization error that arises from representing continuous equations on a finite grid. This practice explores the foundational technique of Richardson extrapolation to estimate this error and then extends the concept to a Bayesian framework, allowing you to characterize the uncertainty in your grid convergence study .",
            "id": "3345875",
            "problem": "Consider a sequence of computational fluid dynamics (CFD) simulations of a steady, laminar flow with a dimensionless Quantity of Interest (QoI), denoted by $Q$, computed on three uniform grids with increasing mesh sizes $h_1$, $h_2$, and $h_3$. Assume a second-order finite-volume scheme with smooth solution, so that the discretization error admits a leading-order asymptotic truncation error model derived from Taylor-series consistency of the governing partial differential equations: specifically, for sufficiently small $h$, the grid-dependent QoI satisfies $Q(h) = Q_{\\infty} + C h^{p} + \\mathcal{O}(h^{p+1})$, where $Q_{\\infty}$ is the asymptotic QoI in the limit $h \\to 0$, $C$ is an unknown constant, and $p$ is the (unknown) order of accuracy. Define a constant ratio $r$ such that $h_2 = r h_1$ and $h_3 = r h_2$.\n\nYou are provided the following data from three grids with a constant ratio $r$:\n- $h_1 = 5.0 \\times 10^{-2}$, $h_2 = 1.0 \\times 10^{-1}$, $h_3 = 2.0 \\times 10^{-1}$, so that $r = 2$.\n- $Q(h_1) = 1.60125$, $Q(h_2) = 1.60500$, $Q(h_3) = 1.62000$.\n\nTask A: Starting from the given asymptotic error model and without assuming $p$ is known a priori, derive the observed-order formula $p$ solely in terms of $(Q(h_1), Q(h_2), Q(h_3))$ and the constant ratio $r$. Then, using the provided data, evaluate $p$.\n\nTask B: Using the same asymptotic error model, derive the Richardson extrapolation expression for $Q_{\\infty}$ expressed in terms of two grid solutions $(Q(h_1), Q(h_2))$, the ratio $r$, and the order $p$. Do not assume the particular numerical values until after you have derived the general expression. Then, evaluate $Q_{\\infty}$ using the observed order from Task A and the provided data.\n\nTask C: To characterize uncertainty in the extrapolation, construct a Bayesian Grid Convergence Index (GCI) by modeling the three observed QoIs with additive random discrepancies about the asymptotic model:\n$$\nQ(h_i) = Q_{\\infty} + C h_i^{p} + \\varepsilon_i, \\quad i \\in \\{1,2,3\\},\n$$\nwhere the $\\varepsilon_i$ are independent and identically distributed zero-mean Gaussian random variables with variance $\\sigma^{2}$, and $p$ is the observed order from Task A. Adopt an improper noninformative prior $p(Q_{\\infty}, C) \\propto 1$ on $(Q_{\\infty}, C)$, and assume the data are conditionally independent given $(Q_{\\infty}, C)$. Let the known noise level be $\\sigma = 1.0 \\times 10^{-3}$ (dimensionless). Derive in closed form the posterior distribution for $Q_{\\infty}$ under this model. Then, compute the posterior mean of $Q_{\\infty}$ numerically using the provided data.\n\nExpress the final numerical answer for the posterior mean of $Q_{\\infty}$ as a dimensionless number. Round your final answer to five significant figures.",
            "solution": "The user has presented a multi-part problem concerning grid convergence analysis in computational fluid dynamics (CFD). The problem is deemed valid as it is scientifically grounded in standard numerical analysis techniques (Richardson extrapolation), well-posed with sufficient and consistent data, and objective in its language. The tasks progress logically from deterministic parameter estimation to a Bayesian uncertainty characterization.\n\nThe solution proceeds by addressing each task sequentially.\n\n### Task A: Derivation and Evaluation of the Observed Order of Accuracy, $p$\n\nThe problem states that for a sufficiently small mesh size $h$, the Quantity of Interest (QoI) $Q(h)$ follows the asymptotic error model:\n$$\nQ(h) = Q_{\\infty} + C h^{p} + \\mathcal{O}(h^{p+1})\n$$\nwhere $Q_{\\infty}$ is the exact solution, $C$ is a constant, and $p$ is the order of accuracy. We are given three solutions $Q_1 = Q(h_1)$, $Q_2 = Q(h_2)$, and $Q_3 = Q(h_3)$ on grids with sizes related by a constant ratio $r$, such that $h_2 = r h_1$ and $h_3 = r h_2 = r^2 h_1$.\n\nNeglecting the higher-order terms $\\mathcal{O}(h^{p+1})$, we can write the following system of equations:\n\\begin{align*}\nQ_1 & \\approx Q_{\\infty} + C h_1^{p} \\quad &(1) \\\\\nQ_2 & \\approx Q_{\\infty} + C h_2^{p} = Q_{\\infty} + C (r h_1)^{p} = Q_{\\infty} + C r^{p} h_1^{p} \\quad &(2) \\\\\nQ_3 & \\approx Q_{\\infty} + C h_3^{p} = Q_{\\infty} + C (r h_2)^{p} = Q_{\\infty} + C r^{p} h_2^{p} = Q_{\\infty} + C r^{2p} h_1^{p} \\quad &(3)\n\\end{align*}\n\nTo find $p$, we first eliminate the unknown $Q_{\\infty}$ by taking differences between successive equations:\n$$\nQ_2 - Q_1 \\approx (Q_{\\infty} + C r^{p} h_1^{p}) - (Q_{\\infty} + C h_1^{p}) = C h_1^{p} (r^{p} - 1)\n$$\n$$\nQ_3 - Q_2 \\approx (Q_{\\infty} + C r^{2p} h_1^{p}) - (Q_{\\infty} + C r^{p} h_1^{p}) = C h_1^{p} (r^{2p} - r^{p}) = C h_1^{p} r^{p} (r^{p} - 1)\n$$\nNow, we can eliminate the unknown term $C h_1^p$ by taking the ratio of these two differences:\n$$\n\\frac{Q_3 - Q_2}{Q_2 - Q_1} \\approx \\frac{C h_1^{p} r^{p} (r^{p} - 1)}{C h_1^{p} (r^{p} - 1)} = r^{p}\n$$\nSolving for $p$ by taking the natural logarithm of both sides gives the formula for the observed order of accuracy:\n$$\n\\ln\\left(\\frac{Q_3 - Q_2}{Q_2 - Q_1}\\right) \\approx p \\ln(r) \\implies p \\approx \\frac{\\ln\\left(\\frac{Q_3 - Q_2}{Q_2 - Q_1}\\right)}{\\ln(r)}\n$$\nNow, we substitute the provided numerical data:\n$Q_1 = Q(h_1) = 1.60125$\n$Q_2 = Q(h_2) = 1.60500$\n$Q_3 = Q(h_3) = 1.62000$\n$r = 2$\n\nFirst, compute the differences:\n$Q_2 - Q_1 = 1.60500 - 1.60125 = 0.00375$\n$Q_3 - Q_2 = 1.62000 - 1.60500 = 0.01500$\n\nNext, compute the ratio:\n$$\n\\frac{Q_3 - Q_2}{Q_2 - Q_1} = \\frac{0.01500}{0.00375} = 4\n$$\nFinally, calculate $p$:\n$$\np = \\frac{\\ln(4)}{\\ln(2)} = \\frac{\\ln(2^2)}{\\ln(2)} = \\frac{2 \\ln(2)}{\\ln(2)} = 2\n$$\nThe observed order of accuracy is exactly $p=2$.\n\n### Task B: Richardson Extrapolation for $Q_{\\infty}$\n\nTo derive the Richardson extrapolation formula for $Q_{\\infty}$, we use the first two equations from the system, (1) and (2), which form a system of two linear equations in the unknowns $Q_{\\infty}$ and $C$:\n\\begin{align*}\nQ_1 & \\approx Q_{\\infty} + C h_1^{p} \\\\\nQ_2 & \\approx Q_{\\infty} + C r^{p} h_1^{p}\n\\end{align*}\nTo eliminate $C$, we can multiply the first equation by $r^p$ and subtract the second equation:\n$$\nr^{p} Q_1 \\approx r^{p} Q_{\\infty} + r^{p} C h_1^{p}\n$$\n$$\n(r^{p} Q_1) - Q_2 \\approx (r^{p} Q_{\\infty} - Q_{\\infty}) + (r^{p} C h_1^{p} - r^{p} C h_1^{p})\n$$\n$$\nr^{p} Q_1 - Q_2 \\approx Q_{\\infty} (r^{p} - 1)\n$$\nSolving for $Q_{\\infty}$ yields the Richardson extrapolation formula:\n$$\nQ_{\\infty} \\approx \\frac{r^{p} Q_1 - Q_2}{r^{p} - 1}\n$$\nAn alternative common form is derived as follows:\n$$\nQ_{\\infty} \\approx \\frac{r^{p} Q_1 - Q_1 + Q_1 - Q_2}{r^{p} - 1} = Q_1 + \\frac{Q_1 - Q_2}{r^{p} - 1} = Q_1 - \\frac{Q_2 - Q_1}{r^{p} - 1}\n$$\nWe now use the derived formula and the data, including the value $p=2$ from Task A, to evaluate $Q_{\\infty}$:\n$r=2$, $p=2 \\implies r^p = 2^2 = 4$.\n$Q_1 = 1.60125$\n$Q_2 = 1.60500$\n$$\nQ_{\\infty} \\approx \\frac{4 \\times 1.60125 - 1.60500}{4 - 1} = \\frac{6.40500 - 1.60500}{3} = \\frac{4.8}{3} = 1.6\n$$\nThe extrapolated value for the QoI is $Q_{\\infty} = 1.6$.\n\n### Task C: Bayesian Grid Convergence Index for $Q_{\\infty}$\n\nThe problem defines a Bayesian model where the observed QoIs, $Q_i = Q(h_i)$, are given by:\n$$\nQ_i = Q_{\\infty} + C h_i^{p} + \\varepsilon_i, \\quad i \\in \\{1,2,3\\}\n$$\nThe errors $\\varepsilon_i$ are independent and identically distributed as zero-mean Gaussian random variables, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, with known variance $\\sigma^2$. The order of accuracy $p$ is fixed to the value from Task A, $p=2$. This is a Bayesian linear regression problem. Let us define the vectors and matrices:\n$$\n\\mathbf{Q} = \\begin{pmatrix} Q_1 \\\\ Q_2 \\\\ Q_3 \\end{pmatrix}, \\quad \\mathbf{\\theta} = \\begin{pmatrix} Q_{\\infty} \\\\ C \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix} 1 & h_1^p \\\\ 1 & h_2^p \\\\ 1 & h_3^p \\end{pmatrix}, \\quad \\mathbf{\\epsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3 \\end{pmatrix}\n$$\nThe model in matrix form is $\\mathbf{Q} = \\mathbf{X}\\mathbf{\\theta} + \\mathbf{\\epsilon}$. The likelihood of the data given the parameters $\\mathbf{\\theta}$ is:\n$$\np(\\mathbf{Q}|\\mathbf{\\theta}, \\sigma^2) = (2\\pi\\sigma^2)^{-3/2} \\exp\\left( -\\frac{1}{2\\sigma^2} (\\mathbf{Q} - \\mathbf{X}\\mathbf{\\theta})^T (\\mathbf{Q} - \\mathbf{X}\\mathbf{\\theta}) \\right)\n$$\nAn improper noninformative prior is used for the parameters: $p(\\mathbf{\\theta}) = p(Q_{\\infty}, C) \\propto 1$.\nAccording to Bayes' theorem, the posterior distribution for $\\mathbf{\\theta}$ is:\n$$\np(\\mathbf{\\theta}|\\mathbf{Q}, \\sigma^2) \\propto p(\\mathbf{Q}|\\mathbf{\\theta}, \\sigma^2) p(\\mathbf{\\theta}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} (\\mathbf{Q} - \\mathbf{X}\\mathbf{\\theta})^T (\\mathbf{Q} - \\mathbf{X}\\mathbf{\\theta}) \\right)\n$$\nThis posterior is a multivariate normal distribution, $p(\\mathbf{\\theta}|\\mathbf{Q}, \\sigma^2) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{post}}, \\boldsymbol{\\Sigma}_{\\text{post}})$, with mean and covariance given by:\n$$\n\\boldsymbol{\\mu}_{\\text{post}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Q}\n$$\n$$\n\\boldsymbol{\\Sigma}_{\\text{post}} = \\sigma^2 (\\mathbf{X}^T\\mathbf{X})^{-1}\n$$\nThe posterior mean $\\boldsymbol{\\mu}_{\\text{post}}$ is the Ordinary Least Squares (OLS) estimate of $\\mathbf{\\theta}$. The marginal posterior distribution for any single parameter is also Gaussian. Specifically, the posterior distribution for $Q_{\\infty}$ is a normal distribution:\n$$\np(Q_{\\infty}|\\mathbf{Q}, \\sigma^2) \\sim \\mathcal{N}\\left( (\\boldsymbol{\\mu}_{\\text{post}})_1, (\\boldsymbol{\\Sigma}_{\\text{post}})_{11} \\right)\n$$\nwhere $(\\cdot)_1$ denotes the first component of the vector and $(\\cdot)_{11}$ denotes the element in the first row and first column of the matrix.\n\nThe task is to compute the posterior mean of $Q_{\\infty}$, which is $(\\boldsymbol{\\mu}_{\\text{post}})_1$. This is simply the OLS estimate of $Q_{\\infty}$.\nA key observation simplifies the calculation. In Tasks A and B, we found that the provided data points perfectly satisfy the deterministic model $Q(h) = Q_{\\infty} + C h^p$ for the parameters $p=2$ and $Q_{\\infty}=1.6$. We can also find the value of $C$ that gives a perfect fit. Using the data for $h_1$:\n$$\n1.60125 = 1.6 + C (5.0 \\times 10^{-2})^2 \\implies 0.00125 = C (0.0025) \\implies C = 0.5\n$$\nWe can verify that $Q(h) = 1.6 + 0.5 h^2$ perfectly fits all three data points.\nThe OLS procedure finds the parameter values $(\\hat{Q}_{\\infty}, \\hat{C})$ that minimize the sum of squared residuals, $\\sum_{i=1}^3 (Q_i - (Q_{\\infty} + C h_i^p))^2$. Since the data perfectly fit the model for $(Q_{\\infty}, C) = (1.6, 0.5)$, the minimum possible sum of squared residuals is $0$, which is achieved at these parameter values. Therefore, the OLS estimate, and consequently the posterior mean, must be:\n$$\n\\boldsymbol{\\mu}_{\\text{post}} = \\begin{pmatrix} \\hat{Q}_{\\infty} \\\\ \\hat{C} \\end{pmatrix} = \\begin{pmatrix} 1.6 \\\\ 0.5 \\end{pmatrix}\n$$\nThe posterior mean of $Q_{\\infty}$ is thus exactly $1.6$. The value of $\\sigma$ affects the width (variance) of the posterior distribution but not its mean in this model configuration.\n\nThe problem asks for the numerical answer rounded to five significant figures. The value $1.6$ expressed with five significant figures is $1.6000$.",
            "answer": "$$\n\\boxed{1.6000}\n$$"
        },
        {
            "introduction": "Before launching into computationally expensive calibration procedures, a critical prerequisite is to assess parameter identifiability: can the parameters of our model even be determined from the available data? This practice introduces the Fisher Information Matrix as a powerful tool to answer this question by relating parameter sensitivities to the noise characteristics of the measurements. By analyzing the properties of this matrix, you will gain a rigorous method for diagnosing potential non-identifiability, preventing misleading calibration results and guiding experimental design .",
            "id": "3345885",
            "problem": "A linearized calibration of a compressible external aerodynamic solver is performed using sparse surface pressure coefficient data at four sensor locations. Let the observation model be expressed as\n$$\ny = y^{\\star} + S\\left(\\theta - \\theta^{\\star}\\right) + H\\,\\beta + \\varepsilon,\n$$\nwhere $y \\in \\mathbb{R}^{4}$ collects the pressure coefficient residuals relative to a nominal parameter $\\theta^{\\star} \\in \\mathbb{R}^{3}$, $S \\in \\mathbb{R}^{4 \\times 3}$ is the sensitivity matrix with entries $S_{ij} = \\partial y_i/\\partial \\theta_j$ evaluated at $\\theta^{\\star}$, $\\beta \\in \\mathbb{R}$ is a scalar coefficient representing a low-rank model-form discrepancy, and $\\varepsilon \\in \\mathbb{R}^{4}$ is measurement noise. Assume that $\\beta \\sim \\mathcal{N}(0,\\Sigma_{\\beta})$ with $\\Sigma_{\\beta} = \\tau^{2}$, and $\\varepsilon \\sim \\mathcal{N}(0,\\Sigma_{e})$ with\n$$\n\\Sigma_{e} = \\sigma^{2}\\left((1-\\rho) I_{4} + \\rho\\,\\mathbf{1}\\mathbf{1}^{\\top}\\right),\n$$\nwhere $I_{4}$ is the $4 \\times 4$ identity and $\\mathbf{1} \\in \\mathbb{R}^{4}$ is the all-ones vector. The model-form term enters through $H \\in \\mathbb{R}^{4 \\times 1}$ defined by $H = \\mathbf{1}$.\n\nYou are given the following data and hyperparameters:\n$$\nS = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n1 & -1 & 0 \\\\\n1 & 0 & -1 \n\\end{pmatrix},\\quad\n\\sigma^{2} = 0.04,\\quad\n\\rho = 0.3,\\quad\n\\tau^{2} = 0.09.\n$$\n\nStarting from first principles of linear-Gaussian inference and the definition of Fisher information for Gaussian models whose mean depends on parameters, first obtain the effective data covariance after marginalizing the model-form discrepancy, and then derive the Fisher information matrix for $\\theta$. Use this to decide the local identifiability of the three parameters by examining the rank of the Fisher information. Finally, compute the Cramér–Rao lower bound (the inverse of the Fisher information) for the variance of the first parameter $\\theta_{1}$. Express your final answer as a single dimensionless number rounded to four significant figures.",
            "solution": "The user has provided a problem statement that has been validated and found to be scientifically grounded, well-posed, and objective. There are no identifiable flaws; therefore, a full solution is provided below.\n\nThe problem asks for an analysis of a linearized calibration problem within a Bayesian inference framework. The observation model is given by\n$$y = y^{\\star} + S\\left(\\theta - \\theta^{\\star}\\right) + H\\,\\beta + \\varepsilon$$\nwhere $y \\in \\mathbb{R}^{4}$ is the data, $\\theta \\in \\mathbb{R}^{3}$ is the parameter vector, $\\beta \\in \\mathbb{R}$ is a model discrepancy term, and $\\varepsilon \\in \\mathbb{R}^{4}$ is measurement noise. Let us define the observable quantity as the residual data vector $d = y - y^{\\star}$. The model can then be written as:\n$$d = S(\\theta - \\theta^{\\star}) + H\\beta + \\varepsilon$$\nThe statistical assumptions are that $\\beta$ and $\\varepsilon$ are independent, zero-mean Gaussian random variables:\n$$ \\beta \\sim \\mathcal{N}(0, \\Sigma_{\\beta}) \\quad \\text{with} \\quad \\Sigma_{\\beta} = \\tau^2 $$\n$$ \\varepsilon \\sim \\mathcal{N}(0, \\Sigma_{e}) \\quad \\text{with} \\quad \\Sigma_{e} = \\sigma^{2}\\left((1-\\rho) I_{4} + \\rho\\,\\mathbf{1}\\mathbf{1}^{\\top}\\right) $$\n\nThe first step is to obtain the effective data covariance after marginalizing out the model-form discrepancy parameter $\\beta$. In this linear-Gaussian setting, marginalizing the nuisance parameter $\\beta$ is equivalent to incorporating its contribution into the total noise term. The total noise term is $\\eta = H\\beta + \\varepsilon$. Since $\\beta$ and $\\varepsilon$ are independent and have zero mean, the total noise $\\eta$ is also a zero-mean Gaussian variable. Its covariance, which we denote as the effective covariance $\\Sigma_{\\text{eff}}$, is the sum of the individual covariances:\n$$ \\Sigma_{\\text{eff}} = \\text{Cov}(\\eta) = \\text{Cov}(H\\beta) + \\text{Cov}(\\varepsilon) $$\nUsing the property $\\text{Cov}(AX) = A\\text{Cov}(X)A^{\\top}$, we have:\n$$ \\text{Cov}(H\\beta) = H\\Sigma_{\\beta}H^{\\top} $$\nSubstituting the given forms for $H=\\mathbf{1}$ and $\\Sigma_{\\beta}=\\tau^2$:\n$$ H\\Sigma_{\\beta}H^{\\top} = \\mathbf{1}(\\tau^2)\\mathbf{1}^{\\top} = \\tau^2 \\mathbf{1}\\mathbf{1}^{\\top} $$\nThe effective covariance is thus:\n$$ \\Sigma_{\\text{eff}} = \\tau^2 \\mathbf{1}\\mathbf{1}^{\\top} + \\sigma^{2}\\left((1-\\rho) I_{4} + \\rho\\,\\mathbf{1}\\mathbf{1}^{\\top}\\right) $$\nGrouping terms, we obtain the final form of the effective data covariance:\n$$ \\Sigma_{\\text{eff}} = \\sigma^2(1-\\rho)I_4 + (\\tau^2 + \\sigma^2\\rho)\\mathbf{1}\\mathbf{1}^{\\top} $$\nThis matrix is of the general form $aI_n + b\\mathbf{1}\\mathbf{1}^{\\top}$. Let's define the scalar coefficients $a = \\sigma^2(1-\\rho)$ and $b = \\tau^2 + \\sigma^2\\rho$.\n\nThe second step is to derive the Fisher Information Matrix (FIM) for $\\theta$. For a Gaussian model where the data $d$ follows $\\mathcal{N}(\\mu(\\theta), \\Sigma)$, and the covariance $\\Sigma$ is independent of the parameters $\\theta$, the FIM is given by:\n$$ \\mathcal{I}(\\theta) = \\left(\\frac{\\partial \\mu(\\theta)}{\\partial \\theta}\\right)^{\\top} \\Sigma^{-1} \\left(\\frac{\\partial \\mu(\\theta)}{\\partial \\theta}\\right) $$\nIn our problem, the mean is $\\mu(\\theta) = S(\\theta - \\theta^{\\star})$ and the covariance is $\\Sigma = \\Sigma_{\\text{eff}}$. The Jacobian of the mean with respect to $\\theta$ is simply the sensitivity matrix $S$. Therefore, the FIM is:\n$$ \\mathcal{I} = S^{\\top} \\Sigma_{\\text{eff}}^{-1} S $$\nTo compute $\\mathcal{I}$, we must first find the inverse of $\\Sigma_{\\text{eff}} = aI_4 + b\\mathbf{1}\\mathbf{1}^{\\top}$. Using the Sherman-Woodbury formula for $(A+uv^{\\top})^{-1}$ with $A=aI_4$, $u=b\\mathbf{1}$, and $v=\\mathbf{1}$, we find:\n$$ \\Sigma_{\\text{eff}}^{-1} = \\frac{1}{a}I_4 - \\frac{b/a^2}{1+(b/a)\\mathbf{1}^{\\top}\\mathbf{1}}\\mathbf{1}\\mathbf{1}^{\\top} $$\nWith $\\mathbf{1}^{\\top}\\mathbf{1} = 4$, the inverse simplifies to:\n$$ \\Sigma_{\\text{eff}}^{-1} = \\frac{1}{a}I_4 - \\frac{b}{a(a+4b)}\\mathbf{1}\\mathbf{1}^{\\top} $$\nNow we substitute this into the FIM expression:\n$$ \\mathcal{I} = S^{\\top} \\left( \\frac{1}{a}I_4 - \\frac{b}{a(a+4b)}\\mathbf{1}\\mathbf{1}^{\\top} \\right) S = \\frac{1}{a}S^{\\top}S - \\frac{b}{a(a+4b)}S^{\\top}\\mathbf{1}\\mathbf{1}^{\\top}S $$\nWe compute the matrix products using the given matrix $S$:\n$$ S = \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & -1 & 0 \\\\ 1 & 0 & -1 \\end{pmatrix} $$\n$$ S^{\\top}S = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 0 & 1 & -1 & 0 \\\\ 1 & 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & -1 & 0 \\\\ 1 & 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} $$\n$$ S^{\\top}\\mathbf{1} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 0 & 1 & -1 & 0 \\\\ 1 & 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe term $S^{\\top}\\mathbf{1}\\mathbf{1}^{\\top}S$ becomes $(S^{\\top}\\mathbf{1})(S^{\\top}\\mathbf{1})^{\\top}$:\n$$ S^{\\top}\\mathbf{1}\\mathbf{1}^{\\top}S = \\begin{pmatrix} 4 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 4 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 16 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nSubstituting these into the expression for $\\mathcal{I}$:\n$$ \\mathcal{I} = \\frac{1}{a}\\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} - \\frac{b}{a(a+4b)}\\begin{pmatrix} 16 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nThe FIM is a diagonal matrix. Its elements are:\n$$ \\mathcal{I}_{11} = \\frac{4}{a} - \\frac{16b}{a(a+4b)} = \\frac{4(a+4b)-16b}{a(a+4b)} = \\frac{4a}{a(a+4b)} = \\frac{4}{a+4b} $$\n$$ \\mathcal{I}_{22} = \\frac{2}{a}, \\quad \\mathcal{I}_{33} = \\frac{2}{a} $$\nThe Fisher Information Matrix is:\n$$ \\mathcal{I} = \\begin{pmatrix} \\frac{4}{a+4b} & 0 & 0 \\\\ 0 & \\frac{2}{a} & 0 \\\\ 0 & 0 & \\frac{2}{a} \\end{pmatrix} $$\n\nFor identifiability, we examine the rank of $\\mathcal{I}$. The parameters are locally identifiable if and only if the FIM is full rank. Since $\\mathcal{I}$ is a $3 \\times 3$ diagonal matrix, it is full rank if all its diagonal entries are non-zero. Let's compute the numerical values for $a$ and $b$:\n$$ a = \\sigma^2(1-\\rho) = 0.04(1-0.3) = 0.04 \\times 0.7 = 0.028 $$\n$$ b = \\tau^2 + \\sigma^2\\rho = 0.09 + 0.04(0.3) = 0.09 + 0.012 = 0.102 $$\nSince $a > 0$ and $b > 0$, all diagonal entries of $\\mathcal{I}$ are positive and thus non-zero. The rank of $\\mathcal{I}$ is $3$, which equals the number of parameters. Therefore, all three parameters are locally identifiable.\n\nFinally, we compute the Cramér–Rao lower bound (CRLB) for the variance of the first parameter, $\\theta_1$. The CRLB is given by the inverse of the FIM, $C = \\mathcal{I}^{-1}$. The lower bound on the variance of any unbiased estimator of $\\theta_1$ is the first diagonal element of $C$, $C_{11}$.\nSince $\\mathcal{I}$ is diagonal, its inverse is also diagonal with entries being the reciprocals of the diagonal entries of $\\mathcal{I}$:\n$$ \\mathcal{I}^{-1} = \\begin{pmatrix} \\frac{a+4b}{4} & 0 & 0 \\\\ 0 & \\frac{a}{2} & 0 \\\\ 0 & 0 & \\frac{a}{2} \\end{pmatrix} $$\nThe CRLB for the variance of $\\theta_1$ is:\n$$ \\text{CRLB}(\\theta_1) = (\\mathcal{I}^{-1})_{11} = \\frac{a+4b}{4} $$\nSubstituting the numerical values for $a$ and $b$:\n$$ \\text{CRLB}(\\theta_1) = \\frac{0.028 + 4(0.102)}{4} = \\frac{0.028 + 0.408}{4} = \\frac{0.436}{4} = 0.109 $$\nRounding to four significant figures as requested, the final answer is $0.1090$.",
            "answer": "$$\\boxed{0.1090}$$"
        },
        {
            "introduction": "In advanced UQ, we must recognize that numerical errors and parametric inference are not independent; in fact, they can be strongly coupled. This hands-on coding exercise demonstrates how unacknowledged discretization error in a simulation can systematically bias the posterior distribution of a calibrated physical parameter. You will implement a procedure to track this \"posterior drift\" as the computational grid is refined, learning to diagnose and quantify the impact of numerical error on physical model calibration .",
            "id": "3345874",
            "problem": "Consider a single-parameter closure within the Reynolds-Averaged Navier-Stokes (RANS) framework where the eddy viscosity depends on a constant $C_\\mu$. For a fixed canonical flow configuration and a single scalar quantity of interest $y$ (for example, a spatially averaged wall shear surrogate), assume a local linear surrogate $y \\approx \\beta\\,C_\\mu$ for fixed Reynolds number and boundary conditions. Adopt a finite-volume discretization with grid characteristic size $h$ and a scheme of order $p$, so that the leading-order discretization error in the predicted quantity of interest has magnitude that scales as $\\delta(h) = \\alpha\\,h^p$. We aim to characterize model-form and parametric uncertainty induced by discretization-model coupling, by performing Bayesian inference on $C_\\mu$ across progressively refined meshes while initially ignoring the grid-dependent discrepancy term $\\delta(h)$, then diagnosing the induced posterior drift and estimating $\\delta(h)$ from the drift.\n\nAssume an observation model in which the discretized solver produces values $y_h$ on grid size $h$ as $y_h = \\beta\\,C_\\mu^\\star + \\delta(h) + \\eta$, where $C_\\mu^\\star$ is the true but unknown parameter, and $\\eta$ is zero-mean measurement or solver noise. For the inference step, adopt a Gaussian prior $C_\\mu \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$ and a Gaussian likelihood that ignores $\\delta(h)$, i.e., $y_h \\mid C_\\mu \\sim \\mathcal{N}(\\beta\\,C_\\mu,\\sigma^2)$, where $\\sigma^2$ is the observation variance. Under these assumptions, the posterior $p(C_\\mu \\mid y_h)$ is Gaussian, and the posterior mean will generally depend on $h$ because the inference model is misspecified when $\\delta(h) \\neq 0$. Define the mesh refinement process by halving the grid size $h$ at each step until the change in the posterior mean between successive refinements is below a stabilization tolerance $\\zeta$, or a maximum number of refinement steps $N_{\\max}$ is reached.\n\nYour program must implement the following tasks:\n- Starting from a given initial grid size $h_0$, repeatedly refine $h$ by a factor of $1/2$ and, at each grid, compute the posterior mean of $C_\\mu$ under the misspecified Gaussian model described above. Stop when $|\\mu_{\\text{post}}(h_{k}) - \\mu_{\\text{post}}(h_{k-1})| \\le \\zeta$, or when $k = N_{\\max}$.\n- Track the posterior drift by storing the sequence of posterior means $\\{\\mu_{\\text{post}}(h_k)\\}$ and compute the drift magnitude as the Euclidean norm $D = \\left(\\sum_k \\left[\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{\\text{final}})\\right]^2 \\right)^{1/2}$, where $h_{\\text{final}}$ is the last grid used.\n- Infer a grid-dependent discrepancy term by first defining $\\widehat{\\delta}(h_k) = \\beta \\left[\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{\\text{final}})\\right]$ and then fitting the model $\\widehat{\\delta}(h) \\approx \\alpha\\,h^p$ via linear regression in logarithmic coordinates, i.e., $\\log|\\widehat{\\delta}(h)| \\approx \\log|\\alpha| + p \\log h$, using only those $\\widehat{\\delta}(h)$ not equal to zero in magnitude. Recover $\\alpha$ with the sign determined by the average sign of $\\widehat{\\delta}(h)$ values used in the regression.\n- Report stabilization, drift, and discrepancy estimates for each test case as specified below.\n\nUse the following test suite of parameter values:\n- Case A (general case): $\\beta = 2.5$, $\\mu_0 = 0.08$, $\\sigma_0 = 0.1$, $\\sigma = 0.02$, $C_\\mu^\\star = 0.09$, $\\alpha = 0.4$, $p = 2.0$, $h_0 = 0.2$, $N_{\\max} = 6$, $\\zeta = 10^{-4}$, and $\\eta = 0$.\n- Case B (no discretization error): $\\beta = 2.5$, $\\mu_0 = 0.08$, $\\sigma_0 = 0.1$, $\\sigma = 0.02$, $C_\\mu^\\star = 0.09$, $\\alpha = 0.0$, $p = 2.0$ (irrelevant here), $h_0 = 0.2$, $N_{\\max} = 4$, $\\zeta = 10^{-6}$, and $\\eta = 0$.\n- Case C (strong prior and larger noise): $\\beta = 2.5$, $\\mu_0 = 0.085$, $\\sigma_0 = 0.005$, $\\sigma = 0.08$, $C_\\mu^\\star = 0.09$, $\\alpha = 0.15$, $p = 1.0$, $h_0 = 0.3$, $N_{\\max} = 7$, $\\zeta = 5 \\times 10^{-5}$, and $\\eta = 0$.\n\nFor each case, your program must output a list of five values:\n- The final grid size $h_{\\text{final}}$ at which stabilization occurred (float).\n- A stabilization indicator equal to $1$ if stabilization was achieved before reaching $N_{\\max}$, and $0$ otherwise (integer).\n- The drift magnitude $D$ as defined above (float).\n- The estimated order $p_{\\text{hat}}$ from the log-log regression (float; return $0.0$ if estimation is not possible due to zero drift).\n- The estimated coefficient $\\alpha_{\\text{hat}}$ from the log-log regression (float; return $0.0$ if estimation is not possible due to zero drift).\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case result and is itself a list in the order described above. For example, the output must look like \"[[h_A,stab_A,D_A,p_hat_A,alpha_hat_A],[h_B,stab_B,D_B,p_hat_B,alpha_hat_B],[h_C,stab_C,D_C,p_hat_C,alpha_hat_C]]\". All quantities in the output are dimensionless, so no physical unit conversion is required. Angles do not appear, and values must not be expressed with a percentage sign; any fractional values must be expressed as decimal numbers.",
            "solution": "The user has provided a problem from the field of computational fluid dynamics, specifically concerning the quantification of uncertainty. The task is to analyze the interaction between parametric uncertainty in a Reynolds-Averaged Navier-Stokes (RANS) closure model and numerical discretization error. This analysis involves performing a sequence of Bayesian inference calculations on progressively refined computational grids and diagnosing the resulting drift in the inferred parameter.\n\n### Problem Validation\n\nFirst, the problem statement is subjected to rigorous validation.\n\n**Step 1: Extracted Givens**\n- **Model:** A linear surrogate for a quantity of interest $y \\approx \\beta\\,C_\\mu$.\n- **Observation:** $y_h = \\beta\\,C_\\mu^\\star + \\delta(h) + \\eta$, with discretization error $\\delta(h) = \\alpha\\,h^p$ and solver noise $\\eta$.\n- **Inference (Misspecified):** A Bayesian model ignoring $\\delta(h)$, with prior $C_\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$ and likelihood $y_h \\mid C_\\mu \\sim \\mathcal{N}(\\beta\\,C_\\mu, \\sigma^2)$.\n- **Procedure:**\n    1.  Iteratively refine the grid ($h_k = h_0 / 2^k$) and compute the posterior mean $\\mu_{\\text{post}}(h_k)$.\n    2.  Stop when the change between successive means falls below a tolerance, $|\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{k-1})| \\le \\zeta$, or after $N_{\\max}$ steps.\n    3.  Compute drift magnitude $D = \\left(\\sum_k [\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{\\text{final}})]^2 \\right)^{1/2}$.\n    4.  Estimate discrepancy parameters $(\\alpha_{\\text{hat}}, p_{\\text{hat}})$ by fitting the model $\\widehat{\\delta}(h) \\approx \\alpha h^p$ to the derived values $\\widehat{\\delta}(h_k) = \\beta [\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{\\text{final}})]$ via log-log linear regression.\n- **Test Cases:** Three distinct sets of parameters (Cases A, B, C) are provided, covering a general case, a case with no discretization error, and a case with a strong prior. For all cases, $\\eta=0$.\n\n**Step 2: Validation Assessment**\n- **Scientific Grounding:** The problem is firmly rooted in established principles of Uncertainty Quantification (UQ) for computational models. The concept of model misspecification leading to biased inference that depends on numerical parameters (like grid size) is a well-studied topic in computational science. The use of Bayesian inference, Gaussian models, and log-log regression for error analysis are all standard, valid techniques.\n- **Well-Posedness:** The problem is mathematically and algorithmically well-defined. All necessary parameters and initial conditions are provided for each case. The procedure is unambiguous, and the required outputs are explicitly described. The termination conditions are clear, and edge cases (e.g., inability to perform regression) are handled.\n- **Objectivity:** The problem is stated in precise, formal, and objective language, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict**\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. We may proceed to the solution.\n\n### Principle-Based Solution\nThe core of the problem lies in deriving and analyzing the posterior distribution of the RANS parameter $C_\\mu$. Given a Gaussian prior and a Gaussian likelihood, the resulting posterior distribution is also Gaussian.\n\n**1. Posterior Mean Derivation**\nThe prior distribution for $C_\\mu$ is $p(C_\\mu) = \\mathcal{N}(C_\\mu \\mid \\mu_0, \\sigma_0^2)$.\nThe misspecified likelihood for an observation $y_h$ on a grid of size $h$ is $p(y_h \\mid C_\\mu) = \\mathcal{N}(y_h \\mid \\beta C_\\mu, \\sigma^2)$.\n\nThe posterior for $C_\\mu$ is $p(C_\\mu \\mid y_h) \\propto p(y_h \\mid C_\\mu) p(C_\\mu)$. For conjugate Gaussian models, the posterior mean $\\mu_{\\text{post}}$ is a precision-weighted average of the prior mean and the data-implied mean. The posterior precision ($1/\\sigma_{\\text{post}}^2$) is the sum of the prior precision ($1/\\sigma_0^2$) and the likelihood precision (which, for the parameter $C_\\mu$, is $\\beta^2/\\sigma^2$).\n$$ \\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma_0^2} + \\frac{\\beta^2}{\\sigma^2} $$\nThe posterior mean is then given by:\n$$ \\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{\\mu_0}{\\sigma_0^2} + \\frac{\\beta y_h}{\\sigma^2} \\right) = \\frac{\\mu_0 \\sigma^2 + \\beta y_h \\sigma_0^2}{\\sigma^2 + \\beta^2 \\sigma_0^2} $$\nThe \"observed\" data $y_h$ is generated by the true model: $y_h = \\beta C_\\mu^\\star + \\delta(h) + \\eta$. With $\\delta(h) = \\alpha h^p$ and $\\eta=0$, this becomes $y_h = \\beta C_\\mu^\\star + \\alpha h^p$. Substituting this into the expression for $\\mu_{\\text{post}}$ reveals its dependence on the grid size $h$:\n$$ \\mu_{\\text{post}}(h) = \\frac{\\mu_0 \\sigma^2 + \\beta (\\beta C_\\mu^\\star + \\alpha h^p) \\sigma_0^2}{\\sigma^2 + \\beta^2 \\sigma_0^2} = \\frac{(\\mu_0 \\sigma^2 + \\beta^2 C_\\mu^\\star \\sigma_0^2) + (\\beta \\alpha \\sigma_0^2) h^p}{\\sigma^2 + \\beta^2 \\sigma_0^2} $$\nThis expression can be conveniently structured as a grid-independent part and a grid-dependent part:\n$$ \\mu_{\\text{post}}(h) = A + B h^p $$\nwhere $A = \\frac{\\mu_0 \\sigma^2 + \\beta^2 C_\\mu^\\star \\sigma_0^2}{\\sigma^2 + \\beta^2 \\sigma_0^2}$ is the posterior mean on an infinitely fine grid ($h \\to 0$), and $B = \\frac{\\beta \\alpha \\sigma_0^2}{\\sigma^2 + \\beta^2 \\sigma_0^2}$ quantifies the bias introduced by the discretization error at finite $h$. This analytical form will be used to compute the sequence of posterior means.\n\n**2. Algorithmic Implementation**\nFor each test case, we implement the following algorithm:\n-   **Initialization:** Set up parameters for the given case. Initialize empty lists to store the sequence of grid sizes, `h_values`, and posterior means, `mu_post_values`.\n-   **Refinement Loop:** An iterative process is executed up to $N_{\\max}$ times. In each iteration $k$:\n    1.  The grid size is refined: $h_k = h_0 / 2^k$.\n    2.  The posterior mean $\\mu_{\\text{post}}(h_k)$ is calculated using the derived formula.\n    3.  $h_k$ and $\\mu_{\\text{post}}(h_k)$ are stored.\n    4.  For $k > 0$, the absolute difference $|\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{k-1})|$ is compared to the stabilization tolerance $\\zeta$. If the criterion is met, the loop terminates, and a stabilization flag is set to $1$. If the loop completes without meeting the criterion, the flag is set to $0$.\n-   **Drift Calculation:** The final grid size $h_{\\text{final}}$ and posterior mean $\\mu_{\\text{post}}(h_{\\text{final}})$ are identified. The posterior drift $D$ is computed as the Euclidean norm of the vector of deviations of each posterior mean from the final one.\n-   **Discrepancy Estimation:**\n    1.  A sequence of discrepancy surrogates is computed: $\\widehat{\\delta}(h_k) = \\beta [\\mu_{\\text{post}}(h_k) - \\mu_{\\text{post}}(h_{\\text{final}})]$.\n    2.  The pairs $(\\log h_k, \\log|\\widehat{\\delta}(h_k)|)$ are collected for all $h_k$ where $\\widehat{\\delta}(h_k)$ is non-zero. Note that $\\widehat{\\delta}(h_{\\text{final}})$ is guaranteed to be zero.\n    3.  If fewer than two data points are available for regression (e.g., if all $\\widehat{\\delta}$ are zero, as in Case B), the estimated parameters $p_{\\text{hat}}$ and $\\alpha_{\\text{hat}}$ are set to $0.0$.\n    4.  Otherwise, a linear regression is performed to fit $\\log|\\widehat{\\delta}| = \\log|\\alpha| + p \\log h$. The slope of the fit gives $p_{\\text{hat}}$, and the intercept gives $\\log|\\alpha_{\\text{hat}}|$.\n    5.  The sign of $\\alpha_{\\text{hat}}$ is determined from the average sign of the non-zero $\\widehat{\\delta}(h_k)$ values used in the fit.\n\nThis procedure is applied to each of the three test cases to generate the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Solves the problem of characterizing model-form and parametric uncertainty\n    by analyzing posterior drift across grid refinements.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"beta\": 2.5, \"mu_0\": 0.08, \"sigma_0\": 0.1, \"sigma\": 0.02,\n            \"C_mu_star\": 0.09, \"alpha\": 0.4, \"p\": 2.0, \"h_0\": 0.2,\n            \"N_max\": 6, \"zeta\": 1e-4, \"eta\": 0.0\n        },\n        # Case B\n        {\n            \"beta\": 2.5, \"mu_0\": 0.08, \"sigma_0\": 0.1, \"sigma\": 0.02,\n            \"C_mu_star\": 0.09, \"alpha\": 0.0, \"p\": 2.0, \"h_0\": 0.2,\n            \"N_max\": 4, \"zeta\": 1e-6, \"eta\": 0.0\n        },\n        # Case C\n        {\n            \"beta\": 2.5, \"mu_0\": 0.085, \"sigma_0\": 0.005, \"sigma\": 0.08,\n            \"C_mu_star\": 0.09, \"alpha\": 0.15, \"p\": 1.0, \"h_0\": 0.3,\n            \"N_max\": 7, \"zeta\": 5e-5, \"eta\": 0.0\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Extract parameters for the current case\n        beta = case[\"beta\"]\n        mu_0 = case[\"mu_0\"]\n        sigma_0 = case[\"sigma_0\"]\n        sigma = case[\"sigma\"]\n        C_mu_star = case[\"C_mu_star\"]\n        alpha = case[\"alpha\"]\n        p = case[\"p\"]\n        h_0 = case[\"h_0\"]\n        N_max = case[\"N_max\"]\n        zeta = case[\"zeta\"]\n        eta = case[\"eta\"]\n\n        h_values = []\n        mu_post_values = []\n        \n        stabilized = 0\n        h_final = 0.0\n\n        # Common terms for posterior mean calculation\n        sigma2 = sigma**2\n        sigma0_2 = sigma_0**2\n        beta2 = beta**2\n        denominator = sigma2 + beta2 * sigma0_2\n        \n        # Grid-independent part of the posterior mean\n        A = (mu_0 * sigma2 + beta2 * C_mu_star * sigma0_2) / denominator\n        # Grid-dependent coefficient\n        B = (beta * alpha * sigma0_2) / denominator\n        \n        for k in range(N_max):\n            h_k = h_0 / (2**k)\n            \n            # The observation model y_h would be:\n            # y_h = beta * C_mu_star + alpha * (h_k**p) + eta\n            \n            # The posterior mean formula derived from the problem description\n            # mu_post = (mu_0 * sigma2 + beta * y_h * sigma0_2) / denominator\n            # Substituting y_h gives:\n            mu_post_k = A + B * (h_k**p)\n\n            h_values.append(h_k)\n            mu_post_values.append(mu_post_k)\n            \n            if k > 0:\n                diff = abs(mu_post_values[k] - mu_post_values[k-1])\n                if diff = zeta:\n                    stabilized = 1\n                    break\n        \n        h_final = h_values[-1]\n        mu_final = mu_post_values[-1]\n        \n        # Compute drift magnitude D\n        drift_sum_sq = 0.0\n        for mu in mu_post_values:\n            drift_sum_sq += (mu - mu_final)**2\n        drift_D = np.sqrt(drift_sum_sq)\n\n        # Infer discrepancy term\n        p_hat = 0.0\n        alpha_hat = 0.0\n        \n        delta_hats = [beta * (mu - mu_final) for mu in mu_post_values]\n        \n        # Prepare data for log-log regression\n        log_h_reg = []\n        log_delta_abs_reg = []\n        delta_hat_reg = []\n        \n        for i in range(len(delta_hats)):\n            # Exclude points where delta_hat is zero (or very close)\n            # The last point where h=h_final will always be zero\n            if abs(delta_hats[i]) > 1e-15:\n                log_h_reg.append(np.log(h_values[i]))\n                log_delta_abs_reg.append(np.log(abs(delta_hats[i])))\n                delta_hat_reg.append(delta_hats[i])\n        \n        # Perform regression if at least 2 points are available\n        if len(log_h_reg) >= 2:\n            try:\n                regression_result = linregress(log_h_reg, log_delta_abs_reg)\n                p_hat = regression_result.slope\n                \n                # Calculate alpha_hat magnitude from intercept\n                alpha_hat_abs = np.exp(regression_result.intercept)\n                \n                # Determine the sign of alpha_hat\n                sign = np.sign(np.mean(delta_hat_reg)) if delta_hat_reg else 1.0\n                alpha_hat = alpha_hat_abs * sign\n\n            except ValueError:\n                # This case should not be reached with the current data\n                p_hat = 0.0\n                alpha_hat = 0.0\n\n        case_results = [h_final, stabilized, drift_D, p_hat, alpha_hat]\n        all_results.append(case_results)\n\n    # Format output to match specification \"[[...],[...],[...]]\"\n    # Ensure no spaces inside the inner lists.\n    results_as_str_list = []\n    for res in all_results:\n        # Convert each float/int in the list to string and join with commas\n        results_as_str_list.append(f\"[{','.join(map(str, res))}]\")\n    \n    # Join the string representations of each case result list\n    final_output_str = f\"[{','.join(results_as_str_list)}]\"\n        \n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}