{
    "hands_on_practices": [
        {
            "introduction": "Training Physics-Informed Neural Networks for complex phenomena like those in fluid dynamics often requires computing high-order derivatives to enforce the governing partial differential equations. This exercise  provides a practical analysis of the computational and memory costs associated with this process, specifically when using Automatic Differentiation (AD) for second-order derivatives. By deriving scaling laws for a typical network, you will gain a concrete understanding of how network architecture and batch size are constrained by available hardware resources, a crucial consideration for any practitioner.",
            "id": "3351988",
            "problem": "A Physics-Informed Neural Network (PINN) is trained to approximate incompressible two-dimensional Navier–Stokes fields, where the viscosity term requires second-order spatial derivatives of the velocity components for the residual of the partial differential equation. Consider a fully connected feedforward network with $L$ affine layers, hidden-layer width $W$, input dimension $d=3$ (spatial variables $x$, $y$, and time $t$), and output dimension $m=3$ (velocity components $u$, $v$, and pressure $p$). The hidden layers all have width $W$, and the output layer maps width $W$ to $m$. The network uses Automatic Differentiation (AD), specifically the reverse-over-forward strategy, to compute diagonal second-order spatial derivatives $\\frac{\\partial^{2} u}{\\partial x^{2}}$, $\\frac{\\partial^{2} u}{\\partial y^{2}}$, $\\frac{\\partial^{2} v}{\\partial x^{2}}$, and $\\frac{\\partial^{2} v}{\\partial y^{2}}$, needed for the viscosity term in the residual. Assume the following:\n\n- The cost of a forward evaluation through one affine layer of width $W$ scales proportionally to the number of multiply-adds in a dense matrix–vector product, which is $W^{2}$ when both input and output widths are $W$. Across $L$ layers, the forward cost scales as $c_{\\mathrm{f}} L W^{2}$ for some constant $c_{\\mathrm{f}}$.\n- For reverse-mode AD computing the gradient of a scalar output, the time complexity is proportional to the forward cost with constant factor $c_{\\mathrm{rev}}$ (which includes the forward tape creation and the backward sweep).\n- For second-order diagonal spatial derivatives via reverse-over-forward AD for a single scalar output, the time complexity is proportional to $d_{s} c_{\\mathrm{rev}} L W^{2}$, where $d_{s}=2$ is the number of spatial input directions ($x$ and $y$). To compute the four diagonal second derivatives for the two velocity outputs $u$ and $v$, scale appropriately by the number of outputs.\n- Activation tapes for reverse mode require storing both preactivations and activations for each hidden layer. Let the per-hidden-layer storage be $c_{\\mathrm{act}} W$ floats per sample. Additional AD workspace for second derivatives requires $c_{\\mathrm{AD}} W$ floats per hidden layer per sample. Per-sample memory thus scales as $(c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$ floats for hidden layers only. Each float is $4$ bytes (single precision).\n- The total parameter count (weights plus biases) is\n$$\nN_{p} = d W + (L-2)W^{2} + W m + (L-1)W + m,\n$$\nand parameters are kept in memory independent of batch size.\n\nTasks:\n\n1. Derive an asymptotic expression for the total time complexity $T(B,L,W)$, in units proportional to multiply-add operations, to compute the set of diagonal second-order spatial derivatives $\\left\\{\\frac{\\partial^{2} u}{\\partial x^{2}}, \\frac{\\partial^{2} u}{\\partial y^{2}}, \\frac{\\partial^{2} v}{\\partial x^{2}}, \\frac{\\partial^{2} v}{\\partial y^{2}}\\right\\}$ for a batch of size $B$ using reverse-over-forward AD, in terms of $B$, $L$, $W$, $c_{\\mathrm{f}}$, and $c_{\\mathrm{rev}}$.\n2. Derive a leading-order expression for the total memory complexity $M(B,L,W)$ (in bytes) as a function of $B$, $L$, and $W$, including both parameter memory and per-sample AD workspace, in terms of $N_{p}$, $c_{\\mathrm{act}}$, and $c_{\\mathrm{AD}}$.\n3. Evaluate the maximum batch size $B_{\\max}$ that fits into a graphics processing unit (GPU) with total available memory of $8 \\times 2^{30}$ bytes, assuming $L=12$, $W=1024$, $d=3$, $m=3$, $c_{\\mathrm{f}}=1$, $c_{\\mathrm{rev}}=2$, $c_{\\mathrm{act}}=2$, $c_{\\mathrm{AD}}=4$, single-precision floats ($4$ bytes each), and that optimizer state and gradient checkpointing are disabled. Ignore any additional framework overhead beyond what is specified. Express your final answer for $B_{\\max}$ as an integer. Also discuss, qualitatively, the implications of these complexity estimates for the choice of batch size in PINN training when second-order derivatives are required.",
            "solution": "The problem asks for an analysis of the computational and memory complexity of a Physics-Informed Neural Network (PINN) used for incompressible Navier–Stokes equations, focusing on the calculation of second-order spatial derivatives via reverse-over-forward Automatic Differentiation (AD). The problem is well-posed, scientifically grounded, and contains sufficient information for a unique solution.\n\nThe analysis is divided into three tasks:\n1. Derivation of the total time complexity $T(B,L,W)$ for computing the required second derivatives for a batch of size $B$.\n2. Derivation of the total memory complexity $M(B,L,W)$ including network parameters and AD workspace.\n3. Calculation of the maximum batch size $B_{\\max}$ for a given memory budget and a qualitative discussion of the implications.\n\n**Task 1: Time Complexity $T(B,L,W)$**\n\nThe goal is to find the total time complexity to compute the set of four diagonal second-order spatial derivatives: $\\left\\{\\frac{\\partial^{2} u}{\\partial x^{2}}, \\frac{\\partial^{2} u}{\\partial y^{2}}, \\frac{\\partial^{2} v}{\\partial x^{2}}, \\frac{\\partial^{2} v}{\\partial y^{2}}\\right\\}$.\n\nThe problem statement provides the following scaling law for the computation: The time complexity for computing the set of diagonal second derivatives for a single scalar output with respect to $d_s$ spatial input directions, using reverse-over-forward AD, is proportional to $d_{s} c_{\\mathrm{rev}} L W^{2}$ for a single sample.\n\nIn this problem:\n- The network outputs are $(u, v, p)$. We are interested in the velocity components $u$ and $v$.\n- The spatial input dimensions are $x$ and $y$, so the number of spatial directions is $d_s=2$.\n\nFirst, consider the derivatives for the velocity component $u$. We need to compute $\\frac{\\partial^{2} u}{\\partial x^{2}}$ and $\\frac{\\partial^{2} u}{\\partial y^{2}}$. This corresponds to one scalar output, $u$, and $d_s=2$ spatial directions. According to the provided scaling law, the time complexity for computing this set of derivatives for a single sample is:\n$$ T_{u, \\text{sample}} = d_s c_{\\mathrm{rev}} L W^{2} = 2 c_{\\mathrm{rev}} L W^{2} $$\n\nNext, consider the derivatives for the velocity component $v$. We need to compute $\\frac{\\partial^{2} v}{\\partial x^{2}}$ and $\\frac{\\partial^{2} v}{\\partial y^{2}}$. This again corresponds to one scalar output, $v$, and $d_s=2$ spatial directions. The computation for $v$ is independent of that for $u$. Therefore, the time complexity is the same:\n$$ T_{v, \\text{sample}} = d_s c_{\\mathrm{rev}} L W^{2} = 2 c_{\\mathrm{rev}} L W^{2} $$\n\nThe total time complexity to compute all four required derivatives for a single sample is the sum of the complexities for $u$ and $v$:\n$$ T_{\\text{sample}} = T_{u, \\text{sample}} + T_{v, \\text{sample}} = 2 c_{\\mathrm{rev}} L W^{2} + 2 c_{\\mathrm{rev}} L W^{2} = 4 c_{\\mathrm{rev}} L W^{2} $$\nThis aligns with the instruction to \"scale appropriately by the number of outputs,\" which is $2$ in this case ($u$ and $v$).\n\nThis complexity is for a single input sample. For a batch of size $B$, assuming each sample is processed independently, the total time complexity $T(B,L,W)$ is the per-sample complexity multiplied by the batch size $B$:\n$$ T(B,L,W) = B \\cdot T_{\\text{sample}} = 4 B c_{\\mathrm{rev}} L W^{2} $$\n\n**Task 2: Memory Complexity $M(B,L,W)$**\n\nThe total memory complexity $M(B,L,W)$ is the sum of two components: the memory for storing the network parameters, $M_{\\text{params}}$, and the memory for the per-sample computational workspace (e.g., AD tapes), $M_{\\text{batch}}$.\n\n1.  **Parameter Memory ($M_{\\text{params}}$):** This memory is required to store the weights and biases of the neural network. Its size is independent of the batch size $B$. The problem provides the formula for the total number of parameters, $N_{p}$:\n    $$ N_{p} = d W + (L-2)W^{2} + W m + (L-1)W + m $$\n    Each parameter is stored as a single-precision float, which occupies $4$ bytes. Therefore, the parameter memory in bytes is:\n    $$ M_{\\text{params}} = 4 N_{p} $$\n\n2.  **Per-Sample AD Workspace Memory ($M_{\\text{batch}}$):** This memory scales linearly with the batch size $B$. The problem states that the per-sample memory for hidden layers, which includes activation tapes and AD workspace for second derivatives, is $(c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$ floats.\n    For a single sample, the memory in bytes is $4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$.\n    For a batch of size $B$, the total workspace memory is:\n    $$ M_{\\text{batch}} = 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W $$\n\nThe total memory complexity $M(B,L,W)$ is the sum of these two components:\n$$ M(B,L,W) = M_{\\text{params}} + M_{\\text{batch}} = 4 N_{p} + 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W $$\nThis expression represents the leading-order memory complexity as specified, incorporating the provided terms for parameter count and batch-dependent workspace.\n\n**Task 3: Maximum Batch Size $B_{\\max}$ and Discussion**\n\nThe maximum batch size $B_{\\max}$ is limited by the total available GPU memory, $M_{\\text{GPU}}$. We must solve for $B$ in the inequality $M(B,L,W) \\le M_{\\text{GPU}}$.\n$$ 4 N_{p} + 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W \\le M_{\\text{GPU}} $$\nSolving for $B$:\n$$ 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W \\le M_{\\text{GPU}} - 4 N_{p} $$\n$$ B \\le \\frac{M_{\\text{GPU}} - 4 N_{p}}{4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W} $$\nThe maximum batch size $B_{\\max}$ is the largest integer satisfying this inequality:\n$$ B_{\\max} = \\left\\lfloor \\frac{M_{\\text{GPU}} - 4 N_{p}}{4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W} \\right\\rfloor $$\nWe are given the following values:\n- $L=12$\n- $W=1024$\n- $d=3$\n- $m=3$\n- $c_{\\mathrm{act}}=2$\n- $c_{\\mathrm{AD}}=4$\n- $M_{\\text{GPU}} = 8 \\times 2^{30}$ bytes $= 8589934592$ bytes\n\nFirst, we calculate the number of parameters $N_{p}$:\n$$ N_{p} = (3)(1024) + (12-2)(1024)^{2} + (1024)(3) + (12-1)(1024) + 3 $$\n$$ N_{p} = 3072 + 10(1048576) + 3072 + 11(1024) + 3 $$\n$$ N_{p} = 3072 + 10485760 + 3072 + 11264 + 3 $$\n$$ N_{p} = 10503171 $$\nThe parameter memory is:\n$$ M_{\\text{params}} = 4 N_{p} = 4 \\times 10503171 = 42012684 \\text{ bytes} $$\nNext, we calculate the per-sample memory consumption, which is the denominator of the fraction for $B$:\n$$ M_{\\text{per\\_sample}} = 4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W = 4 (2 + 4)(12-1)(1024) $$\n$$ M_{\\text{per\\_sample}} = 4(6)(11)(1024) = 264 \\times 1024 = 270336 \\text{ bytes/sample} $$\nNow we can find the maximum batch size $B_{\\max}$:\n$$ B_{\\max} = \\left\\lfloor \\frac{8589934592 - 42012684}{270336} \\right\\rfloor $$\n$$ B_{\\max} = \\left\\lfloor \\frac{8547921908}{270336} \\right\\rfloor $$\n$$ B_{\\max} = \\lfloor 31619.46... \\rfloor = 31619 $$\n\n**Qualitative Discussion:**\nThese complexity estimates have significant implications for training PINNs that require second-order derivatives.\nThe total memory $M(B,L,W)$ is a sum of a batch-independent term (parameters, $M_{\\text{params}} \\propto L W^{2}$) and a batch-dependent term (AD workspace, $M_{\\text{batch}} \\propto B L W$).\nFor large networks (large $L$ and $W$), the parameter memory is already substantial. However, the critical constraint often comes from the per-sample AD workspace. The need to compute second-order derivatives requires storing a larger computation graph or additional intermediate products, which is reflected in the $c_{\\mathrm{AD}}$ term. In our example, the per-sample memory footprint is over $270$ kilobytes.\nThis large per-sample memory cost creates a severe trade-off. To fit a large, expressive model (high $L, W$) into a fixed GPU memory, the batch size $B$ must be reduced. This is evident from the formula for $B_{\\max}$, where the large per-sample memory term $4 (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1) W$ is in the denominator.\nUsing a smaller batch size can have negative consequences for training, such as introducing higher variance into the stochastic gradient estimates. This may slow down convergence and require more careful tuning of the optimizer, for instance, by using smaller learning rates. While larger batches are generally preferred for stable training and maximizing parallel computation on GPUs, the memory overhead from second-order AD in PINNs forces a compromise, pushing practitioners towards smaller batches than otherwise optimal. This motivates the development of memory-efficient AD techniques, such as activation checkpointing (which was explicitly excluded in this problem), where activations are recomputed during the backward pass to save memory at the cost of additional computation.",
            "answer": "$$\n\\boxed{\n\\pmatrix{\n4 B c_{\\mathrm{rev}} L W^{2} & 4 N_{p} + 4 B (c_{\\mathrm{act}} + c_{\\mathrm{AD}})(L-1)W & 31619\n}\n}\n$$"
        },
        {
            "introduction": "After understanding the computational costs, we now explore a common and subtle cause of training failure in PINNs: gradient pathologies in multi-scale problems. This hands-on coding exercise  uses the viscous Burgers' equation as a prototype to demonstrate how the relative importance of different physical terms can lead to an imbalanced loss landscape. You will quantitatively diagnose how, as viscosity decreases, the gradients associated with advection can overwhelm those from diffusion, stalling optimization and preventing the network from learning the correct dynamics.",
            "id": "3352001",
            "problem": "You are asked to analyze the sensitivity and potential training failure of Physics-Informed Neural Networks (PINNs) for the one-dimensional viscous Burgers equation. Your analysis must be implemented as a complete, runnable program that computes the residual magnitudes and gradient norms for a fixed parametric trial field and predicts failure due to gradient pathologies as viscosity decreases. The underlying governing law is the one-dimensional viscous Burgers equation, a prototypical model for convection-diffusion in Computational Fluid Dynamics (CFD), written as the balance law of nonlinear advection and viscous diffusion: $$u_{t} + u\\,u_{x} - \\nu\\,u_{xx} = 0,$$ where $u=u(x,t)$, $x$ is position, $t$ is time, and $\\nu$ is the kinematic viscosity. Your numerical computations will be conducted nondimensionally, so no physical units are required. The test suite consists of a set of viscosity values $\\nu$ that span from moderate to vanishingly small.\n\nYour program must proceed from the following fundamental base and definitions. Consider a fixed parametric trial field that mimics a small neural network with linear output layer and fixed basis features, $$u(x,t;\\boldsymbol{\\theta}) = \\sum_{m=1}^{3}\\theta_{m}\\,\\phi_{m}(x,t),$$ where the basis functions are $$\\phi_{1}(x,t)=\\sin(\\pi x)\\,e^{-t},\\quad \\phi_{2}(x,t)=\\sin(2\\pi x)\\,e^{-t},\\quad \\phi_{3}(x,t)=\\cos(\\pi x)\\,e^{-2t},$$ and the parameter vector is fixed as $$\\boldsymbol{\\theta}=\\begin{bmatrix}\\theta_{1}\\\\\\theta_{2}\\\\\\theta_{3}\\end{bmatrix}=\\begin{bmatrix}1.0\\\\-0.5\\\\0.75\\end{bmatrix}.$$ From the fundamental definitions of derivatives, compute $$u_{t}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{t}\\phi_{m},\\quad u_{x}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{x}\\phi_{m},\\quad u_{xx}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{xx}\\phi_{m},$$ and define the pointwise partial differential equation residual $$\\mathcal{R}(x,t;\\nu,\\boldsymbol{\\theta}) = u_{t} + u\\,u_{x} - \\nu\\,u_{xx}.$$ Define the mean-squared residual over a rectangular spatio-temporal collocation grid as $$\\mathcal{L}(\\nu,\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})^{2},$$ where $(x_{i},t_{i})$ are the collocation points and $N$ is the total number of points. The gradient of the loss with respect to the parameters, by the chain rule, is $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\nu,\\boldsymbol{\\theta}) = \\frac{2}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})\\,\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta}).$$ To analyze imbalance across physics terms, decompose the residual as $$\\mathcal{R}=\\mathcal{T}+\\mathcal{A}-\\mathcal{D},\\quad \\text{with}\\quad \\mathcal{T}=u_{t},\\ \\mathcal{A}=u\\,u_{x},\\ \\mathcal{D}=\\nu\\,u_{xx}.$$ Then the residual Jacobian with respect to the parameters admits the exact decomposition $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R} = \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{T}}_{\\text{time}} + \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{A}}_{\\text{advection}} - \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{D}}_{\\text{diffusion}}.$$ Using the linearity of $u$ in $\\boldsymbol{\\theta}$, and the product rule for $\\mathcal{A}=u\\,u_{x}$, obtain $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{T}=\\begin{bmatrix}\\partial_{t}\\phi_{1}\\\\ \\partial_{t}\\phi_{2}\\\\ \\partial_{t}\\phi_{3}\\end{bmatrix},\\quad \\nabla_{\\boldsymbol{\\theta}}\\mathcal{A}=\\begin{bmatrix}\\phi_{1}\\,u_{x}+u\\,\\partial_{x}\\phi_{1}\\\\ \\phi_{2}\\,u_{x}+u\\,\\partial_{x}\\phi_{2}\\\\ \\phi_{3}\\,u_{x}+u\\,\\partial_{x}\\phi_{3}\\end{bmatrix},\\quad \\nabla_{\\boldsymbol{\\theta}}\\mathcal{D}=\\nu\\begin{bmatrix}\\partial_{xx}\\phi_{1}\\\\ \\partial_{xx}\\phi_{2}\\\\ \\partial_{xx}\\phi_{3}\\end{bmatrix}.$$ Consequently, the loss gradient splits additively into three components, $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}=\\mathbf{g}_{\\mathcal{T}}+\\mathbf{g}_{\\mathcal{A}}+\\mathbf{g}_{\\mathcal{D}},\\quad \\text{where}\\quad \\mathbf{g}_{\\mathcal{X}}=\\frac{2}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i})\\,\\nabla_{\\boldsymbol{\\theta}}\\mathcal{X}(x_{i},t_{i}),$$ for each $\\mathcal{X}\\in\\{\\mathcal{T},\\mathcal{A},-\\mathcal{D}\\}$, with the sign convention absorbed into the definition of $\\mathbf{g}_{\\mathcal{D}}$.\n\nYour tasks are:\n- Use the above definitions to compute, for each prescribed viscosity $\\nu$, the following quantities over a uniform grid with $x\\in[0,1]$, $t\\in[0,1]$, $N_{x}=64$, $N_{t}=64$, and $N=N_{x}N_{t}$:\n  - The root-mean-square residual $$\\mathrm{RMS}(\\nu)=\\sqrt{\\mathcal{L}(\\nu,\\boldsymbol{\\theta})}.$$\n  - The Euclidean norms of the loss-gradient components $\\lVert \\mathbf{g}_{\\mathcal{T}}\\rVert_{2}$, $\\lVert \\mathbf{g}_{\\mathcal{A}}\\rVert_{2}$, and $\\lVert \\mathbf{g}_{\\mathcal{D}}\\rVert_{2}$, and the advection-diffusion gradient imbalance ratio $$\\rho(\\nu)=\\frac{\\lVert \\mathbf{g}_{\\mathcal{A}}\\rVert_{2}}{\\max\\{\\lVert \\mathbf{g}_{\\mathcal{D}}\\rVert_{2},\\varepsilon\\}},$$ with $\\varepsilon=10^{-16}$ to avoid division by zero.\n  - The condition number of the residual Jacobian matrix $$\\mathbf{J}(\\nu)\\in\\mathbb{R}^{N\\times 3},\\quad \\mathbf{J}_{i,m}=\\left[\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})\\right]_{m},$$ defined as $$\\kappa(\\mathbf{J})=\\frac{\\sigma_{\\max}(\\mathbf{J})}{\\sigma_{\\min}(\\mathbf{J})},$$ where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $\\mathbf{J}$.\n- Predict a training failure for PINNs due to gradient pathologies if either of the following holds: $$\\rho(\\nu)>\\kappa_{\\mathrm{th}} \\quad \\text{or} \\quad \\kappa(\\mathbf{J}(\\nu))>\\chi_{\\mathrm{th}},$$ with thresholds $\\kappa_{\\mathrm{th}}=10^{3}$ and $\\chi_{\\mathrm{th}}=10^{8}$.\n\nTest suite and inputs:\n- Use the fixed parameter vector $\\boldsymbol{\\theta}=\\begin{bmatrix}1.0\\\\-0.5\\\\0.75\\end{bmatrix}$.\n- Use the viscosities $\\nu\\in\\{1.0,\\,0.1,\\,0.01,\\,0.001,\\,0.0\\}$.\n\nRequired outputs and formatting:\n- For each $\\nu$ in the test suite, compute and collect the list $$[\\nu,\\ \\mathrm{RMS}(\\nu),\\ \\rho(\\nu),\\ f(\\nu)],$$ where $f(\\nu)$ is $1$ if training failure is predicted and $0$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list of such lists, with no whitespace and each floating-point number formatted to six significant figures, for example, $$[[1,0.123456,789.123,0],\\ldots].$$ The actual numbers will differ; this format is mandatory.",
            "solution": "The objective of this problem is to analyze the sensitivity of a Physics-Informed Neural Network (PINN) training process for the one-dimensional viscous Burgers equation as the viscosity parameter $\\nu$ decreases. This analysis is performed by computing several key metrics for a fixed, simplified trial solution that mimics the output of a neural network. The metrics include the root-mean-square (RMS) of the PDE residual, the norms of individual components of the loss function's gradient, and the condition number of the residual Jacobian. These quantities are used to predict potential training failures arising from gradient pathologies, specifically the imbalance between different physical terms (advection vs. diffusion) and the ill-conditioning of the optimization landscape.\n\nThe analysis proceeds through the following steps:\n\n1.  **Analytical Formulation**: We begin with the analytical definitions provided. The trial solution $u(x,t;\\boldsymbol{\\theta})$ is a linear combination of three predefined basis functions $\\phi_m(x,t)$ with a fixed parameter vector $\\boldsymbol{\\theta}$. The first and second partial derivatives of $u$ with respect to space, $u_x$ and $u_{xx}$, and time, $u_t$, are computed analytically by differentiating the sum term-by-term. These analytical expressions are crucial for accurately evaluating the PDE residual and its gradients without introducing numerical differentiation errors.\n\n2.  **Grid Discretization**: The continuous spatio-temporal domain $(x,t) \\in [0,1] \\times [0,1]$ is discretized into a uniform grid of $N_x \\times N_t = 64 \\times 64 = 4096$ collocation points. All subsequent calculations are performed on this grid. The use of vectorized operations in `NumPy` allows for efficient computation across all points simultaneously.\n\n3.  **Residual and Gradient Computation**: For each viscosity value $\\nu$ in the test suite, we compute the following quantities at each grid point:\n    -   The trial solution $u$ and its derivatives $u_t$, $u_x$, and $u_{xx}$.\n    -   The PDE residual, $\\mathcal{R} = u_{t} + u\\,u_{x} - \\nu\\,u_{xx}$, which measures how well the trial solution satisfies the Burgers equation.\n    -   The gradients of the residual with respect to the parameters $\\boldsymbol{\\theta}$, known as the residual Jacobian $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}$. These are decomposed into contributions from the temporal ($\\mathcal{T}=u_t$), advection ($\\mathcal{A}=u\\,u_x$), and diffusion ($\\mathcal{D}=\\nu\\,u_{xx}$) terms. The linearity of the trial solution in $\\boldsymbol{\\theta}$ simplifies these gradient calculations significantly. For instance, $\\nabla_{\\boldsymbol{\\theta}} u_t$ is simply the vector of the time derivatives of the basis functions, $[\\partial_t\\phi_1, \\partial_t\\phi_2, \\partial_t\\phi_3]^T$. The gradient of the nonlinear advection term is computed using the product rule.\n\n4.  **Metric Calculation**: Using the pointwise quantities computed above, we aggregate them into the required global metrics:\n    -   **RMS Residual, $\\mathrm{RMS}(\\nu)$**: This is the square root of the mean of the squared residuals over all collocation points, $\\sqrt{\\frac{1}{N}\\sum_i \\mathcal{R}_i^2}$. It provides an overall measure of the solution's accuracy.\n    -   **Loss Gradient Component Norms**: The gradient of the mean-squared error loss function, $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}$, is additively composed of three vectors, $\\mathbf{g}_{\\mathcal{T}}$, $\\mathbf{g}_{\\mathcal{A}}$, and $\\mathbf{g}_{\\mathcal{D}}$, corresponding to the temporal, advection, and diffusion terms. Each vector is computed by summing the product of the residual $\\mathcal{R}$ and the respective component of the residual Jacobian over all grid points, i.e., $\\mathbf{g}_{\\mathcal{X}} \\propto \\sum_i \\mathcal{R}_i \\nabla_{\\boldsymbol{\\theta}}\\mathcal{X}_i$. We then compute the Euclidean norm for each of these three vectors.\n    -   **Advection-Diffusion Imbalance Ratio, $\\rho(\\nu)$**: This ratio, $\\lVert\\mathbf{g}_\\mathcal{A}\\rVert_2 / \\max(\\lVert\\mathbf{g}_\\mathcal{D}\\rVert_2, \\varepsilon)$, quantifies the relative magnitude of the advection gradient component to the diffusion gradient component. A large value indicates that the training dynamics are dominated by the advection term, which can be problematic as viscosity vanishes and the problem becomes convection-dominated.\n    -   **Condition Number, $\\kappa(\\mathbf{J})$**: The residual Jacobian matrix $\\mathbf{J} \\in \\mathbb{R}^{N\\times 3}$ is formed by stacking the gradient vectors $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}$ from each of the $N$ collocation points. Its condition number, the ratio of its largest to smallest singular value, measures the sensitivity of the residual to changes in the parameters. A high condition number indicates an ill-conditioned problem, where small changes in parameters can lead to large changes in the residual, often slowing down or stalling gradient-based optimization.\n\n5.  **Failure Prediction**: Based on the computed metrics, a training failure is predicted if the imbalance ratio $\\rho(\\nu)$ exceeds a threshold $\\kappa_{\\mathrm{th}}=10^3$ or if the condition number $\\kappa(\\mathbf{J}(\\nu))$ exceeds a threshold $\\chi_{\\mathrm{th}}=10^8$. These conditions signal that the underlying optimization problem is pathological, either due to severe gradient imbalance or ill-conditioning, both of which are common failure modes for PINNs in multi-scale problems.\n\nThe implementation encapsulates these steps into a single program that iterates through the provided list of viscosity values, performs the calculations for each, and formats the results according to the specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes sensitivity and predicts training failure for a PINN model of the\n    1D viscous Burgers equation for a range of viscosity values.\n    \"\"\"\n    \n    # Define problem constants and parameters from the problem statement.\n    THETA = np.array([1.0, -0.5, 0.75])\n    NU_VALUES = [1.0, 0.1, 0.01, 0.001, 0.0]\n    NX, NT = 64, 64\n    N = NX * NT\n    KAPPA_TH = 1e3\n    CHI_TH = 1e8\n    EPSILON = 1e-16\n\n    # Create the spatio-temporal grid.\n    x_coords = np.linspace(0, 1, NX)\n    t_coords = np.linspace(0, 1, NT)\n    X, T = np.meshgrid(x_coords, t_coords)\n\n    # --- Pre-compute basis functions and their symbolic derivatives on the grid ---\n    pi = np.pi\n    \n    # phi_1(x,t) = sin(pi*x) * exp(-t)\n    phi1 = np.sin(pi * X) * np.exp(-T)\n    phi1_t = -phi1\n    phi1_x = pi * np.cos(pi * X) * np.exp(-T)\n    phi1_xx = -pi**2 * phi1\n    \n    # phi_2(x,t) = sin(2*pi*x) * exp(-t)\n    phi2 = np.sin(2 * pi * X) * np.exp(-T)\n    phi2_t = -phi2\n    phi2_x = 2 * pi * np.cos(2 * pi * X) * np.exp(-T)\n    phi2_xx = - (2 * pi)**2 * phi2\n\n    # phi_3(x,t) = cos(pi*x) * exp(-2t)\n    phi3 = np.cos(pi * X) * np.exp(-2 * T)\n    phi3_t = -2 * phi3\n    phi3_x = -pi * np.sin(pi * X) * np.exp(-2 * T)\n    phi3_xx = -pi**2 * phi3\n\n    # Store basis functions and their derivatives in arrays for efficient computation.\n    # Shape: (3, NT, NX) where 3 is the number of basis functions.\n    PHI = np.array([phi1, phi2, phi3])\n    PHI_t = np.array([phi1_t, phi2_t, phi3_t])\n    PHI_x = np.array([phi1_x, phi2_x, phi3_x])\n    PHI_xx = np.array([phi1_xx, phi2_xx, phi3_xx])\n\n    # --- Main calculation loop over viscosity values ---\n    results_list = []\n    \n    for nu in NU_VALUES:\n        # Calculate the trial solution u and its derivatives using Einstein summation.\n        # This computes the sum over the first axis (m=1,2,3).\n        u = np.einsum('i,ijk->jk', THETA, PHI)\n        u_t = np.einsum('i,ijk->jk', THETA, PHI_t)\n        u_x = np.einsum('i,ijk->jk', THETA, PHI_x)\n        u_xx = np.einsum('i,ijk->jk', THETA, PHI_xx)\n        \n        # Calculate components of the PDE residual.\n        term_T = u_t\n        term_A = u * u_x\n        term_D = nu * u_xx\n        \n        # Calculate the pointwise PDE residual R = u_t + u*u_x - nu*u_xx.\n        R = term_T + term_A - term_D\n        \n        # Task 1: Compute root-mean-square residual, RMS(nu).\n        rms_val = np.sqrt(np.mean(R**2))\n        \n        # --- Compute gradients with respect to parameters theta ---\n        \n        # Gradient of the temporal term, grad_theta(T). Shape: (NT, NX, 3)\n        grad_T_theta = np.moveaxis(PHI_t, 0, -1)\n        \n        # Gradient of the advection term, grad_theta(A).\n        # grad_A_theta_m = phi_m * u_x + u * phi_x_m\n        grad_A_theta = np.zeros((NT, NX, 3))\n        for m in range(3):\n            grad_A_theta[:, :, m] = PHI[m, :, :] * u_x + u * PHI_x[m, :, :]\n            \n        # Gradient of the diffusion term, grad_theta(D).\n        grad_D_theta = nu * np.moveaxis(PHI_xx, 0, -1)\n        \n        # --- Compute components of the loss gradient: g_T, g_A, g_D ---\n        # g_X = (2/N) * sum_{i,j} (R * grad_X_theta)\n        R_reshaped = R[:, :, np.newaxis] # Reshape for broadcasting\n        \n        g_T = (2 / N) * np.sum(R_reshaped * grad_T_theta, axis=(0, 1))\n        g_A = (2 / N) * np.sum(R_reshaped * grad_A_theta, axis=(0, 1))\n        # g_D is defined with a negative sign: based on grad_theta(-D)\n        g_D = (2 / N) * np.sum(R_reshaped * (-grad_D_theta), axis=(0, 1))\n        \n        # Task 2: Compute norms of gradient components and the imbalance ratio rho(nu).\n        norm_g_A = np.linalg.norm(g_A)\n        norm_g_D = np.linalg.norm(g_D)\n        \n        rho_val = norm_g_A / max(norm_g_D, EPSILON)\n        \n        # Task 3: Compute the condition number of the residual Jacobian matrix J.\n        # J combines gradients from all terms: J = grad_T + grad_A - grad_D\n        grad_R_theta = grad_T_theta + grad_A_theta - grad_D_theta\n        J_matrix = grad_R_theta.reshape((N, 3))\n        cond_J = np.linalg.cond(J_matrix)\n        \n        # Task 4: Predict training failure based on the given thresholds.\n        failure_predicted = 1 if (rho_val > KAPPA_TH) or (cond_J > CHI_TH) else 0\n        \n        results_list.append([nu, rms_val, rho_val, failure_predicted])\n        \n    # --- Format the output as a single-line string ---\n    formatted_results = []\n    for res in results_list:\n        nu_str = f\"{res[0]:.6g}\"\n        rms_str = f\"{res[1]:.6g}\"\n        rho_str = f\"{res[2]:.6g}\"\n        fail_str = str(res[3])\n        formatted_results.append(f\"[{nu_str},{rms_str},{rho_str},{fail_str}]\")\n        \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "This practice demonstrates the power of physics-informed learning for solving challenging inverse problems, a key application area for these methods. Here, you will tackle the problem of identifying a hidden obstacle within a flow field using only sparse external measurements . The exercise frames this task as a form of sparse model discovery, where you will implement a search to find the geometric parameters of the obstacle's indicator field by minimizing a regularized, physics-informed objective function, with each candidate evaluation requiring the solution of the governing PDE.",
            "id": "3352066",
            "problem": "Consider a one-dimensional, steady, incompressible flow along a line segment represented by a scalar velocity field $u(x)$ defined on the closed interval $[0,1]$. A solid obstacle occupies an unknown sub-interval $\\Omega_s = [a,b] \\subset (0,1)$, which we represent with an indicator field $\\chi(x)$ that equals $1$ in $\\Omega_s$ and $0$ outside. Using the Brinkman penalization commonly employed in immersed boundary methods, the steady momentum balance in the low Reynolds number limit reduces to\n$$\n-\\nu \\, \\frac{d^2 u}{dx^2} + \\alpha \\, \\chi(x) \\, u(x) = 0,\n$$\nwith Dirichlet boundary conditions\n$$\nu(0) = U_0, \\quad u(1) = U_1,\n$$\nwhere $\\nu > 0$ is a given (dimensionless) kinematic viscosity and $\\alpha \\gg 1$ is a large penalization parameter enforcing $u \\approx 0$ inside the solid region where $\\chi = 1$. This Brinkman-penalized model is derived from the steady Stokes limit of the Navier–Stokes equations coupled with a porous medium term, and serves as a physics-informed constraint.\n\nYou are given external flow data at points outside the (unknown) obstacle, generated synthetically by solving the above boundary value problem for a known true obstacle $[a^\\star, b^\\star]$. Your task is to discover the shape of the indicator field $\\chi(x)$, parameterized as a single interval $[a,b]$, by minimizing a physics-informed loss that compares predicted flow values to data while enforcing sparsity of the solid region. Specifically, for any candidate interval $[a,b]$ you must:\n- Solve the boundary value problem numerically by a second-order centered finite difference discretization on a uniform grid with $N$ nodes. Use the standard three-point stencil for the Laplacian and enforce the Dirichlet boundary conditions exactly at the endpoints.\n- Evaluate the predicted $u(x)$ at the given data points (all of which lie in the fluid region of the true obstacle) using linear interpolation.\n- Minimize the objective\n$$\n\\mathcal{J}(a,b) = \\frac{1}{M}\\sum_{j=1}^{M} \\left(u(x_j; a,b) - \\tilde{u}_j\\right)^2 \\;+\\; \\lambda \\, (b - a),\n$$\nwhere $x_j$ are the data locations, $\\tilde{u}_j$ are the observed velocities, $M$ is the number of data points, and $\\lambda > 0$ is a sparsity-promoting regularization weight that encourages the solid region to be small. This encodes a sparse model discovery bias on the indicator field $\\chi$.\n\nFor numerical stability and to keep the problem self-contained, do not perform gradient-based optimization with respect to $a$ and $b$. Instead, perform a discrete search over candidate centers and half-lengths, $c \\in \\mathcal{C}$ and $s \\in \\mathcal{S}$, with $a=c-s$ and $b=c+s$, and only accept candidates that satisfy $0 < a < b < 1$. For each valid candidate, solve the discretized physics and compute $\\mathcal{J}(a,b)$; choose the candidate with the minimal objective. This procedure is a physics-informed identification because every candidate prediction $u(\\cdot; a,b)$ is obtained by solving the governing equation, so the partial differential equation residual is controlled by construction.\n\nFundamental base and discretization requirements:\n- Start from the steady Stokes limit with Brinkman penalization,\n$$\n-\\nu \\, u''(x) + \\alpha \\, \\chi(x) \\, u(x) = 0,\\quad x\\in(0,1),\n$$\nwith $u(0)=U_0$ and $u(1)=U_1$.\n- Discretize with $N$ uniformly spaced grid points $x_i = i h$, where $h = 1/(N-1)$ and $i \\in \\{0,1,\\dots,N-1\\}$. For interior indices $i=1,\\dots,N-2$, use\n$$\n-\\nu \\, \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + \\alpha \\, \\chi_i \\, u_i = 0,\n$$\nwhere $\\chi_i = 1$ if $x_i \\in [a,b]$ and $\\chi_i=0$ otherwise. Incorporate boundary conditions by moving known boundary values to the right-hand side.\n\nData generation protocol for each test case:\n- Use $N_{\\text{true}} = 257$ for data generation and $N = 257$ for identification to avoid inverse crime simplifications while keeping discretizations compatible.\n- Use $\\nu = 0.01$, $\\alpha = 1000$, $U_0 = 1$, $U_1 = 0$.\n- Generate the true solution $u^\\star(x)$ by solving the discretized problem with the true obstacle $[a^\\star,b^\\star]$.\n- Select $M_{\\text{tot}} = 25$ equispaced candidate points in $[0,1]$ and retain only those outside the true obstacle, that is, $x_j \\in [0,a^\\star) \\cup (b^\\star,1]$, to get $M$ data points. Form the observed data $\\tilde{u}_j = u^\\star(x_j) + \\eta_j$, where $\\eta_j$ is zero-mean noise with standard deviation $\\sigma = 0.001$.\n- Use linear interpolation to evaluate model predictions at $x_j$.\n\nSearch spaces and regularization:\n- Use centers $c \\in \\mathcal{C} = \\{0.10, 0.12, 0.14, \\dots, 0.90\\}$ and half-lengths $s \\in \\mathcal{S} = \\{0.02, 0.03, 0.04, \\dots, 0.25\\}$. Only consider candidates with $0 < c - s < c + s < 1$.\n- Use $\\lambda = 0.001$.\n\nTest suite:\n- Case $1$ (happy path): $[a^\\star,b^\\star] = [0.40, 0.60]$.\n- Case $2$ (small obstacle): $[a^\\star,b^\\star] = [0.20, 0.25]$.\n- Case $3$ (near boundary): $[a^\\star,b^\\star] = [0.80, 0.95]$.\n\nYour program must implement the above data generation and identification for each test case (with a fixed random seed for noise to ensure reproducibility) and return, for each case, the identified interval endpoints $[\\hat{a}, \\hat{b}]$ rounded to three decimal places. All quantities are dimensionless, so no physical units are required. The final output format must be a single line containing a comma-separated list of three items, each item being a two-element list of floats corresponding to $[\\hat{a}, \\hat{b}]$ for the cases in the order given. For example,\n\"[ [0.400,0.600],[0.200,0.250],[0.800,0.950] ]\"\nis a valid output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in this exact style, without any additional text.",
            "solution": "The problem requires the identification of an unknown solid obstacle within a one-dimensional, steady, viscous flow. This is a classic inverse problem, which we will solve using a physics-informed model discovery framework. The core idea is to find the parameters of a physical model—in this case, the start and end points $[a,b]$ of an obstacle—that best explain a set of sparse, noisy observations, while a penalty term encourages a simpler or more \"sparse\" model.\n\nThe physical model is a Brinkman-penalized steady Stokes equation in one dimension, a well-established formulation for simulating flows with immersed boundaries. The governing equation is a second-order ordinary differential equation (ODE):\n$$\n-\\nu \\, \\frac{d^2 u}{dx^2} + \\alpha \\, \\chi(x) \\, u(x) = 0, \\quad x \\in (0,1)\n$$\nsubject to Dirichlet boundary conditions $u(0) = U_0$ and $u(1) = U_1$. Here, $u(x)$ is the scalar velocity field, $\\nu$ is the kinematic viscosity, and $\\alpha$ is a large penalization parameter. The function $\\chi(x)$ is an indicator for the solid obstacle, defined as $\\chi(x) = 1$ if $x \\in [a,b]$ and $\\chi(x) = 0$ otherwise. The term $\\alpha \\chi(x) u(x)$ acts as a \"flow resistance\" term that forces the velocity $u(x)$ to be nearly zero inside the obstacle, mimicking a no-slip condition in the limit of $\\alpha \\to \\infty$.\n\nTo solve this problem computationally, we first discretize the domain $[0,1]$ using a uniform grid of $N$ points, $x_i = i h$ for $i \\in \\{0, 1, \\dots, N-1\\}$, where the grid spacing is $h = 1/(N-1)$. We denote the numerical approximation of $u(x_i)$ as $u_i$. The second derivative $u''(x_i)$ is approximated using a second-order centered finite difference stencil:\n$$\n\\frac{d^2 u}{dx^2}\\bigg|_{x=x_i} \\approx \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2}\n$$\nSubstituting this into the governing ODE for each interior grid point $i \\in \\{1, 2, \\dots, N-2\\}$ yields a system of linear algebraic equations:\n$$\n-\\nu \\left( \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} \\right) + \\alpha \\, \\chi_i \\, u_i = 0\n$$\nwhere $\\chi_i = 1$ if $x_i \\in [a,b]$ and $0$ otherwise. Rearranging the terms for a generic interior node $i$, we get:\n$$\n-\\frac{\\nu}{h^2} u_{i-1} + \\left( \\frac{2\\nu}{h^2} + \\alpha \\chi_i \\right) u_i - \\frac{\\nu}{h^2} u_{i+1} = 0\n$$\nThis system of $N-2$ equations is for the $N-2$ unknown interior velocities $\\{u_1, u_2, \\dots, u_{N-2}\\}$. The boundary values $u_0 = U_0$ and $u_{N-1} = U_1$ are known. We incorporate them by modifying the equations for the first ($i=1$) and last ($i=N-2$) interior nodes:\nFor $i=1$:\n$$\n\\left( \\frac{2\\nu}{h^2} + \\alpha \\chi_1 \\right) u_1 - \\frac{\\nu}{h^2} u_2 = \\frac{\\nu}{h^2} u_0 = \\frac{\\nu}{h^2} U_0\n$$\nFor $i=N-2$:\n$$\n-\\frac{\\nu}{h^2} u_{N-3} + \\left( \\frac{2\\nu}{h^2} + \\alpha \\chi_{N-2} \\right) u_{N-2} = \\frac{\\nu}{h^2} u_{N-1} = \\frac{\\nu}{h^2} U_1\n$$\nThis results in a tridiagonal linear system of the form $A\\mathbf{v} = \\mathbf{f}$, where $\\mathbf{v} = [u_1, u_2, \\dots, u_{N-2}]^T$ is the vector of unknown velocities. The matrix $A$ is tridiagonal and diagonally dominant, which guarantees a unique solution that can be found very efficiently using specialized algorithms, such as the one provided by `scipy.linalg.solve_banded`.\n\nThe inverse problem consists of finding the obstacle parameters $[a,b]$ that best explain a set of noisy velocity measurements $\\{\\tilde{u}_j\\}$ at locations $\\{x_j\\}$. The identification is performed by minimizing a physics-informed objective function $\\mathcal{J}(a,b)$:\n$$\n\\mathcal{J}(a,b) = \\frac{1}{M}\\sum_{j=1}^{M} \\left(u(x_j; a,b) - \\tilde{u}_j\\right)^2 \\;+\\; \\lambda \\, (b - a)\n$$\nThe first term is a data fidelity term, a mean squared error between the model prediction $u(x_j; a,b)$ and the observed data $\\tilde{u}_j$. For each candidate interval $[a,b]$, the prediction $u(x; a,b)$ is generated by solving the discretized BVP, ensuring that every candidate solution respects the underlying physics. The values $u(x_j; a,b)$ at off-grid data points are obtained via linear interpolation of the numerical solution. The second term, $\\lambda(b-a)$, is a regularization term that penalizes larger obstacles. This term promotes a \"sparse\" solution by favoring smaller intervals, which is a common technique in model discovery inspired by methods like LASSO regression.\n\nWe are instructed not to use gradient-based optimization but to perform a grid search over a discrete set of candidate intervals. Each candidate is parameterized by its center $c$ and half-length $s$, where $a=c-s$ and $b=c+s$. The search spaces are $\\mathcal{C} = \\{0.10, 0.12, \\dots, 0.90\\}$ and $\\mathcal{S} = \\{0.02, 0.03, \\dots, 0.25\\}$. We iterate through all valid pairs $(c,s)$ that satisfy the constraint $0 < a < b < 1$, compute $\\mathcal{J}(a,b)$ for each, and select the pair $(\\hat{c}, \\hat{s})$ that minimizes the objective. The final identified obstacle is $[\\hat{a}, \\hat{b}] = [\\hat{c}-\\hat{s}, \\hat{c}+\\hat{s}]$.\n\nThe data generation for each test case follows a fixed protocol. First, a \"true\" velocity field $u^\\star(x)$ is computed by solving the BVP with a known true obstacle $[a^\\star, b^\\star]$. Then, $M$ observation points are selected from a uniform grid, excluding points inside the true obstacle. Synthetic data $\\tilde{u}_j$ are created by evaluating $u^\\star$ at these points and adding a small amount of zero-mean Gaussian noise. This process realistically simulates an experimental scenario where measurements are sparse, noisy, and available only in the accessible fluid region. A fixed random seed ensures the reproducibility of the noise and thus the entire experiment.\n\nThe overall algorithm is implemented as follows:\n1.  For each test case with a given $[a^\\star, b^\\star]$:\n    a.  Generate the synthetic data $(\\{x_j\\}, \\{\\tilde{u}_j\\})$ according to the specified protocol.\n    b.  Initialize a minimum objective value to infinity.\n    c.  Iterate through all candidate centers $c \\in \\mathcal{C}$ and half-lengths $s \\in \\mathcal{S}$.\n    d.  For each pair $(c,s)$, form the candidate interval $[a,b]=[c-s, c+s]$ and check if it is valid (i.e., $0 < a < b < 1$).\n    e.  If valid, solve the discretized PDE with this $[a,b]$ to get the predicted velocity field $u(x; a,b)$.\n    f.  Linearly interpolate the solution to find predicted velocities at data points $x_j$.\n    g.  Compute the objective function $\\mathcal{J}(a,b)$.\n    h.  If the computed $\\mathcal{J}$ is lower than the current minimum, update the minimum and store $[a,b]$ as the best-found interval.\n2.  After the search is complete, the best-found interval $[\\hat{a}, \\hat{b}]$ is recorded for the test case.\n3.  The final output is a list of the identified intervals for all test cases, formatted as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the physics-informed model discovery problem for three test cases.\n    Identifies the location of an obstacle in a 1D flow by minimizing a\n    physics-informed objective function over a discrete search space.\n    \"\"\"\n\n    # Fixed parameters for all test cases\n    params = {\n        'N': 257,\n        'nu': 0.01,\n        'alpha': 1000.0,\n        'U0': 1.0,\n        'U1': 0.0,\n        'lambda_reg': 0.001,\n        'noise_std': 0.001,\n        'M_tot': 25,\n        'seed': 0,\n    }\n\n    # Test cases: each with a true obstacle [a_star, b_star]\n    test_cases = [\n        [0.40, 0.60],  # Case 1: Happy path\n        [0.20, 0.25],  # Case 2: Small obstacle\n        [0.80, 0.95],  # Case 3: Near boundary\n    ]\n    \n    # Search space for candidate obstacles\n    centers_c = np.arange(0.10, 0.90 + 1e-9, 0.02)\n    half_lengths_s = np.arange(0.02, 0.25 + 1e-9, 0.01)\n\n    results = []\n\n    for a_star, b_star in test_cases:\n        # Step 1: Data Generation\n        \n        # Grid setup\n        x_grid = np.linspace(0, 1, params['N'])\n        \n        # Generate true solution\n        u_true = solve_bvp(a_star, b_star, x_grid, params)\n        \n        # Generate observation points\n        x_obs_candidates = np.linspace(0, 1, params['M_tot'])\n        data_x = []\n        for x_val in x_obs_candidates:\n            if not (a_star <= x_val <= b_star):\n                data_x.append(x_val)\n        data_x = np.array(data_x)\n        \n        # Get true velocity at observation points via interpolation\n        u_obs_true = np.interp(data_x, x_grid, u_true)\n        \n        # Add noise\n        rng = np.random.default_rng(params['seed'])\n        noise = rng.normal(0, params['noise_std'], size=u_obs_true.shape)\n        data_u_tilde = u_obs_true + noise\n\n        # Step 2: Identification via Grid Search\n        \n        min_J = float('inf')\n        best_ab = None\n\n        for c in centers_c:\n            for s in half_lengths_s:\n                a_cand = c - s\n                b_cand = c + s\n\n                # Check validity of the candidate interval\n                if not (0 < a_cand < b_cand < 1):\n                    continue\n\n                # Solve BVP for the candidate interval\n                u_pred = solve_bvp(a_cand, b_cand, x_grid, params)\n                \n                # Interpolate to get predictions at data points\n                u_pred_at_data = np.interp(data_x, x_grid, u_pred)\n                \n                # Calculate objective function J\n                mse = np.mean((u_pred_at_data - data_u_tilde)**2)\n                regularization = params['lambda_reg'] * (b_cand - a_cand)\n                J = mse + regularization\n                \n                # Update best candidate\n                if J < min_J:\n                    min_J = J\n                    best_ab = [a_cand, b_cand]\n\n        # Round results to three decimal places\n        if best_ab:\n            final_a = round(best_ab[0], 3)\n            final_b = round(best_ab[1], 3)\n            results.append([final_a, final_b])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is already in the correct format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_bvp(a, b, x_grid, params):\n    \"\"\"\n    Solves the 1D Brinkman-penalized BVP using finite differences.\n    -nu * u_xx + alpha * chi * u = 0\n    u(0) = U0, u(1) = U1\n    \"\"\"\n    N = params['N']\n    nu = params['nu']\n    alpha = params['alpha']\n    U0 = params['U0']\n    U1 = params['U1']\n    \n    h = 1.0 / (N - 1)\n    \n    # Interior grid points (N-2 unknowns)\n    x_int = x_grid[1:-1]\n    \n    # Indicator function on the grid\n    chi = np.zeros(N)\n    chi[(x_grid >= a) & (x_grid <= b)] = 1.0\n    chi_int = chi[1:-1]\n    \n    # Construct the tridiagonal matrix A for the system Av = f\n    # The system is for the N-2 interior points.\n    \n    # Main diagonal\n    main_diag = (2.0 * nu / h**2) + (alpha * chi_int)\n    \n    # Off-diagonals\n    off_diag_val = -nu / h**2\n    upper_diag = np.full(N - 2, off_diag_val)\n    lower_diag = np.full(N - 2, off_diag_val)\n\n    # Scipy's solve_banded expects the matrix in a specific format\n    # (number of bands, length of main diagonal)\n    # Row 0: upper diagonal (padded with a 0 at the start)\n    # Row 1: main diagonal\n    # Row 2: lower diagonal (padded with a 0 at the end)\n    A_banded = np.zeros((3, N - 2))\n    A_banded[0, 1:] = upper_diag[:-1]\n    A_banded[1, :] = main_diag\n    A_banded[2, :-1] = lower_diag[1:]\n    \n    # Construct the right-hand side vector f\n    f = np.zeros(N - 2)\n    f[0] = -off_diag_val * U0\n    f[-1] = -off_diag_val * U1\n\n    # Solve the linear system for interior points\n    u_int = solve_banded((1, 1), A_banded, f)\n    \n    # Combine with boundary conditions to form the full solution\n    u_full = np.concatenate(([U0], u_int, [U1]))\n    \n    return u_full\n\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}