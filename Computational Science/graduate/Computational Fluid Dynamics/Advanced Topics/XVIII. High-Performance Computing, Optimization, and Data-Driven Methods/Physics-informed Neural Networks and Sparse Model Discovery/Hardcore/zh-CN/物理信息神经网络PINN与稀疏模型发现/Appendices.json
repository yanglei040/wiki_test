{
    "hands_on_practices": [
        {
            "introduction": "数据驱动的模型发现方法，例如偏微分方程函数辨识（PDE-FIND），依赖于从测量数据中精确计算的导数。然而，现实世界的数据总是被噪声所污染，而数值微分会显著放大这些噪声。本实践 () 提供了一个基础的理论分析，旨在量化测量噪声如何传播到导数估计中，并最终增加通过稀疏回归发现的模型系数的方差。通过推导方差膨胀因子，您将更深刻地理解从含噪观测中发现物理定律所面临的内在挑战。",
            "id": "3352042",
            "problem": "考虑一个一维扩散过程，由偏微分方程 $\\frac{\\partial u}{\\partial t} = c \\frac{\\partial^{2} u}{\\partial x^{2}}$ 控制，其中 $c$ 是一个常数扩散系数。假设在空间步长为 $\\Delta x$、时间步长为 $\\Delta t$ 的均匀网格上观测时空场 $u(x,t)$，但观测值受到加性、独立同分布的零均值高斯测量噪声的污染：$\\tilde{u}(x_i,t_j) = u(x_i,t_j) + \\eta_{i,j}$，其中 $\\eta_{i,j} \\sim \\mathcal{N}(0,\\sigma_u^{2})$ 且对所有 $(i,j)$ 独立。使用中心有限差分法从 $\\tilde{u}$ 估计导数：\n- 时间导数估计：$\\tilde{u}_{t}(x_i,t_j) \\approx \\frac{\\tilde{u}(x_i,t_{j+1}) - \\tilde{u}(x_i,t_{j-1})}{2 \\Delta t}$。\n- 空间二阶导数估计：$\\tilde{u}_{xx}(x_i,t_j) \\approx \\frac{\\tilde{u}(x_{i+1},t_j) - 2 \\tilde{u}(x_i,t_j) + \\tilde{u}(x_{i-1},t_j)}{\\Delta x^{2}}$。\n\n你应用偏微分方程函数辨识 (PDE-FIND) 方法，该方法构建一个包含单一回归量 $\\Theta = u_{xx}$ 和目标 $y = u_t$ 的库，并通过普通最小二乘法在 $K$ 个选定的时空样本上估计 $c$：$\\hat{c} = \\left((\\Theta^{\\top}\\Theta)^{-1}\\Theta^{\\top}y\\right)$，其中 $\\Theta$ 和 $y$ 均由含噪声的导数估计值 $\\tilde{u}_{xx}$ 和 $\\tilde{u}_{t}$ 构成。假设：\n- $K$ 个样本的选择方式使得用于计算不同样本的 $(\\tilde{u}_{t},\\tilde{u}_{xx})$ 的有限差分模板互不相交，从而确保了各样本间噪声的独立性。\n- $\\Theta$ 和 $y$ 中的噪声完全源于 $\\tilde{u}$ 中的测量噪声通过上述线性有限差分算子的传播。\n- 关于无噪声量的小噪声线性化是有效的，因此主阶误差传播决定了 $\\hat{c}$ 的方差。\n\n从独立高斯随机变量线性变换的性质和普通最小二乘估计量的定义出发，推导导数估计量由测量噪声引起的方差，然后推导当 $\\Theta$ 和 $y$ 均含噪声时 $\\hat{c}$ 的主阶方差。最后，计算方差膨胀因子，其定义为当 $\\Theta$ 和 $y$ 均含噪声时的 $\\operatorname{Var}(\\hat{c})$ 与仅 $y$ 含噪声时的 $\\operatorname{Var}(\\hat{c})$ 之比，并将该因子表示为以 $c$、$\\Delta x$ 和 $\\Delta t$ 表示的闭式解析表达式。请以精确表达式的形式提供最终答案；无需四舍五入，也无需物理单位。",
            "solution": "该问题要求在偏微分方程函数辨识 (PDE-FIND) 的背景下，推导通过普通最小二乘法 (OLS) 估计的系数的方差膨胀因子。该因子是当回归量和目标变量均含噪声时估计量的方差与仅目标变量含噪声时方差之比。推导过程主要分为四个步骤：1) 计算含噪声导数估计的方差，2) 计算标准情况下（回归量无噪声）OLS 估计量 $\\hat{c}$ 的方差，3) 计算变量含误差情况下（回归量含噪声）$\\hat{c}$ 的主阶方差，以及 4) 计算这些方差的比值。\n\n首先，我们确定含噪声的有限差分导数估计量的方差。在网格点 $(x_i, t_j)$ 上的观测场 $\\tilde{u}$ 由 $\\tilde{u}(x_i,t_j) = u(x_i,t_j) + \\eta_{i,j}$ 给出，其中 $u$ 是真实场，而 $\\eta_{i,j}$ 是来自正态分布 $\\mathcal{N}(0, \\sigma_u^2)$ 的独立同分布随机变量。\n\n在点 $(x_i, t_j)$ 处的时间导数估计 $\\tilde{u}_t$ 为：\n$$ \\tilde{u}_t(x_i, t_j) = \\frac{\\tilde{u}(x_i, t_{j+1}) - \\tilde{u}(x_i, t_{j-1})}{2 \\Delta t} = \\frac{u(x_i, t_{j+1}) - u(x_i, t_{j-1})}{2 \\Delta t} + \\frac{\\eta_{i, j+1} - \\eta_{i, j-1}}{2 \\Delta t} $$\n令 $y_k^{\\text{obs}}$ 表示含噪声时间导数的第 $k$ 个样本，$y_k$ 为其真实对应值（由真实场 $u$ 的有限差分近似），$\\epsilon_{y,k}$ 为噪声贡献。噪声项为 $\\epsilon_{y,k} = \\frac{\\eta_{i, j+1} - \\eta_{i, j-1}}{2 \\Delta t}$。由于测量噪声 $\\eta_{i,j}$ 是独立的，方差为 $\\sigma_u^2$，因此时间导数估计中噪声的方差（我们记为 $\\sigma_y^2$）为：\n$$ \\sigma_y^2 = \\operatorname{Var}(\\epsilon_{y,k}) = \\operatorname{Var}\\left(\\frac{\\eta_{i, j+1} - \\eta_{i, j-1}}{2 \\Delta t}\\right) = \\frac{1}{(2 \\Delta t)^2} \\left( \\operatorname{Var}(\\eta_{i, j+1}) + \\operatorname{Var}(-\\eta_{i, j-1}) \\right) = \\frac{\\sigma_u^2 + \\sigma_u^2}{4 \\Delta t^2} = \\frac{2\\sigma_u^2}{4 \\Delta t^2} = \\frac{\\sigma_u^2}{2 \\Delta t^2} $$\n\n类似地，在点 $(x_i, t_j)$ 处的空间二阶导数估计 $\\tilde{u}_{xx}$ 为：\n$$ \\tilde{u}_{xx}(x_i, t_j) = \\frac{\\tilde{u}(x_{i+1}, t_j) - 2 \\tilde{u}(x_i, t_j) + \\tilde{u}(x_{i-1}, t_j)}{\\Delta x^2} = u_{xx}^{\\text{FD}} + \\frac{\\eta_{i+1, j} - 2\\eta_{i, j} + \\eta_{i-1, j}}{\\Delta x^2} $$\n令 $\\Theta_k^{\\text{obs}}$ 表示含噪声空间导数的第 $k$ 个样本，$\\Theta_k$ 为其真实对应值，$\\epsilon_{\\Theta,k}$ 为噪声贡献。噪声项为 $\\epsilon_{\\Theta,k} = \\frac{\\eta_{i+1, j} - 2\\eta_{i, j} + \\eta_{i-1, j}}{\\Delta x^2}$。此噪声的方差（记为 $\\sigma_\\Theta^2$）为：\n$$ \\sigma_\\Theta^2 = \\operatorname{Var}(\\epsilon_{\\Theta,k}) = \\operatorname{Var}\\left(\\frac{\\eta_{i+1, j} - 2\\eta_{i, j} + \\eta_{i-1, j}}{\\Delta x^2}\\right) = \\frac{1}{(\\Delta x^2)^2} \\left( \\operatorname{Var}(\\eta_{i+1, j}) + \\operatorname{Var}(-2\\eta_{i, j}) + \\operatorname{Var}(\\eta_{i-1, j}) \\right) $$\n$$ \\sigma_\\Theta^2 = \\frac{\\sigma_u^2 + (-2)^2\\sigma_u^2 + \\sigma_u^2}{\\Delta x^4} = \\frac{6\\sigma_u^2}{\\Delta x^4} $$\n问题陈述指出，在给定样本点上，$\\tilde{u}_t$ 和 $\\tilde{u}_{xx}$ 的有限差分模板是不相交的，不同样本 $k$ 的模板也是如此。$\\tilde{u}_t$ 中的噪声取决于时间步 $t_{j\\pm 1}$ 处的 $\\eta$，而 $\\tilde{u}_{xx}$ 中的噪声取决于时间 $t_j$ 处空间位置 $x_{i\\pm 1}, x_i$ 的 $\\eta$。因此，对于任何样本 $k$，$\\epsilon_{y,k}$ 和 $\\epsilon_{\\Theta,k}$ 是独立的。此外，所有噪声项在不同样本 $k$ 之间也是独立的。\n\n接下来，我们计算仅目标变量 $y$ 含噪声（回归量 $\\Theta$ 无噪声）情况下 $\\hat{c}$ 的方差。模型为 $y_k^{\\text{obs}} = c \\Theta_k + \\epsilon_{y,k}$。单个系数 $c$ 的 OLS 估计量为：\n$$ \\hat{c} = \\frac{\\sum_{k=1}^K \\Theta_k y_k^{\\text{obs}}}{\\sum_{k=1}^K \\Theta_k^2} = \\frac{\\sum_{k=1}^K \\Theta_k (c \\Theta_k + \\epsilon_{y,k})}{\\sum_{k=1}^K \\Theta_k^2} = c + \\frac{\\sum_{k=1}^K \\Theta_k \\epsilon_{y,k}}{\\sum_{k=1}^K \\Theta_k^2} $$\n该估计量的方差为：\n$$ \\operatorname{Var}(\\hat{c})_{\\text{clean } \\Theta} = \\operatorname{Var}\\left(c + \\frac{\\sum_{k=1}^K \\Theta_k \\epsilon_{y,k}}{\\sum_{k=1}^K \\Theta_k^2}\\right) = \\frac{1}{\\left(\\sum_{k=1}^K \\Theta_k^2\\right)^2} \\operatorname{Var}\\left(\\sum_{k=1}^K \\Theta_k \\epsilon_{y,k}\\right) $$\n由于 $\\epsilon_{y,k}$ 在样本 $k$ 之间是独立的，这变为：\n$$ \\operatorname{Var}(\\hat{c})_{\\text{clean } \\Theta} = \\frac{1}{\\left(\\sum_{k=1}^K \\Theta_k^2\\right)^2} \\sum_{k=1}^K \\Theta_k^2 \\operatorname{Var}(\\epsilon_{y,k}) = \\frac{\\sigma_y^2 \\sum_{k=1}^K \\Theta_k^2}{\\left(\\sum_{k=1}^K \\Theta_k^2\\right)^2} = \\frac{\\sigma_y^2}{\\sum_{k=1}^K \\Theta_k^2} $$\n\n现在，我们考虑回归量和目标均含噪声的情况（变量含误差）。观测到的量是 $y_k^{\\text{obs}} = y_k + \\epsilon_{y,k}$ 和 $\\Theta_k^{\\text{obs}} = \\Theta_k + \\epsilon_{\\Theta,k}$。我们假设真实关系 $y_k = c \\Theta_k$ 成立。OLS 估计量为：\n$$ \\hat{c} = \\frac{\\sum_{k=1}^K \\Theta_k^{\\text{obs}} y_k^{\\text{obs}}}{\\sum_{k=1}^K (\\Theta_k^{\\text{obs}})^2} = \\frac{\\sum_{k=1}^K (\\Theta_k + \\epsilon_{\\Theta,k})(c \\Theta_k + \\epsilon_{y,k})}{\\sum_{k=1}^K (\\Theta_k + \\epsilon_{\\Theta,k})^2} $$\n利用小噪声线性化假设，我们展开并只保留噪声的一阶项。令 $S_{\\Theta\\Theta} = \\sum_{k=1}^K \\Theta_k^2$。\n$$ \\hat{c} \\approx \\frac{c S_{\\Theta\\Theta} + \\sum_k (\\Theta_k \\epsilon_{y,k} + c \\Theta_k \\epsilon_{\\Theta,k})}{S_{\\Theta\\Theta} + 2 \\sum_k \\Theta_k \\epsilon_{\\Theta,k}} \\approx \\left(c + \\frac{\\sum_k (\\Theta_k \\epsilon_{y,k} + c \\Theta_k \\epsilon_{\\Theta,k})}{S_{\\Theta\\Theta}}\\right) \\left(1 - \\frac{2 \\sum_k \\Theta_k \\epsilon_{\\Theta,k}}{S_{\\Theta\\Theta}}\\right) $$\n展开此乘积并仅保留噪声误差 $\\epsilon$ 的线性项，得到估计误差的一阶近似：\n$$ \\hat{c} - c \\approx \\frac{\\sum_k \\Theta_k \\epsilon_{y,k}}{S_{\\Theta\\Theta}} + \\frac{c \\sum_k \\Theta_k \\epsilon_{\\Theta,k}}{S_{\\Theta\\Theta}} - \\frac{2c \\sum_k \\Theta_k \\epsilon_{\\Theta,k}}{S_{\\Theta\\Theta}} = \\frac{\\sum_k \\Theta_k (\\epsilon_{y,k} - c \\epsilon_{\\Theta,k})}{S_{\\Theta\\Theta}} $$\n$\\hat{c}$ 的主阶方差是此表达式的方差：\n$$ \\operatorname{Var}(\\hat{c})_{\\text{noisy } \\Theta, y} \\approx \\operatorname{Var}\\left(\\frac{\\sum_k \\Theta_k (\\epsilon_{y,k} - c \\epsilon_{\\Theta,k})}{S_{\\Theta\\Theta}}\\right) = \\frac{1}{S_{\\Theta\\Theta}^2} \\sum_k \\Theta_k^2 \\operatorname{Var}(\\epsilon_{y,k} - c \\epsilon_{\\Theta,k}) $$\n由于对于每个样本 $k$，$\\epsilon_{y,k}$ 和 $\\epsilon_{\\Theta,k}$ 是独立的：\n$$ \\operatorname{Var}(\\epsilon_{y,k} - c \\epsilon_{\\Theta,k}) = \\operatorname{Var}(\\epsilon_{y,k}) + c^2 \\operatorname{Var}(\\epsilon_{\\Theta,k}) = \\sigma_y^2 + c^2 \\sigma_\\Theta^2 $$\n将此代回，我们得到：\n$$ \\operatorname{Var}(\\hat{c})_{\\text{noisy } \\Theta, y} \\approx \\frac{1}{S_{\\Theta\\Theta}^2} \\sum_k \\Theta_k^2 (\\sigma_y^2 + c^2 \\sigma_\\Theta^2) = \\frac{(\\sigma_y^2 + c^2 \\sigma_\\Theta^2) \\sum_k \\Theta_k^2}{S_{\\Theta\\Theta}^2} = \\frac{\\sigma_y^2 + c^2 \\sigma_\\Theta^2}{\\sum_{k=1}^K \\Theta_k^2} $$\n\n最后，我们计算方差膨胀因子，即两个方差之比：\n$$ \\text{VIF} = \\frac{\\operatorname{Var}(\\hat{c})_{\\text{noisy } \\Theta, y}}{\\operatorname{Var}(\\hat{c})_{\\text{clean } \\Theta}} \\approx \\frac{\\frac{\\sigma_y^2 + c^2 \\sigma_\\Theta^2}{\\sum_{k=1}^K \\Theta_k^2}}{\\frac{\\sigma_y^2}{\\sum_{k=1}^K \\Theta_k^2}} = \\frac{\\sigma_y^2 + c^2 \\sigma_\\Theta^2}{\\sigma_y^2} = 1 + c^2 \\frac{\\sigma_\\Theta^2}{\\sigma_y^2} $$\n现在，我们代入 $\\sigma_y^2$ 和 $\\sigma_\\Theta^2$ 的表达式：\n$$ \\text{VIF} = 1 + c^2 \\frac{\\left(\\frac{6\\sigma_u^2}{\\Delta x^4}\\right)}{\\left(\\frac{\\sigma_u^2}{2\\Delta t^2}\\right)} $$\n测量噪声方差 $\\sigma_u^2$ 被消去：\n$$ \\text{VIF} = 1 + c^2 \\left(\\frac{6}{\\Delta x^4}\\right) \\left(2\\Delta t^2\\right) = 1 + \\frac{12 c^2 \\Delta t^2}{\\Delta x^4} $$\n该表达式表示，由于测量噪声除了污染目标变量 $\\tilde{u}_t$ 外，还污染了回归变量 $\\tilde{u}_{xx}$，从而导致估计系数 $\\hat{c}$ 的方差被放大的倍数。",
            "answer": "$$\n\\boxed{1 + \\frac{12 c^{2} \\Delta t^{2}}{\\Delta x^{4}}}\n$$"
        },
        {
            "introduction": "尽管物理信息神经网络（PINNs）将物理定律直接嵌入学习过程中，但其训练并非总是一帆风顺，尤其是在处理涉及不同尺度的多种物理现象的问题时。本动手编程练习 () 深入探讨了一种常见的训练失败模式——梯度病态（gradient pathology）。在这种情况下，偏微分方程残差中不同项（例如，对流项与扩散项）的梯度之间的不平衡会阻碍训练过程。通过分析粘性伯格斯方程在粘度降低时的行为，您将学会如何通过监测梯度范数来诊断此问题，并理解其对 PINNs 收敛性的关键影响。",
            "id": "3352001",
            "problem": "要求您分析物理信息神经网络 (PINNs) 在求解一维粘性 Burgers 方程时的敏感性及潜在的训练失败问题。您的分析必须实现为一个完整的、可运行的程序，该程序能为一个固定的参数化试探场计算残差大小和梯度范数，并随着粘度的降低预测由梯度病态引起的失败。其基本控制定律是一维粘性 Burgers 方程，这是计算流体动力学 (CFD) 中对流-扩散问题的一个原型模型，写作非线性平流和粘性扩散的平衡定律：$$u_{t} + u\\,u_{x} - \\nu\\,u_{xx} = 0,$$ 其中 $u=u(x,t)$，$x$ 是位置，$t$ 是时间，$\\nu$ 是运动粘度。您的数值计算将在无量纲下进行，因此不需要物理单位。测试套件包含一组粘度值 $\\nu$，这些值从中等大小延伸到趋近于零。\n\n您的程序必须基于以下基本基础和定义。考虑一个固定的参数化试探场，它模拟了一个具有线性输出层和固定基函数特征的小型神经网络：$$u(x,t;\\boldsymbol{\\theta}) = \\sum_{m=1}^{3}\\theta_{m}\\,\\phi_{m}(x,t),$$ 其中基函数为 $$\\phi_{1}(x,t)=\\sin(\\pi x)\\,e^{-t},\\quad \\phi_{2}(x,t)=\\sin(2\\pi x)\\,e^{-t},\\quad \\phi_{3}(x,t)=\\cos(\\pi x)\\,e^{-2t},$$ 且参数向量固定为 $$\\boldsymbol{\\theta}=\\begin{bmatrix}\\theta_{1}\\\\\\theta_{2}\\\\\\theta_{3}\\end{bmatrix}=\\begin{bmatrix}1.0\\\\-0.5\\\\0.75\\end{bmatrix}.$$ 根据导数的基本定义，计算 $$u_{t}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{t}\\phi_{m},\\quad u_{x}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{x}\\phi_{m},\\quad u_{xx}=\\sum_{m=1}^{3}\\theta_{m}\\,\\partial_{xx}\\phi_{m},$$ 并定义逐点的偏微分方程残差 $$\\mathcal{R}(x,t;\\nu,\\boldsymbol{\\theta}) = u_{t} + u\\,u_{x} - \\nu\\,u_{xx}.$$ 定义在一个矩形时空配置网格上的均方残差为 $$\\mathcal{L}(\\nu,\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})^{2},$$ 其中 $(x_{i},t_{i})$ 是配置点，$N$ 是总点数。根据链式法则，损失函数关于参数的梯度为 $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\nu,\\boldsymbol{\\theta}) = \\frac{2}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})\\,\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta}).$$ 为分析不同物理项之间的不平衡，将残差分解为 $$\\mathcal{R}=\\mathcal{T}+\\mathcal{A}-\\mathcal{D},\\quad \\text{with}\\quad \\mathcal{T}=u_{t},\\ \\mathcal{A}=u\\,u_{x},\\ \\mathcal{D}=\\nu\\,u_{xx}.$$ 那么残差关于参数的雅可比矩阵可以进行如下精确分解 $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R} = \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{T}}_{\\text{time}} + \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{A}}_{\\text{advection}} - \\underbrace{\\nabla_{\\boldsymbol{\\theta}}\\mathcal{D}}_{\\text{diffusion}}.$$ 利用 $u$ 对于 $\\boldsymbol{\\theta}$ 的线性，以及 $\\mathcal{A}=u\\,u_{x}$ 的乘法法则，可得 $$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{T}=\\begin{bmatrix}\\partial_{t}\\phi_{1}\\\\ \\partial_{t}\\phi_{2}\\\\ \\partial_{t}\\phi_{3}\\end{bmatrix},\\quad \\nabla_{\\boldsymbol{\\theta}}\\mathcal{A}=\\begin{bmatrix}\\phi_{1}\\,u_{x}+u\\,\\partial_{x}\\phi_{1}\\\\ \\phi_{2}\\,u_{x}+u\\,\\partial_{x}\\phi_{2}\\\\ \\phi_{3}\\,u_{x}+u\\,\\partial_{x}\\phi_{3}\\end{bmatrix},\\quad \\nabla_{\\boldsymbol{\\theta}}\\mathcal{D}=\\nu\\begin{bmatrix}\\partial_{xx}\\phi_{1}\\\\ \\partial_{xx}\\phi_{2}\\\\ \\partial_{xx}\\phi_{3}\\end{bmatrix}.$$ 因此，损失梯度可加性地分解为三个分量，$$\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}=\\mathbf{g}_{\\mathcal{T}}+\\mathbf{g}_{\\mathcal{A}}+\\mathbf{g}_{\\mathcal{D}},\\quad \\text{where}\\quad \\mathbf{g}_{\\mathcal{X}}=\\frac{2}{N}\\sum_{i=1}^{N}\\mathcal{R}(x_{i},t_{i})\\,\\nabla_{\\boldsymbol{\\theta}}\\mathcal{X}(x_{i},t_{i}),$$ 对于每个 $\\mathcal{X}\\in\\{\\mathcal{T},\\mathcal{A},-\\mathcal{D}\\}$，其中符号约定已被吸收到 $\\mathbf{g}_{\\mathcal{D}}$ 的定义中。\n\n您的任务是：\n- 使用以上定义，在均匀网格（$x\\in[0,1]$, $t\\in[0,1]$, $N_{x}=64$, $N_{t}=64$，以及 $N=N_{x}N_{t}$）上，为每个给定的粘度 $\\nu$ 计算以下量：\n  - 均方根残差 $$\\mathrm{RMS}(\\nu)=\\sqrt{\\mathcal{L}(\\nu,\\boldsymbol{\\theta})}.$$\n  - 损失梯度分量 $\\lVert \\mathbf{g}_{\\mathcal{T}}\\rVert_{2}$、$\\lVert \\mathbf{g}_{\\mathcal{A}}\\rVert_{2}$ 和 $\\lVert \\mathbf{g}_{\\mathcal{D}}\\rVert_{2}$ 的欧几里得范数，以及平流-扩散梯度不平衡比 $$\\rho(\\nu)=\\frac{\\lVert \\mathbf{g}_{\\mathcal{A}}\\rVert_{2}}{\\max\\{\\lVert \\mathbf{g}_{\\mathcal{D}}\\rVert_{2},\\varepsilon\\}},$$ 其中 $\\varepsilon=10^{-16}$ 以避免除以零。\n  - 残差雅可比矩阵的条件数 $$\\mathbf{J}(\\nu)\\in\\mathbb{R}^{N\\times 3},\\quad \\mathbf{J}_{i,m}=\\left[\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}(x_{i},t_{i};\\nu,\\boldsymbol{\\theta})\\right]_{m},$$ 定义为 $$\\kappa(\\mathbf{J})=\\frac{\\sigma_{\\max}(\\mathbf{J})}{\\sigma_{\\min}(\\mathbf{J})},$$ 其中 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 分别是 $\\mathbf{J}$ 的最大和最小奇异值。\n- 如果因梯度病态满足以下任一条件，则预测 PINNs 的训练失败：$$\\rho(\\nu)>\\kappa_{\\mathrm{th}} \\quad \\text{or} \\quad \\kappa(\\mathbf{J}(\\nu))>\\chi_{\\mathrm{th}},$$ 其中阈值为 $\\kappa_{\\mathrm{th}}=10^{3}$ 和 $\\chi_{\\mathrm{th}}=10^{8}$。\n\n测试套件和输入：\n- 使用固定的参数向量 $\\boldsymbol{\\theta}=\\begin{bmatrix}1.0\\\\-0.5\\\\0.75\\end{bmatrix}$。\n- 使用粘度值 $\\nu\\in\\{1.0,\\,0.1,\\,0.01,\\,0.001,\\,0.0\\}$。\n\n要求的输出和格式：\n- 对于测试套件中的每个 $\\nu$，计算并收集列表 $$[\\nu,\\ \\mathrm{RMS}(\\nu),\\ \\rho(\\nu),\\ f(\\nu)],$$ 其中如果预测训练失败，则 $f(\\nu)$ 为 $1$，否则为 $0$。\n- 您的程序应生成单行输出，其中包含一个由这类列表组成的逗号分隔列表，无空格，且每个浮点数格式化为六位有效数字，例如，$$[[1,0.123456,789.123,0],\\ldots].$$ 实际数字会有所不同；此格式是强制性的。",
            "solution": "本问题的目标是分析，当一维粘性 Burgers 方程的粘度参数 $\\nu$ 降低时，物理信息神经网络 (PINN) 训练过程的敏感性。该分析通过为一个固定的、模拟神经网络输出的简化试探解计算几个关键指标来执行。这些指标包括偏微分方程残差的均方根 (RMS)、损失函数梯度的各个分量的范数，以及残差雅可比矩阵的条件数。这些量被用来预测由梯度病态引起的潜在训练失败，特别是不同物理项（平流与扩散）之间的不平衡以及优化景观的病态条件。\n\n分析按以下步骤进行：\n\n1.  **解析公式**：我们从提供的解析定义开始。试探解 $u(x,t;\\boldsymbol{\\theta})$ 是三个预定义基函数 $\\phi_m(x,t)$ 的线性组合，其参数向量 $\\boldsymbol{\\theta}$ 是固定的。$u$ 关于空间的一阶和二阶偏导数 $u_x$ 和 $u_{xx}$，以及关于时间的偏导数 $u_t$，都是通过对求和项逐项求导来解析计算的。这些解析表达式对于精确评估偏微分方程残差及其梯度，而不引入数值微分误差至关重要。\n\n2.  **网格离散化**：连续时空域 $(x,t) \\in [0,1] \\times [0,1]$ 被离散化为一个由 $N_x \\times N_t = 64 \\times 64 = 4096$ 个配置点组成的均匀网格。所有后续计算都在此网格上执行。使用 `NumPy` 中的向量化操作可以高效地同时对所有点进行计算。\n\n3.  **残差和梯度计算**：对于测试套件中的每个粘度值 $\\nu$，我们在每个网格点上计算以下量：\n    -   试探解 $u$ 及其导数 $u_t$、$u_x$ 和 $u_{xx}$。\n    -   偏微分方程残差 $\\mathcal{R} = u_{t} + u\\,u_{x} - \\nu\\,u_{xx}$，它衡量试探解满足 Burgers 方程的程度。\n    -   残差关于参数 $\\boldsymbol{\\theta}$ 的梯度，称为残差雅可比 $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}$。这些梯度被分解为来自时间项 ($\\mathcal{T}=u_t$)、平流项 ($\\mathcal{A}=u\\,u_x$) 和扩散项 ($\\mathcal{D}=\\nu\\,u_{xx}$) 的贡献。试探解在 $\\boldsymbol{\\theta}$ 上的线性特性显著简化了这些梯度计算。例如，$\\nabla_{\\boldsymbol{\\theta}} u_t$ 就是基函数时间导数的向量 $[\\partial_t\\phi_1, \\partial_t\\phi_2, \\partial_t\\phi_3]^T$。非线性平流项的梯度使用乘法法则计算。\n\n4.  **指标计算**：使用上面计算的逐点量，我们将它们聚合成所需的全局指标：\n    -   **均方根残差, $\\mathrm{RMS}(\\nu)$**: 这是所有配置点上残差平方均值的平方根，即 $\\sqrt{\\frac{1}{N}\\sum_i \\mathcal{R}_i^2}$。它提供了对解精度的总体度量。\n    -   **损失梯度分量范数**: 均方误差损失函数的梯度 $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}$ 由三个向量 $\\mathbf{g}_{\\mathcal{T}}$、$\\mathbf{g}_{\\mathcal{A}}$ 和 $\\mathbf{g}_{\\mathcal{D}}$ 加性构成，分别对应于时间项、平流项和扩散项。每个向量的计算方法是，将残差 $\\mathcal{R}$ 与相应残差雅可比分量的乘积在所有网格点上求和，即 $\\mathbf{g}_{\\mathcal{X}} \\propto \\sum_i \\mathcal{R}_i \\nabla_{\\boldsymbol{\\theta}}\\mathcal{X}_i$。然后我们计算这三个向量中每一个的欧几里得范数。\n    -   **平流-扩散不平衡比, $\\rho(\\nu)$**: 该比率 $\\lVert\\mathbf{g}_\\mathcal{A}\\rVert_2 / \\max(\\lVert\\mathbf{g}_\\mathcal{D}\\rVert_2, \\varepsilon)$ 量化了平流梯度分量相对于扩散梯度分量的量级。一个大的值表明训练动态由平流项主导，当粘度趋于零且问题变为对流主导时，这可能会产生问题。\n    -   **条件数, $\\kappa(\\mathbf{J})$**: 残差雅可比矩阵 $\\mathbf{J} \\in \\mathbb{R}^{N\\times 3}$ 是通过堆叠来自 $N$ 个配置点中每个点的梯度向量 $\\nabla_{\\boldsymbol{\\theta}}\\mathcal{R}$ 形成的。它的条件数，即其最大奇异值与最小奇异值之比，衡量了残差对参数变化的敏感性。高条件数表示一个病态问题，其中参数的微小变化可能导致残差的巨大变化，这通常会减慢或停止基于梯度的优化。\n\n5.  **失败预测**：基于计算出的指标，如果不平衡比 $\\rho(\\nu)$ 超过阈值 $\\kappa_{\\mathrm{th}}=10^3$ 或者条件数 $\\kappa(\\mathbf{J}(\\nu))$ 超过阈值 $\\chi_{\\mathrm{th}}=10^8$，则预测训练失败。这些条件表明底层的优化问题是病态的，原因可能是严重的梯度不平衡或病态条件，这两者都是 PINNs 在多尺度问题中常见的失败模式。\n\n该实现将这些步骤封装在一个单独的程序中，该程序遍历所提供的粘度值列表，为每个值执行计算，并根据指定的输出格式格式化结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes sensitivity and predicts training failure for a PINN model of the\n    1D viscous Burgers equation for a range of viscosity values.\n    \"\"\"\n    \n    # Define problem constants and parameters from the problem statement.\n    THETA = np.array([1.0, -0.5, 0.75])\n    NU_VALUES = [1.0, 0.1, 0.01, 0.001, 0.0]\n    NX, NT = 64, 64\n    N = NX * NT\n    KAPPA_TH = 1e3\n    CHI_TH = 1e8\n    EPSILON = 1e-16\n\n    # Create the spatio-temporal grid.\n    x_coords = np.linspace(0, 1, NX)\n    t_coords = np.linspace(0, 1, NT)\n    X, T = np.meshgrid(x_coords, t_coords)\n\n    # --- Pre-compute basis functions and their symbolic derivatives on the grid ---\n    pi = np.pi\n    \n    # phi_1(x,t) = sin(pi*x) * exp(-t)\n    phi1 = np.sin(pi * X) * np.exp(-T)\n    phi1_t = -phi1\n    phi1_x = pi * np.cos(pi * X) * np.exp(-T)\n    phi1_xx = -pi**2 * phi1\n    \n    # phi_2(x,t) = sin(2*pi*x) * exp(-t)\n    phi2 = np.sin(2 * pi * X) * np.exp(-T)\n    phi2_t = -phi2\n    phi2_x = 2 * pi * np.cos(2 * pi * X) * np.exp(-T)\n    phi2_xx = - (2 * pi)**2 * phi2\n\n    # phi_3(x,t) = cos(pi*x) * exp(-2t)\n    phi3 = np.cos(pi * X) * np.exp(-2 * T)\n    phi3_t = -2 * phi3\n    phi3_x = -pi * np.sin(pi * X) * np.exp(-2 * T)\n    phi3_xx = -pi**2 * phi3\n\n    # Store basis functions and their derivatives in arrays for efficient computation.\n    # Shape: (3, NT, NX) where 3 is the number of basis functions.\n    PHI = np.array([phi1, phi2, phi3])\n    PHI_t = np.array([phi1_t, phi2_t, phi3_t])\n    PHI_x = np.array([phi1_x, phi2_x, phi3_x])\n    PHI_xx = np.array([phi1_xx, phi2_xx, phi3_xx])\n\n    # --- Main calculation loop over viscosity values ---\n    results_list = []\n    \n    for nu in NU_VALUES:\n        # Calculate the trial solution u and its derivatives using Einstein summation.\n        # This computes the sum over the first axis (m=1,2,3).\n        u = np.einsum('i,ijk->jk', THETA, PHI)\n        u_t = np.einsum('i,ijk->jk', THETA, PHI_t)\n        u_x = np.einsum('i,ijk->jk', THETA, PHI_x)\n        u_xx = np.einsum('i,ijk->jk', THETA, PHI_xx)\n        \n        # Calculate components of the PDE residual.\n        term_T = u_t\n        term_A = u * u_x\n        term_D = nu * u_xx\n        \n        # Calculate the pointwise PDE residual R = u_t + u*u_x - nu*u_xx.\n        R = term_T + term_A - term_D\n        \n        # Task 1: Compute root-mean-square residual, RMS(nu).\n        rms_val = np.sqrt(np.mean(R**2))\n        \n        # --- Compute gradients with respect to parameters theta ---\n        \n        # Gradient of the temporal term, grad_theta(T). Shape: (NT, NX, 3)\n        grad_T_theta = np.moveaxis(PHI_t, 0, -1)\n        \n        # Gradient of the advection term, grad_theta(A).\n        # grad_A_theta_m = phi_m * u_x + u * phi_x_m\n        grad_A_theta = np.zeros((NT, NX, 3))\n        for m in range(3):\n            grad_A_theta[:, :, m] = PHI[m, :, :] * u_x + u * PHI_x[m, :, :]\n            \n        # Gradient of the diffusion term, grad_theta(D).\n        grad_D_theta = nu * np.moveaxis(PHI_xx, 0, -1)\n        \n        # --- Compute components of the loss gradient: g_T, g_A, g_D ---\n        # g_X = (2/N) * sum_{i,j} (R * grad_X_theta)\n        R_reshaped = R[:, :, np.newaxis] # Reshape for broadcasting\n        \n        g_T = (2 / N) * np.sum(R_reshaped * grad_T_theta, axis=(0, 1))\n        g_A = (2 / N) * np.sum(R_reshaped * grad_A_theta, axis=(0, 1))\n        # g_D is defined with a negative sign: based on grad_theta(-D)\n        g_D = (2 / N) * np.sum(R_reshaped * (-grad_D_theta), axis=(0, 1))\n        \n        # Task 2: Compute norms of gradient components and the imbalance ratio rho(nu).\n        norm_g_A = np.linalg.norm(g_A)\n        norm_g_D = np.linalg.norm(g_D)\n        \n        rho_val = norm_g_A / max(norm_g_D, EPSILON)\n        \n        # Task 3: Compute the condition number of the residual Jacobian matrix J.\n        # J combines gradients from all terms: J = grad_T + grad_A - grad_D\n        grad_R_theta = grad_T_theta + grad_A_theta - grad_D_theta\n        J_matrix = grad_R_theta.reshape((N, 3))\n        cond_J = np.linalg.cond(J_matrix)\n        \n        # Task 4: Predict training failure based on the given thresholds.\n        failure_predicted = 1 if (rho_val > KAPPA_TH) or (cond_J > CHI_TH) else 0\n        \n        results_list.append([nu, rms_val, rho_val, failure_predicted])\n        \n    # --- Format the output as a single-line string ---\n    formatted_results = []\n    for res in results_list:\n        nu_str = f\"{res[0]:.6g}\"\n        rms_str = f\"{res[1]:.6g}\"\n        rho_str = f\"{res[2]:.6g}\"\n        fail_str = str(res[3])\n        formatted_results.append(f\"[{nu_str},{rms_str},{rho_str},{fail_str}]\")\n        \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "除了发现控制方程，物理信息方法在解决模型部分结构或参数未知的反问题（inverse problems）方面也表现出色。本实践练习 () 将演示如何仅利用来自外部区域的稀疏、含噪测量数据，来辨识流体中未知障碍物的几何形状。通过将数值偏微分方程求解器集成到基于搜索的优化框架中，您将在一个平衡数据保真度与障碍物尺寸稀疏性惩罚的损失函数下，通过最小化该函数来发现障碍物的形状。本实践展示了物理信息稀疏发现在一个具体的工程问题中的强大应用。",
            "id": "3352066",
            "problem": "考虑一个一维、定常、不可压缩流，该流沿着一个线段发生，由定义在闭区间 $[0,1]$ 上的标量速度场 $u(x)$ 表示。一个固体障碍物占据了一个未知的子区间 $\\Omega_s = [a,b] \\subset (0,1)$，我们用一个指示场 $\\chi(x)$ 来表示它，该指示场在 $\\Omega_s$ 内等于 $1$，在区域外等于 $0$。使用浸入边界法中常用的 Brinkman 惩罚法，在低雷诺数极限下，稳态动量平衡方程简化为\n$$\n-\\nu \\, \\frac{d^2 u}{dx^2} + \\alpha \\, \\chi(x) \\, u(x) = 0,\n$$\n并附有 Dirichlet 边界条件\n$$\nu(0) = U_0, \\quad u(1) = U_1,\n$$\n其中 $\\nu > 0$ 是给定的（无量纲）运动粘度，$\\alpha \\gg 1$ 是一个大的惩罚参数，用于在固体区域（$\\chi = 1$）内强制实现 $u \\approx 0$。该 Brinkman 惩罚模型源自 Navier-Stokes 方程与多孔介质项耦合后的定常 Stokes 极限，并作为一种物理信息约束。\n\n给定（未知）障碍物外部点上的外部流场数据，这些数据是通过对一个已知的真实障碍物 $[a^\\star, b^\\star]$ 求解上述边值问题而人工生成的。你的任务是通过最小化一个物理信息损失函数来发现指示场 $\\chi(x)$ 的形状（参数化为单个区间 $[a,b]$）。该损失函数将预测的流场值与数据进行比较，同时对固体区域强制施加稀疏性。具体来说，对于任何候选区间 $[a,b]$，你必须：\n- 在一个有 $N$ 个节点的均匀网格上，通过二阶中心有限差分格式对边值问题进行数值求解。对拉普拉斯算子使用标准的三点模板，并在端点处精确施加 Dirichlet 边界条件。\n- 使用线性插值在给定的数据点（所有数据点都位于真实障碍物的流体区域内）上评估预测的 $u(x)$。\n- 最小化目标函数\n$$\n\\mathcal{J}(a,b) = \\frac{1}{M}\\sum_{j=1}^{M} \\left(u(x_j; a,b) - \\tilde{u}_j\\right)^2 \\;+\\; \\lambda \\, (b - a),\n$$\n其中 $x_j$ 是数据点位置，$\\tilde{u}_j$ 是观测到的速度，$M$ 是数据点数量，$\\lambda > 0$ 是一个促进稀疏性的正则化权重，鼓励固体区域更小。这在指示场 $\\chi$ 上编码了一种稀疏模型发现的偏好。\n\n为保证数值稳定性并使问题自洽，不要对 $a$ 和 $b$ 进行基于梯度的优化。相反，应对候选中心点 $c \\in \\mathcal{C}$ 和半长度 $s \\in \\mathcal{S}$ 进行离散搜索，其中 $a=c-s$ 和 $b=c+s$，并且只接受满足 $0  a  b  1$ 的候选者。对于每个有效的候选者，求解离散化的物理方程并计算 $\\mathcal{J}(a,b)$；选择目标函数值最小的候选者。这个过程是一种物理信息识别，因为每个候选预测 $u(\\cdot; a,b)$ 都是通过求解控制方程得到的，因此偏微分方程的残差在构造上是受控的。\n\n基本原理和离散化要求：\n- 从带有 Brinkman 惩罚的定常 Stokes 极限开始，\n$$\n-\\nu \\, u''(x) + \\alpha \\, \\chi(x) \\, u(x) = 0,\\quad x\\in(0,1),\n$$\n其中 $u(0)=U_0$ 且 $u(1)=U_1$。\n- 使用 $N$ 个均匀间隔的网格点 $x_i = i h$ 进行离散化，其中 $h = 1/(N-1)$ 且 $i \\in \\{0,1,\\dots,N-1\\}$。对于内部索引 $i=1,\\dots,N-2$，使用\n$$\n-\\nu \\, \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + \\alpha \\, \\chi_i \\, u_i = 0,\n$$\n其中如果 $x_i \\in [a,b]$，则 $\\chi_i = 1$，否则 $\\chi_i=0$。通过将已知的边界值移到右侧来引入边界条件。\n\n每个测试用例的数据生成协议：\n- 使用 $N_{\\text{true}} = 257$ 生成数据，并使用 $N = 257$ 进行识别，以避免“反演犯罪”的简化，同时保持离散化方案的兼容性。\n- 使用 $\\nu = 0.01$，$\\alpha = 1000$，$U_0 = 1$，$U_1 = 0$。\n- 通过使用真实障碍物 $[a^\\star,b^\\star]$ 求解离散化问题，生成真实解 $u^\\star(x)$。\n- 在 $[0,1]$ 中选择 $M_{\\text{tot}} = 25$ 个等间距候选点，并只保留真实障碍物外部的点，即 $x_j \\in [0,a^\\star) \\cup (b^\\star,1]$，以获得 $M$ 个数据点。通过公式 $\\tilde{u}_j = u^\\star(x_j) + \\eta_j$ 形成观测数据，其中 $\\eta_j$ 是标准差为 $\\sigma = 0.001$ 的零均值噪声。\n- 使用线性插值在 $x_j$ 处评估模型预测。\n\n搜索空间和正则化：\n- 使用中心点 $c \\in \\mathcal{C} = \\{0.10, 0.12, 0.14, \\dots, 0.90\\}$ 和半长度 $s \\in \\mathcal{S} = \\{0.02, 0.03, 0.04, \\dots, 0.25\\}$。只考虑满足 $0  c - s  c + s  1$ 的候选者。\n- 使用 $\\lambda = 0.001$。\n\n测试套件：\n- 情况 1 (理想路径): $[a^\\star,b^\\star] = [0.40, 0.60]$。\n- 情况 2 (小障碍物): $[a^\\star,b^\\star] = [0.20, 0.25]$。\n- 情况 3 (靠近边界): $[a^\\star,b^\\star] = [0.80, 0.95]$。\n\n你的程序必须为每个测试用例实现上述数据生成和识别过程（使用固定的噪声随机种子以确保可复现性），并为每个用例返回识别出的区间端点 $[\\hat{a}, \\hat{b}]$，四舍五入到三位小数。所有量都是无量纲的，因此不需要物理单位。最终输出格式必须是单行文本，包含一个由逗号分隔的三个项目的列表，每个项目都是一个双元素浮点数列表，按给定顺序对应于每个用例的 $[\\hat{a}, \\hat{b}]$。例如，\n\"[ [0.400,0.600],[0.200,0.250],[0.800,0.950] ]\"\n是一个有效的输出格式。你的程序应生成单行输出，其中包含以此精确样式用方括号括起来的、以逗号分隔的结果列表，不得包含任何额外文本。",
            "solution": "该问题要求在一维、定常、粘性流中识别一个未知的固体障碍物。这是一个经典的反问题，我们将使用物理信息模型发现框架来解决。其核心思想是找到一个物理模型的参数——在本例中是障碍物的起点和终点 $[a,b]$——该参数能最好地解释一组稀疏且带噪声的观测数据，同时通过一个惩罚项来鼓励模型更简单或更“稀疏”。\n\n物理模型是一维 Brinkman 惩罚定常 Stokes 方程，这是模拟含浸入边界流动的成熟公式。控制方程是一个二阶常微分方程（ODE）：\n$$\n-\\nu \\, \\frac{d^2 u}{dx^2} + \\alpha \\, \\chi(x) \\, u(x) = 0, \\quad x \\in (0,1)\n$$\n服从 Dirichlet 边界条件 $u(0) = U_0$ 和 $u(1) = U_1$。这里，$u(x)$ 是标量速度场，$\\nu$ 是运动粘度，$\\alpha$ 是一个大的惩罚参数。函数 $\\chi(x)$ 是固体障碍物的指示函数，定义为如果 $x \\in [a,b]$ 则 $\\chi(x) = 1$，否则 $\\chi(x) = 0$。项 $\\alpha \\chi(x) u(x)$ 充当一个“流动阻力”项，它迫使速度 $u(x)$ 在障碍物内部接近于零，在 $\\alpha \\to \\infty$ 的极限情况下模拟了无滑移条件。\n\n为了在计算上解决这个问题，我们首先使用一个包含 $N$ 个点的均匀网格来离散化定义域 $[0,1]$，这些点为 $x_i = i h$，其中 $i \\in \\{0, 1, \\dots, N-1\\}$，网格间距为 $h = 1/(N-1)$。我们将 $u(x_i)$ 的数值近似表示为 $u_i$。二阶导数 $u''(x_i)$ 使用二阶中心有限差分模板进行近似：\n$$\n\\frac{d^2 u}{dx^2}\\bigg|_{x=x_i} \\approx \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2}\n$$\n将此近似代入每个内部网格点 $i \\in \\{1, 2, \\dots, N-2\\}$ 的控制常微分方程（ODE），得到一个线性代数方程组：\n$$\n-\\nu \\left( \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} \\right) + \\alpha \\, \\chi_i \\, u_i = 0\n$$\n其中如果 $x_i \\in [a,b]$，则 $\\chi_i = 1$，否则为 $0$。对一个通用内部节点 $i$ 重新整理各项，我们得到：\n$$\n-\\frac{\\nu}{h^2} u_{i-1} + \\left( \\frac{2\\nu}{h^2} + \\alpha \\chi_i \\right) u_i - \\frac{\\nu}{h^2} u_{i+1} = 0\n$$\n这个包含 $N-2$ 个方程的系统用于求解 $N-2$ 个未知内部速度 $\\{u_1, u_2, \\dots, u_{N-2}\\}$。边界值 $u_0 = U_0$ 和 $u_{N-1} = U_1$ 是已知的。我们通过修改第一个 ($i=1$) 和最后一个 ($i=N-2$) 内部节点的方程来引入这些边界值：\n对于 $i=1$：\n$$\n\\left( \\frac{2\\nu}{h^2} + \\alpha \\chi_1 \\right) u_1 - \\frac{\\nu}{h^2} u_2 = \\frac{\\nu}{h^2} u_0 = \\frac{\\nu}{h^2} U_0\n$$\n对于 $i=N-2$：\n$$\n-\\frac{\\nu}{h^2} u_{N-3} + \\left( \\frac{2\\nu}{h^2} + \\alpha \\chi_{N-2} \\right) u_{N-2} = \\frac{\\nu}{h^2} u_{N-1} = \\frac{\\nu}{h^2} U_1\n$$\n这产生了一个形如 $A\\mathbf{v} = \\mathbf{f}$ 的三对角线性系统，其中 $\\mathbf{v} = [u_1, u_2, \\dots, u_{N-2}]^T$ 是未知速度向量。矩阵 $A$ 是三对角且对角占优的，这保证了存在唯一解，并且可以使用专门的算法（例如 `scipy.linalg.solve_banded` 提供的算法）非常高效地求解。\n\n反问题在于找到能最好地解释一组位于 $\\{x_j\\}$ 位置的带噪声速度测量值 $\\{\\tilde{u}_j\\}$ 的障碍物参数 $[a,b]$。识别过程通过最小化一个物理信息目标函数 $\\mathcal{J}(a,b)$ 来完成：\n$$\n\\mathcal{J}(a,b) = \\frac{1}{M}\\sum_{j=1}^{M} \\left(u(x_j; a,b) - \\tilde{u}_j\\right)^2 \\;+\\; \\lambda \\, (b - a)\n$$\n第一项是数据保真项，即模型预测值 $u(x_j; a,b)$ 与观测数据 $\\tilde{u}_j$ 之间的均方误差。对于每个候选区间 $[a,b]$，其预测值 $u(x; a,b)$ 是通过求解离散化的边值问题（BVP）生成的，这确保了每个候选解都遵循底层物理规律。对于不在网格上的数据点，其 $u(x_j; a,b)$ 值是通过对数值解进行线性插值得到的。第二项 $\\lambda(b-a)$ 是一个正则化项，用于惩罚较大的障碍物。该项通过偏好较小的区间来促进“稀疏”解，这是模型发现中一种常见的技术，其灵感来源于 LASSO 回归等方法。\n\n根据指示，我们不使用基于梯度的优化，而是对一组离散的候选区间进行网格搜索。每个候选区间由其中心 $c$ 和半长度 $s$ 参数化，其中 $a=c-s$，$b=c+s$。搜索空间为 $\\mathcal{C} = \\{0.10, 0.12, \\dots, 0.90\\}$ 和 $\\mathcal{S} = \\{0.02, 0.03, \\dots, 0.25\\}$。我们遍历所有满足约束条件 $0  a  b  1$ 的有效对 $(c,s)$，为每一对计算 $\\mathcal{J}(a,b)$，并选择使目标函数最小化的对 $(\\hat{c}, \\hat{s})$。最终识别出的障碍物为 $[\\hat{a}, \\hat{b}] = [\\hat{c}-\\hat{s}, \\hat{c}+\\hat{s}]$。\n\n每个测试用例的数据生成都遵循一个固定的协议。首先，通过求解带有已知真实障碍物 $[a^\\star, b^\\star]$ 的边值问题（BVP），计算出“真实”速度场 $u^\\star(x)$。然后，从一个均匀网格中选择 $M$ 个观测点，排除真实障碍物内部的点。通过在这些点上评估 $u^\\star$ 并添加少量零均值高斯噪声来创建合成数据 $\\tilde{u}_j$。这个过程真实地模拟了一个实验场景，其中测量数据是稀疏、带噪声的，并且仅在可及的流体区域内可用。固定的随机种子确保了噪声的可复现性，从而保证了整个实验的可复现性。\n\n总体算法实现如下：\n1.  对于每个给定 $[a^\\star, b^\\star]$ 的测试用例：\n    a.  根据指定协议生成合成数据 $(\\{x_j\\}, \\{\\tilde{u}_j\\})$。\n    b.  将最小目标值初始化为无穷大。\n    c.  遍历所有候选中心点 $c \\in \\mathcal{C}$ 和半长度 $s \\in \\mathcal{S}$。\n    d.  对于每一对 $(c,s)$，形成候选区间 $[a,b]=[c-s, c+s]$ 并检查其是否有效（即 $0  a  b  1$）。\n    e.  如果有效，则使用此 $[a,b]$ 求解离散化的偏微分方程（PDE），得到预测速度场 $u(x; a,b)$。\n    f.  对解进行线性插值，以找到在数据点 $x_j$ 处的预测速度。\n    g.  计算目标函数 $\\mathcal{J}(a,b)$。\n    h.  如果计算出的 $\\mathcal{J}$ 小于当前最小值，则更新最小值并将 $[a,b]$ 存储为找到的最佳区间。\n2.  搜索完成后，记录该测试用例找到的最佳区间 $[\\hat{a}, \\hat{b}]$。\n3.  最终输出是所有测试用例的已识别区间的列表，并按要求格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the physics-informed model discovery problem for three test cases.\n    Identifies the location of an obstacle in a 1D flow by minimizing a\n    physics-informed objective function over a discrete search space.\n    \"\"\"\n\n    # Fixed parameters for all test cases\n    params = {\n        'N': 257,\n        'nu': 0.01,\n        'alpha': 1000.0,\n        'U0': 1.0,\n        'U1': 0.0,\n        'lambda_reg': 0.001,\n        'noise_std': 0.001,\n        'M_tot': 25,\n        'seed': 0,\n    }\n\n    # Test cases: each with a true obstacle [a_star, b_star]\n    test_cases = [\n        [0.40, 0.60],  # Case 1: Happy path\n        [0.20, 0.25],  # Case 2: Small obstacle\n        [0.80, 0.95],  # Case 3: Near boundary\n    ]\n    \n    # Search space for candidate obstacles\n    centers_c = np.arange(0.10, 0.90 + 1e-9, 0.02)\n    half_lengths_s = np.arange(0.02, 0.25 + 1e-9, 0.01)\n\n    results = []\n\n    for a_star, b_star in test_cases:\n        # Step 1: Data Generation\n        \n        # Grid setup\n        x_grid = np.linspace(0, 1, params['N'])\n        \n        # Generate true solution\n        u_true = solve_bvp(a_star, b_star, x_grid, params)\n        \n        # Generate observation points\n        x_obs_candidates = np.linspace(0, 1, params['M_tot'])\n        data_x = []\n        for x_val in x_obs_candidates:\n            if not (a_star = x_val = b_star):\n                data_x.append(x_val)\n        data_x = np.array(data_x)\n        \n        # Get true velocity at observation points via interpolation\n        u_obs_true = np.interp(data_x, x_grid, u_true)\n        \n        # Add noise\n        rng = np.random.default_rng(params['seed'])\n        noise = rng.normal(0, params['noise_std'], size=u_obs_true.shape)\n        data_u_tilde = u_obs_true + noise\n\n        # Step 2: Identification via Grid Search\n        \n        min_J = float('inf')\n        best_ab = None\n\n        for c in centers_c:\n            for s in half_lengths_s:\n                a_cand = c - s\n                b_cand = c + s\n\n                # Check validity of the candidate interval\n                if not (0  a_cand  b_cand  1):\n                    continue\n\n                # Solve BVP for the candidate interval\n                u_pred = solve_bvp(a_cand, b_cand, x_grid, params)\n                \n                # Interpolate to get predictions at data points\n                u_pred_at_data = np.interp(data_x, x_grid, u_pred)\n                \n                # Calculate objective function J\n                mse = np.mean((u_pred_at_data - data_u_tilde)**2)\n                regularization = params['lambda_reg'] * (b_cand - a_cand)\n                J = mse + regularization\n                \n                # Update best candidate\n                if J  min_J:\n                    min_J = J\n                    best_ab = [a_cand, b_cand]\n\n        # Round results to three decimal places\n        if best_ab:\n            final_a = round(best_ab[0], 3)\n            final_b = round(best_ab[1], 3)\n            results.append([final_a, final_b])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is already in the correct format.\n    # We must manually format it to avoid spaces after commas.\n    print(f\"[{','.join(str(item) for item in results)}]\")\n\n\ndef solve_bvp(a, b, x_grid, params):\n    \"\"\"\n    Solves the 1D Brinkman-penalized BVP using finite differences.\n    -nu * u_xx + alpha * chi * u = 0\n    u(0) = U0, u(1) = U1\n    \"\"\"\n    N = params['N']\n    nu = params['nu']\n    alpha = params['alpha']\n    U0 = params['U0']\n    U1 = params['U1']\n    \n    h = 1.0 / (N - 1)\n    \n    # Interior grid points (N-2 unknowns)\n    x_int = x_grid[1:-1]\n    \n    # Indicator function on the grid\n    chi = np.zeros(N)\n    chi[(x_grid >= a)  (x_grid = b)] = 1.0\n    chi_int = chi[1:-1]\n    \n    # Construct the tridiagonal matrix A for the system Av = f\n    # The system is for the N-2 interior points.\n    \n    # Main diagonal\n    main_diag = (2.0 * nu / h**2) + (alpha * chi_int)\n    \n    # Off-diagonals\n    off_diag_val = -nu / h**2\n    upper_diag = np.full(N - 2, off_diag_val)\n    lower_diag = np.full(N - 2, off_diag_val)\n\n    # Scipy's solve_banded expects the matrix in a specific format\n    # (number of bands, length of main diagonal)\n    # Row 0: upper diagonal (padded with a 0 at the start)\n    # Row 1: main diagonal\n    # Row 2: lower diagonal (padded with a 0 at the end)\n    A_banded = np.zeros((3, N - 2))\n    A_banded[0, 1:] = upper_diag[:-1]\n    A_banded[1, :] = main_diag\n    A_banded[2, :-1] = lower_diag[1:]\n    \n    # Construct the right-hand side vector f\n    f = np.zeros(N - 2)\n    f[0] = -off_diag_val * U0\n    f[-1] = -off_diag_val * U1\n\n    # Solve the linear system for interior points\n    u_int = solve_banded((1, 1), A_banded, f)\n    \n    # Combine with boundary conditions to form the full solution\n    u_full = np.concatenate(([U0], u_int, [U1]))\n    \n    return u_full\n\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}