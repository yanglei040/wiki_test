{
    "hands_on_practices": [
        {
            "introduction": "The way data is organized in memory can dramatically affect performance on an accelerator, where memory bandwidth is often the primary bottleneck. This exercise explores the fundamental concept of memory coalescing on a Graphics Processing Unit (GPU), comparing the Array of Structures (AoS) and Structure of Arrays (SoA) layouts. By calculating the number of memory transactions required for each layout , you will gain a concrete understanding of why SoA is often superior for the unit-stride access patterns common in scientific computing.",
            "id": "3287336",
            "problem": "Consider a three-dimensional compressible flow solver storing five primitive variables per cell, $(\\rho,u,v,w,p)$, in single precision (each variable is a $4$-byte floating-point number). You are implementing neighbor-independent per-cell reads on a Graphics Processing Unit (GPU) where each warp consists of $32$ threads. The memory system coalesces a warpâ€™s memory accesses for a single load instruction into the minimal number of $128$-byte aligned transactions, where a transaction covers one distinct $128$-byte segment. Assume the following scientifically standard and widely observed rules and setup:\n\n- The base address of any array involved in the access is aligned to a $128$-byte boundary.\n- Each thread in the warp reads the five variables of exactly one cell with a contiguous cell index mapping: thread $t$ reads cell index $i=t$ for $t \\in \\{0,1,\\dots,31\\}$.\n- Coalescing occurs per load instruction, not across different load instructions. Each scalar variable read corresponds to one load instruction and is coalesced independently.\n- In the Array of Structures (AoS) layout, the per-cell record is tightly packed as $(\\rho,u,v,w,p)$ with no padding, so the per-cell stride is $20$ bytes. In the Structure of Arrays (SoA) layout, the five variables are stored in five separate arrays, each with $4$-byte elements and per-element stride $4$ bytes.\n\nStarting from these fundamentals, derive and compute the total number of $128$-byte memory transactions issued per warp for the AoS layout versus the SoA layout when reading $(\\rho,u,v,w,p)$ for the $32$ consecutive cells. Express your final answer as a row matrix containing the two integer counts $(N_{\\mathrm{AoS}}, N_{\\mathrm{SoA}})$, with no units required. No rounding is needed.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Number of primitive variables per cell: $5$ $(\\rho, u, v, w, p)$.\n- Data type: Single precision, $4$ bytes per variable.\n- GPU warp size: $32$ threads.\n- Memory transaction size: $128$ bytes.\n- Memory transaction basis: Minimal number of distinct $128$-byte aligned segments per single load instruction.\n- Array alignment: Base addresses are aligned to a $128$-byte boundary.\n- Thread-to-cell mapping: Thread $t$ reads cell index $i=t$, for $t \\in \\{0, 1, \\dots, 31\\}$.\n- Load instruction scope: Each scalar variable read $(\\rho, u, v, w, p)$ corresponds to a separate, independent load instruction.\n- Array of Structures (AoS) layout: Tightly packed record $(\\rho, u, v, w, p)$ with a stride of $5 \\times 4 = 20$ bytes between the start of consecutive cells.\n- Structure of Arrays (SoA) layout: $5$ separate arrays, one for each variable, with an element size and stride of $4$ bytes.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing a standard memory access optimization scenario in high-performance computing on GPUs. The concepts of AoS/SoA data layouts, warp-based execution, and memory coalescing are fundamental to GPU architecture (e.g., CUDA or OpenCL programming models). The provided parameters (warp size of $32$, single-precision floats of $4$ bytes, transaction size of $128$ bytes) are standard and realistic for modern hardware. The problem is well-posed, providing all necessary information (data sizes, strides, alignment, thread mapping) to uniquely calculate the number of memory transactions. The language is objective and precise. Therefore, the problem is deemed valid.\n\n**Step 3: Proceed to Solution**\n\nThe goal is to calculate the total number of $128$-byte memory transactions for a full warp of $32$ threads reading $5$ variables each, for both AoS and SoA layouts. A memory transaction is defined as a fetch of a single, unique $128$-byte aligned memory segment. The total number of transactions for a given load instruction is the number of unique $128$-byte segments that contain the data requested by all $32$ threads in the warp.\n\nLet the base address of any array be $A_{base}$. Since all base addresses are $128$-byte aligned, we can analyze the memory access patterns by considering the offset from this base address, which we can set to $0$ without loss of generality. The size of each variable is $S_{var} = 4$ bytes. The number of threads in a warp is $N_{threads} = 32$. The memory transaction size is $S_{trans} = 128$ bytes.\n\n**Analysis of the Array of Structures (AoS) Layout**\n\nIn the AoS layout, the data for a single cell is stored contiguously. The size of a single cell's data record is $5 \\text{ variables} \\times 4 \\frac{\\text{bytes}}{\\text{variable}} = 20$ bytes. This is the stride, $L_{AoS}$, between the start of consecutive cell records.\n\nThe problem states that reading each of the $5$ scalar variables corresponds to a separate load instruction. We must analyze each of these $5$ loads.\n\n1.  **Load for variable $\\rho$**:\n    Thread $t$ (where $t \\in \\{0, 1, \\dots, 31\\}$) reads the $\\rho$ variable from cell $t$. The byte address for this variable is given by $A_{\\rho}(t) = A_{base} + t \\times L_{AoS} + \\text{offset}(\\rho)$. The offset for $\\rho$ within the structure is $0$.\n    The addresses accessed by the warp are: $0 \\times 20, 1 \\times 20, 2 \\times 20, \\dots, 31 \\times 20$.\n    This simplifies to addresses $0, 20, 40, \\dots, 620$.\n    Each thread reads $4$ bytes. So, thread $t$ accesses the byte range $[t \\times 20, t \\times 20 + 3]$.\n    The lowest address accessed by the warp is $0$ (by thread $0$).\n    The highest address accessed by the warp is $620 + 3 = 623$ (by thread $31$).\n    The total span of memory touched is from byte $0$ to byte $623$.\n    To find the number of $128$-byte transactions, we determine how many unique $128$-byte segments this range intersects. A segment $k$ covers bytes $[k \\times 128, (k+1) \\times 128 - 1]$.\n    The first segment accessed is for byte $0$: segment $\\lfloor \\frac{0}{128} \\rfloor = 0$.\n    The last segment accessed is for byte $623$: segment $\\lfloor \\frac{623}{128} \\rfloor = \\lfloor 4.867... \\rfloor = 4$.\n    The unique segments accessed are segments $0, 1, 2, 3, 4$. This is a total of $5$ distinct segments.\n    Thus, the load for $\\rho$ requires $5$ transactions.\n\n2.  **Load for variable $u$**:\n    The offset for $u$ within the structure is $4$ bytes. The address for thread $t$ is $A_{u}(t) = t \\times 20 + 4$.\n    The addresses accessed by the warp are $4, 24, 44, \\dots, 624$.\n    The lowest address is $4$. The highest address is $624 + 3 = 627$.\n    The first segment is $\\lfloor \\frac{4}{128} \\rfloor = 0$.\n    The last segment is $\\lfloor \\frac{627}{128} \\rfloor = \\lfloor 4.9... \\rfloor = 4$.\n    Again, this spans segments $0, 1, 2, 3, 4$, resulting in $5$ transactions.\n\nThe same logic applies to the remaining variables $v$, $w$, and $p$, which have offsets of $8$, $12$, and $16$ bytes respectively. In each case, the memory addresses accessed by the warp will span a range that falls into $5$ distinct $128$-byte segments.\n- For $v$: addresses $8, ..., 628$. Range $[8, 631]$. Segments $\\lfloor 8/128 \\rfloor=0$ to $\\lfloor 631/128 \\rfloor=4$. $5$ transactions.\n- For $w$: addresses $12, ..., 632$. Range $[12, 635]$. Segments $\\lfloor 12/128 \\rfloor=0$ to $\\lfloor 635/128 \\rfloor=4$. $5$ transactions.\n- For $p$: addresses $16, ..., 636$. Range $[16, 639]$. Segments $\\lfloor 16/128 \\rfloor=0$ to $\\lfloor 639/128 \\rfloor=4$. $5$ transactions.\n\nThe total number of transactions for the AoS layout, $N_{\\mathrm{AoS}}$, is the sum of transactions for each of the $5$ loads:\n$$N_{\\mathrm{AoS}} = 5 \\text{ loads} \\times 5 \\frac{\\text{transactions}}{\\text{load}} = 25$$\n\n**Analysis of the Structure of Arrays (SoA) Layout**\n\nIn the SoA layout, each of the $5$ variables is stored in a separate array. The elements in each array are of size $4$ bytes and stored contiguously. The stride, $L_{SoA}$, between consecutive elements in any given array is $4$ bytes.\n\n1.  **Load for $\\rho$ array**:\n    Thread $t$ reads the element at index $t$ from the $\\rho$ array. The address is $A_{\\rho}(t) = A_{base, \\rho} + t \\times L_{SoA}$.\n    The addresses accessed by the warp are: $0 \\times 4, 1 \\times 4, 2 \\times 4, \\dots, 31 \\times 4$.\n    This simplifies to addresses $0, 4, 8, \\dots, 124$.\n    The total amount of data read is $32 \\text{ threads} \\times 4 \\frac{\\text{bytes}}{\\text{thread}} = 128$ bytes.\n    The lowest address is $0$. The highest address is $124+3 = 127$.\n    The entire range of memory accessed is $[0, 127]$.\n    Since the base address is $128$-byte aligned, this contiguous $128$-byte block of data fits perfectly into a single $128$-byte memory segment (segment $0$).\n    Therefore, the load for the entire warp is perfectly coalesced into a single transaction.\n\nThis ideal coalescing applies to each of the $5$ separate arrays. The read for the $u$ array will access a contiguous $128$-byte block, the read for the $v$ array will do the same, and so on. Each of these reads will require only $1$ transaction.\n\nThe total number of transactions for the SoA layout, $N_{\\mathrm{SoA}}$, is the sum of transactions for each of the $5$ array reads:\n$$N_{\\mathrm{SoA}} = 5 \\text{ loads} \\times 1 \\frac{\\text{transaction}}{\\text{load}} = 5$$\n\nThe resulting transaction counts are $N_{\\mathrm{AoS}} = 25$ and $N_{\\mathrm{SoA}} = 5$. This demonstrates the significant performance advantage of the SoA layout for this type of unit-stride, warp-synchronous access pattern on a GPU.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n25 & 5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving beyond basic memory access patterns, designing high-performance kernels requires intelligently managing the GPU's memory hierarchy to maximize data reuse. This practice challenges you to devise an optimal blocking, or tiling, strategy for a 3D 7-point stencil computation, a pattern central to many CFD solvers . You will learn to maximize computational efficiency by leveraging fast on-chip shared memory while carefully balancing multiple hardware constraints, such as threads per block and available memory size, a critical skill in accelerator-aware algorithm design.",
            "id": "3287388",
            "problem": "A Graphics Processing Unit (GPU) with Streaming Multiprocessor (SM) organizations is used to accelerate a three-dimensional structured-grid Computational Fluid Dynamics (CFD) kernel that updates a scalar field $u(i,j,k)$ using a $7$-point stencil (center plus six axis-aligned nearest neighbors). The kernel maps a tile of interior cells of size $T_{x} \\times T_{y} \\times T_{z}$ to a thread block, with one thread per interior cell. To exploit on-chip data reuse, each block cooperatively loads into shared memory a tile including a single-cell halo ($h=1$) on all faces, so the shared-memory footprint per block is the product of the padded extents times the element size.\n\nThe hardware provides $64\\,\\mathrm{KB}$ of shared memory per SM, and register availability limits occupancy to $50\\%$. To avoid shared memory from further reducing occupancy below $50\\%$, the shared-memory allocation per block must not exceed half the SM shared memory, i.e., no more than $32\\,\\mathrm{KB}$ per block. The scalar field is stored in double precision, so each element occupies $8$ bytes. The architecture allows at most $1024$ threads per block. Assume that the block performs a single stencil update per tile and that only one field is staged in shared memory (no multi-field coupling).\n\nDefine the data-reuse factor as $R=\\dfrac{T_{x}T_{y}T_{z}}{(T_{x}+2)(T_{y}+2)(T_{z}+2)}$, representing the fraction of loaded shared-memory elements that are updated. Using the fundamental definitions of shared-memory footprint, occupancy, and the stencil halo requirement, and assuming a symmetric tile to minimize halo overhead, determine the largest integer symmetric interior tile edge length $T$ for which $T_{x}=T_{y}=T_{z}=T$ that both (i) maximizes $R$ under the constraints and (ii) does not reduce SM occupancy below $50\\%$ due to shared memory. Report your answer as the triplet $(T_{x},T_{y},T_{z})$. No rounding is required, and no units are to be included in the final answer.",
            "solution": "The problem requires us to determine the largest integer symmetric interior tile edge length, denoted by $T$, where $T_{x}=T_{y}=T_{z}=T$, for a specific Computational Fluid Dynamics (CFD) kernel. This value of $T$ must satisfy several hardware constraints while maximizing the data-reuse factor, $R$.\n\nFirst, let us formalize the constraints provided in the problem statement.\n\n1.  **Thread Block Size Constraint**: A tile of interior cells of size $T_{x} \\times T_{y} \\times T_{z}$ is mapped to a thread block, with one thread per interior cell. The maximum number of threads allowed per block is $1024$. For a symmetric tile where $T_{x} = T_{y} = T_{z} = T$, the total number of threads is $T \\times T \\times T = T^3$. Thus, we have the constraint:\n    $$T^3 \\leq 1024$$\n\n2.  **Shared Memory Constraint**: The kernel loads a tile of data into shared memory, including a single-cell halo ($h=1$) on all faces. The dimensions of the data loaded into shared memory are therefore $(T_{x}+2h) \\times (T_{y}+2h) \\times (T_{z}+2h)$. With $h=1$ and the symmetric tile assumption, this becomes $(T+2) \\times (T+2) \\times (T+2) = (T+2)^3$ elements. The scalar field is stored in double precision, meaning each element occupies $8$ bytes. The problem states that to maintain at least $50\\%$ occupancy, the shared-memory allocation per block must not exceed half of the total SM shared memory. The total shared memory is $64\\,\\mathrm{KB}$, so the limit per block is $32\\,\\mathrm{KB}$. We must convert kilobytes to bytes, where $1\\,\\mathrm{KB} = 1024$ bytes. The limit is $32 \\times 1024 = 32768$ bytes. This leads to the second constraint:\n    $$(T+2)^3 \\times 8 \\leq 32768$$\n\nNext, we must consider the objective function to be maximized, which is the data-reuse factor $R$. It is defined as the ratio of the number of interior cells (updated elements) to the total number of cells loaded into shared memory (including the halo). For a symmetric tile:\n$$R = \\frac{T_{x}T_{y}T_{z}}{(T_{x}+2)(T_{y}+2)(T_{z}+2)} = \\frac{T^3}{(T+2)^3} = \\left(\\frac{T}{T+2}\\right)^3$$\n\nTo maximize $R$, we need to maximize the base of the exponent, the function $f(T) = \\frac{T}{T+2}$, for positive integer values of $T$. We examine the derivative of $f(T)$ with respect to $T$:\n$$f'(T) = \\frac{d}{dT}\\left(\\frac{T}{T+2}\\right) = \\frac{1 \\cdot (T+2) - T \\cdot 1}{(T+2)^2} = \\frac{2}{(T+2)^2}$$\nSince $T$ represents a tile dimension, it must be a positive integer ($T \\geq 1$). Therefore, $(T+2)^2 > 0$, and $f'(T) > 0$. This shows that $f(T)$ is a strictly increasing function for all $T > 0$. Consequently, $R = (f(T))^3$ is also a strictly increasing function of $T$. To maximize $R$, we must find the largest integer value of $T$ that satisfies both hardware constraints.\n\nLet us now solve the inequalities for $T$.\n\nFrom the thread block size constraint:\n$$T^3 \\leq 1024$$\n$$T \\leq \\sqrt[3]{1024}$$\nWe know that $10^3 = 1000$, so $\\sqrt[3]{1024}$ is slightly greater than $10$. Specifically, $\\sqrt[3]{1024} \\approx 10.079$. Since $T$ must be an integer, this constraint implies:\n$$T \\leq 10$$\n\nFrom the shared memory constraint:\n$$8(T+2)^3 \\leq 32768$$\n$$(T+2)^3 \\leq \\frac{32768}{8}$$\n$$(T+2)^3 \\leq 4096$$\nTaking the cube root of both sides:\n$$T+2 \\leq \\sqrt[3]{4096}$$\nWe can recognize that $4096 = 2^{12}$, so $\\sqrt[3]{4096} = (2^{12})^{1/3} = 2^{12/3} = 2^4 = 16$.\n$$T+2 \\leq 16$$\n$$T \\leq 14$$\n\nTo satisfy both constraints simultaneously, $T$ must be less than or equal to the minimum of the two upper bounds:\n$$T \\leq \\min(10, 14)$$\n$$T \\leq 10$$\n\nThe largest integer value for $T$ that satisfies all conditions is $T=10$. This value will maximize the data-reuse factor $R$ while adhering to the architectural limits on threads per block and shared memory per block.\n\nLet's verify the solution with $T = 10$:\n-   Number of threads: $10^3 = 1000$, which is $\\leq 1024$. The constraint is satisfied.\n-   Shared memory usage: $8 \\times (10+2)^3 = 8 \\times 12^3 = 8 \\times 1728 = 13824$ bytes. This is $\\leq 32768$ bytes. The constraint is satisfied.\n\nIf we were to try $T = 11$, the number of threads would be $11^3 = 1331$, which violates the $1024$ thread limit. Thus, $T=10$ is indeed the maximum possible integer value.\n\nThe problem asks for the answer as the triplet $(T_{x}, T_{y}, T_{z})$. Since $T_x = T_y = T_z = T$, the result is $(10, 10, 10)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 10 & 10 & 10 \\end{pmatrix}}$$"
        },
        {
            "introduction": "An efficient kernel is only one piece of the puzzle; overall application performance depends on the entire data pipeline, including communication with the host system. This problem uses a high-level performance model to analyze the bottlenecks in a typical CFD workflow, comparing the bandwidth of on-device High Bandwidth Memory (HBM) with the host-to-device PCIe interconnect . By estimating the total time for scenarios with and without host-device data transfers, you will learn to perform back-of-the-envelope calculations to identify the dominant performance limiter, which is essential for guiding optimization efforts at a system level.",
            "id": "3287394",
            "problem": "A computational fluid dynamics kernel updates a structured field with $N=10^{9}$ cell-centered updates per time step. Each update performs $F=60$ floating-point operations and streams $B=48$ bytes from and to device global memory. The kernel runs on a General-Purpose Graphics Processing Unit (GPU) that has High Bandwidth Memory (HBM) with a sustainable bandwidth of $900$ gigabytes per second and is connected to the host via Peripheral Component Interconnect Express (PCIe) Generation $4$ $x16$ with a sustainable bandwidth of $32$ gigabytes per second. Assume $1$ gigabyte equals $10^{9}$ bytes. Consider two scenarios: (i) the field remains device-resident and no host-device transfers occur during the time step, and (ii) at each time step the full field of size $8$ gigabytes must be transferred host-to-device before the kernel and device-to-host after the kernel, with no overlap between computation and transfers due to a strict read-after-write dependency. Using fundamental definitions of bandwidth, arithmetic intensity, and work, derive expressions from first principles to estimate the time to complete the time step in each scenario and determine the dominant bottleneck link under these conditions. Report your final answer as a row matrix whose first entry is the device-resident kernel time in seconds, whose second entry is the time in seconds including both host-to-device and device-to-host transfers, and whose third entry is a code for the dominant bottleneck in the transfer-inclusive scenario where $1$ denotes HBM and $2$ denotes PCIe. Round all reported times to three significant figures and express the time in seconds.",
            "solution": "The problem requires an estimation of execution times for a computational fluid dynamics (CFD) kernel under two different data management scenarios, and an identification of the performance bottleneck. The solution will be derived from first principles of performance modeling, primarily focusing on data movement and bandwidth limitations.\n\nFirst, we establish the fundamental definitions. The time $T$ required to move a volume of data $D$ over a link with a sustainable bandwidth $BW$ is given by $T = \\frac{D}{BW}$. The total work $W$ is the total number of floating-point operations. The arithmetic intensity $AI$ of a computation is the ratio of work to data movement, $AI = \\frac{W}{D}$.\n\nThe problem specifies the following given quantities:\n- Number of cell updates per time step: $N = 10^9$.\n- Floating-point operations per update: $F = 60$.\n- Bytes streamed per update from/to device global memory: $B = 48$.\n- GPU High Bandwidth Memory (HBM) sustainable bandwidth: $BW_{HBM} = 900 \\frac{\\text{GB}}{\\text{s}} = 900 \\times 10^9 \\frac{\\text{bytes}}{\\text{s}}$.\n- Peripheral Component Interconnect Express (PCIe) sustainable bandwidth: $BW_{PCIe} = 32 \\frac{\\text{GB}}{\\text{s}} = 32 \\times 10^9 \\frac{\\text{bytes}}{\\text{s}}$.\n- Full field size for transfer: $S_{field} = 8 \\text{ GB} = 8 \\times 10^9 \\text{ bytes}$.\n\nLet us first analyze the computational kernel itself.\nThe total work performed by the kernel is the product of the number of updates and the operations per update:\n$$W = N \\times F = 10^9 \\times 60 = 60 \\times 10^9 \\text{ FLOPs}$$\nThe total data volume moved between the GPU's processing cores and its HBM during the kernel execution is:\n$$D_{HBM} = N \\times B = 10^9 \\times 48 = 48 \\times 10^9 \\text{ bytes} = 48 \\text{ GB}$$\nThe kernel's execution time, $T_{kernel}$, is determined by the maximum of the time required for computation and the time required for data movement from HBM. This is the principle of the roofline model. The time to complete the floating-point operations would be $T_{compute} = \\frac{W}{P_{peak}}$, where $P_{peak}$ is the peak performance of the GPU in FLOPs/s. The time for data movement is $T_{memory} = \\frac{D_{HBM}}{BW_{HBM}}$.\nThe arithmetic intensity of the kernel is:\n$$AI_{kernel} = \\frac{W}{D_{HBM}} = \\frac{60 \\times 10^9 \\text{ FLOPs}}{48 \\times 10^9 \\text{ bytes}} = 1.25 \\frac{\\text{FLOP}}{\\text{byte}}$$\nThis arithmetic intensity is relatively low. For modern GPUs with HBM, the machine balance (the ratio of peak FLOPs/s to HBM bandwidth) is typically higher than this value. Therefore, it is a sound assumption that the kernel is memory-bound, meaning its performance is limited by the HBM bandwidth, not the computational speed. The problem's structure, providing bandwidth data but not peak performance, supports this assumption. Thus, we estimate the kernel execution time as:\n$$T_{kernel} = \\frac{D_{HBM}}{BW_{HBM}} = \\frac{48 \\times 10^9 \\text{ bytes}}{900 \\times 10^9 \\frac{\\text{bytes}}{\\text{s}}} = \\frac{48}{900} \\text{ s} \\approx 0.05333... \\text{ s}$$\n\nNow we can analyze the two scenarios.\n\n**Scenario (i): Device-Resident Field**\nIn this scenario, the field data remains on the GPU, and no data is transferred over the PCIe bus during the time step. The total time for the step, $T_{(i)}$, is simply the kernel execution time.\n$$T_{(i)} = T_{kernel} = \\frac{48}{900} \\text{ s} \\approx 0.05333... \\text{ s}$$\nRounding to three significant figures, we get $T_{(i)} \\approx 0.0533 \\text{ s}$.\n\n**Scenario (ii): Host-Device Transfers Included**\nIn this scenario, the total time for the step, $T_{(ii)}$, is the sum of three sequential phases as there is no overlap: the host-to-device transfer, the kernel execution, and the device-to-host transfer.\n$$T_{(ii)} = T_{H2D} + T_{kernel} + T_{D2H}$$\nThe data transferred in each direction is the full field size, $S_{field} = 8 \\text{ GB}$. These transfers occur over the PCIe link.\nThe time for the host-to-device transfer is:\n$$T_{H2D} = \\frac{S_{field}}{BW_{PCIe}} = \\frac{8 \\times 10^9 \\text{ bytes}}{32 \\times 10^9 \\frac{\\text{bytes}}{\\text{s}}} = \\frac{8}{32} \\text{ s} = 0.25 \\text{ s}$$\nAssuming symmetric link performance, the time for the device-to-host transfer is the same:\n$$T_{D2H} = \\frac{S_{field}}{BW_{PCIe}} = 0.25 \\text{ s}$$\nThe total transfer time is $T_{transfer} = T_{H2D} + T_{D2H} = 0.25 \\text{ s} + 0.25 \\text{ s} = 0.5 \\text{ s}$.\nThe total time for scenario (ii) is the sum of the transfer time and the kernel execution time:\n$$T_{(ii)} = T_{transfer} + T_{kernel} = 0.5 \\text{ s} + \\frac{48}{900} \\text{ s} \\approx 0.5 \\text{ s} + 0.05333... \\text{ s} = 0.55333... \\text{ s}$$\nRounding to three significant figures, $T_{(ii)} \\approx 0.553 \\text{ s}$.\n\n**Dominant Bottleneck for Scenario (ii)**\nThe dominant bottleneck is the part of the process that consumes the most time. We compare the time spent on operations limited by the PCIe bus versus those limited by the HBM.\n- Time limited by HBM bandwidth (kernel execution): $T_{kernel} \\approx 0.0533 \\text{ s}$.\n- Time limited by PCIe bandwidth (data transfers): $T_{transfer} = 0.5 \\text{ s}$.\nSince $T_{transfer} \\gg T_{kernel}$, the overall process is overwhelmingly dominated by the time spent transferring data over the PCIe bus. Therefore, the dominant bottleneck is the PCIe link, which corresponds to the code $2$.\n\nThe final results are:\n- Time for scenario (i): $T_{(i)} \\approx 0.0533 \\text{ s}$.\n- Time for scenario (ii): $T_{(ii)} \\approx 0.553 \\text{ s}$.\n- Bottleneck code for scenario (ii): $2$ (PCIe).",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.0533 & 0.553 & 2 \\end{pmatrix}}$$"
        }
    ]
}