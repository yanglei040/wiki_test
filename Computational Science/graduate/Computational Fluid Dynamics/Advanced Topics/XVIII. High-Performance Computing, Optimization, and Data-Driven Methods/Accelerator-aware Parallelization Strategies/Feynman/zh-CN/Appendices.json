{
    "hands_on_practices": [
        {
            "introduction": "在GPU编程中，高效的数据访问是实现高性能的基石。本练习探讨了两种基本数据布局——结构数组(AoS)和数组结构(SoA)——如何影响内存合并，这是现代加速器上一项关键的性能优化机制。通过计算一个典型计算流体力学(CFD)场景中的内存事务数量，你将亲身体验到数据布局选择对性能的巨大影响。",
            "id": "3287336",
            "problem": "考虑一个三维可压缩流求解器，每个单元存储五个原始变量 $(\\rho,u,v,w,p)$，采用单精度（每个变量是一个 $4$ 字节浮点数）。您正在图形处理器（GPU）上实现独立于邻居的每单元读取操作，其中每个线程束（warp）由 $32$ 个线程组成。内存系统会将一个线程束对于单条加载指令的内存访问合并为最少数量的 $128$ 字节对齐的事务，其中一个事务覆盖一个不同的 $128$ 字节段。假设存在以下科学上标准且广泛遵循的规则和设置：\n\n- 任何参与访问的数组的基地址都与一个 $128$ 字节的边界对齐。\n- 线程束中的每个线程通过连续的单元索引映射读取恰好一个单元的五个变量：线程 $t$ 读取单元索引 $i=t$，其中 $t \\in \\{0,1,\\dots,31\\}$。\n- 合并是按加载指令发生的，而不是跨不同的加载指令。每个标量变量的读取都对应一条加载指令，并被独立地合并。\n- 在结构体数组（AoS）布局中，每个单元的记录被紧密打包为 $(\\rho,u,v,w,p)$，没有填充，因此每个单元的步长为 $20$ 字节。在数组结构（SoA）布局中，这五个变量存储在五个独立的数组中，每个数组包含 $4$ 字节的元素，每个元素的步长为 $4$ 字节。\n\n从这些基本原理出发，推导并计算在读取 $32$ 个连续单元的 $(\\rho,u,v,w,p)$ 时，AoS 布局与 SoA 布局下每个线程束发出的 $128$ 字节内存事务的总数。将最终答案表示为一个包含两个整数计数 $(N_{\\mathrm{AoS}}, N_{\\mathrm{SoA}})$ 的行矩阵，无需单位。无需四舍五入。",
            "solution": "首先对问题进行验证，以确保其具有科学依据、良构且客观。\n\n**步骤 1：提取已知条件**\n- 每个单元的原始变量数量：$5$ 个 $(\\rho, u, v, w, p)$。\n- 数据类型：单精度，每个变量 $4$ 字节。\n- GPU 线程束大小：$32$ 个线程。\n- 内存事务大小：$128$ 字节。\n- 内存事务基础：单条加载指令所需的不同 $128$ 字节对齐段的最小数量。\n- 数组对齐：基地址与 $128$ 字节边界对齐。\n- 线程到单元的映射：线程 $t$ 读取单元索引 $i=t$，其中 $t \\in \\{0, 1, \\dots, 31\\}$。\n- 加载指令范围：每个标量变量 $(\\rho, u, v, w, p)$ 的读取对应一条独立的加载指令。\n- 结构体数组 (AoS) 布局：紧密打包的记录 $(\\rho, u, v, w, p)$，连续单元起始地址之间的步长为 $5 \\times 4 = 20$ 字节。\n- 数组结构 (SoA) 布局：$5$ 个独立的数组，每个变量一个，元素大小和步长均为 $4$ 字节。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题具有科学依据，描述了 GPU 高性能计算中的一个标准内存访问优化场景。AoS/SoA 数据布局、基于线程束的执行以及内存合并等概念是 GPU 架构（例如，CUDA 或 OpenCL 编程模型）的基础。所提供的参数（$32$ 的线程束大小、$4$ 字节的单精度浮点数、$128$ 字节的事务大小）对于现代硬件是标准且现实的。该问题是良构的，提供了唯一计算内存事务数量所需的所有信息（数据大小、步长、对齐、线程映射）。语言客观而精确。因此，该问题被认定为有效。\n\n**步骤 3：开始求解**\n\n目标是计算一个完整的、包含 $32$ 个线程的线程束在读取 $5$ 个变量时，对于 AoS 和 SoA 两种布局，所需的 $128$ 字节内存事务的总数。一个内存事务定义为对单个、唯一的 $128$ 字节对齐内存段的一次读取。对于给定的加载指令，其总事务数是包含该线程束中所有 $32$ 个线程所请求数据的不同 $128$ 字节段的数量。\n\n设任何数组的基地址为 $A_{base}$。由于所有基地址都是 $128$ 字节对齐的，我们可以通过考虑相对于此基地址的偏移量来分析内存访问模式，为不失一般性，可将此基地址设为 $0$。每个变量的大小为 $S_{var} = 4$ 字节。一个线程束中的线程数为 $N_{threads} = 32$。内存事务大小为 $S_{trans} = 128$ 字节。\n\n**结构体数组（AoS）布局分析**\n\n在 AoS 布局中，单个单元的数据是连续存储的。单个单元数据记录的大小为 $5 \\text{ variables} \\times 4 \\frac{\\text{bytes}}{\\text{variable}} = 20$ 字节。这就是连续单元记录起始地址之间的步长 $L_{AoS}$。\n\n问题陈述，读取 $5$ 个标量变量中的每一个都对应于一条单独的加载指令。我们必须分析这 $5$ 次加载中的每一次。\n\n1.  **加载变量 $\\rho$**：\n    线程 $t$（其中 $t \\in \\{0, 1, \\dots, 31\\}$）从单元 $t$ 中读取变量 $\\rho$。该变量的字节地址由 $A_{\\rho}(t) = A_{base} + t \\times L_{AoS} + \\text{offset}(\\rho)$ 给出。结构体中 $\\rho$ 的偏移量为 $0$。\n    线程束访问的地址是：$0 \\times 20, 1 \\times 20, 2 \\times 20, \\dots, 31 \\times 20$。\n    这可以简化为地址 $0, 20, 40, \\dots, 620$。\n    每个线程读取 $4$ 字节。因此，线程 $t$ 访问的字节范围是 $[t \\times 20, t \\times 20 + 3]$。\n    线程束访问的最低地址是 $0$（由线程 $0$ 访问）。\n    线程束访问的最高地址是 $620 + 3 = 623$（由线程 $31$ 访问）。\n    触及的内存总跨度是从字节 $0$ 到字节 $623$。\n    为了求出 $128$ 字节事务的数量，我们确定这个范围与多少个不同的 $128$ 字节段相交。段 $k$ 覆盖的字节范围是 $[k \\times 128, (k+1) \\times 128 - 1]$。\n    访问的第一个段是针对字节 $0$ 的：段 $\\lfloor \\frac{0}{128} \\rfloor = 0$。\n    访问的最后一个段是针对字节 $623$ 的：段 $\\lfloor \\frac{623}{128} \\rfloor = \\lfloor 4.867... \\rfloor = 4$。\n    访问的独立段是段 $0, 1, 2, 3, 4$。这总共是 $5$ 个不同的段。\n    因此，加载 $\\rho$ 需要 $5$ 个事务。\n\n2.  **加载变量 $u$**：\n    结构体中 $u$ 的偏移量为 $4$ 字节。线程 $t$ 的地址是 $A_{u}(t) = t \\times 20 + 4$。\n    线程束访问的地址是 $4, 24, 44, \\dots, 624$。\n    最低地址是 $4$。最高地址是 $624 + 3 = 627$。\n    第一个段是 $\\lfloor \\frac{4}{128} \\rfloor = 0$。\n    最后一个段是 $\\lfloor \\frac{627}{128} \\rfloor = \\lfloor 4.9... \\rfloor = 4$。\n    同样，这跨越了段 $0, 1, 2, 3, 4$，导致 $5$ 个事务。\n\n相同的逻辑适用于剩余的变量 $v$、$w$ 和 $p$，它们的偏移量分别为 $8$、$12$ 和 $16$ 字节。在每种情况下，线程束访问的内存地址将跨越一个落入 $5$ 个不同 $128$ 字节段的范围。\n- 对于 $v$：地址 $8, ..., 628$。范围 $[8, 631]$。段 $\\lfloor 8/128 \\rfloor=0$ 到 $\\lfloor 631/128 \\rfloor=4$。$5$ 个事务。\n- 对于 $w$：地址 $12, ..., 632$。范围 $[12, 635]$。段 $\\lfloor 12/128 \\rfloor=0$ 到 $\\lfloor 635/128 \\rfloor=4$。$5$ 个事务。\n- 对于 $p$：地址 $16, ..., 636$。范围 $[16, 639]$。段 $\\lfloor 16/128 \\rfloor=0$ 到 $\\lfloor 639/128 \\rfloor=4$。$5$ 个事务。\n\nAoS 布局的总事务数 $N_{\\mathrm{AoS}}$ 是 $5$ 次加载的事务数之和：\n$$N_{\\mathrm{AoS}} = 5 \\text{ loads} \\times 5 \\frac{\\text{transactions}}{\\text{load}} = 25$$\n\n**数组结构（SoA）布局分析**\n\n在 SoA 布局中， $5$ 个变量中的每一个都存储在一个单独的数组中。每个数组中的元素大小为 $4$ 字节并连续存储。在任何给定数组中，连续元素之间的步长 $L_{SoA}$ 为 $4$ 字节。\n\n1.  **加载 $\\rho$ 数组**：\n    线程 $t$ 从 $\\rho$ 数组中读取索引为 $t$ 的元素。地址为 $A_{\\rho}(t) = A_{base, \\rho} + t \\times L_{SoA}$。\n    线程束访问的地址是：$0 \\times 4, 1 \\times 4, 2 \\times 4, \\dots, 31 \\times 4$。\n    这可以简化为地址 $0, 4, 8, \\dots, 124$。\n    读取的数据总量为 $32 \\text{ threads} \\times 4 \\frac{\\text{bytes}}{\\text{thread}} = 128$ 字节。\n    最低地址是 $0$。最高地址是 $124+3 = 127$。\n    访问的整个内存范围是 $[0, 127]$。\n    由于基地址是 $128$ 字节对齐的，这个连续的 $128$ 字节数据块完美地适配单个 $128$ 字节内存段（段 $0$）。\n    因此，整个线程束的加载被完美地合并为单个事务。\n\n这种理想的合并适用于 $5$ 个独立数组中的每一个。对 $u$ 数组的读取将访问一个连续的 $128$ 字节块，对 $v$ 数组的读取也是如此，依此类推。这些读取中的每一次都将只需要 $1$ 个事务。\n\nSoA 布局的总事务数 $N_{\\mathrm{SoA}}$ 是 $5$ 次数组读取的事务数之和：\n$$N_{\\mathrm{SoA}} = 5 \\text{ loads} \\times 1 \\frac{\\text{transaction}}{\\text{load}} = 5$$\n\n最终的事务计数为 $N_{\\mathrm{AoS}} = 25$ 和 $N_{\\mathrm{SoA}} = 5$。这表明对于 GPU 上的此类单位步长、线程束同步的访问模式，SoA 布局具有显著的性能优势。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n25  5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "对于具有数据复用特性的算法（如CFD中常见的模板计算），利用片上共享内存是减少对高延迟全局内存访问的关键策略。本练习要求你为一个三维7点模板核函数设计一个最优的计算“瓦片”(tile)大小。你需要权衡硬件限制（如共享内存容量和线程块大小）与最大化数据复用率的目标，这是编写高性能GPU核函数的核心技能之一。",
            "id": "3287388",
            "problem": "一个具有流式多处理器（Streaming Multiprocessor, SM）组织的图形处理单元（Graphics Processing Unit, GPU）被用来加速一个三维结构化网格的计算流体动力学（Computational Fluid Dynamics, CFD）内核，该内核使用一个7点模板（中心点加上六个轴对齐的最近邻点）来更新一个标量场 $u(i,j,k)$。该内核将一个大小为 $T_{x} \\times T_{y} \\times T_{z}$ 的内部单元瓦片（tile）映射到一个线程块，每个内部单元对应一个线程。为了利用片上数据的复用，每个线程块协作地将一个包含所有面上单层单元晕圈（halo）（$h=1$）的瓦片加载到共享内存中，因此每个线程块的共享内存占用量是填充后范围与元素大小的乘积。\n\n硬件为每个SM提供 $64\\,\\mathrm{KB}$ 的共享内存，并且寄存器的可用性将占用率（occupancy）限制在 $50\\%$。为避免共享内存将占用率进一步降低到 $50\\%$ 以下，每个线程块的共享内存分配不得超过SM共享内存的一半，即每个线程块不超过 $32\\,\\mathrm{KB}$。标量场以双精度存储，因此每个元素占用 $8$ 字节。该架构每个线程块最多允许 $1024$ 个线程。假设每个线程块对每个瓦片执行一次模板更新，并且只有一个场被暂存到共享内存中（没有多场耦合）。\n\n将数据复用因子定义为 $R=\\dfrac{T_{x}T_{y}T_{z}}{(T_{x}+2)(T_{y}+2)(T_{z}+2)}$，它表示被更新的共享内存元素占已加载共享内存元素总数的比例。使用共享内存占用量、占用率和模板晕圈需求的基本定义，并假设使用对称瓦片以最小化晕圈开销，确定最大的整数对称内部瓦片边长 $T$（其中 $T_{x}=T_{y}=T_{z}=T$），该边长需同时满足 (i) 在约束条件下最大化 $R$ 和 (ii) 不会因共享内存而将SM占用率降低到 $50\\%$ 以下。以三元组 $(T_{x},T_{y},T_{z})$ 的形式报告你的答案。不需要四舍五入，最终答案中不包含单位。",
            "solution": "问题要求我们为一个特定的计算流体动力学（CFD）内核确定最大的整数对称内部瓦片边长，记为 $T$，其中 $T_{x}=T_{y}=T_{z}=T$。这个 $T$ 值必须满足几个硬件约束，同时最大化数据复用因子 $R$。\n\n首先，让我们将问题陈述中给出的约束条件形式化。\n\n1.  **线程块大小约束**：一个大小为 $T_{x} \\times T_{y} \\times T_{z}$ 的内部单元瓦片映射到一个线程块，每个内部单元一个线程。每个线程块允许的最大线程数为 $1024$。对于对称瓦片，其中 $T_{x} = T_{y} = T_{z} = T$，总线程数为 $T \\times T \\times T = T^3$。因此，我们有以下约束：\n    $$T^3 \\leq 1024$$\n\n2.  **共享内存约束**：内核将一个数据瓦片加载到共享内存中，包括所有面上的单层单元晕圈（$h=1$）。因此，加载到共享内存中的数据维度为 $(T_{x}+2h) \\times (T_{y}+2h) \\times (T_{z}+2h)$。在 $h=1$ 和对称瓦片的假设下，这变为 $(T+2) \\times (T+2) \\times (T+2) = (T+2)^3$ 个元素。标量场以双精度存储，意味着每个元素占用 $8$ 字节。问题陈述指出，为了保持至少 $50\\%$ 的占用率，每个线程块的共享内存分配不得超过SM总共享内存的一半。总共享内存为 $64\\,\\mathrm{KB}$，因此每个线程块的限制是 $32\\,\\mathrm{KB}$。我们必须将千字节转换为字节，其中 $1\\,\\mathrm{KB} = 1024$ 字节。限制为 $32 \\times 1024 = 32768$ 字节。这引出了第二个约束：\n    $$(T+2)^3 \\times 8 \\leq 32768$$\n\n接下来，我们必须考虑要最大化的目标函数，即数据复用因子 $R$。它被定义为内部单元数（被更新的元素）与加载到共享内存中的总单元数（包括晕圈）之比。对于对称瓦片：\n$$R = \\frac{T_{x}T_{y}T_{z}}{(T_{x}+2)(T_{y}+2)(T_{z}+2)} = \\frac{T^3}{(T+2)^3} = \\left(\\frac{T}{T+2}\\right)^3$$\n\n为了最大化 $R$，我们需要对正整数 $T$ 最大化指数的底，即函数 $f(T) = \\frac{T}{T+2}$。我们考察 $f(T)$ 关于 $T$ 的导数：\n$$f'(T) = \\frac{d}{dT}\\left(\\frac{T}{T+2}\\right) = \\frac{1 \\cdot (T+2) - T \\cdot 1}{(T+2)^2} = \\frac{2}{(T+2)^2}$$\n由于 $T$ 代表瓦片的维度，它必须是一个正整数（$T \\geq 1$）。因此，$(T+2)^2 > 0$，且 $f'(T) > 0$。这表明对于所有 $T > 0$，$f(T)$ 是一个严格递增的函数。因此，$R = (f(T))^3$ 也是 $T$ 的一个严格递增函数。为了最大化 $R$，我们必须找到满足两个硬件约束的 $T$ 的最大整数值。\n\n现在让我们求解关于 $T$ 的不等式。\n\n从线程块大小约束来看：\n$$T^3 \\leq 1024$$\n$$T \\leq \\sqrt[3]{1024}$$\n我们知道 $10^3 = 1000$，所以 $\\sqrt[3]{1024}$ 略大于 $10$。具体来说，$\\sqrt[3]{1024} \\approx 10.079$。由于 $T$ 必须是整数，此约束意味着：\n$$T \\leq 10$$\n\n从共享内存约束来看：\n$$8(T+2)^3 \\leq 32768$$\n$$(T+2)^3 \\leq \\frac{32768}{8}$$\n$$(T+2)^3 \\leq 4096$$\n对两边取立方根：\n$$T+2 \\leq \\sqrt[3]{4096}$$\n我们可以识别出 $4096 = 2^{12}$，所以 $\\sqrt[3]{4096} = (2^{12})^{1/3} = 2^{12/3} = 2^4 = 16$。\n$$T+2 \\leq 16$$\n$$T \\leq 14$$\n\n为了同时满足两个约束，$T$ 必须小于或等于两个上界中的最小值：\n$$T \\leq \\min(10, 14)$$\n$$T \\leq 10$$\n\n满足所有条件的 $T$ 的最大整数值是 $T=10$。该值将在遵守每个线程块的线程数和每个线程块的共享内存的架构限制的同时，最大化数据复用因子 $R$。\n\n让我们用 $T = 10$ 来验证解：\n-   线程数：$10^3 = 1000$，小于等于 $1024$。该约束得到满足。\n-   共享内存使用量：$8 \\times (10+2)^3 = 8 \\times 12^3 = 8 \\times 1728 = 13824$ 字节。这小于等于 $32768$ 字节。该约束得到满足。\n\n如果我们尝试 $T = 11$，线程数将是 $11^3 = 1331$，这违反了 $1024$ 个线程的限制。因此，$T=10$ 确实是可能的最大整数值。\n\n问题要求以三元组 $(T_{x}, T_{y}, T_{z})$ 的形式给出答案。由于 $T_x = T_y = T_z = T$，结果是 $(10, 10, 10)$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 10  10  10 \\end{pmatrix}}$$"
        },
        {
            "introduction": "将视角从单个GPU扩展到多加速器集群，我们面临着新的挑战：并行扩展性。本练习基于一个典型的强扩展性场景，探讨了通信开销如何根据阿姆达尔定律(Amdahl's law)限制性能增益。通过分析真实的性能数据，你将学习如何诊断并行效率瓶颈，并提出如计算-通信重叠等高级策略来优化大规模科学计算应用的整体性能。",
            "id": "3287363",
            "problem": "一个基于有限体积法的三维可压缩 Navier–Stokes 求解器在一个加速器集群上执行。该求解器随时间推进一个固定大小的网格，在每个时间步执行通量重构、限制器和残差更新，并在相邻子域之间交换晕环层数据。该代码使用通过计算统一设备架构 (CUDA) 启动的图形处理单元 (GPU) 核函数，并利用消息传递接口 (MPI) 实现设备间通信，且启用了设备到设备的直接传输。\n\n假设在单个 GPU 上处理完整问题规模的基准墙上时钟时间为 $T_{1} = 100$ s，在八个 GPU 上（问题规模保持不变）的墙上时钟时间为 $T_{8} = 15$ s。使用强标度加速比和效率的第一性原理定义，计算在八个加速器上的强标度效率。将您的答案四舍五入至四位有效数字，并以无量纲值的形式表示。\n\n然后，从阿姆达尔定律的陈述以及对设备内存带宽和互连延迟等加速器特定瓶颈的认识出发，论证两种可以为该求解器将强标度效率提高到 $0.8$ 以上的加速器感知并行化策略。您的论证应说明每种策略如何改变并行工作、通信和同步之间的平衡，以及为什么这种转变会提高在强标度下的实测效率。您的最终数值答案应只包含计算出的效率值。",
            "solution": "在尝试解答之前，需对问题进行验证。\n\n### 第 1 步：提取给定信息\n- 求解器类型：三维可压缩 Navier–Stokes 求解器。\n- 数值方法：有限体积法。\n- 硬件和软件环境：一个图形处理单元 (GPU) 集群，使用计算统一设备架构 (CUDA) 执行核函数，并使用消息传递接口 (MPI) 进行设备间通信，启用了设备到设备直接传输。\n- 标度分析类型：强标度（固定问题规模）。\n- 单个加速器上的基准墙上时钟时间：$T_{1} = 100$ s。\n- 八个加速器上的墙上时钟时间：$T_{8} = 15$ s。\n- 并行运行的加速器数量：$N = 8$。\n- 要求 1：计算在八个加速器上的强标度效率，四舍五入至四位有效数字。\n- 要求 2：基于阿姆达尔定律和加速器特定瓶颈（设备内存带宽、互连延迟），论证两种可将强标度效率提高到 $0.8$ 以上的加速器感知并行化策略。\n\n### 第 2 步：使用提取的给定信息进行验证\n根据验证标准评估问题陈述。\n- **科学依据**：该问题设置在高性能计算 (HPC)应用于计算流体力学 (CFD) 这一成熟领域。Navier-Stokes 方程、有限体积法、使用 CUDA 的 GPU 加速、MPI、强标度、加速比、效率和阿姆达尔定律等概念都是计算科学与工程中的基本和标准原则。给定的性能数据对于一个代表性的基准测试是合理的。\n- **适定性**：问题结构清晰，分为两部分。第一部分要求基于标准、明确的加速比和效率定义进行计算，并提供了所有必要数据（$T_1$、$T_8$、$N=8$）。第二部分要求对优化策略进行定性论证，但受限于特定的理论原则（阿姆达尔定律）和实际瓶颈（内存/互连），使其成为一个适定的工程和计算机科学推理问题，而非主观看法。\n- **客观性**：语言技术性强、精确，且没有主观或带偏见的陈述。\n\n该问题没有表现出任何科学不健全、信息缺失、内部矛盾或依赖伪科学等缺陷。它代表了并行科学计算研究生课程中一个典型且有效的问题。\n\n### 第 3 步：结论与行动\n问题有效。将提供完整解答。\n\n### 解答\n\n按照要求，解答分为两部分：强标度效率的计算和两种优化策略的论证。\n\n#### 第 1 部分：强标度效率的计算\n\n强标度衡量的是在固定总问题规模下，增加处理单元数量时性能的提升。\n\n在 $N$ 个处理器上的强标度加速比 $S_N$ 定义为在单个处理器上的执行时间 $T_1$ 与在 $N$ 个处理器上的执行时间 $T_N$ 之比。\n$$S_N = \\frac{T_1}{T_N}$$\n对于此问题，给定在 $N=8$ 个加速器上，$T_1 = 100$ s 和 $T_8 = 15$ s。加速比为：\n$$S_8 = \\frac{T_1}{T_8} = \\frac{100 \\text{ s}}{15 \\text{ s}} = \\frac{20}{3}$$\n强标度效率 $E_N$ 定义为每个处理器的加速比。它衡量了额外处理器被利用的有效程度。它是实际加速比与理想线性加速比（即 $N$）之比。\n$$E_N = \\frac{S_N}{N}$$\n对于 $N=8$ 个加速器，效率为：\n$$E_8 = \\frac{S_8}{8} = \\frac{20/3}{8} = \\frac{20}{24} = \\frac{5}{6}$$\n为提供四舍五入至四位有效数字的答案，我们将分数转换为小数表示：\n$$E_8 = \\frac{5}{6} \\approx 0.833333...$$\n四舍五入至四位有效数字，得到：\n$$E_8 \\approx 0.8333$$\n\n#### 第 2 部分：加速器感知并行化策略的论证\n\n计算出的效率 $E_8 \\approx 0.8333$ 小于理想值 $1$，表明存在并行开销。阿姆达尔定律为此行为提供了模型。在 $N$ 个处理器上的理论加速比 $S(N)$ 由以下公式给出：\n$$S(N) = \\frac{1}{f + \\frac{1-f}{N}}$$\n其中 $f$ 是程序中固有串行或无法并行的部分所占的比例。在像此 CFD 求解器这样的实际应用中，$f$ 不仅包括严格的串行代码，还包括不随 $N$ 扩展的并行开销，如进程间通信、同步和资源争用。优化的目标是减小 $f$ 的有效值。问题指明内存带宽和互连延迟是关键瓶颈，它们是此开销的主要来源。\n\n两种通过缓解这些瓶颈来提高强标度效率的策略是：\n\n**1. 通信-计算重叠**\n\n该策略旨在隐藏基于 MPI 的跨互连晕环层数据交换的延迟，在强标度场景中，随着 $N$ 的增加，计算-通信比会减小，这种延迟是主要的开销。\n\n*   **策略描述**：重构求解器代码，使其结合使用非阻塞 MPI 通信（`MPI_Isend`、`MPI_Irecv`）和 CUDA 流。每个时间步的操作序列将是：\n    1.  启动非阻塞接收，以获取来自相邻子域的晕环层数据。\n    2.  启动非阻塞发送，将本地边界数据发送给邻居。\n    3.  启动 GPU 核函数，为本地子域的*内部*单元计算通量和其他更新。这些单元不依赖于正在传输中的晕环层数据。\n    4.  同步以确保通信完成（`MPI_Waitall`）。\n    5.  启动 GPU 核函数，为*边界*单元计算更新，此时这些单元已有所需的晕环层数据。\n\n*   **论证**：根据阿姆达尔定律，通信延迟是不可并行部分 $f$ 的一部分。在一个简单的阻塞实现中，处理器在等待数据时处于空闲状态。该策略将通信时间与内部单元的计算时间重叠。这有效地隐藏了通信延迟，减少了处理器的空闲时间。这降低了给定 $N$ 下的总墙上时钟时间 $T_N$，从而直接增加了实测加速比 $S_N$ 和效率 $E_N = S_N/N$。这是一种加速器感知的策略，因为它需要对 GPU（通过 CUDA 流）和网络接口上的并发操作进行显式管理。\n\n**2. 核函数融合与数据局部性优化**\n\n该策略旨在减少核函数启动延迟带来的开销，更重要的是，缓解设备内存带宽瓶颈。\n\n*   **策略描述**：将计算的不同阶段（例如，一个用于通量重构的核函数，另一个用于应用限制器的核函数，第三个用于残差更新的核函数）的操作组合或“融合”成一个单一、更大的核函数，而不是启动一系列独立的 GPU 核函数。这通常需要重组数据结构（例如，从结构数组（AoS）到数组结构（SoA），或数组的结构数组（AoSoA）），以实现合并内存访问并促进数据重用。\n\n*   **论证**：此策略从两个方面解决并行开销。首先，每次核函数启动都会产生固定的延迟，这是一种串行开销。将 $M$ 个核函数融合成 1 个，可将此开销减少 $M$ 倍。其次，也是更关键的一点，它改善了数据局部性。在一系列独立的核函数中，每个核函数执行后，中间结果被写入缓慢的全局设备内存，然后由下一个核函数读回。而融合后的核函数可以将这些中间结果保存在速度快得多的片上内存（寄存器和共享内存）中，避免了与全局内存之间代价高昂的往返。这显著减少了总内存流量，使计算受内存带宽的限制更小。这增加了代码的算术强度（浮点运算次数与传输字节数之比），使得并行工作（$1-f$）能够更快地执行。并行部分的更快执行减少了总时间 $T_N$，从而提高了加速比 $S_N$ 和效率 $E_N$。这是一种根本性的加速器感知优化，因为它需要对 GPU 的内存层次结构和执行模型有详细的了解。",
            "answer": "$$\\boxed{0.8333}$$"
        }
    ]
}