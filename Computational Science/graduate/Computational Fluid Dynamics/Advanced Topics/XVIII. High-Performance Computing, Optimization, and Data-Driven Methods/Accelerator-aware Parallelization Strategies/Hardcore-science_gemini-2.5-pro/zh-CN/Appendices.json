{
    "hands_on_practices": [
        {
            "introduction": "在GPU编程中，性能优化的第一步往往是审视数据在内存中的组织方式。对于并行处理单元（如GPU中的线程束）而言，最高效的内存访问模式是“合并访问”，即一次性读取一块连续的内存。本练习将通过一个具体计算流体动力学（CFD）场景，量化比较两种基本数据布局——结构体数组（AoS）和数组的结构（SoA）——在内存访问效率上的巨大差异。通过这个练习，你将深刻理解为何选择正确的数据布局是发挥加速器性能的关键。",
            "id": "3287336",
            "problem": "考虑一个三维可压缩流求解器，每个单元存储五个原始变量 $(\\rho,u,v,w,p)$，采用单精度（每个变量是一个 $4$ 字节的浮点数）。您正在图形处理器（GPU, Graphics Processing Unit）上实现邻居无关的逐单元读取，其中每个 warp 由 $32$ 个线程组成。内存系统将一个 warp 在单条加载指令中的内存访问合并为最少数目的 $128$ 字节对齐的事务，其中一个事务覆盖一个独立的 $128$ 字节段。假设遵循以下科学上标准且被广泛遵守的规则和设置：\n\n- 任何参与访问的数组的基地址都与 $128$ 字节边界对齐。\n- warp 中的每个线程读取一个单元的五个变量，采用连续的单元索引映射：线程 $t$ 读取单元索引 $i=t$，其中 $t \\in \\{0,1,\\dots,31\\}$。\n- 合并是按加载指令发生的，而不是跨不同的加载指令。每个标量变量的读取对应一条加载指令，并被独立地合并。\n- 在结构体数组（AoS）布局中，每个单元的记录被紧密打包为 $(\\rho,u,v,w,p)$，没有填充，因此每个单元的步长为 $20$ 字节。在数组的结构（SoA）布局中，五个变量存储在五个独立的数组中，每个数组的元素大小为 $4$ 字节，元素步长为 $4$ 字节。\n\n从这些基本原理出发，推导并计算当读取 $32$ 个连续单元的 $(\\rho,u,v,w,p)$ 时，对于 AoS 布局与 SoA 布局，每个 warp 发出的 $128$ 字节内存事务的总数。将您的最终答案表示为一个行矩阵，包含两个整数计数 $(N_{\\mathrm{AoS}}, N_{\\mathrm{SoA}})$，无需单位。不需要四舍五入。",
            "solution": "首先验证问题以确保其具有科学依据、问题定义良好且客观。\n\n**步骤 1：提取给定条件**\n- 每个单元的原始变量数量：$5$ $(\\rho, u, v, w, p)$。\n- 数据类型：单精度，每个变量 $4$ 字节。\n- GPU warp 大小：$32$ 个线程。\n- 内存事务大小：$128$ 字节。\n- 内存事务基础：每条加载指令所需的最少数目的、不同的、$128$ 字节对齐的段。\n- 数组对齐：基地址与 $128$ 字节边界对齐。\n- 线程到单元的映射：线程 $t$ 读取单元索引 $i=t$，其中 $t \\in \\{0, 1, \\dots, 31\\}$。\n- 加载指令范围：每个标量变量 $(\\rho, u, v, w, p)$ 的读取对应一条独立的、分开的加载指令。\n- 结构体数组（AoS）布局：紧密打包的记录 $(\\rho, u, v, w, p)$，连续单元起始点之间的步长为 $5 \\times 4 = 20$ 字节。\n- 数组的结构（SoA）布局：$5$ 个独立的数组，每个变量一个，元素大小和步长均为 $4$ 字节。\n\n**步骤 2：使用提取的条件进行验证**\n该问题具有科学依据，描述了在 GPU 上进行高性能计算时的一个标准内存访问优化场景。AoS/SoA 数据布局、基于 warp 的执行以及内存合并等概念是 GPU 架构（例如 CUDA 或 OpenCL 编程模型）的基础。所提供的参数（warp 大小为 $32$，单精度浮点数为 $4$ 字节，事务大小为 $128$ 字节）对于现代硬件是标准且现实的。该问题定义良好，提供了唯一计算内存事务数量所需的所有必要信息（数据大小、步长、对齐、线程映射）。语言客观而精确。因此，该问题被认为是有效的。\n\n**步骤 3：进行求解**\n\n目标是计算一个完整的 warp（包含 $32$ 个线程）在为每个线程读取 $5$ 个变量时，对于 AoS 和 SoA 两种布局，所需的 $128$ 字节内存事务的总数。一个内存事务被定义为对单个、唯一的 $128$ 字节对齐内存段的读取。对于给定的加载指令，事务总数是包含 warp 中所有 $32$ 个线程请求的数据的唯一 $128$ 字节段的数量。\n\n设任何数组的基地址为 $A_{base}$。由于所有基地址都是 $128$ 字节对齐的，我们可以通过考虑相对于此基地址的偏移量来分析内存访问模式，为不失一般性，可将其设为 $0$。每个变量的大小是 $S_{var} = 4$ 字节。一个 warp 中的线程数是 $N_{threads} = 32$。内存事务的大小是 $S_{trans} = 128$ 字节。\n\n**结构体数组（AoS）布局分析**\n\n在 AoS 布局中，单个单元的数据是连续存储的。单个单元数据记录的大小为 $5 \\text{ 个变量} \\times 4 \\frac{\\text{字节}}{\\text{变量}} = 20$ 字节。这是连续单元记录起始点之间的步长 $L_{AoS}$。\n\n问题陈述，读取 $5$ 个标量变量中的每一个都对应一条独立的加载指令。我们必须分析这 $5$ 次加载中的每一次。\n\n1.  **加载变量 $\\rho$**：\n    线程 $t$（其中 $t \\in \\{0, 1, \\dots, 31\\}$）从单元 $t$ 中读取变量 $\\rho$。该变量的字节地址由 $A_{\\rho}(t) = A_{base} + t \\times L_{AoS} + \\text{offset}(\\rho)$ 给出。$\\rho$ 在结构体内的偏移量为 $0$。\n    warp 访问的地址是：$0 \\times 20, 1 \\times 20, 2 \\times 20, \\dots, 31 \\times 20$。\n    这简化为地址 $0, 20, 40, \\dots, 620$。\n    每个线程读取 $4$ 字节。因此，线程 $t$ 访问字节范围 $[t \\times 20, t \\times 20 + 3]$。\n    warp 访问的最低地址是 $0$（由线程 $0$ 访问）。\n    warp 访问的最高地址是 $620 + 3 = 623$（由线程 $31$ 访问）。\n    所触及的内存总范围是从字节 $0$到字节 $623$。\n    为了找到 $128$ 字节事务的数量，我们确定这个范围与多少个唯一的 $128$ 字节段相交。段 $k$ 覆盖字节 $[k \\times 128, (k+1) \\times 128 - 1]$。\n    访问的第一个段是针对字节 $0$：段 $\\lfloor \\frac{0}{128} \\rfloor = 0$。\n    访问的最后一个段是针对字节 $623$：段 $\\lfloor \\frac{623}{128} \\rfloor = \\lfloor 4.867... \\rfloor = 4$。\n    访问的唯一段是段 $0, 1, 2, 3, 4$。这总共是 $5$ 个不同的段。\n    因此，加载 $\\rho$ 需要 $5$ 个事务。\n\n2.  **加载变量 $u$**：\n    $u$ 在结构体内的偏移量为 $4$ 字节。线程 $t$ 的地址是 $t \\times 20 + 4$。\n    warp 访问的地址是 $4, 24, 44, \\dots, 624$。\n    最低地址是 $4$。最高地址是 $624 + 3 = 627$。\n    第一个段是 $\\lfloor \\frac{4}{128} \\rfloor = 0$。\n    最后一个段是 $\\lfloor \\frac{627}{128} \\rfloor = \\lfloor 4.9... \\rfloor = 4$。\n    同样，这跨越了段 $0, 1, 2, 3, 4$，导致 $5$ 个事务。\n\n同样的逻辑适用于其余变量 $v$、$w$ 和 $p$，它们的偏移量分别为 $8$、$12$ 和 $16$ 字节。在每种情况下，warp 访问的内存地址将跨越一个落入 $5$ 个不同 $128$ 字节段的范围。\n- 对于 $v$：地址 $8, ..., 628$。范围 $[8, 631]$。段 $\\lfloor 8/128 \\rfloor=0$ 到 $\\lfloor 631/128 \\rfloor=4$。$5$ 个事务。\n- 对于 $w$：地址 $12, ..., 632$。范围 $[12, 635]$。段 $\\lfloor 12/128 \\rfloor=0$ 到 $\\lfloor 635/128 \\rfloor=4$。$5$ 个事务。\n- 对于 $p$：地址 $16, ..., 636$。范围 $[16, 639]$。段 $\\lfloor 16/128 \\rfloor=0$ 到 $\\lfloor 639/128 \\rfloor=4$。$5$ 个事务。\n\nAoS 布局的总事务数 $N_{\\mathrm{AoS}}$ 是 $5$ 次加载的事务数之和：\n$$N_{\\mathrm{AoS}} = 5 \\text{ 次加载} \\times 5 \\frac{\\text{个事务}}{\\text{次加载}} = 25$$\n\n**数组的结构（SoA）布局分析**\n\n在 SoA 布局中， $5$ 个变量中的每一个都存储在一个单独的数组中。每个数组中的元素大小为 $4$ 字节并连续存储。在任何给定数组中，连续元素之间的步长 $L_{SoA}$ 为 $4$ 字节。\n\n1.  **加载 $\\rho$ 数组**：\n    线程 $t$ 从 $\\rho$ 数组中读取索引为 $t$ 的元素。地址为 $A_{\\rho}(t) = A_{base, \\rho} + t \\times L_{SoA}$。\n    warp 访问的地址是：$0 \\times 4, 1 \\times 4, 2 \\times 4, \\dots, 31 \\times 4$。\n    这简化为地址 $0, 4, 8, \\dots, 124$。\n    读取的数据总量为 $32 \\text{ 个线程} \\times 4 \\frac{\\text{字节}}{\\text{线程}} = 128$ 字节。\n    最低地址是 $0$。最高地址是 $124+3 = 127$。\n    访问的整个内存范围是 $[0, 127]$。\n    由于基地址是 $128$ 字节对齐的，这个连续的 $128$ 字节数据块完全符合一个单一的 $128$ 字节内存段（段 $0$）。\n    因此，整个 warp 的加载被完美地合并为一个单一的事务。\n\n这种理想的合并适用于 $5$ 个独立数组中的每一个。对 $u$ 数组的读取将访问一个连续的 $128$ 字节块，对 $v$ 数组的读取也是如此，依此类推。这些读取中的每一次都只需要 $1$ 个事务。\n\nSoA 布局的总事务数 $N_{\\mathrm{SoA}}$ 是 $5$ 次数组读取的事务数之和：\n$$N_{\\mathrm{SoA}} = 5 \\text{ 次加载} \\times 1 \\frac{\\text{个事务}}{\\text{次加载}} = 5$$\n\n最终的事务计数为 $N_{\\mathrm{AoS}} = 25$ 和 $N_{\\mathrm{SoA}} = 5$。这表明对于 GPU 上的这种单位步长、warp 同步的访问模式，SoA 布局具有显著的性能优势。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n25  5\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在选择了高效的全局内存布局（如SoA）之后，下一步优化的重点便转向了内核内部，即如何最大限度地重用已加载到芯片上的数据。GPU的共享内存（Shared Memory）提供了一个可编程的高速缓存，是实现这一目标的核心工具。本练习以一个经典的三维七点模板计算为例，指导你如何在硬件资源（如共享内存大小和线程块限制）的约束下，设计出最优的数据块（Tile）尺寸。这个过程是典型的计算密集型任务优化的核心环节，旨在通过提升数据重用率来降低对高延迟的全局内存的访问压力。",
            "id": "3287388",
            "problem": "一个具有流式多处理器（SM）组织的图形处理单元（GPU）被用于加速一个三维结构化网格的计算流体力学（CFD）内核，该内核使用一个7点模板（中心点加上六个轴对齐的最近邻点）来更新一个标量场 $u(i,j,k)$。该内核将一个大小为 $T_{x} \\times T_{y} \\times T_{z}$ 的内部单元瓦片映射到一个线程块，每个内部单元对应一个线程。为了利用片上数据的复用，每个线程块协同地将一个瓦片加载到共享内存中，该瓦片在所有面上都包含一个单层单元的光环（$h=1$），因此每个线程块的共享内存占用量是填充后的尺寸乘以元素大小的乘积。\n\n硬件为每个SM提供 $64\\,\\mathrm{KB}$ 的共享内存，并且寄存器的可用性将占用率限制在 $50\\%$。为避免共享内存进一步将占用率降低到 $50\\%$ 以下，每个线程块的共享内存分配不得超过SM共享内存的一半，即每个线程块不超过 $32\\,\\mathrm{KB}$。标量场以双精度存储，因此每个元素占用 $8$ 字节。该架构允许每个线程块最多有 $1024$ 个线程。假设每个线程块对每个瓦片执行一次模板更新，并且只有一个场被暂存在共享内存中（没有多场耦合）。\n\n将数据复用因子定义为 $R=\\dfrac{T_{x}T_{y}T_{z}}{(T_{x}+2)(T_{y}+2)(T_{z}+2)}$，它表示被更新的共享内存元素占已加载共享内存元素总数的比例。利用共享内存占用、占用率和模板光环需求的基本定义，并假设使用对称瓦片以最小化光环开销，确定最大的整数对称内部瓦片边长 $T$（其中 $T_{x}=T_{y}=T_{z}=T$），该边长应同时满足 (i) 在约束条件下最大化 $R$ 和 (ii) 不会因共享内存而将SM占用率降低到 $50\\%$ 以下。以三元组 $(T_{x},T_{y},T_{z})$ 的形式报告你的答案。不需要四舍五入，最终答案中不包含单位。",
            "solution": "题目要求我们为一个特定的计算流体力学（CFD）内核确定最大的整数对称内部瓦片边长，记为 $T$，其中 $T_{x}=T_{y}=T_{z}=T$。这个 $T$ 值必须在满足几个硬件约束的同时，最大化数据复用因子 $R$。\n\n首先，我们来形式化题目陈述中给出的约束条件。\n\n1.  **线程块大小约束**：一个大小为 $T_{x} \\times T_{y} \\times T_{z}$ 的内部单元瓦片被映射到一个线程块，每个内部单元对应一个线程。每个线程块允许的最大线程数是 $1024$。对于一个对称瓦片，其中 $T_{x} = T_{y} = T_{z} = T$，总线程数为 $T \\times T \\times T = T^3$。因此，我们有以下约束：\n    $$T^3 \\leq 1024$$\n\n2.  **共享内存约束**：该内核将一个数据瓦片加载到共享内存中，包括在所有面上的一层单元光环（$h=1$）。因此，加载到共享内存中的数据维度为 $(T_{x}+2h) \\times (T_{y}+2h) \\times (T_{z}+2h)$。当 $h=1$ 并且假设为对称瓦片时，这变为 $(T+2) \\times (T+2) \\times (T+2) = (T+2)^3$ 个元素。标量场以双精度存储，意味着每个元素占用 $8$ 字节。题目指出，为维持至少 $50\\%$ 的占用率，每个线程块的共享内存分配不得超过SM总共享内存的一半。总共享内存为 $64\\,\\mathrm{KB}$，因此每个线程块的限制是 $32\\,\\mathrm{KB}$。我们必须将千字节转换为字节，其中 $1\\,\\mathrm{KB} = 1024$ 字节。该限制为 $32 \\times 1024 = 32768$ 字节。这引出了第二个约束：\n    $$(T+2)^3 \\times 8 \\leq 32768$$\n\n接下来，我们必须考虑需要最大化的目标函数，即数据复用因子 $R$。它被定义为内部单元（被更新的元素）数量与加载到共享内存中的总单元（包括光环）数量之比。对于对称瓦片：\n$$R = \\frac{T_{x}T_{y}T_{z}}{(T_{x}+2)(T_{y}+2)(T_{z}+2)} = \\frac{T^3}{(T+2)^3} = \\left(\\frac{T}{T+2}\\right)^3$$\n\n为了最大化 $R$，我们需要对正整数 $T$ 最大化指数的底，即函数 $f(T) = \\frac{T}{T+2}$。我们考察 $f(T)$ 关于 $T$ 的导数：\n$$f'(T) = \\frac{d}{dT}\\left(\\frac{T}{T+2}\\right) = \\frac{1 \\cdot (T+2) - T \\cdot 1}{(T+2)^2} = \\frac{2}{(T+2)^2}$$\n由于 $T$ 代表瓦片维度，它必须是一个正整数（$T \\geq 1$）。因此，$(T+2)^2  0$，且 $f'(T)  0$。这表明 $f(T)$ 对于所有 $T  0$ 都是一个严格递增函数。因此，$R = (f(T))^3$ 也是 $T$ 的一个严格递增函数。为了最大化 $R$，我们必须找到满足两个硬件约束的 $T$ 的最大整数值。\n\n现在我们来解关于 $T$ 的不等式。\n\n根据线程块大小约束：\n$$T^3 \\leq 1024$$\n$$T \\leq \\sqrt[3]{1024}$$\n我们知道 $10^3 = 1000$，所以 $\\sqrt[3]{1024}$ 略大于 $10$。具体来说，$\\sqrt[3]{1024} \\approx 10.079$。因为 $T$ 必须是整数，所以此约束意味着：\n$$T \\leq 10$$\n\n根据共享内存约束：\n$$8(T+2)^3 \\leq 32768$$\n$$(T+2)^3 \\leq \\frac{32768}{8}$$\n$$(T+2)^3 \\leq 4096$$\n对两边取立方根：\n$$T+2 \\leq \\sqrt[3]{4096}$$\n我们可以识别出 $4096 = 2^{12}$，所以 $\\sqrt[3]{4096} = (2^{12})^{1/3} = 2^{12/3} = 2^4 = 16$。\n$$T+2 \\leq 16$$\n$$T \\leq 14$$\n\n为了同时满足两个约束，$T$ 必须小于或等于两个上界中的最小值：\n$$T \\leq \\min(10, 14)$$\n$$T \\leq 10$$\n\n满足所有条件的 $T$ 的最大整数值是 $T=10$。这个值将在遵守每个线程块的线程数和每个线程块的共享内存的架构限制的同时，最大化数据复用因子 $R$。\n\n我们用 $T = 10$ 来验证解：\n-   线程数：$10^3 = 1000$，这 $\\leq 1024$。约束得到满足。\n-   共享内存使用量：$8 \\times (10+2)^3 = 8 \\times 12^3 = 8 \\times 1728 = 13824$ 字节。这 $\\leq 32768$ 字节。约束得到满足。\n\n如果我们尝试 $T = 11$，线程数将是 $11^3 = 1331$，这违反了 $1024$ 个线程的限制。因此，$T=10$ 确实是可能的最大整数值。\n\n题目要求以三元组 $(T_{x}, T_{y}, T_{z})$ 的形式给出答案。由于 $T_x = T_y = T_z = T$，结果是 $(10, 10, 10)$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 10  10  10 \\end{pmatrix}}$$"
        },
        {
            "introduction": "一个经过精细优化的内核，其最终性能表现仍可能受限于整个计算系统的其他环节，尤其是主机（CPU）与设备（GPU）之间的数据传输。在许多实际应用中，内核执行前需要从主机传输大量输入数据，执行后又需要传回结果。本练习将引导你进行一次经典的“信封背面计算”（back-of-the-envelope calculation），通过对比内核执行时间（受限于设备内存带宽）与数据传输时间（受限于PCIe总线带宽），来判断在包含数据传输的完整计算流程中，真正的性能瓶颈在何处。这种宏观层面的性能分析能力对于设计高效的端到端加速器应用至关重要。",
            "id": "3287394",
            "problem": "一个计算流体动力学内核在每个时间步长内对一个结构化场执行 $N=10^{9}$ 次单元中心更新。每次更新执行 $F=60$ 次浮点运算，并从设备全局内存中读写 $B=48$ 字节的数据。该内核运行在一台通用图形处理器 (GPU) 上，该 GPU 配备有高带宽内存 (HBM)，其可持续带宽为每秒 $900$ GB，并通过可持续带宽为每秒 $32$ GB 的第四代 $x16$ 外围组件互连快速标准 (PCIe) 连接到主机。假设 $1$ GB 等于 $10^{9}$ 字节。考虑两种情景：(i) 场数据保留在设备上，时间步长内不发生主机-设备传输；(ii) 在每个时间步长，大小为 $8$ GB 的完整场数据必须在内核运行前从主机传输到设备，并在内核运行后从设备传输回主机，由于严格的写后读依赖，计算与传输之间没有重叠。使用带宽、算术强度和工作量的基本定义，从第一性原理出发推导表达式，以估算在每种情景下完成一个时间步长所需的时间，并确定在这些条件下占主导地位的瓶颈环节。将您的最终答案以行矩阵的形式报告，其中第一个条目是设备驻留内核的时间（以秒为单位），第二个条目是包括主机到设备和设备到主机传输的时间（以秒为单位），第三个条目是包含传输情景中主要瓶颈的代码，其中 $1$ 表示 HBM，$2$ 表示 PCIe。将所有报告的时间四舍五入到三位有效数字，并以秒为单位表示。",
            "solution": "该问题要求在两种不同的数据管理情景下，估算一个计算流体动力学 (CFD) 内核的执行时间，并识别性能瓶颈。解决方案将从性能建模的第一性原理推导得出，主要关注数据移动和带宽限制。\n\n首先，我们建立基本定义。通过一个可持续带宽为 $BW$ 的链路移动数据量为 $D$ 所需的时间 $T$ 由公式 $T = \\frac{D}{BW}$ 给出。总工作量 $W$ 是浮点运算的总数。计算的算術強度 $AI$ 是工作量与数据移动量的比率，即 $AI = \\frac{W}{D}$。\n\n问题指明了以下给定数量：\n- 每个时间步长的单元更新次数：$N = 10^9$。\n- 每次更新的浮点运算次数：$F = 60$。\n- 每次更新从/向设备全局内存流式传输的字节数：$B = 48$。\n- GPU 高带宽内存 (HBM) 可持续带宽：$BW_{HBM} = 900 \\frac{\\text{GB}}{\\text{s}} = 900 \\times 10^9 \\frac{\\text{字节}}{\\text{s}}$。\n- 外围组件互连快速标准 (PCIe) 可持续带宽：$BW_{PCIe} = 32 \\frac{\\text{GB}}{\\text{s}} = 32 \\times 10^9 \\frac{\\text{字节}}{\\text{s}}$。\n- 用于传输的完整场大小：$S_{field} = 8 \\text{ GB} = 8 \\times 10^9 \\text{ 字节}$。\n\n让我们首先分析计算内核本身。\n内核执行的总工作量是更新次数与每次更新操作数的乘积：\n$$W = N \\times F = 10^9 \\times 60 = 60 \\times 10^9 \\text{ FLOPs}$$\n在内核执行期间，GPU 处理核心与其 HBM 之间移动的总数据量为：\n$$D_{HBM} = N \\times B = 10^9 \\times 48 = 48 \\times 10^9 \\text{ bytes} = 48 \\text{ GB}$$\n内核的执行时间 $T_{kernel}$ 由计算所需时间和从 HBM 进行数据移动所需时间中的最大值决定。这是屋顶线模型 (roofline model) 的原理。完成浮点运算的时间为 $T_{compute} = \\frac{W}{P_{peak}}$，其中 $P_{peak}$ 是 GPU 的峰值性能，单位为 FLOPs/s。数据移动的时间是 $T_{memory} = \\frac{D_{HBM}}{BW_{HBM}}$。\n内核的算术强度为：\n$$AI_{kernel} = \\frac{W}{D_{HBM}} = \\frac{60 \\times 10^9 \\text{ FLOPs}}{48 \\times 10^9 \\text{ bytes}} = 1.25 \\frac{\\text{FLOP}}{\\text{byte}}$$\n这个算术强度相对较低。对于配备 HBM 的现代 GPU，机器平衡点（峰值 FLOPs/s 与 HBM 带宽的比率）通常高于此值。因此，可以合理地假设该内核是内存受限的，这意味着其性能受到 HBM 带宽的限制，而不是计算速度的限制。问题的结构（提供了带宽数据但未提供峰值性能）支持了这一假设。因此，我们估计内核执行时间为：\n$$T_{kernel} = \\frac{D_{HBM}}{BW_{HBM}} = \\frac{48 \\times 10^9 \\text{ bytes}}{900 \\times 10^9 \\frac{\\text{bytes}}{\\text{s}}} = \\frac{48}{900} \\text{ s} \\approx 0.05333... \\text{ s}$$\n\n现在我们可以分析这两种情景。\n\n**情景 (i)：场数据驻留在设备上**\n在这种情景下，场数据保留在 GPU 上，在时间步长内没有数据通过 PCIe 总线传输。该步骤的总时间 $T_{(i)}$ 就是内核执行时间。\n$$T_{(i)} = T_{kernel} = \\frac{48}{900} \\text{ s} \\approx 0.05333... \\text{ s}$$\n四舍五入到三位有效数字，我们得到 $T_{(i)} \\approx 0.0533 \\text{ s}$。\n\n**情景 (ii)：包括主机-设备传输**\n在这种情景下，由于没有重叠，该步骤的总时间 $T_{(ii)}$ 是三个连续阶段的总和：主机到设备的传输、内核执行以及设备到主机的传输。\n$$T_{(ii)} = T_{H2D} + T_{kernel} + T_{D2H}$$\n每个方向传输的数据是完整的场大小，$S_{field} = 8 \\text{ GB}$。这些传输通过 PCIe 链路进行。\n主机到设备的传输时间为：\n$$T_{H2D} = \\frac{S_{field}}{BW_{PCIe}} = \\frac{8 \\times 10^9 \\text{ bytes}}{32 \\times 10^9 \\frac{\\text{bytes}}{\\text{s}}} = \\frac{8}{32} \\text{ s} = 0.25 \\text{ s}$$\n假设链路性能对称，设备到主机的传输时间是相同的：\n$$T_{D2H} = \\frac{S_{field}}{BW_{PCIe}} = 0.25 \\text{ s}$$\n总传输时间为 $T_{transfer} = T_{H2D} + T_{D2H} = 0.25 \\text{ s} + 0.25 \\text{ s} = 0.5 \\text{ s}$。\n情景 (ii) 的总时间是传输时间与内核执行时间的总和：\n$$T_{(ii)} = T_{transfer} + T_{kernel} = 0.5 \\text{ s} + \\frac{48}{900} \\text{ s} \\approx 0.5 \\text{ s} + 0.05333... \\text{ s} = 0.55333... \\text{ s}$$\n四舍五入到三位有效数字，$T_{(ii)} \\approx 0.553 \\text{ s}$。\n\n**情景 (ii) 的主要瓶颈**\n主要瓶颈是流程中耗时最多的部分。我们比较受 PCIe 总线限制的操作所花费的时间与受 HBM 限制的操作所花费的时间。\n- 受 HBM 带宽限制的时间（内核执行）：$T_{kernel} \\approx 0.0533 \\text{ s}$。\n- 受 PCIe 带宽限制的时间（数据传输）：$T_{transfer} = 0.5 \\text{ s}$。\n由于 $T_{transfer} \\gg T_{kernel}$，整个过程绝大部分时间被通过 PCIe 总线传输数据所占据。因此，主要瓶颈是 PCIe 链路，对应代码 $2$。\n\n最终结果如下：\n- 情景 (i) 的时间：$T_{(i)} \\approx 0.0533 \\text{ s}$。\n- 情景 (ii) 的时间：$T_{(ii)} \\approx 0.553 \\text{ s}$。\n- 情景 (ii) 的瓶颈代码：$2$ (PCIe)。",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.0533  0.553  2 \\end{pmatrix}}$$"
        }
    ]
}