{
    "hands_on_practices": [
        {
            "introduction": "To write efficient CFD codes, we must first understand the performance limitations of a single compute node. The Roofline model provides a powerful visual and analytical framework for this, helping us determine whether a computation is limited by memory bandwidth or processor speed. This first practice exercise  challenges you to apply the Roofline model from first principles to a sparse matrix-vector product, a core operation in many implicit solvers, to diagnose its performance characteristics.",
            "id": "3329287",
            "problem": "Consider a matrix-vector product in a Computational Fluid Dynamics (CFD) solver using the Compressed Sparse Row (CSR) data structure to apply a finite-volume discretization of the three-dimensional Poisson operator with a seven-point stencil. The sparse matrix has exactly $z=7$ nonzeros in every row. The CSR representation uses double-precision values and vectors (each stored in $8$ bytes) and $32$-bit integers for column indices and the row pointer array (each stored in $4$ bytes). The operation is the Sparse Matrix-Vector multiplication (SpMV), $y = A x$, executed on a Central Processing Unit (CPU) node that sustains a main-memory bandwidth of $200$ gigabytes per second and has a double-precision peak performance of $1.6 \\times 10^{12}$ floating-point operations per second. For bandwidth, interpret $1$ gigabyte as $10^{9}$ bytes.\n\nAssume an implementation with streamed access to the CSR arrays where each nonzero contributes one multiplication and one addition to the accumulation of a single output entry, the accumulation is kept entirely in registers, and the output entry $y_{i}$ is written once per row. Assume no reuse of the input vector entries $x_{j}$ across rows due to the combination of problem size and access pattern, and count the two row-pointer entries required per row to delimit the interval of nonzeros (one for the start and one for the end of the row).\n\nStarting from the definitions of CSR memory access patterns and the Roofline model, determine the total bytes moved and floating-point operations per row, compute the operational intensity of this SpMV, and then determine the Roofline performance bound. Express the final performance bound in gigaflops per second (GFLOP/s). Round your final answer to three significant figures.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- Operation: Sparse Matrix-Vector multiplication (SpMV), $y = A x$.\n- Solver context: Computational Fluid Dynamics (CFD).\n- Discretization: Finite-volume, three-dimensional Poisson operator, seven-point stencil.\n- Matrix structure: Sparse, with exactly $z=7$ nonzeros per row.\n- Data structure: Compressed Sparse Row (CSR).\n- Data types:\n    - Matrix values (`A_vals`) and vector entries ($x, y$): double-precision, $8$ bytes per element.\n    - Column indices (`A_cols`) and row pointers (`A_row_ptr`): $32$-bit integers, $4$ bytes per element.\n- Hardware specifications:\n    - Main-memory bandwidth ($\\beta$): $200$ gigabytes per second ($200 \\times 10^{9}$ bytes/s).\n    - Peak double-precision performance ($\\pi_{peak}$): $1.6 \\times 10^{12}$ floating-point operations per second (FLOP/s).\n- Implementation assumptions:\n    - Floating-point operations per nonzero: $1$ multiplication and $1$ addition.\n    - Accumulation of the output entry $y_i$ is performed entirely in registers.\n    - The output entry $y_i$ is written to memory once per row.\n    - Memory access is streamed with no cache reuse for input vector entries ($x_j$) across rows.\n    - Two row-pointer entries are read per row to delimit the nonzeros.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the established principles of high-performance computing, numerical linear algebra, and computational fluid dynamics. The CSR format, SpMV operation, and the Roofline performance model are standard topics in this field. The seven-point stencil for a 3D Poisson equation is a classic example.\n- **Well-Posed**: The problem is well-posed. It provides all necessary parameters and a set of clear, explicit assumptions (e.g., data sizes, operations per nonzero, memory access patterns) required to compute the operational intensity and apply the Roofline model to find a unique performance bound.\n- **Objective**: The problem is stated using precise, objective, and technical language, free from ambiguity or subjective claims.\n- **Flaw Check**:\n    1.  **Scientific or Factual Unsoundness**: None. The concepts and their application are standard. The provided numerical values for hardware performance are realistic for a modern CPU node.\n    2.  **Non-Formalizable or Irrelevant**: None. The problem is entirely formalizable and directly relevant to the performance analysis of numerical kernels in scientific computing.\n    3.  **Incomplete or Contradictory Setup**: None. The problem is self-contained and all necessary information is provided. The assumption of a constant number of nonzeros per row ($z=7$) is a simplification, but it is explicitly stated and not a contradiction.\n    4.  **Unrealistic or Infeasible**: None. The conditions are physically plausible and common in performance modeling exercises.\n    5.  **Ill-Posed or Poorly Structured**: None. The problem structure logically guides the user from first principles (FLOP and byte counts) to the final application of the Roofline model.\n    6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem requires a correct and non-trivial application of the Roofline model, a key concept in performance engineering.\n    7.  **Outside Scientific Verifiability**: None. The entire calculation is based on verifiable mathematical definitions and models.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Solution Derivation\nThe solution proceeds by first calculating the number of floating-point operations (FLOPs) and the total bytes transferred to/from main memory for a single row of the matrix-vector product. These values are then used to compute the operational intensity, which is a key parameter in the Roofline model. Finally, the Roofline model is applied to determine the performance bound.\n\n**1. Floating-Point Operations per Row**\nThe problem states that each nonzero element of the matrix contributes one multiplication and one addition. Since there are $z=7$ nonzeros per row, the total number of FLOPs per row is:\n$$\n\\text{FLOPs per row} = 2 \\times z = 2 \\times 7 = 14 \\text{ FLOPs}\n$$\n\n**2. Memory Traffic per Row**\nThe total memory traffic (bytes moved) is the sum of all data read from and written to main memory for the computation of one row. We account for the CSR data structure components and the input/output vectors.\n\n- **CSR `values` array**: For each of the $z=7$ nonzeros, one double-precision value is read. A double-precision float is $8$ bytes.\n$$\n\\text{Bytes}_{\\text{values}} = z \\times (\\text{size of double}) = 7 \\times 8 = 56 \\text{ bytes}\n$$\n- **CSR `col_ind` array**: For each of the $z=7$ nonzeros, one $32$-bit integer column index is read. A $32$-bit integer is $4$ bytes.\n$$\n\\text{Bytes}_{\\text{col\\_ind}} = z \\times (\\text{size of int32}) = 7 \\times 4 = 28 \\text{ bytes}\n$$\n- **CSR `row_ptr` array**: To identify the nonzeros for row $i$, the entries `row_ptr[i]` and `row_ptr[i+1]` are read. These are two $32$-bit integers.\n$$\n\\text{Bytes}_{\\text{row\\_ptr}} = 2 \\times (\\text{size of int32}) = 2 \\times 4 = 8 \\text{ bytes}\n$$\n- **Input vector `x`**: For each of the $z=7$ nonzeros $A_{ij}$, the corresponding element $x_j$ is read. The problem states to assume no cache reuse, so each of these $z=7$ accesses results in a read from main memory. The vector elements are double-precision.\n$$\n\\text{Bytes}_{x} = z \\times (\\text{size of double}) = 7 \\times 8 = 56 \\text{ bytes}\n$$\n- **Output vector `y`**: The single resulting element $y_i$ is written to memory once per row. This is a double-precision value.\n$$\n\\text{Bytes}_{y} = 1 \\times (\\text{size of double}) = 1 \\times 8 = 8 \\text{ bytes}\n$$\nThe total bytes moved per row is the sum of these components:\n$$\n\\text{Total Bytes per row} = \\text{Bytes}_{\\text{values}} + \\text{Bytes}_{\\text{col\\_ind}} + \\text{Bytes}_{\\text{row\\_ptr}} + \\text{Bytes}_{x} + \\text{Bytes}_{y}\n$$\n$$\n\\text{Total Bytes per row} = 56 + 28 + 8 + 56 + 8 = 156 \\text{ bytes}\n$$\n\n**3. Operational Intensity**\nOperational intensity ($I$) is the ratio of floating-point operations to bytes of data moved between the processor and main memory.\n$$\nI = \\frac{\\text{FLOPs per row}}{\\text{Total Bytes per row}} = \\frac{14}{156} \\text{ FLOPs/byte}\n$$\nThis simplifies to $I = \\frac{7}{78}$ FLOPs/byte.\n\n**4. Roofline Performance Bound**\nThe Roofline model gives the upper bound on performance, $P$, as the minimum of the peak computational performance, $\\pi_{peak}$, and the peak performance sustainable by the memory bandwidth, $P_{mem} = I \\times \\beta$.\n$$\nP = \\min(\\pi_{peak}, I \\times \\beta)\n$$\nThe given values are:\n- $\\pi_{peak} = 1.6 \\times 10^{12}$ FLOP/s\n- $\\beta = 200 \\times 10^9$ bytes/s\n\nFirst, we calculate the memory-bandwidth-limited performance:\n$$\nP_{mem} = I \\times \\beta = \\left(\\frac{14}{156}\\right) \\frac{\\text{FLOPs}}{\\text{byte}} \\times (200 \\times 10^9) \\frac{\\text{bytes}}{\\text{s}}\n$$\n$$\nP_{mem} = \\frac{14 \\times 200 \\times 10^9}{156} \\frac{\\text{FLOPs}}{\\text{s}} = \\frac{2800}{156} \\times 10^9 \\text{ FLOP/s}\n$$\n$$\nP_{mem} \\approx 17.9487179... \\times 10^9 \\text{ FLOP/s}\n$$\nThis is approximately $17.95$ GFLOP/s.\n\nNow, we compare this with the peak computational performance:\n$$\n\\pi_{peak} = 1.6 \\times 10^{12} \\text{ FLOP/s} = 1600 \\times 10^9 \\text{ FLOP/s} = 1600 \\text{ GFLOP/s}\n$$\nThe Roofline performance bound is:\n$$\nP = \\min(1600 \\times 10^9 \\text{ FLOP/s}, 17.9487... \\times 10^9 \\text{ FLOP/s})\n$$\nSince $17.9487... \\times 10^9 \\ll 1600 \\times 10^9$, the performance is limited by memory bandwidth.\n$$\nP \\approx 17.9487... \\times 10^9 \\text{ FLOP/s}\n$$\nThe problem asks for the result in GFLOP/s, rounded to three significant figures.\n$$\nP \\approx 17.9 \\text{ GFLOP/s}\n$$",
            "answer": "$$\n\\boxed{17.9}\n$$"
        },
        {
            "introduction": "Large-scale CFD simulations require distributing the computational domain across many nodes, which introduces communication overhead via the Message Passing Interface (MPI). Analyzing and minimizing this communication is key to achieving good parallel scalability. This exercise  asks you to model the communication costs associated with different boundary conditions, specifically quantifying the extra network traffic introduced by periodic domains, a common feature in simulations of turbulence and materials science.",
            "id": "3329344",
            "problem": "Consider a three-dimensional uniform Cartesian finite-volume Computational Fluid Dynamics (CFD) solver advancing a system of scalar conservation laws explicitly in time on a structured grid. The computational domain is decomposed into a logical Cartesian block topology with $P_x \\times P_y \\times P_z$ Message Passing Interface (MPI) ranks, where each rank owns a subdomain (block) of $n_x \\times n_y \\times n_z$ interior control volumes. The numerical scheme uses a compact stencil that requires $s \\in \\mathbb{N}$ layers of ghost cells (halo width) on each of the six faces to update all interior cells on each time step. The solver performs face-only halo exchanges: in each coordinate direction, exactly one nearest-neighbor exchange is posted per time step, sending $s$ contiguous planes of ghost data per face; there are no direct edge or corner messages.\n\nEach global coordinate direction may be either periodic or nonperiodic. Introduce indicator variables $\\chi_x, \\chi_y, \\chi_z \\in \\{0,1\\}$, where $\\chi_d = 1$ if direction $d \\in \\{x,y,z\\}$ is periodic and $\\chi_d = 0$ otherwise. In any nonperiodic direction, the low global face is an inflow boundary where ghost cells are set by prescribed Dirichlet data, and the high global face is an outflow boundary where ghost cells are set by a one-sided zero-gradient extrapolation; both inflow and outflow are applied locally without inter-rank communication. In any periodic direction, the global low-face ghost cells are populated by data wrapped from the global high-face neighbor and vice versa via MPI point-to-point communication in a periodic Cartesian communicator.\n\nDefine precisely, in words, the parallel treatment of ghost-cell population for inflow, outflow, and periodic faces under this decomposition and exchange model. Then, treating a single halo exchange send as the transmission of all $s$ ghost planes on one face from one rank to its neighbor in that direction, derive a closed-form analytic expression for the total additional number of scalar ghost-cell values sent across the network per time step that is induced solely by periodicity, relative to the nonperiodic case in which all physical boundary ghost cells are applied locally. Your answer must be a single closed-form expression in terms of $(P_x,P_y,P_z)$, $(n_x,n_y,n_z)$, $s$, and $(\\chi_x,\\chi_y,\\chi_z)$. Do not introduce any other parameters. Express the final answer as a pure count of scalar values (no physical units).",
            "solution": "The problem statement describes a standard parallel programming model for a finite-volume solver on a structured grid using domain decomposition. The components, including MPI, ghost cells (halos), and the handling of periodic and nonperiodic boundary conditions, are all standard concepts in high-performance scientific computing. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a rigorous derivation. It is therefore deemed valid.\n\nThe solution is presented in two parts as requested: first, a qualitative description of the ghost-cell population mechanisms, and second, a quantitative derivation of the additional communication cost due to periodicity.\n\n### Part 1: Parallel Treatment of Ghost-Cell Population\n\nThe population of the $s$ layers of ghost cells for each subdomain depends on whether a given face is an internal (inter-rank) boundary or a physical (global domain) boundary. For physical boundaries, the treatment further depends on the boundary condition type.\n\n1.  **Inflow Boundary Condition**: This condition is applied on a global low face if the corresponding direction $d \\in \\{x,y,z\\}$ is nonperiodic ($\\chi_d=0$). A rank whose subdomain has a face coincident with this global boundary must populate its $s$ external ghost-cell layers. The problem specifies that these ghost cells are set by prescribed Dirichlet data. This operation is performed \"locally,\" meaning each rank on this boundary face computes or looks up the required values for its own ghost cells independently, without any MPI communication with other ranks. The data is determined by the problem's physics and does not originate from another rank's computational domain.\n\n2.  **Outflow Boundary Condition**: This condition is applied on a global high face if the corresponding direction $d \\in \\{x,y,z\\}$ is nonperiodic ($\\chi_d=0$). A rank whose subdomain has a face at this global boundary populates its $s$ external ghost-cell layers using a one-sided zero-gradient extrapolation. This means the values in the ghost cells are set equal to the values in the nearest interior cells of that same rank. This operation is also performed \"locally\" and requires no inter-rank MPI communication.\n\n3.  **Periodic Boundary Condition**: This condition is applied if a direction $d \\in \\{x,y,z\\}$ is periodic ($\\chi_d=1$). In this case, the global domain \"wraps around\" on itself in that direction. The ranks on the global low face of the domain decomposition are considered neighbors to the ranks on the global high face. For instance, in the $x$-direction, a rank with MPI index $i_x=0$ has its low-$x$ face as a global boundary, and a rank with index $i_x=P_x-1$ has its high-$x$ face as a global boundary. To populate the ghost cells for the low-$x$ face of the rank at $i_x=0$, it must receive $s$ planes of data from the interior of its periodic neighbor at $i_x=P_x-1$. Conversely, to populate the ghost cells for the high-$x$ face of the rank at $i_x=P_x-1$, it must receive $s$ planes of data from its periodic neighbor at $i_x=0$. This data exchange is accomplished via MPI point-to-point communication, typically managed by an MPI Cartesian communicator which automatically identifies the periodic neighbors. Unlike the nonperiodic cases, this boundary treatment is inherently communication-bound.\n\n### Part 2: Derivation of Additional Communication Cost\n\nThe goal is to find the total *additional* number of scalar ghost-cell values sent across the network per time step, induced solely by periodicity. This is the difference in communication volume between the general case (with specified periodicities) and the baseline case where all three directions are nonperiodic ($\\chi_x=\\chi_y=\\chi_z=0$).\n\nIn the baseline nonperiodic case, communication only occurs between MPI ranks at interior boundaries. Physical boundaries are handled locally without MPI communication. When a direction is made periodic, the local boundary condition application at the two global faces in that direction is replaced by MPI communication between those faces. Therefore, the additional communication cost is precisely the communication volume across these newly-established periodic connections.\n\nWe calculate this additional communication volume for each Cartesian direction and sum the results.\n\n**Contribution from Periodicity in the $x$-direction:**\nThis contribution exists only if the $x$-direction is periodic, i.e., if $\\chi_x=1$. If $\\chi_x=0$, the contribution is $0$.\nWhen $\\chi_x=1$, MPI communication occurs between the ranks on the global low-$x$ face (MPI index $i_x=0$) and the global high-$x$ face (MPI index $i_x=P_x-1$).\nThe number of ranks on one such global face is the product of the number of ranks in the other two directions: $P_y P_z$.\nFor each of the $P_y P_z$ pairs of communicating ranks, two sends are performed per time step:\n1.  A rank at $i_x=0$ sends data to its periodic neighbor at $i_x=P_x-1$.\n2.  The rank at $i_x=P_x-1$ sends data to its periodic neighbor at $i_x=0$.\nThus, the total number of sends across the periodic seam in the $x$-direction is $2 P_y P_z$.\nEach send transmits the data for $s$ ghost planes. A plane perpendicular to the $x$-direction has dimensions corresponding to the number of interior cells in the other two directions, which is $n_y \\times n_z$.\nThe number of scalar values in a single send is therefore $s \\cdot n_y \\cdot n_z$.\nThe total additional number of scalar values sent due to periodicity in the $x$-direction, $N_{add,x}$, is the product of the number of sends and the values per send, multiplied by the indicator variable $\\chi_x$:\n$$N_{add,x} = \\chi_x \\cdot (2 P_y P_z) \\cdot (s n_y n_z) = 2s \\chi_x n_y n_z P_y P_z$$\n\n**Contribution from Periodicity in the $y$-direction:**\nBy symmetry, the logic is identical. The contribution is non-zero only if $\\chi_y=1$. The global faces are $xz$-planes of ranks, so there are $P_x P_z$ ranks per face. The data planes are perpendicular to the $y$-direction, with size $n_x \\times n_z$.\nThe number of sends is $2 P_x P_z$.\nThe number of values per send is $s n_x n_z$.\nThe total additional number of values sent is:\n$$N_{add,y} = \\chi_y \\cdot (2 P_x P_z) \\cdot (s n_x n_z) = 2s \\chi_y n_x n_z P_x P_z$$\n\n**Contribution from Periodicity in the $z$-direction:**\nSimilarly, for the $z$-direction, the contribution requires $\\chi_z=1$. The global faces are $xy$-planes of ranks ($P_x P_y$ ranks per face). Data planes are perpendicular to the $z$-direction, with size $n_x \\times n_y$.\nThe number of sends is $2 P_x P_y$.\nThe number of values per send is $s n_x n_y$.\nThe total additional number of values sent is:\n$$N_{add,z} = \\chi_z \\cdot (2 P_x P_y) \\cdot (s n_x n_y) = 2s \\chi_z n_x n_y P_x P_y$$\n\n**Total Additional Communication Cost:**\nThe total additional number of scalar values sent per time step, $N_{add}$, is the sum of the contributions from each direction.\n$$N_{add} = N_{add,x} + N_{add,y} + N_{add,z}$$\n$$N_{add} = 2s \\chi_x n_y n_z P_y P_z + 2s \\chi_y n_x n_z P_x P_z + 2s \\chi_z n_x n_y P_x P_y$$\nFactoring out the common term $2s$ yields the final closed-form expression:\n$$N_{add} = 2s (\\chi_x n_y n_z P_y P_z + \\chi_y n_x n_z P_x P_z + \\chi_z n_x n_y P_x P_y)$$\n\nThis expression correctly counts the total number of scalar values transmitted across all MPI channels that exist only due to periodic boundary conditions, relative to a baseline where all global boundaries are handled locally.",
            "answer": "$$\n\\boxed{2s(\\chi_x n_y n_z P_y P_z + \\chi_y n_x n_z P_x P_z + \\chi_z n_x n_y P_x P_y)}\n$$"
        },
        {
            "introduction": "Modern HPC systems heavily rely on GPUs, whose performance hinges on effectively using their complex memory hierarchy and massive parallelism. For stencil-based kernels common in CFD, shared memory tiling is a critical optimization technique to reduce expensive global memory traffic. In this final practice , you will tackle the real-world problem of selecting an optimal tile size to maximize hardware occupancy, balancing constraints from registers, threads, and available shared memory.",
            "id": "3329340",
            "problem": "You are asked to formalize and implement an occupancy-driven tile selection strategy for a three-dimensional stencil arising in Computational Fluid Dynamics (CFD) on a Graphics Processing Unit (GPU) using shared memory tiling. The goal is to choose integer tile dimensions $(t_x,t_y,t_z)$ that maximize Streaming Multiprocessor (SM) occupancy under resource constraints due to threads, registers, and shared memory. Your program must search over tile shapes, evaluate resource usage, and select the optimal tile according to a well-defined objective and explicit tie-breaking rules.\n\nThe physical and algorithmic context is as follows. Consider a three-dimensional Navierâ€“Stokes discretization (finite volume or finite difference) with a power-of-two aligned warp size. A tile held in shared memory must include a halo of width $h$ along each dimension in order to compute all interior stencil points. Let the interior (threaded) work per block be $N_{\\mathrm{int}} = t_x t_y t_z$. The block assigns one thread per interior cell. The shared memory footprint per block is\n$$\nS_{\\mathrm{block}} = (t_x + 2h)(t_y + 2h)(t_z + 2h) \\, F \\, s,\n$$\nwhere $F$ is the number of per-cell state variables and $s$ is the bytes per variable. The register usage per thread is $r$ registers, so registers per block is $R_{\\mathrm{block}} = r \\, N_{\\mathrm{int}}$.\n\nYou will use the following fundamental definitions and well-tested facts as the basis of your derivation and implementation:\n- A GPU executes threads in groups of warps of size $W$ threads. The number of warps per block is $W_b = \\lceil N_{\\mathrm{int}}/W \\rceil$.\n- A Streaming Multiprocessor (SM) has resource limits: maximum threads per block $T_{\\mathrm{block}}^{\\max}$, maximum blocks per SM $B_{\\mathrm{SM}}^{\\max}$, maximum threads per SM $T_{\\mathrm{SM}}^{\\max}$, registers per SM $R_{\\mathrm{SM}}$, shared memory per SM $M_{\\mathrm{sh}}$, and maximum shared memory per block $M_{\\mathrm{sh}}^{\\mathrm{block}}$. The maximum warps per SM is $W_{\\mathrm{SM}}^{\\max} = T_{\\mathrm{SM}}^{\\max}/W$.\n- The number of resident blocks on an SM is limited by each resource:\n$$\nb_T = \\left\\lfloor \\frac{T_{\\mathrm{SM}}^{\\max}}{N_{\\mathrm{int}}} \\right\\rfloor, \\quad\nb_R = \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{R_{\\mathrm{block}}} \\right\\rfloor, \\quad\nb_S = \\left\\lfloor \\frac{M_{\\mathrm{sh}}}{S_{\\mathrm{block}}} \\right\\rfloor, \\quad\nb_{\\max} = B_{\\mathrm{SM}}^{\\max}.\n$$\nThe active blocks per SM are $B = \\min(b_T,b_R,b_S,b_{\\max})$, provided the block itself respects $N_{\\mathrm{int}} \\le T_{\\mathrm{block}}^{\\max}$ and $S_{\\mathrm{block}} \\le M_{\\mathrm{sh}}^{\\mathrm{block}}$.\n- The SM occupancy is the fraction of active warps over the maximum warps:\n$$\n\\phi = \\frac{\\min\\left(W_{\\mathrm{SM}}^{\\max}, \\, B \\, W_b \\right)}{W_{\\mathrm{SM}}^{\\max}}.\n$$\n\nYour program must select $(t_x,t_y,t_z)$ by maximizing $\\phi$ subject to feasibility constraints. In addition, to emphasize shared-memory tiling efficacy for CFD stencils, impose the following practical constraints:\n- $N_{\\mathrm{int}} = t_x t_y t_z$ must be a multiple of $W$ to avoid partially filled warps.\n- $t_x, t_y, t_z$ are integers in the inclusive range $\\{4,5,6,\\dots,32\\}$.\n- Feasibility requires $N_{\\mathrm{int}} \\le T_{\\mathrm{block}}^{\\max}$ and $S_{\\mathrm{block}} \\le M_{\\mathrm{sh}}^{\\mathrm{block}}$.\n- Assume $h \\ge 1$ unless otherwise specified in the test suite.\n\nTo break ties between multiple $(t_x,t_y,t_z)$ with identical maximal $\\phi$, use the following order of preference:\n1. Maximize the shared-memory reuse ratio\n$$\n\\rho = \\frac{N_{\\mathrm{int}}}{(t_x + 2h)(t_y + 2h)(t_z + 2h)}.\n$$\n2. Maximize $N_{\\mathrm{int}}$.\n3. Choose the lexicographically smallest $(t_x,t_y,t_z)$.\n\nYour program must implement a complete search over all integer $(t_x,t_y,t_z)$ in the specified range, apply the constraints, compute $\\phi$ and $\\rho$ for each feasible block, and select the optimal tile according to the rules above. For each test case, the output should be the selected $(t_x,t_y,t_z)$ and the corresponding occupancy $\\phi$ as a real number between $0$ and $1$.\n\nThere are no physical units in this problem; all quantities are dimensionless or measured in bytes. Angles are not involved. Percentages must not be used; any fractional quantity should be expressed as a decimal number.\n\nTest Suite. Use the following test cases. Each case is a tuple containing $(W, T_{\\mathrm{block}}^{\\max}, B_{\\mathrm{SM}}^{\\max}, T_{\\mathrm{SM}}^{\\max}, R_{\\mathrm{SM}}, M_{\\mathrm{sh}}, M_{\\mathrm{sh}}^{\\mathrm{block}}, r, h, F, s)$ where all numbers are integers:\n- Case $1$: $(32, 1024, 32, 2048, 65536, 102400, 98304, 64, 1, 5, 8)$.\n- Case $2$: $(32, 1024, 32, 2048, 65536, 49152, 49152, 32, 1, 7, 8)$.\n- Case $3$: $(32, 1024, 32, 2048, 65536, 102400, 98304, 128, 1, 3, 8)$.\n- Case $4$: $(32, 512, 16, 1536, 32768, 65536, 49152, 40, 1, 4, 4)$.\n- Case $5$: $(32, 1024, 32, 2048, 65536, 102400, 98304, 64, 2, 9, 8)$.\n\nFinal Output Format. Your program should produce a single line of output containing a list of results, one per test case, in order. Each result is itself a list of four numbers $[t_x, t_y, t_z, \\phi]$. The final line must be a single Python-style list with comma-separated entries and no additional text, for example:\n$[ [t_{x,1}, t_{y,1}, t_{z,1}, \\phi_1], [t_{x,2}, t_{y,2}, t_{z,2}, \\phi_2], \\dots ]$.\n\nYour solution must be a complete, runnable program that implements the above logic and prints the required single-line output exactly as specified.",
            "solution": "The problem is valid. It presents a well-posed optimization task grounded in the established principles of high-performance computing and GPU architecture. The problem is self-contained, with all necessary constants, formulas, and constraints clearly defined. The objective function, search space, and multi-level tie-breaking rules are specified unambiguously, ensuring a unique solution exists and can be found via a deterministic algorithm. The parameters provided in the test suite are realistic for modern GPU hardware. There are no scientific inaccuracies, contradictions, or ill-posed elements.\n\nThe objective is to determine the optimal integer tile dimensions $(t_x, t_y, t_z)$ for a three-dimensional stencil computation on a GPU. The optimization goal is to maximize the Streaming Multiprocessor (SM) occupancy, $\\phi$, subject to a series of hardware and algorithmic constraints. The problem requires a complete search over the specified parameter space for $(t_x, t_y, t_z)$.\n\nThe methodology to solve this problem is a systematic search and evaluation process for each test case.\n\nFirst, we define the search space. The tile dimensions $t_x$, $t_y$, and $t_z$ are integers within the inclusive range $\\{4, 5, \\dots, 32\\}$. Our algorithm will perform an exhaustive search by iterating through all possible combinations of $(t_x, t_y, t_z)$ in this range.\n\nFor each candidate tile configuration $(t_x, t_y, t_z)$, we first evaluate its feasibility based on a set of given constraints. A tile is considered feasible only if it satisfies all of the following conditions:\n$1$. The number of threads in the block, $N_{\\mathrm{int}} = t_x t_y t_z$, must be a multiple of the warp size $W$.\n$2$. The number of threads per block must not exceed the hardware limit: $N_{\\mathrm{int}} \\le T_{\\mathrm{block}}^{\\max}$.\n$3$. The shared memory required for the tile, including a halo of width $h$, must not exceed the maximum allowed per block. The shared memory footprint is given by $S_{\\mathrm{block}} = (t_x + 2h)(t_y + 2h)(t_z + 2h) \\cdot F \\cdot s$, where $F$ is the number of state variables per cell and $s$ is the size in bytes of each variable. The constraint is $S_{\\mathrm{block}} \\le M_{\\mathrm{sh}}^{\\mathrm{block}}$.\n\nIf a tile configuration $(t_x, t_y, t_z)$ fails any of these checks, it is discarded, and the search proceeds to the next configuration.\n\nIf a tile is feasible, we proceed to calculate its performance metrics. The primary metric is the SM occupancy, $\\phi$. Its calculation depends on the number of blocks that can reside concurrently on a single SM. This number, denoted by $B$, is limited by the SM's resources: total threads, total registers, and total shared memory.\nThe number of registers used by a block is $R_{\\mathrm{block}} = r \\cdot N_{\\mathrm{int}}$, where $r$ is the register count per thread.\nThe maximum number of concurrent blocks, as limited by each resource, is calculated as:\n-   By threads: $b_T = \\lfloor T_{\\mathrm{SM}}^{\\max} / N_{\\mathrm{int}} \\rfloor$.\n-   By registers: $b_R = \\lfloor R_{\\mathrm{SM}} / R_{\\mathrm{block}} \\rfloor$, assuming $R_{\\mathrm{block}} > 0$.\n-   By shared memory: $b_S = \\lfloor M_{\\mathrm{sh}} / S_{\\mathrm{block}} \\rfloor$, assuming $S_{\\mathrm{block}} > 0$.\n\nThe actual number of active blocks per SM is the minimum of these values, further capped by the architectural limit $B_{\\mathrm{SM}}^{\\max}$:\n$$B = \\min(b_T, b_R, b_S, B_{\\mathrm{SM}}^{\\max})$$\nThe number of warps in a block is $W_b = \\lceil N_{\\mathrm{int}} / W \\rceil$. Since $N_{\\mathrm{int}}$ is constrained to be a multiple of $W$, this simplifies to $W_b = N_{\\mathrm{int}} / W$.\nThe maximum number of warps an SM can support is $W_{\\mathrm{SM}}^{\\max} = T_{\\mathrm{SM}}^{\\max} / W$.\nThe total number of active warps on the SM is $B \\cdot W_b$, capped by $W_{\\mathrm{SM}}^{\\max}$. The occupancy $\\phi$ is the ratio of active warps to the maximum possible warps:\n$$\\phi = \\frac{\\min(W_{\\mathrm{SM}}^{\\max}, B \\cdot W_b)}{W_{\\mathrm{SM}}^{\\max}}$$\n\nWith $\\phi$ calculated, we compare the current tile configuration to the best one found so far. The comparison follows a strict-lexicographical tie-breaking procedure:\n$1$. The primary objective is to maximize the occupancy $\\phi$.\n$2$. If two configurations yield the same $\\phi$, we select the one with the higher shared-memory reuse ratio, $\\rho = \\frac{N_{\\mathrm{int}}}{(t_x + 2h)(t_y + 2h)(t_z + 2h)}$.\n$3$. If a tie persists, we select the configuration with the larger number of threads, $N_{\\mathrm{int}}$.\n$4$. As a final tie-breaker, we choose the configuration $(t_x, t_y, t_z)$ that is lexicographically smallest.\n\nTo implement this multi-level optimization efficiently, we can construct a scoring tuple for each feasible tile: $(\\phi, \\rho, N_{\\mathrm{int}}, -t_x, -t_y, -t_z)$. We seek to find the tile that maximizes this score tuple, where standard tuple comparison in Python will correctly implement the specified ordered criteria. The negation of $t_x, t_y, t_z$ transforms the minimization of the lexicographical tuple $(t_x, t_y, t_z)$ into a maximization problem, fitting neatly into the single-objective framework.\n\nThe algorithm proceeds by initializing a `best_score` tuple with placeholder values lower than any possible outcome (e.g., $(-1.0, -1.0, -1, 0, 0, 0)$). As the search iterates through all $29 \\times 29 \\times 29$ tile configurations, this `best_score` and the corresponding optimal tile dimensions are updated whenever a better configuration is found. After the search is complete, the optimal $(t_x, t_y, t_z)$ and its corresponding occupancy $\\phi$ are recorded for the test case. This process is repeated for all cases in the test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GPU tile selection problem for a series of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # (W, T_block_max, B_SM_max, T_SM_max, R_SM, M_sh, M_sh_block, r, h, F, s)\n        (32, 1024, 32, 2048, 65536, 102400, 98304, 64, 1, 5, 8),\n        (32, 1024, 32, 2048, 65536, 49152, 49152, 32, 1, 7, 8),\n        (32, 1024, 32, 2048, 65536, 102400, 98304, 128, 1, 3, 8),\n        (32, 512, 16, 1536, 32768, 65536, 49152, 40, 1, 4, 4),\n        (32, 1024, 32, 2048, 65536, 102400, 98304, 64, 2, 9, 8),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        W, T_block_max, B_SM_max, T_SM_max, R_SM, M_sh, M_sh_block, r, h, F, s = case\n        \n        best_tile_dims = None\n        # Score is a tuple: (phi, rho, N_int, -tx, -ty, -tz) for maximization\n        best_score = (-1.0, -1.0, -1, 0, 0, 0)\n\n        tile_dim_range = range(4, 33)\n\n        for tx in tile_dim_range:\n            for ty in tile_dim_range:\n                for tz in tile_dim_range:\n                    N_int = tx * ty * tz\n\n                    # Constraint 1: N_int must be a multiple of W\n                    if N_int % W != 0:\n                        continue\n                    \n                    # Constraint 2: N_int = T_block_max\n                    if N_int > T_block_max:\n                        continue\n\n                    S_block_vol = (tx + 2 * h) * (ty + 2 * h) * (tz + 2 * h)\n                    S_block = S_block_vol * F * s\n\n                    # Constraint 3: S_block = M_sh_block\n                    if S_block > M_sh_block:\n                        continue\n\n                    # Tile is feasible, calculate performance metrics\n                    \n                    # Registers per block\n                    R_block = r * N_int\n                    \n                    # Blocks per SM limitations\n                    # Note: Denominators cannot be zero due to problem constraints\n                    b_T = T_SM_max // N_int if N_int > 0 else B_SM_max\n                    b_R = R_SM // R_block if R_block > 0 else B_SM_max\n                    b_S = M_sh // S_block if S_block > 0 else B_SM_max\n\n                    # Active blocks per SM\n                    B = min(b_T, b_R, b_S, B_SM_max)\n\n                    # Warps per block\n                    W_b = N_int // W\n                    \n                    # Max warps per SM\n                    W_SM_max = T_SM_max // W\n\n                    # Occupancy\n                    active_warps = B * W_b\n                    phi = min(W_SM_max, active_warps) / W_SM_max\n\n                    # Shared memory reuse ratio\n                    rho = N_int / S_block_vol\n\n                    # Compare with best tile found so far\n                    current_score = (phi, rho, N_int, -tx, -ty, -tz)\n\n                    if current_score > best_score:\n                        best_score = current_score\n                        best_tile_dims = (tx, ty, tz)\n\n        # Append result for the current case\n        final_phi = best_score[0]\n        result_list = [*best_tile_dims, final_phi]\n        all_results.append(result_list)\n        \n    # Format the final output string as specified\n    formatted_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]\n    final_output_str = f\"[{','.join(formatted_strings)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}