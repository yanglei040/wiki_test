{
    "hands_on_practices": [
        {
            "introduction": "Understanding whether a computational kernel is limited by the processor's calculation speed or by the memory system's data transfer rate is the first step in performance optimization. This practice introduces the Roofline model, a foundational tool for diagnosing performance bottlenecks by calculating a kernel's operational intensity—the ratio of arithmetic operations to data movement. By applying this model to a sparse matrix-vector product , a key operation in many CFD solvers, you will learn to identify whether the algorithm is compute-bound or memory-bound on a given hardware platform.",
            "id": "3329287",
            "problem": "Consider a matrix-vector product in a Computational Fluid Dynamics (CFD) solver using the Compressed Sparse Row (CSR) data structure to apply a finite-volume discretization of the three-dimensional Poisson operator with a seven-point stencil. The sparse matrix has exactly $z=7$ nonzeros in every row. The CSR representation uses double-precision values and vectors (each stored in $8$ bytes) and $32$-bit integers for column indices and the row pointer array (each stored in $4$ bytes). The operation is the Sparse Matrix-Vector multiplication (SpMV), $y = A x$, executed on a Central Processing Unit (CPU) node that sustains a main-memory bandwidth of $200$ gigabytes per second and has a double-precision peak performance of $1.6 \\times 10^{12}$ floating-point operations per second. For bandwidth, interpret $1$ gigabyte as $10^{9}$ bytes.\n\nAssume an implementation with streamed access to the CSR arrays where each nonzero contributes one multiplication and one addition to the accumulation of a single output entry, the accumulation is kept entirely in registers, and the output entry $y_{i}$ is written once per row. Assume no reuse of the input vector entries $x_{j}$ across rows due to the combination of problem size and access pattern, and count the two row-pointer entries required per row to delimit the interval of nonzeros (one for the start and one for the end of the row).\n\nStarting from the definitions of CSR memory access patterns and the Roofline model, determine the total bytes moved and floating-point operations per row, compute the operational intensity of this SpMV, and then determine the Roofline performance bound. Express the final performance bound in gigaflops per second (GFLOP/s). Round your final answer to three significant figures.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- Operation: Sparse Matrix-Vector multiplication (SpMV), $y = A x$.\n- Solver context: Computational Fluid Dynamics (CFD).\n- Discretization: Finite-volume, three-dimensional Poisson operator, seven-point stencil.\n- Matrix structure: Sparse, with exactly $z=7$ nonzeros per row.\n- Data structure: Compressed Sparse Row (CSR).\n- Data types:\n    - Matrix values (`A_vals`) and vector entries ($x, y$): double-precision, $8$ bytes per element.\n    - Column indices (`A_cols`) and row pointers (`A_row_ptr`): $32$-bit integers, $4$ bytes per element.\n- Hardware specifications:\n    - Main-memory bandwidth ($\\beta$): $200$ gigabytes per second ($200 \\times 10^{9}$ bytes/s).\n    - Peak double-precision performance ($\\pi_{peak}$): $1.6 \\times 10^{12}$ floating-point operations per second (FLOP/s).\n- Implementation assumptions:\n    - Floating-point operations per nonzero: $1$ multiplication and $1$ addition.\n    - Accumulation of the output entry $y_i$ is performed entirely in registers.\n    - The output entry $y_i$ is written to memory once per row.\n    - Memory access is streamed with no cache reuse for input vector entries ($x_j$) across rows.\n    - Two row-pointer entries are read per row to delimit the nonzeros.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the established principles of high-performance computing, numerical linear algebra, and computational fluid dynamics. The CSR format, SpMV operation, and the Roofline performance model are standard topics in this field. The seven-point stencil for a 3D Poisson equation is a classic example.\n- **Well-Posed**: The problem is well-posed. It provides all necessary parameters and a set of clear, explicit assumptions (e.g., data sizes, operations per nonzero, memory access patterns) required to compute the operational intensity and apply the Roofline model to find a unique performance bound.\n- **Objective**: The problem is stated using precise, objective, and technical language, free from ambiguity or subjective claims.\n- **Flaw Check**:\n    1.  **Scientific or Factual Unsoundness**: None. The concepts and their application are standard. The provided numerical values for hardware performance are realistic for a modern CPU node.\n    2.  **Non-Formalizable or Irrelevant**: None. The problem is entirely formalizable and directly relevant to the performance analysis of numerical kernels in scientific computing.\n    3.  **Incomplete or Contradictory Setup**: None. The problem is self-contained and all necessary information is provided. The assumption of a constant number of nonzeros per row ($z=7$) is a simplification, but it is explicitly stated and not a contradiction.\n    4.  **Unrealistic or Infeasible**: None. The conditions are physically plausible and common in performance modeling exercises.\n    5.  **Ill-Posed or Poorly Structured**: None. The problem structure logically guides the user from first principles (FLOP and byte counts) to the final application of the Roofline model.\n    6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem requires a correct and non-trivial application of the Roofline model, a key concept in performance engineering.\n    7.  **Outside Scientific Verifiability**: None. The entire calculation is based on verifiable mathematical definitions and models.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Solution Derivation\nThe solution proceeds by first calculating the number of floating-point operations (FLOPs) and the total bytes transferred to/from main memory for a single row of the matrix-vector product. These values are then used to compute the operational intensity, which is a key parameter in the Roofline model. Finally, the Roofline model is applied to determine the performance bound.\n\n**1. Floating-Point Operations per Row**\nThe problem states that each nonzero element of the matrix contributes one multiplication and one addition. Since there are $z=7$ nonzeros per row, the total number of FLOPs per row is:\n$$\n\\text{FLOPs per row} = 2 \\times z = 2 \\times 7 = 14 \\text{ FLOPs}\n$$\n\n**2. Memory Traffic per Row**\nThe total memory traffic (bytes moved) is the sum of all data read from and written to main memory for the computation of one row. We account for the CSR data structure components and the input/output vectors.\n\n- **CSR `values` array**: For each of the $z=7$ nonzeros, one double-precision value is read. A double-precision float is $8$ bytes.\n$$\n\\text{Bytes}_{\\text{values}} = z \\times (\\text{size of double}) = 7 \\times 8 = 56 \\text{ bytes}\n$$\n- **CSR `col_ind` array**: For each of the $z=7$ nonzeros, one $32$-bit integer column index is read. A $32$-bit integer is $4$ bytes.\n$$\n\\text{Bytes}_{\\text{col\\_ind}} = z \\times (\\text{size of int32}) = 7 \\times 4 = 28 \\text{ bytes}\n$$\n- **CSR `row_ptr` array**: To identify the nonzeros for row $i$, the entries `row_ptr[i]` and `row_ptr[i+1]` are read. These are two $32$-bit integers.\n$$\n\\text{Bytes}_{\\text{row\\_ptr}} = 2 \\times (\\text{size of int32}) = 2 \\times 4 = 8 \\text{ bytes}\n$$\n- **Input vector `x`**: For each of the $z=7$ nonzeros $A_{ij}$, the corresponding element $x_j$ is read. The problem states to assume no cache reuse, so each of these $z=7$ accesses results in a read from main memory. The vector elements are double-precision.\n$$\n\\text{Bytes}_{x} = z \\times (\\text{size of double}) = 7 \\times 8 = 56 \\text{ bytes}\n$$\n- **Output vector `y`**: The single resulting element $y_i$ is written to memory once per row. This is a double-precision value.\n$$\n\\text{Bytes}_{y} = 1 \\times (\\text{size of double}) = 1 \\times 8 = 8 \\text{ bytes}\n$$\nThe total bytes moved per row is the sum of these components:\n$$\n\\text{Total Bytes per row} = \\text{Bytes}_{\\text{values}} + \\text{Bytes}_{\\text{col\\_ind}} + \\text{Bytes}_{\\text{row\\_ptr}} + \\text{Bytes}_{x} + \\text{Bytes}_{y}\n$$\n$$\n\\text{Total Bytes per row} = 56 + 28 + 8 + 56 + 8 = 156 \\text{ bytes}\n$$\n\n**3. Operational Intensity**\nOperational intensity ($I$) is the ratio of floating-point operations to bytes of data moved between the processor and main memory.\n$$\nI = \\frac{\\text{FLOPs per row}}{\\text{Total Bytes per row}} = \\frac{14}{156} \\text{ FLOPs/byte}\n$$\nThis simplifies to $I = \\frac{7}{78}$ FLOPs/byte.\n\n**4. Roofline Performance Bound**\nThe Roofline model gives the upper bound on performance, $P$, as the minimum of the peak computational performance, $\\pi_{peak}$, and the peak performance sustainable by the memory bandwidth, $P_{mem} = I \\times \\beta$.\n$$\nP = \\min(\\pi_{peak}, I \\times \\beta)\n$$\nThe given values are:\n- $\\pi_{peak} = 1.6 \\times 10^{12}$ FLOP/s\n- $\\beta = 200 \\times 10^9$ bytes/s\n\nFirst, we calculate the memory-bandwidth-limited performance:\n$$\nP_{mem} = I \\times \\beta = \\left(\\frac{14}{156}\\right) \\frac{\\text{FLOPs}}{\\text{byte}} \\times (200 \\times 10^9) \\frac{\\text{bytes}}{\\text{s}}\n$$\n$$\nP_{mem} = \\frac{14 \\times 200 \\times 10^9}{156} \\frac{\\text{FLOPs}}{\\text{s}} = \\frac{2800}{156} \\times 10^9 \\text{ FLOP/s}\n$$\n$$\nP_{mem} \\approx 17.9487179... \\times 10^9 \\text{ FLOP/s}\n$$\nThis is approximately $17.95$ GFLOP/s.\n\nNow, we compare this with the peak computational performance:\n$$\n\\pi_{peak} = 1.6 \\times 10^{12} \\text{ FLOP/s} = 1600 \\times 10^9 \\text{ FLOP/s} = 1600 \\text{ GFLOP/s}\n$$\nThe Roofline performance bound is:\n$$\nP = \\min(1600 \\times 10^9 \\text{ FLOP/s}, 17.9487... \\times 10^9 \\text{ FLOP/s})\n$$\nSince $17.9487... \\times 10^9 \\ll 1600 \\times 10^9$, the performance is limited by memory bandwidth.\n$$\nP \\approx 17.9487... \\times 10^9 \\text{ FLOP/s}\n$$\nThe problem asks for the result in GFLOP/s, rounded to three significant figures.\n$$\nP \\approx 17.9 \\text{ GFLOP/s}\n$$",
            "answer": "$$\n\\boxed{17.9}\n$$"
        },
        {
            "introduction": "Modern scientific software must achieve high performance on diverse architectures, from multi-core CPUs to GPUs. Performance portability frameworks like Kokkos address this challenge by abstracting hardware details, but effective use requires understanding how these abstractions map to machine performance. This exercise guides you through building a predictive performance model for a stencil kernel , exploring how choices in data layout and parallel execution policies affect memory access patterns, cache utilization, and ultimately, runtime on different processors.",
            "id": "3329257",
            "problem": "You are to construct a performance prediction program for a three-dimensional finite-difference stencil kernel representative of Computational Fluid Dynamics (CFD), mapped through Kokkos abstractions. The focus is on how explicit Kokkos execution spaces, memory spaces, data layouts, and parallel execution policies influence memory access patterns and, consequently, performance on Central Processing Units (CPU) versus Graphics Processing Units (GPU). Your program must apply a principled roofline-based model to predict runtime per time-step for several prespecified test configurations.\n\nUse the following base principles and definitions:\n\n- Begin from the roofline performance model, which states that for a given kernel with arithmetic intensity, the attainable performance is bounded by computational throughput and memory bandwidth limits. Let the total number of lattice updates be $N$, the floating-point operations per update be $F$ (in floating-point operations), the peak device floating-point rate be $R$ (in floating-point operations per second), the total bytes transferred per update be $Q$ (in bytes), and the effective memory bandwidth be $B$ (in bytes per second). Then the time to solution per time-step is modeled as\n  $$ t_{\\text{total}} = t_0 + \\max\\left(\\frac{N \\cdot F}{R_{\\text{eff}}}, \\frac{N \\cdot Q}{B_{\\text{eff}}}\\right), $$\n  where $t_0$ is an execution overhead, $R_{\\text{eff}} = u_R \\cdot R$ is the achieved compute rate with utilization factor $u_R \\in (0,1]$, and $B_{\\text{eff}} = u_B \\cdot c \\cdot B$ is the achieved bandwidth including a utilization factor $u_B \\in (0,1]$ and a coalescing factor $c \\in (0,1]$ that captures memory access efficiency due to data layout and thread mapping. This definition is grounded in the well-tested roofline model.\n\n- For a three-dimensional stencil update with $S$ points (including the center), in double precision with scalar size $b$ bytes, model the data traffic per lattice update as\n  $$ Q = b \\cdot \\left(1 + w_{\\text{alloc}} + 1 + g \\cdot \\rho \\cdot (S - 1)\\right), $$\n  where the terms are, respectively, one store, an optional write-allocate read $w_{\\text{alloc}} \\in \\{0,1\\}$ (relevant on many CPUs), one load of the center, and a fraction $g \\cdot \\rho$ of the neighbor loads that must come from global memory. Here $\\rho \\in (0,1]$ quantifies the fraction of neighbors that miss caches due to spatial-temporal reuse and policy-induced locality, and $g \\in (0,1]$ quantifies the fraction of neighbor loads that go to global memory when on-chip scratch (for example, shared memory) is used; take $g = 1$ when no shared-memory tiling is applied. This captures well-tested caching and coalescing effects.\n\n- Arithmetic intensity is then\n  $$ I = \\frac{F}{Q} \\quad \\text{(in floating-point operations per byte)}, $$\n  and the kernel is memory-bound if $\\frac{N \\cdot Q}{B_{\\text{eff}}} \\ge \\frac{N \\cdot F}{R_{\\text{eff}}}$; otherwise compute-bound. This is a fundamental implication of the roofline model.\n\nKokkos-specific mappings to parameters:\n\n- Execution space and memory space:\n  - CPU: Kokkos Open Multi-Processing (OpenMP) execution space with HostSpace memory.\n  - GPU: Kokkos Compute Unified Device Architecture (CUDA) execution space with CudaSpace memory.\n\n- Layouts:\n  - LayoutRight implies the $i$-index is the fastest varying dimension in memory.\n  - LayoutLeft implies the $k$-index is the fastest varying dimension in memory.\n\n- Policies:\n  - RangePolicy one-dimensional mapping may lead to contiguous or strided device memory accesses depending on how indices are linearized.\n  - MDRangePolicy tiled mapping can improve cache reuse on CPU.\n  - TeamPolicy with scratch memory (for example, CUDA shared memory) can reduce global memory traffic, modeled by $g < 1$.\n\nYour program must implement the above model exactly as follows:\n\n- Inputs are hard-coded by you as a test suite of cases. For each case, the parameters are:\n  - Grid sizes $(n_x,n_y,n_z)$, stencil size $S$, floating-point operations per update $F$, scalar size $b$ (in bytes).\n  - Write-allocate flag $w_{\\text{alloc}} \\in \\{0,1\\}$, reuse factor $\\rho \\in (0,1]$, shared memory global fraction $g \\in (0,1]$.\n  - Device peaks: $R$ (floating-point operations per second), $B$ (bytes per second).\n  - Utilization factors: $u_R \\in (0,1]$, $u_B \\in (0,1]$, coalescing $c \\in (0,1]$.\n  - Overhead $t_0$ (in seconds).\n- Compute $N = n_x \\cdot n_y \\cdot n_z$, $Q$, $R_{\\text{eff}}$, $B_{\\text{eff}}$, then $t_{\\text{total}}$ per case.\n- All outputs must be in seconds.\n\nTest suite to implement:\n\nUse double precision with $b = 8$ bytes and a seven-point stencil with $S = 7$ and a per-update floating-point operation count $F = 13$ for all cases. The grid dimensions and device parameters differ per case.\n\n- Case $1$ (CPU, HostSpace, LayoutRight, MDRangePolicy tiled):\n  - $(n_x,n_y,n_z) = (512, 512, 128)$.\n  - $w_{\\text{alloc}} = 1$, $\\rho = 0.5$, $g = 1$.\n  - $R = 1.0 \\times 10^{12}$, $u_R = 0.6$.\n  - $B = 1.5 \\times 10^{11}$, $u_B = 0.8$, $c = 1.0$.\n  - $t_0 = 5 \\times 10^{-6}$.\n\n- Case $2$ (GPU, CudaSpace, LayoutRight, RangePolicy contiguous-$i$ mapping):\n  - $(n_x,n_y,n_z) = (512, 512, 128)$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 1.0$.\n  - $R = 1.9 \\times 10^{13}$, $u_R = 0.8$.\n  - $B = 9.0 \\times 10^{11}$, $u_B = 1.0$, $c = 0.95$.\n  - $t_0 = 2 \\times 10^{-5}$.\n\n- Case $3$ (GPU, CudaSpace, LayoutLeft, RangePolicy strided-$i$ mapping):\n  - $(n_x,n_y,n_z) = (512, 512, 128)$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 1.0$.\n  - $R = 1.9 \\times 10^{13}$, $u_R = 0.8$.\n  - $B = 9.0 \\times 10^{11}$, $u_B = 1.0$, $c = 0.2$.\n  - $t_0 = 2 \\times 10^{-5}$.\n\n- Case $4$ (GPU, CudaSpace, LayoutRight, TeamPolicy with shared memory tiling):\n  - $(n_x,n_y,n_z) = (512, 512, 128)$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 0.4$.\n  - $R = 1.9 \\times 10^{13}$, $u_R = 0.8$.\n  - $B = 9.0 \\times 10^{11}$, $u_B = 1.0$, $c = 0.9$.\n  - $t_0 = 2 \\times 10^{-5}$.\n\n- Case $5$ (GPU, CudaSpace, LayoutRight, RangePolicy contiguous-$i$, small grid overhead-sensitive):\n  - $(n_x,n_y,n_z) = (64, 64, 64)$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 1.0$.\n  - $R = 1.9 \\times 10^{13}$, $u_R = 0.8$.\n  - $B = 9.0 \\times 10^{11}$, $u_B = 1.0$, $c = 0.95$.\n  - $t_0 = 2 \\times 10^{-5}$.\n\nAngle units do not apply. All physical quantities are in International System of Units (SI). Express all times in seconds as decimal floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each entry is the predicted per time-step runtime $t_{\\text{total}}$ for the corresponding case, in seconds.",
            "solution": "We ground the analysis in the roofline model, which bounds performance by both floating-point throughput and memory bandwidth. For a three-dimensional stencil kernel, the total number of lattice updates is $N = n_x n_y n_z$. Let the floating-point operations per update be $F$, the data transferred per update be $Q$, the peak floating-point rate be $R$, and the peak bandwidth be $B$.\n\nThe achieved compute rate and bandwidth incorporate utilization and access efficiency:\n$$ R_{\\text{eff}} = u_R \\cdot R, \\quad B_{\\text{eff}} = u_B \\cdot c \\cdot B. $$\nAn execution overhead $t_0$ captures non-amortized costs, such as kernel launch. The total time per step is thus\n$$ t_{\\text{total}} = t_0 + \\max\\left( \\frac{N \\cdot F}{R_{\\text{eff}}}, \\frac{N \\cdot Q}{B_{\\text{eff}}} \\right). $$\n\nWe now derive the data traffic model $Q$ for a $S$-point stencil on a double-precision grid:\n- Each update performs one store, contributing $b$ bytes.\n- On many Central Processing Units (CPU) with write-allocate caches, stores cause an extra read of the destination cache line unless non-temporal stores are used. Model this by $w_{\\text{alloc}} \\in \\{0,1\\}$, adding $w_{\\text{alloc}} \\cdot b$ bytes.\n- One load of the center contributes $b$ bytes.\n- Neighbor loads: there are $(S-1)$ neighbors. Due to spatial-temporal reuse and parallel policy scheduling, only a fraction $\\rho \\in (0,1]$ must be fetched from cache or memory for each update. On a Graphics Processing Unit (GPU), when using TeamPolicy and on-chip shared memory (scratch), a fraction $g \\in (0,1]$ of neighbor loads has to access global memory because the rest are served from shared memory; without scratch tiling, take $g=1$. Hence, the neighbor contribution is $g \\cdot \\rho \\cdot (S-1) \\cdot b$.\n\nCombining these gives\n$$ Q = b \\cdot \\left(1 + w_{\\text{alloc}} + 1 + g \\cdot \\rho \\cdot (S-1)\\right). $$\n\nThe arithmetic intensity is $I = F / Q$, and the kernel is memory-bound if $\\frac{NQ}{B_{\\text{eff}}} \\ge \\frac{NF}{R_{\\text{eff}}}$, otherwise compute-bound.\n\nWe apply this framework to the test suite.\n\nParameters common to all cases: $S = 7$, $b = 8$, $F = 13$.\n\n- Case $1$ (CPU, HostSpace, LayoutRight, MDRangePolicy tiled):\n  - $(n_x,n_y,n_z) = (512,512,128)$ so $N = 512 \\cdot 512 \\cdot 128 = 33{,}554{,}432$.\n  - $w_{\\text{alloc}} = 1$, $\\rho = 0.5$, $g = 1$.\n  - $Q = 8 \\cdot (1 + 1 + 1 + 1 \\cdot 0.5 \\cdot (7-1)) = 8 \\cdot (3 + 3) = 8 \\cdot 6 = 48$ bytes.\n  - $R = 1.0 \\times 10^{12}$, $u_R = 0.6$, so $R_{\\text{eff}} = 0.6 \\times 10^{12} = 6.0 \\times 10^{11}$.\n  - $B = 1.5 \\times 10^{11}$, $u_B = 0.8$, $c = 1.0$, so $B_{\\text{eff}} = 0.8 \\cdot 1.0 \\cdot 1.5 \\times 10^{11} = 1.2 \\times 10^{11}$ bytes per second.\n  - $t_0 = 5 \\times 10^{-6}$.\n  - Compute-bound time: $\\frac{N F}{R_{\\text{eff}}} = \\frac{33{,}554{,}432 \\cdot 13}{6.0 \\times 10^{11}} \\approx 7.270126933\\cdot 10^{-4}$ seconds.\n  - Memory-bound time: $\\frac{N Q}{B_{\\text{eff}}} = \\frac{33{,}554{,}432 \\cdot 48}{1.2 \\times 10^{11}} \\approx 1.34217728 \\cdot 10^{-2}$ seconds.\n  - The max is the memory time, so $t_{\\text{total}} \\approx 1.34217728 \\cdot 10^{-2} + 5 \\cdot 10^{-6} \\approx 1.34267728 \\cdot 10^{-2}$ seconds.\n\n- Case $2$ (GPU, CudaSpace, LayoutRight, RangePolicy contiguous-$i$):\n  - $(n_x,n_y,n_z) = (512,512,128)$, $N = 33{,}554{,}432$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 1.0$.\n  - $Q = 8 \\cdot (1 + 0 + 1 + 1.0 \\cdot 1.0 \\cdot 6) = 8 \\cdot 8 = 64$ bytes.\n  - $R = 1.9 \\times 10^{13}$, $u_R = 0.8$, so $R_{\\text{eff}} = 1.52 \\times 10^{13}$.\n  - $B = 9.0 \\times 10^{11}$, $u_B = 1.0$, $c = 0.95$, so $B_{\\text{eff}} = 8.55 \\times 10^{11}$ bytes per second.\n  - $t_0 = 2 \\times 10^{-5}$.\n  - Compute-bound time: $\\frac{N F}{R_{\\text{eff}}} = \\frac{33{,}554{,}432 \\cdot 13}{1.52 \\times 10^{13}} \\approx 2.870436947 \\cdot 10^{-5}$ seconds.\n  - Memory-bound time: $\\frac{N Q}{B_{\\text{eff}}} = \\frac{33{,}554{,}432 \\cdot 64}{8.55 \\times 10^{11}} \\approx 2.511\\cdot 10^{-3}$ seconds.\n  - The max is the memory time, so $t_{\\text{total}} \\approx 2.511\\cdot 10^{-3} + 2 \\cdot 10^{-5} \\approx 2.531\\cdot 10^{-3}$ seconds.\n\n- Case $3$ (GPU, CudaSpace, LayoutLeft, RangePolicy strided-$i$):\n  - Same $N$ and $Q$ as Case $2$.\n  - $R_{\\text{eff}}$ same as Case $2$.\n  - $B_{\\text{eff}} = 1.0 \\cdot 0.2 \\cdot 9.0 \\times 10^{11} = 1.8 \\times 10^{11}$ bytes per second due to poor coalescing.\n  - $t_0 = 2 \\times 10^{-5}$.\n  - Compute-bound time same as Case $2$: $\\approx 2.870436947 \\cdot 10^{-5}$ seconds.\n  - Memory-bound time: $\\frac{33{,}554{,}432 \\cdot 64}{1.8 \\times 10^{11}} \\approx 1.193 \\cdot 10^{-2}$ seconds.\n  - The max is the memory time, so $t_{\\text{total}} \\approx 1.193 \\cdot 10^{-2} + 2 \\cdot 10^{-5} \\approx 1.195 \\cdot 10^{-2}$ seconds.\n\n- Case $4$ (GPU, CudaSpace, LayoutRight, TeamPolicy with shared memory tiling):\n  - Same $N$ as Case $2$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 0.4$.\n  - $Q = 8 \\cdot (1 + 0 + 1 + 0.4 \\cdot 6) = 8 \\cdot 4.4 = 35.2$ bytes.\n  - $R_{\\text{eff}} = 1.52 \\times 10^{13}$.\n  - $B_{\\text{eff}} = 1.0 \\cdot 0.9 \\cdot 9.0 \\times 10^{11} = 8.1 \\times 10^{11}$ bytes per second.\n  - $t_0 = 2 \\times 10^{-5}$.\n  - Compute-bound time identical to Case $2$: $\\approx 2.870436947 \\cdot 10^{-5}$ seconds.\n  - Memory-bound time: $\\frac{33{,}554{,}432 \\cdot 35.2}{8.1 \\times 10^{11}} \\approx 1.45 \\cdot 10^{-3}$ seconds.\n  - The max is the memory time, so $t_{\\text{total}} \\approx 1.45 \\cdot 10^{-3} + 2 \\cdot 10^{-5} \\approx 1.47 \\cdot 10^{-3}$ seconds.\n\n- Case $5$ (GPU, CudaSpace, LayoutRight, RangePolicy contiguous-$i$, small grid):\n  - $(n_x,n_y,n_z) = (64,64,64)$ so $N = 64 \\cdot 64 \\cdot 64 = 262{,}144$.\n  - $w_{\\text{alloc}} = 0$, $\\rho = 1.0$, $g = 1.0$, thus $Q = 64$ bytes.\n  - $R_{\\text{eff}} = 1.52 \\times 10^{13}$, $B_{\\text{eff}} = 8.55 \\times 10^{11}$ bytes per second, $t_0 = 2 \\times 10^{-5}$.\n  - Compute-bound time: $\\frac{262{,}144 \\cdot 13}{1.52 \\times 10^{13}} \\approx 2.2417763158 \\cdot 10^{-7}$ seconds.\n  - Memory-bound time: $\\frac{262{,}144 \\cdot 64}{8.55 \\times 10^{11}} \\approx 1.962 \\cdot 10^{-5}$ seconds.\n  - The max is the memory time, and overhead is comparable, giving $t_{\\text{total}} \\approx 3.962 \\cdot 10^{-5}$ seconds.\n\nThese estimates show:\n- On the CPU with tiling, the kernel is memory-bound due to relatively low bandwidth per core relative to data motion, despite reuse ($\\rho = 0.5$).\n- On the GPU with good coalescing, the kernel is strongly memory-bound but fast due to high bandwidth.\n- Poor coalescing on GPU (LayoutLeft with RangePolicy mapping across the fastest index) throttles bandwidth and significantly increases runtime.\n- TeamPolicy with shared memory reduces global memory traffic (smaller $Q$ via $g = 0.4$), improving performance.\n- For small grids, kernel launch overhead $t_0$ is non-negligible and materially affects runtime.\n\nYour program will implement these calculations precisely and output the predicted runtimes in seconds for the five cases as a single comma-separated list in one line.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef predict_time(nx, ny, nz, S, F, b, walloc, rho, g,\n                 R_peak, u_R, B_peak, u_B, c, t0):\n    \"\"\"\n    Predict time per time-step using a roofline-based model.\n\n    Parameters:\n        nx, ny, nz: grid dimensions\n        S: stencil size (including center)\n        F: flops per update\n        b: bytes per scalar\n        walloc: write-allocate flag (0 or 1)\n        rho: neighbor miss fraction (0,1]\n        g: fraction of neighbor loads going to global memory (0,1]\n        R_peak: peak flops/s\n        u_R: compute utilization factor\n        B_peak: peak bytes/s\n        u_B: bandwidth utilization factor\n        c: coalescing factor\n        t0: overhead (s)\n\n    Returns:\n        Predicted total time per time-step (s).\n    \"\"\"\n    N = nx * ny * nz\n    # Bytes per lattice update: 1 store + optional write-allocate read + 1 center read + reduced neighbor reads\n    Q = b * (1 + walloc + 1 + g * rho * (S - 1))\n    R_eff = u_R * R_peak\n    B_eff = u_B * c * B_peak\n    # Avoid division by zero\n    t_comp = (N * F) / R_eff if R_eff > 0 else float('inf')\n    t_mem = (N * Q) / B_eff if B_eff > 0 else float('inf')\n    t_total = t0 + max(t_comp, t_mem)\n    return t_total\n\ndef solve():\n    # Common parameters across cases\n    S = 7           # stencil points\n    F = 13          # flops per update\n    b = 8           # bytes per double\n\n    test_cases = [\n        # Case 1: CPU, HostSpace, LayoutRight, MDRangePolicy tiled\n        dict(nx=512, ny=512, nz=128,\n             walloc=1, rho=0.5, g=1.0,\n             R_peak=1.0e12, u_R=0.6,\n             B_peak=1.5e11, u_B=0.8, c=1.0,\n             t0=5e-6),\n\n        # Case 2: GPU, CudaSpace, LayoutRight, RangePolicy contiguous-i\n        dict(nx=512, ny=512, nz=128,\n             walloc=0, rho=1.0, g=1.0,\n             R_peak=1.9e13, u_R=0.8,\n             B_peak=9.0e11, u_B=1.0, c=0.95,\n             t0=2e-5),\n\n        # Case 3: GPU, CudaSpace, LayoutLeft, RangePolicy strided-i (poor coalescing)\n        dict(nx=512, ny=512, nz=128,\n             walloc=0, rho=1.0, g=1.0,\n             R_peak=1.9e13, u_R=0.8,\n             B_peak=9.0e11, u_B=1.0, c=0.2,\n             t0=2e-5),\n\n        # Case 4: GPU, CudaSpace, LayoutRight, TeamPolicy with shared memory tiling\n        dict(nx=512, ny=512, nz=128,\n             walloc=0, rho=1.0, g=0.4,\n             R_peak=1.9e13, u_R=0.8,\n             B_peak=9.0e11, u_B=1.0, c=0.9,\n             t0=2e-5),\n\n        # Case 5: GPU, CudaSpace, LayoutRight, RangePolicy contiguous-i, small grid\n        dict(nx=64, ny=64, nz=64,\n             walloc=0, rho=1.0, g=1.0,\n             R_peak=1.9e13, u_R=0.8,\n             B_peak=9.0e11, u_B=1.0, c=0.95,\n             t0=2e-5),\n    ]\n\n    results = []\n    for case in test_cases:\n        t = predict_time(\n            nx=case[\"nx\"], ny=case[\"ny\"], nz=case[\"nz\"],\n            S=S, F=F, b=b,\n            walloc=case[\"walloc\"], rho=case[\"rho\"], g=case[\"g\"],\n            R_peak=case[\"R_peak\"], u_R=case[\"u_R\"],\n            B_peak=case[\"B_peak\"], u_B=case[\"u_B\"], c=case[\"c\"],\n            t0=case[\"t0\"]\n        )\n        results.append(t)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "For memory-bandwidth-limited kernels on GPUs, such as the stencil computations common in CFD, on-chip shared memory provides a powerful mechanism for performance optimization. However, its effective use requires a careful balancing of hardware resources to maximize parallelism. This hands-on design problem challenges you to find the optimal dimensions for a shared-memory tile , navigating the trade-offs between threads, registers, and memory capacity to maximize the GPU's Streaming Multiprocessor (SM) occupancy.",
            "id": "3329340",
            "problem": "You are asked to formalize and implement an occupancy-driven tile selection strategy for a three-dimensional stencil arising in Computational Fluid Dynamics (CFD) on a Graphics Processing Unit (GPU) using shared memory tiling. The goal is to choose integer tile dimensions $(t_x,t_y,t_z)$ that maximize Streaming Multiprocessor (SM) occupancy under resource constraints due to threads, registers, and shared memory. Your program must search over tile shapes, evaluate resource usage, and select the optimal tile according to a well-defined objective and explicit tie-breaking rules.\n\nThe physical and algorithmic context is as follows. Consider a three-dimensional Navier–Stokes discretization (finite volume or finite difference) with a power-of-two aligned warp size. A tile held in shared memory must include a halo of width $h$ along each dimension in order to compute all interior stencil points. Let the interior (threaded) work per block be $N_{\\mathrm{int}} = t_x t_y t_z$. The block assigns one thread per interior cell. The shared memory footprint per block is\n$$\nS_{\\mathrm{block}} = (t_x + 2h)(t_y + 2h)(t_z + 2h) \\, F \\, s,\n$$\nwhere $F$ is the number of per-cell state variables and $s$ is the bytes per variable. The register usage per thread is $r$ registers, so registers per block is $R_{\\mathrm{block}} = r \\, N_{\\mathrm{int}}$.\n\nYou will use the following fundamental definitions and well-tested facts as the basis of your derivation and implementation:\n- A GPU executes threads in groups of warps of size $W$ threads. The number of warps per block is $W_b = \\lceil N_{\\mathrm{int}}/W \\rceil$.\n- A Streaming Multiprocessor (SM) has resource limits: maximum threads per block $T_{\\mathrm{block}}^{\\max}$, maximum blocks per SM $B_{\\mathrm{SM}}^{\\max}$, maximum threads per SM $T_{\\mathrm{SM}}^{\\max}$, registers per SM $R_{\\mathrm{SM}}$, shared memory per SM $M_{\\mathrm{sh}}$, and maximum shared memory per block $M_{\\mathrm{sh}}^{\\mathrm{block}}$. The maximum warps per SM is $W_{\\mathrm{SM}}^{\\max} = T_{\\mathrm{SM}}^{\\max}/W$.\n- The number of resident blocks on an SM is limited by each resource:\n$$\nb_T = \\left\\lfloor \\frac{T_{\\mathrm{SM}}^{\\max}}{N_{\\mathrm{int}}} \\right\\rfloor, \\quad\nb_R = \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{R_{\\mathrm{block}}} \\right\\rfloor, \\quad\nb_S = \\left\\lfloor \\frac{M_{\\mathrm{sh}}}{S_{\\mathrm{block}}} \\right\\rfloor, \\quad\nb_{\\max} = B_{\\mathrm{SM}}^{\\max}.\n$$\nThe active blocks per SM are $B = \\min(b_T,b_R,b_S,b_{\\max})$, provided the block itself respects $N_{\\mathrm{int}} \\le T_{\\mathrm{block}}^{\\max}$ and $S_{\\mathrm{block}} \\le M_{\\mathrm{sh}}^{\\mathrm{block}}$.\n- The SM occupancy is the fraction of active warps over the maximum warps:\n$$\n\\phi = \\frac{\\min\\left(W_{\\mathrm{SM}}^{\\max}, \\, B \\, W_b \\right)}{W_{\\mathrm{SM}}^{\\max}}.\n$$\n\nYour program must select $(t_x,t_y,t_z)$ by maximizing $\\phi$ subject to feasibility constraints. In addition, to emphasize shared-memory tiling efficacy for CFD stencils, impose the following practical constraints:\n- $N_{\\mathrm{int}} = t_x t_y t_z$ must be a multiple of $W$ to avoid partially filled warps.\n- $t_x, t_y, t_z$ are integers in the inclusive range $\\{4,5,6,\\dots,32\\}$.\n- Feasibility requires $N_{\\mathrm{int}} \\le T_{\\mathrm{block}}^{\\max}$ and $S_{\\mathrm{block}} \\le M_{\\mathrm{sh}}^{\\mathrm{block}}$.\n- Assume $h \\ge 1$ unless otherwise specified in the test suite.\n\nTo break ties between multiple $(t_x,t_y,t_z)$ with identical maximal $\\phi$, use the following order of preference:\n1. Maximize the shared-memory reuse ratio\n$$\n\\rho = \\frac{N_{\\mathrm{int}}}{(t_x + 2h)(t_y + 2h)(t_z + 2h)}.\n$$\n2. Maximize $N_{\\mathrm{int}}$.\n3. Choose the lexicographically smallest $(t_x,t_y,t_z)$.\n\nYour program must implement a complete search over all integer $(t_x,t_y,t_z)$ in the specified range, apply the constraints, compute $\\phi$ and $\\rho$ for each feasible block, and select the optimal tile according to the rules above. For each test case, the output should be the selected $(t_x,t_y,t_z)$ and the corresponding occupancy $\\phi$ as a real number between $0$ and $1$.\n\nThere are no physical units in this problem; all quantities are dimensionless or measured in bytes. Angles are not involved. Percentages must not be used; any fractional quantity should be expressed as a decimal number.\n\nTest Suite. Use the following test cases. Each case is a tuple containing $(W, T_{\\mathrm{block}}^{\\max}, B_{\\mathrm{SM}}^{\\max}, T_{\\mathrm{SM}}^{\\max}, R_{\\mathrm{SM}}, M_{\\mathrm{sh}}, M_{\\mathrm{sh}}^{\\mathrm{block}}, r, h, F, s)$ where all numbers are integers:\n- Case $1$: $(32, 1024, 32, 2048, 65536, 102400, 98304, 64, 1, 5, 8)$.\n- Case $2$: $(32, 1024, 32, 2048, 65536, 49152, 49152, 32, 1, 7, 8)$.\n- Case $3$: $(32, 1024, 32, 2048, 65536, 102400, 98304, 128, 1, 3, 8)$.\n- Case $4$: $(32, 512, 16, 1536, 32768, 65536, 49152, 40, 1, 4, 4)$.\n- Case $5$: $(32, 1024, 32, 2048, 65536, 102400, 98304, 64, 2, 9, 8)$.\n\nFinal Output Format. Your program should produce a single line of output containing a list of results, one per test case, in order. Each result is itself a list of four numbers $[t_x, t_y, t_z, \\phi]$. The final line must be a single Python-style list with comma-separated entries and no additional text, for example:\n$[ [t_{x,1}, t_{y,1}, t_{z,1}, \\phi_1], [t_{x,2}, t_{y,2}, t_{z,2}, \\phi_2], \\dots ]$.\n\nYour solution must be a complete, runnable program that implements the above logic and prints the required single-line output exactly as specified.",
            "solution": "The problem is valid. It presents a well-posed optimization task grounded in the established principles of high-performance computing and GPU architecture. The problem is self-contained, with all necessary constants, formulas, and constraints clearly defined. The objective function, search space, and multi-level tie-breaking rules are specified unambiguously, ensuring a unique solution exists and can be found via a deterministic algorithm. The parameters provided in the test suite are realistic for modern GPU hardware. There are no scientific inaccuracies, contradictions, or ill-posed elements.\n\nThe objective is to determine the optimal integer tile dimensions $(t_x, t_y, t_z)$ for a three-dimensional stencil computation on a GPU. The optimization goal is to maximize the Streaming Multiprocessor (SM) occupancy, $\\phi$, subject to a series of hardware and algorithmic constraints. The problem requires a complete search over the specified parameter space for $(t_x, t_y, t_z)$.\n\nThe methodology to solve this problem is a systematic search and evaluation process for each test case.\n\nFirst, we define the search space. The tile dimensions $t_x$, $t_y$, and $t_z$ are integers within the inclusive range $\\{4, 5, \\dots, 32\\}$. Our algorithm will perform an exhaustive search by iterating through all possible combinations of $(t_x, t_y, t_z)$ in this range.\n\nFor each candidate tile configuration $(t_x, t_y, t_z)$, we first evaluate its feasibility based on a set of given constraints. A tile is considered feasible only if it satisfies all of the following conditions:\n$1$. The number of threads in the block, $N_{\\mathrm{int}} = t_x t_y t_z$, must be a multiple of the warp size $W$.\n$2$. The number of threads per block must not exceed the hardware limit: $N_{\\mathrm{int}} \\le T_{\\mathrm{block}}^{\\max}$.\n$3$. The shared memory required for the tile, including a halo of width $h$, must not exceed the maximum allowed per block. The shared memory footprint is given by $S_{\\mathrm{block}} = (t_x + 2h)(t_y + 2h)(t_z + 2h) \\cdot F \\cdot s$, where $F$ is the number of state variables per cell and $s$ is the size in bytes of each variable. The constraint is $S_{\\mathrm{block}} \\le M_{\\mathrm{sh}}^{\\mathrm{block}}$.\n\nIf a tile configuration $(t_x, t_y, t_z)$ fails any of these checks, it is discarded, and the search proceeds to the next configuration.\n\nIf a tile is feasible, we proceed to calculate its performance metrics. The primary metric is the SM occupancy, $\\phi$. Its calculation depends on the number of blocks that can reside concurrently on a single SM. This number, denoted by $B$, is limited by the SM's resources: total threads, total registers, and total shared memory.\nThe number of registers used by a block is $R_{\\mathrm{block}} = r \\cdot N_{\\mathrm{int}}$, where $r$ is the register count per thread.\nThe maximum number of concurrent blocks, as limited by each resource, is calculated as:\n-   By threads: $b_T = \\lfloor T_{\\mathrm{SM}}^{\\max} / N_{\\mathrm{int}} \\rfloor$.\n-   By registers: $b_R = \\lfloor R_{\\mathrm{SM}} / R_{\\mathrm{block}} \\rfloor$, assuming $R_{\\mathrm{block}} > 0$.\n-   By shared memory: $b_S = \\lfloor M_{\\mathrm{sh}} / S_{\\mathrm{block}} \\rfloor$, assuming $S_{\\mathrm{block}} > 0$.\n\nThe actual number of active blocks per SM is the minimum of these values, further capped by the architectural limit $B_{\\mathrm{SM}}^{\\max}$:\n$$B = \\min(b_T, b_R, b_S, B_{\\mathrm{SM}}^{\\max})$$\nThe number of warps in a block is $W_b = \\lceil N_{\\mathrm{int}} / W \\rceil$. Since $N_{\\mathrm{int}}$ is constrained to be a multiple of $W$, this simplifies to $W_b = N_{\\mathrm{int}} / W$.\nThe maximum number of warps an SM can support is $W_{\\mathrm{SM}}^{\\max} = T_{\\mathrm{SM}}^{\\max} / W$.\nThe total number of active warps on the SM is $B \\cdot W_b$, capped by $W_{\\mathrm{SM}}^{\\max}$. The occupancy $\\phi$ is the ratio of active warps to the maximum possible warps:\n$$\\phi = \\frac{\\min(W_{\\mathrm{SM}}^{\\max}, B \\cdot W_b)}{W_{\\mathrm{SM}}^{\\max}}$$\n\nWith $\\phi$ calculated, we compare the current tile configuration to the best one found so far. The comparison follows a strict-lexicographical tie-breaking procedure:\n$1$. The primary objective is to maximize the occupancy $\\phi$.\n$2$. If two configurations yield the same $\\phi$, we select the one with the higher shared-memory reuse ratio, $\\rho = \\frac{N_{\\mathrm{int}}}{(t_x + 2h)(t_y + 2h)(t_z + 2h)}$.\n$3$. If a tie persists, we select the configuration with the larger number of threads, $N_{\\mathrm{int}}$.\n$4$. As a final tie-breaker, we choose the configuration $(t_x, t_y, t_z)$ that is lexicographically smallest.\n\nTo implement this multi-level optimization efficiently, we can construct a scoring tuple for each feasible tile: $(\\phi, \\rho, N_{\\mathrm{int}}, -t_x, -t_y, -t_z)$. We seek to find the tile that maximizes this score tuple, where standard tuple comparison in Python will correctly implement the specified ordered criteria. The negation of $t_x, t_y, t_z$ transforms the minimization of the lexicographical tuple $(t_x, t_y, t_z)$ into a maximization problem, fitting neatly into the single-objective framework.\n\nThe algorithm proceeds by initializing a `best_score` tuple with placeholder values lower than any possible outcome (e.g., $(-1.0, -1.0, -1, 0, 0, 0)$). As the search iterates through all $29 \\times 29 \\times 29$ tile configurations, this `best_score` and the corresponding optimal tile dimensions are updated whenever a better configuration is found. After the search is complete, the optimal $(t_x, t_y, t_z)$ and its corresponding occupancy $\\phi$ are recorded for the test case. This process is repeated for all cases in the test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GPU tile selection problem for a series of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # (W, T_block_max, B_SM_max, T_SM_max, R_SM, M_sh, M_sh_block, r, h, F, s)\n        (32, 1024, 32, 2048, 65536, 102400, 98304, 64, 1, 5, 8),\n        (32, 1024, 32, 2048, 65536, 49152, 49152, 32, 1, 7, 8),\n        (32, 1024, 32, 2048, 65536, 102400, 98304, 128, 1, 3, 8),\n        (32, 512, 16, 1536, 32768, 65536, 49152, 40, 1, 4, 4),\n        (32, 1024, 32, 2048, 65536, 102400, 98304, 64, 2, 9, 8),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        W, T_block_max, B_SM_max, T_SM_max, R_SM, M_sh, M_sh_block, r, h, F, s = case\n        \n        best_tile_dims = None\n        # Score is a tuple: (phi, rho, N_int, -tx, -ty, -tz) for maximization\n        best_score = (-1.0, -1.0, -1, 0, 0, 0)\n\n        tile_dim_range = range(4, 33)\n\n        for tx in tile_dim_range:\n            for ty in tile_dim_range:\n                for tz in tile_dim_range:\n                    N_int = tx * ty * tz\n\n                    # Constraint 1: N_int must be a multiple of W\n                    if N_int % W != 0:\n                        continue\n                    \n                    # Constraint 2: N_int = T_block_max\n                    if N_int > T_block_max:\n                        continue\n\n                    S_block_vol = (tx + 2 * h) * (ty + 2 * h) * (tz + 2 * h)\n                    S_block = S_block_vol * F * s\n\n                    # Constraint 3: S_block = M_sh_block\n                    if S_block > M_sh_block:\n                        continue\n\n                    # Tile is feasible, calculate performance metrics\n                    \n                    # Registers per block\n                    R_block = r * N_int\n                    \n                    # Blocks per SM limitations\n                    # Note: Denominators cannot be zero due to problem constraints\n                    b_T = T_SM_max // N_int if N_int > 0 else B_SM_max\n                    b_R = R_SM // R_block if R_block > 0 else B_SM_max\n                    b_S = M_sh // S_block if S_block > 0 else B_SM_max\n\n                    # Active blocks per SM\n                    B = min(b_T, b_R, b_S, B_SM_max)\n\n                    # Warps per block\n                    W_b = N_int // W\n                    \n                    # Max warps per SM\n                    W_SM_max = T_SM_max // W\n\n                    # Occupancy\n                    active_warps = B * W_b\n                    phi = min(W_SM_max, active_warps) / W_SM_max\n\n                    # Shared memory reuse ratio\n                    rho = N_int / S_block_vol\n\n                    # Compare with best tile found so far\n                    current_score = (phi, rho, N_int, -tx, -ty, -tz)\n\n                    if current_score > best_score:\n                        best_score = current_score\n                        best_tile_dims = (tx, ty, tz)\n\n        # Append result for the current case\n        final_phi = best_score[0]\n        result_list = [*best_tile_dims, final_phi]\n        all_results.append(result_list)\n        \n    # Format the final output string as specified\n    formatted_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]\n    final_output_str = f\"[{','.join(formatted_strings)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}